# 来吧，改变我的想法

> 原文：<https://towardsdatascience.com/go-ahead-change-my-ai-mind-89f64feab8eb?source=collection_archive---------39----------------------->

## [公平和偏见](https://towardsdatascience.com/tagged/fairness-and-bias)

## 代理作为人工智能公平辩论中缺失的成分。

![](img/0c10a1485d989b1d5b7a05cc479eeff7.png)

你曾经试图改变别人的想法吗？你当然有。毕竟，从根本上讲，这是大部分交流的目的。这也非常困难。

当谈到塑造了我们的根深蒂固的政治、宗教或社会信仰时，就更难了，因为它们往往与我们自己的身份联系在一起。许多信念甚至是完全无意识的，只能通过[间接手段](https://implicit.harvard.edu/implicit/takeatest.html)引出。没有人知道我分发的工作申请是否会因为我的种族或性别而受到不同的对待，因为我交给的人可能甚至没有意识到他们自己的偏见。如果是的话，他们很容易隐藏。以持续、持久的方式改变一个人的想法需要多年的时间和奉献，尤其是当一种信仰[根植于某人的情感经历](https://www.lesswrong.com/posts/i9xyZBS3qzA8nFXNQ)时。

人工智能系统并非如此。尽管存在缺陷和偏见，但人工智能系统与人类决策系统的一个区别在于，暴露它们的偏见并改变它们的想法相对容易。

如果你想发现某人在评估求职者时的偏见，正确的方法可能是给他们数千份申请，根据你寻求的属性，对照所有其他属性，审查和衡量结果的统计差异。虽然当有足够多的申请人通过标准化流程时，这通常会在团体层面进行，但对于任何单个决策者来说，这是不可能的，这使得很难进行干预，除非是在政策层面。你不可能让一个人审查成千上万份申请，然后得到有统计学意义的结果。你不能问决策者他们的偏见，因为他们可能没有意识到这些偏见，或者他们可能有隐藏这些偏见的既得利益。

另一方面，人工智能系统可以处理成千上万的测试样本。事实上，正是这种可伸缩性使它们成为如此多审查的对象:很容易发现它们，并且通过增加样本大小，任何缺陷通常都可以用可接受的 [p 值](https://en.wikipedia.org/wiki/P-value)发现。这也是一件好事，因为它使我们能够深入研究那些有可能以各种方式对人们的生活产生巨大影响的系统。

但是这种[可用性偏差](https://en.wikipedia.org/wiki/Availability_heuristic)并不经常被认识到:因为类似的偏差研究很难在人身上进行，所以它们很少被执行。我们听到的许多关于人工智能偏见的叙述正是因为这是我们可以衡量的，而影响当今世界的绝大多数决策系统都是人类的，不透明的，经得起统计审查。

当我们*在人工智能系统中发现偏差时，我们可以当场改变他们的想法。我们可以控制他们接触到的数据和事实，以便建立他们的知识库。作为模型设计者，我们可以修改我们将哪些归纳偏差编码到其中，以及它试图优化的成本函数。作为一个社区，我们越来越好地提高了对我们所拥有的相关旋钮的理解。例如，研究表明，用[非常](https://arxiv.org/abs/2101.00190) [很少](https://arxiv.org/abs/2005.14165)的基础陈述，你可以影响一个非常大的神经模型的整个‘信念系统’。您还可以[微调 ML 系统做出的权衡](https://arxiv.org/abs/1610.02413)，以确保公平的结果，而不牺牲性能。这是很了不起的，尤其是当你把它与几乎不可能对我们基本上无意识的人类认知偏见有任何作用的情况进行对比时。*

结果的一致性也有内在的公平性。没有根本原因而改变的决策本质上是不公平的，这就是为什么自动决策系统不一定需要比人类同行表现得更好才能提供更公平的结果，只要它们减少了结果的差异。人工智能不会遭受决策疲劳。

> 在评估机器学习的公平性和道德含义时，可审计性、可控性和一致性值得占据中心位置。

有些人会说，这里的障碍不是人类决策系统，而是基于规则的系统，这些系统也可以说是一致的、可控的和可审计的。但是这里存在着另一个陷阱:越糟越不公平。一个在任务中表现更差的系统，即使更简单、更容易推理，通常在结果的公平性方面更差。我曾经从事自动语音识别的工作，在早期，这些系统在识别英语口语方面效果不佳。但受影响最大的并不是在美国出生的中年白人男性，他们在完全安静的办公室环境中工作。年轻人和老年人，母语非英语的人，生活在充满挑战的嘈杂的声学环境中的人。以透明性和可检验性的名义争论更粗糙、更简单的模型常常忽略了一个简单的事实，即一个更好的决策系统通常比分布的主要模式更积极地影响“长尾”。事实上，尽管人们对使用越来越大、越来越复杂的语言模型感到担忧，但有强有力的证据表明，语言模型做得越好，它们就越符合人类共同的价值观。

人们实际上可以说，在人工智能系统的基本性能方面所做的工作对公平结果的影响比许多直接针对人工智能公平的文献要深远得多，尽管如果没有今天在这个主题上的大量工作，我们甚至无法评估这种影响。我们在这个领域的更多集体工作应该越来越少地以仅仅揭露他们的缺陷为目标，而是更多地利用我们对人工智能系统的这种代理，以及我们改变他们的想法以实现更公平和更公正结果的前所未有的能力。

*(感谢 Ed H. Chi 对本文的宝贵反馈。)*