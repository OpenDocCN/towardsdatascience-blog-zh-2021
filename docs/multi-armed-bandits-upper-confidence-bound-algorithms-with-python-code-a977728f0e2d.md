# å¤šè‡‚å¼ºç›—:å¸¦ Python ä»£ç çš„ç½®ä¿¡ä¸Šé™ç®—æ³•

> åŸæ–‡ï¼š<https://towardsdatascience.com/multi-armed-bandits-upper-confidence-bound-algorithms-with-python-code-a977728f0e2d?source=collection_archive---------7----------------------->

## [å®è·µæ•™ç¨‹](https://towardsdatascience.com/tagged/hands-on-tutorials)

## äº†è§£ä¸åŒçš„ç½®ä¿¡ä¸Šé™ bandit ç®—æ³•(UCB1ã€UCB1-Tunedã€UCB1-Normal)ã€‚ä¸ºæ‰€æœ‰å®éªŒæä¾›çš„ Python ä»£ç ã€‚

![](img/55534e703ccc2b3b1e5452e84fa73b22.png)

ç”±[ä¸¹å°¼æ–¯Â·ç®€æ–¯](https://unsplash.com/@dmjdenise?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)åœ¨ [Unsplash](https://unsplash.com/s/photos/film?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText) ä¸Šæ‹æ‘„çš„ç…§ç‰‡

# ä»‹ç»

åœ¨è¿™ä¸€ç³»åˆ—çš„å¸–å­ä¸­ï¼Œæˆ‘ä»¬å°è¯•äº†ä¸åŒçš„ bandit ç®—æ³•æ¥ä¼˜åŒ–æˆ‘ä»¬çš„ç”µå½±ä¹‹å¤œâ€”â€”æ›´å…·ä½“åœ°è¯´ï¼Œæˆ‘ä»¬å¦‚ä½•é€‰æ‹©ç”µå½±å’Œé¤é¦†æ¥é€é¤ï¼

å¯¹äºæ–°æ¥çš„äººæ¥è¯´ï¼Œbandit è¿™ä¸ªåå­—æ¥è‡ªåƒè§’å­è€è™æœº(è¢«ç§°ä¸ºç‹¬è‡‚å¼ºç›—)ã€‚ä½ å¯ä»¥æŠŠå®ƒæƒ³æˆæ¯æ¬¡å’Œå®ƒäº’åŠ¨(æ‹‰å®ƒèƒ³è†Š)éƒ½èƒ½å¥–åŠ±ä½ (æˆ–è€…ä¸å¥–åŠ±ä½ )çš„ä¸œè¥¿ã€‚ç›®æ ‡æ˜¯ï¼Œç»™å®šä¸€ç¾¤ç»™å‡ºä¸åŒå¥–åŠ±çš„å¼ºç›—ï¼Œå°½å¯èƒ½å¿«åœ°æ‰¾å‡ºç»™å‡ºæœ€é«˜å¥–åŠ±çš„å¼ºç›—ã€‚å½“æˆ‘ä»¬å¼€å§‹ç©æ¸¸æˆå¹¶ä¸æ–­æ”¶é›†å…³äºæ¯ä¸ªå¼ºç›—çš„æ•°æ®æ—¶ï¼Œå¼ºç›—ç®—æ³•ä¼šå¸®åŠ©æˆ‘ä»¬åœ¨åˆ©ç”¨è¿„ä»Šä¸ºæ­¢ç»™æˆ‘ä»¬æœ€é«˜å¥–åŠ±çš„å¼ºç›—å’Œæ¢ç´¢å…¶ä»–å¼ºç›—ä¹‹é—´è¿›è¡Œé€‰æ‹©ã€‚

åœ¨æˆ‘ä»¬çš„æ•…äº‹ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨é¤é¦†è€Œä¸æ˜¯å¼ºç›—ï¼Œæˆ‘ä»¬çš„å›æŠ¥æ˜¯é£Ÿç‰©çš„æ»¡è¶³æ„Ÿï¼:)

<https://medium.com/analytics-vidhya/multi-armed-bandits-part-1-epsilon-greedy-algorithm-with-python-code-534b9e2abc9>  <https://medium.com/swlh/multi-armed-bandits-optimistic-initial-values-algorithm-with-python-code-3970e611b5ab>  

ä½ å’Œä½ çš„æœ‹å‹å·²ç»æœ‰ä¸€æ®µæ—¶é—´æ²¡æœ‰ä½¿ç”¨ bandit ç®—æ³•æ¥ä¼˜åŒ–ä½ çš„ç”µå½±ä¹‹å¤œäº†ã€‚åˆ°ç›®å‰ä¸ºæ­¢ï¼Œä½ ä¸€ç›´åœ¨ä½¿ç”¨ [Epsilon-Greedy](https://medium.com/analytics-vidhya/multi-armed-bandits-part-1-epsilon-greedy-algorithm-with-python-code-534b9e2abc9) (ä¸ğ›†=1/#actions ä¸€èµ·)æ¥å¸®åŠ©å¯»æ‰¾æœ€å¥½çš„é¤é¦†ï¼Œè€Œä½ çš„æœ‹å‹ä¸€ç›´å›ºæ‰§åœ°ä½¿ç”¨[ä¹è§‚åˆå§‹å€¼](https://medium.com/swlh/multi-armed-bandits-optimistic-initial-values-algorithm-with-python-code-3970e611b5ab)ç®—æ³•æ¥æŒ‘é€‰ç”µå½±å¯¼æ¼”(å³ä½¿ä½ çš„æ¨¡æ‹Ÿå·²ç»è¡¨æ˜å®ƒå¯èƒ½ä¸é€‚åˆä½ çš„ç”¨ä¾‹)ã€‚

å½“ä½ åœ¨äº’è”ç½‘ä¸Šæ¶ˆç£¨æ—¶é—´ï¼Œå› ä¸ºå°é”è€Œè¢«å›°åœ¨é‡Œé¢æ—¶ï¼Œä½ ä¼šé‡åˆ°ä¸€äº›åŸºäº**ç½®ä¿¡ä¸Šé™** (UCB)çš„å¼ºç›—ç®—æ³•ã€‚ä½ å…´å¥‹åœ°å‘ç°æœ‰å¯èƒ½æ”¹è¿›å¯¹æœ€ä½³é¤é¦†çš„æœç´¢ï¼Œå¹¶æ›´æœ‰æ•ˆåœ°è§£å†³å‰¥å‰Š-æ¢ç´¢çš„å›°å¢ƒï¼Œä½ å¼€å§‹é˜…è¯»ï¼ŒæœŸå¾…ç€ä¸‹ä¸€ä¸ªç”µå½±ä¹‹å¤œã€‚

ä»–ä»¬ä¸­æœ‰è°ä¼šæ¯”Îµ-Greedy æ›´å¥½å—ï¼Ÿè®©æˆ‘ä»¬æ¥äº†è§£ä¸€ä¸‹ï¼

# ç®—æ³•

è¿™ä¸ªæƒ³æ³•æ˜¯åˆ©ç”¨æˆ‘ä»¬ç›®å‰æ”¶é›†çš„æ•°æ®ï¼Œç”¨æ¦‚ç‡è®ºå¾—å‡ºæ¯ä¸ªåœŸåŒªå¹³å‡æŠ¥é…¬çš„ä¸Šé™ã€‚æˆ‘ä»¬å°±æŠŠè¿™ä¸ªä¸Šç•Œå«åšåœŸåŒªçš„**ä¸Šç½®ä¿¡** **æŒ‡æ ‡**ï¼Œå°±åƒ[åŸè®ºæ–‡](https://link.springer.com/article/10.1023/A:1013689704352) **é‡Œä¸€æ ·ã€‚**ä¸ç½®ä¿¡åŒºé—´ä¸€æ ·ï¼Œå¦‚æœæˆ‘ä»¬æ²¡æœ‰å¤§é‡æ•°æ®ï¼Œæ ·æœ¬å‡å€¼çš„è¾¹ç•Œä¼šå¾ˆå®½ã€‚åœ¨æ¯ä¸€æ­¥ï¼Œæˆ‘ä»¬é€‰æ‹©å…·æœ‰æœ€é«˜ä¸Šç½®ä¿¡æŒ‡æ•°çš„å¼ºç›—ï¼Œè·å¾—å¥–åŠ±å¹¶éšåæ›´æ–°å…¶æŒ‡æ•°ã€‚

è¿™å¬èµ·æ¥æœ‰ç‚¹ç±»ä¼¼äº[ä¹è§‚åˆå§‹å€¼ç®—æ³•](https://medium.com/swlh/multi-armed-bandits-optimistic-initial-values-algorithm-with-python-code-3970e611b5ab)å§ï¼Ÿæå‡ºè¿™ä¸ªæŒ‡æ•°ä¼¼ä¹æ¯”ä»…ä»…é€‰æ‹©ä¸€ä¸ªä¹è§‚çš„åˆå§‹å€¼æ›´å¤æ‚ï¼Œä½†æ˜¯å®ç°ä»ç„¶ç›¸å¯¹ç®€å•(æˆ‘å‘èª“)ã€‚å®é™…ä¸Šï¼Œå®ƒä»¬ä½¿ç”¨èµ·æ¥æ›´ç®€å•ï¼Œå› ä¸ºå®ƒä»¬æ²¡æœ‰ä»»ä½•éœ€è¦æˆ‘ä»¬é€‰æ‹©çš„å‚æ•°ï¼

ä¸€èˆ¬æ¥è¯´ï¼Œæ¯ä¸ª bandit çš„ç½®ä¿¡ä¸Šé™æŒ‡æ•°å…·æœ‰ä»¥ä¸‹å½¢å¼ã€‚

```
Bandit Upper Confidence Index = X_sample_mean + Cwhere
 X_sample_mean:  sample mean for rewards so far
 C > 0:          size of one sided confidence interval
```

éšç€æˆ‘ä»¬ä¸ºå¼ºç›—æ”¶é›†æ›´å¤šçš„æ•°æ®ï¼Œå®ƒåº”è¯¥å‡å°‘ï¼Œè¿™æ˜¯æœ‰æ„ä¹‰çš„ï¼Œå› ä¸ºæˆ‘ä»¬å˜å¾—æ›´åŠ ç¡®å®šå¹³å‡å¥–åŠ±çš„å¯èƒ½å€¼ã€‚æ²¡æœ‰è¢«å……åˆ†æ¢ç´¢çš„å¼ºç›—ä¼šæœ‰ä¸€ä¸ªæ›´å¤§çš„`C`ï¼Œè¿™å°†å¯¼è‡´æˆ‘ä»¬é€‰æ‹©ä»–ä»¬è¿›è¡Œæ¢ç´¢ã€‚

åœ¨ä¼ªä»£ç ä¸­:

```
Choose n # number of iterationsInitialisation step. # for UCB1 & UCB1-Tuned we play each bandit oncefor i = 1 to n do:
  current_bandit = pick bandit with greatest upper confidence index
  reward = current_bandit.pull_arm()    Update upper confidence index for current_bandit
```

å¥½çš„â€¦â€¦æˆ‘ä»¬çŸ¥é“å¦‚ä½•è®¡ç®—æ ·æœ¬å‡å€¼ï¼Œä½†æ˜¯æˆ‘ä»¬å¦‚ä½•è®¡ç®—`C`ï¼Ÿå—¯ï¼Œçœ‹æƒ…å†µå§ï¼

åœ¨[åŸå§‹è®ºæ–‡](https://link.springer.com/article/10.1023/A:1013689704352)ä¸­æå‡ºäº†å‡ ä¸ª UCB ç®—æ³•ï¼Œæ¯ä¸ªç®—æ³•å¯¹`C`éƒ½æœ‰ä¸åŒçš„å®šä¹‰ã€‚ **UCB1** å’Œ **UCB1-Tuned** è®¾è®¡ç”¨äºå¥–åŠ±ä¸º 0 æˆ– 1ï¼Œå³æ¥è‡ªä¼¯åŠªåˆ©åˆ†å¸ƒçš„å¥–åŠ±(å‚è§[æœ¬æ–‡](https://docs.microsoft.com/en-us/archive/msdn-magazine/2019/august/test-run-the-ucb1-algorithm-for-multi-armed-bandit-problems))ã€‚å¦‚æœä½ çš„å›æŠ¥æ¥è‡ªæ­£æ€åˆ†å¸ƒï¼Œæœ‰å¦ä¸€ä¸ªç®—æ³•ï¼Œå«åš**UCB-æ­£æ€**ï¼Œå°±æ˜¯ä¸ºæ­¤è®¾è®¡çš„ã€‚ä½ *å¯ä»¥*ä½¿ç”¨ UCB1 è·å¾—å…¶ä»–å‘è¡Œç‰ˆçš„å¥–åŠ±ï¼Œä½†æ˜¯æ®è¯´æ€§èƒ½æ²¡æœ‰é‚£ä¹ˆå¥½(æˆ‘ä»¬å°†æµ‹è¯•è¿™ä¸€ç‚¹)ã€‚

## UCB1

UCB1 çš„ bandit ç½®ä¿¡æŒ‡æ•°ä¸Šé™ç­‰äº

```
Bandit Upper Confidence Index = Sample Mean + âˆš(2logN / n)where 
  N = total rounds (e.g. number of movie nights so far)
  n = number of times we've played the bandit (
    e.g. number of times we've ordered from a restaurant
  ).
```

![](img/000b69db22358ce6f0671b16e06b86cc.png)

ä½¿ç”¨ UCB1 ä¸å››ä¸ªè·å¾—å¥–åŠ±çš„æ¦‚ç‡ç­‰äº 1 çš„å¼ºç›—(0.6ï¼Œ0.7ï¼Œ0.8ï¼Œ0.9)è¿›è¡Œå®éªŒã€‚æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ä¸Šç½®ä¿¡æŒ‡æ•°æ˜¯å¦‚ä½•å˜åŒ–çš„ï¼Œæ¯ä¸ªå¼ºç›—ç©äº†å¤šå°‘æ¬¡ï¼Œä»¥åŠæˆ‘ä»¬å¯¹æ¯ä¸€æ­¥çš„æ ·æœ¬å‡å€¼çš„ä¼°è®¡ã€‚ä½œè€…çš„åŠ¨ç”»ã€‚

å®ƒæ˜¯ä½¿ç”¨ [**åˆ‡å°”è¯ºå¤«-èµ«å¤«ä¸ç•Œé™**](https://people.eecs.berkeley.edu/~sinclair/cs271/n13.pdf) æ¨å¯¼å‡ºæ¥çš„ï¼Œå³

```
P( Sample Mean + t â‰¤ True Mean) â‰¤ exp(-2ntÂ²) for t > 0
```

å…¶ä¸­`n`æ˜¯æˆ‘ä»¬è¿„ä»Šä¸ºæ­¢ä»ç‰¹å®šçš„åœŸåŒªé‚£é‡Œæ”¶é›†çš„æ•°æ®ç‚¹çš„æ•°é‡ã€‚è¿™æ„å‘³ç€ï¼Œæ ·æœ¬å‡å€¼çš„è¯¯å·®å¤§äº`t`çš„æ¦‚ç‡éšç€`n`çš„å¢åŠ è€Œé™ä½ã€‚

ä¸ºäº†æ¨å¯¼ä¸Šé™ï¼Œæˆ‘ä»¬é¦–å…ˆè®¾ç½®å³ä¾§ç­‰äº`b`ï¼Œå¹¶æ±‚è§£`t`ã€‚

```
Let 
 b = exp(-2ntÂ²).Then
 logb = -2ntÂ² <=> t = âˆš(-logb / (2n))Choose(this is a heuristic)b = (1 / Nâ´)Then
 t = âˆš(2logN / n)
```

åœ¨ä¸Šé¢ï¼Œ`n`æ˜¯æˆ‘ä»¬ç©ç‰¹å®šå¼ºç›—çš„æ¬¡æ•°ï¼Œ`N`æ˜¯æˆ‘ä»¬ç©çš„æ€»æ¬¡æ•°(å›åˆæ•°æˆ–ç”µå½±å¤œæ•°ï¼Œå¦‚æœä½ å–œæ¬¢çš„è¯)ã€‚ä½ å¯ä»¥æŠŠ`b`çœ‹æˆä¸€ä¸ªè¶…å‚æ•°ã€‚

```
We know 
 P( Sample Mean + t â‰¤ True Mean) â‰¤ exp(-2ntÂ²) for t > 0Substitute for t
 P( Sample Mean + âˆš(2logN / n) â‰¤ True Mean) â‰¤ 1 / Nâ´**Define**
  **Upper Confidence Index = Sample Mean + âˆš(2logN / n)****Hence 
  C = âˆš(2logN / n)**
```

ä»ä¸Šé¢æˆ‘ä»¬å¯ä»¥æ¨æ–­å‡ºï¼ŒçœŸå®å¹³å‡å€¼å¤§äºç½®ä¿¡æŒ‡æ•°ä¸Šé™çš„æ¦‚ç‡å°äº 1/nâ´â€”â€”éšç€å›åˆæ•°`N`çš„å¢åŠ ï¼Œè¿™æ”¶æ•›åˆ° 0ã€‚å› æ­¤ï¼Œä½¿ç”¨å®ƒä½œä¸ºä¸Šé™æ˜¯æœ‰æ„ä¹‰çš„ã€‚

æœ‰ä¸€ç‚¹éœ€è¦æ³¨æ„çš„æ˜¯æ¯”ä¾‹`logN / n`ã€‚éšç€å›åˆæ•°`N`å’Œæˆ‘ä»¬ç©å¼ºç›—çš„æ¬¡æ•°`n`ä¹‹é—´çš„å·®è·å¢åŠ ï¼Œç•Œé™ä¹Ÿå¢åŠ ã€‚

![](img/f9b433dfb7c2381cac72f99009c4109f.png)

ä¿æŒ N ä¸å˜ï¼Œå¢åŠ  Nï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œå½“æˆ‘ä»¬ç©ä¸æ¶‰åŠå¼ºç›—çš„å›åˆæ—¶ï¼ŒC å¢åŠ ã€‚å›¾ç‰‡ç”±ä½œè€…æä¾›ã€‚

![](img/dfc637b068f1cc29e3dd8f030bf464d8.png)

ä¿æŒ N / n æ¯”ä¸å˜ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œéšç€ N å’Œ N éƒ½å¢åŠ ï¼Œç•Œé™å‡å°ã€‚è¿™æ˜¯å› ä¸ºä½œè€…åœ¨ N. Image ä¸Šçš„å¯¹æ•°ã€‚

## UC B1-è°ƒè°

å¯¹äº UCB1-Tunedï¼Œæˆ‘ä»¬ç”¨ä»¥ä¸‹å†…å®¹æ›¿æ¢`C`

```
C = âˆš( (logN / n) x min(1/4, V(n)) )where V(n) is an upper confidence bound on the variance of the bandit, i.e.V(n) = Î£(x_iÂ² / n) - (Î£ x_i / n)Â² + âˆš(2log(N) / n) and x_i are the rewards we got from the bandit so far.
```

åœ¨[åŸå§‹è®ºæ–‡](https://link.springer.com/article/10.1023/A:1013689704352)(è§ç¬¬ 4 èŠ‚)ä¸­ï¼ŒUCB1 è°ƒä¼˜çš„åœ¨æ¯ä¸ªå®éªŒä¸­éƒ½ä¼˜äº UCB1ã€‚

![](img/8b41fd09c8272d049c932dd7e771862c.png)

å®éªŒä½¿ç”¨ UCB1-Tuned ä¸å››ä¸ªåœŸåŒªçš„æ¦‚ç‡(0.6ï¼Œ0.7ï¼Œ0.8ï¼Œ0.9)è·å¾—å¥–åŠ±ç­‰äº 1ã€‚æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ä¸Šç½®ä¿¡æŒ‡æ•°æ˜¯å¦‚ä½•å˜åŒ–çš„ï¼Œæ¯ä¸ªå¼ºç›—ç©äº†å¤šå°‘æ¬¡ï¼Œä»¥åŠæˆ‘ä»¬å¯¹æ¯ä¸€æ­¥çš„æ ·æœ¬å‡å€¼çš„ä¼°è®¡ã€‚ä½œè€…çš„åŠ¨ç”»ã€‚

## UC B1-æ­£å¸¸

å¯¹äº UCB1-Normalï¼Œ`C`åŸºäº[æ ·æœ¬æ–¹å·®](https://www.saddleback.edu/faculty/pquigley/math10/shortcut.pdf)ï¼Œä¸å‰ä¸¤ç§ç®—æ³•ä¸åŒï¼Œå®ƒæ˜¯ä¸ºæ­£æ€åˆ†å¸ƒçš„å¥–åŠ±è€Œè®¾è®¡çš„ã€‚

```
C = âˆš( 16 SV(n) log(N - 1) / n )where the sample variance is
  SV(n) = ( Î£ x_iÂ² - n (Î£ x_i / n)Â² ) / (n - 1)

x_i are the rewards we got from the bandit so far
```

UCB1-Normal é€šå¸¸æ²¡æœ‰åˆå§‹åŒ–æ­¥éª¤ã€‚ç›¸åï¼Œåœ¨æ¯ä¸€è½®`N`ä¸­ï¼Œå®ƒä¼šæ£€æŸ¥æ˜¯å¦æœ‰ä¸€ä¸ªåœŸåŒªçš„å‡ºç‰Œæ¬¡æ•°å°‘äº`8logN`çš„ä¸Šé™ï¼Œç„¶åå°±æ‰“é‚£ä¸ªåœŸåŒªï¼Œè€Œä¸æ˜¯çœ‹ä¸Šé¢çš„ä¿¡å¿ƒæŒ‡æ•°ã€‚ç„¶è€Œï¼Œä¸ºäº†è®¡ç®—`C`ï¼Œæˆ‘ä»¬éœ€è¦`n`è‡³å°‘ä¸º 2ï¼Œè¿™æ ·æˆ‘ä»¬å°±ä¸ä¼šè¢«é›¶é™¤ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å°†æ¯ä¸ªå¼ºç›—ç©ä¸¤æ¬¡ä½œä¸ºåˆå§‹åŒ–æ­¥éª¤ã€‚

![](img/3f45bfded9b9be2abe1bff8730ed0cf1.png)

åœ¨æ¯ä¸€ä¸ªå¼ºç›—è¢«ç©äº†å°†è¿‘ 50 æ¬¡ä¹‹å‰ï¼Œæˆ‘ä»¬ä¸ä¼šåœ¨å¤§å¤šæ•°å›åˆä¸­ä½¿ç”¨è¾ƒé«˜çš„ä¿¡å¿ƒæŒ‡æ•°æ¥é€‰æ‹©å¼ºç›—ã€‚

![](img/6ea8a9a12f8443e52bd1854435e85e9c.png)

ä½¿ç”¨ UCB1-Normal å¯¹äº”ä¸ªåœŸåŒªè¿›è¡Œå®éªŒï¼Œå¥–åŠ±å‡å€¼ä¸º(2ï¼Œ5ï¼Œ7ï¼Œ9ï¼Œ11)ï¼Œæ ‡å‡†å·®ä¸º 2.5ã€‚ä½œè€…çš„åŠ¨ç”»ã€‚

# ç»™æˆ‘çœ‹çœ‹ä»£ç 

å”·â€¦é‚£æ˜¯å¾ˆå¤šç•Œé™ã€‚è¿™äº›ä»£ç ä¸­çš„ä¸€éƒ¨åˆ†å·²ç»åœ¨ä»¥å‰çš„å¸–å­ä¸­å‡ºç°è¿‡ï¼Œä½†æ˜¯ä¸ºäº†å®Œæ•´èµ·è§ï¼Œæˆ‘è¿˜æ˜¯å°†å®ƒåŒ…å«åœ¨è¿™é‡Œã€‚é¦–å…ˆï¼Œæˆ‘ä»¬éœ€è¦ä¸€äº›ç±»æ¥ä»£è¡¨å¼ºç›—ï¼Œä¸€ä¸ªç”¨äºä¼¯åŠªåˆ©å¥–åŠ±ï¼Œä¸€ä¸ªç”¨äºæ™®é€šå¥–åŠ±ã€‚

æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å°†ä¸ºä»£ç†å®šä¹‰ä¸€äº›åŸºç±»(ä»¥åŠä¸€ä¸ªåŠ©æ‰‹ç±»æ¥è·Ÿè¸ªå¥–åŠ±â€”â€”å¦‚æœæ‚¨å·²ç»é˜…è¯»è¿‡ä¸Šä¸€ç¯‡æ–‡ç« çš„è¯,`BanditRewardsLog`å·²ç»è¢«ä¿®æ”¹ä¸ºè·Ÿè¸ªå¥–åŠ±å¹³æ–¹çš„æ€»å’Œ)ã€‚

æˆ‘ä»¬ä»ç„¶éœ€è¦å®šä¹‰Îµè´ªå©ªä»£ç†çš„ç±»ï¼Œå› ä¸ºæˆ‘ä»¬å°†ä½¿ç”¨å®ƒæ¥è¿›è¡Œæ¯”è¾ƒï¼Œä»¥åŠ UCB ä»£ç†ã€‚å®ƒä»¬å°†ä»`Agent`ç±»ç»§æ‰¿ï¼Œå¹¶ä¸”éœ€è¦å®ç°`take_action`ï¼Œå› ä¸ºå®ƒæ˜¯ä¸€ä¸ªæŠ½è±¡æ–¹æ³•ã€‚

æˆ‘ä»¬ç°åœ¨æœ‰äº†å¼ºç›—å’Œç‰¹å·¥çš„æ‰€æœ‰å¿…è¦èŒä¸šã€‚å¨±ä¹æ—¶é—´åˆ°äº†ï¼

# ç®—æ³•æˆ˜åœº

æˆ‘ä»¬å°†ç›´æ¥æŠŠæ‰€æœ‰çš„ UCB ç®—æ³•å’ŒÎµè´ªå©ªç®—æ³•æŠ•å…¥æˆ˜åœºã€‚æˆ‘ä»¬å°†è€ƒè™‘ä¸¤ç§æƒ…å†µï¼Œä¸€ç§å¥–åŠ±æ¥è‡ªä¼¯åŠªåˆ©åˆ†å¸ƒï¼Œå¦ä¸€ç§å¥–åŠ±æ¥è‡ªæ­£æ€åˆ†å¸ƒã€‚

æˆ‘ä»¬å°†åˆ©ç”¨ä»¥ä¸‹å®ç”¨å‡½æ•°ã€‚

```
*def* compare_agents(
    *agents*: List[Agent], 
    *bandits*: List[Bandit], 
    *iterations*: int,
    *show_plot*=*True*,
):
    *for* agent *in agents*:
        logger.info("Running for agent = %s", agent)
        agent.bandits = *bandits
        if* isinstance(agent, ucb.UCBAgent):
            agent.initialise()

        N = *iterations* - agent.rewards_log.total_actions
        agent.take_actions(N)
        *if show_plot*:
            cumulative_rewards = np.cumsum(
                agent.rewards_log.all_rewards,
            )
            plt.plot(cumulative_rewards, label=str(agent))

    *if show_plot*:
        plt.xlabel("iteration")
        plt.ylabel("total rewards")
        plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))
        plt.show() def get_agents():
    agents = [
        EpsilonGreedyAgent(),
        ucb.UCB1Agent(),
        ucb.UCB1TunedAgent(),
        ucb.UCB1NormalAgent(),
    ]
    return agents def run_comparison_ucb(bandits):
    win_count = [0, 0, 0, 0]

    for _ in range(1000):
        agents = get_agents()
        iterations = 1000
        compare_agents(agents, bandits, iterations, show_plot=False)

        rewards = [
            agent.rewards_log.total_rewards 
            for agent in agents
        ]
        win_count[np.argmax(rewards)] += 1

    return win_count
```

æ„¿æœ€å¥½çš„ç®—æ³•èƒœå‡ºï¼

## ä¼¯åŠªåˆ©å¥–åŠ±

æˆ‘ä»¬æ¥å®šä¹‰ä¸€ä¸‹æˆ‘ä»¬çš„åœŸåŒªå’Œç‰¹å·¥ã€‚

```
probs = [0.6, 0.7, 0.8, 0.9]
bernoulli_bandits = [BernoulliBandit(p) for p in probs]
agents = get_agents()
```

é‚£ä¹ˆæˆ‘ä»¬å¯ä»¥ç®€å•åœ°åš

```
compare_agents(agents, bernoulli_bandits, 1000, show_plot=True)
```

![](img/0f5d59044033bc9b85287d9f670ea3d3.png)

æ¯ä¸ªä»£ç†çš„æ€»ç´¯ç§¯å¥–åŠ±ã€‚å›¾ç‰‡ç”±ä½œè€…æä¾›ã€‚

çœ‹èµ·æ¥ Epsilon-Greedy å’Œ UCB1-Tuned æ¯”å…¶ä»–ä¸¤ä¸ªåšå¾—æ›´å¥½ï¼Œä½†å®ƒå¾ˆæ¥è¿‘ã€‚æˆ‘ä»¬å°†é‡å¤è¿™ä¸ªå®éªŒ 1000 æ¬¡ï¼Œå¹¶è®°å½•ä¸‹å“ªä¸ªä»£ç†å¾—åˆ°äº†æœ€å¤šçš„å¥–åŠ±ã€‚

```
run_comparison_ucb(bernoulli_bandits)
```

æˆ‘çš„ç»“æœæ˜¯ Epsilon-Greedy 497 èƒœï¼ŒUC B1 3 èƒœï¼ŒUC B1-Tuned 500 èƒœï¼ŒUC B1-Normal 0 èƒœã€‚çœ‹èµ·æ¥é€‰æ‹© Epsilon-Greedy æˆ– UCB1-Tuned æ˜¯æ­£ç¡®çš„é€‰æ‹©ã€‚

## æ­£å¸¸å¥–åŠ±

æˆ‘ä»¬é‡å¤äº†åŒæ ·çš„å®éªŒï¼Œä½†è¿™æ¬¡æ˜¯å¯¹æŒ‰ç…§æ­£æ€åˆ†å¸ƒç»™äºˆå¥–åŠ±çš„å¼ºç›—ã€‚

```
means = [3, 5, 7, 9]
normal_bandits = [Bandit(m=m, sigma=1) for m in means]
```

![](img/82e79aa03cabaf814704d6b0ba989491.png)

æ¯ä¸ªä»£ç†çš„æ€»ç´¯ç§¯å¥–åŠ±ã€‚å›¾ç‰‡ç”±ä½œè€…æä¾›ã€‚

ä»¤äººæƒŠè®¶çš„æ˜¯ï¼Œæœ€å·®çš„ä¸€ä¸ªå¥½åƒæ˜¯ UC B1â€”â€”æ­£å¸¸â€¦â€¦ä¸¢äººã€‚ä¸æ¸…æ¥šå“ªä¸€ä¸ªæ˜¯å…¶ä»–ä¸‰ä¸ªä¸­æœ€å¥½çš„ï¼Œæ‰€ä»¥æˆ‘ä»¬å°†é‡å¤å®éªŒå¾ˆå¤šæ¬¡ï¼Œå¹¶è®°å½•å“ªä¸ªä»£ç†è·å¾—äº†æœ€å¤šçš„å¥–åŠ±ã€‚

æˆ‘ä¸º Epsilon-Greedy èµ¢å¾—äº† 156 åœºèƒœåˆ©ï¼Œä¸º UCB1 èµ¢å¾—äº† 364 åœºèƒœåˆ©ï¼Œä¸º UCB1-Tuned èµ¢å¾—äº† 480 åœºèƒœåˆ©ï¼Œä¸º UCB1-Normal èµ¢å¾—äº† 0 åœºèƒœåˆ©ã€‚

# ç»“è®º

åœ¨è¿™ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘ä»¬ç ”ç©¶äº†ç½®ä¿¡ä¸Šé™ bandit ç®—æ³•æ˜¯å¦‚ä½•å·¥ä½œçš„ï¼Œç”¨ Python å¯¹å®ƒä»¬è¿›è¡Œäº†ç¼–ç ï¼Œå¹¶å°†å®ƒä»¬ä¸å…¶ä»–ç®—æ³•ä»¥åŠ Epsilon-Greedy(ä¸ğ›†=1/#actions).)ç®—æ³•è¿›è¡Œäº†æ¯”è¾ƒ

æˆ‘ä»¬å‘ç° UCB1-Tuned åœ¨ä¼¯åŠªåˆ©å’Œæ™®é€šå¥–åŠ±æ–¹é¢è¡¨ç°æœ€ä½³ï¼Œå°½ç®¡å®ƒä¸æ˜¯ä¸ºæ™®é€šå¥–åŠ±è®¾è®¡çš„ã€‚å…¶ä¸­æœ€å·®çš„æ˜¯ UC B1â€”â€”åˆ°ç›®å‰ä¸ºæ­¢è¿˜ç®—æ­£å¸¸â€”â€”è¿™å¯èƒ½æ˜¯å®ƒä¸å¦‚å…¶ä»–äº§å“å—æ¬¢è¿çš„åŸå› ã€‚

# å‚è€ƒ

[1] Auerï¼Œp .ï¼ŒCesa-Bianchiï¼Œn .å’Œ Fischerï¼Œp .ï¼Œå¤šè‡‚ Bandit é—®é¢˜çš„æœ‰é™æ—¶é—´åˆ†æã€‚*æœºå™¨å­¦ä¹ * **47ã€**235â€“256(2002)ã€‚[https://doi.org/10.1023/A:1013689704352](https://doi.org/10.1023/A:1013689704352)

<https://eminik355.medium.com/subscribe>  

[**æˆä¸º Medium**](https://eminik355.medium.com/membership)**ä¼šå‘˜ï¼Œè·å¾—å…¨éƒ¨æ•…äº‹æƒé™ã€‚ä½ çš„ä¼šå‘˜è´¹ç›´æ¥æ”¯æŒä½ è¯»çš„ä½œå®¶ã€‚**

**æ›´å¤šæ¥è‡ªåŒä¸€ä½œè€…ã€‚**

</multi-armed-bandits-thompson-sampling-algorithm-fea205cf31df>  <https://medium.com/analytics-vidhya/calculating-using-monte-carlo-simulations-337cff638ac5>  <https://medium.com/analytics-vidhya/multi-armed-bandits-part-1-epsilon-greedy-algorithm-with-python-code-534b9e2abc9>  <https://medium.com/swlh/multi-armed-bandits-optimistic-initial-values-algorithm-with-python-code-3970e611b5ab> 