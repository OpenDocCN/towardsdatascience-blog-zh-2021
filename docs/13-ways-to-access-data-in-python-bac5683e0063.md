# ç”¨ Python è®¿é—®æ•°æ®çš„ 13 ç§æ–¹æ³•

> åŸæ–‡ï¼š<https://towardsdatascience.com/13-ways-to-access-data-in-python-bac5683e0063?source=collection_archive---------0----------------------->

## å¦‚ä½•ä»æœ¬åœ°æ–‡ä»¶ã€æ•°æ®åº“ã€API å’Œæ•°æ®è®¿é—®åº“ä¸­è·å–æ•°æ®åˆ°æ‚¨çš„ Python ç¯å¢ƒä¸­

![](img/030484439198b4889d599d0f8e41a26e.png)

å…°æ–¯Â·æ ¼å…°è¾¾å°”åœ¨ [Unsplash](https://unsplash.com/photos/nShLC-WruxQ) æ‹æ‘„çš„å›¾ç‰‡

# ä»‹ç»

å¤§å¤šæ•° Python åˆ†æéƒ½æ˜¯ä»å°†æ•°æ®å¯¼å…¥ç¯å¢ƒå¼€å§‹çš„ã€‚ä½†æ˜¯ï¼Œå¦‚æœè¿™äº›æ•°æ®æ»ç•™åœ¨æ•°æ®åº“ä¸­ï¼Œè¯¥æ€ä¹ˆåŠå‘¢ï¼Ÿè¿˜æ˜¯èƒŒåä¸€ä¸ª APIï¼Ÿè¿˜æ˜¯åœ¨ä¸€å †å°æ–‡ä»¶é‡Œï¼Ÿ

å¹¸è¿çš„æ˜¯ï¼ŒPython éå¸¸çµæ´»ï¼Œæœ‰å¾ˆå¤šç”¨äºè®¿é—®å’Œå¤„ç†æ•°æ®çš„å¼€æºåº“ã€‚åœ¨æœ¬æ•™ç¨‹ä¸­ï¼Œæˆ‘ä»¬å°†å­¦ä¹  13 ç§å°†æ•°æ®æ”¾å…¥ç†ŠçŒ«æ•°æ®æ¡†çš„æ–¹æ³•ï¼Œä¹‹åå¯ä»¥å¯¹æ•°æ®è¿›è¡Œæ¸…ç†ã€åˆ†æå’Œå¯è§†åŒ–ã€‚æˆ‘ä»¬å¯ä»¥å°†è¿™äº›æ–¹æ³•åˆ†ä¸ºå››å¤§ç±»:

1.  æœ¬åœ°æ–‡ä»¶
2.  æ•°æ®åº“
3.  èœœèœ‚
4.  æ•°æ®é›†è®¿é—®åº“

å”¯ä¸€çš„ä¸»è¦éœ€æ±‚æ˜¯å®‰è£…`pandas`åº“:

```
$ pip install pandas
```

å¥½äº†ï¼Œè®©æˆ‘ä»¬å¼€å§‹å§ï¼

# ğŸ“æœ¬åœ°æ–‡ä»¶

é€šå¸¸ï¼Œæ‚¨éœ€è¦çš„æ•°æ®å­˜å‚¨åœ¨è®¡ç®—æœºçš„æœ¬åœ°æ–‡ä»¶ä¸­ã€‚æ ¹æ®æ‚¨è¿è¡Œ Python ç¯å¢ƒçš„ä½ç½®ï¼Œæ‚¨å¯ä»¥å°†æ–‡ä»¶åæŒ‡å®šä¸ºç›¸å¯¹è·¯å¾„[æˆ–ç»å¯¹è·¯å¾„](https://www.earthdatascience.org/courses/intro-to-earth-data-science/python-code-fundamentals/work-with-files-directories-paths-in-python/):

```
# Absolute path
file1 = "~/Users/johnreid/Documents/my_project/data/example.csv"# Relative path, assuming current working directory is my_project
file2 = "./data/example.csv"
```

## 1.CSV æ–‡ä»¶

CSV æ˜¯å­˜å‚¨è¡¨æ ¼æ•°æ®çš„æµè¡Œé€‰æ‹©ï¼Œä¹Ÿæ˜¯æœ€ç®€å•çš„å…¥é—¨æ–¹å¼ã€‚è®©æˆ‘ä»¬å‡è®¾æ‚¨å·²ç»ä»æ•°æ®ä¸­çš„[æˆ‘ä»¬çš„ä¸–ç•Œä¸‹è½½äº†è¿™ä¸ªäººå£æ•°æ®é›†:](https://ourworldindata.org/grapher/population-by-country)

```
import pandas as pdcsv_file = "/Users/johnreid/Downloads/population-by-country.csv"
df_from_csv = pd.read_csv(csv_file)
df_from_csv.info()
```

å¯¼å…¥æ•°æ®åï¼Œè¿è¡Œ`df.info()`æœ‰åŠ©äºäº†è§£æ•°æ®çš„ç»“æ„ï¼Œä¾‹å¦‚æœ‰å¤šå°‘è¡Œã€åˆ—å’Œéç©ºå€¼ã€‚è¿è¡Œè¯¥ä»£ç ä¼šäº§ç”Ÿä»¥ä¸‹è¾“å‡º:

![](img/0e9121386f0580897f8405e1ff6d6ccc.png)

è¿™ç§æ–¹æ³•ä¹Ÿé€‚ç”¨äºé€šè¿‡ URL è®¿é—®çš„æ–‡ä»¶ï¼Œæ¯”å¦‚å…¬å…± Github repo ä¸­çš„å…¬å…± Google Sheet æˆ– CSV æ–‡ä»¶ã€‚å¦å¤–ï¼Œå¦‚æœä½ ä¸€ç›´å¾—åˆ°ä¸€ä¸ª`FileNotFoundError`ï¼Œé‚£ä¹ˆå°è¯•é‡å‘½åä½ çš„æ–‡ä»¶åï¼Œç”¨ä¸‹åˆ’çº¿ä»£æ›¿ç©ºæ ¼ï¼Œä¾‹å¦‚â€œFinancial Sample.xlsxâ€å˜æˆâ€œFinancial_Sample.xlsxâ€ã€‚

## 2.Excel æ–‡ä»¶

æ‚¨éœ€è¦å¯¹ Excel æ–‡ä»¶æ›´åŠ å°å¿ƒï¼Œå› ä¸ºå®ƒä»¬å¯èƒ½åŒ…å«ä¸æ­¢ä¸€å¼ æ•°æ®å’Œå¤æ‚çš„è§†è§‰æ ¼å¼ï¼Œä¾‹å¦‚é¢å¤–çš„æ ‡é¢˜è¡Œã€‚é™¤æ­¤ä¹‹å¤–ï¼Œè¯­æ³•éå¸¸ç›¸ä¼¼â€”â€”ä¸‹é¢æ˜¯ä¸€ä¸ª[è´¢åŠ¡æ•°æ®](https://go.microsoft.com/fwlink/?LinkID=521962)çš„ä¾‹å­:

```
import pandas as pdexcel_file = "/Users/johnreid/Downloads/Financial_Sample.xlsx"
df_from_excel = pd.read_excel(excel_file, sheet_name = "Sheet1")
df_from_excel.info()
```

## 3.æ–‡æœ¬æ–‡ä»¶

æ–‡æœ¬æ–‡ä»¶é€šå¸¸éœ€è¦æ›´å¤šçš„æ•°æ®å¤„ç†â€”â€”é¦–å…ˆçœ‹çœ‹æ•°æ®æ˜¯å¦‚ä½•å­˜å‚¨çš„ï¼Œä»¥åŠæ‚¨å¸Œæœ›å¦‚ä½•åœ¨ Python ä¸­è¡¨ç¤ºå®ƒã€‚ä»é‚£é‡Œï¼Œæ‚¨å¯ä»¥ç¼–å†™ä»£ç å°†æ–‡æœ¬è¾“å…¥è½¬æ¢æˆæ•°æ®å¸§ã€‚è®©æˆ‘ä»¬ç”¨ä¸€ä¸ªè´­ç‰©æ¸…å•çš„ä¾‹å­ï¼Œæ¯ä¸€è¡ŒåŒ…å«ä¸€ä¸ªé¡¹ç›®å’Œæ•°é‡:

![](img/79f4bc84b9e5a056376ffa45d3e48c3d.png)

è¦å°†å…¶è½¬æ¢ä¸ºæ•°æ®å¸§ï¼Œå¯ä»¥è¿è¡Œä»¥ä¸‹å‘½ä»¤:

```
shopping_list = "/Users/johnreid/Downloads/shopping_list.txt"results = []with open(shopping_list) as f:
    line = f.readline() while line:
        results.append(line.strip().split(" "))
        line = f.readline()f.close()df_from_textfile = pd.DataFrame(results, columns = ["Item", "Quantity"])
```

æˆ‘ä»¬ä¸€è¡Œä¸€è¡Œåœ°è¯»ï¼Œå»æ‰å¤šä½™çš„ç©ºæ ¼ï¼ŒæŠŠè¿™ä¸€è¡Œåˆ†æˆä¸¤éƒ¨åˆ†ã€‚å½“æˆ‘ä»¬åˆ›å»ºä¸€ä¸ªæ•°æ®å¸§æ—¶ï¼Œæˆ‘ä»¬è¿˜éœ€è¦æŒ‡å®šåˆ—åã€‚

## 4.å¤šä¸ªæ–‡ä»¶/æ–‡ä»¶å¤¹

å¦‚æœéœ€è¦ä»å¤šä¸ªå­˜å‚¨çš„æ–‡ä»¶ä¸­æå–æ•°æ®ä¼šæ€ä¹ˆæ ·ï¼Ÿè®©æˆ‘ä»¬ç»“åˆæˆ‘ä»¬å­¦åˆ°çš„ä¸€äº›ä¸œè¥¿ï¼Œä» [BBC ä½“è‚²æ–‡æœ¬æ•°æ®é›†](http://mlg.ucd.ie/datasets/bbc.html)ä¸­æå–æ•°æ®ã€‚

![](img/c5e5f72b357f6d653599d22d00879386.png)

æˆ‘ä»¬æœ‰ 5 ä¸ªå­æ–‡ä»¶å¤¹ï¼Œæ¯ä¸ªå¤§çº¦æœ‰ 100 ä¸ªæ–‡ä»¶ã€‚æ¯ä¸ªæ–‡ä»¶éƒ½ä»¥æ ‡é¢˜å¼€å§‹ï¼Œç„¶åæ˜¯æ–‡ç« çš„æ­£æ–‡ã€‚æˆ‘ä»¬çš„ç›®æ ‡æ˜¯å°†æ‰€æœ‰è¿™äº›æ–‡ä»¶åˆå¹¶æˆä¸€ä¸ªå…·æœ‰â€œæ ‡é¢˜â€ã€â€œå‰¯æ ‡é¢˜â€ã€â€œæ­£æ–‡â€å’Œâ€œæµæ´¾â€åˆ—çš„å•ä¸€æ•°æ®æ¡†æ¶ã€‚è¿™é‡Œçš„`glob`åº“åˆ—å‡ºäº†æ‰€æœ‰å¯èƒ½çš„æ–‡ä»¶åï¼Œéå¸¸æ–¹ä¾¿:

```
import glob
import pandas as pdbase_path = "/Users/johnreid/Downloads/bbcsport/"
genres = ["athletics", "cricket", "football", "rugby", "tennis"]def read_and_split_file(filename):
    with open(filename, 'r', encoding="latin-1") as f:
        lines = f.readlines() # Get lines as a list of strings
        lines = list(map(str.strip, lines)) # Remove /n characters
        lines = list(filter(None, lines)) # Remove empty strings return linesdef get_df_from_genre(path, genre):
    files = glob.glob(path + genre + "/*.txt")
    titles = []
    subtitles = []
    bodies = [] for f in files:
        lines = read_and_split_file(f)
        titles.append(lines[0]) # First line is the title
        subtitles.append(lines[1]) # Second line is the subtitle
        bodies.append(' '.join(lines[2:])) # Combine all the rest return(pd.DataFrame({
        'genre': genre,
        'title': titles,
        'subtitle': subtitles,
        'body': bodies
        })
    )final_df = pd.concat([get_df_from_genre(base_path, g) for g in genres])final_df
```

æˆ‘ä»¬åœ¨ glob ä¸­ä½¿ç”¨`*`æ“ä½œç¬¦æ¥è·å–æ‰€æœ‰å¯èƒ½ä»¥`.txt`ç»“å°¾çš„æ–‡ä»¶åã€‚æ³¨æ„ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨`pd.concat`å°†å¤šä¸ªæ•°æ®å¸§è¿æ¥åœ¨ä¸€èµ·ã€‚è¿è¡Œè¯¥ä»£ç ä¼šäº§ç”Ÿä»¥ä¸‹è¾“å‡º:

![](img/f3e43fafcdbb18ffb0edd1ea6297084d.png)

# ğŸ—„æ•°æ®åº“

å¤§å¤šæ•°ç»„ç»‡å°†ä»–ä»¬çš„ä¸šåŠ¡å…³é”®æ•°æ®å­˜å‚¨åœ¨ç±»ä¼¼ Postgres æˆ– MySQL çš„å…³ç³»æ•°æ®åº“ä¸­ï¼Œä½ éœ€è¦çŸ¥é“ç»“æ„åŒ–è¯­è¨€æ¥è®¿é—®æˆ–æ›´æ–°å­˜å‚¨åœ¨é‚£é‡Œçš„æ•°æ®ã€‚æ•°æ®åº“æœ‰å¾ˆå¤šä¼˜ç‚¹ï¼Œæ¯”å¦‚æ•°æ®æ ‡å‡†åŒ–

## 5.SQLite

SQLite æ˜¯ä¸€ä¸ªå­˜å‚¨ä¸ºå•ä¸ªæ–‡ä»¶çš„åµŒå…¥å¼æ•°æ®åº“ï¼Œæ‰€ä»¥å®ƒæ˜¯å¼€å§‹æµ‹è¯•æŸ¥è¯¢çš„å¥½åœ°æ–¹ã€‚è¿™é‡Œæˆ‘ä»¬å°†å±•ç¤ºä¸€ä¸ªè¿æ¥åˆ° [Chinook](https://github.com/lerocha/chinook-database) æ•°æ®åº“çš„ SQLite æ–‡ä»¶çš„ä¾‹å­:

```
import pandas as pd
import sqlite3 as sqlconn = sql.connect('/Users/johnreid/Downloads/chinook.db')# First pattern - turn query directly into dataframe:
df1 = pd.read_sql_query("SELECT * FROM invoice", conn)# Second pattern - get row-level data, but no column names
cur = conn.cursor()
results = cur.execute("SELECT * FROM invoice LIMIT 5").fetchall()
df2 = pd.DataFrame(results)
```

å¦‚æœä½ æœ‰å…´è¶£ï¼Œå¯ä»¥åœ¨è¿™é‡Œé˜…è¯»æˆ‘å…³äºä½¿ç”¨ SQL æ„å»ºäº¤äº’å¼ä»ªè¡¨æ¿çš„å®Œæ•´æ•™ç¨‹:

[](/building-an-interactive-python-dashboard-using-sql-and-datapane-46bd92294fd3) [## ä½¿ç”¨ SQL å’Œ Datapane æ„å»ºäº¤äº’å¼ Python ä»ªè¡¨æ¿

towardsdatascience.com](/building-an-interactive-python-dashboard-using-sql-and-datapane-46bd92294fd3) 

## 6.è¿œç¨‹æ•°æ®åº“

è¿æ¥åˆ°è¿œç¨‹æ•°æ®åº“(å¦‚ Postgresã€Redshift æˆ– SQLServer)ä½¿ç”¨å‡ ä¹ç›¸åŒçš„è¯­æ³•ï¼Œä½†éœ€è¦è®¿é—®å‡­è¯ã€‚å‡ºäºå®‰å…¨åŸå› ï¼Œæœ€å¥½å°†è¿™äº›å‡­è¯å­˜å‚¨åœ¨ä¸€ä¸ªé…ç½®æ–‡ä»¶ä¸­ï¼Œå¹¶å°†å…¶åŠ è½½åˆ° Python è„šæœ¬ä¸­ã€‚æ‚¨å¯ä»¥ä½¿ç”¨ä»¥ä¸‹ä¿¡æ¯åˆ›å»ºä¸€ä¸ªå•ç‹¬çš„`.py`æ–‡ä»¶:

```
host = "localhost"
database= "suppliers"
user = "postgres"
password = "SecurePas$1"
```

ç„¶åå°†å…¶å¯¼å…¥ Python è„šæœ¬ï¼Œå¦‚ä¸‹æ‰€ç¤º(æ‚¨è¿˜éœ€è¦`psychopg2`åº“):

```
import psycopg2
import configconn = psycopg2.connect(
    host=config.host,
    database=config.database,
    user=config.user,
    password=config.password)df1 = pd.read_sql_query("SELECT * FROM invoice", conn)
```

ç¡®ä¿ä¿å­˜å¥½ä½ çš„`config.py`æ–‡ä»¶ï¼Œä¸è¦æŠŠå®ƒä¸Šä¼ åˆ°å…¶ä»–åœ°æ–¹â€”â€”ä½ å¯ä»¥æŠŠå®ƒæ·»åŠ åˆ°ä½ çš„`.gitignore`ä¸­ï¼Œä»¥ç¡®ä¿å®ƒä¸ä¼šåŒ…å«åœ¨ git æäº¤ä¸­ã€‚

## 7.SQLAlchemy

å¦‚æœæ‚¨æƒ³è¦ä¸€ç§æ›´â€œpythonic åŒ–â€çš„æ–¹å¼æ¥æŸ¥è¯¢æ•°æ®åº“ï¼Œå¯ä»¥å°è¯•ä¸€ä¸‹ [SQLAlchemy](https://www.sqlalchemy.org/) åº“ï¼Œå®ƒæ˜¯ä¸€ä¸ªå¯¹è±¡å…³ç³»æ˜ å°„å™¨ã€‚å®ƒé€šå¸¸ç”¨äºåº”ç”¨ç¨‹åºï¼Œè¿™æ ·å¼€å‘äººå‘˜å°±ä¸å¿…ç¼–å†™çº¯ SQL æ¥æ›´æ–°ä»–ä»¬çš„æ•°æ®åº“ï¼Œä½†æ˜¯æ‚¨ä¹Ÿå¯ä»¥ä½¿ç”¨å®ƒæ¥æŸ¥è¯¢æ•°æ®ï¼

ä¸‹é¢æ˜¯ä¸€ä¸ªä½¿ç”¨ç›¸åŒ Chinook éŸ³ä¹å•†åº—æ•°æ®åº“çš„ç¤ºä¾‹:

```
import sqlalchemy as dbengine = db.create_engine('sqlite:///chinook.db')
connection = engine.connect()
metadata = db.MetaData()invoice = db.Table('invoice', metadata, autoload=True, autoload_with=engine)# Get the first 10 invoices from the USA
query = (db.select([invoice])
    .filter_by(billing_country = 'USA')
    .limit(10)
        )df = pd.read_sql(query, engine)
```

åœ¨è¿™æ®µä»£ç ä¸­ï¼Œæˆ‘ä»¬è¿æ¥åˆ°æ•°æ®åº“ï¼Œç„¶ååœ¨ SQLAlchemy ä¸­è®¾ç½®ä¸€äº›è¡¨å’Œå…ƒæ•°æ®ã€‚ä¸€æ—¦å®šä¹‰å¥½äº†ï¼Œæˆ‘ä»¬å°±å¯ä»¥ç”¨ä¸€ç§æ›´â€œpythonic åŒ–â€çš„æ–¹å¼ç¼–å†™ä¸€ä¸ªæŸ¥è¯¢ï¼Œå¹¶å°†ç»“æœç›´æ¥è¯»å–åˆ° Pandas æ•°æ®å¸§ä¸­ã€‚è¿è¡Œè¯¥ä»£ç ä¼šäº§ç”Ÿä»¥ä¸‹è¾“å‡º:

![](img/e97a5dd60299fad5af00634136d83ce8.png)

# ğŸ“¶èœœèœ‚

æœ‰æ—¶å€™ï¼Œä½ éœ€è¦ä»ä½ å…¬å¸ä½¿ç”¨çš„ç‰¹å®šå¹³å°è·å–æ•°æ®ï¼Œæ¯”å¦‚ Hubspotã€Twitter æˆ– Trelloã€‚è¿™äº›å¹³å°é€šå¸¸æœ‰ä¸€ä¸ªå…¬å…± APIï¼Œæ‚¨å¯ä»¥ç›´æ¥åœ¨ Python ç¯å¢ƒä¸­ä»ä¸­æå–æ•°æ®ã€‚

åŸºæœ¬æ€æƒ³æ˜¯å‘ç«¯ç‚¹å‘é€ä¸€ä¸ªè¯·æ±‚(å¯èƒ½åŒ…æ‹¬æŸ¥è¯¢å‚æ•°å’Œè®¿é—®å‡­è¯)ã€‚è¯¥ç«¯ç‚¹å°†è¿”å›ä¸€ä¸ªå“åº”ä»£ç å’Œæ‚¨è¯·æ±‚çš„æ•°æ®(å¸Œæœ›å¦‚æ­¤)ã€‚æ‚¨éœ€è¦æŸ¥çœ‹ API æ–‡æ¡£æ¥äº†è§£å“ªäº›æ•°æ®å­—æ®µæ˜¯å¯ç”¨çš„ã€‚æ•°æ®é€šå¸¸ä»¥ JSON æ ¼å¼è¿”å›ï¼Œè¿™å…è®¸æ·±åº¦åµŒå¥—çš„æ•°æ®ã€‚

## **8ã€‚æ²¡æœ‰è¯ä»¶**

è®©æˆ‘ä»¬ä½¿ç”¨ [OpenNotify](http://open-notify.org/Open-Notify-API/People-In-Space/) API æ¥åšä¸€ä¸ªæœ€å°çš„ä¾‹å­ï¼Œå®ƒè·Ÿè¸ªå½“å‰åœ¨ç©ºé—´ä¸­çš„æ‰€æœ‰äºº:

```
import requestsresponse = requests.get("http://api.open-notify.org/astros.json")print(response.status_code)
print(response.json())res = pd.DataFrame(response.json()["people"])res.head()
```

è¿è¡Œè¯¥ä»£ç ä¼šäº§ç”Ÿä»¥ä¸‹è¾“å‡º:

![](img/ed79b00054e24aa39d4560a15f7ae77b.png)

å“åº”ä»£ç å‘Šè¯‰æ‚¨ API è°ƒç”¨çš„ç»“æœâ€”â€”æ ¹æ® [Dataquest](https://www.dataquest.io/blog/python-api-tutorial/) æœ€å¸¸è§çš„æ˜¯:

*   ä¸€åˆ‡é¡ºåˆ©ï¼Œç»“æœå·²ç»è¿”å›(å¦‚æœæœ‰çš„è¯)ã€‚
*   `301`:æœåŠ¡å™¨æ­£åœ¨å°†æ‚¨é‡å®šå‘åˆ°ä¸åŒçš„ç«¯ç‚¹ã€‚å½“å…¬å¸åˆ‡æ¢åŸŸåæˆ–ç«¯ç‚¹åç§°æ›´æ”¹æ—¶ï¼Œå¯èƒ½ä¼šå‘ç”Ÿè¿™ç§æƒ…å†µã€‚
*   `400`:æœåŠ¡å™¨è®¤ä¸ºä½ æå‡ºäº†ä¸€ä¸ªé”™è¯¯çš„è¯·æ±‚ã€‚å½“ä½ æ²¡æœ‰å‘é€æ­£ç¡®çš„æ•°æ®æ—¶ï¼Œå°±ä¼šå‘ç”Ÿè¿™ç§æƒ…å†µã€‚
*   `403`:ä½ è¯•å›¾è®¿é—®çš„èµ„æºè¢«ç¦æ­¢:ä½ æ²¡æœ‰æŸ¥çœ‹å®ƒçš„æƒé™ã€‚
*   `404`:åœ¨æœåŠ¡å™¨ä¸Šæ‰¾ä¸åˆ°æ‚¨è¯•å›¾è®¿é—®çš„èµ„æºã€‚
*   `503`:æœåŠ¡å™¨æœªå‡†å¤‡å¥½å¤„ç†è¯·æ±‚ã€‚

## 9.ä½¿ç”¨å‡­æ®å’ŒæŸ¥è¯¢å‚æ•°

æœ‰æ—¶æ‚¨å¯èƒ½éœ€è¦æ¥è‡ª API çš„æ›´å…·ä½“çš„ä¿¡æ¯ï¼Œæˆ–è€…å¿…é¡»è¿›è¡Œèº«ä»½éªŒè¯ã€‚æœ‰å‡ ç§æ–¹æ³•å¯ä»¥åšåˆ°è¿™ä¸€ç‚¹ï¼Œä½†æ˜¯æœ€å¸¸ç”¨çš„ä¸€ç§æ–¹æ³•æ˜¯åœ¨è¯·æ±‚ä¸­æ·»åŠ  URL å‚æ•°ã€‚

å‡è®¾æˆ‘ä»¬æœ‰ä¸€ä¸ªåŒ…å« API å¯†é’¥çš„`config.py`æ–‡ä»¶:

```
personal_api_key = "wouldntyouliketoknow"
```

ç„¶åæˆ‘ä»¬ä¸ºæ‰€æœ‰çš„å‚æ•°åˆ›å»ºä¸€ä¸ªå­—å…¸(è¿™æ˜¯ä¸€ä¸ªè™šæ„çš„ä¾‹å­)å¹¶ä¼ é€’å®ƒ:

```
import config
import pandas as pd
import requestsparameters = {
    "personal_api_key": config.personal_api_key, 
    "date": "2021-09-22"
}response = requests.get(url, params = parameters)

print(response.status_code)
print(response.json())

res = pd.DataFrame(response.json()["people"])
res.head()
```

å¦‚æœä½ ä¸æƒ³å¤„ç† JSONï¼Œä½ å¯ä»¥å°è¯•æœç´¢è¯¥ API çš„ Python åº“â€”â€”è¿™äº›åº“é€šå¸¸æ˜¯å¼€æºçš„ï¼Œç”±å…¬å¸æˆ–ç¬¬ä¸‰æ–¹ç»´æŠ¤ã€‚

# ğŸ“šæ•°æ®é›†è®¿é—®åº“

å¦‚æœæ‚¨éœ€è¦ä¸€äº›å‚è€ƒæ•°æ®æ¥è¿›è¡Œæ¯”è¾ƒæˆ–æ·»åŠ ä¸Šä¸‹æ–‡ï¼Œè¯¥æ€ä¹ˆåŠï¼Ÿæœ‰è®¸å¤šåº“å¯ä»¥å°†å…¬å…±æ•°æ®é›†ç›´æ¥ä¸‹è½½åˆ°æ‚¨çš„ç¯å¢ƒä¸­â€”â€”å¯ä»¥æŠŠå®ƒæƒ³è±¡æˆä» API ä¸­æå–ï¼Œè€Œä¸å¿…ç®¡ç†æ‰€æœ‰é¢å¤–çš„å¤æ‚æ€§ã€‚

## 10.ç†ŠçŒ« _datareader

[Pandas_datareader](https://pandas-datareader.readthedocs.io/en/latest/index.html) æ˜¯ä¸€ç§å°†æ•°æ®ä»äº’è”ç½‘æå–åˆ° Python ç¯å¢ƒä¸­çš„å¥½æ–¹æ³•ã€‚å®ƒç‰¹åˆ«é€‚åˆé‡‘èæ•°æ®ï¼Œä½†ä¹Ÿæœ‰ä¸€äº›ä¸–ç•Œé“¶è¡Œçš„æ•°æ®æºã€‚è¦äº†è§£ Zoom è¿‡å»å‡ å¹´çš„æ¯æ—¥è‚¡ä»·ï¼Œè¯·å°è¯•ä»¥ä¸‹æ–¹æ³•:

```
from pandas_datareader import data
import datetime as dtzm = data.DataReader(
    "ZM",
    start='2019-1-1',
    end=dt.datetime.today(),
    data_source='yahoo'
).reset_index()zm.head()
```

è¿è¡Œè¯¥ä»£ç ä¼šäº§ç”Ÿä»¥ä¸‹è¾“å‡º:

![](img/c21a79123bd06588ce232883aaa09a9b.png)

## 11.æ•°æ®å…±äº«

[Datacommons](https://datacommons.org/) æ˜¯è°·æ­Œçš„ä¸€ä¸ªé¡¹ç›®ï¼Œæä¾›å¯¹æ ‡å‡†åŒ–å’Œæ¸…æ´çš„å…¬å…±æ•°æ®é›†çš„è®¿é—®ã€‚åº•å±‚æ•°æ®ä»¥å›¾è¡¨çš„å½¢å¼å‘ˆç°ï¼Œè¿™ä½¿å¾—æŸ¥è¯¢å’Œè¿æ¥æ¥è‡ªè®¸å¤šä¸åŒæ•°æ®æºçš„æ•°æ®å˜å¾—éå¸¸å®¹æ˜“ï¼Œä¾‹å¦‚ç¾å›½äººå£æ™®æŸ¥å±€ã€ä¸–ç•Œé“¶è¡Œã€ç»´åŸºç™¾ç§‘ã€ç–¾ç—…æ§åˆ¶ä¸­å¿ƒç­‰ç­‰ã€‚è¿™é‡Œæœ‰ä¸€ä¸ªåŸºæœ¬çš„ä¾‹å­:

```
!pip install datacommons datacommons_pandas --upgrade --quietimport datacommons_pandas as dc
import pandas as pdcity_dcids = dc.get_property_values(["CDC500_City"], "member", limit=500)["CDC500_City"]cdc500_df = dc.build_multivariate_dataframe(
    city_dcids,
    ["Percent_Person_Obesity",  # Prevalence of obesity from CDC
    "Median_Income_Person",
    "Median_Age_Person",
    "UnemploymentRate_Person",  # Unemployment rate from BLS
    "Count_Person_BelowPovertyLevelInThePast12Months",  # Persons living below the poverty line from Census
    "Count_Person",  # Total population from Census
    ],
)cdc500_df.info()
```

è¿è¡Œè¯¥ä»£ç ä¼šå¾—åˆ°ä»¥ä¸‹ç»“æœ:

![](img/45fb498b570fa11c1240e11c0fa240c7.png)

å¦‚æœä½ æƒ³å­¦ä¹ å¦‚ä½•ä½¿ç”¨ DataCommonsï¼Œè¯·åœ¨è¿™é‡Œé˜…è¯»æˆ‘çš„å®Œæ•´æ•™ç¨‹:

[](/exploring-datacommons-the-api-powering-google-search-afc366ec242b) [## æ¢ç´¢æ•°æ®å…±äº«â€”â€”æ”¯æŒè°·æ­Œæœç´¢çš„ API

### è®¿é—®å…¬å¼€å¯ç”¨æ•°æ®é›†çš„æ–°èŒƒå¼

towardsdatascience.com](/exploring-datacommons-the-api-powering-google-search-afc366ec242b) 

## 12.PyTrends(è°·æ­Œè¶‹åŠ¿)

PyTrends æ˜¯ä¸€ä¸ªéå®˜æ–¹ä½†å¾ˆæœ‰ç”¨çš„åº“ï¼Œç”¨äºæŸ¥è¯¢ [Google Trends](https://trends.google.com/trends/explore?date=today%205-y&q=oat%20milk,soy%20milk,almond%20milk) æ•°æ®â€”â€”è¿™é‡Œæœ‰ä¸€ä¸ªç®€å•çš„ä¾‹å­:

```
import pandas as pdfrom pytrends.request import TrendReqpytrends = TrendReq()
keywords = ["oat milk", "soy milk", "almond milk"]pytrends.build_payload(keywords, cat=0, geo='', gprop='') # Get data from the last 5 yearstop_queries = pytrends.interest_over_time()[keywords]top_queries.head()
```

è¿è¡Œè¯¥ä»£ç ä¼šäº§ç”Ÿä»¥ä¸‹è¾“å‡º:

![](img/42dff0556a84ef75ffe175cc572be6af.png)

## 13.å¡æ ¼å°”

Kaggle æ˜¯ä¸€ä¸ªæ•°æ®ç§‘å­¦ç¤¾åŒºï¼Œå®ƒæ‹¥æœ‰å¤§é‡å­¦ä¹  Python çš„æ•°æ®é›†å’Œç«èµ›ã€‚æ‚¨å¯ä»¥é€šè¿‡å‘½ä»¤è¡Œç•Œé¢ä¸‹è½½å…¶ä¸­çš„ä¸€äº›æ•°æ®é›†è¿›è¡Œå®éªŒ(æ³¨æ„:æ‚¨éœ€è¦æ³¨å†Œä¸€ä¸ª Kaggle å¸æˆ·)ã€‚ä¾‹å¦‚ï¼Œå‡è®¾æˆ‘ä»¬æƒ³è¦ä¸‹è½½ä¸€äº› [Zillow economics æ•°æ®](https://www.kaggle.com/zillow/zecon)ï¼Œæˆ‘ä»¬å¯ä»¥åœ¨æˆ‘ä»¬çš„ç»ˆç«¯ä¸­è¿è¡Œä»¥ä¸‹å‘½ä»¤(Jupyter ç”¨æˆ·:ç”¨ Python ä»£ç ä¸­çš„`!`æ›¿æ¢`$`:

```
$ pip install kaggle
$ export KAGGLE_USERNAME=datadinosaur
$ export KAGGLE_KEY=xxxxxxxxxxxxxx
$ kaggle datasets download zillow/zecon
$ unzip zecon.zip
```

è¿™å°†ä¸‹è½½æ•°æ®é›†çš„å‹ç¼©æ–‡ä»¶ï¼Œç„¶åè§£å‹ç¼©å®ƒä»¬ã€‚ä»é‚£é‡Œï¼Œæ‚¨å¯ä»¥å°†å®ƒä»¬ä½œä¸ºç†ŠçŒ«çš„æœ¬åœ°æ–‡ä»¶æ‰“å¼€:

```
import pandas as pdcsv_file = "/Users/johnreid/Downloads/Zip_time_series.csv"
df_from_csv = pd.read_csv(csv_file)
df_from_csv.info()
```

è¦äº†è§£æ›´å¤šä¿¡æ¯ï¼Œè¯·æŸ¥çœ‹ [Kaggle API æ–‡æ¡£](https://github.com/Kaggle/kaggle-api)ã€‚

# **ç»“è®º**

å›¾ç‰‡æ¥è‡ª [Giphy](https://giphy.com/gifs/angry-spongebob-squarepants-strong-D7z8JfNANqahW)

ä½ æˆåŠŸäº†ï¼ç°åœ¨ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨æ–°è·å¾—çš„èƒ½åŠ›æ¥è®¿é—®å¤šä¸ªæ•°æ®æºï¼Œå¹¶ä½¿ç”¨`pd.merge`æˆ–`pd.concat`å°†å®ƒä»¬è¿æ¥åœ¨ä¸€èµ·ï¼Œç„¶åä½¿ç”¨ Altairã€Pandas æˆ– leav ç­‰äº¤äº’å¼åº“æ¥å¯è§†åŒ–å®ƒä»¬ã€‚

æœ‰æ²¡æœ‰æˆ‘æ¼æ‰çš„æ–¹æ³•ï¼Ÿè¯·åœ¨è¯„è®ºä¸­å‘Šè¯‰æˆ‘ã€‚