# 所以，你想建立一个强化学习库

> 原文：<https://towardsdatascience.com/so-you-want-to-build-a-reinforcement-learning-library-658d51ddbcd?source=collection_archive---------35----------------------->

## RL 算法编码指南

![](img/c513412aded2e0da0bdbeb5a1668200a.png)

Avel Chuklanov 在 [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral) 上拍摄的照片

我最近决定建立自己的强化学习库，目标是为我的博士学位建立快速创意原型。如果你很好奇， [**这里有一个链接**](https://github.com/LondonNode/Pearl) ，虽然它肯定还没有完全完成😅

现在我自己已经经历了这种折磨，我感到受到了启发，要对我所犯的一些错误提出警告，并希望有助于防止一些深夜调试复杂的 PyTorch 代码！

# 1.从简单开始！

这是迄今为止我能给出的最重要的建议。当我第一次了解强化学习时，我陷入了寻找最新、最先进的算法的探索中，我可以用它来最终掌握每一部视频。这让我了解了 [**【阶段性政策梯度】(PPG)**](/phasic-policy-gradient-ppg-part-1-98e01e0f1ffe) ，我决心从这里开始。自然地，当它不起作用时，我意识到算法有这么多不同的活动部分，几乎不可能缩小到底是哪个部分出了问题。

在 RL 世界中，许多算法都是建立在现有工作的基础上的(例如，A2C → PPO → PPG)，同样的，你应该首先实现最简单的算法，然后构建更复杂的算法，这样每个阶段的调试就更少了。例如，最后，我决定放弃 PPG，只采用简单的深度 Q 网络(DQN)算法。这是更容易测试，只有真正担心的评论家网络，现在我可以更有信心，组成 DQN 算法的组件按预期工作。现在，当我用 DQN 构建一个有重叠组件的更复杂的算法时，我知道如果有什么东西不工作，我就不需要重新访问它们了。

# 2.先学习理论，再尝试实现

一般来说，RL 算法可能比标准的监督或非监督方法更复杂，具有复杂的损失函数和大量的超参数调整。这意味着它们往往不能在每种环境下都准确地开箱即用，如果没有算法如何工作的理论知识，您可能会认为您一定只是编写了错误的代码，而实际上问题可能是超参数的计算错误，或者只是在特定环境下使用了错误的算法。对理论的良好理解有助于更快地进行诊断，并凭直觉找到错误所在。

# 3.不要害怕一开始就使用快速脚本

编写好的软件有许多原则和指南。不幸的是，它们通常需要大量的思考和时间来正确地实现。当涉及到编码 RL 算法时，最重要的事情是它按照预期工作，如果它不工作，那么不管代码有多好，它都是不可用的！一个好的策略是首先确保底层功能是健全的，然后再考虑如何恰当地设计它。Jupyter 笔记本是这种用例的一个很好的工具，在获取代码并在 Python 模块中形式化之前，可以在笔记本上快速设计和测试算法。总的来说，这种方法可以节省您的时间，并增强您对代码库的信心。

现在你知道了，在实现你自己的 RL 算法时，三个技巧可以节省你的时间和压力。

如果您觉得这篇文章有用，请考虑:

*   跟踪我🙌
*   [**订阅我的邮件通知**](https://medium.com/subscribe/@rohan.tangri) 永不错过上传📧
*   使用我的媒介 [**推荐链接**](https://medium.com/@rohan.tangri/membership) 直接支持我并获得无限量的优质文章🤗
*   使用我的 [**自由贸易链接**](https://magic.freetrade.io/join/rohan/1095e108) 获得价值 3- 200 英镑的免费份额🤑