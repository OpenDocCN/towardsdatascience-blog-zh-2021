# 你的 AI 想要什么？

> 原文：<https://towardsdatascience.com/what-does-your-ai-want-4dd5a6044412?source=collection_archive---------26----------------------->

## [播客](https://towardsdatascience.com/tagged/tds-podcast)

## 瑞安·凯里对理解人工智能系统动机的探索

[苹果](https://podcasts.apple.com/ca/podcast/towards-data-science/id1470952338?mt=2) | [谷歌](https://www.google.com/podcasts?feed=aHR0cHM6Ly9hbmNob3IuZm0vcy8zNmI0ODQ0L3BvZGNhc3QvcnNz) | [SPOTIFY](https://open.spotify.com/show/63diy2DtpHzQfeNVxAPZgU) | [其他](https://anchor.fm/towardsdatascience)

*编者按:这一集是我们关于数据科学和机器学习新兴问题的播客系列的一部分*，*由 Jeremie Harris 主持。除了主持播客，Jeremie 还帮助运营一家名为*[*sharpes minds*](http://sharpestminds.com)*的数据科学导师初创公司。*

人工智能安全研究人员越来越专注于理解人工智能系统想要什么。这听起来可能是一件奇怪的事情:毕竟，我们不就是通过为人工智能提供一个损失函数或一个要优化的数字来编程人工智能以获得某些东西吗？

嗯，不一定。事实证明，人工智能系统可以有基于其初始编程不一定显而易见的激励。例如，Twitter 运行一个推荐系统，其名义上的工作是找出你最有可能参与的推文。虽然这可能会让你认为它应该为匹配*推文*和*人*进行优化，Twitter 可以实现其目标的另一种方式是通过匹配*人*和*推文*——也就是说，通过将他们推向简单和党派的世界观，使人们更容易预测。一些人认为这是社交媒体对在线政治话语产生如此分裂性影响的一个关键原因。

因此，许多当前人工智能的激励已经在重要和显著的方面偏离了它们的程序员——这些方面实际上正在塑造社会。但它们之所以重要，还有一个更大的原因:随着人工智能系统继续开发更多能力，它们的激励和我们自己的激励之间的不一致将变得越来越重要。这就是为什么这一集的嘉宾瑞安·凯里将他的大部分研究集中在识别和控制人工智能的激励上。Ryan 是一名前医学博士，现在在牛津大学人类未来研究所攻读机器学习博士学位，从事人工智能安全研究。

以下是我在对话中最喜欢的一些观点:

*   大多数从事人工智能安全工作的人都来自有效利他主义(EA)社区。EA 是一个哲学运动，专注于找出一个时间和金钱有限的人如何在世界上做得最好。EA 社区中的许多人都有一个长期的展望，随之而来的是对人类未来的关注，以及人工智能等技术的未来，许多人认为这些技术将决定人类在遥远的未来是繁荣还是失败。
*   Ryan 在人工智能激励方面的工作适用于许多当前系统，对于长期的人工智能校准工作也很重要。出于这个原因，这是一个长期和近期人工智能安全研究人员可以清楚地达成一致的领域。这种事情并不像你想象的那样经常发生，因为在人工智能安全社区内部正在就长期或近期安全问题是否应该优先进行积极的辩论。
*   长期人工智能安全研究人员面临的挑战之一是[抽象泄漏](https://en.wikipedia.org/wiki/Leaky_abstraction)。这就是我们赖以描述世界的基本概念——比如“苹果”、“GPU”和“算法”——实际上是模糊的。例如，一个苹果实际上只是细胞的集合，这些细胞是原子和基本粒子的集合。但为什么这是人工智能安全的一个问题？我们使用抽象是因为我们的感官和计算带宽有限。我们的大脑不认为苹果是细胞的集合，尽管这是一种更准确的感知它们的方式，因为我们没有足够的计算能力和带宽来做到这一点。所以我们使用快捷方式，比如“苹果”这个概念，让这个世界变得足够可压缩，便于理解。但超级智能的人工智能系统不会面临这种限制:原则上，它们可以比我们更详细地看到世界，并可以通过发现我们无法发现的子抽象(如苹果中的细胞)之间的关系来对这些细节采取行动。因此，依赖于某些固定抽象(例如，“不要伤害人类”，或“不要吃那个苹果”)的人工智能安全策略，在面对超级智能系统时，可能远不如我们想象的有效。

你可以[在推特上关注瑞安](https://twitter.com/ryancareyai)，或者[在推特上关注我](https://twitter.com/jeremiecharris)

## 播客中引用的链接:

*   Ryan 一直在研究的因果激励理论网站。
*   这篇文章是关于预测人类和超人的人工智能何时会被开发出来。
*   [本报道](https://www.alignmentforum.org/posts/KrJfoZzpSDpnrv9va/draft-report-on-ai-timelines)关于人工智能时间轴。

![](img/139c13e68a925509fd14bc97c92f56d9.png)

## 章节:

*   0:00 介绍
*   1:20 瑞安的背景
*   5:25 探索替代方案
*   6:45 抵御可扩展的影响
*   11:30 珍惜未来生活
*   13:50 存在风险和 AI
*   23:30 人工智能安全的进展
*   34:30 人工智能战略的组成部分
*   41:30 到达 AGI
*   49:50 五年时间跨度
*   55:10 冒险
*   56:50 总结