# ä½¿ç”¨è½¬æ¢å™¨çš„è¯­ä¹‰ç›¸ä¼¼åº¦

> åŸæ–‡ï¼š<https://towardsdatascience.com/semantic-similarity-using-transformers-8f3cb5bf66d6?source=collection_archive---------4----------------------->

## ä½¿ç”¨ Pytorch å’Œ SentenceTransformers è®¡ç®—ä¸¤ä¸ªæ–‡æœ¬ä¹‹é—´çš„è¯­ä¹‰æ–‡æœ¬ç›¸ä¼¼åº¦

![](img/112c01ef2a9a3cd5d222c16b3cf9268f.png)

ç…§ç‰‡ç”±[ğŸ‡¸ğŸ‡®Â·æ‰¬ç§‘Â·è²åˆ©](https://unsplash.com/@itfeelslikefilm?utm_source=medium&utm_medium=referral)åœ¨ [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral) ä¸Šæ‹æ‘„

# ä»‹ç»

[**è¯­ä¹‰ç›¸ä¼¼åº¦**](https://en.wikipedia.org/wiki/Semantic_similarity) ï¼Œæˆ–è¯­ä¹‰æ–‡æœ¬ç›¸ä¼¼åº¦ï¼Œæ˜¯ [**è‡ªç„¶è¯­è¨€å¤„ç†(NLP)**](https://en.wikipedia.org/wiki/Natural_language_processing) é¢†åŸŸä¸­çš„ä¸€é¡¹ä»»åŠ¡ï¼Œä½¿ç”¨ä¸€ä¸ªå®šä¹‰çš„åº¦é‡å¯¹æ–‡æœ¬æˆ–æ–‡æ¡£ä¹‹é—´çš„å…³ç³»è¿›è¡Œè¯„åˆ†ã€‚è¯­ä¹‰ç›¸ä¼¼åº¦æœ‰ç€å¹¿æ³›çš„åº”ç”¨ï¼Œå¦‚ä¿¡æ¯æ£€ç´¢ã€æ–‡æœ¬æ‘˜è¦ã€æƒ…æ„Ÿåˆ†æç­‰ã€‚

è¯­ä¹‰ç›¸ä¼¼åº¦æœ‰å¾ˆå¤šæ–¹æ³•ã€‚**ç°åœ¨æœ€ç›´æ¥æœ‰æ•ˆçš„æ–¹æ³•æ˜¯ä½¿ç”¨ä¸€ä¸ªå¼ºå¤§çš„æ¨¡å‹(ä¾‹å¦‚ transformer)å¯¹å¥å­è¿›è¡Œç¼–ç ï¼Œä»¥è·å¾—å®ƒä»¬çš„åµŒå…¥ï¼Œç„¶åä½¿ç”¨ä¸€ä¸ªç›¸ä¼¼æ€§åº¦é‡(ä¾‹å¦‚ä½™å¼¦ç›¸ä¼¼æ€§)æ¥è®¡ç®—å®ƒä»¬çš„ç›¸ä¼¼æ€§å¾—åˆ†ã€‚**ç›¸ä¼¼åº¦å¾—åˆ†è¡¨ç¤ºä¸¤ä¸ªæ–‡æœ¬æ˜¯å¦å…·æœ‰ç›¸ä¼¼æˆ–æ›´å¤šä¸åŒçš„å«ä¹‰ã€‚**æœ¬å¸–å°†å±•ç¤ºå¦‚ä½•ä½¿ç”¨**[**Transformers**](https://en.wikipedia.org/wiki/Transformer_(machine_learning_model))å®ç°è¯­ä¹‰ç›¸ä¼¼æ€§ï¼Œè¿™æ˜¯ä¸€ä¸ªå¼ºå¤§çš„ NLP æ¶æ„ï¼Œä¸ºå„ç§ NLP ä»»åŠ¡å¸¦æ¥äº†ä¸€æµçš„æ€§èƒ½ã€‚

æ‰€ä»¥äº‹ä¸å®œè¿Ÿï¼Œè®©æˆ‘ä»¬å¼€å§‹å§ï¼

# æ•™ç¨‹æ¦‚è¿°

1.  å®‰è£…ä¾èµ–é¡¹å’Œåº“
2.  å¯¼å…¥åº“
3.  æ¨¡å‹é€‰æ‹©å’Œåˆå§‹åŒ–
4.  è®¡ç®—ä¸¤ä¸ªå¥å­ä¹‹é—´çš„è¯­ä¹‰ç›¸ä¼¼åº¦
5.  è®¡ç®—ä¸¤ä¸ªå¥å­åˆ—è¡¨ä¹‹é—´çš„è¯­ä¹‰ç›¸ä¼¼åº¦
6.  ä»ç»™å®šå¥å­çš„è¯­æ–™åº“ä¸­æ£€ç´¢å‰ K ä¸ªæœ€ç›¸ä¼¼çš„å¥å­

# å®‰è£…ä¾èµ–é¡¹

æˆ‘ä»¬å°†è¦ç”¨æ¥è®¡ç®—è¯­ä¹‰ç›¸ä¼¼åº¦çš„ä¸»è¦åº“æ˜¯[sentence transformers](https://www.sbert.net/index.html)([Github source link](https://github.com/UKPLab/sentence-transformers))ï¼Œè¿™æ˜¯ä¸€ä¸ªç®€å•çš„åº“ï¼Œæä¾›äº†ä¸€ç§ç®€å•çš„æ–¹æ³•æ¥è®¡ç®—æ–‡æœ¬çš„å¯†é›†å‘é‡è¡¨ç¤º(ä¾‹å¦‚åµŒå…¥)ã€‚å®ƒåŒ…å«è®¸å¤šå…ˆè¿›çš„é¢„è®­ç»ƒæ¨¡å‹ï¼Œé’ˆå¯¹å„ç§åº”ç”¨è¿›è¡Œäº†å¾®è°ƒã€‚å®ƒæ”¯æŒçš„ä¸»è¦ä»»åŠ¡ä¹‹ä¸€æ˜¯è¯­ä¹‰æ–‡æœ¬ç›¸ä¼¼æ€§ï¼Œè¿™æ˜¯æˆ‘ä»¬å°†åœ¨è¿™ç¯‡æ–‡ç« ä¸­å…³æ³¨çš„ã€‚

è¦å®‰è£… SentenceTransformersï¼Œä½ å¿…é¡»å…ˆå®‰è£…ä¾èµ–é¡¹ [Pytorch](https://pytorch.org/) å’Œ [Transformers](https://github.com/huggingface/transformers) ã€‚

## å®‰è£… Pytorch

è¿›å…¥ [Pytorch å®˜ç½‘](https://pytorch.org/)æŒ‰ç…§è¯´æ˜å®‰è£… Pytorchã€‚

## å®‰è£…å˜å‹å™¨

è¦å®‰è£…å˜å‹å™¨ï¼Œè¯·è¿è¡Œ:

```
pip install transformers
```

## å®‰è£…å¥å­å˜å‹å™¨

ç°åœ¨ï¼Œæ‚¨å·²ç»å®‰è£…äº† Pytorch å’Œ transformersï¼Œæ‚¨å¯ä»¥é€šè¿‡ä»¥ä¸‹æ–¹å¼å®‰è£… SentenceTransformers:

```
pip install sentence-transformers
```

*æ³¨:SentenceTransformers æ¨è Python 3.6 ä»¥ä¸Šï¼ŒPyTorch 1.6.0 ä»¥ä¸Šï¼Œtransformers v3.1.0 ä»¥ä¸Šã€‚*

# å¯¼å…¥åº“

åœ¨æˆåŠŸå®‰è£…äº† SentenceTransformers åº“åŠå…¶ä¾èµ–é¡¹ä¹‹åï¼Œæˆ‘ä»¬å¯ä»¥å¼€å§‹ä½¿ç”¨è¿™ä¸ªåº“äº†ã€‚æˆ‘ä»¬å¯ä»¥é€šè¿‡ä»¥ä¸‹æ–¹å¼å¯¼å…¥å®ƒ:

```
from sentence_transformers import SentenceTransformer, util
import numpy as np
```

# æ¨¡å‹é€‰æ‹©å’Œåˆå§‹åŒ–

SentenceTransformers æ”¯æŒå„ç§é¢„è®­ç»ƒæ¨¡å‹ï¼Œè¿™äº›æ¨¡å‹é’ˆå¯¹å¼€ç®±å³ç”¨çš„ä¸åŒä»»åŠ¡è¿›è¡Œäº†å¾®è°ƒã€‚è¦æŸ¥æ‰¾é’ˆå¯¹è¯­ä¹‰æ–‡æœ¬ç›¸ä¼¼æ€§ä¼˜åŒ–çš„æ¨¡å‹åˆ—è¡¨ï¼Œæ‚¨å¯ä»¥åœ¨è¿™é‡Œçœ‹åˆ°å®ƒ[ã€‚](https://docs.google.com/spreadsheets/d/14QplCdTCDwEmTqrn1LH4yrbKvdogK4oQvYO1K1aPR5M/edit#gid=0)

æˆªè‡³æœ¬æ–‡æ’°å†™ä¹‹æ—¶ï¼Œ`stsb-roberta-large`ä½¿ç”¨ [ROBERTA-large](https://arxiv.org/abs/1907.11692) ä½œä¸ºåŸºç¡€æ¨¡å‹å’Œå‡å€¼æ± ï¼Œæ˜¯è¯­ä¹‰ç›¸ä¼¼æ€§ä»»åŠ¡çš„æœ€ä½³æ¨¡å‹ã€‚å› æ­¤ï¼Œæˆ‘ä»¬ç”¨è¿™ä¸ªæ¨¡å‹æ¥è¯æ˜ã€‚

é€‰æ‹©æ¨¡å‹åï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡ä»¥ä¸‹æ–¹å¼åˆå§‹åŒ–å®ƒ:

```
model = SentenceTransformer('stsb-roberta-large')
```

# è®¡ç®—ä¸¤ä¸ªå¥å­ä¹‹é—´çš„è¯­ä¹‰ç›¸ä¼¼åº¦

åœ¨å®šä¹‰äº†æˆ‘ä»¬çš„æ¨¡å‹ä¹‹åï¼Œæˆ‘ä»¬ç°åœ¨å¯ä»¥è®¡ç®—ä¸¤ä¸ªå¥å­çš„ç›¸ä¼¼æ€§å¾—åˆ†ã€‚å¦‚å¼•è¨€ä¸­æ‰€è®¨è®ºçš„ï¼Œè¯¥æ–¹æ³•æ˜¯ä½¿ç”¨è¯¥æ¨¡å‹å¯¹ä¸¤ä¸ªå¥å­è¿›è¡Œç¼–ç ï¼Œç„¶åè®¡ç®—å¾—åˆ°çš„ä¸¤ä¸ªåµŒå…¥çš„ä½™å¼¦ç›¸ä¼¼åº¦ã€‚æœ€ç»ˆç»“æœå°†æ˜¯è¯­ä¹‰ç›¸ä¼¼åº¦å¾—åˆ†ã€‚

ä¸€èˆ¬æ¥è¯´ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ä¸åŒçš„å…¬å¼æ¥è®¡ç®—æœ€ç»ˆçš„ç›¸ä¼¼æ€§å¾—åˆ†(ä¾‹å¦‚ï¼Œç‚¹ç§¯ã€Jaccard ç­‰ã€‚)ï¼Œä½†åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œç”±äºä½™å¼¦ç›¸ä¼¼æ€§çš„æ€§è´¨ï¼Œæˆ‘ä»¬ä½¿ç”¨ä½™å¼¦ç›¸ä¼¼æ€§ã€‚æ›´é‡è¦çš„å› ç´ æ˜¯ç”±æ¨¡å‹äº§ç”Ÿçš„åµŒå…¥ï¼Œå› æ­¤ä½¿ç”¨åˆé€‚çš„ç¼–ç æ¨¡å‹æ˜¯å¾ˆé‡è¦çš„ã€‚

ä¸ºäº†ä½¿ç”¨æ‰€è®¨è®ºçš„æ–¹æ³•æ¥è®¡ç®—è¯­ä¹‰ç›¸ä¼¼æ€§å¾—åˆ†ï¼Œæˆ‘ä»¬å¯ä»¥è¿è¡Œä»¥ä¸‹å†…å®¹:

```
sentence1 = "I like Python because I can build AI applications"
sentence2 = "I like Python because I can do data analytics"# encode sentences to get their embeddings
embedding1 = model.encode(sentence1, convert_to_tensor=True)
embedding2 = model.encode(sentence2, convert_to_tensor=True)# compute similarity scores of two embeddings
cosine_scores = util.pytorch_cos_sim(embedding1, embedding2)print("Sentence 1:", sentence1)
print("Sentence 2:", sentence2)
print("Similarity score:", cosine_scores.item())
```

æˆ‘ä»¬é¦–å…ˆå®šä¹‰ä¸¤ä¸ªå¥å­ï¼Œå¥å­ 1 å’Œå¥å­ 2ï¼Œç„¶åä½¿ç”¨æˆ‘ä»¬ä¹‹å‰å®šä¹‰çš„æ¨¡å‹å¯¹å®ƒä»¬è¿›è¡Œç¼–ç ã€‚æˆ‘ä»¬å°†æœ€ç»ˆçš„åµŒå…¥è½¬æ¢ä¸ºå¼ é‡ï¼Œè¿™æ · GPU å¯ä»¥æ›´å¿«åœ°å¤„ç†å®ƒä»¬ã€‚å¯¹äºåƒæˆ‘ä»¬è¿™æ ·çš„å°æ•°æ®ï¼Œè¿™ä¸æ˜¯å¿…éœ€çš„ï¼Œä½†ä»ç„¶æ˜¯ä¸€ä¸ªå¾ˆå¥½çš„å®è·µã€‚

ç„¶åï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ util æä¾›çš„ pytorch_cos_sim å‡½æ•°æ–¹ä¾¿åœ°è®¡ç®—ä¸¤ä¸ªåµŒå…¥ä¹‹é—´çš„ä½™å¼¦ç›¸ä¼¼æ€§å¾—åˆ†ï¼Œè¿™è¦æ„Ÿè°¢å¥å­è½¬æ¢å™¨ã€‚

![](img/946e7021861df84c08aa6b99c428c89c.png)

æŠ½æ ·è¾“å‡º

æœ€åå¯ä»¥çœ‹åˆ°ç›¸ä¼¼åº¦å¾—åˆ†ã€‚æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œåœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œå¥å­ 1 å’Œå¥å­ 2 çš„åˆ†æ•°æ¥è¿‘ 1ï¼Œè¿™æ„å‘³ç€å®ƒä»¬éå¸¸ç›¸ä¼¼ã€‚

# è®¡ç®—ä¸¤ä¸ªå¥å­åˆ—è¡¨ä¹‹é—´çš„è¯­ä¹‰ç›¸ä¼¼åº¦

å½“ä½ æƒ³ç›¸äº’æ¯”è¾ƒæ›´å¤šçš„å¥å­æ—¶ï¼Œä½ å¯ä»¥æŠŠå¥å­æ”¾å…¥ä¸¤ä¸ªåˆ—è¡¨ä¸­ï¼Œç”¨å’Œä¸Šé¢ä¸€æ ·çš„ä»£ç è®¡ç®—å®ƒä»¬ä¹‹é—´çš„ç›¸ä¼¼åº¦å¾—åˆ†ã€‚æœ€ç»ˆç»“æœå°†æ˜¯ä¸€ä¸ªç›¸ä¼¼æ€§å¾—åˆ†çŸ©é˜µï¼Œå…¶ä¸­`i, j`å…ƒç´ åŒ…å«åˆ—è¡¨ 1 ä¸­çš„å¥å­`i`å’Œåˆ—è¡¨ 2 ä¸­çš„å¥å­`j`ä¹‹é—´çš„ç›¸ä¼¼æ€§å¾—åˆ†ã€‚ä¸ºæ­¤ï¼Œè¯·è¿è¡Œä»¥ä¸‹å‘½ä»¤:

```
sentences1 = ["I like Python because I can build AI applications", "The cat sits on the ground"]   
sentences2 = ["I like Python because I can do data analytics", "The cat walks on the sidewalk"]# encode list of sentences to get their embeddings
embedding1 = model.encode(sentences1, convert_to_tensor=True)
embedding2 = model.encode(sentences2, convert_to_tensor=True)# compute similarity scores of two embeddings
cosine_scores = util.pytorch_cos_sim(embedding1, embedding2)for i in range(len(sentences1)):
    for j in range(len(sentences2)):
        print("Sentence 1:", sentences1[i])
        print("Sentence 2:", sentences2[j])
        print("Similarity Score:", cosine_scores[i][j].item())
        print()
```

![](img/bc765e66f9109e1248f82b5c27aefb80.png)

æŠ½æ ·è¾“å‡º

ä½ å¯ä»¥çœ‹åˆ°è¿™ä¸¤ä¸ªå¥å­å¯¹(â€œæˆ‘å–œæ¬¢ Pythonï¼Œå› ä¸ºæˆ‘å¯ä»¥æ„å»º AI åº”ç”¨â€ã€â€œæˆ‘å–œæ¬¢ Pythonï¼Œå› ä¸ºæˆ‘å¯ä»¥åšæ•°æ®åˆ†æâ€)å’Œ(â€œçŒ«ååœ¨åœ°ä¸Šâ€ã€â€œçŒ«èµ°åœ¨äººè¡Œé“ä¸Šâ€)æ¯”è¾ƒç›¸ä¼¼ã€‚å› æ­¤ï¼Œè¾“å‡ºçš„ç›¸ä¼¼æ€§å¾—åˆ†ä¹Ÿç›¸å¯¹è¾ƒé«˜ã€‚å¦ä¸€æ–¹é¢ï¼ŒPython ä¸çŒ«éå¸¸ä¸ç›¸ä¼¼ï¼Œåä¹‹äº¦ç„¶ï¼Œå› æ­¤å…¶ä»–ä¸¤ä¸ªå¥å­å¯¹çš„ç›¸ä¼¼åº¦å¾—åˆ†è¾ƒä½ã€‚

# **ä»ç»™å®šå¥å­çš„è¯­æ–™åº“ä¸­æ£€ç´¢å‰ K ä¸ªæœ€ç›¸ä¼¼çš„å¥å­**

è¯­ä¹‰ç›¸ä¼¼æ€§çš„ä¸€ä¸ªæµè¡Œç”¨ä¾‹æ˜¯åœ¨ç»™å®šæŸ¥è¯¢å¥å­çš„è¯­æ–™åº“ä¸­æ‰¾åˆ°æœ€ç›¸å…³çš„å¥å­ã€‚è¿™ä¹Ÿå¯ä»¥ç§°ä¸ºè¯­ä¹‰æœç´¢ã€‚ä¸ºäº†è¿›è¡Œè¯­ä¹‰æœç´¢ï¼Œæˆ‘ä»¬éœ€è¦ä¸€ä¸ªå¥å­è¯­æ–™åº“å’Œä¸€ä¸ªå……å½“æŸ¥è¯¢çš„å¥å­ã€‚ç„¶åï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨æˆ‘ä»¬çš„æ¨¡å‹å¯¹è¯­æ–™åº“å’Œæˆ‘ä»¬çš„å¥å­è¿›è¡Œç¼–ç ï¼Œç„¶åä½¿ç”¨ä¸å‰é¢ç›¸åŒçš„æ–¹æ³•è®¡ç®—æˆ‘ä»¬çš„å¥å­å’Œè¯­æ–™åº“ä¸­æ¯ä¸ªå¥å­ä¹‹é—´çš„ä½™å¼¦ç›¸ä¼¼æ€§å¾—åˆ†ã€‚æœ€åï¼Œé€šè¿‡è·å¾—å‰ k ä¸ªæœ€é«˜ç›¸ä¼¼åº¦å¾—åˆ†ï¼Œæˆ‘ä»¬å¯ä»¥è·å¾—å‰ k ä¸ªæœ€ç›¸ä¼¼çš„å¥å­ã€‚

```
corpus = ["I like Python because I can build AI applications",
          "I like Python because I can do data analytics",
          "The cat sits on the ground",
         "The cat walks on the sidewalk"]# encode corpus to get corpus embeddings
corpus_embeddings = model.encode(corpus, convert_to_tensor=True)sentence = "I like Javascript because I can build web applications"# encode sentence to get sentence embeddings
sentence_embedding = model.encode(sentence, convert_to_tensor=True)# top_k results to return
top_k=2# compute similarity scores of the sentence with the corpus
cos_scores = util.pytorch_cos_sim(sentence_embedding, corpus_embeddings)[0]# Sort the results in decreasing order and get the first top_k
top_results = np.argpartition(-cos_scores, range(top_k))[0:top_k]print("Sentence:", sentence, "\n")
print("Top", top_k, "most similar sentences in corpus:")
for idx in top_results[0:top_k]:
    print(corpus[idx], "(Score: %.4f)" % (cos_scores[idx]))
```

![](img/1f22f0288bb2557481f58240cf453812.png)

æŠ½æ ·è¾“å‡º

åœ¨æˆ‘ä»¬çš„ä¾‹å­ä¸­ï¼Œæˆ‘ä»¬çš„è¯­æ–™åº“æœ‰ 4 ä¸ªå¥å­ï¼Œæˆ‘ä»¬å°† top_k è®¾ç½®ä¸º 2ï¼Œä»¥æ£€ç´¢ä¸æˆ‘ä»¬çš„æŸ¥è¯¢å¥å­æœ€ç›¸ä¼¼çš„å‰ 2 ä¸ªå¥å­ã€‚æˆ‘ä»¬çš„æŸ¥è¯¢è¯­å¥æ˜¯â€œæˆ‘å–œæ¬¢ Javascriptï¼Œå› ä¸ºæˆ‘å¯ä»¥æ„å»º web åº”ç”¨ç¨‹åºâ€ï¼Œè¿”å›å…³äº Python çš„ä¸¤ä¸ªè¯­å¥æ˜¯å› ä¸º Javascript æ¯” cats æ›´ç±»ä¼¼äº Pythonã€‚

# ç»“è®º

ç°åœ¨ä½ æœ‰äº†ï¼ç°åœ¨ï¼Œä½ åº”è¯¥èƒ½å¤Ÿç†è§£å¦‚ä½•è®¡ç®—å¥å­ä¹‹é—´çš„è¯­ä¹‰ç›¸ä¼¼åº¦ï¼Œå¥å­åˆ—è¡¨ï¼Œä»¥åŠä»è¯­æ–™åº“ä¸­æ£€ç´¢æœ€ç›¸ä¼¼çš„æ•°æ®ã€‚ä¸ºäº†æ–¹ä¾¿èµ·è§ï¼Œä¸‹é¢æ˜¯è¿™ç¯‡æ–‡ç« çš„å…¨éƒ¨ Jupyter ä»£ç :

å¦‚æœä½ æœ‰ä»»ä½•é—®é¢˜ï¼Œè¯·åœ¨ä¸‹é¢çš„è¯„è®ºä¸­å‘Šè¯‰æˆ‘ï¼

å¦‚æœä½ å–œæ¬¢æˆ‘çš„ä½œå“ï¼Œå¯ä»¥éšæ„æµè§ˆæˆ‘çš„å…¶ä»–æ–‡ç« :)

[](/top-nlp-books-to-read-2020-12012ef41dc1) [## 2020 å¹´æœ€ä½³ NLP è¯»ç‰©

### è¿™æ˜¯æˆ‘ä¸ªäººä¸ºè‡ªç„¶è¯­è¨€å¤„ç†æ¨èçš„ä¹¦ç±åˆ—è¡¨ï¼Œä¾›å®è·µè€…å’Œç†è®ºå®¶å‚è€ƒ

towardsdatascience.com](/top-nlp-books-to-read-2020-12012ef41dc1) [](/top-nlp-libraries-to-use-2020-4f700cdb841f) [## 2020 å¹´å°†ä½¿ç”¨çš„é¡¶çº§ NLP åº“

### AllenNLPï¼ŒFast.aiï¼ŒSpacyï¼ŒNLTKï¼ŒTorchTextï¼ŒHuggingfaceï¼ŒGensimï¼ŒOpenNMTï¼ŒParlAIï¼ŒDeepPavlov

towardsdatascience.com](/top-nlp-libraries-to-use-2020-4f700cdb841f) [](/bert-text-classification-using-pytorch-723dfb8b6b5b) [## ä½¿ç”¨ Pytorch çš„ BERT æ–‡æœ¬åˆ†ç±»

### æ–‡æœ¬åˆ†ç±»æ˜¯è‡ªç„¶è¯­è¨€å¤„ç†ä¸­çš„ä¸€é¡¹å¸¸è§ä»»åŠ¡ã€‚æˆ‘ä»¬åº”ç”¨ BERTï¼Œä¸€ä¸ªæµè¡Œçš„å˜å‹å™¨æ¨¡å‹ï¼Œå¯¹å‡æ–°é—»æ£€æµ‹ä½¿ç”¨â€¦

towardsdatascience.com](/bert-text-classification-using-pytorch-723dfb8b6b5b) [](/fine-tuning-gpt2-for-text-generation-using-pytorch-2ee61a4f1ba7) [## ä½¿ç”¨ Pytorch å¾®è°ƒç”¨äºæ–‡æœ¬ç”Ÿæˆçš„ GPT2

### ä½¿ç”¨ Pytorch å’Œ Huggingface å¾®è°ƒç”¨äºæ–‡æœ¬ç”Ÿæˆçš„ GPT2ã€‚æˆ‘ä»¬åœ¨ CMU å›¾ä¹¦æ‘˜è¦æ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒï¼Œä»¥ç”Ÿæˆâ€¦

towardsdatascience.com](/fine-tuning-gpt2-for-text-generation-using-pytorch-2ee61a4f1ba7) 

# **å‚è€ƒæ–‡çŒ®**

[1] [Pytorch å®˜ç½‘](https://pytorch.org/)ï¼Œè„¸ä¹¦

ã€2ã€‘[å˜å½¢é‡‘åˆš Github](https://github.com/huggingface/transformers) ï¼Œæ‹¥æŠ±è„¸

[3] [SentenceTransformers ç½‘ç«™](https://www.sbert.net/)ï¼Œæ— å¤„ä¸åœ¨çš„çŸ¥è¯†å¤„ç†å®éªŒå®¤

[4][sentence transformers Github](https://github.com/UKPLab/sentence-transformers)ï¼Œæ— å¤„ä¸åœ¨çš„çŸ¥è¯†å¤„ç†å®éªŒå®¤

[5]åˆ˜ï¼Œï¼Œç­‰.[â€œRoberta:ä¸€ç§ç¨³å¥ä¼˜åŒ–çš„ bert é¢„è®­ç»ƒæ–¹æ³•â€](https://arxiv.org/abs/1907.11692) *arXiv é¢„å°æœ¬ arXiv:1907.11692* (2019)ã€‚