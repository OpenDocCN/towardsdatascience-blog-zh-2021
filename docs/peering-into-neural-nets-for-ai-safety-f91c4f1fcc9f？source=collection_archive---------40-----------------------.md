# 人工智能安全的神经网络研究

> 原文：<https://towardsdatascience.com/peering-into-neural-nets-for-ai-safety-f91c4f1fcc9f?source=collection_archive---------40----------------------->

## [播客](https://towardsdatascience.com/tagged/tds-podcast)

## 丹尼尔·菲兰谈可解释性，人工智能安全，以及如何找到重要的问题来解决

[苹果](https://podcasts.apple.com/ca/podcast/towards-data-science/id1470952338?mt=2) | [谷歌](https://www.google.com/podcasts?feed=aHR0cHM6Ly9hbmNob3IuZm0vcy8zNmI0ODQ0L3BvZGNhc3QvcnNz) | [SPOTIFY](https://open.spotify.com/show/63diy2DtpHzQfeNVxAPZgU) | [其他](https://anchor.fm/towardsdatascience)

*编者按:这一集是我们关于数据科学和机器学习新兴问题的播客系列的一部分*，*由 Jeremie Harris 主持。除了主持播客，Jeremie 还帮助运营一家名为*[*sharpes minds*](http://sharpestminds.com)*的数据科学导师初创公司。*

许多人工智能研究人员认为，随着人工智能能力的增加，很难设计出继续保持安全的人工智能系统。我们已经在播客上看到，人工智能对齐领域已经出现来解决这个问题，但一个相关的努力也正针对安全问题的一个单独的维度:人工智能的可解释性。

我们解释人工智能系统如何处理信息和做出决策的能力将可能成为未来确保人工智能可靠性的重要因素。本期播客的嘉宾将他的研究集中在这个主题上。丹尼尔·菲兰是伯克利的人工智能安全研究员，在那里他受到人工智能先驱斯图尔特·拉塞尔的指导。丹尼尔还运营 AXRP，这是一个致力于人工智能技术对齐研究的播客。

以下是我最喜欢的一些外卖食品:

*   像有效利他主义社区的许多人一样，丹尼尔经常在重要事件上下赌注，以测试他的世界模型。他发现这是一个有用的练习，原因有几个:在游戏中有金融皮肤可以迫使你更客观地思考你认为是真实的或可能的。第二，当你输掉一场赌博时，不得不付钱给某人，这可以作为一个有用的提醒，让你重新评估自己的心智模式:交出钱的痛苦是一个很好的“可教的时刻触发器”。在政治等话题上，赌博尤其有助于避免陷入盲目的意识形态陷阱，在这些话题上，谈论(和无支持的预测)是廉价的:通过在游戏中加入皮肤，你被迫评估你可能会保护或认为理所当然的信念。
*   我们讨论了人工智能风险怀疑论者通常提出的两个论点。第一个想法是，普遍比人更聪明的人工智能可能是不可能的。丹尼尔认为这不太可能，因为我们知道物质的特定排列可以提高人类的智力，人类之间的差异表明人类大脑本身可以轻微调整以大大提高认知能力。出于这个原因和其他原因，当谈到智力时，人类代表了物理可能的顶点似乎是相当不可信的。
*   我们讨论的人工智能怀疑论的第二个论点是一个更常见的论点:随着人工智能变得越来越聪明，它们自然会变得更有道德，因此也更安全。丹尼尔不同意这种观点，并引用了正交命题，即智力和道德不一定相关。我们探索了几个支持正交命题的例子，包括世界上许多最糟糕的独裁者客观上非常聪明的观点。最终，丹尼尔认为，没有理由想象极端水平的智力不能被部署为不道德或不受欢迎的目标服务。
*   丹尼尔的研究涉及在特定任务上训练神经网络(例如，MNIST 数据集上的手写数字分类)，并在训练过程后检查它们，以查看网络是否形成有趣的结构。丹所说的“有趣的结构”指的是通过大权重紧密相连的神经元簇，以及通过小得多的权重与网络其余部分松散相连的神经元簇。值得注意的是，事实证明神经网络确实形成了这些结构，这可能为它们的操作和“思考”方式提供了线索。丹尼尔希望，随着时间的推移，这些结构将允许我们解释并最终预测神经网络的行为，并减少事故和人工智能对抗性发展的机会。

你可以在这里的 Twitter 上关注丹尼尔，或者在这里的 Twitter 上关注我[。](https://twitter.com/jeremiecharris)

![](img/57edcfd1cf70e2da16b27644a16c69eb.png)

## 章节:

*   0:00 介绍
*   1:30 有效利他主义
*   5:05 带来统计数据
*   10:55 与其他论点互动
*   17:45 人类类比的风险
*   23:30 更新心智模型
*   31:00 人工智能安全研究中的警告
*   35:47 清晰度和安全性之间的关系
*   45:20 我们对现实的假设
*   59:55 人工智能社区内部的紧张关系
*   1:04:40 总结