# 站点位置数据的机器学习和分析

> 原文：<https://towardsdatascience.com/machine-learning-and-analysis-of-site-position-data-69e3d91fe473?source=collection_archive---------33----------------------->

## 谷歌的机器学习算法是什么，它们如何帮助在 SERP 中获得更高的职位

![](img/f2055ecf5ddf8af3b03cf78907335cf5.png)

[KOBU 机构](https://unsplash.com/@kobuagency?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)在 [Unsplash](https://unsplash.com/s/photos/machine-learning?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText) 上拍摄的照片

机器学习(ML)在过去十年中一直在积极发展。我们必须承认，进步取得了令人印象深刻的成功。然而，尽管 ML 的积极发展，许多 SEO 专家还没有使用这个选项，因为神经网络的使用仍然是昂贵的。

与此同时，谷歌越来越多地引入算法，在 SERP 中对网站进行排名。因此，如果程序员推广一个个人网站或在客户端网站上工作，他肯定需要知道谷歌的下一代算法是如何工作的，并把它们考虑在内。那么，搜索引擎教给神经网络什么，这又如何影响网站的排名呢？

# 机器学习如何影响搜索引擎排名？

为了让网站排名靠前，网站所有者需要定期发布新鲜的[内容](https://towardsdatascience.com/tagged/content-creation)。但是，有两个关键要求:

*   内容必须相关；
*   对用户应该也是有用的。

机器人的进步已经达到了这样一种程度，它们可以独立地决定这个副本是否对用户有用。谷歌评估员可以理解文本，并确定信息的质量。因此，写点东西，仅仅用关键词填充文字已经不够了。文章包含的信息要有用，主题要充分公开。

SEO 的主要原则是优化一个网站，不仅是为了搜索机器人的需求，也是为了给用户提供重要的信息。谷歌还衡量用户体验，并可以确定网站上的内容是否会激励用户与公司互动。

基于 ML 的算法不断接收新的数据进行分析，并学会从表面上破译查询并理解它们。得益于此，谷歌为用户提供了更好、更有趣的内容，从而保持了用户对搜索引擎本身的兴趣。所以谷歌机器学习会不断进化，对内容的要求会越来越高。

如果一个网站上的内容能让用户获得积极的体验，那么这样的网站就更容易进入 SERP 的前 10 名。人工智能越来越有效，每天都在学习越来越深入地分析文本。

# 基于神经网络的谷歌算法

分析站点的主要工具之一是神经网络。神经网络的原理操作类似于人脑中神经元的操作原理。由于某些算法，神经网络分析内容并给出自己的评估。这些技术极大地提高了搜索引擎机器人和内容分析的质量。

谷歌神经网络的确切算法没有公开，是公司机密。尽管如此，该公司的员工会定期在博客上发布更新，并为用户提供优化网站的建议。谷歌的算法体系统称为核心算法。这个系统包括几十种附加算法。以下是其中一些的简要概述。

1.  **熊猫**。该算法是目前运行的最古老的算法之一，它于 2011 年推出。熊猫算法是为了识别低质量的内容、重复和非原创文本而创建的。
2.  **企鹅**是一种更新的算法，专门用来处理低质量的链接。如果一家公司在昏暗的平台或链接交换上发布[到其网站的反向链接](https://seranking.com/blog/backlink-quality/)，就有被企鹅算法惩罚的风险。为了不遇到此类问题，建议定期监控指向网站的链接。如果有所谓的“垃圾”链接，最好尽快删除。为此使用[否认工具](https://www.google.com/webmasters/tools/disavow-links-main)。
3.  **蜂鸟**。这种算法由谷歌在 2013 年推出，具有确定内容质量和意义的重要功能。在蜂鸟开发之前，搜索机器人只能识别文本的语义部分，优化只针对关键词就足够了。Hummingbird 分析整个文本，并为用户提供页面，即使用户在搜索栏中输入的不是逐字查询。如果内容能对读者有帮助，蜂鸟会建议。
4.  **RankBrain** 。它是一个独特的谷歌系统，结合了人工智能和机器学习。RankBrain 自 2015 年开始运营，旨在将特定网站上的文本与 SERP 中其他网站的内容进行比较。基于比较结果，谷歌决定应该向用户显示什么信息。RankBrain 还考虑了其他用户的行为，这也有助于确定内容的有用性和相关性。
5.  **负鼠**。该算法于 2016 年推出，旨在区分一般查询和区域查询。该算法的主要任务是从搜索结果中删除具有相同地址、电话号码和其他联系人的公司。同时增加有本地地址和联系方式的公司排名。

# 基于机器学习的新 BERT 和 SMITH 算法

新的 [BERT 算法](https://blog.google/products/search/search-language-understanding-bert/)是专门为基于神经网络的内容分析而设计的。它通过处理内容的意义和质量而不是关键字来增加结果的相关性。

当前的算法直接对关键字或短语进行分析，这允许它们选择相关的结果。伯特是一个完全不同的概念。谷歌专家的任务是教会算法评估查询，不是字面上的，而是确定其本质。为此，分析将不仅用于关键词，还用于其他词，包括定义和介词。因此，BERT 旨在从搜索结果中排除不符合查询本质的内容。

该算法于 2019 年 10 月公布，但到目前为止，尚未在最终版本中推出。谷歌继续开发该算法。该算法在长搜索查询上进行测试，这些长搜索查询不能被分成块来丢失短语的含义。在处理短查询时，不使用 BERT，包括那些带有品牌名称的查询。未来，谷歌计划将这种算法用于世界上大多数语言。迄今为止，谷歌尚未正式宣布确切的推出日期。

此外，谷歌已经公布了另一个算法，SMITH。在理解长查询和文档方面优于 BERT。SMITH 算法试图理解整个文档，而 BERT 处理单个片段。不过，谷歌开发者至今没有宣布开始使用。

# 如何将 ML 算法的知识用于 SEO？

那么，一个 SEO 专家应该如何处理这些信息呢？有几个简单的步骤可以提高网站的排名。

*   **改进元标签。**建议不时重写，让所有元标签逻辑正确。元标签中的信息应该反映内容的核心。
*   **围绕关键词创作内容。**无论如何，算法仍然会分析关键词，因此选择正确的关键词并围绕语义核心构建内容非常重要。使用现代关键词收集工具，如 [SE 排名](https://seranking.com/keyword-suggestion-tool.html)和 [Similarweb](https://www.similarweb.com/corp/solution/find-competitors-keywords-nt/) 。
*   **写翔实有用的内容。用户从文章中获得的信息越多，搜索机器人在搜索结果中提升页面的几率就越高。**
*   **想到可读性。尽量用清晰的语言来写。结构文本。突出重点。不要忘记链接到可以提供关于该主题的附加信息的文章。**
*   **请教专家。**如果任何领域的知识薄弱，请提供专家的报价。医学领域的搜索引擎对专家内容的排名尤其靠前——为此；甚至引入了额外的 [E-A-T 算法](https://www.searchenginejournal.com/google-eat-misconceptions/333875/)。

# 摘要

谷歌的搜索算法是一个不断发展的复杂系统。算法的使用是为了在搜索结果中只留下完全符合用户预期的高质量内容。我们可以预期，很快会有更多基于神经网络的算法出现。所以，如果程序员做个人网站的 SEO，不如重建策略，在竞争中领先一步。