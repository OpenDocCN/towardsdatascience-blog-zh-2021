# 引领人工智能进入下一阶段的 5 个深度学习趋势

> 原文：<https://towardsdatascience.com/5-deep-learning-trends-leading-artificial-intelligence-to-the-next-stage-11f2ef60f97e?source=collection_archive---------1----------------------->

## 人工智能|深度学习|未来

## 深度学习主导了人工智能，但它需要更新以保持其霸权，并推动该领域向前发展。

![](img/6c9b20007ce9137e1a3bb8f6cdb669b2.png)

来源:[顾](https://unsplash.com/photos/-HSvETdLVbk)在 Unsplash

人类是发明家的一种。世界提供给我们原材料，我们用熟练的工艺将它们转化。技术创造了无数的工具和设备:轮子、印刷机、蒸汽机、汽车、电力、互联网……这些发明已经并仍在塑造着我们的文明和文化。

我们最新的技术孩子之一是人工智能，这是一种近年来与我们的生活交织在一起的工具。它对我们社会的影响是显著的，预计在未来的几十年里[会继续增长。人工智能的领军人物之一吴恩达甚至说“](https://news.usc.edu/trojan-family/five-ways-ai-will-change-the-world-by-2050/)[人工智能是新的电力](https://www.wipo.int/wipo_magazine/en/2019/03/article_0001.html)”在斯坦福商业的采访中，他说“就像 100 年前电力改变了几乎一切一样，今天我实际上很难想象一个我认为人工智能在未来几年内不会改变的行业。”

但人工智能并不新鲜。它存在于 1956 年，当时约翰·麦卡锡创造了术语，并提出人工智能是一个独立的研究领域。从那以后，它经历了[完全漠不关心](https://en.wikipedia.org/wiki/AI_winter)和[源源不断的资金和利息](https://www.privateequitywire.co.uk/2020/11/19/292458/ai-startups-raised-usd734bn-total-funding-2020)的交替时期。今天，机器学习和深度学习(DL)垄断了 AI。2012 年开始的 DL 革命还没有结束。DL 戴上了人工智能的桂冠，但专家们认为它需要一些改变来保持它。让我们看看 DL 的未来。

# 摆脱卷积神经网络

在“人工智能教父”杰弗里·辛顿(Geoffrey Hinton)和他的团队凭借[一个基于卷积神经网络(CNN)](https://papers.nips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf)的模型赢得 2012 年 ImageNet 挑战赛之后，DL 的人气飙升。他们以+10%的误差率击败了他们的(非 DL)对手，达到了 63.30%的最高准确率。我们可以说，在过去的十年中，数字图书馆引起了 CNN 的成功和兴趣，这是我们的功劳。

基于 CNN 的模型在计算机视觉任务中非常受欢迎，如[分类图像](https://arxiv.org/abs/2003.10580v4)、[检测物体](https://viso.ai/deep-learning/object-detection/)，或[识别人脸](https://www.thalesgroup.com/en/markets/digital-identity-and-security/government/biometrics/facial-recognition)。然而，尽管它们很有用，但辛顿在他的 [AAAI 2020 主题演讲](https://www.youtube.com/watch?v=UX8OubxsY8w)中强调了一个重要的缺点:“【CNN】不太擅长处理旋转和缩放等视角变化的[…]影响。”

CNN 可以处理翻译，但人类的视觉系统也可以识别不同视角、背景或光照条件下的物体，这是 CNN 做不到的。当今天的顶级 CNN 系统——在 ImageNet 基准测试中获得了+90%的顶级准确性(——尝试在真实世界的对象数据集中对图像进行分类时，它们会经历 40 %- 45%的性能下降(。

另一个问题是所谓的[反面例子](https://arxiv.org/abs/1312.6199)。辛顿再次强调了人类视觉系统和 CNN 之间的差异:“我可以拍摄一张图像和一点点噪声，CNN 会将其识别为完全不同的东西，我几乎看不出它发生了变化。[……]我认为这证明 CNN 实际上在使用与我们非常不同的信息来识别图像。”CNN 从根本上不同于人类的视觉系统。由于它们的不可预测性，我们根本不能依赖它们。

Hinton 更进一步解释说，CNN 系统不能解释他们在图像中看到的物体。我们知道物体存在于世界中，我们对它们有经验。从很小的时候起，我们就知道了坚固性、形状恒常性或物体的持久性。我们可以利用这些知识来理解奇怪的物体，但 CNN 只能看到一堆像素。我们可能需要从根本上改变计算机视觉的统治模式，也许是朝着[胶囊网络](https://arxiv.org/abs/1710.09829)的方向。套用量子力学之父马克斯·普朗克的话:

> “科学一次进步一个葬礼。”

# 自我监督的深度学习

> "人工智能的下一场革命将不会受到监督，也不会得到纯粹的强化."
> 
> ——脸书大学首席人工智能科学家 Yann LeCun

今天的 DL 的一个明显的限制是它对大量标记数据和计算能力的依赖。另一位人工智能先驱 Yann LeCun 说，我们需要用他所谓的[自我监督学习](https://bdtechtalks.com/2020/03/23/yann-lecun-self-supervised-learning/)来取代监督学习——这是大多数人工智能系统的训练方法:

> "*【自我监督学习】*是在学习一个任务之前，先学习代表世界的思想。这是婴儿和动物的行为。一旦我们对世界有了好的描述，学习一项任务就需要很少的试验和样本。”

系统将从原始数据中学习来标记它，而不是用标记的数据来训练系统。我们人类学习的速度比监督(或强化)系统快几个数量级。孩子们不会通过看成百上千张树的照片来学会辨认一棵树。他们看到一个，然后把“树”这个标签放在他们直觉上知道属于这个类别的所有东西上。我们部分是通过观察来学习的，这是计算机目前还做不到的。

Yann LeCun 在 2019 年 12 月就该话题对[进行了深度访谈](https://www.youtube.com/watch?v=A7AnCvYDQrU&t=45s)。他认为，自我监督系统将能够“从任何其他部分预测输入的任何部分。”例如，它可以从过去预测未来，或者从可见的事物中预测不可见的事物。然而，尽管这种类型的学习对离散输入很有效，如文本([谷歌的 BERT](https://venturebeat.com/2018/11/02/google-open-sources-bert-a-state-of-the-art-training-technique-for-natural-language-processing/) 或 [OpenAI 的 GPT-3](https://arxiv.org/abs/2005.14165) )，但对连续数据如图像、音频或视频则不太有效。为此，他解释说，我们将需要[潜在的基于可变能源的模型](https://openai.com/blog/energy-based-models/)，这种模型更适合处理世界固有的不确定性。

自我监督学习将取代监督学习。未来仍有一些挑战，但我们已经在搭建弥合差距的桥梁。可以肯定的是，一旦到了彼岸，就不会回头。

> "标签是机器学习研究者的鸦片."
> 
> —吉坦德拉·马利克，加州大学伯克利分校 EECS 教授

# 混合模型:符号 AI +深度学习

自从人工智能的概念提出以来，有两种范式在人工智能中受到了空前的欢迎:符号人工智能(也称为基于规则的人工智能)和 DL。从 50 年代到 80 年代，象征性人工智能主导了这个领域，但今天大多数专家反对这个框架。约翰·豪格兰德在他的书《人工智能:非常理念》中称之为 GOFAI(优秀的老式人工智能)。

> *【符号人工智能】*处理现实世界的抽象表现，这些表现是用主要基于数理逻辑的表现性语言建模的

这是一种自上而下的人工智能方法。它旨在通过使用“问题的高级符号表示”来赋予机器智能，遵循由艾伦·纽厄尔和司马贺提出的物理符号系统假设。例如，[专家系统](https://www.osti.gov/biblio/5675197)——最受欢迎的符号人工智能形式——被设计成通过遵循一套如果-那么规则来模拟人类决策。

混合模型是结合符号 AI 和 DL 优点的一种尝试。马丁·福特在他的《智能建筑师<https://bookshop.org/books/architects-of-intelligence-the-truth-about-ai-from-the-people-building-it/9781789131512?aid=11092>*一书中，就这种方法采访了人工智能专家。吴恩达强调了它在解决我们只有小数据集的问题时的有用性。麻省理工学院计算认知科学教授乔希·特南鲍姆(Josh Tenenbaum)和他的团队开发了一种混合模型[“在没有任何明确监督的情况下，学习视觉概念、单词和句子的语义解析。”](https://openreview.net/forum?id=rJgMlhRctm)*

*纽约大学心理学教授加里·马库斯认为，混合模型可以更好地处理常识推理。[在最近的一篇论文](https://arxiv.org/ftp/arxiv/papers/2002/2002.06177.pdf)中，马库斯通过提及人类智力来强调他的观点:*

> *“某种形式的符号操作似乎对人类的认知至关重要，比如当一个孩子知道“姐妹”这个词的含义时，这个词可以应用在无数个家庭中。”*

*尽管前景看好，混合方法也有重要的批评者。杰弗里·辛顿(Geoffrey Hinton)批评了那些打算用象征性人工智能宠坏 DL 的人。“他们不得不承认深度学习正在做令人惊讶的事情，他们希望将深度学习作为一种低级仆人，为他们提供让他们的符号推理工作所需的东西，”他说。不管它是否奏效，混合动力车型都是未来几年需要关注的事情。*

> *“我预测，几年之内，许多人会想知道，为什么深度学习这么长时间以来一直试图在没有符号操纵工具的情况下做到这一点。”*
> 
> *—加里·马库斯*

# *系统 2 深度学习*

*完成 [2018 图灵奖获奖者三重奏](https://awards.acm.org/about/2018-turing)的 Yoshua Bengio(与 Hinton 和 LeCun 一起)在 2019 年发表了题为 [*的演讲，从系统 1 深度学习到系统 2 深度学习*](https://www.youtube.com/watch?v=T3sxeTgT4qc) 。他谈到了 DL 的现状，其中的趋势是让一切变得更大:更大的数据集、更大的计算机和更大的神经网络。他认为，我们不会在这个方向上进入人工智能的下一个阶段。*

> *“我们有以非常狭窄的方式学习的机器。他们需要比人类智能例子多得多的数据来学习任务*仍然会犯愚蠢的错误。”**

**本吉奥从丹尼尔·卡内曼在其里程碑式的著作 [*《思考，快与慢*](https://bookshop.org/books/thinking-fast-and-slow/9780374533557) 》中的洞见中汲取了双系统框架。卡尼曼将系统 1 描述为“自动且快速地运行，很少或没有努力，也没有自愿控制的感觉”，而系统 2“将注意力分配给需要它的费力的精神活动[……]通常与代理、选择和集中的主观体验有关。”**

**[Rob Toews](https://www.forbes.com/sites/robtoews/2020/10/29/the-next-generation-of-artificial-intelligence-part-2/?sh=687177627a30) 总结了 DL 的现状:“今天的尖端人工智能系统擅长系统 1 的任务，但与系统 2 的任务斗争激烈。”本吉奥同意。“我们(人类)想出算法、配方，我们可以计划、推理、使用逻辑，”他说，“通常，如果你与计算机解决这些问题相比，这些事情非常慢。这些也是我们希望未来深度学习做的事情。”**

**Bengio [认为](https://bdtechtalks.com/2019/12/23/yoshua-bengio-neurips-2019-deep-learning/)系统 2 DL 将能够归纳为“不同的数据分布”，这被称为[无序分布](https://ai.googleblog.com/2019/12/improving-out-of-distribution-detection.html)。目前，DL 系统需要在具有相同分布的数据集中进行训练和测试，这响应了[独立同分布数据](https://en.wikipedia.org/wiki/Independent_and_identically_distributed_random_variables)的假设。“我们需要能够应对这些变化并不断学习的系统。”使用非统一的真实世界数据，系统 2 DL 将会成功。**

**为此，我们需要具备更好的迁移学习能力的系统。本吉奥[认为](http://www.iro.umontreal.ca/~bengioy/AAAI-9feb2020.pdf)注意力机制和元学习——学会学习——是系统 2 认知的基本组成部分。这里有一段引言，[经常被误认为是达尔文的作品，](https://quoteinvestigator.com/2014/05/04/adapt/)概括了达尔文关于物种起源的名著*的中心思想，并强调了在不断变化的世界中学会适应的重要性:***

> **“幸存下来的不是最强壮的物种，也不是最聪明的物种，而是最能适应变化的物种。”**

# **基于神经科学的深度学习**

> **"人工神经网络只是大脑工作方式的粗略类比."**
> 
> **—大卫·苏西洛，谷歌大脑小组**

**在 1950 年的十年里，几项重要的科学突破为人工智能的诞生奠定了基础。神经学研究发现，大脑是由“要么全有要么全无脉冲”的神经网络组成的。这一发现，连同来自[控制论](https://mitpress.mit.edu/books/cybernetics-second-edition)、[信息论](https://pure.mpg.de/rest/items/item_2383162_7/component/file_2456978/content)和[艾伦·图灵的计算理论](https://londmathsoc.onlinelibrary.wiley.com/doi/abs/10.1112/plms/s2-42.1.230)的理论描述，暗示了创造人工大脑的可能性。**

**人工智能起源于人脑，但今天的 DL [并不像它那样工作](https://verzeo.com/blog-artificial-neural-network-vs-human-brain)。我已经巧妙地提到了 DL 系统和人脑之间的一些差异。CNN 不像我们的视觉系统那样工作。我们观察这个世界，而不是从标记的数据中学习。我们将自底向上的处理与自顶向下的符号操作结合起来。我们进行系统 2 认知。人工智能的最终目的是建立一个可以模拟我们的电子大脑，一个人工通用智能(有人称之为[强人工智能](https://www.cambridge.org/core/journals/behavioral-and-brain-sciences/article/abs/minds-brains-and-programs/DC644B47A4299C637C89772FACC2706A))。神经科学可以帮助 DL 向这个目标迈进。**

**一个重要的方法是神经形态计算，这是指模拟大脑结构的硬件。正如我在上一篇文章中写的[，“生物和人工神经网络之间有很大的区别:大脑中的神经元以尖峰脉冲的时间和频率携带信息，而信号的强度(电压)是恒定的。人工神经元则完全相反。它们携带的信息只是输入的强度，而不是时间或频率。”神经形态计算正试图减少这些差异。](/5-reasons-why-i-left-the-ai-industry-2c88ea183cdd)**

**人工神经元的另一个缺点是简单。它们是假设生物神经元是“[基础数学](https://bdtechtalks.com/2020/01/20/neuroscience-artificial-intelligence-synergies/)的哑计算器”而构建的然而，这与事实相去甚远。在[发表在*科学*上的一项研究](https://science.sciencemag.org/content/367/6473/83)中，一组德国研究人员表明“单个神经元可能能够计算真正复杂的功能。例如，它本身可能能够识别一个物体。”**

> **"也许你在单个神经元(大脑)中有一个很深的网络."**
> 
> **IMBB 的约塔·波伊拉齐**

**DeepMind 首席执行官兼联合创始人戴密斯·哈萨比斯在发表于 *Neuron* 的论文中表达了[利用神经科学推动 AI 前进的重要性。除了我已经讨论过的一些想法之外，有两个关键方面很突出:直觉物理学和规划。](https://www.cell.com/neuron/fulltext/S0896-6273(17)30509-3?_returnURL=https%3A%2F%2Flinkinghub.elsevier.com%2Fretrieve%2Fpii%2FS0896627317305093%3Fshowall%3Dtrue)**

**[James R. Kubricht 和他的同事](https://www.cell.com/trends/cognitive-sciences/fulltext/S1364-6613(17)30126-2)将直觉物理学定义为“人类理解物理环境并与经历动态变化的物体和物质相互作用的能力的基础知识，至少对观察到的事件将如何展开做出近似预测。”DL 系统做不到这一点。他们不在这个世界上，他们没有具体化，他们缺乏进化的包袱，这让我们有优势在周围导航。乔希·特南鲍姆正致力于将这种能力灌输给机器。**

**[规划](https://thegradient.pub/when-ai-plans-ahead/)可以理解为“决定需要采取什么行动来实现既定目标的探索。”我们每天都在这样做，然而，对于机器来说，现实世界太复杂了。 [DeepMind 的 MuZero](https://deepmind.com/blog/article/muzero-mastering-go-chess-shogi-and-atari-without-rules) 可以通过规划玩几款世界级水平的游戏，但这些游戏都有完美定义的规则和边界。**

**[著名的咖啡测试](https://web.archive.org/web/20190717071152/https://www.talkyblog.com/artificial_general_intelligence_agi/#The_Coffee_Test)提出，一个具有规划能力的 AI 应该能够进入一个普通的房子，进入厨房，获取配料并制作咖啡。计划要求我们将复杂的任务分解成子任务，但是这种能力超出了今天 DL 系统所能做到的。Yann LeCun 承认“我们不知道该怎么做。”**

**DL 可以从神经科学中汲取很多理念。如果我们想更接近智慧，为什么不看看我们唯一的例子呢？正如戴密斯·哈萨比斯所说，**

> **“有这么多的利害关系，神经科学和人工智能领域走到一起的需求现在比以往任何时候都更加迫切。”**

# **结论**

**DL 系统非常有用。在过去的几年里，他们单枪匹马地改变了技术领域。然而，如果我们想要创造真正智能的机器，DL 将需要一个质的更新——拒绝越大越好的观念。**

**今天有几种方法可以实现这一里程碑:摆脱 CNN 及其局限性，摆脱带标签的数据，将自下而上与自上而下的处理结合起来，向机器灌输系统 2 认知，并从神经科学和人类大脑中汲取思想和进步。**

**我们不知道实现真正智能系统的最佳途径是什么。用 Yann LeCun 的话说，“没有人有一个完全好的答案”然而，我希望我们最终会到达那里。**

****推荐阅读****

**</5-reasons-why-i-left-the-ai-industry-2c88ea183cdd> **