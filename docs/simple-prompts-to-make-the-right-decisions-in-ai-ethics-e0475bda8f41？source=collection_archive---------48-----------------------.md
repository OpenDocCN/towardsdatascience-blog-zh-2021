# 人工智能伦理中做出正确决定的简单提示

> 原文：<https://towardsdatascience.com/simple-prompts-to-make-the-right-decisions-in-ai-ethics-e0475bda8f41?source=collection_archive---------48----------------------->

## [变更数据](https://towardsdatascience.com/tagged/data-for-change)

![](img/094a2150d555e67f540b5a3eeb738d3d.png)

Paul Skorupskas 在 Unsplash 上拍摄的照片

随着最近结束的 [FAccT 2021 会议](http://facctconference.org/2021/)，我们享受到了许多论文和研讨会，这些论文和研讨会讨论了统计技术、交叉讨论、跨学科想法以及将负责任的人工智能付诸实践的许多不同尝试。Twitter 上有许多有趣的讨论，有助于从会谈、小组讨论和演示中挖掘出更多的见解，这无疑帮助我提升了自己对人工智能伦理的理解，我鼓励每个人都去看看。

但是，在一切结束的时候，仍然有一些东西是缺少的。

> *我经常被这样一个问题困扰:“如果我们能让在人工智能伦理中做出正确的决定变得更容易，会怎么样？”*

在与一些 HCI 专家和从业者交谈时，我提出了以下 3 步方法，我认为这将有助于我们解决上述问题提出的核心问题。我的目标是让做*从人工智能伦理的角度来看是正确的事情成为默认和更容易的选择。*

# 为什么

从组织的文化角度来看，使命应该清晰地表达出来，并融入员工的日常工作中。这不仅仅是将使命宣言写在海报或宣传材料上，而是员工在日常工作中可以在内心深处产生共鸣的东西。

拥有这些将为每个从事这项工作的人提供一个强有力的“为什么”,这将帮助他们(当然有正确的个人道德指南针)每次都选择正确的事情去做，而不必通过审计、审查、委员会和其他治理框架被迫这样做。

这是令人向往的，但希望这些机制成为我们很少需要依赖的东西，因为每个人都是一致的，并被激励去做正确的事情。

# 怎么做

从纯组织和运营的角度来看，我发现当有人努力将这些想法带给他们的同事，以消除摩擦的方式(嗯，至少尽可能地)时，这是很了不起的。

我经常提到微软的包容性设计实践，这使得包括可访问的设计元素(比如隐藏式字幕)变得非常容易，因为它们在所有员工的工作流程中以一种良好集成和可访问的方式呈现。从 HCI 的角度来看，这也需要深思熟虑的努力和理解，以使包含可访问性元素成为一种默认，从而提高它们的接受度。

您也可以开始考虑在员工的工作流程中使用简单的提示，例如在采购周期中(比如，在您的内部工具中列出供应商名单时)，第三方数据会要求采购人员停下来，考虑他们是否已经与领域专家交谈过，以了解他们正在采购的数据的局限性。或者，考虑一个简单的提示，询问你的 UX 设计师在提交草稿时是否考虑了残疾人的需求。

简单的提示有巨大的力量,以一种低摩擦的方式，将组织内的行为推向更道德的姿态。

# 什么

这涉及到实际的工具考虑。想想这些提示的位置，它们有多持久，它们有多突出，它们什么时候显示，它们显示的背景是什么，等等。，这是 HCI 从业者在开发这些提示时的常规考虑。

我的建议是从小处着手。如果您无法控制所使用的工具，并且无法改变核心功能，那么总会有创造性的方法来解决这个问题。想象一下，您可以构建一个 Slackbot，它在您的组织的设计频道中读取消息，并在读取“准备提交的草稿”或其他短语时，向团队成员发送消息，要求他们考虑他们是否已将残疾需求纳入他们的设计中。这些小步骤是一个很好的开始方式，然后随着时间的推移在整个组织中规范实践。

# 我们一直不知道什么是正确的事情…

我同意！这一点并不总是为人所知，但是如果我们坚持上面的三个步骤，我们就能到达一个地方，在那里我们有能力走出去寻找正确的事情而不会受到影响。希望有一种足够热情和友好的文化，能够包容相互矛盾的思想和观点，通过仔细的思考，共同帮助达成正确的事情。

如果你已经尝试过类似的东西，并且有什么样的结果，我很乐意在评论区听到你关于如何应用它的想法。

> 在这里注册以获得关于[可操作人工智能伦理书](https://actionableaiethics.substack.com/)的更新！你可以在这里了解更多关于我的作品[！](https://atg-abhishek.github.io/)
> 
> 你可以通过订阅[人工智能伦理简报](https://brief.montrealethics.ai/)从[蒙特利尔人工智能伦理研究所](https://montrealethics.ai/)了解更多这样的见解！