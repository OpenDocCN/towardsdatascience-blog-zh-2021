# 人工智能系统中的偏见及其解决方法

> 原文：<https://towardsdatascience.com/bias-in-ai-systems-and-efforts-to-fix-it-318798f5b39f?source=collection_archive---------35----------------------->

## 理解人工智能系统中的种族和性别偏见是如何发生的，以及如何解决它们

![](img/63da985053d76f3ce974d88f59a44bd6.png)

杰西卡·菲利西奥在 [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral) 上的照片

2019 年 4 月，纽约大学的 [AI Now Institute 发布了一份关于人工智能系统中偏见的影响的报告](https://ainowinstitute.org/discriminatingsystems.pdf)。大约 12 个月后，你可能还记得当一个人工智能模型将巴拉克·奥巴马的像素化图像变成这位前总统的更高分辨率图像时，Twitter 引起的风暴。[除了当模型得到一个黑人的像素化图像时，它返回一个白人的高分辨率图像。这是 AI Now 研究所在 NYU 报告中详述的偏见的一个典型例子。随着我们越来越多地将人工智能系统融入我们的日常生活，我认为描述这些问题并就如何解决这些问题展开对话是至关重要的。在我们了解这些偏见是如何产生的以及需要采取什么措施来解决它之前，我们先来快速了解一下有监督的机器学习算法是如何工作的。](https://www.vice.com/en/article/7kpxyy/this-image-of-a-white-barack-obama-is-ais-racial-bias-problem-in-a-nutshell)

## 有监督的机器学习算法是如何工作的？

监督机器学习算法的目的是学习一个函数，该函数最接近输入数据和目标数据之间的关系。监督学习算法从带有期望输出的标记为的给定训练数据集*中学习。一个好的算法必须是可推广的，这意味着它必须能够根据在训练数据中学习到的内容来准确预测新数据的输出。如果输入的新数据包含以前在训练数据集中没有看到的特征，人工智能将很难对这些新数据进行分类。如果这一切听起来有点抽象，这里有一个简单的例子。假设您想要训练一个 ML 模型来分类照片中是否有马或狗。在训练数据集中，你给它提供了成千上万张带有标签的吉娃娃和种马的图片。如果你给你的模型看一张大丹犬的图片，它很可能会误认为是一匹马。该模型对所有犬种不够通用，因为它没有用其他犬种的图像进行训练。*

## ML 模型中偏差是如何产生的？

![](img/8e5619fc5c12aaf26b03c8431ceceb20.png)

马库斯·斯皮斯克在 [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral) 上的照片

简单的答案是，有偏差的 ML 输出是由于有偏差的训练数据而产生的。当然，数据本身并不存在固有的偏差，人工智能中出现偏差的真正原因是，负责提供训练数据的人存在偏差，或者未能注意到他们数据中的偏差。

未能注意到有偏差的数据是一个微妙的概念，但会导致与不小心收集有偏差的数据一样糟糕的结果。[看看亚马逊 2015 年的招聘工具就是一个经典例子。](https://www.reuters.com/article/us-amazon-com-jobs-automation-insight/amazon-scraps-secret-ai-recruiting-tool-that-showed-bias-against-women-idUSKCN1MK08G)该工具旨在更好地自动选择公司招聘的最佳候选人。这位模特显然是在十年前接受过 CVs 训练的。理论上，到目前为止一切顺利。简历库中应该有各种各样的人。事实并非如此，2005 年申请这些职位的人比 2015 年更加偏向男性。最终结果是该工具被发现对完全合格的女性有偏见，当事情变得明显时，亚马逊很快放弃了整个项目。就模型而言，男性是这项工作的最佳候选人，因为所有的训练数据似乎都表明了这一点。

人工智能中出现偏见的另一个原因是建立这些模型的公司缺乏代表性。以下是 AI Now 研究所报告中的一些统计数据；人工智能的教授 80%是白人男性；在谷歌、脸书和微软之间，没有一家公司的黑人员工比例超过 4%。该领域的女性人数同样惨淡，尽管推动了更多女性进入科技行业。此外，该领域的人往往很富有，这就带来了潜在的社会经济偏见

## 我们如何解决这个问题？

![](img/0fbf9db02ce85c117b15900c541bbfc8.png)

照片由[克里斯蒂娜@ wocintechchat.com](https://unsplash.com/@wocintechchat?utm_source=medium&utm_medium=referral)在 [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral) 上拍摄

我认为，解决这个问题的第一步是承认这个问题。我在这里看到了令人鼓舞的迹象，因为人工智能专业人士和机构似乎认识到了这些问题，并正在努力解决这些问题。应对人工智能系统中的偏见的另一个步骤是，在开发和分发系统的公司的所有级别中，纳入受到偏见的人。对这些公司领导职位的需求有偏见的团体。一些科技公司的少数群体、女性和残疾人的数量似乎在朝着正确的方向发展，但不清楚这些人中有多少人能够做出影响其产品偏见的决定。

## 结束语

像我们社会的许多领域一样，我们在人工智能系统中看到了不可接受的偏见。这是一个需要尽快解决的问题，因为这些系统融入了我们生活的更多方面，公司使用它们来做出影响整个社区的决策。令人鼓舞的是，AI Now Institute 等人工智能研究机构认识到了这个问题的严重性，并致力于解决这个问题。现在，科技公司有责任在领导岗位上增加女性、有色人种和残疾人的代表，并赋予他们权力，成为解决人工智能偏见的一部分。

我将留给你们一段我认为是来自艾安娜·霍华德教授和 T2 教授讨论人工智能种族偏见的有力的引用。

> “有时有色人种在场，而我们没被发现。其他时候我们失踪了，但是我们的缺席没有被注意到。这里的问题是后者。”