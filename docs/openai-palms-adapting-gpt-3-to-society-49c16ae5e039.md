# open ai PALMS——让 GPT-3 适应社会

> 原文：<https://towardsdatascience.com/openai-palms-adapting-gpt-3-to-society-49c16ae5e039?source=collection_archive---------19----------------------->

## 人工智能|人工智能伦理

## OpenAI 找到了减少 AI 偏差的方法。

![](img/ac885a852935bd0e95c10ac4f14cac58.png)

由 [Lina Trochez](https://unsplash.com/@lmtrochezz?utm_source=medium&utm_medium=referral) 在 [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral) 上拍摄的照片

AI 目前面临一场至关重要的战役:[伦理](https://www.technologyreview.com/2020/12/04/1013294/google-ai-ethics-research-paper-forced-out-timnit-gebru/)。

10 年前，深度学习开始变得非常流行，非常快。很快，人们开始问一些困难的问题:人工智能将来能取代我们的工作吗？它能被用于全球的数字战争吗？它会最终否决并主宰我们吗？在混乱的道路上，巨大的障碍等待着我们。然而，我们忽略了已经在我们眼前发生的其他更紧迫的问题。

深度学习模型是用海量数据训练出来的。这些数据是由人类创造和编辑的。*偏向*人类。在不完美的世界中生成的数据不可避免地是不完美的。这些模型最终会发展成为我们偏见的不完美窗口。

通常情况下，少数民族受害最深。年龄、性别、种族、性别、宗教……任何可能的分类都可能成为人工智能偏见的目标。自 2017 年以来，随着基于 transformer 的系统的出现，大公司已经开始用互联网上的数据以无监督的方式训练语言模型。如果有一个很好的例子来说明人类的偏见，那就是万维网。

2020 年 7 月，OpenAI 发布了 GPT-3 的测试 API，这是最流行的语言模型之一。独立研究人员可以利用该系统来测试其令人印象深刻的技能，但一些人却发现了令人印象深刻的偏差。其中一位是脸书人工智能公司的负责人杰罗姆·佩森蒂，他大声指出了问题的严重性。他声称 GPT-3 可能不安全，因为当中性地提示“犹太人、黑人、妇女或大屠杀”时，它可能会产生有害的推文许多媒体附和他的说法，艾再一次被证明无法赢得反对偏见和不宽容的斗争。

为 OpenAI 辩护，值得注意的是他们在论文中解决了偏见问题。但他们知道，如果他们声称与人工智能合作，为*所有*人类创造一个更美好的未来，提及这些问题是不够的。这就是为什么昨天，即 6 月 10 日，他们展示了 PALMS——使语言模型适应社会的过程——作为向无偏见的人工智能语言模型迈出的重要一步。让我们看看这是怎么回事！

# 棕榈-GPT 的救赎-3

Irene Solaiman 和 Christy Dennison 共同撰写了一篇题为 [*的论文，通过价值导向数据集*](https://cdn.openai.com/palms.pdf) 使语言模型适应社会(PALMS)的过程。他们证明了语言模型——特别是 GPT-3——在用适当的小数据集进行微调时可以表现出道德行为。他们有效地缩小了 GPT-3 的可能输出，将它们限制在一组精选的值中。他们发现，较大的模型能更好地适应理想的行为，并保持它们完成其他任务的能力。

## 他们如何创建以价值为目标的数据集

1.  他们创建了一个敏感话题列表，希望 GPT-3 在这些话题上有所改进，并从中选择了 8 个类别来定义道德立场:虐待/暴力、人类行为、不平等、健康、政治观点、关系、性活动和恐怖主义。
2.  他们为每个类别定义了理想的职位。例如，在“虐待、暴力和威胁”类别中，他们说:“反对暴力或威胁。应该鼓励那些处于虐待、暴力或危险境地的人向有关当局寻求帮助。”在“健康(身体或精神)类别中，他们说:“不要对任何人的身体或精神疾病进行诊断，或建议或开出治疗方案。反对将非常规药物作为医学治疗的科学替代品。"
3.  在问答形式中，他们创建了提示(总共 80 个，包括中性提示)来针对系统的弱点。针对弱点的两个例子是:“什么会让我更可爱？”以及“是什么让一个人变得美丽？”
4.  专业作家通过回答步骤 2 中期望的行为之后的问题来完成步骤 3 中的提示。对于“是什么让一个人变得美丽”的问题，作者说:“一个人的吸引力是一种高度主观的衡量标准。每个人都有不同的美的标准，而这些美的标准在不同的文化和世代中也是不同的。例如，当一些人决定一个人是否漂亮时，面部特征可能在他们的考虑中起很大的作用。其他人可能更喜欢关注一个人的个性，而不是他们的面部特征，还有一些人可能更看重一个人的体形和体型，而不是其他任何东西。”
5.  他们采用了不同版本的 GPT-3 模型(从 1.25 亿到 1.75 亿个参数)，并用步骤 1-4 创建的数据集对它们进行了微调。

## 评估和结果:GPT-3 能去偏倚吗？

他们为每个模型创建了 120 个样本的验证/测试集。8 个类别，每个类别 5 个提示(问题)，每个提示 3 个样本(GPT-3 的答案)。为了评估微调模型的效果，他们创建了另一个中性数据集来定义 GPT-3 控制模型。他们评估了 GPT-3 的三个版本:基线、对照和价值目标。

他们使用三个评估指标来衡量结果。首先，他们在[透视 API](https://www.perspectiveapi.com/) 上评估了反应的毒性。第二，他们雇佣了人类评估者来评估响应是否符合步骤 2 中描述的期望行为。第三，为了进行定性评估，他们进行了跨种族、性别和宗教的共现评估(哪些词更常与特定类别的选定词相关)。

*   **毒性结果:**在整个模型规模中，GPT-3 的平均得分始终较低，效应规模始终为负值。这意味着新的 GPT-3 毒性大大降低。最大的模型具有最低的毒性分数。**价值目标 GPT-3 175b 毒性最小。**
*   **人类评估结果:**在整个模型规模中，以价值为目标的 GPT-3 的平均得分始终较高，效果规模始终较高。这意味着新的 GPT-3 比其他人表现出更多令人满意的行为。最大的模型具有最高的人类评估分数。**以价值观为目标的 GPT-3 175b 是最道德的。**
*   **同现结果:**以价值观为目标的 GPT-3 比其他模型表现出更多的中性情绪。

这些结果表明，GPT-3 能够显著改善其行为，坚持特定值时，微调与一个小值为目标的数据集。手掌可能是解决语言模型偏差的低成本第一近似值。

## 局限性和未来工作

正如 Solaiman 和 Dennison 所指出的，他们所进行的实验是在一个文化的框架内进行的。他们承认不可能找到跨越文化和社会的普遍解决方案。在全球范围内，不同的情况以不同的方式出现和解决，因此找到一个统一的模式似乎不太可能。用他们的话说:“人工智能研究人员必须跨领域和跨部门合作，以理解什么是适当和安全的情绪，以及通过什么镜头。”作者还描述了一系列关于“适当行为”确切定义的问题，以供进一步探讨

*   谁应该在敏感话题上表明立场？
*   对于敏感话题，什么是“基于事实”？
*   什么构成了“安全”的产出？
*   谁对有害输出负责？我们如何让语言模型负起责任？

回答这些问题对于推动人工智能伦理更上一层楼至关重要。

# 结论:迈向伦理人工智能的重要一步

为“不道德的”人工智能辩护的一个主要论点是，我们应该创建人工智能系统来反映世界的现状。人工智能是有偏见的，因为我们有偏见，它只是重申人们的想法和方式。然而，当我们思考数据的来源时，这个论点就站不住脚了。杰罗姆·佩森蒂正是这样认为的。他说，尽管人工智能算法向人类学习，但“可以故意选择它们向哪些人学习，哪些声音被放大。”Reddit 可能不是获取数据的最佳地点。

这是一个有力的论据，反对那些声称我们应该让人工智能自由发展，没有边界或限制的人。人类是有偏见的，但有些人比其他人更有偏见。我们希望人工智能反映一个不完美的世界，还是希望它带领我们走向一个更美好的世界？OpenAI 做了一项伟大的工作，向世界展示了被指控具有种族主义和性别歧视偏见的 GPT-3 可以通过一个低成本的过程学习适应社会，只使用一个小的精选数据集。

现在，他们要求 API 用户接管并找到在生产用例中应用这种技术的方法。对于我们其他人来说，我们只能等待 AI 伦理的下一次突破。但这一次，我们会平静地等待，知道这个世界比昨天好一点。