# 所有机器学习如何变成强化学习

> 原文：<https://towardsdatascience.com/how-all-machine-learning-becomes-reinforcement-learning-6f085f4389f?source=collection_archive---------13----------------------->

## [思想和理论](https://towardsdatascience.com/tagged/thoughts-and-theory)

## 我解释了为什么迭代训练任何模型的人应该学习强化学习的一些核心问题。

## 思维练习

考虑这些对其模型有 ***跨时间*** *影响的 ML 问题的例子。强化学习的这种依赖于时间的功能是大多数“**核心片段**”(下)将出现的地方。*

考虑一个试图减少付费在线会员流失的系统。这样的公司可以训练一个模型来减少当前一批最易受攻击的用户(例如，那些在算法的重播缓冲区中不经常点击的用户——稍后更多)，但这可能会在下一个模型被重新训练之前以非预期用户的流失为代价。

更准确地说，这个*动作*可以采用新参数的形式，将电子邮件主题中的文本调整为电子邮件列表中的个人。这种*政策*(决定行动的模型)可以改变每个注册电子邮件地址的文本，针对各种电子邮件偏好，例如喜欢表情符号的人(许多人都喜欢)。由于任何优化都没有免费的午餐，添加表情符号不会是每个用户的最爱，这可以通过工程师在部署这种新模式后看到的*奖励*来表示。这种调谐可以自动完成。不过，这种变化可能会让那些目前最忠诚的客户产生意想不到的行为——可以说是一种流失的 [**正反馈循环**](https://en.wikipedia.org/wiki/Positive_feedback) 。

另一个例子是**建议**。我是一个经常看 YouTube 的人，随着时间的推移，我已经看到我的兴趣在更广泛的网络上，在现实世界中，特别是 YouTube 上发生了变化。YouTube 算法已经表现出了类似强化的行为(除了关于他们用于调整内容选择的 RL 算法有多普遍[的争论之外)。简单的行为是，我点击了一个我已经几个月或几年没有浏览过的话题，第二天我关于这个话题的订阅量就会大幅增长。我怀疑谷歌的模型要么有一个权重很大的历史术语，要么它允许其推荐引擎的参数快速变化。这种反馈与我在 YouTube 之外的兴趣演变无关，所以这是应该更深入考虑的事情。你想让算法定义你是谁吗？](https://dl.acm.org/doi/pdf/10.1145/3289600.3290999?casa_token=MJrmS7SMrecAAAAA:D6DrDx_MrVGy98Ct9_YCCN_5LfYdfC_1pSBr50BY9_2JzjXG5ROeBRP5x-M6FSHngTlP1GEALKdtSQ)

这里的推荐和流失算法显示了反馈的清晰概念，这是一种效果，其中你未来的状态和动态随着时间的推移而演变，并且取决于你当前(和最近)的状态和行动。**强化学习** (RL)是围绕未知世界制定的机器学习领域，在未知世界中，我们关于如何改进任务的唯一信号是从世界反馈给我们的嘈杂信息。这种表示通常是通过*奖励函数*实现的，但也可以推广到许多信号，这些信号会传播回我们经常使用的应用程序，以告知它们如何**调整** **我们的行为。**

我并不是想说知道 RL 会很容易地减轻应用程序内外用户行为的任何明显变化。我是说，通过研究 RL，可能更容易诊断面向人的 ML 工具可能的长期/时变影响。监督学习被框定为一个实例的工具，因此随着现实世界的不断变化，所有模型都过时了——问题是过时了多少？

# 更广泛的背景

大型科技公司正在**反复**部署大型*模型*，从全球收集*新数据*，再*重新训练*他们的解决方案。在工程师的理想中，一旦环境动作发生(通常通过用户)，所述模型将根据新数据进行改进。这听起来很像**强化学习** (RL)，其中将有一个模型将寻求改进的设定指标。如果工程师不让模型完全控制它的动作(稍后将详细介绍)，这听起来可能不像强化学习，但 RL 的**反馈环路**核心就在那里，它提供的信号将影响系统。这些信号可以在较长的时间范围内复合并偏置系统。

RL 的理想概念在许多地方都有所欠缺。考虑这种常见于概念 RL 教学的框架(如加州大学伯克利分校的[人工智能简介](https://inst.eecs.berkeley.edu/~cs188/sp20/)):*代理与世界交互，收集更多数据，并在其下一次推出之前更新其策略*。这对在电脑上玩玩具物理问题的年轻研究人员有效，但对构成大部分互联网流量的系统无效。这些站点有不同的范例:*一个部署的模型为大量用户提供一个任务，新的数据被收集，几个小时到几天到几个星期后，一个新的模型被训练和部署*。这个循环中的模型训练步骤可能需要难以置信的大量计算，因为数据集上的计算很容易超过一百万个样本。

可以在应用环境中使用 RL 的系统的一个很好的例子是任何形式的**推荐系统**。想象一下，某种新闻提要决定了它应该为您提供哪些内容。这很容易采取从当前用户状态(时间、位置、最近的点击)转换为动作(内容)的策略的形式。

深度学习模型也在网上完成许多其他任务，包括广告投放、文本分析、图像识别等。这些严重影响用户体验的模型中的任何一个都将改变它们和它们的对等模型在未来被训练的结果数据集。例如，通过部署更有用的文本分析模型，手机的搜索引擎可能会变得更好，从而减少阅读结果所花的时间和加载的定向广告。所有这些模型都在不断地反馈关于累积系统优先考虑什么的信号:*这些* *信号决定了模型的下一次迭代将稍微优先考虑什么*。

缩小来看，这完全是一个定义松散的多主体强化学习系统的大混乱。许多政策已经部署，但离独立还差得很远。

# RL 结构的核心部分

即使 RL 的框架发生了变化，它的一些结构仍然存在。这些关键部分意味着 RL 研究的某些核心概念和关注点将出现在这些应用循环中。它们在多大程度上出现有待讨论和发现。鉴于 RL 已经被用来解决一些世界上最难的游戏，并且容易出现意想不到的行为，我们应该让研究过它的人监控这些系统。**大规模的 RL 可能会产生巨大的影响，因此其潜在的风险也相应增加。**

需要了解的 RL 的一些核心方面包括:

*   **政策脆弱性**(遗忘和不稳定性):众所周知，RL 算法的多次尝试的性能非常不稳定——在一次迭代中解决了任务的代理可能在下一次迭代中做不出任何智能的事情，现在只有几个数据点和梯度步骤不同。
*   **政策利用**:由于奖励函数定义的狭窄性，RL 经常[表现出无意识的行为](https://robotic.substack.com/p/rl-exploitation)。
*   **反馈**和**奖励**信号:RL 是围绕反馈到学习系统中的代理或系统的交互而构思的。它被设计为**试错**学习。
*   **神秘性**:众所周知，RL 很难调试，并且容易出现数值问题。这给公司带来了风险(但在我描述的伪 RL 系统中，这种情况可能不太严重)。

这些特征中的大多数在我的公式中合并，因为 RL 随时间而动作，而我们使用的许多其他 ML 系统是在静态场景中训练和使用的。

很多人把 RL 看作是一种扔给玩具模拟问题的黑盒算法。这是研究通常看起来的样子，但是那里学到的经验教训适用于任何迭代学习任务，其中新数据以迭代的方式反馈。在这种情况下，applied ML 变成了广义的 RL，在这里我们训练一个新的模型，而没有完全理解它在测试时的表现。这些测试为我们提供了在下一次迭代中使用的有价值的新数据。

![](img/23702b117d524f512ef5c6b254c764c0.png)

照片由[Tine ivani](https://unsplash.com/@tine999?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)在 [Unsplash](https://unsplash.com/s/photos/loop?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText) 上拍摄

# 走出教室

当 RL 脱离一个完全概念上受控的环境时，两个关键的假设就失效了。本节针对的是这个想法: ***如果说*** 大部分应用的 ML 系统呈现出 RL、 ***的特征，那么*** 最大的差异可能在哪里？

通常研究的 RL 环路和这些公司发生的事情的第一个区别:**时标移位**。由于大多数模型部署基础设施千差万别(甚至没有考虑大多数技术问题在范围、重要性和解决方案方面的差异)，这些应用程序离强化学习的共同概念有多远也有不同的程度。规模较小的公司可能数据相对较少，所以可以每小时训练一次。许多公司可能希望每小时培训一次，但是他们可能缺少工具来完成。获取数据集、从中生成和建模，并将其发送给面向用户的堆栈非常复杂。

影响时标转换的两个核心组件是 1)数据量和 2)模型基础设施。这两者可以以相同的方式影响新数据的反馈:更长的模型更新延迟(假设所有相同的数据都被使用)给当前模型更多的时间来影响下一个模型的整体训练集。时间尺度甚至可以进一步解释为模型选择新动作的速率。

接下来，实验室中的 RL 研究与现实世界的一个核心区别:**分布转移**。在研究中，分布转移至多扮演一个次要角色。大多数任务都假设独立同分布(iid)数据，但在现实世界中，这种分布会随着时间的推移而变化。新的数据总是会出现，而手头任务的最佳解决方案很容易随着时间的推移而改变。

RL 系统的核心部分是重放缓冲器的概念，这是一种先进先出(FIFO)缓冲器，用于存储世界上最近的转换。TL；重放缓冲区上的 DR 是用于存储最近转换的数据结构— RL 代理使用重放缓冲区中的样本进行计算。关于重放缓冲器的更多信息，这篇[论文](https://arxiv.org/pdf/2007.06700.pdf)是关于它们如何影响 RL 的现代观点。

使用正确大小的重放缓冲区对于稳定的行为至关重要(太小的缓冲区会很快忘记需要的数据，而太大的缓冲区会优先处理不再相关的数据)。我怀疑这些系统中的模型，特别是考虑到它们在监督学习中的强大根基，不具有定义 RL 基线的相同的“从零开始学习”性质，例如机器人中的那些。这就引出了一个更加开放的问题，即系统如何处理数据移位。

这里的核心思想是现实世界是会变的。它不是一个模拟器。可能会有类似于刷新模拟中的重放缓冲区的效果，但是来自模拟的数据实际上总是来自同一个系统，这对于解决给定的任务来说并不是最优的。拥有不再代表你的世界真实情况的数据可能会以一种更加有害的方式破坏学习系统。

RL 的概念化和它在实践中的使用之间的差异是很难确定的，那就是动作空间的尺度。内容推荐或互动模型比机器人任务要复杂得多。如果一个人在深度学习中保持一定的信念基线，那么 RL 的应用应该跨领域过渡而不失一般性。我听说的经验法则是，使用 RL，某个模型或策略的数据需要与输入或操作的数量平方(以及其他因素)成比例…因此这些应用系统将需要它们生成的所有大数据。

# 工业 ML 环路

在不同的技术领域，公司实际部署的内容和方式有很大不同。例如，像谷歌和脸书这样的大玩家已经建立了他们自己的模型训练、部署和监控栈。与此同时，有很多很多初创公司试图构建工具来改进模型部署堆栈的各个部分，如模型部署、更新、效率等。(这不是我最了解的领域)。

这些大型工具不能在黑盒中运行。它们极大地影响着我们的个人和社会行为。这种影响会导致公司所依据的基础数据发生变化。*公司等待更新模型的时间长短会使模型行为中的某个错误在用于下一个训练循环的数据中变得更加普遍* — **强化**。

同样重要的是要记住，谷歌和脸书的经营规模远远超出了人们的心理承受能力。人类不可能筛选、注释或理解他们收到的所有数据。毫无疑问，每秒钟都有许多意想不到的行为发生。大型数据集还需要更多的时间来训练策略，因此更新每个样本实际上是不可能的。即使有这样的规模，也有巨大的推动力推动快速发展，因为用户的注意力在数字服务中高度竞争。

我听说过两个在模型训练和部署方面的工业失误的例子，当然还有更多的例子(注意:这些例子的准确性可能并不完美，在与应用 ML 领域的其他人交谈后，我找不到清楚描述它们的博客帖子)。这些不是来自 RL 部署，但是考虑这些问题如何转化为 RL 范例是很酷的:

1.  *早期* [*A/B 测试*](https://en.wikipedia.org/wiki/A/B_testing) *止于* **脸书**(也可参见 Stich Fix 的这篇[帖子](https://multithreaded.stitchfix.com/blog/2020/08/05/bandits/)——他们有一个很棒的数据博客和可能的团队):A/B 测试用于确定哪个特性变化是最好的。以 ML 为例，一个公司可以使用两种模型，并确定哪一种是最好的。这些测试运行一段固定的时间，以帮助避免数字困难(如 P 值)，但脸书的工程文化有一个习惯，当它们看起来很明显时，就提前停止测试。这加强了工程师的偏见，更重要的是，对这篇文章来说，他们认为模型“更好”根据这一过程持续的时间以及他们将结果反馈到新模型的时间尺度，不同的 RL 属性可能会出现。
2.  *在* **NextDoor** 丢失模型训练集:听说有一段时间 NextDoor 的工程师不知道哪个模型产生了哪些数据。实际上，这意味着他们没有足够详细地存储模型训练属性，这可能与一堆野生潜在框架有关。例如，这可能类似于没有策略检查点的 RL(保存每个训练步骤的参数，以及导致它的重放缓冲)。任何在 RL 工作的人都知道，对于任何具有挑战性的问题来说，这无异于自杀。
3.  这就像重新训练一个策略并丢弃所有过去的策略，或者保留一个可能被新策略损坏的训练数据集(不容易知道何时损坏)。从希望跟踪峰值行为的 RL 工程师的角度来看，这两种选择都是可怕的，因此他们可以返回并从那里重新分叉策略。如果不这样做，就会存在巨大的偏差和模型利用的可能性，因为在工程师注意到之前，无意的行为可能会在底层数据集中得到加强。

问题是，NextDoor 不可能是唯一一个犯这些错误的人。许多工程师偷工减料，了解您系统中的模式将使您更容易回到期望的操作。

![](img/23b821c863f3e0260bfdad27ede436a0.png)

来源，作者。

我们讨论的 RL 范例:

1.  **Coursework RL** : *一个智能体与世界互动，收集更多的数据，并在其下一次推出*之前更新其策略。
2.  **将 ML 归约应用于 RL** : *一个部署的模型为成吨的用户服务一个任务，新的数据被收集，几小时到几天到几周之后一个新的模型被训练和部署*。

让我知道，如果你看到了任何可能看起来像这样的东西，它可能会有所不足，或任何东西。这是我一直试图解决的一个想法，但它似乎不可避免地会在某种程度上发挥出来。如果没有，那么我们将不得不把 RL 做得非常好，让每个人都直接使用和认可它。

在这种情况下，让 RL 工作的重要一点是批量 RL 领域的不断发展，批量 RL 设计为以我所讨论的应用程序的较慢更新频率方式运行。它获取最新的所有任务数据，并提取新策略——这种策略提取可以在部署工程师需要的任何时候进行。

*这个原本出现在我的自由子栈* [***民主化自动化***](https://robotic.substack.com/) *。请查看或关注我的* [*推特*](https://twitter.com/natolambert) *！*