# 算法可以种族歧视吗？

> 原文：<https://towardsdatascience.com/can-algorithms-be-racist-6dddf8d69065?source=collection_archive---------18----------------------->

## 语言模型中法律和社会困境的简要探讨

![](img/89dfc44c7d48830a77d886fba38a8ce8.png)

由[马库斯·斯皮斯克](https://unsplash.com/@markusspiske?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)在 [Unsplash](https://unsplash.com/s/photos/racism?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText) 上拍摄的照片

随着人工智能(A.I .)继续快速融入日常生活，一些伦理困境也同步出现，它们对用例的影响成为了许多辩论的主题(Kilbertus 等人，2017；哈特等人，2016；Pazzanese，2020)。本文关注的一个困境与包容性和边缘化有关(Bender 等人，2021)。在算法模型的形成中，强化霸权力量的训练数据如何影响参与的概念？因此，本文将通过对文献进行定性调查以讨论偏见放大而获得的基础解释主义观点，来寻求强调人工智能中的伦理挑战。正如 Bender 等人(2021)所概述的，关于语言模型的发展和利用，存在几个法律和社会困境。这包括强化对边缘化人口的霸权力量，这是具体化力量不平衡的训练数据特征的结果。

富裕国家从语言模型中受益更多，而贫穷国家则受到训练数据的负面环境影响，而这些数据对它们几乎没有好处(Pratik et al .，2020)；他们也在很大程度上被排除在技术进步之外或代表性不足。在解决这些问题时，Bender 等人(2021)建议仔细规划和管理数据集，以期采用价值敏感的设计方法，从而减少语言模型不准确以及进一步征服某些群体的定型观念的永久化所造成的负面影响。

此外，建议在构建模型时建立基准，通过预记录严格审查数据收集和使用情况，以提高包容性，确保公平性并鼓励准确性。参与的概念也受到训练数据的负面影响，这些数据的背景和制度基础是深度学习中的偏见。例如，美国惩教罪犯管理概况分析替代制裁(COMPAS)中使用的算法对非裔美国人有固有的偏见，因为它给予这一群体比高加索人更高的风险评级，即使两者都有确切的罪犯概况(Mehrabi 等人，2019 年)。Obermeyer 等人(2019)引用了一种在美国医院中广泛使用的流行算法，该算法减少了黑人在与其他群体相同的复杂健康情况下访问医疗计划的可能性。

这种在深度学习算法模型的形成中霸权力量的加强突出了人工智能中的偏见放大，以及对识别、根除和丢弃歧视性训练数据的严格检查的需要，无论是在数据收集、预处理、处理中还是后处理阶段(Mehrabi 等人，2019)。计算机视觉是人工智能中另一个存在霸权结构的领域。Buolamwini 等人(2018 年)强调了分析人脸的算法中的性别和种族问题:肤色较浅的男性比肤色较深的女性更少出现任务错误。同样，算法模型如何以一种形成标准自然智能的方式安排视觉世界，取决于它接收的数据集，这些数据集通常不具有包容性。

例如，在谷歌上快速搜索冠状病毒图片，会显示大量彩色分子的图片，而搜索埃博拉病毒，会显示各种非洲人处于虚弱状态的图片。尽管冠状病毒在全球范围内产生了更大的负面影响，但网上的图片并没有反映出一个群体比另一个群体更糟糕。人们可以提出这样一个问题:谁来决定应该看什么和如何看？谁来决定现实世界和虚拟世界是如何协商的？因此，如果计算机视觉有能力塑造感知，从而塑造行为和思想，那么，正如 Pereira (2021)所建议的，有必要从根本上摆脱这些歧视性的概念。但是如何做到这一点呢？在上述人工智能领域，研究人员对伦理挑战保持警惕的动机是什么？

首先，财政补贴应该给予那些在训练数据或部署模型中积极展示权力动态平衡的研究人员，也就是说，建议消除一个人口统计学对另一个人口统计学的显性或隐性优势。其次，事实上反过来说，经济支持或公共资金应该被取消，所创建的算法模型对那些在研究中不注意伦理困境的人工智能研究人员来说是一种令人厌恶的刺激。第三，应该从高质量的研究中建立一个经过同行评审的标准化和最佳实践指标分类框架，该框架应在整个研究过程中积极、一致地纳入道德规范的使用。

人工智能在主要行业中得到成功应用，因此，该领域面临的各种挑战是全方位的。考虑到这一点，如果训练算法的数据集错综复杂地局限于某些霸权原则，那么数据化过程最终将扩大这一点，进一步挑战参与的概念。因此，伦理在人工智能中至关重要，研究人员有责任确保系统地捍卫和推进是非、公平和平衡的概念。

文献学

Bender，e .，Gebru，t .，McMillan-Major，A & Shmitchell，S. (2021)论随机鹦鹉的危险:语言模型会不会太大？。关于公平性、问责制和透明度的会议。纽约，3 月 3 日至 10 日，1 日至 14 日。

J .布朗利(2019 年)。具有深度学习的多标签分类。可在网上获得:[https://machine learning mastery . com/impact-of-dataset-size-on-deep-learning-model-skill-and](https://machinelearningmastery.com/impact-of-dataset-size-on-deep-learning-model-skill-and)-performance-estimates/【访问于 2021 年 12 月 12 日】。

J .布朗利(2021 年)。如何选择深度学习的激活函数
网上有:[https://machine Learning mastery . com/impact-of-dataset-size-on-Deep-Learning-model-skill-and](https://machinelearningmastery.com/impact-of-dataset-size-on-deep-learning-model-skill-and)-performance-estimates/【2021 年 12 月 12 日访问】。

Buolamwini，j .和 Gebru，T. (2018 年)《机器学习研究会议录》81:1–15。

*2018 公平、问责和透明性别阴影会议:商业性别分类的交叉准确性差异。*纽约，2 月 23 日至 24 日，1 日至 15 日。

Doshi，S. (2019)训练神经网络的各种优化算法。正确的优化算法可以成倍地减少训练时间。可在线获得:[https://towards data science . com/optimizer-for-training-neural-network-59450d 71 caf 6](/optimizers-for-training-neural-network-59450d71caf6)【访问日期:2021 年 12 月 12 日】。

Dabbura，I. (2017)梯度下降算法及其变体。可在线查阅:https://towards data science . com/gradient-descent-algorithm-and-its-variants-10f 65280 6a 3[访问时间:2021 年 12 月 14 日]。

《监督学习中的机会均等》。*第 30 届神经信息处理系统会议，*巴塞罗纳，12 月 5 日至 11 日，3315 年至 3323 年。

图片(无日期)。可在:【https://pixabay.com/ (2021 年 12 月 13 日进入)。

kilbertus，m . Rojas-carulla，g . parascandolo，Hardt，m . janzing，D & Sch ̈olkopf，B. (2017 年)通过因果推理避免歧视。*第 31 届神经信息处理系统会议*，加利福尼亚州，2017 年 8 月 6 日，656–666。

Nguyen，H & Evangelos，T. (2021)数据挖掘中过拟合和过泛化对分类准确性的影响。路易斯安那州立大学博士论文。

Obermeyer，z .，Powers，b .，Vogeli，C & Mullainathan，S. (2019)剖析用于管理人口健康的算法中的种族偏见。*理科，366* (6464)，447–453。

Pazzanese，C. (2020)随着人工智能在更多行业中发挥更大的决策作用，伦理问题越来越多。网上有:[https://news . Harvard . edu/gazette/story/2020/10/ethical-concerns-mount-as-ai-take-bigger-de](https://news.harvard.edu/gazette/story/2020/10/ethical-concerns-mount-as-ai-takes-bigger-de)cision-making-role/【2021 年 11 月 24 日访问】。

佩雷拉，G. (2021)走向拒绝作为一个关键的技术实践:与霸权计算机视觉作斗争。安普哈 10(1)，2245–7755。

Pratik，j .，Sebastin，s .，Amar，b .，Kalika，B& mono JIT c .(2020)NLP 世界中语言多样性和包容性的状况和命运。*计算语言学协会第 58 届年会*。在线，6282–6293。

Rohith，G. (2018)提高神经网络的性能。网上有:[https://www . kdnugges . com/2018/05/improving-performance-neural-network . html](https://www.kdnuggets.com/2018/05/improving-performance-neural-network.html)【2021 年 13 月 12 日访问】。