# 重建链式法则以自动微分

> 原文：<https://towardsdatascience.com/rebuild-the-chain-rule-to-automatic-differentiation-d77afaa361c4?source=collection_archive---------23----------------------->

## 从链式法则到自动微分有一个环节。让我们跨越从数学到计算机科学的桥梁。

![](img/42ffbb7714c0c9d206f27713bdb7ec8b.png)

作者图片

神经网络很酷。不同的机器学习框架甚至更酷。您可能知道，现代神经网络是一个包含大量变量的大型公式。给定一个问题，这些框架会帮你找到一组合适的参数值，这个过程叫做“训练”。由于自动微分或自动微分，这个训练过程可以非常有效地完成。与其说是“幕后”，我倒不如说它影响了整个培训过程*在*幕后。这是整个训练过程的基础。

Auto Diff 通常是通过首先理解所需的数学知识，然后从一个空白源文件实现它来实现的。但是也有可能从链规则的定义中发展出 Auto Diff。人们应该对基础数学仍有一些了解，但看着链式法则演变成自动微分会给人一种不同的满足感。

# 自动差异

![](img/c51fb4f7ead51936eaa65c524097bb70.png)

作者图片

自动差异给了我们什么？它可以自动告诉我们几乎任何公式中的每个参数如何单独影响最终结果。“但是这有什么用呢？”，你可能会想。想象一下，你有一个幸福公式，输入是冰淇淋、煎蛋卷、书籍和睡眠的数字，输出是幸福程度的单一值。对于任何一组输入值，auto diff 可以告诉你每个输入对幸福输出的影响程度和影响方向。然后，你可以用这些新知识来改变输入，以接近你想要的幸福水平。

当然，一个更重要的应用是神经网络。输入可以由两部分组成。首先，一个固定的部分，像图像的像素值。然后我们需要第二部分，这是我们的公式与图像数据相结合的一组变量/参数。输出也可以分成两部分。第一个值对于狗可以是 0，对于猫可以是 1。最后一个值是第一个狗/猫值和输入图像的真实动物类型之间的误差。Auto diff 可以发现每个参数如何影响误差值。然后我们可以调整参数，使误差在下一次变得更小。

# 这个计划

文章的其余部分是关于写一个*基本*版本的自动微分。它以使用自动差分求解方程结束。我们将:

1.  将链规则的定义转换为代码
2.  将链式规则的简单实现转换为自动差异
3.  使用自动差异解决问题

# 实施链式规则

这是两个自变量的链式法则， ***a*** 和 ***b*** :

![](img/bd2fc44de687ab5dc38f625d89df519f.png)

法典版本中最重要的部分:

显示了实现上述链规则的本质

备注:

*   第 8 & 9 行:值被封装在节点对象中。这使得它更容易使用。
*   第 11–13 行:定义显示了三个不同的函数***【g()******【h()******【f()***。这里，我们使用同一个函数，一个***mul***tiplication 函数。
*   第 16 行: ***pdo_x_wrt_a*** 表示“**p**artial**d**erivative**o**f**x**t**w**with**r**SPECT**t**o**a**，对应 ***∂x/∂a* 其值通过求导 ***x = a * b*** 得到。由于是*相对于**，所以只把'*'作为一个变量。然后我们可以把方程解释为 ***x = a *常数*** 。这就给了我们***x′=常数*** ，在我们这里就是 ***b*** 。****
*   **第 15–28 行:我们将把这段代码移到系统的其他部分，并用一个单独的调用替换它。**

# **转换为自动微分**

**目标是将主计算下面的所有代码移动到系统的其他部分。最终，我们希望避免任何衍生代码，同时，能够调用一个方法来触发几乎任何计算的梯度计算。**

**我们将 ***pdo_z_wrt_a*** 和 *pdo_z_wrt_b* 移入一个新的 ***渐变*** 属性中各自的节点 ***a*** 和 ***b*** 。gradient 属性将始终包含最后一个值/节点的*偏导数，在我们的示例中为***z****相对于渐变值所属的*节点。此外，我们还拆分了行:***

*我们可以把所有的偏导数值移到它们所属的节点上。它们存储在地图/字典中。偏导数值所涉及的变量成为每个值的关键:*

*接下来，我们找到其余节点的梯度，' ***x*** '，' ***y*** '，以及' ***z*** '。x 的梯度是指最后一个值相对于' ***x*** '的偏导数。在我们的例子中是指' ***z*** '关于' ***x*** '的偏导数，我们已经得到为***z . pdwrt . get(x)***。这同样适用于 y 的梯度。z 的梯度必然意味着*本身*相对于*本身*的偏导数。不确定这是否会影响到数学家，但我会推导出 ***z = z*** ，这将给我们 ***1*** :*

*然后我们可以重写这些行，使其具有相同的形式。这是通过用 x 和 y 的梯度代替对一些偏导数的直接访问来完成的。每个部分都被重新排列，所以我们每次从单个节点读取 ***:****

*此后，该部分可以由环代替:*

*…我们可以将其转换为节点类中的实例方法:*

*对 x 和 y 上的***pushGradientToDependencies()***的调用可以移动到那个完全相同的方法中，从而让一个节点递归地调用其依赖关系上的相同方法:*

*可惜的是，推梯度法有一个 bug。如果我们的主计算多次引用同一个节点，就会破坏我们的计算。那是因为渐变的累积(第 17 行，***dependency . gradient+=***)是使用一个节点的*总渐变*计算的(第 17 行，读取实例字段 ***渐变*** )。当我们第二次访问同一个节点时，我们将合并第一次已经推送的梯度量。修复方法只是累计一个根据当前推送的数量计算的值:*

*此外，我们通过将梯度的累积移动到方法的顶部，对代码进行了一些清理。这将累积的责任转移到节点本身，外部调用方不再需要在调用此方法之前添加渐变。*

*最后一步是将乘法运算的偏导数值移入特定函数:*

*最终实现的核心:*

*为了简洁起见，没有列出诸如加法、平方、减法和求反等其他数学运算的实现。详见[代码报告](https://github.com/codecicles/autodiff-simple/blob/main/src/main/java/com/codecicles/autodiff/AutoDiff.java)。*

# *解决年龄问题*

*现在，我们可以通过梯度下降使用 auto diff 实现来求解方程。*

*问题是:*

> *亚当比贝儿大 24 岁，但再过 6 年，
> 亚当将比贝儿大 3 倍。
> 问:亚当多大了？*

*这可以通过首先计算亚当和贝尔年龄的任何值的总误差来解决。然后，我们可以让我们的 auto diff 计算出梯度，并使用它来逐渐修改值，因此误差最终几乎为零:*

*有了这个误差函数，我们就可以写所谓的‘训练循环’了。亚当和贝尔的年龄首先被初始化为随机值。然后根据我们的自动区分代码的梯度对它们进行修改。我们使用梯度下降来改变这些值:*

*产生的误差接近于零，而 *adamValue* 和 *belleValue* 分别接近于 30 和 6:*

*控制台输出中的选定行*

# *限制*

*根据整个计算图的整体连通性，这种简单而不刺激的实现可以多次处理计算图的一部分。结果还是会是正确的，但是效率低。*

*它也不能处理在相同的操作中使用相同的变量。这意味着像 ***mul(a，a)*** 这样的代码将无法工作。这是由于使用地图/字典来存储偏导数值。这可以通过使用另一种类型的数据结构来弥补。*

*尽管如此，梯度计算仍然有效，我们可以使用它来解决问题，而无需手动编码函数的导数。*

# *超越自动差异*

*自动微分算出的梯度当然有用。令人着迷的是，我们可以简单地通过加/减梯度找到几乎任何函数的好解。这几乎是不可思议的，让我想起仰望天空的感觉，想到每一颗星星都是一个类似太阳的物体。*

*它不止于此。本文只描述了简单的“梯度下降”。你可以用更复杂的方式使用渐变。这将允许您更快地找到解决方案，甚至找到用我们的标准梯度下降法很难达到的解决方案。你可以通过研究“带动量的梯度下降”和“RMSProp”来找到这些被称为“优化器”的算法。*

# *最后的想法*

*我觉得有趣的是，这样一个美妙的算法可以用几行代码来表达(我确信它甚至可以更短，但我是一个 Java 程序员，1k 行以下的都很短)。我不是指我们在这里实现了什么，而是这个过程本身，它在机器学习和许多其他领域都非常重要，可以由一个人非常快地编程。我知道，这个版本只支持标量值不支持张量，但还是*自动微分*。我认为这仍然很酷。*

*最后一个有趣的因素是，我们或多或少可以用两个独立变量来重构链式规则，以实现 auto diff。这让我笑了，以一种超级书呆子的方式。*

# *代码库*

*你可以在这里找到代码:【https://github.com/codecicles/autodiff-simple】T3*

*你可以在这里找到作者一步一步的详细视频:*