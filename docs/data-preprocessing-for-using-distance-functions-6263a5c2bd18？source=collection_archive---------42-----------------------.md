# 使用距离函数的数据预处理

> 原文：<https://towardsdatascience.com/data-preprocessing-for-using-distance-functions-6263a5c2bd18?source=collection_archive---------42----------------------->

## 为了确保输入要素对距离函数的贡献成比例

![](img/c72a848ff5d31b7f60a7b23c88945132.png)

[Ellen Qin](https://unsplash.com/@ellenqin?utm_source=medium&utm_medium=referral) 在 [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral) 上拍照

假设我们经营一个小吃摊，我们想根据顾客的饮料偏好对他们进行分类。我们将为每个客户创建一个特征向量，然后应用 k-means 等聚类算法。我们的餐厅出售 7 种饮料:可乐、零度可乐、雪碧、零度雪碧、芬达橙、芬达葡萄和咖啡。我们询问顾客对每种饮料的喜好，获得长度为 7 的特征向量，其值在 0 和 1 之间。考虑以下客户:

*   顾客 A 喜欢所有的汽水，但他不喜欢咖啡。对应的特征向量是[1，1，1，1，1，1，1，0]。
*   顾客 B 喜欢零卡路里的汽水，但是她不喜欢普通的汽水，也不喜欢咖啡。对应的特征向量是[0，1，0，1，0，0，0]。
*   顾客 C 只喜欢咖啡。对应的特征向量是[0，0，0，0，0，0，0，1]。

直觉上，我们会认为客户 A 和 B 有点相似，而客户 C 与 A 和 B 完全不同。然而，当我们计算特征向量之间的欧几里德距离时，我们得到:

*   |AB| = √4 = 2
*   |BC| = √3 ≈ 1.73

因此，客户 B 实际上离 C 比离 A 更近！注意，这个结果不限于欧几里德距离。如果我们使用其他的 *Lp-* 范数，比如曼哈顿距离，也会得到同样的结果。

## 特征对欧氏距离的贡献

这种反直觉结果的根本原因是与汽水相关的特征比与咖啡相关的特征多得多。然后，两个特征向量之间的欧几里德距离由它们在苏打相关特征上的差异决定，而在唯一的咖啡相关特征上的差异没有太大影响。为了进一步说明这一点，让我们为咖啡设计更多的特性。

我们用“喜欢黑咖啡”、“喜欢加糖的咖啡”和“喜欢加牛奶的咖啡”三个特征代替了“喜欢咖啡”这个单一特征。假设顾客 A 和 B 不喜欢任何一种咖啡，而顾客 C 喜欢所有种类的咖啡，他们对应的特征向量变成:

*   答:[1，1，1，1，1，1，0，0，0]
*   乙:[0，1，0，1，0，0，0，0，0]
*   C: [0，0，0，0，0，0，1，1，1]

它们的欧几里得距离是:

*   |AB| = √4 = 2 和之前一样，但是
*   |BC| = √5 ≈ 2.24

因此，客户 B 现在更接近客户 a。这是由于咖啡相关特征对客户 B 和 c 之间的欧几里德距离的贡献增加。

## 调整出资比例

回到最初的场景，我们将聚类算法应用于 7 维饮料偏好特征向量。假设客户 B 和 C 属于同一个集群，我们的下游应用程序不能很好地处理这个结果。除了调整我们的聚类算法，我们还可以通过增加由咖啡相关特征贡献的欧几里德距离的比例来调整聚类。有几种方法可以实现这一点:

1.  设计更多的咖啡相关的功能，正如我们在上面看到的
2.  删除一些与苏打水相关的功能
3.  扩大与咖啡相关的功能
4.  缩小与汽水相关的特征

(2)和(4)基本上与(1)和(3)相反。它们降低了汽水相关特征的重要性，从而使咖啡相关特征变得更加重要。

(3)和(4)是通常的特征缩放的变体。它们既不需要特征工程，也不需要特征选择，因此更加简便。对于通常的特征缩放，我们将诸如最小-最大归一化的缩放方法应用于每个特征，使得每个特征贡献相等。这里的不同之处在于，我们对每组要素应用了一种缩放方法，因此每组要素的贡献是相等的。

作为一个具体的例子，让我们选择(3)并扩大咖啡相关的功能。首先，我们注意到有 6 个与苏打水相关的特征，但只有 1 个与咖啡相关的特征，并且它们的值都在 0 到 1 之间。如果我们希望这两组特性的贡献相等，我们可以将咖啡相关的特性乘以√6。客户 A、B 和 C 的新特征向量是:

*   答:[1，1，1，1，1，1，0]
*   乙:[0，1，0，1，0，0，0]
*   C: [0，0，0，0，0，0，√6]

由此得出|BC| = √8 ≈ 2.83 大于|AB| = √4 = 2。

## 注意

1.  上述数据预处理步骤不限于聚类任务。它们可以应用于利用特征向量之间的距离的任何任务。
2.  在上面的例子中，我们认为咖啡相关特征的贡献太小，因此我们应用特征缩放。然而，有些情况下，我们实际上希望咖啡相关的功能只贡献一点点。也许我们的小吃摊会更加关注苏打水。也许我们会使用不同的数据集和不同的模型来分析顾客的咖啡偏好。无监督学习没有标签，大量的数据预处理步骤由下游应用和业务用例驱动。

## 常见问题

问:余弦距离有没有一些反直觉的结果？

答:是的。回想一下，顾客 A 喜欢所有的苏打水，但不喜欢任何一种咖啡。顾客 C 喜欢各种咖啡，但不喜欢任何汽水。现在假设我们有一个顾客 D，他喜欢各种饮料。直觉上，我们可能认为客户 D 与 A 和 c 的距离相等。然而，D 的特征向量是[1，1，1，1，1，1，1]，并且

*   *cos _ dist*(AD)= 1–6/(√6 x√7)≈0.074
*   *cos _ dist*(AD)= 1–1/√7≈0.622

请注意，我们可以使用上一节中描述的方法之一，使客户 D 与 A 和 C 等距。

问:为什么我们在监督学习中看不到这些数据预处理步骤？

答:是因为 XGBoost、神经网络等流行的监督学习算法不计算特征向量之间的欧氏距离。如果我们使用监督学习算法，该算法利用特征向量之间的欧几里德距离(例如，k-最近邻)，那么将需要上述数据预处理步骤。

## 进一步阅读

1.  我们缩放某些要素以获得新的距离值的方式可以被视为 Mahalanobis 距离的特例，并且与度量学习相关。快速介绍见[3]。
2.  一种提高基于欧几里德距离的聚类算法的性能的方法是使用核函数。例如，k-means 成为核 k-means，参见[1]和[2]的概述。可以将特征缩放合并到内核函数中。

## 参考

1.  R.Chitta，R. Jin，T.C. Havens 和 A.K. Jain[近似核 k 均值:大规模核聚类的解决方案](https://dl.acm.org/doi/abs/10.1145/2020408.2020558) (2011)，KDD 2011
2.  迪伦、关和库利斯。[核 k 均值、谱聚类和归一化截集](https://www.cs.utexas.edu/~inderjit/public_papers/kdd_spectral_kernelkmeans.pdf) (2004)，KDD 2004
3.  [http://contrib . sci kit-learn . org/metric-learn/introduction . html](http://contrib.scikit-learn.org/metric-learn/introduction.html)