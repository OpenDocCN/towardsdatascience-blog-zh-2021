# 梯度下降初学者指南

> 原文：<https://towardsdatascience.com/beginners-guide-to-gradient-descent-47f8d0f4ce3b?source=collection_archive---------14----------------------->

## 关于梯度下降法你需要知道的一切

![](img/7c4b44df760701045591b6c136296ece.png)

由 [Ales Krivec](https://unsplash.com/@aleskrivec?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText) 在 [Unsplash](https://unsplash.com/s/photos/descent?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText) 上拍摄的照片

梯度下降法是最优化问题的解决指南，在它的帮助下，人们可以找到函数的最小值或最大值。这种方法用于训练模型的机器学习领域，在那里被称为梯度下降法。

# 梯度下降法有什么用？

人工智能的目标通常是创建一种算法，该算法可以在输入值的帮助下尽可能准确地进行预测，即非常接近实际结果。预测和现实之间的差异通过所谓的损失函数转换成一个数学值。梯度下降法用于寻找损失函数的最小值，因为这样就找到了模型的最佳训练条件。

AI 算法的训练然后用于尽可能最小化损失函数，以便具有良好的预测质量。例如，[人工神经网络](https://databasecamp.de/en/ml/artificial-neural-networks)在每个训练步骤中改变单个神经元的权重，以逼近实际值。为了能够专门解决损失函数的最小化问题并且不随机改变各个权重的值，使用了特殊的优化方法。在[人工智能](https://databasecamp.de/en/machine-learning)领域，最常使用的是所谓的梯度下降法。

# 为什么我们接近最小值而不仅仅是计算它？

从微积分中我们知道，我们可以通过设置一阶导数等于 0，然后检查二阶导数在这一点上是否不等于 0 来确定一个最小值或最大值。理论上，我们也可以将这个过程应用于人工神经网络损失函数，并使用它来精确地计算最小值。然而，在具有许多变量的更高的数学维度中，精确的计算是非常昂贵的，并且将花费大量的计算时间，最重要的是，资源。在神经网络中，我们可以很快拥有几百万个神经元，从而在函数中相应地拥有几百万个变量。

因此，我们使用近似方法来快速接近最小值，并确保在一些重复之后找到接近最小值的点。

# 梯度下降的基本思想

梯度下降法的背后是一个数学原理，即函数的梯度(函数的导数有一个以上的自变量)指向函数上升最多的方向。相应地，相反的情况也成立，即函数在梯度的相反方向上下降得最厉害。

在梯度下降法中，我们试图尽可能快地找到函数的最小值。在人工智能的情况下，我们正在寻找损失函数的最小值，我们希望非常快地接近它。所以如果我们沿着梯度的负方向走，我们知道函数下降得最多，因此我们也最快地接近最小值。

对于函数 f(x) = x，我们已经画出了在某些点上具有梯度 f′(x)的切线。在这个例子中，最小值在点 x = 0 处。在点 x = -3 处，切线的斜率为-6。根据梯度法，我们应该向梯度的负方向移动，以更接近最小值，所以— (- 6) = 6。这意味着最小值的 x 值大于-3。然而，在点 x = 1 处，导数 f′(1)的值为 2。因此，梯度的相反方向将是-2，这意味着最小值的 x 值小于 x = 1。这使我们逐渐接近最小值。

简而言之，梯度下降状态:

*   如果函数的导数在 x 点是负的，我们在 x 方向上向前寻找最小值。
*   如果函数的导数在点 x 处为正，我们在 x 方向上向后寻找最小值。

在函数有多个变量的情况下，我们不仅要考虑导数，还要考虑梯度。在多维空间中，梯度相当于二维空间中的导数。

# 我们需要学习率做什么？

梯度下降法给了我们每个起点对应的方向。然而，它并没有告诉我们应该在相应的方向上走多远。在我们的例子中，借助梯度法，我们无法知道在 x = -3 的位置，我们是否应该在正 x 方向上走一步、两步，甚至三步。

在学习率的帮助下，我们可以设置每一步应该有多大。存在学习率的值可以在每一步中改变的方法，但是在大多数情况下，学习率的值是恒定的，并且例如是 0.001 的值。

# 我们如何找到最佳的学习速度？

学习率大小的决定实际上似乎很简单。在小的学习率下，我们接近最小值的速度非常慢，尤其是当我们的起点远离极值点的时候。另一方面，如果我们使用较大的学习率，我们可能会更快地向最小值移动，所以我们应该选择较大的学习率。不幸的是，事情没那么简单。

例如，在我们不知道的情况下，起点已经非常接近最小值。那么大的学习率可能太大，以至于我们永远不会达到最小值。如果我们从点 x=1 处的函数 f(x) = x 开始，并使用学习率> 1，我们将不能很快达到最小值 x = 0，因为到最小值的距离只有 1。

因此，选择一个合适的学习速率并不容易回答，也没有一个我们可以使用的最佳值。在机器学习中，这样的参数有一个名字:超参数。这些是模型中的参数，其值对成败至关重要。这意味着，通过改变超参数，表现不佳的模型可能会变得明显更好，反之亦然。学习率是许多超参数中的一个，并且应该在不同的训练运行中简单地变化，直到达到模型的最佳值。

# 梯度下降的问题

使用梯度法时，我们可能需要处理两个主要问题:

*   我们最终得到的是函数的局部最小值，而不是全局最小值:有许多变量的函数很可能不只有一个最小值。如果一个函数有几个极值，比如最小值，我们称之为具有最低函数值的最小值的全局最小值。其他最小值是所谓的局部最小值。梯度下降不会自动使我们免于寻找局部最小值而不是全局最小值。然而，为了避免这个问题，我们可以测试许多不同的起点，看看它们是否都收敛到同一个最小值。
*   当我们在神经网络及其损失函数的上下文中使用梯度下降法时，会出现另一个问题。在特殊情况下，例如当使用前馈网络时，梯度可能是不稳定的，即它可能变得非常大或者非常小并且趋向于 0。借助于神经元的其他激活函数或权重的某些初始值，可以防止这些影响。但是，这超出了本文的范围。

# 多维空间中的梯度法

对于我们最初的例子 f(x) = x，极值点很容易计算，也可以不用梯度法确定。由于变量不止一个，这就不再那么容易了。比如，我们取函数 f(x，y) = x + y，试着分几步逼近最小值。

1.**起点的确定**:如果要用梯度下降法，我们需要一个起点。有许多方法可以找到一个最佳的起点，但这不是本文要详细讨论的。我们简单地从点 P(2，1)开始搜索。

2.**计算导数/梯度**:接下来我们要计算函数的一阶导数。在多维空间中，这被称为梯度。它是变量所有导数的向量。在我们的例子中，函数由两个变量组成，所以我们需要形成两个导数。一个是以 x 为变量的一阶导数，另一个是以 y 为变量的一阶导数:

![](img/8ecbb73cc3c73b96533a668addb3b7a1.png)

梯度就是向量，第一项是对 x 的导数，第二项是对 y 的导数:

![](img/d04d1b0400668500a3b5b52d32ddad9b.png)

3.**插入起点**:现在我们将起点插入渐变:

![](img/51b411450bd42bb53385c245fb44866a.png)

4.**将步长为负的渐变添加到起点**:最后一步，我们现在从起点开始沿着渐变的负方向前进。也就是我们从起点减去梯度乘以学习率。学习率为 0.01，这意味着:

![](img/8b6b0a1b221fac8e0d2ccfd1f8db0d90.png)

因此，我们现在有了我们的新点 p₂(1.96；0.98)，用它我们可以重新开始该过程以接近最小值。

# 这是你应该带走的东西

*   梯度下降用于尽可能快地逼近函数的最小值。
*   为此目的，人们总是在梯度的负方向上在一个点上迭代。步长由学习速率决定。
*   梯度下降可以有不同的问题，这些问题可以借助于不同的激活函数或初始权重来解决。

*如果你喜欢我的作品，请在这里订阅*<https://medium.com/subscribe/@niklas_lang>**或者查看我的网站* [*数据大本营*](http://www.databasecamp.de/en/homepage) *！此外，媒体允许你每月免费阅读* ***3 篇*** *。如果你想让***无限制地访问我的文章和数以千计的精彩文章，不要犹豫，通过点击我的推荐链接:*[https://medium.com/@niklas_lang/membership](https://medium.com/@niklas_lang/membership)获得会员资格，每个月只需支付 ***5*****

***</understanding-mapreduce-with-the-help-of-harry-potter-5b0ae89cc88>  </introduction-to-random-forest-algorithm-fed4b8c8e848>  </beginners-guide-extract-transform-load-etl-49104a8f9294> ***