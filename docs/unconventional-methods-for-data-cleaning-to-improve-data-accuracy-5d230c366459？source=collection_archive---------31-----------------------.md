# 非常规的数据清理方法，以提高数据准确性

> 原文：<https://towardsdatascience.com/unconventional-methods-for-data-cleaning-to-improve-data-accuracy-5d230c366459?source=collection_archive---------31----------------------->

## 创造性地对字符串值进行数据清理

迪格斯/魏会泽/魏/郑

![](img/880ead16c7ebb960fe7d878e360c19e4.png)

照片由来自 [Pexels](https://www.pexels.com/photo/matrix-background-1089438/?utm_content=attributionCopyText&utm_medium=referral&utm_source=pexels) 的 [Markus Spiske](https://www.pexels.com/@markusspiske?utm_content=attributionCopyText&utm_medium=referral&utm_source=pexels) 拍摄

数据分析正日益成为业务决策的核心部分，并推动着日常流程。问题不再是我们如何收集信息，而是我们用它做什么？对于进入数据驱动决策阶段的公司来说，可能会出现许多挑战。建立一个目标并为共同的问题做好准备对于获得有用和有益的结果是必不可少的。例如了解数据的关系和缺点，检查统一的区域，以及确定在简单或复杂模型之间进行决策的能力。本教程将提供一个指南，帮助您在分析大型数据集和初始化数据分析项目时进行批判性思考。如果你对数据分析知之甚少，对学习非传统的数据清理方法感兴趣，或者试图通过应用先进的技术来解决数据清理问题，本教程将变得非常方便。

> *数据分析的使用不再局限于财大气粗的大公司。它现在已经普及，59%的企业在某种程度上使用了分析。*[羽衣甘蓝](https://www.forbes.com/sites/forbesagencycouncil/2019/10/01/the-age-of-analytics-and-the-importance-of-data-quality/?sh=4ea626b55c3c)

数据分析的时代已经到来。随着数据分析为许多大公司带来持续的成功，越来越多的工具变得可用，数据分析已经成为任何企业的必需品。 [Deloitte](https://www2.deloitte.com/content/dam/Deloitte/global/Documents/Deloitte-Analytics/dttl-analytics-analytics-advantage-report-061913.pdf) 最近进行了一项调查，发现近一半的受访者(49%)声称数据分析是更好的决策能力的关键因素，近三分之二的受访者表示分析在推动业务战略方面发挥着重要作用。此外，数据分析对营销的影响正在上升，55%的受访者表示，他们的营销和销售团队在分析方面的投资仅次于财务运营。作为来自加州大学戴维斯分校的[商业分析科学硕士](https://gsm.ucdavis.edu/master-science-business-analytics-msba)【MSBA】[学生，我们是这一趋势的前沿专家。凭借与全球投资研究领域领先的技术解决方案提供商](https://gsm.ucdavis.edu/news/msba-ranked-no-4-globally-value-money#:~:text=4%20Globally%20for%20Value%20for%20Money,-News&text=QS%20World%20University%20Rankings%3A%20Master,investment%20of%20nearly%20%241%20million.) [BlueMatrix](https://www.bluematrix.com/) 的一年[实习经验](https://gsm.ucdavis.edu/master-science-business-analytics/academics/practicum-projects)，我们完成了几个分析项目，并学会了如何利用数据分析的力量。

BlueMatrix 的客户拥有丰富的数据，可以在决策过程中使用。BlueMatrix 参与了这个新的数据分析项目，以帮助他们从数据中获得新的见解。鉴于在进行任何类型的分析之前，干净的数据是“必须的”,公司应该寻求更有效的方法来加快这一过程，同时提高准确性，以便及时产生见解。我们利用相似性得分计算和机器学习的能力来自动化数据清理过程，这大大提高了数据质量，我们相信这些技术可以很容易地被像我们项目这样情况类似的公司采用。我们已经看到，越来越多的公司试图在改善工作流程方面发挥创造力和创新性，但他们不确定具体要做些什么来实现这一目标。在这种情况下，本教程中介绍的两个解决方案将引导您完成思考过程，并展示潜在用例的示例。

# **问题公式化**

文本数据有时比数字数据更难处理。简单地删除重复或截断文本是不够的，还需要更复杂的方法。例如，一家公司可能希望了解购买其产品的客户和他们工作的公司是否有任何趋势。如果客户能够在字段中输入自定义文本，一个人可能会输入“ABC 公司”，另一个人可能会输入“ABC 公司”，还有一个人可能会简单地列出“ABC”(图 1)。识别这些定制条目并将其转换为统一标准可能是一项挑战。另一个可能出现的问题是，如果客户注册了多个配置文件，以利用新的客户促销。未能消除重复的客户将降低分析的准确性。使用更先进的数据清理方法的目的是提高数据的准确性，从而提高分析的准确性。

![](img/e8ae689ede0dc4aac8fc322b742dd43c.png)

作者照片

# **方法 1:相似性评分定位重复记录**

**为什么使用相似性评分进行数据清洗**

相似性得分是一个非常通用的工具。一般来说，相似性分数衡量在比较数据库的源中找到的文本的百分比。最流行的是，它被用来检测抄袭和建立数据分割的集群。虽然这可能是一种更复杂的方法，但它在数据清理过程中非常有用。例如，如果您的公司刚刚开始使用数据分析，并且有大量数据需要进行质量检查。您可能会找到重复的记录。在一些简单的情况下，精确的字符串匹配足以定位重复项。然而，尤其是最近，一些任务需要比 SQL 中的简单连接函数更多的功能。例如，如果您使用名称匹配两个不同表中的记录，如果名称的拼写或标点不同，则精确匹配不会考虑在内。此外，如果名称有前缀或后缀，或者有错别字，您将需要使用更复杂的方法，如相似性得分计算。

**如何计算相似度得分**

有几种不同的方法来计算字符串的相似性得分。它们通常都涉及确定两个字符串之间的距离，但是根据您的具体情况，一种方法可能比另一种更好。例如，Levenshtein-Distance 是最常见和最广泛使用的度量之一。然而，你会发现很难用一种系统的方式来解释结果，而且这种方法不能很好地识别模式。Jaro-Winkler 是另一种距离度量，它更适合于字符换位，拾取近似匹配，如“Kiera”和“Keira”或“Marc”和“Mark”，并将调整常见前缀的权重。实际上，最好是测试出几种不同的方法来找到解决问题的正确方法。在我们的例子中，python 包 [FuzzyWuzzy](https://pypi.org/project/fuzzywuzzy/) 表现最好。

FuzzyWuzzy 方法使用模糊字符串匹配来识别模式。它还使用 Levenshtein-Distance 来计算序列之间的差异。 [SeatGeek](https://github.com/seatgeek/fuzzywuzzy) 开发了这个包，并提供了如何在你的电脑上安装 FuzzyWuzzy 的说明。这里的一个技巧是确保安装 python-Levenshtein 包，因为它通过将速度提高 4 到 10 倍，极大地提高了该过程的性能。为了解释 FuzzyWuzzy 的结果，您需要使用两个序列之间的相似性比率来获得相似性百分比。相似性百分比的分数为 100 表示两个序列之间完全匹配。分数越低，两个序列的差异越大。

```
pip install fuzzywuzzy
pip install fuzzywuzzy[speedup]#OR
pip install git+git://github.com/seatgeek/fuzzywuzz.git@0.18#egg=fuzzywuzzyfrom fuzzywuzzy import fuzz
from fuzzywuzzy import process fuzz.partial_ratio("Nicki Jonas", "Nicki Jones")
#Match = 91
```

在安装包和收集数据集之后，您就可以为自己的项目计算相似性得分了。

```
choices = ["Nick Jonas", "Nicki Jonas", "Nick Jerry Jonas", "Joe Jonas"] 
process.extract("Nick Jonas", choices, limit=2)
```

**计算相似度分数的技巧:**

以下是使用 FuzzyWuzzy 软件包的一些提示。我们分享这些信息的目的是帮助您简化流程并避免潜在的错误。

***1。)确保您的输入数据尽可能准确* :**

本质上，数据越干净，字符串匹配过程就越快。例如，去掉所有的前缀或后缀、任何职务头衔或任何不需要的符号。

***2。)设定阈值* :**

对于您自己的业务问题，您应该选择一个阈值来对相似性得分进行分类。例如，如果您只想删除重复的记录并保留唯一的记录，则任何得分低于 100 的匹配都将被删除。但是，在其他情况下，您可能希望设置一个较低的阈值，以允许一些误报或漏报来弥补其他错误。例如，如果您正在比较电子邮件地址以查找相同的记录，则设置较低的阈值可能会更好，因为误报比漏报造成的伤害更小。

```
#Similarity Ratio
a = fuzz.ratio("Nick Jonas", "Nick Jerry Jonas")b = fuzz.ratio("Nick Jonas", "Nicki Jones")c = fuzz.ratio("Nick Jonas", "Joe Jonas")d = fuzz.ratio("Nick Jonas", "Nick Jonas")e = fuzz.ratio("Nick Jonas", "Kevin Jonas")lis = [a, b, c, d, e]print("Here is a list of similarity scores:"+ str(lis))passed = [number for number in lis if number >= 80]print("Here is a list of similarity scores that passed the threshold:" + str(passed))#Output of similarity scores: [77, 86, 83, 63, 100, 67]
#Output of similarity scores that passed the threshold: [86, 100]
```

***3。)选择一个比例* :**

您可以使用四种比率方法。请务必查看文档中的样本，以了解每个比率的详细信息。

**简单比**:比较两个字符串的简单明了的方法。

```
fuzz.ratio("Nick Jonas", "Nicki Jones")
#Match = 86
```

**部分比率**:它建立在简单比率的基础上，并考虑到字符串中有多余的标点符号。

```
fuzz.partial_ratio("Nicki Jonas", "Nicki Jones")
#Match = 91
```

**Token sort ratio** :这种方法在字符串之间存在单词换位模式时非常有用。

```
fuzz.token_sort_ratio("Nick Jonas", "Nick Jerry Jonas")
#Match = 77
```

**Token set ratio** :当字符串中有重复元素时，这种方法很有用。

```
fuzz.token_set_ratio("Nick Jonas", "Nick Jerry Jonas")
#Match = 100
```

# 方法 2:预测缺失值-机器学习模型

**为什么机器学习很有帮助**

假设您正在管理一个客户信息数据库，您有如下表所示的信息。当客户注册您的产品时，他们会向您提供年龄和性别等个人信息。一些字段，例如感兴趣的区域，可以在注册时跳过，以便信息不是由所有客户填写的。

![](img/0aa8e449e56f960ca82536d624e810ee.png)

稍后，您计划构建一个模型来预测每个客户的交易可能性，并意识到利息列中大约有 15%的值丢失。由于填写列的“平均值”或“中值”没有意义，一个潜在的解决方案是将它们放入“未指定”类别，然后继续使用您的模型。然而，我们发现，如果模型精度高，使用机器学习模型来尝试将用户分类到可能的兴趣组可能是更好的方法。在这种情况下，有足够的信息来对每个用户的兴趣做出有根据的猜测。这可能会大大提高流失预测的准确性。

另一方面，我们建议从评估模型中的列开始。运行机器学习模型可能很耗时，因此应该首先检查信息。在应用这种方法之后，您的模型的性能可能不会有太大的变化，那么采用类似的方法将是一个更好的解决方案。

**帮助你入门的几个步骤:**

***1。)决定型号* :**

如果你已经决定继续进行机器学习模型，我们建议你首先仔细看看你的模型的用例。模型什么时候用？多久一次？将在哪里实施？问这些问题可以引导你思考你的输入数据和你的能力。在我们的案例中，为了进行预测，我们需要考虑在您对客户的流失预测兴趣进行分类时可以获得哪些信息。

在决定使用哪个模型时，其他的考虑因素包括你需要多长时间做出决定，以及你对准确性的重视程度。例如，如果你想通过使用深度学习模型来提高结果的准确性，而不是使用树模型，那么评估模型之间的权衡就非常重要。与深度学习模型相比，树模型往往成本更低，需要的调整更少。另一方面，深度学习模型可能会表现得更好，但根据情况的不同，它们可能会有更高的开发成本。此外，该模型可能比树模型更难解释，如果没有正确调整，性能可能不会提高。就像 James Montantes 在他的[文章](/3-reasons-to-use-random-forest-over-a-neural-network-comparing-machine-learning-versus-deep-f9d65a154d89)中提到的，“在一个行业环境中，我们需要一个模型，能够为利益相关者赋予特性/变量以意义。这些利益相关者很可能是除了具有深度学习或机器学习知识的人以外的任何人。”。在开始构建模型之前，模型的用例、可用资源的数量以及成本和收益之间的最佳平衡都是需要考虑的事情。

**2*。)数据准备***

在这个阶段，您已经最终选择了要测试的模型，并将开始为它准备数据。除了典型的检查和步骤，我们建议您开始考虑模型的准确性。在监督学习的情况下，如果它对业务问题有意义，您是否应该考虑减少模型中的类别数量？在我们前面的例子中，如果有太多不同的兴趣类别，那么就很难做出合理的预测。在这种情况下，我们可以根据层次结构对兴趣进行分组。比如把音乐、电影和电视节目放在一个叫做娱乐的大类别下。在我们的实习项目中，我们将彼此相似的小子集分成更大的集合。这样做是为了减少不平衡数据的影响，并通过合理数量的类别来提高模型的预测能力。

***3。)车型评价:***

一旦建立了模型，最后一步是评估模型的性能。有几个常用的指标，如分类准确度、曲线下面积和 F1 值。互联网上有大量关于这些指标和使用案例的信息，因此我们想重点谈谈我们从自身经历中学到的两件事。

首先是处理缺失的信息。在前面的章节中，我们已经讨论了在构建模型时识别可用输入数据的重要性。我们建议在没有其中一个功能的情况下测试模型，以查看该信息缺失时的性能。例如，如上表所示，如果“公司”是确定兴趣的最重要的特征之一，那么当添加一组新的客户并且他们的公司还不在训练数据库中时，模型的性能可能会差得多。因此，我们认为在没有最重要的列的情况下再次运行模型可以帮助评估模型在这些情况下的性能。

最后一件事是，随着新信息的到来，定期调整模型。为了不断提供有意义的见解，需要维护模型。也许你会开始收集你的客户的一个新特性，并想把它添加到你的模型中，或者也许模型需要重新训练久而久之来提高它的性能。不要只是把你的模型放在那里，认为它会永远有效。监控其性能并努力维护它。

# 结论

到目前为止，您应该对两种数据清理方法有了深入的了解，可以在自己的文本数据上进行尝试。虽然计算相似性得分和使用机器学习模型可能更复杂，但有时需要更复杂的方法来解决数据的复杂性和特质。本教程提供了一个起点，供您思考您的目标，完成整个过程，并有希望获得一个干净的、转换的、统一的数据集，为分析做好准备。因此，无论这是您的第一个分析项目，还是您刚刚遇到字符串变量的困难，我们希望我们的提示和建议将帮助您避免一些常见问题，并节省整个过程中的故障排除时间。