# 你的强化学习算法绝对应该超越的三个基准策略

> 原文：<https://towardsdatascience.com/three-baseline-policies-your-reinforcement-learning-algorithm-absolutely-should-outperform-d2ff4d1175b8?source=collection_archive---------25----------------------->

## 在花费太多时间重新设计和微调你的算法之前，检查一下它是否能学到任何有用的东西

![](img/99e7b9cf2da88449117fe6a3c24e637c.png)

在 [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral) 上 [Magnet.me](https://unsplash.com/@magnetme?utm_source=medium&utm_medium=referral) 拍照

作为机器学习的一个子域，强化学习(RL)经常被比作一个黑盒。你尝试了几个动作，将结果输入神经网络，然后输出一些价值观——一个深奥的*政策*告诉你在任何给定的情况下该做什么。

当你穿越一个冰冻的湖泊或者玩电子游戏时，你很快就会明白这个政策是否有用。然而，在没有解决方案质量的清晰概念、没有下限和上限、没有视觉辅助的情况下，存在许多问题。想象一下控制一个大型卡车车队，随着时间的推移重新平衡一个股票投资组合，为一家超市决定订单政策。确定你的 RL 算法是否好可能会变得非常困难。

对于这样的问题，在算法开发过程中，手边有一些快速的基线策略是非常重要的。本文中概述的三个策略非常容易实现，可以用作健全性检查，并在出现问题时立即告诉您。

# 随机策略

![](img/fa76d789f4fcd57080ff3d67d663e7ba.png)

安德烈·木桐在 [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral) 上拍摄的照片

大多数 RL 算法都有一些探索参数，例如， *ϵ* 转化为在 5%的时间内采取随机行动。把它设为 100%，你就一直在探索；确实容易实现。

很明显，蒙住眼睛的猴子扔飞镖不是一个明智的政策，这也正是为什么你的 RL 算法应该总是——持续地、显著地——胜过它。

然而，还有更多，特别是如果您不确定您的环境在多大程度上是可预测的。如果你未能全面超越随机基线，这可能表明没有简单的**可预测的模式**可以学习。毕竟，即使是最复杂的神经网络也无法从纯噪声中学习任何东西。(不幸的是，也可能是你的算法太烂了)

# 短视的政策

![](img/3d2289a1fb83f0d4247d0e628ac5d505.png)

照片由 [flickr 上的](https://www.flickr.com/photos/75894308@N03/7163532770)[J·马什](https://www.flickr.com/photos/75894308@N03/)拍摄

RL 的巨大吸引力在于它允许解决复杂的*序列*决策问题。现在决定最佳行动可能是一个简单的问题，但预测这个行动如何在很久以后继续影响我们的回报和环境是另一回事。

自然地，如果我们把所有的努力放在建模和从未来学习上，我们希望看到更好的结果。如果我们可以在不考虑下游影响的情况下做出类似质量的决策，为什么还要做更多呢？对于大多数问题来说，短视政策只是最大化(最小化)了直接回报(成本),并且很容易实施。

类似于随机政策，短视政策可能是一个试金石。在高度随机的环境中，你今天采取的行动可能对明天的世界影响有限，对后天的影响就更小了。从一个状态到另一个状态的转换通常包含一个随机分量[和一个确定分量](/what-are-post-decision-states-and-what-do-they-want-from-us-9e02105b7f40)；如果随机成分很大，而且大部分是噪声，那么预测下游效应就没什么好处。对比有无前瞻的政策，可以量化对未来的预测实际上有多大帮助。

# 现成的算法

![](img/841f6daa69feee9bc9662a6433e04e4d.png)

柏林自然商品在 [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral) 上拍摄的照片

我们大多数人都不得不与“站在巨人的肩膀上”这一事实作斗争。全新算法的突破性发展是罕见的。您的问题的解决方案可能是某个已存在的 RL 算法的调整版本，而不是从头开始构建。

自然地，我们都相信我们知道得更多，我们可以聪明地重组技术，构建架构和调整参数来获得更好的结果。也许我们可以，但必须检查。如果你打算花几周的时间来建立一个自定义的演员-评论家模型，它最好比基本的增强算法有相当大的优势。时间和资源总是稀缺的；有时候“足够好”就是你所需要的。

# 最终注释

这篇文章中的基线对一些人来说可能有点傻，但是坦率地说，它不止一次地帮助了我。尤其是在高维业务问题中，不加比较地盯着大向量通常没有太大帮助。在某些情况下(尤其是金融)，数据集是否隐藏了实际上有助于当今决策的可预测模式，确实值得怀疑。基线政策有助于验证您是否在正确的轨道上。

在总结之前，强调一下基准**和竞争**基准**之间的显著差异是有好处的。这篇文章中提到的基线应该被大大超越，以证明你的 RL 算法学习了*一些*有用的东西。尽管如此，仅此还不足以说明这是一个好的解决方案。如果你想发表一篇学术论文，或者想升级你公司的规划系统，你最好将你的算法与一些严重的竞争对手进行对比。**

但是在你这样做之前，请先检查你是否能用飞镖打败鼹鼠和猴子。