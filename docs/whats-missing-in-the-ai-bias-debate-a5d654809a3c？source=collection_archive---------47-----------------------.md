# 人工智能偏见的辩论中缺少了什么

> 原文：<https://towardsdatascience.com/whats-missing-in-the-ai-bias-debate-a5d654809a3c?source=collection_archive---------47----------------------->

## [公平和偏见](https://towardsdatascience.com/tagged/fairness-and-bias)

## 关于人工智能偏见的争论经常忽略一个重要的问题:人工智能应该首先用于给定的决策吗？

![](img/74555e691954978eb230c88c19ecb391.png)

照片由[希拉·墨菲](https://unsplash.com/@shelaghmurphy?utm_source=medium&utm_medium=referral)在 [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral) 上拍摄

幸运的是，人工智能(AI)中的偏见最近受到了很多关注。然而，从关于有偏见的人工智能的故事中产生的大部分辩论是关于如何修复数据，使它们不再有偏见，或者固有的[可解释的机器学习](https://kaspergroesludvigsen.medium.com/explainable-ai-vs-interpretable-ai-b6df8e57ac70)模型是否应该用于更复杂的模型(例如[1])。很少讨论是否应该首先使用基于机器学习的工具来做出给定的决策。在这篇文章中，我提出了一些问题，你可以问自己，以确定你是否应该使用机器学习来解决手头的问题。

# 不适合人工智能的决策

让我举几个例子，这些例子应该让我们问“人工智能应该用于这些决策吗？”在我看来，答案应该是“不”

**为面临家庭虐待风险的儿童分配风险分值**

2018 年，两个城市的丹麦社会工作者开始使用基于机器学习的工具来支持他们关于是否将孩子从父母身边带走的决定。机器学习模型为其他公共部门员工通知的儿童分配了一个风险分数。在发现机器学习模型存在缺陷后，该模型被停产之前，该模型评估了 200 份关于儿童惊人行为的通知。事实证明，该算法具有高度的年龄偏见，因此，除了年龄之外，所有输入特征都相同的两个孩子会得到不同的分数，年龄较大的孩子被赋予更高的风险分数[2]。

你为什么想用人工智能来做这个风险评估？人类更善于理解因果关系[3]，之前已经发现社会工作者完全有能力在没有机器学习模型“帮助”的情况下自己做出决定[4]

**预测成绩**

> “2020 年，英国许多成绩优异的学生受到了一种标准化算法的惩罚，该算法旨在预测新冠肺炎疫情 A-level 考试的成绩。为了匹配历史分布，该算法提高了小型私立学校的预测分数，降低了大型公立学校的分数，这些学校在历史上培养了更大比例的黑人、亚裔和少数族裔(BAME)学生。结果，与同龄人相比，BAME 和贫困学生不成比例地看到他们的预测成绩下降。”[5，第 3 页]

为什么不让更擅长评估学生学术能力的老师来做这项工作呢？对于几个小时的额外工作，你可以消除大量学生被不公平评估的风险。的确，老师的评价也会有偏差，但至少这种偏差不会大规模存在。

**预测一个人是否会再次犯罪**

在美国，当局使用一种名为“替代制裁的罪犯管理概况”的所谓风险评估工具，根据估计的被告重新犯罪的可能性对其进行分类。有人认为该模型存在种族偏见[6]，尽管这一发现受到质疑[7]。

为什么你想用人工智能来预测一个人是否有违反假释条款的风险？这并不意味着它有巨大的成本节约潜力，尽管它可以消除单个法官做出偏见决定的风险，但该模型是根据已经被人类偏见和几个世纪的种族主义污染的历史数据进行训练的。

# **问是不是应该，不是可能**

在我看来，在上述情况下使用人工智能的决定更多地受到问题“*我们能使用人工智能来做出这些决定吗？”我们应该用人工智能来做这些决定吗？”。*

*下面，我提出了一些问题，可以帮助确定是否应该使用人工智能来做出一个给定的决定。*

*   ***人类能以合理的成本水平同样出色地做出决策吗？使用一种算法来确定地图上两点之间的最短路径是非常合理的，因为人类需要不合理的时间来完成这项工作。然而，使用一种算法来预测累犯是没有意义的，因为算法在这项任务上的表现还不够好，不足以抵消使人类偏见永久化的风险[8]***
*   ***这个决定对一个人的生活有重大影响吗？**虽然打翻花瓶的自动吸尘器可能会给花瓶的主人带来一些不便，但与造成人身伤害的自动车辆事故相比，这种决定对人类生活的影响就相形见绌了。*
*   *这个人工智能用例的好处是否大于风险？如果在某个时候我们有了完全自动驾驶的汽车，它们可能会不时伤害人类，但它们可能不会像人类司机那样频繁。*

*以下是显示为流程图的问题:*

*![](img/d4ff656082fc985217d7bc486e9ed06f.png)*

*“我是否应该使用人工智能来解决这个问题”流程图*

*如果您认为人工智能应该用于您的决策问题，则可以考虑标准保健因素来回答人工智能*是否能够*解决问题，例如数据质量、模型性能、风险和概念漂移的缓解等。*

*虽然上面列出的问题并不全面(甚至可能不正确)，但我相信它们是一个很好的起点。通过问“我们应该用人工智能来做这个决定吗？”来开始你的人工智能项目我们可以减少一些对人类未来做出不公正决定的风险，并避免浪费资源来构建人工智能，以做出人工智能不适合的决定。请让我知道你的想法。*

# ***参考文献***

*[1] C. Rudin，[停止解释高风险决策的黑盒机器学习模型，转而使用可解释的模型](https://arxiv.org/abs/1811.10154) (2018)，作者 2019 年自然机器智能文章的预发布版本。较短的版本发表在 NIPS 2018 机器学习趋势批判和纠正研讨会上。在十字路口网上研讨会上也扩展了 NSF 的统计数据*

*[2] F. Kulager，[什么是算法？](https://www.zetland.dk/historie/s8YxAamr-aOZj67pz-e30df#:~:text=Kendskabet%20til%20hundredtusindvis%20af%20gamle,er%20et%20liv%20som%20anbragt.) (2021)，Zetland.dk*

*[3] J. Pearl，D. Mackenzie，[原因之书:因果的新科学](http://bayes.cs.ucla.edu/WHY/) (2018)。纽约:基础书籍。*

*[4] Blinde Vinkler，第 10 集:AI I jobcentrene—hjlpende eller stemplende？(2021).[https://soundcloud . com/IDA-blinde-vin kler/ai-I-jobcentrene-hjaelpende-eller-stemplende](https://soundcloud.com/ida-blinde-vinkler/ai-i-jobcentrene-hjaelpende-eller-stemplende)*

*[5]沃希特，米特施塔特，拉塞尔[机器学习中的偏见保持:欧盟非歧视法下公平指标的合法性](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3792772) (2021)。沃希特、桑德拉和米特斯塔特、布伦特和拉塞尔、克里斯，《机器学习中的偏见保持:欧盟非歧视法下公平指标的合法性》(2021 年 1 月 15 日)。西弗吉尼亚州法律评论，即将出版，可在 https://ssrn.com/abstract=3792772 的[或 http://dx.doi.org/10.2139/ssrn.3792772 的](https://ssrn.com/abstract=3792772)[获得](https://dx.doi.org/10.2139/ssrn.3792772)*

*[6] J. Angwin，J. Larson，S. Mattu 和 L. Kirchner，[机器偏差](https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing) (2016)。ProPublica.org。*

*[7] C. Rudin，C. Wang，B. Coker，[再犯预测中的保密与不公平时代](https://arxiv.org/abs/1811.00731) (2018)。哈佛数据科学评论 2(1)。*

*[8] M .朱，[《一个算法陪审团:利用人工智能预测再犯罪率》](https://www.yalescientific.org/2020/05/an-algorithmic-jury-using-artificial-intelligence-to-predict-recidivism-rates/)，(2020)。耶鲁科学。*