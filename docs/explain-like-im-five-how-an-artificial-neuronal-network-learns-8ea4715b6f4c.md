# 像我五岁一样解释:人工神经网络如何学习

> 原文：<https://towardsdatascience.com/explain-like-im-five-how-an-artificial-neuronal-network-learns-8ea4715b6f4c?source=collection_archive---------28----------------------->

![](img/f38edb1eeefde644cf4faab960b558ca.png)

凯文·Ku 在 [Unsplash](https://unsplash.com/s/photos/machine-learning?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText) 上拍摄的照片。

## 人工神经网络学习范式简介。

# 人工神经网络是如何学习的？

人工神经网络(“ann”)的学习能力属于**机器学习**的科学领域。机器学习是从经验中人工生成知识的通称。更具体地说，人工神经网络从历史实例中学习，并且可以在学习阶段之后通过学习实例中包含的模式来概括这些实例。在机器学习中，有三种学习范式。其中包括**监督**和**非监督**学习以及**强化学习**。

## 监督学习

在监督学习中，人工神经网络定义一个目标函数，尽可能准确地预测目标值。人工神经网络通过创建一个目标函数来实现这一点，该目标函数将假定的输出值分配给每个输入值。然后，它将其输出值与给定的目标值进行比较，并调整其目标函数，直到达到期望的精度。因此，这种方法是基于一个预先确定结果的方案。这里有一个例子:

假设，我们玩一个电子游戏，在这个游戏中，我们在港口驾驶一艘船。游戏的主要目标是当你在港口时不要让船沉没，或者征服所有后续的关卡(每一关都会更难)。第二个目标是在途中收集尽可能多的黄金。当你完成最后一关时，金币会重新出现，游戏结束。⁴

在监督学习中，人工智能将试图找到一个数学函数，使其能够完成所有关卡，同时收集最多的黄金。人工智能通过从一个简单的功能开始实现这个目标，这个功能不断失败并使船沉没。然而，每次失败时，它都会检查游戏的最优路径，并调整其功能以尽可能接近最优。当提供的目标值以用最大数量的黄金结束游戏时，AI 也将总是试图结束游戏。⁴

## 无监督学习

在无监督学习中，人工神经网络识别输入模式，而无需访问先前已知的目标值或奖励。AI 根据输入值的相似性来确定自己的方向，并相应地调整权重。因此，学习过程的结果不能与已知的结果相比较。让我们看看这会如何改变我们的示例:

我们的人工智能还在玩同样的游戏。首要目标是活下去，而次要目标是收集尽可能多的黄金。然而，这一次人工智能不知道击败游戏的最佳路径。它没有目标值来比较它的方法，并告诉它必须完成游戏。⁴

*在无监督学习中，人工智能被传递一个或多个目标，但不被告知如何实现这些目标。结果，人工智能创建了一个目标函数，并试图自己优化它，以实现其给定的目标。因此，无监督学习的结果变得不可预测。⁴*

*在这种情况下，一个可能的结果是人工智能学会驾驶船只并在港口收集黄金。它进入下一个级别，但决定风险回报率不够高。因此，人工智能决定留在港口，并在其中永远巡航，同时不断收集重生的黄金。这样，它以一种最佳的方式实现了它的目标:船永远不会沉，人工智能收集无限量的黄金。⁴*

## 强化学习

在强化学习中，人工智能独立学习一种策略，以最大限度地获得回报。AI 不会显示在什么情况下什么动作是最好的。它会在特定的时间获得回报，这也可能是负面的。基于这个奖励，人工智能然后估计一个效用函数，它决定了具体行动的价值。关于我们的例子，我们可以得出以下结论:

人工智能用同样的规则玩同样的游戏。然而，这一次没有给它任何目标。它不知道它应该做什么，但它从游戏中收到两个反馈:它要么死亡并失去所有(负奖励)，要么它活着并积累黄金(正奖励)。⁴

*与无监督学习类似，强化学习的结果可能是不可预测的，因为很难判断人工智能将如何对呈现的奖励做出反应。在我们的游戏场景中，结果可能与无人监管的 learning:⁴相同*

*人工智能开始在港口巡航，击沉船只，并开始收集黄金。它会尝试进入下一个层次，但会发现负面奖励在增加，而正面奖励保持不变。结果，它会试图通过永远停留在最低水平来最小化负面回报。⁴*

# 结论

监督学习是人工神经网络最常用的学习方法之一。在监督学习中，人工智能通过形成目标函数来学习，该目标函数试图预测与呈现给它的目标值尽可能相似的值。如果你想了解更多关于监督学习与非监督学习的区别，你可以在这里继续阅读。

尽管在我们的例子中，无监督学习和强化学习的结果可能是相同的，但这两种学习方法是根本不同的。虽然无监督学习是基于目标得出结论的，但强化学习侧重于最大化/最小化回报。如果你有兴趣了解更多关于强化学习的知识，推荐你阅读[这篇文章](https://www.analyticsvidhya.com/blog/2021/02/introduction-to-reinforcement-learning-for-beginners/)。

# 来源

1.  赖特迈尔。2015.结构信息下的分类问题，18–167。卡塞尔:卡塞尔大学。
2.  Mohri，Rostamizadeh 和 Talwalkar。2012.机器学习基础，21–112。剑桥:麻省理工学院出版社。
3.  拉戈，德·里德和德·舒特。2018.预测现货电价:深度学习方法和传统算法的经验比较，403。阿姆斯特丹:爱思唯尔
4.  弗里德曼，莱克斯。“麻省理工学院 6。S094:深度学习和自动驾驶汽车介绍。”2017 年 1 月 16 日。教育视频，38:45。https://youtu.be/2UElC_YZ0Eo.