# 利用你的超参数:批量大小和学习率作为正则化

> 原文：<https://towardsdatascience.com/exploit-your-hyperparameters-batch-size-and-learning-rate-as-regularization-9094c1c99b55?source=collection_archive---------11----------------------->

## 重新考虑这些超参数将提高您的模型的真实性能

梯度下降是许多人在学习机器或深度学习时首先学习的概念之一。这种**优化算法是大多数机器学习的基础，包括神经网络中的反向传播**。当学习梯度下降时，我们知道学习速率和批量大小很重要。

具体来说，增加学习速率会加快模型的学习，但有超过最小损失的风险。减少批量意味着你的模型在每次迭代学习中使用更少的样本来计算损失。

除此之外，这些*珍贵的超参数很少受到关注*。我们调整它们以最小化我们的训练损失。然后使用“更高级”的正则化方法来改进我们的模型，减少过度拟合。 ***这是正确的做法吗？***

# 重新想象梯度下降

让我们后退一步。假设我们想要绝对最小的损失。如果我们有无限的计算能力，我们会用我们的学习速度做什么？我们可能会把它降低到绝对最小值。然而，与线性回归等模型不同，神经网络不是凸的。它们看起来不像碗，而是像山脉:

![](img/5f1abcfe58c070054d84a9bf4773f527.png)

谢尔盖·佩斯特耶夫在 [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral) 上的照片

**降低你的学习速度**肯定会让你陷入更深的低谷，但**不会阻止你掉进一个随机的次优洞。**这是一个**局部最小值** *或者一个看起来像最低点*的点，其实不是。它很可能会超出你的训练数据，这意味着它不会推广到现实世界。

我们想要的是进入一个更平坦的高原。具有更平坦最小值的神经网络具有更大的泛化能力*(加上像防止对你的网络的攻击这样的好处)*。

一种直观的思考方式是，一个**窄洞很可能是针对你的训练数据**的。您的数据的微小变化*(即在该空间中向任何方向移动)*将导致损失的重大变化，而平坦的地形则不太敏感。这就是正则化的用武之地:防止我们的模型陷入这些狭窄的深度最小值的技术。

有许多正则化技术，如删除、数据集扩充和提取。然而，为什么要在充分利用我们现有的工具之前增加我们的工具箱呢？我们的工具总是存在，学习率和批量大小，可以为我们执行一定程度的正规化。

# 找到广泛的最小值

![](img/5e512d94368819a87516b6bcb1d53f32.png)

朱莉·莫里弗在 [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral) 上的照片

正如前面的例子所显示的，将学习率降低到一个非常低的数值，很容易陷入局部极小值，而这个极小值可能会过拟合。

通过**增加学习速率**，我们实现了很少讨论的好处，即允许我们的模型**摆脱过度拟合的最小值**。该模型将错过局部最小值，并找到更宽、更平坦的最小值。

我们仍然需要确保学习率不要太大，潜在地利用像自适应学习率这样的技术。然而，最终，我们希望确保小的学习率不会使我们的模型在培训中看起来很好，而在其他地方很差。

类似地，**减少批量会增加收敛的噪声**。样本越小，彼此之间的差异就越大，因此上述地形上的收敛速度和方向就越易变。因此，该模型更有可能找到更广泛的局部最小值。这与采用大批量甚至所有样本数据形成对比，后者导致平滑收敛到深度局部最小值。因此，较小的批量可以为模型提供隐式正则化。

# 摘要

对于神经网络的正则化技术已经有了大量的研究。研究人员甚至质疑这种技术是否有必要，因为神经网络似乎显示出隐含的正则化。然而，在应用其他正则化步骤之前，我们可以重新想象学习率和批量大小的作用。这样做可以减少过度拟合，以创建更好、更简单的模型。

更高的学习率和更小的批量可以防止我们的模型陷入深度狭窄的极小值。因此，它们将对我们的训练数据和真实世界数据之间的变化更加稳健，在我们需要它们的地方表现得更好。