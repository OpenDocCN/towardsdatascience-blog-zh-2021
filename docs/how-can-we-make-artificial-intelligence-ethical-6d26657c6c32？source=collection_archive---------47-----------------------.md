# 如何才能让人工智能具有伦理性？

> 原文：<https://towardsdatascience.com/how-can-we-make-artificial-intelligence-ethical-6d26657c6c32?source=collection_archive---------47----------------------->

## [人工智能校准和安全](https://towardsdatascience.com/tagged/ai-alignment-and-safety)

## 为了一个更加公正的世界，集体努力是必要的。

![](img/224cb16de39271ead8a6d8db612f408a.png)

[附身摄影](https://unsplash.com/@possessedphotography?utm_source=medium&utm_medium=referral)在 [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral) 上拍照

上周，我们在我的研究生院入门课程中完成了一项令人大开眼界的活动。在某些情况下，本课程旨在介绍人机交互(HCI)及相关领域的不同研究范式。我们在前半个季度讨论了高质量研究的高层次要素，最近还讨论了衡量学术研究的道德和可信度的方法。

在活动中，我们的教授让我们每个人分析一篇选择的研究论文，并写一篇 300 字的短文，讨论研究中直接出现或隐含的伦理问题。然后，我们将所有的文章汇集成一个小小的“虚拟杂志”,在阅读学术论文时，可以作为未来的快速参考。

最终的结果令人着迷，特别是因为我们能够发现许多伦理问题仍然存在于实际的、*发表的*研究中。以下是人们发现的一些问题:

*   根据一个不具代表性的小样本的数据做出一般性的断言
*   引用过时的研究
*   不尊重收集数据的隐私
*   对被边缘化的社区进行研究，但之后再也没有回来真正帮助那些同样的社区
*   使用不可靠的数据(例如，在 MTurk 上收集的数据没有正确衡量用户是否真正回答了问题)

我们的伪杂志故意泛泛而谈——在翻阅它时，我忍不住将所有讨论的问题与我自己的计算机科学背景联系起来。特别是，我考虑了上述问题中有多少与人工智能(AI)有关。

人工智能多年来一直是一个蓬勃发展的领域，但最近人们对道德人工智能越来越感兴趣。这是由于意识到许多当前存在的人工智能形式都有很强的偏见——例如，有证据表明[面部识别算法在肤色较深的人身上表现较差[1]。](https://sitn.hms.harvard.edu/flash/2020/racial-discrimination-in-face-recognition-technology/)

![](img/268464222eccdbc85c4363a19c6ad24c.png)

简·坎迪在 [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral) 上拍摄的照片

这个问题源于人工智能算法的训练数据。当模型基于不完整或不准确的数据进行训练时，不管它有多好，它的预测能力总是很差。

这就是我们在课堂上做的活动。在这个过程中，我问了自己一个问题:在寻找符合伦理的人工智能的过程中，我们能扩展并建立这个策略吗？如果是这样，我们从哪里开始？

我相信答案是肯定的，我们从确定我的班级活动的一个重要方面开始，这使它非常独特:**丰富多样的观点**。

让我详细解释一下。我的研究生项目是跨学科的，接待来自不同背景的学生。在这个房间里，我们有专攻计算机科学、社会学、UI/UX、经济学、平面设计、公共卫生、数学等等的学生。我们探索的研究论文跨越了从信息可视化到价值论(哲学的一个分支，关注什么使事物有价值)的整个范围。正因为如此，我们能够创造出一种既具体又全面的杂志——具体的原因在于它让我们有效地探索当代研究中的伦理问题，而全面的原因在于它允许我们在一系列不同的研究领域中这样做。

这让我想到了我的主要观点:在研究的初始阶段纳入不同的观点对于创造人工智能的伦理形式至关重要。如果一个 AI 技术有偏差，那是因为底层数据也有偏差。很多时候，这可能是无意的，因为研究人员根本没有意识到他们的数据收集技术的缺陷。如果你不知道偏见，你怎么可能解决它？通过让来自不同背景的人参与这些算法的创建，我们可以确保最终产品是合乎道德和包容性的。

让我们回到面部识别技术的例子。上面链接的文章中提到的算法在深色皮肤的女性身上表现不佳。虽然我不能对设计这一算法的研究团队做出直接的声明，但有充分的证据表明，女性和有色人种在 STEM 领域，尤其是计算机科学领域的代表性不足。因此，由于团队自身的同质性，团队没有注意到其较差的训练数据并非不可能。

要让人工智能合乎伦理，就要从一开始做起。创建一个算法，然后回头考虑如何使它合乎道德和包容性，这是不够的。相反，我们应该在研究的初始阶段使用新技术，并将多种观点融入到我们的算法中。为此，我们需要两个关键因素:

1.  包括来自所有社区的研究人员的研究团队，这将更容易确保产生的算法不是无意的歧视。
2.  使用混合方法的研究团队——换句话说，结合从事创建和编程算法的数学工作的定量研究人员，我们需要能够设计更好的数据收集和分析方法的定性研究人员，这些方法考虑到人工智能的*人类*方面——而不仅仅是机器。

当然，这本身还不够——但这是一个开始。人工智能在近期没有放缓的迹象。如果我们不学会让它合乎道德，那么在 21 世纪剩下的时间里，等待我们的将绝不是好事。

# 参考

[1] A .纳吉比，*人脸识别技术中的种族歧视* (2020)。https://sitn . HMS . Harvard . edu/flash/2020/种族歧视人脸识别技术/