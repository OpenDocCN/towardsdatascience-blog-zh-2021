# 对齐问题

> 原文：<https://towardsdatascience.com/the-alignment-problem-2c890c47fca4?source=collection_archive---------22----------------------->

## [播客](https://towardsdatascience.com/tagged/tds-podcast)

## 布莱恩·克里斯蒂安谈人类最大的——也是最后的——挑战

[苹果](https://podcasts.apple.com/ca/podcast/towards-data-science/id1470952338?mt=2) | [谷歌](https://www.google.com/podcasts?feed=aHR0cHM6Ly9hbmNob3IuZm0vcy8zNmI0ODQ0L3BvZGNhc3QvcnNz) | [SPOTIFY](https://open.spotify.com/show/63diy2DtpHzQfeNVxAPZgU) | [其他](https://anchor.fm/towardsdatascience)

*编者按:这一集是我们关于数据科学和机器学习新兴问题的播客系列的一部分*，*由 Jeremie Harris 主持。除了主持播客，Jeremie 还帮助运营一家名为*[*sharpes minds*](http://sharpestminds.com)*的数据科学导师初创公司。*

2016 年，OpenAI [发表了一篇博客](https://openai.com/blog/faulty-reward-functions/)，描述了他们的一项人工智能安全实验的结果。在这篇文章中，他们描述了一个经过训练以在赛船游戏中最大化其分数的人工智能如何最终发现了一个奇怪的黑客行为:人工智能没有尽可能快地完成比赛线路，而是了解到它可以通过绕过一系列目标来获得基本上无限数量的奖励分，在这个过程中，它需要撞上障碍物，甚至在线路的某些部分以错误的方向行进。

这是对齐问题的一个很好的例子:如果我们不非常小心，我们可能会训练人工智能找到危险的创造性方法来优化我们告诉他们优化的任何事情。因此，构建安全的人工智能——与我们的价值观一致的人工智能——需要找到非常清楚和正确地量化我们希望我们的人工智能做什么的方法。这听起来似乎是一项简单的任务，但事实并非如此:几个世纪以来，人类一直在努力定义经济健康或人类繁荣等方面的“好”指标，但收效甚微。

今天的播客专题是 Brian Christian——他是几本关于人类与计算机科学和人工智能之间联系的畅销书的作者。他的最新著作《排列问题》探索了排列研究的历史，以及如果我们打算安全地将推理外包给机器，我们将不得不回答的技术和哲学问题。布莱恩对对齐问题的观点将我们迄今为止在播客中探索的许多主题联系在一起，从人工智能偏见和伦理到人工智能的存在风险。

以下是我在对话中最喜欢的一些观点:

*   没有人知道人类水平的人工通用智能将如何或何时被开发出来，或者它对人类的影响将是什么。但两种情况中的一种似乎是可能的:要么 AGI 将在一个特定的研究实验室中展开的训练过程中，在几天甚至几分钟内迅速出现(快速起飞)，要么它将由许多不同的组织逐步发展，在其发展过程中没有任何一个阶段明确代表越过 AGI 终点线的关键时刻(缓慢起飞)。布莱恩倾向于认为后一种情况更有可能。
*   布莱恩指出，在某种程度上，人类在其历史的大部分时间里一直在解决排列问题。国家和大型组织必须保持其劳动力和人口的一致性，朝着使其具有经济竞争力的目标努力。通常，这些目标是用狭义的指标(GDP、年利润或股市表现)来表达的，但这也产生了狭义指标定义对机器学习系统产生的所有相同问题。具体来说:一个单一的指标很少能捕捉到我们希望一个系统(一个人工智能，一个公司，一个国家)做的所有事情。让一个人工智能最大限度地生产回形针，你就给了它杀死任何可能与它竞争原材料的人的动机。要求政府最大化其经济的 GDP，任何没有被 GDP 捕获的东西(如生活质量、污染或非市场交易)都有被忽视的风险——或者更糟，被积极伤害。
*   像许多人工智能安全研究人员一样，布莱恩不相信我们会通过定义狭窄的指标并要求人工智能系统优化它们来构建安全的人工智能。这就是为什么他对一种叫做逆向强化学习(IRL)的潜在替代策略感到兴奋。IRL 系统的设计不是为了优化一个给定的指标，而是为了学习他们自己的人类价值模型，然后为此进行优化。通常，这是通过基于观察人类行为来推断人类偏好来完成的。例如，一个团队通过让强化学习代理随机执行两个动作来训练它做后空翻，让一个人选择他们认为最“像后空翻”的动作，并在重复这个过程之前相应地更新它的后空翻策略。虽然 Brian 不认为这是对齐问题的完整解决方案，但他认为这可能是其中的一部分。
*   度量标准在远处看起来很好，但是当你开始对它们进行太多的积极优化时，它们很快就变成了你真正想要的东西的糟糕代理。以股市为例:在 20 世纪初，可以肯定地说，只要股市在上涨，美国经济就会好转。然而，在 2020 年，我们已经看到美国股市飙升至历史新高，尽管失业率极高，经济基本面可能是负面的，部分原因是选民和政治家太关注它，将其作为判断政府表现的指标。于是黑客被发现，钞票被印制，美联储比率被降低，有效地将股票市场与它本应衡量的经济状况脱钩。布莱恩讨论了一种称为早期停止的策略，可以用来部分解决这类问题:这个想法是训练一个人工智能系统来优化一个指标，但不允许它优化太多——换句话说，就是尽早停止。
*   我们讨论了为什么人类可能是进化早期停滞的一个例子，从进化的角度来看，我们不是一个“成品”。在某种程度上，这可能是为什么很难确定可以持续优化以使我们快乐的指标:我们可能实际上没有如此清晰和明确定义的偏好，因为我们是进化的过客——只是足够一致以在进化过程中竞争我们的位置，但不够一致以拥有一致的道德偏好集。很难用几句话就把对话的这一部分(以及所有适用的注意事项)说清楚，所以如果你感兴趣，我强烈建议你看看这一集。

你可以[在 Twitter 上关注 Brian 这里](https://twitter.com/brianchristian)，或者 [me 这里](https://twitter.com/jeremiecharris)。

![](img/22534214f9a6a6d3e88173676a95fcaf.png)

## 章节:

*   0:00 介绍
*   1:35 对准问题概述
*   7:02 AI 安全范围内的摩擦
*   10:30 AI 与决策
*   12:30 大赦国际和刑事司法
*   改变的激励措施
*   23:20 当代问题和存在主义考虑
*   30:45 存在风险的出现
*   38:02 组织味的对齐问题
*   42:20 对准问题成为一切问题
*   50:00 我们的各种偏好
*   56:20 人类的模糊
*   1:03:10 宇宙的奖励功能
*   1:06:05 总结