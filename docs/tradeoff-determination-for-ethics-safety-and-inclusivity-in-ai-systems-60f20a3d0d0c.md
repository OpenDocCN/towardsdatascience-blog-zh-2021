# 人工智能系统中伦理、安全和包容性的权衡决定

> 原文：<https://towardsdatascience.com/tradeoff-determination-for-ethics-safety-and-inclusivity-in-ai-systems-60f20a3d0d0c?source=collection_archive---------28----------------------->

![](img/13d125d6a22f1571164453b72d57fb68.png)

由 [Startaê Team](https://unsplash.com/@startaeteam?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText) 在 [Unsplash](https://unsplash.com/s/photos/post-it-notes?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText) 上拍摄的照片

人工智能系统的设计决策包括价值判断和优化选择。一些与技术考虑有关，如延迟和准确性，另一些与业务指标有关。但是每一个都需要仔细考虑，因为它们对系统的最终结果有影响。

需要明确的是，并不是所有的事情都需要权衡。通常会对问题进行巧妙的重新表述，这样您就可以满足用户和客户的需求，同时也满足内部业务考虑。

以早期 LinkedIn 的一个功能为例，该功能通过要求关系向目标用户推荐特定的职位发布，基于他们认为这些职位发布对目标用户的合适程度。它通过只将相关工作分享给用户的关系，同时帮助 LinkedIn 向用户提供更多相关的推荐，为推荐用户提供了目的感和善意。这是一个双赢的局面，相比之下，必须不断地深入调查用户，以获得更多的数据，为他们提供更有针对性的工作建议。

本文将基于[在产品开发中设定目标以实现负责任的人工智能的重要性](/the-importance-of-goal-setting-in-product-development-to-achieve-responsible-ai-eda040809292)，为构建道德、安全和包容的人工智能系统增加另一个考虑维度。

# 为什么要权衡决定？

权衡决定的主要目的是突出项目各种目标之间的内在矛盾。

让我们考虑一个场景，其中业务指标是通过平台上的广告获得的收入。有助于此的一个设计决策是实现类似无限滚动的东西，通过不断提供更多穿插广告的相关内容，让用户尽可能长时间地停留在平台上。通过利用用户的个人数据，广告本身可以变得更有吸引力或与用户更相关，这可能会增加这些广告的点击率。

这里有很多顾虑。也许最明显的是侵犯隐私和扭曲用户同意如何使用他们的个人数据。另一个是对已知的黑暗设计模式的依赖，这种模式倾向于增加花费在平台上的时间，但不一定谈论这些时间的质量。

当你慢慢利用更复杂的模型时，你可能会遇到其他选择，比如偏差与方差的权衡，从可解释性的角度来看，你冒着使事情变得不透明的风险。如果你想证明你平台上的广告不是基于敏感属性的歧视，这可能很重要。更复杂的模型可能会提高性能，但代价是什么？

当在这里考虑权衡决定时，明确地强调这一点，即什么对用户有益，什么对商业有益，这是帮助有效解决问题的第一步。

# 如何有效地做权衡决定？

有 3 个初始步骤可以用来开始权衡决定。

## 1.明确列出权衡

正如上面的例子所强调的，当技术和设计决策的一阶效应清晰时，它们应该被明确地列出来。一旦做到这一点，采取一种系统思考的方法，考虑这些设计的二阶效应。

二阶效应很难追踪，而且可能以意想不到的方式表现出来。然而，他们通常要对很大一部分伤害负责，因为没有任何明确的保护措施来防止这些伤害。一个明确的列表也有助于让每个在系统上工作的人记住这些权衡。

## 2.对照你的目标检查这些决定

正如在[产品开发中设定目标对实现负责任的人工智能的重要性](/the-importance-of-goal-setting-in-product-development-to-achieve-responsible-ai-eda040809292)中提到的，目标可以作为北极星，让我们对自己负责，就我们试图用系统实现的目标而言。随着您对项目细节的深入了解，很可能细节会偏离项目最初的愿景和目标。这是引入由使用该系统导致的不必要的伤害的候选。

当您做出技术和架构决策时，经常对照目标进行检查，并思考它会导致什么样的权衡，可以帮助您确定正确的方向。对于那些熟悉约翰·伯伊德的 OODA(观察、定位、决定、行动)的人来说，这构成了东方的关键一步，以确保接下来的事情朝着正确的方向发展。

## 3.互补性与输赢情景

并不是所有支持商业目标的决策都必须是对用户福利不利的方案。回到 LinkedIn 工作推荐的例子，我们可以看到一个用同行推荐取代大量数据收集的设计如何继续满足希望人们因为相关的工作发布而使用平台的商业目标，而不需要诉诸侵犯隐私和滥用个人数据。

这是一个获得创造力的机会，随着更多的从业者进入人工智能领域，你可以将此作为自己的差异化品质:在不牺牲业务需求的情况下实现道德一致性。

# 在人工智能的生命周期中，你能为权衡决定做些什么？

一旦你完成了上面的一些步骤，你需要在人工智能生命周期中做的一件关键的事情就是监控权衡。虽然在一开始就考虑它们并做出符合道德考虑的选择是很好的，但鉴于人工智能系统的动态性和复杂性，如果没有持续的监控，你就有可能遭受突发伤害，从而削弱你工作的影响。

为系统的可接受行为设置阈值是实现上述目标的具体方法。这可能包括用户在一个平台上连续花费的时间，以及是否有一个简短的通知弹出来，要求他们伸展一下身体，到外面走走，可以打破消极的模式。我们已经在健身追踪器和安卓手机的通话屏幕上看到了类似的东西。

最后，仅仅确定权衡决定是不够的。是的，承认有问题总是第一步，但我们需要超越。做到这一点的方法是将可行的补救措施与您列出的每一项权衡相关联。这有助于涉众打破惰性，并有意义地按照建议行动，以改善系统结果。

希望这种权衡决定的想法是你觉得自然的，并且已经可以看到在你的 AI 生命周期的设计和开发阶段你可以集成它们。请在下面留下您的评论，告诉我们您是如何在您的组织中实践这一理念的。

> **在这里注册接收更新** [**可操作的人工智能伦理书**](https://actionableaiethics.substack.com/) **！而你可以在这里** **了解更多我的作品** [**！**](https://atg-abhishek.github.io/)
> 
> **您可以通过订阅** [**人工智能伦理简报**](https://brief.montrealethics.ai/) **从** [**蒙特利尔人工智能伦理研究所**](https://montrealethics.ai/) **了解更多这样的见解！**