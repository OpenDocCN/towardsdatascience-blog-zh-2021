# 强化学习背后的直觉

> 原文：<https://towardsdatascience.com/the-intuition-behind-reinforcement-learning-d88fccbb145b?source=collection_archive---------23----------------------->

## 强化学习的简短介绍

视频由作者通过 [vimeo](https://vimeo.com/555052663) 提供。这段视频显示了一名训练有素的特工试图通过改变车道和控制速度来避免迎面而来的车辆。这是用 DQN 算法实现的。

乍一看，强化学习似乎令人难以置信地势不可挡。状态、行动、环境、奖励、价值函数、Q 和过多的其他这样的术语和定义对初学者来说可能是相当大的负担；如果我告诉你，我们一生都在贯彻这个概念，真的，会怎么样？是的。你没看错！在日常生活中，我们几乎每天都在使用 RL 技术。通过这篇文章，我希望向你展示直觉强化学习是多么的真实，它在我们的生活中是多么的平常。也许这种方法有助于解开这个概念的神秘面纱。我希望专注于直觉而不是实现，所以我尽量少用数学公式。

## **举个例子**

在我们谈论术语和定义之前，让我们分析几个例子。

***考试*** :假设你明天有一场考试。由于一些紧急情况，你无法准备考试；所以，作为最后的手段，你可以查阅前一年的课程试卷。现在，在看完其中的几个之后，你开始注意到一个模式——超过 60%的分数分配给了前三章的问题，而剩下的 40 %平均分配给了其余五章。由于时间不够，你可以学习前三章，也可以学习后五章。你需要至少得 40 分才能通过这门课程。

![](img/466f8a43b39ca1bae9ee9ef9d4ccc479.png)

图片由 [Unsplash](https://unsplash.com/photos/qDgTQOYk6B8) 提供

现在我要问你的问题是--你会把剩余的时间花在哪些章节上？

当然是前三章！根据你对以前数据的分析，你计算出前三章很有可能会被分配相当多的分数，因此如果你有喘息的机会，你有更大的机会获得 40%的分数，所以即使你可能会犯一些错误，你也能通过这门课程。然而，如果你选择了剩下的 5 章，即使是一个简单的错误也会让你的生活变得困难。

## 术语

让我们更深入地研究这个算法提供的术语。

*环境:*

顾名思义，这就是环境，代理与之交互的空间。对我们来说，考试就是我们的环境。我们，代理人，与考试环境互动。这是我们需要我们的代理以特定方式进行交互的空间。我们使用算法来定义代理的行为。在开始给出的视频中，我们可以看到智能体(绿色汽车)与其环境(充满蓝色汽车的高速公路)进行交互。

*代理人*:

代理用于定义与环境交互的程序本身。在我们的检查示例*中，我们*是代理。在高速公路视频中，绿色汽车是与高速公路环境互动的主体

*动作:*

这是代理需要执行的特定操作。在我们的案例中，是在前三章还是后五章这两个选项之间做出选择。在汽车驾驶的例子中，如何引导交通是代理的责任，是减速、变道还是加速。

*奖励:*

最大化的概念是这个概念如此激动人心的原因。如果仔细观察考试例题，可以注意到我们得出了自己选择前三章的结论。与监督学习不同的是，在监督学习中，我们只有很少的几种情况，在这些情况下，我们有具体严格的指导，我们只知道这些情况(去年的试卷)，我们必须得出自己的结论。促使我们做出最终选择的是分数。我们的主要目标是最大化我们的分数或者说是奖励。正是这种最大化回报的激励是代理人的核心动机。正是这种动机允许代理人得出结论，几乎所有强化学习的算法都是围绕着以最有效的方式最大化回报。我们可以以某种方式定义奖励，以便向代理传达我们的意图。在考试示例中，我们的奖励被定义为考试中获得的分数，我们的目标是最大化它。我们所有的决定都受到这个主要动机的影响。在高速公路示例视频中，我以这样一种方式定义了奖励，即每当汽车学会转弯并提高速度时，奖励就会大幅增加。每当它崩溃或减速时，就会给出一个巨大的负奖励。通过以这种方式定义参数，我们实现了这样一种直觉，即汽车越能学会在高速下操纵而不撞车或减速，就越好，就像我们教代理人鲁莽驾驶一样。这就是奖励这个概念的强大之处。

*状态和历史:*

这是强化学习的另一个非常重要的概念。每当我们(代理人)与环境互动，执行一些操作(行动)时，我们观察环境中的反应，作为回报，我们从环境中获得奖励。就像根据我们选择学习的章节，我们会得到相应的分数，或者根据汽车选择做的事情(减速、加速、转弯等等)。)它得到了回报，我们看到它在高速公路上巡航，或者与另一辆车相撞。现在我们需要一种方法来记录这些观察，行动和在每一步得到的回报。这就是历史的由来。这是一系列的行动、奖励和观察，直到时间 t。

![](img/fdf1270f57c964e041768d0327740f3a.png)

t 时刻的历史[图片由作者提供]

代理将如何与环境交互取决于历史。它将分析历史，并根据其优先级决定下一步行动。在这一点上，很明显，保持所有观察、行动和奖励的顺序变得非常耗费空间和不切实际。我们需要的是对历史的简明概述。告诉我们代理需要知道的所有重要细节，但不要太不切实际。这正是国家所做的。国家本质上只是历史的一个函数。

![](img/eba6efe6b393da42279bc6ee63ef890f.png)

国家是历史的函数。[图片由作者提供]

我们使用一个非常特殊的假设，马尔可夫状态，来定义函数。马尔可夫假设主要是这样的:

![](img/66c3367604780dde505ec19abfd8cbc3.png)

马尔可夫状态定义。[图片由作者提供]

这意味着，鉴于现在，未来独立于过去。因此，鉴于目前的状态，我们可以准确地预测未来，从而使历史变得多余。这就是革命性的马尔可夫假设。例如在高速公路汽车的例子中，过去发生了什么并不重要。只要我们能够正确地分析当前的情况，即蓝色汽车的位置和速度，我们就没事(我们也可以将其视为一个序列模型，但这是一个不同的问题)。

*政策与价值函数:*

策略是描述代理行为的函数，而价值函数是告诉我们处于特定状态的好坏程度的度量。它有助于预测我们在特定状态下可以获得的回报。

![](img/b1509783f217bb63fdd82096a728038d.png)

pi 所代表的策略。它基本上将一个状态映射到一个特定的动作。[图片由作者提供]

![](img/812516dfe803cb23178d2a6b94b6affa.png)

这是给定政策π和状态 s 在 t 时刻的价值函数，是我们希望在未来得到的回报的预期(t+1，t+2 …等等)。**贴现因子** **γ** 用于在估算我们的期望值时，维持我们对未来回报的重视程度。它介于 0 和 1 之间。0 意味着我们只关心在时间步 t 返回的回报，而 0.99 的贴现因子意味着未来的所有后续步骤都很重要。[图片由作者提供]

如果我们观察这个方程，我们就能知道价值函数到底是什么。它估计在给定政策的情况下未来获得的回报。价值函数的公式有许多不同的变体，它可以以各种不同的方式应用，但事实是它的主要功能是为我们提供一种方法来估计我们希望获得的未来回报；本质上就是我们用数学公式来预测未来。很漂亮不是吗？

## 结论

希望这个简短的介绍能让你对强化学习的直觉有所了解。这确实是一个美丽的概念，它可以用于许多不同的领域，包括机器人，游戏，股票市场等。我强烈推荐查看大卫·西尔弗关于强化学习的[播放列表](https://www.youtube.com/watch?v=2pWv7GOvuf0&list=PLqYmG7hTraZDM-OYHWgPebj2MfCFzFObQ)，以了解强化学习的数学和实现。如果你想看强化学习的实践，请访问我的 [repo](https://github.com/19-ade/RL_dino_try) ，在那里我使用 Deep Q Network 训练了一个代理来玩谷歌 chrome dino 游戏。