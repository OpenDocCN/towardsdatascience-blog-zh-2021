# 在 PyTorch 中实现高效的广义核感知器

> 原文：<https://towardsdatascience.com/implementing-an-efficient-generalised-kernel-perceptron-in-pytorch-9e9fa6b30761?source=collection_archive---------8----------------------->

## 理论、实现和实验

![](img/393c741cd10085c9c52bf265cabe795c.png)

图片来自[模型图](https://unsplash.com/@mockupgraphics)

感知器是一种古老的线性二进制分类算法，它形成了许多机器学习方法的基础，包括神经网络。像许多线性方法一样，核技巧可以用来使感知器在非线性数据上表现良好，并且与所有二进制分类算法一样，它可以推广到 k 类问题。

我第一次实现感知器算法是在 Numpy，我在 MNIST 数据集上进行实验，以确定一般化模型在各种任务上的表现。不幸的是，当我写我的初始实现时，我生病了，所以它充满了错误，非常慢，而且占用大量内存。休息了一段时间并更好地思考了这个问题后，我决定再试一次，但这次使用我新喜欢的库:PyTorch。

本文从简单介绍感知器算法和内核化版本开始。然后深入到 k 类的归纳、实现和优化，最后到实验，在实验中，通过多种测试来确定模型在一系列任务中的性能。本文以我在处理这个问题时遇到的一些陷阱和关键要点作为结尾。

# 感知机是如何工作的

## 在线学习与批量学习

感知器使用在线学习来优化其权重，这与大多数其他使用批量学习的机器学习模型不同。这些方法的关键区别在于如何计算权重。

利用批量学习，基于所有数据一次性计算成本函数，结果是在同一个步骤中更新**权重。**

![](img/b67860c7fdb50f3aec099ae35ad38cfa.png)

例如，在梯度下降法中，我们计算批次(或最小批次)J 的总成本，然后求出它相对于重量的导数。然后我们一次性更新权重向量。λ是步长。

在在线学习中，数据是按顺序输入的，因此，**权重会随着每个新的数据点**而更新。

![](img/299a5de4d4be344c9c35c1e066f6f5e8.png)

数据被视为一个序列(如时间序列)。因此，下一个权重由当前权重和当前数据点(也可能包括所有先前的数据点)确定

记住这一点，让我们看看感知器的训练算法是如何工作的:

![](img/afed3b9409fe7e2097dc38c90d801c7d.png)

单个时期(通过数据的一个完整周期)的感知器训练算法。如果你有兴趣了解更多关于模型的复杂性，请访问这个[讲座](https://www.cs.utah.edu/~zhe/pdf/lec-10-perceptron-upload.pdf)。

请注意该模型的几个关键方面:

*   由于点积运算，该模型只能学习线性决策边界
*   对于 **wt** **点** **xi** 为零的情况，符号函数具有模糊性。在这种情况下，我们可以随机选择-1 或 1 作为输出，或者我们可以选择一个，例如≥ 0 → 1，< 0 → -1 或> 0 → 1，≤ 0 → -1(我们将在后面修改)
*   由于学习是在线的，我们不能对 for 循环操作进行矢量化
*   λ是学习率
*   第一个权重始终为零

现在，对于使用模型的预测，我们使用“离线”方案，也就是说，我们不再有更新方法。因此，我们的测试简单地由下式给出:

![](img/6f0d33a431114e467f2b0e8f13456a80.png)

由于这个方法是离线的，我们可以将 for 循环转换成一个有效的矢量化实现

感知器的一个明显的局限性是，点积的线性使其无法捕捉数据中的非线性相关性。

幸运的是，内核方法可以用来纠正这一点。

## 内核方法的修订

在数学中，核是一种允许你从线性空间映射到复杂度为 **O(n)** 的非线性空间的函数，而不是非线性空间所需的复杂度。
假设我们有一个包含 3 个特征的数据点，我们希望将其映射到一个非线性空间中:

![](img/dffc21b60c1ee9f5bd4fd4a1dd0bcac6.png)

如果我们取 **x** 自身的点积，我们就得到一个线性映射:

![](img/ed6f03f9776344763b33b8eb6081524d.png)

不要被方块弄糊涂了。二次项确实是“非线性”的，然而这里我们所说的线性是指关于特征的线性，也就是说，我们是否有特征的交叉相互作用？在上述情况下，答案是否定的。

然而，如果我们平方它，我们得到下面的非线性映射:

![](img/b41f056eb7994d6a8d12dfff5feab7f3.png)

我们的非线性项来自 x1 和 x2、x1 和 x3 以及 x2 和 x3 的相互作用。

这是强大的，因为:a)我们现在已经捕获了非线性项，b)我们只需要复杂的点积运算。

如果我们想在不使用核函数的情况下进行这个映射，那么我们的复杂度将为 **O(N)** ，其中 N=6，因为我们有 6 个新的特征(映射的大小)。

所以形式上，我们把内核写成:

![](img/7292e9b8d4222a418ba2b6c0c27ff4a0.png)

这只是一种类型的核，多项式核。

当使用内核方法时，关键的区别在于测试模型。代替**符号(w_star** **dot x_test)** 用于预测**，**我们有:

![](img/cdee9c304a65535a6e1e14b1643888b5.png)

其中**α**是与每个训练点相关联的权重(在训练期间确定)，而 **K(x_test，x_j)** 是测试点和训练点 **x_j** 之间的核。

> **注意:**内核是机器学习的一个活跃领域，它随处可见。如果你对它不熟悉，我强烈建议你深入研究一下，了解一下执行线性回归的对偶和原始形式(Tibshirani 的[统计学习元素](https://hastie.su.domains/Papers/ESLII.pdf)是一个很好的参考)

## 核感知器算法

内核感知器训练算法如下所示:

![](img/2faf0c659994a8aecd5c30e0a93f098e.png)

更多详情见此[链接](http://aritter.github.io/courses/5523_slides/kernels.pdf)

和测试:

![](img/c8f938d26ca91e0c34c0ce21b4dae450.png)

# 推广到 k 类

一般来说，我发现有两种方法可以概括二元分类器，它们是:

*   **One vs. All(或 OvA):** 这个方法创建了 k 个分类器，每个分类器都被训练来挑选一个类，而不是其余的类。这意味着对于每个类 **c_i** ，创建一个合成训练集 **y`** ,其中:

![](img/70276bc6a8b21243657a34dc8a60c609.png)

在预测过程中，我们可以让经过训练的模型“竞争”，看看哪个类最适合给定的 **X_test** 。因此，预测由下式给出:

![](img/c849924eeb2ba433f62106a9725efb3e.png)

这里 H_i 代表正在讨论的分类器。我们找到对我们的输入给出最大预测的分类器，然后找到它出现的索引。该索引对应于我们的数据所属的类别！

> 重要的是，要注意这种方法要求分类输出是连续的。因此，当使用感知器算法时，我们不能在预测步骤应用**符号**函数。

对于每个历元，该算法的复杂度为 **O(Mk)** 。其中 **M(m，n)** 是所使用的二元分类器的复杂度。

*   **一对一(或 OvO):** 这种方法创建 k(k-1)/2 个分类器，每个分类器在每一个类的组合上被训练。与上面类似，创建一个合成数据集。然而，这里我们在应用转换之前，在每一步过滤数据，只包括属于 **c_i** 和 **c_j** 的类。

为了更好地理解这一点，让我们考虑一个 10 级的例子(数字 0-9，如在 MNIST)。我们将有以下组合:(0，1)，(0，2)，(0，3)，……(1，2)，(1，3)……(8，9)，因此有以下分类器:

![](img/ac8a40edfc36650f646e96be7f7f8f9e.png)

记住每个 H_ij 是一个分类器。这里 H 代表我们必须训练的分类器集合。注意，我们不需要为排列进行训练，例如(0，1)和(1，0)，因为从(0，1)分类的输出，我们可以推导出类 0 和类 1 的预测！

所有这些都是在过滤的数据集上训练的，**y _ filtered = y[(y = = I)|(y = = j)]**。变量的变换是这样进行的:

![](img/3ee04e591204ec8dc6e10becbd0ff587.png)

最终分类输出由下式给出:

![](img/237b311deea68b4def0e2c92db727e97.png)

与“一个对所有”不同，分类器之间并没有太多的“竞争”,而是对分类输出进行“投票”。有了这个公式，我们可以将**符号**函数应用于感知器的输出，因为我们关心计数。或者，如果我们决定不应用**符号**，那么我们将取一个加权和，其中感知机输出的值是模型对我们预测的数字的信任度(这里它的行为更像一个‘竞争’)。

这个模型的复杂性更难确定，因为 m 对于每个组合都是可变的(记住我们使用的是过滤数据集！).假设类是均匀分布的情况，每次迭代的数据集大小 m_ij = 2m/k。因此，复杂度是 **O(Mk(k-1)/2)** 其中 **M=M(2m/k，n)** 是二元分类器的复杂度。

## 算法在我们的数据上会有什么不同？

为了更直观地了解每种算法的工作原理，让我们检查一下数据。

[MNIST](https://en.wikipedia.org/wiki/MNIST_database) 数据集是从 0 到 9 的手写数字的集合。每个图像的大小为 16x16，但为了建模，它们被展平。因此，每个图像由一个 256 长的向量表示。数据集包含大约 9000 张这样的图片。

为了更好地理解 256 个维度代表了什么，我用 TSNE 在 2D 笛卡尔坐标轴上绘制了这些图像。

![](img/2a62efe7e258674ef4b85c29cb2be814.png)

图 1:MNIST 数据集的 TSNE 嵌入。请注意，不同类之间有很多噪声。每个簇代表一个数字(见图例)。如果你不熟悉 TSNE 的做法，可以把它看作是一种将数据“压缩”到更小维度的方法。在这种情况下，每个 256 像素的图像被“压缩”成二维，这样我们就可以在笛卡尔(x，y)平面上绘制它。(注意添加 MNIST 图片)

对于一对所有分类器，每个分类器学习预测一个数字对其余数字。例如，我们的第一个分类器将尝试预测给定的输入是否为 0。肯定的分类是“+1”，例如模型认为输入图像是 0，而不成功的分类是“-1”，例如模型认为图像不是 0。下图显示了分类器对 MNIST 数据集的作用。

![](img/c3de07f20b75300986944cae70c83274.png)

图 2:对于每一个类，该模型学习一个决策边界来将它与其余的类分开

在一对一的情况下，我们正在学习使用在较小数据集上训练的 k(k-1)/2 个分类器来预测每个类别组合。下面提供了一个关于 MNIST 数据集的示例。

![](img/1f28bf8020f348679604e206f45b35cc.png)

图 3:对于每一个类的组合，模型学习一个决策边界来将它们彼此分开。最终输出是所有分类器的“投票”。在这种情况下，k=10，所以我们有(10)(9)/2 = 45 个分类器！然而，仅仅因为这听起来很多并不意味着它一定会更慢。回想一下复杂性。

## 理论性能比较

以上两种方法都是将二进制分类问题推广到 k 类的合理方法。然而，它们都可能遭受所做预测的不确定性。在一对一分类器和加权一对一分类器中，在应用 **argmax** 操作之前，输出权重的值可能非常相似。在非加权一对一分类器中，两个类别可以具有相同的投票数。

总的来说，我认为 OvO 模型比 OvA 模型表现得更好，因为后者存在阶级不平衡的问题。考虑到数据集中的 *k* 类同样频繁。这意味着在那次培训中，你的班级比例将是 1:*k*1，分别代表*y*′= 1 和*y*′= 1。这对于大值的 *k* 来说是非常不平衡的。此外，由于所有其他类都被集中在一起，因此将一个类与所有类分开的边界可能会很复杂，也更难预测。另一方面，一对一模型在数据集很小且噪声很大的情况下可能不太有效。对于上述的相等类别分布，一对一模型具有实际训练集的 2/ *k* 的有效训练规模。如果成对的类非常嘈杂，并且由于数据集很小，由于类的高度重叠，决策边界将很难区分。在这种情况下，一个对所有分类器更多地强调训练样本的其余部分，而不是噪声。

然而，我的实验结果只是部分证实了上述观点，我有兴趣进一步探索这个问题。

# 优化

如果设计不当，感知器和广义分类器的计算效率会非常低。仅对于感知器，你循环通过 **N** 个时期的数据，每个时期通过 **m** 个数据点，计算总计为 **m** 和内核 **K(xi，xj)** (这需要 **n** 个运算，因为它是基于点积的)。在最坏的情况下，复杂度是 **O(Nm n)** ，如果不使用矢量化，操作会非常慢。盲目推广感知器只会让情况变得更糟。对于一个对所有分类器，一个简单的实现将重新初始化感知器 **k** 次，导致总体复杂度为 **O(Nm nk)** 。对于一对一分类器，假设类分布相等，我们得到复杂度为 **O(2Nm n(k-1)/k)** 。

因此，我们仔细研究了这个问题，以确保 a)尽可能多地使用矢量化，b)尽可能少地重复概括。

## 感知器优化

首先，注意到内核可以通过创建一个 [Gram 矩阵](https://en.wikipedia.org/wiki/Gram_matrix)来矢量化(例如，一个矩阵，其中每个单元是 **xi 点 xj** 的函数)。因此，代替在训练循环期间为每个数据点计算 **K(xi，xj)** ，我们可以预先创建矩阵 **K=(K(xi，xj) : i，j = 1 … m)** 。这样，总和:

![](img/acbd8fc161a7d4410124e7e6c3d080fc.png)

计算方法如下:

![](img/31f42982b89b4923988026a05dc12d96.png)

其中 **Ki** 是指克矩阵的第 **i** 行。当然，优势在于内核计算现在相对于数据集的循环是线性的，并且它一次使用 PyTorch 的后端向量操作。这意味着复杂度现在是**O(m ^ n+Nmn)**而不是**O(Nm ^ n)**，其中第一项 m ^ n 是完全向量化的运算。

对于多项式核，创建 Gram 矩阵非常简单，因为它在表达式中使用了点积。

对于多项式核，这相当简单，因为格拉姆矩阵由下式给出:

![](img/0c35642fbbbcacc22a7955e3445da95e.png)

我们可以将此改写为:

![](img/1d0f48dc80051523a76ab1b5cedc8dfc.png)

记住一个矩阵(m，n)乘以它的转置将得到一个矩阵(m，m ),其中每个单元是相关向量的点积！

对于这个项目，高斯核也被使用。它由下式给出:

![](img/11436050dd13ab515a387b2776e8c7c5.png)

这更难以向量化，因为范数有一个减法运算。之前在我的 **numpy** 实现中，我尝试将 **X** 传播到第三维，然后减去 2D 版本，最后使用求和操作将第三维折叠回来。但是，这种方法不适合大型数组，因为它占用太多内存。也是扑朔迷离，牵扯其中。

对于 PyTorch 实现，我做了更多的思考，我意识到范数运算可以写成点积的和:

![](img/c93380375baaebb3fcaf435d5587b231.png)

因此，高斯核可以由下式给出:

![](img/3df1a8f88b60112b920b13612076b617.png)

两个内核都被有意地写成这样，以确保操作可以应用于具有形状 **(m1，n)** 和 **(m2，n)** 的任意两个矩阵 **X1** 和 **X2** ，以确保在创建预测 Gram 矩阵时，它们可以在预测期间被重用。

## 一对所有优化

这里有两个主要的认识。首先，不需要每次都创建 Gram 矩阵，因为输入 **X** 总是相同的。因此，不是复杂度为 **O(k(m n+Nmn))** ，而是复杂度为 **O(m n+Nmnk)** 。第二，在预测而不是重复期间

![](img/f046face6b8e449968c5f7b6c754c204.png)

对于每个分类器，保存的字母被连接起来并用于执行单个操作:

![](img/153bc4dc183e13e3378f16922ec4d572.png)

在应用 **argmax** 函数之前，给出数据的正确格式。

## 一对一优化

理论上，因为内核方法使用对偶公式，所以在过滤的数据上训练 **k(k-1)/2** 分类器应该比在未过滤的数据上训练 **k** 分类器表现得更好(就像一个对所有分类器一样)，因为我们在小得多的 **m** 上训练。但是，因为 k 分类器只使用单个 Gram 矩阵，所以在我们的实验中它们更快。标准一对一实施无法重复使用 Gram 矩阵，因为基于 and **ci** 和 **cj** 对 **X** 进行过滤。

我确实尝试过改进这一点，通过实现一个积极矢量化的一对一算法的替代公式。这就对感知器做了一点小小的修改，所以我们不使用 if 语句来更新权重，而是这样做:

![](img/20e95ef5f9a9d19a10294015b8e5130c.png)

如果 **yt** 在 **[-1，1]** 中，那么 alpha 按预期更新。然而，如果我们允许 **yt** 包含 0，那么当 **yt=0** 时，不管分类输出如何，权重将保持不变。因此，模型忽略了 **yt=0** 的数据点。

我们进行以下转换，而不是上述转换:

![](img/a12b7087fcb67a35e3bc81b9f76e8fc7.png)

这允许我们在训练时输入未经过滤的 **X** 。因此，尽管我们在全尺寸数据集 **m** 上训练 **k(k-1)/2** 个分类器，但我们不必重新初始化 Gram 矩阵 **k(k-1)/2** 次，我在实验中使用了单个时期，因此时间复杂度中的主要项是 Gram 矩阵的时间复杂度，因此这种实现导致了稍微更快的结果。然而，一般来说，情况可能不是这样，速度的瓶颈来自训练循环，而不是来自创建 Gram 矩阵，这是一种矢量化操作。对于非常大的 **k** 值，这可能很有用。

# 实施策略

基于以上所述，我们可以通过多种方式为不同的任务设计感知器，总结如下:

*   基于二进制数据训练的内核感知器，具有高斯或多项式内核
*   使用一对一公式在 k 类上训练的广义感知器
*   使用一对一公式在 k 类上训练的广义感知器
*   以上既可以用加权票，也可以用等额票
*   它还可以选择使用积极的矢量化

为了在不编写高度特定的代码的情况下促进所有上述功能，模型是使用 Python 类编写的，并遵循带有`fit`和`predict`方法的`sklearn` API 风格。这实现了“即插即用”功能，使得执行子任务更加容易。

感知器初始化取入`epsilon`、`epochs`、、`kernel`和`**kernel_kw`。ε是添加的术语，因此:

![](img/3be4e091f2b8d796371aa4e4f3c7d156.png)

这是因为**符号(0)** 不明确，所以`epsilon`允许用户在以下两者之间选择:

![](img/3224ee51d015d65d3cc0154ae9e56401.png)

或者

![](img/c75fd47ad177f8ef4c332409c4f05b38.png)

`kernel`是为以下输入实现特定内核(例如多项式或高斯)的函数:

*   矩阵输入:(m1，n)和(m2，n) →输出 Gram 矩阵形状:(m1，m2)
*   向量输入:(n，1)和(n，)→输出核形状:(1，1)
*   矩阵和向量输入:(m1，n)和(n，)→输出 Gram 矩阵形状:(m1，1)

并且`**kernel_kw`是与核相关联的任何参数，因此`d`用于多项式核， **c** 用于高斯核。这种编写函数的方式允许使用多个参数。

`fit`方法是标准的，不需要解释，除了权重被初始化并保存为类变量。`predict`方法也以标准方式工作，但是有两个额外的可选参数，`weights=None`和`return_labels=True`。前者在 OneVsAll 和 OneVsOne 期间使用，在预测步骤使用来自每个分类器的训练权重。默认情况下，它是 None，这意味着使用模型中包含的权重。return_labels 确定感知器的输出是否是预测的强度

![](img/94bbb67e91759a535bb9fa9177aa7d02.png)

或者实际的预测

![](img/78583d212b52b3414474a65163c14e4a.png)

这使我们能够很容易地指定一个对所有或一个对一个的不同行为。OneVsAll 和 OneVsOne 的类遵循类似的结构，并被编写为与任何使用操作 **M 点 w** ( **M** 是矩阵， **w** 是权重)来确定分类输出的二元分类器一起工作。这意味着这可以很好地用于原始公式，只要这些模型遵循与上述感知器算法相同的风格。

# 实验

## 多项式与高斯核

多项式核和高斯核训练和测试精度使用一个对所有实现进行了比较。多项式核的范围是 d=1…7，高斯核的范围是 c = 0.01…0.5。结果以及 d 和 c 的最佳值(分别为 d_star 和 c_star)如下表所示。

![](img/44b85dc4e95365574ea839d789e76c3b.png)

表 1:具有多项式内核的 OvA 感知器的训练和测试误差

![](img/e4af31c72706465974e6ee8da061f383.png)

表 2:具有多项式内核的 OvA 感知器的最佳 d_star，以及相应的测试误差

![](img/e368408a180a7fcb9e21c03594fc2250.png)

表 3:具有高斯核的 OvA 感知器的训练和测试误差

![](img/2af0828d393d2f80e7e79c124384a2eb.png)

表 4:具有高斯核的 OvA 感知器的最佳 c_star，以及相应的测试误差

高斯核的性能优于多项式核。这可能是因为高斯核可以捕获比多项式核所能捕获的更复杂的决策边界。然而，这是有代价的。高斯核倾向于更大程度的过度拟合(比较表 1 和表 3 中从训练到测试的误差增加)。这表明高斯核记住了训练数据中的“噪声”,而不是学习潜在的趋势。

## 卵子对卵子

![](img/d5ff11318a47ed4c909b7455e3dca066.png)

表 5:具有多项式内核的 OvO 感知器的训练和测试误差

![](img/b8cbec08a74b12321fda47c7f9e9cf4b.png)

表 6:具有多项式内核的 OvO 感知器的最佳 d_star，以及相应的测试误差

比较 OvO(表 5)模型与 OvA 模型(表 1)的训练/测试误差，我们可以看到，对于 d=1，OvO 的性能稍好。这可能是因为正在学习的决策曲面是线性的，但是 OvA 实现显然更适合非线性曲面(参见图 1 并与图 2 进行比较)。随着模型复杂度的增加，我们发现 OvA 比 OvO 表现得更好。

## 难以分类的图像

![](img/34cd964963a5d0fb173e237e643e394a.png)

图 3:用 1 到 7 度的 OvA 感知器正确分类困难的图像

绘制了 OvA 多项式核模型发现难以区分的一些示例。上面的强度项表示应用符号函数之前的分类输出。不出所料，这些都是负面的。结果并不特别令人惊讶。对于第一个图像，虽然预测正确，但它可能很容易是 6。第二幅图像可能是阿德画得不好，但无论哪种方式，每个像素的激活都是暗淡的，形状是高度扭曲和单薄的。下一张图片看起来很像 4，如果人类平均正确地将其分类，我会感到惊讶。下一个图像也是稀疏的。最终图像具有很强的曲率，可以被模型解释为 2。

## 混淆矩阵

最后，为每个测试模型绘制混淆矩阵。这是为了让读者更好地理解模型在不同图像下的表现。

![](img/11d20d985e635741eaf89b4d34880b9d.png)

图 4:在表 4 所示的最佳 c_star 上训练的 OvA 高斯核的混淆矩阵

![](img/6a739fda8b82d17608899b88543afe66.png)

图 5:表 2 中在最佳 d_star 上训练的 OvA 多项式核的混淆矩阵

![](img/4673f92a11f0bad3add66a6db66d87eb.png)

图 6:在表 6 所示的最佳 d_star 上训练的 OvO 多项式核的混淆矩阵

# 结束语

## 陷阱

*   如果使用 PyTorch，确保你实例化你知道你的数据类型，我得到的一个常见错误是转换 double 为 float！
*   确保你对你的张量运算有信心，特别是当你试图使它一般化的时候
*   从`itertools`返回一个只能运行一次的生成器对象。所以如果你想保存它，你必须把它转换成另一个保存在内存中的对象(如`list`)。使用`torch.combinations`等替代方法是可能的，但这需要一个`torch.Tensor`作为类。
*   非常小心一对一分类器中的逻辑要求。训练 k(k-1)/2 模型很容易，但是当你试图把它转换成 k 类的预测时，问题就变得不那么简单了。

## 关键要点

*   核心感知器使用核心方法，通过经典感知器算法实现非线性决策表面的学习
*   由于在线学习训练，如果没有正确初始化，感知器是高度计算密集型的。优化可以通过向量化 Gram 矩阵和编写广义核函数来实现
*   任何二元分类器都可以使用 OvA 或 OvO 方案来推广
*   OvA 创建 k 个分类器，为每个类别区分 1 个类别和其余类别。这些方法可能会受到类不平衡的影响，但是当数据集很小且有噪声时，它们可能会工作得更好。它们将数据置于直接竞争中，并且当分类输出未被加权时无法工作。
*   OvO 创建 k(k-1)/2 个分类器，用于区分类别(ci，cj)的唯一组合。他们的优势是允许加权和平等的投票方案。

## 再现性

如果你想重现结果，请查看 GitHub 的页面。如果您对它的任何方面有任何问题，请联系我！

如果你真的喜欢我的工作并愿意支持我，请考虑使用我的[推荐链接](https://namiyousef96.medium.com/membership)来获得一个媒体订阅。

*除非另有说明，所有图片均由作者创作。*