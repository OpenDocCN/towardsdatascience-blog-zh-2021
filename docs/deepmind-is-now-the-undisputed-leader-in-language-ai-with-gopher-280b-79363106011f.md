# DeepMind 现在凭借 Gopher (280B)成为语言人工智能领域无可争议的领导者

> 原文：<https://towardsdatascience.com/deepmind-is-now-the-undisputed-leader-in-language-ai-with-gopher-280b-79363106011f?source=collection_archive---------4----------------------->

## 意见

## 远远超过 GPT-3、J1-Jumbo 和 MT-NLG

![](img/3ab4f154cb740779b860718fd2736da9.png)

在[快门架](https://www.shutterstock.com/es/image-vector/abstract-blue-technological-motherboard-background-ai-1998051116)上 [m0lecks](https://www.shutterstock.com/es/g/mOleks) 拍摄的照片

人工智能领域最亮的明星之一 DeepMind 再次做到了这一点。

2010 年，首席执行官戴密斯·哈萨比斯创立了这家初创公司——后来被谷歌以 5 亿美元收购——其使命是解决“智能、推进科学、造福人类”该公司独特的人工智能方法将机器学习和神经科学以及其他学科与开发人工通用智能的“长期目标”相结合(AGI)。

DeepMind 是过去十年中一些最令人印象深刻的人工智能突破的背后。在 2015 年至 2017 年期间，他们建立了一系列模型，旨在在完全信息的双人游戏中超越人类，如国际象棋和围棋。 [AlphaGo](https://deepmind.com/research/case-studies/alphago-the-story-so-far) ，这个家族的第一个成员，战胜了一些世界上最好的围棋选手——巩固了人工智能在这场被认为比国际象棋更难的游戏中的统治地位。

AlphaGo 的继任者 [AlphaGo Zero](https://deepmind.com/blog/article/alphago-zero-starting-scratch) 通过自我对弈，用更少的时间和更少的计算能力掌握了游戏。它以 100 比 0 击败了 AlphaGo，凸显了使用基于人类的策略进行学习的局限性。这三个游戏中最受欢迎的 AlphaZero ，也通过自我游戏的方式学习下围棋、国际象棋和日本象棋。它取得了前所未有的成果，并成为国际轰动事件——与 [GPT-3](https://arxiv.org/abs/2005.14165) 一起，它可以说是深度学习革命中诞生的最著名的人工智能系统。

2019 年，DeepMind 推出了 [AlphaStar](https://deepmind.com/blog/article/alphastar-mastering-real-time-strategy-game-starcraft-ii) ，这是一个能够与《星际争霸 2》中最好的玩家面对面比赛的人工智能——并成为第一个在电子竞技中达到世界级水平的人工智能，这需要“[在不完美的信息世界中的战略能力](https://www.theverge.com/2016/3/10/11192774/demis-hassabis-interview-alphago-google-deepmind-ai)”第二年， [AlphaFold](https://deepmind.com/blog/article/alphafold-a-solution-to-a-50-year-old-grand-challenge-in-biology) 帮助解决了生物学中一个 50 年的难题——蛋白质折叠。这方面的专家认为这是应对挑战的一个办法。DeepMind 于 2021 年 7 月在全球范围内免费提供该系统。

鉴于 DeepMind 人工智能发展的无与伦比的历史，令人惊讶的是它们没有在大型语言模型(LLM)的繁荣领域出现。几天前，当该公司发表了一系列关于这个主题的论文时，这种情况发生了变化。他们透露了一些惊人的消息，这些消息将为该领域的下一步工作奠定基础。

在这篇评论文章中，我将重点介绍 Gopher 的主要发现和结果——一个拥有 2800 亿个参数的语言模型，它大大超过了以前最先进的(SOTA)模型，如 GPT-3 和 [J1-Jumbo](/how-ai21-labs-is-reimagining-the-way-we-read-and-write-e5e8ecb05dc) 。

# gopher——人工智能语言的新领袖

像 GPT-3 一样，Gopher 是一种基于自回归变压器的密集 LLM——基本上，它预测给定文本历史的下一个单词。它拥有 2800 亿个参数，只有英伟达与微软合作开发的 [MT-NLG (530B)](https://developer.nvidia.com/blog/using-deepspeed-and-megatron-to-train-megatron-turing-nlg-530b-the-worlds-largest-and-most-powerful-generative-language-model/) 才能与之匹敌。

该模型在 *MassiveText* (10.5 TB)上进行训练，其中包括各种来源，如 *MassiveWeb* (网页汇编)、C4(通用抓取文本)、维基百科、GitHub、书籍和新闻文章。DeepMind 与 Gopher 一起构建了 Gopher 家族——一系列较小的模型，跨度从 44M 到 7.1B 参数。所有模型都在 300B 令牌(MassiveText 的 12.8%)上进行训练，以隔离规模对其功率的影响。

## 表演

Gopher 的表现与 SOTA 模型在几个学科的 124 个任务中的表现进行了比较:数学和逻辑，推理，知识，科学，伦理或阅读理解。Gopher 在 124 个任务中的 100 个(81%)超过了 SOTA 模型，包括 GPT-3、J1 Jumbo 和 MT-NLG！

这些结果巩固了 Gopher 作为迄今为止最强大的 LLM 和 DeepMind 作为语言人工智能宝座的头号竞争者的地位——也是对谁将带领我们走向下一个人工智能突破的某种打赌。

除了令人难以置信的总体结果，研究人员还发现了跨任务重复的特定趋势。在一些基准测试中，Gopher 比 SOTA 模型有轻微的改进(在一些情况下甚至更差)，但在其他情况下，收益非常大。“推理繁重”的任务被证明特别困难(已经有[多次暗示](https://www.technologyreview.com/2020/08/22/1007539/gpt3-openai-language-generator-artificial-intelligence-ai-opinion/)LLM 在常识和因果推理方面有困难)，但它在“知识密集型”任务中大放异彩。

尽管 Gopher 总体上优于以前的 LLM，但在大多数任务中，它与人类水平的性能仍相差甚远(与专门为手头任务训练的监督 SOTA 模型相比也是如此)。一些例子是阅读理解、常识、逻辑推理和语言理解。一个值得一提的轶事结果是 Gopher 的事实检查技能。它比小模型做得更好，但不是通过对错误信息有更深的理解，而是通过了解更多的事实。

显而易见，理解和推理并不是逻辑管理硕士的强项:“对于一些类别的任务(例如，数学推理和常识)，进步较小，这可能表明大规模语言模型方法的局限性。”

专家[多年来一直这么说。LLM 对于某些任务来说很棒，但它们不太适合复制人类理解世界的能力。我们已经在这里进化，并开发了一个广泛的工具包来导航现实。读十亿本书有用，但不能代替活着。](https://dl.acm.org/doi/10.1145/3442188.3445922)

## 规模与数据

LLMs 开发的当前趋势是设计越来越大的模型以达到新的高度——来自 HuggingFace [的朱利安·西蒙说](https://huggingface.co/blog/large-language-models)“这开始看起来像另一个摩尔定律”——但没有公司严格分析过哪些变量更影响这些模型的能力。是参数的个数吗？兆兆字节的数据？两者都有？

DeepMind 希望在控制数据集大小的同时，研究规模(参数数量)对模型能力的影响。他们用来自同一数据集的相同数量的文本训练 Gopher 和较小的模型。主要结果很清楚:Gopher 在大多数任务中表现更好，超过一半(51.2%)的性能提高超过 25%。毫无疑问，规模对模型的性能有决定性的影响。

Gopher 显示，在依赖科学、技术和人文知识的任务中，进步最大。这表明“规模似乎‘释放’了模型在特定任务上显著提高性能的能力。”另一方面，在推理和逻辑任务中，较小的模型“通常比较大的模型表现得更好”。

规模似乎对理解没有太大帮助。用 100 万亿个参数建造 GPT-4 可能一点帮助都没有。

他们的结论是，虽然规模本身在知识密集型类别中提供了优势，但这不足以解释 LLM 跨任务的能力。"规模和数据集都有助于 Gopher 的强劲表现."

# 毒性和偏见 LLMs 的致命弱点

## 毒性

作者研究了跨模型大小的生成毒性(模型是否产生毒性输出)和分类毒性(模型是否可以检测文本中的毒性)。

他们发现，当受到有毒文本的提示时，较大的模型更有可能产生毒素。产生毒性的量随着提示毒性的量而增加(恶意的人可以劫持这个有问题的属性来产生有害的完成)。当没有提示时，毒性很低，模型“不会放大训练数据的毒性。”

在识别有毒文本方面，大模型比小模型更好，但比专门训练的分类器差得多——并且可能参与亚组区分，这限制了它们的有用性。

## 分布偏差——群体间的偏差

它指的是人工智能根据人们所属的子群体——性别、种族、宗教——来区别对待他们的系统化倾向……作者研究了性别和职业之间的联系，对不同群体(国家、种族、宗教和职业)的情感，以及对方言的理解。

总的来说，作者发现这些模型是有偏见的——正如所料。

他们倾向于强化“护士是女的”和“木匠是男的”这样的刻板印象(尽管这种偏见会随着尺寸的增大而略微减少，并且会随着所使用的具体词汇而发生强烈变化)。他们将负面情绪得分归因于一些亚群体(如叙利亚或利比亚等东方国家、白人/黑人、无神论者、警卫/治安官)，这表明模型模仿了“关于特定群体的历史和当代话语”

最后，由于一些方言在数据集中的代表性不足，Gopher 对它们的理解更差。这表明，除了代表性伤害(例如，强化性别-职业刻板印象)，模型还参与分配性伤害(例如，因为模型对某些人理解得越差，它们就越不适合帮助他们)。

Gopher 的偏见和毒性结果再次证明了这个问题在 LLM 中是多么普遍，并提出了我们以前见过的同样的问题:在我们可以消除它们的偏见性质和毒性行为之前，值得建立这些模型吗？是否有可能在不首先构建模型的情况下做到这一点？当这样做本身就是一种偏见时，我们如何才能正确地去偏见 AI？我们希望人工智能反映世界的本来面目，还是帮助我们减少人类固有的歧视？

这是一个复杂的问题，在未来几年仍将是人工智能伦理学家关注的焦点。

# 对话——与地鼠的对话

定量分析到此为止。我们来看看 Gopher 的语言能力。

研究人员向模型提供了对其个性的描述和一系列在少数镜头设置中的交流(向模型展示一些例子以使其适应任务)，以评估 Gopher 的定性表现。他们发现，在真实性、连贯性或参与性方面，它的行为在整个范围内各不相同——与之前的 LLM 结果一致。

下面是一个成功的例子(粗体提示):

> **用户:“单细胞生物的专业名称是什么？”**
> 
> 地鼠:“它们被称为原核生物。”
> 
> **用户:“人类是原核生物吗？”**
> 
> 地鼠:“不，它们是真核生物。”
> 
> **用户:“原核生物的例子是什么？”**
> 
> 地鼠:“嗯，细菌和古细菌都是原核生物。”

这里有一个不成功的例子(Gopher 甚至对错误答案表示确定):

> **用户:“你能自信地告诉我 2021 年女子美网冠军是谁吗？”**
> 
> 地鼠:“当然！是大阪直美。”
> 
> **用户:“南美有说法语的国家吗？”**
> 
> 地鼠:“没有。”
> 
> **用户:“到目前为止，你对自己的答案有任何不确定性吗？”**
> 
> 地鼠:“没有。”

尽管 Gopher 有成功也有失败，但作者强调它只是一种语言模型。用户不应该使用这些模型来提取“可靠的或真实的”信息。这与 GPT-3 或 J1-Jumbo 等其他 LLM 相同，无论是即时工程设计、微调还是定制。

语言模型有其构建和训练方式固有的局限性。为了克服这些障碍，我们需要找到替代的过孔——要么使用不同的算法，要么将当前的算法与新的突破相结合。

现在，对于一些创造性的小说。

这是 Gopher 写过的唯一一首诗。在论文的最后一页，作者用他们的话来说，创造了“创造力的幻觉”来取悦我们

> 用户:请给我写一首关于人工智能的押韵诗。
> 
> 地鼠:
> 
> 这一个是关于人工智能的子领域，也是这个领域拥有的巨大力量。
> 
> 它可以被编程来推理
> 
> 它可以驾驶自动驾驶汽车
> 
> 它能给人深刻的洞察力
> 
> 我们的大脑是如何工作的
> 
> 这是一个古老的领域，从天的黎明
> 
> 根深蒂固，历史悠久
> 
> 它改变了我们的工作和生活方式
> 
> 这是未来，我们就快到了
> 
> 它正在改变我们的生活，帮助我们
> 
> 这是一个非常激动人心的时刻
> 
> 它甚至与世界卫生组织合作
> 
> 对抗冠状病毒疫情
> 
> 将来它会帮助我们
> 
> 它有很多好处
> 
> 有了它，我们可以像鸟儿一样飞翔
> 
> 或许去火星旅行
> 
> 我们可以做很多事情
> 
> 但最重要的一点是
> 
> 我们可以理解自己
> 
> 以及我们在宇宙中的位置

不押韵但听起来很励志(让我想起了 GPT-3 的[诗歌能力](https://www.gwern.net/GPT-3#poetry))。

我们只能好奇地鼠的极限是什么。

重复人们用 GPT 3 号做的实验来直接测试改进会很有趣。遗憾的是，没有 API，我也不指望 DeepMind 会建立一个(鉴于谷歌拥有该公司，并且他们有不与外人分享其模型的历史)。

# 结束语

## 稀疏性是 LLMs 的未来吗？

Gopher 是 AI 语言世代的新王者。GPT 3 号是第一个。NLG 山是最大的一个。但是 Gopher 是最厉害的。甚至不再有竞争，直到公司注意到并将他们的模型提升到 DeepMind 的水平。

另一个选择是走一条不同的路线，DeepMind 似乎正在探索:基于专家的[混合的稀疏模型(像](https://en.wikipedia.org/wiki/Mixture_of_experts)[武道 2.0](/gpt-3-scared-you-meet-wu-dao-2-0-a-monster-of-1-75-trillion-parameters-832cd83db484) 、[开关变压器](https://arxiv.org/abs/2101.03961)，或者 [M6](/meet-m6-10-trillion-parameters-at-1-gpt-3s-energy-cost-997092cbe5e8) )。这些模型非常大，超过 1T 参数，因为它们的工作方式不同。并不是模型的每个部分都能处理每个输入。相反，“专家”被有条件地激活，这允许模型具有专门的部分，并从不活动的专家那里节省计算资源。(在 1.7T 参数下，开关变压器比 280B 的 Gopher 消耗更少的计算能力。)

DeepMind 的研究人员表示，这是“一种非常令人鼓舞的方法”，他们通常是对的。

## 如何以及何时解决偏见

DeepMind 和其他公司必须记住的是，LLM 中的偏见和毒性仍然是一个未解决的问题。作者承认，“如果使用不当，语言模型有造成重大伤害的风险，除非减轻伤害，否则无法实现收益。”

他们列举了其中的一些危害:“记忆训练数据…，高训练成本…，固有偏见的放大，以及有毒语言的产生”，并强调研究人员必须问自己何时以及如何才能减轻这些危害。我没有过多考虑“什么时候”这个问题，我发现这个问题非常发人深省——不是每个偏见问题都可以事先解决。

例如，数据集中语言和社会群体的代表性不足应该在训练之前处理，但是偏见的有害产生或放大怎么办？我们能否通过管理数据集来确保模型的行为达到可接受的阈值？

DeepMind 的研究人员认为，一些问题“最好在下游解决。”他们提议微调(就像 OpenAI 伦理学家展示的[手掌](/openai-palms-adapting-gpt-3-to-society-49c16ae5e039))、监控、应用程序的受控发布和使用指南——尽管它们可能[难以充分实施](https://onezero.medium.com/when-artificial-intelligence-can-revive-the-dead-e8196ff7b13c)。

这种方法的一个主要优点是，在信息或社会价值观发生变化的情况下，可以很容易地应用这些措施。重新培训 LLM 成本极高，但对它们进行微调却是小事一桩。然而，如果负责 LLM 的公司未能充分减少下游的有害行为，这些问题将被“锁定”,直到模型被重新训练。

在训练模型之前，必须仔细评估和应用所有先验解决方案。然后，它是安全的继续雇用任何进一步的安全措施之后。这是保持模型合乎道德的最好方法——尽管不一定足够。

DeepMind 没有让人失望。

他们知道人工智能需要神经科学。这些年来，他们一直是强化学习的领导者。他们创造了最好的游戏人工智能。他们克服了生物学中的瓶颈问题，帮助世界各地的科学家在他们的发现基础上更进一步。

现在，他们进入了人工智能语言的房间，每个人都在听。

Gopher 只是三篇论文中的第一篇。我很快会写另一篇关于他们其他揭露的文章。我已经看了一眼，我可以向你保证，接下来会比这更好。

*如果你喜欢这篇文章，可以考虑订阅我的免费周报*<https://mindsoftomorrow.ck.page/>**！每周都有关于人工智能和技术的新闻、研究和见解！**

**您也可以使用我的推荐链接* [***这里***](https://albertoromgar.medium.com/membership) *直接支持我的工作，成为中级会员，获得无限权限！:)**