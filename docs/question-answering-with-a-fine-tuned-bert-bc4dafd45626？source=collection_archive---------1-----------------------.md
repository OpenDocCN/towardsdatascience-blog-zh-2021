# ç”¨å¾®è°ƒè¿‡çš„ BERT å›ç­”é—®é¢˜

> åŸæ–‡ï¼š<https://towardsdatascience.com/question-answering-with-a-fine-tuned-bert-bc4dafd45626?source=collection_archive---------1----------------------->

## â€¦åœ¨æ–¯å¦ç¦å¤§å­¦çš„ CoQA æ•°æ®é›†ä¸Šä½¿ç”¨æ‹¥æŠ±è„¸å˜å½¢å™¨å’Œ PyTorch

![](img/eb573f3398fe9a8bc935821f915ded53.png)

[æ³°å‹’](https://unsplash.com/@taypaigey)åœ¨ [Unsplash](https://unsplash.com/photos/NTur2_QKpg0) ä¸Šçš„ç…§ç‰‡

æ¯å½“æˆ‘æƒ³åˆ°é—®é¢˜å›ç­”ç³»ç»Ÿæ—¶ï¼Œæˆ‘é¦–å…ˆæƒ³åˆ°çš„æ˜¯æ•™å®¤â€”â€”ä¸€ä¸ªè€å¸ˆæå‡ºçš„é—®é¢˜ï¼Œä¸€ä¸ªæˆ–å‡ ä¸ªå­¦ç”Ÿä¸¾æ‰‹ğŸ™‹æ¥å›ç­”è¿™ä¸ªé—®é¢˜ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œå›ç­”é—®é¢˜å¯¹äººç±»æ¥è¯´å¯èƒ½æ˜¯ä¸€é¡¹å¾®ä¸è¶³é“çš„ä»»åŠ¡ï¼Œä½†å¯¹æœºå™¨æ¥è¯´å´ä¸æ˜¯å¦‚æ­¤å¾®ä¸è¶³é“ã€‚è¦å›ç­”ä»»ä½•é—®é¢˜ï¼Œæœºå™¨éƒ½éœ€è¦å…‹æœè®¸å¤šä¸åŒçš„æŒ‘æˆ˜ï¼Œå¦‚è¯æ±‡ç©ºç¼ºã€å…±æŒ‡æ¶ˆè§£ã€è¯­è¨€æ­§ä¹‰ç­‰ã€‚ğŸ˜ä¸ºæ­¤ï¼Œæœºå™¨éœ€è¦å¤§é‡çš„è®­ç»ƒæ•°æ®å’Œæ™ºèƒ½æ¶æ„æ¥ç†è§£å’Œå­˜å‚¨æ–‡æœ¬ä¸­çš„é‡è¦ä¿¡æ¯ã€‚NLP çš„æœ€æ–°è¿›å±•å·²ç»é‡Šæ”¾äº†æœºå™¨ç†è§£æ–‡æœ¬å’Œæ‰§è¡Œä¸åŒä»»åŠ¡çš„èƒ½åŠ›ã€‚ğŸ‘Œ

åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å°†ä¸€èµ·å®Œæˆä¸€é¡¹å¸¸ç”¨çš„ä»»åŠ¡â€”â€”é—®ç­”ã€‚æˆ‘ä»¬å°†ä½¿ç”¨æ‹¥æŠ±è„¸å˜å½¢é‡‘åˆšåº“ä¸­å·²ç»å¯ç”¨çš„å¾®è°ƒ BERT æ¨¡å‹æ¥å›ç­”åŸºäº CoQA æ•°æ®é›†çš„æ•…äº‹çš„é—®é¢˜ã€‚æˆ‘ç¡®ä¿¡ï¼Œé€šè¿‡æŸ¥çœ‹ä»£ç ï¼Œæ‚¨ä¼šæ„è¯†åˆ°ä½¿ç”¨ä¸€ä¸ªå¾®è°ƒçš„æ¨¡å‹æ¥è¾¾åˆ°æˆ‘ä»¬çš„ç›®çš„æ˜¯å¤šä¹ˆå®¹æ˜“ã€‚ğŸ˜

***æ³¨æ„:*** *åœ¨æœ¬æ–‡ä¸­æˆ‘ä»¬å°†ä¸æ·±å…¥è®¨è®º BERT æ¶æ„çš„ç»†èŠ‚ã€‚ä½†æ˜¯ï¼Œåœ¨éœ€è¦æˆ–å¯èƒ½çš„æƒ…å†µä¸‹ï¼Œæˆ‘ä¼šæä¾›è§£é‡Šã€‚*

**æ–‡ä¸­ä½¿ç”¨çš„ç‰ˆæœ¬:**ç«ç‚¬- 1.7.1ï¼Œå˜å½¢é‡‘åˆš- 4.4.2

è®©æˆ‘ä»¬é¦–å…ˆå›ç­”ä¸æœ¬æ–‡ç›¸å…³çš„å‡ ä¸ªé‡è¦é—®é¢˜ã€‚

**ä»€ä¹ˆæ˜¯æŠ±è„¸å’Œå˜å½¢é‡‘åˆšï¼Ÿ**ğŸ¤”

[æ‹¥æŠ±è„¸](https://huggingface.co/)æ˜¯è‡ªç„¶è¯­è¨€å¤„ç†(NLP)æŠ€æœ¯çš„å¼€æºæä¾›å•†ã€‚æ‚¨å¯ä»¥ä½¿ç”¨æ‹¥æŠ±è„¸æœ€å…ˆè¿›çš„æ¨¡å‹æ¥æ„å»ºã€è®­ç»ƒå’Œéƒ¨ç½²æ‚¨è‡ªå·±çš„æ¨¡å‹ã€‚[å˜å½¢é‡‘åˆš](https://huggingface.co/transformers/)æ˜¯ä»–ä»¬çš„ NLP åº“ã€‚æˆ‘å¼ºçƒˆæ¨èä½ å»çœ‹çœ‹æ‹¥æŠ±è„¸å›¢é˜Ÿæ‰€åšçš„æƒŠäººçš„å·¥ä½œï¼Œä»¥åŠä»–ä»¬æ”¶é›†çš„å¤§é‡é¢„å…ˆè®­ç»ƒå¥½çš„ NLP æ¨¡å‹ã€‚

**ä»€ä¹ˆæ˜¯ CoQAï¼Ÿ**ğŸ¤”

[CoQA](https://arxiv.org/pdf/1808.07042.pdf) æ˜¯æ–¯å¦ç¦ NLP äº 2019 å¹´å‘å¸ƒçš„ä¼šè¯å¼é—®ç­”æ•°æ®é›†ã€‚è¿™æ˜¯ä¸€ä¸ªç”¨äºæ„å»ºå¯¹è¯å¼é—®ç­”ç³»ç»Ÿçš„å¤§è§„æ¨¡æ•°æ®é›†ã€‚è¯¥æ•°æ®é›†æ—¨åœ¨æµ‹é‡æœºå™¨ç†è§£æ–‡æœ¬æ®µè½å¹¶å›ç­”å¯¹è¯ä¸­å‡ºç°çš„ä¸€ç³»åˆ—ç›¸äº’å…³è”çš„é—®é¢˜çš„èƒ½åŠ›ã€‚è¯¥æ•°æ®é›†çš„ç‹¬ç‰¹ä¹‹å¤„åœ¨äºï¼Œæ¯ä¸ªå¯¹è¯éƒ½æ˜¯é€šè¿‡ä¸¤ä¸ªäººç¾¤å·¥ä½œè€…ä»¥é—®ç­”çš„å½¢å¼å°±ä¸€æ®µå†…å®¹è¿›è¡ŒèŠå¤©æ¥æ”¶é›†çš„ï¼Œå› æ­¤ï¼Œè¿™äº›é—®é¢˜æ˜¯å¯¹è¯å¼çš„ã€‚è¦äº†è§£ JSON æ•°æ®çš„æ ¼å¼ï¼Œè¯·å‚è€ƒè¿™ä¸ª[é“¾æ¥](http://downloads.cs.stanford.edu/nlp/data/coqa/coqa-train-v1.0.json)ã€‚æˆ‘ä»¬å°†ä½¿ç”¨æ¥è‡ª JSON æ•°æ®é›†çš„æ•…äº‹ã€é—®é¢˜å’Œç­”æ¡ˆæ¥æ„æˆæˆ‘ä»¬çš„æ•°æ®æ¡†æ¶ã€‚

**ä¼¯ç‰¹æ˜¯ä»€ä¹ˆï¼Ÿ**ğŸ¤”

[BERT](https://arxiv.org/pdf/1810.04805.pdf) æ˜¯æ¥è‡ªå˜å‹å™¨çš„åŒå‘ç¼–ç å™¨è¡¨ç¤ºã€‚å®ƒæ˜¯æœ€æµè¡Œå’Œæœ€å¹¿æ³›ä½¿ç”¨çš„ NLP æ¨¡å‹ä¹‹ä¸€ã€‚BERT æ¨¡å‹å¯ä»¥é€šè¿‡æŸ¥çœ‹å•è¯å‰åçš„å•è¯æ¥è€ƒè™‘å•è¯çš„å®Œæ•´ä¸Šä¸‹æ–‡ï¼Œè¿™å¯¹äºç†è§£æŸ¥è¯¢èƒŒåçš„æ„å›¾ç‰¹åˆ«æœ‰ç”¨ã€‚ç”±äºå®ƒçš„åŒå‘æ€§ï¼Œå®ƒå¯¹è¯­è¨€çš„ä¸Šä¸‹æ–‡å’Œæµç¨‹æœ‰æ›´æ·±çš„ç†è§£ï¼Œå› æ­¤ï¼Œç°åœ¨è¢«ç”¨äºè®¸å¤š NLP ä»»åŠ¡ä¸­ã€‚å…³äº BERT çš„æ›´å¤šç»†èŠ‚ä»¥åŠä»£ç ã€‚ğŸ™ƒ

å˜å½¢é‡‘åˆšåº“æœ‰å¾ˆå¤šä¸åŒçš„ [BERT æ¨¡å‹](https://huggingface.co/transformers/model_doc/bert.html#)ã€‚å¾ˆå®¹æ˜“ä»è¿™ä¸ªåº“ä¸­æ‰¾åˆ°ä¸€ä¸ªç‰¹å®šäºä»»åŠ¡çš„æ¨¡å‹å¹¶å®Œæˆæˆ‘ä»¬çš„ä»»åŠ¡ã€‚

é‚£ä¹ˆï¼Œè®©æˆ‘ä»¬å¼€å§‹å§ï¼Œä½†é¦–å…ˆè®©æˆ‘ä»¬çœ‹çœ‹æˆ‘ä»¬çš„æ•°æ®é›†ã€‚ğŸ˜Š

![](img/b80a4ed2c024a536f0f93f51d2800e53.png)![](img/7ef70211d691055b80928d65998a6918.png)

æ–¯å¦ç¦çš„ JSON æ•°æ®

JSON æ•°æ®æœ‰å¾ˆå¤šå­—æ®µã€‚å‡ºäºæˆ‘ä»¬çš„ç›®çš„ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨æ¥è‡ªâ€œé—®é¢˜â€å’Œâ€œç­”æ¡ˆâ€çš„â€œæ•…äº‹â€ã€â€œè¾“å…¥æ–‡æœ¬â€ï¼Œå¹¶å½¢æˆæˆ‘ä»¬çš„æ•°æ®æ¡†æ¶ã€‚

## **å®‰è£…å˜å‹å™¨**

```
**!pip install** transformers
```

## **å¯¼å…¥åº“**

```
**import** pandas **as** pd
**import** numpy **as** np
**import** torch
**from** transformers **import** BertForQuestionAnswering
**from** transformers **import** BertTokenizer
```

## ä»æ–¯å¦ç¦ç½‘ç«™åŠ è½½æ•°æ®

```
coqa = **pd.read_json**('[http://downloads.cs.stanford.edu/nlp/data/coqa/coqa-train-v1.0.json'](http://downloads.cs.stanford.edu/nlp/data/coqa/coqa-train-v1.0.json'))
coqa**.head()**
```

![](img/28fe40a70537d638d3af47ad9338967e.png)

åŠ è½½çš„æ•°æ®é›†

## æ•°æ®æ¸…ç†

æˆ‘ä»¬å°†å¤„ç†â€œæ•°æ®â€åˆ—ï¼Œæ‰€ä»¥è®©æˆ‘ä»¬åˆ é™¤â€œç‰ˆæœ¬â€åˆ—ã€‚

```
**del** coqa["version"]
```

å¯¹äºæ¯ä¸€ä¸ªé—®ç­”é…å¯¹ï¼Œæˆ‘ä»¬å°†æŠŠé“¾æ¥çš„æ•…äº‹é™„åŠ åˆ°å®ƒä¸Šé¢ã€‚

```
*#required columns in our dataframe*
cols = ["text","question","answer"]*#list of lists to create our dataframe*
comp_list = []
**for** index, row **in** coqa**.iterrows()**:
    **for** i **in** **range**(**len**(row["data"]["questions"])):
        temp_list = []
        temp_list**.append**(row["data"]["story"])
        temp_list**.append**(row["data"]["questions"][i]["input_text"])
        temp_list**.append**(row["data"]["answers"][i]["input_text"])
        comp_list**.append**(temp_list)new_df = **pd.DataFrame**(comp_list, columns=cols) *#saving the dataframe to csv file for further loading*
new_df**.to_csv**("CoQA_data.csv", index=**False**)
```

## ä»æœ¬åœ° CSV æ–‡ä»¶åŠ è½½æ•°æ®

```
data = **pd.read_csv**("CoQA_data.csv")
data**.head()**
```

è¿™æ˜¯æˆ‘ä»¬æ¸…ç†åçš„æ•°æ®ã€‚

![](img/8e02cd9f9d751a590bd96f257d3d7840.png)

æ¸…ç†çš„æ•°æ®

```
**print**("Number of question and answers: ", **len**(data))
```

æ•°æ®é›†æœ‰å¾ˆå¤šé—®é¢˜å’Œç­”æ¡ˆï¼Œæˆ‘ä»¬æ¥æ•°ä¸€ä¸‹ã€‚

```
Number of question and answers:  108647
```

## æ„å»ºèŠå¤©æœºå™¨äºº

ä½¿ç”¨è¿™äº›é¢„å…ˆè®­ç»ƒå¥½çš„æ¨¡å‹çš„æœ€å¤§å¥½å¤„æ˜¯ï¼Œåªéœ€ä¸¤è¡Œç®€å•çš„ä»£ç å°±å¯ä»¥åŠ è½½æ¨¡å‹åŠå…¶æ ‡è®°å™¨ã€‚ğŸ˜²ä¸å°±æ˜¯å•çº¯çš„å“‡å—ï¼Ÿå¯¹äºæ–‡æœ¬åˆ†ç±»è¿™æ ·çš„ä»»åŠ¡ï¼Œæˆ‘ä»¬éœ€è¦åœ¨æ•°æ®é›†ä¸Šå¾®è°ƒ BERTã€‚ä½†å¯¹äºé—®ç­”ä»»åŠ¡ï¼Œæˆ‘ä»¬ç”šè‡³å¯ä»¥ä½¿ç”¨å·²ç»è®­ç»ƒå¥½çš„æ¨¡å‹ï¼Œå³ä½¿æˆ‘ä»¬çš„æ–‡æœ¬æ¥è‡ªå®Œå…¨ä¸åŒçš„é¢†åŸŸï¼Œä¹Ÿèƒ½è·å¾—ä¸é”™çš„ç»“æœã€‚ä¸ºäº†å¾—åˆ°æ›´å¥½çš„ç»“æœï¼Œæˆ‘ä»¬ä½¿ç”¨äº†ä¸€ä¸ªä¼¯ç‰¹æ¨¡å‹ï¼Œè¿™ä¸ªæ¨¡å‹æ˜¯æ ¹æ®å°é˜ŸåŸºå‡†è¿›è¡Œå¾®è°ƒçš„ã€‚

å¯¹äºæˆ‘ä»¬çš„ä»»åŠ¡ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨å˜å½¢é‡‘åˆšåº“ä¸­çš„ **BertForQuestionAnswering** ç±»ã€‚

```
model = **BertForQuestionAnswering.from_pretrained**('bert-large-uncased-whole-word-masking-finetuned-squad')
tokenizer = **BertTokenizer.from_pretrained**('bert-large-uncased-whole-word-masking-finetuned-squad')
```

é¢„è®¡ä¸‹è½½éœ€è¦å‡ åˆ†é’Ÿï¼Œå› ä¸º BERT-large æ˜¯ä¸€ä¸ªéå¸¸å¤§çš„æ¨¡å‹ï¼Œæœ‰ 24 å±‚å’Œ 340M å‚æ•°ï¼Œä½¿å…¶æˆä¸º 1.34GB çš„æ¨¡å‹ã€‚

## é—®é—®é¢˜

è®©æˆ‘ä»¬éšæœºé€‰æ‹©ä¸€ä¸ªé—®é¢˜ç¼–å·ã€‚

```
random_num = **np.random.randint**(0,**len**(data))question = data["question"][random_num]
text = data["text"][random_num]
```

è®©æˆ‘ä»¬å°†é—®é¢˜å’Œæ–‡æœ¬æ ‡è®°æˆä¸€å¯¹ã€‚

```
input_ids = tokenizer**.encode**(question, text)
**print**("The input has a total of {} tokens."**.format**(**len**(input_ids)))
```

æˆ‘ä»¬æ¥çœ‹çœ‹è¿™ä¸ªé—®é¢˜å’Œæ–‡æœ¬å¯¹æœ‰å¤šå°‘ä¸ªä»¤ç‰Œã€‚

```
The input has a total of 427 tokens.
```

ä¸ºäº†çœ‹çœ‹æˆ‘ä»¬çš„è®°å·èµ‹äºˆå™¨åœ¨åšä»€ä¹ˆï¼Œè®©æˆ‘ä»¬æ‰“å°å‡ºè®°å·å’Œå®ƒä»¬çš„ idã€‚

```
tokens = tokenizer**.convert_ids_to_tokens**(input_ids)**for** token, id **in** **zip**(tokens, input_ids):
    **print**('{:8}{:8,}'**.format**(token,id))
```

![](img/e4491b6a5fd79dd3cc5e55e79c0db047.png)![](img/c9f28639e370952ec381b0ae38a1ed21.png)![](img/41d186c2f452033ab6286def8bf581e8.png)

å¸¦æœ‰ id çš„ä»¤ç‰Œ

BERT æœ‰ä¸€ç§å¤„ç†æ ‡è®°åŒ–è¾“å…¥çš„ç‹¬ç‰¹æ–¹æ³•ã€‚ä»ä¸Šé¢çš„æˆªå›¾ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°ä¸¤ä¸ªç‰¹æ®Šçš„ä»¤ç‰Œ[CLS]å’Œ[SEP]ã€‚[CLS] token ä»£è¡¨åˆ†ç±»ï¼Œå®ƒä»£è¡¨å¥å­çº§åˆ«çš„åˆ†ç±»ï¼Œåœ¨æˆ‘ä»¬åˆ†ç±»æ—¶ä½¿ç”¨ã€‚BERT ä½¿ç”¨çš„å¦ä¸€ä¸ªä»¤ç‰Œæ˜¯[SEP]ã€‚å®ƒç”¨äºåˆ†éš”ä¸¤æ®µæ–‡æœ¬ã€‚ä½ å¯ä»¥åœ¨ä¸Šé¢çš„æˆªå›¾é‡Œçœ‹åˆ°ä¸¤ä¸ª[SEP]ä»¤ç‰Œï¼Œä¸€ä¸ªåœ¨é—®é¢˜åé¢ï¼Œä¸€ä¸ªåœ¨æ­£æ–‡åé¢ã€‚

é™¤äº†â€œæ ‡è®°åµŒå…¥â€ï¼ŒBERT å†…éƒ¨è¿˜ä½¿ç”¨äº†â€œæ®µåµŒå…¥â€å’Œâ€œä½ç½®åµŒå…¥â€ã€‚ç‰‡æ®µåµŒå…¥æœ‰åŠ©äº BERT åŒºåˆ†é—®é¢˜å’Œæ–‡æœ¬ã€‚åœ¨å®è·µä¸­ï¼Œå¦‚æœåµŒå…¥æ¥è‡ªå¥å­ 1ï¼Œæˆ‘ä»¬ä½¿ç”¨ 0 çš„å‘é‡ï¼Œå¦åˆ™å¦‚æœåµŒå…¥æ¥è‡ªå¥å­ 2ï¼Œæˆ‘ä»¬ä½¿ç”¨ 1 çš„å‘é‡ã€‚ä½ç½®åµŒå…¥æœ‰åŠ©äºç¡®å®šå•è¯åœ¨åºåˆ—ä¸­çš„ä½ç½®ã€‚æ‰€æœ‰è¿™äº›åµŒå…¥éƒ½è¢«é¦ˆé€åˆ°è¾“å…¥å±‚ã€‚

å˜å½¢é‡‘åˆšåº“å¯ä»¥ä½¿ç”¨*pretrainedtokenizer . encode _ plus()*è‡ªè¡Œåˆ›å»ºç‰‡æ®µåµŒå…¥ã€‚ä½†æ˜¯ï¼Œæˆ‘ä»¬ç”šè‡³å¯ä»¥åˆ›é€ æˆ‘ä»¬è‡ªå·±çš„ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬åªéœ€è¦ä¸ºæ¯ä¸ªä»¤ç‰ŒæŒ‡å®šä¸€ä¸ª 0 æˆ– 1ã€‚

```
*#first occurence of [SEP] token*
sep_idx = input_ids**.index**(tokenizer**.sep_token_id**)
**print**("SEP token index: ", sep_idx)*#number of tokens in segment A (question) - this will be one more than the sep_idx as the index in Python starts from 0* num_seg_a = sep_idx+1
**print**("Number of tokens in segment A: ", num_seg_a)*#number of tokens in segment B (text)*
num_seg_b = **len**(input_ids) - num_seg_a
**print**("Number of tokens in segment B: ", num_seg_b)*#creating the segment ids*
segment_ids = [0]*num_seg_a + [1]*num_seg_b#making sure that every input token has a segment id **assert** **len**(segment_ids) == **len**(input_ids)
```

è¿™æ˜¯è¾“å‡ºã€‚

```
SEP token index: 8
Number of tokens in segment A: 9
Number of tokens in segment B: 418
```

ç°åœ¨è®©æˆ‘ä»¬æŠŠå®ƒè¾“å…¥åˆ°æˆ‘ä»¬çš„æ¨¡å‹ä¸­ã€‚

```
*#token input_ids to represent the input and token segment_ids to differentiate our segments - question and text*
output = **model**(**torch.tensor**([input_ids]),  token_type_ids=**torch.tensor**([segment_ids]))
```

æŸ¥çœ‹æœ€å¯èƒ½çš„å¼€å§‹å’Œç»“æŸå•è¯ï¼Œå¹¶ä¸”ä»…å½“ç»“æŸæ ‡è®°åœ¨å¼€å§‹æ ‡è®°ä¹‹åæ—¶æ‰æä¾›ç­”æ¡ˆã€‚

```
*#tokens with highest start and end scores*
answer_start = **torch.argmax**(output.start_logits)
answer_end = **torch.argmax**(output.end_logits)**if** answer_end >= answer_start:
    answer = " "**.join**(tokens[answer_start:answer_end+1])
**else:**
    **print**("I am unable to find the answer to this question. Can you please ask another question?")

**print**("\nQuestion:\n{}"**.format**(question**.capitalize()**))
**print**("\nAnswer:\n{}."**.format**(answer**.capitalize()**))
```

è¿™æ˜¯æˆ‘ä»¬çš„é—®é¢˜å’Œç­”æ¡ˆã€‚

```
**Question:**
Who is the acas director?
 **Answer:**
Agnes karin ##gu.
```

å“‡ï¼ä¼¯ç‰¹é¢„æµ‹åˆ°äº†æ­£ç¡®çš„ç­”æ¡ˆâ€”â€”â€œè‰¾æ ¼å°¼ä¸Â·å¡ç³å¤â€ã€‚ä½†æ˜¯ï¼Œå›å¤é‡Œè¿™ä¸ªâ€œ##â€æ˜¯ä»€ä¹ˆï¼Ÿç»§ç»­è¯»ä¸‹å»ï¼ğŸ“™

ä¼¯ç‰¹ä½¿ç”¨**è¯å—æ ‡è®°åŒ–**ã€‚åœ¨ BERT ä¸­ï¼Œç”Ÿåƒ»å­—è¢«åˆ†è§£æˆå­å­—/ç‰‡æ®µã€‚å•è¯å—æ ‡è®°åŒ–ä½¿ç”¨##æ¥åˆ†éš”å·²æ‹†åˆ†çš„æ ‡è®°ã€‚ä¾‹å¦‚:â€œKarinâ€æ˜¯ä¸€ä¸ªå¸¸ç”¨è¯ï¼Œæ‰€ä»¥ wordpiece ä¸ä¼šæ‹†åˆ†å®ƒã€‚ç„¶è€Œï¼Œâ€œKaringuâ€æ˜¯ä¸€ä¸ªç½•è§çš„è¯ï¼Œæ‰€ä»¥ wordpiece å°†å…¶æ‹†åˆ†ä¸ºå•è¯â€œKarinâ€å’Œâ€œ##guâ€ã€‚æ³¨æ„ï¼Œå®ƒåœ¨å¤å‰é¢åŠ äº†##ï¼Œè¡¨ç¤ºå®ƒæ˜¯æ‹†åˆ†åçš„ç¬¬äºŒä¸ªè¯ã€‚

ä½¿ç”¨å•è¯å—æ ‡è®°åŒ–èƒŒåçš„æƒ³æ³•æ˜¯å‡å°‘è¯æ±‡è¡¨çš„å¤§å°ï¼Œä»è€Œæé«˜è®­ç»ƒæ€§èƒ½ã€‚æƒ³æƒ³è¿™äº›è¯ï¼Œè·‘ï¼Œè·‘ï¼Œè·‘è€…ã€‚å¦‚æœæ²¡æœ‰å•è¯å—æ ‡è®°åŒ–ï¼Œæ¨¡å‹å¿…é¡»ç‹¬ç«‹åœ°å­˜å‚¨å’Œå­¦ä¹ æ‰€æœ‰ä¸‰ä¸ªå•è¯çš„å«ä¹‰ã€‚ä½†æ˜¯ï¼Œé€šè¿‡è¯å—æ ‡è®°åŒ–ï¼Œè¿™ä¸‰ä¸ªå•è¯ä¸­çš„æ¯ä¸€ä¸ªéƒ½å°†è¢«æ‹†åˆ†ä¸ºâ€œrunâ€å’Œç›¸å…³çš„â€œ# #åç¼€â€(å¦‚æœæœ‰åç¼€çš„è¯ï¼Œä¾‹å¦‚ï¼Œâ€œrunâ€ï¼Œâ€œ##ningâ€ï¼Œâ€œ##nerâ€)ã€‚ç°åœ¨ï¼Œæ¨¡å‹å°†å­¦ä¹ å•è¯â€œrunâ€çš„ä¸Šä¸‹æ–‡ï¼Œå…¶ä½™çš„æ„æ€å°†è¢«ç¼–ç åœ¨åç¼€ä¸­ï¼Œè¿™å°†ä»å…·æœ‰ç±»ä¼¼åç¼€çš„å…¶ä»–å•è¯ä¸­å­¦ä¹ ã€‚

å¾ˆæœ‰è¶£ï¼Œå¯¹å§ï¼Ÿæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ä¸‹é¢çš„ç®€å•ä»£ç æ¥é‡æ„è¿™äº›å•è¯ã€‚

```
answer = tokens[answer_start]**for** i **in** **range**(answer_start+1, answer_end+1):
    **if** tokens[i][0:2] == "##":
        answer += tokens[i][2:]
  **  else:**
        answer += " " + tokens[i]
```

ä¸Šé¢çš„ç­”æ¡ˆç°åœ¨å°†å˜æˆ: **Agnes karingu**

ç°åœ¨è®©æˆ‘ä»¬æŠŠè¿™ä¸ªé—®ç­”è¿‡ç¨‹å˜æˆä¸€ä¸ªç®€å•çš„å‡½æ•°ã€‚

```
**def** question_answer(question, text):

    *#tokenize question and text as a pair*
    input_ids = tokenizer**.encode**(question, text)

   * #string version of tokenized ids*
    tokens = tokenizer**.convert_ids_to_tokens**(input_ids)

  *  #segment IDs*
    #first occurence of [SEP] token
    sep_idx = input_ids**.index**(tokenizer**.sep_token_id**) *#number of tokens in segment A (question)*
    num_seg_a = sep_idx+1 *#number of tokens in segment B (text)*
    num_seg_b = **len**(input_ids) - num_seg_a

    *#list of 0s and 1s for segment embeddings*
    segment_ids = [0]*num_seg_a + [1]*num_seg_b **assert** **len**(segment_ids) == **len**(input_ids)

    *#model output using input_ids and segment_ids*
    output = **model**(**torch.tensor**([input_ids]), token_type_ids=**torch.tensor**([segment_ids]))

    *#reconstructing the answer*
    answer_start = **torch.argmax**(output.start_logits)
    answer_end = **torch.argmax**(output.end_logits) **if** answer_end >= answer_start:
        answer = tokens[answer_start]
        **for** i **in** range(answer_start+1, answer_end+1):
            **if** tokens[i][0:2] == "##":
                answer += tokens[i][2:]
            **else:**
                answer += " " + tokens[i]

    **if** answer**.startswith**("[CLS]"):
        answer = "Unable to find the answer to your question."

    **print**("\nPredicted answer:\n{}"**.format**(answer**.capitalize()**))
```

è®©æˆ‘ä»¬ä½¿ç”¨æ•°æ®é›†ä¸­çš„ä¸€ä¸ªæ–‡æœ¬å’Œé—®é¢˜æ¥æµ‹è¯•è¿™ä¸ªå‡½æ•°ã€‚ğŸ˜›

```
**text** = """New York (CNN) -- More than 80 Michael Jackson collectibles -- including the late pop star's famous rhinestone-studded glove from a 1983 performance -- were auctioned off Saturday, reaping a total $2 million. Profits from the auction at the Hard Rock Cafe in New York's Times Square crushed pre-sale expectations of only $120,000 in sales. The highly prized memorabilia, which included items spanning the many stages of Jackson's career, came from more than 30 fans, associates and family members, who contacted Julien's Auctions to sell their gifts and mementos of the singer. Jackson's flashy glove was the big-ticket item of the night, fetching $420,000 from a buyer in Hong Kong, China. Jackson wore the glove at a 1983 performance during \"Motown 25,\" an NBC special where he debuted his revolutionary moonwalk. Fellow Motown star Walter \"Clyde\" Orange of the Commodores, who also performed in the special 26 years ago, said he asked for Jackson's autograph at the time, but Jackson gave him the glove instead. "The legacy that [Jackson] left behind is bigger than life for me,\" Orange said. \"I hope that through that glove people can see what he was trying to say in his music and what he said in his music.\" Orange said he plans to give a portion of the proceeds to charity. Hoffman Ma, who bought the glove on behalf of Ponte 16 Resort in Macau, paid a 25 percent buyer's premium, which was tacked onto all final sales over $50,000\. Winners of items less than $50,000 paid a 20 percent premium."""**question** = "Where was the Auction held?"question_answer(question, text)*#original answer from the dataset* **print**("Original answer:\n", data**.loc**[data["question"] == question]["answer"].values[0]))
```

è¾“å‡º:

```
**Predicted answer:**
Hard rock cafe in new york ' s times square**Original answer:**
Hard Rock Cafe
```

ä¸€ç‚¹ä¹Ÿä¸å·®ã€‚äº‹å®ä¸Šï¼Œæˆ‘ä»¬çš„ä¼¯ç‰¹æ¨¡å‹ç»™å‡ºäº†æ›´è¯¦ç»†çš„å›ç­”ã€‚

è¿™æ˜¯ä¸€ä¸ªå°å‡½æ•°ï¼Œç”¨æ¥æµ‹è¯• BERT å¯¹ä¸Šä¸‹æ–‡çš„ç†è§£ç¨‹åº¦ã€‚æˆ‘åªæ˜¯æŠŠå›ç­”é—®é¢˜çš„è¿‡ç¨‹ä½œä¸ºä¸€ä¸ªå¾ªç¯æ¥ç©è¿™ä¸ªæ¨¡å‹ã€‚ğŸ’ƒ

```
**text** = **input**("Please enter your text: \n")
**question** = **input**("\nPlease enter your question: \n")**while** **True**:
    question_answer(question, text)

    flag = **True**
    flag_N = **False**

    **while** flag:
        response = **input**("\nDo you want to ask another question based on this text (Y/N)? ")
        **if** response[0] == "Y":
            question = **input**("\nPlease enter your question: \n")
            flag = False
        **elif** response[0] == "N":
            **print**("\nBye!")
            flag = False
            flag_N = True

    **if** flag_N == **True**:
        **break**
```

è€Œä¸”ï¼Œç»“æœï¼ğŸ˜

```
**Please enter your text:** 
The Vatican Apostolic Library (), more commonly called the Vatican Library or simply the Vat, is the library of the Holy See, located in Vatican City. Formally established in 1475, although it is much older, it is one of the oldest libraries in the world and contains one of the most significant collections of historical texts. It has 75,000 codices from throughout history, as well as 1.1 million printed books, which include some 8,500 incunabula.   The Vatican Library is a research library for history, law, philosophy, science and theology. The Vatican Library is open to anyone who can document their qualifications and research needs. Photocopies for private study of pages from books published between 1801 and 1990 can be requested in person or by mail.   In March 2014, the Vatican Library began an initial four-year project of digitising its collection of manuscripts, to be made available online.   The Vatican Secret Archives were separated from the library at the beginning of the 17th century; they contain another 150,000 items.   Scholars have traditionally divided the history of the library into five periods, Pre-Lateran, Lateran, Avignon, Pre-Vatican and Vatican.   The Pre-Lateran period, comprising the initial days of the library, dated from the earliest days of the Church. Only a handful of volumes survive from this period, though some are very significant.

**Please enter your question:** 
When was the Vat formally opened?

**Answer:**
1475
 **Do you want to ask another question based on this text (Y/N)?** Y

**Please enter your question:** 
what is the library for?

**Answer:**
Research library for history , law , philosophy , science and theology

**Do you want to ask another question based on this text (Y/N)?** Y

**Please enter your question:** 
for what subjects?

**Answer:**
History , law , philosophy , science and theology**Do you want to ask another question based on this text (Y/N)?** N

Bye!
```

ç§å•Šã€‚å¾ˆå¥½ç”¨ï¼ğŸ¤—

æˆ‘å¸Œæœ›è¿™ç¯‡æ–‡ç« èƒ½è®©ä½ äº†è§£æˆ‘ä»¬å¦‚ä½•è½»æ¾åœ°ä½¿ç”¨æ¥è‡ªæ‹¥æŠ±é¢éƒ¨å˜å½¢åº“çš„é¢„è®­ç»ƒæ¨¡å‹å¹¶æ‰§è¡Œæˆ‘ä»¬çš„ä»»åŠ¡ã€‚å¦‚æœä½ æƒ³æŠŠä»£ç çœ‹åšä¸€ä¸ªç¬”è®°æœ¬æ–‡ä»¶ï¼Œè¿™é‡Œæœ‰ [Github](https://github.com/chetnakhanna16/CoQA_QuesAns_BERT/blob/main/CoQA_BERT_QuestionAnswering.ipynb) é“¾æ¥ã€‚

**å‚è€ƒæ–‡çŒ®:**

1.  [https://huggingface.co/](https://huggingface.co/)
2.  [https://arxiv.org/pdf/1810.04805.pdf](https://arxiv.org/pdf/1810.04805.pdf)
3.  [https://arxiv.org/pdf/1808.07042.pdf](https://arxiv.org/pdf/1808.07042.pdf)
4.  [https://github.com/google-research/bert/issues/44](https://github.com/google-research/bert/issues/44)

æ„Ÿè°¢å¤§å®¶é˜…è¯»è¿™ç¯‡æ–‡ç« ã€‚è¯·åˆ†äº«æ‚¨å®è´µçš„åé¦ˆæˆ–å»ºè®®ã€‚å¿«ä¹é˜…è¯»ï¼ğŸ“— ğŸ–Œ