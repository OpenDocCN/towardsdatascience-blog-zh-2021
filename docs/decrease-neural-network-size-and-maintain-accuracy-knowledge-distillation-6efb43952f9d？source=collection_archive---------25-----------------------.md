# 降低神经网络规模并保持准确性:知识蒸馏

> 原文：<https://towardsdatascience.com/decrease-neural-network-size-and-maintain-accuracy-knowledge-distillation-6efb43952f9d?source=collection_archive---------25----------------------->

## 有些神经网络太大，无法使用。有一种方法可以使它们更小，但保持其准确性。请继续阅读，找出方法。

![](img/30eebe4771a79b7eb23abd4d6763e814.png)

埃弗里·埃文斯在 [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral) 上拍摄的照片

实用的机器学习都是关于权衡的。我们可以通过使神经网络变大来获得更好的准确性，但在现实生活中，大型神经网络很难使用。具体来说，问题不是出在训练上，而是出在部署上。大型神经网络可以在巨型超级计算机集群上成功训练，但当需要在常规消费设备上部署这些网络时，问题就出现了。普通人的电脑或电话无法处理运行这些大型网络。因此，如果我们想在实践中使用这些网络，我们需要在保持精度的同时减小它们的尺寸。这可能吗？

这是一个重要的问题，所以做了很多研究也就不足为奇了。我想强调两种方法。一种方法是在训练过程中进行规模缩减，系统地删除网络中最不重要的权重。这叫做修剪，如果你感兴趣，你可以在这里阅读更多相关内容[。](/make-your-neural-networks-smaller-pruning-da1fcdb6f206)

第二种方法是我们将在这里讨论的，称为**知识提炼**。我们首先以全尺寸训练网络，而不是在训练期间减小网络尺寸。然后，我们训练另一个更小的网络，使用完全训练的大网络作为真理的来源。使用网络来训练其他网络的想法是一种迁移学习。在高层次上，知识升华是一种分两步走的迁移学习形式。首先，训练大网。然后，用大网训练最后，小网。现在我们来看看细节。

因为知识提炼有两个训练步骤，我们自然地将数据分成两个训练集。第二个训练集(用于训练小型神经网络)称为**转移集**。因为在第二个训练步骤中，我们使用大的神经网络作为事实的来源(与数据标签相反)，**我们不需要将转移集标记为**。这是一个很大的优势，也是使用像蒸馏这样的迁移学习方法的原因之一。我们来澄清一下这是什么意思。典型的机器学习范式是使用一些数据 x 和标签 y 来学习一个逼近 y 的函数 f(x)，我们对转移集所做的是用 g(x)代替 y，其中 g 是经过训练的大网络。所以不需要 y。

为什么这样好？在现实生活中，标签很难获得。通常，标签是由人类创建的，因此对于大型数据集，必须投入大量精力来标记数据。这需要花费金钱和时间。有了知识的升华，我们只需要标签来训练大网络，也就是说我们只需要标注我们数据的一部分而不是全部。

训练大型网络的过程在一个方面不同于常规训练。不同之处在于从倒数第二层到输出层的 softmax 激活函数。回想一下，softmax 函数的目的是使最终输出节点总和为 1。该函数的形式为 o_i = exp(z_i)/σ_ j(exp(z _ j))，其中 o _ I 为第 I 个输出层节点，z _ I 为倒数第二层节点。在知识提炼中，我们对这个公式进行了调整。具体来说，在第一阶段(训练大网络)，我们将函数改为 exp(z _ I/T)/σ_ j(exp(z _ j/T))，其中 T 为大于 1 的常数。t 控制最终输出的所谓的“**柔和度**”。t 本身称为**温度**。T(更高的温度)越大，最终输出将越接近(更柔和)均匀。在知识提炼的文献中，你经常会看到“使用软目标”这个短语——使用高温就是这个短语的意思。

为什么我们要用一个很大的 T 值？两个原因。第一，在大网络中使用软目标的时候，到了训练小网络的时候，**梯度的方差会更小**。这让我们提高了小网络的学习率，并使用更少的样本。第二，软目标可以提供比常规目标更多的信息。例如，假设 softmax 的常规输出是向量[0.98，0.011，0.009]。如果 T 值很高，输出可能会变成类似于[0.8，0.12，0.08]的值。第二个输出显示了第二个和第三个输出节点之间的明显差异(0.12 对 0.08)，而第一个输出(0.011 对 0.009)几乎没有显示出来。因此，[0.8，0.12，0.08]输出有更多的信息，我们宁愿用它来训练这个小网络。

如前所述，一旦训练了大网络，就使用转移集上的大网络输出(将是软的)作为标签，在转移集上训练小网络。这个小型网络的培训过程是常规的，这里没有特殊的步骤。综上所述，知识升华的整个过程自始至终是:

1.把你的训练集分成两组:第一组训练大网络，第二组(转移)训练小网络。

2.用第一个训练集用高温训练大网络。使用高温的目的是获得更软的目标，它比常规目标包含更多的信息。

3.使用转移集上的大网络输出作为真实的来源来训练转移集上的小网络。

我们现在已经解释了知识提炼是如何工作的。还有更复杂的版本，但从概念上讲，它们和我们的简单版本是一样的。现在——它真的有用吗？是的。做了多个实验，我们来看一个最简单的。研究人员首先在 MNIST 数据集上训练了一个大型神经网络(2 个隐藏层，每层 1200 个节点)。在测试集上，这个大型神经网络有 67 个错误。接下来，研究人员在 MNIST 数据集上训练了一个较小的网络(2 个隐藏层，每层 800 个模式)。在测试中，这个较小的神经网络有 146 个错误。如果用来自大型网络(温度 T = 20)的知识提炼来训练这个较小的网络，它在测试集上只有 74 个错误，这与大型网络测试性能相当。这个实验证明了知识提炼确实将大部分知识从大网络传输到小网络，并且还证明了没有大网络，小网络就不能实现可比较的性能。语音识别等其他领域的实验也已经完成——你可以在最初的[论文](https://arxiv.org/pdf/1503.02531.pdf)中读到更多关于它们的内容。

底线:知识提炼是在较小的网络中实现较大的网络性能的一种有前途的技术。它解决了大型网络因其尺寸而无法使用的实际问题，为消费设备上更高性能的神经网络打开了大门。我期望看到这种技术在未来得到广泛应用。感谢阅读，并请留下任何问题/评论！

名词（noun 的缩写）B 12/21:我看到了一篇优秀的[文章](https://neptune.ai/blog/knowledge-distillation)，这篇文章更深入地解释了知识的升华。对于对这个主题感兴趣的人来说，这是一本很好的读物。