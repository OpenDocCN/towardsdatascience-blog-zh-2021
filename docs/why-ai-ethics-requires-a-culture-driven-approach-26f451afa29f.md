# 为什么人工智能伦理学需要文化驱动的方法

> 原文：<https://towardsdatascience.com/why-ai-ethics-requires-a-culture-driven-approach-26f451afa29f?source=collection_archive---------41----------------------->

## 许多组织都缺少一大块:通过合作建立代表原则的文化。

![](img/06ee5e7d465534c3c1527bfaeb26642f.png)

照片由 [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral) 上 [43 点击北](https://unsplash.com/@43clicksnorth?utm_source=medium&utm_medium=referral)

蒂姆尼特·格布鲁(Timnit Gebru)是一名埃塞俄比亚裔美国人，以其进步的人工智能伦理研究而闻名，他是谷歌伦理人工智能团队的联合负责人，周四在一条推文中表示，她因一篇强调人工智能偏见的研究论文而被谷歌解雇。虽然人工智能伦理研究人员和社会学家表达了他们对这一举动的担忧，但在当前组织对人工智能伦理的方法中存在着明显的差距。那就是合作建立文化来代表人工智能伦理原则。

数据或技术的所有权或使用权总是不平衡的，一边是经济和政治，另一边是权利和透明度。由于对偏见、政治影响、仇恨言论和歧视的担忧，技术伦理(人工智能伦理)正在成为全球科技企业在平台或产品伦理和负责任的人工智能方面的董事会议。

学术智库和技术巨头已经开发并分享了人工智能伦理的几个框架和原则。像谷歌和脸书这样的公司已经公开了他们的道德原则，并启动了一个分享见解的过程，分享他们在当前时代用什么和如何处理关键的道德问题。通常，框架或公司原则试图涵盖许多主题(如果不是全部的话)，包括隐私、不歧视、安全和安保、问责制、透明度和可解释性以及保护人类价值。

**聚焦人工智能伦理**

公司的努力围绕三个主要领域发展:( a)建立原则、政策、指南、清单和焦点小组，以处理人工智能伦理，包括负责任的人工智能领导者和/或产品经理；(b)开展研究，以了解关键的道德问题并找到解决方案，(必要时利用或配合学术/学者的支持)，并定期发布或分享有关努力或研究成果的最新情况；(c)使战略和倡议与人工智能伦理原则相一致，并在某些情况下带来或建立工具来帮助整个社区解决选定的伦理挑战。

这些努力虽然值得称赞，但极其有限。这些努力中有许多是由少数个人或群体做出的，公平和责任的定义是非常动态的或不断发展的，并且这些努力是针对最明显的挑战。它没有解决组织内存在的相互冲突的方法。这其中缺少了一大块，那就是合作建立文化来代表原则。

**缺失的缺口**

关于人工智能中的伦理，我们揭露的挑战比实际上要少。当 Joy Buolamwini 在 2017 年的 Ted 演讲(我如何与算法中的偏见作斗争)中发言时，使用面部识别的组织开始重新审视他们的产品和这类产品中固有的偏见。算法中的偏见并没有就此结束，随着越来越多的模型被开发，越来越多的数据被注释，越来越多的用例被识别，它还在继续。作为一个社会，我们天生就有偏见，并试图在某些方面采取初步措施来恢复(例如男女薪酬平等)

虽然政策和原则是一个很好的开始，但培养这种文化可能是持久的；人们有着共同的使命，并自愿与负责任的行为保持一致。文化不能用一套任务来建立；对于那些已经将人工智能伦理原则确立为他们的目标的组织来说，他们需要着眼于朝着它前进。这一进程将需要协调一些因素，包括信念(接受某事存在/为真，无需证明)、感知(理解或看待某事的方式)、身份(决定某人/某事是谁或什么的特征)、意象(视觉象征)、判断(结论或意见)和为任务协同工作的利益相关者的情绪(本能或直觉)。

**人工智能伦理的文化驱动方法**

这些因素可能都不仅仅取决于事实。这些因素中的每一个都对我们个人和集体的思想和行为产生独立或相互依赖的影响。让我们看看关键因素，以及影响这些因素是如何驱动文化现象的:

**1。培养对道德准则的更广泛的情感:**

情绪是公司所代表的原则的强有力的传达者和影响者。他们在宏观层面上是有效沟通的推动者，在个人层面上，它激发了许多携带这种原则情感的个人的自豪感和人生目标。结构化的交流和参与故事，生活经历，努力使社会的一部分从父权制中发展出来，是情感可以发展的一些方式。当对人工智能伦理的规定原则采取行动和努力时，情绪能够产生积极的神经反应，并对偏离这些原则表现出消极的神经反应，从而起到威慑作用。

**2。向组织灌输对道德原则的信念**

信念和感知很重要，因为员工并不总是了解组织对人工智能伦理原则的感受有多深。灌输共同的信念和认知需要战略方法来塑造与目标同步的商业模式，协调各级领导，设计沟通和展示代表性行为。例如，如果利益相关者不认为一个组织解决歧视问题的努力是有限和不可靠的，那么上述努力(原则、政策、研究等)对他们的影响将是有限的。

如果组织期望员工和利益相关者以某种方式行事，就有必要注意灌输这些信念。要做到这一点，可以将道德原则作为员工和利益相关方目标的一部分，或者确保道德原则成为利益相关方的关键战略讨论点。例如，Martin Fishbein 和 Icek Ajzen 在他们的“理性行动理论”中提到，行为的意图先于实际行为，这种意图是相信实施这种行为会导致某些结果的结果。这些反过来帮助组织创造一个环境，帮助人们提高他们对组织应该代表的价值观和原则的声音，从而引导方向。

**3。不断丰富信念和情感**

灌输信念和情绪的方法本身不会有显著的影响，除非这种努力是一致的。在这种情况下，保持一致并不局限于做同样的事情，而是通过每一次尝试来丰富其中的努力。例如，确定或创新与员工和利益相关者互动的方式，将故事讲述与真实世界的见解/事件联系起来等。Eric Van den Steen 在他的研究(“共同信念的起源”)中提到，人们更喜欢与分享他们信念和假设的人一起工作，因为这样的人“会做正确的事情”，并坚持认为信念会随着共同学习的时间而演变。这种方法必须渗透到新员工，包括横向招聘、第三方和业务合作伙伴，尤其是董事会和高级管理层。

**结论**

在某些方面，在人工智能伦理问题上保持沉默可能会反映出组织的不良表现，并表现出共谋，正如在 Gebru 的案例中可以看到的那样。这与组织内外沟通方式的语义差异无关。着眼于灌输共同的信念或实现积极的感知，并创造一种对人工智能道德的令人信服的情感，这不仅是必不可少的，而且是品牌和业务蓬勃发展的必要条件，因为与既定原则/价值观不一致的声誉的负面影响可能是灾难性的。因此，有必要对人工智能伦理有一个整体的看法，并通过向各种利益相关者灌输价值观来合作建立负责任的人工智能或人工智能伦理文化。

此前发布于 Linkedin Pulse ( [此处](https://www.linkedin.com/pulse/why-ai-ethics-requires-culture-driven-approach-narayanan/))