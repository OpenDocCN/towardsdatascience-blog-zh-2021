# 人工智能必须是可理解的才是道德的吗？

> 原文：<https://towardsdatascience.com/does-ai-have-to-be-understandable-to-be-ethical-b86747594df2?source=collection_archive---------36----------------------->

## [播客](https://towardsdatascience.com/tagged/tds-podcast)

## Margot Gerritsen 在 [TDS 播客](https://towardsdatascience.com/podcast/home)

*编者按:这一集是我们关于数据科学和机器学习新兴问题的播客系列的一部分，由 Jeremie Harris 主持。除了主持播客，Jeremie 还帮助运营一家名为*[*sharpes minds*](http://sharpestminds.com)*的数据科学导师初创公司。可以听下面的播客:*

[苹果](https://podcasts.apple.com/ca/podcast/towards-data-science/id1470952338?mt=2) | [谷歌](https://www.google.com/podcasts?feed=aHR0cHM6Ly9hbmNob3IuZm0vcy8zNmI0ODQ0L3BvZGNhc3QvcnNz) | [SPOTIFY](https://open.spotify.com/show/63diy2DtpHzQfeNVxAPZgU) | [其他](https://anchor.fm/towardsdatascience)

随着人工智能系统变得越来越普遍，人们开始更多地关注它们的伦理含义。这些潜在的影响是巨大的:谷歌的搜索算法和 Twitter 的推荐系统都有能力有意义地左右公众对任何问题的看法。因此，谷歌和推特的选择有着巨大的影响——不仅对他们的直接用户群，而且对整个社会。

这种权力伴随着被有意滥用的风险(例如，Twitter 可能会选择提升表达与他们偏好的政策一致的观点的推文)。尽管故意误用是一个重要问题，但同样具有挑战性的是避免人工智能系统无意中产生不良输出的问题。

非故意的坏人工智能可能会导致各种偏见，使算法对一些人来说比其他人表现得更好，或者更普遍地说，使系统对我们实际上长期不想要的东西进行优化。比如，Twitter 和 YouTube 等平台在美国(以及全世界)用户群日益两极分化的过程中发挥了重要作用。当然，他们从来没有打算这样做，但他们对社会凝聚力的影响可以说是基于狭隘的度量优化的内部文化的结果:当你为了短期参与而优化时，你往往会牺牲长期的用户福祉。

几乎根据定义，人工智能系统的意外后果很难预测。但它们的潜在影响使它们非常值得思考和讨论——这就是为什么我在本期播客中采访了斯坦福大学教授、数据科学中的女性(WiDS)倡议的联合主任和 WiDS 播客的主持人 Margot Gerritsen。

以下是我最喜欢的外卖:

*   为了确保人工智能系统合乎道德地运行，我们需要了解它们是如何得出结论的。这就要求这些系统是*可解释的*。如果我们不仅关心人工智能将预测什么，而且关心它为什么做出预测，那么可解释性就成为建立伦理系统的先决条件。
*   科学理论是人类可以解释的，但人工智能——其中一些被设计来做出与这些理论相同的预测——不是。探究这是为什么很有趣。我们的结论是，答案与表达科学理论的语言有关，例如，像物理学理论。它们往往指的是我们知道或能够概念化的物体，如电子或光粒子。人工智能系统完全没有这种限制:原则上，它们可以用完全抽象的术语“思考”世界，这些术语不能映射到人类可以理解的概念上。这就是为什么可解释性研究的一些分支探索我们如何约束人工智能用来进行预测的抽象概念，使它们成为人类可以理解的。
*   任何时候，一项重要的技术——比如这次的人工智能——迅速起飞，它的演变和行为都会反映出它发展时所处的文化泡沫。世界上大多数先进的人工智能都是由硅谷的科技公司开发的。这些公司有相当强烈的典型人口统计和政治偏见，这就是为什么它们必须做出特别努力，纳入在硅谷技术领域代表性不足的观点，例如某些少数族裔的观点(非亚洲人和非白人[相对于总人口而言，在技术领域代表性不足](https://recruitinginnovation.com/blog/diversity-in-tech/))、女性(在技术员工中，[占 20%](https://recruitinginnovation.com/blog/diversity-in-tech/) )和政治保守派(在过去 5 次大选中，三藩市和山景城[有 80%至 85%的民主党人](https://en.wikipedia.org/wiki/Politics_of_San_Francisco))。

你可以[点击这里](https://twitter.com/margootjeg)在 Twitter 上关注玛戈特，或者[点击这里](https://twitter.com/jeremiecharris)在 Twitter 上关注我

## 播客中引用的链接:

*   斯坦福网站上玛戈特的页面在这里。

![](img/4befd254207b0e094efb5facfab6cedc.png)

## 章节:

*   0:00 介绍
*   今天使用人工智能的 1:00 个问题
*   4:12 解释和结果之间的自然分离
*   15:55 相信数据
*   19:54 应用于数据的工具
*   26:36 预测结果
*   35:38 结合数学
*   45:27 冲向底部
*   56:18 硅谷文化与多元化问题
*   1:04:43 文化差异
*   1:06:56 AI 研究的责任
*   1:09:22 公平标准
*   1:11:06 计算公平性
*   1:18:00 这些系统的复杂性
*   1:19:43 数据科学领域的女性
*   1:21:48 总结