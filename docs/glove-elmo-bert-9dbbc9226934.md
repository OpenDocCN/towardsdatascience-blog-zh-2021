# 埃尔莫&伯特手套公司

> 原文：<https://towardsdatascience.com/glove-elmo-bert-9dbbc9226934?source=collection_archive---------4----------------------->

## 使用 Spark NLP 的最新文本分类指南

![](img/98380dc8446743830e275125967da6d5.png)

阿玛多·洛雷罗在 [Unsplash](/photos/GkinCd2enIY?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText) 上的照片

机器学习模型最具挑战性的任务之一是找到最佳方式来生成单词的数字表示，以便模型可以在计算中使用这些信息。

在计算机视觉任务中，彩色(RGB)图像中的红色通道总是指红色通道，绿色通道指绿色通道。然而，文本在很大程度上是基于上下文的，因此同一个词可以根据它的用法有多种含义。例如，熊猫可以指可爱而模糊的熊或 Python 数据分析库。

当考虑句子和段落时，这变得更加复杂。请考虑以下情况:

*熊猫可爱又毛茸茸。他们不用熊猫数据分析库，因为他们是熊。*

现在我意识到这不是一个普利策奖获奖故事的开头，但重点是要引起你对第二句话中的单词 **They** 的注意。他们是谁？为了回答这个问题，必须有一个内置的内存组件，允许算法生成与第二句中的**熊猫**相似而与第二句中的**熊猫**不同的**熊猫**(第一句中)的表示。

今天，我将通过使用开源自然语言处理库 **Spark NLP** 的示例，使用最先进的算法来执行文本分类任务，这些算法已经尝试处理上述挑战(以及其他挑战)。

我将比较以下模型的结果

*   **手套**
*   **埃尔莫**
*   **伯特**

## 什么是 Spark NLP？

在我们进入文本分类挑战之前，先简单描述一下 Spark NLP:

> [Spark NLP](https://vkocaman.medium.com/) 是一个开源的自然语言处理库，构建在 Apache Spark 和 Spark ML 之上。它提供了一个简单的 API 来集成 ML 管道，并由 John Snow 实验室提供商业支持。Spark NLP 的注释器利用基于规则的算法、机器学习和一些在引擎盖下运行的 [Tensorflow](https://www.tensorflow.org/) 来支持特定的深度学习实现。

我鼓励每个人看一看 [Spark NLP](https://medium.com/spark-nlp) 的媒体页面和他们的 [Github](https://github.com/JohnSnowLabs) 以获得大量 NLP 任务的深入教程。

如果您的笔记本已经打开并准备好，您可以通过使用下面的代码安装和导入必要的包来开始使用 Spark NLP。

瞧啊！现在我们准备开始工作了。

## 数据集

对于今天的挑战，我们将使用在找到的[数据集。在这个](https://www.kaggle.com/c/nlp-getting-started)[回购](https://github.com/ryancburke/nlp_disasterTweets)上，你也可以跟着我的笔记本。

首先，您可以阅读。csv 文件，使用以下代码输出数据帧示例:

我们感兴趣的是按目标对我们的文本列进行分类，其中 **0 =无灾难**和 **1 =灾难**。使用下面的代码可以让我们看到目标变量的分布。

有 **1211 个空值**，它们对我们的分类没有用处。我们可以使用下面的代码将它们从数据帧中删除。

接下来，我们将经历我用来转换原始文本的预处理步骤。

## 构建预处理管道

Spark NLP 管道中的每一步都需要某些列作为输入，并产生不同类型的新列作为输出。在这一节中，我们将回顾在整个项目中保持一致的预处理管道。关于所有 Spark NLP [转换器](https://nlp.johnsnowlabs.com/docs/en/transformers)和[注释器](https://nlp.johnsnowlabs.com/docs/en/annotators)的完整描述，包括所有可修改的参数，请参见链接。

*这是我们管道的入口点，它将我们的原始文本转换成一个*文档*类型。这个注释器使用包含我们的 tweets 的输入列 *text，*，并输出一个新列 *document* 。我还选择了清理模式*收缩*来去除空白(新行、制表符、多个空格)。*

*2. ***【标记器()】****

*记号赋予器将我们的句子分解成单独的成分(例如，单词或短语)。这个注释器的输入是*文档*列，它将输出一个新列*令牌*。我修改了参数 *SplitChars* 来拆分由连字符连接的标记，还修改了参数 *ContextChars* 来删除标记中的某些字符，如下面的代码所示。*

*3. ***规格化器()****

*规范化器是一个文本清理注释器，它接受来自我们的*标记*列的输入，并输出和*规范化的*列。参数 *CleanupPatterns* 用于删除所有标点符号，同时保留所有字母数字字符。*

*4.*【stopwodsguner】)**

**停用字词注释器删除基于库的预定义字词列表。这个库是 Spark ML 和 scikit-learn 使用的同一个库，完整的列表可以在[这里](http://ir.dcs.gla.ac.uk/resources/linguistic_utils/stop_words)找到。这些单词通常是非常常用的单词，它们在文本分类问题中不会提供任何效用(例如，and、am、be、do、for、get、no)。**

**5. ***Lemmatizer()*****

**词汇化指的是一个规范化文本的过程，其目标是通过将派生相似的单词转换为基本形式(例如，writes、written、written、writer → **write** )来减少可变性。你会注意到下面 Spark NLP 有一个预训练的词条库[*ant BNC*](https://www.laurenceanthony.net/software/antconc/)*，它是基于英国国家语料库中的单词。***

***下面你可以看到预处理管道的完整代码。在最后一个模块中，管道被定义为我们已经介绍过的步骤序列，从原始文本开始，到词条结束。***

## ***可视化管道的效果***

***接下来，我们将通过一个例子来看看原始文本在预处理管道的每一步是如何被转换的。为此，我们将使用下面简要介绍的 Spark NLPs *光管道*。***

> **[光管道](https://medium.com/spark-nlp/spark-nlp-101-lightpipeline-a544e93f20f1)是 Spark NLP 专用管道，相当于 Spark ML 管道，但意在处理更少量的数据。它们在处理小型数据集、调试结果，或者从服务于一次性请求的 API 运行训练或预测时非常有用。**

**下面你可以看到，拟合由 7000 多条推文组成的火车数据帧花了 15.2 秒。**

**假设我们想要保存这个预处理模型，并将其用于另一个任务。下面，我们使用一个示例文本创建一个 Spark 数据帧，然后使用相同的管道转换它。尽管文本的大小是训练数据帧的 1/7000，但它花费了 1.55 秒(1/10)的时间。**

**进入*光管线*。正如你在下面看到的，在 121 毫秒，这增加了将近 13%的速度**。****

**现在，为了可视化这个预处理管道将如何影响我们的数据帧，我们可以使用下面的代码。**

**为了便于阅读，我将结果插入了一个表格。**

***注意，在第 10 行你会看到<逗号>。当我创建一个要点时，我不得不替换实际的逗号，因为它将此解释为另一列。***

**也就是说，我们可以看到规格化器是如何去掉标点符号的。停用词清理器移除了诸如 *as、she、The 等词…* 最后，词条整理器将词转换为它们的词条，例如 *sat → sit，watching → watch，catch→catch。***

**到目前为止，我们已经清理了我们的数据帧，但它仍然包含文本。在下一节中，我们将研究各种用于将明文转换成数字表示的预训练嵌入。**

## **单词嵌入**

**如上所述，本节将简要描述用于转换干净文本的不同嵌入。详细讨论每一个的机制已经超出了本文的范围，但是我将为那些想看得更深一点的人提供原始论文的链接。**

*   **[**手套**](https://nlp.stanford.edu/pubs/glove.pdf) (单词表示的全局向量)**
*   **[**ELMo**](https://arxiv.org/abs/1802.05365) (来自语言模型的嵌入)**
*   **[**伯特**](https://arxiv.org/abs/1810.04805) (来自变压器的双向编码器表示)**

**单词嵌入生成单词的多维向量表示。目标是为具有相似含义的单词生成相似的表示。比如*海啸、飓风、龙卷风*可能有类似的表示，但是*龙卷风*呢？这就是语境重要性的一个例子。Twister 可以指游戏或龙卷风，但是游戏和龙卷风肯定不应该有类似的表示。让我们快速看一下每个模型是如何创建嵌入的。**

**根据创造者的说法，手套是:**

> **本质上是具有加权最小二乘目标的对数双线性模型。该模型背后的主要直觉是简单的观察，即单词-单词共现概率的比率具有对某种形式的意义进行编码的潜力**

**考虑以下我们的目标 ***灾难*** 和 ***无灾难*** 和四个单词(风、损害、汉堡、驾驶)的同现概率的虚构示例。对此的解释是，**比率**为 1 意味着分子**P(k |灾难)**和分母**P(k |无灾难)**之间没有差异。随着值变得大于 1，它与 ***灾难*** 相关联的概率增加。反之，随着数值变得小于 1，它与 ***无*** ***灾难*** 相关联的概率增加。**

**从我们的例子来看，*伤害*与我们的目标 ***灾难*** 关联最强，而*汉堡*与目标 ***无灾难*** *关联最强。*驱动的*风*和*都接近于 1，意味着与任一目标都没有强关联。***

**GloVe 的一个局限性是，它没有考虑到单词在什么样的上下文中使用。考虑这两句话:**

*   ***喜欢开着我的敞篷车，头发上带着* ***风*** *！***
*   ******风*** *那么大，我的敞篷车就这么飞走了！****

**ELMo 的创造者找到了一种方法来解决这个问题，从而产生了语境化的单词嵌入。这里，没有生成固定的嵌入。取而代之的是，在分配嵌入之前，先读取整个句子。使用双向 LSTM，ELMo 学习下一个和上一个单词。**

**GloVe 和 ELMo 都是在无人监督的情况下对大量文本进行预训练的。一个关键的区别是，使用 GloVe，我们将学习到的向量用于一些下游任务。对于 ELMo，我们使用学习向量**和**预训练模型作为一些下游任务的组件。**

**如果 NLP 的研究人员不那么喜欢布偶，我认为比 BERT 更好的名字应该是 Frankenstein，因为它包含了许多最近的突破。为了获得非常详细和有用的指南，我鼓励你看看这篇[文章](http://jalammar.github.io/illustrated-bert/)。**

> **BERT 建立在最近在 NLP 社区兴起的许多聪明想法的基础上——包括但不限于[半监督序列学习](https://arxiv.org/abs/1511.01432)(由 Andrew Dai 和 Quoc Le)[ELMo](https://arxiv.org/abs/1802.05365)(由 Matthew Peters 和来自[的研究人员](https://allenai.org/)和[UW CSE](https://www.engr.washington.edu/about/bldgs/cse))[ulm fit](https://arxiv.org/abs/1801.06146)(由 fast.ai 创始人和 Sebastian Ruder)[OpenAI 转换器](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf)(由 OpenAI 研究人员、Narasimhan**

**尽管 ELMo 使用双向 LSTMs，但是它连接正向和反向模型的事实限制了表示同时利用两种上下文的能力。**

**伯特使用了一个掩蔽语言目标，意思是单词被随机隐藏并被一个掩蔽所取代。使用一个变换器，然后使用它周围的非屏蔽单词(左右)来预测屏蔽单词。**

**在下一节中，我们将比较这些模型的嵌入如何用于将**灾难**与**非灾难**进行分类。**

## **结果**

**在公布结果之前，我想花点时间介绍一下所使用的分类器。 [ClassifierDL](https://nlp.johnsnowlabs.com/docs/en/annotators) 是由 John Snow Labs 在 TensorFlow 内部构建的通用分类器，支持多达 100 个类。我设置的关键参数是:**

*   **学习率(0.001)**
*   **批量大小(8)**
*   **最大训练次数(5)**

**还值得注意的是，我使用以下方法将原始数据帧分成训练集和验证集:**

1.  ****手套****

**下面你可以找到完整的管道，它是我上面描述的预处理管道的扩展。如您所见，GloVe word 嵌入被用作另一个 Spark NLP 注释器 SentenceEmbeddings 的输入，然后使用 ClassifierDL 进行分类。**

**然后使用下面的代码对验证集进行预测，预测的示例显示在下面的表中。**

**在显示的十个预测中，一个错误完美地突出了手套嵌入的局限性。由于缺乏上下文，我想要一些海啸外卖被归类为灾难。**

**使用 sklearn，我们可以看到手套嵌入可以很好地分类我们的文本。事实上，以 81%的准确率，它做得相当不错！**

**2.埃尔莫**

**接下来，让我们看看 ELMo 嵌入是否能够提高分类精度。要使用预训练的 ELMo 嵌入件，只需将以下部件插入上述管道，作为手套嵌入件的替代品。**

**使用上面所有相同的代码，对验证集进行了预测，如下所示。这里我们看到 ELMo 能够学习手套漏掉的单词*海啸*的上下文。干得好埃尔莫！**

**尽管如此，模型精度仅提高了 1%。**

**3.**伯特****

**最后，让我们看看 BERT 嵌入在分类任务中的表现。就像我们上面做的一样，简单地把下面的插入同一个点。**

**再次展示了验证集的预测示例。有趣的是，我们看到 BERT 并没有正确地对句子进行分类，*见见 Brinco 你自己的地震和海啸预警信标。***

**不过，这次我不得不站在伯特一边。在我看来，这条推文描述的是一个灾难探测系统，而不是一场灾难。**

**无论哪种方式，BERT 嵌入的最终结果都没有提高 ELMo 所达到的精度。**

## **我们学到了什么？**

**今天我们讨论了几个使用 Spark NLP 进行文本分类的例子。我们重点介绍了 3 个单词嵌入模型，提供了如何将它们包含在您自己的分类任务中的分步说明:**

*   ****手套****
*   ****埃尔莫****
*   **伯特**

**虽然结果变化不大，但我们看到了一些不同嵌入模型如何工作的例子。具体来说，我们能够看到手套嵌入缺乏背景。它无法区分海啸、餐馆和真正的灾难。**

**这个项目专注于单词嵌入。在未来的帖子中，我们将探索 Spark NLPs 句子嵌入选项，如通用句子编码器(USE)和 BERT 句子嵌入。**

**感谢阅读！**

## **有用的资源**

**[NER 和伯特在 Spark NLP](/named-entity-recognition-ner-with-bert-in-spark-nlp-874df20d1d77)**

**[使用 Bert 和通用语句编码器在 Spark NLP 中进行文本分类](/text-classification-in-spark-nlp-with-bert-and-universal-sentence-encoders-e644d618ca32)**

**[如何在两周内开始使用 spark NLP](/how-to-get-started-with-sparknlp-in-2-weeks-cb47b2ba994d)**

**[图示变压器](https://jalammar.github.io/illustrated-transformer/)**

**[可视化神经机器翻译模型(注意 Seq2seq 模型的机制)](https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/)**

**[插图作者伯特·埃尔莫等人(NLP 如何破解迁移学习)](http://jalammar.github.io/illustrated-bert/)**

**[语言建模二:ULMFiT 和 ELMo](/language-modelingii-ulmfit-and-elmo-d66e96ed754f)**

**[单词嵌入(第二部分) :直觉和(一些)数学来理解端到端手套模型](/word-embedding-part-ii-intuition-and-some-maths-to-understand-end-to-end-glove-model-9b08e6bf5c06)**