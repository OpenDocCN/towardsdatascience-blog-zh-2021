# 公平且可解释的机器学习

> 原文：<https://towardsdatascience.com/fair-and-explainable-machine-learning-25b802b00bec?source=collection_archive---------24----------------------->

## [模型可解释性](https://towardsdatascience.com/tagged/model-interpretability)

## 关于如何防止机器学习模型中的偏差并理解其决策的指南。

![](img/3b5b125a16299568bf878dd0504f56bd.png)

由[尼古拉斯·乔西](https://unsplash.com/@nicopic?utm_source=medium&utm_medium=referral)在 [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral) 上拍摄的照片

> “虽然可以说神经网络可以编写自己的程序，但它们是朝着人类设定的目标，使用为人类目的收集的数据来这样做的。如果数据有偏差，即使是偶然的，计算机也会放大不公正。”
> 
> ——卫报[1]

# 介绍

由于围绕使用算法作为自动决策工具的伦理问题，机器学习在医学、金融和教育等领域的应用目前仍然非常复杂。

这种不信任的两个主要根源是:偏见和低可解释性。在本文中，我们将探讨这两个概念，并介绍可以应用的不同技术，以使我们的模型更加公平和可解释。

# 偏见

在机器学习中，偏差这个术语通常与我们试图预测的正确值和我们的模型做出的预测之间的差异相关联。然而，在认知科学中，偏见一词可以有许多其他含义。认知偏差的一些例子有[2]:

*   **报告偏差:**与我们数据中考虑的其他结果/特性相比，结果/特性在我们数据中记录的频率并不能真正代表它在现实世界中出现的频率。
*   **选择偏倚:**从不能真正代表关键群体特征的群体中选择样本(非随机样本)。
*   **过度概括:**过度简化流程可能会导致错误的结论。
*   **相关性谬误:**将虚假的相关性误解为因果驱动的过程。
*   **确认偏差:**只专注于寻找证实我们先前信念的来源而不是公正地评估关于一个主题的不同观点/选项。

如果偏见嵌入到我们的数据和/或机器学习模型中，这可能会导致**算法偏见**(有偏见的自动决策工具)。为了试图理解机器学习模型是否有偏见，最直观的方法之一是分解对我们模型的评估。

例如，假设我们训练了一个模型来分类某人是否受到疾病的影响，我们达到了大约 80%的分类准确率。在这一点上，为了确保我们的模型是真正公平的，我们仔细看看如果我们首先只考虑女性，然后只考虑男性，准确性会如何变化。仅考虑女性，我们得到大约 95%的准确率，而仅考虑男性，我们得到大约 55%的准确率！为了试图理解为什么在性能上有如此大的差异，我们可以试着检查我们的训练数据。经过仔细分析后，我们注意到大约 75%的训练数据只包含女性患者的样本，因此我们的模型对这些样本给予了总体上更大的权重，以便最终尽可能降低我们的错误分类分数。因此，进行这种类型的分析有助于我们理解，尽管我们的模型能够整体表现良好，但它不应该潜在地用于医疗环境中，以便对男性患者进行诊断。考虑到每个亚组的准确性差异，将该模型用于男性和女性诊断实际上是“不公平的”。为了进一步分析，应考虑由此产生的混淆矩阵和精确度/召回率指标，以评估假阳性/假阴性率的影响。

当试图理解数据驱动的系统是否可能有偏差时，另一个应该考虑的因素是首先对训练数据是如何产生的进行一些背景研究(例如，参与者是否自愿参与，他们是否来自不同的国家，他们是否有不同的背景，研究是否由任何组织赞助等等)。

因此，在数据科学工作流程中，可以在三个可能的阶段应用偏差缓解算法:**预处理**(在训练数据上)、**处理中**(在训练模型时)、**后处理**(在预测标签上)。这些阶段中最常见的偏置抑制算法有[3]:

*   **预处理:**重新加权，不同影响去除器，学习公平表示。
*   **处理中:**对抗性去偏置，偏见去除器，元公平分类器。
*   **后处理:**拒绝选项分类和校准均衡赔率。

所有这些不同的方法都可以用 Python 实现，利用 [AI Fairness 360 库](https://github.com/Trusted-AI/AIF360)。

# 可解释的人工智能

现代机器学习模型中的一个关键权衡是性能与复杂性。更复杂的模型，如基于深度学习的架构，事实上往往优于更传统的模型，如回归技术和线性分类器。

复杂的模型通常被称为**黑盒**(例如集合模型、神经网络)，传统上，当试图理解为什么他们会做出选择而不是另一个时，它们很难解释。相比之下，诸如决策树和线性回归之类的模型反而被认为是**白盒**(它们更容易理解预测是如何产生的)。

可解释的人工智能是人工智能的一个新分支，旨在通过向最终用户提供不仅是预测，还有支持证据，来试图“揭开”机器学习模型预测的神秘面纱。

创建可解释的模型会有好处:

*   **用户/客户:为什么我的贷款被拒绝了？为什么有人建议我接受这种治疗？**
*   **监管者/政府:**提供模型公平的证据。
*   **数据科学家/开发人员:**我们的模型实际表现如何？如何改进？

目前，人们使用不同的方法来解释人工智能，一些主要的例子是:

*   **SHAP (Shapley 附加解释):**旨在通过测量每个特征对预测的贡献大小来解释模型预测(Shapley 值的绝对值越大，特征被认为越重要)。
*   **LIME(局部可解释的模型不可知解释):**主要用于处理高度非线性的模型。LIME 旨在通过将原始特征空间划分成不同的线性子部分来线性化非线性空间，从而使得该模型可由局部基础上的线性模型来解释。
*   **基于树的特征重要性:**计算每个输入特征导致的一棵树(或一片树林)的平均杂质减少量。根据这一原则，在创建模型预测时，将结点分割为更靠近三个结点顶部的要素将具有更大的权重。
*   **部分相关图(PDP):** 总结一组输入变量和我们的模型预测之间的关系。这样，可以直观地理解模型预测如何依赖于不同的输入变量。
*   **个体条件期望(ICE)图:**通过模拟输入数据稍加修改会发生什么，帮助我们理解个体观察预测。

为了执行可解释的人工智能任务，一些最常用的 Python 库是:[人工智能可解释性 360 (AIX360)](https://github.com/Trusted-AI/AIX360) 和 [Captum](https://captum.ai/) 。

# 联系人

如果你想了解我最新的文章和项目[，请通过媒体](https://pierpaoloippolito28.medium.com/subscribe)关注我，并订阅我的[邮件列表](http://eepurl.com/gwO-Dr?source=post_page---------------------------)。以下是我的一些联系人详细信息:

*   [领英](https://uk.linkedin.com/in/pier-paolo-ippolito-202917146?source=post_page---------------------------)
*   [个人网站](https://pierpaolo28.github.io/?source=post_page---------------------------)
*   [中等轮廓](https://towardsdatascience.com/@pierpaoloippolito28?source=post_page---------------------------)
*   [GitHub](https://github.com/pierpaolo28?source=post_page---------------------------)
*   [卡格尔](https://www.kaggle.com/pierpaolo28?source=post_page---------------------------)

# 文献学

[1]卫报关于机器学习的观点:人必须决定。社论。访问:[https://www . the guardian . com/commentis free/2016/oct/23/the-guardian-view-on-machine-learning-people-must-decision](https://www.theguardian.com/commentisfree/2016/oct/23/the-guardian-view-on-machine-learning-people-must-decide)

[2]斯坦福 CS224N:具有深度学习的| Winter 2019 |第 19 讲—AI 中的偏见。斯坦福在线。访问:[https://www.youtube.com/watch?v=XR8YSRcuVLE&ab _ channel =斯坦福在线](https://www.youtube.com/watch?v=XR8YSRcuVLE&ab_channel=stanfordonline)

[3]公平且可解释的 AI。玛格瑞特·格罗宁迪克。访问地址:[https://marg riet-groenendijk . git book . io/trusted-ai-workshop/](https://margriet-groenendijk.gitbook.io/trusted-ai-workshop/)