# 如何在数据流上训练深度神经网络

> 原文：<https://towardsdatascience.com/how-to-train-deep-neural-networks-over-data-streams-fdab15704e66?source=collection_archive---------10----------------------->

![](img/785b73b980fcf11c48262199ccd2bf43.png)

[来源](https://unsplash.com/photos/rB7-LCa_diU)

历史上，已经开发了许多机器学习算法来处理和学习传入的数据流。例如，支持向量机和逻辑回归机等模型已被推广到学习者无法获得整个数据集的情况，训练必须在输入的连续数据流上进行[1，2]。类似地，许多聚类算法已经被提出用于数据流的学习[3]。这些方法迫使底层模型从一个连续的数据流中学习，该数据流一次只提供一个示例，从而消除了立即提供整个数据集的需要。有趣的是，尽管已经为更传统的机器学习算法开发了流学习的方法，但是流学习并没有被广泛地用于深度神经网络，其中离线训练(即，在整个数据集上执行若干循环/时期)占主导地位。

为了缩小这一差距，深度学习社区最近的工作探索了通过流式传输训练深度网络的可能性。在深度学习领域中，流可以被描述为一种学习设置，其中 *(i)* 数据集一次学习一个示例(即，数据集被呈现为传入的数据流) *(ii)* 数据集中的每个唯一示例仅出现一次， *(iii)* 流中数据的排序是任意的， *(iv)* 正在训练的模型可以在数据流中的任何点进行评估。与训练神经网络的典型离线方法相比，这种设置可能看起来相当苛刻，这解释了为什么在流设置中实现高性能通常很困难。尽管如此，流式学习反映了工业中的许多应用，并且如果正确使用，为深度学习实践者提供了强大的工具。

在这篇文章中，我将概述深度学习中的流。我将从激励使用流学习开始，重点关注流学习在实际应用中的效用。然后，我将概述在流设置中训练深度神经网络的现有方法，强调在实践中最有用的方法。通过这篇文章，我的目标是 *i)* 说明流学习和深度学习之间的关系，以及 *ii)* 概述在实际应用中利用流学习的力量的有用发现。

# 为什么选择流式学习？

**分流与其他培训模式有何关系？**在提出流学习[4]之前，研究了许多训练设置，这些训练设置探索了不同的策略，这些策略将部分不相交的数据子集顺序暴露给模型，而不是以离线方式对整个数据集进行训练。在这篇文章中，我将把所有这样的方法(包括流学习)统称为“在线”学习设置。通常，在线学习将数据集分成几个不相交的“批”数据，模型一次只暴露一批。一旦学习了一批数据，该模型就不能在以后的训练过程中返回到该批数据(即，只有“当前”数据可以被直接访问以训练该模型)。

已经提出了在线学习的许多不同变体(例如，终身学习、持续学习、批量/班级增量学习、流学习等)。).这些不同的变体中的每一个都涉及上述在线学习的相同概念，通常对实验设置有微小的改变。例如，终身学习倾向于按顺序学习不同的任务(即，每个任务可以被认为是来自包含所有任务的数据集中的数据的子集/批次)，而类增量学习针对每个批次内的总体分类问题学习严格的类子集(例如，CIFAR100 的前 20 个类)。值得注意的是，在线学习过程中的每一批数据都可能非常大。例如，类增量学习实验经常在 ImageNet 上进行，其中每批包含完整数据集的 100 个类子集(即~130K 数据示例)。

流式学习也可以被解释为上述在线学习描述的变体，其中每个“批”数据只是来自数据集的单个示例。然而，流学习似乎比其他相关的方法更偏离通常的在线学习设置。也就是说，流式学习，因为它被限制为一次学习一个实例的数据集，所以不能在每次有新的一批数据可用时执行任意数量的离线训练。这是因为一次只有一个数据示例可供模型使用，而按顺序对同一数据执行多次更新会迅速降低模型性能。因此，*流式学习方法倾向于在新数据可用时执行简短的实时模型更新*，而其他在线学习方法通常执行昂贵的离线培训程序来学习每一批新数据。

**为什么在线学习很难？当一个人第一次遇到在线学习的话题时，他们可能会认为解决这个问题很容易。*为什么不在新数据可用时对模型进行微调呢？这种幼稚的方法在某些情况下是有效的。特别是，如果传入数据流是 [i.i.d.](https://en.wikipedia.org/wiki/Independent_and_identically_distributed_random_variables) ，它将工作，这意味着每一段传入数据都是从所有可能的数据示例中均匀采样的。在这种情况下，从数据流中学习等同于为每个训练迭代从数据集中均匀地采样一个例子(即，这只是随机梯度下降！)，而且幼稚的微调也挺管用的。然而，我们不能总是确保传入的数据是独立身份的。事实上，许多值得注意的实际应用的特征在于非独立身份的数据流(例如，视频流、个性化行为跟踪、对象跟踪等。).***

虽然当输入数据是独立身份数据时，在线学习很容易解决，但当数据是非独立身份数据时，会出现一个有趣的现象-模型从新数据中学习，但很快就会忘记之前学习的所有内容。例如，在类增量学习的情况下，模型可能开始学习如何对马进行分类(即，它以前没有遇到过的某个类)，但是完全忘记了如何对狗、猫、松鼠以及过去已经学会分类的所有其他动物进行分类。这个问题——通常被称为灾难性遗忘[5，6]——是所有在线学习技术面临的基本问题。也就是说，因为数据流通常是非独立的，以在线方式学习的模型通常会遭受灾难性的遗忘，这极大地影响了它们的性能(尤其是在最近没有观察到的数据上)。

**流学习更实用。**现在，我们对流式学习、一般的在线学习以及二者所面临的问题有了更好的理解，有人可能会问这样一个问题:*为什么要特别关注流式学习？*主要原因是流学习 *i)* 实时发生， *ii)* 更好地反映了实践中出现的常见学习范式。

因为学习在流内一次发生一个示例，所以模型更新往往是简短的(即，每个示例一次或几次向前/向后传递)。因此，在新数据示例到达和底层模型适应该数据示例之间存在最小的延迟— *当新数据可用时，模型会实时更新*。相比之下，当 *i)* 等待足够大批量的新数据积累，或者 *ii)* 在新一批数据可用之后更新模型时，其他通常研究的在线学习设置可能会遭受延迟——必须执行许多向前/向后传递，以更新大批量数据的模型，特别是如果对数据执行了几次循环。虽然从研究的角度来看，这种在线学习的替代实验设置很有趣，但是当他们有能力在每个新样本到来后更新模型时，为什么任何从业者都要等待数据积累呢？

流学习使模型实时适应数据流的能力在工业中也有广泛的应用。例如，考虑一个推荐系统，该系统在用户每次与网站交互时(例如，购买、点击或者甚至鼠标的移动)执行动态更新。或者，可以利用深度网络来执行视频插值(例如，在给定前几帧的情况下，预测人在下一帧中的位置)，并利用流学习来基于其在每一帧中所犯的错误在视频流上更新该模型。流式学习的可能性几乎是无限的，因为它适用于深度网络应该立即从输入数据中学习的任何情况。因此，它(在我看来)是一个值得深度学习从业者关注的话题。

# 深度流学习的方法

现在，流学习已经被定义和激励，是时候学习如何在数据流上实际训练神经网络，而不会严重降低它们的性能。最近，在深度学习社区中提出了几种以流方式训练深度神经网络的算法[4，7，8]。对于每个算法，我将概述该方法的主要组件和细节，并强调在实践中实现该算法的主要实际考虑。在这一节中，我将重点介绍每种方法的主要细节，这些细节可以让实践者更好地理解哪种方法最适合给定的应用程序。

## ExStream [4]

ExStream 于 2019 年 2 月提出，是一种基于重放的流式学习方法。这里，“重放”(也称为预演)用于描述将来自传入数据流的先前遇到的数据存储在缓冲区中的方法。然后，当新数据变得可用时，基于重放的方法将新数据与来自重放缓冲区的数据样本混合，并使用这种新旧数据的混合来更新模型，从而确保先前的知识得以保留。简而言之，这些方法用新旧数据的混合来训练网络，以确保网络暴露于来自数据流的一组平衡的示例。重播是在线学习中广泛使用的一种方法，既简单又有效，但它需要存储以前的数据，这会产生不可忽略的内存占用。

虽然 ExStream 证明了“完全”重放(即，将所有传入数据存储在重放缓冲区中，并在每次遇到新数据时循环通过重放缓冲区中的所有示例)消除了灾难性遗忘，但是还探索了不需要存储整个数据流的更节省存储器的重放机制。使用 ResNet50 [9]来执行图像分类，ExStream 预训练并固定底层模型的卷积层(即，在流式传输过程中不会更新任何卷积层参数)，并专注于以流式传输方式学习最终的全连接层。因此，存储在重放缓冲器中的所有例子都是简单的向量(即，ResNet 的特征提取器/主干的输出)。使用这些特征向量作为输入，ExStream 为每类数据维护一个单独的固定大小的重放缓冲区(即，每类只能存储`c`个向量)，并旨在发现一种算法，该算法使 *i)* 最大限度地减少必须存储以供重放的向量数量(即，这限制了内存开销) *ii)* 保持最先进的分类性能(即，可与完全重放相比)。

为了实现这些目标，ExStream 利用以下规则集在流式传输期间维护其重放缓冲区:

*   保持`c`簇质心(即，只是向量！)每个类，每个类都有一个与之相关联的“计数”。
*   在数据流中遇到一个类的`c`示例之前，只需将每个向量添加到重放缓冲区，计数为 1。
*   一旦给定类的缓冲区已满，并且该类的新示例到达，找到两个最近的聚类质心(基于欧几里德距离)，并通过基于它们各自的计数对向量进行加权平均来将它们合并在一起。将结果质心的计数设置为前两个质心的计数之和。
*   一旦两个质心合并(从而为新的质心腾出空间)，将新的数据示例添加到缓冲区中，计数为 1。

使用如上所述维护的重放缓冲区，ExStream 然后通过采样和混合来自不同类的聚类质心与传入数据来执行重放，以对模型参数执行更新。与用于在重放缓冲区内维护聚类质心的几种其他算法(例如，在线 k-means、CluStream [10]、HPStream [11]等)相比，)，ExStream 表现出最佳性能。此外，通过调整重放缓冲区内每个类所允许的质心数量，可以很容易地调整算法的内存占用量(尽管重放缓冲区太小可能会导致较差的性能)。ExStream 在 Core50 和 iCUB 数据集上表现良好，但在后来的工作发表之前，没有应用于大规模分类问题(如 ImageNet)。

## 深沉的 SLDA [7]

深度流线性判别分析(SLDA)于 2020 年 4 月提出，是深度流学习的另一种方法，它脱离了基于重放的方法(即，它不保持任何重放缓冲)。因此，与需要重放缓冲区的 ExStream 等方法相比，它的内存效率非常高，从而(潜在地)使其适合内存受限的学习场景(例如，设备上学习)。SLDA 是一个已经建立的算法[12],已经用于数据挖掘社区中的数据流分类。在深度 SLDA 中，通过使用固定的 ResNet18 [9]主干的 *i)* 将 SLDA 算法与深度神经网络相结合，以获得每个数据示例的特征向量，并通过使用 SLDA 的【T2 ii)以流的方式对这些特征向量进行增量分类。同样，除了最终分类模块(即，SLDA 组件)，所有网络层在深度 SLDA 的流式传输过程中是固定的。

用 SLDA 以递增的方式对特征向量进行分类的细节超出了本文的范围——这种算法很复杂，可能需要一篇完整的博文来真正理解它。然而，在高层次上，SLDA 通过 *i)* 用相关计数维护每个类的单个平均向量，以及 *ii)* 构建表征类表示之间关系的共享协方差矩阵来操作。当新数据变得可用时，为每个类更新均值向量，而协方差矩阵可以保持固定(在训练数据子集上的一些基础初始化之后)或者在流式传输过程中递增地更新。在测试时，可以使用平均类向量与协方差矩阵的逆矩阵的封闭形式的矩阵乘法来推断新数据示例的类输出。

与 ExStream 等基于重放的方法相比，SLDA 具有优势，因为它显著降低了内存需求，只需存储每个类的一个向量和一个共享协方差矩阵。此外，即使在 ImageNet 这样的大规模数据集上，SLDA 也能产生令人印象深刻的分类性能，优于 ExStream [4]、iCarl [13]和端到端增量学习[14]等流行方法；参见[8]中的表 1。此外，与大规模数据集上的正常离线神经网络训练相比，深层 SLDA 的挂钟训练时间几乎可以忽略不计。总的来说，*该方法在规模上令人惊讶地有效，因为它的计算和内存需求最小*。

## 提醒[8]

REMIND 于 2020 年 7 月发布，是最近提出的一种基于重放的深度流学习方法。REMIND 不像 ExStream 那样在重放缓冲区中维护簇质心，而是为传入流中遇到的每个数据示例存储单独的缓冲区条目。在以前的工作中，这是通过在重放缓冲器[13，14]中存储原始图像来完成的。然而，在 REMIND 中，作者提出，神经网络中的中间激活(即，这些激活不仅仅是一个向量——它们可能具有空间维度)应该存储在重放缓冲区中，而不是原始图像中，这 *i)* 极大地减少了每样本的内存需求，而 *ii)* 模拟了海马索引理论概述的大脑中压缩记忆的重放。

为了使上述策略成为可能，REMIND 采用了 ResNet18 [9]架构，并冻结了网络的初始层，使得这些层的参数在流式传输过程中不会改变。类似于深度 SLDA，这些冻结参数的值是使用某个基础初始化阶段在训练数据的子集上设置的。然后，对于在流式传输期间遇到的每个示例，REMIND *i)* 通过网络的冻结层传递示例以提取中间激活， *ii)* 使用乘积量化(PQ)策略压缩激活张量[15]，并且 *iii)* 将量化的向量存储在重放缓冲器中。然后，使用新数据(在它已经被量化之后)和来自重放缓冲器的采样激活的组合作为输入，在最终网络层(即，那些没有被冻结的网络层)上执行在线更新。在实践中，REMIND 对从重放缓冲区采样的激活执行随机裁剪和混合，这提供了正则化的好处，并产生适度的性能改进。

由于 REMIND 的内存高效的重放方法，它可以用有限的开销维护非常大的重放缓冲区。例如，在 ImageNet 数据集上，REMIND 可以维护大约 1M 样本的缓冲区，其内存占用与 10K 原始图像的重放缓冲区相同。因此，REMIND 在许多在线学习的常见基准测试中明显优于 ExStream 和 Deep SLDA；参见[8]中的表 1。在大规模数据集(例如 Imagenet)上，REMIND 的性能优势尤其明显，在这种情况下，REMIND 能够在有限的内存中存储许多重放示例，这使得它真正脱颖而出。目前，REMIND 是在流领域训练深度网络的最佳方法。

## 与一般在线学习的联系

现在已经概述了深度流学习的现有方法，人们可能会开始问这些方法如何与为其他在线学习设置(例如，批量增量学习或终身学习)提出的方法相关联。为了更全面地描述在线学习的所有方法，我推荐[我之前关于这个话题的文章](/a-broad-and-practical-exposition-of-online-learning-techniques-a4cbc300dcd4)。然而，我试图在下面的高层次上概述这些方法。

*   **Replay:** 广泛用于流学习和在线学习。
*   **知识蒸馏:**广泛应用于在线学习技术中，但还没有探索用于流学习。虽然知识提炼可以为流式学习技术提供一些好处，但最近的几篇论文认为，当与重放结合时，知识提炼提供的好处很少[16，17]。
*   **偏差修正:**未在流设置内测试，但对增量学习非常有益。

在线学习的若干其他方法还没有在流设置中探索过(例如，架构修改或基于正则化的方法)。然而，这些方法中的许多在当前的在线学习研究中不太受欢迎，因为它们在应用于大规模问题时表现不佳。因此，这些方法不太可能胜过大规模流式学习的成熟方法，如 REMIND。

## 实践中用什么最好？

尽管这篇文章中概述的深度流学习的所有方法都是有用的，但从业者可能想知道哪种方法最适合他们的应用。在性能方面，REMIND 是迄今为止为深度流学习提出的性能最好的方法。这可以从 REMIND [8]论文的表 1 中看出，REMIND 的表现明显优于 ExStream 和 Deep SLDA。此外，REMIND 甚至已经扩展到问题领域，如对象检测[16]，从而展示了它在图像分类以外的应用中的效用。

ExStream 和 REMIND 都需要存储一个重放缓冲区，并且具有相似的计算效率，因此 REMIND 是两者之间显而易见的选择。然而，深度 SLDA 不需要维护这样的重放缓冲器，并且可以非常快速地被训练。因此，即使 REMIND 实现了更好的性能，深度 SLDA 在内存或计算资源有限的情况下可能是有利的。否则，REMIND 在实践中是深度流学习的最佳选择，因为它可以实现令人印象深刻的性能，并通过量化最小化重放缓冲区的内存占用。

对于那些感兴趣的人来说，REMIND [17]和 Deep SLDA [18]的实现都可以通过 github 公开获得。

# 结论

在这篇文章中，我概述了数据流上深度神经网络的训练，包括讨论为什么这样的训练范式实际上是相关的，并描述了以这种方式训练深度网络的现有方法。在深度流学习的相关方法中，REMIND 实现了最佳性能，而深度 SLDA 等方法可能在内存或计算资源有限的情况下有用。

非常感谢你阅读这篇文章。如果你有任何反馈或总体上喜欢这篇文章，并希望了解我未来的工作，请随时在 [twitter](https://twitter.com/cwolferesearch) 上关注我或访问[我的网站](https://wolfecameron.github.io/)。这项工作是我在 [Alegion](https://www.alegion.com/) 做研究科学家和在莱斯大学做博士生工作的一部分。如果你喜欢这篇文章中的内容，我鼓励你去看看 Alegion 的[空缺职位](https://www.alegion.com/company/careers)，或者联系[我的研究实验室](http://akyrillidis.github.io/group/)！

***引文***

[1][https://arxiv.org/abs/1412.2485](https://arxiv.org/abs/1412.2485)

[2][https://ieeexplore.ieee.org/abstract/document/8622392](https://ieeexplore.ieee.org/abstract/document/8622392)

[3]https://epubs.siam.org/doi/abs/10.1137/1.9781611974317.7

[4]https://arxiv.org/abs/1809.05922

[5][https://www . science direct . com/science/article/ABS/pii/s 0079742108605368](https://www.sciencedirect.com/science/article/abs/pii/S0079742108605368)

[https://arxiv.org/abs/1708.02072](https://arxiv.org/abs/1708.02072)

[https://arxiv.org/abs/1909.01520](https://arxiv.org/abs/1909.01520)

[https://arxiv.org/abs/1910.02509](https://arxiv.org/abs/1910.02509)

[https://arxiv.org/abs/1512.03385](https://arxiv.org/abs/1512.03385)

[https://www.vldb.org/conf/2003/papers/S04P02.pdf](https://www.vldb.org/conf/2003/papers/S04P02.pdf)

[11][https://www . semantic scholar . org/paper/A-Framework-for-Projected-Clustering-of-High-Data-Aggarwal-Han/317 c 0 F3 A 829 ee 8 f 0 B3 d 7 E5 A 915 A 93 c 52 baa 7 a9 e 8](https://www.semanticscholar.org/paper/A-Framework-for-Projected-Clustering-of-High-Data-Aggarwal-Han/317c0f3a829ee8f0b3d7e5a915a93c52baa7a9e8)

[https://ieeexplore.ieee.org/document/1510767](https://ieeexplore.ieee.org/document/1510767)

[https://arxiv.org/abs/1611.07725](https://arxiv.org/abs/1611.07725)

[https://arxiv.org/abs/1807.09536](https://arxiv.org/abs/1807.09536)

[15][https://lear . inrialpes . fr/pubs/2011/JD S11/jegou _ searching _ with _ quantization . pdf](https://lear.inrialpes.fr/pubs/2011/JDS11/jegou_searching_with_quantization.pdf)

[https://arxiv.org/abs/2008.06439](https://arxiv.org/abs/2008.06439)

[https://github.com/tyler-hayes/REMIND](https://github.com/tyler-hayes/REMIND)

[https://github.com/tyler-hayes/Deep_SLDA](https://github.com/tyler-hayes/Deep_SLDA)