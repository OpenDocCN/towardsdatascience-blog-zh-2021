# é‡‡ç”¨å¼ é‡æµæ—¶é—´åˆ†å¸ƒåŒ…è£…å™¨çš„å˜é•¿è¾“å…¥ç¥ç»ç½‘ç»œ

> åŸæ–‡ï¼š<https://towardsdatascience.com/neural-network-for-input-of-variable-length-using-tensorflow-timedistributed-wrapper-a45972f4da51?source=collection_archive---------5----------------------->

## å¦‚ä½•ä½¿ç”¨ Tensorflow æ—¶é—´åˆ†å¸ƒåŒ…è£…å™¨å¤„ç†å¯å˜é•¿åº¦è¾“å…¥(é€šå¸¸æ˜¯ä¿¡å·)çš„æŒ‡å—

# ç›®å½•

1.  ä¸ºä»€ä¹ˆè¾“å…¥é•¿åº¦å¯å˜ï¼Ÿ
2.  Tensorflow æ—¶é—´åˆ†å¸ƒå¼åŒ…è£…å™¨
3.  æ•°æ®ç”Ÿæˆç¨‹åº
4.  å‚è€ƒ

# ä¸ºä»€ä¹ˆè¾“å…¥é•¿åº¦å¯å˜ï¼Ÿ

ä½ æ˜¯å¦æ›¾ç»æƒ³è¦å°†ç¥ç»ç½‘ç»œåº”ç”¨äºä½ çš„æ•°æ®é›†ï¼Œä½†æ˜¯æ•°æ®(ä¿¡å·ã€æ—¶é—´åºåˆ—ã€æ–‡æœ¬ç­‰ã€‚)æœ‰äº†ä¸€ä¸ª**å¯å˜é•¿åº¦**ï¼Ÿä¸å¹¸çš„æ˜¯ï¼Œè¿™ç§æƒ…å†µå¯¹äºæ•°æ®ç§‘å­¦å®¶æ¥è¯´å¾ˆå¸¸è§ã€‚

> æ­£å¦‚æˆ‘ä»¬æ‰€çŸ¥ï¼Œåœ¨ç°å®ä¸–ç•Œä¸­ï¼Œæ•°æ®æ°¸è¿œä¸ä¼šåƒæˆ‘ä»¬å¸Œæœ›çš„é‚£æ ·æ¼‚äº®å’Œæœ‰æ¡ç†ã€‚

æœ‰å„ç§å„æ ·çš„æ–¹æ³•æ¥è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œä½†æ˜¯æ²¡æœ‰ä¸€ä¸ªè®©æˆ‘æ»¡æ„ã€‚

æœ€æ™®éé‡‡ç”¨çš„è§£å†³æ–¹æ¡ˆæ˜¯**å°†æ‰€æœ‰è¾“å…¥æˆªæ–­**åˆ°ç›¸åŒé•¿åº¦ï¼Œè¿™é€šå¸¸ä¸è¾ƒçŸ­é•¿åº¦çš„è¾“å…¥ç›¸ä¸€è‡´ã€‚ç„¶è€Œï¼Œè¿™é€ æˆäº†å·¨å¤§çš„æ•°æ®æŸå¤±ï¼Œæ­£å¦‚æˆ‘ä»¬æ‰€çŸ¥ï¼Œæ•°æ®å¯¹æˆ‘ä»¬æ¥è¯´æ˜¯é‡‘å­ã€‚

ä¸€ä¸ªå¯èƒ½çš„æ›¿ä»£æ–¹æ¡ˆæ˜¯å®ƒçš„åé¢ï¼Œå³ [**å¡«å……**](https://www.tensorflow.org/guide/keras/masking_and_padding) (æ·»åŠ æ•°æ®ç›´åˆ°æ‰€æœ‰ä¿¡å·é•¿åº¦ç›¸åŒ)ã€‚å¡«å……çš„é—®é¢˜æ˜¯ï¼Œå®ƒæ·»åŠ äº†æ²¡æœ‰å®é™…æ„ä¹‰çš„æ•°æ®ï¼Œè€Œä¸”è¾“å…¥éå¸¸é•¿ï¼Œç½‘ç»œçš„è§„æ¨¡å˜å¾—ä¸å¯æŒç»­ã€‚å½“ç„¶ï¼Œå¡«å……å¯ä»¥é€šè¿‡ [**å¢å¼º**](https://www.tensorflow.org/tutorials/images/data_augmentation) æ¥å®Œæˆã€‚ç„¶è€Œï¼Œç‰¹åˆ«æ˜¯å¯¹äºæ•°æ®é¡ºåºéå¸¸é‡è¦çš„ä¿¡å·ï¼Œåº”ç”¨å¢å¼ºä¼šâ€œæ±¡æŸ“â€è¿™äº›ä¿¡æ¯ã€‚

> æˆ‘æ„Ÿåˆ°å¾ˆå¤±è½ï¼Œä½†å½“æˆ‘çœ‹åˆ°è¿™ä¸ªåŒ…è£…çº¸æ—¶ï¼Œæˆ‘çŸ¥é“å®ƒå°±æ˜¯æˆ‘è¦çš„ã€‚

# Tensorflow æ—¶é—´åˆ†å¸ƒå¼åŒ…è£…å™¨

The[**time distributed**](https://www.tensorflow.org/api_docs/python/tf/keras/layers/TimeDistributed)wrapper å…è®¸å¯¹è¾“å…¥çš„æ¯ä¸ªæ—¶é—´ç‰‡åº”ç”¨ä¸€ä¸ªå±‚ã€‚

å‡è®¾ä½œä¸ºè¾“å…¥ï¼Œæˆ‘ä»¬æœ‰ä¸€ä¸ªç”±ä»¥ 100 Hz(æ¯ç§’ 100 ç‚¹)é‡‡æ ·çš„**ä¿¡å·**ç»„æˆçš„æ•°æ®é›†ã€‚æˆ‘ä»¬çš„ç›®æ ‡æ˜¯å¯¹æ¯ä¸ª 30 ç§’çš„ç‰‡æ®µè¿›è¡Œåˆ†ç±»(ç§°ä¸º**çºªå…ƒ**)ã€‚

å› æ­¤ï¼Œæˆ‘ä»¬å¸Œæœ›å»ºç«‹ä¸€ä¸ªæ·±åº¦ç¥ç»ç½‘ç»œï¼Œé€’å½’åœ°åº”ç”¨äºè¿™äº›ç‰‡æ®µä¸­çš„æ¯ä¸€ä¸ªã€‚ä¸ºäº†é˜æ˜ç½‘ç»œçš„å·¥ä½œåŸç†ï¼Œä¸‹é¢æ˜¯ä¸€ä¸ªç®€åŒ–çš„ç¤ºæ„å›¾:

![](img/9c3cfb2ee0261a493e74bd5d72945413.png)

ç½‘ç»œå¦‚ä½•è¿ä½œçš„æ¨¡å¼[å›¾ç‰‡ç”±ä½œè€…æä¾›]

è®©æˆ‘ä»¬ä»å¯¼å…¥æ‰€æœ‰å¿…éœ€çš„å…ƒç´ å¼€å§‹:

```
from tensorflow.keras.layers import Conv2D, TimeDistributed,Dropout,Input, Dense,\
    BatchNormalization, GRU, Layer, Flatten
from tensorflow.keras.regularizers import l2
from tensorflow.keras.models import Model
from tensorflow.keras.utils import plot_model
from tensorflow.keras.optimizers import Adam
```

ç°åœ¨æˆ‘ä»¬å¯ä»¥å»ºç«‹æˆ‘ä»¬çš„ç½‘ç»œã€‚æˆ‘ä»¬å°†ä½¿ç”¨[**ã€CNNã€‘**](https://www.tensorflow.org/tutorials/images/cnn)å·ç§¯å—ä»åŸå§‹ä¿¡å·ä¸­æå–ç‰¹å¾ï¼Œéšåä½¿ç”¨**[**ã€GRUã€‘**](https://www.tensorflow.org/api_docs/python/tf/keras/layers/GRU)ç»„åˆæå–çš„ç‰¹å¾ã€‚æ‰€ä»¥è®©æˆ‘ä»¬å†™æˆ‘ä»¬çš„å‡½æ•°:**

```
def nn(shape_1,shape_2):
    input = Input(shape=[None, shape_1,shape_2,1])

    conv1 = TimeDistributed(Conv2D(filters=32, kernel_size=[32,1], activation='relu',strides =(3,1)))(input)
    batch1 = TimeDistributed(BatchNormalization())(conv1)

    conv2 = TimeDistributed(Conv2D(filters=32, kernel_size=[32,1], activation='relu',strides =(2,1)))(batch1)
    batch2 = TimeDistributed(BatchNormalization())(conv2)

    conv3 = TimeDistributed(Conv2D(filters=32, kernel_size=[32,1], activation='relu',strides =(2,1)))(batch2)
    batch3 = TimeDistributed(BatchNormalization())(conv3)

    conv4 = TimeDistributed(Conv2D(filters=32, kernel_size=[32,1], activation='relu',strides =(2,1)))(batch3)
    batch4 = TimeDistributed(BatchNormalization())(conv4)

    flat = TimeDistributed(Flatten())(batch4)

    gru1 = GRU(256, activation='relu',return_sequences=True, kernel_regularizer=l2(0.01))(flat)
    drop1 = Dropout(rate=0.4)(gru1)
    batch1 = BatchNormalization()(drop1)

    gru2 = GRU(128, activation='relu',return_sequences=True, kernel_regularizer=l2(0.01))(batch1)
    drop2 = Dropout(rate=0.4)(gru2)
    batch2 = BatchNormalization()(drop2)

    dense = TimeDistributed(Dense(2, activation='softmax'),name = 'output')(batch2)

    return [input], [dense]
```

**æ­£å¦‚æˆ‘ä»¬æ‰€è§ï¼Œç½‘ç»œç”±**å››ä¸ªå·ç§¯å±‚**å’Œ**ä¸¤ä¸ªå·ç§¯å±‚**ç»„æˆã€‚ç½‘ç»œä¸­è¿˜æœ‰å…¶ä»–å…ƒç´ ï¼Œå¦‚ [**æ‰¹é‡å½’ä¸€åŒ–å±‚**](https://www.tensorflow.org/api_docs/python/tf/keras/layers/BatchNormalization) ï¼Œç”±äºæ—¶é—´åŸå› æˆ‘ä»¬ä¸å†èµ˜è¿°ã€‚æœ€åï¼Œä¸€ä¸ª[**å¯†é›†å±‚**](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense) å…è®¸åˆ†ç±»ã€‚å¦‚æœæ‚¨æƒ³è¦å¯¹æ•´ä¸ªä¿¡å·è€Œä¸æ˜¯æ¯ä¸ªæ—¶æœŸè¿›è¡Œåˆ†ç±»ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ä»¥ä¸‹å†…å®¹ä½œä¸ºæœ€åä¸€å±‚:**

```
dense = Dense(2, activation='sigmoid',name = 'status_output')(batch2)
```

**éœ€è¦æ³¨æ„çš„é‡è¦ä¸€ç‚¹æ˜¯ï¼ŒåŒ…è£…å™¨**ä¸åº”è¯¥åº”ç”¨äºæ—¶é—´å±‚**ï¼Œæ¯”å¦‚ GRU æˆ– LSTMã€‚é»˜è®¤æƒ…å†µä¸‹ï¼Œè¿™ç§ç±»å‹çš„å±‚å·²ç»å¯ä»¥å¤„ç†å¯å˜é•¿åº¦ã€‚**

**ä¸€æ—¦å‡½æ•°å‡†å¤‡å°±ç»ªï¼Œè®©æˆ‘ä»¬æ„å»ºæ¨¡å‹å¹¶è¯¦ç»†æŸ¥çœ‹å®ƒ:**

```
EPOCH_LENGTH = 30
SAMPLE_RATE = 100

input, output = nn(SAMPLE_RATE*EPOCH_LENGTH,1)
model = Model(inputs=input,outputs=output)

optimizer = Adam(learning_rate=2*1e-4)

# Compile Model
model.compile(optimizer=optimizer, loss={
                  'output': 'sparse_categorical_crossentropy', },
              metrics={
                  'output': 'sparse_categorical_accuracy', },
              sample_weight_mode='temporal')
model.summary()
```

*****è¾“å‡º:*****

```
Layer (type)                 Output Shape              Param #   
=================================================================
input_13 (InputLayer)        [(None, None, 3000, 1, 1) 0         
_________________________________________________________________
time_distributed_58 (TimeDis (None, None, 990, 1, 32)  1056      
_________________________________________________________________
time_distributed_59 (TimeDis (None, None, 990, 1, 32)  128       
_________________________________________________________________
time_distributed_60 (TimeDis (None, None, 480, 1, 32)  32800     
_________________________________________________________________
time_distributed_61 (TimeDis (None, None, 480, 1, 32)  128       
_________________________________________________________________
time_distributed_62 (TimeDis (None, None, 225, 1, 32)  32800     
_________________________________________________________________
time_distributed_63 (TimeDis (None, None, 225, 1, 32)  128       
_________________________________________________________________
time_distributed_64 (TimeDis (None, None, 97, 1, 32)   32800     
_________________________________________________________________
time_distributed_65 (TimeDis (None, None, 97, 1, 32)   128       
_________________________________________________________________
time_distributed_66 (TimeDis (None, None, 3104)        0         
_________________________________________________________________
gru_22 (GRU)                 (None, None, 256)         2582016   
_________________________________________________________________
dropout_20 (Dropout)         (None, None, 256)         0         
_________________________________________________________________
batch_normalization_48 (Batc (None, None, 256)         1024      
_________________________________________________________________
gru_23 (GRU)                 (None, None, 128)         148224    
_________________________________________________________________
dropout_21 (Dropout)         (None, None, 128)         0         
_________________________________________________________________
batch_normalization_49 (Batc (None, None, 128)         512       
_________________________________________________________________
output (TimeDistributed)     (None, None, 2)           258       
=================================================================
Total params: 2,832,002
Trainable params: 2,830,978
Non-trainable params: 1,024
_________________________________________________________________
```

**æ­£å¦‚æˆ‘ä»¬æ‰€çœ‹åˆ°çš„ï¼Œç½‘ç»œæœ‰ 280 ä¸‡ä¸ªå¯è®­ç»ƒå‚æ•°ï¼Œæ€»çš„æ¥è¯´è¿™æ˜¯ä¸€ä¸ªå¾ˆå°çš„æ•°ç›®ã€‚è®©æˆ‘ä»¬å½¢è±¡åœ°æè¿°ä¸€ä¸‹:**

```
model._layers = [
    layer for layer in model._layers if isinstance(layer, Layer)
]

plot_model(model, 'model.png', show_shapes=True)
```

**![](img/f2fc229c62c387ad6c748f6a48c71adf.png)**

**æ¨¡å‹å½¢çŠ¶[å›¾ç‰‡æ¥è‡ªä½œè€…]**

**ä»ç½‘ç»œçš„å½¢çŠ¶ä¸­ï¼Œæˆ‘ä»¬æ³¨æ„åˆ° **TimeDistributed åœ¨é»˜è®¤çš„åŸºç¡€ä¸Šå¢åŠ äº†ä¸€ä¸ªç»´åº¦**(å³ç¬¬äºŒä¸ªé—®å·)ï¼Œå®ƒå¯¹åº”äºæ¯ä¸ªä¿¡å·çš„ä¸åŒæ—¶æœŸæ•°ã€‚**

**æ­¤å¤–ï¼ŒGRU(æˆ– [**LSTM**](https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTM) )å…è®¸æˆ‘ä»¬åˆ©ç”¨æ—¶é—´ä¿¡æ¯ï¼Œè¿™åœ¨ä¿¡å·ä¸­æ˜¯å¿…ä¸å¯å°‘çš„ã€‚**

**ä¸å¹¸çš„æ˜¯ï¼Œè¿™ä¸ªç½‘ç»œä¸èƒ½è‡ªæˆ‘è®­ç»ƒï¼Œç‰¹åˆ«æ˜¯åœ¨æˆ‘ä»¬çš„ä¾‹å­ä¸­ï¼Œæˆ‘ä»¬æœ‰å„ç§ç”± IDs æ ‡è¯†çš„æ–‡ä»¶ã€‚ç„¶åï¼Œæˆ‘ä»¬æ„å»ºä¸€ä¸ªç”Ÿæˆå™¨æ¥ç®¡ç†è¾“å…¥ã€‚**

# **æ•°æ®ç”Ÿæˆç¨‹åº**

**åœ¨æˆ‘ä»¬çš„ä¾‹å­ä¸­ï¼Œæˆ‘ä»¬æœ‰ä¸€ä¸ªæ–‡ä»¶åˆ—è¡¨ï¼Œæ¯ä¸ªæ–‡ä»¶ä»£è¡¨ä¸€ä¸ªè¾“å…¥ä¿¡å·ã€‚å› æ­¤ï¼Œæˆ‘ä»¬éœ€è¦ä¸º Tensorflow æ„å»ºä¸€ä¸ªç”Ÿæˆå™¨ï¼Œåœ¨è¿è¡Œæ—¶æ¥æ”¶ä¿¡å·å¹¶ä¸ºç½‘ç»œåšå¥½å‡†å¤‡ã€‚**

**ç°åœ¨è®©æˆ‘ä»¬æ„å»ºä¸€ä¸ªç±»ï¼Œå®ƒå°†ç±»å‹ä¸º [**çš„å¯¹è±¡åºåˆ—**](https://www.tensorflow.org/api_docs/python/tf/keras/utils/Sequence) ä½œä¸ºè¾“å…¥ï¼Œå¹¶å®ç°è®­ç»ƒç½‘ç»œæ‰€éœ€çš„æ‰€æœ‰æ–¹æ³•( *__init__ï¼Œ__len__ï¼Œ__getitem__ï¼Œon_epoch_end* ):**

```
import numpy as np
from keras.utils import Sequence
from keras.preprocessing.sequence import pad_sequences

class DataGenerator(Sequence):
    *"""Generates data for Keras
    Sequence based data generator. Suitable for building data generator for training and prediction.
    """* def __init__(self, list_IDs, input_path, target_path,
                 to_fit=True, batch_size=32, shuffle=True):
        *"""Initialization* ***:param*** *list_IDs: list of all 'label' ids to use in the generator* ***:param*** *to_fit: True to return X and y, False to return X only* ***:param*** *batch_size: batch size at each iteration* ***:param*** *shuffle: True to shuffle label indexes after every epoch
        """* self.input_path = input_path
        self.target_path = target_path
        self.list_IDs = list_IDs
        self.to_fit = to_fit
        self.batch_size = batch_size
        self.shuffle = shuffle
        self.on_epoch_end()

    def __len__(self):
        *"""Denotes the number of batches per epoch* ***:return****: number of batches per epoch
        """* return int(np.floor(len(self.list_IDs) / self.batch_size))

    def __getitem__(self, index):
        *"""Generate one batch of data* ***:param*** *index: index of the batch* ***:return****: X and y when fitting. X only when predicting
        """* # Generate indexes of the batch
        indexes = self.indexes[index * self.batch_size:(index + 1) * self.batch_size]

        # Find list of IDs
        list_IDs_temp = [self.list_IDs[k] for k in indexes]

        # Generate data
        X = self._generate_X(list_IDs_temp)

        if self.to_fit:
            y = self._generate_y(list_IDs_temp)
            return [X], y
        else:
            return [X]

    def on_epoch_end(self):
        *"""
        Updates indexes after each epoch
        """* self.indexes = np.arange(len(self.list_IDs))
        if self.shuffle:
            np.random.shuffle(self.indexes)

    def _generate_X(self, list_IDs_temp):
        *"""Generates data containing batch_size images* ***:param*** *list_IDs_temp: list of label ids to load* ***:return****: batch of images
        """* # Initialization
        X = []

        # Generate data
        for i, ID in enumerate(list_IDs_temp):
            # Store sample
            temp = self._load_input(self.input_path, ID)
            X.append(temp)

        X = pad_sequences(X, value=0, padding='post')

        return X

    def _generate_y(self, list_IDs_temp):
        *"""Generates data containing batch_size masks* ***:param*** *list_IDs_temp: list of label ids to load* ***:return****: batch if masks
        """* y = []

        # Generate data
        for i, ID in enumerate(list_IDs_temp):
            # Store sample
            y.append(self._load_target(self.target_path, ID))

        y = pad_sequences(y, value=0, padding='post')

        return y
```

**ä¸€æ—¦ç”Ÿæˆå™¨ç±»è¢«ç¼–å†™ï¼Œè®©æˆ‘ä»¬æå–æ–‡ä»¶çš„ id åˆ—è¡¨ï¼Œå°†å®ƒä»¬åˆ†æˆ**è®­ç»ƒ**ã€**éªŒè¯**å’Œ**æµ‹è¯•**ã€‚æ³¨æ„ï¼Œæˆ‘ä»¬æ„å»ºäº†ä¸¤ä¸ªç”Ÿæˆå™¨ï¼Œä¸€ä¸ªç”¨äºè®­ç»ƒï¼Œä¸€ä¸ªç”¨äºéªŒè¯:**

```
import numpy as np
import re
from os import listdir
from os.path import isfile, joinTEST_SIZE = 128
onlyfiles = [f for f in listdir(input_path) if isfile(join(input_path, f))]

id = [re.search('(.+?).npz', x).group(1) for x in onlyfiles]
id.sort()

np.random.seed(1234)
id_test = np.random.choice(id, size=TEST_SIZE,replace=False)
id = list(set(id) - set(id_test))
id_validation = np.random.choice(id, size=TEST_SIZE,replace=False)
id = list(set(id) - set(id_validation))

print(len(id))

training_generator = DataGenerator(id,  input_path = input_path,
                                   target_path=target_path)

validation_generator = DataGenerator(id_validation, input_path = input_path,
                                   target_path=target_path)
```

**æˆ‘ä»¬ç°åœ¨ç»ˆäºå¯ä»¥è¿›è¡Œæ¨¡å‹è®­ç»ƒäº†ï¼Œç¥ˆç¥·å¥½è¿å§ğŸ˜†ã€‚**

```
model.fit(training_generator,
                    validation_data=validation_generator,
                    epochs=8,
                    use_multiprocessing=True)
```

**å¯¹äºæœ¬æ–‡ï¼Œæˆ‘ä»¬å·²ç»å®Œæˆäº†ï¼Œæˆ‘åœ¨å‚è€ƒèµ„æ–™ä¸­ä¸ºæ‚¨ç•™ä¸‹äº†åŒ…å«ä»£ç çš„ repo çš„é“¾æ¥ã€‚**

**ä¸€ä¼šå„¿è§ï¼Œ**

**å¼—æœ—è¥¿æ–¯ç§‘**

# **å‚è€ƒ**

1.  **[æ—¶é—´åˆ†å¸ƒå¼å¼ é‡æµåŒ…è£…å™¨](https://www.tensorflow.org/api_docs/python/tf/keras/layers/TimeDistributed)**
2.  **å¸¦æœ‰ä»£ç çš„ Github [åº“](https://github.com/dallanoce/timedistributed)**