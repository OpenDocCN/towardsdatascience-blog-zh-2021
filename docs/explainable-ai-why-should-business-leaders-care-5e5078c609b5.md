# 可解释的人工智能:为什么企业领导人应该关心？

> 原文：<https://towardsdatascience.com/explainable-ai-why-should-business-leaders-care-5e5078c609b5?source=collection_archive---------17----------------------->

## 可解释的机器学习模型如何为企业提供战略利益。

![](img/f5f5e7886a3d0cf9056fd383668ce4a9.png)

Gert RDA valasevi it 在 [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral) 上拍摄的照片

**人工智能与模型可解释性的挑战**

人工智能(AI)已经变得越来越普遍，正在所有行业中被广泛采用。面对越来越大的竞争压力和观察同行的人工智能成功故事，越来越多的组织正在他们业务的各个方面采用人工智能。机器学习(ML)模型是驱动人工智能系统的关键组件，正变得越来越强大，在大多数任务中显示出超人的能力。然而，这种性能的提高伴随着模型复杂性的增加，将人工智能系统变成了一个黑匣子，其决策可能很难被人类理解。采用黑盒模型可能会产生严重的后果，因为系统做出的决策不仅会影响业务成果，还会影响许多人的生活。从驾驶汽车和预防犯罪到产品推荐、做出投资决策、批准贷款和雇用员工，ML 模型越来越多地被用来代替人类决策。因此，对于利益相关者来说，了解这些算法如何做出决定以获得对人工智能在运营中的使用的信任和信心变得越来越重要。因此，对可解释的人工智能(XAI)的兴趣越来越大，这是一个与解释和帮助解释机器学习模型的方法的发展有关的领域[1]。

**什么是可解释的 AI？**

可解释人工智能(XAI)领域专注于开发工具、框架和方法，以帮助理解机器学习模型如何做出决策。它的目标是提供对复杂的 ML 模型内部工作的洞察，并帮助理解模型决策的逻辑。XAI 有助于为人工智能带来透明度，使其有可能打开黑盒，并以人类容易理解的方式揭示决策过程。模型解释通常是一些视觉或文本指南形式的额外元数据信息，这些信息提供了对特定人工智能决策的洞察或揭示模型整体的内部功能[2]。表达元数据的机制包括文本解释、视觉解释、示例解释、简化解释和特征相关性解释。XAI 是一个快速发展的领域，已经有大量关于可解释机制和技术的文献。我在本文末尾提供了一些参考资料。本文的重点是为可解释的人工智能构建商业案例。

为什么模型的可解释性很重要？

公平、信任和透明是驱动可解释性需求的三个主要关注点。已经发现人工智能系统在许多情况下会产生不公平、有偏见和不道德的决策[3]。例如，人工智能系统筛选申请人已被证明对雇用女性和其他少数族裔有偏见，如亚马逊的招聘引擎显示出对女性申请人的偏见([亚马逊废弃了显示对女性有偏见的秘密人工智能招聘工具](https://www.reuters.com/article/us-amazon-com-jobs-automation-insight/amazon-scraps-secret-ai-recruiting-tool-that-showed-bias-against-women-idUSKCN1MK08G))。当管理人员盲目依赖人工智能的输出来增加或取代他们的决策时，公平性就会受到破坏，而不知道模型如何以及为什么做出这些决策，模型是如何训练的，所使用的数据集的质量如何，或者模型何时以及何时效果不好。通过提供对模型工作的深入了解，XAI 促进了公平性，并有助于减轻可能由输入数据集或糟糕的模型架构引入的偏差。

随着模型的复杂性及其决策影响的增加，信任是另一个重要因素。很难相信一个人无法观察和理解的系统的决策。例如，在不清楚算法为什么会提出这些建议的情况下，医生或患者对遵循人工智能算法的建议做出诊断有多大信心？人工智能诊断可能被证明更准确，但缺乏可解释性会导致缺乏信任，从而犹豫使用。模型的可解释性可以帮助建立对其结果的信任，并巩固利益相关者对其使用的信心。

透明度是驱动可解释性需求的第三个关键因素。透明度有助于评估输出预测的质量，了解与模型使用相关的风险，并了解模型可能表现不佳的情况。通过获得对模型行为的直观理解，负责模型的个人可以识别模型可能失败的场景，并采取适当的行动。它还可以通过让业务用户了解可以操纵模型输入来影响输出的方式，来帮助阻止敌对攻击。

除了提高公平性、信任度和透明度，可解释性还可以通过提供对潜在弱点的理解来帮助提高模型性能。理解模型为什么和如何工作以及为什么它有时会失败使 ML 工程师能够改进和优化它。例如，了解不同输入数据分布的模型行为可以帮助解释输入数据中的偏斜和偏差，ML 工程师可以使用这些数据进行调整，并生成更稳健和公平的模型。

**可解释人工智能的商业价值**

可解释的人工智能对商业领袖也有战略价值。可解释性可以加速人工智能的采用，实现问责制，提供战略洞察力，并确保道德和合规性[4]。由于可解释性有助于在 ML 中建立利益相关者的信任和信心，它增加了 AI 系统在组织中的采用，为其提供了竞争优势。可解释性给了组织领导者信心，让他们在业务中接受人工智能系统的责任，因为这让他们更好地理解系统的行为和潜在风险。这促进了对人工智能项目的更大的执行买入和赞助。在关键利益相关方和高管对人工智能的支持下，该组织将更好地促进创新、转型和开发下一代能力。

可解释的模型还可以帮助提供对关键业务指标的有价值的见解，如销售、客户流失、产品声誉、员工流动等。，从而可以改善决策和战略规划[4]。例如，许多公司采用机器学习模型来衡量客户情绪。虽然理解客户情绪是有价值的，但模型解释也可以提供对价格、客户服务、产品质量等情绪驱动因素的洞察。，以及它们对客户的影响，允许企业采取适当的措施来解决这些问题。同样，许多公司使用销售预测模型来预测销售和计划库存。如果预测模型也能显示价格、促销、竞争等关键因素。有助于销售预测，该信息可用于促进销售。

监管合规性正迫使一些企业采用可解释的人工智能实践([新的人工智能法规即将出台。你的组织准备好了吗？](https://hbr.org/2021/04/new-ai-regulations-are-coming-is-your-organization-ready))。组织面临来自客户、监管机构和行业联盟的越来越大的压力，以确保他们的人工智能技术符合道德规范，并在公众可接受的范围内运行。监管重点包括保护弱势消费者、确保数据隐私、促进道德行为和防止偏见。表现出非故意的人口统计偏差的模型尤其令人担忧。使用可解释的模型是检查偏见和决策的一种方式，这种方式不会违反商业道德规范并防止声誉损失。从数据隐私的角度来看，XAI 可以帮助确保只有被允许的数据被用于模型训练的商定目的，并使之有可能在需要时删除数据。重要的是从一开始就在人工智能训练中建立道德指南针，并通过 XAI 评估监控人工智能的行为。

**可解释的人工智能应该是一个组织的人工智能原则的必要元素。**

由于可解释性是一个如此关键的要求，可解释的人工智能必须包含在每个组织的人工智能原则中，并成为他们人工智能战略中的一个关键考虑因素。可解释性不能是事后的想法，必须从一开始就计划好，并集成到整个 ML 生命周期中。一种将公司的人工智能设计和开发与其道德价值观、原则和风险偏好相结合的正式机制可能是必要的*。*确保业务经理理解风险和无法解释的模型的局限性，并能够对风险负责，这一点很重要。

**参考文献**:

*【1】Linardatos，p .，Papastefanopoulos，v .，& Kotsiantis，S. (2021)。可解释的人工智能:机器学习可解释性方法综述。熵，23(1)，18。*

*【2】达什·阿勒德(2020)&拉德普。可解释人工智能(xai)的机遇和挑战:一项调查。预印本 arXiv:2006.11371。*

[3]罗伯特，L. P .，皮尔斯，c .，马奎斯，l .，金，s .，T12 阿拉马德，R. (2020)。为管理组织中的雇员设计公平的人工智能:回顾、评论和设计议程。人机交互，35(5–6)，545–575。

[4]奥克斯伯勒，c .、卡梅伦，e .、拉奥，a .、伯查尔，a .、汤森，a .、&韦斯特曼，C. (2018)。可解释的人工智能:通过更好的理解驱动商业价值。*检索自普华永道网站:https://www . PWC . co . uk/audit-assurance/assets/explable-ai。pdf* 。

*【5】阿里埃塔，A. B .，迪亚斯-罗德里格斯，n .，德尔塞尔，j .，本尼托，a .，塔比克，s .，巴尔巴多，a，...&埃雷拉，F. (2020)。可解释的人工智能(XAI):面向负责任的人工智能的概念、分类法、机遇和挑战。信息融合，58，82–115。*