# 让情感模型变得可以解释

> 原文：<https://towardsdatascience.com/making-a-sentiment-model-explainable-5413437d9db2?source=collection_archive---------19----------------------->

## [模型可解释性](https://towardsdatascience.com/tagged/model-interpretability)

## 理解人工智能的内部运作有很多目的，每一个都进一步扩大了它的范围和有效性。

我们一生中花了大量的时间去挖掘他人的动机。(“她为什么说*那个*？”).我们在这个世界上经常被别人的言行所困惑，通过假设和归因来解决我们之间的理解差距(“我*确定*这是因为我昨天迟到了。”)或者通过询问，既间接("*她的问题到底是什么*？")并且直接(“除非你告诉我*为什么*你那样说，否则我不会离开。”).换句话说，我们想象为什么或者问为什么。

随着人工智能变得越来越普遍，我们开始想知道它的动机是很自然的。是的，这部分是因为我们害怕会奴役我们所有人的超级智能可能会从人工智能中进化出来，并希望看到它的到来。但至关重要的是，这也是因为这种新智能的机制对我们来说是如此陌生。我们有兴趣了解它是如何工作的。如果它做出的决定对我们的生活有重大影响，我们很自然会想知道它是如何做出这些决定的。

我们想知道，就像我们经常对人类做的那样:

*   **决策正确吗**:人工智能出错的频率有多高？决策是否基于所有相关信息？
*   **这些决定公平吗**:人工智能表现出对人类如此普遍的有害的社会偏见吗？
*   **决策可靠吗**:**AI 每次遇到类似的输入都会做出一致的决策吗？**
*   **这些决定合理吗 : 这些决定能向我们解释吗？**
*   ****决策是确定的** : 特定决策的 AI 有多有把握？**

**理解是建立信任的先决条件。使用算法技术来理解人工智能的决策被称为人工智能可解释性。在 Dialpad，我们使用可解释性技术来帮助我们的客户理解他们通过产品功能进行交互的深度学习模型的输出。我们还使用这些技术来改进我们的深度学习模型。**

# **拨号键盘的可解释性**

**让我们以目前在产品中运行的[情感分析模型](https://help.dialpad.com/hc/en-us/articles/360044565512-Voice-Intelligence-Moment-Spotlight-Sentiment)为例，可解释性在其中扮演着重要的角色。情绪分析功能挑选出通话记录中表现出积极或消极情绪的部分。**

**每当一个说话者说了一些表示他们对某事非常高兴或满意的话，积极情绪就会被识别和强调。**

**一个例子:*“我们喜欢这个演示，它提供了很多信息。”***

**每当说话者说了一些表示他们对某事非常沮丧或恼火的话，消极情绪就会被识别和强调。**

**一个例子:*“这不起作用，我真的很沮丧。”***

**关于我们的情感分析功能的旧版本，一个常见的用户抱怨是，他们不明白为什么某些句子被标记为积极或消极的情感。**

**在句子很长或者表达的情感不太明显的情况下尤其如此。此外，一些情感判断是主观的价值判断。例如，情感分析模型对脏话应该有多敏感？**

**有些人不赞成在商务交流中使用任何脏话，认为它们是“负面的”，而另一些人则根据上下文评估它们的适当性，或者根本不介意它们。在这种情况下，情感分析的“正确性”取决于用户的视角。**

**有时尽管模型是完全错误的。例如，当今的情感分析模型在讽刺方面表现很差，通常标记为积极的情感，因为使用了“积极”的词，这显然是(无论如何对人类来说是显而易见的)轻蔑的表示。**

**许多这些灰色区域存在于情感检测的任务中。为了在情感分析功能中建立信任，我们需要使作为其基础的深度学习模型变得可解释。**

# **如何审问人工智能**

## **当我们戳**

**当我们敦促深度学习模型解释自身时，我们希望揭示模型功能的哪些错综复杂之处？我们可能想知道:**

*   ****输入数据的哪些元素对做出的决策最重要。**例如，在一个帮助自动驾驶汽车感知的深度学习模型中，我们可能想知道相机图像的哪些部分最常帮助模型区分类似的物体，如路灯杆和交通灯杆。**
*   ****一个模型需要学习哪些中间任务才能达到它的既定目标**。例如，为了学习如何概括一段文本，模型需要学习词类吗？我们可能还想知道模型的哪些部分专门执行这些中间任务。**
*   ****为什么模型失败，什么时候失败**。当语音助手无法理解对他们说的话时，那个特定的输入或者那个输入和模型之间的交互会导致这个失败。**
*   ****是否存在某些模型表现不佳的可识别输入组？**这条询问线对于确定你的模型是否已经学会了基于性别、种族和其他受保护类别的有害偏见至关重要。**

## **当他们回应时**

**深度学习的解释有两大类:**

****全局可解释性/结构分析:**这包括研究模型的内部结构，检测哪些输入模式被捕获，以及这些模式如何被转换以产生输出。这类似于进行大脑核磁共振成像或脑电图检查，可以让你窥视大脑，看看它是如何连接的，研究各个组成部分及其对不同刺激的反应，记录结构和操作故障等等。**

****局部可解释性/行为分析:**这包括通过观察单个特定实例的行为来推断模型如何工作。这类似于心理学实验，要求参与者在干预前后执行指定的任务，以研究所述干预的效果。**

## **提问的艺术**

**有几种技术可以用来查询深度学习模型的工作情况。虽然这些技术的机制超出了本文的范围，但我将尝试总结主要的[类](https://arxiv.org/pdf/1909.12072.pdf)调查:**

****用局部扰动解释**:在这条研究路线中，我们非常细微地改变单个输入的不同部分或方面，以观察这如何影响输出。如果输入的一个部分或一个方面——一组可识别的像素，一个特定的单词——在输出中产生明显的变化，人们可以得出结论，输入的这些部分对输出最重要。**

**例如，在自动语音识别模型中，一个将口头语音转录为文本的模型，从头到尾一个接一个地丢弃音频的小部分，并查看这种干预如何改变转录的文本，将告诉您特定音频文件的哪些部分对其转录最重要。**

**这一类技术会给你局部的解释。这类技术包括[渐变](https://arxiv.org/abs/2107.11400)、[遮挡](https://arxiv.org/abs/1311.2901)、[激活最大化](https://www.researchgate.net/profile/Aaron-Courville/publication/265022827_Visualizing_Higher-Layer_Features_of_a_Deep_Network/links/53ff82b00cf24c81027da530/Visualizing-Higher-Layer-Features-of-a-Deep-Network.pdf)。**

****用代理人解释**:通过这类技术，我们建立更简单的、基于树的或线性的机器学习模型，来解释更复杂的深度学习模型的个体预测。线性模型让我们非常清楚地知道哪些输入要素对预测很重要，因为它们本质上是为不同的输入要素分配权重。权重越高，特征越重要。**

**这一类技术会给你局部的解释。这一类的技术包括[石灰](https://arxiv.org/pdf/1602.04938v1.pdf)、 [SHAP](https://arxiv.org/abs/1705.07874) 、 [SmoothGrad](https://arxiv.org/abs/1706.03825) 。**

****基于传播的方法(利用结构)**:在这里，我们深入研究模型结构，并研究单个神经元的贡献，以了解哪些神经元和层对输出影响最大。使用一种名为[层相关性传播](https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0130140) (LRP)的技术，[发现](https://www.nature.com/articles/s41467-019-08987-4.pdf)在[流行图像数据集](http://host.robots.ox.ac.uk/pascal/VOC/)上训练的分类模型只学会了如何识别一匹马，因为大约五分之一的马图像中存在源标签。移除标签也移除了将图片分类为马的能力。此外，在汽车图像上插入标签会将分类从汽车更改为马。**

**![](img/cfde2c78839fb21df754b70701a33cd7.png)**

**资料来源:[https://www.nature.com/articles/s41467-019-08987-4.pdf](https://www.nature.com/articles/s41467-019-08987-4.pdf)(抄送 4.0)**

**可解释的模型可以帮助模型建立者发现这种错误的相关性并加以纠正。**

**这一类技术会给你全面的解释。这类技术包括 [LRP](https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0130140) 、[解卷积](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.849.3679&rep=rep1&type=pdf)、[导向反向传播](https://arxiv.org/abs/1412.6806)。**

****元解释**:我们使用技术来发现相关的或基础的任务，这些任务是模型为了完成工作必须首先学习的。为此，我们插入模型的中间层，即位于输入和输出之间的层，并要求它们执行各种感兴趣的任务。例如，一项[研究](https://aclanthology.org/P19-1356.pdf)的作者发现 BERT 的中间层编码了丰富的语言信息层次，例如学习句子中的名词是复数还是单数，动词的时态是过去、现在还是将来，动词的类型和句子的主语是否匹配。他们得出结论，这些技能的学习似乎是执行高级自然语言任务(如文本分类和生成连贯文本)的先决条件。**

**这一类技术会给你全面的解释。这类技术包括[喷雾](https://arxiv.org/pdf/1902.10178.pdf)、[探测](https://arxiv.org/pdf/1610.01644.pdf)。**

# **使情绪模型可解释:指南**

****步骤#1:决定解释的种类****

**如上文部分[所述，解释本质上可以是全局的(基于结构)或局部的(基于预测)。我们想帮助用户理解个人情绪预测，所以我们选择使用局部可解释技术，专门解释个人预测。](#2de3)**

****步骤#2:决定解释的形式****

**接下来，我们必须决定解释应该采取什么形式。它应该是视觉的，文本的，表格的，图形的？什么形式的解释既容易理解，又可能回答用户对输出的问题。我们决定向用户展示哪些单词对迫使模型将一个句子标记为积极或消极情绪最有影响。我们决定在产品中突出这些词。用户研究表明这种形式的解释是最有帮助的。**

**在某种意义上，这种解释的汇总为用户提供了一个观察情绪模型世界观的窗口——该模型发现哪些类型的负面情绪特别值得注意，它对隐喻等比喻性语言的理解程度如何，什么习惯性地混淆了它。**

**![](img/a00422b92b85098bde517a6817563274.png)**

**图片由 Dialpad 提供**

****步骤#3:在可解释技术之间进行选择****

**你选择的技术将取决于你建立的深度学习模型的类型。有些技术是模型不可知的，也就是说，它们适用于每一种模型，而有些技术是特定于模型的。我们尝试了各种技术，对输出结果进行了多次人工评估，并在[局部扰动](#f70c)技术家族中选择了一种技术，它给了我们最直观、最丰富的解释。**

**在这个过程中，我们必须做出几个设计选择——例如，我们选择的技术返回对预测最重要的单词和短语的加权列表。我们必须做出选择——我们是应该向用户显示所有重要的单词，也许用渐变的颜色来表示哪些单词更重要和更不重要，还是只显示单词的子集。最后，我们定义了一个阈值，决定只突出显示最重要的单词。我们还编制了一个格式规则列表，用于清理和完善解释——例如，如果短语中的大部分单词被认为是重要的，则突出显示整个短语。**

**您用于个人预测的任何技术都必须集成到模型推理代码中，并在模型做出预测后执行，因为您实际上是在解释*那个*预测。有许多[打开](https://github.com/albermax/innvestigate) [源](https://captum.ai/) [包](https://github.com/wangyongjie-ntu/Awesome-explainable-AI#python-librariessort-in-alphabeta-order)你可以适应解释你的模型。**

****步骤#4:优化技术****

**我们必须优化所选择的技术，因为情感功能必须足够快速和灵活，以便在实时环境中操作，并且对于做出预测所花费的时间，我们现在添加了一个解释预测的额外任务。你可以通过删除无关的操作，使用更有效的库和函数来完成中间任务，甚至改变技术来更快地返回结果。**

**我们最重要的学习？**

**关键是要让你的听众站在最前面，以听众可能喜欢的解释的形式和类型为基础做出决定。为满足管理标准而给出的解释可能与为日常用户的教育而给出的解释有很大不同。**

**我们的用户发现，这一功能升级有助于他们更好地理解和信任情感分析功能，其在产品中的使用和可见性也有所增加。**

**可解释性也帮助我们建立了一个更好的模型。通过各种解释技巧，特别是本地解释的集合，我们能够*识别*和*地址*:**

*   **误报的原因，模型不应触发的次数。**
*   **可能导致不利结果的数据偏差。**

# **结论**

**理解人工智能的内部工作有很多目的，每一个目的都促进了人工智能的范围和有效性。我们必须理解建立更好和更公平的模式，建立信任，并建立一个包容和致力于人类繁荣的数字未来。**