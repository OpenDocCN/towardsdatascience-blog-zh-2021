# 你能建立一个机器学习模型来监控另一个模型吗？

> 原文：<https://towardsdatascience.com/can-you-build-a-machine-learning-model-to-monitor-another-model-15ad561d26df?source=collection_archive---------10----------------------->

![](img/b7bffa232dbc2401babbde1d05917410.png)

图片作者。

你能训练一个机器学习模型来预测你的模型的错误吗？

没有什么能阻止你去尝试。但很有可能，没有它你会过得更好。

我们不止一次看到有人提出这个想法。

表面上听起来很合理。机器学习模型会出错。让我们利用这些错误训练另一个模型来预测第一个模型的失误！一种“信任检测器”，基于我们的模型在过去的表现。

![](img/8a3252d4cf8fd895727e6ccce1678ad2.png)

图片作者。

从错误中学习本身就很有意义。

这种精确的方法是机器学习中 boosting 技术的基础。它在很多集成算法中实现，比如决策树上的梯度提升。每一个下一个模型都被训练来纠正前一个模型的错误。模型合成比一个执行得更好。

![](img/6b6fdec9957d7eedbdb1832bba9e5e8a.png)

图片作者。

但是它能帮助我们训练一个独立的第二个模型来预测第一个模型是否正确吗？

答案可能会令人失望。

让我们通过例子来思考。

# 训练看门狗

**比方说，你有一个需求预测模型。**你想在它不对的时候抓住它。

你决定在第一个模型错误的基础上训练一个新模型。这到底意味着什么？

这是一个回归任务，我们预测一个连续变量。一旦我们知道了实际的销售量，我们就可以计算模型误差。我们可以选择像 MAPE 或 RMSE 这样的城市。然后，我们将使用该指标的值作为目标来训练模型。

![](img/a0bcf012f9684befe1eb7c1431ecece6.png)

图片作者。

**或者我们来举个分类的例子:**信用贷款违约的概率。

我们的贷款预测模型可能是一个概率分类。每个客户都会得到一个从 0 到 100 的分数，来衡量他们违约的可能性。在某个临界值，我们拒绝贷款。

总有一天，我们会知道真相。我们的一些预测可能是假阴性:我们给了那些仍然违约的人贷款。

但是，如果我们对所有的预测不加审查就采取行动，我们永远也不会了解假阳性。如果我们错误地拒绝了贷款，这个反馈会留给客户。

我们仍然可以利用我们得到的部分知识。或许，采用违约客户的预测概率，然后训练一个新的模型来预测类似的错误？

![](img/3d691e6859f5181882e395cc3963c602.png)

图片作者。

# 这能行吗？

是，也不是。

技术上是可行的。也就是说，你可以训练一个模型来预测一些事情。

但是如果是这样，这意味着你应该重新训练初始模型！

让我们解释一下。

**为什么机器学习模型会出错？**撇开数据质量不谈，通常是以下两种情况之一:

1.  **模型训练的数据中没有足够的信号。**还是数据不够。总体而言，或针对失败的特定部分。该模型没有学到任何有用的东西，现在返回一个奇怪的响应。
2.  我们的模型不够好。从数据中正确捕捉信号太简单了。它不知道它可以潜在地学习的东西。

在第一种情况下，模型误差没有模式。因此，任何训练“看门狗”模型的尝试都会失败。没有什么新东西要学。

在第二种情况下，你也许能训练出一个更好的模型！一个更复杂的模型，更适合数据来捕捉所有的模式。

但如果可以这样做，为什么还要训练“看门狗”呢？为什么不更新第一个模型呢？它可以从我们第一次应用它时得到的真实世界的反馈中学习。

![](img/eed2c8d815a92c307b6b7ef77c24b800.png)

图片作者。

# 一个模型来统治他们

**很有可能，我们最初的模型并不“糟糕”**这些可能是发生变化的客户，或者是带来新模式的一些现实情况。认为疫情影响了销售和信用行为。我们已经讨论过的相同的旧的[数据和概念漂移](https://evidentlyai.com/blog/machine-learning-monitoring-data-and-concept-drift)。

我们可以获取关于销售和贷款违约的新数据，并将其添加到旧的训练集中。

我们不会预测“错误”我们将教会我们的模型预测完全相同的事情。一个人拖欠贷款的可能性有多大？销售量会是多少？但这将是一个新的、更新的模式，从自己的错误中吸取教训。

就是这样！

旁边的“看门狗”型号不会增值。

它根本没有其他数据可以借鉴。这两种型号使用相同的功能集，并且可以访问相同的信号。

如果新模型出错，“看门狗”模型也会错过它。

![](img/6f6d53c0f9894ed4d957987703789610.png)

我们在相同特征集上训练的第二个模型将信任第一个模型。(图片由作者提供)。

**一个例外是，我们无法访问原始模型，也无法直接对其进行重新培训。**例如，它属于第三方或因法规而固定。

如果我们从现实生活的应用环境和实际标签中获得新数据，我们确实可以构建第二个模型。然而，这是一种人为的限制。如果我们自己在维护原来的模型，这样做是没有意义的。

# 我们能做些什么呢？

“看门狗”模式的想法行不通。我们还能做什么？

先说为什么。

**我们的主要目标是构建在生产中表现良好的值得信赖的模型。**我们希望尽量减少错误的预测。其中一些对我们来说可能很昂贵。

假设我们在建模方面尽了最大努力，我们可以使用其他方法来确保我们的模型可靠地运行。

## **首先，建立常规监控流程。**

是的，这种方法不能直接解决模型产生的每个错误。但它建立了一种保持和改善模型性能的方法，从而最大限度地减少规模误差。

这包括通过监控[输入分布](https://evidentlyai.com/blog/evidently-001-open-source-tool-to-analyze-data-drift)和[预测](https://evidentlyai.com/blog/evidently-014-target-and-prediction-drift)的变化来检测[数据和概念漂移](https://evidentlyai.com/blog/machine-learning-monitoring-data-and-concept-drift)的早期迹象。

![](img/91392a2b44d0327f4550e046a6319de0.png)

*使用* [的数据漂移监控示例显然是](https://github.com/evidentlyai/evidently) [需求预测](https://evidentlyai.com/blog/tutorial-1-model-analytics-in-production) *教程中的 *。(图片由作者提供。)**

## **第二，考虑用好的旧规则耦合机器学习。**

如果我们更详细地分析我们的模型行为，我们可以识别出它表现不好的地方。然后，我们可以将模型应用限制在那些我们知道模型有更多机会成功的情况下。

在详细的[教程](https://evidentlyai.com/blog/tutorial-2-model-evaluation-hr-attrition)中，我们探索了如何在员工流失预测任务中应用这个想法。我们还考虑为概率分类添加一个自定义阈值，以平衡假阳性和假阴性错误。

![](img/d6d39ac39ddecffda5481439c3f2738b.png)

图片作者。

## 第三，我们可以在模型输入上添加统计检查。

在“看门狗”模型中，想法是判断我们是否可以信任模型输出。相反，我们可以检测输入数据中的异常值。目标是验证它与模型所训练的内容有多大的不同。例如，如果一个特定的输入与模型之前看到的“太不一样”,我们可以发送它进行人工检查。

*旁注。感谢我们的一位读者引发了这场对话！*

**在回归问题中，有时候可以建立一个“看门狗”模型。考虑到预测误差的符号，当您的原始模型优化预测误差时，就会出现这种情况。如果第二个“看门狗”模型预测的是绝对错误，它可能会从数据集中获得更多信息。**

但这里有一件事:如果它有效，这并不意味着这个模型是“错误的”或者如何纠正它。相反，它是评估数据输入不确定性的一种间接方法。(这里有一篇[整篇论文](https://arxiv.org/pdf/2006.10562.pdf)详细探讨了这一点)。

实际上，这又让我们回到了同一个备选方案。让我们检查输入数据是否属于相同的分布，而不是训练第二个模型！

# 总结

我们都希望我们的机器学习模型表现良好，并知道我们可以信任模型输出。

虽然用另一个受监督的“看门狗”模型来监控你的机器学习模型的乐观想法成功的机会很低，但这种意图本身有其优点。还有其他方法可以保证你的模型的生产质量。

这些包括建立一个全面的监控过程，设计定制的模型应用场景，检测异常值，等等。

在下面的文章中，我们将更详细地探讨它们。

*最初发表于*[*【https://evidentlyai.com】*](https://evidentlyai.com/blog/can-you-build-a-model-to-monitor-a-model)*并与* [*伊莲娜*](https://www.linkedin.com/in/elenasamuylova/) *合著。*

在 appeally AI，我们创建了开源工具来分析和监控机器学习模型。在 GitHub *上查看我们的* [*项目，如果喜欢就给它一颗星吧！*](https://github.com/evidentlyai/evidently)

想留在圈子里吗？

*   [*报名参加*](https://evidentlyai.com/sign-up) *我们的每月简讯。*
*   *关注* [*推特*](https://twitter.com/EvidentlyAI) *和*[*Linkedin*](https://www.linkedin.com/company/evidently-ai/)*。*
*   *加入我们的* [*不和谐社区*](https://discord.gg/xZjKRaNp8b) *聊天连线。*