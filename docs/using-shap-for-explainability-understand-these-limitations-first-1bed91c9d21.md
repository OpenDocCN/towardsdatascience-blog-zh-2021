# 使用 SHAP 解释——首先理解这些限制

> 原文：<https://towardsdatascience.com/using-shap-for-explainability-understand-these-limitations-first-1bed91c9d21?source=collection_archive---------11----------------------->

## 可解释性做对了

![](img/67ae20977d3d0d04dbd385d52276e118.png)

照片由[德鲁·比默](https://unsplash.com/@drew_beamer?utm_source=medium&utm_medium=referral)在 [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral) 上拍摄

我非常享受我的 MBA 学位；我非常喜欢每一个主题，并且能够将每个主题的知识如何帮助我成为一个更好的拥有强大全局观的经理联系起来。然而，有一门课我从来不愿意走进教室，那就是商业伦理。我强烈认为伦理是不可教授的。每个人都有不同程度的信息技术，这是他/她的价值体系的功能。对我来说，也许偷一支笔可以，但偷一辆车就不行了。对其他人来说可能会不同。

最后，普通的定义太好了，不能进一步扩展——“做正确的事情。”

然而，这种“做正确的事情”只对人类有效，因为道德价值是普遍定义的。对于机器来说，价值体系的概念失效了。因此，在我们到达“奇点”这个有争议的阶段之前，为机器定义伦理是人类的责任。

无论是模特的种族偏见、twitter 显著性问题([链接](https://blog.twitter.com/engineering/en_us/topics/insights/2021/learnings-from-the-first-algorithmic-bias-bounty-challenge))还是人工智能反基督([链接](https://onezero.medium.com/is-a-i-the-antichrist-a2fd6d853610))——围绕模特偏见和公平问题的例子数不胜数。目前这是人工智能/人工智能发展的最大障碍。

在过去的 5 年里，可解释的人工智能有了巨大的发展。让复杂的模型变得可解释，并试图消除模型中的任何偏见和诱导公平，一直是数据科学家的主要目标之一。

在所有的方法中，SHAP 和莱姆是两个特别的方法，得到了极大的关注。SHAP 以某种方式计算特征的边际贡献，使得总贡献是 100%。另一方面，LIME 关注于**局部保真度**。即，用局部的、可解释的线性模型来近似任何黑盒机器学习模型以解释每个单独预测的技术。

如果你想了解 SHAP 的运作，这里有一篇关于它的好文章:

[](/can-shap-trigger-a-paradigm-shift-in-risk-analytics-c01278e4dd77) [## SHAP 能引发风险分析的范式转变吗？

### 由 Chandan Durgia 和 Prasun Biswas 撰写

towardsdatascience.com](/can-shap-trigger-a-paradigm-shift-in-risk-analytics-c01278e4dd77) 

在这两者中，虽然 LIME 更快，但是 SHAP 提供了全局和局部的一致性和可解释性，并且在行业中更常用。

在使用 SHAP 之前，应该考虑围绕该方法的各种限制，以便更好地理解和解释该模型。

**局限性 1:** **相关性而非因果性:**重要的是要承认，SHAP 只是“解释”了按照模型结构定义的变量“相关性”。这并不意味着定义的变量也有因果关系。在因果关系缺失的情况下，这可能是因为虚假的相关性，或者是因为模型中省略了变量(一些本可以更好地定义产出的变量从数据集中缺失，而其他变量试图替代这些缺失变量的影响)。重要的是，要对模型中的每个变量的重要性、标志和因果行为进行单独检查。

**限制二:** **对模型的依赖:** SHAP 按设计是“一个特性对模型有多重要”，并不暗示“这个特性在现实中有多重要”。粗略地说，SHAP 显示了变量对输出全球平均值的敏感性——给定模型。

这提出了两个关键限制:

1.注意，由于 SHAP 有助于推断给定模型的特征的重要性，如果模型被不正确地开发/训练，SHAP 推断将存在固有的问题。

2.因为变量的重要性和符号是在全局平均值(称之为基准值)的基础上定义的。基准值本身的不正确可能会导致变量的推断错误——无论是在标牌上还是在特征的重要性上。

**限制三:******特征重要性和标志:**值得注意的是，SHAP 值的推断与模型的“目标”有很强的相关性。例如，如果一个模型是为选择一个好的股票(a 股)而开发的，如果模型的目标是投资组合优化而不是购买/不购买股票，则输出可能具有不同的特征重要性(或标志)，尽管这两个模型都旨在增加回报。因此，应始终考虑模型目标来分析 SHAP 输出。**

****限制 4:** **多重共线性问题:**如果存在高度多重共线性的变量，则其中一个变量的 SHAP 值会很高，而另一个变量的值为零/非常低。这可能与特性重要性的概念相矛盾。这里的问题不在于 SHAP 如何分配这些值，而是与模型的训练方式有关。如果机器以首先将权重分配给一个变量(比如 x1)的方式被训练，则其他相关变量的贡献(比如 x2)将是最小的。如果从商业角度来看，第二个变量(x2)更直观，这可能看起来是违反直觉的。**

**正如亚瑟·C·克拉克第三定律所说，“任何足够先进的技术都和魔法没什么区别。”，是像 SHAP 这样的方法论揭示了魔术背后的现实，并为机器定义了道德科学。**

**SHAP 是提高模型解释力的一个很好的手段。然而，像任何其他方法一样，它有自己的优点和缺点。当务之急是，在使用这种方法时要牢记局限性，并在适当的背景下评估 SHAP 价值观。**

**如果你遇到了 SHAP 的更多限制，请在评论中分享。**

***免责声明*:本文所表达的观点是作者以个人身份发表的观点，而非其各自雇主的观点。**