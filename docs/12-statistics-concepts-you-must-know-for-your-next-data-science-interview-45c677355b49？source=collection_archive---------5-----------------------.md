# 下一次数据科学面试中你必须知道的 12 个统计学概念

> 原文：<https://towardsdatascience.com/12-statistics-concepts-you-must-know-for-your-next-data-science-interview-45c677355b49?source=collection_archive---------5----------------------->

## 自信地赢得下一次数据科学面试

统计学提供了工具和方法来发现数据的结构和有意义的见解，还帮助我们量化隐含的不确定性，因此，拥有良好的统计学基础对任何数据科学家都至关重要。因此，对于任何数据科学面试，我们都会根据我们的统计知识进行测试。

![](img/746360a6fc8afbe2653147abb9ffcb28.png)

米利安·耶西耶在 [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral) 上拍摄的照片

在这篇文章中，我整理了 12 个统计学概念，我发现它们对破解面试非常有用。因此，我将在这里回顾这 12 个概念，并解释它们的全部内容。

1.  **条件概率与贝叶斯定理**
    对于任意两个事件 A 和 B，P(A|B)表示在事件 B 已经发生的情况下，事件 A 发生的条件概率。条件概率的公式由以下等式给出

![](img/8de81bc37b07f348cb718ee73fd2ee75.png)

继续条件概率的讨论，当新的信息可用时，修正我们对事件的先验概率是一个关键阶段，这就是**贝叶斯定理**变得有用的地方。下面的数学等式总结了**贝叶斯定理**。

![](img/f3d053160169eb564b525e9ef7f8b851.png)

在这个等式中，A 是一个事件，B 是从数据中获得的经验证据或信息。因此，P(A)是事件 A 的先验概率，P(B)是基于数据证据的事件 B 的概率，P(B|A)被称为可能性。因此，贝叶斯定理根据我们对事件的先验知识给出了事件的概率，并在我们获得新的信息时更新条件概率。

贝叶斯定理的一个非常简单的例子是，假设早晨多云，预测某一天下雨的概率。假设，在六月的某一天下雨的概率是 10%,如果下雨，早上多云的概率是 50%。此外，六月任何一天早晨多云的概率，即 P(云)是 40%，那么应用贝叶斯定理，我们可以得出结论，假定今天早晨多云，今天下雨的概率是:

![](img/9b2def24278f5ab17d6c664c4c6b5546.png)

**2。取样技术**

如果我们把一组中的所有元素称为总体，那么这个总体的一个子集称为样本。样本中观察值或个体的总数称为样本容量。人口的任何统计常数，例如，人口的任何特征的平均值、方差等描述性度量被称为参数。当基于样本观察值计算时，相同的统计度量将被称为统计量。从总体中选择样本的方法有两种——概率性的和非概率性的。

**概率抽样方法**

**答:简单随机抽样**

在简单随机抽样中，每个样本从总体中选取的可能性是相等的。这项技术减少了选择偏差。然而，这种技术的缺点是，我们可能无法根据任何特定的特征来选择样本，这是不常见的。

**举例—** 简单随机抽样的一个例子是通过抽签从一个 100 人的班级中选出 10 名学生。

**b .系统采样**

在系统抽样中，样本是在固定的特定时间间隔内收集的。系统抽样的优点是它有助于保持足够的抽样规模。

系统抽样的一个例子是在一家杂货店中选择每 10 个顾客进行顾客满意度调查。

**c .分层抽样**

在分层抽样中，整个人口被分成不同的小组，也称为具有相似特征的阶层。这种取样的主要目的是从所有在感兴趣的特征中表现出可变性的亚组中获得代表性。。

例如，如果我们有兴趣了解人们对印度任何特定话题的看法，我们可以根据城市、性别和年龄组来划分人口。基于所选特征的这些子群体被称为层，并且层中的每个样本是基于概率抽样方法(例如简单随机抽样)来选择的。

**非概率抽样方法**

**a .方便取样**

在方便抽样中，参与者是根据他们的可用性和参与研究的意愿来选择的。然而，这种技术可能会受到志愿者偏见的影响，这可能是所有非概率抽样方法的风险，因为样本是自己选择的，而不是随机挑选的。

**b .配额抽样**

配额抽样在市场调查中相当流行。在这种情况下，采访者只需根据任何调查/研究所需的配额选择样本。例如，一家智能手机公司可能希望调查某个城市中哪个年龄段的人使用哪个品牌的手机。然后他们申请配额，例如每个年龄组 100 人，例如 21-30 岁、31-40 岁、41-50 岁，或者他们可以进一步根据性别实行配额，例如每个年龄组 50 名男性和 50 名女性。他们将根据获取的难易程度和他们的研究预算来选择样本。然而，如前所述，配额抽样作为一种非概率方法，仍可能存在自愿偏差。

**c .有目的的取样**

在有目的抽样中，研究人员运用他们对目标人群的专业知识，以非随机方式选择“代表性”样本。有目的的取样无疑是省时、省钱的，但是除了志愿者的偏见之外，这种类型的取样会导致研究者在选择参与者时的判断偏差/错误。

**3。概率分布**

概率分布对于理解任何数据结构都是至关重要的。主要有两种分布类型——离散分布和连续分布。

**答:离散概率分布**

这些分布模拟了以离散值作为输出的随机变量的概率。例如，如果 X 是一个离散的随机变量，描述了一次投掷中的人头数，我们可以重复多次投掷，并记录每个可能结果的概率。如果我们用函数的形式来表示它，那么它将被称为“概率质量函数(PMF)”，它定义了离散随机变量 X 取特定值 X 的概率。这种分布的例子有伯努利分布、二项式分布、泊松分布。

**b .连续概率分布:**另一方面，连续概率分布模拟随机变量的概率，这些随机变量可能具有本质上连续的任何可能结果。例如，代表城镇居民体重的随机变量 X 的可能值可以是任何值，如 54.5、47.2、60.3 等。示例可以是正态分布、学生 T 分布、卡方分布、指数分布等。

**4。假设检验**

我们使用一个假设来对总体参数进行断言或假设。现在，为了在我们收集的样本数据的基础上验证我们的主张，我们创建了两个假设——无效假设和替代假设。

零假设反映了研究者对假设检验结果的中立观点。它基本上陈述了默认情况，例如，在药物功效实验中，无效假设可以是获得药物的患者和获得安慰剂的患者组之间的治愈率没有差异。与零假设完全相反的陈述就是替代假设。替代假设总是挑战零假设。接受或拒绝零假设的决定是基于样本数据做出的。

**5。置信水平**

继续假设检验的概念，理解什么是置信水平是非常重要的。假设检验中的置信水平是当原假设实际上为真时，不拒绝原假设的概率。它传达了如果我们再次重复这个实验，我们对得到同样的结果有多大的把握。因此，我们构造了一个置信区间，它是由统计推断产生的参数的区间估计。该时间间隔通过以下公式计算:

[基于选定置信水平下样本分布的点估计临界值*样本的标准偏差]

例如，如果我们有一个均值为 60、标准差为 5 的抽样分布，那么 95%的置信区间将在(60–1.96 * 5)即 50.2 和(60 +1.96*5)即 69.8 之间。这里，1.96 是临界值，其来源于这样的事实，即对于正态分布，95%的区域位于 1.96 的标准偏差内

我们永远不可能对统计数据有 100 %的信心，因为我们总会有一些不确定性。因此，在统计研究中，我们通常会遇到 99%、95%或 90%的置信度。

**6。p 值和显著性水平**

对于任何统计推断研究，我们都需要做出拒绝/接受零假设的决定，该决定是基于随机样本的观察值做出的。然而，我们从总体样本中得出的结论总是有可能是错误的。当零假设实际上为真时，与拒绝零假设相关的误差被称为 I 型误差，当零假设为假时，与接受零假设相关的误差被称为 II 型误差。现在，让我们假设，犯第一类错误的概率是某个数字，α，这是任何研究人员为他的研究设定的重要性水平。5%的α表示当我们拒绝零假设时，我们愿意接受 5%的错误概率。另一方面，p 值是观察到的显著性水平，它给出了当零假设为真时，我们获得与从我们的样本数据计算的结果一样极端的结果的概率。

**7。A/B 测试**

A/B 测试是对 A 和 B 两组进行的随机实验，用非常简单的语言来说，它是一种比较单个变量的两个变量的方法，以找出在受控环境中哪个变量的表现优于另一个变量。这种技术通常用于提高客户满意度的营销策略。

例如，可以向一家公司总客户群的 30%发送两封具有不同行动号召的营销电子邮件，以测试哪一封的点击率更高。营销团队然后等待几个小时，他们选择向其余 70%的客户发送点击量更多的电子邮件

**8。z 检验和 t 检验**

在假设检验中，z 检验和 t 检验都非常有用。当样本量较大(即 n ≥ 30)且总体标准差已知时，当我们想要了解两个样本均值是否存在显著差异时，通常会使用 z 检验。另一方面，对于 n >0 的小样本量，t 检验可用于比较两个样本的均值，即使我们不知道总体标准差。t 检验遵循学生的 t 分布，而 z 检验假设样本分布是正态的。在大样本的情况下，Z 检验和 t 检验往往会给出相同的结果，因为对于足够大的样本量，t 分布接近正态分布，Z 得分和 t 得分之间的差异变得可以忽略不计。现在，当我们想要了解样本均值是否与总体均值有显著差异时，可以使用 t 检验来执行单样本 t 检验。因为我们不知道 t 检验中的总体标准差，所以我们使用样本标准差。

**9。协方差和相关性之间的差异**

协方差度量为我们提供了两个变量之间线性关系的方向。另一方面，相关性给了我们这两个变量之间关系的强度和方向。如果我们思考一下数学公式，我们可以用这两个变量的协方差除以同一个变量的标准差的乘积来计算这两个变量的相关系数。因此，相关值是标准化的。因此，相关系数位于-1 到+1 之间，但是协方差可以位于-∞到+∞之间。

**10。线性回归 vs 逻辑回归**

当因变量连续时，通常使用线性回归。线性回归的关键假设之一是因变量和自变量之间存在线性关系。另一方面，当因变量是二元时，使用逻辑回归。它通过将数据拟合到 logit 函数中来预测事件发生的概率。对于逻辑回归，我们不需要因变量和自变量之间的线性关系。

线性回归拟合数据中的直线，而逻辑回归拟合数据中的曲线。线性回归假设预测误差为高斯(或正态)分布。然而，基于线性回归的模型的因变量或自变量不需要遵循正态分布。有时，当训练数据有一些异常值或因变量过于倾斜时，模型中产生的误差也可能不符合正态分布。逻辑回归假设因变量呈二项式分布。

**11。中心极限定理和大数定律**

中心极限定理指出，无论特定变量在总体中的分布如何，当我们从总体中收集样本时，随着样本量的增加，变量均值的采样分布将接近正态分布。中心极限定理在统计学中非常重要，原因有两个:正态性假设对于参数假设检验和估计的精度至关重要。在现实生活中，我们经常会遇到非正态分布。因此，我们可能会担心我们正在进行的假设检验或我们正在得到的参数估计没有给我们准确的结果。然而，如果我们有一个大样本量(> 30)，那么 CLT 允许我们使用测试和估计，即使数据不是正态分布。

另一方面，根据大数定律，随着实验中试验次数的增加，试验的平均结果将最终接近真实的总体平均值。例如，当我们投掷一枚公平的硬币 1000 次时，我们更有可能看到一半的时间正面朝上，而相比之下，同样的硬币只投掷 10 次。

**12。最大似然估计(MLE)**

最大似然估计涉及通过最大化似然函数来估计参数，以找到最好地解释观察数据的参数。MLE 就是这样一种预测性建模框架，其中模型参数是通过优化问题找到的。这里，似然函数 p(y| θ)描述了给定参数θ时观察数据 y 的似然性。我们通过最大化给我们最大似然值的参数集(θ)来解决优化问题。最大似然估计在处理大数据量时效果很好，并给出了无偏最小方差估计量。

**尾注:**

感谢阅读！

对统计学有非常深刻的理解是成为成功的数据科学家的先决条件。我希望这篇文章能帮助你准备下一次数据科学面试。我在这里讨论的 12 个概念将为你建立进一步学习的重点领域。