# 我们必须停止人工智能情感识别

> 原文：<https://towardsdatascience.com/we-have-to-stop-doing-ai-emotion-recognition-ca5ed159370?source=collection_archive---------29----------------------->

## 人工智能

## 对这个价值数十亿美元的行业意味着什么？

![](img/3d384e7d3da271591a891ba3d223f596.png)

照片由[悉尼·西姆斯](https://unsplash.com/@fairytailphotography?utm_source=medium&utm_medium=referral)在 [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral) 上拍摄

情感识别是人工智能的一个分支，旨在识别人脸的情感。在过去的十年里，学术界和工业界对它的兴趣越来越大，预计到 2025 年，这个市场将增长到[850 亿美元。它有几种应用，其中大多数至少在伦理上是有问题的。它允许雇主通过对潜在雇员的同情心、情商和其他特质进行评分来评估他们。它可以帮助教师远程监控学生在学校的参与度，或者当他们在家做作业的时候。它用于](https://www.marketresearchengine.com/reportdetails/emotion-detection-and-recognition-edr-market)[识别“危险人物](https://www.article19.org/wp-content/uploads/2021/01/ER-Tech-China-Report.pdf)”它已经被用来控制美国与墨西哥的边境。

科技巨头通常是潜在有利可图技术的早期采用者，他们很快理解了这种系统的价值，并已经部署；情感识别软件，他们提供了大量的数据。[2016 年推出的亚马逊 Rekognition](https://docs.aws.amazon.com/rekognition/latest/dg/faces.html) ，可以从图像或视频中的人脸识别基本情绪。同年，微软宣布了[人脸 API](https://azure.microsoft.com/en-au/services/cognitive-services/face/#demo) ，这是一种可以检测“愤怒、轻蔑、厌恶、恐惧、快乐、中性、悲伤和惊讶”的算法他们进一步声称，“这些情绪被认为是跨文化的，普遍通过特定的面部表情进行交流。”

除了成熟的公司之外，建立情绪识别系统的动力如此强大，以至于催生了一大批创业公司。总部位于南约旦的 HireVue “将应聘者的语调[…]和细微的面部表情与之前被认为表现出色的人进行比较。Eyeris 的目标是利用面部和情感识别让汽车“比现在安全得多”。苹果于 2016 年收购的[emotient，实时扫描消费者的面部，以评估他们对广告和产品的情绪反应。](https://www.wsj.com/articles/apple-buys-artificial-intelligence-startup-emotient-1452188715)

处于初创行业前沿的是 [Affectiva](https://www.affectiva.com/) ，一家 2009 年从麻省理工学院媒体实验室崛起的初创公司。然而，与其他人相比，他们一直致力于解决一些与从面部信息中提取情绪状态的困难相关的问题。Affectiva 的首席执行官 Rana el Kaliouby 说,[他们正试图结合“文化特定基准”来解决情境和社会因素影响情感表达普遍性的情况。](https://www.washingtonpost.com/business/2019/07/31/emotion-detection-ai-is-billion-industry-new-research-says-it-cant-do-what-it-claims/)

Affectiva 的做法背后的原因是**情感识别技术不起作用**。至少不是所有这些公司声称的那样。这项技术是基于[保罗·艾克曼的基本情绪理论](https://onlinelibrary.wiley.com/doi/10.1002/0470013494.ch3)，用[泰勒特尔福德](https://www.washingtonpost.com/business/2019/07/31/emotion-detection-ai-is-billion-industry-new-research-says-it-cant-do-what-it-claims/)的话说，“六种情绪——快乐、悲伤、厌恶、恐惧、愤怒和惊讶——在所有文化中都由通用的面部表情所代表。”埃克曼辩护说，我们可以从面部表情中可靠地推断出情绪状态，对每个人都一视同仁。然而，这种观念一次又一次地受到挑战，现在人们普遍认为证据是不确定的。情感识别技术建立在不可靠的科学基础上。

但让我们回过头来看看近几十年来这场争论是如何展开的，科学界的最新结论是什么，这对这个数十亿美元的行业意味着什么。

加州大学旧金山分校的心理学家保罗·艾克曼将他的情绪研究方法建立在普林斯顿心理学家西尔万·汤姆金斯的工作基础上。汤姆金斯认为，正如凯特·克劳福德所说，“情感是一套天生的进化反应。”但即使是“情感研究之父”何也认识到，情感的具体表现取决于“个人、社会和文化因素”他承认“面部语言”并不是在所有地方都以同样的方式说的。

埃克曼想要建立一个可以克服这些问题的普适理论。他认为，如果情感表达是一种进化优势，那么它应该在所有社会和文化中都是普遍的。在过去的 50 年里，借助计算能力的进步和可用人脸数据集的急剧增长，他一直在改进他的方法。在 21 世纪的第一个十年里，他的理论被广泛接受，他的影响在学者和业界都有所扩大。

然而，他的方法论[在有效性问题上受到了批评](https://pubmed.ncbi.nlm.nih.gov/8202574/)。科学历史学家 Ruth Leys [认为【Ekman 的方法存在固有的缺陷，因为他使用了在受控环境中提取的数据，在这些环境中，情绪被人为地展示出来，忽略了个体和背景的差异。莱斯声称，他使用的面部图像“已经摆脱了文化的影响。”](https://press.uchicago.edu/ucp/books/book/chicago/A/bo26773949.html)

对埃克曼工作最重要的批评来自心理学家丽莎·费尔德曼·巴雷特及其同事在 2019 年发表的一篇综述[。这篇论文的主要目的是评估科学证据是否“足够强和足够清楚”来证明我们可以从面部运动推断情绪状态的想法。他们分析了有关埃克曼观点的文献——也称为*普遍观点——*，即情绪状态与面部运动明确相关。](https://journals.sagepub.com/doi/10.1177/1529100619832930)

他们根据三个标准评估了情绪产生(当感受到特定情绪时做出面部运动)和情绪感知(从面部运动推断出情绪)的共同观点:可靠性(例如，愤怒是否在大多数时间产生皱眉)、特异性(皱眉是否仅由愤怒产生)和概括能力(愤怒-皱眉关系是否也发生在现实世界场景中以及在非常不同的社区中)。他们考虑了广泛的人群:美国的成年人、偏远农村社区的成年人、儿童和先天失明的人。

巴雷特的总体结论是明确的:**“我们不可能自信地从微笑中推断出快乐，从皱眉中推断出愤怒，或者从皱眉中推断出悲伤，而当前的许多技术试图在应用被错误地认为是科学事实的东西时做到这一点。”**他们发现了有限的可靠性(情绪并不总是产生相同的面部动作)，缺乏特异性(情绪-面部动作对并不呈现唯一的映射)，以及有限的普遍性(文化和背景因素尚未得到充分的记录)。

巴雷特认为，科学家进行研究的方式“可能无意中”误导了情感研究的消费者——科技公司和其他研究人员——认为情绪和面部运动之间存在完美的映射。根据这些结论，我们可以说两件事:**埃克曼的理论充其量是部分错误的，学术界和工业界可能都没有很好地意识到它的错误程度。**

巴雷特的发现引发了一个新的问题:有了这个证据，数百万的情绪识别行业还会将那些并不像他们宣称的那样有效的产品商业化吗？从某种程度上来说，情感研究的最新进展已经让这个行业的主要参与者重新思考他们的情感识别方法。HireVue [在今年一月宣布](https://www.hirevue.com/blog/hiring/industry-leadership-new-audit-results-and-decision-on-visual-analysis)他们将停止在他们的算法中使用视觉分析，并表示希望“这一决定成为行业标准。”微软 Face API web 现在显示了一个不同的描述:“需要注意的是，面部表情本身并不能代表人的内部状态。”他们已经取消了跨文化情感表达普遍性的说法。许多公司似乎已经改变了他们的道路，走上了 Affectiva 几年前就已经在铺设的道路。

然而，尽管最新的研究揭示了情绪识别技术的一些关键问题，但我们应该问自己最后一个问题:**即使有一天这些技术完全没有偏见，我们是否希望政府和公司有能力随时随地了解我们内心的情绪状态？**黛博拉·拉吉(Deborah Raji)是揭示亚马逊面部识别系统中性别和种族偏见的一项重要研究[的合著者，](https://www.nytimes.com/2019/01/24/technology/amazon-facial-technology-study.html)[说](https://www.technologyreview.com/2020/06/12/1003482/amazon-stopped-selling-police-face-recognition-fight/)即使这项技术工作完美，它也可能“很容易被用来对社区进行武器化的骚扰。”

面部和情感识别技术中偏见的暴露让位于一场更重要的辩论。目前，科技巨头[已经停止](https://www.technologyreview.com/2020/06/12/1003482/amazon-stopped-selling-police-face-recognition-fight/)向政府出售他们的产品，但是监管[还没有在美国](https://iapp.org/news/a/u-s-facial-recognition-roundup/)实施。即使偏见消失了，这些系统仍然可能以我们只能想象的方式危及我们的隐私。亚历克莎·哈格蒂和亚历山德拉·艾伯特[说得好:“当技术没有正常工作时，它们可能是危险的。当它们在一个不完美的世界中完美地工作时，它们也可能是危险的。”](https://theconversation.com/ai-is-increasingly-being-used-to-identify-emotions-heres-whats-at-stake-158809)