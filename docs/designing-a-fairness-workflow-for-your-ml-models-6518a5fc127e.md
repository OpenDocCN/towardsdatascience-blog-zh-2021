# 为你的 ML 模型设计一个公平的工作流程

> 原文：<https://towardsdatascience.com/designing-a-fairness-workflow-for-your-ml-models-6518a5fc127e?source=collection_archive---------20----------------------->

## [公平与偏见](https://towardsdatascience.com/tagged/fairness-and-bias)，值得信赖的 AI

## 你如何确保你的模型从头到尾都是公平的？

*与拉塞尔·霍尔茨合著。*

![](img/7a3c7fd23e582d4911e846fe7cfebb6f.png)

照片由 [Gio Bartlett](https://unsplash.com/@giobartlett?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText) 在 [Unsplash](https://unsplash.com/s/photos/pathway-steps?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText) 上拍摄

在本系列的第一篇博文[中，我们讨论了创建全面公平工作流以确保机器学习模型结果公平的三个关键点。它们是:](/what-does-it-mean-to-be-fair-measuring-and-understanding-fairness-4ab873245c4c)

*   **识别偏差**(两个或更多组的不同结果)
*   **执行根本原因分析**以确定差异是否合理，以及
*   采用**有针对性的缓解策略**。

然后，在本系列的第二篇文章中，我们深入研究了一个模型如何变得有偏差。

但这些如何将公平的意图转化为公平的机器学习模型，并随着时间的推移继续保持公平？

数据科学家可以创建确保持续公平的工作流。在此，我们强调一些对实现公平工作流程至关重要的方面:

*   通过阐明工作流程的独特之处，使公平具有目的性，避免滥用公平这一概念。
*   支持根本原因分析，挖掘任何偏差的来源以及它们是否合理。
*   定期向相关利益相关方报告，重点介绍一段时间内的关键分析和结果

对于以下工作流程步骤，我们将参考 [TruEra 诊断解决方案](https://truera.com/diagnostics/)，并使用该解决方案的屏幕截图来说明如何创建工作流程。然而，这些步骤只是一个一般性的解释——让我们使用现有的工具重新设想一个公平性分析工作流程。

![](img/241880e8dbd0452c5dc896c8ea0b8b7b.png)

图片作者。公平工作流涉及到模型生命周期中每一步的仔细考虑。

# 有意实现公平

公平分析不被误用是至关重要的。比较公平性时考虑两个任意段是没有意义的；该模型可能完全有理由偏爱某一部分。例如，信贷审批环境中的“高负债”部分与“低负债”部分是可变处理的合理示例。缺乏意图会导致不加选择地使用公平工具包，而不考虑为什么和如何使用。

相反，确定某个细分市场是否受到法律或监管标准的“保护”至关重要。为了解决这个问题，在开始公平工作流之前，明确地将数据段标记为受保护是很重要的。在 TruEra Diagnostics 中，段管理页面允许用户这样做。尽管仍然允许对任意段进行特别的公平性分析，但在评估和报告流程中会突出显示关于如何将段设置为受保护的明确警告和指导。

![](img/c726de23a370a6e31b82b94a00059ed0.png)

*作者图片。预先设置受保护的段使得工作流是有意的。*

如果您使用管理公平性的解决方案，用户可以集中精力持续监控受保护数据段的公平性指标。理想情况下，您将能够快速查看导致任何差异来源的特征，如下图所示。

如果您不使用解决方案，而是设置自己的监控，您将需要定期运行受保护细分市场的分析，以确保模型输出随着时间的推移保持公平。审查的频率和一致性很重要，这样可以快速识别和解决任何偏差。今天，生产模型的公平性审查经常不进行，或者进行得如此之少，以至于增加了不良结果的风险和组织的业务风险。

![](img/1b834ce14a6f855cc34e19fe21408c24.png)

*作者图片。一旦设置了受保护的数据段，就可以持续监控公平性，从而节省了使用任意审查窗口创建临时审查的时间。*

# 根本原因分析—快速缓解的关键

一旦差异被识别，识别差异的特征级贡献者的根本原因分析可以帮助模型开发人员和验证人员了解偏差是如何进入 ML 模型的，我们在之前的 TDS 博客文章[中讨论过这个问题，“我的 ML 模型是如何变得不公平的？”](/how-did-my-machine-learning-model-become-unfair-c6508a795989)检查关键影响特征有助于确定偏差是否合理，并指导缓解策略的选择。

让我们用一个具体的例子来说明这一点。假设我们正在一个类似于 UCI 的[成人数据](https://archive.ics.uci.edu/ml/datasets/adult)的数据集上训练一个模型。该模型使用人口普查数据来预测一个家庭的年收入是否大于 5 万美元。数据集包含一些人口统计特征，如性别和种族，但这些特征不会提供给模型，因为开发人员不希望模型使用受保护的属性来进行预测。开发人员决定不同的影响比率(被分配积极结果的男女比例之间的比率)是用于该问题的适当的公平性度量。

在训练该模型时，开发人员发现，与男性相比，女性在该模型中处于不利地位，因此给予她们较低的分数:

![](img/5e286dbaf5e983edbaa8654fb5345925.png)![](img/af46c7f79e41072c21d5b80019c3fa18.png)

*作者图片。根本原因分析的第一步—发现对受保护组的不同影响。*

开发人员对为什么会出现这种情况感到困惑，并执行根本原因分析来挖掘造成差异的功能级别的因素。本质上，这项分析着眼于男性比女性获得更高模型分数的核心驱动因素。很明显,“婚姻状况”是造成男女不平等的最主要因素:

![](img/305167eb8bfb2aea180ec539c14c664f.png)

*作者图片。深入挖掘，找到差距的驱动因素:婚姻状况。*

开发人员进一步探究了“婚姻状况”，并注意到在测试数据中，女性报告自己婚姻状况为“单身”的比例远高于男性，尽管现实世界中男女结婚率大致相等。然后，开发人员用模型验证器确认这种数据异常是意外的，并对数据进行重新采样，使得男性和女性以大致相同的比率报告“单身”的婚姻状态。他们的缓解策略不必对关键影响因素的知识视而不见，使用重采样方法，使用完全不同的影响比率指标，与男性相比，女性仅略微处于不利地位:

![](img/cf9cdfafcaaca7f07cc9b64ad4c6ee8d.png)

*图片作者。在测试数据问题被识别并且数据被重新采样之后，差异的影响被减轻。*

# “人在回路”的合理性和重要性

任何公平工作流程的一个关键组成部分是**合理性**。我们发现，受保护的特征，如性别，有时与收入等特征相关，合理的模型可以也应该使用这些特征。在这些情况下，模型应该*而不是*立即被宣布为不公平。它可能是以一种敏感的方式使用收入特征，而没有创造一个性别的代理。一个人需要参与进来，因为专家可以理清模型的行为是否合理。

![](img/b3d1eb7421d703c4091e2c826835e844.png)

*图片作者。*

为了捕捉这种合理行为的想法，模型验证者或数据科学家能够“证明”模型偏差是很重要的。在这个工作流程中，人可以进入循环，系统地检查偏差，并确定模型是否仍然适合生产。我们建议任何模型的公平报告都包含这种专家分析的明确部分，这为评论任何偏见的合理性提供了空间。

![](img/0d8ab0d6c55e92127e4ee6124be9edad.png)

*作者图片。举例说明专家如何对公平性评估进行审查，以确定偏见是否合理。*

# 定期报告:满足利益相关者和合规性要求

多个组织利益相关者通常对机器学习模型的公平性感兴趣。由于潜在公平性问题的重要性和敏感性，报告和传达模型的公平性至关重要。诊断报告应该使数据科学家和模型验证者能够轻松地向同行和其他业务利益相关者共享和呈现报告。这些报告应该代表完整工作流程的单一快照，并允许用户理解和剖析公平分析的组成部分。以下是来自 TruEra 的示例报告，但是任何组织都可以使用即席监控分析、用于可视化的集成商业智能解决方案以及使用幻灯片或文字处理解决方案的模板的组合来创建自己的报告。

与公平监控分析本身类似，报告应该定期进行，特别是在那些有法规遵从性要求的行业，如金融服务。

![](img/b4cd8c2672d4a349f79add04531a1437.png)

*作者图片。公平性报告允许数据科学家、模型开发人员和业务利益相关者轻松地就模型的公平性进行交流。*

# 语言和视觉化:细节决定成败

您可能已经注意到，我们选择了术语“公平工作流”，而不是“偏见工作流”这是故意的。我们的团队投入了相当多的时间来讨论关于公平工作流的语言和视觉元素的微妙问题。公平的抽象本质重视与用户的清晰交流。

由于这个原因，工作流程被重新命名为公平而不是偏见。“公平”从根本上说是确保算法符合社会标准；由于机器学习中的术语超载,“偏差”导致了混乱。此外，“公平”隐含地传达了工作流程的积极意图。

# 把所有的放在一起

本系列涵盖了很多领域，并总结了一个完整的公平工作流程。我们讨论了公平的理论、实践和社会基础。我们还认为，完整的公平工作流程将识别偏差，执行根本原因分析以确定偏差是否合理，然后采用有针对性的缓解策略。我们还后退一步，揭示了 AI/ML 模型最初是如何变得不公平的。

然后，为了让它变得真实，我们将这些概念与以人为中心的设计联系起来。工作流应该是有目的的，并且在自动化过程的同时，为人留有余地。一个清晰的、自动化的工作流程将会帮助任何与 ML 模型交互的开发人员、验证人员或者商业利益相关者。同样重要的是要指出，虽然这些工作流存在解决方案，但随着时间、努力和意图，它们也可以从现有的工具中创建并整合到现有的流程中。随着人工智能成为我们生活中更重要的一部分，确保模型的开发考虑到公平并受到监控以确保持续的公平是企业和社会的一个巨大优先事项。