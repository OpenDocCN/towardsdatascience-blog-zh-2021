# 医学中的机器学习——第六部分

> 原文：<https://towardsdatascience.com/machine-learning-in-medicine-part-vi-5470b7f634b7?source=collection_archive---------29----------------------->

## 针对医生和医疗保健专业人员的机器学习技术实践入门课程

# 人工神经网络

![](img/28ac832a70f86e4bf65b5da029c1ff84.png)

[哈尔·盖特伍德在 Unsplash 上拍摄的照片](https://unsplash.com/photos/OgvqXGL7XO4)

# 概述

在本课程的 [第五部分](/machine-learning-in-medicine-part-v-10231d5511e5)中，我们探讨了两种集成决策树算法，随机森林/额外树和梯度推进机。我们首先讨论了单一决策树模型的优势，以及更重要的局限性。我们引入了装袋和子空间的概念，作为最小化过度拟合的手段。我们演示了使用服装工具包构建随机森林/额外树木模型以及渐变加速机器的步骤。我们还研究了这两种建模技术的各种优化技术。最后，我们讨论了计算特征重要性的概念和算法。

集成决策树模型相对于 kNN 和逻辑/线性回归的一个主要优势是它们支持任意决策边界的能力，但是决策树中的每个节点一次只能对一个特征进行操作。人工神经网络代表了能够实现高级抽象的进一步改进。

# 礼服套件更新

在继续本课程的其余部分之前，请花点时间更新[服装套件](https://github.com/waihongchung/dress)以发布 **1.3.0** 。此版本包含对神经网络算法的重大功能改进和更改，以及新数据转换算法的添加。

# 模型概述

人工神经网络的工作原理是假设存在一个可计算的函数，将预测值映射到结果。顾名思义，人工神经网络是受生物神经网络的启发。每个人工神经元是人工神经网络的基本操作单元，是生物神经元的功能抽象。它接受来自网络中上游神经元的一个或多个输入，就像生物神经元通过其树突接受来自上游神经元的信号一样。在生物神经元中，如果膜电位达到某个去极化阈值，就会触发一个动作电位，并沿着轴突传递到下游神经元。在人工神经元中，输入由激活函数(加上一些其他数学运算)处理，输出被传输到下游神经元。

![](img/267c34519a52797144f2d1eb718d9792.png)

神经元。图片作者。

我想到的第一个问题是，一个相互连接的人工神经元网络是如何学习任何东西的。关键在于上述的数学运算。每个输入乘以两个神经元之间连接固有的权重因子，并且每个神经元固有的另一个因子(称为偏差)在最终值被激活函数处理之前被添加到所有加权输入的总和。神经网络可以通过调整这些权重和偏差来“学习”新信息。

需要注意的是，人工神经网络只是一个统称。根据人工神经元之间的连接方式，可以创建各种机器学习模型。最基本和最常用的人工神经网络被称为多层感知器。

![](img/90bf5ab2330cf15332910065348ff125.png)

多层感知器。图片作者。

在多层感知器中，神经元排列在离散的层中。同一层中的所有神经元共享相同的激活函数。特定层中的每个神经元都与下一层中的所有神经元相连。要创建一个多层感知器，首先必须确定层数、每层中神经元的数量以及每层中使用的激活函数。网络中的权重和偏差首先被初始化为一些随机值。然后将训练样本按顺序或分批输入到模型中，并将模型的输出与训练数据集中的真实值进行比较。最初，模型的输出会非常不准确，但是误差通过模型反向传播到网络中的每一层神经元。调整与每个连接相关联的权重和与每个神经元相关联的偏置，以便减少整个网络的误差。通过多次重复这些正向传播和反向传播步骤，多层感知器可以慢慢地学习预测值和结果值之间的关系。

![](img/e463b7c9835738204f877552b5c97726.png)

向前和向后传播。图片作者。

反向传播过程背后的详细数学相当复杂，涉及求解一系列偏导数。可以说，这些导数函数的结果允许权重和偏置以与每个神经元的输出误差相反的方向和成比例地被调整。

# 优势和局限性

一般来说，多层感知器和人工神经网络的最大优势是它支持高级抽象的能力。事实上，人工神经网络被认为是[通用函数逼近器](https://en.wikipedia.org/wiki/Universal_approximation_theorem)，这意味着只要有足够数量的相互连接的神经元和层，它们就可以逼近几乎任何已知的函数。换句话说，理论上，人工神经网络可以模拟预测者和结果之间的任何关系，即使这种关系本质上是随机的。

多层感知器的另一个优势是它能够同时模拟多个结果，方法是将网络的输出层设置为所需的结果数，并指定适当的损失函数。如果我们需要建立一个模型来预测几个不同但多少有些关联的结果，这可能会非常方便。例如，我们可以建立一个模型来预测自发性细菌性腹膜炎发作后 30 天的死亡率和 1 年的全因死亡率。

这种逼近预测者和结果之间任何关系的能力伴随着几个重要的权衡。首先，训练一个多层感知器计算量很大。每次向前和向后传播都涉及多个矩阵计算。与集成决策树模型相比，通常需要大得多的训练数据集来训练人工神经网络。训练过程通常包括重复整个训练数据集数百或数千次。

其次，众所周知，人工神经网络很难优化。必须考虑神经元的层数、每层中神经元的数量、每层中激活函数的选择、权重/偏差初始化的方法、学习速率、退出率、训练周期的数量、损失函数以及一系列其他高级优化参数。给定上述长的训练时间，通常很难系统地测试超参数的所有排列。

第三，人工神经网络起到了黑匣子的作用。通过检查这些权重和偏差的值来解释神经网络如何以及为什么做出某个决定实际上是不可能的。事实上，在相同数据集上训练的具有相同结构并且具有相似性能的两个神经网络可能具有非常不同的权重和偏差集。此外，因为每个神经元都连接到下游层的所有神经元，所以预测器都混合在网络的更深层。不可能准确地确定一个特定的预测器如何影响网络的结果。

第四，人工神经网络只对数值型预测因子起作用，需要大量的数据准备(将不同预测因子的值归一化，并转换其分布)，才能达到良好的效果。

# 履行

让我们开始建立我们的多层感知器。正如我们前面提到的，训练一个神经网络在计算上是很昂贵的，所以为了避免脚本超时错误，最好异步进行。

该脚本与我们用来创建随机森林/额外树木模型或梯度推进机器的脚本略有不同。让我们详细了解一下这些变化。我们不直接调用`DRESS.multilayerPerceptron`函数，而是调用`DRESS.async`，这是 DRESS Kit 中的一个实用函数，支持异步函数调用。第一个参数是我们希望异步运行的 DRESS Kit 函数的名称。需要注意的是，我们不是将实际的函数作为参数传递，而是将函数的名称作为字符串传递。后续参数的传递方式与我们同步调用`DRESS.multilayerPerceptron`的方式相同。`DRESS.async`函数返回一个 JavaScript [承诺](https://developer.mozilla.org/en-US/docs/web/javascript/reference/global_objects/promise)，一旦函数完成，它将最终解析为一个结果对象。这里有必要回顾一下异步编程的基本概念。为了对结果对象进行操作，我们需要调用与承诺对象相关联的`.then`函数，并传递一个[箭头函数](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Functions/Arrow_functions)作为参数。arrow 函数应该接收结果对象作为参数。

由于 JavaScript 固有的局限性，Promise 对象解析的结果对象不包含任何可执行函数。在我们可以调用`model.predict`函数或另一个与模型相关的函数之前，我们必须重建模型。幸运的是，可以通过将结果对象再次同步传递到`DRESS.multilayerPerceptron`中来轻松高效地完成。我们将整个 Promise 对象和`then`函数一起传递给`DRESS.print`，它将自动显示一个计时器来帮助我们跟踪异步函数调用的进度。我们从`then`函数中返回预测结果，这样它也可以由`DRESS.print`函数显示。

我们可以通过将结果代入`state`并将分类标志代入`true`来构建分类模型，但重要的是要注意，分类模型的训练时间要长得多，因为构建所有 50 个状态的模型所需的神经元数量要大很多倍。

# 在线培训

在人工神经网络的训练阶段，应用每个训练样本来更新网络的权重和偏差，并且之后不再需要。因此，人工神经网络的训练过程可以顺序进行。我们可以用一个小数据集(甚至只有一个样本)来训练模型，测试模型的性能，并在更多的训练样本变得可用和必要时应用它们，而不是一次性应用整个数据集。

我们可以通过将训练数据集分成多个批次并一次应用一个批次来模拟这种在线训练行为。首先，我们通过调用`DRESS.multilayerPerceptron`使用第一批训练样本构建一个多层感知器，然后使用`model.performance`测试性能，并使用`model.train`应用额外的训练样本。我们可以从打印输出中看到，模型的性能在最初几批中波动很大，但在训练过程结束时会有所改善和收敛。

值得注意的是，为了使用这种在线训练方法建立分类模型，我们必须确保第一批训练样本包含所有可能的结果值，因为神经网络的输出层是在模型首次由`DRESS.multilayerPerceptron`函数建立时设置的。通过`model.train`应用额外的训练样本不会改变神经网络的布局。

# 最佳化

优化人工神经网络可能相当具有挑战性。我们将从评估模型的基线性能开始。请注意，交叉验证时间相当长，因为算法需要建立和训练五个独立的模型。

在我们详细研究每个超参数之前，重要的是要强调一些优化技术可能是特定于实现的，这意味着根据底层代码，某些优化可能比其他优化具有更高的产量，而某些超参数可能由于特定于实现的限制而不可用。例如，多层感知器的服装工具包实现只支持[亚当优化器](https://optimization.cbe.cornell.edu/index.php?title=Adam)，但是还有其他几个优化器，比如 [RMSProp](https://optimization.cbe.cornell.edu/index.php?title=RMSProp) 和 [AdaGrad](https://en.wikipedia.org/wiki/Stochastic_gradient_descent#Extensions_and_variants) 。

## 网络布置

设计神经网络时，我们需要考虑的第一件事是神经网络的结构。通常，多层感知器的输入层被设置为与模型中预测器的数量相同的大小，而输出层被设置为回归模型中预测器的数量或分类模型中不同类别的数量。隐藏层(输入和输出之间的层)中的神经元数量基本上没有限制。以一种过于简化的方式，我们可以将隐藏层中的每个神经元视为控制输入信号如何流向输出层的决策单元。通过增加每层神经元的数量，理论上，网络可以处理预测者和结果之间更复杂的关系。当然，拥有大量神经元的缺点是训练时间更长。拥有太多的神经元也会导致过度拟合，尽管由于其他的[正则化技术](https://en.wikipedia.org/wiki/Regularization_%28mathematics%29)，这对于多层感知器来说一般不是问题。

增加多层感知器的层数也会增加网络处理复杂问题的能力，但方式不同。由于隐藏层中的每个神经元都完全连接到相邻层中的所有神经元，因此更深层中的神经元同时基于多个预测器的组合来做出决策。因此，添加层使网络能够达到更高的抽象级别。然而，重要的是要注意，增加额外的层会成倍地增加训练时间。(即，具有一个大小为 10 的隐藏层和另一个大小为 20 的隐藏层的网络具有 200 个连接，而具有三个大小为 10 的隐藏层的网络具有 1000 个连接)。一般来说，具有 2-3 个隐藏层的多层感知器对于大多数应用来说已经足够了。

默认情况下，多层感知器的 DRESS Kit 实现有两个隐藏层，该算法使用简单的启发式算法来确定每层神经元的适当数量。我们可以通过指定`layout`超参数来改变模型的结构。以下是我们尝试过的几种不同布局的结果。我们可以看到，基于 5 重交叉验证，网络的[决定系数](https://en.wikipedia.org/wiki/Coefficient_of_determination) (R2)没有显著变化，尽管从[5，3](意味着 5 个神经元的隐藏层后接 3 个神经元的隐藏层)到[20，10](20 个神经元的隐藏层后接 10 个神经元的隐藏层)增加了大量神经元。与此同时，训练时间几乎翻了两番。有趣的是，我们还看到，增加额外的层实际上会降低性能，并成倍增加训练时间。

```
[5] R2 0.94 
[7] R2 0.95 
[10] R2 0.96 
[15] R2 0.96 
[3, 3] R2 0.93 
[5, 3] R2 0.94 
[7, 5] R2 0.94 
[10, 5] R2 0.95 
[10, 10] R2 0.96 
[20, 10] R2 0.96 
[7, 5, 3] R2 0.95 
[10, 7, 5] R2 0.94 
[7, 5, 5, 3] R2 0.93
```

## 激活功能

设计神经网络时要考虑的下一个超参数是使用的激活函数。激活函数的主要目的是给网络引入非线性。有各种各样的[激活功能](https://en.wikipedia.org/wiki/Activation_function)，每种功能都有其独特的操作特性。好消息是，泄漏整流线性单元(Leaky ReLU)已被证明在大多数情况下比其他激活功能提供更好的性能。除非有非常具体的理由考虑另一个激活函数，否则在多层感知器的所有隐藏层中使用 Leaky ReLU 通常是安全的。

## 世

在训练过程中，有多种方法可以将训练数据集应用于网络。在传统的梯度下降算法中，整个数据集一次性应用于网络，平均损失反向传播以更新所有神经元的权重和偏差。这种方法需要大量的内存和计算能力。另一种被称为[随机梯度下降](https://en.wikipedia.org/wiki/Stochastic_gradient_descent)的方法是一次对一个样本进行前向和后向传播。还有批量梯度下降，其中样品以小批量应用。无论如何应用训练数据集，仅通过网络应用一次是不够的。为了优化所有神经元的权重和偏差，需要许多周期或时期。显然，如果纪元设置得太高，训练过程将会非常慢，并且还会增加过度拟合的风险。

```
25 R2 0.92 
50 R2 0.93 
100 R2 0.94 
250 R2 0.95 
500 R2 0.96
```

## 稀释

有许多用于减少人工神经网络中过拟合的正则化技术，其中之一是[稀释](https://en.wikipedia.org/wiki/Dilution_(neural_networks))。稀释的概念有点类似于集合决策树模型中子空间的概念。它包括在训练过程中随机关闭一小组神经元，这样这些神经元的权重就不会对输出做出贡献。最终结果是，在每个周期中，每个神经元仅在整个训练数据集的一部分上被训练，并且那些在特定周期中没有被关闭的神经元被迫学习额外的信息。稀释的一个意想不到的好处是，通过关闭某些神经元，训练时间也略有减少，因为不需要对这些神经元进行反向传播。

```
0.10 R2 0.94 
0.25 R2 0.95 
0.50 R2 0.96 
0.75 R2 0.95 
0.90 R2 0.94
```

## 学习率

大多数神经网络实现支持学习率超参数，有时称为`alpha`。这个想法类似于集合决策树模型。它限制了每次反向传播过程中权重和偏差的变化量。这最小化了异常值的影响，并防止那些权重和偏差的值过度波动(并导致一些神经元在 ReLU 激活的情况下被关闭)。高学习率可能导致梯度下降算法在最佳点附近振荡，而不是收敛到最佳点，而低学习率可能需要高历元计数的补偿(因此需要更长的训练时间)。

```
0.0005 R2 0.95 
0.001 R2 0.95 
0.005 R2 0.95 
0.01 R2 0.94 
0.05 R2 0.88
```

## 动力

如果学习速率很小，则标准随机梯度下降算法收敛到最优点会很慢。理想情况下，该算法应该在开始时以较高的学习速率开始，并在接近最佳点时逐渐降低速率。许多基于动量的优化算法使用这种技术来加速训练过程。DRESS Kit 实现使用的 Adam 优化器支持与动量相关的超参数`beta1`和`beta2`。这些超参数的默认值适用于大多数用例。

# 其他类型的人工神经网络

正如我们前面提到的，多层感知器只是许多人工神经网络类型中的一种。大多数多层感知器只需要几个隐藏层就能很好地工作。虽然没有正式的定义，但包含许多(例如几十个)隐藏层的多层感知器有时被称为深度神经网络。

除了建立分类和回归模型，人工神经网络还可以执行其他任务。Autoencoder 是一种人工神经网络，它执行无监督的学习任务，如降维和特征提取。卷积神经网络通常用于图像处理，因为它对特征之间的空间关系敏感。生成式对抗网络可用于生成具有与训练数据集相同的统计数据的新样本。

# 包裹

让我们复习一下第六部分所学的内容。我们讨论了人工神经网络的基本解剖，并解释了相互连接的人工神经元网络如何能够通过向前和向后传播来执行学习。我们演示了使用`DRESS.multilayerPerceptron`和`DRESS.async`异步构建多层感知器的代码。我们还演示了使用在线训练方法构建和训练多层感知器的代码。我们探讨了各种超参数，并讨论了全面优化人工神经网络的挑战。最后，我们谈到了除了分类和回归之外的人工神经网络的其他用例。

# 模型摘要

*多层感知器*

*强项*

*   支持任意决策边界
*   支持高级抽象
*   可以同时模拟多种结果
*   不易过度拟合

*限制*

*   仅适用于数值预测值
*   黑箱操作
*   训练缓慢
*   难以优化