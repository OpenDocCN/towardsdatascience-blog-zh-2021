# 人工智能监管的现状和未来

> 原文：<https://towardsdatascience.com/the-present-and-future-of-ai-regulation-afb889a562b7?source=collection_archive---------22----------------------->

## [播客](https://towardsdatascience.com/tagged/tds-podcast)

## 安东尼·哈巴耶布谈企业人工智能治理

[苹果](https://podcasts.apple.com/ca/podcast/towards-data-science/id1470952338?mt=2) | [谷歌](https://www.google.com/podcasts?feed=aHR0cHM6Ly9hbmNob3IuZm0vcy8zNmI0ODQ0L3BvZGNhc3QvcnNz) | [SPOTIFY](https://open.spotify.com/show/63diy2DtpHzQfeNVxAPZgU) | [其他](https://anchor.fm/towardsdatascience)

*编者按:TDS 播客由 Jeremie Harris 主持，他是数据科学导师初创公司 SharpestMinds 的联合创始人。每周，Jeremie 都会与该领域前沿的研究人员和商业领袖聊天，以解开围绕数据科学、机器学习和人工智能的最紧迫问题。*

人工智能的公司治理听起来不像是一个性感的话题，但对于依赖机器学习模型为客户提供价值的大公司来说，它正迅速成为最重要的挑战之一。人们越来越期望他们开发和实现治理策略，以减少偏见的发生，并增加他们的人工智能系统和开发过程的透明度。这些期望历来来自消费者，但政府也开始施加硬性要求。

因此，在今天的节目中，我采访了 Anthony Habayeb， [Monitaur](https://monitaur.ai/) 的创始人兼首席执行官，这是一家专注于帮助企业预测和遵守新的和即将到来的人工智能法规和治理要求的初创公司。在过去的几年里，安东尼一直在密切关注人工智能监管的世界，并非常友好地分享了他对该领域目前状况和未来方向的见解。

以下是我在对话中最喜欢的一些观点:

*   当谈到人工智能治理的最佳实践时，政府和公司不必从头开始想办法。已经有大量现有的监管和商业原则，大公司预计将应用于他们的运营，这些原则通常可以直接扩展到人工智能。例如，安东尼引用了 SOC 2，该认证要求公司证明他们拥有正确的人员、流程和技术控制，以降低关键系统被破坏(例如，数据泄露或网络攻击)的风险。围绕过程责任的规范，包括确保系统由不同的人构建和评估，也很容易推广到人工智能的情况。安东尼预计，在人工智能的背景下，这些类型的框架将对公司有用，可能还会得到政府的授权。
*   安东尼认为正在出现的一个最重要的趋势是，政府期望公司开发人工智能的方式会留下证据，证明开发人员做出某些设计选择的原因，这些选择会影响他们系统的公平和偏见。安东尼不指望政府为此提出自己的偏见和公平定义(出于许多原因，政府没有这方面的专业知识)，但他们希望公司能够证明，他们已经投入了一定的努力，用道德考量来换取绩效。
*   所有的人工智能系统都有偏见。事实上，人工智能系统必须有偏见才能有用:根据定义，有用的模型是一种倾向于以不受随机机会支配的方式将某些输入与某些输出相关联的模型。真正的问题是我们将允许什么样的偏见——虽然这看起来像是吹毛求疵，但实际上最有趣的伦理问题存在于“我们想要的偏见”和“我们不想要的偏见”之间的模糊界限中。
*   当算法中存在不必要的偏差时，很自然地得出结论，该算法不应该被部署。但重要的是要考虑账本的两边——如果我们决定不部署算法，我们可能会保留什么价值。这一价值可能很大，甚至对那些算法偏差和故障模式错误的人来说，这也是值得权衡的。

你可以点击这里在 LinkedIn 上关注[安东尼，或者点击这里](https://www.linkedin.com/in/anthonyhabayeb/)在 Twitter 上关注[我。](https://twitter.com/jeremiecharris)

![](img/bc3c5dbb991cb0b6a582224e8153593b.png)

## 章节:

*   0:00 介绍
*   1:45 安东尼的背景
*   围绕监管的 6:20 理念
*   14:50 政府的作用
*   17:30 理解公平
*   25:35 艾的公关问题
*   35:20 政府监管
*   42:25 对数据科学团队有用的技术
*   46:10 人工智能治理的未来
*   49:20 总结