# 检测和消除机器学习模型中偏差的 5 种工具

> 原文：<https://towardsdatascience.com/5-tools-to-detect-and-eliminate-bias-in-your-machine-learning-models-fb6c7b28b4f1?source=collection_archive---------6----------------------->

## [公平和偏见](https://towardsdatascience.com/tagged/fairness-and-bias)

## 你不需要独自承担重担。

![](img/f2949bb1aa981b1e20d7a77b833f28f2.png)

[NeONBRAND](https://unsplash.com/@neonbrand?utm_source=medium&utm_medium=referral) 在 [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral) 上的照片

如果你曾经开发或研究过任何类型的机器学习算法，那么你一定在某个时候需要检查你的模型是否有偏差，并确保这种偏差被消除。一个有偏见的系统会导致不准确的结果，这可能会危及你的整个项目。

机器学习算法已经在各个应用领域证明了它们的价值，从医疗应用到自动驾驶汽车和天气预测。虽然机器学习有很多优势，但如果你的机器学习模型包含任何类型的偏见，你就无法利用它的全部潜力。

不同的来源可能导致机器学习模型中的偏差。您使用的数据可能已经有偏差，或者您选择的样本并不能代表整个样本库，或者您的算法可能不适合您的特定问题。不管是什么原因，检测你的机器学习算法中的偏差都不是一件容易的事情。

</5-types-of-machine-learning-bias-every-data-science-should-know-efab28041d3f>  

这就是为什么在过去几年中，检测机器学习模型中的偏差一直是许多研究人员关注的焦点。这项研究开发了一些工具，你可以用来检查你的机器学习模型是否有偏差。

本文将带您了解 5 种工具，它们可以帮助您在下一个机器学习模型中检测和减轻偏差。

# №1:假设

为了检查你的机器学习模型是否有偏见，你需要问许多问题，并在你的数据中测试不同的场景。例如，如果一个数据点发生变化，或者可能使用不同的数据样本来训练或测试模型，您将需要测试模型性能是否会发生变化。这些变化如何影响您的模型的最终结果？

你可能经历过，或者想象过，回答这样的问题根本不是一件容易的事情。这可能非常耗费时间和资源，因为您需要编写和测试不同的代码场景来检测偏差。幸运的是，在 2018 年，谷歌推出了一款名为*[*What-If*](https://pair-code.github.io/what-if-tool/)*的工具，来帮助这项任务变得更加容易。**

**What-If 是一个开源的交互工具，让每个人——甚至非程序员——都可以更容易地测试、探索和调试机器学习模型。假设分析为您提供了操作数据点、编辑数据点、生成图以及指定评估模型的标准的能力，所有这一切都使用一个清晰而简单的 GUI。**

**谷歌还提供了许多材料，你可以使用这些材料来了解如何避开假设，并有效地使用它。**

**</6-ways-to-improve-your-ml-model-accuracy-ec5c9599c436>  

# №2: AI 公平 360

假设帮助你检测你的机器学习模型中偏见的存在，但它没有给你一个直接的方法来消除这种偏见。消除模型中的偏差就像检测偏差的存在一样具有挑战性。

IBM 提供了[*AI Fairness 360*](https://aif360.mybluemix.net/)*，*一个开源的综合工具包，用于检测和消除机器学习模型中的偏差。AI Fairness 360 包括 70 多个公平指标，可以帮助您检测模型中的偏差，如曼哈顿和欧几里德。

该工具还包括超过 10 个算法，帮助您消除基础，如果你找到一个。这些偏见减轻算法包括优化预处理阶段，偏见消除器，以及定期和更多。此外，您可以使用这个工具来开发和构建您自己的度量和缓解算法。

IBM 还提供了大量的[教程和材料](https://aif360.mybluemix.net/resources#tutorials)，你可以用它们来学习如何使用 AI Fairness 360。最后，他们提出了一个针对偏差的[评级系统，可用于验证任何机器学习应用结果。](https://arxiv.org/pdf/1808.00089.pdf)

</6-web-scraping-tools-that-make-collecting-data-a-breeze-457c44e4411d>  

# №3:众包

假设分析和人工智能公平 360 是通用工具，可用于检测和减轻任何机器学习模型中的偏差。但是，有时，您需要针对特定应用的工具。微软和马里兰大学的研究人员使用众包来精确检测自然语言处理应用程序中的偏见。

众包是一个用来描述让人们——群众——参与创新、解决问题或提高效率的实践的术语。使用众包可以用来研究问题的不同类别，以确定偏见的潜在原因。

使用众包来检测机器学习应用中的偏见是受隐式关联测试(IAT)的启发。公司和研究人员经常使用 IAT 来衡量和检测人类偏见。使用众包的主要目的是从任何机器学习应用的第一步(也是最重要的一步)中消除偏见，这一步是数据收集和清理，或者通常所说的数据预处理。

</10-nlp-terms-every-data-scientist-should-know-43d3291643c0>  

# №4:局部可解释的模型不可知解释(LIME)

机器学习应用在我们身边随处可见。这些应用程序给我们的预测是我们应该 100%相信的。有时这些应用非常关键，例如使用机器学习来诊断疾病，或者用于自动驾驶汽车。这些预测中的任何错误都可能导致致命的结果。

如果你的模型给出了错误的或有缺陷的结果，在你着手解决问题之前，你需要明白的一件重要的事情是为什么模型首先给出这些预测。理解模型的行为可以帮助您发现偏差并最终减轻它。

[*【本地可解释模型不可知解释(LIME)*](https://www.oreilly.com/content/introduction-to-local-interpretable-model-agnostic-explanations-lime/) 是一个用于为不同机器学习模型的行为生成解释的工具。Lime 允许您操作模型的不同组件，以便您可以更好地理解它，并能够指出偏差的来源(如果存在的话)。

</6-data-science-certificates-to-level-up-your-career-275daed7e5df>  

# №5: FairML

这个列表中我们最后的工具箱是 [FairML](https://github.com/adebayoj/fairml) 。FairML 是一个 Python 开源工具箱，用于审计机器学习预测模型，以检测偏差。FairML 的构建和开发是为了回答一个问题，即特定的输入对模型的性能有多大影响。

使用不同的输入数据集轻松测试模型性能的能力可以帮助您检测模型中是否存在偏差。FairML 提供了一个端到端的工具，允许您通过量化特定输入的相对重要性来测试您的模型性能。

# 最后的想法

开发机器学习模型的每一步都会对项目的最终结果产生重大影响。这些步骤中的每一步对结果都有不同的影响，并且需要不同的时间和精力来完成。

最费时费力的步骤之一是检测和减少模型中的偏差。因为偏差可能来自不同的来源和原因，所以开发人员很难指出您的模型中是否存在偏差。

</data-science-lingo-101-10-terms-you-need-to-know-as-a-data-scientist-981aa17d5cdf>  

作为开发人员，我们喜欢创建工具，让我们的未来生活变得更加轻松。这就是为什么在过去的几年里，研究人员努力开发和构建可以帮助检测和减轻偏见的工具，以节省经常浪费在这方面的大量时间和精力。

时间就是金钱，如果有一种工具可以帮助你更快更有效地开发下一个项目，那么为什么不利用这一点，帮助自己建立更多的项目并获得更多的经验呢？

本文介绍了 5 种不同的工具和方法，您可以使用它们来加快在即将到来的项目中检测和减轻机器学习模型偏差的过程。因为众所周知，无偏的模型给出的结果是最准确的。**