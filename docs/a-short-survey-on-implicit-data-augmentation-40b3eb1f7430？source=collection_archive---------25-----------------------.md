# 隐式数据增强综述

> 原文：<https://towardsdatascience.com/a-short-survey-on-implicit-data-augmentation-40b3eb1f7430?source=collection_archive---------25----------------------->

## 隐式数据增强技术综述及未来研究方向

![](img/e2576cddbfd799e6b9837b6a8f7d31f9.png)

作者图片

数据扩充是一种流行的技术，用于提高可能过度拟合的模型的概化能力。通过生成额外的训练数据并将模型暴露给同一类中的不同版本的数据，训练过程变得更加健壮，因此更有可能将结果模型推广到测试集。最近，与直接转换输入数据的显式数据扩充相比，间接从隐藏空间生成样本的隐式数据扩充显示出良好的性能。本文重点介绍了隐式数据增强的主要研究方向，并提出了该领域的几个研究方向。

**简介**

衡量训练集和测试集之间性能差异的泛化差距很大程度上来自它们的分布差异。当潜在数据生成过程的所有数据变化在训练集中可用时，即使是简单的模型，如 k-最近邻，也会在测试集中获得近乎完美的泛化性能(Cover & Hart，1967)。然而，在实际应用中，通常会观察到训练集和测试集之间存在一定程度的分布差异。这种分布差异的一个直接影响是，在训练集中表现良好的模型不一定能推广到测试集中，如果没有适当地正则化的话。此外，神经网络通常对输入数据中的噪声敏感(Tang & Eliasmith，2010)，因此需要通过数据扩充实现不变性。

**文献综述**

从数据的角度来看，与模型架构、损失函数或整个模型训练过程的优化算法等其他组件相比，数据扩充是一种常用的正则化技术。它旨在通过对现有训练数据进行随机但真实的转换来生成人工样本，从而增加可用训练数据的多样性。在图像数据的情况下，通过诸如旋转和镜像的技术，显式数据扩充直接转换现有数据以获得新的样本。参见(Shorten & Khoshgoftaar，2019)对常见显式图像数据增强技术的全面综述。

虽然过程很简单，但直接应用随机变换可能无法生成有用的数据，因为生成过程是无人监督的，并且不是针对当前的建模任务。为了缓解这一挑战，(Zhang et al .，2017)提出了一种基于训练特征及其标签的凸组合来生成样本的混合方法。事实上，如果训练集和测试集之间的相似性相当好，显式地对中间样本进行插值，即使是线性插值，也可以生成比随机生成更有用的训练样本。如果测试集与训练集非常不同，则有必要使用原始样本和生成的样本之间的某种形式的距离来生成更具挑战性的对立样本。例如，距离现有样本最远，但仍在同一类目标标签内的人工样本是极端样本，可能有助于支持推广到完全不同的测试集，如(沃尔皮等人，2018 年)的工作所示。

数据扩充也可以在不直接调整输入数据的情况下实现。在变分自动编码器(Kingma & Welling，2013)中，输入数据首先被编码到隐藏空间中，然后通过在隐藏状态变量中引入随机噪声来控制新数据的生成。通过将原始特征空间投影到一个紧凑且深度隐藏的空间，得到的特征往往会被线性化和去纠缠，这使得基于采样或插值的生成过程在实际应用中更有意义(Bengio，Yoshua 等人，2013)。例如，(Wang et al .，2019)在隐藏空间中沿着具有最大类内变化的维度内插新样本，从而在投影回原始输入空间时产生语义上有意义的变换。由这种在自动编码器类型的神经网络中的隐藏空间执行的目标凸插值产生的一个有趣的特性是等价于鲁棒损失函数。这种鲁棒性是通过在解码输出层对网络权重的范数进行加压，经由添加到损失函数的惩罚项来实现的(Bishop，1995a)。这一特性允许通过重构损失函数来进行有效的训练，但仍然引入了隐含的数据正则化效果。

**潜在研究方向**

显式数据扩充会在生成和训练人工样本时引入相当大的训练开销。相比之下，隐式数据增强可以通过对损失函数的轻微修改来实现类似甚至更好的正则化效果，而不会导致过多的额外训练开销。在下文中，我们为这一激动人心的领域提供了几个潜在的研究方向。

**数据**。数据扩充本质上旨在生成不同变化的附加样本，以逼近测试集的数据分布。由于特征解缠结效应，在编码操作之后在深层隐藏空间上工作似乎更有希望，而不是显式地调整输入数据，导致更线性化和可分离的特征空间。就在隐藏空间采样候选点而言，我们观察到两个共同的流。一个流侧重于观察点之间的插值，可能沿着具有最大方差的维度，以提高可解释性，如(Wang et al .，2019)所示。另一个流集中于观察样本的受控外推和扰动，以在隐藏空间中生成“硬”对抗样本。我们注意到(沃尔皮等人，2018 年)中使用的第二个流中有一个类似的方法，尽管是在输入数据上。

**网络架构**。模型架构定义了从输入数据到输出预测的信息流。我们现在将不同版本的网络架构暴露给相同的训练数据，而不是生成不同版本的训练数据。网络架构中引入的随机噪声，如 dropout (Srivastava 等人，2014 年)及其变体，防止了对网络特定部分的过度依赖，使模型更加稳健，对数据中的噪声不太敏感。然而，隐式正则化效果可以以多对多的方式进一步利用，同时引入多个版本的数据和架构，并优先考虑具有高方差的维度。

**损失功能**。在估计期间，通过模型系数的适当正则化，可以引入几个好的特性。例如，基于范数的惩罚(如 Lasso)已被证明通过在特征空间中隐式引入随机扰动来稳健地估计过程(Xu 等人，2010)，而基于协方差的惩罚可用作隐式数据增强技术，如(Wang 等人，2019)所示。因此，研究损失函数中其他形式的正则化的影响是有趣的。此外，对系数的调整大多是即时的和一步到位的，这并不保证它从长远来看是最优的。对系数的多步惩罚似乎是有希望的，它平衡了短期和长期效果之间的权衡，就像在强化学习代理的训练过程中一样。

**优化算法。**随机梯度下降是训练神经网络中最常用的算法，通过对邻近更新进行去相关，该算法被证明是一种隐式正则化算法(Daniel A. Roberts，2018)。这意味着，对于相同的输入数据，与每次迭代的一批样本相比，模型在对每个样本进行单独训练时更有可能泛化。其他技术，例如小权重初始化和大初始学习率，也能够隐式地正则化模型。这些方法本质上关注于输入数据与权重的交互方式，以及呈现给权重的方式，以便持续更新。在这方面，我们期待在输入数据和权重之间的这种相互作用机制上有更多创新的研究进展。例如，权重更新中每个输入数据点的相对贡献可以被插值以生成额外的假更新，这对应于生成更多的训练样本。此外，权重更新时间表也可以遵循基于某些一般化标准的多步前瞻方案。

**结论**

本文总结了隐式数据增强领域的主要研究进展，并提出了未来的研究方向，包括数据、网络结构、损失函数和优化算法。由于这是一个连接多个领域的重要主题，我们希望我们的工作能够激发研究社区的更多兴趣，并为更广泛的模型概化领域做出贡献。

**参考文献**

最近邻模式分类。IEEE 信息论汇刊，13(1):21–27，1967。

主教，c . m .(1995 年 a)。模式识别的神经网络。牛津大学出版社。(ˆ1, 5, 7, 9, 23)

唐，伊川&埃利亚史密斯，克里斯。(2010).用于鲁棒视觉识别的深度网络。ICML 2010——第 27 届机器学习国际会议论文集。1055–1062.

用于深度学习的图像数据增强的调查。j 大数据 6，60 (2019)。[https://doi.org/10.1186/s40537-019-0197-0](https://doi.org/10.1186/s40537-019-0197-0)

张，&西塞，穆斯塔法&多芬，扬&洛佩斯-帕斯，大卫。(2017).混淆:超越经验风险最小化。

沃尔皮，里卡多&南孔，洪泽克&塞内，奥赞&杜奇，约翰&穆里诺，维托里奥&萨瓦雷塞，西尔维奥。(2018).通过对抗性数据增强推广到未知领域。

金玛，迪德里克&韦林，马克斯。(2013).自动编码变分贝叶斯。ICLR。

透过深度表现更好的混合。ICML (2013 年)。

王、俞林、潘、、宋、史集、张、洪、吴、程、黄、高。(2019).面向深度网络的隐式语义数据增强。

斯利瓦斯塔瓦，尼蒂什&辛顿，杰弗里&克里热夫斯基，亚历克斯&苏茨基弗，伊利亚&萨拉胡季诺夫，鲁斯兰。(2014).辍学:防止神经网络过度拟合的简单方法。机器学习研究杂志。15.1929–1958.

徐，桓&卡拉马尼斯，康斯坦丁&曼诺尔，。(2010).稳健回归和套索。信息论，IEEE 汇刊。56.3561–3574.10.1109/份。2010.2048848656366

丹尼尔·罗伯茨(2018)。SGD 隐式正则化泛化错误。Neurips (2018)。