# 对抗性鲁棒性实用指南

> 原文：<https://towardsdatascience.com/a-practical-guide-to-adversarial-robustness-ef2087062bec?source=collection_archive---------34----------------------->

## 我们正在进入计算机系统威胁建模的新时代

**简介** 机器学习模型已被证明容易受到敌对攻击，这种攻击包括在测试期间添加到输入中的扰动，旨在欺骗模型，而这些扰动通常是人类察觉不到的。在这份文件中，我强调了几种产生对抗性例子的方法和评估对抗性鲁棒性的方法。

**背景
下面是伊恩·古德费勒论文中的典型例子。**

([解释和利用对立的例子](https://arxiv.org/abs/1412.6572) (ICLR 2015)伊恩·j·古德菲勒，黄邦贤·施伦斯，克里斯蒂安·塞格迪)

虽然对抗性机器学习仍然是一个非常年轻的领域(不到 10 年)，但围绕攻击此类模型并找到其漏洞的论文和工作已经出现了爆炸式增长，变成了防御者和攻击者之间名副其实的军备竞赛。攻击者本质上占了上风，因为破坏东西比修理东西容易。与对抗性 ML 的一个很好的类比是 50 年代的密码学:研究人员不断尝试复杂的方法来保护系统，研究人员不断尝试破解它们，直到他们发明了一种复杂的算法，这种算法可能在计算上过于昂贵而无法破解(DES)。

为此，让我们尝试定义一个模型上的敌对样本是什么样子的。数学上，让我们假设我们有一个带有输入 **x** 的模型 **f** ，它可以产生一个预测 **y.** 然后，模型 **f** 和输入 **x** 的对立示例 **d** 可以定义为:

*   **f(x+d)！= y** ，意味着加到 **x** 上的扰动 **d** 不会产生与 **x** 相同的预测
*   **L(d) < T** ，其中 **L** 是测量 **d** 的*范数*的某个通用函数，其中 **T** 是该范数的某个上界。

基于上述参数，有一大类算法可用于产生这种扰动。大体上，它们可以用下图来划分。

基于(1)对模型的梯度访问和(2)用于生成样本的规范的不同对立示例的分类。这捕获了几乎所有的攻击，但绝不是一个详尽的列表(图片由作者制作)

我们对对手的威胁建模如下:

1.  **梯度访问:**梯度访问控制谁可以访问模型 **f** 和谁不可以。
    **白盒**对手通常可以完全访问模型参数、架构、训练例程和训练超参数，并且通常是文献中使用的最强大的攻击。白盒攻击通常使用梯度信息来寻找对立的例子。
    **黑盒**对手很少甚至没有访问模型参数的权限，模型被抽象为某种 API。黑盒攻击可以使用基于非梯度的优化方法发起，例如(1)遗传算法，(2)随机搜索和(3)进化策略。就计算资源而言，它们通常效率不高，但却是最现实的对手类别。
2.  **扰动界:**扰动界决定了扰动 **d** 的大小，通常用一些数学范数如 Lp 范数来度量。
    **L0 范数:**L0 范数有界攻击通常涉及修改模型输入信号的一定数量的特征。L0 范数有界攻击通常非常真实，可以在真实世界的系统上发起。一个常见的例子是在停车标志上添加一个贴纸，可以迫使自动驾驶汽车不要减速——所有的背景都被保留下来，只有一小部分环境被修改。
    **L1 范数:**L1 范数有界攻击涉及总扰动值的**和**的上限。这种攻击相当罕见。
    **L2 范数:**L2 范数有界攻击涉及扰动的上限**欧几里德距离/毕达哥拉斯距离**d .由于 L2 范数在线性代数和几何中的数学相关性，L2 范数有界攻击被非常普遍地使用。
    **L_infinity 范数:**L _ infinity 范数有界攻击涉及扰动**，**的**最大值**的上界，是最先被发现的攻击。在所有攻击中，无限攻击是研究最多的，因为它们在鲁棒性优化中的简单性和数学便利性。

在非常高的层次上，如果我们使用 Lp 范数作为鲁棒性度量，我们可以有 8 种不同类型的攻击(2 x 4)在下面突出显示。还有其他几种特定于域的方法来量化扰动 **d** 的幅度，但上述方法可以推广到所有输入类型。注意，下面引用的攻击严格针对图像，但是一般原理可以应用于任何模型 **f.**

按(1)梯度访问和攻击规范类型(作者制作的图像)分类的常见对抗示例的示例

**结论:**

随着机器学习模型越来越多地嵌入到我们周围的产品和服务中，它们的安全漏洞和威胁变得越来越重要。我们已经强调了对手可能对预先训练的模型发起敌对攻击的不同方法，但是随着我们找到更多的例子，我们将确保再写一篇文章并分享这些发现。

[1] Modas 等人， ***SparseFool:几个像素造成很大的不同*** ，【CVPR】2019
【2】paper not 等人， ***针对机器学习的实用黑盒攻击，***ASIA CCS 2017
【3】Sharma 等人， ***EAD:对深度神经网络的弹性网攻击。***AAAI-2018
【4】卡里尼等， ***走向评估神经网络的鲁棒性，***IEEE Security&Privacy，2017
【5】马德瑞等， ***走向抗对抗性攻击的深度学习模型，***ICLR 2018
【6】古德费勒等， ***解释与治理一种基于进化的针对神经网络的对抗性攻击，*** arxiv 预印本:
【8】Croce 等人， ***Sparse-RS:查询高效的稀疏黑盒对抗性攻击的通用框架，*** ECCV 关于真实世界中对抗性鲁棒性的第 20 次研讨会
【9】Alzantot 等人， ***GenAttack:具有梯度的实用黑盒攻击***