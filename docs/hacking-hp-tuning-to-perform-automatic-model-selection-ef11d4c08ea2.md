# 黑掉惠普调优来执行自动型号选择

> 原文：<https://towardsdatascience.com/hacking-hp-tuning-to-perform-automatic-model-selection-ef11d4c08ea2?source=collection_archive---------41----------------------->

![](img/e6de370e378a3ca81911aeefd538e5dc.png)

瓦伦丁·彼得科夫在 [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral) 上的照片

超参数调整的任务是选择与目标相关的最佳模型配置。我们在之前的[帖子](/automl-for-fast-hyperparameters-tuning-with-smac-4d70b1399ce6)中已经看到，尽管这看起来是一项具有挑战性的任务，但是当使用基于模型的方法时，编码并不困难。

但是等等！如果惠普调优可以为给定的 ML 任务选择最佳参数，我们是否可以重用它来选择最佳模型？也就是说，是否有可能创建一种方法，像使用其他参数一样使用模型类型作为参数，并让超级参数优化为我们选择正确的参数？

这难道不是解决 ML 问题的巨大加速吗？答案不仅是肯定的是的！而且，实现起来也不是很复杂。

我们将在本文中详细探讨这一点。

# 基于型号的惠普调整快速提醒

超参数调整(HPT，有时也称为 HPO 超参数优化)的目标是找到最大化或最小化给定目标的配置。有各种方法来执行超参数调整:蛮力，随机搜索，贝叶斯搜索，或基于模型的方法。

我以前提倡使用基于模型的方法，这在我遇到的一些问题上证明是非常有效的。你可以在这里找到更多关于 [SMAC](https://automl.github.io/SMAC3/master/index.html) 的细节，这是一个高效的 HPO 图书馆:

</automl-for-fast-hyperparameters-tuning-with-smac-4d70b1399ce6>  

您可能也有兴趣更好地理解这些方法是如何工作的，并构建自己的超参数优化库。

这里有一个有趣的方法，利用你对 XGBoost、CatBoost 或 RandomForest 等标准模型的了解:

</tuning-xgboost-with-xgboost-writing-your-own-hyper-parameters-optimization-engine-a593498b5fba> [## 用 XGBoost 调优 XGBoost:编写自己的 Hyper Parameters 优化引擎

towardsdatascience.com](/tuning-xgboost-with-xgboost-writing-your-own-hyper-parameters-optimization-engine-a593498b5fba) 

除了令人兴奋之外(使用 ML 模型来调整 ML 模型)，基于模型的超参数优化提供了一个优于其他解决方案的非常有趣的优势:它支持分类参数。这意味着我们可以将模型类型编码为分类参数。

也就是说，让我们看看如何破解标准的超参数调整来执行模型选择和加速模型构建。

# 超级模型

正如介绍中所提到的，这次黑客攻击背后的想法是这样的:我们能否像其他任何一个参数一样，将 *model_type* (即 XGBoost，Prophet，S/ARIMA……)视为一个参数，并让 Hyper Parameter Tuning 方法为我们完成这项工作？

令人高兴的是，如上所述，使用基于模型的方法的超参数优化支持分类参数。毕竟，他们的底层模型通常是一个增强的(或者不是)决策树。

这个属性的一个直接结果是，我们可以创建一个 ***超级模型*** ，它将由*模型类型*以及其他参数来定义。 *model_type* 将对用于训练的底层模型进行编码。在 python 中，这给出了:

一个超级模型，它将底层模型类型作为一个参数。作者代码。

超模型的实现遵循 scikit-learn 模型接口，即它提供了 *fit、predict、set_params* 和 *get_params* 方法。为了简单起见，我们只支持两个模型:XGBoost 和 RandomForest，但是再增加一个只会多几行。例如，您应该尝试一下 SVR。

您可能已经注意到，我们使用白名单来只保留那些适用于给定模型的参数。这不是支持参数因模型而异这一事实的最佳方式。正确的做法是使用条件配置。ConfigSpace python 库支持这一点，但 scikit 并不支持这一点。

使用我们的新类进行训练和预测是立竿见影的。假设我们想使用 XGBoost 作为底层模型。这给出了:

用我们的超模。作者代码。

我们现在要做的就是在 HPT/HPO 步骤中使用这个模型，并让它选择最佳候选模型类型。

# 寻找最佳模型

有了主参数是模型类型的超级模型，我们可以使用标准的超级参数调整方法来识别最佳模型。

需要记住的重要一点是，绝对没有“最佳模式”。当我写“最佳模型”时，我指的是给定分数的最佳模型。在本文中，我们将使用平均绝对误差作为得分。

尽管在我之前的两篇关于这个主题的文章中，我一直在使用(并建议)SMAC 或一个定制的超参数优化实现来执行 HP 调优，但在本文中，我们将尝试另一种方法。不要错过尝试新事物的机会:)

这一次，我们将使用 BayesSearchCV 来探索配置空间。贝叶斯搜索的基本原理是使用高斯过程建立一个代理模型，估计模型得分。

每个新的训练更新代理模型的后验知识。然后为这个代理提供随机挑选的配置，给出最佳分数的配置被保留用于训练。

因为它使用高斯过程模型来学习超参数和候选模型的分数之间的关系，所以它可以被认为是基于模型的方法。

将所有这些放在一起会产生以下代码行:

使用我们的超级模型来确定波士顿数据集的最佳模式。作者的代码。

配置空间主要使用参数的均匀分布来定义。这意味着在给定范围内选择一个值的概率在任何地方都是相同的。例如， *max_features* 、 *n_estimators* 或 *max_depth* 就是这种情况。相反，跨越多个数量级的 *gamma* 和 *learning_rate* 使用*对数均匀分布*选取。

运行这段代码将向您展示 XGBoost 似乎是这个数据集的最佳选择。

# 检查

像我一样，你可能不相信一个算法的结果，在执行一些检查之前。

幸运的是，许多其他数据科学家已经研究了波士顿数据集挑战。更具体地说，在 Kaggle [这里](https://www.kaggle.com/shreayan98c/boston-house-price-prediction)由 [Shreayan Chaudhary](https://www.kaggle.com/shreayan98c) 研究过，他得出了与我们的算法相同的结论。这是好消息。

然而，我们再谨慎也不为过。让我们执行另一个简单的检查，以确保如果我们对 HP Tuning exploration 进行更多迭代，并且只针对随机森林进行优化，RandomForest 不会优于 XGBoost:

这次调优单一模型:RandomForestRegressor。作者代码。

我们简单地重用了我们的超模型，但是这一次我们强制将探索集中在一个模型上:RandomForestRegressor。我们也允许更多的迭代:随机森林 50 次，而以前两个模型都是 50 次。

结论是一样的。就平均绝对误差而言，XGboost 精度保持得更好:随机森林为 2.68，而 XGBoost 为 2.57。

我们也可能是“幸运的”，XGBoost 优于 RandomForestRegressor 的事实可能完全是随机的，并且与用于初始化贝叶斯搜索的初始种子相关: *random_state=0。*

使用各种 *random_states* 多次运行代码应该会让你相信我们并不幸运，在这种情况下，XGBoost 是关于我们选择的分数的最佳选项:平均绝对误差。

# 我们的自动模型选择丢弃坏选项的速度有多快？

另一个要考虑的非常有趣的方面是我们的模型选择丢弃跛脚鸭的速度。为了说明这一点，我们将向超模型支持的模型列表中添加另一个模型:LinearRegression。

直觉上，这是最糟糕的选择。让我们看看我们的自动模型选择花了多少时间来探索这种可能性。首先，我们将它添加到超模:

向超级模型添加 LinearRegression。作者代码

我们将使用一个肮脏的黑客来测量探索一个给定配置所花费的时间。我们只在第 42 行打印它(信不信由你)，并对它执行一些 greps，得到以下统计数据:

*   线性回归研究了 3 次。
*   随机森林 17 次。
*   XGBoost 30 次。

值得注意的是，在仅仅 50 次迭代中，我们的模型选择已经学会关注最有前途的模型:XGBoost，并且在仅仅 3 次迭代中就放弃了线性回归。

如果我们使用 ConfigSpace 的条件配置，而不是使用允许参数的白名单，我们可以更快地排除线性回归。这就人为增加了不必要的审判。

# 结论

扩展模型选择的超参数调整。用几行代码演示它非常容易。

我们一直使用贝叶斯搜索来探索配置空间，但我们也可以使用任何其他有效的超参数调整方法。

另一个值得尝试的想法是使用超参数优化方法进行特征选择。很有可能它也能有效工作。