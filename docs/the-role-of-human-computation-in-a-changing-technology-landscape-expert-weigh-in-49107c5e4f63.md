# 人类计算在不断变化的技术环境中的作用:专家参与

> 原文：<https://towardsdatascience.com/the-role-of-human-computation-in-a-changing-technology-landscape-expert-weigh-in-49107c5e4f63?source=collection_archive---------15----------------------->

## 在今年的 [HCOMP](https://www.humancomputation.com/) 会议期间，与会者讨论了人工计算在人工智能产品开发中的作用

![](img/7c8aebed2282070ea0532787d0fff336.png)

[本·斯威特](https://unsplash.com/@benjaminsweet?utm_source=medium&utm_medium=referral)在 [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral) 上拍照

来自 Alphabet 的 Chris Welty 、来自亚马逊的 Kumar Chellapilla 、来自微软的 Besmira Nushi 、来自 Brainworks 的 Markus Krause 、来自 Toloka 的 Olga Megorskaya[和来自 Google Research 的 Lora Aroyo](https://www.linkedin.com/in/omegorskaya/) 分享了他们的观点。

**显式数据标注的问题**

克里斯解释说，人类的计算通常有两种形式——显式和隐式。隐式指的是数据标签是副产品的情况，例如当我们播放电影、在 YouTube 上听音乐或进行网络搜索时。该系统的算法了解其用户，即使没有这样的数据标记发生。

另一方面，像众包这样更“传统”的数据标签形式是显式的。这里的主要挑战是质量和了解如何提高质量。问题是大多数机器学习系统花费大量时间来生成数据集，但没有足够的时间来表征它们的准确性。

因此，出现了很多失败的人工智能产品。但是有一种方法可以从我们的错误中吸取教训。Crowd Camp 最近发起的对抗性众包挑战是一种有趣的方式，可以激励人们检查 ML 模型所犯的错误。

**偏见和含糊不清**

坐在观众席上的 Lora Aroyo 受邀详细阐述这一挑战。她解释说，使用图像分类模型，她的团队能够计算出 60%的标记内容是敌对的，并混淆了训练模型。发生这种情况是因为人类评分员经常不同意歧义。这表明数据影响模型，而标注者通过他们的主观性和偏见影响数据。像“松饼”或“背心”这样简单的概念在不同的文化中可能有不同的含义，有时甚至在说同一种语言的人之间也是如此。

另一位小组成员 Markus 认为我们应该接受这种模糊性，而不是与之斗争。工程师们需要明白，没有所谓的地面真相。一旦你接受了这一点，你就可以开始寻找合适的指标来衡量绩效，并考虑改进。数据标注中的模糊性不会消失，但这不是从等式中消除它，而是学习如何处理它。

贝斯米拉进一步阐述了这一点，并认为错误信息和偏见实际上可以帮助我们建立一个分类器，在未来检测这种内容。尽管如此，我们不太可能在不久的将来创建一个能够解决我们所有机器学习问题的平台。目前，我们只是在搞清楚事情并进行实验——这是一个游戏场。仅仅因为我们有一个能够训练的模型，并不意味着我们应该试图尽快推出一个人工智能产品。我们不应该仓促行事。

**数据标注的连续性**

[Toloka](https://toloka.ai/) 的 Olga Megorskaya 认为，这一点还有更多的意义，因为训练你的 ML 模型不是一个静态的、一次性的努力，而是一项持续的事业。所以，这不仅仅是质量甚至耐心的问题。这也是一个数量问题——你需要一个持续的训练数据流来控制你的模型在生产中的行为。因此，定期更新数据和连续标记的问题是我们在推出真正可靠的人工智能产品之前需要解决的问题。

库马尔认为这是一个有效的论点——监测大有帮助，特别是对于视觉和语言任务非常具有挑战性。数据会漂移，偏差会改变，所以你不能只是假设如果配料是正确的，食谱可以永远保持不变。每天都有新的项目出现——从新短语到新相机——所有这些都需要输入给 AI。

此外，库马尔表示，未来十年，人类计算的需求可能会大幅上升，这将与增强人工智能的崛起相吻合。这意味着大多数人的生活，包括人群工作者和数据标签员，将会改善，因为许多任务将由人工智能接管。Kumar 估计，未来的工作负载将会是 80/20，80%的工作将由机器来完成。

**从人性化到人性化的标签**

Markus 指出了另一个至关重要的组成部分——数据标签的人性化方面。如果我们继续像对待巨轮上的齿轮一样对待人们，我们不会很快进入增强人工智能的世界。那些寻求数据标签服务的人需要对他们提供的任务负责。最好的办法就是报名成为一名群众工作者，试着自己完成工作量。这将允许您预先估计贡献者所期望的工作量和精确度。在这里，你需要既是数据科学家又是社会科学家。

Marcus 继续说，成功的数据标签必须保留，因为你需要同样的人或供应商回来与你再次合作。所以，你需要付给你的人类标签员更多的钱，而不是试图从最少的钱里榨取尽可能多的钱。这从长远来看是适得其反的。如果贴标签的人对他们的条件不满意，数据和整个项目都会受到影响。任务越复杂，投入的资金就越多，从中获得的潜在收益也就越多。所以，底线是不要贪婪。

奥尔加还认为，在制造洗钱数据的过程中，道德问题至关重要。这就是为什么托洛卡参与研究的原因——包括与[赛弗·萨维奇](http://www.saiph.org/)合作的[项目](https://slideslive.at/38935824/a-future-of-work-for-the-invisible-workers-in-ai?ref=speaker-42606-latest)——研究众包背景下数据标签员的福祉。奥尔加说，最终目标是让人类贴标机的工作不那么费力，更有成就感。事实上，Toloka 一直在寻找有才华的研究人员和从业者，与他们一起寻找解决这个问题的有效方法。

**公民科学**

贝斯米拉同意其他发言者的观点，并补充说，公民科学社区是一个很好的例子，说明个人贡献者如何以自我维持的方式帮助共同的事业。在最新的项目中，美国宇航局在业余科学家的帮助下标记火星地形的倡议。我们需要的是将同样的窄领域专业知识的逻辑转移到更广泛的付费众包领域。这将保证人在回路 ML 的健康发展，专业知识的积累和职业发展。

Markus 认为，这一切都可以归结为拥有正确的策略，并让工程师、机器和人类贡献者能够理解这些策略。换句话说，我们需要一个双向系统，在这个系统中，贴标签者可以自由地与政策制定者交谈，这样政策制定者就可以为了贴标签者的利益，最终也是为了公司的利益，完善事情的法律方面。