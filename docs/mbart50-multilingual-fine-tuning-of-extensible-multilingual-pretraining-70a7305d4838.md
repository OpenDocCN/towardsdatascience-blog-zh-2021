# mBART50:可扩展多语言预训练的多语言微调

> 原文：<https://towardsdatascience.com/mbart50-multilingual-fine-tuning-of-extensible-multilingual-pretraining-70a7305d4838?source=collection_archive---------28----------------------->

## 训练 50 种语言的多语言机器翻译，对单语数据进行预处理

![](img/e42d65ba79222c3ac257c8908c69aaaa.png)

照片由 [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral) 上的 [Soner Eker](https://unsplash.com/@sonereker?utm_source=medium&utm_medium=referral) 拍摄

注意:这是三篇系列文章中的第二篇。
第一部分:[双语机器翻译的海量预训练](/massive-pretraining-for-bilingual-machine-translation-3e26bfd85432)
第二部分: **mBART50:可扩展的多语言预训练的多语言微调**
第三部分:[多阶段训练的多语言语音翻译](/multilingual-speech-translation-with-multi-phase-pretraining-305d642b8a66)

在上一篇文章中，我们看到了预训练的 mBART 如何用于训练强大的双语机器翻译模型，以及它们如何对低资源和中等资源的语言特别有竞争力。该论文为更多关于预训练编码器-解码器模型的问题铺平了道路:

1.  是否可以扩展预训练语言集，以打开更多下游微调的可能性？
2.  既然预训练是多语言的，为什么微调不也是多语言的？

这些都是在[【1】](https://arxiv.org/abs/2008.00401)中回答的问题，其贡献如下:

1.  将 mBART 扩展到 50 种语言，而不损失双语微调的准确性
2.  建议对多对英语、英语对多和多对多(通过旋转)进行多语言微调，增益超过所有基线。多对英语方向的收益尤其大
3.  为 ML50 基准测试提供标准化的培训/开发/测试划分，涵盖低、中、高资源语言，共 2.3 亿个平行句子。

# 多语言微调

多语言微调过程并不是特别复杂。
首先，所有语言方向 **s- > t** 的数据被收集在单个训练集中。每种语言方向的可用文本数量极其多样，涵盖了从 4K 到每种语言超过 1000 万个句子对的全部范围。因此，每个批次的数据都是使用语言说明上的[温度采样](https://stats.stackexchange.com/questions/255223/the-effect-of-temperature-in-temperature-sampling)从不同的组中选择的。如前一篇文章[ [2](https://arxiv.org/pdf/1907.05019.pdf) ]所述，温度采样产生一个更平衡的分布，因此低资源语言对以更高的概率被选择。

多语言微调在三种不同的设置中进行了实验:多对英语、英语对多和多对多。最后一个是结合另外两个设定的数据训练出来的，这样英语就是一种支点语言。

## 25 种语言的结果

第一个实验旨在在用于训练 mBART 的 25 种语言上比较多语言微调与双语微调和从零开始的多语言。

结果显示，在多对英语方向上，**多语言模型明显优于所有双语模型**。另外，**多语言微调明显优于《从零开始》多语言，**除了超过 1M 句子对的语言方向，优势相当小。另一方面，对于资源较少的语言(7K-30K 句子对)，多语言微调相对于双语从头开始的优势是一个突出的 **18.03 平均 BLEU 分**，这使得玩具和可用模型之间存在巨大差异。相比之下，第二好的模型是从零开始多语言，提高了 14.63，而双语微调在从零开始双语基线提高 10.80 BLEU 点之后停止。

英语对多设置呈现了一个完全不同的场景。
首先，很重要的一点是，本文中描述的结果与表格并不完全匹配，与之前的结果相比，表格受到的关注较少。事实上，除非我遗漏了什么，否则以**粗体**标记的值是一行中最好的值，但实际上并不是这样。

然后，通过只阅读数字而不阅读文本，我们可以看到，多语言微调仅在大于 10M 的句子对组上取得最佳结果(严格来说),而对于其他组，从零开始的多语言英语对多一直更好。不幸的是，*作者未能解释为什么会发生这种情况*,但它代表了一个有趣的结果供进一步研究。

而且，在资源最少的组上，多对多模型与**多语言微调**只比**双语** **从零开始**基线好 0.90，而**多语言从零开始**比基线有 7.9 BLEU 分的优势。多语言微调模型的改进在一对多场景中要好得多，但仍然比从零开始训练的对应模型稍差(7.6 对 8.1)。

虽然作者未能解释一对多的违反直觉的结果，即从头训练比微调更好，但他们非常清楚地解释了他们的最佳情景。

多对英语方向如此强大是因为该模型观察 49 个语言方向的英语目标侧，**并且该模型仅在英语上被评估**。另一方面，在英语对多的方向上，可能没有足够的数据来学习所有语言方向的良好条件概率。

我认为有趣的是，在整个数据规模范围内，多语言系统显示出相对于双语基线的改进。通常，对于高资源语言，多语言模型实际上比双语模型更糟糕。我希望看到更多的实验，如果 mBART 解决了这个问题，如果是的话，为什么。

# mBART50:扩展预训练模型

在上一篇文章中，我们已经看到 mBART 对于微调至少有一种**看不见的语言的语言对也是有用的。**虽然结果不如预训练语言好。当看不见的语言在源端时，退化尤其严重。

因此，预训练语言将可能的下游任务约束到这些语言。然后，为了在新语言中有效地使用 mBART，重要的是要有更多的单语数据可以添加到预训练模型中。然而，考虑到训练它所需的计算和时间资源，最好不要从头开始。

为了克服这个问题，作者提议用 25 种额外的语言来扩展最终的 [mBART25](https://github.com/pytorch/fairseq/tree/main/examples/mbart) 检查点，有效地将它们的数量翻倍。这个过程在概念上很简单，但很有效。他们首先在嵌入层中添加 25 个随机初始化的语言标记，每个标记对应一种新语言。然后，它们将最初的 25 种语言的单语训练数据与 25 种新语言的数据连接起来，并使用得到的训练集来继续对保存的检查点进行训练。他们还重复使用相同的[句子片断](https://jacky2wong.medium.com/understanding-sentencepiece-under-standing-sentence-piece-ac8da59f6b08)模型进行分词，因为它是在 100 种语言上训练的，而不仅仅是在最初的 25 种语言上。此外，作者执行了一些数据过滤，以防止训练和开发/测试集之间的任何重叠，并过滤掉 [fasttext](https://fasttext.cc/) 未识别为正确语言的句子。

这种新的预训练模型的多语言微调带来了巨大的改进，特别是对于资源较少的语言。然而，改进主要是在多对一的方向上，而一对多模型的性能通常不如双语微调，除非是资源非常少的语言对。

此外，当对原有的 25 种语言执行 mBART50 的双语微调时，结果与微调 mBART25 获得的结果非常相似。*更多的语言不会导致任何语言对的性能下降*。

# 开放式问题

关于 mBART50 的多语言微调的工作确实很有趣，结果也很令人兴奋，但这为进一步的研究留下了空间。

第一个问题，也是研究的对象，涉及如何更好地扩展预训练模型。在该论文中，新旧数据被合并到一个新的训练集中，以获得一个新的模型，该模型不降级于旧的语言，但它假设访问旧的训练数据。当我们无法访问原始数据时，如何才能获得类似的结果？

然后，作者表明 mBART25 和 mBART50 对于最初的 25 种语言在双语微调上获得相同的结果，并因此推断出模型质量在那些语言上没有降级。然而，仅基于 mBART 论文，我们不知道下游任务的翻译质量如何与预训练模型的任何质量度量相关。深入探索这一点会很有趣，但是对于许多实验室来说，进行实验的成本太高了。

最后，给定语言的数量，论文中显示的结果是许多语言的平均值。从平均值中，我们可以看到，与双语微调相比，多对一有很大的改进，而一对多则略有下降。然而，关于个别语言的结果显示了一个更复杂的情况。虽然对于多对一模型的基本上所有语言来说，这些改进都是显著的，但是对于一对多模型来说，稍微低于零的平均性能下降是对高度变化的结果进行平均的结果。对于单一语言，与基线的差异可能高达+- 4 个 BLEU 点！这是一个有趣的问题，以了解如此高波动的原因，以及是否有可能将更多的语言推向积极的一面。

# 结论

mBART50 的多语言微调是另一个重要的章节，它显示了如何训练强大的多语言模型，这些模型也可以用于资源非常少的语言对。

多语言模型比生产中有过多的模型具有维护优势。对于资源最丰富的语言来说，这种优势总是以较低的质量来换取。使用 mBART50 后，似乎不再出现降级现象。实验只在每个语言方向的单一测试上运行，但如果在更多测试数据上得到证实，那就太好了。

在下一篇文章中，我将分析脸书在 IWSLT 2021 中提交的多语言语音翻译共享任务，它明显优于其他参与者。这个解决方案由 wave2vec2.0 提供支持，用于处理语音输入，显然 mBART 用于翻译端。

# 参考

[1]唐，余庆等，“具有可扩展的多语言预训练和微调的多语言翻译” *arXiv 预印本 arXiv:2008.00401* (2020)。

[2] Arivazhagan，Naveen 等，“野外大规模多语言神经机器翻译:发现与挑战。” *arXiv 预印本 arXiv:1907.05019* (2019)。[ [链接](https://arxiv.org/pdf/1907.05019.pdf)

# 中等会员

你喜欢我的文章吗？你是否正在考虑申请一个中级会员来无限制地阅读我的文章？

如果您通过此链接订阅，您将通过您的订阅支持我，无需为您支付额外费用[https://medium.com/@mattiadigangi/membership](https://medium.com/@mattiadigangi/membership)