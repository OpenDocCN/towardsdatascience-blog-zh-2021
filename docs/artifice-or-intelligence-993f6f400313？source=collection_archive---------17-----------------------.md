# 技巧还是智慧？

> 原文：<https://towardsdatascience.com/artifice-or-intelligence-993f6f400313?source=collection_archive---------17----------------------->

## 在看到任何数据之前，报告您的建模策略或统计分析计划

![](img/dc103f37a33b88b2350d77ca4e2f168e.png)

照片由[Karen lark Boshoff](https://laarkstudio.com/)在[https://www . pexels . com/photo/black-red-and-white-round-decor-6758919/](https://www.pexels.com/photo/black-red-and-white-round-decor-6758919/)

> 如果你的模型在新数据上表现不佳，未跟踪的倾听可能是原因。
> 
> 想象一下，在你成功之后，在台球上打你的球！这是一个坏的研究习惯。预注册是指你甚至在走到桌子前就开始打电话。你会赢一些，也会输一些，但你会公平地比赛——这就是你如何提高你的比赛。

# 技巧

在纽约州北部，我的大学室友在我们的[宿舍](https://cornell.campusgroups.com/rrc/welcome-to-the-castle!/)房间里搭建了一个飞镖靶。一个好朋友——让我们称他们为“纽约的詹姆斯”——会经常过来。詹姆斯喜欢炫耀自己是未来的飞镖高手。

詹姆斯会在地板上的胶带前摆好姿势。他们会仔细评估镖靶，手中的飞镖，以及两者之间的轨迹。最后，随着戏剧性的结束，詹姆斯会开始，“看，当我灵巧地击中…

然后，詹姆斯会夸张地把飞镖抛向空中，甚至发出低沉的运动咕噜声。当看到它果断地落在从靶心延伸到标有“16”的外弧的三角形带内时，詹姆斯会胜利地总结道，“……六！”(这是飞镖游戏“[板球](https://en.wikipedia.org/wiki/Cricket_(darts))”中“16”的俚语。)

如果你就在那时走进来，你可能会惊叹于詹姆斯投掷飞镖的高超技艺和准确性。我的意思是，他们称之为公平公正的拍摄，对不对？"看，我灵巧地击中了第六个！"

> 当你谈论一个新的见解时，好像你一直都在期待它，你移动了科学证实的目标来适应你的发现。

## 听着:德克萨斯神枪手

在科学、研究和统计领域，詹姆斯被称为德州神枪手。这位骗子以“结果已知后的假设”而闻名——同时声称假设是在结果已知前*创造的([听](https://en.wikipedia.org/wiki/HARKing))。这通常表现为“[研究人员自由度](https://en.wikipedia.org/wiki/Researcher_degrees_of_freedom)”，通常用于 [p-hacking 和数据钓鱼或窥探](https://en.wikipedia.org/wiki/Data_dredging)。*

这很糟糕，因为这让你看起来像是在寻找支持你最初假设的证据，而事实上你在寻找一个新假设的可能证据——一个需要来自*单独的额外研究*的*新证据*来进一步支持的假设:“我找到了六个。也许我擅长打六？让我再试一次。”

> 每次你听的时候，你都冒着宣称不可信的风险:你在实际上只有噪音的地方复制了一个真实的信号。

这种常见的混淆使得一项科学发现看起来比实际上更真实、更可复制。例如，每当你在之后写下你的建模策略、统计分析计划(SAP)或方法部分*时，你可能会无意中这样做。*

也就是说，即使在最好的情况下，也很容易忘记或没有注意到你是如何简单地通过检查你的初步发现，或通过查看工作图或数据可视化来改变你的初始假设或模型的。这种“假设蠕变”意味着你在缓慢但肯定地调整你的想法以适应你的研究数据，而不是用你的研究数据来测试或证实你的原始想法。

这些*根据经验更新的*假设就是你最终在 SAP 或方法部分呈现的*初始*假设！你试图“让数据自己说话”——但在你给了他们那一英寸后，他们又后退了一码。

## 样品内部性能

> 您错误地将您对模型性能的理解“颠倒”了，从样本内(真)到样本外(假)。

在机器学习中，HARKing 的一个版本可以产生一种微妙类型的[统计过拟合](https://en.wikipedia.org/wiki/Overfitting)(即在没有结构的变化中找到结构，或者在只有噪声的地方找到信号)。如果你的模型在新数据上表现不佳，未跟踪的倾听可能是原因。

*   ***“看着，我灵巧地打……”***假设你适合，交叉验证，用你的训练数据选择一个模型。然后，在维持数据上测试模型，并计算其维持性能。
*   您觉得可以改进这种维持性能，并决定调整您最初的建模和交叉验证参数。然后，使用您的培训数据重新拟合、交叉验证并选择一个新模型。
*   您在相同的维持数据上测试您的模型，注意到您的新模型的维持性能得到了改进，现在感到满意了。你报告这是它的样本外表现。***“……第六！”***

不幸的是，您只是优化了您的模型以适应您的定型数据和维持数据，从而恶化了它的真实维持性能(即，它归纳为新数据的能力)。你忘记了你做了这些，并且在总结你的结果时，你没有报告你“在已知维持拟合(结果)后调整了你的模型(假设)”。

通过倾听，您隐式地对样本内数据进行了重新分类，以包括您的训练数据*和*维持数据。但随后你混淆了结果模型拟合反映出样本外的表现。也就是说，您错误地将您对模型性能的解释“颠倒”了，从样本内(真)到样本外(假)。

附录(2021 年 12 月 4 日星期六):事实上，Hastie 等人(2009 年)在他们的开创性文本[统计学习的要素](https://hastie.su.domains/ElemStatLearn/)(第 7.2 节)中明确警告了这一点:

> 如果我们处于数据丰富的情况下，解决这两个问题的最佳方法是
> 将数据集随机分成三部分:训练集、验证集
> 和测试集。训练集用于拟合模型；验证
> 集用于估算模型选择的预测误差；测试集
> 用于评估最终选择模型的泛化误差。
> 理想情况下，测试集应该保存在一个“保险库”中，只有在数据分析结束时才拿出来
> 。**假设我们重复使用测试集
> ，选择测试集误差最小的模型。那么最终选择的模型的测试
> 设定误差会低估真实的测试误差，
> 有时会大大降低。**

# 智力

![](img/d1bcab78ddf46386ffb51f59ea1cd594.png)

Rodolfo Clix 在[https://www.pexels.com/photo/five-bulb-lights-1036936/](https://www.pexels.com/photo/five-bulb-lights-1036936/)拍摄的照片

> 重复的想法，确认的发现，样本内的非样本表现。

詹姆斯显然是在夸大其词，实际上并没有声称自己在飞镖方面有不可思议的技能。但是，许多(如果不是大多数的话)研究人员通过倾听以这种方式玩科学板球，通常是无意识的。它非常普遍、自然且容易做到:在获得应用统计学硕士学位和生物统计学博士学位多年后，并且拥有 18 年以上的专业数据分析师经验后，我仍然发现自己在听！

重复的想法，确认的发现，样本内的非样本表现。每次你听的时候，你都冒着宣称不可信的风险:你*复制了*一个实际上只有噪音的真实信号。

这种说法令人不安地普遍存在，产生了真实而深远的后果；倾听是科学可修复但顽固的缺陷之一。因此，它是“生物医学和心理学复制危机的一个关键因素”，正如我[在其他地方](/ditch-statistical-significance-8b6532c175cb?sk=ca0fdc1447067647a9af8eddd2d24190)写的关于[显著性谬误](https://pubmed.ncbi.nlm.nih.gov/25888971/)(呼应了研究界的一种[普遍情绪](https://www.vox.com/future-perfect/21504366/science-replication-crisis-peer-review-statistics))。

## 方法:预注册

所以在某个时候，你意识到你一直在用你的研究玩“詹姆斯在纽约板球”。你想改掉这个坏习惯。怎么会？

在 SAP 或方法部分:

1.  **预案。**在收集和处理任何研究数据进行分析之前(即“先验”或“事前”)，始终陈述您计划的分析和模型，即使这些分析和模型取决于出现的分析结果*；预先存在)。*
2.  **报告。**报告、发布或以其他方式标记该版本，以便于验证这些假设和模型确实是先验的。随着研究的进展，如果需要的话，报告带有时间戳的文本修改。

这个操作程序被称为[预注册](https://www.cos.io/initiatives/prereg)，正式或非正式都可以。对于美国的临床试验，你可以通过[clinicaltrials.gov](https://clinicaltrials.gov/ct2/manage-recs/how-register)来完成，这有助于[减少发表和结果报告偏差](https://clinicaltrials.gov/ct2/manage-recs/background)。

> 预先注册有助于你的观众(和你！)了解如何正确解释你的分析结果。

## 结果:发现还是证实？

一般来说，每个分析主要为两个互补的研究目标之一提供证据。两者都是正确的科学研究设计的基础。

*   **发现真知灼见:**使用你的研究数据构建新的假设或模型，这些假设或模型是在你评估结果时出现的(即事后分析，或“让数据说话”)，如[假设生成或*探索性*研究](https://en.wikipedia.org/wiki/Research_design#Confirmatory_versus_exploratory_research)，或[探索性数据分析(EDA)](https://en.wikipedia.org/wiki/Exploratory_data_analysis) 。
*   **确认见解:**使用你的研究数据试图[重现](https://en.wikipedia.org/wiki/Reproducibility)你的先验假设和模型，就像大多数临床[随机对照试验](https://en.wikipedia.org/wiki/Randomized_controlled_trial)和 [A/B 测试](https://en.wikipedia.org/wiki/A/B_testing)中的[假设检验或*验证性*研究](https://en.wikipedia.org/wiki/Research_design#Confirmatory_versus_exploratory_research)。

当你谈论一个新的见解时，好像你一直都在期待它，你移动了科学证实的目标来适应你的发现。这给了你错误的信心，认为这些见解比实际情况更真实。

想象一下，在成功后，你在台球上打了*分！这是一个坏的研究习惯。预注册是指在*走上牌桌之前，你甚至要对每一个镜头都打电话*。你会赢一些，也会输一些，但你会公平地比赛——这就是你如何提高你的比赛。*

也就是说，预先注册有助于你的观众(和你！)了解如何正确解释你的分析结果。那么你应该如何报告你的结果呢？

*   **标签。**清楚地将每个分析结果标记为事后(探索性)或先验(证实性)发现。
*   **发表。至少，发布或提供一份预先注册的 SAP 或方法资料，让你的观众可以用来给自己的结果贴上标签。**

这样做将使其他研究人员能够为规划他们自己的研究得出正确的结论。这种更强大的科学过程帮助你和其他人以你的发现为基础进行优化——为每个人优化科学。

# 参考

*   统计学习的要素:数据挖掘、推理和预测。纽约:统计学中的斯普林格系列。2009.

# 关于作者

Eric J. Daza 是数字健康领域的数据科学统计学家。他获得了应用统计学硕士学位和生物统计学博士学位，在临床试验、公共卫生和行为改变研究方面拥有 18 年以上的工作经验。Daza 博士为用于个性化健康建议的个人内(即，n-of-1，单病例，独特的)数字健康研究开发因果推断方法。| ericjdaza.com[🇺🇸🇵🇭](https://www.ericjdaza.com/)@埃里克森 linkedin.com/in/ericjdaza|[statsof1.org](https://statsof1.org/)[@ stats of](https://twitter.com/statsof1)[@ fsbiostats](https://twitter.com/fsbiostats)