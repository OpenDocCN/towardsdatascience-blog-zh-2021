# 控制中的不当学习

> 原文：<https://towardsdatascience.com/making-the-case-for-improper-learning-in-control-bb5c5e1692b0?source=collection_archive---------19----------------------->

## 一个众所周知的 ML 技术正在改变强化学习和控制理论

![](img/8c3eb84e9ebd2ba60ba43fc41e60a3de.png)

罗伯特·尼克森在 [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral) 上的照片

历史上，控制系统是这样建立的:首先使用熟知的模型，如线性二次调节器(LQR)或列表马尔可夫决策过程(MDP)，来近似受控系统(或“设备”)，然后为这个假定的模型设计(近)最优控制器。这种方法非常有效，从保持飞机漂浮到防止化工厂爆炸，经典控制理论支持的众多应用领域证明了这一点。事实上，这些领域的研究已经产生了一个现成的设计原则和经验法则的仓库，可以毫不费力地生成一个显示可容忍性能的控制器菜单。

这种经典方法提供的另一个优势是产生的控制策略的可解释性——它们直观上令人满意并且有意义。这就是为什么简单的线性反馈控制器和阈值策略在如此长的时间里仍然是控制理论的支柱，尽管在性能上还有很大的改进空间。相比之下，你将很难向外行人解释，例如，你最喜欢的深度神经网络的 500，000 个权重实际上“意味着什么”，尽管它“有效”。

然而，由于现代控制应用的复杂性或对更快的部署和更好的性能的不断增长的需求，这种方法可能会大大低于预期。例如，缺乏理解的系统——以及由此产生的错误指定的模型——可能会导致实际中控制器的过拟合和欠匹配。或者用这些方法接近最优所需的数据量可能会对性能和部署速度产生不利影响。因此，像自动驾驶车辆和机器人团队这样复杂的系统的出现，现在要求我们对控制的基本问题进行范式转变。

![](img/090822ec558e1acd500285f93d00b0ea.png)

Brian McGowan 在 [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral) 上拍摄的照片

> 像自动驾驶车辆和机器人团队这样复杂的系统的出现，现在要求我们在看待控制的基本问题的方式上进行范式转变。

在这种新的情况下，一种新兴的通用强化学习(RL)方法似乎是采用给定的、预先设计的“基本”(或“原子”)控制器集合的组合，这(a)允许灵活地组合给定的控制器，以获得比原子策略更丰富的策略(我们将互换地使用术语“策略”和“控制器”)，同时(b)可以保留给定控制器类的基本结构，并赋予所得到的混合策略高度的可解释性。这里，我们使用术语*策略*的通常含义，即，为工厂的每个状态输出一个动作的映射。

**什么是不当学习？**在机器学习的说法中，给定一类控制器(或假设或分类器，视情况而定)，从可用的算法中严格挑选的算法被称为*合适的学习器*，从(潜在地)给定类之外输出的算法被称为*不合适的学习器*【2】。一个简单的例子是使用有限的一组 *N* 线性预测器，即 *N* 权重向量的分类问题。在这种情况下,(训练后)总是从给定的 *N* 个预测器中选出最佳预测器的学习算法将被称为合适的学习器。或者，该算法可以从这个集合的凸包中输出最好的，然后将被称为不适当的学习器。多年来，虽然不当学习(IL)在统计和在线学习社区中受到了一些关注，但它在很大程度上仍未得到控制。因此，这篇文章的目的之一是让研究者和实践者注意到这项技术是一个有前途的和及时的研究方向。

**统计学习。**不当学习已经侵入了统计学习领域，其结果非常明显地令人鼓舞。一个明显的例子是*增强*的技术，在【5】的分类上下文中进行了彻底的探索。著名的 *AdaBoost* 算法现在已经适用于从分类到算法交易的机器学习的每一个可以想象的应用领域，甚至为其发明者赢得了 2003 年的 G *o* del 奖。最近，例如，在[3]的监督学习的背景下，调查了*模型错误设定*的问题，这也表明，即使在使用不正确的参数模型时，不正确的学习者的后悔表现也有显著改善(正确的学习者表现更差)。类似地，最近在[4]中调查了使用 T10 逻辑回归 T11 的有限时间后悔问题，其中，后悔表现再次通过利用不当学习得到显著改善。请注意，在这两种情况下，学习者只需扩展其搜索，以包括可用的原子预测器的凸组合。

**控制中学习不当。**不当学习开始受到控制群体的关注，并且已经可以观察到两种不同的方法。第一种[6]遵循上述范例，使用(非自适应)控制策略的基本类或原子类以及自适应元学习器，该自适应元学习器组合这些策略的输出，以产生性能严格优于基本类的不适当控制器。事实上，[6]也展示了稳定控制器从一组不稳定的原子控制器中出现的例子。重要的是，不合适的控制器仅通过基础控制器*至*与受控系统交互，即它在每一轮中选择一个基础控制器，该控制器进而对系统实施其控制动作。从自适应算法中产生的控制策略不需要与每个系统状态的任何基本策略精确匹配，因此显然是不合适的。

另一方面，第二种方法[1]实质上是将增压的概念扩展到控制。这包括维护一组“弱学习”算法的实例(例如在线梯度下降)。弱学习者被认为是自适应的，并向非自适应助推器提供控制建议。反过来，助推器将这些建议组合成一个控制动作，在受控系统上实施。与之前相同的逻辑表明了为什么强化者可以被认为是一个不合适的学习者。但是请注意，这里的架构本质上与[6]中的架构相反——控制器是非自适应的，直接与系统交互。

**向前移动。虽然这些初步尝试看起来很有希望，但仍有很大的改进空间。例如，上面描述的两种架构是仅有的两种可能吗？对于给定的应用程序，有没有一种原则性的方法来选择基类？在理论方面，关于后悔界限和收敛速度的问题比比皆是。如何将这一理论扩展到包含多个学习代理的情况？因此，许多非常基本的理论和实践问题仍然是开放的，并为研究人员提供了激动人心的机会，以推动这一新兴控制领域的发展。**

**关键词:**不当学习，强化学习，Boosting，MetaRL，AdaBoost。

# **参考文献**

[1]纳曼·阿加瓦尔、纳塔利·布鲁希姆、埃拉德·哈赞和周露。*助力控制动力系统。*机器学习国际会议，第 96–103 页。PMLR，2020 年。

[2]维基百科贡献者。*分布学习理论* —维基百科，免费百科. https://en . Wikipedia . org/w/index . PHP？title =分配 _ 学习 _ 理论，2020。

[3]约翰·杜奇、安妮·马斯登和格雷戈里·瓦兰特。*关于预测问题中的错误设定和通过不当学习的鲁棒性*。arXiv 预印本 arXiv:2101.05234，2021。

[4]Dylan J . Foster、Satyen Kale、Haipeng Luo、Mehryar Mohri 和 Karthik Sridharan。*逻辑回归:不当的重要性。*学习理论会议，第 167-208 页。PMLR，2018。

[5]罗伯特·沙皮雷和约夫·弗罗因德。*助推:基础与算法。麻省理工学院出版社。, 2013.*

[6]穆罕默迪·扎基、阿维·莫汉、阿迪蒂亚·戈帕兰和阏氏·曼诺尔。*基于梯度的策略优化学习不当。arXiv 预印本 arXiv:2102.08201，2021。*