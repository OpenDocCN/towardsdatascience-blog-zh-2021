# 人工智能领导人应该如何为即将到来的欧盟法规做准备

> 原文：<https://towardsdatascience.com/how-ai-leaders-should-prepare-for-the-looming-eu-regulations-99e9d4f4c039?source=collection_archive---------39----------------------->

## 新法规对欧盟所有使用“高风险”人工智能的公司施加了义务，包括上市后监控

![](img/e2937623600ea52c969a5383fcc20b18.png)

克里斯蒂安·卢在 [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral) 上的照片

上周，欧盟备受期待的关于欧洲人工智能方法的[法规草案](https://drive.google.com/file/d/1ZaBPsfor_aHKNeeyXxk9uJfTru747EOn/view)被泄露。官方版本预计本周发布。

尽管作为非律师，我们无法对该法规进行法律分析，但我们可以说，该草案清楚地表明，欧盟正准备采取强有力的方法来监管人工智能，从确保良好的数据集，到系统的适当测试和培训，要求在新创建的欧盟数据库中注册，强制性的上市后监测，以及对不遵守者的惩罚。

这项规定有广泛的影响。尽管大多数条款都局限于“高风险”人工智能系统，但“高风险”的定义似乎相当宽泛。草案直接列出了一些“高风险”的人工智能用例，如员工/候选人评估，确定信誉和运营重要的公共基础设施。此外，草案宣布，定义“高风险 AI”的参数将是动态的，委员会有权根据造成伤害的严重性和可能性，在这些定义下包括额外的用例。

时间会告诉我们这一规定的应用范围有多广，以及涵盖了多少不同类型的人工智能系统。然而，由于“高风险”一词的定义非常宽泛，而且该法规鼓励非高风险人工智能的提供商也遵守规定，因此具有前瞻性思维的公司应该现在就开始规划他们将如何遵守这项新的全面法规。

# **那么，你应该做些什么准备呢？**

## 1.是时候改变你的想法了

在整个草案中，“人工智能系统”而不是“模型”是流行的术语。这种从“模型”到“系统”的转变对于数据科学家来说可能具有挑战性，因为传统上，“模型”一直是研究项目的最终目标。此外，数据科学家使用的许多工具都是以模型为中心的。这可能适用于研究阶段，但生产系统通常包含不止一个模型(以及模型之外的许多其他部分)，而且需要注册的是整个人工智能系统(是的，所有高风险的人工智能系统都将在欧盟数据库中注册)。整个人工智能系统需要通过一致性测试。

## 2.变得有条理！

![](img/36ea1a8c127bda88dc0dea976923f84a.png)

Jonah Pettrich 在 [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral) 上的照片

草案明确表示，你的人工智能系统必须是有组织的、透明的、有文档记录的。具体来说，您必须:

*   保存用于训练和测试每个模型版本的数据集
*   为您的系统创建清晰的技术文档，它可以追溯到您在构建系统时所做的每一个决策
*   保持透明，告知你的用户在使用你的人工智能系统时的注意事项

幸运的是，已经存在一系列工具来帮助围绕您的数据集、模型和系统设计进行文档和记录。MLFlow 和其他 ML 平台使您能够管理实验、注册模型和保存训练数据，而最终用户通信工具已经在每个组织中很常见。

## 3.确保人工智能系统的人工监督

![](img/debcbd91e50f5afe34e28c4561d7aa70.png)

照片由[疾控中心](https://unsplash.com/@cdc?utm_source=medium&utm_medium=referral)在 [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral) 拍摄

该法规的一个关键点是，人工智能系统必须有人类的监督。一个人不能仅仅处于“循环”中，他必须在意想不到的事情发生时得到提醒。这个人必须能够理解正在发生的事情，并有能力超越系统。

如今，在成熟的数据驱动流程中，如欺诈检测和风险评估，人类分析师会补充模型和预测。我们预测，在许多其他领域，当然还有“高风险”人工智能驱动的过程中，这样的分析师将接受培训，并配备更好的人机界面工具。

## 4.建立一个智能的去/不去机制，以避免在更新你的人工智能系统时出现偏差

![](img/f477b5f89bb42ace8f710122f4bd2655.png)

照片由 [Eliobed Suarez](https://unsplash.com/@eliobedsuarez?utm_source=medium&utm_medium=referral) 在 [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral) 上拍摄

您的数据质量至关重要。草案要求你确保你的训练和测试数据不包含低质量的数据或偏见。

我们从这些数据质量/偏差相关规定中得出两个结论。首先，你必须超越模型的输入和输出。利用元数据(例如，种族、性别)和业务维度(例如，地理位置)来评估您的数据和整个系统在子群体中的行为，以确保适当的统计表示并避免偏差。第二，团队将需要建立一个健壮的、自动化的过程来验证无论何时发布新版本都没有偏见(并且当存在偏见时阻止发布)。

## 5.实施端到端人工智能监控解决方案

![](img/4f148baaf7757547ed913e979bf343eb.png)

版权所有:莫纳实验室公司(2021)

该法规对售后市场监控系统提出了具体要求。根据草案第 83 节:

> 为了确保他们设计和开发的高风险人工智能系统的使用经验被考虑在内，以改进开发流程或及时采取任何可能的纠正措施，**所有供应商都应建立上市后监控系统**。

如果你一直在关注我们的文章，你会知道我们相信[一个全面的监控策略可以为团队](/the-definitive-guide-to-ai-monitoring-2427812cc1b)带来真正的变化，旨在将他们的人工智能研究投资转化为可扩展的业务运营。

草案特别讨论了生产中“不断学习的人工智能系统”。这是承认人工智能系统的潜在风险会随着时间的推移而出现，并且不能在这些系统最初推出时预先减轻。监控是人工智能环境中安全、可靠的持续进化的关键使能因素的另一个有力论据。

# **在**中**结论**

欧盟正在带头规范和管理庞大的新兴人工智能市场。然而，我们相信，在不久的将来，我们会看到其他政府也出台类似的规定。这对整个人工智能行业的影响是巨大的，现在最好是通过实施正确的工具和流程，提前为这些法规做好准备。

你是在一个团队建设或操作人工智能系统，可以涵盖在这些新的规定？[取得联系](mailto:itai@monalabs.io)。我们很想听听您如何看待这一新的监管格局。

*感谢* [*约坦·柳文欢*](https://www.linkedin.com/in/yotam-oren-4955733/) *与我合作。原帖*[*blog . monalabs . io*](http://blog.monalabs.io)*。*