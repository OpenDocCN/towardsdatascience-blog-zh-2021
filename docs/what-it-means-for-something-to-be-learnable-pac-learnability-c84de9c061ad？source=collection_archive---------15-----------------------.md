# 用 PAC 框架定义可学性

> 原文：<https://towardsdatascience.com/what-it-means-for-something-to-be-learnable-pac-learnability-c84de9c061ad?source=collection_archive---------15----------------------->

## 在机器学习中，我们经常说某些东西是“可学习的”。这到底意味着什么？请继续阅读，寻找答案。

![](img/8348214ec7bcebca4d8409011d20e2a1.png)

Lyman Hansel Gerona 在 [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral) 上拍摄的照片

是什么让一个特定的函数或一组函数“可以学习”呢？从表面上看，这似乎是一个简单的问题。一个简单的答案是:如果有某种训练算法可以在训练集上训练并在测试集上实现低错误，那么函数是可学习的。毕竟，这就是绝大多数机器学习算法的工作方式。

这个定义够好了吗？它有一个主要问题，特别是训练集。我们说的是什么训练组合？想象一个非常不幸的训练集，它由一个重复多次的例子组成。任何看到该训练集的机器学习算法都会很好地学习该特定示例，但不会学习其他任何东西。因此，测试集误差(或者用更正式的术语来说，泛化误差)将会很高。我们看到，任何算法的性能都取决于它所训练的样本的质量。

如果一个函数只能在几个特定的训练集上很好地训练，我们就不能说它是可学习的，即使算法在那几个训练集上取得了很大的泛化误差。因此，我们需要在可学习的定义中添加一个警告，即算法必须在许多可能的训练集上工作。更正式的说法是:**如果存在一个算法，当该算法在随机选择的训练集上训练时，我们得到良好的泛化误差，那么函数是可学习的。**

我们刚刚提出的定义是对可学习性最流行的理论定义的一种非正式表述方式，即 **PAC 可学习性**。PAC 代表“**大概近似正确**”。“大概”对应于我们非正式定义的第一部分(**高概率**，当那个算法在随机选择的训练集上训练的时候)，而“近似正确”对应于第二部分(我们得到**良好的泛化误差**)。

从数学上讲，PAC 可学习性的设置是这样的。我们要学习一个函数 f。用于训练 f 的样本选自分布 d。m 是样本大小。e 是误差函数。δ和ε都是 0 到 1 之间的实数。最后，A 是任意学习算法。我们说 f 是 PAC 可学习的，如果对于任意δ和ε，存在一个学习算法 A，并且样本大小 m 是 1/δ和 1/ε的多项式，使得 P_D(E(A(S)) < ε) > 1-δ。让我们一步一步地了解这意味着什么。

首先，我们有语句 P_D(E(A(S)) < ε) > 1-δ。说白了，这就是说当训练样本 S 按分布 D 抽取时，泛化误差小于ε的概率大于 1-δ。理想情况下，我们希望ε和δ是什么？我们希望泛化误差尽可能小(我们希望ε小)，我们也希望泛化误差小的概率尽可能高(我们希望 1-δ大，因此我们希望δ小)。现在我们转到 PAC 学习陈述的第一部分:“对于任何δ和ε，存在一个学习算法 A，并且样本大小 m 是 1/δ和 1/ε中的多项式”。这意味着，对于任意高的概率和低的误差(任意小的δ和ε),我们总是可以找到一个学习算法 A 和一个样本大小 m 来实现这种高概率和低误差。

最后，我们要求 m 是 1/δ和 1/ε中的多项式。这意味着 m 可以表示为 1/δ和 1/ε的多项式函数。一个可能的例子是 m = 2*(1/δ) + 3*(1/ε) + 4*(1/εδ)。从直觉上讲，m 应该是 1/δ和 1/ε的增函数是有道理的——较小的δ和ε对应于更好的学习，因此实现更好的学习所需的样本量应该增加。最后，我们要求 m 是多项式函数，而不是任何函数。这是为了给样本量 m 设定一个合理的上限，想象一下，如果我们有一个学习算法，它根据这个等式依赖于样本量:m = 10^(1/δ + 1/ε)。对于较小的δ和ε值，比如δ = 0.1，ε = 0.1，m = 10 ⁰.这显然是不合理的，所以我们认为这样的情况是不可学习的。多项式函数确实会增加，但没有这么快。

我们已经讨论了 PAC 可学性的数学定义的每一部分。现在我们试着用直观的方式总结一下。学习基本上有两个重要的目标:低泛化误差，以及实现低泛化误差的高概率。因此，如果某样东西是可以学习的，我们应该能够用一个合理的(多项式)样本量实现这两个目标。这是我们对 PAC 可学性的直观总结——**如果我们可以用一个多项式样本大小实现高概率低泛化误差，那么一个函数就是 PAC 可学的。**

在这篇文章中，我们首先讨论了是什么让某些东西“可以学习”。这让我们得到了 PAC 可学性的数学形式定义，我们对此进行了深入解释。最后，我们总结了 PAC 可学性的本质。在下一篇文章中，我们将看看 PAC 学习的应用，换句话说，一些如何证明事物是 PAC 可学习的例子。敬请关注，并请留下任何问题/评论。另外，如果你对机器学习概念感兴趣，可以看看我的其他文章。感谢阅读！