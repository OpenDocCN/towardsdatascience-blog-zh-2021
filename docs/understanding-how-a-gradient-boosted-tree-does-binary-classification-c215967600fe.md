# 了解梯度增强树如何进行二叉分类

> 原文：<https://towardsdatascience.com/understanding-how-a-gradient-boosted-tree-does-binary-classification-c215967600fe?source=collection_archive---------10----------------------->

## 使用银行券数据集，在 LightGBM 和 R 中从数据到预测的逐步重新计算

![](img/1e3676d47f24b2c13d537f11f32489e8.png)

由 [Eric Prouzet](https://unsplash.com/@eprouzet?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText) 在 [Unsplash](https://unsplash.com/s/photos/banknote?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText) 上拍摄的照片

几周前，我深入研究了梯度增强模型如何工作。我提出的主要观点是，人们不必对梯度增强模型(如 XGBoost 和 LightGBM)的“黑盒”感到恐惧，理解模型正在做什么实际上是相当简单的。

在我的例子中，我使用泊松回归作为目标，现在我认为用不同的、**更常用的目标:二元分类**来重复这个练习会很有趣。

在这篇文章中，我们将:

1.  训练二元分类模型，
2.  重新计算二进制日志丢失和二进制错误度量，
3.  检查预测概率和原始模型结果之间的联系，
4.  根据梯度和 hessian 公式重新计算模型结果。

我们不会深入到每一个小细节，相反，我们将把重点放在二元分类目标的特殊性上。我推荐我之前关于这个话题的[帖子](/this-weeks-unboxing-gradient-boosted-models-black-box-138c3a0c6d80)作为起点。

# 环境

我们将在 LightGBM 包中工作，编程在 r 中完成。(不用说，我们讨论的所有内容都应该适用于 Python，以及其他梯度增强包，如 XGBoost。)

完整脚本请参考我的 [GitHub](https://github.com/MatePocs/quick_projects/blob/main/lightgbm_binary.R) 。

我们将要使用的数据是钞票数据集，可以在 [UCI 机器学习库](https://archive.ics.uci.edu/ml/datasets/banknote+authentication)上免费获得。该数据包含 1372 张钞票的信息，目标是找出它们是否是伪造的。

# 数据

假设数据存储在`data`文件夹中，为 LightGBM 准备数据的脚本:

我们有一个 LightGBM 可以使用的`dtrain`对象。

# 训练模型

我们现在可以这样训练模型:

注意事项:

*   在现实生活中，**你永远不会训练出这样的分类模型**。我们有连续的特征，这意味着模型可以完美地过度拟合，除非我们用某种形式的正则化来限制它。然而，在这个项目中，我们不关心这个，我们只是想理解计算。
*   超参数应该非常简单，除了`sigmoid`。这是一个特定于二进制分类的参数，我们将很快看到它是如何在计算中使用的。

# 预言

我们将需要使用下面的模型预测，让我们来解决这个问题。

注意事项:

*   `num_iteration`定义我们希望包含在预测中的树的数量。通过将它设置为 1，我们在仅一轮后就获得了模型预测的结果。
*   `rawscore = TRUE`在将原始模型结果转换为预测之前返回它们。在二元分类的情况下，这意味着返回的是对数几率而不是概率。
*   来自分类模型的预测将是概率，而不是实际的标签。现在，我们将通过选择 0.5 作为阈值来进行预测，即 1 的概率超过 0.5 的每个观察值都将被预测为 1。在实际项目中，您会花很多心思来选择正确的阈值。

# 误差度量

我们在模型中要求两个错误度量:二进制错误和二进制日志丢失。这些指标是在模型训练期间打印出来的。我们也可以使用`lgb.get.eval.result`函数来获取它们(用`booster = bst`、`data_name = “train”`、`eval_name = “binary_logloss”`或`eval_name = “binary_error”`)。

## 二进制错误

第一轮后的`binary_error`度量是`0.08454810`，第二轮后是`0.07069971`。这只是数据中不正确预测的比率。例如，在第二轮中，这是实际与预测表:

```
 predict label   N
1:       0     0 734
2:       0     1  69
3:       1     0  28
4:       1     1 541
```

我们可以将误差计算为:

```
 (69 + 28) / 1372 = 0.07069971
```

我们希望这个度量是一个低值，如果我们让模型训练更多轮次，最终，它将达到 0。(同样，这是因为我们现在没有使用任何规范化。)

## binary_logloss

该指标的值在第一轮后为`0.5102119`，在第二轮后为`0.3966159`。`binary_logloss`计算为观察值的单个二元对数损失的负平均值。观察值的二进制对数损失就是目标值为 1 的观察值的预测概率的对数，以及 1 的对数减去目标值为 0 的观察值的预测概率。

我们可以将二进制对数丢失列添加到`data.table`中，就像第二轮那样:

然后像这样计算 binary_logloss:

```
-sum(dt[,binary_logloss_nround2])/dt[,.N] = 0.3966159
```

当然，第一轮结果也可以重复同样的过程。

# 预测的叶分数

让我们来看看叶子分数是如何变成预测的！如果我们在第一轮后打印出独特的预测(原始分数和概率)，我们会得到以下结果:

```
 predict_proba_nround1 predict_rawscore_nround1   N
1:             0.3373846               -0.9642444 679
2:             0.3702293               -0.7589048 105
3:             0.5472543                0.2708327  37
4:             0.5905585                0.5232494 551
```

第一列包含实际预测，即概率，第二列是原始分数。

## 公式

为了将原始分数转换为预测值，我们使用所谓的 **sigmoid 函数**:

```
1 / (1 + exp(-0.5232494 * 0.7)) = 0.5905585
```

与此相反的是 **logit 函数**:

```
log(0.5905585 / (1 - 0.5905585)) / 0.7 = 0.5232495
```

注意事项:

*   上面公式中的`0.7`是我们在训练模型时设置的`sigmoid`参数。它在 LightGBM [参数文档](https://lightgbm.readthedocs.io/en/latest/Parameters.html)中被称为“sigmoid”，但是我找不到关于这个参数的任何数学背景。**如果你知道这个参数来自哪里，请给我发消息。嗯，我们知道它的用法，这很了不起！**

## 术语摘要

我发现这里的术语有些混乱，让我快速回顾一下:

*   **概率:** `p`
*   **赔率** : `(p/(1–p))`
*   **对数赔率** : `log((p/(1–p)))`

模型中的原始分数是对数比除以 sigmoid 参数。

## 在树形图中查找分数

这些都很好，但是这些价值实际上来自哪里呢？

如果我们查看`lgb.model.dt.tree(bst)`表，可以很容易地通过叶子计数得到它们:这些是第一棵树中的值(`tree_index = 0`)。打印出树形图表格的一部分，我们得到这个:

```
 tree_index leaf_index leaf_value leaf_count
1:          0          0  0.5232494        551
2:          0          2 -0.7589048        105
3:          0          1  0.2708327         37
4:          0          3 -0.9642444        679
```

这与我们之前看到的原始分数一致。

# 叶分数计算

现在我们知道了指标是如何计算的，分数是从哪里来的，以及分数是如何转换成概率的。我们唯一缺少的是分数本身。

## 出发点

在模型甚至开始考虑分裂之前，它将计算一个基线预测，所有观察都将从该基线预测开始。

运行`lgb.train`功能后，您可以在终端输出中注意到这一行:

```
[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.444606 -> initscore=-0.317839
```

我们需要的线索差不多就这些了。训练数据中标签的平均值为`0.444606`，因此预测将从以下位置开始:

```
log(0.4446064 / (1 - 0.4446064)) / 0.7 = 0.3178395
```

我认为这很简单:在做任何分割之前，任何一个单独的观察都有一个属于第一组的`0.4446064`概率。哦，`0.7`又是那个讨厌的`sigmoid`参数。

## 第一轮目标

如果我们观察第一棵树，我们会看到它在:

*   `variance = 0.30942`
*   `skewness = 7.60565`
*   `curtosis = -4.48625`

所以在第一轮之后，我们会有四组不同的观察，每组都有不同的预测，基于上面的三个分裂。让我们试着重现这些叶子是如何得分的！

第一轮有点特殊，因为所有的观察仍然有相同的起始分数:我们在上一节看到的`0.3178395`。

先说`variance <= 0.30942`和`skewness <= 7.60565`所在的组。该组有`551`个观察值，其中`512`的标签为 1，`39`的标签为 0。我们还知道这组得到了一个`0.5232494`的预测。这就是我们想要复制的数字。

## 第一轮梯度和 Hessian

我们不能再推迟了，我们必须钻研梯度和黑森计算。你可以从理论的角度试一试，但是我有点困惑。梯度和 hessian 被认为是目标函数相对于预测的一阶和二阶导数，但是我很难将`sigmoid`参数放入混合中...

所以我认为**最直接的方法就是看一下 LightGBM** 的 [**源代码**](https://github.com/microsoft/LightGBM/blob/d517ba12f2e7862ac533908304dddbd770655d2b/src/objective/binary_objective.hpp) **。我们需要的位在`GetGradients`函数中。(一要小心，`label`这里跟你原来的目标不一样。对于目标为 1 的观测值，`label`为 1，但对于目标为 0 的观测值，`label`为-1。为了避免混淆，我将我们想要预测的值称为目标。)**

在这个术语中，这个分数是当前的预测，将是我们在第一轮所有观察中的`0.3178395`。梯度和 hessian 的主要构件是所谓的`response`值。

*   对于**目标为 1** 的观测值，`response`计算如下:

```
-1 * 0.7 / (1 + exp(1 * 0.7 * -0.317839)) = - 0.3887755
```

*   对于**目标为 0** 的观察值，响应计算如下:

```
1 * 0.7 / (1 + exp(-1 * 0.7 * -0.317839)) = 0.3112245
```

从这些值中，我们可以很容易地计算出这个组的梯度和 hessian:我们只需将观察值的各个梯度和 hessian 相加。

根据这些值，新的叶子分数是这样计算的:

```
- (gradient / hessian) * 0.3 + (-0.317839) = 0.5232497
```

注意:

*   上面公式中的`0.3`就是`learning_rate`。
*   `512`和`39`是受检组中目标值为 1 和 0 的观察次数。
*   注意我们是如何将起始共享预测`-0.317839`添加到`leaf_score`中的。这可能是特定于 LightGBM 的事情…本质上，总分数因此在第一轮中被滚动到叶分数中。在随后的回合中，不需要公式的这一部分。我们将为第 2 轮做一个例子来看看区别。
*   如果我们有权重，这些公式会更长。我不想让事情变得不必要的复杂，请参考[源代码](https://github.com/microsoft/LightGBM/blob/d517ba12f2e7862ac533908304dddbd770655d2b/src/objective/binary_objective.hpp)进行计算。请记住，这里有两个可能的权重:`label_weight`是您对正确预测目标 1 或 0 的重视程度，而`weights[i]`是指您通过观察定义的权重。

## 第二轮目标

看看能不能把这个知识转移到第二棵树上。如果我们观察树的结构，我们可以看到，与第一个不同，这个结构是不对称的:第一次分裂的一个节点变成了一片叶子。我们会观察这个群体。通过打印`variance > 0.77605`所在组中唯一的`predict_rawscore_nround1`和`predict_rawscore_nround2`，这就是我们得到的表格(具体如何得到表格，请参考我的 [GitHub repo](https://github.com/MatePocs/quick_projects/blob/main/lightgbm_binary.R) ):

```
 label predict_rawscore_nround1 predict_rawscore_nround2   N
1:     0               -0.9642444               -1.4947746 575
2:     0                0.2708327               -0.2596975   8
3:     1               -0.9642444               -1.4947746  23
4:     1                0.2708327               -0.2596975  20
```

第一列和第二列之间的差异是一个常数，`-0.5305303`，这就是我们在树形图中看到的该组的叶分数。这就是我们想要复制的数字。

## 第二轮渐变和 Hessian

与第一轮计算的唯一区别是，我们不再有一个共同的预测(如上表所示)，因此计算时间有点长。请注意，我按组重新计算一切，以更好地展示正在发生的事情，但从技术上讲，您可以单独计算每个观察值的梯度和 hessian，然后将它们相加。

根据这些值，叶分数值计算如下:

```
- (gradient / hessian) * 0.3 = -0.5305302
```

# 摘要

在这篇文章中，我们重新计算了 LightGBM 在进行二进制分类时计算的指标、得分和预测。

我认为从这样的练习中得到的主要收获是**对超参数如何影响模型训练的更深理解**。对`learning_rate`做了什么有一个大概的了解是一回事，自己重新计算这些结果是完全不同的另一回事。

**我不是说像 XGBoost，LightGBM 等型号。在任何方面都很简单。我们没有触及非常重要的一点:分割点是如何确定的。这些决定是最重要的，这些车型在这方面实现的**性能提升确实令人惊讶**。然而，我的观点是，一旦这些分裂点被确定，大部分的工作基本上只是一长串的算法，基于你，用户设置的超参数。**

[](https://matepocs.medium.com/membership) [## 加入我的推荐链接-伴侣概念

### 作为一个媒体会员，你的会员费的一部分会给你阅读的作家，你可以完全接触到每一个故事…

matepocs.medium.com](https://matepocs.medium.com/membership)