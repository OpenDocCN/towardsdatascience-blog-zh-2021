# æ•°æ®æ“ä½œç†ŠçŒ«-PySpark è½¬æ¢æŒ‡å—

> åŸæ–‡ï¼š<https://towardsdatascience.com/data-manipulation-pandas-pyspark-conversion-guide-abed50a818?source=collection_archive---------32----------------------->

## ä½¿ç”¨ PySpark on Databricks è¿›è¡Œæ¢ç´¢æ€§æ•°æ®åˆ†æ(EDA)æ‰€éœ€çš„ä¸€åˆ‡

![](img/ca705709bd91a9310cedf0cd600e9c58.png)

è¨ç±³Â·ç±³æç‰¹é²å¤šåœ¨ [Unsplash](https://unsplash.com/s/photos/red-and-white-bricks?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText) ä¸Šçš„ç…§ç‰‡

åœ¨å¤„ç†å¤§æ•°æ®æ—¶ï¼ŒJupyter notebook ä¼šå¤±è´¥ã€‚è¿™ä¹Ÿæ˜¯ä¸€äº›å…¬å¸ä½¿ç”¨ ML å¹³å°å¦‚ Databricksï¼ŒSageMakerï¼ŒAlteryx ç­‰çš„ä¸€ä¸ªåŸå› ã€‚ä¸€ä¸ªå¥½çš„ ML å¹³å°æ”¯æŒä»æ•°æ®æ‘„å–åˆ°å»ºæ¨¡å’Œç›‘æ§çš„æ•´ä¸ªæœºå™¨å­¦ä¹ ç”Ÿå‘½å‘¨æœŸï¼Œä»è€Œæé«˜å›¢é˜Ÿçš„ç”Ÿäº§åŠ›å’Œæ•ˆç‡ã€‚åœ¨è¿™ä¸ªç®€å•çš„æ•™ç¨‹ä¸­ï¼Œæˆ‘å°†åˆ†äº«æˆ‘å…³äºå°† Pandas è„šæœ¬è½¬æ¢ä¸º Pyspark çš„ç¬”è®°ï¼Œè¿™æ ·æ‚¨ä¹Ÿå¯ä»¥æ— ç¼åœ°è½¬æ¢è¿™ä¸¤ç§è¯­è¨€ï¼

# ä»‹ç»

## **ä»€ä¹ˆæ˜¯ç«èŠ±ï¼Ÿ**

Spark æ˜¯ä¸€ä¸ªå¼€æºçš„äº‘è®¡ç®—æ¡†æ¶ã€‚è¿™æ˜¯ä¸€ä¸ªå¯æ‰©å±•çš„ã€å¤§è§„æ¨¡å¹¶è¡Œçš„å†…å­˜æ‰§è¡Œç¯å¢ƒï¼Œç”¨äºè¿è¡Œåˆ†æåº”ç”¨ã€‚Spark æ˜¯å¤„ç† Hadoop æ•°æ®çš„å¿«é€Ÿè€Œå¼ºå¤§çš„å¼•æ“ã€‚å®ƒé€šè¿‡ Hadoop YARN æˆ– Spark çš„ç‹¬ç«‹æ¨¡å¼è¿è¡Œåœ¨ Hadoop é›†ç¾¤ä¸­ï¼Œå®ƒå¯ä»¥å¤„ç† HDFSã€HBaseã€Cassandraã€Hive å’Œä»»ä½• Hadoop InputFormat ä¸­çš„æ•°æ®ã€‚å®ƒæ—¨åœ¨æ‰§è¡Œä¸€èˆ¬çš„æ•°æ®å¤„ç†(ç±»ä¼¼äº MapReduce)å’Œæ–°çš„å·¥ä½œè´Ÿè½½ï¼Œå¦‚æµã€äº¤äº’å¼æŸ¥è¯¢å’Œæœºå™¨å­¦ä¹ ã€‚ä½ å¯ä»¥åœ¨è¿™é‡Œé˜…è¯»æ›´å¤šå…³äº Mapreduce çš„å†…å®¹:[https://medium . com/@ francescomandru/Mapreduce-explained-45a 858 C5 ac1d](https://medium.com/@francescomandru/mapreduce-explained-45a858c5ac1d)(P . S æˆ‘æœ€å–œæ¬¢çš„è§£é‡Šä¹‹ä¸€ï¼)

## **ä»€ä¹ˆæ˜¯æ•°æ®å—ï¼Ÿ**

Databricks ä¸ºæ‚¨çš„æ‰€æœ‰æ•°æ®æä¾›äº†ä¸€ä¸ªç»Ÿä¸€çš„å¼€æ”¾å¹³å°ã€‚å®ƒä¸ºæ•°æ®ç§‘å­¦å®¶ã€æ•°æ®å·¥ç¨‹å¸ˆå’Œæ•°æ®åˆ†æå¸ˆæä¾›äº†ä¸€ä¸ªç®€å•çš„åä½œç¯å¢ƒã€‚å°±æ•°æ®æœåŠ¡è€Œè¨€ï¼Œå®ƒæ˜¯å¸‚åœºé¢†å¯¼è€…ä¹‹ä¸€ã€‚å®ƒç”± Ali Gozii äº 2013 å¹´åˆ›å»ºï¼Œä»–æ˜¯é˜¿å¸•å¥‡ç«èŠ±ä¸‰è§’æ´²æ¹–å’Œ MLflow çš„åŸå§‹åˆ›é€ è€…ä¹‹ä¸€ã€‚å®ƒæ¥è‡ªä¸€äº›ä¸–ç•Œä¸Šæœ€å—æ¬¢è¿çš„å¼€æºé¡¹ç›®çš„åŸå§‹åˆ›å»ºè€…ï¼ŒApache Sparkã€Delta Lakeã€MLflow å’Œ Koalasã€‚å®ƒå»ºç«‹åœ¨è¿™äº›æŠ€æœ¯ä¹‹ä¸Šï¼Œæä¾›äº†ä¸€ä¸ªçœŸæ­£çš„æ¹–å±‹æ¶æ„ï¼Œç»“åˆäº†æœ€å¥½çš„æ•°æ®æ¹–å’Œæ•°æ®ä»“åº“ï¼Œå½¢æˆäº†ä¸€ä¸ªå¿«é€Ÿã€å¯æ‰©å±•å’Œå¯é çš„å¹³å°ã€‚ä¸“ä¸ºäº‘æ„å»ºï¼Œæ‚¨çš„æ•°æ®å­˜å‚¨åœ¨ä½æˆæœ¬çš„äº‘å¯¹è±¡å­˜å‚¨ä¸­ï¼Œå¦‚ AWS s3 å’Œ Azure data lake storageï¼Œé€šè¿‡ç¼“å­˜ã€ä¼˜åŒ–çš„æ•°æ®å¸ƒå±€å’Œå…¶ä»–æŠ€æœ¯å®ç°æ€§èƒ½è®¿é—®ã€‚æ‚¨å¯ä»¥å¯åŠ¨åŒ…å«æ•°ç™¾å°æœºå™¨çš„é›†ç¾¤ï¼Œæ¯å°æœºå™¨éƒ½æ··åˆäº†æ‚¨çš„åˆ†ææ‰€éœ€çš„ CPU å’Œ GPUã€‚

# ä¸»è¦æ¦‚å¿µ

**æ¢ç´¢æ•°æ®-** ä¸‹è¡¨æ€»ç»“äº†ç”¨äºè·å¾—æ•°æ®æ¦‚è§ˆçš„ä¸»è¦å‡½æ•°ã€‚

```
 Pandas      |            PySpark               
 -------------------------|------------------------------------ 
  pd.read_csv(path)       | spark.read.csv(path)
  df.shape                | print(df.count(), len(df.columns)) 
  df.head(10)             | df.limit(10).toPandas()            
  df[col].isnull().sum()  | df.where(df.col.isNull()).count()
  df                      | Display(df)
```

# æ•°æ®é¢„å¤„ç†

*   **åˆ é™¤é‡å¤é¡¹-** åˆ é™¤æ‰€é€‰å¤šåˆ—ä¸­çš„é‡å¤è¡Œ
*   **è¿‡æ»¤-** æˆ‘ä»¬å¯ä»¥æ ¹æ®ä¸€äº›æ¡ä»¶è¿‡æ»¤è¡Œ
*   **æ›´æ”¹åˆ—-** æ›´æ”¹åˆ—åï¼Œè½¬æ¢æ•°æ®ç±»å‹ï¼Œåˆ›å»ºæ–°åˆ—
*   **æ¡ä»¶åˆ—-****åˆ—å¯ä»¥ä½¿ç”¨ä¸‹é¢çš„ R å‘½ä»¤é’ˆå¯¹ä¸€ç»„ç‰¹å®šçš„æ¡ä»¶å–ä¸åŒçš„å€¼**
*   ****æ’åº-** å¯¹æ•°æ®è¿›è¡Œæ’åº**
*   ****æ—¥æœŸæ—¶é—´è½¬æ¢-** åŒ…å«æ—¥æœŸæ—¶é—´å€¼çš„å­—æ®µä»å­—ç¬¦ä¸²è½¬æ¢ä¸ºæ—¥æœŸæ—¶é—´**
*   ****Groupby -** æ•°æ®å¸§å¯ä»¥æ ¹æ®ç»™å®šçš„åˆ—è¿›è¡Œèšåˆ**
*   ****Join()** -å°†åˆ—è½¬æ¢ä¸ºé€—å·åˆ†éš”çš„åˆ—è¡¨**
*   ****å¡«å…… nan å€¼-** ç”¨å€¼æ›¿æ¢åˆ—ä¸­ç¼ºå¤±çš„ç©ºå€¼**

```
 Pandas       |              PySpark               
 -------------------------|------------------------------------         
  df.drop_duplicates()    | df.dropDuplicates()
  df.drop(xx,axis=1)      | df.drop(xx)
  df[['col1','col2']]     | df.select('col1','col2')
  df[df.isin(xxx)]        | df.filter(df.xx.isin()).show()
  df.rename(columns={})   | df.withColumnRenamed('col1','col2')
  df.x.astype(str)        |df.withColumn(x,col(x).cast(StringType())
  df.sort_values()        | df.sort() or df.orderby()
  np.datetime64('today')  | current_date()
  pd.to_datetime()        | to_date(column, time_format)
  df.groupby()            | df.groupBy()
  ','.join()         |','.join(list(df.select[col].toPandas()[col]))
  df.fillna(0)            | df.x.fill(value=0)
```

# **æ•°æ®å¸§è½¬æ¢**

*   ****åˆå¹¶æ•°æ®å¸§-** æˆ‘ä»¬å¯ä»¥æ ¹æ®ç»™å®šçš„å­—æ®µåˆå¹¶ä¸¤ä¸ªæ•°æ®å¸§**
*   ****è¿æ¥æ•°æ®å¸§-** å°†ä¸¤ä¸ªæˆ–å¤šä¸ªæ•°æ®å¸§è¿æ¥æˆä¸€ä¸ªæ•°æ®å¸§**

```
 Pandas        |             PySpark               
 -------------------------|------------------------------------ 
  df1.join(df2, on=,how=) | df1.join(df2,df1.id=df2.id, how='')
  pd.concat([df1, df2])   | df1.unionAll(df1,df2)
```

# **ç»“è®º**

**å¸Œæœ›è¿™ä¸ªç®€å•çš„å¯¹æ¯”å¤‡å¿˜å•å¯ä»¥å¸®åŠ©ä½ æ›´å¿«çš„ä¸Šæ‰‹ PySpark å’Œ Databricksã€‚ä»¥ä¸Šå¯¹æ¯”åªæ˜¯èµ·ç‚¹ï¼æ›´å¤šç»†èŠ‚å’Œä¾‹å­ï¼Œè¯·æŸ¥çœ‹è¿™ä¸ª [Pyspark åˆå­¦è€…æ•™ç¨‹é¡µé¢](https://sparkbyexamples.com/)ã€‚å½“è°ˆåˆ°å­¦ä¹ ä»»ä½•æ–°çš„è¯­è¨€æˆ–å·¥å…·æ—¶ï¼Œæœ€å¥½çš„å­¦ä¹ æ–¹æ³•å°±æ˜¯å®è·µã€‚æˆ‘å¼ºçƒˆå»ºè®®ä½ è‡ªå·±ä»”ç»†é˜…è¯» pyspark ä»£ç ï¼Œç”šè‡³å¯ä»¥åœ¨ [Databricks](https://databricks.com/) ä¸Šå¼€å§‹ä¸€ä¸ªé¡¹ç›®ï¼ğŸ˜‰**

**å¦‚æœä½ è§‰å¾—è¿™å¾ˆæœ‰å¸®åŠ©ï¼Œè¯·å…³æ³¨æˆ‘ï¼Œçœ‹çœ‹æˆ‘çš„å…¶ä»–åšå®¢ã€‚æ•¬è¯·å…³æ³¨æ›´å¤šå†…å®¹ï¼â¤**

**[](/10-tips-to-land-your-first-data-science-job-as-a-new-grad-87ecc06c17f7) [## ä½œä¸ºæ–°æ¯•ä¸šç”Ÿè·å¾—ç¬¬ä¸€ä»½æ•°æ®ç§‘å­¦å·¥ä½œçš„ 10 ä¸ªæŠ€å·§

### ä»æˆ‘çš„æ±‚èŒä¹‹æ—…ä¸­å­¦åˆ°çš„ç»éªŒ

towardsdatascience.com](/10-tips-to-land-your-first-data-science-job-as-a-new-grad-87ecc06c17f7) [](/how-to-prepare-for-business-case-interview-as-an-analyst-6e9d68ce2fd8) [## ä½œä¸ºåˆ†æå¸ˆå¦‚ä½•å‡†å¤‡å•†ä¸šæ¡ˆä¾‹é¢è¯•ï¼Ÿ

### ä½œä¸ºæ•°æ®åˆ†æå¸ˆæˆ–æ•°æ®ç§‘å­¦å®¶ï¼Œæˆ‘ä»¬ä¸ä»…éœ€è¦çŸ¥é“æ¦‚ç‡å’Œç»Ÿè®¡ï¼Œæœºå™¨å­¦ä¹ ç®—æ³•â€¦

towardsdatascience.com](/how-to-prepare-for-business-case-interview-as-an-analyst-6e9d68ce2fd8) [](/10-questions-you-must-know-to-ace-any-sql-interviews-2faa0a424f07) [## æƒ³è¦åœ¨ SQL é¢è¯•ä¸­èƒœå‡ºï¼Œä½ å¿…é¡»çŸ¥é“çš„ 10 ä¸ªé—®é¢˜

### 2021 å¹´ä½ åº”è¯¥çŸ¥é“çš„ SQL é¢è¯•é—®é¢˜

towardsdatascience.com](/10-questions-you-must-know-to-ace-any-sql-interviews-2faa0a424f07)**