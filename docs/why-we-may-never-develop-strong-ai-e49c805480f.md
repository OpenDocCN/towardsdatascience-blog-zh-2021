# 为什么我们永远发展不了 AGI

> 原文：<https://towardsdatascience.com/why-we-may-never-develop-strong-ai-e49c805480f?source=collection_archive---------20----------------------->

## 以及为什么我们仍然会

![](img/13afa91db98fc497fffb293a36a41a7d.png)

约书亚·索蒂诺在 [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral) 上拍摄的照片

如果你还没有听说人类对实现强大人工智能(或最近被称为 AGI)的巨大追求，你一定是生活在岩石下。每天，我们都被宣布人工智能研究取得最新突破的新闻轰炸，我们被告知超级智能机器的时代即将到来。像 DeepMind 和 OpenAI 这样的公司正在努力使这一追求成为现实，像 Elon Musk 或 Max Tegmark 这样的人担心人工智能会使人类灭绝(幸运的是，到那时我们可能会逃到火星)。

有些人甚至不同意强人工智能在理论上的可实现性。有些人甚至说，人类和我们能制造的任何机器之间有一个根本的区别，无论我们如何努力，总会有一些领域人类会更胜一筹。

在这篇文章中，我将提出这些人用来破坏 AGI 发展的五个主要论点，并对这些论点提出反驳。

# 由于哥德尔的不完全性定理，人工智能可能从根本上受到限制

我们的第一个论点是一个老论点:事实上，它最早是在 1961 年由已故哲学家 J. R .卢卡斯提出的。潜在地，它甚至比那更古老，可能起源于杰出的奥地利数学家库尔特·哥德尔。

这一论点最重要的部分是哥德尔的不完全性定理，这就是为什么我将尝试解释它们。原则上，通过公理化数学思维，**哥德尔表明，在任何足够强大的形式(公理化)算术系统 F 中，F 中有不能在 F 中被证明或被证伪的陈述，即该系统是不完整的。**这意味着给定数论和命题逻辑(形式系统 F)的任何公理，都不可能构造一台输出 F 的所有定理的计算机。哥德尔展示这一点的方式基本上是首先假定一个算术形式系统，然后**在这个形式系统中构造一个特殊的句子**(恰当地称为**哥德尔句子 G** )来表达 F 中的这个陈述:

**哥德尔句 G:“我不是 f 的定理”**

这是一个自我指称的陈述，感觉有些自相矛盾:这个句子暗示了它自己的非定理性。它显然不可能是 F 的一个定理，因为如果它是，它将导致一个矛盾。不幸的是，如果它不是一个定理，那么 F 就是不完全的，因为完全性要求对于 F 的所有陈述，要么陈述本身要么它的否定是一个定理。如果 G 不是 F 的一个定理，那么它表达了一个不能用 F 证明的真陈述。

不完备性定理也是以下述方式循环的:我们可能把 G 作为附加公理加到 F 上，试图“塞住漏洞”，使系统完备。不幸的是，由于与之前完全相同的论证，这一点失败了:我们再次构造了另一个哥德尔句子 G’:

哥德尔句 G ':“我不是 F+G 的定理。”

即使我们能够在一个单一的句法表达式中捕获所有这些哥德尔句子，比如说来自一类表达式 Gω的任何 G，我们仍然能够构造另一个哥德尔句子 Gω+1 等等。等等。可以想象，序数中的下一个将是 2Gω，然后是 Gω ** 2 等等。因此，无论我们如何努力，这个系统将永远是不完整的。**而且，没有算法可以进行这种“哥德化”。**

这样一来，我们现在可以提出卢卡斯论点了。大致是这样的:

既然 AGI 必须能够完成人类能完成的任何智力任务，它也必须能像我们一样完成哥德尔魔术。然而，如果一台机器是纯计算性的，它必须在某种正式的系统中运行，因此它不能检测自己的不完整性。另一方面，我们人类可以“在系统之外”思考，并像哥德尔一样认识到它的缺陷。所以，头脑简直比任何机器都强大，无论多快多强。

虽然这一论点主要适用于昨天的人工智能系统，这些系统基于一阶逻辑，大致像定理证明器一样工作，但它也可以同样适用于当前的神经网络式人工智能，因为它必须嵌入到某种图灵机中，因此必须像任何计算系统一样受到限制。

另一方面，我认为同样的论点也适用于人类！虽然乍一看我们似乎能够逃脱哥德式的陷阱，但我强烈怀疑我们能否无限期地这样做。毕竟，即使是序数命名系统在有限的步骤之后也会崩溃。如上所述，我们有ω，2ω，ω**2，ω**ω，ω**ω**ω，…，但在某些时候，我们没有进一步的命名方案，我们遇到了问题。没有一个单一的方案来命名所有的序数。

> 没有递归相关的符号系统给每一个建设性的序数命名。
> 
> -阿隆佐·邱奇&斯蒂芬·克莱尼

作为一个更实际的说明，我认为我们每天都无法走出我们的“正式系统”，例如我们思考出生和死亡的方式。既然我们被困在这个空间和这个时间，我们就不可能想象自己已经死了。直觉上，我们可能意识到宇宙即使在我们死后也会存在，但我们很难想象。

我们更难想象没有任何空间或时间。在这种情况下，我们的直觉完全失灵了。也许这就是为什么我们不能到达物质现实的底部的原因。既然我们根植于其中，我们就没有办法从外部感知它。然而，这并没有对 AGI 的发展构成重大威胁，因为如果 AGI 有和我们一样的基础(物理现实)，它最终会受到同样机制的限制。**因此，如果没有其他证据表明我们的思想具有超凡脱俗的力量，我想我们可以放心地驳回这个论点。**

# 人类意识不是算法

<https://www.amazon.com/Emperors-New-Mind-Concerning-Computers/dp/0192861980>  

这个论点基本上是前一个论点的延续，扩展到适应我们最近的发现，并通过关于我们意识机制和假设物理学的假设得到加强。它最初是由最近的诺贝尔奖获得者罗杰·彭罗斯表达的。在他的书《皇帝的新思想》中，他认为人类意识不能被任何图灵机建模，因此也不能被今天已知的任何计算机建模，因为它从根本上**是基于量子力学**，更具体地说是量子引力。假设是真核细胞中被称为**微管**的微小结构小到足以展示量子行为，因此它们可能是大脑推理能力的来源。例如，微管可以处于不同状态的量子叠加状态，它们甚至可以与其他微管纠缠在一起，可能将量子状态传播到整个人脑。这并不意味着大脑是一台量子计算机。实际上，彭罗斯声称大脑不仅仅是量子计算机。为什么？因为即使量子计算机能够在多项式时间内解决 NP 完全问题(顺便说一句，该假设似乎是错误的，尽管它尚未被证明)，它也永远无法解决无法计算的问题，而这正是彭罗斯声称大脑能够做到的。

术语“不可计算的函数”源自 Alan Turing，他将哥德尔的数学理论应用于计算，并提出了著名的**停止问题。**停机问题询问给定的计算机程序在给定特定输入的情况下是否最终会停止或无限期运行。图灵已经表明，一般来说，没有算法可以解决这个问题或任何可简化为停机问题的问题。这个论点是循环的，就像哥德尔的论点一样:即使你有一个解决停机问题的神谕(一个可以免费为你解决问题的神奇机器)，总会有**另一个与你的有神谕的新机器有关的停机问题。**

因此，彭罗斯的论点甚至比乍看起来更有力:由于不可计算性的循环性质，根据该理论，大脑不仅优于任何图灵机，而且优于任何图灵机和任何可能存在的神谕。

所有这一切都意味着，我们永远不能指望建立一个像人类思维一样有能力的人工智能，因为我们知道的机器只能通过算法解决问题。

老实说，作为一个人，知道我们有办法克服甚至无法计算的问题，因此我们的思维在某种程度上是在系统之外的，这种感觉会很好。然而不幸的是，我认为不可计算性的论点适用于人类，就像卢卡斯的论点一样:在某个时刻，我们的理解会崩溃。对于我们这些凡人来说，这一点可能来得更早，对于彭罗斯或哥德尔这样的天才来说，这一点可能来得更晚，但它不可避免地会出现。

让我也提一下微管的理论有一个固有的缺陷:物理学家 **Max Tegmark 指出微管太大了，以至于不能展示量子力学效应**:它们在仅仅 10e-13s 后就会发生退相干(量子信息的丢失)。虽然这些时间后来被其他人修正为 10 到 100 微秒，但仍然太快了。当然，这并没有完全排除彭罗斯的理论:大脑中可能还有其他更小的结构能够进行量子计算。

对彭罗斯理论的另一个批评是争论性的:使用物理学、神经科学或生物学尚不知道的效应有点太模糊，无助于证伪主张。这些主张似乎是人为构建的，因此目前我们无法指望用经验来证明或反驳它们。从科学的角度来看，不可证伪的理论是无用的。

# 人工智能必须在世界上行动，以获得人类拥有的知识和思维

一个流行的理论认为，我们人类由于所处的特定环境而进化出了高级思维能力。根据哲学家 Hubert Dreyfus 的说法，没有身体的计算机根本无法获得智能，因为人类的大部分知识都是隐性的。它不能通过读书来学习，而必须通过模仿擅长某项技能的老师来获得。对德雷福斯来说，遵循规则(无论是编程的还是学习的)是最低级的能力。最高层次，专家表现，是隐性知识，即使是经常练习的专家也不能轻易解释。例如，你当然知道如何走路。然而，如果你试图向某人解释你是如何做的，那将是非常困难的。

与此相关的另一个问题是，因为计算机没有身体，也不与世界互动，它们不理解因果关系，也没有简单的方法让它们获得因果关系。处理反事实和回答因果问题是我们人类拥有一般智能的先决条件。

  

对我来说，这个论点似乎有一定的正确性，因为与世界互动是我们人类学习的主要方式之一。另一方面，我不确定这样说是否公平，因为我们不能解释我们获得的隐性知识，它不能被计算机学习。在我看来，这更多的是关于有意识和无意识知识的争论。虽然有意识的知识位于层次的“顶端”并使我们感知它，但无意识的知识是自动的，不能被体验。因此，我们也很难解释它。

举个例子，再考虑一下步行。当你小的时候，你试着走路，那是非常困难的。你必须有意识地思考你脚的每一个小动作。然而，随着你的技能增加，你慢慢忘记了个别的小动作，因为知识慢慢变得无意识。这是一件好事，因为有意识的感知能力肯定非常有限。同样的过程也可能发生在 AGI。首先，AGI 将获得技能，它将不得不“思考”每一小步。随后，这种技能将被添加到“无意识记忆”中，或者被硬编码到子神经网络中，调整这种硬编码程序的能力将变得更加有限，而执行能力将会提高。

关于因果关系，人们普遍承认它是人工智能所必需的(例如，见[https://spectrum . IEEE . org/tech-talk/artificial-intelligence/machine-learning/understanding-causality-is-the next-challenge-for-machine-learning](https://spectrum.ieee.org/tech-talk/artificial-intelligence/machine-learning/understanding-causality-is-the-next-challenge-for-machine-learning))，并且有人试图将其纳入当前的模型中([https://syncedreview . com/2020/10/29/deep mind-introduces-algorithms-for-causage-reason-in-probability-trees/](https://syncedreview.com/2020/10/29/deepmind-introduces-algorithms-for-causal-reasoning-in-probability-trees/))。

# 人工智能可能永远不会达到自我提高的程度

最近，AGI 的大部分讨论都围绕着一个观点，即**奇点类型的事件导致自我发展的人工智能系统在没有任何进一步人类干预的情况下以失控的方式发展**，导致它在几天甚至几小时内变得超级智能，达到人类水平的智能。这个理论是基于指数增长的概念:一旦有一个人工智能具有人类水平的智能，并且这个人工智能可以稍微改进自己，进化的人工智能将再次稍微改进自己，以此类推，直到我们拥有远远优于任何人类的超级智能。在某种程度上，自我完善的人工智能假说模拟了我们自己的生物进化，只是在更小的时间范围内。因为与我们不同，人工智能应该也有能力改进它的硬件(我们还不能通过技术显著地改进我们的大脑)，这个过程将更加强大。

然而，这个故事的问题是，我们实际上并没有成功构建任何自我改进的人工智能。今天的机器学习算法能够在一定程度上修改自己，例如一次性学习算法，但它远没有达到人类发展的水平。遗传算法可能能够通过类似于生物生命的机制进行自我改进，但它们通常需要人类进行大量的微调才能正确工作。另一方面，自我改进的人工智能最初只需要新生儿的智力，并可以从那时起发展到任何需要的智力水平。

  

然而，已经有了一种理论上的方法来开发自我完善的人工智能。它被称为**哥德尔机器**(哥德尔又来了)，由于尔根·施密德胡伯发明。哥德尔机器是一个假设的计算机程序，每当它为当前任务找到一个更好的策略时，它就使用一个**递归策略来自我改进**。机器具有定义的输入和输出，并试图最大化效用函数，这可能取决于机器的状态和输入(环境)。这台机器被证明是最优的，尽管它受到限制，即它需要设定非常具体的目标，不像人类那样，在生活中似乎没有任何大的预定义目标。

# 我们可能永远不会承认人工智能是智能的

最后但同样重要的是，我向你们提出一个论点，不是反对发展 AGI 的可能性，而是反对我们认识这一伟大壮举的能力。

这场争论与人工智能定义的模糊性密切相关。因为我们并不真正知道我们在寻找什么，所以可能我们永远无法实现它，或者更糟糕的是，我们永远无法意识到我们已经实现了。

如果我们能精确地定义智力，我们也许能发现更多关于我们自己的智力，我们不仅能量化其他生物的智力，也能量化无生命物体的智力。那样的话，在某个确定的时刻，我们可以说我们已经构建了一个和我们的智能大致相当的人工智能。现在，我们正在来回讨论智力的本质，我们还没有真正得出一个明确的答案。

由于定义如此不精确，我们会遇到“移动门柱”的危险。让我引用一篇来自 [MindHacks](https://mindhacks.com/2005/06/15/minsky-slams-modern-ai/) 的关于马文·明斯基的批评文章:

> [马文·明斯基]对人工智能认知中的“移动门柱”问题做了一个简单的评论，这掩盖了人们如何看待人工智能的大部分问题。

就人工智能的历史发展而言，我当然觉得这是一个有效的论点。让我给你举几个例子，人们期望从人工智能中得到的东西，最终都被开发出来了，但是从来没有让社区满意过:

“机器永远不会下棋。”(IBM 深蓝在 1997 年击败了加里·卡斯帕罗夫)。

"机器可以下棋，但它完全是机械的."(DeepMind AlphaZero 不仅击败了人类棋手，还击败了使用直观方法下棋的最佳国际象棋引擎)。

“机器永远不会翻译。”(Google Translate 支持 109 种语言，可以在它们之间进行高准确率的翻译)。

"机器永远也不可能驱动汽车。"(尽管就全自动驾驶汽车而言，我们仍未达到这一水平，但人工智能可以说在通常的道路场景中比人类驾驶得更好)。

当然，仍然有一些领域人类比机器更好，但它们越来越少了。我们人类似乎还剩下两个主要优势:

1.  我们可以同时完成所有这些任务。人工智能可以完成一些特定的任务，但它还不能适应一个全新的任务。
2.  我们知道自己在做什么。可以说，就智力而言，这算不上什么优势，但它赋予了我们机器还不具备的品质，比如移情和情感。

我想说，如果第一个问题解决了，我们可以确信我们已经实现了 AGI，因为一个能够适应新任务并有效解决它们的智能机器不一定也需要完成第二个问题。此外，我觉得第二点(意识)是一个更难解决的问题，甚至在我们开发出 AGI 后的一段时间内也可能无法解决。

# 结论

最终，我认为我们将能够开发出强大的人工智能，尽管这可能不是在我们有生之年。此外，我们甚至可能不知道我们实现了这一非凡的壮举，要么是因为人工智能的本质与我们太不一样，要么是因为人工智能可能真的太聪明了，以至于我们不知道。据我们所知，这可能已经发生了…

无论如何，即使我们对实现 AGI 没有那么大的把握，我们仍然应该花一些精力研究和评估这种可能性会带来的风险，并思考我们如何处理任何潜在的负面后果。它可能会拯救我们的文明。