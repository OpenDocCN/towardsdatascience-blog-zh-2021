# 算法偏差

> 原文：<https://towardsdatascience.com/algorithmic-bias-fff4d8c31290?source=collection_archive---------41----------------------->

## 当算法模仿和放大刻板印象时

![](img/cd40532425133d447ecf79c663af3020.png)

资料来源:联合国人类住区规划署

# 谁在泰坦尼克号灾难中幸存了下来？

回想一下泰坦尼克号电影，救生艇附近的男人在尖叫——女人和孩子优先！现在，随着一些崭露头角的数据科学家探索著名的泰坦尼克号数据集([也可在 Kaggle](https://www.kaggle.com/c/titanic) 上获得)以预测谁幸存，在探索性数据分析的最初几步中，人们会得出结论，妇女和儿童幸存的概率更高。这些数字还表明，坐在较高阶层的人比坐在较低阶层的人有更高的生存机会，避免贿赂或地位偏见。

现在，如果有人随机问我——美国广播公司在泰坦尼克号灾难中幸存了吗？我的回答是 ABC 是女性吗？他/她富有吗？我只是放大了我的认知系统通过观看电影和探索数据集建立起来的*经济地位偏见*和*性别偏见*。

如果假设，泰坦尼克号数据集将被旅行社用于邮轮保险，它将预测女性的存活率高于男性的存活率；因此，与男子相比，向妇女收取的保险费较低。

# **主流应用中的偏差**

***偏见与信用额度*** 2019 年，苹果联合高盛推出信用卡。当科技企业家大卫·海涅迈尔·汉森在推特上说他被批准的信贷限额比他妻子被批准的限额高 20 倍时，事情发生了错误，尽管他们申报了联合税。包括苹果联合创始人史蒂夫·沃兹尼亚克在内的许多其他人表示，同样的事情也发生在他们身上。在这种情况下，如果算法是预测信用额度的算法，保持其他一切不变(税收、资产和负债)，女性的信用额度预计会低于男性。

***偏见和工作门户*** 几年前，亚马逊创建了一个内部招聘工具，根据简历是否符合工作描述/历史角色定义对简历进行排名。后来人们发现，这个工具[并没有产生不分性别的排名](https://www.reuters.com/article/us-amazon-com-jobs-automation-insight-idUSKCN1MK08G)。使用这一工具时，妇女的级别比男子低。

***偏向和面部识别*** 许多面部识别技术已经以生物识别支付、机场移民检查、相机中的内置应用以及各种社交媒体网络的形式进入主流。然而，这些面部识别应用程序错误识别有色人种的事件越来越多。路透社文章:[美国政府研究发现面部识别工具存在种族偏见](https://www.reuters.com/article/us-usa-crime-face/u-s-government-study-finds-racial-bias-in-facial-recognition-tools-idUSKBN1YN2V1)。与对非白人的预测相比，该工具可以更准确地识别白人的性别。

# **它是如何发生的？**

***训练数据中的偏差*** 以苹果信用卡为例，最有可能是算法，根据历史数据训练，预测其新客户的信用额度。历史上，在算法接管之前，人类以承销商的形式做出信贷限额决定。也许，这是一种历史的非有意的人类偏见。由于多种原因，如男性和女性的工作稳定性更高，更容易因生育和照顾孩子而中断工作，承保人可能会给男性更高的信贷限额，给女性更低的信贷限额。

在招聘工具的例子中，模型是根据主要来自男性的历史简历进行训练的，而科技是男性主导的行业。因此，该工具惩罚像“女子象棋俱乐部”等词。从而导致女性的排名较低。

当算法根据历史数据进行训练时，它们会重复和混合社会/行业中存在的刻板印象/错误，从而降低特定人群的优先级。

***训练数据的有限可变性*** 在面部识别算法存在偏见的情况下，种族偏见被注入算法的一个关键原因是训练数据在很大程度上偏向白人群体。如果训练数据适当地代表了人口的所有部分，可能就不会出现偏差。

因此，如果我们要训练一个只有女性人口的乳腺癌算法，那么如果一些男性被扔进预测数据，它将会抛出古怪的结果。不要误会我的意思，但乳腺癌也可能发生在男性身上！

***直接偏差特征*** 作为数据科学家，当我们在开发算法时，我们的倾向是获得尽可能高的预测精度。我们踏上了特征提取之旅，将一切可能的东西(性别、收入、地址)放入模型中以提高准确性。我们很少考虑这个问题——基于性别做出这种预测是否道德？如果在历史上，人类有意识或无意识地考虑性别来做出信贷决策，我的算法也会这样做吗？将性别作为一个特征并不总是一件坏事。在一些场合，例如检测乳腺癌的概率，包括性别特征将是非常合适的决定。

***间接偏见特征*** 虽然性别和收入等变量会直接引入偏见，但其他一些变量，如俱乐部会员资格、姐妹会/兄弟会隶属关系等也是会引入偏见的间接变量。加入兄弟会意味着客户是男性，加入姐妹会意味着客户是女性，交易数据中向印度俱乐部支付的会员费可能间接将客户描述为亚洲人。所有这些变量，如果不去偏差，涵盖一个或另一个偏差。

# 未来意味着什么？

***对于数据科学家和开发人员来说***
在我们追逐准确性最大化目标的时候，我们应该看看这个过程的其他一些结果。例如，大多数基于树的算法(决策树、随机森林、xgboost 等)会产生特征重要性度量，如果对其进行适当的分析，可以预先阻止算法中的任何偏差。

***对于学术界来说*** 传统上，在社会、工作场所和其他地方有偏见的监管法规都坐在法律部门里。可能学术界需要创建专门的法律程序，培训律师理解算法中的偏见，并对机器学习产品进行“法律审计”。有了适当的算法知识，法律应该成为人工智能团队的一部分。

**对于一个更广泛的人工智能网络** 类似于从逻辑回归到 CNN 的一系列算法，为了自动化无数后端决策和预测，我们应该创建一系列算法——每一个都检测不同类型的偏差。比如模型中种族偏见的一个 XGBoost，收入偏见的深度学习等等。