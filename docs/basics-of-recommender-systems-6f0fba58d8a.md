# 推荐系统基础

> 原文：<https://towardsdatascience.com/basics-of-recommender-systems-6f0fba58d8a?source=collection_archive---------13----------------------->

## 用户相似性，项目相似性，协同过滤，基于内容的模型，潜在空间模型

![](img/57f0dec634e29524eb077248e6559101.png)

来自 Pixabay

推荐系统*主动*向用户推荐*相关*项目。*当*宜。“主动”意味着这些项目只是出现——用户不需要搜索它们，甚至不需要意识到它们的存在。“相关”意味着当用户出现时，他们倾向于*参与*。“与他们接触”的确切含义取决于上下文。对于电影来说，engage 可能意味着*看*；对于产品*购买*。“适当的时候”是关键，这也是“智能”的来源。

让我们看几个例子。

*   你在线观看电影、节目和视频。推荐系统被动地观察你的互动。*你看什么*，*你什么时候看*，你看某个项目多长*。它会推荐其他它认为你会喜欢的视频。如果您正在使用遥控器观看电视，这将非常方便。在这种情况下，浏览(个性化)产品比搜索更方便。*
*   *你在电子商务网站浏览或购买商品。一个推荐系统监视你的一举一动。和其他买家的信息。然后推荐新的项目给你考虑。*

***协同过滤***

*想象你在你的网站上销售商品。很多项目，比如 1000 个。您将购买数据保存在数据库中。你的数据库跟踪*购买时间*、*用户标识*、*物品标识*和*数量*。用户*用户标识*在时间*购买时间*购买数量物品*物品标识*的实例。*

*让我们看一个来自虚构数据库的数据示例，总结如下。我们忽略了购买时间。我们已经把其余的数据排列成一个矩阵。行索引用户。列索引项目。*

```
 ***I1 I2 I3 I4**
**U1** 4  0   5 0
**U2** 6  1   7 0
**U3** 0  0   0 1
**U4** 0  0   1 1
**U5** 3  0   4 0*
```

*在这个数据中，我们有五个用户 U1 到 U5，四个项目 I1 到 I4。Cell (U，I)跟踪用户 U 购买商品 I 的次数(到目前为止)。例如，U1 四次购买 I1。*

***超出购买频率***

*虽然在该数据单元(U，I)中记录了用户 U 购买项目 I 的次数，但是更一般地，该单元可以记录我们认为合理地跟踪用户 U 对项目 I 的密切关系的任何数字，例如从 1 到 5 的评级。在评价设置中，许多单元格没有值，因为不是每个用户都对每个项目进行评价。事实上，人们会认为评级矩阵中的大多数单元格都有空值。*

*我们能从这些数据中挖掘出什么？*

*在这个例子中，我们可以推断用户 U1 和 U5 是相似的。这是因为他们购买相同物品的次数大致相同。我们从下面的行向量中可以看到这一点。*

```
***U1** 4 0 5 0
**U5** 3 0 4 0*
```

*因此，通过比较矩阵的行向量，我们可以推断出用户之间的相似关系。*

*现在我们来比较列向量。我们可以推断 I1 和 I3 是相似的。它们被相同的用户购买了大致相同的次数。*

```
*I1 I3
4  5
6  7
0  0
0  1
3  4*
```

*因此，通过比较矩阵的列向量，我们可以推断出项目之间的相似关系。*

*这是协同过滤的关键点:仅从交易数据我们就可以推断出用户之间的相似关系和项目之间的相似关系。*

***购物篮***

*回想一下，我们在前面的部分中描述的矩阵，即从交易中构建的矩阵，是从四元组得到的(*购买时间*、*用户 id* 、*物品 id* 、*数量*)。这样的四倍代表个人购买。这就丢失了一条关键信息:哪些商品是在同一次交易中一起购买的，即在同一个购物车中。该信息对于根据“购买了 A 的人也购买了 B”范例操作的某些类型的推荐系统非常有用。当我们谈到这个话题时，我们会更详细地讨论。目前，我们只是把它浮出水面。*

***新用户，新物品***

*一个新用户没有任何交易历史，所以我们无法找到邻居。同样，一个新项目没有任何购买，所以我们无法找到它的邻居。因此，依赖用户邻居的算法对新用户无效，而依赖项目邻居的算法对新项目无效。*

*我们现在提出这些问题，以便在这篇文章的后面，我们可以看到各种算法如何处理它们。*

***内容***

*现在假设，除了交易数据，我们还有描述商品和用户的数据。我们将其称为*内容*。项目可能有描述、标签、名称、产品品牌、产品类别、价格等。(产品类别的示例有*服装*、*电子产品*、*家用*、…)某些属性可能取决于类别。如智能手机的*存储容量*和*相机分辨率*，服装的*面料类型*。*

*用户可以选择提供关于他们自己的附加信息。如*性别*和*年龄*。甚至可能是*价格偏好*、*喜欢的产品*等。*

*现在我们有了额外的数据，可以帮助我们推断项目之间的相似关系。事实上，甚至可能是比相似性更微妙的关系。比如一个 *iPhone 充电器*是一个 *iPhone* 的配件。*

*内容也可以帮助新用户或新项目。*

***对用户项目矩阵的第二次传递***

*在我们已经看到的用户商品矩阵中，单元格(U，I)记录了用户 U 购买商品 I 的次数(到目前为止)。考虑一个值为 0 的单元格(U，I)。我们想区分两种情况:*

*   *用户 U 知道商品 I，并选择不购买它。*
*   *用户 U 甚至可能不知道项目 I 的存在。*

*为什么要区分这两者？第一种情况，我们知道用户不想要 I，所以不应该推荐。*

*我们可以收集什么样的证据来证明用户有机会查看某个特定的项目。我们可以跟踪这个项目在过去被推荐给用户的次数。我们还可以跟踪用户最终访问项目页面的次数。从搜索开始，或者通过电子商务网站上的导航链接浏览。*

*代替推荐跟踪购买计数的用户项目矩阵，或者除此之外，我们可以考虑推荐跟踪评级的用户项目矩阵。比方说，这个电子商务网站让用户给商品打分，从 1 到 5。我们到目前为止介绍的概念，加上我们将很快介绍的概念，也适用于这种矩阵。简单地说，只要我们能从两个用户的行向量中量化他们有多相似，或者从他们的列向量中量化两个项目有多相似，我们就很好。事实上，我们甚至不需要能够计算两种类型的相似性。只有用户对用户或项目对项目可能就足够了。见下文。*

*使用评级代替计数有以下好处。评级是用户对项目好坏的明确反馈。此外，如果单元格的用户没有对单元格的项目进行评级，则评级矩阵中的单元格可以被明确标记为“未评级”。*

*另一方面，通常来说，评价商品的用户比购买商品的用户少得多。因此，对于相同的用户和商品，用户商品评级矩阵往往比用户商品购买频率矩阵稀疏得多。*

*总之，评级矩阵具有潜在的更原始的信号(评级),但是它的稀疏性可能抵消一些(或全部)这种优势。*

*在这篇文章中，我们不会对哪个更好采取立场。通常，如果两种类型的数据都可用，那么在推荐系统中使用购买频率和评级是有意义的。*

***推荐问题***

*现在让我们为推荐的实际问题做准备。我们有一个用户 u 和她最近的购买(或其他交互)。我们还拥有所有用户和所有商品的历史交易数据。我们还可能有关于项目和用户的附加内容。*

*使用所有这些信息，我们将向用户 u 推荐我们认为她会喜欢的商品。*

*一种方法是先找一些和 u 足够相似的用户，叫他们 v1，…，vk。在纯协同过滤中，这些用户可以通过比较各个用户的行向量与 u's 来发现。然后，我们向你推荐出现在他们的用户向量中的合适的项目。(我们将在后面详述“合适的”。)这叫做*基于用户的协同过滤*。我们向你推荐基于相似用户口味的商品。*

*如果条目有可用的内容，我们可以考虑将它们的属性维添加到条目向量中，或者替换它们。所以现在和以前一样，相似的行向量对应相似的用户。这一次，我们将同时考虑交易模式和商品内容。*

*让我们用一个简单的例子来说明这一点。我们将维度设为*

```
***iPhone6 iPihone7 iPhone8 iPhone10 iPhone12 Brand=Apple Product Family=iPhone***
```

*下面我们在这些维度上看到两个用户向量。*

```
***U1** 1 0 1 0 1 3 3
**U2** 0 1 0 1 0 3 3*
```

*把 iPhone6 到 iPhone12 想象成特定的物品，它们的品牌和产品系列就是内容。尽管 U1 和 U2 没有共同购买的物品，但是内容特征揭示了他们购买的相似性。*

*人们通常表现出品牌忠诚度。例如，iPhone 用户可能倾向于继续购买(最新的)iPhone。正如我们在上面的例子中所看到的，基于内容的建模可以显示这种趋势，从而提供更好的推荐。简单来说，在观察到某个用户一直在购买较新版本的 iPhone 后，就可以期望向该用户推荐最新的 iPhone 型号。*

*找到其行向量与特定用户的行向量相似的用户的先决条件是能够定量地比较两个行向量。我们先讨论这个。然后我们将回来讨论建立在此基础上的推荐算法。*

***测量行向量之间的相似度***

*有许多方法可以量化两个向量有多相似。不同的方法产生不同的结果。出于这个原因，我们将讨论其中的几个，并检查它们不同的行为。这将有助于读者了解何时使用哪一种。为了便于说明，我们将使用来自推荐设置的示例。*

*在开始之前，提醒一下用户项矩阵中的一些单元格可能缺少值。考虑一个评级矩阵。并非所有用户和项目的组合都有评级，事实上，很可能没有。很多用户从来懒得给任何东西打分！*

*在应用下面给出的相似性度量之前，我们将丢弃两个向量中缺少一个或另一个向量值的任何分量。所以下面描述的度量适用于没有丢失值的向量。*

*好了，我们开始吧。我们的第一个是*

***点积***

*这将两个向量按分量相乘，然后对结果项求和。考虑我们前面介绍的例子，为了简洁起见，省略了特性名称。*

***例 1** : *物品特性的效果**

```
***U1** 1 0 1 0 1 3 3
**U2** 0 1 0 1 0 3 3*
```

*点积是 1 * 0+0 * 1+1 * 0+0 * 1+1 * 0+**3 * 3+3 * 3**。最后两项用粗体表示，使得点积非常正。因此，我们认为 U1 和 U2 非常相似。在我们的场景中，这是有意义的，因为向量的最后两个组件分别代表*品牌*和*产品系列*。由于 U1 和 U2 多次偏向同一品牌和同一产品系列，认为它们相似是有道理的。*

*现在考虑一个不同的例子。*

***例 2** : *购买次数与购买与否**

```
***U1** 10 2 3 1
**U2**  3 2 3 1
**U3**  9 0 0 0*
```

*在这种情况下，四个维度代表不同的项目，只能通过标识符(如 SKU 或条形码)来识别。*

*{U1，U3}的点积比{U1，U2}的点积更正。因此，前一对具有更高的相似性得分。人们可以认为相似性得分应该颠倒。这是因为 U1 和 U2 购买的物品的*集合*是相同的，而 U3 丢失了 U1 购买的 4 件物品中的 3 件。这真正让我们思考的是，购买数量相对于所购商品的身份有多重要。*

*缓解这个问题的一个方法是在应用点积之前适当地预处理数据。例如，我们可以将所有正数转换为 1。也就是说，我们只是跟踪某个特定用户是否购买了某件商品。*

*三个向量的预处理版本变成*

```
***U1** 1 1 1 1
**U2** 1 1 1 1
**U3** 1 0 0 0*
```

*现在，在这种情况下，点积的行为符合预期。*

*我们正在失去信息。在其他情况下，我们可能会为此付出代价。其实下面这个。*

***例 3** : *采购盘点事项**

```
***U1** 9 1 2
**U2** 9 1 2
**U3** 1 1 1*
```

*这些向量的二进制版本上的点积将失去辨别 U2 比 U3 更类似于 U1 的能力。*

***居中点积***

*好吧，让我们试试不同的方法。让我们*将矢量居中*而不是二值化它们。将向量居中会从每个分量中减去向量的平均值。这在例 3 中有帮助吗？让我们看看。*

*首先，我们将复制示例 3，添加一列记录相应行向量的平均值。(本栏为黑体字。)*

```
***U1** 9 1 2 **4
U2** 9 1 2 **4
U3** 1 1 1 **1***
```

*因此，示例 3 中的居中版本的载体是*

```
***U1c** 5 -3 -2
**U2c** 5 -3 -2
**U3c** 0  0  0*
```

*作为健全性检查，我们可以看到上面的每个行向量的总和为 0。这是必须的。*

*嗯，确实有帮助！U1c 和 U2c 的点积非常正，而 U1c 和 U3c 的点积为 0。*

*为什么会这样？实际上，居中显示了向量中值的相对差异。点产品能够有效地利用这些相对差异。也就是说，首先居中，然后取点积，这是两个向量协方差的一个很好的度量。*

*那么这在示例 2 中有效吗？让我们看看。首先，让我们复制下面的例子，像前面一样，添加一列相应行向量的平均值。*

```
***U1** 10 2 3 1   **4
U2**  3 2 3 1 **9/4
U3**  9 0 0 0 **9/4***
```

*居中的版本是*

```
***U1c** 6     -2       -1    -3
**U2c** 3–9/4 2–9/4 3–9/4 1–9/4
**U3c** 9–9/4  -9/4  -9/4  -9/4*
```

*和以前一样，健全性检查显示每行总和为 0。*

*在这种情况下，居中(单独)没有帮助。{U3c，U1c}的点积还是大于{U2c，U1c}的点积。*

*潜在的问题是，在我们的设置中，值 0 和 1 之间的差异远远大于 9 和 10 之间的差异。0 表示没有购买物品。1 表示买了。所以，对我们来说，1 和 0 之间的差别，远远大于 10 和 9 之间的差别。*

***题外话:居中评级向量***

*在研究我们可以做些什么来解决上一段描述的问题之前，让我们注意到，如果用户向量由评级组成，居中是一个动机良好的操作。居中将评级(通常为 1 到 5 等正等级)转换为明确的正、负或中性评级。正如我们之前看到的，这种转换有助于点积作为更好的相似性度量。*

*也就是说，当用户使用时，需要小心地进行居中操作。考虑一个刚刚给两个项目分别评分为 4 和 5 的用户。居中的评级分别为-0.5 和+0.5。这有道理吗？*

*一种可能更好的方法是对用户的平均评级使用贝叶斯估计。下面是它的一个简单(有效)的形式。在用户的实际评分上加上从所有用户中随机抽取的 *n* 个评分。现在计算这些综合评分的平均值。 *n* 是这里的一个自由参数，它控制我们先前的信念，即这个用户像一个典型的用户一样评价。如果实际上这个用户没有，那么随着用户进行更多的评级，平均评级将向用户实际评级的平均值移动。*

***尝试拉伸小数值***

*让我们回到由购买计数组成的用户向量。*

*好吧，所以 0 和 1 应该被认为不如 9 和 10 相似。同时，5 和 10 应该被认为不如 9 和 10 相似。*

*让我们看看是否可以通过“拉伸”小值来实现这一点，这样它们的差异就会被聚合。假设 x 是一个值。考虑一下转型*

```
*f(x) = 1–1/e^x*
```

*这将实现这样的拉伸，因为*

```
*f(x)-f(x-1) = 1–1/e^x — (1–1/e^{x-1}) 
            = 1/e^{x-1} — 1/e^x 
            = (1/e^{x-1})*(1–1/e)*
```

*我们看到，随着 x 的增加，x 的连续值之间的差值减小。所以我们拉伸了小值，但没有拉伸大值。*

***或者，使用乙状结肠***

*我们可以从稍微不同的角度来看这个问题。我们可以从二进制化开始，即将每个正计数截断为 1，并使用类似 sigmoid 的函数(如 tanh)将其软化。我们可以调整双曲正切函数的参数(失调和增益),使 0 映射到接近 0，1 映射到接近 1。此外，它还具有所需的属性，即较高的计数映射到较高的值。*

***然后使用居中点积***

*在前面提到的计数的非线性变换之一之后，我们将每个向量居中，并像前面一样取点积。*

*这样够好了吗？*

*考虑这个例子。*

***例 4** : *计数分布**

```
***U1**  1  2  3
**U2** 10 20 30
**U3**  3  2  1*
```

*人们可以认为 U1 和 U2 应该比 U1 和 U3 更相似，因为它们具有相同的相对*计数。实际数字之间的差异可以用一个简单的因素来解释:U1 是一个多产的买家。**

*接下来，让我们看看如果使用中心点积作为相似性度量，我们会得到什么。*

*最后一列粗体字包含向量平均值。*

```
***U1**  1  2  3  **2
U2** 10 20 30 **20
U3**  3  2  1  **2***
```

*居中的版本是*

```
***U1c**  -1 0  1
**U2c** -10 0 10
U3c   1 0 -1*
```

*{U1c，U2c}的相似性得分高于{U1c，U3c}。很好！如你所愿。*

*我们运气好吗？这在一般情况下行得通吗？我不会在这里讨论这个问题。相反，我会观察到，如果我们寻求一个基于相对计数的度量，那么一个合理的候选就是将计数归一化为概率向量。然后，我们可以使用合适的概率分布相似性度量。比如相对熵。*

*下面我们举例说明标准化。*

*首先，我们复制上面的例子，最后一列包含向量和。*

```
***U1**  1  2  3  **6
U2** 10 20 30 **60
U3**  3  2  1  **6***
```

*接下来，我们将每个向量除以它的和。我们得到了*

```
***U1** 1/6 2/6 3/6
**U2** 1/6 2/6 3/6
**U3** 3/6 2/6 1/6*
```

*我们不会继续解释相对熵和计算值。相反，我们只需观察中心点积在概率向量上的表现。*

***仔细查看零计数***

*到目前为止，在我们的示例中，我们已经隐含地将没有购买特定商品的用户视为该(*用户*、*商品*)对的负面信号。这种假设应该受到质疑。考虑一个项目数量非常大的设置，比方说至少有几十万个。这在电子商务环境中很常见。用户可能没有购买特定的商品，因为她甚至不知道它的存在。在这样的设置中，即使不是所有的用户向量，也是大部分的用户向量非常稀疏。也就是说，用户将只购买了目录中很小一部分项目。使用零计数作为负信号会放大这种假设的效果。*

*我们来阐述一下“放大这个假设的效果”。对于任何两个用户 u 和 v，以及我从大量商品中随机挑选的一件商品，很有可能 u 和 v 都不会购买 I。如下图 1 所示，居中的点积将此作为 u 和 v 相似的证据。因为宇宙中的大多数物品都不会被 u 或 v 购买，这样不正确的证据会被放大，如下图所示。*

```
*(u, i) = 0 implies u dislikes i (1)
(v, i) = 0 implies v dislikes i (2)
(1)+(2) contributes evidence towards u and v being similar*
```

***图 1** : *错误的推论被放大**

*使用评级而不是计数可以避免这个问题，因为我们有办法区分缺失值(没有评级)和实际值。也就是说，忽略购买次数，即只使用评分是没有意义的。前者要丰富得多。他们也给出了一个直接的购买信号。评级可能会有偏差。另一方面，一个项目被购买。也就是说，它是为禁止免费赠送的促销商品而付费的，这些商品可以被过滤掉。*

*那么我们如何处理零计数呢？我们可以考虑总是将零计数解释为缺失值。在这种解释下，被比较的两个行向量的所有分量将具有正计数。这是因为其中一个或另一个计数为零的组件将被丢弃。这没什么大不了的，所有用户向量现在只有正数！*

*如果可能的话，一个更好的方法是跟踪更多的数据，以便能够以合理的准确度估计用户是否只是不知道某件商品或者选择不购买它。例如跟踪用户是否访问了该项目的页面。*

*一个更高级的想法是假设我们有可用的互斥对，即从不一起购买的物品对，更一般的是属性对。如果用户的购买频率向量具有来自这样一对元素中的一个元素的正计数，以及来自另一个元素的零计数，则结合了互斥信息的零计数为用户没有(或不会)购买后者的概念提供了支持。当我们讨论基于项目的协同过滤时，我们将讨论推断互斥对。*

***Jaccard 系数:测量两组的重叠度***

*假设我们将用户过去的购买行为表示为购买物品的*集合*。暂且不说我们正在丢失信息——购买计数——集合表示确实因其简单性而吸引人。*

*用户的集合可以被一般化，以获取其他布尔值属性。如*产品家族= PF* 。在下面的讨论中，为了简单起见，我们将只讨论项目集。*

*有一个简单的衡量标准，允许我们根据两个用户的项目集的重叠程度对他们的相似性进行评分。这个度量称为 Jaccard 系数，是两个集合的交集大小与并集大小的比值。*

*举个例子。假设用户 U 和 V 分别购买了{a，b，c}和{b，c，d}。Jaccard 系数为|{b，c||/|{a，b，c，d}| = 2/4。*

*这项措施因其简单而吸引人。它也有助于非常快速的得分，尽管我们不会在这篇文章中详细阐述这方面的内容。它还缓解了前面讨论的“零计数”问题。与前面介绍的方法不同，Jaccard 系数忽略了任何一个用户都没有购买的所有商品。也就是说，它避免了图 1 中的错误推断。*

*也就是说，如果一个商品被其中一个用户购买，而另一个用户没有购买，则该商品总是对 Jaccard 系数产生负信号。这可不好。*

***多重性***

*Jaccard 系数可以推广到购买次数中吗？在集合的语言中，我们在问，它能被推广到多重集上的相似性度量吗？*

*是的。这包括将交集、并集和基数运算从集合推广到多重集合。*

*两个多重集 X 和 Y 的交集可以定义如下。首先，我们取多重集下面的集合的交集。接下来，对于这个交集中的每个元素 e，我们计算它的重数*

```
*m(e) = min(m(e,X), m(e,Y))*
```

*这里 m(e，Z)表示多重集 Z 中元素 e 的重数。*

*两个多重集 X 和 Y 的并集也有类似的定义。首先，我们取多重集下面的集合的并集。接下来，对于这个并集中的每个元素 e，我们计算它的重数*

```
*m(e) = max(m(e,X), m(e,Y))*
```

*多重集的基数是其中不同元素的多重数之和。*

*让我们看一个例子。考虑多重集 X = {3a，3b}和 Y = {2a，2b，1d}。“3a”表示重数为 3 的“a”。X 和 Y 的交点是{2a，2b}。X 和 Y 的并集是{3a，3b，1d}。所以 Jaccard 系数是 4/7，交集的基数除以并集的基数。*

***雅克卡系数对加权雅克卡系数***

*让我们称 Jaccard 系数到多重集的扩展为加权 Jaccard 系数。现在两者都有了，就可以问哪个效果更好了。答案是“看情况”。我们来细说一下。*

*考虑 X = {2a，2b}，Y = {2a，2b，c}。加权雅克卡系数给出了⅘.丢弃多重性信息的未加权 Jaccard 系数给出了⅔.直观上，前者更准确地量化了这两个多重集的相似性。到目前为止一切顺利。*

*接下来，改为考虑 X={4a，4b}和 Y={2a，2b}。加权的 Jaccard 系数给出 4/8 =而未加权的给出 1。哪个更准确？人们可以为未加权的一个，因为 X 和 Y 有相同的项目集。*

*我们再次看到，根本原因是我们混淆了集合成员和多重性(比如购买计数)。正如我们在前面章节中所做的那样，我们可以通过适当的预处理来分离出这些影响。*

***基于用户的推荐器***

*为了重述，让我们首先总结一下到目前为止我们所描述的内容。我们有历史交易数据，可能还会增加内容。后者可以是关于项目的内容、关于用户的内容或者两者都有。根据这些数据，我们以前面描述的方式为每个用户构建一个向量。*

*基于用户的推荐器首先找到其行向量与 *u* 的行向量最相似(并且足够相似)的 *k* 个用户。( *k* 是自由参数。)我们将这些称为 *u* 的 *k* 邻居。*

*然后，它将这些邻居的行向量与 *u* 的行向量进行比较，以决定向 *u* 推荐什么。*

*让我们看一个简单的例子。将 *k* 设置为 3。比方说 *u* 的项目集是{a，b，c，d}。说 *u* 的三个最近邻的项集是{a，b，d， **e** }，{b，c，d， **e** }，以及{a，c，d， **e** }。突出的是‘e’出现在所有 *u* 的邻居的项目集合中，而不是在 *u* 的项目集合中，所以向 *u* 推荐‘e’是有意义的。*

*让我们将这个例子中的观点形式化和一般化，如下所示。*

```
*1\. Find the intersection I of the item sets of the *k* neighbors.
2\. Recommend to *u* the items in I that are not currently in *u*’s item
   set.*
```

*当 *k* 不是很小时，比如说 *k* = 10，步骤 1 可能太保守了，因为我们取的是很多集合的交集。我们可以通过降低 *k* 来缓解这种情况。然而，这会影响推荐的质量，因为我们现在基于更少的邻居。一种不同的方法是放宽这种 *k* 的相交标准。这类似于谷歌对包含许多单词的查询所做的。并非所有这些都需要出现在匹配的文档中。*

***基于项目的推荐器***

*考虑一个新用户。到目前为止，这个用户几乎没有与系统交互(如果有的话)。所以没有足够的数据来找到这个用户的邻居。因此，基于用户的推荐器在这里是无效的。*

*基于项目的推荐器工作方式不同。它识别出在同一购物车中出现的比预期的更频繁的商品集合。这些集合被称为频繁项目集。对于购买了频繁项目集中的一些项目的新用户，可以推荐相同集合中的一些其他项目。*

*更深入一点，我们从例子开始。考虑下面的一系列交易。*

```
*{a, c}, {a, c}, {a, c}, {b, c}, {d, c}*
```

*a、b、c 和 d 是项目。每一组代表一个交易，想想购物车。在本例中，所有事务每个都有两个项目。我们能从这些交易中收集到什么？首先，对{a，c}是一个频繁项集。更重要的是，关联规则 a → b 是一个很好的规则。我们来阐述一下后者。*

*关联规则 X → Y 表示*

```
*IF X THEN Y*
```

*在我们的设置中，X 和 Y 是不相交的项目集，关联规则捕获了我们的意图，即如果用户购买了 X，推荐 Y。*

*不是所有的关联规则都一样好，所以我们需要一个关联规则良好性的概念。(事实上，如果没有这一点，我们将有大量的关联规则需要考虑，我们将一事无成。)*

*关联规则优度通常沿着两个维度定义:*支持度*和*置信度*。关联规则的支持量化了规则的适用范围。规则的可信度量化了规则的结果给定先行结果的可能性。*

*在形式上，支持度与 P(X+Y)成正比，即 X+Y 中的所有商品都在同一个购物车中的概率。(“+”表示集合并集。)置信度与 P(Y|X)成正比，即假设购物车包含所有 X，购物车包含所有 Y 的概率。*

*好的关联规则支持度高，可信度高。*

*让我们把这个应用到我们的例子中。在这个例子中，为了简单起见，我们将把一个项目集的支持定义为它发生的事务的数量。考虑规则 a → c。当{a，c}出现在三个事务中时，它的支持度是 3。它的置信度为 1，因为在 a 发生的每个事务中，c 也发生。现在考虑规则 c → a，它的支持度也是 3。然而，它的信心更低，⅗.这是因为在发生 c 的 5 个事务中，有 3 个也发生了 a。所以规则 a → c 优于规则 c → a，也就是说，当有人买 a 时我们应该推荐 c，而不是相反。*

***电梯***

*前面提到过，一个关联规则 X → Y 的置信度是 P(Y|X)，Y 在包含 X 的购物车中的概率，现在考虑 P(Y)。如果这也很高呢？就是 Y 里面的物品很受欢迎。仅仅因为这个事实，P(Y|X)就可能很高。也就是说，这个关联规则的高可信度可能主要来自于它的结果非常受欢迎。*

*一种不同的方法叫做*升力*，可以更好地解决这个问题。X → Y 的升力定义为 P(Y|X)/P(Y)。如果 Y 在包含 X 的篮子中比在随机篮子中更可能被找到，那么这个提升大于 1。*

*作为一个关联规则强度的度量，lift 是否总是比 confidence 更有效？不总是。因为提升需要取 Y 的两个概率的比值，所以当 P(Y)很小时，它容易受到噪声的影响，这是经常发生的情况。*

*例如，假设 P(Y) = 0.001。也就是说，平均 1000 个篮子中有 1 个包含 Y，现在假设有 400 个篮子包含 X，其中一个恰好也包含 Y，给定 X，Y 的升力为 1000/400 = 2.5。*

***对项目间的负面关联进行建模***

*考虑一个关联规则 i → j，这个表示:*如果用户购买我推荐 j* 。如果我们也对负面联想建模，会有意义吗？即型号*如果用户购买我不推荐 j* 。*

*经历过从推荐系统接收推荐的读者可能倾向于回答是。这是因为他们经常看到推荐的商品对他们购买的商品毫无意义。用数据科学的语言来说，这样的建模可以减少误报。*

*规则 i → j 的解除可以在这方面帮助我们。远小于 1 的升力表示 j 与 I 负相关。*

*使用 lift 来推断负面关联应该保守地进行。让我们用一个例子来说明这一点。说 P(j)是 0.001，即。j 平均每 1000 个篮子里出现 1 个。现在考虑包含 I 的篮子，假设这些篮子都不包含 j，我们需要几千个这样的篮子，才能有把握地推断 j 不会出现在包含 I 的篮子里。*

***混合方法***

*考虑一个描述“当用户购买 a 时推荐 b”的关联规则 a → b。在基于项目的方法中，支持度和信心同等地衡量所有用户的贡献。我们可以通过了解我们推荐给谁来改进这一点。具体来说，我们会给予被推荐人的邻居的贡献更高的权重。*

*我们可以将此总结为*

```
*recommend b when user u buys a and many users with tastes similar to those of u also buy b when they buy a.*
```

***用户和物品在同一个空间***

*考虑物品的特征。如*品牌*、*价格*、*产品*、*类别。*(还有很多其他的。)说这些特征是已知的。从交易数据中，我们可以推断出用户的喜好投射到这些特征上。我们来细说一下。*

*假设某个用户一直购买苹果智能手机。由此可以推断出用户对苹果的偏好，至少是对智能手机的偏好。或许与安卓智能手机相反。我们甚至可以选择包含多种功能组合的偏好。喜欢苹果智能手机的用户可能更喜欢运行 Windows 的笔记本电脑。我们可以学习这种多特征偏好。*

*在同一个空间中把用户和项目都表示为向量是很吸引人的。我们可以使用我们到目前为止讨论过的任何向量相似性度量(例如点积)来查找匹配给定用户向量的项目向量，或者反过来。*

*用户的向量表示用户喜欢各种属性和组合的程度。例如品牌名称、价格、产品类别等等。项目的向量表示项目的属性。因此，类似于项目向量的用户向量意味着用户的偏好与项目的属性相匹配。*

*由于这些向量具有透明的解释，推荐者还可以附加一个解释来伴随任何特定的推荐。比如“我们向您推荐这款商品，因为它具备这些特质”。这里，特征是从用户和项目向量中匹配的那些特征中选择的。*

***潜在空间***

*对于某些类型的物品，很难手工设计一组好的属性，同时对描述物品的*和喜欢*或者不喜欢物品的*有效。接下来，当然是给每件物品贴上属性标签。**

*考虑 Youtube 视频。特定视频的潜在特征是什么，揭示了为什么它会引起某些人的共鸣？如果我们可以推断出这些，我们就可以向这些人推荐其他具有类似特征的视频。*

*这些特征甚至很难描述。(双关。)即使他们不是，谁会给一个新视频贴上适用于它的特定特征的标签呢？*

*事实证明，某些算法可以从用户-项目矩阵开始，并以某种方式从中推导出用户和项目都可以投影到的空间。这听起来很难理解。怎么会？*

*首先，让我们观察到，这个潜在空间通常比我们开始的用户向量或项目向量的维度低得多。用户向量的维度是项目的数量。这可能是数百万。一个项目的维度就是用户的数量。这也可能是数百万。相比之下，潜在空间模拟物品的特征有两个目的，一是描述它们，二是辨别用户的口味。在许多设置中，例如电影，少于一百个特征可能就足够了。当然，我们并没有说这些特征是什么，只是限制了多少“不动产”足以捕捉服务于我们目的的特征。*

***推断用户和项目向量具有相同的潜在空间***

*假设我们有可用的评价三元组(u，I，rui ),其中 rui 是用户 u 给予项目 I 的评价(比如从 1 到 5)。在有许多用户和许多项目的设置中，这可以被视为用户 X 项目的高维矩形稀疏矩阵。(稀疏，因为只有一小部分用户项目组合具有评级。)*

*从这个矩阵中，我们可以推断出低维度潜在空间中的用户和项目向量。推断的用户和项目向量应该使得观察到的评级可以从潜在空间中的相似性计算中预测。*

*我们所说的“潜在空间中的相似性计算”是什么意思？下面我们来解释一下这个。以我们的三元组数据集{(u，I，rui)}为例。将其随机分为训练集和测试集。比如五五开。从训练集中推断用户和项目向量。使用这些来预测每个三元组(u，I，rui)的评级 rui。如下。*

*设 latent(u)和 latent(i)表示用户 u 和项目 I 的潜在向量，设点积 latent(u)点 latent(i)作为 rui 的预测。*

*现在我们来看看训练过程。首先，我们必须选择潜在空间的维度。原则上，这本身可以由训练方法决定。(这将对应于模型选择或网格搜索，将该维度视为超参数。)即便如此，我们也必须选择应该对哪些维度值进行网格搜索。*

*我们经常跳过超参数调整，只选择看起来合理的值。考虑电影。我能马上想到至少五个特征:*动作*、*喜剧*、*戏剧*、…预料到可能还有其他有用的组合，我可能会选数字 20。当然，这看起来像猜测。我们仍然可以期望得到一个推荐器，它比明显错误的极端值(比如 1 和 100000)有所改进。*

*好了，现在我们已经选择了潜在空间的维度。接下来，让我们讨论我们寻求优化的标准，以便在这个空间中找到好的用户和项目向量。幸运的是，我们已经在前面的段落中暗示过这一点。用户向量 u 和项目向量 I 应该使得它们在训练集上预测的评级是尽可能最好的，如通过一些损失函数所测量的。通常使用平方损失。形式上，这个优化标准是找到最小化的用户和项目向量*

```
*sum_{triple (u, i, rui) in training set} 
    (rui — latent(u) *dot* latent(i))²*
```

*通常我们还会添加一个正则项来支持稀疏的用户和项目向量。*

*让我们用电影的背景来说明这种直觉。假设我们选择 n = 20，因为我们合理地认为我们大约需要这么多的特征来覆盖同一空间中的电影和用户口味的范围。任何特定用户的口味都不可能涵盖所有 20 个方面。更有可能的是，用户的口味涵盖了这 20 种口味中的一小部分(当然我们还不知道是哪一种)。同样，任何一部电影的向量都可能是稀疏的。*

*好了，还剩下什么。训练算法。在这种情况下，一种流行且有效的选择是一种特殊形式的*随机梯度下降*。*

*假设我们已经初始化了各种潜在的用户和项目向量。然后，我们从训练集中的三元组(u，I，rui)开始迭代训练，通过在整个训练集中的一次或多次传递。*

*考虑任何单一的训练迭代。根据(u，I，rui ),我们首先计算预测误差*

```
*eui = rui — latent(u) *dot* latent(v)*
```

*接下来，我们独立地将 latent(u)和 latent(i)中的每一个推向使 eui 更接近 0 的值。然后在下一组训练中重复。*

***总结***

*在这篇文章中，我们讨论了推荐系统的主题。我们研究了我们可以从中学习推荐的数据的性质。具体来说，(I)购买及其数量，以及(ii)用户评级。我们讨论了基于用户间相似性或项目间相似性的协同过滤。在这次讨论中，我们还讨论了各种相似性度量，如点积和 Jaccard 系数，以及各种预处理，如二值化和居中。*

*我们还讨论了如何利用内容来提高推荐的质量。最后，我们讨论了通过将用户数据和项目数据投影到同一个空间来操作的方法。*

***延伸阅读***

1.  *[基于用户和基于项目的协同过滤推荐服务的比较](https://www.diva-portal.org/smash/get/diva2:1111865/FULLTEXT01.pdf)*
2.  *[推荐系统—维基百科](https://en.wikipedia.org/wiki/Recommender_system)*