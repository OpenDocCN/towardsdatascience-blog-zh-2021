# è‡ªç„¶è¯­è¨€å¤„ç†åˆçº§è¯»æœ¬

> åŸæ–‡ï¼š<https://towardsdatascience.com/a-natural-language-processing-nlp-primer-6a82667e9aa5?source=collection_archive---------10----------------------->

## ä½¿ç”¨ Python çš„å¸¸è§ NLP ä»»åŠ¡æ¦‚è¿°

![](img/01978fd3c6e0522c424cbd4dec76d202.png)

ç…§ç‰‡ç”± [Skylar Kang](https://www.pexels.com/@skylar-kang?utm_content=attributionCopyText&utm_medium=referral&utm_source=pexels) ä» [Pexels](https://www.pexels.com/photo/german-text-on-pieces-of-paper-6045366/?utm_content=attributionCopyText&utm_medium=referral&utm_source=pexels) æ‹æ‘„

è‡ªç„¶è¯­è¨€å¤„ç† **NLP** ç”¨äºåˆ†ææ–‡æœ¬æ•°æ®ã€‚è¿™å¯èƒ½æ˜¯æ¥è‡ªç½‘ç«™ã€æ‰«ææ–‡æ¡£ã€ä¹¦ç±ã€æœŸåˆŠã€æ¨ç‰¹ã€YouTube è¯„è®ºç­‰æ¥æºçš„æ•°æ®ã€‚

è¿™æœ¬åˆçº§è¯»æœ¬ä»‹ç»äº†ä¸€äº›å¯ä»¥ä½¿ç”¨ Python æ‰§è¡Œçš„å¸¸è§ NLP ä»»åŠ¡ã€‚ç¤ºä¾‹å¤§å¤šä½¿ç”¨è‡ªç„¶è¯­è¨€å·¥å…·åŒ…(NLTK)å’Œ scikit å­¦ä¹ åŒ…ã€‚å‡è®¾æ‚¨å…·å¤‡ Python å’Œæ•°æ®ç§‘å­¦åŸç†çš„åŸºæœ¬å·¥ä½œçŸ¥è¯†ã€‚

è‡ªç„¶è¯­è¨€æŒ‡çš„æ˜¯åƒè‹±è¯­ã€æ³•è¯­ã€é˜¿æ‹‰ä¼¯è¯­å’Œæ±‰è¯­è¿™æ ·çš„è¯­è¨€ï¼Œè€Œä¸æ˜¯åƒ Pythonã€R å’Œ C++è¿™æ ·çš„è®¡ç®—æœºè¯­è¨€ã€‚NLP ä½¿æ–‡æœ¬æ•°æ®çš„éƒ¨åˆ†åˆ†æè‡ªåŠ¨åŒ–ï¼Œè¿™åœ¨ä»¥å‰åªæœ‰å®šæ€§æ–¹æ³•æ‰æœ‰å¯èƒ½ã€‚è¿™äº›å®šæ€§æ–¹æ³•ï¼Œå¦‚æ¡†æ¶/ä¸»é¢˜åˆ†æï¼Œä¸èƒ½æ‰©å±•åˆ°å¤§é‡çš„æ–‡æœ¬æ•°æ®ã€‚è¿™å°±æ˜¯ NLP çš„ç”¨æ­¦ä¹‹åœ°ã€‚å®ƒè¿˜è¢«ç”¨äºåˆ›å»ºèŠå¤©æœºå™¨äººã€æ•°å­—åŠ©ç†(å¦‚ Siri å’Œ Alexa)ç­‰å…¶ä»–ç”¨é€”ã€‚

æœ¬ç¬”è®°æœ¬ä¸­ä½¿ç”¨çš„æ•°æ®æ¥è‡ª[https://www.english-corpora.org/corona/](https://www.english-corpora.org/corona/)ã€1ã€‘çš„æ•°æ®å­æ ·æœ¬ã€‚è¿™äº›æ•°æ®æ˜¯å…³äºå† çŠ¶ç—…æ¯’ç–«æƒ…çš„ï¼Œä»£è¡¨äº†æ¥è‡ªå„ç§åª’ä½“æ¥æº(ä¾‹å¦‚æŠ¥çº¸ã€ç½‘ç«™)çš„å­é›†ï¼Œå¹¶ä¸”æ˜¯ 2020 å¹´ 1 æœˆè‡³ 5 æœˆæœŸé—´çš„æ•°æ®ã€‚è¯¥æ•°æ®åŒ…å«å¤§çº¦ 320 ä¸‡ä¸ªè‹±è¯­å•è¯ã€‚

## ä½¿ç”¨ Python åŠ è½½æ–‡æœ¬æ•°æ®

è®©æˆ‘ä»¬ä»æŸ¥çœ‹æ–‡æœ¬æ•°æ®å¼€å§‹ï¼Œçœ‹çœ‹å®ƒæ˜¯ä»€ä¹ˆæ ·å­çš„ã€‚æˆ‘ä»¬å¯ä»¥ä½¿ç”¨æ“ä½œç³»ç»Ÿ( **os** )åº“æ¥åˆ—å‡ºæˆ‘ä»¬çš„æ–‡æœ¬æ–‡ä»¶å¤¹ä¸­çš„æ‰€æœ‰æ–‡ä»¶ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæ–‡ä»¶ä½äºåä¸º **text** çš„æ–‡ä»¶å¤¹ä¸­ï¼Œè¯¥æ–‡ä»¶å¤¹ä½äºåä¸º **NLP** çš„æ–‡ä»¶å¤¹ä¸­ï¼Œè¯¥æ–‡ä»¶å¤¹ä¸ Python æºä»£ç æ–‡ä»¶ç›¸å…³(ä¾‹å¦‚ Jupyter notebook æˆ–ã€‚py æ–‡ä»¶)å¹¶ç”¨â€œ.â€è¡¨ç¤ºã€‚/".

```
import os
os.listdir("./NLP/text")
```

è¿™å°†ç”Ÿæˆä¸€ä¸ªåŒ…å« 5 ä¸ªæ–‡æœ¬æ–‡ä»¶çš„åˆ—è¡¨:

```
>>> ['20â€“01.txt', '20â€“02.txt', '20â€“03.txt', '20â€“04.txt', '20â€“05.txt']
```

å¦‚æ‚¨æ‰€è§ï¼Œæœ‰ 5 ä¸ªæ–‡æœ¬æ–‡ä»¶(*ã€‚txt)ã€‚æˆ‘ä»¬å¯ä»¥ä½¿ç”¨æ ‡å‡†çš„ Python æ–‡ä»¶å¤„ç†å‡½æ•°æ¥æ‰“å¼€æ–‡ä»¶ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬å°†æŠŠå®ƒé™åˆ¶åœ¨å‰ 10 è¡Œï¼Œä»¥äº†è§£æ–‡ä»¶çš„ç»“æ„å’Œå®ƒåŒ…å«çš„ä¿¡æ¯ã€‚åœ¨ä¸‹é¢çš„è¾“å‡ºä¸­ï¼Œä¸ºäº†ç®€æ´èµ·è§ï¼Œè¿™è¢«ç¼©çŸ­äº†ã€‚ **next** å‡½æ•°ç”¨äºéå†æ–‡ä»¶ä¸­çš„è¡Œã€‚

```
with open("./NLP/text/20â€“01.txt") as txt_file:
    head = [next(txt_file) for i in range(10)]
print(head)>>> ['\n', '@@31553641 <p> The government last July called the energy sector debt situation a " state of emergency . " <p> This was during the mid-year budget review during which the Finance Minister Ken Ofori-Atta castigated the previous NDC government for entering into " obnoxious take-or-pay contracts signed by the NDC , which obligate us to pay for capacity we do not need . " <p> The government pays over GH ? 2.5 billion annually for some 2,300MW in installed capacity which the country does not consume . <p> He sounded alarmed that " from 2020 if nothing is done , we will be facing annual excess gas capacity charges of between $550 and $850 million every year . <p> JoyNews \' Business editor George Wiafe said the latest IMF Staff report expressing fears over a possible classification is " more of a warning " to government . <p> He said the latest assessment raises concerns about the purpose of government borrowings , whether it goes into consumption or into projects capable of generating revenue to pay back the loan . <p> The move could increase the country \'s risk profile and ability to borrow on the international market . <p> @ @ @ @ @ @ @ @ @ @ issue another Eurobond in 2020 . <p> The Finance Minister Ken Ofori-Atta wants to return to the Eurobond market to raise $3bn to pay for expenditure items the country can not fund from domestic sources . <p> The government wants to spend GH ? 86m in 2020 but is projecting to raise only GH ? 67bn . It leaves a deficit of GH ? 19bn , monies that the Eurobond could make available . <p> The planned return to the Eurobond market is the seventh time in the past eight years . <p> Ghana is already among 10 low-income countries ( LICs ) in Africa that were at high risk of debt distress . <p> The country in April 2019 , successfully completion an Extended Credit Facility ( ECF ) programme , or bailout , of the International Monetary fund ( IMF ) . \n']
```

## æ–‡æœ¬è¡¨ç¤º

ä¸ºäº†ä»¥æ•°å­—æ–¹å¼å­˜å‚¨å’Œä½¿ç”¨å­—ç¬¦ï¼Œé€šå¸¸ç”¨ç¼–ç ç³»ç»Ÿæ¥è¡¨ç¤ºå®ƒä»¬ã€‚æœ‰ **ASCII** (ç¾å›½ä¿¡æ¯äº¤æ¢æ ‡å‡†ç )ç­‰ä¸åŒçš„å­—ç¬¦ç¼–ç æ ‡å‡†ã€‚ä¾‹å¦‚ï¼Œå­—æ¯â€œaâ€ç”± ASCII ç  097 è¡¨ç¤ºï¼Œåœ¨äºŒè¿›åˆ¶ä¸­ä¹Ÿæ˜¯ 01100001ã€‚è¿˜æœ‰å…¶ä»–ç¼–ç é›†ï¼Œå¦‚ **UTF-8** (Unicode(æˆ–é€šç”¨ç¼–ç å­—ç¬¦é›†)è½¬æ¢æ ¼å¼ï¼Œ8 ä½)æ”¯æŒå¯å˜å®½åº¦çš„å­—ç¬¦ã€‚è¿™ä¸ªç³»ç»Ÿä¸­çš„å­—æ¯â€œaâ€æ˜¯ U+0061ã€‚åœ¨ Python ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥åƒè¿™æ ·ç›´æ¥ä½¿ç”¨ UTF-8:

```
u"\u0061">>> 'a'
```

æ‚¨å¯èƒ½éœ€è¦è½¬æ¢å¯¼å…¥çš„æ–‡æœ¬æ•°æ®çš„å­—ç¬¦ç¼–ç ï¼Œä»¥æ‰§è¡Œè¿›ä¸€æ­¥çš„å¤„ç†å¹¶æ›´å¥½åœ°è¡¨ç¤ºæŸäº›ç¬¦å·(ä¾‹å¦‚è¡¨æƒ…ç¬¦å·ğŸŒ).æ‚¨å¯ä»¥ä½¿ç”¨ **sys** æ¨¡å—ä¸­çš„ **getdefaultencoding** å‡½æ•°æ£€æŸ¥æ‚¨æ­£åœ¨ä½¿ç”¨çš„ç¼–ç ç±»å‹ã€‚è¦æ›´æ”¹ç¼–ç ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨ **setdefaultencoding** å‡½æ•°ï¼Œä¾‹å¦‚**sys . setdefaultencoding(" utf-8 ")**ã€‚

```
import sys
sys.getdefaultencoding()>>> 'utf-8'
```

## æ•°æ®é¢„å¤„ç†

åº”ç”¨ NLP æ—¶ï¼Œæ•°æ®å¤„ç†æœ‰å‡ ä¸ªé˜¶æ®µã€‚è¿™äº›æ ¹æ®ç¡®åˆ‡çš„ä¸Šä¸‹æ–‡è€Œæœ‰æ‰€ä¸åŒï¼Œä½†é€šå¸¸éµå¾ªç±»ä¼¼äºä¸‹å›¾æ‰€ç¤ºçš„è·¯å¾„ã€‚è¿™é€šå¸¸åŒ…æ‹¬è®¿é—®æ–‡æœ¬æ•°æ®ï¼Œæ— è®ºæ˜¯ç½‘é¡µã€æ¨æ–‡ã€è¯„è®ºã€PDF æ–‡æ¡£è¿˜æ˜¯åŸå§‹æ–‡æœ¬æ ¼å¼ã€‚ç„¶åå°†å…¶åˆ†è§£ä¸ºç®—æ³•å¯ä»¥è½»æ¾å¤„ç†çš„è¡¨ç¤ºå½¢å¼(ä¾‹å¦‚ï¼Œè¡¨ç¤ºå•ä¸ªå•è¯æˆ–å­—æ¯çš„æ ‡è®°)ï¼Œç§»é™¤å¸¸ç”¨å•è¯(åœç”¨è¯)(ä¾‹å¦‚ï¼Œâ€œandâ€ã€â€œorâ€ã€â€œtheâ€)ã€‚è¿›ä¸€æ­¥çš„æ ‡å‡†åŒ–ä¹‹åæ˜¯è¯¸å¦‚ç‰¹å¾æå–å’Œ/æˆ–å»é™¤å™ªå£°çš„ä»»åŠ¡ã€‚æœ€åï¼Œå„ç§æ¨¡å‹å’Œæ–¹æ³•(ä¾‹å¦‚ï¼Œä¸»é¢˜å»ºæ¨¡ã€æƒ…æ„Ÿåˆ†æã€ç¥ç»ç½‘ç»œç­‰ã€‚)éƒ½é€‚ç”¨ã€‚

![](img/a8765a2444b14ea5c8c9f528e99072df.png)

æ–‡æœ¬æ•°æ®çš„é€šç”¨é¢„å¤„ç†æ–¹æ³•(å›¾ç‰‡ç”±ä½œè€…æä¾›)

å› ä¸ºæˆ‘ä»¬å·²ç»æœ‰äº†ä¸€äº›è¦å¤„ç†çš„æ–‡æœ¬ï¼Œæ‰€ä»¥æˆ‘ä»¬å¯ä»¥çœ‹çœ‹é¢„å¤„ç†æ•°æ®çš„åç»­æ­¥éª¤ã€‚æˆ‘ä»¬å¯ä»¥ä»æ ‡è®°åŒ–å¼€å§‹ã€‚å¾ˆå¤š Python åº“éƒ½å¯ä»¥å®Œæˆæ ‡è®°åŒ–ï¼ŒåŒ…æ‹¬æœºå™¨å­¦ä¹ åº“ scikit learnã€‚è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡çš„ä¸€ä¸ªæµè¡Œåº“æ˜¯è‡ªç„¶è¯­è¨€å·¥å…·åŒ…(NLTK):

```
import nltk
```

**æ³¨æ„:**Python ä¸­å¦ä¸€ä¸ªå¼ºå¤§çš„ NLP æ›¿ä»£åº“æ˜¯ **spaCy** ã€‚

## ç¬¦å·åŒ–

å³ä½¿åœ¨åº“ä¸­ï¼Œä¹Ÿç»å¸¸æœ‰ä¸åŒçš„æ ‡è®°å™¨å¯ä¾›é€‰æ‹©ã€‚ä¾‹å¦‚ï¼ŒNLTK ä¹Ÿæœ‰ä¸€ä¸ª RegexpTokenizer(ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼)ã€‚è¿™é‡Œæˆ‘ä»¬å°†ä½¿ç”¨ TreebankWordTokenizer æ¥è¿‡æ»¤æ‰ä¸€äº›æ ‡ç‚¹ç¬¦å·å’Œç©ºæ ¼ã€‚

```
from nltk.tokenize import TreebankWordTokenizer
```

æˆ‘ä»¬å°†ä»æ–‡æœ¬æ•°æ®ä¸­æå–ä¸€å°æ®µæ–‡æœ¬æ¥è¯´æ˜è¿™æ˜¯å¦‚ä½•å·¥ä½œçš„ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬å°†å®ƒä½œä¸ºä¸€ä¸ªå­—ç¬¦ä¸²(Python ä¸­ç”¨æ¥å­˜å‚¨æ–‡æœ¬æ•°æ®çš„ä¸€ç§æ•°æ®ç±»å‹)å­˜å‚¨åœ¨ä¸€ä¸ªåä¸º **txt** çš„å˜é‡ä¸­ã€‚

```
txt = "The government last July called the energy sector debt situation a state of emergency. <p> This was during the mid-year budget review during which the Finance Minister Ken Ofori-Atta castigated the previous NDC government for entering into obnoxious take-or-pay contracts signed by the NDC , which obligate us to pay for capacity we do not need . <p> The government pays over GH ? 2.5 billion annually for some 2,300MW in installed capacity which the country does not consume ."
```

ç®€å•çš„ç¬¬ä¸€æ­¥å¯èƒ½æ˜¯å°†æ‰€æœ‰æ–‡æœ¬è½¬æ¢æˆå°å†™å­—æ¯ã€‚æˆ‘ä»¬å¯ä»¥é€šè¿‡**ä¸‹**åŠŸèƒ½æ¥å®ç°ã€‚

```
txt = txt.lower()
txt>>> 'the government last july called the energy sector debt situation a state of emergency. <p> this was during the mid-year budget review during which the finance minister ken ofori-atta castigated the previous ndc government for entering into obnoxious take-or-pay contracts signed by the ndc , which obligate us to pay for capacity we do not need .  <p> the government pays over gh ? 2.5 billion annually for some 2,300mw in installed capacity which the country does not consume .'
```

æ¥ä¸‹æ¥æˆ‘ä»¬å¯ä»¥åˆ›å»ºä¸€ä¸ª**treebankwodtokenizer**ç±»çš„å®ä¾‹ï¼Œå¹¶ä½¿ç”¨ **tokenize** å‡½æ•°ï¼Œä¼ å…¥æˆ‘ä»¬çš„ **txt** å˜é‡ã€‚è¾“å‡ºå¦‚ä¸‹æ‰€ç¤º(æ˜¾ç¤ºå‰ 20 ä¸ª)ã€‚

```
tk = TreebankWordTokenizer()
tk_words = tk.tokenize(txt)
tk_words[:20]>>> ['the',
     'government',
     'last',
     'july',
     'called',
     'the',
     'energy',
     'sector',
     'debt',
     'situation',
     'a',
     'state',
     'of',
     'emergency.',
     '<',
     'p',
     '>',
     'this',
     'was',
     'during']
```

è¿™ä¸ª **casual_tokenize** å¯¹äºç¤¾äº¤åª’ä½“ç¬¦å·åŒ–å¾ˆæœ‰ç”¨ï¼Œå› ä¸ºå®ƒå¯ä»¥å¾ˆå¥½åœ°å¤„ç†ç”¨æˆ·åå’Œè¡¨æƒ…ç¬¦å·ä¹‹ç±»çš„äº‹æƒ…ã€‚ **TweetTokenizer** ä¹Ÿä¸º Twitter åˆ†æä¿æŒäº†å®Œæ•´çš„æ•£åˆ—æ ‡ç­¾ã€‚

## å¤„ç†åœç”¨è¯å’Œæ ‡ç‚¹ç¬¦å·

ä¸‹ä¸€ä»¶å¸¸è§çš„äº‹æƒ…æ˜¯åˆ é™¤åœç”¨è¯ã€‚è¿™äº›æ˜¯åœ¨å¥å­ç»“æ„ä¸­ä½¿ç”¨çš„å¸¸è§é«˜é¢‘è¯ï¼Œä½†å¯¹åˆ†ææ¥è¯´æ²¡æœ‰å¤ªå¤šæ„ä¹‰ã€‚è¿™äº›è¯åŒ…æ‹¬â€œtheâ€ã€â€œandâ€ã€â€œtoâ€ã€â€œaâ€ç­‰ã€‚æˆ‘ä»¬å¯ä»¥ä» NLTK åº“ä¸­ä¸‹è½½è¿™äº›å•è¯çš„åˆ—è¡¨ï¼Œå¹¶å°†å®ƒä»¬å­˜å‚¨åœ¨ä¸€ä¸ªå˜é‡( **sw** )ä¸­ï¼Œå¦‚ä¸‹æ‰€ç¤º:

```
nltk.download("stopwords", quiet=True)
sw = nltk.corpus.stopwords.words("english")
```

æˆ‘ä»¬å¯ä»¥çœ‹çœ‹è¿™äº›å•è¯çš„å‰ 20 ä¸ªã€‚

```
sw[:20]>>> ['i',
     'me',
     'my',
     'myself',
     'we',
     'our',
     'ours',
     'ourselves',
     'you',
     "you're",
     "you've",
     "you'll",
     "you'd",
     'your',
     'yours',
     'yourself',
     'yourselves',
     'he',
     'him',
     'his']
```

åœ¨å†™è¿™ç¯‡æ–‡ç« çš„æ—¶å€™ï¼Œåˆ—è¡¨ä¸­æœ‰è¶…è¿‡ 100 ä¸ªåœç”¨è¯ã€‚å¦‚æœä½ æ„Ÿå…´è¶£ï¼Œå¯ä»¥ä½¿ç”¨ **len** å‡½æ•°æŸ¥çœ‹åˆ—è¡¨ä¸­æœ‰å¤šå°‘å•è¯(ä¾‹å¦‚ **len(sw)** )ã€‚

ç°åœ¨è®©æˆ‘ä»¬æŠŠè¿™äº›åœç”¨è¯ä»æ ‡è®°è¯ä¸­å»æ‰ã€‚æˆ‘ä»¬å¯ä»¥ä½¿ç”¨ Python list comprehension æ¥è¿‡æ»¤ **tk_words** åˆ—è¡¨ä¸­ä¸åœ¨åœç”¨å­—è¯( **sw** )åˆ—è¡¨ä¸­çš„å­—è¯ã€‚

```
tk_words_filtered_sw = [word for word in tk_words if word not in sw]
```

å¦‚æœæ‚¨ä¸ç†Ÿæ‚‰åˆ—è¡¨ç†è§£ï¼Œå®ƒä»¬æœ¬è´¨ä¸Šæ˜¯ä¸€ç§åˆ›å»ºåˆ—è¡¨å’Œéå†åˆ—è¡¨æ•°æ®ç»“æ„çš„ç®€æ´æ–¹æ³•ã€‚è¿™é€šå¸¸é¿å…äº†éœ€è¦å¤šå‡ è¡Œå•ç‹¬çš„â€œfor å¾ªç¯â€ä»£ç ã€‚å‡è®¾æˆ‘æƒ³æ±‚ 0 åˆ° 5 çš„å¹³æ–¹ã€‚æˆ‘ä»¬å¯ä»¥åƒè¿™æ ·ä½¿ç”¨ for å¾ªç¯:

```
squared_nums = []
for n in range(5):
    squared_nums.append(n**2)
```

è™½ç„¶è¿™æœ‰é¢„æœŸçš„æ•ˆæœï¼Œä½†æˆ‘ä»¬å¯ä»¥åˆ›å»ºåˆ—è¡¨ï¼Œå¹¶ä½¿ç”¨åˆ—è¡¨ç†è§£æ¥éå†å®ƒï¼Œè€Œä¸æ˜¯å°†å®ƒç»„åˆæˆä¸€è¡Œä»£ç :

```
squared_nums = [n**2 for n in range(5)]
```

å¦‚æœæˆ‘ä»¬è¾“å‡ºå˜é‡ **tk_words_filtered_sw** çš„å†…å®¹ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°å¾ˆå¤šåœç”¨è¯ç°åœ¨éƒ½è¢«åˆ é™¤äº†(ä¸ºç®€æ´èµ·è§ï¼Œç¼©å†™ä¸º):

```
tk_words_filtered_sw>>> ['government',
     'last',
     'july',
     'called',
     'energy',
     'sector',
     'debt',
     'situation',
     'state',
     'emergency.',
     '<',
     'p',
     '>',
     'mid-year',
     'budget',
     'review',
     'finance',
     'minister',
     'ken',
     'ofori-atta',
     'castigated',
     'previous',
     'ndc',
     'government',
     'entering',
     'obnoxious',
     'take-or-pay',
     'contracts',
     'signed',
     'ndc',
     ','
...
```

å¯ä»¥çœ‹åˆ°æ–‡æœ¬ä¸­ä»ç„¶æœ‰å¥å·(å¥å·)ã€é—®å·ã€é€—å·ä¹‹ç±»çš„æ ‡ç‚¹ç¬¦å·ã€‚åŒæ ·ï¼Œæˆ‘ä»¬é€šå¸¸ä¼šä»åˆ—è¡¨ä¸­åˆ é™¤è¿™äº›å†…å®¹ã€‚è¿™å¯ä»¥é€šè¿‡å¤šç§ä¸åŒçš„æ–¹å¼æ¥å®ç°ã€‚è¿™é‡Œï¼Œæˆ‘ä»¬ä½¿ç”¨å­—ç¬¦ä¸²åº“æ¥åˆ é™¤å­˜å‚¨ç»“æœçš„æ ‡ç‚¹ç¬¦å·ï¼Œè¿™ä¸ªç»“æœå­˜å‚¨åœ¨ä¸€ä¸ªåä¸º **no_punc** çš„å˜é‡ä¸­ã€‚

```
import string
no_punc = ["".join( j for j in i if j not in string.punctuation) for i in  tk_words_filtered_sw]
no_punc>>> ['government',
     'last',
     'july',
     'called',
     'energy',
     'sector',
     'debt',
     'situation',
     'state',
     'emergency',
     '',
     'p',
     '',
     'midyear',
     'budget'
...
```

ç„¶åæˆ‘ä»¬å¯ä»¥ä»åˆ—è¡¨ä¸­è¿‡æ»¤æ‰è¿™äº›ç©ºå­—ç¬¦ä¸²(â€œâ€)ã€‚æˆ‘ä»¬å¯ä»¥å°†ç»“æœå­˜å‚¨åœ¨ä¸€ä¸ªåä¸º **filtered_punc** çš„å˜é‡ä¸­ã€‚

```
filtered_punc = list(filter(None, no_punc))
filtered_punc>>> ['government',
     'last',
     'july',
     'called',
     'energy',
     'sector',
     'debt',
     'situation',
     'state',
     'emergency',
     'p',
     'midyear',
     'budget'
...
```

æœ€åï¼Œæˆ‘ä»¬å¯èƒ½è¿˜æƒ³ä»åˆ—è¡¨ä¸­åˆ é™¤æ•°å­—ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ **isdigit** å‡½æ•°æ£€æŸ¥å­—ç¬¦ä¸²æ˜¯å¦åŒ…å«ä»»ä½•æ•°å­—ã€‚

```
str_list = [i for i in filtered_punc if not any(j.isdigit() for j in i)]
```

å¦‚æ‚¨æ‰€è§ï¼Œæ–‡æœ¬æ•°æ®å¯èƒ½éå¸¸æ··ä¹±ï¼Œéœ€è¦åœ¨å¼€å§‹è¿è¡Œå„ç§åˆ†æä¹‹å‰è¿›è¡Œå¤§é‡æ¸…ç†ã€‚å¦ä¸€ç§å»é™¤ä¸å¿…è¦å•è¯çš„å¸¸ç”¨æ–¹æ³•æ˜¯ä½¿ç”¨è¯å¹²æ³•æˆ–è¯å¹²æ³•ã€‚

## è¯å¹²åŒ–å’Œè¯æ±‡åŒ–

è¯å¹²åŒ–æŒ‡çš„æ˜¯å°†å•è¯ç¼©å‡åˆ°å®ƒä»¬çš„è¯æ ¹(è¯å¹²)å½¢å¼ã€‚ä¾‹å¦‚ï¼Œå•è¯â€œwaitedâ€ã€â€œwaitiesâ€ã€â€œwaitingâ€å¯ä»¥ç®€åŒ–ä¸ºâ€œwaitâ€ã€‚æˆ‘ä»¬å¯ä»¥çœ‹ä¸€ä¸ªå«åš**æ³¢ç‰¹è¯å¹²åˆ†æå™¨**çš„æ™®é€šè¯å¹²åˆ†æå™¨çš„ä¾‹å­ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬å¯ä»¥åˆ›å»ºä¸€ä¸ªç®€çŸ­çš„å•è¯åˆ—è¡¨æ¥æ¼”ç¤ºè¿™æ˜¯å¦‚ä½•å·¥ä½œçš„ã€‚è¿™ä¸€é˜¶æ®µé€šå¸¸è·Ÿéšå•è¯çš„æ ‡è®°åŒ–ã€‚

```
word_list = ["flying", "flies", "waiting", "waits", "waited", "ball", "balls", "flyer"]
```

æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å°†ä» NLTK å¯¼å…¥ Porter è¯å¹²åˆ†æå™¨ã€‚

```
from nltk.stem.porter import PorterStemmer
```

åˆ¶ä½œä¸€ä¸ªåä¸º **ps** çš„ **PorterStemmer** ç±»çš„å®ä¾‹ã€‚

```
ps = PorterStemmer()
```

æœ€åç”¨ä¸€ä¸ªåˆ—è¡¨ç†è§£åˆ—è¡¨ä¸­çš„æ¯ä¸ªå•è¯ã€‚å…¶ç»“æœå¯ä»¥åœ¨ä¸‹é¢çœ‹åˆ°ã€‚

```
stem_words = [ps.stem(word) for word in word_list]
stem_words>>> ['fli', 'fli', 'wait', 'wait', 'wait', 'ball', 'ball', 'flyer']
```

åœ¨æœ¬åº”å…·æœ‰ä¸åŒè¯å¹²çš„å•è¯è¢«è¯å¹²åŒ–ä¸ºåŒä¸€ä¸ªè¯æ ¹çš„æƒ…å†µä¸‹ï¼Œå¯èƒ½ä¼šå‘ç”Ÿè¯å¹²è¿‡åº¦ã€‚ä¹Ÿæœ‰å¯èƒ½å¾—åˆ°ä¸‹é¢çš„è¯å¹²ã€‚è¿™åœ¨æœ¬è´¨ä¸Šæ˜¯ç›¸åçš„(åº”è¯¥è¯æ ¹ç›¸åŒçš„å•è¯å´ä¸æ˜¯)ã€‚æœ‰å„ç§ä¸åŒçš„è¯å¹²åˆ†æå™¨å¯ä»¥ä½¿ç”¨ï¼Œå¦‚æ³¢ç‰¹ï¼Œè‹±è¯­æ–¯ç‰¹æ¢…å°”ï¼Œæ´¾æ–¯å’Œæ´›æ–‡æ–¯ä»…ä¸¾å‡ ä¾‹ã€‚æ³¢ç‰¹æ¢—æ˜¯æœ€å¹¿æ³›ä½¿ç”¨çš„ä¸€ç§ã€‚

æœ‰äº›è¯å¹²åˆ†æå™¨æ¯”å…¶ä»–è¯å¹²åˆ†æå™¨æ›´â€œè‹›åˆ»â€æˆ–æ›´â€œæ¸©å’Œâ€ï¼Œå› æ­¤æ‚¨å¯èƒ½éœ€è¦å°è¯•ä¸åŒçš„è¯å¹²åˆ†æå™¨æ¥è·å¾—æƒ³è¦çš„ç»“æœã€‚æ‚¨ä¹Ÿå¯ä»¥æ ¹æ®è¾“å‡ºå†³å®šåœ¨åœç”¨è¯ç§»é™¤ä¹‹å‰æˆ–ä¹‹ååº”ç”¨è¯å¹²ã€‚

å¦ä¸€æ–¹é¢ï¼Œè¯æ¡é‡Šä¹‰æ˜¯é€šè¿‡è¯†åˆ«ä¸€ä¸ªå•è¯ç›¸å¯¹äºå®ƒå‡ºç°çš„å¥å­çš„æ„ä¹‰æ¥å·¥ä½œçš„ã€‚æœ¬è´¨ä¸Šï¼Œä¸Šä¸‹æ–‡å¯¹äºå¼•ç†æ»¡è¶³å¾ˆé‡è¦ã€‚è¿™ç§æƒ…å†µä¸‹çš„è¯æ ¹å«åšå¼•ç†ã€‚ä¾‹å¦‚ï¼Œå•è¯â€œbetterâ€å¯ä»¥ç”¨å•è¯â€œgoodâ€æ¥è¡¨ç¤ºï¼Œå› ä¸ºè¿™ä¸ªå•è¯å°±æ˜¯å®ƒçš„æ¥æºã€‚è¿™ä¸ªè¿‡ç¨‹åœ¨è®¡ç®—ä¸Šæ¯”è¯å¹²æå–æ›´æ˜‚è´µã€‚

æˆ‘ä»¬éœ€è¦é¦–å…ˆä» NLTK(ä¸€ä¸ªåŒ…å«è‹±è¯­åè¯ã€å½¢å®¹è¯ã€å‰¯è¯å’ŒåŠ¨è¯çš„å¤§å‹å•è¯æ•°æ®åº“)ä¸‹è½½ wordnet èµ„æºã€‚

```
nltk.download('wordnet', quiet=True)
```

æˆ‘ä»¬å°†å¯¼å…¥å¹¶åˆ›å»ºä¸€ä¸ª **WordNetLemmatizer** çš„å®ä¾‹ï¼Œå¹¶å°†å…¶åº”ç”¨äºæˆ‘ä»¬ä¹‹å‰ä½¿ç”¨çš„å•è¯åˆ—è¡¨ã€‚

```
word_list>>> ['flying', 'flies', 'waiting', 'waits', 'waited', 'ball', 'balls', 'flyer']from nltk.stem import WordNetLemmatizer
lm = WordNetLemmatizer()lem_words = [lm.lemmatize(word) for word in word_list]
lem_words>>> ['flying', 'fly', 'waiting', 'wait', 'waited', 'ball', 'ball', 'flyer']
```

å¦‚æœæˆ‘ä»¬å°† **lem_words** ä¸ **stem_words** (å¦‚ä¸‹)è¿›è¡Œæ¯”è¾ƒï¼Œæ‚¨å¯ä»¥çœ‹åˆ°ï¼Œå°½ç®¡åœ¨æŸäº›æƒ…å†µä¸‹ç›¸ä¼¼/ç›¸åŒï¼Œä½†å¯¹äºä¸€äº›å•è¯ï¼Œå¦‚â€œflyingâ€å’Œâ€œflies â€,ä½¿ç”¨å¼•ç†åŒ–ä¿ç•™äº†å…¶å«ä¹‰ï¼Œå¦åˆ™è¯å¹²åŒ–å°†ä¼šä¸¢å¤±è¯¥å«ä¹‰ã€‚

```
stem_words>>> ['fli', 'fli', 'wait', 'wait', 'wait', 'ball', 'ball', 'flyer']
```

## æ ‡å‡†åŒ–/ç¼©æ”¾

ç§»é™¤ç‰¹å¾(å•è¯/æœ¯è¯­)çš„å¦ä¸€ç§æ–¹æ³•æ˜¯ä½¿ç”¨ä»£è¡¨æœ¯è¯­é¢‘ç‡çš„ **tf-idf** é‡æ–°è°ƒæ•´æ•°æ®ï¼Œé€†æ–‡æ¡£é¢‘ç‡ã€‚

![](img/4c423847e3e0e4164bd96d7286503a9f.png)

tf-idf æ–¹ç¨‹(å›¾ç‰‡ç”±ä½œè€…æä¾›)

è¿™ç”¨äºæŸ¥çœ‹ä¸€ä¸ªæœ¯è¯­(å•è¯)å¯¹æ–‡æ¡£é›†åˆ(è¯­æ–™åº“)ä¸­çš„å•ä¸ªæ–‡æ¡£æœ‰å¤šé‡è¦ã€‚æˆ‘ä»¬æœ¬è´¨ä¸Šæ˜¯ç”¨è¿™ä¸ªä½œä¸ºæƒé‡ã€‚æœ¯è¯­é¢‘ç‡æ˜¯è¯¥å•è¯/æœ¯è¯­åœ¨æ–‡æ¡£ä¸­å‡ºç°çš„é¢‘ç‡ã€‚ä¾‹å¦‚ï¼Œå¦‚æœæˆ‘ä»¬æœ‰ä¸¤ä¸ªæ–‡ä»¶ğ‘‘1 å’Œğ‘‘2 çœ‹èµ·æ¥åƒè¿™æ ·ã€‚

ğ‘‘1 =â€œé‚£ä¸ªå°ç”·å­©åœ¨æˆ¿å­é‡Œã€‚â€ğ‘‘2 =â€œé‚£ä¸ªå°ç”·å­©ä¸åœ¨æˆ¿å­é‡Œã€‚â€

å¦‚æœåœ¨æ–‡æ¡£ 1 ( ğ‘‘1)ä¸­æ„Ÿå…´è¶£çš„æœ¯è¯­æ˜¯â€œç”·å­©â€ï¼Œè¿™åœ¨ä¸ƒä¸ªå•è¯ä¸­å‡ºç°ä¸€æ¬¡ 1/7=0.1428ã€‚æˆ‘ä»¬å¯ä»¥å¯¹æ¯ä¸ªæ–‡æ¡£ä¸­çš„æ¯ä¸ªæœ¯è¯­åšåŒæ ·çš„äº‹æƒ…ã€‚ä¸€æ—¦æˆ‘ä»¬è®¡ç®—å‡ºæœ¯è¯­é¢‘ç‡ï¼Œæˆ‘ä»¬å°±å¯ä»¥ç”¨è¯­æ–™åº“ä¸­æ–‡æ¡£æ€»æ•°çš„å¯¹æ•°ä¹˜ä»¥åŒ…å«è¯¥æœ¯è¯­çš„æ–‡æ¡£æ•°ã€‚è¿™å‘Šè¯‰æˆ‘ä»¬ä¸€ä¸ªç‰¹å®šçš„æœ¯è¯­ä¸ä¸€ä¸ªæ–‡æ¡£æ˜¯æ›´ç›¸å…³ã€æ›´ä¸ç›¸å…³è¿˜æ˜¯åŒç­‰ç›¸å…³ã€‚åœ¨ä¸Šé¢çš„ç¤ºä¾‹ä¸­ï¼Œæ–‡æ¡£äºŒä¸­çš„å•è¯â€œnotâ€å¯¹äºåŒºåˆ†è¿™ä¸¤ä¸ªæ–‡æ¡£éå¸¸é‡è¦ã€‚é«˜æƒé‡æ˜¯ç”±äºæ–‡æ¡£ä¸­çš„é«˜æœ¯è¯­é¢‘ç‡å’Œè¯­æ–™åº“(æ–‡æ¡£é›†åˆ)ä¸­çš„ä½æœ¯è¯­é¢‘ç‡ã€‚TF-IDF ä¹Ÿå¯ä»¥ç”¨äºæ–‡æœ¬æ‘˜è¦ä»»åŠ¡ã€‚

ä¸ºäº†åœ¨å®è·µä¸­ä½¿ç”¨ TF-IDFï¼Œæˆ‘ä»¬å°†éœ€è¦ä½¿ç”¨æˆ‘ä»¬å‰é¢çœ‹åˆ°çš„æ–‡æœ¬æ–‡ä»¶ã€‚ä¸ºäº†åšå¾—æ›´å¥½ï¼Œæˆ‘ä¿å­˜äº†ä¸€ä¸ªæ²¡æœ‰åœç”¨è¯å’Œ HTML æ®µè½æ ‡ç­¾çš„ç‰ˆæœ¬ã€‚è¿™ä¹Ÿè¢«è½¬æ¢æˆå°å†™ã€‚æœ¬è´¨ä¸Šï¼Œè¿™æ˜¯ä½¿ç”¨ä¸€ä¸ªå¾ªç¯ä¸€ä¸ªæ¥ä¸€ä¸ªåœ°æ‰“å¼€æ–‡ä»¶ï¼Œä¸€è¡Œä¸€è¡Œåœ°è¯»å–å®ƒä»¬ï¼Œåˆ†å‰²æˆå•è¯ã€‚å¦‚æœè¿™ä¸ªè¯ä¸æ˜¯åœç”¨è¯ï¼Œå®ƒå°†è¢«å†™å…¥ä¸€ä¸ªæ–°æ–‡ä»¶ã€‚æœ€åï¼Œ **BeautifulSoup** åº“ç”¨äºä»æ–‡æœ¬æ•°æ®ä¸­å»é™¤æ‰€æœ‰çš„ HTML æ ‡ç­¾ã€‚

```
**from** bs4 **import** BeautifulSoup

file_path **=** "./NLP/text/"
file_list **=** ['20-01', '20-02', '20-03', '20-04']

**for** file **in** file_list:
    current_file **=** open(file_path **+** file **+** ".txt")
    line **=** current_file.read().lower()
    soup **=** BeautifulSoup(line)
    words **=** soup.get_text().split()
    **for** word **in** words:
        **if** word **not** **in** sw:
            formated_list **=** open((file_path **+** file **+** '-f.txt'),'a')
            formated_list.write(" "**+**word)
            formated_list.close()

    current_file.close()
```

æˆ‘ä»¬ç°åœ¨å¯ä»¥åŠ è½½è¿™äº›æ–‡ä»¶å¹¶å°†å®ƒä»¬çš„å†…å®¹å­˜å‚¨åœ¨å˜é‡ä¸­ã€‚

```
path = "./NLP/text/processed/"txt_file_1 = open(path + "20â€“01-f.txt") 
file_1 = txt_file_1.read()txt_file_2 = open(path + "20â€“02-f.txt") 
file_2 = txt_file_2.read()txt_file_3 = open(path + "20â€“03-f.txt")
file_3 = txt_file_3.read()txt_file_4 = open(path + "20â€“04-f.txt") 
file_4 = txt_file_4.read()
```

æˆ‘ä»¬å¯ä»¥å°†ä»æ–‡ä»¶ä¸­æå–çš„æ–‡æœ¬æ•°æ®æ”¾åœ¨ä¸€ä¸ªåˆ—è¡¨ä¸­ï¼Œä»¥ä¾¿äºä½¿ç”¨ã€‚

```
data_files = [file_1, file_2, file_3, file_4]
```

æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å°†å¯¼å…¥ **pandas** åº“ï¼Œå®ƒåœ¨æ•°æ®ç§‘å­¦ä¸­ç»å¸¸ä½¿ç”¨ï¼Œå¹¶æä¾›ä»¥è¡¨æ ¼ç»“æ„(æ•°æ®æ¡†)è¡¨ç¤ºæ•°æ®çš„åŠŸèƒ½ã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å°†ä» **sklearn** æœºå™¨å­¦ä¹ åº“ä¸­å¯¼å…¥**tfidf çŸ¢é‡å™¨**ï¼Œå®ƒå°†æ ‡è®°æ–‡æ¡£å¹¶åº”ç”¨ IDF æƒé‡ã€‚

```
import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
```

æœ€åï¼Œæˆ‘ä»¬å¯ä»¥åˆ›å»ºè¯¥ç±»çš„ä¸€ä¸ªå®ä¾‹ï¼Œå¹¶ä½¿å…¶é€‚åˆæ•°æ®ã€‚æˆ‘ä»¬å¯ä»¥åœ¨æ•°æ®æ¡†ä¸­æ˜¾ç¤ºç»“æœï¼Œå…¶ä¸­æ¯ä¸ªè¦ç´ å’Œç›¸å…³çš„ TF-IDF æƒé‡æŒ‰é™åºæ’åˆ—ã€‚æœ€åï¼Œä¸ºäº†ç®€æ´èµ·è§ï¼Œæˆ‘ä»¬å°†åªè¾“å‡ºå‰ 30 ä¸ªæœ¯è¯­ã€‚

```
tv = TfidfVectorizer(use_idf=True)
tfIdf = tv.fit_transform(data_files)
df = pd.DataFrame(tfIdf[0].T.todense(), index=tv.get_feature_names(), columns=["TF-IDF"])
df = df.sort_values('TF-IDF', ascending=False)
df.head(30)
```

![](img/55c92196c3adb20a26abbafa9346b9b6.png)

ç†ŠçŒ«æ•°æ®æ¡†çš„è¾“å‡º-ä¸ºç®€æ´èµ·è§è€Œç¼©çŸ­(å›¾ç‰‡ç”±ä½œè€…æä¾›)

## è¯é¢‘

æˆ‘ä»¬å¯ä»¥å¯¹å•è¯æ•°æ®åšçš„æœ€ç®€å•çš„äº‹æƒ…ä¹‹ä¸€æ˜¯æŸ¥çœ‹ä¸€ä¸ªç‹¬ç‰¹çš„å•è¯åœ¨æ–‡æ¡£ä¸­å‡ºç°çš„é¢‘ç‡(å’Œ/æˆ–å®ƒåœ¨è¯­æ–™åº“ä¸­çš„ç´¯ç§¯é¢‘ç‡)ã€‚æˆ‘ä»¬å¯ä»¥ä½¿ç”¨ **FreqDist** å‡½æ•°æ¥è®¡ç®—ä¸€ä¸ªå­—å…¸ï¼Œå®ƒçš„é”®/å€¼å¯¹åŒ…å«æ¯ä¸ªæœ¯è¯­(å•è¯)ï¼Œåè·Ÿå®ƒåœ¨æ–‡æœ¬ä¸­å‡ºç°çš„æ¬¡æ•°ã€‚ä¾‹å¦‚ï¼Œå•è¯â€œgovernmentâ€åœ¨æˆ‘ä»¬ä¹‹å‰ä½¿ç”¨çš„åŸå§‹ç®€çŸ­ç¤ºä¾‹æ–‡æœ¬ä¸­å‡ºç°äº† 3 æ¬¡ã€‚

```
dist = nltk.FreqDist(str_list)
dist>>> FreqDist({'government': 3, 'p': 2, 'ndc': 2, 'capacity': 2, 'last': 1, 'july': 1, 'called': 1, 'energy': 1, 'sector': 1, 'debt': 1, ...})
```

ä¸ºäº†ä½¿å®ƒæ›´å®¹æ˜“å¯è§†åŒ–ï¼Œæˆ‘ä»¬å¯ä»¥å°†å®ƒè¾“å‡ºä¸ºä¸€ä¸ªå›¾ã€‚æ­£å¦‚æ‚¨æ‰€çœ‹åˆ°çš„ï¼Œå•è¯ government å‡ºç°äº† 3 æ¬¡ï¼Œå­—æ¯ p(æ¥è‡ª HTML æ®µè½æ ‡è®°)å‡ºç°äº†ä¸¤æ¬¡ï¼Œå…¶ä»–å•è¯åªå‡ºç°äº†ä¸€æ¬¡ã€‚

```
dist.plot();
```

![](img/d64d358fa033e41d04b2c59b5927c19a.png)

å•è¯çš„æƒ…èŠ‚å’Œå‡ºç°é¢‘ç‡(å›¾ç‰‡ç”±ä½œè€…æä¾›)

å¦ä¸€ç§æ˜¾ç¤ºå•è¯é¢‘ç‡çš„è§†è§‰æ–¹å¼æ˜¯ä½¿ç”¨å•è¯äº‘ï¼Œè¿™é‡Œå•è¯è¶Šå¤§ï¼Œå‡ºç°çš„æ¬¡æ•°å°±è¶Šå¤šã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ **WordCloud** åº“ã€‚

```
from wordcloud import WordCloud
```

æˆ‘ä»¬è¿˜éœ€è¦æ¥è‡ªç”¨äºå„ç§å¯è§†åŒ–çš„ **matplotlib** åº“ä¸­çš„ç»˜å›¾( **plt** )ã€‚ **%matplotlib inline** è®¾ç½®ç»˜å›¾å‘½ä»¤ï¼Œä»¥ç¡®ä¿å½“ä¸ Jupyter ç¬”è®°æœ¬ç­‰å­—ä½“ç«¯ä¸€èµ·ä½¿ç”¨æ—¶ï¼Œç»˜å›¾å‡ºç°åœ¨ä»£ç å•å…ƒæ ¼ä¸‹æ–¹å¹¶å­˜å‚¨åœ¨ç¬”è®°æœ¬ä¸­ã€‚

```
import matplotlib.pyplot as plt
%matplotlib inline
```

æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬éœ€è¦å°†æ•°æ®è½¬æ¢æˆ word cloud å‡½æ•°çš„æ­£ç¡®æ ¼å¼ã€‚å®ƒæ¥å—ä¸€ä¸ªå­—ç¬¦ä¸²ï¼Œå› æ­¤æˆ‘ä»¬å°†æŠŠæ ‡è®°åŒ–çš„å­—ç¬¦ä¸²åˆ—è¡¨æŠ˜å æˆä¸€ä¸ªå•ç‹¬çš„å­—ç¬¦ä¸²ï¼Œå•è¯ä¹‹é—´ç•™æœ‰ç©ºæ ¼ï¼Œä½¿ç”¨å†…ç½®çš„ **join** å‡½æ•°è¿›è¡Œå­—ç¬¦ä¸²è¿æ¥(å°†å­—ç¬¦ä¸²è¿æ¥åœ¨ä¸€èµ·)ã€‚

```
flattend_text = " ".join(str_list)
```

æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ä¼ å…¥æ–‡æœ¬å­—ç¬¦ä¸²çš„ **generate** å‡½æ•°åˆ›å»ºå•è¯äº‘ã€‚

```
wc = WordCloud().generate(flattend_text)
```

æœ€åï¼Œæˆ‘ä»¬å°†ä½¿ç”¨**åŒçº¿æ€§**æ’å€¼é€‰é¡¹è¾“å‡ºå…³é—­è½´æ–‡æœ¬çš„å•è¯äº‘ï¼Œä»¥å¹³æ»‘å›¾åƒçš„å¤–è§‚ã€‚

```
plt.imshow(wc, interpolation='bilinear')
plt.axis("off")
plt.show()
```

![](img/3d5ba905eb0c387851948ee544b23a8e.png)

è¯äº‘è¾“å‡º(å›¾ç‰‡ç”±ä½œè€…æä¾›)

æ‚¨è¿˜å¯ä»¥ä½¿ç”¨å…¶ä»–å¯é€‰å‚æ•°ã€‚ä¸€äº›æµè¡Œçš„æ–¹æ³•åŒ…æ‹¬è®¾ç½®æœ€å¤§å­—ä½“å¤§å°(max_font_size)å’ŒåŒ…å«çš„å­—æ•°(å¦‚æœä½ æœ‰å¾ˆå¤šå­—çš„è¯ä¼šå¾ˆæœ‰å¸®åŠ©)ä»¥åŠæ”¹å˜èƒŒæ™¯é¢œè‰²ã€‚ä¾‹å¦‚ï¼Œæˆ‘ä»¬å¯ä»¥å°†æ˜¾ç¤ºçš„æœ€å¤§å­—æ•°è®¾ç½®ä¸º 10ï¼Œå¹¶å°†èƒŒæ™¯è®¾ç½®ä¸ºç™½è‰²ã€‚

```
wc_2 = WordCloud(max_words = 10, background_color = "white").generate(flattend_text)
plt.imshow(wc_2, interpolation='bilinear')
plt.axis("off")
plt.show()
```

![](img/e0d6f9740351644d2af0d7abfaa12cb1.png)

è¯äº‘è¾“å‡º(å›¾ç‰‡ç”±ä½œè€…æä¾›)

## n å…ƒè¯­æ³•åˆ†æ

å½“æˆ‘ä»¬æŠŠå•è¯ç¬¦å·åŒ–ï¼Œå¹¶æŠŠå®ƒä»¬è¡¨ç¤ºæˆä¸€ä¸ªå•è¯åŒ…æ—¶ï¼Œæˆ‘ä»¬å°±å¤±å»äº†ä¸€äº›ä¸Šä¸‹æ–‡å’Œæ„ä¹‰ã€‚å•ä¸ªå•è¯æœ¬èº«å¹¶ä¸èƒ½è¯´æ˜å¤ªå¤šï¼Œä½†æ˜¯å®ƒä»¬ä¸å…¶ä»–å•è¯ä¸€èµ·å‡ºç°çš„é¢‘ç‡å¯èƒ½ä¼šè¯´æ˜å¾ˆå¤šã€‚ä¾‹å¦‚,â€œä¿¡æ¯â€å’Œâ€œæ²»ç†â€è¿™ä¸¤ä¸ªè¯å¯èƒ½ç»å¸¸ä¸€èµ·å‡ºç°ï¼Œå¹¶å…·æœ‰ç‰¹å®šçš„å«ä¹‰ã€‚æˆ‘ä»¬å¯ä»¥ç”¨ n å…ƒè¯­æ³•æ¥è§£é‡Šè¿™ä¸€ç‚¹ã€‚è¿™æ˜¯æŒ‡ä¸€èµ·å‡ºç°çš„å¤šä¸ªä»¤ç‰Œã€‚è¿™äº›æ ‡è®°å¯ä»¥æ˜¯å•è¯æˆ–å­—æ¯ã€‚è¿™é‡Œæˆ‘ä»¬å°†ä½¿ç”¨å•è¯ã€‚ä¸¤ä¸ªè¯(ğ‘›=2)è¢«ç§°ä¸ºåŒå­—ï¼Œä¸‰ä¸ªè¯è¢«ç§°ä¸ºä¸‰å­—ï¼Œç­‰ç­‰ã€‚ä½¿ç”¨ n å…ƒè¯­æ³•æœ‰åŠ©äºæˆ‘ä»¬ä¿ç•™æ–‡æœ¬ä¸­çš„ä¸€äº›å«ä¹‰/ä¸Šä¸‹æ–‡ã€‚

æˆ‘ä»¬å°†ä½¿ç”¨å‰é¢ç›¸åŒçš„çŸ­æ–‡æœ¬:

```
txt>>> 'the government last july called the energy sector debt situation a state of emergency. <p> this was during the mid-year budget review during which the finance minister ken ofori-atta castigated the previous ndc government for entering into obnoxious take-or-pay contracts signed by the ndc , which obligate us to pay for capacity we do not need .  <p> the government pays over gh ? 2.5 billion annually for some 2,300mw in installed capacity which the country does not consume .'
```

æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å¯ä»¥ä» NLTK å®ç”¨ç¨‹åºåŒ…ä¸­å¯¼å…¥ ngrams å‡½æ•°ã€‚

```
from nltk.util import ngrams
```

æˆ‘ä»¬å°†ä½¿ç”¨ä¸ä¹‹å‰ç›¸åŒçš„æ ‡è®°å™¨å†æ¬¡æ ‡è®°æ–‡æœ¬ã€‚

```
tk = TreebankWordTokenizer()
tk_words = tk.tokenize(txt)
```

æœ€åï¼Œæˆ‘ä»¬å°†è¿™ä¸ªæ ‡è®°åŒ–çš„åˆ—è¡¨ä¼ é€’åˆ° **ngrams** å‡½æ•°ä¸­ï¼Œå¹¶æŒ‡å®šğ‘›(åœ¨æœ¬ä¾‹ä¸­ä¸ºäºŒè¿›åˆ¶æ•°çš„ 2)(ä¸ºç®€æ´èµ·è§è€Œç¼©å†™)ã€‚

```
bigrams = list(ngrams(tk_words, 2))
bigrams>>> [('the', 'government'),
     ('government', 'last'),
     ('last', 'july'),
     ('july', 'called'),
     ('called', 'the'),
     ('the', 'energy'),
     ('energy', 'sector'),
     ('sector', 'debt'),
     ('debt', 'situation'),
     ('situation', 'a'),
     ('a', 'state'),
     ('state', 'of'),
     ('of', 'emergency.'),
     ('emergency.', '<'),
     ('<', 'p'),
     ('p', '>'),
     ('>', 'this'),
     ('this', 'was'),
     ('was', 'during'),
     ('during', 'the'),
     ('the', 'mid-year'),
     ('mid-year', 'budget')
...
```

åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°åƒâ€œåˆåŒâ€å’Œâ€œç­¾ç½²â€æˆ–â€œæ”¿åºœâ€å’Œâ€œæ”¯ä»˜â€è¿™æ ·çš„æœ¯è¯­ï¼Œå®ƒä»¬æä¾›äº†æ¯”å•ä¸ªå•è¯æ›´å¤šçš„ä¸Šä¸‹æ–‡ã€‚ä½œä¸ºé¢„å¤„ç†çš„ä¸€éƒ¨åˆ†ï¼Œæ‚¨ä¹Ÿå¯ä»¥ä»æ–‡æœ¬æ•°æ®ä¸­è¿‡æ»¤å‡º n å…ƒè¯­æ³•ã€‚

æˆ‘ä»¬è¿˜å¯ä»¥ä½¿ç”¨ **BigramCollocationFinder** ç±»æ¥ç¡®å®šäºŒå…ƒæ¨¡å‹å‡ºç°çš„æ¬¡æ•°ã€‚è¿™é‡Œæˆ‘ä»¬æŒ‰é™åºå¯¹åˆ—è¡¨è¿›è¡Œæ’åºã€‚

```
from nltk.collocations import BigramCollocationFinder
finder = BigramCollocationFinder.from_words(tk_words, window_size=2)
ngram = list(finder.ngram_fd.items())
ngram.sort(key=lambda item: item[-1], reverse=True)
ngram>>> [(('the', 'government'), 2),
     (('<', 'p'), 2),
     (('p', '>'), 2),
     (('which', 'the'), 2),
     (('government', 'last'), 1),
     (('last', 'july'), 1),
     (('july', 'called'), 1),
     (('called', 'the'), 1),
     (('the', 'energy'), 1),
     (('energy', 'sector'), 1),
     (('sector', 'debt'), 1),
     (('debt', 'situation'), 1),
     (('situation', 'a'), 1),
     (('a', 'state'), 1),
     (('state', 'of'), 1),
     (('of', 'emergency.'), 1),
     (('emergency.', '<'), 1),
     (('>', 'this'), 1),
     (('this', 'was'), 1),
     (('was', 'during'), 1),
     (('during', 'the'), 1),
     (('the', 'mid-year'), 1),
     (('mid-year', 'budget'), 1)
...
```

## æƒ…æ„Ÿåˆ†æ

è¿™åŒ…æ‹¬åˆ†ææ–‡æœ¬ä»¥ç¡®å®šæ–‡æœ¬çš„â€œæ­£é¢â€æˆ–â€œè´Ÿé¢â€ç¨‹åº¦ã€‚è¿™å¯ä»¥ç»™æˆ‘ä»¬å…³äºäººä»¬çš„è§‚ç‚¹/æƒ…ç»ªçš„ä¿¡æ¯ã€‚è¿™å¯ä»¥åº”ç”¨åˆ°ä¸€äº›äº‹æƒ…ä¸Šï¼Œæ¯”å¦‚æµè§ˆäº§å“è¯„è®ºï¼Œä»¥è·å¾—äº§å“æ˜¯å¦è¢«çœ‹å¥½çš„æ€»ä½“æ„Ÿè§‰ã€‚æˆ‘ä»¬è¿˜å¯ä»¥å°†å®ƒç”¨äºç ”ç©¶ç›®çš„â€”â€”ä¾‹å¦‚ï¼Œäººä»¬åœ¨ä»–ä»¬çš„æ¨æ–‡ä¸­å¯¹æˆ´å£ç½©æŒè‚¯å®šè¿˜æ˜¯å¦å®šæ€åº¦ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¯ä»¥è®­ç»ƒä¸€ä¸ªæ¨¡å‹æˆ–ä½¿ç”¨ç°æœ‰çš„è¯å…¸ã€‚VADER æ˜¯ç”¨ Python å®ç°çš„ã€‚è¿™ä¼šäº§ç”Ÿä¸€ä¸ªä»‹äº-1 å’Œ+1 ä¹‹é—´çš„æ­£é¢ã€è´Ÿé¢å’Œä¸­æ€§æƒ…ç»ªå¾—åˆ†ã€‚å®ƒè¿˜äº§ç”Ÿä¸€ä¸ªå¤åˆåˆ†æ•°ï¼Œå³**é˜³æ€§+ä¸­æ€§æ ‡å‡†åŒ–** (-1 æ¯” 1)ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬å°†ä¸‹è½½è¯å…¸ã€‚è¯å…¸åŒ…å«ä¸å•ä¸ªå•è¯æˆ–æ–‡æœ¬ä¸²ç›¸å…³çš„ä¿¡æ¯(ä¾‹å¦‚ï¼Œè¯­ä¹‰æˆ–è¯­æ³•)ã€‚

```
nltk.download("vader_lexicon", quiet=True)
```

æˆ‘ä»¬å¯¼å…¥äº†æƒ…æ„Ÿåˆ†æå™¨ç±»**SentimentIntensityAnalyzer**ï¼Œå¹¶åˆ›å»ºäº†ä¸€ä¸ªåä¸º **snt** çš„å®ä¾‹(ç”¨äºæƒ…æ„Ÿ)ã€‚

```
from nltk.sentiment.vader import SentimentIntensityAnalyzer
snt = SentimentIntensityAnalyzer()
```

ç„¶åï¼Œæˆ‘ä»¬å¯ä»¥å‘å‡½æ•° **polarity_scores** ä¼ é€’ä¸€äº›æ–‡æœ¬æ•°æ®(ä¾‹å¦‚ï¼Œæˆ‘ä»¬çš„ç¬¬äºŒä¸ªæ–‡ä»¶)ã€‚æ‚¨å¯ä»¥åœ¨ä¸‹é¢çš„å­—å…¸æ•°æ®ç»“æ„ä¸­çœ‹åˆ°ä½œä¸ºé”®/å€¼å¯¹è¿”å›çš„åˆ†æ•°ã€‚

```
snt.polarity_scores(data_files[1])>>> {'neg': 0.101, 'neu': 0.782, 'pos': 0.117, 'compound': 1.0}
```

åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼ŒæŸ¥çœ‹ç¬¬äºŒä¸ªæ–‡æœ¬æ–‡ä»¶ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°ä¸€ä¸ªå ä¸»å¯¼åœ°ä½çš„ä¸­æ€§æƒ…ç»ªå¾—åˆ†(0.782)ï¼Œåé¢æ˜¯ä¸€ä¸ªæ­£å¾—åˆ†(0.117)å’Œä¸€ä¸ªè´Ÿå¾—åˆ†(0.101)ã€‚ä½ å¯ä»¥åœ¨ä¸åŒçš„æ—¶é—´ç‚¹æ¯”è¾ƒæƒ…ç»ªï¼Œçœ‹çœ‹å®ƒæ˜¯å¦‚ä½•å˜åŒ–çš„ï¼Œæˆ–è€…åœ¨ä¸åŒçš„å°ç»„ä¹‹é—´ã€‚æœ‰æ—¶ä½¿ç”¨çš„å¦ä¸€ä¸ªæŒ‡æ ‡æ˜¯å‡€æƒ…ç»ªå¾—åˆ†(NSS)ï¼Œå®ƒæ˜¯é€šè¿‡ä»ç§¯æå¾—åˆ†ä¸­å‡å»æ¶ˆæå¾—åˆ†æ¥è®¡ç®—çš„ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬éœ€è¦å°†åˆ†æ•°å­˜å‚¨åœ¨ä¸€ä¸ªå˜é‡ä¸­æ¥è®¿é—®å®ƒä»¬ã€‚

```
sent_scores = snt.polarity_scores(data_files[1])
```

ç„¶åæˆ‘ä»¬å¯ä»¥ä»æ­£æ•°ä¸­å‡å»è´Ÿæ•°ã€‚

```
nss = sent_scores['pos'] â€” sent_scores['neg']
print("NSS =", nss)>>> NSS = 0.016
```

## ä¸»é¢˜å»ºæ¨¡

ä¸»é¢˜å»ºæ¨¡é€šå¸¸ç”¨äºæ–‡æœ¬æŒ–æ˜ï¼Œå®ƒå…è®¸æˆ‘ä»¬åˆ›å»ºä¸€ä¸ªç»Ÿè®¡æ¨¡å‹æ¥å‘ç°æ–‡æœ¬æ•°æ®ä¸­çš„ä¸»é¢˜ã€‚æœ‰å„ç§ä¸åŒçš„æ–¹æ³•/ç®—æ³•å¯ä»¥åšåˆ°è¿™ä¸€ç‚¹ã€‚æˆ‘ä»¬å°†åœ¨è¿™é‡Œçœ‹ä¸€å¯¹å¤«å¦‡ã€‚è¿™æ˜¯ä¸€ç§æ— äººç›‘ç®¡çš„æ–¹æ³•ã€‚æˆ‘ä»¬é¦–å…ˆæ¥çœ‹çœ‹æ½œåœ¨è¯­ä¹‰åˆ†æ(LSA)ï¼Œå®ƒä¸ä¸»æˆåˆ†åˆ†æ(PCA)çš„å·¥ä½œåŸç†ç›¸åŒã€‚LSA æ˜¯ä¸€ä¸ªçº¿æ€§æ¨¡å‹ï¼Œå¹¶å‡è®¾æ–‡æ¡£ä¸­çš„æœ¯è¯­å‘ˆæ­£æ€åˆ†å¸ƒã€‚å®ƒè¿˜ä½¿ç”¨ SVD(å¥‡å¼‚å€¼åˆ†è§£),è¿™åœ¨è®¡ç®—ä¸Šæ˜¯æ˜‚è´µçš„ã€‚è¿™ç§æ–¹æ³•å‡å°‘äº†æ•°æ®ä¸­çš„å™ªå£°ã€‚

**æ³¨æ„:** SVD çš„å·¥ä½œåŸç†æ˜¯å°†ä¸€ä¸ªæ–‡æ¡£æœ¯è¯­çŸ©é˜µåˆ†æˆ 3 ä¸ªè¿ç»­çš„æ–¹é˜µ(å…¶ä¸­ä¸€ä¸ªæ˜¯å¯¹è§’çŸ©é˜µ)ï¼Œç„¶åå°†å®ƒä»¬è½¬ç½®å¹¶å†æ¬¡ç›¸ä¹˜ã€‚è¿™ä¸ªæ–¹æ³•å¯ä»¥ç”¨æ¥æ±‚çŸ©é˜µçš„é€†çŸ©é˜µã€‚

ç¬¬ä¸€é˜¶æ®µæ¶‰åŠåˆ›å»ºæ–‡æ¡£æœ¯è¯­çŸ©é˜µã€‚è¿™åœ¨è¡Œä¸­è¡¨ç¤ºæ–‡æ¡£ï¼Œåœ¨åˆ—ä¸­è¡¨ç¤ºæœ¯è¯­ï¼Œåœ¨å•å…ƒæ ¼ä¸­è¡¨ç¤º TF-IDF åˆ†æ•°ã€‚ç„¶åï¼Œæˆ‘ä»¬å°† SVD åº”ç”¨äºè¿™ä¸ªçŸ©é˜µï¼Œä»¥è·å¾—æœ€ç»ˆçš„ä¸»é¢˜åˆ—è¡¨ã€‚

![](img/f97d5da8379f06f2550d93ac3cfd8921.png)

æ–‡æ¡£-æœ¯è¯­çŸ©é˜µï¼Œè¡Œä¸­æœ‰æ–‡æ¡£ï¼Œåˆ—ä¸­æœ‰æœ¯è¯­ã€‚å•å…ƒæ ¼åŒ…å« TF-IDF åˆ†æ•°(å›¾ç‰‡ç”±ä½œè€…æä¾›)

æˆ‘ä»¬å¯ä»¥ä½¿ç”¨ä¹‹å‰ä½¿ç”¨çš„ç›¸åŒçš„**tfidf çŸ¢é‡å™¨**æ¥è®¡ç®— TF-IDF åˆ†æ•°ã€‚æˆ‘ä»¬å°†é™åˆ¶æœ¯è¯­(ç‰¹å¾)çš„æ•°é‡ï¼Œä»¥å°†æ‰€éœ€çš„è®¡ç®—èµ„æºå‡å°‘åˆ° 800ã€‚

```
from sklearn.feature_extraction.text import TfidfVectorizer
v = TfidfVectorizer(stop_words='english', max_features=800, max_df=0.5)
X = v.fit_transform(data_files)
```

å¦‚æœæˆ‘ä»¬æŸ¥çœ‹æ–‡æ¡£-æœ¯è¯­çŸ©é˜µçš„ç»´åº¦ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°è¡Œä¸­æœ‰ 4 ä¸ªæ–‡æ¡£ï¼Œåˆ—ä¸­æœ‰ 800 ä¸ªæœ¯è¯­ã€‚

```
X.shape>>> (4, 800)
```

æˆ‘ä»¬ç°åœ¨éœ€è¦å®ç° SVDï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨**æˆªæ–­çš„ SVD** ç±»æ¥å®ç°å®ƒã€‚è¿™å°†ä¸ºæˆ‘ä»¬åšç¹é‡çš„å·¥ä½œã€‚

```
from sklearn.decomposition import TruncatedSVD
```

æˆ‘ä»¬å¯ä»¥ç”¨å‚æ•° **n_components** æŒ‡å®šä¸»é¢˜çš„æ•°é‡ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬å°†å®ƒè®¾ç½®ä¸º 4ï¼Œå‡è®¾æ¯ä¸ªæ–‡æ¡£æœ‰ä¸€ä¸ªä¸åŒçš„ä¸»ä¸»é¢˜(**æ³¨æ„:**æˆ‘ä»¬è¿˜å¯ä»¥ä½¿ç”¨åƒä¸»é¢˜ä¸€è‡´æ€§è¿™æ ·çš„æ–¹æ³•æ¥ç¡®å®šä¸»é¢˜çš„æœ€ä½³æ•°é‡ *k* )ã€‚

```
svd = TruncatedSVD(n_components=4)
```

æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬æ‹Ÿåˆæ¨¡å‹å¹¶è·å¾—ç‰¹å¾åç§°:

```
svd.fit(X)doc_terms = v.get_feature_names()
```

ç°åœ¨æˆ‘ä»¬å¯ä»¥è¾“å‡ºä¸æ¯ä¸ªä¸»é¢˜ç›¸å…³çš„æœ¯è¯­ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œ4 ä¸ªä¸»é¢˜ä¸­çš„æ¯ä¸€ä¸ªéƒ½æœ‰å‰ 12 ä¸ªã€‚

```
for i, component in enumerate(svd.components_):
    terms_comp = zip(doc_terms, component)
    sorted_terms = sorted(terms_comp, key=lambda x:x[1], reverse=True)[:12]
    print("")
    print("Topic "+str(i+1)+": ", end="")
    for term in sorted_terms:
        print(term[0], " ", end="")>>> Topic 1: rsquo href ldquo rdquo ventilators easter keytruda quebec unincorporated ford books inmates 
Topic 2: davos wef sibley denly stamler comox nortje caf pd rsquo href ldquo 
Topic 3: hopland geely easyjet davos vanderbilt wef asbestos macy jamaat sibley denly stamler 
Topic 4: rsquo href ldquo rdquo quebec div eacute src noopener rel mdash rsv
```

ç„¶åï¼Œç”±æ‚¨æ ¹æ®è¿™äº›ä¸»é¢˜æ‰€åŒ…å«çš„å•è¯æ¥ç¡®å®šå®ƒä»¬å¯èƒ½ä»£è¡¨ä»€ä¹ˆ(ä¸»é¢˜æ ‡ç­¾)ã€‚

ä¸»é¢˜å»ºæ¨¡çš„å¦ä¸€ä¸ªæµè¡Œé€‰é¡¹æ˜¯æ½œåœ¨ç‹„åˆ©å…‹é›·åˆ†é…(LDA ),ä¸è¦ä¸å…¶ä»– LDA(çº¿æ€§åˆ¤åˆ«åˆ†æ)æ··æ·†ã€‚LDA å‡è®¾å•è¯çš„ç‹„åˆ©å…‹é›·åˆ†å¸ƒï¼Œå¹¶åˆ›å»ºè¯­ä¹‰å‘é‡ç©ºé—´æ¨¡å‹ã€‚å…¶å®ç°çš„å…·ä½“ç»†èŠ‚è¶…å‡ºäº†æœ¬å…¥é—¨ä¹¦çš„èŒƒå›´ï¼Œä½†æœ¬è´¨ä¸Šï¼Œå®ƒå°†æ–‡æ¡£æœ¯è¯­çŸ©é˜µè½¬æ¢ä¸ºä¸¤ä¸ªçŸ©é˜µã€‚ä¸€ä¸ªè¡¨ç¤ºæ–‡æ¡£å’Œä¸»é¢˜ï¼Œç¬¬äºŒä¸ªè¡¨ç¤ºä¸»é¢˜å’Œæœ¯è¯­ã€‚ç„¶åï¼Œè¯¥ç®—æ³•å°è¯•æ ¹æ®ä¸»é¢˜ç”Ÿæˆæ‰€è®¨è®ºçš„å•è¯çš„æ¦‚ç‡è®¡ç®—æ¥è°ƒæ•´æ¯ä¸ªæ–‡æ¡£ä¸­æ¯ä¸ªå•è¯çš„ä¸»é¢˜ã€‚

æˆ‘ä»¬ä½¿ç”¨ä¸å‰é¢ LSA ç›¸åŒçš„æ¦‚å¿µã€‚é¦–å…ˆï¼Œæˆ‘ä»¬ç”¨**è®¡æ•°çŸ¢é‡å™¨**å°†æ•°æ®æ ‡è®°åŒ–ã€‚ç„¶åï¼Œæˆ‘ä»¬å†æ¬¡åˆ›å»º LDA ç±»çš„å®ä¾‹ï¼Œå°†ä¸»é¢˜æ•°é‡è®¾ç½®ä¸º 4ï¼Œå¹¶è¾“å‡ºå‰ 12 ä¸ªã€‚

```
from sklearn.feature_extraction.text import CountVectorizer
cv = CountVectorizer()
fitted = cv.fit_transform(data_files)
from sklearn.decomposition import LatentDirichletAllocation
lda = LatentDirichletAllocation(n_components=4, random_state=42)
lda.fit(fitted)
```

ç„¶åï¼Œæˆ‘ä»¬å¯ä»¥åƒä»¥å‰ä¸€æ ·è¾“å‡ºç»“æœ:

```
doc_terms = cv.get_feature_names()for i, component in enumerate(lda.components_):
   terms_comp = zip(doc_terms, component)
   sorted_terms = sorted(terms_comp, key=lambda x:x[1], reverse=True)[:12]
   print("")
   print("Topic "+str(i+1)+": ", end="")
   for term in sorted_terms:
       print(term[0], " ", end="")>>> Topic 1: said  19  covid  people  coronavirus  new  health  also  would  one  pandemic  time  
Topic 2: 021  040  25000  421  4q  712  85th  885  accrues  accuser  acuity  afterthought  
Topic 3: said  coronavirus  people  health  19  new  covid  also  cases  virus  one  would  
Topic 4: 021  040  25000  421  4q  712  85th  885  accrues  accuser  acuity  afterthought
```

å¦‚æœæˆ‘ä»¬è®¤ä¸ºè¿™äº›æ•°å­—ä¸ç›¸å…³ï¼Œæˆ‘ä»¬å¯èƒ½è¿˜æƒ³å›å»è¿‡æ»¤æ‰å®ƒä»¬ã€‚æ‚¨å¯ä»¥çœ‹åˆ°ï¼Œè¿™ä¸æˆ‘ä»¬ä¹‹å‰çœ‹åˆ°çš„ LSA äº§ç”Ÿäº†éå¸¸ä¸åŒçš„ç»“æœã€‚é¦–å…ˆï¼ŒLSA æ˜¯ä¸»é¢˜å»ºæ¨¡çš„è‰¯å¥½å¼€ç«¯ã€‚å¦‚æœéœ€è¦ï¼ŒLDA æä¾›ä¸åŒçš„é€‰é¡¹ã€‚

## æ‘˜è¦

è¿™æœ¬åˆçº§è¯»æœ¬æ¦‚è¿°äº†ç°ä»£ NLP ä»»åŠ¡ä¸­ä½¿ç”¨çš„ä¸€äº›å¸¸ç”¨æ–¹æ³•ï¼Œä»¥åŠå¦‚ä½•ç”¨ Python å®ç°è¿™äº›æ–¹æ³•ã€‚æ¯ç§æ–¹æ³•éƒ½æœ‰ç»†å¾®å·®åˆ«ï¼Œéœ€è¦æ ¹æ®æ‰‹å¤´çš„ä»»åŠ¡æ¥è€ƒè™‘ã€‚è¿˜æœ‰åƒè¯æ€§æ ‡æ³¨è¿™æ ·çš„æ–¹æ³•ï¼Œä¹Ÿç§°ä¸ºè¯­æ³•æ ‡æ³¨ï¼Œå¯ä»¥ä¸ºç®—æ³•æä¾›é¢å¤–çš„ä¿¡æ¯ã€‚ä¾‹å¦‚ï¼Œä½ å¯ä»¥åœ¨æŸäº›å•è¯ä¸Šæ ‡æ³¨è¯¥å•è¯æ˜¯åè¯ã€åŠ¨è¯ã€å½¢å®¹è¯è¿˜æ˜¯å‰¯è¯ç­‰ä¿¡æ¯ã€‚è¿™é€šå¸¸è¢«è¡¨ç¤ºä¸ºå…·æœ‰**å•è¯ã€æ ‡ç­¾**çš„å…ƒç»„åˆ—è¡¨ï¼Œä¾‹å¦‚[('build 'ï¼Œ' v ')ï¼Œ(' walk 'ï¼Œ' v ')ï¼Œ(' mountain 'ï¼Œ' n')]ã€‚è¿˜æœ‰è®¸å¤šæ–¹æ³•å¯ä»¥è¡¨ç¤ºå•è¯å’Œæœ¯è¯­ï¼Œç”¨äºåç»­å¤„ç†ï¼Œä¾‹å¦‚ word2vec å’Œå•è¯åŒ…ç­‰ã€‚æ‚¨é€‰æ‹©çš„æ ¼å¼å°†å†æ¬¡å–å†³äºä»»åŠ¡å’Œç®—æ³•è¦æ±‚ã€‚ä¸æ‰€æœ‰æœºå™¨å­¦ä¹ å’Œæ•°æ®ç§‘å­¦ä¸€æ ·ï¼Œæ•°æ®é¢„å¤„ç†æ‰€èŠ±è´¹çš„æ—¶é—´é€šå¸¸æœ€é•¿ï¼Œå¹¶ä¸”å¯¹ç”Ÿæˆçš„è¾“å‡ºæœ‰å¾ˆå¤§å½±å“ã€‚è¯»å®Œè¿™æœ¬åˆçº§è¯»æœ¬åï¼Œä½ åº”è¯¥æœ‰å¸Œæœ›å¼€å§‹ä»æ–‡æœ¬æ•°æ®ä¸­è·å¾—ä¸€äº›æœ‰è¶£çš„è§è§£ï¼Œå¹¶å¯¹ä½ å¯èƒ½ç”¨æ¥åˆ†æè¿™ç±»æ•°æ®çš„ä¸€äº›æ–¹æ³•æœ‰æ‰€äº†è§£ã€‚

## å‚è€ƒ

[1]æˆ´ç»´æ–¯ï¼Œé©¬å…‹ã€‚(2019-)å† çŠ¶ç—…æ¯’è¯­æ–™åº“ã€‚å¯åœ¨ https://www.english-corpora.org/corona/çš„[åœ¨çº¿è´­ä¹°ã€‚](https://www.english-corpora.org/corona/)