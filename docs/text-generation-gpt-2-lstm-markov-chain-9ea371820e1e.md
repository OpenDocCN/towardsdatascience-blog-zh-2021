# 使用 GPT-2、LSTM 和马尔可夫链的实用文本生成

> 原文：<https://towardsdatascience.com/text-generation-gpt-2-lstm-markov-chain-9ea371820e1e?source=collection_archive---------10----------------------->

## [入门](https://towardsdatascience.com/tagged/getting-started)

## 单词级 NLG 模型概述

![](img/e4b60f8034d264295d3a59bb0e959ad5.png)

由[格伦·卡斯滕斯-彼得斯](https://unsplash.com/@glenncarstenspeters?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)在 [Unsplash](https://unsplash.com/s/photos/writing-computer?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText) 上拍摄的照片

自然语言生成(NLG)或文本生成是自然语言处理(NLP)的一个子领域。它的目标是以人类书写文本的形式生成有意义的短语和句子。它有广泛的使用案例:写长篇内容(如报告，文章)，产品描述，社交媒体帖子，聊天机器人等。

这个项目的目标是实现和测试文本生成的各种方法:从简单的马尔可夫链开始，通过神经网络(LSTM)，到变形金刚架构(GPT-2)。所有这些模型都将用于生成童话文本。

**目录:**

1.  [童话数据集](https://medium.com/p/9ea371820e1e#ed40)
2.  [用马尔可夫链生成文本](https://medium.com/p/9ea371820e1e#4316)
3.  [用 LSTM 文本生成](https://medium.com/p/9ea371820e1e#9605)
4.  [用 GPT-2 生成文本](https://medium.com/p/9ea371820e1e#3c34)

# 童话数据集

该数据集是基于网上可获得的内容创建的——它从两个来源收集:从[民间传说和神话
电子文本](https://www.pitt.edu/~dash/folktexts.html)网站搜集的和从 [Kaggle](https://www.kaggle.com/cuddlefish/fairy-tales) 下载的。收集内容的总大小为 20MB，包括 3150 个文本文件，超过 370 万字。

为了加快计算速度，训练和测试数据集仅从 800 个随机选择的文件中创建。训练集包含 766，970 个单词(50，487 个唯一的)，测试集包含 202，860 个单词(21，630 个唯一的)。

# 基于马尔可夫链的文本生成

马尔可夫链是最早用于文本生成的算法之一(例如，在旧版本的智能手机键盘中)。这是一个随机模型，意味着它是基于随机概率分布的。马尔可夫链仅仅基于前一状态(前一个单词或序列)对未来状态(在文本生成的情况下，下一个单词)建模。该模型是无记忆的——预测仅取决于变量的当前状态(它忘记了过去的状态；它独立于前面的状态)。另一方面，它简单，执行速度快，占用内存少。

使用马尔可夫链模型进行文本生成需要以下步骤:

1.  加载数据集并预处理文本。
2.  从文本中提取长度为 n 的序列(当前状态)和接下来的单词(未来状态)。
3.  用状态转移的概率值构建转移矩阵。
4.  基于状态转换的概率分布预测下一个单词。

有一个笔记本，里面有[马尔可夫链实现](https://github.com/klaudia-nazarko/nlg-text-generation/blob/main/markov_chain.ipynb)的所有细节。

让我们从文本预处理开始——它不同于用于其他 NLP 任务的预处理，例如文本分类。因为模型需要学习如何基于输入创建文本，所以我们不能删除停用词或应用词干或语法化。本项目的文本预处理包括:

*   移除换行符——只是为了使模型更简单(因为文本包含许多“不正确的”换行符；在适当的解决方案中，它们应该被保存以教导模型正确地格式化文本)
*   在标点符号前后添加空格，以便将标点符号识别为单独的符号。
*   删除双空格字符。
*   将文本标记化(空格标记化)。
*   将标记映射到索引(并创建 token2ind 和 ind2token 字典)。

预处理后的示例文本:

> 从前有一个苏丹，他非常喜欢他的花园，在里面种植了来自世界各地的树、花和水果。他每天去看他们三次:第一次是在七点钟…

代币(前 15 个代币):

```
[‘Once’, ‘upon’, ‘a’, ‘time’, ‘there’, ‘lived’, ‘a’, ‘sultan’, ‘who’, ‘loved’, ‘his’, ‘garden’, ‘dearly’, ‘,’, ‘and’]
```

令牌索引(前 15 个令牌):

```
[18409, 21372, 23318, 3738, 23298, 15316, 23318, 9226, 20595, 20453, 6655, 21507, 16532, 5126, 3450]
```

正如我们所看到的，为了在输出中保持原始的文本格式，文本没有被转换成小写。在这种情况下，单词“once”和“Once”将被视为不同的标记。将文本转换成小写可以解决这个问题，但是需要对输出文本进行额外的格式化。

结果，文本被分成 890，750 个记号(25，165 个唯一记号)。

建立马尔可夫链模型的第一步是从文本中提取长度为 n 的序列和接下来的单词。在示例中，我们使用 n=3，因此从上面的摘录中，我们可以提取这样的序列—下一个单词对:

*   ["曾经"，"曾经"，"一个"]-->["时间"]
*   [“在”、“一”、“时间”]-->[“那里”]
*   等等…

首先，让我们构建长度为 n 的序列:

用于构建长度为 n 的序列的代码

它返回 890，748 ngram(555，205 唯一)。

为了构建一个转换矩阵，我们需要遍历整个文本，并计算从特定序列(ngram)到下一个单词的所有转换。我们将这些值存储在一个矩阵中，其中行对应于特定的序列，列对应于特定的标记(下一个单词)。这些值表示特定序列之后每个标记出现的次数。因为转移矩阵应该包含概率，而不是计数，所以最后出现的次数被重新计算成概率。矩阵以 scipy.sparse 格式保存，以限制它在内存中占用的空间。

构建转移矩阵(马尔可夫链)的代码

这是转换矩阵的一部分，显示了序列“和苏丹”的行以及从 12375 到 12385 的标记索引。标记“replied”对应于索引 12380，因此我们可以看到矩阵中的这个位置包含值~0.17。它显示有 0.17 的机会序列“和苏丹”将跟随单词“答复”。

```
matrix([[0\. , 0\. , 0\. , 0\. , 0\. , 0.16666667, 0\. , 0\. , 0\. , 0\. ]])
```

一旦我们有了转换矩阵，我们就可以进行文本生成。为了生成一个单词，我们需要提供长度为 n 的前缀，模型会在转移矩阵中查找这个 ngram，并返回一个随机令牌(根据这个序列对应的概率分布)。

使用马尔可夫链进行下一个单词预测的编码

还有一个引入的温度参数，用于控制采样过程中的随机量，它决定了下一个单词选择的可预测性。

softmax 温度代码

现在，我们已经做好了开始文本生成的一切准备——我们可以通过一个循环生成任意长度的文本，该循环从预测提供的前缀的下一个单词开始，将其附加到输入序列中，然后继续返回下一个单词。

使用马尔可夫链的文本生成代码

马尔可夫链模型为不同的温度水平返回这样的文本。只有温度= 1 的模型生成的文本看起来相当不错。它保持了局部的连贯性，但是它在整体上没有意义。

> 时间过去了，他有了一个女儿，这件事将会广为人知，因为天气很好，而且非常安静，她立刻承认她要出去，杀死所有上上下下的人，没有怜悯
> 
> 温度:0.7
> 从前一个包装拉下区分残疾 Cumhaill 件]第四个 Kilachdiarmid 增长车厢更严厉失望吵闹 Rajas“珍惜无”Combland 刺痛悸动“这是 gol tsch 直到 B 神圣的 o'face 加强自由主义公共汽车睿智 lassie 东西恶棍指示雇用边界红衣主教因此国家[圈吃丢脸的卷心菜更聪明的 Lipenshaw 件”抓人将军蹂躏果园
> 
> 温度:0.4
> 时间流逝，尼利·芬维尔·卡奥伊特勾勒出风的轮廓四月，西莉亚诉说着令人厌恶的事或森林扔着自杀的铲子走了拍手声逃跑的家伙滑稽的斗篷虚弱的最温暖的卡丽菲亚自然的可以结婚受惩罚的约束|结束她将于 1795 年引诱人们盟誓给予奉献乞求陈述歌利亚洞打哈欠的锥子被杀死的版本萨拉哈斯基的婚姻信任的后果
> 
> 温度:0.1
> 从前忠诚的爱步履蹒跚的名字撕咬消失的格伦一块如何罗拉挑选向外伯爵的诱惑 ex drop 的成群的床罩慈善床罩彭曼 Bridgend]匹配百倍的宝贝 Ballycarney et 熟练的硬币绞拧 coorses eked 数量填充惊呼卡尔 1795 颂歌人类梦想的努力 coshering 屠夫 Myrdal 我们很高兴安排窗饰居住隐蔽

# 使用 LSTM 生成文本

LSTM(长短期记忆)神经网络，由于其学习长期依赖性的能力，被成功地用于分类、翻译和文本生成。它们对序列进行归纳，而不是学习单个模式，这使它们成为建模序列数据的合适工具。为了生成文本，他们学习如何根据输入序列预测下一个单词。

文本生成与 LSTM 一步一步:

1.  加载数据集并预处理文本。
2.  提取长度为 n (X，输入向量)的序列和接下来的单词(y，标签)。
3.  构建返回批量数据的 DataGenerator。
4.  定义 LSTM 模型并训练它。
5.  根据顺序预测下一个单词。

[LSTM 模式](https://github.com/klaudia-nazarko/nlg-text-generation/blob/main/lstm.ipynb)的实现可以在这个笔记本里找到。

关于数据预处理，直到文本标记化，我们使用与马尔可夫链模型中相同的方法(见上文)。

由于文本生成是一个监督学习问题，我们将 n 个单词的序列作为输入向量(X ),将接下来的单词作为标签(y)。让我们生成长度= 4 且步长= 3 的序列，这意味着第一个 4 个字的序列从第一个(0 索引)字开始，第二个序列从 3 个字之后开始，因此从第 4 个字(3 索引)开始。

用于生成长度为 n 的序列的代码

它产生 296，916 个长度为 4 的序列。示例:

```
[‘Once’, ‘upon’, ‘a’, ‘time’, ‘there’, ‘lived’, ‘a’, ‘sultan’, ‘who’, ‘loved’]
[10701, 17952, 19552, 289, 10967, 9397, 19552, 21301, 6393, 1702]array([[10701, 17952, 19552, 289],
      [ 289, 10967, 9397, 19552]])
```

TextDataGenerator 对象包含一些有用的特性:它可以在每个时期的开始打乱观察，它将数据转换成正确的格式并返回成批的数据。

这个项目包括两个 LSTM 模型:有和没有嵌入层。第一个 LSTM 模型——没有嵌入层——将单词序列作为输入，其中每个单词由一个热点向量表示。TextDataGenerator 执行这种转换:

LSTM 模型数据生成代码

第二个模型(具有嵌入层)将单词索引序列作为输入，并使用神经网络的第一层来开发单词嵌入。

然后，我们定义模型并训练它。

LSTM 模型建立和训练规范

为了进行预测，模型将前缀作为输入，它需要以与训练数据相同的格式表示。该模型输出大小等于包含分配给每个单词的概率的词汇表大小的向量。为了生成文本，我们提供前缀，并根据预测的概率分布随机选择下一个单词。为了生成更长的文本，就像马尔可夫链模型一样，我们需要实现一个循环。

使用 LSTM 模型生成文本的代码

第一个 LSTM 模型生成的文本(无嵌入层):

> 从前有一个男人和高贵的公主！被如此好脾气，那天早上，当他醒来时，他发现它在他的手里，它的宫殿。年轻的野生小屋住在两天的桥上…
> 
> 从前有很多老鼠，但是可以跑过去看看他有没有那么乖，因为你会看到有人在那里。所有伟大的野生 Huldre 但是我大错特错了…
> 
> 温度:0.4
> 从前有一个父亲，他不得不被允许呆一夜，夜是最遥远的地方——她几乎无法判断国王是否来了，他是否生气了，当王子从屋顶上走下来时…

第二个模型生成的文本(带有嵌入层):

> 从前有一个威严年轻的球我的好姐姐，这是好微笑。我认识一个；如果我在树下一分钟看到回表。所以他很快我有一个灌木丛…
> 
> 温度:0.7
> 从前有一大群人，一顿丰盛的食物，还有一个人在外面，他回到她的小屋，走到他的嘴边说:小渣滓，我想去见国王，我不能拒绝..
> 
> 从前，有一块巨大的绿色无花果树，他们来到他的花园里寻找。小穆克问皮诺奇，他诅咒了唯一一个吻过她声音的人，把她压在身下…

类似于用马尔可夫链模型获得的结果，生成的文本呈现局部连贯性，但是缺乏逻辑。乍一看，这似乎是一篇正确的文章，但一旦我们开始阅读，我们就会发现它没有任何意义。

# 用 GPT-2 生成文本

开放人工智能 GPT-2 是一个基于转换器的自回归语言模型，它在多种语言任务上表现出竞争性能，尤其是(长格式)文本生成。GPT-2 使用预测下一个单词的简单任务，在 40GB 的高质量内容上进行训练。这个模型利用注意力来做到这一点。它允许模型关注与预测下一个单词相关的单词。

拥抱脸变形金刚库提供了你训练/微调/使用变形金刚模型所需的一切。以下是如何微调预训练的 GPT-2 模型:

1.  加载标记器和数据整理器
2.  加载数据并创建数据集对象
3.  加载模型
4.  加载并设置培训师和培训参数
5.  微调模型
6.  使用管道生成文本

为了一步一步地跟随 [GPT-2 的实施](https://github.com/klaudia-nazarko/nlg-text-generation/blob/main/gpt_2.ipynb)，打开这个笔记本。

每个预训练的变压器模型都有其相应的记号赋予器，应该使用这些记号赋予器来保持将单词转换成记号的相同方式(如在预训练期间)。它将文本分割成记号(单词或子单词，标点符号等)。)然后把它们转换成数字(id)。GPT-2 使用字节对编码(BPE ),将空间标记化作为预标记化。它的词汇量为 50，257，最大序列长度等于 1024。

```
“Once upon a time in a little village”
{‘input_ids’: [7454, 2402, 257, 640, 287, 257, 1310, 15425],
‘attention_mask’: [1, 1, 1, 1, 1, 1, 1, 1]}
```

数据整理器是用于从训练和测试数据集形成批次的函数。如果输入的长度不同，DataCollatorForLanguageModelling 会动态地将输入填充到批处理的最大长度。

为了在模型中使用文本数据，我们应该将其作为 Dataset 对象加载(从 PyTorch)。我们使用 TextDataset 的拥抱脸实现。它将文本分割成一定长度的连续块，例如，它将每隔 1024 个记号切割文本。

GPT2LMHeadModel 是致力于语言建模任务的 GPT-2 模型。我们加载一个预训练的模型来微调童话文本。为了训练模型，我们使用训练器(用于功能完整训练的接口)和训练参数(与训练循环相关的参数子集)。

为了生成文本，我们应该使用 Pipeline 对象，它提供了一种使用模型进行推理的非常简单的方法。或者，它采用一个 config 参数，该参数定义 PretrainedConfig 中包含的参数。当我们想要使用不同的解码方法时，例如波束搜索、top-k 或 top-p 采样，这一点尤为重要。

使用**默认配置**生成的文本:

> 从前上帝常说，“哦，百里挑一。当他听说这对他来说是一个多么大的任务时，他非常高兴，并开始想…

用**光束搜索生成的文本**:

> 从前，他认为有一些东西是如此悲伤、孤独和阴郁，难以想象。晚上，他去农舍看他的小妹妹，但从未见过她…

使用 **top-k 采样**生成的文本:

> 很久很久以前，当老人在很远的地方时，仙女向他借牛奶和酒。在她的要求下，所有的仙女都回答说，如果这个男人要把她们都借走，她会…

使用 **top-p 采样**生成的文本:

> 很久以前，这件事发生在阿里巴巴身上，他坐在椅子上，一手拿着装满珍珠的第一碗，另一手拿着他身后的妹妹。阿里巴巴带着……

GPT-2 模型生成的文本看起来令人印象深刻。首先，虽然有些句子听起来可能有点别扭，但语法上是正确的，而且相当符合逻辑。更重要的是，我们要注意它的一致性:eg 句子的主语始终是“他”(波束搜索例子中)或阿里巴巴(top-p 采样)，模型知道一个仙女有仙女(top-k 采样)。另一个有趣的部分是:“*发生了如此悲伤、孤独和阴郁的事情”* —模型知道它应该列出形容词，如悲伤、孤独和阴郁。另外，模特知道应该称呼妹妹为“她”:*“看着他的小妹妹，从来没见过她”*。生成的文本的所有这些特征使它看起来非常逼真，就像是由人写的一样。

对自然语言生成方法的快速概述表明，模型越来越有能力模仿人类的书写。即使像马尔可夫链这样简单快捷的方法也能产生一些有趣的输出。与此同时，像变形金刚这样的高级模型表现出了令人印象深刻的性能。

要查看带有详细解释的完整代码，请查看该项目的资源库:

[](https://github.com/klaudia-nazarko/nlg-text-generation) [## klaudia-nazar ko/NLG-文本生成

### 自然语言生成(NLG)或文本生成是自然语言处理(NLP)的一个子领域。它的目标是…

github.com](https://github.com/klaudia-nazarko/nlg-text-generation) 

# 参考

1.  [https://www . kdnugges . com/2019/11/Markov-chains-train-text-generation . html](https://www.kdnuggets.com/2019/11/markov-chains-train-text-generation.html)
2.  [https://www . upgrad . com/blog/Markov-chain-python-tutorial/](https://www.upgrad.com/blog/markov-chain-in-python-tutorial/)
3.  [https://keras . io/examples/创成式/lstm _ character _ level _ text _ generation/](https://keras.io/examples/generative/lstm_character_level_text_generation/)
4.  [https://colab . research . Google . com/github/tensor flow/TPU/blob/master/tools/colab/Shakespeare _ with _ TPU _ and _ keras . ipynb](https://colab.research.google.com/github/tensorflow/tpu/blob/master/tools/colab/shakespeare_with_tpu_and_keras.ipynb)
5.  [https://huggingface.co/transformers/](https://huggingface.co/transformers/)
6.  [https://towards data science . com/fine-tune-a-non-English-GPT-2-model-with-hugging face-9 ACC 2d c 7635 b](/fine-tune-a-non-english-gpt-2-model-with-huggingface-9acc2dc7635b)
7.  [https://towards data science . com/generate-fresh-movie-stories-for-favorite-genre-with-deep-learning-143 da 14b 29d 6](/generate-fresh-movie-stories-for-your-favorite-genre-with-deep-learning-143da14b29d6)
8.  [https://www.youtube.com/watch?v=rBCqOTEfxvg](https://www.youtube.com/watch?v=rBCqOTEfxvg)