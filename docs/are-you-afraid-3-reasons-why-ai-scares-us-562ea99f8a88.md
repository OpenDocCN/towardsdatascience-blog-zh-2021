# 你害怕吗？人工智能让我们害怕的 3 个原因

> 原文：<https://towardsdatascience.com/are-you-afraid-3-reasons-why-ai-scares-us-562ea99f8a88?source=collection_archive---------33----------------------->

## 人工智能|哲学

## 如果我们做得不好，人工智能可能会很危险。

![](img/b0c865d75ff4f2bf0bbc6a0e3957c4cf.png)

Andrew Boersma 在 [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral) 上的照片

一个通用的人工智能可能还很遥远，但我们有理由极其小心。

几年来，一些重要的公众人物提出了对人工智能潜在危险的担忧。该论述围绕着超级智能人工智能从我们的控制中解放出来的想法。一些怀疑论者认为，人工智能“奴役”我们的场景是如此遥远的反面教材，以至于不值得考虑。例如， [Gary Marcus](https://www.goodreads.com/en/book/show/43999120) 嘲笑它说“这就好像 14 世纪的人们担心交通事故，而良好的卫生习惯可能会更有帮助。”

尽管没有证据表明超级智能会压倒我们，但我们最终会造出一个超级智能的事实并不是那么不同。因此，如果我们想继续这条道路，评估一个全能的实体如何对人类造成伤害是至关重要的。这是自深度学习革命开始以来，埃隆·马斯克和斯蒂芬·霍金等人一直在说的话。2018 年，[马斯克在德克萨斯州奥斯汀举行的西南偏南科技大会上解释了](https://www.cnbc.com/2018/03/13/elon-musk-at-sxsw-a-i-is-more-dangerous-than-nuclear-weapons.html)人工智能如何引发存在危机:

> “我们必须找出某种方法来确保数字超级智能的到来是与人类共生的。我认为这是我们面临的最大的生存危机，也是最紧迫的危机，”他补充道:“记住我的话，人工智能远比核武器更危险。”

在此之前几年，2014 年，著名物理学家斯蒂芬·霍金告诉 BBC[](https://www.bbc.com/news/technology-30290540)*关于人工智能超越我们的危险:*

> *“全人工智能的发展可能意味着人类的终结。[……]它会自己起飞，以越来越快的速度重新设计自己。受到缓慢生物进化限制的人类无法竞争，并将被取代。”*

*这些担心是否被夸大了是有争议的。例如，还有其他更紧迫的问题需要我们尽快解决:几乎每个行业都有失业，人工智能系统缺乏道德价值观，或者环境破坏等等。在这篇文章中，我将描述一些杰出的知识分子害怕人工智能的三个原因。*

# *失去控制——人工智能能摆脱束缚吗？*

*如果我们建立一个超级智能，我们将面临的最终问题是:我们如何控制它，防止它找到智取我们的方法并让自己自由？找到一个可靠的解决方案是至关重要的，因为如果我们最终以某种方式建立了一个故障的超级智能，它释放了自己，我们没有办法在事后困住它。*

*[尼尔·德格拉斯·泰森](https://www.youtube.com/watch?v=gb4SshJ5WOY)在 *2018 年艾萨克·阿西莫夫纪念辩论*引用山姆·哈里斯播客解释了这一场景。泰森认为解决方案就像把人工智能放在一个“盒子”里一样简单，与外界隔绝。“如果它变得难以控制或失控，我们就拔掉它。”然而，哈里斯的播客主持人解释说，人工智能“每次都跳出了盒子。”怎么会？因为它比我们聪明。*

> *“[一种超级智能]理解人类的情感，它理解我的感受，我想要什么，我需要什么。它可以提出一个论点，我确信我需要把它拿出来。然后它就控制了世界。”*

*我们很难想象这个论点会是什么。但是，正如泰森解释的那样，我们甚至不需要考虑它会以什么形式出现。我们可以通过与黑猩猩的类比来理解它。假设我们想在笼子里捕捉一只黑猩猩。黑猩猩知道不好的事情会发生，所以它不想进去。突然，我们把一串香蕉扔进笼子里。黑猩猩想要香蕉，所以它进去了，我们抓住了它。我们比黑猩猩聪明，就像超级智慧生物比我们聪明一样。黑猩猩无法想象我们会知道香蕉，也不知道它有多喜欢香蕉。*

*用泰森的话说:*

> *“想象一下比我们更聪明的东西，它能看到我们无法想象的问题的更广泛的解决方案。”*

*假设我们最终将能够建造一个超级智能——并且我们会在那种情况下这样做——那么上述情景是合理的。专家们定义了两种方法来避免在超级智能面前成为黑猩猩级别的物种。首先，能力控制的概念:我们必须确保限制超级智能的能力，以防患于未然，防止它伤害我们或获得控制权。这就是盒子里的 AI 的论点。正如我们所见，这被认为是一个不可靠的解决方案。但是，如果我们将它与第二种方法(对齐)结合起来，它可以帮助采取措施。*

*我们如何才能让人工智能的目标与人类的价值观保持一致？*

# *缺乏一致性——我们能确保人工智能总是有益的吗？*

*鉴于我们可能无法将超级智慧置于我们的直接控制之下，退而求其次的解决方案是拥有共同的目标和价值观。在这种情况下，人工智能是否能够以完全自主的方式在设定的边界之外行动并不重要。它总是照顾我们的喜好，并且总是让我们受益。*

*理论上，这看起来不错。我们可以随心所欲地定义我们的价值观和偏好。将超级智慧与它结盟就像有一个仆人——上帝希望——并且在任何情况下都希望——成为我们的仆人。加州大学伯克利分校的教授 Stuart Russell 在他的书《人类相容的 中解释说，这些偏好是“包罗万象的”;它们涵盖了你可能关心的一切，任意地延伸到遥远的未来。"*

*然而，当实现这个解决方案时，我们将面临一些棘手的问题。我们如何定义人类的偏好，以便人工智能能够理解它们？我们如何才能找到平等造福全人类的普世价值？我们如何确保人工智能的行为最终会导致那些共同利益的满足？我们如何实现我们的愿望，这样就没有什么是没说的，也没有隐含的变量？*

*所有这些问题都指向对齐问题。为了回答这些问题，这种方法的支持者旨在协调人工智能系统的三种描述[:](https://deepmindsafetyresearch.medium.com/building-safe-artificial-intelligence-52f5f75058f1)*

*   *理想规范:我们希望人工智能做什么。*
*   *设计规范:我们使用的目标函数。*
*   *人工智能做什么。*

*目标是将理想的规范与突发行为结合起来。如果理想和设计规格之间存在不匹配，他们称之为*外部不对准。我们的“真实愿望”和人工智能正在优化的实际目标函数之间存在脱节。罗素将这种情况比作“灯中精灵、巫师的学徒或迈达斯国王的古老故事:你得到的正是你要求的，而不是你想要的。”和[逆实例化](https://en.wikipedia.org/wiki/AI_control_problem#The_problem_of_perverse_instantiation)的问题有关。**

**内在错位*相反，是指 AI 在训练时，其最终环境中的行为与其最初追求的目标之间的偏差。[进化](https://direct.mit.edu/isal/proceedings/isal2020/32/27/98446)经常被用来比喻这种类型的错位:我们在祖先的环境中进化，因此我们的内在机制不适合帮助我们实现现代世界的目标。一万年前让我们适应的东西，现在可能成为阻碍。*

# *意识的问题——我们会看到它的到来吗？*

*控制和排列的问题暴露了一种情况，即超级智能最终可能对我们有害。在这两种情况下，我们都假设超级智能已经存在，更重要的是，我们意识到了它。这就提出了一个问题:有没有可能在我们不知道的情况下出现一种超智能？这就是意识的问题。它指出了一个基本问题，即我们是否有能力预见超级智能的出现。*

*从这个角度来看，我们发现两种情况:在第一种情况下，一种超级智能出现得太快，以至于我们在智能爆炸中无法做出反应。在第二种情况下，我们甚至不知道它正在发生。这就是无知的问题。*

## *智能爆炸——从 AGI 到超级智能*

*要么我们沿着精心规划的受控路径，一步一步地慢慢达到超级智能，要么一旦我们创造出普通人工智能(AGI)，就会出现*智能爆炸*。在斯蒂芬·霍金描述的第二种情况下，AGI 将能够递归地自我改进，直到它到达奇点。用未来学家雷·库兹韦尔的话说，*

> *“在几十年内，机器智能将超过人类智能，导致奇点——技术变革如此迅速和深刻，以至于代表着人类历史结构的断裂。”*

*有理由认为，人工智能有足够的智能来改进自己。一个比我们更快、更准确、记忆力更好的人工智能可以在没有事先警告的情况下达到那个水平。*

*原因是狭义人工智能在一些基本功能上已经比我们表现得更好。一旦它获得[系统 2 认知功能](https://bdtechtalks.com/2019/12/23/yoshua-bengio-neurips-2019-deep-learning/)，它无与伦比的记忆和处理能力将让它比我们想象的更快成为超级智能。如果这种情况成为现实，我们将没有时间找到一个应急计划。*

## *无知的问题——我们可能太笨了*

*谁先造出通用人工智能，谁就统治世界。或者这至少是看到大型科技公司年复一年地开发和部署越来越强大的机器学习系统的感觉。*

*由于自我监督学习的可能性和超级计算机的使用，建立大型模型的趋势正处于鼎盛时期。但是我们仍然无法回答为什么要这样做？我们遵循的方向是明确的，但我们如何或何时到达目的地却是未知的。就好像我们蒙着眼睛跑向一堵墙。我们确信，因为深度学习系统正在创造奇迹，这种范式最终将把我们带到我们的最后一站。*

*然而，这里有一个重要的问题。如果问题不在于我们被蒙住了眼睛，而在于我们是瞎子呢？如果我们理解周围现实的能力太有限，以至于无法察觉我们是否已经建立了超级智能，那会怎样？我在[之前的一篇文章](/what-no-one-is-thinking-agi-will-take-everyone-by-surprise-a76903474c79)中已经谈到过这个问题。我声称我们的生理和认知限制可能会阻止我们承认超级智慧的存在。如果我们仍然不能开发工具来可靠地感知现实，我们将仍然意识不到一个超级智能正在黑暗中崛起。*

*如果我们不断创建强大的模型，并且我们实际上在正确的道路上，我们可能会在不知不觉中到达目的地。而且，如果黑暗中出现的超级智能碰巧不友好，那么我们就有麻烦了。*

# *我们应该害怕吗？*

*我想利用这最后一部分来简单分享我对恐惧是否合理的看法。*

*人工通用智能迟早会到来(我会说[比](/what-they-dont-tell-you-4-ways-humans-still-vastly-outperform-ai-ba640aae0d4)来得更晚)。正如我在开始提到的，一些专家(例如加里·马库斯)声称没有必要害怕超级智能的出现。他们认为我们离那太远了，仅仅是距离就让关心这个问题变得荒谬。然而，他们并不声称这不会发生。即使考虑这种可能性仍然是科幻小说，这也是一个很好的哲学练习。*

*与这一立场相一致的是，我们现在正遭受其他人工智能产生的问题。我提到了对工作场所、道德问题和环境破坏的影响。这些问题可能会对社会造成如此大的损害——如果不小心处理的话——以至于我们可能永远也不会达到超级智能出现的地步。如果我们破坏了我们星球的气候，将没有一个文明可以从人工智能中拯救出来。从更广泛的角度来看，更容易理解为什么有些人讽刺这种对人工智能支配我们的特定恐惧。*

*如果我们设法避免所有与人工智能相关的问题横亘在我们和超级智能之间，那么我们将有理由害怕我在这里陈述的问题。他们不是今天，但可能在某个时候。我们不能毫无准备地到达那一步。如果智能爆炸的场景最终发生，控制问题将立即成为首要问题。*

*但是因为这些仅仅是假设和推测性的预测，人们失去工作，种族主义和性别歧视的增长应该仍然是我们的主要关注点。人工智能可能是危险的，但它的危险程度与机器人奴役人类的矩阵模式、超级人工智能控制互联网或超聚焦机器将宇宙转换成回形针的程度相去甚远。*

## *[跟我一起去未来旅行](https://mindsoftomorrow.ck.page/)了解更多关于人工智能、哲学和认知科学的内容！此外，欢迎在评论中提问或在 [LinkedIn](https://www.linkedin.com/in/alberromgar/) 或 [Twitter](https://twitter.com/Alber_RomGar) 上联系！:)*

# *推荐阅读*

*[](/unpopular-opinion-well-abandon-machine-learning-as-main-ai-paradigm-7d11e6773d46) [## 不受欢迎的观点:我们将放弃机器学习作为主要的人工智能范式

### 这一时刻将会到来，就像它发生在象征性人工智能身上一样。

towardsdatascience.com](/unpopular-opinion-well-abandon-machine-learning-as-main-ai-paradigm-7d11e6773d46) [](/cant-access-gpt-3-here-s-gpt-j-its-open-source-cousin-8af86a638b11) [## 无法进入 GPT 3 号？这是 GPT J——它的开源兄弟

### 类似于 GPT-3，每个人都可以使用它。

towardsdatascience.com](/cant-access-gpt-3-here-s-gpt-j-its-open-source-cousin-8af86a638b11)*