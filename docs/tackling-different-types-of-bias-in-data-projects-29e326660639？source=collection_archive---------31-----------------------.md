# 处理数据项目中不同类型的偏差

> 原文：<https://towardsdatascience.com/tackling-different-types-of-bias-in-data-projects-29e326660639?source=collection_archive---------31----------------------->

## 这是一个难题

![](img/5d7a69718aa30bc77dadfe2b02b36608.png)

艾伦·布兰科·特耶多尔在 [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral) 上的照片

昨天(星期一)，我在一家户外黎巴嫩餐馆度过了一个晚上。食物是如此丰盛，但作为一名数据专业人员，我不禁被在入口处发生的事情所困扰…

[](/7-common-gotchas-of-data-projects-62e8646552f2) [## 数据项目的 7 个常见问题

towardsdatascience.com](/7-common-gotchas-of-data-projects-62e8646552f2) 

新冠肺炎病毒的风险仍然非常突出，因此所有因英国放松监管而开业的餐厅都必须遵守严格的预防措施，以确保我们尽可能的安全。

在我去过的那家餐厅，安全措施包括面部识别系统，该系统记录你在餐厅的存在，并测量你的体温，以发现新冠肺炎的早期迹象。如果你在“安全”范围内，模型会将你归类为阴性，这意味着你可以被允许进入餐厅——阳性扫描意味着你的体温超出了限制，所以你不能被允许进入。

每个人都在享受这个系统，直到轮到我…我调整我的头以适应框架…

"**欢迎尤瑟夫—录取**"

该系统已将我识别为餐厅员工之一。

没多久就意识到哪里出了问题；尤瑟夫是餐厅里唯一的黑人员工，他留着胡子——就像我一样。当我问他们以前是否发生过这种事，他们坚持说我是第一个，但我对此表示怀疑。也许这只是这个特殊的员工第一次经历，没有被报道(我这么说是因为像我这样留胡子的黑人经常去那里)。

这个系统显然存在一些偏差。

偏见是支持或反对一个观点或事物的不相称的力量，通常是以一种封闭的、偏见的或不公平的方式。偏见可能是天生的，也可能是后天习得的。人们可能会对某个人、某个团体或某个信仰产生偏见。在科学和工程中，偏差是一种系统误差。[ **来源** : [维基百科](https://en.wikipedia.org/wiki/Bias) ]

数据所代表的现象可能存在不一致的原因有很多。让我们来探索它们:

## 选择偏差

当我们对数据源的选择出现偏差，导致无法实现适当的随机化时，我们就在数据中引入了选择偏差。当未被发现时，从业者最终分析和建模不代表总体的样本。

例如，如果我决定收集我的博客帖子的意见，但我只将调查发送给在 LinkedIn 上给我发赞美消息的人，很可能所有这些人都会给我提供大量积极的反馈，但是，这些信息不会给我提供任何关于普通读者观点的信息。如果我们的数据中存在偏差，它很可能会在我们的模型中表现出来。

> “选择偏差是倾向于将你对数据源的选择偏向于那些容易获得、方便和/或有成本效益的”——Burk ov，a .机器学习工程。44 页。

为了避免选择偏差进入我们的数据，我们系统地质疑为什么选择一个特定数据源而不是另一个数据源背后的原因是至关重要的。如果原因是因为容易访问或成本低，那么一定要仔细关注并询问更多问题，如为什么以及如何创建数据。

## 自我选择偏差

自我选择偏差是选择偏差的一种不同形式；它出现在任何情况下，我们用来建立数据集的例子选择自己进入一个组，例如，任何由志愿者创建的数据源。

伦敦将于 2021 年 5 月 6 日投票选举新市长。最近在伦敦对 1000 名成年人进行的民意调查表明，现任伦敦市长萨迪克·汗将会以压倒性优势赢得选举。然而，该调查并没有透露出于任何原因没有参加投票的人数。虽然我认为这不太可能，但有可能是萨迪克·汗确保了他所有的朋友都参加了投票，因此他在投票中领先。

由 [@AdamBikov](https://twitter.com/AdamBienkov/status/1384808158103851009) 在[推特](https://twitter.com)上发推文

不幸的是，消除自我选择偏见并不容易。当某人自愿参加调查时，这是自我选择偏差出现的地方，仅仅是那个人同意回答问题就代表了自我选择偏差。

## 省略变量偏差

当我们的数据集遗漏了一个或多个准确预测所必需的特征时，就会出现遗漏变量偏差。因此，我们的模型会将缺失特征的影响归因于训练期间出现的特征。

比方说，我们希望建立一个模型，根据一些特征来确定一个人在一份工作中应该获得的报酬。众所周知，学术成就和高薪工作是密不可分的，所以我们把他们的学术成就作为一个特征。也有研究表明，如果一个人在书架上有很多书的家庭中长大，那么他在学校更有可能表现得更好，所以我们也把这作为一个特征。

我们的模型很可能会了解到，更多的书籍等同于更好的学术成就，这意味着更高的工资，但这忽略了非常重要的信息，如你父母的特征——他们的智商高吗？他们在一起吗？通过忽略我们父母的特征[和其他可能的特征]，我们忽略了一个可能会使我们的结果产生偏差的变量，这将使其他特征看起来似乎比它们实际上更重要(即书架上的书的数量将被赋予比它应有的更大的重要性)。

你听说过“我们不知道我们不知道的东西”这句话吗？这正是避免遗漏变量偏差极其难以完全避免的原因。

也就是说，我们可以简单地使用所有可用的信息——包括不太有价值的特性。这将增加数据的维度，但通过利用良好的正则化技术，我们的模型将能够确定哪些要素有价值，哪些没有价值。

[](https://neptune.ai/blog/fighting-overfitting-with-l1-or-l2-regularization) [## 与 L1 或 L2 正则化的战斗过度拟合-哪一个更好？- neptune.ai

### 机器学习模型的不良性能要么来自过度拟合，要么来自欠拟合，我们将仔细观察…

海王星. ai](https://neptune.ai/blog/fighting-overfitting-with-l1-or-l2-regularization) 

## 资金偏好

资助偏见是一项科学研究倾向于支持研究资助者的利益[ **来源** : [维基百科](https://en.wikipedia.org/wiki/Funding_bias#:~:text=Funding%20bias%2C%20also%20known%20as,of%20the%20study's%20financial%20sponsor.) ]。例如，一家公司可能会买一份提供商业世界新闻的报纸。

如果我们在试图预测市场走势时将这份报纸的数据纳入我们的数据集，我们的模型的性能可能是次优的，因为报纸机构可能会压制关于其所有者的负面新闻，并过分强调他们的成就。

我们可以通过对数据源，更具体地说，对数据源所有者提供数据的动机进行彻底的调查来减少资助偏差。

## 抽样偏误

当用于训练模型的数据不能反映模型在生产时将接收的样本分布时，就会出现采样偏差，这可能是在现实世界场景中观察到的最常见的偏差类型之一。

假设我们想要预测青少年使用毒品的可能性，那么我们去一所中学(11-16 岁)并收集所有青少年的数据(假设他们都诚实地说出他们是否使用非法毒品)。

该数据会有抽样偏差，因为并非每个青少年都在读高中，也就是说，有些人可能在家接受教育，有些人可能不得不辍学，还有许多其他原因。这些数据低估了没有上高中的青少年，而高估了上高中的青少年。

为了避免抽样偏差，对在生产环境中观察到的数据中各种属性的实际比例进行适当的研究是很重要的。下一步是对训练数据进行采样，使其与生产环境中的数据保持相似的比例。

## 刻板印象偏见

刻板印象仅仅是对某一类人或事物的一种广泛持有的、固定的、过于简单化的形象或想法；每当我们使用来自历史来源的数据时，往往会观察到刻板印象偏差，例如:

*   书
*   照片档案
*   社会化媒体
*   在线论坛和评论
*   等等。

感兴趣的读者可能会决定读一下 [*男人对于电脑程序员就像女人对于家庭主妇一样？去除词语嵌入的偏见*](https://arxiv.org/abs/1607.06520) 了解更多关于刻板偏见的流行实例。

为了控制由我们的学习算法做出的有偏见的决定，从业者必须选择将学习算法暴露给更加公平的例子分布。

## 系统价值扭曲

当用于记录测量值的设备或过程出现问题时，通常会出现系统性的价值扭曲——当模型部署到生产中时，我们的机器学习算法会做出次优的预测。

例如，假设我们想要构建一个 catflap，它会在猫靠近时自动打开，但不会为狗打开。我们使用我们的 iPhone，它有一个白平衡(我想)，为我们的训练数据拍照——照片最终看起来有点发黄。

在制作中，团队决定在猫翼上使用质量更好的相机，这个相机看到的是白色。由于相机是在低质量的图像上训练的，因此生产中的模型做出的预测将是次优的。

为了减轻这一困难，我们可以雇佣训练有素的专业人员在测量或观察设备的输出之间进行比较，或者我们可以使用多个测量设备来构建我们的数据集。

## 实验者偏差

实验者偏差是指研究者无意识地影响结果。数据。或者由于他们先前的信念或假设而成为实验的参与者。

在机器学习环境中，模型构建者可能会无意识地以一种确认他们先前存在的信念或假设的方式处理数据。在另一种情况下，模型构建者可以不断增加训练迭代，直到模型输出与他们的假设一致的结果。

解决实验者偏见的方法是让多个实验者验证所做的决定和提出的问题。

## 标记偏差

当有偏见的人或过程将标签分配给未标记的数据时，这会引入标签偏见。

例如，假设我们让一些贴标签机在阅读文档后给文档贴标签。一些贴标签机确实会阅读整个文档，并给它贴上一个经过深思熟虑的标签。另一方面，其他标记器会扫描文档中的关键字/短语，并基于这些为文档分配一个标签。

为了避免标记偏差，使用多个标记器来识别同一个例子是值得的——类似于集合模型。

## 最后的想法

事实上，确切地知道你的数据中存在何种类型的偏差是一项极其困难的任务，更不用说如何避免它们了。一个好的心态是孩子的心态。质疑一切:谁创造了数据？他们为什么要创建数据？数据是如何创建的？等等。本质上，在从事数据项目时，总是要持怀疑态度，会有错误，但只要你愿意学习和改进，你就会做得很好。

> “你无法完全避免数据偏差。没有灵丹妙药。一般来说，让一个人参与进来，尤其是当你的模型会影响到其他人的生活时。49 页。

> **注**:感兴趣的读者应该深入研究安德烈·布尔科夫的 [**机器学习工程**](https://www.amazon.co.uk/Machine-Learning-Engineering-Andriy-Burkov/dp/1999579577) 以获得更多信息。

如果你喜欢这篇文章，请通过订阅我的**免费** [每周简讯](https://mailchi.mp/ef1f7700a873/sign-up)与我联系。不要错过我写的关于人工智能、数据科学和自由职业的帖子。

## 相关文章

[](/always-remember-data-comes-before-the-science-681389992082) [## 永远记住数据先于科学

### 获取数据的不同方法

towardsdatascience.com](/always-remember-data-comes-before-the-science-681389992082) [](/tips-tricks-augmenting-data-for-nlp-tasks-983e33ad55a7) [## 提示和技巧:为 NLP 任务增加数据

### 扩大 NLP 数据集的方法

towardsdatascience.com](/tips-tricks-augmenting-data-for-nlp-tasks-983e33ad55a7) [](/4-data-related-books-ill-be-reading-in-april-efd06b367e35) [## 我将在四月份阅读的 4 本与数据相关的书

### 一定要看看这些书

towardsdatascience.com](/4-data-related-books-ill-be-reading-in-april-efd06b367e35)