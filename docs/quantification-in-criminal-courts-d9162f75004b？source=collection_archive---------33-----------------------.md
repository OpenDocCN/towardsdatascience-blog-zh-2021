# 刑事法庭中的量化

> 原文：<https://towardsdatascience.com/quantification-in-criminal-courts-d9162f75004b?source=collection_archive---------33----------------------->

## 法典正义还是算法不公平？

T 预测分析或风险评估工具侵入司法环境的例子是“COMPAS”在美国的实施([威斯康辛州](https://www.uclalawreview.org/injustice-ex-machina-predictive-algorithms-in-criminal-sentencing/#_ftnref)，早在 2012 年，经过 20 世纪 90 年代以来的多年发展)。这种算法软件使用机器学习技术，在大量“数据”中寻找模式或相关性。法官使用它们来评估罪犯再犯的可能性，同时做出假释、缓刑、保释和量刑决定——谁有可能在未来的某个时间点再次犯罪或无法出席法庭听证会。

由于司法系统已经存在需要解决的传统问题，作为强大的高科技解决方案成果的累犯风险等级被选为“[一种强大的、普遍的、不可阻挡的力量](https://www.cnbc.com/2018/12/14/the-seductive-diversion-of-solving-bias-in-artificial-intelligence.html?__source=sharebar%7Ctwitter&par=sharebar)”。事实上，这种风险评估算法是重新设计的，(zavrnik，2019)[1]，旨在改变传统的保释和判决系统，并最大限度地减少“导致法律适用不平等的人为偏见”(见 [Samuel Greengard](https://cacm.acm.org/news/244263-algorithms-in-the-courtroom/fulltext) ，2020)。毫无疑问，这种量化的风险评分方法提高了法院决策过程的效率和效力，以[预测正义](https://www.nst.com.my/opinion/columnists/2020/02/565890/ethical-questions-risks-using-ai-predictive-justice%20(last%20visited%20Oct%202)的名义，生动地改变了刑事司法范式。

但现有的文献和研究表明，它们反而引起了强烈的担忧——用“通过排斥和歧视的历史产生的偏见数据”创建了一个编纂司法的系统[2](见*例如*)。、 [ProPublica](https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing) ，2016； [Dressel & Farid](https://advances.sciencemag.org/content/4/1/eaao5580) ，2018； [Deeks](https://columbialawreview.org/content/the-judicial-demand-for-explainable-artificial-intelligence/) ，2019)。因此，司法部门的这种量化( [Angèle Christin](https://www.law.nyu.edu/sites/default/files/upload_documents/Angele%20Christin.pdf.) ，2015 年)同时被指明显具有影响力，并且"在宪法、技术和道德上令人不安"( [Starr](https://repository.law.umich.edu/articles/1190/) ，2014 年)；这最终会助长决策过程中的不公平做法。现在，让我们来看看这种方法如何以及为什么会引起人们对法庭上量化和不公平做法的关注。

# 法典化的正义和算法的不公平:

“法典化——正义”的新趋势(参见*例如*。[理查德 M. Re *等人*。，2019)主张统一和标准化高于自由裁量权。刑事司法算法的支持者认为，它们更可取，因为它们可以通过技术手段提高法院的效率、可及性和一致性；也减少了法官的偏见、自由裁量权和任意性。因此，司法环境中的量化和标准化促使法官推动司法法典化的事业，尽管这有可能限制他们的司法自由裁量权。在描述限制自由裁量权的好处时，人们可能会认为人工智能算法是一种使司法判决更加一致和有效的简单解决方案；这将有助于确定法官和检察官对其决定的责任。](https://law.ucla.edu/news/developing-artificially-intelligent-justice)

![](img/caffe3bfe14930d69ec2a6a794e4287d.png)

牛/Unsplash

但大量前沿研究和调查已经表明，算法透明度和可解释性的缺乏在司法背景下会产生严重影响，并在法庭实践中不利地破坏正当程序原则和公平原则。在一个特定的司法系统中，毫无疑问，公平和个性化的司法原则或自由裁量的道德判断是首要考虑因素。

因此，司法系统的基本价值将受到损害；这“倾向于以牺牲公平正义为代价来加强成文正义”([Richard m . Re*et al*T3)。, 2019).这就是为什么，尽管量化和智能计算的这种司法用途可以说改变了法官的态度和法院的做法，但公平原则和正义的理想教会我们将这些伤害最小化。](https://law.ucla.edu/news/developing-artificially-intelligent-justice)

然而，预测分析的倡导者也认为，在处理保释、判刑和缓刑时，仍然有充分个性化刑事案件的空间。之所以如此，是因为累犯风险评分可能不是做出决定的唯一依据，法院仍然应该有酌情权和必要的信息，在适当的时候不同意评估。[3]但反对者在其他地方发现了问题，因为“[这样的警告](https://harvardlawreview.org/2017/03/state-v%20loomis/#:~:text=Wisconsin%20Supreme%20Court%20Requires%20Warning)可能不会起作用，因为它倾向于向判决法院提供的信息数量而不是信息质量”，这最终可能导致“更严厉的判决”([Christin，Rosenblat&Boyd](https://www.law.nyu.edu/sites/default/files/upload_documents/Angele%20Christin.pdf)，2015)，其基础是“未言明的临床预测”([Hyatt，Chanenson，T11 博格斯特伦](https://dsc.duq.edu/dlr/vol49/iss4/5)，2011)。

再者，是这些分数依赖于群体数据的事实，所以累犯分数无法识别特定的高危个体；它更倾向于概率。此外，由于缺乏透明度和可解释性，对司法论坛中这种干预的效力知之甚少。

然而，考虑到司法系统中的公平和平等机会，也可以这样认为，只有专家才能理解的算法不能明确地为所有用户和主体提供公平的游戏；哪些司法机构，特别是法院，承诺这样做。同样，人们可能会认为，在发布这些(不公平和非法的)警告时，'[卢米斯'法院清楚地表明了其愿望](https://harvardlawreview.org/2017/03/state-v%20loomis/#:~:text=Wisconsin%20Supreme%20Court%20Requires%20Warning)灌输对该工具的准确性和该工具对少数民族罪犯构成的风险的评估的普遍怀疑'。

在另一种意义上，有人恰当地指出，机器学习算法不可能符合惩罚的交流理论。[4]但这种叙述并不完全适用于审前保释或警务预测分析的情况，这发生在确定一个人的罪行之前([【焦】](https://www.cambridge.org/core/journals/international-journal-of-law-in-context/article/fairness-accountability-and-transparency-notes-on-algorithmic-decisionmaking-in-criminal-justice/635E1CB265F4F94335D2CAEBDC4D68EE) *，2019 年)*，也有人恰当地认为，风险评估本身并不*削弱惩罚的沟通潜力，或者不一定与莫里斯的“有限报应主义”理论不相容([加勒特和莫纳汉](https://judicature.duke.edu/articles/assessing-risk-the-use-of-risk-assessment-in-sentencing/)，2019 年)。*

*然而，同样不可否认的是，惩罚、威慑和康复的崇高考虑并未嵌入这些算法的当前版本( [Angèle Christin](https://www.law.nyu.edu/sites/default/files/upload_documents/Angele%20Christin.pdf.) et al，2015)。此外，风险评估没有充分纳入因果关系，相反，它“强调一个主要理由，而损害其他理由:丧失能力”。甚至“没有有说服力的证据表明累犯评估工具优于法官的‘非正式预测’(评估风险的个体-临床-判断)，或者是歧视性更小的替代工具”( [Starr](https://repository.law.umich.edu/articles/1190/) ，2014)。因此，这当然是一个问题，这种不公平在司法环境中是永远不光彩的。*

# *需要考虑的事项:*

*总之，提及电子前沿基金会( [EFF](https://www.eff.org/about) )的提案是恰当的，该提案对计分系统的使用施加了[限制](https://www.eff.org/deeplinks/2018/12/eff-urges-california-place-meaningful-restrictions-use-pretrial-risk-assessment)。它生动地强调了精算工具应该是决定拘留个人的决定性因素，因为这种工具可以复制与依赖人类判断的现有系统相同的结果——甚至会犯下新的意想不到的错误([杰米·威廉姆斯](https://www.eff.org/deeplinks/2018/12/eff-urges-california-place-meaningful-restrictions-use-pretrial-risk-assessment)，2018)。由于司法系统总是敏感的，必须是公平和可信的，任何人工智能方法都被认为不仅仅是消除偏见；他们还解释他们的结果，为用户解释它们，并提供结果如何得出的[透明度](https://doi.org/10.1007/s13347-017-0279-x)([Colin Johnson](https://cacm.acm.org/news/233224-overcoming-ai-bias-with-ai-fairness/fulltext)，2018)。*

*换句话说，任何被告都应该有机会看到和质疑用于训练算法的“数据”和分配给每个输入因素的“权重”的延伸，而不仅仅是源代码(s *ee* [Chander](https://repository.law.umich.edu/mlr/vol115/iss6/13/) ，2017。总之，如果在使用的计算方法中有透明度、可解释性和可解释性，那么在法庭诉讼中也会推进和促进公平。然而，算法是否应该用于仲裁法庭判决的公平性仍然是一个复杂的问题。因此，仍然有一个更大的问题，即他们是减少了现存的不平等还是使之变得更糟。[5]*

***注意事项&参考文献:***

**[1]另见 alezavrnik，《算法司法:刑事司法环境中的算法和大数据》，欧洲犯罪学杂志 1–20(2019)。[这篇论文提到“人工智能工具将事半功倍，蒸发人类判断和推理中固有的偏见和启发”，这反过来将增加刑事司法机构的合法性，并将施加惩罚限制在“纯”科学方法和“理性”范围内]。**

**[2]参见 Ruha Benjamin，《技术之后的种族:新吉姆法典的废奴主义工具》(2019)[特别是针对偏见和默认歧视]。**

**[3]州诉卢米斯，881N . w . 2d 764–65(wis . 2016)，证书申请。已提交，编号 16–6387(美国 2016 年 10 月 5 日)**

**[4]参见安东尼·达夫《刑法的境界》(牛津大学出版社 2018)。**

**【5]‌karen 郝与乔纳森流浪，你能让 AI 比法官还公正吗？玩我们的法庭算法游戏，麻省理工科技评论(2019)。[有人生动地争辩说，既然公平的概念在不同的环境中有不同的含义，它在数学领域也是如此。Karen Hao 等人恰当地举例说明了公平的两个定义:保持不同组之间的通行费错误率的可比性，以及以同样的方式对待具有相同风险评分的人。然后有人争辩说，“这两种定义都是完全站得住脚的！但是同时满足两者是不可能的。】**