# 人工智能反对(担心)存在风险的理由

> 原文：<https://towardsdatascience.com/the-case-against-worrying-about-existential-risk-from-ai-d4aaa77e812b?source=collection_archive---------18----------------------->

## [播客](https://towardsdatascience.com/tagged/tds-podcast)

## 柳文欢·埃齐奥尼谈我们应该(和不应该)担心的人工智能风险

[苹果](https://podcasts.apple.com/ca/podcast/towards-data-science/id1470952338?mt=2) | [谷歌](https://www.google.com/podcasts?feed=aHR0cHM6Ly9hbmNob3IuZm0vcy8zNmI0ODQ0L3BvZGNhc3QvcnNz) | [SPOTIFY](https://open.spotify.com/show/63diy2DtpHzQfeNVxAPZgU) | [其他](https://anchor.fm/towardsdatascience)

*编者按:这一集是我们关于数据科学和机器学习新兴问题的播客系列的一部分*，*由 Jeremie Harris 主持。除了主持播客，Jeremie 还帮助运营一家名为*[*sharpes minds*](http://sharpestminds.com)*的数据科学导师初创公司。*

很少有人会不同意人工智能将成为人类历史上最重要的经济和社会力量之一。

但随着其变革潜力而来的是对人工智能可能给人类带来的一种奇怪的新风险的担忧。随着人工智能系统实现其目标的能力呈指数级增长，一些人担心，即使这些目标和我们自己的目标之间有一点点偏差，都可能是灾难性的。OpenAI、DeepMind、CHAI Berkeley、牛津大学等领先实验室的许多知识丰富、经验丰富的人工智能专家都有这些担忧。

但它们并不普遍:我最近让计算机科学教授兼作家梅勒妮·米切尔(Melanie Mitchell)在播客上讨论她对人工智能灾难论的反对意见，她曾就人工智能风险的话题与斯图尔特·罗素(Stuart Russell)展开了著名的辩论。在这一集里，我们将继续探索人工智能灾难性风险怀疑论的案例，采访艾伦人工智能研究所首席执行官柳文欢·埃齐奥尼，艾伦人工智能研究所是世界领先的人工智能研究实验室，开发了许多著名的项目，包括广受欢迎的艾伦 LP 图书馆和语义学者。

柳文欢对人工智能风险有独特的观点，对话非常有趣！以下是我最喜欢的一些外卖食品:

*   柳文欢担心人工智能引发灾难性风险的案例过于依赖纯理论的论证。他强调了人工智能风险社区的一个说法，即一般的人类水平的人工智能可能比怀疑论者认为的更快实现，并认为没有经验主义的理由相信这是事实。柳文欢强调在关于人工智能时间表的辩论中需要更多的经验证据，并提出了一系列基准——他称之为“人工智能煤矿中的金丝雀”——他认为这将预示着具有一般人类水平能力的系统的到来，值得担忧。虽然我不同意他对金丝雀的选择，但我认为他提出一些担心人工智能能力的客观门槛很棒。
*   我们讨论了当预测变革性或一般性人工智能何时将被开发时，人类的直觉可能会有多差。我们有从两个不同的方向接近类人智能的经验:首先，通过**育儿**，它教我们认为新生儿是哑巴，三岁的孩子更聪明，青少年更聪明，等等；第二，通过**进化**，这让我们考虑一个智能连续体，从细菌到蜥蜴到啮齿动物到猴子，最终到我们。三岁的孩子在某些方面是愚蠢的，而猫不会，反之亦然:当智能实体沿着两条路径接近智能时，会犯的错误类型是根本不同的，这就是为什么对一个的直觉不会转化为另一个。人工智能是第三种方法:人工智能系统可能会犯比人类可能犯的错误更愚蠢的错误——但反过来也是如此。因此，仅仅通过观察人工智能今天犯的错误，很难说人工智能离类人的一般性有多近或多远。
*   柳文欢认为，人工智能的灾难性风险通常是以类似于帕斯卡赌注的方式构建的。支持者有时建议，即使人工智能引发灾难的风险很低，其潜在影响也是如此重要，无论如何，我们现在应该专注于人工智能的安全——几乎不考虑风险实现的概率。柳文欢同意对长期安全问题的一些关注是重要的，但认为帕斯卡的赌博式论点导致了在人工智能对齐和人工智能安全方面的过度资源投资。

你可以在这里的推特上关注柳文欢，或者在这里的推特上关注我[。](https://twitter.com/jeremiecharris)

## 播客中引用的链接:

*   [柳文欢的《麻省理工科技评论》文章](https://www.technologyreview.com/2020/02/25/906083/artificial-intelligence-destroy-civilization-canaries-robot-overlords-take-over-world-ai/)谈到了警告信号，他希望看到人类水平的人工智能是否即将到来。
*   链接到艾伦人工智能研究所的推特页面。

![](img/6ff6120c6a11c2bb3fe994905e293110.png)

## 章节:

*   0:00 介绍
*   1:40 担心人工智能安全性和一致性的理由
*   9:35 何时优先考虑超级智能
*   14:25 人工智能协调和能力
*   25:30 人工智能工作时间表
*   34:05 煤矿中的金丝雀
*   37:25 从不同方向接近情报
*   40:55 后续步骤
*   未知的未知
*   53:05 总结