# 深度神经网络和高斯过程:相似性、差异和权衡

> 原文：<https://towardsdatascience.com/deep-neural-networks-vs-gaussian-processes-similarities-differences-and-trade-offs-18647376d799?source=collection_archive---------3----------------------->

## [思想和理论](https://towardsdatascience.com/tagged/thoughts-and-theory)

![](img/84ab9504e16a7c696b3a32710ca29709.png)

[乌列尔 SC](https://unsplash.com/@urielsc26?utm_source=medium&utm_medium=referral) 在 [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral) 上的照片

# 动机:比较最先进的

***深度神经网络*******(DNNs)****和 ***高斯过程(GPs)*******是两类极具表现力的监督学习算法。当考虑这些方法的应用时，一个自然的问题出现了:*“什么时候以及为什么使用一种算法比使用另一种有意义？”****

**![](img/8f9cb7718eff7c4a59690bc88895a233.png)**

**希望这篇文章能为你提供一些指导，告诉你什么时候一个模型比另一个模型更好。照片由 [Kristin Snippe](https://unsplash.com/@frausnippe?utm_source=medium&utm_medium=referral) 在 [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral) 上拍摄**

**在本文中，我们将致力于制定决定使用哪种方法的指南。然而，要开始开发这些指南，我们首先需要了解这些方法之间的关系。在本文中，我们将涵盖:**

1.  ****GPs** 和 **DNNs** 的理论异同**
2.  ****GPs** 与 **DNNs** 的优缺点**
3.  **使用 **GPs** 与 **DNNs** 的示例案例研究**

**我们开始吧！**

*** *有关高斯过程(GPs)的详细介绍/入门知识，请参见**

1.  **[**本条**](/gaussian-process-regression-from-first-principles-833f4aa5f842) (理论)**
2.  **[**本条**](/modern-gaussian-process-regression-9c5196ca87ab) (理论&实现)**
3.  **[**本条**](/batched-multi-dimensional-gaussian-process-regression-with-gpytorch-3a6425185109) (实现)。**

# **(几个)理论差异**

## **A.参数与非参数**

**![](img/fefcb27e5ca6286ae7cfa8185a3aab68.png)**

**从参数的角度来看，dnn 通常比 GPs 有更多通过学习来调整的“旋钮”。黛安·皮凯蒂诺在 [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral) 上拍摄的照片**

**量化这两个模型之间差异的一个方向是考虑每个框架中参数的数量和类型。一般来说，由于高斯过程被认为是*非参数*机器学习技术，**高斯过程(GPs)** 学习明显更少的参数，并且预测很大程度上由定义它们的训练数据集驱动。他们的参数选择完全由以下选项表示:**

1.  **核/协方差函数( **k(x，x’)**)**
2.  **平均函数( **m(x)** )**
3.  **似然协方差噪声( **σ** )。**

**这些选择/变量通常被称为“超参数”。**

**在**高斯过程(GPs)** 中发现的参数缺乏与许多现代**深度神经网络(DNNs)** 形成鲜明对比，后者旨在利用尽可能多的参数，即所谓的**权重**，来解决机器学习问题。在经典的统计学习文献中，使用大量的参数一直不被认可，因为这种想法会导致显著的过度拟合和对非分布数据的较差概括。然而，这种经典的统计学习理论未能解释过度参数化神经网络的经验成功，因此“*过度参数化*或“*插值*”机制的新理论开始占据上风[1，6]。**

**___________________________________________________________________**

****TL/DR #1: GPs** 为(近*) *非参数*，而 **DNNs** 为*过参数。***

**___________________________________________________________________**

***GPs 仅通过其超参数进行参数化，例如上面找到的那些参数。**

## ****B .直接与逆方法****

**与参数/非参数属性相关， **GPs** 和 **DNNs** 的另一个不同之处在于 **DNNs** 是 ***逆方法*** ， **GPs** 是 ***直接方法*** 。**

*****逆方法*** 涉及从训练数据中优化参数，因此也被称为监督学习方法。通常，这些方法涉及参数化，带有一些关于参数的初始信念(称为“先验”)。这些方法通常采用*自上而下*的方法，由此数据被用来更新参数中捕获的信念【7】。**

*****直接方法*** 依赖于直接使用训练数据来进行新的预测/获得洞察力，正如大多数内核机器所完成的那样。特别是核机器有一个简单的现象，即缺少核函数本身的细节，新的预测完全由现有的可用数据驱动。由于这些方法能够进行实质性的数据探索和洞察，而不需要对底层模型产生强烈的初始信念，因此这些方法通常被称为*自底向上*【7】。**

**___________________________________________________________________**

****TL/DR #2: GPs** 是(近*) *正方法*，而 **DNNs** 是*逆方法。***

**___________________________________________________________________**

**GPs 的超参数优化是一个间接的例程，通常使用基于梯度和 Hessian 的学习方法来完成。**

## **C.学习差异**

**![](img/bc31ac229f6a02967f89bc085db8a72e.png)**

**GPs 和 dnn 的部分不同之处在于它们如何遍历用于优化性能的损耗表面。丹尼尔·科波尼亚斯在 [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral) 上拍摄的照片**

**在没有深度高斯过程的情况下，在 **GPs** 和 **DNNs** 之间学习到的内容也不同。然而，完成学习的方法并没有太大的不同:两者都使用第一(在某些情况下，第二)阶方法。这两种方法也优化了不同的函数:对于神经网络，这是一个**损失/风险函数**，对于高斯过程，这是**边际似然函数**。**

**对于高斯过程，边际似然目标往往更加非凸，因此，经常使用二阶梯度下降算法，如[L-BFGS](https://en.wikipedia.org/wiki/Limited-memory_BFGS)【5】，进行优化以避免局部极小值。**

**___________________________________________________________________**

****TL/DR #3: GPs** 一般用二阶方法优化，如 L-BFGS【5】，使用目标函数的*海森*，而 **DNNs** 一般用一阶方法优化，如 SGD【8】，使用目标函数的*梯度*。**

**___________________________________________________________________**

## ****D .可解释性****

**在应用机器学习任务中，能够解释你的结果可能与结果本身一样重要。**

****人工智能:神经网络的可解释性****

**虽然一些较新的 **DNN** 结构允许对不确定性进行更彻底的近似，例如通过任意的和认知的不确定性【9】，但是这些网络中的许多只是提供了估计的预测值，并且可能提供了多类分类的逻辑值(预测概率)。然而，由于 **DNNs** 普遍缺乏可解释性已经成为一个热门的研究课题[10]，我相信未来更多的网络架构将会包含一些预测不确定性的元素。**

**其他进展，如 GradCam [12]等梯度可视化工具，也提高了 dnn 的可解释性，并有助于减少它们的“黑色性”。**

****D . II:GPs 的可解释性****

**相反， **GPs** 固有的高斯结构使其非常适合可解释的不确定性估计。对于某些需要直观风险评估的应用，这可能使该方法更具优势。**

**此外，GPs 具有良好的直观特性，即所有插值均值预测都是作为训练集中现有均值点的加权*线性*组合生成的，并按测试点到给定数据点的距离(在核函数空间中测量)进行缩放[11]。GPs 以线性方式重新组合他们以前看到的点，以产生新的预测。**

**___________________________________________________________________**

****TL/DR # 4:****GPs**的*线性和高斯*特性很好地帮助他们增加这些模型的可解释性。尽管 DNNs 长期以来被批评为“黑箱”,但今天的重大研究工作正在进行中，以帮助使这些模型更容易解释。**

**___________________________________________________________________**

# **(一些)理论上的相似之处**

## **A.“插值机制”中的核心机器**

**最近的研究表明，当具有线性激活函数的神经网络在其隐藏层中接近无限宽度时，它们渐近收敛于核机器[1，2]。这是一个**神经切线核(NTK)**【2】的想法。这种现象发生在所谓的“插值区域”，也称为“双下降曲线”的后半部分[1]。**

**高斯过程也是核机器，因为确定测试点的预测均值和方差的训练点的线性组合是由高斯过程的核函数确定的。**

**___________________________________________________________________**

****TL/DR #5:** 在一定条件下[1，2]， **DNNs** 可以作为所谓“插值”中的核机器进行分析，配备了一个核函数，从观察点的核加权组合中形成对观察点的预测。 **GPs** 本质上是内核机器【11】。**

**___________________________________________________________________**

****B .目标函数的优化****

**虽然有各种二阶方法，如 BFGS 和 BFGS，用于优化全球定位系统，一阶方法也可以用来优化这些模型。像 DNNs 一样，GPs 仍然努力最小化泛函(通常是具有核正则项的负对数似然)，就像神经网络努力最小化损失函数一样。**

**___________________________________________________________________**

****TL/DR #6:** 两个 **DNNs** 和 **GPs** 都是通过一阶和二阶优化方法来改进他们的模型。**

**___________________________________________________________________**

# **每种方法的优势**

**此列表绝非详尽无遗，但在决定使用神经网络还是高斯过程时，这里仅列出了一些优势(相对于其他框架):**

****GP** *优点* / **DNN** *缺点*:**

1.  **通常比 **DNNs** 需要更少的数据，因为它们需要调整的参数更少。然而，拥有更多数据，特别是在固定域上密度不断增加的情况下(称为固定域渐近[1])，有助于显著提高性能。**
2.  **只需要优化少量(超)参数。**
3.  **对爆炸和消失梯度等现象具有鲁棒性(因为，除非您使用深度 **GPs** ，否则在此框架内没有“层结构”)。**

****GP**缺点/**DNN**优点:**

1.  **运行时间与样本数量的比例很小。运行时复杂度为 ***O(n )*** ，其中 ***n*** 为样本数。这是必须执行大协方差矩阵的矩阵求逆(或伪求逆)的结果。**
2.  **相对于神经网络，自动学习更少，并且对于核/协方差函数、均值函数和超参数先验分布的选择，需要进行更多的设计考虑。这些参数会对 **GP** 能够学习的内容产生实质性的影响。**

# **每种技术的示例用例**

**![](img/aa1c83d287aac3d524b4785a9712d941.png)**

**[安德鲁·尼尔](https://unsplash.com/@andrewtneel?utm_source=medium&utm_medium=referral)在 [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral) 上拍照**

**请注意，以下建议不是绝对的，也就是说，这些建议的目的更多的是为了应用我们在上面学到的原则。**

****注意:**虽然我对这两个模型类都有丰富的实践和理论经验，但请不要将下面的这些建议视为绝对——在某些情况下，使用另一个模型类可能会更有利。**

2.  *******数据集很大*** → **使用 DNNs:** 推荐使用 DNNs，因为 GPs 运行时间与示例数据集数量的比例很小，还因为 DNNs 已被证明在给定足够大的数据集的情况下，可以在各种机器学习任务上实现最先进的性能。****
3.  *******执行连续插值→* 使用 GPs:** 推荐使用连续 GPs，因为连续 GPs 使用连续核函数测量距离，如 RBF 核和 Matern 核[11]，从而允许以考虑数据集中所有点的方式对来自现有点的新点进行*线性*加权。通过要求现有点的线性组合，仍然可以观察到精致的插值细节。****
4.  *******执行离散插值→* 使用 GPs:** 推荐使用，因为离散/网格 GPs 使用稀疏、离散核函数测量距离，如网格插值核。稀疏结构仍然允许通过考虑所有现有的点来预测新点，但是是以计算效率更高的方式进行的。****
5.  *******在动态数据集上学习和预测→* 使用 DNNs:** 由于 GPs(几乎)是直接方法，它们的预测机制很大程度上由创建它们的数据集定义。因此，如果定义 GPs 的数据集是动态的，这将需要重新调整/添加新的数据点，这将涉及重新计算协方差矩阵的逆，这是一项成本高昂的操作。相反，DNNs 可以很容易地适应新的数据点，因为它们是逆模型，并且预测仅由这些模型所基于的数据间接确定。****
6.  *******其他情况→您决定:*** 当然还有其他情况没有被上述建议解决。为了解决这些情况，考虑分析上面讨论的相似性/差异/权衡，以确定两个模型类中哪一个性能更好。****

# ******总结******

****![](img/6df0fd7fc70363a5bfaaad127045c693.png)****

****由[凯利·西克玛](https://unsplash.com/@kellysikkema?utm_source=medium&utm_medium=referral)在 [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral) 上拍摄的照片****

****我们已经讨论了**高斯过程(GPs)** 和**深度神经网络(DNNs)** 的理论相似性/差异、优缺点和应用。我们发现了以下情况:****

****___________________________________________________________________****

1.  ******GPs** 为【几乎】*非参数化*，而 **DNNs** 为*过参数化。*****
2.  ******GPs** 为(近)*正方法*，而 **DNNs** 为*逆方法。*****
3.  ******GPs** 一般用二阶方法优化，而 **DNNs** 一般用一阶方法优化。****
4.  ****GPs 的结构使得这些模型具有很强的可解释性。尽管 DNNs 长期以来被批评为“黑箱”,但今天的研究正在帮助这些模型变得更容易解释。****
5.  ****GPs 本质上是内核机器。在一定条件下， **DNNs** 也可以分析为内核机。****
6.  ******DNNs** 和 **GPs** 都通过一阶和二阶优化方法改进了他们的模型。****
7.  ******GPs** 一般比 **DNNs** 需要更少的数据，只需要优化少量(超)参数，对爆炸和消失梯度等现象具有鲁棒性。****
8.  ******GP** 运行时相对于 **DNNs** 的样本数伸缩性较差，相对于神经网络的自动学习较少。****
9.  ****如果:(i) **数据集很小**，或(ii) **执行插值→** 使用 **GPs。******
10.  ****如果:(i) **数据集很大**，或者(ii) **数据集是动态的→** 使用 **DNNs。******

******感谢您的阅读！**想看更多关于计算机视觉、强化学习、机器人技术的内容，请 [**关注我**](https://rmsander.medium.com/) 。考虑加盟 Medium？请考虑通过这里 报名 [**。感谢您的阅读！**](https://rmsander.medium.com/subscribe)****

## ****参考****

****[1]贝尔金，米哈伊尔。"无所畏惧地适应:通过插值棱镜进行深度学习的显著数学现象." *arXiv 预印本 arXiv:2105.14368* (2021)。****

****[2] Jacot，Arthur，Franck Gabriel 和 Clément Hongler。"神经正切核:神经网络中的收敛和泛化." *arXiv 预印本 arXiv:1806.07572* (2018)。****

****[3]达米亚诺、安德烈亚斯和尼尔·劳伦斯。"深度高斯过程。"*人工智能与统计*。PMLR，2013 年。****

****[4] Blomqvist、Kenneth、Samuel Kaski 和 Markus Heinonen。"深度卷积高斯过程." *arXiv 预印本 arXiv:1810.03052* (2018)。****

****[5]Liu d . c .，Nocedal，j .关于用于大规模优化的有限内存 BFGS 方法。*数学规划* **45、**503–528(1989)。[https://doi.org/10.1007/BF01589116](https://doi.org/10.1007/BF01589116)****

****[6]罗伯茨、丹尼尔·a、绍·亚伊达和鲍里斯·哈宁。“深度学习理论的原理。” *arXiv 预印本 arXiv:2106.10165* (2021)。****

****[7]自顶向下与自底向上的数据科学方法[。](https://blog.dataiku.com/top-down-vs.-bottom-up-approaches-to-data-science)****

****[8] *赫伯特·罗宾斯和萨顿·门罗*一种随机逼近方法*《数理统计年鉴》，第 22 卷，№3。(1951 年 9 月)，第 400–407 页，DOI: 10.1214/aoms/1177729586。*****

****[9]阿米尼、亚力山大、威尔科·施瓦廷、艾娃·索莱马尼和丹妮拉·鲁斯。"深度证据回归" *arXiv 预印本 arXiv:1910.02600* (2019)。****

****[10] Park，Sangdon 等人，“深度神经网络分类器的 PAC 置信度预测” *arXiv 预印本 arXiv:2011.00716* (2020)。****

****11 卡尔·爱德华·拉斯姆森。"机器学习中的高斯过程."*关于机器学习的暑期学校*。施普林格，柏林，海德堡，2003。****

****[12] Selvaraju，Ramprasaath R .等人，“Grad-cam:通过基于梯度的定位从深度网络进行视觉解释。”*IEEE 计算机视觉国际会议论文集*。2017.****