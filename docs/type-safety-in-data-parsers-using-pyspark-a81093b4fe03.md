# 使用 PySpark 的数据分析器中的类型安全

> 原文：<https://towardsdatascience.com/type-safety-in-data-parsers-using-pyspark-a81093b4fe03?source=collection_archive---------37----------------------->

## 使用 Apache PySpark 解析数据时确保类型安全

![](img/c78b77f4cd7db843797be7eb61ad0cad.png)

图为[托姆·米尔科维奇](https://unsplash.com/@thommilkovic?utm_source=medium&utm_medium=referral)在 [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral)

数据工程师的主要任务之一是从多个来源获取数据。这些来源可能是 API 端点、流服务、将文件上传到云的 cron 作业等。从这些来源获取的数据被转储到数据湖中，随后用于解析和下游业务逻辑的 ETL。

但是这里的问题是:当涉及到源系统产生的字段的数据类型时，我们能依赖源系统，尤其是第三方吗？

最好不要依赖这些来源，而是在消费级别卸载类型检查和类型转换的过程，即让数据消费者根据自己的需要定义目标数据类型。

本文介绍了一种确保解析器类型安全的方法及其优缺点。

# 方法

以 JavaScript Object Notation (JSON)格式的数据为例，JSON 中的字段可以有多种类型。有时，我们甚至可能不知道哪些字段具有哪些数据类型，这使得跟踪和确保正确维护各个字段的类型变得极其困难。

因此，在 JSON 模式包含的所有字段中实施统一性是至关重要的。最安全的方法是在模式树的所有叶节点上强制执行`StringType`。 ***叶节点*** 是一个字段，可以是字符串、bigint 或时间戳等。类型，但不是结构类型或数组类型。

让我们来看看这是如何做到的:

在 JSON 数据中强制使用 StringType

假设您已经将 JSON 数据加载到一个 dataframe 中，JSON 列以名称`json`出现。

首先，模式是从 json 列中推断出来的。然后，`enforce_stringtype`函数对从 Spark 推断的模式中检索到的模式元数据执行深度优先搜索(DFS)。它检查模式元数据中的特定字段是否是叶。如果是叶字段，它将所有叶字段(不是 struct 或 array 类型的字段)转换为 string 并返回 else，它在 struct 或 array 字段内部遍历并将所有叶字段转换为 string 类型。

然后，模式元数据被转换回 StructType，用于在 json 列上强制执行更改后的模式。

然后，可以将所需的转换应用于可以从`type_safe_json`列获得的字段，以转换为数据消费者指定的目标数据类型。

# 赞成的意见

这种方法有几个优点。

1.  转换为 StringType 不会导致任何数据丢失，也不会在摄取后和消费前过程中删除行。
2.  数据源对 JSON 中的字段进行的任何数据类型更改，例如从整数到双精度，都是通过这种方法处理的。
3.  强制 StringType 后完成的转换可以确保特定字段具有一组数据类型。

# 骗局

尽管这种方法有几个优点，但它也有一些缺点。

1.  强制 StringType 后应用于字段的转换是一项额外的计算。此外，如果源系统是可靠的，这些字段可能已经固有了目标数据类型。因为它们被转换成字符串，然后再转换回目标数据类型，这意味着一个冗余的计算。
2.  为要应用转换的字段设置目标数据类型是手工操作。如果有大量的字段要进行转换，这可能会变得很乏味。
3.  向下转换将导致数据丢失(如 double 到 int)，这必须由使用者解决。

总之，信任源系统保留每个字段的相同数据类型是有问题的。这种方法可以用来确保安全，避免由源系统引起的任何类型的数据类型冲突，但它有上述明显的缺点。将指定目标数据类型的过程转移给消费者，使得解析器独立于任何数据类型检查。这也符合在消费数据时决定模式的情况。