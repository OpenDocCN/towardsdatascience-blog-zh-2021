# å¦‚ä½•ç”¨ FastAPI æ„å»ºå’Œéƒ¨ç½² NLP æ¨¡å‹:ç¬¬ 1 éƒ¨åˆ†

> åŸæ–‡ï¼š<https://towardsdatascience.com/how-to-build-and-deploy-an-nlp-model-with-fastapi-part-1-9c1c7030d40?source=collection_archive---------8----------------------->

## éƒ¨ç½² NLP æ¨¡å‹çš„ç®€å•æ–¹æ³•ã€‚

![](img/81135e439173da889f818a81fdb96c7d.png)

æ¥è‡ª [Pexels](https://www.pexels.com/photo/person-in-blue-denim-jacket-holding-black-smartphone-5053740/?utm_content=attributionCopyText&utm_medium=referral&utm_source=pexels) çš„ [cottonbro](https://www.pexels.com/@cottonbro?utm_content=attributionCopyText&utm_medium=referral&utm_source=pexels) æ‘„å½±

**æ¨¡å‹éƒ¨ç½²**å¦‚æœä½ è¦ä»äº‹ NLP æ¨¡å‹å·¥ä½œï¼Œè¿™æ˜¯ä½ åº”è¯¥æŒæ¡çš„æœ€é‡è¦çš„æŠ€èƒ½ä¹‹ä¸€ã€‚

æ¨¡å‹éƒ¨ç½²æ˜¯å°†æ¨¡å‹é›†æˆåˆ°ç°æœ‰ç”Ÿäº§ç¯å¢ƒä¸­çš„è¿‡ç¨‹ã€‚è¯¥æ¨¡å‹å°†æ¥æ”¶è¾“å…¥ï¼Œå¹¶ä¸ºç‰¹å®šç”¨ä¾‹çš„å†³ç­–é¢„æµ‹è¾“å‡ºã€‚

> *â€œåªæœ‰å½“æ¨¡å‹ä¸ä¸šåŠ¡ç³»ç»Ÿå®Œå…¨é›†æˆæ—¶ï¼Œæˆ‘ä»¬æ‰èƒ½ä»å…¶é¢„æµ‹ä¸­æå–çœŸæ­£çš„ä»·å€¼â€ã€‚â€”å…‹é‡Œæ–¯æ‰˜å¼—Â·è¨ç±³ä¹Œæ‹‰*

æœ‰ä¸åŒçš„æ–¹æ³•å¯ä»¥å°†ä½ çš„ NLP æ¨¡å‹éƒ¨ç½²åˆ°äº§å“ä¸­ï¼Œä½ å¯ä»¥ä½¿ç”¨ Flaskï¼ŒDjangoï¼ŒBottle e.t.c .ä½†æ˜¯åœ¨ä»Šå¤©çš„æ–‡ç« ä¸­ï¼Œä½ å°†å­¦ä¹ å¦‚ä½•ä½¿ç”¨ FastAPI æ„å»ºå’Œéƒ¨ç½²ä½ çš„ NLP æ¨¡å‹ã€‚

åœ¨è¿™ä¸€ç³»åˆ—æ–‡ç« ä¸­ï¼Œæ‚¨å°†äº†è§£åˆ°:

*   å¦‚ä½•å»ºç«‹ä¸€ä¸ªå°† IMDB ç”µå½±è¯„è®ºåˆ†ç±»æˆä¸åŒæƒ…ç»ªçš„ NLP æ¨¡å‹ï¼Ÿ
*   ä»€ä¹ˆæ˜¯ FastAPIï¼Œå¦‚ä½•å®‰è£…ï¼Ÿ
*   å¦‚ä½•ç”¨ FastAPI éƒ¨ç½²æ‚¨çš„æ¨¡å‹ï¼Ÿ
*   å¦‚ä½•åœ¨ä»»ä½• Python åº”ç”¨ç¨‹åºä¸­ä½¿ç”¨æ‚¨éƒ¨ç½²çš„ NLP æ¨¡å‹ã€‚

åœ¨ç¬¬ 1 éƒ¨åˆ†ä¸­ï¼Œæˆ‘ä»¬å°†é‡ç‚¹æ„å»ºä¸€ä¸ª NLP æ¨¡å‹ï¼Œè¯¥æ¨¡å‹å¯ä»¥å°†ç”µå½±è¯„è®ºåˆ†ç±»ä¸ºä¸åŒçš„æƒ…æ„Ÿã€‚æ‰€ä»¥è®©æˆ‘ä»¬å¼€å§‹å§ï¼

# å¦‚ä½•å»ºç«‹è‡ªç„¶è¯­è¨€å¤„ç†æ¨¡å‹

é¦–å…ˆï¼Œæˆ‘ä»¬éœ€è¦å»ºç«‹æˆ‘ä»¬çš„ NLP æ¨¡å‹ã€‚æˆ‘ä»¬å°†ä½¿ç”¨ [IMDB ç”µå½±æ•°æ®é›†](https://www.kaggle.com/c/word2vec-nlp-tutorial/data?ref=hackernoon.com)æ¥æ„å»ºä¸€ä¸ªç®€å•çš„æ¨¡å‹ï¼Œè¯¥æ¨¡å‹å¯ä»¥åˆ†ç±»å…³äºè¯¥ç”µå½±çš„è¯„è®ºæ˜¯æ­£é¢çš„è¿˜æ˜¯è´Ÿé¢çš„ã€‚ä¸‹é¢æ˜¯ä½ åº”è¯¥éµå¾ªçš„æ­¥éª¤ã€‚

## å¯¼å…¥é‡è¦çš„åŒ…

é¦–å…ˆï¼Œæˆ‘ä»¬å¯¼å…¥é‡è¦çš„ python åŒ…æ¥åŠ è½½æ•°æ®ï¼Œæ¸…ç†æ•°æ®ï¼Œåˆ›å»ºæœºå™¨å­¦ä¹ æ¨¡å‹(åˆ†ç±»å™¨)ï¼Œå¹¶ä¿å­˜æ¨¡å‹ä»¥ä¾›éƒ¨ç½²ã€‚

```
# import important modules
import numpy as np
import pandas as pd# sklearn modules
from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.naive_bayes import MultinomialNB # classifier from sklearn.metrics import (
    accuracy_score,
    classification_report,
    plot_confusion_matrix,
)
from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer# text preprocessing modules
from string import punctuation # text preprocessing modules
from nltk.tokenize import word_tokenizeimport nltk
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer 
import re #regular expression# Download dependency
for dependency in (
    "brown",
    "names",
    "wordnet",
    "averaged_perceptron_tagger",
    "universal_tagset",
):
    nltk.download(dependency)

import warnings
warnings.filterwarnings("ignore")
# seeding
np.random.seed(123)
```

ä»æ•°æ®æ–‡ä»¶å¤¹åŠ è½½æ•°æ®é›†ã€‚

```
# load data
data = pd.read_csv("../data/labeledTrainData.tsv", sep='\t')
```

æ˜¾ç¤ºæ•°æ®é›†çš„æ ·æœ¬ã€‚

```
# show top five rows of data
data.head()
```

![](img/3b9084e49dc7382c98161308ef612adc.png)

æˆ‘ä»¬çš„æ•°æ®é›†æœ‰ 3 åˆ—ã€‚

*   **Id** â€”è¿™æ˜¯å®¡æ ¸çš„ Id
*   **æƒ…ç»ª** â€”ç§¯æ(1)æˆ–æ¶ˆæ(0)
*   **è¯„è®º** â€”å¯¹ç”µå½±çš„è¯„è®º

æ£€æŸ¥æ•°æ®é›†çš„å½¢çŠ¶ã€‚

```
# check the shape of the data
data.shape
```

(25000, 3)

è¯¥æ•°æ®é›†æœ‰ 25ï¼Œ000 æ¡è¯„è®ºã€‚

æˆ‘ä»¬éœ€è¦æ£€æŸ¥æ•°æ®é›†æ˜¯å¦æœ‰ä»»ä½•ç¼ºå¤±å€¼ã€‚

```
# check missing values in data
data.isnull().sum()
```

id 0
æ„Ÿæ‚Ÿ 0
å›é¡¾ 0
dtype: int64

è¾“å‡ºæ˜¾ç¤ºæˆ‘ä»¬çš„æ•°æ®é›†æ²¡æœ‰ä»»ä½•ç¼ºå¤±å€¼ã€‚

## å¦‚ä½•è¯„ä»·ç­çº§åˆ†å¸ƒ

æˆ‘ä»¬å¯ä»¥ä½¿ç”¨ pandas åŒ…ä¸­çš„ **value_counts()** æ–¹æ³•æ¥è¯„ä¼°æ•°æ®é›†çš„ç±»åˆ†å¸ƒã€‚

```
# evalute news sentiment distribution
data.sentiment.value_counts()
```

1 12500
0 12500
åç§°:æƒ…æ“ï¼Œç±»å‹:int64

åœ¨è¿™ä¸ªæ•°æ®é›†ä¸­ï¼Œæˆ‘ä»¬æœ‰ç›¸åŒæ•°é‡çš„æ­£é¢å’Œè´Ÿé¢è¯„è®ºã€‚

## å¦‚ä½•å¤„ç†æ•°æ®

åœ¨åˆ†ææ•°æ®é›†ä¹‹åï¼Œä¸‹ä¸€æ­¥æ˜¯åœ¨åˆ›å»ºæˆ‘ä»¬çš„æœºå™¨å­¦ä¹ æ¨¡å‹ä¹‹å‰ï¼Œå°†æ•°æ®é›†é¢„å¤„ç†æˆæ­£ç¡®çš„æ ¼å¼ã€‚

è¿™ä¸ªæ•°æ®é›†ä¸­çš„è¯„è®ºåŒ…å«äº†å¾ˆå¤šæˆ‘ä»¬åœ¨åˆ›å»ºæœºå™¨å­¦ä¹ æ¨¡å‹æ—¶ä¸éœ€è¦çš„ä¸å¿…è¦çš„å•è¯å’Œå­—ç¬¦ã€‚

æˆ‘ä»¬å°†é€šè¿‡åˆ é™¤åœç”¨å­—è¯ã€æ•°å­—å’Œæ ‡ç‚¹æ¥æ¸…ç†é‚®ä»¶ã€‚ç„¶åæˆ‘ä»¬å°†ä½¿ç”¨ NLTK åŒ…ä¸­çš„è¯æ±‡åŒ–è¿‡ç¨‹å°†æ¯ä¸ªå•è¯è½¬æ¢æˆå®ƒçš„åŸºæœ¬å½¢å¼ã€‚

å‡½æ•° **text_cleaning()** å°†å¤„ç†æ‰€æœ‰å¿…è¦çš„æ­¥éª¤æ¥æ¸…ç†æˆ‘ä»¬çš„æ•°æ®é›†ã€‚

```
stop_words =  stopwords.words('english')def text_cleaning(text, remove_stop_words=True, lemmatize_words=True):
    # Clean the text, with the option to remove stop_words and to lemmatize word # Clean the text
    text = re.sub(r"[^A-Za-z0-9]", " ", text)
    text = re.sub(r"\'s", " ", text)
    text =  re.sub(r'http\S+',' link ', text)
    text = re.sub(r'\b\d+(?:\.\d+)?\s+', '', text) # remove numbers

    # Remove punctuation from text
    text = ''.join([c for c in text if c not in punctuation])

    # Optionally, remove stop words
    if remove_stop_words:
        text = text.split()
        text = [w for w in text if not w in stop_words]
        text = " ".join(text)

    # Optionally, shorten words to their stems
    if lemmatize_words:
        text = text.split()
        lemmatizer = WordNetLemmatizer() 
        lemmatized_words = [lemmatizer.lemmatize(word) for word in text]
        text = " ".join(lemmatized_words)

    # Return a list of words
    return(text)
```

ç°åœ¨æˆ‘ä»¬å¯ä»¥é€šè¿‡ä½¿ç”¨ **text_cleaning()** å‡½æ•°æ¥æ¸…ç†æˆ‘ä»¬çš„æ•°æ®é›†ã€‚

```
#clean the review
data["cleaned_review"] = data["review"].apply(text_cleaning)
```

ç„¶åå°†æ•°æ®åˆ†ä¸ºç‰¹å¾å˜é‡å’Œç›®æ ‡å˜é‡ã€‚

```
#split features and target from  data 
X = data["cleaned_review"]
y = data.sentiment.values
```

æˆ‘ä»¬è®­ç»ƒçš„ç‰¹ç‚¹æ˜¯ **cleaned_review** å˜é‡ï¼Œç›®æ ‡æ˜¯**æƒ…ç»ª**å˜é‡ã€‚

ç„¶åï¼Œæˆ‘ä»¬å°†æ•°æ®é›†åˆ†æˆè®­ç»ƒå’Œæµ‹è¯•æ•°æ®ã€‚æµ‹è¯•è§„æ¨¡æ˜¯æ•´ä¸ªæ•°æ®é›†çš„ 15%ã€‚

```
# split data into train and validateX_train, X_valid, y_train, y_valid = train_test_split(
    X,
    y,
    test_size=0.15,
    random_state=42,
    shuffle=True,
    stratify=y,
)
```

## å¦‚ä½•å®é™…åˆ›å»ºæˆ‘ä»¬çš„ NLP æ¨¡å‹

æˆ‘ä»¬å°†è®­ç»ƒå¤šé¡¹å¼æœ´ç´ è´å¶æ–¯ç®—æ³•æ¥åˆ†ç±»è¯„è®ºæ˜¯æ­£é¢è¿˜æ˜¯è´Ÿé¢ã€‚è¿™æ˜¯æ–‡æœ¬åˆ†ç±»æœ€å¸¸ç”¨çš„ç®—æ³•ä¹‹ä¸€ã€‚

ä½†æ˜¯åœ¨è®­ç»ƒæ¨¡å‹ä¹‹å‰ï¼Œæˆ‘ä»¬éœ€è¦å°†æˆ‘ä»¬æ¸…ç†è¿‡çš„è¯„è®ºè½¬æ¢æˆæ•°å€¼ï¼Œä»¥ä¾¿æ¨¡å‹èƒ½å¤Ÿç†è§£è¿™äº›æ•°æ®ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ scikit-learn ä¸­çš„ **TfidfVectorizer** æ–¹æ³•ã€‚TfidfVectorizer å°†å¸®åŠ©æˆ‘ä»¬å°†ä¸€ç»„æ–‡æœ¬æ–‡æ¡£è½¬æ¢ä¸º TF-IDF ç‰¹å¾çŸ©é˜µã€‚

ä¸ºäº†åº”ç”¨è¿™ä¸€ç³»åˆ—æ­¥éª¤(é¢„å¤„ç†å’Œè®­ç»ƒ)ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨æ¥è‡ª scikit-learn çš„ä¸€ä¸ª[ç®¡é“ç±»](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html?ref=hackernoon.com)ï¼Œå®ƒé¡ºåºåº”ç”¨ä¸€ç³»åˆ—è½¬æ¢å’Œä¸€ä¸ªæœ€ç»ˆä¼°è®¡å™¨ã€‚

```
# Create a classifier in pipeline
sentiment_classifier = Pipeline(steps=[
                               ('pre_processing',TfidfVectorizer(lowercase=False)),
                                 ('naive_bayes',MultinomialNB())
                                 ])
```

ç„¶åæˆ‘ä»¬è®­ç»ƒæˆ‘ä»¬çš„åˆ†ç±»å™¨ã€‚

```
# train the sentiment classifier sentiment_classifier.fit(X_train,y_train)
```

ç„¶åï¼Œæˆ‘ä»¬æ ¹æ®éªŒè¯é›†åˆ›å»ºä¸€ä¸ªé¢„æµ‹ã€‚

```
# test model performance on valid data 
y_preds = sentiment_classifier.predict(X_valid)
```

å°†ä½¿ç”¨ **accuracy_score** è¯„ä¼°æŒ‡æ ‡æ¥è¯„ä¼°æ¨¡å‹çš„æ€§èƒ½ã€‚æˆ‘ä»¬ä½¿ç”¨ accuracy_score æ˜¯å› ä¸ºæˆ‘ä»¬åœ¨æƒ…æ„Ÿå˜é‡ä¸­æœ‰ç›¸åŒæ•°é‡çš„ç±»ã€‚

```
accuracy_score(y_valid,y_preds)
```

0.8629333333333333

æˆ‘ä»¬çš„æ¨¡å‹çš„ç²¾åº¦å¤§çº¦ä¸º 86.29%ï¼Œè¿™æ˜¯ä¸€ä¸ªå¾ˆå¥½çš„æ€§èƒ½ã€‚

## ä¿å­˜æ¨¡å‹ç®¡é“

æ¨¡å‹ç®¡é“å°†é€šè¿‡ä½¿ç”¨ **joblib** python åŒ…ä¿å­˜åœ¨æ¨¡å‹çš„ç›®å½•ä¸­ã€‚

```
#save model 
import joblib joblib.dump(sentiment_classifier, '../models/sentiment_model_pipeline.pkl')
```

# åŒ…æ‰

æ­å–œğŸ‘ğŸ‘ï¼Œæ‚¨å·²ç»å®Œæˆäº†ç¬¬ 1 éƒ¨åˆ†çš„å­¦ä¹ ã€‚æˆ‘å¸Œæœ›ä½ å·²ç»å­¦åˆ°äº†ä¸€äº›å…³äºå¦‚ä½•æ„å»º NLP æ¨¡å‹çš„æ–°çŸ¥è¯†ã€‚åœ¨ç¬¬ 2 éƒ¨åˆ†ä¸­ï¼Œæˆ‘ä»¬å°†å­¦ä¹ å¦‚ä½•ç”¨ FastAPI éƒ¨ç½² NLP æ¨¡å‹ï¼Œå¹¶åœ¨ python åº”ç”¨ç¨‹åºä¸­è¿è¡Œå®ƒã€‚

å¦‚æœä½ å­¦åˆ°äº†æ–°çš„ä¸œè¥¿æˆ–è€…å–œæ¬¢é˜…è¯»è¿™ç¯‡æ–‡ç« ï¼Œè¯·åˆ†äº«ç»™å…¶ä»–äººçœ‹ã€‚åœ¨é‚£ä¹‹å‰ï¼Œç¬¬ 2 éƒ¨åˆ†å†è§ï¼

ä½ ä¹Ÿå¯ä»¥åœ¨ Twitter ä¸Šæ‰¾åˆ°æˆ‘ [@Davis_McDavid](https://twitter.com/Davis_McDavid?ref=hackernoon.com) ã€‚

***æœ€åä¸€ä»¶äº‹:*** *åœ¨ä»¥ä¸‹é“¾æ¥é˜…è¯»æ›´å¤šç±»ä¼¼è¿™æ ·çš„æ–‡ç« ã€‚*

[](https://medium.com/geekculture/scikit-learn-0-24-top-5-new-features-you-need-to-know-7af15d8cdeac) [## sci kit-å­¦ä¹  0.24:æ‚¨éœ€è¦äº†è§£çš„ 5 å¤§æ–°åŠŸèƒ½

### Scikit-learn ä»ç„¶æ˜¯æœ€å—æ¬¢è¿çš„ Python å¼€æºå…è´¹æœºå™¨å­¦ä¹ åº“ä¹‹ä¸€ã€‚çš„â€¦

medium.com](https://medium.com/geekculture/scikit-learn-0-24-top-5-new-features-you-need-to-know-7af15d8cdeac) [](/improve-ml-model-performance-by-combining-categorical-features-a23efbb6a215) [## é€šè¿‡ç»„åˆåˆ†ç±»ç‰¹å¾æé«˜ ML æ¨¡å‹æ€§èƒ½

### æé«˜æœºå™¨å­¦ä¹ æ¨¡å‹æ€§èƒ½çš„ç®€å•æŠ€å·§ã€‚

towardsdatascience.com](/improve-ml-model-performance-by-combining-categorical-features-a23efbb6a215) [](https://medium.com/analytics-vidhya/how-to-deploy-a-machine-learning-model-for-free-7-ml-model-deployment-cloud-platforms-fd9191726a10) [## å¦‚ä½•å…è´¹éƒ¨ç½²æœºå™¨å­¦ä¹ æ¨¡å‹â€” 7 ML æ¨¡å‹éƒ¨ç½²äº‘å¹³å°

### æˆ‘è®°å¾—æˆ‘ç¬¬ä¸€æ¬¡åˆ›å»ºä¸€ä¸ªç®€å•çš„æœºå™¨å­¦ä¹ æ¨¡å‹ã€‚è¿™æ˜¯ä¸€ä¸ªå¯ä»¥é¢„æµ‹ä½ å·¥èµ„çš„æ¨¡å‹â€¦

medium.com](https://medium.com/analytics-vidhya/how-to-deploy-a-machine-learning-model-for-free-7-ml-model-deployment-cloud-platforms-fd9191726a10) 

*ä»¥å‰åœ¨è¿™é‡Œå‘è¡¨è¿‡* [*ã€‚*](https://hackernoon.com/how-to-build-and-deploy-an-nlp-model-with-fastapi-part-1-n5w35cj)