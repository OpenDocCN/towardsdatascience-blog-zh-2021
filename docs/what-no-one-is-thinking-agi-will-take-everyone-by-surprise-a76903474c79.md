# 没人想到的是:AGI 会让所有人大吃一惊

> 原文：<https://towardsdatascience.com/what-no-one-is-thinking-agi-will-take-everyone-by-surprise-a76903474c79?source=collection_archive---------18----------------------->

## 人工智能

## 这就是为什么我们可能在不知道的情况下创造了 AGI。

![](img/70979558b01a3dfb06bcc5879f65a9d8.png)

里卡多·安南达尔在 [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral) 上拍摄的照片

人们对人工智能(AGI)的兴趣如此之大，以至于我们可以用关于它的书填满整个[中文](https://en.wikipedia.org/wiki/Chinese_room)房间。然而，你不会在他们身上找到任何结论性的发现或理论。没有人知道 AGI 会是什么样子，我们什么时候会实现它，或者我们应该如何进行下一步。如果我们没有关于 AGI 的答案，为什么有这么多关于它的书、文章和论文？

物理学家劳伦斯·克劳斯在与诺姆·乔姆斯基的对话中拿这个悖论开玩笑。他说“一个领域的知识和关于这个领域的书籍数量成反比。”有很多关于 AGI 的信息，但真正了解的却很少。

AGI 可以说是人类历史上最伟大的发明。我们一直在问关于我们独特智能的问题，但自从 20 世纪中期计算机科学革命以来，我们已经开始最大程度地追求这一追求。巨大的资金和兴趣支持了过去 60 年的人工智能研究，但我们离实现 AGI 仍然很远。尽管人工智能在某些任务上已经达到了人类的熟练程度，但它仍然显示出狭隘的智能。

# GPT-3 &公司:我们离 AGI 最近

2020 年 5 月，OpenAI 的 GPT 3 号取得了最新突破。这个系统显示了比它的前辈更广泛的行为。它是用互联网的文本数据训练出来的，已经学会了学习；这是一个多任务元学习者。它可以从几个用自然语言写的例子中学习完成一项新任务。然而，尽管现在关于 GPT 3 号是 AGI 的争论已经尘埃落定——它离我们还很远——但它产生了一些疑问。

2020 年 7 月，OpenAI 发布了一个测试版 API，供开发人员使用该系统，很快他们就开始发现意想不到的结果，甚至连创造者都没有想到过。给定一套英文说明，GPT-3 被发现能够编写代码，诗歌，小说，歌曲，吉他标签，乳胶…因此，炒作变得疯狂，GPT-3 的受欢迎程度飙升，成为重要媒体杂志的头条新闻。

而炒作一出现，反炒作也不落后。专家和非专家试图降低炒作的语气。GPT-3 被描绘成一个全能的人工智能，但事实并非如此，这不得不说。甚至 OpenAI 的首席执行官 Sam Altman 也说这太过分了:“[GPT-3]令人印象深刻[……]但它仍然有严重的弱点，有时会犯非常愚蠢的错误。人工智能将改变世界，但 GPT 3 号只是非常早期的一瞥。”

人们开始寻找 GPT 3 号的局限性:它在哪里失败了，它不能完成什么任务，这是它的弱点……他们发现了很多——甚至可能太多。这可能是科技博客作者 Gwern Branwen 的想法。GPT 3 号并不完美，也不是 AGI，但是人们在 GPT 3 号应该成功的地方发现了失败。

为了展示科学的严谨性，格温汇编了大量已发表的例子，并重新测试了那些对 GPT-3 来说太难的例子。他[争辩道](https://www.gwern.net/GPT-3#prompts-as-programming)提示(输入到 GPT-3 的描述或例子)经常被定义得很糟糕。他说，提示被更好地理解为一种新的编程范式，必须相应地加以处理:

> **“抽样可以证明知识的存在，但不能证明知识的不存在**
> 
> 如果提示写得不好，没有包括足够的例子，或者使用了错误的采样设置，GPT-3 可能会“失败”。当有人展示 GPT-3 的“失败”时，我已经多次证明了这一点——失败是他们自己的。问题不在于给定的提示是否有效，而在于任何提示是否 works⁠."

他证明了人们在 GPT 3 号上发现的很大一部分弱点是他们在理解如何与系统沟通方面的失败。人们找不到 GPT-3 的极限，因为这超出了他们的测试方法。

# 极限的悖论

凡事都有极限。宇宙是有极限的——物理定律之外的事情是不可能发生的，无论我们怎么努力——甚至是无限的——自然数的集合是无限的，但它不包含实数的集合。

GPT-3 有局限性，而我们，那些试图发现它们的人，也有局限性。格温证明了在寻找 GPT 3 号的极限时，我们找到了自己的极限。不是 GPT 3 号没有完成某些任务，而是我们找不到合适的提示。我们的极限是阻止 GPT 3 号执行任务。我们阻止了 GPT 3 号发挥其真正的潜力。

这就提出了一个直接的问题:如果 GPT-3 的局限性经常被误认为是我们的局限性，我们如何精确地定义该系统能做什么或不能做什么的界限？如果没有办法将失败的是我们的情况与失败的是 GPT-3 的情况分开，未知变量的数量将大于方程的数量，从而不可能找到解决方案。

这可以扩展到我们未来创造的任何其他人工智能。如果 AGI 知道的比我们能评估的更多，我们只能通过观察它的行为来后知后觉地知道它能做什么或不能做什么。如果它最终变得有害，我们只有在事后才知道(就像 GPT-3 的偏向性所发生的那样)。

最终，我们是一个试图评价另一个有限系统的有限系统。谁能保证我们的极限在各方面都超越他们？我们有一个很好的例子说明情况可能并非如此:我们非常不善于评估自己的局限性。我们不断为自己能做的事情感到惊讶，无论是个人还是集体。我们不断打破身体和认知的极限。因此，我们的测量工具很可能达不到足够强大的人工智能的行动能力。

# 我们还没意识到，AGI 就来了

综上所述，一个非常可怕的问题出现了:我们能在不知道的情况下创造一个 AGI 吗？或者甚至没有能力最终知道它？我们的测量工具既不是无限的也不是无限的。我们可以创造一个无法评估其局限性的人工智能。GPT 3 号，不是 AGI(甚至也不接近)，已经部分超出了我们的工具。如果这是真的，下面的假设场景是可能的:我们将越来越接近 AGI，我们的测量工具，定义我们在感官之上感知的现实，将保持落后。当我们最终到达 AGI 时，我们的工具不会反映它，尽管它是真实的，我们不会知道它。

因为我们在探索创造 AGI 的过程中是在黑暗中冒险，在不知道的情况下实现它不属于已知未知的范畴(我们知道我们不知道的事情)，而是属于[未知的未知](https://en.wikipedia.org/wiki/Johari_window)的范畴。我们甚至不会知道我们不知道它。我们将继续相信，真正的现实是我们的工具告诉我们的。我们不会怀疑除此之外是否存在现实，因此，我们不会试图在那里找到任何东西。

这种可能性将保持不被评估，并永远进入未知的黑暗地方。在 AGI 决定展示自己之前，情况将会如此。然后，我们将不得不重新考虑我们所有的计划，并相应地落后几步。这并不意味着 AGI 将是有害或危险的，但不知道如此有影响力的东西总是一种风险。让我们希望 AGI 最终像《T4 I 机器人》中的桑尼一样友好。它会是一个很好的伴侣。

# 最后的想法

我们一直在想 AGI 会是什么样子，我们会如何创造它，或者什么时候……但是没有人想过当它发生的时候我们是否能够意识到它。我们的测量工具是有限的，我们改进它们的能力也是有限的。GPT 3 号已经表明我们的工具不够先进。图灵测试够了吗？我们能创造一个足够的测试吗？

AGI 会让我们大吃一惊。当我们不能评估我们正在询问的事物的存在时，我们现在问的问题似乎是不相关的。这是我们需要的第一个答案，我们很快就需要它。

***免责声明:本文观点为个人观点，不得与他人分享。欢迎在评论中继续讨论。你认为这种情况可能发生吗？我很想看看你要说什么！***

[***跟我一起去未来旅行***](https://mindsoftomorrow.ck.page/) ***了解更多关于人工智能、哲学和认知科学的内容！***

## 推荐阅读

</gpt-3-scared-you-meet-wu-dao-2-0-a-monster-of-1-75-trillion-parameters-832cd83db484> [## GPT 三号吓到你了？遇见武道 2.0:1.75 万亿参数的怪兽

towardsdatascience.com](/gpt-3-scared-you-meet-wu-dao-2-0-a-monster-of-1-75-trillion-parameters-832cd83db484) </artificial-intelligence-and-robotics-will-inevitably-merge-4d4cd64c3b02>  </ai-wont-master-human-language-anytime-soon-3e7e3561f943> [## 人工智能不会很快掌握人类语言

towardsdatascience.com](/ai-wont-master-human-language-anytime-soon-3e7e3561f943)