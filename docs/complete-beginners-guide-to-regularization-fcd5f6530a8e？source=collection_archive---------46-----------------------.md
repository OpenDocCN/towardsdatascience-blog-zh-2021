# 正规化完全初学者指南

> 原文：<https://towardsdatascience.com/complete-beginners-guide-to-regularization-fcd5f6530a8e?source=collection_archive---------46----------------------->

## 如何构建泛化能力强的神经网络

![](img/9a337ba1a3f0c097536cf5b5881ad591.png)

照片由[格温·威斯丁克](https://unsplash.com/@aboeka)在 [Unsplash](https://unsplash.com/photos/I3C1sSXj1i8) 上拍摄

无论我们是建立分类模型还是预测模型，我们的目标都是让模型在我们以前没有见过的数据上表现良好。这是我们从模型中产生价值的地方。在已经被标注或者之前已经被看到的数据上做得好，在驱动价值上并不是很重要。

当我们训练神经网络时，我们的目标是让模型在它学习参数的训练数据之外表现良好。在这种情况下，该模型被认为是通用的。

本文将作为正则化的介绍，正则化是在实践中用来构建高性能模型的技术。我们将研究为什么正规化是重要的，以及当前文献中的最佳实践。虽然这只是一个介绍，但我们将链接到相关的论文和文章，以进一步探索这些主题。

# 为什么正规化很重要

正如引言中所提到的，机器学习管道的一个主要目标是确保模型在它尚未看到的数据上概括得很好或表现得很好。张等人在《论文》中探讨了分类问题中的泛化问题。他们发现了神经网络的一个主要问题。

> “深度神经网络很容易适应随机标签”

他们进行了一项实验，根据随机标记的数据训练了一个模型，发现该网络能够实现零训练错误。这意味着神经网络能够完美地学习随机标记的数据。这显然是危险的，因为它表明神经网络能够完美地学习训练数据，而不用实际学习每个类的东西。尽管训练误差表明该模型是完美的，但是当分类新的、看不见的(并且被正确标记的)数据时，该模型的性能会非常差。

本文认为隐式和显式正则化都有助于改善泛化误差。隐式正则化是某些网络决策的结果，例如使用[随机梯度下降](https://en.wikipedia.org/wiki/Stochastic_gradient_descent) (SGD)，它收敛到一个小的范数，但不明确尝试正则化。虽然隐式正则化绝对值得一提，但它很少被探讨，所以在本文中我们将重点关注显式正则化技术，如权重衰减、丢弃和数据扩充。

规则化的另一个好处是它在防御敌对攻击方面的有效性。这些是人们试图欺骗一个模型，让它相信某样东西属于不同的类别，同时尽可能少地改变特征。

![](img/c3ac1a90f4ba1c38f97abdf0fe916085.png)

由[诺亚·布舍尔](https://unsplash.com/@noahbuscher)在 [Unsplash](https://unsplash.com/photos/M19QtooXPKs) 上拍摄

[在这篇文章](https://openai.com/blog/adversarial-example-research/)中，我们看到一个模型被欺骗，通过轻微改变图像中的像素，以 99.3%的置信度相信一幅熊猫图像是一只长臂猿。另一个更严重的恶意攻击的例子包括欺骗垃圾邮件检测网络，使其相信垃圾邮件不是垃圾邮件。

如果一个模型不能很好地概括，它将更容易受到这些攻击，也更容易找到攻击的方法。通常，这些攻击是通过创建一个与生产中使用的网络相似的网络来完成的，然后对数据的变化使用损失函数，同时最大化模型预测错误类别的机会。我们将在下面讨论的正则化技术有助于使这个过程变得更加困难。

# 正规化最佳实践

在本节中，我们将介绍三种用于训练神经网络以提高模型泛化能力的正则化技术。

## 重量衰减

权重衰减，通常也称为 [L2 正则化](https://en.wikipedia.org/wiki/Regularization_(mathematics))或岭正则化，是一种为模型权重向[交叉熵损失函数](https://machinelearningmastery.com/cross-entropy-for-machine-learning/)添加二次项的技术。

权重较大的模型能够更有效地拟合训练数据。通过将模型中的权重项添加到损失函数中，训练过程现在由于具有较大的权重而受到惩罚。这迫使模型最小化分类误差和权重的大小。这导致较差的训练精度，但通常较好的验证和测试精度(或改进的泛化误差)。

## 拒绝传统社会的人

另一种常见且有效的正则化技术被称为[丢失](https://arxiv.org/pdf/1207.0580.pdf)。顾名思义，Dropout 在训练过程中随机“丢弃”或删除某些节点及其对应的连接。

这篇[论文](https://arxiv.org/abs/1207.0580)展示了 50%缺失层在泛化和消除“特征检测的协同适应”方面的有效性当神经元被随机丢弃时，其他每个神经元被迫学习权重，以有效地对数据进行分类。这种方法极大地提高了性能。

> 如果输入和正确输出之间的关系很复杂，并且网络具有足够的隐藏单元来对其进行精确建模，则通常会有许多不同的权重设置可以对训练集进行近乎完美的建模，尤其是在只有有限数量的标记训练数据的情况下。这些权重向量中的每一个都将对保留的测试数据做出不同的预测，并且几乎所有的权重向量在测试数据上的表现都比在训练数据上的表现差，因为特征检测器已经被调整为在训练数据上很好地协同工作，但是在测试数据上却不是这样。”—辛顿等人。

## 数据扩充

在分类中，给定类中的许多对象都具有非常相似的特征，只是对底层数据做了一些小的修改。例如，在图像分类中，如果图像右上方的狗在屏幕左下方，旋转 180 度，或者在不同的光照下，它就是同一只狗。

![](img/c934217827c092c56a3d92c2209f16bf.png)

由[克里斯·劳顿](https://unsplash.com/@chrislawton)在 [Unsplash](https://unsplash.com/photos/5IHz5WhosQE) 上拍摄

数据扩充是一种用于从现有数据中生成新训练样本的技术。当使用 [PyTorch](https://pytorch.org/docs/stable/index.html) 创建模型时，这里有许多不同的[转换可用](https://pytorch.org/vision/stable/transforms.html)，它们都旨在将这些小的调整样本添加到训练数据中。

通过这些稍微调整的图像，模型能够学习关于每个类别的有用特征，并更好地准备对它尚未看到的图像进行分类。越多的训练数据几乎总是越好。这种技术通过利用现有数据生成更多数据，而不需要收集和标记数据的成本(通常是一个昂贵的过程)。

# 结论

我们已经研究了为什么正则化是重要的，以及当前文献中的一些最佳技术。正则化有助于神经网络在它尚未看到的数据上表现得更好，也提高了模型抵御敌对攻击的机会。

虽然权重衰减、丢弃和数据增加不是仅有的三种可用的正则化技术，但是它们实现起来非常简单，并且非常有效。如果您有兴趣更深入地探索这个主题，其他技术，如[提前停止](https://machinelearningmastery.com/early-stopping-to-avoid-overtraining-neural-network-models/)、[批量调整](https://machinelearningmastery.com/how-to-control-the-speed-and-stability-of-training-neural-networks-with-gradient-descent-batch-size/)和[隐式正则化技术](https://arxiv.org/abs/1709.01953)是继续阅读的好主题。

感谢您阅读本文！