# 数据科学面试中的 7 个 A/B 测试问题和答案

> 原文：<https://towardsdatascience.com/7-a-b-testing-questions-and-answers-in-data-science-interviews-eee6428a8b63?source=collection_archive---------0----------------------->

## [数据科学面试](https://towardsdatascience.com/tagged/data-science-interview)

## 运行 A/B 测试的常见陷阱和解决方案

![](img/6996275b2592938518f659f957c1e4fe.png)

由[维多利亚诺·伊斯基耶多](https://unsplash.com/@victoriano?utm_source=medium&utm_medium=referral)在 [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral) 上拍摄的照片

A/B 测试，也称为受控实验，在工业中广泛用于产品上市决策。它允许科技公司评估用户子集的产品/功能，以推断产品如何被所有用户接受。数据科学家处于 A/B 测试流程的最前沿，A/B 测试被认为是数据科学家的核心能力之一。**数据科学面试反映了这一现实**。面试官通常会问候选人 A/B 测试问题以及业务案例问题(也称为指标问题、产品感觉问题),以评估候选人的产品知识和推动 A/B 测试流程的能力。

在本文中，我们将采用**面试驱动的**方法，将一些最常见的面试问题与 A/B 测试的不同组成部分联系起来，包括选择测试思路、设计 A/B 测试、评估测试结果以及做出是否发货的决定。具体来说，我们将涵盖 **7** 个最常见的面试问题和答案。

在你开始阅读之前，如果你是一个视频爱好者，请随意查看这个 [YouTube 视频](https://youtu.be/X8u6kr4fxXc)以获得这篇文章的缩略版本。

# **目录**

1.  [测试前——不是每个想法都值得测试](#306e)
2.  [设计 A/B 测试](#9339)

*   运行一个测试需要多长时间？
*   [对照组和治疗组之间的干扰](#22a0)
*   [如何应对干扰？](#555e)

3.[分析结果](#53bf)

*   [新奇和首因效应](#e963)
*   [多重测试问题](#4848)

4.[做决策](#3d12)

5.[资源](#0943)

# 测试之前——不是每个想法都值得测试

A/B 测试是一个强有力的工具，但是并不是每个想法都会被测试选中。一些想法的测试成本可能很高，处于早期阶段的公司可能会有资源限制，所以对每个想法都进行测试是不切实际的。因此，我们想首先选择哪些想法值得测试，特别是当人们对改进产品有不同的意见和想法，并且有许多想法可供选择时。例如，UX 设计师可能建议更改一些 UI 元素，产品经理可能建议简化结账流程，工程师可能建议优化后端算法，等等。在这种时候，利益相关方依靠数据科学家来推动基于数据的决策。一个简单的面试问题是:

> 在电子商务网站上有一些增加转化率的想法，例如启用多商品结账(目前用户可以一次结账一个商品)，允许非注册用户结账，改变“购买”按钮的大小和颜色等。，你如何选择投资哪个创意？

评估不同想法的价值的一种方法是使用历史数据进行**定量分析**，以获得每个想法的**机会大小**。例如，在投资一个电子商务网站的多商品结账之前，通过分析每个用户购买的多商品数量来获得影响的上限大小。如果只有很少一部分用户购买了不止一件商品，那么开发这个功能就不值得了。更重要的是调查用户的购买行为，以了解用户为什么不一起购买多件商品。是因为物品选择太少吗？物品是否太贵，他们只能买得起一件？结账过程是否过于复杂，他们不想再经历一遍？

这种分析为 A/B 测试提供了方向性的见解。然而，历史数据只告诉我们过去我们是如何做的。它无法准确预测未来。

为了全面评估每个想法，我们可以通过焦点小组和调查进行**定性分析**。从焦点小组收集的反馈(与用户或感知用户的引导讨论)或从调查中收集的问题提供了对用户痛点和偏好的更多洞察。定性和定性分析的结合可以帮助进一步的想法选择过程。

# 设计 A/B 测试

一旦我们选择了一个要测试的想法，我们需要决定我们要运行测试多长时间，以及如何选择随机单元。在这一部分，我们将逐一讨论这些问题。

## 运行测试需要多长时间？

为了决定测试的持续时间，我们需要获得测试的样本大小，这需要三个参数。这些参数是:

*   第二类错误率 *β* 或幂，因为幂= *1 - β* 。你知道其中一个，你知道另一个。
*   显著性水平 *α*
*   最小可检测效应

经验法则是，样本量 *n* 大约等于 16(基于 *α = 0.05* 和 *β = 0.8* )乘以*样本方差*除以 *δ* 平方，而 *δ* 是处理和对照之间的差值:

![](img/df479490f15caeddaebb946bb1935e10.png)

这个公式来自罗恩·科哈维、黛安·唐和徐亚的可信在线控制实验

如果你有兴趣了解我们是如何得出经验法则公式的，请看[这个视频](https://youtu.be/JEAsoUrX6KQ)一步一步的演示。

在面试中，你不需要解释你是如何得出这个公式的，但是你需要解释我们是如何获得每个参数的，以及每个参数是如何影响样本量的。例如，如果样本方差较大，我们需要更多的样本，如果增量较大，我们需要较少的样本。

样本方差可以从现有数据中获得，但是我们如何估计 *δ* ，即处理和对照之间的差异？

实际上，在我们进行实验之前，我们不知道这一点，这就是我们使用最后一个参数的地方:**最小可检测效应**。在实践中，这是最小的差别。例如，我们可以将收入增加 0.1%视为最低可检测效果。实际上，这个价值是由多个利益相关者讨论和决定的。

一旦我们知道了样本大小，我们就可以通过将样本大小除以每个组中的用户数量来获得运行实验的天数。如果数量少于一周，我们应该运行至少七天的实验来捕获每周的模式。通常建议运行两周。说到为测试收集数据，多总比不够好。

## 对照组和治疗组之间的干扰

通常，我们通过随机选择用户并将每个用户分配到对照组或治疗组来划分对照组和治疗组。我们希望每个用户都是独立的，控制组和治疗组之间没有干扰。然而，有时这种独立性假设并不成立。这可能发生在测试社交网络时，如脸书、Linkedin 和 Twitter，或者双边市场，如优步、Lyft 和 Airbnb。一个简单的面试问题是:

> X 公司测试了一个新功能，目标是增加每个用户创建的帖子数量。他们将每个用户随机分配到控制组或治疗组。就帖子数量而言，测试以 1%的优势胜出。在向所有用户推出新功能后，您预计会发生什么？会和 1%一样吗，如果不一样，会多一点还是少一点？(假设没有新奇效应)

答案是我们会看到一个大于 1%的值。原因如下。

在**社交网络**(例如脸书、Linkedin 和 Twitter)中，用户的行为很可能受到其社交圈中的人的影响。如果用户网络中的人(如朋友和家人)使用某个功能或产品，用户会倾向于使用它。那叫做**网络效应**。因此，如果我们使用“用户”作为随机化单元，并且治疗对用户有影响，该影响可能会溢出到对照组，这意味着对照组中的人的行为会受到治疗组中的人的影响。在这种情况下，对照组和治疗组之间的差异**低估了**治疗效果的真正益处。对于面试问题，会超过 1%。

对于**双边市场**(例如优步、Lyft、ebay 和 Airbnb):控制组和治疗组之间的干扰也会导致对治疗效果的有偏估计。这主要是因为控制组和处理组之间共享资源，这意味着控制组和处理组将竞争相同的资源。例如，如果我们有一种新产品吸引了治疗组的更多司机，那么对照组的司机就会减少。因此，我们无法准确估计治疗效果。与社交网络中治疗效果低估新产品的实际收益不同，在双边市场中，治疗效果会高估实际效果。

## 如何应对干扰？

![](img/482f19d0dc8c66741120ab5c837ff3cf.png)

克里斯·劳顿在 [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral) 上拍摄的照片

既然我们知道了为什么控制和处理之间的干扰会导致发射后效应的表现不同于处理效应，这就把我们带到了下一个问题:**我们如何设计测试来防止控制和处理之间的溢出？**一个示例面试问题是:

> 我们推出了一项新功能，为我们的骑手提供优惠券。目标是通过降低每次乘车的价格来增加乘车次数。概述测试策略以评估新功能的效果。

有许多方法可以解决群体之间的溢出问题，主要目标是**隔离对照组和治疗组中的用户**。下面是一些常用的解决方案，每一个都适用于不同的场景，并且都有局限性。在实践中，我们希望选择在特定条件下效果最好的方法，我们也可以结合多种方法来获得可靠的结果。

**社交网络:**

*   确保隔离的一个方法是创建**网络集群**来代表用户组，这些用户组比组外的人更有可能与组内的人互动。一旦我们有了这些聚类，我们可以把它们分成对照组和治疗组。查看[这篇文章](https://engineering.linkedin.com/blog/2019/06/detecting-interference--an-a-b-test-of-a-b-tests)了解更多关于这种方法的细节。
*   **自我聚类**随机化。这个想法源于 Linkedin。一个集群由一个“自我”(一个焦点个体)和她的“改变者”(与她直接相连的个体)组成。它侧重于测量单出网络效应，即用户的直接连接对该用户的影响，然后每个用户要么具有该功能，要么不具有该功能，用户之间不需要复杂的交互。[本文](https://arxiv.org/abs/1903.08755)详细解释了这种方法。

**双边市场:**

*   **基于地理的**随机化。我们可以按地理位置来划分，而不是按用户来划分。例如，我们可以将纽约大都会地区作为对照组，将旧金山湾区作为治疗组。这将允许我们隔离每个组中的用户，但缺点是会有更大的差异，因为每个市场在某些方面都是独特的，如客户行为、竞争对手等。
*   另一种方法是**基于时间的**随机化，尽管不常用。基本上，我们选择一个随机的时间，例如，一周中的某一天，并将所有用户分配到控制组或治疗组。当治疗效果只持续很短一段时间时，例如当测试一种新的激增价格算法执行得更好时，它就起作用了。当治疗效果需要很长时间才能生效时，如转诊计划，则不起作用。用户可能需要一些时间来查阅他或她的朋友。

# 分析结果

![](img/1033f13a570e58979135f01924aebaa9.png)

[斯科特·格雷厄姆](https://unsplash.com/@sctgrhm?utm_source=medium&utm_medium=referral)在 [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral) 上的照片

## 新奇和首要效应

当产品有变化时，人们会有不同的反应。有些人习惯了产品的工作方式，不愿意改变。这被称为**首要效应**或变化厌恶。其他人可能欢迎改变，一个新的特性吸引他们更多地使用产品。这被称为**新奇效应**。然而，这两种效应都不会持续很久，因为人们的行为会在一定时间后稳定下来。如果 A/B 测试有较大或较小的初始效应，这可能是由于新奇或首要效应。这是实践中常见的问题，很多面试问题都是关于这个话题的。一个简单的面试问题是:

> 我们对一项新功能进行了 A/B 测试，测试成功了，所以我们向所有用户发布了变更。然而，在推出该功能一周后，我们发现治疗效果迅速下降。发生了什么事？

答案是新奇效应。随着时间的推移，随着新鲜感的消退，重复使用将会减少，因此我们观察到治疗效果下降。

现在你了解了新奇和首要效应，**我们如何解决潜在的问题**？这是典型的面试时的跟进问题。

处理这种影响的一种方法是完全排除这些影响的可能性。我们可以只对第一次使用的用户进行测试，因为新奇效应和首要效应显然不会影响这些用户。如果我们已经有一个测试运行，我们想分析是否有一个新奇或首要效应，我们可以 1)比较对照组和治疗组的新用户的结果，以评估新奇效应 2)比较第一次用户的结果和治疗组现有用户的结果，以获得新奇或首要效应的影响的实际估计。

## 多重测试问题

在最简单的 A/B 测试中，有两个变量:对照(A)和治疗(B)。有时，我们会对多个变体进行测试，看看哪一个是所有特性中最好的。当我们想要测试一个按钮的多种颜色或者测试不同的主页时，就会发生这种情况。那我们就不止有一个治疗组了。在这种情况下，我们不应该简单地使用 0.05 的相同显著性水平来决定测试是否显著，因为我们正在处理 2 个以上的变量，错误发现的概率会增加。例如，如果我们有 3 个治疗组与对照组进行比较，观察到至少 1 个假阳性的几率是多少(假设我们的显著性水平是 0.05)？

我们可以得到没有假阳性的概率(假设各组是独立的)，

Pr(FP = 0) = 0.95 * 0.95 * 0.95 = 0.857

然后获得至少有一个假阳性的概率

Pr(FP >= 1) = 1 — Pr(FP = 0) = 0.143

只有 3 个治疗组(4 个变体)，假阳性(或 I 型错误)的概率超过 14%。这就是所谓的“**多重测试**问题。一个简单的面试问题是

> 我们正在对 10 个变体进行测试，尝试不同版本的登录页面。一个治疗成功，p 值小于 0.05。你会改变吗？

答案是否定的，因为多重测试的问题。有几种方法可以接近它。一种常用的方法是 **Bonferroni 校正**。它将显著性水平 0.05 除以测试次数。对于面试问题，由于我们测量 10 个测试，那么测试的显著性水平应该是 0.05 除以 10，即 0.005。基本上，我们只要求一个显著的测试，如果它显示 p 值小于 0.005。Bonferroni 校正的缺点是它往往过于保守。

另一种方法是控制**误发现率** (FDR):

FDR = E[假阳性的数量/拒绝的数量]

它度量了所有对零假设的拒绝，也就是说，您声明具有统计显著差异的所有指标。有多少是真的不同，有多少是假阳性。这只有在你有大量的度量标准时才有意义，比如说几百个。假设我们有 200 个指标，FDR 上限为 0.05。这意味着我们可以接受 5%的假阳性。我们每次都会在这 200 个指标中观察到至少 10 个假阳性。

# 做决定

![](img/b81808fd896b47d0a06cea038bb40ef5.png)

照片由[优 X 创投](https://unsplash.com/@youxventures?utm_source=medium&utm_medium=referral)在 [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral) 上拍摄

理想情况下，我们看到实际上显著的治疗结果，我们可以考虑向所有用户推出该功能。但是有时，我们会看到**与**相矛盾的结果，比如一个指标上升，而另一个指标下降，所以我们需要做出一个双赢的权衡。一个简单的面试问题是:

> 运行测试后，您会看到所需的指标，例如点击率在上升，而印象数在下降。你会如何做决定？

实际上，做出产品发布决策可能非常复杂，因为要考虑各种因素，如实施的复杂性、项目管理工作、客户支持成本、维护成本、机会成本等。

在采访过程中，我们可以提供解决方案的简化版本，重点关注实验的当前目标。是为了最大化参与度、留存率、收益，还是其他？此外，我们希望量化负面影响，即非目标指标的负面变化，以帮助我们做出决策。例如，如果收入是目标，假设负面影响是可接受的，我们可以选择它而不是最大化参与度。

# 资源

最后，我想向您推荐两个资源来学习更多关于 A/B 测试的知识。

*   [Udacity 的免费 A/B 测试课程](https://www.udacity.com/course/ab-testing--ud257)涵盖了 A/B 测试的所有基础知识。Kelly Peng 有一篇[很棒的帖子](/a-summary-of-udacity-a-b-testing-course-9ecc32dedbb1)总结了课程的内容。
*   值得信赖的在线控制实验——A/B 测试实用指南。它深入了解如何在行业中运行 A/B 测试、潜在的陷阱和解决方案。它包含了很多有用的东西，所以我实际上打算写一篇帖子来总结这本书的内容。有兴趣的敬请关注！

# 感谢阅读！

如果你喜欢这个帖子，想支持我…

*   *订阅我的* [*YouTube 频道*](https://www.youtube.com/channel/UCAWsBMQY4KSuOuGODki-l7A)
*   **跟我上* [*中*](https://medium.com/@emmading) *！**
*   **连接上*[*Linkedin*](https://www.linkedin.com/in/emmading001/)*！**
*   *前往[emmading.com/resources](https://www.emmading.com/resources)获取更多关于数据科学面试技巧和策略的免费资源！*

*</how-i-got-4-data-science-offers-and-doubled-my-income-2-months-after-being-laid-off-b3b6d2de6938> *