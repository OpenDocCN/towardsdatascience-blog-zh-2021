# 编写可伸缩的 ML 代码

> 原文：<https://towardsdatascience.com/writing-ml-code-that-scales-51028e9872a0?source=collection_archive---------31----------------------->

## 从单台机器到多工人设置

在你最终创建了培训脚本之后，是时候扩大规模了。从一个本地开发环境，无论是 IDE 还是 Colab，到一个大型的计算机集群，这是一个相当大的挑战。以下最佳实践使这种转变更加容易。

![](img/0bc1e4dc53ad977dc72f3de3dcb1b708.png)

弗兰克·布施在 [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral) 上的照片

## 参数分析器

首先是使用参数解析器。Python 通过 *argparse* 模块提供了这样的功能。通常，您希望将批处理大小、时期数和任何目录作为可选参数。手动浏览一个脚本，找到所有使用特定参数的地方，然后全部手动更改，这非常令人沮丧。因此，避免硬编码这样的参数。而不是写作

写起来更方便

## 将参数解析器的设置与训练代码分开

这是我经常犯的错误。我在训练脚本中创建了参数解析器，并在本地验证了训练。接下来，我部署了这段代码。却看到它在远程机器上失败了:我设置为默认参数的目录指向我的本地磁盘。

**编辑/注释**:虽然我仍然推荐使用参数解析器，但我发现处理超过 10 个、20 个参数是混乱的。在这种情况下，我建议仅将它们用于主要设置。对于更高级的设置，我推荐使用 [*Gin* python 库](https://github.com/google/gin-config)。

点击此处了解更多信息:

</many-hyperparameters-use-gin-fdf6741d282>  

## 将参数解析器的设置与训练代码分开

而不是编写类似于

(覆盖您在部署机器上手动更改的参数)，您有一个不受更新影响的单独文件:

您的本地项目和已部署的项目都有这个文件。然后在主训练代码中导入这个方法；在本地，路径指向您的本地文件夹，在远程机器上，它们指向相应的目录。这样，在从本地单个 GPU 机器转到多工作器设置后，您就不必记得更改默认参数了。

**注意**:和之前类似，我现在推荐用 [*Gin* python 库](https://github.com/google/gin-config)处理参数设置。我在这里写了[一个简短的介绍](/many-hyperparameters-use-gin-fdf6741d282)。

# 单独的模型创建脚本

将您的模型创建例程从主要的训练代码中分离出来会产生精益代码。与上一点类似，您使用一个单独的文件来存储模型及其配置，然后在主代码中导入该方法。将这一点与下一点结合起来，实现巨大的可伸缩性。

# 使超参数易于编辑

当您硬编码任何参数时，您将很难更新它们。使用 512 个内核而不是 16 个？使用更大的致密层？

每次你尝试一个新的配置，你首先要找到相关的代码，然后改变它。这将很快变得令人讨厌。解决方案是将超参数作为脚本参数的一部分，或者用一个单独的文件来存储它们。

第一个选项看起来像

第二个选项类似于

在这两种情况下，您都有一个管理默认参数的中心位置。从 16 核扩展到 512 核？那只是几秒钟的事了。

**注意**:和之前类似，我现在推荐用 [*Gin* python 库](https://github.com/google/gin-config)处理参数设置。我在这里写了[一个简短的介绍](/many-hyperparameters-use-gin-fdf6741d282)。

# 使用与设备无关的代码

这是我在 [TensorFlow](https://www.tensorflow.org/api_docs/python/tf/distribute/Strategy) 上看到的一个极好的特性:无论是单个 GPU 还是 20 个工人，每个人都有 16 个 GPU，编写你的代码，它无论如何都可以工作——无需任何修改。TensorFlow 用*策略*对象为您处理这个问题。这使您能够在单个 GPU 和多个工作人员之间无缝切换，当您在编码时只能访问单个设备，但在部署时使用多个设备时，这非常方便。

而不是写作

您用策略对象包装模型创建和编译例程:

将您的代码包装在策略对象的范围内负责分发模型和优化器。您可以使用 Hugginface 的 Transformer 库中的以下代码片段来创建这样一个对象:

我使用相同的技术从本地非 GPU 设置扩展到多 GPU 集群和 TPU 培训。在一个命令中使用四个、八个甚至更多的 GPU 是很神奇的。

# 让您的框架来处理数据集

您可以让 ML 框架来完成这项任务，而不是手动负责加载数据、解析、缓存等等。许多人为这些项目做出了贡献，使得默认管道非常有效。它们本身支持多处理、缓存、批处理等等。如果您正在使用 TensorFlow，有许多方法可以让您的数据为这些操作做好准备；在这篇文章中，我展示了其中的一些[。](/a-practical-guide-to-tfrecords-584536bc786c)

尤其是对于复杂的环境，这一点至关重要:如何将数据提供给三个工作人员？这些功能通常已经实现。不要多此一举，但**专注于有趣的部分:**

**创建和训练模型。**