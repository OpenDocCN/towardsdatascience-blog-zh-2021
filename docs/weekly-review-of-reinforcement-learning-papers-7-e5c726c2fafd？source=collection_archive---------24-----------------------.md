# 强化学习论文#7 的每周回顾

> 原文：<https://towardsdatascience.com/weekly-review-of-reinforcement-learning-papers-7-e5c726c2fafd?source=collection_archive---------24----------------------->

## 每周一，我都会发表我研究领域的 4 篇论文。大家来讨论一下吧！

![](img/6a94d3673b7cf4203d721359ce23f66a.png)

作者图片

[ [←上一次回顾](/weekly-review-of-reinforcement-learning-papers-6-2f919fe2a479?source=friends_link&sk=da5442ea1ab2dcccd2e6223d4f2d7f9a) ][ [下一次回顾→](https://qgallouedec.medium.com/weekly-review-of-reinforcement-learning-papers-8-9d02a67b2e8a?sk=a9f56ffd7721f0956f8319456eec4d5f)

# 论文 1:神经科学和强化学习之间的学习差距

Wauthier，S. T .，Mazzaglia，p .，atal，o .，De Boom，c .，Verbelen，t .，和 Dhoedt，B. (2021 年)。[神经科学和强化学习之间的学习鸿沟](https://arxiv.org/abs/2104.10995)。arXiv 预印本 arXiv:2104.10995 。

有两种可能的配置。奖励用红圈表示。在第一种配置中，奖励在左边，在第二种配置中，奖励在右边。

![](img/7fa62251ca9de1284155435d2f61fe5b.png)

来自报纸的数字

在其初始位置，代理无法区分这两种配置。所以增加了一条线索:一个窗户后面的圆圈，如果奖励在左边，它是蓝色的，如果奖励在右边，它是绿色的。最后一个细节，特工不允许探索 T 的两个分支，一旦他进去了一个，他就不能回来了。很简单，不是吗？你认为 DQN 或 PPO 会在多少次迭代中解决这个问题？打个赌。

下面是令人惊讶的坏结果:在最好的情况下，**DQN/彩虹、PPO 和 DreamerV2 的成功率是 50%** 。代理人不可能做得比这更好。50%意味着代理似乎知道奖励出现的两个位置，但似乎无法预测奖励是在左边还是在右边(即使底部的彩色圆圈给了它这个信息)。代理人似乎没有把线索圈的颜色和悬赏的位置联系起来。有人可能会说这种观察是不全面的，解决这个问题需要记忆机制。DreamerV2 有记忆机制，成功率还是 50%。

我必须承认我对这些结果感到惊讶，但我认为这揭示了许多 RL 作品的偏见。当前的基准测试限制太多，并且使用许多技巧来使学习过程有效。这些技巧虽然重要，但往往在报纸上甚至没有提及。作者提出的一种方法是再次接近神经科学。我喜欢他们的结论。

# 论文 2:复杂行动空间中的学习和计划

休伯特，t .，施利特维泽，j .，安东诺格鲁，I .，巴雷卡廷，m .，施密特，s .，和西尔弗，D. (2021)。[复杂动作空间的学习与规划](https://arxiv.org/abs/2104.06303)。 *arXiv 预印本 arXiv:2104.06303* 。

作用空间可以是有限离散的，无限离散的，连续的，甚至多维连续的。行动空间越复杂，正确评估政策就越困难。仅仅列举所有可能的行动可能是不可能的。在本文中，作者提出了一个相当自然的想法:**关注发生概率最高的动作**。如何评价这种发生概率？你不用，你只要**样**就可以了。这种抽样是问题的核心，因此他们给他们的框架起了个名字:抽样 MuZero(他们使用 MuZero，但是他们的方法应该在所有基于策略迭代的方法上工作)。
所以我们不再对整个动作空间进行推理，而只对动作的缩减子集进行推理。于是问题出现了:为了让学习正确进行，必须对多少个动作进行采样？显然，采样的动作越多，学习就越好，但是计算时间越长也越重要:这是一种折衷。为了说明这种折衷，作者将他们的方法应用于围棋游戏:362 种可能的动作。是的，行动空间的大小是非常合理的，但它允许他们与理论上的最大值进行比较，理论上的最大值在于可以访问整个行动空间。所以他们通过取样 15、25、50 和 100 个动作来比较学习曲线。结果如下:

![](img/02a4c54245e49570564713137c9ab4f1.png)

图来自[文章](https://arxiv.org/abs/2104.06303):经典围棋棋局结果。Elo 衡量玩家的水平。

我们看到，通过对 50 个动作(不到 15%的可能动作)进行采样，学习曲线非常接近对应于访问所有动作的曲线。
连续动作空间也获得了类似的结果(DeepMind 控制套件，Real-WorldRL 套件)。这是一个有趣的技巧，可以大大减少计算时间。

# 论文 3:基于模型的强化学习的模块库

Pineda，l .，Amos，b .，Zhang，a .，Lambert，N. O .，& Calandra，R. (2021)。MBRL 库:[基于模型的强化学习的模块化库](https://arxiv.org/abs/2104.10159)。 *arXiv 预印本 arXiv:2104.10159* 。

基于模型的强化学习框架不再需要被证明。当与环境交互的成本很高时，基于模型的 RL 仍然是给出最佳结果的方法。然而，实现没有无模型方法那么直接。作者提议建立一个图书馆，他们称之为 MBRL 图书馆。它是一个机器学习库，用于基于连续状态-动作空间中的模型进行强化学习(我们可以很遗憾不能使用离散空间)。图书馆使用 PyTorch。

说实话，我还没用过，不过一直在浏览代码。基于模型学习的示例代码仍然不多。目前，只有 MBPO 和宠物，仅此而已。我猜目标是贡献者丰富他们的图书馆。我很期待看到这一举措是否会在基于模型的 RL 社区中产生诱惑。0.1.0 版本发布不到两周，敬请关注。这里是[回购](https://github.com/facebookresearch/mbrl-lib)。

# 论文 4:进化强化学习算法

共同雷耶斯，法学博士，苗，y，彭，d，雷亚尔，e，莱文，s，乐，Q. V，…和浮士德，A. (2021)。[进化强化学习算法](https://arxiv.org/abs/2101.03958)。arXiv 预印本 arXiv:2101.03958 。

学习就是学会学习。它是机器学习的一个完整分支，与强化学习有一些交叉。这是其中之一。
出发点是所有的强化学习算法都可以用图来表示。这里有一个 DQN 的例子:

![](img/e6a7145ca223eb7f0864a1bdb91dacd5.png)

图来自[文章](https://arxiv.org/abs/2101.03958):DQN 的可视化，作为一个计算图。输出就是损失。

先说一组算法，随机的或者文献上的。这些算法必须首先在所谓的“障碍”环境中表现良好，然后才能被允许在一组更困难的环境中进行训练。如果一个算法过不了这一关，它就会被淘汰。其他的用来更新不时变异的算法群体。在训练结束时，在测试环境中评估性能最佳的算法，该测试环境不同于它已经训练过的所有环境。

![](img/109cd157a5e0eb20ba55d1a2c2b62e58.png)

图来自[文章](https://arxiv.org/abs/2101.03958):方法概述。

令人惊讶的结果:通过在简单的经典控制和 gridworld 任务上从头开始学习(没有任何最先进的算法)，这种方法重新发现了时差(TD)算法！

我很高兴向你们展示我本周的阅读材料。请随时向我发送您的反馈。