# 数据散列:你可能从来不知道的数据分析技巧

> 原文：<https://towardsdatascience.com/the-super-secret-data-analysis-trick-you-probably-never-knew-about-a00aa10ef310?source=collection_archive---------25----------------------->

## 散列数据不仅仅是为了验证软件下载

![](img/b2e1872b610e1489863213926063a78e.png)

图片由 Pixabay 通过 Pexels 提供

几年前，我在一家人工智能初创公司负责运营。我们有一个很棒的团队，由才华横溢但相对年轻且缺乏经验的数据工程师组成，由一位才华横溢且经验丰富的数据科学家领导。这远不是一个不正常的设置。

我们在大部分结构化数据上做了很多模型；然而，清理其中的一些是一个熊。它源自我们的客户，包含历史业务记录。当然，正如所有好的数据函数一样，我们用各种内部和外部数据集适当地丰富了它。

有一天，团队在他们的发言中提到，重复数据消除花费的时间比预期的长得多。它只是没有按照它应该的方式进行，因此我们的时间表有所推迟。

我漫不经心地问:“你在哈希扫描吗？”

整个团队看着我，好像我有三个头。

起初，我怀疑自己是否不知何故与外界脱节了，因为这不是我的主要专业领域，而且我已经有一段时间没有做日常数据工作了。我脱离现实了吗？我是不是问了一个最愚蠢的问题，而没有人有足够的勇气告诉首席运营官，“嗯——当然。”

然后，一个勇敢的年轻人站了出来。“什么意思？”

我松了一口气，但也有点震惊。所以我回答说，“从你的数据中选取一个有代表性的子集，用密码散列它。然后使用生成的哈希进行过滤，而不是试图强行通过整个数据集。”

他们又一次看着我，好像我有三个脑袋一样，直到一个高年级学生说，“让我们试试。”

该站打破了，在几个小时内，他们有他们的结果数据集。

这项技术有明显的优点和缺点。在大型数据集上，哈希算法的计算开销可能很大，有时甚至是大材小用。然而，有时成本会被速度抵消。

几年前，我是一名全职法医和网络调查员。散列数据只是我 DNA 的一部分。我用它来验证重复的证据，以及识别要匹配的数据集——正面或负面的。

在此之前，我是一家小型开发公司的数据库管理员。不管出于什么原因，在我开始工作之前，他们就决定将表中的主键设为 GUIDs(全球唯一标识符)。由于当时是黑暗时代，在我确保我的恐龙被绑好以便它可以在回家的路上，我必须编写模块来确保 GUIDs 在我们的前端应用程序中正确生成。GUID 的构造(我在这里不严格地使用这个术语)本质上是唯一数据点的散列组合。

考虑到这两方面的经验，很自然地，散列可以用于在其他应用程序上提供匹配。

让我们深入了解一下我目前使用的一个用例。

我有一个从各种来源收集各种数据的应用程序。这些来源中的一些非常强大和可靠。其他的，就没那么多了。通常情况下，服务器会变得很忙，或者出现其他问题，导致数据无法正常获取。

现在，在设计这个系统的时候，我知道这些资源的局限性。我考虑在检索过程中创建某种形式的错误纠正，这将进行一轮验证，以确保数据是完整和正确的。但是，这看起来工作量很大。

因为我获取数据并将其放入数据存储，所以我有其他选择。

我查看了整个流程和 API 的功能。我有一个选项来扩展我的数据调用窗口，这样我就可以记录比我需要的更多的数据，并捕获以前尝试中可能遗漏的任何数据。

勾选了一个框。我知道我绝不会冒丢失数据历史的风险，因为丢失的连接可以在随后的尝试中弥补。

然而，我如何有效地将记录放入我的数据库，而不必在以后创建一个过程来清除重复的记录呢？我不介意插入，但删除往往需要清理周期表和膨胀的磁盘空间。这对于临时表之类的东西来说是可以的，但是对于在应用程序中经常使用的主表来说就不行了。

易于修复和快速解决。

我在主表中添加了一个字段，其中包含该行数据的散列。我的数据检索过程只是在数据下来时对其进行哈希处理。

然后，我创建了一个进程，在这个进程中，新检索到的数据首先被写入一个临时表。使用一个基本的查询，我可以轻松快速地识别出主表中没有的新行。这些新行被插入，临时表回到它原来的地方。

如果没有哈希，我能完成所有这些吗？绝对的。会不会更容易？也许吧。如果不经历一系列困难和编写更复杂的代码，我能完成这一切吗？大概不会。

前面的例子只是创建数据的数字指纹的许多有用方法之一。这很简单，也注定如此。但是还有很多其他更强大的应用程序。

什么是哈希？

您可以自由探索它的许多实现，但这里有一个简单的解释。这是一种算法，它检查作为输入提供的整块数据，并使用加密方法生成一组代表该数据的唯一位。

因为它是已知的算法，所以是可重复的。如果将 jpeg 或文本文档或硬盘图像输入算法，如果数据相同，输出也将相同。如果内部数据发生变化，输出也会发生变化。

但是碰撞呢？！？

对于那些没有完全了解这个主题的人来说，各种散列算法，特别是在受控环境中执行的精心制作的输入，会返回与不同数据段相同的输出。在某些应用中，这是一个需要考虑的因素，但这种风险非常小，在实际应用中几乎不值得一提。然而，当提到散列时，它似乎总是不时地出现，所以它值得在这里解决。

减轻这种担心很简单——使用不同的算法，输出更大的数据。使用 MD5，你把指纹压缩成 128 位。如果您使用 SHA256，该空间会急剧扩大。它会占用更多的空间，但如果这能让你和你的超能力者晚上睡得更好，那就值得了。

从数据科学的角度来看，哈希是帮助清理数据的一个非常好的工具。不仅仅是从数据库中。

假设你正在做一个大规模的图像分类项目。根据您的来源，获取一组清晰的独特图像可能是一项挑战。除非你想坐下来看你所有的图像，否则你会有一段艰难的时间，除非你使用哈希。

这是一个简单的功能，可以让每个人的生活稍微轻松一点。