# GitHub 和预训练模型:锁眼视图

> 原文：<https://towardsdatascience.com/github-pre-trained-model-a-keyhole-view-643c6526bc13?source=collection_archive---------44----------------------->

## GitHub 包含预先训练的模型，这些模型可能包含固有的偏见，模型记分卡的缺乏将有助于放大用户对这些回购的偏见。

**设置上下文**

GitHub 是一个著名的软件代码及其版本控制的互联网托管平台。GitHub 使其 5600 万用户(组织和个人)能够在平台上创建他们的作品的存储库，以便于访问、版本控制以及在有或没有许可的情况下共享(Apache License 2.0)。对于研究人员来说，GitHub 是一个指向他们工作的资源目录。此外，对于开源其工作/解决方案的大型组织来说，GitHub 使其可以被社区访问。

GitHub 上有超过 1.9 亿个公共知识库。这些存储库包含源代码、文档、API、数据集和其他与源代码相关的元数据信息。GitHub 允许用户派生(创建存储库的副本)、提交(更改历史)和发出拉请求(原始存储库的更改通知)。这使得能够重用平台上的现有内容。在机器学习的情况下，有超过 331，000 个储存库包含 ML 模型、它们的源数据、结果、度量和与它们相关的分析。

![](img/b499d8f76ddb560ce1a9447e2809b2ae.png)

来源:等距的人与技术自由矢量( [Freepik](https://www.freepik.com/free-vector/isometric-people-working-with-technology_5083799.htm#page=1&query=technology%20people&position=15) )工作

## 迁移学习和内在挑战

GitHub 仍然是支持迁移学习和利用现有模型或源代码的最大平台。迁移学习是利用从解决一个问题到解决另一个相似或不同问题的知识的过程。这有助于通过协作和开源有效地优化现有内容。虽然这种优化放大了可能性，但它也放大了源代码中的错误和偏见，这可能会伤害人和组织。

例如，GitGuardian 在其报告(2021 年 GitHub 上的秘密蔓延状态)中提到，包括 API 密钥、凭证和敏感信息在内的信息都在 GitHub 的公共存储库中。这使得存储库容易受到攻击(不仅是网络攻击，还有数据中毒攻击)，从而对组织和个人造成伤害([此处](https://www.securitymagazine.com/articles/94776-over-two-million-corporate-secrets-detected-on-public-github-in-2020))。

类似地，偏见可以存在于公共知识库上的自然语言处理模型([单词嵌入](https://www.capgemini.com/gb-en/2021/03/bias-in-nlp-models/))中的预先存在的偏见(潜在的或其他的)、技术偏见(特征选择和权衡决策)和紧急偏见(设计和基础事实之间的价值或知识库的不匹配)的性质中。这些模型是在公共数据集上训练的，也可能造成伤害。研究表明，模型或公共数据集中的潜在偏见导致了性别歧视(此处)。值得注意的是，数据科学家做出的优化选择和权衡决策也可能导致模型中的偏差。根据上下文和模型部署的方式，从原始存储库中派生或改编的模型也可能会导致或有时会放大危害的程度。

**底层根源**

潜在的问题是，GitHub 作为一个存储库包含的源代码或模型在被带到平台上之前没有经过验证。此外，GitHub 的方法是使社区能够在各自的存储库上互相审查、评论、使用或共享 bug 或挑战(称为“问题”)。然而，在社区中对潜在问题的认识不一致的情况下(特别是在新兴研究领域)，这种方法可能是不够的。此外，GitHub 并不强制要求在用户入职、声明流程或存储库创建流程中签署行为准则(仅作为建议)。

GitHub 社区指南提供了一个机会，通过表单提交流程，在审核存储库的同时报告滥用和报告内容。此类提交的当前选项包括“我要报告有害代码，如恶意软件、网络钓鱼或加密货币滥用”。然而，鉴于机器学习模型造成的危害的影响，有必要包括更多具体涉及偏见和歧视的选项。

这不是一个孤立的挑战。在使用 [Imagenet](https://paperswithcode.com/dataset/imagenet) 数据集(这里的、这里的和这里的)开发的图像识别模型中，也存在类似的偏见和歧视挑战。如果迁移学习是采用正在变得民主化的新兴技术的途径，那么有必要理解，在当前的形式下，这些民主化并非没有危害。

**需求:预培训模型的记分卡**

需要为 GitHub 上的 ***预训练模型*** 或代码建立记分卡。记分卡应包含关于模型或其基础训练数据是否经过偏见、网络安全或恶意攻击等测试的整理回答。在公开存储库之前，还应收集关于使用模型的限制或可以使用或不可以使用模型的环境、与使用此类模型相关的注意事项等信息。虽然这看起来对 GitHub 用户有很大的期望，但它不可避免地是相关的，并且旨在为社区和广大人民的福祉做出贡献。应该要求用户在用代码更新存储库时更新这些信息，并且应该通知用户从记分卡的角度查看存储库，反之亦然。

像 GPT-3 这样的储存库在储存库上包含特定的模型卡，包括对模型使用的期望、模型的限制和潜在偏差的可能性(此处[为](https://github.com/openai/gpt-3/blob/master/model-card.md))。这是最佳实践之一，因为您在 GitHub 上的许多其他公共存储库中找不到类似的参考资料。然而，即使这样的最佳实践披露也不够具体，不足以分享团队为验证模型免受偏见或敌对攻击所做的努力。

**结论**

鉴于 GitHub 的“Copilot”的宣布，这是至关重要的，Copilot 是一种人工智能工具，在平台上的数十亿行代码上进行训练。该工具有望为用户的项目生成补充代码。除了这个工具的合法性问题之外(这里是)，一个相关的问题是“你会舒服地使用一个有潜在偏见的代码或模型吗？”。记分卡不是目的，而是一种在机器学习中民主化迁移学习的同时带来上下文视图的手段。