# 使用 CNN 开始深度学习

> 原文：<https://towardsdatascience.com/getting-started-with-deep-learning-using-cnns-eaf220d8333d?source=collection_archive---------34----------------------->

## 实现“Hello World！”卷积神经网络

![](img/7bb46ce12898b1527ace6b4bec3de82f.png)

罗马法师在 [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral) 上拍摄的照片

近年来在深度学习方面取得的很大一部分进展是由于[卷积神经网络](https://en.wikipedia.org/wiki/Convolutional_neural_network)或 CNN 的概念。除了图像处理中最琐碎的任务之外，这些网络已经成为事实上的标准。CNN 的基本概念起源于 20 世纪 80 年代，首次应用于图像识别发表于 [1989](http://yann.lecun.com/exdb/publis/pdf/lecun-89e.pdf) 。像深度学习领域的许多主题一样，巨大的进步伴随着更多的计算能力，一个主要因素是从 2000 年代中期开始使用 GPU 而不是 CPU 进行训练。

我们将查看该领域最重要的论文之一，即 [1998](http://yann.lecun.com/exdb/publis/pdf/lecun-98.pdf) 论文“基于梯度的学习应用于文档识别”。由于种种原因，这篇论文在发表 20 多年后仍值得一读。值得注意的是包括 Yann LeCun 和 Joshua Bangio 在内的作者，他们与 Geoffrey Hinton 一起被认为是“深度学习的教父”，并因其在该领域的工作而共同获得了 2018 年图灵奖。

另一个问题是，这篇论文不纯粹是一项研究工作，但那里开发的解决方案已经在商业上应用于 NCR 公司的手写数字识别。由于该论文全面描述了专注于神经网络架构时的工作，我们只需阅读总共 10 节中的第 1 至第 3 节，整篇论文的长度为 46 页，这本身也是值得注意的，远远超过了通常的长度。

这里最相关的问题是，本文中讨论的 LeNet-5 网络通常被认为是 CNN 历史上最相关的网络之一。有时它被称为“你好，世界！”CNN 的。有一些论据支持这一点:

*   “LeNet-5 实现”在谷歌上给了你超过 130，000 次点击，其中很多都是相关的。
*   使用当前的库实现基本架构只需几行代码，而不是本文中描述的大量研究工作。
*   你会很快在 MNIST 数据集上得到很好的结果。这还不足以获得当今的最佳论文奖，但可能比从头开始要好得多。
*   根据所使用的超参数，每个时期网络的训练在 CPU 上将花费大约 100 秒，而在 GPU 上将花费仅仅几秒。这给了你合理的时间来试验网络。

另一方面，从论文中紧密地重新实现网络将是非常困难的，因为在实现的时候，作者必须手工做所有的事情。1998 年还没有 GPU，Python 不是人工智能的通用语言，也没有现成的深度学习库。在从头开始做每件事的同时，他们也做了许多优化，这些优化很难用最先进的库来重现。你会发现，使用谷歌的大多数实现只是让网络变得更大，从而在 MNIST 上快速获得类似的好结果。所以这不是简单的“你好，世界！”以获得可比较的结果，但它仍然是一个很好的入门 CNN 的基础。

# 编码入门

我们甚至不会试图准确地复制这篇论文，但与互联网上的大多数其他实现相反，我们将从一个至少在一些相关参数上遵循这篇论文的版本开始。代码将在 Python 和 Tensorflow 上使用 Keras。使用像 Keras 这样的高级库，网络的代码几乎是微不足道的，这也将使实验变得更容易。

该网络的基本结构在文件第二. b 节中有详细解释。它总共有七层。

*   一个卷积层，有 6 个 5×5 内核，带填充，因此我们实际上有填充的 32×32 图像作为输入，而不是原始的 28×28 MNIST 图像。
*   2x2 池层。这里复制报纸已经开始变得困难，因为你会意识到阅读报纸。为了简单起见，我们使用平均池。
*   具有 16 个 5x5 内核且没有填充的卷积层。这也是本文结构的简化。
*   另一个 2x2 池层使用平均池，再次不是真的。
*   一个有 120 个神经元的全连接层。
*   一个有 84 个神经元的全连接层。
*   一个 softmax 层，最终得到 10 个可能的输出类。虽然这也不同于纸张，但完全连接的层是真实的。

tanh 论文中使用的激活函数和损失函数类似于 MSE 或均方误差。两者都是为论文而优化的，因为它们必须手工编码，而不是像我们一样从高级库中使用它们。对于优化器，我们选择了简单的 SGD 或随机梯度下降。该网络经过 20 个时期的训练，学习率为“前两次为 0.0005，接下来三次为 0.0002，接下来四次为 0.00005，此后为 0.00001”，如论文中明确规定的。

在 Keras 中创建模型只需要几行代码:

```
model = keras.Sequential(
  [
    keras.Input(shape=input_shape),
    layers.Conv2D(6, kernel_size=(5, 5), padding=’same’, activation=’tanh’),
    layers.AveragePooling2D(pool_size=(2, 2)),
    layers.Conv2D(16, kernel_size=(5, 5), padding=’valid’, activation=”tanh”),
    layers.AveragePooling2D(pool_size=(2, 2)),
    layers.Flatten(),
    layers.Dense(120, activation=”tanh”),
    layers.Dense(84, activation=”tanh”),
    layers.Dense(num_classes, activation=”softmax”)
  ]
)
```

如果您查看模型摘要并将其与网络的详细描述进行比较，您会发现只有 C1 层、C5 层和 F6 层在可训练参数的数量方面与文章相符。这是因为我们没有尝试复制论文中所做的优化。

```
Model: “sequential”
_________________________________________________________________
Layer (type) Output Shape Param #
=================================================================
conv2d (Conv2D) (None, 28, 28, 6) 156
_________________________________________________________________
average_pooling2d (AveragePo (None, 14, 14, 6) 0
_________________________________________________________________
conv2d_1 (Conv2D) (None, 10, 10, 16) 2416
_________________________________________________________________
average_pooling2d_1 (Average (None, 5, 5, 16) 0
_________________________________________________________________
flatten (Flatten) (None, 400) 0
_________________________________________________________________
dense (Dense) (None, 120) 48120
_________________________________________________________________
dense_1 (Dense) (None, 84) 10164
_________________________________________________________________
dense_2 (Dense) (None, 10) 850
=================================================================
Total params: 61,706
Trainable params: 61,706
Non-trainable params: 0
```

# 培训和评分

复制纸张的重大挑战并不仅限于网络架构。下一个要回答的问题是如何构建培训过程。从积极的一面来看，本文中使用的 MNIST 数据集仍然是今天使用的 MNIST 数据集。第三部分有一段有趣而简短的 MNIST 历史。这篇论文很值得一读。

除了损失函数、优化器和学习率之外，我们还必须决定批量大小，以及在训练期间是否以及如何使用交叉验证。如果没有使用批次，即批次大小为 1，则论文中给出的低学习速率结合仅学习 20 个时期才有意义。请注意，只有在使用微批处理的情况下，使用 GPU 才能获得相关的加速，因此训练这些参数需要一些时间。

本文还探讨了训练数据量对网络性能的影响，并明确指出使用了来自 MNIST 的 60，000 幅训练图像。这些都是标准 MNIST 中可用的训练图像。因此，我们不使用交叉验证，即验证分割为 0。

这会产生以下用于训练网络的代码:

```
lr_list = [0.0005, 0.0005, 0.0002, 0.0002, 0.0002, 0.00005,
           0.00005, 0.00005, 0.00005, 0.00001, 0.00001, 0.00001,
           0.00001, 0.00001, 0.00001, 0.00001, 0.00001, 0.00001,
           0.00001, 0.00001]def calc_lr():
  elem = lr_list[0]
  del lr_list[0]
  return(elem)optimizer = keras.optimizers.SGD(learning_rate=calc_lr)model.compile(loss=”mean_squared_error”, optimizer=optimizer,
              metrics=[“accuracy”])model.fit(x_train, y_train, batch_size=1, epochs=20,
          validation_split=0.0)
```

训练网络输出训练集的精度，如果使用交叉验证，则输出验证集的精度。网络的唯一相关度量是测试集上的性能，因为这给出了网络在真实生活数据上如何表现的合理提示。在我的例子中，用这些参数训练网络的结果是训练数据的精度为 0.9257，测试数据的精度为 0.9292。这是一个很好的迹象，表明测试集的准确性更高，并暗示了网络的良好泛化能力。

但是总体来说 93%是一个好的准确率吗？实际上不是。本文在 C 节中比较了 MNIST 分类的几种技术和模型，其中提到的最差的一种具有大约 95%的准确度。本文的一个重要结果是 LeNet-5 在 MNIST 上获得了 99%或更高的准确率。因此，即使在 1998 年，我们要赢得最佳论文奖还有很长的路要走。

# 获得更好的分数

如前所述，论文中暗示了许多优化，试图复制它们可能至少需要一篇博士论文。所以我们不会走那条路。获得更好结果的更简单的方法是采用 LeNet-5 的基本结构，并在 CNN 中使用当前的最佳实践，而不是 20 多年前。另一个值得考虑的方法是扩大网络，因为使用现有的硬件训练只需要很少的时间。

为了看看什么是可能的，我们采取了一种捷径，使用网络上找到的许多实现方式中的一种，例如，这种方式恰好是我在谷歌搜索中的第一次点击:

[https://hackmd.io/@bouteille/S1WvJyqmI](https://hackmd.io/@bouteille/S1WvJyqmI)

代码比我们的代码稍微复杂一点，可能是因为使用了旧版本的 Keras。我们的选择没有太多不同，实际上网络看起来完全一样。潜在的相关差异有:

*   使用分类交叉熵作为损失函数，而不是均方误差。
*   使用默认的学习率 0.01，而不是纸上的学习率。
*   使用默认批量 32，而不是没有批量，即批量为 1。
*   交叉验证的使用。

在我的例子中，这些简单的调整极大地提高了训练数据和测试数据的准确度，分别为 0.9890 和 0.9870。回到批量大小 1，没有交叉验证，我得到的训练数据精度为 1.0000，这表明过度拟合。但是因为我在测试数据上也得到 0.9888，所以我并不在乎。所以仅仅通过改变损失函数和提高学习率，我们就离最佳论文奖更近了一点，如果我们在 1998 年才交论文的话。

# 摘要

[重新实现](https://www.kaggle.com/nop287/lenet-5-cnn)LeNet-5 的基本结构很容易，但你不会真的复制论文中的工作。然而，在 MNIST 数据集上，这个有着 20 多年历史的网络的性能仍然令人印象深刻。使用好的超参数至关重要！虽然使用交叉熵进行分类可能被认为是[的最佳实践](https://machinelearningmastery.com/how-to-choose-loss-functions-when-training-deep-learning-neural-networks/)，而不是使用 MSE 进行回归，但是还有更多选择需要决定。在 MNIST 上使用 CNNs】已经做得如此频繁，以至于最大的精确度是众所周知的。

一种替代方案是时尚 MNIST，它可以取代经典 MNIST，而不需要对网络进行任何改变，因为图像分辨率是相同的。它也有 10 个类，但目标是对时尚文章进行分类，而不是手写数字。为了进一步试验“你好，世界！”这是一个更有趣的数据集。