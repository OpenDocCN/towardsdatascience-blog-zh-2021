# 满足 M6 —以 1%的 GPT-3 能源成本提供 10 万亿个参数

> 原文：<https://towardsdatascience.com/meet-m6-10-trillion-parameters-at-1-gpt-3s-energy-cost-997092cbe5e8?source=collection_archive---------2----------------------->

## 较小的玩家现在可以进入大型人工智能模型的游戏

![](img/1ce730a5522b418786719573a5045af1.png)

[vs148](https://www.shutterstock.com/es/g/vs148) 在[快门架](https://www.shutterstock.com/es/image-vector/abstract-artificial-intelligence-technology-web-background-728178127)上拍摄的照片(已编辑)

我可以自信地说，当一个比另一个大 50 倍的神经网络可以用少 100 倍的能量成本训练时，人工智能正在快速发展——中间只有一年！

6 月 25 日，阿里巴巴 DAMO 研究院(阿里巴巴 R&D 分院)[宣布](https://www.infoq.cn/article/xIX9lekuuLcXewc5iphF)他们已经建立了 M6，一个大型多模态、多任务语言模型，拥有 1 万亿个参数——已经是 GPT-3 的 5 倍，是衡量大型人工智能模型进展速度的标准。该模型旨在实现多模态和多任务处理，比以前的模型向一般智能迈进了一步。

就能力而言，M6 类似于 GPT-3 和其他类似的型号，如[武道 2.0](/gpt-3-scared-you-meet-wu-dao-2-0-a-monster-of-1-75-trillion-parameters-832cd83db484) 或 [MT-NGL 530B](/yet-another-largest-neural-network-but-why-f48d231972a9) (我们对此了解甚少)。中国热门科技杂志【InfoQ 整理了 M6 的主要技能:“(它)拥有超越传统人工智能的认知和创造力，擅长绘画、写作、问答，在电子商务、制造业、文学艺术等诸多领域有着广阔的应用前景。”

然而，阿里巴巴研究人员强调的关键方面是效率和能源成本的显著改善。与 1 亿个语言模型相比，它们将模型的消耗减少了 80%，并提高了效率 x11。

符合[绿色 AI](https://arxiv.org/abs/1907.10597) 原则和目标的极其重要的新闻。

# 绿色人工智能去垄断大型语言模型

但是他们并没有就此止步，现在，5 个月后，他们刚刚实现了不止一个，而是两个新的里程碑:他们改进了 M6，使其成为第一个 10 万亿参数大型语言模型，是 GPT-3 的 50 倍。他们已经提高了之前的效率，将能耗降低到 GPT 3 号训练所需能耗的 1%。

他们仅用了 512 个 GPU 就在 10 天内训练了这个模型！

这些成就将对人工智能社区和世界产生深远的积极影响。

一方面，这是在大型人工智能模型的必要性和旨在减少碳足迹的清洁能源运动的要求之间找到共同点的一大飞跃。对大型语言模型的主要批评之一是，它们不能补偿它们产生的大量污染。据估计，训练一个大型人工智能模型(GPT 3 之前)的污染比一辆汽车一生中的污染多 5 倍——它们的用处并不明显。亚马逊和微软等科技公司已经提出了未来几年减少碳排放的计划，但都旨在通过冷却数据中心来解决这个问题，而阿里巴巴已经实现了更好的解决方案；减少训练模型所需的资源。

这还有一个重要的优势。如果阿里巴巴公布他们用来实现其结果的技术和方法，较小的参与者可能会与大型科技公司竞争，这些公司目前垄断了大型人工智能模型的超级利润领域。

研究、培训和推理的成本如此之高，以至于像谷歌这样的巨头都难以为这项技术提供资金。谷歌子公司 DeepMind 决定在创建 AlphaStar 时不调查某个关键组件的不同可能性，以避免超出预算。

OpenAI 可以使用微软提供的 10，000 Nvidia V100 超级计算机(尽管尚未披露他们使用的确切 GPU 数量)，在研究人员发现一个错误后，open ai 决定不重新训练 GPT-3，因为这是不可行的。一些粗略的计算估计[的培训成本至少为 460 万美元](https://lambdalabs.com/blog/demystifying-gpt-3/#:~:text=The%20cost%20of%20AI%20is%20increasing%20exponentially.%20Training%20GPT-3%20would%20cost%20over%20%244.6M%20using%20a%20Tesla%20V100%20cloud%20instance.)，这对于大多数公司来说是可望而不可及的——这还不包括研发成本，这将使这个数字上升到[的 1000-3000 万美元](https://bdtechtalks.com/2020/09/21/gpt-3-economy-business-model/#:~:text=This%20would%20put%20the%20cost%20of%20research%20and%20development%20between%20%2411.5%20million%20and%20%2427.6%20million%2C%20plus%20the%20overhead%20of%20parallel%20GPUs.)。

小公司如何与之竞争？

相比之下，最新版本的 M6 已经在 512 个 GPU 上训练了 10 天。(GPT-3 是在 V100 上训练的，但研究人员[计算出](https://arxiv.org/pdf/2104.04473.pdf)使用 A100s，在 34 天内训练该模型需要 1024 个 GPU。)

通过粗略计算，我们可以比较两种模型的培训成本。我假设阿里巴巴使用 Nvidia A100 和与 AWS 类似的 GPU 实例/小时成本，其中 8 个 Nvidia A100 AWS 实例的成本约为 20 美元/小时。假设他们使用了 512 个 GPU，那么就有 64 个 8-A100 实例。通过计算，我们得出总成本= 64 #实例 20 美元/小时 24 小时/天 10 天= 307，200 美元。

仍然有些昂贵，但远不及 OpenAI 训练 GPT-3 的花费。

# 未来的一线希望

在过去，我一直对大型语言模型非常不满，原因包括歧视和偏见、错误信息的能力、缺乏理解，甚至是因为[为什么我们甚至需要更多的大型语言模型？](/yet-another-largest-neural-network-but-why-f48d231972a9)还因为创建这些系统需要高昂的环境和财务成本。

但今天，我为阿里 DAMO 学院公布的结果鼓掌。

看起来他们致力于改善这种新的人工智能趋势带来的一些问题。仍有许多工作要做——其中一些问题是这些模型固有的，我们只能希望减轻它们——但看到大型科技公司致力于改善当前的景观，是人工智能近期未来的一线希望。

如果你喜欢这篇文章，可以考虑订阅我的免费周报<https://mindsoftomorrow.ck.page/>**！每周都有关于人工智能的新闻、研究和见解！**

**您也可以直接支持我的工作，使用我的推荐链接* [*这里*](https://albertoromgar.medium.com/membership) *成为中级会员，获得无限权限！:)**