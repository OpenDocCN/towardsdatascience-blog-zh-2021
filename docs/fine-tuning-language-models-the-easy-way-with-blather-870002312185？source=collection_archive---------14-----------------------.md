# 微调语言模型容易乱说

> 原文：<https://towardsdatascience.com/fine-tuning-language-models-the-easy-way-with-blather-870002312185?source=collection_archive---------14----------------------->

## 一个开源库，用几行代码训练和使用生成文本模型

![](img/487907277ca07d7bdbd110371a93d9ba.png)

赛义德·卡里米在 [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral) 上的照片

如果您想跳过这一步，只看如何微调您自己的数据集，您可以跳过本文的其余部分，只查看 colab 笔记本！

[](https://github.com/bigthonk/blather) [## GitHub-bigthonk/agw:在几行代码中训练和使用生成文本模型。

### 在几行代码中训练和使用生成文本模型。看看乱说在行动检查…

github.com](https://github.com/bigthonk/blather) 

做客座演讲是我最喜欢的事情之一；没有任何责任的教学乐趣。我在讲座中的目标是让学生对机器学习感到兴奋，让他们玩实际的例子，然后走开。

本周，我有机会与亚利桑那大学网络项目的一些学生谈论机器学习。在一项关于问题的课前调查中，班上学生确定的一个主要问题是高保真机器人的威胁，这些机器人能够影响社交媒体上的对话。

创建能够影响选举或传播虚假信息的僵尸网络是一项范围广泛得多的任务，无论是从传播此类信息的道德问题还是从复杂性来看，都不是一个班级能够完成的。作为简单的第一步，我们可以从讨论如何让一个机器人用合适的内容和风格来表达帖子开始。这个子任务可以通过使用微调的自然语言模型来完成。像 GPT 家族，XLNet，或者我们芝麻街的朋友，BERT 和 ELMo 这样的语言模型，对于大多数个人来说，都是超出从头训练范围的强大模型。令人欣慰的是，开源社区已经加快步伐，开发了预先训练的模型，允许我们这些无法访问重要数据和计算的人进行实验。

不幸的是，这些模型的直接应用将提供对我们的开源朋友训练它们的训练数据集有强烈偏见的解决方案。对于我们提出的僵尸网络应用程序，这将使我们很难获得与我们试图模仿的理想人格接近的帖子。为了克服这个缺点，我们采用了一种称为微调的方法。微调模型是指采用在大型数据集上训练的模型，并在具有更好的样式和内容适用性的较小数据集上对模型进行一些部分再训练。这种方法允许我们使用模型，否则由于缺乏数据或计算资源，我们很难进行计算训练。

在这里，我遇到了一个问题，我想向学生介绍一些自然语言模型的高级概念，并给他们一些工具来训练和试验他们自己的模型，而不会让他们厌烦或以压倒性的方式暴露所有的细节。如果有一个简单的库，允许我训练模型，然后使用它们生成文本，而不必了解表面下发生的许多事情，那该多好。

这让我开发了 agough，这是一个简单的抽象库，位于奇妙的 huggingface 库之上。[https://huggingface.co/](https://huggingface.co/)

![](img/567d5c108668ee38ad6e9b832d067335.png)

废话基本上就是这样，你的选择更少

ough 允许用户从文本文件中读取数据集，并训练模型(目前只支持 GPT-2，但我计划在未来添加对其他模型的支持)，然后用户可以根据需要从模型中生成样本或保存/加载模型。

## 第一步。安装乱说

```
!pip install blather
```

## 第二步。进口废话

```
from blather import Blather
```

## 第三步。创建一个乱说对象

```
blather = Blather()
```

## 第四步。阅读一些文本

```
blather.read("prince.txt")
```

这将产生一个训练时间估计，并最终产生一些训练统计。您可以选择传递一个具有所需周期数的历元参数，因为它默认为仅训练一个历元。

```
Training...
Estimated Training time of 3 minutes 
Running Validation...
  Validation Loss: 1.62[{'Training Loss': 2.586719648164395,
  'Valid. Loss': 1.6193444598011855,
  'epoch': 1}]
```

## 第五步。写一些文字

```
blather.write("The fool was known to blather about")
```

这将调用模型为我们的提示文本生成一个结尾，在这种情况下，我们的马基雅维利式机器人说…

```
The fool was known to blather about it at every time, and I never heard a word of it," said the pope to Olivertronicus in the year 1543
```

# 摘要

我写乱说是为了给那些对微调自己的语言模型感兴趣，但又不想太深入了解这样做的细节的人提供帮助。我希望它对一些人有帮助，或者至少有娱乐性。如果你对这个项目感兴趣，或者只是想谈谈数据科学或 ML，请联系我们。