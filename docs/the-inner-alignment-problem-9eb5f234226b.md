# 内部对齐问题

> 原文：<https://towardsdatascience.com/the-inner-alignment-problem-9eb5f234226b?source=collection_archive---------38----------------------->

## [播客](https://towardsdatascience.com/tagged/tds-podcast)

## 埃文·胡宾格谈建立安全和诚实的人工智能

[苹果](https://podcasts.apple.com/ca/podcast/towards-data-science/id1470952338?mt=2) | [谷歌](https://www.google.com/podcasts?feed=aHR0cHM6Ly9hbmNob3IuZm0vcy8zNmI0ODQ0L3BvZGNhc3QvcnNz) | [SPOTIFY](https://open.spotify.com/show/63diy2DtpHzQfeNVxAPZgU) | [其他](https://anchor.fm/towardsdatascience)

*编者按:这一集是我们关于数据科学和机器学习新兴问题的播客系列的一部分*，*由 Jeremie Harris 主持。除了主持播客，Jeremie 还帮助运营一家名为*[*sharpes minds*](http://sharpestminds.com)*的数据科学导师初创公司。*

你怎么知道一个超级智能的 AI 正在努力做你要求它做的事情？

事实证明，答案是:不容易。不幸的是，越来越多的人工智能安全研究人员警告说，如果我们想避免糟糕的结果，这是一个我们必须尽快解决的问题——这可能包括一场物种层面的灾难。

在人工智能安全的背景下，人工智能针对我们要求之外的事情进行优化的故障模式类型被称为*内部对齐*故障。这与外部对齐失败截然不同，外部对齐失败是指当你要求你的人工智能做一些结果证明是危险的事情时发生的情况，它只是在 2019 年被人工智能安全研究人员视为其自身的风险类别。领导这项工作的研究人员是我本期播客的嘉宾，埃文·胡宾格。

Evan 是一名人工智能安全资深人士，他在 OpenAI 等领先的人工智能实验室进行过研究，他的经验还包括在 Google、Ripple 和 Yelp 工作过。他目前在机器智能研究所(MIRI)担任研究员，并和我一起谈论他对人工智能安全、对齐问题以及人类是否有可能在超智能人工智能出现后生存下来的看法。

以下是我在对话中最喜欢的一些观点:

*   Evan 分享了他的内部对齐失败的玩具示例:他让我们想象一个解决迷宫的人工智能，它在迷宫数据集上训练，以到达给定迷宫的中心。在训练期间，中间的瓷砖被标上了绿色箭头，所以 AI 开发者不知道的是，AI 实际上最终认为它的目标是定位绿色箭头，而不是解决迷宫。因此，当人工智能被置于一个新的设置中时——绿色箭头被移动到一个完全不同的位置，与迷宫的解决方案无关，人工智能无法解决这个新问题。虽然这似乎是一个相当无害的问题，但不难想象在其他情况下，即使开发者认为人工智能正在优化的内容与它实际正在做的事情之间存在轻微的不匹配，也可能变得非常危险。
*   我们讨论了人工智能安全实际上是否需要更多的研究人员，或者更多的资金。虽然 Evan 认为它通常是这样的——而且有充分的理由认为这是任何人都可以解决的最重要的问题——但我们也讨论了一个小型、紧密聚焦、高信号、低噪音社区的价值。很明显，这是一种权衡:人工智能安全研究需要大量非常仔细的思考，如果该领域被兴趣淹没，并最终根深蒂固的学术激励导致复制危机和其他领域低质量研究的泛滥，这将变得更加困难。
*   我们从内在一致性的角度探索了许多过程——从进化到养育孩子，再到公司的形成，我发现这部分对话特别有趣。简而言之，宇宙“试图”将人类与基因繁殖的目标联系起来，但失败了(人类发明了避孕药具，并且没有把醒着的每一秒钟都用来研究如何生育)，父母试图将他们的孩子与他们的价值观联系起来，但失败了(孩子制定了自己的目标，并欺骗他们的父母试图实现这些目标)，社会试图将公司与生产商品的目标联系起来，这些商品为每个人的网络生活增加了更多的价值。 但失败了(公司追求自己的目标，并且很像孩子一样，发展出欺骗周围社会的激励机制，以实现利润最大化)。
*   埃文认为对齐是一个比智能更难解决的问题。因此，他对我们在建造超级智能系统之前就能找到完整的人工智能校准问题的解决方案持悲观态度。虽然他非常不确定——并强调这一点——但他认为，由于高级人工智能系统带来的风险，人类面临灭绝的可能性超过 50%。尽管如此，风险如此之高，以至于他认为尽管面临挑战，这个问题还是值得正面解决的。

你可以[在 Twitter 上关注埃文](https://twitter.com/evanhub?lang=en)，或者[我在这里](https://twitter.com/jeremiecharris)。

![](img/64e82760586f07e85f787f995b959586.png)

# 章节:

*   0:00 介绍
*   1:45 AGI 安全研究
*   8:20 人工智能的能力
*   14:40 内部对齐
*   25:12 拟人化这些人工智能系统
*   29:15 平淡
*   37:36 透过内在校准的透镜进化
*   49 时 06 分内部校准失败
*   54:32 内心校准和养育
*   1:00:35 初创公司和市场需求
*   1:03:30 对人工智能调整持乐观态度
*   1:09:20 总结