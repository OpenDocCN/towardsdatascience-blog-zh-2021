# 自我挑战的表示:一种有趣的鲁棒神经网络方法

> 原文：<https://towardsdatascience.com/representation-self-challenging-an-interesting-approach-towards-robust-neural-networks-5d2380fb32af?source=collection_archive---------15----------------------->

## **一种以模型为中心的方法，用于组装能够跨域泛化的鲁棒神经网络**

自从大约十年前深度学习的复兴以来，模型可解释性和跨领域泛化一直是学术研究和行业反思的重要组成部分，以努力生产和开发可靠和现成的人工智能系统。但是我们所说的领域泛化是什么意思呢？领域泛化指的是利用通过多个领域学习到的有用特征的概念，以便可以学习领域不可知的表示，该表示可以在超出分布的数据上相当好地执行。以这种方式训练的机器学习模型将对真实世界场景中猖獗的数据分布的变化更加鲁棒，因为它已经学习了领域的语义(例如，什么使马成为马)，而不是保留训练数据中存在的分布特定的相关性。

![](img/c45fda24e5b9e7bdf53db02b52e75357.png)

在多个领域上训练神经网络迫使模型从多个来源学习特征，从而产生在未知测试领域上推广良好的模型。作者图片

为了使神经网络在真实世界场景中更加健壮，已经提出了无数的命题和假设，但尽管如此，可解释性和因果性仍然是一个难以捉摸的谜。尽管我们还没有找到模型可解释性的圣杯，但一切还没有失去；我们有某些工具可以用来使我们的人工智能系统更加健壮，跨领域推广，更不容易失败。这些工具可以大致分为两大类，即:

1.  *以数据为中心的方法*
2.  *以模型为中心的方法*

尽管大多数关于一般化、可解释性、领域随机化和不变性的文献主要集中在以模型为中心的方法上，但我所学到的——及其起伏——是以数据为中心的方法是解决这些问题的更直观和可能更合理的解决方案。下面是一些推荐的文章，阐明了这些方法及其各自的优缺点:

*   [关于从数据而非模型的角度重新思考神经网络的概述](/from-model-centric-to-data-centric-artificial-intelligence-77e423f3f593?sk=aa4dbc09c4f3fedede22a272abbd1ed3)
*   [在训练数据集中引入不变性](/invariance-causality-and-robust-deeplearning-df8db9091627?sk=2fd260b5b6e782c711737526c9b88f94)
*   [域随机化概述](/domain-randomization-c7942ed66583?sk=a6f002676eea4128c88c11cecfb913cc)
*   [利用领域不变神经网络的数据扩充](/rethinking-data-augmentations-a-causal-perspective-e0b7810579a7?sk=2724a2c48bb227226b7328364bfadbb3)

然而，在某些情况下，无法利用以数据为中心的方法，即没有足够的数据来解决需要解决的问题，此外，无法以有效的方式生成有用的增强数据。在这些情况下，有几个以模型为中心的方法已经显示出前景，可以用来开发一个强大的深度学习模型。

## 以模型为中心的方法概述

我现在将简要地谈论一些重要的面向领域泛化的以模型为中心的方法。这个列表并不是详尽的，但是已经尽力给出了一个随着时间的推移证明是成功的方法的概述。我也会在概述之后推荐一篇我认为每个领域都值得了解的研究论文。

![](img/7760556d42bae462fb07aa66d89edbfa.png)

重要领域综合方法流程图。作者图片

*   表征学习:这种方法的目标是学习在处理多模态或领域时有用的共享表征，特别是在一个问题的训练数据稀缺而另一个不同但相似的问题的训练数据丰富的问题中。表征学习可以进一步细分为 2)领域对齐和 2)学习解开表征。这两个分支渴望实现相同的目标，但以不同的方式着手实现这一目标。领域对齐的中心思想是最小化用于学习领域不变特征的源领域分布之间的差异。另一方面，解开表征学习学习将输入映射到特征向量的表征，该特征向量包含关于影响变化的组件的信息。推荐:[*【MMD】*](https://openaccess.thecvf.com/content_cvpr_2018/papers/Li_Domain_Generalization_With_CVPR_2018_paper.pdf)
*   元学习:元学习是一种以数据和模型为中心的方法。这种方法在训练数据中引入了域转移，其思想是这将使模型能够在本质上具有不同域看不见域上很好地执行。这些方法将来自不同领域的数据分成元训练集和元测试集，以模拟真实世界场景中的领域转换。推荐: [*深度网络快速适配元学习*](https://arxiv.org/pdf/1703.03400.pdf)
*   集成学习:用于相同任务的多个模型在大多数不同的训练和测试分裂上被训练，并且集成技术(例如多数投票)被用于平衡来自所有模型的预测的使用。推荐: [*域自适应集成学习*](https://arxiv.org/pdf/2003.07325.pdf)
*   不变风险最小化:这些方法消除了在模型中引入数据特定偏差的虚假相关性，而不是学习跨领域的共享模态。这个想法是因果因子在不同的领域是不变的，如果我们去除虚假的相关性，我们就获得了对目标样本负责的因果因子。推荐: [*不变风险最小化*](https://arxiv.org/pdf/1907.02893.pdf)
*   迁移学习:在大型数据集上训练模型，并为下游任务利用有用的功能。推荐: [*自动综合到真实的概括*](https://arxiv.org/pdf/2007.06965.pdf)
*   训练启发法:这些方法试图迫使模型学习全局结构，例如数据中的形状，而不是专注于局部纹理。这将使模型能够放弃不同数据集之间的域转换，并在看不见的数据上执行得更好。推荐:除了阅读我们将在下面讨论的 RSC 论文之外， [*统一深度监督域适配和一般化*](https://arxiv.org/pdf/1709.10190.pdf)

在这篇文章中，从现在开始，我们将讨论一篇由卡内基梅隆大学的人们发表的学术论文，尽管它有着有趣的观点 [***“自我挑战提高跨领域概括能力”*** ，但相对来说它一直没有引起人们的注意。](https://www.ri.cmu.edu/wp-content/uploads/2020/07/RSC_ECCV2020.pdf)

> 深度学习模型从人类的神经病学中获得灵感，就像它们的灵感一样，它们是“善变、愚蠢的生物，记忆力差，具有自我毁灭的伟大天赋”——普鲁塔克。

本文试图通过提出一个非常有趣的想法来解决神经网络的反复无常。在本文中，我们将关注对这些想法及其未来前景的客观分析，而不是实现细节。但是，如果您对代码实现感兴趣，您可以阅读本文作者的代码:

[RSC 的 Github 回购](https://github.com/DeLightCMU/RSC)

# 自我挑战的表现:

大多数计算机视觉任务需要我们识别物体。对于我们将对象分类到不同类别的任务来说是这样的，比如将对象分类到 CIFAR-10 数据集中的 10 个类别中的任何一个。对于对象检测模型也是如此，其通常具有对象性得分，该得分给出了对象属于特定类别的置信度度量，如在 YOLO 的情况。但是是什么让一个物体成为一个物体呢？它的定义特征是什么？是什么使一个物体不同于另一个物体？在某种程度上，小汽车和公共汽车相似，它们都有四个轮子和相似的结构。那么，是什么让汽车不同于公共汽车呢？是相对大小还是别的什么？作为人类，我们如何区分马和驴？它们都有四条腿，一条尾巴，相似的大小和相似的脸，然而大多数人可以很容易地区分这两者，即使图像呈现的质量不规则。如果我们要区分马和驴，我们必须依赖马和驴的复杂特征组合，但我们不需要学习这些特征的功能来做真正好的分类。这里，**“协方差-偏移”**的概念开始发挥作用:

> 条件分布(即，猫的语义)在每个域中都是相同的，但是由于边缘分布的变化，模型可以学习其他东西(即，胖乎乎的脸)。— [自我挑战提高跨领域推广能力](https://www.ri.cmu.edu/wp-content/uploads/2020/07/RSC_ECCV2020.pdf)

![](img/52adea613ba4bfd7dcb0721fce4e0c7c.png)

由 [2 公牛摄影](https://unsplash.com/@2_bull_photography?utm_source=medium&utm_medium=referral)在 [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral) 上拍摄

![](img/b9be78b87ee57fefe83e5a1c2d83404d.png)

照片由[铁木尔·罗马诺夫](https://unsplash.com/@timromanov?utm_source=medium&utm_medium=referral)在 [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral) 上拍摄

这意味着在马的情况下，模型将关注虚假特征而不是因果特征，即马的语义，这可能由于较差的表示学习而导致将驴误分类为马。虽然我们人类也遭受着协变性的痛苦，但我们拥有无限的数据，学习的过程是一个持续的努力。一个从未见过驴的人很可能会把它误认为马，因为它具有马的大部分特征。但由于我们接触了如此多的物体，我们不知不觉地学会了区分两个相似的物体，而没有注意到细微的细节。曝光是一种有价值的资源，但不幸的是，这是大多数神经网络负担不起的奢侈品。

提出的解决方案是强迫模型学习需要识别的对象的启发法。为了解决这个问题，论文的作者认为，我们应该迫使网络屏蔽图像的主要表现形式，并专注于通过其他特征进行预测，就像人类在区分公共汽车和汽车、马和驴时需要专注于更细微的差异一样。就算法而言，这是如何工作的也是相当直观的:当反向传播完成时，占主导地位的特征将具有最高的梯度，如果我们将这些梯度归零，我们将无意中迫使网络在其他特征的基础上做出决定，这将增加模型的跨域泛化。

# 我们需要屏蔽多少功能:

表象自我挑战(RSC)通过学习图像的表象 ***z*** 来工作，然后在其迭代过程中执行以下步骤:

*   定位:该步骤计算网络上层相对于表示 ***z*** 的梯度

![](img/1a2aca2911fb8369794fbe9d4e84bea5.png)

上层相对于表示 z 的梯度，[源](https://www.ri.cmu.edu/wp-content/uploads/2020/07/RSC_ECCV2020.pdf)

然后如果该元素在梯度中元素的前 p 个百分比中，则创建值为 0 的屏蔽向量，否则将其设置为 1。

![](img/b380b4c29c2d2361a449bef88b4cbba4.png)

屏蔽向量第 I 个元素，[源](https://www.ri.cmu.edu/wp-content/uploads/2020/07/RSC_ECCV2020.pdf)

*   静音:RSC 屏蔽掉与较大梯度相关的相应位，即将它们设置为零

![](img/154fe130629a487fba4a4dbc524488d4.png)

屏蔽向量 **m** 和表示法 **z、**源的逐元素乘积

*   更新:计算结果表示的 Softmax

![](img/ea78f09a64cd63903f424dfe024461f2.png)

Softmax 计算，[来源](https://www.ri.cmu.edu/wp-content/uploads/2020/07/RSC_ECCV2020.pdf)

然后梯度的结果用于通过优化器更新模型。

![](img/cfd804dfae759075689b8ab68301981a.png)

将用于下一步更新的梯度计算，[来源](https://www.ri.cmu.edu/wp-content/uploads/2020/07/RSC_ECCV2020.pdf)

因此，在训练期间，RSC 迫使神经网络使用较少的主导特征，与以传统方式训练的模型相比，这反过来有助于网络利用更多的特征进行预测。

这就留下了一个值得思考的问题:在每次迭代中，有多少特性应该被屏蔽掉？这是一个没有唯一解决方案的问题，因此作为一个超参数 ***p*** 可以根据用户的需求通过两种指导直觉进行调整:

> ***p*** 越小，训练误差越小——[自我挑战提高跨领域推广](https://www.ri.cmu.edu/wp-content/uploads/2020/07/RSC_ECCV2020.pdf)

因为对于网络来说，在 i.i.d(独立&同分布)数据集中进行预测会更容易，在训练期间就是这种情况

> ***p*** 越大,(跨领域)泛化误差(即测试误差和训练误差之差)越小— [自我挑战提高跨领域泛化](https://www.ri.cmu.edu/wp-content/uploads/2020/07/RSC_ECCV2020.pdf)

因为我们正在迫使网络也利用其他不占优势的功能；从而提高 o.o.d(分布外)测试数据集的性能。

# 结论:

本文中提出的观点非常有趣，我认为应该付出更多的努力来探索这种特殊的领域泛化的途径。本文中描述的过程遵循一种直观的思维模式，并且很容易使该过程合理化。本文描述的实验结果令人印象深刻，并提供了一些关于未来探索途径的有趣见解。RSC 在 PACS、VLCs、Office-Home 和 ImageNet-Sketch 数据集上获得了最先进的领域综合结果。

![](img/2c9eb159d24b65012b4a193521404a5f.png)

PACS 上的领域概括结果。来自[来源](https://www.ri.cmu.edu/wp-content/uploads/2020/07/RSC_ECCV2020.pdf)

![](img/a19abeeec7032567e930076be23a0432.png)

VLCS 的领域综合结果。来自[来源](https://www.ri.cmu.edu/wp-content/uploads/2020/07/RSC_ECCV2020.pdf)

RSC 展示了减少属于相同体系结构但具有不同隐藏层数的网络之间的性能差距的能力，这在诸如 ResNet50、ResNet101 和 ResNet152 之类的更高容量主干网上的实验结果中是明显的。

> 实际的含义是，与增加模型大小相比，RSC 可能导致更快的性能饱和。因此，我们可以缩小网络规模，以同等性能部署— [自我挑战改善跨领域推广](https://www.ri.cmu.edu/wp-content/uploads/2020/07/RSC_ECCV2020.pdf)

这种差异对于需要在实时场景中执行推理的嵌入式设备尤其有利。从 ResNet101 迁移到 ResNet50 可以使推理过程中执行的操作数量减少 2 倍以上，从而缩短响应时间。

![](img/cdf23be57980635371fe3cd84557e748.png)

ImageNet 上的泛化结果。[来源](https://www.ri.cmu.edu/wp-content/uploads/2020/07/RSC_ECCV2020.pdf)

## 参考资料:

1.  黄等《自我挑战提高跨域泛化》，2020，[https://www . ri . CMU . edu/WP-content/uploads/2020/07/RSC _ ECC v2020 . pdf](https://www.ri.cmu.edu/wp-content/uploads/2020/07/RSC_ECCV2020.pdf)
2.  莫天等《统一深度监督领域适配与泛化》，2017，[https://arxiv.org/pdf/1709.10190.pdf](https://arxiv.org/pdf/1709.10190.pdf)
3.  Arjovsky 等人，“不变风险最小化”，2020 年，[https://arxiv.org/pdf/1907.02893.pdf](https://arxiv.org/pdf/1907.02893.pdf)
4.  李等《带对抗性特征学习的领域泛化》，2018，[https://open access . the CVF . com/content _ cvpr _ 2018/papers/Li _ Domain _ Generalization _ With _ 2018 _ paper . pdf](https://openaccess.thecvf.com/content_cvpr_2018/papers/Li_Domain_Generalization_With_CVPR_2018_paper.pdf)
5.  芬恩等，“面向深度网络快速适应的模型不可知元学习”，2017，[https://arxiv.org/pdf/1703.03400.pdf](https://arxiv.org/pdf/1703.03400.pdf)
6.  陈等.“综合到真实的自动综合”，2020，
7.  周等.“领域自适应集成学习”，2021，