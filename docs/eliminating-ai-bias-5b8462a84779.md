# 消除人工智能偏见

> 原文：<https://towardsdatascience.com/eliminating-ai-bias-5b8462a84779?source=collection_archive---------5----------------------->

## [公平和偏见](https://towardsdatascience.com/tagged/fairness-and-bias)

## 识别人工智能偏差并知道如何防止其在人工智能/人工智能管道中发生

![](img/ea207becaed28bf4339d3dc16ea9d1d3.png)

由 [Sushil Nash](https://unsplash.com/@sushilnash?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText) 在 [Unsplash](https://unsplash.com/s/photos/discrimination?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText) 上拍摄

# 人工智能偏见的证据

人工智能(AI)的主要目的是通过使用机器扫描大量数据的能力来检测潜在的模式和异常，以节省时间和提高效率，从而减少人工劳动。然而，人工智能算法也不能避免偏差。人工智能偏见以几种形式出现，下面重点介绍一些例子:

*   2018 年，亚马逊因对女性有偏见而停止使用其人工智能招聘工具。
*   2018 年 7 月，美国司法系统(如 COMPAS)使用的 AI 工具在用于预测累犯时，因显示种族偏见而受到审查。
*   2019 年 11 月，苹果信用卡因基于性别提供不同的信用额度而固有地对女性比对男性有偏见而受到指控。
*   2020 年，英国使用人工智能算法进行公共 welfare⁴.决策的委员会数量有所增加
*   最近，在 2021 年 9 月，哈佛商学院和埃森哲的一份报告发现，美国约有 2700 万工人被自动化技术从工作申请中过滤掉。这些工作人员包括护理人员和 immigrants⁵.

由于人工智能算法可能会对一个组织的声誉产生长期影响，并对公众产生严重后果，因此确保它们不会偏向人口中的特定子群体非常重要。通俗地说，人工智能算法中的算法偏差发生在结果缺乏公平性或由于具体的类别区分而偏向某个群体时，这些类别包括种族、年龄、性别、资格、残疾和地理位置。

# AI 偏差是如何发生的？

当在机器学习过程中对数据集或模型输出做出错误的假设时，就会发生人工智能偏差，这随后会导致不公平的结果。偏差可能发生在项目设计或数据收集过程中，产生的结果不公平地代表了人口。例如，脸书上发布的一项调查询问人们对维多利亚州新冠肺炎封锁的看法，发现 90%的维多利亚人害怕跨州和海外旅行，因为疫情。这种说法是有缺陷的，因为它基于仅访问社交媒体(即脸书)的个人，可能包括不在维多利亚州的用户，并且可能过度代表特定年龄组(即 20-30 岁之间的年龄超过 50 岁及以上的年龄)、种族或性别，这是由他们使用脸书决定的。

为了有效地识别人工智能偏见，我们需要在人工智能生命周期中寻找偏见的存在，如图 1 所示。

![](img/220111fd1c242880fd90dd9836715276.png)

*图 1:人工智能/人工智能管道的五个阶段和相关的人工智能偏差(*图片由作者提供)

# 人工智能/人工智能管道中人工智能偏差的来源

一个典型的 AI/ML 管道从一个业务问题开始，需要使用数据和分析来理解问题的驱动因素。这个业务问题通常被转换成一个或多个数据假设。然后设计一项研究，以确定需要收集的数据、执行数据收集、注释、准备和转换为可用于模型开发的格式的过程。

让我们来看看不同类型的偏置是如何在流水线的每个阶段引入的。

## 研究设计和假设制定

*抽样偏倚*发生在研究设计中只对人群的一个子集进行抽样，或者从人群中选择的样本是非随机的(见图 2)。这种类型的偏差通常在调查和在线民意测验中普遍存在，在这些调查和在线民意测验中，只对人口的一个子集进行抽样，因为他们可以访问民意测验或自愿完成调查或民意测验，从而导致自愿偏差，这是一种抽样偏差。

![](img/e5ed1cd7802d979a1d75b0f4ee5f3cce.png)

*图 2:抽样偏差的证据，其中只考虑了有权访问纸质调查的个人(*图片由作者提供)

## 数据收集、预处理和探索

一旦研究被设计，数据就被收集、预处理，并以概括数据集的初步图表的形式进行探索。

通常收集的数据是用于代表典型目标人群的样本。然而，由于*设计偏差*，由于数据集中的不平衡，样本可能无法代表总体。例如，谷歌搜索引擎中使用的机器学习模型很难区分人类图像和 gorillas⁶.图像这是因为数据集没有包括大猩猩、黑猩猩和猴子图像的公平表示。

当由于少数民族数据集的代表性不足而导致数据集中存在不平衡时，该算法在这种情况下对过度代表性的类(如熊猫和贵宾犬)表现良好，但对少数民族类(属于猴子家族的那些)表现不佳。由于预处理阶段使用的标签技术并不完善，这一问题变得更加突出。因此，这导致了数据集的不正确注释，这是一种形式的*标签偏差*，称为*回忆偏差。*

*测量偏差*是一种标签偏差，当训练数据集与真实数据集不同时，由于错误的测量导致数据失真，就会出现这种偏差。一个这样的例子发生在一个项目中，在该项目中，不同品牌的分光计被用于收集训练集和测试集中的植物样本的波长数据。这些光谱仪记录不同波长范围的数据。此外，由于电池电量低，用于训练集的老式光谱仪在捕获植物样品波长时不使用光源。因此，需要调整测试数据集来消除灯光效果。

*排除偏差*发生在数据清理步骤中，数据科学家可能会删除被认为不相关的特征。例如，超市的数据集由澳大利亚和新西兰的销售数据组成。在数据探索阶段，发现 95%的客户来自澳大利亚。考虑到位置字段是不相关的，因此从数据集中删除了该字段。这意味着该模型可能不会考虑澳大利亚和新西兰客户之间的差异，例如后者在网上购物上花费更多。

# 模型开发

在开发模型之前，数据集通常分为训练集、验证集和测试集。这可能导致*时间间隔偏差*，数据科学家有意选择一个特定的时间框架来支持假设。一个例子是形成一个结论，即某一特定的泳装系列是有利可图的，因为只考虑夏季月份的销售。

![](img/4c6d31476f8821e6996084637224b1fb.png)

*图 3:时间间隔偏差的证据，在评估泳装系列的利润时，仅考虑澳大利亚的夏季月份*

*存活偏差*是在模型开发阶段出现的另一种偏差，此时数据科学家只包括在选择过程中“存活”下来的数据。一个很好的例子是，二战期间从事海军分析的研究人员被要求找出军方战斗机中最薄弱的地方。为了回答这个问题，研究人员只检查了从战斗任务中返回的飞机，寻找子弹穿透飞机的点(*见图 4* )。利用这些信息，他们提出了加强战斗机上这些精确点的建议。这一分析的问题是，样本排除了没有从战斗任务中返回的飞机。这些飞机会比返回的飞机提供更多有用的信息，因为它们遭受了大面积的 damage⁷.

![](img/c83bc31aec5be2eca3946721277c7a20.png)

*图 4:幸存者偏差的证据，二战期间，只有返回的战斗机被用来分析飞机上最薄弱点的信息(图片由作者提供)*

*遗漏变量偏差*发生在数据科学家从他们的模型中排除一个或多个特征时。这通常发生在预测机器学习模型中。一个例子是开发了一个模型来预测应该给小麦、大麦和油菜作物施用多少氮肥。当开发该模型的训练集时，仅考虑诸如植物类型、植物种植日期、测量植物产量的日期以及植物发射的紫外光波长之类的特征来训练机器学习算法。已知在大多数监督学习问题上表现良好的 *xgboost* 模型(梯度增强的树回归模型)产生了模型准确性的负 R 平方(拟合优度统计)值。这里的问题是模型只考虑了涉众想要包含的特性。在对哪些其他特征可以使模型变得更好进行了一些头脑风暴之后，降雨量、阳光量、风速、气温和土壤含量等附加特征被添加到了模型中。这导致了模型精度的显著提高。

*混杂偏倚*的出现是因为模型中存在混杂因素，即与反应变量和预测变量相关的变量。混杂因素会显著影响模型的准确性。例如，在线性回归模型中，混杂因素会改变回归线的斜率和方向。

## 模型解释和交流

*确认偏差*或观察者偏差发生在模型解释阶段，在这个阶段，数据科学家解释或寻找与他们的信念一致的信息。例如，在模型中，数据科学家可能会为他们认为是更好的预测者的特征分配更高的权重，而不是将权重建立在模型结果的基础上，或者他们甚至会排除与他们关于某些特征的假设不一致的数据。

当一个模型的结果在某种程度上偏向于支持一个项目的财务发起人时，就会出现资金偏向。是*观察者偏向*的一种，就是倾向于看到观察者想看到的东西。

当向业务利益相关者传达机器学习模型时，这是非常常见的情况，这些利益相关者有时不愿意接受模型的结果，因为它不符合他们的期望。例如，客户流失模型的结果表明，最近的营销活动未能有效防止客户离开保险公司，因为它瞄准了错误的客户群体。这个分析没有得到市场部主管的好评。由于客户流失模型的结果不支持营销团队的资金计划，数据科学家被要求尝试不同的技术并探索其他功能，直到实现“可接受的结果”。

*因果偏差*是数据分析中最常见的偏差类型。这就是相关性被误认为因果关系的时候。一个流行的例子是关于 20 世纪 80 年代的一位学者，他研究了纽约市的犯罪率，发现街头小贩出售的冰淇淋数量和犯罪率之间有很强的相关性。如果得出吃冰淇淋会导致犯罪率上升的结论，那就大错特错了。相反，犯罪率在夏季较高更合理，因为这是冰淇淋销售最高的季节。然而，冰淇淋的销售并没有导致 crime⁸.的增长

## 模型验证、测试和监控

在模型验证和测试步骤中，对模型输出进行分析。当模型中没有足够的特征时，模型欠拟合就会发生，因此在训练集上表现很差。这些模型具有较小的*统计方差*(即，模型对特定数据集的敏感性)和较高的*统计偏差*(即，模型学习数据集的真实底层模式的灵活性有限)。回归、朴素贝叶斯、线性和参数算法容易出现欠拟合。

过度拟合的罪魁祸首是基于树的模型、最近邻算法、非线性模型和非参数模型。当模型可以准确预测训练集中的几乎所有值时，就会发生过度拟合。然而，当涉及到对测试集的预测时，模型就失败了。这表明该模型不能一般化，并且随着新的观察值被添加到数据集，它很可能失败，表明它具有高统计方差但低统计偏差。

*数据泄漏*发生在训练和测试数据集之间共享信息时。通常，当样本数据集被分为训练和测试时，两者之间不应共享任何数据。例如，预测模型使用历史数据(从 2005 年 1 月到 2021 年 7 月)来预测 2021 年 8 月到 10 月的产品销售。然而，如果训练数据集包括来自用于预测的时间段的观察值，则已经发生了时间数据泄漏。这种类型的偏差在模型验证阶段很容易识别，因为模型可能会输出非常高的模型准确度分数。

# 如何防止 AI 偏见？

那么，对于 AI 偏见能做些什么呢？以下部分详细介绍了如何在 AI/ML 流水线的每个阶段防止 AI 偏差。图 5 总结了这些信息。

![](img/a60228fc261a0b96f5d29510c58ff5b6.png)

*图 5:AI/ML 管道每个阶段的 AI 偏差缓解策略(图片由作者提供)*

## 研究设计和假设制定

当决定在您的研究中包括哪些数据和特征时，请关注研究的设计。目的使样本数据集能够代表目标人群。例如，在墨尔本大都市开展调查以衡量 UberEats 的满意度时，确保考虑到不同年龄组、不同性别、不同文化、语言和教育背景的个人，墨尔本大都市的所有邮政编码，以及回复电子邮件、邮件和社交媒体调查的客户。该调查还应在一天的不同时间和一周的不同日子在线进行，以实现包容性。此外，在一年中多次进行调查是一个好主意，因为客户满意度可能会在一年中发生变化。

下一步是从回答者样本中选择一个随机且有代表性的数据集，以确保每个回答者在研究中被选中的机会均等。这种类型的研究设计消除了*抽样、自愿和时间间隔偏差*。

## 数据收集、预处理和探索

如果基本的统计假设得到满足，则可以依赖机器学习算法的输出。对于很多算法，中心极限定理假设要求样本和总体是正态分布的。通常可以用足够大的样本量或变量变换来证实这一假设。因此，如果在数据收集步骤之后，发现样本量太小，建议数据科学家尽可能增加样本量。

为了确定样本大小是否足够大，可以进行*功效分析*，其提供满足预定统计显著性水平所需的最小样本大小。

在数据预处理步骤中，记录所有数据清理和转换步骤非常重要，这将有助于确定偏差的来源，例如排除某些特征(*排除偏差*)和不正确的数据标记(*标记偏差*)。

排除偏差也可以通过在丢弃它们之前调查每个特征来防止。这可以通过能够识别冗余特征的主题专家(SME)的帮助或者通过使用机器学习方法来完成，例如输出可变重要性列表的随机森林。

*测量偏差*可以通过检查异常值来减少，异常值是异常小或异常大的值，与平均值相差很大，并使用库克距离或马哈拉诺比斯距离等方法计算它们对结果变量的影响程度。

在数据探索阶段，数据科学家可能会发现某项调查中的受访者样本不平衡，因为女性受访者多于男性，从而导致*设计偏差。*可以通过考虑下采样或过采样方法来平衡数据集。自动做到这一点的 *python* 包的例子是 SMOTE for over-sampling 或 imblearn。

在数据探索步骤中计算模型中包含的所有变量之间的成对相关性是一种很好的做法，这是一种用于识别多重共线性的统计技术。这将有助于识别导致*混杂偏倚的混杂变量。*可以设置相关性阈值，以确定从模型中排除哪些特征。

当发现混杂因素时，最好与企业的 SME 联系，以确定这些变量是否应该包含在模型中。避免混淆的一个例子是在开发预测模型的过程中，该模型使用 LNG(液化天然气)工厂中的传感器数据来预测 LNG 产量。在最初的几次模型运行中，模型产生了非常高的模型准确度分数。在模型演示过程中查看主要预测变量后，生产工程师发现了一个混杂变量。液化天然气工厂中的这一变量在液化天然气生产后会发生变化，因此是一个代理因变量。通过从模型中移除这一变量，模型能够生成更可靠的结果。

去除*标签偏差*可能涉及一些深入的数据探索。例如，几个大学生被用来给植物样品贴上标签，要么是小麦，要么是大麦，要么是油菜。预测模型输出生成了非常低的模型准确度分数。对模型中输入的所有特征进行无监督聚类(k-means 聚类),以确定该算法是否会将小麦、大麦和油菜样本作为单独的聚类进行聚类。模型输出准确地对大部分观察值进行了分类，但是有一些观察值出现在错误的聚类中。这归因于模型中输入的一个特征。该特征和属于特定类的概率之间的关系用于重新标注数据集。

# 模型开发

在开发良好的模型时，要素选择是关键，因为从数据集中过滤掉不相关或冗余的要素非常重要。这可以防止*遗漏变量偏差*的发生。一些监督算法具有内置的特征选择，例如正则化回归和随机森林。

如果没有内置的特征选择(例如最近邻)，数据科学家可以使用*方差阈值*。值保持不变的特征应该从模型中排除，因为它们没有足够的方差来解释结果变量。例如，95%的值为 1 的连续变量不太可能有用。同样，所有观察值都属于一个类别的分类变量不会为模型提供信息。

当数据集具有大量特征时，可使用*主成分分析(PCA)* 将特征数量减少到主成分(特征的线性组合)。但是，这会改变输出要素的比例。主成分也很难解释。

另一种选择是遗传算法(GAs ),它是一种搜索算法，使用变异和交叉(生物学和自然选择原理)来有效地遍历大型解决方案空间；从而有助于从非常高维的数据集中选择特征。与 PCA 不同，GAs 保留了输出特征。

## 模型解释和交流

当解释一个模型时，对于数据科学家来说，查看从一个模型中得到的所有信息并真实地呈现这些信息是很重要的，即使这些信息可能与业务假设不一致。同样，当交流模型输出时，与涉众进行基于事实的对话是很重要的。

为了确保受众能够理解和信任模型输出，而不是将其视为黑盒，数据科学家应该使用能够提高透明度和模型可解释性的技术。

可以使用*全局或局部 explainability⁹.来解释模型*全局可解释性显示了模型的高级视图，以及数据中的特性如何共同影响结果。一个例子是使用多元线性回归模型的系数来解释特征的大小和方向，以解释结果变量的可变性。局部可解释性用于一次使用一个特征来解释数据集中的每个观察值。

*部分相关图(PDP)*提供了一个或两个特征如何影响模型预测值的全局可视化表示，同时保持其他特征不变。这些图有助于确定结果变量和所选特征之间的关系是线性的还是复杂的。PDP 与模型无关。

*个体条件期望(ICE)图*提供了模型特征相对于结果特征的个体效应的局部显示。与 PDP 不同，ICE 图通过提供样本内单个观察值的可视化表示，用于显示结果变量值对特征值的依赖性的单独预测。像 PDP 一样，ICE 图也是与模型无关的。

*累积局部效应(ALE)* 图是另一种用于局部解释的图。与 PDP 不同，该方法关注每个特性的小范围值，改变该特性的值，同时保持所有其他特性不变。计算该范围开始和结束时的预测差异。ALE 图是 PDP 的一种更快、偏差更小的替代方法，因为它关注的是单个预测，而不是 PDP 中使用的平均预测。

另一种方法是*保留一列(LOCO)* ，它在排除一列后重新训练一个模型。然后，它计算每个 LOCO 模型预测得分与原始模型预测得分的差异。如果分数变化很大，则表明对模型很重要的一个特性被遗漏了。

最后，LIME(局部可解释模型不可知解释)是另一种局部可解释性技术，它对一个特性的值执行多重扰动，并测量对输出预测的最终影响。

还有其他一些技术有助于模型的可解释性，比如 Shapley 值、Anchors、DeepLift 和 ProfWeight。高模型可解释性有助于模型输出的解释和交流，从而导致模型透明和信任。

当利益相关者接受了常见机器学习技术及其缺陷的教育后，他们更有可能以批判的方式看待模型输出。为了教育利益相关者，需要在组织内进行变革管理，鼓励企业内的所有员工，从研究生分析师到首席执行官，参加在线学习模块和研讨会，帮助他们进行模型输出评估。

## 模型验证、测试和监控

在验证和测试模型时，可能会观察到过度拟合。它可以通过在训练集上的非常高的拟合优度统计来识别，但是在测试集上的值很低。通过对训练集和测试集的较差拟合度统计，可以发现拟合不足。

防止过度拟合的一个强有力的预防措施是*交叉验证*，其中初始训练数据用于生成多个训练测试分裂，然后用于调整模型(即 *k 倍交叉验证*)。增加样本的大小也有助于将模型的预测推广到现实世界。

用于简化模型的技术被归类为*规范化*。所使用的方法取决于所使用的算法。修剪决策树是降低复杂度和防止过度拟合的一种方法。*惩罚参数*可以添加到回归中的成本函数，以缩小系数的值，有助于防止过拟合。稀释是在神经网络的训练过程中随机排除单元(隐藏的和可见的)的过程，也是已知的防止过度拟合的过程。*提前停止*是防止模型过度拟合的另一种方法，其中模型在运行一定次数后停止，因为额外的迭代不会提高模型精度。这些技术通常作为模型开发中的参数，可以进行超调。

*集成学习*是一种结合多个模型预测的技术。两种最常见的技术是 bagging，它试图通过并行训练许多“强”学习者(高模型精度模型)并组合他们的预测以提供集合模型的最终预测来减少过拟合。Boosting 旨在通过依次训练大量“弱”学习器来提高简单模型(即，在欠拟合是一个问题的情况下)的预测灵活性，其中弱学习器被定义为受约束的模型(即，在基于树的分类器中树的深度是有限的)。每一个连续的模型都从上一个模型的错误中学习。最后一步是将所有弱学习者合并成一个强学习者。

模型建立后，需要持续进行*监控*，以确保它们对于正在建模的数据集仍然有效。例如，随着时间的推移，企业可能会停止捕获一些要素或在数据集中包含新的要素。此外，特征的分布可能改变，这将阻止模型预测超出预定值范围的结果。所有这些实例都需要构建新的模型来替换现有的模型，现有的模型在处理任何新数据时都会表现不佳。

# 有哪些防止 AI 偏差的工具？

有几个现有的工具，包括 Python 包，可以帮助人工智能偏见预防⁰.

Google tensor flow 团队开发的假设工具(WIT)是一个可视化的交互式界面，可以显示数据集和模型。用户无需编写代码即可探索模型结果，并可查看 PDP 等图。

FairML 是另一个有用的工具，它通过量化模型对输入特征的相对预测依赖性来审计预测建模中的各种类型的偏差。然后利用模型特征的相对重要性来评估模型的歧视程度或公平性。

IBM 的 AI Fairness 360 包含一套全面的指标、指标解释和算法，以减轻数据集和模型中的偏差。

微软的 Fairlearn 确定了以分配损害和服务质量损害的形式影响人们的模型偏差。当人工智能系统扩展或保留信息、机会或资源时，就会发生先验。一些用例出现在招聘、学校或大学录取以及银行贷款中。后者指的是一个系统是否对所有人都同样有效，即使没有信息、机会或资源被保留或扩展。这个工具可以用来减轻⁴.的偏见和种族歧视

最后，Aequitas 是另一个著名的开源偏见审计工具包，可用于审计机器学习模型的歧视和偏见。

除了工具包，还有各种用于 AI 偏差检测的 Python 库。在检测人工智能偏差时，特定于算法的 Python 库有公平分类⁵、公平回归⁶和可扩展公平聚类⁷.还有一些 python 包，如偏差检测器⁸和性别偏差⁹，用于检测模型中的性别、民族和种族偏差，这些模型可能由于样本不平衡和缺少目标人群的代表性而存在。

# 结论

这篇博文指出了 AI/ML 管道中可能出现的几种类型的偏差，以及可以用来减少或消除它们的措施。有现成的工具包可以帮助减轻人工智能偏差，而不是重新发明轮子。然而，重要的是，整个企业的个人，从研究生雇员到 C 级高管，都要接受严格评估模型输出的培训，以使他们能够理解 AI/ML 算法，并质疑它们的有效性。

## 参考

[1] Dastin，J. (2018)，" Amazon scraps secret AI recruiting tool that showed-bias-against women "，[https://www . Reuters . com/article/us-Amazon-com-jobs-automation-insight/Amazon-scraps-secret-AI-recruiting-tool-that-showed-bias-against-women-iduscn1 MK 08g](https://www.reuters.com/article/us-amazon-com-jobs-automation-insight/amazon-scraps-secret-ai-recruiting-tool-that-showed-bias-against-women-idUSKCN1MK08G)

[2] Thomas (2019)，“我们如何从 AI 算法中消除偏见？笔测试宣言”，[https://fastdatascience . com/how-can-we-elite-bias-from-ai-algorithms-the-pen-testing-manifesto/](https://fastdatascience.com/how-can-we-eliminate-bias-from-ai-algorithms-the-pen-testing-manifesto/)

[3] BBC 新闻(2019)，“苹果'性别歧视'信用卡被美国监管机构调查”，【https://www.bbc.com/news/business-50365609 

[4]《卫报》(2020)，“英国近一半的议会使用算法来帮助做出索赔决定”，[https://www . The Guardian . com/society/2020/oct/28/英国近一半的议会使用算法来帮助做出索赔决定](https://www.theguardian.com/society/2020/oct/28/nearly-half-of-councils-in-great-britain-use-algorithms-to-help-make-claims-decisions)

[5] Jones，S (2021)，“自动化招聘系统正在对招聘人员‘隐藏’候选人——我们如何才能阻止这种情况？”，[https://www . weforum . org/agenda/2021/09/人工智能-工作-招聘-流程/](https://www.weforum.org/agenda/2021/09/artificial-intelligence-job-recruitment-process/)

[6] Simonite，T (2018)，“当涉及到大猩猩时，谷歌照片仍然是盲目的”，[https://www . wired . com/story/When-It-to-Gorillas-Google-Photos-Remains-Blind/](https://www.wired.com/story/when-it-comes-to-gorillas-google-photos-remains-blind/)

[7] Agarwal，R (2020)，"数据科学中的五种认知偏见(以及如何避免这些偏见)"，[https://towardsdatascience . com/Five-Cognitive-bias-In-Data-Science-And-how-to-avoid-them-2bf 17459 b041](/five-cognitive-biases-in-data-science-and-how-to-avoid-them-2bf17459b041)

[8] Agarwal，R (2020)，"数据科学中的五种认知偏差以及如何避免它们"，[https://towards data science . com/Five-cognitive-bias-in-data-science-and-how-to-avoid-them-2bf 17459 b041](/five-cognitive-biases-in-data-science-and-how-to-avoid-them-2bf17459b041)

[9] Onose (2021)，“ML 中的可解释性和可审计性:定义、技术和工具”，[https://Neptune . ai/blog/explability-Auditability-ML-Definitions-Techniques-Tools](https://neptune.ai/blog/explainability-auditability-ml-definitions-techniques-tools)

[10]“每个数据科学家都应该知道的最基本的 Python 公平库”，[https://techairesearch . com/Most-Essential-Python-Fairness-Libraries-Every-Data-Scientist-Should-Know/](https://techairesearch.com/most-essential-python-fairness-libraries-every-data-scientist-should-know/)

[11] [假设分析工具](https://pair-code.github.io/what-if-tool/)，[https://pair-code.github.io/what-if-tool/](https://pair-code.github.io/what-if-tool/)

[12] Adebayo，Julius，“FairML:预测建模偏差诊断工具箱”，[https://dspace.mit.edu/handle/1721.1/108212](https://dspace.mit.edu/handle/1721.1/108212)

[13]艾公平 360，

[14]费尔勒恩，[https://github.com/fairlearn/fairlearn](https://github.com/fairlearn/fairlearn)

[15]公平分类，[https://github.com/mbilalzafar/fair-classification](https://github.com/mbilalzafar/fair-classification)

[16]公平回归，[https://github.com/jkomiyama/fairregresion](https://github.com/jkomiyama/fairregresion)

[17]可扩展公平聚类，[https://github.com/talwagner/fair_clustering](https://github.com/talwagner/fair_clustering)

[18]https://pypi.org/project/bias-detector/[偏差检测器](https://pypi.org/project/bias-detector/)

[19]性别偏见，【https://github.com/gender-bias/gender-bias】T4