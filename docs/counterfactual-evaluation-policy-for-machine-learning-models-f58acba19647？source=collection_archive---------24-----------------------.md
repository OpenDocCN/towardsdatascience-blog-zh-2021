# 机器学习模型的反事实评估策略

> 原文：<https://towardsdatascience.com/counterfactual-evaluation-policy-for-machine-learning-models-f58acba19647?source=collection_archive---------24----------------------->

## 如何监控其行为阻止我们观察地面真相的模型？

监控任何系统的目标都是跟踪其健康状况。在机器学习的背景下，跟踪我们在生产中服务的模型的性能是至关重要的。当我们的模型不再新鲜，需要重新训练模型时，它可以帮助我们通知。它还可以帮助我们在欺诈检测等案件中检测滥用行为，在这些案件中，可能有敌对行为者试图损害模型。

![](img/e914ed1ec9327dfa5ca9347641bfc7c3.png)

卢克·切瑟在 [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral) 上的照片

# 缺乏基本事实标签

为了监控模型的性能，我们需要将它们的预测与真实标签进行比较。然而，一个模特的行为往往会阻止我们观察地面的真相。

要了解原因，请考虑信用卡欺诈检测模型的示例。我们将使用它作为本文的运行示例。它预测每一笔交易要么是欺诈，要么不是欺诈。应用程序代码现在通过阻止预测的欺诈交易来对这些预测进行操作。一旦事务被阻塞，我们就无法观察如果让它通过会发生什么。这意味着我们无法知道被阻止的交易实际上是否是欺诈性的。我们只能观察和标记我们允许通过的交易。换句话说，我们只能观察到非阻塞事务的偏态分布。

# 问题

我们模型预测的一部分缺乏基础事实标签，这引入了两个主要问题:

1.  如何在生产中持续监控模型的健康——也就是说，检查模型的精度和召回率等指标？
2.  如何才能做好模型的后续再训练？在部署模型的第一个版本之前，我们有一个代表真实世界分布的数据集。然而，在未来，有可能只训练那些被允许的例子，因为那些例子有一个基本的事实标签。这意味着我们只能在一个偏斜的分布上训练，这个分布不同于每次再训练的真实分布。随着时间的推移，这可能会导致模型的性能逐渐下降。

为了评估和重新训练我们的模型，我们希望在没有我们干预的情况下，样本和它们的标签有一个近似的分布。

# 目标

让我们定义我们的目标—我们需要一项政策，让我们:

1.  评估模型在生产中的性能
2.  为将来的再培训生成公正的培训数据

# 反事实评估政策

为了实现我们的两个目标，我们让一小部分本来会被阻塞的事务通过审查。我们姑且称这个分数为`P(allow)`。

回到我们的欺诈检测示例，这意味着允许一小部分预测的欺诈交易通过。这些允许的交易可能有实际成本，但这是在生产中拥有健康模型所要付出的代价，并且这种做法在行业中被广泛采用。

在缺乏反事实评估策略的情况下，我们的模型逻辑可能如下所示。当模型得分大于阈值时，模型预测该交易是欺诈，我们决定阻止它。

```
**if modelScore > 0.5: 
  block()
else:
  allow()**
```

这是用`P(allow) = 0.1`修改后的逻辑。我们允许 10%的交易，否则我们会阻止。对于这 10%的交易，我们能够观察到真实的标签。

```
**if modelScore > 0.5: 
  if random.random() < 0.1:
    allow() 
  else:
    block()
else:
  allow()**
```

# 获得无偏估计

让我们举一个例子，看看我们如何计算精度和召回率。这里`P(allow) = 0.1`。在 1000 个例子中，该模型预测 100 个例子是欺诈。因为我们的反事实评估政策，我们还是让 10 个通过了。通过观察这些交易上的真实标签，其中 8 个被证明是真正的欺诈。

![](img/612b82f6511d126d096d10dc2d8ccce1.png)

一个示例场景。图片作者。

## 精确

使用这种方法，只需使用 review 事务就可以直接计算精度。精度只是我们的审查交易中实际上是欺诈的一部分。

**精度** =审查欺诈/审查交易= 8 / 10 = 80%

## 回忆

要计算召回率，首先需要估计欺诈交易的总量，包括模型捕获的交易量和交易总量。被模型捕获的欺诈交易可以通过用因子 10(即`1/P(allow)`)对审查交易进行加权来估计。这样做是因为每个被阻塞的事务(但被随机选择不阻塞)在某种意义上代表了 10 个事务。

**模型捕获的预计欺诈交易** =审查欺诈* 1/P(允许)= 8*(1/0.1) = 80

**估计的总体欺诈交易量** =模型捕获的估计欺诈交易量+其他欺诈= 80 + 40 = 120

**回忆** =模型捕获的预计欺诈交易/预计总体欺诈交易= 80/120 = 66.67%

## 培养

可以在 910 允许的交易上进行训练，这是我们知道基本事实的交易。对 900 个预测非欺诈交易使用权重 1，对 10 个审核交易使用权重 10(即`1/P(allow)`)。这为训练数据创建了无偏的数据分布。

# 使用自定义倾向函数

我们可以为`P(allow)`定制一个倾向函数，而不是一个常量。背后的逻辑是，我们可以将更多的审查预算用于不太“明显”的欺诈交易。

如果分类器对一项交易给出 1 分，则可以相当确定该交易是欺诈。另一方面，如果分数是，比如说，0.49 或 0.50 或 0.51，则不太确定，我们处于决策边界。有了上面的策略，不管分数如何，都允许以相同的比率审查交易，但在某种意义上，分数越大，就越有信心。为了优化有限的审查预算，明智的做法是在结果不太确定的地方，即在决策界限内，花费更多的预算。

为了做到这一点，可以使用`P(allow)`的倾向函数来代替之前的统一阈值。`P(allow)`将与分数成反比，这样它允许更多的交易接近边界，更少的交易接近 1。同样，这里为了训练和召回估计，我们将为我们决定允许的事务添加一个权重`1/P(allow)`。

## 优势

这可以更有效地利用有限的审查预算，减少总体欺诈事件的发生。更不确定的交易是那些被审查更多的交易。我们试图在探索和利用范例之间找到一个理想的平衡。这有很强的理论背景。

## 不足之处

一个高权重的交易被错误分类将会极大地改变我们的估计。这可能会导致估计值出现很大差异，并使它们变得不可靠。在实践中，这可能是一个巨大的问题。

# 最后的想法

如果您正在开发一个可能会影响观察到的基本事实值的新模型，那么在生产中首次部署之前，必须考虑一个反事实评估策略。这将使您能够监控您的模型，并为再培训提供无偏见的数据分布。从倾向函数的简单统一阈值开始，只有在情况需要额外的复杂性时才超越它。