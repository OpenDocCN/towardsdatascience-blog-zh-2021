# 2021 年 9 月要读的四篇深度学习论文

> 原文：<https://towardsdatascience.com/four-deep-learning-papers-to-read-in-september-2021-3650a30725d?source=collection_archive---------4----------------------->

## [思想和理论](https://towardsdatascience.com/tagged/thoughts-and-theory)

## **从 Auto-ML 到 Vision Transformer 培训&陈述和灾难性的 Fisher 爆炸**

![](img/cd5f36b96b28dc01b5145f8353676c47.png)

欢迎来到九月版的【T4:机器学习拼贴】系列，在这里我提供了不同深度学习研究流的概述。那么什么是 ML 拼贴呢？简单地说，我为我最近最喜欢的一篇论文起草了一张幻灯片的视觉摘要。每一周。在月底，所有由此产生的视觉拼贴都被收集在一个摘要博客帖子中。因此，我希望给你一个视觉和直观的深入了解一些最酷的趋势。所以，废话不多说:这里是我在 2021 年 8 月读过的四篇我最喜欢的论文，以及为什么我认为它们对深度学习的未来很重要。

## **‘Auto-sk learn 2.0:通过元学习实现自动学习’**

*作者:福雷尔等人(2021)* |📝[纸张](https://ml.informatik.uni-freiburg.de/papers/21-ARXIV-ASKL2.pdf) |🤖[代码](https://github.com/automl/auto-sklearn)

**一段总结:** Auto-ML 有望消除繁琐的超参数手动调整和模型选择。一个例子是 [Auto-Sklearn API](https://github.com/automl/auto-sklearn) ，它提供了一个简单的高级接口来自动评估多个预处理和模型拟合管道。以前的 Auto-ML 系统的一个关键因素是使用所谓的元特征，这些元特征最初是为手头的数据集计算的。然后，这些特征被用于选择“策略”,以便在解决方案空间中进行顺序搜索。策略选择基于到代表性数据集的元数据集的元特征距离。如果您的数据集与元数据集有很大不同，有时这可能会导致泛化问题。此外，很难设计代表性的元特征和调整 Auto-ML 算法本身的超参数。Auto-Sklearn 2.0 旨在通过引入两个修改来克服这两个挑战:首先，他们不依赖元功能，而是使用初始管道的元学习组合。最初，这些候选投资组合被评估，以便启动贝叶斯优化内部循环。其次，他们引入了元学习策略选择器，该选择器基于所考虑的数据集中的样本和特征的数量来规定模型选择策略(例如，交叉验证与简单维持评估)和预算分配策略(全额预算与更积极的连续减半)。因此，该系统更接近于一种分层的元-元方法。作者在 [OpenML 基准](https://docs.openml.org/)上验证了他们提出的修改，并为 10 分钟和 60 分钟的时间预算提供了一种新的最先进的方法。

![](img/b6cb1800b0b684fe92e787eb403bb38b.png)

ML-Collage [29/52]:作者的数字。|📝[论文](https://ml.informatik.uni-freiburg.de/papers/21-ARXIV-ASKL2.pdf)

## **‘如何训练自己的 ViT？《视觉变形金刚》中的数据、增强和规则化**

*作者:斯坦纳等人(2021)* 📝[纸张](https://arxiv.org/abs/2106.10270) |🤖[代码](https://github.com/google-research/vision_transformer)

**一段话总结:**虽然视觉变压器(ViT)模型非常灵活，不需要预先支持归纳偏差(如卷积的平移等方差)，但它们的训练协议可能相当复杂，最终结果可能对超参数敏感。Steiner 等人旨在研究计算预算、模型大小、增强/正则化和训练数据量之间的权衡。本文为从业者提供了有价值的见解，并展示了超过 50k ViT 训练运行的结果。具体来说，他们表明，通过使用数据扩充(例如，混合& RandAug)和模型正则化(例如，权重衰减& dropout)的正确组合，可以实现与基于 10x 数据训练的模型相当的模型性能。针对大数据预先训练的 vit 也会产生更适合下游传输的表示。此外，作者表明，仅微调单个最佳转换器(在预训练数据集上评估)通常会产生与基于微调数据选择的模型相当的模型。因此，对单个 ViT 进行微调，以便为您的传输应用获得良好的模型，这可能更具成本效益。最后，作者比较了不同的扩增和正则化技术。他们发现，数据扩充在更多情况下似乎比模型正则化更有效。总的来说，论文的主要优势在于他们使用了标准化的训练体系，这使得他们能够提出有证据支持的主张。

![](img/b88a697978872942c36acabf2f8a1042.png)

ML-Collage [30/52]:作者的数字。|📝[论文](https://arxiv.org/abs/2106.10270)

## **‘灾难性费希尔爆炸:早期费希尔矩阵影响泛化’**

*作者:Jastrzebski 等人(2021)* 📝[论文](https://arxiv.org/abs/2012.14193)

**一段话总结:**经常有人读到深度学习中随机梯度下降的“隐式正则化”。但这实际上指的是什么呢？Jastrzebski 等人研究了一种由于使用大的学习率而产生的正则化。他们表明，在训练的早期，小的学习率会导致 Fisher 信息矩阵轨迹的强烈振荡。这种“爆炸”似乎会导致更差的最终概括，可以通过提高学习速度的训练来避免。基于这一认识，作者定义了一个显式正则化子，它促进了一个小的 Fisher 迹。他们表明，这个正则项缩小了学习率较小的训练运行的性能差距，并提供了正则项在训练早期使用时特别有效的证据。作者认为，这种效应可能是由有限的记忆造成的，并表明费希尔矩阵的轨迹与噪声样本的梯度成比例。通过惩罚轨迹，可以降低这些例子的学习速度并减少过度拟合。最后，他们表明，费希尔罚函数导致平坦的极小值(由黑森轨迹测量的低曲率)，这已被证明是更好的概括。因此，本文的主要贡献在于将早期训练阶段的不稳定性与观察到的 Fisher 信息行为联系起来。

![](img/d3fe9b64e9f650ef7c955c58ceb1ab57.png)

ML-Collage [31/52]:作者的数字。|📝[论文](https://arxiv.org/abs/2012.14193)

## **‘视觉变形金刚看起来像卷积神经网络吗？’**

*作者:Raghu 等人(2021)* |📝[论文](https://arxiv.org/abs/2108.08810)

**一段总结:**视觉变形金刚如何解决任务？它们的表征结构与常规 CNN 相似还是完全不同？研究这个问题的一个强有力的工具是代表性相似性分析(RSA)。RSA 使用中心内核对齐来比较不同输入的两个网络层的激活。得到的数值度量告诉您这些表示有多相似。 [Kornblith 等人(2019)](http://proceedings.mlr.press/v97/kornblith19a/kornblith19a.pdf) 之前使用这种方法来揭示 ResNets 的计算力学。那么和 ViTs 相比有什么区别呢？令人惊讶的是，ViT 层在所有层中具有更一致的相似性。这意味着信息通过架构传播得更强。Raghu 等人表明这是由于两个原因:首先，自我注意机制允许在比局部卷积更早的阶段聚集全局信息。其次，ViTs 中的 skip 连接允许通过几十层来传递早期的聚合信息。如果在某个程序段训练一个禁用跳跃连接的 ViT，这将在所有前面的和所有后面的程序段之间实施“通信”分区。另一个发现是，ResNets 需要更多的早期层来获得用少量 ViT 层就能获得的表示。这也可能是因为注意力机制能够在早期整合全局信息。最后，作者表明，vit 需要在大量数据上进行训练，以学习位置的归纳偏差。甚至 ImageNet-1k 似乎也不够，只有谷歌内部的 JFT-300 数据集。

![](img/ff013e8f8490a40424144b431699576d.png)

ML-Collage [32/52]:作者的数字。|📝[论文](https://arxiv.org/abs/2108.08810)

这是这个月的🤗让我知道你最喜欢的论文是什么。如果你想获得一些每周 ML 拼贴输入，查看 Twitter 上的标签[# ML collage](https://twitter.com/hashtag/mlcollage)。你也可以在最后的总结中找到拼贴画📖博客帖子:

</four-deep-learning-papers-to-read-in-august-2021-7d98385a378d> 