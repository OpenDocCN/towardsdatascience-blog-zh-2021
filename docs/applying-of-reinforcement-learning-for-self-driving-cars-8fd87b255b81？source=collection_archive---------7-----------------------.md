# 强化学习在自动驾驶汽车中的应用

> 原文：<https://towardsdatascience.com/applying-of-reinforcement-learning-for-self-driving-cars-8fd87b255b81?source=collection_archive---------7----------------------->

## 马尔可夫决策过程(MDP)和寻找自动驾驶发展的整体政策

![](img/8eb47a3ebd32ac0b3b1f06a4f71fc3af.png)

Denys Nevozhai 在 [Unsplash](https://unsplash.com/s/photos/mobility?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText) 上拍摄的照片

自动驾驶汽车的人工智能应用的一种广泛方法是监督学习方法，最重要的是，用于解决感知要求。但是自动驾驶汽车在强化学习(RL)方法中非常类似于机器人和代理。我们能用强化学习方法代替监督学习方法吗？**监督方法的缺点是，从数据收集到模型部署，整个人工智能过程都存在人为偏见。**

与环境互动是自动驾驶汽车最重要的任务。感知是第一步，目前是基于人工智能的，并应用了监督的方法。在这种方法中，**你需要考虑车辆是在一个开放的情境环境中行驶，你需要用现实世界中所有可能的场景和情景来训练你的模型。**场景和场景的多样性是特斯拉、Waymo、Cruise 必须通过收集越来越多的数据，并根据收集到的数据验证系统运行来解决的主要困难。**如何确保自动驾驶汽车已经学习了所有可能的场景，并安全地掌握了每一种情况？**

# 强化学习

强化学习可以解决这个问题。**RL 方法意味着代理收集环境信息，并基于定义的策略从一个状态切换到下一个状态，以最大化回报。**智能体作为自动驾驶汽车的大脑，采取哪些行动？简单来说，加速、减速和转向这三个动作是影响车辆动力学和道路安全的最关键动作。最危险的决定是转向，最不关键的决定是刹车。我们如何在强化学习过程中为作为驾驶员的代理定义一个策略和一个奖励函数？

# 奖励函数

为了定义奖励函数，我们可以考虑各种方面，例如功耗、所有道路使用者的安全，或者更快到达目的地且仍然安全驾驶的最佳导航方式。我们应该区分安全驾驶等短期奖励和提前到达目的地等长期奖励。一个智能体，在这种情况下，一辆自动驾驶汽车，需要监控环境，并意识到哪个新状态可以获得最大回报，例如加速、减速或转向，以及所有其他相关参数和变量。

社会不接受自动驾驶汽车过于缓慢和保守的驾驶或频繁制动。有两种应对安全关键事件的解决方案。一个是回到旧的安全状态，第二个是寻找新的“安全状态”由于不清楚较早的“安全状态”是否仍然安全，因此找到新的“安全状态”是唯一可靠的解决方案，而较旧的安全状态是所有其他可能情况中的一个解决方案。**然而，如果一个代理决定回复到过去的状态，这个代理将需要一个新的奖励评估，而现有的奖励将不再有效。**

挑战将是找到与其他代理活动并行执行新的奖励评估的时间间隔。奖励随时间变化，取决于情况和驾驶动态。**一般来说，在道路交通中与其他车辆保持安全距离可以获得较高的奖励值。**这个论证类似于不必要的刹车缩短了跟在自动驾驶汽车后面的汽车的距离。

# 环境监测

自动驾驶汽车通过车载传感器收集相关的环境信息，如道路或交通标志的类型，或者从其他车辆或基础设施远程接收这些信息。自动驾驶汽车的所有必要信息都可以测量吗？不，不是全部，至少没有驾驶风格等历史数据。**如何衡量自动驾驶汽车在混合交通道路或其他自动驾驶车辆上是否与其他驾驶员配合？**

在设计任何基于强化学习方法的自动驾驶智能体之前，我们必须回答这些问题。**社会不会接受代理人长期不合作，我们需要衡量和控制这种行为。**

使用监督学习进行环境监测似乎仍然是正确的解决方案。我们需要通过分离感知部分和决策部分来降低代理决策的复杂性。**由环境传感器收集的所有数据都应被标记并与适当的策略相关联，以促进强化学习设计过程。**

# 为代理选择策略

由于许多可能的情况，为作为自动车辆的代理选择优化的策略是复杂的。代理应该根据流量情况解决不同的问题，并根据这种情况找到适当的响应或策略。如果有多个有效策略，代理应灵活地在其他合适的策略之间切换，或者组合多个策略来计算以下可能状态的奖励。

如本[论文](https://arxiv.org/pdf/2006.04218.pdf)、**中所述，找到最佳策略的一个可能解决方案是使用有经验的驾驶员的行为和相关分布作为参考，并尝试通过代理再现这种行为。**然而，本文分析了一个特殊的场景，即具有深度强化学习的类人驾驶的“静态避障”，需要讨论如何将其扩展到所有可能的场景。

另一个解决方案是分层方法，正如在这篇[论文](https://arxiv.org/pdf/2001.09816.pdf)中所描述的。选择支持的机动动作，例如在车道内行驶，换到右侧车道，换到左侧车道。每个机动都有它的策略和状态，应该单独训练，并且应该有一个主策略来选择正确的机动。**该解决方案描述了一个多代理 RL，在任何单个时间点只有一个代理是活动的，我们需要额外的智能在所有代理之间切换。**

然而，机动动作的分离并没有解决自动驾驶的复杂性，因为我们还必须证明，所有的机动动作，包括所有的变化，都已经在设计和测试过程中考虑到了，并且机动动作之间的切换也是安全进行的。因此，这种方法不太可能提供一种完全自动驾驶的汽车，在每种情况和场景下进行竞争。

驾驶汽车是一项复杂的任务，需要最低水平的智能来确保安全驾驶，因为我们已经有了关于如何获得驾照的规定。许多参数，如车辆动力学参数如速度、加速度、位置、倾斜度等。以及环境参数影响驾驶策略。**一个** [**操作设计域(奇数)**](https://medium.com/rewrite-tech/how-to-model-the-behavior-of-a-self-driving-car-5506a3d7afdd) **应该指定影响强化学习方法的参数。**多种驾驶动作只是设计领域中的一个方面，我们需要基于 ODD 考虑所有参数来训练 RL。

[](https://medium.com/subscribe/@bhbenam) [## 每当贝扎德·贝南出版时，就收到一封电子邮件。

### 每当贝扎德·贝南出版时，就收到一封电子邮件。通过注册，您将创建一个中型帐户，如果您还没有…

medium.com](https://medium.com/subscribe/@bhbenam) 

我的推荐页面:[https://medium.com/@bhbenam/membership](https://medium.com/@bhbenam/membership)