# 为什么强化学习还没有征服世界(至今)？

> 原文：<https://towardsdatascience.com/why-hasnt-reinforcement-learning-conquered-the-world-yet-459ae99982c6?source=collection_archive---------17----------------------->

## 优化已经存在了几十年，机器学习实现了一个又一个突破。以强化学习的形式将两者结合起来，应该是解决问题的圣杯。为什么不是呢？

![](img/8433264b3d65bd926cf62fe593d4a8ab.png)

Artem Beliaikin 在 [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral) 上拍摄的照片

登机时你可能没有意识到这一点，但起飞时间、燃料水平、维修人员和起飞航线很可能都是由数学优化模型决定的。无论是精确解如线性规划还是强大的启发式算法如遗传算法，**优化模型**是许多调度和资源分配问题背后的无声力量。在不断增长的计算能力和求解器进步的推动下，特别是在 20 世纪 90 年代，优化技术在所有行业都得到了广泛应用。虽然不一定是今天的“热门”话题，但计算机仍然在努力解决几乎每个领域中难以想象的大型优化问题。

机器学习和人工智能的进步更加突出。很少有一周没有关于新突破的令人愉快的头条新闻。检测假新闻，面部识别，给一首难以捉摸的歌曲命名——无监督和有监督的学习在现实世界中都获得了巨大的成功。特别是神经网络非常善于发现模式；如今丰富的数据提供了源源不断的新测试用例。

信奉这两个领域会产生一些非常强大的后代，这似乎是很自然的。这个后代被更好地称为**强化学习**。通过从环境中学习来不断改进决策，我们理论上有了一个非常强大的自我学习框架，能够同时做出明智的决定，并从中学习以做出更好的决定。但事实上，我们并不在那里，一点也不在。

当然，AlphaGo 令人印象深刻，训练马里奥导航蘑菇王国也是如此。也有一些现实世界的成功值得庆祝。RL 好像挺适合 A/B 测试的。RL 可能会学习裁剪你看到的广告。 [Optimal Dynamics](https://optimaldynamics.medium.com/) 在运输领域做着非常有趣的工作。简而言之，毫无疑问，有成功的 RL 应用在流通。但是声称 RL 目前被广泛部署来解决现实世界的问题——绝对不是。

# 是什么让强化学习如此艰难？

考虑一个拥有 10，000 种产品类型的仓库。每个产品的预期销售额是已知的。问题是每种类型每天订购多少(比如 10 天的时间跨度)，考虑到交货时间、存储容量、销售损失成本等。这是一个巨大而复杂的问题，但可以通过当代优化技术很好地解决。

*预期*这个词在这里至关重要。实际上，某一天可能会卖出无数种产品组合，每种组合都代表一种独特的场景，导致一种新的库存状态。对于每个州，我们都希望有能力做出正确的决定。决策是按顺序做出的:我们订购产品组合，观察售出的产品，在一天结束时，我们必须能够为可能出现的每一种可能的情况做出重新订购的决定。

换句话说:*确定性*优化提供**决策**，*随机性*优化提供决策**策略**。确定性优化解决单一问题，随机优化解决所有可能出现的问题。RL 没有告诉我们*要做什么*决定，而是*如何*做出决定。这是一个完全不同的游戏。

![](img/b44fc511c812655dea55d3d7ef43b899.png)

[JJ 英](https://unsplash.com/@jjying?utm_source=medium&utm_medium=referral)在 [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral) 上拍照

监督学习和 RL 之间的差异也很大。技术可能是相似的，例如，神经网络在两个领域中都是常见的。在监督学习中，神经网络可以将图像作为输入，并预测正确的标签。在 RL 中，考虑一个 Q 网络，它将问题状态作为输入，并返回对应于该状态的值。两者都可以被视为监督学习的样本。

然而，在前一种情况下，图像标签是静态的。相比之下，对应于状态的 Q 值是依赖于策略的——随着每次策略更新，该值也应该改变。因此，这个问题实质上更具有动态性。此外，RL 中的行为通常只对累积奖励有部分影响，随机性也起着很大的作用。神经网络在检测模式方面非常出色，但也容易产生拟合噪声。即使对于玩具大小的 RL 问题，神经网络也常常难以学习准确的值。

# 逆向物流实践中的瓶颈

我们已经讨论了为什么 RL 在某些方面比确定性优化和监督学习更难(尽管这些领域自然有它们自己的挑战)。然而，人类以前曾攻克过难题，这还不是全部。

其实 RL 文学的理论体还是挺可观的。许多对于动态或随机规划来说完全难以解决的问题已经被 RL 解决了，并且经常有令人印象深刻的结果。计算机科学的例子可能是最著名的，然而工程界也提出了许多聪明的算法。

那么，为什么我们在实践中很少看到这种情况呢？

## 分散的社区

![](img/3393d592a7536bac2f637a4a1fda7566.png)

尽管许多研究团体处理 RL 问题，但是工具和术语有很大不同。照片由[凯蒂·哈普](https://unsplash.com/@kharp?utm_source=medium&utm_medium=referral)在 [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral) 拍摄

无需过多钻研历史；RL 起源于不同的领域，并沿着道路经历了分支和转化。因此，有许多不同的社区，都有自己的应用程序、符号风格、术语和建模约定。部分由于这种分散性，也没有通用和成熟的 RL 算法工具箱，就像有监督学习和确定性问题一样。

也可能有人认为，学术界和实践界作为一个群体被隔离得太多了。RL 可以分为[四个政策等级](/the-four-policy-classes-of-reinforcement-learning-38185daa6c8a)。学术界倾向于关注数学上优雅的解决方案，而工业界更依赖于直接采样和确定性模型的扩展。政策的可解释性在学术著作中也经常被忽视。因此，RL 研究很少付诸实践。

[](/the-four-policy-classes-of-reinforcement-learning-38185daa6c8a) [## 强化学习的四个策略类别

towardsdatascience.com](/the-four-policy-classes-of-reinforcement-learning-38185daa6c8a) 

## 问题建模

在考虑解决方案之前，我们必须首先确定问题所在。马尔可夫决策过程在规范形式上看起来足够简单，然而[的实际实现](/five-things-to-consider-for-reinforcement-learning-in-business-environments-23647a32f01f)可能包含数千行代码。

在不深入理论的情况下，以下几个方面是相关的:

**状态** →决策需要哪些数据？这不仅需要物质资源，还需要信息甚至信念。

**动作** →给定状态下允许哪些动作？在模拟中*可行的*行动在现实中可能是非常*不可取的*。行动空间的大小也经常被证明是一个瓶颈。

**转移函数** →通常[隐藏为一个简单的概率](/what-are-post-decision-states-and-what-do-they-want-from-us-9e02105b7f40)，这个函数捕捉所有的系统动态。对管理一个真实系统的动力学建模是一项非常复杂的任务，即使只是近似地捕捉它的行为。

**奖励** →定义合适的 KPI 很难。在实践中，可能有多个标准和模糊的目标，但没有任何机器学习算法可以在没有明确指令的情况下学习。

将世界视为服从顺序决策的随机系统不一定是自然的或直观的，这导致了许多不适定的问题。

[](/five-things-to-consider-for-reinforcement-learning-in-business-environments-23647a32f01f) [## 商业环境中强化学习需要考虑的五件事

### 强化学习不仅仅可以用于走迷宫和玩棋盘游戏。现实生活中的实施…

towardsdatascience.com](/five-things-to-consider-for-reinforcement-learning-in-business-environments-23647a32f01f) 

## 数据可用性

RL 在游戏中表现如此出色的原因是它们可以无限多次重复。在代理经过适当的训练之前，可能会播放数十亿集。

当从现实生活中进行观察时，不可能在任何接近的地方收集到一个[重放缓冲区](/how-to-model-experience-replay-batch-learning-and-target-networks-c1350db93172)。根据问题的性质，在现实中测试一个策略可能需要几个小时、几天或几个月。此外，在现实世界中失败是昂贵的——想想安全措施，想想财政限制。这样，探索的空间就少了。时间和成本阻碍了建立一个丰富的观察集。重要的是，现实生活中的数据只反映了现实生活中使用的策略，而我们想要测试许多策略。

一个代表现实的仿真模型就避开了这样的问题。然而，说起来容易做起来难。为了逼真地模拟现实，模拟环境需要大量的实时数据。直到最近，数据还没有以这样的规模收集和存储。

如果没有一个非常好的模拟环境，几乎不可能获得足够高质量的观察结果和学习策略。

[](/need-help-making-decisions-ask-your-digital-twin-6e4cf328cb0) [## 需要帮助做决定吗？问问你的数字双胞胎吧！

### 将实时数据与模拟、机器学习和人类推理相结合，以提升决策水平。

towardsdatascience.com](/need-help-making-decisions-ask-your-digital-twin-6e4cf328cb0) 

# 未来展望

数据科学家和学术研究者喜欢新奇，只要他们能驾驭机器学习浪潮，不断取得突破，就会有源源不断的新发展。承诺*即插即用* RL 算法的项目也在增加，不可避免地最终会产生更多的**标准化解决方案**。对于机器学习任务来说，测试一系列算法并选择最合适的算法是很容易的，毫无疑问 RL 会随之而来。

受物联网技术的兴起及其提供的大量实时系统数据的刺激，构建丰富的、数据驱动的模拟环境变得更加容易，这种环境被称为**数字双胞胎**。这项技术能够模拟现实的详细虚拟代理，从而能够进行比现实世界允许的更多的实验。未来的 RL 实现可以在这种替代现实中进行培训，从而极大地增加了学习适用于现实世界的策略的机会。

最后，对机器学习的理解将随着时间的推移而提高。仍然有太多的公司将机器学习视为一个神奇的黑盒子。成功的 RL 需要对顺序决策问题、随机环境和部署的技术有深刻的理解。构建模块就在那里，但通常仍然需要点击。可解释人工智能的进展也加深了对 RL 内部机制的理解。

简而言之，我不认为 RL 是一条死路，一点也不。如果说有什么不同的话，那就是我们正处在现实世界巨大进步的边缘。就在五年前，数据科学的世界看起来与今天大不相同。这场革命不会很快停止，许多实际障碍的解决方案似乎触手可及。

RL 是否真的会‘征服世界’，我不确定。然而，成功的实际实施似乎只是时间问题。

# 外卖食品

*   RL 的目标是找到一个在任何情况下都有效的政策，而不仅仅是一个决定。这使得它比确定性优化困难得多。
*   RL 通常应用于噪声**随机环境**；机器学习算法倾向于适应噪声，而不是可控模式。
*   RL 面临的挑战是许多不同的子领域(没有统一的语言或工具箱)，以数学形式表示问题，以及依赖有限的真实世界观察来学习策略。
*   数据科学和研究的进步将在不久的将来克服某些 RL 挑战。特别是，**数字孪生**环境似乎非常有希望使用现实的虚拟代理来学习政策。

# 进一步阅读

凯捷(2020 年)。强化学习值得大肆宣传吗？[https://www . capgemini . com/g b-en/2020/05/is-reinforcement-learning-worth-the-hype/](https://www.capgemini.com/gb-en/2020/05/is-reinforcement-learning-worth-the-hype/)

Dulac-Arnold，g .，Mankowitz，d .和 Hester，T. (2019)。现实世界强化学习的挑战。https://arxiv.org/pdf/1904.12901.pdf

鲍威尔，W.B. (2019)。随机优化的统一框架。
*《欧洲运筹学杂志》*，第 275 卷第 3 期，第 795–821 页。

鲍威尔。世界银行(2020)。卡车运输行业的顺序决策分析。*最优动态。*