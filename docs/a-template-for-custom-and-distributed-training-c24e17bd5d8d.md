# 定制和分布式培训的模板

> 原文：<https://towardsdatascience.com/a-template-for-custom-and-distributed-training-c24e17bd5d8d?source=collection_archive---------32----------------------->

## 使用此模板快速编写自定义张量流算法

定制训练循环提供了极大的灵活性。您可以快速添加新功能，并深入了解算法的工作原理。然而，一遍又一遍地设置自定义算法是乏味的。总体布局通常是相同的；只有很小的部分会改变。

这就是跟随模板的[发挥作用的地方:它概述了一个定制的分布式训练循环。所有需要修改以适应任务的地方都用待办事项突出显示。](https://github.com/phrasenmaeher/distributed_template)

![](img/d68a404ab67184397fdfb8616d8b60a6.png)

照片由[克里斯里德](https://unsplash.com/@cdr6934?utm_source=medium&utm_medium=referral)在 [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral) 上拍摄

# 自定义分布式循环的一般布局

[定制训练循环](https://www.tensorflow.org/tutorials/distribute/custom_training)——与调用 model.fit()相反——是一种迭代数据集、更新模型权重和计算任何指标的机制。

在迭代任何数据集之前，无论是训练、验证还是测试分割，数据集都必须准备好分发。这是借助 TensorFlow 的分发策略对象完成的。

我们首先创建策略对象，它负责所有的分布式计算。通过选择不同的分发策略，我们可以在各种计算环境中使用我们的算法。这一事实使得定制循环高度灵活。

在第 2 行，在创建了我们的策略之后，我们准备好了我们的数据集分布；TensorFlow 处理所有内部细节。

对于我们的分布式数据集，我们可以使用“ *for i in x* ”方法对其进行迭代:

该循环对于所有子集(训练、验证、测试)都是相同的。主要区别是被调用的*步骤*。上面，我示例性地调用了 *distributed_train_step* ，它处理我们的数据到所有加速器的分发。但是不要担心，TensorFlow 处理大部分内部设备到设备的通信。我们只有它知道我们想做什么。方法已经为我们做好了。

作为培训、测试或验证步骤的一部分，我们还会更新我们希望在培训期间跟踪的任何指标。我们必须手动执行此操作，如下所示:

在这里，我编写了一个训练步骤，该步骤采用单个数据批次，将其解包为要素和标注，计算梯度，更新模型，并计算任何训练度量。这种方法类似于验证和测试步骤；我们只跳过模型更新。

概括总体布局:

*   准备好分发所需的对象(模型、数据集、优化器、指标)
*   迭代数据集
*   调用分布式训练/测试/验证步骤来更新模型并计算指标

总体布局到此为止。在接下来的内容中，我们将检查模板，看看您需要为手头的任务修改什么。

# 分布式定制培训模板

我们从必要的导入和一些全局定义开始:

对于这个模板，我决定将训练、测试和验证步骤中使用的所有对象和变量设为全局。这样，我们就不必一直传递它们，这使得代码更干净。然而，这只是一种方法，我相信还有更好的方法。

所有模型及其优化器都是全局可用的，任何度量和损失也是如此。我没有在模板中包括测试和验证损失，但是我强烈建议您这样做。除此之外，我们还全局注册*全局*批量大小和分配策略(如前所述)。

我们的训练脚本是用几个示例命令调用的，所以我们还导入了 *argparse* 库。完整脚本的代码是

并引出了主要的方法:

# 主要方法

*main* 方法从使用我们之前介绍过的大部分全局对象开始。到目前为止，它们只是占位符，这就是我们现在实例化它们的原因。我们从分布策略(第 13 行)开始，然后是损失(第 16 行)、指标(第 19 行)和模型(第 22 行)。我们将很快详细介绍所有被调用的方法。

在这个模板中，我跳过了数据集创建部分。有太多的方法可以做到这一点，你知道什么最适合你的问题。从第 26 行开始，我只使用了 *None* 作为初始值；将这一部分替换为创建和分发数据集(使用前面代码片段中显示的策略)

从第 31 行开始，我们首先使用分布式训练和验证数据集训练模型，然后在保留测试数据集上评估它。

## 损耗

第一种方法 *create_loss_objects* ，负责创建我们使用的任何损失。如下所示:

在分配策略的范围内，我们创造任何我们需要的损失。在示例代码中，这只是一个虚拟损失——修改它以满足您的需要。无论选择哪个损耗，不要忘记设置如图所示的*减少*参数:我们在后面的方法中手动减少损耗。无论您使用单个损失还是多个损失，都在这里创建并返回对象。您应该全局注册所有返回的亏损对象。我已经对样本 *train_loss_object1* 执行了此操作。对所有损失重复此步骤。

## 韵律学

创建用于跟踪模型进展的指标的代码遵循相同的方案:

在分布策略的范围下，我们实例化所有的度量。在模板中，这只是训练精度。缺少的是任何验证(在培训期间)和测试(在培训之后)的度量。我建议在打印时使用有意义的名称来标识指标。像以前一样，全局注册所有返回的指标。在*主*方法中，我已经用第 9 行中的 *train_metric1* 实现了这一点。

在实例化指标之后，我们现在关注模型和优化器。这种模板方法遵循前两种方法:

和以前一样，我们在选择的策略范围内创建任何东西。这一步是强制性的，以使模型的内部变量和优化器分布准备就绪。在示例代码中，我没有选择任何特定的模型或优化器；修改第 8 行中的任何内容以适合您的任务。此外，不要忘记全局注册返回的对象，正如我在 *main* 函数的第 9 行中对*模型*和*优化器*所做的那样。

## 数据集

在所有必需的对象都被实例化并准备好发布后，就该创建数据集了。我有意将此留作空白，因为创建数据集有多种方法:从 *tf.data.Dataset* 对象到定制生成器或混合方法。你可以在这里找到概述，但是因为这个模板是面向更有经验的程序员的，我怀疑它对你是否有必要。无论如何，在创建数据集之后，您必须使用

一旦数据集准备好，我们就可以继续训练循环(主中的第 31 行)。对于我们使用的模板，我们需要训练和验证数据集，但是您可以随意放弃后者。

# 训练和验证循环

训练循环是我们分布式算法的核心:

在第 12 行，我们开始在数据集上迭代*个时期*次。然后，从第 15 行到第 20 行，我们对训练数据进行循环，并将每一批数据提供给 *distributed_train_step* 方法。在我们完成一次迭代之后，我们计算训练损失，并对验证数据重复这个过程。

完成之后，我们查询所有(全局)度量和丢失对象的当前值，我们打印并为下一个时期重置这些值(第 34 到 39 行)。正如评论所指出的，修改这段代码以考虑任何度量或损失对象。此外，如果您不在每个时期后重置度量(第 37 行)，它们将跟踪所有时期的进度，导致错误的每个时期值。

## 分布式训练步骤

训练循环(从第 17 行开始)在内部调用分布式训练步骤。这个的代码是

我们在这里没有太多事情要做:我们得到一个数据批次( *dataset_inputs* )，告诉策略运行一个单独的训练步骤，并减少返回的损失。现在，我们为什么不直接调用 *train_step* ？因为我们处理分布式数据。第 5 行中的 *strategy.run* 调用说明了这一点；为每个计算设备调用它。例如，如果我们连接了 5 个 GPU，TensorFlow 将自动调用 train 步骤五次。

然后，我们必须合计(考虑“合并”或“合并”)从每个副本到当前设备(我们运行脚本的地方)的火车损失，并减少它以考虑加速器的数量。查看[文档](https://www.tensorflow.org/api_docs/python/tf/distribute/Strategy#reduce)了解更多信息。

## 训练步骤

如果我们没有分配我们的工作负载，我们可以直接调用实际的 train 步骤。然而，由于我们使用的是分布式设置，我们必须让 TensorFlow 知道。我们使用前面的方法做到了这一点，该方法使用 *strategy.run* 来处理分发。在这个调用中，我们说实际的训练方法是 *train_step* 方法，如下所示:

由于 TensorFlow 负责拆分批处理，因此我们接收的是单个批处理。在第 9 行，我们将它分解成特性和标签——这只是一个例子，根据您的需要修改代码。接下来的几行遵循一个标准的定制循环:我们计算损失(第 15 行)，计算并应用梯度(第 18 行和第 19 行)，并更新任何训练指标(第 22 行)。正如我在 TODO 注释中所做的标记，您必须对此进行调整，以考虑您所使用的所有模型、损失和指标。

## 计算(培训)损失

为了计算培训损失，我们使用以下简短模板:

我们在第 10 行查询我们的 loss 对象，它给出了每个副本的损失。让我解释一下:我们的定制算法运行在多个计算设备或副本上。在每个设备上，独立调用训练步骤，导致总共 *n* 个损失。每个损失现在用于计算梯度，通过求和在复制品之间同步。如果我们不衡量损失，结果会被夸大。

还不服气？[在单台机器上，损耗除以小批量样品的数量](https://www.tensorflow.org/tutorials/distribute/custom_training#define_the_loss_function)。在多 GPU 设置中，我们不必将损失除以本地批量大小，而是除以全局批量大小。例如，本地批处理大小可能是 8，而全局批处理大小可能是 32 (=4*8 代表 4 个 GPU)。如果除以 8，我们会假设我们在正向传递中看到的样本总数是 8，这是不正确的。因此，在第 14 行中，我们对损失进行平均，以考虑全局批量大小。

那是为了训练程序。总结一下:我们首先调用 *distributed_train_step* ，它负责将计算分发给每个工人。然后，在每个工人身上，调用 *train_step* ，它接受一个批处理并更新模型的权重。

## 验证步骤

验证程序与培训程序非常相似:

只有微小的区别:我们不更新模型的权重(第 11 行)，我们更新我们的验证指标(第 13 行)。修改这些部分以适应你的需要。*distributed _ validation _ step*遵循与 *distributed_train_step* 相同的布局:获取按比例缩放的每个副本的损失并将其合计(第 25 至 28 行)。

# 测试回路

测试循环与前两个相似:

我们迭代分布式测试数据集(第 11 行)，对损失求和(第 12 行)，并对其求平均值(第 14 行)。之后，我们收集所有指标的结果。我没有包括任何单独的测试指标；根据您的需要修改这一部分(第 16 行和第 17 行)。

一旦我们查看了 *distributed_test_step* (第 12 行)，我们会注意到它再次高度相似:

在第 32 行中，我们收集了缩放后的损失，并在第 36 行中对其进行平均。实际的 *test_step* (第 2 行)取一个批次，打开它(第 9 行)，并计算损耗(第 16 行)和任何测试指标(第 18 行)。在这个片段中，我没有包括任何单独的测试指标。你必须调整它以适应你的问题。通常，您可以在 *create_metrics* 方法中创建所有指标。记住要全局注册它们，以便能够轻松访问它们。

# 摘要

我们已经讨论了定制和分布式算法的基本部分。通过选择适当的分布策略，我们可以将它用于各种计算环境，也可以用于单 GPU 设置。一般来说，所有需要修改的地方都标有 TODO 注释。虽然模板应该涵盖大多数用例，但它只是作为一个起点。像你我这样的从业者应该调整关键部分以满足他们的需求。完整的代码可以在 [GitHub](https://github.com/phrasenmaeher/distributed_template) 获得。