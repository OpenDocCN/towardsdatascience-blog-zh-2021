# 使用 PyTorch 深度学习框架的序列数据批量采样器

> 原文：<https://towardsdatascience.com/batch-sampler-for-sequential-data-using-pytorch-deep-learning-framework-part-3-df19f449f24e?source=collection_archive---------13----------------------->

## 在 PyTorch 框架的 dataloader 中使用零填充顺序数据集时，优化 GPU 利用率

![](img/0bd9d1fe33971fe712355367760fdcdb.png)

杰西卡·约翰斯顿在 [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral) 上的照片

注意——要了解如何为自定义数据集编写数据加载器，无论是顺序的还是图像的，[请参考这里的](https://medium.com/@AnveeNaik/dataloader-for-sequential-data-using-pytorch-deep-learning-framework-part-2-ed3ad5f6ad82)。

对于数据点大小可能不同的顺序数据集，我们使用零填充使所有数据点大小相同。因此，批处理可以转换为张量，并传递到图形卡(GPU)进行并行处理。

但是这种方法不是最佳的。考虑两批 8 个数据点，每个数据点的大小如下:

*   批次 1— [2，5，5，4，6，7，8，2]，批次的最终大小= 8*8
*   批次 2— [2，32，5，36，6，34，8，2]，批次的最终大小= 36*8

在批次 1 中，所有数据点需要使用零填充转换为大小 8，而在批次 2 中，所有数据点需要转换为大小 36。在这两种情况下，最小大小都是 2，但是在批次 1 中，我们只给大小为 2 的元素添加了 6 个零，而在批次 2 中，我们需要添加 34 个零。因此，我们可以看到，批处理 2 在处理无用的零时浪费了大量 GPU 内存，而批处理 1 是一种高效的打包方式，浪费的 GPU 很少。这个例子显示了问题所在，也给出了我们的解决方案。如果我们能以某种方式手工制作批次，那么我们就能确保每个包装都是有效的。

但是我们怎么做呢？？

还记得内部提供的 *__getitem__* 函数中的 *index* 变量吗，如果我们可以手动为每个批次提供索引，那么我们的工作就完成了。为此，我们可以使用数据加载器中的 *batch_sampler* 参数，如下所示。

```
train_dataset = Dataset_seq(word2id, train_path)
sampler = Sampler(tokens, data, bin_size) #data is list of sentences
                                           present in whole corpus
train_batch_sampler_loader = DataLoader(
        train_dataset, 
        batch_sampler = sampler,
        collate_fn = collate_fn)
```

现在，将使用我们将在下面定义的 sampler 函数来提供一个批次的索引。

注意——对于不同的时期，最好在一批中有不同的数据点集，即如果在第一个时期一批通过(*数据 1、数据 2、数据 3、数据 4* )，在其他时期，我们应确保不一起提供相同的数据集(*数据 1、数据 2、数据 3、数据 4* )。这是为了确保我们的模型不会学习提供数据点的模式/顺序。

现在让我们来理解如何为我们的采样器函数编写算法:

*   我们将创建一个包含每个数据点长度的列表 *text_len* 。
*   然后，我们将创建箱(或桶数据),使得每个箱存储大小小于或等于对应于该箱的大小的数据点的索引。这里，对应于每个库的大小取决于 *bin_size。例如，如果 *bin_size* = 2，那么 bin 的大小将是 3，5，7…直到最大尺寸出现在 *text_len* 中。*
*   如果说 *text_len =* [2，3，4，5，6，7，8]，那么我们将得到{3: [0，1]，5: [2，3]，7: [4，5]，8: [6]}即索引 0，1 处的值大小≤ 3，索引 2，3 处的值大小≤ 5，依此类推。最后一个 bin 的大小为 8，因为这是 *text_len 中的最大大小。*
*   现在，我们已经根据大小分离了所有数据，我们将创建我们的批次。为此，我们将使用一个参数 *n_tokens* ，该参数指示 GPU 中可以加载的最大总大小(包括零填充)。因此，如果 *n_tokens=500* ，那么我们可以使每一批在补零之后，一批中每个数据点的大小之和小于或等于 500。
*   现在，为了形成批，我们从最大的桶开始，继续按顺序挑选索引，直到该批的总大小刚好小于或等于 *n_tokens。*一旦形成一个批次，我们将其附加到 *final_indicies* 中，这是一个列表列表。这个过程一直持续到所有的数据点(存在于所有的箱中)被拾取并被分配给一批。
*   为了确保不在不同时期发送相同的一组批，在每个时期之后，我们随机打乱为每个箱存储的列表。因此，当我们按顺序开始从箱子中挑选时，我们每次都会得到不同的数据。

请参考下面的算法代码

我希望这个博客能帮助你理解和探索新的应用。请分享你的反馈和其他方法，让这个博客变得更好。

*成为* [*介质会员*](https://medium.com/@AnveeNaik/membership) *解锁并阅读介质上的许多其他故事。关注我们的* [*中的*](https://medium.com/@AnveeNaik) *，阅读更多此类博文*。