# 理解人工智能系统中的偏见和公平

> 原文：<https://towardsdatascience.com/understanding-bias-and-fairness-in-ai-systems-6f7fbfe267f3?source=collection_archive---------8----------------------->

## [公平和偏见](https://towardsdatascience.com/tagged/fairness-and-bias)

## 对关键问题的一些基本概念的图解介绍

当人工智能成为头条新闻时，往往是因为偏见和公平的问题。一些[最臭名昭著的问题](https://blog.fiddler.ai/2019/11/the-never-ending-issues-around-ai-and-bias-whos-to-blame-when-ai-goes-wrong/)与面部识别、警务和医疗保健有关，但在许多行业和应用中，我们已经看到了机器学习在创造一个一些群体或个人处于不利地位的社会方面的失误。那么，我们如何开发人工智能系统来帮助做出导致公平和公正结果的决策？在 Fiddler，我们发现它始于对人工智能中的偏见和公平的清晰理解。因此，让我们用一些例子来解释我们使用这些术语的意思。

如果你更喜欢看视频而不是读文章，看看这个视频吧！此外，视频中有一些我在本文中提到的偏见类型的具体、真实的例子。

# AI 中的偏见是什么？

偏差可以以多种形式存在，并且可以在模型开发流程的任何阶段引入。从根本上来说，偏见存在于我们周围的世界，并被编码到我们的社会中。我们无法直接解决世界上的偏见。另一方面，我们*可以*采取措施从我们的数据、我们的模型和我们的人工审查过程中剔除偏见。

![](img/d8b269487fa74932bcf2bd5288f5b38f.png)

作者图片

# 数据偏差

数据中的偏差有几种表现形式。以下是罪魁祸首:

历史偏见是世界上已经存在的偏见，已经渗透到我们的数据中。即使给定完美的采样环境和特征选择，这种偏差也可能发生，并且倾向于出现在历史上处于不利地位或被排斥的群体中。2016 年的论文“[男人对于计算机程序员就像女人对于家庭主妇](https://arxiv.org/abs/1607.06520)”说明了历史偏见，该论文的作者表明，在谷歌新闻文章上训练的单词嵌入展示了事实上延续了社会中基于性别的陈规定型观念。

**表示偏差**稍有不同——这发生在我们定义和抽样人口以创建数据集的方式上。例如，[用于训练亚马逊面部识别的数据大多基于白人面孔](https://www.theverge.com/2019/1/25/18197137/amazon-rekognition-facial-recognition-bias-race-gender)，导致检测深色皮肤面孔的问题。代表性偏差的另一个例子是通过智能手机应用程序收集的数据集，这最终可能会低估低收入或老年人口的代表性。

**测量偏差**发生在选择或收集用于预测模型的特征或标签时。容易获得的数据通常是实际特征或感兴趣标签的嘈杂代理。此外，测量过程和数据质量往往因群体而异。随着人工智能被用于越来越多的应用，如预测性警务，这种偏见可能会对人们的生活产生严重的负面影响。[在 2016 年的一份报告中，ProPublica 调查了预测性警务](https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing)并发现，在预测累犯(某人将再次犯罪的可能性)中使用代理测量可能会导致黑人被告比白人被告因同样的罪行获得更严厉的判决。

# 建模偏差

即使我们有完美的数据，我们的建模方法也会引入偏差。这有两种常见的表现方式:

**评估偏差**发生在模型迭代和评估期间。使用训练数据来优化模型，但是它的质量通常是根据某些基准来衡量的。当这些基准不代表总体，或者不适合模型的使用方式时，就会产生偏差。

**聚合偏差**在模型构建过程中出现，不同的群体被不恰当地组合在一起。在许多人工智能应用中，感兴趣的群体是异质的，单一模型不可能适合所有群体。医疗保健就是一个例子。为了诊断和监测糖尿病，模型在历史上使用血红蛋白 AIc(hba1c)的水平来进行预测。然而，[2019 年的一篇论文](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7282707/)显示，这些水平在不同种族之间以复杂的方式存在差异，适用于所有人群的单一模型必然会出现偏差。

# 人类评论中的偏见

即使您的模型做出了正确的预测，当人类评审员决定是接受还是忽略模型的预测时，他们也会引入自己的偏见。例如，一个人类评审员可能会基于他们自己的系统偏差推翻一个正确的模型预测，说一些大意为“我知道那个人口统计，他们从来没有表现得很好。”

# 什么是公平？

在许多方面，人工智能中的偏见和公平是一个硬币的两面。虽然公平没有普遍认同的定义，但我们可以广义地将公平定义为没有基于个人或群体特征的偏见或偏好。记住这一点，让我们看一个机器学习算法如何遇到公平问题的例子。

# 人工智能示例中的公平性

假设我们创建了一个二元分类模型，我们相信我们有非常准确的预测:

![](img/6b91e1269ff68cc61b59ec4206ade6ee.png)

作者图片

如果这个数据实际上包括两个不同的潜在群体，一个绿色群体和一个蓝色群体，那么公平性就会受到质疑。这些组可以代表不同的种族、性别，甚至是地理或时间上的差异，比如早晚用户。

![](img/5467d63409a9239ce986eb3df30bb6b1.png)

作者图片

这种公平性问题可能是聚合偏差的结果，就像前面提到的将糖尿病患者作为一个同质组进行治疗的例子一样。在这种情况下，对这两类人群使用单一阈值会导致不良的健康结果。

# 最佳实践

为了公平地解决这个问题，最佳实践是确保您的预测对每个组都经过了**校准**。如果你的模型的分数没有针对每一组进行校准，很可能你系统地高估或低估了你的一个组的结果的概率。

除了组校准之外，您可以决定为每个组创建单独的模型和决策界限。对于我们这些群体来说，这比单一门槛更公平。

![](img/298eef657e22276a221256cf6ba2cd6c.png)

作者图片

然而，创造额外的门槛会导致一个新的问题:**个人公平**。在下面的图表中，黑色箭头指向蓝色和绿色的个体，它们拥有非常相似的特征，但被 AI 系统完全不同地对待。正如你所看到的，在群体公平和个人公平之间经常存在一种紧张关系。

![](img/e4f25401eee17fa1c51ae069af6cc4c2.png)

作者图片

如图所示，人工智能中的偏见和公平是一个发展中的领域，越来越多的公司，尤其是受监管行业的公司，正在加大投资，以建立治理良好的实践。在以后的帖子中，我将介绍更多关于公平的概念，包括交叉公平，并展示如何使用不同的度量来解决这些问题。

**参考文献:**

鲁本·宾斯。论个体公平与群体公平的表面冲突。更正，abs/1912.06883，2019。网址 http://arxiv . org/ABS/1912.06883。

乔伊.波伦维尼和蒂姆尼特.格布鲁。性别差异:商业性别分类的交叉准确性差异。在公平、问责制和透明度会议上，第 77-91 页，2018 年。

Ninareh Mehrabi、Fred Morstatter、Nripsuta Saxena、Kristina Lerman 和 Aram Galstyan。机器学习中的偏见和公平问题综述。更正，abs/1908.09635，2019。网址[http://arxiv.org/abs/](http://arxiv.org/abs/)1908.09635。