# 为什么你的神经网络以不同的方式不确定

> 原文：<https://towardsdatascience.com/why-your-neural-net-is-uncertain-in-different-ways-a125ce1fa4e5?source=collection_archive---------26----------------------->

## 区分不确定性有助于我们更好地理解神经网络。

![](img/3c5253386b42456d3de0628fcf2d885f.png)

作者图片。由 [phatplus](https://www.flaticon.com/authors/phatplus) 和 [smashicons](https://www.flaticon.com/authors/smashicons) 制作的图标。

欧洲网络充满了不确定性。在通过神经网络提供输入以检索输出之后，我们不能确定我们得到的输出是对现实的正确描述。此外，神经网络的不确定性需要分成两种不同的类型:

> **1。任意的不确定性:***不*用更多的数据解决。
> 
> **2。认知的不确定性:**随着更多的数据变得更好。

将不确定性分成这两个独立的部分，可以更好地理解 ***神经网络实际上是如何学习*以及学习什么**。此外，处理这两种类型的不确定性需要非常不同的技术，我们稍后会看到。

但首先要做的是。让我们慢慢开始，用一个例子让问题更具体。

假设我们正在编写一个医疗软件来预测病人患心脏病的风险。为了进行预测，我们使用一个神经网络，该网络将患者的数据作为输入，包括*年龄、身高和体重。*作为输出，网络产生一个百分比，例如 *2 %* ，意味着患者在未来 10 年内有 *2 %* 的几率患心脏病。

![](img/fb943738013b24a9c46f5febf80f7adf.png)

不用说，我们认为我们做了所有的事情。我们有一个高质量的数据集来进行训练，将数据分成训练、验证和测试部分，并设计和评估了多个架构。结果，我们最终得到了一个神经网络，我们认为它能尽可能好地预测心脏病发作的风险。

现在，病人彼得走了过来。我们把彼得的数据输入我们的神经网络，它会吐出 40 %的心脏病发作风险！这是一个非常高的风险，彼得想知道我们的预测有多确定，这是可以理解的。

![](img/f25b310979ddd95a243bff480c18a5fb.png)

我们预测的不确定性可能有两个原因:

1.与彼得有相同数据(年龄、身高、体重)的人可能有非常不同的心脏病发作风险。我们的网络输出的只是所有这些潜在风险的平均值。比平均值更现实的是可能风险的概率分布。这个分布越分散(方差越大)，我们预测的不确定性就越高。这就是所谓的 ***任意不确定性。***

![](img/cf428da6aa5eb88ae1db5fa62d1be6f2.png)

2.不确定性的第二个潜在来源如下:也许 Peter 的数据相当特殊，在培训期间，我们没有遇到任何类似的数据点，或者像他这样的数据点非常少。因此，基本上输入对我们的神经网络来说是陌生的，它没有任何线索。所以它只输出 40 %，因为它必须给出一些输出。这种不确定性与任意不确定性非常不同，被称为 ***认知不确定性。***

知道这两种不确定性和它们之间的区别对我们给 Peter 建议没有帮助。神经网络预测出 40 %的风险，要么接受，要么放弃。我们没有机会弄清楚神经网络是确定无疑还是毫无头绪。

**那么，我们如何才能建立一个神经网络来告诉我们它有多确定呢？因为随机的和认知的不确定性是如此不同，我们需要用不同的技术来解决它们。**

# 处理任意的不确定性

记住，随机不确定性是数据固有的不确定性。无论我们收集了多少训练数据，总会有年龄、身高和体重相同但心率风险不同的人。**因此，我们改变神经网络来输出概率分布，而不是对每个输入进行单一预测。**

![](img/254f44ad7cf6173438e48b53510424d3.png)

我们如何做到这一点？首先我们选择一种分布类型，例如正态分布 **N(μ，σ )** 。正态分布有两个参数，均值，方差 **σ** 。

现在，我们的神经网络不再产生单一的心脏风险百分比，而是将它改为输出平均值*和方差**的值*****。**

**![](img/d18460d4f77770fa43dc7b7ea9d11c66.png)**

**然后，以这样的方式调整损失函数，即训练网络的输出和 **σ** 最大化观察训练数据的可能性。本质上，我们的神经网络只预测了平均值，但现在它额外预测了来自数据的方差**——随机不确定性。****

> *******趣闻:*** 最小化 MSE 损失(均方误差损失)和最大化关于分布 N(μ，1)的似然性是一样的，意味着方差σ不是学习的而是固定为 1。****
> 
> ****如果你想亲自尝试这些概念，TensorFlow 有一个非常棒的扩展， [Tensorflow Probability](https://www.tensorflow.org/probability?hl=en) 。你可以选择你的发行版，图书馆会处理所有其他的事情。****

****回到我们的不确定性。我们让我们的神经网络告诉我们心脏风险百分比以及它所看到的数据中的不确定性。但是如果没有足够的数据呢？回到我们的例子，如果 Peter 的数据是特殊的，并且**训练数据只包含非常少的与 Peter 相似的数据点，那该怎么办？然后我们的网络会给我们一些随机的风险百分比和随机的方差，但是 T2 基本上不知道。**这直观地解释了为什么任意不确定性和认知不确定性是独立的，以及为什么我们必须分别处理认知不确定性。****

# ****处理认知不确定性****

****认知不确定性是*不确定性，由于信息不完整*，由于没有看到所有的数据。在大多数真实世界的场景中，我们手头永远不会有关于我们问题的所有数据。因此，一些认知上的不确定性将永远存在。尽管如此，认知的不确定性随着更多的数据而减少。****

****让我们暂停一会儿，想想我们如何能够对我们神经网络的当前认知不确定性进行建模。我们实际上不确定什么？如果给我们更多的数据，会有什么变化？****

> ****答案就在我们面前。**我们网络的权重。随着更多的数据，我们的权重将会改变。我们不确定自己的体重。**那么，我们不使用固定的数字作为权重，而是使用适当的概率分布来模拟当前认知的不确定性，如何？****

****这正是贝叶斯神经网络(BNNs)的本质。**bnn 将权重视为概率分布，而非数字。******

****![](img/e6ba52cff228c900078c12663fdb35bf.png)****

****他们使用贝叶斯推理用更多的数据更新这些权重概率分布(这就是这个名字的由来)。训练它们的成本更高，但除了网络的预测，我们还得到一个数字，告诉我们我们的网络在认知上有多不确定。****

****如果你想试试贝叶斯神经网络， [Tensorflow Probability](https://www.tensorflow.org/probability) 支持它们。****

# ****结论****

****神经网络的输出总是带有不确定性。此外，神经网络的不确定性可能是由于数据中的方差(随机不确定性)或由于没有看到所有数据(认知不确定性)。这两种类型的不确定性都可以用它们自己的技术来解决和量化。****

****医学预测，就像我们例子中的彼得的心率风险，只是了解神经网络实际上有多不确定的一个重要方面。在安全关键的环境中，人类的生命往往处于危险之中，概率深度学习技术可以使神经网络更加安全和可靠。****

****觉得这个故事有趣？你可以在这里成为一个中等会员来支持我的写作:[medium.com/@mmsbrggr/membership](https://medium.com/@mmsbrggr/membership)。你将获得所有媒体的访问权，你的部分会员费将直接支持我的写作。****

****欢迎在 [LinkedIn](https://www.linkedin.com/in/marcel-moosbrugger/) 上向我提出私人问题和评论。如果你喜欢这篇文章，让我告诉你我的简讯:【marcelmoos.com/newsletter。****

****如果你想更深入地了解概率深度学习，请查看以下资源:****

*   ****[tensor flow Probability 中的概率层回归](https://blog.tensorflow.org/2019/03/regression-with-probabilistic-layers-in.html) — TensorFlow 博客****
*   ****[我的深度模特不知道的……](https://www.cs.ox.ac.uk/people/yarin.gal/website/blog_3d801aa532c1ce.html)—亚林·加尔****