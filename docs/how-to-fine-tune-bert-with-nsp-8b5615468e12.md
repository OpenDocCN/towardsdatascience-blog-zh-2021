# 如何用 NSP 微调伯特

> 原文：<https://towardsdatascience.com/how-to-fine-tune-bert-with-nsp-8b5615468e12?source=collection_archive---------13----------------------->

## 使用变压器和 PyTorch 轻松微调

![](img/29e02790127127fa914a93b8cccb22f9.png)

硬拉，伯特的最爱——作者图片

下一句预测(NSP)是 BERT 模型(另一个是掩蔽语言建模——MLM)后面的训练过程的一半。

尽管 NSP(和 MLM)被用来预先训练 BERT 模型，我们可以使用这些精确的方法来微调我们的模型，以更好地理解我们自己的用例中特定的语言风格。

因此，在本文中，我们将详细介绍如何获取非结构化的文本，并使用 NSP 对 BERT 模型进行微调。

# 培养

那么，我们如何使用 NSP 对模型进行微调呢？

首先，我们需要数据。因为我们本质上只是在连续句和随机句之间切换——我们可以使用几乎任何文本。我们不需要有标签的或特殊的数据。

这里，我们将使用*马库斯·奥勒留*的*冥想*，来源于[这里](http://classics.mit.edu/Antoninus/meditations.html)并稍加预处理([干净版](https://github.com/jamescalam/transformers/blob/main/data/text/meditations/clean.txt))。

首先，我们将导入/初始化并加载我们的文本数据。

我们需要在标记化之前对数据进行预处理，这样我们就可以得到一组随机和非随机的句子对。

## NSP 预处理

首先，我们将创建一个随机句子的`bag`——我们将根据句点字符对其进行拆分。

这就给我们留下了 1372 个句子样本。从这里开始，我们需要创建句子对，并标记它们是连续的句子对，还是随机的句子对。

我们可以在最后一个细胞的输出中看到我们创建的两对细胞。第一个标记为 *1* 并显示一个`NotNextSentence`对，第二个标记为 *0* 并显示一个`IsNextSentence`对。

## 标记化

我们的数据现在可以进行标记化了，这次我们将每个序列截断/填充到 512 个标记的相同长度。

在 *token_type_ids* 张量中，我们可以看到同样的 *0* 后面跟着 *1* 记号(分别是句子 A 和 B)，后面跟着更多的 *0* 记号。这些用于指示填充标记。

我们还需要一个*标签*张量，它将对应于包含在`label`变量中的值。我们的*标签*张量——和以前一样——必须是一个`torch.LongTensor`。

## 输入管道

现在我们的输入张量已经准备好了，我们可以开始为训练构建模型输入管道。我们首先从数据中创建 PyTorch 数据集。

并初始化数据加载器，这是我们在训练中将用来将数据加载到模型中的。

## 培训设置

现在，在开始训练之前，我们设置最后几件事情。我们将模型移动到 GPU(如果可用)，激活训练模式，并初始化我们的优化器。

我们使用 Adam with weighted decay 作为我们的优化器，这是训练变压器模型的常见选择，因为它减少了过度拟合的可能性(由于大多数变压器的巨大尺寸，这是一个常见的问题)。

## 训练循环

最后，我们可以进入训练循环。我们将训练几个时期来展示损失随时间的变化。

就这样，我们用 NSP 微调了我们的伯特模型！

这就是这篇关于用 NSP 微调伯特的文章。

NSP 有很多，但是概念和实现并不太复杂——但是功能非常强大。

使用我们在这里学到的知识，我们可以采用 NLP 中最好的模型，并对它们进行微调以适应我们更特定于领域的语言用例——只需要未标记的文本——通常是很容易找到的数据源。

我希望你喜欢这篇文章！如果你有任何问题，请通过 [Twitter](https://twitter.com/jamescalam) 或在下面的评论中告诉我。如果你想要更多这样的内容，我也会在 [YouTube](https://www.youtube.com/c/jamesbriggs) 上发布。

感谢阅读！

# 参考

[1] J .德夫林等人。艾尔。， [BERT:语言理解深度双向转换器预训练](https://arxiv.org/pdf/1810.04805.pdf) (2019)，NAACL

[🤖《变形金刚》NLP 课程 70%的折扣](https://bit.ly/nlp-transformers)

如果你有兴趣了解更多关于 NSP 到底是如何运作的——看看这篇文章:

[](/bert-for-next-sentence-prediction-466b67f8226f) [## 用于下一句预测的 BERT

### 另一半用来训练伯特

towardsdatascience.com](/bert-for-next-sentence-prediction-466b67f8226f) 

**所有图片均由作者提供，除非另有说明*