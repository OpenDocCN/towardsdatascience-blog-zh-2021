# 脸书和微软数据科学家编码问题

> 原文：<https://towardsdatascience.com/facebook-microsoft-data-scientist-coding-questions-94c8ae942a90?source=collection_archive---------21----------------------->

![](img/1ad2898aefc0436f7a12481e27a72609.png)

作者原创图片

对于所有有志成为数据科学家的人来说，编码测试通常是你在招聘过程中首先遇到的事情之一。任务通常因公司而异，但是一旦你进行了足够多的编码测试，你就会开始注意到类似的技能被测试。

今天，我们将探讨如何利用过滤、分组和聚合技术来回答脸书和微软编码测试中出现的一些真正的 Python 任务！

这是我们今天要分解的两项任务:

1.**脸书**:用户评论活跃度分布

2.**微软**:按日期进行用户分组分析

我以前也写过关于顶级 SQL 问题的文章，所以请在这里的(第一部分)和这里的(第二部分)查看它们！

这里有一个对 StrataScratch 的人们的大喊，它启发了今天的问题——如果你正在寻找一个你可能在编码测试中实际面对的现实生活问题的宝库，请检查它们！

**问题 1:评论数量的分布**

在这个问题中，我们有两个数据集:

**fb_users** ，以下列:

*   id (int64)
*   名称(字符串)
*   加入时间(日期时间)

```
|----+----------+---------------------|
| id | name     | joined_at           |
|----+----------+---------------------|
| 1  | James    | 2020-06-30 00:00:00 |
| 2  | John     | 2020-06-29 00:00:00 |
| 3  | Zac      | 2020-05-21 00:00:00 |
...
```

**fb_comments** ，以下列:

*   user_id (int64)
*   正文(字符串)
*   创建时间(日期时间)

```
|---------+------------------+---------------------|
| user_id | body             | created_at          |
|---------+------------------+---------------------|
| 1       | wow, great...    | 2020-07-01 00:00:00 |
| 2       | happy birthd...  | 2020-06-28 00:00:00 |
| 3       | thank you eve... | 2020-06-22 00:00:00 |
...
```

> 根据 2018 年至 2020 年间加入脸书的用户数量，编写一个查询来计算 2020 年 1 月的评论分布。输出应该包含评论的数量以及在 2020 年 1 月发表评论的用户数量。例如，你可以计算有多少用户发表了 1 条评论、2 条评论、3 条评论、4 条评论等等。2020 年 1 月。输出中的左栏是评论的数量，右栏是用户的数量。从注释数量最少到最多对输出进行排序。
> 
> 更复杂的是，可能会有一个 bug，用户帖子的日期在用户加入日期之前。您需要从结果中删除这些帖子。(例如，参考上面 user_id 2 的条目，其中注释日期在用户的加入日期之前)

当处理需要处理数据以呈现某种形式的分布的问题时，将任务分解成一系列步骤通常是有用的。许多受访者都熟悉在数据处理过程中需要的各种函数、连接和过滤器，但最具挑战性的是决定处理数据时要采取的步骤的确切顺序。我要先加入数据吗？还是我先过滤数据？这有关系吗？

**向后工作**

开始思考这类问题的一个好方法是逆向工作。我们的预期输出要求我们根据评论的数量返回用户的频率。首先忽略应该在问题的上下文中应用的条件和过滤器，我们知道，为了获得这个结果，我们首先需要获得一个包含每个用户的评论数量的先验表。为了创建这个中间表，我们需要获得一个 prior 表，其中包含每个用户的每个评论的一行。请注意，分发任务总是涉及在最后阶段应用 groupbys，这正是我们打算应用于每个中间数据集的。但是当我们逆向工作时，我们如何知道何时停止呢？

诀窍是问自己，已经反向推导的数据集是否已经在问题的上下文中直接可用，或者通过不涉及分组的某种形式的数据操作(如连接)间接可用。

在这个例子中，我们的中间数据集要求每个用户的每个评论占一行，结果我们已经有了原始数据集，只是没有应用必要的条件。这个数据集是 **fb_comments** ，每个评论占一行，但既不过滤 2020 年 1 月的评论，也不过滤 2018 年至 2020 年加入的用户的评论。

为了引入关于用户加入日期的信息，我们必须将 **fb_users** 数据集与 **fb_comments** 数据集合并，因为变量 **joined_at** 最初包含在用户数据集中。这里，我们使用一个**内部连接**来合并数据集，因为我们打算为每个评论保留每一行，即使用户在数据集中有多个评论。受访者通常会有这样的误解，即内部连接要求被连接的键(在本例中是各自数据集中的“id”和“user_id ”)分别是唯一的。这不是真的。事实上，现实中发生的事情是，当连接重复的键时，结果类似于您从**交叉连接、**或笛卡尔积**中获得的结果。**事实上，将内部连接看作是交叉连接的简化更有用，只是以匹配键值为条件。在下图中，我们看到内部连接中的重复键产生匹配键的所有组合。

![](img/3ba7c7986173f278fa6532b2295009a8.png)

作者插图

在问题的上下文中，我们不必担心连接后的重复结果，因为我们知道 fb_users 数据集中的 **id** 变量已经是惟一的了。使用 pandas 数据框架，我们的第一步是生成 **user_comments** 作为内部连接产品。

```
user_comments = pd.merge(fb_users, fb_comments, how='inner', left_on=['id'], right_on=['user_id'])
```

现在我们已经介绍了用户加入日期的信息，我们可以过滤属于从 2018 年起加入的用户的评论。我们还想考虑到评论日期在加入日期之前可能出现的数据错误。这些简单的过滤器可以使用:

```
**# filtering out comments that are dated before user joined** user_comments = user_comments[user_comments['created_at'] >= user_comments['joined_at']]**# only including comments from users who joined 2018 onwards** user_comments = user_comments[user_comments['joined_at'] >= '01-01-2018']
```

接下来，我们可以过滤 2020 年 1 月发布的评论。我们可以选择直接使用现有的日期时间变量 **created_at** 以下面的方式过滤值，该方式利用了 **between** 方法:

```
user_comments = user_comments[user_comments['created_at'].between('2020-01-01', '2020-01-31')]
```

或者，我们可以使用[**strftime**](https://docs.python.org/3/library/datetime.html)**方法将日期时间变量转换为指定格式为“%Y-%m”的字符串(例如，“2019–12”)。我们称这个新变量为' **created_yr_mnth** '。随后，我们可以通过筛选“created_yr_mnth”的值等于“2020–01”来选择 2020 年 1 月的注释。**

```
user_comments['created_yr_mnth'] = user_comments['created_at'].dt.strftime('%Y-%m')data = user_comments[(user_comments['created_yr_mnth'] == '2020-01')]
```

***注意:在应用了与日期相关的过滤器之后，只使用 strftime 是有意义的，因为我们希望保留 datetime 数据类型，以便应用 between 和≥过滤器。***

**这样，我们获得了我们的中间数据集 **df** ，我们可以开始实现我们在本练习开始时确定的两个 groupbys。**

**首先，我们根据 **id** 对数据进行分组，以便统计每个用户的评论数量。我们重置索引并调用新的变量，其中每个用户的评论数为 **comment_cnt** 。**

```
data = data.groupby('id')['user_id'].count().reset_index(name='comment_cnt')
```

**最后，我们通过按这个 **comment_cnt** 分组来计算对应于每个 comment_count 值的用户数量，从而获得结果。不要忘记根据任务的要求，根据 **comment_cnt** 对**的值**进行排序！**

```
result = data.groupby('comment_cnt')['id'].count().reset_index(name='user_cnt').sort_values('comment_cnt', ascending=True)
```

**你们中的一些人可能想知道过滤的顺序是否真的重要——在这个问题的上下文中，它实际上基本上无关紧要。例如，我们可以在合并关于用户的数据之前先过滤 2020 年 1 月的评论，这实际上可能是一种更有效的方法，因为合并将涉及更少的基于关键字的匹配行。我们可以根据 2018 年至 2020 年的用户加入日期进行同样的过滤。**

**然而，重要的是理解在这种情况下，过滤必须在分组之前完成。相反，如果我们决定过滤掉那些评论日期早于他们加入日期的用户，我们可能会错过那些有一些有效评论的用户。因此，在这种情况下，我们必须认识到过滤是在评论级别进行的，而不是在用户级别，因为 groupby 是在用户级别进行的，所以过滤应该在 groupby 完成之前进行。然而，情况并非总是如此，我们很快就会在问题 2 中看到这一点。**

****问题 2:付费与免费增值****

**这个问题的相关数据集是:**

****ms_user_dimension:****

*   **user_id (int)**
*   **acc_id (int)**

```
|---------+--------|
| user_id | acc_id |
|---------+--------|
| 1       | 716    |
| 2       | 749    |
...
```

****ms_acc_dimension:****

*   **acc_id (int)**
*   **付款客户(字符串)—'是'/'否**

```
|---------+----------------|
| acc_id  | paying_customer|
|---------+----------------|
| 716     | no             |
| 749     | yes            |
...
```

****ms_download_facts:****

*   **日期(时间)**
*   **user_id (int)**
*   **下载量(整数)**

```
|----------------------+---------+-----------|
| date                 | user_id | downloads |
|----------------------+---------+-----------|
| 2020-08-24 00:00:00  | 1       | 6         |
| 2020-08-22 00:00:00  | 2       | 2         |
| 2020-08-24 00:00:00  | 3       | 7         |
...
```

> **按日期查找付费和非付费用户的下载总数。只包括非付费用户比付费用户下载更多的记录。输出应首先按最早日期排序，并包含 3 列日期，非付费下载，付费下载。**

**就像在问题 1 中一样，我们看到我们需要采用某种形式的分组和分组计数。具体来说，我们将不得不首先按日期对下载进行分类，并根据付费和非付费下载对每个日期进行计数。**

**然而，与问题 1 不同，我们的过滤必须在分组完成后进行。我们是怎么知道的？我们看到，该任务要求我们根据两个变量之间的关系进行过滤，这两个变量包含每天付费和非付费下载次数的信息。更一般地说，过滤是基于不存在于原始表中但存在于某个中间表中的值来完成的。因此，简单地检查我们应该过滤哪个变量为我们提供了一个很好的框架，让我们思考应用于数据集的适当步骤序列。**

**反过来，我们知道，为了按日期分组并统计付费和非付费用户下载的次数，原始未分组数据必须包含每个下载事务的一行数据，其中包含以下信息:( I)下载日期;( ii)下载的用户是付费还是非付费用户;( iii)下载次数。这段代码完成了这个任务:**

```
user_acc = pd.merge(ms_user_dimension, ms_acc_dimension, how = 'left',left_on = ['acc_id'], right_on=['acc_id'])data = pd.merge(user_acc, ms_download_facts, how = 'left',left_on = ['user_id'], right_on=['user_id'])
```

**首先，我们将用户-帐户对(ms_user_dimension)与关于它是否付费的帐户细节(ms_acc_dimension)合并，以获得 df。请注意，我们希望使用一个**左连接**，因为我们主要关心的是保留所有现有的**用户 id，**，因为我们将使用那个键来匹配关于下载的信息。第二，我们将用户信息(现在附加有关于支付的信息(df))与下载信息(ms_download_facts)合并，以获得 df1。同样，我们使用了一个**左连接**，因为我们想要保留所有的下载行，对于下载不止一次的用户，我们想要*复制他们关于支付的信息*。**

**这里有一个问题:在这种情况下，合并的顺序真的重要吗？这将测试您对连接实际工作方式的理解。**

**接下来，与前面直接使用 groupby 的例子不同，让我们探索一种不太常用的方法，用于 pandas 数据帧，称为 **pivot_table** 。对于那些熟悉 Excel 中数据透视表的人来说，这个函数的工作方式基本相同。**

**注意 **pandas.pivot_table** 的以下参数:**

*   ****index** :我们希望对结果数据透视表中每一行的数据进行分组的字段(一行一个日期)**
*   ****列**:我们希望对结果数据透视表中每一列的数据进行分组的字段(一列用于支付，一列用于不支付)**
*   ****值**:我们希望根据上述类别(下载次数)执行聚合功能的字段**
*   ****aggfunc** :聚合函数(在本例中是对下载次数求和)**

**综上所述，下一步应该是这样的:**

```
pivot_data = data.pivot_table(index=['date'],columns=['paying_customer'],values=['downloads'],aggfunc='sum')
```

**为了将日期的索引移出到第一个字段，我们可以使用方法 [**to_records**](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_records.html) **()。**然而，因为这会将我们的表转换成 *numpy 数组*，所以我们必须以下面的方式将其转换回 pandas dataframe:**

```
payment_count_by_date = pd.DataFrame(pivot_data.to_records())
```

**我们还通过用空字符串替换所有括号、撇号、逗号和单词“download”来清理这个结果数据帧中的列，以便我们的列将只是 date，no，yes。这一行可以达到目的:**

```
payment_count_by_date.columns = payment_count_by_date.columns.str.replace("[()]","").str.replace("[' ']","").str.replace("[,]","").str.replace("downloads","")**# replace null values with 0** payment_count_by_date = payment_count_by_date.fillna(0)
```

**现在，请注意，这正是我们想要执行筛选的表，因此我们知道我们的分组和聚合现在已经完成。**

**这个问题中需要的剩余过滤器是只包括非付费客户比付费客户下载更多的记录。实现这一点最直接的方法是首先创建一个名为 **diff** 的新列，该列获取非付费下载的数量减去付费下载的数量。然后我们过滤那些 **diff = no-yes > 0，**的行，最后按日期对结果排序，得到我们的最终结果！**

```
**# constructing diff variable** payment_count_by_date['diff'] = payment_count_by_date['no']-payment_count_by_date['yes'] **# filtering** payment_count_by_date = payment_count_by_date[payment_count_by_date["diff"] > 0]result = payment_count_by_date[["date","no","yes"]].sort_values("date")
```