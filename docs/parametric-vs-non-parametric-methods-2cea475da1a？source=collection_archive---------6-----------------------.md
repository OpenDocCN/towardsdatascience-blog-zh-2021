# 机器学习中的参数方法与非参数方法

> 原文：<https://towardsdatascience.com/parametric-vs-non-parametric-methods-2cea475da1a?source=collection_archive---------6----------------------->

## 讨论机器学习中参数方法和非参数方法的区别

![](img/a6d2771e01f5c96001628423f9357c14.png)

Alex Padurariu 在 [Unsplash](https://unsplash.com/s/photos/table?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText) 上拍摄的照片

## 介绍

在我之前的一篇文章中，我在统计学习的背景下讨论了预测和推断之间的[差异。尽管它们在最终目标上有很大不同，但在这两种方法中，我们都需要估计一个未知函数 *f* 。](/inference-vs-prediction-b719da908000)

换句话说，我们需要学习一个将输入(即自变量 X 的集合)映射到输出(即目标变量 Y)的函数，如下所示。

> Y = f(X) + ε

为了估计未知函数，我们需要在数据上拟合一个模型(*训练数据更精确些*)。我们试图估计的函数的形式通常是未知的，因此我们可能不得不应用不同的模型以达到目的，或者对函数 *f* 的形式做出一些假设。一般来说，该过程可以是**参数化或非参数化**。

在今天的文章中，我们将讨论机器学习环境中的参数和非参数方法。此外，我们将探讨它们的主要区别以及它们的主要优点和缺点。

## 参数方法

在参数方法中，我们通常**对函数*f*的形式做出假设。例如，你可以假设未知函数 *f* 是线性的。换句话说，我们假设函数的形式为**

> f(X) = β₀ + β₁ X₁ + … + βₚ Xₚ

其中 *f* (X)为待估计的未知函数， *β* 为待学习的系数， *p* 为自变量的个数， *X* 为相应的输入。

现在，我们已经对要估计的函数的形式做出了假设，并选择了与该假设一致的模型，我们需要一个学习过程，该过程最终将帮助我们训练模型并估计系数。

总之，机器学习中的参数方法通常采用基于模型的方法，其中我们对要估计的函数的形式做出假设，然后我们基于该假设选择合适的模型，以便**估计参数集**。

参数方法最大的缺点是我们做出的 T2 假设并不总是正确的。例如，你可能假设函数的形式是线性的，而它不是。因此，这些方法涉及不太灵活的算法，通常用于不太复杂的问题。

然而，**参数方法往往很快**，而且与非参数方法相比**需要的数据**也少得多(下一节将详细介绍)。此外，由于参数方法往往不太灵活，不适合不太复杂的问题，它们**更容易解释**。

机器学习中的参数方法的一些**例子包括线性判别分析、朴素贝叶斯和感知器。**

## 非参数方法

另一方面，非参数方法指的是一组算法，这些算法**不会对要估计的函数形式做出任何潜在假设**。并且由于没有进行假设，这种方法能够估计可能是任何形式的未知函数 *f* 。

非参数方法往往更准确，因为它们寻求最佳拟合数据点。然而，这是以**需要非常大量的观测值**为代价的，这些观测值是精确估计未知函数 *f* 所需要的。此外，在训练模型时，这些方法往往效率较低。此外，非参数方法有时可能会引入过度拟合。由于这些算法往往更加灵活，它们有时可能会以一种无法很好地推广到新的、看不见的数据点的方式来学习错误和噪声。

另一方面，非参数方法非常灵活，可以带来更好的模型性能，因为没有对基础函数做任何假设。

机器学习中非参数方法的一些例子包括支持向量机和 K-最近邻。

## 最后的想法

在今天的文章中，我们讨论了机器学习环境中的参数和非参数方法。此外，我们还探讨了它们的优缺点。

参数方法指的是一组算法，这些算法往往不太灵活、不太准确，但更容易解释，而非参数方法往往更灵活(因此适用于更复杂的问题)，也更准确，但不太容易解释。

尽管参数方法不太灵活，有时不太准确，但它们在许多用例中仍然有用，因为在较简单的问题中使用相当灵活的非参数方法可能会导致过度拟合。

[**成为会员**](https://gmyrianthous.medium.com/membership) **阅读介质上的每一个故事。你的会员费直接支持我和你看的其他作家。**

**你可能也会喜欢**

[](/inference-vs-prediction-b719da908000) [## 推断和预测的区别是什么

### 在统计学习的背景下讨论预测和推断的区别

towardsdatascience.com](/inference-vs-prediction-b719da908000) [](/feature-scaling-and-normalisation-in-a-nutshell-5319af86f89b) [## 简而言之，特征缩放和标准化

### 为什么、如何以及何时调整要素的比例

towardsdatascience.com](/feature-scaling-and-normalisation-in-a-nutshell-5319af86f89b) [](/mastering-indexing-and-slicing-in-python-443e23457125) [## 掌握 Python 中的索引和切片

### 深入研究有序集合的索引和切片

towardsdatascience.com](/mastering-indexing-and-slicing-in-python-443e23457125)