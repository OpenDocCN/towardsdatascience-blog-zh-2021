# 自动化武器的高成本

> 原文：<https://towardsdatascience.com/the-high-cost-of-automated-weapons-7c07dc9e186f?source=collection_archive---------32----------------------->

## [播客](https://towardsdatascience.com/tagged/tds-podcast)

## 雅各布·福斯特谈人工智能驱动的杀手无人机

[苹果](https://podcasts.apple.com/ca/podcast/towards-data-science/id1470952338?mt=2) | [谷歌](https://www.google.com/podcasts?feed=aHR0cHM6Ly9hbmNob3IuZm0vcy8zNmI0ODQ0L3BvZGNhc3QvcnNz) | [SPOTIFY](https://open.spotify.com/show/63diy2DtpHzQfeNVxAPZgU) | [其他](https://anchor.fm/towardsdatascience)

*编者按:这一集是我们关于数据科学和机器学习新兴问题的播客系列的一部分*，*由 Jeremie Harris 主持。除了主持播客，Jeremie 还帮助运营一家名为*[*sharpes minds*](http://sharpestminds.com)*的数据科学导师初创公司。*

自动化武器意味着更少的伤亡、更快的反应时间和更精确的打击。对任何部署它们的国家来说，这都是一个明显的胜利。你可以看到吸引力。

但它们也是经典的[囚徒困境](https://en.wikipedia.org/wiki/Prisoner%27s_dilemma)。一旦许多国家部署了它们，人类就不再需要被说服投入战斗，引发冲突的障碍也大大降低了。

无人机等自动化武器系统带来的真正风险并不总是显而易见的。其中许多以二阶效应的形式出现——这是建立一个多个国家拥有大型自动化力量的世界所带来的连锁反应。但是我们能做些什么呢？这就是我们在本期播客中与 Jakob Foerster 一起探讨的问题，Jakob Foerster 是多主体强化学习的早期先驱，也是多伦多大学的新任教师。雅各布参与武器化无人机自动化的辩论已经有一段时间了，最近他给德国政界人士写了一封公开信，敦促他们考虑与部署这项技术相关的风险。

以下是我在对话中最喜欢的一些观点:

*   目前，武器系统一般都是在人的参与下建造的，这就防止了未经训练有素的专家批准就发射。虽然这减缓了反应时间，但它在减少意外使用武器攻击错误目标或在错误情况下的机会方面发挥了关键作用。冷战的历史包括许多案例，其中一个人通过中断反应循环避免了灾难——就像俄罗斯海军军官瓦西里·阿尔希波夫的案例。
*   将人类完全排除在反应循环之外，有可能导致一连串被误解的敌人行动导致冲突在人类控制之外迅速升级。这并非没有先例:算法交易导致了类似的失控效应，有时会导致交易暂停和重大价值损失，正如 2010 年闪电崩盘期间[可能发生的那样。](https://en.wikipedia.org/wiki/2010_flash_crash)
*   至关重要的是，人在回路系统还使得高度自动化系统的行动更容易归因于特定的人或权力。将人类从这些决策循环中移除的风险之一是，如果没有一个人具有明确的组织隶属关系，那么就很难判断——即使在原则上——谁应该对给定的攻击或生命损失负责。
*   武器化系统的自动化也对民主原则构成威胁。从历史上看，至少大多数公民的同意——通常是在军队中——对所有治理模式的稳定都至关重要。但自动化系统可能会让一小群人有效地挟持一个国家。因此，公民代表政府的价值将会降低，暴政变得更有可能。
*   我们如何才能最好地避免这些风险？目前的方法侧重于协商无人机等系统的自动化程度。理论是，只要我们让人类参与进来，或者不允许使用自动化来做出某些类型的决策，我们就可以安全地避免这项技术最危险的影响。但 Jakob 不同意这种方法:正如他指出的，一种决策的结束和另一种决策的开始并不总是很清楚，尤其是在涉及机器学习的时候。因此，从“全人类”到“全人工智能”的武器系统有一个连续的范围。由于政府和军队总是面临着向“完全人工智能”方向发展的竞争压力，雅各布担心随着时间的推移，事情会向这个方向发展。
*   因此，雅各布主张进行更清晰的区分。他建议将重点放在武器化上，而不是将“全自动系统”从“其他系统”中分离出来。雅各布认为自治系统应该是合法的，除非它们是武装的。
*   我们的讨论强调了我们之前在播客中讨论过的一个问题:我们政府中制定人工智能政策的人通常不了解他们正在监管的技术。只是因为雅各布的技术专长，他才能够识别使用“自动化程度”作为武器系统合法性的晴雨表所涉及的风险——而类似的错误无疑正在其他同样关键的领域犯下。正如雅各布所言，这就是为什么人工智能研究人员在思考其工作的政策影响方面发挥更积极的作用如此重要。

你可以[在 Twitter 上关注 Jakob 这里](https://twitter.com/j_foerst)，或者 [me 这里](https://twitter.com/jeremiecharris)。

![](img/366741283211d898471b18c0b80f4d6d.png)

## 章节:

*   0:00 介绍
*   2:00 背景故事
*   5:04 技术知识的价值
*   10:45 自动化武器的能力
*   15:05 军备竞赛
*   22:10 这些技术的个人所有权
*   27:05 不稳定的平衡
*   31:05 连接到开源文化
*   36:50 资助政策工作
*   39:10 鼓励沟通
*   41:10 国际合作
*   48:30 弥合知识差距
*   52:00 参与
*   53:45 总结