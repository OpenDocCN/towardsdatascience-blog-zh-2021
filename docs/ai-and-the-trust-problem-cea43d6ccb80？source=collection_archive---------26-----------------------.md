# 人工智能与信任问题

> 原文：<https://towardsdatascience.com/ai-and-the-trust-problem-cea43d6ccb80?source=collection_archive---------26----------------------->

## [播客](https://towardsdatascience.com/tagged/tds-podcast)

## Ayanna Howard 谈人类与人工智能合作的挑战

[苹果](https://podcasts.apple.com/ca/podcast/towards-data-science/id1470952338?mt=2) | [谷歌](https://www.google.com/podcasts?feed=aHR0cHM6Ly9hbmNob3IuZm0vcy8zNmI0ODQ0L3BvZGNhc3QvcnNz) | [SPOTIFY](https://open.spotify.com/show/63diy2DtpHzQfeNVxAPZgU) | [其他](https://anchor.fm/towardsdatascience)

*编者按:TDS 播客由 Jeremie Harris 主持，他是数据科学导师初创公司 SharpestMinds 的联合创始人。每周，Jeremie 都会与该领域前沿的研究人员和商业领袖聊天，以解开围绕数据科学、机器学习和人工智能的最紧迫问题。*

在过去的两年里，人工智能系统的能力已经爆炸了。AlphaFold2、MuZero、CLIP、DALLE、GPT-3 和许多其他模型已经将人工智能的范围扩展到了新的问题类别。有很多值得兴奋的事情。

但正如我们在播客的其他剧集中看到的，从人工智能系统中获得价值不仅仅是提升其能力。越来越多的是，这些额外的缺失因素之一变成了信任。你可以制造你想要的所有强大的人工智能，但如果没有人信任他们的输出——或者如果人们在不应该的时候信任它——你最终可能弊大于利。

这就是我们邀请 Ayanna Howard 上播客的原因。Ayanna 是一名机器人专家、企业家和俄亥俄州立大学工程学院院长，在那里，她专注于人机交互以及在人工智能系统中建立人类信任的因素的研究。她和我一起谈论她的研究，它在医学和教育方面的应用，以及人机信任的未来。

以下是我在对话中最喜欢的一些观点:

*   只有当一个人相信人工智能能够提供良好的输出时，他才会从人工智能的输出中获得最大的价值。人工智能系统的舒适度和信任度差异很大，很难预测。因此，Ayanna 认为，建立信任不能被框定为一个一刀切的问题，即我们试图对所有用户使用相同的技术。从长远来看，她认为培训和筛选相结合是优化建立信任过程的方法。人工智能支持的分类技术也可以发挥作用，通过预测哪种建立信任的策略更有可能对给定的最终用户起作用。
*   有一种现象叫做 Gell-Mann amnaesia **，**，它指的是我们倾向于相信信息来源，即使我们已经看到他们在我们非常了解的话题上犯了令人尴尬的错误。Ayanna 说这延伸到了人类/人工智能的互动。人类经常过度信任人工智能系统，即使他们已经看到它在一项基本任务或一系列任务中失败。一个成功完成的任务可以弥补几次失败造成的信任赤字。
*   Ayanna 的研究发现，人类倾向于更信任人工智能来完成他们自己最不自信的任务。她怀疑这反映了人类希望将不舒服或具有挑战性的决策交给第三方，而不是对人工智能能力的客观评估。
*   当我们不积极参与人工智能系统帮助我们完成的任务时，我们特别容易过度信任它们。例如，如果一个人工智能系统提供建议的行动，而一个人类操作员只是被要求接受或拒绝这些建议，他们更有可能离开并盲目地不加批判地接受它们。为了抵消这种趋势，Ayanna 正在试验迫使用户在一组可能的选项中进行选择的系统，迫使他们继续参与解决问题的过程。

你可以[在推特上关注 Ayanna](https://twitter.com/robotsmarts)，或者[我在这里](https://twitter.com/jeremiecharris)。

![](img/326c3f67e1b820af9cd23bc3934e9e0b.png)

## 章节:

*   0:00 介绍
*   1:30 Ayanna 的背景
*   6:10 神经网络的解释性
*   12:40 人机交互领域
*   17:00 偏好问题
*   20:50 盖尔曼/报纸健忘症
*   26:35 评估一个人的说服力
*   31:40 医生和新技术
*   责任和问责
*   43:15 社会压力方面
*   47:15 Ayanna 乐观吗？
*   53:00 总结