# 强化学习试卷#11 的每周回顾

> 原文：<https://towardsdatascience.com/weekly-review-of-reinforcement-learning-papers-11-7e1780ddf176?source=collection_archive---------39----------------------->

## 每周一，我都会发表我研究领域的 4 篇论文。大家来讨论一下吧！

![](img/a459be11f1f3c9ea85c758b5923a02bb.png)

作者图片

[ [←上一次回顾](/weekly-review-of-reinforcement-learning-papers-10-be5947715b26?sk=3d43102b9d5c0f070360473c478ce87e) ][ [下一次回顾→](/weekly-review-of-reinforcement-learning-papers-12-9ec3a81720?sk=18228793ad496e86a916a7d890e53634)

# 论文 1:奖励就够了

西尔弗博士、辛格博士、普雷科普博士和萨顿博士(2021 年)。[奖励充足](https://www.sciencedirect.com/science/article/pii/S0004370221000862?casa_token=eBT6fsPFa_0AAAAA:y7xJVFADBC686GrpoqfZXhV5myYxmg5fI4fYVo17h0m2j0Mm4L11SWMd7eidBn_2pHgrYr2wYg)。*人工智能*，103535。

本文提出的假设是，在足够复杂的环境中，报酬最大化是智能出现的充分条件。他们在这篇论文中举了一只松鼠的例子，这只松鼠想要得到尽可能多的坚果。为了实现他的目标，他必须执行许多后续任务:感知他的环境，四处移动，爬树，与其他松鼠交流，理解季节的循环……在他们的假设中，**所有这些后续任务都将被隐含地学习**由于单个奖励的最大化，即获得坚果数量的最大化。

![](img/0b7966b958081999d740880ec8ab8390.png)

图片来自[文章](https://www.sciencedirect.com/science/article/pii/S0004370221000862?casa_token=eBT6fsPFa_0AAAAA:y7xJVFADBC686GrpoqfZXhV5myYxmg5fI4fYVo17h0m2j0Mm4L11SWMd7eidBn_2pHgrYr2wYg):松鼠学会了复杂的行为，这些行为是最大限度地消耗食物所必需的。这也与厨房机器人的例子相关。

他们以他们之前关于国际象棋和围棋的工作为例。代理人只受报酬最大化的训练。经纪人没有被教授年轻球员通常被教授的开局和战术。所以他创造了自己的开口和战术。他们被证明是非常创新的，有时与专家们所习惯的非常不同。阿尔法围棋著名的“37 步棋”就是一次完美的演示，彻底动摇了这一局的所有高手。

这个假设正确吗？单一价值的最大化能产生复杂的智能吗？每个人都有自己的看法。一些人认为这是对科学结果的必要简单性的新证明，而另一些人则认为他们的解释往往是同义反复。而你，你怎么看？

> 一切都应该尽可能简单，但不能再简单了。——爱因斯坦

# 论文 2:Android enforcement 学习平台

Toyama，d .，Hamel，p .，Gergely，a .，Comanici，g .，Glaese，a .，Ahmed，z .，…& pre COPD，D. (2021 年)。[andro idenv:Android 的强化学习平台](https://arxiv.org/abs/2105.13231)。 *arXiv 预印本 arXiv:2105.13231* 。

你知道 gridworld，你知道 Atari，你知道 Mujoco 控制环境。今天，你发现了 androidEnv，一个与 Android 环境交互的 RL 环境。该操作系统完全模拟并封装在 OpenAI gym 环境中，您可以像与该框架的所有其他环境一样与它交互。观察对应于像素矩阵，动作对应于您在触摸屏上的移动。

![](img/a429d44d831e98ed784ac4607a1e9f4c.png)

来自[博客](https://deepmind.com/research/publications/androidenv)的动画:无名氏是我们的新联系人。

这为大量可行的任务开辟了道路。这个环境更加有趣，因为集成了两个主要元素:
(1) **实时**。如你所知，一个代理可以花一些时间来选择它的行动。通常，环境被阻塞，并且正在等待代理的动作。这里的情况不是这样。环境在代理的考虑时间内继续进行。此外，Android 渲染可能需要一些时间:例如，当你滚动网页时，会有一个小的滑动效果，使其变得自然。这已经包括在内，所以代理人将不得不学习适应这种影响和自己的审议时间，使其更难学习。
(2) **原始动作**。动作规范是低级的:一个位置和触/举。就是这样。因此，代理必须学习复杂的手势，比如拖放、敲击或滑动。

现在，让我们把焦点放在你从一开始就在疑惑的问题上:要完成的任务是什么？**奖励函数**是什么？实际上，这是由用户通过操作系统日志来指定的。让我们举一个例子:我希望我的代理将 John Doe 添加到我的联系人中。如果完成此任务，则可能会添加一个“*【信息】【2021/06/07】John Doe”形式的系统日志作为新联系人。“通过检测这种日志的出现，我可以决定何时给我的代理人报酬。是的，这有点复杂，但我想这是他们找到的指定奖励的最干净的方法。*

# 论文 3:走向更深层次的强化学习

Bjorck，j .、Gomes，C. P .和 Weinberger，K. Q .，[走向更深的深度强化学习](https://arxiv.org/abs/2106.01151)， *arXiv 预印本 arXiv:2106.01151* 。

正如你可能已经注意到，在深度强化学习中，最佳神经网络结构的问题很少被问到。基本上，如果观察是一个图像，我们用 CNN，如果不是，用 MLP。在 RL 中，主要致力于算法:基于值还是基于策略？探索还是剥削？正规化与否？基于模型与否？很明显，从来没有人问过神经网络的类型问题。

在这篇文章中，作者讨论了使用最新神经网络的影响，使用现代技巧，如跳过连接。目标是提高学习性能，同时保持相同的算法，这里是软演员评论家(SAC)。那么在你看来，神经网络的选择有多重要？

作者研究了增加标准化和剩余连接对 SAC 的影响。如果天真地实现，很可能造成模型的不稳定。因此，他们建议使用一种通过频谱归一化进行平滑的**(图中的“平滑”)。**

**![](img/01009697f404048c82884364a0182e7d.png)**

**摘自[文章](https://arxiv.org/abs/2106.01151)中的一幅图:使用现代架构的 SAC 的学习曲线(归一化和剩余连接)，平滑和不平滑。1 水平增量= 10 万时间步长。**

**这种平滑允许稳定学习。在连续控制任务上获得的结果通常比在网络架构上不工作时获得的结果好得多。**

**结论是显而易见的:我们必须花时间研究 RL 中的网络架构。这会对学习成绩产生重要影响。这是每个人或多或少都有的直觉，感谢他们一劳永逸地证明了这一点。**

# **论文 4:决策转换器:通过序列建模的强化学习**

**Chen，l .，Lu，k .，Rajeswaran，a .，Lee，k .，Grover，a .，Laskin，m .，Abbeel，p .，Srinivas，a .，Mordatch，I. (2021)。[决策转换器:通过序列建模进行强化学习](https://arxiv.org/abs/2106.01345)。 *arXiv 预印本 arXiv:2106.01345* 。**

**增强学习可以利用语言建模学习的巨大进步。特别是，它可以利用变压器的力量，这是一种最近的神经网络架构，它大大提高了学习性能，包括一种注意力机制。**

**作者提出了决策转换器，一种将 RL 问题表示为条件序列建模的架构。忘记你所知道的政策梯度或基于价值的学习方法。方法不同。在这里，它不再是一个最大化回报函数的问题，而是决定允许达到预期回报的行动。**

**![](img/8dad160c43cb53ee2d0b9576198a6d95.png)**

**来自[论文](https://arxiv.org/abs/2106.01345)的图:决策转换器架构**

**基本上，给定一个交互数据集，transformer agent 将学习哪个动作导致了哪个奖励。将状态和期望回报作为输入，代理将**预测获得期望回报的概率最大化的行动**。因此，如果我们想获得最大的奖励，我们只需要在输入中指定最大可能的奖励。**

****为什么这种方法比传统的 RL 方法更有效？**主要原因是 transform 能够将奖励与其产生的行为联系起来。如你所知，奖励是之前所有行为的产物。但是每个行动都或多或少地促成了这种回报。他们中的一些人甚至强烈限制了奖励。当环境具有这种特征(行为远离其结果)时，经典的 LR 方法就有麻烦了。Transformer 在这方面处理的非常好。**

**然而，在现阶段，有些事情我还不清楚。为什么我们会希望一个代理人能够准确地获得不是最大的回报？在一个状态下，代理人如何知道它可以要求的最大报酬？(不行，每次要求十亿的奖励是不行的。)希望能尽快发布更多解释。**

**谢谢你把我的文章看完。我很乐意阅读你的评论。**