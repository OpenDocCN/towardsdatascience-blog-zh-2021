# 乳腺癌诊断的概念主成分分析与内在主成分分析

> 原文：<https://towardsdatascience.com/conceptual-vs-inbuilt-principal-component-analysis-for-breast-cancer-diagnosis-7ec3a8455201?source=collection_archive---------34----------------------->

在 PCA 中拟合特征值、特征向量和 python 内置函数的概念，然后使用 ML 模型来测量乳腺癌(Wisconsin)数据集的高级精度参数。使用 2 个聚类和 4 个分类模型进行分析。

![](img/d7dd6ee4eee9e1efa456e0350873b1da.png)

在 [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral) 上由[斯特凡·斯特凡·契克](https://unsplash.com/@cikstefan?utm_source=medium&utm_medium=referral)拍摄的照片

**简介:**

我们这些热衷于通过 python 探索世界的数据科学爱好者知道，python 只用几行代码就使主成分分析变得非常容易。但是我们不能忘记选择主成分背后的基本概念。在本文中，在将机器学习模型应用于聚类和分类之前，我已经以两种方式实现了 PCA 一种是计算特征值和特征向量，另一种是使用传统的 scikit-learn 工具。

在深入研究这些方法之前，让我先问你两个非常简单的问题:

为什么要处理一个庞大的数据集，使计算成为一项艰巨的任务？

为什么不减少它，同时保留它所提供的几乎所有有价值的信息呢？

这就是 PCA 用匕首来拯救你的地方，它砍掉了数据，减少了数据的维数，使分析更容易，同时保留了手头数据的统计数据。现在你可能会想，PCA 是如何决定哪些变量要删除，哪些要保留的呢？那就是降维的妙处；它不是删除变量，而是将信息组合成新的主成分，这些主成分是原始变量的线性组合。是的，准确性确实受到了一点影响，但获得的简单性更有价值。

当你在思考的时候，让我为你描绘一个非常简单的场景..

假设你打算很久以后去见你的学校朋友。他们在家里急切地等着你。但是，你的朋友圈挺大的，去他们各自的住处拜访，会占用相当多的时间和精力。你能想到的直接解决办法是什么？这时你决定给他们每个人打电话，约他们在附近的咖啡馆见面。这样，你可以用最少的时间和精力去结识所有的朋友，而不必排除任何人。这不就和上面的解释差不多了吗？

PCA 以这样的方式构造新的变量或主要成分，即大部分信息被塞入第一成分，而最大信息保留在第二成分中，以此类推。它创建了与原始变量相同数量的组件，但是每个组件中包含的信息量随着组件数量的增加而减少。一个**简单的图表**将帮助您直观地了解主成分如何解释数据中的最大总方差。当我们在数据集上实现它时，这幅图将会变得更加清晰。

**我们将要处理的数据集-**

我选择了一个维度足够大的数据集来应用 PCA，因为所有真实世界的数据，无论是结构化的还是非结构化的，都是巨大的。Kaggle 的“乳腺癌(威斯康星州)”数据集包含了癌症和非癌症患者的数据。我已经分享了数据的链接-【https://www.kaggle.com/uciml/breast-cancer-wisconsin-data*。*

*分析是在 Python (Google colab)上实现的，这里是我在 GitHub 中的代码的链接*

*[https://github . com/debangana 97/Data-science-works/blob/main/Breast _ cancer _ diagnosis _ using _ PCA _ two _ ways . ipynb](https://github.com/debangana97/Data-science-works/blob/main/Breast_cancer_diagnosis_using_PCA_two_ways.ipynb)*

*从文件中可以明显看出，有 569 行和 30 列，也就是说，30 个不同的变量对应于关于患者的 30 个不同特征的信息。这些特征有助于确定患者是否患有乳腺癌。然而，同时检查所有 30 个变量来预测未来患者的状况是非常乏味的，并且容易出错。对癌症的错误预测是医生最不想做的事！*

***目标:***

*这一分析的主要目标可以清楚地表明在以下几点:-*

1.  *使用主成分分析来降低数据的维数仅仅意味着主成分分析将把 30 个变量组合成更小的主成分*
2.  *根据通过主成分分析获得的新变量(主成分)对数据进行聚类*
3.  *检查减少的数据是否可用于分类*
4.  *基于分类模型的精确度找到主成分的最佳数量*

*既然我们已经清楚了这篇文章的主要目的，那就让我们开始吧。*

***准备数据..***

*数据预处理或数据清理是数据分析中最重要的先决条件之一。数据的适当准备将导致可靠和可理解的结果。这里有几个步骤，我发现对我的分析是必要的-*

*   *删除对研究没有帮助的不必要的列*

```
*#removing the unnecessary columnsexclude = ['Unnamed: 32','id']data = data.drop(exclude, axis = 1)*
```

*   *检查缺少的值。非常幸运的是，我的数据集中没有丢失值，因此我可以使用整个原始数据，而不必操作任何点*
*   *检查数据不平衡*

*![](img/211029304f5204f018169a22ae06172d.png)*

*图 2:显示每一个班级人数的计数图*

*   *将数据分为训练集和测试集，其中 20%的数据被保留用于测试模型的性能，其余的用于训练。训练集的大小为 455 行 30 列*
*   *将训练数据分成两个独立的数据帧。因变量/目标变量(y)包括癌症结果，即数据的**‘诊断’**列，特征变量(x)包括患者的所有特征*
*   *训练集和测试集中的特征矩阵 x 的标准化是为了更好的计算和无偏的结果。 *StandardScalar* 包用于数据的集中和规范化*

```
*#standardization or feature scaling of the matrix of features X does both centering and normalizing of the datafrom sklearn.preprocessing import StandardScalersc = StandardScaler()
x_train_std = sc.fit_transform(x_train)
x_test_std = sc.fit_transform(x_test)*
```

*   *对分类变量“y”进行编码*

***主成分分析:***

***方法 1***

*在进行 PCA 之前，有必要将数据标准化。标准化确保每个原始变量对分析的贡献相等。这种降维方法对原始变量的方差非常敏感，其中值范围较大的变量将比范围较小的变量占优势，因此结果将趋于有偏差。此步骤以这样一种方式转换变量，即整个训练数据的平均值为 0，标准偏差为 1。*

```
*print(np.mean(x_train_std))
print(np.std(x_train_std))**Output:** 4.021203394686281e-17 
        1.0*
```

*在 python 中，我计算了协方差矩阵。矩阵中的正负符号表明特征变量之间是直接相关还是反向相关。*

*从线性代数的知识中，从这个矩阵中，我们可以构造特征向量，这些特征向量实际上是新轴的方向，其中有最多的信息(最高方差)并且被称为主分量。它们对应的特征值是特征向量的系数，表示每个主分量传递的信息量(方差)。*

*那么现在应该选择哪些组件呢？*

*想..*

*想..*

*正确！有最大特征值的那些，对吗？*

*我选择了具有最高特征值的两个分量，并基于这两个分量绘制了显示“恶性”和“良性”之间的区别的二维图。在这里，我也使用不同的组件进行了一些试验，看看哪一对组件能更好地显示这两类患者。*

*   *第一次投影在第一和第二主分量的平面上。第二次投影在第二和第三主分量的平面上。*

*![](img/bd4ec44b2a66a0f088bfca059bb3e71e.png)*

*图:基于第一和第二主成分绘制了由两类患者分组的数据*

*![](img/d5046b49a8a37877e7c6f0acfcb9b38e.png)*

*图:基于第二和第三主成分绘制了由两类患者分组的数据*

*   *从图中可以清楚地看到，第一种投影更好地分离了类别。这是一个明显的结果，因为第一分量包含最高比例的信息。*

> *尝试使用第一个和第三个组件绘制图表。你得到了什么？成绩提高了吗？*

***方法二***

*接下来，我使用 scikit-learn 直接执行主成分分析，这在第一种方法之后看起来非常容易。Python 完成了上述所有计算，并最终向我们展示了一个图表(scree plot ),按照解释的变化百分比顺序显示了主要成分。该图显示累计约**总变化的 73%仅由** **前三个分量**解释。*

*![](img/5050bf20d419735c73965118eeed1c37.png)*

*图:显示 30 个组成部分中每一个解释的变化百分比的 Scree 图*

*为了与方法 1 的结果进行比较，我绘制了显示基于前两个主成分的两个不同类的数据，这给出了与不直接使用 scikit-learn 的结果相似的可视化。因此，这两种方法表现得一样好，python 已经和人脑不相上下了！*

*![](img/233b7ad21774b8e8c334e366ae752740.png)*

*图 5:Python scikit-learn 给出了一个与上图类似的图表*

*下一步是什么？..*

*现在数据从 30 个变量减少到 3 个变量，定义了一个新的特征向量，它包含作为列的三个分量的特征向量。因此，新特征向量的维数是 455(行)乘 3(列)。*

```
*#transforming the data to the reduced dimension data
#this is the data that we are going to work with during the analysispca_data = pca.fit_transform(x_train_std)print('The reduced data is of the dimension: ', pca_data.shape)**Output:** The reduced data is of the dimension:  (455, 3)*
```

*我要实现的第一个机器学习算法是 K-Means 聚类和层次凝聚聚类模型。粗略地说，聚类是一种无监督的 ML 算法，并且不需要任何训练，因为它基于特征变量创建整个数据的聚类。现在让我们看看这些模型如何在我们的数据集上工作。*

***K 均值聚类与层次聚类-***

*在确定聚类之前，我们如何决定要将数据分成多少个聚类？集群应该以这样的方式形成，即它们之间是同质的，同时与其他集群共享异质性。*

*在 K-Means 聚类中，我们使用肘方法图中聚类平方和内的**,找到最佳聚类数。你为什么要问？看一下图，你会注意到曲线形状像一个肘部，因此得名。但是我们如何决定是使用 2 个、3 个还是 5 个集群呢？***

*![](img/dceda50510840d52aa03c8b77eb2879f.png)*

*图:弯头方法图*

*为了消除这种困惑，我使用了一种叫做**轮廓分数**的度量标准，它测量一个聚类中的每个点与相邻聚类中的点的接近程度。侧影分数+1 表明该点离相邻聚类非常远，从而表明这些聚类是不同的，而分数 0 表明该点与另一个聚类中的点重叠或者离它们不是非常远。因此，显然，对应于最高轮廓分数的聚类数是分析的最佳选择。*

*![](img/a80c51fa37a6c0e27fd21a99eae6b554.png)*

*图:每个聚类数的轮廓分数*

*从图中可以看出，2 个集群给出了最好的结果。*

*对于聚类的分层方法，它使用**树状图**来确定最佳的聚类数目，这显著地表明 2 个聚类将给出有希望的结果。*

```
*#Using dendrogram to find the optimal number of clustersimport scipy.cluster.hierarchy as sch
dendrogram = sch.dendrogram(sch.linkage(pca_data, method = 'ward'))*
```

*![](img/32bdf728f11db43a8389f21e8fa2c8b5.png)*

*图:树状图*

*在下面的代码行中，使用 2、3、4 和 5 个集群对这两种方法进行了彻底的比较。在可视化技术的帮助下，集群以不同的颜色呈现。这些图表确实引人注目，不是吗？*

*![](img/3e28a5512b9caa737dd62c03cc6a57f9.png)**![](img/9315cc1c7b22bd4f942b87ac5964e516.png)

图:使用 3 个集群完成的集群* *![](img/f856eff13d708a7dc665d8efd6c0c9e7.png)**![](img/7149f810ab59b275057108cb9cd3ece1.png)

图:使用 5 个集群完成的集群* 

*随着集群数量的增加，它们开始相互重叠。*

*我使用了另一个被称为 Davies-Bouldin 指数的验证指标来确定最佳的集群数量。它评估聚类分散和聚类分离之间的比率。*

**那么，你认为这个指数的理想值是多少？**

*如果群集定义明确、紧凑且独特，则群集分散肯定小于群集间隔。因此，您认为 DV 指数值越低，表示聚类越好，这是正确的。下表给出了不同 k 值的戴维斯-波尔丁指数及其各自的质心坐标。如前所述，该索引值最小的那个是聚类的最佳选择。*

*![](img/14bdb2090ce2b70367a01a53b9ae8763.png)*

*图 2:显示不同数量的聚类的 DV 指数的表格，并附有聚类坐标*

*从上面的讨论和后面的表格中可以明显看出，对于 k = 2，聚类将给出最好的结果。*

***聚类和分类可以一起应用吗？***

*机器学习的基础知识将允许我们问自己，虽然聚类是一种无监督的 ML 算法，但分类是一种有监督的算法，那么两者如何应用于同一数据集？数据是否预先标记？我们可以为未标记的数据生成标签吗？*

*一旦我们计算出聚类的**纯度度量**，就可以给出这些问题的答案。在得到结果之前，我将告诉你一些关于这个指标和它是如何工作的。*

*   *差的聚类将具有接近 0 的纯度值。纯度分数的最高值是 1，其中值 1 意味着每个文档都有自己的聚类。这意味着每个文档都有自己的标签。正如我们所知，分类是一种受监督的机器学习算法，其中类别是预先标记的，因此如果聚类的纯度是 1，这意味着它可以用于分类。*
*   *一般来说，当簇的数量大时，高纯度容易实现。然而，如果纯度等于 1，那么形成簇就没有意义。*
*   *在我们的例子中，我计算了 K-均值聚类和层次凝聚聚类的这个度量，聚类的数量= 2、3、4 和 5，并且发现在所有情况下纯度值都接近 1。因此，即使集群数量较少，我们也可以应用分类。*

***K-NN 分类或者 Logistic 回归或者支持向量机，谁分类最好？***

*因为纯度指标证实了分类模型可以有效地处理数据，所以我决定使用四种不同的模型来比较结果。目标变量(y)显示患者是否被诊断为乳腺癌。它是一个二元变量，其中“M”代表恶性,“B”代表良性。**标签编码**有助于数据的分类和可视化。对于每个模型，我使用了不同数量的组件，分析了结果，并以表格的形式展示了结果。*

```
*#Encoding the target variablefrom sklearn.preprocessing import LabelEncoder
le = LabelEncoder()y_train = le.fit_transform(y_train)y_test = le.fit_transform(y_test)*
```

*分类结果的可视化是一种享受！使用 2 个主要成分，我已经展示了一些迷人的图表供你欣赏..*

*![](img/060fc267f39697d6291f86a295816d57.png)**![](img/cb65869c6d479037d7b2950e24e5dd1f.png)

图 2:训练集和测试集上的 K-最近邻分类器* *![](img/73049152d7ec11b3de1008a4dbbfd89b.png)**![](img/af77e4b24c7ace43bde632fa679dd5a0.png)

图:训练集和测试集的逻辑回归* *![](img/b25c9add2c914cf9f43cf004005b5f7d.png)**![](img/2aea7151a0fa4ed546b83bb694dbaff9.png)

图:训练集和测试集上的线性 SVM* *![](img/bf1c2be116c206c8b99609be3319d1ab.png)**![](img/88ac90808f97ddf6eee65c3c6dc8eb08.png)

图:训练集和测试集上的径向基函数(核)SVM* 

*嗯，一口气画了很多图！但是你能指出它们之间的一些区别吗？看起来线性分类器在这个数据集上会工作得更好。*

*使用不同数量的主成分，我已经应用分类模型回到回答我们的主要问题- **我们实际上需要多少个主成分？***

*从分类分析中获得的准确度结果(百分比)可列表如下-*

*![](img/4a31efbc6bc45a260e4df30f7975221c.png)*

*图 2:显示特定于主成分数量的每个分类模型的总体准确度的表格*

*如果我们仔细研究这张表，可以得出以下几点结论*

*   *对于两个主成分，K-NN 给出最高的精确度，而核 SVM 给出最低的精确度*
*   *K-NN 对 4 个分量达到最大精度，此后开始下降*
*   *使用 4 个主成分，线性 SVM 导致大约 96%的最大准确度*
*   *当考虑 4 个主成分时，逻辑回归也能够获得大约 95%的准确度，此后，它保持恒定*
*   *使用 4 个主成分，核 SVM 提供了大约 94%的准确度*
*   *由核 SVM 结果获得的总体精度低于由线性 SVM 获得的精度，这表明这两类可以通过直线更好地分开*

*我添加的一些其他可比指标包括-*

**混淆矩阵:*这是一个 2x2 的列联表，显示模型做出的正确预测数和错误预测数*

**每个单独类别的准确度分数:*计算模型预测每个类别的准确度*

**平均类别特定准确度:*上面计算的单个准确度的平均值*

**预测正值得分:*被模型预测为患有癌症的患者实际患有癌症的概率。*

*去看看我提供的全部代码，你肯定会理解这些价值。*

***总结我们可以说..***

*因为我们的目标是在保留最大信息量的同时降低数据的维数，所以我认为可以肯定地说主成分的最佳数量应该是 4。*

> *你不这么认为吗？*

*我希望阅读这篇文章有助于您的数据科学知识库，并帮助您获得更深入的见解。因此，请记住，从下次开始，如果你可以用较少的食物提供相同的营养，就不要过量进食你的 python 练习册..谢谢大家！*