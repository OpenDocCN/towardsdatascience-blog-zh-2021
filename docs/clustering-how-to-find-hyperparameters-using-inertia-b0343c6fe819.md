# 聚类:如何利用惯性找到超参数

> 原文：<https://towardsdatascience.com/clustering-how-to-find-hyperparameters-using-inertia-b0343c6fe819?source=collection_archive---------7----------------------->

![](img/2a9674f374cc586ba21e16db94f71e80.png)

Robynne Hu 在 [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral) 上的照片

# 介绍

由于没有标签，聚类是非常强大的。获取标记数据通常既昂贵又耗时。聚类通常用于发现数据中的模式。然后，为了改进某个产品，经常使用找到的模式。一个著名的例子是客户聚类。在客户聚类中，可以找到相似的用户组。如果一个群体的顾客购买了某些产品，这个群体的其他顾客可能也会喜欢他们。因此，可以应用有针对性的广告来增加销售。另一个著名的例子是将网络活动分为欺诈行为和非欺诈行为。

有许多不同的聚类算法。K-Means 算法是一种非常简单而又强大的聚类算法。K-Means 算法要求用户首先定义聚类的数量，可能还需要定义聚类的初始化策略。但是如何找到这些参数呢？对于网络活动的群集，可以使用两个群集。一个用于欺诈活动，一个用于非欺诈活动。但是有多少个聚类用于客户聚类呢？在监督学习中，可以尝试不同的超参数和聚类数，并可以直接计算一些误差度量，如准确度。导致最高精确度的超参数集和聚类数然后可以用于最终模型。但是这对于无监督学习是不可能的，因为缺乏基础真值。那么，为了评估和比较不同的超参数和聚类数，可以为无监督学习做些什么呢？

一种可能性是计算群集内的平方和，也称为**惯性**。

在这篇短文中，介绍了惯性值。然后使用 Scikit-Learn 在小数据集上训练 K-Means 聚类算法。使用计算的惯性值和应用在惯性曲线上的肘方法找到最佳的聚类数。最后但同样重要的是，本文展示了如何使用惯性值找到最佳超参数。代码是用 Python 和 Jupyter 笔记本编写的。代码可以在我的 [Github 页面](https://github.com/patrickbrus/Medium_Notebooks/tree/master/KMeans_Inertia_Tutorial)找到。

# 惯性

惯性或组内平方和值给出了不同组的一致性的指示。等式 1 显示了计算惯性值的公式。

![](img/973e4efda6337b618dc274f89e5521a8.png)

等式 1:惯性公式

n 是数据集内的样本数，C 是聚类的中心。因此，惯量简单地计算聚类中每个样本到其聚类中心的平方距离，并将它们相加。对该数据集中的每个聚类和所有样本进行该过程。惯性值越小，不同的聚类越一致。当添加的聚类数与数据集中的样本数一样多时，惯性值将为零。那么如何利用惯性值找到最优的聚类数呢？

为此，可以使用所谓的**弯管法**。但是让我们通过一个例子来看看这个方法。

# 最优聚类数的肘方法

通过直接应用于实例来学习总是更好的。为此，使用 Scikit-Learns *make_blobs* 函数创建一个具有二维特征的数据集。图 1 显示了创建这个数据集的代码，而图 2 显示了这个数据集的绘图。

图 1:创建集群数据集的代码(作者代码)。

![](img/ee90335c62b6769d938bc1d3e73c6811.png)

图 2:数据集的绘图(图片由作者提供)。

这个数据集中有三个聚类，因此 K-Means 的最佳聚类数应该是三。但是让我们假设我们还不知道。

现在，让我们为不同数量的聚类训练 K 均值模型，并存储每个训练模型的惯性值。然后，可以绘制惯性曲线，以便使用肘方法来寻找最佳的聚类数。这些步骤的代码可以在图 3 中找到。图 4 显示了最终的惯性曲线。

图 3:用不同数量的聚类训练 K 均值模型并绘制惯性曲线的代码(作者编写的代码)。

![](img/f3b65cacf17263ff9c27096b18272d14.png)

图 4:惯性曲线(图片由作者提供)。

红色的 x 标记标记肘点。肘点给出了最佳的聚类数，这里是三个。这完全有意义，因为数据集是这样创建的，有三个不同的聚类。当添加更多的聚类时，惯性值减小，但是聚类中包含的信息也进一步减少。拥有多个集群会导致性能下降，并且也不会产生最佳的集群结果。让我们假设你做了一个客户聚类，你有许多小的聚类。当一个小客户群购买某样东西时，你只能向该产品的少数其他潜在买家推销。但是，当你有一个大的连贯集群，那么你可以直接解决更多的潜在买家。

因此，对于本例，最佳聚类数是 3。图 5 显示了三个不同集群的可视化。

![](img/a3d59ce7e90f74c680170380a189cc5d.png)

图 5:三个聚类的 K-Means 结果的可视化(图片由作者提供)。

# 利用惯性值寻找最优超参数

惯性值也可以用于为无监督 K-均值算法寻找更好的超参数。一个潜在的超参数是初始化方法。在 Scikit-Learn 中，有两种不同的可能性。一个叫 *k-means++* 一个叫 *random* 。但是应该使用哪种初始化呢？为了回答这个问题，可以为每个初始化策略训练一个 K 均值模型，并比较其惯性值。导致较小惯性值的策略可以用作最佳策略。图 6 显示了该评估的代码，图 7 显示了结果的数据框。

图 6:用于比较 K-Means 算法的不同初始化策略的代码(由作者编写)。

![](img/fbcfb7a2a73e10124b2188b463eeed08.png)

图 7:包含每个初始化策略的惯性值的数据框(图片由作者提供)。

可以看出，随机初始化实现了稍小的惯性值，并且在这里可以用作最佳初始化策略。

# 结论

聚类算法在发现数据模式方面非常有效。聚类算法通常只需要几个超参数，如聚类数或聚类的初始化策略。由于缺乏基础真值，寻找最优值不像监督学习那样简单。为了仍然能够找到最佳值，可以使用惯性值和弯头方法。