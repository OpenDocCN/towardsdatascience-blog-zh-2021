# é€šè¿‡ç»„åˆåˆ†ç±»ç‰¹å¾æé«˜ ML æ¨¡å‹æ€§èƒ½

> åŸæ–‡ï¼š<https://towardsdatascience.com/improve-ml-model-performance-by-combining-categorical-features-a23efbb6a215?source=collection_archive---------21----------------------->

## æé«˜æ¨¡å‹æ€§èƒ½çš„ä¸€ä¸ªç®€å•æŠ€å·§ã€‚

![](img/403e2610faf21764adc5cae224811fe0.png)

ç…§ç‰‡ç”±æ¥è‡ª[ä½©å…‹æ–¯](https://www.pexels.com/photo/selective-focus-photoraphy-of-chains-during-golden-hour-119562/?utm_content=attributionCopyText&utm_medium=referral&utm_source=pexels)çš„[ä¹”ä¼ŠÂ·å‡¯ä¼¯](https://www.pexels.com/@joey-kyber-31917?utm_content=attributionCopyText&utm_medium=referral&utm_source=pexels)æ‹æ‘„

å½“æ‚¨è®­ç»ƒæœºå™¨å­¦ä¹ æ¨¡å‹æ—¶ï¼Œæ‚¨çš„æ•°æ®é›†ä¸­å¯ä»¥æœ‰ä¸€äº›è¡¨ç¤ºåˆ†ç±»å€¼çš„è¦ç´ ã€‚åˆ†ç±»ç‰¹å¾æ˜¯å¯ä»¥åˆ†ç»„çš„æ•°æ®ç±»å‹ã€‚

æœ‰ä¸‰ç§å¸¸è§çš„åˆ†ç±»æ•°æ®ç±»å‹ï¼Œå®ƒä»¬æ˜¯:

1.  **åºæ•°** â€”è¿™æœ‰ä¸€ç»„é¡ºåºã€‚ä¾‹å¦‚:ç”¨ 1-10 çš„å°ºåº¦ç»™å¹¸ç¦æ‰“åˆ†
2.  **äºŒè¿›åˆ¶** â€”åªæœ‰ä¸¤ä¸ªå€¼ã€‚ä¾‹å¦‚:ç”·æ€§æˆ–å¥³æ€§
3.  **åä¹‰** â€”å®ƒæ²¡æœ‰ä»»ä½•è®¢å•é›†ã€‚ç¤ºä¾‹:å›½å®¶

å¤§å¤šæ•°æœºå™¨å­¦ä¹ ç®—æ³•éœ€è¦æ•°å­—è¾“å…¥å’Œè¾“å‡ºå˜é‡ã€‚å› æ­¤ï¼Œæ‚¨å¿…é¡»å°†æ•°æ®é›†ä¸­çš„åˆ†ç±»ç‰¹å¾è½¬æ¢ä¸ºæ•´æ•°æˆ–æµ®ç‚¹æ•°ï¼Œä»¥ä¾›æœºå™¨å­¦ä¹ ç®—æ³•ä½¿ç”¨ã€‚æ‚¨å¯ä»¥å¯¹äºŒè¿›åˆ¶ç‰¹å¾ä½¿ç”¨[æ ‡ç­¾ç¼–ç ](https://www.freecodecamp.org/news/feature-engineering-and-feature-selection-for-beginners/)ï¼Œæˆ–è€…å¯¹åä¹‰ç‰¹å¾ä½¿ç”¨[ä¸€æ¬¡çƒ­ç¼–ç ](https://hackernoon.com/what-is-one-hot-encoding-why-and-when-do-you-have-to-use-it-e3c6186d008f)æ–¹æ³•ã€‚

åœ¨æœ¬æ–‡ä¸­ï¼Œæ‚¨å°†äº†è§£ç»„åˆåˆ†ç±»ç‰¹å¾å¦‚ä½•æé«˜æœºå™¨å­¦ä¹ æ¨¡å‹çš„æ€§èƒ½ã€‚

æ‰€ä»¥è®©æˆ‘ä»¬å¼€å§‹å§ã€‚ğŸš€

# åœ¨æœºå™¨å­¦ä¹ æ¨¡å‹ä¸­ç»„åˆåˆ†ç±»ç‰¹å¾

æ‚¨å¯ä»¥åˆ›å»ºä¸€ä¸ªç”±å…¶ä»–ä¸¤ä¸ªåˆ†ç±»è¦ç´ ç»„åˆè€Œæˆçš„æ–°è¦ç´ ã€‚æ‚¨è¿˜å¯ä»¥ç»„åˆä¸‰ä¸ªæˆ–å››ä¸ªä»¥ä¸Šç”šè‡³æ›´å¤šçš„åˆ†ç±»ç‰¹å¾ã€‚

```
df["new_feature"] = (
	df.feature_1.astype(str)
	 + "_"
	 + df.feature_2.astype(str)
	)
```

åœ¨ä¸Šé¢çš„ä»£ç ä¸­ï¼Œæ‚¨å¯ä»¥çœ‹åˆ°å¦‚ä½•ä½¿ç”¨ pandas åˆå¹¶ä¸¤ä¸ªåˆ†ç±»è¦ç´ ï¼Œå¹¶åœ¨æ•°æ®é›†ä¸­å½¢æˆä¸€ä¸ªæ–°è¦ç´ ã€‚

é‚£ä¹ˆä½ åº”è¯¥ç»“åˆå“ªäº›åˆ†ç±»ç‰¹å¾å‘¢ï¼Ÿè¿™ä¸ªé—®é¢˜æ²¡æœ‰ç®€å•çš„ç­”æ¡ˆã€‚è¿™å–å†³äºæ‚¨çš„æ•°æ®å’Œè¦ç´ ç±»å‹ã€‚ä¸€äº›é¢†åŸŸçŸ¥è¯†å¯èƒ½å¯¹åˆ›å»ºè¿™æ ·çš„æ–°ç‰¹æ€§æœ‰ç”¨ã€‚

ä¸ºäº†è¯´æ˜æ•´ä¸ªè¿‡ç¨‹ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨æ¥è‡ª [Zindi competition é¡µé¢](https://zindi.africa/competitions/financial-inclusion-in-africa)çš„[éæ´²é‡‘èåŒ…å®¹æ€§æ•°æ®é›†](https://zindi.africa/competitions/financial-inclusion-in-africa/data)ï¼Œå®ƒå…·æœ‰è®¸å¤šåˆ†ç±»ç‰¹å¾ï¼Œæˆ‘ä»¬å¯ä»¥å°†å…¶ä¸­ä¸€äº›ç‰¹å¾ç»“åˆèµ·æ¥ï¼Œçœ‹çœ‹æˆ‘ä»¬æ˜¯å¦å¯ä»¥æ”¹è¿›æ¨¡å‹æ€§èƒ½ã€‚

è¯¥æ•°æ®é›†çš„ç›®æ ‡æ˜¯é¢„æµ‹è°æœ€æœ‰å¯èƒ½æ‹¥æœ‰é“¶è¡Œå¸æˆ·ã€‚æ‰€ä»¥è¿™æ˜¯ä¸€ä¸ªåˆ†ç±»é—®é¢˜ã€‚

# 1.åŠ è½½æ•°æ®é›†

æˆ‘ä»¬çš„ç¬¬ä¸€æ­¥æ˜¯ç¡®ä¿æˆ‘ä»¬å·²ç»ä¸‹è½½äº†æ¯”èµ›ä¸­æä¾›çš„æ•°æ®é›†ã€‚ä½ å¯ä»¥åœ¨è¿™é‡Œä¸‹è½½æ•°æ®é›†ã€‚

å¯¼å…¥é‡è¦çš„ python åŒ…ã€‚

```
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
np.random.seed(123)
warnings.filterwarnings('ignore')
%matplotlib inline
```

åŠ è½½æ•°æ®é›†ã€‚

```
# Import datadata = pd.read_csv('data/Train_v2.csv')
```

è®©æˆ‘ä»¬è§‚å¯Ÿæ•°æ®é›†çš„å½¢çŠ¶ã€‚

```
# print shapeprint('data shape :', data.shape)data shape : (23524, 13)
```

ä¸Šé¢çš„è¾“å‡ºæ˜¾ç¤ºäº†æ•°æ®é›†ä¸­çš„è¡Œæ•°å’Œåˆ—æ•°ã€‚æ•°æ®é›†ä¸­æœ‰ 13 ä¸ªå˜é‡ï¼Œ12 ä¸ªè‡ªå˜é‡å’Œ 1 ä¸ªå› å˜é‡ã€‚

æˆ‘ä»¬å¯ä»¥é€šè¿‡ä½¿ç”¨ pandas åº“ä¸­çš„ **head()** æ–¹æ³•æ¥è§‚å¯Ÿæ•°æ®é›†ä¸­çš„å‰äº”è¡Œã€‚

```
# inspect data data.head()
```

![](img/03d2a2e4d52d12ffe4c777f455219965.png)

æ ·æœ¬ daa

ç†è§£æ¯ä¸ªè¦ç´ çš„å«ä¹‰éå¸¸é‡è¦ï¼Œè¿™æ ·æ‚¨æ‰èƒ½çœŸæ­£ç†è§£æ•°æ®é›†ã€‚æ‚¨å¯ä»¥é˜…è¯» **VariableDefinition.csv** æ–‡ä»¶æ¥ç†è§£æ•°æ®é›†ä¸­å‡ºç°çš„æ¯ä¸ªå˜é‡çš„å«ä¹‰ã€‚

# 2.äº†è§£æ•°æ®é›†

é€šè¿‡ä½¿ç”¨ pandas çš„ **info()** æ–¹æ³•ï¼Œæˆ‘ä»¬å¯ä»¥è·å¾—æ›´å¤šå…³äºè¿™äº›ç‰¹æ€§çš„ä¿¡æ¯ã€‚

```
#show Some information about the datasetprint(train_data.info())
```

![](img/70d3e0bda6eb738ea6943b11d515406d.png)

æ•°æ®æè¿°

è¾“å‡ºæ˜¾ç¤ºå˜é‡/ç‰¹å¾åˆ—è¡¨ã€å¤§å°(å¦‚æœåŒ…å«ç¼ºå¤±å€¼)ä»¥åŠæ¯ä¸ªå˜é‡çš„æ•°æ®ç±»å‹ã€‚ä»æ•°æ®é›†ä¸­ï¼Œæˆ‘ä»¬æ²¡æœ‰ä»»ä½•ç¼ºå¤±å€¼ï¼Œæˆ‘ä»¬æœ‰ 3 ä¸ªæ•´æ•°æ•°æ®ç±»å‹çš„ç‰¹å¾å’Œ 10 ä¸ªå¯¹è±¡æ•°æ®ç±»å‹çš„ç‰¹å¾(å¤§å¤šæ•°æ˜¯åˆ†ç±»ç‰¹å¾)ã€‚

# 3.æœºå™¨å­¦ä¹ æ¨¡å‹çš„æ•°æ®å‡†å¤‡

ä¸‹ä¸€æ­¥æ˜¯ä»æ•°æ®ä¸­åˆ†ç¦»å‡ºè‡ªå˜é‡å’Œç›®æ ‡(bank_account)ã€‚ç„¶åä½¿ç”¨ [LabelEncoder](/categorical-encoding-using-label-encoding-and-one-hot-encoder-911ef77fb5bd) å°†ç›®æ ‡å€¼ä»å¯¹è±¡æ•°æ®ç±»å‹è½¬æ¢æˆæ•°å€¼ã€‚

```
#import preprocessing module
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import MinMaxScaler# Convert target label to numerical Data
le = LabelEncoder()
data['bank_account'] = le.fit_transform(data['bank_account'])#Separate training features from target
X = data.drop(['bank_account'], axis=1)
y = data['bank_account']print(y)
```

![](img/824a43918a5be8b16d5caa693fad864f.png)

ç›®æ ‡å€¼å·²è½¬æ¢ä¸ºæ•°å­—æ•°æ®ç±»å‹ï¼Œ1 è¡¨ç¤ºâ€œæ˜¯â€ï¼Œ0 è¡¨ç¤ºâ€œå¦â€ã€‚

æˆ‘åˆ›å»ºäº†ä¸€ä¸ªç®€å•çš„é¢„å¤„ç†å‡½æ•°æ¥:

*   å¤„ç†æ•°æ®ç±»å‹çš„è½¬æ¢ã€‚
*   ä½¿ç”¨[ä¸€é”®ç¼–ç å™¨å’Œ/æˆ–æ ‡ç­¾ç¼–ç å™¨](/categorical-encoding-using-label-encoding-and-one-hot-encoder-911ef77fb5bd)å°†åˆ†ç±»ç‰¹å¾è½¬æ¢ä¸ºæ•°å­—ç‰¹å¾ã€‚
*   åˆ é™¤ uniqueid å˜é‡ã€‚
*   æ‰§è¡Œ[ç‰¹å¾ç¼©æ”¾](/preprocessing-with-sklearn-a-complete-and-comprehensive-guide-670cb98fcfb9)ã€‚

```
# function to preprocess our data def preprocessing_data(data): # Convert the following numerical labels from interger to float
    float_array = data[["household_size", "age_of_respondent", "year"]].values.astype(float
    )

    # categorical features to be converted to One Hot Encoding
    categ = [
        "relationship_with_head",
        "marital_status",
        "education_level",
        "job_type",
        "country",
    ]

    # One Hot Encoding conversion
    data = pd.get_dummies(data, prefix_sep="_", columns=categ)

    # Label Encoder conversion
    data["location_type"] = le.fit_transform(data["location_type"])
    data["cellphone_access"] = le.fit_transform(data["cellphone_access"])
    data["gender_of_respondent"] = le.fit_transform(data["gender_of_respondent"])

    # drop uniquid column
    data = data.drop(["uniquid"]), axis=1)

    # scale our data 
    scaler = StandardScaler()
    data = scaler.fit_transform(data)

    return data
```

è®©æˆ‘ä»¬é¢„å¤„ç†æˆ‘ä»¬çš„æ•°æ®é›†ã€‚

```
# preprocess the train data processed_test_data = preprocessing_data(X_train)
```

# 4.æ¨¡å‹å»ºç«‹å’Œå®éªŒ

æ•°æ®é›†çš„ä¸€éƒ¨åˆ†å°†ç”¨äºè¯„ä¼°æˆ‘ä»¬çš„æ¨¡å‹ã€‚

```
# Split train_data
from sklearn.model_selection import train_test_spilt
X_Train, X_val, y_Train, y_val = train_test_split(processed_train_data, y_train, stratify = y, test_size = 0.1, random_state=42)
```

åªæœ‰æ•°æ®é›†çš„ **10%** å°†ç”¨äºè¯„ä¼°æœºå™¨å­¦ä¹ æ¨¡å‹ã€‚å‚æ•°**åˆ†å±‚= y** å°†ç¡®ä¿è®­ç»ƒé›†å’ŒéªŒè¯é›†çš„ä¸¤ä¸ªç±»çš„å€¼ç›¸ç­‰(â€œæ˜¯â€å’Œâ€œå¦â€)ã€‚

å¯¹äºè¿™ä¸ªåˆ†ç±»é—®é¢˜ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨**é€»è¾‘å›å½’ç®—æ³•**æ¥è®­ç»ƒå’Œé¢„æµ‹è°æœ€æœ‰å¯èƒ½æ‹¥æœ‰é“¶è¡Œè´¦æˆ·ã€‚

```
#import classifier algorithm here
from sklearn.linear_model import LogisticRegression# create classifier
lg_model = LogisticRegression()#Training the classifier
lg_model.fit(X_Train,y_Train)
```

è®­ç»ƒå®Œåˆ†ç±»å™¨ä¹‹åï¼Œè®©æˆ‘ä»¬ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹æ¥é¢„æµ‹æˆ‘ä»¬çš„è¯„ä¼°é›†ï¼Œçœ‹çœ‹å®ƒçš„è¡¨ç°å¦‚ä½•ã€‚æˆ‘ä»¬å°†ä½¿ç”¨å‡†ç¡®æ€§ä½œä¸ºæˆ‘ä»¬çš„è¯„ä¼°æ ‡å‡†ã€‚

```
# import evaluation metrics
from sklearn.metrics import confusion_matrix, accuracy_score# evaluate the model
y_pred = lg_model.predict(X_val)# Get the accuracy
print("Accuracy Score of Logistic Regression classifier: ","{:.4f}".format(accuracy_score(y_val, lg_y_pred)))
```

Logistic å›å½’åˆ†ç±»å™¨çš„å‡†ç¡®ç‡å¾—åˆ†: **0.8874**

# ç¬¬ä¸€ä¸ªå®éªŒ:ç»“åˆæ•™è‚²æ°´å¹³å’Œå·¥ä½œç±»å‹ç‰¹å¾ã€‚

ç°åœ¨æˆ‘ä»¬çŸ¥é“äº†åŸºæœ¬çš„æ¨¡å‹æ€§èƒ½ï¼Œè®©æˆ‘ä»¬çœ‹çœ‹æ˜¯å¦å¯ä»¥é€šè¿‡ç»“åˆ **education_level** å’Œ **job_type** ç‰¹æ€§æ¥æ”¹è¿›å®ƒã€‚

æˆ‘ä»¬åœ¨ç¬¬ä¸€ä¸ªå®éªŒä¸­éœ€è¦åšçš„æ˜¯æ›´æ–°æˆ‘ä»¬å·²ç»åˆ›å»ºçš„é¢„å¤„ç†å‡½æ•°ï¼Œç„¶åè¿è¡Œå‰©ä½™çš„ä»£ç ã€‚

```
# function to preprocess our data 

def preprocessing_data(data): # Convert the following numerical labels from integer to float
    float_array = data[["household_size", "age_of_respondent", "year"]].values.astype(float) # combine some cat features 
    data["features_combination"] = (data.education_level.astype(str) + "_" + data.job_type.astype(str) ) # remove individual features that are combined together
    data = data.drop(['education_level','job_type'], axis=1) # categorical features to be converted by One Hot Encoding
    categ = [
      "relationship_with_head",
      "marital_status",
      "features_combination",
      "country"
      ] # One Hot Encoding conversion
    data = pd.get_dummies(data, prefix_sep="_", columns=categ) # Label Encoder conversion
    data["location_type"] = le.fit_transform(data["location_type"])
    data["cellphone_access"] = le.fit_transform(data["cellphone_access"])
    data["gender_of_respondent"] = le.fit_transform(data["gender_of_respondent"]) # drop uniquid column
    data = data.drop(["uniqueid"], axis=1) # scale our data 
    scaler = StandardScaler()
    data = scaler.fit_transform(data) return data
```

åœ¨ä¸Šè¿°é¢„å¤„ç†å‡½æ•°ä¸­ï¼Œæˆ‘é€šè¿‡ä»¥ä¸‹æ–¹å¼æ›´æ–°äº†ä»£ç 

*   å°† educaion _ level å’Œ job_type ç»„åˆèµ·æ¥ï¼Œåˆ›å»ºä¸€ä¸ªåä¸º" **features_combination** "çš„æ–°ç‰¹æ€§ã€‚
*   ä»æ•°æ®é›†ä¸­ç§»é™¤å•ä¸ªè¦ç´ (education_level å’Œ job_type)ã€‚
*   åœ¨åˆ†ç±»ç‰¹å¾åˆ—è¡¨ä¸­æ·»åŠ ä¸€ä¸ªåä¸ºâ€œ**feature _ combination**çš„æ–°ç‰¹å¾ï¼Œé€šè¿‡ **One Hot Encoding** è¿›è¡Œè½¬æ¢ã€‚

**æ³¨æ„:**æˆ‘åªé€‰æ‹©äº†åä¹‰åˆ†ç±»ç‰¹å¾(æœ‰ 2 ä¸ªä»¥ä¸Šçš„å”¯ä¸€å€¼)ã€‚

åœ¨ä¸ºç¬¬ä¸€ä¸ªå®éªŒé‡æ–°è®­ç»ƒé€»è¾‘å›å½’åˆ†ç±»å™¨åï¼Œæ¨¡å‹æ€§èƒ½ä» **0.8874** æé«˜åˆ° **0.8882** ã€‚è¿™è¡¨æ˜ç»„åˆåˆ†ç±»ç‰¹å¾å¯ä»¥æé«˜æ¨¡å‹æ€§èƒ½ã€‚è¯·è®°ä½ï¼Œæˆ‘ä»¬æ²¡æœ‰æ”¹å˜ä»»ä½•ä¸œè¥¿ï¼Œå¦‚æœºå™¨å­¦ä¹ åˆ†ç±»å™¨ä¸­çš„è¶…å‚æ•°ã€‚

# ç¬¬äºŒä¸ªå®éªŒ:ç»“åˆä¸å¤´éƒ¨çš„å…³ç³»å’Œå©šå§»çŠ¶å†µç‰¹å¾

åœ¨æˆ‘ä»¬çš„ç¬¬äºŒä¸ªå®éªŒä¸­ï¼Œæˆ‘ä»¬å°†ç»“åˆå¦å¤–ä¸¤ä¸ªåˆ†ç±»ç‰¹å¾ï¼Œå®ƒä»¬æ˜¯**ä¸å¤´éƒ¨çš„å…³ç³»**å’Œ**å©šå§»çŠ¶å†µ**ã€‚

æˆ‘ä»¬åªéœ€è¦æ›´æ–°é¢„å¤„ç†å‡½æ•°(å°±åƒç¬¬ä¸€ä¸ªå®éªŒä¸€æ ·)ï¼Œç„¶åè¿è¡Œå‰©ä¸‹çš„ä»£ç ã€‚

```
# function to preprocess our data def preprocessing_data(data): # Convert the following numerical labels from integer to float
    float_array = data[["household_size", "age_of_respondent", "year"]].values.astype(
        float
    )

    # combine some cat features 
    data["features_combination"] = (data.relationship_with_head.astype(str) + "_"
                           + data.marital_status.astype(str) 
                      )
    # remove individual features that are combined together
    data = data.drop(['relationship_with_head','marital_status'], axis=1) # categorical features to be converted by One Hot Encoding
    categ = [
        "features_combination",
        "education_level",
        "job_type",
        "country",
    ] # One Hot Encoding conversion
    data = pd.get_dummies(data, prefix_sep="_", columns=categ) # Label Encoder conversion
    data["location_type"] = le.fit_transform(data["location_type"])
    data["cellphone_access"] = le.fit_transform(data["cellphone_access"])
    data["gender_of_respondent"] = le.fit_transform(data["gender_of_respondent"]) # drop uniquid column
    data = data.drop(["uniqueid"], axis=1) # scale our data 
    scaler = StandardScaler()
    data = scaler.fit_transform(data) return data
```

åœ¨ä¸Šè¿°é¢„å¤„ç†å‡½æ•°ä¸­ï¼Œæˆ‘é€šè¿‡ä»¥ä¸‹æ–¹å¼æ›´æ–°äº†ä»£ç 

*   ç»“åˆ relation_with_head å’Œ marriage _ status åˆ›å»ºä¸€ä¸ªåä¸ºâ€œ **features_combination** â€çš„æ–°ç‰¹å¾ã€‚
*   ä»æ•°æ®é›†ä¸­ç§»é™¤å•ä¸ªè¦ç´ (relation_with_head å’Œ marriage _ status)ã€‚
*   åœ¨åˆ†ç±»ç‰¹å¾åˆ—è¡¨ä¸­æ·»åŠ ä¸€ä¸ªåä¸ºâ€œ **feature_combination** çš„æ–°ç‰¹å¾ï¼Œé€šè¿‡ **One Hot Encoding** è¿›è¡Œè½¬æ¢ã€‚

åœ¨ä¸ºç¬¬äºŒä¸ªå®éªŒé‡æ–°è®­ç»ƒé€»è¾‘å›å½’åˆ†ç±»å™¨ä¹‹åï¼Œæ¨¡å‹æ€§èƒ½ä» **0.8874** ä¸‹é™åˆ° **0.8865** ã€‚è¿™è¡¨æ˜ï¼Œæœ‰æ—¶å½“ä½ ç»“åˆåˆ†ç±»ç‰¹å¾æ—¶ï¼Œä½ çš„æœºå™¨å­¦ä¹ æ¨¡å‹ä¸ä¼šåƒä½ é¢„æœŸçš„é‚£æ ·æ”¹å–„ã€‚å› æ­¤ï¼Œä½ å°†éœ€è¦è¿è¡Œå¤§é‡çš„å®éªŒï¼Œç›´åˆ°ä½ ä»ä½ çš„æœºå™¨å­¦ä¹ æ¨¡å‹ä¸­è·å¾—ä»¤äººæ»¡æ„çš„æ€§èƒ½ã€‚

# åŒ…æ‰

åœ¨æœ¬æ–‡ä¸­ï¼Œæ‚¨äº†è§£äº†å¦‚ä½•ç»„åˆæ•°æ®é›†ä¸­çš„åˆ†ç±»ç‰¹å¾ï¼Œä»¥æé«˜æœºå™¨å­¦ä¹ æ¨¡å‹çš„æ€§èƒ½ã€‚æ­£å¦‚æˆ‘æ‰€è¯´çš„ï¼Œä¸ºäº†è®©æ‚¨çš„æ¨¡å‹è·å¾—ä»¤äººæ»¡æ„çš„æ€§èƒ½ï¼Œæ‚¨éœ€è¦æ‹¥æœ‰å…³äºæ‚¨æ­£åœ¨è§£å†³çš„é—®é¢˜çš„é¢†åŸŸçŸ¥è¯†ã€‚æ­¤å¤–ï¼Œæ‚¨éœ€è¦è¿è¡Œå¤§é‡éœ€è¦æ›´å¤šè®¡ç®—èµ„æºçš„å®éªŒã€‚

**æ­å–œ**ğŸ‘ğŸ‘ï¼Œä½ å·²ç»åšåˆ°è¿™ç¯‡æ–‡ç« çš„ç»“å°¾äº†ï¼æˆ‘å¸Œæœ›ä½ å­¦åˆ°äº†ä¸€äº›æ–°çš„ä¸œè¥¿ï¼Œå¯¹ä½ çš„ä¸‹ä¸€ä¸ªæœºå™¨å­¦ä¹ æˆ–æ•°æ®ç§‘å­¦é¡¹ç›®æœ‰æ‰€å¸®åŠ©ã€‚

å¦‚æœä½ å­¦åˆ°äº†æ–°çš„ä¸œè¥¿æˆ–è€…å–œæ¬¢é˜…è¯»è¿™ç¯‡æ–‡ç« ï¼Œè¯·åˆ†äº«ç»™å…¶ä»–äººçœ‹ã€‚åœ¨é‚£ä¹‹å‰ï¼Œä¸‹æœŸå¸–å­å†è§ï¼

ä¹Ÿå¯ä»¥åœ¨ Twitter [@Davis_McDavid](https://twitter.com/Davis_McDavid) ä¸Šæ‰¾æˆ‘ã€‚

ä¹‹å‰å‘å¸ƒçš„[æ­¤å¤„](https://hackernoon.com/improve-machine-learning-model-performance-by-combining-categorical-features-g21u34ep)ã€‚