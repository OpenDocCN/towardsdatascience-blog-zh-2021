# Batch Norm 直观地解释了它是如何工作的，以及为什么神经网络需要它

> 原文：<https://towardsdatascience.com/batch-norm-explained-visually-how-it-works-and-why-neural-networks-need-it-b18919692739?source=collection_archive---------0----------------------->

## [动手教程](https://towardsdatascience.com/tagged/hands-on-tutorials)，直观深度学习系列

## 一个非常重要的深度学习层的温和指南，用简单的英语

![](img/903ff3ed2be54a5aca8f2bc92503c4f0.png)

由[鲁本·特奥](https://unsplash.com/@reubenteo?utm_source=medium&utm_medium=referral)在 [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral) 上拍摄的照片

Batch Norm 是现代深度学习实践者的工具箱中必不可少的一部分。在批量标准化[论文](https://arxiv.org/pdf/1502.03167.pdf)中介绍之后不久，它就被认为在创建可以更快训练的更深层次的神经网络方面具有变革性。

Batch Norm 是现在很多架构中普遍使用的神经网络层。它通常作为线性或卷积块的一部分添加，有助于在训练期间稳定网络。

在本文中，我们将探讨什么是批处理规范，为什么我们需要它，以及它是如何工作的。

你可能也会喜欢阅读我的另一篇关于批处理规范的文章，这篇文章解释了为什么批处理规范如此有效。

[](/batch-norm-explained-visually-why-does-it-work-90b98bcc58a0) [## 直观地解释了批量定额——它为什么有效

### 一个温和的指南的原因，批量规范层的成功，使训练收敛更快，在平原英语

towardsdatascience.com](/batch-norm-explained-visually-why-does-it-work-90b98bcc58a0) 

如果你对一般的神经网络架构感兴趣，我有一些你可能会喜欢的文章。

1.  [优化器算法](/neural-network-optimizers-made-simple-core-algorithms-and-why-they-are-needed-7fd072cd2788)*(SGD、Momentum、RMSProp、Adam 等梯度下降优化器使用的基本技术)*
2.  [差分和自适应学习率](/differential-and-adaptive-learning-rates-neural-network-optimizers-and-schedulers-demystified-2edc589fa2c9) ( *优化器和调度器如何用于增强模型训练和调整超参数*)

但是在我们讨论批量规范化本身之前，我们先来了解一些关于规范化的背景。

# 标准化输入数据

当向深度学习模型输入数据时，标准做法是将数据归一化为零均值和单位方差。这意味着什么，我们为什么要这样做？

假设输入数据由几个特征 x1，x2，…xn 组成。每个要素可能有不同范围的值。例如，特性 x1 的值可能在 1 到 5 之间，而特性 x2 的值可能在 1000 到 99999 之间。

因此，对于每个特征列，我们分别获取数据集中所有样本的值，并计算平均值和方差。然后使用下面的公式对这些值进行归一化。

![](img/6476918e2e42549e8758b450d14a9f44.png)

*我们如何标准化(作者图片)*

在下图中，我们可以看到数据归一化的效果。原始值(蓝色)现在以零为中心(红色)。这确保了所有的特征值现在都在相同的比例上。

![](img/7ae3a83e96cbee40e2cb28991c8448f7.png)

*标准化数据是什么样子的(图片由作者提供)*

为了理解不进行归一化会发生什么，让我们来看一个只有两个比例完全不同的要素的示例。由于网络输出是每个特征向量的线性组合，这意味着网络学习每个特征在不同尺度上的权重。否则，大特性会淹没小特性。

然后在梯度下降过程中，为了“移动指针”以减少损失，网络必须对一个权重进行较大的更新。这可以导致梯度下降轨迹沿着一个维度来回振荡，从而采取更多的步骤来达到最小值。

![](img/372f4c5e839f4bfcefd765f2bfde075f.png)

*不同尺度的特征需要更长时间才能达到最小值(图片由作者提供)*

在这种情况下，损失景观看起来像一个狭窄的峡谷。我们可以沿着二维分解梯度。沿着一个维度是陡峭的，而沿着另一个维度则平缓得多。

由于梯度较大，我们最终对一个权重进行了较大的更新。这将导致渐变下降反弹到斜坡的另一侧。另一方面，沿着第二方向的较小梯度导致我们进行较小的权重更新，从而采取较小的步骤。这种不均匀的轨迹需要网络更长的时间才能收敛。

![](img/7b743e68307a77a06165bc2b14e29ec3.png)

*狭窄的山谷导致梯度下降从一个斜坡反弹到另一个斜坡(图片由作者提供)*

相反，如果要素在相同的比例上，损失景观会像碗一样更加均匀。梯度下降然后可以平稳地进行到最小值。

![](img/e6eab8997cf9583ba0c4b6dc82fad713.png)

*归一化数据帮助网络更快收敛(图片由作者提供)*

如果你想了解更多这方面的内容，请参阅我的《神经网络优化器》,其中详细解释了这一点，以及不同的优化器算法如何发展来应对这些挑战。

[](/neural-network-optimizers-made-simple-core-algorithms-and-why-they-are-needed-7fd072cd2788) [## 神经网络优化器变得简单:核心算法和为什么需要它们

### 梯度下降优化器使用的基本技术的温和指南，如 SGD，Momentum，RMSProp，Adam 和…

towardsdatascience.com](/neural-network-optimizers-made-simple-core-algorithms-and-why-they-are-needed-7fd072cd2788) 

# 批量定额的必要性

既然我们理解了什么是规范化，那么需要批量规范化的原因就开始变得清晰了。

考虑网络的任何隐藏层。来自前一层的激活只是这一层的输入。例如，从下图中第 2 层的角度来看，如果我们“清空”所有之前的层，来自第 1 层的激活与原始输入没有什么不同。

要求我们对第一层的输入进行规范化的逻辑同样适用于每一个隐藏层。

![](img/36dca3ba0c4ca3fa5c9339a7b646e72f.png)

*每个隐藏层的输入是来自前一层的激活，也必须被标准化(作者的图像)*

换句话说，如果我们能够以某种方式标准化来自每个先前层的激活，那么梯度下降将在训练期间更好地收敛。这正是批处理规范层为我们做的。

# 批量定额是如何工作的？

Batch Norm 只是插入到一个隐藏层和下一个隐藏层之间的另一个网络层。它的工作是获取第一个隐藏层的输出，并在将它们作为下一个隐藏层的输入传递之前对它们进行归一化。

![](img/8169fcfe6b67f0b617c65544765a212e.png)

*批处理规范层在激活到达第 2 层之前对第 1 层的激活进行规范化*

就像任何网络层的参数(如权重、偏差)一样，批规范层也有自己的参数:

*   两个可学的参数叫做β和γ。
*   两个不可学习的参数(平均移动平均值和方差移动平均值)被保存为批次标准层“状态”的一部分。

![](img/9bae046cdb6d7f0551c1ef6bf583c59c.png)

*一批规范图层的参数(图片由作者提供)*

这些参数是每批定额层。因此，如果我们在网络中有三个隐藏层和三个批次范数层，我们将有三个可学习的β和γ参数用于这三个层。移动平均参数也是如此。

![](img/48689dbc9ff00c67e9a0e016c2e78669.png)

*每个批次定额层都有自己的参数副本(图片由作者提供)*

在训练期间，我们一次向网络提供一小批数据。在转发过程中，网络的每一层都处理这一小批数据。批次定额层按如下方式处理其数据:

![](img/3ca91c56a70130da62a4eb11b5b25c11.png)

*批量定额层执行的计算(图片由作者提供)*

## **1。激活**

来自前一层的激活作为输入被传递给批量定额。数据中的每个特征都有一个激活向量。

## 2.计算平均值和方差

对于每个激活向量，分别计算小批量中所有值的平均值和方差。

## 3.使标准化

使用相应的平均值和方差计算每个激活特征向量的归一化值。这些标准化值现在具有零均值和单位方差。

## 4.缩放和移位

这一步是 Batch Norm 引入的巨大创新，赋予了它力量。与要求所有归一化值的均值和单位方差为零的输入图层不同，Batch Norm 允许其值被移动(到不同的均值)和缩放(到不同的方差)。这是通过将归一化值乘以系数γ，再加上系数β来实现的。请注意，这是一个元素级乘法，而不是矩阵乘法。

这项创新的巧妙之处在于这些因素不是超参数(即由模型设计者提供的常数)但是是由网络学习的可训练参数。换句话说，每个批次范数层都能够以最佳方式找到自己的最佳因子，因此可以移动和缩放归一化值以获得最佳预测。

## 5.移动平均数

此外，Batch Norm 还保存平均值和方差的指数移动平均(EMA)的运行计数。在训练期间，它只是计算这个均线，但不做任何事情。在训练结束时，它只是将该值保存为层状态的一部分，供推断阶段使用。

我们将在稍后谈到推论时回到这一点。移动平均计算使用下面用 alpha 表示的标量“动量”。这是一个超参数，仅用于批处理范数移动平均值，不应与优化器中使用的动量相混淆。

## 向量形状

下面，我们可以看到这些向量的形状。计算特定要素的矢量时涉及的值也以红色突出显示。但是，请记住，所有的特征向量都是在一次矩阵运算中计算出来的。

![](img/64f143d67513aa3298ae27ad36dd325d.png)

*批量范数向量的形状(图片由作者提供)*

向前传球后，我们像平常一样向后传球。对所有层权重以及批标准层中的所有β和γ参数计算梯度并进行更新。

# 推理过程中的批量规范

正如我们上面讨论的，在训练过程中，批量定额从计算小批量的平均值和方差开始。然而，在推断过程中，我们有一个单一的样本，而不是一个小批量。在这种情况下，我们如何获得均值和方差？

这就是两个移动平均参数的用武之地，它们是我们在训练过程中计算出来并与模型一起保存的。在推断过程中，我们将这些保存的平均值和方差值用于批量定额。

![](img/27bd614a1bb591c33efec65a4860cbcb.png)

*推理时批量定额计算(图片由作者提供)*

理想情况下，在训练期间，我们可以计算并保存全部数据的平均值和方差。但这将非常昂贵，因为我们必须在训练期间将整个数据集的值保存在内存中。相反，移动平均线很好地代表了数据的均值和方差。这样效率更高，因为计算是增量式的——我们只需记住最近的移动平均值。

# 批量定额层的放置顺序

对于批处理规范层应该放在架构中的什么位置，有两种观点——激活之前和激活之后。最初的论文把它放在前面，虽然我想你会发现文献中经常提到这两个选项。有人说“之后”会有更好的结果。

![](img/83b6240765fdd4cf0698cecb839db267.png)

*批量定额可在激活前或激活后使用(图片由作者提供)*

# 结论

Batch Norm 是一个非常有用的层，您最终会在您的网络架构中经常使用它。希望这能让你很好地理解*如何使用*批处理规范。

理解*为什么*批处理规范有助于网络训练也是有用的，我将在另一篇文章中详细介绍。

最后，如果你喜欢这篇文章，你可能也会喜欢我关于变形金刚、音频深度学习和地理定位机器学习的其他系列。

[](/transformers-explained-visually-part-1-overview-of-functionality-95a6dd460452) [## 直观解释的变压器(第 1 部分):功能概述

### NLP 变形金刚的简明指南，以及为什么它们比 rnn 更好，用简单的英语。注意力如何帮助…

towardsdatascience.com](/transformers-explained-visually-part-1-overview-of-functionality-95a6dd460452) [](/audio-deep-learning-made-simple-part-1-state-of-the-art-techniques-da1d3dff2504) [## 音频深度学习变得简单(第一部分):最新技术

### 颠覆性深度学习音频应用和架构世界的温和指南。以及为什么我们都需要…

towardsdatascience.com](/audio-deep-learning-made-simple-part-1-state-of-the-art-techniques-da1d3dff2504) [](/leveraging-geolocation-data-for-machine-learning-essential-techniques-192ce3a969bc) [## 利用地理位置数据进行机器学习:基本技术

### 简明的地理空间数据特征工程和可视化指南

towardsdatascience.com](/leveraging-geolocation-data-for-machine-learning-essential-techniques-192ce3a969bc) 

让我们继续学习吧！