# ImageNet 的衰落

> 原文：<https://towardsdatascience.com/the-fall-of-imagenet-5792061e5b8a?source=collection_archive---------21----------------------->

## 数据隐私和再现性的挑战

![](img/d0148e6ea1347739ab3d3f1039745a73.png)

(来源:图片由作者和 Shutterstock 提供)

上周，我开始研究一个机器学习的例子，这个例子使用了非常流行的 ImageNet 数据集。这是每个机器学习实践者都知道的经典图像分类问题。这里有一个图像，分类它属于 1000 个类别中的哪一个。

但是这次我注意到了一些奇怪的事情。首先，网站关闭了，当它恢复时，一切都变了。

我并没有多想，直到我意识到下载散列是不同的，并且模型的准确性略有偏差。直到我偶然发现《连线》杂志的[这篇文章](https://www.wired.com/story/researchers-blur-faces-launched-thousand-algorithms/)，一切才开始联系起来。

ImageNet 的维护者修改了数据集中的每一张图片，模糊了人脸。

# 数据保密

这一决定背后的理由是高尚的。他们想让数据集更具“隐私意识”

今天，大多数最先进的计算机视觉模型都在 ImageNet 上进行了预训练。其中呈现的自然场景和物体为大多数计算机视觉问题提供了一个强大的基础。

该团队发布的研究论文《T2，一项关于 ImageNet 中脸部模糊的研究》告诉了我们原因。共享的日常图像通常包含敏感信息。向公众发布大型数据集显然会带来大量潜在的隐私风险。由于 ImageNet 挑战不是关于识别人，而是关于识别物体，团队决定推进模糊数据集中人的脸。最终，他们修改了 243198 张图片。

![](img/12b0b51edac60a4948a8c3721dd7130c.png)

(来源:图片来自[ImageNet](https://arxiv.org/abs/2103.06191)中的人脸混淆研究)

你很难找到一个不同意保护人们隐私的人。匿名数据是数据科学的核心部分，保护从个人身份到健康记录的一切。无数的诉讼都集中在对人们数据的不当管理上，团队让数据容易受到黑客和攻击者的攻击。你不用很努力就能意识到，即使是世界上最大的组织也遭受过泄露未受保护信息的重大攻击。

在机器学习中，我们有机会从安全的核心开始。互联网开始于一个不同的时代，在那个时代，安全和隐私是事后才想到的。互联网的创造者看不到所有会出错的东西，也看不到所有利用互联网设计弱点的方法。但今天，我们可以看到这些问题在一英里之外出现，我们现在可以通过将隐私和安全作为数据科学的核心来解决这些问题。这已经发生了，因为我们已经有了致力于隐私和安全的整个机器学习领域。

但是有一个问题。如果我们希望隐私保护成为机器学习的核心，那么我们必须接受我们的数据会不断变化。把我们引向一个更大的问题。

再现性。

# 再现性危机

在机器学习中有一个[再现性危机](https://www.wired.com/story/artificial-intelligence-confronts-reproducibility-crisis/)，而且它只会变得更大。

![](img/b31b7fd62838172fedaf3dcf89d7c1e6.png)

《自然》杂志 2016 年再现性调查的主要结果(来源: [CMU ML 博客](https://blog.ml.cmu.edu/2020/08/31/5-reproducibility/))

再现性是科学方法的基础。为了在化学和量子力学中获得可靠的见解，我们依赖于再现性，机器学习也不例外。神经网络是计算机视觉背后的主要技术，其规模和复杂性都有了巨大的增长，通常需要大型 GPU 集群、大规模数据集和对训练过程的微妙调整，这些往往在出版物中没有报道。即使源代码公布了，复制研究的成本可能会让除了资金最雄厚的机构之外的所有机构都难以企及。如今，一些最大的模型需要花费数千美元甚至数百万美元来训练。OpenAI 的 Dota 基于强化学习的系统每天运行费用超过 25，000 美元，因为它模拟了比赛。

这个问题在机器学习研究中变得如此普遍，以至于 2019 年神经信息处理系统(NeurIPS)大会推出了一个[再现性清单](https://www.cs.mcgill.ca/~jpineau/ReproducibilityChecklist.pdf)，其中包含“一个数据集或模拟环境的可下载版本的链接”。

因此我们陷入了僵局。如果我们不能使用用于训练模型的原始数据集，我们就不能再现它。我们可以在新的数据集上重新训练模型，但我们不能复制或比较以前的研究。

这意味着所有建立在原始 ImageNet 上的模型现在都无法重新创建。

十多年来每个人都参考和依赖的挑战不再能够作为规范的计算机视觉基准，因为数据集已经不一样了。由于没有实现可复制性的途径，ImageNet 作为基准计算机视觉数据集的地位已经下降。

# 新的希望

这就把我们带到了问题的关键。我们如何保持再现性，同时仍然允许数据包含隐私变化？

我们必须开始将数据视为一等公民。数据集开发阶段不再仅仅是模型开发之前的一个步骤。这是一个持续的过程。数据集的变化是不可避免的。从减少偏倚到提高准确性，对数据集的修改是不可避免的。我们需要接受数据变化，而不是回避它们。

但这是否意味着我们失去了再现性？如果我们一开始就解释清楚就不会了。我们已经习惯了这种对代码的思考方式。我们知道它是动态的。会改变的。许多人将会在这上面合作。变化是意料之中的—新功能、错误修复、性能改进。我们还需要将这种思维扩展到数据上。

但与在软件开发中编写代码不同，机器学习有两个移动的部分，代码和数据，它们产生我们的模型。我已经在[完成机器学习循环](https://jimmymwhitaker.medium.com/completing-the-machine-learning-loop-e03c784eaab4)中广泛地写了这个主题，要点是，我们需要支持在下图所示的每个“两个循环”中实现迭代的过程和工具。

![](img/cde39445caa6d91be6767b396efec928.png)

机器学习生命周期中的“两个循环”。在机器学习开发中，我们有两个移动的部分需要结合在一起以产生我们的模型:代码和数据。这两个循环代表了每一个处于不断迭代中的开发生命周期。(来源:[完成机器学习循环](https://jimmymwhitaker.medium.com/completing-the-machine-learning-loop-e03c784eaab4))

你不仅需要能够迭代，还需要跟踪所有移动的部分以获得可再现性。对数据集的每一次修改和我们代码中的任何新的训练技术都需要被捕获，以重现从它们得到的模型。当我们的数据改变时，我们需要我们的实验过程来更新。2

这就是像厚皮动物这样的工具的用武之地。Pachyderm 是一个数据科学和处理平台，具有内置的版本控制和数据沿袭功能。它的核心是数据版本控制，以支持数据驱动的管道。它起着“生命系统”的作用。管道依靠它们的输入数据来告诉它们何时开始。在我们的情况下，每当数据集被修改时，就会训练一个机器学习模型，从而保持我们的数据安全并保持我们的模型相关。

![](img/5c87e0308faef910e7805b60f65b162d.png)

当新的、模糊的面部图像被提交到版本化的数据存储库时，连接的管道被自动重新运行，以保持整个系统的可再现状态。(来源:图片由作者提供)

如果 ImageNet 在 Pachyderm 中被组织为数据集，隐私感知版本可能会覆盖原始数据集。任何连接的模型训练管道，无论是原始的 AlexNet 代码还是最先进的预训练模型，都将自动在新的数据集上运行，为您提供一个完全可再现的模型来进行任何实验。允许您的数据发生变化并保持再现性。

像 Pachyderm 这样将数据放在第一位并考虑变化的工具对于将数据隐私引入人工智能至关重要。没有它们，我们会发现自己迷失在不断变化的数据和代码的复杂性中。

# 结论

在日益现代化的世界中，数据隐私对于保护我们的安全至关重要。但是，数据隐私的改善往往是以牺牲可再现性为代价的。ImageNet 最近的变化使无数模型成为孤儿，没有办法重现它们。

我们可以通过采用支持更改的工具，在不影响可再现性的情况下，将隐私更改纳入我们的数据集。Pachyderm 是我管理不断变化的数据的首选工具。它极大地提高了在我的机器学习系统上迭代的可靠性和效率。这些类型的工具对于为人工智能的安全可靠的未来铺平道路至关重要。

我写了一篇关于管理机器学习生命周期的广泛文章，[完成机器学习循环](https://jimmymwhitaker.medium.com/completing-the-machine-learning-loop-e03c784eaab4)，更深入地探讨了这个主题，或者关于我在 NLP 和语音识别方面的一些工作，请参见我的书[NLP 和语音的深度学习](https://jimmymwhitaker.medium.com/deep-learning-for-nlp-and-speech-recognition-b8ef2d46822)。