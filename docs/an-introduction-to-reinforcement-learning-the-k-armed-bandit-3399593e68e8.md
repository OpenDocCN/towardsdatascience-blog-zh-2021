# 强化学习导论:K 臂强盗

> 原文：<https://towardsdatascience.com/an-introduction-to-reinforcement-learning-the-k-armed-bandit-3399593e68e8?source=collection_archive---------31----------------------->

## k 臂土匪问题引入了很多强化学习的思想。多读点了解一下。

![](img/27edd3350f8f0f86bd5942523af8fd6c.png)

[亲爱的](https://unsplash.com/@riverse?utm_source=medium&utm_medium=referral)在 [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral) 上的照片

最近有很多关于强化学习(RL)的宣传，这是理所当然的。自从 DeepMind 发表论文“[用深度强化学习玩雅达利](https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf)”以来，已经出现了许多有希望的结果，其中最著名的可能是 AlphaGo。然而，在我们能够理解这些模型如何工作之前，我们需要理解强化学习的一些基本原理。我认为对这些概念最好的介绍是通过一个简单得多的问题——所谓的“k 臂土匪问题”。

# 问题是

首先，我们来定义一下什么是 k 臂土匪问题。“k 臂强盗”这个词让人想起《星球大战》中波巴·费特和章鱼杂交的画面，但幸运的是事情没有那么疯狂。相反，你可以把强盗想象成一个老虎机，而 k 个手臂中的每一个都是一个杠杆。拉动 k 杆中的一个会给你一个正的回报。在大多数强化学习问题中，你的目标是**最大化你的总回报，给定固定数量(比如 1000)的拉力**。你唯一的先验信息是每个杠杆的平均回报不随时间变化。这里的最佳策略是什么？

如果我们知道每个杠杆的平均回报，这个问题就简单了。我们总是拉着平均回报最高的杠杆。所以一个可能的方法是首先使用一些拉力来计算出哪一个杠杆有最高的平均回报，然后只拉那个杠杆。

应该怎么搞清楚哪个杠杆最好？我们需要多次尝试所有的杠杆，因为即使每个杠杆的平均回报是固定的，每次拉动杠杆都会得到不同的回报。一旦我们多次尝试每个杠杆，我们可以取所有拉力的平均值作为真实回报的近似值。换句话说，一种可能的算法是:

**算法 1**

拉动每根杠杆 n 次，平均每根杠杆的 n 次回报。

平均值最高的杠杆就是最好的杠杆。从现在开始只拉那个。

这就有一个问题，就是我们不知道如何设置 n，比如说我们用 n = 3。对于其中一个杠杆，我们拉 3 次，我们得到 14、15 和 16 的回报，平均下来是 15。然而，那个杠杆的真实平均回报是 5——我们只是非常幸运地拉了三次。现在我们有麻烦了，因为我们总是假设这个杠杆比它实际上要好得多。我们可以使 n 变大，这将使我们更加确信我们的平均值是准确的。然而，由于我们的时间步数有限，我们不想浪费杠杆拉动。因此，设置大的 n 也有问题。

我们可以完全避免固定一个数字 n。相反，我们保留了每个杠杆的运行平均回报。另一种可能的算法是:

**算法二**

在做任何拉动之前，将每个杠杆的移动平均值设置为 0。

对于每个时间步长:

*   选择运行平均值最高的杠杆。如果有平局，在平局的杠杆之间随机选择。
*   拉动我们刚刚选择的杠杆，观察回报，并用回报修改该杠杆的运行平均值。

从理论上讲，这种算法的优势在于，它总是越来越接近每个杠杆的真实平均回报。没有每根杠杆 n 的拉动次数，在此之后我们停止学习。可惜行不通。在第一个时间步，这个算法将拉动一个随机杠杆，得到一个正的回报。然后，因为所有其他运行平均值都设置为 0，它将永远从第一个时间步长继续拉动杠杆。它永远不会碰到其他杠杆。

解决这个问题的一个办法是使用另一个参数 epsilon，它控制我们是拉具有最高运行平均值的杠杆，还是拉随机的杠杆。我们的算法变成了:

**算法三**

在做任何拉动之前，将每个杠杆的移动平均值设置为 0。

将 epsilon 设置为[0，1]中的一个数字。

对于每个时间步长:

*   在[0，1]中得到一个随机数(来自均匀分布)x。
*   如果 x >ε，选择运行平均值最高的杠杆。如果有平局，在平局的杠杆之间随机选择。
*   如果 x
*   拉动我们刚刚选择的杠杆，观察回报，并用回报修改该杠杆的运行平均值。

这个算法确实有效。通过 x 的随机性，最终我们将被迫尝试所有的杠杆足够的次数，以获得所有杠杆的精确运行平均值。然而，我们必须决定ε是什么。如果我们将 epsilon 设置得更接近 1，算法将更均匀地尝试杠杆，从而得到更好的估计，但总回报会更低。如果我们将 epsilon 设置得更接近 0，算法将更多地关注几个杠杆，导致不太准确的估计，但可能会有更高的回报。

# RL 视角

现在让我们把所有这些放在强化学习的术语中。首先，我们注意到我们想要得到每个杠杆的真实平均回报的精确估计。我们称每个杠杆的真实平均回报为问题的**行动价值函数**。为什么叫这个？嗯，在我们的问题中，我们可以采取的可能的**行动**是拉动每个杠杆。每拉一次杠杆，就有一个真实的平均奖励值。更正式地说，动作值函数由 **Q(a)** 表示。 **Q** 是函数， **a** 代表可能的动作。我们不知道 **Q** ，所以我们试着估计一下，记为 **Q*** 。我们的希望是 Q*将接近 Q。

我们也想出了一些具体的算法来解决这个土匪问题。算法 1 有两个独立的部分。第一部分试图通过使用每个杠杆 n 次拉动的样本平均值来近似动作值函数(求 Q*)。第二部分用 Q*来决定拉哪个杠杆。在高层次上，算法的第一部分，寻找 Q*，被称为**探索**。算法的第二部分，使用由 Q*确定的最佳动作/拉动，称为**利用**。我们在设置该算法的参数 n 时遇到了一些问题。同样，我们很难决定如何在勘探和开采之间划分时间间隔。这是强化学习中的一个常见主题。

我们把总是利用**贪婪算法的算法叫做**。贪婪算法总是试图最大化每个时间步长的回报；它不探索。我们在算法 2 中看到了这个问题。因为没有进行探索，所以在短期内贪婪算法可能工作得很好，但从长期来看，它几乎肯定是次优的。

我们提出的另一个算法(算法 3)使用结合了参数ε的移动平均值。艾司隆有效地控制了勘探和开采之间的平衡。如果ε接近 0，大部分时间我们都会在做剥削，或者贪婪算法。正因为如此，这类算法被称为“ **epsilon-greedy** ”，因为 epsilon 控制着我们想要有多贪婪。ε-贪婪算法是非常常用的，包括在本文开头链接的 Atari 论文中。

让我们回顾一下我们已经讲过的内容。首先，我们解释了 k 臂土匪问题和几种可能的解决方案。我们注意到，逼近**动作值函数**是解决问题的重要一步。我们看到了**平衡探索和利用的困难**，这是强化学习问题中的一个常见主题。最后，我们讨论了**贪婪**和**ε贪婪**算法，并讨论了它们如何绑定到探索/开发框架中。强化学习是一个相当广泛的话题，但是我希望这篇文章能够阐明一些基本概念。