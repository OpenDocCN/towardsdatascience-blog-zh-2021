# 如何创建更高效的深度学习模型

> 原文：<https://towardsdatascience.com/how-to-create-more-efficient-deep-learning-models-c27bf49e070c?source=collection_archive---------40----------------------->

## 方法和技术概述

![](img/7b49c06b2791a984ec10b4f0de2d0121.png)

托马斯·凯利在 [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral) 上的照片

在这篇文章中，我将介绍和讨论几种重要的方法和技术，这些方法和技术可以帮助在不同层面上提高深度学习模型的效率。这些类型的优化现在变得更加重要，因为深度学习模型的新改进也带来了参数数量、训练资源要求、延迟、存储要求等的增加。

我将讨论的主要议题如下:

*   **按压手法**
*   **学习技巧**
*   **高效架构**
*   **自动化**

## 压缩技术

这些类型的技术以整个模型的表示效率为目标，这是可能的，主要是因为许多最先进的模型是过度参数化的。主模型的多个组件可以受益，而不会影响(在一定范围内)对原始版本计算的评分结果，例如训练时间、推理延迟、内存占用。通过压缩神经网络模型的计算图的一部分，我们还可以提高其泛化能力。

这里探讨了多种想法，其中一些最成功的是:

*   **修剪** —指的是使用各种策略从神经网络中删除一组参数或将其设置为零，以挑选受影响的权重，从而获得一个不需要像以前那样多内存的稀疏网络。最流行的剪枝策略基于:**显著性、随机结构化/非结构化、调度、稀疏预算分布、再生长。**通常在修剪后，建议对生成的模型进行微调。

PyTorch 中的模型修剪示例:

```
**import** torch.nn.utils.prune **as** prune**model** **=** **Model()****layers_to_prune** **=** **(**
    **(model.conv1,** 'weight'**),**
    **(model.fc1,** 'weight'**)**
**)**// This will prune 20% of the parameters with lowest L1-norm
**prune.global_unstructured(**
    **layers_to_prune,**
    **pruning_method=prune.L1Unstructured,**
    **amount=**0.2**,**
**)**
```

*   **量化** —降低用于模型权重和激活的数据类型的精度(例如:从 32 位浮点值降低到 8 位定点)。大多数时候，当我们应用量化时，我们可以看到内存占用和延迟方面的改进。通常有两种量化方式:**后培训**和**量化感知培训**；我认为这些术语大多是不言自明的(至少从高层次的角度来看)，唯一需要提到的是，第一个术语可能会影响用于推断的模型的质量。

PyTorch 有多种量化策略，下面是最简单的一种:

```
**quantized_model=** **torch.quantization.quantize_dynamic(**
    **model,** 
    **qconfig_spec={torch.nn.Linear},** 
    **dtype=torch.qint8**
**)****//**qconfig_spec specifies the list of submodule names in model to apply quantization to.
```

*   **基于矩阵的压缩技术**:低秩逼近、字典学习、层拼接等。

## 学习技巧

这些类型的技术试图通过改变训练过程的某些方面来提高模型的质量。通过只针对培训阶段，验证/测试得分应该保持对生产的代表性。

学习技巧的类型:

*   **蒸馏** —对于这种方法，我们引入了“学生”网络和“教师”网络的概念。基本上，我们有一个或一组较大的网络在“教导”较小的网络来重现整个过程或只是一些中间的表现。我们还可以使用教师网络来创建软标签，我们可以在损失函数中使用这些软标签以及地面真实标签，其想法是软标签可以捕捉类之间的一些关系，这可以帮助训练。
*   **数据扩充** —当处理深度模型时，我们通常需要大量的样本来确保我们的模型能够一般化。但是，由于特定数据类型的高成本或稀缺性，这有时会成为一个问题。数据集大小问题的一个可能的改进是数据扩充，它基本上是一套通过应用某种变换或插值来生成合成样本的方法。大多数数据增强技术针对计算机视觉任务，例如:调整大小、旋转、翻转、裁剪等。

在 PyTorch 中，我们可以堆叠多种类型的转换，并在自定义数据集类中直接使用它们:

```
train_transforms = A.Compose(
    [
        A.Resize(width=320, height=320),
        A.RandomCrop(height=728, width=728),
        A.HorizontalFlip(p=0.5),
        A.VerticalFlip(p=0.5),
        A.RandomRotate90(p=0.5),
        ToTensor(),
    ]
)
```

*   自我监督学习(Self-Supervised Learning)——代表了对没有足够大的标签数据集的“老”问题的最有趣的解决方案之一。当应用自我监督学习方法时，我们使用未标记的数据创建了一个“借口任务”，允许我们生成良好的表示，这些表示可以在以后用于更具体的任务。一旦我们有足够好的嵌入，我们可以添加一个预测头，并用标记的数据微调模型。例如，在 NLP 中，应用自我监督来预测未标记句子中的屏蔽词是相当常见的。在 CV 中有对比学习的概念，其中模型被训练来区分不同的图像。由于对标记数据的要求较低，这种技术被认为是数据高效的。

## 高效架构

提高深度学习系统效率的另一种方法是后退一步，尝试在模型的架构层面解决问题，一些神经网络层设计更适合特定的任务或数据类型。

接下来，我将尝试向您展示几个模型架构设计的示例，这些设计为计算机视觉和自然语言处理带来了一些改进:

*   计算机视觉:**卷积层** —这种类型的层彻底改变了计算机视觉领域。它利用了图像特征的空间位置，通过堆叠它们，它创建了多级表示，从而允许在后面的层中检测更复杂的特征。此外，因为卷积操作对整个图像重复使用相同的滤波器，这也大大减少了模型的参数数量。
*   自然语言处理:**变形金刚**——给 NLP 领域带来了巨大的改进(从*注意力是你所需要的，* [Ashish Vaswani](https://arxiv.org/search/cs?searchtype=author&query=Vaswani%2C+A) 等开始)，使用这种类型的神经网络的主要优势是它消除了只有单一特征向量来表示整个输入序列上下文的瓶颈。Transformer 体系结构使用自我关注和交叉关注来编码序列中每个输入元素的上下文。[这里的](https://huggingface.co/course/)是一个很棒的课程，解释了变压器的基本原理和用法。

## 自动化

寻找提高机器学习模型效率的新途径的另一种方法是通过使用不同的自动搜索技术来“强力”搜索不同的想法。最大的缺点是基于搜索的方法需要大量的计算资源和时间。

我们可以通过考虑搜索空间的级别来划分自动化的类型:

*   **超参数优化(HO)** —顾名思义，这种类型的自动化试图通过改变一些超参数的值来搜索更有效的模型，如学习速率、层数、重量衰减、批量等。即使我们使用 k 折叠分裂策略来迭代每个折叠中超参数的不同值，也仍然需要大量的计算来遍历它们。有几种搜索策略我们可以遵循: ***网格搜索、随机搜索、贝叶斯搜索、由粗到细搜索。***
*   **神经架构搜索(NAS)** —这可以被认为是超参数优化的扩展，允许搜索空间中的其他元素，如不同的操作块(卷积、线性层、池)以及组合它们的不同方式。此外，对于 NAS，研究人员还使用强化学习来寻找更好的体系结构。

最后，我在这篇文章中列出的列表绝不是全面的，还有许多正在进行的研究努力试图改善深度学习系统的每个瓶颈。

感谢您的阅读，如果您想了解最新的机器学习新闻和一些优质的模因:)，您可以在 Twitter 上关注我[这里](https://twitter.com/SurdoiuT)。

## 资源

*   [https://py torch . org/blog/introduction-to-quantization-on-py torch/](https://pytorch.org/blog/introduction-to-quantization-on-pytorch/)
*   [https://blog . tensor flow . org/2020/02/matrix-compression-operator-tensor flow . html](https://blog.tensorflow.org/2020/02/matrix-compression-operator-tensorflow.html)
*   高拉夫·蒙哈尼:*高效深度学习:关于让深度学习模型更小、更快、更好的调查(*[https://arxiv.org/abs/2106.08962](https://arxiv.org/abs/2106.08962)*)*