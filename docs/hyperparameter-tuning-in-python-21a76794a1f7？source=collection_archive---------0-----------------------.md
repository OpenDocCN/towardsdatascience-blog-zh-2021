# Python 中超参数调优

> 原文：<https://towardsdatascience.com/hyperparameter-tuning-in-python-21a76794a1f7?source=collection_archive---------0----------------------->

## 调整机器学习中超参数的技巧和窍门，有助于提高模型精度

![](img/e61bbaf49fdbb8eee233f6fd0019f670.png)

阿菲夫·库苏马在 [Unsplash](https://unsplash.com/s/photos/accuracy?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText) 上拍摄的照片

当我还是机器学习的新手时，超参数调整曾经是我的一个挑战。我总是讨厌我的项目中的超参数调整部分，通常会在尝试了几个模型并手动选择所有模型中精度最高的一个后就离开它们。但是现在我的概念已经很清楚了，我给你这篇文章是为了让任何新手在我当前项目的超参数被调整的时候都很容易。

让我们从参数和超参数之间的区别开始，了解这一点非常重要。**参数**是在训练过程中学习到的模型组件，我们永远无法手动设置。模型以随机参数值开始训练过程，并在整个过程中调整它们。鉴于，**超参数**是您在模型训练前设置的组件。超参数的值可能会提高或降低模型的精度。

# 机器学习中超参数调优的需求是什么？

机器学习模型不够智能，无法知道什么样的超参数会导致给定数据集的最高可能精度。然而，设置正确的超参数值可以构建高度精确的模型，因此我们允许我们的模型在训练过程中尝试不同的超参数组合，并使用最佳超参数值组合进行预测。随机森林分类器中的一些超参数是 n_estimators(森林中树木的总数)、max_depth(森林中每棵树的深度)和 criterion(在每棵树中进行分割的方法)。n_estimators 设置为 1 或 2 是没有意义的，因为一个森林必须有更多的树，但是我们如何知道多少树会产生最好的结果呢？为此，我们尝试不同的值，如[100，200，300]。该模型将尝试所有三个给定值，我们可以很容易地确定森林中的最佳树木数量。

# Python 中超参数调优

python 中有三种超参数调优方法，分别是网格搜索、随机搜索和知情搜索。下面就详细说说吧。

## **网格搜索**

![](img/468fc1627d603eb0222b2b6ac6cfbdb9.png)

莎伦·麦卡琴在 [Unsplash](https://unsplash.com/s/photos/grid?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText) 上的照片

网格是由相交的线组成的网络，形成一组正方形或长方形，如上图所示。在网格搜索中，网格中的每个方块都有超参数的组合，模型必须在每个组合上训练自己。为了更清楚地理解，假设我们想要用下面的一组超参数来训练一个随机森林分类器。

n _ 估计值:[100，150，200]

最大深度:[20，30，40]

看看这些超参数值构成的网格。我们的模型在 n 估计量和最大深度的每个组合上运行训练过程

![](img/9bb0246e5f10242f8a0e7ba6fa325f3b.png)

作者创建的超参数网格的表示

**网格搜索在 Python 中的实现**

Python 中的 Scikit-learn 库为我们提供了一种简单的方法，只需几行代码就可以实现网格搜索。看看下面的例子

在第 1 行和第 2 行，我们从 sklearn.model_selection 导入 GridSearchCV，并定义我们想要对其执行超参数调优的模型。在第 3 行中，超参数值被定义为一个字典，其中键是超参数名称和一个包含我们想要尝试的超参数值的值列表。

在第 4 行中，GridSearchCV 被定义为 grid_lr，其中 estimator 是我们想要使用的机器学习模型，它是第 2 行中定义为模型的逻辑回归。因此，estimator 等于 model，param_grid 等于我们在第 3 行中定义的 grid_vals，scoring 等于 accuracy，这意味着我们希望使用 accuracy 作为模型的评估技术，cv 设置为 6，意味着我们希望模型经历 6 次交叉验证，refit 参数设置为 True，以便我们可以轻松地拟合和进行预测。

在第 9 行，我们将 grid_lr 拟合到我们的训练数据集，在第 10 行，我们使用具有最佳超参数值的模型，使用 grid_lr.best_estimator_ 对测试数据集进行预测。

**网格搜索的利弊**

网格搜索很容易实现，可以在网格中找到最佳模型。然而，当我们添加新的超参数值时，随着模型的数量继续增加，这在计算上是昂贵的。

## **随机搜索**

像网格搜索一样，我们仍然在随机搜索中设置想要调优的超参数值。然而，该模型并不训练超参数的每个组合，而是随机选择它们。我们必须定义想要从网格中选择的样本数量。

**Python 中随机搜索的实现**

在第 1 行和第 2 行中，我们导入了随机搜索并定义了我们的模型，在本例中使用了随机森林。在第 3 行，我们定义了想要检查的超参数值。

在第 5 行中，RandomizedSearchCV 被定义为 random_rf，其中 estimator 等于第 2 行中定义为 model 的 RandomForestClassifier。Param_distributions(与网格搜索中的 param_grid 相同)等于我们在第 3 行中定义的 param_vals，n_iter 指的是我们希望从所有设置为 10 的超参数组合中抽取的样本数，scoring 等于 accuracy，这意味着我们希望使用 accuracy 作为我们模型的评估技术，cv 设置为 5，这意味着我们希望模型经历 5 次交叉验证， refit 参数设置为 True，这样我们就可以很容易地进行拟合和预测，n_jobs 等于-1 意味着我们希望使用所有可用的资源来进行这种随机搜索。

在第 11 行和第 12 行，我们将 random_rf 拟合到我们的训练数据集，并使用使用 random_rf.best_estimator_ 的最佳模型对测试数据集进行预测。

请注意，总迭代次数等于 n_iter * cv，在我们的示例中为 50，因为每次交叉验证将从所有超参数组合中抽取 10 个样本。

**随机搜索的利弊**

随机搜索在计算上更便宜。但是，不能保证从样本空间中找到最好的分数。

## **知情搜索**

知情搜索是我最喜欢的超参数调整方法，因为它利用了网格和随机搜索的优点。但是，它也有自己的缺点。与网格和随机搜索不同，知情搜索通过以下过程从以前的迭代中学习

1.  随机搜索
2.  寻找得分高的区域
3.  在较小的区域内运行网格搜索
4.  继续，直到获得最优解

遗传算法是一种基于现实世界遗传学概念的通知超参数调整方法。我们首先创建一些模型，从中挑选最好的，创建与最好的模型相似的新模型，并添加一些随机性，直到我们达到我们的目标。

**遗传算法在 Python 中的实现**

我们在这里使用的库是 tpot，它有 generation(运行训练的迭代次数)、population_size(每次迭代后要保留的模型数量)和 subject _ size(每次迭代中要生成的模型数量)作为关键参数。看看下面的例子

在第 1 行，我们导入了 TPOTClassifier。在第 2 行，我们将分类器定义为 tpot_clf。Generations、population_size 和 off_spring_size 设置为 100。Verbose = 2 将让我们看到每一次生成(迭代)的输出，cv 设置为 6，这意味着我们希望为每次迭代运行 6 次交叉验证。

在第 6 行和第 7 行，我们将 tpot_clf 训练到我们的训练集，并在测试集上进行预测。

注意，我们在这里没有定义任何模型，因为 TPOTClassifier 负责为我们的数据集选择模型。

**遗传算法的利弊**

如上所述，它利用了网格和随机搜索的优点。遗传算法从以前的迭代中学习，tpot 库负责估计最佳超参数值和选择最佳模型。然而，它计算量大且耗时。

# 结论

在本文中，我们使用 Python 研究了三种超参数调优技术。网格搜索、随机搜索和知情搜索这三种方法都有各自的优点和缺点，因此我们需要根据我们的需求来选择最适合我们问题的技术。

我希望这篇文章能帮助你在更短的时间内提高你的机器学习模型的准确性。

如果你喜欢这篇文章，请提供你的反馈并分享。感谢您的阅读！

# 参考

[https://camp . data camp . com/courses/hyperparameter-tuning-in-python](https://campus.datacamp.com/courses/hyperparameter-tuning-in-python)