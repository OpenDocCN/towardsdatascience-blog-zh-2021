# 从头开始发展和解释交叉熵

> 原文：<https://towardsdatascience.com/developing-and-explaining-cross-entropy-from-scratch-ba2eff9036dd?source=collection_archive---------33----------------------->

## 继续阅读，理解交叉熵背后的直觉，以及为什么机器学习算法试图最小化它。

![](img/789b3b629630bb45a881697782cbb91c.png)

约尔根·哈兰在 [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral) 上的照片

交叉熵是一个重要的概念。它通常在机器学习中用作成本函数——通常我们的目标是最小化交叉熵。但是为什么我们要最小化交叉熵，交叉熵到底是什么意思？让我们来回答这些问题。

首先，我们需要充分理解信息和熵的概念。如果你想彻底理解这些概念，我写了一篇关于它们的详细文章[在这里](/developing-the-concepts-of-information-and-entropy-from-scratch-11faca089d44)。总而言之，信息是衡量一个事件有多令人惊讶/低概率的指标。概率越低(越让人吃惊)，信息量越高。信息也可以解释为表示一个事件需要多少位。公式为 I = -log_2(p) bits，其中 I 为信息，p 为概率。熵是所有可能事件的平均值，所以熵的公式是 E = -∑ p_i * log_2(p_i)。

有了这个背景，我们可以继续讨论交叉熵。交叉熵提出的问题是:**当我用不同的概率分布代替真实的概率分布时，信息/比特表示长度会发生什么变化？让我们来看一个具体的例子，看看这是什么意思。**

假设我们有一枚公平的硬币。真正的概率分布是正面，反面。头和尾事件的信息是相同的:I = -log_2(1/2) = 1 位。熵是两个事件的信息的平均值，所以也是 1 比特。现在让我们假设，出于某种原因，我们认为硬币是不公平的(即使它实际上是公平的)。我们认为概率分布是正面，反面。在我们看来，正面比实际情况更罕见，反面更常见。因此，我们认为正面包含的信息比它实际包含的多，反面包含的信息少。从我们的角度来看，确切的信息量是 I_heads = -log_2(1/4) = 2，I_tails = -log_2(3/4) = 0.42。

然而，真正的概率分布仍然是正面，反面。正因为如此，从我们的角度来看，翻转的期望信息将是* I _ heads+* I _ tails = * 2+* 0.42 = 1.21。这个数字就是交叉熵:通过假设与真实概率分布不同的概率分布计算出的平均信息。如果真实概率记为 p_i，假设概率记为 q_i，则交叉熵的公式为 CE = -∑ p_i * log_2(q_i)。

让我们从另一个角度来看这个例子，以建立更多的直觉。与其抛硬币，不如考虑一个只有两个字母的字母表:A 和 B。我和我的朋友都使用这个字母表，但我更喜欢字母 A，他更喜欢字母 B。假设我的 A 到 B 的字母用法是 80/20，我朋友的是 20/80。我想想出一个高效的二进制代码来表示这个字母表。因为我使用字母 A 的次数远远多于 B，所以我将用较少的比特数表示 A，用较多的比特数表示 B。我的朋友也想出了一个代码，但是由于他用 B 比用 A 多，他的代码用 A 的比特多，用 B 的比特少。

现在，考虑当我试图用我的代码来表示他所说的话(他的语言)时会发生什么。因为他说了很多次 B，所以我很多次都要用我的高比特位表示 B，用我的低比特位 A 表示的机会不多。因此，我的代码的总比特使用量(在我朋友的语言中)将远远高于他自己的代码(有一个低位 B 和一个高位 A)。同样，我朋友的代码在表示我的语言时会非常低效。那么这个例子有什么意义呢？我的代码需要我朋友的语言的位数是交叉熵。他的代码为他的语言使用的比特数是熵。如我们所见，**交叉熵高于熵**。

我们还可以改变字母使用分布。比如说我的信使用分布是 60/40 A/B 而不是 80/20，我的朋友是 40/60 而不是 20/80。现在，由于我们的发行版更接近了，使用我的优化代码来表示我朋友的语言不再像以前那样低效了。换句话说，交叉熵更小。你可以通过插入我们上面陈述的交叉熵公式来自己看到这一点。

这表明了交叉熵的一个重要用途:比较两个概率分布。**两个分布越接近，交叉熵就会越小**。因此，如果我们的目标是使一个概率分布尽可能接近另一个，我们需要**来最小化它们之间的交叉熵**。这是怎么用的？在机器学习问题中，一个常见的模式是，我们有一个真实分布 P 和一个输出另一个分布 Q 的模型 M。目标是找到 M 的正确参数，使 P 和 Q 之间的交叉熵尽可能小。一种流行的方法是最大化对数似然，这与最小化交叉熵是一回事。你可以在这里看到证明。

在本文中，我们解释了什么是交叉熵，探索了一些交叉熵构建直觉的例子，并讨论了它在机器学习中的用法。我希望一切都很清楚，请留下您可能有的任何问题/意见。如果你对这类话题感兴趣，我打算再写一些关于重要理论机器学习概念的帖子，敬请关注！