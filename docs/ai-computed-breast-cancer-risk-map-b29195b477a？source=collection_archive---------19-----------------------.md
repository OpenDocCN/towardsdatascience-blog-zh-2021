# äººå·¥æ™ºèƒ½è®¡ç®—ä¹³è…ºç™Œé£é™©å›¾

> åŸæ–‡ï¼š<https://towardsdatascience.com/ai-computed-breast-cancer-risk-map-b29195b477a?source=collection_archive---------19----------------------->

## [å®è·µæ•™ç¨‹](https://towardsdatascience.com/tagged/hands-on-tutorials)

## ç»“åˆ Boosting å’Œæ”¯æŒå‘é‡æœºçš„å±‚æ¬¡èšç±»å¯è§†åŒ–å¯é ä¹³è…ºç™Œè¯Šæ–­æ¨¡å‹

![](img/879291910f486f5d9197d97f0416c59c.png)

# ä¸€.å¯¼è¨€

**1.1 å·¥ä½œç›®æ ‡**

å¿«é€Ÿå¯é çš„ä¹³è…ºç™Œæ£€æµ‹æ˜¯ä¸€é¡¹é‡è¦çš„æŒ‘æˆ˜ï¼Œå› ä¸ºå®ƒä»£è¡¨äº†é‡å¤§çš„å…¬å…±å«ç”Ÿé—®é¢˜(2018 å¹´å…¨çƒçº¦æœ‰ 200 ä¸‡ä¾‹æ–°ç—…ä¾‹è¢«æ£€æµ‹åˆ°[1])ã€‚åœ¨ç–¾ç—…çš„æ—©æœŸé˜¶æ®µè¯†åˆ«æ¶æ€§ä¹³è…ºç™Œä¼šæ˜¾è‘—å¢åŠ æ‚£è€…çš„ç”Ÿå­˜æœºä¼šï¼Œå¹¶å‡å°‘æ²»ç–—çš„å‰¯ä½œç”¨ã€‚

è¯Šæ–­ä¾èµ–äºä»å¯ç–‘è‚¿ç˜¤åŒºåŸŸæå–çš„æ ·æœ¬åˆ†æã€‚è¯¥è¿‡ç¨‹æ—¨åœ¨è¯„ä¼°ç»™å®šæ ·æœ¬æ˜¯å¦ç”±ä»£è¡¨ä¸å—æ§åˆ¶çš„å¢æ®–é£é™©çš„ç»†èƒæ„æˆã€‚èƒ½å¤Ÿåœ¨ä¸€ä¸ªæ—¶é—´å†…åˆ†ææ›´å¤šçš„æ ·æœ¬å¯èƒ½ä¼šæœ‰æœºä¼šè¯Šæ–­æ›´å¤šçš„æ‚£è€…ï¼Œå¹¶åœ¨éœ€è¦é‡‡å–æ²»ç–—æªæ–½æ—¶æ›´å¿«åœ°åšå‡ºååº”ã€‚å‡ºäºè¿™ä¸ªç›®çš„ï¼Œäººå·¥æ™ºèƒ½ç®—æ³•å¯èƒ½ä¼šå¸¦æ¥å¾ˆå¤šå¥½å¤„ã€‚åœ¨éœ€è¦åˆ†ææ•°åƒç”šè‡³æ•°ç™¾ä¸‡ä¸ªæ ·æœ¬çš„æƒ…å†µä¸‹ï¼Œå®ƒä»¬å¯ä»¥ç”¨æ¥è¿›è¡Œåˆæ­¥é€‰æ‹©ï¼Œå¹¶å»ºè®®å“ªäº›æ ·æœ¬éœ€è¦ä¼˜å…ˆè¿›è¡Œç²¾ç¡®çš„ä¸“å®¶ç ”ç©¶ã€‚

ä¸ºæ­¤ï¼Œå·²ç»æå‡ºäº†ä¸€äº›æœºå™¨å­¦ä¹ æ¨¡å‹(è§[2]ã€[3])æ¥é¢„æµ‹æ¶æ€§æˆ–è‰¯æ€§è‚¿ç˜¤çš„å‘å±•é£é™©ã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹æ˜¯åŸºäºä»è‚¿ç˜¤ç»†èƒæˆåƒæ¨å¯¼å‡ºçš„è®¸å¤šå˜é‡ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å»ºè®®å°†å˜é‡çš„æ•°é‡å‡å°‘åˆ°ä¸¤ä¸ªã€‚è¿™ç§ç»´åº¦ç¼©å‡çš„ç›®çš„æ˜¯å½¢æˆå¯è§†çš„æ¶æ€§è‚¿ç˜¤é£é™©å›¾ã€‚æˆ‘ä»¬åœ¨è¿™é¡¹å·¥ä½œä¸­æå‡º:

*   1.ç²¾ç¡®åˆ†ææ¯ä¸ªåˆå§‹å˜é‡åœ¨é¢„æµ‹ä¸­çš„ä½œç”¨ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†ä¸€ç§ä½¿ç”¨åˆ†å±‚èšç±»(HC)ç®—æ³•çš„ç‰¹å¾èšç±»æ–¹æ³•ã€‚ç¬¬äºŒæ¬¡ï¼Œä½¿ç”¨æ¢¯åº¦æ¨è¿›æ ‘åˆ†ç±»å™¨(GBTC)ç®—æ³•è¯„ä¼°ç‰¹å¾é‡è¦æ€§ã€‚
*   2.é€‰æ‹©åœ¨å‰ä¸€æ­¥ä¸­æ¨å¯¼å‡ºçš„ä¸¤ä¸ªæœ€æœ‰æ„ä¹‰çš„å˜é‡ã€‚
*   3.æ„å»ºæ”¯æŒå‘é‡æœºåˆ†ç±»å™¨(SVMC)ç®—æ³•ä»¥å¯¼å‡ºæ¶æ€§ä¹³è…ºç™Œå‘å±•çš„æ¦‚ç‡é£é™©å›¾ã€‚

**I.2 æ•°æ®å¯¼å…¥**

æˆ‘ä»¬ä»å‡ ä¸ªå¯¼å…¥å¼€å§‹ã€‚

```
from matplotlib import cm
import seaborn as sn
import matplotlib as matplotlib
import pandas as pd
import pylab as plt
import numpy as np
from scipy.cluster import hierarch# Sklearn imports
from sklearn.datasets import load_breast_cancer
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.metrics import accuracy_score
from sklearn.model_selection import cross_validate,GridSearchCV
from sklearn.svm import SVC
from sklearn.gaussian_process import GaussianProcessClassifier
from sklearn.gaussian_process.kernels import RBF
```

åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨ UCI æœºå™¨å­¦ä¹ æ•°æ®åº“[3]æ”¶é›† 569 åæ‚£è€…çš„æ•°æ®ã€‚å½“åˆ†æè‚¿ç˜¤ç»†èƒæ—¶ï¼Œä¸ºæ¯ä¸ªç»†èƒæ ¸è®¡ç®—åç§ç‰¹å¾ç±»å‹[3]:

*   1)åŠå¾„(ä»ä¸­å¿ƒåˆ°å‘¨è¾¹å„ç‚¹çš„å¹³å‡è·ç¦»)
*   2)çº¹ç†(ç°åº¦å€¼çš„æ ‡å‡†åå·®)
*   3)å‘¨é•¿
*   4)é¢ç§¯
*   5)å¹³æ»‘åº¦(åŠå¾„é•¿åº¦çš„å±€éƒ¨å˜åŒ–)
*   6)å¯†å®åº¦(ğ‘ğ‘’ğ‘Ÿğ‘–ğ‘šğ‘’ğ‘¡ğ‘’ğ‘Ÿ /é¢ç§¯â€” 1.0)
*   7)å‡¹åº¦(è½®å»“å‡¹å…¥éƒ¨åˆ†çš„ä¸¥é‡ç¨‹åº¦)
*   8)å‡¹ç‚¹(è½®å»“çš„å‡¹å…¥éƒ¨åˆ†çš„æ•°é‡)
*   9)å¯¹ç§°æ€§
*   10)åˆ†å½¢ç»´æ•°(â€œæµ·å²¸çº¿è¿‘ä¼¼å€¼â€-1)

å¯¹äºè¿™äº›ç‰¹å¾ä¸­çš„æ¯ä¸€ä¸ªï¼ŒæŠ¥å‘Šå¹³å‡å€¼ã€æ ‡å‡†åå·®å’Œæœ€å¤§å€¼ï¼Œè¿™å¯¼è‡´æ€»å…± 30 ä¸ªç‰¹å¾ã€‚åˆ†æè¿™äº›ç‰¹å¾ä¸­çš„æ¯ä¸€ä¸ªå¯¼è‡´å†³å®šè‚¿ç˜¤æ˜¯æ¶æ€§çš„(1)è¿˜æ˜¯è‰¯æ€§çš„(0)ã€‚é¢„æµ‹å°†æ˜¯æˆ‘ä»¬çš„æ ‡ç­¾ã€‚

é¦–å…ˆï¼Œæˆ‘ä»¬ä¸‹è½½æ•°æ®å¹¶éªŒè¯ä¸å­˜åœ¨â€œnoneâ€å€¼ã€‚

```
# Load the data
feat_all,label_all=load_breast_cancer(return_X_y=True,as_frame=True)feat_all.isnull().sum()mean radius                0
mean texture               0
mean perimeter             0
mean area                  0
mean smoothness            0
mean compactness           0
mean concavity             0
mean concave points        0
mean symmetry              0
mean fractal dimension     0
radius error               0
texture error              0
perimeter error            0
area error                 0
smoothness error           0
compactness error          0
concavity error            0
concave points error       0
symmetry error             0
fractal dimension error    0
worst radius               0
worst texture              0
worst perimeter            0
worst area                 0
worst smoothness           0
worst compactness          0
worst concavity            0
worst concave points       0
worst symmetry             0
worst fractal dimension    0
dtype: int64
```

ç°åœ¨æ˜¯æ—¶å€™å¼€å§‹ç‰¹å¾é€‰æ‹©å·¥ä½œï¼Œå°†æˆ‘ä»¬çš„é—®é¢˜ä» 30 ç»´ç©ºé—´ç®€åŒ–åˆ° 2 ç»´ç©ºé—´ã€‚

# ä¸€.åŠŸèƒ½é€‰æ‹©

åœ¨è¿™ä¸€éƒ¨åˆ†ä¸­ï¼Œæˆ‘ä»¬é¦–å…ˆè®¡ç®—æˆå¯¹ç›¸å…³æ€§ï¼Œä»¥ä¾¿é‡æ–°ç»„åˆå½¼æ­¤ä¹‹é—´æœ€ç›¸å…³çš„ç‰¹å¾ã€‚ä½¿ç”¨å±‚æ¬¡èšç±»(HC)ç®—æ³•æ”¶é›†ç›¸å…³ç‰¹å¾ç»„ã€‚ç¬¬äºŒæ¬¡ï¼Œä½¿ç”¨æ¢¯åº¦æ¨è¿›æ ‘åˆ†ç±»å™¨(GBTC)å°†è¿™ç§èšç±»æ–¹æ³•ä¸ç‰¹å¾é‡è¦æ€§è¯„ä¼°ç›¸ç»“åˆã€‚

**I.1 ç‰¹å¾ç›¸å…³æ€§åˆ†æ**

å¦‚ä¸Šæ‰€è¿°ï¼Œæˆ‘ä»¬è®¡ç®—ç‰¹å¾ä¹‹é—´çš„æˆå¯¹ç›¸å…³æ€§:

```
correl_fig,ax=plt.subplots(1,1,figsize=(10,10),)
indexax=np.arange(0,len(feat_all.columns))
sn.heatmap(feat_all.corr())
ax.set_xticks(indexax)
ax.set_xticklabels(feat_all.columns)
ax.set_yticks(indexax)
ax.set_yticklabels(feat_all.columns)
correl_fig.tight_layout()
plt.show()
```

![](img/4512cf0a646b9a26b24207b0d6978627.png)

å›¾ 1:æ‰€æœ‰ 30 ä¸ªç‰¹å¾çš„æˆå¯¹ç›¸å…³å›¾ã€‚

ç‰¹å¾é€‰æ‹©çš„ç¬¬ä¸€æ­¥æ˜¯æ„å»ºç›¸å…³ç‰¹å¾ç»„(æ³¨æ„ï¼Œä½¿ç”¨ Spearman ç›¸å…³æ€§å¯ä»¥éµå¾ªç±»ä¼¼çš„æ–¹æ³•[5])ã€‚äº‹å®ä¸Šï¼Œå½“ç‰¹å¾é«˜åº¦ç›¸å…³æ—¶ï¼Œæ„å‘³ç€å®ƒä»¬ä¼ è¾¾äº†ç›¸è¿‘æˆ–ç›¸ä¼¼çš„ä¿¡æ¯ã€‚ç»“æœï¼Œè¯¥ç»„çš„å•ä¸ªç‰¹å¾è€Œä¸æ˜¯æ‰€æœ‰ç‰¹å¾å¯ä»¥è¢«è€ƒè™‘ç”¨äºé¢„æµ‹ï¼Œä»è€Œé¿å…å†—ä½™ã€‚

æˆ‘ä»¬é€šè¿‡ä½¿ç”¨å±‚æ¬¡èšç±»(HC)æ¥æ„å»ºç›¸å…³ç‰¹å¾çš„ç»„(æ›´å¤šç»†èŠ‚å‚è§[6])ã€‚å› æ­¤ï¼Œåº”ç”¨äº†ä»¥ä¸‹è¿‡ç¨‹ï¼Œè¯¥è¿‡ç¨‹èƒ½å¤Ÿæ„å»ºä»æ ‘å¶åˆ°æ ¹çš„æ ‘(ç§°ä¸º HC æ ‘ï¼Œå…¶ä¸­æ¯ä¸ªèŠ‚ç‚¹ä»£è¡¨ä¸€ä¸ªç‰¹å¾é›†ç¾¤):

*   1.é¦–å…ˆå°†æ¯ä¸ªç‰¹å¾å®šä¹‰ä¸ºä¸€ä¸ªç°‡(HC æ ‘çš„ç¬¬ 0 å±‚ï¼Œæ¯ä¸ªç‰¹å¾æ˜¯ä¸€ä¸ªå¶å­)ã€‚
*   2.è®¡ç®—æ¯ä¸ªèšç±»ä¹‹é—´çš„è·ç¦»ã€‚å¦‚æœğ¹k æ˜¯èšç±» I ä¸­çš„ç‰¹å¾ï¼Œğºğ‘¡æ˜¯èšç±» 2 ä¸­çš„ç‰¹å¾ï¼Œåˆ™èšç±» I å’Œ j ä¹‹é—´çš„è·ç¦»æ˜¯ğ‘šğ‘–ğ‘›_ğ‘˜,ğ‘¡(ğ¶ğ‘œğ‘Ÿğ‘Ÿ(ğ¹ğ‘˜,ğºğ‘¡)).å…¶ä¸­ğ¶ğ‘œğ‘Ÿğ‘Ÿ(ğ¹ğ‘˜,ğºğ‘¡)æ˜¯ç‰¹å¾ f å’Œ g ä¹‹é—´çš„ç›¸å…³æ€§
*   3.å¯¹äºç»™å®šçš„ç°‡ Iï¼Œå°†å…¶ä¸æœ€æ¥è¿‘çš„ä¸€ä¸ª(è®°ä¸º j)åˆå¹¶ã€‚è¿™ç§åˆå¹¶ç”±ä¸¤ä¸ªåˆå¹¶åˆ†æ”¯è¡¨ç¤ºï¼Œä¸€ä¸ªæ¥è‡ªä¸ I ç›¸å…³è”çš„èŠ‚ç‚¹ï¼Œå¦ä¸€ä¸ªæ¥è‡ªä¸ j ç›¸å…³è”çš„èŠ‚ç‚¹ã€‚è¿™äº›åˆ†æ”¯å½¢æˆäº†è¡¨ç¤º I å’Œ j ä¹‹é—´çš„åˆå¹¶çš„æ ‘ä¸­çš„ä¸Šå±‚èŠ‚ç‚¹ã€‚
*   4.é‡å¤è¿™äº›èšç±»åˆå¹¶æ“ä½œï¼Œç›´åˆ°æœ‰ä¸€ä¸ªé›†åˆäº†æ‰€æœ‰ç‰¹å¾(å¯¹åº”äº HC æ ‘çš„æ ¹)ã€‚

åœ¨å›ºå®šä¸€ä¸ªçº§åˆ«åï¼Œè¯¥çº§åˆ«çš„ä¸åŒèŠ‚ç‚¹äº§ç”Ÿç‰¹å¾çš„èšç±»ã€‚æ›´å‡†ç¡®åœ°è¯´ï¼Œå¦‚æœè€ƒè™‘ä¸ä¸€ä¸ªé›†ç¾¤ç›¸å…³è”ç»™å®šèŠ‚ç‚¹ã€‚ä¸ä¹‹ç›¸è¿çš„å¶å­ä»£è¡¨äº†è¿™ä¸ªé›†ç¾¤ä¸­æ¶‰åŠçš„ç‰¹å¾ã€‚

```
corr_fig1,ax1=plt.subplots(1,1,figsize=(10, 8),)# Compute pair wise correlations
corr=feat_all.corr().values# Compute the hierarchical clustering tree
link=hierarchy.ward(corr)
dendro=hierarchy.dendrogram(link,labels=feat_all.columns,ax=ax1,leaf_rotation=90,leaf_font_size=10)
dendro_index=np.arange(0,len(dendro["ivl"]))
corr_fig1.tight_layout()
plt.show()
corr_fig1.savefig("corrtree.png")
```

![](img/9121878de034c2c0ad721def7bbe0fcb.png)

å›¾ 2:ç”± HC ç®—æ³•è®¡ç®—å‡ºçš„æ ‘ã€‚

```
corr_fig2,ax2=plt.subplots(1,1,figsize=(10, 8),)# Compute the correlation heat map
im=ax2.imshow(corr[dendro["leaves"],:][:,dendro["leaves"]])
ax2.set_xticks(dendro_index)
ax2.set_yticks(dendro_index)
ax2.set_xticklabels(np.array(dendro["ivl"]),rotation="vertical")
ax2.set_yticklabels(np.array(dendro["ivl"]))
corr_fig2.tight_layout()
plt.colorbar(im)
plt.show()
corr_fig2.savefig("corrmap.png")
```

![](img/9cd19534525061efefc1ced720090761.png)

å›¾ 3:HC èšç±»ä¸­é‡æ’ç‰¹å¾çš„ç›¸å…³çƒ­å›¾ã€‚

å›¾ 2 æ˜¾ç¤ºäº†æ ¹æ® HC æ–¹æ³•æ„å»ºçš„æ ‘ã€‚é€šè¿‡å›ºå®šç­‰çº§ 4ï¼Œçœ‹èµ·æ¥å¯ä»¥å®šä¹‰å››ä¸ªä¸åŒçš„ç‰¹å¾èšç±»(å‚è§å›¾ 2ï¼ŒHC æ ‘çš„å¶å­ä»å·¦åˆ°å³:èšç±» 1ï¼Œä»â€œå¹³å‡å¯¹ç§°â€åˆ°â€œåˆ†å½¢ç»´æ•°è¯¯å·®â€ï¼›èšç±» 2ï¼Œä»â€œå¹³å‡çº¹ç†â€åˆ°â€œå¯¹ç§°è¯¯å·®â€ï¼›èšç±» 3ï¼Œä»â€œå¹³å‡å‡¹ç‚¹â€åˆ°â€œå¹³å‡å‡¹åº¦â€ï¼›èšç±» 4ï¼Œä»â€œå¹³å‡å‘¨é•¿â€åˆ°â€œå‘¨é•¿è¯¯å·®â€)ã€‚

å¯¹äºæˆ‘ä»¬çš„é™ç»´é—®é¢˜ï¼Œå› æ­¤éœ€è¦é€‰æ‹©ä¸å±äºåŒä¸€èšç±»çš„ç‰¹å¾ã€‚å›¾ 3 ç¤ºå‡ºäº†çƒ­å›¾ç›¸å…³çŸ©é˜µï¼Œå…¶ä¸­é‡æ–°æ’åˆ—çš„ç‰¹å¾å–å†³äºå®ƒä»¬çš„èšç±»(å³éµå¾ªå›¾ 2 ä¸­ HC æ ‘ç»™å‡ºçš„é¡ºåº)ã€‚

æˆ‘ä»¬ç¡®å®šæ¯ä¸ªç‰¹å¾çš„èšç±»ï¼Œå¹¶å°†ä¿¡æ¯å­˜å‚¨åœ¨å­—å…¸ä¸­ã€‚

```
# Color list of the four feature clusters
color_list=[â€œredâ€,â€navyâ€,â€blackâ€,â€greenâ€]# Fix a level of four in the HC tree to determine feature clusters
clusterlevel=4 # Determine the id cluster list
clusterid_list= hierarchy.fcluster(link,clusterlevel, \criterion=â€™distanceâ€™)# This dictionary will contain the list of features for each 
# cluster
featurecluster_dict = dict()for idx, clusterid in enumerate(clusterid_list):
 if clusterid not in featurecluster_dict.keys():
     featurecluster_dict[clusterid]=dict()
     featurecluster_dict[clusterid][â€œnumfeatâ€]=[]
     featurecluster_dict[clusterid][â€œnamefeatâ€]=[]
 featurecluster_dict[clusterid][â€œcolorâ€]=color_list[clusterid-1]
 featurecluster_dict[clusterid][â€œnumfeatâ€].append(idx)
 featurecluster_dict[clusterid]\
 [â€œnamefeatâ€].append(feat_all.columns[idx])
```

## I.2 æ•°æ®é¢„å¤„ç†

ç°åœ¨ï¼Œæˆ‘ä»¬å¯¹æ•°æ®è¿›è¡Œé¢„å¤„ç†ï¼Œä»¥:

*   1.ä¸ºæœªæ¥çš„æœºå™¨å­¦ä¹ æ¨¡å‹è§„èŒƒåŒ–ç‰¹å¾ã€‚è¿™ä¸€æ ‡å‡†åŒ–è¿‡ç¨‹æ˜¯é€šè¿‡å‡å»å¹³å‡å€¼å¹¶é™¤ä»¥æ¯ä¸ªç‰¹å¾çš„æ ‡å‡†åå·®æ¥å®ç°çš„ã€‚
*   2.åœ¨å®šå‹é›†å’Œæµ‹è¯•é›†ä¹‹é—´æ‹†åˆ†æ•°æ®ã€‚

```
def preprocess(trainrate=1.0):
    '''
    Load data, normalize and split between training and testing
    sets Input:
      trainrate: [Float] Relative size of the training set Output:
      feat_all:    [DataFrame] All the data features
      label_all:   [DataFrame] All the labels
      featnames:   [List] All the feature names
      featN_all:   [DataFrame] All the normalized features
      feat_train:  [DataFrame] Training features
      featN_train: [DataFrame] Normalized training features
      feat_test:   [DataFrame] Testing features
      featN_test: [DataFrame] Normalized testing features
      label_train: [DataFrame] Training labels
      label_test:  [DataFrame] Testing labels
      normmean_arr:[Array] All the features' means for
                           normalization
      normstd_arr: [Array] All features' standard 
                           deviation for normalization '''
    # Load the data and get the name of all features    feat_all,label_all=\
    load_breast_cancer(return_X_y=True,as_frame=True)
    featnames=np.array(feat_all.columns) # Shuffle the data
    data_all=pd.concat([feat_all,label_all],axis=1).sample(frac=1)    
    label_all=data_all["target"]
    feat_all=data_all.drop("target",axis=1) # Get normalized features    
    StdSc=StandardScaler()
    StdSc.fit(feat_all)
    featN_all=StdSc.transform(feat_all)
    featN_all=pd.DataFrame(featN_all,columns=feat_all.columns)

    # Split between training and testing sets
    trainsize=int(trainrate*len(feat_all.index))
    feat_train=feat_all[:trainsize]
    featN_train=featN_all[:trainsize]
    label_train=label_all[:trainsize]
    feat_test=feat_all[trainsize:]
    featN_test=featN_all[trainsize:]
    label_test=label_all[trainsize:] normmean_arr=StdSc.mean_
    normstd_arr=(StdSc.var_)**0.5 return\ 
    feat_all,label_all,featnames,featN_all,feat_train,\
    featN_train,feat_test,featN_test,label_train,label_test,\
    normmean_arr,normstd_arrtrainrate=0.8
feat_all,label_all,featnames,featN_all,feat_train,featN_train,feat_test,featN_test,label_train,\
label_test,mean_feat,std_feat=preprocess(trainrate=trainrate)
```

éšæœºé€‰æ‹©ä»£è¡¨æ‰€æœ‰æ•°æ®çš„ 20%çš„æµ‹è¯•é›†ï¼Œå…¶ä½™çš„ç”¨äºè®­ç»ƒã€‚

# I.3 ç¡®å®šç‰¹æ€§çš„é‡è¦æ€§

æˆ‘ä»¬ç°åœ¨é€šè¿‡è¯„ä¼°ç‰¹å¾é‡è¦æ€§åˆ†æ•°æ¥ç»“åˆè¿™ç§ç‰¹å¾èšç±»æ–¹æ³•ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å»ºç«‹äº†ä¸€ä¸ªæ¢¯åº¦æ¨è¿›æ ‘åˆ†ç±»å™¨(GBTC)ç®—æ³•ã€‚ä¸ºäº†è§£é‡Šä»€ä¹ˆæ˜¯ GBTCï¼Œæˆ‘ä»¬éœ€è¦é¦–å…ˆæ¾„æ¸…ä»€ä¹ˆæ˜¯å†³ç­–æ ‘ã€‚å†³ç­–æ ‘æ˜¯ä¸€ç§æœºå™¨å­¦ä¹ ç®—æ³•ï¼Œé€šå¸¸ç”¨äºå›å½’å’Œå†³ç­–ä»»åŠ¡ã€‚ä»ç”±å‡ ä¸ªç‰¹å¾è¡¨å¾çš„æ•°æ®é›†åˆä¸­ï¼Œç›®çš„æ˜¯ä¸ºæ¯ä¸ªæ•°æ®å¾—å‡ºå†³ç­–ï¼Œå¦‚æœæ˜¯å›å½’é—®é¢˜ï¼Œè¯¥å†³ç­–å¯ä»¥æ˜¯æ•°å­—ï¼Œæˆ–è€…åœ¨åˆ†ç±»çš„æƒ…å†µä¸‹æ˜¯ç¦»æ•£æ ‡ç­¾ã€‚ä¸ºäº†è·å¾—å†³ç­–ï¼Œé€šè¿‡é€’å½’åœ°æ„é€ èŠ‚ç‚¹æ¥æ„å»ºæ ‘ï¼Œå…¶ä¸­åœ¨æ¯ä¸ªèŠ‚ç‚¹ä¸Šåº”ç”¨è¿™äº›æ¡ä»¶æ¥åˆ†å‰²æ•°æ®(å‚è§å›¾ 4)ã€‚è¿™äº›æ¡ä»¶æ˜¯å›ºå®šçš„ï¼Œä»¥ä¾¿æœ€å¤§åŒ–ä¸€ä¸ªæ ‡å‡†ï¼Œå¦‚è¿™é‡Œçš„åŸºå°¼åˆ†ç±»æ ‡å‡†ã€‚

![](img/a0c23ddf47f0ec92dfa6d6c7de668d35.png)

å›¾ 4:å†³ç­–æ ‘çš„ç¤ºæ„å›¾ï¼Œå½“å½¢æˆå†³ç­–èŠ‚ç‚¹ç›´åˆ°åˆ°è¾¾å¶å­æ—¶ï¼Œè®­ç»ƒæ•°æ®è¢«é€’å½’åˆ†è£‚ã€‚

åœ¨æ¯ä¸ªèŠ‚ç‚¹ï¼Œé€‰æ‹©ä¸€ä¸ªç‰¹å¾æˆ–ç‰¹å¾é›†åˆï¼Œå¹¶ä¸”å¯¹äºå®ƒä»¬ä¸­çš„æ¯ä¸€ä¸ªï¼Œé˜ˆå€¼æ˜¯å›ºå®šçš„(åœ¨è®­ç»ƒæœŸé—´ä½¿ç”¨æ‰€è€ƒè™‘çš„æ ‡å‡†æ¥ç¡®å®šæ‰€é€‰æ‹©çš„ç‰¹å¾é›†åˆä»¥åŠé˜ˆå€¼)ã€‚æ ¹æ®å®šä¹‰çš„é˜ˆå€¼ï¼Œæ•°æ®åœ¨ä¸åŒçš„èŠ‚ç‚¹ä¹‹é—´æ‹†åˆ†ã€‚é‡å¤è¯¥åˆ†è£‚è¿‡ç¨‹ï¼Œç›´åˆ°åˆ°è¾¾å…·æœ‰æœ€å°æ•°é‡æ•°æ®çš„èŠ‚ç‚¹ï¼Œæˆ–è€…å¦‚æœæ ‘è¾¾åˆ°æé™æ·±åº¦(è¿™äº›æé™è¢«é¢„å…ˆç¡®å®šä¸ºè¶…å‚æ•°)ã€‚è¿™äº›æœ€ç»ˆèŠ‚ç‚¹è¢«ç§°ä¸ºå¶å­ï¼Œå¹¶ä¸”æ¯ä¸ªéƒ½ä¸ä¸€ä¸ªå†³ç­–å€¼ç›¸å…³è”ã€‚

GBTC ç®—æ³•æ˜¯ç”±ä¸€ç»„å†³ç­–æ ‘ç»„æˆçš„ã€‚å¯¹äºæˆå¯¹çš„è®­ç»ƒç‰¹å¾å‘é‡å’Œæ ‡ç­¾(ï¼Œyi)ï¼Œæ„å»ºè¯¥é›†åˆä½¿å¾—å¯¹äºæ¯ä¸ªï¼Œ

![](img/d0124aa4954604a6e0295b5796f85f17.png)

ä¸€å®šæ˜¯ç¦»æ˜“æœ€è¿‘çš„ã€‚å¦‚æœå¼•å…¥æŸå¤±å‡½æ•°

![](img/3749cfa45f99ffc6dba84614ef7530e4.png)

æ„å»º p+1 å†³ç­–æ ‘å¯¹åº”äºæ‰¾åˆ° T å‡½æ•°ï¼Œä½¿å¾— T(xi)=-Giã€‚å…¶ä¸­ Gi æ˜¯æŸå¤±å‡½æ•°åœ¨å‡½æ•° Hp ä¸Šçš„ xi æ¢¯åº¦ã€‚æ¢å¥è¯è¯´

![](img/270fa6b253ef464e59e275a8057d7b47.png)

å½“æˆ‘ä»¬åœ¨è¿™é‡Œå°†è¯¥æ¨¡å‹ç”¨äºåˆ†ç±»ä»»åŠ¡æ—¶ï¼Œé€šè¿‡å°† sigmoid å‡½æ•°åº”ç”¨äºæ„å»ºçš„ GTBC å‡½æ•°æ¥è·å¾—é¢„æµ‹çš„ç±»åˆ«ã€‚

![](img/9edf1303e8d349feabe0b7d050492aa0.png)

å…³äºæ¢¯åº¦å¢å¼ºçš„æ›´å¤šç»†èŠ‚åœ¨[4]ä¸­æä¾›ã€‚GBTC å¯¹äºå›å½’å’Œåˆ†ç±»éƒ½æ˜¯æœ‰ç”¨çš„ï¼Œå¹¶ä¸”å¯ä»¥è¢«è®¤ä¸ºæ˜¯ç¡®å®šç‰¹å¾çš„é‡è¦æ€§ã€‚å¯¹äºç»™å®šçš„ç‰¹å¾ï¼Œå…¶åœ¨ä¸åŒå†³ç­–æ ‘ä¸­çš„å¹³å‡æ°´å¹³ä½ç½®è¢«ç”¨äºè¯„ä¼°å…¶åœ¨æ ‡ç­¾é¢„æµ‹ä¸­çš„é‡è¦æ€§ã€‚æœ€é‡è¦çš„ç‰¹å¾æ˜¯é‚£äº›ä½äºå†³ç­–æ ‘æ„é€ ä¹‹åˆçš„ç‰¹å¾ã€‚å®ƒä»¬å®é™…ä¸Šå¯¹åº”äºå¯¹å†³ç­–æ ‘æ„é€ çš„æ•°æ®åˆ†å‰²å…·æœ‰æœ€å¼ºå½±å“çš„é‚£äº›ã€‚ä¸‹é¢æˆ‘ä»¬ä»‹ç»ä¸€ä¸ªåŠŸèƒ½è¡¨:

*   æ¥ç¡®å®šå†³ç­–æ ‘é›†åˆçš„æœ€ä½³å¤§å°ã€‚
*   æ¥è®­ç»ƒ GBTC ç®—æ³•ã€‚
*   è¯„ä¼°ç‰¹å¾çš„é‡è¦æ€§ã€‚

```
def trainGbc(params,feat_train,label_train,feat_test,label_test,\
setbestestim=False,setfeatimp=False,featurecluster_dict=None):
    '''
     This function trains a gradient boosting algorithm, if 
     required, it determines the best number of n_estimators
     and evaluates feature importances Input:
       params: [Dict] Parameters for the GBTC's construction
       feat_train:  [DataFrame] Training features
       label_train: [DataFrame] Training labels
       feat_test:   [DataFrame] Testing features
       label_test:  [DataFrame] Testing labels
       setbestestim: [Bool] If True, determines the best size
                            of the decision trees' ensemble
       setfeatimp: [Bool] If True determines features' importances
       featurecluster_dict: [Dict] If not None, dictionary of the
                                   feature clusters Output:
       Gbc: [Sklearn Instance] A trained GBTC estimator ''' # If the best number of estimators has to be determined
    if setbestestim: Gbc=GradientBoostingClassifier(**params)
        Gbc.fit(feat_train,label_train) # Determine the best n_estimators
        scoretest_list=[]
        scoretrain_list=[]

        # Compute accuracy scores for training and testing with
        # different n_estimators for pred_test in Gbc.staged_predict(feat_test):
            scoretest=accuracy_score(label_test,pred_test)
            scoretest_list.append(scoretest)
        for pred_train in Gbc.staged_predict(feat_train):
            scoretrain=accuracy_score(label_train,pred_train)
            scoretrain_list.append(scoretrain) # Plot the figure showing the training and testing
      # accuracies' evolution with n_estimators       nestim_fig,ax=plt.subplots(1,1,figsize=(10,8),)
       plt.plot(np.arange(params["n_estimators"]),\
       scoretrain_list,label="Train")
       plt.plot(np.arange(params["n_estimators"]),
       scoretest_list,label="Test")
       plt.legend()
       plt.xlabel("n_estimators")
       plt.ylabel("Accuracy")
       nestim_fig.savefig("nestim.pdf")
       plt.show()
      # Cross validate and fit a GBTC estimator
    else:
       Gbc=GradientBoostingClassifier(**params)
       score=cross_validate(Gbc,feat_train,label_train,\
        cv=5,scoring="accuracy")
       print("Gbc Cross Validation Accuracy (Testing)")
       print(np.mean(score["test_score"]))
       Gbc.fit(feat_train,label_train) #Determine feature importance
    if setfeatimp:
       impfeat_list=Gbc.feature_importances_
       indexsort=np.argsort(Gbc.feature_importances_)
       impfeat_fig,ax=plt.subplots(1,1,figsize=(10,8),)
       pos=np.arange(len(indexsort))+0.5
       plt.barh(pos,impfeat_list[indexsort]) 
       plt.yticks(pos,np.array(feat_train.columns)
       [indexsort],fontsize=10,color="red") # If feature clustering, color the features depending on 
       # their clusters
       if featurecluster_dict!=None:
           for ifeat,featname in\
            enumerate(np.array(feat_train.columns)[indexsort]):
               for clusterkey in featurecluster_dict.keys():
                   if featname in \
                   featurecluster_dict[clusterkey]["namefeat"]:

                         ax.get_yticklabels()[ifeat].set_color\
                        (featurecluster_dict\
                        [clusterkey]["color"])

            plt.xlabel("Importance")
            plt.xscale("log")
            plt.xticks(size=10)
            impfeat_fig.savefig("impfeat.pdf")
            plt.show()
    return Gbc
```

æˆ‘ä»¬é¦–å…ˆéœ€è¦ç¡®å®šå†³ç­–æ ‘é›†åˆçš„å¤§å°æ¥æ„é€  GBTC ä¼°è®¡é‡ã€‚ä¸ºæ­¤ï¼Œæ ¹æ®æ‰€ç”¨å†³ç­–æ ‘çš„æ•°é‡å¯¹è®­ç»ƒé›†å’Œæµ‹è¯•é›†çš„å‡†ç¡®æ€§è¿›è¡Œè¯„ä¼°ã€‚

```
# Determine the best number of estimators
params=\{"n_estimators":500,"learning_rate":0.01,"min_samples_split":5,"max_depth":4}Gbc=trainGbc(params,feat_train,label_train,feat_test,label_test,setbestestim=True,setfeatimp=False)
```

å¯ä»¥çœ‹å‡ºï¼Œ300 ä¸ªå†³ç­–æ ‘èƒ½å¤Ÿè¾¾åˆ°å¤§çº¦ 0.95 çš„æµ‹è¯•ç²¾åº¦å’Œ 0.99 çš„è®­ç»ƒç²¾åº¦(è§å›¾ 5)ã€‚å› æ­¤ï¼Œæˆ‘ä»¬é€‰æ‹©è¿™ç§è§„æ¨¡çš„å†³ç­–æ ‘é›†åˆã€‚ä¿®å¤å®ƒï¼Œè¯„ä¼° 30 ä¸ªç‰¹å¾ä¸­æ¯ä¸€ä¸ªçš„é‡è¦æ€§ã€‚

![](img/022ac472240a6c3a374a84f8528e54ad.png)

å›¾ 5:è®­ç»ƒå’Œæµ‹è¯•å‡†ç¡®ç‡éšå†³ç­–æ ‘é›†åˆå¤§å°çš„å˜åŒ–ã€‚

```
# Determine feature importance with n_estimator=300
params=\
{"n_estimators":300,"learning_rate":0.01,"min_samples_split":5,"max_depth":4}Gbc=trainGbc(params,feat_train,label_train,feat_test,\
label_test,setbestestim=False,setfeatimp=True,featurecluster_dict=featurecluster_dict)
```

![](img/89daeacf361fd6f153d4ea98fd5389f1.png)

å›¾ 6:æ ¹æ® GBTC ç®—æ³•è¯„ä¼°ç‰¹å¾çš„é‡è¦æ€§ã€‚ç›¸åŒé¢œè‰²çš„ç‰¹å¾åç§°æ˜¯é‚£äº›å‚åŠ ç”± HC æ¨å¯¼çš„ç›¸åŒèšç±»çš„ç‰¹å¾åç§°ã€‚

æˆ‘ä»¬æ£€æŸ¥äº†ï¼Œåœ¨æµ‹è¯•é›†ä¸Šï¼Œå…·æœ‰ 300 ä¸ªä¼°è®¡é‡çš„ GBTC ç®—æ³•è¾¾åˆ°äº† 0.951 çš„ç²¾åº¦ã€‚

```
predGBC_test=Gbc.predict(feat_test)
accGBC=accuracy_score(label_test,predGBC_test)
print("Testing Accuracy")
print(accGBC)Testing Accuracy
0.9513684210526315
```

åœ¨å›¾ 6 ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°â€œæœ€å·®åŠå¾„â€å’Œâ€œæœ€å·®å‡¹ç‚¹â€æ˜¯å±äºå…ˆå‰å»ºç«‹çš„ä¸åŒèšç±»çš„ä¸¤ä¸ªæœ€é‡è¦çš„ç‰¹å¾ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å±•ç¤ºäº†äº”ä¸ªæœ€é‡è¦ç‰¹å¾çš„é…å¯¹å›¾ã€‚æˆ‘ä»¬å¯ä»¥å¦‚é¢„æœŸçš„é‚£æ ·æ³¨æ„åˆ°ä¸¤ä¸ªæœ€é‡è¦çš„ç‰¹å¾â€œæœ€å·®åŠå¾„â€å’Œâ€œæœ€å·®å‘¨é•¿â€æ˜¯å¼ºç›¸å…³çš„(å®ƒä»¬å±äºç›¸åŒçš„ç‰¹å¾èšç±»)ã€‚è¿™å°±æ˜¯ä¸ºä»€ä¹ˆåœ¨ç‰¹å¾é€‰æ‹©è¿‡ç¨‹ä¸­â€œæœ€å·®å‘¨é•¿â€è¢«â€œæœ€å·®å‡¹ç‚¹â€æ‰€å–ä»£ã€‚

```
def pairplot(feat_all,label_all,featimp_list):

    '''
    Compute the pair plots from a list of features
    Input:
      feat_all: [DataFrame] All the features
      label_all: [DataFrame] All the labels
      featimp_list: [List] Name of the features to compute the
                           pair plots ''' mostfeat_list=feat_all.columns[np.argsort(\
    Gbc.feature_importances_)][-5:]
    pairplot_fig=plt.figure()
    sns_plot=sn.pairplot(pd.concat(\
    [feat_all[mostfeat_list],label_all],axis=1), hue='target')
    fig = sns_plot
    fig.savefig("pairplot.pdf") featimp_list=list(Gbc.feature_importances_)
pairplot(feat_all,label_all,featimp_list)
```

![](img/3362aaef7f7cd2dfbed87e993e0a95c3.png)

å›¾ 7:ä» GBTC æ¨å¯¼å‡ºçš„äº”ä¸ªæœ€é‡è¦ç‰¹å¾çš„é…å¯¹å›¾ã€‚

æˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªå‡½æ•°ï¼Œè¯¥å‡½æ•°åŸºäºæ‰€é€‰æ‹©çš„ç‰¹å¾æ¥é™ä½ç‰¹å¾(æœªå½’ä¸€åŒ–å’Œå½’ä¸€åŒ–)å‘é‡çš„ç»´æ•°ã€‚

```
def selectfeat(feat_selected,feat_train,\
feat_test,feat_all,featN_train,featN_test,featN_all\
,meanfeat_arr,stdfeat_arr): '''
   Reduce the dimensionality of the feature vectors based on
   the feature selected Input:
      feat_selected:  [List] Names of the features selected
      feat_train:  [DataFrame] Training features
      feat_test:   [DataFrame] Testing features
      feat_all: [DataFrame] All the features
      featN_train: [DataFrame] Normalized training
                               features
      featN_test: [DataFrame] Normalized testing
                              features
      featN_all: [DataFrame] All the normalized features
      meanfeat_arr:[Array] All the features' means for 
                          normalization
      stdfeat_arr: [Array] All features' standard 
                           deviation for normalization Output:
      featS_train: [DataFrame] Training selected features
      featS_test: [DataFrame] Testing selected features
      featS_all:  [DataFrame] All selected features
      mean_featS: [Array] Means of the selected features
      std_featS: [Array] Standard deviations of the selected
                         features
      featNS_train: [DataFrame] Training selected and normalized
                                features
      featNS_test:[DataFrame] Testing selected and normalized
                              features
      featNS_all: [DataFrame] All selected and normalized
                              features ''' # Identify selected features' indexes 
    indexfeat1=np.where(feat_train.columns==\
    feat_selected[0])[0][0]
    indexfeat2=np.where(feat_train.columns==\
    feat_selected[1])[0][0] # Determine the means and standard deviations corresponding
    # to the selected features    mean_featS=meanfeat_arr[[indexfeat1,indexfeat2]]
    std_featS=stdfeat_arr[[indexfeat1,indexfeat2]] # Select the features for unormalized and normalized data
    featS_train=feat_train[feat_selected]
    featS_test=feat_test[feat_selected]
    featS_all=feat_all[feat_selected]
    featNS_train=featN_train[feat_selected]
    featNS_test=featN_test[feat_selected]
    featNS_all=featN_all[feat_selected] return  featS_train,featS_test,featS_all,mean_featS,\
    std_featS,featNS_train,featNS_test,featNS_all# Select the two most important variables (unormalized and normalized)
feat_selected=["worst radius","worst concave points"]
featS_train,featS_test,featS_all,mean_featS,std_featS,featNS_train,featNS_test,featNS_all\
=selectfeat(feat_selected,feat_train,feat_test,feat_all,featN_train,featN_test,featN_all)
```

æˆ‘ä»¬åœ¨å¹³é¢å›¾ä¸­ç»˜åˆ¶ä¸¤ä¸ªé€‰å®šç‰¹å¾çš„æ•°æ®ï¼Œâ€œæœ€å·®åŠå¾„â€å’Œâ€œæœ€å·®å‡¹ç‚¹â€ã€‚æ³¨æ„ï¼Œè¿™ä¸ªäºŒç»´ç©ºé—´ä¸­çš„æ•°æ®æŠ•å½±èƒ½å¤Ÿè¯†åˆ«ä¸¤ä¸ªä¸åŒçš„èšç±»ï¼Œæ¯ä¸ªèšç±»ä¸ä¸¤ä¸ªæ ‡ç­¾ä¸­çš„ä¸€ä¸ªç›¸å…³è”ã€‚

```
def plotlabel(featS_all,feat_selected):'''
   Plot the data using the selected features
   Input:
     featS_all:  [DataFrame] All selected features
     feat_selected:  [List] Names of the features selected ''' label_fig,ax=plt.subplots(1,1,figsize=(10,8),) # Plot the data in the 2D-space
    plt.scatter(featS_all.values[:,0]\
   ,featS_all.values[:,1],c=label_all,\
    cmap=matplotlib.colors.ListedColormap(["red","navy"])) # Plots for the legend
    plt.scatter(featS_all.values[:,0]\
    [np.where(label_train.values==0)[0]][0],\
    featS_all.values[:,1][np.where(label_train.values==0)\
    [0]][0],c="red",label="Malignant") plt.scatter(featS_all.values[:,0\
   [np.where(label_train.values==1)[0]][0],\
   featS_all.values[:,1][np.where(label_train.values==1)\
   [0]][0],c="navy",label="Benign") plt.xlabel(feat_selected[0])
   plt.ylabel(feat_selected[1])
   plt.legend()
   label_fig.savefig("label.png")
   plt.show()# plot the data in the 2D-space
plotlabel(featS_all,feat_selected)
```

![](img/5adf026e378b8301c7c37e5e2aa99454.png)

å›¾ 8:è®¡åˆ’ä¸­â€œæœ€å·®åŠå¾„â€å’Œâ€œæœ€å·®å‡¹ç‚¹â€çš„æ•°æ®æŠ•å½±ã€‚

æˆ‘ä»¬ç°åœ¨çš„ç›®çš„æ˜¯åœ¨ç”±ä¸¤ä¸ªç‰¹å¾â€œæœ€å·®åŠå¾„â€å’Œâ€œæœ€å·®å‡¹ç‚¹â€å½¢æˆçš„äºŒç»´ç©ºé—´ä¸­å¯¼å‡ºä¸€ä¸ªå‡½æ•°ï¼Œä»¥è¯„ä¼°å‘å±•ä¸ºæ¶æ€§ä¹³è…ºç™Œçš„æ¦‚ç‡é£é™©ã€‚ä¸ºæ­¤ï¼Œä¸‹ä¸€èŠ‚å°†å¼€å‘ä¸€ç§æ”¯æŒå‘é‡æœºåˆ†ç±»å™¨(SVMC)ç®—æ³•

# äºŒã€‚ä» SVMC æ¨å¯¼å‡ºæ¦‚ç‡é£é™©

åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬æ„å»º SVC æ¨¡å‹ï¼Œæ ¹æ®ä¸¤ä¸ªå…ˆå‰é€‰æ‹©çš„ç‰¹å¾æ¥ä¼°è®¡å‘å±•ä¸ºæ¶æ€§ä¹³è…ºç™Œçš„æ¦‚ç‡é£é™©ã€‚é¦–å…ˆæˆ‘ä»¬ç¡®å®š SVC çš„è¶…å‚æ•°ã€‚ç¬¬äºŒæ¬¡ï¼Œæ¨¡å‹è¢«ç”¨äºå®ç°è®¡åˆ’ä¸­çš„é¢„æµ‹(â€œå·¥ä½œåŠå¾„â€ã€â€œæœ€å·®å‡¹ç‚¹â€)ã€‚æœ€ç»ˆç¡®å®šä¸é¢„æµ‹ç›¸å…³çš„æ¦‚ç‡ä¼°è®¡ã€‚

**äºŒ. 1 è¶…å‚æ•°çš„ç¡®å®š**

SVC æ—¨åœ¨ç¡®å®šå…·æœ‰ç›¸åŒæ ‡ç­¾çš„æ•°æ®è¢«é‡æ–°åˆ†ç»„çš„åŒºåŸŸã€‚è¿™äº›åŒºåŸŸç”±è¾¹ç•Œç•Œå®šï¼Œè¿™äº›è¾¹ç•Œè¢«è®¡ç®—ä»¥æœ€å¤§åŒ–å®ƒä»¬ä¸æ•°æ®ä¹‹é—´çš„è·ç¦»ã€‚è¿™ä¸ªè·ç¦»å«åšè¾¹ç¼˜ã€‚ä¸ºäº†è®¡ç®—ç‰¹å¾ç©ºé—´ä¸­çš„è·ç¦»ï¼Œå¯ä»¥ä½¿ç”¨æ ¸ã€‚ä½¿ç”¨æ ¸ä½œä¸ºéçº¿æ€§å‡½æ•°ç¡®å®èƒ½å¤Ÿæ¨¡æ‹Ÿæ›´çµæ´»çš„è¾¹ç•Œï¼Œä»è€Œå¯¹äºæ•°æ®åˆ†ç¦»æ›´æœ‰æ•ˆã€‚è¿™é‡Œï¼Œæˆ‘ä»¬é‡‡ç”¨ä¸€ä¸ªç§°ä¸º RBF çš„é«˜æ–¯å‹æ ¸ï¼Œå®ƒå–å†³äºæ¯”ä¾‹å‚æ•°Î³ï¼Œä½¿å¾—ä¸¤ç‚¹ x å’Œ y ä¹‹é—´çš„è·ç¦»ä¸º:

![](img/3b5ae1e49d83e951369974c38da4a224.png)

å› æ­¤ï¼Œå¿…é¡»è°ƒæ•´è¯¥ä¼½é©¬å‚æ•°ã€‚é™¤æ­¤ä¹‹å¤–ï¼Œè¿˜å¿…é¡»ç¡®å®šå½±å“è¾¹ç•Œå¹³æ»‘åº¦çš„æ­£åˆ™åŒ–å› å­ Cã€‚ç”±äºäº¤å‰éªŒè¯ç ”ç©¶ï¼Œä¸¤ä¸ªå‚æ•°éƒ½æ˜¯å›ºå®šçš„ã€‚

![](img/82dcb858f86440b50c466d9b8ab9e32f.png)

å›¾ 9:æ ¸ SVC èƒ½å¤Ÿä½¿è¾¹ç•Œå…·æœ‰æ›´å¤æ‚çš„å½¢çŠ¶ï¼Œä»è€Œæ›´æœ‰æ•ˆåœ°è¿›è¡Œæ•°æ®åˆ†ç¦»ã€‚

```
def crossvalidateSVC(featNS_all,label_all,gammaonly=True): ''' Realize cross validation operations to determine the best
    SVC's hyperparameters Input:
      featNS_all: [DataFrame] All selected and normalized
                              features
      label_all: [DataFrame] Gathers all the labels
      gammaonly: [Bool] True if the gamma parameter is estimated
                        only, if False C is estimated either
   Output:
      bestgamma:  [Float] The best estimated gamma paramater
      bestc: [Float] The best estimated C parameter if gammaonly
                      is False ''' 
    # If just the gamma parameter is estimated  
    if gammaonly:
        param_grid={"gamma":np.logspace(0,2,200)}
        GridCV=GridSearchCV(SVC(),\
        param_grid=param_grid,scoring="accuracy",cv=10)
        GridCV.fit(featNS_all,label_all) gridcv_fig,ax=plt.subplots(1,1,figsize=(10,8),)
        plt.plot(np.array(GridCV.cv_results_["param_gamma"]),\
        np.array(GridCV.cv_results_["mean_test_score"]))
        plt.xlabel("Gamma")
        plt.ylabel("CV Accuracy")
        gridcv_fig.savefig("gridcv.png")
        plt.show() bestgamma=GridCV.best_params_["gamma"]
        return bestgamma # If both gamma and C are estimated
     else:
        param_grid=\
        {"C":np.logspace(-1,1,10),"gamma":np.logspace(0,2,200)}
        GridCV=GridSearchCV(SVC(),\
        param_grid=param_grid,scoring="accuracy",cv=10)
        GridCV.fit(featNS_all,label_all) gridcv_fig,ax=plt.subplots(1,1,figsize=(10,8),)
        plt.plot(np.array(GridCV.cv_results_["param_gamma"])\
       ,np.array(GridCV.cv_results_["mean_test_score"]))
        plt.xlabel("Gamma",fontsize=15)
        plt.ylabel("CV Accuracy",fontsize=15)
        gridcv_fig.savefig("gridcv.png")
        plt.show()
        bestgamma=GridCV.best_params_["gamma"]
        bestc=GridCV.best_params_["C"] return bestgamma,bestcbestgamma,bestc=crossvalidateSVC(featNS_all,\
label_all,gammaonly=False)
```

![](img/619ac1af9b73c3a9e20f7fce1b38aa9c.png)

å›¾ 9:äº¤å‰éªŒè¯å‡†ç¡®åº¦(æµ‹è¯•å¹³å‡å€¼)ä½œä¸ºä¼½é©¬å‚æ•°çš„å‡½æ•°ã€‚

```
bestgamma,bestc=crossvalidateSVC(featNS_all,label_all,gammaonly=False)
print(â€œBest Gammaâ€)
print(bestgamma)
print(â€œBest Câ€)
print(bestc)Best Gamma
2.0914343584919426
Best C
2.1544346900318834
```

æˆ‘ä»¬å‘ç°ï¼Œå›ºå®šğ›¾=2.09 å’Œ C=2.15 èƒ½å¤Ÿåœ¨ 10 å€äº¤å‰éªŒè¯æ•°æ®é›†ä¸­è¾¾åˆ° 0.9482 çš„å¹³å‡æµ‹è¯•ç²¾åº¦ï¼Œæ ‡å‡†åå·®ä¸º 0.043ã€‚å› æ­¤ï¼Œè¿™ä¸¤ä¸ªå‚æ•°çš„å€¼å°†åœ¨ä¸‹æ–‡ä¸­è€ƒè™‘ã€‚

```
def trainSVC(gamma,c,featNS_train,label_train):

    '''
    Realize a 10-fold cross validation for a SVC model
    and train a SVC model    Input
      gamma:  [Float] The best estimated gamma paramater
      c: [Float] The best estimated C parameter if gammaonly
                      is False
      featNS_train: [DataFrame] Training selected and normalized
                                features
      label_train: [DataFrame] Training labels Output:
     Svc: [Sklearn Instance] Trained SVC model ''' Svc=SVC(C=c,gamma=gamma,probability=True)
    score=cross_validate(Svc,featNS_train,label_train,cv=10)
    print("SVC CV Mean Testing Accuracy")
    print(np.mean(score["test_score"]))
    print("SVC CV Standard Deviation Testing Accuracy")
    print(np.std(score["test_score"]))
    Svc.fit(featNS_train,label_train)
    return Svcbestgamma=2.09
bestc=2.15
Svc=trainSVC(bestgamma,bestc,featNS_train,label_train)SVC CV Mean Testing Accuracy
0.94829178743961352
SVC CV Standard Deviation Testing Accuracy
0.043399848777455924
```

åœ¨æµ‹è¯•é›†ä¸Šè¾¾åˆ°çš„ç²¾åº¦ä¸º 0.964ã€‚

```
predvalid=Svc.predict(featNS_test)
accvalid=accuracy_score(label_test,predvalid)
print("Testing Accuracy")
print(accvalid)Testing Accuracy
0.9649122807017544
```

å¯ä»¥çœ‹å‡ºï¼Œå°†ç‰¹å¾é›†åˆå‡å°‘åˆ°æœ€é‡è¦çš„ç‰¹å¾ï¼Œå¯¹å®ƒä»¬è¿›è¡Œå½’ä¸€åŒ–ï¼Œå¹¶é‡‡ç”¨å¦‚ä¸Šæ‰€è¿°çš„ SVC æ¨¡å‹ï¼Œèƒ½å¤Ÿç¨å¾®æé«˜æµ‹è¯•ç²¾åº¦ï¼Œä» GBTC çš„ 0.95 æé«˜åˆ° 0.96(è§å›¾ 10)ã€‚

![](img/edb9a25c04502f58d3586ea3a8740b5b.png)

å›¾ 10:GBTC å’Œ SVC ç®—æ³•çš„æµ‹è¯•ç²¾åº¦æ¯”è¾ƒã€‚

**ä¸‰. 2 é¢„æµ‹å›¾**

åœ¨è®­ç»ƒ SVC åˆ†ç±»å™¨ä¹‹åï¼Œåœ¨ 2D ç©ºé—´â€œæœ€å·®åŒºåŸŸâ€å’Œâ€œæœ€å·®å‡¹ç‚¹â€ä¸­è¯„ä¼°å…¶é¢„æµ‹ã€‚å›¾ 11 æ˜¾ç¤ºäº†è¿™äº›é¢„æµ‹ã€‚åœ¨å›¾ 11 å’Œå›¾ 12 ä¸­ï¼Œåœ†å½¢ç‚¹ä»£è¡¨ç”¨äºè®­ç»ƒçš„ç‚¹ï¼Œä¸‰è§’å½¢ç‚¹ä»£è¡¨ç”¨äºæµ‹è¯•çš„ç‚¹ã€‚

```
def contpredSVC(Svc,nbpoints,mean_featS,\
std_featS,featS_train,featNS_train,label_train,featS_test,\
featNS_test,label_test,predict_proba=False):'''
   Make predictions in the plan of the two selected features
   using SVC.
   Input:
     Svc: [Sklearn Instance] Trained SVC model
     nbpoints: [Int] Number of points sampling each feature
                      direction
     std_featS: [Array] Standard deviation of each selected
                        feature
     featS_train: [DataFrame] Selected feature training set
     featNS_train: [DataFrame] Selected and normalized feature
                               training set
     label_train: [DataFrame] Training labels  
     featS_test: [DataFrame] Selected feature testing set
     featNS_test: [DataFrame] Selected and normalized feature 
                              testing set
     label_test: [DataFrame] Testing labels
     predict_proba: [Bool] True if a probability risk map
                           is computed, False if label map '''  
    # Determine maximum and minimum normalized values for each
    # selected features and compute the normalized 2D mesh
    minfeat1NS=np.min(featNS_train.values[:,0])
    minfeat2NS=np.min(featNS_train.values[:,1])
    maxfeat1NS=np.max(featNS_train.values[:,0])
    maxfeat2NS=np.max(featNS_train.values[:,1])
    x_arr=np.arange(minfeat1NS,maxfeat1NS,\
   (maxfeat1NS-minfeat1NS)/nbpoints)
    y_arr=np.arange(minfeat2NS,maxfeat2NS,\
   (maxfeat2NS-minfeat2NS)/nbpoints)
    X,Y=np.meshgrid(x_arr,y_arr) # Points (normalized) in the 2D space where the predictions are
    # computed.
    pointstopred=np.array([[X[irange][ipoint],Y[irange][ipoint]]\
    for ipoint in range(len(X[0]))\
    for irange in range(len(X))]) # If a probability risk map is computed
    if predict_proba: # Make prediction with SVC
        valmesh=Svc.predict_proba(pointstopred)
        predsvccont_fig,ax=plt.subplots(1,1,figsize=(10,8),) # Denormalize feature points and plot the predictions 
        denfeat1_arr=pointstopred[:,0]*std_featS[0]+mean_featS[0]
        denfeat2_arr=pointstopred[:,1]*std_featS[1]+mean_featS[1]        
        im=plt.scatter(denfeat1_arr,denfeat2_arr,c=valmesh[:,0],\
        cmap=cm.coolwarm,alpha=1) # If a label map is computed
    else:
        valmesh=Svc.predict(pointstopred)
        predsvccont_fig,ax=plt.subplots(1,1,figsize=(10,8),)
        colors=["red","navy"]

        # Denormalize feature points and plot the predictions 
        denfeat1_arr=pointstopred[:,0]*std_featS[0]+mean_featS[0]
        denfeat2_arr=pointstopred[:,1]*std_featS[1]+mean_featS[1]        
        im=plt.scatter(denfeat1_arr,denfeat2_arr,c=valmesh,
        cmap=matplotlib.colors.ListedColormap(colors),alpha=0.1) # Plot the training data
   plt.scatter(featS_train.values[:,0]\
   [np.where(label_train.values==0)[0]],\
   featS_train.values[:,1][np.where(label_train.values==0)\
   [0]],c="red",s=30,label="Malignant-Training") plt.scatter(featS_train.values[:,0]\
   [np.where(label_train.values==1)[0]],\
   featS_train.values[:,1][np.where(label_train.values==1)\
   [0]],c="navy",s=30,label="Benign-Training") # Plot the testing data
   plt.scatter(featS_test.values[:,0]\
   [np.where(label_test.values==0)[0]],\
   featS_test.values[:,1][np.where(label_test.values==0)\
   [0]],c="red",marker="^",s=30,label="Malignant-Testing") plt.scatter(featS_test.values[:,0]
   [np.where(label_test.values==1)[0]],\
   featS_test.values[:,1][np.where(label_test.values==1)[0]],\
   c="navy",marker="^",s=30,label="Benign-Testing") plt.legend()
   plt.ylim(np.min(featS_all.values[:,1]),\
   np.max(featS_all.values[:,1]))
   plt.xlim(np.min(featS_all.values[:,0]),\
   np.max(featS_all.values[:,0]))
   plt.xlabel(feat_selected[0])
   plt.ylabel(feat_selected[1]) if predict_proba:
        plt.colorbar(im,label="Probability Risk")
        predsvccont_fig.savefig("predsvcproba.png")

        plt.show()
    else:
        predsvccont_fig.savefig("predsvclabel.png")
        plt.show()# Train SVC
nbpoints=150
bestgamma=2.09
bestc=2.15# Plot label map
contpredSVC(Svc,nbpoints,mean_featS,std_featS,featS_train,featNS_train,label_train,featS_test,featNS_test,label_test,predict_proba=False)# Plot probability map
contpredSVC(Svc,nbpoints,mean_featS,std_featS,featS_train,featNS_train,label_train,featS_test,featNS_test,label_test,predict_proba=True)
```

![](img/b7cfd76488ef6873cd790f78b814341c.png)

å›¾ 11: SVC åœ¨è®¡åˆ’ä¸­çš„æ ‡ç­¾é¢„æµ‹â€œæœ€å·®åŠå¾„â€å’Œâ€œæœ€å·®å‡¹ç‚¹â€ã€‚å½©è‰²ç‚¹å¯¹åº”äºè®­ç»ƒé›†(åœ†å½¢)å’Œæµ‹è¯•é›†(ä¸‰è§’å½¢)çš„æ•°æ®ã€‚

åœ¨ SVC åˆ†ç±»å™¨çš„æƒ…å†µä¸‹ï¼Œä¹Ÿå¯ä»¥å¯¼å‡ºé¢„æµ‹çš„æ¦‚ç‡ã€‚å¯¹äºé¢„æµ‹çš„æ ‡æ³¨ğ‘™ğ‘–ï¼Œç»™å®šé¢„æµ‹ğ‘™ğ‘–ï¼Œæ•°æ®è¢«æ ‡æ³¨ä¸º 1 çš„æ¦‚ç‡ç”±ä¸‹å¼ç»™å‡º:

![](img/9f9149c0af525b37008d9392f9da146b.png)

å…¶ä¸­å¸¸æ•°ğ›¼å’Œğ›½æ˜¯åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­é€šè¿‡æœ€å¤§ä¼¼ç„¶ä¼˜åŒ–è·å¾—çš„(è§[6])ã€‚

ä½¿ç”¨è¯¥æ¨¡å‹ï¼Œå›¾ 12 å› æ­¤åœ¨è¯†åˆ«çš„ 2D ç©ºé—´ä¸­ç¤ºå‡ºäº†å‘å±•æˆæ¶æ€§ä¹³è…ºç™Œçš„ä¼°è®¡æ¦‚ç‡ã€‚

![](img/879291910f486f5d9197d97f0416c59c.png)

å›¾ 12:SVC æ¨¡å‹çš„æ ‡ç­¾æ¦‚ç‡è¯„ä¼°ã€‚å®ƒå¯ä»¥è¢«è§£é‡Šä¸ºå‘å±•ä¸ºæ¶æ€§ä¹³è…ºç™Œçš„æ¦‚ç‡å›¾ã€‚

# ä¸‰ã€‚ç»“è®º

è¿™é¡¹å·¥ä½œæ˜¯ç¡®å®š 30 ä¸ªè¢«æè®®ç”¨æ¥è¯„ä¼°æ¶æ€§ä¹³è…ºç™Œé£é™©çš„å˜é‡ä¸­æœ€é‡è¦çš„ä¸¤ä¸ªå˜é‡çš„æœºä¼šã€‚é€šè¿‡ç»“åˆå±‚æ¬¡èšç±»ç®—æ³•å’Œä½¿ç”¨æ¢¯åº¦æ¨è¿›çš„ç‰¹å¾é‡è¦æ€§ä¼°è®¡æ¥é€‰æ‹©ç‰¹å¾ã€‚ä½¿ç”¨é€‰æ‹©çš„ç‰¹å¾å»ºç«‹æ”¯æŒå‘é‡æœºåˆ†ç±»å™¨ã€‚

è·å¾—çš„æœºå™¨å­¦ä¹ æ¨¡å‹ä¸ä½¿ç”¨æ‰€æœ‰ 30 ä¸ªå˜é‡çš„å…¶ä»–æ¨¡å‹å…·æœ‰ç›¸åŒçš„å¯é æ€§ã€‚è¿™ç§æ–¹æ³•çš„ä¼˜ç‚¹æ˜¯é€šè¿‡è®¡ç®—æ˜“äºè§£é‡Šçš„ 2D é£é™©å›¾æ¥ç®€åŒ–é£é™©è¯„ä¼°ã€‚ç°åœ¨éœ€è¦æ›´å¤šçš„æ•°æ®æ¥è¯å®æ‰€ç¡®å®šçš„åŒºåŸŸå’Œè¶‹åŠ¿ã€‚

# æ¥æº

[1][https://www . wcrf . org/dietandcancer/cancer-trends/breast-cancer-statistics](https://www.wcrf.org/dietandcancer/cancer-trends/breast-cancer-statistics)

[2][https://towards data science . com/building-a-simple-machine-learning-model-on-breast-cancer-data-ECA 4 B3 b 99 fa 3](/building-a-simple-machine-learning-model-on-breast-cancer-data-eca4b3b99fa3)

[3][https://towards data science . com/how-to-use-scikit-learn-datasets-for-machine-learning-d 6493 b 38 ECA 3](/how-to-use-scikit-learn-datasets-for-machine-learning-d6493b38eca3)

[4][http://archive . ics . UCI . edu/ml/datasets/breast+cancer+Wisconsin+% 28 diagnostic % 29](http://archive.ics.uci.edu/ml/datasets/breast+cancer+wisconsin+%28diagnostic%29)

[5][https://sci kit-learn . org/stable/auto _ examples/inspection/plot _ permutation _ importance _ multicollinear . html # sphx-glr-auto-examples-inspection-plot-permutation-importance-multicollinear-py](https://scikit-learn.org/stable/auto_examples/inspection/plot_permutation_importance_multicollinear.html#sphx-glr-auto-examples-inspection-plot-permutation-importance-multicollinear-py)

[https://en.wikipedia.org/wiki/Hierarchical_clustering](https://en.wikipedia.org/wiki/Hierarchical_clustering)

[6][https://sci kit-learn . org/stable/modules/ensemble . html # gradient-boosting](https://scikit-learn.org/stable/modules/ensemble.html#gradient-boosting)

[7]https://scikit-learn.org/stable/modules/calibration.html