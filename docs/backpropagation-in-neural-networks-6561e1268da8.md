# 神经网络中的反向传播

> 原文：<https://towardsdatascience.com/backpropagation-in-neural-networks-6561e1268da8?source=collection_archive---------5----------------------->

## 从零开始的神经网络，包括数学和 python 代码

![](img/d1639bf01333af791e65168650707174.png)

照片由 [JJ 英](https://unsplash.com/@jjying?utm_source=medium&utm_medium=referral)在 [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral)

# 介绍

你曾经使用过神经网络，并想知道它背后的数学是如何工作的吗？在这篇博文中，我们将从头开始推导前向和后向传播，从中编写一个神经网络 python 代码，并学习线性代数和多元微积分的一些概念。

我将从解释一些线性代数基础开始。如果你对这个足够精通，你可以跳过下一部分。

# 向量和矩阵

简单的数字(标量)用小写字母书写。

![](img/2b9b92fe0924d83b2a03597b1177a90a.png)

作者图片

向量用粗体字母表示，默认情况下它们是列向量。

行向量的名称也用粗体表示，但是用大写字母 T 表示转置。

![](img/59af15dabc81ee470ce769111a4d384b.png)

autor 提供的图像

矩阵用粗体大写字母表示。

![](img/7ae997e8cd203b50fa786582ee24cdf3.png)

autor 提供的图像

当转置一个矩阵时，我们交换行和列。

![](img/9edcf6fd75bff1cbd891c241e1b8d5d6.png)

作者图片

矩阵或向量的维数是:

(行数、列数)

![](img/2f9d81045d2ae3decabc641e4ce17b1e.png)![](img/c808d5e0e1a999a06dad643e23af0306.png)![](img/ddfbb1814687323e51e7af89633e0db7.png)

作者图片

# 点积⋅

当两个向量或矩阵乘以点积时，左边向量或矩阵的列数必须与右边向量或矩阵的行数相匹配。

![](img/c0aed267f39ca0431132633f82bd85c9.png)

(图片由作者提供)

结果的维数可以计算如下:

![](img/021a344be3c3dac15ffbba8783e6918f.png)

作者图片

该乘法中的“内部”维度必须匹配，并在输出维度中消失。

# 元素态乘积(Hadamard 乘积)

当使用元素乘积将两个向量或矩阵相乘时，它们必须具有相同的维数。

![](img/bc612d3ad9a8276e9c1f1c58431389a7.png)

(图片由作者提供)

# 矩阵计算

我们还需要向量和矩阵的微积分。我们使用雅可比矩阵的概念。雅可比就是一个向量或导数矩阵。

标量值函数相对于矢量 **𝑥** 的导数定义如下:

![](img/08342f1263342399176cc8cd5e538494.png)

作者图片

我们用输出向量 **𝑓** 定义一个函数对单个变量 x 的导数，如下所示:

![](img/f26b09043a1e43cabb1768311bac9554.png)

作者图片

具有输出向量和输入变量向量的函数的导数定义如下:

![](img/56dc6aeea4c3f6d83949165270f9c4c7.png)

作者图片

# 向量链规则

矢量链规则看起来非常类似于标量链规则。在这个例子中，向量 **𝑓** 是向量 **𝑔** 的函数，向量 **𝑔** 本身是向量 **𝑥** 的函数。所以**𝑓**(**𝑔**(**𝑥**))相对于 **𝑥** 的导数计算如下:

![](img/b1ef08441ab030189c97225d2fc0594e.png)

作者图片

# 单输入向量神经元

![](img/4ed57e2b143c65e4695e94e9d437bc45.png)

作者图片

**𝑥** :是进入神经元的输入样本。在上图中，它有 3 个特征。例如，这可以是一个人的身高(1.75 米)、体重(80 公斤)和年龄(30 岁)。注意，我们一次只输入一个样本(在我们的例子中是一个人)到神经元中。

𝑎:被称为激活，它是神经元的输出。它是神经元根据一个输入样本做出的预测。我们可以选择我们的神经元应该预测什么。在我们人类的例子中，一个有用的预测可能是身体质量指数。

**𝑤** :神经元的权重。它对向量 **𝑥** 的每个输入特征有单独的权重。

𝑏: **神经元的**偏差。一个神经元只有一种偏向。

𝑦:是网络输出的真实值或目标值。我们希望𝑎尽可能靠近𝑦。因此，我们在训练阶段改变权重 **𝑤** 和偏差𝑏。

𝐿:被称为损失。𝑎离𝑦越近，损失越小，因此越好。因此，我们想尽量减少损失。最小化神经网络中损失的最常见方法是梯度下降。在这篇博文中，我将使用“均方误差”损失。

## 梯度下降

在训练期间 **w** 以下列方式更新。

![](img/e24b0e67411ca722de0c441552595fda.png)

作者图片

其中 **w** 是新更新的权重向量，𝜆是学习率。𝜆是一个超参数，可以自由选择。通常在 0.05 左右。

![](img/c2f9ce5e347c522da9408f0b86941c0d.png)

作者图片

∇𝐿( **w** 是损失相对于权重的梯度。它只是损失𝐿的导数相对于权重的转置。一般来说，函数对某些变量的梯度总是相应导数的转置。

![](img/ae402c743a18a3e454f801d9b95d97e5.png)

作者图片

𝑏是新的更新的偏见。∇𝐿(𝑏)是损耗相对于权重的梯度。

![](img/ac8c48571ab1a9d1c34445fbeca1a8dd.png)

作者图片

在这种情况下，梯度是导数的转置并不重要，因为它是一个单一的数，转置一个单一的数不会改变它。

## 计算渐变

应用向量链规则:

![](img/761235bb72e7fd388302625483844df6.png)![](img/81478a6ce0949c136ae028295352d311.png)![](img/9142c0be30232ab3db440f68f3df0a3a.png)

作者图片

重量的导数:

![](img/228d72aaf14a2381f4f0999ed4227053.png)![](img/776c54c77986cfbc2b5bfc2440cce0b3.png)

作者图片

**𝑥** 不依赖于 **w** ，因此它被视为常数，导数必须仅针对**w**

关于偏差的导数:

![](img/70b12d2238be40d7f52096c7f12c30a5.png)![](img/12c165df44f4214475c2b5ecdd848880.png)

作者图片

## **梯度下降**

![](img/3871b9d81f740d4d2be0305edea92430.png)![](img/dc35e4d25cf747b19247013d5a69ea39.png)

作者图片

## **代码**

作者代码

测试我们神经元的代码:

作者代码

然后，我们用虚拟数据集测试我们的数据，看看它是否有效。每个样本有 3 个特征。如果第一个特征是 1，则期望的输出是 1。其他两个特征对结果并不重要。因此，我们希望第一个权重变为 1，而其他权重和偏差将变为 0。

![](img/02e89d65f070e26f7efd738c3a52cfe7.png)

显示训练过程中权重、偏差和损失变化的图表(图片由作者提供)

我们对训练过的网络在训练中从未见过的样本进行测试，以了解它对未见过的数据的推广情况。

```
Test: 
predicted y: 0.9992918862001037, true y: 1
```

测试样本的输出显示，我们非常接近真实值。

# 能够同时对多个样本进行训练

## 前进传球

每个数据点的大小为 3:

我们有数据点𝑥1，𝑥2，…

圆点表示我们可以输入神经元的数据点(𝑚)的数量是可变的。

![](img/f606ef186f6d0e77c957a858ca937f0e.png)

作者图片

为了获得输入矩阵，我们垂直堆叠输入向量:

![](img/d9885db2fd7547dcc6e730ea978cea95.png)

作者图片

我们必须按如下方式调整损失:

![](img/15fdc8e7113b6f99501fbddc5e96ff0a.png)

作者图片

其中𝑚是我们使用的数据点数。

**𝑎，**激活向量然后以如下方式计算:

![](img/1542eee7de89c6266bd6f661ce96779f.png)

作者图片

![](img/57dfffa82c2731e7a4cc4ec45e82ae98.png)

作者图片

## 偶数道次

应用向量链规则:

![](img/859c920a3bd7cb2d90b61627944689aa.png)![](img/5a1663a0b0b7d7d71598d06d52b57597.png)

作者图片

计算导数。

![](img/0edafab420c5e5621f6bc1545c42b9d1.png)

作者图片

关于重量:

![](img/bea08a98110129aabf99e99591db9d8b.png)![](img/3b77cf070f377fe35cbfb08dcd6e5824.png)

作者图片

关于偏差:

![](img/3ff4152cc6d482f8d7fec520e81c2eac.png)![](img/cfdbca46a05f36de160c86c2608fbc70.png)

作者图片

## 梯度下降

![](img/1019fec49aa348f584d8aa143f48b432.png)![](img/9df2fb32e1fe841fc0d86be98358d974.png)

作者图片

## 密码

作者代码

## 随机梯度下降

随机梯度下降意味着，我们同时对所有训练样本进行训练。

我们可以看到权重、偏差和损失的相同演变，就像在具有单一输入的神经元中一样。

![](img/0b0eadaac410d1c16be3a64498bdd009.png)

作者图片

我们可以在下面看到，预测的输出非常接近真实的 y:

```
Test: 
predicted y: 0.9999991060923578, true y: 1
```

## 小批量梯度下降

小批量梯度下降意味着，我们同时在训练集的子集上训练神经元。

我们必须编写一个函数来生成随机批次。

作者代码

![](img/879d280b9cb3d934994051bbd60c2c9b.png)

作者图片

```
predicted y: 0.9956078874633946, true y: 1
```

# 添加激活功能

激活函数是非线性函数。

3 个非常重要的激活函数及其导数:

tanh:

![](img/6584c5a2c1bff3375969e5c69fa9dd3b.png)![](img/abb401525e9ff7af3c225885cf177aeb.png)

作者图片

西蒙德:

![](img/bea306f7a297e2bad0ce93070015820a.png)![](img/bb5654ca25a272a55b2d8397ac2a63ab.png)

作者图片

整流线性单位:

![](img/6a2555783ca805cdae3489d99e95b5e9.png)

作者图片

## 前进传球

![](img/b3caabbac6006032871a6ed0bf359896.png)![](img/f9bd5c928e4834904261fc5d56cd16ca.png)![](img/cf96ec5c324ea06d948cec8d5a442028.png)![](img/a698f7e4e848b6d5ade137e4401a8245.png)![](img/65f73c10a48e0f7ca6185307efd7ec04.png)![](img/5469abb413b9829d12a11baa38bd4f29.png)

作者提供的图片

## 向后传球

应用向量链规则:

![](img/09a972ebbb6a3695025ab43751bc1c0f.png)![](img/bb5f85cb75c105363ce3bafc1cdf25bc.png)

作者图片

计算导数:

![](img/d817ed5500282eb245af731665780ab6.png)![](img/b6e3aedd293d22781cdd9593f6a40538.png)

作者图片

np.diag 函数从一个向量构造一个矩阵，其中矩阵的对角线等于该向量，并且它是非对角线的。

关于重量:

![](img/e34f83b468bbd93d65d236307f2c3213.png)![](img/1f7273c3750913297755a3db7e9c898b.png)

作者图片

关于偏差:

![](img/0cbde222fcb2376df66821a210a46a5d.png)![](img/34c887f4074d657f7d2dd83a1a88c5e4.png)

作者图片

## 密码

激活功能:

作者代码

神经元:

作者代码

**迷你批次梯度下降**

![](img/105b73c2cab80d7dab8ea454d3daa2ad.png)

作者图片

值、权重和偏差收敛到不同于先前的情况。

```
prediceted y: 0.9394357707018176, true y: 1
```

# **深度神经网络**

**向前传球**

![](img/6ddcbeb0ff19a677c5d00a4054ac1519.png)

作者图片

括号中的上标表示该层。

***第 1 层(=输入层):***

![](img/400e187c363b7a294065f9e7eb31564c.png)![](img/1c0dea57436f772abba9fd28ecaf17dc.png)

作者图片

***第二层(=输出层):***

![](img/9b23364ca7a09c603227ac71967e41f9.png)![](img/6d5aa9dfaee130ef51d51ceb8f0e240a.png)

作者图片

***损失:***

![](img/906aacfb894c874d5d1b4743d4d22940.png)

作者图片

## **向后传球**

***第一层(=输入层):***

![](img/2c35341370d1621fdb2cabd18bb7ab9d.png)![](img/c4c87fd8ce2857e4be2fea63fdbdf6ab.png)

作者图片

***第二层(=输出层):***

![](img/be5576c055314de2e4c7acbc040d3cfe.png)![](img/b37cc133d6daafca6e2f29807c692994.png)

作者图片

计算第 2 层的导数是一个问题，因为我们必须对矩阵求向量的偏导数，对矩阵求矩阵的偏导数。这将导致张量，这将使它变得不必要的复杂。

我们不使用向量链规则，而是直接计算损失对权重和偏差的最终导数。为此，我们:

1.  用索引符号重写向前传递。在这个符号中，我们可以看到损失的最终导数的每个元素相对于权重和偏差的样子。
2.  计算导数。
3.  为了编写高效的代码，请返回到矢量化形式。

***第一层(=输入层):***

![](img/775eca9842a053c66eb2f8f2577a9833.png)![](img/c212e1ea7f81441dccb98cb582a644f5.png)

作者图片

***第二层(=输出层):***

![](img/9fd77fbbbdef08c890145745357faae5.png)![](img/793459de837f55556ae71eb490653a8a.png)

作者图片

***损失:***

![](img/906aacfb894c874d5d1b4743d4d22940.png)

作者图片

## **向后传球**

***第二层(=输出层):***

为了推导索引符号中的公式，我们可以查看计算图。例如，为了推导第 2 层权重的公式，我们构建了一个计算图，其中所有相互依赖的变量都是相连的。

![](img/d9cc190a91af02a25199a95d2eed76c2.png)

作者图片

我们用索引符号重写了相同的计算图。

![](img/1cd90ae9ff1bca96b86a95b3ecf17050.png)

作者图片

我们现在想知道损失对每个重量的导数。对于每个权重，我们必须对该权重出现的所有分支求和。作为层 2 的重量 1 的例子:

![](img/59aef4fa0f732e395b6a478892188fb3.png)

作者图片

损失相对于所有重量的导数为:

![](img/d0badf9924b05048f9aa879d59293c40.png)

作者图片

以矢量化形式写回:

![](img/c9577e2b7f7d7f01b28429f7a2234dbb.png)

作者图片

注意，通过观察等式的左侧，我们可以看到我们的矢量化形式最终必须具有的维数。这有助于我们以正确的方式构造点积，并给我们必须转置的信息。

![](img/edae0216b7f666c339cfeec72acb1f8f.png)

作者图片

以矢量化形式写回:

![](img/d61acbc916afb239fc0c587eae249db2.png)

作者图片

***第一层(=输入层):***

![](img/8fce9fe2bb56e257e6ea6f7c5113e287.png)

作者图片

以矢量化形式写回:

![](img/558283de54ef02399d174de4aeff0888.png)

作者图片

重新划分偏差:

![](img/997199e55ecc246e3ff2dd2f1fbb5ed2.png)

作者图片

以矢量化形式写回:

![](img/058b1c3e3efccbe3ddc4b1599430fab4.png)

作者图片

为了使求和更清楚:

![](img/de963d66eb5dcd626814f9cd09c5b4af.png)

作者图片

“轴= 1”意味着我们对各列求和。

# 通式

为了推广我们上面导出的公式，我们将输入损耗的概念引入到层中的神经元中。输入错误 **𝛿** (𝐿)进入最后一层𝐿是:

![](img/37dd6879b1063712faebf7ea03c1a05b.png)

作者图片

以矢量化的形式:

![](img/a77dd3fc8e13d1788fd2580b0d69639a.png)

作者图片

对于我们网络的所有其他层 l:

![](img/b32cc496b3324dbd4abaa2a6ba90f75f.png)

作者图片

以矢量化的形式:

![](img/91ab444c7320a2c3558aaa3036d8d126.png)

作者图片

𝑓表示非线性激活函数。这可以是例如乙状结肠或 relu。𝑓′表示𝑓.的导数

***关于重量的导数:***

![](img/303d66e68454a203a51c02c14caaa09d.png)

作者图片

以矢量化的形式:

![](img/05b3bf78933beb81fad8d2d3fa3bb491.png)

作者图片

***关于偏差的导数:***

![](img/b6a212f7bf542e5a205cc5b93ff9b282.png)

作者图片

以矢量化的形式:

![](img/72c06a64625124f6cdc762b8b1d11560.png)![](img/8c82f0d0532946ff4bac72ef1d64b019.png)

作者图片

# **最终代码**

此代码可用于创建和训练任意深度神经网络。传递给 __init__ 方法的列表“层”可以更改，这将更改网络。列表的长度等于层数。这些数字等于每层中神经元的数量。第一个必须等于特征的数量。

作者代码

测试我们的代码:

作者代码

我们再次查看输出层的权重和偏差的演变以及训练期间损失的演变。

![](img/9ab47c991dc20351851d9c28e179eacf.png)

作者图片

```
predited y: 0.9514167066315814, true y: 1
```

# **结论**

如果你还和我在一起，我祝贺你，虽然这是一个旅程。如果第一次没有成功，也不要绝望。我花了一个多月才想明白这一切。

今天你已经学会了如何在任意深度神经网络中导出权重梯度。你现在也应该能够将这些知识应用到其他问题上，比如线性回归、递归神经网络和强化学习等等。

但是要知道，现在有非常好的自动签名框架，它能够为你数值计算梯度。尽管如此，我仍然认为，手工推导这些公式有助于我们对神经网络如何运行以及出现的一些问题(如香草 RNN 中的消失和爆炸梯度)的总体理解。

## 相关著作

</snake-with-policy-gradients-deep-reinforcement-learning-5e6e921db054>  <https://medium.com/@Vincent.Mueller/einstein-index-notation-d62d48795378>  

## 想联系支持我？

LinkedIn
[https://www.linkedin.com/in/vincent-m%C3%BCller-6b3542214/](https://www.linkedin.com/in/vincent-m%C3%BCller-6b3542214/)
脸书
[https://www.facebook.com/profile.php?id=100072095823739](https://www.facebook.com/profile.php?id=100072095823739)
Twitter
[https://twitter.com/Vincent02770108](https://twitter.com/Vincent02770108)
Medium
[https://medium.com/@Vincent.Mueller](https://medium.com/@Vincent.Mueller)
成为 Medium 会员并支持我(你的部分会费直接归我)
[https://medium.com/@Vincent.Mueller/membership](https://medium.com/@Vincent.Mueller/membership)