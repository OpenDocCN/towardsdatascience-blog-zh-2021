# NLP 101 ⅔ —文本分类的线性模型

> 原文：<https://towardsdatascience.com/nlp-101-%E2%85%94-linear-models-for-text-classification-8ced8199c2a8?source=collection_archive---------25----------------------->

![](img/ac363cc75f34592825d86d1341757084.png)

在 [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral) 上由 [Darya Tryfanava](https://unsplash.com/@darya_tryfanava?utm_source=medium&utm_medium=referral) 拍摄的照片

声明:本文是我为 [shecancode](https://shecancode.io/) 撰写的三部分系列文章的第二部分。与我的其他文章相比，它更深入于理论，并且不包含任何代码(关于这些，请查看我的其他文章)。

在这篇博文中，我想给大家温柔的介绍一下*文本分类*。文本分类(也称为*文本分类*或*文本标记*)是将自然语言文本映射到预定义分类变量的过程。文本分类有许多应用，例如垃圾邮件检测(类别:垃圾邮件、ham)、情感分析(类别:正面、中性、负面)、主题标记(例如，检测新闻标题的主题)和意图检测(例如，聊天机器人检测您是否想要预订新的航班或重新安排航班)。

![](img/86a2e60a4a882b7c16bab8eaf24ad279.png)

在[之前的一篇博文](https://shecancode.io/blog/no-fear-of-machine-learning-how-to-classify-text-in-less-than-10-lines-of-code)中，我回顾了一些重要的机器学习概念，如训练测试分割、交叉验证和准确性测量，并展示了如何使用 scikit-learn 在几行代码中实现朴素贝叶斯分类器。在这篇博文中，我想更深入地研究数学，并向您展示线性分类算法是如何工作的。

![](img/86b19b3eadbcd658dc7de8edec6256ab.png)

让我们以基于主题的文本分类为例——比如根据内容对新闻文章进行分类:体育、政治和技术。在这种情况下，语法并不重要，为了进行分类，你得到的核心信号是正在使用的单词。因此，考虑句法和语义特征的复杂模型是不必要的，线性模型适合于这项任务。

![](img/5374c7f3b61d9f27318650d2c7f4c1e5.png)

左边的三个单词的云根据上述三种类型的新闻文章的频率和相关性突出显示了最常见的单词。

因此，如果你看到一篇新文章，其中“比赛”、“赛季”和“比赛”的出现频率高于“在线”和“网络”，那么这篇新闻很可能是关于体育的。

下面你会看到一篇假设的新闻文章的向量，这些向量使用了一种词汇袋的方法。在本系列的前一篇文章中，我介绍了如何从文本中提取特征，并将人类语言文本表示为计算机可读的向量。现在，我们将使用这些提取的特征来预测一篇新闻文章是否是关于体育的(我们现在将这个问题视为二元预测任务)。

![](img/ab3ce575a42a4833dec0ec66a4c0400e.png)

在监督机器学习中，您有输入特征和标签集。要根据您的数据进行预测，您可以使用带有一些参数θ的函数 F 将您的要素映射到输出标注。要获得从要素到标注的最佳映射，需要通过比较输出ŷ与数据中真实标注 y 的接近程度来最小化成本函数。学习是通过更新参数和重复这个过程来进行的，直到你的成本降到最低。

![](img/a7c502847b1eaab2b357784e2363c972.png)

我将介绍两种重要的算法:逻辑回归和朴素贝叶斯。这两种算法尽管相对简单，但通常都能产生好的结果，是很好的开始算法。它们通常用作与更复杂的方法进行比较的基线。这两种算法经常一起出现的另一个原因是为了展示判别分类器(逻辑回归)和生成分类器(朴素贝叶斯)之间的差异。对于如何建立机器学习模型，这是两个非常不同的框架。考虑一个视觉隐喻:想象我们正试图区分大象图像和狮子图像。生成模型的目标是理解大象和狮子的样子。假设，你可以让那个模型‘生成’(比如画)一头大象。给定一个测试图像，系统然后询问是大象模型还是狮子模型更适合该图像，并选择它作为其标签。

相比之下，判别模型只是试图学习区分类别。大象有鼻子，而狮子没有。如果这一特征很好地将类分开，那么模型是令人满意的。但如果你问这样一个模型，它对狮子了解多少，它只能说它们没有躯干。

# 逻辑回归

让我们假设一个体育向量，代表一篇关于体育的新闻文章，并将这个向量乘以我们在训练阶段学到的类别权重向量，其中词汇表中的每个单词都有一个权重。这个权重指示在给定的类中拥有这个特定的单词是否有用。在这个例子中，权重为 1.5 的“player”意味着它很可能是一个“sports”单词，而权重为-1.1 的“election”很可能不是。分数是这两个向量的点积。

![](img/d85f428bcd8975c976870c4c68bf6301.png)

判别分类试图找到一条线(超平面),最好地将类别分开。在二维空间中形象化:每个点都是一篇新闻文章的特征表示。权重向量是与该超平面正交的向量，该超平面将体育文章与非体育文章分开。分数是特征向量(空间中的点)在权重向量上的投影减去偏差项(截距)。因此，你可以看到，这个点积正是我们所需要的，因为它在运动和非运动实例之间绘制了超平面。

![](img/28acac681a2d557fb48f4ae272f5318c.png)

一旦我们有了这些分数，下一步就是给这些分数分配概率。目前，这些分数可能在负无穷大到正无穷大之间。我们不需要这个来对我们的测试集进行预测——分数足以判断一篇文章是关于体育的(正分数)还是关于体育的(负分数)。然而，为了量化我们的损失，这种概率映射在训练中是很重要的。

这就是 sigmoid 函数派上用场的地方——它将负无穷大到正无穷大压缩到 0 和 1 之间的空间，这正是我们需要获得概率的地方。然后，我们得到的是特定句子被标记为“运动”的概率，条件是句子的内容 *x* 是应用于 x 的分数的 sigmoid 函数:

![](img/3d7086cd51230b3e34ff75903574dfc3.png)

而随之而来的是非运动的概率是 1 减去运动的概率。现在，我们选择概率最高的标签，并将该标签分配给文章。我们用帽子符号^来表示“我们对正确类别的估计”。

> *ŷ= arg max pθ(y | x)yϵ{运动，不运动}*

如果你有一个多类问题(体育，政治，技术)，则使用 softmax 函数代替 sigmoid。

现在让我们考虑损失函数:在训练过程中，我们会有一些训练损失。这种训练损失是根据训练数据集(在我们的例子中，我们的新闻文章)定义的，其中我们有成对的 *x* 和 *y* ，其中 *x* 是文章， *y* 是主题标签。训练损失是我们训练数据中所有实例(文章)的总损失。对于每个实例，我们有一个特定于实例的损失，它主要检查给定当前模型参数时，预测与真实标签相比有多好。

用于逻辑回归的损失函数称为负对数似然。

> 数据集:D = (x1，y1) … (xn，yn)
> 
> 损耗:L(D，θ)= 1/n L(Xi，易，θ)(L =每实例损耗)
> 
> 条件对数似然:l(x，y，θ)=-log pθ(y | x)

直觉上，这意味着如果对于每个训练实例，金色标签 *y* 具有高概率，那么我的损失就低。因此，训练实例的实际训练标签的概率越高，损失就越低。负号的原因是因为优化通常会最小化一个函数，所以最大化可能性与最小化负可能性是一样的。

通过在训练期间更新模型的参数以最小化损失，我们讨论的权重向量被学习。用于此的优化算法被称为梯度下降。单独讨论梯度下降超出了本博客的范围，但其思想是在当前点沿着函数梯度的相反方向迈出小步(因为这是最陡下降的方向)，直到你到达最小值，在那里损失最小。

我将结束我对文本分类的逻辑回归的介绍。我可以强烈推荐[这个关于逻辑回归的视频系列](https://www.youtube.com/watch?v=vN5cNN2-HWE&t=2s)，[这个关于梯度下降的视频](https://www.youtube.com/watch?v=sDv4f4s2SB8)，还有丹尼尔·茹拉夫斯基和詹姆斯·马丁的《言语与语言处理一书的[本章](https://web.stanford.edu/~jurafsky/slp3/)。

# 朴素贝叶斯

现在让我们看看朴素贝叶斯分类器。这是一个基于事件概率的简单分类器。如上所述，我们希望在给定输入 *d* (对于文档为 *d* )的情况下，最大化预测类 *c* 的概率。

> ĉ= arg max pθ(c | d)cϵc

贝叶斯分类的直觉是使用贝叶斯规则将上面的等式转换为具有一些有用属性的概率。

![](img/9fa8b2154ca0e66f44a088c83e1830fd.png)

然后，我们将第一个等式代入第二个等式，得到:

![](img/92f1748648ea3a37d540456b16adb096.png)

我们可以通过去掉分母 *P(d)来方便地简化上面的等式。*这是可能的，因为我们将为每个可能的类计算 *P(d|c)P(c) / P(d)* ，但是 *P(d)* 不会为每个类改变；我们总是在问同一个文章 *d* 的最可能类，这个类一定有相同的概率 *P(d)。因此，我们可以选择最大化简单公式的类*

![](img/da4910b9c15664e3a1822b4c58c246f9.png)

现在你可以看到朴素贝叶斯是一个如何生成的模型:因为我们可以将上面的内容理解为陈述了一种关于文档如何生成的隐含假设:首先从 *P(c)* 中采样一个类，然后通过从 *P(d|c)* 中采样生成单词。我们可以想象按照这个过程生成人工文档，或者至少是它们的字数。

我们通过选择具有两个概率的最高乘积的类来选择给定某个文章 *d* 的可能类 *ĉ* :先验概率 *P(c)* 和文章 *P(d|c)* 的可能性。我们将文章表示为一组特征: *x = (f1，f2，f3…fn)* 。

我们已经通过使用单词袋模型做了一个简化假设，该模型假设单词的位置无关紧要。朴素贝叶斯作出的第二个简化假设是给定类 *c* 的概率 *P(fi|c)* 的条件独立性假设。也就是说，NB 假设单个单词彼此独立(因此是朴素贝叶斯)，因此可能性可以计算为

![](img/ef4874bbf338b53fa97e66fed48db40f.png)

因此，由朴素贝叶斯分类器选择的类的最终等式是:

![](img/2aa44c2db0eb3cf58e3d944a29555c0a.png)

让我们看一个数字例子。

考虑下面的句子:

![](img/439348f9cfbb304dd96b6e4da58113e6.png)

为了让这个例子更容易理解，让我们假设我们对句子进行了一些预处理，并删除了停用词。由此产生的句子是:

![](img/8f1a2716e27d59395d9e4518fb195090.png)

“运动”类总字数:3
非运动类总字数:4

我们想为下面的句子“这是一场非常接近和难忘的比赛”指定一个类别。停用词删除后:“关闭值得纪念的比赛”。

我们可以看到两个类的先验是相同的:
P(运动)= 0.5
P(非运动)= 0.5

可能性 P("势均力敌的难忘比赛" |体育)= P("势均力敌" |体育)* P("难忘" |体育)* P("比赛" |体育)

![](img/69ff15bf19e4129dfc9134c097380a78.png)

现在我们遇到了一个问题:因为体育课的词汇中没有“接近”和“难忘”这两个词，所以当我们乘以个体概率时，结果是 0。最简单的解决方案是加一(拉普拉斯)平滑:我们将分子加 1，将总词汇量的单词数加到分母。这会产生以下值:

![](img/139e3111dde9337a0c067c0c9cb5c791.png)

可能性 P("接近的值得纪念的比赛" |体育)= 1/10 * 1/10 * 3/10 = 0.003
可能性 P("接近的值得纪念的比赛" |不是体育)= 2/11 * 1/11 * 1/11 = 0.0015

后验=似然*先验，因此句子“这是一场非常接近且难忘的比赛”属于类别“体育”的概率是 0.0015，而属于类别“非体育”的概率是 0.00075。所以这个句子被贴上了“运动”的标签。

我强烈建议您阅读 Daniel Jurafsky 和 James H. Martin 所著的《[语音和语言处理](https://web.stanford.edu/~jurafsky/slp3/)》一书的这一章，因为它不仅涵盖了朴素贝叶斯，还涵盖了评估文本分类的度量标准。

我希望这篇博文对你有所帮助，并让你更好地理解了逻辑回归和朴素贝叶斯在文本分类中的工作原理。显然，您不必从头开始实现这些算法，因为所有 Python 的机器学习库已经为您完成了这些工作。但是，了解它们的工作方式很重要，这样才能了解它们的优点和潜在的缺点，以便您可以设计出最适合您的特定类型的数据或问题的模型。