# ä½¿ç”¨ SequenceMatcher å’Œ spaCy æ„å»ºå•†å®¶åç§°æ¸…ç†å¼•æ“æ¦‚è¿°

> åŸæ–‡ï¼š<https://towardsdatascience.com/an-overview-of-building-a-merchant-name-cleaning-engine-with-sequencematcher-and-spacy-9d8138b9aace?source=collection_archive---------10----------------------->

![](img/6c647e0e2601e1670514fbd16e4c3ab0.png)

ç…§ç‰‡ç”±[ç±³ç§‘æ‹‰Â·é©¬èµ«è±](https://unsplash.com/@ko1unb?utm_source=medium&utm_medium=referral)åœ¨ [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral) ä¸Šæ‹æ‘„

# é—®é¢˜é™ˆè¿°

å•†å®¶åç§°æ¸…ç†å¯èƒ½æ˜¯ä¸€ä¸ªç›¸å½“å…·æœ‰æŒ‘æˆ˜æ€§çš„é—®é¢˜ã€‚ç”±äºä¸åŒçš„é“¶è¡Œæä¾›ä¸åŒè´¨é‡çš„äº¤æ˜“æ•°æ®ï¼Œæ²¡æœ‰ä¸€ä¸ªéå¸¸æˆç†Ÿçš„æ–¹æ³•æ¥æ¸…ç†æ•°æ®ã€‚é€šå¸¸ï¼Œå•†å®¶åç§°æ¸…ç†å¯ä»¥è¢«åˆ†ç±»ä¸º**å‘½åå®ä½“è¯†åˆ«** (NER)ä»»åŠ¡ï¼Œå¹¶ä¸”ä»¥ä¸å®ä½“æå–é—®é¢˜ç±»ä¼¼çš„æ–¹å¼æ¥è§£å†³ã€‚

å¯¹äºä¸€å®¶é‡‘èç§‘æŠ€å…¬å¸æ¥è¯´ï¼Œå•†æˆ·åç§°æ¸…ç†æ­¥éª¤éå¸¸é‡è¦ï¼Œå› ä¸ºå¼€å‘äººå‘˜éœ€è¦åˆ©ç”¨è¿™äº›ä»åŸå§‹æ··ä¹±çš„äº¤æ˜“æ•°æ®ä¸­æ¸…ç†å‡ºæ¥çš„å•†æˆ·åç§°æ¥ç”Ÿæˆæ­£ç¡®çš„**äº¤æ˜“åˆ†ç±»**ï¼Œä»¥ä¾¿åœ¨ç®¡ç†ä¸ªäººé‡‘èæ–¹é¢æä¾›æ›´å¥½çš„å®¢æˆ·ä½“éªŒã€‚

æˆ‘å‘ç°è¿™ä¸ªè¯é¢˜éå¸¸æœ‰è¶£ï¼Œæˆ‘å·²ç»æœç´¢äº†å‡ ä¸ªæ˜ŸæœŸçš„èµ„æºï¼Œç”¨æˆ‘è‡ªå·±çš„åŸºæœ¬è§£å†³æ–¹æ¡ˆå†™äº†è¿™ä¸ªæ¦‚è¿°ã€‚å› æ­¤ï¼Œæˆ‘å¸Œæœ›æˆ‘å¯¹è¿™ä¸ªè¯é¢˜çš„ä¸€äº›æƒ³æ³•èƒ½å¤Ÿå¯¹è¯»è€…æœ‰æ‰€å¸®åŠ©ï¼Œä»¥æ›´å¥½åœ°è§£å†³è¿™ä¸ªå•†å®¶åç§°æ¸…ç†é—®é¢˜ã€‚

å¦‚æœä½ æ­£åœ¨å¯»æ‰¾æ›´å¤šå…³äºè¿™ä¸ªä¸»é¢˜çš„é˜…è¯»èµ„æ–™ï¼Œæ¬¢è¿ä½ æŸ¥çœ‹æœ¬æ–‡æœ«å°¾çš„å‚è€ƒä¹¦ç›®**ã€‚**

# å·¥ç¨‹è®¡åˆ’

å¯¹äºä¸€ä¸ªåŸºæœ¬çš„å•†å®¶åç§°æ¸…æ´—å¼•æ“ï¼Œæˆ‘è®¡åˆ’å°†å…¶åˆ†ä¸ºä¸‰å±‚:

*   ç¬¬ä¸€å±‚:åˆ é™¤ç‰¹æ®Šå­—ç¬¦å’Œæ•°å­—ï¼Œå¹¶è½¬æ¢å¤§å°å†™ã€‚
*   ç¬¬äºŒå±‚:æ ¹æ®ç›¸ä¼¼æ€§å¾—åˆ†è¿”å›åç§°åŒ¹é…ã€‚
*   ç¬¬ä¸‰å±‚:è®­ç»ƒä¸€ä¸ªç©ºé—´æ¨¡å‹æ¥æ£€æµ‹æ¨¡å¼å’Œæ¸…ç†è¾“å…¥ã€‚

è¿™ä¸ªé¡¹ç›®çš„å·¥å…·åŒ…åŒ…æ‹¬ python ä¸­çš„**æ­£åˆ™è¡¨è¾¾å¼**è¿ç®—ã€**FuzzyWuzzy/sequence matcher***(åº“)ä»¥åŠ spaCy æ¨¡å‹ç®—æ³•ä¸­çš„ä¸€äº›çŸ¥è¯†ã€‚*

*éšç€é˜…è¯»çš„è¿›è¡Œï¼Œæˆ‘ä¹Ÿä¼šåˆ†äº«ä¸€äº›æˆ‘è§‰å¾—æœ‰å¸®åŠ©çš„ç›¸å…³é˜…è¯»ã€‚*

# *ç¬¬ä¸€å±‚:é¢„å¤„ç†æ­¥éª¤*

## *åˆ é™¤ç‰¹æ®Šå­—ç¬¦å’Œæ•°å­—:*

*åˆ é™¤æ‰€æœ‰ç‰¹æ®Šå­—ç¬¦å’Œæ•°å­—å°†æ˜¯è¯¥é¡¹ç›®çš„ç¬¬ä¸€æ­¥ã€‚è¿™å¾ˆæœ‰ç”¨ï¼Œå› ä¸ºå½“æˆ‘ä»¬è¯•å›¾æŸ¥æ‰¾å•†å®¶åç§°åŒ¹é…å’Œè®¡ç®—ç›¸ä¼¼æ€§åˆ†æ•°æ—¶ï¼Œç‰¹æ®Šå­—ç¬¦å’Œæ•°å­—é€šå¸¸ä¼šå¢åŠ å¤æ‚æ€§ã€‚å®Œå…¨åˆ é™¤æ‰€æœ‰ç‰¹æ®Šå­—ç¬¦å’Œæ•°å­—å¯èƒ½æœ‰ç‚¹æ¿€è¿›ã€‚ä½†æ˜¯ï¼Œè€ƒè™‘åˆ°æœ‰æ•°åƒä¸ªåŸå§‹å•†å®¶åç§°çš„æ•°æ®é›†ï¼Œæ‚¨å¯èƒ½ä¼šå‘ç°å¤§å¤šæ•°ç‰¹æ®Šå­—ç¬¦å’Œæ•°å­—éƒ½å¯ä»¥åˆ é™¤ï¼Œè€Œä¸ä¼šå½±å“å•†å®¶åç§°ä¸­çš„ä»»ä½•å…³é”®å­—ã€‚*

*å€ŸåŠ© Python [**Re**](https://docs.python.org/3/library/re.html) åº“ï¼Œå¯ä»¥é«˜æ•ˆåœ°å®Œæˆç‰¹æ®Šå­—ç¬¦å’Œæ•°å­—çš„åˆ é™¤ã€‚ä¸€äº›å¿…å¤‡çŸ¥è¯†æ˜¯ Python ä¸­çš„[**æ­£åˆ™è¡¨è¾¾å¼**](https://github.com/python/cpython/blob/3.9/Lib/re.py) ã€‚*

*[](https://medium.com/better-programming/introduction-to-regex-8c18abdd4f70) [## æ­£åˆ™è¡¨è¾¾å¼ç®€ä»‹

### ä½¿ç”¨ Python é€æ­¥ä»‹ç»æ­£åˆ™è¡¨è¾¾å¼

medium.com](https://medium.com/better-programming/introduction-to-regex-8c18abdd4f70)  [## Python æ­£åˆ™è¡¨è¾¾å¼

### æ­£åˆ™è¡¨è¾¾å¼å¯ä»¥è¢«è®¤ä¸ºæ˜¯æŒ‡å®šæ–‡æœ¬æ¨¡å¼çš„å°å‹è¯­è¨€

medium.com](https://medium.com/@devopslearning/python-regular-expression-8ee28d35f3a7) 

## æ¡ˆä¾‹è½¬æ¢

é€šè¿‡æˆåŠŸå®Œæˆä¸Šè¿°æ­¥éª¤ï¼Œæ‚¨ç°åœ¨æ‹¥æœ‰äº†ä¸€ä¸ªä»…åŒ…å«å­—æ¯çš„å•†å®¶åç§°æ•°æ®é›†ã€‚ä½†æ˜¯ï¼Œæ‚¨å¯èƒ½ä»ç„¶ä¼šå‘ç°ä¸€äº›å•†å®¶åç§°çš„å¤§å°å†™ä¸åŒï¼Œä¾‹å¦‚â€œamazonâ€ã€â€œAmazonâ€æˆ–â€œAMAZONâ€ã€‚è¦è½¬æ¢æ¡ˆä¾‹ï¼Œå¯ä»¥åœ¨ä»¥ä¸‹æ–‡ç« ä¸­æ‰¾åˆ°ä¸€äº›æœ‰ç”¨çš„å­—ç¬¦ä¸²å‡½æ•°:

[](/useful-string-functions-that-few-people-use-in-python-5a071fb0cbd1) [## Python ä¸­å¾ˆå°‘æœ‰äººä½¿ç”¨çš„æœ‰ç”¨çš„å­—ç¬¦ä¸²å‡½æ•°

### å¦‚æœ Python æä¾›çš„è¯å°±ä¸è¦é‡æ–°å‘æ˜è½®å­äº†ï¼

towardsdatascience.com](/useful-string-functions-that-few-people-use-in-python-5a071fb0cbd1) 

# ç¬¬äºŒå±‚:è®¡ç®—ç›¸ä¼¼æ€§å¾—åˆ†è¡¨

å¯¹äºè¿™ä¸€å±‚ï¼Œæˆ‘ä»¬çš„ä¸»è¦ç›®æ ‡æ˜¯è®¡ç®—ä¸€ä¸ªç›¸ä¼¼æ€§å¾—åˆ†è¡¨ï¼Œå¹¶è¿”å›å…·æœ‰å‰ 3 ä¸ªæœ€å¤§ç›¸ä¼¼æ€§å¾—åˆ†çš„åŒ¹é…åç§°ã€‚è¿™æ˜¯ä¸€ä¸ªæœ‰ç”¨çš„æ–¹æ³•æ¥æ¸…ç†å•†å®¶åç§°ï¼Œå‡è®¾ä½ å·²ç»æœ‰ä¸€ä¸ªåŒ¹é…çš„åç§°ç›®å½•ï¼Œå¹¶ä¸”åŸå§‹è¾“å…¥ä¸æ˜¯æ‚ä¹±çš„ã€‚

## ç›¸ä¼¼æ€§åº¦é‡

FuzzyWuzzy æ˜¯ä¸€ä¸ª Python åº“ï¼Œå®ƒä½¿ç”¨ [**Levenshtein è·ç¦»**](https://en.wikipedia.org/wiki/Levenshtein_distance) æ¥è®¡ç®—ä¸€ä¸ªç®€å•æ˜“ç”¨çš„åŒ…ä¸­åºåˆ—ä¹‹é—´çš„å·®å¼‚ã€‚

[](https://github.com/seatgeek/fuzzywuzzy) [## seatgeek/fuzzywuzzy

### åƒè€æ¿ä¸€æ ·çš„æ¨¡ç³Šå­—ç¬¦ä¸²åŒ¹é…ã€‚å®ƒä½¿ç”¨ Levenshtein è·ç¦»æ¥è®¡ç®—åºåˆ—é—´çš„å·®å¼‚

github.com](https://github.com/seatgeek/fuzzywuzzy) 

ä½¿ç”¨ FuzzyWuzzy çš„ä¸€äº›å®ä¾‹å¦‚ä¸‹:

*   ç®€å•æ¯”ç‡

```
>>> fuzz.ratio("this is a test", "this is a test!")
    97
```

*   éƒ¨åˆ†æ¯”ç‡

```
>>> fuzz.partial_ratio("this is a test", "this is a test!")
    100
```

*   ä»¤ç‰Œæ’åºæ¯”ç‡

```
>>> fuzz.ratio("fuzzy wuzzy was a bear", "wuzzy fuzzy was a bear")
    91
>>> fuzz.token_sort_ratio("fuzzy wuzzy was a bear", "wuzzy fuzzy was a bear")
    100
```

*   ä»¤ç‰Œé›†æ¯”ç‡

```
>>> fuzz.token_sort_ratio("fuzzy was a bear", "fuzzy fuzzy was a bear")
    84
>>> fuzz.token_set_ratio("fuzzy was a bear", "fuzzy fuzzy was a bear")
    100
```

[](/natural-language-processing-for-fuzzy-string-matching-with-python-6632b7824c49) [## ç”¨ Python å®ç°æ¨¡ç³Šå­—ç¬¦ä¸²åŒ¹é…çš„è‡ªç„¶è¯­è¨€å¤„ç†

### åœ¨è®¡ç®—æœºç§‘å­¦ä¸­ï¼Œæ¨¡ç³Šå­—ç¬¦ä¸²åŒ¹é…æ˜¯ä¸€ç§å¯»æ‰¾ä¸æ¨¡å¼è¿‘ä¼¼åŒ¹é…çš„å­—ç¬¦ä¸²çš„æŠ€æœ¯â€¦

towardsdatascience.com](/natural-language-processing-for-fuzzy-string-matching-with-python-6632b7824c49) [](/string-matching-with-fuzzywuzzy-e982c61f8a84) [## ç”¨ FuzzyWuzzy è¿›è¡Œå­—ç¬¦ä¸²åŒ¹é…

### æœ¬æ•™ç¨‹å°†ä»‹ç»å¦‚ä½•é€šè¿‡ç›¸ä¼¼æ€§æ¥åŒ¹é…å­—ç¬¦ä¸²ã€‚FuzzyWuzzy å¯ä»¥èŠ‚çœä½ å¤§é‡çš„æ—¶é—´â€¦

towardsdatascience.com](/string-matching-with-fuzzywuzzy-e982c61f8a84) 

æˆ–è€…ï¼Œ[**sequence matcher**](https://github.com/python/cpython/blob/master/Lib/difflib.py)ä¹Ÿæ˜¯ä¸€ä¸ªå¸¸ç”¨äºè®¡ç®—è¾“å…¥ä¹‹é—´ç›¸ä¼¼æ€§çš„ä¼Ÿå¤§å·¥å…·ã€‚

> åŸºæœ¬æ€æƒ³æ˜¯æ‰¾åˆ°ä¸åŒ…å«â€œåƒåœ¾â€å…ƒç´ çš„æœ€é•¿è¿ç»­åŒ¹é…å­åºåˆ—ã€‚ç„¶åï¼Œç›¸åŒçš„æ€æƒ³è¢«é€’å½’åœ°åº”ç”¨äºåŒ¹é…å­åºåˆ—çš„å·¦è¾¹å’Œå³è¾¹çš„åºåˆ—ç‰‡æ®µã€‚è¿™ä¸ä¼šäº§ç”Ÿæœ€å°‘çš„ç¼–è¾‘åºåˆ—ï¼Œä½†ä¼šäº§ç”Ÿå¯¹äººä»¬æ¥è¯´â€œçœ‹èµ·æ¥æ­£ç¡®â€çš„åŒ¹é…ã€‚

```
>>> s = SequenceMatcher(lambda x: x == " ", 
"private Thread currentThread;", 
"private volatile Thread currentThread;")  

>>> .ratio() returns a float in [0, 1], measuring the "similarity" of the sequences.  As a rule of thumb, a .ratio() value over 0.6 means the sequences are close matches>>> print(round(s.ratio(), 3))    
0.866
```

[](/sequencematcher-in-python-6b1e6f3915fc) [## Python ä¸­çš„åºåˆ—åŒ¹é…å™¨

### ä¸€ä¸ªäººæ€§åŒ–çš„æœ€é•¿è¿ç»­æ— åƒåœ¾åºåˆ—æ¯”è¾ƒå™¨

towardsdatascience.com](/sequencematcher-in-python-6b1e6f3915fc) 

## å®šåˆ¶ç›¸ä¼¼æ€§å‡½æ•°&è®¡ç®—ç›¸ä¼¼æ€§è¡¨

å¯¹æˆ‘æ¥è¯´ï¼Œæˆ‘é€‰æ‹© SequenceMatcher ä½œä¸ºè¯„ä»·ç›¸ä¼¼æ€§çš„åº¦é‡ã€‚å¦‚æœæ‚¨é€‰æ‹© FuzzyWuzzy åº“ï¼Œè¿‡ç¨‹å°†æ˜¯ç›¸ä¼¼çš„ã€‚

```
# define a function to calculate similarity between input sequences
def similarity_map(word1, word2):

    seq = difflib.SequenceMatcher(None,word1,word2) d = seq.ratio()

    return d
```

ä¸Šé¢çš„å®šåˆ¶å‡½æ•°å°†ä¸¤ä¸ªåºåˆ—ä½œä¸ºè¾“å…¥ï¼Œå¹¶è¿”å›ç›¸ä¼¼æ€§å¾—åˆ†çš„æ¯”å€¼ã€‚

ä¸ºäº†è¿›ä¸€æ­¥è®¡ç®—ç›¸ä¼¼æ€§å¾—åˆ†è¡¨ï¼Œæˆ‘åˆ¶ä½œäº†ä¸€ä¸ªæ•°æ®é›†ï¼Œå®ƒå°†åŸå§‹çš„å•†å®¶åç§°ä½œä¸ºè¡Œç´¢å¼•ï¼Œå°†ç»è¿‡æ¸…ç†çš„å•†å®¶åç§°ç›®å½•ä½œä¸ºåˆ—åã€‚é€šè¿‡è¿è¡Œä¸‹é¢çš„ä»£ç å•å…ƒæ ¼ï¼Œå®ƒå°†ä¸ºæ¯å¯¹è¡Œç´¢å¼•å’Œåˆ—åç”Ÿæˆä¸€ä¸ªç›¸ä¼¼æ€§å¾—åˆ†è¡¨ã€‚

```
# prepare a sample dataset
df = pd.DataFrame(data, 
index =['amazon co com', 'www netflix com', 'paypal payment', 'apple com bill', 'google play', 'facebook ads'],
columns = ['amazon', 'netflix', 'paypal', 'apple', 'google', 'facebook']) 

# print the data 
dffrom tqdm import tqdmfor i in tqdm(range(6)):
    for j in range(6):
        df.loc[df.index[i], df.columns[j]] = similarity_map(str(df.index[i]), str(df.columns[j]))

df.head()
```

ä¸€æ—¦æ‚¨è¿è¡Œå®Œä¸Šé¢çš„å•å…ƒæ ¼ï¼Œæ‚¨åº”è¯¥æœ‰ä¸€ä¸ªç›¸ä¼¼æ€§å¾—åˆ†è¡¨ï¼Œå¦‚ä¸‹æ‰€ç¤º:

![](img/65e0605201cef6d86642868ac6e2ea43.png)

ç›¸ä¼¼æ€§å¾—åˆ†è¡¨

## è¿”å›æœ€ä½³åŒ¹é…

åŸºäºä¸Šè¡¨ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡è¿”å›æ¯è¡Œå…·æœ‰å‰ 3 ä¸ªæœ€é«˜ç›¸ä¼¼æ€§å¾—åˆ†çš„å•†å®¶åç§°æ¥è¿›ä¸€æ­¥åˆ†ææ´å¯ŸåŠ›ã€‚

ç¼–å†™ä¸€ä¸ªå‡½æ•°***top*** ï¼Œè¯¥å‡½æ•°å°†ä¸Šè¿°æ•°æ®é›†ä½œä¸ºè¾“å…¥ï¼Œå¹¶è¿”å›ä¸€ä¸ªåŒ…å«å‰ 3 åå§“ååŠå…¶ç›¸ä¼¼æ€§å¾—åˆ†çš„æ•°æ®é›†ã€‚

```
similarity = df.reset_index()
similarity.head()def top(x):
    x.set_index('index', inplace=True)
    df = pd.DataFrame({'Max1Name':[],'Max2Name':[],'Max3Name':[],'Max1Value':[],'Max2Value':[],'Max3Value':[]})
    df.index.name='index'
    df.loc[x.index.values[0],['Max1Name', 'Max2Name', 'Max3Name']] = x.sum().nlargest(3).index.tolist()
    df.loc[x.index.values[0],['Max1Value', 'Max2Value', 'Max3Value']] = x.sum().nlargest(3).values
    return dftest = similarity.groupby('index').apply(top).reset_index(level=1, drop=True).reset_index()
test.head()
```

[](https://stackoverflow.com/questions/29919306/find-the-column-name-which-has-the-maximum-value-for-each-row) [## æŸ¥æ‰¾æ¯è¡Œä¸­å…·æœ‰æœ€å¤§å€¼çš„åˆ—å

### æ„Ÿè°¢è´¡çŒ®ä¸€ä¸ªå †æ ˆæº¢å‡ºçš„ç­”æ¡ˆï¼è¯·åŠ¡å¿…å›ç­”é—®é¢˜ã€‚æä¾›è¯¦ç»†ä¿¡æ¯å¹¶åˆ†äº«â€¦

stackoverflow.com](https://stackoverflow.com/questions/29919306/find-the-column-name-which-has-the-maximum-value-for-each-row) [](https://stackoverflow.com/questions/37494844/find-the-column-names-which-have-top-3-largest-values-for-each-row) [## æŸ¥æ‰¾æ¯è¡Œå‰ 3 ä¸ªæœ€å¤§å€¼çš„åˆ—å

### ä¾‹å¦‚ï¼Œæ•°æ®çœ‹èµ·æ¥åƒè¿™æ ·:df={'a1':[5ï¼Œ6ï¼Œ3ï¼Œ2ï¼Œ5]ï¼Œ' a2':[23ï¼Œ43ï¼Œ56ï¼Œ2ï¼Œ6]ï¼Œ' a3':[4ï¼Œ2ï¼Œ3ï¼Œ6ï¼Œ7]â€¦

stackoverflow.com](https://stackoverflow.com/questions/37494844/find-the-column-names-which-have-top-3-largest-values-for-each-row) 

é€šè¿‡æˆåŠŸå®ç°ä¸Šè¿°ä»£ç å•å…ƒï¼Œæ‚¨åº”è¯¥å¾—åˆ°å¦‚ä¸‹æ‰€ç¤ºçš„è¿”å›æ•°æ®é›†:

![](img/9c2612df69ddef72c2678e8abd75c814.png)

æ¯è¡Œå‰ 3 ä¸ªæœ€ç›¸ä¼¼çš„åŒ¹é…é¡¹

å°½ç®¡è¿™åªæ˜¯å¯¹ä¸€ä¸ªæ ·æœ¬æ•°æ®é›†çš„æµ‹è¯•ï¼Œä½†æ˜¯å¦‚æœæˆ‘ä»¬æœ‰éæ‚ä¹±çš„è¾“å…¥ä»¥åŠä¸€ä¸ªç»è¿‡æ¸…ç†çš„å•†å®¶åç§°ç›®å½•ï¼Œæˆ‘ä»¬ä»ç„¶ä¼šå‘ç°è¿™ç§æ–¹æ³•å¾ˆæœ‰ç”¨ã€‚

ç„¶è€Œï¼Œè¿™ç§æ–¹æ³•å¯¹äºæ›´å¤æ‚çš„å•†å®¶è¾“å…¥å¯èƒ½è¡¨ç°ä¸å¥½ã€‚ä¾‹å¦‚ï¼Œbooking.com å¤šä¼¦å¤šä¸Šä¸€ä¸ªåä¸º *paypal *å¡æ”¯ä»˜çš„å•†å®¶å¯èƒ½ä¼šè¿”å›ä¸€ä¸ªå…³äº paypal æˆ– Booking *çš„ä½ç›¸ä¼¼æ€§åˆ†æ•°(å°äº 0.5)ã€‚**

åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œéœ€è¦æ›´å…ˆè¿›çš„æ–¹æ³•æ¥æ£€æµ‹æˆ‘ä»¬æƒ³è¦çš„â€œçœŸå®â€å•†å®¶åç§°çš„ä½ç½®ã€‚

# ç¬¬ 3 å±‚:ç”¨ spaCy æ¸…ç†å•†æˆ·åç§°

é€šè¿‡å®Œæˆå‰ä¸¤å±‚ï¼Œæˆ‘ä»¬èƒ½å¤Ÿé€šè¿‡ç®€å•åœ°è¿”å›ç›¸ä¼¼æ€§å¾—åˆ†è¡¨æ¥è§£å†³ä¸€äº›å•†å®¶åç§°æ¸…ç†é—®é¢˜ï¼Œä¾‹å¦‚æ‹¼å†™é”™è¯¯ã€å¤§å°å†™ä¸åŒã€ç¼ºå°‘å­—ç¬¦/ç©ºæ ¼ï¼Œç”šè‡³ä¸€äº›éæ‚ä¹±çš„å•†å®¶è¾“å…¥ã€‚

ç„¶è€Œï¼Œæˆ‘ä»¬å®é™…ä¸Šä»å¤„äºä½¿ç”¨åŸºäº**è§„åˆ™çš„**æ¸…ç†å¼•æ“çš„é˜¶æ®µï¼Œè¿™æ„å‘³ç€è¿„ä»Šä¸ºæ­¢æˆ‘ä»¬ä»æœªä»æ•°æ®ä¸­å­¦ä¹ ã€‚æ­¤å¤–ï¼Œå³ä½¿é€šè¿‡ä½¿ç”¨å…¸å‹çš„æœºå™¨å­¦ä¹ æ¨¡å‹ï¼Œè®­ç»ƒé˜¶æ®µä»ç„¶éœ€è¦å¤§é‡çš„æ—¶é—´æ¥æ‰§è¡Œç‰¹å¾å·¥ç¨‹ä»¥åˆ›å»ºæ›´å¤šçš„ä¿¡æ¯ç‰¹å¾ã€‚

> â€¦æ½œåœ¨çš„äº¤æ˜“çº§ä¿¡æ¯ç‰¹å¾ï¼Œå¦‚é‡‘é¢å’Œç±»åˆ«ï¼ŒåŒæ—¶è¿˜ç”Ÿæˆå•è¯çº§è‡ªç„¶è¯­è¨€ç‰¹å¾ï¼Œå¦‚æ ‡ç­¾å†…çš„å•è¯ä½ç½®(å¦‚ç¬¬ä¸€ã€ç¬¬äºŒ)ã€å•è¯é•¿åº¦ã€å…ƒéŸ³æ¯”ä¾‹ã€è¾…éŸ³å’Œå­—æ¯æ•°å­—å­—ç¬¦ç­‰ã€‚
> 
> [**CleanMachine:é’±åŒ…çš„é‡‘èäº¤æ˜“æ ‡ç­¾ç¿»è¯‘ã€‚è‰¾**](https://medium.com/@liverence/cleanmachine-financial-transaction-label-translation-for-wallet-ai-5cc379c8f523)

å› æ­¤ï¼Œæˆ‘ç ”ç©¶äº†å¦‚ä½•ä½¿ç”¨æ·±åº¦å­¦ä¹ æ¨¡å‹æ¥åˆ›å»ºæ¸…æ´å¼•æ“ã€‚ä½¿ç”¨æ·±åº¦å­¦ä¹ æ¨¡å‹çš„ä¼˜åŠ¿åœ¨äºï¼Œæˆ‘ä»¬èƒ½å¤Ÿâ€œè·³è¿‡â€ç‰¹å¾å·¥ç¨‹æ­¥éª¤ï¼Œè®©æ¨¡å‹æœ¬èº«ä»è¾“å…¥ä¸­æ£€æµ‹ä»»ä½•æœ‰è§åœ°çš„æ¨¡å¼ã€‚

## ç©ºé—´ä»‹ç»

ä¸€ä¸ªå…è´¹çš„çŸ­æœŸç©ºé—´è¯¾ç¨‹å¯ä»¥æ‰¾åˆ°å¦‚ä¸‹:

[](https://course.spacy.io/en) [## å¸¦ç©ºé—´çš„é«˜çº§è‡ªç„¶è¯­è¨€å¤„ç†å…è´¹åœ¨çº¿è¯¾ç¨‹

### spaCy æ˜¯ä¸€ä¸ªç”¨äºå·¥ä¸šçº§è‡ªç„¶è¯­è¨€å¤„ç†çš„ç°ä»£ Python åº“ã€‚åœ¨è¿™ä¸ªè‡ªç”±å’Œäº’åŠ¨çš„â€¦

course.spacy.io](https://course.spacy.io/en) 

æ ¹æ® [**ç©ºé—´æŒ‡å—**](https://spacy.io/usage/training#section-basics) :

**spaCy** æ˜¯ Python å’Œ Cython ä¸­é«˜çº§**è‡ªç„¶è¯­è¨€å¤„ç†**çš„åº“ã€‚å®ƒå»ºç«‹åœ¨æœ€æ–°ç ”ç©¶çš„åŸºç¡€ä¸Šï¼Œä»ç¬¬ä¸€å¤©èµ·å°±è¢«è®¾è®¡ç”¨äºçœŸæ­£çš„äº§å“ã€‚spaCy è‡ªå¸¦é¢„è®­ç»ƒçš„**ç»Ÿè®¡æ¨¡å‹**å’Œ**è¯å‘é‡**ï¼Œç›®å‰æ”¯æŒ **60+è¯­è¨€**çš„æ ‡è®°åŒ–ã€‚

å®ƒå…·æœ‰æœ€å…ˆè¿›çš„é€Ÿåº¦ã€**å·ç§¯ç¥ç»ç½‘ç»œ**æ¨¡å‹ï¼Œç”¨äºæ ‡è®°ã€è§£æå’Œ**å‘½åå®ä½“è¯†åˆ«**ä»¥åŠè½»æ¾çš„æ·±åº¦å­¦ä¹ é›†æˆã€‚è¿™æ˜¯åœ¨éº»çœç†å·¥å­¦é™¢è®¸å¯ä¸‹å‘å¸ƒçš„å•†ä¸šå¼€æºè½¯ä»¶ã€‚

[](https://github.com/explosion/spaCy) [## çˆ†ç‚¸/ç©ºé—´

### spaCy æ˜¯ Python å’Œ Cython ä¸­çš„é«˜çº§è‡ªç„¶è¯­è¨€å¤„ç†åº“ã€‚å®ƒå»ºç«‹åœ¨æœ€æ–°çš„â€¦â€¦

github.com](https://github.com/explosion/spaCy) 

ç”±äºå•†å®¶åç§°æ¸…æ´—é—®é¢˜å¯ä»¥å½’å…¥**å‘½åå®ä½“è¯†åˆ«** (NER)çš„ä¸»é¢˜ä¸‹ï¼Œæˆ‘ç›¸ä¿¡é€šè¿‡è¾“å…¥ä¸€ç»„**æœ‰ä»£è¡¨æ€§çš„**è¾“å…¥æ•°æ®ï¼ŒspaCy æ¨¡å‹ä¼šæœ‰å¾ˆå¥½çš„è¡¨ç°ã€‚

## è®­ç»ƒç©ºé—´çš„ç»Ÿè®¡æ¨¡å‹

![](img/85a5d31e82480950a38ae9a9d2eaa6be.png)

[https://spacy.io/usage/training#section-basics](https://spacy.io/usage/training#section-basics)

ä¸ºäº†è®­ç»ƒä¸€ä¸ªç©ºé—´æ¨¡å‹ï¼Œæˆ‘ä»¬ä¸ä»…ä»…å¸Œæœ›å®ƒè®°ä½æˆ‘ä»¬çš„ä¾‹å­â€”â€”æˆ‘ä»¬å¸Œæœ›å®ƒæå‡ºä¸€ä¸ªç†è®ºï¼Œè¿™ä¸ªç†è®ºå¯ä»¥**æ¨å¹¿åˆ°å…¶ä»–ä¾‹å­**ã€‚

å› æ­¤ï¼Œè®­ç»ƒæ•°æ®åº”è¯¥æ€»æ˜¯ä»£è¡¨æˆ‘ä»¬æƒ³è¦å¤„ç†çš„æ•°æ®ã€‚å¯¹äºæˆ‘ä»¬çš„é¡¹ç›®ï¼Œæˆ‘ä»¬å¯èƒ½å¸Œæœ›ä»ä¸åŒç±»å‹çš„å•†å®¶åç§°ä¸­é€‰æ‹©è®­ç»ƒæ•°æ®ã€‚æœ€ç»ˆï¼Œæˆ‘ä»¬çš„åŸ¹è®­æ•°æ®å°†é‡‡ç”¨å¦‚ä¸‹å®ä½“åˆ—è¡¨çš„å½¢å¼:

```
TRAIN_DATA = 
[
('Amazon co ca', {'entities': [(0, 6, 'BRD')]}),
('AMZNMKTPLACE AMAZON CO', {'entities': [(13, 19, 'BRD')]}),
('APPLE COM BILL', {'entities': [(0, 5, 'BRD')]}),
('BOOKING COM New York City', {'entities': [(0, 7, 'BRD')]}),
('STARBUCKS Vancouver', {'entities': [(0, 9, 'BRD')]}),
('Uber BV', {'entities': [(0, 4, 'BRD')]}),
('Hotel on Booking com Toronto', {'entities': [(9, 16, 'BRD')]}),
('UBER com', {'entities': [(0, 4, 'BRD')]}),
('Netflix com', {'entities': [(0, 7, 'BRD')]})]
]
```

æˆ‘é€‰æ‹©çš„è®­ç»ƒæ•°æ®åªæ˜¯ä¸€ä¸ªæ ·æœ¬ã€‚è¯¥æ¨¡å‹å¯ä»¥æ¥å—æ›´å¤æ‚çš„è¾“å…¥ã€‚ä½†æ˜¯ï¼Œæ³¨é‡Šä¸€é•¿ä¸²å•†å®¶åç§°å¯èƒ½ä¼šæœ‰ç‚¹æ— èŠã€‚æˆ‘æƒ³æ¨èå¦ä¸€ä¸ªæ•°æ®æ ‡æ³¨å·¥å…· [***UBIAI***](https://ubiai.tools/Docs) æ¥å®Œæˆè¿™ä¸ªä»»åŠ¡ï¼Œå› ä¸ºå®ƒæ”¯æŒ spaCy æ ¼å¼ç”šè‡³äºšé©¬é€Šç†è§£æ ¼å¼çš„è¾“å‡ºã€‚

 [## æ˜“äºä½¿ç”¨çš„æ–‡æœ¬æ³¨é‡Šå·¥å…·|ä¸Šä¼ æ–‡æ¡£ï¼Œå¼€å§‹æ³¨é‡Šï¼Œå¹¶åˆ›å»ºé«˜çº§ NLPâ€¦

### æ˜“äºä½¿ç”¨çš„æ–‡æœ¬æ³¨é‡Šå·¥å…·|ä¸Šä¼  PDFã€CSVã€Docxã€html æˆ– ZIP æ ¼å¼çš„æ–‡æ¡£ï¼Œå¼€å§‹æ³¨é‡Šï¼Œå¹¶åˆ›å»ºâ€¦

ubiai.tools](https://ubiai.tools/) [](https://medium.com/@walidamamou/how-to-automate-job-searches-using-named-entity-recognition-part-1-16c150acafa0) [## å¦‚ä½•ä½¿ç”¨å‘½åå®ä½“è¯†åˆ«è‡ªåŠ¨æœç´¢å·¥ä½œâ€”ç¬¬ 1 éƒ¨åˆ†

### æ‰¾å·¥ä½œçš„ç®€å•æœ‰æ•ˆçš„æ–¹æ³•

medium.com](https://medium.com/@walidamamou/how-to-automate-job-searches-using-named-entity-recognition-part-1-16c150acafa0) [](https://medium.com/swlh/building-a-job-entity-recognizer-using-amazon-comprehend-5dd2c33faa82) [## ä½¿ç”¨ Amazon understand æ„å»ºä½œä¸šå®ä½“è¯†åˆ«å™¨

### ä»‹ç»

medium.com](https://medium.com/swlh/building-a-job-entity-recognizer-using-amazon-comprehend-5dd2c33faa82) 

å¯èƒ½éœ€è¦ä¸€äº›å¦‚ä½•é€‰æ‹©ä»£è¡¨**æ•°æ®è¾“å…¥çš„ç»éªŒã€‚éšç€ä½ ç»ƒä¹ å¾—è¶Šæ¥è¶Šå¤šï¼Œè§‚å¯Ÿç©ºé—´æ¨¡å‹å­¦ä¹ çš„æ–¹å¼ï¼Œä½ ä¼šè¶Šæ¥è¶Šæ¸…æ¥šâ€œä»£è¡¨â€å¯èƒ½æ„å‘³ç€â€œä¸åŒçš„ä½ç½®â€ã€‚è¿™å°±æ˜¯ä¸ºä»€ä¹ˆæˆ‘ä»¬éœ€è¦åœ¨è¾“å…¥æ•°æ®ä¸­æä¾›ä¸€ä¸ªå®ä½“ start & end ç´¢å¼•çš„åŸå› ï¼Œå› ä¸ºå®ƒå¯ä»¥å¸®åŠ©æ¨¡å‹ä»ä¸åŒçš„ä¸Šä¸‹æ–‡ä¸­å­¦ä¹ æ¨¡å¼ã€‚**

å¦‚æœæ¨¡å‹ç»å¸¸è¢«è®­ç»ƒä¸ºç¬¬ä¸€ä¸ªå•è¯çš„ä½ç½®æ˜¯å•†å®¶åç§°(Amazon ca)ï¼Œé‚£ä¹ˆå®ƒå€¾å‘äºè®¤ä¸ºå•†å®¶åç§°åªä½äºè¾“å…¥çš„å¼€å¤´ã€‚è¿™å¯èƒ½ä¼šå¯¼è‡´åè§ï¼Œå¹¶å¯¼è‡´å¯¹è¾“å…¥(å¦‚â€œéŸ³ä¹ Spotify â€)çš„é”™è¯¯é¢„æµ‹ï¼Œå› ä¸ºâ€œSpotifyâ€æ°å¥½æ˜¯ç¬¬äºŒä¸ªå•è¯ã€‚

ç„¶è€Œï¼Œåœ¨è¾“å…¥ä¸­åŒ…å«å„ç§å•†å®¶åç§°ä¹Ÿå¾ˆé‡è¦ã€‚è¯·æ³¨æ„ï¼Œæˆ‘ä»¬ä¸å¸Œæœ›æˆ‘ä»¬çš„æ¨¡å‹ä»…ä»…è®°ä½å®ƒä»¬ã€‚

ä¸€æ—¦å®Œæˆäº†å¯¹è®­ç»ƒæ•°æ®çš„è°ƒä¼˜ï¼Œå‰©ä¸‹çš„è¿‡ç¨‹å‡ ä¹å°±ä¼šè‡ªåŠ¨å®Œæˆã€‚

```
import spacy
import randomdef train_spacy(data,iterations):
    TRAIN_DATA = data
    nlp = spacy.blank('en')  # create blank Language class
    # create the built-in pipeline components and add them to the pipeline
    # nlp.create_pipe works for built-ins that are registered with spaCy
    if 'ner' not in nlp.pipe_names:
        ner = nlp.create_pipe('ner')
        nlp.add_pipe(ner, last=True)# add labels
    for _, annotations in TRAIN_DATA:
         for ent in annotations.get('entities'):
            ner.add_label(ent[2])# get names of other pipes to disable them during training
    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'ner']
    with nlp.disable_pipes(*other_pipes):  # only train NER
        optimizer = nlp.begin_training()
        for itn in range(iterations):
            print("Statring iteration " + str(itn))
            random.shuffle(TRAIN_DATA)
            losses = {}
            for text, annotations in TRAIN_DATA:
                nlp.update(
                    [text],  # batch of texts
                    [annotations],  # batch of annotations
                    drop=0.2,  # dropout - make it harder to memorise data
                    sgd=optimizer,  # callable to update weights
                    losses=losses)
            print(losses)
    return nlpprdnlp = train_spacy(TRAIN_DATA, 20)# Save our trained Model
modelfile = input("Enter your Model Name: ")
prdnlp.to_disk(modelfile)#Test your text
test_text = input("Enter your testing text: ")
doc = prdnlp(test_text)
for ent in doc.ents:
    print(ent.text, ent.start_char, ent.end_char, ent.label_)
```

ä¸Šé¢çš„ä»£ç æ¥è‡ªä¸‹é¢çš„ Medium æ–‡ç« ï¼Œå› ä¸ºæˆ‘å‘ç°å®ƒéå¸¸æœ‰å¸®åŠ©ï¼Œå¹¶å¯å‘æˆ‘æµ‹è¯• spaCy çš„ä¸€ä¸ªå•†å®¶åç§°æ¸…æ´—é—®é¢˜ã€‚

[](https://manivannan-ai.medium.com/how-to-train-ner-with-custom-training-data-using-spacy-188e0e508c6) [## å¦‚ä½•ä½¿ç”¨ spaCy è‡ªå®šä¹‰è®­ç»ƒæ•°æ®è®­ç»ƒ NERï¼Ÿ

### ä½¿ç”¨æˆ‘ä»¬çš„è‡ªå®šä¹‰æ•°æ®é›†è®­ç»ƒç©ºé—´åç§°å®ä½“è¯†åˆ«(NER)

manivannan-ai.medium.com](https://manivannan-ai.medium.com/how-to-train-ner-with-custom-training-data-using-spacy-188e0e508c6) [](https://www.machinelearningplus.com/nlp/training-custom-ner-model-in-spacy/) [## å¦‚ä½•è®­ç»ƒç©ºé—´è‡ªåŠ¨æ£€æµ‹æ–°å®ä½“(NER)[å®Œå…¨æŒ‡å—]

### å‘½åå®ä½“è¯†åˆ«(NER)æ˜¯è‡ªåŠ¨è¯†åˆ«æ–‡æœ¬ä¸­è®¨è®ºçš„å®ä½“çš„è¿‡ç¨‹

www.machinelearningplus.com](https://www.machinelearningplus.com/nlp/training-custom-ner-model-in-spacy/) 

## è¯„ä¼°ç©ºé—´æ¨¡å‹

é€šè¿‡æˆåŠŸå®Œæˆè®­ç»ƒæ­¥éª¤ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡æ£€æŸ¥å…¶æŸå¤±å€¼æ¥ç›‘æ§æ¨¡å‹è¿›åº¦ã€‚

```
Statring iteration 0
{'ner': 18.696674078702927}
Statring iteration 1
{'ner': 10.93641816265881}
Statring iteration 2
{'ner': 7.63046314753592}
Statring iteration 3
{'ner': 1.8599222962139454}
Statring iteration 4
{'ner': 0.29048295595632395}
Statring iteration 5
{'ner': 0.0009769084971516626}
```

> ç„¶åå‘æ¨¡å‹æ˜¾ç¤ºæœªæ ‡è®°çš„æ–‡æœ¬ï¼Œå¹¶è¿›è¡Œé¢„æµ‹ã€‚å› ä¸ºæˆ‘ä»¬çŸ¥é“æ­£ç¡®çš„ç­”æ¡ˆï¼Œæ‰€ä»¥æˆ‘ä»¬å¯ä»¥ä»¥è®¡ç®—è®­ç»ƒæ ·æœ¬å’Œé¢„æœŸè¾“å‡ºä¹‹é—´çš„å·®å¼‚çš„**æŸå¤±å‡½æ•°**çš„**è¯¯å·®æ¢¯åº¦**çš„å½¢å¼ç»™å‡ºå…³äºå…¶é¢„æµ‹çš„æ¨¡å‹åé¦ˆã€‚å·®å¼‚è¶Šå¤§ï¼Œæ¢¯åº¦å’Œæ¨¡å‹æ›´æ–°å°±è¶Šæ˜¾è‘—ã€‚

è¦æµ‹è¯•æ‚¨çš„æ¨¡å‹ï¼Œæ‚¨å¯ä»¥è¿è¡Œä¸‹é¢çš„ä»£ç å•å…ƒæ ¼:

```
#Test your text
test_text = input("Enter your testing text: ")
doc = prdnlp(test_text)
for ent in doc.ents:
    print(ent.text, ent.start_char, ent.end_char, ent.label_)
```

ä¾‹å¦‚ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨â€œpaypal paymentâ€ä½œä¸ºè¾“å…¥ï¼Œå¹¶æµ‹è¯•æ¨¡å‹æ˜¯å¦èƒ½æ£€æµ‹å‡ºâ€œpaypalâ€æ˜¯æ­£ç¡®çš„å“ç‰Œåç§°ã€‚

![](img/c8ea1c43afca068897ae16ddf7392988.png)

è€ƒè™‘åˆ° PayPal æ²¡æœ‰å‡ºç°åœ¨è®­ç»ƒè¾“å…¥ä¸­ï¼Œè¯¥æ¨¡å‹åšå¾—éå¸¸å¥½ã€‚

è¿™ä¹Ÿç»“æŸäº†æˆ‘çš„é¡¹ç›®ï¼Œå»ºç«‹ä¸€ä¸ªå•†ä¸šåç§°ä¸ç©ºé—´æ¨¡å‹æ¸…æ´å¼•æ“ã€‚

# ç»“è®º

é¦–å…ˆï¼Œæ„Ÿè°¢æ‚¨èŠ±æ—¶é—´é˜…è¯»è¿™ç¯‡é•¿æ–‡ï¼Œæˆ‘çœŸè¯šåœ°å¸Œæœ›å®ƒå¯¹æ‚¨æœ‰æ‰€å¸®åŠ©~

*   æˆ‘é¦–å…ˆä»‹ç»äº†ä¸ºä»€ä¹ˆå•†æˆ·åç§°æ¸…ç†å¾ˆé‡è¦ã€‚
*   ç„¶åï¼Œæˆ‘æŠŠæ¸…æ´å¼•æ“åˆ†æˆä¸‰å±‚ã€‚
*   æœ€åï¼Œå¯¹äºæ¯ä¸€å±‚ï¼Œæˆ‘è§£é‡Šäº†è¾“å…¥å’Œè¾“å‡ºï¼Œä»¥åŠä¸ºä»€ä¹ˆæ¯ä¸€å±‚éƒ½æ˜¯å¿…è¦çš„ã€‚

ä½œä¸ºå¯¹è¿™ä¸ªæŒ‘æˆ˜æ€§é—®é¢˜çš„æ¦‚è¿°ï¼Œæˆ‘å¹¶ä¸å®Œå…¨æœŸæœ›ç»™å‡ºä¸€ä¸ªå®Œç¾çš„è§£å†³æ–¹æ¡ˆã€‚ä½†æ˜¯æˆ‘ç›¸ä¿¡åˆ†äº«æˆ‘çš„æƒ³æ³•å’Œæˆ‘åœ¨ç ”ç©¶è¿‡ç¨‹ä¸­å‘ç°çš„ä»»ä½•æœ‰ç”¨çš„è¯»ç‰©ä¼šæœ‰æ‰€å¸®åŠ©ã€‚

å› æ­¤ï¼Œæˆ‘å¸Œæœ›ä½ å–œæ¬¢é˜…è¯»è¿™ç¯‡æ–‡ç« ã€‚åŒæ—¶ï¼Œæˆ‘åœ¨æœ¬æ–‡æœ«å°¾æ·»åŠ äº†ä¸€ä¸ªå‚è€ƒåˆ—è¡¨éƒ¨åˆ†ï¼Œä»¥é˜²æ‚¨æœ‰å…´è¶£äº†è§£å…³äºè¿™ä¸ªä¸»é¢˜çš„æ›´å¤šä¿¡æ¯ã€‚

è°¢è°¢~

# é¡µï¼ˆpage çš„ç¼©å†™ï¼‰sã€‚

## ç¬¬ä¸€æ¬¡æ›´æ–°:2021 å¹´ 2 æœˆ 5 æ—¥

å¯¹ spaCy æ¨¡å‹çš„è®­ç»ƒç»“æœè¿›è¡Œäº†ä¸€ç‚¹æ›´æ–°ï¼Œå¤§çº¦æœ‰ 1000 è¡Œè¾“å…¥æ•°æ®ã€‚(1000 æ¡æ³¨é‡Š)

æˆ‘å·²ç»å°†æ¨¡å‹è®¾ç½®ä¸ºæ¯è½®è®­ç»ƒ 50 æ¬¡è¿­ä»£ï¼Œå¹¶å¯¹å…¶è¿›è¡Œ 10 è½®è®­ç»ƒï¼Œä»¥æŸ¥çœ‹è®­ç»ƒæŸå¤±å¦‚ä½•å˜åŒ–ã€‚ä¼¼ä¹å¦‚æœæˆ‘ä»¬ä»¥æ­£ç¡®çš„æ–¹å¼é¢„å¤„ç†æ•°æ®ï¼Œæˆ‘ä»¬åº”è¯¥èƒ½å¤Ÿé€šè¿‡ 50 æ¬¡è¿­ä»£æ¯æ¬¡è¾¾åˆ°ä½ä¸”ä¸€è‡´çš„è®­ç»ƒæŸå¤±ï¼Œå¦‚ä¸‹å›¾æ‰€ç¤º:

![](img/8c3274010dc40d32a534efd5e9d61fc2.png)

10 è½®è®­ç»ƒï¼Œæ¯è½® 50 æ¬¡è¿­ä»£

## ç¬¬äºŒæ¬¡æ›´æ–°:2021 å¹´ 2 æœˆ 8 æ—¥

ğŸ¤”ä½†æ˜¯ï¼Œè¿˜æœ‰ä»€ä¹ˆæ–¹æ³•å¯ä»¥è®©æˆ‘ä»¬çš„ spaCy æ¨¡å‹æ›´å…·äº¤äº’æ€§å‘¢ï¼Ÿå¦‚æœæˆ‘ä»¬èƒ½æŠŠ spaCy model å’Œ Streamlit é›†æˆåˆ°ä¸€ä¸ª web ç•Œé¢ä¸­ï¼Œä¼šä¸ä¼šæ›´åŠ ç”¨æˆ·å‹å¥½ï¼Ÿ

æˆ‘å—åˆ°äº†å¦‚ä¸‹[ç©ºé—´æ–‡æ¡£](https://spacy.io/usage/projects)çš„å¯å‘:

![](img/faff4014a0c032ca17597634c9443238.png)

[https://spacy.io/usage/projects](https://spacy.io/usage/projects)

ä¼¼ä¹ spaCy æ”¯æŒåŒ…æ‹¬ Streamlit åœ¨å†…çš„è®¸å¤šèŠ±å“¨çš„å·¥å…·ã€‚å› æ­¤ï¼Œæˆ‘å†³å®šå°è¯•å°† spaCy å’Œ Streamlit æ•´åˆåˆ°ä¸€ä¸ª web åº”ç”¨ç¨‹åºä¸­ã€‚

å¦‚æœæ‚¨æƒ³äº†è§£æ›´å¤šå…³äº Streamlit çš„çŸ¥è¯†ï¼Œä¸‹é¢è¿™ç¯‡æ–‡ç« æ˜¯ä¸€ä¸ªå¥½çš„å¼€å§‹ã€‚

[](/streamlit-and-spacy-create-an-app-to-predict-sentiment-and-word-similarities-with-minimal-domain-14085085a5d4) [## Streamlit å’Œ spaCy:åˆ›å»ºä¸€ä¸ªåº”ç”¨ç¨‹åºæ¥é¢„æµ‹æƒ…ç»ªå’Œå•è¯çš„ç›¸ä¼¼æ€§ï¼Œæœ€å°åŸŸâ€¦

### åªéœ€è¦ 10 è¡Œä»£ç ï¼

towardsdatascience.com](/streamlit-and-spacy-create-an-app-to-predict-sentiment-and-word-similarities-with-minimal-domain-14085085a5d4) 

ç”±äºä¹‹å‰æˆ‘ä»¬å·²ç»å°†ç©ºé—´è®­ç»ƒå‡½æ•°å®šä¹‰ä¸º **train_spacy** ï¼Œå‰©ä¸‹çš„å·¥ä½œå°†åœ¨ 10 è¡Œä»£ç ä¹‹å†…ã€‚è¿™ä¹Ÿæ˜¯æˆ‘è®¤ä¸ºæˆ‘åº”è¯¥åœ¨åŒä¸€ç¯‡æ–‡ç« ä¸‹ç»™å‡ºä¸€ä¸ªæ›´æ–°çš„åŸå› ğŸ¤—

å‡è®¾æˆ‘ä»¬å·²ç»å‡†å¤‡äº†ä¹‹å‰ **TRAIN_DATA æ ¼å¼çš„è¾“å…¥æ³¨é‡Šåˆ—è¡¨ã€‚**

Streamlit ä¸­çš„ web åº”ç”¨ç¨‹åºç•Œé¢ä»£ç å¦‚ä¸‹æ‰€ç¤º:

```
import pandas as pd
import numpy as np 
import random
import spacy
import re
import warnings
import streamlit as st 
warnings.filterwarnings('ignore') # ignore warnings nlp = train_spacy(TRAIN_DATA, 50) # number of iterations set as 50# Save our trained Model 
# Once you obtained a trained model, you can switch to load a model for merchant name cleaning
modelfile = input("Enter your Model Name: ")
nlp.to_disk(modelfile)# Load our saved Model 
# Load your model to clean a user input instead of training a new model once again when there is a new input
# nlp = spacy.load(modelfile/) # path to the saved file foldertext = 'Amazon/ca' # default text input on web interfacest.title('Merchant Names Cleaning App') # web app title nameuser_input = st.text_input("Text", text) # input text placedoc = nlp(user_input) for ent in doc.ents:

    st.write('Text:', ent.text) # display model output 
    st.write('Label:', ent.label_) # display model output
```

ä¸€ä¸ªæˆåŠŸçš„ web ç•Œé¢åº”è¯¥å¦‚ä¸‹æ‰€ç¤º:

![](img/369fd73511719580d52b262ba3b75b85.png)

ä½¿ç”¨ Streamlit çš„ç¤ºä¾‹ web ç•Œé¢

å¸Œæœ›ä½ å–œæ¬¢é˜…è¯»ï¼

# å¼•ç”¨è¡¨

[1] [ä» Reddit ä¸Šæ¸…ç†ä¼ä¸šåç§°](https://www.reddit.com/r/datascience/comments/76yu3a/cleaning_up_business_names/)

[2] [åˆ©ç”¨æœºå™¨å­¦ä¹ æ”¹è¿›æ‚¨çš„äº¤æ˜“æ•°æ®åˆ†ç±»](https://blog.truelayer.com/improving-the-classification-of-your-transaction-data-with-machine-learning-c36d811e4257)

[3] [ç†æ¸…æ‚ä¹±çš„é“¶è¡Œæ•°æ®](https://plaid.com/blog/making-sense-of-messy-data/)

[4] [ä½¿ç”¨ Greenplum Hadoop æ ‡å‡†åŒ–é‡‘èæœåŠ¡é¢†åŸŸçš„ 1 . 13 äº¿å¤šå®¶å•†æˆ·åç§°](https://www.slideshare.net/datasciencelondon/greenplum-hd-merchant-standardization)

[5] [ä¸ FuzzyWuzzy åŒ¹é…çš„å­—ç¬¦ä¸²](/string-matching-with-fuzzywuzzy-e982c61f8a84)

[6] [ç”¨ Python è¿›è¡Œæ¨¡ç³Šå­—ç¬¦ä¸²åŒ¹é…çš„è‡ªç„¶è¯­è¨€å¤„ç†](/natural-language-processing-for-fuzzy-string-matching-with-python-6632b7824c49)

ã€7ã€‘[å¤§è§„æ¨¡æ¨¡ç³Šå§“ååŒ¹é…çš„é›†æˆæ–¹æ³•](https://medium.com/bcggamma/an-ensemble-approach-to-large-scale-fuzzy-name-matching-b3e3fa124e3c)

ã€8ã€‘[æ··åˆæ¨¡ç³Šåç§°åŒ¹é…](/hybrid-fuzzy-name-matching-52a4ec8b749c)

[9] [CleanMachine:é’±åŒ…çš„é‡‘èäº¤æ˜“æ ‡ç­¾ç¿»è¯‘ã€‚è‰¾](https://medium.com/@liverence/cleanmachine-financial-transaction-label-translation-for-wallet-ai-5cc379c8f523)

ã€10ã€‘[æ·±åº¦å­¦ä¹ é­”æ³•:å°ä¼ä¸šç±»å‹](/deep-learning-magic-small-business-type-8ac484d8c3bf)

[11] [ä½¿ç”¨ Pytorch æ£€æµ‹å•†æˆ·åç§°](https://github.com/MaxinAI/merchant_name_extraction_cnn)

[12] [å¯¹é“¶è¡Œäº¤æ˜“æ•°æ®è¿›è¡Œåˆ†ç±»](https://github.com/eli-goodfriend/banking-class)

[13] [è®­ç»ƒç©ºé—´çš„ç»Ÿè®¡æ¨¡å‹](https://spacy.io/usage/training#_title)

[14] [å¦‚ä½•è®­ç»ƒ spaCy è‡ªåŠ¨æ£€æµ‹æ–°å®ä½“(NER)ã€å®Œå…¨æŒ‡å—ã€‘](https://www.machinelearningplus.com/nlp/training-custom-ner-model-in-spacy/)

[15] [ä½¿ç”¨ Amazon comprehension](https://aws.amazon.com/cn/blogs/machine-learning/build-a-custom-entity-recognizer-using-amazon-comprehend/)æ„å»ºè‡ªå®šä¹‰å®ä½“è¯†åˆ«å™¨

[16] [ç”¨äºšé©¬é€Š SageMaker Ground Truth å’Œäºšé©¬é€Š comprehension å¼€å‘ NER æ¨¡å‹](https://aws.amazon.com/cn/blogs/machine-learning/developing-ner-models-with-amazon-sagemaker-ground-truth-and-amazon-comprehend/)

[17] [UBIAI æ–‡æ¡£](https://ubiai.tools/Docs)

[18] [å¦‚ä½•ä½¿ç”¨å‘½åå®ä½“è¯†åˆ«å®ç°å·¥ä½œæœç´¢è‡ªåŠ¨åŒ–â€”ç¬¬ 1 éƒ¨åˆ†](https://medium.com/@walidamamou/how-to-automate-job-searches-using-named-entity-recognition-part-1-16c150acafa0)

[19] [ä½¿ç”¨ Amazon understand æ„å»ºä½œä¸šå®ä½“è¯†åˆ«å™¨](https://medium.com/swlh/building-a-job-entity-recognizer-using-amazon-comprehend-5dd2c33faa82)

[20] [Streamlit å’Œ spaCy:ç”¨æœ€å°‘çš„é¢†åŸŸçŸ¥è¯†åˆ›å»ºä¸€ä¸ªé¢„æµ‹æƒ…æ„Ÿå’Œå•è¯ç›¸ä¼¼åº¦çš„åº”ç”¨](/streamlit-and-spacy-create-an-app-to-predict-sentiment-and-word-similarities-with-minimal-domain-14085085a5d4)*