# 通过辩论让 AI 变得安全

> 原文：<https://towardsdatascience.com/making-ai-safe-through-debate-935fe8a0ec5?source=collection_archive---------19----------------------->

## [播客](https://towardsdatascience.com/tagged/tds-podcast)

## 伊桑·佩雷斯解释了人工智能辩论如何让我们安全地获得超级智能

要选择章节，请访问 Youtube 视频[这里](https://youtu.be/rT7sP5CMAU4)。

*编者按:这一集是我们关于数据科学和机器学习新兴问题的播客系列的一部分，由 Jeremie Harris 主持。除了主持播客，Jeremie 还帮助运营一家名为*[*sharpes minds*](http://sharpestminds.com)*的数据科学导师初创公司。可以听下面的播客:*

[苹果](https://podcasts.apple.com/ca/podcast/towards-data-science/id1470952338?mt=2) | [谷歌](https://www.google.com/podcasts?feed=aHR0cHM6Ly9hbmNob3IuZm0vcy8zNmI0ODQ0L3BvZGNhc3QvcnNz) | [SPOTIFY](https://open.spotify.com/show/63diy2DtpHzQfeNVxAPZgU) | [其他](https://anchor.fm/towardsdatascience)

大多数人工智能研究人员相信，有一天我们将创造出超级智能系统——在各种各样的任务中远远超过人类的机器。

如果这种情况最终发生，将会带来一些潜在的严重问题。具体来说:如果一个系统是超智能的，我们如何保持对它的控制？这是人工智能对齐问题的核心——将高级人工智能系统与人类价值观对齐的问题。

对齐问题的完整解决方案必须包括至少两件事。首先，我们必须确切地知道我们希望超级智能系统做什么，并确保当我们要求它们做时，它们不会误解我们(“外部对齐”问题)。但是第二，我们必须确保这些系统是真正地在尝试优化我们要求他们做的事情，并且他们没有试图欺骗我们(“内部对齐”问题)。

创造内部一致的超级智能系统可能看起来是不同的问题——许多人认为它们是不同的。但在过去的几年里，人工智能研究人员一直在探索一种新的策略家族，一些人希望这种策略能够让我们同时实现超级智能和内在一致性。今天的嘉宾 Ethan Perez 正在使用这些方法构建语言模型，他希望这些语言模型能够成为未来超级智能系统的重要组成部分。Ethan 曾在谷歌、脸书和 MILA 从事前沿研究，现在正全职致力于开发具有泛化能力的学习系统，有朝一日这种能力可能会超过人类。

以下是我在对话中最喜欢的一些观点:

*   研究创造超级智能人工智能系统的方法的挑战之一是，超级智能的含义相当不清楚。比 90%的人类都聪明的系统是超级智能吗？99%呢？如果一个系统只和人类中值一样聪明，但却能以百万倍的速度思考，因为它工作在计算机时钟时间上，那会怎样？或者，如果人类水平的智能可以简单地被复制大量次，允许并行探索许多不同的可能性，会怎么样？这些问题没有普遍接受的答案。
*   有可能监督学习技术无法让我们一路到达超级智能系统。这是因为他们依赖于人类产生的数据，只能学习像那些人类一样表现(有一些额外的优势，我不会进入)。从一个角度来看，监督学习系统可以说是“模拟器”而不是“原始思考者”，但伊森认为，通过教它们简单的逻辑，或许有可能将它们的能力扩展到超智能领域。
*   这种逻辑就是分解:伊桑的工作包括训练语言模型来分解抽象和复杂的问题(比如，“苹果应该是非法的吗？”)转化为他们所依赖的更简单、更易处理的子问题(比如，“苹果有多有害？”，以及“一件事要有多有害，我们才能宣布它为非法？”).然后在子问题上重复这种分解方法，这个过程可以根据需要重复多次，以确保我们得到的最终子问题是简单的。如果这些最后的问题足够简单，像 GPT-3 这样的人类训练语言模型可以用来直接回答它们。这种技术属于统称为迭代蒸馏和扩增(IDA)的策略家族。
*   这种方法的一个好处是它是人类可以理解的。至少在原则上，人类可以调查每个分支子问题，并确信人工智能使用的逻辑是合理的，并且人工智能没有试图欺骗他们(这是当前在[人工智能比对文献](https://www.alignmentforum.org/posts/SzecSPYxqRa5GCaSF/clarifying-inner-alignment-terminology)中深入探索的一个严重问题)。但在实践中，Ethan 的系统生成的问题和子问题树太大，人类无法详细解析。因此，Ethan 正在探索一些方法，可以让人类更有效地发现给定的人工智能系统是否提供了真诚的答案。其中一种方法是辩论:通过让两个人工智能系统在一个精心控制的环境中相互辩论，逻辑错误或欺骗可能会浮出水面，并被人类法官抓住。我们讨论了这种方法可能有效的一些原因，以及可能无效的一些原因。

你可以[在推特上关注伊森](https://twitter.com/EthanJPerez)，或者[在推特上关注我](https://twitter.com/jeremiecharris)

## 播客中引用的链接:

*   伊森引出的情节在这里。
*   [伊森的个人网站在这里](https://ethanperez.net/)。

![](img/73e697079f7cfb0f98d332eef9737668.png)

## 章节:

*   0:00 介绍
*   1:37 伊森的背景
*   7:26 与 IDA 的问题
*   12:09 哥德尔不完全定理
*   15:01 当今 IDA 的角色
*   26:45 国际开发协会和 GPT-3 的能力
*   29:54 系统和辩论流程
*   41:21 让 AGI 发挥作用
*   49:33 我们控制这些系统的能力
*   51:41 总结

## 请查看下面的文字记录:

杰里米·哈里斯(00:00):
大家好。我是杰里米。欢迎回到迈向数据科学播客。今天的这一集是关于人工智能中最大的未决问题之一，句号。这就是我们是否能够使用当前的人工智能系统达到超人智能水平的问题。

Jeremie Harris (00:18):
许多人怀疑我们是否能够使用传统的机器学习来实现超人的智能，原因之一是传统的机器学习算法通常是根据人类创造的数据进行训练的。因此，逻辑是，你怎么能通过在人类水平的数据上训练它来达到超人的智能呢？

Jeremie Harris (00:38):
现在，有很多策略可以解决这个问题，试图从人类水平的数据中获得超人的智能，其中一种叫做迭代蒸馏和放大，简称 IDA。虽然 IDA 本身有多种形状和大小，但迄今为止，IDA 最常见的卓有成效的应用之一是问答系统。现在，这通常包括将复杂的问题分解成简单的问题，从理论上讲，这些问题可能非常复杂，人类无法理解，人类实际上可以解析，处于人类水平的人工智能系统实际上可以回答和处理。

Jeremie Harris (01:12):
我今天的嘉宾 Ethan Perez 是 IDA 风格问答系统的专家。我们将会讨论这些系统，他对这些系统如何产生 AGI 的想法，以及他对人工智能安全更普遍的思考。所以，有很多要了解的。我希望你和我一样喜欢这次谈话，没有任何进一步的麻烦，让我们开始吧。

杰瑞米·哈里斯(01:32):
你好，伊桑。非常感谢你参加我的播客。

伊桑·佩雷斯(01:34):
嘿。很好聊天。

杰瑞米·哈里斯(01:37):
你能来我真的很高兴。实际上，这是一次我一直很兴奋能和某人进行的谈话，而你绝对是最好的人选。我真的很好奇你对一种特定的比对 AI 安全，AI 比对策略的看法，这是一种可能会在辩论或 IDA 中出现的策略。我们一会儿会讲到这是什么。但首先我想了解你一点，探索你的背景，你是如何来到这个空间的。那么，你是如何发现人工智能安全和人工智能对齐的，是什么让你全职研究它？

Ethan Perez (02:12):
我对人工智能的长期影响普遍感到兴奋。我，在早期，有一些数学背景，我在想，哦，有了这种背景我能做些什么有影响力的事情。对机器学习有了更多的了解。然后我读了尼克·博斯特罗姆的书《超智能》，我觉得这本书大概是有这些兴趣的人的常见读物。我认为这让我开始思考人工智能的长期影响，哦，我真的应该思考当我们的技术在能力上接近人类水平，甚至超过人类水平时会发生什么。

伊桑·佩雷斯(02:51):
这可能是我生命终结时发生的事情，甚至不是在我的有生之年。但我认为，这真的是人工智能将产生许多影响的地方，也是我们能够使用人工智能来实际提升我们人类知识的地方，这是我真正关心的事情。

Jeremie Harris (03:06):
顺便提一下，我想问一点关于超智能的事情。是什么超级智慧改变了你对事物的看法？读了这本书，你的观点是如何转变的？

Ethan Perez (03:17):
我认为这本书非常注重长期考虑。我不认为我有足够的专业知识或时间去衡量尼克当时对存在风险和其他事情的非常具体的论点。但我认为它确实提出了一些论点，即人工智能和非常强大的人工智能系统是可能的。

Ethan Perez (03:41):
我记得的一个关键论点是，假设我们有一个系统可以在某些任务上达到人类水平的能力。也许一些可以想象的方法就是模仿人类的大脑。这可能在计算上非常昂贵，但至少在原则上，这似乎是可能的，因为我们是这种智能系统的一个例子。我们只需要复制它。

Ethan Perez (04:09):
然后，如果你做某种复制程序，也许如果你认为神经科学[听不清 00:04:13]是不可能的，我们可以在很大规模上对人类文本或数据进行监督学习或无监督学习，并获得类似的效果。但这似乎有点道理，然后他提出了这一点，哦，你可以在很大程度上加速或并行化该软件，然后得到某种超人的东西，在某种意义上，它可以比人类更快地做事情，但在人类水平的能力质量上。

伊森·佩雷兹(04:40):
我认为这让我很兴奋，哦，我的阅读速度和我能学到的东西太多了。如果我能有 1000 份我的拷贝，去阅读互联网上的不同部分，那将是非常令人兴奋的。我正在寻找一种新的饮食或寻找一个新的哲学问题，然后向我报告他们收集的所有信息。我认为这是一个似是而非的论点，对我来说似乎很现实，哦，这似乎很合理，我们可能会得到比我们在集成方面做得更多的系统。

耶雷米·哈里斯(05:18):
是啊。从某种意义上来说，这几乎就像是在玩智力是一个定义非常模糊的东西的游戏，在这里你甚至可以通过几乎完全复制人类的神经硬件而变得超级聪明。在一个不同的媒介中进行，在那里计算会更快，因为它是[听不清 00:05:37]细胞或其他东西的基质。你马上就有了某种东西，它实际上是超人的，尽管算法只是人类的。这是一种有趣的…是的，硬件软件方面。

杰瑞米·哈里斯(05:49):
这是否给了你提示？就像，你准备好了超级智慧，你就像，“我知道我想把这作为我一生的工作。”

伊森·佩雷斯(05:53):
这促使我接受了我的第一份主要研究实习，并且-

Jeremie Harris (05:58):
在 MILA，对吗？

伊森·佩雷兹(05:59):
是的。在 MILA。没错。艾伦·库维尔是一位伟大的导师。我在那里有其他非常好的同行，并最终有了一篇成功的论文。然后，我真的很享受整个过程。我对我们如何利用机器学习来帮助人类提升我们的集体知识感到非常兴奋。我只是读了很多书，总是很兴奋地想了解这个世界。有时候，我觉得非常瓶颈，或者我觉得我们真的需要在回答一些关于世界的问题的能力上有所改变。尤其是，我在哲学方面思考这个问题，看起来，哦，这些真的是很难的问题，看起来我们需要很多很多年才能回答一些问题。我想，我们真的需要思考新的方法来解决真正具有挑战性的问题。

Ethan Perez (06:50):
然后，我想在第一次研究实习后，我开始更广泛地思考，哦，还有哪些方法可以让我们超越人类的能力。是的，我的意思是，没有太多的方法可以做到这一点，这似乎是合理的，这就是我遇到迭代蒸馏和放大的地方，以及在不同范式的背景下进行的辩论，以超越人类的能力，而不仅仅是模仿人类的能力，监督和非监督方法都可以做到。

Jeremie Harris (07:26):
这实际上给我们带来了另一个有趣的方面，也就是从一开始，你就在接近 IDA，迭代蒸馏和放大，这是我从对话开始就一直在取笑的东西。你谈到这是从人工智能系统获得超人能力的一种方式。我很乐意讨论这一点，因为通常当我听到 IDA 谈论时，我会花更多的时间与 AI 安全社区的人交谈，所以他们专注于 IDA，将其作为实现可以被询问的安全 AI 系统的一种方式。我也想谈谈这个，但我真的很好奇，是什么阻止我们在没有 IDA 的情况下获得超人的性能。我认为那可能是一个有趣的起点。

伊森·佩雷斯(08:06):
是的。是的，我的意思是，我认为基线，思考它的起点是，嗯，我们训练系统回答问题的默认方式是监督学习。这是训练系统回答问题的所有最先进和标准的方法，MLP 只是使用监督学习，我们收集问题答案的大型数据集，然后我们学习从问题到答案的映射。这是一个很好的范例，只要我们能够收集问题的答案，这对于人们在谷歌上搜索的许多问题和非常实际的问题都是可能的，比如，这个人是什么时候出生的？但是，当我们无法收集问题的答案时，这种范式基本上就崩溃了。

Ethan Perez (08:53):
然后，基本上，然后我们只是没有任何训练数据来训练我们的监督学习算法。这基本上是迭代蒸馏和放大试图解决的问题是，假设你有一个系统可以进行人类水平的问题回答，因为你已经在我们有人们收集的问题答案的数据上训练了它。所以你有了这个黑盒子，投标问答系统。现在的问题是，我们如何使用这个系统来回答一些超出其分布的问题，因为它没有被一个问题的答案所标记？

伊桑·佩雷兹(09:30):
比方说，就像哲学中的问题，我认为这是一个很好的例子，因为我们没有这类问题的答案。因此，它们可能不在分布范围内，可能需要不同的推理过程才能得到答案。但是我们有一个非常好的问答系统，可以回答很多其他相关的问题。

Ethan Perez (09:49):
我们可以考虑的解决这个问题的方法是，将我们没有实际答案的问题分解成子问题，我们可以用现有的问答系统来回答这些子问题。所以我们可以把它分解成，我不知道，如果这样的问题，人们有自由意志吗？我们首先会有一个问题，自由意志是什么意思？然后我们可以围绕自由意志的神经科学提出一些问题，等等，等等。然后，对于这些问题中的每一个，我们可以用我们的问答系统来回答它们，或者如果因为它们具有挑战性而需要进一步细分，我们可以进一步细分它们，然后回答这些子问题。

Ethan Perez (10:34):
我们希望通过不断地将这些问题分解为子问题，我们最终会找到一个有监督的问答系统需要训练的问题；其中这些子问题是我们可靠地期望我们的训练模型准确回答的那种问题。那么，现在我们已经回答了这个大问题的一系列不同部分，给定这些子问题的答案，我们预测最终答案应该容易得多。

Jeremie Harris (11:04):
所以，简单地说，为了确保我理解它，你有一些复杂或非常困难的问题，这个问题名义上需要超人的智慧来回答。你拥有一种，也许是人类的，甚至是你训练出来的亚人类水平的智力。你可以训练它，因为你有一大堆真实人类给出的问题和答案，所以你可以用这两样东西来训练这种大致人类水平的智能。然后我们希望可以把这个非常复杂的问题分解成子问题，最终可以用这个略低于人类或大致相当于人类水平的智能来回答。这就是计划吗？

伊桑·佩雷兹(11:46):
是的。没错。

Jeremie Harris (11:50):
这太酷了，因为对我来说，如果这是真的，如果我们真的可以做到这一点，那么原则上没有问题，如果我错了，请纠正我，但是没有问题不能被分解成最终人类可以理解的子问题。

Ethan Perez (12:09):
我想你可能会进入，如果你做非常一般的陈述，你可能会进入围绕哥德尔不完全性定理的问题，我们知道有些真实的陈述我们无法证明答案。但是也许有一个适度的版本，我的意思是，一些用来证明哥德尔不完全性定理的例子有点奇怪，人们并不真正关心它们。但是你可能会说，对于大多数我们真正关心的问题，可能有一些合理的方法把它们分成子问题，足够多的子问题。在某些情况下，我们可能会回答非常多的子问题，是的。

哈利斯(12:56):
对不起。你能详细解释一下与此相关的哥德尔不完全性定理吗？我想可能有些听众不熟悉哥德尔的不完全性定理。

伊桑·佩雷斯(13:02):
哦，是的。

杰里米·哈里斯(13:03):
这是一个有趣的方面。

伊桑·佩雷斯(13:05):
是啊。我是说，我肯定不是这方面的专家。

杰里米·哈里斯(13:08):
当然，是的。

Ethan Perez (13:08):
但据我所知，这种说法的简单陈述是存在真实陈述，我们知道这些陈述是真实的，但无法证明这些陈述是真实的。这些通常是自我参照的陈述，就像-

耶雷米·哈里斯(13:27):
就像一个能容纳自己的筛子，或者类似的东西，是吗？

伊桑·佩雷兹(13:30):
是的，或者这种说法是错误的。他们有一种自我指涉的特质。这最初让我很难过。我想，哦，我们不能证明每一个真实的陈述。这真的令人失望，但我认为这将是一个很大的进步。我的意思是，这些很奇怪，因为它们是自我指涉的，而且有很多…有一个存在的证明，我们不能证明对每一个真实的陈述进行推理，但我认为…对我来说，这有点不清楚，在实践中实际上在多大程度上成立。

Jeremie Harris (14:04):
我想这本身就是一个有趣的问题，就像谈论 IDA 的局限性或任何策略的局限性一样，我的意思是，从安全的角度来看，我们希望使用 IDA 这样的东西来制造一种超级智能，可以探索任意复杂的想法。如果这些想法最终涉及的逻辑领域最终不是人类可以理解的，也就是说，我们实际上不能把它们分解成甚至人类可以理解的术语，那么这就像是妖怪从瓶子里出来了。从本质上讲，我们没有希望在预测这个系统的行为时重新控制这个思维过程。

伊桑·佩雷兹(14:44):
是的。我猜你可能想要一个可以说“嘿，我不能回答这个问题”的系统。

耶雷米·哈里斯(14:51):
对。

伊桑·佩雷斯(14:53):
就像“给定我的计算资源，找不到证据”之类的话。

Jeremie Harris (15:01):
我们通过引用人类水平系统或几乎人类水平系统来谈论 IDA，然后我们谈论它如何导致超级智能。我想我的一个问题是，艾达今天怎么样？游戏的状态是怎样的，你可以用 IDA 系统做些什么，否则你可能做不了什么？

伊桑·佩雷斯(15:21):
哦，是的。我认为这是一个很好的问题。我认为这种系统现在很有用，因为从根本上来说，它们，我的意思是，这就像迭代放大中的放大，你正在放大一个现有系统的能力，你已经训练它能够做更多的事情。

伊桑·佩雷斯(15:43):
例如，在我的一些工作中，我采用了标准的问答系统，它可以回答关于维基百科中某个段落的问题。他们非常擅长回答关于维基百科上单个段落的问题，但他们很难回答问题，或者不清楚如何将他们应用于答案在多个不同维基百科文章或不同段落中的问题。

伊桑·佩雷斯(16:08):
例如，乔治·华盛顿和亚伯拉罕·林肯谁出生得早？这些信息仅出现在维基百科的两篇文章中。我在工作中所做的是训练一个模型，首先为原始问题生成子问题，所以你会得到一些子问题，大意是，乔治·华盛顿什么时候出生？亚伯拉罕·林肯什么时候出生？然后，您可以使用预先训练的模型单独回答每个问题，然后将子问题的两个答案与子问题一起传递给另一个问答系统。然后它应该能够预测答案，基本上，我想是关于子问题的两个答案的争论。

Ethan Perez (16:55):
它使整个过程变得更加容易，然后我们表明它改善了当前流行的问答基准的结果。

Jeremie Harris (17:06):
你如何训练它识别子问题？我在努力思考这个问题……是不是只有一个带标签的训练集？你用的是这个吗？

伊桑·佩雷斯(17:13):
是的。我认为这是最简单的事情，但它们是一种奇怪的监督方式。得到这些问题和子问题对并不常见。是的，另一个简单的策略，如果你没有那么多的监督，我有一篇论文可以做到这一点，就是基本上利用这些大型语言模型，这些模型可以根据一些例子进行调整，从而进行推广。

Ethan Perez (17:41):
是的，所以我们基本上一直在使用 GPT-3，加上一些其他技巧，来生成子问题。您以一个问题和子问题对或几个子问题为条件，然后提示一个新问题。它可以很好地生成子问题。还有，在我的另一篇论文中，我们基本上只是想看看我们是否能在没有监督的情况下完全生成子问题。在那里，我们使用了一些来自未预见的机器翻译的疯狂方法来做到这一点。不过，如果你感兴趣的话，我可以更详细地介绍一下。但是有一种…这是你可以使用的不同方法的一个高层次。

耶雷米·哈里斯(18:22):
是的。不，我觉得非常有趣的是，你在看，有效地，用，我不知道术语是“一点点逻辑”还是一种你正在烘焙的逻辑结构来扩充 GPT-3。这让我想到了另一个问题，尤其是作为一个在这个领域工作的人，他有很多 Q & A 类型的问题。当 GPT3 问世时，你对它的印象如何？你对 lit 的出色表现有多惊讶？

伊森·佩雷斯(18:50):
我印象深刻。我想我印象最深的地方可能是使用模型来生成论点或建议，或者其他形式的长文本生成。这真的很好，尤其是大多数人问的那种问题，因为这方面可能有最多的训练数据。所以任何与政治或政治争论或经济，经济争论，护教学和宗教辩论有关的东西。它实际上只是做了一个合理的工作，我不知道，引用圣经经文或其他非常疯狂的事情；因为互联网上可能有很多关于这类事情的讨论。所以它比我见过的做这些事情的任何模型都要好。

伊桑·佩雷斯(19:46):
是的，我认为有些地方可能会失败，或者如果你想要非常精确的东西，如果正确的输出数量是一个，并且没有任何其他正确的输出，如果你想要一个问题的答案，你需要一个问题的准确答案，这仍然是…我的意思是，这不是真正使用监督。我认为这基本上是监督有帮助的地方，就是得到你想要的那种事情。

耶雷米·哈里斯(20:20):
你认为它背后有一个合理的世界模型吗？在多大程度上…因为我认为人们一直在争论的一个领域是，GPT-3 基本上是一种美化的记忆工具吗，就像你说的，“哦，我以前见过这个。我以前见过有人对圣经中的一节有这样的争论。我知道引用什么。诗句…”它实际上是在更高的抽象层次上连接点。你认为它落在光谱的哪个位置？

伊桑·佩雷斯(20:47):
嗯，这是个好问题。肯定是在做一些概括。我和一个朋友最近在玩它，他有点怀疑，我就想，“哦，那么是什么让你相信它在做某种推广呢？”他说，“嗯，它可能在网上看过很多食谱。也可能看到过关于独角兽的文章，但可能从未见过这两者在一起。”因为我不知道，这似乎不太可能。所以他只是[听不清 00:21:14]用一个…比如，“给我一个如何做独角兽眼汤的配方。”然后我们开始谈论我们如何需要红宝石，我们需要熔炼铁矿石，把它放进汤里，在上面撒上一些金粉作为装饰。

耶雷米·哈里斯(21:35):
哇。

伊桑·佩雷斯(21:35):
写得非常详细。是的，我不知道。这看起来像是一条鱼在寻找证据，证明它正在进行某种推广。我认为在某种程度上，似乎很难抓住那些没有真正做好工作的案例。

Jeremie Harris (21:53):
在此基础上，你是否预计随着 GPT3 的扩展，我的意思是，显然谷歌刚刚推出了他们的一万亿参数模型，我们可以预计在不久的将来会有更多的扩展。你认为扩大规模可能会使 GPT-3 达到你一直在探索的 IDA 战略，至少是最近探索的 IDA 战略被它所包含的程度吗；它本身就足够好，不需要这种增强，或者这种增强可以用来针对该模型的下一层失败？换句话说，这是 GP-3 加 IDA 的扩展，能让我们任意地走多远吗，我想这是我想问的。

伊桑·佩雷斯(22:35):
是的。我认为 IDA 的好处在于，它可以利用你所拥有的任何能力，得到一个可以做更多事情的模型。我认为……我不担心更大的语言模型包含了 IDA 的需求，因为无论你有什么大的语言模型，你总是可以通过编写这个深度问题分解过程来让它回答更多种类的问题。到目前为止，我们的模型越好，似乎就越容易从问题分解中获益；因为，是的，如果你有一个更好的语言模型，你就可以产生更好的子问题。

Ethan Perez (23:22):
来自 GPT-3 的问题分解比来自我们以前的序列到序列方法的问题分解做得更好。即使是在预训练的基础上，也比不使用序列的方法做得更好。这只是一个明显的梯度，从子问题中获得的好处有所提高。是的，所以我认为我们会看到这种问题分解带来的越来越大的影响。

Jeremie Harris (23:48):
这很有意思，因为我猜想在某个时候，随着这些语言模型变得越来越复杂，它们就像一个越来越复杂的世界模型一样，这个世界模型将包括问题分解的值，因此……基本上，我们可以说，GPT-N 可以同时优化问题分解和回答问题。换句话说，如果这一切都发生在同一个优化引擎的引擎盖下，你会从这种互动中受益？

伊桑·佩雷兹(24:21):
是的。我认为这似乎是正确的。只要你有你需要做的分解量的训练数据。我不知道，GPT-3 可能会在一些问题上接受训练，可能需要几个或两到四个子问题。所以它可能只是在做这个内部分解过程，并预测答案。谁知道呢？你甚至可以从内部权重中解码出子问题。

伊桑·佩雷斯(24:53):
但我认为棘手的是，你如何归纳出比人们通常在互联网上回答的问题更需要分解的问题。我认为这就是这个结构化过程有帮助的地方，因为你可以任意地将问题分解成子问题。

Jeremie Harris (25:13):
我喜欢它是因为它在玩这个……我有一个物理的心理模型，或者你可以把它称为物理、逻辑和机器学习作为一个连续体的两端。在物理学中，我们走进世界，进行一系列实验，试图找出似乎一直适用的潜在规则，比如不管发生什么，这些规则总是正确的。任何参照系中的光速都是一样的。重力也是如此等等。

杰瑞米·哈里斯(25:48):
逻辑法则有点类似于……好像你在这里假设了一条逻辑法则。问题分解定律你可以连贯地把复杂的命题分解成简单的命题，永远都是这样。这几乎就像你在试错，机器学习，让我在大象周围摸索，但实际上从来没有…就像，机器学习模型实际上不会提出规则。他们提出了预测模型，这些模型有一点点不同，也没有那么基础。

Jeremie Harris (26:18):
感觉就像你把两者结合在一起。你会说，好吧，你真的，真的很擅长建立这个世界的模型。这是一个非常灵活的模型，但是当它遇到没有被训练过的东西时，它就会失败。因此，我们认为，这个原则的补充，将是真实的，独立于上下文，给予它更大的影响。这是一个公平的框架或描述吗[听不清 00:26:41]？

伊桑·佩雷兹(26:41):
是的。不，我想差不多就是这样。是的，你做到了。

杰瑞米·哈里斯(26:45):
你认为有什么是艾达加 GPT-3，或者 GPT-N，我应该说，做不到的吗？IDA 能让我们越过超级智能 AGI 终点线吗，或者有其他东西，比如自我游戏或强化学习，必须与这些系统相结合吗？

伊桑·佩雷斯(27:06):
是的。我认为这是一个好问题，至少如果你想回答问题，而不是说，在世界上采取行动。然后，我认为一个语言模型加上问题分解会让你走得很远。我的一些不确定性是围绕…我发现的一个问题是，如果您错误地回答了一个子问题，那么这可能会作为一个错误传播到最终答案中。如果你回答“乔治·华盛顿是什么时候出生的”，就说是 1900 年。然后，你会错误地回答“他比亚伯拉罕·林肯早出生吗”这个问题。

Ethan Perez (27:56):
我认为我们需要深入思考如何进行问题分解，如何使用子问题的答案。我们可能只需要对任何不同的问题做许多不同的问题分解。我们也可能想直接问这个问题。也许维基百科上有一些文字说，“乔治·华盛顿出生在亚伯拉罕·林肯之前。”所以我们可能只想直接问这个问题。也许出生日期无法得知，所以我们想看看他们的死亡时间，也许如果他们死亡时间的差异足够大，那么我们可以得到一些估计。这将让我们对他们的出生日期做一些贝叶斯更新。

Ethan Perez (28:40):
我们可能希望将这些不同的问题分解结果集合在一起。另外，我认为其他重要的事情，是看回答子问题的模型在预测答案时的置信度。我发现的一件事是，当模型对自己的答案不太有信心时，整个过程很可能会以一个不正确的答案结束，我认为这是有道理的。

耶雷米·哈里斯(29:09):
有道理，是的。

伊桑·佩雷斯(29:11):
这是一种我们应该仔细调整的事情。它出现在我们建立的系统中，但这是我们应该小心的事情，哦，也许我们应该实际训练更好校准的模型，看看我们应该如何利用它们的信心来影响我们对整体预测的信心。这个系统有很多不同的部分，我认为确实需要仔细研究，以使整个过程正常进行。

耶雷米·哈里斯(29:38):
是的。毫无疑问，这是一个很高的赌注，尤其是就我认为的那些早期的问题而言，对吗？系统越早出错，整个树越早受到危害。那是…

伊桑·佩雷斯(29:53):
是的。

杰里米·哈里斯(29:54):
好的。这实际上把我们带到了，因为你提到了组装。你有一大堆不同的问题分解。问题分解一得出一个结论。问题分解二导致不同的结论等等。最后，你平均一下，或者你让他们都投票，然后你用这个来集合他们的预测，你会得到一些更可靠的结果。另一个角度呢，让这些系统，也许，经历某种辩论过程。我知道这是你一直在做的另一件事。

伊桑·佩雷斯(30:24):
是的。

哈里斯(30:25):
你能谈谈辩论及其在这方面的重要性吗？

伊桑·佩雷斯(30:28):
是的。我认为高层次的想法是相似的，那就是当我们有一个更困难或更复杂或不知何故不分布的问题时，我们需要以某种方式递归地分解它，以便达到这样一个点，即我们实际上可以从一个经过监督学习训练的模型中做出评估。

Ethan Perez (30:48):
辩论的方式是不同的，我将从强化学习的角度出发，讨论如何解决这个问题。假设你想问一个模式，我该不该吃 keto？给我一个答案，并解释一下为什么我会认为你给了我一个好的答案。如果你对那些解释没有那种监督，你可能想做的一件事就是让你的语言模型生成一个解释。然后，你就查一下解释，然后你看，哦，这个好像有道理吧？如果它看起来合理，那么 RL 类型的方法将奖励给出良好解释和答案的语言模型，如果它看起来不正确，则给予负面奖励。

伊桑·佩雷兹(31:39):
如果你认为答案很容易核实但很难找到，我认为这种事情很有意义。数学证明可能是一个例子，其中有一个非常复杂的推理过程，以生成整个证明。但是一旦我们有了证据，那么检查答案就容易多了，然后每个人都可以积极地奖励提出证据的模型。这种强化学习类型的方法似乎会让我们超越人类的能力，因为我们实际上并没有产生答案。我们让模型做一些昂贵的计算来得出答案，然后我们检查它们。

伊桑·佩雷斯(32:16):
问题在于它不安全，或者可能与我们的价值观不一致，因为我们基本上只是在优化一个模型，以生成令人信服的答案，或者看起来正确的答案。但是

耶雷米·哈里斯(32:27):
我们认为有说服力的事情。

伊桑·佩雷斯(32:30):
是的。是啊，没错。我认为这就是辩论的动机所在，我们有一个模型来回答一个问题。然后我们可以让同一个模型带给我们对这个问题的答案的思考，我们可能会错过，这是反对这个答案的非常有说服力的案例。所以也许它提供了另一个答案，所以现在我们既有了最初的答案，也有了相反的答案。对于一个好的模型来说，这些可能是这个问题的两个最佳答案。我们可能会想，好吧，现在我们知道的更多了，也许是一些我们可能没有的缺失信息。

伊桑·佩雷斯(33:17):
辩论，以类似的方式，是重复的放大。你可以重复整个过程，然后你可以说，好的，给定我这个答案和反驳的答案，我可以对那个回答产生一个反驳的论点，一个反驳的论点，等等，等等，直到基本上，我们有足够的信息让一个人以相当高的信心预测正确的答案是什么。然后我们可以用它来…既然我们对这些数据有可靠的人类判断，那么我们可以训练可靠的模型来做出同样的预测，只是使用监督学习。

Jeremie Harris (33:59):
这里的希望是让两个人工智能系统相互辩论，在这个过程中，它们最终以人类可理解的方式揭示彼此论点的优缺点。

伊桑·佩雷兹(34:09):
是的。

耶雷米·哈里斯(34:10):
这样我们就可以走了，哦，这个人工智能是恶意的，它试图愚弄我来帮助它实现统治世界的计划，而这个人工智能正在做正确的事情。当我看到这场辩论结束时，它的答案实际上是有道理的。这真的有用吗？我想我会想象的一件事，或者我在这里会想象的一个问题是，当这两个争论的系统大大超过人类的推理能力时，你最终会处于一种脱节的情况，就像你说的，听起来对人类有说服力的东西和实际逻辑会支配的东西之间。这种逻辑变得非常复杂，看起来欺骗一个人会变得非常非常容易，而传达真实的论点会变得非常困难。

伊桑·佩雷兹(35:04):
是的。我认为这是这种方法的主要不确定性。你可能会认为，随着运行这种辩论的模型变得更好，那么论点和反论点都会变得更好，以一种有益的方式，因为你更有可能抓住论点中的某些领域，但是是的。看起来确实有…我的意思是，有一些有趣的实验，比如 OpenAI 进行的人体实验，他们让了解物理的聪明人参与辩论过程，并试图以一种方式…非物理的，不太物理的人尝试判断辩论。对我来说，评估这些争论真的非常非常困难。

伊桑·佩雷斯(35:54):
所以我认为，这是另一件需要调整的事情，以便找出我们如何能够最容易地评判辩论，这样我们仍然可以有信心对整个过程做出可靠的评判。

杰瑞米·哈里斯(36:11):
对，因为我不知道，对我来说，这看起来很天真，这就像是一个维度的诅咒，问题领域变得越复杂，他们的推理就越复杂，高维度就越高，这些人工智能就有更多的自由。看起来这些自由度，就像是你需要楔入欺骗的空间。随着问题复杂性的增加，空间似乎也在增长。

Jeremie Harris (36:46):
但是，是的，希望有一些潜在的原则，所以我们可以实际上揭示[听不清 00:36:50]规模多一点。

Ethan Perez (36:52):
你可能需要将这些方法结合起来。我不知道。训练一些 IDA 类型的系统，然后因为它完成了这种分解，然后它能回答什么样的问题，也能判断什么样的辩论，这超出了人类的能力。然后，你可能想用这个系统作为这个辩论过程的裁判。它可能比直接使用人类注释者作为评判者做得更好。

伊桑·佩雷斯(37:22):
对于辩论，我认为动机之一是这可能是一个很好的训练信号，如果你可以训练一个模型来准确判断辩论的结果。然后，你可以把它作为某种自我游戏的奖励信号，甚至是 AlphaZero 类型的系统，其中模型优化奖励，并更好地产生这些论点。然后随着他们变得更好，你也可能认为他们可能是整个系统更好的法官。这些拼图有不同的组合，我们可以试着把它们拼在一起，但这是一个悬而未决的问题，关于怎样做才能得到可靠的判断。

Jeremie Harris (38:04):
我也很好奇的一件事，这不仅影响了研究人员选择的工作策略，也影响了他们对安全的态度以及他们认为必须要研究的校准和人工智能安全技术。这些技术问题和关于人工智能时间线的想法之间有着紧密的联系。当你认为一个 AGI 人类水平的一般智能，或者我们可以用那些术语来指的东西，将会出现；你有，我不知道，你对此有什么看法？如果是，这是否告知了您的立场，即是否要在能力、一致性或其他方面开展工作？

伊桑·佩雷斯(38:41):
是的。对此我做了一个预测。有一根很好的线。我可能要晚些时候发给你，但这个研究小组 AUT 有一个非常好的帖子，他们收集了许多不同人的人工智能时间轴。我认为从这点来看…

杰里米·哈里斯(38:53):
我会发布一个链接。我确实看到了你的非法情节，所以每个人都可以查看播客附带的博客帖子，你将能够看到伊森的非法帖子，因为我认为它有点耐人寻味。

伊森·佩雷斯(39:02):
好的。很好，是的。我认为这在很大程度上取决于你对 AGI 的定义。我想我在考虑，它既需要良好的语言建模能力或语言能力，良好的视觉能力，又能够在世界上采取行动。我认为，在我的预测中，我把它分解成两种情况。有一种情况是，我们通过组合我们目前拥有的现有组件并利用我们拥有的计算对它们进行扩展来获得一般智能。然后，我的分布还有另一个组成部分，那就是在另一个场景不成立的情况下，只是在下一个世纪或几个世纪之前，嗯，不确定它什么时候会发生。所以我会有一些时间线上的衰减分布。

伊桑·佩雷斯(39:57):
在第二种情况下，我们何时能获得总体情报似乎还不确定，因为这很可能是一些非常困难的突破……我们已经在这个问题上努力了很长时间，所以可能需要很长时间才能获得我们需要的必要突破。但在另一种情况下，似乎很有可能在短期内，如果我们得到更大的 GPT 式模型，如果我们有足够的计算能力来获得良好的性能；那么我认为我们得到相当好的语言模型似乎是合理的。然后我们只需要将它们与视觉组件结合起来，我认为这是 CLIP 和 DALL-E 以及其他一些模型的发展方向。然后，我们只需要给他们一些在世界上采取行动的能力，或者能够像我们希望的那样训练他们。

耶雷米·哈里斯(40:56):
某个身体。

伊桑·佩雷斯(40:58):
是的。也许…我不知道我是否被束缚在身体上，但是至少模型应该能够，例如，经营一家公司，选择优化一些目标的行动。除了获得和理解视觉和语言，这似乎还需要更多的工作。

Jeremie Harris (41:21):
我认为你提出这个问题的方式是每个人都在他们的脑海中含蓄地提出了这个问题，但是从来没有人像你这样明确地提出来，或者就我在评论中看到的那样。核心问题是，嘿，我们基本上解决了所有需要解决的概念性问题了吗？基本上，我们是否已经拥有了所有这些成分，强化学习，深度学习，我们只是将它们混合在一起，并添加少量的香料，然后我们就有了 AGI？或者有一些基本的东西，是否是一些奇怪的量子效应，或者一些我们必须弄清楚的东西，使 AGI 实际工作？

Jeremie Harris (41:53):
如果我没记错的话，您实际上说过，您认为目前我们有四分之一的机会拥有实现起飞所需的一切。对吗？

伊桑·佩雷斯(42:03):
是的。我想那是对的。我的意思是，这很大程度上取决于你对 AGI 的定义。我认为语言模型本身非常强大，可能已经足够好了-

杰里米·哈里斯(42:17):
[听不清 00:42:17]

伊桑·佩雷兹(42:17):
…对世界有巨大的影响。我在那篇文章中说的是四分之一，但我认为可能大多数人在这个问题上比我低。有些人会更高。

哈里斯(42:29):
哦，有意思。所以你认为大多数人会认为可能有一些缺失的概念成分？

伊桑·佩雷兹(42:37):
是的。我认为这可能是一种被 OpenAI 的一些扩展工作所削弱的观点，但一般来说，大多数研究人员似乎都是这样的，“哦，我们确实需要一些新的想法来处理事情。”

Jeremie Harris (42:55):
当我看着你的图表时，给我留下深刻印象的一件事是，你已经做到了，基本上，我们在未来 10 年左右的某个时候达到 AGI 峰值的可能性，基本上是你的大幅提升，这与我们可能已经拥有一切，我们需要的只是规模。所以在接下来的 10 年内，我们将能够扩大我们的规模。

杰里米·哈里斯(43:13):
然后，当然，你有他的高度不确定性的大时期，你喜欢，“我不知道。如果在这段时间内没有实现，扩展也不起作用，那么谁知道我们什么时候会有大想法呢？”这引起了关注吗？你担心未来 10 年我们可能会袭击 AGI 吗？你认为我们从安全的角度，从结盟的角度，从哲学的角度准备好了吗？

伊桑·佩雷斯(43:38):
是啊。这个问题问得好。我认为，在某种程度上，如果没有[听不清 00:43:47]模型，似乎很难研究这些安全方法。

哈里斯(43:48):
能力？

伊桑·佩雷斯(43:48):
是的。在我们…几年前，很难对一个问题提出一个好的子问题。是的，所以我的一部分感觉是，哦，随着我们越来越接近非常强大的语言模型，这些方法将变得越来越有用。是的，但是看起来，我不知道，也许我们需要很长一段时间来接近拥有强大的语言模型，并且有很多时间来开发我们的方法。然后，哦，然后我们会为此做好准备，我们会有所有这些超越人类能力的方法，以一种适当的方式推广。

伊桑·佩雷斯(44:32):
是的，也许更多的是轨迹的形状…

哈里斯(44:35):
有意思。

伊桑·佩雷斯(44:36):
这对我很重要。

杰里米·哈里斯(44:40):
这是否意味着我们会知道？因为现在我认为我们正在扩展世界上最大的机器学习模型。我认为每年都有 10 倍的增长。对我来说，这似乎表明，当我们找出解决方案时，我们很可能不会在人类智力水平之下哼唱，而是突破那个门槛，然后创造出比我们能处理的更大的东西。这是一个看似合理的担忧，还是你认为其他事情可以消除这种担忧？

伊桑·佩雷斯(45:13):
嗯，我认为它的计算能力非常有限。在这种情况下，我们从我们当前的方法集中得到的方法通常是智能的，看起来未来的方向是，我们需要大量的扩展。在这种情况下，在你得到的模型的质量和你使用它的计算量之间会有一些权衡。我认为这确实限制了……是的，我想就智力而言，它确实可以快速增长。

伊森·佩雷兹(45:49):
是的，看起来……我的意思是，我们确实需要更高数量级的计算能力来达到接近人类水平的智能。在这一点上，这就像是，你在用一大笔预算，谷歌研发预算的很大一部分来训练模型。这是一个问题，嗯，还有多少钱可以花在这些上面？这似乎是一个需要一段时间的过程，至少需要一些时间，甚至可能需要某种政府项目来筹集比谷歌研发预算更多的资金来扩展模型。但似乎很难就这么过去了。

Ethan Perez (46:28):
我们也有这些非常可预测的幂律曲线，随着计算的增加，语言建模的损失将如何下降？也就是说……就我们从更多计算的训练中获得的语言建模的改善程度而言，我们获得的收益正在放缓。我认为这些因素都向我暗示，事情会有所放缓。

Jeremie Harris (46:52):
嗯，我想如果他们这样做了，这将表明像 IDA prolyl 这样的策略将变得更加有价值，因为我们需要这里或那里的一些额外推动。IDA 是否有必要跨越 AGI 门槛，或者是否有其他可信的技术……IDA 本身是否是一种可信的方式，换句话说，让我们从基于人类水平数据训练的系统中获得超人水平的性能？

伊桑·佩雷斯(47:25):
是的。好的，所以我脑海中的描述是这样的，一个非常好的语言模型将捕捉到人类智力和能力的全部分布。它知道人类智力的最高端是什么样的。例如，我提出 GPT-3 就好像是斯坦福大学两个教员之间的对话。然后对话是非常独特的，这是他们引用不同的名人和非常高质量，格式良好的英语之类的东西。

Jeremie Harris (48:01):
非常复杂的对话。

伊森·佩雷兹(48:02):
对，没错。那里，我想，那里很清楚。我的意思是，那似乎…我的意思是，就质量而言，它高于人类智力的平均值。所以我认为这似乎是一个合理的地方…我的意思是，也许你可能已经认为是超人了，这取决于你的想法-

耶雷米·哈里斯(48:22):
对。没错。它选择了人类人口的前百分之一。那是超人的东西吗？我是说，也许，是的。是啊。

伊桑·佩雷兹(48:32):
是的。我是说，那已经很有用了。然后我想，也许如果你使用某种基于强化的方法，就像我描述的那样，你可以超越它。但我认为你会开始看到我描述的一些失败，模型产生的答案令人信服，但不一定正确。在那里，我认为你会陷入危险，嗯，人们可能会使用这些答案，因为它们可能是正确的，但它们也是经过优化的，只是为了说服你-

杰里米·哈里斯(49:04):
【听不清 00:49:04】。

伊桑·佩雷斯(49:04):
因此，你可能会根据错误的信息做出重要的决定。我认为，当我们试图超越人类能力的水平时，我们可能会想要…我们可能基本上需要使用蒸馏和放大或辩论的方法，以一种我们有信心会奏效的方式让我们超越人类能力的这一体系。

Jeremie Harris (49:33):
你对……总体上持乐观态度吗？因为我们已经讨论了 IDA 作为一种增强这些系统的潜力，以获得超人水平的性能。看起来我们确实有策略，至少在原则上，看起来他们有很好的机会让我们拥有超级智慧。不管这些策略中的哪一个有效，还是其他什么有效，我们似乎很有可能最终会实现目标。你对我们控制这些系统的能力持乐观态度吗？也许明智地使用它们是另外一个问题。而是我们控制它们的能力，以确保我们不会把我们宁愿放回瓶子里的精灵从瓶子里放出来。

伊桑·佩雷斯(50:12):
是的。我认为问答系统的好处之一就是我们可以决定如何处理这些信息。这并不是说我们让系统自己自主行动，我认为这是很多困难的来源。我的意思是，正确回答这个问题仍然有困难，但我认为如果我们真的能解决这个问题，我认为更简单的问题，只是让模型概括过去人类的能力，只是在预测的领域，回答问题；然后，我认为我们可以使用这些方法中的一些来帮助我们在更普遍的情况下，我们希望我们的系统只是以一种不受人类监督的方式自动采取行动。

Ethan Perez (51:03):
例如，我认为建立这种联系的一种方式是使用这个经过 IDA 或辩论训练的问答系统，为运行制造业的系统提供奖励，如亚马逊制造或在科技公司做出 CEO 级别的决策等。我认为，如果我们有某种超人的问答系统，可能会更容易获得对这些采取长期行动的系统的准确评估。

耶雷米·哈里斯(51:41):
有意思。好吧，我们希望问答系统将会成为解决方案的重要组成部分。非常感谢你的时间和想法，伊森。你有没有一个个人网站，你想让人们实际上-

伊桑·佩雷斯(51:53):
是的。

杰里米·哈里斯(51:53):
……看看他们是否有兴趣关注你的工作？

伊桑·佩雷斯(51:55):
是的。只是 ethanperez.net。

哈利斯(51:57):
太棒了。点网。太棒了。我喜欢。非常，是什么？我不想说是潮人，但它有很好的美感，是的。太好了。

伊森·佩雷斯(52:07):
很好。这是唯一可用的。[听不清 00:52:09]

杰里米·哈里斯(52:09):
不错。很好。我想，欢迎来到 2021 年。伊森，非常感谢。我真的很感激。

伊桑·佩雷斯(52:16):
是的。谢谢你邀请我。