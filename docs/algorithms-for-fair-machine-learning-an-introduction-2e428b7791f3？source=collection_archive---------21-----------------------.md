# 公平机器学习算法:导论

> 原文：<https://towardsdatascience.com/algorithms-for-fair-machine-learning-an-introduction-2e428b7791f3?source=collection_archive---------21----------------------->

## [公平和偏见](https://towardsdatascience.com/tagged/fairness-and-bias)

## 三种类型的偏差缓解算法

![](img/2b7f61806410bf6b4457822a06b646fe.png)

由[埃琳娜·莫日维洛](https://unsplash.com/@miracleday?utm_source=medium&utm_medium=referral)在 [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral) 上拍摄的照片

负责任的 ML 模型开发的一个关键组成部分是确保我们的模型不会不公平地伤害我们人口中的任何亚群体。第一步是识别和量化模型中的任何潜在偏差，人们已经提出了许多不同的群体公平性定义。在亚瑟公司，我们感兴趣的不仅仅是识别偏见，而是努力让模型更加公平。在这篇文章中，我们简要描述了偏差缓解技术家族:改善不公平模型的方法。

虽然这是一个活跃的研究领域，但当前的缓解技术针对模型开发生命周期的特定部分:

*   训练数据的预处理或调整。
*   处理中，或算法特别是为了公平。
*   后处理，或调整模型的输出。

正确的解决方案可能取决于用例、行业和模型部署；在这篇文章中，我们概述了几种公平 ML 的方法。

# 为什么我们会观察到不公平的量词？

通常，我们会观察到不公平的分类器，因为用于训练模型的数据在某种程度上是“有偏见的”。正式描述“有偏数据”与“有偏模型”的关系是一项稍微复杂一些的任务。

学习模型中偏差的一个常见原因是数据中子群之间的不平衡。根据定义，如果我们有一个在训练数据中代表少数群体的子群体，那么这意味着与多数群体相比，我们对他们的观察更少。当分类器被训练时，它在整个数据集上优化损失函数。如果主要类别确实占主导地位，那么在训练数据上实现高总体准确性的最佳方式可能是在多数群体上尽可能准确，同时在少数群体上招致[错误](http://proceedings.mlr.press/v81/buolamwini18a/buolamwini18a.pdf)。

因此，如果多数群体和少数群体在属性和与目标变量的关系上有任何差异，模型很可能主要遵循主要数据的模式，并可能忽略少数群体的贡献。这可能意味着这个模型对于大多数群体来说是相当准确的，但是对于[较小的群体](http://proceedings.mlr.press/v119/khani20a.html)来说[就不那么准确了。](https://arxiv.org/abs/1805.12002)

在其他情况下，偏差的产生是因为模型从数据集中编码的系统偏差中学习。。在“基础事实”结果与社会系统交叉的领域，如贷款履行、雇佣、刑事司法、住房等。数据可以对该系统内存在的歧视和偏见进行编码。例如，根据上世纪 60 年代的招聘数据训练的模型可能会表明，女性最适合做秘书，而不是高管。无论你选择哪种技术来改善这些选项中的偏差，都应该密切关注历史背景，通过它可以更好地理解数据收集实践和社会影响。

# 预处理偏差缓解

减轻偏差的预处理技术往往都与数据有关。如前一节所述，训练数据的特定特征可能直接导致学习模型的有问题的性能。由于这个原因，许多预处理技术集中于修改训练集以克服数据集版本的不平衡。

这可以通过多种方式实现，包括对数据行进行重新采样、对数据行进行重新加权、跨组翻转类标签以及省略敏感变量或代理。其他技术考虑学习实现期望的公平性约束的直接[修改](https://arxiv.org/pdf/1412.3756.pdf)和[变换函数](https://papers.nips.cc/paper/6988-optimized-pre-processing-for-discrimination-prevention.pdf)。

在所有情况下，策略都是更改基础训练数据，然后使用任何所需的分类算法继续进行训练。通过以这些特定的方式修改训练数据，所学习的分类器的输出将更少偏差。

# 处理中偏差缓解

利用处理中技术，我们希望创建一个明确知道我们的公平目标的分类器。也就是说，在训练分类器时，仅仅优化训练数据的准确性是不够的。相反，我们修改损失函数来同时考虑我们的两个目标:我们的模型应该既准确又公平。

这种修改可以通过多种方式实现，比如使用[对抗性](https://arxiv.org/pdf/1809.04663.pdf) [技术](https://arxiv.org/pdf/1801.07593.pdf)，确保底层[表示是公平的](http://proceedings.mlr.press/v28/zemel13.html)，或者通过框定[约束和规则化](https://arxiv.org/abs/1806.06055)。在每种情况下，目标都是底层分类器直接考虑公平性。

因此，与对公平性一无所知的分类器相比，经过训练的分类器的结果将更少有偏差。

# 后处理偏差缓解

最后，有一系列技术旨在只调整模型的输出，而不改变底层分类器和数据。这里的好处在于它的简单性——在使用后处理方法时，我们允许模型开发团队使用他们希望的任何建模算法，他们不需要修改他们的算法或重新训练新的模型来使其更加公平。

相反，后处理方法的核心思想是调整不公平模型的输出，使最终输出变得公平。例如，该领域的早期工作集中在以[组特定的方式](https://arxiv.org/pdf/1709.02012.pdf)修改结果和[阈值](https://arxiv.org/abs/1610.02413)。

假设我们建立一个分类模型来帮助信用风险决策。经过大量的超参数调整，我们得到了一个准确且概括良好的模型，但我们注意到它倾向于支持年长的贷款申请人而不是年轻的申请人。

利用后处理技术，我们将保持分类器不变，但是调整结果，使得总体接受率更加公平。我们会挑选一个公平的定义(比如说，[人口统计均等](https://www.ece.ubc.ca/~mjulia/publications/Fairness_Definitions_Explained_2018.pdf))，然后调整不同组之间的待遇，以使最终结果符合预期。这意味着我们可能有特定于组的阈值，而不是用于分类器的单个阈值。

值得注意的是，在这种情况下，围绕偏见缓解仍有许多法律模糊之处。对于法院将如何处理算法歧视有如此多的未知，许多组织严重依赖他们的法律团队如何驾驭这种复杂性！

许多[后处理技术](https://arxiv.org/abs/2004.03424)都有这个共同的基本结构:它们不去管分类器和数据，只是以一种依赖于组的方式调整结果。虽然二元分类在过去一直是焦点，但最近的工作试图将这些想法扩展到[回归](https://arxiv.org/abs/2006.07286)模型[以及](https://arxiv.org/pdf/1905.12843.pdf)。总体框架可以有效地实现 ML 系统中的公平性，尽管在某些用例中，区别对待不同的组可能是一个令人不舒服的提议，甚至是非法的。

# 公平和准确

当我们着手部署更加公平的 ML 模型时，我们必须承认这种公平不是免费的；事实上，在许多情况下，它可能会与模型准确性相冲突。

考虑一个极端:一个尽可能精确的模型(相对于现有的基本事实)可能是相当不公平的，并且歧视至少一个亚群。

考虑另一个极端:一个完全公平且对所有人群都公平的模型，这个模型肯定不如不考虑公平作为约束的模型准确。(尽管[一些](https://arxiv.org/abs/1912.01094) [最近的工作](http://papers.nips.cc/paper/9082-unlocking-fairness-a-trade-off-revisited)表明折衷不一定总是发生，但是“公平算法”在现实世界中部署时的行为并不总是与理论上证明的结果相匹配；因此，理解公平性和准确性之间的关系对我们选择使用的模型充满信心至关重要。)

在这两个极端之间存在着一个平衡准确性和公平性的可能模型的大家族。这组模型在准确性与公平性之间形成了一个帕累托边界(有效边界)。

下图来自 2019 年的一篇调查论文，描述了许多流行的公平 ML 算法的量化性能，说明了这种权衡。形象地说:我们想分享一个很好的图表，展示不同影响之间的权衡，衡量公平性，x 轴是不同影响，衡量公平性，而 y 轴是准确性。(整篇论文值得一读；它很好地介绍了公平 ML 模型性能的许多常见考虑因素。)作为实践者和利益相关者，我们必须面对关于这种权衡的问题:对于每一个用例，我们必须权衡通过不公平造成的潜在危害的成本和通过降低准确性造成的潜在危害的成本。

这些都是具有挑战性的问题，没有唯一正确的答案。相反，ML 从业者必须与利益相关者合作，如商业领袖、人文专家、合规和法律团队，并制定一个如何最好地对待你的人口的计划。更广泛地说，公平只是负责任地部署机器学习所需的一小部分:这篇文章应该是对话的开始，而不是结束。