<html>
<head>
<title>7 Steps to Design a Basic Neural Network (part 2 of 2)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">设计基本神经网络的7个步骤(第2部分，共2部分)</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/7-steps-to-design-a-basic-neural-network-part-2-of-2-792cb7273c3?source=collection_archive---------39-----------------------#2021-05-04">https://towardsdatascience.com/7-steps-to-design-a-basic-neural-network-part-2-of-2-792cb7273c3?source=collection_archive---------39-----------------------#2021-05-04</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="e83d" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">一个数学不太密集的，一步一步的指南，从头开始建立一个隐层神经网络</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/8e339c0b21787db185379649de1360fb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4ibkvHEbKAx3CCjBgNG8Hw.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图片由Lindsay Henwood在Unsplash上提供</p></figure><p id="c1ab" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在本部分的第1部分(共2部分)中，我们看到了使用传统预测模型(如逻辑回归)来正确分类嘈杂数据集中的两种颜色的局限性。然后，我们建立自己的神经网络结构，初始化参数，并计算前向传播激活函数。</p><p id="c5c4" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在第2部分中，我们将完成神经网络模型的构建，以更好地对原始数据集中的色点进行分类。具体来说，我们将回顾成本函数、反向传播、参数更新和最终的模型装配/预测。</p><h2 id="8f53" class="ls lt iq bd lu lv lw dn lx ly lz dp ma le mb mc md li me mf mg lm mh mi mj mk bi translated">第四步:估算成本</h2><p id="3937" class="pw-post-body-paragraph kv kw iq kx b ky ml jr la lb mm ju ld le mn lg lh li mo lk ll lm mp lo lp lq ij bi translated">我们之前讨论了在我们的神经网络模型中最小化成本的重要性。事实证明，成本函数从我们的正向传播<strong class="kx ir"> </strong>和原始的<strong class="kx ir"> Y </strong>输出中获取我们已经知道的变量——<strong class="kx ir">a[2]</strong>。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mq"><img src="../Images/b0eac2b4653f0bc5064a405a491972e7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1054/format:webp/1*D1g5P5NltPpE5qSm_UW6yg.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">带有变量A2和Y的成本函数(图片由作者提供)</p></figure><p id="1205" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">回想一下<strong class="kx ir">a【2】</strong>是输出层最后的激活函数输出。上面的<strong class="kx ir"> m </strong>变量仅仅是我们原始数据集中的观察数量。下面是向前传播的视觉示意图，供您回忆。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mr"><img src="../Images/7558527f6e150a405b0b3df0f5fa7541.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*97F5I7pWFuc_eKY2mhe8vQ.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">1-隐层神经网络的z和激活函数(图片由作者提供)</p></figure><p id="3c07" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">因此，将这个成本等式转换为python，我们可以使用np.dot来计算乘积，使用np.log来计算对数。让我们现在做那件事。</p><pre class="kg kh ki kj gt ms mt mu mv aw mw bi"><span id="4500" class="ls lt iq mt b gy mx my l mz na">def compute_cost(A2, Y, parameters):<br/>    """Computes cost using the cost function, where logprobs is actually the loss.<br/>    Arguments: Takes A2 output, Y and parameters.<br/>    Returns: cost"""<br/>    <br/>    # m is the number of total observations<br/>    m = Y.shape[1]<br/>    <br/>    # logprobs is the loss, which we can use to compute the cost<br/>    logprobs = np.multiply(np.log(A2), Y) + np.multiply((1-Y),np.log(1-A2))<br/>    cost = (1./m) * -np.sum(logprobs) <br/>    <br/>    cost = float(np.squeeze(cost))  <br/>    assert(isinstance(cost, float))<br/>    <br/>    return cost</span></pre><p id="8cab" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">如果我们现在跑:</p><pre class="kg kh ki kj gt ms mt mu mv aw mw bi"><span id="6d02" class="ls lt iq mt b gy mx my l mz na">cost(A2, Y, parameters)</span></pre><p id="1fe9" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我们返回0.693的成本，这不是很大，但只是我们在梯度下降的初始进入点(0次迭代后)的成本。反向传播将通过多次迭代来降低成本。让我们看看怎么做。</p><h2 id="852f" class="ls lt iq bd lu lv lw dn lx ly lz dp ma le mb mc md li me mf mg lm mh mi mj mk bi translated">步骤5:反向传播</h2><p id="3db4" class="pw-post-body-paragraph kv kw iq kx b ky ml jr la lb mm ju ld le mn lg lh li mo lk ll lm mp lo lp lq ij bi translated">反向传播在数学上非常密集。有很多很棒的文章回顾了偏导数微积分。但从概念上讲，我们通过网络反向传播关于我们误差的信息，以调整我们的初始参数和微调我们的权重。经过多次迭代，我们每次都逐渐降低成本。这一过程优化了我们的模型，使其更具普遍性。</p><p id="46b4" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">当我们在各层中向后移动时，首先计算最后一层权重的梯度，最后计算第一层权重。来自一层的部分计算(即偏导数)被传递到下一层。这允许错误信息反向流动。</p><p id="a907" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在前向传播中我们计算了<strong class="kx ir"> z1 </strong>、<strong class="kx ir"> w1 </strong>、<strong class="kx ir"> b1 </strong>和<strong class="kx ir"> z2 </strong>、<strong class="kx ir"> w2 </strong>和<strong class="kx ir"> b2 </strong>，在后向传播中我们将计算<strong class="kx ir"> dz1 </strong>、<strong class="kx ir"> dw1 </strong>、<strong class="kx ir"> db1 </strong>和<strong class="kx ir"> dz2 </strong>、<strong class="kx ir"> dw2 </strong>和<strong class="kx ir">DB2<strong class="kx ir"/></strong></p><pre class="kg kh ki kj gt ms mt mu mv aw mw bi"><span id="0a53" class="ls lt iq mt b gy mx my l mz na">def backward_propagation(parameters, cache, X, Y):<br/>    """Computes backward propagation.<br/>    <br/>    Arguments: Takes parameters, cache (z1, a1, z2, a2), X and Y.<br/>    Recall:<br/>    Parameters - w1, b1, w2, b2<br/>    X -- input data of shape (2, number of examples)<br/>    Y -- "true" labels vector of shape (1, number of examples)<br/>    <br/>    Returns:<br/>    grads -- dictionary containing your gradients with respect to different parameters.<br/>    """<br/>    m = X.shape[1]<br/>    <br/>    # First, retrieve W1 and W2 from the dictionary "parameters".<br/>    W1 = parameters["W1"]<br/>    W2 = parameters["W2"]<br/>        <br/>    # Retrieve also A1 and A2 from dictionary "cache".<br/>    A1 = cache["A1"]<br/>    A2 = cache["A2"]<br/>    <br/>    # Backward propagation: calculate dW1, db1, dW2, db2. <br/>    dZ2 = A2 - Y<br/>    dW2 = (1./m) * np.dot(dZ2, A1.T)<br/>    db2 = (1./m) * np.sum(dZ2, axis=1, keepdims=True)<br/>    dZ1 = np.dot(W2.T, dZ2) * (1 - np.power(A1, 2))<br/>    dW1 = (1./m) * np.dot(dZ1, X.T)<br/>    db1 = (1./m) *(np.sum(dZ1,axis = 1,keepdims = True))<br/>    <br/>    grads = {"dW1": dW1,<br/>             "db1": db1,<br/>             "dW2": dW2,<br/>             "db2": db2}<br/>    <br/>    return grads</span></pre><p id="37af" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我们刚刚创建的grad字典包含导数参数，我们将在下一步中使用学习率更新这些参数。</p><p id="958c" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx ir">第六步:更新参数</strong></p><p id="4a7d" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">为了更新我们的原始参数(<strong class="kx ir"> w1 </strong>、<strong class="kx ir"> b1 </strong>、<strong class="kx ir"> w2 </strong>、<strong class="kx ir"> b2 </strong>)，我们计算学习率和我们先前在反向传播中计算的每个导数参数的乘积。然后我们从原始参数中减去这些值。这显示在下面的python代码中。</p><pre class="kg kh ki kj gt ms mt mu mv aw mw bi"><span id="2c8f" class="ls lt iq mt b gy mx my l mz na">def update_parameters(parameters, grads, learning_rate = 1.5):<br/>    """Computes updates to our original parameters using a predefined learning rate.<br/>    Arugments: Takes parameters, grads, learning_rate (you can set)<br/>    Outputs: New parameters """<br/>    <br/>    # utilize pre-existing parameters for weights and bias vectors<br/>    W1 = parameters['W1']<br/>    b1 = parameters['b1']<br/>    W2 = parameters['W2']<br/>    b2 = parameters['b2']<br/>    <br/>    # Retrieve each parameter from the dictionary "parameters"<br/>    dW1 = grads['dW1']<br/>    db1 = grads['db1']<br/>    dW2 = grads['dW2']<br/>    db2 = grads['db2']<br/>    <br/>    # update rule for each parameter<br/>    W1 = W1 - learning_rate * dW1<br/>    b1 = b1 - learning_rate * db1<br/>    W2 = W2 - learning_rate * dW2<br/>    b2 = b2 - learning_rate * db2<br/>    <br/>    parameters = {'W1': W1,<br/>                  'b1': b1,<br/>                  'W2': W2,<br/>                  'b2': b2}<br/>    <br/>    return parameters</span></pre><p id="91e3" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">看看更新后的参数:</p><pre class="kg kh ki kj gt ms mt mu mv aw mw bi"><span id="5478" class="ls lt iq mt b gy mx my l mz na">parameters = update_parameters(parameters, grads)</span><span id="4b1f" class="ls lt iq mt b gy nb my l mz na">print("W1 = " + str(parameters["W1"]))<br/>print("b1 = " + str(parameters["b1"]))<br/>print("W2 = " + str(parameters["W2"]))<br/>print("b2 = " + str(parameters["b2"]))</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nc"><img src="../Images/e23538e1b78487965968c702826ff696.png" data-original-src="https://miro.medium.com/v2/resize:fit:892/format:webp/1*6CbXHoIVmL28zMDxdiNDgQ.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">原始参数现已更新(图片由作者提供)</p></figure><h2 id="1eea" class="ls lt iq bd lu lv lw dn lx ly lz dp ma le mb mc md li me mf mg lm mh mi mj mk bi translated">步骤7:模型装配/预测</h2><p id="fae4" class="pw-post-body-paragraph kv kw iq kx b ky ml jr la lb mm ju ld le mn lg lh li mo lk ll lm mp lo lp lq ij bi translated">在最后一步中，我们将组合并运行我们之前的所有函数10，000次迭代，试图降低我们的总成本，并以最低的成本返回最佳参数。</p><pre class="kg kh ki kj gt ms mt mu mv aw mw bi"><span id="4add" class="ls lt iq mt b gy mx my l mz na">def model_nn(X, Y, n_h, num_iterations = 10000, print_cost = True):<br/>    <br/>    np.random.seed(5)<br/>    <br/>    # define input and output layer shape<br/>    n_x = layer_sizes(X, Y)[0]<br/>    n_y = layer_sizes(X, Y)[2]<br/>    <br/>    # define the parameters<br/>    parameters = initialize_parameters(n_x, n_h, n_y)<br/>    W1 = parameters["W1"]<br/>    b1 = parameters["b1"]<br/>    W2 = parameters["W2"]<br/>    b2 = parameters["b2"]<br/>    <br/>    # compute gradient descent for specified number of iterations<br/>    <br/>    for i in range(0, num_iterations):<br/>        <br/>        # forward propagation<br/>        A2, cache = forward_propagation(X, parameters)<br/>        <br/>        # cost<br/>        cost = compute_cost(A2, Y, parameters)<br/>        <br/>        # backpropagation<br/>        grads = backward_propagation(parameters, cache, X, Y)<br/> <br/>        # Gradient descent parameter update. Inputs: "parameters, grads". Outputs: "parameters".<br/>        parameters = update_parameters(parameters, grads)<br/>        <br/>        # Print the cost every 1000 iterations<br/>        if print_cost and i % 1000 == 0:<br/>            print ("Cost after iteration %i: %f" %(i, cost))<br/>        <br/>    return parameters</span></pre><p id="9d12" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">当我们运行这段代码时，我们可以看到成本随着每1，000次迭代而下降，直到大约为0.094，这比我们最初的0.693成本有了很大的提高。</p><pre class="kg kh ki kj gt ms mt mu mv aw mw bi"><span id="e8d7" class="ls lt iq mt b gy mx my l mz na">model_nn(X, Y, n_h, num_iterations = 10000, print_cost = True)</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nd"><img src="../Images/89f9108d8d757462784c402dfdc07a07.png" data-original-src="https://miro.medium.com/v2/resize:fit:1080/format:webp/1*k92338u5L0w1QE_y9t5KTA.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">10，000次迭代后的成本(图片由作者提供)</p></figure><p id="45eb" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">然后，我们使用参数(与最低成本相关联)来更准确地对初始数据集中呈现的点颜色进行分类。为此，我们将再次运行正向传播步骤来计算最终的A2输出，并使用0.5作为阈值来确定预测应该是0还是1。</p><pre class="kg kh ki kj gt ms mt mu mv aw mw bi"><span id="e5d0" class="ls lt iq mt b gy mx my l mz na">def predict(parameters, X):<br/>    """Takes learned parameters to predict each class in X<br/>    Arguments: Takes parameters, and X<br/>    Outputs: Prediction values -- vector of predictions in our model (red: 0 / blue: 1)<br/>    """<br/>    # Computes forward propagation again to determine probabilities<br/>    # Classifies to 0/1 using 0.5 as the threshold.<br/>    A2, cache = forward_propagation(X, parameters)<br/>    predictions = (A2 &gt; 0.5)<br/>    <br/>    return predictions</span></pre><p id="1658" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">通过10，000次迭代应用我们的最终模型(model_nn ),我们可以再次生成要在决策边界图中使用的那些参数。</p><pre class="kg kh ki kj gt ms mt mu mv aw mw bi"><span id="9c99" class="ls lt iq mt b gy mx my l mz na"># Build a model with a n_h-dimensional hidden layer<br/>parameters = model_nn(X, Y, n_h = 4, num_iterations = 10000, print_cost=True)</span><span id="6c9d" class="ls lt iq mt b gy nb my l mz na"># Plot the decision boundary<br/>plot_decision_boundary(lambda x: predict(parameters, x.T), X, Y)<br/>plt.title("Decision Boundary for hidden layer size " + str(4))</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ne"><img src="../Images/74a0257e15883745fbb50ee4972689ce.png" data-original-src="https://miro.medium.com/v2/resize:fit:938/format:webp/1*gVfRBy527x1LpVZVZX78Lw.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">改进了原始数据集的决策界限(图片由作者提供)</p></figure><p id="fde3" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">正如我们所看到的，我们的神经网络工作在正确地将点分类为红色或蓝色方面做得更好。打印我们的预测值和真实值之间的准确度分数，发现准确度确实是96%。</p><pre class="kg kh ki kj gt ms mt mu mv aw mw bi"><span id="cc35" class="ls lt iq mt b gy mx my l mz na"># Print accuracy<br/>new_predictions = predict(parameters, X)<br/>print ('Accuracy: %d' % float((np.dot(Y,new_predictions.T) + <br/>    np.dot(1-Y,1-new_predictions.T))/float(Y.size)*100) + '%')</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nf"><img src="../Images/c6abbcde6d0b85e38ea4996c33831e60.png" data-original-src="https://miro.medium.com/v2/resize:fit:218/format:webp/1*Q_JCxzRU2rcr-0za6ny_WA.png"/></div></figure><p id="3aea" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我希望这篇由两部分组成的文章提供了丰富的信息。如果你刚刚开始你的神经网络之旅，我强烈推荐吴恩达的Coursera讲座。它们是我撰写本文的灵感来源，并帮助我巩固了其中的许多概念。下次见！</p></div></div>    
</body>
</html>