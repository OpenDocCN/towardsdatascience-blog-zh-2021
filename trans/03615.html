<html>
<head>
<title>CNNs for Audio Classification</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用于音频分类的CNN</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/cnns-for-audio-classification-6244954665ab?source=collection_archive---------3-----------------------#2021-03-24">https://towardsdatascience.com/cnns-for-audio-classification-6244954665ab?source=collection_archive---------3-----------------------#2021-03-24</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="7293" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">使用张量流进行音频分类的深度学习入门</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi kf"><img src="../Images/654711da5534a86a8a5a10308f4989ad.png" data-original-src="https://miro.medium.com/v2/resize:fit:994/format:webp/1*8SpdHlATy61y3ccUkOPX6Q.png"/></div><p class="kn ko gj gh gi kp kq bd b be z dk translated">作者图片</p></figure><h1 id="d9c4" class="kr ks iq bd kt ku kv kw kx ky kz la lb jw lc jx ld jz le ka lf kc lg kd lh li bi translated">卷积神经网络</h1><p id="cf23" class="pw-post-body-paragraph lj lk iq ll b lm ln jr lo lp lq ju lr ls lt lu lv lw lx ly lz ma mb mc md me ij bi translated">CNN或卷积神经网络是一种深度学习算法，在学习图像方面做得非常好。</p><p id="c0c9" class="pw-post-body-paragraph lj lk iq ll b lm mf jr lo lp mg ju lr ls mh lu lv lw mi ly lz ma mj mc md me ij bi translated">那是因为他们可以学习到<strong class="ll ir"> <em class="mk">平移不变</em> </strong>且具有<strong class="ll ir"> <em class="mk">空间层次</em> </strong>的模式(F. Chollet，2018)。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="mm mn di mo bf mp"><div class="gh gi ml"><img src="../Images/e1202761cdaf04ac02c2656590cccc20.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GkeG1E37l44fHGJ66VxYJw.png"/></div></div><p class="kn ko gj gh gi kp kq bd b be z dk translated">作者图片</p></figure><p id="e70b" class="pw-post-body-paragraph lj lk iq ll b lm mf jr lo lp mg ju lr ls mh lu lv lw mi ly lz ma mj mc md me ij bi translated">也就是说如果如果CNN学习了上图左角的狗，那么它就可以识别出另外两张被移动过的图片中的狗(<strong class="ll ir"> <em class="mk">平移不变性</em> </strong>)。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="mm mn di mo bf mp"><div class="gh gi ml"><img src="../Images/b7277c058c252b97edeb05c412565cf9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LsBqadp7H452J1HFI1Mv6A.png"/></div></div></figure><p id="3a6c" class="pw-post-body-paragraph lj lk iq ll b lm mf jr lo lp mg ju lr ls mh lu lv lw mi ly lz ma mj mc md me ij bi translated">如果CNN从上图的左上角学习这只狗，它会在另外两张图片中识别出原始图像的<strong class="ll ir">部分</strong>，因为它已经学习了她患有异色症的眼睛的边缘是什么样子，她狼一样的鼻子和她时尚耳机的形状(<strong class="ll ir"> <em class="mk">空间层次</em> </strong>)。</p><p id="5e2d" class="pw-post-body-paragraph lj lk iq ll b lm mf jr lo lp mg ju lr ls mh lu lv lw mi ly lz ma mj mc md me ij bi translated">这些属性使CNN成为强大的图像学习者，因为真实世界并不总是看起来完全像训练数据。</p><h1 id="6143" class="kr ks iq bd kt ku kv kw kx ky kz la lb jw lc jx ld jz le ka lf kc lg kd lh li bi translated">我能把这个用于音频吗？</h1><p id="f663" class="pw-post-body-paragraph lj lk iq ll b lm ln jr lo lp lq ju lr ls lt lu lv lw lx ly lz ma mb mc md me ij bi translated"><strong class="ll ir">是的。你可以提取看起来像T21图像的特征，并以某种方式塑造它们，以便将它们输入CNN。</strong></p><h1 id="d15e" class="kr ks iq bd kt ku kv kw kx ky kz la lb jw lc jx ld jz le ka lf kc lg kd lh li bi translated">本文解释了如何训练CNN根据音频信息对物种进行分类。</h1><p id="e130" class="pw-post-body-paragraph lj lk iq ll b lm ln jr lo lp lq ju lr ls lt lu lv lw lx ly lz ma mb mc md me ij bi translated">这个例子的数据是来自Kaggle竞赛<a class="ae mq" href="https://www.kaggle.com/c/rfcx-species-audio-detection/data" rel="noopener ugc nofollow" target="_blank">雨林连接物种音频检测</a>的鸟和青蛙的录音。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="mm mn di mo bf mp"><div class="gh gi mr"><img src="../Images/f58662f474bea346c97b373715355c84.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jJJ29iMv1U8mQiaaGY6O9Q.jpeg"/></div></div><p class="kn ko gj gh gi kp kq bd b be z dk translated">作者图片</p></figure><p id="b037" class="pw-post-body-paragraph lj lk iq ll b lm mf jr lo lp mg ju lr ls mh lu lv lw mi ly lz ma mj mc md me ij bi translated">首先，加载必要的输入:</p><pre class="kg kh ki kj gt ms mt mu mv aw mw bi"><span id="e207" class="mx ks iq mt b gy my mz l na nb">import pandas as pd<br/>import os<br/>import librosa<br/>import librosa.display<br/>import matplotlib.pyplot as plt<br/>from sklearn.preprocessing import normalize<br/>import warnings<br/>warnings.filterwarnings('ignore')<br/>import numpy as np<br/>import pickle<br/>import joblib<br/>from sklearn.model_selection import train_test_split<br/>from tensorflow.keras import models, layers<br/>import tensorflow as tf</span></pre><p id="a100" class="pw-post-body-paragraph lj lk iq ll b lm mf jr lo lp mg ju lr ls mh lu lv lw mi ly lz ma mj mc md me ij bi translated">然后是数据帧:</p><pre class="kg kh ki kj gt ms mt mu mv aw mw bi"><span id="77d8" class="mx ks iq mt b gy my mz l na nb">os.chdir('/kaggle/input/rfcx-species-audio-detection')<br/>df = pd.read_csv('train_tp.csv')</span></pre><p id="8696" class="pw-post-body-paragraph lj lk iq ll b lm mf jr lo lp mg ju lr ls mh lu lv lw mi ly lz ma mj mc md me ij bi translated">该数据集是一个csv文件，音频文件的名称列在recording_id下，标签列在species_id下，音频样本的开始/结束列在t_min和t_max下:</p><pre class="kg kh ki kj gt ms mt mu mv aw mw bi"><span id="86dc" class="mx ks iq mt b gy my mz l na nb">df.head()</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nc"><img src="../Images/ac7140d5c08f6e35e420a9aeb2017aa8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1022/format:webp/0*yCYftdHKn6x5abyZ.png"/></div></figure><p id="7f17" class="pw-post-body-paragraph lj lk iq ll b lm mf jr lo lp mg ju lr ls mh lu lv lw mi ly lz ma mj mc md me ij bi translated">使用<a class="ae mq" href="https://librosa.org/doc/latest/index.html" rel="noopener ugc nofollow" target="_blank"> librosa </a>包加载并显示一个音频文件，如下所示:</p><pre class="kg kh ki kj gt ms mt mu mv aw mw bi"><span id="91b8" class="mx ks iq mt b gy my mz l na nb">sample_num=3 #pick a file to display<br/>#get the filename <br/>filename=df.recording_id[sample_num]+str('.flac')<br/>#define the beginning time of the signal<br/>tstart = df.t_min[sample_num] <br/>tend = df.t_max[sample_num] #define the end time of the signal<br/>y,sr=librosa.load('train/'+str(filename)) #load the file<br/>librosa.display.waveplot(y,sr=sr, x_axis='time', color='cyan')</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nd"><img src="../Images/c167ae1c3de3068c1684e9d62199ad69.png" data-original-src="https://miro.medium.com/v2/resize:fit:772/format:webp/1*XlnMbwnOJtJiu2NOd4OMug.png"/></div></figure><h1 id="175c" class="kr ks iq bd kt ku kv kw kx ky kz la lb jw lc jx ld jz le ka lf kc lg kd lh li bi translated">棘手的部分</h1><p id="71e3" class="pw-post-body-paragraph lj lk iq ll b lm ln jr lo lp lq ju lr ls lt lu lv lw lx ly lz ma mb mc md me ij bi translated">CNN期待一个图像:</p><ul class=""><li id="a2ff" class="ne nf iq ll b lm mf lp mg ls ng lw nh ma ni me nj nk nl nm bi translated">灰度图像(1个通道)</li><li id="b9d1" class="ne nf iq ll b lm nn lp no ls np lw nq ma nr me nj nk nl nm bi translated">具有三个通道的彩色图像:红色、绿色和蓝色(RGB)</li></ul><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="mm mn di mo bf mp"><div class="gh gi ns"><img src="../Images/e8294e765b1c1c66152fc094d96e2281.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*n38bUO8fMA0PuMVHMO6AFg.png"/></div></div><p class="kn ko gj gh gi kp kq bd b be z dk translated">作者图片</p></figure><p id="718e" class="pw-post-body-paragraph lj lk iq ll b lm mf jr lo lp mg ju lr ls mh lu lv lw mi ly lz ma mj mc md me ij bi translated">所以你必须让你的音频特征看起来像图像。</p><ul class=""><li id="ba2c" class="ne nf iq ll b lm mf lp mg ls ng lw nh ma ni me nj nk nl nm bi translated">为灰度图像(一个要素)选择1D，或为彩色图像(表示多个要素)选择3D。</li><li id="7562" class="ne nf iq ll b lm nn lp no ls np lw nq ma nr me nj nk nl nm bi translated">缩放和填充音频功能，使每个“通道”大小相同。</li></ul><pre class="kg kh ki kj gt ms mt mu mv aw mw bi"><span id="0a7d" class="mx ks iq mt b gy my mz l na nb">#This code was adapted from Nicolas Gervais on <a class="ae mq" href="https://stackoverflow.com/questions/59241216/padding-numpy-arrays-to-a-specific-size" rel="noopener ugc nofollow" target="_blank">https://stackoverflow.com/questions/59241216/padding-numpy-arrays-to-a-specific-size</a> on 1/10/2021</span><span id="654f" class="mx ks iq mt b gy nt mz l na nb">def padding(array, xx, yy):<br/>    """<br/>    :param array: numpy array<br/>    :param xx: desired height<br/>    :param yy: desirex width<br/>    :return: padded array<br/>    """</span><span id="f3ee" class="mx ks iq mt b gy nt mz l na nb">h = array.shape[0]<br/>    w = array.shape[1]</span><span id="3d93" class="mx ks iq mt b gy nt mz l na nb">a = max((xx - h) // 2,0)<br/>    aa = max(0,xx - a - h)</span><span id="3098" class="mx ks iq mt b gy nt mz l na nb">b = max(0,(yy - w) // 2)<br/>    bb = max(yy - b - w,0)</span><span id="28e7" class="mx ks iq mt b gy nt mz l na nb">return np.pad(array, pad_width=((a, aa), (b, bb)), mode='constant')</span></pre><h2 id="189f" class="mx ks iq bd kt nu nv dn kx nw nx dp lb ls ny nz ld lw oa ob lf ma oc od lh oe bi translated">难道我不能把我的音频特征分成三等份，重新塑造成3D形状吗？</h2><p id="0111" class="pw-post-body-paragraph lj lk iq ll b lm ln jr lo lp lq ju lr ls lt lu lv lw lx ly lz ma mb mc md me ij bi translated">它们毕竟只是数字。</p><p id="1583" class="pw-post-body-paragraph lj lk iq ll b lm mf jr lo lp mg ju lr ls mh lu lv lw mi ly lz ma mj mc md me ij bi translated"><strong class="ll ir">号</strong>它必须使视觉<strong class="ll ir">感知</strong>。垃圾进，垃圾出。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi of"><img src="../Images/a7ea54ca02689c0c35c25bfb2a38e25a.png" data-original-src="https://miro.medium.com/v2/resize:fit:606/format:webp/1*KaezHOievDzZTmWwmFDAkw.png"/></div><p class="kn ko gj gh gi kp kq bd b be z dk translated">作者图片</p></figure><h2 id="323a" class="mx ks iq bd kt nu nv dn kx nw nx dp lb ls ny nz ld lw oa ob lf ma oc od lh oe bi translated">建模的特征</h2><p id="5c1b" class="pw-post-body-paragraph lj lk iq ll b lm ln jr lo lp lq ju lr ls lt lu lv lw lx ly lz ma mb mc md me ij bi translated">Librosa 有关于如何提取特征的很棒的教程<a class="ae mq" href="https://librosa.org/doc/latest/tutorial.html" rel="noopener ugc nofollow" target="_blank">在这里</a>。</p><p id="b1da" class="pw-post-body-paragraph lj lk iq ll b lm mf jr lo lp mg ju lr ls mh lu lv lw mi ly lz ma mj mc md me ij bi translated">对于这个例子，我将计算:</p><ul class=""><li id="5134" class="ne nf iq ll b lm mf lp mg ls ng lw nh ma ni me nj nk nl nm bi translated">梅尔谱图(<a class="ae mq" href="https://librosa.org/doc/latest/generated/librosa.feature.melspectrogram.html" rel="noopener ugc nofollow" target="_blank"> MFCCs </a>)</li><li id="4648" class="ne nf iq ll b lm nn lp no ls np lw nq ma nr me nj nk nl nm bi translated"><a class="ae mq" href="https://librosa.org/doc/latest/generated/librosa.feature.spectral_bandwidth.html" rel="noopener ugc nofollow" target="_blank">光谱带宽</a></li><li id="55a3" class="ne nf iq ll b lm nn lp no ls np lw nq ma nr me nj nk nl nm bi translated"><a class="ae mq" href="https://librosa.org/doc/latest/generated/librosa.feature.spectral_centroid.html#librosa.feature.spectral_centroid" rel="noopener ugc nofollow" target="_blank">光谱质心</a></li><li id="92db" class="ne nf iq ll b lm nn lp no ls np lw nq ma nr me nj nk nl nm bi translated">色度图</li><li id="d3f0" class="ne nf iq ll b lm nn lp no ls np lw nq ma nr me nj nk nl nm bi translated">短时傅立叶变换(<a class="ae mq" href="https://librosa.org/doc/latest/generated/librosa.stft.html#librosa.stft" rel="noopener ugc nofollow" target="_blank"> stft </a>)</li></ul><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi og"><img src="../Images/f712a3f8d5129bbf438cef2fb1304c16.png" data-original-src="https://miro.medium.com/v2/resize:fit:1042/format:webp/1*rFhL3CYygk0gGlHOmlL_Jg.png"/></div><p class="kn ko gj gh gi kp kq bd b be z dk translated">输入到CNN的3D图像是4D张量</p></figure><p id="b093" class="pw-post-body-paragraph lj lk iq ll b lm mf jr lo lp mg ju lr ls mh lu lv lw mi ly lz ma mj mc md me ij bi translated">第一个轴是音频文件id，代表tensorflow-speak中的批处理。在这个例子中，第二轴是光谱带宽、质心和色谱图，其被重复、填充并适合第三轴(stft)和第四轴(MFCCs)的形状。</p><pre class="kg kh ki kj gt ms mt mu mv aw mw bi"><span id="bb0b" class="mx ks iq mt b gy my mz l na nb">#The eventual shape of the features<br/>print(X_train.shape,X_test.shape)</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/07d910b157d0a1d8d1e888e7eced8df9.png" data-original-src="https://miro.medium.com/v2/resize:fit:656/format:webp/1*9KDFq4gv15GFDlsI4lO1PQ.png"/></div></figure><p id="1124" class="pw-post-body-paragraph lj lk iq ll b lm mf jr lo lp mg ju lr ls mh lu lv lw mi ly lz ma mj mc md me ij bi translated">第一轴1226是批量大小，128是高度，1000是宽度(由下面代码中的<strong class="ll ir"> max_size </strong>设置), 3是训练数据中的通道数。如果我有1226个音频文件，那么批量大小是1226。如果我们只提取dataframe.head()图中所示的5个音频文件的特征，则输入的形状将是5×128×1000×3。如果您想在训练时<a class="ae mq" href="https://stats.stackexchange.com/questions/153531/what-is-batch-size-in-neural-network" rel="noopener ugc nofollow" target="_blank">使用更少的内存，您可以减小批量大小。对于这个例子，批量大小被设置为音频文件的数量。</a></p><pre class="kg kh ki kj gt ms mt mu mv aw mw bi"><span id="419b" class="mx ks iq mt b gy my mz l na nb">def generate_features(y_cut):<br/>    <strong class="mt ir">max_size</strong>=1000 #my max audio file feature width<br/>    stft = padding(np.abs(librosa.stft(y_cut, n_fft=255, hop_length        = 512)), 128, max_size)<br/>    MFCCs = padding(librosa.feature.mfcc(y_cut, n_fft=n_fft, hop_length=hop_length,n_mfcc=128),128,max_size)<br/>    spec_centroid = librosa.feature.spectral_centroid(y=y_cut, sr=sr)<br/>    chroma_stft = librosa.feature.chroma_stft(y=y_cut, sr=sr)<br/>    spec_bw = librosa.feature.spectral_bandwidth(y=y_cut, sr=sr)</span><span id="8391" class="mx ks iq mt b gy nt mz l na nb">    #Now the padding part<br/>    image = np.array([padding(normalize(spec_bw),1, max_size)]).reshape(1,max_size)<br/>    image = np.append(image,padding(normalize(spec_centroid),1, max_size), axis=0) </span><span id="db65" class="mx ks iq mt b gy nt mz l na nb">#repeat the padded spec_bw,spec_centroid and chroma stft until they are stft and MFCC-sized<br/>    for i in range(0,9):<br/>        image = np.append(image,padding(normalize(spec_bw),1, max_size), axis=0)<br/>        image = np.append(image, padding(normalize(spec_centroid),1, max_size), axis=0)<br/>        image = np.append(image, padding(normalize(chroma_stft),12, max_size), axis=0)<br/>    image=np.dstack((image,np.abs(stft)))<br/>    image=np.dstack((image,MFCCs))<br/>    return image</span></pre><p id="f832" class="pw-post-body-paragraph lj lk iq ll b lm mf jr lo lp mg ju lr ls mh lu lv lw mi ly lz ma mj mc md me ij bi translated">以下三个特征被挤压、填充和重复…</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="mm mn di mo bf mp"><div class="gh gi oi"><img src="../Images/6833fcc44a364e1599d27691a8950ba3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZctQHmKnMkHBk6-LJVADtg.png"/></div></div></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="mm mn di mo bf mp"><div class="gh gi oj"><img src="../Images/906291369d288d3bf56aa734bc37164c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*665b8LxaVJX65b4-WoxqdQ.png"/></div></div></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="mm mn di mo bf mp"><div class="gh gi ok"><img src="../Images/a2015614f1ae8261c667eac4d9d8d3a4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qj2JMw8FgpfqNPInyPXxjg.png"/></div></div></figure><p id="2f3b" class="pw-post-body-paragraph lj lk iq ll b lm mf jr lo lp mg ju lr ls mh lu lv lw mi ly lz ma mj mc md me ij bi translated">…进入以下轴:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="mm mn di mo bf mp"><div class="gh gi ol"><img src="../Images/f5acf265236987a05e2742a58e90129a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BI_Ap7LK5TQjBenhVPvbSA.png"/></div></div><p class="kn ko gj gh gi kp kq bd b be z dk translated">第二轴</p></figure><p id="bef6" class="pw-post-body-paragraph lj lk iq ll b lm mf jr lo lp mg ju lr ls mh lu lv lw mi ly lz ma mj mc md me ij bi translated">最后两个轴被设计成相同的形状:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="mm mn di mo bf mp"><div class="gh gi ol"><img src="../Images/a9c789a21056aabc2ec6efda401c44c0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*049xJoCkGvsbKzjkm1JTWA.png"/></div></div><p class="kn ko gj gh gi kp kq bd b be z dk translated">stft</p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="mm mn di mo bf mp"><div class="gh gi ol"><img src="../Images/10a076e65a22b9113e1838cd3662934b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TB760_ERlL3KV3JDrW6Glg.png"/></div></div><p class="kn ko gj gh gi kp kq bd b be z dk translated">MFCCs</p></figure><h2 id="cfc3" class="mx ks iq bd kt nu nv dn kx nw nx dp lb ls ny nz ld lw oa ob lf ma oc od lh oe bi translated">我必须计算这些完全相同的特征吗？</h2><p id="ae55" class="pw-post-body-paragraph lj lk iq ll b lm ln jr lo lp lq ju lr ls lt lu lv lw lx ly lz ma mb mc md me ij bi translated"><strong class="ll ir">否</strong>。只要你把它们垫成同样的形状，就用建模中效果最好的。</p><pre class="kg kh ki kj gt ms mt mu mv aw mw bi"><span id="1b7b" class="mx ks iq mt b gy my mz l na nb">X=df.drop('species_id',axis=1)<br/>y=df.species_id</span></pre><h2 id="7799" class="mx ks iq bd kt nu nv dn kx nw nx dp lb ls ny nz ld lw oa ob lf ma oc od lh oe bi translated">提取训练集、测试集和验证集</h2><pre class="kg kh ki kj gt ms mt mu mv aw mw bi"><span id="4c21" class="mx ks iq mt b gy my mz l na nb">#Split once to get the test and training set<br/>X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=123, stratify=y)<br/>print(X_train.shape,X_test.shape)</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi om"><img src="../Images/d8dc8f354066321ca8214731ce687e87.png" data-original-src="https://miro.medium.com/v2/resize:fit:550/format:webp/1*QiLL4SrY_coFeuP_9kgcmA.png"/></div></figure><pre class="kg kh ki kj gt ms mt mu mv aw mw bi"><span id="4334" class="mx ks iq mt b gy my mz l na nb">#Split twice to get the validation set<br/>X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=123)<br/>print(X_train.shape, X_test.shape, X_val.shape, len(y_train), len(y_test), len(y_val))</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi on"><img src="../Images/560ff12529c37c48518b0f012f85d250.png" data-original-src="https://miro.medium.com/v2/resize:fit:992/format:webp/1*MngpRJs443WhfCJ9nzgFoA.png"/></div></figure><p id="9bec" class="pw-post-body-paragraph lj lk iq ll b lm mf jr lo lp mg ju lr ls mh lu lv lw mi ly lz ma mj mc md me ij bi translated">为每个音频文件计算这些特征，并存储为特征和标签:</p><pre class="kg kh ki kj gt ms mt mu mv aw mw bi"><span id="cf14" class="mx ks iq mt b gy my mz l na nb"><strong class="mt ir">def</strong> get_features(df_in):   <br/>    features=[]     <br/>    labels = [] <em class="mk">#empty array to store labels</em>     <br/>    <em class="mk">#For each species, determine how many augmentations are needed<br/></em>    df_in=df_in.reset_index()     <br/>    <strong class="mt ir">for</strong> i <strong class="mt ir">in</strong> df_in.species_id.unique():<br/>           print('species_id:',i)    <br/>           <em class="mk">#all the file indices with the same species_id</em>     <br/>           filelist = df_in.loc[df_in.species_id == i].index         <br/>    <strong class="mt ir">for</strong> j <strong class="mt ir">in</strong> range(0,len(filelist)):             <br/>           filename = df_in.iloc[filelist[j]].recording_id<br/>            +str('.flac') <em class="mk">#get the filename</em>   <br/>            <em class="mk">#define the beginning time of the signal</em>          <br/>            tstart = df_in.iloc[filelist[j]].t_min             <br/>            tend = df_in.iloc[filelist[j]].t_max <em class="mk">#end of signal<br/></em>            recording_id = df_in.iloc[filelist[j]].recording_id<br/>            species_id = i<br/>            songtype_id = df_in.iloc[filelist[j]].songtype_id   <br/>            <em class="mk">#Load the file<br/>    </em>        y, sr = librosa.load(filename,sr=28000)  <br/>            <em class="mk">#cut the file to signal start and end</em>  <br/>            y_cut=y[int(round(tstart*sr)):int(round(tend*sr))]  <br/>            <em class="mk">#generate features &amp; output numpy</em> <em class="mk">array</em>          <br/>            data = generate_features(y_cut) <br/>            features.append(data[np.newaxis,...])    <br/>            labels.append(species_id)     <br/>     output=np.concatenate(features,axis=0)     <br/>     <strong class="mt ir">return</strong>(np.array(output), labels)</span><span id="87b6" class="mx ks iq mt b gy nt mz l na nb">#use get_features to calculate and store the features<br/>test_features, test_labels = get_features(pd.concat([X_test,y_test],axis=1))<br/>train_features, train_labels = get_features_noOS(pd.concat([X_train,y_train],axis=1))</span></pre><h2 id="bb0e" class="mx ks iq bd kt nu nv dn kx nw nx dp lb ls ny nz ld lw oa ob lf ma oc od lh oe bi translated">将数据规范化并转换为numpy数组</h2><pre class="kg kh ki kj gt ms mt mu mv aw mw bi"><span id="1dcb" class="mx ks iq mt b gy my mz l na nb">X_train = np.array((X_train-np.min(X_train))/(np.max(X_train)-np.min(X_train)))<br/>X_test = np.array((X_test-np.min(X_test))/(np.max(X_test)-np.min(X_test)))<br/>X_train = X_train/np.std(X_train)<br/>X_test = X_test/np.std(X_test)<br/>y_train = np.array(y_train)<br/>y_test = np.array(y_test)</span></pre><h1 id="5899" class="kr ks iq bd kt ku kv kw kx ky kz la lb jw lc jx ld jz le ka lf kc lg kd lh li bi translated">创建一个CNN</h1><p id="c6ca" class="pw-post-body-paragraph lj lk iq ll b lm ln jr lo lp lq ju lr ls lt lu lv lw lx ly lz ma mb mc md me ij bi translated">在下面的示例模型中，2D <strong class="ll ir">卷积层</strong> (Conv2D)单元是学习<em class="mk">平移不变空间模式</em>及其<em class="mk">空间层次的部分。</em></p><p id="bd36" class="pw-post-body-paragraph lj lk iq ll b lm mf jr lo lp mg ju lr ls mh lu lv lw mi ly lz ma mj mc md me ij bi translated"><strong class="ll ir">最大池</strong> <strong class="ll ir">图层</strong>通过将特征地图缩减采样至窗口内的最大值，将特征地图的大小减半。为什么要缩减采样？因为否则它会导致一个巨大数量的参数，你的计算机会崩溃，毕竟，模型会大规模地过度拟合数据。这种神奇的层是CNN能够处理图像中大量数据的原因。最大池对模型有好处。</p><p id="9958" class="pw-post-body-paragraph lj lk iq ll b lm mf jr lo lp mg ju lr ls mh lu lv lw mi ly lz ma mj mc md me ij bi translated"><strong class="ll ir">剔除</strong> <strong class="ll ir">层</strong>通过将一部分数据的权重随机设置为零来防止过度拟合，而<strong class="ll ir">密集</strong>单元包含与模型必须尝试拟合数据的自由度相关的隐藏层。数据越复杂，模型需要的自由度就越多。<em class="mk">注意</em> <em class="mk">不要添加一堆这样的东西，否则会过度拟合数据</em>。</p><p id="46fc" class="pw-post-body-paragraph lj lk iq ll b lm mf jr lo lp mg ju lr ls mh lu lv lw mi ly lz ma mj mc md me ij bi translated"><strong class="ll ir">展平层</strong>将所有特征地图信息压缩成一列，以便输入到密集层，最后一层输出模型应该将音频记录分类到的24个种类。</p><h2 id="396a" class="mx ks iq bd kt nu nv dn kx nw nx dp lb ls ny nz ld lw oa ob lf ma oc od lh oe bi translated">CNN模型架构示例</h2><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oo"><img src="../Images/123b26ade86f71c2753e4857d9b84071.png" data-original-src="https://miro.medium.com/v2/resize:fit:1368/format:webp/1*yFwgkKzX5bXrHBg1SJv1vg.png"/></div><p class="kn ko gj gh gi kp kq bd b be z dk translated">作者图片</p></figure><h2 id="39e9" class="mx ks iq bd kt nu nv dn kx nw nx dp lb ls ny nz ld lw oa ob lf ma oc od lh oe bi translated">在tensorflow中，您可以像这样创建上面的模型</h2><pre class="kg kh ki kj gt ms mt mu mv aw mw bi"><span id="8d92" class="mx ks iq mt b gy my mz l na nb">input_shape=(128,1000,3)<br/>CNNmodel = models.Sequential()<br/>CNNmodel.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=input_shape))<br/>CNNmodel.add(layers.MaxPooling2D((2, 2)))<br/>CNNmodel.add(layers.Dropout(0.2))<br/>CNNmodel.add(layers.Conv2D(64, (3, 3), activation='relu'))<br/>CNNmodel.add(layers.MaxPooling2D((2, 2)))<br/>CNNmodel.add(layers.Dropout(0.2))<br/>CNNmodel.add(layers.Conv2D(64, (3, 3), activation='relu'))<br/>CNNmodel.add(layers.Flatten())<br/>CNNmodel.add(layers.Dense(64, activation='relu'))<br/>CNNmodel.add(layers.Dropout(0.2))<br/>CNNmodel.add(layers.Dense(32, activation='relu'))<br/>CNNmodel.add(layers.Dense(24, activation='softmax'))</span></pre><p id="c33d" class="pw-post-body-paragraph lj lk iq ll b lm mf jr lo lp mg ju lr ls mh lu lv lw mi ly lz ma mj mc md me ij bi translated"><strong class="ll ir">激活</strong>功能赋予模型向模型添加非线性的能力。这里使用了<strong class="ll ir"> relu </strong>函数，它将负权重清零。你可以在这里阅读其他激活功能<a class="ae mq" href="https://www.tensorflow.org/api_docs/python/tf/keras/activations" rel="noopener ugc nofollow" target="_blank">，但这是一个很好的开始。最后一个<strong class="ll ir">密集</strong>层的激活函数类型是<strong class="ll ir"> softmax </strong>，它为每个类输出一个概率。</a></p><h2 id="238f" class="mx ks iq bd kt nu nv dn kx nw nx dp lb ls ny nz ld lw oa ob lf ma oc od lh oe bi translated">编译模型</h2><pre class="kg kh ki kj gt ms mt mu mv aw mw bi"><span id="3bfa" class="mx ks iq mt b gy my mz l na nb">CNNmodel.compile(optimizer='adam',loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),metrics=['accuracy'])</span></pre><p id="651a" class="pw-post-body-paragraph lj lk iq ll b lm mf jr lo lp mg ju lr ls mh lu lv lw mi ly lz ma mj mc md me ij bi translated"><a class="ae mq" href="https://arxiv.org/abs/1412.6980" rel="noopener ugc nofollow" target="_blank"> <strong class="ll ir"> Adam </strong> </a>优化器为您管理学习率，<strong class="ll ir"> loss </strong>函数用于评估预测数据和实际数据的差异，并对预测不佳的模型进行惩罚。在这个例子中，损失函数是<strong class="ll ir">SparseCategoricalCrossentropy</strong>，当每个样本属于一个标签时使用<a class="ae mq" href="https://datascience.stackexchange.com/questions/41921/sparse-categorical-crossentropy-vs-categorical-crossentropy-keras-accuracy#:~:text=Use%20sparse%20categorical%20crossentropy%20when,0.5%2C%200.3%2C%200.2%5D)." rel="noopener ugc nofollow" target="_blank">，而不是一个以上，<strong class="ll ir">和</strong>它不是二元分类。这是一个合适的选择，因为每个音频样本属于一个物种，而它们有24个。</a></p><h2 id="3ec0" class="mx ks iq bd kt nu nv dn kx nw nx dp lb ls ny nz ld lw oa ob lf ma oc od lh oe bi translated">符合模型</h2><pre class="kg kh ki kj gt ms mt mu mv aw mw bi"><span id="1cc6" class="mx ks iq mt b gy my mz l na nb">history = CNNmodel.fit(X_train, y_train, epochs=20, validation_data= (X_val, y_val))</span></pre><h2 id="8ca0" class="mx ks iq bd kt nu nv dn kx nw nx dp lb ls ny nz ld lw oa ob lf ma oc od lh oe bi translated">为了避免过度拟合，从最简单的模型开始，一步步向上</h2><p id="236c" class="pw-post-body-paragraph lj lk iq ll b lm ln jr lo lp lq ju lr ls lt lu lv lw lx ly lz ma mb mc md me ij bi translated">这是因为如果模型过于复杂，它将准确地学习你的训练数据，而无法推广到看不见的数据。</p><p id="777a" class="pw-post-body-paragraph lj lk iq ll b lm mf jr lo lp mg ju lr ls mh lu lv lw mi ly lz ma mj mc md me ij bi translated">试试这个:</p><pre class="kg kh ki kj gt ms mt mu mv aw mw bi"><span id="7203" class="mx ks iq mt b gy my mz l na nb">input_shape=(128,1000,3)<br/>CNNmodel = models.Sequential()<br/>CNNmodel.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=input_shape))<br/>CNNmodel.add(layers.MaxPooling2D((2, 2)))<br/>CNNmodel.add(layers.Flatten())<br/>CNNmodel.add(layers.Dense(32, activation='relu'))<br/>CNNmodel.add(layers.Dense(24, activation='softmax'))<br/>CNNmodel.summary()</span></pre><p id="88cf" class="pw-post-body-paragraph lj lk iq ll b lm mf jr lo lp mg ju lr ls mh lu lv lw mi ly lz ma mj mc md me ij bi translated">注意:这个模型太简单了，根本无法预测数据<em class="mk">(如个位数精度)。</em></p><h2 id="5c80" class="mx ks iq bd kt nu nv dn kx nw nx dp lb ls ny nz ld lw oa ob lf ma oc od lh oe bi translated">接下来，添加层，直到你的模型已经开始适应数据。</h2><h1 id="9647" class="kr ks iq bd kt ku kv kw kx ky kz la lb jw lc jx ld jz le ka lf kc lg kd lh li bi translated">评估您的模型训练和验证集</h1><ol class=""><li id="607c" class="ne nf iq ll b lm ln lp lq ls op lw oq ma or me os nk nl nm bi translated">注意训练集和测试集之间的性能差异。如果训练集表现明显更好，它不会很好地推广到看不见的数据。</li><li id="5f8d" class="ne nf iq ll b lm nn lp no ls np lw nq ma nr me os nk nl nm bi translated">如果验证集的性能开始下降，停止迭代。</li></ol><pre class="kg kh ki kj gt ms mt mu mv aw mw bi"><span id="7fbb" class="mx ks iq mt b gy my mz l na nb">#Adapted from Deep Learning with Python by Francois Chollet, 2018<br/>history_dict=history.history<br/>loss_values=history_dict['loss']<br/>acc_values=history_dict['accuracy']<br/>val_loss_values = history_dict['val_loss']<br/>val_acc_values=history_dict['val_accuracy']<br/>epochs=range(1,21)<br/>fig,(ax1,ax2)=plt.subplots(1,2,figsize=(15,5))<br/>ax1.plot(epochs,loss_values,'bo',label='Training Loss')<br/>ax1.plot(epochs,val_loss_values,'orange', label='Validation Loss')<br/>ax1.set_title('Training and validation loss')<br/>ax1.set_xlabel('Epochs')<br/>ax1.set_ylabel('Loss')<br/>ax1.legend()<br/>ax2.plot(epochs,acc_values,'bo', label='Training accuracy')<br/>ax2.plot(epochs,val_acc_values,'orange',label='Validation accuracy')<br/>ax2.set_title('Training and validation accuracy')<br/>ax2.set_xlabel('Epochs')<br/>ax2.set_ylabel('Accuracy')<br/>ax2.legend()<br/>plt.show()</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="mm mn di mo bf mp"><div class="gh gi ot"><img src="../Images/15387ad93edb82ba17f0425200f5e5ce.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*84CYwqHILT0gJ90T8e7WoQ.png"/></div></div></figure><h1 id="33cf" class="kr ks iq bd kt ku kv kw kx ky kz la lb jw lc jx ld jz le ka lf kc lg kd lh li bi translated">结束语</h1><p id="8cc0" class="pw-post-body-paragraph lj lk iq ll b lm ln jr lo lp lq ju lr ls lt lu lv lw lx ly lz ma mb mc md me ij bi translated">现在，您知道了如何创建用于音频分类的CNN。从简单的模型开始，然后添加层，直到您开始看到训练数据的表现优于测试数据的迹象。添加Dropout和Max池层以防止过度拟合。最后，当您注意到与训练数据相比，验证数据的性能下降时，停止迭代。</p><h1 id="74c3" class="kr ks iq bd kt ku kv kw kx ky kz la lb jw lc jx ld jz le ka lf kc lg kd lh li bi translated">快乐造型！</h1><h2 id="88aa" class="mx ks iq bd kt nu nv dn kx nw nx dp lb ls ny nz ld lw oa ob lf ma oc od lh oe bi translated">来源</h2><p id="7df9" class="pw-post-body-paragraph lj lk iq ll b lm ln jr lo lp lq ju lr ls lt lu lv lw lx ly lz ma mb mc md me ij bi translated"><strong class="ll ir">萨卡尔，迪潘坚</strong> (2021)个人通信。</p><p id="4fb0" class="pw-post-body-paragraph lj lk iq ll b lm mf jr lo lp mg ju lr ls mh lu lv lw mi ly lz ma mj mc md me ij bi translated"><strong class="ll ir"> Chollet，F. </strong>用Python进行深度学习(2018)，v. 361，纽约州:曼宁。</p><p id="cea7" class="pw-post-body-paragraph lj lk iq ll b lm mf jr lo lp mg ju lr ls mh lu lv lw mi ly lz ma mj mc md me ij bi translated"><strong class="ll ir"> Gervias，Nicolas，</strong> (2021) <strong class="ll ir"> </strong>代码摘自<a class="ae mq" href="https://stackoverflow.com/questions/59241216/padding-numpy-arrays-to-a-specific-size" rel="noopener ugc nofollow" target="_blank">https://stack overflow . com/questions/59241216/padding-numpy-arrays-to-a-specific-size</a>，2021年1月10日检索。</p><p id="7a10" class="pw-post-body-paragraph lj lk iq ll b lm mf jr lo lp mg ju lr ls mh lu lv lw mi ly lz ma mj mc md me ij bi translated"><strong class="ll ir">frenzykryger</strong>(2021)<a class="ae mq" href="https://datascience.stackexchange.com/questions/41921/sparse-categorical-crossentropy-vs-categorical-crossentropy-keras-accuracy#:~:text=Use%20sparse%20categorical%20crossentropy%20when,0.5%2C%200.3%2C%200.2%5D" rel="noopener ugc nofollow" target="_blank">https://data science . stack exchange . com/questions/41921/sparse-category-cross entropy-vs-category-cross entropy-keras-accuracy #:~:text = Use % 20 sparse % 20 category % 20 cross entropy % 20 when，0.5%2C%200.3%2C%200.2%5D </a>，2021年2月21日检索</p></div></div>    
</body>
</html>