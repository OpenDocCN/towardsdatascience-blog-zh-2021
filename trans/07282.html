<html>
<head>
<title>How causal inference lifts augmented analytics beyond flatland</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">因果推理如何将增强分析提升到平地之外</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/how-causal-inference-lifts-augmented-analytics-beyond-flatland-95648fe30055?source=collection_archive---------10-----------------------#2021-07-02">https://towardsdatascience.com/how-causal-inference-lifts-augmented-analytics-beyond-flatland-95648fe30055?source=collection_archive---------10-----------------------#2021-07-02</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="4505" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsdatascience.com/tagged/notes-from-industry" rel="noopener" target="_blank">行业笔记</a></h2><div class=""/><div class=""><h2 id="ff96" class="pw-subtitle-paragraph jw iz iq bd b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn dk translated"><em class="ko">因果推理技术和商业分析方法揭示了什么真正改变了你的KPI，而预测建模却失败了。</em></h2></div><p id="5252" class="pw-post-body-paragraph kp kq iq kr b ks kt ka ku kv kw kd kx ky kz la lb lc ld le lf lg lh li lj lk ij bi translated">如果世界是二维的，生活将会非常奇怪。想想看:地球将不是一个球体，而是一个圆形——就像2D宇宙中的所有其他恒星和行星一样。生物也会变平，在平面景观和存在中航行。例如，在街上超过某人，你必须跳过那个人，因为没有任何深度。出于同样的原因，仅仅看你的身后，你真的得把自己翻个底朝天。幸运的是，这不是我们生活的世界。但不幸的是，这是当今大多数企业运行的基础——甚至可能是您的企业。在任何技术驱动的业务中，您的决策质量不可避免地基于您的数据洞察质量。然而，在太多的公司里，这些“见解”实际上是二维的:平淡、不切实际、毫无结果。</p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><div class="gh gi ll"><img src="../Images/92b8aebc3302cef41c57d708c6a20a62.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7v_FbcIMYWYCa0vNkPssOg.png"/></div></div><p class="lx ly gj gh gi lz ma bd b be z dk translated">超级马里奥</p></figure><p id="b181" class="pw-post-body-paragraph kp kq iq kr b ks kt ka ku kv kw kd kx ky kz la lb lc ld le lf lg lh li lj lk ij bi translated">企业通常用KPI来衡量他们的表现。因此，在给定历史数据的情况下，找到未来KPI值的最佳预测模型已成为数据分析的目标。尽管这些模型的表现可能出人意料地好，但从中提取价值同样困难。除了缺乏可解释性，这也是因为预测模型无法捕捉现实，并且仅限于低维解释。在本文中，我们将基于大多数预测模型中糟糕的缩放和不切实际的假设，给出为什么会出现这种情况的两个论据。</p><p id="bbbf" class="pw-post-body-paragraph kp kq iq kr b ks kt ka ku kv kw kd kx ky kz la lb lc ld le lf lg lh li lj lk ij bi translated">但何必呢？并不是表现好的模式提高了经营业绩。相反，改善企业的唯一途径是通过决策，而决策最终应该由人来完成。商业中数据分析的目标应该是通过揭示洞察力来为决策提供信息。不幸的是，这些就像大海捞针一样藏在你的数据里。这个远非无足轻重的问题激发了数据分析的一个相对年轻的分支——增强分析，并在最近的Gartner报告中得到推动[1]。</p><p id="4bb4" class="pw-post-body-paragraph kp kq iq kr b ks kt ka ku kv kw kd kx ky kz la lb lc ld le lf lg lh li lj lk ij bi translated">我们想要挑战预测模型应该是用于通知商业决策的默认选项的观念。他们在寻求洞察力的过程中引入了一条代价高昂的弯路，甚至可能使其实际上不可行。我们将在一个简单的问题中强调，除了巨大的开销之外，预测建模只能提供很少的东西。相反，我们将试图模仿一个业务分析师是如何操作的。这自然会把我们带到因果推论的方法。</p><h1 id="f462" class="mb mc iq bd md me mf mg mh mi mj mk ml kf mm kg mn ki mo kj mp kl mq km mr ms bi translated">有影响的变化</h1><p id="4dab" class="pw-post-body-paragraph kp kq iq kr b ks mt ka ku kv mu kd kx ky mv la lb lc mw le lf lg mx li lj lk ij bi translated">我们将考虑在大数据场景下诊断回归模型中的错误的问题。大多数读者应该都遇到过以下场景:</p><figure class="lm ln lo lp gt lq"><div class="bz fp l di"><div class="my mz l"/></div><p class="lx ly gj gh gi lz ma bd b be z dk translated">图1</p></figure><p id="121d" class="pw-post-body-paragraph kp kq iq kr b ks kt ka ku kv kw kd kx ky kz la lb lc ld le lf lg lh li lj lk ij bi translated">显然，KPI发生了重大变化，这种变化似乎会持续一段时间。从技术面来看，合理的反应是在转折点之后，根据数据重新训练你的预测模型。你同意吗？如果是这样，也许要记住这一点。</p><figure class="lm ln lo lp gt lq"><div class="bz fp l di"><div class="na mz l"/></div><p class="lx ly gj gh gi lz ma bd b be z dk translated">图2</p></figure><p id="1f00" class="pw-post-body-paragraph kp kq iq kr b ks kt ka ku kv kw kd kx ky kz la lb lc ld le lf lg lh li lj lk ij bi translated">好消息是你的模型似乎是准确的。坏消息是，你的经理不可避免地会问KPI怎么了。但是不要害怕。这是证明你对公司价值的理想情况。你能找出这种变化背后的原因吗？你能发现有助于正确决策的见解吗？</p><h1 id="bc6d" class="mb mc iq bd md me mf mg mh mi mj mk ml kf mm kg mn ki mo kj mp kl mq km mr ms bi translated">对为什么的探索</h1><p id="e50c" class="pw-post-body-paragraph kp kq iq kr b ks mt ka ku kv mu kd kx ky mv la lb lc mw le lf lg mx li lj lk ij bi translated">假设您公司的数据集如下所示:</p><pre class="lm ln lo lp gt nb nc nd ne aw nf bi"><span id="217d" class="ng mc iq nc b gy nh ni l nj nk">import pandas as pd<br/>df = pd.read_csv(“..\\dataset\\ecommerce_sample.csv”)<br/>df.head()</span></pre><figure class="lm ln lo lp gt lq"><div class="bz fp l di"><div class="nl mz l"/></div></figure><p id="116c" class="pw-post-body-paragraph kp kq iq kr b ks kt ka ku kv kw kd kx ky kz la lb lc ld le lf lg lh li lj lk ij bi translated">让我们进一步假设您正在处理一个最简单的情况:跳跃前后的数据点的KPI值使用线性回归模型完全拟合。这里，您正在处理分类数据，您需要适当地处理这些数据以便在回归中使用。一种标准的方法是对分类值进行一次性编码:对于每个分类值，您引入一个可以是真或假的特性。例如，在上面的数据集中，您将定义特性<em class="nm">customer _ country = = Germany</em>。为了最终实现特征选择，有必要使用一种正则化形式。这里，你将使用套索正则化(十重交叉验证)。</p><p id="6ff5" class="pw-post-body-paragraph kp kq iq kr b ks kt ka ku kv kw kd kx ky kz la lb lc ld le lf lg lh li lj lk ij bi translated">在训练两个Lasso正则化线性回归模型(一个在跳跃之前，一个在跳跃之后)后，您可以查看这些模型之间的要素权重差异的排序列表。</p><pre class="lm ln lo lp gt nb nc nd ne aw nf bi"><span id="90d7" class="ng mc iq nc b gy nh ni l nj nk">from sklearn.linear_model import LassoCV<br/>from bokeh.io import show<br/>from bokeh.plotting import figure</span><span id="bdd1" class="ng mc iq nc b gy nn ni l nj nk">#get kpi_axis<br/>kpi_axis = ‘kpi’<br/>time_axis = ‘time’<br/>df[time_axis] = pd.to_datetime(df[time_axis],format = ‘%d/%m/%Y’)<br/>y_before = df[df[time_axis] &lt;= ‘2019–09–11’][kpi_axis]<br/>y_after = df[df[time_axis] &gt; ‘2019–09–11’][kpi_axis]</span><span id="eb50" class="ng mc iq nc b gy nn ni l nj nk">#one-hot encoding categorical features<br/>for col in df.drop([kpi_axis,time_axis],axis=1).columns:<br/>   one_hot = pd.get_dummies(df[col])<br/>   df = df.drop(col,axis = 1)<br/>   df = df.join(one_hot)</span><span id="33be" class="ng mc iq nc b gy nn ni l nj nk">X_before = df[df[time_axis] &lt;= ‘2019–09 11’].drop([kpi_axis,time_axis],axis = 1).to_numpy()<br/>X_after = df[df[time_axis] &gt; ‘2019–09–11’].drop([kpi_axis,time_axis],axis = 1).to_numpy()</span><span id="882d" class="ng mc iq nc b gy nn ni l nj nk">#training left and right<br/>regression_model_before = LassoCV(cv = 10)<br/>regression_model_after = LassoCV(cv = 10)<br/>regression_model_before.fit(X_before,y_before)<br/>regression_model_after.fit(X_after,y_after)</span><span id="7e94" class="ng mc iq nc b gy nn ni l nj nk">#plotting results<br/>features = df.columns<br/>dweights =regression_model_after — regression_model_before<br/>index = np.argsort(-abs(dweights))<br/>x_axis = features[index[0:3]].to_list()</span><span id="3f31" class="ng mc iq nc b gy nn ni l nj nk">p = figure(x_range=x_axis,title = “Feature weights difference”,plot_width=1000)<br/>p.vbar(x=x_axis, top=(abs(dweights[index[0:3]])),width = 0.8)<br/>show(p)</span></pre><figure class="lm ln lo lp gt lq"><div class="bz fp l di"><div class="no mz l"/></div><p class="lx ly gj gh gi lz ma bd b be z dk translated">图3</p></figure><p id="ca38" class="pw-post-body-paragraph kp kq iq kr b ks kt ka ku kv kw kd kx ky kz la lb lc ld le lf lg lh li lj lk ij bi translated">看起来，android用户或46岁以上用户等子群体在跳跃前后的表现有所不同。很好，看起来你找到了KPI上升的原因…或者你找到了吗？</p><h1 id="e6b3" class="mb mc iq bd md me mf mg mh mi mj mk ml kf mm kg mn ki mo kj mp kl mq km mr ms bi translated">维度的诅咒</h1><p id="c3c9" class="pw-post-body-paragraph kp kq iq kr b ks mt ka ku kv mu kd kx ky mv la lb lc mw le lf lg mx li lj lk ij bi translated">事实上，这是一个比我们迄今所理解的更重要的情况。想象一下将此展示给KPI负责人。他们会很高兴你向他们提供了KPI变化的原因，他们现在会想知道根据这些信息该做些什么。这将自动引导他们提出如下问题:“<em class="nm">KPI变化的实际驱动因素是所有android-tv客户、所有46岁以上的客户以及所有之前购买过产品的客户吗？也许是46岁以上的回头客和安卓电视用户…或者是以前买过东西的安卓电视用户？更糟糕的是，有没有其他你错过的功能组合？</em></p><p id="4456" class="pw-post-body-paragraph kp kq iq kr b ks kt ka ku kv kw kd kx ky kz la lb lc ld le lf lg lh li lj lk ij bi translated">因此，为了能够更有信心地回答这些问题，您必须使用更复杂的一次性编码特征重复您的回归分析…现在表示比以前更精细的子群。因此，您可以在数据集的更深的子组中进行搜索，参见图4，新功能包括<em class="nm"> customer_age == 46+和first_order_made == yes、customer _ age = = 18–21和first_order_made == no. </em></p><p id="74ed" class="pw-post-body-paragraph kp kq iq kr b ks kt ka ku kv kw kd kx ky kz la lb lc ld le lf lg lh li lj lk ij bi translated">同样，这些子群通过一键编码进入。这显然是有问题的，因为你现在成为了<em class="nm">维度诅咒</em>的受害者。这是一个大数据时代，你只是增加了你的功能数量的阶乘[2]。可以用来生成这些细化的子组的一段代码是</p><pre class="lm ln lo lp gt nb nc nd ne aw nf bi"><span id="56e2" class="ng mc iq nc b gy nh ni l nj nk">def binarize(df,cols,kpi_axis,time_axis,order):<br/>   cols = cols.drop([kpi_axis,time_axis])<br/>   features = []<br/>   for k in range(0,order):<br/>      features.append(cols)<br/>   fs = []<br/>   for f in itertools.product(*features):<br/>    # list(set(f)).sort()<br/>      f = np.unique(f)<br/>      fs.append(tuple(f))<br/>   fs = tuple(set(i for i in fs))<br/>   print(fs)<br/>   for f in fs:<br/>      print(len(f))<br/>      states =[]<br/>      for d in f:<br/>         states.append(tuple(set(df[d].astype(‘category’))))<br/>      for state in itertools.product(*states):<br/>         z = 1<br/>         name = str()<br/>         for d in range(0,len(f)):<br/>            z = z*df[f[d]]==state[d]<br/>            name += f[d] + “ == “ +str(state[d])<br/>            if d&lt;len(f)-1:<br/>               name += “ AND “<br/>         df[name] = z<br/>   for d in cols:<br/>      df = df.drop([d],axis = 1)<br/>   return df</span></pre><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><div class="gh gi np"><img src="../Images/1c110bbaf30fffeef3577e3424536afb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AXLHc45Ll77VYlB3UzoYdA.png"/></div></div><p class="lx ly gj gh gi lz ma bd b be z dk translated">图4 —子组深度</p></figure><p id="e057" class="pw-post-body-paragraph kp kq iq kr b ks kt ka ku kv kw kd kx ky kz la lb lc ld le lf lg lh li lj lk ij bi translated">请记住，线性回归是基于所有要素之间的协方差矩阵的反演，其标度为O(d ), d是要素的数量，也就是说，在我们的情况下，是可能的子组的数量。与非预测特征选择方法相比，这引入了显著的机会成本——这将在后面讨论。</p><pre class="lm ln lo lp gt nb nc nd ne aw nf bi"><span id="0673" class="ng mc iq nc b gy nh ni l nj nk">df = pd.read_csv(“..\\dataset\\ecommerce_sample.csv”)<br/>df[time_axis] = pd.to_datetime(df[time_axis],format = ‘%d/%m/%Y’)</span><span id="f30a" class="ng mc iq nc b gy nn ni l nj nk">#get kpi_axis<br/>kpi_axis = ‘kpi’<br/>time_axis = ‘time’<br/>y_before = df[df[time_axis] &lt;= ‘2019–09–11’][kpi_axis]<br/>y_after = df[df[time_axis] &gt; ‘2019–09–11’][kpi_axis]</span><span id="0e99" class="ng mc iq nc b gy nn ni l nj nk">#one-hot encoding categorical features<br/>df = binarize(df,df.columns,kpi_axis,time_axis,3)<br/>X_before = df[df[time_axis] &lt;= ‘2019–09–11’].drop([kpi_axis,time_axis],axis = 1).to_numpy()<br/>X_after = df[df[time_axis] &gt; ‘2019–09–11’].drop([kpi_axis,time_axis],axis = 1).to_numpy()</span><span id="c2b9" class="ng mc iq nc b gy nn ni l nj nk">#training left and right<br/>regression_model_before = LassoCV(cv = 10)<br/>regression_model_after = LassoCV(cv = 10)<br/>regression_model_before.fit(X_before,y_before)<br/>regression_model_after.fit(X_after,y_after)</span><span id="5732" class="ng mc iq nc b gy nn ni l nj nk">#plotting results<br/>features = df.columns<br/>dweights =regression_model_after — regression_model_before<br/>index = np.argsort(-abs(dweights))<br/>x_axis = features[index[0:3]].to_list()</span><span id="65b4" class="ng mc iq nc b gy nn ni l nj nk">p = figure(x_range=x_axis,title = “Feature weights difference”,plot_width=1000)<br/>p.vbar(x=x_axis, top=(abs(dweights[index[0:3]])),width = 0.8)<br/>show(p)</span></pre><figure class="lm ln lo lp gt lq"><div class="bz fp l di"><div class="nq mz l"/></div><p class="lx ly gj gh gi lz ma bd b be z dk translated">图5</p></figure><p id="e28c" class="pw-post-body-paragraph kp kq iq kr b ks kt ka ku kv kw kd kx ky kz la lb lc ld le lf lg lh li lj lk ij bi translated">一段时间后，你的计算完成。虽然你之前的计算只花了0.1秒，但搜索三阶特征已经花了一分多钟。但似乎是值得的。您发现推动KPI变化的组的数量实际上是一个，见图5。向您的经理展示这一见解，他可以迅速指出直接影响您报告的子组的更新。</p><blockquote class="nr ns nt"><p id="de7a" class="kp kq nm kr b ks kt ka ku kv kw kd kx nu kz la lb nv ld le lf nw lh li lj lk ij bi translated">-通过细化子组，您可以使其具有可操作性。</p></blockquote><p id="71e8" class="pw-post-body-paragraph kp kq iq kr b ks kt ka ku kv kw kd kx ky kz la lb lc ld le lf lg lh li lj lk ij bi translated">虽然您的回归方法最终奏效了，但计算时间非常长，导致您公司的机会成本。在真实的大数据场景中，你的方法会失败得很惨。此外，仅包含浅子群的原始集合描绘了不正确的画面。只有在精炼集合和巨大的计算工作之后，你才能精确定位驱动KPI跳跃的实际子组。</p><p id="b029" class="pw-post-body-paragraph kp kq iq kr b ks kt ka ku kv kw kd kx ky kz la lb lc ld le lf lg lh li lj lk ij bi translated">这引发了几个问题:</p><ul class=""><li id="80ee" class="nx ny iq kr b ks kt kv kw ky nz lc oa lg ob lk oc od oe of bi translated">你真的需要学习一个预测模型来回答为什么会发生跳跃吗？</li><li id="37e0" class="nx ny iq kr b ks og kv oh ky oi lc oj lg ok lk oc od oe of bi translated">你如何降低机会成本？</li><li id="5ff4" class="nx ny iq kr b ks og kv oh ky oi lc oj lg ok lk oc od oe of bi translated">如何在适当的粒度级别找到子组？</li><li id="cb80" class="nx ny iq kr b ks og kv oh ky oi lc oj lg ok lk oc od oe of bi translated">为了这些信息，每次跳跃都要重新训练模型，这样经济吗？</li></ul><p id="eda2" class="pw-post-body-paragraph kp kq iq kr b ks kt ka ku kv kw kd kx ky kz la lb lc ld le lf lg lh li lj lk ij bi translated">虽然回答所有这些问题超出了这篇文章的范围，但我们将提供一个新的观点来帮助解决这些问题。为此，我们将开发一种改进线性回归的特征选择方法。增强分析依赖于它。</p><h1 id="7895" class="mb mc iq bd md me mf mg mh mi mj mk ml kf mm kg mn ki mo kj mp kl mq km mr ms bi translated">从商业分析师和因果推理中学习</h1><p id="5c29" class="pw-post-body-paragraph kp kq iq kr b ks mt ka ku kv mu kd kx ky mv la lb lc mw le lf lg mx li lj lk ij bi translated">我们退一步说…这里发生了什么？您从一个预测模型开始，您看到它既不能预测也不能解释KPI中观察到的跳跃。这是为什么呢？因为预测模型无法捕捉现实。他们假设所有数据都是独立同分布的[3]。然而，在实际应用中，这通常是不正确的，如本例所示。跳跃前后的数据是在不同的条件下产生的。当你使用两个独立的预测模型时，你甚至直觉地利用了这一事实，这两个模型(在一些技巧之后)帮助我们揭示了跳跃的原因。</p><p id="ad90" class="pw-post-body-paragraph kp kq iq kr b ks kt ka ku kv kw kd kx ky kz la lb lc ld le lf lg lh li lj lk ij bi translated">当你不得不放弃预测，最终没有预测到任何东西时，预测模型实际上为你做了什么？仔细想想，关键是您对预测KPI作为所有可能子组的函数不感兴趣—您感兴趣的是影响KPI的子组！因此，为了寻找更深层次的见解，你必须远离预测建模。这是数据科学家可以向业务分析师学习的地方。</p><p id="e440" class="pw-post-body-paragraph kp kq iq kr b ks kt ka ku kv kw kd kx ky kz la lb lc ld le lf lg lh li lj lk ij bi translated">业务分析师通过包含有意义的数据摘要的仪表板来搜索见解。业务分析师不是像上面的回归方法那样将所有特性关联在一起，而是通过迭代过滤不同条件下的数据，尝试根据汇总(如均值、直方图或度量)来查明数据中发生了什么变化。最重要的是，业务分析师永远不必一次查看所有特性。你如何教一台机器做到这一点？你如何向商业分析师学习？</p><p id="54af" class="pw-post-body-paragraph kp kq iq kr b ks kt ka ku kv kw kd kx ky kz la lb lc ld le lf lg lh li lj lk ij bi translated">让我们用数学符号将上述形式化。设X是一个子组，例如<em class="nm"> X = customer_age == 46+和first_order_made == yes </em>和</p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div class="gh gi ol"><img src="../Images/6e50a12fb248f69204debb77b1622d12.png" data-original-src="https://miro.medium.com/v2/resize:fit:328/1*aM2Y63NJe0T0aLhzlSNcrQ.gif"/></div></figure><p id="5fff" class="pw-post-body-paragraph kp kq iq kr b ks kt ka ku kv kw kd kx ky kz la lb lc ld le lf lg lh li lj lk ij bi translated">KPI跳跃前后KPI分布的一些摘要。然后，引入条件汇总</p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div class="gh gi om"><img src="../Images/44ef92cf99b17d5b31648b416b3a9038.png" data-original-src="https://miro.medium.com/v2/resize:fit:392/1*eA06k7A83mlGLXbg7JUOkw.gif"/></div></figure><p id="4c86" class="pw-post-body-paragraph kp kq iq kr b ks kt ka ku kv kw kd kx ky kz la lb lc ld le lf lg lh li lj lk ij bi translated">其中计算X为真的KPI值子集的汇总。我们的方法现在需要做的就是计算每个子组的条件汇总，并对它们进行排序。我想强调的是，在实践中，这些抽象的总结可以是作为手段的对象，直方图等等</p><p id="de2e" class="pw-post-body-paragraph kp kq iq kr b ks kt ka ku kv kw kd kx ky kz la lb lc ld le lf lg lh li lj lk ij bi translated">上面详述的过程实际上是因果推理的一种常用技术[4]。你因此含蓄地改变了我们的观点。现在，你认为KPI中的神秘跳跃是一种干预，现在假设这是由于外部或内部<em class="nm">治疗</em>而发生的。一个外部处理的例子可能是假期，一个内部处理可能是一个广告活动，一个价格的变化，或者，在我们的例子中，一个软件更新。因此，你显然<em class="nm">解除了</em>所有数据都是独立同分布的错误假设。您现在正在搜索KPI变化的<em class="nm">因果</em>子组。</p><h1 id="c38b" class="mb mc iq bd md me mf mg mh mi mj mk ml kf mm kg mn ki mo kj mp kl mq km mr ms bi translated">对为什么的探索——再访</h1><p id="9ef5" class="pw-post-body-paragraph kp kq iq kr b ks mt ka ku kv mu kd kx ky mv la lb lc mw le lf lg mx li lj lk ij bi translated">现在您已经有了一个业务分析师如何操作的模型，让我们继续实际的实现。现在，你将使用因果推断中使用的标准总结，称为条件平均处理效应(CATE) [4]，我们的总结成为</p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div class="gh gi on"><img src="../Images/5ceac93fa470df8571f1619503d8fbab.png" data-original-src="https://miro.medium.com/v2/resize:fit:954/1*rRA8u7gok0BAYBo8kX8cyg.gif"/></div></figure><p id="ea91" class="pw-post-body-paragraph kp kq iq kr b ks kt ka ku kv kw kd kx ky kz la lb lc ld le lf lg lh li lj lk ij bi translated">CATE对应于KPI平均值的变化，条件是子组X为真。然后，通过数量级排序，我们可以得到正确的子群。为了检测多个子组，我们在每次迭代后移除表现最好的子组后重复该过程:</p><pre class="lm ln lo lp gt nb nc nd ne aw nf bi"><span id="7af6" class="ng mc iq nc b gy nh ni l nj nk">df = pd.read_csv(“..\\dataset\\ecommerce_sample.csv”)<br/>df[time_axis] = pd.to_datetime(df[time_axis],format = ‘%d/%m/%Y’)</span><span id="7bff" class="ng mc iq nc b gy nn ni l nj nk">#get kpi_axis<br/>kpi_axis = ‘kpi’<br/>time_axis = ‘time’<br/>y_before = df[df[time_axis] &lt;= ‘2019–09–11’][kpi_axis]<br/>y_after = df[df[time_axis] &gt; ‘2019–09–11’][kpi_axis]</span><span id="9788" class="ng mc iq nc b gy nn ni l nj nk">df = binarize(df,df.columns,kpi_axis,time_axis,3)<br/>df_before = df[df[time_axis] &lt;= ‘2019–09–11’]<br/>df_after = df[df[time_axis] &gt; ‘2019–09–11’]<br/>features = copy(df.drop([time_axis,kpi_axis], axis=1).columns)</span><span id="ac6f" class="ng mc iq nc b gy nn ni l nj nk">K = 3 #number of subgroups to detect<br/>subgroups=[]<br/>score=[]<br/>for k in range(0,K):<br/>   CATE = []<br/>   y_before = df_before[kpi_axis]<br/>   y_after= df_after[kpi_axis]</span><span id="ddd2" class="ng mc iq nc b gy nn ni l nj nk">#compute CATEs for all subgroups<br/>   for d in features:<br/>      g = df_before[d] == True<br/>      m_before = np.mean(y_before[g])<br/>      g = df_after[d] == True<br/>      m_after = np.mean(y_after[g])<br/>      CATE.append(m_after-m_before)</span><span id="2962" class="ng mc iq nc b gy nn ni l nj nk">#find subgroup with biggest CATE<br/>   index = np.argsort(-abs(np.array(CATE)))<br/>   subgroups.append(features[index[0]])<br/>   score.append(abs( CATE [index[0]]))</span><span id="beca" class="ng mc iq nc b gy nn ni l nj nk">#remove found subgroups from dataset<br/>   df_before = df_before[df_before[features[index[0]]] == False]<br/>   df_after = df_after[df_after[features[index[0]]] == False]<br/>   features = features.drop(features[index[0]])</span><span id="e95c" class="ng mc iq nc b gy nn ni l nj nk">p = figure(x_range=subgroups,title = “Conditional Average Treatment Effect”,plot_width=1200,)<br/>p.vbar(x=subgroups, top=score,width = 0.8,color=’black’)<br/>show(p)</span></pre><figure class="lm ln lo lp gt lq"><div class="bz fp l di"><div class="oo mz l"/></div><p class="lx ly gj gh gi lz ma bd b be z dk translated">图6</p></figure><p id="00cd" class="pw-post-body-paragraph kp kq iq kr b ks kt ka ku kv kw kd kx ky kz la lb lc ld le lf lg lh li lj lk ij bi translated">这只是我们预测模型成本的一小部分。计算一阶特征只需要0.02秒，搜索三阶特征不到一秒。</p><p id="3fe7" class="pw-post-body-paragraph kp kq iq kr b ks kt ka ku kv kw kd kx ky kz la lb lc ld le lf lg lh li lj lk ij bi translated">让我们后退一步，将这种方法与基于回归的早期方法进行比较，并比较它们各自的目标。通过回归的特征选择回答了这个问题:“哪些子群最好地预测了你的KPI？”。而采用因果推理的观点回答了这个问题:“哪些子群对KPI有最大的因果影响？”。比较CATE的简单实现与图7中线性回归的优化<em class="nm"> sklearn </em>实现的运行时间，我们发现它们的数量级不同。这清楚地表明，这些问题虽然表面上相似，但有着根本的区别。</p><figure class="lm ln lo lp gt lq"><div class="bz fp l di"><div class="na mz l"/></div><p class="lx ly gj gh gi lz ma bd b be z dk translated">图7</p></figure><h1 id="b3da" class="mb mc iq bd md me mf mg mh mi mj mk ml kf mm kg mn ki mo kj mp kl mq km mr ms bi translated">结论</h1><p id="da48" class="pw-post-body-paragraph kp kq iq kr b ks mt ka ku kv mu kd kx ky mv la lb lc mw le lf lg mx li lj lk ij bi translated">预测模型作为理解KPI变化的手段有很大的缺点，尤其是在多维环境中。这些模型从根本上回答了错误假设下的错误问题。相反，商业分析关注的是事情为什么会发生，而不是将会发生什么。将他们的思想从预测未来KPI值的辅助任务中解放出来，分析人员在数据中寻找原因，以了解KPI为什么会发生变化，试图找到正确问题的答案。</p><p id="3563" class="pw-post-body-paragraph kp kq iq kr b ks kt ka ku kv kw kd kx ky kz la lb lc ld le lf lg lh li lj lk ij bi translated">下次你想解释什么的时候要小心。首先，你应该问正确的问题。此外，多维环境需要基于因果推理和业务分析方法的可扩展技术。这是我们在<a class="ae op" href="http://www.kausa.ai" rel="noopener ugc nofollow" target="_blank"> Kausa </a>的使命:扩展业务分析逻辑，并将其与因果推理相结合，为KPI变化提供正确的答案。</p><p id="7fb6" class="pw-post-body-paragraph kp kq iq kr b ks kt ka ku kv kw kd kx ky kz la lb lc ld le lf lg lh li lj lk ij bi translated"><em class="nm"> PS:复制本文结果的代码和数据可从[6]获得。</em></p><h1 id="9585" class="mb mc iq bd md me mf mg mh mi mj mk ml kf mm kg mn ki mo kj mp kl mq km mr ms bi translated">关于作者和Kausa</h1><p id="005c" class="pw-post-body-paragraph kp kq iq kr b ks mt ka ku kv mu kd kx ky mv la lb lc mw le lf lg mx li lj lk ij bi translated">迈克尔·克拉普特是联合创始人兼首席技术官，张秀坤·林兹纳是Kausa的因果推理工程师。</p><p id="54bb" class="pw-post-body-paragraph kp kq iq kr b ks kt ka ku kv kw kd kx ky kz la lb lc ld le lf lg lh li lj lk ij bi translated"><a class="ae op" href="http://www.kausa.ai" rel="noopener ugc nofollow" target="_blank"> <em class="nm">考萨</em> </a> <em class="nm">通过全面持续地测试所有假设，加速数据探索，在几秒钟内提供可操作的见解。</em> <strong class="kr ja"> <em class="nm">对试用产品感兴趣？</em> </strong> <a class="ae op" href="http://www.kausa.ai" rel="noopener ugc nofollow" target="_blank"> <strong class="kr ja"> <em class="nm">报名提前入场排队</em> </strong> </a> <strong class="kr ja"> <em class="nm">。</em>T19】</strong></p><h1 id="8424" class="mb mc iq bd md me mf mg mh mi mj mk ml kf mm kg mn ki mo kj mp kl mq km mr ms bi translated">参考</h1><ol class=""><li id="5150" class="nx ny iq kr b ks mt kv mu ky oq lc or lg os lk ot od oe of bi translated"><a class="ae op" href="https://www.gartner.com/en/doc/441420-how-augmented-analytics-will-transform-your-organization" rel="noopener ugc nofollow" target="_blank">增强分析将如何改变您的组织</a></li><li id="aa23" class="nx ny iq kr b ks og kv oh ky oi lc oj lg ok lk ot od oe of bi translated"><a class="ae op" href="https://medium.com/swlh/stop-one-hot-encoding-your-categorical-features-avoid-curse-of-dimensionality-16743c32cea4" rel="noopener">停止一次性编码你的分类特征——避免维数灾难</a></li><li id="3c79" class="nx ny iq kr b ks og kv oh ky oi lc oj lg ok lk ot od oe of bi translated"><a class="ae op" href="http://arxiv.org/abs/1911.10500" rel="noopener ugc nofollow" target="_blank">schlkopf，B. (2019年)。机器学习的因果关系。1–20.</a></li><li id="d4f2" class="nx ny iq kr b ks og kv oh ky oi lc oj lg ok lk ot od oe of bi translated"><a class="ae op" href="http://proceedings.mlr.press/v67/gutierrez17a/gutierrez17a.pdf" rel="noopener ugc nofollow" target="_blank">古铁雷斯，p .&amp;热拉尔迪，J.-Y. (2016)。因果推理和隆起建模文献综述(第67卷)</a></li><li id="b037" class="nx ny iq kr b ks og kv oh ky oi lc oj lg ok lk ot od oe of bi translated"><a class="ae op" rel="noopener" target="_blank" href="/be-careful-when-interpreting-predictive-models-in-search-of-causal-insights-e68626e664b6">在解释预测模型以寻求因果洞察力时要小心</a></li><li id="86ac" class="nx ny iq kr b ks og kv oh ky oi lc oj lg ok lk ot od oe of bi translated"><a class="ae op" href="https://github.com/kausa-ai/blog/tree/master/how_causal_inference_lifts_augmented_analytics_beyond_flatland" rel="noopener ugc nofollow" target="_blank">电子商务数据集</a></li></ol></div></div>    
</body>
</html>