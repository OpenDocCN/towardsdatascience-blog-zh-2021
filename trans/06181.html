<html>
<head>
<title>Why You Need to Stop Using Bulky Neural Network Models</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">为什么你需要停止使用庞大的神经网络模型</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/why-you-need-to-stop-using-bulky-neural-network-models-d43681d9916f?source=collection_archive---------36-----------------------#2021-06-02">https://towardsdatascience.com/why-you-need-to-stop-using-bulky-neural-network-models-d43681d9916f?source=collection_archive---------36-----------------------#2021-06-02</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="4109" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">…而是提取他们的知识。</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/7820d33134a61a2091d83cc99cdd7352.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*pT_Sgh3klmdxJzl4"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><a class="ae ky" href="https://unsplash.com/@nci?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">国家癌症研究所</a>在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="d727" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在大学期间，一个人会接触到各种各样的人。在所有的种类中，我特别羡慕一种特殊的类型。这些人整个学期都不会学习。在考试的前一天，他们会下载一个高水平的主题，最后在考试中创建一个有针对性的答案视图，以成功通过考试。</p><p id="6210" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我一直以为他们是被祝福的。拥有从高水平的学习中提取精华的能力，以及将这种能力转化为具体观点的能力，简直令人钦佩。</p><p id="a724" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我永远做不到，甚至没有勇气去尝试。</p><p id="9a0c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">最近，在我遇到的一个基于学生和教师的框架中，感觉就像一个赵雨菁。一次具有相同个性的替代性会面。</p><p id="3504" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">简单来说，在学生-教师框架中，学生模型(一个轻型神经网络)从教师模型(一个复杂的神经网络)中学习“提炼”的知识。这导致“轻”学生模型具有“重”教师模型的知识，具有相似或稍低的准确度水平。</p><p id="d922" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在这篇文章中，我们将讨论对这样一个框架的需求，蒸馏过程是如何工作的，以及最后学生模型是如何被微调的。</p><p id="93fa" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">为什么是师生框架？</strong></p><p id="bf62" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="lv">“如果它不适合你的口袋，它就没有销路”</em></p><p id="d714" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">随着迁移学习模型和其他复杂神经网络(NN)模型的出现，模型的准确性显著提高。毫不奇怪，这是有代价的。多个神经网络层和众多参数通常会导致庞大的模型，这需要高计算能力来部署。这极大地限制了它们的使用和销售。</p><p id="2fc7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">显然需要具有相似精确度的更轻的模型。需要一种方法将这些庞大复杂的模型压缩成更小的尺寸，而不显著降低精度。</p><p id="4037" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这就是学生-教师框架的目标。</p><p id="5be1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">什么是师生框架，它是如何运作的？</strong></p><p id="318b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在学生-教师框架中，利用复杂的NN模型(新的或预训练的)来训练较轻的NN模型。从复杂的NN(老师)到较轻的NN(学生)传授知识的过程叫做<strong class="lb iu">“蒸馏”</strong></p><p id="cb47" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在深入了解蒸馏过程之前，理解两个术语非常重要:</p><ol class=""><li id="b914" class="lw lx it lb b lc ld lf lg li ly lm lz lq ma lu mb mc md me bi translated">Softmax温度:这是Softmax的调整版本。Softmax函数创建跨分类类别的模型输出的预期概率分布，确保概率总和为1。Softmax温度做同样的事情，但是另外它<em class="lv">将概率更多地分散在类别中。(见下图)扩散程度由参数“T”定义。T越高，概率在类别间的分布就越高。T = 1与正常的Softmax相同。</em></li></ol><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mf"><img src="../Images/265d16c6b056a1afbe13125f410b029e.png" data-original-src="https://miro.medium.com/v2/resize:fit:812/format:webp/1*vC5cgILqp1ftdQqvTP-LOQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><p id="45a5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">请注意，虽然与直觉相反，但在许多情况下，为了获得各种潜在的结果，“软”概率是首选的。例如，在文本生成中，人们期望有更大的潜在选项集来生成多样化的文本。</p><p id="3a9b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在蒸馏过程中，模型中不使用Softmax，而是使用Softmax温度。</p><p id="2a70" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">2.<strong class="lb iu">暗知识:</strong>在Softmax温度的例子中，由于Softmax参数“T”的变化，我们从概率分布中获得的所有“额外知识”被称为暗知识。换句话说，通过使用Softmax温度(T &gt; 1)而不是Softmax获得的增量知识是暗知识。</p><p id="dcf3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">蒸馏过程</strong>工作如下:</p><p id="0928" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">1.<strong class="lb iu">训练(或使用预训练)复杂(比方说10层)“老师”模型</strong>。不出所料，这是一个计算密集型步骤。教师模型的输出将是跨类别/标签分布的Softmax温度输出概率。</p><p id="3f35" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">2.<strong class="lb iu">以“学生层”的Softmax温度输出尽可能接近“教师”模型的方式，创建并训练一个更简单的(假设2层)“学生”模型</strong>。为此，学生和教师之间称为<strong class="lb iu">“蒸馏损失”</strong>的交叉熵损失应最小化。这就是所谓的“<strong class="lb iu">软瞄准”。通过这种方式，我们将“黑暗知识”从老师传递给学生。注意，这里的知识是跨各个层面传递的，而不仅仅是最后的损失。</strong></p><p id="6202" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这里出现的一个显而易见的问题是，我们为什么要瞄准软目标(一个展开的概率分布)。这是因为软目标比硬目标提供更多的信息。这导致更小的模型更有知识，从而需要更少的额外培训。</p><p id="6c02" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">3.同时，使用Softmax激活(不是Softmax温度)训练“学生”模型，以使模型输出尽可能接近实际值。<strong class="lb iu"> </strong>为此，学生和实际之间的交叉熵损失被称为<strong class="lb iu">“学生损失”</strong>被最小化。这叫<strong class="lb iu">“硬瞄准”。</strong></p><p id="1a9c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">4.对于#2和#3，<strong class="lb iu">最小化总损失- </strong>，即蒸馏损失和学生损失的加权和，<strong class="lb iu">。</strong></p><blockquote class="mg mh mi"><p id="cd98" class="kz la lv lb b lc ld ju le lf lg jx lh mj lj lk ll mk ln lo lp ml lr ls lt lu im bi translated"><strong class="lb iu"> <em class="it">损耗= a *学生损耗+ b *蒸馏损耗</em> </strong></p></blockquote><p id="980d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">概略地说，所提到的过程可以描述如下:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mm"><img src="../Images/ef1d28c1a3c12543a6b5bc66db0ac6c0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1138/format:webp/1*5nhplTXKFq3HTPPj-MjVwQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><p id="9011" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">结论</strong>:蒸馏(学生-教师框架)是一个很棒的模型无关工具，可以在没有太多计算能力的边缘设备上部署模型。虽然理论上该框架可以应用于任何大型神经网络模型；我个人已经看到它被用于一个6层CNN模型和蒸馏伯特。随着大量边缘设备的出现，我预计这个框架很快就会无处不在。</p><p id="f9e3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">快乐学习！！</p><p id="c58c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">PS:为那些帮助我为这篇文章动笔的了不起的大学名人干杯！</p></div><div class="ab cl mn mo hx mp" role="separator"><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms"/></div><div class="im in io ip iq"><p id="9c71" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="lv">免责声明:本文表达的观点是作者以个人身份发表的意见，而非其雇主的意见</em></p></div></div>    
</body>
</html>