<html>
<head>
<title>Why there is a problem with the Policy Gradient theorem in Deep Reinforcement Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">为什么深度强化学习中的策略梯度定理有问题</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/why-there-is-a-problem-with-the-policy-gradient-theorem-in-deep-reinforcement-learning-958d845218f1?source=collection_archive---------25-----------------------#2021-06-28">https://towardsdatascience.com/why-there-is-a-problem-with-the-policy-gradient-theorem-in-deep-reinforcement-learning-958d845218f1?source=collection_archive---------25-----------------------#2021-06-28</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="39bc" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">或者，我喜欢称之为“折扣因子的魔法”</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/205d936a1f0db6f5cf5c257d85e5aa13.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LKtEKhbvCm5pWB2X2pazXQ.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">伊卡洛斯号的飞行，b  y雅各布·彼得·戈威——【https://commons.wikimedia.org/w/index.php?curid=27493281】T2</p></figure><p id="0102" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">多亏了我的导师Proutiere教授，我第一次发现这个问题，是在检查我去年2020年秋季在KTH 举办的<a class="ae kw" href="https://www.kth.se/student/kurser/kurs/EL2805?l=en" rel="noopener ugc nofollow" target="_blank">强化学习(EL2805)课程第二次实验室会议的提案时。</a></p><p id="a11d" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated"><strong class="kz ir">这个问题严重到足以影响大多数深度强化学习算法，包括A3C [7]、SAC [8]、ACKTR算法[9]等。</strong></p><p id="2100" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">大致上，这是我的主管发给我的电子邮件:</p><blockquote class="lt"><p id="6652" class="lu lv iq bd lw lx ly lz ma mb mc ls dk translated">嘿，阿莱西奥，我有一个关于政策梯度定理的问题。平均值是关于μ的，μ是一个贴现态分布。但是，一般来说，人们如何证明我们使用由政策π诱导的静态政策分布产生的经验，而不是贴现分布μ？</p></blockquote><p id="6334" class="pw-post-body-paragraph kx ky iq kz b la md jr lc ld me ju lf lg mf li lj lk mg lm ln lo mh lq lr ls ij bi translated">为了充分理解他的意思，我和我的主管之间来回发送了几封电子邮件是必要的(<em class="mi">在我的辩护中，他的第一封电子邮件含糊不清，前面引用的句子是那封电子邮件的“澄清”版本:)</em>)。</p><p id="b276" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">如果你还不知道我在说什么，让我简单地提醒你一下政策梯度定理[1]是如何工作的。</p></div><div class="ab cl mj mk hu ml" role="separator"><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo"/></div><div class="ij ik il im in"><h1 id="56eb" class="mq mr iq bd ms mt mu mv mw mx my mz na jw nb jx nc jz nd ka ne kc nf kd ng nh bi translated">政策梯度定理</h1><p id="87aa" class="pw-post-body-paragraph kx ky iq kz b la ni jr lc ld nj ju lf lg nk li lj lk nl lm ln lo nm lq lr ls ij bi translated">尽管PG定理很简单，但它已经实现了我们目前看到的深度强化学习的许多成就。</p><p id="c691" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">基本想法如下:</p><blockquote class="lt"><p id="e0cc" class="lu lv iq bd lw lx ly lz ma mb mc ls dk translated">我们是否可以通过使用某个目标函数J(θ)相对于θ的梯度来改善由θ参数化的策略π的性能？然后，通过简单地使用随机梯度上升方法，我们可以根据该梯度改善策略的性能。</p></blockquote><p id="7389" class="pw-post-body-paragraph kx ky iq kz b la md jr lc ld me ju lf lg mf li lj lk mg lm ln lo mh lq lr ls ij bi translated">为了定义目标函数J(θ),通常使用期望总折扣报酬标准</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/7b5c6085623c3e0bb2a3a2302660efa7.png" data-original-src="https://miro.medium.com/v2/resize:fit:876/format:webp/1*cnhk_do5a6G3_qRa_eN8Ww.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">由θ参数化的政策π的期望报酬标准。</p></figure><p id="2550" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">简要描述:</p><ol class=""><li id="3776" class="no np iq kz b la lb ld le lg nq lk nr lo ns ls nt nu nv nw bi translated"><strong class="kz ir">第一个期望就是保单的贴现值π，</strong> <strong class="kz ir">，而第二个期望是第一个期望的对偶形式</strong>(第一个考虑时间，第二个考虑状态-行为空间<em class="mi">；</em>查看<a class="ae kw" href="https://en.wikipedia.org/wiki/Ergodic_theory" rel="noopener ugc nofollow" target="_blank">遍历定理</a>更好的理解这种对偶性)。</li><li id="f60e" class="no np iq kz b la nx ld ny lg nz lk oa lo ob ls nt nu nv nw bi translated"><strong class="kz ir">在第二个期望中，我们对策略π诱导的贴现状态分布μ取一个期望。</strong></li></ol><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oc"><img src="../Images/ba83b7a9486cc810a5131299f3cdb25b.png" data-original-src="https://miro.medium.com/v2/resize:fit:838/format:webp/1*pBjG4e8HkvhcDhcxrElvJA.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">π诱导的贴现态分布；p0指初始状态分布；P(s_t=s|s_0=z)是使用策略π时，在时间t达到状态s的概率(假设初始状态为z)。</p></figure><p id="71d8" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">PG定理是如何发挥作用的？对于贴现准则，PG定理规定J(θ)的梯度简单地为</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi od"><img src="../Images/33b78841de890ba11bbc5805de6d26b0.png" data-original-src="https://miro.medium.com/v2/resize:fit:790/format:webp/1*UmfC94MfYrBo40qWe3KFYg.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">政策梯度定理(另见[1])；为简洁起见，我省略了归一化因子1/1-γ(在论文中经常被省略，因为它与贴现态分布中的1-γ因子相抵消)</p></figure><p id="431b" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">注意期望是如何类似于J(θ)的对偶形式的。该公式可能看起来微不足道，但事实并非如此，原因如下:</p><blockquote class="lt"><p id="138f" class="lu lv iq bd lw lx ly lz ma mb mc ls dk translated">在PG定理中，J(θ)的梯度不依赖于贴现态分布μ的梯度，即使μ依赖于策略π。</p></blockquote><p id="aedf" class="pw-post-body-paragraph kx ky iq kz b la md jr lc ld me ju lf lg mf li lj lk mg lm ln lo mh lq lr ls ij bi translated">这个结果在强化学习博客中经常被忽略，但它特别重要，因为它说明了J(θ)的梯度是相对于贴现状态分布μ取的平均值，当θ变化时，我们不需要考虑这个分布如何相对于θ变化。</p><p id="4bfd" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">然而，贴现态分布是所有问题的根源。</p></div><div class="ab cl mj mk hu ml" role="separator"><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo"/></div><div class="ij ik il im in"><h1 id="05c2" class="mq mr iq bd ms mt mu mv mw mx my mz na jw nb jx nc jz nd ka ne kc nf kd ng nh bi translated">那么，现代深度强化学习算法有什么问题呢？</h1><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oe"><img src="../Images/96386eeb08247056757e207420f646fa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*4FEKMs1rZSJODVne"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">由<a class="ae kw" href="https://unsplash.com/@hishahadat?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">沙哈达特拉赫曼</a>在<a class="ae kw" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="ae50" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">现在，几乎所有的强化学习算法都使用一个折扣因子。因此，如前所述，<strong class="kz ir">总期望贴现回报的梯度取决于贴现状态分布μ。</strong></p><p id="4681" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">因此，<strong class="kz ir">用于更新策略的样本应按照贴现态分布μ进行分布。</strong></p><blockquote class="lt"><p id="b387" class="lu lv iq bd lw lx ly lz ma mb mc ls dk translated">然而，在几乎所有的论文中，如果不是全部的话，策略是使用在执行策略π时收集的状态样本来更新的。换句话说，为了更新策略，我们使用根据π诱导的策略上分布分布的样本，而不是折扣分布μ。</p></blockquote><p id="0a9f" class="pw-post-body-paragraph kx ky iq kz b la md jr lc ld me ju lf lg mf li lj lk mg lm ln lo mh lq lr ls ij bi translated">有什么区别？简而言之，这类似于从状态分布中去掉贴现因子。</p><ul class=""><li id="3c2e" class="no np iq kz b la lb ld le lg nq lk nr lo ns ls of nu nv nw bi translated"><strong class="kz ir">策略梯度计算不正确，因为我们使用的样本不是按照μ分布的。</strong></li><li id="0a77" class="no np iq kz b la nx ld ny lg nz lk oa lo ob ls of nu nv nw bi translated"><strong class="kz ir">因此，我们没有优化原始目标函数J(θ)。</strong></li><li id="bd80" class="no np iq kz b la nx ld ny lg nz lk oa lo ob ls of nu nv nw bi translated"><strong class="kz ir">很明显，这个问题对深度强化学习领域中获得的所有经验结果的有效性提出了质疑，并且可能会损害现代深度强化学习算法在现实世界应用中的适用性。</strong></li></ul><h1 id="03e0" class="mq mr iq bd ms mt og mv mw mx oh mz na jw oi jx nc jz oj ka ne kc ok kd ng nh bi translated">以前没有人讨论过这个问题吗？</h1><p id="1fc6" class="pw-post-body-paragraph kx ky iq kz b la ni jr lc ld nj ju lf lg nk li lj lk nl lm ln lo nm lq lr ls ij bi translated">当我第一次和我的主管讨论这个问题时，它看起来像是一个没有人考虑过的新事物。但快速搜索发现，作者在[2]，“<em class="mi">政策梯度是梯度吗？</em>(Nota和Thomas)，最近对同一问题提出了担忧(它于去年，2020年发表)。</p><ul class=""><li id="6ed5" class="no np iq kz b la lb ld le lg nq lk nr lo ns ls of nu nv nw bi translated"><strong class="kz ir">在【2】中Nota和Thomas声称这样计算的梯度不是任何函数</strong>的梯度。<strong class="kz ir">使用这种梯度的算法不能保证收敛到一个“好的”参数θ。</strong></li><li id="1d43" class="no np iq kz b la nx ld ny lg nz lk oa lo ob ls of nu nv nw bi translated">此外，总是在[2]中，他们表明<strong class="kz ir">有可能构造"<em class="mi">一个反例，其中不动点相对于贴现和未贴现目标</em>"</strong>[2]都是全局相似的。</li><li id="daf0" class="no np iq kz b la nx ld ny lg nz lk oa lo ob ls of nu nv nw bi translated">再深入一点，我们会注意到，类似的问题以前也在[3]和[4]中提出过(这两篇论文中托马斯都是作者之一)。</li></ul><blockquote class="lt"><p id="ef30" class="lu lv iq bd lw lx ol om on oo op ls dk translated">非常令人惊讶的是，没有多少研究人员意识到这个问题。<em class="kv">这可能是由于缺乏对强化学习和马尔可夫决策过程的理论理解。</em></p></blockquote><h1 id="caae" class="mq mr iq bd ms mt og mv mw mx oh mz na jw oq jx nc jz or ka ne kc os kd ng nh bi translated">怎么才能解决呢？</h1><p id="1efe" class="pw-post-body-paragraph kx ky iq kz b la ni jr lc ld nj ju lf lg nk li lj lk nl lm ln lo nm lq lr ls ij bi translated"><strong class="kz ir">有修复。</strong></p><p id="4715" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">很脏，但很简单。</p><p id="fbb0" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated"><strong class="kz ir">在马尔可夫决策过程中，使用折扣因子和达到最终状态是等价的</strong>(另见puter man【10】，第5.3节，他讨论了使用折扣因子和拥有最终状态之间的相似性)。</p><p id="2538" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated"><strong class="kz ir">贴现相当于在马尔可夫决策过程中引入一个终态。</strong>这也相当于拥有一个几何分布的视界长度。</p><p id="a4eb" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">因此，我们只需要人工引入以某个概率1-γ终止轨迹的可能性。</p><p id="c93a" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">这显然是一个疯狂的想法。强化学习算法已经受到高样本复杂性的困扰，因此这一变化可能会使它变得更糟。此外，<strong class="kz ir">这种变化并不真正适用于现实世界的情况。</strong></p><h1 id="d2e4" class="mq mr iq bd ms mt og mv mw mx oh mz na jw oi jx nc jz oj ka ne kc ok kd ng nh bi translated">…也许我们应该降低折扣系数？</h1><p id="aaf0" class="pw-post-body-paragraph kx ky iq kz b la ni jr lc ld nj ju lf lg nk li lj lk nl lm ln lo nm lq lr ls ij bi translated">或者，另一个想法是简单地贬低贴现因子。萨顿和巴尔托已经在[6]中讨论过，贴现因子在持续的环境中毫无意义。</p><p id="9b12" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated"><strong class="kz ir">对于连续设置，没有剧集的概念，优化平均奖励更有意义</strong>。此外，<strong class="kz ir">这是最接近真实应用程序的持续设置。</strong></p><p id="1d55" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">不幸的是，这方面的研究很少，因为大部分努力都放在创建新的基于折扣的算法上。</p><p id="e518" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated"><strong class="kz ir">同样，在[5]中，作者讨论了当使用函数逼近时，折扣强化学习如何不是优化问题，以及我们应该求助于平均奖励强化学习。然而，请注意，作者在[5]中讨论的问题与我在本文中强调的问题不同</strong></p><h1 id="6d8c" class="mq mr iq bd ms mt og mv mw mx oh mz na jw oi jx nc jz oj ka ne kc ok kd ng nh bi translated">结论</h1><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/205d936a1f0db6f5cf5c257d85e5aa13.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LKtEKhbvCm5pWB2X2pazXQ.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><em class="kv">伊卡洛斯的飞行，b </em> y雅各布·彼得·戈威——<a class="ae kw" href="https://commons.wikimedia.org/w/index.php?curid=27493281" rel="noopener ugc nofollow" target="_blank">https://commons.wikimedia.org/w/index.php?curid=27493281</a></p></figure><p id="225c" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">这篇文章的目的是提高正确使用贴现版本的政策梯度定理的意识。绝大多数论文不正确地使用了这一梯度，并给出了几乎没有理论依据的结果。</p><p id="cb7d" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">深度强化学习的经验进展与我们对该领域的理论理解不匹配。</p><p id="ec41" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">“深入研究政策梯度”[11]的作者研究了这个问题，发现梯度估计值与“真实”梯度相关性很差。此外，他们注意到优化景观往往不能反映潜在MDP的回报景观。</p><p id="3475" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">简而言之，他们声称</p><blockquote class="ot ou ov"><p id="0b85" class="kx ky mi kz b la lb jr lc ld le ju lf ow lh li lj ox ll lm ln oy lp lq lr ls ij bi translated">总的来说，我们的结果表明，深层强化学习算法的理论框架通常无法预测实践中出现的现象。这表明，构建可靠的深度RL算法需要超越以基准为中心的评估，转而从多方面理解它们通常不直观的行为[11]。</p></blockquote><p id="6c77" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated"><strong class="kz ir">相反，我们正在见证一场战斗，以创建尖端的深度强化学习算法来提高Atari游戏或MuJoCo环境中的分数，尽管这是以减少理论理解为代价的。</strong></p><p id="2ae9" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">我们应该更多地思考这些问题，避免重蹈伊卡洛斯的覆辙。</p><p id="1d1e" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">感谢您的阅读！</p></div><div class="ab cl mj mk hu ml" role="separator"><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo"/></div><div class="ij ik il im in"><h2 id="d33c" class="oz mr iq bd ms pa pb dn mw pc pd dp na lg pe pf nc lk pg ph ne lo pi pj ng pk bi translated">参考</h2><ol class=""><li id="ff1a" class="no np iq kz b la ni ld nj lg pl lk pm lo pn ls nt nu nv nw bi translated">用函数逼近强化学习的政策梯度方法。<em class="mi">辊隙</em>。第99卷。1999.</li><li id="1bd6" class="no np iq kz b la nx ld ny lg nz lk oa lo ob ls nt nu nv nw bi translated">克里斯·诺塔和菲利普·托马斯。2020.政策梯度是梯度吗？。AAMAS (2020)。</li><li id="f298" class="no np iq kz b la nx ld ny lg nz lk oa lo ob ls nt nu nv nw bi translated">托马斯菲利普。“自然演员-评论家算法中的偏见。”<em class="mi">机器学习国际会议</em>。PMLR，2014年。</li><li id="0277" class="no np iq kz b la nx ld ny lg nz lk oa lo ob ls nt nu nv nw bi translated">经典政策梯度:保持贝尔曼的最优原则。<em class="mi"> arXiv预印本arXiv:1906.03063 </em> (2019)。</li><li id="eb53" class="no np iq kz b la nx ld ny lg nz lk oa lo ob ls nt nu nv nw bi translated">纳伊克，阿布舍克等人。“折扣强化学习不是一个优化问题。”<em class="mi"> arXiv预印本arXiv:1910.02140 </em> (2019)。</li><li id="caba" class="no np iq kz b la nx ld ny lg nz lk oa lo ob ls nt nu nv nw bi translated">萨顿，理查德s和安德鲁g巴尔托。强化学习:介绍。麻省理工学院出版社，2018。</li><li id="f472" class="no np iq kz b la nx ld ny lg nz lk oa lo ob ls nt nu nv nw bi translated">深度强化学习的异步方法。<em class="mi">机器学习国际会议</em>。PMLR，2016。</li><li id="cc6a" class="no np iq kz b la nx ld ny lg nz lk oa lo ob ls nt nu nv nw bi translated">软演员-评论家算法和应用。<em class="mi"> arXiv预印本arXiv:1812.05905 </em> (2018)。</li><li id="6f3b" class="no np iq kz b la nx ld ny lg nz lk oa lo ob ls nt nu nv nw bi translated">吴，，等，“基于kronecker分解近似的深度强化学习的可扩展信赖域方法”<em class="mi"> arXiv预印本arXiv:1708.05144 </em> (2017)。</li><li id="7ffa" class="no np iq kz b la nx ld ny lg nz lk oa lo ob ls nt nu nv nw bi translated">马尔可夫决策过程:离散随机动态规划。约翰威利&amp;之子，2014。</li><li id="ad97" class="no np iq kz b la nx ld ny lg nz lk oa lo ob ls nt nu nv nw bi translated">安德鲁·易勒雅斯等人，《深入研究政策梯度》<em class="mi">学习代表国际会议(ICLR) </em>。2020.</li></ol></div></div>    
</body>
</html>