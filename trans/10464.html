<html>
<head>
<title>Linear regression explained</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">线性回归解释</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/linear-regression-explained-89cc3886ab48?source=collection_archive---------25-----------------------#2021-10-05">https://towardsdatascience.com/linear-regression-explained-89cc3886ab48?source=collection_archive---------25-----------------------#2021-10-05</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/19f5dc803ea2516a13c6802720c8630e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*XLVb-vWJ_F0jCdUR"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">由<a class="ae jg" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上<a class="ae jg" href="https://unsplash.com/@leyko?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Lea Kobal </a>拍摄的照片</p></figure><h2 id="a3a3" class="jh ji jj bd b dl jk jl jm jn jo jp dk jq translated" aria-label="kicker paragraph">数据科学基础</h2><div class=""/><div class=""><h2 id="173a" class="pw-subtitle-paragraph kp js jj bd b kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg dk translated">了解这种受监督的机器学习算法是如何工作的</h2></div><p id="0b93" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><em class="md">线性回归</em>可能是最知名的机器学习算法。由于它的简单性、速度和可解释性，它通常是学习或实践数据科学时遇到的第一个算法。如果你想加深对线性回归的理解，这篇文章展示了算法背后的数学，重点是<em class="md">正规方程</em>以及如何解释线性回归的参数。</p><figure class="mf mg mh mi gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi me"><img src="../Images/e58e7f33e481e7ec75c90a1004ef0845.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*cy1ueHs7NWF3iTtP"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">照片由<a class="ae jg" href="https://unsplash.com/@timswaanphotography?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Tim Swaan </a>在<a class="ae jg" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><p id="3632" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><em class="md">我们将看到一些用Python编写的代码示例，然而，读者可以在不了解Python的情况下吸收概念性知识。</em></p></div><div class="ab cl mj mk hx ml" role="separator"><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo"/></div><div class="im in io ip iq"><h1 id="86d7" class="mq mr jj bd ms mt mu mv mw mx my mz na ky nb kz nc lb nd lc ne le nf lf ng nh bi translated">概观📜</h1><blockquote class="ni nj nk"><p id="f192" class="lh li md lj b lk ll kt lm ln lo kw lp nl lr ls lt nm lv lw lx nn lz ma mb mc im bi translated">线性回归是一种预测连续目标的机器学习算法。</p></blockquote><p id="721d" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">线性回归方程由下式表示:</p><figure class="mf mg mh mi gt iv gh gi paragraph-image"><div class="gh gi no"><img src="../Images/26597acc9317c8ddef3a3f5f71e8b880.png" data-original-src="https://miro.medium.com/v2/resize:fit:712/format:webp/1*u6-poWSSJX8ozVcqJYFIDA.png"/></div></figure><p id="aac4" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">根据所用特征的数量，线性回归可以进一步分为两类。如果使用单一特征来预测目标，则称为<em class="md">简单线性回归</em>。如果使用了多个(即两个或更多)特征，则称为<em class="md">多元</em>或<em class="md">多元回归</em>。</p></div><div class="ab cl mj mk hx ml" role="separator"><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo"/></div><div class="im in io ip iq"><h1 id="e337" class="mq mr jj bd ms mt mu mv mw mx my mz na ky nb kz nc lb nd lc ne le nf lf ng nh bi translated">训练线性回归🔧</h1><p id="9b9c" class="pw-post-body-paragraph lh li jj lj b lk np kt lm ln nq kw lp lq nr ls lt lu ns lw lx ly nt ma mb mc im bi translated">现在，让我们学习模型如何学习最优的<a class="ae jg" href="https://machinelearningmastery.com/difference-between-a-parameter-and-a-hyperparameter/" rel="noopener ugc nofollow" target="_blank">模型参数</a>(即截距和系数)。在训练线性回归时，我们希望找到最佳的参数组合，以便与任何其他组合相比，它们在所有训练示例中产生最低的误差。对于给定的一组参数，我们可以使用<em class="md">均方误差(又名MSE) </em>来测量总误差:</p><figure class="mf mg mh mi gt iv gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/0c701a3da662b3f1853134afe52232fe.png" data-original-src="https://miro.medium.com/v2/resize:fit:914/format:webp/1*pjSnuNEQvbLLNJ2-Kp9bbA.png"/></div></figure><p id="05af" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">误差是平方的，因此正负误差不会相互抵消。有两种方法可以找到最佳的参数设置:<br/> ◼使用标准方程<br/> ◼使用优化算法</p><h2 id="9e53" class="nv mr jj bd ms nw nx dn mw ny nz dp na lq oa ob nc lu oc od ne ly oe of ng jp bi translated">📍使用正常方程</h2><p id="27c8" class="pw-post-body-paragraph lh li jj lj b lk np kt lm ln nq kw lp lq nr ls lt lu ns lw lx ly nt ma mb mc im bi translated">我们将需要了解<em class="md">矩阵转置</em>、<em class="md">矩阵乘法、</em>和<em class="md">矩阵求逆</em>的基础知识，以遵循即将到来的正规方程示例。在我们深入讨论之前，让我们先快速回顾一下这些主题。</p><p id="cedc" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jt">矩阵转置:</strong>下面是转置一个2x2矩阵的公式:</p><figure class="mf mg mh mi gt iv gh gi paragraph-image"><div class="gh gi og"><img src="../Images/dfce63fb6b97861909360d1aaafb8db8.png" data-original-src="https://miro.medium.com/v2/resize:fit:670/format:webp/1*l9CiZlH6kL0PYNpwoz_ZXA.png"/></div></figure><p id="27bb" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">您可以想象您正在沿着从左上延伸到右下的对角线翻转值。这里有一个例子:</p><figure class="mf mg mh mi gt iv gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/53ad1e78471de386acda2999d2f3d3d5.png" data-original-src="https://miro.medium.com/v2/resize:fit:326/format:webp/1*UQ_P7aQ_lNAQ_V7zYfJcHA.png"/></div></figure><p id="0b34" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jt">矩阵乘法:</strong>下面是两个2x2矩阵相乘的公式:</p><figure class="mf mg mh mi gt iv gh gi paragraph-image"><div class="gh gi oi"><img src="../Images/c3b4cd1d7ef943358498b3adf12fed15.png" data-original-src="https://miro.medium.com/v2/resize:fit:1120/format:webp/1*ilfAsDIxBELEHRgj8sDU7w.png"/></div></figure><p id="017c" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">这里有一个例子:</p><figure class="mf mg mh mi gt iv gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/28795a68fd287ee8a83d32634ed0adb5.png" data-original-src="https://miro.medium.com/v2/resize:fit:986/format:webp/1*UnmMrEAZ3YluqrXyChBBug.png"/></div></figure><p id="d2e9" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jt">逆矩阵:</strong>矩阵要有逆矩阵，矩阵必须是方阵(即行数=列数)。下面是计算2x2矩阵的逆矩阵的公式:</p><figure class="mf mg mh mi gt iv gh gi paragraph-image"><div class="gh gi ok"><img src="../Images/02c2af1744607371c8e515188fddc957.png" data-original-src="https://miro.medium.com/v2/resize:fit:1172/format:webp/1*91z6CFO0D2WqjgN0cHLwTg.png"/></div></figure><p id="847e" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">如果一个矩阵的行列式(即<code class="fe ol om on oo b">ad-bc</code>)为零，那么这个矩阵就不会有逆矩阵。这样的矩阵称为奇异矩阵。</p><p id="1780" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">下面是一个求逆矩阵的例子:</p><figure class="mf mg mh mi gt iv gh gi paragraph-image"><div class="gh gi op"><img src="../Images/c977aeb977799b82293c40220a5b78ef.png" data-original-src="https://miro.medium.com/v2/resize:fit:1168/format:webp/1*JazD6Ti6wyq0i26qOS_Dgw.png"/></div></figure><figure class="mf mg mh mi gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi oq"><img src="../Images/7503592da1e80c28bc32f530a6550738.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*erLgcImZzb6m51n5"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">照片由<a class="ae jg" href="https://unsplash.com/@v2osk?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> v2osk </a>在<a class="ae jg" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><p id="dd68" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">现在，我们准备好熟悉<em class="md">正规方程</em>:</p><figure class="mf mg mh mi gt iv gh gi paragraph-image"><div class="gh gi or"><img src="../Images/aa25b94c993a1deeaaa25a81a857c63c.png" data-original-src="https://miro.medium.com/v2/resize:fit:424/format:webp/1*9t_oEfFP2c07Kqh3FCxD1g.png"/></div></figure><p id="6c72" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">让我们重写我们的线性方程，使得截距是一个特殊的系数，其中对应的特征<code class="fe ol om on oo b">x0</code>取常数值1:</p><figure class="mf mg mh mi gt iv gh gi paragraph-image"><div class="gh gi os"><img src="../Images/e4e0ac3eae21b8b318dff10d6f65e9fd.png" data-original-src="https://miro.medium.com/v2/resize:fit:964/format:webp/1*6nuTbFNDvXkPLsvWT5PU4Q.png"/></div></figure><p id="202f" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">让我们使用玩具数据集上的正态方程来寻找简单线性回归的参数，该数据集具有一个解释特征<code class="fe ol om on oo b">x1</code>和一个目标<code class="fe ol om on oo b">y</code>，以便我们自己可以管理计算。</p><figure class="mf mg mh mi gt iv gh gi paragraph-image"><div class="gh gi ot"><img src="../Images/db7c65016acd626fcb942d9dbf89afb4.png" data-original-src="https://miro.medium.com/v2/resize:fit:292/format:webp/1*O6MvsO4vhr4ZuBiXj0pMIQ.png"/></div></figure><p id="ec5e" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">从下面，我们看到了<code class="fe ol om on oo b">X</code>，一个5x2的特征矩阵。第一列显示了5个训练示例的<code class="fe ol om on oo b">x0</code>，第二列显示了<code class="fe ol om on oo b">x1</code>。首先，我们将<code class="fe ol om on oo b">X^T</code>乘以<code class="fe ol om on oo b">X</code>:</p><figure class="mf mg mh mi gt iv gh gi paragraph-image"><div class="gh gi ou"><img src="../Images/2b534a57a4bca056c439b5918bafd38c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1230/format:webp/1*LaLNw0mEBOgBNwdTmbu5jQ.png"/></div></figure><p id="1b92" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">现在，让我们找到它的逆:</p><figure class="mf mg mh mi gt iv gh gi paragraph-image"><div class="gh gi ov"><img src="../Images/4c87b6d936c655020c26733c0ec14319.png" data-original-src="https://miro.medium.com/v2/resize:fit:1082/format:webp/1*O3swQNKu8DeSqPrrjqUQtg.png"/></div></figure><p id="5543" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">接下来，我们将它乘以<code class="fe ol om on oo b">X_T</code>:</p><figure class="mf mg mh mi gt iv gh gi paragraph-image"><div class="gh gi ow"><img src="../Images/24108154b59be9bf092438d55bff6d01.png" data-original-src="https://miro.medium.com/v2/resize:fit:1010/format:webp/1*Gzj6RCWdQSlu5ca9M96oDA.png"/></div></figure><p id="d6eb" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">最后，我们将它乘以实际目标值，以找到最佳参数:</p><figure class="mf mg mh mi gt iv gh gi paragraph-image"><div class="gh gi ox"><img src="../Images/148154116f8289fc2cf5950008379fc3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1202/format:webp/1*oPdf3l7LxPpFkHkTyelOfw.png"/></div></figure><p id="be7f" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">就是这样！我们发现截距是0.3588，系数是0.2647。我们也可以在Python中进行同样的计算来计算参数:</p><pre class="mf mg mh mi gt oy oo oz pa aw pb bi"><span id="7d9c" class="nv mr jj oo b gy pc pd l pe pf">import numpy as np<br/>import pandas as pd<br/>pd.options.display.precision = 4</span><span id="1a38" class="nv mr jj oo b gy pg pd l pe pf">from sklearn.linear_model import LinearRegression</span><span id="4f7a" class="nv mr jj oo b gy pg pd l pe pf"># Create sample data<br/>train = pd.DataFrame({'x1': [8, 15, 6, 16, 20], <br/>                      'y': [4, 3, 1, 5, 6]})<br/>train</span></pre><figure class="mf mg mh mi gt iv gh gi paragraph-image"><div class="gh gi ph"><img src="../Images/76a2df594b97301ce73759de30c6c130.png" data-original-src="https://miro.medium.com/v2/resize:fit:162/format:webp/1*r2sQg_VAFQXzLPW3-2vfxQ.png"/></div></figure><pre class="mf mg mh mi gt oy oo oz pa aw pb bi"><span id="0bc0" class="nv mr jj oo b gy pc pd l pe pf"># Add x0=[1, 1, .., 1] for the intercept such that X is 5x2 matrix<br/>x0 = np.ones(train.shape[0])[:, np.newaxis]<br/>X = np.concatenate((x0, train.drop(columns='y')), axis=1)</span><span id="40e4" class="nv mr jj oo b gy pg pd l pe pf"># Reshape y from 1D to 2D such that Y is 5x1 matrix<br/>Y = train['y'].values.reshape(-1,1)</span><span id="4cb5" class="nv mr jj oo b gy pg pd l pe pf"># Find best parameters for the model<br/>theta = np.linalg.inv(X.T.dot(X)).dot(X.T).dot(Y)<br/>print(f"Intercept: {theta[0][0]:.4f}")<br/>print(f"Coefficient for x1: {theta[1][0]:.4f}")</span><span id="7e6a" class="nv mr jj oo b gy pg pd l pe pf"># Predict<br/>train['y_hat_manual'] = X.dot(theta)<br/>train['residual'] = train['y'] - train['y_hat_manual']<br/>train</span></pre><figure class="mf mg mh mi gt iv gh gi paragraph-image"><div class="gh gi pi"><img src="../Images/cfa6827acde02f8d1506aa4d8c67d337.png" data-original-src="https://miro.medium.com/v2/resize:fit:540/format:webp/1*lDcShElEf_oYAaAorxPx3A.png"/></div></figure><p id="bc92" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">让我们对照Scikit-learn的输出快速检查一下:</p><pre class="mf mg mh mi gt oy oo oz pa aw pb bi"><span id="fd41" class="nv mr jj oo b gy pc pd l pe pf"># Check parameters against sklearn<br/>model = LinearRegression()<br/>model.fit(train[['x1']], train['y'])<br/>print(f"Intercept: {model.intercept_:.4f}")<br/>print(f"Coefficient for x1: {model.coef_[0]:.4f}")</span><span id="1d65" class="nv mr jj oo b gy pg pd l pe pf"># Check predictions against sklearn<br/>train['y_hat_sklearn'] = model.predict(train[['x1']])<br/>train</span></pre><figure class="mf mg mh mi gt iv gh gi paragraph-image"><div class="gh gi pj"><img src="../Images/a5bfb6762541aa029aafa69924f26000.png" data-original-src="https://miro.medium.com/v2/resize:fit:764/format:webp/1*K0v694_uG8MkDui0d9LE4w.png"/></div></figure><p id="7508" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">完美，结果匹配！</p><p id="0134" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">值得注意的是，如果<code class="fe ol om on oo b">X^T ⋅ X</code>是奇异的(即行列式是零)，那么有些时候正规方程不起作用。</p><h2 id="132d" class="nv mr jj bd ms nw nx dn mw ny nz dp na lq oa ob nc lu oc od ne ly oe of ng jp bi translated">📍使用优化算法</h2><p id="610a" class="pw-post-body-paragraph lh li jj lj b lk np kt lm ln nq kw lp lq nr ls lt lu ns lw lx ly nt ma mb mc im bi translated">训练模型的另一种方法是使用优化算法，如<a class="ae jg" href="https://ml-cheatsheet.readthedocs.io/en/latest/gradient_descent.html" rel="noopener ugc nofollow" target="_blank">梯度下降</a>，以找到使总误差最小化的最佳参数。优化算法迭代求解参数的最佳组合。使用优化算法训练线性回归时，具有相同比例的要素有助于更快地收敛到全局最小值。实际上，优化算法的选择取决于实现。</p></div><div class="ab cl mj mk hx ml" role="separator"><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo"/></div><div class="im in io ip iq"><h1 id="6efa" class="mq mr jj bd ms mt mu mv mw mx my mz na ky nb kz nc lb nd lc ne le nf lf ng nh bi translated">解释线性回归参数🔎</h1><blockquote class="ni nj nk"><p id="057c" class="lh li md lj b lk ll kt lm ln lo kw lp nl lr ls lt nm lv lw lx nn lz ma mb mc im bi translated"><strong class="lj jt">截距</strong>告诉我们所有特征都为0时的预期目标值。</p></blockquote><figure class="mf mg mh mi gt iv gh gi paragraph-image"><div class="gh gi pk"><img src="../Images/b1ce8fef1cf355e6a3c33c31e185f12e.png" data-original-src="https://miro.medium.com/v2/resize:fit:944/format:webp/1*dPdhKNsKviZrQlPYEoUKPg.png"/></div></figure><p id="a0ec" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">在我们的示例中，当特征<code class="fe ol om on oo b">x1</code>为零时，目标预计为0.2647:</p><figure class="mf mg mh mi gt iv gh gi paragraph-image"><div class="gh gi pl"><img src="../Images/dc32c66c80ad78b22f29f94e7d7b611c.png" data-original-src="https://miro.medium.com/v2/resize:fit:358/format:webp/1*BbtqXutePILRLxbIganx3w.png"/></div></figure><blockquote class="ni nj nk"><p id="57de" class="lh li md lj b lk ll kt lm ln lo kw lp nl lr ls lt nm lv lw lx nn lz ma mb mc im bi translated"><strong class="lj jt">系数</strong>告诉我们，当感兴趣的特性增加一个单位，而所有其他特性保持不变时，目标值的预期变化。系数的符号告诉我们特征和目标之间的关系。例如，正系数表示正关系。</p></blockquote><figure class="mf mg mh mi gt iv gh gi paragraph-image"><div class="gh gi pm"><img src="../Images/cf46e20db20cfd7dff13d0e0e7f1282d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1192/format:webp/1*ZujNYNGfZoK0FVI1bpbDsg.png"/></div></figure><p id="7f4d" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">当特性<code class="fe ol om on oo b">x1</code>增加一个单位时，目标预计增加0.3588:</p><figure class="mf mg mh mi gt iv gh gi paragraph-image"><div class="gh gi pn"><img src="../Images/c043fff1cf5911b346f4cbe4c808d80b.png" data-original-src="https://miro.medium.com/v2/resize:fit:446/format:webp/1*Hj-b8VeDeYfJr1HIRLW0jg.png"/></div></figure><figure class="mf mg mh mi gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi po"><img src="../Images/f3c3a2f91fb38e22cd91e57ee6c868a6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*WmSB8S4fxN1CxRxs"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated"><a class="ae jg" href="https://unsplash.com/@davidmarcu?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> David Marcu </a>在<a class="ae jg" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="42fd" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">这就是这篇文章的全部内容！希望您喜欢加深对线性回归的了解。在此过程中，我们还复习了基本的线性代数(如矩阵乘法和矩阵求逆)。最后，在向他人解释模型的驱动因素时，能够解释如何解释线性回归结果是很有帮助的。如果你渴望了解更多，请查看关于线性回归假设的资源<a class="ae jg" href="https://blog.uwgb.edu/bansalg/statistics-data-analytics/linear-regression/what-are-the-four-assumptions-of-linear-regression/" rel="noopener ugc nofollow" target="_blank">。</a></p><p id="c99f" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><em class="md">您想要访问更多这样的内容吗？媒体会员可以无限制地访问媒体上的任何文章。如果您使用</em> <a class="ae jg" href="https://zluvsand.medium.com/membership" rel="noopener"> <em class="md">我的推荐链接</em> </a>，<em class="md">成为会员，您的一部分会费将直接用于支持我。</em></p></div><div class="ab cl mj mk hx ml" role="separator"><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo"/></div><div class="im in io ip iq"><p id="9080" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">感谢您阅读我的文章。如果你感兴趣，这里有我其他一些帖子的链接:</p><p id="4e1f" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">◼️️ <a class="ae jg" rel="noopener" target="_blank" href="/k-nearest-neighbours-explained-52c910c035c5"> K近邻讲解</a> <br/> ◼️️ <a class="ae jg" rel="noopener" target="_blank" href="/comparing-random-forest-and-gradient-boosting-d7236b429c15">比较随机森林和梯度推进</a> <br/> ◼️️ <a class="ae jg" rel="noopener" target="_blank" href="/how-are-decision-trees-built-a8e5af57ce8?source=your_stories_page-------------------------------------">决策树是如何建立的？</a> <br/> ◼️️ <a class="ae jg" rel="noopener" target="_blank" href="/pipeline-columntransformer-and-featureunion-explained-f5491f815f?source=your_stories_page-------------------------------------">管道、ColumnTransformer和FeatureUnion说明</a> <br/> ◼️️ <a class="ae jg" rel="noopener" target="_blank" href="/featureunion-columntransformer-pipeline-for-preprocessing-text-data-9dcb233dbcb6"> FeatureUnion、ColumnTransformer &amp;管道用于预处理文本数据</a></p><p id="434d" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">再见🏃 💨</p></div><div class="ab cl mj mk hx ml" role="separator"><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo"/></div><div class="im in io ip iq"><h1 id="a39e" class="mq mr jj bd ms mt mu mv mw mx my mz na ky nb kz nc lb nd lc ne le nf lf ng nh bi translated">参考📁</h1><ul class=""><li id="1ee3" class="pp pq jj lj b lk np ln nq lq pr lu ps ly pt mc pu pv pw px bi translated">Aurelien Geron，<em class="md">使用Scikit-Learn、Keras和TensorFlow进行动手机器学习，2017年</em> -第4章</li></ul></div></div>    
</body>
</html>