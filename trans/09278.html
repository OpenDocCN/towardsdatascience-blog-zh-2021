<html>
<head>
<title>Hyperparameter Tuning Methods - Grid, Random or Bayesian Search?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">超参数调整方法-网格，随机或贝叶斯搜索？</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/bayesian-optimization-for-hyperparameter-tuning-how-and-why-655b0ee0b399?source=collection_archive---------10-----------------------#2021-08-28">https://towardsdatascience.com/bayesian-optimization-for-hyperparameter-tuning-how-and-why-655b0ee0b399?source=collection_archive---------10-----------------------#2021-08-28</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><figure class="ip iq gp gr ir is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi io"><img src="../Images/9032bda97a730ce55e78a4dc4c89f87f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wsFq6waE5H1DSfWAwNP8dQ.jpeg"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">局部最小值和最大值的示例(图片由作者提供— Mayrhofen 2019)</p></figure><div class=""/><div class=""><h2 id="573b" class="pw-subtitle-paragraph kc je jf bd b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt dk translated">使用三种方法的超参数优化实用指南:网格、随机和贝叶斯搜索</h2></div><p id="4662" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">当我在做我的上一个项目时，在我训练了模型的第一个版本之后，我得到了一个新的数据块。那天，我觉得有点懒，并尝试使用相同的模型类型和超参数，用新数据重新训练我的模型。可惜没有按照我的预期进行。由于数据量的增加，它的性能没有得到改善，反而略有下降。添加的数据与我之前的数据分布不同，与原始数据相比，它的数量不可忽略，这与我的预期相反。</p><p id="6360" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">因此，我发现自己处于一个未知的领域，不知道应该使用哪个超参数。我手动尝试了一些选项，但有太多可能的组合，我在管理我的实验时遇到了麻烦。那时，我决定更深入地研究超参数调优领域。</p><h2 id="6937" class="lq lr jf bd ls lt lu dn lv lw lx dp ly ld lz ma mb lh mc md me ll mf mg mh mi bi translated">你对这篇文章有什么期待？</h2><ol class=""><li id="fdbd" class="mj mk jf kw b kx ml la mm ld mn lh mo ll mp lp mq mr ms mt bi translated">超参数调谐简介。</li><li id="e488" class="mj mk jf kw b kx mu la mv ld mw lh mx ll my lp mq mr ms mt bi translated">解释超参数搜索方法。</li><li id="ba2b" class="mj mk jf kw b kx mu la mv ld mw lh mx ll my lp mq mr ms mt bi translated">每种方法的代码示例。</li><li id="e927" class="mj mk jf kw b kx mu la mv ld mw lh mx ll my lp mq mr ms mt bi translated">比较和结论。</li></ol><p id="d65b" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">对于帖子中的所有例子，我使用了Kaggles的<a class="ae mz" href="https://www.kaggle.com/rashikrahmanpritom/heart-attack-analysis-prediction-dataset" rel="noopener ugc nofollow" target="_blank">心脏病发作分析预测数据集</a>。</p><figure class="na nb nc nd gt is"><div class="bz fp l di"><div class="ne nf l"/></div></figure><p id="8aaa" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">我准备了一个简单的管道，用于所有的例子。</p><figure class="na nb nc nd gt is"><div class="bz fp l di"><div class="ne nf l"/></div></figure><h1 id="23cf" class="ng lr jf bd ls nh ni nj lv nk nl nm ly kl nn km mb ko no kp me kr np ks mh nq bi translated">什么是超参数调整，为什么它很重要？</h1><p id="17bf" class="pw-post-body-paragraph ku kv jf kw b kx ml kg kz la mm kj lc ld nr lf lg lh ns lj lk ll nt ln lo lp ij bi translated">超参数是控制算法整体行为的算法变量。它影响其速度、分辨率、结构，并最终影响性能。有时它只有很小的影响，但在其他情况下，它是至关重要的。一个很好的例子是<strong class="kw jg">学习率</strong>。当它太大时，学习不够敏感，模型结果不稳定。但是当它太小时，模型学习起来就有困难，可能会卡住。</p><p id="9747" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">当算法有许多参数时，很难尝试所有可能的组合来找到最佳集合。因此，我们希望以一种可管理的方式高效地进行超参数调整。</p><h1 id="c252" class="ng lr jf bd ls nh ni nj lv nk nl nm ly kl nn km mb ko no kp me kr np ks mh nq bi translated">超参数搜索的类型</h1><p id="d429" class="pw-post-body-paragraph ku kv jf kw b kx ml kg kz la mm kj lc ld nr lf lg lh ns lj lk ll nt ln lo lp ij bi translated">执行超参数搜索有三种主要方法:</p><ol class=""><li id="8f0d" class="mj mk jf kw b kx ky la lb ld nu lh nv ll nw lp mq mr ms mt bi translated">网格搜索</li><li id="f681" class="mj mk jf kw b kx mu la mv ld mw lh mx ll my lp mq mr ms mt bi translated">随机搜索</li><li id="5f0f" class="mj mk jf kw b kx mu la mv ld mw lh mx ll my lp mq mr ms mt bi translated">贝叶斯搜索</li></ol><h2 id="2889" class="lq lr jf bd ls lt lu dn lv lw lx dp ly ld lz ma mb lh mc md me ll mf mg mh mi bi translated">网格搜索</h2><p id="dcad" class="pw-post-body-paragraph ku kv jf kw b kx ml kg kz la mm kj lc ld nr lf lg lh ns lj lk ll nt ln lo lp ij bi translated">执行超参数调整的基本方法是尝试所有可能的参数组合。例如，如果您想要调整<strong class="kw jg"> learning_rate </strong>和<strong class="kw jg"> max_depth </strong>，您需要指定您认为与搜索相关的所有值。然后，当我们运行超参数优化时，我们尝试两个列表中的所有组合。</p><p id="ad2a" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">在下面的例子中，我试图找到<strong class="kw jg"> learning_rate </strong> (5个值)、<strong class="kw jg"> max_depth </strong> (5个值)和<strong class="kw jg"> n_estimators </strong> (5个值)的最佳值——总共125次迭代。</p><figure class="na nb nc nd gt is"><div class="bz fp l di"><div class="ne nf l"/></div></figure><h2 id="148b" class="lq lr jf bd ls lt lu dn lv lw lx dp ly ld lz ma mb lh mc md me ll mf mg mh mi bi translated">随机搜索</h2><p id="67c6" class="pw-post-body-paragraph ku kv jf kw b kx ml kg kz la mm kj lc ld nr lf lg lh ns lj lk ll nt ln lo lp ij bi translated">与网格搜索不同，在随机搜索中，只尝试了部分参数值。参数值从给定的<strong class="kw jg">列表</strong>或<strong class="kw jg">列表</strong>指定的<strong class="kw jg">分布</strong>中采样。被采样的参数设置的数量由<strong class="kw jg"> n_iter </strong>给出。当参数以列表形式呈现时，执行无替换采样(如网格搜索)。但是如果参数以分布形式给出，则使用带有替换的采样<strong class="kw jg">(推荐)。</strong></p><p id="61fb" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">根据我的经验，随机搜索的优点是你可以在不增加迭代次数(耗时)的情况下扩展你的搜索范围。此外，您可以使用它来找到狭窄的限制，以便在更小的区域内继续进行彻底的搜索。</p><p id="5fb2" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">在下面的例子中，我使用参数分布进行替换采样。</p><figure class="na nb nc nd gt is"><div class="bz fp l di"><div class="ne nf l"/></div></figure><h1 id="fa1e" class="ng lr jf bd ls nh ni nj lv nk nl nm ly kl nn km mb ko no kp me kr np ks mh nq bi translated">贝叶斯搜索</h1><p id="2977" class="pw-post-body-paragraph ku kv jf kw b kx ml kg kz la mm kj lc ld nr lf lg lh ns lj lk ll nt ln lo lp ij bi translated">贝叶斯搜索与其他方法的主要区别在于，调整算法根据前一轮得分在每一轮中优化其参数选择。因此，该算法不是随机选择下一组参数，而是优化选择，并且可能比前两种方法更快地达到最佳参数组。也就是说，这种方法只选择相关的搜索空间，并放弃最有可能不提供最佳解决方案的范围。因此，当您拥有大量数据，学习速度较慢，并且您希望<strong class="kw jg">最大限度地减少调整时间</strong>时，这是非常有益的。</p><p id="1e88" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">与随机搜索示例相同，我在这个示例中使用了用于采样的参数分布:</p><figure class="na nb nc nd gt is"><div class="bz fp l di"><div class="ne nf l"/></div></figure><h2 id="edee" class="lq lr jf bd ls lt lu dn lv lw lx dp ly ld lz ma mb lh mc md me ll mf mg mh mi bi translated">参数搜索的可视化——学习率</h2><figure class="na nb nc nd gt is"><div class="bz fp l di"><div class="ne nf l"/></div></figure><figure class="na nb nc nd gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nx"><img src="../Images/a4b64fe369cf2fa529e970da6228bdfe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oZ0Msc9RA2qmo1CzKnnAnQ.png"/></div></div></figure><p id="ed81" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">这个比较中的最佳学习率参数是0.008(通过贝叶斯搜索找到)。</p><h2 id="038c" class="lq lr jf bd ls lt lu dn lv lw lx dp ly ld lz ma mb lh mc md me ll mf mg mh mi bi translated">每次迭代平均分数的可视化</h2><figure class="na nb nc nd gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi ny"><img src="../Images/ca5ece8594acb87b1cf0acca9cb49c23.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7Qow88WibgsLWgFakMF_RA.png"/></div></div></figure><p id="a917" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">我们可以看到贝叶斯搜索比其他方法略胜一筹。这种影响在更大的数据集和更复杂的模型中更加明显。</p><h1 id="f882" class="ng lr jf bd ls nh ni nj lv nk nl nm ly kl nn km mb ko no kp me kr np ks mh nq bi translated">讨论和结论</h1><p id="6faf" class="pw-post-body-paragraph ku kv jf kw b kx ml kg kz la mm kj lc ld nr lf lg lh ns lj lk ll nt ln lo lp ij bi translated">我在相同的参数范围内运行了三种搜索方法。网格搜索运行125次迭代，随机和贝叶斯运行70次迭代。这个数据集相对简单，所以分数的变化并不明显。尽管如此，随机搜索和贝叶斯搜索比网格搜索表现更好，迭代次数更少。<strong class="kw jg">贝叶斯</strong>搜索找到了实现<strong class="kw jg">最佳得分</strong>的超参数。</p><p id="a74c" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">关于该主题的进一步阅读，我推荐阅读以下精彩帖子:<a class="ae mz" rel="noopener" target="_blank" href="/a-conceptual-explanation-of-bayesian-model-based-hyperparameter-optimization-for-machine-learning-b8172278050f">https://towardsdatascience . com/a-conceptual-explain-of-Bayesian-model-based-hyperparameter-optimization-for-machine-learning-b 8172278050 f</a>。</p><p id="6e04" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">祝你好运！</p></div></div>    
</body>
</html>