<html>
<head>
<title>Fine-Tuning Transformer Model for Invoice Recognition</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用于发票识别的微调变压器模型</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/fine-tuning-transformer-model-for-invoice-recognition-1e55869336d4?source=collection_archive---------5-----------------------#2021-06-21">https://towardsdatascience.com/fine-tuning-transformer-model-for-invoice-recognition-1e55869336d4?source=collection_archive---------5-----------------------#2021-06-21</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="9a32" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">从注释到训练的逐步指南</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/dfd605c21da31027eaf140b9e5a116d3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dj69AL1yQYIj7ln2XPoqNg.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">安德烈·波波夫摄于<a class="ae ky" href="https://www.dreamstime.com/robotic-hand-examining-financial-data-elevated-view-magnifying-glass-image148941585" rel="noopener ugc nofollow" target="_blank"> Dreamstime </a></p></figure><h1 id="8eb6" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">介绍</h1><p id="09e0" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">基于我最近关于<a class="ae ky" rel="noopener" target="_blank" href="/how-to-annotate-pdfs-and-scanned-images-for-nlp-applications-f7b7b1db5c4a">如何为NLP应用注释pdf和扫描图像的教程</a>，我们将尝试在一个包含法语和英语发票的注释定制数据集上微调最近发布的微软<a class="ae ky" href="https://github.com/microsoft/unilm/tree/master/layoutlm" rel="noopener ugc nofollow" target="_blank">布局LM模型</a>。虽然之前的教程侧重于使用公开可用的<a class="ae ky" href="https://guillaumejaume.github.io/FUNSD/" rel="noopener ugc nofollow" target="_blank"> FUNSD数据集</a>来微调模型，但这里我们将展示从注释和预处理到训练和推理的整个过程。</p><h1 id="dd00" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">LayoutLM模型</h1><p id="d534" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">LayoutLM模型基于BERT架构，但增加了两种类型的输入嵌入。第一种是2d位置嵌入，表示文档内标记的相对位置，第二种是文档内扫描标记图像的图像嵌入。该模型在几个下游任务中取得了新的最先进的结果，包括表单理解(从70.72到79.27)、收据理解(从94.02到95.24)和文档图像分类(从93.07到94.42)。有关更多信息，请参考<a class="ae ky" href="https://arxiv.org/abs/1912.13318" rel="noopener ugc nofollow" target="_blank">原文</a>。</p><p id="2aff" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">幸运的是，这个模型是开源的，可以在huggingface库中获得。谢谢微软！</p><p id="108f" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">对于本教程，我们将直接从huggingface库中克隆模型，并在我们自己的数据集上对其进行微调，下面是google colab的链接。但是首先，我们需要创建训练数据。</p><p id="dc2c" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><a class="ae ky" href="https://colab.research.google.com/drive/1KnkVuYW6Ne25hOZ_IApiv_MIYb4lxCAq?usp=sharing" rel="noopener ugc nofollow" target="_blank">https://colab . research . Google . com/drive/1 nkvuyw 6 ne 25 hoz _ IApiv _ miyb 4 LX caq？usp =共享</a></p><h1 id="7a73" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">发票注释</h1><p id="051c" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">使用<a class="ae ky" href="https://ubiai.tools" rel="noopener ugc nofollow" target="_blank"> UBIAI文本注释工具</a>，我已经注释了大约50张个人发票。我对提取实体的键和值感兴趣；例如，在下面的文本“日期:06/12/2021”中，我们会将“日期”注释为DATE_ID，将“06/12/2021”注释为Date。提取键和值将有助于我们将数值与它们的属性相关联。以下是所有已被注释的实体:</p><pre class="kj kk kl km gt ms mt mu mv aw mw bi"><span id="69d7" class="mx la it mt b gy my mz l na nb">DATE_ID, DATE, INVOICE_ID, INVOICE_NUMBER,SELLER_ID, SELLER, MONTANT_HT_ID, MONTANT_HT, TVA_ID, TVA, TTC_ID, TTC</span></pre><p id="721a" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">以下是一些实体定义:</p><pre class="kj kk kl km gt ms mt mu mv aw mw bi"><span id="9901" class="mx la it mt b gy my mz l na nb">MONTANT_HT: Total price pre-tax</span><span id="270c" class="mx la it mt b gy nc mz l na nb">TTC: Total price with tax</span><span id="7756" class="mx la it mt b gy nc mz l na nb">TVA: Tax amount</span></pre><p id="a8d8" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">以下是使用<a class="ae ky" href="https://ubiai.tools" rel="noopener ugc nofollow" target="_blank"> UBIAI </a>的注释发票示例:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nd"><img src="../Images/c9663b8019e86ab35529514f345f49bb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9u59CcYS9C2AW5dzF6iCOQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片:带注释的发票</p></figure><p id="8b2e" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">标注后，我们直接从UBIAI中以正确的格式导出训练和测试文件，没有任何<a class="ae ky" href="https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/LayoutLM/Fine_tuning_LayoutLMForTokenClassification_on_FUNSD.ipynb" rel="noopener ugc nofollow" target="_blank">预处理步骤</a>。导出将包括每个训练和测试数据集的三个文件，以及一个包含所有名为labels.txt的标签的文本文件:</p><p id="1530" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">培训/测试. txt</p><pre class="kj kk kl km gt ms mt mu mv aw mw bi"><span id="c119" class="mx la it mt b gy my mz l na nb">2018	O<br/>Sous-total	O<br/>en	O<br/>EUR	O<br/>3,20	O<br/>€	O<br/>TVA	S-TVA_ID<br/>(0%)	O<br/>0,00 €	S-TVA<br/>Total	B-TTC_ID<br/>en	I-TTC_ID<br/>EUR	E-TTC_ID<br/>3,20	S-TTC<br/>€	O<br/>Services	O<br/>soumis	O<br/>au	O<br/>mécanisme	O<br/>d'autoliquidation	O<br/>-	O</span></pre><p id="9f10" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">Train/Test_box.txt(包含每个令牌的边界框):</p><pre class="kj kk kl km gt ms mt mu mv aw mw bi"><span id="e9bd" class="mx la it mt b gy my mz l na nb">€	912 457 920 466<br/>Services	80 486 133 495<br/>soumis	136 487 182 495<br/>au	185 488 200 495<br/>mécanisme	204 486 276 495<br/>d'autoliquidation	279 486 381 497<br/>-	383 490 388 492</span></pre><p id="885b" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">Train/Test_image.txt(包含边界框、文档大小和名称):</p><pre class="kj kk kl km gt ms mt mu mv aw mw bi"><span id="5d1e" class="mx la it mt b gy my mz l na nb">€ 912 425 920 434 1653 2339 image1.jpg<br/>TVA 500 441 526 449 1653 2339  image1.jpg<br/>(0%) 529 441 557 451 1653 2339  image1.jpg<br/>0,00 € 882 441 920 451 1653 2339  image1.jpg<br/>Total 500 457 531 466 1653 2339  image1.jpg<br/>en 534 459 549 466 1653 2339  image1.jpg<br/>EUR 553 457 578 466 1653 2339  image1.jpg<br/>3,20 882 457 911 467 1653 2339  image1.jpg<br/>€ 912 457 920 466 1653 2339  image1.jpg<br/>Services 80 486 133 495 1653 2339  image1.jpg<br/>soumis 136 487 182 495 1653 2339  image1.jpg<br/>au 185 488 200 495 1653 2339  image1.jpg<br/>mécanisme 204 486 276 495 1653 2339  image1.jpg<br/>d'autoliquidation 279 486 381 497 1653 2339  image1.jpg<br/>- 383 490 388 492 1653 2339  image1.jpg</span></pre><p id="e250" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">标签. txt:</p><pre class="kj kk kl km gt ms mt mu mv aw mw bi"><span id="8112" class="mx la it mt b gy my mz l na nb">B-DATE_ID<br/>B-INVOICE_ID<br/>B-INVOICE_NUMBER<br/>B-MONTANT_HT<br/>B-MONTANT_HT_ID<br/>B-SELLER<br/>B-TTC<br/>B-DATE<br/>B-TTC_ID<br/>B-TVA<br/>B-TVA_ID<br/>E-DATE_ID<br/>E-DATE<br/>E-INVOICE_ID<br/>E-INVOICE_NUMBER<br/>E-MONTANT_HT<br/>E-MONTANT_HT_ID<br/>E-SELLER<br/>E-TTC<br/>E-TTC_ID<br/>E-TVA<br/>E-TVA_ID<br/>I-DATE_ID<br/>I-DATE<br/>I-SELLER<br/>I-INVOICE_ID<br/>I-MONTANT_HT_ID<br/>I-TTC<br/>I-TTC_ID<br/>I-TVA_ID<br/>O<br/>S-DATE_ID<br/>S-DATE<br/>S-INVOICE_ID<br/>S-INVOICE_NUMBER<br/>S-MONTANT_HT_ID<br/>S-MONTANT_HT<br/>S-SELLER<br/>S-TTC<br/>S-TTC_ID<br/>S-TVA<br/>S-TVA_ID</span></pre><h1 id="cac5" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">微调LayoutLM模型:</h1><p id="bbbd" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">在这里，我们使用带有GPU的google colab来微调模型。以下代码基于<a class="ae ky" href="https://github.com/microsoft/unilm/tree/master/layoutlm" rel="noopener ugc nofollow" target="_blank">原创layoutLM论文</a>和<a class="ae ky" href="https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/LayoutLM/Fine_tuning_LayoutLMForTokenClassification_on_FUNSD.ipynb" rel="noopener ugc nofollow" target="_blank">本教程</a>。</p><p id="aa16" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">首先，安装layoutLM包…</p><pre class="kj kk kl km gt ms mt mu mv aw mw bi"><span id="6b79" class="mx la it mt b gy my mz l na nb">! rm -r unilm</span><span id="2a2f" class="mx la it mt b gy nc mz l na nb">! git clone -b remove_torch_save https://github.com/NielsRogge/unilm.git</span><span id="d358" class="mx la it mt b gy nc mz l na nb">! cd unilm/layoutlm</span><span id="2ea1" class="mx la it mt b gy nc mz l na nb">! pip install unilm/layoutlm</span></pre><p id="1386" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">…以及将从其中下载模型的transformer包:</p><pre class="kj kk kl km gt ms mt mu mv aw mw bi"><span id="a7a9" class="mx la it mt b gy my mz l na nb">! rm -r transformers</span><span id="a687" class="mx la it mt b gy nc mz l na nb">! git clone https://github.com/huggingface/transformers.git</span><span id="4d36" class="mx la it mt b gy nc mz l na nb">! cd transformers</span><span id="8952" class="mx la it mt b gy nc mz l na nb">! pip install ./transformers</span></pre><p id="94a7" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">接下来，创建一个包含labels.txt中唯一标签的列表:</p><pre class="kj kk kl km gt ms mt mu mv aw mw bi"><span id="a2d3" class="mx la it mt b gy my mz l na nb">from torch.nn import CrossEntropyLoss</span><span id="9c61" class="mx la it mt b gy nc mz l na nb">def get_labels(path):<br/>    with open(path, "r") as f:<br/>        labels = f.read().splitlines()<br/>    if "O" not in labels:<br/>        labels = ["O"] + labels<br/>    return labels</span><span id="b3d7" class="mx la it mt b gy nc mz l na nb">labels = get_labels("./labels.txt")<br/>num_labels = len(labels)<br/>label_map = {i: label for i, label in enumerate(labels)}<br/>pad_token_label_id = CrossEntropyLoss().ignore_index</span></pre><p id="967d" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">然后，创建pytorch数据集和数据加载器:</p><pre class="kj kk kl km gt ms mt mu mv aw mw bi"><span id="c94c" class="mx la it mt b gy my mz l na nb">from transformers import LayoutLMTokenizer<br/>from layoutlm.data.funsd import FunsdDataset, InputFeatures<br/>from torch.utils.data import DataLoader, RandomSampler, SequentialSampler</span><span id="913f" class="mx la it mt b gy nc mz l na nb">args = {'local_rank': -1,<br/>        'overwrite_cache': True,<br/>        'data_dir': '/content/data',<br/>        'model_name_or_path':'microsoft/layoutlm-base-uncased',<br/>        'max_seq_length': 512,<br/>        'model_type': 'layoutlm',}</span><span id="871f" class="mx la it mt b gy nc mz l na nb"># class to turn the keys of a dict into attributes<br/>class AttrDict(dict):<br/>    def __init__(self, *args, **kwargs):<br/>        super(AttrDict, self).__init__(*args, **kwargs)<br/>        self.__dict__ = self</span><span id="6b7e" class="mx la it mt b gy nc mz l na nb">args = AttrDict(args)</span><span id="ac63" class="mx la it mt b gy nc mz l na nb">tokenizer = LayoutLMTokenizer.from_pretrained("microsoft/layoutlm-base-uncased")</span><span id="bea2" class="mx la it mt b gy nc mz l na nb"># the LayoutLM authors already defined a specific FunsdDataset, so we are going to use this here<br/>train_dataset = FunsdDataset(args, tokenizer, labels, pad_token_label_id, mode="train")<br/>train_sampler = RandomSampler(train_dataset)<br/>train_dataloader = DataLoader(train_dataset,<br/>                              sampler=train_sampler,<br/>                              batch_size=2)</span><span id="71d4" class="mx la it mt b gy nc mz l na nb">eval_dataset = FunsdDataset(args, tokenizer, labels, pad_token_label_id, mode="test")<br/>eval_sampler = SequentialSampler(eval_dataset)<br/>eval_dataloader = DataLoader(eval_dataset,<br/>                             sampler=eval_sampler,<br/>                            batch_size=2)</span><span id="fa3c" class="mx la it mt b gy nc mz l na nb">batch = next(iter(train_dataloader))</span><span id="c8f9" class="mx la it mt b gy nc mz l na nb">input_ids = batch[0][0]</span><span id="3417" class="mx la it mt b gy nc mz l na nb">tokenizer.decode(input_ids)</span></pre><p id="fad5" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">从huggingface加载模型。这将在数据集上进行微调。</p><pre class="kj kk kl km gt ms mt mu mv aw mw bi"><span id="f5f7" class="mx la it mt b gy my mz l na nb">from transformers import LayoutLMForTokenClassification<br/>import torch</span><span id="706f" class="mx la it mt b gy nc mz l na nb">device = torch.device("cuda" if torch.cuda.is_available() else "cpu")</span><span id="d005" class="mx la it mt b gy nc mz l na nb">model = LayoutLMForTokenClassification.from_pretrained("microsoft/layoutlm-base-uncased", num_labels=num_labels)<br/>model.to(device)</span></pre><p id="f967" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">最后，开始训练:</p><pre class="kj kk kl km gt ms mt mu mv aw mw bi"><span id="bca9" class="mx la it mt b gy my mz l na nb">from transformers import AdamW<br/>from tqdm import tqdm</span><span id="c8c4" class="mx la it mt b gy nc mz l na nb">optimizer = AdamW(model.parameters(), lr=5e-5)</span><span id="a852" class="mx la it mt b gy nc mz l na nb">global_step = 0<br/>num_train_epochs = 50<br/>t_total = len(train_dataloader) * num_train_epochs # total number of training steps</span><span id="4b3d" class="mx la it mt b gy nc mz l na nb">#put the model in training mode<br/>model.train()<br/>for epoch in range(num_train_epochs):<br/>  for batch in tqdm(train_dataloader, desc="Training"):<br/>      input_ids = batch[0].to(device)<br/>      bbox = batch[4].to(device)<br/>      attention_mask = batch[1].to(device)<br/>      token_type_ids = batch[2].to(device)<br/>      labels = batch[3].to(device)</span><span id="b3f2" class="mx la it mt b gy nc mz l na nb"># forward pass<br/>      outputs = model(input_ids=input_ids, bbox=bbox, attention_mask=attention_mask, token_type_ids=token_type_ids,<br/>                      labels=labels)<br/>      loss = outputs.loss</span><span id="0573" class="mx la it mt b gy nc mz l na nb"># print loss every 100 steps<br/>      if global_step % 100 == 0:<br/>        print(f"Loss after {global_step} steps: {loss.item()}")</span><span id="943d" class="mx la it mt b gy nc mz l na nb"># backward pass to get the gradients <br/>      loss.backward()</span><span id="6995" class="mx la it mt b gy nc mz l na nb">#print("Gradients on classification head:")<br/>      #print(model.classifier.weight.grad[6,:].sum())</span><span id="668d" class="mx la it mt b gy nc mz l na nb"># update<br/>      optimizer.step()<br/>      optimizer.zero_grad()<br/>      global_step += 1</span></pre><p id="b64c" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">你应该可以看到训练进度和损失得到更新。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ne"><img src="../Images/591a746a36865c2fa205826be27482b3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pnUCTmC4-47C8S2q-FFFpg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图片作者:布局LM训练进行中</p></figure><p id="a4b2" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">训练后，使用以下函数评估模型性能:</p><pre class="kj kk kl km gt ms mt mu mv aw mw bi"><span id="3a73" class="mx la it mt b gy my mz l na nb">import numpy as np<br/>from seqeval.metrics import (<br/>    classification_report,<br/>    f1_score,<br/>    precision_score,<br/>    recall_score,<br/>)</span><span id="8705" class="mx la it mt b gy nc mz l na nb">eval_loss = 0.0<br/>nb_eval_steps = 0<br/>preds = None<br/>out_label_ids = None</span><span id="a6fd" class="mx la it mt b gy nc mz l na nb"># put model in evaluation mode<br/>model.eval()<br/>for batch in tqdm(eval_dataloader, desc="Evaluating"):<br/>    with torch.no_grad():<br/>        input_ids = batch[0].to(device)<br/>        bbox = batch[4].to(device)<br/>        attention_mask = batch[1].to(device)<br/>        token_type_ids = batch[2].to(device)<br/>        labels = batch[3].to(device)</span><span id="cfe6" class="mx la it mt b gy nc mz l na nb"># forward pass<br/>        outputs = model(input_ids=input_ids, bbox=bbox, attention_mask=attention_mask, token_type_ids=token_type_ids,<br/>                        labels=labels)<br/>        # get the loss and logits<br/>        tmp_eval_loss = outputs.loss<br/>        logits = outputs.logits</span><span id="d5c0" class="mx la it mt b gy nc mz l na nb">eval_loss += tmp_eval_loss.item()<br/>        nb_eval_steps += 1</span><span id="16b6" class="mx la it mt b gy nc mz l na nb"># compute the predictions<br/>        if preds is None:<br/>            preds = logits.detach().cpu().numpy()<br/>            out_label_ids = labels.detach().cpu().numpy()<br/>        else:<br/>            preds = np.append(preds, logits.detach().cpu().numpy(), axis=0)<br/>            out_label_ids = np.append(<br/>                out_label_ids, labels.detach().cpu().numpy(), axis=0<br/>            )</span><span id="0615" class="mx la it mt b gy nc mz l na nb"># compute average evaluation loss<br/>eval_loss = eval_loss / nb_eval_steps<br/>preds = np.argmax(preds, axis=2)</span><span id="915a" class="mx la it mt b gy nc mz l na nb">out_label_list = [[] for _ in range(out_label_ids.shape[0])]<br/>preds_list = [[] for _ in range(out_label_ids.shape[0])]</span><span id="93e4" class="mx la it mt b gy nc mz l na nb">for i in range(out_label_ids.shape[0]):<br/>    for j in range(out_label_ids.shape[1]):<br/>        if out_label_ids[i, j] != pad_token_label_id:<br/>            out_label_list[i].append(label_map[out_label_ids[i][j]])<br/>            preds_list[i].append(label_map[preds[i][j]])</span><span id="b8b4" class="mx la it mt b gy nc mz l na nb">results = {<br/>    "loss": eval_loss,<br/>    "precision": precision_score(out_label_list, preds_list),<br/>    "recall": recall_score(out_label_list, preds_list),<br/>    "f1": f1_score(out_label_list, preds_list),<br/>}</span></pre><p id="4d8a" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">只有50个文档，我们得到以下分数:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nf"><img src="../Images/0884f4985e10189a7f34bcb3a2a7affa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XlkxvCe7nynTLW6PakHtFg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片:培训后的评估分数</p></figure><p id="c8d0" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">注释多了，肯定能得更高的分数。</p><p id="bf5e" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">最后，保存模型以供将来预测:</p><pre class="kj kk kl km gt ms mt mu mv aw mw bi"><span id="31c3" class="mx la it mt b gy my mz l na nb">PATH='./drive/MyDrive/trained_layoutlm/layoutlm_UBIAI.pt'</span><span id="ed6f" class="mx la it mt b gy nc mz l na nb">torch.save(model.state_dict(), PATH)</span></pre><h1 id="1343" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">推论:</h1><p id="1501" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">现在有趣的部分来了，让我们上传一张发票，对其进行OCR，并提取相关实体。对于此测试，我们使用的发票不在培训或测试数据集中。为了解析发票中的文本，我们使用开源的Tesseract包。让我们安装软件包:</p><pre class="kj kk kl km gt ms mt mu mv aw mw bi"><span id="1c27" class="mx la it mt b gy my mz l na nb">!sudo apt install tesseract-ocr</span><span id="8d05" class="mx la it mt b gy nc mz l na nb">!pip install pytesseract</span></pre><p id="6b77" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">在运行预测之前，我们需要解析图像中的文本，并将标记和边界框预处理为特征。为此，我创建了一个预处理python文件<a class="ae ky" href="https://github.com/UBIAI/layout_lm_tutorial.git" rel="noopener ugc nofollow" target="_blank"> layoutLM_preprocess.py </a>，这将使预处理图像更加容易:</p><pre class="kj kk kl km gt ms mt mu mv aw mw bi"><span id="ff1a" class="mx la it mt b gy my mz l na nb">import sys<br/>sys.path.insert(1, './drive/MyDrive/UBIAI_layoutlm')<br/>from layoutlm_preprocess import *</span><span id="e03b" class="mx la it mt b gy nc mz l na nb">image_path='./content/invoice_test.jpg'</span><span id="2dcc" class="mx la it mt b gy nc mz l na nb">image, words, boxes, actual_boxes = preprocess(image_path)</span></pre><p id="a3eb" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">接下来，加载模型并获得单词预测及其边界框:</p><pre class="kj kk kl km gt ms mt mu mv aw mw bi"><span id="e92c" class="mx la it mt b gy my mz l na nb">model_path='./drive/MyDrive/trained_layoutlm/layoutlm_UBIAI.pt'</span><span id="a885" class="mx la it mt b gy nc mz l na nb">model=model_load(model_path,num_labels)</span><span id="0865" class="mx la it mt b gy nc mz l na nb">word_level_predictions, final_boxes=convert_to_features(image, words, boxes, actual_boxes, model)</span></pre><p id="92e6" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">最后，显示带有预测实体和边界框的图像:</p><pre class="kj kk kl km gt ms mt mu mv aw mw bi"><span id="9ed3" class="mx la it mt b gy my mz l na nb">draw = ImageDraw.Draw(image)</span><span id="e732" class="mx la it mt b gy nc mz l na nb">font = ImageFont.load_default()</span><span id="5758" class="mx la it mt b gy nc mz l na nb">def iob_to_label(label):<br/>  if label != 'O':<br/>    return label[2:]<br/>  else:<br/>    return ""</span><span id="2fe3" class="mx la it mt b gy nc mz l na nb">label2color = {'data_id':'green','date':'green','invoice_id':'blue','invoice_number':'blue','montant_ht_id':'black','montant_ht':'black','seller_id':'red','seller':'red', 'ttc_id':'grey','ttc':'grey','':'violet', 'tva_id':'orange','tva':'orange'}</span><span id="f9ff" class="mx la it mt b gy nc mz l na nb">for prediction, box in zip(word_level_predictions, final_boxes):<br/>    predicted_label = iob_to_label(label_map[prediction]).lower()<br/>    draw.rectangle(box, outline=label2color[predicted_label])</span><span id="4ff3" class="mx la it mt b gy nc mz l na nb">    draw.text((box[0] + 10, box[1] - 10), text=predicted_label, fill=label2color[predicted_label], font=font)</span><span id="7b99" class="mx la it mt b gy nc mz l na nb">image</span></pre><p id="e11e" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">瞧吧:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ng"><img src="../Images/3a3d25a1e00eb0452ea043ba756a67c4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tVr0lhUHJclI6xUEMg55VA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片:测试发票上的预测</p></figure><p id="61df" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">虽然该模型犯了一些错误，如将TTC标签分配给购买的商品或不识别某些id，但它能够正确地提取卖家、发票号码、日期和TTC。鉴于带注释的文档数量很少(只有50个)，结果令人印象深刻，非常有希望！有了更多带注释的发票，我们将能够达到更高的F分数和更准确的预测。</p><h1 id="906a" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">结论:</h1><p id="03dc" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">总的来说，LayoutLM模型的结果非常有希望，证明了transformers在分析半结构化文本中的有用性。该模型可以在任何其他半结构化文档上进行微调，如驾照、合同、政府文档、财务文档等。</p><p id="4931" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">如果您有任何问题，请不要犹豫，在下面提问或发送电子邮件至admin@ubiai.tools。</p><p id="52b3" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">如果你喜欢这篇文章，请喜欢并分享！</p><p id="0b16" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">在Twitter上关注我们<a class="ae ky" href="https://twitter.com/UBIAI5" rel="noopener ugc nofollow" target="_blank"> @UBIAI5 </a>或<a class="ae ky" href="https://walidamamou.medium.com/subscribe" rel="noopener">订阅这里</a>！</p></div></div>    
</body>
</html>