<html>
<head>
<title>Clustering types with various applications</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">具有各种应用的聚类类型</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/clustering-types-with-various-applications-b6be738fb2e5?source=collection_archive---------23-----------------------#2021-09-09">https://towardsdatascience.com/clustering-types-with-various-applications-b6be738fb2e5?source=collection_archive---------23-----------------------#2021-09-09</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="8c45" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsdatascience.com/tagged/hands-on-tutorials" rel="noopener" target="_blank">实践教程</a></h2><div class=""/><div class=""><h2 id="de3a" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">通过python实现解释了聚类类型及其使用范围</h2></div><pre class="kr ks kt ku gt kv kw kx ky aw kz bi"><span id="22be" class="la lb it kw b gy lc ld l le lf"><strong class="kw jd"><em class="lg">Table of Contents<br/></em>1. Introduction<br/>2. Clustering Types<br/>2.1. K-Means<br/>-----Theory<br/>-----The optimal number of clusters<br/>-----Implementation<br/>2.2. Mini-Batch K-Means<br/>2.3. DBSCAN<br/>2.4. Agglomerative Clustering<br/>2.5. Mean-Shift<br/>2.6. BIRCH<br/>3. Image Segmentation with Clustering<br/>4. Data Preprocessing with Clustering<br/>5. Gaussian Mixture Model<br/>-----Implementation<br/>-----How to select the number of clusters?<br/>6. Summary</strong></span></pre><h1 id="4df8" class="lh lb it bd li lj lk ll lm ln lo lp lq ki lr kj ls kl lt km lu ko lv kp lw lx bi translated">1.介绍</h1><p id="e093" class="pw-post-body-paragraph ly lz it ma b mb mc kd md me mf kg mg mh mi mj mk ml mm mn mo mp mq mr ms mt im bi translated">利用无监督学习技术，可以通过考虑未标记数据集的相似属性来对其进行分组。但是，这些相似特征在每个算法中的观点是不同的。无监督学习提供关于数据集的详细信息，并标记数据。有了这些获得的信息，数据集可以重新排列，变得更容易理解。通过这种方式，无监督学习被用于客户细分、图像细分、遗传学(聚类DNA模式)、推荐系统(将具有相似观看模式的用户分组在一起)、异常检测等等。根据数据集的统计特性，使用PCA获得新的和简洁的成分，PCA是最常用的降维技术之一，在前一篇文章中提到过。本文详细介绍了聚类的类型，利用聚类进行图像分割，利用聚类进行数据预处理，以及高斯混合方法。python实现支持所有解释。</p><figure class="kr ks kt ku gt mv gh gi paragraph-image"><div role="button" tabindex="0" class="mw mx di my bf mz"><div class="gh gi mu"><img src="../Images/e4b6eb04dd436b0af736622d81d4c650.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*DrQ4Z0RFVEbBIG1Y"/></div></div><p class="nc nd gj gh gi ne nf bd b be z dk translated">由<a class="ae ng" href="https://unsplash.com/@valentinsalja?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">瓦伦丁·萨尔加</a>在<a class="ae ng" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><h1 id="71e4" class="lh lb it bd li lj lk ll lm ln lo lp lq ki lr kj ls kl lt km lu ko lv kp lw lx bi translated">2.聚类类型</h1><h2 id="56ca" class="la lb it bd li nh ni dn lm nj nk dp lq mh nl nm ls ml nn no lu mp np nq lw iz bi translated"><strong class="ak"> 2.1。K-Means </strong></h2><h2 id="77c3" class="la lb it bd li nh ni dn lm nj nk dp lq mh nl nm ls ml nn no lu mp np nq lw iz bi translated"><strong class="ak">理论</strong></h2><p id="b4b9" class="pw-post-body-paragraph ly lz it ma b mb mc kd md me mf kg mg mh mi mj mk ml mm mn mo mp mq mr ms mt im bi translated">K-means聚类是常用的聚类算法之一。基本思想是按照用户确定的数目，根据距聚类中心的距离来放置样本。下面的代码块解释了如何从头开始构建k-means聚类。</p><figure class="kr ks kt ku gt mv"><div class="bz fp l di"><div class="nr ns l"/></div><p class="nc nd gj gh gi ne nf bd b be z dk translated"><a class="ae ng" href="https://github.com/Suji04/ML_from_Scratch/blob/master/K-means%20Clustering.ipynb" rel="noopener ugc nofollow" target="_blank">来源</a></p></figure><p id="d570" class="pw-post-body-paragraph ly lz it ma b mb nt kd md me nu kg mg mh nv mj mk ml nw mn mo mp nx mr ms mt im bi translated">确定数量的聚类的中心被随机放置在数据集中。所有样本都被分配到最近的中心，这种接近度是用上述代码块中的欧几里德距离计算的，但是也可以使用不同的计算方法，例如曼哈顿距离。样本被分配到的中心根据它们的人口更新它们的位置(平均值)。阶段2，即重新计算样本距中心的距离，并分配给最近的中心，并重复该过程。阶段3，每个聚类中心根据数据集使自己重新居中。这个过程持续到一个平衡状态。通过从<code class="fe ny nz oa kw b">mglearn </code>库中导入这里口头描述的步骤，上面的代码块在图1中被可视化。</p><figure class="kr ks kt ku gt mv gh gi paragraph-image"><div class="gh gi ob"><img src="../Images/0a7ea5e5814f9b00aede609ecc55931f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1150/format:webp/1*Xy4_1XC6tVXyqRaDN-trMQ.png"/></div><p class="nc nd gj gh gi ne nf bd b be z dk translated">图一。mg learn . plots . plot _ k means _ algorithm()，图片作者</p></figure><p id="8279" class="pw-post-body-paragraph ly lz it ma b mb nt kd md me nu kg mg mh nv mj mk ml nw mn mo mp nx mr ms mt im bi translated">上面的代码是由sklearn库导入的，并在实践部分中举例说明。众所周知，方法有<em class="lg">如</em><code class="fe ny nz oa kw b">.fit</code><code class="fe ny nz oa kw b">.predict</code><code class="fe ny nz oa kw b"> .transform</code>。使用 <code class="fe ny nz oa kw b">.fit</code>导入<em class="lg">的K-means算法通过将样本放入聚类中、更新聚类中心并重复这些操作来执行上述操作。当达到平衡状态时，即没有变化时，算法完成。星团的中心可以用<code class="fe ny nz oa kw b">Kmeans.cluster_centers_</code> <em class="lg">看到。</em>使用<code class="fe ny nz oa kw b">.predict</code>我们可以预测任何外部样本的聚类，我们想知道它将属于哪个聚类。<em class="lg">用</em> <code class="fe ny nz oa kw b"><em class="lg"> </em>.transform</code>，可以得到样本和每个聚类之间的距离作为一个矩阵。该矩阵的每一行代表样本距每个聚类中心的距离。选择最小值(将样本分配到最近的中心)称为<em class="lg">硬聚类</em>。<strong class="ma jd">在这一点上，值得一提的是，作为每个样本到所选数目的聚类的距离的结果而获得的新数据集<em class="lg"> </em>(与</strong> <code class="fe ny nz oa kw b"><strong class="ma jd">.transform</strong></code> <strong class="ma jd">)实际上是一种非常有效的非线性降维技术。</strong></em></p><h2 id="f902" class="la lb it bd li nh ni dn lm nj nk dp lq mh nl nm ls ml nn no lu mp np nq lw iz bi translated"><strong class="ak">最佳集群数</strong></h2><p id="41df" class="pw-post-body-paragraph ly lz it ma b mb mc kd md me mf kg mg mh mi mj mk ml mm mn mo mp mq mr ms mt im bi translated">找到每个k值的惯性值是选择最佳聚类数的方法之一。<em class="lg">惯性是每个聚类的误差平方和</em>。根据这个定义，具有最低惯性值的数字k将给出最准确的结果。虽然理论上是正确的，但是我们在概化模型的时候，惯性值达到最小值即0的地方就是k =样本数，也就是每个样本接受聚类的点。从这一点来看，最好的方法是检查图形并确定断点，以便选择最准确的集群。</p><figure class="kr ks kt ku gt mv gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/69635a6f003f9caeabf40027df3e7edb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1014/format:webp/1*ZA-2Cy4O52O_UhzONGUOUA.png"/></div><p class="nc nd gj gh gi ne nf bd b be z dk translated">图二。惯性图——聚类数，作者图片</p></figure><p id="df4b" class="pw-post-body-paragraph ly lz it ma b mb nt kd md me nu kg mg mh nv mj mk ml nw mn mo mp nx mr ms mt im bi translated">图2显示了实际部分中用k-means分组的数据集的聚类图的惯性数。查看图表，可以看到衍射(肘形)发生在n=2点，这被解释为要选择的最佳簇数。</p><p id="395d" class="pw-post-body-paragraph ly lz it ma b mb nt kd md me nu kg mg mh nv mj mk ml nw mn mo mp nx mr ms mt im bi translated">另一种方法是确定轮廓得分并比较聚类数中的轮廓得分值。剪影得分是一个介于-1和1之间的值，等于(b-a)/max(a，b)，其中a是到同一聚类中其他实例的平均距离，b是平均最近聚类距离。根据定义，最高分用于确定最佳聚类级别。唯一的缺点是计算复杂。在图3中，显示了实际零件的轮廓分数图形。</p><figure class="kr ks kt ku gt mv gh gi paragraph-image"><div class="gh gi od"><img src="../Images/949e4020e686b670eb0ef31243d37525.png" data-original-src="https://miro.medium.com/v2/resize:fit:1008/format:webp/1*Ei6DvNxjAaJ4S7zyOmAXNQ.png"/></div><p class="nc nd gj gh gi ne nf bd b be z dk translated">图3。剪影评分，作者图像</p></figure><p id="bd62" class="pw-post-body-paragraph ly lz it ma b mb nt kd md me nu kg mg mh nv mj mk ml nw mn mo mp nx mr ms mt im bi translated">可以看出，最高值是在2个集群的级别获得的。</p><blockquote class="oe of og"><p id="f85f" class="ly lz lg ma b mb nt kd md me nu kg mg oh nv mj mk oi nw mn mo oj nx mr ms mt im bi translated">不添加值1，因为至少需要2个聚类来确定轮廓分数。</p></blockquote><h2 id="26e8" class="la lb it bd li nh ni dn lm nj nk dp lq mh nl nm ls ml nn no lu mp np nq lw iz bi translated">履行</h2><p id="2203" class="pw-post-body-paragraph ly lz it ma b mb mc kd md me mf kg mg mh mi mj mk ml mm mn mo mp mq mr ms mt im bi translated">导入Sklearn库中的数据集，然后将K-Means聚类算法应用于这两个数据集，如下所示:</p><figure class="kr ks kt ku gt mv"><div class="bz fp l di"><div class="nr ns l"/></div></figure><p id="fdb6" class="pw-post-body-paragraph ly lz it ma b mb nt kd md me nu kg mg mh nv mj mk ml nw mn mo mp nx mr ms mt im bi translated">肘图和剪影的结果已经显示在上面，k-means的效果显示在图4中。</p><figure class="kr ks kt ku gt mv gh gi paragraph-image"><div class="gh gi ok"><img src="../Images/64e89e6947b2a63f3dc2cb807d77561e.png" data-original-src="https://miro.medium.com/v2/resize:fit:948/format:webp/1*cenxDhICakI2-N_p_D6SvQ.png"/></div><p class="nc nd gj gh gi ne nf bd b be z dk translated">图4。k-means在数据集上的聚类能力，按作者分类的图像</p></figure><h2 id="87b7" class="la lb it bd li nh ni dn lm nj nk dp lq mh nl nm ls ml nn no lu mp np nq lw iz bi translated">2.2.小批量K均值</h2><p id="cb10" class="pw-post-body-paragraph ly lz it ma b mb mc kd md me mf kg mg mh mi mj mk ml mm mn mo mp mq mr ms mt im bi translated">顾名思义，它以小批量更新聚类中心，而不是整个数据集。正如预期的那样，惯性值更高，尽管与k-means相比它缩短了时间。它可用于大型数据集。它在Python中实现如下:</p><figure class="kr ks kt ku gt mv"><div class="bz fp l di"><div class="nr ns l"/></div></figure><p id="5b39" class="pw-post-body-paragraph ly lz it ma b mb nt kd md me nu kg mg mh nv mj mk ml nw mn mo mp nx mr ms mt im bi translated">如图5所示，与K-Means相比，结果几乎相同。</p><figure class="kr ks kt ku gt mv gh gi paragraph-image"><div class="gh gi ol"><img src="../Images/31ef32abaa4bf3a0a74c425e8122243f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1394/format:webp/1*md-twlfRTeWOj_78rrorcQ.png"/></div><p class="nc nd gj gh gi ne nf bd b be z dk translated">图5。小批量k-means在数据集上的聚类能力，按作者分类的图像</p></figure><h2 id="f23f" class="la lb it bd li nh ni dn lm nj nk dp lq mh nl nm ls ml nn no lu mp np nq lw iz bi translated">2.3.基于密度的噪声应用空间聚类</h2><p id="8a6f" class="pw-post-body-paragraph ly lz it ma b mb mc kd md me mf kg mg mh mi mj mk ml mm mn mo mp mq mr ms mt im bi translated">我们可以将带噪声的应用程序的基于密度的空间聚类(DBSCAN)的原理与在一个装满链的盒子中整理出链进行比较。聚类是根据数据集彼此的接近度而不是总体中的样本与中心的接近度来创建的，并且聚类的数量不是由用户设置的。用更正式的术语来说，该算法对距离比用户设置的<code class="fe ny nz oa kw b">€</code>超参数更近的样本进行分组。另一个超参数<code class="fe ny nz oa kw b">min_samples</code>是将该集合分配为一个聚类所需的最小样本数。从表达式和名称可以理解，分组是根据数据集的密度来完成的。密集区的样品称为<em class="lg">岩心样品</em>。上面的菜谱如图6所示，使用了<code class="fe ny nz oa kw b">mglearn</code>库。</p><figure class="kr ks kt ku gt mv gh gi paragraph-image"><div role="button" tabindex="0" class="mw mx di my bf mz"><div class="gh gi om"><img src="../Images/13f165ae93b3b412d6e1e7eb86256631.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vIOc0t6VuaFza3ZGWNehCQ.png"/></div></div><p class="nc nd gj gh gi ne nf bd b be z dk translated">图6。mglearn.plots.plot_dbscan()，图片作者</p></figure><p id="f042" class="pw-post-body-paragraph ly lz it ma b mb nt kd md me nu kg mg mh nv mj mk ml nw mn mo mp nx mr ms mt im bi translated">当<code class="fe ny nz oa kw b">eps=1.5</code>被检查时，同时创建3个不同的组，并在<code class="fe ny nz oa kw b">min_samples=2</code>的情况下对所有样本进行标记。因为所有数据集都比<code class="fe ny nz oa kw b">eps=1.5</code>更接近最近的数据。在相同的情况下，当<code class="fe ny nz oa kw b">min_samples=5</code>被设置时，仅创建5个数据簇，而7个数据保持未标记。因为数据离所创建的簇的距离远。另外，没有形成新的聚类的原因是在<code class="fe ny nz oa kw b">eps=1.5</code>距离处没有5个彼此接近的样本。可以看出，簇的数量是根据用DBSCAN设置的超参数来确定的。<strong class="ma jd">如预期的那样，在具有适当设置的异常检测中使用非常有用。</strong></p><p id="86ac" class="pw-post-body-paragraph ly lz it ma b mb nt kd md me nu kg mg mh nv mj mk ml nw mn mo mp nx mr ms mt im bi translated">它已应用于以下代码块中的数据集:</p><figure class="kr ks kt ku gt mv"><div class="bz fp l di"><div class="nr ns l"/></div></figure><p id="d35d" class="pw-post-body-paragraph ly lz it ma b mb nt kd md me nu kg mg mh nv mj mk ml nw mn mo mp nx mr ms mt im bi translated">Moons数据集在结构上不适合用k-means进行分离，但经过数据标准化处理后，用DBSCAN进行了相当优雅的聚类。与K-Means不同，DBSCAN的结构中不包含<code class="fe ny nz oa kw b"> .predict</code>方法，因此无法根据这个数据集确定外部数据属于哪个聚类。但是用<code class="fe ny nz oa kw b">dbscan.component_</code>提取DBSCAN分量，然后称为矩阵X；使用<code class="fe ny nz oa kw b">dbscan.labels_</code>方法提取标签，然后调用y，<strong class="ma jd">在最后一步，使用KNeighbourClassifier对标签进行训练，该模型对外部数据也很有用。</strong></p><figure class="kr ks kt ku gt mv gh gi paragraph-image"><div class="gh gi ol"><img src="../Images/c269c35e6710047342ea672440ea49b5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1394/format:webp/1*IHCteyoImhRbg4icFd-PAg.png"/></div><p class="nc nd gj gh gi ne nf bd b be z dk translated">图7。DBSCAN对数据集的聚类能力，按作者分类的图像</p></figure><h2 id="bcad" class="la lb it bd li nh ni dn lm nj nk dp lq mh nl nm ls ml nn no lu mp np nq lw iz bi translated">2.4.凝聚聚类</h2><p id="060c" class="pw-post-body-paragraph ly lz it ma b mb mc kd md me mf kg mg mh mi mj mk ml mm mn mo mp mq mr ms mt im bi translated">每个样本从一个聚类开始，小聚类(样本聚类)与用户选择的条件相结合，直到达到指定的聚类数。这些条件是:</p><p id="8594" class="pw-post-body-paragraph ly lz it ma b mb nt kd md me nu kg mg mh nv mj mk ml nw mn mo mp nx mr ms mt im bi translated"><code class="fe ny nz oa kw b">linkage= ‘ward’</code>:最小化被合并聚类的方差(默认)</p><p id="1d54" class="pw-post-body-paragraph ly lz it ma b mb nt kd md me nu kg mg mh nv mj mk ml nw mn mo mp nx mr ms mt im bi translated"><code class="fe ny nz oa kw b">linkage= ‘average’</code>:使用每次观察距离的平均值</p><p id="d3ec" class="pw-post-body-paragraph ly lz it ma b mb nt kd md me nu kg mg mh nv mj mk ml nw mn mo mp nx mr ms mt im bi translated"><code class="fe ny nz oa kw b">linkage= ‘complete’</code>:使用两个集合的所有观测值之间的最大距离。</p><p id="4ce5" class="pw-post-body-paragraph ly lz it ma b mb nt kd md me nu kg mg mh nv mj mk ml nw mn mo mp nx mr ms mt im bi translated"><code class="fe ny nz oa kw b">linkage =‘single’</code>:使用两个集合的所有观测值之间的最小距离。</p><p id="0bf5" class="pw-post-body-paragraph ly lz it ma b mb nt kd md me nu kg mg mh nv mj mk ml nw mn mo mp nx mr ms mt im bi translated">此外，聚类之间的距离可以用亲和度超参数来调整。<em class="lg">【欧几里得】</em><em class="lg">【L1】</em><em class="lg">【L2】</em><em class="lg">【曼哈顿】</em><em class="lg">【余弦】</em>，或者<em class="lg">【预计算】</em>。用图8所示的<code class="fe ny nz oa kw b">mglearn </code>库可视化凝聚聚类。</p><figure class="kr ks kt ku gt mv gh gi paragraph-image"><div role="button" tabindex="0" class="mw mx di my bf mz"><div class="gh gi on"><img src="../Images/7cc6b04c95dd755fcf17abc8aa1ee6ee.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*h4ETybykaPjH626usIbhww.png"/></div></div><p class="nc nd gj gh gi ne nf bd b be z dk translated">图8。mg learn . plots . plot _ aggregate _ algorithm()，图片由作者提供</p></figure><p id="6166" class="pw-post-body-paragraph ly lz it ma b mb nt kd md me nu kg mg mh nv mj mk ml nw mn mo mp nx mr ms mt im bi translated">可以看出，聚类过程从最接近的样本开始，并且合并继续进行，直到用户确定了聚类的数量。由于其结构，凝聚聚类不包括<code class="fe ny nz oa kw b">.predict</code>方法，就像DBSCAN一样。外部数据是用fit_predict方法估计的。凝聚聚类通过生成层次聚类来工作。可视化这些层次聚类的最佳方式是创建树状图。它通过以下代码块应用于我们的moons数据集:</p><figure class="kr ks kt ku gt mv"><div class="bz fp l di"><div class="nr ns l"/></div></figure><p id="d434" class="pw-post-body-paragraph ly lz it ma b mb nt kd md me nu kg mg mh nv mj mk ml nw mn mo mp nx mr ms mt im bi translated">代码块的输出如图9所示。</p><figure class="kr ks kt ku gt mv gh gi paragraph-image"><div class="gh gi ol"><img src="../Images/ec7845939ef4ab879f61499d8ab57e35.png" data-original-src="https://miro.medium.com/v2/resize:fit:1394/format:webp/1*RQ0BBybfiJgxtsO_ru1Nmw.png"/></div><p class="nc nd gj gh gi ne nf bd b be z dk translated">图9。聚集聚类在数据集上的聚类能力，按作者分类的图像</p></figure><p id="7c1c" class="pw-post-body-paragraph ly lz it ma b mb nt kd md me nu kg mg mh nv mj mk ml nw mn mo mp nx mr ms mt im bi translated">如果从低级到高级来解读，每个样本就是一个聚类。所以，我们有一些样本群。由于它们的数量太多，合并过程发生在底部，如图10所示。这一过程将继续，直到用户确定集群的数量，并将在指定点中断。</p><figure class="kr ks kt ku gt mv gh gi paragraph-image"><div role="button" tabindex="0" class="mw mx di my bf mz"><div class="gh gi oo"><img src="../Images/4318f1fd618df9b88cc524e3c246e8ca.png" data-original-src="https://miro.medium.com/v2/resize:fit:958/format:webp/1*vqG0V82u5ngwRrriRwzpeQ.png"/></div></div><p class="nc nd gj gh gi ne nf bd b be z dk translated">图10。月球数据集的树状图，图片由作者提供</p></figure><h2 id="143e" class="la lb it bd li nh ni dn lm nj nk dp lq mh nl nm ls ml nn no lu mp np nq lw iz bi translated">2.5.均值漂移</h2><p id="c873" class="pw-post-body-paragraph ly lz it ma b mb mc kd md me mf kg mg mh mi mj mk ml mm mn mo mp mq mr ms mt im bi translated">它从放置在数据集中的一个圆开始，然后移动以显示该圆中数据的平均值。到达新位置后，将计算内部数据的平均值，并再次将其置于中心位置。这个过程一直重复到平衡状态。具有高密度的地方可以被定义为基于密度的算法，因为它们会将平均值拉向它们(即均值偏移)。我们还可以在密集区域内检测不同的集群，假设我们缩小了圆圈的大小。一旦带宽发生变化，就会创建许多集群，如图12所示。</p><figure class="kr ks kt ku gt mv"><div class="bz fp l di"><div class="nr ns l"/></div></figure><p id="d782" class="pw-post-body-paragraph ly lz it ma b mb nt kd md me nu kg mg mh nv mj mk ml nw mn mo mp nx mr ms mt im bi translated"><code class="fe ny nz oa kw b">bandwith=0.75</code>设置在月球数据集中，结果如下:</p><figure class="kr ks kt ku gt mv gh gi paragraph-image"><div class="gh gi ol"><img src="../Images/6551a474e247ccd82736514b1d3ecd42.png" data-original-src="https://miro.medium.com/v2/resize:fit:1394/format:webp/1*BLPBio6leTnwRAYMcwMtkQ.png"/></div><p class="nc nd gj gh gi ne nf bd b be z dk translated">图11。Mean-Shift对数据集的聚类能力，按作者分类的图像</p></figure><figure class="kr ks kt ku gt mv gh gi paragraph-image"><div role="button" tabindex="0" class="mw mx di my bf mz"><div class="gh gi op"><img src="../Images/ba290837a9e92ebd15725e7afe7362f8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4tNSq6bFSrG9GIQKHBKsoQ.png"/></div></div><p class="nc nd gj gh gi ne nf bd b be z dk translated">图12。不同带宽对均值漂移聚类算法的影响，图片由作者提供</p></figure><h2 id="36f1" class="la lb it bd li nh ni dn lm nj nk dp lq mh nl nm ls ml nn no lu mp np nq lw iz bi translated">2.6.桦树</h2><p id="bc07" class="pw-post-body-paragraph ly lz it ma b mb mc kd md me mf kg mg mh mi mj mk ml mm mn mo mp mq mr ms mt im bi translated">BIRCH(使用层次结构的平衡迭代缩减和聚类)是一种基于树的算法，适用于大型数据集。与Mini Batch k means相比，它能以相似的成功率提供更快的结果。由于聚类过程是基于树的，因此它可以快速地将样本分配到聚类中，而无需将它们与创建的模型一起存储。所以才快。但是，由于它是基于树的，所以建议在特征数不超过20时使用它。</p><figure class="kr ks kt ku gt mv"><div class="bz fp l di"><div class="nr ns l"/></div></figure><figure class="kr ks kt ku gt mv gh gi paragraph-image"><div class="gh gi ol"><img src="../Images/5d5507384ec2c81b1b0c41392d185f62.png" data-original-src="https://miro.medium.com/v2/resize:fit:1394/format:webp/1*Pf_fxrunFoKIY6iZMXJloQ.png"/></div><p class="nc nd gj gh gi ne nf bd b be z dk translated">图12。Birch在数据集上的聚类能力，按作者分类的图像</p></figure><h1 id="4a17" class="lh lb it bd li lj lk ll lm ln lo lp lq ki lr kj ls kl lt km lu ko lv kp lw lx bi translated">3.聚类图像分割</h1><p id="68e9" class="pw-post-body-paragraph ly lz it ma b mb mc kd md me mf kg mg mh mi mj mk ml mm mn mo mp mq mr ms mt im bi translated">由于在本节之前我们已经处理了足够多的sklearn库，所以让我们通过使用CV2库将K-Means应用于图13中看到的图像。以下代码块已应用于不同k值的图像。</p><figure class="kr ks kt ku gt mv"><div class="bz fp l di"><div class="nr ns l"/></div></figure><ul class=""><li id="6251" class="oq or it ma b mb nt me nu mh os ml ot mp ou mt ov ow ox oy bi translated">K均值的结果如下:</li></ul><figure class="kr ks kt ku gt mv gh gi paragraph-image"><div role="button" tabindex="0" class="mw mx di my bf mz"><div class="gh gi oz"><img src="../Images/4b610a8ba05c5e021774f0a7a9417c67.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*D4zgiM4fixB080S7MyfvhQ.jpeg"/></div></div><p class="nc nd gj gh gi ne nf bd b be z dk translated">图13。使用k-means的图像分割，按作者分类的图像</p></figure><ul class=""><li id="c24e" class="oq or it ma b mb nt me nu mh os ml ot mp ou mt ov ow ox oy bi translated">DBSCAN的结果如下:</li></ul><figure class="kr ks kt ku gt mv gh gi paragraph-image"><div role="button" tabindex="0" class="mw mx di my bf mz"><div class="gh gi pa"><img src="../Images/e27b43a6e696cafef65679b6fb406c0d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6wpqzXU24REQBtN9FoRphg.png"/></div></div><p class="nc nd gj gh gi ne nf bd b be z dk translated">图14。使用DBSCAN的图像分割，按作者分类的图像</p></figure><h1 id="afaa" class="lh lb it bd li lj lk ll lm ln lo lp lq ki lr kj ls kl lt km lu ko lv kp lw lx bi translated">4.聚类数据预处理</h1><p id="e9cc" class="pw-post-body-paragraph ly lz it ma b mb mc kd md me mf kg mg mh mi mj mk ml mm mn mo mp mq mr ms mt im bi translated">如果我们从图像数据集中对其进行解释，则有数百个特征，并且如果这些特征是通过聚类得到的，则可以认为这些特征被分组并且进行了降维。在以下代码块中，使用决策树算法对Load_digits数据集进行分类，包括应用聚类和不应用聚类:</p><figure class="kr ks kt ku gt mv"><div class="bz fp l di"><div class="nr ns l"/></div></figure><p id="6287" class="pw-post-body-paragraph ly lz it ma b mb nt kd md me nu kg mg mh nv mj mk ml nw mn mo mp nx mr ms mt im bi translated">可以看出，当聚类数设置为40时，得分率从83增加到88。</p><h1 id="f2ad" class="lh lb it bd li lj lk ll lm ln lo lp lq ki lr kj ls kl lt km lu ko lv kp lw lx bi translated">5.高斯混合模型</h1><p id="a476" class="pw-post-body-paragraph ly lz it ma b mb mc kd md me mf kg mg mh mi mj mk ml mm mn mo mp mq mr ms mt im bi translated">GMM是一种概率模型，它假设数据集由具有未知参数的单个高斯分布的组合组成。如上所述，使用K-means进行硬聚类，并将样本分配到最近的聚类。考虑到k均值数据集中的分布和密度等因素，这是一种基于聚类中心的无监督学习技术，k均值可能会给出误导性的结果。对于密度，上面可以看出DBSCAN更成功。如果按照分布来处理，GMM的聚类效果会更好。为了更好地理解GMM，我们来定义构造块高斯。什么是高斯，它是主要成分，它揭示了什么？</p><p id="5bb9" class="pw-post-body-paragraph ly lz it ma b mb nt kd md me nu kg mg mh nv mj mk ml nw mn mo mp nx mr ms mt im bi translated">当样本绘制在直方图中时，高斯分布代表独特的钟形曲线。当存在与测量值相互作用的随机因素时，会出现正态(高斯)分布。在正态分布中，大多数数据点将具有类似于平均值的度量，并且平均值是钟形曲线的中心。然而，更少的数据点具有比平均值大得多或小得多的值。分布中曲线的宽度对应于标准差。在1.0 std区间中，整个数据集的68.3%发生，而在2.0 std区间中，则为95.4%。图15显示了从直方图中绘制的正态(高斯)分布和均值、标准差点。</p><figure class="kr ks kt ku gt mv gh gi paragraph-image"><div class="gh gi pb"><img src="../Images/93d234ddd55110c5e4c53b6e3d0bf2e7.png" data-original-src="https://miro.medium.com/v2/resize:fit:768/format:webp/1*GsjoefOkWBU95aDNFda9xw.png"/></div><p class="nc nd gj gh gi ne nf bd b be z dk translated">图15。高斯(正态)分布，图片由作者提供</p></figure><p id="9837" class="pw-post-body-paragraph ly lz it ma b mb nt kd md me nu kg mg mh nv mj mk ml nw mn mo mp nx mr ms mt im bi translated">高斯混合模型组是高斯组合的结果，它揭示了数据集中的统计分布。从高斯模型中创建的标准偏差值导出的方差(对于1D)或协方差(对于2D)值在这一点上起着积极的作用。K-means不考虑方差值，只根据与聚类中心的接近程度进行分布。虽然高斯分布生成关于数据属于哪个聚类的概率比率(这些比率之和=1)，但这意味着软聚类；K-Means聚类偏好硬聚类。还应该注意到，GMM使用期望最大化算法来拟合形成它的高斯分布。高斯混合模型将在后续文章中详细介绍。</p><h2 id="d8c3" class="la lb it bd li nh ni dn lm nj nk dp lq mh nl nm ls ml nn no lu mp np nq lw iz bi translated">履行</h2><p id="6ba7" class="pw-post-body-paragraph ly lz it ma b mb mc kd md me mf kg mg mh mi mj mk ml mm mn mo mp mq mr ms mt im bi translated">由于在sklearn数据集上实现GMM非常容易，因此在下面的代码块中，它被应用于图像分割中使用的花朵图像:</p><figure class="kr ks kt ku gt mv"><div class="bz fp l di"><div class="nr ns l"/></div></figure><figure class="kr ks kt ku gt mv gh gi paragraph-image"><div role="button" tabindex="0" class="mw mx di my bf mz"><div class="gh gi pc"><img src="../Images/6510626bbd6c508217f8e7e27b270475.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zbov6nRzv0BerIYMS7kijg.png"/></div></div><p class="nc nd gj gh gi ne nf bd b be z dk translated">图16。GMM的图像分割，作者的图像</p></figure><p id="4fcf" class="pw-post-body-paragraph ly lz it ma b mb nt kd md me nu kg mg mh nv mj mk ml nw mn mo mp nx mr ms mt im bi translated">有必要在实现部分提到一个基本的超参数，协方差_type。协方差类型可以是:</p><p id="7306" class="pw-post-body-paragraph ly lz it ma b mb nt kd md me nu kg mg mh nv mj mk ml nw mn mo mp nx mr ms mt im bi translated"><code class="fe ny nz oa kw b">covariance_type=‘full’</code>:每个组件都有自己的通用协方差矩阵，这意味着聚类可以是任何形状、大小、方向(默认)</p><p id="a4d0" class="pw-post-body-paragraph ly lz it ma b mb nt kd md me nu kg mg mh nv mj mk ml nw mn mo mp nx mr ms mt im bi translated"><code class="fe ny nz oa kw b">covariance_type=‘tied’</code>:所有分量共享相同的一般协方差矩阵，这意味着所有聚类具有相同的椭球形状、大小、方向</p><p id="9fa2" class="pw-post-body-paragraph ly lz it ma b mb nt kd md me nu kg mg mh nv mj mk ml nw mn mo mp nx mr ms mt im bi translated"><code class="fe ny nz oa kw b">covariance_type=‘diag’</code>:每个分量都有自己的对角协方差矩阵，这意味着聚类可以是任何椭球大小。</p><p id="282c" class="pw-post-body-paragraph ly lz it ma b mb nt kd md me nu kg mg mh nv mj mk ml nw mn mo mp nx mr ms mt im bi translated"><code class="fe ny nz oa kw b">covariance_type=‘spherical’</code>:每个组件都有自己的单个方差，这意味着所有簇必须是球形的，但它们可以有不同的直径。</p><figure class="kr ks kt ku gt mv gh gi paragraph-image"><div role="button" tabindex="0" class="mw mx di my bf mz"><div class="gh gi pd"><img src="../Images/8b541d59731a0668ccd0bef4492230c0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DsYoNVJhR0tfmfFhp5rdyw.png"/></div></div><p class="nc nd gj gh gi ne nf bd b be z dk translated">图17。协方差类型的可视化，<a class="ae ng" href="https://github.com/scikit-learn/scikit-learn/issues/10863" rel="noopener ugc nofollow" target="_blank">来源</a></p></figure><h2 id="844a" class="la lb it bd li nh ni dn lm nj nk dp lq mh nl nm ls ml nn no lu mp np nq lw iz bi translated">如何选择集群的数量？</h2><p id="8d6f" class="pw-post-body-paragraph ly lz it ma b mb mc kd md me mf kg mg mh mi mj mk ml mm mn mo mp mq mr ms mt im bi translated">上面提到轮廓分数或惯性可以用在k均值头中。然而，因为高斯混合模型的聚类形状可能不是球形的或者可能具有不同的大小，所以根据特定的度量系统进行选择可能会产生误导。在为GMM选择最佳聚类数时，使用贝叶斯信息准则或赤池信息准则更为正确。</p><p id="c6c4" class="pw-post-body-paragraph ly lz it ma b mb nt kd md me nu kg mg mh nv mj mk ml nw mn mo mp nx mr ms mt im bi translated"><em class="lg"> AIC和BIC是用于评分和选择最佳模型的概率模型选择技术。</em></p><p id="fac6" class="pw-post-body-paragraph ly lz it ma b mb nt kd md me nu kg mg mh nv mj mk ml nw mn mo mp nx mr ms mt im bi translated">让我们看看上面月亮数据集的bic&amp;aic值，并将其可视化:</p><figure class="kr ks kt ku gt mv"><div class="bz fp l di"><div class="nr ns l"/></div></figure><figure class="kr ks kt ku gt mv gh gi paragraph-image"><div class="gh gi pe"><img src="../Images/3f970b7192611d57b5de82137d31fdc4.png" data-original-src="https://miro.medium.com/v2/resize:fit:790/format:webp/1*z6Vs3qIuwDGLg5YSGUYvUA.png"/></div><p class="nc nd gj gh gi ne nf bd b be z dk translated">图18。AIC和BIC-k的图表，图片由作者提供</p></figure><p id="81e7" class="pw-post-body-paragraph ly lz it ma b mb nt kd md me nu kg mg mh nv mj mk ml nw mn mo mp nx mr ms mt im bi translated">可以看出，在n=2的情况下，AIC和BIC都是最大值。在代码块的中间部分，还测试了它们针对不同协方差类型的性能，结果如图19所示。</p><figure class="kr ks kt ku gt mv gh gi paragraph-image"><div role="button" tabindex="0" class="mw mx di my bf mz"><div class="gh gi pf"><img src="../Images/444cd9ba3ee0c946478df6cd2f656870.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*f_CbFIRntqDAOsy8JsDBUw.png"/></div></div><p class="nc nd gj gh gi ne nf bd b be z dk translated">图19。不同协方差类型的比较，按作者分类的图像</p></figure><p id="2bd2" class="pw-post-body-paragraph ly lz it ma b mb nt kd md me nu kg mg mh nv mj mk ml nw mn mo mp nx mr ms mt im bi translated">可以看出，对于每个k值，获得了最佳BIC值<code class="fe ny nz oa kw b">covariance_type=‘spherical’</code>。</p><h1 id="88c5" class="lh lb it bd li lj lk ll lm ln lo lp lq ki lr kj ls kl lt km lu ko lv kp lw lx bi translated">6.摘要</h1><p id="384f" class="pw-post-body-paragraph ly lz it ma b mb mc kd md me mf kg mg mh mi mj mk ml mm mn mo mp mq mr ms mt im bi translated">上面讨论了大多数聚类技术。解释了它们的理论部分，并用python实现了它们，并给出了基本示例。图20显示了包括聚类技术摘要的图像。</p><figure class="kr ks kt ku gt mv gh gi paragraph-image"><div role="button" tabindex="0" class="mw mx di my bf mz"><div class="gh gi pg"><img src="../Images/393942a44e78c001b2b64e557896c792.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kCHwSnWZCTTEpo3Pe0_RYw.png"/></div></div><p class="nc nd gj gh gi ne nf bd b be z dk translated">图20。<a class="ae ng" href="https://scikit-learn.org/stable/auto_examples/cluster/plot_cluster_comparison.html#sphx-glr-auto-examples-cluster-plot-cluster-comparison-py" rel="noopener ugc nofollow" target="_blank">聚类技术总结，来源</a></p></figure><p id="3588" class="pw-post-body-paragraph ly lz it ma b mb nt kd md me nu kg mg mh nv mj mk ml nw mn mo mp nx mr ms mt im bi translated">上一篇文章涉及到最有用的降维技术之一，即主成分分析(PCA)。本文涵盖了集群类型及其在Python实现中的一些使用领域。</p><blockquote class="oe of og"><p id="d5d4" class="ly lz lg ma b mb nt kd md me nu kg mg oh nv mj mk oi nw mn mo oj nx mr ms mt im bi translated">剩余的主题，如离群点检测，期望最大化元算法(EM)，自组织映射(SOM)，模糊C均值等。将在后续文章中讨论。</p></blockquote><div class="ph pi gp gr pj pk"><a rel="noopener follow" target="_blank" href="/comprehensive-guide-for-principal-component-analysis-7bf2b4a048ae"><div class="pl ab fo"><div class="pm ab pn cl cj po"><h2 class="bd jd gy z fp pp fr fs pq fu fw jc bi translated">主成分分析综合指南</h2><div class="pr l"><h3 class="bd b gy z fp pp fr fs pq fu fw dk translated">用python实现主成分分析的理论和实践部分</h3></div><div class="ps l"><p class="bd b dl z fp pp fr fs pq fu fw dk translated">towardsdatascience.com</p></div></div><div class="pt l"><div class="pu l pv pw px pt py na pk"/></div></div></a></div><div class="ph pi gp gr pj pk"><a href="https://ibrahimkovan.medium.com/machine-learning-guideline-959da5c6f73d" rel="noopener follow" target="_blank"><div class="pl ab fo"><div class="pm ab pn cl cj po"><h2 class="bd jd gy z fp pp fr fs pq fu fw jc bi translated">机器学习指南</h2><div class="pr l"><h3 class="bd b gy z fp pp fr fs pq fu fw dk translated">所有与机器学习相关的文章</h3></div><div class="ps l"><p class="bd b dl z fp pp fr fs pq fu fw dk translated">ibrahimkovan.medium.com</p></div></div><div class="pt l"><div class="pz l pv pw px pt py na pk"/></div></div></a></div></div></div>    
</body>
</html>