<html>
<head>
<title>Kaggler’s Guide to LightGBM Hyperparameter Tuning with Optuna in 2021</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">2021年使用Optuna调整LightGBM超参数的Kaggler指南</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/kagglers-guide-to-lightgbm-hyperparameter-tuning-with-optuna-in-2021-ed048d9838b5?source=collection_archive---------0-----------------------#2021-09-03">https://towardsdatascience.com/kagglers-guide-to-lightgbm-hyperparameter-tuning-with-optuna-in-2021-ed048d9838b5?source=collection_archive---------0-----------------------#2021-09-03</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="9126" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">充分发挥LightGBM型号的性能</h2></div><p id="af2c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">关于LightGBM超参数以及如何使用Optuna调整它们的综合教程。</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="gh gi le"><img src="../Images/3ac19efd360a48be88e90ef604dea41e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9yLM3nI8IR5I7w2A2mdQlQ.jpeg"/></div></div><p class="lq lr gj gh gi ls lt bd b be z dk translated"><strong class="bd lu">照片由</strong> <a class="ae lv" href="https://www.pexels.com/@pixabay?utm_content=attributionCopyText&amp;utm_medium=referral&amp;utm_source=pexels" rel="noopener ugc nofollow" target="_blank"> <strong class="bd lu"> Pixabay </strong> </a> <strong class="bd lu">上的</strong> <a class="ae lv" href="https://www.pexels.com/photo/silhouette-of-person-holding-sparkler-digital-wallpaepr-266429/?utm_content=attributionCopyText&amp;utm_medium=referral&amp;utm_source=pexels" rel="noopener ugc nofollow" target="_blank"> <strong class="bd lu">像素组成。</strong> </a> <strong class="bd lu">除特别注明外，所有图片均为作者所有。</strong></p></figure><h1 id="9e43" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">介绍</h1><p id="9d5b" class="pw-post-body-paragraph ki kj it kk b kl mo ju kn ko mp jx kq kr mq kt ku kv mr kx ky kz ms lb lc ld im bi translated">在上一篇文章中，我们讨论了LightGBM的基础知识，并创建了几乎在各个方面都胜过XGBoost的LGBM模型。本文关注任何机器学习项目的最后一个阶段——超参数调优(如果我们省略模型集成的话)。</p><p id="7fb0" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">首先，我们将看看最重要的LGBM超参数，按其影响程度和区域分组。然后，我们将看到一个使用Optuna(下一代贝叶斯超参数调优框架)调优LGBM参数的实践示例。</p><p id="907e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">最重要的是，我们将以类似于顶级Kagglers调整他们的LGBM模型的方式来实现这一点，这些模型取得了令人印象深刻的结果。</p><blockquote class="mt mu mv"><p id="ef1e" class="ki kj mw kk b kl km ju kn ko kp jx kq mx ks kt ku my kw kx ky mz la lb lc ld im bi translated">如果你是LGBM的新手，我强烈建议你阅读文章的第一部分。虽然我将简要解释Optuna是如何工作的，但我也建议阅读我关于它的单独文章,以便更好地理解这篇文章。</p></blockquote><div class="na nb gp gr nc nd"><a rel="noopener follow" target="_blank" href="/how-to-beat-the-heck-out-of-xgboost-with-lightgbm-comprehensive-tutorial-5eba52195997"><div class="ne ab fo"><div class="nf ab ng cl cj nh"><h2 class="bd iu gy z fp ni fr fs nj fu fw is bi translated">你错过了LightGBM。它在各个方面都击败了XGBoost</h2><div class="nk l"><h3 class="bd b gy z fp ni fr fs nj fu fw dk translated">编辑描述</h3></div><div class="nl l"><p class="bd b dl z fp ni fr fs nj fu fw dk translated">towardsdatascience.com</p></div></div><div class="nm l"><div class="nn l no np nq nm nr lo nd"/></div></div></a></div><div class="na nb gp gr nc nd"><a rel="noopener follow" target="_blank" href="/why-is-everyone-at-kaggle-obsessed-with-optuna-for-hyperparameter-tuning-7608fdca337c"><div class="ne ab fo"><div class="nf ab ng cl cj nh"><h2 class="bd iu gy z fp ni fr fs nj fu fw is bi translated">为什么Kaggle的所有人都痴迷于Optuna进行超参数调优？</h2><div class="nk l"><h3 class="bd b gy z fp ni fr fs nj fu fw dk translated">编辑描述</h3></div><div class="nl l"><p class="bd b dl z fp ni fr fs nj fu fw dk translated">towardsdatascience.com</p></div></div><div class="nm l"><div class="ns l no np nq nm nr lo nd"/></div></div></a></div><blockquote class="mt mu mv"><p id="f76f" class="ki kj mw kk b kl km ju kn ko kp jx kq mx ks kt ku my kw kx ky mz la lb lc ld im bi translated">点击这里获取Kaggle上的文章<a class="ae lv" href="https://www.kaggle.com/bextuychiev/lgbm-optuna-hyperparameter-tuning-w-understanding/edit" rel="noopener ugc nofollow" target="_blank">的笔记本。</a></p></blockquote></div><div class="ab cl nt nu hx nv" role="separator"><span class="nw bw bk nx ny nz"/><span class="nw bw bk nx ny nz"/><span class="nw bw bk nx ny"/></div><div class="im in io ip iq"><h1 id="6777" class="lw lx it bd ly lz oa mb mc md ob mf mg jz oc ka mi kc od kd mk kf oe kg mm mn bi translated">最重要参数概述</h1><p id="c4de" class="pw-post-body-paragraph ki kj it kk b kl mo ju kn ko mp jx kq kr mq kt ku kv mr kx ky kz ms lb lc ld im bi translated">通常，大多数基于树的模型的超参数可以分为4类:</p><ol class=""><li id="6ac3" class="of og it kk b kl km ko kp kr oh kv oi kz oj ld ok ol om on bi translated">影响决策树的结构和学习的参数</li><li id="a805" class="of og it kk b kl oo ko op kr oq kv or kz os ld ok ol om on bi translated">影响训练速度的参数</li><li id="ca72" class="of og it kk b kl oo ko op kr oq kv or kz os ld ok ol om on bi translated">更高精度的参数</li><li id="ee71" class="of og it kk b kl oo ko op kr oq kv or kz os ld ok ol om on bi translated">对抗过度拟合的参数</li></ol><p id="c783" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">大多数时候，这些类别有很多重叠，提高一个类别的效率可能会降低另一个类别的效率。这就是为什么手动调整它们是一个巨大的错误，应该避免。</p><p id="c1ee" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如果给定足够好的参数网格，像Optuna这样的框架可以自动找到这些类别之间的“中间值”。</p><div class="na nb gp gr nc nd"><a href="https://ibexorigin.medium.com/membership" rel="noopener follow" target="_blank"><div class="ne ab fo"><div class="nf ab ng cl cj nh"><h2 class="bd iu gy z fp ni fr fs nj fu fw is bi translated">通过我的推荐链接加入Medium-BEXGBoost</h2><div class="nk l"><h3 class="bd b gy z fp ni fr fs nj fu fw dk translated">获得独家访问我的所有⚡premium⚡内容和所有媒体没有限制。支持我的工作，给我买一个…</h3></div><div class="nl l"><p class="bd b dl z fp ni fr fs nj fu fw dk translated">ibexorigin.medium.com</p></div></div><div class="nm l"><div class="ot l no np nq nm nr lo nd"/></div></div></a></div><p id="c7bd" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">获得由强大的AI-Alpha信号选择和总结的最佳和最新的ML和AI论文:</p><div class="na nb gp gr nc nd"><a href="https://alphasignal.ai/?referrer=Bex" rel="noopener  ugc nofollow" target="_blank"><div class="ne ab fo"><div class="nf ab ng cl cj nh"><h2 class="bd iu gy z fp ni fr fs nj fu fw is bi translated">阿尔法信号|机器学习的极品。艾总结的。</h2><div class="nk l"><h3 class="bd b gy z fp ni fr fs nj fu fw dk translated">留在循环中，不用花无数时间浏览下一个突破；我们的算法识别…</h3></div><div class="nl l"><p class="bd b dl z fp ni fr fs nj fu fw dk translated">alphasignal.ai</p></div></div><div class="nm l"><div class="ou l no np nq nm nr lo nd"/></div></div></a></div></div><div class="ab cl nt nu hx nv" role="separator"><span class="nw bw bk nx ny nz"/><span class="nw bw bk nx ny nz"/><span class="nw bw bk nx ny"/></div><div class="im in io ip iq"><h1 id="4b1d" class="lw lx it bd ly lz oa mb mc md ob mf mg jz oc ka mi kc od kd mk kf oe kg mm mn bi translated">控制树结构的超参数</h1><blockquote class="mt mu mv"><p id="4a94" class="ki kj mw kk b kl km ju kn ko kp jx kq mx ks kt ku my kw kx ky mz la lb lc ld im bi translated">如果你不熟悉决策树，可以看看StatQuest制作的这个传奇视频<a class="ae lv" href="https://www.youtube.com/watch?v=_L39rN6gz7Y" rel="noopener ugc nofollow" target="_blank">。</a></p></blockquote><p id="ec87" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在LGBM中，控制树结构最重要的参数是<code class="fe ov ow ox oy b">num_leaves</code>。顾名思义，它控制单个树中决策叶的数量。树的决策叶是“实际决策”发生的节点。</p><p id="f4d1" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">接下来是<code class="fe ov ow ox oy b">max_depth</code>。<code class="fe ov ow ox oy b">max_depth</code>越高，树的层次就越多，这使得树更加复杂，容易过度拟合。太低，你会吃不饱。尽管这听起来很难，但这是最容易调整的参数——只需选择3到12之间的值(这个范围在Kaggle上对任何数据集都适用)。</p><p id="c4ca" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">一旦确定了<code class="fe ov ow ox oy b">max_depth</code>，调整<code class="fe ov ow ox oy b">num_leaves</code>也很容易。LGBM文档中给出了一个简单的公式——对<code class="fe ov ow ox oy b">num_leaves</code>的最大限制应该是<code class="fe ov ow ox oy b">2^(max_depth)</code>。这意味着<code class="fe ov ow ox oy b">num_leaves</code>的最佳值在(2^3，2^12)或(8，4096)的范围内。</p><p id="35d1" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">然而，<code class="fe ov ow ox oy b">num_leaves</code>对LGBM学习的影响大于<code class="fe ov ow ox oy b">max_depth</code>。这意味着您需要指定一个更保守的搜索范围，比如(20，3000)——这是我经常做的。</p><p id="aece" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">树的另一个重要结构参数是<code class="fe ov ow ox oy b">min_data_in_leaf</code>。它的大小也与你是否过度节食有关。简单地说，<code class="fe ov ow ox oy b">min_data_in_leaf</code>指定了一个叶中符合决策标准的最小观察值数量。</p><p id="23a3" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">例如，如果决策叶检查一个特征是否大于13，比如说，将<code class="fe ov ow ox oy b">min_data_in_leaf</code>设置为100意味着只有当至少100个训练观察大于13时，我们才想要评估这个叶。这是我外行话中的要点。</p><p id="6d9b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><code class="fe ov ow ox oy b">min_data_in_leaf</code>的最佳值取决于训练样本的数量和<code class="fe ov ow ox oy b">num_leaves</code>。对于大型数据集，以百或千为单位设置一个值。</p><p id="3b67" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">查看LGBM文档的本节了解更多详情。</p></div><div class="ab cl nt nu hx nv" role="separator"><span class="nw bw bk nx ny nz"/><span class="nw bw bk nx ny nz"/><span class="nw bw bk nx ny"/></div><div class="im in io ip iq"><h1 id="73e9" class="lw lx it bd ly lz oa mb mc md ob mf mg jz oc ka mi kc od kd mk kf oe kg mm mn bi translated">超参数提高精确度</h1><p id="5246" class="pw-post-body-paragraph ki kj it kk b kl mo ju kn ko mp jx kq kr mq kt ku kv mr kx ky kz ms lb lc ld im bi translated">获得更高准确度的一个常见策略是使用许多决策树并降低学习率。换句话说，在LGBM中找到<code class="fe ov ow ox oy b">n_estimators</code>和<code class="fe ov ow ox oy b">learning_rate</code>的最佳组合。</p><p id="abf0" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><code class="fe ov ow ox oy b">n_estimators</code>控制决策树的数量，而<code class="fe ov ow ox oy b">learning_rate</code>是梯度下降的步长参数。</p><p id="c724" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">像LGBM这样的集成在迭代中构建树，每个新树都用于纠正以前树的“错误”。这种方法快速而强大，但容易过度拟合。</p><p id="1303" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这就是为什么梯度增强集成有一个控制学习速度的<code class="fe ov ow ox oy b">learning_rate</code>参数。典型值在0.01和0.3之间，但也可能超过这些值，特别是接近0。</p><p id="2659" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">因此，这两个参数(<code class="fe ov ow ox oy b">n_estimators</code>和<code class="fe ov ow ox oy b">learning_rate</code>)的最佳设置是使用许多提前停止的树，并为<code class="fe ov ow ox oy b">learning_rate</code>设置一个较低的值。我们稍后会看到一个例子。</p><p id="09c5" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">您也可以将<code class="fe ov ow ox oy b">max_bin</code>增加到默认值(255)以上，但同样存在过度拟合的风险。</p><p id="7075" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">查看LGBM文档的本节了解更多详情。</p></div><div class="ab cl nt nu hx nv" role="separator"><span class="nw bw bk nx ny nz"/><span class="nw bw bk nx ny nz"/><span class="nw bw bk nx ny"/></div><div class="im in io ip iq"><h1 id="2020" class="lw lx it bd ly lz oa mb mc md ob mf mg jz oc ka mi kc od kd mk kf oe kg mm mn bi translated">更多超参数来控制过度拟合</h1><p id="7429" class="pw-post-body-paragraph ki kj it kk b kl mo ju kn ko mp jx kq kr mq kt ku kv mr kx ky kz ms lb lc ld im bi translated">LGBM也有重要的正则化参数。</p><p id="bc87" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><code class="fe ov ow ox oy b">lambda_l1</code>和<code class="fe ov ow ox oy b">lambda_l2</code>指定L1或L2正则化，像XGBoost的<code class="fe ov ow ox oy b">reg_lambda</code>和<code class="fe ov ow ox oy b">reg_alpha</code>。这些参数的最佳值更难调整，因为它们的大小与过度拟合没有直接关系。然而，对于两者来说，一个好的搜索范围是(0，100)。</p><p id="c7cc" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">接下来我们有<code class="fe ov ow ox oy b">min_gain_to_split</code>，类似XGBoost的<code class="fe ov ow ox oy b">gamma</code>。保守的搜索范围是(0，15)。它可以用作大参数网格中的额外正则化。</p><p id="fc3a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">最后，我们有<code class="fe ov ow ox oy b">bagging_fraction</code>和<code class="fe ov ow ox oy b">feature_fraction</code>。<code class="fe ov ow ox oy b">bagging_fraction</code>取(0，1)内的一个值，并指定用于训练每棵树的训练样本的百分比(与XGBoost中的<code class="fe ov ow ox oy b">subsample</code>完全一样)。要使用该参数，还需要将<code class="fe ov ow ox oy b">bagging_freq</code>设置为一个整数值，这里解释<a class="ae lv" href="https://lightgbm.readthedocs.io/en/latest/Parameters.html#:~:text=frequency%20for%20bagging" rel="noopener ugc nofollow" target="_blank">为</a>。</p><p id="7766" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><code class="fe ov ow ox oy b">feature_fraction</code>指定训练每棵树时要采样的特征的百分比。所以，它也取(0，1)之间的一个值。</p><p id="4be3" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们已经讨论了影响过拟合的其他参数(<code class="fe ov ow ox oy b">max_depth</code>、<code class="fe ov ow ox oy b">num_leaves</code>等)。)在前面的章节中。</p></div><div class="ab cl nt nu hx nv" role="separator"><span class="nw bw bk nx ny nz"/><span class="nw bw bk nx ny nz"/><span class="nw bw bk nx ny"/></div><div class="im in io ip iq"><h1 id="0f33" class="lw lx it bd ly lz oa mb mc md ob mf mg jz oc ka mi kc od kd mk kf oe kg mm mn bi translated">在Optuna中创建搜索网格</h1><p id="bd38" class="pw-post-body-paragraph ki kj it kk b kl mo ju kn ko mp jx kq kr mq kt ku kv mr kx ky kz ms lb lc ld im bi translated">Optuna中的优化过程需要一个名为<em class="mw"> objective </em>的函数:</p><ul class=""><li id="8fd6" class="of og it kk b kl km ko kp kr oh kv oi kz oj ld oz ol om on bi translated">包括要作为字典搜索的参数网格</li><li id="be97" class="of og it kk b kl oo ko op kr oq kv or kz os ld oz ol om on bi translated">创建一个模型来尝试超参数组合集</li><li id="1e03" class="of og it kk b kl oo ko op kr oq kv or kz os ld oz ol om on bi translated">用单个候选集将模型拟合到数据</li><li id="9278" class="of og it kk b kl oo ko op kr oq kv or kz os ld oz ol om on bi translated">使用此模型生成预测</li><li id="8b8c" class="of og it kk b kl oo ko op kr oq kv or kz os ld oz ol om on bi translated">根据用户定义的指标对预测进行评分并返回</li></ul><p id="71c3" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">下面是它在代码中的样子:</p><figure class="lf lg lh li gt lj"><div class="bz fp l di"><div class="pa pb l"/></div></figure><p id="3411" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在上面的<code class="fe ov ow ox oy b">objective</code>函数中，我们还没有指定网格。</p><p id="4acc" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这是可选的，但我们正在交叉验证中进行培训。这确保了每个超参数候选集在完整数据上得到训练，并得到更稳健的评估。它也使我们能够使用早期停止。在最后一行，我们返回CV分数的平均值，这是我们想要优化的。</p><p id="540e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在让我们专注于创建网格。我们将包括今天介绍的超参数及其推荐的搜索范围:</p><figure class="lf lg lh li gt lj"><div class="bz fp l di"><div class="pa pb l"/></div></figure><blockquote class="mt mu mv"><p id="b41c" class="ki kj mw kk b kl km ju kn ko kp jx kq mx ks kt ku my kw kx ky mz la lb lc ld im bi translated">如果你不理解上面的网格或者<code class="fe ov ow ox oy b">trial</code>物体，可以看看我在Optuna上的<a class="ae lv" rel="noopener" target="_blank" href="/why-is-everyone-at-kaggle-obsessed-with-optuna-for-hyperparameter-tuning-7608fdca337c">文章</a>。</p></blockquote></div><div class="ab cl nt nu hx nv" role="separator"><span class="nw bw bk nx ny nz"/><span class="nw bw bk nx ny nz"/><span class="nw bw bk nx ny"/></div><div class="im in io ip iq"><h1 id="40be" class="lw lx it bd ly lz oa mb mc md ob mf mg jz oc ka mi kc od kd mk kf oe kg mm mn bi translated">创建Optuna研究并运行试验</h1><p id="cd1c" class="pw-post-body-paragraph ki kj it kk b kl mo ju kn ko mp jx kq kr mq kt ku kv mr kx ky kz ms lb lc ld im bi translated">是时候开始搜索了。下面是完整的目标函数供参考:</p><figure class="lf lg lh li gt lj"><div class="bz fp l di"><div class="pa pb l"/></div></figure><p id="4482" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">对于这个网格，我还添加了来自Optuna的<code class="fe ov ow ox oy b">integration</code>模块的<code class="fe ov ow ox oy b">LightGBMPruningCallback</code>。这个回调类很方便——它可以在对数据进行训练之前检测出没有希望的超参数集，从而显著减少搜索时间。</p><p id="3328" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">您应该将它传递给LGBM的<code class="fe ov ow ox oy b">callbacks</code>下的<code class="fe ov ow ox oy b">fit</code>方法，并设置<code class="fe ov ow ox oy b">trial</code>对象和您用作参数的评估指标。</p><p id="77f8" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在，让我们创建研究并运行一些试验:</p><figure class="lf lg lh li gt lj"><div class="bz fp l di"><div class="pa pb l"/></div></figure><p id="50f4" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">搜索完成后，调用<code class="fe ov ow ox oy b">best_value</code>和<code class="fe ov ow ox oy b">bast_params</code>属性，会得到类似这样的输出:</p><figure class="lf lg lh li gt lj"><div class="bz fp l di"><div class="pa pb l"/></div></figure></div><div class="ab cl nt nu hx nv" role="separator"><span class="nw bw bk nx ny nz"/><span class="nw bw bk nx ny nz"/><span class="nw bw bk nx ny"/></div><div class="im in io ip iq"><h1 id="0290" class="lw lx it bd ly lz oa mb mc md ob mf mg jz oc ka mi kc od kd mk kf oe kg mm mn bi translated">结论</h1><p id="4f44" class="pw-post-body-paragraph ki kj it kk b kl mo ju kn ko mp jx kq kr mq kt ku kv mr kx ky kz ms lb lc ld im bi translated">就是这样！你现在是LGBM的专业用户。如果你实现了这两篇文章中学到的东西，相信我，你已经比很多用LightGBM的Kagglers强了。</p><p id="d240" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">那是因为你对这个库的工作方式，它的参数代表什么有了更深的理解，并且熟练地调优了它们。这种类型的库基础知识总是比没有一点理解的猖獗的代码重用要好。</p><p id="b8f5" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">要从<em class="mw"> pro </em>过渡到<em class="mw"> master </em>，我建议花点时间在<a class="ae lv" href="https://lightgbm.readthedocs.io/en/latest/index.html" rel="noopener ugc nofollow" target="_blank">文档</a>上。感谢您的阅读！</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="gh gi pc"><img src="../Images/f91b8a21f12d3c3b7d428e0f8d5f48be.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*KeMS7gxVGsgx8KC36rSTcg.gif"/></div></div></figure><figure class="lf lg lh li gt lj gh gi paragraph-image"><a href="https://ibexorigin.medium.com/membership"><div class="gh gi pd"><img src="../Images/9eb4976c578910227077de97ec17e118.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4gJvpmETueLg_jn47vJ0MA.png"/></div></a></figure><h1 id="d177" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">您可能也会感兴趣…</h1><figure class="lf lg lh li gt lj gh gi paragraph-image"><a href="https://towardsdatascience.com/25-numpy-functions-you-never-knew-existed-p-guarantee-0-85-64616ba92fa8"><div class="gh gi pe"><img src="../Images/d1f61af83178ef33ca100733ae58308c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RUpe8vQVwFgcA3sU7w-xmw.png"/></div></a></figure><figure class="lf lg lh li gt lj gh gi paragraph-image"><a href="https://towardsdatascience.com/fail-proof-formula-to-learn-any-package-ruthlessly-in-less-than-a-day-c85e49a55459?source=your_stories_page-------------------------------------"><div class="gh gi pf"><img src="../Images/64ff4180c5cd4b01dd243e64cbce80b2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tid9Qm6QsmU06BV3eVQ12A.png"/></div></a></figure><figure class="lf lg lh li gt lj gh gi paragraph-image"><a href="https://towardsdatascience.com/10-underrated-sklearn-features-you-can-use-for-your-advantage-right-now-3a87b10a8d7f"><div class="gh gi pg"><img src="../Images/8b7298e1750218f40fae6e8fca8be3d2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*E0GdHWC6Xa24QLr2tv0qqw.png"/></div></a></figure><figure class="lf lg lh li gt lj gh gi paragraph-image"><a href="https://towardsdatascience.com/beginners-guide-to-umap-for-reducing-dimensionality-and-visualizing-100-dimensional-datasets-ff5590fb17be"><div class="gh gi ph"><img src="../Images/8bbce478590709d2df931885bd152d1a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xUZ2RWmFdyl9O7arCh_Mbg.png"/></div></a></figure><div class="na nb gp gr nc nd"><a rel="noopener follow" target="_blank" href="/tired-of-cliché-datasets-here-are-18-awesome-alternatives-from-all-domains-196913161ec9"><div class="ne ab fo"><div class="nf ab ng cl cj nh"><h2 class="bd iu gy z fp ni fr fs nj fu fw is bi translated">厌倦了陈词滥调的数据集？以下是来自所有领域的18个令人敬畏的选择</h2><div class="nk l"><h3 class="bd b gy z fp ni fr fs nj fu fw dk translated">编辑描述</h3></div><div class="nl l"><p class="bd b dl z fp ni fr fs nj fu fw dk translated">towardsdatascience.com</p></div></div><div class="nm l"><div class="pi l no np nq nm nr lo nd"/></div></div></a></div><div class="na nb gp gr nc nd"><a rel="noopener follow" target="_blank" href="/love-3blue1brown-animations-learn-how-to-create-your-own-in-python-in-10-minutes-8e0430cf3a6d"><div class="ne ab fo"><div class="nf ab ng cl cj nh"><h2 class="bd iu gy z fp ni fr fs nj fu fw is bi translated">喜欢3Blue1Brown动画？了解如何在10分钟内用Python创建自己的</h2><div class="nk l"><h3 class="bd b gy z fp ni fr fs nj fu fw dk translated">编辑描述</h3></div><div class="nl l"><p class="bd b dl z fp ni fr fs nj fu fw dk translated">towardsdatascience.com</p></div></div><div class="nm l"><div class="pj l no np nq nm nr lo nd"/></div></div></a></div></div></div>    
</body>
</html>