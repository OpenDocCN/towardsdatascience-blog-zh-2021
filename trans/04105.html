<html>
<head>
<title>Beginner’s Guide to XGBoost for Classification Problems</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">XGBoost分类问题入门指南</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/beginners-guide-to-xgboost-for-classification-problems-50f75aac5390?source=collection_archive---------1-----------------------#2021-04-07">https://towardsdatascience.com/beginners-guide-to-xgboost-for-classification-problems-50f75aac5390?source=collection_archive---------1-----------------------#2021-04-07</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="f600" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">利用最热门的ML库实现一流的性能</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/acb8d62b9f4bf486d0a932d6ebc6804c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QROfVQn2bKxrgKf9-VAVrQ.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd ky">照片由</strong> <a class="ae kz" href="https://www.pexels.com/@dom-gould-105501?utm_content=attributionCopyText&amp;utm_medium=referral&amp;utm_source=pexels" rel="noopener ugc nofollow" target="_blank"> <strong class="bd ky">唐古尔德</strong> </a> <strong class="bd ky">上</strong> <a class="ae kz" href="https://www.pexels.com/photo/rear-view-of-silhouette-man-against-sky-during-sunset-325790/?utm_content=attributionCopyText&amp;utm_medium=referral&amp;utm_source=pexels" rel="noopener ugc nofollow" target="_blank"> <strong class="bd ky">像素</strong> </a></p></figure><h2 id="8ffa" class="la lb it bd lc ld le dn lf lg lh dp li lj lk ll lm ln lo lp lq lr ls lt lu lv bi translated">XGBoost是什么，为什么这么受欢迎？</h2><p id="f2d3" class="pw-post-body-paragraph lw lx it ly b lz ma ju mb mc md jx me lj mf mg mh ln mi mj mk lr ml mm mn mo im bi translated">下面给大家介绍一下ML社区最火的机器学习库——XGBoost。近年来，它一直是赢得大规模ML竞赛的算法背后的主要驱动力。它的速度和性能是无与伦比的，它始终优于任何其他旨在监督学习任务的算法。</p><p id="b918" class="pw-post-body-paragraph lw lx it ly b lz mp ju mb mc mq jx me lj mr mg mh ln ms mj mk lr mt mm mn mo im bi translated">该库是可并行化的，这意味着核心算法可以在GPU集群上运行，甚至可以在计算机网络上运行。这使得通过对上亿训练样本进行高性能训练来解决ML任务成为可能。</p><p id="d1df" class="pw-post-body-paragraph lw lx it ly b lz mp ju mb mc mq jx me lj mr mg mh ln ms mj mk lr mt mm mn mo im bi translated">最初，它是作为命令行应用程序用C++编写的。在赢得了物理学领域的巨大竞争后，它开始被ML社区广泛采用。结果，现在这个库有了其他几种语言的API，包括Python、R和Julia。</p><p id="c596" class="pw-post-body-paragraph lw lx it ly b lz mp ju mb mc mq jx me lj mr mg mh ln ms mj mk lr mt mm mn mo im bi translated">在这篇文章中，您将学习XGBoost解决分类任务的基础知识、XGBoost的超参数的大量列表的概述以及如何调优它们。</p><div class="mu mv gp gr mw mx"><a href="https://ibexorigin.medium.com/membership" rel="noopener follow" target="_blank"><div class="my ab fo"><div class="mz ab na cl cj nb"><h2 class="bd iu gy z fp nc fr fs nd fu fw is bi translated">通过我的推荐链接加入Medium-BEXGBoost</h2><div class="ne l"><h3 class="bd b gy z fp nc fr fs nd fu fw dk translated">获得独家访问我的所有⚡premium⚡内容和所有媒体没有限制。支持我的工作，给我买一个…</h3></div><div class="nf l"><p class="bd b dl z fp nc fr fs nd fu fw dk translated">ibexorigin.medium.com</p></div></div><div class="ng l"><div class="nh l ni nj nk ng nl ks mx"/></div></div></a></div><p id="40ff" class="pw-post-body-paragraph lw lx it ly b lz mp ju mb mc mq jx me lj mr mg mh ln ms mj mk lr mt mm mn mo im bi translated">获得由强大的AI-Alpha信号选择和总结的最佳和最新的ML和AI论文:</p><div class="mu mv gp gr mw mx"><a href="https://alphasignal.ai/?referrer=Bex" rel="noopener  ugc nofollow" target="_blank"><div class="my ab fo"><div class="mz ab na cl cj nb"><h2 class="bd iu gy z fp nc fr fs nd fu fw is bi translated">阿尔法信号|机器学习的极品。艾总结的。</h2><div class="ne l"><h3 class="bd b gy z fp nc fr fs nd fu fw dk translated">留在循环中，不用花无数时间浏览下一个突破；我们的算法识别…</h3></div><div class="nf l"><p class="bd b dl z fp nc fr fs nd fu fw dk translated">alphasignal.ai</p></div></div><div class="ng l"><div class="nm l ni nj nk ng nl ks mx"/></div></div></a></div></div><div class="ab cl nn no hx np" role="separator"><span class="nq bw bk nr ns nt"/><span class="nq bw bk nr ns nt"/><span class="nq bw bk nr ns"/></div><div class="im in io ip iq"><h2 id="d835" class="la lb it bd lc ld le dn lf lg lh dp li lj lk ll lm ln lo lp lq lr ls lt lu lv bi translated">术语复习</h2><p id="4074" class="pw-post-body-paragraph lw lx it ly b lz ma ju mb mc md jx me lj mf mg mh ln mi mj mk lr ml mm mn mo im bi translated">在我们继续XGBoost的代码示例之前，让我们回顾一下我们将在本文中使用的一些术语。</p><p id="6b2e" class="pw-post-body-paragraph lw lx it ly b lz mp ju mb mc mq jx me lj mr mg mh ln ms mj mk lr mt mm mn mo im bi translated"><strong class="ly iu">分类任务</strong>:一个有监督的机器学习任务，通过研究一个实例的特征来预测该实例是否属于某个类别。例如，通过查看一个人的身体测量值、病史和葡萄糖水平，您可以预测一个人是属于“患有糖尿病”还是“没有糖尿病”。</p><p id="4c00" class="pw-post-body-paragraph lw lx it ly b lz mp ju mb mc mq jx me lj mr mg mh ln ms mj mk lr mt mm mn mo im bi translated"><strong class="ly iu">二进制分类</strong>:一种分类类型，目标实例只能属于两个类中的一个。例如，预测电子邮件是否是垃圾邮件，客户是否购买了某种产品等。</p><p id="fc13" class="pw-post-body-paragraph lw lx it ly b lz mp ju mb mc mq jx me lj mr mg mh ln ms mj mk lr mt mm mn mo im bi translated"><strong class="ly iu">多类分类</strong>:另一类分类问题，目标可以属于多个类别中的一个。比如预测一只鸟的种类，猜测某人的血型等等。</p><p id="21d8" class="pw-post-body-paragraph lw lx it ly b lz mp ju mb mc mq jx me lj mr mg mh ln ms mj mk lr mt mm mn mo im bi translated">如果你发现自己被其他术语搞糊涂了，我为初学者写了一个小的ML字典:</p><div class="mu mv gp gr mw mx"><a rel="noopener follow" target="_blank" href="/codeless-machine-learning-dictionary-for-dummies-fa912cc7bdfe"><div class="my ab fo"><div class="mz ab na cl cj nb"><h2 class="bd iu gy z fp nc fr fs nd fu fw is bi translated">傻瓜用无代码机器学习词典</h2><div class="ne l"><h3 class="bd b gy z fp nc fr fs nd fu fw dk translated">编辑描述</h3></div><div class="nf l"><p class="bd b dl z fp nc fr fs nd fu fw dk translated">towardsdatascience.com</p></div></div><div class="ng l"><div class="nu l ni nj nk ng nl ks mx"/></div></div></a></div></div><div class="ab cl nn no hx np" role="separator"><span class="nq bw bk nr ns nt"/><span class="nq bw bk nr ns nt"/><span class="nq bw bk nr ns"/></div><div class="im in io ip iq"><h2 id="9d0a" class="la lb it bd lc ld le dn lf lg lh dp li lj lk ll lm ln lo lp lq lr ls lt lu lv bi translated">如何为XGBoost预处理数据集</h2><p id="fcf4" class="pw-post-body-paragraph lw lx it ly b lz ma ju mb mc md jx me lj mf mg mh ln mi mj mk lr ml mm mn mo im bi translated">除了基本的<a class="ae kz" href="https://towardsdev.com/data-type-constraints-data-range-constraints-duplicate-data-with-pandas-44897a350b1e?source=your_stories_page-------------------------------------" rel="noopener ugc nofollow" target="_blank">数据清理</a>操作之外，XGBoost还需要满足一些要求才能达到最佳性能。主要是:</p><ul class=""><li id="31d7" class="nv nw it ly b lz mp mc mq lj nx ln ny lr nz mo oa ob oc od bi translated">数字要素应进行缩放</li><li id="fad3" class="nv nw it ly b lz oe mc of lj og ln oh lr oi mo oa ob oc od bi translated">分类特征应该被编码</li></ul><p id="99ef" class="pw-post-body-paragraph lw lx it ly b lz mp ju mb mc mq jx me lj mr mg mh ln ms mj mk lr mt mm mn mo im bi translated">为了展示这些步骤是如何完成的，我们将使用来自Kaggle的<a class="ae kz" href="https://www.kaggle.com/jsphyg/weather-dataset-rattle-package" rel="noopener ugc nofollow" target="_blank"> Rain in Australia </a>数据集，我们将根据一些天气测量来预测今天是否会下雨。在本节中，我们将通过利用Scikit-Learn管道来关注预处理。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oj ok l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="ab gu cl ol"><img src="../Images/412481a8a5a3e70efa8e67cdfeb95042.png" data-original-src="https://miro.medium.com/v2/format:webp/1*-0TO2LFfYl7EgmuDEsAMkA.png"/></div></figure><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oj ok l"/></div></figure><p id="c541" class="pw-post-body-paragraph lw lx it ly b lz mp ju mb mc mq jx me lj mr mg mh ln ms mj mk lr mt mm mn mo im bi translated">该数据集包含澳大利亚多个气象站10年来的天气测量数据。你既可以预测明天下雨，也可以预测今天下雨，所以数据集中有两个目标，名为<code class="fe om on oo op b">RainToday</code>、<code class="fe om on oo op b">RainTomorrow</code>。</p><p id="97c4" class="pw-post-body-paragraph lw lx it ly b lz mp ju mb mc mq jx me lj mr mg mh ln ms mj mk lr mt mm mn mo im bi translated">由于我们将只预测<code class="fe om on oo op b">RainToday</code>，我们将删除另一个以及其他一些不必要的特性:</p><pre class="kj kk kl km gt oq op or os aw ot bi"><span id="b6ba" class="la lb it op b gy ou ov l ow ox">cols_to_drop = ["Date", "Location", "RainTomorrow", "Rainfall"]<br/><br/>rain.drop(cols_to_drop, axis=1, inplace=True)</span></pre><blockquote class="oy oz pa"><p id="1458" class="lw lx pb ly b lz mp ju mb mc mq jx me pc mr mg mh pd ms mj mk pe mt mm mn mo im bi translated">删除<code class="fe om on oo op b">Rainfall</code>列是必须的，因为它记录了以毫米为单位的降雨量。</p></blockquote><p id="f742" class="pw-post-body-paragraph lw lx it ly b lz mp ju mb mc mq jx me lj mr mg mh ln ms mj mk lr mt mm mn mo im bi translated">接下来，让我们处理缺失值，首先查看它们在每列中的比例:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oj ok l"/></div></figure><p id="5036" class="pw-post-body-paragraph lw lx it ly b lz mp ju mb mc mq jx me lj mr mg mh ln ms mj mk lr mt mm mn mo im bi translated">如果比例高于40%，我们将删除该列:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oj ok l"/></div></figure><p id="999a" class="pw-post-body-paragraph lw lx it ly b lz mp ju mb mc mq jx me lj mr mg mh ln ms mj mk lr mt mm mn mo im bi translated">三列包含超过40%的缺失值。我们将放弃它们:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oj ok l"/></div></figure><p id="66c7" class="pw-post-body-paragraph lw lx it ly b lz mp ju mb mc mq jx me lj mr mg mh ln ms mj mk lr mt mm mn mo im bi translated">现在，在我们继续讨论管道之前，让我们先将数据分成特性和目标数组:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oj ok l"/></div></figure><p id="721f" class="pw-post-body-paragraph lw lx it ly b lz mp ju mb mc mq jx me lj mr mg mh ln ms mj mk lr mt mm mn mo im bi translated">接下来，有分类和数字特征。我们将构建两个独立的管道，稍后将它们合并。</p><blockquote class="oy oz pa"><p id="e9da" class="lw lx pb ly b lz mp ju mb mc mq jx me pc mr mg mh pd ms mj mk pe mt mm mn mo im bi translated">接下来的代码示例将大量使用Sklearn-Pipelines。如果你不熟悉它们，可以看看我的另一篇文章中关于它们的<a class="ae kz" rel="noopener" target="_blank" href="/how-to-use-sklearn-pipelines-for-ridiculously-neat-code-a61ab66ca90d">完整指南</a>。</p></blockquote><p id="9f1b" class="pw-post-body-paragraph lw lx it ly b lz mp ju mb mc mq jx me lj mr mg mh ln ms mj mk lr mt mm mn mo im bi translated">对于分类特征，我们将使用列的模式估算缺失值，并使用一键编码对其进行编码:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oj ok l"/></div></figure><p id="2084" class="pw-post-body-paragraph lw lx it ly b lz mp ju mb mc mq jx me lj mr mg mh ln ms mj mk lr mt mm mn mo im bi translated">对于数字特征，我将选择平均值作为估算值和<code class="fe om on oo op b">StandardScaler</code>,这样特征的平均值为0，方差为1:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oj ok l"/></div></figure><p id="fb6b" class="pw-post-body-paragraph lw lx it ly b lz mp ju mb mc mq jx me lj mr mg mh ln ms mj mk lr mt mm mn mo im bi translated">最后，我们将使用一个列转换器来组合这两个管道。要指定管道是为哪些列设计的，我们应该首先隔离分类和数字特征名称:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oj ok l"/></div></figure><p id="7c5f" class="pw-post-body-paragraph lw lx it ly b lz mp ju mb mc mq jx me lj mr mg mh ln ms mj mk lr mt mm mn mo im bi translated">接下来，我们将把它们和它们对应的管道一起输入到一个<code class="fe om on oo op b">ColumnTransFormer</code>实例中:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oj ok l"/></div></figure><p id="0329" class="pw-post-body-paragraph lw lx it ly b lz mp ju mb mc mq jx me lj mr mg mh ln ms mj mk lr mt mm mn mo im bi translated">完整的管道终于准备好了。唯一缺少的是XGBoost分类器，我们将在下一节添加它。</p></div><div class="ab cl nn no hx np" role="separator"><span class="nq bw bk nr ns nt"/><span class="nq bw bk nr ns nt"/><span class="nq bw bk nr ns"/></div><div class="im in io ip iq"><h2 id="6793" class="la lb it bd lc ld le dn lf lg lh dp li lj lk ll lm ln lo lp lq lr ls lt lu lv bi translated">分类问题的XGBoost示例</h2><p id="aafd" class="pw-post-body-paragraph lw lx it ly b lz ma ju mb mc md jx me lj mf mg mh ln mi mj mk lr ml mm mn mo im bi translated">要开始使用<code class="fe om on oo op b">xgboost</code>，只需安装<code class="fe om on oo op b">pip</code>或<code class="fe om on oo op b">conda</code>:</p><pre class="kj kk kl km gt oq op or os aw ot bi"><span id="8fdb" class="la lb it op b gy ou ov l ow ox"># pip<br/>pip install xgboost<br/><br/># conda<br/>conda install -c conda-forge xgboost</span></pre><p id="3b23" class="pw-post-body-paragraph lw lx it ly b lz mp ju mb mc mq jx me lj mr mg mh ln ms mj mk lr mt mm mn mo im bi translated">安装完成后，可以用它的标准别名— <code class="fe om on oo op b">xgb</code>导入。对于分类问题，库提供了<code class="fe om on oo op b">XGBClassifier</code>类:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oj ok l"/></div></figure><p id="c2ab" class="pw-post-body-paragraph lw lx it ly b lz mp ju mb mc mq jx me lj mr mg mh ln ms mj mk lr mt mm mn mo im bi translated">幸运的是，分类器遵循熟悉的<code class="fe om on oo op b">sklearn</code>拟合预测模式，这意味着我们可以自由地将其用作任何<code class="fe om on oo op b">sklearn</code>模型。</p><p id="0d29" class="pw-post-body-paragraph lw lx it ly b lz mp ju mb mc mq jx me lj mr mg mh ln ms mj mk lr mt mm mn mo im bi translated">在训练分类器之前，我们先对数据进行预处理，并将其分为训练集和测试集:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oj ok l"/></div></figure><p id="e43d" class="pw-post-body-paragraph lw lx it ly b lz mp ju mb mc mq jx me lj mr mg mh ln ms mj mk lr mt mm mn mo im bi translated">由于目标包含<code class="fe om on oo op b">NaN</code>，所以我是手工估算的。此外，将<code class="fe om on oo op b">y_processed</code>传递到<code class="fe om on oo op b">stratify</code>也很重要，这样拆分在两个集合中包含相同比例的类别。</p><p id="601b" class="pw-post-body-paragraph lw lx it ly b lz mp ju mb mc mq jx me lj mr mg mh ln ms mj mk lr mt mm mn mo im bi translated">现在，我们用默认参数拟合分类器，并评估其性能:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oj ok l"/></div></figure><p id="7d62" class="pw-post-body-paragraph lw lx it ly b lz mp ju mb mc mq jx me lj mr mg mh ln ms mj mk lr mt mm mn mo im bi translated">即使使用默认参数，我们也得到了85%的准确率，这已经相当不错了。在接下来的部分中，我们将通过使用Scikit-learn提供的<code class="fe om on oo op b">GridSearchCV</code>来进一步改进这个模型。</p></div><div class="ab cl nn no hx np" role="separator"><span class="nq bw bk nr ns nt"/><span class="nq bw bk nr ns nt"/><span class="nq bw bk nr ns"/></div><div class="im in io ip iq"><h2 id="727e" class="la lb it bd lc ld le dn lf lg lh dp li lj lk ll lm ln lo lp lq lr ls lt lu lv bi translated">XGBoost的幕后动力是什么</h2><p id="9bf9" class="pw-post-body-paragraph lw lx it ly b lz ma ju mb mc md jx me lj mf mg mh ln mi mj mk lr ml mm mn mo im bi translated">与许多其他算法不同，XGBoost是一种集成学习算法，这意味着它结合了许多模型的结果，称为<strong class="ly iu">基本学习器</strong>来进行预测。</p><p id="e091" class="pw-post-body-paragraph lw lx it ly b lz mp ju mb mc mq jx me lj mr mg mh ln ms mj mk lr mt mm mn mo im bi translated">就像在随机森林中一样，XGBoost使用决策树作为基础学习器:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="ab gu cl ol"><img src="../Images/3d95cf65735e1c9fcb7cabf9e9a88ce0.png" data-original-src="https://miro.medium.com/v2/format:webp/1*0e-3tgPVUJOhTfe2vRNjqg.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图片由作者提供。预测降雨的决策树</p></figure><p id="4035" class="pw-post-body-paragraph lw lx it ly b lz mp ju mb mc mq jx me lj mr mg mh ln ms mj mk lr mt mm mn mo im bi translated">上面可以看到一个决策树的例子。在每个决策节点(圆圈)中，有一个问题只有两个可能的答案。在每棵树的底部，都有一个决策(矩形)。在上面的树上，第一个问题是是否是晴天。如果是的话，你马上决定不会下雨。否则，你会继续问更多的二元(是/否)问题，最终在最后一片“叶子”(矩形)上做出一些决定。</p><p id="a5de" class="pw-post-body-paragraph lw lx it ly b lz mp ju mb mc mq jx me lj mr mg mh ln ms mj mk lr mt mm mn mo im bi translated">个体决策树是低偏差、高方差的模型。他们非常善于在任何类型的训练数据中找到关系，但却很难在看不见的数据上很好地概括。</p><p id="fe85" class="pw-post-body-paragraph lw lx it ly b lz mp ju mb mc mq jx me lj mr mg mh ln ms mj mk lr mt mm mn mo im bi translated">但是，XGBoost使用的树与传统的决策树有点不同。它们被称为CART树(分类和回归树),而不是在每个“叶”节点中包含一个决策，它们包含一个实例是否属于一个组的实值分数。在树达到最大深度之后，可以通过使用某个阈值将分数转换成类别来做出决定。</p><p id="1159" class="pw-post-body-paragraph lw lx it ly b lz mp ju mb mc mq jx me lj mr mg mh ln ms mj mk lr mt mm mn mo im bi translated">谈到XGBoost的内部，我绝不是专家。这就是为什么我推荐你去看看<a class="ae kz" href="https://www.youtube.com/playlist?list=PLblh5JKOoLULU0irPgs1SnKO6wqVjKUsQ" rel="noopener ugc nofollow" target="_blank">这个完全在XGBoost上的超棒的YouTube播放列表</a>和<a class="ae kz" href="https://www.youtube.com/playlist?list=PLblh5JKOoLUJjeXUvUE0maghNuY2_5fY6" rel="noopener ugc nofollow" target="_blank">另一个</a>完全针对梯度增强的播放列表，我根本没有提到它。</p></div><div class="ab cl nn no hx np" role="separator"><span class="nq bw bk nr ns nt"/><span class="nq bw bk nr ns nt"/><span class="nq bw bk nr ns"/></div><div class="im in io ip iq"><h2 id="4861" class="la lb it bd lc ld le dn lf lg lh dp li lj lk ll lm ln lo lp lq lr ls lt lu lv bi translated">XGBoost分类器超参数概述</h2><p id="9431" class="pw-post-body-paragraph lw lx it ly b lz ma ju mb mc md jx me lj mf mg mh ln mi mj mk lr ml mm mn mo im bi translated">到目前为止，我们只使用了XGBoost分类器的默认超参数:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oj ok l"/></div></figure><blockquote class="oy oz pa"><p id="8bf0" class="lw lx pb ly b lz mp ju mb mc mq jx me pc mr mg mh pd ms mj mk pe mt mm mn mo im bi translated">术语复习:模型的<em class="it">超参数</em>是模型的设置，应该由用户提供。模型本身不能从给定的训练数据中学习这些。</p></blockquote><p id="0671" class="pw-post-body-paragraph lw lx it ly b lz mp ju mb mc mq jx me lj mr mg mh ln ms mj mk lr mt mm mn mo im bi translated">如你所见，它有很多。尽管我们使用默认值取得了相当好的结果，但是调整上述参数可能会导致性能的显著提高。但是在我们开始调优之前，让我们来看一下最常调优的超参数的概述:</p><ol class=""><li id="53f5" class="nv nw it ly b lz mp mc mq lj nx ln ny lr nz mo pf ob oc od bi translated"><code class="fe om on oo op b">learning_rate</code>:也称为<em class="pb"> eta </em>，它指定了通过使用额外的基本学习器，模型拟合残差的速度。</li></ol><ul class=""><li id="79d0" class="nv nw it ly b lz mp mc mq lj nx ln ny lr nz mo oa ob oc od bi translated">典型值:0.01–0.2</li></ul><p id="0256" class="pw-post-body-paragraph lw lx it ly b lz mp ju mb mc mq jx me lj mr mg mh ln ms mj mk lr mt mm mn mo im bi translated">2.<code class="fe om on oo op b">gamma, reg_alpha</code>、<code class="fe om on oo op b">reg_lambda</code>:这3个参数分别指定了XGBoost完成的3种正则化类型的值——创建新分裂的最小损失减少、L1 reg的叶权重、L2 reg的叶权重</p><ul class=""><li id="e516" class="nv nw it ly b lz mp mc mq lj nx ln ny lr nz mo oa ob oc od bi translated"><code class="fe om on oo op b">gamma</code>的典型值:0 - 0.5，但高度依赖于数据</li><li id="d5a3" class="nv nw it ly b lz oe mc of lj og ln oh lr oi mo oa ob oc od bi translated"><code class="fe om on oo op b">reg_alpha</code>和<code class="fe om on oo op b">reg_lambda</code>的典型值:0 - 1是一个很好的起点，但同样取决于数据</li></ul><p id="bedb" class="pw-post-body-paragraph lw lx it ly b lz mp ju mb mc mq jx me lj mr mg mh ln ms mj mk lr mt mm mn mo im bi translated">3.<code class="fe om on oo op b">max_depth</code> -树的决策节点可以有多深。必须是正整数</p><ul class=""><li id="ae11" class="nv nw it ly b lz mp mc mq lj nx ln ny lr nz mo oa ob oc od bi translated">典型值:1–10</li></ul><p id="7a34" class="pw-post-body-paragraph lw lx it ly b lz mp ju mb mc mq jx me lj mr mg mh ln ms mj mk lr mt mm mn mo im bi translated">4.<code class="fe om on oo op b">subsample</code> -可用于训练每棵树的训练集部分。如果该值过低，可能会导致拟合不足；如果该值过高，可能会导致拟合过度</p><ul class=""><li id="2f82" class="nv nw it ly b lz mp mc mq lj nx ln ny lr nz mo oa ob oc od bi translated">典型值:0.5–0.9</li></ul><p id="6fc1" class="pw-post-body-paragraph lw lx it ly b lz mp ju mb mc mq jx me lj mr mg mh ln ms mj mk lr mt mm mn mo im bi translated">5.<code class="fe om on oo op b">colsample_bytree</code> -可用于训练每棵树的特征的分数。较大的值意味着几乎所有的特征都可以用于构建决策树</p><ul class=""><li id="d0b4" class="nv nw it ly b lz mp mc mq lj nx ln ny lr nz mo oa ob oc od bi translated">典型值:0.5–0.9</li></ul><p id="680f" class="pw-post-body-paragraph lw lx it ly b lz mp ju mb mc mq jx me lj mr mg mh ln ms mj mk lr mt mm mn mo im bi translated">以上是人们经常调的主要超参数。如果你不完全理解它们(像我一样),这是完全可以的，但是你可以参考这篇<a class="ae kz" href="http://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/" rel="noopener ugc nofollow" target="_blank">文章</a>,它给出了上述每个参数如何工作以及如何调整它们的全面概述。</p></div><div class="ab cl nn no hx np" role="separator"><span class="nq bw bk nr ns nt"/><span class="nq bw bk nr ns nt"/><span class="nq bw bk nr ns"/></div><div class="im in io ip iq"><h2 id="516b" class="la lb it bd lc ld le dn lf lg lh dp li lj lk ll lm ln lo lp lq lr ls lt lu lv bi translated">使用GridSearchCV对XGBoost进行超参数调优</h2><p id="5adb" class="pw-post-body-paragraph lw lx it ly b lz ma ju mb mc md jx me lj mf mg mh ln mi mj mk lr ml mm mn mo im bi translated">最后，是时候给我们的XGBoost分类器充电了。我们将使用来自Scikit-learn的<code class="fe om on oo op b">GridSearchCV</code>类，它接受所需超参数的可能值，并为每个超参数组合的给定数据拟合单独的模型。我不会详细介绍GridSearch是如何工作的，但您可以查看我关于该主题的另一篇综合文章:</p><div class="mu mv gp gr mw mx"><a rel="noopener follow" target="_blank" href="/automatic-hyperparameter-tuning-with-sklearn-gridsearchcv-and-randomizedsearchcv-e94f53a518ee"><div class="my ab fo"><div class="mz ab na cl cj nb"><h2 class="bd iu gy z fp nc fr fs nd fu fw is bi translated">使用Sklearn GridSearchCV和RandomizedSearchCV自动调整超参数</h2><div class="ne l"><h3 class="bd b gy z fp nc fr fs nd fu fw dk translated">编辑描述</h3></div><div class="nf l"><p class="bd b dl z fp nc fr fs nd fu fw dk translated">towardsdatascience.com</p></div></div><div class="ng l"><div class="pg l ni nj nk ng nl ks mx"/></div></div></a></div><p id="76c7" class="pw-post-body-paragraph lw lx it ly b lz mp ju mb mc mq jx me lj mr mg mh ln ms mj mk lr mt mm mn mo im bi translated">我们将在两轮中只调整几个参数，因为调整既费时又费力。让我们为第一轮创建参数网格:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oj ok l"/></div></figure><p id="7c3f" class="pw-post-body-paragraph lw lx it ly b lz mp ju mb mc mq jx me lj mr mg mh ln ms mj mk lr mt mm mn mo im bi translated">在网格中，我将<code class="fe om on oo op b">subsample</code>和<code class="fe om on oo op b">colsample_bytree</code>固定为推荐值，以加快速度并防止过度拟合。</p><p id="7986" class="pw-post-body-paragraph lw lx it ly b lz mp ju mb mc mq jx me lj mr mg mh ln ms mj mk lr mt mm mn mo im bi translated">我们将从<code class="fe om on oo op b">sklearn.model_selection</code>导入<code class="fe om on oo op b">GridSearchCV</code>，实例化并使其适合我们预处理的数据:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oj ok l"/></div></figure><p id="680d" class="pw-post-body-paragraph lw lx it ly b lz mp ju mb mc mq jx me lj mr mg mh ln ms mj mk lr mt mm mn mo im bi translated">经过漫长的折磨，我们终于得到了最好的参数和最好的分数:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oj ok l"/></div></figure><p id="bbaf" class="pw-post-body-paragraph lw lx it ly b lz mp ju mb mc mq jx me lj mr mg mh ln ms mj mk lr mt mm mn mo im bi translated">这一次，我选择了<code class="fe om on oo op b">roc_auc</code>指标，它计算ROC(接收机工作特性)曲线下的面积。对于不平衡分类问题，它是最受欢迎和最健壮的评估度量之一。你可以在这里了解更多<a class="ae kz" href="https://www.youtube.com/watch?v=4jRBRDbJemM&amp;t=47s" rel="noopener ugc nofollow" target="_blank">。让我们看看最好的参数:</a></p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oj ok l"/></div></figure><p id="adf8" class="pw-post-body-paragraph lw lx it ly b lz mp ju mb mc mq jx me lj mr mg mh ln ms mj mk lr mt mm mn mo im bi translated">如你所见，只有<code class="fe om on oo op b">scale_pos_weight</code>在它提供的范围中间。其他参数处于其范围的末端，这意味着我们必须继续探索:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oj ok l"/></div></figure><p id="f880" class="pw-post-body-paragraph lw lx it ly b lz mp ju mb mc mq jx me lj mr mg mh ln ms mj mk lr mt mm mn mo im bi translated">我们将使用更新的参数网格为数据拟合一个新的GridSearch对象，并查看我们是否在最佳得分上有所改进:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oj ok l"/></div></figure><p id="c65f" class="pw-post-body-paragraph lw lx it ly b lz mp ju mb mc mq jx me lj mr mg mh ln ms mj mk lr mt mm mn mo im bi translated">看起来第二轮调优导致了性能的轻微下降。我们别无选择，只能坚持第一组参数，即:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oj ok l"/></div></figure><p id="d954" class="pw-post-body-paragraph lw lx it ly b lz mp ju mb mc mq jx me lj mr mg mh ln ms mj mk lr mt mm mn mo im bi translated">让我们用上面的参数创建一个最终的分类器:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oj ok l"/></div></figure><p id="812c" class="pw-post-body-paragraph lw lx it ly b lz mp ju mb mc mq jx me lj mr mg mh ln ms mj mk lr mt mm mn mo im bi translated">最后，对测试集进行预测:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oj ok l"/></div></figure></div><div class="ab cl nn no hx np" role="separator"><span class="nq bw bk nr ns nt"/><span class="nq bw bk nr ns nt"/><span class="nq bw bk nr ns"/></div><div class="im in io ip iq"><h2 id="3963" class="la lb it bd lc ld le dn lf lg lh dp li lj lk ll lm ln lo lp lq lr ls lt lu lv bi translated">结论</h2><p id="7e7b" class="pw-post-body-paragraph lw lx it ly b lz ma ju mb mc md jx me lj mf mg mh ln mi mj mk lr ml mm mn mo im bi translated">关于分类问题，我们已经在XGBoost的介绍性指南的末尾做了介绍。尽管我们已经介绍了很多内容，但是就XGBoost本身和分类主题而言，仍然有许多主题需要探索。</p><p id="b094" class="pw-post-body-paragraph lw lx it ly b lz mp ju mb mc mq jx me lj mr mg mh ln ms mj mk lr mt mm mn mo im bi translated">我强烈建议您查看我提供的链接，作为学习XGBoost的额外资源，并建议您阅读更多关于如何解决分类问题的内容。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><a href="https://towardsdatascience.com/5-step-workflow-into-the-heart-of-matplotlib-and-create-amazing-plots-7f9d2164ff2b"><div class="gh gi ph"><img src="../Images/2808d1e53006e4630468590eb4df19d8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FfBrfUTSvs28Kc7wTq2RyA.png"/></div></a></figure></div></div>    
</body>
</html>