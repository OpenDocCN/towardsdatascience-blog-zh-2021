<html>
<head>
<title>Softmax Regression in Python: Multi-class Classification</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Python中的Softmax回归:多类分类</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/softmax-regression-in-python-multi-class-classification-3cb560d90cb2?source=collection_archive---------1-----------------------#2021-04-25">https://towardsdatascience.com/softmax-regression-in-python-multi-class-classification-3cb560d90cb2?source=collection_archive---------1-----------------------#2021-04-25</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="8e3a" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">从零开始的机器学习:第7部分</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/cef147af1738758b643f5b881e54d0db.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NLRmKVQiAv5PeZ2kGHxFRQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><p id="8005" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在本文中，我们将研究用于多类分类问题的Softmax回归，并在MNIST手写数字识别数据集上实现它。</p><p id="aac9" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">首先，我们将基于<strong class="la iu">逻辑回归</strong>来理解<strong class="la iu"> Softmax函数</strong>，然后我们将查看<strong class="la iu">交叉熵损失</strong>、<strong class="la iu">一键编码</strong>，并对其进行编码。最后，我们将对<strong class="la iu">训练</strong>函数(<code class="fe lu lv lw lx b"><strong class="la iu">fit</strong></code>)进行编码，看看我们的<strong class="la iu">精度</strong>。我们还将<strong class="la iu">绘制</strong>我们的预测。</p><p id="4d89" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们将使用Python NumPy和Matplotlib来完成这一切。我们将看到如何在训练集和测试集中获得91%  的<strong class="la iu"> <em class="ly">。</em></strong></p><div class="lz ma gp gr mb mc"><a rel="noopener follow" target="_blank" href="/logistic-regression-from-scratch-in-python-ec66603592e2"><div class="md ab fo"><div class="me ab mf cl cj mg"><h2 class="bd iu gy z fp mh fr fs mi fu fw is bi translated">Python中从头开始的逻辑回归</h2><div class="mj l"><h3 class="bd b gy z fp mh fr fs mi fu fw dk translated">从零开始的机器学习:第5部分</h3></div><div class="mk l"><p class="bd b dl z fp mh fr fs mi fu fw dk translated">towardsdatascience.com</p></div></div><div class="ml l"><div class="mm l mn mo mp ml mq ks mc"/></div></div></a></div></div><div class="ab cl mr ms hx mt" role="separator"><span class="mu bw bk mv mw mx"/><span class="mu bw bk mv mw mx"/><span class="mu bw bk mv mw"/></div><div class="im in io ip iq"><h1 id="2c67" class="my mz it bd na nb nc nd ne nf ng nh ni jz nj ka nk kc nl kd nm kf nn kg no np bi translated">逻辑回归概述</h1><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nq"><img src="../Images/9b0e14df3190b3c5861ccf9c3405a8d7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5f8Ejo2YEUniUN6QHbECAQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">逻辑回归模型；作者图片</p></figure><p id="01e8" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">正如我们在上面看到的，在逻辑回归模型中，我们取了一个大小为<code class="fe lu lv lw lx b">n</code>(特征)的向量<code class="fe lu lv lw lx b">x</code>(它仅代表<code class="fe lu lv lw lx b">m</code>中的一个例子)，并取了一个与<code class="fe lu lv lw lx b">weights</code>的点积，然后加上一个<code class="fe lu lv lw lx b">bias</code>。我们就叫它<code class="fe lu lv lw lx b">z</code>(线性部分)也就是<code class="fe lu lv lw lx b">w.X + b</code>。之后，我们应用逻辑回归的sigmoid激活函数来计算<code class="fe lu lv lw lx b">y_hat</code>。</p><blockquote class="nr"><p id="561d" class="ns nt it bd nu nv nw nx ny nz oa lt dk translated">y_hat = sigmoid(z) = sigmoid(w.X + b)</p></blockquote><blockquote class="ob oc od"><p id="de70" class="ky kz ly la b lb oe ju ld le of jx lg og oh lj lk oi oj ln lo ok ol lr ls lt im bi translated">从<code class="fe lu lv lw lx b">X</code>到直线部分的每条边代表一个<code class="fe lu lv lw lx b">weight</code>，直线部分的每一圈都有一个<code class="fe lu lv lw lx b">bias</code>。</p></blockquote><p id="e751" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">逻辑回归用于二进制分类，这意味着有2个类别(<strong class="la iu"> 0 </strong>或<strong class="la iu"> 1 </strong>)，并且由于sigmoid函数，我们得到0和1之间的输出(y_hat)。我们将逻辑模型的这个输出(<code class="fe lu lv lw lx b">y_hat</code>)解释为<code class="fe lu lv lw lx b">y</code>为1的概率，那么<code class="fe lu lv lw lx b">y</code>为0的概率就变成了<code class="fe lu lv lw lx b">(1-y_hat)</code>。</p><p id="74c9" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">问你一个问题——上图中的<code class="fe lu lv lw lx b">w</code>是什么形状？</p></div><div class="ab cl mr ms hx mt" role="separator"><span class="mu bw bk mv mw mx"/><span class="mu bw bk mv mw mx"/><span class="mu bw bk mv mw"/></div><div class="im in io ip iq"><h1 id="1537" class="my mz it bd na nb nc nd ne nf ng nh ni jz nj ka nk kc nl kd nm kf nn kg no np bi translated">Softmax回归</h1><p id="cf3b" class="pw-post-body-paragraph ky kz it la b lb om ju ld le on jx lg lh oo lj lk ll op ln lo lp oq lr ls lt im bi translated">现在，我们为自己设定一个目标— <strong class="la iu">来识别图像中的数字。</strong></p><p id="fcf8" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们将使用MNIST手写数据集作为理解Softmax回归的激励性示例。它有<strong class="la iu"> 10个等级</strong>，每个等级代表从<strong class="la iu"> 0 </strong>到<strong class="la iu"> 9 </strong>的一个数字。让我们先看看数据集。</p><p id="1f93" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">从<code class="fe lu lv lw lx b">keras.datasets</code>加载MNIST数据集并绘图。此外，拆分训练集和测试集。它在训练集中有60，000个示例，在测试集中有10，000个示例。</p><pre class="kj kk kl km gt or lx os ot aw ou bi"><span id="5e01" class="ov mz it lx b gy ow ox l oy oz">#Loading<br/><strong class="lx iu">from keras.datasets import mnist</strong></span><span id="793b" class="ov mz it lx b gy pa ox l oy oz"><strong class="lx iu">(train_X, train_y), (test_X, test_y) = mnist.load_data()</strong></span><span id="2adc" class="ov mz it lx b gy pa ox l oy oz">#Plotting</span><span id="fafa" class="ov mz it lx b gy pa ox l oy oz"><strong class="lx iu">fig = plt.figure(figsize=(10,7))</strong></span><span id="de8e" class="ov mz it lx b gy pa ox l oy oz"><strong class="lx iu">for i in range(15):  <br/>    ax = fig.add_subplot(3, 5, i+1)<br/>    ax.imshow(train_X[i], cmap=plt.get_cmap('gray'))<br/>    ax.set_title('Label (y): {y}'.format(y=train_y[i]))<br/>    plt.axis('off')</strong></span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pb"><img src="../Images/8a6319f1c9e2b98f4482e01baf5df3fa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1086/format:webp/1*QlSR3mXPjwA9r4guoJaPsg.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">MNIST数据；作者图片</p></figure><p id="7dee" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们的数据有数字图像和代表数字的标签。每幅图像都是<strong class="la iu">灰度</strong>，大小为<strong class="la iu"> 28x28像素</strong>。</p><blockquote class="ob oc od"><p id="c9e6" class="ky kz ly la b lb lc ju ld le lf jx lg og li lj lk oi lm ln lo ok lq lr ls lt im bi translated">第一个问题— <strong class="la iu">我们如何将图像作为输入？</strong></p></blockquote><p id="1729" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">每个图像都可以表示为一个三维矩阵。如果我们有一个尺寸为<strong class="la iu"> 28x28 </strong>的彩色图像，我们就有<strong class="la iu"> 28 x 28 x 3 </strong>个数字来表示该图像(RGB(红绿蓝)通道的<strong class="la iu"> 3 </strong>)。</p><p id="c5f8" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">但是我们有灰度图像，这意味着每个图像只有一个通道。所以，每张图片都是28x28像素。为了将图像输入到模型中，我们将把这个<strong class="la iu"> 28x28 </strong>展平成一个长度为<strong class="la iu"> 784 </strong> (28*28)的向量，这些<strong class="la iu"> 784 </strong>数字只是代表数据集中每个示例(图像)的<code class="fe lu lv lw lx b"><strong class="la iu">n</strong></code> <strong class="la iu"> </strong>特征的像素。</p><pre class="kj kk kl km gt or lx os ot aw ou bi"><span id="15cc" class="ov mz it lx b gy ow ox l oy oz"><strong class="lx iu">X_train = train_X.reshape(60000,28*28)<br/>X_test = test_X.reshape(10000,28*28)</strong></span></pre><blockquote class="ob oc od"><p id="4a90" class="ky kz ly la b lb lc ju ld le lf jx lg og li lj lk oi lm ln lo ok lq lr ls lt im bi translated">X_train是一个具有60，000行和784列的矩阵。</p><p id="07ea" class="ky kz ly la b lb lc ju ld le lf jx lg og li lj lk oi lm ln lo ok lq lr ls lt im bi translated">m = 6万；n = 784</p></blockquote></div><div class="ab cl mr ms hx mt" role="separator"><span class="mu bw bk mv mw mx"/><span class="mu bw bk mv mw mx"/><span class="mu bw bk mv mw"/></div><div class="im in io ip iq"><h1 id="ed9f" class="my mz it bd na nb nc nd ne nf ng nh ni jz nj ka nk kc nl kd nm kf nn kg no np bi translated">一键编码</h1><p id="3ef7" class="pw-post-body-paragraph ky kz it la b lb om ju ld le on jx lg lh oo lj lk ll op ln lo lp oq lr ls lt im bi translated">现在，我们的<code class="fe lu lv lw lx b">y</code>是一个像这样的向量—</p><pre class="kj kk kl km gt or lx os ot aw ou bi"><span id="b445" class="ov mz it lx b gy ow ox l oy oz"><strong class="lx iu">train_y</strong><br/>&gt;&gt; array([5, 0, 4, ..., 5, 6, 8], dtype=uint8)</span></pre><p id="5694" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这是一个带有标签的NumPy数组。我们不能把它用于我们的模型，我们必须以某种方式把它修改成0和1，这就是我们所说的一键编码。我们希望我们的<code class="fe lu lv lw lx b">y</code>看起来像这样(一个大小为60，000 x 10的矩阵)—</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pc"><img src="../Images/6c12fd5d5acc8b381a9f35370628d904.png" data-original-src="https://miro.medium.com/v2/resize:fit:874/format:webp/1*YIdLfp9G9R3Z2qeYzimI6Q.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">y的一热表示；作者图片</p></figure><p id="c5e6" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">行代表第I个例子，第I列告诉我们标签。例如，对于第一个示例，有一个1，其中列名为5，其余的为零。因此，第一个示例的标签是5，其他示例也是如此。对于每个示例，只有一列的值为1.0，其余的值为零。</p><p id="c8b8" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">让我们编写一个函数来一次性编码我们的标签—</p><blockquote class="ob oc od"><p id="522e" class="ky kz ly la b lb lc ju ld le lf jx lg og li lj lk oi lm ln lo ok lq lr ls lt im bi translated"><code class="fe lu lv lw lx b"><strong class="la iu">c </strong></code> =班级人数</p></blockquote><pre class="kj kk kl km gt or lx os ot aw ou bi"><span id="5c4c" class="ov mz it lx b gy ow ox l oy oz"><strong class="lx iu">def one_hot(y, c):</strong><br/>    <br/>    # y--&gt; label/ground truth.<br/>    # c--&gt; Number of classes.<br/>    <br/>    # A zero matrix of size (m, c)<br/>    <strong class="lx iu">y_hot = np.zeros((len(y), c))</strong><br/>    <br/>    # Putting 1 for column where the label is,<br/>    # Using multidimensional indexing.<br/>    <strong class="lx iu">y_hot[np.arange(len(y)), y] = 1</strong><br/>    <br/>  <strong class="lx iu">  return y_hot</strong></span></pre><blockquote class="ob oc od"><p id="3084" class="ky kz ly la b lb lc ju ld le lf jx lg og li lj lk oi lm ln lo ok lq lr ls lt im bi translated">引用—<a class="ae pd" href="https://docs.scipy.org/doc/numpy/user/basics.indexing.html#indexing-multi-dimensional-arrays" rel="noopener ugc nofollow" target="_blank">NumPy中的多维索引</a></p></blockquote></div><div class="ab cl mr ms hx mt" role="separator"><span class="mu bw bk mv mw mx"/><span class="mu bw bk mv mw mx"/><span class="mu bw bk mv mw"/></div><div class="im in io ip iq"><h1 id="b4f2" class="my mz it bd na nb nc nd ne nf ng nh ni jz nj ka nk kc nl kd nm kf nn kg no np bi translated">Softmax函数</h1><p id="ef2f" class="pw-post-body-paragraph ky kz it la b lb om ju ld le on jx lg lh oo lj lk ll op ln lo lp oq lr ls lt im bi translated">在使用Softmax回归进行多类分类时，我们有一个<strong class="la iu">约束</strong>，即我们的模型将只预测<code class="fe lu lv lw lx b"><strong class="la iu">c</strong></code> <strong class="la iu"> </strong>类中的一个类。对于我们的数据，这意味着模型将只预测图像中的一个数字(从0到9)。</p><p id="06d6" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们将逻辑模型的输出解释为概率。类似地，我们希望将多类分类模型的输出解释为<strong class="la iu">概率分布</strong>。因此，我们希望我们的模型输出一个大小为<code class="fe lu lv lw lx b"><strong class="la iu">c</strong></code> <strong class="la iu"> </strong>的向量，向量中的每个值代表每个类的概率。换句话说，向量中的第<strong class="la iu">个</strong>值代表我们的预测成为第<strong class="la iu">个</strong>类的概率。由于它们都是概率，所以它们的和等于1。</p><p id="8808" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">为了符合上述假设，我们使用softmax函数。</p><p id="7e31" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">第<strong class="la iu">级</strong>的softmax定义为—</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pe"><img src="../Images/789f5d2f1ec23fb52b4bd321d18f5105.png" data-original-src="https://miro.medium.com/v2/resize:fit:244/format:webp/1*A_PAhFZsclWtFDScnxvA0g.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">Softmax函数；作者图片</p></figure><p id="a8e9" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">其中，<code class="fe lu lv lw lx b"><strong class="la iu">z</strong></code> <strong class="la iu"> </strong>是直线部分。例如<code class="fe lu lv lw lx b"><strong class="la iu">z1 = w1.X + b1</strong></code>，对其他人也是如此。</p><blockquote class="nr"><p id="8e92" class="ns nt it bd nu nv nw nx ny nz oa lt dk translated">y_hat = softmax(w.X + b)</p></blockquote><blockquote class="ob oc od"><p id="b3ed" class="ky kz ly la b lb oe ju ld le of jx lg og oh lj lk oi oj ln lo ok ol lr ls lt im bi translated">对于我们的数据，<code class="fe lu lv lw lx b"><strong class="la iu">c</strong></code>(类的数量)=10。</p></blockquote><h2 id="75d6" class="ov mz it bd na pf pg dn ne ph pi dp ni lh pj pk nk ll pl pm nm lp pn po no pp bi translated"><strong class="ak">让我们借助下面的模型图来理解Softmax函数和Softmax回归。</strong></h2><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pq"><img src="../Images/4c63fef9f34391bc68a45ec5186eb68f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xbKo-8awaX5jpuMdPfBLyQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">Softmax回归模型；作者图片</p></figure><ol class=""><li id="0a3e" class="pr ps it la b lb lc le lf lh pt ll pu lp pv lt pw px py pz bi translated">首先，我们将我们的<strong class="la iu"> 28x28 </strong>图像展平成一个长度为<strong class="la iu"> 784 </strong>的向量，在上图中用<code class="fe lu lv lw lx b">x</code>表示。</li><li id="d609" class="pr ps it la b lb qa le qb lh qc ll qd lp qe lt pw px py pz bi translated">其次，我们计算每个类的线性部分→ <code class="fe lu lv lw lx b">zc = wc.X + bc</code>,其中<code class="fe lu lv lw lx b">zc</code>是第<strong class="la iu">个</strong>类的线性部分，而<code class="fe lu lv lw lx b">wc</code>是第<strong class="la iu">个</strong>类的权重集。<code class="fe lu lv lw lx b">bc</code>是对第c类的偏向。</li><li id="3951" class="pr ps it la b lb qa le qb lh qc ll qd lp qe lt pw px py pz bi translated">第三，我们使用上面的公式计算每个类的soft max<code class="fe lu lv lw lx b">zc</code>。组合所有的类，我们得到一个大小为<code class="fe lu lv lw lx b">c</code>的向量，它们的和等于1。而哪个类的值(概率)最高，就成为我们的预测。</li></ol><blockquote class="ob oc od"><p id="da79" class="ky kz ly la b lb lc ju ld le lf jx lg og li lj lk oi lm ln lo ok lq lr ls lt im bi translated"><strong class="la iu">请注意，每个类别都有不同的权重集和不同的偏差。</strong></p><p id="18be" class="ky kz ly la b lb lc ju ld le lf jx lg og li lj lk oi lm ln lo ok lq lr ls lt im bi translated"><strong class="la iu">还要注意，上图显示的</strong> <code class="fe lu lv lw lx b"><strong class="la iu">x</strong></code> <strong class="la iu">只是一个例子。</strong></p></blockquote><p id="2208" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">每个<code class="fe lu lv lw lx b">zc </code>都是一个(1，1)矩阵。但是因为我们有了<code class="fe lu lv lw lx b">m</code>的例子，我们将把<code class="fe lu lv lw lx b">zc</code>(第c个类的<code class="fe lu lv lw lx b">z</code>)表示为一个大小为<code class="fe lu lv lw lx b">m</code>的向量。现在，为了组合所有的<code class="fe lu lv lw lx b"><strong class="la iu">z</strong></code> <strong class="la iu">的</strong>类，我们将把它们并排放在一起，这将给出一个大小矩阵(<code class="fe lu lv lw lx b">m</code>，<code class="fe lu lv lw lx b">c</code>)。</p><p id="70ee" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">类似地，对于每个类，<code class="fe lu lv lw lx b">weights</code>的集合是一个大小为<code class="fe lu lv lw lx b">n</code>的向量。因此，为了组合所有的类，我们将把它们并排放在一起，这将给出一个大小矩阵(<code class="fe lu lv lw lx b">n</code>，<code class="fe lu lv lw lx b">c</code>)。此外，我们对每个类都有一个偏差，组合偏差将是一个大小为<code class="fe lu lv lw lx b">c</code>的向量。</p><p id="e15f" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">结合softmax的所有类给出了一个大小为<code class="fe lu lv lw lx b">c</code>的向量。并且结合所有的<code class="fe lu lv lw lx b">m</code>例子给出一个大小的矩阵(<code class="fe lu lv lw lx b">m</code>，<code class="fe lu lv lw lx b">c</code>)。</p><h2 id="12eb" class="ov mz it bd na pf pg dn ne ph pi dp ni lh pj pk nk ll pl pm nm lp pn po no pp bi translated"><strong class="ak">形状— </strong></h2><ol class=""><li id="8d3b" class="pr ps it la b lb om le on lh qf ll qg lp qh lt pw px py pz bi translated"><strong class="la iu"> X →(m，n) </strong></li><li id="9a1e" class="pr ps it la b lb qa le qb lh qc ll qd lp qe lt pw px py pz bi translated"><strong class="la iu"> y →(m，c)【一个热编码】</strong></li><li id="6cda" class="pr ps it la b lb qa le qb lh qc ll qd lp qe lt pw px py pz bi translated"><strong class="la iu"> w →(n，c) </strong></li><li id="a70c" class="pr ps it la b lb qa le qb lh qc ll qd lp qe lt pw px py pz bi translated"><strong class="la iu">b→大小为c的向量</strong></li></ol><p id="8c59" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">让我们为softmax函数编写代码。请参见注释(#)。</p><pre class="kj kk kl km gt or lx os ot aw ou bi"><span id="82eb" class="ov mz it lx b gy ow ox l oy oz"><strong class="lx iu">def softmax(z):</strong><br/>    <br/>    # z--&gt; linear part.<br/>    <br/>    # subtracting the max of z for numerical stability.<br/>    <strong class="lx iu">exp = np.exp(z - np.max(z))</strong><br/>    <br/>    # Calculating softmax for all examples.<br/>    <strong class="lx iu">for i in range(len(z)):<br/>        exp[i] /= np.sum(exp[i])</strong><br/>        <br/><strong class="lx iu">    return exp</strong></span></pre><blockquote class="ob oc od"><p id="6485" class="ky kz ly la b lb lc ju ld le lf jx lg og li lj lk oi lm ln lo ok lq lr ls lt im bi translated">reference—<a class="ae pd" href="https://stackoverflow.com/questions/34968722/how-to-implement-the-softmax-function-in-python" rel="noopener ugc nofollow" target="_blank">数值稳定的softmax </a></p><p id="8dab" class="ky kz ly la b lb lc ju ld le lf jx lg og li lj lk oi lm ln lo ok lq lr ls lt im bi translated">reference—<a class="ae pd" href="https://deepnotes.io/softmax-crossentropy" rel="noopener ugc nofollow" target="_blank">soft max</a>的导数</p></blockquote></div><div class="ab cl mr ms hx mt" role="separator"><span class="mu bw bk mv mw mx"/><span class="mu bw bk mv mw mx"/><span class="mu bw bk mv mw"/></div><div class="im in io ip iq"><h1 id="7055" class="my mz it bd na nb nc nd ne nf ng nh ni jz nj ka nk kc nl kd nm kf nn kg no np bi translated">交叉熵损失</h1><p id="64dc" class="pw-post-body-paragraph ky kz it la b lb om ju ld le on jx lg lh oo lj lk ll op ln lo lp oq lr ls lt im bi translated">对于每个参数机器学习算法，我们需要一个损失函数，我们希望最小化该函数(找到其全局最小值)，以确定最佳参数(<code class="fe lu lv lw lx b">w</code>和<code class="fe lu lv lw lx b">b</code>)，这将帮助我们做出最佳预测。</p><p id="4371" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">对于softmax回归，我们使用交叉熵(CE)损失—</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi qi"><img src="../Images/c0c218a11fcbbe1e2f1470e35c71c75e.png" data-original-src="https://miro.medium.com/v2/resize:fit:524/format:webp/1*O4aLWAOiacxwk0KbvnX72g.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">CE损耗；作者图片</p></figure><blockquote class="ob oc od"><p id="2408" class="ky kz ly la b lb lc ju ld le lf jx lg og li lj lk oi lm ln lo ok lq lr ls lt im bi translated"><strong class="la iu">参考如何计算损失的导数。</strong></p><p id="04c3" class="ky kz ly la b lb lc ju ld le lf jx lg og li lj lk oi lm ln lo ok lq lr ls lt im bi translated">参考— <a class="ae pd" href="https://deepnotes.io/softmax-crossentropy" rel="noopener ugc nofollow" target="_blank">与Softmax </a>交叉熵损失的导数</p><p id="1ad1" class="ky kz ly la b lb lc ju ld le lf jx lg og li lj lk oi lm ln lo ok lq lr ls lt im bi translated">reference—<a class="ae pd" href="https://math.stackexchange.com/questions/945871/derivative-of-softmax-loss-function" rel="noopener ugc nofollow" target="_blank">soft max损失函数的导数</a></p></blockquote><p id="5763" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在代码中，损失看起来像这样—</p><pre class="kj kk kl km gt or lx os ot aw ou bi"><span id="3dab" class="ov mz it lx b gy ow ox l oy oz"><strong class="lx iu">loss = -np.mean(np.log(y_hat[np.arange(len(y)), y]))</strong></span></pre><p id="9fbc" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">再次使用多维索引—<a class="ae pd" href="https://docs.scipy.org/doc/numpy/user/basics.indexing.html#indexing-multi-dimensional-arrays" rel="noopener ugc nofollow" target="_blank">NumPy中的多维索引</a></p><blockquote class="ob oc od"><p id="9785" class="ky kz ly la b lb lc ju ld le lf jx lg og li lj lk oi lm ln lo ok lq lr ls lt im bi translated">注意<code class="fe lu lv lw lx b">y</code>在损失函数中不是一键编码的。</p></blockquote></div><div class="ab cl mr ms hx mt" role="separator"><span class="mu bw bk mv mw mx"/><span class="mu bw bk mv mw mx"/><span class="mu bw bk mv mw"/></div><div class="im in io ip iq"><h1 id="cd79" class="my mz it bd na nb nc nd ne nf ng nh ni jz nj ka nk kc nl kd nm kf nn kg no np bi translated">培养</h1><ol class=""><li id="7f80" class="pr ps it la b lb om le on lh qf ll qg lp qh lt pw px py pz bi translated">初始化参数— <code class="fe lu lv lw lx b">w</code>和<code class="fe lu lv lw lx b">b</code>。</li><li id="ea0f" class="pr ps it la b lb qa le qb lh qc ll qd lp qe lt pw px py pz bi translated">使用梯度下降法找到最佳的<code class="fe lu lv lw lx b">w</code>和<code class="fe lu lv lw lx b">b</code>。</li><li id="1f32" class="pr ps it la b lb qa le qb lh qc ll qd lp qe lt pw px py pz bi translated">用<code class="fe lu lv lw lx b">softmax(w.X + b)</code>预测。</li></ol><pre class="kj kk kl km gt or lx os ot aw ou bi"><span id="1e45" class="ov mz it lx b gy ow ox l oy oz"><strong class="lx iu">def fit(X, y, lr, c, epochs):</strong><br/>    <br/>    # X --&gt; Input.<br/>    # y --&gt; true/target value.<br/>    # lr --&gt; Learning rate.<br/>    # c --&gt; Number of classes.<br/>    # epochs --&gt; Number of iterations.<br/>    <br/>        <br/>    # m-&gt; number of training examples<br/>    # n-&gt; number of features <br/>    <strong class="lx iu">m, n = X.shape</strong><br/>    <br/>    # Initializing weights and bias randomly.<br/>    <strong class="lx iu">w = np.random.random((n, c))<br/>    b = np.random.random(c)</strong></span><span id="a032" class="ov mz it lx b gy pa ox l oy oz">    # Empty list to store losses.<br/>  <strong class="lx iu">  losses = []</strong><br/>    <br/>    # Training loop.<br/>    <strong class="lx iu">for epoch in range(epochs):</strong><br/>        <br/>        # Calculating hypothesis/prediction.<br/>        <strong class="lx iu">z = X@w + b<br/>        y_hat = softmax(z)</strong><br/>        <br/>        # One-hot encoding y.<br/>       <strong class="lx iu"> y_hot = one_hot(y, c)<br/>        </strong><br/>        # Calculating the gradient of loss w.r.t w and b.<br/>       <strong class="lx iu"> w_grad = (1/m)*np.dot(X.T, (y_hat - y_hot)) <br/>        b_grad = (1/m)*np.sum(y_hat - y_hot)</strong><br/>        <br/>        # Updating the parameters.<br/>       <strong class="lx iu"> w = w - lr*w_grad<br/>        b = b - lr*b_grad</strong><br/>        <br/>        # Calculating loss and appending it in the list.<br/>       <strong class="lx iu"> loss = -np.mean(np.log(y_hat[np.arange(len(y)), y]))<br/>        losses.append(loss)</strong></span><span id="22d6" class="ov mz it lx b gy pa ox l oy oz">        # Printing out the loss at every 100th iteration.<br/>        <strong class="lx iu">if epoch%100==0:<br/>            print('Epoch {epoch}==&gt; Loss = {loss}'<br/>                  .format(epoch=epoch, loss=loss))</strong></span><span id="63da" class="ov mz it lx b gy pa ox l oy oz">   <strong class="lx iu"> return w, b, losses</strong></span></pre><h1 id="abcd" class="my mz it bd na nb qj nd ne nf qk nh ni jz ql ka nk kc qm kd nm kf qn kg no np bi translated">现在让我们训练我们的MNIST数据—</h1><p id="9f6a" class="pw-post-body-paragraph ky kz it la b lb om ju ld le on jx lg lh oo lj lk ll op ln lo lp oq lr ls lt im bi translated">请参见注释(#)。</p><pre class="kj kk kl km gt or lx os ot aw ou bi"><span id="7d67" class="ov mz it lx b gy ow ox l oy oz"># Flattening the image.<br/><strong class="lx iu">X_train = train_X.reshape(60000,28*28)</strong></span><span id="d049" class="ov mz it lx b gy pa ox l oy oz"># Normalizing. <br/><strong class="lx iu">X_train = X_train/255</strong></span><span id="1bee" class="ov mz it lx b gy pa ox l oy oz"># Training<br/><strong class="lx iu">w, b, l = fit(X_train, train_y, lr=0.9, c=10, epochs=1000)<br/>&gt;&gt; </strong>Epoch 0==&gt; Loss = 4.765269749988195<br/>Epoch 100==&gt; Loss = 0.41732772667703605<br/>Epoch 200==&gt; Loss = 0.36146764856576774<br/>Epoch 300==&gt; Loss = 0.3371995534802398<br/>Epoch 400==&gt; Loss = 0.32295154931574305<br/>Epoch 500==&gt; Loss = 0.31331228168677683<br/>Epoch 600==&gt; Loss = 0.3062124554422963<br/>Epoch 700==&gt; Loss = 0.3006810669534496<br/>Epoch 800==&gt; Loss = 0.29619875396394774<br/>Epoch 900==&gt; Loss = 0.29246033264255616</span></pre><blockquote class="ob oc od"><p id="20f7" class="ky kz ly la b lb lc ju ld le lf jx lg og li lj lk oi lm ln lo ok lq lr ls lt im bi translated">我们可以看到，每次迭代后损失都在下降，这意味着我们做得很好。</p></blockquote><h1 id="aa91" class="my mz it bd na nb qj nd ne nf qk nh ni jz ql ka nk kc qm kd nm kf qn kg no np bi translated">预测</h1><p id="243b" class="pw-post-body-paragraph ky kz it la b lb om ju ld le on jx lg lh oo lj lk ll op ln lo lp oq lr ls lt im bi translated">请参见注释(#)。</p><pre class="kj kk kl km gt or lx os ot aw ou bi"><span id="c6d5" class="ov mz it lx b gy ow ox l oy oz"><strong class="lx iu">def predict(X, w, b):</strong><br/>    <br/>    # X --&gt; Input.<br/>    # w --&gt; weights.<br/>    # b --&gt; bias.<br/>    <br/>    # Predicting<br/>  <strong class="lx iu">  z = X@w + b<br/>    y_hat = softmax(z)</strong><br/>    <br/>    # Returning the class with highest probability.<br/>    <strong class="lx iu">return np.argmax(y_hat, axis=1)</strong></span></pre><h1 id="44c3" class="my mz it bd na nb qj nd ne nf qk nh ni jz ql ka nk kc qm kd nm kf qn kg no np bi translated">计量精度</h1><pre class="kj kk kl km gt or lx os ot aw ou bi"><span id="98a4" class="ov mz it lx b gy ow ox l oy oz"><strong class="lx iu">def accuracy(y, y_hat):<br/>    return np.sum(y==y_hat)/len(y)</strong></span></pre><p id="1087" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">让我们在训练和测试集上计算我们的模型的准确性。</p><pre class="kj kk kl km gt or lx os ot aw ou bi"><span id="836d" class="ov mz it lx b gy ow ox l oy oz"># Accuracy for training set.<br/><strong class="lx iu">train_preds = predict(X_train, w, b)<br/>accuracy(train_y, train_preds)<br/>&gt;&gt; 0.9187666</strong></span><span id="f76b" class="ov mz it lx b gy pa ox l oy oz"># Accuracy for test set.</span><span id="955b" class="ov mz it lx b gy pa ox l oy oz"># Flattening and normalizing.<br/><strong class="lx iu">X_test = test_X.reshape(10000,28*28)<br/>X_test = X_test/255</strong></span><span id="60a7" class="ov mz it lx b gy pa ox l oy oz"><strong class="lx iu">test_preds = predict(X_test, w, b)<br/>accuracy(test_y, test_preds)<br/>&gt;&gt; 0.9173</strong></span></pre><blockquote class="ob oc od"><p id="441f" class="ky kz ly la b lb lc ju ld le lf jx lg og li lj lk oi lm ln lo ok lq lr ls lt im bi translated"><strong class="la iu">我们的模型在训练集上的准确率为91.8%，在测试集上的准确率为91.7%。</strong></p><p id="00f8" class="ky kz ly la b lb lc ju ld le lf jx lg og li lj lk oi lm ln lo ok lq lr ls lt im bi translated"><strong class="la iu">不错。</strong></p></blockquote><h1 id="53c5" class="my mz it bd na nb qj nd ne nf qk nh ni jz ql ka nk kc qm kd nm kf qn kg no np bi translated">绘制预测</h1><pre class="kj kk kl km gt or lx os ot aw ou bi"><span id="4485" class="ov mz it lx b gy ow ox l oy oz"><strong class="lx iu">fig = plt.figure(figsize=(15,10))</strong></span><span id="2461" class="ov mz it lx b gy pa ox l oy oz"><strong class="lx iu">for i in range(40):  <br/>    ax = fig.add_subplot(5, 8, i+1)<br/>    ax.imshow(test_X[i], cmap=plt.get_cmap('gray'))<br/>    <br/>    ax.set_title('y: {y}/ y_hat: {y_hat}'<br/>                 .format(y=test_y[i], y_hat=test_preds)<br/>    plt.axis('off')</strong></span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi qo"><img src="../Images/c76110b461dfd0344f8fcd1eb102884e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Fhqhv96lQsZr6nLErBTjPw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">预测；作者图片</p></figure><p id="c106" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">上图显示了模型对训练集中40个示例的预测。我们可以看到其中有一些是预测错误的，比如上图中用红色圈出的那个。</p></div><div class="ab cl mr ms hx mt" role="separator"><span class="mu bw bk mv mw mx"/><span class="mu bw bk mv mw mx"/><span class="mu bw bk mv mw"/></div><div class="im in io ip iq"><p id="8efe" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">感谢阅读。对于问题、评论、顾虑，请在回复部分进行讨论。更多的ML从零开始即将推出。</p><p id="09d1" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu">看看从零开始学习的机器系列— </strong></p><ul class=""><li id="ec1d" class="pr ps it la b lb lc le lf lh pt ll pu lp pv lt qp px py pz bi translated">第1部分:<a class="ae pd" href="https://medium.com/analytics-vidhya/linear-regression-from-scratch-in-python-b6501f91c82d?source=your_stories_page-------------------------------------" rel="noopener"><strong class="la iu">Python中从零开始的线性回归</strong> </a></li><li id="2983" class="pr ps it la b lb qa le qb lh qc ll qd lp qe lt qp px py pz bi translated">第二部分:<a class="ae pd" rel="noopener" target="_blank" href="/locally-weighted-linear-regression-in-python-3d324108efbf?source=your_stories_page-------------------------------------"><strong class="la iu">Python中的局部加权线性回归</strong> </a></li><li id="de85" class="pr ps it la b lb qa le qb lh qc ll qd lp qe lt qp px py pz bi translated">第三部分:<a class="ae pd" rel="noopener" target="_blank" href="/normal-equation-in-python-the-closed-form-solution-for-linear-regression-13df33f9ad71?source=your_stories_page-------------------------------------"> <strong class="la iu">使用Python的正规方程:线性回归的封闭解</strong> </a></li><li id="379e" class="pr ps it la b lb qa le qb lh qc ll qd lp qe lt qp px py pz bi translated">第四部分:<a class="ae pd" rel="noopener" target="_blank" href="/polynomial-regression-in-python-b69ab7df6105"><strong class="la iu">Python中的多项式回归从零开始</strong> </a></li><li id="4404" class="pr ps it la b lb qa le qb lh qc ll qd lp qe lt qp px py pz bi translated">第五部分:<a class="ae pd" rel="noopener" target="_blank" href="/logistic-regression-from-scratch-in-python-ec66603592e2"><strong class="la iu">Python中的逻辑回归从零开始</strong> </a></li><li id="c801" class="pr ps it la b lb qa le qb lh qc ll qd lp qe lt qp px py pz bi translated">第六部分:<a class="ae pd" rel="noopener" target="_blank" href="/perceptron-algorithm-in-python-f3ac89d2e537"> <strong class="la iu">用Python实现感知器算法</strong> </a></li></ul></div></div>    
</body>
</html>