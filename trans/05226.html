<html>
<head>
<title>Neural Networks Backpropagation Made Easy</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">神经网络反向传播变得容易</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/neural-networks-backpropagation-by-dr-lihi-gur-arie-27be67d8fdce?source=collection_archive---------8-----------------------#2021-05-08">https://towardsdatascience.com/neural-networks-backpropagation-by-dr-lihi-gur-arie-27be67d8fdce?source=collection_archive---------8-----------------------#2021-05-08</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="48a4" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">香草神经网络背后的数学直觉的友好指南。</h2></div></div><div class="ab cl kf kg hu kh" role="separator"><span class="ki bw bk kj kk kl"/><span class="ki bw bk kj kk kl"/><span class="ki bw bk kj kk"/></div><div class="ij ik il im in"><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="gh gi km"><img src="../Images/34f82b485316494072adbbb9fed16824.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*diQ7YbcLAZvqTMSOU8TYNw.png"/></div></div><p class="ky kz gj gh gi la lb bd b be z dk translated">图一。神经网络中的反向传播|作者图片</p></figure><h1 id="ba15" class="lc ld iq bd le lf lg lh li lj lk ll lm jw ln jx lo jz lp ka lq kc lr kd ls lt bi translated">介绍</h1><p id="44c5" class="pw-post-body-paragraph lu lv iq lw b lx ly jr lz ma mb ju mc md me mf mg mh mi mj mk ml mm mn mo mp ij bi translated">在设计高效的深度模型时，理解神经网络(NNs)背后的数学操作数对于数据科学家的能力非常重要。在本文中，将展示全连接神经网络的高级演算，重点是反向传播步骤。这篇文章面向对神经网络有基本知识的人，他们试图更深入地研究神经网络的结构。</p><h1 id="f1ee" class="lc ld iq bd le lf lg lh li lj lk ll lm jw ln jx lo jz lp ka lq kc lr kd ls lt bi translated">背景</h1><p id="8382" class="pw-post-body-paragraph lu lv iq lw b lx ly jr lz ma mb ju mc md me mf mg mh mi mj mk ml mm mn mo mp ij bi translated">训练过程的目标是找到使误差最小化的权重(W)和偏差(b)。这是通过<strong class="lw ir">梯度下降</strong>算法完成的。首先，随机初始化权重，并且执行细微权重变化的迭代过程，直到收敛。</p><p id="41e5" class="pw-post-body-paragraph lu lv iq lw b lx mq jr lz ma mr ju mc md ms mf mg mh mt mj mk ml mu mn mo mp ij bi translated">每次迭代从正向传递开始，输出当前预测，并通过成本函数J(等式6)评估模型的误差。接下来，进行反向传递来计算权重梯度。</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div class="gh gi mv"><img src="../Images/83cb79e5e5d86a7dea8ec9beb020d0bd.png" data-original-src="https://miro.medium.com/v2/resize:fit:854/format:webp/1*TQxFoFiow3Ko92d7WfAqVQ.png"/></div><p class="ky kz gj gh gi la lb bd b be z dk translated">图二。简化的成本函数曲线。在神经网络中，这个函数可能不是凸的。|作者图片</p></figure><p id="099e" class="pw-post-body-paragraph lu lv iq lw b lx mq jr lz ma mr ju mc md ms mf mg mh mt mj mk ml mu mn mo mp ij bi translated">为了找到使误差最小的最佳参数，我们使用梯度下降算法。目标是找到成本函数(J)的最小点，其中梯度接近于零(图2)。该算法在最陡下降的方向上朝着最小点逐步迭代移动。步长被称为“<strong class="lw ir">学习速率</strong>，它是一个标量，决定了每次迭代中权重的变化，从而决定了NN收敛的速度。学习率是一个必须调整的超参数。较小的学习率需要更多的训练时间，而较大的学习率会导致快速训练，但可能会损害性能和不稳定性。</p><p id="5843" class="pw-post-body-paragraph lu lv iq lw b lx mq jr lz ma mr ju mc md ms mf mg mh mt mj mk ml mu mn mo mp ij bi translated">为了更新权重，梯度乘以学习率(α),并且通过以下公式计算新的权重:</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div class="gh gi mw"><img src="../Images/5c0d93f39c40664b07cf41ee6905b236.png" data-original-src="https://miro.medium.com/v2/resize:fit:566/format:webp/1*2wULsk4M4HG12bZ5cB-bPA.png"/></div><p class="ky kz gj gh gi la lb bd b be z dk translated">等式1。梯度下降的权重更新公式。W =权重，alpha =学习率，J =成本。</p></figure><p id="120a" class="pw-post-body-paragraph lu lv iq lw b lx mq jr lz ma mr ju mc md ms mf mg mh mt mj mk ml mu mn mo mp ij bi translated">随着模型的迭代，梯度逐渐向零收敛，其中误差很可能是最低的(或者，模型可能收敛到局部最优并呈现次优性能)。</p><h1 id="d13f" class="lc ld iq bd le lf lg lh li lj lk ll lm jw ln jx lo jz lp ka lq kc lr kd ls lt bi translated"><strong class="ak">描述网</strong></h1><p id="6ef5" class="pw-post-body-paragraph lu lv iq lw b lx ly jr lz ma mb ju mc md me mf mg mh mi mj mk ml mm mn mo mp ij bi translated">本文将遵循两层神经网络的结构，其中X(也称为A[0])是输入向量，A[1]是隐藏层，Y-hat是输出层。网络的基本架构如下图所示(每层中的神经元数量与方程无关，因为我们使用的是矢量化方程):</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div class="gh gi mx"><img src="../Images/bd5672fb87ed9ac3dacf886a626086b8.png" data-original-src="https://miro.medium.com/v2/resize:fit:616/format:webp/1*u7eyYT88D6lYn-p7UXA1hw.png"/></div><p class="ky kz gj gh gi la lb bd b be z dk translated">图3。两层神经网络|作者图片</p></figure></div><div class="ab cl kf kg hu kh" role="separator"><span class="ki bw bk kj kk kl"/><span class="ki bw bk kj kk kl"/><span class="ki bw bk kj kk"/></div><div class="ij ik il im in"><h1 id="428a" class="lc ld iq bd le lf my lh li lj mz ll lm jw na jx lo jz nb ka lq kc nc kd ls lt bi translated"><strong class="ak">向前传球</strong></h1><p id="a357" class="pw-post-body-paragraph lu lv iq lw b lx ly jr lz ma mb ju mc md me mf mg mh mi mj mk ml mm mn mo mp ij bi translated">为了理解反向传播是如何计算的，我们首先需要概述一下正向传播。我们的网络从一个矢量化的线性方程开始，其中的层数用方括号表示。</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div class="gh gi nd"><img src="../Images/b385b766492cb9b9d68edaaf2a42a244.png" data-original-src="https://miro.medium.com/v2/resize:fit:470/format:webp/1*w2pRQKB05bynMRIyOFPuuA.png"/></div><p class="ky kz gj gh gi la lb bd b be z dk translated">等式2。直线方程。</p></figure><p id="894b" class="pw-post-body-paragraph lu lv iq lw b lx mq jr lz ma mr ju mc md ms mf mg mh mt mj mk ml mu mn mo mp ij bi translated">接下来，添加非线性<strong class="lw ir">激活函数</strong> (A)。这种激活使网络能够打破线性并适应数据中的复杂模式。可以使用几种不同的激活函数(例如，<a class="ae ne" href="https://en.wikipedia.org/wiki/Sigmoid_function" rel="noopener ugc nofollow" target="_blank"> sigmoid </a>、<a class="ae ne" href="https://en.wikipedia.org/wiki/Rectifier_(neural_networks)" rel="noopener ugc nofollow" target="_blank"> ReLU </a>、<a class="ae ne" href="https://en.wikipedia.org/wiki/Hyperbolic_function" rel="noopener ugc nofollow" target="_blank"> tanh </a>)，这里我们将使用我们的NN的sigmoid激活。</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div class="gh gi nf"><img src="../Images/c01df94ee19b737b5a4488105c16cf55.png" data-original-src="https://miro.medium.com/v2/resize:fit:530/format:webp/1*CgxLfczklWQJUhoUvt3kqw.png"/></div><p class="ky kz gj gh gi la lb bd b be z dk translated">等式3。乙状结肠激活。</p></figure><p id="a597" class="pw-post-body-paragraph lu lv iq lw b lx mq jr lz ma mr ju mc md ms mf mg mh mt mj mk ml mu mn mo mp ij bi translated">到目前为止，我们计算了第一层。第二层和第一层一样，由一个线性方程(Z[2])组成，后面是一个sigmoid激活(A[2])。由于这是我们网中的最后一层，激活结果(A[2])就是模型的预测(Y-hat)。</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div class="gh gi ng"><img src="../Images/ed10d95981c1053ef07001e5ea6d71a1.png" data-original-src="https://miro.medium.com/v2/resize:fit:522/format:webp/1*MdUKW54-iIKsADhTGN_m_w.png"/></div><p class="ky kz gj gh gi la lb bd b be z dk translated">方程式4 + 5。直线和sigmoid方程。</p></figure><p id="4744" class="pw-post-body-paragraph lu lv iq lw b lx mq jr lz ma mr ju mc md ms mf mg mh mt mj mk ml mu mn mo mp ij bi translated">最后，为了评估和最小化误差，我们定义了一个<strong class="lw ir">成本函数</strong>(关于激活和成本函数的更多信息，请参考参考文献[ <a class="ae ne" href="https://medium.com/@zeeshanmulla/cost-activation-loss-function-neural-network-deep-learning-what-are-these-91167825a4de" rel="noopener"> 1 </a> ])。这里我们使用的是“均方误差”(MSE)函数。为了简单起见，我们将使用随机梯度下降(SGD)方法[ <a class="ae ne" rel="noopener" target="_blank" href="/stochastic-gradient-descent-clearly-explained-53d239905d31"> 2 </a> ]，这意味着在每次迭代中只处理一个样本。</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div class="gh gi nh"><img src="../Images/4dd87cbc7de9458bc0510fad1825a47d.png" data-original-src="https://miro.medium.com/v2/resize:fit:492/format:webp/1*PnQLB1iThfcMKef0I5xauw.png"/></div><p class="ky kz gj gh gi la lb bd b be z dk translated">等式6。均方误差成本函数。</p></figure><p id="7fd6" class="pw-post-body-paragraph lu lv iq lw b lx mq jr lz ma mr ju mc md ms mf mg mh mt mj mk ml mu mn mo mp ij bi translated">我们可以总结出向前传球的计算图</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="gh gi ni"><img src="../Images/7079d71bcd79c7bb95e690e6dea204ef.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BEc2lyHB_Vs94GuNGsv0Yw.png"/></div></div><p class="ky kz gj gh gi la lb bd b be z dk translated">图4。作者提供的向前传递|图像的计算图</p></figure></div><div class="ab cl kf kg hu kh" role="separator"><span class="ki bw bk kj kk kl"/><span class="ki bw bk kj kk kl"/><span class="ki bw bk kj kk"/></div><div class="ij ik il im in"><h1 id="7c97" class="lc ld iq bd le lf my lh li lj mz ll lm jw na jx lo jz nb ka lq kc nc kd ls lt bi translated">偶数道次</h1><p id="035e" class="pw-post-body-paragraph lu lv iq lw b lx ly jr lz ma mb ju mc md me mf mg mh mi mj mk ml mm mn mo mp ij bi translated">为了在每次迭代后更新权重和偏差，我们需要计算梯度。在我们的两层网络中，有4个参数需要更新:W[2]，b[2]，W[1]和b[1]，因此需要计算4个梯度:</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div class="gh gi nj"><img src="../Images/0da37ea1e30685fdb838e01ec668a63e.png" data-original-src="https://miro.medium.com/v2/resize:fit:884/format:webp/1*D7GkHwrJZVaOk7Z1HWrvjw.png"/></div></figure><p id="44ca" class="pw-post-body-paragraph lu lv iq lw b lx mq jr lz ma mr ju mc md ms mf mg mh mt mj mk ml mu mn mo mp ij bi translated">但是我们如何找到这些复合函数的导数呢？根据'<strong class="lw ir">链式法则</strong>'，我们将沿着连接变量的所有路径构建导数的乘积。让我们遵循第二层(W[2])梯度的权重:</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="gh gi nk"><img src="../Images/79b37c2271962f3d831917580a5cfb58.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fHBovXUdLMNPkslyteGMag.png"/></div></div><p class="ky kz gj gh gi la lb bd b be z dk translated">图5。W[2]渐变|作者图片</p></figure><p id="2f97" class="pw-post-body-paragraph lu lv iq lw b lx mq jr lz ma mr ju mc md ms mf mg mh mt mj mk ml mu mn mo mp ij bi translated">从上图中，我们可以清楚地看到，成本J相对于W[2]的变化为:</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div class="gh gi nl"><img src="../Images/fff68a3c208c839e89f3723e79727f31.png" data-original-src="https://miro.medium.com/v2/resize:fit:654/format:webp/1*9kum11XITdmLPg1VpsTF-g.png"/></div><p class="ky kz gj gh gi la lb bd b be z dk translated">方程式7。成本J相对于第二层W[2]的权重的梯度</p></figure><p id="5921" class="pw-post-body-paragraph lu lv iq lw b lx mq jr lz ma mr ju mc md ms mf mg mh mt mj mk ml mu mn mo mp ij bi translated">为了解决这个问题，我们将从计算成本J对A[2]的偏导数开始，这也是预测Y-hat。原始成本函数显示在左侧，导数显示在右侧:</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div class="gh gi nm"><img src="../Images/0e64dede328c507e422678884e65afd5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1298/format:webp/1*xsaDHR3djze2tPXjzVSLxA.png"/></div><p class="ky kz gj gh gi la lb bd b be z dk translated">等式8。MSE成本函数(左)及其对A[2]的偏导数(右)。</p></figure><p id="0cb4" class="pw-post-body-paragraph lu lv iq lw b lx mq jr lz ma mr ju mc md ms mf mg mh mt mj mk ml mu mn mo mp ij bi translated">A[2]中的sigmoid激活相对于Z[2]的偏导数由下式表示(sigmoid导数的数学发展在参考文献[ <a class="ae ne" href="https://becominghuman.ai/what-is-derivative-of-sigmoid-function-56525895f0eb" rel="noopener ugc nofollow" target="_blank"> 3 </a> ]中描述):</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/d29ba8a3ac6a3a8fda2ee16a07e1f341.png" data-original-src="https://miro.medium.com/v2/resize:fit:1308/format:webp/1*MB0_hh-Wafpi7HF8wEEnzw.png"/></div><p class="ky kz gj gh gi la lb bd b be z dk translated">等式9。第二层上的Sigmoid激活(左)及其对Z[2]的偏导数(右)。</p></figure><p id="c367" class="pw-post-body-paragraph lu lv iq lw b lx mq jr lz ma mr ju mc md ms mf mg mh mt mj mk ml mu mn mo mp ij bi translated">Z[2]相对于权重W[2]的偏导数:</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div class="gh gi no"><img src="../Images/b42ec07aa8298f15e3e01550ecb87c20.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*99Yfs8rflHDp43xVgNZghw.png"/></div><p class="ky kz gj gh gi la lb bd b be z dk translated">方程式10。第二层的直线方程(左)，它对W[2]的偏导数(右)。</p></figure><p id="b8da" class="pw-post-body-paragraph lu lv iq lw b lx mq jr lz ma mr ju mc md ms mf mg mh mt mj mk ml mu mn mo mp ij bi translated">让我们将所有内容链接在一起，计算W[2]梯度:</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="gh gi np"><img src="../Images/279a05d8699163642cc94b623f237798.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OxOr1HD4zTGlLcDZNgKI0A.png"/></div></div><p class="ky kz gj gh gi la lb bd b be z dk translated">方程式11。成本J相对于第二层的权重的梯度(W[2])</p></figure><p id="1d45" class="pw-post-body-paragraph lu lv iq lw b lx mq jr lz ma mr ju mc md ms mf mg mh mt mj mk ml mu mn mo mp ij bi translated">太好了！接下来，我们将以类似的方式计算b[1]梯度。让我们跟随梯度:</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="gh gi nk"><img src="../Images/7e1db09e60e2ffb7f8b4e6b0e96588c2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ty2OeNbDr8yfX_Gn-iUN9A.png"/></div></div><p class="ky kz gj gh gi la lb bd b be z dk translated">图6。成本J相对于第二层b[2]的偏差的梯度|作者的图像</p></figure><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/cebd321e6ea5f030b938ec47ccfd633e.png" data-original-src="https://miro.medium.com/v2/resize:fit:604/format:webp/1*a_rX0KfQG0wmeFoejJKCKQ.png"/></div><p class="ky kz gj gh gi la lb bd b be z dk translated">方程式12。成本J相对于第二层(b[2])的偏差的梯度</p></figure><p id="72d2" class="pw-post-body-paragraph lu lv iq lw b lx mq jr lz ma mr ju mc md ms mf mg mh mt mj mk ml mu mn mo mp ij bi translated">上面已经计算了b[2]梯度的前两部分(成本J相对于Z[2]的偏导数)，最后一部分等于1:</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/8d78ea15a8338fd9d9171903bd221c17.png" data-original-src="https://miro.medium.com/v2/resize:fit:1206/format:webp/1*KI4u6siJqY5I6YI6MMpArw.png"/></div><p class="ky kz gj gh gi la lb bd b be z dk translated">方程式13。第二层的直线方程(左)，它是关于b[2]的偏导数(右)。</p></figure><p id="1dc0" class="pw-post-body-paragraph lu lv iq lw b lx mq jr lz ma mr ju mc md ms mf mg mh mt mj mk ml mu mn mo mp ij bi translated">所以总的b[2]梯度是:</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="gh gi ns"><img src="../Images/b71903a27fe305871dd1343cf0a77339.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AkiueU2ADTib5NPAJZJZag.png"/></div></div><p class="ky kz gj gh gi la lb bd b be z dk translated">方程式14。成本J相对于第二层W[2]的权重的梯度</p></figure><p id="5b23" class="pw-post-body-paragraph lu lv iq lw b lx mq jr lz ma mr ju mc md ms mf mg mh mt mj mk ml mu mn mo mp ij bi translated">最后，我们完成了第二层的梯度计算。第一层的梯度有点长，但我们已经计算了它的一部分。让我们遵循W[1]更新的梯度:</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="gh gi nk"><img src="../Images/f079c64b1f9918c589a18c3423a0da15.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sqsFvarf5uXjydd2ixyEDQ.png"/></div></div><p class="ky kz gj gh gi la lb bd b be z dk translated">图7。W[1]渐变|作者图片</p></figure><p id="6c85" class="pw-post-body-paragraph lu lv iq lw b lx mq jr lz ma mr ju mc md ms mf mg mh mt mj mk ml mu mn mo mp ij bi translated">梯度的前两部分是先前为层2计算的。Z[2]关于A[1]的偏导数是W[2]:</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/ce7955cec2bf92bea13fcd577dd35118.png" data-original-src="https://miro.medium.com/v2/resize:fit:1228/format:webp/1*nZv8CDAHVv3LuXIWkV1Q3w.png"/></div><p class="ky kz gj gh gi la lb bd b be z dk translated">方程式15。第二层的直线方程(左)，它对A[1]的偏导数(右)。</p></figure><p id="0626" class="pw-post-body-paragraph lu lv iq lw b lx mq jr lz ma mr ju mc md ms mf mg mh mt mj mk ml mu mn mo mp ij bi translated">最后两部分的计算方式与第2层中的计算方式相同。综合起来，我们得到:</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="gh gi nu"><img src="../Images/bf1d7bfeaf3bbbf256c54067ab5f63b7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xprorNZvirLsg-13_l0zcQ.png"/></div></div><p class="ky kz gj gh gi la lb bd b be z dk translated">方程式16。成本J相对于第一层W[1]的权重的梯度</p></figure><p id="fb33" class="pw-post-body-paragraph lu lv iq lw b lx mq jr lz ma mr ju mc md ms mf mg mh mt mj mk ml mu mn mo mp ij bi translated">如果我们遵循b[1]梯度:</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="gh gi nk"><img src="../Images/6ecc3f09647c24da50270bbb61281826.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jHA6VvCU7A7YKDggqNGfGQ.png"/></div></div><p class="ky kz gj gh gi la lb bd b be z dk translated">图8。b[1]渐变|作者图片</p></figure><p id="71e1" class="pw-post-body-paragraph lu lv iq lw b lx mq jr lz ma mr ju mc md ms mf mg mh mt mj mk ml mu mn mo mp ij bi translated">我们会得到:</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="gh gi nv"><img src="../Images/04470e135d1b9f787cd879cb28f6e84a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*R8YhHvIoDf7v_wEurT9ZAQ.png"/></div></div><p class="ky kz gj gh gi la lb bd b be z dk translated">方程式17。成本J相对于第一层b[1]的偏差的梯度</p></figure><p id="20a3" class="pw-post-body-paragraph lu lv iq lw b lx mq jr lz ma mr ju mc md ms mf mg mh mt mj mk ml mu mn mo mp ij bi translated">通过这一部分，我们完成了对我们的神经网络的一次迭代的权重和偏差的所有梯度的计算。</p><h1 id="da8a" class="lc ld iq bd le lf lg lh li lj lk ll lm jw ln jx lo jz lp ka lq kc lr kd ls lt bi translated"><strong class="ak">体重更新</strong></h1><p id="280c" class="pw-post-body-paragraph lu lv iq lw b lx ly jr lz ma mb ju mc md me mf mg mh mi mj mk ml mm mn mo mp ij bi translated">一旦计算出梯度，我们就可以更新模型的新参数，并再次迭代直到模型收敛。表示α是学习速率，一个将决定收敛速率的超参数。</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div class="gh gi nw"><img src="../Images/38fa9eedf230124f6dd812b527984065.png" data-original-src="https://miro.medium.com/v2/resize:fit:1372/format:webp/1*EUCEVDMpnCNIHB27ySlsuQ.png"/></div><p class="ky kz gj gh gi la lb bd b be z dk translated">权重更新。W =权重，alpha =学习率，J =成本。层号用方括号表示。</p></figure><h1 id="3761" class="lc ld iq bd le lf lg lh li lj lk ll lm jw ln jx lo jz lp ka lq kc lr kd ls lt bi translated">最后的想法</h1><p id="6cd8" class="pw-post-body-paragraph lu lv iq lw b lx ly jr lz ma mb ju mc md me mf mg mh mi mj mk ml mm mn mo mp ij bi translated">我希望这篇文章有助于更深入地理解神经网络背后的数学。在本文中，我解释了小型网络的工作原理。然而，这些基本概念可以推广并适用于更深层次的神经网络。</p><h1 id="78fb" class="lc ld iq bd le lf lg lh li lj lk ll lm jw ln jx lo jz lp ka lq kc lr kd ls lt bi translated">感谢您的阅读！</h1><p id="2eda" class="pw-post-body-paragraph lu lv iq lw b lx ly jr lz ma mb ju mc md me mf mg mh mi mj mk ml mm mn mo mp ij bi translated"><strong class="lw ir">想了解更多信息？</strong></p><ul class=""><li id="f2db" class="nx ny iq lw b lx mq ma mr md nz mh oa ml ob mp oc od oe of bi translated"><a class="ae ne" href="https://medium.com/@lihigurarie" rel="noopener"> <strong class="lw ir">探索</strong> </a>我写的附加文章</li><li id="d318" class="nx ny iq lw b lx og ma oh md oi mh oj ml ok mp oc od oe of bi translated"><a class="ae ne" href="https://medium.com/@lihigurarie/subscribe" rel="noopener"> <strong class="lw ir">订阅</strong> </a> <strong class="lw ir"> </strong>在我发布文章时获得通知</li><li id="c8ed" class="nx ny iq lw b lx og ma oh md oi mh oj ml ok mp oc od oe of bi translated">关注我的<a class="ae ne" href="https://www.linkedin.com/in/lihi-gur-arie/" rel="noopener ugc nofollow" target="_blank"> <strong class="lw ir"> Linkedin </strong> </a></li></ul><p id="ff5c" class="pw-post-body-paragraph lu lv iq lw b lx mq jr lz ma mr ju mc md ms mf mg mh mt mj mk ml mu mn mo mp ij bi translated"><strong class="lw ir"> <em class="ol">参考文献</em> </strong></p><p id="5726" class="pw-post-body-paragraph lu lv iq lw b lx mq jr lz ma mr ju mc md ms mf mg mh mt mj mk ml mu mn mo mp ij bi translated">[1] <em class="ol">激活和成本函数。</em><a class="ae ne" href="https://medium.com/@zeeshanmulla/cost-activation-loss-function-neural-network-deep-learning-what-are-these-91167825a4de" rel="noopener">https://medium . com/@ zeeshanmulla/cost-activation-loss-function-neural-network-deep-learning-what-thes-91167825 a4 de</a></p><p id="e2d4" class="pw-post-body-paragraph lu lv iq lw b lx mq jr lz ma mr ju mc md ms mf mg mh mt mj mk ml mu mn mo mp ij bi translated">[2]随机梯度下降。<a class="ae ne" rel="noopener" target="_blank" href="/stochastic-gradient-descent-clearly-explained-53d239905d31">https://towards data science . com/random-gradient-descent-clearly-explained-53d 239905 d31</a></p><p id="1616" class="pw-post-body-paragraph lu lv iq lw b lx mq jr lz ma mr ju mc md ms mf mg mh mt mj mk ml mu mn mo mp ij bi translated">[3] <em class="ol">乙状结肠衍生物。</em><a class="ae ne" href="https://becominghuman.ai/what-is-derivative-of-sigmoid-function-56525895f0eb" rel="noopener ugc nofollow" target="_blank">https://becoming human . ai/what-is-derivative-of-sigmoid-function-56525895 f0eb</a></p></div></div>    
</body>
</html>