<html>
<head>
<title>Advanced Techniques for Fine-tuning Transformers</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">微调变压器的先进技术</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/advanced-techniques-for-fine-tuning-transformers-82e4e61e16e?source=collection_archive---------0-----------------------#2021-09-17">https://towardsdatascience.com/advanced-techniques-for-fine-tuning-transformers-82e4e61e16e?source=collection_archive---------0-----------------------#2021-09-17</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="ef0d" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsdatascience.com/tagged/tips-and-tricks" rel="noopener" target="_blank">提示和技巧</a></h2><div class=""/><div class=""><h2 id="960d" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">学习这些先进的技术，看看它们如何帮助改善结果</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/bde6f23f2c88c2b9a9d1c28ba3913f2b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SPndw4T5KOIihqxD7yS4Iw.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">用Python代码生成的Transformer单词云。作者图片</p></figure><p id="153b" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi md translated"><span class="l me mf mg bm mh mi mj mk ml di"> T </span>变压器——大家好，我们又见面了。我们有个约会，不是吗，罗伯塔？</p><p id="48cb" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">如果你已经阅读并坚持了我之前关于<a class="ae mm" rel="noopener" target="_blank" href="/transformers-can-you-rate-the-complexity-of-reading-passages-17c76da3403"> <strong class="lj jd">变形金刚的帖子，你能评价阅读段落的复杂性吗？</strong> </a> <strong class="lj jd"> </strong>太棒了！这意味着你很可能已经熟悉了变压器微调或培训过程的基础。如果你没有看到这个帖子，你可以访问下面的链接。</p><div class="mn mo gp gr mp mq"><a rel="noopener follow" target="_blank" href="/transformers-can-you-rate-the-complexity-of-reading-passages-17c76da3403"><div class="mr ab fo"><div class="ms ab mt cl cj mu"><h2 class="bd jd gy z fp mv fr fs mw fu fw jc bi translated">变形金刚，你能评价阅读段落的复杂程度吗？</h2><div class="mx l"><h3 class="bd b gy z fp mv fr fs mw fu fw dk translated">用PyTorch微调RoBERTa以预测文本摘录的阅读难易程度</h3></div><div class="my l"><p class="bd b dl z fp mv fr fs mw fu fw dk translated">towardsdatascience.com</p></div></div><div class="mz l"><div class="na l nb nc nd mz ne lb mq"/></div></div></a></div><p id="0e13" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">你的模特怎么样了？它能取得相当好的结果吗？或者你的变压器模型遭受性能和不稳定性？如果是，根本原因通常很难诊断和确定。这类问题通常在大型模型和小型数据集上更为普遍。相关数据和下游任务的性质和特征也有一定影响。</p><p id="170c" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">如果您的变压器的性能没有达到您的预期，您能做什么？您可以尝试超参数调谐。此外，你也可以尝试使用一些高级的训练技巧，我将在这篇文章中介绍。这些技术可用于微调BERT、ALBERT、RoBERTa等变压器。</p><h1 id="4b70" class="nf ng it bd nh ni nj nk nl nm nn no np ki nq kj nr kl ns km nt ko nu kp nv nw bi translated">内容</h1><p id="cea0" class="pw-post-body-paragraph lh li it lj b lk nx kd lm ln ny kg lp lq nz ls lt lu oa lw lx ly ob ma mb mc im bi translated"><a class="ae mm" href="#6196" rel="noopener ugc nofollow"> 1。分层学习率衰减(LLRD) </a> <br/> <a class="ae mm" href="#ee6f" rel="noopener ugc nofollow"> 2。预热步骤</a> <br/> <a class="ae mm" href="#1fbc" rel="noopener ugc nofollow"> 3。重新初始化预训练层</a> <br/> <a class="ae mm" href="#12b3" rel="noopener ugc nofollow"> 4。</a> <br/> <a class="ae mm" href="#dea1" rel="noopener ugc nofollow"> 5随机加权平均法。频繁评估</a> <br/> <a class="ae mm" href="#811e" rel="noopener ugc nofollow">结果</a> <br/> <a class="ae mm" href="#7f5b" rel="noopener ugc nofollow">总结</a></p><blockquote class="oc od oe"><p id="ed40" class="lh li of lj b lk ll kd lm ln lo kg lp og lr ls lt oh lv lw lx oi lz ma mb mc im bi translated">对于我们在这篇文章中要做的所有高级微调技术，我们将使用我们从<a class="ae mm" rel="noopener" target="_blank" href="/transformers-can-you-rate-the-complexity-of-reading-passages-17c76da3403"> <strong class="lj jd">变形金刚中得到的相同模型和数据集，你能评价阅读段落的复杂性吗？</strong> </a></p><p id="5f97" class="lh li of lj b lk ll kd lm ln lo kg lp og lr ls lt oh lv lw lx oi lz ma mb mc im bi translated">最后，我们将能够将基本微调的结果与我们通过应用高级微调技术获得的结果进行相对比较。</p></blockquote></div><div class="ab cl oj ok hx ol" role="separator"><span class="om bw bk on oo op"/><span class="om bw bk on oo op"/><span class="om bw bk on oo"/></div><div class="im in io ip iq"><h1 id="6196" class="nf ng it bd nh ni oq nk nl nm or no np ki os kj nr kl ot km nt ko ou kp nv nw bi translated">1.分层学习率衰减(LLRD)</h1><p id="ae2a" class="pw-post-body-paragraph lh li it lj b lk nx kd lm ln ny kg lp lq nz ls lt lu oa lw lx ly ob ma mb mc im bi translated">在<a class="ae mm" href="https://arxiv.org/abs/2006.05987" rel="noopener ugc nofollow" target="_blank">重温少样本BERT微调</a>中，作者将<strong class="lj jd">逐层学习速率衰减</strong>描述为"<em class="of">一种对顶层应用较高学习速率、对底层应用较低学习速率的方法。这是通过设置顶层的学习率并使用乘法衰减率从上到下逐层降低学习率来实现的</em>”。</p><p id="69c0" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">在<a class="ae mm" href="https://arxiv.org/abs/1801.06146" rel="noopener ugc nofollow" target="_blank">用于文本分类的通用语言模型微调</a>中也表达了被称为<strong class="lj jd">区别性微调</strong>的类似概念。</p><blockquote class="ov"><p id="2a13" class="ow ox it bd oy oz pa pb pc pd pe mc dk translated">“区别性微调允许我们以不同的学习率调整每一层，而不是对模型的所有层使用相同的学习率”</p></blockquote><p id="481f" class="pw-post-body-paragraph lh li it lj b lk pf kd lm ln pg kg lp lq ph ls lt lu pi lw lx ly pj ma mb mc im bi translated">所有这些都是有意义的，因为Transformer模型中的不同层通常捕获不同种类的信息。底层通常编码更常见、通用和广泛的信息，而更接近输出的顶层编码更本地化和特定于手头任务的信息。</p><p id="6461" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">在我们开始实施之前，让我们快速回顾一下我们为<a class="ae mm" rel="noopener" target="_blank" href="/transformers-can-you-rate-the-complexity-of-reading-passages-17c76da3403"> <strong class="lj jd">变形金刚所做的基本微调，你能评价一下阅读段落的复杂性吗？</strong> </a></p><p id="cacc" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">在由一个嵌入层和12个隐藏层组成的<code class="fe pk pl pm pn b">roberta-base</code>模型上，我们使用了线性调度器，并在优化器中设置了初始学习速率<code class="fe pk pl pm pn b">1e-6</code>(即0.000001)。如图1所示，调度程序创建了一个学习率在训练步骤中从<code class="fe pk pl pm pn b">1e-6</code>线性下降到零的时间表。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi po"><img src="../Images/16d83d809f7b0995e54b6b2be2b2b2cf.png" data-original-src="https://miro.medium.com/v2/resize:fit:794/format:webp/1*uHYhA9imEyVhIOPJ48s1XA.png"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">图1:初始学习率为1e-6的线性时间表。作者图片</p></figure><p id="53d0" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">现在，要实现逐层学习速率衰减(或区别性微调)，有两种可能的方法。</p><p id="e265" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">第一种方法是遵循<a class="ae mm" href="https://arxiv.org/abs/2006.05987" rel="noopener ugc nofollow" target="_blank">中描述的方法，重新考虑少样本BERT微调</a>。我们为顶层选择学习速率<code class="fe pk pl pm pn b">3.5e-6</code>，并使用乘法衰减速率<code class="fe pk pl pm pn b">0.9</code>从上到下逐层降低学习速率。这将导致底层(<code class="fe pk pl pm pn b">embeddings</code>和<code class="fe pk pl pm pn b">layer0</code>)的学习率大致接近<code class="fe pk pl pm pn b">1e-6</code>。我们在一个名为<code class="fe pk pl pm pn b"><strong class="lj jd">roberta_base_AdamW_LLRD</strong></code>的函数中实现了这一点。</p><p id="3cd0" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">好了，我们已经设置了隐藏层的学习速率。<code class="fe pk pl pm pn b">pooler</code>和<code class="fe pk pl pm pn b">regressor</code>头怎么样？对于他们，我们选择<code class="fe pk pl pm pn b">3.6e-6</code>，比顶层略高的学习率。</p><p id="3ee8" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">在下面的代码中，<code class="fe pk pl pm pn b">head_params</code>、<code class="fe pk pl pm pn b">layer_params</code>和<code class="fe pk pl pm pn b">embed_params</code>是定义我们想要优化的参数、学习率和权重衰减的字典。所有这些参数组都被传递到由函数返回的<code class="fe pk pl pm pn b">AdamW</code>优化器中。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="pp pq l"/></div></figure><p id="7968" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">下面是具有逐层学习率衰减的线性时间表的样子:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pr"><img src="../Images/302fde5bbc80ff7242c56b2d8d3a84e9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*N7stA7ypytFYMAdpKG6HcQ.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">图2:具有逐层学习速率衰减的线性时间表。作者图片</p></figure><p id="2e73" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">实现逐层学习速率衰减(或区别性微调)的第二种方法是将层分成不同的组，并对每个组应用不同的学习速率。我们将此称为<strong class="lj jd">分组LLRD </strong>。</p><p id="b0d6" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">使用一个新的函数<code class="fe pk pl pm pn b"><strong class="lj jd">roberta_base_AdamW_grouped_LLRD</strong></code>，我们将<code class="fe pk pl pm pn b">roberta-base</code>模型的12个隐藏层分成3组，其中<code class="fe pk pl pm pn b">embeddings</code>附加到第一组。</p><ul class=""><li id="0c7e" class="ps pt it lj b lk ll ln lo lq pu lu pv ly pw mc px py pz qa bi translated">集合1:嵌入+层0，1，2，3(学习率:<code class="fe pk pl pm pn b">1e-6</code>)</li><li id="87b7" class="ps pt it lj b lk qb ln qc lq qd lu qe ly qf mc px py pz qa bi translated">集合2:第4、5、6、7层(学习率:<code class="fe pk pl pm pn b">1.75e-6</code>)</li><li id="5654" class="ps pt it lj b lk qb ln qc lq qd lu qe ly qf mc px py pz qa bi translated">集合3:第8、9、10、11层(学习率:<code class="fe pk pl pm pn b">3.5e-6</code>)</li></ul><p id="d9f2" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">与第一种方法相同，我们将<code class="fe pk pl pm pn b">3.6e-6</code>用于<code class="fe pk pl pm pn b">pooler</code>和<code class="fe pk pl pm pn b">regressor</code>头部，这是一个略高于顶层的学习速率。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="pp pq l"/></div></figure><p id="d7bb" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">下面是分组LLRD的线性时间表:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi qg"><img src="../Images/91944686de01aefa17fdc8e09d281e2d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9V8_w5nihnj1v-HDTR-NpA.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">图3:分组LLRD的线性时间表。作者图片</p></figure><h1 id="ee6f" class="nf ng it bd nh ni nj nk nl nm nn no np ki nq kj nr kl ns km nt ko nu kp nv nw bi translated">2.热身步骤</h1><p id="e3cb" class="pw-post-body-paragraph lh li it lj b lk nx kd lm ln ny kg lp lq nz ls lt lu oa lw lx ly ob ma mb mc im bi translated">对于我们使用的线性调度程序，我们可以应用预热步骤。例如，应用50个预热步骤意味着在前50个步骤(预热阶段)，学习率将从0线性增加到优化器中设置的初始学习率。之后，学习率将开始线性下降至0。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi qh"><img src="../Images/7e18c967b2b4834afd9a027d8bf892cf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0l1t8IcW9475gwhXuQ2RcQ.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">图4:带LLRD和50个热身步骤的线性时间表。作者图片</p></figure><p id="ea16" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">下图显示了在步骤50中各层的学习率。这些是我们为优化器设置的学习率。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi qi"><img src="../Images/562516a1f24c4ea722f6328260757616.png" data-original-src="https://miro.medium.com/v2/resize:fit:1324/format:webp/1*bmLzfbgf2GE_SYscQ9tCsg.png"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">图5:悬停文本反映了第50步的学习率。作者图片</p></figure><p id="c1f6" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">要应用预热步骤，在<code class="fe pk pl pm pn b">get_scheduler</code>功能上输入参数<code class="fe pk pl pm pn b">num_warmup_steps</code>。</p><pre class="ks kt ku kv gt qj pn qk ql aw qm bi"><span id="a04e" class="qn ng it pn b gy qo qp l qq qr">scheduler = transformers.get_scheduler(<br/>                "linear",    <br/>                optimizer = optimizer,<br/>                num_warmup_steps = 50,<br/>                num_training_steps = train_steps<br/>)</span></pre><p id="3e92" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">或者，你也可以使用<code class="fe pk pl pm pn b">get_linear_schedule_with_warmup</code>。</p><pre class="ks kt ku kv gt qj pn qk ql aw qm bi"><span id="96f2" class="qn ng it pn b gy qo qp l qq qr">scheduler = transformers.get_linear_schedule_with_warmup(                <br/>                optimizer = optimizer,<br/>                num_warmup_steps = 50,<br/>                num_training_steps = train_steps<br/>)</span></pre><h1 id="1fbc" class="nf ng it bd nh ni nj nk nl nm nn no np ki nq kj nr kl ns km nt ko nu kp nv nw bi translated">3.重新初始化预训练层</h1><p id="d0e4" class="pw-post-body-paragraph lh li it lj b lk nx kd lm ln ny kg lp lq nz ls lt lu oa lw lx ly ob ma mb mc im bi translated">微调变压器是一件轻而易举的事，因为我们使用预先训练的模型。这意味着我们不是从零开始训练，这会占用大量的资源和时间。这些模型通常已经在大型文本数据语料库上进行了预训练，并且它们包含我们可以使用的预训练权重。然而，为了获得更好的微调结果，有时我们需要在微调过程中丢弃一些权重并重新初始化它们。</p><p id="0f9f" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">那么我们如何做到这一点呢？前面，我们讨论了转换器的不同层捕获不同类型的信息。底层通常编码更一般的信息。这些是有用的，所以我们想保留这些低级的表示。我们想要刷新的是更接近输出的顶层。它们是对预训练任务更具体的信息进行编码的层，现在我们希望它们适应我们的。</p><p id="7383" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">我们可以在之前创建的<code class="fe pk pl pm pn b">MyModel</code>类中这样做。初始化模型时，我们传入一个参数，指定要重新初始化的顶部<code class="fe pk pl pm pn b">n</code>层。你可能会问，为什么是<code class="fe pk pl pm pn b">n</code>？事实证明，为<code class="fe pk pl pm pn b">n</code>选择一个最佳值是至关重要的，可以导致更快的收敛。也就是说，有多少顶层要重新初始化？这要视情况而定，因为每个模型和数据集都是不同的。在我们的例子中，<code class="fe pk pl pm pn b">n</code>的最佳值是5。如果重新初始化超过最佳点的更多层，您可能会开始经历恶化的结果。</p><p id="a404" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">在下面的代码中，我们使用模型的<code class="fe pk pl pm pn b">initializer_range</code>定义的平均值0和标准偏差重新初始化<code class="fe pk pl pm pn b">nn.Linear</code>模块的权重，并使用值1重新初始化<code class="fe pk pl pm pn b">nn.LayerNorm</code>模块的权重。偏差被重新初始化为值0。</p><p id="1977" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">如代码所示，我们还重新初始化了<code class="fe pk pl pm pn b">pooler</code>层。如果你没有在你的模型中使用<code class="fe pk pl pm pn b">pooler</code>，你可以省略<code class="fe pk pl pm pn b">_do_reinit</code>中与它相关的部分。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="pp pq l"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">重新初始化层的代码改编自<a class="ae mm" href="https://www.kaggle.com/rhtsingh/on-stability-of-few-sample-transformer-fine-tuning?scriptVersionId=67176591&amp;cellId=9" rel="noopener ugc nofollow" target="_blank">这里的</a>。</p></figure><h1 id="12b3" class="nf ng it bd nh ni nj nk nl nm nn no np ki nq kj nr kl ns km nt ko nu kp nv nw bi translated">4.随机加权平均法</h1><p id="74ab" class="pw-post-body-paragraph lh li it lj b lk nx kd lm ln ny kg lp lq nz ls lt lu oa lw lx ly ob ma mb mc im bi translated"><strong class="lj jd">随机权重平均(SWA) </strong>是<a class="ae mm" href="https://arxiv.org/abs/1803.05407" rel="noopener ugc nofollow" target="_blank">中提出的深度神经网络训练技术，平均权重导致更宽的最优值和更好的泛化能力</a>。据作者称，</p><blockquote class="ov"><p id="d1d9" class="ow ox it bd oy oz pa pb pc pd pe mc dk translated">“SWA非常容易实现，与传统的训练方案相比，几乎没有计算开销”</p></blockquote><p id="099c" class="pw-post-body-paragraph lh li it lj b lk pf kd lm ln pg kg lp lq ph ls lt lu pi lw lx ly pj ma mb mc im bi translated">那么，SWA是如何工作的呢？如PyTorch博客<a class="ae mm" href="https://pytorch.org/blog/pytorch-1.6-now-includes-stochastic-weight-averaging/" rel="noopener ugc nofollow" target="_blank">所述，SWA由两部分组成:</a></p><ul class=""><li id="4683" class="ps pt it lj b lk ll ln lo lq pu lu pv ly pw mc px py pz qa bi translated">首先，它使用一个<strong class="lj jd">修改的学习率</strong>时间表。例如，我们可以在训练时间的前75%使用标准的衰减学习率策略(例如我们正在使用的线性时间表),然后在剩余的25%时间将学习率设置为一个相当高的恒定值。</li><li id="376b" class="ps pt it lj b lk qb ln qc lq qd lu qe ly qf mc px py pz qa bi translated">第二，它对所遍历的网络的权重进行<strong class="lj jd">平均。例如，在最后25%的训练时间内，我们可以保持最后获得的重量的连续平均值。训练完成后，我们将网络的权重设置为计算出的SWA平均值。</strong></li></ul><blockquote class="oc od oe"><p id="84c0" class="lh li of lj b lk ll kd lm ln lo kg lp og lr ls lt oh lv lw lx oi lz ma mb mc im bi translated"><strong class="lj jd">py torch中如何使用SWA？</strong></p><p id="6daa" class="lh li of lj b lk ll kd lm ln lo kg lp og lr ls lt oh lv lw lx oi lz ma mb mc im bi translated">在<code class="fe pk pl pm pn b">torch.optim.swa_utils</code>中，我们实现了所有的SWA成分，以方便在任何模型中使用SWA。</p><p id="9de5" class="lh li of lj b lk ll kd lm ln lo kg lp og lr ls lt oh lv lw lx oi lz ma mb mc im bi translated">特别地，我们实现了用于SWA模型的<code class="fe pk pl pm pn b">AveragedModel</code>类、<code class="fe pk pl pm pn b">SWALR</code>学习速率调度器和<code class="fe pk pl pm pn b">update_bn</code>实用函数，以在训练结束时更新SWA批量标准化统计。</p><p id="19c9" class="lh li of lj b lk ll kd lm ln lo kg lp og lr ls lt oh lv lw lx oi lz ma mb mc im bi translated">来源:<a class="ae mm" href="https://pytorch.org/blog/pytorch-1.6-now-includes-stochastic-weight-averaging/" rel="noopener ugc nofollow" target="_blank"> PyTorch博客</a></p></blockquote><p id="9bdf" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">SWA很容易在PyTorch中实现。您可以参考下面来自<a class="ae mm" href="https://pytorch.org/docs/stable/optim.html#stochastic-weight-averaging" rel="noopener ugc nofollow" target="_blank"> PyTorch文档</a>的示例代码来实现SWA。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi qs"><img src="../Images/d43dbdfb83a80ae1c6e497bce891743b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1380/format:webp/1*ShWebh7PWjKlnFH93MzQWQ.png"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">用于实施SWA的示例代码来自<a class="ae mm" href="https://pytorch.org/docs/stable/optim.html#stochastic-weight-averaging" rel="noopener ugc nofollow" target="_blank"> PyTorch文档</a></p></figure><p id="a1af" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">为了在我们的<code class="fe pk pl pm pn b">run_training</code>函数中实现SWA，我们为<code class="fe pk pl pm pn b">swa_lr</code>接受一个参数。该参数是设定为常数值的SWA学习率。在我们的例子中，我们将使用<code class="fe pk pl pm pn b">2e-6</code>来表示<code class="fe pk pl pm pn b">swa_lr</code>。</p><p id="9dd6" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">因为我们想切换到SWA学习率计划，并开始收集时段3的参数的SWA平均值，我们为<code class="fe pk pl pm pn b">swa_start</code>指定3。</p><p id="b0b8" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">对于每个文件夹，我们初始化<code class="fe pk pl pm pn b">swa_model</code>和<code class="fe pk pl pm pn b">swa_scheduler</code>以及数据加载器、模型、优化器和调度器。<code class="fe pk pl pm pn b">swa_model</code>是累计权重平均值的SWA模型。</p><p id="f533" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">接下来，我们遍历这些时期，调用<code class="fe pk pl pm pn b">train_fn</code>并传递给它<code class="fe pk pl pm pn b">swa_model</code>、<code class="fe pk pl pm pn b">swa_scheduler</code>和一个布尔指示器<code class="fe pk pl pm pn b">swa_step</code>。这是一个指示符，告诉程序在时段3切换到<code class="fe pk pl pm pn b">swa_scheduler</code>。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="pp pq l"/></div></figure><p id="8282" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">在<code class="fe pk pl pm pn b">train_fn</code>中，从<code class="fe pk pl pm pn b">run_training</code>函数传入的参数<code class="fe pk pl pm pn b">swa_step</code>控制切换到<code class="fe pk pl pm pn b">SWALR</code>以及平均模型<code class="fe pk pl pm pn b">swa_model</code>的参数更新。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="pp pq l"/></div></figure><p id="9cde" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">SWA的优点是我们可以将它用于任何优化器和大多数调度器。在LLRD的线性时间表中，我们可以从图6中看到，在时段3切换到SWA学习速率时间表后，学习速率如何在<code class="fe pk pl pm pn b">2e-6</code>保持不变。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi qt"><img src="../Images/03c1cf96cd63987d2fa7a76b034cbc8c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PlfJI0oJmCHkjGzB8Ui2cQ.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">图6:带LLRD、50个热身步和SWA的线性时间表。作者图片</p></figure><p id="8c0a" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">下面是在分组LLRD上实施SWA后，线性时间表的样子，有50个热身步骤:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi qu"><img src="../Images/4608ec8a4662c5fa06485395226a0931.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0ftU3Qa7FB0v6pmHACp7fg.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">图7:具有分组LLRD、50个热身步骤和SWA的线性时间表。作者图片</p></figure><p id="ff92" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">你可以在这个<a class="ae mm" href="https://pytorch.org/blog/pytorch-1.6-now-includes-stochastic-weight-averaging/" rel="noopener ugc nofollow" target="_blank"> PyTorch博客</a>和这个<a class="ae mm" href="https://pytorch.org/docs/stable/optim.html#stochastic-weight-averaging" rel="noopener ugc nofollow" target="_blank"> PyTorch文档</a>上阅读更多关于西南铝的细节。</p><h1 id="dea1" class="nf ng it bd nh ni nj nk nl nm nn no np ki nq kj nr kl ns km nt ko nu kp nv nw bi translated">5.频繁评估</h1><p id="803a" class="pw-post-body-paragraph lh li it lj b lk nx kd lm ln ny kg lp lq nz ls lt lu oa lw lx ly ob ma mb mc im bi translated">频繁评估是另一种值得探索的技术。它简单的意思是，我们将对该时期内的每<code class="fe pk pl pm pn b">x</code>批训练数据执行验证，而不是在每个时期验证一次。这需要在我们的代码中做一点结构上的改变，因为目前训练和验证功能是分开的，每个时期都调用一次。</p><p id="c04c" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">我们要做的是创建一个新函数，<code class="fe pk pl pm pn b">train_and_validate</code>。对于每个时期，<code class="fe pk pl pm pn b">run_training</code>将调用这个新函数，而不是分别调用<code class="fe pk pl pm pn b">train_fn</code>和<code class="fe pk pl pm pn b">validate_fn</code>。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi qv"><img src="../Images/0726e23cc0129aa9b3590363d1264197.png" data-original-src="https://miro.medium.com/v2/resize:fit:1292/format:webp/1*dJ-j-TvzZoYtNg5p3twckw.png"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">图8:作者图片</p></figure><p id="9e52" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">在<code class="fe pk pl pm pn b">train_and_validate</code>内部，对于每一批训练数据，都会运行模型训练代码。然而，为了验证，只会在每<code class="fe pk pl pm pn b">x</code>批训练数据上调用<code class="fe pk pl pm pn b">validate_fn</code>。因此，如果<code class="fe pk pl pm pn b">x</code>是10，并且如果我们有50批训练数据，那么每个时期将进行5次验证。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi qw"><img src="../Images/c44caf7b1ba9d13c1cbc05b2f8dbd211.png" data-original-src="https://miro.medium.com/v2/resize:fit:1044/format:webp/1*6j1m15ifBceXPoedLnbYKg.png"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">图9:作者图片</p></figure><h1 id="811e" class="nf ng it bd nh ni nj nk nl nm nn no np ki nq kj nr kl ns km nt ko nu kp nv nw bi translated">结果</h1><p id="003f" class="pw-post-body-paragraph lh li it lj b lk nx kd lm ln ny kg lp lq nz ls lt lu oa lw lx ly ob ma mb mc im bi translated">好了，激动人心的部分来了…结果！</p><p id="6aa9" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">这些技术能对结果有如此大的改善，这是相当令人惊讶的。结果显示在下表中。</p><p id="044f" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">平均RMSE分数从0.589到0.5199，在应用了这篇文章中提到的所有先进技术后，有了基本的微调。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="pp pq l"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">表1:微调技术的结果</p></figure><h1 id="7f5b" class="nf ng it bd nh ni nj nk nl nm nn no np ki nq kj nr kl ns km nt ko nu kp nv nw bi translated">摘要</h1><p id="90d6" class="pw-post-body-paragraph lh li it lj b lk nx kd lm ln ny kg lp lq nz ls lt lu oa lw lx ly ob ma mb mc im bi translated">在本帖中，我们介绍了用于微调变压器的各种技术。</p><p id="ea11" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">☑️首先，我们使用<strong class="lj jd">逐层学习率衰减(LLRD) </strong>。LLRD背后的主要思想是将不同的学习率应用于变压器的每一层，或者在<strong class="lj jd">分组LLRD </strong>的情况下应用于层的分组。具体来说，顶层应该比底层具有更高的学习速率。</p><p id="01e2" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">☑️接下来，我们将<strong class="lj jd">热身步骤</strong>应用到学习率计划中。对于线性时间表中的预热步骤，学习率从0线性增加到预热阶段优化器中设置的初始学习率，之后开始线性减少到0。</p><p id="dbcb" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">☑️:我们还为变压器的顶层<code class="fe pk pl pm pn b">n</code>执行了<strong class="lj jd">重新初始化</strong>。为<code class="fe pk pl pm pn b">n</code>选择一个最佳值是至关重要的，因为如果重新初始化超过最佳点的更多层，您可能会开始经历恶化的结果。</p><p id="1cba" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">☑️:然后我们应用了<strong class="lj jd">随机加权平均(SWA) </strong>，这是一种深度神经网络训练技术，它使用了一种修改的学习速率表。在训练时间的最后一段，它还保持了在结束时获得的重量的连续平均值。</p><p id="694b" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">☑️最后但同样重要的是，我们在变压器微调过程中引入了<strong class="lj jd">频繁评估</strong>。我们不是在每个时期验证一次，而是在该时期内对每<code class="fe pk pl pm pn b">x</code>批训练数据进行验证。</p><p id="c401" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">使用所有这些技术，我们看到结果有了很大的改善，如表1所示。</p></div><div class="ab cl oj ok hx ol" role="separator"><span class="om bw bk on oo op"/><span class="om bw bk on oo op"/><span class="om bw bk on oo"/></div><div class="im in io ip iq"><p id="4c3d" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><em class="of">如果你喜欢我的帖子，别忘了点击</em> <a class="ae mm" href="https://peggy1502.medium.com/" rel="noopener"> <strong class="lj jd"> <em class="of">关注</em> </strong> </a> <em class="of">和</em> <a class="ae mm" href="https://peggy1502.medium.com/subscribe" rel="noopener"> <strong class="lj jd"> <em class="of">订阅</em> </strong> </a> <em class="of">获取邮件通知。</em></p><p id="79ea" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><em class="of">可选地，您也可以</em> <a class="ae mm" href="https://peggy1502.medium.com/membership" rel="noopener"> <em class="of">注册</em> </a> <em class="of">成为媒体会员，以获得媒体上每个故事的全部访问权限。</em></p><p id="4ceb" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">📑<em class="of">访问此</em><a class="ae mm" href="https://github.com/peggy1502/Data-Science-Articles/blob/main/README.md" rel="noopener ugc nofollow" target="_blank"><em class="of">GitHub repo</em></a><em class="of">获取我在帖子中分享的所有代码和笔记本。</em></p><p id="40d3" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">2021保留所有权利。</p></div><div class="ab cl oj ok hx ol" role="separator"><span class="om bw bk on oo op"/><span class="om bw bk on oo op"/><span class="om bw bk on oo"/></div><div class="im in io ip iq"><h1 id="549e" class="nf ng it bd nh ni oq nk nl nm or no np ki os kj nr kl ot km nt ko ou kp nv nw bi translated">参考</h1><p id="7163" class="pw-post-body-paragraph lh li it lj b lk nx kd lm ln ny kg lp lq nz ls lt lu oa lw lx ly ob ma mb mc im bi translated">[1] T. Zhang，F. Wu，A. Katiyar，K. Weinberger和Y. Artzi，<a class="ae mm" href="https://arxiv.org/abs/2006.05987" rel="noopener ugc nofollow" target="_blank">重温少样本BERT微调</a> (2021)</p><p id="f21d" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">[2] A .塔库尔，<a class="ae mm" href="https://www.amazon.com/dp/8269211508" rel="noopener ugc nofollow" target="_blank">接近(几乎)任何机器学习问题</a> (2020)</p><p id="f515" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">[3] C .孙，x .邱，y .徐，x .黄，<a class="ae mm" href="https://arxiv.org/abs/1905.05583" rel="noopener ugc nofollow" target="_blank">如何微调用于文本分类的BERT？</a> (2020)</p><p id="4b69" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">[4] P .伊兹迈洛夫、d .波多普里欣、t .加里波夫、d .维特罗夫和a .威尔逊，<a class="ae mm" href="https://arxiv.org/abs/1803.05407" rel="noopener ugc nofollow" target="_blank">平均权重导致更宽的最优值和更好的泛化</a> (2019)</p><p id="b3a2" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">[5] J. Howard和S. Ruder，<a class="ae mm" href="https://arxiv.org/abs/1801.06146" rel="noopener ugc nofollow" target="_blank">用于文本分类的通用语言模型微调</a> (2018)</p></div></div>    
</body>
</html>