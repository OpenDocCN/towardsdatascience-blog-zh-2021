<html>
<head>
<title>A Better Approach to Text Summarization</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">一种更好的文本摘要方法</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/a-better-approach-to-text-summarization-d7139b571439?source=collection_archive---------8-----------------------#2021-11-08">https://towardsdatascience.com/a-better-approach-to-text-summarization-d7139b571439?source=collection_archive---------8-----------------------#2021-11-08</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="2129" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">用Python完整实现的分步指南</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/6965ae145afd13c494b1add3c8e69ac6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nqGTNHxasaJZudVs_9WVAA.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图片由<a class="ae ky" href="https://pixabay.com/users/diannehope14-266432/?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=1106196" rel="noopener ugc nofollow" target="_blank"> Dianne Hope </a>发自<a class="ae ky" href="https://pixabay.com/?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=1106196" rel="noopener ugc nofollow" target="_blank"> Pixabay </a></p></figure><p id="7047" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">有几种方法来执行自动文本摘要。它可以通过<a class="ae ky" href="https://medium.com/programming-for-beginners/unsupervised-versus-supervised-machine-learning-1e55aeb3d2df" rel="noopener">监督或无监督学习</a>来完成，通过深度或只有机器学习。在这些类别中，有各种各样的方法。</p><p id="3a62" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">就摘要的类型而言，有两种——提取的和抽象的。这里介绍的程序使用无监督学习，并生成一个提取摘要。</p><p id="7b27" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">摘要是指摘要是原始文本的子集，因为摘要中的所有单词都包含在原始文本中。对于自动文本摘要领域的概述，我推荐这个<a class="ae ky" href="http://arxiv.org/abs/1707.02268v3" rel="noopener ugc nofollow" target="_blank">调查</a>。</p><h1 id="8d82" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">不同的词，但是相同的意思</h1><p id="3e2b" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">一般来说，在简单的解释中，摘要者的目标是找到最相关的单词，然后选择包含这些单词的句子作为摘要的一部分。</p><p id="1c5b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="ms">不同但意思相同的单词怎么办？</em></p><p id="747a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">举个例子，</p><ul class=""><li id="c660" class="mt mu it lb b lc ld lf lg li mv lm mw lq mx lu my mz na nb bi translated">哈里搬到了美国。</li><li id="89b9" class="mt mu it lb b lc nc lf nd li ne lm nf lq ng lu my mz na nb bi translated">王子移居北美。</li></ul><p id="ef68" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">他们没有共同语言，但这两个句子有很强的关联性<strong class="lb iu">。</strong>“该”和“到”不算。这些单词在预处理步骤中已经被删除了。下面将详细介绍。</p><h1 id="1394" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">问题</h1><p id="8f0a" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">在一种简单的方法中，“Harry”或“US”可以被选择为相关的单词，但是句子“The prince relocated to North America<em class="ms">”</em>将没有机会成为摘要的一部分。</p><p id="525b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这篇<a class="ae ky" href="http://www.aclweb.org/anthology/W17-1003" rel="noopener ugc nofollow" target="_blank">研究论文</a>中提出的技术解决了这个问题。</p><p id="2864" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu"> <em class="ms">本帖显示的代码只是稍加修改的版本(结构、方法名等。)的出处</em> </strong> <em class="ms"> </em> <strong class="lb iu"> <em class="ms">可用上</em></strong><a class="ae ky" href="https://github.com/gaetangate/text-summarizer" rel="noopener ugc nofollow" target="_blank"><strong class="lb iu"><em class="ms">Github</em></strong></a><em class="ms">。</em></p><h1 id="ee6b" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">算法步骤</h1><p id="bdc4" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">我在本教程中使用的文本与由<a class="nh ni ep" href="https://medium.com/u/ec3b4cbc2b59?source=post_page-----d7139b571439--------------------------------" rel="noopener" target="_blank"> Praveen Dubey </a>撰写的这篇优秀的<a class="ae ky" rel="noopener" target="_blank" href="/understand-text-summarization-and-create-your-own-summarizer-in-python-b26a9f09fc70">文章</a>中使用的文本相同。确保也阅读它(为了证明概念，我改变了其中一个句子)。</p><h2 id="baba" class="nj lw it bd lx nk nl dn mb nm nn dp mf li no np mh lm nq nr mj lq ns nt ml nu bi translated">1.生成嵌入模型</h2><p id="8c96" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">摘要器依赖于单词嵌入，因此它还选择包含与最相关单词(质心)具有相同含义的单词的句子，即使这些单词是不同的。</p><p id="b671" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对于<strong class="lb iu">更多关于单词嵌入</strong>的细节，请看这篇<a class="ae ky" rel="noopener" target="_blank" href="/a-beginners-guide-to-word-embedding-with-gensim-word2vec-model-5970fa56cc92">文章</a>作者<a class="nh ni ep" href="https://medium.com/u/19bb3e3825ba?source=post_page-----d7139b571439--------------------------------" rel="noopener" target="_blank">李智</a></p><p id="ef28" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">因此，在预处理文本之前，程序必须定义嵌入模型。我使用被总结的文本作为数据来获得嵌入模型。</p><p id="bb9f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">但它可能是维基百科的转储或其他文本。也已经有可用的模型可以使用。为了获得单词嵌入，我将文本分割成单词，并将其传递给Word2vec。</p><pre class="kj kk kl km gt nv nw nx ny aw nz bi"><span id="0ca2" class="nj lw it nw b gy oa ob l oc od">for sent in clean_sentences:<br/>    words.append(nlkt_word_tokenize(sent))<br/><strong class="nw iu">model =</strong> Word2Vec(words, min_count=1, sg = 1)</span></pre><h2 id="4b40" class="nj lw it bd lx nk nl dn mb nm nn dp mf li no np mh lm nq nr mj lq ns nt ml nu bi translated">2.预处理</h2><p id="5acc" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">您必须对文本进行预处理。它包括将文本拆分成句子，降低所有单词的大小写，删除停用词(是，安，等。)和标点符号等任务。</p><p id="1d78" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><a class="ae ky" href="https://betterprogramming.pub/how-to-prepare-text-data-for-natural-language-processing-nlp-97dadce77661" rel="noopener ugc nofollow" target="_blank">我在另一篇文章</a>中单独介绍了这一步。目标是不要浪费资源(计算能力、时间)来处理那些对提取语义和理解文本没有太大价值的事情。</p><p id="66cb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">特别是识别和分割文本成句子，这是至关重要的，因为稍后算法将评分并选择它们作为摘要的一部分。</p><pre class="kj kk kl km gt nv nw nx ny aw nz bi"><span id="4e88" class="nj lw it nw b gy oa ob l oc od"><strong class="nw iu">def sent_tokenize(text):</strong><br/>    sents = nlkt_sent_tokenize(text)<br/>    sents_filtered = []<br/>    for s in sents:<br/>        sents_filtered.append(s)<br/>    return sents_filtered</span><span id="5048" class="nj lw it nw b gy oe ob l oc od"><strong class="nw iu">def cleanup_sentences(text):</strong><br/>    stop_words = set(stopwords.words('english'))<br/>    sentences = sent_tokenize(text)<br/>    sentences_cleaned = []<br/>    for sent in sentences:<br/>        words = nlkt_word_tokenize(sent)<br/>        words = [w for w in words if w not in string.punctuation]<br/>        words = [w for w in words if not w.lower() in stop_words]<br/>        words = [w.lower() for w in words]<br/>        sentences_cleaned.append(" ".join(words))<br/>    return sentences_cleaned</span></pre><h2 id="e2e2" class="nj lw it bd lx nk nl dn mb nm nn dp mf li no np mh lm nq nr mj lq ns nt ml nu bi translated">3.查找最相关的单词嵌入表示</h2><p id="5ede" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">该算法使用<a class="ae ky" href="https://levelup.gitconnected.com/what-the-heck-is-tf-idf-69ead52c908b" rel="noopener ugc nofollow" target="_blank"> TF-IDF </a>来查找文本中最相关的单词。这些词是文章的重心。</p><p id="a19d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">找到单词质心后，程序对作为质心一部分的单词的向量求和，这个和就是质心的嵌入表示。</p><p id="f1c2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">假设最相关的词是:微软，程序，AI</p><p id="76cd" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">质心的嵌入表示(最相关项)=微软向量+程序向量+ AI向量。</p><p id="c986" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">提醒一下，嵌入模型将每个单词表示为一个向量。这就是为什么每个单词都有一个向量。</p><pre class="kj kk kl km gt nv nw nx ny aw nz bi"><span id="d528" class="nj lw it nw b gy oa ob l oc od"><strong class="nw iu">def build_embedding_representation(words, word_vectors, embedding_model):</strong><br/>    embedding_representation = np.zeros(embedding_model.vector_size, dtype="float32")<br/>    word_vectors_keys = set(word_vectors.keys())<br/>    count = 0<br/>    for w in words:<br/>        if w in word_vectors_keys:<br/>            embedding_representation = embedding_representation + word_vectors[w]<br/>            count += 1<br/>    if count != 0:<br/>       embedding_representation = np.divide(embedding_representation, count)<br/>    return embedding_representation</span><span id="5f77" class="nj lw it nw b gy oe ob l oc od"><strong class="nw iu">def get_tf_idf(sentences):</strong><br/>    vectorizer = CountVectorizer()<br/>    sent_word_matrix = vectorizer.fit_transform(sentences)</span><span id="0de1" class="nj lw it nw b gy oe ob l oc od">transformer = TfidfTransformer(norm=None, sublinear_tf=False, smooth_idf=False)<br/>    tfidf = transformer.fit_transform(sent_word_matrix)<br/>    tfidf = tfidf.toarray()</span><span id="937c" class="nj lw it nw b gy oe ob l oc od">centroid_vector = tfidf.sum(0)<br/>    centroid_vector = np.divide(centroid_vector, centroid_vector.max())</span><span id="2cfe" class="nj lw it nw b gy oe ob l oc od">feature_names = vectorizer.get_feature_names()</span><span id="3c23" class="nj lw it nw b gy oe ob l oc od">relevant_vector_indices = np.where(centroid_vector &gt; 0.3)[0]</span><span id="bc4d" class="nj lw it nw b gy oe ob l oc od">word_list = list(np.array(feature_names)[relevant_vector_indices])<br/>    return word_list</span><span id="7970" class="nj lw it nw b gy oe ob l oc od"><strong class="nw iu">centroid_words</strong> = get_tf_idf(clean_sentences)</span><span id="b354" class="nj lw it nw b gy oe ob l oc od"><strong class="nw iu">centroid_vector</strong> = build_embedding_representation(centroid_words, word_vectors, emdedding_model)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi of"><img src="../Images/b89fed452f7652ba1cc1ef19e905157c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CDlvIppOhX26fMcpWD3NJw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">最相关的单词(质心)。图片作者。</p></figure><h2 id="5bfc" class="nj lw it bd lx nk nl dn mb nm nn dp mf li no np mh lm nq nr mj lq ns nt ml nu bi translated">4.给句子打分</h2><p id="6642" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">基于句子与质心嵌入的相似程度对句子进行评分。</p><p id="1002" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为了与质心嵌入进行比较，该算法计算每个句子的嵌入表示。</p><p id="a27e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">句子嵌入=作为句子一部分的单词向量的总和。</p><p id="b043" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">举个例子，</p><pre class="kj kk kl km gt nv nw nx ny aw nz bi"><span id="4665" class="nj lw it nw b gy oa ob l oc od"><strong class="nw iu">Sentence1</strong> = word1, word2, word3<br/><strong class="nw iu">Sentence1 embedding</strong> = word1 vector + word2 vector + word3 vector</span></pre><p id="9a89" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">最后，在定义了句子嵌入之后，该算法使用<a class="ae ky" rel="noopener" target="_blank" href="/cosine-similarity-intuition-with-implementation-in-python-51eade2674f6"> <strong class="lb iu">余弦相似度</strong> </a>来计算质心和句子嵌入之间的相似度。</p><p id="cff0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">每个句子根据它们与质心的相似程度得到一个分数。</p><pre class="kj kk kl km gt nv nw nx ny aw nz bi"><span id="4a1b" class="nj lw it nw b gy oa ob l oc od"><strong class="nw iu">def build_embedding_representation(words, word_vectors, embedding_model):</strong><br/>    embedding_representation = np.zeros(embedding_model.vector_size, dtype="float32")<br/>    word_vectors_keys = set(word_vectors.keys())<br/>    count = 0<br/>    for w in words:<br/>        if w in word_vectors_keys:<br/>            embedding_representation = embedding_representation + word_vectors[w]<br/>            count += 1<br/>    if count != 0:<br/>       embedding_representation = np.divide(embedding_representation, count)<br/>    <strong class="nw iu">return embedding_representation</strong><br/></span><span id="9086" class="nj lw it nw b gy oe ob l oc od">sentences_scores = []<br/>for i in range(len(clean_sentences)):<br/>        scores = []<br/>        words = clean_sentences[i].split()</span><span id="faec" class="nj lw it nw b gy oe ob l oc od">        #Sentence embedding representation<br/>        sentence_vector = build_embedding_representation(words,    word_vectors, emdedding_model)</span><span id="7500" class="nj lw it nw b gy oe ob l oc od">        #Cosine similarity between sentence embedding and centroid embedding<br/>        score = similarity(sentence_vector, centroid_vector)<br/>        sentences_scores.append((i, raw_sentences[i], score, sentence_vector))</span><span id="eb59" class="nj lw it nw b gy oe ob l oc od">sentence_scores_sort = sorted(sentences_scores, key=lambda el: el[2], reverse=True)</span><span id="8052" class="nj lw it nw b gy oe ob l oc od">sentence_scores_sort = sorted(sentences_scores, key=lambda el: el[2], reverse=True)</span></pre><p id="7f9b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">句子得分</strong></p><p id="a6e8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">红色句子的形心部分没有单词。尽管如此，它还是取得了不错的成绩。高于另一个包含质心词的词。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi og"><img src="../Images/0a5d3b71f58975ad286ee498e4b75992.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mzSJiaBm3-YbSQtJk-Jkug.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">按分数排序的句子。图片作者。</p></figure><h2 id="1fd3" class="nj lw it bd lx nk nl dn mb nm nn dp mf li no np mh lm nq nr mj lq ns nt ml nu bi translated">5.选择句子并解决冗余</h2><p id="0462" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">这些句子是根据它们的分数选择的。选择的句子数量受到摘要应包含多少单词的限制(50个单词，100个单词，还是？).</p><p id="2e9f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">处理自动摘要的一个常见问题是处理冗余——摘要中包含太相似的句子。</p><p id="4105" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为了克服这一点，在选择句子时，你要将它们与已经在摘要中的句子进行比较。如果选择的句子与摘要中的句子太相似，你就不会把它添加到最终文本中。</p><p id="4773" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">该算法在进行比较时使用余弦相似度和一些预定义的阈值。</p><pre class="kj kk kl km gt nv nw nx ny aw nz bi"><span id="2881" class="nj lw it nw b gy oa ob l oc od">count = 0<br/>sentences_summary = []</span><span id="ab34" class="nj lw it nw b gy oe ob l oc od">#Handle redundancy<br/>for s in sentence_scores_sort:<br/>        if count &gt; 100:<br/>            break<br/>        include_flag = True<br/>        for ps in sentences_summary:<br/>            sim = similarity(s[3], ps[3])<br/>            if sim &gt; 0.95:<br/>                include_flag = False<br/>        if include_flag:<br/>            sentences_summary.append(s)<br/>            count += len(s[1].split())</span><span id="1d44" class="nj lw it nw b gy oe ob l oc od">sentences_summary = sorted(sentences_summary, key=lambda el: el[0], reverse=False)</span><span id="4f85" class="nj lw it nw b gy oe ob l oc od"><strong class="nw iu">summary =</strong> "\n".join([s[1] for s in sentences_summary])</span></pre><h1 id="a3ec" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">原始文本和生成的摘要</h1><p id="9d1f" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated"><strong class="lb iu">原文</strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oh"><img src="../Images/a8c98648201a81025c629a286e2ea127.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*emu8wQLy7tcjfI5QxGr2-w.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">原文。图片作者。</p></figure><p id="113c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">生成汇总</strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oi"><img src="../Images/a81442a7961aac621125f671ea8bbbbd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dzsjVERJGVXa56LcWuWzfA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">生成的摘要。图片作者。</p></figure><h1 id="6b9c" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">Python中的完整代码</h1><p id="1789" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">为了让我更好地理解并在本文中解释，代码被稍微修改/简化了。但是您可以查看研究论文作者的Github以获得完整和更准确的实现。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oj ok l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">原文在这里:<a class="ae ky" href="https://github.com/gaetangate/text-summarizer" rel="noopener ugc nofollow" target="_blank">https://github.com/gaetangate/text-summarizer</a>。我玩它只是为了更好地理解和稍微改变一下。</p></figure><h1 id="df60" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">最后的想法</h1><p id="1444" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">我发现这个项目真的很有趣。它比其他只依赖单词包(BOW)或TF-IDF的算法更有效。我用作主要参考的研究论文要详细得多。它针对其他算法评估生成的摘要，并涵盖多语言文档摘要。</p><p id="c212" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">接下来是评估。我计划再写一篇关于ROUGE的文章，ROUGE是一个常用于评估摘要好坏的工具。我还将深入研究基于监督机器学习的摘要器。以及深度学习。</p><p id="e31e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">感谢阅读。</p><h1 id="69ff" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">参考</h1><p id="eb73" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">[1] <a class="ae ky" href="https://arxiv.org/abs/1707.02268v3" rel="noopener ugc nofollow" target="_blank">文本摘要技术:简要综述</a>。作者:Mehdi Allahyari，Seyedamin Pouriyeh，Mehdi Assefi，Saeid Safaei，Elizabeth D. Trippe，Juan B. Gutierrez，Krys Kochut</p><p id="9c5d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">【2】<a class="ae ky" href="http://www.aclweb.org/anthology/W17-1003" rel="noopener ugc nofollow" target="_blank">通过单词嵌入的复合性进行基于质心的文本摘要</a>。作者:盖塔诺·罗西耶洛，皮耶保罗·巴西勒和乔瓦尼·塞梅罗|<strong class="lb iu">Github page:</strong><a class="ae ky" href="https://github.com/gaetangate/text-summarizer" rel="noopener ugc nofollow" target="_blank">https://github.com/gaetangate/text-summarizer</a></p></div></div>    
</body>
</html>