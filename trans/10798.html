<html>
<head>
<title>How To Change The Column Type in PySpark DataFrames</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如何更改PySpark数据框架中的列类型</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/change-column-type-pyspark-df-eecbe726fdbc?source=collection_archive---------7-----------------------#2021-10-19">https://towardsdatascience.com/change-column-type-pyspark-df-eecbe726fdbc?source=collection_archive---------7-----------------------#2021-10-19</a></blockquote><div><div class="fc ii ij ik il im"/><div class="in io ip iq ir"><div class=""/><div class=""><h2 id="226b" class="pw-subtitle-paragraph jr it iu bd b js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki dk translated">讨论如何转换PySpark数据帧中列的数据类型</h2></div><figure class="kk kl km kn gu ko gi gj paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gi gj kj"><img src="../Images/5d45e4d341c1e68e9120103248848572.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oKyfMtQvDoBwJXdwCmSMXw.jpeg"/></div></div><p class="kv kw gk gi gj kx ky bd b be z dk translated">Javier Allegue Barros 在<a class="ae kz" href="https://unsplash.com/s/photos/change?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><h2 id="f723" class="la lb iu bd lc ld le dn lf lg lh dp li lj lk ll lm ln lo lp lq lr ls lt lu lv bi translated">介绍</h2><p id="d265" class="pw-post-body-paragraph lw lx iu ly b lz ma jv mb mc md jy me lj mf mg mh ln mi mj mk lr ml mm mn mo in bi translated">PySpark中一个相当常见的操作是类型转换，当我们需要更改数据帧中特定列的数据类型时，通常需要进行类型转换。例如，这很常见(而且是一种不好的做法！)将日期时间存储为字符串，甚至将整数和双精度数存储为<code class="fe mp mq mr ms b">StringType</code>。</p><p id="26a7" class="pw-post-body-paragraph lw lx iu ly b lz mt jv mb mc mu jy me lj mv mg mh ln mw mj mk lr mx mm mn mo in bi translated">在今天的简短指南中，我们将探讨如何在PySpark中更改某些DataFrame列的列类型。具体来说，我们将讨论如何使用</p><ul class=""><li id="29d0" class="my mz iu ly b lz mt mc mu lj na ln nb lr nc mo nd ne nf ng bi translated"><code class="fe mp mq mr ms b">cast()</code>功能</li><li id="9861" class="my mz iu ly b lz nh mc ni lj nj ln nk lr nl mo nd ne nf ng bi translated"><code class="fe mp mq mr ms b">selectExpr()</code>功能</li><li id="65ad" class="my mz iu ly b lz nh mc ni lj nj ln nk lr nl mo nd ne nf ng bi translated">Spark SQL</li></ul></div><div class="ab cl nm nn hy no" role="separator"><span class="np bw bk nq nr ns"/><span class="np bw bk nq nr ns"/><span class="np bw bk nq nr"/></div><div class="in io ip iq ir"><p id="a9c2" class="pw-post-body-paragraph lw lx iu ly b lz mt jv mb mc mu jy me lj mv mg mh ln mw mj mk lr mx mm mn mo in bi translated">首先，让我们创建一个示例数据框架，我们将在本文中引用它来演示一些概念。</p><pre class="kk kl km kn gu nt ms nu nv aw nw bi"><span id="9e04" class="la lb iu ms b gz nx ny l nz oa">from pyspark.sql import SparkSession</span><span id="a403" class="la lb iu ms b gz ob ny l nz oa"># Create an instance of spark session<br/>spark_session = SparkSession.builder \<br/>    .master('local[1]') \<br/>    .appName('Example') \<br/>    .getOrCreate()</span><span id="1bdc" class="la lb iu ms b gz ob ny l nz oa">df = spark_session.createDataFrame(<br/>    [<br/>        (1, '10-01-2020', '1.0', '100'),<br/>        (2, '14-02-2021', '2.0', '200'),<br/>        (3, '15-06-2019', '3.0', '300'),<br/>        (4, '12-12-2020', '4.0', '400'),<br/>        (5, '01-09-2019', '5.0', '500'),<br/>    ],<br/>    ['colA', 'colB', 'colC', 'colD']<br/>)<br/><br/>df.show()<br/><em class="oc">+----+----------+----+----+<br/>|colA|      colB|colC|colD|<br/>+----+----------+----+----+<br/>|   1|10-01-2020| 1.0| 100|<br/>|   2|14-02-2021| 2.0| 200|<br/>|   3|15-06-2019| 3.0| 300|<br/>|   4|12-12-2020| 4.0| 400|<br/>|   5|01-09-2019| 5.0| 500|<br/>+----+----------+----+----+</em></span><span id="b6d2" class="la lb iu ms b gz ob ny l nz oa">df.printSchema()<br/><em class="oc">root<br/> |-- colA: long (nullable = true)<br/> |-- colB: string (nullable = true)<br/> |-- colC: string (nullable = true)<br/> |-- colD: string (nullable = true)</em></span></pre><p id="d9f4" class="pw-post-body-paragraph lw lx iu ly b lz mt jv mb mc mu jy me lj mv mg mh ln mw mj mk lr mx mm mn mo in bi translated">在接下来的章节中，我们将展示如何将<code class="fe mp mq mr ms b">colB</code>、<code class="fe mp mq mr ms b">colC</code>和<code class="fe mp mq mr ms b">colD</code>列的类型分别更改为<code class="fe mp mq mr ms b">DateType</code>、<code class="fe mp mq mr ms b">DoubleType</code>和<code class="fe mp mq mr ms b">IntegerType</code>。</p></div><div class="ab cl nm nn hy no" role="separator"><span class="np bw bk nq nr ns"/><span class="np bw bk nq nr ns"/><span class="np bw bk nq nr"/></div><div class="in io ip iq ir"><h2 id="6fa6" class="la lb iu bd lc ld le dn lf lg lh dp li lj lk ll lm ln lo lp lq lr ls lt lu lv bi translated">使用cast()函数</h2><p id="6988" class="pw-post-body-paragraph lw lx iu ly b lz ma jv mb mc md jy me lj mf mg mh ln mi mj mk lr ml mm mn mo in bi translated">转换数据类型的第一个选项是将输入列转换为指定数据类型的<code class="fe mp mq mr ms b"><a class="ae kz" href="https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.sql.Column.cast.html" rel="noopener ugc nofollow" target="_blank">pyspark.sql.Column.cast()</a></code>函数。</p><pre class="kk kl km kn gu nt ms nu nv aw nw bi"><span id="8b7a" class="la lb iu ms b gz nx ny l nz oa">from datetime import datetime<br/>from pyspark.sql.functions import col, udf<br/>from pyspark.sql.types import DoubleType, IntegerType, DateType</span><span id="ba61" class="la lb iu ms b gz ob ny l nz oa"><br/># UDF to process the date column<br/>func = udf(lambda x: datetime.strptime(x, '%d-%m-%Y'), DateType())</span><span id="c30b" class="la lb iu ms b gz ob ny l nz oa"><strong class="ms iv">df = df \<br/>    .withColumn('colB', func(col('colB'))) \<br/>    .withColumn('colC', col('colC').cast(DoubleType())) \<br/>    .withColumn('colD', col('colD').cast(IntegerType()))<br/></strong></span><span id="8183" class="la lb iu ms b gz ob ny l nz oa">df.show()<br/><em class="oc">+----+----------+----+----+<br/>|colA|      colB|colC|colD|<br/>+----+----------+----+----+<br/>|   1|2020-01-10| 1.0| 100|<br/>|   2|2021-02-14| 2.0| 200|<br/>|   3|2019-06-15| 3.0| 300|<br/>|   4|2020-12-12| 4.0| 400|<br/>|   5|2019-09-01| 5.0| 500|<br/>+----+----------+----+----+</em></span><span id="2cdd" class="la lb iu ms b gz ob ny l nz oa">df.printSchema()<br/><em class="oc">root<br/> |-- colA: long (nullable = true)<br/></em><strong class="ms iv"><em class="oc"> |-- colB: date (nullable = true)<br/> |-- colC: double (nullable = true)<br/> |-- colD: integer (nullable = true)</em></strong></span></pre><p id="158e" class="pw-post-body-paragraph lw lx iu ly b lz mt jv mb mc mu jy me lj mv mg mh ln mw mj mk lr mx mm mn mo in bi translated">注意，为了将字符串转换成<code class="fe mp mq mr ms b">DateType</code>，我们需要指定一个UDF来处理字符串日期的确切格式。</p></div><div class="ab cl nm nn hy no" role="separator"><span class="np bw bk nq nr ns"/><span class="np bw bk nq nr ns"/><span class="np bw bk nq nr"/></div><div class="in io ip iq ir"><h2 id="d7c6" class="la lb iu bd lc ld le dn lf lg lh dp li lj lk ll lm ln lo lp lq lr ls lt lu lv bi translated">使用selectExpr()函数</h2><p id="b712" class="pw-post-body-paragraph lw lx iu ly b lz ma jv mb mc md jy me lj mf mg mh ln mi mj mk lr ml mm mn mo in bi translated">或者，您可以通过指定相应的SQL表达式来使用<code class="fe mp mq mr ms b"><a class="ae kz" href="https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.sql.DataFrame.selectExpr.html" rel="noopener ugc nofollow" target="_blank">pyspark.sql.DataFrame.selectExpr</a></code>函数，该表达式可以转换所需列的数据类型，如下所示。</p><pre class="kk kl km kn gu nt ms nu nv aw nw bi"><span id="40d4" class="la lb iu ms b gz nx ny l nz oa"><strong class="ms iv">df = df.selectExpr(<br/>    'colA',<br/>    'to_date(colB, \'dd-MM-yyyy\') colB',<br/>    'cast(colC as double) colC',<br/>    'cast(colD as int) colD',<br/>)</strong></span><span id="8dd0" class="la lb iu ms b gz ob ny l nz oa">df.show()<br/><em class="oc">+----+----------+----+----+<br/>|colA|      colB|colC|colD|<br/>+----+----------+----+----+<br/>|   1|2020-01-10| 1.0| 100|<br/>|   2|2021-02-14| 2.0| 200|<br/>|   3|2019-06-15| 3.0| 300|<br/>|   4|2020-12-12| 4.0| 400|<br/>|   5|2019-09-01| 5.0| 500|<br/>+----+----------+----+----+</em></span><span id="d5ab" class="la lb iu ms b gz ob ny l nz oa">df.printSchema()<br/><em class="oc">root<br/> |-- colA: long (nullable = true)<br/></em><strong class="ms iv"><em class="oc"> |-- colB: date (nullable = true)<br/> |-- colC: double (nullable = true)<br/> |-- colD: integer (nullable = true)</em></strong></span></pre></div><div class="ab cl nm nn hy no" role="separator"><span class="np bw bk nq nr ns"/><span class="np bw bk nq nr ns"/><span class="np bw bk nq nr"/></div><div class="in io ip iq ir"><h2 id="d882" class="la lb iu bd lc ld le dn lf lg lh dp li lj lk ll lm ln lo lp lq lr ls lt lu lv bi translated">使用Spark SQL</h2><p id="2bab" class="pw-post-body-paragraph lw lx iu ly b lz ma jv mb mc md jy me lj mf mg mh ln mi mj mk lr ml mm mn mo in bi translated">最后，您甚至可以使用Spark SQL，以类似于我们使用<code class="fe mp mq mr ms b">selectExpr</code>函数的方式来转换所需的列。</p><pre class="kk kl km kn gu nt ms nu nv aw nw bi"><span id="7a73" class="la lb iu ms b gz nx ny l nz oa"># First we need to register the DF as a global temporary view<br/>df.createGlobalTempView("df")</span><span id="71e5" class="la lb iu ms b gz ob ny l nz oa"><strong class="ms iv">df = spark_session.sql(<br/>    """<br/>    SELECT <br/>        colA,<br/>        to_date(colB, 'dd-MM-yyyy') colB,<br/>        cast(colC as double) colC,<br/>        cast(colD as int) colD<br/>    FROM global_temp.df<br/>    """<br/>)</strong></span><span id="a400" class="la lb iu ms b gz ob ny l nz oa">df.show()<br/><em class="oc">+----+----------+----+----+<br/>|colA|      colB|colC|colD|<br/>+----+----------+----+----+<br/>|   1|2020-01-10| 1.0| 100|<br/>|   2|2021-02-14| 2.0| 200|<br/>|   3|2019-06-15| 3.0| 300|<br/>|   4|2020-12-12| 4.0| 400|<br/>|   5|2019-09-01| 5.0| 500|<br/>+----+----------+----+----+</em></span><span id="6bcb" class="la lb iu ms b gz ob ny l nz oa">df.printSchema()<br/><em class="oc">root<br/> |-- colA: long (nullable = true)<br/></em><strong class="ms iv"><em class="oc"> |-- colB: date (nullable = true)<br/> |-- colC: double (nullable = true)<br/> |-- colD: integer (nullable = true)</em></strong></span></pre></div><div class="ab cl nm nn hy no" role="separator"><span class="np bw bk nq nr ns"/><span class="np bw bk nq nr ns"/><span class="np bw bk nq nr"/></div><div class="in io ip iq ir"><h2 id="78f3" class="la lb iu bd lc ld le dn lf lg lh dp li lj lk ll lm ln lo lp lq lr ls lt lu lv bi translated">最后的想法</h2><p id="f311" class="pw-post-body-paragraph lw lx iu ly b lz ma jv mb mc md jy me lj mf mg mh ln mi mj mk lr ml mm mn mo in bi translated">在今天的简短指南中，我们讨论了在PySpark中更改DataFrame列类型的几种不同方法。具体来说，我们探索了如何将<code class="fe mp mq mr ms b">withColumn()</code>函数与<code class="fe mp mq mr ms b">cast()</code>结合使用，以及如何使用更多类似SQL的方法，如<code class="fe mp mq mr ms b">selectExpr()</code>或Spark SQL。</p></div><div class="ab cl nm nn hy no" role="separator"><span class="np bw bk nq nr ns"/><span class="np bw bk nq nr ns"/><span class="np bw bk nq nr"/></div><div class="in io ip iq ir"><p id="1b77" class="pw-post-body-paragraph lw lx iu ly b lz mt jv mb mc mu jy me lj mv mg mh ln mw mj mk lr mx mm mn mo in bi translated"><a class="ae kz" href="https://gmyrianthous.medium.com/membership" rel="noopener"> <strong class="ly iv">成为会员</strong> </a> <strong class="ly iv">阅读媒体上的每一个故事。你的会员费直接支持我和你看的其他作家。你也可以在媒体上看到所有的故事。</strong></p></div><div class="ab cl nm nn hy no" role="separator"><span class="np bw bk nq nr ns"/><span class="np bw bk nq nr ns"/><span class="np bw bk nq nr"/></div><div class="in io ip iq ir"><p id="676e" class="pw-post-body-paragraph lw lx iu ly b lz mt jv mb mc mu jy me lj mv mg mh ln mw mj mk lr mx mm mn mo in bi translated"><strong class="ly iv">你可能也会喜欢</strong></p><div class="od oe gq gs of og"><a rel="noopener follow" target="_blank" href="/sparksession-vs-sparkcontext-vs-sqlcontext-vs-hivecontext-741d50c9486a"><div class="oh ab fp"><div class="oi ab oj cl cj ok"><h2 class="bd iv gz z fq ol fs ft om fv fx it bi translated">spark session vs spark context vs SQLContext vs hive context</h2><div class="on l"><h3 class="bd b gz z fq ol fs ft om fv fx dk translated">SparkSession、SparkContext HiveContext和SQLContext有什么区别？</h3></div><div class="oo l"><p class="bd b dl z fq ol fs ft om fv fx dk translated">towardsdatascience.com</p></div></div><div class="op l"><div class="oq l or os ot op ou kt og"/></div></div></a></div></div><div class="ab cl nm nn hy no" role="separator"><span class="np bw bk nq nr ns"/><span class="np bw bk nq nr ns"/><span class="np bw bk nq nr"/></div><div class="in io ip iq ir"><div class="kk kl km kn gu og"><a rel="noopener follow" target="_blank" href="/how-to-efficiently-convert-a-pyspark-dataframe-to-pandas-8bda2c3875c3"><div class="oh ab fp"><div class="oi ab oj cl cj ok"><h2 class="bd iv gz z fq ol fs ft om fv fx it bi translated">加快PySpark和Pandas数据帧之间的转换</h2><div class="on l"><h3 class="bd b gz z fq ol fs ft om fv fx dk translated">将大型Spark数据帧转换为熊猫时节省时间</h3></div><div class="oo l"><p class="bd b dl z fq ol fs ft om fv fx dk translated">towardsdatascience.com</p></div></div><div class="op l"><div class="ov l or os ot op ou kt og"/></div></div></a></div></div><div class="ab cl nm nn hy no" role="separator"><span class="np bw bk nq nr ns"/><span class="np bw bk nq nr ns"/><span class="np bw bk nq nr"/></div><div class="in io ip iq ir"><div class="kk kl km kn gu og"><a rel="noopener follow" target="_blank" href="/apache-spark-3-0-the-five-most-exciting-new-features-99c771a1f512"><div class="oh ab fp"><div class="oi ab oj cl cj ok"><h2 class="bd iv gz z fq ol fs ft om fv fx it bi translated">Apache Spark 3.0:5个最激动人心的新特性</h2><div class="on l"><h3 class="bd b gz z fq ol fs ft om fv fx dk translated">Apache Spark 3.0新版本中最激动人心的5个特性</h3></div><div class="oo l"><p class="bd b dl z fq ol fs ft om fv fx dk translated">towardsdatascience.com</p></div></div><div class="op l"><div class="ow l or os ot op ou kt og"/></div></div></a></div></div></div>    
</body>
</html>