<html>
<head>
<title>Variational Autoencoders and Bioinformatics</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">变分自动编码器和生物信息学</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/variational-autoencoders-and-bioinformatics-8a1828170031?source=collection_archive---------20-----------------------#2021-07-11">https://towardsdatascience.com/variational-autoencoders-and-bioinformatics-8a1828170031?source=collection_archive---------20-----------------------#2021-07-11</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="0d80" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">变分自动编码器及其应用简介生物信息学/宏基因组学分析</h2></div><p id="3ded" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">一般来说，自动编码器旨在学习数据的低维表示。自动编码器的主要优势之一是它们能够学习复杂得多的低维，而PCA类分解受到其线性性质的限制。请随意看看我关于自动编码器的文章。</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi lb"><img src="../Images/d0a2167bad247a7baab2a54f4fc2b563.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uogTxM0a-lVa__VkjFBjew.jpeg"/></div></div><p class="ln lo gj gh gi lp lq bd b be z dk translated">照片由<a class="ae lr" href="https://unsplash.com/@hiteshchoudhary?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Hitesh Choudhary </a>在<a class="ae lr" href="https://unsplash.com/s/photos/python?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><div class="ls lt gp gr lu lv"><a rel="noopener follow" target="_blank" href="/machine-learning-autoencoders-712337a07c71"><div class="lw ab fo"><div class="lx ab ly cl cj lz"><h2 class="bd ir gy z fp ma fr fs mb fu fw ip bi translated">机器学习:自动编码器</h2><div class="mc l"><h3 class="bd b gy z fp ma fr fs mb fu fw dk translated">使用自动编码器将高维数据拟合到更密集的表示中</h3></div><div class="md l"><p class="bd b dl z fp ma fr fs mb fu fw dk translated">towardsdatascience.com</p></div></div><div class="me l"><div class="mf l mg mh mi me mj ll lv"/></div></div></a></div><blockquote class="mk"><p id="407a" class="ml mm iq bd mn mo mp mq mr ms mt la dk translated">像往常一样，我将谈论生物信息学中的一个应用。</p></blockquote><h1 id="d567" class="mu mv iq bd mw mx my mz na nb nc nd ne jw nf jx ng jz nh ka ni kc nj kd nk nl bi translated">可变自动编码器</h1><p id="f1bd" class="pw-post-body-paragraph kf kg iq kh b ki nm jr kk kl nn ju kn ko no kq kr ks np ku kv kw nq ky kz la ij bi translated">与传统的自动编码器(AEs)相比，变分自动编码器(VAEs)属于生成模型家族。这是因为VAEs学习输入数据的潜在分布。因此，在给定这些分布中的新点的情况下，它们能够重建新的数据点。然而，VAEs的生成方面并不经常使用，因为gan在这方面做得更好。</p><p id="021f" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">VAEs的一个主要重要性是能够学习数据中的不同分布(类似于高斯混合)。这有助于对展示不同基础分布的数据进行聚类。</p><h1 id="8b1f" class="mu mv iq bd mw mx my mz na nb nc nd ne jw nr jx ng jz ns ka ni kc nt kd nk nl bi translated">Python中的必要导入</h1><p id="86f9" class="pw-post-body-paragraph kf kg iq kh b ki nm jr kk kl nn ju kn ko no kq kr ks np ku kv kw nq ky kz la ij bi translated">为了让您的下一个代码脚本工作，我们将需要以下导入。</p><pre class="lc ld le lf gt nu nv nw nx aw ny bi"><span id="9f3e" class="nz mv iq nv b gy oa ob l oc od">import torch<br/>from torch.nn import functional as F<br/>from torch import nn, optim<br/>from torch.utils.data import DataLoader<br/>from torch.utils.data.dataset import TensorDataset<br/>import numpy as np<br/>from tqdm import tqdm, trange<br/>import seaborn as sns<br/>import umap<br/>import matplotlib.pyplot as plt</span></pre><h1 id="be1d" class="mu mv iq bd mw mx my mz na nb nc nd ne jw nr jx ng jz ns ka ni kc nt kd nk nl bi translated">VAE的建筑</h1><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi oe"><img src="../Images/648040ab7807ff608759432ad871d903.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DvFrReASer7cLiyGiqC_rg.png"/></div></div><p class="ln lo gj gh gi lp lq bd b be z dk translated">作者图表</p></figure><p id="ad87" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">与AE类似，我们有一个瓶颈阶段，随后是重建阶段。更正式地说，我们有一个编码器和一个解码器。注意潜在表示<code class="fe of og oh nv b">VL</code>和下面的<code class="fe of og oh nv b">sigma</code>和<code class="fe of og oh nv b">mu</code>变量。从这些分布参数生成重建<strong class="kh ir">。我们完整的VAE类如下所示。</strong></p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div class="gh gi oi"><img src="../Images/a1947e8a656441b4716f0df70381504e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1010/format:webp/1*cBq7j-D9sizM42jpcuNkUA.png"/></div><p class="ln lo gj gh gi lp lq bd b be z dk translated">作者图片</p></figure><h1 id="3023" class="mu mv iq bd mw mx my mz na nb nc nd ne jw nr jx ng jz ns ka ni kc nt kd nk nl bi translated">重新参数化技巧</h1><p id="c33c" class="pw-post-body-paragraph kf kg iq kh b ki nm jr kk kl nn ju kn ko no kq kr ks np ku kv kw nq ky kz la ij bi translated">注意，我们有<code class="fe of og oh nv b">sigma</code>和<code class="fe of og oh nv b">mu</code>来传递渐变。换句话说，我们将设置VAE来学习给定输入数据集的适当的<code class="fe of og oh nv b">sigma</code>和<code class="fe of og oh nv b">mu</code>。我们使用重新参数化技巧来实现这一点。也就是；</p><pre class="lc ld le lf gt nu nv nw nx aw ny bi"><span id="2d52" class="nz mv iq nv b gy oa ob l oc od">decoder_input = mu + epsilon * e ^ std</span></pre><p id="adee" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">注意，当我们选择使用<code class="fe of og oh nv b">e^std</code>时，我们正在讨论对数方差。因此出现了代码<code class="fe of og oh nv b">logvar</code>中的术语。</p><h1 id="b3af" class="mu mv iq bd mw mx my mz na nb nc nd ne jw nr jx ng jz ns ka ni kc nt kd nk nl bi translated">激活和损失函数</h1><p id="63db" class="pw-post-body-paragraph kf kg iq kh b ki nm jr kk kl nn ju kn ko no kq kr ks np ku kv kw nq ky kz la ij bi translated">请注意，我们在许多地方使用RELU激活。但是，我们在<code class="fe of og oh nv b">logvar</code>潜变量选择了<code class="fe of og oh nv b">softplus</code>。这是因为对数方差总是大于零。在最终重建中，我们使用<code class="fe of og oh nv b">sigmoid</code>，因为我们的输入数据维数范围在0和1之间。</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/488f8f8cd11e92127a8ceceea859077d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1272/format:webp/1*6YCa7dOpfcn_bS0DCvaqQw.png"/></div><p class="ln lo gj gh gi lp lq bd b be z dk translated">作者图片</p></figure><p id="4a82" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们的损失函数由两部分组成。重建损失(RL)和KL散度(KLD)。我们特别使用KLD，以确保学习到的分布尽可能接近正态分布(或高斯分布，或我们喜欢的一些分布)。你可以在这里阅读更多<a class="ae lr" href="https://wiseodd.github.io/techblog/2016/12/10/variational-autoencoder/" rel="noopener ugc nofollow" target="_blank"/>。</p><p id="5a9a" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">重建是一个很好的旧的均方误差。根据应用的不同，这可能会有所不同。例如，黑白图像(MNIST)可以使用二进制交叉熵。</p><p id="6d5c" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">最后，我们可以为支持最佳聚类(或宁滨等)的RL和KLD搜索适当的权重超参数。</p><h1 id="1ad0" class="mu mv iq bd mw mx my mz na nb nc nd ne jw nr jx ng jz ns ka ni kc nt kd nk nl bi translated">使用PyTorch运行示例</h1><p id="efd0" class="pw-post-body-paragraph kf kg iq kh b ki nm jr kk kl nn ju kn ko no kq kr ks np ku kv kw nq ky kz la ij bi translated">让我们考虑一下我最近的一篇论文中的宏基因组数据集。使用<a class="ae lr" href="https://academic.oup.com/bioinformatics/article/32/17/2704/2450740" rel="noopener ugc nofollow" target="_blank"> <strong class="kh ir"> SimLoRD </strong> </a>应用程序模拟长读取。</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi ok"><img src="../Images/21fe62babc5fb0e85ac03bc8601ae8c8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NQYsrf3dobMJOsKkDUKNXw.png"/></div></div><p class="ln lo gj gh gi lp lq bd b be z dk translated">LRBinner数据集(许可证抄送)</p></figure><p id="1f6a" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在，数据集使用以下工具进行了矢量化；</p><div class="ls lt gp gr lu lv"><a href="https://github.com/anuradhawick/seq2vec" rel="noopener  ugc nofollow" target="_blank"><div class="lw ab fo"><div class="lx ab ly cl cj lz"><h2 class="bd ir gy z fp ma fr fs mb fu fw ip bi translated">anuradhawick/seq2vec</h2><div class="mc l"><h3 class="bd b gy z fp ma fr fs mb fu fw dk translated">该工具旨在用于生物信息学机器学习相关任务中的数据生成。你可以用…</h3></div><div class="md l"><p class="bd b dl z fp ma fr fs mb fu fw dk translated">github.com</p></div></div><div class="me l"><div class="ol l mg mh mi me mj ll lv"/></div></div></a></div><p id="de76" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">数据加载器可以设计如下:</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi om"><img src="../Images/af540fb8a0f79eef6f1bc01903532ad6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ngy9qBlSmpyQi7vKcIMaFA.png"/></div></div><p class="ln lo gj gh gi lp lq bd b be z dk translated">作者图片</p></figure><p id="d749" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">训练VAE的功能；</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi on"><img src="../Images/e026d3e65bf5b8911218aeee6106e501.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3tRyP53k9xo-UNvM9aJ5gg.png"/></div></div><p class="ln lo gj gh gi lp lq bd b be z dk translated">作者图片</p></figure><p id="7d03" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">初始化；</p><pre class="lc ld le lf gt nu nv nw nx aw ny bi"><span id="baab" class="nz mv iq nv b gy oa ob l oc od">data = LOAD DATA<br/>truth = LOAD GROUND TRUTH # for visualization</span><span id="7738" class="nz mv iq nv b gy oo ob l oc od">device = "cuda" if torch.cuda.is_available() else "cpu"</span><span id="b7d7" class="nz mv iq nv b gy oo ob l oc od">model = VAE(data.shape[1], 8).to(device)<br/>optimizer = optim.Adam(model.parameters(), lr=1e-2200)</span></pre><p id="8500" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">培训；</p><pre class="lc ld le lf gt nu nv nw nx aw ny bi"><span id="02d2" class="nz mv iq nv b gy oa ob l oc od">train_loader = make_data_loader(data, batch_size=1024, drop_last=True, shuffle=True, device=device)</span><span id="0759" class="nz mv iq nv b gy oo ob l oc od">epochs = 50</span><span id="2b44" class="nz mv iq nv b gy oo ob l oc od">train(model, train_loader, epochs, device)</span></pre><p id="d182" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">获得潜在表征；</p><pre class="lc ld le lf gt nu nv nw nx aw ny bi"><span id="8b1e" class="nz mv iq nv b gy oa ob l oc od">with torch.no_grad():<br/>    model.eval()<br/>    data = LOAD DATA<br/>    data = torch.from_numpy(data).float().to(device)<br/>    em, _ = model.encode(data)</span></pre><p id="5d35" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">可视化；</p><pre class="lc ld le lf gt nu nv nw nx aw ny bi"><span id="6d49" class="nz mv iq nv b gy oa ob l oc od">import random</span><span id="c8b1" class="nz mv iq nv b gy oo ob l oc od">sidx = random.sample(range(len(data)), 10000) # just use 10000<br/>em_2d = umap.UMAP().fit_transform(em.cpu().numpy()[sidx])</span><span id="21d1" class="nz mv iq nv b gy oo ob l oc od">plt.figure(figsize=(10,10))<br/>sns.scatterplot(x=em_2d.T[0], y=em_2d.T[1], hue=truth[sidx])<br/>plt.legend(bbox_to_anchor=(1.05, 1))</span></pre><h1 id="bbae" class="mu mv iq bd mw mx my mz na nb nc nd ne jw nr jx ng jz ns ka ni kc nt kd nk nl bi translated">宁滨宏基因组学</h1><p id="e866" class="pw-post-body-paragraph kf kg iq kh b ki nm jr kk kl nn ju kn ko no kq kr ks np ku kv kw nq ky kz la ij bi translated">一旦VAE被训练，我们就能获得潜在的表象。在这个例子中，我使用UMAP将10000个读数的样本投射到2D进行可视化。它看起来像下面这样。</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi op"><img src="../Images/91fd0d2c3d791c6992926e09d31b12c6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MQR0uc8vBWQBHjSGDt8G1g.png"/></div></div><p class="ln lo gj gh gi lp lq bd b be z dk translated">作者图片</p></figure><p id="5d68" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">就像它看起来的那样，我们可以到处看到大量的数据点。人们可以很容易地使用像<a class="ae lr" href="https://hdbscan.readthedocs.io/en/latest/how_hdbscan_works.html" rel="noopener ugc nofollow" target="_blank"> <strong class="kh ir"> HDBSCAN </strong> </a>这样的工具从中提取密集的簇。</p><h1 id="ac18" class="mu mv iq bd mw mx my mz na nb nc nd ne jw nr jx ng jz ns ka ni kc nt kd nk nl bi translated">结论性的评论</h1><p id="5875" class="pw-post-body-paragraph kf kg iq kh b ki nm jr kk kl nn ju kn ko no kq kr ks np ku kv kw nq ky kz la ij bi translated">最初用VAE代表宁滨的想法是由VAMB代表宁滨议会提出的。VAMB通常要求高重叠群计数(&gt; 10000左右)。这是因为你总是需要更多的数据才能在深度学习中表现得更好。考虑到所有这些挑战和机遇，我们开发了自己的工具<a class="ae lr" href="https://github.com/anuradhawick/LRBinner/" rel="noopener ugc nofollow" target="_blank"> LRBinner </a>来绑定宏基因组学读数。在那里我们总是有数百万的阅读量。完整的LRBinner工具比我在本文中介绍的要复杂得多。但是直觉和想法保持不变。我们也使用了不同的聚类算法。如果你感兴趣，看看下面。</p><div class="ls lt gp gr lu lv"><a href="https://github.com/anuradhawick/LRBinner/" rel="noopener  ugc nofollow" target="_blank"><div class="lw ab fo"><div class="lx ab ly cl cj lz"><h2 class="bd ir gy z fp ma fr fs mb fu fw ip bi translated">阿努拉德哈维克/伊尔宾纳</h2><div class="mc l"><h3 class="bd b gy z fp ma fr fs mb fu fw dk translated">CUDA培训现已推出！新的更新将于2021年6月底推出，具有更快的矢量化和…</h3></div><div class="md l"><p class="bd b dl z fp ma fr fs mb fu fw dk translated">github.com</p></div></div><div class="me l"><div class="oq l mg mh mi me mj ll lv"/></div></div></a></div><p id="d164" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在这里找到完整的Jupyter笔记本<a class="ae lr" href="https://gist.github.com/anuradhawick/0d5bd7f198757f57ab38aa08b790a91c" rel="noopener ugc nofollow" target="_blank"/>。原PyTorch示例代码为<a class="ae lr" href="https://github.com/pytorch/examples/blob/master/vae/main.py" rel="noopener ugc nofollow" target="_blank">此处为</a>。</p><p id="9e25" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我希望你喜欢阅读这篇文章。祝您愉快！</p></div></div>    
</body>
</html>