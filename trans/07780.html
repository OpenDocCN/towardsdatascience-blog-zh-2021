<html>
<head>
<title>Hyperspherical Alternatives to Softmax</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Softmax的超球面替代方案</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/hyperspherical-alternatives-to-softmax-6da03388fe3d?source=collection_archive---------18-----------------------#2021-07-16">https://towardsdatascience.com/hyperspherical-alternatives-to-softmax-6da03388fe3d?source=collection_archive---------18-----------------------#2021-07-16</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="00e6" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">超球面替代品能和Softmax竞争吗？</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/65521e425bb0ae80f2c73de0e00d8342.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*uIHQtsvcZ_00_2Yk"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">迈克尔·泽兹奇在<a class="ae kv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="d0e4" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在分类问题的上下文中，具有交叉熵损失的softmax分类器通常是首选方法。但是，在有许多类的情况下，softmax的训练速度可能会很慢，因为它要求每个类都有一个输出节点，从而导致输出层非常大。例如，一个隐藏层大小为300和100，000个输出类的网络仅在输出层就有3000万个参数。</p><p id="555c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在应用人工智能设置中，这些类型的问题经常发生。这方面的一个例子是学习匹配论文与作者，或产品描述与实际产品。虽然这看起来很适合分类问题，但可能的作者或产品的空间如此之大，以至于使用softmax训练它不太可能可行。解决这个问题的一个方法是学习或预先定义每个类的原型，例如，原型网络<strong class="ky ir">【1】</strong>。使用这些预定义或学习的原型允许您使用恒定的输出大小，而不管类的数量。</p><p id="f098" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在这篇文章中，我们研究了超球原型网络(HPNs)<strong class="ky ir">【2】</strong>，它承诺了恒定的输出大小，性能与softmax网络相当，甚至更好。我们将进一步深入HPNs的网络输出和性能之间的关系。我们将证明，虽然hpn在相对较低的输出维度下工作良好，但随着类别数量的增加，softmax的性能大大优于它。<strong class="ky ir">有趣的是，这与我们预期的<em class="ls">正好相反</em>，这就是为什么我们决定写一篇关于它的博文。</strong></p><p id="1f7f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在深入研究我们令人惊讶的结果之前，我将首先解释一下softmax网络和HPN网络的一些基础知识。</p><h1 id="8097" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated">Softmax网络公司</h1><p id="ae60" class="pw-post-body-paragraph kw kx iq ky b kz ml jr lb lc mm ju le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated">当在单标签分类设置中使用神经网络时，典型的方法是使用softmax网络。也就是说，不确定大小和图层的网络具有| <strong class="ky ir"> C </strong> |个单位的输出图层，其中<strong class="ky ir"> C </strong>是输出类的集合，以及softmax激活函数:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mq"><img src="../Images/2bc6a4f28db97ece5d515c4700be3dfe.png" data-original-src="https://miro.medium.com/v2/resize:fit:666/0*R7JSu69blFdepijf"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">情商。softmax激活功能。</p></figure><p id="cc5c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">Softmax分类器通常通过最小化网络和目标预测之间的交叉熵来训练。这可以理解为试图相对于不正确的输出单位最大化正确输出的幅度。</p><p id="14fc" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">由于softmax分类器具有<strong class="ky ir"> |C| </strong>个输出单元，因此对于每个类，softmax网络的输出层以因子<strong class="ky ir"> H </strong>增长，其中<strong class="ky ir"> H </strong>是最后一层中隐藏单元的数量。</p><h1 id="e821" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated">超球面原型网络</h1><p id="3cd4" class="pw-post-body-paragraph kw kx iq ky b kz ml jr lb lc mm ju le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated">相比之下，超球面原型网络(HPN)<strong class="ky ir">【2】</strong>的输出层具有预定义的固定维度，而不是类的数量。理论上，这意味着HPN中输出单元的数量可以比类的数量小得多，这对于大量的类来说是有效的。HPNs通过<strong class="ky ir">最小化从网络输出到预定义原型的余弦平方距离来学习，而不是交叉熵:</strong></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mr"><img src="../Images/9d1edeb52bb85b2efc2e7bf8ebc20dd8.png" data-original-src="https://miro.medium.com/v2/resize:fit:708/0*4b29yGSXzgqa-UVu"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">情商。2:由HPN最小化的损失函数</p></figure><p id="1cb1" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在某种意义上，这需要网络学会将一个实例“旋转”到正确类的原型。注意，这个损失函数只需要正确类的原型，不像例如原型网络，它需要访问所有相关的原型。现在，超球面原型网络的特别之处在于，原型是<strong class="ky ir">静态的</strong>，即在训练期间不更新，并且<strong class="ky ir">正交</strong>。让我们解释一下为什么这两个属性都很重要。</p><h2 id="ed2c" class="ms lu iq bd lv mt mu dn lz mv mw dp md lf mx my mf lj mz na mh ln nb nc mj nd bi translated">深入静态和正交属性</h2><p id="bdde" class="pw-post-body-paragraph kw kx iq ky b kz ml jr lb lc mm ju le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated">如果原型不是静态的<strong class="ky ir"/>，上面的损失函数就会变成一个<strong class="ky ir">平凡解</strong>。众所周知，如果一个问题有一个微不足道的解决方案，反向传播就会找到它。为了理解“为什么这变成一个微不足道的解决方案”,考虑损失函数仅仅使网络最小化原型和网络输出之间的距离，但是没有规定代表不同类别的原型应该是不同的。因此，通过输出一个常数矢量(例如一个全零矢量)，并给所有原型分配完全相同的矢量，网络可以完美地解决任何学习问题。如果学习的原型是期望的，例如，对于零次或少次学习，这个问题可以通过<strong class="ky ir">负采样</strong><strong class="ky ir">【3】</strong>来规避。</p><p id="46e2" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">为什么原型应该是<strong class="ky ir">正交的</strong>就不那么明显了。<strong class="ky ir"> </strong>直觉上，属于不同类的两个原型不应该有相同的表示，因为这意味着类是模糊的。所以，不太相似的原型更容易分开。从字面上看，正交表示是最不相似的，因此它们也可以被认为是最优可分的。因此，在某种意义上，表示应该是正交的是不正确的；他们只是表现得更好。在一些设置中，例如零镜头或少镜头分类，具有有意义相关的表示对性能非常有益，例如参见<strong class="ky ir">【1】</strong>。</p><p id="b8a1" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">因为正交表示的维度是可控的，所以HPN允许输出单元数量的灵活性。如果正交表示的维数等于类的数目，则HPN与等效的softmax分类器具有相同数目的参数。</p><h1 id="c758" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated">创建正交表示</h1><p id="7885" class="pw-post-body-paragraph kw kx iq ky b kz ml jr lb lc mm ju le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated">制作正交表示似乎很容易，但并不容易。正如<strong class="ky ir">【2】</strong>所指出的，寻找与给定点集正交的点的问题仍然是公开的，并且被称为塔门问题。<strong class="ky ir">【4】</strong></p><p id="f19a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">本文研究了一种引入超球面原型网络的解决方案。<strong class="ky ir">【2】</strong>简而言之，他们首先随机生成向量，然后，对于每个向量，最小化与最接近向量的余弦相似度，同时在每次迭代后进行重正化。在我们的实施中，我们没有明确地重整，但是我们将L2范数的平均偏差从1增加到损失。这导致原型是准正交的，而没有显式重正化。与每次迭代后显式规格化相比，这具有导致更低损失的优点，因此平均来说表示更正交。</p><p id="2433" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">注意，通过反向投影来训练这些表示是相对有效的，但是它仍然会花费很长时间，特别是如果我们试图在低维空间中打包许多表示，例如，将100，000个表示打包到100维空间中。</p><p id="3fb5" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">为了显示生成正交表示的有效性，我们还比较了从N维正态分布采样的随机生成的表示。随着维数的增加，高维随机向量变得越来越随机，但永远不会真正正交。因此，随机生成的表示和训练的表示之间的任何分数差异显示了正交化过程的附加值。</p><h1 id="67e1" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated">实际的实验</h1><p id="5543" class="pw-post-body-paragraph kw kx iq ky b kz ml jr lb lc mm ju le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated">我们特别感兴趣的是一方面在类别的维度和数量之间的权衡，另一方面在性能、训练和推理速度之间的权衡。与高维正交原型相反，低维正交原型的训练速度更快，但可能会牺牲性能。</p><p id="ab88" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">为了测试softmax和HPN网络之间的差异，我们使用了两个相同的网络，由两个具有批量归一化、校正线性单元激活、丢失和300个隐藏单元的隐藏层组成。网络之间的唯一区别是顶层。在softmax的情况下，这是具有(300，<strong class="ky ir"> |C| </strong>)个单位的线性层，而在HPN的情况下，该层的大小取决于正交原型的输出维度。使用ADAM优化器<strong class="ky ir">【5】</strong>对所有网络进行优化，学习率为1e-3，使用固定验证集上的早期停止。</p><p id="d895" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们生成5个不同的数据集，分别有10、100和1000个类。对于每个数据集中的每个类，我们通过从区间[-1，1]均匀采样300维表示来定义中心。对于每个中心，我们从正态分布中抽取样本，样本的中心和标准差取决于类别的数量。标准偏差列于表1。</p><pre class="kg kh ki kj gt ne nf ng nh aw ni bi"><span id="ed89" class="ms lu iq nf b gy nj nk l nl nm">.--------------------.----.-----.------.<br/>|     n classes      | 10 | 100 | 1000 |<br/>:--------------------+----+-----+------:<br/>| standard deviation |  3 | 2.5 | 1.25 |<br/>'--------------------'----'-----'------'<br/><em class="ls">Table 1: the standard deviations for the different numbers of classes</em></span></pre><p id="3a9a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">对于每个数据集，我们还分别生成3组10、100和1000维的原型。这些原型集允许我们研究输出维度对性能的影响。我们还尝试使用1000多个类(10，000和100，000)，但是为这些类创建正交原型花费了太长时间，比训练网络要长得多。</p><h1 id="323f" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated">结果</h1><h2 id="3b7d" class="ms lu iq bd lv mt mu dn lz mv mw dp md lf mx my mf lj mz na mh ln nb nc mj nd bi translated">表演</h2><p id="85ad" class="pw-post-body-paragraph kw kx iq ky b kz ml jr lb lc mm ju le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated">表2中列出了测试集的F分数，是五次运行的平均值。请注意，只有原型维度与类的数量一样多的模型才能与softmax模型相媲美。</p><p id="6e3a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">对于10级和100级，HPNs的表现往往与softmax模型相当，而对于1000级模型，HPN表现不佳。增加原型的维度只会导致分数的边际增加，这表明对性能的上限效应。一旦你有了足够的工作空间，增加更多的空间并没有多大作用。</p><p id="642a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">比较随机生成的原型和优化的原型可以发现一些有趣的模式。对于相应的维度，即10个类的10维原型，选择随机化的原型会降低性能。然而，当转向更高维的原型时，这种性能差异就消失了。重温上面的比喻:如果维度足够高，即使随机化的原型也为模型提供了足够的工作空间。因为从正态分布中生成随机样本几乎不需要时间，这对于许多类的问题来说可能是有趣的，在这些类中生成真正正交的原型是不可行的。</p><pre class="kg kh ki kj gt ne nf ng nh aw ni bi"><span id="13d4" class="ms lu iq nf b gy nj nk l nl nm">.-----------.---------.------.------.------.-------.------.-------.<br/>| n classes | softmax |  10  | 100  | 1000 |  r10  | r100 | r1000 |<br/>:-----------+---------+------+------+------+-------+------+-------:<br/>|        10 |    91.3 | 91.2 | 92.6 | 92.5 | 78.22 | 92.5 |  92.8 |<br/>:-----------+---------+------+------+------+-------+------+-------:<br/>|       100 |    84.5 | 72.9 | 86.0 | 85.3 |  73.6 | 85.7 |  86.1 |<br/>:-----------+---------+------+------+------+-------+------+-------:<br/>|      1000 |    83.8 | 15.8 | 58.3 | 47.3 |  14.9 | 54.3 |  47.3 |<br/>'-----------'---------'------'------'------'-------'------'-------'<br/><em class="ls">Table 2: The F-scores on the test set for classes and various models. r stands for “random”</em></span></pre><h2 id="310d" class="ms lu iq bd lv mt mu dn lz mv mw dp md lf mx my mf lj mz na mh ln nb nc mj nd bi translated">速度</h2><p id="a243" class="pw-post-body-paragraph kw kx iq ky b kz ml jr lb lc mm ju le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated">每个时期的速度对所有模型都是可比较的。这是令人惊讶的，因为我们预计随着类别数量的增加，与余弦相似性相比，softmax会变慢。类似地，在收敛之前的历元数之间也有明显的差异，但前提是类别数等于原型的维数。表3总结了一些例子。</p><pre class="kg kh ki kj gt ne nf ng nh aw ni bi"><span id="9ce2" class="ms lu iq nf b gy nj nk l nl nm">.-------------------------.----.-----.------.<br/>|        n classes        | 10 | 100 | 1000 |<br/>:-------------------------+----+-----+------:<br/>| softmax                 | 50 | 146 |  181 |<br/>:-------------------------+----+-----+------:<br/>| HPN dim == n_prototypes | 69 | 147 |  206 |<br/>'-------------------------'----'-----'------'<br/><em class="ls">Table 3: The number of epochs until convergence for softmax, and the HPNs in which the prototype dimensionality matched the number of classes.</em></span></pre><h1 id="5da2" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated">讨论</h1><p id="d47c" class="pw-post-body-paragraph kw kx iq ky b kz ml jr lb lc mm ju le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated">在我们的实验中，我们看到使用HPNs的优势很小。我们期望它们在与许多类一起使用时性能优于softmax，并且时间效率更高。这两件事都被证明是错误的，但可能是出于不同的原因。</p><p id="67f1" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">让我们先来解决速度差异:我们假设，因为HPNs只查看正类的原型，也就是说，它们不需要负采样，所以它们也会更有效。尽管理论上效率很高，但网络的输出维度需要大致匹配类的数量，如上所示。这意味着HPNs和softmax网络在计算损耗时所执行的操作数量大致相等。此外，很可能softmax可以更高效地实现。</p><p id="fc05" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">比速度差异更令人惊讶、也更关键的是性能差异。虽然HPN有时在较低维度上优于softmax，但1000维的原型根本不起作用。随着空间维度的增加，正交化原型的损失函数很可能不能填充足够的原型。假设使用hpn的优势似乎很少，即使对于它们工作的环境，我们认为正确的反应不是改善空间的正交性，而是找到对相关输出表示鲁棒的hpn公式。</p><p id="0fc7" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">最后，我们想补充一点，我们使用了人工数据，因为我们希望能够控制班级的数量。在现实生活中，softmax和HPN之间的差异可能更小或更大。例如，我们可能无意中制造了一个特别适合HPNs或softmax的问题。原始论文<strong class="ky ir">【2】</strong>的作者尝试了各种数据集，主要是图像集，并显示HPNs略微优于softmax。重要的是，他们没有使用超过200个类的数据集。</p><h1 id="cf42" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated">结论</h1><p id="e339" class="pw-post-body-paragraph kw kx iq ky b kz ml jr lb lc mm ju le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated">我们发现，虽然HPNs似乎是一种有前途的分类方法，但随着类别数量的增加，其性能会急剧下降。这些结果对超球原型网络用于分类任务的效用提出了一些疑问。</p><h1 id="ff52" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated">参考</h1><p id="3ec5" class="pw-post-body-paragraph kw kx iq ky b kz ml jr lb lc mm ju le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated"><strong class="ky ir">【1】</strong>Snell，j .、Swersky，k .和Zemel，r .，《少投学习的原型网络》，(2017)，<em class="ls">《第31届神经信息处理系统国际会议论文集</em>(第4080-4090页)</p><p id="0316" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">【2】</strong>Mettes，p .，van der Pol，e .和Snoek，C.G .，超球面原型网络，(2019)，<em class="ls"> arXiv预印本arXiv:1901.10514 </em></p><p id="b4b8" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">【3】</strong>米科洛夫、苏茨科夫、陈、科拉多和迪安..单词和短语的分布式表示及其组合性，(2013)，载于<em class="ls">神经信息处理系统进展</em>(第3111-3119页)。</p><p id="209c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">【4】</strong>tam mes，P.M.L .，《关于花粉粒表面出口位置的数量和排列的起源》(1930年)，<em class="ls">《荷兰植物研究杂志</em>，第1–84页。</p><p id="9f7f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">【5】</strong>Kingma，D.P .和Ba，j .，Adam:一种随机优化的方法，(2014)<em class="ls">arXiv预印本arXiv:1412.6980 </em>。</p></div></div>    
</body>
</html>