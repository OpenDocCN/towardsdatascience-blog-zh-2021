<html>
<head>
<title>Logistic Regression with PyTorch</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用PyTorch进行逻辑回归</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/logistic-regression-with-pytorch-3c8bbea594be?source=collection_archive---------2-----------------------#2021-10-04">https://towardsdatascience.com/logistic-regression-with-pytorch-3c8bbea594be?source=collection_archive---------2-----------------------#2021-10-04</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="2126" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">使用PyTorch应用逻辑回归进行二元分类的介绍。</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/6855ad10d3711395f5fbded848ab3737.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rTT6HKLV1z_N3M6x0xC1OA.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">我们选哪个门？(<em class="kv">图片来自iStock，授权给Dennis Loevlie </em>)</p></figure><p id="9af7" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">二元逻辑回归用于对两个线性可分的组进行分类。这种线性可分的假设使得逻辑回归对于简单的ML任务非常快速和强大。我们将对其执行逻辑回归的线性可分数据示例如下所示:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ls"><img src="../Images/160ceb41f92b3fdd9299d9f70be27b1b.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/format:webp/1*mYhCzKyXWbhFf1E33cGS-Q.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">线性可分数据的示例(图片由作者提供)</p></figure><p id="aab2" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">这里的线性可分群是:</strong></p><ol class=""><li id="6f21" class="lt lu iq ky b kz la lc ld lf lv lj lw ln lx lr ly lz ma mb bi translated">红色= 0</li><li id="bce9" class="lt lu iq ky b kz mc lc md lf me lj mf ln mg lr ly lz ma mb bi translated">蓝色= 1</li></ol><p id="c22d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们希望使用逻辑回归将任何[ <em class="mh"> x1 </em>，<em class="mh"> x2 </em> ] <em class="mh"> </em>对映射到相应的类(红色或蓝色)。</p><h2 id="653d" class="mi mj iq bd mk ml mm dn mn mo mp dp mq lf mr ms mt lj mu mv mw ln mx my mz na bi translated">第一步。将我们的数据集分割成训练/测试分割。</h2><p id="b241" class="pw-post-body-paragraph kw kx iq ky b kz nb jr lb lc nc ju le lf nd lh li lj ne ll lm ln nf lp lq lr ij bi translated">我们这样做是为了评估我们的模型在数据上的表现，这些数据在训练中是看不到的。通常，如果你告诉某人你的模型有97%的准确性，这被认为是你在谈论验证/测试的准确性。</p><p id="fd70" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">你自己可以很容易地做到这一点，但是说实话，<em class="mh">sk learn . train _ test _ split</em>函数在可读性方面非常好。</p><pre class="kg kh ki kj gt ng nh ni nj aw nk bi"><span id="3fe5" class="mi mj iq nh b gy nl nm l nn no">X_train, X_test, y_train, y_test = train_test_split(<br/> inputs, labels, test_size=0.33, random_state=42)</span></pre><h2 id="95e0" class="mi mj iq bd mk ml mm dn mn mo mp dp mq lf mr ms mt lj mu mv mw ln mx my mz na bi translated">步骤2:构建PyTorch模型类</h2><p id="ef4c" class="pw-post-body-paragraph kw kx iq ky b kz nb jr lb lc nc ju le lf nd lh li lj ne ll lm ln nf lp lq lr ij bi translated">我们可以使用以下代码创建逻辑回归模型:</p><pre class="kg kh ki kj gt ng nh ni nj aw nk bi"><span id="23b8" class="mi mj iq nh b gy nl nm l nn no">import torch</span><span id="bac4" class="mi mj iq nh b gy np nm l nn no">class LogisticRegression(torch.nn.Module):<br/>     def __init__(self, input_dim, output_dim):<br/>         super(LogisticRegression, self).__init__()<br/>         self.linear = torch.nn.Linear(input_dim, output_dim)</span><span id="accf" class="mi mj iq nh b gy np nm l nn no">     def forward(self, x):<br/>         outputs = torch.sigmoid(self.linear(x))<br/>         return outputs</span></pre><p id="3eae" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在PyTorch神经网络(实际上只是一个感知器)的“正向”传递中，视觉表示和相应的方程如下所示:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nq"><img src="../Images/d22b54060e3d5092309f4ebb523c4279.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*H0IT5bIZZ9d3e_Gu3Siyjw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">神经网络架构可视化(图片由作者提供)</p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/e20633130473a931c66e73eb016f7971.png" data-original-src="https://miro.medium.com/v2/resize:fit:884/format:webp/1*Wa4ETY1J1euyLpDsKeKJjA.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">(图片由作者提供)</p></figure><p id="70a8" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">其中:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/50dae32da41133b3737a2fcd286587a0.png" data-original-src="https://miro.medium.com/v2/resize:fit:652/format:webp/1*93nVNpxoS7pFYqqon4JuFQ.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">(图片由作者提供)</p></figure><p id="4230" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">sigmoid函数非常有用，主要有两个原因:</p><ol class=""><li id="473b" class="lt lu iq ky b kz la lc ld lf lv lj lw ln lx lr ly lz ma mb bi translated">它将我们的线性回归输出转换为从0到1的概率。然后，我们可以将任何大于0.5的概率视为1，小于0的概率视为0。</li><li id="8f16" class="lt lu iq ky b kz mc lc md lf me lj mf ln mg lr ly lz ma mb bi translated">与逐步函数(也会将数据转换为二进制形式)不同，sigmoid是可微分的，这对于使用梯度下降优化参数是必要的(我们将在后面介绍)。</li></ol><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ls"><img src="../Images/84d8402a51e0c0e707b145ffbeee155a.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/format:webp/1*7iA0ubf7gJBJ0eWhvNwSfA.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">具有选择蓝色或红色的决策边界的Sigmoid函数(图片由作者提供)</p></figure><h2 id="b21a" class="mi mj iq bd mk ml mm dn mn mo mp dp mq lf mr ms mt lj mu mv mw ln mx my mz na bi translated">步骤3:初始化模型</h2><p id="5239" class="pw-post-body-paragraph kw kx iq ky b kz nb jr lb lc nc ju le lf nd lh li lj ne ll lm ln nf lp lq lr ij bi translated">此外，我们应该分配一些超参数:</p><pre class="kg kh ki kj gt ng nh ni nj aw nk bi"><span id="01c5" class="mi mj iq nh b gy nl nm l nn no">epochs = 200000<br/>input_dim = 2 # Two inputs x1 and x2 <br/>output_dim = 1 # Single binary output <br/>learning_rate = 0.01</span></pre><p id="a7b3" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">参数定义:</strong></p><ul class=""><li id="5bc9" class="lt lu iq ky b kz la lc ld lf lv lj lw ln lx lr nt lz ma mb bi translated"><strong class="ky ir">历元</strong> —表示网络已经完成的通过整个训练数据集的次数</li><li id="dcd8" class="lt lu iq ky b kz mc lc md lf me lj mf ln mg lr nt lz ma mb bi translated"><strong class="ky ir"> learning_rate </strong> —优化算法中的一个调整参数，用于确定每次迭代的步长，同时向损失函数的最小值移动<br/> *高学习率意味着您可能永远无法达到最小值。<br/> *低学习率需要更长时间。</li></ul><p id="5880" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">下面的代码初始化了我们之前创建的模型类:</p><pre class="kg kh ki kj gt ng nh ni nj aw nk bi"><span id="34a0" class="mi mj iq nh b gy nl nm l nn no">model = LogisticRegression(input_dim,output_dim)</span></pre><p id="12a3" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">步骤4:初始化损失函数和优化器</p><pre class="kg kh ki kj gt ng nh ni nj aw nk bi"><span id="cadb" class="mi mj iq nh b gy nl nm l nn no">criterion = torch.nn.BCELoss()</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/aa00815ee018531214c497a8bf72b810.png" data-original-src="https://miro.medium.com/v2/resize:fit:702/format:webp/1*LXB1tJsI2K1sWzptWlDi7Q.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><strong class="bd nv">二值交叉熵损失</strong> <strong class="bd nv">(图片由作者提供)</strong></p></figure><ul class=""><li id="c98b" class="lt lu iq ky b kz la lc ld lf lv lj lw ln lx lr nt lz ma mb bi translated">m =训练样本的数量</li><li id="8bc0" class="lt lu iq ky b kz mc lc md lf me lj mf ln mg lr nt lz ma mb bi translated">y =真y值</li><li id="5966" class="lt lu iq ky b kz mc lc md lf me lj mf ln mg lr nt lz ma mb bi translated">y^ =预测的y值</li></ul><pre class="kg kh ki kj gt ng nh ni nj aw nk bi"><span id="1ec9" class="mi mj iq nh b gy nl nm l nn no">optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)</span></pre><p id="e950" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">有很多常见的神经网络优化器，但大多数都是基于<strong class="ky ir">梯度下降。</strong>这种优化技术朝着损失函数的最小值前进，其方向由损失函数的梯度决定，即由学习速率决定的权重和幅度或步长。</p><p id="1037" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">注:</strong>为了准确快速的达到损失函数的最小值，慢慢的降低你的学习速率是有好处的，像PyTorch也实现的自适应运动估计算法(<strong class="ky ir"> ADAM)、</strong>这样的优化器，为我们做到这一点。你可以在<a class="ae nw" href="https://pytorch.org/docs/stable/optim.html" rel="noopener ugc nofollow" target="_blank">https://pytorch.org/docs/stable/optim.html</a>找到更多关于这些优化器的PyTorch实现。</p><p id="ece9" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们使用以下等式更新参数以最小化损失函数:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/470b7aa2c58a6bd9e4e94001aaa8dfeb.png" data-original-src="https://miro.medium.com/v2/resize:fit:464/format:webp/1*sLSJwSrq74UaorHkV99pIQ.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">(图片由作者提供)</p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/acf9bab68373e6bd409701d0b1887f43.png" data-original-src="https://miro.medium.com/v2/resize:fit:440/format:webp/1*isbkPmkOfcZCc6zcNb9SCA.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">(图片由作者提供)</p></figure><ul class=""><li id="bb97" class="lt lu iq ky b kz la lc ld lf lv lj lw ln lx lr nt lz ma mb bi translated"><strong class="ky ir"> Alpha </strong> —学习率</li></ul><p id="8a53" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">你可能想知道我们从哪里得到<em class="mh"> dL/dw </em>和<em class="mh"> dL/dbeta，</em>这将是一个很好的问题！在神经网络中，我们使用反向传播来获得偏导数。幸运的是，在逻辑回归中，方程简化了，我将在下面展示这一点(以及网络的反向投影)。</p><p id="49f4" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">利用链式法则我们可以推导出:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nz"><img src="../Images/4a126df0be6b23cc019224bb1dad9ee6.png" data-original-src="https://miro.medium.com/v2/resize:fit:704/format:webp/1*4fmrv5WrJigDlE7hTbntJQ.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">(图片由作者提供)</p></figure><p id="d6aa" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">偏导数如下所示:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oa"><img src="../Images/7a8d198535f7b8a8ef3dd6db3c63eb27.png" data-original-src="https://miro.medium.com/v2/resize:fit:1088/format:webp/1*37V8l6PrQlgu4Lyd-dhc8w.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">(图片由作者提供)</p></figure><p id="f50b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">简单的方程式，你会得到:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ob"><img src="../Images/2d7e112d8c6f9ed78af1c44dcb960a19.png" data-original-src="https://miro.medium.com/v2/resize:fit:520/format:webp/1*okZvfE7RBtOLLf0zvBifgg.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">(图片由作者提供)</p></figure><p id="6916" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">所以在现实中你会这样做:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/6b1897e565431726912bf51d04dbad86.png" data-original-src="https://miro.medium.com/v2/resize:fit:508/format:webp/1*aRY-hEDH-7vo3kGtsZetRw.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">(图片由作者提供)</p></figure><p id="789c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们可以类似地导出<em class="mh"> dL/dbeta </em>。幸运的是，亲笔签名帮助我们做到了这一切！</p><h2 id="c6aa" class="mi mj iq bd mk ml mm dn mn mo mp dp mq lf mr ms mt lj mu mv mw ln mx my mz na bi translated">第五步:训练模型</h2><p id="c68b" class="pw-post-body-paragraph kw kx iq ky b kz nb jr lb lc nc ju le lf nd lh li lj ne ll lm ln nf lp lq lr ij bi translated">首先，我们将输入和标签从numpy数组转换成张量。</p><pre class="kg kh ki kj gt ng nh ni nj aw nk bi"><span id="cc09" class="mi mj iq nh b gy nl nm l nn no">X_train, X_test = torch.Tensor(X_train),torch.Tensor(X_test)<br/>y_train, y_test = torch.Tensor(y_train),torch.Tensor(y_test)</span></pre><p id="f0f7" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">接下来，我们构建训练循环并存储损失。我们还可以不时地打印出测试数据的准确性，看看我们的模型做得如何。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="od oe l"/></div></figure><p id="9d07" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">每10，000个时期的损失:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ls"><img src="../Images/e4479e94925f93f8874679e5808c7c63.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/format:webp/1*yENJyfzGscdOVr_83pVnWQ.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">BCE损失与纪元的关系(图片由作者提供)</p></figure><h2 id="788c" class="mi mj iq bd mk ml mm dn mn mo mp dp mq lf mr ms mt lj mu mv mw ln mx my mz na bi translated">步骤6:绘制结果</h2><p id="8057" class="pw-post-body-paragraph kw kx iq ky b kz nb jr lb lc nc ju le lf nd lh li lj ne ll lm ln nf lp lq lr ij bi translated">由于我们知道决策边界将是<em class="mh"> w*x + b = 0.5 </em>，我们可以绘制决策边界。结果如下:</p><p id="d9a0" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">列车:</strong></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ls"><img src="../Images/5627af257ef59bd8ce0c06e4b73ac8c3.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/format:webp/1*uDpxtUEXqaDnjyqENWSNlw.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">(图片由作者提供)</p></figure><p id="58f8" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">测试:</strong></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ls"><img src="../Images/d5882bf7bb060737f02a632db6a8642e.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/format:webp/1*8duSTKBBRdQLdCqBVnzgXQ.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">(图片由作者提供)</p></figure><h2 id="7c88" class="mi mj iq bd mk ml mm dn mn mo mp dp mq lf mr ms mt lj mu mv mw ln mx my mz na bi translated">第七步:如何在新数据上获得预测！</h2><p id="02ef" class="pw-post-body-paragraph kw kx iq ky b kz nb jr lb lc nc ju le lf nd lh li lj ne ll lm ln nf lp lq lr ij bi translated">如果你有一个新的点在<em class="mh"> x1=1 </em>，<em class="mh"> x2=1 </em>视觉上(在2维空间)，很容易告诉我们应该把这个点归类为“红色”。因此，让我们检查我们的模型是否工作正常，并展示如何根据新数据从模型中获得预测:</p><pre class="kg kh ki kj gt ng nh ni nj aw nk bi"><span id="7eed" class="mi mj iq nh b gy nl nm l nn no">x1 = 1<br/>x2 = 1<br/>new_data = torch.tensor([x1,x2]).type(torch.FloatTensor)</span><span id="64e5" class="mi mj iq nh b gy np nm l nn no">with torch.no_grad():<br/>    prediction = model(new_data).round()<br/>    if prediction == 1.0:<br/>        print(f'The model classifies this point as RED')<br/>    else:<br/>        print(f'The model classifies this point as BLUE')</span></pre><p id="fd63" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">根据下面的训练数据绘制新点:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ls"><img src="../Images/0d4fed82f6040970c85175bf2d9251c4.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/format:webp/1*b8-uPVGKIpFBAUYPUPftiQ.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">根据新数据进行预测(图片由作者提供)</p></figure><p id="44f3" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">输出:</strong></p><pre class="kg kh ki kj gt ng nh ni nj aw nk bi"><span id="38eb" class="mi mj iq nh b gy nl nm l nn no">&gt;&gt;&gt; The model classifies this point as RED</span></pre><h2 id="0059" class="mi mj iq bd mk ml mm dn mn mo mp dp mq lf mr ms mt lj mu mv mw ln mx my mz na bi translated">完整代码:</h2><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="od oe l"/></div></figure><h2 id="dfe3" class="mi mj iq bd mk ml mm dn mn mo mp dp mq lf mr ms mt lj mu mv mw ln mx my mz na bi translated">其他资源:</h2><ul class=""><li id="4966" class="lt lu iq ky b kz nb lc nc lf of lj og ln oh lr nt lz ma mb bi translated"><a class="ae nw" href="https://pytorch.org/docs/stable/nn.html" rel="noopener ugc nofollow" target="_blank">https://pytorch.org/docs/stable/nn.html</a></li><li id="3df4" class="lt lu iq ky b kz mc lc md lf me lj mf ln mg lr nt lz ma mb bi translated"><a class="ae nw" href="https://deeplearning.cs.cmu.edu/F21/index.html" rel="noopener ugc nofollow" target="_blank">https://deeplearning.cs.cmu.edu/F21/index.html</a></li><li id="598f" class="lt lu iq ky b kz mc lc md lf me lj mf ln mg lr nt lz ma mb bi translated">https://www.youtube.com/watch?v=MswxJw-8PvE&amp;t = 304s</li></ul><h2 id="802b" class="mi mj iq bd mk ml mm dn mn mo mp dp mq lf mr ms mt lj mu mv mw ln mx my mz na bi translated">我们连线吧！</h2><ol class=""><li id="3f48" class="lt lu iq ky b kz nb lc nc lf of lj og ln oh lr ly lz ma mb bi translated"><a class="ae nw" href="https://twitter.com/DennisLoevlie" rel="noopener ugc nofollow" target="_blank">推特</a></li><li id="c3f8" class="lt lu iq ky b kz mc lc md lf me lj mf ln mg lr ly lz ma mb bi translated"><a class="ae nw" href="https://www.linkedin.com/in/dennisloevlie/" rel="noopener ugc nofollow" target="_blank">领英</a></li><li id="3a16" class="lt lu iq ky b kz mc lc md lf me lj mf ln mg lr ly lz ma mb bi translated"><a class="ae nw" href="https://github.com/loevlie" rel="noopener ugc nofollow" target="_blank"> GitHub </a></li></ol></div></div>    
</body>
</html>