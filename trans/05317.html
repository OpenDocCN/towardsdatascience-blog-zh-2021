<html>
<head>
<title>Evolutionary Strategy: A Theoretical Implementation Guide</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">进化策略:理论实施指南</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/evolutionary-strategy-a-theoretical-implementation-guide-9176217e7ed8?source=collection_archive---------36-----------------------#2021-05-10">https://towardsdatascience.com/evolutionary-strategy-a-theoretical-implementation-guide-9176217e7ed8?source=collection_archive---------36-----------------------#2021-05-10</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi jn"><img src="../Images/b7ea253a08cbeade47d77a340fafa31d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*8Xg4X4htM1r-eCJv"/></div></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">Sebastian Unrau 在<a class="ae kc" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="11fc" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在这里，我将谈谈OpenAI在<a class="ae kc" href="https://arxiv.org/abs/1703.03864" rel="noopener ugc nofollow" target="_blank"> <strong class="kf ir">本文</strong> </a>中描述的进化策略背后的直觉和一些统计数据。</p><p id="abb4" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如果你想要一些代码，<a class="ae kc" href="https://github.com/09tangriro/AdamES/blob/main/es.py" rel="noopener ugc nofollow" target="_blank"> <strong class="kf ir">这里就是</strong> </a>！</p><h1 id="ce2d" class="lb lc iq bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">优化简介</h1><p id="470d" class="pw-post-body-paragraph kd ke iq kf b kg lz ki kj kk ma km kn ko mb kq kr ks mc ku kv kw md ky kz la ij bi translated">在传统的优化设置中，目标是找到某个函数的全局最小值。例如，函数<strong class="kf ir"> f(x) = x </strong>的最小值在<strong class="kf ir"> x = 0 </strong>处:</p><figure class="mf mg mh mi gt jr gh gi paragraph-image"><div class="gh gi me"><img src="../Images/0bf475ade13b53be67b77ae57c7eaf77.png" data-original-src="https://miro.medium.com/v2/resize:fit:880/format:webp/1*LLG8iShw1eY1A2MzOcrq8A.png"/></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">图片作者:f(x) = x的图形很明显，当f(x) = 0时，这个函数的极小点在x = 0处</p></figure><p id="2044" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这个例子的解决方案相当明显，但是这些函数可以变得更加复杂，并且存在于更高维度中，在那里解决方案不是如此微不足道。传统优化方法背后的思想是使用函数的梯度来尝试“滑下斜坡”，直到到达底部。可以想象，这些算法都要求函数的梯度是明确定义的；事实上，一些像牛顿-拉夫森方法甚至需要二阶导数信息！通常，这些计算起来非常困难或耗时。<strong class="kf ir">有没有一种不需要这些额外信息就能优化函数的方法？</strong></p><h1 id="a318" class="lb lc iq bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">ES背后的一些直觉</h1><p id="b865" class="pw-post-body-paragraph kd ke iq kf b kg lz ki kj kk ma km kn ko mb kq kr ks mc ku kv kw md ky kz la ij bi translated">ES是另一种优化方法。将这种方法与更传统的优化算法分开的是缺少关于策略所需的函数本身的信息，es根本不需要直接的梯度信息！这意味着我们可以将它应用于更抽象的优化问题，在这些问题中，被优化的函数在某种程度上是一个黑盒。</p><p id="c446" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">那么这实际上是如何工作的呢？假设有一个函数<strong class="kf ir"> </strong>我们想要针对某些参数进行优化。这是一种迭代算法，其中我们不断更新由参数给出的位置<strong class="kf ir"> </strong>，以有希望地收敛到一个最佳点(在这种情况下，<strong class="kf ir">算法优化最大值而不是最小值</strong>)。从一个开始位置，我们随机采样点，并计算这些点的函数值，以收集关于函数本身的信息。利用这些信息，我们可以更新自己，朝着上升的方向前进！基本上，我们在从采样点计算出更多大值的方向上移动一段距离。</p><h1 id="dfc5" class="lb lc iq bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">方程式</h1><p id="12ea" class="pw-post-body-paragraph kd ke iq kf b kg lz ki kj kk ma km kn ko mb kq kr ks mc ku kv kw md ky kz la ij bi translated">让我们简单地浏览一下更新方程，看看发生了什么，我们想要优化一些函数，<strong class="kf ir"> F </strong>关于一些参数，<strong class="kf ir"> θ </strong>:</p><figure class="mf mg mh mi gt jr gh gi paragraph-image"><div class="gh gi mj"><img src="../Images/2f180106a1b4a810ac8604c3a72f6fcf.png" data-original-src="https://miro.medium.com/v2/resize:fit:646/format:webp/1*uglvYNH6HUGGUjTyy697JA.png"/></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">图片作者:ES更新方程式</p></figure><p id="2604" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在每个时间步<strong class="kf ir"> t </strong>，我们更新上面定义的参数。这里最重要的特征是得到上升方向的和项。这里，我们计算围绕当前位置随机采样的<strong class="kf ir"> n </strong>个点的总体的加权和<strong class="kf ir"> θ_t </strong>，其中权重是这些点的函数值。采样是通过高斯分布完成的:</p><figure class="mf mg mh mi gt jr gh gi paragraph-image"><div class="gh gi mk"><img src="../Images/1ac5abf948378dce73998369b0722295.png" data-original-src="https://miro.medium.com/v2/resize:fit:262/format:webp/1*LS_OVI_b1WqDAISt2vBe1w.png"/></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">作者图片:高斯分布样本</p></figure><p id="6d72" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">使得更新等式中的<strong class="kf ir"> σ </strong>表示该噪声的标准偏差。最后，<strong class="kf ir"> α </strong>代表学习率，控制我们在求和项确定的方向上行进多远。</p><h1 id="023a" class="lb lc iq bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">先说统计！</h1><p id="3362" class="pw-post-body-paragraph kd ke iq kf b kg lz ki kj kk ma km kn ko mb kq kr ks mc ku kv kw md ky kz la ij bi translated">这里的目标是更深入地理解方程，并预测算法的一些行为。这里又是等式，所以你不需要向上滚动来查阅😊</p><figure class="mf mg mh mi gt jr gh gi paragraph-image"><div class="gh gi mj"><img src="../Images/2f180106a1b4a810ac8604c3a72f6fcf.png" data-original-src="https://miro.medium.com/v2/resize:fit:646/format:webp/1*uglvYNH6HUGGUjTyy697JA.png"/></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">图片作者:ES更新方程式</p></figure><p id="cef8" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">一、<strong class="kf ir"> θ_{t+1} </strong>的分布是什么样子的？不幸的是，这取决于被优化的函数<strong class="kf ir"> F </strong>。然而，如果我们假设总和中的项无论如何都具有有限方差，那么该方程遵守中心极限定理，并且我们可以假设当<strong class="kf ir"> n </strong>增加到无穷大时是高斯分布！</p><p id="012b" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在这种情况下，我们可以简单地得到期望和方差的项，并对分布感到相当有信心(注意:我们也假设独立抽样)。</p><figure class="mf mg mh mi gt jr gh gi paragraph-image"><div class="gh gi mj"><img src="../Images/8026256736ed94e42a3b7c543755f54c.png" data-original-src="https://miro.medium.com/v2/resize:fit:646/format:webp/1*lSuBdcdKcGjMeL_FpfZ8yA.png"/></div></figure><figure class="mf mg mh mi gt jr gh gi paragraph-image"><div class="gh gi ml"><img src="../Images/bffcd9f4521f1d7df2e69ae6b34abdce.png" data-original-src="https://miro.medium.com/v2/resize:fit:686/format:webp/1*CZLWNH_7qBNogw2XAEE1xA.png"/></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">作者形象:转型的期待与变异</p></figure><p id="e5a2" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">你可能想知道这一切有什么意义。这些方程似乎没什么用，而且函数本身也有很多不确定性。然而，这也让我们对超参数调整和算法行为有了一些直觉:</p><ul class=""><li id="cd42" class="mm mn iq kf b kg kh kk kl ko mo ks mp kw mq la mr ms mt mu bi translated">较大的<strong class="kf ir"> n </strong>不会对下一个状态的期望值产生影响，但会减少该期望值周围的方差，从而带来更大的稳定性。这可以解释为增加了搜索的信息密度，尽管这样做的代价是增加了计算时间。</li><li id="c19b" class="mm mn iq kf b kg mv kk mw ko mx ks my kw mz la mr ms mt mu bi translated">增加<strong class="kf ir"> α </strong>推动我们沿着上升方向走得更远，这可以允许更快的收敛。然而，它也增加了更新的方差，导致最优解附近的更大抖动。如果太大，甚至会导致算法发散时不稳定。</li><li id="3b90" class="mm mn iq kf b kg mv kk mw ko mx ks my kw mz la mr ms mt mu bi translated">改变<strong class="kf ir"> σ </strong>不太确定，因为它被输入到<strong class="kf ir"> F </strong>。然而，直觉上它增加了搜索空间，这允许算法逃脱局部最大值陷阱。这是以降低信息密度为代价的，因此增加<strong class="kf ir"> σ </strong>可能会受益于<strong class="kf ir"> n </strong>的增加。</li></ul><h1 id="b4f6" class="lb lc iq bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">实验</h1><p id="c10d" class="pw-post-body-paragraph kd ke iq kf b kg lz ki kj kk ma km kn ko mb kq kr ks mc ku kv kw md ky kz la ij bi translated">为了简单起见，让我们在测试中使用函数，<strong class="kf ir"> f(x) = -x </strong>。作为参考，下面是学习曲线的大致情况:</p><figure class="mf mg mh mi gt jr gh gi paragraph-image"><div class="gh gi na"><img src="../Images/f282be766bea76caa8ff122ba1b3fcb8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1084/format:webp/1*RcsNiJ1ZQvxzq66aduUyVQ.png"/></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">图片作者:ES f(x)=-x的学习曲线</p></figure><p id="51d8" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">该算法有两个阶段，过渡状态和稳定状态，在过渡状态下，算法移向解，在稳定状态下，算法已经到达解。<strong class="kf ir">理想情况下，我们希望过渡状态尽可能短，同时在稳定状态下具有最小方差</strong>。让我们看看当我们改变超参数时，这些指标是如何变化的。</p><figure class="mf mg mh mi gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nb"><img src="../Images/9d5d8874a66491068c3747b1115eb072.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*itOxBt96f0Dq2T8yseZjLg.png"/></div></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">图片由作者提供:收敛时间和变化的稳态标准偏差<strong class="bd nc"> α </strong></p></figure><p id="8907" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">正如所料，增加学习率<strong class="kf ir"> α </strong>，以增加稳态标准偏差为代价，减少了收敛时间。由于收敛时间似乎趋于平稳，增加<strong class="kf ir"> α </strong>超过一个点将导致收敛时间仅有微小的改善，代价是稳态性能下降。</p><figure class="mf mg mh mi gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nd"><img src="../Images/8bb61872b0bfb02e3e673ae446bf4bd2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*V_kkMkqoXpV9LoEwaaVDBQ.png"/></div></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">作者图片:收敛时间和稳态标准偏差，用于改变<strong class="bd nc"> σ </strong></p></figure><p id="caf0" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">增加采样噪声标准偏差，<strong class="kf ir"> σ </strong>，会增加收敛时间。这是预料之中的，因为每次更新的信息密度较小，这也反映在总体增加的稳态标准偏差中；尽管在非常小的<strong class="kf ir"> σ </strong>下偏差减小是出乎意料的。</p><figure class="mf mg mh mi gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi ne"><img src="../Images/5a4f61eb3471d9b6b703b2c33a80cfc5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hqD3PjasjKyxPxRG5yF-vg.png"/></div></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">作者图片:收敛时间和稳态标准偏差，用于改变<strong class="bd nc"> n </strong></p></figure><p id="9e17" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">增加人口规模，<strong class="kf ir"> n </strong>，也符合预期。减少收敛时间和稳态标准偏差与增加信息密度的想法一致，尽管与其他超参数的变化相比，这种影响很快达到平稳状态。</p></div><div class="ab cl nf ng hu nh" role="separator"><span class="ni bw bk nj nk nl"/><span class="ni bw bk nj nk nl"/><span class="ni bw bk nj nk"/></div><div class="ij ik il im in"><p id="e582" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">ES实现起来非常简单，并且可以产生很好的效果。下次你需要优化一个函数(提示:机器学习👀)，记住这个工具！</p><p id="33b6" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我希望你觉得这篇文章很有见地，并让我知道你的想法😁</p></div></div>    
</body>
</html>