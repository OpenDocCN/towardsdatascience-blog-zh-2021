<html>
<head>
<title>Decision Tree Classifier explained in real-life: picking a vacation destination</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">决策树分类器在现实生活中的解释:选择度假目的地</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/decision-tree-classifier-explained-in-real-life-picking-a-vacation-destination-6226b2b60575?source=collection_archive---------0-----------------------#2021-06-28">https://towardsdatascience.com/decision-tree-classifier-explained-in-real-life-picking-a-vacation-destination-6226b2b60575?source=collection_archive---------0-----------------------#2021-06-28</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="0fc2" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsdatascience.com/tagged/getting-started" rel="noopener" target="_blank">入门</a></h2><div class=""/><div class=""><h2 id="ede6" class="pw-subtitle-paragraph jw iz iq bd b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn dk translated">决策树是一种受监督的机器学习算法，它使用一组规则来做出决策，类似于人类做出决策的方式。</h2></div><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ko"><img src="../Images/17f06901f4cca0000b7c5280f0ef3d71.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uxYR0d4QSAcaEk-kh_qk4g.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">图片作者。</p></figure><p id="aa0b" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated"><em class="ma">这是致力于基于树的算法系列的第一篇文章，基于树的算法是一组广泛使用的监督机器学习算法。</em></p><p id="a27f" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated"><em class="ma">如果你想看决策树、随机森林和梯度推进决策树，请继续关注，用现实生活中的例子和一些Python代码来解释。</em></p></div><div class="ab cl mb mc hu md" role="separator"><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg"/></div><div class="ij ik il im in"><p id="7804" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated"><strong class="lg ja">决策树</strong>是一种受监督的机器学习算法，它使用一组规则来做出决策，类似于人类做出决策的方式。</p><p id="5417" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">思考机器学习分类算法的一种方式是，它是为了做出决策而构建的。</p><p id="b175" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">你通常说<em class="ma">模型预测</em>新的、以前从未见过的输入的类别，但是在幕后，算法必须<em class="ma">决定</em>分配哪个类别。</p><p id="ceab" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">一些分类算法是概率性的，如<a class="ae mi" href="https://en.wikipedia.org/wiki/Naive_Bayes_classifier" rel="noopener ugc nofollow" target="_blank">朴素贝叶斯</a>，但也有一种基于规则的方法。</p><p id="a4a1" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">我们人类也一直在做基于规则的决定。</p><p id="eb8d" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">当你计划下一个假期时，你使用基于规则的方法。你可能会根据你要去度假多长时间、可用的预算或者你的大家庭是否会一起来选择不同的目的地。</p><p id="9971" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">这些问题的答案决定了最终的决定。如果你根据你对每个问题的回答不断缩小可用度假目的地的范围，你可以把这个决策过程想象成一棵(决策)树。</p><h1 id="eee2" class="mj mk iq bd ml mm mn mo mp mq mr ms mt kf mu kg mv ki mw kj mx kl my km mz na bi translated">一棵做决定的树</h1><p id="1a2e" class="pw-post-body-paragraph le lf iq lg b lh nb ka lj lk nc kd lm ln nd lp lq lr ne lt lu lv nf lx ly lz ij bi translated"><strong class="lg ja">决策树</strong>可以执行分类和回归任务，所以你会看到作者称它们为<strong class="lg ja"> CART算法:分类和回归树</strong>。这是一个总括术语，适用于所有基于树的算法，而不仅仅是决策树。</p><p id="d681" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">但是让我们把重点放在决策树上进行分类。</p><p id="6c26" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated"><strong class="lg ja">决策树</strong>背后的直觉是，你使用数据集特征来创建<em class="ma">是/否</em>问题，并不断分割数据集，直到你分离出属于每个类的所有数据点。</p><p id="f60e" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">在这个过程中，你将数据组织成一个树形结构。</p><p id="534b" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">每次你问一个问题，你就给树增加了一个节点。而第一个节点称为<strong class="lg ja">根节点</strong>。</p><p id="af3d" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated"><em class="ma">提问的结果</em>根据一个特性的值分割数据集，并创建新的节点。</p><p id="dbdc" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">如果您决定在分割后停止流程，那么最后创建的节点称为<strong class="lg ja">叶节点</strong>。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ng"><img src="../Images/9384cf478a86b8c26a0f6f57f0ec25ba.png" data-original-src="https://miro.medium.com/v2/resize:fit:1298/format:webp/1*Ehq7_oHrR_eIpzTGBFcyXA.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">包含树节点、根节点和两个叶节点的决策树示例。(图片由作者提供)</p></figure><p id="15a6" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">每次你<em class="ma">回答一个问题</em>，你也在创建分支并将特征空间分割成不相交的<strong class="lg ja">区域</strong>【1】。</p><p id="76c5" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">树的一个分支具有对应于回答<em class="ma">是</em>到<em class="ma">问题</em>的所有数据点，这是前一个节点中隐含的规则。另一个分支有一个包含剩余数据点的节点。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi nh"><img src="../Images/f857bf2ea144128b4e1517018d2be4e3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CU5uGBi8LDXlP0W_H31UQQ.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated"><em class="ni">树中不同区域和节点类型的例子。</em>(图片作者提供)</p></figure><p id="a9bc" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">这样，通过树中的每个分裂或分支来缩小特征空间，并且每个数据点将只属于一个区域。</p><p id="d6d9" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">目标是继续<em class="ma">分割</em>特征空间，并应用规则，直到没有更多规则可应用或没有数据点留下。</p><p id="3206" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">然后，该给每个叶节点中的所有数据点分配一个类了。</p><h1 id="a243" class="mj mk iq bd ml mm mn mo mp mq mr ms mt kf mu kg mv ki mw kj mx kl my km mz na bi translated">分配课程</h1><p id="3e88" class="pw-post-body-paragraph le lf iq lg b lh nb ka lj lk nc kd lm ln nd lp lq lr ne lt lu lv nf lx ly lz ij bi translated">该算法试图完全分离数据集，使得所有叶节点，即不再进一步分割数据的节点，属于单个类。这些被称为<strong class="lg ja">纯叶节点。</strong></p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi nj"><img src="../Images/0db25050058620f88b645c73658b2b70.png" data-original-src="https://miro.medium.com/v2/resize:fit:1228/format:webp/1*BSK8Phs8td8WCuD-jwdjzA.png"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated"><em class="ni">只有纯叶节点的树的例子。</em>(图片作者提供)</p></figure><p id="a6dc" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">但是大多数情况下，你最终会得到混合的叶节点，其中并不是所有的数据点都属于同一个类。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi nk"><img src="../Images/5b7614ebdbf1c563a8d1225b7ed40a10.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KOAC6DwDsSX1tpHHg-FYhQ.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated"><em class="ni">在将最终类分配给每个节点之前，具有纯叶节点和混合叶节点的树的示例。</em>(图片作者提供)</p></figure><p id="7b6a" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">最终，<strong class="lg ja">算法只能给每个叶节点中的数据点分配一个类别</strong>。</p><p id="36c7" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">因为该节点中的所有数据点都具有相同的类。</p><p id="cb93" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">但是对于混合叶节点，该算法在该节点的所有数据点中分配<strong class="lg ja">最常见的类别</strong>。</p><h1 id="6358" class="mj mk iq bd ml mm mn mo mp mq mr ms mt kf mu kg mv ki mw kj mx kl my km mz na bi translated">很难造出完美的树</h1><p id="079d" class="pw-post-body-paragraph le lf iq lg b lh nb ka lj lk nc kd lm ln nd lp lq lr ne lt lu lv nf lx ly lz ij bi translated">理想的树是尽可能最小的树，即具有较少的分裂，能够准确地分类所有数据点。</p><p id="b225" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">这听起来很简单，但实际上是一个<a class="ae mi" href="https://en.wikipedia.org/wiki/NP-hardness" rel="noopener ugc nofollow" target="_blank"> NP难题</a>。构建理想的树需要<a class="ae mi" href="https://en.wikipedia.org/wiki/Time_complexity#Polynomial_time" rel="noopener ugc nofollow" target="_blank">多项式时间</a>，随着数据集的增长，多项式时间会呈指数增长。</p><p id="be04" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">例如，对于一个只有10个数据点的数据集和一个具有<a class="ae mi" href="https://en.wikipedia.org/wiki/Big_O_notation" rel="noopener ugc nofollow" target="_blank">二次复杂度</a>，O(n)的算法，该算法执行10*10 = 100次迭代来构建树。将该数据集进一步扩展到100个数据点，算法将执行的迭代次数将增加到10，000次。</p><blockquote class="nl nm nn"><p id="7eb0" class="le lf ma lg b lh li ka lj lk ll kd lm no lo lp lq np ls lt lu nq lw lx ly lz ij bi translated">寻找<em class="iq"/>最佳树在理论上是理想的，但随着数据集的增长，这在计算上变得不可行！</p></blockquote><p id="5168" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">为了将这个NP难问题转化为计算上可行的问题，该算法使用了一种<a class="ae mi" href="https://en.wikipedia.org/wiki/Greedy_algorithm" rel="noopener ugc nofollow" target="_blank"> <strong class="lg ja">贪婪方法</strong> </a>来构建下一个最佳树。</p><p id="4b6d" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">一种<strong class="lg ja">贪婪的方法</strong>做出局部最优的决定来挑选每个分割中使用的特征，而不是试图做出最佳的整体决定【2】。</p><p id="3b4d" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">因为它针对本地决策进行了优化，所以它只关注手边的节点，特别是对该节点最好的节点。因此，它不需要探索该节点及其以外的所有可能的拆分。</p><h1 id="9621" class="mj mk iq bd ml mm mn mo mp mq mr ms mt kf mu kg mv ki mw kj mx kl my km mz na bi translated">选择最佳分割</h1><p id="cc64" class="pw-post-body-paragraph le lf iq lg b lh nb ka lj lk nc kd lm ln nd lp lq lr ne lt lu lv nf lx ly lz ij bi translated">在每次分割时，该算法试图将数据集分割成尽可能小的子集[2]。所以，像任何其他机器学习算法一样，目标是<strong class="lg ja">尽可能最小化损失函数</strong>。</p><p id="7b85" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">分类算法的一个流行损失函数是<a class="ae mi" rel="noopener" target="_blank" href="/stochastic-gradient-descent-explained-in-real-life-predicting-your-pizzas-cooking-time-b7639d5e6a32">随机梯度下降</a>，但是它要求损失函数是可微分的。所以，在这种情况下这不是一个选项。</p><p id="6269" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">但是，由于您正在分离属于不同类的数据点，损失函数应该根据分割前后属于每个类的数据点的比例来评估分割。</p><blockquote class="nl nm nn"><p id="6afe" class="le lf ma lg b lh li ka lj lk ll kd lm no lo lp lq np ls lt lu nq lw lx ly lz ij bi translated">决策树使用损失函数来评估基于结果节点的<em class="iq">纯度</em>的分割。</p></blockquote><p id="6b95" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">换句话说，你需要一个损失函数来评估基于结果节点的纯度的分割。一个损失函数，比较分裂前后的阶级分布[2]，像<strong class="lg ja">基尼杂质</strong>和<strong class="lg ja">熵</strong>。</p><h2 id="4f25" class="nr mk iq bd ml ns nt dn mp nu nv dp mt ln nw nx mv lr ny nz mx lv oa ob mz iw bi translated">基尼杂质</h2><p id="3d1b" class="pw-post-body-paragraph le lf iq lg b lh nb ka lj lk nc kd lm ln nd lp lq lr ne lt lu lv nf lx ly lz ij bi translated"><a class="ae mi" href="https://en.wikipedia.org/wiki/Decision_tree_learning#Gini_impurity" rel="noopener ugc nofollow" target="_blank">基尼系数</a>是衡量不同阶层间差异的指标[1]。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/86b8794214fccd7cbaeaf14f574965b0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1148/format:webp/1*0HGZlU81sbTasamgcJUGdw.png"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">节点的基尼杂质。</p></figure><h2 id="21b0" class="nr mk iq bd ml ns nt dn mp nu nv dp mt ln nw nx mv lr ny nz mx lv oa ob mz iw bi translated">熵</h2><p id="3396" class="pw-post-body-paragraph le lf iq lg b lh nb ka lj lk nc kd lm ln nd lp lq lr ne lt lu lv nf lx ly lz ij bi translated">与基尼不纯相似，熵是节点内混沌的一种度量。在决策树的上下文中，混乱是指在一个节点上，所有的类都平等地出现在数据中。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi od"><img src="../Images/d4e4deb5b5b4929eb27cce73dabfb513.png" data-original-src="https://miro.medium.com/v2/resize:fit:1088/format:webp/1*LOMIhf5QSCyTvsr60UCcyg.png"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">节点的熵。</p></figure><p id="cef5" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">使用熵作为损失函数，仅当每个结果节点的熵低于父节点的熵时，才执行分裂。否则，拆分不是<em class="ma">局部</em> <em class="ma">最优</em>。</p><h1 id="e518" class="mj mk iq bd ml mm mn mo mp mq mr ms mt kf mu kg mv ki mw kj mx kl my km mz na bi translated">为什么要使用决策树</h1><p id="2e7f" class="pw-post-body-paragraph le lf iq lg b lh nb ka lj lk nc kd lm ln nd lp lq lr ne lt lu lv nf lx ly lz ij bi translated">尽管决策树是一种简单的算法，但它有几个优点:</p><ul class=""><li id="d13b" class="oe of iq lg b lh li lk ll ln og lr oh lv oi lz oj ok ol om bi translated"><strong class="lg ja">可解释性</strong>你可以将决策树形象化。</li><li id="1d8c" class="oe of iq lg b lh on lk oo ln op lr oq lv or lz oj ok ol om bi translated"><strong class="lg ja">无需预处理</strong>在构建模型之前，您不需要准备数据。</li><li id="443a" class="oe of iq lg b lh on lk oo ln op lr oq lv or lz oj ok ol om bi translated"><strong class="lg ja">数据健壮性</strong>该算法可以很好地处理所有类型的数据。</li></ul><h2 id="d4d8" class="nr mk iq bd ml ns nt dn mp nu nv dp mt ln nw nx mv lr ny nz mx lv oa ob mz iw bi translated">可解释性:可视化树</h2><p id="749a" class="pw-post-body-paragraph le lf iq lg b lh nb ka lj lk nc kd lm ln nd lp lq lr ne lt lu lv nf lx ly lz ij bi translated">基于树的算法的一个最大的优点是，你可以实际可视化模型。</p><p id="a23a" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">你可以<em class="ma">看到</em>算法做出的决定，以及它如何对不同的数据点进行分类。</p><p id="f0a0" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">这是一个主要的优势，因为大多数算法都像黑盒一样工作，很难清楚地查明是什么让算法预测到特定的结果。</p><h2 id="b4a3" class="nr mk iq bd ml ns nt dn mp nu nv dp mt ln nw nx mv lr ny nz mx lv oa ob mz iw bi translated">不需要预处理</h2><p id="1e9d" class="pw-post-body-paragraph le lf iq lg b lh nb ka lj lk nc kd lm ln nd lp lq lr ne lt lu lv nf lx ly lz ij bi translated">一些机器学习算法要求特征值尽可能相似，因此算法可以最好地解释这些特征的变化如何影响目标。</p><p id="573b" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">最常见的预处理要求是特征<em class="ma">归一化，</em>因此相同比例的所有特征以及这些值的任何变化都具有相同的比例权重。</p><p id="d7b9" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">基于树的算法中的规则是围绕每个单独的特征建立的，而不是考虑整个特征集。每个<em class="ma">决策</em>都是一次查看一个特征，因此它们的值不需要标准化。</p><h2 id="68a5" class="nr mk iq bd ml ns nt dn mp nu nv dp mt ln nw nx mv lr ny nz mx lv oa ob mz iw bi translated">数据稳健性</h2><p id="b316" class="pw-post-body-paragraph le lf iq lg b lh nb ka lj lk nc kd lm ln nd lp lq lr ne lt lu lv nf lx ly lz ij bi translated">基于树的算法非常擅长处理不同的数据类型。你的数据集可以混合数字和分类数据，你不需要对任何分类特征进行编码。</p><p id="a6a8" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">这是基于树的算法自己处理的预处理清单上的一项。</p><h1 id="8c9f" class="mj mk iq bd ml mm mn mo mp mq mr ms mt kf mu kg mv ki mw kj mx kl my km mz na bi translated">🛩🏝选择度假目的地</h1><p id="9385" class="pw-post-body-paragraph le lf iq lg b lh nb ka lj lk nc kd lm ln nd lp lq lr ne lt lu lv nf lx ly lz ij bi translated">计划下一个假期可能很有挑战性。假期总是不够长，有预算限制，有时大家庭想一起来，这使得后勤工作更加复杂。</p><p id="f9a1" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">当需要做一个涉及太多变量的决策时，你喜欢从算法中寻求第二种意见。挑选一个度假目的地是一个完美的例子！</p><p id="7d42" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">每当你开始计划假期时，你总是会考虑到:</p><ul class=""><li id="026d" class="oe of iq lg b lh li lk ll ln og lr oh lv oi lz oj ok ol om bi translated">假期的持续时间，</li><li id="5621" class="oe of iq lg b lh on lk oo ln op lr oq lv or lz oj ok ol om bi translated">个人预算，</li><li id="6b9e" class="oe of iq lg b lh on lk oo ln op lr oq lv or lz oj ok ol om bi translated">天气预报，</li><li id="13f4" class="oe of iq lg b lh on lk oo ln op lr oq lv or lz oj ok ol om bi translated">如果你的大家庭要加入，</li><li id="c220" class="oe of iq lg b lh on lk oo ln op lr oq lv or lz oj ok ol om bi translated">如果你喜欢冒险，想要探索新的地方。</li></ul><p id="905c" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">因为决策树据说是模仿人类如何做决定，这就是你正在使用的算法。</p><p id="1b4e" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">好处是，最终，你将能够可视化决策树，并且<em class="ma">看到</em>算法如何选择目的地。</p><p id="cbe2" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">仔细考虑你以前度假的决策过程和你经常考虑的标准，你建立了一个数据集。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi os"><img src="../Images/9aa715f55df5160bef27c231b97f07a0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*A8u6I-QqHoKGRMMrF55zWw.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">来自先前休假决策流程的数据集。(图片由作者提供)</p></figure><p id="9431" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">尽管决策树算法可以处理不同的数据类型，ScikitLearn当前的实现<a class="ae mi" href="https://github.com/scikit-learn/scikit-learn/issues/5442" rel="noopener ugc nofollow" target="_blank">不支持分类数据</a>。</p><p id="28fc" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">要使用来自<a class="ae mi" href="https://scikit-learn.org/stable/" rel="noopener ugc nofollow" target="_blank"> ScikitLearn </a>的决策树分类器，您不能跳过预处理步骤，需要在训练模型之前对所有分类特征和目标进行编码。</p><pre class="kp kq kr ks gt ot ou ov ow aw ox bi"><span id="39b3" class="nr mk iq ou b gy oy oz l pa pb">import numpy as np<br/>from sklearn import preprocessing</span><span id="815d" class="nr mk iq ou b gy pc oz l pa pb"><br/>def encode_feature(array):<br/>    <em class="ma">""" Encode a categorical array into a number array<br/>    <br/>    :param array: array to be encoded<br/>    :return: numerical array<br/>    """<br/>  <br/>    </em>encoder = preprocessing.LabelEncoder()<br/>    encoder.fit(array)<br/>    return encoder.transform(array)<br/></span><span id="ac39" class="nr mk iq ou b gy pc oz l pa pb">feature_names = ['number_days', 'family_joining', 'personal_budget', 'weather_forecast', 'explore_new_places']</span><span id="0a6f" class="nr mk iq ou b gy pc oz l pa pb">class_names = ['Countryside', 'Beach']</span><span id="a3d7" class="nr mk iq ou b gy pc oz l pa pb">features = np.array([[10, 'Yes', 950, 75, 'Yes'],<br/>                     [10, 'Yes', 250, 78, 'Yes'],<br/>                     [7, 'Yes', 600, 80, 'No'],<br/>                     [8, 'Yes', 750, 67, 'Yes'],<br/>                     [10, 'Yes', 800, 73, 'Yes'],<br/>                     [8, 'Yes', 850, 64, 'Yes'],<br/>                     [15, 'No', 350, 78, 'No'],<br/>                     [8, 'Yes', 850, 81, 'Yes'],<br/>                     [6, 'No', 750, 59, 'Yes'],<br/>                     [12, 'Yes', 1050, 54, 'Yes'],<br/>                     [10, 'No', 230, 74, 'No'],<br/>                     [3, 'Yes', 630, 74, 'Yes'],<br/>                     [10, 'Yes', 830, 74, 'No'],<br/>                     [12, 'No', 730, 52, 'Yes']])<br/></span><span id="38f4" class="nr mk iq ou b gy pc oz l pa pb"># Encoding categorical features<br/>features[:, 1] = encode_feature(features[:, 1])<br/>features[:, 4] = encode_feature(features[:, 4])</span><span id="2826" class="nr mk iq ou b gy pc oz l pa pb">targets = np.array(['Countryside','Beach','Beach','Countryside',<br/>                    'Beach', 'Countryside', 'Beach','Countryside',<br/>                    'Beach', 'Beach', 'Countryside','Countryside',<br/>                    'Beach', 'Beach'])</span><span id="c19b" class="nr mk iq ou b gy pc oz l pa pb">targets = encode_feature(targets)</span></pre><p id="f261" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">预处理:✅完成</p><p id="1738" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">现在是时候构建并可视化决策树了。</p><pre class="kp kq kr ks gt ot ou ov ow aw ox bi"><span id="ed38" class="nr mk iq ou b gy oy oz l pa pb">import pandas as pd<br/>from sklearn import tree<br/>import matplotlib.pyplot as plt<br/>from sklearn.model_selection import train_test_split</span><span id="c726" class="nr mk iq ou b gy pc oz l pa pb"><br/>def print_feature_importance(names_array, importances_array):</span><span id="ffcf" class="nr mk iq ou b gy pc oz l pa pb"><em class="ma">    """ Prints out a feature importance array as a dataframe. """</em></span><span id="e24f" class="nr mk iq ou b gy pc oz l pa pb">    importances = pd.DataFrame(data=names_array)<br/>    importances[1] = importances_array</span><span id="90b8" class="nr mk iq ou b gy pc oz l pa pb">    importances = importances.T<br/>    importances.drop(0, axis=0, inplace=True)<br/>    importances.columns = feature_names<br/>    <br/>    print(str(importances.reset_index(drop=True)))</span><span id="b288" class="nr mk iq ou b gy pc oz l pa pb">def build_tree(features, targets, feature_names, class_names):</span><span id="0ef9" class="nr mk iq ou b gy pc oz l pa pb"><em class="ma">    """ Builds a decision tree.<br/>        Prints out the decision tree 1) as a plot, 2) as text.<br/>        Also outputs: 1) feature importance, 2) training set and test set mean accuracy of tree<br/>    <br/>        :param features: model features<br/>        :param targets: model targets<br/>        :param feature_names: names of the dataset features<br/>    """<br/>  <br/>    </em>train_features, test_features, train_targets, test_targets = train_test_split(features, targets, test_size=0.2, random_state=123)</span><span id="1a37" class="nr mk iq ou b gy pc oz l pa pb"><br/>    decision_tree = tree.DecisionTreeClassifier(random_state=456)<br/>    decision_tree = decision_tree.fit(train_features, train_targets)</span><span id="d034" class="nr mk iq ou b gy pc oz l pa pb">    # Visualizing the decision tree<br/>    <br/>    # 1. Saving the image of the decision as a png   </span><span id="58b4" class="nr mk iq ou b gy pc oz l pa pb">    plt.subplots(figsize=(17, 12))<br/>    tree.plot_tree(decision_tree, feature_names=feature_names, filled=True, rounded=True, class_names=class_names)<br/>    plt.savefig("decision_tree.png")</span><span id="0134" class="nr mk iq ou b gy pc oz l pa pb">    # 2. Output the tree as text in the console<br/>    tree_as_text = tree.export_text(decision_tree, feature_names=feature_names)<br/>    print(tree_as_text)</span><span id="2d3f" class="nr mk iq ou b gy pc oz l pa pb">    # Feature Importance<br/>    # Turns the feature importance array into a dataframe, so it has a table-like output format<br/>    print_feature_importance(feature_names, decision_tree.feature_importances_)</span><span id="63e3" class="nr mk iq ou b gy pc oz l pa pb">    # Training and test mean accuracy<br/>    train_error = np.round(decision_tree.score(train_features, train_targets), 2)<br/>    test_error = np.round(decision_tree.score(test_features, test_targets), 2)<br/>    <br/>    print("Training Set Mean Accuracy = " + str(train_error))<br/>    print("Test Set Mean Accuracy = " + str(test_error))</span><span id="a7fb" class="nr mk iq ou b gy pc oz l pa pb"><br/>build_tree(features, targets, feature_names, class_names)</span></pre><p id="490f" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">这段代码一次做很多事情，所以让我们来解开它。</p><p id="c859" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">它从<strong class="lg ja">使用<a class="ae mi" href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split" rel="noopener ugc nofollow" target="_blank"> train_test_split </a>函数将数据集分割成训练和测试</strong>开始，因此您可以使用未用于训练模型的数据点来测试模型准确性。</p><h1 id="8172" class="mj mk iq bd ml mm mn mo mp mq mr ms mt kf mu kg mv ki mw kj mx kl my km mz na bi translated">构建模型并可视化树</h1><p id="db82" class="pw-post-body-paragraph le lf iq lg b lh nb ka lj lk nc kd lm ln nd lp lq lr ne lt lu lv nf lx ly lz ij bi translated">默认情况下，ScikitLearn使用基尼系数作为损失函数。但是您也可以使用熵作为损失函数，并在<a class="ae mi" href="https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html" rel="noopener ugc nofollow" target="_blank"> DecisionTreeClassifier </a>中调整其他参数。</p><p id="b11f" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">训练好模型后，您可以使用<a class="ae mi" href="https://scikit-learn.org/stable/modules/generated/sklearn.tree.plot_tree.html#sklearn.tree.plot_tree" rel="noopener ugc nofollow" target="_blank"> plot_tree </a>方法可视化生成的决策树，并保存为<em class="ma"> decision_tree.png </em>。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi pd"><img src="../Images/405a73d8d477f6284493d1599d78252e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QUs9OK6bLm9OWDuAXbtfyg.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">通过训练模型生成的决策树。(图片由作者提供)</p></figure><p id="9ce6" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">您也可以在输出控制台中可视化该树，您可以使用<a class="ae mi" href="https://scikit-learn.org/stable/modules/generated/sklearn.tree.export_text.html" rel="noopener ugc nofollow" target="_blank"> export_text </a> <em class="ma"> </em>方法<em class="ma">。</em></p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi pe"><img src="../Images/b4773769fbd1f50019cd1cd67b221a6c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1030/format:webp/1*I6C0TD-0BA6Kdvnp2jHq2Q.png"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">在输出控制台中将决策树可视化为文本。(图片由作者提供)</p></figure><h1 id="8a42" class="mj mk iq bd ml mm mn mo mp mq mr ms mt kf mu kg mv ki mw kj mx kl my km mz na bi translated">特征重要性</h1><p id="d3de" class="pw-post-body-paragraph le lf iq lg b lh nb ka lj lk nc kd lm ln nd lp lq lr ne lt lu lv nf lx ly lz ij bi translated">但是那个决策树有问题！</p><p id="adcc" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">您注意到功能<em class="ma"> explore_new_places </em>没有出现在任何地方。即使你确定这是你决策过程中重要的一部分。</p><p id="e959" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">为了弄清这一点并理解为什么模型中没有使用<em class="ma"> explore_new_places </em>，您可以在决策树模型中查找<a class="ae mi" href="https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier.feature_importances_" rel="noopener ugc nofollow" target="_blank"> feature_importances_ </a>属性。这将告诉你<em class="ma">每个</em>特征对模型准确性的贡献有多大。</p><p id="d725" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated"><a class="ae mi" href="https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier.feature_importances_" rel="noopener ugc nofollow" target="_blank"> feature_importances_ </a>属性只是一个值数组，每个值对应于模型的一个特征，其顺序与输入数据集的顺序相同。</p><p id="aaec" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">因此，为了更好的可读性，您可以决定创建函数print_feature_importance，并将值数组从<a class="ae mi" href="https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier.feature_importances_" rel="noopener ugc nofollow" target="_blank"> feature_importances_ </a>属性转换为<a class="ae mi" href="https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html" rel="noopener ugc nofollow" target="_blank"> dataframe </a>，并使用特性名称作为标题。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi pf"><img src="../Images/d71be87a594054e85d0811ec036a2251.png" data-original-src="https://miro.medium.com/v2/resize:fit:1156/format:webp/1*x-qNDWffJOTEEwRy80q5IA.png"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">可视化特征重要性。(图片由作者提供)</p></figure><p id="ee28" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated"><em class="ma"> explore_new_places </em>的要素重要性为0，这意味着在预测中根本不使用它。</p><p id="8309" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">乍一看，你可能会想，<em class="ma">我可以去掉这个特性。</em></p><p id="3228" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated"><strong class="lg ja">但是特性的重要性并不一定意味着该特性永远不会在模型中使用。</strong>这只意味着它没有在这个树中使用，这是一个特定的训练-测试分割。</p><p id="668b" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">所以你不能马上消除这个特性。</p><p id="41c7" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">为了确认<em class="ma"> explore_new_places </em>与模型无关，您可以用数据集的不同训练测试分割构建几棵树，并检查<em class="ma"> explore_new_places </em>是否仍然具有零重要性。</p><h1 id="22b9" class="mj mk iq bd ml mm mn mo mp mq mr ms mt kf mu kg mv ki mw kj mx kl my km mz na bi translated">模型性能</h1><p id="f8e9" class="pw-post-body-paragraph le lf iq lg b lh nb ka lj lk nc kd lm ln nd lp lq lr ne lt lu lv nf lx ly lz ij bi translated">最后，为了评估算法的性能，您可以使用<a class="ae mi" href="https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier.score" rel="noopener ugc nofollow" target="_blank">得分</a> <em class="ma"> </em>方法计算训练集和测试集上预测的平均准确度。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi pg"><img src="../Images/e6fe0e56a410fb55c9fae239d7ad1b57.png" data-original-src="https://miro.medium.com/v2/resize:fit:1394/format:webp/1*vp1lN2FxlVYrHjXPfYcjcA.png"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">训练集和测试集预测的平均准确性。(图片由作者提供)</p></figure><p id="94e3" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">正如你所看到的，这个模型被过度拟合并记住了训练集。由于测试集的平均准确率为67%，它不能很好地推广到以前从未见过的观察结果。</p><h1 id="6c3b" class="mj mk iq bd ml mm mn mo mp mq mr ms mt kf mu kg mv ki mw kj mx kl my km mz na bi translated">一个决策树是不够的</h1><p id="9f12" class="pw-post-body-paragraph le lf iq lg b lh nb ka lj lk nc kd lm ln nd lp lq lr ne lt lu lv nf lx ly lz ij bi translated">尽管决策树有其优势，但它不能提供与其他分类和回归算法相同的精确度。</p><p id="02b8" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated"><strong class="lg ja">决策树容易过拟合</strong>。如果您构建了一个非常高的树，分割特征集直到您得到纯叶节点，您可能会过度适应训练集。生成的树如此复杂，以至于很难<em class="ma">阅读</em>和解释。</p><p id="9f38" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">另一方面，如果你的决策树很小，它会对数据进行欠拟合，导致高偏差。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi ph"><img src="../Images/b1c5cb96d5e11998082f45734fabe347.png" data-original-src="https://miro.medium.com/v2/resize:fit:1350/format:webp/1*cj-pzRLTVxrlkVc25WwtcQ.png"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">高偏差和高方差的决策树示例。(图片由作者提供)</p></figure><p id="daab" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">决策树在它们可以处理的数据类型方面是健壮的，但是算法本身不是很健壮。数据的微小变化可能会彻底改变树，从而改变最终结果[1]。</p><h1 id="96c3" class="mj mk iq bd ml mm mn mo mp mq mr ms mt kf mu kg mv ki mw kj mx kl my km mz na bi translated">隐藏在森林中的力量</h1><p id="2774" class="pw-post-body-paragraph le lf iq lg b lh nb ka lj lk nc kd lm ln nd lp lq lr ne lt lu lv nf lx ly lz ij bi translated">与其他机器学习算法相比，一个单独的<strong class="lg ja">决策树</strong>本身的准确性就很低。单独一棵树通常不会产生最佳预测，但是树结构使得控制<a class="ae mi" rel="noopener" target="_blank" href="/bias-variance-tradeoff-in-machine-learning-models-a-practical-example-cf02fb95b15d">偏差-方差权衡</a>变得容易。</p><blockquote class="nl nm nn"><p id="c5fc" class="le lf ma lg b lh li ka lj lk ll kd lm no lo lp lq np ls lt lu nq lw lx ly lz ij bi translated">一个单独的决策树不够强大，但是整个森林足够强大！</p></blockquote><p id="f9cc" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">在结合多个树并控制偏差或方差的算法中，如<a class="ae mi" href="https://en.wikipedia.org/wiki/Random_forest" rel="noopener ugc nofollow" target="_blank">随机森林</a>，与单个决策树相比，该模型具有更好的性能。</p><p id="4b69" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated"><em class="ma">请继续关注本系列的下一篇文章</em>，因为它们是关于增压和装袋的。这些技术应用于基于树的算法，并分别解决偏差和方差问题。</p><h1 id="3e35" class="mj mk iq bd ml mm mn mo mp mq mr ms mt kf mu kg mv ki mw kj mx kl my km mz na bi translated">结论</h1><p id="87ea" class="pw-post-body-paragraph le lf iq lg b lh nb ka lj lk nc kd lm ln nd lp lq lr ne lt lu lv nf lx ly lz ij bi translated">决策树是一种基于规则的分类和回归问题的方法。它们使用每个要素中的值将数据集分割成一个点，在该点处，具有相同类的所有数据点被分组在一起。</p><p id="482a" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">然而，在可解释性和性能之间有一个明显的平衡。</p><p id="451f" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">你可以很容易地想象和<em class="ma"> </em>解读一棵小树，但是<em class="ma"> </em>它有很高的方差。训练集中的微小变化，可能会导致完全不同的树和完全不同的预测。</p><p id="5537" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">另一方面，具有多个分支的高树产生更好的分类。但很可能是在记忆训练数据集。所以它不擅长对从未见过的数据进行分类。</p></div><div class="ab cl mb mc hu md" role="separator"><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg"/></div><div class="ij ik il im in"><p id="ca0b" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">希望你喜欢学习决策树！</p><p id="6c3a" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">敬请关注本系列的下一篇文章！接下来的文章将探索基于树的集成算法，这些算法使用Boosting和Bagging技术来控制偏差和方差。</p><p id="8a8b" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated"><em class="ma">感谢阅读！</em></p><h1 id="4183" class="mj mk iq bd ml mm mn mo mp mq mr ms mt kf mu kg mv ki mw kj mx kl my km mz na bi translated">参考</h1><ol class=""><li id="46fc" class="oe of iq lg b lh nb lk nc ln pi lr pj lv pk lz pl ok ol om bi translated">加雷斯·詹姆斯，丹妮拉·威滕，特雷弗·哈斯蒂，罗伯特·蒂布拉尼。(2013).统计学习导论:在r .纽约的应用</li><li id="e1c7" class="oe of iq lg b lh on lk oo ln op lr oq lv or lz pl ok ol om bi translated">页（page的缩写）tan m . stein Bach和V. Kumar。(2005)数据挖掘简介。<em class="ma">艾迪森·卫斯理</em></li></ol></div></div>    
</body>
</html>