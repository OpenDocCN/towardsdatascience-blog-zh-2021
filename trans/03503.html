<html>
<head>
<title>XGBoost: Extreme Gradient Boosting — How to Improve on Regular Gradient Boosting?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">XGBoost:极端梯度增强—如何改进常规梯度增强？</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/xgboost-extreme-gradient-boosting-how-to-improve-on-regular-gradient-boosting-5c6acf66c70a?source=collection_archive---------3-----------------------#2021-03-21">https://towardsdatascience.com/xgboost-extreme-gradient-boosting-how-to-improve-on-regular-gradient-boosting-5c6acf66c70a?source=collection_archive---------3-----------------------#2021-03-21</a></blockquote><div><div class="fc ii ij ik il im"/><div class="in io ip iq ir"><h2 id="d8dd" class="is it iu bd b dl iv iw ix iy iz ja dk jb translated" aria-label="kicker paragraph">机器学习</h2><div class=""/><div class=""><h2 id="b7cd" class="pw-subtitle-paragraph ka jd iu bd b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr dk translated">详细了解这两种算法之间的差异，以及何时应该选择其中一种算法</h2></div><figure class="kt ku kv kw gu kx gi gj paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="gi gj ks"><img src="../Images/36aa2d6e71174a1a5dad574c544b20e0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*66fzuTlxboYEgTox3r9ZvA.png"/></div></div><p class="le lf gk gi gj lg lh bd b be z dk translated">XGBoost。图片由<a class="ae li" href="https://solclover.com/" rel="noopener ugc nofollow" target="_blank">作者</a>提供。</p></figure><h1 id="7ad4" class="lj lk iu bd ll lm ln lo lp lq lr ls lt kj lu kk lv km lw kn lx kp ly kq lz ma bi translated">介绍</h1><p id="7881" class="pw-post-body-paragraph mb mc iu md b me mf ke mg mh mi kh mj mk ml mm mn mo mp mq mr ms mt mu mv mw in bi translated">如果你想成为一名成功的数据科学家，你需要了解各种机器学习算法之间的差异。</p><p id="e9b8" class="pw-post-body-paragraph mb mc iu md b me mx ke mg mh my kh mj mk mz mm mn mo na mq mr ms nb mu mv mw in bi translated">这个故事是研究不同算法如何工作的系列的一部分，并为您提供示例和Python代码，以帮助您踏上数据科学之旅。</p><h1 id="84e2" class="lj lk iu bd ll lm ln lo lp lq lr ls lt kj lu kk lv km lw kn lx kp ly kq lz ma bi translated">内容</h1><ul class=""><li id="6b3c" class="nc nd iu md b me mf mh mi mk ne mo nf ms ng mw nh ni nj nk bi translated">XGBoost所属的算法类别</li><li id="58ab" class="nc nd iu md b me nl mh nm mk nn mo no ms np mw nh ni nj nk bi translated">XGBoost和梯度增强的基础知识</li><li id="efa8" class="nc nd iu md b me nl mh nm mk nn mo no ms np mw nh ni nj nk bi translated">XGBoost和渐变增强构建树的方式不同</li><li id="8932" class="nc nd iu md b me nl mh nm mk nn mo no ms np mw nh ni nj nk bi translated">计算输出值</li><li id="c0e7" class="nc nd iu md b me nl mh nm mk nn mo no ms np mw nh ni nj nk bi translated">XGBoost优化</li></ul><h1 id="37be" class="lj lk iu bd ll lm ln lo lp lq lr ls lt kj lu kk lv km lw kn lx kp ly kq lz ma bi translated">XGBoost属于哪一类算法？</h1><p id="5800" class="pw-post-body-paragraph mb mc iu md b me mf ke mg mh mi kh mj mk ml mm mn mo mp mq mr ms mt mu mv mw in bi translated">极端梯度推进是一种基于树的算法，它位于机器学习的监督分支之下。虽然它可以用于分类和回归问题，但本文中的所有公式和示例都是指该算法用于<strong class="md je">分类</strong>。</p><blockquote class="nq nr ns"><p id="41ef" class="mb mc nt md b me mx ke mg mh my kh mj nu mz mm mn nv na mq mr nw nb mu mv mw in bi translated">旁注，由于神经网络独特的机器学习方法，我已经将它们归为一类。然而，它们可以用于解决广泛的问题，包括但不限于分类和回归。下图是<strong class="md je">互动</strong>所以一定要点击👇在不同的类别上对<strong class="md je">进行放大并展示更多的</strong>。</p></blockquote><figure class="kt ku kv kw gu kx"><div class="bz fq l di"><div class="nx ny l"/></div><p class="le lf gk gi gj lg lh bd b be z dk translated">机器学习算法分类。由<a class="ae li" href="https://solclover.com/" rel="noopener ugc nofollow" target="_blank">作者</a>创建的互动图表。</p></figure><p id="165e" class="pw-post-body-paragraph mb mc iu md b me mx ke mg mh my kh mj mk mz mm mn mo na mq mr ms nb mu mv mw in bi translated"><strong class="md je"> <em class="nt">如果你喜欢数据科学和机器学习</em> </strong> <em class="nt">，请</em> <a class="ae li" href="https://solclover.com/subscribe" rel="noopener ugc nofollow" target="_blank"> <em class="nt">订阅</em> </a> <em class="nt">每当我发表一个新的故事，你都会收到一封电子邮件。</em></p><h1 id="c737" class="lj lk iu bd ll lm ln lo lp lq lr ls lt kj lu kk lv km lw kn lx kp ly kq lz ma bi translated">XGBoost和梯度增强的基础知识</h1><p id="de23" class="pw-post-body-paragraph mb mc iu md b me mf ke mg mh mi kh mj mk ml mm mn mo mp mq mr ms mt mu mv mw in bi translated">在深入细节之前，让我们回顾一下这些算法的基础。</p><ul class=""><li id="c3c6" class="nc nd iu md b me mx mh my mk nz mo oa ms ob mw nh ni nj nk bi translated"><strong class="md je">基于树的算法</strong>—XG boost和梯度增强都使用决策树作为它们的基本估计器。</li><li id="c434" class="nc nd iu md b me nl mh nm mk nn mo no ms np mw nh ni nj nk bi translated"><strong class="md je">预测目标</strong> —使用残差而不是实际的分类标签来构建树。因此，尽管我们关注分类问题，这些算法中的基本估计量是回归树而不是分类树。这是因为残差是连续的，而不是离散的。但是，与此同时，您将在下面看到的一些公式对于分类是独特的，所以请不要假设完全相同的公式适用于回归问题。</li><li id="8e7f" class="nc nd iu md b me nl mh nm mk nn mo no ms np mw nh ni nj nk bi translated"><strong class="md je">树深度</strong> —两种算法都允许您控制树的最大大小，以最小化数据过度拟合的风险。</li><li id="00d7" class="nc nd iu md b me nl mh nm mk nn mo no ms np mw nh ni nj nk bi translated"><strong class="md je">集成方法</strong> —类似于随机森林或AdaBoost，这些算法在这个过程中构建许多树。最后，最终的预测是基于所有的树。</li><li id="b74e" class="nc nd iu md b me nl mh nm mk nn mo no ms np mw nh ni nj nk bi translated"><strong class="md je">学习率</strong> —每棵树的价值由学习率决定。这使得算法在每一步都有更加渐进和稳定的改进。</li><li id="cd4d" class="nc nd iu md b me nl mh nm mk nn mo no ms np mw nh ni nj nk bi translated"><strong class="md je">流程图</strong> —最后，这里简单说明一下Gradient Boosting和XGBoost使用的流程。</li></ul><figure class="kt ku kv kw gu kx gi gj paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="gi gj oc"><img src="../Images/1f44cafbbebc659ebbd5bb8dfd433d49.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OiGbqPYF7QFASYlCQLOUBA.png"/></div></div><p class="le lf gk gi gj lg lh bd b be z dk translated">梯度增强和XGBoost算法的流程图。图片由<a class="ae li" href="https://solclover.com/" rel="noopener ugc nofollow" target="_blank">作者</a>提供。</p></figure><p id="cff0" class="pw-post-body-paragraph mb mc iu md b me mx ke mg mh my kh mj mk mz mm mn mo na mq mr ms nb mu mv mw in bi translated">虽然这些算法还有更多的部分，但以上应该给了你足够的基本理解，所以我们可以在接下来的几节中深入了解更多的细节。</p><h1 id="69fd" class="lj lk iu bd ll lm ln lo lp lq lr ls lt kj lu kk lv km lw kn lx kp ly kq lz ma bi translated">XGBoost和渐变增强构建树的方式不同</h1><h2 id="e616" class="od lk iu bd ll oe of dn lp og oh dp lt mk oi oj lv mo ok ol lx ms om on lz ja bi translated">梯度推进</h2><p id="5a60" class="pw-post-body-paragraph mb mc iu md b me mf ke mg mh mi kh mj mk ml mm mn mo mp mq mr ms mt mu mv mw in bi translated">您会很高兴听到常规梯度提升使用标准方法来构建回归树，其中使用MSE(均方误差)等典型指标或类似指标来确定树的最佳分割。</p><p id="835a" class="pw-post-body-paragraph mb mc iu md b me mx ke mg mh my kh mj mk mz mm mn mo na mq mr ms nb mu mv mw in bi translated">下面是MSE的公式:</p><figure class="kt ku kv kw gu kx gi gj paragraph-image"><div class="gi gj oo"><img src="../Images/52309fecbeb456771931ec50c9a8135a.png" data-original-src="https://miro.medium.com/v2/resize:fit:916/format:webp/1*0o1rvHRlign-mhkL-5SBCg.png"/></div></figure><p id="53e3" class="pw-post-body-paragraph mb mc iu md b me mx ke mg mh my kh mj mk mz mm mn mo na mq mr ms nb mu mv mw in bi translated">该算法为每个可能的节点分裂计算MSE，然后挑选具有最低MSE的一个作为在树中使用的一个。</p><p id="c4bb" class="pw-post-body-paragraph mb mc iu md b me mx ke mg mh my kh mj mk mz mm mn mo na mq mr ms nb mu mv mw in bi translated">如果你不熟悉这是如何工作的，我强烈建议你回顾一下我关于梯度增强树的故事中的深入解释:</p><div class="op oq gq gs or os"><a rel="noopener follow" target="_blank" href="/gradient-boosted-trees-for-classification-one-of-the-best-machine-learning-algorithms-35245dab03f2"><div class="ot ab fp"><div class="ou ab ov cl cj ow"><h2 class="bd je gz z fq ox fs ft oy fv fx jd bi translated">用于分类的梯度增强树——最好的机器学习算法之一</h2><div class="oz l"><h3 class="bd b gz z fq ox fs ft oy fv fx dk translated">梯度增强在分类树中如何工作的分步指南</h3></div><div class="pa l"><p class="bd b dl z fq ox fs ft oy fv fx dk translated">towardsdatascience.com</p></div></div><div class="pb l"><div class="pc l pd pe pf pb pg lc os"/></div></div></a></div><h2 id="3460" class="od lk iu bd ll oe of dn lp og oh dp lt mk oi oj lv mo ok ol lx ms om on lz ja bi translated">XGBoost</h2><p id="8b67" class="pw-post-body-paragraph mb mc iu md b me mf ke mg mh mi kh mj mk ml mm mn mo mp mq mr ms mt mu mv mw in bi translated">与常规的梯度增强相反，XGBoost使用自己的方法来构建树，其中相似性得分和增益决定最佳节点分裂。</p><figure class="kt ku kv kw gu kx gi gj paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="gi gj ph"><img src="../Images/378dc6a33b389eb58ae278a0634042ce.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FNxbJl7KNAQb0W2irktpmw.png"/></div></div></figure><ul class=""><li id="7654" class="nc nd iu md b me mx mh my mk nz mo oa ms ob mw nh ni nj nk bi translated"><strong class="md je">剩余的</strong>就是<code class="fe pi pj pk pl b">actual (observed) value — predicted value</code></li><li id="1247" class="nc nd iu md b me nl mh nm mk nn mo no ms np mw nh ni nj nk bi translated"><strong class="md je">先前概率</strong>是在先前步骤中计算的事件的概率。假设每个观察值的初始概率为0.5，用于构建第一棵树。对于任何后续的树，基于初始预测和来自所有先前树的预测，重新计算先前的概率，如过程图中所示。</li><li id="c6dc" class="nc nd iu md b me nl mh nm mk nn mo no ms np mw nh ni nj nk bi translated"><strong class="md je">λ</strong>是正则化参数。增加λ不成比例地降低了小叶子(具有很少观测值的叶子)的影响，而对大叶子(具有很多观测值的叶子)只有很小的影响。</li></ul><p id="9aed" class="pw-post-body-paragraph mb mc iu md b me mx ke mg mh my kh mj mk mz mm mn mo na mq mr ms nb mu mv mw in bi translated">一旦我们有了每片叶子的相似性分数，我们就可以使用下面的公式计算增益:</p><figure class="kt ku kv kw gu kx gi gj paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="gi gj pm"><img src="../Images/a158ae7fa5b5b8bae106ea131af05ac1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WsK8DbhOqBhjMmymHsBV4g.png"/></div></div></figure><p id="1f3f" class="pw-post-body-paragraph mb mc iu md b me mx ke mg mh my kh mj mk mz mm mn mo na mq mr ms nb mu mv mw in bi translated">然后选择具有最高增益<strong class="md je">的节点分裂作为树的最佳分裂。</strong></p><blockquote class="nq nr ns"><p id="777e" class="mb mc nt md b me mx ke mg mh my kh mj nu mz mm mn nv na mq mr nw nb mu mv mw in bi translated"><strong class="md je">重要:</strong>T3】XGBoost还有一个<strong class="md je">伽马超参数</strong>，可以手动设置。它允许您删除增益最小的节点。修剪发生在<code class="fe pi pj pk pl b">Gain — Gamma &lt; 0.</code></p><p id="d235" class="mb mc nt md b me mx ke mg mh my kh mj nu mz mm mn nv na mq mr nw nb mu mv mw in bi translated">此外，请注意，较高的λ值会导致较低的相似性得分，从而导致较低的增益。因此，您可以结合使用Gamma和Lamba，通过修剪小叶子并将它们与其他观察值结合来降低树对单个观察值的敏感度。</p></blockquote><h1 id="ce6d" class="lj lk iu bd ll lm ln lo lp lq lr ls lt kj lu kk lv km lw kn lx kp ly kq lz ma bi translated">计算输出值</h1><p id="f88c" class="pw-post-body-paragraph mb mc iu md b me mf ke mg mh mi kh mj mk ml mm mn mo mp mq mr ms mt mu mv mw in bi translated">两种算法计算每片叶子的输出值的方式几乎相同，唯一的区别是lambda超参数。</p><p id="a1fe" class="pw-post-body-paragraph mb mc iu md b me mx ke mg mh my kh mj mk mz mm mn mo na mq mr ms nb mu mv mw in bi translated"><strong class="md je">梯度增强</strong>算法使用以下公式:</p><figure class="kt ku kv kw gu kx gi gj paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="gi gj pn"><img src="../Images/ae0f8091d8e00d369145e61ef60bf259.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ce_kQzIGaNxgl3dpn4rJ5g.png"/></div></div></figure><p id="2144" class="pw-post-body-paragraph mb mc iu md b me mx ke mg mh my kh mj mk mz mm mn mo na mq mr ms nb mu mv mw in bi translated">同时，<strong class="md je"> XGBoost </strong>使用:</p><figure class="kt ku kv kw gu kx gi gj paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="gi gj po"><img src="../Images/31b20802a1e4deb6732cec40292c1a0f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ERRAjbQooIf9-FqR8ZcfxA.png"/></div></div></figure><h2 id="a74e" class="od lk iu bd ll oe of dn lp og oh dp lt mk oi oj lv mo ok ol lx ms om on lz ja bi translated">例子</h2><p id="3dce" class="pw-post-body-paragraph mb mc iu md b me mf ke mg mh mi kh mj mk ml mm mn mo mp mq mr ms mt mu mv mw in bi translated">让我们用一个简单的例子把理论付诸实践。假设我们有一个包含3个观测值的数据集，我们使用“湿度3pm”来预测明天是否会下雨。</p><p id="893a" class="pw-post-body-paragraph mb mc iu md b me mx ke mg mh my kh mj mk mz mm mn mo na mq mr ms nb mu mv mw in bi translated">下面是梯度推进算法如何构建它的第一棵树:</p><figure class="kt ku kv kw gu kx gi gj paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="gi gj pp"><img src="../Images/889b27da6d7e1288c8a9ab5787877797.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_uGLQadAwuS1FpVbLluUlw.png"/></div></div><p class="le lf gk gi gj lg lh bd b be z dk translated">梯度增强示例。图片由<a class="ae li" href="https://solclover.com/" rel="noopener ugc nofollow" target="_blank">作者</a>提供。</p></figure><p id="036d" class="pw-post-body-paragraph mb mc iu md b me mx ke mg mh my kh mj mk mz mm mn mo na mq mr ms nb mu mv mw in bi translated">这是XGBoost算法的等价形式:</p><figure class="kt ku kv kw gu kx gi gj paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="gi gj pq"><img src="../Images/500a433cd54027c630a82f6d8dfe7d78.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*W00bdW-ppVtxlQ9KPd9OZQ.png"/></div></div><p class="le lf gk gi gj lg lh bd b be z dk translated">极端梯度推进的例子。图片由<a class="ae li" href="https://solclover.com/" rel="noopener ugc nofollow" target="_blank">作者</a>提供。</p></figure><p id="9c94" class="pw-post-body-paragraph mb mc iu md b me mx ke mg mh my kh mj mk mz mm mn mo na mq mr ms nb mu mv mw in bi translated">我们可以看到，虽然计算结果有很大不同，但在这种情况下，两种算法都会为第一棵树选择相同的分割。</p><p id="a32b" class="pw-post-body-paragraph mb mc iu md b me mx ke mg mh my kh mj mk mz mm mn mo na mq mr ms nb mu mv mw in bi translated">但是，树叶的输出值是不同的(即使lambda设置为0 ),因为每个算法都从不同的初始预测开始。梯度增强使用整个数据集的事件概率作为其预测，而XGBoost总是从0.5开始(如果需要，可以手动更改)。</p><h1 id="4341" class="lj lk iu bd ll lm ln lo lp lq lr ls lt kj lu kk lv km lw kn lx kp ly kq lz ma bi translated">XGBoost优化</h1><p id="7ac6" class="pw-post-body-paragraph mb mc iu md b me mf ke mg mh mi kh mj mk ml mm mn mo mp mq mr ms mt mu mv mw in bi translated">除了使用自己的方式构建和修剪树，XGBoost还内置了几个优化功能，以便在处理大型数据集时加快训练速度。以下是几个主要的例子:</p><ul class=""><li id="44c6" class="nc nd iu md b me mx mh my mk nz mo oa ms ob mw nh ni nj nk bi translated"><strong class="md je">近似贪婪算法</strong> —在寻找最佳节点分割时使用加权分位数，而不是评估每个可能的分割。</li><li id="f747" class="nc nd iu md b me nl mh nm mk nn mo no ms np mw nh ni nj nk bi translated"><strong class="md je">并行学习</strong> —它可以将数据分割成更小的数据集，以并行运行流程。</li><li id="15e8" class="nc nd iu md b me nl mh nm mk nn mo no ms np mw nh ni nj nk bi translated"><strong class="md je">稀疏感知分裂发现</strong> —当您有一些缺失数据时，它通过将带有缺失值的观察值放入左叶来计算增益。然后，它通过将它们放入右边的叶子来做同样的事情，并选择产生更高增益的场景。</li><li id="51ff" class="nc nd iu md b me nl mh nm mk nn mo no ms np mw nh ni nj nk bi translated"><strong class="md je">现金感知访问</strong> — XGBoost使用CPU的高速缓冲存储器来存储梯度，因此它可以更快地计算相似性得分。</li></ul><div class="kt ku kv kw gu ab cb"><figure class="pr kx ps pt pu pv pw paragraph-image"><a href="https://solclover.com/membership"><img src="../Images/63320331b74bd98eea6402472b4209ac.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*qkXay39OnVc2IosW6rkxtw.png"/></a></figure><figure class="pr kx ps pt pu pv pw paragraph-image"><a href="https://www.linkedin.com/in/saulius-dobilas/"><img src="../Images/60fb21d1cb2701bfb6b71f61c99403e2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*vabxOXtQ4T034N_mscHSmQ.png"/></a></figure></div><h1 id="2f32" class="lj lk iu bd ll lm ln lo lp lq lr ls lt kj lu kk lv km lw kn lx kp ly kq lz ma bi translated">Python代码</h1><p id="6149" class="pw-post-body-paragraph mb mc iu md b me mf ke mg mh mi kh mj mk ml mm mn mo mp mq mr ms mt mu mv mw in bi translated">现在让我们使用澳大利亚的天气数据用Python构建一个简单的XGBoost模型。</p><h2 id="1dcb" class="od lk iu bd ll oe of dn lp og oh dp lt mk oi oj lv mo ok ol lx ms om on lz ja bi translated">设置</h2><p id="bb3b" class="pw-post-body-paragraph mb mc iu md b me mf ke mg mh mi kh mj mk ml mm mn mo mp mq mr ms mt mu mv mw in bi translated">我们将使用以下数据和库:</p><ul class=""><li id="a081" class="nc nd iu md b me mx mh my mk nz mo oa ms ob mw nh ni nj nk bi translated">来自Kaggle的澳大利亚天气数据</li><li id="4f24" class="nc nd iu md b me nl mh nm mk nn mo no ms np mw nh ni nj nk bi translated"><a class="ae li" href="https://scikit-learn.org/stable/index.html" rel="noopener ugc nofollow" target="_blank"> Scikit-learn库</a>，用于将数据拆分为<a class="ae li" href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html?highlight=train_test_split#sklearn.model_selection.train_test_split" rel="noopener ugc nofollow" target="_blank">训练测试</a>样本和<a class="ae li" href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html?highlight=classification_report#sklearn.metrics.classification_report" rel="noopener ugc nofollow" target="_blank">模型评估</a></li><li id="4fb4" class="nc nd iu md b me nl mh nm mk nn mo no ms np mw nh ni nj nk bi translated"><a class="ae li" href="https://xgboost.readthedocs.io/en/latest/index.html" rel="noopener ugc nofollow" target="_blank"> XGBoost库</a>用于构建模型</li><li id="0e21" class="nc nd iu md b me nl mh nm mk nn mo no ms np mw nh ni nj nk bi translated">用于数据操作的<a class="ae li" href="https://pandas.pydata.org/docs/" rel="noopener ugc nofollow" target="_blank">熊猫</a>和<a class="ae li" href="https://numpy.org/" rel="noopener ugc nofollow" target="_blank"> Numpy </a></li></ul><p id="117e" class="pw-post-body-paragraph mb mc iu md b me mx ke mg mh my kh mj mk mz mm mn mo na mq mr ms nb mu mv mw in bi translated">让我们导入所有的库:</p><figure class="kt ku kv kw gu kx"><div class="bz fq l di"><div class="px ny l"/></div></figure><p id="ca42" class="pw-post-body-paragraph mb mc iu md b me mx ke mg mh my kh mj mk mz mm mn mo na mq mr ms nb mu mv mw in bi translated">然后我们从Kaggle获取澳大利亚的天气数据，你可以按照这个链接下载:<a class="ae li" href="https://www.kaggle.com/jsphyg/weather-dataset-rattle-package" rel="noopener ugc nofollow" target="_blank">https://www . ka ggle . com/jsphyg/weather-dataset-rattle-package</a>。</p><p id="a23e" class="pw-post-body-paragraph mb mc iu md b me mx ke mg mh my kh mj mk mz mm mn mo na mq mr ms nb mu mv mw in bi translated">我们接收数据并推导出一些新的变量用于模型中。</p><figure class="kt ku kv kw gu kx"><div class="bz fq l di"><div class="px ny l"/></div></figure><p id="6fde" class="pw-post-body-paragraph mb mc iu md b me mx ke mg mh my kh mj mk mz mm mn mo na mq mr ms nb mu mv mw in bi translated">数据看起来是这样的:</p><figure class="kt ku kv kw gu kx gi gj paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="gi gj py"><img src="../Images/e9e46f9ae4002460cc2f48b5130c3181.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UirJ8_pVPZ5K6XOqABWFaQ.png"/></div></div><p class="le lf gk gi gj lg lh bd b be z dk translated">一小段<a class="ae li" href="https://www.kaggle.com/jsphyg/weather-dataset-rattle-package" rel="noopener ugc nofollow" target="_blank"> Kaggle的澳大利亚天气数据</a>做了一些修改。图片由<a class="ae li" href="https://solclover.medium.com/" rel="noopener">作者</a>提供。</p></figure><h2 id="5213" class="od lk iu bd ll oe of dn lp og oh dp lt mk oi oj lv mo ok ol lx ms om on lz ja bi translated">模特培训</h2><p id="d169" class="pw-post-body-paragraph mb mc iu md b me mf ke mg mh mi kh mj mk ml mm mn mo mp mq mr ms mt mu mv mw in bi translated">接下来，我们定义一个用于模型训练的函数，并运行模型以产生结果:</p><ul class=""><li id="ae97" class="nc nd iu md b me mx mh my mk nz mo oa ms ob mw nh ni nj nk bi translated">步骤1 —将数据分为训练样本和测试样本</li><li id="5d7a" class="nc nd iu md b me nl mh nm mk nn mo no ms np mw nh ni nj nk bi translated">步骤2-设置模型参数并训练(拟合)模型</li><li id="1fc6" class="nc nd iu md b me nl mh nm mk nn mo no ms np mw nh ni nj nk bi translated">步骤3-使用我们的模型预测训练和测试数据上的类别标签</li><li id="789a" class="nc nd iu md b me nl mh nm mk nn mo no ms np mw nh ni nj nk bi translated">步骤4-生成模型摘要统计数据</li><li id="9891" class="nc nd iu md b me nl mh nm mk nn mo no ms np mw nh ni nj nk bi translated">步骤5-运行模型并显示结果</li></ul><figure class="kt ku kv kw gu kx"><div class="bz fq l di"><div class="px ny l"/></div></figure><p id="cf3e" class="pw-post-body-paragraph mb mc iu md b me mx ke mg mh my kh mj mk mz mm mn mo na mq mr ms nb mu mv mw in bi translated">这些是上述函数返回的模型评估指标。</p><figure class="kt ku kv kw gu kx gi gj paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="gi gj pz"><img src="../Images/7623e348f64591d4eb04678a85e15ff3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*F6sVyO7kzovoenjM5BNlHQ.png"/></div></div><p class="le lf gk gi gj lg lh bd b be z dk translated">XGBoost模型结果。图片来自<a class="ae li" href="https://solclover.medium.com/" rel="noopener">作者</a>。</p></figure><h1 id="757b" class="lj lk iu bd ll lm ln lo lp lq lr ls lt kj lu kk lv km lw kn lx kp ly kq lz ma bi translated">结论</h1><p id="65e5" class="pw-post-body-paragraph mb mc iu md b me mf ke mg mh mi kh mj mk ml mm mn mo mp mq mr ms mt mu mv mw in bi translated">XGBoost是tin上显示的内容，即常规梯度增强算法的更精确和优化的版本。在大多数情况下，这些算法产生的结果可能会非常相似。如果您处理大型数据集，那么极端梯度增强应该是更好的选择。</p><p id="34d9" class="pw-post-body-paragraph mb mc iu md b me mx ke mg mh my kh mj mk mz mm mn mo na mq mr ms nb mu mv mw in bi translated">我希望这个故事能让你对这两种算法的主要区别有一个很好的了解，但是如果你有任何问题或建议，请随时联系我们。快乐造型！</p><p id="cf8c" class="pw-post-body-paragraph mb mc iu md b me mx ke mg mh my kh mj mk mz mm mn mo na mq mr ms nb mu mv mw in bi translated">干杯！👏<br/> <strong class="md je">索尔·多比拉斯</strong></p></div><div class="ab cl qa qb hy qc" role="separator"><span class="qd bw bk qe qf qg"/><span class="qd bw bk qe qf qg"/><span class="qd bw bk qe qf"/></div><div class="in io ip iq ir"><p id="31cb" class="pw-post-body-paragraph mb mc iu md b me mx ke mg mh my kh mj mk mz mm mn mo na mq mr ms nb mu mv mw in bi translated"><strong class="md je"> <em class="nt">如果你已经花光了这个月的学习预算，下次请记得我。</em> </strong> <em class="nt">我的个性化链接加入媒介是:</em></p><div class="op oq gq gs or os"><a href="https://solclover.com/membership" rel="noopener  ugc nofollow" target="_blank"><div class="ot ab fp"><div class="ou ab ov cl cj ow"><h2 class="bd je gz z fq ox fs ft oy fv fx jd bi translated">通过我的推荐链接加入Medium索尔·多比拉斯</h2><div class="oz l"><h3 class="bd b gz z fq ox fs ft oy fv fx dk translated">作为一个媒体会员，你的会员费的一部分会给你阅读的作家，你可以完全接触到每一个故事…</h3></div><div class="pa l"><p class="bd b dl z fq ox fs ft oy fv fx dk translated">solclover.com</p></div></div><div class="pb l"><div class="qh l pd pe pf pb pg lc os"/></div></div></a></div></div><div class="ab cl qa qb hy qc" role="separator"><span class="qd bw bk qe qf qg"/><span class="qd bw bk qe qf qg"/><span class="qd bw bk qe qf"/></div><div class="in io ip iq ir"><p id="7c9b" class="pw-post-body-paragraph mb mc iu md b me mx ke mg mh my kh mj mk mz mm mn mo na mq mr ms nb mu mv mw in bi translated">如果你喜欢这篇文章，你可能也会觉得有趣:</p><div class="op oq gq gs or os"><a rel="noopener follow" target="_blank" href="/adaboost-algorithm-remarkably-capable-but-with-one-interesting-limitation-cf95905bf8a0"><div class="ot ab fp"><div class="ou ab ov cl cj ow"><h2 class="bd je gz z fq ox fs ft oy fv fx jd bi translated">AdaBoost机器学习算法:如何提高难以预测案例的性能</h2><div class="oz l"><h3 class="bd b gz z fq ox fs ft oy fv fx dk translated">对自适应Boosting算法及其与其他基于决策树的机器的区别的直观解释…</h3></div><div class="pa l"><p class="bd b dl z fq ox fs ft oy fv fx dk translated">towardsdatascience.com</p></div></div><div class="pb l"><div class="qi l pd pe pf pb pg lc os"/></div></div></a></div><div class="op oq gq gs or os"><a rel="noopener follow" target="_blank" href="/svm-classifier-and-rbf-kernel-how-to-make-better-models-in-python-73bb4914af5b"><div class="ot ab fp"><div class="ou ab ov cl cj ow"><h2 class="bd je gz z fq ox fs ft oy fv fx jd bi translated">SVM分类器和RBF核——如何用Python制作更好的模型</h2><div class="oz l"><h3 class="bd b gz z fq ox fs ft oy fv fx dk translated">支持向量机(SVM)和径向基函数(RBF)核的内部工作的完整解释</h3></div><div class="pa l"><p class="bd b dl z fq ox fs ft oy fv fx dk translated">towardsdatascience.com</p></div></div><div class="pb l"><div class="qj l pd pe pf pb pg lc os"/></div></div></a></div></div></div>    
</body>
</html>