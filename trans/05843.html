<html>
<head>
<title>7 Hyperparameter Optimization Techniques Every Data Scientist Should Know</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">每个数据科学家都应该知道的7种超参数优化技术</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/7-hyperparameter-optimization-techniques-every-data-scientist-should-know-12cdebe713da?source=collection_archive---------14-----------------------#2021-05-25">https://towardsdatascience.com/7-hyperparameter-optimization-techniques-every-data-scientist-should-know-12cdebe713da?source=collection_archive---------14-----------------------#2021-05-25</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="d674" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">从手动调整到自动超参数优化—带实践示例</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/a501ccfd1bc1ddb829e530d446db0e3b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6zEWRB8VMaNLcpEcy1Ohpw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图片由<a class="ae ky" href="https://pixabay.com/users/mohamed_hassan-5229782/?utm_source=link-attribution&amp;amp;utm_medium=referral&amp;amp;utm_campaign=image&amp;amp;utm_content=3679741" rel="noopener ugc nofollow" target="_blank">穆罕默德·哈桑</a>来自<a class="ae ky" href="https://pixabay.com/?utm_source=link-attribution&amp;amp;utm_medium=referral&amp;amp;utm_campaign=image&amp;amp;utm_content=3679741" rel="noopener ugc nofollow" target="_blank"> Pixabay </a></p></figure><p id="6ecb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">选择正确的机器学习模型和相应的正确的超参数集对于训练健壮的机器学习模型是必不可少的。机器学习模型的性能随着超参数调整而提高。</p><p id="9b32" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">超参数是指模型无法学习，需要在训练前提供的参数。超参数调优基本上是指微调模型的参数，这基本上是一个漫长的过程。</p><p id="8d44" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在本文中，我们将讨论7种超参数优化技术，并给出一些实际例子。</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="229b" class="ma mb it lw b gy mc md l me mf"><strong class="lw iu">Hyperparameter Optimization Checklist:<br/><em class="mg">1) Manual Search<br/>2) Grid Search<br/>3) Randomized Search<br/>4) Halving Grid Search<br/>5) Halving Randomized Search<br/>6) HyperOpt-Sklearn<br/>7) Bayes Search</em></strong></span></pre><h1 id="96b5" class="mh mb it bd mi mj mk ml mm mn mo mp mq jz mr ka ms kc mt kd mu kf mv kg mw mx bi translated">预热:</h1><p id="32ae" class="pw-post-body-paragraph kz la it lb b lc my ju le lf mz jx lh li na lk ll lm nb lo lp lq nc ls lt lu im bi translated">信用卡欺诈检测数据集将用于训练基线逻辑回归模型。逻辑回归模型的其他超参数将使用各种技术进行调整，以提高性能。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nd ne l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">(作者代码)，信用卡欺诈检测数据集的处理</p></figure><h1 id="b257" class="mh mb it bd mi mj mk ml mm mn mo mp mq jz mr ka ms kc mt kd mu kf mv kg mw mx bi translated">手动搜索:</h1><p id="a22b" class="pw-post-body-paragraph kz la it lb b lc my ju le lf mz jx lh li na lk ll lm nb lo lp lq nc ls lt lu im bi translated">不需要专门的库来手动调整超参数，相反，开发人员需要为模型尝试不同的超参数组合，并选择性能最佳的模型。<code class="fe nf ng nh lw b"><strong class="lb iu">max_depth, gamma, reg_lambda, scale_pos_weight</strong></code>是一些可以为XBGClassifer模型调整的超参数。</p><p id="52ac" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">人们可以尝试超参数值的所有组合并为每个组合训练模型，并挑选具有最佳性能的模型。遍历超参数的不同值并评估每个组合也可以是另一种用于超参数优化的手动搜索方法。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nd ne l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">(作者代码)，手动搜索超参数优化</p></figure><p id="2264" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">手动搜索是一个有点繁琐的过程，因此需要不必要的人力。</p><h1 id="5cfc" class="mh mb it bd mi mj mk ml mm mn mo mp mq jz mr ka ms kc mt kd mu kf mv kg mw mx bi translated">网格搜索:</h1><p id="f696" class="pw-post-body-paragraph kz la it lb b lc my ju le lf mz jx lh li na lk ll lm nb lo lp lq nc ls lt lu im bi translated">网格搜索可以被称为手动搜索超参数优化的自动化版本。Scikit-Learn库附带了一个GridSearchCV实现。GridSearch不是计算友好的，因为它需要大量的时间来优化，但人们可以不必编写多行代码。</p><p id="6bff" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">可以将字典格式的训练模型和超参数列表提供给GridSeachCV函数，并且其返回执行模型及其得分度量。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nd ne l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">(作者代码)，网格搜索CV</p></figure><h1 id="c987" class="mh mb it bd mi mj mk ml mm mn mo mp mq jz mr ka ms kc mt kd mu kf mv kg mw mx bi translated">随机搜索:</h1><p id="aa70" class="pw-post-body-paragraph kz la it lb b lc my ju le lf mz jx lh li na lk ll lm nb lo lp lq nc ls lt lu im bi translated">网格搜索尝试超参数的所有组合，因此增加了计算的时间复杂度。基于随机超参数组合的随机搜索训练模型。与网格搜索相比，随机搜索训练若干模型的组合总数较少。</p><p id="5e0d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">Scikit-Learn包还附带了RandomSearchCV实现。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nd ne l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">(作者代码)，随机搜索简历</p></figure><h1 id="a8cc" class="mh mb it bd mi mj mk ml mm mn mo mp mq jz mr ka ms kc mt kd mu kf mv kg mw mx bi translated">减半网格搜索:</h1><p id="b76e" class="pw-post-body-paragraph kz la it lb b lc my ju le lf mz jx lh li na lk ll lm nb lo lp lq nc ls lt lu im bi translated">减半网格搜索是网格搜索超参数优化的优化版本。等分网格搜索使用连续等分方法在指定的超参数列表中进行搜索。搜索策略开始在数据的一个小样本上评估所有候选，并使用越来越大的样本迭代地选择最佳候选。</p><p id="914f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">与网格搜索方法相比，减半网格搜索的计算成本更低。Scikit-Learn库实现了HalvingGridSearch。</p><blockquote class="ni nj nk"><p id="65d1" class="kz la mg lb b lc ld ju le lf lg jx lh nl lj lk ll nm ln lo lp nn lr ls lt lu im bi translated">阅读下面提到的文章中的<a class="ae ky" rel="noopener" target="_blank" href="/20x-times-faster-grid-search-cross-validation-19ef01409b7c">，了解将网格搜索CV减半如何将超参数优化速度提高20倍。</a></p></blockquote><div class="no np gp gr nq nr"><a rel="noopener follow" target="_blank" href="/20x-times-faster-grid-search-cross-validation-19ef01409b7c"><div class="ns ab fo"><div class="nt ab nu cl cj nv"><h2 class="bd iu gy z fp nw fr fs nx fu fw is bi translated">网格搜索交叉验证速度提高20倍</h2><div class="ny l"><p class="bd b dl z fp nw fr fs nx fu fw dk translated">towardsdatascience.com</p></div></div><div class="nz l"><div class="oa l ob oc od nz oe ks nr"/></div></div></a></div><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nd ne l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">(作者代码)，减半网格搜索CV</p></figure><h1 id="cc1b" class="mh mb it bd mi mj mk ml mm mn mo mp mq jz mr ka ms kc mt kd mu kf mv kg mw mx bi translated">减半随机搜索:</h1><p id="6b4e" class="pw-post-body-paragraph kz la it lb b lc my ju le lf mz jx lh li na lk ll lm nb lo lp lq nc ls lt lu im bi translated">减半随机搜索使用相同的连续减半方法，并且与减半网格搜索相比进一步优化。与对半网格搜索不同，它不在所有超参数组合上训练，而是随机选取一组超参数组合。Scikit-Learn库还提供了HalvingRandomizedSeachCV。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nd ne l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">(作者代码)，减半随机搜索简历</p></figure><h1 id="e02a" class="mh mb it bd mi mj mk ml mm mn mo mp mq jz mr ka ms kc mt kd mu kf mv kg mw mx bi translated">Hyperopt-Sklearn:</h1><p id="8155" class="pw-post-body-paragraph kz la it lb b lc my ju le lf mz jx lh li na lk ll lm nb lo lp lq nc ls lt lu im bi translated">Hyperopt是一个用于贝叶斯优化的开源Python库，专为具有数百个参数的模型的大规模优化而设计。它允许超参数优化跨CPU的多个内核进行扩展。</p><p id="3fd5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">Hyperopt-Sklearn是Hyperopt库的扩展，它允许自动搜索机器学习算法，并为分类和回归任务建立超参数模型。</p><blockquote class="ni nj nk"><p id="de1a" class="kz la mg lb b lc ld ju le lf lg jx lh nl lj lk ll nm ln lo lp nn lr ls lt lu im bi translated">阅读<a class="ae ky" href="https://machinelearningmastery.com/hyperopt-for-automated-machine-learning-with-scikit-learn/" rel="noopener ugc nofollow" target="_blank">这篇文章</a>以了解更多关于Hyperopt-Sklearn包的用法和实现。</p></blockquote><h1 id="2d06" class="mh mb it bd mi mj mk ml mm mn mo mp mq jz mr ka ms kc mt kd mu kf mv kg mw mx bi translated">贝叶斯网格搜索:</h1><p id="605b" class="pw-post-body-paragraph kz la it lb b lc my ju le lf mz jx lh li na lk ll lm nb lo lp lq nc ls lt lu im bi translated">贝叶斯网格搜索使用贝叶斯优化技术来模拟搜索空间，以尽快达到优化的参数值。它利用搜索空间的结构来优化搜索时间。贝叶斯搜索方法使用过去的评估结果来采样最有可能给出更好结果的新候选。</p><p id="c6f3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><a class="ae ky" href="https://pypi.org/project/scikit-optimize/" rel="noopener ugc nofollow" target="_blank"> Scikit-Optimize </a>库附带了BayesSearchCV实现。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nd ne l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">(作者代码)，贝叶斯搜索简历</p></figure><h1 id="7a94" class="mh mb it bd mi mj mk ml mm mn mo mp mq jz mr ka ms kc mt kd mu kf mv kg mw mx bi translated">结论:</h1><p id="e204" class="pw-post-body-paragraph kz la it lb b lc my ju le lf mz jx lh li na lk ll lm nb lo lp lq nc ls lt lu im bi translated">在本文中，我们讨论了7种超参数优化技术，可以用来获得最佳的超参数集，从而训练一个健壮的机器学习模型。最佳模型性能和最佳优化技术之间的权衡是影响某人选择的一个因素。</p><p id="e858" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对于一些组件，可以使用GridSearchCV技术。但是当组件数量增加时，可以尝试将网格搜索CV或随机搜索CV减半，因为它们在计算上并不昂贵。</p><h1 id="cea0" class="mh mb it bd mi mj mk ml mm mn mo mp mq jz mr ka ms kc mt kd mu kf mv kg mw mx bi translated">参考资料:</h1><p id="eb19" class="pw-post-body-paragraph kz la it lb b lc my ju le lf mz jx lh li na lk ll lm nb lo lp lq nc ls lt lu im bi translated">[1] Scikit-Learn文档:<a class="ae ky" href="https://scikit-learn.org/stable/modules/grid_search.html" rel="noopener ugc nofollow" target="_blank">https://scikit-learn.org/stable/modules/grid_search.html</a></p><blockquote class="of"><p id="3091" class="og oh it bd oi oj ok ol om on oo lu dk translated">感谢您的阅读</p></blockquote></div></div>    
</body>
</html>