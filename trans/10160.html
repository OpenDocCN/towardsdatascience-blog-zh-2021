<html>
<head>
<title>Multi-Armed Bandits: Thompson Sampling Algorithm with Python Code</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">多臂强盗:带Python代码的Thompson采样算法</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/multi-armed-bandits-thompson-sampling-algorithm-fea205cf31df?source=collection_archive---------4-----------------------#2021-09-26">https://towardsdatascience.com/multi-armed-bandits-thompson-sampling-algorithm-fea205cf31df?source=collection_archive---------4-----------------------#2021-09-26</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="b749" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">了解汤普森采样(贝叶斯土匪)算法。</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/033330da36c6b4d02882ac5b3390d8be.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OM-ZMH7BvoAhXfiOZqij5g.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">丹尼斯·简斯在<a class="ae ky" href="https://unsplash.com/s/photos/movie?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>拍摄的照片</p></figure><h1 id="a05b" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">介绍</h1><p id="b8a7" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">在这一系列的帖子中，我们尝试了不同的bandit算法来优化我们的电影之夜——更具体地说，我们如何选择电影和餐馆来送餐！</p><p id="1db9" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">对于新人来说，土匪这个名字来自老虎机(被称为独臂土匪)。你可以把它想成每次和它互动(拉它胳膊)都能奖励你(或者不奖励你)的东西。目标是，给定一群给出不同奖励的强盗，尽可能快地找出给出最高奖励的强盗。当我们开始玩游戏并不断收集关于每个强盗的数据时，强盗算法会帮助我们在利用迄今为止给我们最高奖励的强盗和探索其他强盗之间进行选择。</p></div><div class="ab cl ms mt hx mu" role="separator"><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx"/></div><div class="im in io ip iq"><div class="kj kk kl km gt mz"><a href="https://medium.com/analytics-vidhya/multi-armed-bandits-part-1-epsilon-greedy-algorithm-with-python-code-534b9e2abc9" rel="noopener follow" target="_blank"><div class="na ab fo"><div class="nb ab nc cl cj nd"><h2 class="bd iu gy z fp ne fr fs nf fu fw is bi translated">多臂强盗:Epsilon-Greedy算法和Python代码</h2><div class="ng l"><h3 class="bd b gy z fp ne fr fs nf fu fw dk translated">了解Epsilon-Greedy的工作原理。为所有实验提供完整的python代码。</h3></div><div class="nh l"><p class="bd b dl z fp ne fr fs nf fu fw dk translated">medium.com</p></div></div><div class="ni l"><div class="nj l nk nl nm ni nn ks mz"/></div></div></a></div><div class="no np gp gr nq mz"><a rel="noopener follow" target="_blank" href="/multi-armed-bandits-upper-confidence-bound-algorithms-with-python-code-a977728f0e2d"><div class="na ab fo"><div class="nb ab nc cl cj nd"><h2 class="bd iu gy z fp ne fr fs nf fu fw is bi translated">多臂强盗:带Python代码的置信上限算法</h2><div class="ng l"><h3 class="bd b gy z fp ne fr fs nf fu fw dk translated">了解不同的置信上限bandit算法。为所有实验提供的Python代码。</h3></div><div class="nh l"><p class="bd b dl z fp ne fr fs nf fu fw dk translated">towardsdatascience.com</p></div></div><div class="ni l"><div class="nr l nk nl nm ni nn ks mz"/></div></div></a></div></div><div class="ab cl ms mt hx mu" role="separator"><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx"/></div><div class="im in io ip iq"><p id="6b47" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">你和你的朋友一直在使用bandit算法来优化每周电影之夜选择哪家餐馆和哪部电影。到目前为止，你已经尝试了不同的强盗算法，如<a class="ae ky" href="https://medium.com/analytics-vidhya/multi-armed-bandits-part-1-epsilon-greedy-algorithm-with-python-code-534b9e2abc9" rel="noopener">ε贪婪</a>、<a class="ae ky" href="https://medium.com/swlh/multi-armed-bandits-optimistic-initial-values-algorithm-with-python-code-3970e611b5ab" rel="noopener">乐观初始值</a>和<a class="ae ky" rel="noopener" target="_blank" href="/multi-armed-bandits-upper-confidence-bound-algorithms-with-python-code-a977728f0e2d">置信上限</a> (UCB)。您已经发现<strong class="lt iu"> UCB1调整的</strong>算法在伯努利和普通奖励方面都比其他算法稍好，并且已经在过去几个月中使用了它。</p><p id="cd6b" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">尽管你的电影之夜因UCB1-Tuned做出的选择而进展顺利，但你错过了尝试新算法的兴奋感。</p><p id="d07b" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">“你听说过汤普森抽样吗？”你的朋友问。</p><p id="7bb9" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">兴奋的你拿起手机开始阅读。</p><h1 id="2a5a" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">该算法</h1><p id="0e7d" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">汤普森抽样，也称为贝叶斯土匪，是多武装土匪问题的贝叶斯方法。基本思想是把每个强盗的平均报酬𝛍视为一个随机变量，并使用我们目前收集的数据来计算它的分布。然后，在每一步，我们将从每个强盗的平均奖励分布中抽取一个点，并选择其样本值最高的一个。我们随后从选中的强盗那里得到奖励，并更新它的平均奖励分布。</p><p id="84eb" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">我听到你问我们如何计算/更新分布？贝叶斯强盗这个名字可能已经泄露了，但是我们将使用<strong class="lt iu">贝叶斯定理</strong>。</p><pre class="kj kk kl km gt ns nt nu nv aw nw bi"><span id="58df" class="nx la it nt b gy ny nz l oa ob">Bayes Theorem<br/> P(A|B) = P(B|A) P(A) / P(B)</span><span id="6d23" class="nx la it nt b gy oc nz l oa ob">where<br/> A, B: events in sample space<br/> P(A|B): conditional probability of A given B<br/> P(B|A): conditional probability of B given A<br/> P(A): probability of A<br/> P(B): probability of B</span></pre><p id="4a29" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">以上定义是针对离散变量，对于<a class="ae ky" href="http://galton.uchicago.edu/~eichler/stat24600/Handouts/l06.pdf" rel="noopener ugc nofollow" target="_blank">连续变量</a>是针对概率密度函数定义的。</p><p id="513f" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">在我们的例子中，对于每一个土匪，我们想计算他们的P( 𝛍 |数据)，也就是说，给定我们到目前为止收集的数据，𝛍的不同值的可能性有多大。</p><pre class="kj kk kl km gt ns nt nu nv aw nw bi"><span id="7bdf" class="nx la it nt b gy ny nz l oa ob">P( 𝛍 | data ) = P(data | 𝛍) P(𝛍) / P(data)<br/>              ∝ P(data | 𝛍) P(𝛍)<br/>    posterior ∝ likelihood x prior</span></pre><p id="7c19" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">具有许多数据点的土匪将具有狭窄的分布，因此我们采样的数据点将有很高的机会接近真实平均值。然而，拥有很少数据点的盗匪将会有一个广泛的分布，给高值一个被选择的好机会。这种机制平衡了探索和开发。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi od"><img src="../Images/97a7c56b934c2bd415465b841badfede.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/1*6f9TooVSPNXhPdm9-ooAZA.gif"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图1:更新单个伯努利强盗的平均奖励分布，奖励概率为0.8。平均回报遵循贝塔(成功+ 1，失败+ 1)分布，假设贝塔(1，1)先验和二项式可能性。作者制作的动画。</p></figure><p id="fd44" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">现在，计算P( 𝛍 |数据)是棘手的，除非可能性和先验是<a class="ae ky" href="https://en.wikipedia.org/wiki/Conjugate_prior" rel="noopener ugc nofollow" target="_blank"> <strong class="lt iu">共轭先验</strong> </a>。这意味着，当我们结合似然性和先验时，我们得到的后验分布与先验分布来自同一个家族。这很方便，原因有二:</p><ul class=""><li id="8c5c" class="oe of it lt b lu mn lx mo ma og me oh mi oi mm oj ok ol om bi translated">我们可以避免计算后验概率的麻烦，因为它有一个封闭的表达式。</li><li id="8320" class="oe of it lt b lu on lx oo ma op me oq mi or mm oj ok ol om bi translated">我们可以使用步骤<em class="os"> i-1 </em>的后验概率作为步骤<em class="os"> i </em>的先验概率。这使得从P( 𝛍 |直到步骤<em class="os"> i-1 </em>的数据)到P( 𝛍 |直到步骤<em class="os"> i </em>的数据)变得容易，并确保我们只使用共轭先验。</li></ul><h2 id="47ad" class="nx la it bd lb ot ou dn lf ov ow dp lj ma ox oy ll me oz pa ln mi pb pc lp pd bi translated">伯努利奖励</h2><p id="922c" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">在这篇文章的其余部分，我们将假设一个强盗给𝛍的概率是1，否则是0，即</p><pre class="kj kk kl km gt ns nt nu nv aw nw bi"><span id="53c2" class="nx la it nt b gy ny nz l oa ob">ith bandit rewards ~ Bernoulli(prob = 𝛍_i)</span></pre><p id="41fe" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">现在我们需要为𝛍.选择一个先验分布由于𝛍是成功的概率，它只能取0到1之间的值。因此，我们需要一个支持[0，1]的分布。一种这样的分布是具有参数<code class="fe pe pf pg nt b">a</code>和<code class="fe pe pf pg nt b">b</code>的贝塔分布。在我们收集任何数据之前，𝛍的所有值都是同样可能的，因此[0，1]上的统一先验是有意义的。选择<code class="fe pe pf pg nt b">a = 1</code>和<code class="fe pe pf pg nt b">b = 1</code>给了我们这样的机会。</p><p id="eede" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">接下来，我们需要选择一个可能性，最好是β分布是共轭先验的可能性。二项式分布满足这一点，它对我们的用例也有意义，因为它用成功概率<code class="fe pe pf pg nt b">p</code>(这是我们试图估计的𝛍)模拟了从<code class="fe pe pf pg nt b">N</code>试验(我们玩强盗的次数)中获得<code class="fe pe pf pg nt b">k</code>成功的概率(强盗奖励的总和)。</p><pre class="kj kk kl km gt ns nt nu nv aw nw bi"><span id="0acf" class="nx la it nt b gy ny nz l oa ob">If<br/>  Prior is 𝛍 ~ Beta(𝛼, 𝛽)<br/>  Likelihood is k | 𝛍 ~ Binomial(N, 𝛍)<br/>Then<br/>  Posterior is 𝛍 ~ Beta(𝛼 + successes, 𝛽 + failures)<br/>Where<br/>  N = successes + failures</span></pre><p id="4257" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">这是一个简单的更新规则，如果强盗的奖励是1，我们增加它的<code class="fe pe pf pg nt b">a</code>，如果是0，我们增加它的<code class="fe pe pf pg nt b">b</code>。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi od"><img src="../Images/bdb6dbc1a5eb259b1b01503557d8b168.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/1*T5HEk7dIupNG99jQY7t2HA.gif"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图2:使用Thompson抽样的实验，四个强盗获得奖励的概率(0.6，0.7，0.8，0.9)等于1。我们可以看到，随着我们为每个土匪收集更多的数据，分布变得越来越窄。作者的动画。</p></figure><h1 id="c68e" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">给我看看代码</h1><p id="cbc6" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">在之前的文章中，我们已经定义了你将在这里看到的基类，但是为了完整起见，我们再次将它们包括在内。</p><p id="9f01" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">下面的代码定义了代表强盗的类<code class="fe pe pf pg nt b">BernoulliBandit</code>，代表代理的基类<code class="fe pe pf pg nt b">Agent</code>和跟踪奖励的助手类<code class="fe pe pf pg nt b">BanditRewardsLog</code>。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ph pi l"/></div></figure><p id="f3d1" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">现在，剩下的唯一事情就是子类化<code class="fe pe pf pg nt b">Agent</code>类并实现Thompson采样。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ph pi l"/></div></figure><p id="f4c4" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">…我们完成了！现在，让我们看看它如何与以前帖子中的算法相比较，即<a class="ae ky" href="https://medium.com/analytics-vidhya/multi-armed-bandits-part-1-epsilon-greedy-algorithm-with-python-code-534b9e2abc9" rel="noopener">ε贪婪</a>、<a class="ae ky" href="https://medium.com/swlh/multi-armed-bandits-optimistic-initial-values-algorithm-with-python-code-3970e611b5ab" rel="noopener">乐观初始值</a>和<a class="ae ky" rel="noopener" target="_blank" href="/multi-armed-bandits-upper-confidence-bound-algorithms-with-python-code-a977728f0e2d">置信上限</a> (UCB)。</p><h1 id="46e3" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">实验</h1><p id="17fb" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">我们将使用下面的代码来比较不同的算法。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ph pi l"/></div></figure><p id="7df4" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">首先，我们来定义一下我们的土匪。</p><pre class="kj kk kl km gt ns nt nu nv aw nw bi"><span id="f980" class="nx la it nt b gy ny nz l oa ob">probs = [0.6, 0.7, 0.8, 0.9]<br/>bernoulli_bandits = [BernoulliBandit(p) for p in probs]</span></pre><p id="bb82" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">在这之后，我们可以简单地运行</p><pre class="kj kk kl km gt ns nt nu nv aw nw bi"><span id="39ed" class="nx la it nt b gy ny nz l oa ob">compare_agents(<br/>  get_agents(), <br/>  bernoulli_bandits, <br/>  iterations=500, <br/>  show_plot=True,<br/>)</span></pre><p id="d926" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">这给了我们以下。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pj"><img src="../Images/9327ce404d3aeef5c85d9eaa89e4e4d4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1278/format:webp/1*zWasQgPwKeH4ap4virVT7A.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">Tota每个代理的累积奖励。图片由作者提供。</p></figure><p id="fe2a" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">嗯…不是很清楚，但是好像Epsilon-Greedy和Bayesian土匪有类似的表现。为了找到最好的一个，让我们重复上面的实验，并计算每个算法得到最佳结果的次数。</p><pre class="kj kk kl km gt ns nt nu nv aw nw bi"><span id="c52d" class="nx la it nt b gy ny nz l oa ob">wins = run_comparison(bernoulli_bandits)<br/>for g, w in zip(get_agents(), wins):<br/>    print(g, w)</span></pre><p id="cc9c" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">我的结果是Epsilon-Greedy为397，UCB1为0，UCB1-Tuned为220，Thompson Sampling为383。看起来Thompson采样和Epsilon-Greedy是我们特定设置的赢家。UCB1-Tuned获得第三名有点令人惊讶，因为在之前的实验中，它的表现超过了ε-Greedy算法。这可能是由于Thompson Sampling窃取了它的一些成果，但这需要更多的调查。</p><h1 id="8936" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">结论</h1><p id="ca67" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">在这篇文章中，我们研究了汤普森采样算法是如何工作的，并为伯努利强盗实现了它。然后，我们将它与其他多臂强盗算法进行比较，发现它的性能与Epsilon-Greedy差不多。</p></div><div class="ab cl ms mt hx mu" role="separator"><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx"/></div><div class="im in io ip iq"><p id="302d" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><a class="ae ky" href="https://eminik355.medium.com/membership" rel="noopener"> <strong class="lt iu">成为Medium</strong></a><strong class="lt iu">的会员，获得所有故事的全部权限。你的会员费直接支持你读的作家。</strong></p></div><div class="ab cl ms mt hx mu" role="separator"><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx"/></div><div class="im in io ip iq"><p id="fb0b" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><strong class="lt iu">出自同一作者。</strong></p><div class="no np gp gr nq mz"><a rel="noopener follow" target="_blank" href="/going-bayesian-testing-rate-metrics-82e872b79175"><div class="na ab fo"><div class="nb ab nc cl cj nd"><h2 class="bd iu gy z fp ne fr fs nf fu fw is bi translated">走向贝叶斯:测试速率度量</h2><div class="ng l"><h3 class="bd b gy z fp ne fr fs nf fu fw dk translated">如何在没有p值和置信区间的情况下运行速率度量的A/B测试？</h3></div><div class="nh l"><p class="bd b dl z fp ne fr fs nf fu fw dk translated">towardsdatascience.com</p></div></div><div class="ni l"><div class="pk l nk nl nm ni nn ks mz"/></div></div></a></div><div class="no np gp gr nq mz"><a href="https://medium.com/analytics-vidhya/calculating-using-monte-carlo-simulations-337cff638ac5" rel="noopener follow" target="_blank"><div class="na ab fo"><div class="nb ab nc cl cj nd"><h2 class="bd iu gy z fp ne fr fs nf fu fw is bi translated">使用蒙特卡罗模拟计算𝛑</h2><div class="ng l"><h3 class="bd b gy z fp ne fr fs nf fu fw dk translated">用Python代码介绍蒙特卡罗模拟。</h3></div><div class="nh l"><p class="bd b dl z fp ne fr fs nf fu fw dk translated">medium.com</p></div></div><div class="ni l"><div class="pl l nk nl nm ni nn ks mz"/></div></div></a></div></div><div class="ab cl ms mt hx mu" role="separator"><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx"/></div><div class="im in io ip iq"><h1 id="cad9" class="kz la it bd lb lc pm le lf lg pn li lj jz po ka ll kc pp kd ln kf pq kg lp lq bi translated">参考</h1><p id="be5e" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">[1]汤普森采样教程。(未注明)。检索于2021年9月25日，来自<a class="ae ky" href="https://web.stanford.edu/~bvr/pubs/TS_Tutorial.pdf." rel="noopener ugc nofollow" target="_blank">https://web.stanford.edu/~bvr/pubs/TS_Tutorial.pdf.</a></p></div></div>    
</body>
</html>