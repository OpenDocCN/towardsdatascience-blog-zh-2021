<html>
<head>
<title>The Making of an AI Storyteller</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">人工智能说书人的制作</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/the-making-of-an-ai-storyteller-c3b8d5a983f5?source=collection_archive---------8-----------------------#2021-07-30">https://towardsdatascience.com/the-making-of-an-ai-storyteller-c3b8d5a983f5?source=collection_archive---------8-----------------------#2021-07-30</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="2fae" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsdatascience.com/tagged/hands-on-tutorials" rel="noopener" target="_blank">实践教程</a></h2><div class=""/><div class=""><h2 id="b22d" class="pw-subtitle-paragraph jw iz iq bd b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn dk translated">如何在Google AI平台上为故事生成准备数据和微调T5模型</h2></div><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ko"><img src="../Images/95d22826e82e0bec9721fbe1af2b7a8f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*uZadPvEDPE8T5MIu"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">照片由<a class="ae le" href="https://unsplash.com/@fredmarriage?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">弗雷迪婚姻</a>在<a class="ae le" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a></p></figure><p id="89a4" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">你还记得上一次学习一门新语言是什么时候吗？你有一长串的单词需要记忆，有时你并没有完全理解这些单词是如何使用的。你想读书，但大多数书都超出了你的水平，课本让你昏昏欲睡。</p><p id="0817" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">现在，想象一下，你有一个工具，可以在你的水平上制作无限数量的例句，甚至一口大小的故事。你玩不同的单词组合，学习它们是如何相互作用的。这些故事读起来也很有趣！</p><p id="45c7" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">这是我对人工智能故事生成如何帮助语言学习者的看法。</p><p id="2fb9" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">在过去的几周里，我一直在做一个概念验证项目，从给定的单词列表中造出句子和故事。下面的屏幕记录显示了它的样子，使用实际训练的模型:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi mb"><img src="../Images/d46d6ae5f51179cdf8599226f7fe7c5d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*8r0M6ho4AzaWqPzsTIhmeQ.gif"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">使用基于T5的微调模型，根据单词列表制作故事的演示</p></figure><p id="1f5c" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">这个项目是受报纸的启发</p><ul class=""><li id="cd85" class="mc md iq lh b li lj ll lm lo me ls mf lw mg ma mh mi mj mk bi translated"><a class="ae le" href="https://arxiv.org/abs/1811.05701v1" rel="noopener ugc nofollow" target="_blank">计划和写作:实现更好的自动讲故事</a></li><li id="a3f5" class="mc md iq lh b li ml ll mm lo mn ls mo lw mp ma mh mi mj mk bi translated"><a class="ae le" href="https://arxiv.org/abs/1910.10683" rel="noopener ugc nofollow" target="_blank">利用统一的文本到文本转换器探索迁移学习的局限性</a></li></ul><p id="74ea" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">在本文的剩余部分，我将分享我为这个POC项目准备深度学习模型的经验。我将谈论模型训练的高级方法和一些技术方面，这是未来迭代的基础。我将特别介绍:</p><ul class=""><li id="d408" class="mc md iq lh b li lj ll lm lo me ls mf lw mg ma mh mi mj mk bi translated">方法论</li><li id="079c" class="mc md iq lh b li ml ll mm lo mn ls mo lw mp ma mh mi mj mk bi translated">如何准备培训数据</li><li id="41fd" class="mc md iq lh b li ml ll mm lo mn ls mo lw mp ma mh mi mj mk bi translated">如何使用拥抱脸的<a class="ae le" href="https://huggingface.co/transformers/" rel="noopener ugc nofollow" target="_blank">变形金刚</a>库和<a class="ae le" href="https://cloud.google.com/ai-platform/docs/technical-overview" rel="noopener ugc nofollow" target="_blank">谷歌人工智能平台</a>微调一个预先训练好的<a class="ae le" href="https://ai.googleblog.com/2020/02/exploring-transfer-learning-with-t5.html" rel="noopener ugc nofollow" target="_blank"> T5模型</a></li><li id="706b" class="mc md iq lh b li ml ll mm lo mn ls mo lw mp ma mh mi mj mk bi translated">如何在一个(免费的)Google Colab笔记本上监控进度<a class="ae le" href="https://colab.research.google.com" rel="noopener ugc nofollow" target="_blank"/></li><li id="4864" class="mc md iq lh b li ml ll mm lo mn ls mo lw mp ma mh mi mj mk bi translated">一旦有了模型，如何生成文本</li><li id="f47c" class="mc md iq lh b li ml ll mm lo mn ls mo lw mp ma mh mi mj mk bi translated">由人(我)进行的模型评估🙋‍♂️)</li></ul><p id="9f8e" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">剧透:像拥抱脸的变形金刚这样的库已经变得如此成熟和方便，以至于你甚至不需要知道PyTorch或Tensorflow或任何深度学习理论，就可以训练和使用最先进的深度学习模型，如GPT，T5，BERT等。相反，最耗时和最容易出错的步骤是正确设置培训管道。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi mq"><img src="../Images/86b37664955dfc99d64242718553ab45.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DGFOTGKNSsYPnP_qNxjHeg.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">继续读下去，这并不像预期的那么容易</p></figure><p id="ac24" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">如果你想了解更多关于谷歌人工智能平台、深度学习语言模型或人工智能故事生成的信息，你可能会在文章末尾找到一些额外的资源。</p><p id="186d" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">免责声明:我不隶属于谷歌。我正在探索谷歌的云服务，因为我在工作中主要使用AWS，所以尝试其他平台进行辅助项目似乎很有趣。</p><h1 id="bfff" class="ms mt iq bd mu mv mw mx my mz na nb nc kf nd kg ne ki nf kj ng kl nh km ni nj bi translated">方法学</h1><p id="2691" class="pw-post-body-paragraph lf lg iq lh b li nk ka lk ll nl kd ln lo nm lq lr ls nn lu lv lw no ly lz ma ij bi translated">我们的任务是制作一个模型:</p><pre class="kp kq kr ks gt np nq nr ns aw nt bi"><span id="420d" class="nu mt iq nq b gy nv nw l nx ny"><strong class="nq ja">Me:</strong> Hi Model, tell me a story about "Friday evening". The story must contain the words "Tom" and "dinner".</span><span id="8e2e" class="nu mt iq nq b gy nz nw l nx ny"><strong class="nq ja">Model:</strong> Tom was walking to his favourite restaurant on a Friday evening. He would have dinner with his family. When he arrived he found the door was closed.</span></pre><p id="2b37" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">我们可以将此表述为文本到文本的转换问题，其中输入源包含标题和所需的单词:</p><pre class="kp kq kr ks gt np nq nr ns aw nt bi"><span id="0ce0" class="nu mt iq nq b gy nv nw l nx ny">"&lt;title&gt; Friday evening &lt;words&gt; Tom, dinner"</span></pre><p id="f2db" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">输出目标是故事:</p><pre class="kp kq kr ks gt np nq nr ns aw nt bi"><span id="aed7" class="nu mt iq nq b gy nv nw l nx ny">"&lt;story&gt; Tom was walking... was closed."</span></pre><p id="39c2" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">为了实现这一点，我们可能会创建和训练我们自己的深度学习模型架构，就像在<a class="ae le" href="https://arxiv.org/abs/1811.05701v1" rel="noopener ugc nofollow" target="_blank">的规划和撰写</a>论文中那样。虽然这很有趣，并可能实现最佳性能，但对于POC项目来说，这需要太多的工作和成本(GPU的使用)。</p><p id="fcfd" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">或者，我们可以微调一个预先训练好的最先进的语言模型，如<a class="ae le" href="http://jalammar.github.io/illustrated-transformer/" rel="noopener ugc nofollow" target="_blank">变形金刚</a>或<a class="ae le" href="http://jalammar.github.io/illustrated-gpt2/" rel="noopener ugc nofollow" target="_blank"> GPT-2 </a>。在拥抱脸的网站上，有数以千计的预先训练好的模特，有英语的也有非英语的。如果我们选择这条路径，我们将故事标题和要使用的单词作为写作提示，模型将继续完成故事。</p><p id="2892" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">顺便说一下，如果你对通用文本生成器会产生什么感到好奇，你可以试试<a class="ae le" href="https://bellard.org/textsynth/" rel="noopener ugc nofollow" target="_blank">这个</a>和<a class="ae le" href="https://transformer.huggingface.co" rel="noopener ugc nofollow" target="_blank">这个</a>。</p><p id="9d34" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">在这个项目中，我选择了谷歌的T5型号，它是为任何文本到文本的转换任务而设计的。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi oa"><img src="../Images/1cab3b03b9bf800dc719ff5c1750b5f8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HIOf9llsqWzun1C9c8aynA.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated"><a class="ae le" href="https://ai.googleblog.com/2020/02/exploring-transfer-learning-with-t5.html" rel="noopener ugc nofollow" target="_blank">https://ai . Google blog . com/2020/02/exploring-transfer-learning-with-t5 . html</a></p></figure><p id="ed90" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">无论如何，如果在这个概念验证项目中，Transformer、GPT-2或T5之间在输出质量方面存在巨大差异，我会感到惊讶。数据集和<a class="ae le" href="https://huggingface.co/blog/how-to-generate" rel="noopener ugc nofollow" target="_blank">解码方法</a>的选择在这里可能会发挥更大的作用。</p><h1 id="0001" class="ms mt iq bd mu mv mw mx my mz na nb nc kf nd kg ne ki nf kj ng kl nh km ni nj bi translated">数据准备</h1><p id="a7cf" class="pw-post-body-paragraph lf lg iq lh b li nk ka lk ll nl kd ln lo nm lq lr ls nn lu lv lw no ly lz ma ij bi translated">在线词典、语言学习网站和儿童书籍可能是这项工作的理想来源:句子和故事将是最相关和最准确的语言学习。缺点是实现爬虫的额外工作和潜在的版权问题。</p><p id="d7c9" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">因此，在这个项目中，我使用了<a class="ae le" href="https://cs.rochester.edu/nlp/rocstories/" rel="noopener ugc nofollow" target="_blank">故事完形填空和ROCStories语料库</a>来训练模型。这个数据集由98，159个五句话的故事组成:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ob"><img src="../Images/d33728d27bdc2522e6fd1d51eddcc7d8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*L62bU1HRG0-95Y5jZlhRag.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">ROCStories数据集示例。来源:https://cs.rochester.edu/nlp/rocstories/<a class="ae le" href="https://cs.rochester.edu/nlp/rocstories/" rel="noopener ugc nofollow" target="_blank"/></p></figure><p id="2809" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">我选择这个数据集是因为它很小，故事也很有趣。请记住，我们的目标是为语言学习者制作一个故事生成器来掌握词汇和基本语法结构。像这样的小故事听起来很完美。</p><p id="d2c8" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">但是等等。</p><p id="a5ed" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">我们只有故事，我们希望我们训练过的模型能产生故事。培训的输入(提示)在哪里？</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ko"><img src="../Images/4cef365fc37751902924576a587514e3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*NDJFb42rjcFIQbnP"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">Miguel A. Amutio 在<a class="ae le" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="08b5" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">幸运的是，我们可以使用另外两种NLP技术:自动<a class="ae le" href="https://en.wikipedia.org/wiki/Automatic_summarization" rel="noopener ugc nofollow" target="_blank">文本摘要</a>和<a class="ae le" href="https://en.wikipedia.org/wiki/Keyword_extraction" rel="noopener ugc nofollow" target="_blank">关键词提取</a>。这个想法是，</p><ol class=""><li id="3738" class="mc md iq lh b li lj ll lm lo me ls mf lw mg ma oc mi mj mk bi translated">我们从原文中提取一个摘要或几个关键短语。</li><li id="9e53" class="mc md iq lh b li ml ll mm lo mn ls mo lw mp ma oc mi mj mk bi translated">在训练期间，模型学习扩展摘要或关键短语以恢复原始文本。</li></ol><p id="0639" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">正如<a class="ae le" href="https://arxiv.org/abs/1811.05701v1" rel="noopener ugc nofollow" target="_blank">计划和撰写</a>论文的作者所提议的，为了准备训练数据集，从故事的每个句子中提取一个单词；输入提示将是故事标题和五个单词(作者称之为故事情节)。例如，这个故事:</p><pre class="kp kq kr ks gt np nq nr ns aw nt bi"><span id="c3e5" class="nu mt iq nq b gy nv nw l nx ny"><strong class="nq ja">Pizzeria</strong></span><span id="1754" class="nu mt iq nq b gy nz nw l nx ny">My friend and I walked into the <strong class="nq ja">pizza</strong> store.<br/>The server greeted us and offered <strong class="nq ja">specials</strong> to choose from.<br/>We decided to order a large pizza with <strong class="nq ja">coke</strong>.<br/>After a few minutes, the server placed the <strong class="nq ja">food</strong> on the table.<br/>We slowly <strong class="nq ja">ate</strong> it.</span></pre><p id="8b10" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">会有输入提示:</p><pre class="kp kq kr ks gt np nq nr ns aw nt bi"><span id="59ab" class="nu mt iq nq b gy nv nw l nx ny">Storyline: pizza -&gt; special -&gt; coke -&gt; food -&gt; eat<br/>Title: Pizzeria</span></pre><p id="812d" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">以下是训练数据集的示例:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi od"><img src="../Images/e77ba054564e56c0739e28661a4a7513.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Qy_lL8JSTr-M5SVIp69jJg.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">训练数据集(示例)</p></figure><p id="09db" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">我使用T5的特殊标记<code class="fe oe of og nq b">&lt;extra_id_0&gt;</code>和<code class="fe oe of og nq b">&lt;extra_id_1&gt;</code>作为分隔符，但这不是严格要求的。</p><p id="bfc9" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated"><a class="ae le" href="https://medium.com/mlearning-ai/10-popular-keyword-extraction-algorithms-in-natural-language-processing-8975ada5750c" rel="noopener">RAKE和TextRank </a>等算法常用于从文章中提取关键词。然而，由于我们的故事很短，这些依赖于词频和共现的算法经常给大多数单词分配相同的分数。例如，在上面的比萨饼店故事中，RAKE为所有这些单词返回score=1:</p><pre class="kp kq kr ks gt np nq nr ns aw nt bi"><span id="02fd" class="nu mt iq nq b gy nv nw l nx ny">walked, table, order, minutes, friend, food, decided, coke, choose</span></pre><p id="fa0d" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">因此，为了从故事中提取关键词(故事线)，我使用库<a class="ae le" href="https://github.com/MaartenGr/KeyBERT" rel="noopener ugc nofollow" target="_blank"> KeyBERT </a>，它利用BERT嵌入来评估每个单词的重要性。这里我们可以看到所有的单词都有不同的分数:</p><pre class="kp kq kr ks gt np nq nr ns aw nt bi"><span id="fb50" class="nu mt iq nq b gy nv nw l nx ny">('pizza', 0.6226), ('coke', 0.3874), ('food', 0.332),<br/>('ate', 0.3044), ('decided', 0.2379), ('specials', 0.236),<br/>('store', 0.2174), ('order', 0.2131), ('greeted', 0.212),<br/>('friend', 0.1753), ('walked', 0.1696), ('minutes', 0.166), ('table', 0.1624), ('offered', 0.1551), ('large', 0.1443),<br/>('server', 0.1185), ('choose', 0.1057), ('placed', 0.0816),<br/>('slowly', 0.0454)</span></pre><p id="af0e" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">当然，这并不一定意味着分数更正确或者提取的顶部单词更适合训练我们的故事生成模型。也许如果我们在训练中随机选择单词，我们会有一个更好的模型？但至少现在，我不需要担心关系破裂。😅</p><p id="8c1b" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">不管怎样，下面是Python的源代码:</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="oh oi l"/></div></figure><p id="07ed" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">请注意，故事情节中的单词是词汇化的(例如，“游泳”变成了“游泳”)。这可能会给模型更多的自由来决定故事发生的时间。用户可以使用“昨天”或“明天”这样的词来控制时间。</p><h1 id="5372" class="ms mt iq bd mu mv mw mx my mz na nb nc kf nd kg ne ki nf kj ng kl nh km ni nj bi translated">模特培训</h1><p id="4c59" class="pw-post-body-paragraph lf lg iq lh b li nk ka lk ll nl kd ln lo nm lq lr ls nn lu lv lw no ly lz ma ij bi translated">一旦我们准备好数据集，我们就可以训练我们的模型。您可以在这里查看我的Python脚本和其他相关的设置文件:</p><div class="oj ok gp gr ol om"><a href="https://github.com/kenilc/finetune4textgen" rel="noopener  ugc nofollow" target="_blank"><div class="on ab fo"><div class="oo ab op cl cj oq"><h2 class="bd ja gy z fp or fr fs os fu fw iz bi translated">GitHub - kenilc/finetune4textgen:一个python脚本，用于微调文本的预训练模型…</h2><div class="ot l"><h3 class="bd b gy z fp or fr fs os fu fw dk translated">一个小python脚本，用于微调文本生成的预训练模型。该脚本大量使用了内置的…</h3></div><div class="ou l"><p class="bd b dl z fp or fr fs os fu fw dk translated">github.com</p></div></div><div class="ov l"><div class="ow l ox oy oz ov pa ky om"/></div></div></a></div><p id="a98f" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">剧本很直白；它本质上是用于训练和参数解析的<a class="ae le" href="https://huggingface.co/transformers/" rel="noopener ugc nofollow" target="_blank">拥抱脸函数</a>的包装器，以及一些下载和上传数据文件和模型检查点的辅助函数。</p><p id="8b09" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">当你决定在<a class="ae le" href="https://cloud.google.com/ai-platform/training/docs" rel="noopener ugc nofollow" target="_blank">谷歌人工智能平台</a>上训练你的模型时，乐趣就开始了。</p><p id="9b1f" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">当然，不可能在一个中帖里提供一步一步使用平台的指南。我要分享的是在我为我的故事生成模型建立培训工作的过程中遇到的阻碍。如果你对这个平台完全陌生，我强烈建议你遵循Google的官方快速入门指南，理解标准的项目结构和命令是如何工作的。</p><div class="oj ok gp gr ol om"><a href="https://github.com/GoogleCloudPlatform/ai-platform-samples/tree/master/quickstart" rel="noopener  ugc nofollow" target="_blank"><div class="on ab fo"><div class="oo ab op cl cj oq"><h2 class="bd ja gy z fp or fr fs os fu fw iz bi translated">人工智能-平台-示例/master Google cloud platform/人工智能-平台-示例快速入门</h2><div class="ot l"><h3 class="bd b gy z fp or fr fs os fu fw dk translated">如果这是你第一次使用人工智能平台，我们建议你看一下人工智能平台介绍文档…</h3></div><div class="ou l"><p class="bd b dl z fp or fr fs os fu fw dk translated">github.com</p></div></div><div class="ov l"><div class="pb l ox oy oz ov pa ky om"/></div></div></a></div><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi pc"><img src="../Images/c981d7222a41d8900c658a235b3ce95f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*fv8msqPtOnfyxgl0"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">在<a class="ae le" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上由<a class="ae le" href="https://unsplash.com/@kreyatif?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">micha Mancewicz</a>拍摄的照片</p></figure><h2 id="5e71" class="nu mt iq bd mu pd pe dn my pf pg dp nc lo ph pi ne ls pj pk ng lw pl pm ni iw bi translated">墨菲定律</h2><p id="ce99" class="pw-post-body-paragraph lf lg iq lh b li nk ka lk ll nl kd ln lo nm lq lr ls nn lu lv lw no ly lz ma ij bi translated">这是我第一次在人工智能平台上训练模型。当我在谷歌托管的Jupyter笔记本电脑上进行本地测试时，它开始得很好，但是当我转移到GPU时，这里那里都有障碍。</p><p id="9368" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">以下是一些可能出错的事情:</p><ul class=""><li id="8f9b" class="mc md iq lh b li lj ll lm lo me ls mf lw mg ma mh mi mj mk bi translated">错误的存储桶权限。您需要为ML引擎设置一个服务帐户，并授予它读写您的<a class="ae le" href="https://cloud.google.com/ai-platform/prediction/docs/working-with-cloud-storage" rel="noopener ugc nofollow" target="_blank"> GCS bucket </a>的权限。bucket将存储临时包，以及日志和您的模型检查点。</li><li id="0118" class="mc md iq lh b li ml ll mm lo mn ls mo lw mp ma mh mi mj mk bi translated">无法安装必要的软件包。我用<code class="fe oe of og nq b">setup.up</code>尝试了默认的打包方式，但是库<code class="fe oe of og nq b">pyarrow</code>和<code class="fe oe of og nq b">datasets</code>有错误。经过多次尝试，我用一个定制的docker容器解决了这个问题(见下文)。</li></ul><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi pn"><img src="../Images/573d2921ba8290da149e3d3f9c132f67.png" data-original-src="https://miro.medium.com/v2/resize:fit:1314/format:webp/1*FNT5yGa3Ui9HEZ3vFevVxA.png"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">如果你知道如何解决这个问题，请留下评论。</p></figure><ul class=""><li id="ea81" class="mc md iq lh b li lj ll lm lo me ls mf lw mg ma mh mi mj mk bi translated">即使作业在本地或CPU实例上运行良好，当您在GPU上运行它时，它仍然可能会失败。</li><li id="a65e" class="mc md iq lh b li ml ll mm lo mn ls mo lw mp ma mh mi mj mk bi translated">并非所有GPU在所有地区都可用。查看<a class="ae le" href="https://cloud.google.com/compute/docs/gpus/gpu-regions-zones" rel="noopener ugc nofollow" target="_blank"> GPU区域和分区的可用性</a>。</li><li id="2d6d" class="mc md iq lh b li ml ll mm lo mn ls mo lw mp ma mh mi mj mk bi translated">一些地区的GPU资源比其他地区少。您可能会得到一个超时错误，说明当前没有可用的资源。</li><li id="3483" class="mc md iq lh b li ml ll mm lo mn ls mo lw mp ma mh mi mj mk bi translated">在训练期间，所有文件都存储在本地，除非它们被显式上传到GCS bucket(或其他地方)。永远不要忘记这一点。</li><li id="c200" class="mc md iq lh b li ml ll mm lo mn ls mo lw mp ma mh mi mj mk bi translated">根据docker容器的创建方式，每当您更改训练脚本时，您可能需要创建一个新的映像。一个更好的解决方案是创建一个包含所有包依赖项的docker映像，然后使用<code class="fe oe of og nq b">setup.py</code>打包您的源代码。</li></ul><h2 id="39c5" class="nu mt iq bd mu pd pe dn my pf pg dp nc lo ph pi ne ls pj pk ng lw pl pm ni iw bi translated">Dockerfile文件</h2><p id="e3fa" class="pw-post-body-paragraph lf lg iq lh b li nk ka lk ll nl kd ln lo nm lq lr ls nn lu lv lw no ly lz ma ij bi translated">设置很简单，除了我需要在docker容器中使用一个虚拟Python环境来避免一些警告。</p><p id="6a17" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">同样，这可以通过仅将包依赖关系编译到docker映像中并使用<code class="fe oe of og nq b">setup.py</code>打包训练脚本来优化。</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="oh oi l"/></div></figure><p id="7da7" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">使用定制docker容器的一个优点是，包安装错误会在编译时被发现。</p><h2 id="2c08" class="nu mt iq bd mu pd pe dn my pf pg dp nc lo ph pi ne ls pj pk ng lw pl pm ni iw bi translated">Docker映像构建脚本</h2><p id="5548" class="pw-post-body-paragraph lf lg iq lh b li nk ka lk ll nl kd ln lo nm lq lr ls nn lu lv lw no ly lz ma ij bi translated">在使用之前，我们需要将自定义docker图像推送到Google容器注册表。为了减少<a class="ae le" href="https://cloud.google.com/container-registry/pricing" rel="noopener ugc nofollow" target="_blank">成本</a>，不要忘记从注册表中删除不再需要的图像。</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="oh oi l"/></div></figure><h2 id="b2ed" class="nu mt iq bd mu pd pe dn my pf pg dp nc lo ph pi ne ls pj pk ng lw pl pm ni iw bi translated">作业提交脚本</h2><p id="72eb" class="pw-post-body-paragraph lf lg iq lh b li nk ka lk ll nl kd ln lo nm lq lr ls nn lu lv lw no ly lz ma ij bi translated">没有比在GPU上花费数小时，却发现我们忘记保存模型文件更令人沮丧的了。所以永远记得正确上传模型检查点和日志<strong class="lh ja"/>和<strong class="lh ja">定期</strong>到GCS桶。每个文件，包括日志和输出目录，只存在于容器中，并且必须明确上传到远程作业目录中进行永久存储。</p><p id="240d" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">在我的脚本中，我用抱抱脸的<a class="ae le" href="https://huggingface.co/transformers/main_classes/callback.html" rel="noopener ugc nofollow" target="_blank">训练器回调</a>类实现了常规上传(点击查看源代码<a class="ae le" href="https://github.com/kenilc/finetune4textgen/blob/74e819ab6f93531c718d0f03e751ee3ee3e4e77c/trainer/util.py#L65" rel="noopener ugc nofollow" target="_blank">)。</a></p><p id="9a3c" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">为了用<a class="ae le" href="https://www.tensorflow.org/tensorboard/" rel="noopener ugc nofollow" target="_blank"> TensorBoard </a>监控训练进度，我们需要添加参数<code class="fe oe of og nq b">--report_to tensorboard</code>。TensorBoard格式的登录由Hugging Face的<a class="ae le" href="https://huggingface.co/transformers/main_classes/callback.html#transformers.integrations.TensorBoardCallback" rel="noopener ugc nofollow" target="_blank"> TensorBoardCallback </a>类处理。它对Tensorflow和PyTorch都有效。</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="oh oi l"/></div></figure><p id="48cc" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">实现所有这些命令行参数似乎很乏味。幸运的是，拥抱脸为此专门提供了两个类:<a class="ae le" href="https://huggingface.co/transformers/_modules/transformers/training_args.html" rel="noopener ugc nofollow" target="_blank"> TrainingArguments </a>和<a class="ae le" href="https://huggingface.co/transformers/_modules/transformers/hf_argparser.html" rel="noopener ugc nofollow" target="_blank"> HfArgumentParser </a>。更好的是，参数解析器是通用的，您可以将它用于任何需要命令行界面的Python程序。</p><div class="oj ok gp gr ol om"><a href="https://python.plainenglish.io/how-to-automatically-generate-command-line-interface-for-python-programs-e9fd9b6a99ca" rel="noopener  ugc nofollow" target="_blank"><div class="on ab fo"><div class="oo ab op cl cj oq"><h2 class="bd ja gy z fp or fr fs os fu fw iz bi translated">如何为Python程序自动生成命令行界面</h2><div class="ot l"><h3 class="bd b gy z fp or fr fs os fu fw dk translated">拥抱脸的HfArgumentParser是你的朋友。</h3></div><div class="ou l"><p class="bd b dl z fp or fr fs os fu fw dk translated">python .平原英语. io</p></div></div><div class="ov l"><div class="po l ox oy oz ov pa ky om"/></div></div></a></div><h2 id="7876" class="nu mt iq bd mu pd pe dn my pf pg dp nc lo ph pi ne ls pj pk ng lw pl pm ni iw bi translated">最后的步骤</h2><p id="2be3" class="pw-post-body-paragraph lf lg iq lh b li nk ka lk ll nl kd ln lo nm lq lr ls nn lu lv lw no ly lz ma ij bi translated">经过所有的努力，我们终于可以开始工作了！💪</p><pre class="kp kq kr ks gt np nq nr ns aw nt bi"><span id="ea3f" class="nu mt iq nq b gy nv nw l nx ny"># Step 1: Upload data to Google Cloud Storage<br/>&gt; gsutil cp -r ./data gs://storygen-bucket</span><span id="2e05" class="nu mt iq nq b gy nz nw l nx ny"># Step 2: Build docker image and push to Google Container Registry<br/>&gt; ./build.sh</span><span id="27b4" class="nu mt iq nq b gy nz nw l nx ny"># Step 3: Start the training process<br/>&gt; ./run.sh</span></pre><h1 id="0f23" class="ms mt iq bd mu mv mw mx my mz na nb nc kf nd kg ne ki nf kj ng kl nh km ni nj bi translated">作业监控</h1><h2 id="65c8" class="nu mt iq bd mu pd pe dn my pf pg dp nc lo ph pi ne ls pj pk ng lw pl pm ni iw bi translated">系统利用</h2><p id="620c" class="pw-post-body-paragraph lf lg iq lh b li nk ka lk ll nl kd ln lo nm lq lr ls nn lu lv lw no ly lz ma ij bi translated">用一个有几个训练步骤的作业来测试运行，并检查计算和内存利用率，这总是一个好主意。例如，您可以通过增加批处理大小来加速训练，直到GPU的内存得到充分利用。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi pp"><img src="../Images/4db9186b00a532faddb339748e4c4333.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rpnD5oKtMSpaDhqYV5lYKw.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">在作业详细信息选项卡上检查GPU利用率</p></figure><h2 id="d22c" class="nu mt iq bd mu pd pe dn my pf pg dp nc lo ph pi ne ls pj pk ng lw pl pm ni iw bi translated">模型训练进度</h2><p id="2bcf" class="pw-post-body-paragraph lf lg iq lh b li nk ka lk ll nl kd ln lo nm lq lr ls nn lu lv lw no ly lz ma ij bi translated">我们可能会启动一个免费的Google Colab笔记本来监控培训进度。关于单元格1、2和3的进一步解释，请参见<a class="ae le" href="https://medium.com/analytics-vidhya/how-to-access-files-from-google-cloud-storage-in-colab-notebooks-8edaf9e6c020" rel="noopener">如何从Colab笔记本的Google云存储中访问文件</a>。TensorBoard会自动下载日志文件，但我们仍然需要刷新以获得最新结果。</p><pre class="kp kq kr ks gt np nq nr ns aw nt bi"><span id="2b49" class="nu mt iq nq b gy nv nw l nx ny"># Cell 1<br/>from google.colab import auth<br/>auth.authenticate_user()</span><span id="06e3" class="nu mt iq nq b gy nz nw l nx ny"># Cell 2<br/>!curl https://sdk.cloud.google.com | bash</span><span id="afae" class="nu mt iq nq b gy nz nw l nx ny"># Cell 3<br/>!gcloud init</span><span id="bd74" class="nu mt iq nq b gy nz nw l nx ny"># Cell 4<br/>%reload_ext tensorboard<br/>%tensorboard --logdir gs://bucket_name/job_dir/logs</span></pre><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi pq"><img src="../Images/ca61e817cac1b43ba86704ceb7b59866.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ykDwjec68tM7dx5VfXbaRw.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">在Colab笔记本上运行TensorBoard以监控训练进度</p></figure><h1 id="1afa" class="ms mt iq bd mu mv mw mx my mz na nb nc kf nd kg ne ki nf kj ng kl nh km ni nj bi translated">使用训练模型的文本生成</h1><p id="6424" class="pw-post-body-paragraph lf lg iq lh b li nk ka lk ll nl kd ln lo nm lq lr ls nn lu lv lw no ly lz ma ij bi translated">多么漫长的旅程啊！</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi pr"><img src="../Images/d21e08b879c9b55c275635ba3ab9b6fe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1054/format:webp/1*JfhuabkfuzYdRKyAUUzFXw.png"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">最终模型检查点</p></figure><p id="4251" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">给定一个经过训练的模型，我们仍然需要实验并决定如何使用它来生成文本。这就是所谓的解码，不同的解码方法和参数产生的结果大相径庭。</p><div class="oj ok gp gr ol om"><a href="https://huggingface.co/blog/how-to-generate" rel="noopener  ugc nofollow" target="_blank"><div class="on ab fo"><div class="oo ab op cl cj oq"><h2 class="bd ja gy z fp or fr fs os fu fw iz bi translated">如何生成文本:使用不同的解码方法通过转换器生成语言</h2><div class="ot l"><h3 class="bd b gy z fp or fr fs os fu fw dk translated">近年来，由于大规模语言生成的兴起，人们对开放式语言生成越来越感兴趣。</h3></div><div class="ou l"><p class="bd b dl z fp or fr fs os fu fw dk translated">huggingface.co</p></div></div><div class="ov l"><div class="ps l ox oy oz ov pa ky om"/></div></div></a></div><p id="6bd7" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">这是文本生成的函数。<code class="fe oe of og nq b">**kwargs</code>包含所有可选参数，如温度、top-p、是否进行采样或光束搜索等。默认是贪婪解码:概率最高的单词一个一个输出。当语句结束标记或最大。已达到长度。</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="oh oi l"/></div></figure><h1 id="0589" class="ms mt iq bd mu mv mw mx my mz na nb nc kf nd kg ne ki nf kj ng kl nh km ni nj bi translated">模型评估</h1><p id="b257" class="pw-post-body-paragraph lf lg iq lh b li nk ka lk ll nl kd ln lo nm lq lr ls nn lu lv lw no ly lz ma ij bi translated"><a class="ae le" href="https://arxiv.org/abs/2006.14799" rel="noopener ugc nofollow" target="_blank">文本生成</a>模型和系统的评估是困难的，仍然是一个开放的研究问题。已经提出了度量标准，但是人类的判断仍然被广泛使用并被认为是黄金标准。有趣的是，最近有一篇<a class="ae le" href="https://arxiv.org/abs/2107.00061" rel="noopener ugc nofollow" target="_blank">论文</a>指出，即使是人类的评价也经常是不一致和矛盾的。🤦🏼‍♂️</p><p id="0a80" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">我将在未来发布其他配方、模型及其评估的更新。同时，以下是一些生成的故事(标题和故事情节作为输入):</p><pre class="kp kq kr ks gt np nq nr ns aw nt bi"><span id="a8e5" class="nu mt iq nq b gy nv nw l nx ny">Title: Joey's First Snowman<br/>Storyline: snow, snowman, snowball, smile, Joey</span><span id="352f" class="nu mt iq nq b gy nz nw l nx ny">Joey was excited to get his first snowman.<br/>He decided to make a <strong class="nq ja">snowman</strong> for his birthday.<br/>He sat down and watched the <strong class="nq ja">snowballs</strong>.<br/>He <strong class="nq ja">smiled</strong> and screamed.<br/><strong class="nq ja">Joey's</strong> first snow man was a great day.</span><span id="4234" class="nu mt iq nq b gy nz nw l nx ny">* * *</span><span id="97ad" class="nu mt iq nq b gy nz nw l nx ny">Title: Back in Class<br/>Storyline: Lucy, basement, teacher, hour, laugh</span><span id="f7cc" class="nu mt iq nq b gy nz nw l nx ny"><strong class="nq ja">Lucy</strong> was back in class.<br/>She was in the <strong class="nq ja">basement</strong>.<br/>The <strong class="nq ja">teacher</strong> was a bit nervous.<br/>Lucy had to wait for <strong class="nq ja">hours</strong>.<br/>She <strong class="nq ja">laughed</strong> and sat down to watch the show.</span><span id="4013" class="nu mt iq nq b gy nz nw l nx ny">* * *</span><span id="b688" class="nu mt iq nq b gy nz nw l nx ny">Title: The Rock<br/>Storyline: yard, stone, heavy, chisel, rock</span><span id="0c3f" class="nu mt iq nq b gy nz nw l nx ny">I was walking in my <strong class="nq ja">yard</strong>.<br/>I saw a large <strong class="nq ja">stone</strong>.<br/>It was too <strong class="nq ja">heavy</strong>.<br/>I <strong class="nq ja">chiseled</strong> it.<br/>I realized that the <strong class="nq ja">rock</strong> was too big.</span><span id="094f" class="nu mt iq nq b gy nz nw l nx ny">* * *</span><span id="99e8" class="nu mt iq nq b gy nz nw l nx ny">Title: Playing video games<br/>Storyline: Calvin, play, hard, start, beat</span><span id="cfeb" class="nu mt iq nq b gy nz nw l nx ny"><strong class="nq ja">Calvin</strong> was playing video games.<br/>He wanted to <strong class="nq ja">play</strong> it.<br/>He tried very <strong class="nq ja">hard</strong>.<br/>He <strong class="nq ja">started</strong> playing it.<br/>Eventually, he <strong class="nq ja">beat</strong> himself.</span><span id="d86f" class="nu mt iq nq b gy nz nw l nx ny">* * *</span><span id="afa8" class="nu mt iq nq b gy nz nw l nx ny">Title: Chicken Nuggets<br/>Storyline: Lily, chicken, fry, cool, delicious</span><span id="92a4" class="nu mt iq nq b gy nz nw l nx ny"><strong class="nq ja">Lily</strong> was cooking dinner.<br/>She decided to make <strong class="nq ja">chicken</strong> nuggets.<br/>She <strong class="nq ja">fried</strong> them in the oven.<br/>When she <strong class="nq ja">cooled</strong> it, it was a good idea.<br/>It was <strong class="nq ja">delicious</strong>!</span></pre><p id="fc8a" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">你怎么想呢?</p><h1 id="c502" class="ms mt iq bd mu mv mw mx my mz na nb nc kf nd kg ne ki nf kj ng kl nh km ni nj bi translated">额外资源</h1><p id="7a37" class="pw-post-body-paragraph lf lg iq lh b li nk ka lk ll nl kd ln lo nm lq lr ls nn lu lv lw no ly lz ma ij bi translated">我希望你喜欢这篇文章。以下是我发现对这个项目有用的一些文档、博客文章或论文。如果你也在做一些故事生成项目，请留下评论，分享你的作品。或者如果你对使用文本生成进行语言学习有任何想法，也请分享你的想法。😀</p><h2 id="961e" class="nu mt iq bd mu pd pe dn my pf pg dp nc lo ph pi ne ls pj pk ng lw pl pm ni iw bi translated">在谷歌人工智能平台上</h2><ol class=""><li id="d1ed" class="mc md iq lh b li nk ll nl lo pt ls pu lw pv ma oc mi mj mk bi translated"><a class="ae le" href="https://cloud.google.com/compute/gpus-pricing" rel="noopener ugc nofollow" target="_blank">GPU定价</a></li><li id="8be3" class="mc md iq lh b li ml ll mm lo mn ls mo lw mp ma oc mi mj mk bi translated"><a class="ae le" href="https://cloud.google.com/compute/docs/gpus/gpu-regions-zones" rel="noopener ugc nofollow" target="_blank"> GPU区域和分区可用性</a></li><li id="ffb9" class="mc md iq lh b li ml ll mm lo mn ls mo lw mp ma oc mi mj mk bi translated"><a class="ae le" href="https://cloud.google.com/ai-platform/training/docs/using-containers" rel="noopener ugc nofollow" target="_blank">在AI平台上使用容器培训</a></li><li id="6970" class="mc md iq lh b li ml ll mm lo mn ls mo lw mp ma oc mi mj mk bi translated">【PyTorch入门</li><li id="5f4f" class="mc md iq lh b li ml ll mm lo mn ls mo lw mp ma oc mi mj mk bi translated"><a class="ae le" href="https://cloud.google.com/ai-platform/training/docs" rel="noopener ugc nofollow" target="_blank">培训概述</a></li><li id="5a9a" class="mc md iq lh b li ml ll mm lo mn ls mo lw mp ma oc mi mj mk bi translated"><a class="ae le" href="https://medium.com/analytics-vidhya/how-to-access-files-from-google-cloud-storage-in-colab-notebooks-8edaf9e6c020" rel="noopener">如何在Colab笔记本上访问谷歌云存储中的文件</a></li><li id="61d4" class="mc md iq lh b li ml ll mm lo mn ls mo lw mp ma oc mi mj mk bi translated"><a class="ae le" href="https://medium.com/nordcloud-engineering/training-pytorch-transformers-on-gcp-ai-platform-e10ecb8a9e11" rel="noopener">在GCP AI平台上训练PyTorch变形金刚</a></li></ol><h2 id="6660" class="nu mt iq bd mu pd pe dn my pf pg dp nc lo ph pi ne ls pj pk ng lw pl pm ni iw bi translated">论故事与开放式文本生成</h2><ol class=""><li id="a64d" class="mc md iq lh b li nk ll nl lo pt ls pu lw pv ma oc mi mj mk bi translated"><a class="ae le" href="https://mark-riedl.medium.com/an-introduction-to-ai-story-generation-7f99a450f615" rel="noopener">人工智能故事生成简介</a></li><li id="f933" class="mc md iq lh b li ml ll mm lo mn ls mo lw mp ma oc mi mj mk bi translated"><a class="ae le" href="https://arxiv.org/abs/1811.05701v1" rel="noopener ugc nofollow" target="_blank">计划和写作:实现更好的自动讲故事</a></li><li id="4101" class="mc md iq lh b li ml ll mm lo mn ls mo lw mp ma oc mi mj mk bi translated"><a class="ae le" href="https://lilianweng.github.io/lil-log/2021/01/02/controllable-neural-text-generation.html" rel="noopener ugc nofollow" target="_blank">可控神经文本生成</a></li><li id="6577" class="mc md iq lh b li ml ll mm lo mn ls mo lw mp ma oc mi mj mk bi translated"><a class="ae le" href="https://huggingface.co/blog/how-to-generate" rel="noopener ugc nofollow" target="_blank">如何生成文本:使用不同的解码方法通过变形金刚生成语言</a></li><li id="ea7b" class="mc md iq lh b li ml ll mm lo mn ls mo lw mp ma oc mi mj mk bi translated"><a class="ae le" href="https://arxiv.org/abs/2006.14799" rel="noopener ugc nofollow" target="_blank">文本生成评估:一项调查</a></li><li id="bc11" class="mc md iq lh b li ml ll mm lo mn ls mo lw mp ma oc mi mj mk bi translated"><a class="ae le" href="https://arxiv.org/abs/2107.00061" rel="noopener ugc nofollow" target="_blank">人不一定是金子:评估人对生成文本的评估</a></li></ol><h2 id="bf77" class="nu mt iq bd mu pd pe dn my pf pg dp nc lo ph pi ne ls pj pk ng lw pl pm ni iw bi translated">在数据集上</h2><ol class=""><li id="5b55" class="mc md iq lh b li nk ll nl lo pt ls pu lw pv ma oc mi mj mk bi translated"><a class="ae le" href="https://cs.rochester.edu/nlp/rocstories/" rel="noopener ugc nofollow" target="_blank"> Story完形填空和ROCStories语料库</a></li><li id="ff8a" class="mc md iq lh b li ml ll mm lo mn ls mo lw mp ma oc mi mj mk bi translated"><a class="ae le" rel="noopener" target="_blank" href="/dirty-secrets-of-bookcorpus-a-key-dataset-in-machine-learning-6ee2927e8650">机器学习关键数据集BookCorpus的肮脏秘密</a></li></ol><h2 id="bc6a" class="nu mt iq bd mu pd pe dn my pf pg dp nc lo ph pi ne ls pj pk ng lw pl pm ni iw bi translated">深度学习语言模型研究</h2><ol class=""><li id="1c72" class="mc md iq lh b li nk ll nl lo pt ls pu lw pv ma oc mi mj mk bi translated"><a class="ae le" href="https://ai.googleblog.com/2020/02/exploring-transfer-learning-with-t5.html" rel="noopener ugc nofollow" target="_blank">用统一的文本到文本转换器探索迁移学习的极限</a></li><li id="9b2b" class="mc md iq lh b li ml ll mm lo mn ls mo lw mp ma oc mi mj mk bi translated"><a class="ae le" href="http://jalammar.github.io/illustrated-transformer/" rel="noopener ugc nofollow" target="_blank">图示变压器</a></li><li id="9654" class="mc md iq lh b li ml ll mm lo mn ls mo lw mp ma oc mi mj mk bi translated"><a class="ae le" href="http://jalammar.github.io/illustrated-gpt2/" rel="noopener ugc nofollow" target="_blank">图文并茂的GPT-2(可视化变压器语言模型)</a></li></ol><h2 id="e4f2" class="nu mt iq bd mu pd pe dn my pf pg dp nc lo ph pi ne ls pj pk ng lw pl pm ni iw bi translated">关键词抽取研究</h2><ol class=""><li id="0d00" class="mc md iq lh b li nk ll nl lo pt ls pu lw pv ma oc mi mj mk bi translated"><a class="ae le" href="https://github.com/MaartenGr/KeyBERT" rel="noopener ugc nofollow" target="_blank">使用BERT进行最小关键词提取</a></li><li id="fbea" class="mc md iq lh b li ml ll mm lo mn ls mo lw mp ma oc mi mj mk bi translated"><a class="ae le" href="https://medium.com/mlearning-ai/10-popular-keyword-extraction-algorithms-in-natural-language-processing-8975ada5750c" rel="noopener">自然语言处理中10种流行的关键词提取算法</a></li></ol></div></div>    
</body>
</html>