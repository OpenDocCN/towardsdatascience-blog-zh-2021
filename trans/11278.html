<html>
<head>
<title>Hugging Face Transformer Inference Under 1 Millisecond Latency</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">1毫秒延迟下的拥抱面部变形推断</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/hugging-face-transformer-inference-under-1-millisecond-latency-e1be0057a51c?source=collection_archive---------0-----------------------#2021-11-05">https://towardsdatascience.com/hugging-face-transformer-inference-under-1-millisecond-latency-e1be0057a51c?source=collection_archive---------0-----------------------#2021-11-05</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="aaa5" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsdatascience.com/tagged/notes-from-industry" rel="noopener" target="_blank">行业笔记</a></h2><div class=""/><div class=""><h2 id="25ee" class="pw-subtitle-paragraph jw iz iq bd b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn dk translated">使用微软和Nvidia开源工具投入生产</h2></div><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ko"><img src="../Images/2eca330d9ca045d9de056ce73df224ea.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yL0PlovtwOvYmUXrgI3pZg.jpeg"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">“哦，我的皮毛和胡须！我迟到了，我迟到了，我迟到了！”(来自<a class="ae le" href="https://commons.wikimedia.org/wiki/File:The_White_Rabbit_(Tenniel)_-_The_Nursery_Alice_(1890)_-_BL.jpg" rel="noopener ugc nofollow" target="_blank">https://Commons . wikimedia . org/wiki/File:The _ White _ Rabbit _(Tenniel)_-_ The _ Nursery _ Alice _(1890)_-_ bl . jpg</a>、<a class="ae le" href="https://en.wikipedia.org/wiki/en:Creative_Commons" rel="noopener ugc nofollow" target="_blank"> Creative Commons </a> CC0 1.0通用公共领域专用)</p></figure><p id="2380" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">最近，🤗拥抱脸(<a class="ae le" href="https://github.com/huggingface/transformers" rel="noopener ugc nofollow" target="_blank">变形金刚</a>库背后的初创公司)发布了一款名为“Infinity”的新产品。<a class="ae le" href="https://huggingface.co/infinity" rel="noopener ugc nofollow" target="_blank">将</a>描述为在“企业规模”执行推理的服务器。YouTube上有一个<a class="ae le" href="https://www.youtube.com/watch?v=jiftCAhOYQA" rel="noopener ugc nofollow" target="_blank">公开演示</a>(下面是演示中使用的时间和配置截图)。交流围绕着产品可以在GPU上以1毫秒的延迟执行变压器推理的承诺。根据演示者的说法，拥抱Face Infinity服务器的成本至少为💰部署在单台机器上的单一型号每年20，0 00美元(没有公开的价格可扩展性信息)。</p><p id="04a3" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">这让我很好奇，想挖掘一下并检查一下<strong class="lh ja">使用微软和Nvidia的开源工具，是否有可能使用演示</strong>中使用的相同AWS VM/model/input达到那些性能(详情见下面的截图)？<strong class="lh ja">剧透:是的，有了这个教程，很容易重现和适应你的现实生活项目</strong>。</p><blockquote class="mb mc md"><p id="04be" class="lf lg me lh b li lj ka lk ll lm kd ln mf lp lq lr mg lt lu lv mh lx ly lz ma ij bi translated"><em class="iq">💻</em>项目源代码可从以下地址获得:</p><p id="4ddd" class="lf lg me lh b li lj ka lk ll lm kd ln mf lp lq lr mg lt lu lv mh lx ly lz ma ij bi translated"><a class="ae le" href="https://github.com/ELS-RD/transformer-deploy" rel="noopener ugc nofollow" target="_blank">https://github.com/ELS-RD/transformer-deploy</a></p><p id="c44f" class="lf lg me lh b li lj ka lk ll lm kd ln mf lp lq lr mg lt lu lv mh lx ly lz ma ij bi translated">自述文件提供了如何运行代码的说明，并在带有深度学习图像版本44的AWS VM和带有Nvidia 3090 GPU的裸机服务器上进行了测试(文中发布的测量值来自AWS机器)。</p><p id="1ec2" class="lf lg me lh b li lj ka lk ll lm kd ln mf lp lq lr mg lt lu lv mh lx ly lz ma ij bi translated">如果你对这个话题感兴趣，在推特上关注我:<a class="ae le" href="https://twitter.com/pommedeterre33" rel="noopener ugc nofollow" target="_blank">https://twitter.com/pommedeterre33</a></p></blockquote><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi mi"><img src="../Images/4522958a36f66efcf1bb3630afc0f455.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*pvKqQdlwX7PboPMt"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated"><em class="mj">在2种输入尺寸的公开演示期间，执行了相当稳定的措施(来自https://www.youtube.com/watch?v=jiftCAhOYQA</em><a class="ae le" href="https://www.youtube.com/watch?v=jiftCAhOYQA" rel="noopener ugc nofollow" target="_blank"/>，作者截图)</p></figure><p id="650c" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">我在<a class="ae le" href="https://www.lefebvre-sarrut.eu/" rel="noopener ugc nofollow" target="_blank"> Lefebvre Sarrut </a> R &amp; D工作，这是一家领先的欧洲法律出版商，我的团队在生产中部署了相当多的模型，包括几个变形金刚，从小型蒸馏模型到大型模型，以执行法律文件的各种任务。这些作品中的一些已经在这里<a class="ae le" rel="noopener" target="_blank" href="/why-we-switched-from-spacy-to-flair-to-anonymize-french-legal-cases-e7588566825f">和那里</a>被描述过<a class="ae le" rel="noopener" target="_blank" href="/benchmark-ner-algorithm-d4ab01b2d4c3">和</a>。</p><p id="5791" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">在本文中，我们将了解如何在工业环境中部署现代NLP模型。关于这个主题有几十个教程，但是，据我所知，它们不是针对生产的，也没有涵盖性能、可伸缩性、CPU和GPU任务的解耦或GPU监控。其中一些看起来像这样:1/采用FastAPI HTTP server，2/添加Pytorch，和voilà🤪</p><blockquote class="mb mc md"><p id="69d8" class="lf lg me lh b li lj ka lk ll lm kd ln mf lp lq lr mg lt lu lv mh lx ly lz ma ij bi translated">如果你有<em class="iq">有趣的</em>内容想让我链接到，请发表评论…</p></blockquote><p id="1dee" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">本教程的目的是解释如何从Hugging Face大量优化一个转换器，并将其端到端地部署在一个<strong class="lh ja">生产就绪的</strong>推理服务器上。你可以从Nvidia和微软那里找到一些有趣的技术内容，关于这个过程的一些具体部分。</p><p id="f0cc" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">最接近的匹配和对我的一个启发是<a class="ae le" href="https://medium.com/nvidia-ai/how-to-deploy-almost-any-hugging-face-model-on-nvidia-triton-inference-server-with-an-8ee7ec0e6fc4" rel="noopener">这篇文章</a>。它仍然错过了两个关键点，推理服务器端的重要优化和标记化(否则你不能<em class="me">轻松地</em>在Python之外调用推理服务器)。我们将讨论这两点。</p><p id="8e27" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">这个过程带来的性能改进适用于所有场景，从短序列到长序列，从大小为1的一批到大批量。当架构符合工具的期望时，与普通PyTorch相比，该过程总是带来显著的性能提升。</p><p id="74f1" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">该过程分为<strong class="lh ja"> 3个步骤:</strong></p><ul class=""><li id="8f1a" class="mk ml iq lh b li lj ll lm lo mm ls mn lw mo ma mp mq mr ms bi translated">将 Pytorch模型转换成图形</li><li id="f764" class="mk ml iq lh b li mt ll mu lo mv ls mw lw mx ma mp mq mr ms bi translated">优化图表</li><li id="9d01" class="mk ml iq lh b li mt ll mu lo mv ls mw lw mx ma mp mq mr ms bi translated">在一个高性能的推理服务器上部署图</li></ul><p id="a31f" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">最后，我们将我们的推理服务器的性能与演示期间humping Face显示的数字进行比较，并将看到<strong class="lh ja">对于批量大小为1 </strong>的16和128令牌输入序列，我们都更快(据我所知，humping Face尚未公开分享其他场景的信息)。</p><p id="e666" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">更重要的是，更多的机器学习实践者将能够做一些比在非推理专用HTTP服务器上部署开箱即用Pytorch模型更可靠的事情。</p><h1 id="aded" class="my mz iq bd na nb nc nd ne nf ng nh ni kf nj kg nk ki nl kj nm kl nn km no np bi translated">从Pytorch到ONNX图</h1><p id="e8de" class="pw-post-body-paragraph lf lg iq lh b li nq ka lk ll nr kd ln lo ns lq lr ls nt lu lv lw nu ly lz ma ij bi translated">你大概知道吧，Pytorch相比Tensorflow 1的一大卖点。x的易用性:你只需编写熟悉的命令式代码，而不是构建一个图表。感觉你在写以GPU速度运行的numpy代码。</p><p id="7f4f" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">让用户开心很棒，更棒的是让优化工具也开心。与人类不同，这些工具喜欢用图表来执行(有时是离线)分析。这很有意义，图形提供了从数据点到模型输出的整个过程的静态完整视图。此外，该图提供了若干机器学习框架可能共有的中间表示。</p><p id="c3f3" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">我们需要一种方法将我们的命令式Pytorch代码转换成图形。有几种选择，但我们感兴趣的是ONNX。<em class="me">“ONNX是为表示机器学习模型而构建的开放格式。ONNX定义了一组通用的运算符——机器学习和深度学习模型的构建模块——和一种通用的文件格式，使人工智能开发人员能够将模型与各种框架、工具、运行时和编译器一起使用。”</em>(【https://onnx.ai/】T4)。这种格式最初是由脸书和微软创建的，目的是在Pytorch(研究)和Caffee2(生产)之间架起一座桥梁。</p><p id="12f3" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">Pytorch包括一个导出到ONNX的工具。导出工具背后的原理非常简单，我们将使用“跟踪”模式:我们向模型发送一些(虚拟)数据，工具将在模型内部跟踪它们，这样它将猜测图形看起来像什么。</p><p id="c843" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">跟踪模式并不神奇，例如它看不到你在numpy中做的操作(如果有的话)，图形将是静态的，一些if/else代码将永远固定，for循环将展开，等等。这没什么大不了的，因为拥抱脸和模型作者注意到主要/大多数模型是跟踪模式兼容的。</p><p id="da4e" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">供您参考，还有另一种称为“脚本”的导出模式，它需要以某种方式编写模型才能工作，其主要优势是动态逻辑保持不变<a class="ae le" href="https://github.com/huggingface/transformers/pull/6846#issuecomment-690363367" rel="noopener ugc nofollow" target="_blank">，但它会在模型编写方式中添加太多约束，而没有明显的性能增益</a>。</p><p id="4767" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">以下注释代码执行ONNX转换:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi nv"><img src="../Images/026cb07b736aba29492fe80cc13836c7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*m5xYfUZltPShozz5NWss7g.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">Pytorch到ONNX的转换代码(图片由作者提供)</p></figure><p id="6ce0" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">特别的一点是，我们将某个轴声明为动态的。如果我们不这样做，图形将只接受与我们用来构建它的张量形状完全相同的张量(虚拟数据)，因此序列长度或批量大小将是固定的。我们赋予输入和输出字段的名称将在其他工具中重用。</p><blockquote class="mb mc md"><p id="0289" class="lf lg me lh b li lj ka lk ll lm kd ln mf lp lq lr mg lt lu lv mh lx ly lz ma ij bi translated">请注意，ONNX export也可以像在句子变形库中一样进行特征提取，但在这种情况下，它需要一些小技巧。</p></blockquote><p id="6f1c" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">🥳<em class="me">félications</em>，你知道如何准备一个图形进行优化！</p><h1 id="2788" class="my mz iq bd na nb nc nd ne nf ng nh ni kf nj kg nk ki nl kj nm kl nn km no np bi translated">图形优化:2个工具，1个任务</h1><p id="c363" class="pw-post-body-paragraph lf lg iq lh b li nq ka lk ll nr kd ln lo ns lq lr ls nt lu lv lw nu ly lz ma ij bi translated">我们将重点介绍2款优化Pytorch模型的工具:来自微软的<a class="ae le" href="https://onnxruntime.ai/" rel="noopener ugc nofollow" target="_blank"> ONNX Runtime </a>(麻省理工学院许可下的开源)和来自英伟达的<a class="ae le" href="https://developer.nvidia.com/tensorrt" rel="noopener ugc nofollow" target="_blank">TensorRT</a>(Apache 2许可下的开源，优化引擎为闭源)。</p><p id="7249" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">他们可以单独或一起工作。在我们的例子中，我们将一起使用它们，这意味着通过ONNX运行时API使用TensorRT。</p><blockquote class="mb mc md"><p id="0b8c" class="lf lg me lh b li lj ka lk ll lm kd ln mf lp lq lr mg lt lu lv mh lx ly lz ma ij bi translated">&gt; #protip:如果你想听起来像个MLOps，就不要说ONNX Runtime / TensorRT，而要说ORT和TRT。另外，你可能会在Github issues/PR中找到这些名字。</p></blockquote><p id="9eda" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">这两个工具执行相同类型的操作来优化ONNX模型:</p><ul class=""><li id="824c" class="mk ml iq lh b li lj ll lm lo mm ls mn lw mo ma mp mq mr ms bi translated"><strong class="lh ja">找出并移除多余的操作</strong>:例如，在训练循环之外，dropout没有任何用处，它可以被移除，而不会对推理产生任何影响；</li><li id="78c7" class="mk ml iq lh b li mt ll mu lo mv ls mw lw mx ma mp mq mr ms bi translated"><strong class="lh ja">执行常数折叠</strong>:意思是找到由常数表达式组成的图的某些部分，在编译时而不是运行时计算结果(类似于大多数编程语言编译器)；</li><li id="fc58" class="mk ml iq lh b li mt ll mu lo mv ls mw lw mx ma mp mq mr ms bi translated"><strong class="lh ja">将一些操作合并在一起</strong>:避免1/加载时间，2/共享内存避免与全局内存来回传递。显然，它将主要有利于内存绑定操作(像乘法和加法操作，深度学习中非常常见的模式)，它被称为“内核融合”；</li></ul><p id="9b36" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">他们可以有选择地将模型权重一次性转换为较轻的表示(从32位浮点到16位浮点，甚至在量化的情况下转换为8位整数)。</p><p id="9ce6" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated"><a class="ae le" href="https://github.com/lutzroeder/netron" rel="noopener ugc nofollow" target="_blank"> Netron </a>可以产生优化前后ONNX图的显示:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi nw"><img src="../Images/1ebda7c8781e71c2071d6e5a146ed571.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*pt_-x0aWA5uXqUif"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">来自<a class="ae le" href="https://wiki.lfaidata.foundation/download/attachments/22249690/2.3%202020_Apr_ONNX%20MEETUP_ORT%20breakthrough%20optimizations%20for%20transformer%20inference%20on%20GPU%20and%20CPU.pdf?version=1&amp;modificationDate=1586564917000&amp;api=v2" rel="noopener ugc nofollow" target="_blank"> ONNX运行时——在GPU和CPU上实现变压器推理的突破性优化</a></p></figure><p id="b07a" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">这两种工具有一些基本的区别，主要是:</p><ul class=""><li id="e131" class="mk ml iq lh b li lj ll lm lo mm ls mn lw mo ma mp mq mr ms bi translated"><strong class="lh ja">易用性</strong> : TensorRT是为高级用户打造的，实现细节没有被它主要面向C++的API所隐藏(包括Python包装器，它的工作方式与C++ API完全一样，如果你不是C++开发人员，可能会感到惊讶)。另一方面，ONNX运行时文档很容易理解，即使你不是机器学习硬件专家，它提供了Pythonic API和许多这种语言的示例，你会发现更多NLP专用的示例和工具。</li><li id="0fcf" class="mk ml iq lh b li mt ll mu lo mv ls mw lw mx ma mp mq mr ms bi translated"><strong class="lh ja">优化范围</strong>:tensort通常提供最佳性能。这个过程有点复杂，我不会在这里提供细节，但是基本上你可以为不同的硬件、模型和数据形状建立“配置文件”。TensorRT将在您的硬件上执行一些基准测试，以找到最佳的优化组合(因此一个模型与一个特定的硬件相关联)。有时这有点太激进，特别是在混合精度中，你的变压器模型的精度可能会下降。最后，让我们补充一点，这个过程是不确定的，因为它取决于内核执行时间。简而言之，我们可以说TensorRT带来的额外性能是有学习曲线成本的。ONNX运行时有两种优化，一种称为“在线”优化，在模型加载后自动应用(只需要使用一个标志)，另一种称为“离线”优化，特定于某些模型，尤其是基于transformer的模型。我们将在本文中使用它们。根据模型、数据和硬件的不同，ONNX运行时+离线优化有时与TensorRT不相上下，其他时候我见过TensorRT在真实场景中快33%。TensorRT API比ONNX运行时提供的更完整，例如，您可以判断哪个张量形状是最佳的，并固定一些尺寸限制，因此它将生成所有需要的轮廓。<strong class="lh ja">如果你真的需要最好的性能，你需要学习TensorRT API。</strong></li><li id="005d" class="mk ml iq lh b li mt ll mu lo mv ls mw lw mx ma mp mq mr ms bi translated"><strong class="lh ja">多后端</strong> : ONNX运行时有自己的CUDA和CPU推理引擎，但它也可以将推理委托给第三方后端……包括TensorRT、TVM或openVINO！在这种情况下，ONNX运行时是一个很好的、文档完善的API，可以利用一个更复杂的工具。你猜怎么着我们将在下面进行测试！</li><li id="7383" class="mk ml iq lh b li mt ll mu lo mv ls mw lw mx ma mp mq mr ms bi translated"><strong class="lh ja">多个硬件目标</strong> : TensorRT专用于Nvidia硬件(很多GPU和Jetson)，ONNX运行时目标GPU (Nvidia CUDA和AMD RocM)，CPU，包括浏览器部署在内的边缘计算等。</li></ul><blockquote class="nx"><p id="7496" class="ny nz iq bd oa ob oc od oe of og ma dk translated">如果你没有得到它，ONNX运行时是你足够好的API来完成大多数推理工作。</p></blockquote><p id="960f" class="pw-post-body-paragraph lf lg iq lh b li oh ka lk ll oi kd ln lo oj lq lr ls ok lu lv lw ol ly lz ma ij bi translated">关于所支持的转换器架构，您可以通过查看本页了解ONNX运行时的基本功能。包括伯特、罗伯塔、GPT-2、XLM、layoutlm、巴特、T5等。关于TensorRT，我已经尝试了许多架构，没有任何问题，但据我所知，没有测试模型的列表。至少你可以在那里找到T5和GPT-2笔记本<a class="ae le" href="https://github.com/NVIDIA/TensorRT/tree/main/demo/HuggingFace" rel="noopener ugc nofollow" target="_blank"/>，与vanilla Pytorch相比，它的推理速度快了5倍。</p><p id="59e0" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">根据这个<a class="ae le" href="https://github.com/NVIDIA/TensorRT/tree/main/demo/HuggingFace/NNDF" rel="noopener ugc nofollow" target="_blank">自述</a>，Nvidia正在努力减轻变形金刚在其框架上的加速，这对我们所有人来说都是一个好消息！</p><h1 id="2626" class="my mz iq bd na nb nc nd ne nf ng nh ni kf nj kg nk ki nl kj nm kl nn km no np bi translated">离线优化</h1><p id="5810" class="pw-post-body-paragraph lf lg iq lh b li nq ka lk ll nr kd ln lo ns lq lr ls nt lu lv lw nu ly lz ma ij bi translated">如前所述，一些优化是在将模型加载到内存中之后应用的。也有可能在执行图形的静态分析时应用一些更深层次的优化，以便更容易管理动态轴或删除一些不必要的转换节点。此外，更改模型精度(从FP32到FP16)需要离线。查看本<a class="ae le" href="https://github.com/microsoft/onnxruntime/blob/master/onnxruntime/python/tools/transformers/Dev_Guide.md" rel="noopener ugc nofollow" target="_blank">指南</a>了解更多关于这些优化的信息。</p><p id="69c9" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">ONNX运行时在其<a class="ae le" href="https://github.com/microsoft/onnxruntime/tree/master/onnxruntime/python/tools/transformers" rel="noopener ugc nofollow" target="_blank">工具文件夹</a>中提供了这样的东西。支持大多数经典变压器架构，包括miniLM。您可以通过命令行运行优化:</p><pre class="kp kq kr ks gt om on oo op aw oq bi"><span id="ae87" class="or mz iq on b gy os ot l ou ov"><em class="me">python -m onnxruntime.transformers.optimizer ...</em></span></pre><p id="670c" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">在我们的例子中，我们将在Python代码中执行它们，以便执行一个命令。在下面的代码中，我们启用了所有可能的优化，并执行了到浮点16精度的转换。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ow"><img src="../Images/03973fc11065db1b3bf05b832147cc5a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*o7zYsgeAOC_MsIaoseN6ow.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">ONNX运行时离线优化代码(图片由作者提供)</p></figure><p id="4a2b" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">性能改进的一部分来自于在CUDA级别执行的一些近似:在激活层(GELU)和注意屏蔽层。这些近似值可能对模型输出有很小的影响。根据我的经验，与在训练期间使用不同的种子相比，它对模型准确性的影响更小。</p><p id="01db" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">关于TensorRT，没有离线优化，但是在ONNX运行时文档中建议在普通ONNX模型上执行符号形状推理。这很有用，因为ONNX运行时可能会将图形分割成几个子图，因此tensort的(张量)形状信息会丢失。符号形状推理将把信息放回任何需要的地方。如果你和我一样，想知道为什么它被称为符号，那是因为它真的会用一个叫做<a class="ae le" href="https://www.sympy.org/en/index.html" rel="noopener ugc nofollow" target="_blank"> sympy </a>的Python库来执行符号计算，这个库致力于……一般的符号计算。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ox"><img src="../Images/5936544af036e5c99afb3834f4bd0192.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gk33-15W_rM896N4IJ7nuQ.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">ONNX形状推理代码(图片由作者提供)</p></figure><h1 id="1ace" class="my mz iq bd na nb nc nd ne nf ng nh ni kf nj kg nk ki nl kj nm kl nn km no np bi translated">关于GPU int-8量化的一句话</h1><p id="29ed" class="pw-post-body-paragraph lf lg iq lh b li nq ka lk ll nr kd ln lo ns lq lr ls nt lu lv lw nu ly lz ma ij bi translated">CPU量子化开箱即用，GPU则是另一个故事。你可能已经看到Nvidia的基准测试显示了与FP16精度相比int-8量化的惊人性能，并且可能想知道，为什么你找不到任何NLP教程来做同样的事情(在CV中有很多)。</p><p id="334e" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">原因有几个:</p><ul class=""><li id="2a12" class="mk ml iq lh b li lj ll lm lo mm ls mn lw mo ma mp mq mr ms bi translated">首先，自去年夏天发布的TensorRT 8以来，变形金刚量化已全面启用。</li><li id="7297" class="mk ml iq lh b li mt ll mu lo mv ls mw lw mx ma mp mq mr ms bi translated">第二，现有的工具(至少是我尝试过的那个)有问题，文档并不总是最新的，并且它不能很好地与Pytorch的上一个版本一起工作(我在导出包含QDQ节点的图形时遇到了一个问题)。不过，如果您深入研究Pytorch代码，您可以用一个脏补丁来解决这个错误。</li><li id="c07b" class="mk ml iq lh b li mt ll mu lo mv ls mw lw mx ma mp mq mr ms bi translated">第三，结果取决于硬件，这意味着对不同的形状/模型/硬件组合进行了大量的实验。</li><li id="c648" class="mk ml iq lh b li mt ll mu lo mv ls mw lw mx ma mp mq mr ms bi translated">第四，根据你的量化方法，它可以通过添加大量的“重新格式化”节点，使你的模型比FP16中的模型慢。</li></ul><p id="6d13" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">量化为各种转换器架构带来了最佳性能，因为除了减少计算，它还减少了许多权重的内存传输，这是任何内核融合都无法达到的。</p><p id="0201" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">另一件要记住的事情是，不是所有的模型都可以开箱即用int-8量化，有时你会得到一些“找不到节点的任何实现…”的错误消息，这意味着你可以重做模型，等待tensort的新版本，或者，如果你有很多空闲时间，像这里的<a class="ae le" href="https://paulbridger.com/posts/tensorrt-object-detection-quantized/" rel="noopener ugc nofollow" target="_blank">一样分叉tensort</a>。香草伯特效果很好。miniLM可以在一些工具上工作，但不是全部，不知道为什么。</p><p id="6999" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">不是所有的层都应该被量化。训练后量化将量化所有图层，并注重性能(但精度可能会下降)，这取决于用户选择他想要排除的图层以保持高精度。查询感知训练只量化特定的层，例如在Bert情况下，你会发现注意力层，因此它通常是准确性和性能之间的权衡。</p><p id="5d38" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">最后，校准(将浮点数转换为整数和小数位数的必要操作)仍然是一个未解决的问题，有几个选项，您需要再次试验，以找到适合您的模型和任务的选项。</p><blockquote class="mb mc md"><p id="0bdc" class="lf lg me lh b li lj ka lk ll lm kd ln mf lp lq lr mg lt lu lv mh lx ly lz ma ij bi translated">关于量化的更多信息，你可以查看Nvidia的这篇非常好的论文:<a class="ae le" href="https://arxiv.org/abs/2004.09602" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/2004.09602</a></p></blockquote><p id="564e" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">当你在像微软、<a class="ae le" href="https://blogs.bing.com/Engineering-Blog/october-2021/Bing-delivers-more-contextualized-search-using-quantized-transformer-inference-on-NVIDIA-GPUs-in-Azu" rel="noopener ugc nofollow" target="_blank">这样的互联网规模工作时，投资这项技术</a>是有意义的。对于我们大多数人来说，直到软件部分改进后才明显。</p><h1 id="f922" class="my mz iq bd na nb nc nd ne nf ng nh ni kf nj kg nk ki nl kj nm kl nn km no np bi translated">⌛Inference基准(本地执行)</h1><p id="81a0" class="pw-post-body-paragraph lf lg iq lh b li nq ka lk ll nr kd ln lo ns lq lr ls nt lu lv lw nu ly lz ma ij bi translated">好了，现在是时候基准测试了。为此，我们将使用一个简单的装饰函数来存储每个计时。代码的其余部分非常简单。<strong class="lh ja">脚本末尾的度量是用16个令牌输入来执行的(就像Infinity演示中的一个度量)。</strong></p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi oy"><img src="../Images/335588839d8626dc3d3b216493a4fc77.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ek9vp5fny3JbLt2IWElBTw.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">推理基准代码(图片由作者提供)</p></figure><p id="cde2" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">结果如下:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi mi"><img src="../Images/5c6b456a53b9f8986e483f8e9c72620d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*0JHSu186allsDu_u"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">针对16个令牌输入的每个ONNX运行时提供程序的度量(图片由作者提供)</p></figure><blockquote class="mb mc md"><p id="62f4" class="lf lg me lh b li lj ka lk ll lm kd ln mf lp lq lr mg lt lu lv mh lx ly lz ma ij bi translated">💨TensorRT(第1行)的<strong class="lh ja"> 0.64 ms </strong>，优化ONNX运行时的<strong class="lh ja"> 0.63 ms </strong>(第3行)，比vanilla Pytorch快了近10倍！<strong class="lh ja">我们远远低于1毫秒的限制。</strong></p></blockquote><p id="cdd1" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">我们得救了，这篇文章的标题很荣幸:-)</p><p id="5f2f" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">有趣的是，在Pytorch上，16位精度(5.9毫秒)比全精度(5毫秒)慢。这是由于我们的输入，没有批处理，序列非常短，并且在一天结束时，从FP32到FP16的转换增加了比它所暗示的计算简化更多的开销。</p><blockquote class="mb mc md"><p id="56b4" class="lf lg me lh b li lj ka lk ll lm kd ln mf lp lq lr mg lt lu lv mh lx ly lz ma ij bi translated">当然，我们已经检查过<strong class="lh ja">所有的模型输出都是相似的</strong>(如上所述，由于小的近似，它们不会相等，加上不同的后端在图中执行舍入略有不同)。</p></blockquote><p id="d9db" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">除了基准或极限用例之外，并不是每天都要在GPU上的一个非常小的模型上对单个16序列令牌执行推理，因为它没有利用GPU的主要优势。此外，即使您的查询以单个序列的形式出现，大多数严肃的推理服务器都有一个特性，可以成批地将单个的推理请求组合在一起。其目的是以增加的吞吐量换取几毫秒的延迟，这可能是您在尝试优化硬件总拥有成本时所寻求的。</p><p id="fb63" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated"><strong class="lh ja">仅供参考，与拥抱脸演示</strong>无关，请查看以下在同一虚拟机(T4 GPU)上针对<code class="fe oz pa pb on b">bert-base-uncased</code>、384个令牌的序列和32个大小的批次(这些是我们在Lefebvre Sarrut的用例中使用的常用参数)的测量结果:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi pc"><img src="../Images/6b99ddd487aac878ef8a43e2b96d49b1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eqkIeuQRIvmlNDs4aJAmFg.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">在bert-base上的性能-在大批量数据的情况下(图片由作者提供)</p></figure><p id="9089" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">可以看到，TensorRT和ONNX运行时带来的延迟降低非常显著，<strong class="lh ja"> ONNX运行时+TensorRT延迟(4.72 ms)比vanilla py torch fp32(25.9 ms)</strong>⚡️低5倍以上🏃🏻💨💨<strong class="lh ja">！</strong>对于TensorRT，在百分位数99时，我们仍低于5毫秒阈值。正如所料，Pytorch上的FP16大约比FP32快2倍，ONNX运行时(CUDA提供程序)的表现与TensorRT提供程序非常相似。</p><p id="7141" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">这些结果不会让我们感到惊讶，因为我们似乎使用了与拥抱脸相同的工具:</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="pd pe l"/></div></figure><h1 id="db43" class="my mz iq bd na nb nc nd ne nf ng nh ni kf nj kg nk ki nl kj nm kl nn km no np bi translated">🍎相对🍎:第一次尝试，ORT+FastAPI vs拥抱脸无限</h1><p id="230d" class="pw-post-body-paragraph lf lg iq lh b li nq ka lk ll nr kd ln lo ns lq lr ls nt lu lv lw nu ly lz ma ij bi translated">将前面部分的计时与拥抱脸演示的计时进行比较是不公平的:我们没有服务器通信，没有令牌化，没有任何开销，我们只是执行推理。</p><p id="81bb" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">所以，我们将再次这样做，但这一次使用一个简单的HTTP服务器:FastAPI(就像你可以在人工智能创业博客上找到的几十个营销内容一样)。</p><blockquote class="mb mc md"><p id="fff6" class="lf lg me lh b li lj ka lk ll lm kd ln mf lp lq lr mg lt lu lv mh lx ly lz ma ij bi translated">请注意，无论FastAPI的性能如何，它都不是继续生产的好的推理服务器选择，它缺少基本的东西，如GPU监控、高级ML性能工具等。</p></blockquote><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi nv"><img src="../Images/941bfa393033c000301aa394ee731d59.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FkkF9L_3Gr76cjR3TLdNQw.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">至少，它很容易写:-)(图片由作者提供)</p></figure><p id="2c26" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">时间看起来是这样的:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi mi"><img src="../Images/c553fcf22eced80c75ff305140b894e4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*3dXCHb3rakVgqAkx"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">简单基准ONNX运行时+ FastAPI基准(如果用请求库替换curl，则采用类似的方法)(图片由作者提供)</p></figure><p id="b316" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated"><strong class="lh ja">【讥讽开】</strong>whhhhhhhaaaaat？？？？在fastAPI内部执行推理比本地推理慢10倍？真令人吃惊，谁会料到呢？<strong class="lh ja">【讥讽关】</strong></p><p id="b268" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">如果我们检查一个<a class="ae le" href="https://www.techempower.com/benchmarks/?utm_source=pocket_mylist#section=data-r20&amp;hw=ph&amp;test=db" rel="noopener ugc nofollow" target="_blank">著名的web框架基准</a>，我们可以看到，与其他语言的其他选项相比，FastAPI并没有那么快。在单个查询延迟方面，它甚至是最慢的(比fasthttp Go服务器慢38倍以上)。这并不意味着它是一个糟糕的软件，当然，凭借其自动打字等功能，它确实是一个很好的工具。，但这不是我们这里需要的。</p><blockquote class="mb mc md"><p id="4608" class="lf lg me lh b li lj ka lk ll lm kd ln mf lp lq lr mg lt lu lv mh lx ly lz ma ij bi translated">拥抱脸仅在批量大小为1的非常短(16)和短(128)的序列上通信。该模型可以用现成的工具进行优化，但是如果我们停留在Python世界中，端到端的性能是无法达到的。<strong class="lh ja">在其他一些场景中(大批量、长序列)，几毫秒的开销差异可能是微不足道的。</strong></p></blockquote><p id="f10e" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">那么，如果不是fastAPI，我们用什么呢？</p><h1 id="a775" class="my mz iq bd na nb nc nd ne nf ng nh ni kf nj kg nk ki nl kj nm kl nn km no np bi translated">🍎相对🍎:第二次尝试，Nvidia Triton vs拥抱脸无限</h1><p id="2572" class="pw-post-body-paragraph lf lg iq lh b li nq ka lk ll nr kd ln lo ns lq lr ls nt lu lv lw nu ly lz ma ij bi translated">Nvidia发布了一款漂亮的推理服务器，名为<a class="ae le" href="https://github.com/triton-inference-server/server" rel="noopener ugc nofollow" target="_blank"> Triton </a>(之前被称为TensorRT，非常混乱)。</p><p id="ff61" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">它提供了你可能需要的几乎所有东西，GPU监控，漂亮的文档，我见过的最好的错误消息(说真的，就像他们把某人放在里面告诉你要修复什么和如何修复)。它有很多非常强大的可能性(我们不会在这个已经太长的教程中详述)，并且对于简单的情况仍然易于使用。它提供了一个GRPC加一个HTTP API，一个相当好的Python客户端(请求库就像FastAPI，简单的API，接近完美的文档，但是性能一般)和一个好的C++客户端。它附带了一系列重要的工具来优化硬件的使用。为了更好地理解什么是一个好的推理机，查看一下<a class="ae le" href="https://arxiv.org/pdf/2011.02327.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/2011.02327.pdf</a></p><p id="19f1" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">出于我不明白的原因，它在NLP社区中不是一个非常知名的工具(在CV社区中情况稍微好一点)。</p><h2 id="133d" class="or mz iq bd na pf pg dn ne ph pi dp ni lo pj pk nk ls pl pm nm lw pn po no iw bi translated">1/在Triton推理服务器上设置ONNX运行时后端</h2><p id="4b90" class="pw-post-body-paragraph lf lg iq lh b li nq ka lk ll nr kd ln lo ns lq lr ls nt lu lv lw nu ly lz ma ij bi translated">推断海卫一很简单。基本上，你需要准备一个文件夹，里面有我们生成的ONNX文件和一个配置文件，如下所示，给出了输入和输出张量的描述。然后你启动Triton Docker容器…就这样！</p><p id="fea3" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">这里是配置文件:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi pp"><img src="../Images/52e28fa55e92d1f1f8e7bccc1563a192.png" data-original-src="https://miro.medium.com/v2/resize:fit:778/format:webp/1*MYfhHNcqdtACN82nx7_dTg.png"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">它闻起来像json，但它不是(图片由作者提供)</p></figure><p id="d20f" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">2条评论:</p><ul class=""><li id="acf3" class="mk ml iq lh b li lj ll lm lo mm ls mn lw mo ma mp mq mr ms bi translated"><strong class="lh ja"> max_batch_size: 0 </strong>表示没有动态批处理(上面描述的用吞吐量交换延迟的高级特性)。</li><li id="cc43" class="mk ml iq lh b li mt ll mu lo mv ls mw lw mx ma mp mq mr ms bi translated"><strong class="lh ja"> -1 </strong>在形状上表示动态轴，也就是这个维度可能会从一个查询改变到另一个</li></ul><h2 id="24d1" class="or mz iq bd na pf pg dn ne ph pi dp ni lo pj pk nk ls pl pm nm lw pn po no iw bi translated">2/设置客户端</h2><p id="9373" class="pw-post-body-paragraph lf lg iq lh b li nq ka lk ll nr kd ln lo ns lq lr ls nt lu lv lw nu ly lz ma ij bi translated">在与本文相关的repo(开头的链接)中，有2个Python客户端脚本，一个基于<a class="ae le" href="https://github.com/triton-inference-server/client" rel="noopener ugc nofollow" target="_blank"> tritonclient </a>库(performant)，一个基于请求库(不是performant，但是如果需要在Python之外调用Triton，它可以作为草稿使用)和一个简单的curl调用(在存储库自述文件中)。</p><p id="df65" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">我们用于查询Triton推理服务器的基于tritonclient的脚本:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi pq"><img src="../Images/37393e4e7a5f9075187b528b1c60a6f5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QASTSiYTWOvtOOqpxhtUbA.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">不是最pythonic化的API🐍(图片由作者提供)</p></figure><p id="417f" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">客户端库中的微妙之处在于，首先声明输入和输出变量，然后将数据加载到它们中。</p><blockquote class="mb mc md"><p id="1b04" class="lf lg me lh b li lj ka lk ll lm kd ln mf lp lq lr mg lt lu lv mh lx ly lz ma ij bi translated"><em class="iq">注意，输入ids张量是</em> <strong class="lh ja"> <em class="iq">为每个请求随机生成</em> </strong> <em class="iq">，避免任何缓存效应(据我所知，默认没有缓存但总是好查的)。</em></p></blockquote><p id="6f71" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">16(第一次测量)和128(第二次测量)令牌输入长度的结果:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi pr"><img src="../Images/b6bf0778eb3635bf8af3a5a2e6460ca8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*ZAYz8yUqTKqOhHll"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">基准ONNX运行时+ Triton推理服务器(图片由作者提供)</p></figure><p id="f47c" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">太棒了，我们发现了一些东西:对于16和128令牌序列长度，我们仍然低于拥抱脸基线。在128个令牌情况下，裕量相当大。</p><p id="b79e" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">我们仍然在GPU上做纯模型计算，为了有一些我们可以与拥抱无限相比较的东西，我们仍然需要将令牌化部分移动到服务器。</p><h2 id="e727" class="or mz iq bd na pf pg dn ne ph pi dp ni lo pj pk nk ls pl pm nm lw pn po no iw bi translated">3/在服务器端添加令牌化</h2><p id="dc45" class="pw-post-body-paragraph lf lg iq lh b li nq ka lk ll nr kd ln lo ns lq lr ls nt lu lv lw nu ly lz ma ij bi translated"><strong class="lh ja">我跟你说过英伟达Triton服务器很牛逼吗？</strong>正是。它支持几个后端，包括一个叫做“Python”的后端。在Python后端，我们可以调用免费的Python代码，例如准备我们的数据(在我们的例子中是标记化)。</p><blockquote class="mb mc md"><p id="f0e8" class="lf lg me lh b li lj ka lk ll lm kd ln mf lp lq lr mg lt lu lv mh lx ly lz ma ij bi translated">在实现这一部分之前，您需要在Triton Docker容器中安装变压器。有关更多信息，请查看与本文相关的存储库的自述文件。</p></blockquote><p id="4af9" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">Python代码看起来像什么:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ps"><img src="../Images/76167ff1340c67c381547460a9e83f23.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rtO8biVst5yxobiHoTPz3w.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">非常简单的Python代码(图片由作者提供)</p></figure><p id="f095" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">基本上，有一种<code class="fe oz pa pb on b">__init__()</code>函数供我们下载标记化器，还有一种<code class="fe oz pa pb on b">execute</code>函数执行标记化本身。for循环是因为动态批处理功能。非常简短的代码。</p><p id="2b50" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">我们需要一个配置来告诉Triton输入和输出是什么。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi pt"><img src="../Images/a0e74fa9bbd36a634e833a4da05118cf.png" data-original-src="https://miro.medium.com/v2/resize:fit:798/format:webp/1*fZp7Q869NZiHLnBRz7OAGg.png"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">类似于之前的配置(图片由作者提供)</p></figure><p id="e349" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">现在我们想将标记器插入到模型中，我们需要第三个配置。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi pu"><img src="../Images/249f20aebd4d6b8eaaa7c984f190a548.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IyhGaRNn6mee1X1Lln8txQ.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">将标记器输出插入模型输入的配置文件(图片由作者提供)</p></figure><p id="3cab" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">第一部分声明整个过程的输入和输出，第二部分将所有内容连接在一起。基本上，它说接收一个字符串，把它发送给记号赋予器，从记号赋予器得到输出，并把它作为模型的输入传递，返回模型输出。</p><p id="9a28" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">如前所述，服务器文档写得很好。很容易使用错误的类型，错误的维度，或者在张量名称中插入一个错别字，Triton错误消息会告诉你到底要修复什么和在哪里。</p><h2 id="f3d2" class="or mz iq bd na pf pg dn ne ph pi dp ni lo pj pk nk ls pl pm nm lw pn po no iw bi translated">4/ 👀最终基准！</h2><p id="f05f" class="pw-post-body-paragraph lf lg iq lh b li nq ka lk ll nr kd ln lo ns lq lr ls nt lu lv lw nu ly lz ma ij bi translated">最后，这里是最终基准的时间。下面你可以看到我们最终的客户端脚本。与之前的客户端脚本的主要区别在于，我们现在的目标是整个流程，而不仅仅是模型。我们不发送整数张量，只发送字符串(存储在numpy数组中，这是与Triton通信的方式)。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi oy"><img src="../Images/baecabc79dbb3a7b4a62cd5ce1d3b9d3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qEKdfThA90s_rf0ziCheow.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">*本文中最重要的基准脚本。简单，就够了。tritonclient提供了更好的工具(不幸的是，这个工具需要编译才能在Ubuntu 18.06上工作。2021年不要像AWS一样，用最新发布的Ubuntu！)(图片由作者提供)</p></figure><p id="e69f" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">以及16(第一度量)和128(第二度量)令牌的度量:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi pv"><img src="../Images/72f4d37e98d081fada9b798419666646.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*4vC7ua8_Z5x6P2xi"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">最终的结果，目标达到了！(图片由作者提供)</p></figure><blockquote class="mb mc md"><p id="cce0" class="lf lg me lh b li lj ka lk ll lm kd ln mf lp lq lr mg lt lu lv mh lx ly lz ma ij bi translated">🎯我们成功了！<strong class="lh ja">1.5毫秒</strong>用于16个令牌(相对于1.7毫秒拥抱脸无限)，以及<strong class="lh ja">2毫秒</strong>用于128个令牌长度(相对于2.5毫秒拥抱脸无限)。</p></blockquote><p id="f6f2" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">我们已经构建了一个快速推理服务器，准备部署在我们的集群中。我们现在将欣赏GPU监控如何使自动扩展更容易设置，或者成熟的性能测量工具如何帮助我们修复管道中的瓶颈。<strong class="lh ja">对于机器学习从业者、初创公司和企业来说，这绝对是令人敬畏的。</strong></p><p id="6bde" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">那么现在，我们能对未来有什么期待呢？</p></div><div class="ab cl pw px hu py" role="separator"><span class="pz bw bk qa qb qc"/><span class="pz bw bk qa qb qc"/><span class="pz bw bk qa qb"/></div><div class="ij ik il im in"><h1 id="1bf4" class="my mz iq bd na nb qd nd ne nf qe nh ni kf qf kg nk ki qg kj nm kl qh km no np bi translated">结尾部分</h1><p id="17b0" class="pw-post-body-paragraph lf lg iq lh b li nq ka lk ll nr kd ln lo ns lq lr ls nt lu lv lw nu ly lz ma ij bi translated">与你将在一些媒体上读到的不同，许多人认为机器学习社区仍处于起步阶段。特别是，有一个话题一再出现，即大多数机器学习项目从未在生产中部署，它只是机器学习爱好者的营销内容和帖子，就像在<a class="ae le" rel="noopener" target="_blank" href="/why-90-percent-of-all-machine-learning-models-never-make-it-into-production-ce7e250d5a4a#:~:text=The%20data%20speaks%20for,something%20useful%20for%20the%20company">为什么90%的机器学习模型从未进入生产</a>或<a class="ae le" href="https://venturebeat.com/2019/07/19/why-do-87-of-data-science-projects-never-make-it-into-production/" rel="noopener ugc nofollow" target="_blank">为什么87%的数据科学项目从未进入生产？</a>或<a class="ae le" href="https://www.wallaroo.ai/blog-posts/production-ml" rel="noopener ugc nofollow" target="_blank">为什么ML车型很少投产，你能做些什么</a>等。</p><blockquote class="mb mc md"><p id="9abd" class="lf lg me lh b li lj ka lk ll lm kd ln mf lp lq lr mg lt lu lv mh lx ly lz ma ij bi translated">很明显，有人用GPT-9生成了这些内容。如果你知道哪里可以下载GPT-9的重量，请给我一个下午:-)</p></blockquote><p id="a728" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">我怀疑是否有人能真正衡量它，但可以肯定的是，关于NLP模型部署的严肃文章太少了。我希望你喜欢这个，并且我希望，如果你有时间，也许你可以在Medium或其他地方与社区分享你的优化/部署经验。在NLP中有如此多的其他重要事情需要解决，CPU/GPU部署在2021年应该不是一个挑战。</p><p id="e6df" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">作为一个社区，我们还需要关键人物的适当沟通，即使是在向企业销售产品时。例如，像<em class="me">“部署和加速20毫秒延迟下的BERT模型需要2个月x 3名高技能ML工程师”</em>这样的消息错过了关键的技术细节(模型、输入大小、硬件)，使得任何比较都不可能。此外，称这些工程师“技术高超”，但在几个月的工作后仍然无法实现他们的目标，这意味着恐惧、不确定性和对我们自己做同样事情的能力的怀疑。</p><p id="20f2" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">最后，我们需要更多针对开源工具中NLP模型的推理优化！</p><p id="d39a" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">在双LSTM/GRU &amp;朋友时代，在几年时间里，好像每个月都有一个大的架构变化。然后变形金刚来了，吃了大部分NLP任务。仍然有一些架构变化，拥抱脸变形金刚库在这里帮助机器学习从业者跟随炒作，而无需在代码重写方面进行大量投资。</p><blockquote class="nx"><p id="1d46" class="ny nz iq bd oa ob qi qj qk ql qm ma dk translated">我有一种感觉，模型架构现在已经稳定下来，社区中对错过最新流行架构的担心正在减少。</p></blockquote><p id="94e8" class="pw-post-body-paragraph lf lg iq lh b li oh ka lk ll oi kd ln lo oj lq lr ls ok lu lv lw ol ly lz ma ij bi translated">换句话说，如果您的工具箱中已经有一个Roberta、一个像miniLM这样的提炼模型，以及一个像T5或Bart这样的生成模型，那么您可能可以使用大多数工业NLP用例1或2年。</p><p id="f493" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">这对机器学习从业者来说是个好消息，因为这种稳定为微软、英特尔、英伟达和其他公司优化NLP模型的努力打开了大门。延迟时间的大幅减少或吞吐量的大幅增加不仅会转化为更低的成本推断，还会带来新的用途和新产品。我们还可能希望有一天，我们甚至可以使用那些据称能够做许多令人敬畏的事情的非常大的语言模型(那些有数千亿个参数的模型)。我个人相信这将会发生，因为开发NLP用途符合硬件制造商和云提供商的利益，并且他们有资源和知识来做这些事情。</p></div></div>    
</body>
</html>