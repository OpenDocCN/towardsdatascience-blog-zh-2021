<html>
<head>
<title>Integrating 🤗 Transformers with MedCAT for biomedical NER+L</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">(使)融入🤗带MedCAT的变压器，用于生物医学NER+L</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/integrating-transformers-with-medcat-for-biomedical-ner-l-8869c76762a?source=collection_archive---------26-----------------------#2021-07-08">https://towardsdatascience.com/integrating-transformers-with-medcat-for-biomedical-ner-l-8869c76762a?source=collection_archive---------26-----------------------#2021-07-08</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><figure class="ip iq gp gr ir is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi io"><img src="../Images/7a2582a634b4492303dc26acac99c6fd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RV1bBrTUlV2OZm-naYeRvw.jpeg"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">由<a class="ae jd" href="https://unsplash.com/@aboutiwe?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Irwan iwe </a>在<a class="ae jd" href="https://unsplash.com/s/photos/medical-record?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>拍摄的照片</p></figure><div class=""/><p id="e572" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">生物医学NER+L致力于从电子健康记录(EHRs)中的自由文本中提取概念，并将它们链接到大型生物医学数据库，如SNOMED-CT和UMLS。</p><p id="9aa0" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">医学概念注释工具包(MedCAT)使用基于Word2Vec的浅层神经网络来检测和消除生物医学概念的歧义。这种方法给了我们:1)无监督训练；2)检测数百万个独特概念的可能性；3)训练速度快，资源要求低；4)仅从正面例子中学习的能力；</p><p id="3a70" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">对于一些我们有足够训练样本的用例，基于变压器(如BERT)的监督学习方法可能更合适。</p><p id="3527" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在这项工作中，我们展示了如何整合🤗使用MedCAT的数据集/转换器，或者更准确地说，我们展示了如何将MedCATtrainer导出(手动注释的项目)转换为🤗数据集和训练a🤗变压器型号。</p><p id="a892" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">先决条件:</p><ul class=""><li id="139d" class="lb lc jg kf b kg kh kk kl ko ld ks le kw lf la lg lh li lj bi translated">熟悉<a class="ae jd" href="https://github.com/CogStack/MedCAT" rel="noopener ugc nofollow" target="_blank"> MedCAT </a> ( <a class="ae jd" rel="noopener" target="_blank" href="/medcat-introduction-analyzing-electronic-health-records-e1c420afa13a"> TDS教程</a>)、<a class="ae jd" href="https://github.com/CogStack/MedCATtrainer" rel="noopener ugc nofollow" target="_blank"> MedCATtrainer </a>、<a class="ae jd" href="https://huggingface.co/transformers/" rel="noopener ugc nofollow" target="_blank">拥抱脸变形金刚和数据集</a></li></ul><p id="01f3" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">随附的Jupyter笔记本可以在MedCAT <a class="ae jd" href="https://github.com/CogStack/MedCAT/blob/master/notebooks/BERT%20for%20NER.ipynb" rel="noopener ugc nofollow" target="_blank">资源库</a>中找到。</p><h1 id="b9c5" class="lk ll jg bd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh bi translated">数据准备</h1><p id="4bd4" class="pw-post-body-paragraph kd ke jg kf b kg mi ki kj kk mj km kn ko mk kq kr ks ml ku kv kw mm ky kz la ij bi translated"><a class="ae jd" href="https://github.com/CogStack/MedCATtrainer/" rel="noopener ugc nofollow" target="_blank"> MedCATtrainer </a>用于手动注释任何生物医学概念的自由文本文档(例如<a class="ae jd" href="https://www.snomed.org/" rel="noopener ugc nofollow" target="_blank"> SNOMED </a>或<a class="ae jd" href="https://www.nlm.nih.gov/research/umls/index.html" rel="noopener ugc nofollow" target="_blank"> UMLS </a>)。注释过程完成后，项目可以以如下所示的<code class="fe mn mo mp mq b">.json</code>格式导出(有限视图):</p><pre class="mr ms mt mu gt mv mq mw mx aw my bi"><span id="6e0e" class="mz ll jg mq b gy na nb l nc nd">{'projects': [{'name': '&lt;something&gt;', <br/>               'documents': [{'text': '&lt;some text&gt;', <br/>                              'annotations': [{'start': 23,<br/>                                               'end': 32,<br/>                                               'cui': &lt;label&gt;},<br/>                                              ...]}]}]}</span></pre><p id="0b58" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">简化对一个🤗Transformer模型，我们将JSON输出转换成🤗数据集。我们只保留JSON中的重要特性，而丢弃其他特性:</p><pre class="mr ms mt mu gt mv mq mw mx aw my bi"><span id="a016" class="mz ll jg mq b gy na nb l nc nd">features=datasets.Features(<br/>{<br/>"id": datasets.Value("int32"),<br/>"text": datasets.Value("string"),<br/>"ent_starts": datasets.Sequence(datasets.Value("int32")),<br/>"ent_ends": datasets.Sequence(datasets.Value("int32")),<br/>"ent_cuis": datasets.Sequence(datasets.Value("string")),<br/>}),</span></pre><p id="2c98" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这里，<code class="fe mn mo mp mq b">id</code>是文档的ID，<code class="fe mn mo mp mq b">text</code>是文档的文本，<code class="fe mn mo mp mq b">ent_starts</code>是文档中所有被手工注释的实体的起始字符的位置列表，<code class="fe mn mo mp mq b">ent_ends</code>是结束字符的位置，<code class="fe mn mo mp mq b">ent_cuis</code>是标签。请注意，MedCATtrainer使用在线学习，虽然用户能够创建新实体，但大多数实体都由MedCAT预先注释，并由用户简单验证(正确/不正确)。因此，当在dataset类中生成示例时，我们只保留<code class="fe mn mo mp mq b">correct</code>示例和那些由用户添加的<code class="fe mn mo mp mq b">manually_created</code>示例，换句话说:</p><pre class="mr ms mt mu gt mv mq mw mx aw my bi"><span id="f0bd" class="mz ll jg mq b gy na nb l nc nd">for entity in document['annotations']:<br/>    if entity.get('correct', True) or entity.get('manually_created', False):<br/>        # Use the annotation<br/>        ...</span></pre><p id="e8f2" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">加载<code class="fe mn mo mp mq b">.json</code>文件现在很简单:</p><pre class="mr ms mt mu gt mv mq mw mx aw my bi"><span id="81c6" class="mz ll jg mq b gy na nb l nc nd">import os<br/>import datasets<br/>from medcat.datasets import medcat_ner</span><span id="6239" class="mz ll jg mq b gy ne nb l nc nd">DATA_PATH = '&lt;path to my .json export from medcattrainer&gt;'<br/>dataset=datasets.load_dataset(os.path.abspath(medcat_ner.__file__), <br/>                              data_files=DATA_PATH, <br/>                              split=datasets.Split.TRAIN)</span></pre><p id="bbdc" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">一旦数据集被加载，我们需要把它转换成正确的格式🤗变形金刚模型。也就是说，对文本进行标记并分配标签。我们写了一个包装器🤗tokenizers，这将照顾一切:</p><pre class="mr ms mt mu gt mv mq mw mx aw my bi"><span id="4109" class="mz ll jg mq b gy na nb l nc nd">from medcat.datasets.tokenizer_ner import TokenizerNER<br/>from transformers import AutoTokenizer</span><span id="e8a1" class="mz ll jg mq b gy ne nb l nc nd">hf_tokenizer = AutoTokenizer.from_pretrained('&lt;name&gt;')<br/>id2type = {}<br/>for i in range(hf_tokenizer.vocab_size):<br/>    id2type[i] = 'sub' if hf_tokenizer.convert_ids_to_tokens(i).startswith("##") else 'start'<br/>tokenizer = TokenizerNER(hf_tokenizer, id2type=id2type)</span></pre><p id="82a2" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">使用新的标记器，我们可以将数据集转换成所需的格式:</p><pre class="mr ms mt mu gt mv mq mw mx aw my bi"><span id="abb1" class="mz ll jg mq b gy na nb l nc nd">encoded_dataset = dataset.map(<br/> lambda examples: tokenizer.encode(examples, ignore_subwords=True),<br/> batched=True,<br/> remove_columns=['ent_cuis', 'ent_ends', 'ent_starts', 'text'])</span></pre><p id="9c04" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">数据集现在看起来像这样:</p><pre class="mr ms mt mu gt mv mq mw mx aw my bi"><span id="ae3a" class="mz ll jg mq b gy na nb l nc nd">Dataset({<br/>    features: ['id', 'input_ids', 'labels'],<br/>    num_rows: 4935<br/>})</span></pre><h1 id="6d35" class="lk ll jg bd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh bi translated">培训a🤗变形金刚模型</h1><p id="025f" class="pw-post-body-paragraph kd ke jg kf b kg mi ki kj kk mj km kn ko mk kq kr ks ml ku kv kw mm ky kz la ij bi translated">如果我们的用例具有相对较少数量的独特概念(不是几十个，几千个)，我们可以使用来自🤗使用TokenClassification头:</p><pre class="mr ms mt mu gt mv mq mw mx aw my bi"><span id="988a" class="mz ll jg mq b gy na nb l nc nd"># It is important to set the num_labels, which is the number of unique concepts</span><span id="d307" class="mz ll jg mq b gy ne nb l nc nd">model = AutoModelForTokenClassification.from_pretrained("emilyalsentzer/Bio_ClinicalBERT", num_labels=len(tokenizer.label_map))</span></pre><p id="5926" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">为了对数据进行批处理并填充到需要的地方，我们使用了来自<code class="fe mn mo mp mq b">MedCAT.datasets</code>的<code class="fe mn mo mp mq b">CollateAndPadNER</code>，对于<code class="fe mn mo mp mq b">metrics</code>，我们编写了一个简单的函数来打印令牌分类报告。现在一切都准备好了，我们从🤗并运行培训:</p><pre class="mr ms mt mu gt mv mq mw mx aw my bi"><span id="f80d" class="mz ll jg mq b gy na nb l nc nd">trainer = Trainer(<br/>    model=model,                         <br/>    args=training_args,                 <br/>    train_dataset=encoded_dataset['train'],       <br/>    eval_dataset=encoded_dataset['test'],     <br/>    compute_metrics=metrics,<br/>    data_collator=collate_fn,<br/>    tokenizer=None # We've tokenized earlier<br/>)<br/>trainer.train()</span></pre><h1 id="4348" class="lk ll jg bd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh bi translated">结果</h1><p id="3565" class="pw-post-body-paragraph kd ke jg kf b kg mi ki kj kk mj km kn ko mk kq kr ks ml ku kv kw mm ky kz la ij bi translated">我们在MedMentions (MM)上测试性能，因为它是一个相当完整的数据集，有大量的注释(它并不完美，因为注释者有一些分歧，但它已经足够好了)。</p><p id="0bdd" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">该模型在三个不同版本的MM上进行测试:1)整个数据集；2)只有频率在300以上的概念；3)只有1000以上的频率。</p><h2 id="8b09" class="mz ll jg bd lm nf ng dn lq nh ni dp lu ko nj nk ly ks nl nm mc kw nn no mg np bi translated">完整的MM数据集(测试集中的13069个概念)</h2><p id="11a4" class="pw-post-body-paragraph kd ke jg kf b kg mi ki kj kk mj km kn ko mk kq kr ks ml ku kv kw mm ky kz la ij bi translated">BERT最糟糕的用例，大量的概念，其中大多数有&lt;10 occurrences. As it can be seen BERT cannot handle this use-case at all — at least not in this form. All scores are macro averaged.</p><ul class=""><li id="10fe" class="lb lc jg kf b kg kh kk kl ko ld ks le kw lf la lg lh li lj bi translated">MedCAT (unsupervised): F1=0.67, P=0.80, R=0.69</li><li id="0c0e" class="lb lc jg kf b kg nq kk nr ko ns ks nt kw nu la lg lh li lj bi translated">BERT: F1=0.0, R=0.01, P=0.0</li></ul><h2 id="07dc" class="mz ll jg bd lm nf ng dn lq nh ni dp lu ko nj nk ly ks nl nm mc kw nn no mg np bi translated">Only concepts with frequency &gt; 300个(测试集中有107个概念)</h2><p id="4cce" class="pw-post-body-paragraph kd ke jg kf b kg mi ki kj kk mj km kn ko mk kq kr ks ml ku kv kw mm ky kz la ij bi translated">第一个用例在医疗保健领域是相当标准的，大量的概念带有不同数量的注释。稍微不太标准的是，我们已经为每个概念添加了不少注释。有趣的是表演几乎是一样的。</p><ul class=""><li id="b350" class="lb lc jg kf b kg kh kk kl ko ld ks le kw lf la lg lh li lj bi translated">MedCAT(监督):P=0.50，R=0.44，F1=0.43</li><li id="a681" class="lb lc jg kf b kg nq kk nr ko ns ks nt kw nu la lg lh li lj bi translated">伯特:P=0.47，R=0.46，F1=0.43</li></ul><h2 id="3090" class="mz ll jg bd lm nf ng dn lq nh ni dp lu ko nj nk ly ks nl nm mc kw nn no mg np bi translated">仅频率&gt; 1000的概念(测试集中有12个概念)</h2><p id="b026" class="pw-post-body-paragraph kd ke jg kf b kg mi ki kj kk mj km kn ko mk kq kr ks ml ku kv kw mm ky kz la ij bi translated">这个用例应该最适合BERT，因为我们只关注具有大量训练数据的概念。像这样的用例非常罕见，这一个有点特殊，因为出现这么多次的概念主要是定性概念——它们非常多样(许多不同的术语属于同一个概念),更适合类似BERT的模型。</p><ul class=""><li id="4a9a" class="lb lc jg kf b kg kh kk kl ko ld ks le kw lf la lg lh li lj bi translated">MedCAT(受监督):F1=0.34，P=0.24，R=0.70</li><li id="83a2" class="lb lc jg kf b kg nq kk nr ko ns ks nt kw nu la lg lh li lj bi translated">伯特:F1=0.59，P=0.60，R=0.59</li></ul><h1 id="f25d" class="lk ll jg bd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh bi translated">结论</h1><p id="c46e" class="pw-post-body-paragraph kd ke jg kf b kg mi ki kj kk mj km kn ko mk kq kr ks ml ku kv kw mm ky kz la ij bi translated">生物医学NER+1是一项艰巨的任务，就像其他任何事情一样，一个模型并不适合所有情况。我们表明，对于具有有限数量的训练样本或相对较低的术语方差的用例，基于Word2Vec的浅层神经网络是更好的选择。但是，如果我们有大量的训练样本和较高的长期方差，基于BERT的模型表现更好。</p><p id="60e2" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">最后，这两种方法现在都是MedCAT的一部分。</p></div></div>    
</body>
</html>