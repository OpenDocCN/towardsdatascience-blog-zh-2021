<html>
<head>
<title>Embeddings, Beyond Just Words</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">嵌入，不仅仅是文字</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/embeddings-beyond-just-words-2c835678dae2?source=collection_archive---------24-----------------------#2021-06-04">https://towardsdatascience.com/embeddings-beyond-just-words-2c835678dae2?source=collection_archive---------24-----------------------#2021-06-04</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="066d" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">除了单词之外的空间嵌入的一些应用的好处</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/90dc826d262ecdd9cc444ab6be71a3ae.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*UH1itkH0uy-OPmPV"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">帕特里克·托马索在<a class="ae kv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="fc67" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">嵌入是机器学习(ML)中一个方便的概念，大多数时候，像向量和单词表示这样的术语经常出现在上下文中。本文描述了向量大小对ML模型意味着什么，以及嵌入与模型输入有什么关系。</p><p id="0ffc" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">嵌入只是一个映射函数，可以将离散的值列表映射到连续的向量空间。连续空间是密集的，用多维向量表示。在稀疏数据上训练模型是一项非常艰巨的任务；因此，在这种情况下，嵌入变得非常有影响力。</p><h1 id="4824" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">表现</h1><p id="c0cc" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">自然语言处理(NLP)模型使用嵌入来表示单词，主要是在语料库包含许多单词的情况下。机器学习中的单词需要数字表示。嵌入可以帮助建立一个多维空间，通常代表每个单词的语义。<br/>例如，如果我们有1000个单词，我们可以用几种技术来表示每个单词:</p><ul class=""><li id="5d4c" class="mp mq iq ky b kz la lc ld lf mr lj ms ln mt lr mu mv mw mx bi translated">给每个单词分配一个索引:在这种情况下，我们可以为长度为(<em class="my"> n </em>)的每个单词构建一个向量<strong class="ky ir">。<em class="my"> n: </em>代表词汇量。</strong></li></ul><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mz"><img src="../Images/a20fd929e4803913a2976320b4efe2f1.png" data-original-src="https://miro.medium.com/v2/resize:fit:942/format:webp/1*FwOwWu5i0VL06VDFtAkg9A.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图1:使用单词包方法编码单词。向量的长度等于词汇量的大小。</p></figure><ul class=""><li id="c7cb" class="mp mq iq ky b kz la lc ld lf mr lj ms ln mt lr mu mv mw mx bi translated">或者，我们可以将每个单词映射到n个嵌入的向量。神经网络学会了如何将语义相似的单词相对地放在一起。这种表示的主要好处是词汇表中的相似单词将被映射到输入空间中的相似区域。认识到这样的区域有助于我们建立多种能力，例如相似性、相关性，这些能力可以在推荐模型中使用。</li></ul><p id="1a72" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">有更多的编码表示。如果我们在一个巨大的词汇表列表上训练一个模型，我们应该预料到模型正在更新的许多权重，这可能会显著影响模型的性能。</p><h1 id="ec81" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">单词嵌入</h1><p id="949f" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">单词嵌入是一种普遍使用的方法，也是一种非常创新的处理输入词汇域的方法。它们在NLP问题中被广泛用于建立语言模型。语言模型可以执行许多具有挑战性的任务，例如分类、翻译、情感分析和文本生成。Word2Vec、Glove Vectors只是构建神经网络来学习给定语料库中的单词嵌入的两个例子。</p><p id="2e6c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在图1提供的例子中，如果我们的词汇表是1000个单词的长度，那么["Machine "，" Learning"]输入数组用两个1000维的向量表示。</p><h2 id="cc5d" class="na lt iq bd lu nb nc dn ly nd ne dp mc lf nf ng me lj nh ni mg ln nj nk mi nl bi translated">例如:Word2Vec</h2><p id="8dda" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">Word2Vec是一个依赖机器学习建立模型的学习表示的例子。该模型的目的是计算单词的含义，而不是对其在句子中的出现进行编码。基于每个向量之间的距离来构建词向量。为了建立这个模型，我们使用神经网络来学习将这些向量与预测它们周围单词的隐含意义相关联。</p><p id="1250" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">下面的交互式图形说明了如何构建该模型，其中使用单个隐藏层构建了一个神经网络。这个模型使我们能够通过学习输入文本提供的隐式交互来轻松地比较词汇，如图2和图3所示。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nm"><img src="../Images/ecfe31c75f45694bde9fbcd56e0c767f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/1*ww8Is9u7YeWq5tDvvQOEjQ.gif"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图2:word 2 vec学习过程的可视化</p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nm"><img src="../Images/b79d01243bd2d6e1dbefd54bf6fb212f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/1*vnmTaKQv-key5n2gUe6-MA.gif"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图3:word 2 vec学习过程的二维可视化</p></figure><h2 id="6419" class="na lt iq bd lu nb nc dn ly nd ne dp mc lf nf ng me lj nh ni mg ln nj nk mi nl bi translated">示例:神经嵌入</h2><p id="0201" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">构建神经网络时，我们可以创建一个嵌入层，以较小的维度对输入进行分组。为了说明在NLP问题中使用嵌入层，我们将看一个简单的例子。假设我们有十个句子，每个句子包含一些ML和其他通用计算主题。</p><pre class="kg kh ki kj gt nn no np nq aw nr bi"><span id="3988" class="na lt iq no b gy ns nt l nu nv">sentences = [<br/>    'Machine Learning', <br/>    'Model training',<br/>    'Supervised learning', <br/>    'Unsupervised learning', <br/>    'Reinforcement learning', <br/>    'Computer Hardware Engineering', <br/>    'Computer Security', <br/>    'Computer architecture', <br/>    'Operating System', <br/>    'Parallel computing'<br/>]</span></pre><p id="483c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们将跳过文本处理细节，我们也简化它。例如，我们没有将字符转换成小写字母。然而，本文末尾的链接中提供了完整的代码。对句子中的每个单词进行编码是我们的下一个目标。</p><pre class="kg kh ki kj gt nn no np nq aw nr bi"><span id="5112" class="na lt iq no b gy ns nt l nu nv">encoded_sentences = []<br/>for sentence in sentences: <br/>    encoded_sentences.append(label_encoder.transform(sentence.split(' ')) + 1)</span></pre><p id="a6e4" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">如果我们打印出这个数组的内容，我们应该得到如下结果:</p><pre class="kg kh ki kj gt nn no np nq aw nr bi"><span id="8cb4" class="na lt iq no b gy ns nt l nu nv">[array([5, 4]),<br/> array([ 6, 17]),<br/> array([11, 16]),<br/> array([13, 16]),<br/> array([ 9, 16]),<br/> array([1, 3, 2]),<br/> array([ 1, 10]),<br/> array([ 1, 14]),<br/> array([ 7, 12]),<br/> array([ 8, 15])]</span></pre><p id="e246" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">请注意，第六个句子有三个单词，而所有其他句子都只有两个。因此，我们需要在较短的句子末尾填充零。</p><pre class="kg kh ki kj gt nn no np nq aw nr bi"><span id="a91c" class="na lt iq no b gy ns nt l nu nv">from keras.preprocessing.sequence import pad_sequences<br/># Since the sentences size is different, we need to add padding. <br/>max_length = 3 # No more than three words per sentence <br/>#padding = post since we need to fill the zeros at the end<br/>padded_enc_sentences = pad_sequences(encoded_sentences, maxlen=max_length, padding='post')</span></pre><p id="3b9a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">让我们再看一眼编码的句子。</p><pre class="kg kh ki kj gt nn no np nq aw nr bi"><span id="2721" class="na lt iq no b gy ns nt l nu nv">array([[ 5,  4,  0],<br/>       [ 6, 17,  0],<br/>       [11, 16,  0],<br/>       [13, 16,  0],<br/>       [ 9, 16,  0],<br/>       [ 1,  3,  2],<br/>       [ 1, 10,  0],<br/>       [ 1, 14,  0],<br/>       [ 7, 12,  0],<br/>       [ 8, 15,  0]], dtype=int32)</span></pre><p id="ff2c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">构建嵌入层非常简单。我们将使用Keras [1]来创建这一层并编译模型。注意，有三个参数传递给嵌入层:input_dim、output_dim和input_length。Input_dim表示语料库的大小(词汇量)，output_dim是我们想要构建的嵌入向量的大小，以及输入的向量大小。</p><pre class="kg kh ki kj gt nn no np nq aw nr bi"><span id="49d9" class="na lt iq no b gy ns nt l nu nv">model = Sequential()<br/>embedding_layer = Embedding(input_dim=len(all_tokens), output_dim=2, input_length=3)<br/>model.add(embedding_layer)<br/>model.compile('adam', 'mse')</span></pre><p id="dc95" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">如果我们想要查看句子“机器学习”的嵌入权重，该模型将返回以下权重:</p><pre class="kg kh ki kj gt nn no np nq aw nr bi"><span id="f10d" class="na lt iq no b gy ns nt l nu nv">array([[[ 0.02791763, -0.02687442],<br/>        [ 0.00129487,  0.02928725],<br/>        [ 0.00248948,  0.04000784]]], dtype=float32)</span></pre><p id="4e22" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">同样值得一提的是，NLP中的嵌入面临许多挑战。例如，一个句子中的一个单词如果放在另一个上下文中可能有完全不同的意思。有更高级的嵌入方法来补充这种基本方法。</p><h1 id="1050" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">超越单词嵌入</h1><p id="cf9e" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">正如我们所看到的，嵌入可以给传递给模型的输入提供有价值的见解。它可以有效地作为一种降维技术，同时学习隐藏的关系。</p><p id="5f95" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">嵌入建模已经与传统算法(如贝叶斯网络)和神经网络(前馈、卷积神经网络等)一起使用。关键特征是嵌入提供了输入到一些可学习的上下文表示中的组合。可以嵌入特性，提供不同数据类型的语义相似性。通常，它们是非结构化的，可以包含图像。</p><h2 id="908f" class="na lt iq bd lu nb nc dn ly nd ne dp mc lf nf ng me lj nh ni mg ln nj nk mi nl bi translated">应用:</h2><p id="0e9f" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">除了NLP之外，在许多领域中，嵌入都是非常有用的特征工程任务，这样的例子有很多。</p><ul class=""><li id="715c" class="mp mq iq ky b kz la lc ld lf mr lj ms ln mt lr mu mv mw mx bi translated">学习浏览在线商店时的用户行为。</li><li id="891f" class="mp mq iq ky b kz nw lc nx lf ny lj nz ln oa lr mu mv mw mx bi translated">学习在一组给定的数据中发现异常。</li><li id="4727" class="mp mq iq ky b kz nw lc nx lf ny lj nz ln oa lr mu mv mw mx bi translated">理解推荐系统的项目-用户关系。</li><li id="f1e1" class="mp mq iq ky b kz nw lc nx lf ny lj nz ln oa lr mu mv mw mx bi translated">学习在一系列交易中发现欺诈。</li><li id="5ef3" class="mp mq iq ky b kz nw lc nx lf ny lj nz ln oa lr mu mv mw mx bi translated">学习蛋白质序列。</li></ul><h2 id="fdca" class="na lt iq bd lu nb nc dn ly nd ne dp mc lf nf ng me lj nh ni mg ln nj nk mi nl bi translated">例如:电影推荐</h2><p id="5eb3" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">我们将使用MovieLens数据集来学习用户以前对电影的评级，以预测他们接下来有兴趣看什么。数据集可以在<a class="ae kv" href="https://www.kaggle.com/grouplens/movielens-20m-dataset" rel="noopener ugc nofollow" target="_blank"> Kaggle </a>上找到。数据集由几个CSV文件组成，我们将使用的是“评级”和“电影”。数据集必须准备好合并这两个CSV文件，从而产生两千万条包含六列的记录。最小额定值为0.5，最大额定值为5。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ob"><img src="../Images/97067435ec1d1768f79c8eef8bc7789f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SoI1RNLcnsyoHy-yu1yEeQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">MovieLens数据集的示例。</p></figure><p id="79ef" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们的预测模型将学习两列:userId和movieId。两列都包含离散的高基数数值。我们将使用嵌入为每个值构建一个密集向量。输入由一个userId和一个movieId的组合组成，分为两个输入层，每个输入长度为一。</p><pre class="kg kh ki kj gt nn no np nq aw nr bi"><span id="bcd5" class="na lt iq no b gy ns nt l nu nv"># userId input:<br/>user_id_input = Input(shape=(1,), dtype='int64', name='user_id')<br/># movieId input: <br/>movie_id_input = Input(shape=(1,), dtype='int64', name='movie_id')</span></pre><p id="423b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们将创建两个嵌入层:一个用于userId，另一个用于movieId。嵌入的大小设置为8，但可以根据模型的性能进行调整。</p><pre class="kg kh ki kj gt nn no np nq aw nr bi"><span id="25d5" class="na lt iq no b gy ns nt l nu nv"># Building the embedding layers <br/>user_embedding_size = 8<br/>movie_embedding_size = 8</span><span id="05df" class="na lt iq no b gy oc nt l nu nv">user_embedding = Embedding(len(unq_users), user_embedding_size, input_length=1, embeddings_regularizer=l2(1e-4))(user_id_input)</span><span id="43c0" class="na lt iq no b gy oc nt l nu nv">movie_embedding = Embedding(len(unq_movies), movie_embedding_size, input_length=1, embeddings_regularizer=l2(1e-4))(movie_id_input)</span></pre><p id="be24" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在，在构建模型之前，我们需要设计这两个嵌入层是如何结合在一起的？例如，我们将它们合并为两个独立的输入，或者使用点积。在这个例子中，我们将使用Keras点函数。</p><pre class="kg kh ki kj gt nn no np nq aw nr bi"><span id="bf0b" class="na lt iq no b gy ns nt l nu nv">dot_out = dot([user_embedding, movie_embedding], axes=-1, normalize=False)</span><span id="1b66" class="na lt iq no b gy oc nt l nu nv">out = Flatten()(dot_out)</span><span id="f257" class="na lt iq no b gy oc nt l nu nv">model = Model(<br/>    inputs = [user_id_input, movie_id_input],<br/>    outputs = out,<br/>)</span><span id="c55f" class="na lt iq no b gy oc nt l nu nv">model.compile(Adam(0.001), loss='mse')</span></pre><p id="957b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在上面的编译说明中，我们选择了损失的最小平方误差函数。让我们打印出我们模型的摘要:</p><pre class="kg kh ki kj gt nn no np nq aw nr bi"><span id="44cd" class="na lt iq no b gy ns nt l nu nv">__________________________________________________________________________________________________<br/>Layer (type)                    Output Shape         Param #     Connected to                     <br/>==================================================================================================<br/>user_id (InputLayer)            (None, 1)            0                                            <br/>__________________________________________________________________________________________________<br/>movie_id (InputLayer)           (None, 1)            0                                            <br/>__________________________________________________________________________________________________<br/>embedding_1 (Embedding)         (None, 1, 8)         1085000     user_id[0][0]                    <br/>__________________________________________________________________________________________________<br/>embedding_2 (Embedding)         (None, 1, 8)         141872      movie_id[0][0]                   <br/>__________________________________________________________________________________________________<br/>dot_1 (Dot)                     (None, 1, 1)         0           embedding_1[0][0]                <br/>                                                                 embedding_2[0][0]                <br/>__________________________________________________________________________________________________<br/>flatten_1 (Flatten)             (None, 1)            0           dot_1[0][0]                      <br/>==================================================================================================<br/>Total params: 1,226,872<br/>Trainable params: 1,226,872<br/>Non-trainable params: 0</span></pre><p id="ce70" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在的培训步骤应该很简单:</p><pre class="kg kh ki kj gt nn no np nq aw nr bi"><span id="0868" class="na lt iq no b gy ns nt l nu nv">model.fit(<br/>    [df.userId, df.movieId],<br/>    df.rating, <br/>    batch_size=64, <br/>    epochs=1, <br/>    validation_split=.05,<br/>)</span></pre><p id="1013" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">该模型的实现被简化，因为其目的是向仅使用用户id的用户展示嵌入如何捕捉电影之间的相似性。在捕获语义的同时，可以将userId和movieId列的高维度转换到低维空间。</p><h1 id="5d1a" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">摘要</h1><p id="ba0e" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">在本文中，我们演示了嵌入如何向ML模型展示发现数据模式的机会。这是产生更有效的低维空间特征的非常有用的技术。我们已经看到单词袋向量是如何被转化成更小的向量的。我们还讨论了在神经网络中使用嵌入作为一个层。如果你使用的是ML工具包，比如Keras，那么要确保向量的大小是固定的。</p><p id="de4e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">你可以在我的Github上找到完整的代码示例。如果你仍然渴望了解更多，请查看[2]中的速成课程。谢谢大家！</p><h1 id="6452" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">参考</h1><p id="1716" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">[1] Keras，在https://keras.io/api/layers/core_layers/embedding/<a class="ae kv" href="https://keras.io/api/layers/core_layers/embedding/" rel="noopener ugc nofollow" target="_blank">有售</a></p><p id="ed00" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">[2]嵌入:谷歌的速成课程可在<a class="ae kv" href="https://developers.google.com/machine-learning/crash-course/embeddings/" rel="noopener ugc nofollow" target="_blank">https://developers . Google . com/machine-learning/Crash-Course/Embeddings/</a>上获得</p></div></div>    
</body>
</html>