<html>
<head>
<title>Fine-tuning pretrained NLP models with Huggingface’s Trainer</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用Huggingface的训练器微调预训练的NLP模型</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/fine-tuning-pretrained-nlp-models-with-huggingfaces-trainer-6326a4456e7b?source=collection_archive---------3-----------------------#2021-03-25">https://towardsdatascience.com/fine-tuning-pretrained-nlp-models-with-huggingfaces-trainer-6326a4456e7b?source=collection_archive---------3-----------------------#2021-03-25</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="a31f" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">无需原生Pytorch或Tensorflow即可微调预训练NLP模型的简单方法</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/97f241263270297433c9cb377721cec5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*QQIQkO_uYhjiyKjf"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">克里斯托夫·高尔在<a class="ae kv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="373d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">动机</strong>:在参加一个数据科学竞赛时，我正在微调一个预先训练好的模型，并意识到使用原生PyTorch或Tensorflow来微调一个模型是多么乏味。我用Huggingface的训练器API进行了实验，并对它的简单程度感到惊讶。由于网上关于如何使用Huggingface的训练器API的例子很少，我希望提供一个简单的例子，说明如何使用训练器来微调你的预训练模型。</p><p id="5815" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在我们开始之前，这里有一些理解本文的先决条件:</p><ol class=""><li id="92ac" class="ls lt iq ky b kz la lc ld lf lu lj lv ln lw lr lx ly lz ma bi translated">对Python的中级理解</li><li id="863e" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">对训练神经网络模型的基本理解</li><li id="8664" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">对迁移学习的基本理解</li></ol></div><div class="ab cl mg mh hu mi" role="separator"><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml"/></div><div class="ij ik il im in"><p id="bc48" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">为了节省您的时间，我将只为您提供代码，这些代码可用于使用训练器API来训练和预测您的模型。然而，如果你有兴趣了解它是如何工作的，请继续阅读。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="mn mo l"/></div></figure></div><div class="ab cl mg mh hu mi" role="separator"><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml"/></div><div class="ij ik il im in"><p id="f70f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">步骤1: </strong>初始化预训练模型和记号赋予器</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mp"><img src="../Images/d5ff1571160e2e345bf932145d76d0aa.png" data-original-src="https://miro.medium.com/v2/resize:fit:976/format:webp/1*0_QEJGpoZ26bRVorHpA6cg.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">代码所基于的示例数据集</p></figure><p id="cfaa" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在上面的代码中，使用的数据是IMDB电影情感数据集。这些数据允许我们训练一个模型来检测电影评论的情绪- 1是积极的，而0是消极的。这是序列分类的NLP任务，因为我们想要将每个评论(文本序列)分类为正面或负面。</p><p id="3059" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">有许多预训练的模型可以用来训练我们的情感分析模型，让我们以预训练的BERT为例。预训练的BERT模型有很多变体，<em class="mq"> bert-base-uncased </em>只是其中一种变体。你可以从<a class="ae kv" href="https://huggingface.co/models" rel="noopener ugc nofollow" target="_blank">拥抱面部模型页面</a>搜索更多预训练模型。</p><pre class="kg kh ki kj gt mr ms mt mu aw mv bi"><span id="4367" class="mw mx iq ms b gy my mz l na nb">model_name = "bert-base-uncased"<br/>tokenizer = BertTokenizer.from_pretrained(model_name)<br/>model = BertForSequenceClassification.from_pretrained(model_name, num_labels=2)</span></pre><p id="55fb" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">由于我们使用的是预训练模型，我们需要确保输入数据的形式与预训练模型的训练形式相同。因此，我们需要使用模型的名称来实例化记号赋予器。</p><p id="2b4a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">既然已经初始化了模型和记号化器，我们可以继续预处理数据了。</p><p id="c736" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">步骤2: </strong>使用预训练的标记器预处理文本</p><pre class="kg kh ki kj gt mr ms mt mu aw mv bi"><span id="c739" class="mw mx iq ms b gy my mz l na nb">X_train_tokenized = tokenizer(X_train, padding=True, truncation=True, max_length=512)<br/>X_val_tokenized = tokenizer(X_val, padding=True, truncation=True, max_length=512)</span></pre><p id="164e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">让我们使用前面初始化的标记器对文本进行预处理。</p><p id="d40c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们用于标记器的输入文本是一个字符串列表。</p><p id="1b18" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们已经设置了<code class="fe nc nd ne ms b">padding=True, truncation=True, max_length=512</code>,这样我们可以为模型获得相同长度的输入——长文本将被截断为512个标记，而短文本将添加额外的标记，使其成为512个标记。</p><p id="fa5f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">使用512个令牌，因为这是BERT模型可以采用的最大令牌长度。</p><p id="98da" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">对文本进行标记后，您将得到一个包含3个键的python字典:</p><ol class=""><li id="4238" class="ls lt iq ky b kz la lc ld lf lu lj lv ln lw lr lx ly lz ma bi translated">输入标识</li><li id="3bf2" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">令牌类型标识</li><li id="d9ff" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">注意_屏蔽</li></ol><p id="8aa9" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">步骤3: </strong>创建torch数据集</p><p id="896c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">训练器API要求模型在<code class="fe nc nd ne ms b"><strong class="ky ir">torch.utils.data.Dataset</strong></code> <strong class="ky ir"> </strong>类中。因此，我们需要创建一个从torch Dataset类继承的新类。</p><p id="010a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在继承的类中，我们需要有<code class="fe nc nd ne ms b">__getitem__</code>和<code class="fe nc nd ne ms b">__len__</code>方法，它们允许训练器创建批量数据并分别获得长度。</p><pre class="kg kh ki kj gt mr ms mt mu aw mv bi"><span id="986e" class="mw mx iq ms b gy my mz l na nb">class Dataset(torch.utils.data.Dataset):    <br/>    def __init__(self, encodings, labels=None):          <br/>        self.encodings = encodings        <br/>        self.labels = labels<br/>     <br/>    def __getitem__(self, idx):<br/>        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}<br/>        if self.labels:<br/>            item["labels"] = torch.tensor(self.labels[idx])<br/>        return item</span><span id="9b0d" class="mw mx iq ms b gy nf mz l na nb">     def __len__(self):<br/>        return len(self.encodings["input_ids"])</span><span id="ba56" class="mw mx iq ms b gy nf mz l na nb">train_dataset = Dataset(X_train_tokenized, y_train)<br/>val_dataset = Dataset(X_val_tokenized, y_val)</span></pre><p id="29cc" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">将默认labels参数设置为None的目的是，我们可以重用该类来预测看不见的数据，因为这些数据没有标签。</p><p id="777f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><code class="fe nc nd ne ms b">__getitem__</code>方法基本上为每个文本返回一个值字典。通过运行该方法，当在训练过程中批量处理数据时，它为每个文本创建一个带有<code class="fe nc nd ne ms b">input_ids</code>、<code class="fe nc nd ne ms b">attention_mask</code>和<code class="fe nc nd ne ms b">token_type_ids</code>的字典。</p><p id="4759" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><code class="fe nc nd ne ms b">__len__</code>方法需要返回输入数据的长度。</p><p id="a395" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">步骤4: </strong>定义培训参数和培训师</p><pre class="kg kh ki kj gt mr ms mt mu aw mv bi"><span id="8e26" class="mw mx iq ms b gy my mz l na nb">def compute_metrics(p):    <br/>    pred, labels = p<br/>    pred = np.argmax(pred, axis=1)<br/>    accuracy = accuracy_score(y_true=labels, y_pred=pred)<br/>    recall = recall_score(y_true=labels, y_pred=pred)<br/>    precision = precision_score(y_true=labels, y_pred=pred)<br/>    f1 = f1_score(y_true=labels, y_pred=pred)</span><span id="3779" class="mw mx iq ms b gy nf mz l na nb">    return {"accuracy": accuracy, "precision": precision, "recall": recall, "f1": f1} </span><span id="4e6a" class="mw mx iq ms b gy nf mz l na nb"># Define Trainer<br/>args = TrainingArguments(<br/>    output_dir="output",<br/>    evaluation_strategy="steps",<br/>    eval_steps=500,<br/>    per_device_train_batch_size=8,<br/>    per_device_eval_batch_size=8,<br/>    num_train_epochs=3,<br/>    seed=0,<br/>    load_best_model_at_end=True,)<br/>trainer = Trainer(<br/>    model=model,<br/>    args=args,<br/>    train_dataset=train_dataset,<br/>    eval_dataset=val_dataset,<br/>    compute_metrics=compute_metrics,<br/>    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)],)<br/> <br/># Train pre-trained model<br/>trainer.train()</span></pre><p id="3b33" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这就是培训师功能的神奇之处。我们可以在TrainingArguments和Trainer类中定义训练参数，也可以用一个命令训练模型。</p><p id="8bfe" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们需要首先定义一个函数来计算验证集的指标。由于这是一个二元分类问题，我们可以使用准确度、精确度、召回率和f1分数。</p><p id="5f01" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">接下来，我们在TrainingArgs和Trainer类中指定一些训练参数，设置预训练模型、训练数据和评估数据。</p><p id="0464" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在我们定义了参数之后，只需运行<code class="fe nc nd ne ms b">trainer.train()</code>来训练模型。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ng"><img src="../Images/c1ae9368863ffa1ea62f020f8184b877.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fmA11iS54j5Az3ZdYhfEaQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">培训师的培训示例</p></figure><p id="8445" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">第五步:</strong>进行预测</p><pre class="kg kh ki kj gt mr ms mt mu aw mv bi"><span id="0e3d" class="mw mx iq ms b gy my mz l na nb"># Tokenize test data<br/>X_test_tokenized = tokenizer(X_test, padding=True, truncation=True, max_length=512) </span><span id="11ac" class="mw mx iq ms b gy nf mz l na nb"># Create torch dataset<br/>test_dataset = Dataset(X_test_tokenized) </span><span id="e19e" class="mw mx iq ms b gy nf mz l na nb"># Load trained model<br/>model_path = "output/checkpoint-50000"<br/>model = BertForSequenceClassification.from_pretrained(model_path, num_labels=2) </span><span id="e74f" class="mw mx iq ms b gy nf mz l na nb"># Define test trainer<br/>test_trainer = Trainer(model) </span><span id="a84e" class="mw mx iq ms b gy nf mz l na nb"># Make prediction<br/>raw_pred, _, _ = test_trainer.predict(test_dataset) </span><span id="2aff" class="mw mx iq ms b gy nf mz l na nb"># Preprocess raw predictions<br/>y_pred = np.argmax(raw_pred, axis=1)</span></pre><p id="f284" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">训练完模型后，我们对测试数据重复相同的步骤:</p><ol class=""><li id="b290" class="ls lt iq ky b kz la lc ld lf lu lj lv ln lw lr lx ly lz ma bi translated">使用预训练的标记器标记测试数据</li><li id="c1ec" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">创建torch数据集</li><li id="3bb0" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">负载训练模型</li><li id="0845" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">定义培训师</li></ol><p id="bd80" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">要从前面的步骤中加载已训练模型，请将model_path设置为包含已训练模型权重的路径。</p><p id="e3dd" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">为了进行预测，也只需要一个命令<code class="fe nc nd ne ms b">test_trainer.predict(test_dataset)</code>。</p><p id="2ece" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">做了一个预测之后，你只会得到原始的预测。将它转换成可用的格式需要额外的预处理步骤。</p><p id="719d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">因为这个任务只是一个简单的序列分类任务，所以我们可以只获得轴1上的argmax。请注意，其他NLP任务可能需要不同的方式来预处理原始预测。</p></div><div class="ab cl mg mh hu mi" role="separator"><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml"/></div><div class="ij ik il im in"><p id="1014" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我希望这篇文章将有助于简化你微调预训练NLP模型的过程。请随意阅读官方的Huggingface文档，以更好地理解代码并了解它还能做什么。</p></div><div class="ab cl mg mh hu mi" role="separator"><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml"/></div><div class="ij ik il im in"><p id="f0e7" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi">Edit 1 (23/6/21): Removed <code class="fe nc nd ne ms b">save_steps</code> parameter from <code class="fe nc nd ne ms b">TrainingArgument</code> as it is ignored when <code class="fe nc nd ne ms b">load_best_model_at_end</code> is set to True. Thanks 杨程 for the feedback!</p><h2 id="24aa" class="mw mx iq bd nh ni nj dn nk nl nm dp nn lf no np nq lj nr ns nt ln nu nv nw nx bi translated">参考</h2><p id="145e" class="pw-post-body-paragraph kw kx iq ky b kz ny jr lb lc nz ju le lf oa lh li lj ob ll lm ln oc lp lq lr ij bi translated">[1]https://huggingface.co/transformers/training.html<a class="ae kv" href="https://huggingface.co/transformers/training.html" rel="noopener ugc nofollow" target="_blank"/></p><p id="795a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">[2]<a class="ae kv" href="https://huggingface.co/transformers/main_classes/trainer.html" rel="noopener ugc nofollow" target="_blank">https://hugging face . co/transformers/main _ classes/trainer . html</a></p></div></div>    
</body>
</html>