<html>
<head>
<title>Entropy in Soft Actor-Critic (Part 1)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">软演员-评论家中的熵(上)</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/entropy-in-soft-actor-critic-part-1-92c2cd3a3515?source=collection_archive---------24-----------------------#2021-05-04">https://towardsdatascience.com/entropy-in-soft-actor-critic-part-1-92c2cd3a3515?source=collection_archive---------24-----------------------#2021-05-04</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="903a" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsdatascience.com/tagged/thoughts-and-theory" rel="noopener" target="_blank">思想和理论</a></h2><div class=""/><p id="6f9e" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">在概率论中，与熵相关的原理有两个:最大熵的<em class="ku">原理和最小交叉熵</em>的<em class="ku">原理。一开始我们注意到有两种类型的熵，然而还有更多。</em></p><figure class="kw kx ky kz gt la gh gi paragraph-image"><div role="button" tabindex="0" class="lb lc di ld bf le"><div class="gh gi kv"><img src="../Images/82ea80a0843864cdae1760108bdee26a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rGSaA1QwECXyYprS0BEM4w.jpeg"/></div></div><p class="lh li gj gh gi lj lk bd b be z dk translated">资料来源:123rf.com</p></figure><h1 id="a633" class="ll lm iq bd ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi bi translated"><strong class="ak">熵的许多面</strong></h1><p id="0203" class="pw-post-body-paragraph jw jx iq jy b jz mj kb kc kd mk kf kg kh ml kj kk kl mm kn ko kp mn kr ks kt ij bi translated">首先让我们强调一下，最大熵原理和最小交叉熵原理<em class="ku"> </em>都不是定理，它们只是统计推断的原理。这和哲学教义很像。然而，这些学说当然有数学含义。所以我们有两种不同类型的熵:<em class="ku">熵</em>和<em class="ku">交叉熵</em>。它们由所谓的<em class="ku">相关熵</em>连接:</p><figure class="kw kx ky kz gt la gh gi paragraph-image"><div role="button" tabindex="0" class="lb lc di ld bf le"><div class="gh gi mo"><img src="../Images/4f65777ac22f126bf69da69d17085891.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*K-BZWVp5inlJnNOWUcoDeA.png"/></div></div><p class="lh li gj gh gi lj lk bd b be z dk translated"><strong class="bd mp">熵、交叉熵和KL散度(</strong>图片由作者提供)</p></figure><p id="cb54" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">相对熵的另一个更广为人知的名字是kull back-lei bler散度，或<em class="ku"> KL散度</em>。</p><blockquote class="mq mr ms"><p id="d9db" class="jw jx ku jy b jz ka kb kc kd ke kf kg mt ki kj kk mu km kn ko mv kq kr ks kt ij bi translated">“……这一措施在技术文献中可能以九种不同的名称出现……我自己的偏好是术语<strong class="jy ja">歧视信息</strong>及其最小歧视信息(MDI)原则的基础”<em class="iq"/>S . kull back<em class="iq">【5】</em></p></blockquote><p id="64e8" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated"><strong class="jy ja">熵作为一种学习工具</strong></p><p id="8f6e" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">在强化学习中，<a class="ae mw" href="https://lilianweng.github.io/lil-log/2020/06/07/exploration-strategies-in-deep-reinforcement-learning.html" rel="noopener ugc nofollow" target="_blank"> <em class="ku">探索vs .</em></a>是概念的重要组成部分。在没有充分探索的情况下过快地做出决定可能会是一个大失败。探索是学习的主要组成部分。众所周知，<a class="ae mw" rel="noopener" target="_blank" href="/three-aspects-of-deep-rl-noise-overestimation-and-exploration-122ffb4bb92b">在动作中加入噪音</a>是探索中合理的事情之一。熵是另一个强大的探索工具。高熵可以确保我们避免重复利用相同的不一致性。</p><p id="5d6f" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated"><strong class="jy ja">香农熵</strong></p><p id="d521" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">熵是一种通常与无序或不确定状态联系在一起的物理属性。设<strong class="jy ja"> <em class="ku"> X </em> </strong>为某个离散变量，{ <strong class="jy ja"> <em class="ku"> x </em> ₁ </strong>，…，<strong class="jy ja"> <em class="ku"> xn </em> </strong> }为概率为<strong class="jy ja"> <em class="ku"> X </em> </strong>的可能值<strong class="jy ja"><em class="ku">pᵢ= p</em>(<em class="ku">xᵢ</em>)</strong>。那么<em class="ku">信息熵</em>，或者仅仅是<em class="ku">熵</em>，或者<em class="ku">香农熵</em>定义如下:</p><figure class="kw kx ky kz gt la gh gi paragraph-image"><div role="button" tabindex="0" class="lb lc di ld bf le"><div class="gh gi mx"><img src="../Images/883c457849f601575eff72ae70735bd1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DGqjdy0GmlhmKhDOlkLEaQ.png"/></div></div><p class="lh li gj gh gi lj lk bd b be z dk translated"><strong class="bd mp">熵作为不确定性的度量(</strong>图片由作者提供)</p></figure><p id="d7fb" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">直观地说，熵是变量<strong class="jy ja"><em class="ku"/></strong>的分布<strong class="jy ja"><em class="ku">【p】</em></strong>的<em class="ku">不确定性度量。一个典型的例子是与硬币相关的概率分布。让硬币公平，即正面和反面都有相等的概率:<strong class="jy ja"> <em class="ku"> p = p </em> ( </strong>正面<strong class="jy ja">)<em class="ku"/></strong>=<strong class="jy ja"><em class="ku">p</em></strong>(反面)= 1/2。然后，</em></p><p id="b24e" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated"><strong class="jy ja"><em class="ku">H</em>(<em class="ku">p</em>)</strong>=-(1/2 log(1/2)+1/2 log(1/2))=-log(1/2)=-(-1)= 1，既然我们考虑以2为底的log，这是<strong class="jy ja">最大不确定性</strong>，因为很难预测接下来的折腾。让我们取一个不公平的硬币，<strong class="jy ja"> <em class="ku"> p </em> </strong> =0.1。然后，</p><p id="e177" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated"><strong class="jy ja"><em class="ku">H</em>(<em class="ku">p</em>)</strong>=-(0.1 log(0.1)+0.9 log(0.9))=-0.332-0.137 = 0.469。在这种情况下，不确定性明显小于最大不确定性= 1。</p><figure class="kw kx ky kz gt la gh gi paragraph-image"><div class="gh gi my"><img src="../Images/db54878ecf21b3bb4cab0dbcbcfb8902.png" data-original-src="https://miro.medium.com/v2/resize:fit:1110/format:webp/1*u37zKLhcU_USrjJuXjCsgQ.png"/></div><p class="lh li gj gh gi lj lk bd b be z dk translated"><strong class="bd mp">熵曲线(</strong>图片由作者提供)</p></figure><blockquote class="mq mr ms"><p id="b2ac" class="jw jx ku jy b jz ka kb kc kd ke kf kg mt ki kj kk mu km kn ko mv kq kr ks kt ij bi translated">均匀概率产生最大的不确定性，因此也产生最大的熵。</p></blockquote><p id="6179" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">有关熵曲线生成，请参见应用程序2中的python代码。</p><p id="6c78" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated"><strong class="jy ja">统计力学中的最大熵</strong></p><p id="25a1" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">在统计力学中，<em class="ku">玻尔兹曼分布</em>是一种概率分布，它给出了一个系统的状态<strong class="jy ja"> <em class="ku"> i </em> </strong>的概率<strong class="jy ja"> <em class="ku"> pᵢ </em> </strong>:</p><figure class="kw kx ky kz gt la gh gi paragraph-image"><div role="button" tabindex="0" class="lb lc di ld bf le"><div class="gh gi mz"><img src="../Images/1b4fc1293b187ff353219c443e9c10a1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ruDVIjMctTxLn7ECZksZqA.png"/></div></div><p class="lh li gj gh gi lj lk bd b be z dk translated"><strong class="bd mp">玻尔兹曼分布<em class="na">最大化熵H(p) </em> ( </strong>图片由作者提供)</p></figure><p id="d3c1" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">其中<strong class="jy ja">ε<em class="ku">ᵢ</em>T3】是状态<strong class="jy ja">t5】I</strong>，<strong class="jy ja"> <em class="ku"> T </em> </strong>是<strong class="jy ja"> <em class="ku"> </em> </strong>系统的温度，<strong class="jy ja"> <em class="ku"> k </em> </strong>是玻尔兹曼常数。玻尔兹曼分布是<em class="ku">最大化熵</em><strong class="jy ja"><em class="ku">H(p)</em></strong><em class="ku">的分布。</em>玻尔兹曼分布也被称为<em class="ku">吉布斯分布</em>。</strong></p><blockquote class="mq mr ms"><p id="55ca" class="jw jx ku jy b jz ka kb kc kd ke kf kg mt ki kj kk mu km kn ko mv kq kr ks kt ij bi translated">“……一种涉及熵概念的方法，它的存在依赖于热力学第二定律，对许多人来说无疑是牵强的，可能会因为晦涩难懂而使初学者感到厌恶。这种不便可能被一种方法的优点所抵消，这种方法使热力学第二定律如此突出，并给它如此清晰和基本的表达。”(<a class="ae mw" href="https://en.wikisource.org/wiki/Scientific_Papers_of_Josiah_Willard_Gibbs,_Volume_1/Chapter_I" rel="noopener ugc nofollow" target="_blank"> J.W .吉布斯</a>，<em class="iq">【6】</em>)</p></blockquote><p id="2804" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">玻尔兹曼分布与<em class="ku"> softmax函数</em>有关，它在机器学习和神经网络中起着核心作用。我们将在下一篇文章中回到这个问题上(第二部分)</p><h1 id="824c" class="ll lm iq bd ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi bi translated"><strong class="ak">内置熵项的贝尔曼方程</strong></h1><p id="b627" class="pw-post-body-paragraph jw jx iq jy b jz mj kb kc kd mk kf kg kh ml kj kk kl mm kn ko kp mn kr ks kt ij bi translated"><strong class="jy ja">带有熵项的目标函数</strong></p><p id="6bbc" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">SAC算法基于最大熵RL框架。行动者学习机制优化策略以最大化策略的期望收益和期望熵。标准目标(最大化期望回报)增加了一个熵项<strong class="jy ja"/><strong class="jy ja"><em class="ku">【H(p)</em></strong>:</p><figure class="kw kx ky kz gt la gh gi paragraph-image"><div role="button" tabindex="0" class="lb lc di ld bf le"><div class="gh gi nb"><img src="../Images/541662d1c07c94705c337e6196674e97.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BtNHxW64l4LqwvClL2fTSQ.png"/></div></div><p class="lh li gj gh gi lj lk bd b be z dk translated"><strong class="bd mp">熵项H用α加权的目标函数(</strong>作者图片)</p></figure><p id="9cfb" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">这里，<strong class="jy ja"> <em class="ku"> γ </em> </strong>为<em class="ku">折扣</em>因子，0&lt;<strong class="jy ja"><em class="ku">γ</em></strong><em class="ku">&lt;</em>1，要求保证奖励和熵之和是有限的；<strong class="jy ja"> α </strong> &gt; 0 <strong class="jy ja"> </strong>是所谓的<em class="ku">温度</em>参数，决定了熵项<strong class="jy ja"> <em class="ku"> H(p) </em> </strong> <em class="ku">的相对重要性；</em> <strong class="jy ja"> <em class="ku"> R_k </em> </strong>是在时间<strong class="jy ja"> <em class="ku"> k </em> </strong>的奖励；<strong class="jy ja">𝜋</strong>(<strong class="jy ja">*</strong>|<strong class="jy ja">|<em class="ku">s _ k</em></strong>)-在状态<strong class="jy ja"> <em class="ku"> s_k执行的策略<strong class="jy ja"> 𝜋 </strong>的概率分布；τ </em> </strong> = ( <strong class="jy ja"> <em class="ku"> s_k，a_k) </em> </strong> <em class="ku"> - </em>对<strong class="jy ja"> <em class="ku"> </em> </strong>(状态，动作)在时间<strong class="jy ja"> <em class="ku"> k </em> </strong>。</p><p id="5d6a" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated"><strong class="jy ja">最大报酬和最大熵</strong></p><p id="3549" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">在SAC目标函数(3)中，奖励<strong class="jy ja"> <em class="ku"> R_k </em> </strong>增加如下:</p><figure class="kw kx ky kz gt la gh gi paragraph-image"><div role="button" tabindex="0" class="lb lc di ld bf le"><div class="gh gi nc"><img src="../Images/5fecf26f3f53d968f784a64a34c5480b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sybity6u3aRlyEhoWbC0qw.png"/></div></div><p class="lh li gj gh gi lj lk bd b be z dk translated"><strong class="bd mp">熵增奖励(</strong>图片由作者提供)</p></figure><p id="8c03" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">这意味着标准的<em class="ku">最大回报目标</em>增加了熵最大化。SAC算法旨在同时最大化预期收益和熵。</p><blockquote class="mq mr ms"><p id="0635" class="jw jx ku jy b jz ka kb kc kd ke kf kg mt ki kj kk mu km kn ko mv kq kr ks kt ij bi translated">“…最大熵公式在探索和稳健性方面提供了实质性的改进…最大熵策略在面对模型和估计错误时是稳健的…它们通过获得不同的行为来改进探索。”<em class="iq">【1】</em></p></blockquote><p id="a89f" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated"><strong class="jy ja">状态值和动作值功能</strong></p><p id="2f8b" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">状态-价值函数<strong class="jy ja"><em class="ku">v</em>𝜋(<em class="ku">s _ t</em>)</strong>估计如果我们<em class="ku">在状态</em> <strong class="jy ja"> <em class="ku"> s_t </em> </strong>开始时的期望收益。包括<strong class="jy ja">熵项</strong>的状态值函数定义如下:</p><figure class="kw kx ky kz gt la gh gi paragraph-image"><div role="button" tabindex="0" class="lb lc di ld bf le"><div class="gh gi nd"><img src="../Images/9fa466e07368ad77bf9862605eef0254.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LGzZVW3WF_pfPNi0u_UR7g.png"/></div></div><p class="lh li gj gh gi lj lk bd b be z dk translated"><strong class="bd mp">带熵项的状态值函数(</strong>图片由作者提供)</p></figure><p id="d348" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">动作值函数<strong class="jy ja"> Q𝜋( <em class="ku"> s </em> _t，<em class="ku"> a </em> _t) </strong>估计期望值，如果我们<em class="ku">在状态</em> <strong class="jy ja"> <em class="ku"> s_t，</em> </strong>开始并执行任意动作<strong class="jy ja"> <em class="ku"> a_t. </em> </strong>包含<strong class="jy ja">熵项</strong>的动作值函数定义如下</p><figure class="kw kx ky kz gt la gh gi paragraph-image"><div role="button" tabindex="0" class="lb lc di ld bf le"><div class="gh gi ne"><img src="../Images/13326e760a65e222c4c9775c78491c93.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0kKL3oQkeg37vtFZ1QoVsQ.png"/></div></div><p class="lh li gj gh gi lj lk bd b be z dk translated"><strong class="bd mp">带有熵项的动作值函数(</strong>图片由作者提供)</p></figure><p id="d01f" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated"><strong class="jy ja">软状态值功能</strong></p><p id="8c29" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">注意，在熵项中，和从k=1开始。因此，动作值函数<strong class="jy ja"> Q𝜋( <em class="ku"> s </em> _ <em class="ku"> t </em>，<em class="ku"> a </em> _ <em class="ku"> t </em> ) </strong>与状态值函数<strong class="jy ja"><em class="ku">v</em>𝜋(<em class="ku">s _ t</em>)</strong>仅在一项上不同:</p><figure class="kw kx ky kz gt la gh gi paragraph-image"><div role="button" tabindex="0" class="lb lc di ld bf le"><div class="gh gi nf"><img src="../Images/84c69afbf32cfc0309e0d40e740b111d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*B8CXdkZ7m-qP8mS-LyL3pg.png"/></div></div><p class="lh li gj gh gi lj lk bd b be z dk translated"><strong class="bd mp">状态值和动作值函数之间的联系(</strong>图片由作者提供)</p></figure><p id="0f84" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">将表达式(2)中的熵<strong class="jy ja"> <em class="ku"> H(p) </em> </strong>代入(6):</p><figure class="kw kx ky kz gt la gh gi paragraph-image"><div role="button" tabindex="0" class="lb lc di ld bf le"><div class="gh gi ng"><img src="../Images/b6ae607f31cb806e38de0cd20fe659d5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gGkulR6Z5ea186yVN4sBFQ.png"/></div></div><p class="lh li gj gh gi lj lk bd b be z dk translated"><strong class="bd mp">软状态值函数(</strong>图片由作者提供)</p></figure><p id="449a" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">函数<strong class="jy ja"><em class="ku">v</em>𝜋(<em class="ku">s</em>)</strong>就是所谓的<strong class="jy ja">软态值函数</strong>，参见<a class="ae mw" href="https://arxiv.org/abs/1812.05905" rel="noopener ugc nofollow" target="_blank"> <em class="ku">“软优演算法及应用</em> </a>”，第5页</p><p id="7292" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated"><strong class="jy ja">软贝尔曼方程</strong></p><p id="5d65" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">让我们把等式中的第一项和第一项分开。(5)</p><figure class="kw kx ky kz gt la gh gi paragraph-image"><div role="button" tabindex="0" class="lb lc di ld bf le"><div class="gh gi nh"><img src="../Images/37623c3bcc11b371adb947b99311a558.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*55Q1v_X5UzQI3NNxJBGfIw.png"/></div></div><p class="lh li gj gh gi lj lk bd b be z dk translated"><strong class="bd mp">轻微变换的动作值函数(</strong>图片由作者提供)</p></figure><p id="787e" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">通过代入<em class="ku"> k </em> = <em class="ku"> p </em> +1我们得到</p><figure class="kw kx ky kz gt la gh gi paragraph-image"><div role="button" tabindex="0" class="lb lc di ld bf le"><div class="gh gi ni"><img src="../Images/53fa13833d1e670b8e53495b72b1d7e4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*d-UgO286scunF6yJh44kkg.png"/></div></div><p class="lh li gj gh gi lj lk bd b be z dk translated"><strong class="bd mp">代入k = p+1 ( </strong>图片由作者提供)</p></figure><p id="2990" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">eq中<strong class="jy ja"> <em class="ku"> γ </em> </strong>后的表达式。(9)是状态值函数<strong class="jy ja"><em class="ku">v</em>𝜋(<em class="ku">s _</em>{<em class="ku">t+</em>1 })</strong>。</p><figure class="kw kx ky kz gt la gh gi paragraph-image"><div role="button" tabindex="0" class="lb lc di ld bf le"><div class="gh gi nj"><img src="../Images/0bd49c837f3aeffc0225e3d2265d8273.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RxxYMaInvgqjqcqR7uIxWg.png"/></div></div><p class="lh li gj gh gi lj lk bd b be z dk translated"><strong class="bd mp">软贝尔曼方程(</strong>图片由作者提供)</p></figure><p id="0851" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">情商。(10)是将<em class="ku">修正后的贝尔曼方程</em>，参见<a class="ae mw" href="https://arxiv.org/abs/1812.05905" rel="noopener ugc nofollow" target="_blank"> <em class="ku">《软优算法与应用</em> </a>》，第2页.此方程称为<a class="ae mw" href="https://arxiv.org/abs/1702.08165" rel="noopener ugc nofollow" target="_blank"> <strong class="jy ja">软贝尔曼方程</strong> </a> <strong class="jy ja">.</strong>熵是这个方程不可分割的一部分。</p><h1 id="2ae4" class="ll lm iq bd ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi bi translated">演员-评论家神经网络</h1><p id="0099" class="pw-post-body-paragraph jw jx iq jy b jz mj kb kc kd mk kf kg kh ml kj kk kl mm kn ko kp mn kr ks kt ij bi translated">下面，我们展示与一些张量和神经网络的计算相关的SAC算法的片断。使用<em class="ku">神经网络</em> <code class="fe nk nl nm nn b"><strong class="jy ja"><em class="ku">critic</em></strong></code>计算q值<code class="fe nk nl nm nn b"><strong class="jy ja"><em class="ku">qf1</em></strong></code>、<code class="fe nk nl nm nn b"><strong class="jy ja"><em class="ku">qf2</em></strong></code>、<code class="fe nk nl nm nn b"><strong class="jy ja"><em class="ku">qf1_next</em></strong></code>、<code class="fe nk nl nm nn b"><strong class="jy ja"><em class="ku">qf2_next</em></strong></code>、<em class="ku">、</em>。张量<code class="fe nk nl nm nn b"><strong class="jy ja"><em class="ku">next_state_action</em></strong></code>、<code class="fe nk nl nm nn b"><strong class="jy ja"><em class="ku">next_state_log_pi</em></strong></code>使用<em class="ku">神经网络</em><code class="fe nk nl nm nn b"><strong class="jy ja"><em class="ku">policy</em></strong></code><strong class="jy ja"><em class="ku"/></strong>(actor)计算。网络<code class="fe nk nl nm nn b"><strong class="jy ja"><em class="ku">critic</em></strong></code>由类<em class="ku"> QNetwork </em>定义，网络<code class="fe nk nl nm nn b"><strong class="jy ja"><em class="ku">policy</em></strong></code>由类<em class="ku"> GaussianPolicy </em>定义。第三种神经网络将在下一篇文章中讨论(第2部分，软行动者-批评家中的<em class="ku">熵)。</em></p><p id="408c" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated"><strong class="jy ja">计算张量V(s)和Q(s，a) </strong></p><p id="98b6" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">数组<code class="fe nk nl nm nn b"><strong class="jy ja"><em class="ku">state_batch,</em></strong></code><strong class="jy ja"><em class="ku"/></strong><code class="fe nk nl nm nn b"><strong class="jy ja"><em class="ku">action_batch,</em></strong></code><strong class="jy ja"><em class="ku"/></strong><code class="fe nk nl nm nn b"><strong class="jy ja"><em class="ku">reward_batch,</em></strong></code><strong class="jy ja"><em class="ku"/></strong><code class="fe nk nl nm nn b"><strong class="jy ja"><em class="ku">next_action_batch,</em></strong></code>和<code class="fe nk nl nm nn b"><strong class="jy ja"><em class="ku">mask_batch</em></strong></code> <strong class="jy ja"> <em class="ku"> </em> </strong>是从类<code class="fe nk nl nm nn b"><strong class="jy ja"><em class="ku">ReplayMemory</em></strong></code>中先前保存的剧集中提取的<em class="ku"> </em>。</p><figure class="kw kx ky kz gt la"><div class="bz fp l di"><div class="no np l"/></div></figure><p id="eaeb" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">来自(7)的软状态值函数<strong class="jy ja"><em class="ku">V</em>(<em class="ku">s</em>)</strong>由张量<code class="fe nk nl nm nn b"><strong class="jy ja"><em class="ku">min_qf_next</em></strong></code>表示:</p><figure class="kw kx ky kz gt la gh gi paragraph-image"><div role="button" tabindex="0" class="lb lc di ld bf le"><div class="gh gi nq"><img src="../Images/d778213094b83e1661912fd1989e7f17.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*L9S3aNkSpyP7V5ttdnTUUQ.png"/></div></div><p class="lh li gj gh gi lj lk bd b be z dk translated">S <strong class="bd mp"> oft状态值函数</strong> <code class="fe nk nl nm nn b"><strong class="bd mp"><em class="na">V</em>(<em class="na">s</em>) </strong></code> <strong class="bd mp"> ( </strong>图片由作者提供)</p></figure><p id="a409" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">来自贝尔曼方程(10)的软动作值函数<strong class="jy ja"> <em class="ku"> Q </em> ( <em class="ku"> s，</em> ) </strong>由张量<code class="fe nk nl nm nn b"><strong class="jy ja"><em class="ku">next_q_value</em></strong></code> <strong class="jy ja"> <em class="ku"> </em> </strong>表示:</p><figure class="kw kx ky kz gt la gh gi paragraph-image"><div role="button" tabindex="0" class="lb lc di ld bf le"><div class="gh gi nr"><img src="../Images/5f6eef7f7375c41a7cb0789cfb52bdaf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*U_tC7HEX-gk6HJ3T55wh1w.png"/></div></div><p class="lh li gj gh gi lj lk bd b be z dk translated"><strong class="bd mp">软动作值函数<em class="na"> Q </em> ( <em class="na"> s，a </em> ) ( </strong>图片由作者提供)</p></figure><figure class="kw kx ky kz gt la"><div class="bz fp l di"><div class="no np l"/></div><p class="lh li gj gh gi lj lk bd b be z dk translated">获取张量qf1_next，qf2_next，<code class="fe nk nl nm nn b"><strong class="ak">min_qf_next </strong></code>和next_q_value</p></figure><h1 id="2b7f" class="ll lm iq bd ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi bi translated">结论</h1><p id="a990" class="pw-post-body-paragraph jw jx iq jy b jz mj kb kc kd mk kf kg kh ml kj kk kl mm kn ko kp mn kr ks kt ij bi translated">J.W. Gibbs使用了热力学中的熵概念，约翰·冯·诺依曼将经典的Gibbs熵扩展到量子力学领域来表征纠缠的熵。根据约翰·冯·诺依曼的建议，克劳德·香农将这种类似于统计力学中使用的缺失信息实体命名为熵。这样信息论就诞生了。很明显，熵的概念，作为一个可以通过系统的单个部分来描述系统整体特征的概念，在各个知识领域都非常有用。今天，我们看到熵概念在与深度学习和人工智能相关的系统中非常有用。我们可能很快就会看到其他概念和定律，比如熵将物理学和人工智能联系起来。</p><figure class="kw kx ky kz gt la gh gi paragraph-image"><div class="ab gu cl ns"><img src="../Images/976635b93ae270b4cbfb978927ecb98f.png" data-original-src="https://miro.medium.com/v2/format:webp/1*UiOwLz-BXv2XUSyJzTyxVQ.png"/></div></figure><h1 id="d07e" class="ll lm iq bd ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi bi translated">App 1。软影评人的一些特征</h1><p id="e4a5" class="pw-post-body-paragraph jw jx iq jy b jz mj kb kc kd mk kf kg kh ml kj kk kl mm kn ko kp mn kr ks kt ij bi translated"><strong class="jy ja">不符合政策</strong></p><p id="5762" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">SAC是一个<em class="ku">非策略</em>算法。这意味着，SAC算法允许我们重用已经收集的数据。</p><pre class="kw kx ky kz gt nt nn nu nv aw nw bi"><span id="3011" class="nx lm iq nn b gy ny nz l oa ob">memory<!-- --> = <em class="ku">ReplayMemory</em>(replay_size)<br/># Sample a batch from memory, _batch_size_ = 256<br/>state_batch, action_batch, reward_batch, next_state_batch, mask_batch = memory.sample(batch_size=batch_size)</span></pre><p id="ff39" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">至于<em class="ku"> on-policy </em>学习算法，如<a class="ae mw" href="https://arxiv.org/abs/1707.06347" rel="noopener ugc nofollow" target="_blank">近似策略优化</a> (PPO)和<a class="ae mw" href="https://arxiv.org/abs/1702.08165" rel="noopener ugc nofollow" target="_blank">软Q学习</a> (SQL)，这些算法的样本复杂度都很低。这些算法需要为每个梯度步骤收集新的样本，这变得非常昂贵。</p><p id="ab0e" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated"><strong class="jy ja">随机演员-评论家训练</strong></p><p id="c135" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">非策略算法<a class="ae mw" href="https://arxiv.org/abs/1509.02971" rel="noopener ugc nofollow" target="_blank"> DDPG </a>(深度确定性策略梯度)可以被视为一种<em class="ku">确定性行动者-批评家</em>算法和一种<em class="ku">近似</em>Q<em class="ku">-学习</em>算法。然而，这两种算法之间的相互作用使得DDPG容易受到超参数设置的影响。SAC避免了基于软Q学习的先验非策略最大熵算法中与近似推理相关的潜在不稳定性。相反，这个SAC将政策外的行动者-批评家训练与一个<strong class="jy ja">随机行动者</strong>相结合，并进一步以<br/> <strong class="jy ja">最大化这个行动者</strong>的熵为目标<strong class="jy ja">熵最大化<br/></strong>。</p><p id="0e1f" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated"><strong class="jy ja">双Q绝招</strong></p><p id="553c" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">软演员评论家不是TD3的直接继承者(大约同时出版)，但它结合了<em class="ku">剪辑的双Q技巧</em>:</p><pre class="kw kx ky kz gt nt nn nu nv aw nw bi"><span id="1817" class="nx lm iq nn b gy ny nz l oa ob">    qf1, qf2 = self.critic(state_batch, action_batch) <br/>    qf1_loss = F.mse_loss(qf1, next_q_value) <br/>    qf2_loss = F.mse_loss(qf2, next_q_value) </span><span id="d659" class="nx lm iq nn b gy oc nz l oa ob">    pi, log_pi, _ = self.policy.sample(state_batch)</span><span id="7539" class="nx lm iq nn b gy oc nz l oa ob">    qf1_pi, qf2_pi = self.critic(state_batch, pi)<br/>    min_qf_pi = torch.min(qf1_pi, qf2_pi)</span><span id="2258" class="nx lm iq nn b gy oc nz l oa ob">    policy_loss = ((self.alpha * log_pi) - min_qf_pi).mean()</span></pre><p id="af01" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">在政策改进步骤中，两个Q函数用于<strong class="jy ja">减轻正偏差</strong>。</p><p id="378b" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated"><strong class="jy ja">最大熵与熵正则化</strong></p><p id="8bfa" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">在<a class="ae mw" href="https://arxiv.org/abs/1707.06347" rel="noopener ugc nofollow" target="_blank"> PPO算法</a>中，一个<em class="ku">熵正则项</em>被添加到目标函数中，以保证<em class="ku">充分探索:</em></p><figure class="kw kx ky kz gt la gh gi paragraph-image"><div class="gh gi od"><img src="../Images/885b2a4926ba337af2f076ffdf00065c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1322/format:webp/1*nDwPj5kwoodKOB_fIbNf9g.png"/></div><p class="lh li gj gh gi lj lk bd b be z dk translated"><strong class="bd mp">添加<em class="na">熵加成</em> ( </strong>图片由作者提供)</p></figure><p id="4bb5" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">SAC中最大熵的<em class="ku">原理提供的探索允许代理发现比使用<em class="ku">熵正则项</em>的其他算法(如PPO)更快更好的策略。</em></p><p id="057b" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated"><strong class="jy ja"> App 2。Python代码:</strong> <strong class="jy ja">熵曲线生成</strong></p><figure class="kw kx ky kz gt la"><div class="bz fp l di"><div class="no np l"/></div></figure><p id="8d63" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated"><a class="ae mw" rel="noopener" target="_blank" href="/entropy-in-soft-actor-critic-part-2-59821bdd5671">软优评中的熵(下)</a></p><p id="f713" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated"><strong class="jy ja">参考文献</strong></p><p id="7693" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">[1] <a class="ae mw" href="https://arxiv.org/abs/1801.01290" rel="noopener ugc nofollow" target="_blank">软行动者-批评家:带随机行动者的离策最大熵深度强化学习</a>，2018，arXiv</p><p id="31aa" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">[2] <a class="ae mw" href="https://arxiv.org/abs/1812.05905" rel="noopener ugc nofollow" target="_blank">软演员-评论家算法及应用</a> s，2019，arXiv</p><p id="cf7e" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">[3] <a class="ae mw" href="https://arxiv.org/abs/1702.08165" rel="noopener ugc nofollow" target="_blank">基于深度能量策略的强化学习</a>，2017，arXiv</p><p id="e538" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">[4] E.T .杰恩斯，信息理论和统计力学。一、二(1957年)</p><p id="179a" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">[5] S .库尔贝克，致编辑的信:库尔贝克-莱布勒距离，1987年</p><p id="4d7c" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">[6] J.W.Gibbs，流体热力学中的图解法，2011年检索，维基资源</p><p id="5377" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">[7] <a class="ae mw" href="https://spinningup.openai.com/en/latest/algorithms/sac.html" rel="noopener ugc nofollow" target="_blank">软演员-评论家，</a> 2018，OpenAI，Spinning Up</p><p id="e640" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">[8] <a class="ae mw" rel="noopener" target="_blank" href="/soft-actor-critic-demystified-b8427df61665">软演员-评论家揭秘</a>，2019，TDS</p><p id="77e9" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">[9] <a class="ae mw" rel="noopener" target="_blank" href="/three-aspects-of-deep-rl-noise-overestimation-and-exploration-122ffb4bb92b">深度RL的三个方面:噪声、高估和勘探</a>，2020，TDS</p><p id="0a97" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">[10] <a class="ae mw" rel="noopener" target="_blank" href="/a-pair-of-interrelated-neural-networks-in-dqn-f0f58e09b3c4">深度Q网络中一对相互关联的神经网络</a>，2020，TDS</p><p id="6bcf" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">【Bellman方程在Deep RL中是如何工作的？，2020年，全港发展策略</p><p id="4be6" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">[12] <a class="ae mw" href="https://lilianweng.github.io/lil-log/2020/06/07/exploration-strategies-in-deep-reinforcement-learning.html" rel="noopener ugc nofollow" target="_blank">深度强化学习中的探索策略</a>，2020，<a class="ae mw" href="https://lilianweng.github.io/lil-log/2020/06/07/exploration-strategies-in-deep-reinforcement-learning.html" rel="noopener ugc nofollow" target="_blank"> github.io </a></p><p id="7e43" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">[13]<a class="ae mw" href="https://github.com/Rafael1s/Deep-Reinforcement-Learning-Algorithms/blob/master/HopperBulletEnv-v0-SAC/sac_agent.py" rel="noopener ugc nofollow" target="_blank">Project-HopperBulletEnv with Soft演员-评论家(SAC) </a>，2020，github</p><p id="5639" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">[14] <a class="ae mw" href="https://arxiv.org/abs/1707.06347" rel="noopener ugc nofollow" target="_blank">近似策略优化算法</a>，2017，arXiv</p><p id="e8b6" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">[15] <a class="ae mw" href="https://arxiv.org/abs/1509.02971" rel="noopener ugc nofollow" target="_blank">深度强化学习的连续控制</a>，v6，2015，arXiv</p><p id="1515" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">[16] <a class="ae mw" rel="noopener" target="_blank" href="/entropy-in-soft-actor-critic-part-2-59821bdd5671">熵在软优评(下)</a>，2021</p></div></div>    
</body>
</html>