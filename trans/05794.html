<html>
<head>
<title>Facial Keypoints Detection: Image and Keypoints Augmentation</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">面部关键点检测:图像和关键点增强</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/facial-keypoints-detection-image-and-keypoints-augmentation-6c2ea824a59?source=collection_archive---------10-----------------------#2021-05-24">https://towardsdatascience.com/facial-keypoints-detection-image-and-keypoints-augmentation-6c2ea824a59?source=collection_archive---------10-----------------------#2021-05-24</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="f2e6" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">使用Imgaug库进行增强</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/53afc304311f41f8285355d4e4919a07.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Sjn1i4MXE6yHjQIDtC_33Q.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">日落！图片作者<a class="ae ky" href="https://www.flickr.com/photos/suvob/48831928047/in/dateposted-public/" rel="noopener ugc nofollow" target="_blank">作者</a></p></figure><p id="5f65" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">由于深度神经网络(DNN)具有大量的可学习参数，因此需要大量的标记数据，以便这些参数能够很好地推广到我们手头的任务。数据扩充是一种非常常见的技术，用于增加标记训练数据的大小和多样性。在深度学习专业化课程中，吴恩达提到，与其他领域不同，在计算机视觉中，拥有更多数据几乎总是更好。图像增强也已经成为一种常见的隐式正则化技术，以解决DNNs中的过拟合问题。通常在图像增强中，我们使用翻转、旋转、缩放等的组合。但是在关键点检测任务中，我们还需要随着图像增加关键点。所以这篇文章是关于如何在关键点检测任务中增加训练数据的简单描述。链接到完整的笔记本包括培训DNN在文章的结尾。让我们开始吧—</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h2 id="be38" class="mc md it bd me mf mg dn mh mi mj dp mk li ml mm mn lm mo mp mq lq mr ms mt mu bi translated">面部关键点数据:</h2><p id="6088" class="pw-post-body-paragraph kz la it lb b lc mv ju le lf mw jx lh li mx lk ll lm my lo lp lq mz ls lt lu im bi translated">我们将使用来自<a class="ae ky" href="https://www.kaggle.com/c/facial-keypoints-detection/data" rel="noopener ugc nofollow" target="_blank"> Kaggle </a>的数据集，并引用那里的话——</p><blockquote class="na nb nc"><p id="57d8" class="kz la nd lb b lc ld ju le lf lg jx lh ne lj lk ll nf ln lo lp ng lr ls lt lu im bi translated">每个预测的关键点由像素索引空间中的(x，y)实值对指定。共有15个关键点，代表面部的以下元素:</p><p id="134e" class="kz la nd lb b lc ld ju le lf lg jx lh ne lj lk ll nf ln lo lp ng lr ls lt lu im bi translated">左眼中心、右眼中心、左眼内角、左眼外角、右眼内角、右眼外角、左眼外角、左眉内端、左眉外端、右眉内端、右眉外端、右眉外端、鼻尖、嘴左角、嘴右角、嘴中上唇、嘴中下唇</p></blockquote><p id="238a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们加载数据—</p><pre class="kj kk kl km gt nh ni nj nk aw nl bi"><span id="22d8" class="mc md it ni b gy nm nn l no np">train_read = pd.read_csv(data_path + '/training.csv', sep=',')<br/>print ('training data shape; ', train_read.shape)</span><span id="25c3" class="mc md it ni b gy nq nn l no np">&gt;&gt;&gt; training data shape;  (7049, 31)</span></pre><p id="b2c1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在训练数据中有7049个图像，但是问题是在数据集中有许多空值。准确地说，在31列中，除了‘nose _ tip _ x’、‘nose _ tip _ y’和‘Image’列之外，所有列都有空值。让我们检查一下关键点的分布—</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nr"><img src="../Images/2a8d4317340b8e97850418805030d2b0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Bk6mb677CeouMjEH5QsHqw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图一。关键点的分布(图片由作者提供)</p></figure><p id="dcba" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">关键点遵循正态分布，因此，数据插补的最简单策略之一是用分布平均值替换NaN条目。<em class="nd">对于数据填补，从最大似然的角度来看，有必要先将数据拆分成训练测试，然后进行转换，否则，我们很容易导致数据泄漏。</em>然而，用Colab中的7049幅图像+增强图像来训练一个网络是非常耗时的，所以这里我决定只使用干净的数据。</p><pre class="kj kk kl km gt nh ni nj nk aw nl bi"><span id="bc53" class="mc md it ni b gy nm nn l no np">train_clean = train_read.dropna(axis=0, how=’any’, inplace=False)</span><span id="1e17" class="mc md it ni b gy nq nn l no np">train_clean = train_clean.reset_index(drop=True)</span><span id="ab3f" class="mc md it ni b gy nq nn l no np">print (‘data-frame shape with no null values: ‘, train_clean.shape)</span><span id="215d" class="mc md it ni b gy nq nn l no np">&gt;&gt;&gt; data-frame shape with no null values:  (2140, 31)</span></pre><p id="2344" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">正如我们看到的，仅使用干净的数据大大减少了训练规模(从7049幅图像到2140幅)，在这里，增强将非常方便。但是在增强之前，我们需要做更多的处理。image列包含字符串形式的像素值，字符串之间有一个空格。</p><pre class="kj kk kl km gt nh ni nj nk aw nl bi"><span id="a537" class="mc md it ni b gy nm nn l no np">clean_imgs = []</span><span id="59d8" class="mc md it ni b gy nq nn l no np">for i in range(0, len(train_clean)):</span><span id="a9d2" class="mc md it ni b gy nq nn l no np">x_c = train_clean[‘Image’][i].split(‘ ‘) # split the pixel values based on the space</span><span id="3f5d" class="mc md it ni b gy nq nn l no np">x_c = [y for y in x_c] # create the listed pixels</span><span id="6eb7" class="mc md it ni b gy nq nn l no np">clean_imgs.append(x_c)</span><span id="5c01" class="mc md it ni b gy nq nn l no np">clean_imgs_arr = np.array(clean_imgs, dtype=’float’) </span></pre><p id="3247" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">从Kaggle中的数据描述来看，图像有维度(96，96)，所以多几步—</p><pre class="kj kk kl km gt nh ni nj nk aw nl bi"><span id="2735" class="mc md it ni b gy nm nn l no np">clean_imgs_arr = np.reshape(clean_imgs_arr, (train_clean.shape[0], 96, 96, 1))</span><span id="27e5" class="mc md it ni b gy nq nn l no np">train_ims_clean = clean_imgs_arr/255. # scale the images</span></pre><p id="9a49" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们从数据框中获取关键点—</p><pre class="kj kk kl km gt nh ni nj nk aw nl bi"><span id="5346" class="mc md it ni b gy nm nn l no np">clean_keypoints_df = train_clean.drop(‘Image’, axis=1)</span><span id="44ce" class="mc md it ni b gy nq nn l no np">clean_keypoints_arr = clean_keypoints_df.to_numpy()</span></pre><p id="76a5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们定义一个函数来帮助我们一起可视化图像和关键点—</p><pre class="kj kk kl km gt nh ni nj nk aw nl bi"><span id="910c" class="mc md it ni b gy nm nn l no np">def vis_im_keypoint_notstandard(img, points, axs): </span><span id="be90" class="mc md it ni b gy nq nn l no np">  axs.imshow(img.reshape(96, 96))</span><span id="8bae" class="mc md it ni b gy nq nn l no np">  xcoords = (points[0::2] + 0.)</span><span id="ebf1" class="mc md it ni b gy nq nn l no np">  ycoords = (points[1::2] + 0.)</span><span id="8ef6" class="mc md it ni b gy nq nn l no np">  axs.scatter(xcoords, ycoords, color=’red’, marker=’o’)</span></pre><p id="a5ef" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">太好了！现在我们都准备进入扩增。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h2 id="b23e" class="mc md it bd me mf mg dn mh mi mj dp mk li ml mm mn lm mo mp mq lq mr ms mt mu bi translated">线性对比度和高斯模糊:</h2><p id="5f82" class="pw-post-body-paragraph kz la it lb b lc mv ju le lf mw jx lh li mx lk ll lm my lo lp lq mz ls lt lu im bi translated">首先，我们在关键点不受影响的地方做了一些增强。使用<a class="ae ky" href="https://imgaug.readthedocs.io/en/latest/index.html" rel="noopener ugc nofollow" target="_blank"> Imgaug </a>库，我们将在图像上添加高斯模糊和线性对比度。正如您所理解的，我们不需要单独考虑关键点，因为它们不会受到影响。让我们看看下面的代码—</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ns nt l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">添加增强:高斯模糊和噪声</p></figure><p id="0757" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><code class="fe nu nv nw ni b">LinearContrast</code>修改图像的对比度，<code class="fe nu nv nw ni b">GaussianBlur</code>增强器用于使用高斯内核模糊图像，<code class="fe nu nv nw ni b">sigma</code>是高斯内核的标准偏差。通过<code class="fe nu nv nw ni b">Sometimes</code>，模糊仅适用于所有图像中随机80%的图像。我们可以将生成的原始和增强图像可视化如下—</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nx"><img src="../Images/6c07a52f62b921bad1df0df1c0d1a125.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*31tZ7AoVn1pss82mUC6aeg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图二。通过添加来自Imgaug库的<a class="ae ky" href="https://imgaug.readthedocs.io/en/latest/source/api_augmenters_blur.html#imgaug.augmenters.blur.GaussianBlur" rel="noopener ugc nofollow" target="_blank">高斯模糊</a>和<a class="ae ky" href="https://imgaug.readthedocs.io/en/latest/source/overview/contrast.html" rel="noopener ugc nofollow" target="_blank">线性对比度</a>进行增强。(图片由作者提供)</p></figure></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h2 id="723b" class="mc md it bd me mf mg dn mh mi mj dp mk li ml mm mn lm mo mp mq lq mr ms mt mu bi translated">缩放和旋转图像和关键点:</h2><p id="0115" class="pw-post-body-paragraph kz la it lb b lc mv ju le lf mw jx lh li mx lk ll lm my lo lp lq mz ls lt lu im bi translated">为了在我们的数据集中包含旋转和缩放的图像，我们还需要相应地改变关键点。这是主要原因之一，不建议在这个问题中使用<a class="ae ky" href="https://keras.io/api/preprocessing/image/" rel="noopener ugc nofollow" target="_blank">Keras imagedata generator</a>类。在这里，Imgaug库非常方便。与其描述如何正确地完成它的步骤，我不如分享下面的代码块—</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ns nt l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">同时旋转和缩放图像和关键点。</p></figure><p id="2001" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们使用<code class="fe nu nv nw ni b">Sequential</code>定义由旋转和缩放图像组成的增强序列。旋转角度设置为15度，缩放范围设置为原始图像的80%到120%。图像和相应的关键点将以这些范围之间的任意数量随机增加。让我们想象一下结果—</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ny"><img src="../Images/1de9164ab1dba89305c6ecc0e94bad90.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dZEOMpvVEB74-oZ72qCRkg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图三。通过添加旋转和缩放来增强图像和关键点。(图片由作者提供)</p></figure></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h2 id="48d4" class="mc md it bd me mf mg dn mh mi mj dp mk li ml mm mn lm mo mp mq lq mr ms mt mu bi translated">水平翻转:图像和关键点:</h2><p id="b55f" class="pw-post-body-paragraph kz la it lb b lc mv ju le lf mw jx lh li mx lk ll lm my lo lp lq mz ls lt lu im bi translated">与之前类似，我们可以使用Imgaug库进行水平翻转，但是显式编写代码也相对容易。对于图像的水平翻转，我使用了<code class="fe nu nv nw ni b"><a class="ae ky" href="https://numpy.org/doc/stable/reference/generated/numpy.fliplr.html" rel="noopener ugc nofollow" target="_blank">numpy fliplr</a></code>；对于关键点，在水平翻转中，y坐标不会改变，但x坐标会改变。由于图像的维数是(96，96)，我们通过做(96-x)得到翻转的x点。下面是代码块—</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ns nt l"/></div></figure><p id="bd4c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">包括一切，我们现在有完整的数据集。从2140张图片开始，现在我们有17120张图片。让我们想象一下其中的一些—</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nz"><img src="../Images/dfae6786cb043c3da0a66d17c61afa53.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vRN8aK4WqsDB2H0vZo2-uw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图4。通过添加水平翻转来增强图像和关键点。(图片由作者提供)</p></figure><p id="acc8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们在扩增结束时打乱数据—</p><pre class="kj kk kl km gt nh ni nj nk aw nl bi"><span id="ed61" class="mc md it ni b gy nm nn l no np">from sklearn.utils import shuffle</span><span id="4757" class="mc md it ni b gy nq nn l no np">aug_ims_train_final, aug_points_train_final = shuffle(aug_ims_train_clean_g3, aug_points_train_clean_g3)</span></pre></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h2 id="a16e" class="mc md it bd me mf mg dn mh mi mj dp mk li ml mm mn lm mo mp mq lq mr ms mt mu bi translated">训练“类似盗梦空间”的深度神经网络:</h2><p id="6509" class="pw-post-body-paragraph kz la it lb b lc mv ju le lf mw jx lh li mx lk ll lm my lo lp lq mz ls lt lu im bi translated">扩充之后，我们现在开始建立DNN模型并准备培训。下面是我使用的神经网络结构，它是从最初的InceptionV3结构稍微简化的。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oa"><img src="../Images/36bcbafcdd7b534eca73bfc73c22c193.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ND9ykb4iIypY8vOR9zG1PQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图五。盗梦空间就像‘深度神经网络’。(图片由作者提供)</p></figure><p id="bdc7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们也添加一些<code class="fe nu nv nw ni b">‘Callbacks’ </code>如下—</p><pre class="kj kk kl km gt nh ni nj nk aw nl bi"><span id="e97d" class="mc md it ni b gy nm nn l no np">class customCallbacks(tf.keras.callbacks.Callback):<br/>  def on_epoch_end(self, epoch, logs=None):<br/>    self.epoch = epoch + 1<br/>    if self.epoch % 50 == 0:<br/>      print ('epoch num {}, train acc: {}, validation acc:          </span><span id="90dc" class="mc md it ni b gy nq nn l no np">            {}'.format(epoch, logs['mae'], logs['val_mae']))</span><span id="b5a5" class="mc md it ni b gy nq nn l no np">reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_mae', factor=0.8,<br/>                              patience=25, min_lr=1e-5, verbose=1)</span></pre><p id="bff7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">下一步是编译和训练模型—</p><pre class="kj kk kl km gt nh ni nj nk aw nl bi"><span id="0114" class="mc md it ni b gy nm nn l no np">face_key_model2_aug.compile(loss='mse', <br/>                       optimizer=Adam(learning_rate=3e-3), <br/>                       metrics=['mae'])</span><span id="1c35" class="mc md it ni b gy nq nn l no np">face_key_model2_aug_train_clean = face_key_model2_aug.fit(aug_ims_train_final, aug_points_train_final, <br/>                                                  validation_split= 0.05, <br/>                                                  batch_size=64, epochs=300, <br/>                                                  callbacks=[customCallbacks(), reduce_lr], <br/>                                          verbose=0)</span></pre><p id="1358" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">训练和验证曲线绘制如下，作为时期的函数—</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ob"><img src="../Images/a886aa68b7c351ca542e352f1e3c31bf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zSyJYKehdLPtEo6bMhyIFg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图6:左图:超过300个时期的训练和验证损失。右图:与左图相同，但平均绝对误差不同。(图片由作者提供)</p></figure><p id="7de5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">下一步是预测测试图像的关键点—</p><pre class="kj kk kl km gt nh ni nj nk aw nl bi"><span id="349a" class="mc md it ni b gy nm nn l no np">predict_points_aug2_clean = face_key_model2_aug.predict(test_ims)<br/><br/>print ('check shape of predicted points: ', predict_points_aug2_clean.shape)</span><span id="10d3" class="mc md it ni b gy nq nn l no np">&gt;&gt;&gt; check shape of predicted points:  (1783, 30)</span></pre><p id="7747" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">所以有1783张测试图像，我们可以看到下面的一些预测—</p><pre class="kj kk kl km gt nh ni nj nk aw nl bi"><span id="578b" class="mc md it ni b gy nm nn l no np">fig = plt.figure(figsize=(10, 8))<br/>npics= 12<br/>count = 1<br/>for i in range(npics):<br/>  # ipic = i<br/>  ipic = np.random.choice(test_ims.shape[0])<br/>  ax = fig.add_subplot(npics/3 , 4, count, xticks=[],yticks=[])<br/>  vis_im_keypoint_notstandard(test_ims[ipic], predict_points_aug2_clean[ipic], ax)<br/>  count = count + 1<br/><br/><br/>plt.tight_layout()<br/>plt.savefig(data_path+'/prediction_keypoints.png', dpi=200, bbox_inches = 'tight', pad_inches = 0)<br/>plt.show()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oc"><img src="../Images/54e458077cb229e9bf9ae1756658828f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YoB3MFdxMr1rQavIZDW0zA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图7。测试图像上的预测关键点。(图片由作者提供)</p></figure></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h2 id="eabc" class="mc md it bd me mf mg dn mh mi mj dp mk li ml mm mn lm mo mp mq lq mr ms mt mu bi translated">结论:</h2><p id="7d8d" class="pw-post-body-paragraph kz la it lb b lc mv ju le lf mw jx lh li mx lk ll lm my lo lp lq mz ls lt lu im bi translated">最后，我想强调的是，这篇文章更专注于数据增强部分，而不是明确地专注于在Kaggle中取得更好的成绩。一篇精彩的博文可以在<a class="ae ky" href="https://fairyonice.github.io/Achieving-top-5-in-Kaggles-facial-keypoints-detection-using-FCN.html" rel="noopener ugc nofollow" target="_blank">这里</a>找到，它讨论了如何在Kaggle中获得高分的一步一步的方法。由于我们专注于预测图像中的所有关键点，因此有一些帖子介绍了如何针对人脸上的单个关键点训练网络，并将它们组合起来以获得更好的结果。我希望这篇文章能帮助你开始解决这类问题，然后根据资源和时间，你可以尝试各种东西。</p><p id="4500" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">保持坚强，干杯！！</p><p id="7f43" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">[1] <a class="ae ky" href="https://github.com/suvoooo/Learn-TensorFlow/blob/master/Facial_Keypoint_Kaggle.ipynb" rel="noopener ugc nofollow" target="_blank"> GitHub链接</a>获取完整笔记本。</p></div></div>    
</body>
</html>