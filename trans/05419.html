<html>
<head>
<title>Understanding Positional Encoding in Transformers</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">了解变压器中的位置编码</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/understanding-positional-encoding-in-transformers-dc6bafc021ab?source=collection_archive---------18-----------------------#2021-05-13">https://towardsdatascience.com/understanding-positional-encoding-in-transformers-dc6bafc021ab?source=collection_archive---------18-----------------------#2021-05-13</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="1229" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph">理解ML</h2><div class=""/><div class=""><h2 id="10a9" class="pw-subtitle-paragraph jw iz iq bd b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn dk translated">变压器模型位置编码方法的可视化。</h2></div><h1 id="06b2" class="ko kp iq bd kq kr ks kt ku kv kw kx ky kf kz kg la ki lb kj lc kl ld km le lf bi translated">什么是位置编码？</h1><p id="6e4a" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated">正如我在<a class="ae mc" href="https://erdem.pl/2021/05/introduction-to-attention-mechanism" rel="noopener ugc nofollow" target="_blank">“注意力机制介绍”</a>中解释的，注意力并不关心输入的位置。为了解决这个问题，我们必须引入一种叫做<strong class="li ja">的位置编码</strong>。这种编码在最初的<a class="ae mc" href="https://arxiv.org/abs/1706.03762" rel="noopener ugc nofollow" target="_blank">“注意力是你所需要的全部”</a>论文中有所涉及，它被添加到每个输入中(不是连接而是添加)。</p><figure class="me mf mg mh gt mi gh gi paragraph-image"><div class="gh gi md"><img src="../Images/10ddec93cf96b1dfba9efe522a7adf54.png" data-original-src="https://miro.medium.com/v2/resize:fit:1260/format:webp/0*QfAmBHAmcTjvO8Ac.png"/></div><p class="ml mm gj gh gi mn mo bd b be z dk translated"><em class="mp">图1:原变压器架构，来源</em> <a class="ae mc" href="https://arxiv.org/abs/1706.03762" rel="noopener ugc nofollow" target="_blank"> <em class="mp">《注意力就是你需要的一切》，2017 </em> </a></p></figure><p id="f25e" class="pw-post-body-paragraph lg lh iq li b lj mq ka ll lm mr kd lo lp ms lr ls lt mt lv lw lx mu lz ma mb ij bi translated">这篇论文只考虑了固定的(不可训练的)位置编码，这就是我要解释的。现在编码是和模型一起训练的，但是这需要另一篇文章。为了计算位置编码的值，我们必须转到本文的第<strong class="li ja"> 3.5节</strong>。作者使用<strong class="li ja"> sin </strong>和<strong class="li ja"> cos </strong>函数来计算每个输入向量的值。</p><figure class="me mf mg mh gt mi gh gi paragraph-image"><div class="gh gi mv"><img src="../Images/c7f1cfb84af7f413992ce8a5509b9805.png" data-original-src="https://miro.medium.com/v2/resize:fit:1044/format:webp/1*fX8TN02pB5G1pLNsJIC6QA.png"/></div></figure><p id="70b9" class="pw-post-body-paragraph lg lh iq li b lj mq ka ll lm mr kd lo lp ms lr ls lt mt lv lw lx mu lz ma mb ij bi translated">如你所见，这些值取决于<em class="mw"> d_{model} </em>(输入尺寸)和i <em class="mw"> i </em>(位置向量的索引)。原始论文对512维向量进行操作，但为了简单起见，我将使用<em class="mw"> d_{model} = 50 </em>或<em class="mw"> d_{model} = 20 </em>。作者还附上了他们为什么选择这种函数的注释:</p><blockquote class="mx my mz"><p id="c734" class="lg lh mw li b lj mq ka ll lm mr kd lo na ms lr ls nb mt lv lw nc mu lz ma mb ij bi translated">我们选择这个函数，因为我们假设它将允许模型容易地学习通过相对位置来参与，因为对于任何固定的偏移k，PE_{pos+k}可以表示为<em class="iq"> PE_{pos} </em>的线性函数。</p></blockquote><h1 id="3b5c" class="ko kp iq bd kq kr ks kt ku kv kw kx ky kf kz kg la ki lb kj lc kl ld km le lf bi translated">位置编码可视化</h1><figure class="me mf mg mh gt mi gh gi paragraph-image"><div role="button" tabindex="0" class="ne nf di ng bf nh"><div class="gh gi nd"><img src="../Images/b5c07b9d5008200778f880db7cf3d302.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BN7nCvub6tZb5-qCUwM9VA.png"/></div></div><p class="ml mm gj gh gi mn mo bd b be z dk translated">图2:位置编码可视化，设计基于<a class="ae mc" href="https://arxiv.org/abs/1706.03762" rel="noopener ugc nofollow" target="_blank"><em class="mp"/>“注意力是你所需要的全部”，NeurIPS 2017 </a>论文</p></figure><blockquote class="mx my mz"><p id="f93f" class="lg lh mw li b lj mq ka ll lm mr kd lo na ms lr ls nb mt lv lw nc mu lz ma mb ij bi translated"><strong class="li ja">注意</strong> <br/>这只是原图中的一个屏幕，不幸的是我不能在介质上包含部分应用程序。如果你想玩，请直接进入<a class="ae mc" href="https://erdem.pl/2021/05/understanding-positional-encoding-in-transformers#positional-encoding-visualization" rel="noopener ugc nofollow" target="_blank">https://erdem . pl/2021/05/understanding-positional-encoding-in-transformers # positional-encoding-visualization</a></p></blockquote><h2 id="73f9" class="ni kp iq bd kq nj nk dn ku nl nm dp ky lp nn no la lt np nq lc lx nr ns le iw bi translated">价值观念</h2><p id="c3f3" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated">我们使用给定指数的公式计算每个指数的值。值得注意的是，<em class="mw"> cos </em>函数中的2 <em class="mw"> i </em>值是一个偶数，因此为了计算第0个和第1个索引的值，我们将使用:</p><figure class="me mf mg mh gt mi gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/497e132e6d28372af017f799c41b0e4b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1392/format:webp/1*7GEQ2a0-CFRP9LKI2htyJQ.png"/></div></figure><p id="9f44" class="pw-post-body-paragraph lg lh iq li b lj mq ka ll lm mr kd lo lp ms lr ls lt mt lv lw lx mu lz ma mb ij bi translated">这就是为什么第0个和第1个索引的值只取决于<em class="mw">位置</em>的值，而不是同时取决于<em class="mw">位置</em>和d_{model} <em class="mw"> dmodel </em>的值。这从第二个指数向前改变，因为被除数不再等于0，所以整个除数大于1。</p><h2 id="cfd8" class="ni kp iq bd kq nj nk dn ku nl nm dp ky lp nn no la lt np nq lc lx nr ns le iw bi translated">维度依赖性</h2><p id="84f9" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated">如果切换到第二步，那么可以比较一下<strong class="li ja"> <em class="mw"> PE </em> </strong>值是如何根据<em class="mw"> d_{model} </em>变化的。</p><figure class="me mf mg mh gt mi gh gi paragraph-image"><div role="button" tabindex="0" class="ne nf di ng bf nh"><div class="gh gi nu"><img src="../Images/bd114bd62767b2f17a64d96dd3486748.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*q-B7sF3ZjRFDLEet.png"/></div></div><p class="ml mm gj gh gi mn mo bd b be z dk translated"><em class="mp">图3:不同维度PE值对比(d)，来源:</em> <a class="ae mc" href="https://erdem.pl/2021/05/understanding-positional-encoding-in-transformers#positional-encoding-visualization" rel="noopener ugc nofollow" target="_blank"> <em class="mp">位置编码可视化</em> </a></p></figure><p id="5b97" class="pw-post-body-paragraph lg lh iq li b lj mq ka ll lm mr kd lo lp ms lr ls lt mt lv lw lx mu lz ma mb ij bi translated">前两个指标的周期不随<em class="mw"> d_{model} </em>的变化而变化，但随<em class="mw"> d_{model} </em>的减小，后续指标(第2个及以上)的周期变宽。这可能是显而易见的，但仍然很好地看到了差异。</p><h2 id="77f1" class="ni kp iq bd kq nj nk dn ku nl nm dp ky lp nn no la lt np nq lc lx nr ns le iw bi translated">功能周期</h2><p id="9a6d" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated">当我们绘制前20个<em class="mw">位置</em>向量的<strong class="li ja"> <em class="mw"> PE </em> </strong>值时，我们得到如下结果:</p><figure class="me mf mg mh gt mi gh gi paragraph-image"><div role="button" tabindex="0" class="ne nf di ng bf nh"><div class="gh gi nu"><img src="../Images/273fed2128b2aefa295f3e37bbb1e6a4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*uqBAhS8qPW__9ion.png"/></div></div><p class="ml mm gj gh gi mn mo bd b be z dk translated"><em class="mp">图4:前20位的位置编码值，使用</em> <a class="ae mc" href="https://www.tensorflow.org/tutorials/text/transformer#positional_encoding" rel="noopener ugc nofollow" target="_blank"> <em class="mp"> Tensorflow —位置编码</em> </a> <em class="mp">代码</em>生成</p></figure><p id="5a27" class="pw-post-body-paragraph lg lh iq li b lj mq ka ll lm mr kd lo lp ms lr ls lt mt lv lw lx mu lz ma mb ij bi translated">这个图是从Tensorflow的教程之一生成的，你可以在谷歌的帮助下直接从他们的网站上运行它。如你所见，位置向量的较低维度具有非常短的波长(相同点之间的距离)。指数i = 6的函数的波长具有大约19 (2 * 10^{12/25}).)的波长</p><p id="7e4c" class="pw-post-body-paragraph lg lh iq li b lj mq ka ll lm mr kd lo lp ms lr ls lt mt lv lw lx mu lz ma mb ij bi translated">我们知道周期是随着<em class="mw"> i </em>的增加而增加的。当<em class="mw"> i </em>到达<em class="mw"> d_{model} </em>边时，你需要大量的<em class="mw"> pos </em>向量来覆盖整个函数周期。</p><figure class="me mf mg mh gt mi gh gi paragraph-image"><div role="button" tabindex="0" class="ne nf di ng bf nh"><div class="gh gi nu"><img src="../Images/367da5888836d7b42f53ae7983f3250a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*reYizcKzHyuVrm74.png"/></div></div><p class="ml mm gj gh gi mn mo bd b be z dk translated"><em class="mp">图5:进一步索引的函数值，来源:</em> <a class="ae mc" href="https://erdem.pl/2021/05/understanding-positional-encoding-in-transformers#positional-encoding-visualization" rel="noopener ugc nofollow" target="_blank"> <em class="mp">位置编码可视化</em> </a></p></figure><p id="8ca0" class="pw-post-body-paragraph lg lh iq li b lj mq ka ll lm mr kd lo lp ms lr ls lt mt lv lw lx mu lz ma mb ij bi translated">较高索引的前20个位置的值几乎是恒定的。你可以在图4中看到同样的事情，其中30-50列的颜色几乎没有变化。为了看到这种变化，我们必须绘制成千上万个位置的值:</p><figure class="me mf mg mh gt mi gh gi paragraph-image"><div role="button" tabindex="0" class="ne nf di ng bf nh"><div class="gh gi nu"><img src="../Images/93e021ca9f7103383c29fdbf0c038b5d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*5zJYozQ_ge5rnGmM.png"/></div></div><p class="ml mm gj gh gi mn mo bd b be z dk translated"><em class="mp">图6:使用</em> <a class="ae mc" href="https://www.tensorflow.org/tutorials/text/transformer#positional_encoding" rel="noopener ugc nofollow" target="_blank"> <em class="mp"> Tensorflow —位置编码</em> </a> <em class="mp">代码</em>生成的进一步索引的位置编码周期</p></figure><blockquote class="mx my mz"><p id="0b08" class="lg lh mw li b lj mq ka ll lm mr kd lo na ms lr ls nb mt lv lw nc mu lz ma mb ij bi translated"><strong class="li ja"> <em class="iq">警告</em> </strong> <em class="iq">这个图有一个内置的错觉，它实际上不是错觉，但因为它试图在670px(高度)上打印40k+值，所以它不能显示任何波长小于1px的值。这就是为什么第24列之前的任何内容在视觉上都是不正确的，即使使用了正确的值来生成该图。</em></p></blockquote><h1 id="ca1c" class="ko kp iq bd kq kr ks kt ku kv kw kx ky kf kz kg la ki lb kj lc kl ld km le lf bi translated">结论</h1><p id="3e61" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated">位置嵌入为转换器提供了关于输入向量位置的知识。它们被添加(而不是连接)到相应的输入向量。编码取决于三个值:</p><ul class=""><li id="8674" class="nv nw iq li b lj mq lm mr lp nx lt ny lx nz mb oa ob oc od bi translated"><em class="mw">位置</em> —矢量的位置</li><li id="cbc6" class="nv nw iq li b lj oe lm of lp og lt oh lx oi mb oa ob oc od bi translated"><em class="mw"> i </em> —向量内的索引</li><li id="3468" class="nv nw iq li b lj oe lm of lp og lt oh lx oi mb oa ob oc od bi translated">d_{model} —输入的维度</li></ul><p id="b70e" class="pw-post-body-paragraph lg lh iq li b lj mq ka ll lm mr kd lo lp ms lr ls lt mt lv lw lx mu lz ma mb ij bi translated">在周期函数(和)的帮助下交替计算值，这些函数的波长随着输入向量维数的增加而增加。当更远的索引需要很多位置来改变一个值(大周期)时，更靠近向量顶部的索引值(较低的索引)变化很快。</p><p id="298a" class="pw-post-body-paragraph lg lh iq li b lj mq ka ll lm mr kd lo lp ms lr ls lt mt lv lw lx mu lz ma mb ij bi translated">这只是进行位置编码的一种方式。当前的SOTA模型具有与模型一起训练的编码器，而不是使用预定义的函数。作者甚至在论文中提到了这个选项，但没有注意到结果的不同:</p><blockquote class="mx my mz"><p id="80c5" class="lg lh mw li b lj mq ka ll lm mr kd lo na ms lr ls nb mt lv lw nc mu lz ma mb ij bi translated"><em class="iq">我们还尝试使用习得的位置嵌入，发现两个版本产生了几乎相同的结果(见表3行(E))。我们选择正弦版本，因为它可以允许模型外推至比训练期间遇到的序列长度更长的序列长度。</em></p></blockquote><h1 id="edaf" class="ko kp iq bd kq kr ks kt ku kv kw kx ky kf kz kg la ki lb kj lc kl ld km le lf bi translated">参考资料:</h1><ul class=""><li id="6e4c" class="nv nw iq li b lj lk lm ln lp oj lt ok lx ol mb oa ob oc od bi translated">阿希什·瓦斯瓦尼等人，“注意力是你所需要的一切”，neur IPS 2017<a class="ae mc" href="https://arxiv.org/abs/1706.03762" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1706.03762</a></li></ul></div><div class="ab cl om on hu oo" role="separator"><span class="op bw bk oq or os"/><span class="op bw bk oq or os"/><span class="op bw bk oq or"/></div><div class="ij ik il im in"><p id="07b7" class="pw-post-body-paragraph lg lh iq li b lj mq ka ll lm mr kd lo lp ms lr ls lt mt lv lw lx mu lz ma mb ij bi translated"><em class="mw">原载于</em><a class="ae mc" href="https://erdem.pl/2021/05/understanding-positional-encoding-in-transformers" rel="noopener ugc nofollow" target="_blank"><em class="mw">https://erdem . pl</em></a><em class="mw">。</em></p></div></div>    
</body>
</html>