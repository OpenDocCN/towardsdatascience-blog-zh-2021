<html>
<head>
<title>Role of Data in Implicit Regularization</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">数据在隐式正则化中的作用</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/role-of-data-in-implicit-regularization-a37cdd6cce2e?source=collection_archive---------40-----------------------#2021-05-24">https://towardsdatascience.com/role-of-data-in-implicit-regularization-a37cdd6cce2e?source=collection_archive---------40-----------------------#2021-05-24</a></blockquote><div><div class="fc ii ij ik il im"/><div class="in io ip iq ir"><h2 id="9e1e" class="is it iu bd b dl iv iw ix iy iz ja dk jb translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsdatascience.com/tagged/hands-on-tutorials" rel="noopener" target="_blank">实践教程</a></h2><div class=""/><div class=""><h2 id="7838" class="pw-subtitle-paragraph ka jd iu bd b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr dk translated">使用勒让德多项式作为特征可以显著提高收敛性</h2></div><figure class="kt ku kv kw gu kx gi gj paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="gi gj ks"><img src="../Images/b09ee8f29182f2a89f148e5758872284.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*2vS6NJLQ10rMFyZ3"/></div></div><p class="le lf gk gi gj lg lh bd b be z dk translated">照片由<a class="ae li" href="https://unsplash.com/@dizzyd718?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">德鲁·格拉汉姆</a>在<a class="ae li" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><h2 id="dae4" class="lj lk iu bd ll lm ln dn lo lp lq dp lr ls lt lu lv lw lx ly lz ma mb mc md ja bi translated">TL；速度三角形定位法(dead reckoning)</h2><p id="8c0a" class="pw-post-body-paragraph me mf iu mg b mh mi ke mj mk ml kh mm ls mn mo mp lw mq mr ms ma mt mu mv mw in bi translated">在本文中，我们重温了基于梯度下降的优化算法的一个明显的<em class="mx">隐式正则化</em>效应。我们认为这是教科书上的过度拟合的例子，你可以在scikit-learn网站上找到。我们证明，当我们使用<em class="mx">先验</em>不相关多项式作为新特征时，隐式正则化效应消失。</p><h2 id="6fac" class="lj lk iu bd ll lm ln dn lo lp lq dp lr ls lt lu lv lw lx ly lz ma mb mc md ja bi translated">过度拟合和正则化</h2><p id="be1c" class="pw-post-body-paragraph me mf iu mg b mh mi ke mj mk ml kh mm ls mn mo mp lw mq mr ms ma mt mu mv mw in bi translated">在机器学习中，<em class="mx">过度拟合</em>是指一个经过训练的模型非常适合数据，但对看不见的数据不能很好地概括。一个相关的概念是<em class="mx">不适定问题</em>，当训练数据的微小变化导致模型参数的巨大变化时，不适定问题导致模型不稳定【1】。这种情况会阻止模型对来自与训练数据相同分布的数据进行归纳。这个问题可以通过<em class="mx">正则化</em>来解决，这是一组帮助创建模型的技术，不仅适用于训练数据，也适用于相同分布的未知数据。但是，请注意，一般来说，正则化并不能保证泛化。</p><p id="ed6e" class="pw-post-body-paragraph me mf iu mg b mh my ke mj mk mz kh mm ls na mo mp lw nb mr ms ma nc mu mv mw in bi translated">文章中给出了一个过拟合的好例子:</p><div class="nd ne gq gs nf ng"><a href="https://scikit-learn.org/stable/auto_examples/model_selection/plot_underfitting_overfitting.html" rel="noopener  ugc nofollow" target="_blank"><div class="nh ab fp"><div class="ni ab nj cl cj nk"><h2 class="bd je gz z fq nl fs ft nm fv fx jd bi translated">欠拟合与过拟合-sci kit-了解0.24.2文档</h2><div class="nn l"><h3 class="bd b gz z fq nl fs ft nm fv fx dk translated">这个例子演示了欠拟合和过拟合的问题，以及我们如何使用线性回归…</h3></div><div class="no l"><p class="bd b dl z fq nl fs ft nm fv fx dk translated">scikit-learn.org</p></div></div><div class="np l"><div class="nq l nr ns nt np nu lc ng"/></div></div></a></div><p id="703e" class="pw-post-body-paragraph me mf iu mg b mh my ke mj mk mz kh mm ls na mo mp lw nb mr ms ma nc mu mv mw in bi translated">这是一个回归问题，用一个简单的函数来描述基本事实</p><figure class="kt ku kv kw gu kx"><div class="bz fq l di"><div class="nv nw l"/></div></figure><p id="d8b4" class="pw-post-body-paragraph me mf iu mg b mh my ke mj mk mz kh mm ls na mo mp lw nb mr ms ma nc mu mv mw in bi translated">添加了高斯噪声。我们正试图用不同次数的多项式来逼近它。我们观察到，对于较低次数的多项式，该模型不太适合数据。这也被称为<em class="mx">欠配合</em>。相反，如果我们使用高次多项式来近似函数，则模型很好地拟合数据，但是曲线并不平滑，并且与我们试图学习的地面真实函数非常不同。下面是不同程度参数值的三个图。</p><figure class="kt ku kv kw gu kx gi gj paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="gi gj nx"><img src="../Images/1dea7a4910d36e39001642e9bc1fcc0e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pXM2JRYAwh8g--gHBnf6jg.png"/></div></div></figure><p id="2e3c" class="pw-post-body-paragraph me mf iu mg b mh my ke mj mk mz kh mm ls na mo mp lw nb mr ms ma nc mu mv mw in bi translated">在本例中，我们看到一次多项式对数据拟合不足，15次多项式对数据拟合过度，而4次多项式似乎效果最好。</p><h2 id="7daa" class="lj lk iu bd ll lm ln dn lo lp lq dp lr ls lt lu lv lw lx ly lz ma mb mc md ja bi translated">隐式正则化</h2><p id="4b3d" class="pw-post-body-paragraph me mf iu mg b mh mi ke mj mk ml kh mm ls mn mo mp lw mq mr ms ma mt mu mv mw in bi translated">然而，请注意，在这个例子中，我们使用线性回归的封闭形式的解决方案。如果我们决定通过改变<code class="fe ny nz oa ob b">solver </code>参数采用基于梯度下降的方法:</p><figure class="kt ku kv kw gu kx"><div class="bz fq l di"><div class="oc nw l"/></div></figure><p id="3ba6" class="pw-post-body-paragraph me mf iu mg b mh my ke mj mk mz kh mm ls na mo mp lw nb mr ms ma nc mu mv mw in bi translated">对于15次多项式，我们得到一个非常不同的图像:</p><figure class="kt ku kv kw gu kx gi gj paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="gi gj od"><img src="../Images/78fcf5fe59ec64b11b69c0b324b75ec7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rnCMnB3YcLwdCk_zk2GHlQ.png"/></div></div></figure><p id="3c05" class="pw-post-body-paragraph me mf iu mg b mh my ke mj mk mz kh mm ls na mo mp lw nb mr ms ma nc mu mv mw in bi translated">在这个图中，我们尝试了四个不同的精度值(这决定了优化算法进行多少次迭代)。我们可以看到，几乎所有的预测曲线都非常接近真实函数，即使我们使用了更高次的多项式。诸如此类的例子让一些人(包括本文作者)相信基于梯度下降的优化过程具有正则化效果。隐式正则化是一个活跃的研究领域，其对泛化的影响尚未完全确定[2]。</p><p id="5868" class="pw-post-body-paragraph me mf iu mg b mh my ke mj mk mz kh mm ls na mo mp lw nb mr ms ma nc mu mv mw in bi translated">然而，如果我们从另一个角度来看待这一现象，我们可以将我们的观察视为<strong class="mg je">优化算法未能达到全局最小值</strong>。这是学习算法的问题(或特征)吗？不一定。原因可能在数据上。为了证明这一点，让我们考虑特征之间的相关性:</p><figure class="kt ku kv kw gu kx"><div class="bz fq l di"><div class="oc nw l"/></div></figure><figure class="kt ku kv kw gu kx gi gj paragraph-image"><div class="gi gj oe"><img src="../Images/a64a06e13fe5b5f148f31d9f0dd2ee84.png" data-original-src="https://miro.medium.com/v2/resize:fit:1296/format:webp/1*9cFVUHcmMxbnKp1e7Vjqpg.png"/></div></figure><p id="d294" class="pw-post-body-paragraph me mf iu mg b mh my ke mj mk mz kh mm ls na mo mp lw nb mr ms ma nc mu mv mw in bi translated">我们看到生成的特征高度相关。为了去相关特征，许多人使用像PCA这样的方法。然而，我们更喜欢先验不相关的特征<em class="mx">。</em>我们将使用勒让德多项式，而不是使用x的幂作为附加特征。</p><h2 id="5aef" class="lj lk iu bd ll lm ln dn lo lp lq dp lr ls lt lu lv lw lx ly lz ma mb mc md ja bi translated">构建勒让德多项式</h2><p id="0590" class="pw-post-body-paragraph me mf iu mg b mh mi ke mj mk ml kh mm ls mn mo mp lw mq mr ms ma mt mu mv mw in bi translated">为简单起见，考虑区间[-1，1]中定义的函数。我们将使用与所有低次多项式不相关的多项式。设f(x)是自变量x的先验分布的概率密度函数，那么两个函数a(x)和b(x)是不相关的，如果:</p><figure class="kt ku kv kw gu kx"><div class="bz fq l di"><div class="of nw l"/></div></figure><p id="36ac" class="pw-post-body-paragraph me mf iu mg b mh my ke mj mk mz kh mm ls na mo mp lw nb mr ms ma nc mu mv mw in bi translated">因此，我们的计划是选择与分布f(x)不相关的多项式:</p><figure class="kt ku kv kw gu kx"><div class="bz fq l di"><div class="og nw l"/></div></figure><p id="4865" class="pw-post-body-paragraph me mf iu mg b mh my ke mj mk mz kh mm ls na mo mp lw nb mr ms ma nc mu mv mw in bi translated">为简单起见，假设独立变量x的均匀先验分布。在贝叶斯术语中，它被称为<em class="mx">无信息先验</em>，通常在我们对变量没有任何先验信念时使用。那么等式就变得简单了:</p><figure class="kt ku kv kw gu kx"><div class="bz fq l di"><div class="oh nw l"/></div></figure><p id="8d30" class="pw-post-body-paragraph me mf iu mg b mh my ke mj mk mz kh mm ls na mo mp lw nb mr ms ma nc mu mv mw in bi translated">我们将从零阶多项式开始，通过选择与所有其他低阶多项式正交的多项式来逐步提高。无需实际计算，我可以给你前10个多项式的结果:</p><figure class="kt ku kv kw gu kx gi gj paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="gi gj oi"><img src="../Images/a28460bc23cca9e38fffaba803df5577.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_cpbYIDyuglVQ_LlZcNqGg.png"/></div></div><p class="le lf gk gi gj lg lh bd b be z dk translated">来源:<a class="ae li" href="https://en.wikipedia.org/wiki/Legendre_polynomials" rel="noopener ugc nofollow" target="_blank">https://en.wikipedia.org/wiki/Legendre_polynomials</a></p></figure><p id="8124" class="pw-post-body-paragraph me mf iu mg b mh my ke mj mk mz kh mm ls na mo mp lw nb mr ms ma nc mu mv mw in bi translated">这些多项式被称为勒让德多项式，用于数学和物理的许多领域。以下是前六次勒让德多项式的曲线图，供您参考:</p><figure class="kt ku kv kw gu kx gi gj paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="gi gj oj"><img src="../Images/042ce75a7d3ae05bc70634bdee24a711.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*REAqP0LmGn0cGyoF_VjXPQ.png"/></div></div><p class="le lf gk gi gj lg lh bd b be z dk translated">来源:<a class="ae li" href="https://en.wikipedia.org/wiki/Legendre_polynomials" rel="noopener ugc nofollow" target="_blank">https://en.wikipedia.org/wiki/Legendre_polynomials</a></p></figure><p id="2a4a" class="pw-post-body-paragraph me mf iu mg b mh my ke mj mk mz kh mm ls na mo mp lw nb mr ms ma nc mu mv mw in bi translated">然而，请注意，这不是多项式的归一化形式，因为按照惯例，P_n(1) = 1。</p><p id="b600" class="pw-post-body-paragraph me mf iu mg b mh my ke mj mk mz kh mm ls na mo mp lw nb mr ms ma nc mu mv mw in bi translated">在<code class="fe ny nz oa ob b">numpy</code>和<code class="fe ny nz oa ob b">scipy</code> Python库中都有一种计算勒让德多项式的有效方法。在我们的示例中，我们将添加基于勒让德多项式的特征，并将解决我们的回归问题:</p><figure class="kt ku kv kw gu kx"><div class="bz fq l di"><div class="oc nw l"/></div></figure><p id="0482" class="pw-post-body-paragraph me mf iu mg b mh my ke mj mk mz kh mm ls na mo mp lw nb mr ms ma nc mu mv mw in bi translated">在上面的代码中，我们将输入转换到区间[-1，1]中，然后基于直到指定次数的所有次数的勒让德多项式创建要素。结果真的很惊人:</p><figure class="kt ku kv kw gu kx gi gj paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="gi gj ok"><img src="../Images/ee93345ec37c990a4cb951d6aef19364.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1yOu8nhpwUNiJxVjvyiWYw.png"/></div></div></figure><p id="9030" class="pw-post-body-paragraph me mf iu mg b mh my ke mj mk mz kh mm ls na mo mp lw nb mr ms ma nc mu mv mw in bi translated">仅用13次迭代，该模型就能达到全局最小值并匹配封闭形式的解。</p><p id="752d" class="pw-post-body-paragraph me mf iu mg b mh my ke mj mk mz kh mm ls na mo mp lw nb mr ms ma nc mu mv mw in bi translated"><strong class="mg je">结论</strong></p><p id="7cf3" class="pw-post-body-paragraph me mf iu mg b mh my ke mj mk mz kh mm ls na mo mp lw nb mr ms ma nc mu mv mw in bi translated">在本文中，我们考虑了高度相关的特性阻止优化算法找到全局最小值的情况。我们使用先验不相关<em class="mx">的勒让德多项式构建特征，而不是使用PCA去相关。实验表明，优化算法能较快地收敛到封闭解。这表明所谓的<em class="mx">隐式正则化</em>可以是数据的特征，而不是优化算法，并且它可以随着适当的数据预处理而消失。本文的代码可以在<a class="ae li" href="https://github.com/mlarionov/machine_learning_POC/blob/master/regularization/legendre_polynomials.ipynb" rel="noopener ugc nofollow" target="_blank">我的github库</a>中找到。</em></p><h2 id="988d" class="lj lk iu bd ll lm ln dn lo lp lq dp lr ls lt lu lv lw lx ly lz ma mb mc md ja bi translated">参考</h2><p id="6a41" class="pw-post-body-paragraph me mf iu mg b mh mi ke mj mk ml kh mm ls mn mo mp lw mq mr ms ma mt mu mv mw in bi translated">[1]弗拉基米尔·瓦普尼克..<em class="mx">统计学习理论的本质</em>。:斯普林格，2000年。</p><p id="b9d1" class="pw-post-body-paragraph me mf iu mg b mh my ke mj mk mz kh mm ls na mo mp lw nb mr ms ma nc mu mv mw in bi translated">[2]阿罗拉、桑吉夫、科恩、纳达夫、胡、魏、罗、俞平。"深度矩阵分解中的隐式正则化."<a class="ae li" href="https://arxiv.org/abs/1905.13655" rel="noopener ugc nofollow" target="_blank"> arXiv:1905.13655 </a>，2019。</p></div></div>    
</body>
</html>