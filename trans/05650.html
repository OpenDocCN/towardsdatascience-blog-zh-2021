<html>
<head>
<title>Are Transformers better than CNN’s at Image Recognition?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">变形金刚在图像识别方面比CNN强吗？</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/are-transformers-better-than-cnns-at-image-recognition-ced60ccc7c8?source=collection_archive---------0-----------------------#2021-05-20">https://towardsdatascience.com/are-transformers-better-than-cnns-at-image-recognition-ced60ccc7c8?source=collection_archive---------0-----------------------#2021-05-20</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="8639" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">了解视觉转换器:用于图像识别的转换器</h2></div><p id="f65f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">论文链接—<a class="ae le" href="https://arxiv.org/pdf/2010.11929.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/2010.11929.pdf</a></p><p id="2867" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如今在自然语言处理(NLP)任务中，变换器已经成为goto架构(如BERT、GPT-3等)。另一方面，变形金刚在计算机视觉任务中的使用仍然非常有限。对于计算机视觉应用(如Xception、ResNet、EfficientNet、DenseNet、Inception等)，大多数研究人员直接使用卷积层，或者在卷积块的同时添加某些注意块。关于视觉变换器(ViT)的论文实现了一个纯变换器模型，不需要卷积块，对图像序列进行分类。该论文展示了ViT如何在各种图像识别数据集上获得比大多数最先进的CNN网络更好的结果，同时使用相当少的计算资源。</p><p id="8ab3" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">视觉变压器(ViT) </strong></p><p id="71bf" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">转换器是对数据序列(例如一组单词)进行操作的网络。这些单词集首先被标记化，然后被输入到转换器中。转换器增加了注意力(二次运算——计算每对标记化单词之间的成对内积)。字数越多，运算次数也越多)。</p><p id="d5d8" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">图像因此更难在变形金刚上训练。图像由像素组成，每个图像可以包含数千到数百万个像素。因此，在转换器中，每个像素将与图像中的每个其他像素进行成对操作。在一个大小为500*500像素的图像中，这是500^2，所以注意机制将花费(500^2)^2运算。即使使用多个GPU，这也是一项艰巨的任务。因此，对于图像，研究人员大多使用某种形式的局部注意力(像素簇)，而不是使用全局注意力。</p><p id="6f5f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">ViT的作者通过使用全局注意力来解决这个问题，但不是在整个图像上，而是在多个图像块上。因此，首先将一幅大图像分成多个小块(例如16*16像素)。这如图1所示。</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi lf"><img src="../Images/f4ba32ede6cf98e2c1c2add35717f903.png" data-original-src="https://miro.medium.com/v2/resize:fit:1056/format:webp/1*j_9SEDQ5_Bwwvm3hFijxNw.png"/></div><p class="ln lo gj gh gi lp lq bd b be z dk translated">图一。图像分成多个小块(来源:来自原始纸张的图像)</p></figure><p id="1cdc" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">然后，这些图像块被展开成一系列图像，如图2所示。这些图像序列具有位置嵌入。</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi lr"><img src="../Images/dfea4c37fa78ea4bf66ed977f3154d1e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AdCioPGeXWutVfQZcxABBA.png"/></div></div><p class="ln lo gj gh gi lp lq bd b be z dk translated">图二。展开成一系列图像的图像块(来源:来自原始文件的图像)</p></figure><p id="4bb4" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">最初，变形金刚不知道哪个补丁应该放在哪里。因此，位置嵌入有助于转换器理解每个补丁应该放在哪里。在论文中，作者使用1，2，3…n的简单编号来指定补丁的位置，如图3所示。这些不仅仅是数字，而是可以学习的向量。也就是说，不直接使用数字1，而是存在一个查找表，该查找表包含代表补片位置的每个数字的向量。因此，对于第一个补丁，从表中获取第一个向量，并与补丁一起放入转换器中。类似地，对于第二个面片，从表中获取第二个向量，并将其与第二个面片一起放入转换器中，依此类推。这如图4所示。</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi lw"><img src="../Images/4218163d1b566b5de3f555f76a2c33b9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yudIe83dNMcHE7Sbyn1Gug.png"/></div></div><p class="ln lo gj gh gi lp lq bd b be z dk translated">图3。具有位置嵌入的补丁(来源:图片来自原始论文)</p></figure><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi lx"><img src="../Images/bf2bbdf26c2dc88ad8fc9e0f77625ac0.png" data-original-src="https://miro.medium.com/v2/resize:fit:842/format:webp/1*GeJQCBCAy6sHGDpS8tvhbQ.png"/></div></div><p class="ln lo gj gh gi lp lq bd b be z dk translated">图4。作为向量表示的位置嵌入(来源:图片由作者创建)</p></figure><p id="b030" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">图像补丁是一个小图像(16*16像素)。这需要以某种方式输入，以便变压器能够理解。一种方法是将图像展开成16*16 = 256维的向量。然而，论文作者使用了线性投影。这意味着只有一个矩阵，表示为“E”(嵌入)。取一个小块，首先展开成一个线性向量。然后，这个向量与嵌入矩阵e相乘。然后，最终结果与位置嵌入一起被馈送到变换器。</p><p id="168c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">然后，所有的补片(线性投影)连同它们各自的位置嵌入被输入到一个变换编码器中。这个变压器是一个标准的变压器架构(你需要的只是注意力——纸)。</p><p id="646f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">有一个额外的可学习嵌入，标记为位置0，如图5所示。这种嵌入的输出用于最终对整个图像进行分类。</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi ly"><img src="../Images/01274bbfc5bde13221d0de047e68fe54.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*58xYTXTkcwsu2VwmYczcrg.png"/></div></div><p class="ln lo gj gh gi lp lq bd b be z dk translated">图5。整个ViT架构，带有额外的可学习嵌入——用红色标记，最左边的嵌入(来源:图片来自原始论文)</p></figure><p id="0706" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">结果</strong></p><p id="dd6c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">表1显示了ViT与最先进的CNN架构在不同数据集上的结果对比。ViT在JFT-300数据集上进行了预训练。下面的结果表明，在所有数据集上，ViT的性能都优于基于ResNet的体系结构和EfficentNet-L2体系结构(针对有噪声的学生权重进行预训练)。这两种模型都是当前最先进的CNN架构。表1中，ViT-H指的是ViT-Huge(32层)，ViT-L指的是ViT-Large(24层)。ViT-H/L后面的数字14和16表示从每个图像中创建的补丁大小(14*14或16*16)。</p><p id="53ef" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">该表还显示，ViT比其他两种CNN模型需要更少的计算资源。</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi lz"><img src="../Images/e6231673d31104c789f42669aaf0f3eb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6RVVISCaExEwnaZWeo25Hw.png"/></div></div><p class="ln lo gj gh gi lp lq bd b be z dk translated">表1。在各种图像数据集上将ViT结果与其他CNN架构进行比较(来源:原始论文中的表格)</p></figure><p id="1e81" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">图6显示了变压器在对各种图像进行分类时所给予的关注。</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi ma"><img src="../Images/237bb829d5fb932eec11736f5a9093e4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1084/format:webp/1*VmrxJx_yPAqEQ0lfFLQTQQ.png"/></div><p class="ln lo gj gh gi lp lq bd b be z dk translated">图6:从输出记号到输入空间的注意机制(来源:图片来自原纸)</p></figure><p id="decf" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">结论</strong></p><p id="f11b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu"> <em class="mb">视觉变形金刚会在计算机视觉任务中取代CNN吗？</em> </strong></p><p id="09c1" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">到目前为止，CNN已经统治了计算机视觉任务。图像基于这样一种思想，即一个像素依赖于它的相邻像素，下一个像素依赖于它的紧邻像素(无论是颜色、亮度、对比度等等)。美国有线电视新闻网的工作就是基于这个想法，在一幅图像上使用过滤器来提取重要的特征和边缘。这有助于模型仅从图像中学习必要的重要特征，而不是图像的每个像素的细节。</p><p id="68d2" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">但是，如果将整个图像数据输入到模型中，而不仅仅是过滤器可以提取的部分(或它认为重要的部分)，则模型表现更好的机会更高。这正是视觉变形金刚内部正在发生的事情。这可能是为什么在这种情况下，视觉变形金刚比大多数CNN模型更好的一个原因。</p><p id="0e63" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu"> <em class="mb">但这是否意味着未来在计算机视觉任务中，变形金刚会取代CNN的？</em>T9】</strong></p><p id="9898" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">答案是，不会这么快。就在几天前，EfficientNet V2模型发布了，它的性能甚至比视觉变形金刚还要好。这只是意味着，随着更新、更好、更高效的模型在不久的将来不断推出，我们可以期待两种类型(CNN和变形金刚)的新架构一决高下。</p></div></div>    
</body>
</html>