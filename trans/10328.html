<html>
<head>
<title>Better Quantifying the Performance of Object Detection in Video</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">更好地量化视频中对象检测的性能</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/better-quantifying-the-performance-of-object-detection-in-video-c21d5b16493c?source=collection_archive---------21-----------------------#2021-10-01">https://towardsdatascience.com/better-quantifying-the-performance-of-object-detection-in-video-c21d5b16493c?source=collection_archive---------21-----------------------#2021-10-01</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="3289" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">视频对象检测有几个独特的挑战，使其区别于静态图像中的对象检测。</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/e37f30d730064dce5e57577a957ed412.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oMQun4oLQWF6w-6XH617tw.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><a class="ae ky" href="https://unsplash.com/photos/oyXis2kALVg" rel="noopener ugc nofollow" target="_blank">来源</a></p></figure><p id="8eb2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对象检测现在是计算机视觉中最常见的应用之一，特别是随着普通分类问题已经变得更容易用现代深度学习架构来解决。由于大规模数据集(例如COCO [1])的广泛可用性，对象检测研究对于计算机视觉社区来说变得更加容易，从而导致技术以惊人的速度发展。随着对目标检测的研究越来越深入，在视频中进行目标检测的并行研究已经出现。这样的问题是几个通常研究的计算机视觉问题(即，跟踪、对象检测和分类)的混合，其中对象必须通过视频内的相关帧被正确地定位、分类和关联(即，相同的对象在不同的视频帧中通过唯一的标识符被如此标识)。</p><p id="00e3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">视频对象检测有几个独特的挑战，使其区别于静态图像中的对象检测。在这篇文章中，我不会概述任何当前的视频对象检测方法(见[2]更全面的视频对象检测概述)。相反，我将讨论该领域目前阻碍持续技术进步的主要问题之一:<strong class="lb iu">缺乏一种可以评估视频中对象检测器性能的综合指标。</strong>没有这样一个评估视频对象检测器的标准化指标，<strong class="lb iu">T5】不同方法之间的比较变得困难，导致研究人员质疑他们的贡献是否真正有价值。</strong></p><p id="66ca" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">作为这个问题的解决方案，我认为高阶跟踪精度(HOTA)是对象跟踪社区中的一个标准化指标，是视频对象检测的正确指标，<em class="lv">只要它具有分类意识</em>。我将通过提供开发更好的视频对象检测指标的动机来开始这篇文章。然后，我将介绍一些与对象检测相关的概念和定义，然后讨论该领域中存在的当前指标。我将以对HOTA指标的讨论来结束这篇文章，强调它作为视频对象检测评估指标的适用性。</p><h1 id="821d" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">为什么我们需要更好的指标？</h1><p id="6a17" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">简而言之，视频对象检测的度量标准的当前状态非常差。特别是，该领域研究人员的常用方法是在视频的每一帧上分别评估他们的模型，然后平均所有帧的性能。结果，完全忽略了视频对象检测性能的时间方面，并且不可能根据这样的度量来确定整个视频中预测的时间一致性。换句话说，这种评估协议将视频对象检测简化为在静态图像内执行对象检测(即，每个帧被视为独立的检测问题)，这(如前所述)是完全不同的问题。尽管这种方法可能是合理的起点，但是对于将时间数据结合到它们的预测中以鼓励随时间的一致预测的基于视频的对象检测器(例如，基于光流的检测方法)来说，这显然是不合适的。</p><p id="dac5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">与静态图像中的对象检测相比，视频对象检测的时间方面是使问题变得困难和有趣的关键部分。因此，最广泛使用的视频对象检测度量不能捕捉这种时间行为是完全不可原谅的。如果没有更好地量化视频对象检测性能的所有方面的指标，社区将无法取得一致、向前的进展，因为无法正确比较所提出方法的性能。为了解决这个问题，必须标准化和采用统一的度量标准(即一个度量标准，而不是几个度量标准的组合)。<strong class="lb iu">理想情况下，视频对象检测的适当度量应该提供单个可解释的分数(可能分解成多个更具体的子分数)，该分数捕获视频对象检测性能的所有相关方面，包括检测、定位、关联和分类。</strong></p><h1 id="42f4" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">相关概念</h1><p id="df99" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">在本节中，我将简要讨论几个有用的概念，这些概念对于理解目标检测评估非常重要。特别地，我将描述匈牙利算法(即，用于将预测对象与真实对象相关联的对象检测中最常用的算法)和平均精度度量(即，用于静态图像中对象检测的最广泛使用的评估度量)。</p><h2 id="94a0" class="mt lx it bd ly mu mv dn mc mw mx dp mg li my mz mi lm na nb mk lq nc nd mm ne bi translated">匈牙利算法</h2><p id="7e37" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">在任何形式的对象检测中，我们在评估期间被给予一组基础事实和预测对象。但是，现在还不清楚这些预测中的哪一个符合事实。因此，在评估预测的质量之前，必须做一些工作来确定基础事实和预测对象之间的最佳映射。更具体地说，必须产生预测对象和基础事实对象之间的一对一映射。然而，预测的或基本事实对象也可能没有匹配(即，在这种情况下，映射在技术上不是一对一的，因为存在不成对的元素)。</p><p id="afbc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><a class="ae ky" href="https://en.wikipedia.org/wiki/Hungarian_algorithm" rel="noopener ugc nofollow" target="_blank">匈牙利算法</a>是一种组合优化算法，可用于在多项式时间内解决<a class="ae ky" href="https://en.wikipedia.org/wiki/Assignment_problem" rel="noopener ugc nofollow" target="_blank">集合匹配</a>问题。在对象检测领域中，它通常用于生成预测对象和真实对象之间的映射，通常基于对象之间的成对<a class="ae ky" href="https://en.wikipedia.org/wiki/Jaccard_index" rel="noopener ugc nofollow" target="_blank">交集/并集</a> (IoU)分数。此外，对于在匈牙利算法中要相互匹配的对象，通常会施加一些额外的要求(例如，两个边界框对象检测的IoU必须大于0.5才能被认为是可行的匹配)。由于匈牙利算法的性能和效率，它对于这样的应用是理想的。</p><h2 id="ed08" class="mt lx it bd ly mu mv dn mc mw mx dp mg li my mz mi lm na nb mk lq nc nd mm ne bi translated">平均精度</h2><p id="cb0d" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">如前所述，平均精度(mAP)是评估静态图像中对象检测器的标准度量。为了计算mAP，必须首先计算图像中的真阳性/阴性和假阳性/阴性的数量。在运行匈牙利算法之后，计算这样的度量是非常简单的(即，我们仅仅检查检测是否丢失了一对、对是否是错误的，等等)。).由此，可以在图像内的不同“置信度”水平上计算精度和召回率，以确定平均精度(AP)。对于给定问题领域中的每个不同的语义类，这样的过程被单独重复。然后，计算所有语义类的平均精度的平均值，形成平均平均精度(mAP)度量。</p><p id="8737" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">想要更深入的描述mAP，我推荐阅读<a class="ae ky" href="https://blog.roboflow.com/mean-average-precision/" rel="noopener ugc nofollow" target="_blank">这篇文章</a>。然而，理解(1) mAP是用于评估静态图像中的对象检测性能的首选指标，以及(2) mAP是通过查看单个图像中的预测指标来计算的，这对于理解本文的剩余部分应该是足够的。</p><h1 id="e00b" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">我们已经有了哪些衡量标准？</h1><p id="152d" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">在我介绍(在我看来)最适合评估视频中的对象检测器的指标之前，有必要考虑一些可以使用的其他指标。我将根据指标是用于对象检测还是对象跟踪来将此讨论分为两类，并对每种指标进行简要的高级描述，旨在展示其在评估视频对象检测方面的不足之处。首先，回想一下测量视频中对象检测的性能有四个主要部分:检测、定位、关联和分类。如本节所述，目标跟踪不考虑评估的分类部分<em class="lv">，但目标跟踪的分类感知指标有可能捕捉视频目标检测性能的所有相关部分</em>。</p><h2 id="4f11" class="mt lx it bd ly mu mv dn mc mw mx dp mg li my mz mi lm na nb mk lq nc nd mm ne bi translated">目标检测</h2><p id="4f66" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">如前所述，评估视频中对象检测的最常见指标(在撰写本文时)是<strong class="lb iu">图</strong>。具体地，跨视频的每个帧单独计算mAP，并且跨所有视频帧取mAP分数的平均值，以形成最终的性能度量。这种方法完全不能捕捉视频对象检测的时间方面(即，没有关联的概念)，因此不足以作为评估协议。然而，mAP目前是视频中对象检测的首选度量，并且关于改变/修改该度量的讨论似乎很少。</p><p id="46f4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">一个包含时间信息的有趣的地图变体是<strong class="lb iu">基于速度的地图</strong>【3】(也就是说，我为这篇博文的上下文编造了这个名字；相关联的引用不提供特定的名称)。为了计算该度量，首先基于对象在帧之间移动的速度(即，使用相邻帧的运动IOU来计算)，将对象分成三个不同的组(即，慢、中和快)。然后，为这三组中的每一组中的对象分别计算mAP，并且分别呈现三个mAP分数。尽管基于速度的图基于视频中的对象速度提供了对象检测性能的更细粒度的视图，但是它提供了三个度量而不是一个，并且仍然不能捕捉视频对象检测的时间方面；同样，映射不包含关联的概念。因此，基于速度映射不是视频对象检测的合适度量。</p><p id="f128" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为视频对象检测提出的最后一个度量是<strong class="lb iu">平均延迟(AD)</strong>【4】。AD捕捉的是物体进入视频和被探测器“拾取”之间的延迟，这种延迟被称为“算法延迟”。这种延迟是以帧为单位测量的，因此AD为2意味着，平均而言，对象将在视频中存在两个完整的帧，直到它被模型实际检测到。虽然AD在视频对象检测中捕获时间信息，<em class="lv">它被提议作为一种与mAP </em>结合使用的度量。因此，它不是一个可以用来全面评估视频中对象检测器性能的独立指标。</p><h2 id="fadc" class="mt lx it bd ly mu mv dn mc mw mx dp mg li my mz mi lm na nb mk lq nc nd mm ne bi translated">目标跟踪</h2><p id="99f2" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">尽管还没有针对视频对象检测提出全面的度量，但是在对象跟踪社区中存在许多有用的度量，从中可以获得灵感。对象跟踪要求在整个视频中以一致的方式识别、定位和关联对象(每个视频帧中的一个或多个对象)。虽然对象跟踪不包含分类的概念，但是在对象跟踪和视频对象检测之间存在足够的重叠，以保证对对象跟踪中的当前评估度量进行更深入的检查。</p><p id="498f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">多目标跟踪精度(MOTA)</strong>【5】是目标跟踪中最广泛使用的度量之一。MOTA将基础事实与每个检测的预测对象相匹配，这意味着在评估过程中，每个预测和基础事实检测都被视为一个独立的实体。在高层次上，MOTA(基于匈牙利算法提供的匹配)确定所有视频帧中的身份切换(即，相同的对象在相邻的视频帧中被分配不同的标识符)、假阳性和假阴性检测的数量。然后，通过用视频中地面实况对象的总数对这些分量的总和进行归一化来计算MOTA，如下式所示。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nf"><img src="../Images/b2fd67e5931b658734c0db874ddfc2a6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XgT8h_P_EMy7bObQsC3Kmw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">MOTA跟踪度量</p></figure><p id="cbfa" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">MOTA通过身份切换(即，表示为上面的<code class="fe ng nh ni nj b">IDSW</code>)来捕获关联性能，而检测性能通过假阳性和假阴性来捕获。但是，MOTA不考虑本地化。相反，定位必须通过一个单独的指标来衡量，<strong class="lb iu">多目标跟踪精度(MOTP) </strong> [5]，它对视频中所有检测的定位分数进行平均。尽管MOTA是用于对象跟踪的长期标准化度量，但是它有几个缺点，这限制了它在视频对象检测中的适用性。也就是说，它过分强调检测性能(即，身份转换对上述分数的影响是最小的)，不考虑相邻帧之外的关联，不考虑定位，提供多个分数而不是统一的度量，高度依赖于帧速率，并且提供可能难以解释的无界分数(即，MOTA可以具有[-∞，1]的值)。因此，<em class="lv"> MOTA不足以作为视频对象检测的度量</em>。</p><p id="f765" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对象跟踪社区中其他广泛使用的指标是<strong class="lb iu">id f1</strong>【6】和<strong class="lb iu"> track-mAP </strong>(也称为3D-IoU)，它们在轨迹级别上将地面实况与预测进行匹配(即，轨迹被定义为贯穿视频帧的预测或地面实况对象的序列，它们共享相同的唯一标识符)。与MOTA相比，IDF1和track-mAP在对象跟踪社区中没有得到广泛使用，因此我不会对这些指标进行深入讨论(有关指标之间更全面的讨论和比较，请参见[7])。然而，<em class="lv"> IDF1和track-mAP都有许多限制，这阻碍了它们被采用为视频对象检测的标准度量</em>。也就是说，IDF1过分强调关联性能，不考虑定位，并且忽略彼此不匹配的轨迹之外的所有关联/检测。类似地，轨迹地图要求每个轨迹预测包含置信度得分，要求轨迹距离度量由用户定义，并且可以容易地“游戏化”(即，可以提供简单的反例，其表现很差但是实现了高轨迹地图得分)。</p><h1 id="2925" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">走向全面的衡量标准</h1><p id="9615" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">尽管上一节概述了指标的局限性，但截至2020年中期，对象跟踪社区中有一个新的标准指标:<strong class="lb iu">更高阶跟踪精度(HOTA)</strong>【7】。在本节中，我将介绍HOTA指标，并解释为什么我认为它是评估视频对象检测器的合适指标。虽然HOTA不考虑分类性能，但它的分类感知对应物CA-HOTA在单个评估指标内有效地捕获了视频对象检测性能的所有相关方面。</p><h2 id="9c62" class="mt lx it bd ly mu mv dn mc mw mx dp mg li my mz mi lm na nb mk lq nc nd mm ne bi translated">为什么是HOTA？</h2><p id="ba3d" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">HOTA的目标是(1)提供一个单一的分数，该分数捕捉跟踪性能的所有相关方面，(2)使长期关联性能能够被测量(即，两个相邻帧之外的关联)，以及(3)分解成捕捉跟踪性能的更详细方面的不同子度量。HOTA旨在缓解以前跟踪指标的问题，它提供了一个在[0，1]范围内的单一分数，该分数以平衡的方式捕获跟踪性能的所有相关方面(即检测、关联和定位)。此外，这个单一的、可解释的分数可以分解为子指标，这些子指标在更细粒度的级别上表征跟踪性能的不同方面。下图总结了与其他广泛使用的跟踪指标相比，HOTA的优势。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nk"><img src="../Images/545328673ff1b8071de699961435c448.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Cf-uOKZGK1jAuSRKcjeGHA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">HOTA与其他跟踪指标的比较[7]</p></figure><p id="c0f8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">尽管HOTA不考虑分类性能，但是已经提出了将分类并入HOTA得分的变体(例如CA-HOTA)。CA-HOTA在一个可解释的指标内捕获视频对象检测性能的所有方面(即关联、定位、检测和分类)。因此，CA-HOTA可以被认为是用于视频对象检测的(相对)全面的度量。</p><h2 id="af49" class="mt lx it bd ly mu mv dn mc mw mx dp mg li my mz mi lm na nb mk lq nc nd mm ne bi translated">HOTA是什么？</h2><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nl"><img src="../Images/79683b46e500133e7d248461f233356b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2JWyK-uK3zDOXAwAwz7z-A.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">`，HOTA度量的说明[7]</p></figure><p id="ef43" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">与MOTA类似，HOTA在探测层面(即，与IDF1或track-mAP中的轨迹层面相反)匹配预测和地面实况对象。在HOTA中，从基础事实和预测对象匹配(同样由匈牙利算法产生)中计算出两类度量:检测组件和关联组件。检测组件只是真阳性、假阴性和假阳性，这在前面已经讨论过了。有些不同的关联成分包括真阳性关联(TPA)、假阴性关联(FNA)和假阳性关联(FPA)。</p><p id="598c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">考虑在特定帧中具有有效匹配的对象(即，真正的肯定检测)，我们将其表示为<code class="fe ng nh ni nj b">c</code>。在这种真正的肯定检测中，预测和基础事实对象都必须被分配一个唯一的标识符。为了计算TPA的数量，只需找到与<code class="fe ng nh ni nj b">c</code>共享相同基本事实和预测标识符的其他帧中的真阳性的数量，并对视频内的每个可能的<code class="fe ng nh ni nj b">c</code>重复该过程(即，每个真阳性检测)。FNA和FPA以类似的方式定义，但是必须在其他帧中找到分别具有相同基本事实标识符和不同预测标识符或者相同预测标识符和不同基本事实标识符的检测。实质上，TPA、FPA和FNA允许跨视频内的所有帧测量关联性能，而不仅仅是相邻帧之间的关联性能。因此，命名为“高阶”跟踪精度。</p><p id="1189" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">给定某个定位阈值(即，定位阈值修改由匈牙利算法产生的匹配)，我们可以计算HOTA的所有检测和关联组件。然后，在给定定位阈值(我们表示为α)下的合计HOTA分数可以计算如下。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nm"><img src="../Images/64aa0116d313d759fe282f65f5672d54.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AsTNHrXB_T8SWnxzXLfFbQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">单一定位阈值下的HOTA度量</p></figure><p id="3d15" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">一旦在特定本地化阈值下计算出HOTA，就可以通过在几个不同的本地化阈值上对上述度量进行平均来导出合计HOTA分数。通常，定位阈值的样本取值范围为[0，1]，如下式所示。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nn"><img src="../Images/00f89ed567a1072367f6e43228576d93.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sOaSy4fXhvS47vbv4Zq-tg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">HOTA度量</p></figure><p id="5aaa" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如上面的等式所示，HOTA捕获检测(通过检测组件)、关联(通过关联组件)和定位(通过定位阈值)性能。此外，HOTA可以分解成许多子指标。下图中概述的每个子指标都可以用来分析跟踪器性能的更具体方面(例如，定位、检测或孤立的关联性能)。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi no"><img src="../Images/d3c5d0506f531ad458e02e3153e5fee3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KeSW1VPzkWCnfLBWD-dCkw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">HOTA中的子指标[由作者创建]</p></figure><p id="8a06" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">关于如何计算HOTA度量的更全面的讨论，也可以阅读相关论文[7]或博客文章[8]。</p><h2 id="2498" class="mt lx it bd ly mu mv dn mc mw mx dp mg li my mz mi lm na nb mk lq nc nd mm ne bi translated">分类呢？</h2><p id="9fd2" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">我之前提到过，可以修改HOTA来获取分类性能，但是从来没有明确说明这是如何实现的。尽管提出了分类感知HOTA的多种变体[7]，但一种可能的方法是在某个定位阈值处修改HOTA得分，如下所示。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi np"><img src="../Images/db2fa53a97438eb6439421689363052b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zReZ7-3a-KxhpdGar3qwpw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">分类感知HOTA (CA-HOTA)</p></figure><p id="ef58" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">从上面可以看出，分类性能是通过用给定真阳性检测的正确类别的相关置信度来缩放对HOTA度量的所有贡献而合并的。因此，如果分类性能很差，HOTA度量将会恶化。给定以上定义，然后可以将合计CA-HOTA度量计算为不同本地化阈值上的得分的平均值(即，就像对于普通HOTA一样)，产生CA-HOTA度量。</p><h1 id="6225" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">结论</h1><p id="91f8" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">目前，视频对象检测社区缺乏正确捕捉模型性能的统一评估度量。通常，使用mAP度量并跨视频帧进行平均来表征性能，但是这种技术完全忽略了模型性能的时间方面。根据对象跟踪领域的最新进展，我认为高阶跟踪精度(HOTA)度量是视频中对象检测的合适评估标准。HOTA的分类感知变体CA-HOTA捕获视频对象检测性能的所有相关方面，包括检测、关联、定位和分类。因此，<em class="lv">它是一个全面的指标(尤其是与mAP等静态指标相比)，可以而且应该用于对视频对象检测中的不同方法进行基准测试</em>。</p><p id="af8e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我希望这篇文章能在社区内引发讨论，并为视频中的对象检测带来更标准化和更全面的基准测试。这项工作是我在<a class="ae ky" href="https://www.alegion.com/" rel="noopener ugc nofollow" target="_blank"> Alegion </a>做研究科学家工作的一部分。如果你对这个话题感兴趣，我鼓励你去看看这家公司，或者申请一个空缺的职位。我们一直在寻找更多对机器学习相关话题感兴趣的人！要了解我的博客文章，请随时<a class="ae ky" href="https://wolfecameron.github.io/" rel="noopener ugc nofollow" target="_blank">访问我的网站</a>或在<a class="ae ky" href="https://twitter.com/cwolferesearch" rel="noopener ugc nofollow" target="_blank"> twitter </a>上关注我。</p><p id="c718" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="lv">参考文献</em></p><p id="0986" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">[1]<a class="ae ky" href="https://cocodataset.org/#home" rel="noopener ugc nofollow" target="_blank">https://cocodataset.org/#home</a></p><p id="c236" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">[2]<a class="ae ky" href="https://www.mdpi.com/2076-3417/10/21/7834" rel="noopener ugc nofollow" target="_blank">https://www.mdpi.com/2076-3417/10/21/7834</a></p><p id="4ad5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><a class="ae ky" href="https://arxiv.org/abs/1703.10025" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1703.10025</a></p><p id="2b02" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">[4]<a class="ae ky" href="https://www.researchgate.net/publication/335258204_A_Delay_Metric_for_Video_Object_Detection_What_Average_Precision_Fails_to_Tell" rel="noopener ugc nofollow" target="_blank">https://www . research gate . net/publication/335258204 _ A _ Delay _ Metric _ for _ Video _ Object _ Detection _ What _ Average _ Precision _ Fails _ to _ Tell</a></p><p id="912b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">【https://arxiv.org/abs/1603.00831 T2】</p><p id="3889" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">[6]https://arxiv.org/abs/1609.01775<a class="ae ky" href="https://arxiv.org/abs/1609.01775" rel="noopener ugc nofollow" target="_blank"/></p><p id="b320" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">[7]<a class="ae ky" href="https://arxiv.org/abs/2009.07736" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/2009.07736</a></p><p id="e65e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">[8]<a class="ae ky" href="https://jonathonluiten.medium.com/how-to-evaluate-tracking-with-the-hota-metrics-754036d183e1" rel="noopener">https://jonathonluiten . medium . com/how-to-evaluate-tracking-with-the-hota-metrics-754036d 183 e 1</a></p></div></div>    
</body>
</html>