<html>
<head>
<title>Everything you need to know about “activation functions” for deep learning models</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">关于深度学习模型的“激活函数”，你需要知道的一切</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/everything-you-need-to-know-activation-functions-for-deep-learning-models-ac6f528da604?source=collection_archive---------27-----------------------#2021-09-29">https://towardsdatascience.com/everything-you-need-to-know-activation-functions-for-deep-learning-models-ac6f528da604?source=collection_archive---------27-----------------------#2021-09-29</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="203e" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">想知道什么是激活功能，为什么它们必不可少？</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/42a7640d0a5049bab117dbd61f9489a7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*81WwpYbAN-f_K0k8"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">艾莉娜·格鲁布尼亚克在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="f9f9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们知道深度学习模型是激活函数、批量归一化、动量、梯度下降等不同成分的组合。因此，在这篇博客中，我选择了DL的一部分，通过回答以下问题给出了关于激活功能的详细解释:</p><ul class=""><li id="c5fd" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated">什么是激活函数？</li><li id="1534" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">为什么我们在神经网络中需要一个激活函数，如果我们不使用，会发生什么？</li><li id="34b1" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">激活函数的期望属性是什么？</li><li id="09c4" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">有哪些不同类型的激活功能及其用途？</li></ul><p id="dae0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对于这个博客，我假设你们都对神经网络有一个基本的了解。所以事不宜迟，让我们更深入地研究激活函数。</p><h1 id="c8a4" class="mj mk it bd ml mm mn mo mp mq mr ms mt jz mu ka mv kc mw kd mx kf my kg mz na bi translated">激活功能</h1><p id="5d08" class="pw-post-body-paragraph kz la it lb b lc nb ju le lf nc jx lh li nd lk ll lm ne lo lp lq nf ls lt lu im bi translated">激活函数是一个<strong class="lb iu">函数</strong>，用于<strong class="lb iu">变换神经网络中前一节点</strong>的输出信号。这种转换有助于网络学习数据中的复杂模式。就是这样！一开始我也不相信事情会这么简单，但这是真的。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ng"><img src="../Images/d7ad2bff150eb613d8a450364293a477.png" data-original-src="https://miro.medium.com/v2/resize:fit:1302/format:webp/0*O4dLOjE1fPV71phV.jpeg"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">激活函数转换前一个节点的输出(图片由作者提供)</p></figure><h1 id="089e" class="mj mk it bd ml mm mn mo mp mq mr ms mt jz mu ka mv kc mw kd mx kf my kg mz na bi translated">需要激活功能</h1><p id="166d" class="pw-post-body-paragraph kz la it lb b lc nb ju le lf nc jx lh li nd lk ll lm ne lo lp lq nf ls lt lu im bi translated">是的，你现在已经知道基本原因了。有必要使神经网络能够<strong class="lb iu">学习数据</strong>中的复杂模式。但是它是如何实现的呢？激活函数通常是非线性函数，它给神经网络增加了非线性，从而允许它们学习更复杂的模式。还不清楚？让我们看一个神经网络的例子来理解这一点。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nh"><img src="../Images/8d476265d5e1afb5957a1c1d1eef669c.png" data-original-src="https://miro.medium.com/v2/resize:fit:960/format:webp/1*EX0TgncRJZYw_tsBMjmwOg.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">神经网络(图片作者提供)</p></figure><p id="e769" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对于这个例子，请假设我们没有添加偏差。我们可以将输出Y写成:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/fdd4e91459c405ec0a370e6e6dffad7c.png" data-original-src="https://miro.medium.com/v2/resize:fit:316/1*CWug9wFNvcZ0V0OTqZqfsQ.gif"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><p id="7b23" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">其中<em class="nj"> Act </em>表示我们的激活函数的输出，(这是一个非线性变换)。现在假设我们没有网络的任何激活函数，那么我们的Y看起来像这样:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/3395d21f40931fe082fcd7cfa4e0d82a.png" data-original-src="https://miro.medium.com/v2/resize:fit:232/1*AWiOM65l6UV4QlCXW1LymA.gif"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><p id="c263" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果你仔细看上面的等式，那么</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nl"><img src="../Images/574546139e57ed67961d84cd09fac96a.png" data-original-src="https://miro.medium.com/v2/resize:fit:602/1*8Ns9PPKtT5aTTxlINTybdw.gif"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><p id="9516" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这意味着，即使我们的网络中有两层，输入和输出关系实际上也可以由单个权重矩阵来定义(这是两个权重矩阵的乘积)。因此我们看到，在没有激活函数的情况下；我们给神经网络增加多少层并不重要，它们都可以被简化，只用一个权重矩阵来表示。但是，当我们添加激活函数时，它们会添加非线性转换，这使得我们无法简化多层神经网络。</p><p id="2a07" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">激活函数的另一个重要性是，它有助于将神经元的输出值限制在我们需要的范围内。这一点很重要，因为激活函数的输入是<strong class="lb iu"> W*x + b </strong>，其中<strong class="lb iu"> W </strong>是单元的重量，而<strong class="lb iu"> x </strong>是输入，然后还有偏置<strong class="lb iu"> b </strong>加入其中。如果不受限制，该值可以达到非常高的量级，尤其是在处理数百万个参数的非常深的神经网络的情况下。这进而导致计算问题以及值溢出问题。</p><p id="0138" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我知道没有激活函数的例子，很难理解我扔给你的所有事实，但是请耐心听我说。我们将很快在一个例子中回顾所有这些概念，但是为了你更好的理解，它们需要在这里被覆盖。</p><h1 id="3281" class="mj mk it bd ml mm mn mo mp mq mr ms mt jz mu ka mv kc mw kd mx kf my kg mz na bi translated">激活函数的期望属性</h1><ul class=""><li id="1499" class="lv lw it lb b lc nb lf nc li nm lm nn lq no lu ma mb mc md bi translated">从上面可以清楚地看出，激活函数<strong class="lb iu">应该</strong>是非线性的<strong class="lb iu">。</strong></li><li id="de40" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">在神经网络的每一层之后使用激活函数，因此，总是希望激活函数在计算上<strong class="lb iu">高效</strong>。</li><li id="6609" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">对神经网络的所有组件的主要要求是它们应该是可微分的，因此激活函数<strong class="lb iu">应该</strong>也<strong class="lb iu"> </strong>是<strong class="lb iu">可微分的</strong>。</li><li id="c926" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">设计激活函数的一个重要方面是防止<strong class="lb iu">消失梯度问题</strong>。深入解释这个问题超出了本博客的范围，但让我给你要点。为了防止消失梯度问题，要求激活函数w.r.t输入参数的<strong class="lb iu">导数</strong>在理论上是<strong class="lb iu">不有界</strong>在<strong class="lb iu"> -1到1 </strong>之间。</li></ul><h1 id="951f" class="mj mk it bd ml mm mn mo mp mq mr ms mt jz mu ka mv kc mw kd mx kf my kg mz na bi translated">不同类型的激活功能</h1><h2 id="5b4b" class="np mk it bd ml nq nr dn mp ns nt dp mt li nu nv mv lm nw nx mx lq ny nz mz oa bi translated">热卢</h2><p id="edfd" class="pw-post-body-paragraph kz la it lb b lc nb ju le lf nc jx lh li nd lk ll lm ne lo lp lq nf ls lt lu im bi translated">ReLU代表<strong class="lb iu">Re</strong>ctived<strong class="lb iu">L</strong>linear<strong class="lb iu">U</strong>nit，定义为<strong class="lb iu"> f(x) = max(0，f(x)) </strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ob"><img src="../Images/11794e5841d18f856f88eafd5feb36ee.png" data-original-src="https://miro.medium.com/v2/resize:fit:1134/format:webp/0*TpFYiJCUfPKPxr4R.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">ReLU图(图片由作者提供)</p></figure><p id="85df" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这是一个广泛使用的激活函数，尤其是与CNN(卷积神经网络)一起使用。它易于计算，不会饱和，也不会导致梯度消失的问题。但是，它有一个问题，即对于负输入，它的值变为零。由于所有负输入的输出都是零，这导致一些节点完全死亡，什么也学不到。为了处理这个问题，使用了漏ReLU或者参数ReLU，即<strong class="lb iu"> F(x) = max(αx，x)。</strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oc"><img src="../Images/dec4152f5253a5b51ed5b98bc640d284.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5PzRhVap8XeXUefUFNc-7g.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">泄漏的ReLU(图片由作者提供)</p></figure><h2 id="ec4e" class="np mk it bd ml nq nr dn mp ns nt dp mt li nu nv mv lm nw nx mx lq ny nz mz oa bi translated">乙状结肠的</h2><p id="3df5" class="pw-post-body-paragraph kz la it lb b lc nb ju le lf nc jx lh li nd lk ll lm ne lo lp lq nf ls lt lu im bi translated">这个激活函数在计算上是昂贵的，导致消失梯度问题，并且不是零中心的。这种方法通常用于二进制分类问题，并且仅在神经网络的末端使用，以将输出转换为范围[0，1]。这个函数<strong class="lb iu">一般不在神经网络内部使用</strong>。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi od"><img src="../Images/3626965c3c7e8c443a2b00483ccd473c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*DabJvqgTdiXAY0nc.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">乙状结肠(此处<a class="ae ky" href="https://en.wikipedia.org/wiki/Sigmoid_function" rel="noopener ugc nofollow" target="_blank">提供</a>与<a class="ae ky" href="https://commons.wikimedia.org/wiki/File:Fun%C3%A7%C3%A3o_Sigmoidal.png" rel="noopener ugc nofollow" target="_blank"> CC许可</a></p></figure><h2 id="f047" class="np mk it bd ml nq nr dn mp ns nt dp mt li nu nv mv lm nw nx mx lq ny nz mz oa bi translated">Softmax</h2><p id="6543" class="pw-post-body-paragraph kz la it lb b lc nb ju le lf nc jx lh li nd lk ll lm ne lo lp lq nf ls lt lu im bi translated">用于<strong class="lb iu">多类分类问题</strong>。像sigmoid一样，它产生0-1范围内的值，因此，它被用作分类模型中的最后一层。</p><h1 id="31b5" class="mj mk it bd ml mm mn mo mp mq mr ms mt jz mu ka mv kc mw kd mx kf my kg mz na bi translated">结论</h1><p id="7c13" class="pw-post-body-paragraph kz la it lb b lc nb ju le lf nc jx lh li nd lk ll lm ne lo lp lq nf ls lt lu im bi translated">我希望在这个解释之后，你现在能更好地理解为什么神经网络需要激活函数，以及激活函数的性质和类型。如果你觉得有帮助，请在这个博客上发表评论，让我们知道。</p><p id="86cf" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">关注我们的<a class="ae ky" href="https://medium.com/@AnveeNaik" rel="noopener"> medium </a>了解更多此类内容。</p><p id="5c35" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="nj">成为</em> <a class="ae ky" href="https://medium.com/@AnveeNaik/membership" rel="noopener"> <em class="nj">介质会员</em> </a> <em class="nj">解锁并阅读介质上的许多其他故事。</em></p></div></div>    
</body>
</html>