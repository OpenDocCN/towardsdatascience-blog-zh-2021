<html>
<head>
<title>Pandas Runs on Spark!</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">熊猫靠火花奔跑！</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/pandas-on-spark-current-issues-and-workarounds-dc9ed30840ce?source=collection_archive---------11-----------------------#2021-11-15">https://towardsdatascience.com/pandas-on-spark-current-issues-and-workarounds-dc9ed30840ce?source=collection_archive---------11-----------------------#2021-11-15</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="1099" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">在那里，我参加了一个常规的熊猫项目，并尝试在Spark pandas下运行它</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/9a6775e2fef21d2e0785e7da5a02cd1a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*hrCNAiQ5ritsjzex"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><a class="ae kv" href="https://unsplash.com/@zanilic?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Zan </a>在<a class="ae kv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><h1 id="8a7b" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">安达斯和火花</h1><p id="e277" class="pw-post-body-paragraph lw lx iq ly b lz ma jr mb mc md ju me mf mg mh mi mj mk ml mm mn mo mp mq mr ij bi translated">Pandas是数据分析和数据科学的关键工具，已经存在了十多年。它是稳定的，经过验证的。但是pandas有一个重大的限制，每个数据工程师都会在某个时候碰到——它只能在一台计算机上运行。pandas的数据大小限制约为100M行或100GB，只有在功能强大、价格昂贵的计算机上才能达到这些限制。</p><p id="f88a" class="pw-post-body-paragraph lw lx iq ly b lz ms jr mb mc mt ju me mf mu mh mi mj mv ml mm mn mw mp mq mr ij bi translated">Apache Spark最近发布了这个问题的解决方案，在<a class="ae kv" href="https://spark.apache.org" rel="noopener ugc nofollow" target="_blank"> Spark 3.2 </a>中包含了pyspark.pandas库。由于Spark运行在几乎无限的计算机集群上，它可以处理的数据集大小实际上没有限制。熊猫程序员可以移动他们的代码来激发和消除以前的数据约束。</p><h1 id="1b18" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">测试</h1><p id="14a8" class="pw-post-body-paragraph lw lx iq ly b lz ma jr mb mc md ju me mf mg mh mi mj mk ml mm mn mo mp mq mr ij bi translated">当然，像这样的API仿真工作在第一个版本中是不完美的，也不会覆盖所有的目标特性。因此，我在Spark中测试了pandas的功能，将一些中等复杂的pandas代码从Python 3.8.8和pandas 1.2.4迁移到Spark 3.2。我在<a class="ae kv" href="https://databricks.com" rel="noopener ugc nofollow" target="_blank"> Databricks 10.0 </a>中这样做，因为该平台使得创建和管理Spark集群变得容易。</p><p id="4260" class="pw-post-body-paragraph lw lx iq ly b lz ms jr mb mc mt ju me mf mu mh mi mj mv ml mm mn mw mp mq mr ij bi translated">我的目标是在Spark下运行代码，尽可能少做改动。我移植的pandas代码是一对程序——一个是<a class="ae kv" href="https://github.com/ChuckConnell/articles/blob/master/MakeVaccineMortalityFile.py" rel="noopener ugc nofollow" target="_blank">导入/清理/规范化/连接</a>三个数据集，一个是<a class="ae kv" href="https://github.com/ChuckConnell/articles/blob/master/AnalyzeCountyVaccineMortality.py" rel="noopener ugc nofollow" target="_blank">分析</a>组合数据。由此产生的pyspark.pandas代码是一个Databricks笔记本，这里称为<a class="ae kv" href="https://github.com/ChuckConnell/articles/blob/master/VaxMortality_pyspark_pandas.dbc" rel="noopener ugc nofollow" target="_blank"> DBC </a>和<a class="ae kv" href="https://github.com/ChuckConnell/articles/blob/master/VaxMortality_pyspark_pandas.py" rel="noopener ugc nofollow" target="_blank"> Python </a>。</p><h1 id="02af" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">结果</h1><p id="f5a1" class="pw-post-body-paragraph lw lx iq ly b lz ma jr mb mc md ju me mf mg mh mi mj mk ml mm mn mo mp mq mr ij bi translated">总的来说，pyspark.pandas工作得很好，产生了正确的结果，无论是在数字上还是在各种图形的视觉上。需要对代码进行一些修改，但没有一处是令人失望的，也没有一处会导致错误或不完整的答案。</p><p id="c137" class="pw-post-body-paragraph lw lx iq ly b lz ms jr mb mc mt ju me mf mu mh mi mj mv ml mm mn mw mp mq mr ij bi translated">本文列出了我发现的妨碍我的代码不加修改地运行的问题，以及每个问题的解决方法。我已经向Apache-Spark吉拉报告了这些问题，并附上了这些链接。它们是:</p><ul class=""><li id="481a" class="mx my iq ly b lz ms mc mt mf mz mj na mn nb mr nc nd ne nf bi translated">版本检查</li><li id="4081" class="mx my iq ly b lz ng mc nh mf ni mj nj mn nk mr nc nd ne nf bi translated">读取输入文件</li><li id="82e5" class="mx my iq ly b lz ng mc nh mf ni mj nj mn nk mr nc nd ne nf bi translated">保存结果</li><li id="5503" class="mx my iq ly b lz ng mc nh mf ni mj nj mn nk mr nc nd ne nf bi translated">输入的拉丁1编码</li><li id="12ed" class="mx my iq ly b lz ng mc nh mf ni mj nj mn nk mr nc nd ne nf bi translated">to_numeric(错误=)</li><li id="2fab" class="mx my iq ly b lz ng mc nh mf ni mj nj mn nk mr nc nd ne nf bi translated">。地图()。菲尔娜</li><li id="9077" class="mx my iq ly b lz ng mc nh mf ni mj nj mn nk mr nc nd ne nf bi translated">从字符串中删除后缀</li><li id="6389" class="mx my iq ly b lz ng mc nh mf ni mj nj mn nk mr nc nd ne nf bi translated">调整日期时间值</li><li id="a04c" class="mx my iq ly b lz ng mc nh mf ni mj nj mn nk mr nc nd ne nf bi translated">数据帧中一列的直方图</li><li id="f26d" class="mx my iq ly b lz ng mc nh mf ni mj nj mn nk mr nc nd ne nf bi translated">直方图标题</li><li id="0496" class="mx my iq ly b lz ng mc nh mf ni mj nj mn nk mr nc nd ne nf bi translated">直方图范围</li></ul><p id="9d74" class="pw-post-body-paragraph lw lx iq ly b lz ms jr mb mc mt ju me mf mu mh mi mj mv ml mm mn mw mp mq mr ij bi translated">我使用的计算环境是免费的<a class="ae kv" href="https://community.cloud.databricks.com/" rel="noopener ugc nofollow" target="_blank"> Databricks社区版</a> 10.0，其中包括Spark 3.2。为了清楚起见，我用普通的别名“pd”来称呼普通的熊猫，用“pspd”来称呼PySpark熊猫。</p><h2 id="d888" class="nl kx iq bd ky nm nn dn lc no np dp lg mf nq nr li mj ns nt lk mn nu nv lm nw bi translated">检查熊猫版本</h2><p id="c2e5" class="pw-post-body-paragraph lw lx iq ly b lz ma jr mb mc md ju me mf mg mh mi mj mk ml mm mn mo mp mq mr ij bi translated">鉴于pyspark.pandas不是独立的软件，除了pyspark之外不进行更新，请将第一行中出现的任何内容更改为第二行。</p><pre class="kg kh ki kj gt nx ny nz oa aw ob bi"><span id="7369" class="nl kx iq ny b gy oc od l oe of">#print(pd.__version__)</span><span id="7baf" class="nl kx iq ny b gy og od l oe of">print (sc.version)  # sc = Spark context</span></pre><p id="1dee" class="pw-post-body-paragraph lw lx iq ly b lz ms jr mb mc mt ju me mf mu mh mi mj mv ml mm mn mw mp mq mr ij bi translated">要跟踪此问题，请参见<a class="ae kv" href="https://issues.apache.org/jira/browse/SPARK-37180" rel="noopener ugc nofollow" target="_blank">https://issues.apache.org/jira/browse/SPARK-37180</a>。</p><h2 id="fe54" class="nl kx iq bd ky nm nn dn lc no np dp lg mf nq nr li mj ns nt lk mn nu nv lm nw bi translated">读取输入文件</h2><p id="5b49" class="pw-post-body-paragraph lw lx iq ly b lz ma jr mb mc md ju me mf mg mh mi mj mk ml mm mn mo mp mq mr ij bi translated">对于从普通熊猫转向Spark-cluster熊猫的程序员来说，这个问题可能会令人困惑。对于普通的熊猫来说，程序、数据和用户都在同一台电脑上。有了PySpark pandas，你的电脑只是操作Spark集群的前端，而Spark集群位于云中的某个地方。因此，<code class="fe oh oi oj ny b">pspd.read_csv()</code>看不到电脑本地磁盘上的文件。</p><p id="ad99" class="pw-post-body-paragraph lw lx iq ly b lz ms jr mb mc mt ju me mf mu mh mi mj mv ml mm mn mw mp mq mr ij bi translated">你如何给你的程序输入信息？有两种方法都很有效。</p><ul class=""><li id="9964" class="mx my iq ly b lz ms mc mt mf mz mj na mn nb mr nc nd ne nf bi translated">使用Databricks GUI将输入文件从您的计算机复制到Databricks文件系统(DBFS)。详细说明在我的<a class="ae kv" href="https://medium.com/@chuck.connell.3/pandas-on-databricks-via-koalas-a-review-9876b0a92541" rel="noopener">上一篇</a>里。一旦到了DBFS，文件就在Spark的本地，并且<code class="fe oh oi oj ny b">pspd.read_csv()</code>可以正常打开它们。</li><li id="e273" class="mx my iq ly b lz ng mc nh mf ni mj nj mn nk mr nc nd ne nf bi translated">将文件放入云存储中，如<a class="ae kv" href="https://docs.databricks.com/data/data-sources/aws/amazon-s3.html" rel="noopener ugc nofollow" target="_blank">亚马逊S3 </a>或<a class="ae kv" href="https://docs.databricks.com/data/data-sources/azure/azure-storage.html" rel="noopener ugc nofollow" target="_blank"> Azure blob </a>，将该存储挂载到DBFS，然后在<code class="fe oh oi oj ny b">pspd.read_csv().</code>中使用该路径</li></ul><p id="0414" class="pw-post-body-paragraph lw lx iq ly b lz ms jr mb mc mt ju me mf mu mh mi mj mv ml mm mn mw mp mq mr ij bi translated">要跟踪此问题，请参见<a class="ae kv" href="https://issues.apache.org/jira/browse/SPARK-37198" rel="noopener ugc nofollow" target="_blank">https://issues.apache.org/jira/browse/SPARK-37198</a>。</p><h2 id="17d2" class="nl kx iq bd ky nm nn dn lc no np dp lg mf nq nr li mj ns nt lk mn nu nv lm nw bi translated">保存结果</h2><p id="2162" class="pw-post-body-paragraph lw lx iq ly b lz ma jr mb mc md ju me mf mg mh mi mj mk ml mm mn mo mp mq mr ij bi translated">用<code class="fe oh oi oj ny b">pspd.to_csv()</code>写输出文件会导致两个问题。Spark写DBFS而不是你的本地机器。由于Spark运行在多个计算节点上，它将输出写成一组名称丑陋、不可预测的文件<em class="ok">集合</em>。你可以找到这些文件，并把它们合并成一个普通的CSV文件，但这很麻烦。</p><p id="03b4" class="pw-post-body-paragraph lw lx iq ly b lz ms jr mb mc mt ju me mf mu mh mi mj mv ml mm mn mw mp mq mr ij bi translated">要解决这些问题:</p><ul class=""><li id="b297" class="mx my iq ly b lz ms mc mt mf mz mj na mn nb mr nc nd ne nf bi translated">对于可视化结果，如直方图或散点图，按下图中的相机图标以在本地机器上获得PNG图像。</li><li id="e238" class="mx my iq ly b lz ng mc nh mf ni mj nj mn nk mr nc nd ne nf bi translated">对于多达10，000行的数据集，使用<code class="fe oh oi oj ny b">display(pspdDataFrame).</code>初始输出显示1000行，您可以选择获得10，000行。有一个按钮可以将CSV文件下载到您的本地机器上。</li><li id="13d2" class="mx my iq ly b lz ng mc nh mf ni mj nj mn nk mr nc nd ne nf bi translated">对于稍后要处理的较大数据集，将数据帧保存为Databricks表，该表在计算会话之间作为单个实体保存在DBFS中。当您以后想要读取该数据时，只需在Databricks中打开该表。</li></ul><pre class="kg kh ki kj gt nx ny nz oa aw ob bi"><span id="60c1" class="nl kx iq ny b gy oc od l oe of"># DataFrame to table<br/>pspdDF.to_spark().write.mode(“OVERWRITE”).saveAsTable(“my_table”)<br/> <br/># Table to DataFrame<br/>pspdDF = sqlContext.table(“my_table”).to_pandas_on_spark()</span></pre><p id="5434" class="pw-post-body-paragraph lw lx iq ly b lz ms jr mb mc mt ju me mf mu mh mi mj mv ml mm mn mw mp mq mr ij bi translated">要跟踪此问题，请参见<a class="ae kv" href="https://issues.apache.org/jira/browse/SPARK-37198" rel="noopener ugc nofollow" target="_blank">https://issues.apache.org/jira/browse/SPARK-37198</a>。</p><h2 id="16bc" class="nl kx iq bd ky nm nn dn lc no np dp lg mf nq nr li mj ns nt lk mn nu nv lm nw bi translated">拉丁1编码</h2><p id="a980" class="pw-post-body-paragraph lw lx iq ly b lz ma jr mb mc md ju me mf mg mh mi mj mk ml mm mn mo mp mq mr ij bi translated">在撰写本文时，<code class="fe oh oi oj ny b">pspd.read_csv()</code>还不支持latin-1编码。相反，使用<code class="fe oh oi oj ny b">pspd.read_csv(encoding='Windows-1252')</code>这两种编码并不相同，但它们很接近。</p><p id="4242" class="pw-post-body-paragraph lw lx iq ly b lz ms jr mb mc mt ju me mf mu mh mi mj mv ml mm mn mw mp mq mr ij bi translated">要跟踪此问题，请参见<a class="ae kv" href="https://issues.apache.org/jira/browse/SPARK-37181" rel="noopener ugc nofollow" target="_blank">https://issues.apache.org/jira/browse/SPARK-37181</a>。</p><h2 id="019c" class="nl kx iq bd ky nm nn dn lc no np dp lg mf nq nr li mj ns nt lk mn nu nv lm nw bi translated">to_numeric(错误=)</h2><p id="5aa0" class="pw-post-body-paragraph lw lx iq ly b lz ma jr mb mc md ju me mf mg mh mi mj mk ml mm mn mo mp mq mr ij bi translated">pandas代码的一个常见行如下所示，它将文本数字转换成适当的数字数据类型。</p><pre class="kg kh ki kj gt nx ny nz oa aw ob bi"><span id="cf61" class="nl kx iq ny b gy oc od l oe of">VaxDF["CompleteCount"] = \<br/>    pd.to_numeric(VaxDF["CompleteCount"], errors='coerce')\<br/>    .fillna(0)\<br/>    .astype(int)</span></pre><p id="cb28" class="pw-post-body-paragraph lw lx iq ly b lz ms jr mb mc mt ju me mf mu mh mi mj mv ml mm mn mw mp mq mr ij bi translated"><code class="fe oh oi oj ny b">errors='coerce'</code>参数导致任何错误的值被设置为NaN。但是pspd.to_numeric()不支持Spark 3.2中的errors参数。幸运的是，pspd的默认行为是模仿上面的，所以下面的代码就像包含了<code class="fe oh oi oj ny b">errors='coerce'.</code>一样工作</p><pre class="kg kh ki kj gt nx ny nz oa aw ob bi"><span id="55c1" class="nl kx iq ny b gy oc od l oe of">VaxDF["CompleteCount"] = \<br/>    pspd.to_numeric(VaxDF["CompleteCount"])\<br/>    .fillna(0)\<br/>    .astype(int)</span></pre><p id="5c45" class="pw-post-body-paragraph lw lx iq ly b lz ms jr mb mc mt ju me mf mu mh mi mj mv ml mm mn mw mp mq mr ij bi translated">要跟踪此问题，请参见<a class="ae kv" href="https://issues.apache.org/jira/browse/SPARK-36609" rel="noopener ugc nofollow" target="_blank">https://issues.apache.org/jira/browse/SPARK-36609</a>。请注意，此修复将提交给Spark代码库，并将出现在Spark 3.3中。</p><h2 id="55ca" class="nl kx iq bd ky nm nn dn lc no np dp lg mf nq nr li mj ns nt lk mn nu nv lm nw bi translated">DF[列]。地图()。菲尔娜</h2><p id="7702" class="pw-post-body-paragraph lw lx iq ly b lz ma jr mb mc md ju me mf mg mh mi mj mk ml mm mn mo mp mq mr ij bi translated">习语。地图()。fillna()可用于控制在字典不包含某个键的情况下设置什么值。酪之后不支持fillna()。PySpark Pandas 3.2中的map()，所以改一行这样的:</p><pre class="kg kh ki kj gt nx ny nz oa aw ob bi"><span id="4112" class="nl kx iq ny b gy oc od l oe of">DF["state_abbr"] = \ <br/>    DF['state'].map(us_state_to_abbrev).fillna(DF["state"])</span></pre><p id="2801" class="pw-post-body-paragraph lw lx iq ly b lz ms jr mb mc mt ju me mf mu mh mi mj mv ml mm mn mw mp mq mr ij bi translated">对此:</p><pre class="kg kh ki kj gt nx ny nz oa aw ob bi"><span id="5226" class="nl kx iq ny b gy oc od l oe of">DF["state_abbr"] = DF['state'].map(us_state_to_abbrev)</span></pre><p id="5201" class="pw-post-body-paragraph lw lx iq ly b lz ms jr mb mc mt ju me mf mu mh mi mj mv ml mm mn mw mp mq mr ij bi translated">确保字典包含所有需要的键，或者为缺少的键提供有意义的值。</p><p id="94f6" class="pw-post-body-paragraph lw lx iq ly b lz ms jr mb mc mt ju me mf mu mh mi mj mv ml mm mn mw mp mq mr ij bi translated">追踪本期:<a class="ae kv" href="https://issues.apache.org/jira/browse/SPARK-37183" rel="noopener ugc nofollow" target="_blank">https://issues.apache.org/jira/browse/SPARK-37183</a></p><h2 id="6e2e" class="nl kx iq bd ky nm nn dn lc no np dp lg mf nq nr li mj ns nt lk mn nu nv lm nw bi translated">. str.split删除即可</h2><p id="7c50" class="pw-post-body-paragraph lw lx iq ly b lz ma jr mb mc md ju me mf mg mh mi mj mk ml mm mn mo mp mq mr ij bi translated">在普通的熊猫中，你可以像这样去掉一个后缀:</p><p id="3b54" class="pw-post-body-paragraph lw lx iq ly b lz ms jr mb mc mt ju me mf mu mh mi mj mv ml mm mn mw mp mq mr ij bi translated"><code class="fe oh oi oj ny b">DF["column"] = DF["column"].str.split("suffix").str[0]</code></p><p id="8d74" class="pw-post-body-paragraph lw lx iq ly b lz ms jr mb mc mt ju me mf mu mh mi mj mv ml mm mn mw mp mq mr ij bi translated">在pyspark.pandas 3.2中，这种说法不起作用。相反，请这样做:</p><p id="d157" class="pw-post-body-paragraph lw lx iq ly b lz ms jr mb mc mt ju me mf mu mh mi mj mv ml mm mn mw mp mq mr ij bi translated"><code class="fe oh oi oj ny b">DF["column"] = DF["column"].str.replace("suffix", '', 1)</code></p><p id="52f6" class="pw-post-body-paragraph lw lx iq ly b lz ms jr mb mc mt ju me mf mu mh mi mj mv ml mm mn mw mp mq mr ij bi translated">如果后缀只在末尾出现一次，两者都将正常工作。但是，请注意，如果后缀不止一次出现或者出现在整个字符串的中间，这两行代码的行为会有所不同。</p><p id="3f69" class="pw-post-body-paragraph lw lx iq ly b lz ms jr mb mc mt ju me mf mu mh mi mj mv ml mm mn mw mp mq mr ij bi translated">追踪本期:<a class="ae kv" href="https://issues.apache.org/jira/browse/SPARK-37184" rel="noopener ugc nofollow" target="_blank">https://issues.apache.org/jira/browse/SPARK-37184</a></p><h2 id="bc21" class="nl kx iq bd ky nm nn dn lc no np dp lg mf nq nr li mj ns nt lk mn nu nv lm nw bi translated">offsets()来调整日期时间</h2><p id="f887" class="pw-post-body-paragraph lw lx iq ly b lz ma jr mb mc md ju me mf mg mh mi mj mk ml mm mn mo mp mq mr ij bi translated">在常规的pandas中，您可以使用pandas.offsets来创建一个时间增量，允许这样的行:</p><p id="334b" class="pw-post-body-paragraph lw lx iq ly b lz ms jr mb mc mt ju me mf mu mh mi mj mv ml mm mn mw mp mq mr ij bi translated"><code class="fe oh oi oj ny b">this_period_start = OVERALL_START_DATE + pd.offsets.Day(NN)</code></p><p id="a6c7" class="pw-post-body-paragraph lw lx iq ly b lz ms jr mb mc mt ju me mf mu mh mi mj mv ml mm mn mw mp mq mr ij bi translated">这在pyspark.pandas 3.2中不起作用。改为写:</p><pre class="kg kh ki kj gt nx ny nz oa aw ob bi"><span id="b207" class="nl kx iq ny b gy oc od l oe of">import datetime</span><span id="04e5" class="nl kx iq ny b gy og od l oe of">this_period_start = OVERALL_START_DATE + datetime.timedelta(days=NN)</span></pre><p id="9043" class="pw-post-body-paragraph lw lx iq ly b lz ms jr mb mc mt ju me mf mu mh mi mj mv ml mm mn mw mp mq mr ij bi translated">追踪本期:<a class="ae kv" href="https://issues.apache.org/jira/browse/SPARK-37186" rel="noopener ugc nofollow" target="_blank">https://issues.apache.org/jira/browse/SPARK-37186</a></p><h2 id="b663" class="nl kx iq bd ky nm nn dn lc no np dp lg mf nq nr li mj ns nt lk mn nu nv lm nw bi translated">大型数据框架中一列的直方图</h2><p id="2828" class="pw-post-body-paragraph lw lx iq ly b lz ma jr mb mc md ju me mf mg mh mi mj mk ml mm mn mo mp mq mr ij bi translated">当试图从一个较大的数据帧中创建一个列的直方图时，我的代码总是崩溃。下面一行产生了如下所示的错误:</p><pre class="kg kh ki kj gt nx ny nz oa aw ob bi"><span id="fe74" class="nl kx iq ny b gy oc od l oe of">DF.plot.hist(column=”FullVaxPer100", bins=20) # there are many other columns</span><span id="5c8c" class="nl kx iq ny b gy og od l oe of">cannot resolve ‘least(min(EndDate), min(EndDeaths), min(`STATE-COUNTY`), min(StartDate), min(StartDeaths), min(POPESTIMATE2020), min(ST_ABBR), min(VaxStartDate), min(Series_Complete_Yes_Start), min(Administered_Dose1_Recip_Start), ...’ due to data type mismatch: The expressions should all have the same type, got LEAST(timestamp, bigint, string, timestamp, bigint, bigint, string, timestamp, bigint, bigint, timestamp, bigint, bigint,...).;</span></pre><p id="13f3" class="pw-post-body-paragraph lw lx iq ly b lz ms jr mb mc mt ju me mf mu mh mi mj mv ml mm mn mw mp mq mr ij bi translated">奇怪的是，pyspark.pandas似乎在所有的列上操作，而只需要一个列。</p><p id="01d8" class="pw-post-body-paragraph lw lx iq ly b lz ms jr mb mc mt ju me mf mu mh mi mj mv ml mm mn mw mp mq mr ij bi translated">作为一种解决方法，首先提取列，然后绘制直方图:</p><pre class="kg kh ki kj gt nx ny nz oa aw ob bi"><span id="2608" class="nl kx iq ny b gy oc od l oe of">OneColumnDF = FullDF["FullVaxPer100"]</span><span id="d51d" class="nl kx iq ny b gy og od l oe of">OneColumnDF.plot.hist(bins=20, title="US Counties - FullVaxPer100")</span></pre><p id="6f47" class="pw-post-body-paragraph lw lx iq ly b lz ms jr mb mc mt ju me mf mu mh mi mj mv ml mm mn mw mp mq mr ij bi translated">追踪本期:<a class="ae kv" href="https://issues.apache.org/jira/browse/SPARK-37187" rel="noopener ugc nofollow" target="_blank">https://issues.apache.org/jira/browse/SPARK-37187</a>。</p><h2 id="92b5" class="nl kx iq bd ky nm nn dn lc no np dp lg mf nq nr li mj ns nt lk mn nu nv lm nw bi translated">直方图标题不起作用</h2><p id="abb0" class="pw-post-body-paragraph lw lx iq ly b lz ma jr mb mc md ju me mf mg mh mi mj mk ml mm mn mo mp mq mr ij bi translated">在pyspark.pandas 3.2中，直方图的title参数没有任何作用。所以这个命令:</p><pre class="kg kh ki kj gt nx ny nz oa aw ob bi"><span id="c20e" class="nl kx iq ny b gy oc od l oe of">DF.plot.hist(bins=20, title="US Counties – FullVaxPer100")</span></pre><p id="00f1" class="pw-post-body-paragraph lw lx iq ly b lz ms jr mb mc mt ju me mf mu mh mi mj mv ml mm mn mw mp mq mr ij bi translated">运行时没有错误，但标题不与直方图一起显示。</p><p id="ac3b" class="pw-post-body-paragraph lw lx iq ly b lz ms jr mb mc mt ju me mf mu mh mi mj mv ml mm mn mw mp mq mr ij bi translated">作为一种变通方法，使用普通的print()语句在绘图之前或之后输出标题。</p><p id="7b17" class="pw-post-body-paragraph lw lx iq ly b lz ms jr mb mc mt ju me mf mu mh mi mj mv ml mm mn mw mp mq mr ij bi translated">追踪本期:<a class="ae kv" href="https://issues.apache.org/jira/browse/SPARK-37188" rel="noopener ugc nofollow" target="_blank">https://issues.apache.org/jira/browse/SPARK-37188</a></p><h2 id="f52a" class="nl kx iq bd ky nm nn dn lc no np dp lg mf nq nr li mj ns nt lk mn nu nv lm nw bi translated">直方图范围不起作用</h2><p id="92a8" class="pw-post-body-paragraph lw lx iq ly b lz ma jr mb mc md ju me mf mg mh mi mj mk ml mm mn mo mp mq mr ij bi translated">在pyspark.pandas 3.2中，直方图的范围参数不起任何作用。所以这个命令:</p><pre class="kg kh ki kj gt nx ny nz oa aw ob bi"><span id="bc12" class="nl kx iq ny b gy oc od l oe of">DF.plot.hist(bins=20, range=[0, 50], title="US Counties")</span></pre><p id="ad9c" class="pw-post-body-paragraph lw lx iq ly b lz ms jr mb mc mt ju me mf mu mh mi mj mv ml mm mn mw mp mq mr ij bi translated">运行时没有错误，但显示的值不限于0到50之间。</p><p id="ae9b" class="pw-post-body-paragraph lw lx iq ly b lz ms jr mb mc mt ju me mf mu mh mi mj mv ml mm mn mw mp mq mr ij bi translated">一种解决方法是，在运行直方图之前选择范围。</p><pre class="kg kh ki kj gt nx ny nz oa aw ob bi"><span id="a534" class="nl kx iq ny b gy oc od l oe of">OneRangeDF = (DF[DF.DeathsPer100k &lt;= 50])["DeathsPer100k"]<br/>print ("US Counties -- DeathsPer100k (&lt;=50)")<br/>OneRangeDF.plot.hist(bins=20)</span></pre><p id="ab37" class="pw-post-body-paragraph lw lx iq ly b lz ms jr mb mc mt ju me mf mu mh mi mj mv ml mm mn mw mp mq mr ij bi translated">追踪本期:<a class="ae kv" href="https://issues.apache.org/jira/browse/SPARK-37189" rel="noopener ugc nofollow" target="_blank">https://issues.apache.org/jira/browse/SPARK-37189</a>。</p></div><div class="ab cl ol om hu on" role="separator"><span class="oo bw bk op oq or"/><span class="oo bw bk op oq or"/><span class="oo bw bk op oq"/></div><div class="ij ik il im in"><p id="0efd" class="pw-post-body-paragraph lw lx iq ly b lz ms jr mb mc mt ju me mf mu mh mi mj mv ml mm mn mw mp mq mr ij bi translated">就像我上面说的，星火上的熊猫效果不错。上面链接的pandas代码在Spark上运行并产生了正确的结果，给出了这里概述的小的代码更改。</p><p id="3716" class="pw-post-body-paragraph lw lx iq ly b lz ms jr mb mc mt ju me mf mu mh mi mj mv ml mm mn mw mp mq mr ij bi translated">欢迎对本文进行评论和更正。</p></div></div>    
</body>
</html>