<html>
<head>
<title>Temporal Fusion Transformer: Time Series Forecasting with Interpretability</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">时间融合转换器:具有可解释性的时间序列预测</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/temporal-fusion-transformer-googles-model-for-interpretable-time-series-forecasting-5aa17beb621?source=collection_archive---------1-----------------------#2021-11-23">https://towardsdatascience.com/temporal-fusion-transformer-googles-model-for-interpretable-time-series-forecasting-5aa17beb621?source=collection_archive---------1-----------------------#2021-11-23</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="16fb" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">谷歌最先进的变形金刚应有尽有</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/168aa29805806e81887d0b02b6e96212.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*IZtzJZjGHjTPrCIf"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">照片由<a class="ae ky" href="https://unsplash.com/@jjying?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> JJ英</a>在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a></p></figure><h1 id="7451" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">预赛</h1><p id="3662" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">首先，让我们明确一点:为单个时间序列(无论是单变量还是多变量)定制模型的时代已经一去不复返了。</p><p id="7e07" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">如今大数据时代，新数据点的创造极其廉价。想象一下，一家大型电气公司拥有数千个传感器来测量不同实体(例如家庭、工厂)的功耗，或者拥有大量股票、共同基金、债券等的投资组合。换句话说，时间序列可能是多元的，具有不同的分布，并可能伴随着额外的探索性变量。当然，不要忘记通常的疑点:缺失数据、趋势、季节性、波动性、漂移和罕见事件！为了在预测能力方面创建一个有竞争力的模型，除了历史数据之外，所有变量都应该考虑在内。</p><p id="a050" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">让我们后退一步，重新思考一个最新的时间序列模型应该考虑哪些规格:</p><ol class=""><li id="f700" class="ms mt it lt b lu mn lx mo ma mu me mv mi mw mm mx my mz na bi translated">显然，该模型应适用于单维或多维序列。</li><li id="636a" class="ms mt it lt b lu nb lx nc ma nd me ne mi nf mm mx my mz na bi translated">该模型应该考虑多个时间序列，最好是数千个。不要将此与多元时间序列混淆。它意味着具有不同分布的时间序列，在单一模型上训练。</li><li id="0dc7" class="ms mt it lt b lu nb lx nc ma nd me ne mi nf mm mx my mz na bi translated">除了时间数据之外，模型应该能够使用未来未知的历史信息。例如，如果我们要创建一个预测空气污染水平的模型，我们希望能够使用湿度作为外部时间序列，这是到目前为止才知道的。例如，所有的自回归方法(如ARIMA模型)包括<strong class="lt iu">亚马逊的DeepAR </strong> [1]都受到这个限制。</li><li id="5ab1" class="ms mt it lt b lu nb lx nc ma nd me ne mi nf mm mx my mz na bi translated">非时间性的外部静态变量也应考虑在内。比如不同城市的天气预报(城市是静态变量)。</li><li id="178e" class="ms mt it lt b lu nb lx nc ma nd me ne mi nf mm mx my mz na bi translated">该模型应该具有极强的适应性。时间序列可能相当复杂或嘈杂，而其他时间序列可以简单地用季节性朴素预测器建模。理想情况下，模型应该能够区分这些情况。</li><li id="c0f7" class="ms mt it lt b lu nb lx nc ma nd me ne mi nf mm mx my mz na bi translated">多步预测功能也是必须的。递归提供预测的先行一步预测模型也可以工作。然而，请记住，对于长期预测，误差开始累积。</li><li id="5605" class="ms mt it lt b lu nb lx nc ma nd me ne mi nf mm mx my mz na bi translated">在许多情况下，仅仅预测目标变量是不够的。该算法还应该能够输出反映预测不确定性的预测区间。</li><li id="b731" class="ms mt it lt b lu nb lx nc ma nd me ne mi nf mm mx my mz na bi translated">理想模型易于使用，并且可以在生产环境中无缝部署。</li><li id="5fa7" class="ms mt it lt b lu nb lx nc ma nd me ne mi nf mm mx my mz na bi translated">最后但同样重要的是，过去几年的“黑箱模型”已经开始不受欢迎。可解释性现在已经成为头等大事，尤其是在生产方面。在某些情况下，可解释性比准确性更受青睐。</li></ol><blockquote class="ng nh ni"><p id="d3a5" class="lr ls nj lt b lu mn ju lw lx mo jx lz nk mp mc md nl mq mg mh nm mr mk ml mm im bi translated"><strong class="lt iu">注:</strong>关于时间融合变压器的动手项目，查看这篇<a class="ae ky" href="https://medium.com/p/d32c1e51cd91" rel="noopener">文章</a>。另外，查看我的<a class="ae ky" href="/@nikoskafritsas/list/timeseries-deep-learning-ultimate-collection-3955c636a768" rel="noopener ugc nofollow" target="_blank">最佳深度学习预测模型列表</a>。</p></blockquote><h1 id="d47a" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated"><strong class="ak">进入时间融合变压器(TFT) </strong></h1><p id="d082" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">什么是<strong class="lt iu">时间融合转换器</strong>？<strong class="lt iu">时间融合转换器</strong> (TFT)是一个基于注意力的深度神经网络，针对出色的性能和可解释性进行了优化。在深入研究这种酷炫架构的细节之前，我们先简要描述一下它的优势和新颖之处:</p><ol class=""><li id="5794" class="ms mt it lt b lu mn lx mo ma mu me mv mi mw mm mx my mz na bi translated"><strong class="lt iu">丰富的特性</strong> : TFT支持3种类型的特性:I)输入到未来的已知时态数据ii)目前已知的时态数据iii)外生分类/静态变量，也称为<strong class="lt iu">时不变</strong>特性。</li><li id="56a5" class="ms mt it lt b lu nb lx nc ma nd me ne mi nf mm mx my mz na bi translated"><strong class="lt iu">异构时间序列:</strong>支持来自不同分布的多个时间序列的训练。为了实现这一点，TFT架构将处理分为两个部分:局部处理，侧重于特定事件的特征；全局处理，捕捉所有时间序列的集体特征。</li><li id="2b6b" class="ms mt it lt b lu nb lx nc ma nd me ne mi nf mm mx my mz na bi translated"><strong class="lt iu">多时段预测</strong>:支持多步预测。除了实际预测，TFT还通过使用分位数损失函数输出预测区间。</li><li id="55ff" class="ms mt it lt b lu nb lx nc ma nd me ne mi nf mm mx my mz na bi translated"><strong class="lt iu">可解释性:</strong>TFT的核心是基于变压器的架构。通过利用自我注意，该模型提出了一种新的多头注意机制，当对其进行分析时，可以提供关于特征重要性的额外见解。例如，<em class="nj">多时域分位数递归预测器(MQRNN) </em> [3]是另一个DNN实现，具有良好的性能，但不提供任何关于特征可解释性的见解。</li><li id="4fc6" class="ms mt it lt b lu nb lx nc ma nd me ne mi nf mm mx my mz na bi translated"><strong class="lt iu">高性能:</strong>在基准测试期间，TFT的表现超过了传统的统计模型(ARIMA)以及基于DNN的模型，如<em class="nj"> DeepAR </em>、<em class="nj"> MQRNN </em>和<em class="nj">深空状态模型(DSSM)</em>【4】。</li><li id="eee7" class="ms mt it lt b lu nb lx nc ma nd me ne mi nf mm mx my mz na bi translated"><strong class="lt iu">文档:</strong>虽然这是一个相对较新的模型，但是在Tensorflow和Python中都已经有TFT的开源实现。</li></ol><p id="8c05" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><strong class="lt iu">图1 </strong>显示了<strong class="lt iu">时间融合变换器</strong>的顶层架构:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nn"><img src="../Images/20d63e0ea348ac8b04c8818d77d1c7cf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7rXe_MVn5QI9oLP2vrMdvQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图TFT的顶层架构及其主要组件(<a class="ae ky" href="https://arxiv.org/pdf/1912.09363.pdf" rel="noopener ugc nofollow" target="_blank">来源</a>)</p></figure><p id="fb5c" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">虽然这张图片看起来有点吓人，但这个模型实际上很容易理解。</p><p id="8a67" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">对于给定的时间步长<code class="fe no np nq nr b">t</code>、回顾窗口<code class="fe no np nq nr b">k</code>和前一步窗口<code class="fe no np nq nr b">τmax</code>，其中<code class="fe no np nq nr b">t</code> ⋹ <code class="fe no np nq nr b">[t-k..t+τmax]</code>，模型将以下作为输入:I)在时间段<code class="fe no np nq nr b">[t-k..t]</code>内观察到的过去输入<code class="fe no np nq nr b">x</code>、在时间段<code class="fe no np nq nr b">[t+1..t+τmax]</code>内的未来已知输入<code class="fe no np nq nr b">x</code>和一组静态变量<code class="fe no np nq nr b">s</code>(如果存在)。目标变量<code class="fe no np nq nr b">y</code>也跨越时间窗口<code class="fe no np nq nr b">[t+1..t+τmax]</code>。</p><p id="c586" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">接下来，我们将一步一步地描述所有单个组件以及它们如何协同工作。</p><p id="f617" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><strong class="lt iu">门控剩余网络(GRN) </strong></p><p id="de44" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><strong class="lt iu">图2 </strong>显示了论文提出的一个组件，称为<strong class="lt iu">门控残差网络(GRN) </strong>，它在整个TFT中被多次用作基本块。该网络的要点如下:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/129d78a37ea9f3228375d4f204cfb505.png" data-original-src="https://miro.medium.com/v2/resize:fit:684/format:webp/1*9eIgK7rVAwnXyHje2YKjkA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图2:门控剩余网络(<a class="ae ky" href="https://arxiv.org/pdf/1912.09363.pdf" rel="noopener ugc nofollow" target="_blank">来源</a>)</p></figure><ul class=""><li id="4f91" class="ms mt it lt b lu mn lx mo ma mu me mv mi mw mm nt my mz na bi translated">它有两个致密层和两种类型的激活函数，称为<strong class="lt iu"> ELU(指数线性单位)</strong>和<strong class="lt iu"> GLU(门控线性单位)</strong>。GLU首先用于<a class="ae ky" href="https://paperswithcode.com/paper/language-modeling-with-gated-convolutional" rel="noopener ugc nofollow" target="_blank">门控卷积网络</a>【5】架构，用于选择预测下一个单词的最重要特征。事实上，这两个激活函数都有助于网络理解哪些输入转换是简单的，哪些需要更复杂的建模。</li><li id="2b37" class="ms mt it lt b lu nb lx nc ma nd me ne mi nf mm nt my mz na bi translated">最终输出通过标准图层标准化。GRN还包含一个剩余连接，这意味着如果有必要，网络可以学习完全跳过输入。在某些情况下，根据GRN的位置，网络也可以利用静态变量。</li></ul><p id="14c8" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><strong class="lt iu">变量选择网络(VSN) </strong></p><p id="80b4" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">该组件如图3所示。顾名思义，它的功能是一种特征选择机制。记住我们之前说过的:不是所有的时间序列都是复杂的。该模型应该能够区分有洞察力的特征和有噪声的特征。此外，由于有3种类型的输入，TFT使用变量选择网络的3个实例。因此，每个实例具有不同的权重(注意图1 中<strong class="lt iu">每个VSN单元的不同颜色)。</strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/1374525645d74ab308deeae6e437d5c8.png" data-original-src="https://miro.medium.com/v2/resize:fit:688/format:webp/1*mcf8w_N_kT6Jln94TXFC7w.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图3:变量选择网络(<a class="ae ky" href="https://arxiv.org/pdf/1912.09363.pdf" rel="noopener ugc nofollow" target="_blank">来源</a>)</p></figure><p id="5e50" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">自然地，<strong class="lt iu"> VSN </strong>利用发动机罩下的GRN实现其过滤功能。它是这样工作的:</p><ul class=""><li id="f91a" class="ms mt it lt b lu mn lx mo ma mu me mv mi mw mm nt my mz na bi translated">在时间<code class="fe no np nq nr b">t</code>处，相应回看周期的所有过去输入(称为<code class="fe no np nq nr b">Ξ_t</code>)的展平向量通过GRN单元(蓝色)和softmax函数馈送，产生权重<code class="fe no np nq nr b">u</code>的归一化向量。</li><li id="8e8c" class="ms mt it lt b lu nb lx nc ma nd me ne mi nf mm nt my mz na bi translated">此外，每个特性都通过自己的GRN，这导致创建一个名为<code class="fe no np nq nr b">ξ_t</code>的处理过的向量，每个变量一个。</li><li id="85e5" class="ms mt it lt b lu nb lx nc ma nd me ne mi nf mm nt my mz na bi translated">最后，输出被计算为<code class="fe no np nq nr b">ξ_t</code>和<code class="fe no np nq nr b">u</code>的线性组合。</li><li id="4dd3" class="ms mt it lt b lu nb lx nc ma nd me ne mi nf mm nt my mz na bi translated">请注意，每个特征都有其自己的GRN，但在同一回望周期内，每个特征的GRN在所有时间步长上都是相同的。</li><li id="6f8b" class="ms mt it lt b lu nb lx nc ma nd me ne mi nf mm nt my mz na bi translated">静态变量的VSN不考虑上下文向量<code class="fe no np nq nr b">c</code></li></ul><p id="8a34" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><strong class="lt iu"> LSTM编码器解码层</strong></p><p id="4d4d" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">LSTM编码器解码器层是许多实现的一部分，尤其是在NLP中。显示在图1<strong class="lt iu">中。这个部件有两个用途:</strong></p><p id="1109" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">到目前为止，输入已经通过了<strong class="lt iu"> VSN </strong>，并且已经对特征进行了适当的编码和加权。然而，由于我们的输入是时间序列数据，该模型还应该理解时间/序列排序。因此，LSTM编码器/解码器模块的第一个目标是产生上下文感知嵌入，称为<code class="fe no np nq nr b">φ</code>。这类似于经典变压器中使用的位置编码，我们将正弦和余弦信号相加。但是为什么作者选择LSTM编码器而不是解码器呢？</p><p id="e75f" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">因为模型应该考虑所有类型的输入。已知的过去输入被馈送到编码器，而已知的未来输入被馈送到解码器。静态信息呢？是否有可能将LSTM编码器解码器产生的上下文感知嵌入与静态变量的上下文向量<code class="fe no np nq nr b">c</code>合并？</p><p id="be06" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">不幸的是，这是不准确的，因为我们将时间信息与静态信息混合在一起。正确的做法是应用[6]使用的方法，根据外部数据正确调节输入:具体来说，不是将LSTM的初始<code class="fe no np nq nr b">h_0</code>隐藏状态和单元状态<code class="fe no np nq nr b">c_0</code>设置为0，而是分别用<code class="fe no np nq nr b">c_h</code>和<code class="fe no np nq nr b">c_c</code>向量(由TFT的静态协变编码器产生)进行初始化。因此，最终的上下文感知嵌入<code class="fe no np nq nr b">φ</code>将适当地以外部信息为条件，而不改变时间动态。</p><p id="176b" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><strong class="lt iu">可解读的多头注意力</strong></p><p id="9a4a" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">这是TFT架构的最后一部分。在这个步骤中，应用了熟悉的自我注意机制[7],这有助于模型学习跨不同时间步骤的长期依赖性。</p><p id="e389" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">所有基于变压器的架构都利用注意力来学习输入数据之间的复杂依赖关系。如果您不熟悉基于注意力的实现，请查看这个资源[8](这是理解Transformer模型的最佳在线资源)。</p><p id="3183" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><strong class="lt iu">时间融合转换器</strong>提出了一种新颖的<strong class="lt iu">可解释</strong> <strong class="lt iu">多头注意力</strong>机制，与标准实现相反，它提供了特征可解释性。在原始架构中，有不同的“头”(查询/关键字/值权重矩阵)，以便将输入投射到不同的表示子空间。这种方法的缺点是权重矩阵没有共同点，因此无法解释。TFT的多头注意力增加了一个新的矩阵/分组，使得不同的头共享一些权重，然后可以根据季节性分析进行解释。</p><p id="0f18" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><strong class="lt iu">分位数回归</strong></p><p id="20ed" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">在许多涉及时间序列预测的应用中，仅仅预测目标变量是不够的。同样重要的是估计预测的不确定性。通常，这以预测间隔的形式出现。如果我们决定在输出中包含预测区间，线性回归和均方误差将不再适用。</p><p id="9b20" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">标准线性回归使用普通最小二乘法(OLS)来计算不同特征值的目标变量的条件均值。OLS解的预测区间基于残差具有恒定方差的假设，但事实并非总是如此。另一方面，<strong class="lt iu">分位数回归，</strong>是标准线性回归的扩展，估计目标变量的条件中值，可在不满足线性回归假设时使用。除了中位数之外，分位数回归还可以计算0.25和0.75分位数(或任何百分点)，这意味着模型能够输出实际预测值周围的预测区间。<strong class="lt iu">图</strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nv"><img src="../Images/4eafb0dbf673a2194e961c7b055cc178.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zi61Rx7zjIJSepE05WXDIg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图4:分位数回归来源:<a class="ae ky" href="https://upload.wikimedia.org/wikipedia/commons/thumb/d/de/Quantilsregression.svg/1024px-Quantilsregression.svg.png" rel="noopener ugc nofollow" target="_blank">维基百科</a></p></figure><p id="9250" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">给定<strong class="lt iu"><em class="nj"/></strong>和<strong class="lt iu"/>分别为实际值和预测值，并且<code class="fe no np nq nr b">q</code> <strong class="lt iu"> </strong>为0和1之间的分位数值，分位数损失函数定义为:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nw"><img src="../Images/cb7d54c252964589a61a7a14f0afa915.png" data-original-src="https://miro.medium.com/v2/resize:fit:1020/format:webp/1*xPxuEBpciKeaMi7Q4L_nRw.png"/></div></figure><p id="f70f" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">随着<code class="fe no np nq nr b">q</code>值的增加，与低估相比，高估会受到更大的惩罚。例如，对于等于0.75的<code class="fe no np nq nr b">q</code>，高估将被罚因子0.75，低估将被罚因子0.25。这就是预测区间的创建方式。</p><p id="9615" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">通过最小化在<code class="fe no np nq nr b">q</code> ⋹ [0.1，0.5，0.9]上求和的分位数损失来训练<strong class="lt iu">时间融合变换器</strong>的实现。这样做是为了进行基准测试，以便与其他流行型号使用的实验配置相匹配。此外，不言而喻，分位数损失的使用不是排他性的-可以使用其他类型的损失函数，如MSE，MAPE等。</p><h1 id="16eb" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">Python实现</h1><p id="f75d" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">在原始论文中，<strong class="lt iu">时间融合转换器</strong>模型与其他流行的时间序列模型如DeepAR、ARIMA等进行了比较。作者用于基准测试的一些数据集是:</p><ul class=""><li id="2ed2" class="ms mt it lt b lu mn lx mo ma mu me mv mi mw mm nt my mz na bi translated"><a class="ae ky" href="https://archive.ics.uci.edu/ml/datasets/ElectricityLoadDiagrams20112014#" rel="noopener ugc nofollow" target="_blank">电力负荷图表数据集</a>(UCI)【9】</li><li id="677b" class="ms mt it lt b lu nb lx nc ma nd me ne mi nf mm nt my mz na bi translated"><a class="ae ky" href="https://archive.ics.uci.edu/ml/datasets/PEMS-SF" rel="noopener ugc nofollow" target="_blank"> PEM-SF交通数据集</a>(UCI)【9】</li><li id="9835" class="ms mt it lt b lu nb lx nc ma nd me ne mi nf mm nt my mz na bi translated"><a class="ae ky" href="https://www.kaggle.com/azzabiala/corporacin-favorita-grocery-sales-forecasting" rel="noopener ugc nofollow" target="_blank"> Favorita杂货店销售</a>(ka ggle)【10】</li></ul><p id="a9e2" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">有关每个数据集使用哪些配置/超参数的更多信息，请查看原始论文[2]。</p><blockquote class="nx"><p id="3dad" class="ny nz it bd oa ob oc od oe of og mm dk translated">在基准测试中，TFT的表现优于传统的统计模型(ARIMA)以及基于DNN的模型，如DeepAR、MQRNN和深空状态模型(DSSM)</p></blockquote><p id="4048" class="pw-post-body-paragraph lr ls it lt b lu oh ju lw lx oi jx lz ma oj mc md me ok mg mh mi ol mk ml mm im bi translated">此外，作者善意地提供了Tensorflow 1.x中TFT的<a class="ae ky" href="https://github.com/google-research/google-research/tree/master/tft" rel="noopener ugc nofollow" target="_blank">开源实现</a>，以及关于每个数据集的相应超参数配置，用于再现性目的。而且，你还可以在这里找到Tensorflow 2.x <a class="ae ky" href="https://github.com/greatwhiz/tft_tf2" rel="noopener ugc nofollow" target="_blank">的修改版本。</a></p><p id="fd49" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">让我们使用<a class="ae ky" href="https://archive.ics.uci.edu/ml/datasets/ElectricityLoadDiagrams20112014#" rel="noopener ugc nofollow" target="_blank">电力负荷图表数据集</a>创建一个最小工作示例，我们将简称为<em class="nj">电力</em>。该数据集包含370个消费者的电力消耗(单位为千瓦)。数据点每15分钟采样一次。在进行预测之前，首先对数据集进行预处理:</p><ol class=""><li id="1720" class="ms mt it lt b lu mn lx mo ma mu me mv mi mw mm mx my mz na bi translated">时间粒度变成每小时。</li><li id="268d" class="ms mt it lt b lu nb lx nc ma nd me ne mi nf mm mx my mz na bi translated">使用日期信息，我们创建以下(数字)特征:<code class="fe no np nq nr b">hour</code>、<code class="fe no np nq nr b">day of week</code>和<code class="fe no np nq nr b">hours from start</code>。</li><li id="4479" class="ms mt it lt b lu nb lx nc ma nd me ne mi nf mm mx my mz na bi translated"><code class="fe no np nq nr b">categorical_id</code>是每个消费者的id。</li><li id="fb06" class="ms mt it lt b lu nb lx nc ma nd me ne mi nf mm mx my mz na bi translated">目标变量是<code class="fe no np nq nr b">power_usage.</code></li><li id="5cb0" class="ms mt it lt b lu nb lx nc ma nd me ne mi nf mm mx my mz na bi translated">数据集被分成训练集、验证集和测试集。</li><li id="c9d4" class="ms mt it lt b lu nb lx nc ma nd me ne mi nf mm mx my mz na bi translated">训练数据集被归一化。具体来说，数值变量(包括目标变量)被标准化(z-归一化),并且单个分类特征被标签编码。必须理解，标准化是针对每个时间序列/消费者分别进行的，因为时间序列具有不同的特征(均值和方差)。定标器也用于将预测值恢复到原始值。</li></ol><blockquote class="ng nh ni"><p id="36fd" class="lr ls nj lt b lu mn ju lw lx mo jx lz nk mp mc md nl mq mg mh nm mr mk ml mm im bi translated">目标是通过使用上周(7*24小时)来预测第二天(1*24小时)的用电量。</p></blockquote><p id="db66" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">对于此示例，我们将使用Tensorflow 2.x的TFT更新版本。您可以在Conda中快速设置一个最小工作示例:</p><h2 id="d927" class="om la it bd lb on oo dn lf op oq dp lj ma or os ll me ot ou ln mi ov ow lp ox bi translated">Tensorflow 2.x</h2><pre class="kj kk kl km gt oy nr oz pa aw pb bi"><span id="cad1" class="om la it nr b gy pc pd l pe pf"># Download TFT. Kudos to greatwhiz for making TFT compatible to TF # 2.x!<br/>!git clone <a class="ae ky" href="https://github.com/greatwhiz/tft_tf2.git" rel="noopener ugc nofollow" target="_blank">https://github.com/greatwhiz/tft_tf2.git</a></span><span id="8c00" class="om la it nr b gy pg pd l pe pf"># Install any missing libraries in Conda environment<br/>!pip install pyunpack<br/>!pip install wget</span></pre><p id="f757" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">该实现还包含用于下载和预处理上述数据集的脚本:对于<a class="ae ky" href="https://archive.ics.uci.edu/ml/datasets/ElectricityLoadDiagrams20112014#" rel="noopener ugc nofollow" target="_blank">电力</a>数据集，执行:</p><pre class="kj kk kl km gt oy nr oz pa aw pb bi"><span id="a85e" class="om la it nr b gy pc pd l pe pf"># The structure of the command is:<br/># python3 -m script_download_data $EXPT $OUTPUT_FOLDER</span><span id="dc8b" class="om la it nr b gy pg pd l pe pf">!python3 tft_tf2/script_download_data.py electricity electricity_dataset</span></pre><p id="12ce" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">其中<code class="fe no np nq nr b">electricity_dataset</code>是存储预处理数据的文件夹。预处理数据集如下所示:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ph"><img src="../Images/44aca5056be7d36d3d1db7061c6fc041.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cQJCj4IIDMyuNbxuTWHSMQ.png"/></div></div></figure><p id="30b4" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">然而，并非所有这些变量都被考虑用于训练。该模型将利用我们上面讨论的变量。</p><p id="6258" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">最后，执行培训脚本:</p><pre class="kj kk kl km gt oy nr oz pa aw pb bi"><span id="f644" class="om la it nr b gy pc pd l pe pf"># The structure of the command is:<br/># python3 -m script_train_fixed_params $EXPT $OUTPUT_FOLDER $USE_GPU</span><span id="530b" class="om la it nr b gy pg pd l pe pf">!python3 tft_tf2/script_train_fixed_params.py electricity electricity_dataset ‘yes’</span></pre><p id="5427" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">默认情况下，该脚本在测试模式下运行，这意味着模型将只训练1个时期，并且分别只使用100和10个训练和验证实例。在<code class="fe no np nq nr b">script_train_fixed_params.py</code>集合<code class="fe no np nq nr b">use_testing_mode=True</code>中，使用原始文件中找到的最佳超参数启动完整的训练。对于完整的培训，该模型将在启用GPU的情况下在Colab上花费大约7-8个小时。</p><h2 id="88b3" class="om la it bd lb on oo dn lf op oq dp lj ma or os ll me ot ou ln mi ov ow lp ox bi translated"><strong class="ak"> Pytorch </strong></h2><p id="795c" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated"><strong class="lt iu">时间融合变压器</strong>在PyTorch中也有。查看本<a class="ae ky" href="https://pytorch-forecasting.readthedocs.io/en/latest/tutorials/stallion.html" rel="noopener ugc nofollow" target="_blank">综合教程</a>了解更多信息。</p><h1 id="d053" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">可解释性</h1><p id="7430" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">关于<strong class="lt iu">时间融合转换器</strong>最强的一点是可解释性。在时间序列问题的背景下，可解释性在许多情况下是有意义的。</p><p id="9cdb" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><strong class="lt iu">功能方面的</strong></p><p id="9a19" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">首先，<strong class="lt iu">时间融合变换器</strong>试图通过考虑预测的鲁棒性来计算每个特征的影响。可以通过分析整个测试集中所有<strong class="lt iu">变量选择网络</strong>模块的权重<code class="fe no np nq nr b">u</code>来测量特征重要性。对于<strong class="lt iu">表1 </strong>中的电力数据集，我们有:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pi"><img src="../Images/63476564959a7cebc8280eb03333d1ee.png" data-original-src="https://miro.medium.com/v2/resize:fit:698/format:webp/1*CDDJmxOgWnqekxi_rS1JUg.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">表1:电力数据集的特征重要性(<a class="ae ky" href="https://arxiv.org/pdf/1912.09363.pdf" rel="noopener ugc nofollow" target="_blank">来源</a></p></figure><p id="03de" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">所有特征分数的值都在0到1之间。<code class="fe no np nq nr b">ID</code>变量起着重要作用，因为它将一个时间序列与另一个时间序列区分开来。接下来是<code class="fe no np nq nr b">Hour of Day</code>，这是意料之中的，因为功耗在一天中遵循特定的模式。</p><p id="b148" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><strong class="lt iu">季节性</strong></p><p id="3a58" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">使用可解释的多头注意力层，我们可以更进一步，计算“持续时间模式”。更具体地说，来自该层的注意力权重可以揭示回望期间哪个时间步长是最重要的。因此，这些重量的可视化揭示了最突出的季节性。例如，在<strong class="lt iu">图5 </strong>中，我们有<strong class="lt iu"> : </strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pj"><img src="../Images/8b1aa81cdc3fd8cc92783b22a7cbdd32.png" data-original-src="https://miro.medium.com/v2/resize:fit:910/format:webp/1*a0QxOT-FAXfIg5tffU3U1g.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图5:电力数据集的时间模式(<a class="ae ky" href="https://arxiv.org/pdf/1912.09363.pdf" rel="noopener ugc nofollow" target="_blank">来源</a></p></figure><p id="80ea" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">其中<code class="fe no np nq nr b">a(t,n,1)</code>是地平线等于1(与领先一步相同)和<code class="fe no np nq nr b">n</code> ⋹ <code class="fe no np nq nr b">[-(7*24)..0]</code>的关注度得分。换句话说，该图清楚地显示了数据集呈现出每日的季节性模式。</p><h1 id="e501" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated"><strong class="ak">结束语</strong></h1><p id="047c" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">综上所述，<strong class="lt iu">时间融合变压器</strong>是一款高性能的通用机型。<strong class="lt iu">Temporal Fusion Transformer</strong>的架构融合了深度学习领域的众多关键进步，同时提出了一些自己的创新。然而，其最基本的特性是能够在预测方面提供可解释的见解。此外，根据<a class="ae ky" href="https://www.gartner.com/en/documents/3988118/hype-cycle-for-data-science-and-machine-learning-2020" rel="noopener ugc nofollow" target="_blank"> Gartner </a>的说法，这是深度学习未来的发展方向之一。</p></div><div class="ab cl pk pl hx pm" role="separator"><span class="pn bw bk po pp pq"/><span class="pn bw bk po pp pq"/><span class="pn bw bk po pp"/></div><div class="im in io ip iq"><h1 id="00b1" class="kz la it bd lb lc pr le lf lg ps li lj jz pt ka ll kc pu kd ln kf pv kg lp lq bi translated">感谢您的阅读！</h1><ul class=""><li id="dd8a" class="ms mt it lt b lu lv lx ly ma pw me px mi py mm nt my mz na bi translated">订阅我的<a class="ae ky" href="https://medium.com/subscribe/@nikoskafritsas" rel="noopener">简讯</a>！</li><li id="7dfd" class="ms mt it lt b lu nb lx nc ma nd me ne mi nf mm nt my mz na bi translated">在Linkedin上关注我！</li></ul><h1 id="64a5" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">参考</h1><p id="e449" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">[1] D. Salinas等人，<a class="ae ky" href="https://arxiv.org/pdf/1704.04110.pdf" rel="noopener ugc nofollow" target="_blank"> DeepAR:用自回归递归网络进行概率预测</a>，国际预测杂志(2019)。</p><p id="15c0" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">[2] Bryan Lim等人，<a class="ae ky" href="https://arxiv.org/pdf/1912.09363.pdf" rel="noopener ugc nofollow" target="_blank">用于可解释多时间范围时间序列预测的时间融合变换器</a>，2020年9月</p><p id="97a7" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">[3] R. Wen等，<a class="ae ky" href="https://arxiv.org/abs/1711.11053" rel="noopener ugc nofollow" target="_blank">一个多地平线分位数循环预测器</a>，NIPS，2017</p><p id="7897" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">[4] S. S. Rangapuram等，<a class="ae ky" href="https://papers.nips.cc/paper/2018/file/5cf68969fb67aa6082363a6d4e6468e2-Paper.pdf" rel="noopener ugc nofollow" target="_blank">时间序列预测的深态空间模型</a>，NIPS，2018。</p><p id="28ba" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">[5] Y. Dauphin等，<a class="ae ky" href="https://arxiv.org/pdf/1612.08083v3.pdf" rel="noopener ugc nofollow" target="_blank">用门控卷积网络进行语言建模</a>，ICML，2017</p><p id="b8a6" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">[6]安德烈·卡帕西，李菲菲，<a class="ae ky" href="https://arxiv.org/abs/1412.2306" rel="noopener ugc nofollow" target="_blank">用于生成图像描述的深度视觉语义对齐</a></p><p id="b023" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">[7] A .瓦斯瓦尼等人<a class="ae ky" href="https://arxiv.org/abs/1706.03762" rel="noopener ugc nofollow" target="_blank">关注是你所需要的全部</a>，2017年6月</p><p id="46fe" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">[8] J .阿拉玛，<a class="ae ky" href="https://jalammar.github.io/illustrated-transformer/" rel="noopener ugc nofollow" target="_blank">《图解变压器》</a></p><p id="31eb" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">[9]杜瓦和格拉夫(2019年)。UCI机器学习知识库。加州欧文:加州大学信息与计算机科学学院。</p><p id="75fa" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">[10] Favorita杂货销售预测，<a class="ae ky" href="https://www.kaggle.com/azzabiala/corporacin-favorita-grocery-sales-forecasting" rel="noopener ugc nofollow" target="_blank"> Kaggle </a>，license<br/>CC0:公共领域</p></div></div>    
</body>
</html>