<html>
<head>
<title>Temporal Loops: Intro to Recurrent Neural Networks for Time Series Forecasting in Python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">时间循环:Python中时间序列预测的递归神经网络介绍</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/temporal-loops-intro-to-recurrent-neural-networks-for-time-series-forecasting-in-python-b0398963dc1f?source=collection_archive---------0-----------------------#2021-10-31">https://towardsdatascience.com/temporal-loops-intro-to-recurrent-neural-networks-for-time-series-forecasting-in-python-b0398963dc1f?source=collection_archive---------0-----------------------#2021-10-31</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="47cc" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsdatascience.com/tagged/hands-on-tutorials" rel="noopener" target="_blank">实践教程</a></h2><div class=""/><div class=""><h2 id="5f04" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">一个关于LSTM，GRU和香草RNNs的教程-由Darts多方法预测库包装</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi kr"><img src="../Images/fb11ed2cd33541bdcb3f6a80c9de98fa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*OMUs6E4EZyFpbzw_249fTQ.jpeg"/></div><p class="kz la gj gh gi lb lc bd b be z dk translated"><a class="ae ld" href="https://pixabay.com/illustrations/people-collective-group-knowledge-3286018/" rel="noopener ugc nofollow" target="_blank"> People Collective Group，由geralt-pix abay上的免费图片</a></p></figure><h1 id="6ea8" class="le lf it bd lg lh li lj lk ll lm ln lo ki lp kj lq kl lr km ls ko lt kp lu lv bi translated">0.介绍</h1><p id="4094" class="pw-post-body-paragraph lw lx it ly b lz ma kd mb mc md kg me mf mg mh mi mj mk ml mm mn mo mp mq mr im bi translated">今天的文章将继续讨论，超越我在10月早些时候写的两篇关于时间序列预测的文章。早期的教程介绍了<em class="ms">飞镖</em>多方法预测库，在飞镖游戏中的五个预测者之间进行比赛:</p><ul class=""><li id="ac9c" class="mt mu it ly b lz mv mc mw mf mx mj my mn mz mr na nb nc nd bi translated">天真的漂移+季节性预测</li><li id="2f08" class="mt mu it ly b lz ne mc nf mf ng mj nh mn ni mr na nb nc nd bi translated">指数平滑法</li><li id="f486" class="mt mu it ly b lz ne mc nf mf ng mj nh mn ni mr na nb nc nd bi translated">θ方法</li><li id="840b" class="mt mu it ly b lz ne mc nf mf ng mj nh mn ni mr na nb nc nd bi translated">萨里玛</li><li id="0b69" class="mt mu it ly b lz ne mc nf mf ng mj nh mn ni mr na nb nc nd bi translated">脸书先知。</li></ul><p id="c727" class="pw-post-body-paragraph lw lx it ly b lz mv kd mb mc mw kg me mf nj mh mi mj nk ml mm mn nl mp mq mr im bi translated">第二篇文章将这五个组合成一个<em class="ms">集合</em>场景，形成了第六个预测者。</p><ul class=""><li id="a61d" class="mt mu it ly b lz mv mc mw mf mx mj my mn mz mr na nb nc nd bi translated"><a class="ae ld" rel="noopener" target="_blank" href="/darts-swiss-knife-for-time-series-forecasting-in-python-f37bb74c126">飞镖的时间序列预测瑞士刀|走向数据科学</a></li><li id="abb2" class="mt mu it ly b lz ne mc nf mf ng mj nh mn ni mr na nb nc nd bi translated"><a class="ae ld" rel="noopener" target="_blank" href="/wisdom-of-the-forecaster-crowd-f70b398f190b">预测者群体的智慧。面向数据科学的Python |时间序列集合预测</a></li></ul><p id="ebc3" class="pw-post-body-paragraph lw lx it ly b lz mv kd mb mc mw kg me mf nj mh mi mj nk ml mm mn nl mp mq mr im bi translated">今天的教程将提供如何将<strong class="ly jd">递归神经网络(RNNs) </strong>应用于时间序列的实践介绍:三个RNN变量将成为本系列文章中展示Python中时间序列分析的方法# 7–9。</p><p id="0ecb" class="pw-post-body-paragraph lw lx it ly b lz mv kd mb mc mw kg me mf nj mh mi mj nk ml mm mn nl mp mq mr im bi translated">Darts包装了<strong class="ly jd"> PyTorch </strong>库(由来自谷歌、Twitter和脸书等公司的数据科学家维护——其他用于神经网络的开源“OEM”包包括谷歌开发的TensorFlow和Keras，后者基于TensorFlow和Theano构建)。</p><p id="61ff" class="pw-post-body-paragraph lw lx it ly b lz mv kd mb mc mw kg me mf nj mh mi mj nk ml mm mn nl mp mq mr im bi translated">Darts提供三种类型的RNNs:</p><ul class=""><li id="ab02" class="mt mu it ly b lz mv mc mw mf mx mj my mn mz mr na nb nc nd bi translated"><strong class="ly jd"> LSTM，</strong></li><li id="0308" class="mt mu it ly b lz ne mc nf mf ng mj nh mn ni mr na nb nc nd bi translated"><strong class="ly jd"> GRU，</strong></li><li id="285a" class="mt mu it ly b lz ne mc nf mf ng mj nh mn ni mr na nb nc nd bi translated">香草。</li></ul><p id="1d95" class="pw-post-body-paragraph lw lx it ly b lz mv kd mb mc mw kg me mf nj mh mi mj nk ml mm mn nl mp mq mr im bi translated">包装将使我们能够将RNNs与飞镖中可用的其他预测方法并行使用，然后举办一场他们可以竞争的锦标赛。</p><h1 id="20a1" class="le lf it bd lg lh li lj lk ll lm ln lo ki lp kj lq kl lr km ls ko lt kp lu lv bi translated">1.递归神经网络:概念</h1><p id="ac6d" class="pw-post-body-paragraph lw lx it ly b lz ma kd mb mc md kg me mf mg mh mi mj mk ml mm mn mo mp mq mr im bi translated"><strong class="ly jd">递归神经网络(RNNs) </strong>是深度学习模型，通常用于解决时序等顺序输入数据的问题。它们是什么，我们如何在时间序列预测中使用它们？</p><p id="f3c3" class="pw-post-body-paragraph lw lx it ly b lz mv kd mb mc mw kg me mf nj mh mi mj nk ml mm mn nl mp mq mr im bi translated">RNNs是一种神经网络，它保留了已经处理过的内容的记忆，因此可以在训练期间从以前的迭代中学习。</p><p id="c81e" class="pw-post-body-paragraph lw lx it ly b lz mv kd mb mc mw kg me mf nj mh mi mj nk ml mm mn nl mp mq mr im bi translated">当我们第一次听到任何技术术语时，你可能会做我们大多数人都会做的事情。你试图通过点击顶部列出的非广告谷歌搜索结果来理解什么是递归神经网络。然后你会发现维基百科的文章展现了高度的抽象性。当我们试图理解RNN是什么以及它们的用途时，它的用处是有限的:“一个<strong class="ly jd">递归神经网络</strong> ( <strong class="ly jd"> RNN </strong>)是一类<a class="ae ld" href="https://en.wikipedia.org/wiki/Artificial_neural_network" rel="noopener ugc nofollow" target="_blank">人工神经网络</a>，其中节点之间的连接沿着时间序列形成一个<a class="ae ld" href="https://en.wikipedia.org/wiki/Directed_graph" rel="noopener ugc nofollow" target="_blank">有向图</a>。这允许它展示时间动态行为。从<a class="ae ld" href="https://en.wikipedia.org/wiki/Feedforward_neural_networks" rel="noopener ugc nofollow" target="_blank">前馈神经网络</a>衍生而来，RNNs可以使用它们的内部状态(记忆)来处理可变长度的输入序列。递归神经网络理论上是图灵完全的，可以运行任意程序来处理任意输入序列。说什么？</p><p id="6618" class="pw-post-body-paragraph lw lx it ly b lz mv kd mb mc mw kg me mf nj mh mi mj nk ml mm mn nl mp mq mr im bi translated">Michael Phi在他的前一篇《走向数据科学》文章中提供了一个关于RNNs的优秀的、非数学的指南:“<a class="ae ld" rel="noopener" target="_blank" href="/illustrated-guide-to-recurrent-neural-networks-79e5eb8049c9">回归神经网络图解指南|作者Michael Phi |走向数据科学</a>”。Will Koehrsen在“<a class="ae ld" rel="noopener" target="_blank" href="/recurrent-neural-networks-by-example-in-python-ffd204f99470">Python中的递归神经网络示例| Will Koehrsen |迈向数据科学</a>”中也是如此</p><p id="f050" class="pw-post-body-paragraph lw lx it ly b lz mv kd mb mc mw kg me mf nj mh mi mj nk ml mm mn nl mp mq mr im bi translated">让我用非数学术语总结一下我们应该了解的关于RNNs的基础知识(然后我会向你推荐迈克尔和威尔在2018年写的两篇文章中的额外解释和插图)。</p><p id="5e4c" class="pw-post-body-paragraph lw lx it ly b lz mv kd mb mc mw kg me mf nj mh mi mj nk ml mm mn nl mp mq mr im bi translated">一个神经网络——递归神经网络是其中的一种，还有其他类型，如卷积神经网络——由三个基本组件组成:输入层、隐藏层和输出层。每层由所谓的<em class="ms">节点</em>(又名<em class="ms">神经元</em>)组成。</p><p id="9528" class="pw-post-body-paragraph lw lx it ly b lz mv kd mb mc mw kg me mf nj mh mi mj nk ml mm mn nl mp mq mr im bi translated">我读过以下对三种主要类型的神经网络的类比，据说它们以特定的方式模仿人类大脑的功能。下面的比较过于简单，所以最好持保留态度。</p><ul class=""><li id="e3c0" class="mt mu it ly b lz mv mc mw mf mx mj my mn mz mr na nb nc nd bi translated">我们大脑的颞叶=&gt; <em class="ms">人工</em>神经网络= &gt;主要用于分类和回归问题= &gt;颞叶的功能之一是长期记忆</li><li id="7b1b" class="mt mu it ly b lz ne mc nf mf ng mj nh mn ni mr na nb nc nd bi translated">枕叶=&gt; <em class="ms">卷积</em>神经网络= &gt;主要用于计算机视觉问题(虽然<em class="ms">时态</em>卷积网络，TCNs，可以应用于时间序列)</li><li id="1624" class="mt mu it ly b lz ne mc nf mf ng mj nh mn ni mr na nb nc nd bi translated">额叶=&gt; <em class="ms">递归</em>神经网络RNN = &gt;主要用于时间序列分析、序列和列表——例如，在语言处理中，它处理按语法排序的字符、单词和句子的序列；或者时间序列，由观察的时间序列组成= &gt;额叶的功能之一是短期记忆</li></ul><p id="abbb" class="pw-post-body-paragraph lw lx it ly b lz mv kd mb mc mw kg me mf nj mh mi mj nk ml mm mn nl mp mq mr im bi translated">前馈神经网络(ffnn)——如神经网络中的祖父，最初的单层感知器，于1958年开发——出现在递归神经网络之前。在FFNNs中，信息只在一个方向上流动:从输入层，通过隐藏层，到输出层，但在反馈回路中不会反向。FFNN常用于模式识别。FFNN将加权因子矩阵与输入相乘，并从这些加权输入产生输出。前馈神经网络不会保留它们处理过的输入的记忆。他们患有顺行性健忘症，无法形成新的记忆(类似于克里斯托弗·诺兰电影<em class="ms">Memento—</em><a class="ae ld" href="https://en.wikipedia.org/wiki/Memento_(film)" rel="noopener ugc nofollow" target="_blank">Wikipedia</a>【这似乎是一个在数据科学文章中提及顺行性健忘症和Memento的难得机会】)。</p><p id="80fa" class="pw-post-body-paragraph lw lx it ly b lz mv kd mb mc mw kg me mf nj mh mi mj nk ml mm mn nl mp mq mr im bi translated">相比之下，<strong class="ly jd">递归</strong>神经网络保留了它在<em class="ms">最近</em>先前步骤中处理的内容的记忆(我们一会儿将回到“最近”限定词)。它通过<em class="ms">时间反馈循环</em>进行循环连接:前一步骤的输出用作当前过程步骤的输入。与失忆症FFNNs不同，这种记忆使RNNs能够处理输入序列而不会丢失轨迹。这些环路使它成为一个循环网络。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi nm"><img src="../Images/a6b48d3b5c6596ae2ef94d77a59fcce8.png" data-original-src="https://miro.medium.com/v2/resize:fit:852/format:webp/1*AXeigtBfPb8UIwsGm7rD7g.jpeg"/></div><p class="kz la gj gh gi lb lc bd b be z dk translated"><a class="ae ld" href="https://pixabay.com/photos/grape-vine-tendril-climbing-plant-592995/" rel="noopener ugc nofollow" target="_blank">葡萄藤卷须攀缘植物，由stevepb拍摄Pixabay上的免费照片</a></p></figure><p id="704a" class="pw-post-body-paragraph lw lx it ly b lz mv kd mb mc mw kg me mf nj mh mi mj nk ml mm mn nl mp mq mr im bi translated">隐藏层位于输入层和输出层之间。在RNN中，它们不仅产生输出，而且还反馈它(“反向传播”)作为下一次观察时训练隐藏层的输入。他们通过调整整个神经网络的突触权重来进行训练。网络重新校准当前和先前输入的权重，将输入值的向量与新权重的向量相乘(从而相对于降低预测误差的训练目标提高或降低它们的重要性)，并将结果的向量作为输入传递给下一层。通过调整权重，隐藏层递增地导出一种函数，该函数将输入值转换成近似训练数据集中的实际观察值的输出值。但是，将输入映射到输出的函数并没有以封闭形式的方程来表达——它仍然是隐藏的。</p><p id="b0a8" class="pw-post-body-paragraph lw lx it ly b lz mv kd mb mc mw kg me mf nj mh mi mj nk ml mm mn nl mp mq mr im bi translated">为了建立我们在神经网络描述中遇到的技术术语的词汇表，让我们浏览一些其他构建模块的列表。</p><p id="426d" class="pw-post-body-paragraph lw lx it ly b lz mv kd mb mc mw kg me mf nj mh mi mj nk ml mm mn nl mp mq mr im bi translated">例如，分析图像的神经网络对于图像中的每个像素都有一个输入节点；节点保存它的颜色值。如果RNN处理时间序列，每个周期将由一个节点表示，保存该周期的观测值。</p><p id="f734" class="pw-post-body-paragraph lw lx it ly b lz mv kd mb mc mw kg me mf nj mh mi mj nk ml mm mn nl mp mq mr im bi translated">当输入值从一层的节点传递到另一层的节点时，它沿着节点之间的<em class="ms">边</em>(连接线)传播。边缘相当于大脑的突触。</p><p id="bd0f" class="pw-post-body-paragraph lw lx it ly b lz mv kd mb mc mw kg me mf nj mh mi mj nk ml mm mn nl mp mq mr im bi translated">接收节点将其接收的所有输入相加成为一个<em class="ms">总净输入</em>。</p><p id="85e4" class="pw-post-body-paragraph lw lx it ly b lz mv kd mb mc mw kg me mf nj mh mi mj nk ml mm mn nl mp mq mr im bi translated">它将这个净输入输入到一个<em class="ms">激活函数</em>(又名<em class="ms">转移</em>函数)来计算输出:该节点将对下一层贡献多少。在经常使用的激活函数类型中，你会发现逻辑函数或sigmoid函数；step或heaviside函数(相当于布尔或二进制是/否决策)；双曲正切函数(tanh)；以及ReLU函数(整流线性单位，max(0，x))。当将输入映射到训练输出时，像逻辑或双曲正切函数这样的非线性激活函数有助于网络使自己适应非线性问题。</p><p id="f3be" class="pw-post-body-paragraph lw lx it ly b lz mv kd mb mc mw kg me mf nj mh mi mj nk ml mm mn nl mp mq mr im bi translated">当激活函数的输出值沿着一条边被发送到另一层中的一个节点，或者通过多条边被发送到一个以上的接收节点时，激活函数的输出值被乘以一个权重因子。</p><p id="e04e" class="pw-post-body-paragraph lw lx it ly b lz mv kd mb mc mw kg me mf nj mh mi mj nk ml mm mn nl mp mq mr im bi translated">递归神经网络应用的校准机制由两个术语描述，这两个术语在大多数RNN描述中都会遇到:</p><ul class=""><li id="dab4" class="mt mu it ly b lz mv mc mw mf mx mj my mn mz mr na nb nc nd bi translated"><em class="ms">穿越时间的反向传播(BPTT) </em>又名<em class="ms">时间循环</em>；</li><li id="4a9b" class="mt mu it ly b lz ne mc nf mf ng mj nh mn ni mr na nb nc nd bi translated">和<em class="ms">梯度下降</em>。</li></ul><p id="0537" class="pw-post-body-paragraph lw lx it ly b lz mv kd mb mc mw kg me mf nj mh mi mj nk ml mm mn nl mp mq mr im bi translated">当网络生成预测值时，它还会计算预测误差，即与训练数据集的偏差。网络试图通过在训练期间调整其内部权重来最小化误差。反向传播计算误差相对于权重的偏导数。然后，RNN会根据偏导数向上或向下重新校准砝码。</p><p id="efdd" class="pw-post-body-paragraph lw lx it ly b lz mv kd mb mc mw kg me mf nj mh mi mj nk ml mm mn nl mp mq mr im bi translated">术语<em class="ms">梯度下降</em>是指通过计算偏导数来搜索全局最小值(<a class="ae ld" href="https://en.wikipedia.org/wiki/Gradient_descent" rel="noopener ugc nofollow" target="_blank">梯度下降——维基百科</a>)。权重的重复调整，沿着朝向最小误差的下降，将使模型朝向逐渐减小的预测误差移动。</p><p id="bf85" class="pw-post-body-paragraph lw lx it ly b lz mv kd mb mc mw kg me mf nj mh mi mj nk ml mm mn nl mp mq mr im bi translated">RNN工艺步骤的简化顺序如下:</p><ul class=""><li id="54e3" class="mt mu it ly b lz mv mc mw mf mx mj my mn mz mr na nb nc nd bi translated">它向前传递并计算预测误差，以获得训练数据集和验证集的损失值。</li><li id="11cd" class="mt mu it ly b lz ne mc nf mf ng mj nh mn ni mr na nb nc nd bi translated">它计算每一层的梯度，并通过t个时间步长反向传播误差。然后它更新权重；并且循环到另一个向前传球。</li></ul><p id="bc6b" class="pw-post-body-paragraph lw lx it ly b lz mv kd mb mc mw kg me mf nj mh mi mj nk ml mm mn nl mp mq mr im bi translated">RNN的拟合过程被称为<em class="ms">映射</em>，以使其输出与训练数据集中的实际观测值的偏差最小化。</p><p id="6557" class="pw-post-body-paragraph lw lx it ly b lz mv kd mb mc mw kg me mf nj mh mi mj nk ml mm mn nl mp mq mr im bi translated"><em class="ms">成本函数(又名损失、误差或目标函数)</em>将网络的预测误差度量为一个数字，一个标量。RNN的目标是最小化成本函数。预测精度指标，如均方差MSE或均方根误差r MSE，我们从其他时间序列方法中知道，可以作为成本函数；你会看到提到的替代方法包括海灵格距离或库尔贝克-莱布勒散度。</p><p id="8bf9" class="pw-post-body-paragraph lw lx it ly b lz mv kd mb mc mw kg me mf nj mh mi mj nk ml mm mn nl mp mq mr im bi translated">网络计算输出节点的值与其对应的实际观测值之间的差异。它将个体差异——这些<em class="ms">局部的、</em>节点特定的误差——浓缩在成本函数中，以获得网络的<em class="ms">总误差</em>或<em class="ms">全局误差，</em>成本函数的结果。</p><p id="c522" class="pw-post-body-paragraph lw lx it ly b lz mv kd mb mc mw kg me mf nj mh mi mj nk ml mm mn nl mp mq mr im bi translated">然后，RNN研究总误差是如何在网络中的所有权重上分布的。它通过计算<em class="ms">偏导数</em>，也就是<em class="ms">梯度(【斜率】</em>)来找出它们各自对总误差的贡献。损失函数包括多个权重。相对于某一重量的偏导数揭示了该特定重量如何影响总误差。网络改变单个权重，记录其对总误差的影响，从而获得其梯度。这听起来计算量很大，因此神经网络通常需要很长的训练阶段。<em class="ms">梯度下降</em>表示搜索全局最小值，该组权重将最小化总误差。</p><p id="c0ed" class="pw-post-body-paragraph lw lx it ly b lz mv kd mb mc mw kg me mf nj mh mi mj nk ml mm mn nl mp mq mr im bi translated">梯度分解了总误差，因此RNN可以将块重新分配回贡献块的每个权重。这是<em class="ms">反向传播</em>。</p><p id="890c" class="pw-post-body-paragraph lw lx it ly b lz mv kd mb mc mw kg me mf nj mh mi mj nk ml mm mn nl mp mq mr im bi translated">然后，RNN通过从旧权重中减去它们各自梯度的一部分来更新旧权重。分数代表<em class="ms">学习率</em>，一个大于0且最大为1的值。</p><ul class=""><li id="5e00" class="mt mu it ly b lz mv mc mw mf mx mj my mn mz mr na nb nc nd bi translated">新权重=旧权重-梯度*学习率</li></ul><p id="cc88" class="pw-post-body-paragraph lw lx it ly b lz mv kd mb mc mw kg me mf nj mh mi mj nk ml mm mn nl mp mq mr im bi translated">更高的学习率(接近1.0)可以加快RNN的训练过程，但也可能导致超调，使网络无法确定最小的总误差。</p><p id="740c" class="pw-post-body-paragraph lw lx it ly b lz mv kd mb mc mw kg me mf nj mh mi mj nk ml mm mn nl mp mq mr im bi translated">一个<em class="ms">时期</em>包含整个训练数据集通过网络的过程，包括一次向前和一次向后。历元的数量将决定训练RNN所需的时间与其准确性之间的权衡。</p><p id="d6d6" class="pw-post-body-paragraph lw lx it ly b lz mv kd mb mc mw kg me mf nj mh mi mj nk ml mm mn nl mp mq mr im bi translated"><em class="ms">批量训练</em>是一种梯度下降的形式，在完成一次训练后更新权重，然后启动另一个训练周期。<em class="ms">小批量训练</em>在处理了一定数量的训练值(少于完整集)后更新权重。随机(又名在线)培训是另一种选择。我建议将批量大小设置得高于确认的或假设的季节性，这样批量就不会错过完整的季节性序列。</p><p id="3d2a" class="pw-post-body-paragraph lw lx it ly b lz mv kd mb mc mw kg me mf nj mh mi mj nk ml mm mn nl mp mq mr im bi translated"><em class="ms"> Dropout </em> level表示打开或关闭网络中节点的选项。这是为了防止过度拟合。节点不容易越来越深地陷入连接节点的特定配置中。</p><p id="d777" class="pw-post-body-paragraph lw lx it ly b lz mv kd mb mc mw kg me mf nj mh mi mj nk ml mm mn nl mp mq mr im bi translated">一个<em class="ms">深度</em>神经网络有不止一个隐藏层。不同的隐藏层可以专注于识别输入数据中的不同模式，例如季节性或趋势。</p><p id="2f04" class="pw-post-body-paragraph lw lx it ly b lz mv kd mb mc mw kg me mf nj mh mi mj nk ml mm mn nl mp mq mr im bi translated">自20世纪90年代末以来，出现了几种<strong class="ly jd">递归神经网络</strong>的变体:</p><p id="21cd" class="pw-post-body-paragraph lw lx it ly b lz mv kd mb mc mw kg me mf nj mh mi mj nk ml mm mn nl mp mq mr im bi translated"><strong class="ly jd"> LSTM </strong>代表<strong class="ly jd">“长短期记忆”</strong>，于1997年作为增强的RNN算法首次推出。LSTMs是早期RNNs的扩展，能够保持长期记忆，并使用它来学习较长序列源数据中的模式。在LSTMs之前，rnn是健忘的。他们可以保留一段记忆，但仅仅是关于他们刚刚过去的过程步骤。相比之下，LSTM引入了可以生成长时间渐变的循环。它可以在经历循环时坚持自己发现的长期模式。在每个时间步，它可以将三条信息联系在一起:当前的输入数据，它从前一个细胞接收的短期记忆(所谓的<em class="ms">隐藏状态</em>，以及来自更远细胞的长期记忆(所谓的<em class="ms">细胞状态</em>)，RNN细胞从中产生一个新的隐藏状态。</p><p id="8e42" class="pw-post-body-paragraph lw lx it ly b lz mv kd mb mc mw kg me mf nj mh mi mj nk ml mm mn nl mp mq mr im bi translated">长持续时间梯度解决了一个称为<em class="ms">消失梯度下降</em>的问题，当模型停止学习时，因为梯度的斜率变得太浅，搜索无法进一步提高权重。当重复梯度计算中涉及的许多值小于1时，会发生这种情况。相反的问题，<em class="ms">爆炸梯度</em>，在RNN执行的重复矩阵乘法中，当许多值超过1时出现。过大的梯度最终会导致RNN不稳定。消失梯度问题将RNN的记忆限制在短期相关性，而LSTM的公式保持梯度足够陡，这样搜索就不会陷入死胡同。如果模型必须处理跨越数百个周期的长时间序列，就会出现消失梯度问题。每个周期都必须由一个网络层来反映；但是具有许多层的深层网络涉及一长串矩阵乘法。</p><p id="28a2" class="pw-post-body-paragraph lw lx it ly b lz mv kd mb mc mw kg me mf nj mh mi mj nk ml mm mn nl mp mq mr im bi translated">LSTM的一个牢房据说是“有门的”。信息通过门有选择地添加或删除。细胞就像一个筛子，决定了有多少信息被捕获，有多少信息被保留。该模型可以决定它是否打开一个<em class="ms">输入</em>门来存储信息，拒绝并从长期记忆中删除它(<em class="ms">忘记</em>门)，或者将信息传递到下一层(<em class="ms">输出</em>门)。当RNN试图最小化错误并在时间循环中踱步时，它根据学习分配给信息的重要性权重来做出这些决定。这些门在它们从短期或长期记忆中作为当前输入接收的信息值之间执行矩阵乘法。随着时间的推移，LSTM学会了哪些信息在减少预测误差方面是有效的；它将通过向信息值分配0到1之间的更高或更低的权重来相应地打开和关闭门。通过它的循环，它会让权重较高的有用值通过输出门，形成新的短期记忆，但会丢弃权重较低的值。</p><p id="f813" class="pw-post-body-paragraph lw lx it ly b lz mv kd mb mc mw kg me mf nj mh mi mj nk ml mm mn nl mp mq mr im bi translated"><strong class="ly jd">2014年推出的门控循环单元(GRU </strong>)可以被视为LSTM的变体，内部架构略有不同，简化了原LSTM的某些方面。GRUs组合构建模块，例如将<em class="ms">输入</em>和<em class="ms">忘记</em>门合并成单个<em class="ms">更新</em>门。</p><p id="8a73" class="pw-post-body-paragraph lw lx it ly b lz mv kd mb mc mw kg me mf nj mh mi mj nk ml mm mn nl mp mq mr im bi translated">普通的RNN使用基本的反向传播算法，该算法在1980年代中期设计，早于1997年的LSTM创新。“香草”指的是这样一个事实，即它不包含后来的“更奇特”的数学成分，如一种<em class="ms">细胞状态</em>来保留一种记忆的长期模式。与LSTM及其衍生品相比，普通rnn更难学会长期依赖关系。香草RNN可能会被消失梯度问题绊倒。</p><p id="38d0" class="pw-post-body-paragraph lw lx it ly b lz mv kd mb mc mw kg me mf nj mh mi mj nk ml mm mn nl mp mq mr im bi translated">注意，普通神经网络(与普通RNN相对)是一个<em class="ms">前馈</em>神经网络FFNN的标签；这和香草RNN不一样。</p><p id="dce6" class="pw-post-body-paragraph lw lx it ly b lz mv kd mb mc mw kg me mf nj mh mi mj nk ml mm mn nl mp mq mr im bi translated">其他RNN变体——甚至其他LSTM风味——也存在；例如，深度门控RNN或RNN时钟工程。研究发现，RNN口味并不总是胜过其他口味(<a class="ae ld" href="http://proceedings.mlr.press/v37/jozefowicz15.pdf" rel="noopener ugc nofollow" target="_blank">实证研究</a>；以及<a class="ae ld" href="https://arxiv.org/pdf/1503.04069.pdf" rel="noopener ugc nofollow" target="_blank"> 1503.04069 </a>)。似乎没有最好的RNN变体。</p><p id="d23e" class="pw-post-body-paragraph lw lx it ly b lz mv kd mb mc mw kg me mf nj mh mi mj nk ml mm mn nl mp mq mr im bi translated">对我们来说，这意味着当我们想解决一个具体的时间序列问题时，我们应该测试不止一种类型的递归神经网络。Darts包提供了三种选择，我们可以通过改变模型设置中的单个参数来替换它们。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi kr"><img src="../Images/60861ffd8058791fe51314202584d013.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*Hr6am3C9gR0Aq8tq5tr6KA.jpeg"/></div><p class="kz la gj gh gi lb lc bd b be z dk translated"><a class="ae ld" href="https://pixabay.com/photos/man-fishing-net-lake-sunset-6342665/" rel="noopener ugc nofollow" target="_blank">chan wity拍摄的男子渔网湖Pixabay上的免费照片</a></p></figure><h1 id="8c49" class="le lf it bd lg lh li lj lk ll lm ln lo ki lp kj lq kl lr km ls ko lt kp lu lv bi translated">2a。省道的安装</h1><p id="6d11" class="pw-post-body-paragraph lw lx it ly b lz ma kd mb mc md kg me mf mg mh mi mj mk ml mm mn mo mp mq mr im bi translated">和往常一样，您应该在尝试安装一个庞大的Python包之前创建一个新的虚拟环境，这个包有许多依赖项，比如<em class="ms"> Darts的</em>。您希望避免基础环境中的现有软件包因新安装而降级或升级。</p><p id="5ce0" class="pw-post-body-paragraph lw lx it ly b lz mv kd mb mc mw kg me mf nj mh mi mj nk ml mm mn nl mp mq mr im bi translated">例如，在<em class="ms"> Anaconda: </em>中创建虚拟环境</p><ul class=""><li id="a6ab" class="mt mu it ly b lz mv mc mw mf mx mj my mn mz mr na nb nc nd bi translated">康达创建——名称<your_chosen_env_name/></li></ul><p id="75be" class="pw-post-body-paragraph lw lx it ly b lz mv kd mb mc mw kg me mf nj mh mi mj nk ml mm mn nl mp mq mr im bi translated">省道包裹所需的神经网络方法<em class="ms"> PyTorch </em>。PyTorch具有非Python依赖性，因此如果试图安装完整的库时出现错误消息，安装可以在一个<em class="ms"> Darts </em>核心包和某些附加包之间进行。</p><p id="4ba5" class="pw-post-body-paragraph lw lx it ly b lz mv kd mb mc mw kg me mf nj mh mi mj nk ml mm mn nl mp mq mr im bi translated">关于安装的附加信息:<a class="ae ld" href="https://pythonrepo.com/repo/unit8co-darts-python-machine-learning" rel="noopener ugc nofollow" target="_blank">一个python库，用于时间序列的简单操作和预测。| PythonRepo </a></p><p id="7299" class="pw-post-body-paragraph lw lx it ly b lz mv kd mb mc mw kg me mf nj mh mi mj nk ml mm mn nl mp mq mr im bi translated">用<em class="ms">安装<em class="ms">飞镖</em>及其所有</em>延伸部分<em class="ms">FB prophet；pmdarima </em>(针对SARIMAX)；<em class="ms"> </em>和<em class="ms"> PyTorch </em>(用于神经网络)——使用命令:</p><ul class=""><li id="0153" class="mt mu it ly b lz mv mc mw mf mx mj my mn mz mr na nb nc nd bi translated"><em class="ms"> pip安装' u8darts[all]' </em></li></ul><p id="9c9a" class="pw-post-body-paragraph lw lx it ly b lz mv kd mb mc mw kg me mf nj mh mi mj nk ml mm mn nl mp mq mr im bi translated">如果你遇到问题，你可以先安装飞镖而不需要额外的东西:</p><ul class=""><li id="3447" class="mt mu it ly b lz mv mc mw mf mx mj my mn mz mr na nb nc nd bi translated"><em class="ms"> pip安装u8飞镖</em></li></ul><p id="e3d8" class="pw-post-body-paragraph lw lx it ly b lz mv kd mb mc mw kg me mf nj mh mi mj nk ml mm mn nl mp mq mr im bi translated">然后尝试逐步添加额外内容:</p><ul class=""><li id="fdc7" class="mt mu it ly b lz mv mc mw mf mx mj my mn mz mr na nb nc nd bi translated">pip安装<em class="ms"> 'u8darts[fbprophet]' </em></li><li id="fc66" class="mt mu it ly b lz ne mc nf mf ng mj nh mn ni mr na nb nc nd bi translated">pip安装<em class="ms"> 'u8darts[pmdarima]' </em></li><li id="19f1" class="mt mu it ly b lz ne mc nf mf ng mj nh mn ni mr na nb nc nd bi translated">pip安装<em class="ms">' u8飞镖【火炬】'</em></li></ul><h1 id="27ab" class="le lf it bd lg lh li lj lk ll lm ln lo ki lp kj lq kl lr km ls ko lt kp lu lv bi translated">2b。属国</h1><p id="7c43" class="pw-post-body-paragraph lw lx it ly b lz ma kd mb mc md kg me mf mg mh mi mj mk ml mm mn mo mp mq mr im bi translated">我们从PyTorch和Darts库中导入了一些子类。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="nn no l"/></div></figure><p id="2e43" class="pw-post-body-paragraph lw lx it ly b lz mv kd mb mc mw kg me mf nj mh mi mj nk ml mm mn nl mp mq mr im bi translated">您想要设置的最重要的常数之一是神经网络配置中的<em class="ms">历元</em>的数量。出于调试或测试的目的，将其设置为一个较低的值，如3。要获得真实的预测，将其设置为300。请注意，如果您将纪元常数设置为300，Jupyter笔记本及其三个RNN变体将需要大约12-15分钟来训练一个模型，当您想要获得较低个位数的良好平均绝对百分比误差时，请考虑绕街区走一圈。当epoch值为3时，它将在几秒钟内遍历代码，但当然平均绝对百分比误差会高得离谱。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="nn no l"/></div></figure><p id="dbb8" class="pw-post-body-paragraph lw lx it ly b lz mv kd mb mc mw kg me mf nj mh mi mj nk ml mm mn nl mp mq mr im bi translated">在本教程中，我们将重点关注Box &amp; Jenkins的经典月度<em class="ms">航空乘客</em>数据集，该数据集随Darts安装一起提供，因此无需读取外部文件即可加载。</p><p id="421f" class="pw-post-body-paragraph lw lx it ly b lz mv kd mb mc mw kg me mf nj mh mi mj nk ml mm mn nl mp mq mr im bi translated">在航空公司乘客的例子中，我选择1959年1月1日作为测试周期的开始，用常量FC_START表示。我们将预测36个月，输入常量FC_N。</p><h1 id="b59d" class="le lf it bd lg lh li lj lk ll lm ln lo ki lp kj lq kl lr km ls ko lt kp lu lv bi translated">3.准备源数据</h1><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="nn no l"/></div></figure><p id="0652" class="pw-post-body-paragraph lw lx it ly b lz mv kd mb mc mw kg me mf nj mh mi mj nk ml mm mn nl mp mq mr im bi translated">Darts的<em class="ms"> load() </em>函数允许我们将时间序列读入一个<em class="ms"> timeseries </em>对象<em class="ms">，ts。</em></p><p id="385f" class="pw-post-body-paragraph lw lx it ly b lz mv kd mb mc mw kg me mf nj mh mi mj nk ml mm mn nl mp mq mr im bi translated">函数<em class="ms"> pd_dataframe() </em>可以将timeseries对象转换为dataframe，以便于在我们想要使用pandas提供的数据争论方法时进行处理。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi np"><img src="../Images/853b81ac370adafdc1a42381c4cab61c.png" data-original-src="https://miro.medium.com/v2/resize:fit:762/format:webp/1*OvulGM4xmdRoqU3Hdort7Q.png"/></div><p class="kz la gj gh gi lb lc bd b be z dk translated">作者图片</p></figure><p id="91fa" class="pw-post-body-paragraph lw lx it ly b lz mv kd mb mc mw kg me mf nj mh mi mj nk ml mm mn nl mp mq mr im bi translated">单变量时间序列由从1949年到1960年的144个月组成。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/00bd473484bd25332a8a35ba47bb236d.png" data-original-src="https://miro.medium.com/v2/resize:fit:676/format:webp/1*70qBC41xQYU9p_9hSOazqg.png"/></div><p class="kz la gj gh gi lb lc bd b be z dk translated">作者图片</p></figure><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="ns nt di nu bf nv"><div class="gh gi nr"><img src="../Images/a16e40ccc35ebeda938aaa88d24775bb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nkWKBO56NFf4zNuDQYU54g.png"/></div></div><p class="kz la gj gh gi lb lc bd b be z dk translated">作者图片</p></figure><p id="8bde" class="pw-post-body-paragraph lw lx it ly b lz mv kd mb mc mw kg me mf nj mh mi mj nk ml mm mn nl mp mq mr im bi translated">时间序列似乎表现出12个月后重复的季节性。我们可以通过更精确的测试来确认视觉线索。darts '<em class="ms">check _季节性()</em>函数计算自相关函数ACF，并返回12.0个月的季节性顺序。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="nn no l"/></div></figure><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi nw"><img src="../Images/c509ea2ee9084846b1093075df23cfc5.png" data-original-src="https://miro.medium.com/v2/resize:fit:812/format:webp/1*H2q6lbgJNnP0EC_uayFopQ.png"/></div><p class="kz la gj gh gi lb lc bd b be z dk translated">作者图片</p></figure><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="ns nt di nu bf nv"><div class="gh gi nx"><img src="../Images/59f45bbe5d4ed449a860dc49736091cc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*g6kqmurSWvMACXONL6bcbA.png"/></div></div><p class="kz la gj gh gi lb lc bd b be z dk translated">作者图片</p></figure><p id="9d7b" class="pw-post-body-paragraph lw lx it ly b lz mv kd mb mc mw kg me mf nj mh mi mj nk ml mm mn nl mp mq mr im bi translated">我们将时间序列分为训练数据集和验证数据集。</p><p id="d559" class="pw-post-body-paragraph lw lx it ly b lz mv kd mb mc mw kg me mf nj mh mi mj nk ml mm mn nl mp mq mr im bi translated">在dependencies单元格中，我们选择FC _ START = 1959/01/01——时间序列结束前的两年——作为我们要进行切割的点。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="nn no l"/></div></figure><p id="ebca" class="pw-post-body-paragraph lw lx it ly b lz mv kd mb mc mw kg me mf nj mh mi mj nk ml mm mn nl mp mq mr im bi translated">在将源数据输入神经网络之前，我们需要通过应用<em class="ms"> Scaler() </em>函数对它们进行规范化。缩放将使神经网络更容易应用其秘方:重新校准它应用于输入值的权重。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="nn no l"/></div></figure><p id="05c7" class="pw-post-body-paragraph lw lx it ly b lz mv kd mb mc mw kg me mf nj mh mi mj nk ml mm mn nl mp mq mr im bi translated">最后，为了使神经网络能够检测时间序列的属性，我们从编码日期的字符串中派生出<em class="ms">年</em>和<em class="ms">月</em>。我们将把<em class="ms">年序列</em>和<em class="ms">月序列</em>作为协变量提供给RNN模型(“回归变量”)，就像当我们拟合SARIMAX模型时可以包含两个(或更多)外生变量的列数组X一样。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="nn no l"/></div></figure><h1 id="6fa2" class="le lf it bd lg lh li lj lk ll lm ln lo ki lp kj lq kl lr km ls ko lt kp lu lv bi translated">4.RNN模型的设置和运行</h1><p id="e1c9" class="pw-post-body-paragraph lw lx it ly b lz ma kd mb mc md kg me mf mg mh mi mj mk ml mm mn mo mp mq mr im bi translated">我命名为<em class="ms">口味</em>的列表(下面的第2行)包含了我们想要在时间序列上释放的三个RNN变体。</p><p id="7419" class="pw-post-body-paragraph lw lx it ly b lz mv kd mb mc mw kg me mf nj mh mi mj nk ml mm mn nl mp mq mr im bi translated">第5行中的list comprehension一个接一个地调用这三种口味，并将它们馈送到RNN预测器<em class="ms"> run_RNN </em>。除了三种变体的名称之外，它们在所需的参数化方面没有区别。只是他们内部的算法会导致不同的结果。预测器<em class="ms"> run_RNN </em>将接受它要应用的算法的名称作为它的输入参数；以及经过重新调整或变换的时间序列及其训练和验证数据集。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="nn no l"/></div></figure><p id="9683" class="pw-post-body-paragraph lw lx it ly b lz mv kd mb mc mw kg me mf nj mh mi mj nk ml mm mn nl mp mq mr im bi translated">接下来，<em class="ms"> run_RNN() </em>函数配置神经网络。</p><p id="f3f2" class="pw-post-body-paragraph lw lx it ly b lz mv kd mb mc mw kg me mf nj mh mi mj nk ml mm mn nl mp mq mr im bi translated">rnn附带了一组用户需要设置的超参数(或者在搜索递增的更好的参数元组时让调整算法多次评估模型)，其中包括:</p><ul class=""><li id="fb17" class="mt mu it ly b lz mv mc mw mf mx mj my mn mz mr na nb nc nd bi translated">输入节点、隐藏层及其节点、输出节点的数量；它们越多，RNN能处理的模式就越复杂；太多的话，RNN就有被卡住或过度适应的风险</li><li id="a9ec" class="mt mu it ly b lz ne mc nf mf ng mj nh mn ni mr na nb nc nd bi translated">权重值及其变化率(学习率)</li><li id="dd94" class="mt mu it ly b lz ne mc nf mf ng mj nh mn ni mr na nb nc nd bi translated">辍学率，如果有的话(见下文)</li></ul><p id="253e" class="pw-post-body-paragraph lw lx it ly b lz mv kd mb mc mw kg me mf nj mh mi mj nk ml mm mn nl mp mq mr im bi translated">它们的最佳设置不是先验已知的，它们是针对RNN要攻击的具体问题的。PyTorch和Darts有默认值。超参数调整算法可用于微调它们，作为手动实验的替代方法，但寻找更好的超参数需要时间进行重复评估。我将这个练习限制在几个不同的历元数(训练循环)值上；以及学习率(各遍之间权重的调整量)。</p><ul class=""><li id="b45b" class="mt mu it ly b lz mv mc mw mf mx mj my mn mz mr na nb nc nd bi translated">它的<em class="ms">模型</em>参数接受我们想要应用到源数据的三个拟合过程中的每一个的“味道”:首先是LSTM，然后是GRU，最后是香草(简单地标记为“RNN”作为默认变量)。</li><li id="cc50" class="mt mu it ly b lz ne mc nf mf ng mj nh mn ni mr na nb nc nd bi translated">神经网络的<em class="ms"> epoch </em>参数设置训练期间的通过次数。一个时期包括一次向前和一次向后通过整个训练数据集。</li><li id="f2a7" class="mt mu it ly b lz ne mc nf mf ng mj nh mn ni mr na nb nc nd bi translated">我们将<em class="ms"> input_chunk_length — </em>用于预测的过去周期数— <em class="ms"> </em>设置为周期性12.0，这是<em class="ms">check _季节性</em>测试返回的结果。RNN将回顾过去12个月，一个完整的季节周期，来计算预测。我们可以增加这个值，从而迫使RNN更加依赖它的长期记忆。这将发挥LSTM或GRU相对于香草味的短期记忆的优势，但不一定会导致更高的准确性。</li><li id="2008" class="mt mu it ly b lz ne mc nf mf ng mj nh mn ni mr na nb nc nd bi translated"><em class="ms"> training_length </em> —训练期间使用的周期数。不应低于它将用于预测的input_chunk_length。</li><li id="a172" class="mt mu it ly b lz ne mc nf mf ng mj nh mn ni mr na nb nc nd bi translated"><em class="ms"> random_state </em>可以使用给定的种子号来初始化权重，以提高模型结果的可重复性。</li><li id="1256" class="mt mu it ly b lz ne mc nf mf ng mj nh mn ni mr na nb nc nd bi translated"><em class="ms">丢失</em> —在训练过程中，如果设置大于0，RNN可以随机将一些输入清零，丢失概率等于参数值，模拟不同的网络架构。在每个时期，RNN都会选择一组不同的神经元，并将其暂时移除。不同的神经元组合然后做出预测。这种所谓的网络细化的目的是通过不断训练其全套神经元来防止RNN过度拟合。</li><li id="7c83" class="mt mu it ly b lz ne mc nf mf ng mj nh mn ni mr na nb nc nd bi translated"><em class="ms"> optimizer_kwargs </em>及其参数“lr”设置了一个<em class="ms">学习率。</em>RNN试图通过调整权重来最小化预测误差。权重在循环中更新的量被称为学习率。RNN估计节点导致错误的程度。然后，每次更新权重时，将估计误差乘以学习率。更高的速率将使RNN能够更快地学习，但它可能会因超调而导致不稳定，并使其面临在训练时段之间振荡的风险，甚至会在正反馈循环中结束，从而导致爆炸梯度。然而，在我们当前的例子中，替代的学习率——0.1或0.001——并没有显著改变每个模型10 — 15分钟的处理时间(尽管时期的数量改变了)。较大的值0.1确实会导致MAPE增加三倍的不良影响，相比之下，我们将在乘客示例中使用0.001的比率来实现更好的预测精度。</li><li id="adf8" class="mt mu it ly b lz ne mc nf mf ng mj nh mn ni mr na nb nc nd bi translated"><em class="ms"> hidden_dim </em>控制隐藏状态的大小，模型的深度。隐藏状态越大，它从源数据中推断出的模式就越复杂。</li><li id="d187" class="mt mu it ly b lz ne mc nf mf ng mj nh mn ni mr na nb nc nd bi translated"><em class="ms"> force_reset </em>放弃任何同名的先前模型，重新开始训练。</li></ul><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="nn no l"/></div></figure><p id="f155" class="pw-post-body-paragraph lw lx it ly b lz mv kd mb mc mw kg me mf nj mh mi mj nk ml mm mn nl mp mq mr im bi translated">在模型建立之后，我们将它与训练和验证数据集一起传递给拟合函数，<em class="ms"> fit_it()，</em>。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="nn no l"/></div></figure><p id="e5e7" class="pw-post-body-paragraph lw lx it ly b lz mv kd mb mc mw kg me mf nj mh mi mj nk ml mm mn nl mp mq mr im bi translated">装配或训练过程是耗时的。根据您计算机的处理器性能，预计三个RNN版本的每一个都将运行10-14分钟。</p><p id="0103" class="pw-post-body-paragraph lw lx it ly b lz mv kd mb mc mw kg me mf nj mh mi mj nk ml mm mn nl mp mq mr im bi translated">在dependencies单元格的顶部，如果您只想在一分钟内运行脚本进行代码测试，而对准确的预测结果不感兴趣，您可以将<em class="ms"> EPOCH </em>参数降低到远小于300，例如一个小至3的EPOCH数。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="ns nt di nu bf nv"><div class="gh gi ny"><img src="../Images/abb753703c83e915cd4a6c08c6db92be.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CWS_3ip9o6WbcH6Von6cwQ.jpeg"/></div></div><p class="kz la gj gh gi lb lc bd b be z dk translated">作者图片</p></figure><p id="e099" class="pw-post-body-paragraph lw lx it ly b lz mv kd mb mc mw kg me mf nj mh mi mj nk ml mm mn nl mp mq mr im bi translated">当辅助函数fit_it()返回拟合的模型时，主函数run_RNN()将接受它并使用它来计算预测(第26行)。</p><p id="d825" class="pw-post-body-paragraph lw lx it ly b lz mv kd mb mc mw kg me mf nj mh mi mj nk ml mm mn nl mp mq mr im bi translated">run_RNN()中的第29行调用绘图仪函数<em class="ms"> plot_fitted() </em>来绘制预测和实际观测的图表。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="nn no l"/></div></figure><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="ns nt di nu bf nv"><div class="gh gi nz"><img src="../Images/4f94d208549a2b384801c63e6009e0ff.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RhXEeVlhZbJMGBpTswMf5A.png"/></div></div><p class="kz la gj gh gi lb lc bd b be z dk translated">作者图片</p></figure><p id="6c7d" class="pw-post-body-paragraph lw lx it ly b lz mv kd mb mc mw kg me mf nj mh mi mj nk ml mm mn nl mp mq mr im bi translated">绘制完预测图表后，run_RNN()调用下一个帮助函数<em class="ms">【accuracy _ metrics()</em>，计算预测准确度指标。这将比我们从图表中获得的视觉线索更精确地确定预测值和实际值之间的差异。</p><p id="7aba" class="pw-post-body-paragraph lw lx it ly b lz mv kd mb mc mw kg me mf nj mh mi mj nk ml mm mn nl mp mq mr im bi translated">在accuracy_metrics()的顶部，我使用Darts的'<em class="ms"> slice_intersect() </em>函数来确保我们想要比较的两个序列(预测和实际)具有相同的长度，只包括它们的重叠周期。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="nn no l"/></div></figure><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi oa"><img src="../Images/c32b9a815e1676cfa7ea97869ce9ca2f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1392/format:webp/1*AnuLJlVAA7P_gyCNJ-9gcw.png"/></div><p class="kz la gj gh gi lb lc bd b be z dk translated">作者图片</p></figure><p id="3cb6" class="pw-post-body-paragraph lw lx it ly b lz mv kd mb mc mw kg me mf nj mh mi mj nk ml mm mn nl mp mq mr im bi translated">该函数计算Darts在其工具类中提供的三个度量:MAPE、RMSE和R平方。</p><p id="f8d7" class="pw-post-body-paragraph lw lx it ly b lz mv kd mb mc mw kg me mf nj mh mi mj nk ml mm mn nl mp mq mr im bi translated">r平方表示实际观测值的变化在多大程度上可以用预测值的相应变化来解释。值1.0表示预测完全反映了实际情况。如果你想知道为什么R平方显示负值:虽然R平方本身是一个介于0和1之间的非负数，但我反转了它的符号，以使它的方向指引与其他度量一致。对于所有其他误差测量指标，较低的值是首选。R平方前面的负号使其成为一个“越低越好”的指标，就像其他指标一样，这将有助于在下面进一步显示的数据框架中可视化它们。</p><p id="271a" class="pw-post-body-paragraph lw lx it ly b lz mv kd mb mc mw kg me mf nj mh mi mj nk ml mm mn nl mp mq mr im bi translated">该函数还计算RMSPE，即均方根百分比误差；以及预测的标准误差。</p><p id="d008" class="pw-post-body-paragraph lw lx it ly b lz mv kd mb mc mw kg me mf nj mh mi mj nk ml mm mn nl mp mq mr im bi translated">第20–22行收集字典中的指标及其名称，该函数将这些信息传递回主函数run_RNN()。</p><h1 id="5494" class="le lf it bd lg lh li lj lk ll lm ln lo ki lp kj lq kl lr km ls ko lt kp lu lv bi translated">5.查看预测结果</h1><p id="d9a2" class="pw-post-body-paragraph lw lx it ly b lz ma kd mb mc md kg me mf mg mh mi mj mk ml mm mn mo mp mq mr im bi translated">事实证明，简单的RNN香草口味生成的预测具有最低的平均绝对百分比误差，为3.93%，领先于这个特定示例的三个RNN变体之间的一根头发的宽度。LSTM风味紧随其后，为4.11%。实际上，它们是无法区分的。GRU风味的MAPE，5.01%，远远落后于这两个领先者。</p><p id="26b5" class="pw-post-body-paragraph lw lx it ly b lz mv kd mb mc mw kg me mf nj mh mi mj nk ml mm mn nl mp mq mr im bi translated">在RNN模型的设置中，我们对学习率(0.001)和纪元(300)进行了相对精细的梳理。我们还可以使用dropout参数(让RNN在训练期间尝试各种节点子集)；以及隐藏状态的大小(较高的隐藏维度值增加了RNN在较长时间范围内处理更复杂模式的能力)。调整算法可以在重新运行拟合过程时调整它们，以尝试实现更低的MAPE。但是这个特殊的时间序列并不复杂。耗时的调整工作似乎不太可能将MAPE从4%降低到1%。然而，在现实世界的项目中，我们可以考虑一个更长的通宵调优过程，以尝试逐步消除MAPE。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="ns nt di nu bf nv"><div class="gh gi ob"><img src="../Images/18c2ee907550708cc079e9f15bd4df5f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-hYmOm50EJnfe9igEw2JDg.png"/></div></div><p class="kz la gj gh gi lb lc bd b be z dk translated">作者图片</p></figure><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/ac931f125340bbc3fc98de64ce3edf3a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1380/format:webp/1*x_q3z_bImSJOSBI-S4vmdg.png"/></div><p class="kz la gj gh gi lb lc bd b be z dk translated">作者图片</p></figure><p id="fbe3" class="pw-post-body-paragraph lw lx it ly b lz mv kd mb mc mw kg me mf nj mh mi mj nk ml mm mn nl mp mq mr im bi translated">为了将RNN方法与基线预测进行比较，我们转向我们在早期文章中回顾过的方法之一(<a class="ae ld" rel="noopener" target="_blank" href="/darts-swiss-knife-for-time-series-forecasting-in-python-f37bb74c126"> Darts的时间序列预测瑞士刀| 2021年10月|走向数据科学</a>):Theta方法，一种简单而快速的方法。</p><p id="5ed8" class="pw-post-body-paragraph lw lx it ly b lz mv kd mb mc mw kg me mf nj mh mi mj nk ml mm mn nl mp mq mr im bi translated">我们通过在第4-18行中搜索100个可选值来调整它的theta参数。然后，我们将这个最佳θ参数传递给第22–23行中的装配工，并计算第24行中的预测。</p><p id="8aa9" class="pw-post-body-paragraph lw lx it ly b lz mv kd mb mc mw kg me mf nj mh mi mj nk ml mm mn nl mp mq mr im bi translated">第24行下面的部分绘制了预测曲线，并计算了准确性指标。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="nn no l"/></div></figure><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi od"><img src="../Images/4e21e4d1b6897e57ac648ab4487dfe07.png" data-original-src="https://miro.medium.com/v2/resize:fit:828/format:webp/1*7g56YCYR2FJKgQZczcvGJQ.png"/></div><p class="kz la gj gh gi lb lc bd b be z dk translated">作者图片</p></figure><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="ns nt di nu bf nv"><div class="gh gi oe"><img src="../Images/9cd247e010c657315c242efd0996f0ed.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*blbHksCCy2hKILeGiaMw_w.png"/></div></div><p class="kz la gj gh gi lb lc bd b be z dk translated">作者图片</p></figure><p id="280e" class="pw-post-body-paragraph lw lx it ly b lz mv kd mb mc mw kg me mf nj mh mi mj nk ml mm mn nl mp mq mr im bi translated">θ返回的MAPE只有3.11%</p><p id="a6cf" class="pw-post-body-paragraph lw lx it ly b lz mv kd mb mc mw kg me mf nj mh mi mj nk ml mm mn nl mp mq mr im bi translated">为了便于比较这些方法，让我们将三个RNN变量和Theta预测的准确性指标收集到一个字典中(第3-4行)，我们在第5行将其转换为数据帧。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="nn no l"/></div></figure><h1 id="450d" class="le lf it bd lg lh li lj lk ll lm ln lo ki lp kj lq kl lr km ls ko lt kp lu lv bi translated">6.结论</h1><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi of"><img src="../Images/6a98bd712501f3b1e0bc61f83ac1c64b.png" data-original-src="https://miro.medium.com/v2/resize:fit:966/format:webp/1*6c7x0LPonEgwYLrEMPOE4A.jpeg"/></div><p class="kz la gj gh gi lb lc bd b be z dk translated">作者图片</p></figure><ul class=""><li id="3415" class="mt mu it ly b lz mv mc mw mf mx mj my mn mz mr na nb nc nd bi translated">我们观察到，相对于MAPE标准，θ方法领先于该领域:预测误差仅为3.112%。</li><li id="8d24" class="mt mu it ly b lz ne mc nf mf ng mj nh mn ni mr na nb nc nd bi translated">普通RNN在均方根百分比误差RMSE方面保持其竞争优势，</li><li id="67b0" class="mt mu it ly b lz ne mc nf mf ng mj nh mn ni mr na nb nc nd bi translated">而LSTM提供了最低的绝对RMSE，以及最佳的R平方值和最小的预测标准误差。</li></ul><p id="7a16" class="pw-post-body-paragraph lw lx it ly b lz mv kd mb mc mw kg me mf nj mh mi mj nk ml mm mn nl mp mq mr im bi translated">没有整合LSTM算法的“长”方面的普通RNN的良好性能意味着时间序列遵循一种不需要太多长期记忆的模式。</p><p id="3204" class="pw-post-body-paragraph lw lx it ly b lz mv kd mb mc mw kg me mf nj mh mi mj nk ml mm mn nl mp mq mr im bi translated">RMSE对预测误差进行平方，对较大误差的惩罚力度比MAPE更大。当残差的分布是左偏或右偏时，就会出现偏差。平均值将高于或低于中间值。最小化RMSE的预测将显示出较小的偏差。</p><p id="7cbf" class="pw-post-body-paragraph lw lx it ly b lz mv kd mb mc mw kg me mf nj mh mi mj nk ml mm mn nl mp mq mr im bi translated">在我们当前的例子中，LSTM在预测的RMSE、R平方和标准误差方面具有竞争优势。综上所述，我认为这三个标准足以证明LSTM是这场赛马的赢家。</p><ul class=""><li id="52dc" class="mt mu it ly b lz mv mc mw mf mx mj my mn mz mr na nb nc nd bi translated">Jupyter笔记本可以在GitHub上下载:<a class="ae ld" href="https://github.com/h3ik0th/Darts_RNN" rel="noopener ugc nofollow" target="_blank">h3ik0th/Darts _ RNN:Python Darts中的递归神经网络(github.com)</a></li><li id="a6ad" class="mt mu it ly b lz ne mc nf mf ng mj nh mn ni mr na nb nc nd bi translated">标题图片:<a class="ae ld" href="https://pixabay.com/illustrations/people-collective-group-knowledge-3286018/" rel="noopener ugc nofollow" target="_blank">人民集体组，作者geralt——Pixabay上的免费图片</a></li><li id="aa5d" class="mt mu it ly b lz ne mc nf mf ng mj nh mn ni mr na nb nc nd bi translated">葡萄藤图片:<a class="ae ld" href="https://pixabay.com/photos/grape-vine-tendril-climbing-plant-592995/" rel="noopener ugc nofollow" target="_blank">葡萄藤卷须攀缘植物，由stevepb拍摄Pixabay上的免费照片</a></li><li id="477b" class="mt mu it ly b lz ne mc nf mf ng mj nh mn ni mr na nb nc nd bi translated">渔夫图片:<a class="ae ld" href="https://pixabay.com/photos/man-fishing-net-lake-sunset-6342665/" rel="noopener ugc nofollow" target="_blank">chan wity拍摄的渔网湖Pixabay上的免费照片</a></li><li id="d3f2" class="mt mu it ly b lz ne mc nf mf ng mj nh mn ni mr na nb nc nd bi translated">所有其他图片:作者</li></ul></div></div>    
</body>
</html>