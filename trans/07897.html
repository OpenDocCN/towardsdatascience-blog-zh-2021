<html>
<head>
<title>How to use Deep Learning for Time-Series Data</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如何对时间序列数据使用深度学习</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/how-to-use-deep-learning-for-time-series-data-f641b1b41a96?source=collection_archive---------3-----------------------#2021-07-20">https://towardsdatascience.com/how-to-use-deep-learning-for-time-series-data-f641b1b41a96?source=collection_archive---------3-----------------------#2021-07-20</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="0016" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsdatascience.com/tagged/hands-on-tutorials" rel="noopener" target="_blank">实践教程</a></h2><div class=""/><div class=""><h2 id="a9be" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">了解时间序列深度学习的可用选项</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/62e60675b8ba27ae6c7100ae6365b66f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*7T82e9osT3Y21OQs"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">由<a class="ae lh" href="https://unsplash.com/@ewitsoe?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">埃里克·维特索</a>在<a class="ae lh" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="9dc0" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi me translated">关于机器学习和数据科学，ime系列数据通常被单独归为一类。然而，由于分布随着时间和顺序测试而变化，数据科学的这一领域可能很难管理。</p><p id="99c6" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">本文将介绍三种不同的深度学习模型，用于基于时序数据创建预测模型。第一个将是LSTM，随后是两个具有一维卷积和二维卷积的CNN。</p><p id="1263" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我包含了代码块，这样您可以通过简单地更新正在使用的数据和调整参数来复制和重用这些代码。</p><p id="e6fa" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><strong class="lk jd">请随意将本页加入书签，以供将来参考。</strong></p><p id="309c" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">请注意，这些模型可能比时间序列预测所需的要多得多。然而，从简单的模型和2D表格数据开始，您可以对您可能期望的性能进行基准测试。</p><p id="3a7f" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">要将时间序列数据转换为2D数据，请在整个可用时间段或时间段的子集内进行聚合。这里不讨论其他选择，其中最常见的是自回归(AR)模型。</p></div><div class="ab cl mn mo hx mp" role="separator"><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms"/></div><div class="im in io ip iq"><h1 id="8699" class="mu mv it bd mw mx my mz na nb nc nd ne ki nf kj ng kl nh km ni ko nj kp nk nl bi translated">数据，数据，数据</h1><p id="9e3d" class="pw-post-body-paragraph li lj it lk b ll nm kd ln lo nn kg lq lr no lt lu lv np lx ly lz nq mb mc md im bi translated">任何精通数据的科学家都知道，数据争论、操作和全面的数据工程是数据科学家最耗时的任务。特别是有了机器学习和深度学习，这个任务就更复杂了。例如，与使用表格数据(基本熊猫数据框架或2D数字阵列中的数据)的模型相比，时间序列数据需要额外的维度才能使模型正常运行。</p><p id="1a74" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">将数据处理成三维或四维的直接方法是<strong class="lk jd">一次考虑一个实例</strong>。这样，您可以一次查看一个实例的数据，以在进一步转换之前确认正确的信息(如标准时间间隔)。</p><pre class="ks kt ku kv gt nr ns nt nu aw nv bi"><span id="93f3" class="nw mv it ns b gy nx ny l nz oa">X = []</span><span id="84fb" class="nw mv it ns b gy ob ny l nz oa">for i in instances:<br/>    # ‘df’ is a 2D pandas dataframe or numpy array<br/>    df = generate_instance(i) # Produce a single instance here<br/>    X.append(df)</span><span id="b07b" class="nw mv it ns b gy ob ny l nz oa">X_3D = np.array(X)<br/>X_4D = X_3D.reshape((X_3D.shape[0], X_3D.shape[1], X_3D.shape[2], 1))</span></pre><p id="43d0" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">对于每个实例，创建一个2D数据帧或numpy数组。该数据帧的列将包含特征，行代表单个实例的不同时间步长。</p><p id="fddb" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">一次创建一个实例，并将它们添加到列表中。一旦每个数据帧都在一个列表中，就转换成一个numpy数组，然后轻松地将信息输入到下面的模型中。</p><p id="8e31" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">请注意，每个实例必须具有相同的尺寸。如果它们不同，你会得到一些令人兴奋的错误。</p><p id="a7ec" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">对于二维卷积，数据必须有一个额外的维度。对于多元时间序列数据，这意味着使用reshape函数添加另一个维度。</p></div><div class="ab cl mn mo hx mp" role="separator"><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms"/></div><div class="im in io ip iq"><h1 id="a481" class="mu mv it bd mw mx my mz na nb nc nd ne ki nf kj ng kl nh km ni ko nj kp nk nl bi translated">模型的结构</h1><p id="33fc" class="pw-post-body-paragraph li lj it lk b ll nm kd ln lo nn kg lq lr no lt lu lv np lx ly lz nq mb mc md im bi translated">这些模型使用“relu”或sigmoid激活函数以及最终输出的sigmoid激活。</p><p id="306b" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">对于这些模型，您可以调整卷积层数、内核大小、过滤器数量、学习速率、密集层中的节点数量、LSTM单元数量、批量大小和池大小。</p><p id="c3de" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">这里的模型被建立来处理使用多元时间序列数据的分类问题，并使用二元交叉熵作为它们的损失函数。此外，这些模型包括根据其发生率的比例类加权。最后用Adam优化对整个网络进行优化。</p><h2 id="5d60" class="nw mv it bd mw oc od dn na oe of dp ne lr og oh ng lv oi oj ni lz ok ol nk iz bi translated">T-CNN模型:</h2><p id="0be4" class="pw-post-body-paragraph li lj it lk b ll nm kd ln lo nn kg lq lr no lt lu lv np lx ly lz nq mb mc md im bi translated">两个CNN模型<strong class="lk jd"> </strong>的结构遵循相似的格式:</p><ol class=""><li id="0e0b" class="om on it lk b ll lm lo lp lr oo lv op lz oq md or os ot ou bi translated">数据经过一系列一维或二维的卷积层。</li><li id="2adc" class="om on it lk b ll ov lo ow lr ox lv oy lz oz md or os ot ou bi translated">汇集层。合并后，输出被展平并连接到完全连接的图层。</li><li id="0e21" class="om on it lk b ll ov lo ow lr ox lv oy lz oz md or os ot ou bi translated">在完全连接的层之后，产生预测的概率。</li></ol><h2 id="b32f" class="nw mv it bd mw oc od dn na oe of dp ne lr og oh ng lv oi oj ni lz ok ol nk iz bi translated"><strong class="ak"> LSTM模式:</strong></h2><ol class=""><li id="c270" class="om on it lk b ll nm lo nn lr pa lv pb lz pc md or os ot ou bi translated">数据通过具有可变数量单元的LSTM单元。</li><li id="4c0b" class="om on it lk b ll ov lo ow lr ox lv oy lz oz md or os ot ou bi translated">输出被展平并连接到完全连接的图层。</li><li id="0396" class="om on it lk b ll ov lo ow lr ox lv oy lz oz md or os ot ou bi translated">在完全连接的层之后，产生预测的概率。</li></ol><p id="5b91" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">结构故意保持相似。因此，很容易只交换CNN和LSTM模型之间不同的第一层。</p></div><div class="ab cl mn mo hx mp" role="separator"><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms"/></div><div class="im in io ip iq"><h1 id="0194" class="mu mv it bd mw mx my mz na nb nc nd ne ki nf kj ng kl nh km ni ko nj kp nk nl bi translated">超参数和导入</h1><pre class="ks kt ku kv gt nr ns nt nu aw nv bi"><span id="210c" class="nw mv it ns b gy nx ny l nz oa">from tensorflow.keras.models import Sequential<br/>from tensorflow.keras.layers import LSTM, Dense, Flatten, Conv1D, Conv2D, MaxPooling1D, MaxPooling2D<br/>from tensorflow.keras.optimizers import Adam, schedules, SGD</span><span id="e8e8" class="nw mv it ns b gy ob ny l nz oa">KERNEL_SIZE = 2<br/>POOL_SIZE = 2<br/>N_FILTERS = 24<br/>BATCH_SIZE = 128<br/>LEARNING_RATE = 0.00001<br/>DENSE_UNITS = 50<br/>LSTM_UNITS = 50<br/>EPOCHS = 100</span><span id="7387" class="nw mv it ns b gy ob ny l nz oa">N_STEPS = X.shape[1]<br/>N_FEATURES = X.shape[2]<br/>class_weight = {0: 1., 1: 1/np.mean(y_train)}</span><span id="51b7" class="nw mv it ns b gy ob ny l nz oa">ACTIVATION_FUNC = ‘relu’<br/>FINAL_ACTIVATION = ‘sigmoid’</span><span id="114c" class="nw mv it ns b gy ob ny l nz oa">LOSS_FUNC = ‘binary_crossentropy’<br/>METRIC = ‘accuracy’<br/>OPTIMIZER = Adam(learning_rate=LEARNING_RATE)</span><span id="a87e" class="nw mv it ns b gy ob ny l nz oa"># Split you own data from X_3D or X_4D, and a target<br/>X_train, y_train, X_val, y_val = None<br/>X_test, y_test = None</span></pre><p id="1e22" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">这里有几件事需要注意。我认为数据的第一个维度是实例的数量，第二个维度是时间步长的数量，第三个维度是特征的数量。对于2D卷积模型的四维数据，第四维是“虚拟”维度，以便进行适当的处理。然而，其余的维度保持相同的解释。</p><p id="42ba" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><strong class="lk jd">根据您为这些Keras型号使用的</strong>后端，了解订购很重要。不幸的是，约定并不一致，这可能是您错误的来源。如果是这种情况，请考虑调整数据的形状，以更改维度的顺序。</p><p id="3005" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我还特意将训练、校准和测试数据设置为“无”,以便您可以根据自己的喜好调整数据分割。在拟合期间，校准集用于评估模型。</p><p id="0cad" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">关于如何为不同类型的问题和训练模型分离数据的更多细节，你可以阅读这篇文章:</p><div class="pd pe gp gr pf pg"><a rel="noopener follow" target="_blank" href="/how-to-select-a-data-splitting-method-4cf6bc6991da"><div class="ph ab fo"><div class="pi ab pj cl cj pk"><h2 class="bd jd gy z fp pl fr fs pm fu fw jc bi translated">如何选择数据拆分方法</h2><div class="pn l"><h3 class="bd b gy z fp pl fr fs pm fu fw dk translated">不同数据拆分方法的优缺点及其背后的原因。</h3></div><div class="po l"><p class="bd b dl z fp pl fr fs pm fu fw dk translated">towardsdatascience.com</p></div></div><div class="pp l"><div class="pq l pr ps pt pp pu lb pg"/></div></div></a></div></div><div class="ab cl mn mo hx mp" role="separator"><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms"/></div><div class="im in io ip iq"><h1 id="ead5" class="mu mv it bd mw mx my mz na nb nc nd ne ki nf kj ng kl nh km ni ko nj kp nk nl bi translated"><strong class="ak">长短期记忆模型</strong></h1><p id="5746" class="pw-post-body-paragraph li lj it lk b ll nm kd ln lo nn kg lq lr no lt lu lv np lx ly lz nq mb mc md im bi translated">长短期记忆模型(LSTM)是递归神经网络(RNN)的一种变体。这些模型由多个通道组成，数据可以通过这些通道来维护以前步骤中的信息，以便在将来的步骤中使用。</p><p id="0b50" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">此处显示的预测模型中使用的LSTM单元遵循相同的结构。乍一看，它们看起来很复杂，但通过一些观察可以直观地解释。</p><p id="235c" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">穿过LSTM单元有两条主要路径；这些大致可以解释为短时记忆和长时记忆。短期是通过单元的底部路径，长期是通过单元的顶部路径。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pv"><img src="../Images/9bf028edbc8243468d75745c76de62b3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Xsv13wXJvbwBQDTE_J-ZxQ.jpeg"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">LSTM细胞(作者照片)</p></figure><p id="7f62" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">对于短期记忆，注意输入是如何被引导到这条路径的。正如人们对短期记忆的预期，模型与新数据Xt和上一步的输出相互作用。然后，该数据在单元中有几个可能的路径。</p><p id="85e1" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">长期记忆是沿着细胞顶部的路径。这条路径有来自先前长期记忆输出的输入。然后通过细胞，基于来自几个不同通道的更新来更新长期记忆。</p><p id="6da9" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">从短期到长期，LSTM细胞的不同路径通过所使用的激活函数和中间输出的组合来最清楚地区分。</p><p id="f762" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">请务必注意，sigmoid函数用于某些路径，双曲正切函数用于其他路径。这个选择是有意义的。这里的重要观察结果是，sigmoid函数的输出为0到1。相比之下，双曲正切函数的输出是-1比1。</p><p id="0c1f" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><em class="pw">现在，为什么会有区别呢？</em></p><p id="ef74" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">在短期和长期记忆路径之间的第一个通道中，激活是sigmoid函数，这意味着输出在0和1之间。因此，这条路径决定了当前步骤的输入是否对长期记忆更重要。如果这个输出是零，那么这个输出和前一个长期记忆输出之间的最终乘积为0。另一方面，如果这个输出更接近于1，那么先前的长期记忆会持续。</p><p id="b84e" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">第二和第三通道分别使用sigmoid和双曲线正切激活。这种组合作为一个两步过程，根据双曲正切函数的-1到1输出来确定当前信息是否相关，以及它的贡献是正还是负。细胞的这一部分基本上回答了这两个问题。然后将当前时间步长对长期记忆的贡献添加到长期记忆中。</p><p id="5a4f" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">最终路径结合了当前时间步长和包含在长期记忆路径中的当前理解。这条通路的输出成为下一个LSTM细胞的输出。</p><h2 id="1e5b" class="nw mv it bd mw oc od dn na oe of dp ne lr og oh ng lv oi oj ni lz ok ol nk iz bi translated">LSTM模型的设置</h2><pre class="ks kt ku kv gt nr ns nt nu aw nv bi"><span id="fba4" class="nw mv it ns b gy nx ny l nz oa">model = Sequential()</span><span id="7846" class="nw mv it ns b gy ob ny l nz oa">model.add(<br/>    LSTM(LSTM_UNITS, activation=ACTIVATION_FUNC, <br/>         input_shape=(N_STEPS, N_FEATURES) <br/>))<br/>model.add(Flatten())<br/>model.add(Dense(DENSE_UNITS, activation=ACTIVATION_FUNC))<br/>model.add(Dense(1, activation=FINAL_ACTIVATION))</span><span id="2917" class="nw mv it ns b gy ob ny l nz oa">model.compile(<br/>    optimizer=OPTIMIZER,<br/>    loss=LOSS_FUNC,<br/>    metrics=[tf.keras.metrics.AUC(), METRIC]<br/>)</span></pre></div><div class="ab cl mn mo hx mp" role="separator"><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms"/></div><div class="im in io ip iq"><h1 id="aa89" class="mu mv it bd mw mx my mz na nb nc nd ne ki nf kj ng kl nh km ni ko nj kp nk nl bi translated"><strong class="ak">卷积神经网络</strong></h1><p id="0bfb" class="pw-post-body-paragraph li lj it lk b ll nm kd ln lo nn kg lq lr no lt lu lv np lx ly lz nq mb mc md im bi translated">卷积网络是深度学习中最复杂的模型之一；它们通常由许多不同的层组成，并且这些层可以有不同的行为。</p><p id="b912" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">与具有全连接层的标准神经网络相比，卷积神经网络使用卷积层、汇集层和全连接层。一些先进的模型甚至使用了这些层越来越复杂的变化。</p><p id="e561" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">卷积神经网络CNN通常与图像和视频机器学习应用相关联，因为它们能够有效地捕捉像素之间的关系，并建立开放的这些关系以模拟日益复杂的结构。</p><p id="8cb2" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">在其核心，这些网络依靠其卷积层来有效地捕捉邻近像素之间的关系。然而，这种架构不限于多维图像。例如，很容易将时间序列数据转换成与CNN一致的格式。</p><p id="1707" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">您还可以使用卷积来模拟时间序列中不同步骤之间的交互。与图像相反，一维卷积可以模拟先前和未来时间步骤的行为，以从系列中捕捉复杂的关系。</p><p id="7053" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">一些一维卷积非常常见，你可能在不知道的情况下使用过它们。<strong class="lk jd">例如，移动平均是一种卷积</strong>类型，其中每一步的权重相等。所以，粗略地说，你可以把一维卷积看作不同长度的移动平均。周期中的每个时间步长的权重不同。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi px"><img src="../Images/96f12802c80a7a4733f3ae2ec88d122a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*E2lDgAjr1YLH2Y5AFP_QMA.jpeg"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">卷积比较(作者供图)</p></figure></div><div class="ab cl mn mo hx mp" role="separator"><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms"/></div><div class="im in io ip iq"><h1 id="f0f7" class="mu mv it bd mw mx my mz na nb nc nd ne ki nf kj ng kl nh km ni ko nj kp nk nl bi translated">1D卷积—时间序列CNN</h1><pre class="ks kt ku kv gt nr ns nt nu aw nv bi"><span id="6ebd" class="nw mv it ns b gy nx ny l nz oa">model = Sequential()<br/># Add one or more convolutional layers<br/>model.add(Conv1D(filters=N_FILTERS, kernel_size=10, activation=ACTIVATION_FUNC))</span><span id="cf92" class="nw mv it ns b gy ob ny l nz oa">model.add(MaxPooling1D(pool_size=2))<br/>model.add(Flatten())<br/>model.add(Dense(DENSE_UNITS, activation=ACTIVATION_FUNC))<br/>model.add(Dense(1, activation=FINAL_ACTIVATION))<br/>model.compile(<br/>    optimizer=OPTIMIZER, <br/>    loss=LOSS_FUNC,<br/>    metrics=[tf.keras.metrics.AUC(), METRIC]<br/>)</span></pre></div><div class="ab cl mn mo hx mp" role="separator"><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms"/></div><div class="im in io ip iq"><p id="0413" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">当你有多个时间序列时，一维卷积仍然是可能的；但是，使用二维卷积组合不同的序列可以捕获更复杂的关系。</p><p id="b948" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">这些高维卷积<strong class="lk jd">捕捉不同时间序列</strong>之间的关系。</p><h1 id="8a78" class="mu mv it bd mw mx py mz na nb pz nd ne ki qa kj ng kl qb km ni ko qc kp nk nl bi translated"><strong class="ak"> 2D卷积——时间序列CNN </strong></h1><pre class="ks kt ku kv gt nr ns nt nu aw nv bi"><span id="5637" class="nw mv it ns b gy nx ny l nz oa">model = Sequential()<br/># Add one or more convolutional layers<br/>model.add(Conv2D(<br/>    filters=N_FILTERS,<br/>    padding=”same”,<br/>    kernel_size=(2,2),<br/>    activation=ACTIVATION_FUNC,<br/>    input_shape=(N_STEPS, N_FEATURES, 1)),<br/>)</span><span id="7a44" class="nw mv it ns b gy ob ny l nz oa">model.add(MaxPooling2D(pool_size=2))<br/>model.add(Flatten())<br/>model.add(Dense(DENSE_UNITS, activation=ACTIVATION_FUNC))<br/>model.add(Dense(1, activation=FINAL_ACTIVATION))</span><span id="7bf6" class="nw mv it ns b gy ob ny l nz oa">model.compile(<br/>    optimizer=OPTIMIZER,<br/>    loss=LOSS_FUNC,<br/>    metrics=[tf.keras.metrics.AUC(), METRIC]<br/>)</span></pre></div><div class="ab cl mn mo hx mp" role="separator"><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms"/></div><div class="im in io ip iq"><h1 id="9a54" class="mu mv it bd mw mx my mz na nb nc nd ne ki nf kj ng kl nh km ni ko nj kp nk nl bi translated"><strong class="ak">拟合模型</strong></h1><p id="aad7" class="pw-post-body-paragraph li lj it lk b ll nm kd ln lo nn kg lq lr no lt lu lv np lx ly lz nq mb mc md im bi translated">为了训练每个模型，您可以使用以下相同的代码块。因为模型被假定为分类器，所以拟合包括类别权重。在大多数情况下，职业并不是完全平衡的，所以这个增加考虑到了这一点。</p><pre class="ks kt ku kv gt nr ns nt nu aw nv bi"><span id="2e8e" class="nw mv it ns b gy nx ny l nz oa">model.fit(<br/>    X_train, y_train, epochs=EPOCHS,<br/>    verbose=1, class_weight=class_weight,<br/>    batch_size=BATCH_SIZE,<br/>    validation_data=(X_val, y_val),<br/>)</span></pre></div><div class="ab cl mn mo hx mp" role="separator"><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms"/></div><div class="im in io ip iq"><h1 id="8e24" class="mu mv it bd mw mx my mz na nb nc nd ne ki nf kj ng kl nh km ni ko nj kp nk nl bi translated"><strong class="ak">结论</strong></h1><p id="b74a" class="pw-post-body-paragraph li lj it lk b ll nm kd ln lo nn kg lq lr no lt lu lv np lx ly lz nq mb mc md im bi translated">时间序列问题有许多活动部分。然而，许多其他简单得多的模型也可以用于合理程度的性能。</p><p id="50a0" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">当从多变量时间序列数据中创建分类器(或回归器)时，我还忽略了一些更多涉及时间的部分。两个最重要的是:</p><ul class=""><li id="3d0d" class="om on it lk b ll lm lo lp lr oo lv op lz oq md qd os ot ou bi translated">当时间步长不一致时会发生什么，以及如何标准化数据和处理差距。</li><li id="0008" class="om on it lk b ll ov lo ow lr ox lv oy lz oz md qd os ot ou bi translated">深度学习模型架构如何优化，不是所有的模型都是生而平等的。虽然您可能不需要100层的网络，但您仍然可以从一些调整中受益。</li></ul><p id="4233" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">然而，使用多元时间序列数据构建模型可能是一项令人愉快的任务。它迫使您不要将所有数据都视为可用于scikit-learn模型的表格数据。这些模型非常灵活，可以捕捉复杂的时间相关关系。你应该将它们添加到你现有的车型库中。</p></div><div class="ab cl mn mo hx mp" role="separator"><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms"/></div><div class="im in io ip iq"><p id="32a2" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><em class="pw">如果你有兴趣阅读关于新颖的数据科学工具和理解机器学习算法的文章，可以考虑在Medium上关注我。</em></p><p id="4f22" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><em class="pw">如果你对我的写作感兴趣，想直接支持我，请通过以下链接订阅。这个链接确保我会收到你的会员费的一部分。</em></p><div class="pd pe gp gr pf pg"><a href="https://zjwarnes.medium.com/membership" rel="noopener follow" target="_blank"><div class="ph ab fo"><div class="pi ab pj cl cj pk"><h2 class="bd jd gy z fp pl fr fs pm fu fw jc bi translated">通过我的推荐链接加入Medium-Zachary Warnes</h2><div class="pn l"><h3 class="bd b gy z fp pl fr fs pm fu fw dk translated">阅读扎卡里·沃恩斯(以及媒体上成千上万的其他作家)的每一个故事。您的会员费直接支持…</h3></div><div class="po l"><p class="bd b dl z fp pl fr fs pm fu fw dk translated">zjwarnes.medium.com</p></div></div><div class="pp l"><div class="qe l pr ps pt pp pu lb pg"/></div></div></a></div></div></div>    
</body>
</html>