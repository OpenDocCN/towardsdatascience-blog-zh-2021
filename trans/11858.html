<html>
<head>
<title>Run Pandas as Fast as Spark</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">跑熊猫像火花一样快</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/run-pandas-as-fast-as-spark-f5eefe780c45?source=collection_archive---------1-----------------------#2021-11-27">https://towardsdatascience.com/run-pandas-as-fast-as-spark-f5eefe780c45?source=collection_archive---------1-----------------------#2021-11-27</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="d6dd" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">为什么Spark上的熊猫API完全改变了游戏规则</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/f98a4256583d756dc9687b1bc4a37855.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*NFPFqCGZ7Pb6NSSk"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">克莱顿·霍尔姆斯在<a class="ae kv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="f2ae" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">就是这样。它出来了。Spark现在有一个熊猫API。</p><p id="f538" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">似乎每次你想使用数据框架时，你都必须打开一个放着所有工具的杂乱抽屉，并仔细寻找合适的工具。</p><p id="9bd9" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">如果你处理结构化数据，<a class="ae kv" href="https://www.dataquest.io/blog/why-sql-is-the-most-important-language-to-learn/" rel="noopener ugc nofollow" target="_blank">你需要SQL </a>。熊猫也总是在那里。Spark是大数据不可或缺的。工具箱里有满足各种需求的工具。但是你再也不需要工具箱了，因为Spark已经成为了终极的瑞士军刀。</p><p id="00b7" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这一切都始于2019 Spark + AI峰会。考拉，一个开源项目，使熊猫能够在Spark上使用，已经启动。起初，它只涵盖了熊猫功能的一小部分，但它逐渐成长。两年过去了，现在，在新的Spark 3.2版本中，考拉已经并入PySpark。结果很棒。</p><p id="a188" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">Spark现在集成了Pandas API，所以你可以在Spark上运行Pandas。你只需要修改一行代码:</p><pre class="kg kh ki kj gt ls lt lu lv aw lw bi"><span id="54ad" class="lx ly iq lt b gy lz ma l mb mc">import pyspark.pandas as ps</span></pre><p id="75f1" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">是不是很棒？</p><p id="8dff" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">由于这一点，我们可以获得广泛的好处:</p><ul class=""><li id="ad98" class="md me iq ky b kz la lc ld lf mf lj mg ln mh lr mi mj mk ml bi translated">如果你用熊猫但是不熟悉Spark，马上就可以用Spark工作，没有学习曲线。</li><li id="838e" class="md me iq ky b kz mm lc mn lf mo lj mp ln mq lr mi mj mk ml bi translated">你可以有一个单一的代码库来处理所有事情:小数据和大数据。单机和分布式机器。</li><li id="5b74" class="md me iq ky b kz mm lc mn lf mo lj mp ln mq lr mi mj mk ml bi translated">你可以更快地运行你的熊猫代码。</li></ul><p id="f35e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这最后一点尤其值得注意。</p><p id="a3f5" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">一方面，您可以在Pandas中将分布式计算应用到您的代码中。但好处不止于此。多亏了Spark引擎，你的代码会更快<a class="ae kv" href="https://databricks.com/blog/2021/10/04/pandas-api-on-upcoming-apache-spark-3-2.html#attachment_165784" rel="noopener ugc nofollow" target="_blank">甚至在单机上</a>！</p><p id="4226" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">如果你想知道，是的，看起来熊猫星火也比Dask快。</p><p id="8ab8" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我对这一突破感到非常兴奋，那么，我们为什么不进入正题呢？让我们用Spark上的熊猫API做一些代码吧！</p><h1 id="64b7" class="mr ly iq bd ms mt mu mv mw mx my mz na jw nb jx nc jz nd ka ne kc nf kd ng nh bi translated">在熊猫、熊猫火花和火花之间切换</h1><p id="6fa3" class="pw-post-body-paragraph kw kx iq ky b kz ni jr lb lc nj ju le lf nk lh li lj nl ll lm ln nm lp lq lr ij bi translated">我们需要知道的第一件事是我们到底在做什么。当与熊猫一起工作时，我们使用类<code class="fe nn no np lt b">pandas.core.frame.DataFrame</code>。当在Spark中使用pandas API时，我们使用类<code class="fe nn no np lt b">pyspark.pandas.frame.DataFrame</code>。两者相似，但不相同。主要区别是前者在一台机器上，而后者是分布式的。</p><p id="3bab" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们可以创建一个熊猫在火花上的数据帧，并将其转换为熊猫，反之亦然:</p><pre class="kg kh ki kj gt ls lt lu lv aw lw bi"><span id="9598" class="lx ly iq lt b gy lz ma l mb mc"># import Pandas-on-Spark<br/>import pyspark.pandas as ps</span><span id="e399" class="lx ly iq lt b gy nq ma l mb mc"># Create a DataFrame with Pandas-on-Spark<br/>ps_df = ps.DataFrame(range(10))</span><span id="05e7" class="lx ly iq lt b gy nq ma l mb mc"># Convert a Pandas-on-Spark Dataframe into a Pandas Dataframe<br/>pd_df = ps_df.to_pandas()</span><span id="87ae" class="lx ly iq lt b gy nq ma l mb mc"># Convert a Pandas Dataframe into a Pandas-on-Spark Dataframe<br/>ps_df = ps.from_pandas(pd_df)</span></pre><p id="9918" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">请注意，如果您使用多台机器，当将Pandas-on-Spark数据帧转换为Pandas数据帧时，数据会从多台机器传输到一台机器，反之亦然(参见<a class="ae kv" href="https://spark.apache.org/docs/latest/api/python/user_guide/pandas_on_spark/pandas_pyspark.html" rel="noopener ugc nofollow" target="_blank"> PySpark指南</a>)。</p><p id="7250" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们还可以将Pandas-on-Spark数据帧转换为Spark数据帧，反之亦然:</p><pre class="kg kh ki kj gt ls lt lu lv aw lw bi"><span id="9e1f" class="lx ly iq lt b gy lz ma l mb mc"># Create a DataFrame with Pandas-on-Spark<br/>ps_df = ps.DataFrame(range(10))</span><span id="dd01" class="lx ly iq lt b gy nq ma l mb mc"># Convert a Pandas-on-Spark Dataframe into a Spark Dataframe<br/>spark_df = ps_df.to_spark()</span><span id="ecad" class="lx ly iq lt b gy nq ma l mb mc"># Convert a Spark Dataframe into a Pandas-on-Spark Dataframe<br/>ps_df_new = spark_df.to_pandas_on_spark()</span></pre><h2 id="05a2" class="lx ly iq bd ms nr ns dn mw nt nu dp na lf nv nw nc lj nx ny ne ln nz oa ng ob bi translated">数据类型会发生什么？</h2><p id="e500" class="pw-post-body-paragraph kw kx iq ky b kz ni jr lb lc nj ju le lf nk lh li lj nl ll lm ln nm lp lq lr ij bi translated">当使用Pandas-on-Spark和Pandas时，数据类型基本相同。当将Pandas-on-Spark数据帧转换为Spark数据帧时，数据类型会自动转换为适当的类型(参见<a class="ae kv" href="https://spark.apache.org/docs/latest/api/python/user_guide/pandas_on_spark/types.html" rel="noopener ugc nofollow" target="_blank"> PySpark指南</a>)</p><h1 id="6ed8" class="mr ly iq bd ms mt mu mv mw mx my mz na jw nb jx nc jz nd ka ne kc nf kd ng nh bi translated"><strong class="ak">用熊猫火花复制火花功能</strong></h1><p id="3a33" class="pw-post-body-paragraph kw kx iq ky b kz ni jr lb lc nj ju le lf nk lh li lj nl ll lm ln nm lp lq lr ij bi translated">本节的目的是提供一个备忘单，其中包含管理Spark中的数据帧以及Pandas-on-Spark中类似数据帧的最常用函数。注意，Pandas-on-Spark和Pandas在语法上的唯一区别只是<code class="fe nn no np lt b">import pyspark.pandas as ps</code>行。</p><p id="a516" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">您将看到，即使您不熟悉Spark，也可以通过Pandas API轻松使用它。</p><h2 id="ab5f" class="lx ly iq bd ms nr ns dn mw nt nu dp na lf nv nw nc lj nx ny ne ln nz oa ng ob bi translated"><strong class="ak">导入库</strong></h2><pre class="kg kh ki kj gt ls lt lu lv aw lw bi"><span id="576c" class="lx ly iq lt b gy lz ma l mb mc"># For running Spark<br/>from pyspark.sql import SparkSession<br/>spark = SparkSession.builder.appName("Spark").getOrCreate()</span><span id="9b8f" class="lx ly iq lt b gy nq ma l mb mc"># For running Pandas on top of Spark<br/>import pyspark.pandas as ps</span></pre><h2 id="90ae" class="lx ly iq bd ms nr ns dn mw nt nu dp na lf nv nw nc lj nx ny ne ln nz oa ng ob bi translated"><strong class="ak">读取数据</strong></h2><p id="d2bf" class="pw-post-body-paragraph kw kx iq ky b kz ni jr lb lc nj ju le lf nk lh li lj nl ll lm ln nm lp lq lr ij bi translated">让我们以老狗虹膜数据集为例。</p><pre class="kg kh ki kj gt ls lt lu lv aw lw bi"><span id="8a05" class="lx ly iq lt b gy lz ma l mb mc"># SPARK<br/>sdf = spark.read.options(inferSchema='True',<br/>              header='True').csv('iris.csv')</span><span id="d2d7" class="lx ly iq lt b gy nq ma l mb mc"># PANDAS-ON-SPARK<br/>pdf = ps.read_csv('iris.csv')</span></pre><h2 id="3de2" class="lx ly iq bd ms nr ns dn mw nt nu dp na lf nv nw nc lj nx ny ne ln nz oa ng ob bi translated"><strong class="ak">选择</strong></h2><pre class="kg kh ki kj gt ls lt lu lv aw lw bi"><span id="ad67" class="lx ly iq lt b gy lz ma l mb mc"># SPARK<br/>sdf.select("sepal_length","sepal_width").show()</span><span id="fe39" class="lx ly iq lt b gy nq ma l mb mc"># PANDAS-ON-SPARK<br/>pdf[["sepal_length","sepal_width"]].head()</span></pre><h2 id="d548" class="lx ly iq bd ms nr ns dn mw nt nu dp na lf nv nw nc lj nx ny ne ln nz oa ng ob bi translated"><strong class="ak">删除列</strong></h2><pre class="kg kh ki kj gt ls lt lu lv aw lw bi"><span id="bdf9" class="lx ly iq lt b gy lz ma l mb mc"># SPARK<br/>sdf.drop('sepal_length').show()</span><span id="a12d" class="lx ly iq lt b gy nq ma l mb mc"># PANDAS-ON-SPARK<br/>pdf.drop('sepal_length').head()</span></pre><h2 id="41be" class="lx ly iq bd ms nr ns dn mw nt nu dp na lf nv nw nc lj nx ny ne ln nz oa ng ob bi translated">删除重复项</h2><pre class="kg kh ki kj gt ls lt lu lv aw lw bi"><span id="7ac5" class="lx ly iq lt b gy lz ma l mb mc"># SPARK<br/>sdf.dropDuplicates(["sepal_length","sepal_width"]).show()</span><span id="b738" class="lx ly iq lt b gy nq ma l mb mc"># PANDAS-ON-SPARK<br/>pdf[["sepal_length", "sepal_width"]].drop_duplicates()</span></pre><h2 id="6b1d" class="lx ly iq bd ms nr ns dn mw nt nu dp na lf nv nw nc lj nx ny ne ln nz oa ng ob bi translated"><strong class="ak">滤镜</strong></h2><pre class="kg kh ki kj gt ls lt lu lv aw lw bi"><span id="d2ac" class="lx ly iq lt b gy lz ma l mb mc"># SPARK<br/>sdf.filter( (sdf.flower_type == "Iris-setosa") &amp; (sdf.petal_length &gt; 1.5) ).show()</span><span id="3a4c" class="lx ly iq lt b gy nq ma l mb mc"># PANDAS-ON-SPARK<br/>pdf.loc[ (pdf.flower_type == "Iris-setosa") &amp; (pdf.petal_length &gt; 1.5) ].head()</span></pre><h2 id="7223" class="lx ly iq bd ms nr ns dn mw nt nu dp na lf nv nw nc lj nx ny ne ln nz oa ng ob bi translated"><strong class="ak">计数</strong></h2><pre class="kg kh ki kj gt ls lt lu lv aw lw bi"><span id="5915" class="lx ly iq lt b gy lz ma l mb mc"># SPARK<br/>sdf.filter(sdf.flower_type == "Iris-virginica").count()</span><span id="5a66" class="lx ly iq lt b gy nq ma l mb mc"># PANDAS-ON-SPARK<br/>pdf.loc[pdf.flower_type == "Iris-virginica"].count()</span></pre><h2 id="d8df" class="lx ly iq bd ms nr ns dn mw nt nu dp na lf nv nw nc lj nx ny ne ln nz oa ng ob bi translated"><strong class="ak">截然不同的</strong></h2><pre class="kg kh ki kj gt ls lt lu lv aw lw bi"><span id="c0b3" class="lx ly iq lt b gy lz ma l mb mc"># SPARK<br/>sdf.select("flower_type").distinct().show()</span><span id="0352" class="lx ly iq lt b gy nq ma l mb mc"># PANDAS-ON-SPARK<br/>pdf["flower_type"].unique()</span></pre><h2 id="658f" class="lx ly iq bd ms nr ns dn mw nt nu dp na lf nv nw nc lj nx ny ne ln nz oa ng ob bi translated"><strong class="ak">排序</strong></h2><pre class="kg kh ki kj gt ls lt lu lv aw lw bi"><span id="4b1d" class="lx ly iq lt b gy lz ma l mb mc"># SPARK<br/>sdf.sort("sepal_length", "sepal_width").show()</span><span id="08a0" class="lx ly iq lt b gy nq ma l mb mc"># PANDAS-ON-SPARK<br/>pdf.sort_values(["sepal_length", "sepal_width"]).head()</span></pre><h2 id="35ea" class="lx ly iq bd ms nr ns dn mw nt nu dp na lf nv nw nc lj nx ny ne ln nz oa ng ob bi translated"><strong class="ak">分组依据</strong></h2><pre class="kg kh ki kj gt ls lt lu lv aw lw bi"><span id="231d" class="lx ly iq lt b gy lz ma l mb mc"># SPARK<br/>sdf.groupBy("flower_type").count().show()</span><span id="017f" class="lx ly iq lt b gy nq ma l mb mc"># PANDAS-ON-SPARK<br/>pdf.groupby("flower_type").count()</span></pre><h2 id="70b2" class="lx ly iq bd ms nr ns dn mw nt nu dp na lf nv nw nc lj nx ny ne ln nz oa ng ob bi translated"><strong class="ak">更换</strong></h2><pre class="kg kh ki kj gt ls lt lu lv aw lw bi"><span id="e3e7" class="lx ly iq lt b gy lz ma l mb mc"># SPARK<br/>sdf.replace("Iris-setosa", "setosa").show()</span><span id="91d0" class="lx ly iq lt b gy nq ma l mb mc"># PANDAS-ON-SPARK<br/>pdf.replace("Iris-setosa", "setosa").head()</span></pre><h2 id="8897" class="lx ly iq bd ms nr ns dn mw nt nu dp na lf nv nw nc lj nx ny ne ln nz oa ng ob bi translated"><strong class="ak">加入</strong></h2><pre class="kg kh ki kj gt ls lt lu lv aw lw bi"><span id="39ed" class="lx ly iq lt b gy lz ma l mb mc"># SPARK<br/>sdf.union(sdf)</span><span id="e260" class="lx ly iq lt b gy nq ma l mb mc"># PANDAS-ON-SPARK<br/>pdf.append(pdf)</span></pre><h1 id="c106" class="mr ly iq bd ms mt mu mv mw mx my mz na jw nb jx nc jz nd ka ne kc nf kd ng nh bi translated">结论</h1><p id="3c2c" class="pw-post-body-paragraph kw kx iq ky b kz ni jr lb lc nj ju le lf nk lh li lj nl ll lm ln nm lp lq lr ij bi translated">从现在开始，你将可以在Spark中使用熊猫。这导致了Pandas速度的提高，迁移到Spark时学习曲线的减少，以及单机计算和分布式计算在同一代码库中的合并。</p><p id="22af" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我想留给读者几个问题:</p><ul class=""><li id="c2d0" class="md me iq ky b kz la lc ld lf mf lj mg ln mh lr mi mj mk ml bi translated">你认为Spark会成为管理数据框架的终极瑞士军刀吗？</li><li id="ca81" class="md me iq ky b kz mm lc mn lf mo lj mp ln mq lr mi mj mk ml bi translated">熊猫会像Dask或者Vaex一样杀光其他库吗？</li><li id="70bc" class="md me iq ky b kz mm lc mn lf mo lj mp ln mq lr mi mj mk ml bi translated">熊猫用户会逐渐迁移到Spark吗？</li><li id="5633" class="md me iq ky b kz mm lc mn lf mo lj mp ln mq lr mi mj mk ml bi translated">我们将来会看到<code class="fe nn no np lt b">import pandas as pd</code>吗？</li></ul></div><div class="ab cl oc od hu oe" role="separator"><span class="of bw bk og oh oi"/><span class="of bw bk og oh oi"/><span class="of bw bk og oh"/></div><div class="ij ik il im in"><h1 id="0633" class="mr ly iq bd ms mt oj mv mw mx ok mz na jw ol jx nc jz om ka ne kc on kd ng nh bi translated">参考</h1><ul class=""><li id="37a5" class="md me iq ky b kz ni lc nj lf oo lj op ln oq lr mi mj mk ml bi translated"><a class="ae kv" href="https://spark.apache.org/docs/latest/api/python/user_guide/pandas_on_spark/index.html" rel="noopener ugc nofollow" target="_blank"> Spark用户指南:Spark上的熊猫API</a></li><li id="9461" class="md me iq ky b kz mm lc mn lf mo lj mp ln mq lr mi mj mk ml bi translated"><a class="ae kv" href="https://databricks.com/blog/2021/10/04/pandas-api-on-upcoming-apache-spark-3-2.html" rel="noopener ugc nofollow" target="_blank">即将发布的Apache Spark 3.2的熊猫API</a></li></ul></div></div>    
</body>
</html>