<html>
<head>
<title>How To Create A JSON Data Stream With PySpark &amp; Faker</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如何用PySpark &amp; Faker创建JSON数据流</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/build-a-data-streaming-job-in-5-mins-with-pyspark-faker-a4f3d2420384?source=collection_archive---------11-----------------------#2021-07-20">https://towardsdatascience.com/build-a-data-streaming-job-in-5-mins-with-pyspark-faker-a4f3d2420384?source=collection_archive---------11-----------------------#2021-07-20</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="e291" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">在本教程中，您将学习如何生成JSON格式的实时数据流，将文件存储在AWS S3桶中，以及使用Python (Spark)动态聚合数据。</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/604aa6b216e1134f9245149dcf276e7e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5joWSX0H2iOC6QQXoUd5mw.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">Evgeny <a class="ae ky" href="https://www.pexels.com/@evgeny-tchebotarev-1058775" rel="noopener ugc nofollow" target="_blank"> Tchebotarev </a>在<a class="ae ky" href="https://www.pexels.com/@evgeny-tchebotarev-1058775" rel="noopener ugc nofollow" target="_blank">像素</a>上拍摄的照片</p></figure><h2 id="0fdf" class="kz la it bd lb lc ld dn le lf lg dp lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">建议的点播课程</h2><p id="cf54" class="pw-post-body-paragraph lv lw it lx b ly lz ju ma mb mc jx md li me mf mg lm mh mi mj lq mk ml mm mn im bi translated"><em class="mo">我的一些读者联系我，要求提供点播课程，以了解更多关于</em><strong class="lx iu"><em class="mo">Apache Spark</em></strong><em class="mo">的信息。这是我推荐的3个很好的资源:</em></p><ul class=""><li id="6eec" class="mp mq it lx b ly mr mb ms li mt lm mu lq mv mn mw mx my mz bi translated"><a class="ae ky" href="https://imp.i115008.net/zaX10r" rel="noopener ugc nofollow" target="_blank"> <strong class="lx iu">用阿帕奇Kafka进行数据流&amp;阿帕奇Spark nano degree(uda city)</strong></a></li><li id="2d3e" class="mp mq it lx b ly na mb nb li nc lm nd lq ne mn mw mx my mz bi translated"><a class="ae ky" href="https://imp.i115008.net/zaX10r" rel="noopener ugc nofollow" target="_blank"> <strong class="lx iu">数据工程纳米学位(Udacity) </strong> </a></li></ul><h1 id="131a" class="nf la it bd lb ng nh ni le nj nk nl lh jz nm ka ll kc nn kd lp kf no kg lt np bi translated">问题:生成流数据集</h1><p id="c2d8" class="pw-post-body-paragraph lv lw it lx b ly lz ju ma mb mc jx md li me mf mg lm mh mi mj lq mk ml mm mn im bi translated">如果您熟悉PySpark及其结构化流API，您就会知道将流作业表示为标准批处理作业是多么容易，不同之处在于数据流可以被视为一个不断追加的表。</p><p id="4838" class="pw-post-body-paragraph lv lw it lx b ly mr ju ma mb ms jx md li nq mf mg lm nr mi mj lq ns ml mm mn im bi translated">尽管以简单明了的方式编写了一个流处理模型，但是寻找流数据源可能是一项具有挑战性的任务，尤其是在部署应用程序之前测试应用程序，或者在学习PySpark时设置第一个流作业。</p><p id="aa73" class="pw-post-body-paragraph lv lw it lx b ly mr ju ma mb ms jx md li nq mf mg lm nr mi mj lq ns ml mm mn im bi translated">为了让你的工作生活更轻松，学习过程更快，在本教程中，我将向你展示如何使用Python的<code class="fe nt nu nv nw b">faker</code>和<code class="fe nt nu nv nw b">random</code>包产生一个模拟数据流，以JSON格式序列化。</p><p id="9352" class="pw-post-body-paragraph lv lw it lx b ly mr ju ma mb ms jx md li nq mf mg lm nr mi mj lq ns ml mm mn im bi translated">此外，您将学习如何将数据流保存在S3桶中(免费)，然后使用PySpark动态读取和聚合数据流。所有这些步骤只需要5分钟。</p><h1 id="22df" class="nf la it bd lb ng nh ni le nj nk nl lh jz nm ka ll kc nn kd lp kf no kg lt np bi translated">先决条件和安装</h1><p id="6138" class="pw-post-body-paragraph lv lw it lx b ly lz ju ma mb mc jx md li me mf mg lm mh mi mj lq mk ml mm mn im bi translated">为了快速完成这一过程，在您进一步阅读之前，我建议您:</p><ol class=""><li id="ae7c" class="mp mq it lx b ly mr mb ms li mt lm mu lq mv mn nx mx my mz bi translated">在<a class="ae ky" href="https://aws.amazon.com/" rel="noopener ugc nofollow" target="_blank"> AWS </a>上创建一个免费账户以及一个<a class="ae ky" href="https://docs.aws.amazon.com/rekognition/latest/dg/setting-up.html" rel="noopener ugc nofollow" target="_blank"> IAM用户。记得把<code class="fe nt nu nv nw b">ACCESS KEY ID</code>和<code class="fe nt nu nv nw b">SECRET ACCESS KEY</code>都留着以后用。设置您的IAM用户，使其拥有一个<code class="fe nt nu nv nw b">AmazonS3FullAccess</code>。</a></li><li id="db42" class="mp mq it lx b ly na mb nb li nc lm nd lq ne mn nx mx my mz bi translated">安装<code class="fe nt nu nv nw b">boto3</code>包并创建一个配置文件，以编程方式使用上面生成的两个密钥。</li><li id="322e" class="mp mq it lx b ly na mb nb li nc lm nd lq ne mn nx mx my mz bi translated">访问<a class="ae ky" href="https://community.cloud.databricks.com/login.html" rel="noopener ugc nofollow" target="_blank"> Databricks的社区版</a>，然后<a class="ae ky" href="https://docs.databricks.com/clusters/configure.html" rel="noopener ugc nofollow" target="_blank">配置一个标准集群</a>并将其连接到您将运行PySpark代码的笔记本上。即使您以前没有使用该平台的经验，这项任务也应该相当直观。</li><li id="4938" class="mp mq it lx b ly na mb nb li nc lm nd lq ne mn nx mx my mz bi translated">安装剩余的软件包，其中有<code class="fe nt nu nv nw b">faker</code>、<code class="fe nt nu nv nw b">random</code>、<code class="fe nt nu nv nw b">uuid</code>和<code class="fe nt nu nv nw b">json</code>。</li></ol><p id="82fc" class="pw-post-body-paragraph lv lw it lx b ly mr ju ma mb ms jx md li nq mf mg lm nr mi mj lq ns ml mm mn im bi translated">如果是第一次在<code class="fe nt nu nv nw b">S3</code>创建IAM用户，以及<code class="fe nt nu nv nw b">config</code>文件，建议先看一下<a class="ae ky" href="https://realpython.com/python-boto3-aws-s3/" rel="noopener ugc nofollow" target="_blank">本教程</a>的第一段。</p><p id="2ef9" class="pw-post-body-paragraph lv lw it lx b ly mr ju ma mb ms jx md li nq mf mg lm nr mi mj lq ns ml mm mn im bi translated">一旦你完成了这些安装(<em class="mo">大约需要30-45分钟</em>)，你就可以开始生成一个模拟数据流了！</p><h1 id="bef5" class="nf la it bd lb ng nh ni le nj nk nl lh jz nm ka ll kc nn kd lp kf no kg lt np bi translated">创建您的数据生产者</h1><p id="9a0f" class="pw-post-body-paragraph lv lw it lx b ly lz ju ma mb mc jx md li me mf mg lm mh mi mj lq mk ml mm mn im bi translated">第一步是使用以下命令创建一个带有<code class="fe nt nu nv nw b">boto3</code>(在本例中为<code class="fe nt nu nv nw b">data-stream-dump</code>)的<code class="fe nt nu nv nw b">S3</code>桶:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ny nz l"/></div></figure><p id="6b3c" class="pw-post-body-paragraph lv lw it lx b ly mr ju ma mb ms jx md li nq mf mg lm nr mi mj lq ns ml mm mn im bi translated">然后，我创建一个名为<code class="fe nt nu nv nw b">Transaction</code>的<code class="fe nt nu nv nw b">data class</code> ( <em class="mo">如果数据类和装饰器对你来说是一个新概念，请看</em> <a class="ae ky" href="https://realpython.com/python-data-classes/" rel="noopener ugc nofollow" target="_blank"> <em class="mo">本教程</em> </a>)，它由3个字段组成:</p><ul class=""><li id="68a8" class="mp mq it lx b ly mr mb ms li mt lm mu lq mv mn mw mx my mz bi translated"><code class="fe nt nu nv nw b">username</code>是一个字符串，将通过调用<code class="fe nt nu nv nw b">faker.user_name</code>随机生成；</li><li id="a521" class="mp mq it lx b ly na mb nb li nc lm nd lq ne mn mw mx my mz bi translated"><code class="fe nt nu nv nw b">currency</code>是一个字符串，在属于<code class="fe nt nu nv nw b">currencies</code>列表的字符串中取一个随机值。我将货币限制为3种，以使稍后在PySpark中执行的聚合更加明显，但是您也可以使用<code class="fe nt nu nv nw b">faker.currency</code>生成随机货币。</li><li id="3f6a" class="mp mq it lx b ly na mb nb li nc lm nd lq ne mn mw mx my mz bi translated"><code class="fe nt nu nv nw b">amount</code>也是一个字符串，被定义为介于<code class="fe nt nu nv nw b">100</code>和<code class="fe nt nu nv nw b">200,000</code>之间的随机整数。</li></ul><p id="ec2c" class="pw-post-body-paragraph lv lw it lx b ly mr ju ma mb ms jx md li nq mf mg lm nr mi mj lq ns ml mm mn im bi translated">我现在可以使用<code class="fe nt nu nv nw b">Transaction</code>来定义一个<code class="fe nt nu nv nw b">serialize(self)</code>方法(<em class="mo">是的，您可以将方法定义为数据类</em>的一部分)，它有效地用于将这3个字段格式化为一个字典:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ny nz l"/></div></figure><p id="3a94" class="pw-post-body-paragraph lv lw it lx b ly mr ju ma mb ms jx md li nq mf mg lm nr mi mj lq ns ml mm mn im bi translated">现在每次我打电话:</p><pre class="kj kk kl km gt oa nw ob oc aw od bi"><span id="9b44" class="kz la it nw b gy oe of l og oh">print(Transaction().serialize())</span></pre><p id="8e27" class="pw-post-body-paragraph lv lw it lx b ly mr ju ma mb ms jx md li nq mf mg lm nr mi mj lq ns ml mm mn im bi translated">我得到一个假的<em class="mo">银行交易</em>，由一个假的<code class="fe nt nu nv nw b">username</code>在一个假的<code class="fe nt nu nv nw b">currency</code>和一个假的<code class="fe nt nu nv nw b">amount</code>中创建:</p><pre class="kj kk kl km gt oa nw ob oc aw od bi"><span id="21ee" class="kz la it nw b gy oe of l og oh">{‘username’: ‘jjohnson’, ‘currency’: ‘GBP’, ‘amount’: 102884}</span></pre><p id="c1a6" class="pw-post-body-paragraph lv lw it lx b ly mr ju ma mb ms jx md li nq mf mg lm nr mi mj lq ns ml mm mn im bi translated">使用数据类和faker包很酷的一点是，您可以不断增加或改变字段的数量和类型，以满足您的特定需求。</p><blockquote class="oi"><p id="9a15" class="oj ok it bd ol om on oo op oq or mn dk translated">使用数据类和faker包很酷的一点是，您可以不断增加或改变字段的数量和类型，以满足您的特定需求。</p></blockquote><p id="32f0" class="pw-post-body-paragraph lv lw it lx b ly os ju ma mb ot jx md li ou mf mg lm ov mi mj lq ow ml mm mn im bi translated">我终于准备好定义一个<code class="fe nt nu nv nw b">Producer()</code>函数，并将其传递给一个运行100次的循环:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ny nz l"/></div></figure><p id="b39d" class="pw-post-body-paragraph lv lw it lx b ly mr ju ma mb ms jx md li nq mf mg lm nr mi mj lq ns ml mm mn im bi translated">每次调用<code class="fe nt nu nv nw b">Producer()</code>函数时，它都会以<code class="fe nt nu nv nw b">json</code>格式将单个事务写入一个文件(<em class="mo">上传到S3 </em>)，该文件的名称采用标准根<code class="fe nt nu nv nw b">transaction_</code>加上一个<code class="fe nt nu nv nw b">uuid</code>代码，以使其唯一。</p><p id="6560" class="pw-post-body-paragraph lv lw it lx b ly mr ju ma mb ms jx md li nq mf mg lm nr mi mj lq ns ml mm mn im bi translated">在这种情况下，<strong class="lx iu">循环将生成100个文件，每个文件之间的间隔为3秒</strong>，以模拟真实的数据流，其中流应用程序监听外部微服务。</p><p id="e90b" class="pw-post-body-paragraph lv lw it lx b ly mr ju ma mb ms jx md li nq mf mg lm nr mi mj lq ns ml mm mn im bi translated">完整的代码，直到这一点是可用的<a class="ae ky" href="https://gist.github.com/anbento0490/05f718cf4797a8d9536d00cb2e9704cb" rel="noopener ugc nofollow" target="_blank">在这里</a>。我将很快编译它，但在此之前，我需要先创建一个流作业。</p><h2 id="1809" class="kz la it bd lb lc ld dn le lf lg dp lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">在数据砖的FS中安装你的S3桶</h2><p id="2755" class="pw-post-body-paragraph lv lw it lx b ly lz ju ma mb mc jx md li me mf mg lm mh mi mj lq mk ml mm mn im bi translated">我的流作业将写在DataBrick CE笔记本中，如下所示:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ox"><img src="../Images/a53c26cc858efe2f97ed8f59b75ee2d5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5O7M6Kln9lZzRX3Cix4LoQ.png"/></div></div></figure><p id="7c64" class="pw-post-body-paragraph lv lw it lx b ly mr ju ma mb ms jx md li nq mf mg lm nr mi mj lq ns ml mm mn im bi translated">如果您希望您的流作业监听S3存储桶中发生的事情，您需要将您的亚马逊S3存储桶作为文件系统“挂载”。</p><p id="8d0c" class="pw-post-body-paragraph lv lw it lx b ly mr ju ma mb ms jx md li nq mf mg lm nr mi mj lq ns ml mm mn im bi translated">挂载是指你可以使用Databricks的平台“<a class="ae ky" href="https://cloud.netapp.com/blog/amazon-s3-as-a-file-system" rel="noopener ugc nofollow" target="_blank"> <em class="mo">”与亚马逊S3桶进行交互，对文件和文件夹</em> </a>进行读写操作。</p><p id="90b3" class="pw-post-body-paragraph lv lw it lx b ly mr ju ma mb ms jx md li nq mf mg lm nr mi mj lq ns ml mm mn im bi translated">有一些关于如何在Databricks中挂载文件系统的文档，但是它们让您相信只有获得高级许可才有可能。原来，在这篇伟大的文章 <a class="oy oz ep" href="https://medium.com/u/758d96208dd7?source=post_page-----a4f3d2420384--------------------------------" rel="noopener" target="_blank">中，Sneha Mehrin </a>找到了一个非常简单的方法来解决这个问题。</p><p id="dcdc" class="pw-post-body-paragraph lv lw it lx b ly mr ju ma mb ms jx md li nq mf mg lm nr mi mj lq ns ml mm mn im bi translated">你需要做的一切，就是在你的笔记本上运行下面的代码(<em class="mo">记住使用你在本教程开始时保存的2个键</em>):</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ny nz l"/></div></figure><p id="aaaa" class="pw-post-body-paragraph lv lw it lx b ly mr ju ma mb ms jx md li nq mf mg lm nr mi mj lq ns ml mm mn im bi translated">如果您没有得到任何错误，这意味着您的S3文件系统已经正确挂载，但是，因为您的bucket仍然是空的，所以您不会通过运行以下命令看到任何文件:</p><pre class="kj kk kl km gt oa nw ob oc aw od bi"><span id="b01c" class="kz la it nw b gy oe of l og oh">%fs ls /mnt/s3Data/ # --&gt; where /s3Data is the folder name</span></pre><p id="8db9" class="pw-post-body-paragraph lv lw it lx b ly mr ju ma mb ms jx md li nq mf mg lm nr mi mj lq ns ml mm mn im bi translated">相反，如果您很好奇并且已经运行了<code class="fe nt nu nv nw b">Producer()</code>函数(<em class="mo">希望少于100次迭代:D </em>)，您将会看到这种类型的文件结构:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pa"><img src="../Images/ee036fdac23c464b2d7809e1793d9725.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4Qst2E_D8z1orVLR-RVVvQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">一旦生成器运行，您将看到装载的bucket的文件结构。</p></figure><p id="81a7" class="pw-post-body-paragraph lv lw it lx b ly mr ju ma mb ms jx md li nq mf mg lm nr mi mj lq ns ml mm mn im bi translated">最后，用PySpark写一些代码就都设置好了！</p><h2 id="4ce3" class="kz la it bd lb lc ld dn le lf lg dp lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">创建Spark会话并定义模式</h2><p id="dda2" class="pw-post-body-paragraph lv lw it lx b ly lz ju ma mb mc jx md li me mf mg lm mh mi mj lq mk ml mm mn im bi translated">首先，我定义了一个<code class="fe nt nu nv nw b">SparkSession</code> ( <em class="mo">在Databricks CE中不是必需的，但是很好的实践</em>)，然后我手动声明了在使用<code class="fe nt nu nv nw b">readStream</code>读取文件时必须提供的数据流模式(<code class="fe nt nu nv nw b">JSONschema</code>):</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ny nz l"/></div></figure><p id="3579" class="pw-post-body-paragraph lv lw it lx b ly mr ju ma mb ms jx md li nq mf mg lm nr mi mj lq ns ml mm mn im bi translated">请注意，<code class="fe nt nu nv nw b">Producer()</code>函数已经将字典转换为JSON文件，因此在PySpark中，金额类型现在被读取为<code class="fe nt nu nv nw b">LongType()</code>整数。</p><h2 id="fc10" class="kz la it bd lb lc ld dn le lf lg dp lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">消耗数据流并在内存中输出</h2><p id="250f" class="pw-post-body-paragraph lv lw it lx b ly lz ju ma mb mc jx md li me mf mg lm mh mi mj lq mk ml mm mn im bi translated">下面我要做的是读取挂载文件夹中以<code class="fe nt nu nv nw b">transactions</code>根目录开始的所有文件，然后将它们保存到<code class="fe nt nu nv nw b">ds</code> DataFrame:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ny nz l"/></div></figure><p id="b1f1" class="pw-post-body-paragraph lv lw it lx b ly mr ju ma mb ms jx md li nq mf mg lm nr mi mj lq ns ml mm mn im bi translated">如果<code class="fe nt nu nv nw b">ds.isStreaming</code>为<code class="fe nt nu nv nw b">True</code>,则表示流处理处于活动状态。</p><p id="ffcd" class="pw-post-body-paragraph lv lw it lx b ly mr ju ma mb ms jx md li nq mf mg lm nr mi mj lq ns ml mm mn im bi translated">然后，我希望通过<code class="fe nt nu nv nw b">currency</code>计算总数<code class="fe nt nu nv nw b">amount</code>，因此我只从<code class="fe nt nu nv nw b">ds</code>中选择这两个字段，并执行简单的聚合(将其保存到<code class="fe nt nu nv nw b">trxs</code></p><p id="e4b5" class="pw-post-body-paragraph lv lw it lx b ly mr ju ma mb ms jx md li nq mf mg lm nr mi mj lq ns ml mm mn im bi translated">最终，我只需要开始将<code class="fe nt nu nv nw b">trxs</code>写成一个流，并保存在<code class="fe nt nu nv nw b">memory</code>(对于本教程的范围来说<em class="mo">已经足够了</em>)。因为我正在执行聚合，所以我必须使用<code class="fe nt nu nv nw b">complete</code>作为<code class="fe nt nu nv nw b">outputMode</code>。注意，写入存储器的流被命名为<code class="fe nt nu nv nw b">transactions_stream_output:</code></p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ny nz l"/></div></figure><p id="7dcf" class="pw-post-body-paragraph lv lw it lx b ly mr ju ma mb ms jx md li nq mf mg lm nr mi mj lq ns ml mm mn im bi translated">现在，我要做的第一个操作是运行PySpark代码，开始读写一个流。这是因为它可能需要30秒，所以在此期间你可以跳到别的东西上。</p><p id="9e6b" class="pw-post-body-paragraph lv lw it lx b ly mr ju ma mb ms jx md li nq mf mg lm nr mi mj lq ns ml mm mn im bi translated">我要执行的第二个操作是运行第一个生成数据的Python脚本。当您这样做时，您会看到屏幕上出现编号的迭代:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pb"><img src="../Images/ac6f1c85285595109ea5ec21faf0ea5b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XalLp0GsOu4KVQ3w2yjbDQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">您将看到每3秒钟出现一次迭代(PyCharm)</p></figure><p id="b32d" class="pw-post-body-paragraph lv lw it lx b ly mr ju ma mb ms jx md li nq mf mg lm nr mi mj lq ns ml mm mn im bi translated">此外，如果您检查您的S3用户界面，您会看到文件被写入桶:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pc"><img src="../Images/e96e3b30cbbc04139a17bdf6609fa9e9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LJ3zMzxkSacgWHJBH9BSYw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">用事务文件填充的S3时段。</p></figure><p id="e023" class="pw-post-body-paragraph lv lw it lx b ly mr ju ma mb ms jx md li nq mf mg lm nr mi mj lq ns ml mm mn im bi translated">现在回到数据砖笔记本。希望您应该看到流处于活动状态，并且随着新文件写入S3并被流作业读取，图形会不断变化:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pd"><img src="../Images/09afe2727c3ade506bdbbb6246458969.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pZW41bK7LxwQ_4kpDPHDvA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">Databricks的数据流仪表板。</p></figure><p id="29f9" class="pw-post-body-paragraph lv lw it lx b ly mr ju ma mb ms jx md li nq mf mg lm nr mi mj lq ns ml mm mn im bi translated">PySpark的一个很酷的特性是，您可以使用SQL轻松查询由流作业创建的内存表。如您所见，每次在S3写入新文件并由流式作业处理时，表中保存的聚合都会更新:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pe"><img src="../Images/380629ef3d93ace1cefddc78eac5e2b0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Zsi5bw5__IQF5L5xu8ZYrg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">查询1:在执行了10次迭代之后。</p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pf"><img src="../Images/1cca4fe26ae3c7fe38cd5064553df5f8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pUN-wR5p3wwEuSPCEQzClA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">查询2:在执行了20次迭代之后。total_amount持续增长！</p></figure><h1 id="1a94" class="nf la it bd lb ng nh ni le nj nk nl lh jz nm ka ll kc nn kd lp kf no kg lt np bi translated">结论</h1><p id="d006" class="pw-post-body-paragraph lv lw it lx b ly lz ju ma mb mc jx md li me mf mg lm mh mi mj lq mk ml mm mn im bi translated">恭喜你！如果您已经做到了这一点，那么您已经成功地在PySpark中构建了一个工作数据流作业，该作业从一个S3桶中读取JSON文件(<em class="mo">，每个文件包含一个模拟银行事务</em>，并将它们写入内存表中。</p><p id="a096" class="pw-post-body-paragraph lv lw it lx b ly mr ju ma mb ms jx md li nq mf mg lm nr mi mj lq ns ml mm mn im bi translated">既然您已经知道如何构建一个动态数据类(使用<code class="fe nt nu nv nw b">field()</code>和<code class="fe nt nu nv nw b">defaulty_factory</code>参数),那么您可以扩展这个概念并创建自己的流数据集。极限只是你的创意！</p><p id="3cb5" class="pw-post-body-paragraph lv lw it lx b ly mr ju ma mb ms jx md li nq mf mg lm nr mi mj lq ns ml mm mn im bi translated">如果你觉得这个教程有用，请留下一些掌声，或者如果你想阅读更多类似主题的教程，请跟我来:D</p></div></div>    
</body>
</html>