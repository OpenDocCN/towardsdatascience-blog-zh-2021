<html>
<head>
<title>An Example of a Reinforcement Learning Exam: The Rationale behind the Questions (Part 2)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">强化学习考试的一个例子:问题背后的基本原理(第二部分)</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/an-example-of-reinforcement-learning-exam-rationale-behind-the-questions-part-2-e66509ba50ee?source=collection_archive---------37-----------------------#2021-08-24">https://towardsdatascience.com/an-example-of-reinforcement-learning-exam-rationale-behind-the-questions-part-2-e66509ba50ee?source=collection_archive---------37-----------------------#2021-08-24</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="52bc" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">在考试中你应该期待什么？我当老师的经历</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/420d4fb7cc93c3dbbc2211d8daebedba.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*e-yP3U2DLVWMDWDB"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><a class="ae kv" href="https://unsplash.com/@sincerelymedia?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">真诚媒体</a>在<a class="ae kv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><ul class=""><li id="acb2" class="kw kx iq ky b kz la lb lc ld le lf lg lh li lj lk ll lm ln bi translated"><strong class="ky ir">学生在强化学习考试中最常犯的错误是什么？</strong></li><li id="33b3" class="kw kx iq ky b kz lo lb lp ld lq lf lr lh ls lj lk ll lm ln bi translated"><strong class="ky ir">有哪些可能的问题？</strong></li></ul><blockquote class="lt"><p id="f164" class="lu lv iq bd lw lx ly lz ma mb mc lj dk translated">在这一系列文章中，我将回答这些问题，并提供如何应对测试的见解。</p></blockquote><p id="e82b" class="pw-post-body-paragraph md me iq ky b kz mf jr mg lb mh ju mi ld mj mk ml lf mm mn mo lh mp mq mr lj ij bi translated">由于我的教学经验，我遇到了学生可能遇到的不同问题，我将主要集中在我自己的经验。</p><p id="eab1" class="pw-post-body-paragraph md me iq ky b kz la jr mg lb lc ju mi ld ms mk ml lf mt mn mo lh mu mq mr lj ij bi translated">对于那些不再是学生的人来说，这篇文章可能仍然有助于获得新知识。</p><p id="9225" class="pw-post-body-paragraph md me iq ky b kz la jr mg lb lc ju mi ld ms mk ml lf mt mn mo lh mu mq mr lj ij bi translated"><em class="mv">查看前面的文章</em> <a class="ae kv" href="https://russoalessio.medium.com/list/rl-exams-3bcc5a4ba5cb" rel="noopener"> <em class="mv">这里的</em> </a> <em class="mv">。</em></p></div><div class="ab cl mw mx hu my" role="separator"><span class="mz bw bk na nb nc"/><span class="mz bw bk na nb nc"/><span class="mz bw bk na nb"/></div><div class="ij ik il im in"><p id="3485" class="pw-post-body-paragraph md me iq ky b kz la jr mg lb lc ju mi ld ms mk ml lf mt mn mo lh mu mq mr lj ij bi translated"><strong class="ky ir">在这里，我将涵盖我在上一篇文章中遗漏的几个问题(你可以在这里找到</strong><a class="ae kv" rel="noopener" target="_blank" href="/an-example-of-reinforcement-learning-exam-rationale-behind-the-questions-part-1-682d1358b571"><strong class="ky ir"/></a><strong class="ky ir">)。</strong></p><p id="f044" class="pw-post-body-paragraph md me iq ky b kz la jr mg lb lc ju mi ld ms mk ml lf mt mn mo lh mu mq mr lj ij bi translated">下面的问题是整个练习中最难的<strong class="ky ir">、</strong>，但是正如我们所看到的，它们可以用几行来回答。</p><p id="a501" class="pw-post-body-paragraph md me iq ky b kz la jr mg lb lc ju mi ld ms mk ml lf mt mn mo lh mu mq mr lj ij bi translated"><strong class="ky ir">问题测试学生的概率和强化学习知识，以及解决问题的能力。</strong></p><p id="8150" class="pw-post-body-paragraph md me iq ky b kz la jr mg lb lc ju mi ld ms mk ml lf mt mn mo lh mu mq mr lj ij bi translated">这些问题如下:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nd"><img src="../Images/c5e1749b180992f4a5e603faae2f74fd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gH_-AhKzsI2Hk_sFvfNH9w.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="7c25" class="pw-post-body-paragraph md me iq ky b kz la jr mg lb lc ju mi ld ms mk ml lf mt mn mo lh mu mq mr lj ij bi translated">首先，做一些澄清:</p><ul class=""><li id="a7dd" class="kw kx iq ky b kz la lb lc ld le lf lg lh li lj lk ll lm ln bi translated"><strong class="ky ir"> π是这里的策略，</strong>其中π( <em class="mv"> a|s </em>)表示给定状态<em class="mv"> s. </em>时选择动作<em class="mv"> a </em>的概率</li><li id="af31" class="kw kx iq ky b kz lo lb lp ld lq lf lr lh ls lj lk ll lm ln bi translated"><strong class="ky ir">目标<em class="mv"> y </em>是TD-target </strong>，如果我们使用on-policy或off-policy算法，它会发生变化(如果这不清楚，请查看我以前的文章<a class="ae kv" rel="noopener" target="_blank" href="/an-example-of-reinforcement-learning-exam-rationale-behind-the-questions-part-1-682d1358b571">https://towards data science . com/an-example-of-enforcement-learning-exam-rational-behind-The-questions-part-1-682d 1358 b571</a>)。</li><li id="b9ce" class="kw kx iq ky b kz lo lb lp ld lq lf lr lh ls lj lk ll lm ln bi translated"><strong class="ky ir">SARSA</strong>(又名<em class="mv">预期SARSA </em> ) <strong class="ky ir">的变体是一种算法，其中TD-target根据以下公式</strong>进行计算</li></ul><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ne"><img src="../Images/e7c3ab6bf1e2701349750cb3ad6a033b.png" data-original-src="https://miro.medium.com/v2/resize:fit:940/format:webp/1*98H0eUcxeIVxIkp0fiLfEA.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">TD-SARSA变体的目标(也称为预期SARSA)；作者图片</p></figure><ul class=""><li id="a293" class="kw kx iq ky b kz la lb lc ld le lf lg lh li lj lk ll lm ln bi translated">偏见只是衡量一项政策好坏的标准。如果策略是最优的，我们期望偏差项为0。</li></ul></div><div class="ab cl mw mx hu my" role="separator"><span class="mz bw bk na nb nc"/><span class="mz bw bk na nb nc"/><span class="mz bw bk na nb"/></div><div class="ij ik il im in"><h2 id="a41e" class="nf ng iq bd nh ni nj dn nk nl nm dp nn ld no np nq lf nr ns nt lh nu nv nw nx bi translated">问题(a)的解决方案</h2><p id="8fe9" class="pw-post-body-paragraph md me iq ky b kz ny jr mg lb nz ju mi ld oa mk ml lf ob mn mo lh oc mq mr lj ij bi translated"><strong class="ky ir">问题(a)是关于熟悉概率中的高塔法则属性，并知道TD-target是什么。</strong></p><p id="1969" class="pw-post-body-paragraph md me iq ky b kz la jr mg lb lc ju mi ld ms mk ml lf mt mn mo lh mu mq mr lj ij bi translated">这是一个经典的解决问题的练习，学生需要一次走一步。<strong class="ky ir">简单地写下你希望证明/解决的事情通常是个好主意。</strong></p><p id="72a3" class="pw-post-body-paragraph md me iq ky b kz la jr mg lb lc ju mi ld ms mk ml lf mt mn mo lh mu mq mr lj ij bi translated"><strong class="ky ir">首先，为了不混淆，让我们用SARSA <em class="mv">、</em>用<em class="mv"> y </em>表示目标值，用SARSA </strong>的变体用<em class="mv">y’</em>表示目标值。因为<strong class="ky ir"> <em class="mv"> Q(s，a) </em>在每个偏置项</strong>中是相同的值，所以该练习归结为表明以下等式成立</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi od"><img src="../Images/76406ab13273fd3840056c2a5665ac5d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1350/format:webp/1*Vx90Chm2QqSXRad3VmRnXQ.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">我们需要证明什么！(图片由作者提供)</p></figure><p id="8798" class="pw-post-body-paragraph md me iq ky b kz la jr mg lb lc ju mi ld ms mk ml lf mt mn mo lh mu mq mr lj ij bi translated"><strong class="ky ir">接下来:什么是<em class="mv"> y </em>和<em class="mv">y’</em>？这些只是TD目标。我们已经知道如何写它们了。</strong></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oe"><img src="../Images/a8b457d2e0a937367bc2048eb4b49fdc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uLEkpo0jPENWDYCEmz25HQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="b418" class="pw-post-body-paragraph md me iq ky b kz la jr mg lb lc ju mi ld ms mk ml lf mt mn mo lh mu mq mr lj ij bi translated">因此，原问题简化为证明以下等式</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="of og l"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">这是我们需要证明是真实的平等(图片由作者提供)</p></figure><p id="e7eb" class="pw-post-body-paragraph md me iq ky b kz la jr mg lb lc ju mi ld ms mk ml lf mt mn mo lh mu mq mr lj ij bi translated">这确实是练习的第一个难点。我们怎样才能从这里搬走一个呢？不要惊慌。</p><ol class=""><li id="bdb2" class="kw kx iq ky b kz la lb lc ld le lf lg lh li lj oh ll lm ln bi translated"><strong class="ky ir">注意，在左手边我们有两个随机变量，t+1时刻的状态-动作对，而在右手边只有一个随机变量</strong>(右手边只有一个随机变量，t+1时刻的状态)。</li></ol><p id="cf90" class="pw-post-body-paragraph md me iq ky b kz la jr mg lb lc ju mi ld ms mk ml lf mt mn mo lh mu mq mr lj ij bi translated">这是一个人需要更新他/她的概率工具包的地方。<strong class="ky ir">这个想法是通过使用条件期望的塔属性来洗去左侧额外随机变量的影响。</strong></p><p id="fbc9" class="pw-post-body-paragraph md me iq ky b kz la jr mg lb lc ju mi ld ms mk ml lf mt mn mo lh mu mq mr lj ij bi translated">只需执行以下操作(从左至右阅读):</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oi"><img src="../Images/95a184583632653802d2422b95101fc1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zSG6ZdjkIe6AanBrmQhQLA.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">从左至右:应用(1)塔规；(2)把内心的期望写成和；作者图片</p></figure><ul class=""><li id="cd88" class="kw kx iq ky b kz la lb lc ld le lf lg lh li lj lk ll lm ln bi translated"><strong class="ky ir">解释:考虑左手边的表达式，对下一个动作取一个平均值，假装你知道下一个状态是什么</strong>(我们得到中间的表达式)<strong class="ky ir">。<br/>然后，写出平均值是多少</strong>(右边最后一步)<strong class="ky ir">。</strong></li><li id="7b53" class="kw kx iq ky b kz lo lb lp ld lq lf lr lh ls lj lk ll lm ln bi translated">这表明Sarsa的偏差相当于SARSA变体的偏差(也称为预期SARSA)。</li><li id="edf1" class="kw kx iq ky b kz lo lb lp ld lq lf lr lh ls lj lk ll lm ln bi translated"><strong class="ky ir">塔性质为我们提供了一次处理一个随机变量的能力，在处理多个随机变量的期望时，我们的生活变得更加轻松。</strong></li></ul></div><div class="ab cl mw mx hu my" role="separator"><span class="mz bw bk na nb nc"/><span class="mz bw bk na nb nc"/><span class="mz bw bk na nb"/></div><div class="ij ik il im in"><h2 id="d58a" class="nf ng iq bd nh ni nj dn nk nl nm dp nn ld no np nq lf nr ns nt lh nu nv nw nx bi translated">问题(b)的解决方案</h2><p id="abf2" class="pw-post-body-paragraph md me iq ky b kz ny jr mg lb nz ju mi ld oa mk ml lf ob mn mo lh oc mq mr lj ij bi translated"><strong class="ky ir">在第二个问题(b)中，我们被要求回答两种算法是否收敛到相同的Q值</strong>(你可以再次检查下面的问题)。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nd"><img src="../Images/c5e1749b180992f4a5e603faae2f74fd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gH_-AhKzsI2Hk_sFvfNH9w.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="1aca" class="pw-post-body-paragraph md me iq ky b kz la jr mg lb lc ju mi ld ms mk ml lf mt mn mo lh mu mq mr lj ij bi translated">这是一种开放式问题，学生被迫思考这个问题。学生需要了解哪些假设会导致肯定(或否定)的答案。</p><p id="2c49" class="pw-post-body-paragraph md me iq ky b kz la jr mg lb lc ju mi ld ms mk ml lf mt mn mo lh mu mq mr lj ij bi translated"><strong class="ky ir">需要注意的一件重要事情是，政策π是固定的，不会随着时间而改变。</strong> <em class="mv">这意味着我们并不是真的对学习更好的政策感兴趣，而只是学习π的值。</em></p><p id="6a39" class="pw-post-body-paragraph md me iq ky b kz la jr mg lb lc ju mi ld ms mk ml lf mt mn mo lh mu mq mr lj ij bi translated">我们该如何开始？</p><ol class=""><li id="b20f" class="kw kx iq ky b kz la lb lc ld le lf lg lh li lj oh ll lm ln bi translated">首先，我们需要假设罗宾斯-门罗条件得到满足。</li><li id="cac2" class="kw kx iq ky b kz lo lb lp ld lq lf lr lh ls lj oh ll lm ln bi translated"><strong class="ky ir">其次，我们知道SARSA的变体使用一种行为策略，即ϵ-greedy策略。</strong></li></ol><p id="6fc7" class="pw-post-body-paragraph md me iq ky b kz la jr mg lb lc ju mi ld ms mk ml lf mt mn mo lh mu mq mr lj ij bi translated"><strong class="ky ir">这两件事暗示着SARSA的变体会收敛到策略π的Q值(不是最优Q值)。</strong></p><ul class=""><li id="d92a" class="kw kx iq ky b kz la lb lc ld le lf lg lh li lj lk ll lm ln bi translated">由于行为策略将无限频繁地探索每个状态-动作对，那么算法将收敛到我们用来计算TD-target <em class="mv"> y，</em>的策略的Q值，即π的Q值。</li></ul><p id="4ef3" class="pw-post-body-paragraph md me iq ky b kz la jr mg lb lc ju mi ld ms mk ml lf mt mn mo lh mu mq mr lj ij bi translated">【SARSA呢？</p><ul class=""><li id="8f21" class="kw kx iq ky b kz la lb lc ld le lf lg lh li lj lk ll lm ln bi translated">SARSA将学习被无限频繁访问的状态-动作对的值。</li><li id="2e18" class="kw kx iq ky b kz lo lb lp ld lq lf lr lh ls lj lk ll lm ln bi translated">然而，可能有些国家行动对从未被访问过。例如，考虑这样的情况，其中到达特定状态<em class="mv"> z </em>的唯一方式是在状态<em class="mv"> s </em>中执行动作<em class="mv"> a </em>。如果那个概率是0 ( <em class="mv"> π(a|s)=0 </em>，我们将永远无法学习到<em class="mv"> z </em>中的Q值。</li></ul><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oj"><img src="../Images/e08565bbc402b5eb5e1ade9e26166fa0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9yIKGmo_1EqUOwJYxydUFQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">如果在状态“s”中，我们从不执行动作“a ”,就不可能知道状态“z”中动作的值(图片由作者提供)</p></figure><ul class=""><li id="ec7f" class="kw kx iq ky b kz la lb lc ld le lf lg lh li lj lk ll lm ln bi translated">另一方面，SARSA的变体将学习<em class="mv"> z </em>中的q值，因为在这种情况下，行为策略是ϵ-greedy(这保证了状态<em class="mv"> z </em>将在某个点被访问)</li><li id="5c4a" class="kw kx iq ky b kz lo lb lp ld lq lf lr lh ls lj lk ll lm ln bi translated">所以，唯一的办法就是保证π有足够的探索性。这就需要满足以下条件:我们需要一个正概率来挑选每个状态下所有可能的动作，即对于所有的状态-动作对，<em class="mv"> π(a|s) &gt; 0 </em>。</li><li id="396e" class="kw kx iq ky b kz lo lb lp ld lq lf lr lh ls lj lk ll lm ln bi translated">如果满足后一个条件，那么通过SARSA学习的Q值收敛到策略π的Q值。</li></ul></div><div class="ab cl mw mx hu my" role="separator"><span class="mz bw bk na nb nc"/><span class="mz bw bk na nb nc"/><span class="mz bw bk na nb"/></div><div class="ij ik il im in"><h1 id="4143" class="ok ng iq bd nh ol om on nk oo op oq nn jw or jx nq jz os ka nt kc ot kd nw ou bi translated">结论和后续文章</h1><p id="68b7" class="pw-post-body-paragraph md me iq ky b kz ny jr mg lb nz ju mi ld oa mk ml lf ob mn mo lh oc mq mr lj ij bi translated"><strong class="ky ir">这是</strong> <a class="ae kv" href="https://russoalessio.medium.com/list/rl-exams-3bcc5a4ba5cb" rel="noopener"> <strong class="ky ir">系列</strong> </a> <strong class="ky ir">的第二篇文章，我在这里描述了一些你可以在强化学习测试中找到的最常见的问题。</strong></p><p id="68b8" class="pw-post-body-paragraph md me iq ky b kz la jr mg lb lc ju mi ld ms mk ml lf mt mn mo lh mu mq mr lj ij bi translated">在接下来的文章中，我将展示更多的练习，并更详细地讨论其他强化学习问题。</p><p id="5edd" class="pw-post-body-paragraph md me iq ky b kz la jr mg lb lc ju mi ld ms mk ml lf mt mn mo lh mu mq mr lj ij bi translated">我将主要讨论强化学习的方法和理论方面，重点是老师对学生的期望。</p><p id="2922" class="pw-post-body-paragraph md me iq ky b kz la jr mg lb lc ju mi ld ms mk ml lf mt mn mo lh mu mq mr lj ij bi translated"><em class="mv">希望这篇文章对你的解题方式有所启发，或许对你即将到来的考试或者以后的学习有所帮助！</em></p><p id="017e" class="pw-post-body-paragraph md me iq ky b kz la jr mg lb lc ju mi ld ms mk ml lf mt mn mo lh mu mq mr lj ij bi translated"><strong class="ky ir">感谢您的阅读！</strong></p></div></div>    
</body>
</html>