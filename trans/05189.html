<html>
<head>
<title>(Gaussian) Naive Bayes</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">(高斯)朴素贝叶斯</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/gaussian-naive-bayes-4d2895d139a?source=collection_archive---------9-----------------------#2021-05-07">https://towardsdatascience.com/gaussian-naive-bayes-4d2895d139a?source=collection_archive---------9-----------------------#2021-05-07</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="ae4d" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">(高斯)朴素贝叶斯模型介绍，包括理论和Python实现</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/7db7757593916d70c73e2a4da065e7c3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7mAIDKvbhI0s0HDuxaqJag.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">(高斯)朴素贝叶斯模型的决策边界图解。图片作者。</p></figure><h1 id="3897" class="ky kz it bd la lb lc ld le lf lg lh li jz lj ka lk kc ll kd lm kf ln kg lo lp bi translated">内容</h1><p id="077f" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">这篇文章是我将要发表的一系列文章的一部分。你可以通过点击<a class="ae mm" href="https://cookieblues.github.io/guides/2021/04/01/bsmalea-notes-3b/" rel="noopener ugc nofollow" target="_blank">这里</a>在我的个人博客上阅读这篇文章的更详细版本。下面你可以看到该系列的概述。</p><h2 id="2db9" class="mn kz it bd la mo mp dn le mq mr dp li lz ms mt lk md mu mv lm mh mw mx lo my bi translated">1.机器学习导论</h2><ul class=""><li id="d782" class="mz na it ls b lt lu lw lx lz nb md nc mh nd ml ne nf ng nh bi translated"><a class="ae mm" rel="noopener" target="_blank" href="/what-is-machine-learning-91040db474f9">(一)什么是机器学习？</a></li><li id="2eab" class="mz na it ls b lt ni lw nj lz nk md nl mh nm ml ne nf ng nh bi translated"><a class="ae mm" rel="noopener" target="_blank" href="/model-selection-in-machine-learning-813fe2e63ec6"> (b)机器学习中的模型选择</a></li><li id="c2ea" class="mz na it ls b lt ni lw nj lz nk md nl mh nm ml ne nf ng nh bi translated"><a class="ae mm" rel="noopener" target="_blank" href="/the-curse-of-dimensionality-5673118fe6d2"> (c)维度的诅咒</a></li><li id="928d" class="mz na it ls b lt ni lw nj lz nk md nl mh nm ml ne nf ng nh bi translated"><a class="ae mm" rel="noopener" target="_blank" href="/what-is-bayesian-inference-4eda9f9e20a6"> (d)什么是贝叶斯推理？</a></li></ul><h2 id="4ca2" class="mn kz it bd la mo mp dn le mq mr dp li lz ms mt lk md mu mv lm mh mw mx lo my bi translated">2.回归</h2><ul class=""><li id="ba02" class="mz na it ls b lt lu lw lx lz nb md nc mh nd ml ne nf ng nh bi translated"><a class="ae mm" rel="noopener" target="_blank" href="/how-linear-regression-actually-works-theory-and-implementation-8d8dcae3222c"> (a)线性回归的实际工作原理</a></li><li id="2ff7" class="mz na it ls b lt ni lw nj lz nk md nl mh nm ml ne nf ng nh bi translated"><a class="ae mm" rel="noopener" target="_blank" href="/how-to-improve-your-linear-regression-with-basis-functions-and-regularization-8a6fcebdc11c"> (b)如何使用基函数和正则化改进您的线性回归</a></li></ul><h2 id="521d" class="mn kz it bd la mo mp dn le mq mr dp li lz ms mt lk md mu mv lm mh mw mx lo my bi translated">3.分类</h2><ul class=""><li id="20f3" class="mz na it ls b lt lu lw lx lz nb md nc mh nd ml ne nf ng nh bi translated"><a class="ae mm" rel="noopener" target="_blank" href="/overview-of-classifiers-d0a0d3eecfd1"> (a)分类器概述</a></li><li id="6de8" class="mz na it ls b lt ni lw nj lz nk md nl mh nm ml ne nf ng nh bi translated"><a class="ae mm" rel="noopener" target="_blank" href="/quadratic-discriminant-analysis-ae55d8a8148a"> (b)二次判别分析(QDA) </a></li><li id="166f" class="mz na it ls b lt ni lw nj lz nk md nl mh nm ml ne nf ng nh bi translated"><a class="ae mm" rel="noopener" target="_blank" href="/linear-discriminant-analysis-1894bbf04359"> (c)线性判别分析</a></li><li id="4a53" class="mz na it ls b lt ni lw nj lz nk md nl mh nm ml ne nf ng nh bi translated"><strong class="ls iu"> (d)(高斯)朴素贝叶斯</strong></li></ul><h1 id="9376" class="ky kz it bd la lb lc ld le lf lg lh li jz lj ka lk kc ll kd lm kf ln kg lo lp bi translated">设置和目标</h1><p id="1573" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">我们已经了解了二次判别分析(QDA)和线性判别分析(LDA ),前者假设特定于类的协方差矩阵，后者假设类之间共享协方差矩阵，现在我们将了解(高斯)朴素贝叶斯，它也略有不同。</p><p id="f34b" class="pw-post-body-paragraph lq lr it ls b lt nn ju lv lw no jx ly lz np mb mc md nq mf mg mh nr mj mk ml im bi translated">如果你没有看过我在QDA上的帖子，我强烈推荐它，因为Naives Bayes的推导是相同的。</p><p id="3608" class="pw-post-body-paragraph lq lr it ls b lt nn ju lv lw no jx ly lz np mb mc md nq mf mg mh nr mj mk ml im bi translated">朴素贝叶斯假设特征是独立的。这意味着<strong class="ls iu">我们仍然假设特定类别的协方差矩阵(如在QDA)，但是协方差矩阵是对角矩阵</strong>。这是因为假设特征是独立的。</p><p id="6b8b" class="pw-post-body-paragraph lq lr it ls b lt nn ju lv lw no jx ly lz np mb mc md nq mf mg mh nr mj mk ml im bi translated">因此，给定一个具有相应目标变量<em class="ns"> t </em>的<em class="ns"> N </em>输入变量<strong class="ls iu"> x </strong>的训练数据集，(高斯)朴素贝叶斯假设<strong class="ls iu">类条件密度</strong>是正态分布的</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/152fe7860d9297f6232823ee207109e0.png" data-original-src="https://miro.medium.com/v2/resize:fit:626/1*nMKk-gPtpkXk4bcWfBUsoA.gif"/></div></figure><p id="bba3" class="pw-post-body-paragraph lq lr it ls b lt nn ju lv lw no jx ly lz np mb mc md nq mf mg mh nr mj mk ml im bi translated">其中<strong class="ls iu"> <em class="ns"> μ </em> </strong>是<strong class="ls iu">类特定均值向量</strong>，而<strong class="ls iu">σ</strong>是<strong class="ls iu">类特定协方差矩阵</strong>。利用贝叶斯定理，我们现在可以计算后验概率</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/35b4ad99045d0364c5e9b68f6f2a7e5e.png" data-original-src="https://miro.medium.com/v2/resize:fit:962/1*ZgVdGh3MqS84ZXH_4ZgP_g.gif"/></div></figure><p id="aedd" class="pw-post-body-paragraph lq lr it ls b lt nn ju lv lw no jx ly lz np mb mc md nq mf mg mh nr mj mk ml im bi translated">然后我们将把x分类到类中</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nv"><img src="../Images/e72ab5f9f343eabff439e2416f672e13.png" data-original-src="https://miro.medium.com/v2/resize:fit:578/1*gZ64liMGTvuktr5t9ziYgg.gif"/></div></figure><h1 id="6b51" class="ky kz it bd la lb lc ld le lf lg lh li jz lj ka lk kc ll kd lm kf ln kg lo lp bi translated">衍生和培训</h1><p id="e289" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">这种推导实际上与从QDA推导特定类别的先验矩阵、均值矩阵和协方差矩阵是一样的。你可以在我之前关于QDA的文章<a class="ae mm" rel="noopener" target="_blank" href="/quadratic-discriminant-analysis-ae55d8a8148a">这里</a>找到出处。</p><p id="6b73" class="pw-post-body-paragraph lq lr it ls b lt nn ju lv lw no jx ly lz np mb mc md nq mf mg mh nr mj mk ml im bi translated">唯一的区别是，我们必须将特定于类的协方差矩阵中除对角线以外的所有内容都设置为0。因此，我们得到以下结果</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nw"><img src="../Images/ba570564816faf430cabc3a0105925e9.png" data-original-src="https://miro.medium.com/v2/resize:fit:758/1*2wGyjV_KcCRqqkdPHcukdQ.gif"/></div></figure><p id="1bea" class="pw-post-body-paragraph lq lr it ls b lt nn ju lv lw no jx ly lz np mb mc md nq mf mg mh nr mj mk ml im bi translated">其中diag表示我们将不在对角线上的每个值都设置为0。</p><h1 id="37ed" class="ky kz it bd la lb lc ld le lf lg lh li jz lj ka lk kc ll kd lm kf ln kg lo lp bi translated">Python实现</h1><p id="264a" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">下面的代码是我们刚刚讨论过的(高斯)朴素贝叶斯的简单实现。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nx ny l"/></div></figure><p id="4207" class="pw-post-body-paragraph lq lr it ls b lt nn ju lv lw no jx ly lz np mb mc md nq mf mg mh nr mj mk ml im bi translated">下面是一个图表，其中包含数据点(颜色编码以匹配其各自的类)、我们的(高斯)朴素贝叶斯模型找到的类分布，以及由各自的类分布生成的决策边界。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nz"><img src="../Images/7557f1425a8d6578e5ec78413d3f4631.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ovqA6Hh4VSAlLZBD_sRPHw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">数据点的图表，其各自的类别用颜色编码，通过我们的(高斯)朴素贝叶斯模型找到的类别分布，以及从类别分布得到的决策边界。图片作者。</p></figure><p id="983a" class="pw-post-body-paragraph lq lr it ls b lt nn ju lv lw no jx ly lz np mb mc md nq mf mg mh nr mj mk ml im bi translated">注意，虽然在LDA的情况下判定边界不是线性的，但是类别分布是完全圆形高斯分布，因为协方差矩阵是对角矩阵。</p><h1 id="6b68" class="ky kz it bd la lb lc ld le lf lg lh li jz lj ka lk kc ll kd lm kf ln kg lo lp bi translated">摘要</h1><ul class=""><li id="3581" class="mz na it ls b lt lu lw lx lz nb md nc mh nd ml ne nf ng nh bi translated">朴素贝叶斯是一个<strong class="ls iu">生成</strong>模型。</li><li id="e58f" class="mz na it ls b lt ni lw nj lz nk md nl mh nm ml ne nf ng nh bi translated">(高斯)朴素贝叶斯假设<strong class="ls iu">每个类遵循高斯分布</strong>。</li><li id="ec43" class="mz na it ls b lt ni lw nj lz nk md nl mh nm ml ne nf ng nh bi translated">QDA和(高斯)朴素贝叶斯的区别在于<strong class="ls iu">朴素贝叶斯假设特征</strong>的独立性，这意味着<strong class="ls iu">协方差矩阵是对角矩阵</strong>。</li><li id="be4e" class="mz na it ls b lt ni lw nj lz nk md nl mh nm ml ne nf ng nh bi translated">记住，LDA有一个共享的协方差矩阵，其中<strong class="ls iu">朴素贝叶斯有特定于类的协方差矩阵</strong>。</li></ul></div></div>    
</body>
</html>