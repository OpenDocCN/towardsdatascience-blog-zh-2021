<html>
<head>
<title>Reinforcement Learning for anyone: Open AI Gym and Ray</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">面向任何人的强化学习:开放AI健身房和Ray</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/reinforcement-learning-for-anyone-open-ai-gym-and-ray-ee365a7d1ecc?source=collection_archive---------17-----------------------#2021-04-08">https://towardsdatascience.com/reinforcement-learning-for-anyone-open-ai-gym-and-ray-ee365a7d1ecc?source=collection_archive---------17-----------------------#2021-04-08</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="6615" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">使用开放AI Gym和Ray Python库创建和优化强化学习代理的实用介绍</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ki"><img src="../Images/660b6af2f0f6cfe7a14748ab87335081.png" data-original-src="https://miro.medium.com/v2/resize:fit:1216/1*3yyhR7SJHREHqZdRM4DzKA.gif"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">(图片转载自<a class="ae ku" href="https://imgur.com/gallery/OVvOuYZ" rel="noopener ugc nofollow" target="_blank">https://imgur.com/gallery/OVvOuYZ</a>)</p></figure><h1 id="2462" class="kv kw it bd kx ky kz la lb lc ld le lf jz lg ka lh kc li kd lj kf lk kg ll lm bi translated">介绍</h1><p id="9ee6" class="pw-post-body-paragraph ln lo it lp b lq lr ju ls lt lu jx lv lw lx ly lz ma mb mc md me mf mg mh mi im bi translated"><a class="ae ku" href="http://gym.openai.com/docs/" rel="noopener ugc nofollow" target="_blank"> Open AI Gym </a>是典型强化学习(RL)任务的开源接口。因此，使用开放式人工智能健身房可以变得非常容易地开始强化学习，因为我们已经提供了各种不同的环境和代理。我们剩下要做的就是编写不同的算法并测试它们，以便让我们的代理学习如何以最好的方式完成不同的任务(正如我们将在后面看到的，使用<a class="ae ku" href="https://docs.ray.io/en/master/installation.html" rel="noopener ugc nofollow" target="_blank">雷</a>可以很容易地做到这一点)。此外，Gym还兼容其他Python库，如Tensorflow或PyTorch，因此可以轻松创建深度强化学习模型。</p><p id="8230" class="pw-post-body-paragraph ln lo it lp b lq mj ju ls lt mk jx lv lw ml ly lz ma mm mc md me mn mg mh mi im bi translated">开放人工智能健身房提供的不同环境和代理的一些例子有:雅达利游戏、机器人任务、控制系统等…</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mo"><img src="../Images/4eb772e69356324a9fc30ab3327fad01.png" data-original-src="https://miro.medium.com/v2/resize:fit:800/1*RQvnmGUUdMjUkouu5DiM7g.gif"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">图1:雅达利游戏示例[1]</p></figure><p id="84cf" class="pw-post-body-paragraph ln lo it lp b lq mj ju ls lt mk jx lv lw ml ly lz ma mm mc md me mn mg mh mi im bi translated">如果你有兴趣了解更多关于强化学习主要概念背后的理论，更多信息可以在我的前一篇文章中找到。</p><h1 id="7f47" class="kv kw it bd kx ky kz la lb lc ld le lf jz lg ka lh kc li kd lj kf lk kg ll lm bi translated">开放AI健身房</h1><p id="47a2" class="pw-post-body-paragraph ln lo it lp b lq lr ju ls lt lu jx lv lw lx ly lz ma mb mc md me mf mg mh mi im bi translated">在这一节中，我们现在将浏览开放人工智能健身房图书馆的一些基础知识。首先，我们可以通过使用以下命令轻松安装该库:</p><pre class="kj kk kl km gt mp mq mr ms aw mt bi"><span id="7ed0" class="mu kw it mq b gy mv mw l mx my">pip install gym</span></pre><p id="0bb3" class="pw-post-body-paragraph ln lo it lp b lq mj ju ls lt mk jx lv lw ml ly lz ma mm mc md me mn mg mh mi im bi translated">一旦安装了这个库，我们就可以实例化一个环境。对于这个例子，我们将使用<a class="ae ku" href="https://gym.openai.com/envs/LunarLander-v2/" rel="noopener ugc nofollow" target="_blank">月球着陆器</a>。在这个场景中，我们的目标是让我们的代理学会正确地将他们的飞船降落在两面旗帜之间(我们的着陆平台)。代理人着陆越精确，他所能获得的总回报就越大。为了实现这个目标，代理可以在任何时间点选择以下四个动作中的任何一个:启动左发动机，启动右发动机，关闭发动机，什么也不做。</p><p id="af14" class="pw-post-body-paragraph ln lo it lp b lq mj ju ls lt mk jx lv lw ml ly lz ma mm mc md me mn mg mh mi im bi translated">为了快速可视化这个场景，让我们实例化环境，并让我们的代理迭代地采取从环境中采样的随机动作(图2)。</p><pre class="kj kk kl km gt mp mq mr ms aw mt bi"><span id="a3e0" class="mu kw it mq b gy mv mw l mx my">import gym</span><span id="2498" class="mu kw it mq b gy mz mw l mx my">env = gym.make("LunarLander-v2")<br/>env.reset() # Instantiate enviroment with default parameters<br/>for step in range(300):<br/>    env.render() # Show agent actions on screen<br/>    env.step(env.action_space.sample()) # Sample random action<br/>env.close()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi na"><img src="../Images/ea43af09c8e99f237a734c685649e76b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/1*4KdnZZslkw9wTD9MbRncqQ.gif"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">图2:代理执行随机操作</p></figure><p id="d052" class="pw-post-body-paragraph ln lo it lp b lq mj ju ls lt mk jx lv lw ml ly lz ma mm mc md me mn mg mh mi im bi translated">为了提高代理的性能，我们可以利用<strong class="lp iu"> <em class="nb">步骤</em> </strong>函数返回的信息:</p><ul class=""><li id="8d1c" class="nc nd it lp b lq mj lt mk lw ne ma nf me ng mi nh ni nj nk bi translated"><strong class="lp iu"> <em class="nb">观察</em> </strong> =执行动作后的环境信息(如棋盘游戏中的棋盘位置、像素坐标等……)。</li><li id="8488" class="nc nd it lp b lq nl lt nm lw nn ma no me np mi nh ni nj nk bi translated"><strong class="lp iu"> <em class="nb">奖励</em> </strong> =量化表示我们之前的行为有多积极(越积极越好)。</li><li id="9e53" class="nc nd it lp b lq nl lt nm lw nn ma no me np mi nh ni nj nk bi translated"><strong class="lp iu"> <em class="nb"> done </em> </strong> =是一个布尔值，显示我们是否到达了可用的最后一次迭代(例如，我们达到了目标，我们用完了可用的最大步骤)。</li><li id="c149" class="nc nd it lp b lq nl lt nm lw nn ma no me np mi nh ni nj nk bi translated"><strong class="lp iu"> <em class="nb">信息</em> </strong> =调试实用程序。</li></ul><p id="198d" class="pw-post-body-paragraph ln lo it lp b lq mj ju ls lt mk jx lv lw ml ly lz ma mm mc md me mn mg mh mi im bi translated">利用这些信息和学习算法(例如Q学习、SARSA或深度强化学习)，我们将能够快速解决不同环境中的大量问题。</p><h1 id="5dc7" class="kv kw it bd kx ky kz la lb lc ld le lf jz lg ka lh kc li kd lj kf lk kg ll lm bi translated">光线</h1><p id="5d43" class="pw-post-body-paragraph ln lo it lp b lq lr ju ls lt lu jx lv lw lx ly lz ma mb mc md me mf mg mh mi im bi translated">Ray [2]是一个用于多处理的开源Python框架，它还提供了一个强化学习库(RLlib)和一个超参数优化库(Tune ),作为其信息结构的一部分，以便大规模创建强化学习模型。</p><p id="4fa8" class="pw-post-body-paragraph ln lo it lp b lq mj ju ls lt mk jx lv lw ml ly lz ma mm mc md me mn mg mh mi im bi translated">在我们之前的例子中，使用开放式人工智能健身房可以轻松地创建环境和代理，而Ray将帮助我们更容易地测试和优化我们代理的不同学习算法。</p><p id="2e13" class="pw-post-body-paragraph ln lo it lp b lq mj ju ls lt mk jx lv lw ml ly lz ma mm mc md me mn mg mh mi im bi translated">可以使用以下命令为Linux用户轻松安装Ray:</p><pre class="kj kk kl km gt mp mq mr ms aw mt bi"><span id="881b" class="mu kw it mq b gy mv mw l mx my">pip install ray</span></pre><p id="bce6" class="pw-post-body-paragraph ln lo it lp b lq mj ju ls lt mk jx lv lw ml ly lz ma mm mc md me mn mg mh mi im bi translated">对于Windows用户来说，虽然还没有提供完全的支持，但是可能需要使用一个用于Linux的<a class="ae ku" rel="noopener" target="_blank" href="/setting-up-a-data-science-environment-using-windows-subsystem-for-linux-wsl-c4b390803dd"> Windows子系统</a>(或者使用免费工具，比如Google Colab！).</p><h1 id="a20b" class="kv kw it bd kx ky kz la lb lc ld le lf jz lg ka lh kc li kd lj kf lk kg ll lm bi translated">深度Q学习示例</h1><p id="58a4" class="pw-post-body-paragraph ln lo it lp b lq lr ju ls lt lu jx lv lw lx ly lz ma mb mc md me mf mg mh mi im bi translated">现在我们已经成功地引入了Gym和Ray，我们准备着手解决之前描述的<a class="ae ku" href="https://gym.openai.com/envs/LunarLander-v2/" rel="noopener ugc nofollow" target="_blank">月球着陆器</a>问题。本文使用的所有代码都可以在我的Github档案中找到。首先，我们需要导入所有需要的库。</p><pre class="kj kk kl km gt mp mq mr ms aw mt bi"><span id="ebc3" class="mu kw it mq b gy mv mw l mx my">import ray<br/>from ray import tune<br/>from ray.rllib import agents<br/>import pandas as pd<br/>import matplotlib.pyplot as plt<br/>import seaborn as sns<br/>sns.set()</span></pre><p id="ecfd" class="pw-post-body-paragraph ln lo it lp b lq mj ju ls lt mk jx lv lw ml ly lz ma mm mc md me mn mg mh mi im bi translated">接下来，使用Ray我们可以很容易地实例化一个<a class="ae ku" href="https://rllib.readthedocs.io/en/latest/rllib-algorithms.html#dqn" rel="noopener ugc nofollow" target="_blank">深度Q网络</a> (DQN)架构来训练我们的代理。使用深度Q网络是为了通过用能够学习如何正确估计Q值(函数逼近)的神经网络来替换基于表Q学习的算法，从而容易地扩展它们。如果你有兴趣了解更多关于这个话题的信息，我强烈建议你看一看<a class="ae ku" href="https://web.stanford.edu/class/psych209/Readings/SuttonBartoIPRLBook2ndEd.pdf" rel="noopener ugc nofollow" target="_blank">理查德·萨顿和安德鲁·巴尔托</a>的《强化学习:导论》。</p><p id="8220" class="pw-post-body-paragraph ln lo it lp b lq mj ju ls lt mk jx lv lw ml ly lz ma mm mc md me mn mg mh mi im bi translated">在<a class="ae ku" href="https://rllib.readthedocs.io/en/latest/rllib-algorithms.html" rel="noopener ugc nofollow" target="_blank">这个链接中可以找到所有不同的算法的完整列表。</a></p><p id="bfb7" class="pw-post-body-paragraph ln lo it lp b lq mj ju ls lt mk jx lv lw ml ly lz ma mm mc md me mn mg mh mi im bi translated">在run函数中，我们可以设置允许的最大时间步长数，并传递不同的配置参数，以定制我们的网络。最后，使用<strong class="lp iu"> <em class="nb"> tune.grid() </em> </strong>功能，我们可以额外执行网格搜索，以便实时改进我们的一些模型<a class="ae ku" rel="noopener" target="_blank" href="/hyperparameters-optimization-526348bb8e2d">超参数</a>(例如，gamma和学习速率)。</p><pre class="kj kk kl km gt mp mq mr ms aw mt bi"><span id="ef1b" class="mu kw it mq b gy mv mw l mx my">results = tune.run(<br/>    'DQN', <br/>    stop={<br/>        'timesteps_total': 50000<br/>    },<br/>    config={<br/>    "env": 'LunarLander-v2',<br/>    "num_workers": 3,<br/>    "gamma" : tune.grid_search([0.999, 0.8]),<br/>    "lr": tune.grid_search([1e-2, 1e-3, 1e-4]),<br/>    }<br/>)</span></pre><p id="ec69" class="pw-post-body-paragraph ln lo it lp b lq mj ju ls lt mk jx lv lw ml ly lz ma mm mc md me mn mg mh mi im bi translated">训练完我们的代理后，将返回一个包含所有主要训练指标的数据帧(图3 ),然后我们可以使用它来确定哪个超参数组合可以为我们的代理带来更大的回报。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="nr ns di nt bf nu"><div class="gh gi nq"><img src="../Images/fa973518f359d3321b8c59afdfe3fcd5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9FAvXQbFxY9SjQVme4lyUg.png"/></div></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">图3:培训指标的总结</p></figure><p id="47a1" class="pw-post-body-paragraph ln lo it lp b lq mj ju ls lt mk jx lv lw ml ly lz ma mm mc md me mn mg mh mi im bi translated">利用返回的数据框架和记录的信息，我们可以很容易地看到一些关键的训练指标，例如使用不同的参数(如gamma和学习率值)如何随着时间的推移产生不同的奖励(图4)。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="nr ns di nt bf nu"><div class="gh gi nv"><img src="../Images/edfc94fd57f1a7c6fee02e3dd16e0b16.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZaqZZLU1M7jvG93TgPdvKg.png"/></div></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">图4:超参数优化报告</p></figure><p id="7da1" class="pw-post-body-paragraph ln lo it lp b lq mj ju ls lt mk jx lv lw ml ly lz ma mm mc md me mn mg mh mi im bi translated">最后，使用我们确定的最佳超参数，我们可以实例化一个新的DQN模型，并使用模型配置设置中的<strong class="lp iu"><em class="nb">“monitor】:True</em></strong>将代理的性能记录为视频。</p><pre class="kj kk kl km gt mp mq mr ms aw mt bi"><span id="703b" class="mu kw it mq b gy mv mw l mx my">ray.init()</span><span id="6805" class="mu kw it mq b gy mz mw l mx my">config = {'gamma': 0.999,<br/>          'lr': 0.0001,<br/>          "n_step": 1000,<br/>          'num_workers': 3,<br/>          'monitor': True}<br/>trainer2 = agents.dqn.DQNTrainer(env='LunarLander-v2', config=config)<br/>results2 = trainer2.train()</span></pre><p id="a28b" class="pw-post-body-paragraph ln lo it lp b lq mj ju ls lt mk jx lv lw ml ly lz ma mm mc md me mn mg mh mi im bi translated">正如我们在图5中看到的，我们的代理成功地学会了如何完成我们的任务！</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nw"><img src="../Images/ee462a56f24e393cb9a64018627457ef.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/1*24Jac4juG1HfasqYX-j1rw.gif"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">图5:训练有素的代理</p></figure></div><div class="ab cl nx ny hx nz" role="separator"><span class="oa bw bk ob oc od"/><span class="oa bw bk ob oc od"/><span class="oa bw bk ob oc"/></div><div class="im in io ip iq"><p id="293f" class="pw-post-body-paragraph ln lo it lp b lq mj ju ls lt mk jx lv lw ml ly lz ma mm mc md me mn mg mh mi im bi translated"><em class="nb">希望你喜欢这篇文章，感谢你的阅读！</em></p><h1 id="78ec" class="kv kw it bd kx ky kz la lb lc ld le lf jz lg ka lh kc li kd lj kf lk kg ll lm bi translated">联系人</h1><p id="94c7" class="pw-post-body-paragraph ln lo it lp b lq lr ju ls lt lu jx lv lw lx ly lz ma mb mc md me mf mg mh mi im bi translated">如果你想了解我最新的文章和项目<a class="ae ku" href="https://pierpaoloippolito28.medium.com/subscribe" rel="noopener">请关注我的媒体</a>并订阅我的<a class="ae ku" href="http://eepurl.com/gwO-Dr?source=post_page---------------------------" rel="noopener ugc nofollow" target="_blank">邮件列表</a>。以下是我的一些联系人详细信息:</p><ul class=""><li id="f1ba" class="nc nd it lp b lq mj lt mk lw ne ma nf me ng mi nh ni nj nk bi translated"><a class="ae ku" href="https://uk.linkedin.com/in/pier-paolo-ippolito-202917146?source=post_page---------------------------" rel="noopener ugc nofollow" target="_blank"> Linkedin </a></li><li id="646c" class="nc nd it lp b lq nl lt nm lw nn ma no me np mi nh ni nj nk bi translated"><a class="ae ku" href="https://pierpaolo28.github.io/blog/?source=post_page---------------------------" rel="noopener ugc nofollow" target="_blank">个人博客</a></li><li id="506d" class="nc nd it lp b lq nl lt nm lw nn ma no me np mi nh ni nj nk bi translated"><a class="ae ku" href="https://pierpaolo28.github.io/?source=post_page---------------------------" rel="noopener ugc nofollow" target="_blank">个人网站</a></li><li id="09a8" class="nc nd it lp b lq nl lt nm lw nn ma no me np mi nh ni nj nk bi translated"><a class="ae ku" href="https://towardsdatascience.com/@pierpaoloippolito28?source=post_page---------------------------" rel="noopener" target="_blank">中等轮廓</a></li><li id="7869" class="nc nd it lp b lq nl lt nm lw nn ma no me np mi nh ni nj nk bi translated"><a class="ae ku" href="https://github.com/pierpaolo28?source=post_page---------------------------" rel="noopener ugc nofollow" target="_blank"> GitHub </a></li><li id="b3d8" class="nc nd it lp b lq nl lt nm lw nn ma no me np mi nh ni nj nk bi translated"><a class="ae ku" href="https://www.kaggle.com/pierpaolo28?source=post_page---------------------------" rel="noopener ugc nofollow" target="_blank">卡格尔</a></li></ul><h1 id="8be0" class="kv kw it bd kx ky kz la lb lc ld le lf jz lg ka lh kc li kd lj kf lk kg ll lm bi translated">文献学</h1><p id="caf1" class="pw-post-body-paragraph ln lo it lp b lq lr ju ls lt lu jx lv lw lx ly lz ma mb mc md me mf mg mh mi im bi translated">[1]2018秋季CS498DL，作业5:深度强化学习(伊利诺伊大学香槟分校)，SVETLANA LAZEBNIK。访问地点:<a class="ae ku" href="http://slazebni.cs.illinois.edu/fall18/assignment5.html" rel="noopener ugc nofollow" target="_blank">http://slazebni.cs.illinois.edu/fall18/assignment5.html</a></p><p id="1cb1" class="pw-post-body-paragraph ln lo it lp b lq mj ju ls lt mk jx lv lw ml ly lz ma mm mc md me mn mg mh mi im bi translated">[2] PyPI: Ray:统一ML生态系统的并行分布式Python系统。访问地点:<a class="ae ku" href="https://pypi.org/project/ray/" rel="noopener ugc nofollow" target="_blank">https://pypi.org/project/ray/</a></p></div></div>    
</body>
</html>