<html>
<head>
<title>Principal Component Analysis Part 1: The Different Formulations.</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">主成分分析第1部分:不同的公式。</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/principal-component-analysis-part-1-the-different-formulations-6508f63a5553?source=collection_archive---------1-----------------------#2021-09-29">https://towardsdatascience.com/principal-component-analysis-part-1-the-different-formulations-6508f63a5553?source=collection_archive---------1-----------------------#2021-09-29</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="9d3e" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">什么是主成分分析？PCA的最大方差和最小误差公式有哪些？我们如何使用主成分分析降低维数？</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/ffb506e2a1f9989bfd4ca4fde5d674be.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ewfs7RSRgVRPhu6zGdAkbw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图1:以(1，3)为中心的多元高斯的主成分。图片来源:[3]。</p></figure><p id="ea31" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我们都知道主成分分析是降维中使用的标准方法之一。有多个帖子详细介绍了PCA的代码和实现；然而，在本帖中，我们将探究PCA是如何形成的，以及我们如何得出PCA算法。众所周知，主分量是对应于协方差矩阵的最大特征值的特征向量；这篇文章将探究为什么会这样，以及解决方案是如何得出的。这篇文章的内容将基于[1]和[2]的第12章提供的材料。</p><p id="8e87" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在我们进入PCA之前，知道什么是特征值和特征向量是很重要的。设<strong class="kx ir"> A </strong> ∈ <strong class="kx ir"> R </strong> ^{n×n}为一个<strong class="kx ir"> n×n </strong>矩阵。然后，向量<strong class="kx ir"> x </strong> ∈ <strong class="kx ir"> C </strong> ^n称为<strong class="kx ir"> A </strong>的特征向量，如果</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi lr"><img src="../Images/0b97bdf3e9fb6cb10b7f20bea01777a9.png" data-original-src="https://miro.medium.com/v2/resize:fit:378/format:webp/1*5v6N2ZGCTX8uq-7e1cV7aA.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">情商。1:特征值和特征向量</p></figure><p id="1b5e" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">λ ∈ <strong class="kx ir"> C </strong>称为<strong class="kx ir">a</strong>的特征值</p><p id="59aa" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">主成分分析(PCA)问题可以用两种方式公式化:<strong class="kx ir">最大方差公式化</strong>和<strong class="kx ir">最小误差公式化</strong>。在最大方差公式中，目标是找到数据到低维线性空间的正交投影，使得投影数据的方差最大化。在最小误差公式中，PCA被定义为最小化数据点和它们的投影之间的平均投影成本(均方误差)的线性投影。我们将在下面的章节中看到，这两个公式导致相同的解决方案。</p><h1 id="74f3" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">PCA:最大方差公式</h1><p id="f7bd" class="pw-post-body-paragraph kv kw iq kx b ky mk jr la lb ml ju ld le mm lg lh li mn lk ll lm mo lo lp lq ij bi translated">给定一组观察值{ <strong class="kx ir"> x </strong> _n}，n = 1，2，…，n和<strong class="kx ir"> x </strong> _n ∈ <strong class="kx ir"> R </strong> ^D，根据最大方差公式，我们的目标是找到<strong class="kx ir"> x </strong> _n到维度为<strong class="kx ir"><em class="mp">m&lt;</em></strong>的空间的正交投影。</p><p id="7697" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">现在让我们考虑最简单的情况，其中<strong class="kx ir"> M </strong> =1。我们定义一个向量<strong class="kx ir">w1</strong>∈<strong class="kx ir">r</strong>^d<strong class="kx ir"/>作为低维空间的方向。由于我们只对空间的方向感兴趣，我们将<strong class="kx ir"> w1 </strong>设为单位长度。即，</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mq"><img src="../Images/77fe89f283b68b4163d16bc1d38c9352.png" data-original-src="https://miro.medium.com/v2/resize:fit:250/format:webp/1*AN4yU0amfCu-2a1slWgLOQ.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">等式2: <strong class="bd mr"> w1 </strong>是单位矢量。</p></figure><p id="b89c" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">那么数据观察值<strong class="kx ir"> x </strong> _n可以被投影到这个新的空间上</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ms"><img src="../Images/57ee06922d8c1109bcd33673e5161a9c.png" data-original-src="https://miro.medium.com/v2/resize:fit:278/format:webp/1*p5q4E5p0ZGsmIBJVHIiuvw.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">等式3:<strong class="bd mr">x</strong>_ n在<strong class="bd mr"> w1 </strong>定义的新空间上的投影。</p></figure><p id="01e2" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">如果<strong class="kx ir"> x̄ </strong>是原始空间中数据观测值的平均值，那么投影空间中样本的平均值由下式给出</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mt"><img src="../Images/26aeef281d5434d3198a87af3039cc74.png" data-original-src="https://miro.medium.com/v2/resize:fit:222/format:webp/1*j_7foOjWZCBbaYiKruUJ6g.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">情商。3:将<strong class="bd mr"> x̄ </strong>投影到由<strong class="bd mr"> w1 </strong>定义的新空间上。</p></figure><p id="8172" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">现在，我们可以将投影数据的方差写成</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mu"><img src="../Images/783edb73a806c474820a191135a4b347.png" data-original-src="https://miro.medium.com/v2/resize:fit:1252/format:webp/1*3K5GuaWjrHDYRoEjSubDLw.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">情商。4:<strong class="bd mr">x̂</strong>的方差。</p></figure><p id="4c6e" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">其中<strong class="kx ir"> S </strong>是原始高维空间中观测数据的协方差矩阵。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mv"><img src="../Images/6267721841ca4e5a2a46998aba27c87d.png" data-original-src="https://miro.medium.com/v2/resize:fit:594/format:webp/1*l4RD51u2PYqppcBPT7gTpQ.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">情商。6:<strong class="bd mr">x</strong>的协方差。</p></figure><p id="0d9c" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">现在，根据最大方差公式定义，我们需要最大化<strong class="kx ir"> x̂ </strong>的方差。这可以通过最大化情商来实现。4.情商。4有一个平凡解当|| <strong class="kx ir"> w1 </strong> || → ∞时。为了防止这种情况，我们利用之前在等式2中设置的单位范数约束。为了解决这一问题，我们引入了拉格朗日乘数λ1，并将我们的优化目标表述如下:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mw"><img src="../Images/0eb5c687e5ebe278a561959ef1e20670.png" data-original-src="https://miro.medium.com/v2/resize:fit:908/format:webp/1*zmf3-Xru6yD3LSCtha2WyQ.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">情商。7:最大方差优化问题。</p></figure><p id="01cb" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">设置等式的导数。7 w.r.t <strong class="kx ir"> w1 </strong>为0时，我们得到一个驻点，</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mx"><img src="../Images/024b50f61e4264b752a91a816e724b78.png" data-original-src="https://miro.medium.com/v2/resize:fit:800/format:webp/1*623zJO52EXPX4wlOlqHVsQ.png"/></div></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi my"><img src="../Images/13315a2e0c89b45097fb57fb25299095.png" data-original-src="https://miro.medium.com/v2/resize:fit:332/format:webp/1*5_ZE2LR83bpRg01Kk2zK9A.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">情商。8:设置等式的导数。7 w.r.t <strong class="bd mr"> w1 </strong>归零。</p></figure><p id="53ec" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">这说明在驻点处，<strong class="kx ir"> w1 </strong>一定是<strong class="kx ir"> S </strong>的一个特征向量，λ1是特征值。对应特征向量<strong class="kx ir"> w1 </strong>。将等式7与<strong class="kx ir"> w1 </strong> ^T相乘，我们可以看到最大方差等于特征值λ1。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mz"><img src="../Images/322170fd201d7663fdeed343711aae56.png" data-original-src="https://miro.medium.com/v2/resize:fit:990/format:webp/1*NW0R2YVA4Ss6Vb60IoNFXA.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">情商。9:低维空间的最大方差等于特征向量<strong class="bd mr"> w1 </strong>对应的特征值。</p></figure><p id="a3ce" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我们可以通过选择使方差最大化同时又与现有方向正交的方向来识别额外的主成分。对于低维空间为<strong class="kx ir"> M </strong>维为<strong class="kx ir"> M &lt; D，</strong>的一般情况，主成分为对应M个最大特征值λ1，λ2，…，λm的特征向量<strong class="kx ir"> w </strong> 1，<strong class="kx ir"> w </strong> 2，… <strong class="kx ir"> w </strong> m</p><h1 id="229c" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">PCA:最小误差公式</h1><p id="21e0" class="pw-post-body-paragraph kv kw iq kx b ky mk jr la lb ml ju ld le mm lg lh li mn lk ll lm mo lo lp lq ij bi translated">设{ <strong class="kx ir"> x </strong> _n}，n = 1，2，…，n和<strong class="kx ir"> x </strong> _n ∈ <strong class="kx ir"> R </strong> ^D是数据观测值的集合，那么根据PCA的最小误差公式，我们的目标是找到使重建误差最小的变换:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi na"><img src="../Images/864f40b484f1d198ef75102f4123067b.png" data-original-src="https://miro.medium.com/v2/resize:fit:762/format:webp/1*P1-xozurlAAGZuGa30QoSQ.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">情商。10:最小误差目标</p></figure><p id="a5b3" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">其中<strong class="kx ir"> x̃ </strong>是从低维潜变量生成的重建。这里，我们有一个完整的D维正交(正交和单位长度)基<strong class="kx ir"> w </strong> _i，其中i= 1，2，… D。然后我们有</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nb"><img src="../Images/343eb65d87d7ce9c046e8b0fd87a09df.png" data-original-src="https://miro.medium.com/v2/resize:fit:524/format:webp/1*i9UX0xVF56Rt0GB1ys4zeA.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">情商。11:标准正交基。</p></figure><p id="883b" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">δᵢⱼ是克罗内克三角洲。由于基是完整的，我们可以把任何向量表示为基向量的线性组合</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nc"><img src="../Images/1fd3b3a8f725986474e36158f5fc2a28.png" data-original-src="https://miro.medium.com/v2/resize:fit:304/format:webp/1*SqlRwp32k5sX8_xK_CPMPw.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">情商。12</p></figure><p id="35b4" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">因为我们有标准正交基，所以我们有α_ni的解，它是<strong class="kx ir"> x </strong> _n和<strong class="kx ir"> w </strong> _i的点积。12作为</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nd"><img src="../Images/6c4189270a8193ac6d0b3a4c7168b9d6.png" data-original-src="https://miro.medium.com/v2/resize:fit:932/format:webp/1*Gvjn388QF2a6VtNVcto-sQ.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">情商。13这方面的证据可以在[4]中找到</p></figure><p id="bdda" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">通过降维，我们的目标是通过将D维数据投影到更低维的空间上，为D维数据(带有<strong class="kx ir"> <em class="mp"> M &lt; D </em> </strong>)找到M维表示。我们可以用前M个基向量来表示这个M维空间</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ne"><img src="../Images/72ac8a51daa5cdb8500232a07462c462.png" data-original-src="https://miro.medium.com/v2/resize:fit:644/format:webp/1*GteRydGumfzwkN0eROviPQ.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">情商。14</p></figure><p id="984e" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">并且剩余的(D-M)基由所有数据点共享(共享偏移)。在Eq中。14，<em class="mp"> z_ni </em>依赖于单个数据点，而b_i是所有数据点共享的常数。</p><p id="a9f7" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">来自Eq。13和Eq。14我们可以将<strong class="kx ir"> x </strong> _n和<strong class="kx ir"> x̃ </strong> _n之间的差值计算为</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nf"><img src="../Images/9b23c56aa1b0186ba38011d3f59d16c6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*f0C0BCtHPhfLHa-L3vtDbw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">情商。15:<strong class="bd mr">x</strong>n与<strong class="bd mr">x̃</strong>n的区别</p></figure><p id="d141" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">现在，我们可以用Eq来代替。等式中的15。10得到目标函数为:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ng"><img src="../Images/d1fefdd3405f503c13105f3f41e4432d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1018/format:webp/1*xAIElnelJsu8vGW9ZEWQ3w.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">情商。16</p></figure><p id="9e51" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">对w.r.t <em class="mp"> z_nj </em>求导并设为零，我们得到</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nh"><img src="../Images/a447200ffd4ac69dd4172a9633ec6a64.png" data-original-src="https://miro.medium.com/v2/resize:fit:306/format:webp/1*OKMkV82lgDDW7CvXKhSOAA.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">情商。17</p></figure><p id="83de" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">对w.r.t <strong class="kx ir"> b </strong> _j求导并设为零，我们得到</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/51ca22c3cd6d2889958e10f1c8e97a85.png" data-original-src="https://miro.medium.com/v2/resize:fit:276/format:webp/1*R91383fCRbwxEObh74UFIw.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">情商。18</p></figure><p id="5dd6" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">现在，我们可以用Eq来代替。17和Eq。等式中的18。16和得到</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nj"><img src="../Images/4eb5cc8a6df134f8d9bfafc617feb0eb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MzgXNUXw5Ez2rEcgBeHN4Q.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">情商。18</p></figure><p id="c6dc" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我们的目标是最小化J( <strong class="kx ir"> w </strong>)，但是我们观察到当<strong class="kx ir"> w </strong> = 0时，这个问题存在一个平凡的解。为了克服这一点，我们再次利用正交基的性质，并设置归一化约束|| <strong class="kx ir"> w </strong> || <strong class="kx ir"> = </strong> 1。</p><p id="90d8" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">现在，我们来看一个简单的例子，其中D=2，M=1。我们必须选择一个方向，这样我们可以最小化下面的目标</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/4b4124e75055375cd61f1e378f399ff6.png" data-original-src="https://miro.medium.com/v2/resize:fit:586/format:webp/1*LdSfBPKjOcaTWSX50Ycb0w.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">情商。19</p></figure><p id="d50c" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">如前所述，对w.r.t <strong class="kx ir"> w2 </strong>求导并设置为0，我们得到</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mq"><img src="../Images/0c5bfd703d1a909453cba209a8a9abb6.png" data-original-src="https://miro.medium.com/v2/resize:fit:250/format:webp/1*w3gvk19QldrbRdDdea9kyQ.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">情商。20</p></figure><p id="22a5" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">其中<strong class="kx ir"> w2 </strong>为特征值λ2对应的特征向量。代入等式。情商20。19、我们得到<strong class="kx ir"> <em class="mp"> J = λ2 </em> </strong> <em class="mp">，</em>即<strong class="kx ir"> J </strong>在我们选择特征值最小的特征向量时最小化。在一般情况下，当我们有<strong class="kx ir"> <em class="mp"> M &lt; D </em> </strong>时，重构的最小误差<strong class="kx ir"> J </strong>通过选择对应于<strong class="kx ir"><em class="mp">【D-M】</em></strong>最小特征值的特征向量<strong class="kx ir"> w </strong> _i获得，该最小特征值由下式给出</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nl"><img src="../Images/7f34a5ecae63b048ca3a4e2a7b6f3175.png" data-original-src="https://miro.medium.com/v2/resize:fit:234/format:webp/1*DcVch42alEE7fDVjv6NNNA.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">情商。21</p></figure><p id="ddea" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">最小重构误差(失真测量)由下式给出</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nm"><img src="../Images/69b25fea56bc117a448daf034682b199.png" data-original-src="https://miro.medium.com/v2/resize:fit:262/format:webp/1*e8g2VO1agsQph7HXUn0BmA.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">情商。22</p></figure><p id="65fa" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx ir"><em class="mp">【D-M】</em></strong>特征值之和。因此，我们的目标是使用M个最大的特征值，以便使失真测度<strong class="kx ir"> J、</strong>现在构成的<strong class="kx ir"> <em class="mp"> (D-M) </em> </strong>最小的特征值最小化。因此，我们可以得出结论，最小化重建误差最大化了投影的方差。</p><h1 id="439a" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated"><strong class="ak">使用主成分分析进行降维</strong></h1><p id="b3ee" class="pw-post-body-paragraph kv kw iq kx b ky mk jr la lb ml ju ld le mm lg lh li mn lk ll lm mo lo lp lq ij bi translated">既然我们已经看到M个主分量是对应于协方差矩阵的M个最大特征值的M个特征向量，我们继续看如何应用PCA来降低维数。</p><p id="81af" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">使用主成分分析进行降维包括4个步骤:</p><p id="b067" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx ir"> 1。数据居中<br/> </strong>第一步是计算并减去数据点的平均值，使数据以0为中心，因此平均值为零。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/d9373820a19e8a0f674ad7df582b4134.png" data-original-src="https://miro.medium.com/v2/resize:fit:242/format:webp/1*EH0i5lp6i0Ue_GauPfep_w.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">情商。23:居中数据<strong class="bd mr"> X̂ </strong></p></figure><p id="0101" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx ir"> 2。计算协方差矩阵</strong></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mq"><img src="../Images/423a172fb632601efb752513d37c3e4a.png" data-original-src="https://miro.medium.com/v2/resize:fit:250/format:webp/1*YY2ffs_-UM24ajHEm9Ds0Q.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">情商。24:协方差矩阵</p></figure><p id="0cb3" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx ir"> 3。使用特征值分解计算特征值和向量<br/></strong>PCA的目标是坐标系统的变换，使得新轴之间的协方差为0。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi no"><img src="../Images/82aa2a2b3cd9906e09ab57e265d11cd8.png" data-original-src="https://miro.medium.com/v2/resize:fit:992/format:webp/1*9-2QLVeHnOlKtJP50-SQ-Q.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图PCA的目标是找到空间W，使得新轴之间的协方差为0。</p></figure><p id="4245" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">因此我们对协方差矩阵<strong class="kx ir"> S </strong>进行特征值分解</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi np"><img src="../Images/88ad99b70c4cbde8286d1ec0700b71bf.png" data-original-src="https://miro.medium.com/v2/resize:fit:208/format:webp/1*yy6HWo8ZpJNb7OzGOZbRdQ.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">情商。25:协方差矩阵的EVD。</p></figure><p id="2673" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">这里，γ∈ℝ^(dxd)是特征向量的矩阵，而λ∈ℝ^(dxd)是包含特征值的对角矩阵。<br/>4<strong class="kx ir">。降维<br/> </strong>现在我们有了特征向量γ，为了降维，我们可以通过只保留最大M个特征值对应的列(特征向量)来截断γ。我们称截断的γ矩阵为γ’。则缩减空间中的表示由下式获得</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/eb8c1a744246ca50fcf1362fd6de2556.png" data-original-src="https://miro.medium.com/v2/resize:fit:174/format:webp/1*wZ-F7zQZUkutc83iiLEQjg.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">情商。26:降维</p></figure><h2 id="79cd" class="nr lt iq bd lu ns nt dn ly nu nv dp mc le nw nx me li ny nz mg lm oa ob mi oc bi translated">代码:使用PCA进行降维</h2><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="od oe l"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">代码:用主成分分析法降维。</p></figure><h2 id="b329" class="nr lt iq bd lu ns nt dn ly nu nv dp mc le nw nx me li ny nz mg lm oa ob mi oc bi translated"><strong class="ak">具有EVD的PCA的性能</strong></h2><p id="bc1f" class="pw-post-body-paragraph kv kw iq kx b ky mk jr la lb ml ju ld le mm lg lh li mn lk ll lm mo lo lp lq ij bi translated">使用特征值分解(EVD)的PCA非常昂贵，复杂度为O(D ),其中D是输入数据的维数。EVD计算所有的特征值和特征向量对，通常我们只需要对应于M个最大特征值的特征向量。因此，在实践中，许多有效的迭代方法，如幂迭代法，被用来计算特征向量。</p><h2 id="e355" class="nr lt iq bd lu ns nt dn ly nu nv dp mc le nw nx me li ny nz mg lm oa ob mi oc bi translated"><strong class="ak"> PCA和数据标准化[2] </strong></h2><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi of"><img src="../Images/413846166c73806324a3880b7a65bf3c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aOPir1Iu38GZbkVxaAeG0g.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图3主成分分析可能被非标准化数据误导。(a)主成分是偏斜的，因为PCA被非标准化数据误导。(b)当比额表标准化时的PCA。从[5]处的代码生成的图像。</p></figure><p id="7d73" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">PCA的主方向是方差最大的方向。因此，主成分分析可能会被方向误导，沿着这些方向，仅仅因为测量尺度，方差就显得很高。我们可以在图3(a)中看到这一点，其中主成分没有正确对齐，因为它被非标准化的标度误导。图3(b)显示了当量表标准化时的正确主成分。因此，需要注意将数据标准化，以避免此类问题。</p><h1 id="dadd" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">结论</h1><p id="678f" class="pw-post-body-paragraph kv kw iq kx b ky mk jr la lb ml ju ld le mm lg lh li mn lk ll lm mo lo lp lq ij bi translated">在这篇相当长的文章中，我们深入研究了PCA的两个公式:最大方差和最小误差公式。我们看到两个公式具有相同的解决方案/算法——选择对应于数据协方差矩阵的M个最大特征值的特征向量作为新的基础。我们看到了PCA如何用于降维，以及如何在python中实现。最后，我们简要地研究了标准化数据的重要性以及它如何影响算法。这篇文章到此结束，这仅仅是PCA系列的第一部分。接下来的部分将涵盖概率PCA、奇异值分解、自动编码器以及自动编码器、PCA和SVD之间的关系。</p><p id="a2a4" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">跟随<a class="og oh ep" href="https://medium.com/u/82053676fe58?source=post_page-----6508f63a5553--------------------------------" rel="noopener" target="_blank"> Aadhithya Sankar </a>获得下一个零件可用时的通知！</p><p id="52c5" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">如果您发现任何错误，请留下评论，我会修复它们！🙏🏽 ✌🏽</p></div><div class="ab cl oi oj hu ok" role="separator"><span class="ol bw bk om on oo"/><span class="ol bw bk om on oo"/><span class="ol bw bk om on"/></div><div class="ij ik il im in"><h1 id="2df4" class="ls lt iq bd lu lv op lx ly lz oq mb mc jw or jx me jz os ka mg kc ot kd mi mj bi translated">参考</h1><p id="6fb9" class="pw-post-body-paragraph kv kw iq kx b ky mk jr la lb ml ju ld le mm lg lh li mn lk ll lm mo lo lp lq ij bi translated">[1] <a class="ae ou" href="https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf" rel="noopener ugc nofollow" target="_blank"> Bishop，Christopher M .模式识别与机器学习。纽约:斯普林格，2006年。</a></p><p id="46e8" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">[2] <a class="ae ou" href="http://noiselab.ucsd.edu/ECE228/Murphy_Machine_Learning.pdf" rel="noopener ugc nofollow" target="_blank">墨菲，凯文P. <em class="mp">机器学习:概率视角</em>。麻省理工学院出版社，2012年。</a></p><p id="85e1" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">[3]<a class="ae ou" href="https://commons.wikimedia.org/wiki/File:GaussianScatterPCA.svg#/media/File:GaussianScatterPCA.svg" rel="noopener ugc nofollow" target="_blank">https://commons . wikimedia . org/wiki/File:gaussianscatterpca . SVG #/media/File:gaussianscatterpca . SVG</a></p><p id="8be1" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">[4]https://www.math.ucdavis.edu/~linear/old/notes21.pdf<a class="ae ou" href="https://www.math.ucdavis.edu/~linear/old/notes21.pdf" rel="noopener ugc nofollow" target="_blank"/></p><p id="c966" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">[5] <a class="ae ou" href="https://github.com/probml/pyprobml" rel="noopener ugc nofollow" target="_blank">墨菲，k .，索利曼，m .，杜兰-马丁，g .，卡拉，a .，梁昂，m .，雷迪，s .，&amp;帕特尔，D. (2021)。概率机器学习的PyProbML库[计算机软件]。</a></p></div><div class="ab cl oi oj hu ok" role="separator"><span class="ol bw bk om on oo"/><span class="ol bw bk om on oo"/><span class="ol bw bk om on"/></div><div class="ij ik il im in"><h1 id="a272" class="ls lt iq bd lu lv op lx ly lz oq mb mc jw or jx me jz os ka mg kc ot kd mi mj bi translated"><strong class="ak">资源</strong></h1><p id="4340" class="pw-post-body-paragraph kv kw iq kx b ky mk jr la lb ml ju ld le mm lg lh li mn lk ll lm mo lo lp lq ij bi translated">这里有一些资源可以帮助更好地理解这个主题</p><ol class=""><li id="4385" class="ov ow iq kx b ky kz lb lc le ox li oy lm oz lq pa pb pc pd bi translated"><strong class="kx ir"> PCA:最大方差公式(阿姆斯特丹大学)</strong></li></ol><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="pe oe l"/></div></figure><p id="7d6e" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx ir"> 2。PCA最小误差公式(阿马斯特达姆大学)</strong></p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="pe oe l"/></div></figure><h1 id="9d6f" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">作者的更多作品</h1><p id="b5b5" class="pw-post-body-paragraph kv kw iq kx b ky mk jr la lb ml ju ld le mm lg lh li mn lk ll lm mo lo lp lq ij bi translated">如果你喜欢这篇文章，你可能也会喜欢下面的文章:</p><div class="pf pg gp gr ph pi"><a rel="noopener follow" target="_blank" href="/real-time-artwork-generation-using-deep-learning-a33a2084ae98"><div class="pj ab fo"><div class="pk ab pl cl cj pm"><h2 class="bd ir gy z fp pn fr fs po fu fw ip bi translated">使用深度学习的实时艺术品生成</h2><div class="pp l"><h3 class="bd b gy z fp pn fr fs po fu fw dk translated">用于任意内容样式图像对之间的样式转换的自适应实例标准化(AdaIN)。</h3></div><div class="pq l"><p class="bd b dl z fp pn fr fs po fu fw dk translated">towardsdatascience.com</p></div></div><div class="pr l"><div class="ps l pt pu pv pr pw kp pi"/></div></div></a></div><div class="pf pg gp gr ph pi"><a rel="noopener follow" target="_blank" href="/a-primer-on-atrous-convolutions-and-depth-wise-separable-convolutions-443b106919f5"><div class="pj ab fo"><div class="pk ab pl cl cj pm"><h2 class="bd ir gy z fp pn fr fs po fu fw ip bi translated">阿特鲁卷积和深度可分卷积的初步研究</h2><div class="pp l"><h3 class="bd b gy z fp pn fr fs po fu fw dk translated">什么是萎缩/扩张和深度方向可分卷积？与标准卷积有何不同？什么…</h3></div><div class="pq l"><p class="bd b dl z fp pn fr fs po fu fw dk translated">towardsdatascience.com</p></div></div><div class="pr l"><div class="px l pt pu pv pr pw kp pi"/></div></div></a></div><div class="pf pg gp gr ph pi"><a rel="noopener follow" target="_blank" href="/demystified-wasserstein-gans-wgan-f835324899f4"><div class="pj ab fo"><div class="pk ab pl cl cj pm"><h2 class="bd ir gy z fp pn fr fs po fu fw ip bi translated">揭秘:瓦瑟斯坦·甘斯(WGAN)</h2><div class="pp l"><h3 class="bd b gy z fp pn fr fs po fu fw dk translated">什么是瓦瑟斯坦距离？用Wasserstein距离训练GANs背后的直觉是什么？怎么样…</h3></div><div class="pq l"><p class="bd b dl z fp pn fr fs po fu fw dk translated">towardsdatascience.com</p></div></div><div class="pr l"><div class="py l pt pu pv pr pw kp pi"/></div></div></a></div></div></div>    
</body>
</html>