<html>
<head>
<title>Master Machine Learning: Decision Trees From Scratch With Python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">机器学习大师:用Python从头开始做决策树</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/master-machine-learning-decision-trees-from-scratch-with-python-de75b0494bcd?source=collection_archive---------7-----------------------#2021-04-08">https://towardsdatascience.com/master-machine-learning-decision-trees-from-scratch-with-python-de75b0494bcd?source=collection_archive---------7-----------------------#2021-04-08</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="8a64" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">机器学习既简单又直观——这里有一个完整的从头开始的决策树指南</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/6a76b0d921753078f3a7d184364e0438.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*Vyty2y5a3AJRPV9B"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">加里·本迪格在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="534a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">决策树是用于分类和回归的最直观的机器学习算法之一。阅读之后，你将知道如何完全从头开始实现一个决策树分类器。</p><p id="f7ef" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这是许多即将开始的文章中的第五篇，所以如果你想了解更多，请继续关注这个博客。之前文章的链接位于本文末尾。</p><p id="4b7d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这篇文章的结构如下:</p><ul class=""><li id="21f8" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated">决策树介绍</li><li id="3b62" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">决策树背后的数学</li><li id="b3d5" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">递归速成班</li><li id="5740" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">从头开始实施</li><li id="f1d9" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">模型评估</li><li id="73be" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">与Scikit-Learn的比较</li><li id="610d" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">结论</li></ul><p id="c2a6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">你可以在这里下载相应的笔记本<a class="ae ky" href="https://github.com/daradecic/BDS-articles/blob/main/013_MML_Decision_Trees.ipynb" rel="noopener ugc nofollow" target="_blank">。</a></p></div><div class="ab cl mj mk hx ml" role="separator"><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo"/></div><div class="im in io ip iq"><h1 id="9d79" class="mq mr it bd ms mt mu mv mw mx my mz na jz nb ka nc kc nd kd ne kf nf kg ng nh bi translated">决策树介绍</h1><p id="ab02" class="pw-post-body-paragraph kz la it lb b lc ni ju le lf nj jx lh li nk lk ll lm nl lo lp lq nm ls lt lu im bi translated">决策树是一种用于回归和分类任务的非参数模型。从头开始的实现将花费您一些时间来完全理解，但是算法背后的直觉是非常简单的。</p><p id="ec7f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">决策树仅由两个元素构成——节点和分支。我们稍后将讨论不同类型的节点。如果你决定跟随，术语<strong class="lb iu">递归</strong>不应该感觉像一门外语，因为算法是基于这个概念。几分钟后你会得到一个递归的速成课程，所以如果你对这个话题有点生疏，不要着急。</p><p id="a619" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们先来看一个决策树的例子:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nn"><img src="../Images/2d7705ce6f8ecf93d7bc6eaa664915a5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*T7u6eItLyLJb42lKTSLluQ.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图1 —带有节点类型的决策树表示示例(作者图片)</p></figure><p id="69c3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如您所见，有多种类型的节点:</p><ul class=""><li id="2668" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated"><strong class="lb iu">根节点</strong> —树顶端的节点。它包含一个能最好地分割数据的特征(一个能最准确地对目标变量进行分类的单一特征)</li><li id="935d" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated"><strong class="lb iu">决策节点</strong> —评估变量的节点。这些节点有指向它们和远离它们的箭头</li><li id="6855" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated"><strong class="lb iu">叶节点</strong> —进行预测的最终节点</li></ul><p id="fb92" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">根据数据集的大小(行和列)，可能有成千上万种方式来排列节点及其条件。<strong class="lb iu">那么，我们如何确定根节点呢？</strong></p><h2 id="5e29" class="no mr it bd ms np nq dn mw nr ns dp na li nt nu nc lm nv nw ne lq nx ny ng nz bi translated">如何确定根节点</h2><p id="f36e" class="pw-post-body-paragraph kz la it lb b lc ni ju le lf nj jx lh li nk lk ll lm nl lo lp lq nm ls lt lu im bi translated">简而言之，我们需要检查每个输入特征如何独立地对目标变量进行分类。如果没有一个单独的特征在分类中100%正确，我们可以认为这些特征<strong class="lb iu">不纯</strong>。</p><p id="743c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为了进一步确定哪个不纯的特征是最纯的，我们可以使用<strong class="lb iu">熵</strong>度量。我们稍后将讨论公式和计算，但是您应该记住熵值的范围是从0(最好)到1(最差)。</p><p id="fc63" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然后，具有最低熵的变量被用作根节点。</p><h2 id="9fc3" class="no mr it bd ms np nq dn mw nr ns dp na li nt nu nc lm nv nw ne lq nx ny ng nz bi translated">培训过程</h2><p id="83fc" class="pw-post-body-paragraph kz la it lb b lc ni ju le lf nj jx lh li nk lk ll lm nl lo lp lq nm ls lt lu im bi translated">为了开始训练决策树分类器，我们必须确定根节点。那部分已经讨论过了。</p><p id="228e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然后，对于每个单独的分割，计算<strong class="lb iu">信息增益</strong>度量。简而言之，它表示基于特定分割的所有熵值的平均值。我们稍后将讨论公式和计算，但请记住，增益越高，决策分割越好。</p><p id="7eea" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然后，该算法执行贪婪搜索-检查所有输入要素及其唯一值，计算每个组合的信息增益，并保存每个结点的最佳分割要素和阈值。</p><p id="0df1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">以这种方式，树被递归地构建。递归过程可能会永远继续下去，所以我们必须手动指定一些退出条件。最常见的是节点处的最大深度和最小样本。这两者都将在后面的实现中讨论。</p><h2 id="a702" class="no mr it bd ms np nq dn mw nr ns dp na li nt nu nc lm nv nw ne lq nx ny ng nz bi translated">预测过程</h2><p id="4d71" class="pw-post-body-paragraph kz la it lb b lc ni ju le lf nj jx lh li nk lk ll lm nl lo lp lq nm ls lt lu im bi translated">一旦构建了树，我们就可以通过递归遍历树来预测看不见的数据。我们可以根据输入数据和每个节点的学习阈值来检查遍历方向(左或右)。</p><p id="0e5c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">一旦到达叶节点，就返回最常见的值。</p><p id="ad6c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这就是决策树背后的基本理论和直觉。让我们在下一节讨论算法背后的数学。</p></div><div class="ab cl mj mk hx ml" role="separator"><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo"/></div><div class="im in io ip iq"><h1 id="2810" class="mq mr it bd ms mt mu mv mw mx my mz na jz nb ka nc kc nd kd ne kf nf kg ng nh bi translated">决策树背后的数学</h1><p id="2907" class="pw-post-body-paragraph kz la it lb b lc ni ju le lf nj jx lh li nk lk ll lm nl lo lp lq nm ls lt lu im bi translated">决策树代表的更多的是编码挑战，而不是数学挑战。对于学习部分，你只需要实现两个公式——熵和信息增益。</p><p id="ec7d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">先说<strong class="lb iu">熵</strong>。如前所述，它在节点级别测量分裂的纯度。其值范围从0(纯)到1(不纯)。</p><p id="0335" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这是熵的公式:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oa"><img src="../Images/8d9ea2290f647ebe0d5eb39fc171a682.png" data-original-src="https://miro.medium.com/v2/resize:fit:1020/format:webp/1*Y0i8sCx2UvssY5PP-DbB4A.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图2 —熵公式(图片由作者提供)</p></figure><p id="a13b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">正如你所看到的，这是一个相对简单的等式，所以让我们看看它的实际应用。假设你想计算以下向量的纯度:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ob"><img src="../Images/f0cbe921e8b6f8baa19712cba04b51eb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1292/format:webp/1*VKhsXMRT_YJ50buiaGxu1w.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图片3 —熵输入(图片由作者提供)</p></figure><p id="fb9f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">总而言之，零和一是具有以下计数的分类标签:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/5481ac889c5b641b5061dc9a9da503d2.png" data-original-src="https://miro.medium.com/v2/resize:fit:404/format:webp/1*kwRZNjqipFBRJwDlQ5YYzw.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图4 —班级分布汇总(作者提供的图片)</p></figure><p id="3f96" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">从这一点来看，熵的计算非常简单(四舍五入到小数点后五位):</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi od"><img src="../Images/711b66a95fde791d6785ec2139e879d9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aPL5G_m-rJyUCnenNdXQfw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图5 —熵计算(图片由作者提供)</p></figure><p id="344c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">0.88的结果表明分裂远非纯粹。接下来我们用Python重复一下计算。以下代码实现了<code class="fe oe of og oh b">entropy(s)</code>公式，并在同一个向量上计算它:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oi oj l"/></div></figure><p id="53f6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">结果如下图所示:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ok"><img src="../Images/5ca3d58a561bc58b4ee3c08a40bf741b.png" data-original-src="https://miro.medium.com/v2/resize:fit:740/format:webp/1*2NVBduypZYuwL_JvnbE_cA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图Python中的熵计算(图片由作者提供)</p></figure><p id="dfb3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">正如您所看到的，结果是相同的，表明公式实现正确。</p><p id="cd7c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">接下来让我们看看<strong class="lb iu">信息增益</strong>。它表示基于特定分割的所有熵值的平均值。信息增益值越高，决策分裂越好。</p><p id="0b24" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">信息增益可通过以下公式计算:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ol"><img src="../Images/839828bf41b12eaf7b9161e8cefb9fb6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*snhB1PJBaXOdBd22V_wWMQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图7 —信息增益公式(图片由作者提供)</p></figure><p id="16e9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们来看一个示例分割，并计算信息增益:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi om"><img src="../Images/60ab8f5ce506cf1629dc79d5cb3f5561.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tZmyiQi-mi8C_MIGpg6RpA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图8 —信息增益计算的分割示例(图片由作者提供)</p></figure><p id="cffc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如你所见，熵值是事先计算好的，所以我们不必在它们上面浪费时间。计算信息增益现在是一个简单的过程:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi on"><img src="../Images/d46dd8a021f920ec6186d0080c0225e9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Y_NuTRlmibzXyR7tleiuIg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图9 —信息增益计算(图片由作者提供)</p></figure><p id="c955" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">接下来让我们用Python实现它。下面的代码片段实现了<code class="fe oe of og oh b">information_gain()</code>函数，并为之前讨论的分割计算了它:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oi oj l"/></div></figure><p id="d184" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">结果如下图所示:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oo"><img src="../Images/0ee5b226804b1c21ddef608cbc643eef.png" data-original-src="https://miro.medium.com/v2/resize:fit:1120/format:webp/1*wl70ov240DeU1mniwfVmkg.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图10-Python中的信息增益计算(图片由作者提供)</p></figure><p id="eef5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如您所见，这些值是匹配的。</p><p id="d23d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这就是决策树背后的数学。我再重复一遍——这个算法用代码实现比用数学方法理解更具挑战性。这就是为什么你需要额外的递归入门——接下来。</p></div><div class="ab cl mj mk hx ml" role="separator"><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo"/></div><div class="im in io ip iq"><h1 id="1ddf" class="mq mr it bd ms mt mu mv mw mx my mz na jz nb ka nc kc nd kd ne kf nf kg ng nh bi translated">递归速成班</h1><p id="5fd8" class="pw-post-body-paragraph kz la it lb b lc ni ju le lf nj jx lh li nk lk ll lm nl lo lp lq nm ls lt lu im bi translated">许多关于决策树的实现归结为递归。本节将简要介绍递归函数，但绝不是该主题的入门指南。如果这个术语对你来说是新的，如果你想理解决策树，请研究它。</p><p id="396b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">简单来说，递归函数就是调用自身的函数。我们不希望这个过程无限期地进行下去，所以函数需要一个退出条件。你会发现它写在函数的顶部。</p><p id="82b8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们来看看最简单的例子——一个返回整数阶乘的递归函数:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oi oj l"/></div></figure><p id="6ec7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">结果如下图所示:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi op"><img src="../Images/08d92b149dd9277d0c3e30f6e42d7143.png" data-original-src="https://miro.medium.com/v2/resize:fit:704/format:webp/1*R5PrFxceLH0LIjvw_4gL9Q.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图11-Python中的阶乘计算(图片由作者提供)</p></figure><p id="5d67" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如您所见，该函数会调用自身，直到输入的数字不是1。这就是我们函数的退出条件。</p><p id="809d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">决策树分类器中需要递归来建立额外的节点，直到满足某些退出条件。这就是为什么理解这个概念至关重要。</p><p id="de65" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">接下来，我们将实现分类器。这将需要大约200行代码(减去文档字符串和注释)，所以拥抱你自己。</p></div><div class="ab cl mj mk hx ml" role="separator"><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo"/></div><div class="im in io ip iq"><h1 id="21cf" class="mq mr it bd ms mt mu mv mw mx my mz na jz nb ka nc kc nd kd ne kf nf kg ng nh bi translated">从头开始实施</h1><p id="abea" class="pw-post-body-paragraph kz la it lb b lc ni ju le lf nj jx lh li nk lk ll lm nl lo lp lq nm ls lt lu im bi translated">我们需要两节课:</p><ol class=""><li id="690c" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu oq mb mc md bi translated"><code class="fe oe of og oh b">Node</code>–实现决策树的单个节点</li><li id="c821" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu oq mb mc md bi translated"><code class="fe oe of og oh b">DecisionTree</code>–实现算法</li></ol><p id="f747" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">先说<code class="fe oe of og oh b">Node</code>类。它在这里存储关于特征、阈值、向左和向右的数据、信息增益和叶节点值的数据。所有的初始设置都是<code class="fe oe of og oh b">None</code>。根节点和决策节点将包含除叶节点值之外的所有值，而叶节点将包含相反的值。</p><p id="9a9d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">下面是该类的代码:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oi oj l"/></div></figure><p id="dccb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">那是容易的部分。接下来让我们实现分类器。它将包含许多方法，所有这些方法都将在下面讨论:</p><ul class=""><li id="390e" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated"><code class="fe oe of og oh b">__init__()</code>–构造器，保存<code class="fe oe of og oh b">min_samples_split</code>和<code class="fe oe of og oh b">max_depth</code>的值。这些是超参数。第一个用于指定分割节点所需的最小样本数，第二个指定树的最大深度。两者都在递归函数中用作退出条件</li><li id="e2e5" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated"><code class="fe oe of og oh b">_entropy(s)</code>–计算输入向量的杂质<code class="fe oe of og oh b">s</code></li><li id="8d5e" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated"><code class="fe oe of og oh b">_information_gain(parent, left_child, right_child)</code>计算父节点和两个子节点之间拆分的信息增益值</li><li id="6d7e" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated"><code class="fe oe of og oh b">_best_split(X, y)</code>函数计算输入特征<code class="fe oe of og oh b">X</code>和目标变量<code class="fe oe of og oh b">y</code>的最佳分割参数。它通过迭代<code class="fe oe of og oh b">X</code>中的每一列和每一列中的每个阈值来使用信息增益找到最佳分割</li><li id="eead" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated"><code class="fe oe of og oh b">_build(X, y, depth)</code>函数递归构建决策树，直到满足停止标准(构造函数中的超参数)</li><li id="9bc0" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated"><code class="fe oe of og oh b">fit(X, y)</code>函数调用<code class="fe oe of og oh b">_build()</code>函数并将构建的树存储到构造函数中</li><li id="7fb9" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated"><code class="fe oe of og oh b">_predict(x)</code>函数遍历树来分类单个实例</li><li id="21b9" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated"><code class="fe oe of og oh b">predict(X)</code>函数将<code class="fe oe of og oh b">_predict()</code>函数应用于矩阵<code class="fe oe of og oh b">X</code>中的每个实例。</li></ul><p id="9979" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这太多了——没什么好争论的。花点时间理解下面代码片段中的每一行。这是有据可查的，所以注释应该会有所帮助:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oi oj l"/></div></figure><p id="4147" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">你不需要一口气理解每一行代码。给它一点时间，一行一行地检查代码，并尝试解释为什么事情会成功。一旦你理解了算法背后的基本直觉，就没那么难了。</p></div><div class="ab cl mj mk hx ml" role="separator"><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo"/></div><div class="im in io ip iq"><h1 id="e872" class="mq mr it bd ms mt mu mv mw mx my mz na jz nb ka nc kc nd kd ne kf nf kg ng nh bi translated">模型评估</h1><p id="6fe2" class="pw-post-body-paragraph kz la it lb b lc ni ju le lf nj jx lh li nk lk ll lm nl lo lp lq nm ls lt lu im bi translated">接下来让我们测试我们的分类器。我们将使用来自Scikit-Learn的虹膜数据集。以下代码片段加载数据集，并将其分为要素(<code class="fe oe of og oh b">X</code>)和目标(<code class="fe oe of og oh b">y</code>):</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oi oj l"/></div></figure><p id="b5ab" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">接下来，让我们将数据集分成训练和测试部分。以下代码片段就是这样做的，比例为80:20:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oi oj l"/></div></figure><p id="32b2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在让我们开始训练。下面的代码片段使用默认超参数训练模型，并对测试集进行预测:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oi oj l"/></div></figure><p id="a5a8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们来看看生成的预测(<code class="fe oe of og oh b">preds</code>):</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi or"><img src="../Images/0d55a97325574cc147311662664069ca.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wLg99dDIFWZCbBb7ftuBqQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图12 —测试集上的自定义决策树预测(图片由作者提供)</p></figure><p id="283d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在看实际的类标签(<code class="fe oe of og oh b">y_test</code>):</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi os"><img src="../Images/0767efe3a3d5298b81481de495c72ea2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tqF9r2EGTOZZRR3dHbbiLA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图13 —测试集类别标签(作者图片)</p></figure><p id="a291" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如您所见，两者完全相同，表明分类器非常准确。如果你愿意，你可以进一步评估性能。下面的代码打印测试集的准确度分数:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oi oj l"/></div></figure><p id="0e0d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">正如所料，<code class="fe oe of og oh b">1.0</code>的值将被打印出来。不要让这个欺骗你——虹膜数据集非常容易正确分类，特别是如果你有一个好的“随机”测试集。不过，让我们将我们的分类器与Scikit-Learn内置的分类器进行比较。</p></div><div class="ab cl mj mk hx ml" role="separator"><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo"/></div><div class="im in io ip iq"><h1 id="5766" class="mq mr it bd ms mt mu mv mw mx my mz na jz nb ka nc kc nd kd ne kf nf kg ng nh bi translated">与Scikit-Learn的比较</h1><p id="394d" class="pw-post-body-paragraph kz la it lb b lc ni ju le lf nj jx lh li nk lk ll lm nl lo lp lq nm ls lt lu im bi translated">我们想知道我们的模型是否好，所以让我们将它与我们知道效果很好的东西——Scikit-Learn的<code class="fe oe of og oh b">DecisionTreeClassifier</code>类进行比较。</p><p id="c391" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">您可以使用以下代码片段来导入模型类、训练模型、进行预测以及打印准确性得分:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oi oj l"/></div></figure><p id="ab69" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如您所料，我们得到了完美的准确度分数<code class="fe oe of og oh b">1.0</code>。</p><p id="f9f1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">今天到此为止。让我们在下一部分总结一下。</p></div><div class="ab cl mj mk hx ml" role="separator"><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo"/></div><div class="im in io ip iq"><h1 id="45bf" class="mq mr it bd ms mt mu mv mw mx my mz na jz nb ka nc kc nd kd ne kf nf kg ng nh bi translated">结论</h1><p id="afb9" class="pw-post-body-paragraph kz la it lb b lc ni ju le lf nj jx lh li nk lk ll lm nl lo lp lq nm ls lt lu im bi translated">这是我写过的最具挑战性的文章之一。花了大约一周的时间才把一切都做好，并尽可能让代码易于理解。当然，你至少需要阅读几本书才能完全理解这个话题。请随意探索其他资源，因为这将进一步加深您的理解。</p><p id="a586" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">您现在知道如何从头开始实现决策树分类器算法。这是否意味着你应该抛弃事实上的标准机器学习库？不，一点也不。我来详细说明一下。</p><p id="5ff7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">你能从头开始写东西并不意味着你应该这样做。尽管如此，了解算法如何工作的每个细节是一项有价值的技能，可以帮助你从其他<em class="ot"> fit和预测</em>数据科学家中脱颖而出。</p><p id="09cc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">感谢您的阅读，如果您对更多从零开始的机器学习文章感兴趣，请继续关注博客。</p></div><div class="ab cl mj mk hx ml" role="separator"><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo"/></div><div class="im in io ip iq"><p id="da10" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">喜欢这篇文章吗？成为 <a class="ae ky" href="https://medium.com/@radecicdario/membership" rel="noopener"> <em class="ot">中等会员</em> </a> <em class="ot">继续无限制学习。如果你使用下面的链接，我会收到你的一部分会员费，不需要你额外付费。</em></p><div class="ou ov gp gr ow ox"><a href="https://medium.com/@radecicdario/membership" rel="noopener follow" target="_blank"><div class="oy ab fo"><div class="oz ab pa cl cj pb"><h2 class="bd iu gy z fp pc fr fs pd fu fw is bi translated">通过我的推荐链接加入Medium-Dario rade ci</h2><div class="pe l"><h3 class="bd b gy z fp pc fr fs pd fu fw dk translated">作为一个媒体会员，你的会员费的一部分会给你阅读的作家，你可以完全接触到每一个故事…</h3></div><div class="pf l"><p class="bd b dl z fp pc fr fs pd fu fw dk translated">medium.com</p></div></div><div class="pg l"><div class="ph l pi pj pk pg pl ks ox"/></div></div></a></div></div><div class="ab cl mj mk hx ml" role="separator"><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo"/></div><div class="im in io ip iq"><h1 id="21c7" class="mq mr it bd ms mt mu mv mw mx my mz na jz nb ka nc kc nd kd ne kf nf kg ng nh bi translated">了解更多信息</h1><ul class=""><li id="150e" class="lv lw it lb b lc ni lf nj li pm lm pn lq po lu ma mb mc md bi translated"><a class="ae ky" rel="noopener" target="_blank" href="/master-machine-learning-simple-linear-regression-from-scratch-with-python-1526487c5964">掌握机器学习:用Python从头开始简单线性回归</a></li><li id="c870" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated"><a class="ae ky" rel="noopener" target="_blank" href="/master-machine-learning-multiple-linear-regression-from-scratch-with-python-ac716a9b78a4">掌握机器学习:用Python从头开始多元线性回归</a></li><li id="0b5d" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated"><a class="ae ky" rel="noopener" target="_blank" href="/master-machine-learning-logistic-regression-from-scratch-with-python-acfe73a0a424">掌握机器学习:用Python从头开始逻辑回归</a></li><li id="1e57" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated"><a class="ae ky" rel="noopener" target="_blank" href="/master-machine-learning-k-nearest-neighbors-from-scratch-with-python-5009177f523">机器学习高手:用Python从零开始K近邻</a></li></ul></div><div class="ab cl mj mk hx ml" role="separator"><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo"/></div><div class="im in io ip iq"><h1 id="5192" class="mq mr it bd ms mt mu mv mw mx my mz na jz nb ka nc kc nd kd ne kf nf kg ng nh bi translated">保持联系</h1><ul class=""><li id="4207" class="lv lw it lb b lc ni lf nj li pm lm pn lq po lu ma mb mc md bi translated">在<a class="ae ky" href="https://medium.com/@radecicdario" rel="noopener">媒体</a>上关注我，了解更多类似的故事</li><li id="63d7" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">注册我的<a class="ae ky" href="https://mailchi.mp/46a3d2989d9b/bdssubscribe" rel="noopener ugc nofollow" target="_blank">简讯</a></li><li id="178c" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">在LinkedIn 上连接</li><li id="85a1" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">查看我的<a class="ae ky" href="https://www.betterdatascience.com/" rel="noopener ugc nofollow" target="_blank">网站</a></li></ul></div><div class="ab cl mj mk hx ml" role="separator"><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo"/></div><div class="im in io ip iq"><p id="0413" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="ot">原载于2021年4月8日https://betterdatascience.com</em><a class="ae ky" href="https://betterdatascience.com/mml-decision-trees/" rel="noopener ugc nofollow" target="_blank"><em class="ot"/></a><em class="ot">。</em></p></div></div>    
</body>
</html>