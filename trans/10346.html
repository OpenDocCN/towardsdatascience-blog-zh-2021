<html>
<head>
<title>Deriving Backpropagation with Cross-Entropy Loss</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">利用交叉熵损失推导反向传播</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/deriving-backpropagation-with-cross-entropy-loss-d24811edeaf9?source=collection_archive---------3-----------------------#2021-10-02">https://towardsdatascience.com/deriving-backpropagation-with-cross-entropy-loss-d24811edeaf9?source=collection_archive---------3-----------------------#2021-10-02</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="f578" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">最小化分类模型的损失</h2></div><p id="8466" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">你可以为你的神经网络选择无数的损失函数。损失函数的选择对于网络的性能是必不可少的，因为最终网络中的参数将被设置为使得损失最小化。</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi lb"><img src="../Images/ce6347604366beb93cfa47fce2e0ec1f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HRB8zK_PGDB_TPAzjwMrbQ.jpeg"/></div></div><p class="ln lo gj gh gi lp lq bd b be z dk translated">可爱的狗和猫[1]</p></figure><p id="07b3" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如果手头的问题是一个分类问题，交叉熵损失是一个流行的选择，并且它本身可以被分类为类别交叉熵或多类交叉熵(二元交叉熵是前者的特例)。)如果你对这些有多大的不同感到困惑，我会在深入研究它们的起源之前尝试介绍每一个。</p><p id="3190" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">让我们从分类交叉熵开始。对于这个损失函数，我们的<em class="lr"> y </em>被一键编码以表示我们的图像(或任何东西)所属的类别。因此，对于任何<em class="lr"> x，</em> y的长度等于类的数量，并且我们的模型中的最后一层对于每个类有一个神经元。我们在最后一层使用Softmax来得到<em class="lr"> x </em>属于每个类的概率。这些概率总和为1。</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi ls"><img src="../Images/cd4f9fe112790d7d665dacbfb51bf71a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ScIle51yTbf3YcBjmEkiQw.png"/></div></div><p class="ln lo gj gh gi lp lq bd b be z dk translated">范畴交叉熵举了一个例子。<em class="lt"> aᴴ </em> ₘ是最后一层(h)的第<em class="lt">个</em>个神经元</p></figure><p id="ddd0" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们将这个故事作为一个检查点。在那里，我们考虑了二次损失，最后得到了下面的等式。</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi lv"><img src="../Images/6465dfb96498637e4d767b71fb2131f3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xsXYXjCpFwM4SxtA0L5y5Q.png"/></div></div><p class="ln lo gj gh gi lp lq bd b be z dk translated">L=0是第一个隐藏层，L=H是最后一层。δ是∂J/∂z</p></figure><p id="757e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">注意，最后一层的输出(激活向量)是<em class="lr"> aᴴ </em>，在索引符号中，我们将写<em class="lr"> aᴴₙ </em>来表示最后一层中的第<em class="lr">个</em>神经元。这同样适用于预激活向量z <em class="lr"> ᴴ.</em></p><p id="b8a6" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在上面的系统中，我们期望改变的唯一方程是δ <em class="lr"> ᴴ </em>的方程，因为我们使用了损失函数的显式公式来找到它。因此，我们将只处理推导过程中的最后一层<strong class="kh ir"/>，所以我们现在也可以去掉上标，并且记住，每当我们写<em class="lr"> a </em>、<em class="lr"> z </em>或<em class="lr"> δ </em>时，我们都是针对最后一层的。</p><p id="e2b9" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">由此，损失函数为</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi lv"><img src="../Images/3a3590688301fb330e92e3228e48dcec.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PIuEfRWOqoVg6E1N1xB5uQ.png"/></div></div><p class="ln lo gj gh gi lp lq bd b be z dk translated">范畴交叉熵。</p></figure><p id="fc0f" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">随着最后一层中第<em class="lr">个第n</em>个神经元的激活</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi lv"><img src="../Images/c9f4f84089585a03cc904f132c2911d4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*J4W8qLWtjmneMcWJ0yKPUw.png"/></div></div><p class="ln lo gj gh gi lp lq bd b be z dk translated">Softmax激活。我们将在下面多次使用它。牢记在心。</p></figure><p id="b4f4" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">请注意，第<em class="lr">个</em>神经元的激活取决于该层中所有其他神经元的预激活。如果最后一层涉及Sigmoid或ReLU激活，情况就不会如此。由于这个原因，为了找到最后一层中某个神经元的δ，我们使用链式法则</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi lv"><img src="../Images/0fcd00bd7333203c65d32d51aaf9ef57.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*b-o6zeci_THRCsTQ3MVNPQ.png"/></div></div><p class="ln lo gj gh gi lp lq bd b be z dk translated">等式1.0</p></figure><p id="2d51" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如果我们把它写成</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi lv"><img src="../Images/6b053ac6ccc48d634568ca5988068c06.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PvQ0t7Cb-03DAE44n1-cDg.png"/></div></div><p class="ln lo gj gh gi lp lq bd b be z dk translated">考虑m=n且m≠n，然后相加</p></figure><p id="6499" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">从∂J/∂开始<em class="lr">我们可以写一个</em> ₙ</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi lv"><img src="../Images/56f7bbba74e09b6d0481bc7f708b8d4e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GMkOr1bTWU0N4l7BpMbCFQ.png"/></div></div><p class="ln lo gj gh gi lp lq bd b be z dk translated">第n个神经元是∂ <em class="lt">和ₙ</em>求和的唯一幸存者</p></figure><p id="0c60" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">对于∂，我们有一个ₙ/∂zₙ</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi lv"><img src="../Images/483f9cdf292efabb0a7ac2454011fa95.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4ZD7uw8EYeSPx58mvySq3Q.png"/></div></div></figure><p id="ad59" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">你可以用商法则得到导数，但是还有其他方法<a class="ae lu" href="https://essamamin99.medium.com/differentiation-revisiting-the-product-rule-112b8b0bf7b6" rel="noopener">值得一试。</a></p><p id="980e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在把我们的结果相乘，代入原来的方程，我们得到</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi lv"><img src="../Images/3b82429618b18f3bfa1d6ab87dea6f8a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yafv01cpQj8VJW-HgIDryQ.png"/></div></div><p class="ln lo gj gh gi lp lq bd b be z dk translated">通过插入等式。一</p></figure><p id="11a2" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">对于剩余的总和，让我们首先通过写来计算∂J/∂ <em class="lr"> a </em> ₘ</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi lv"><img src="../Images/a38533a63fadb6b58e5446418b55a8d9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UfcDiPld6vy3eaVN3aCcLw.png"/></div></div><p class="ln lo gj gh gi lp lq bd b be z dk translated">如果一个公式包含所有内容的总和，你可以随时改变指数以避免混淆</p></figure><p id="0a07" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">然后对于∂ <em class="lr">一个</em> ₘ/∂zₙ ( <em class="lr"> m≠n </em>)我们有</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi lw"><img src="../Images/57c6c22dcc84fc6e9525f9334543a155.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lotrTYor1z1sbF08E2oXWg.png"/></div></div><p class="ln lo gj gh gi lp lq bd b be z dk translated">你也可以使用商法则得到同样的结果。</p></figure><p id="69f6" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在将两个结果相乘，我们得到</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi lx"><img src="../Images/81ffcf5e2dd6ee4ab796108672745eb6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LQnUG20677GO5VYd9gl09g.png"/></div></div></figure><p id="b719" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">并将其传播回我们得到的原始方程</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi lx"><img src="../Images/2225d84a81d5be7bd2dabed2b4857f34.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iST8c1C7QtEaLBVdXqL36Q.png"/></div></div></figure><p id="40c5" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这可以简化为</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi ly"><img src="../Images/80ee1c5d4f5c2c4c1560f212a2af2e66.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qNa6IMsmMHQ0pFSL9-VVUg.png"/></div></div></figure><p id="7fb2" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">假设y是一个热矢量，我们知道σₘyₘ=1；因此，我们可以写</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi lz"><img src="../Images/95821568918706b89c35248bb44fc09f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KBOfyqg9wcwC_muPbnNayQ.png"/></div></div></figure><p id="2df7" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">或者在重新加上上标(H表示最后一层)后以矢量形式显示</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi lz"><img src="../Images/3159596c2c9092c20403a2ce07faf9c0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*o3V7TYNzLqnr357W2Hn_SA.png"/></div></div></figure><p id="48c6" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">为了结束证明，让我们把反向传播方程更新为</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi ls"><img src="../Images/57e59ed4f0712e21a5d7f1815237f31b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0qti5h6-SI958H7R1vq-3A.png"/></div></div><p class="ln lo gj gh gi lp lq bd b be z dk translated">只有红色的变了。</p></figure><p id="a88c" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们还没有完全做到。使用分类交叉熵，你的模型可以很好地将下面的图片分类为一只狗；但是，您的数据集可能包含许多既有猫又有狗的图像。在这种情况下，如果图像中存在两个类，<em class="lr"> y </em>不再是一个热点向量(而是看起来类似于[0 1 0 0 1]。)于是，我们使用多类交叉熵，在最后一层避免使用Softmax相反，我们使用乙状结肠。如果一个类的Sigmoid激活大于某个阈值(例如0.5)，则我们推断该类存在于图像中。)</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi ma"><img src="../Images/570dbcbdd7ab36c566166c44d8fdbf90.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2MFA75PTQwIknS7rGFNy7A.jpeg"/></div></div><p class="ln lo gj gh gi lp lq bd b be z dk translated">快乐的狗[2]</p></figure><p id="2a0c" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">一个例子的多类交叉熵损失函数由下式给出</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi ls"><img src="../Images/f6d43ec6323e7911a17e0d50dcf3f02d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jt2zx9ca9_iBMJEMbBB7iQ.png"/></div></div><p class="ln lo gj gh gi lp lq bd b be z dk translated"><em class="lt"> aᴴ </em> ₘ是最后一层(h)中的第<em class="lt">个</em>神经元</p></figure><p id="d3a9" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如果我们回到去掉上标，我们可以写</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi ls"><img src="../Images/3bc1f56e57d6a6efd8ec8d2fa4fc7301.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3nphjSLACxRuL9J3_2eFmQ.png"/></div></div></figure><p id="cde7" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">因为我们使用乙状结肠，我们也有</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi ls"><img src="../Images/c6f3298e71f4310b5c80cde367d34264.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ec7vWrcgPUmN8KlcxWQb5w.png"/></div></div></figure><p id="0ade" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">不像Softmax <em class="lr"> a </em> ₙ只是zₙ的一个功能；因此，为了找到最后一层的<em class="lr"> δ </em>，我们需要考虑的是</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi ls"><img src="../Images/bc0b95d5bde06fda3f4da4fd32c43b22.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*da9jfKjKmk4CvkDHH-iKGA.png"/></div></div><p class="ln lo gj gh gi lp lq bd b be z dk translated">情商。2</p></figure><p id="7b22" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">或者更准确地说</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi mb"><img src="../Images/e96debaed7c44b5ac052539d8287782c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jbeK7BTe3nfHesPhPVx4ZA.png"/></div></div></figure><p id="3cf6" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">哪个是</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi mb"><img src="../Images/1e68b3853267a6687e70fe48725249c4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uEih-NGSDGVesEMpHueblA.png"/></div></div></figure><p id="7230" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">通过微分(就像我们之前做的那样)然后把我们得到的两个分数相加</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi mb"><img src="../Images/b36f8b226194fc32774823cccbeac499.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YJ5jWTero5UEhNF8vi5uMA.png"/></div></div></figure><p id="a081" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在我们需要在代入方程2.0之前考虑∂是ₙ/∂zₙ</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi mb"><img src="../Images/80d2d771b6de8fae931a9b0417afccd5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0hX0kMKvmu0v-NL413kcKQ.png"/></div></div></figure><p id="8a81" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这只是Sigmoid函数的导数</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi mc"><img src="../Images/e9a807a2c8df09323305ff73457cc0ce.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jmP16ccmoMnhDAR_a2Z8Kw.png"/></div></div></figure><p id="ac9a" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">用ₙ代替它的定义，我们得到</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi mc"><img src="../Images/acbe41ab441686d24c604de87760e6cb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*08kRt44xFdqi4d4hYCEtog.png"/></div></div><p class="ln lo gj gh gi lp lq bd b be z dk translated">你可能已经知道了。</p></figure><p id="bdc5" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在使用原始方程中的两个结果</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi mc"><img src="../Images/0e32d109bffe15db38303c1f7764336c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iH9kcOvSE1EbaezlbmhJQA.png"/></div></div></figure><p id="86a8" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">因此，</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi mc"><img src="../Images/6edc6fb36e3369f7336457e0e761c90c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pXSM2RiSz-PPmzBYvP0sqQ.png"/></div></div></figure><p id="8795" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在重新加上上标，用向量的形式写出来</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi mc"><img src="../Images/f975e4acd316784a808ce55a3e929817.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3XBE4OSLn7OuRJCZFlmSaw.png"/></div></div></figure><p id="3da1" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这与我们用Softmax作为激活的范畴交叉熵得到的结果非常相似。所以我们仍然使用</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi ls"><img src="../Images/57e59ed4f0712e21a5d7f1815237f31b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0qti5h6-SI958H7R1vq-3A.png"/></div></div></figure><p id="2cb6" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">作为我们的反向传播方程。</p><p id="26e3" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如果你喜欢阅读，并希望看到更多这样的故事，那么请考虑给帖子一些掌声，并跟我来。下次见，再见。</p><p id="5854" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">参考文献:</strong></p><p id="dd36" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">[1] <em class="lr"> Pixabay </em>，2021，<a class="ae lu" href="https://pixabay.com/de/photos/haustiere-niedlich-katze-hund-3715733/." rel="noopener ugc nofollow" target="_blank">https://pix abay . com/de/photos/haustiere-nied lich-katze-hund-3715733/。【2021年10月2日访问。</a></p><p id="8687" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">[2]洛罗伊，波林。“照片由Pauline Loroy在Unsplash上拍摄”。<em class="lr">Unsplash.Com</em>2021年<a class="ae lu" href="https://unsplash.com/photos/U3aF7hgUSrk." rel="noopener ugc nofollow" target="_blank">https://unsplash.com/photos/U3aF7hgUSrk.</a>2021年10月2日访问。</p></div></div>    
</body>
</html>