<html>
<head>
<title>PySpark Neural Network from Scratch</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">PySpark神经网络从零开始</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/pyspark-neural-network-from-scratch-8a19ebad3904?source=collection_archive---------8-----------------------#2021-09-06">https://towardsdatascience.com/pyspark-neural-network-from-scratch-8a19ebad3904?source=collection_archive---------8-----------------------#2021-09-06</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="49d1" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsdatascience.com/tagged/hands-on-tutorials" rel="noopener" target="_blank">实践教程</a></h2><div class=""/><div class=""><h2 id="4618" class="pw-subtitle-paragraph jw iz iq bd b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn dk translated">一个简单的教程，学习如何使用PySpark实现一个浅层神经网络(3个完全连接的层)。</h2></div><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ko"><img src="../Images/ff0082853be05aeae18d4a4099a162cf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*NcHCTDr4LT-ED37u"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">照片由<a class="ae le" href="https://unsplash.com/@jamie452?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">杰米街</a>在<a class="ae le" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>拍摄</p></figure><h1 id="aafe" class="lf lg iq bd lh li lj lk ll lm ln lo lp kf lq kg lr ki ls kj lt kl lu km lv lw bi translated">序</h1><p id="edf6" class="pw-post-body-paragraph lx ly iq lz b ma mb ka mc md me kd mf mg mh mi mj mk ml mm mn mo mp mq mr ms ij bi translated">本文不打算提供神经网络的数学解释，而只是解释如何使用Python中的Spark (MapReduce)逻辑应用数学方程来运行它。为了简单起见，这个实现只使用rdd(没有数据帧)。</p><p id="e760" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated">同样，我假设您的机器上安装了Spark，您可以运行spark-submit或PySpark Jupyter-Notebook。</p><p id="d711" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated">本教程中提供的所有代码都可以在这个<a class="ae le" href="https://github.com/MarvinMartin24/PySpark-Neural-Network" rel="noopener ugc nofollow" target="_blank"> GitHub </a>资源库中获得。</p><div class="my mz gp gr na nb"><a href="https://github.com/MarvinMartin24/PySpark-Neural-Network" rel="noopener  ugc nofollow" target="_blank"><div class="nc ab fo"><div class="nd ab ne cl cj nf"><h2 class="bd ja gy z fp ng fr fs nh fu fw iz bi translated">GitHub-marvinmartin 24/py spark-Neural-Network:py spark实现浅层神经网络…</h2><div class="ni l"><h3 class="bd b gy z fp ng fr fs nh fu fw dk translated">Pyspark实现浅层神经网络从零开始使用MNIST这个项目的目标是使用火花…</h3></div><div class="nj l"><p class="bd b dl z fp ng fr fs nh fu fw dk translated">github.com</p></div></div><div class="nk l"><div class="nl l nm nn no nk np ky nb"/></div></div></a></div><p id="f93f" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated">为了以防万一，这里有一些资源可以设置您的机器来运行代码:</p><ul class=""><li id="2f17" class="nq nr iq lz b ma mt md mu mg ns mk nt mo nu ms nv nw nx ny bi translated">在Mac上用Jupyter笔记本安装<a class="ae le" href="https://medium.com/@roshinijohri/spark-with-jupyter-notebook-on-macos-2-0-0-and-higher-c61b971b5007" rel="noopener">Spark</a></li><li id="e67d" class="nq nr iq lz b ma nz md oa mg ob mk oc mo od ms nv nw nx ny bi translated">在Linux上用Jupyter笔记本安装<a class="ae le" href="https://sicara.ai/blog/2017-05-02-get-started-pyspark-jupyter-notebook-3-minutes" rel="noopener ugc nofollow" target="_blank">Spark</a></li><li id="e590" class="nq nr iq lz b ma nz md oa mg ob mk oc mo od ms nv nw nx ny bi translated">在Windows上安装<a class="ae le" href="https://naomi-fridman.medium.com/install-pyspark-to-run-on-jupyter-notebook-on-windows-4ec2009de21f" rel="noopener">Spark with Jupyter Notebook</a></li></ul><p id="6f8f" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated">此外，在整篇文章中，我将根据我以前的一篇文章来解释三层神经网络背后的数学原理。我将提供的大多数数学公式都是在这里摘录和讨论的:</p><div class="my mz gp gr na nb"><a href="https://medium.com/swlh/mathematics-behind-basic-feed-forward-neural-network-3-layers-python-from-scratch-df88085c8049" rel="noopener follow" target="_blank"><div class="nc ab fo"><div class="nd ab ne cl cj nf"><h2 class="bd ja gy z fp ng fr fs nh fu fw iz bi translated">基本前馈神经网络(3层)+ Python从头开始背后的数学</h2><div class="ni l"><h3 class="bd b gy z fp ng fr fs nh fu fw dk translated">让我们看看基本FFNN背后的数学原理。该架构固定为3层网络(输入层+隐藏层…</h3></div><div class="nj l"><p class="bd b dl z fp ng fr fs nh fu fw dk translated">medium.com</p></div></div><div class="nk l"><div class="oe l nm nn no nk np ky nb"/></div></div></a></div><h1 id="5c85" class="lf lg iq bd lh li lj lk ll lm ln lo lp kf lq kg lr ki ls kj lt kl lu km lv lw bi translated">为什么是火花？(而不是Tensorflow或Pytorch)</h1><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi of"><img src="../Images/6a66eb4196882d934e52d95a00f416ef.png" data-original-src="https://miro.medium.com/v2/resize:fit:564/format:webp/1*zGqcnnbk0r38hGoXjsqFPQ.png"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">作者在imgflip.com创建的Gif</p></figure><p id="d3ca" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated">如果你已经熟悉深度学习，你可能已经遇到了GPU/CPU内存限制。当您试图提供太多资源无法处理的输入数据(通过大批量或大输入要素空间)时，通常会出现这种情况。</p><p id="38c6" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated">因此，根据前面的陈述，向您的模型提供几百千兆字节的输入几乎是不可能的。这对于主要使用CPU计算的机器学习算法(线性回归、SVM、逻辑回归、朴素贝叶斯等等)来说更是如此。</p><p id="0458" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated">Spark是处理大量数据的强大解决方案。它允许在计算机网络(通常称为集群)上分布计算。Spark有助于在一个循环中多次分析一组数据的迭代算法的实现。Spark广泛应用于机器学习项目。</p><p id="4a8f" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated">您可能已经知道，TensorFlow或Pytorch等著名的库一般用于构建神经网络。使用这些库的好处之一是GPU计算，它通过允许并行计算来加速训练。</p><p id="ab49" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated">Spark的最新版本也允许使用GPU，但在本文中，我们将只关注CPU计算(像神经网络的大多数scratch实现一样)以保持简单。本文提出了一个用于学习目的的实现，它不适合工业需求。</p><h1 id="727d" class="lf lg iq bd lh li lj lk ll lm ln lo lp kf lq kg lr ki ls kj lt kl lu km lv lw bi translated">分类任务</h1><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi og"><img src="../Images/86cea3fb38a20a777c2070672f8e222f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1224/format:webp/1*CUBcMsq746wNqZPUj-8ssw.png"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">MNIST数据集样本，<em class="oh">作者图片</em></p></figure><p id="2940" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated">对于本教程，我们将尝试解决众所周知的MNIST手写数字分类任务。MNIST数据集有60，000个样本的训练集和10，000个样本的测试集。</p><p id="6832" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated">每个图像都是手写数字(从0到9)，灰度为28x28。我们Spark神经网络的目标是检测图像中的数字。</p><p id="3f43" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated">由于使用浅层全连接神经网络进行图像分类是一项相当困难的任务，因此我们将仅尝试对0和1的图像进行分类(以获得二元分类器)。在仅选择0和1图像之后，我们获得12665个训练样本和2115个测试样本。</p><h1 id="8d62" class="lf lg iq bd lh li lj lk ll lm ln lo lp kf lq kg lr ki ls kj lt kl lu km lv lw bi translated">模型架构</h1><p id="1b87" class="pw-post-body-paragraph lx ly iq lz b ma mb ka mc md me kd mf mg mh mi mj mk ml mm mn mo mp mq mr ms ij bi translated">在这一节，我们将定义我们的神经网络的架构(例如，隐藏层的数量，每层神经元的数量，激活函数，损失函数)。为了避免复杂的数学或复杂的模块实现，本文提出了一个3层神经网络(只有1个隐藏层)。</p><p id="cd0c" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated">该模型可以被看作如下:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi oi"><img src="../Images/44863ad0c876e9ebdcf1253251085ef8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Pf2Qt7qBIs5q7FE5jJrP_Q.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated"><em class="oh">作者图片</em></p></figure><p id="4cb4" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated">其中<strong class="lz ja"> X </strong>为输入层(有I个神经元)，<strong class="lz ja"> H </strong>为隐藏层(有j个神经元)，<strong class="lz ja"> Y </strong>为输出层(有k个神经元)。这个模型有4个参数:<strong class="lz ja"> W(1)，W(2)，B(1)，B(2) </strong>。</p><p id="be04" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated">当我们使用MNIST图像作为输入数据时，我们有<strong class="lz ja"> i =784 </strong> (28x28)，那么j隐藏神经元的数量是任意选择的，这里我们可以使用<strong class="lz ja"> j=64 </strong>，最后输出神经元的数量<strong class="lz ja"> k = 2 </strong>，因为我们有2个可能的数字要预测(0和1)。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/3e45d0792c0a9156e8f8063bf382a9db.png" data-original-src="https://miro.medium.com/v2/resize:fit:544/format:webp/1*KdMwLjzPJ7WOlSMxzl2CXQ.png"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">乙状结肠激活功能，<em class="oh">作者图片</em></p></figure><p id="3662" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated">为了激活隐藏和输出神经元，我们将使用sigmoid激活函数。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi ok"><img src="../Images/b1f248709914d1ef38a08e9ab233cee9.png" data-original-src="https://miro.medium.com/v2/resize:fit:684/format:webp/1*PQl_w-9GRpHjzI13xRulZg.png"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">MSE损失函数，<em class="oh">作者图片</em></p></figure><p id="6469" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated">最后，为了计算一批训练数据的训练误差，我们将使用一个称为均方误差(MSE)的损失函数。</p><h1 id="0ea7" class="lf lg iq bd lh li lj lk ll lm ln lo lp kf lq kg lr ki ls kj lt kl lu km lv lw bi translated">数学</h1><p id="6d97" class="pw-post-body-paragraph lx ly iq lz b ma mb ka mc md me kd mf mg mh mi mj mk ml mm mn mo mp mq mr ms ij bi translated">如前所述，本文不打算解释神经网络背后的数学，但这里有一个向前和向后传播公式的概述。更多详情，请看我之前在第一节提到的<a class="ae le" href="https://medium.com/swlh/mathematics-behind-basic-feed-forward-neural-network-3-layers-python-from-scratch-df88085c8049" rel="noopener">文章</a>。</p><div class="kp kq kr ks gt ab cb"><figure class="ol kt om on oo op oq paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><img src="../Images/74f049998975a64f8dcbbb871bab2f62.png" data-original-src="https://miro.medium.com/v2/resize:fit:1108/format:webp/1*bUq_GTZQqoes1YPbv2kWhw.png"/></div></figure><figure class="ol kt or on oo op oq paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><img src="../Images/9e12abaac607bc205ca98b057becffaa.png" data-original-src="https://miro.medium.com/v2/resize:fit:894/format:webp/1*8VTiPdwO-6GvKkuE7bv8hA.png"/></div><p class="la lb gj gh gi lc ld bd b be z dk os di ot ou translated">前向(左)和后向(右)传播方程，<em class="oh">作者图片</em></p></figure></div><p id="15e1" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated"><strong class="lz ja">符号</strong>:</p><ul class=""><li id="57ee" class="nq nr iq lz b ma mt md mu mg ns mk nt mo nu ms nv nw nx ny bi translated">e是使用MSE对一批训练数据计算的误差。</li><li id="0cc9" class="nq nr iq lz b ma nz md oa mg ob mk oc mo od ms nv nw nx ny bi translated">f和f’分别是激活函数和激活函数的导数(sigmoid和sigmoid素数)。</li><li id="7e92" class="nq nr iq lz b ma nz md oa mg ob mk oc mo od ms nv nw nx ny bi translated">H^和h分别是激活前后的隐藏神经元。</li><li id="f346" class="nq nr iq lz b ma nz md oa mg ob mk oc mo od ms nv nw nx ny bi translated">Y^和y分别是激活前后的输出神经元。</li><li id="64e1" class="nq nr iq lz b ma nz md oa mg ob mk oc mo od ms nv nw nx ny bi translated">y是模型的预测值</li><li id="e3d7" class="nq nr iq lz b ma nz md oa mg ob mk oc mo od ms nv nw nx ny bi translated">Y*是标签(期望的输出)</li></ul><p id="cb1f" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated">在训练期间，我们使用<strong class="lz ja">小型批量梯度下降</strong>更新模型参数W(1)、W(2)、B(1)、B(2)，使用以下等式:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ov"><img src="../Images/d3d7ea541e607918bd1a078847dc1359.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*v7-MXJTMwEWcwas3i6gpSA.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated"><em class="oh">作者图片</em></p></figure><p id="5a91" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated">Alpha是学习率(我们将使用0.1)，在代码实现中，我们通过训练数据批次的大小来标准化梯度。</p><p id="f901" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated"><em class="ow">注意:与库不同，本文的实现不允许在不改变数学公式或实现的情况下添加额外的层。</em></p></div><div class="ab cl ox oy hu oz" role="separator"><span class="pa bw bk pb pc pd"/><span class="pa bw bk pb pc pd"/><span class="pa bw bk pb pc"/></div><div class="ij ik il im in"><h1 id="5b5f" class="lf lg iq bd lh li pe lk ll lm pf lo lp kf pg kg lr ki ph kj lt kl pi km lv lw bi translated">基于RDDs的神经网络训练</h1><p id="5691" class="pw-post-body-paragraph lx ly iq lz b ma mb ka mc md me kd mf mg mh mi mj mk ml mm mn mo mp mq mr ms ij bi translated">以通常的Python方式只使用Numpy来应用下面的等式是非常简单的，它只需要Numpy矢量化运算，但是当涉及到将计算分布到几台机器时，我们就不能再使用这种逻辑了。</p><h2 id="7652" class="pj lg iq bd lh pk pl dn ll pm pn dp lp mg po pp lr mk pq pr lt mo ps pt lv iw bi translated">弹性分布式数据集(RDD)</h2><p id="d4ea" class="pw-post-body-paragraph lx ly iq lz b ma mb ka mc md me kd mf mg mh mi mj mk ml mm mn mo mp mq mr ms ij bi translated">使用Spark的数据集使用<strong class="lz ja"> RDDs </strong>(弹性分布式数据集)存储。RDD是跨集群节点划分和分布的元素“集合”。借助RDD，Spark可以执行迭代和交互式任务，同时保持集群的可伸缩性和容错能力。</p><p id="16e3" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated">我们的MNIST数据集通常会作为rdd加载(用于训练集和测试集)。rdd是使用键值对来构造的。因此，在我们的实现中，我们可以将<strong class="lz ja"> train_rdd </strong>和<strong class="lz ja"> test_rdd </strong>定义如下:</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="pu pv l"/></div></figure><h2 id="ceef" class="pj lg iq bd lh pk pl dn ll pm pn dp lp mg po pp lr mk pq pr lt mo ps pt lv iw bi translated">MapReduce</h2><p id="3fd1" class="pw-post-body-paragraph lx ly iq lz b ma mb ka mc md me kd mf mg mh mi mj mk ml mm mn mo mp mq mr ms ij bi translated">这些rdd使用两个主要操作来处理:<strong class="lz ja">映射</strong>和<strong class="lz ja">归约</strong>操作。</p><p id="0783" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated"><strong class="lz ja"> →地图操作</strong></p><p id="d343" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated"><strong class="lz ja">映射</strong>操作将分批分解计算。群集的驱动程序会将这些批处理中的每一个发送到群集中的不同计算机(取决于群集的资源和配置)。通过这种方式，群集中的每台计算机都能够计算分配给它的数据块的向前和向后传播。</p><p id="129e" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated">通过查看数学部分的公式，我们可以看到，要计算给定输入的向后传播，我们需要一些向前传播的数据。</p><p id="306c" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated">通过这种实现，我们在等式中观察到的链式法则可以通过连续执行映射操作来实现:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi pw"><img src="../Images/9d7e30c6a64f4d93fc642d762cd896eb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6cgF4TaPyo4XtxN37rrS5Q.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated"><em class="oh">作者图片</em></p></figure><p id="5e85" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated">在上图中，每个箭头代表一个映射转换。这些映射操作是对来自集群中不同计算机的每个输入图像并行执行的。这意味着，如果我们想要增加我们的训练集，我们只需要扩大群集中的资源，就能够计算所有训练图像的梯度。</p><p id="8c99" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated">为了阐明这一步，下面是另一个提供更多粒度的图表:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi px"><img src="../Images/e087d74f494cfcce9c6dea90abf1d499.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0fbL-cguJF7rHeB5r2kl6g.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated"><em class="oh">作者图片</em></p></figure><p id="f042" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated">我们可以看到，我们的原始元组<strong class="lz ja">(图像，标签)</strong>一次又一次地进化映射操作，以最终获得成本、精度以及相对于每个单个训练图像的参数的误差梯度。</p><p id="b6d4" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated">注意<strong class="lz ja"> E </strong>是使用MSE计算的单个输入图像的误差。同样地，<strong class="lz ja"> Acc </strong>(精度)在这个阶段是表示对同一单个图像的预测成功的布尔值(0表示错误，1表示正确预测)。</p><p id="e8a3" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated"><strong class="lz ja"> →减少操作</strong></p><p id="78b2" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated">一旦在聚类上计算了前向和后向传播，就应该对结果进行汇总，以获得平均成本、平均精度以及成本在参数上的平均梯度。<br/>这种聚合在训练数据集上执行，并产生如下定义的单个输出元组:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi py"><img src="../Images/75c729613ad3d20332fa47c8e3962d0c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*APCGjqPaGBGCdXExRuQ1Cg.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated"><em class="oh">作者图片</em></p></figure><p id="5264" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated">使用Spark，这种聚合可以通过使用应用于最终RDD(由贴图产生)的不同方法来实现。在本文提出的实现中，我们使用了<strong class="lz ja">。reduce() </strong>分别对所有元组的每个元素求和。其他方法比如。聚合()或。treeAggregate()也广泛用于这种操作(事实上，它们甚至可以比简单的reduce方法更加优化)。</p><p id="25f9" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated">最后一个“归约”操作(被认为是RDD操作)生成一个包含6个项目的python列表(成本、B2的梯度、W2的梯度、B1的梯度、W1的梯度、精度)。所得到的平均梯度可以直接用于使用梯度下降来更新参数，并且可以在训练结束时存储成本和精度用于可视化目的。</p></div><div class="ab cl ox oy hu oz" role="separator"><span class="pa bw bk pb pc pd"/><span class="pa bw bk pb pc pd"/><span class="pa bw bk pb pc"/></div><div class="ij ik il im in"><h1 id="772f" class="lf lg iq bd lh li pe lk ll lm pf lo lp kf pg kg lr ki ph kj lt kl pi km lv lw bi translated"><strong class="ak">代码示例</strong></h1><p id="5abc" class="pw-post-body-paragraph lx ly iq lz b ma mb ka mc md me kd mf mg mh mi mj mk ml mm mn mo mp mq mr ms ij bi translated">本节提供了说明上述理论解释的代码。大多数数学公式都嵌入在函数中，以使代码更容易阅读。</p><p id="4cf5" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated">算法的核心位于单个FOR循环内部(负责历元的迭代)。我们可以看到8个<strong class="lz ja">映射</strong>操作，随后是单个<strong class="lz ja">减少</strong>操作<strong class="lz ja"> </strong>(第95行到第105行)，允许计算小批量梯度，从而计算梯度下降。</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="pu pv l"/></div></figure><p id="bda8" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated">请注意，小批量是使用<strong class="lz ja">创建的。样品()</strong>法。该方法返回<strong class="lz ja"> RDD </strong> (train_rdd)的采样子集，<strong class="lz ja"> False </strong>参数表示给定子集不会被多次采样，<strong class="lz ja"> 0.7 </strong>表示整个训练集的70%被随机采样以构成小批量(70%相当大，意味着梯度几乎是在训练集上计算的，例如，全批量梯度下降)。训练集越大，这个比率应该越低。</p></div><div class="ab cl ox oy hu oz" role="separator"><span class="pa bw bk pb pc pd"/><span class="pa bw bk pb pc pd"/><span class="pa bw bk pb pc"/></div><div class="ij ik il im in"><h1 id="80ac" class="lf lg iq bd lh li pe lk ll lm pf lo lp kf pg kg lr ki ph kj lt kl pi km lv lw bi translated">培训结果</h1><p id="e6a7" class="pw-post-body-paragraph lx ly iq lz b ma mb ka mc md me kd mf mg mh mi mj mk ml mm mn mo mp mq mr ms ij bi translated">运行上面这段代码后，我们得到了有希望的结果:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi pz"><img src="../Images/9cb3803ead8a836b7135327fcb0291a9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OoB6iKR0m6VtrfVz9bd05w.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated"><em class="oh">作者图片</em></p></figure><p id="0ecb" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated">正如您在上面的图中看到的，我们的成本缓慢但稳定地下降，而准确性在训练数据集的各个时期成比例地增加。</p><p id="568a" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated">是时候在测试数据集(存储在<strong class="lz ja"> test_rdd </strong>中)上测试我们的模型了。要做到这一点，非常简单，因为我们现在有了参数W(1)，W(2)，B(1)，B(2)。代码如下所示:</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="pu pv l"/></div></figure><p id="9fe6" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated">在第7行，使用<strong class="lz ja">。map，</strong>我们简单地将运行前向传播的predict()函数应用于所有测试示例，并使用<strong class="lz ja">。减少</strong>我们汇总指标(真阳性、假阳性、假阴性、真阴性)。</p><p id="1aaf" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated">我们获得了以下指标:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi qa"><img src="../Images/4797c038ef362d6494a0b19be40f0ae6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OpeqGIszf2HJnqLj21RnMQ.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated"><em class="oh">作者图片</em></p></figure><p id="21f5" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated">都在90%以上，说明我们成功训练了我们的模型！</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="qb pv l"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">来自GIPHY捕获的Gif</p></figure><h1 id="7920" class="lf lg iq bd lh li lj lk ll lm ln lo lp kf lq kg lr ki ls kj lt kl lu km lv lw bi translated">丰富</h1><p id="8c43" class="pw-post-body-paragraph lx ly iq lz b ma mb ka mc md me kd mf mg mh mi mj mk ml mm mn mo mp mq mr ms ij bi translated">这个实现是为了学习，熟悉PySpark和MapReduce。所以很多方面都可以改进。</p><p id="1cfd" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated"><strong class="lz ja">可能的Spark改进:<br/> </strong> →使用分区，使用mapPartitions或mapPartitionWithIndex在小批量上运行计算。这种做法会少一些不恰当的洗牌(主要是由于。样本)。<br/> →使用树聚合在分区上进行更优化的聚合(小批量)<br/> →减少映射操作的数量(因为在底层，当一个动作被触发时，映射操作被合并)。</p><p id="8b92" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated"><strong class="lz ja">可能的神经网络改进:<br/> </strong> →使用更模块化的方法更容易地添加层(创建类似PyTorch或Keras的顺序模型)。<br/> →使用更先进的技术(正则化技术、用于图像分类的CNN、更好的优化器等等)。</p></div><div class="ab cl ox oy hu oz" role="separator"><span class="pa bw bk pb pc pd"/><span class="pa bw bk pb pc pd"/><span class="pa bw bk pb pc"/></div><div class="ij ik il im in"><h1 id="e2ca" class="lf lg iq bd lh li pe lk ll lm pf lo lp kf pg kg lr ki ph kj lt kl pi km lv lw bi translated">结论</h1><p id="2710" class="pw-post-body-paragraph lx ly iq lz b ma mb ka mc md me kd mf mg mh mi mj mk ml mm mn mo mp mq mr ms ij bi translated">神经网络本身可能很难理解，即使使用简单的python。能够将其扩展到像Spark这样更具可扩展性的系统是一个伟大的项目，可以帮助更好地理解其背后的复杂概念。</p><p id="7b70" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated">了解这些技术可能面临的限制也很重要，尤其是在扩展方面。</p></div><div class="ab cl ox oy hu oz" role="separator"><span class="pa bw bk pb pc pd"/><span class="pa bw bk pb pc pd"/><span class="pa bw bk pb pc"/></div><div class="ij ik il im in"><p id="cca6" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated">请随时在评论区与我分享您的反馈和想法💻🔥💻。</p><p id="7038" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated">谢谢大家！</p></div></div>    
</body>
</html>