<html>
<head>
<title>CIFAR 100: Transfer Learning using EfficientNet</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">CIFAR 100:使用EfficientNet进行迁移学习</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/cifar-100-transfer-learning-using-efficientnet-ed3ed7b89af2?source=collection_archive---------3-----------------------#2021-03-30">https://towardsdatascience.com/cifar-100-transfer-learning-using-efficientnet-ed3ed7b89af2?source=collection_archive---------3-----------------------#2021-03-30</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="be3c" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">使用最先进的EfficientNet-B0进行迁移学习</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/97b2167db4dcedd5971ae55dc87eca55.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_hI9siTZ5x1jLRiuPeSSqw.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">玛丽娜·维塔莱在<a class="ae kv" href="https://unsplash.com/photos/t809JJ6r9KA" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="82e5" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">卷积神经网络</strong> (CNN)是一类常用于分析图像的深度神经网络。在本文中，我们将一起构建一个CNN模型，该模型可以正确识别物体的彩色图像并将其分类到CIFAR-100数据集的100个可用类中的一个。特别是，我们将重用一个最先进的技术作为我们模型的起点。这种技术被称为迁移学习。➡️</p><p id="ea34" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们先来了解一下什么是迁移学习。我不会讲太多细节，但会尝试分享一些知识。📝</p><h1 id="f72c" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated"><strong class="ak">迁移学习</strong></h1><p id="d761" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated"><em class="mp">如《机器学习应用研究手册》所述，</em> <strong class="ky ir"> <em class="mp">迁移学习</em> </strong> <em class="mp">是通过从一个已经学习过的相关任务中迁移知识，在一个新任务中对学习的提高。</em></p><p id="75e9" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">简单来说，迁移学习<strong class="ky ir"> </strong>是一种机器学习技术，在一个任务上训练的模型被重新用于第二个相关的任务。深度学习网络是资源饥渴的，并且具有数百万个参数，计算成本很高。这些网络用大量数据进行训练，以避免过度拟合。因此，当一个最先进的模型被创造出来时，通常需要研究人员花费大量的时间进行培训。由于一个最先进的模型是在花费如此巨大的资源后训练出来的，研究者认为这种投资的收益应该是多次收获的，因此产生了迁移学习的概念。</p><p id="2b4f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">迁移学习的最大好处是，我们可以重用整个模型或模型的某一部分。嗯，聪明！😎这样我们就不用训练整个模型了。特别是，迁移学习可以节省时间并提供更好的性能。例如，使用可以识别汽车的预训练模型到现在识别卡车。</p><p id="2f24" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在让我们了解一下我们将在这里使用的最先进的模型。</p><h1 id="dcf8" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">EfficientNet-B0:最先进的模型</h1><p id="eb89" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated"><a class="ae kv" href="https://arxiv.org/pdf/1905.11946.pdf" rel="noopener ugc nofollow" target="_blank"> <strong class="ky ir"> EfficientNet </strong> </a>是谷歌打造的CNN的一个家族。与其他最先进的模型相比，✌️these CNN不仅提供了更好的准确性，而且通过减少参数数量提高了模型的效率。EfficientNet-B0模型是一个简单的移动大小的基线架构，并在ImageNet数据集上进行训练。</p><p id="cc15" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在构建神经网络时，我们提高模型性能的基本方法是增加单元数或层数。然而，这种方法或策略并不总是奏效，或者我必须说，在某一点上没有帮助。例如，我为CIFAR-100数据集构建了一个9层卷积神经网络模型，并设法实现了仅59%的准确率。不仅仅是随机的机会。😏我增加层数或单元数的尝试并没有进一步提高精度。☹️ ( <a class="ae kv" href="https://github.com/chetnakhanna16/CIFAR100_ImageRecognition/blob/master/CIFAR100_ImageClassification_FinalCode_001081074.py" rel="noopener ugc nofollow" target="_blank">链接到代码</a>)</p><p id="4122" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir"> EfficientNet的工作理念是，提供一种有效的复合缩放方法(缩放深度/宽度/分辨率的所有维度)来增加模型大小，可以帮助模型实现最大的精度增益。</strong>下图来自原始论文，给出了一个很好的比例可视化。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mq"><img src="../Images/39bd7803926eb84734ddc8d3ab34ff75.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dZIOSPe8gFyOLPyISNwPVw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">【https://arxiv.org/pdf/1905.11946.pdf】来源:<a class="ae kv" href="https://arxiv.org/pdf/1905.11946.pdf" rel="noopener ugc nofollow" target="_blank"/></p></figure><p id="1433" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir"> <em class="mp">注:</em> </strong> <em class="mp"> EfficientNet有很多变种。我用的是EfficientNet-B0，因为它是个小型号。如果你愿意，你可以试试EfficientNet的其他变种。</em></p><p id="7f29" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">所以，让我们用EfficientNet-B0建立一个图像识别模型。请注意，我只是在博文中训练模型。如果你想了解预处理部分，请参考<a class="ae kv" rel="noopener" target="_blank" href="/cifar-100-pre-processing-for-image-recognition-task-68015b43d658">这篇博文</a>。</p><p id="e66a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir"> <em class="mp">注:</em> </strong> <em class="mp">我会尽量把大部分概念说清楚但还是，本文假设对卷积神经网络(CNN)有基本的了解。</em>📖</p><p id="0cfb" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这项任务的代码可以在我的<a class="ae kv" href="https://github.com/chetnakhanna16/CIFAR100_ImageRecognition/blob/master/EffiicientNetB0_CIFAR100.ipynb" rel="noopener ugc nofollow" target="_blank"> Github </a>上找到。请放心使用它来构建更智能的图像识别系统。</p><h1 id="949e" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">使用迁移学习的模型训练</h1><p id="6a2d" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">为了训练机器学习模型，我们需要一个训练集。一个好的做法是保留一个验证集来选择超参数，并保留一个测试集来根据看不见的数据测试模型。</p><p id="4bcc" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">让我们首先导入库。</p><pre class="kg kh ki kj gt ms mt mu mv aw mw bi"><span id="f5cc" class="mx lt iq mt b gy my mz l na nb"><strong class="mt ir">from</strong> sklearn.model_selection <strong class="mt ir">import </strong>StratifiedShuffleSplit<br/><strong class="mt ir">import</strong> cv2<br/><strong class="mt ir">import</strong> albumentations <strong class="mt ir">as</strong> albu<br/><strong class="mt ir">from</strong> skimage.transform <strong class="mt ir">import</strong> resize<br/><strong class="mt ir">import</strong> numpy <strong class="mt ir">as</strong> np<br/><strong class="mt ir">import </strong>pandas<strong class="mt ir"> as </strong>pd<strong class="mt ir"><br/>import</strong> matplotlib.pyplot <strong class="mt ir">as</strong> plt<br/>%matplotlib inline<br/><strong class="mt ir">from</strong> pylab <strong class="mt ir">import</strong> rcParams<br/><strong class="mt ir">from</strong> sklearn.metrics <strong class="mt ir">import</strong> accuracy_score, confusion_matrix, classification_report<br/><strong class="mt ir">from</strong> keras.callbacks <strong class="mt ir">import</strong> Callback, EarlyStopping, ReduceLROnPlateau<br/><strong class="mt ir">import</strong> tensorflow <strong class="mt ir">as</strong> tf<br/><strong class="mt ir">import</strong> keras<br/><strong class="mt ir">from</strong> keras.models <strong class="mt ir">import</strong> Sequential, load_model<br/><strong class="mt ir">from</strong> keras.layers <strong class="mt ir">import</strong> Dropout, Dense, GlobalAveragePooling2D<br/><strong class="mt ir">from</strong> keras.optimizers <strong class="mt ir">import</strong> Adam<br/><strong class="mt ir">import</strong> efficientnet.keras <strong class="mt ir">as</strong> efn</span></pre><p id="8d0e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我使用分层混洗分割将我的训练集分割为训练集和验证集，因为它将保留100个类中每个类的样本百分比。下面是执行分割的代码。</p><pre class="kg kh ki kj gt ms mt mu mv aw mw bi"><span id="46be" class="mx lt iq mt b gy my mz l na nb">sss = StratifiedShuffleSplit(n_splits=2, test_size=0.2, random_state=123)<br/><br/><strong class="mt ir">for</strong> train_index, val_index <strong class="mt ir">in</strong> sss.split(X_train, y_train):<br/>    X_train_data, X_val_data = X_train[train_index], X_train[val_index]<br/>    y_train_data, y_val_data = y_train[train_index], y_train[val_index]<br/><br/><strong class="mt ir">print</strong>("Number of training samples: ", X_train_data.shape[0])<br/><strong class="mt ir">print</strong>("Number of validation samples: ", X_val_data.shape[0])</span></pre><p id="7f6e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">输出给出了每组中的样本数。</p><pre class="kg kh ki kj gt ms mt mu mv aw mw bi"><span id="16bf" class="mx lt iq mt b gy my mz l na nb">Number of training samples:  40000 <br/>Number of validation samples:  10000</span></pre><p id="7568" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">根据EfficientNet，我们不仅需要缩放模型的宽度和深度(这将由预训练的模型负责)，还需要缩放图像的分辨率。EfficientNet-B0模型架构要求映像的大小为(224，224)。因此，让我们将大小为(32，32)的图像调整到新的大小。</p><pre class="kg kh ki kj gt ms mt mu mv aw mw bi"><span id="42ac" class="mx lt iq mt b gy my mz l na nb">height = 224<br/>width = 224<br/>channels = 3</span><span id="dd80" class="mx lt iq mt b gy nc mz l na nb">input_shape = (height, width, channels)</span></pre><p id="5b8c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">下面的函数<strong class="ky ir"> resize_img </strong>将图像和形状作为输入，并调整每个图像的大小。我使用了双三次插值法来放大图像。它考虑总共16个像素的已知像素的最近的4x 4邻域。这种方法产生明显更清晰的图像，被认为是处理时间和输出质量的理想组合。</p><pre class="kg kh ki kj gt ms mt mu mv aw mw bi"><span id="db1a" class="mx lt iq mt b gy my mz l na nb"><strong class="mt ir">def</strong> resize_img(img, shape):<br/>    <strong class="mt ir">return</strong> cv2.resize(img, (shape[1], shape[0]), interpolation=cv2.INTER_CUBIC)</span></pre><p id="dad8" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们都知道，深度学习模型的性能通常会随着更多数据的添加而提高，所以我计划进行图像增强，但内存始终是深度学习模型的一个大限制，因为它们有很多可训练的参数。所以，我选择了python的<a class="ae kv" href="https://albumentations.ai/" rel="noopener ugc nofollow" target="_blank"> <strong class="ky ir">相册</strong> </a>库，它有助于实时数据扩充。(如果你不了解这个库，我强烈推荐你去看看它的网站和<a class="ae kv" href="https://github.com/albumentations-team/albumentations" rel="noopener ugc nofollow" target="_blank"> GitHub </a>页面。)</p><p id="9d0f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我使用Keras数据生成器类创建了自己的自定义数据生成器类。参数“水平翻转”、“垂直翻转”、“网格扭曲”和“弹性变换”被调整为扩展数据集(您也可以尝试其他参数)。</p><p id="86d1" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">由于图像中特征值的分布可能彼此非常不同，因此通过将每个图像除以255来归一化图像，因为每个单独颜色的范围是[0，255]。因此，重新缩放的图像具有新范围[0，1]中的所有特征。</p><p id="4f66" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我分批完成了所有这些转换。此外，我只对训练数据集应用了增强，并保留了验证和测试数据集。</p><p id="ac02" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在编写自定义数据生成器类之前，让我们首先设置我们的常数。</p><pre class="kg kh ki kj gt ms mt mu mv aw mw bi"><span id="f912" class="mx lt iq mt b gy my mz l na nb">n_classes = 100<br/>epochs = 15<br/>batch_size = 8</span></pre><p id="8c2d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">下面是自定义数据生成器类的代码。</p><pre class="kg kh ki kj gt ms mt mu mv aw mw bi"><span id="9d62" class="mx lt iq mt b gy my mz l na nb"><strong class="mt ir">class</strong> DataGenerator(keras.utils.Sequence):<br/>    <strong class="mt ir">def</strong> __init__(<em class="mp">self</em>, images, labels=<strong class="mt ir">None</strong>, mode='fit', batch_size=batch_size, dim=(height, width), channels=channels, n_classes=n_classes, shuffle=<strong class="mt ir">True</strong>, augment=<strong class="mt ir">False</strong>):<br/>        <br/>        <em class="mp">#initializing the configuration of the generator</em><br/>        <em class="mp">self</em>.images = images<br/>        <em class="mp">self</em>.labels = labels<br/>        <em class="mp">self</em>.mode = mode<br/>        <em class="mp">self</em>.batch_size = batch_size<br/>        <em class="mp">self</em>.dim = dim<br/>        <em class="mp">self</em>.channels = channels<br/>        <em class="mp">self</em>.n_classes = n_classes<br/>        <em class="mp">self</em>.shuffle = shuffle<br/>        <em class="mp">self</em>.augment = augment<br/>        <em class="mp">self</em>.on_epoch_end()<br/>   <br/>    <em class="mp">#method to be called after every epoch</em><br/>    <strong class="mt ir">def</strong> on_epoch_end(<em class="mp">self</em>):<br/>        self.indexes = np.arange(<em class="mp">self</em>.images.shape[0])<br/>        <strong class="mt ir">if</strong> <em class="mp">self</em>.shuffle == <strong class="mt ir">True</strong>:<br/>            np.random.shuffle(<em class="mp">self</em>.indexes)<br/>    <br/>    <em class="mp">#return numbers of steps in an epoch using samples &amp; batch size</em><br/>    <strong class="mt ir">def</strong> __len__(<em class="mp">self</em>):<br/>        <strong class="mt ir">return</strong> int(np.floor(<strong class="mt ir">len</strong>(<em class="mp">self</em>.images) / <em class="mp">self</em>.batch_size))<br/>    <br/>    <em class="mp">#this method is called with the batch number as an argument to #obtain a given batch of data</em><br/>    <strong class="mt ir">def</strong> __getitem__(<em class="mp">self</em>, index):<br/>        <em class="mp">#generate one batch of data</em><br/>        <em class="mp">#generate indexes of batch</em><br/>        batch_indexes = <em class="mp">self</em>.indexes[index * <em class="mp">self</em>.batch_size:(index+1) * <em class="mp">self</em>.batch_size]<br/>        <br/>        <em class="mp">#generate mini-batch of X</em><br/>        X = np.empty((<em class="mp">self</em>.batch_size, *<em class="mp">self</em>.dim, <em class="mp">self</em>.channels))</span><span id="8856" class="mx lt iq mt b gy nc mz l na nb">        <strong class="mt ir">for</strong> i, ID <strong class="mt ir">in</strong> enumerate(batch_indexes):<br/>            <em class="mp">#generate pre-processed image</em><br/>            img = <em class="mp">self</em>.images[ID]<br/>            <em class="mp">#image rescaling</em><br/>            img = img.astype(np.float32)/255.<br/>            <em class="mp">#resizing as per new dimensions</em><br/>            img = resize_img(img, <em class="mp">self</em>.dim)<br/>            X[i] = img<br/>            <br/>        <em class="mp">#generate mini-batch of y</em><br/>        <strong class="mt ir">if</strong> <em class="mp">self</em>.mode == 'fit':<br/>            y = <em class="mp">self</em>.labels[batch_indexes]<br/>            <br/>            <em class="mp">#augmentation on the training dataset</em><br/>            <strong class="mt ir">if</strong> <em class="mp">self</em>.augment == <strong class="mt ir">True</strong>:<br/>                X = <em class="mp">self</em>.__augment_batch(X)<br/>            <strong class="mt ir">return</strong> X, y<br/>        <br/>        <strong class="mt ir">elif</strong> <em class="mp">self</em>.mode == 'predict':<br/>            <strong class="mt ir">return</strong> X<br/>        <br/>        <strong class="mt ir">else</strong>:<br/>            <strong class="mt ir">raise</strong> <strong class="mt ir">AttributeError</strong>("The mode should be set to either 'fit' or 'predict'.")<br/>            <br/>    <em class="mp">#augmentation for one image</em><br/>    <strong class="mt ir">def</strong> __random_transform(<em class="mp">self</em>, img):<br/>        composition = albu.Compose([albu.HorizontalFlip(p=0.5),<br/>                                   albu.VerticalFlip(p=0.5),<br/>                                   albu.GridDistortion(p=0.2),<br/>                                   albu.ElasticTransform(p=0.2)])<br/>        <strong class="mt ir">return</strong> composition(image=img)['image']<br/>    <br/>    <em class="mp">#augmentation for batch of images</em><br/>    <strong class="mt ir">def</strong> __augment_batch(<em class="mp">self</em>, img_batch):<br/>        <strong class="mt ir">for</strong> i <strong class="mt ir">in</strong> range(img_batch.shape[0]):<br/>            img_batch[i] = <em class="mp">self</em>.__random_transform(img_batch[i])<br/>        <strong class="mt ir">return</strong> img_batch</span></pre><p id="4a19" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">让我们将数据生成器类应用于我们的训练集和验证集。</p><pre class="kg kh ki kj gt ms mt mu mv aw mw bi"><span id="5b7d" class="mx lt iq mt b gy my mz l na nb">train_data_generator = DataGenerator(X_train_data, y_train_data, augment=<strong class="mt ir">True</strong>) <br/>valid_data_generator = DataGenerator(X_val_data, y_val_data, augment=<strong class="mt ir">False</strong>)</span></pre><p id="ba7b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">Keras中提供了EfficientNet类来帮助轻松转移学习。我使用了带有ImageNet权重的EfficientNet-B0类。因为我使用这个模型只是为了提取特征，所以我没有在网络的顶部包括完全连接的层，而是指定了输入形状和池。我还添加了自己的池和密集层。</p><p id="ff16" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">下面是使用预先训练好的EfficientNet-B0模型的代码。</p><pre class="kg kh ki kj gt ms mt mu mv aw mw bi"><span id="3580" class="mx lt iq mt b gy my mz l na nb">efnb0 = efn.EfficientNetB0(weights='imagenet', include_top=<strong class="mt ir">False</strong>, input_shape=input_shape, classes=n_classes)<br/><br/>model = Sequential()<br/>model.add(efnb0)<br/>model.add(GlobalAveragePooling2D())<br/>model.add(Dropout(0.5))<br/>model.add(Dense(n_classes, activation='softmax'))<br/><br/>model.summary()</span></pre><p id="4865" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这是输出。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nd"><img src="../Images/7d50f76f2f5596e7de31ffa0c66a9fa0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VdWY8FKT6dHjKLX8PHgijw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="4fe1" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">该模型有4，135，648个可训练参数。😳</p><pre class="kg kh ki kj gt ms mt mu mv aw mw bi"><span id="f61c" class="mx lt iq mt b gy my mz l na nb">optimizer = Adam(lr=0.0001)<br/><br/><em class="mp">#early stopping to monitor the validation loss and avoid overfitting</em><br/>early_stop = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10, restore_best_weights=<strong class="mt ir">True</strong>)<br/><br/><em class="mp">#reducing learning rate on plateau</em><br/>rlrop = ReduceLROnPlateau(monitor='val_loss', mode='min', patience= 5, factor= 0.5, min_lr= 1e-6, verbose=1)</span><span id="70a8" class="mx lt iq mt b gy nc mz l na nb"><em class="mp">#model compiling</em><br/>model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])</span></pre><p id="c903" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在编译我们的模型之后，让我们将它放在我们的训练数据集上，并在验证数据集上验证它。</p><pre class="kg kh ki kj gt ms mt mu mv aw mw bi"><span id="49a8" class="mx lt iq mt b gy my mz l na nb">model_history = model.fit_generator(train_data_generator, validation_data = valid_data_generator, callbacks = [early_stop, rlrop],verbose = 1, epochs = epochs)<br/><br/><em class="mp">#saving the trained model weights as data file in .h5 format</em><br/>model.save_weights("cifar_efficientnetb0_weights.h5")</span></pre><p id="a98a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">以下是训练的片段。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ne"><img src="../Images/7d62e1416ad5070255e7d7308ed193ee.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hgsi3SQ3uEJpUdFmwiDiKw.png"/></div></div></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nf"><img src="../Images/814bd93c9cc8cf9f06916249cf500775.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MVRb9ACd4K4e4Vg86FV26A.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者提供的图片</p></figure><p id="d462" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们可以看到，该模型在第14个时期调整了学习率，我们在训练集上获得了84.82%的最终准确率，这是非常好的。但是等等，我们也需要看看测试的准确性。</p><p id="17e6" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">视觉化有助于更好地看待事物。让我们画出精度和损耗图。</p><pre class="kg kh ki kj gt ms mt mu mv aw mw bi"><span id="3e7e" class="mx lt iq mt b gy my mz l na nb"><em class="mp">#plot to visualize the loss and accuracy against number of epochs</em><br/>plt.figure(figsize=(18,8))<br/><br/>plt.suptitle('Loss and Accuracy Plots', fontsize=18)<br/><br/>plt.subplot(1,2,1)<br/>plt.plot(model_history.history['loss'], label='Training Loss')<br/>plt.plot(model_history.history['val_loss'], label='Validation Loss')<br/>plt.legend()<br/>plt.xlabel('Number of epochs', fontsize=15)<br/>plt.ylabel('Loss', fontsize=15)<br/><br/>plt.subplot(1,2,2)<br/>plt.plot(model_history.history['accuracy'], label='Train Accuracy')<br/>plt.plot(model_history.history['val_accuracy'], label='Validation Accuracy')<br/>plt.legend()<br/>plt.xlabel('Number of epochs', fontsize=14)<br/>plt.ylabel('Accuracy', fontsize=14)<br/>plt.show()</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ng"><img src="../Images/b10c9b2d25e39f80f2b39076fe365f40.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mQdRSZZQfUDdq9ZZ3fWYkA.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="0f81" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在让我们评估一下我们的模型。</p><pre class="kg kh ki kj gt ms mt mu mv aw mw bi"><span id="fb27" class="mx lt iq mt b gy my mz l na nb">valid_loss, valid_accuracy = model.evaluate_generator(generator = valid_data_generator, verbose = 1)<br/><br/><strong class="mt ir">print</strong>('Validation Accuracy: ', <strong class="mt ir">round</strong>((valid_accuracy * 100), 2), "%")</span></pre><p id="4286" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">输出:</p><pre class="kg kh ki kj gt ms mt mu mv aw mw bi"><span id="9dd8" class="mx lt iq mt b gy my mz l na nb">1250/1250 [==============================] - 85s 68ms/step Validation Accuracy:  82.3 %</span></pre><p id="cf14" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在，是时候看看测试数据集的准确性了。</p><pre class="kg kh ki kj gt ms mt mu mv aw mw bi"><span id="46b6" class="mx lt iq mt b gy my mz l na nb">y_pred = model.predict_generator(DataGenerator(X_test, mode='predict', augment=<strong class="mt ir">False</strong>, shuffle=<strong class="mt ir">False</strong>), verbose=1)<br/>y_pred = np.argmax(y_pred, axis=1)<br/>test_accuracy = accuracy_score(np.argmax(y_test, axis=1), y_pred)<br/><br/><strong class="mt ir">print</strong>('Test Accuracy: ', <strong class="mt ir">round</strong>((test_accuracy * 100), 2), "%")</span></pre><p id="25a5" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">输出:</p><pre class="kg kh ki kj gt ms mt mu mv aw mw bi"><span id="f182" class="mx lt iq mt b gy my mz l na nb">1250/1250 [==============================] - 78s 63ms/step<br/>Test Accuracy:  81.79 %</span></pre><p id="ddce" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">培训的结果相当不错。我们在测试数据集上获得了81.79%的准确率。💃</p><p id="fa66" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">可以使用下面的代码为模型生成混淆矩阵和分类报告。</p><pre class="kg kh ki kj gt ms mt mu mv aw mw bi"><span id="b627" class="mx lt iq mt b gy my mz l na nb">cm = confusion_matrix(np.argmax(y_test, axis=1), y_pred)<br/><strong class="mt ir">print</strong>(cm)</span><span id="5650" class="mx lt iq mt b gy nc mz l na nb">target = ["Category <strong class="mt ir">{}</strong>".format(i) <strong class="mt ir">for</strong> i <strong class="mt ir">in</strong> range(n_classes)]<br/><strong class="mt ir">print</strong>(classification_report(np.argmax(y_test, axis=1), y_pred, target_names=target))</span></pre><p id="8cc5" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">下面是前11个类的代码片段。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nh"><img src="../Images/b9c4964e70937330ffb68902bff34a30.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YdcnUbPjCIUlG-n2VaItug.png"/></div></div></figure><p id="bf18" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">从分类报告中，我们可以看到一些类别被很好地预测，而一些被错误地预测。</p><p id="7564" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">如果您想可视化预测，这里是代码。</p><pre class="kg kh ki kj gt ms mt mu mv aw mw bi"><span id="f0d5" class="mx lt iq mt b gy my mz l na nb">prediction = pd.DataFrame(y_pred)</span><span id="ca60" class="mx lt iq mt b gy nc mz l na nb">rcParams['figure.figsize'] = 12,15<br/><br/>num_row = 4<br/>num_col = 4<br/><br/>imageId = np.random.randint(0, len(X_test), num_row * num_col)<br/><br/>fig, axes = plt.subplots(num_row, num_col)<br/><br/><strong class="mt ir">for</strong> i <strong class="mt ir">in</strong> range(0, num_row):<br/>    <strong class="mt ir">for</strong> j <strong class="mt ir">in</strong> range(0, num_col):<br/>        k = (i*num_col)+j<br/>        axes[i,j].imshow(X_test[imageId[k]])<br/>        axes[i,j].set_title("True: " + <strong class="mt ir">str</strong>(subCategory.iloc[testData['fine_labels'][imageId[k]]][0]).capitalize() + "<strong class="mt ir">\n</strong>Predicted: " + <strong class="mt ir">str</strong>(subCategory.iloc[prediction.iloc[imageId[k]]]).split()[2].capitalize(), fontsize=14)<br/>        axes[i,j].axis('off')<br/>        fig.suptitle("Images with True and Predicted Labels", fontsize=18) <br/><br/>plt.show()</span></pre><p id="e70f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">下面是输出的片段。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ni"><img src="../Images/20c8157b4f0df50733561ccc76769979.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DyQA-LCyjPv2wdY6rAg-Fw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="29a5" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">你可以看到我们的模型混淆了摩托车和自行车。🙄但是，我们可以看到大多数预测是正确的。✅</p><p id="b458" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">深度学习就是实验。使用EfficientNet的其他最新版本，很有可能可以进一步提高该模型的性能。超参数调整也是深度学习的一个重要方面，可以帮助提高准确性。</p><p id="5428" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我希望这个博客能帮助你理解如何进行迁移学习。请随意尝试更多以获得更好的性能。查看我的<a class="ae kv" href="https://github.com/chetnakhanna16/CIFAR100_ImageRecognition/blob/master/EffiicientNetB0_CIFAR100.ipynb" rel="noopener ugc nofollow" target="_blank"> GitHub </a>的完整代码和我的<a class="ae kv" rel="noopener" target="_blank" href="/cifar-100-pre-processing-for-image-recognition-task-68015b43d658">以前的文章</a>的初始步骤。另外，我强烈推荐你阅读<a class="ae kv" href="https://arxiv.org/pdf/1905.11946.pdf" rel="noopener ugc nofollow" target="_blank">的原创论文</a>。这是一本有趣的读物！</p><p id="44d3" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">相关文章:</strong></p><div class="nj nk gp gr nl nm"><a rel="noopener follow" target="_blank" href="/cifar-100-pre-processing-for-image-recognition-task-68015b43d658"><div class="nn ab fo"><div class="no ab np cl cj nq"><h2 class="bd ir gy z fp nr fr fs ns fu fw ip bi translated">CIFAR-100:图像识别任务预处理</h2><div class="nt l"><h3 class="bd b gy z fp nr fr fs ns fu fw dk translated">常用图像数据集(CIFAR-100)的预处理或数据准备</h3></div><div class="nu l"><p class="bd b dl z fp nr fr fs ns fu fw dk translated">towardsdatascience.com</p></div></div><div class="nv l"><div class="nw l nx ny nz nv oa kp nm"/></div></div></a></div><p id="afdb" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">参考:</strong></p><ol class=""><li id="5c68" class="ob oc iq ky b kz la lc ld lf od lj oe ln of lr og oh oi oj bi translated">原文:<a class="ae kv" href="https://arxiv.org/pdf/1905.11946.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1905.11946.pdf</a></li><li id="be4f" class="ob oc iq ky b kz ok lc ol lf om lj on ln oo lr og oh oi oj bi translated">这个笔记本给了我如何进行迁移学习的指导。</li></ol></div><div class="ab cl op oq hu or" role="separator"><span class="os bw bk ot ou ov"/><span class="os bw bk ot ou ov"/><span class="os bw bk ot ou"/></div><div class="ij ik il im in"><p id="16e9" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">谢谢大家阅读这篇文章。请分享您宝贵的反馈或建议。快乐阅读！📗🖌:我也很想知道你使用迁移学习在CIFAR-100上有没有更好的表现。</p><p id="87f2" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><a class="ae kv" href="https://www.linkedin.com/in/chetna-khanna/" rel="noopener ugc nofollow" target="_blank">领英</a></p></div></div>    
</body>
</html>