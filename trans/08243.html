<html>
<head>
<title>Clustering: How to Find Hyperparameters using Inertia</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">聚类:如何利用惯性找到超参数</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/clustering-how-to-find-hyperparameters-using-inertia-b0343c6fe819?source=collection_archive---------7-----------------------#2021-07-29">https://towardsdatascience.com/clustering-how-to-find-hyperparameters-using-inertia-b0343c6fe819?source=collection_archive---------7-----------------------#2021-07-29</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><figure class="gl gn jr js jt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi jq"><img src="../Images/2a9674f374cc586ba21e16db94f71e80.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*dBRln4ZaXBzkeXDd"/></div></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">Robynne Hu 在<a class="ae kf" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><h1 id="e9e5" class="kg kh it bd ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated">介绍</h1><p id="0985" class="pw-post-body-paragraph le lf it lg b lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">由于没有标签，聚类是非常强大的。获取标记数据通常既昂贵又耗时。聚类通常用于发现数据中的模式。然后，为了改进某个产品，经常使用找到的模式。一个著名的例子是客户聚类。在客户聚类中，可以找到相似的用户组。如果一个群体的顾客购买了某些产品，这个群体的其他顾客可能也会喜欢他们。因此，可以应用有针对性的广告来增加销售。另一个著名的例子是将网络活动分为欺诈行为和非欺诈行为。</p><p id="e688" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">有许多不同的聚类算法。K-Means算法是一种非常简单而又强大的聚类算法。K-Means算法要求用户首先定义聚类的数量，可能还需要定义聚类的初始化策略。但是如何找到这些参数呢？对于网络活动的群集，可以使用两个群集。一个用于欺诈活动，一个用于非欺诈活动。但是有多少个聚类用于客户聚类呢？在监督学习中，可以尝试不同的超参数和聚类数，并可以直接计算一些误差度量，如准确度。导致最高精确度的超参数集和聚类数然后可以用于最终模型。但是这对于无监督学习是不可能的，因为缺乏基础真值。那么，为了评估和比较不同的超参数和聚类数，可以为无监督学习做些什么呢？</p><p id="b5e7" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">一种可能性是计算群集内的平方和，也称为<strong class="lg iu">惯性</strong>。</p><p id="629f" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">在这篇短文中，介绍了惯性值。然后使用Scikit-Learn在小数据集上训练K-Means聚类算法。使用计算的惯性值和应用在惯性曲线上的肘方法找到最佳的聚类数。最后但同样重要的是，本文展示了如何使用惯性值找到最佳超参数。代码是用Python和Jupyter笔记本编写的。代码可以在我的<a class="ae kf" href="https://github.com/patrickbrus/Medium_Notebooks/tree/master/KMeans_Inertia_Tutorial" rel="noopener ugc nofollow" target="_blank"> Github页面</a>找到。</p><h1 id="3c0b" class="kg kh it bd ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated">惯性</h1><p id="f3a5" class="pw-post-body-paragraph le lf it lg b lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">惯性或组内平方和值给出了不同组的一致性的指示。等式1显示了计算惯性值的公式。</p><figure class="mi mj mk ml gt ju gh gi paragraph-image"><div class="gh gi mh"><img src="../Images/973e4efda6337b618dc274f89e5521a8.png" data-original-src="https://miro.medium.com/v2/resize:fit:610/format:webp/1*VG24Pm-Zdx9jBs6axoLeUg.png"/></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">等式1:惯性公式</p></figure><p id="0042" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">n是数据集内的样本数，C是聚类的中心。因此，惯量简单地计算聚类中每个样本到其聚类中心的平方距离，并将它们相加。对该数据集中的每个聚类和所有样本进行该过程。惯性值越小，不同的聚类越一致。当添加的聚类数与数据集中的样本数一样多时，惯性值将为零。那么如何利用惯性值找到最优的聚类数呢？</p><p id="1682" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">为此，可以使用所谓的<strong class="lg iu">弯管法</strong>。但是让我们通过一个例子来看看这个方法。</p><h1 id="062a" class="kg kh it bd ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated">最优聚类数的肘方法</h1><p id="f070" class="pw-post-body-paragraph le lf it lg b lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">通过直接应用于实例来学习总是更好的。为此，使用Scikit-Learns <em class="mm"> make_blobs </em>函数创建一个具有二维特征的数据集。图1显示了创建这个数据集的代码，而图2显示了这个数据集的绘图。</p><figure class="mi mj mk ml gt ju"><div class="bz fp l di"><div class="mn mo l"/></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">图1:创建集群数据集的代码(作者代码)。</p></figure><figure class="mi mj mk ml gt ju gh gi paragraph-image"><div class="gh gi mp"><img src="../Images/ee90335c62b6769d938bc1d3e73c6811.png" data-original-src="https://miro.medium.com/v2/resize:fit:988/format:webp/1*-ag8koMGBxk2mYF_5qv3kA.png"/></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">图2:数据集的绘图(图片由作者提供)。</p></figure><p id="a507" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">这个数据集中有三个聚类，因此K-Means的最佳聚类数应该是三。但是让我们假设我们还不知道。</p><p id="31bf" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">现在，让我们为不同数量的聚类训练K均值模型，并存储每个训练模型的惯性值。然后，可以绘制惯性曲线，以便使用肘方法来寻找最佳的聚类数。这些步骤的代码可以在图3中找到。图4显示了最终的惯性曲线。</p><figure class="mi mj mk ml gt ju"><div class="bz fp l di"><div class="mn mo l"/></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">图3:用不同数量的聚类训练K均值模型并绘制惯性曲线的代码(作者编写的代码)。</p></figure><figure class="mi mj mk ml gt ju gh gi paragraph-image"><div class="gh gi mq"><img src="../Images/f3b65cacf17263ff9c27096b18272d14.png" data-original-src="https://miro.medium.com/v2/resize:fit:1054/format:webp/1*-dx41m6y-QY18yHj4b0R_g.png"/></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">图4:惯性曲线(图片由作者提供)。</p></figure><p id="bb36" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">红色的x标记标记肘点。肘点给出了最佳的聚类数，这里是三个。这完全有意义，因为数据集是这样创建的，有三个不同的聚类。当添加更多的聚类时，惯性值减小，但是聚类中包含的信息也进一步减少。拥有多个集群会导致性能下降，并且也不会产生最佳的集群结果。让我们假设你做了一个客户聚类，你有许多小的聚类。当一个小客户群购买某样东西时，你只能向该产品的少数其他潜在买家推销。但是，当你有一个大的连贯集群，那么你可以直接解决更多的潜在买家。</p><p id="487c" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">因此，对于本例，最佳聚类数是3。图5显示了三个不同集群的可视化。</p><figure class="mi mj mk ml gt ju gh gi paragraph-image"><div class="gh gi mr"><img src="../Images/a3d59ce7e90f74c680170380a189cc5d.png" data-original-src="https://miro.medium.com/v2/resize:fit:992/format:webp/1*V1ZIF9CbrJ-fvIHv48SAqA.png"/></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">图5:三个聚类的K-Means结果的可视化(图片由作者提供)。</p></figure><h1 id="f9f3" class="kg kh it bd ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated">利用惯性值寻找最优超参数</h1><p id="f48a" class="pw-post-body-paragraph le lf it lg b lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">惯性值也可以用于为无监督K-均值算法寻找更好的超参数。一个潜在的超参数是初始化方法。在Scikit-Learn中，有两种不同的可能性。一个叫<em class="mm"> k-means++ </em>一个叫<em class="mm"> random </em>。但是应该使用哪种初始化呢？为了回答这个问题，可以为每个初始化策略训练一个K均值模型，并比较其惯性值。导致较小惯性值的策略可以用作最佳策略。图6显示了该评估的代码，图7显示了结果的数据框。</p><figure class="mi mj mk ml gt ju"><div class="bz fp l di"><div class="mn mo l"/></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">图6:用于比较K-Means算法的不同初始化策略的代码(由作者编写)。</p></figure><figure class="mi mj mk ml gt ju gh gi paragraph-image"><div class="gh gi ms"><img src="../Images/fbcfb7a2a73e10124b2188b463eeed08.png" data-original-src="https://miro.medium.com/v2/resize:fit:358/format:webp/1*uNsPMu6GY86YxrF8OZebmQ.png"/></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">图7:包含每个初始化策略的惯性值的数据框(图片由作者提供)。</p></figure><p id="3713" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">可以看出，随机初始化实现了稍小的惯性值，并且在这里可以用作最佳初始化策略。</p><h1 id="73e8" class="kg kh it bd ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated">结论</h1><p id="1cb8" class="pw-post-body-paragraph le lf it lg b lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">聚类算法在发现数据模式方面非常有效。聚类算法通常只需要几个超参数，如聚类数或聚类的初始化策略。由于缺乏基础真值，寻找最优值不像监督学习那样简单。为了仍然能够找到最佳值，可以使用惯性值和弯头方法。</p></div></div>    
</body>
</html>