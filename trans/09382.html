<html>
<head>
<title>Dimensionality Reduction (PCA) Explained</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">解释了降维(PCA)</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/dimensionality-reduction-explained-5ae45ae3058e?source=collection_archive---------22-----------------------#2021-08-31">https://towardsdatascience.com/dimensionality-reduction-explained-5ae45ae3058e?source=collection_archive---------22-----------------------#2021-08-31</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="c04c" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">用Python解释和实现PCA</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/2bf4f6143aa122fa785ce3b1bf630939.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jJKQ_ppDtRgR--U1IaRiAA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图片来自<a class="ae ky" href="https://unsplash.com/photos/1FxMET2U5dU" rel="noopener ugc nofollow" target="_blank"> Unsplash </a></p></figure><p id="7c9a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">目录</strong></p><ul class=""><li id="fa8c" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated">介绍</li><li id="1789" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">什么是降维？</li><li id="485b" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">线性组合</li><li id="5758" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">特征工程和特征选择</li><li id="98fa" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">数据</li><li id="1c2f" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">PCA <br/> -直觉<br/> -数学分解<br/> -优点<br/> -缺点<br/> -实现</li><li id="9c48" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">摘要</li><li id="3dc9" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">资源</li></ul></div><div class="ab cl mj mk hx ml" role="separator"><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo"/></div><div class="im in io ip iq"><p id="e0c4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">简介</strong></p><p id="1cc2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">降维是数据科学家常用的机器学习中的一种流行方法。本文将重点介绍一种非常流行的无监督学习降维方法，即主成分分析(PCA)。这是一篇介绍性文章，旨在直观地理解什么是降维、PCA如何工作以及如何用Python实现它。</p><h1 id="f206" class="mq mr it bd ms mt mu mv mw mx my mz na jz nb ka nc kc nd kd ne kf nf kg ng nh bi translated">什么是降维</h1><p id="ccfa" class="pw-post-body-paragraph kz la it lb b lc ni ju le lf nj jx lh li nk lk ll lm nl lo lp lq nm ls lt lu im bi translated">在跳到降维之前，我们先来定义一下什么是维度。给定矩阵A，矩阵的维数是行数乘以列数。如果A有3行5列，那么A就是一个3x5的矩阵。</p><pre class="kj kk kl km gt nn no np nq aw nr bi"><span id="b0b8" class="ns mr it no b gy nt nu l nv nw">A = [1, 2, 3]   --&gt; 1 row, 3 columns<br/>The dimension of A is 1x3</span></pre><p id="ce6e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在用最简单的术语来说，降维就是它听起来的样子，你把矩阵的维度降低到比现在更小。给定一个正方形(n乘n)矩阵A，目标是将这个矩阵的维数减少到小于n×n。</p><pre class="kj kk kl km gt nn no np nq aw nr bi"><span id="4e8d" class="ns mr it no b gy nt nu l nv nw">Current Dimension of A : n<br/>Reduced Dimension of A : n - x, where x is some positive integer</span></pre><p id="8b64" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">你可能会问为什么有人想这样做，最常见的应用程序是为了数据可视化的目的。在大于3维的空间中，很难形象化地描述事物。通过降维，您将能够将1000行和列的数据集转换成一个足够小的数据集，以便在3 / 2 / 1维中可视化。</p><h1 id="74ce" class="mq mr it bd ms mt mu mv mw mx my mz na jz nb ka nc kc nd kd ne kf nf kg ng nh bi translated">线性组合</h1><p id="4332" class="pw-post-body-paragraph kz la it lb b lc ni ju le lf nj jx lh li nk lk ll lm nl lo lp lq nm ls lt lu im bi translated">无需深入研究线性代数背后的数学知识，降低矩阵A维数的最简单方法是将其乘以向量/矩阵X，使乘积等于B。公式为<code class="fe nx ny nz no b">Ax=B</code>，当且仅当B是A的列的线性组合时，该公式才有解。</p><p id="f73c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">示例</strong></p><pre class="kj kk kl km gt nn no np nq aw nr bi"><span id="1382" class="ns mr it no b gy nt nu l nv nw">The goal is to reduce the matrix 4x4 dimension matrix A to a matrix of 4x2 dimensions. The values below are from a random example.</span><span id="c0ec" class="ns mr it no b gy oa nu l nv nw">A = [[1,2,3,4],      x = [[1,2],<br/>     [3,4,5,6],           [3,4],<br/>     [4,5,6,7],           [0,1],<br/>     [5,6,7,8]]           [4,0]]</span><span id="735e" class="ns mr it no b gy oa nu l nv nw">Ax = [[23,13],<br/>      [39,27],<br/>      [47,34],<br/>      [55,41]]<br/>The output B now has a dimension of 4x2.</span></pre><h1 id="6da6" class="mq mr it bd ms mt mu mv mw mx my mz na jz nb ka nc kc nd kd ne kf nf kg ng nh bi translated">特征工程和特征选择</h1><p id="cf81" class="pw-post-body-paragraph kz la it lb b lc ni ju le lf nj jx lh li nk lk ll lm nl lo lp lq nm ls lt lu im bi translated">给定包含各种要素的数据集，您可以通过要素工程和要素选择来减少要素的数量。这本身就直观地减少了您正在处理的原始数据集的维度。假设您有一个包含以下各列的数据框架:</p><p id="e3a2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">您发现，通过做一些算术，您可以组合一组特征，而不会丢失关于这些特征的信息。然后，新特征将替换产生它的特征对。通过某种预处理手段创建新特征的过程称为特征工程，而为训练模型而选择特定特征的过程称为特征选择。</p><pre class="kj kk kl km gt nn no np nq aw nr bi"><span id="095e" class="ns mr it no b gy nt nu l nv nw"><strong class="no iu">Example</strong><br/>Feature 1 + Feature 2 = Feature 6</span><span id="f2c8" class="ns mr it no b gy oa nu l nv nw">Now your new features are : <br/>Feature 3 | Feature 4 | Feature 5 | Feature 6</span></pre><h1 id="cccc" class="mq mr it bd ms mt mu mv mw mx my mz na jz nb ka nc kc nd kd ne kf nf kg ng nh bi translated">数据</h1><p id="fe1a" class="pw-post-body-paragraph kz la it lb b lc ni ju le lf nj jx lh li nk lk ll lm nl lo lp lq nm ls lt lu im bi translated">我将展示如何使用NBA数据通过sklearn实现PCA。你可以直接从我的GitHub库<a class="ae ky" href="https://github.com/vatsal220/medium_articles/blob/main/dimensionality_reduction/data/all_seasons.csv" rel="noopener ugc nofollow" target="_blank">这里</a>下载数据，或者从它的原始来源Kaggle <a class="ae ky" href="https://www.kaggle.com/justinas/nba-players-data" rel="noopener ugc nofollow" target="_blank">这里</a>下载</p><h1 id="8d10" class="mq mr it bd ms mt mu mv mw mx my mz na jz nb ka nc kc nd kd ne kf nf kg ng nh bi translated">主成分分析</h1><p id="ba28" class="pw-post-body-paragraph kz la it lb b lc ni ju le lf nj jx lh li nk lk ll lm nl lo lp lq nm ls lt lu im bi translated">PCA是一种高度使用的无监督学习技术，用于降低大型数据集的维度。它将大量的变量转换成包含大部分信息的更小的部分。减少数据集的大小自然会导致信息的丢失，并会影响模型的准确性，但这种负面影响会因便于探索、可视化和分析而抵消。</p><h2 id="1e2b" class="ns mr it bd ms ob oc dn mw od oe dp na li of og nc lm oh oi ne lq oj ok ng ol bi translated">直觉</h2><p id="ee52" class="pw-post-body-paragraph kz la it lb b lc ni ju le lf nj jx lh li nk lk ll lm nl lo lp lq nm ls lt lu im bi translated">假设我们正在查看一个数据集，该数据集与一组运动员及其所有相关统计数据相关联。每一项数据都衡量了运动员的各种特征，比如身高、体重、臂展、效率、得分、篮板、盖帽、失误等等。这将基本上为我们提供一个与我们每个球员相关的各种特征的大型数据集，然而，许多特征可能彼此相关，从而产生冗余信息。</p><p id="9fd2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">PCA的目的是创造新的特征，总结我们最初的特征。通过寻找旧特征的线性组合，PCA可以构建新特征，同时尽量减少信息损失。例如，一个新的特征可以计算为一个球员的平均得分减去每场比赛的平均失误数。它通过识别玩家之间强烈不同的特征来做到这一点。</p><p id="a94b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">数学分解</strong></p><p id="bc17" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">数学上，PCA可以分解为4个简单的步骤。</p><ol class=""><li id="0cc7" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu om mb mc md bi translated">确定数据的中心，将数据和中心重新定位到原点<br/> -这可以通过取每列的平均值并减去原始数据的平均值来完成</li><li id="d04c" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu om mb mc md bi translated">计算中心矩阵的协方差矩阵</li><li id="47ff" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu om mb mc md bi translated">计算协方差矩阵的特征向量</li><li id="60f0" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu om mb mc md bi translated">通过点积将特征向量投影到协方差矩阵上</li></ol><p id="f7c9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在数学上，它是一个高维对象在一个低维向量空间中的投影。</p><h2 id="d4b1" class="ns mr it bd ms ob oc dn mw od oe dp na li of og nc lm oh oi ne lq oj ok ng ol bi translated">优势</h2><ul class=""><li id="4418" class="lv lw it lb b lc ni lf nj li on lm oo lq op lu ma mb mc md bi translated">移除相关要素</li><li id="9cb3" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">允许更容易的可视化</li><li id="ad9e" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">有助于减少过度拟合</li></ul><h2 id="cec7" class="ns mr it bd ms ob oc dn mw od oe dp na li of og nc lm oh oi ne lq oj ok ng ol bi translated">不足之处</h2><ul class=""><li id="0136" class="lv lw it lb b lc ni lf nj li on lm oo lq op lu ma mb mc md bi translated">变量变得更难解释</li><li id="2a6c" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">信息损失</li><li id="0842" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">数据标准化</li></ul><h2 id="dcf5" class="ns mr it bd ms ob oc dn mw od oe dp na li of og nc lm oh oi ne lq oj ok ng ol bi translated">履行</h2><p id="f225" class="pw-post-body-paragraph kz la it lb b lc ni ju le lf nj jx lh li nk lk ll lm nl lo lp lq nm ls lt lu im bi translated"><strong class="lb iu">注意:</strong>您可能需要更改第11行上与您保存数据的位置相关的路径。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oq or l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi os"><img src="../Images/1de417dcf4cac317027ff13bac4118af.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZcjTrfc4_bv-MW7PoNGTgQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">NBA数值特征的二维PCA可视化(图片由作者提供)</p></figure><h1 id="2698" class="mq mr it bd ms mt mu mv mw mx my mz na jz nb ka nc kc nd kd ne kf nf kg ng nh bi translated">摘要</h1><p id="c509" class="pw-post-body-paragraph kz la it lb b lc ni ju le lf nj jx lh li nk lk ll lm nl lo lp lq nm ls lt lu im bi translated">降维是机器学习中常用的方法，有许多方法可以降低数据的维度，从特征工程和特征选择到无监督学习算法(如PCA)的实现。PCA旨在通过识别线性组合来创建概括数据集初始特征的新特征。</p></div><div class="ab cl mj mk hx ml" role="separator"><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo"/></div><div class="im in io ip iq"><h1 id="3ca1" class="mq mr it bd ms mt ot mv mw mx ou mz na jz ov ka nc kc ow kd ne kf ox kg ng nh bi translated">资源</h1><ul class=""><li id="c904" class="lv lw it lb b lc ni lf nj li on lm oo lq op lu ma mb mc md bi translated"><a class="ae ky" href="https://builtin.com/data-science/step-step-explanation-principal-component-analysis" rel="noopener ugc nofollow" target="_blank">https://builtin . com/data-science/step-step-explanation-principal-component-analysis</a></li><li id="b2ae" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated"><a class="ae ky" href="https://machinelearningmastery.com/calculate-principal-component-analysis-scratch-python/" rel="noopener ugc nofollow" target="_blank">https://machinelingmastery . com/calculate-principal-component-analysis-scratch-python/</a></li><li id="3b40" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated"><a class="ae ky" href="https://datascience.stackexchange.com/questions/60351/intuition-behind-the-pca-algorithm" rel="noopener ugc nofollow" target="_blank">https://data science . stack exchange . com/questions/60351/intuition-behind-the-PCA-algorithm</a></li><li id="449f" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated"><a class="ae ky" href="https://stats.stackexchange.com/questions/2691/making-sense-of-principal-component-analysis-eigenvectors-eigenvalues" rel="noopener ugc nofollow" target="_blank">https://stats . stack exchange . com/questions/2691/making-sense-of-principal-component-analysis-features vectors-environments</a></li><li id="fb29" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated"><a class="ae ky" href="https://www.i2tutorials.com/what-are-the-pros-and-cons-of-the-pca/" rel="noopener ugc nofollow" target="_blank">https://www . I2 tutorials . com/what-is-the-pros-and-cons-of-the-PCA/</a></li></ul></div><div class="ab cl mj mk hx ml" role="separator"><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo"/></div><div class="im in io ip iq"><p id="42d0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">感谢你通读我的文章，如果你喜欢这篇文章，这里有一些你可能感兴趣的我写的其他文章。</p><div class="oy oz gp gr pa pb"><a rel="noopener follow" target="_blank" href="/word2vec-explained-49c52b4ccb71"><div class="pc ab fo"><div class="pd ab pe cl cj pf"><h2 class="bd iu gy z fp pg fr fs ph fu fw is bi translated">Word2Vec解释道</h2><div class="pi l"><h3 class="bd b gy z fp pg fr fs ph fu fw dk translated">解释Word2Vec的直观性&amp;用Python实现它</h3></div><div class="pj l"><p class="bd b dl z fp pg fr fs ph fu fw dk translated">towardsdatascience.com</p></div></div><div class="pk l"><div class="pl l pm pn po pk pp ks pb"/></div></div></a></div><div class="oy oz gp gr pa pb"><a rel="noopener follow" target="_blank" href="/recommendation-systems-explained-a42fc60591ed"><div class="pc ab fo"><div class="pd ab pe cl cj pf"><h2 class="bd iu gy z fp pg fr fs ph fu fw is bi translated">推荐系统解释</h2><div class="pi l"><h3 class="bd b gy z fp pg fr fs ph fu fw dk translated">用Python解释和实现基于内容的协同过滤和混合推荐系统</h3></div><div class="pj l"><p class="bd b dl z fp pg fr fs ph fu fw dk translated">towardsdatascience.com</p></div></div><div class="pk l"><div class="pq l pm pn po pk pp ks pb"/></div></div></a></div><div class="oy oz gp gr pa pb"><a rel="noopener follow" target="_blank" href="/bayesian-a-b-testing-explained-344a6df88c1a"><div class="pc ab fo"><div class="pd ab pe cl cj pf"><h2 class="bd iu gy z fp pg fr fs ph fu fw is bi translated">贝叶斯A/B测试解释</h2><div class="pj l"><p class="bd b dl z fp pg fr fs ph fu fw dk translated">towardsdatascience.com</p></div></div><div class="pk l"><div class="pr l pm pn po pk pp ks pb"/></div></div></a></div></div></div>    
</body>
</html>