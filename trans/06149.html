<html>
<head>
<title>Attention in computer vision</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">计算机视觉中的注意力</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/attention-in-computer-vision-fd289a5bd7ad?source=collection_archive---------4-----------------------#2021-06-02">https://towardsdatascience.com/attention-in-computer-vision-fd289a5bd7ad?source=collection_archive---------4-----------------------#2021-06-02</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="6a9d" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsdatascience.com/tagged/hands-on-tutorials" rel="noopener" target="_blank">实践教程</a></h2><div class=""/><div class=""><h2 id="56ab" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">在PyTorch中实现多头和CBAM注意模块</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/d84129792ba6125d3a751cc17cadc05d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*UlqrO56zV6QtaVBr"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">照片由<a class="ae lh" href="https://www.pexels.com/photo/grayscale-photo-of-computer-laptop-near-white-notebook-and-ceramic-mug-on-table-169573/" rel="noopener ugc nofollow" target="_blank">像素</a>上的<a class="ae lh" href="https://www.pexels.com/@negativespace" rel="noopener ugc nofollow" target="_blank">负空间</a>拍摄</p></figure><p id="d0cf" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">自从Transformer在作品“<a class="ae lh" href="https://arxiv.org/abs/1706.03762" rel="noopener ugc nofollow" target="_blank">Attention is all you needle</a>”中引入以来，NLP领域出现了一种向用基于注意力的网络取代递归神经网络(RNN)的转变。在目前的文献中，已经有很多很棒的文章描述了这种方法。下面是我在评测中发现的最好的两个:<a class="ae lh" href="https://nlp.seas.harvard.edu/2018/04/03/attention.html" rel="noopener ugc nofollow" target="_blank"> <em class="me">带注释的变形金刚</em> </a>和<a class="ae lh" rel="noopener" target="_blank" href="/transformers-explained-visually-part-3-multi-head-attention-deep-dive-1c1ff1024853"> <em class="me">可视化解释的变形金刚</em> </a>。</p><p id="4c58" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">然而，在研究了如何在计算机视觉中实现注意力(最佳找到的文章:<a class="ae lh" href="https://medium.com/visionwizard/understanding-attention-modules-cbam-and-bam-a-quick-read-ca8678d1c671" rel="noopener"> <em class="me">理解注意力模块</em> </a>、<a class="ae lh" href="https://blog.paperspace.com/attention-mechanisms-in-computer-vision-cbam/" rel="noopener ugc nofollow" target="_blank"> <em class="me"> CBAM </em> </a>、<a class="ae lh" href="https://paperswithcode.com/methods/category/attention-modules" rel="noopener ugc nofollow" target="_blank"> <em class="me">带代码的论文——注意</em> </a>、<a class="ae lh" href="https://peltarion.com/blog/data-science/self-attention-video" rel="noopener ugc nofollow" target="_blank">自我注意</a>、<a class="ae lh" href="http://jbcordonnier.com/posts/attention-cnn/#:~:text=The%20main%20difference%20between%20CNN,is%20always%20the%20full%20image." rel="noopener ugc nofollow" target="_blank">自我注意和Conv </a>)后，我注意到其中只有少数几篇清楚地描述了注意力机制，并在理论的同时包含了干净的代码。因此，本文的目标是详细描述计算机视觉中两个最重要的注意模块，并使用PyTorch将它们应用到一个实际案例中。文章的结构如下:</p><ol class=""><li id="109f" class="mf mg it lk b ll lm lo lp lr mh lv mi lz mj md mk ml mm mn bi translated">注意力模块介绍</li><li id="e197" class="mf mg it lk b ll mo lo mp lr mq lv mr lz ms md mk ml mm mn bi translated">计算机视觉中的注意方法</li><li id="817f" class="mf mg it lk b ll mo lo mp lr mq lv mr lz ms md mk ml mm mn bi translated">基于注意的网络的实现和结果</li><li id="a83f" class="mf mg it lk b ll mo lo mp lr mq lv mr lz ms md mk ml mm mn bi translated">结论</li></ol></div><div class="ab cl mt mu hx mv" role="separator"><span class="mw bw bk mx my mz"/><span class="mw bw bk mx my mz"/><span class="mw bw bk mx my"/></div><div class="im in io ip iq"><h2 id="b629" class="na nb it bd nc nd ne dn nf ng nh dp ni lr nj nk nl lv nm nn no lz np nq nr iz bi translated">1.注意力模块介绍</h2><p id="11b3" class="pw-post-body-paragraph li lj it lk b ll ns kd ln lo nt kg lq lr nu lt lu lv nv lx ly lz nw mb mc md im bi translated">在机器学习的背景下，<strong class="lk jd">注意力</strong>是一种模仿认知注意力的技术，被定义为选择和专注于相关刺激的能力。换句话说，注意力是一种试图增强重要部分，同时淡出不相关信息的方法。</p><p id="9ba9" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">尽管这种机制可以分为几个家族(<a class="ae lh" href="https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html#a-family-of-attention-mechanisms" rel="noopener ugc nofollow" target="_blank">注意？立正！</a>)，我们关注<em class="me">自我注意力</em>，因为这是计算机视觉任务中最流行的注意力类型。这是指将单个序列的不同位置相关联来计算同一序列的表示的机制。</p><p id="2eb3" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">为了更好的理解这个概念，我们来想一下下面这个句子:<em class="me">河岸</em>。如果我们看不到单词<em class="me"> River </em>我们是否同意单词<em class="me"> Bank </em>失去了它的上下文信息？这实际上是自我关注背后的主要思想。它试图给出每个单词的上下文信息，因为单词的单个含义并不代表它们在句子中的含义。</p><p id="424c" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">正如<a class="ae lh" rel="noopener" target="_blank" href="/an-intuitive-explanation-of-self-attention-4f72709638e1">自我关注的直观解释</a>中所解释的，如果我们考虑上面给出的例子，自我关注的工作方式是将句子中的每个单词与其他每个单词进行比较，并重新加权每个单词的单词嵌入，以包括上下文相关性。输出模块的输入是没有上下文信息的每个单词的嵌入，而输出是具有上下文信息的类似嵌入。</p><h2 id="9381" class="na nb it bd nc nd ne dn nf ng nh dp ni lr nj nk nl lv nm nn no lz np nq nr iz bi translated">2.计算机视觉中的注意方法</h2><p id="ac0b" class="pw-post-body-paragraph li lj it lk b ll ns kd ln lo nt kg lq lr nu lt lu lv nv lx ly lz nw mb mc md im bi translated"><a class="ae lh" href="https://paperswithcode.com/methods/category/attention-modules" rel="noopener ugc nofollow" target="_blank">此处</a>列出了持续更新的关注模块列表。从列出的几个中，我们重点介绍两个最受计算机视觉任务欢迎的:<a class="ae lh" href="https://paperswithcode.com/method/multi-head-attention" rel="noopener ugc nofollow" target="_blank">多头注意力</a>和<a class="ae lh" href="https://paperswithcode.com/method/spatial-attention-module" rel="noopener ugc nofollow" target="_blank">卷积块注意力模块(CBAM) </a>。</p><p id="42f8" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><strong class="lk jd"> <em class="me"> 2.1。多头关注</em> </strong></p><p id="e6f3" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">多头注意力是注意力机制的一个模块，它并行运行一个注意力模块若干次。因此，要理解它的逻辑，首先需要理解注意力模块。两个最常用的注意力函数是<a class="ae lh" href="https://paperswithcode.com/method/additive-attention" rel="noopener ugc nofollow" target="_blank"><em class="me"/></a>和<a class="ae lh" href="https://paperswithcode.com/method/dot-product-attention" rel="noopener ugc nofollow" target="_blank"> <em class="me">点积注意力</em> </a>，后者是本工作感兴趣的一个。</p><p id="9ef8" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">关注模块的基本结构是有两个向量列表<em class="me"> x1 </em>和<em class="me"> x2 </em>，一个是关注的，另一个是出席的。向量<em class="me"> x2 </em>生成“查询”，而向量<em class="me"> x1 </em>创建“键”和“值”。关注函数背后的思想是将查询和设置的键值对映射到输出。输出计算为值的加权和，其中分配给每个值的权重由查询与相应关键字的兼容性函数计算"T17注意是您所需要的全部T18"。输出计算如下:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/3e7d2be3e687f48d0c4e3de9cf9702de.png" data-original-src="https://miro.medium.com/v2/resize:fit:1238/format:webp/1*4Yhw_zN7u1Lh_BvZoX4DRg.png"/></div></figure><p id="7059" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">正如在本<a class="ae lh" href="https://stats.stackexchange.com/questions/421935/what-exactly-are-keys-queries-and-values-in-attention-mechanisms" rel="noopener ugc nofollow" target="_blank">讨论</a>中提到的，键/值/查询概念来自检索系统。例如，当在Youtube上键入一个查询来搜索一些视频时，搜索引擎会将您的<strong class="lk jd">查询</strong>映射到一组<strong class="lk jd">键</strong>(视频标题、描述等)。)与数据库中的候选视频链接。然后，它会为您呈现最匹配的视频(<strong class="lk jd">值</strong>)。</p><p id="e3ad" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">在进入多头注意力之前，让我们运行这个点积注意力，多头注意力是本模块的一个扩展。下面是PyTorch中的实现。输入是<code class="fe ny nz oa ob b">[128, 32, 1, 256]</code>，其中128对应的是批次，32对应的是序列长度，1对应的是头数(对于多个关注头我们会增加)，256是特征数。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="oc od l"/></div></figure><p id="4560" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">输出是:</p><pre class="ks kt ku kv gt oe ob of og aw oh bi"><span id="463d" class="na nb it ob b gy oi oj l ok ol">attn_output: [128, 32, 1, 256], attn_weights: [128, 1, 32, 32]<br/>attn_output: [128, 32, 1, 256], attn_weights: [128, 1, 32, 16]</span></pre><p id="7890" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">从这个基本实现中可以得到一些启示:</p><ul class=""><li id="9f38" class="mf mg it lk b ll lm lo lp lr mh lv mi lz mj md om ml mm mn bi translated">输出将具有与查询输入大小相同的形状。</li><li id="7bae" class="mf mg it lk b ll mo lo mp lr mq lv mr lz ms md om ml mm mn bi translated">每个数据的注意力权重必须是一个矩阵，其中行数对应于查询的序列长度，列数对应于键的序列长度。</li><li id="95f7" class="mf mg it lk b ll mo lo mp lr mq lv mr lz ms md om ml mm mn bi translated">点积注意力中没有可学习的参数。</li></ul><p id="acf6" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">所以，回到多头注意力，这个人并行运行这个解释过的注意力模块几次。独立的注意力输出然后被连接并线性转换成期望的维度。下面是实现过程:</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="oc od l"/></div></figure><p id="8feb" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">输出是:</p><pre class="ks kt ku kv gt oe ob of og aw oh bi"><span id="3a9f" class="na nb it ob b gy oi oj l ok ol">attn_output: [128, 32, 256], attn_weights: [128, 8, 32, 32]<br/>attn_output: [128, 32, 256], attn_weights: [128, 8, 32, 32]</span></pre><p id="b34f" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">从代码中可以看出:</p><ul class=""><li id="690e" class="mf mg it lk b ll lm lo lp lr mh lv mi lz mj md om ml mm mn bi translated">例如，查询的线性层的输入是<code class="fe ny nz oa ob b">[128, 32, 256]</code>。然而，正如在这篇<a class="ae lh" href="https://stackoverflow.com/questions/58587057/multi-dimensional-inputs-in-pytorch-linear-method" rel="noopener ugc nofollow" target="_blank">文章</a>中提到的，<code class="fe ny nz oa ob b">Linear</code>层接受任意形状的张量，其中只有最后一个维度必须与你在构造函数中指定的<code class="fe ny nz oa ob b">in_features</code>参数相匹配。输出将具有与输入完全相同的形状，只有最后一个维度将改变为您在构造函数中指定的<code class="fe ny nz oa ob b">out_features</code>。对于我们的例子，输入形状是一组<code class="fe ny nz oa ob b">128 * 32 = 4096</code>和<code class="fe ny nz oa ob b">256</code>特征。因此，我们将密集网络应用于序列长度的每个元素和批次的每个数据。</li><li id="4876" class="mf mg it lk b ll mo lo mp lr mq lv mr lz ms md om ml mm mn bi translated">此外，我们添加了残余连接和层规范化，因为它是在变压器神经网络中实现的。但是，如果你只是想实现多头注意力模块，这些应该被排除在外。</li></ul><p id="659a" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">因此，在这一点上你可能会疑惑，为什么我们要实现多头注意力而不是一个简单的注意力模块？根据论文<a class="ae lh" href="https://arxiv.org/abs/1706.03762" rel="noopener ugc nofollow" target="_blank">注意力是你所需要的全部</a>，<em class="me">“多头注意力允许模型共同注意来自不同位置的不同表征</em> <strong class="lk jd"> <em class="me">子空间</em> </strong> <em class="me">的信息。用单一的注意力头，平均化抑制了这一点。”</em>换句话说，将特征划分为头部允许每个注意力模块仅关注一组特征，从而提供更大的能力来编码每个单词的多种关系和细微差别。</p><blockquote class="on oo op"><p id="0dd9" class="li lj me lk b ll lm kd ln lo lp kg lq oq ls lt lu or lw lx ly os ma mb mc md im bi translated">如果在这一点上你还想深入了解这种类型的注意力，我鼓励你阅读这篇<a class="ae lh" href="https://theaisummer.com/self-attention/" rel="noopener ugc nofollow" target="_blank">文章</a>，它用很棒的插图详细解释了所有这个模块。</p></blockquote><p id="3202" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">在结束之前，我只想提一下，我们已经使用了这个注意力模块，就好像我们在处理序列一样，但是这篇文章是关于图像的。如果你理解了这一点，序列和图像之间唯一的区别就是输入向量。对于图像来说，与序列长度相对应的是像素。因此，如果输入是<code class="fe ny nz oa ob b">[batch=128, no_channels=256, height=24, width=24]</code>，一个可能的实现可能是:</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="oc od l"/></div></figure><p id="6090" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">输出是:</p><pre class="ks kt ku kv gt oe ob of og aw oh bi"><span id="49ce" class="na nb it ob b gy oi oj l ok ol">attn_output: [128, 256, 24, 24], attn_weights: [128, 8, 576, 576]</span></pre><p id="87ac" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><strong class="lk jd"> <em class="me"> 2.2。卷积块注意模块(CBAM) </em> </strong></p><p id="a4c1" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">在2018年，<a class="ae lh" href="https://www.google.com/search?q=CBAM+attention&amp;oq=CBAM&amp;aqs=chrome.0.69i59j69i57j46i175i199j46i10i433j46i10i175i199j69i60l3.2289j0j7&amp;sourceid=chrome&amp;ie=UTF-8" rel="noopener ugc nofollow" target="_blank"> S. Woo等人(2018) </a>发表了一个新的注意力模块，名为卷积块注意力模块(CBAM)，它和卷积运算一样，强调了沿通道和空间轴的有意义的特征。与多头注意力相比，这种类型的注意力是针对前馈卷积神经网络有意制造的，并且可以应用于深度网络中的每个卷积块。</p><p id="9132" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">CBAM包含两个连续的子模块，称为通道注意模块(CAM)和空间注意模块(SAM)。这两个概念可能是谈论卷积时最重要的两个概念。通道是指每个像素的特征或通道的数量，而空间是指维度(h x w)的特征图。</p><ul class=""><li id="2883" class="mf mg it lk b ll lm lo lp lr mh lv mi lz mj md om ml mm mn bi translated"><em class="me">空间注意模块(SAM) </em>:该模块由三重顺序操作组成。它的第一部分称为通道池，它包括对输入(<em class="me">c</em>×<em class="me">h</em>×<em class="me">w</em>)应用跨通道的最大池和平均池，以生成具有shape (2 × <em class="me"> h </em> × <em class="me"> w </em>)的输出。这是卷积层的输入，卷积层输出一个单通道特征图(1 × <em class="me"> h </em> × <em class="me"> w </em>)。在通过BatchNorm和可选的ReLU传递这个输出之后，数据进入Sigmoid激活层。</li><li id="9378" class="mf mg it lk b ll mo lo mp lr mq lv mr lz ms md om ml mm mn bi translated"><em class="me">通道注意模块(CAM) </em>:该模块首先将输入张量分解成由全局平均池(GAP)和全局最大池(GMP)生成的2个后续维度向量(<em class="me"> c </em> × 1 × 1)。此后，输出通过完全连接层，然后是ReLu激活层。</li></ul><blockquote class="on oo op"><p id="a191" class="li lj me lk b ll lm kd ln lo lp kg lq oq ls lt lu or lw lx ly os ma mb mc md im bi translated">想了解更多关于CBAM的信息，我推荐阅读这篇伟大的<a class="ae lh" href="https://blog.paperspace.com/attention-mechanisms-in-computer-vision-cbam/" rel="noopener ugc nofollow" target="_blank">帖子</a>，里面有很棒的解释图片。</p></blockquote><p id="0c08" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">下面是实现过程:</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="oc od l"/></div></figure><p id="726b" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">输出是:</p><pre class="ks kt ku kv gt oe ob of og aw oh bi"><span id="ec23" class="na nb it ob b gy oi oj l ok ol">attn_output: [128, 256, 24, 24]</span></pre><h2 id="a9e5" class="na nb it bd nc nd ne dn nf ng nh dp ni lr nj nk nl lv nm nn no lz np nq nr iz bi translated">3.基于注意的网络的实现和结果</h2><p id="9457" class="pw-post-body-paragraph li lj it lk b ll ns kd ln lo nt kg lq lr nu lt lu lv nv lx ly lz nw mb mc md im bi translated">在上述理论部分之后，本节将重点介绍两个注意层在一个实际案例中的实现。</p><p id="01ce" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">具体来说，我们选择了STL数据集，并在一些图像中添加了白色补丁，如下图所示。任务是创建一个神经网络来分类这两种类型的图像。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="oc od l"/></div></figure><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi ot"><img src="../Images/f1de3da469a18294a01148e77e2614c6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1018/format:webp/1*rzmgXEF2VJbw6IAB2bA-Cw.png"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">STL图像。那些标记为1的图像属于其图像具有白色斑块的类别，而那些标记为0的图像是没有白色斑块的图像。</p></figure><p id="f1cc" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">然后，我们创建了三个类。第一个只是一个CNN，而第二个包含多头注意力层，第三个包括CBAM模块。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="oc od l"/></div></figure><p id="f906" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">下面是运行培训的代码。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="oc od l"/></div></figure><p id="4ba6" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">这些是输出结果:</p><ul class=""><li id="3c53" class="mf mg it lk b ll lm lo lp lr mh lv mi lz mj md om ml mm mn bi translated"><em class="me"> CNN </em>:</li></ul><pre class="ks kt ku kv gt oe ob of og aw oh bi"><span id="e36c" class="na nb it ob b gy oi oj l ok ol">Min train error: 0.0011167450276843738<br/>Min test error: 0.05411996720208516</span></pre><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="oc od l"/></div></figure><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi ou"><img src="../Images/1e82c5140c0a29db16d24683887974b8.png" data-original-src="https://miro.medium.com/v2/resize:fit:936/format:webp/1*p4e3CPAOyP0F70G8Ce4pjg.png"/></div></figure><ul class=""><li id="a611" class="mf mg it lk b ll lm lo lp lr mh lv mi lz mj md om ml mm mn bi translated"><em class="me"> CNN +多头关注</em>:添加关注层后性能有所提高，但关注图没有突出显示图像中带有白色斑块的部分。</li></ul><pre class="ks kt ku kv gt oe ob of og aw oh bi"><span id="5635" class="na nb it ob b gy oi oj l ok ol">Min train error: 9.811600781858942e-06<br/>Min test error: 0.04209221125441423</span></pre><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="oc od l"/></div></figure><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi ov"><img src="../Images/8ac71d5a12b758e89e825c9ece556d98.png" data-original-src="https://miro.medium.com/v2/resize:fit:866/format:webp/1*FLys6-Jn8ltEgN_EOcKN1Q.png"/></div></figure><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi ow"><img src="../Images/f18b0a6f468b8d4373f8d0ae6ab6e6b3.png" data-original-src="https://miro.medium.com/v2/resize:fit:966/format:webp/1*-M3Yg5KOTBmxXZFCzKGfrA.png"/></div></figure><p id="4a31" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">由于有一些过度拟合和注意力层没有做它应该做的，我用卷积层重新实现了这一层。如果有人有什么建议，请在评论中留下。</p><ul class=""><li id="4beb" class="mf mg it lk b ll lm lo lp lr mh lv mi lz mj md om ml mm mn bi translated"><em class="me">基于CNN + 1DConv的多头关注</em>:这一次稳定性和性能明显提高。此外，还可以观察到注意力层的输出是如何为包含它的图像高亮显示白色斑块的。</li></ul><pre class="ks kt ku kv gt oe ob of og aw oh bi"><span id="9d55" class="na nb it ob b gy oi oj l ok ol">Min train error: 0.00025470180017873645<br/>Min test error: 0.014278276459193759</span></pre><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="oc od l"/></div></figure><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi ox"><img src="../Images/5015cd860e7c7fc0cd0c5f8da31bc581.png" data-original-src="https://miro.medium.com/v2/resize:fit:944/format:webp/1*QOCS7BoIcytdIzs9aIA_YA.png"/></div></figure><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi oy"><img src="../Images/3758f1ee860fee78fa94a5b0dd522dfb.png" data-original-src="https://miro.medium.com/v2/resize:fit:972/format:webp/1*UUoe9kj1LoRbiWzb6FjCOg.png"/></div></figure><ul class=""><li id="9aef" class="mf mg it lk b ll lm lo lp lr mh lv mi lz mj md om ml mm mn bi translated">CNN + CBAM注意:这一个呈现了最好的结果。显然可以在注意力层的输出中观察到白色斑块，并且训练非常稳定，实现了所有模型中最低的验证损失。</li></ul><pre class="ks kt ku kv gt oe ob of og aw oh bi"><span id="6a2e" class="na nb it ob b gy oi oj l ok ol">Min train error: 2.786791462858673e-05<br/>Min test error: 0.028047989653949175</span></pre><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="oc od l"/></div></figure><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi oz"><img src="../Images/861a49eea222c5e0a1b2adf067488ab0.png" data-original-src="https://miro.medium.com/v2/resize:fit:964/format:webp/1*R-R4pZtxymddVm9625ke2w.png"/></div></figure><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi pa"><img src="../Images/6dbf822e06b2fd0ec92aad9623e1d43e.png" data-original-src="https://miro.medium.com/v2/resize:fit:974/format:webp/1*CVABUDkZ5xkv0etvt9harA.png"/></div></figure><h2 id="5702" class="na nb it bd nc nd ne dn nf ng nh dp ni lr nj nk nl lv nm nn no lz np nq nr iz bi translated">4.结论</h2><p id="296d" class="pw-post-body-paragraph li lj it lk b ll ns kd ln lo nt kg lq lr nu lt lu lv nv lx ly lz nw mb mc md im bi translated">总之，本文介绍了多头注意力和CBAM模块，这是计算机视觉中最流行的两个注意力模块。此外，它还包括PyTorch中的一个实现，其中我们对包含白色补丁(手动添加)的CIFAR数据集中的图像进行分类。</p><p id="c2d3" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">对于未来的工作，我认为将位置编码和注意力结合起来是很有趣的。在这里，我为感兴趣的人留下了一个链接。</p></div><div class="ab cl mt mu hx mv" role="separator"><span class="mw bw bk mx my mz"/><span class="mw bw bk mx my mz"/><span class="mw bw bk mx my"/></div><div class="im in io ip iq"><p id="6a67" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><strong class="lk jd"> <em class="me">如果你喜欢这篇文章，请考虑</em> </strong> <a class="ae lh" href="https://javiferfer.medium.com/membership" rel="noopener"> <strong class="lk jd"> <em class="me">订阅</em> </strong> </a> <strong class="lk jd"> <em class="me">。你将获得我所有的内容+所有其他来自牛逼创作者的文章！</em> </strong></p></div></div>    
</body>
</html>