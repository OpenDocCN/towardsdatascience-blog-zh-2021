<html>
<head>
<title>11 Times Faster Hyperparameter Tuning with HalvingGridSearch</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">HalvingGridSearch将超参数调谐速度提高了11倍</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/11-times-faster-hyperparameter-tuning-with-halvinggridsearch-232ed0160155?source=collection_archive---------8-----------------------#2021-04-09">https://towardsdatascience.com/11-times-faster-hyperparameter-tuning-with-halvinggridsearch-232ed0160155?source=collection_archive---------8-----------------------#2021-04-09</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="9be9" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">连续减半彻底粉碎了GridSearch和RandomSearch</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/37ebe71042c835a2d6484f5db735d70d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WZie52w_42pfJFarBj-5Bw.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd ky">照片由</strong> <a class="ae kz" href="https://www.pexels.com/@karolina-grabowska?utm_content=attributionCopyText&amp;utm_medium=referral&amp;utm_source=pexels" rel="noopener ugc nofollow" target="_blank"> <strong class="bd ky">卡罗林娜</strong> </a> <strong class="bd ky">上</strong> <a class="ae kz" href="https://www.pexels.com/photo/person-holding-tuning-pegs-4472108/?utm_content=attributionCopyText&amp;utm_medium=referral&amp;utm_source=pexels" rel="noopener ugc nofollow" target="_blank"> <strong class="bd ky">像素</strong> </a></p></figure><h2 id="975a" class="la lb it bd lc ld le dn lf lg lh dp li lj lk ll lm ln lo lp lq lr ls lt lu lv bi translated">介绍</h2><p id="944d" class="pw-post-body-paragraph lw lx it ly b lz ma ju mb mc md jx me lj mf mg mh ln mi mj mk lr ml mm mn mo im bi translated">一段时间以来，Scikit-learn的<code class="fe mp mq mr ms b">GridSearchCV</code>和<code class="fe mp mq mr ms b">RandomizedSearchCV</code>类一直是超参数调优的首选。给定一个可能参数的网格，两者都使用蛮力方法来计算任何给定模型的最佳超参数集。虽然它们提供了非常健壮的结果，但是在大型数据集上调整较重的模型会花费太多时间(这里我们说的是几个小时)。这意味着除非你有一台16核以上的机器，否则你就有麻烦了。</p><p id="c58b" class="pw-post-body-paragraph lw lx it ly b lz mt ju mb mc mu jx me lj mv mg mh ln mw mj mk lr mx mm mn mo im bi translated">但是在2020年12月，Scikit-learn的0.24版本推出了两个新的超参数调优类— <code class="fe mp mq mr ms b">HalvingGridSearch</code>和<code class="fe mp mq mr ms b">HalvingRandomSearchCV</code>。在<a class="ae kz" href="https://scikit-learn.org/stable/modules/grid_search.html#exhausting-the-resources" rel="noopener ugc nofollow" target="_blank">官方用户指南</a>中，Scikit-learned声称“他们可以更快地找到一个好的参数组合”，天哪，他们是对的吗！</p><p id="388b" class="pw-post-body-paragraph lw lx it ly b lz mt ju mb mc mu jx me lj mv mg mh ln mw mj mk lr mx mm mn mo im bi translated">在我比较<code class="fe mp mq mr ms b">GridSearchCV</code>和<code class="fe mp mq mr ms b">HalvingGridSearchCV</code>的第一个实验中，后者找到最佳超参数集的速度比GridSearch快11倍。在第二个实验中，我将参数网格的大小增加了30倍(大约3000个最终候选)，我使用了<code class="fe mp mq mr ms b">HalvingRandomSearchCV</code>,结果性能提高了6%,整个过程只花了10分钟。</p><p id="7965" class="pw-post-body-paragraph lw lx it ly b lz mt ju mb mc mu jx me lj mv mg mh ln mw mj mk lr mx mm mn mo im bi translated">请继续阅读，了解这两个非凡的类是如何工作的，并学习如何在自己的工作流程中使用它们。</p><div class="my mz gp gr na nb"><a href="https://ibexorigin.medium.com/membership" rel="noopener follow" target="_blank"><div class="nc ab fo"><div class="nd ab ne cl cj nf"><h2 class="bd iu gy z fp ng fr fs nh fu fw is bi translated">通过我的推荐链接加入Medium-BEXGBoost</h2><div class="ni l"><h3 class="bd b gy z fp ng fr fs nh fu fw dk translated">获得独家访问我的所有⚡premium⚡内容和所有媒体没有限制。支持我的工作，给我买一个…</h3></div><div class="nj l"><p class="bd b dl z fp ng fr fs nh fu fw dk translated">ibexorigin.medium.com</p></div></div><div class="nk l"><div class="nl l nm nn no nk np ks nb"/></div></div></a></div><p id="4b9f" class="pw-post-body-paragraph lw lx it ly b lz mt ju mb mc mu jx me lj mv mg mh ln mw mj mk lr mx mm mn mo im bi translated">获得由强大的AI-Alpha信号选择和总结的最佳和最新的ML和AI论文:</p><div class="my mz gp gr na nb"><a href="https://alphasignal.ai/?referrer=Bex" rel="noopener  ugc nofollow" target="_blank"><div class="nc ab fo"><div class="nd ab ne cl cj nf"><h2 class="bd iu gy z fp ng fr fs nh fu fw is bi translated">阿尔法信号|机器学习的极品。艾总结的。</h2><div class="ni l"><h3 class="bd b gy z fp ng fr fs nh fu fw dk translated">留在循环中，不用花无数时间浏览下一个突破；我们的算法识别…</h3></div><div class="nj l"><p class="bd b dl z fp ng fr fs nh fu fw dk translated">alphasignal.ai</p></div></div><div class="nk l"><div class="nq l nm nn no nk np ks nb"/></div></div></a></div></div><div class="ab cl nr ns hx nt" role="separator"><span class="nu bw bk nv nw nx"/><span class="nu bw bk nv nw nx"/><span class="nu bw bk nv nw"/></div><div class="im in io ip iq"><h2 id="d429" class="la lb it bd lc ld le dn lf lg lh dp li lj lk ll lm ln lo lp lq lr ls lt lu lv bi translated">术语注释</h2><p id="5479" class="pw-post-body-paragraph lw lx it ly b lz ma ju mb mc md jx me lj mf mg mh ln mi mj mk lr ml mm mn mo im bi translated">在我们继续之前，让我们确保我们对我今天将使用的一些术语有相同的理解。</p><p id="686c" class="pw-post-body-paragraph lw lx it ly b lz mt ju mb mc mu jx me lj mv mg mh ln mw mj mk lr mx mm mn mo im bi translated"><strong class="ly iu"> 1。超参数</strong>:应该由用户设置的模型内部设置。模型无法从训练数据中学习这些。一个例子是<code class="fe mp mq mr ms b">xgboost</code>估计器中的学习率。</p><p id="f6e0" class="pw-post-body-paragraph lw lx it ly b lz mt ju mb mc mu jx me lj mv mg mh ln mw mj mk lr mx mm mn mo im bi translated"><strong class="ly iu"> 2。参数网格</strong>:一个字典，以参数名作为键，以可能的超参数列表作为值。以下是<code class="fe mp mq mr ms b">XGBClassifier</code>的参数网格示例:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ny nz l"/></div></figure><p id="4c62" class="pw-post-body-paragraph lw lx it ly b lz mt ju mb mc mu jx me lj mv mg mh ln mw mj mk lr mx mm mn mo im bi translated">参数网格的大小或所有可能的组合通过乘以每个参数的可能值的数量来计算。所以，上面的网格有4 * 3 * 3 = 36种可能的组合。一般来说，参数网格会比这个大得多。</p><p id="d746" class="pw-post-body-paragraph lw lx it ly b lz mt ju mb mc mu jx me lj mv mg mh ln mw mj mk lr mx mm mn mo im bi translated"><strong class="ly iu"> 3。候选者</strong>:参数网格中所有可能的超参数集合的单一组合。</p><p id="5bdb" class="pw-post-body-paragraph lw lx it ly b lz mt ju mb mc mu jx me lj mv mg mh ln mw mj mk lr mx mm mn mo im bi translated"><strong class="ly iu"> 4。资源或样本</strong>:手头数据的别称。一个示例引用训练数据中的一行。</p><p id="1380" class="pw-post-body-paragraph lw lx it ly b lz mt ju mb mc mu jx me lj mv mg mh ln mw mj mk lr mx mm mn mo im bi translated"><strong class="ly iu"> 5。迭代</strong>:对训练数据使用一组超参数的任何一轮。</p></div><div class="ab cl nr ns hx nt" role="separator"><span class="nu bw bk nv nw nx"/><span class="nu bw bk nv nw nx"/><span class="nu bw bk nv nw"/></div><div class="im in io ip iq"><h2 id="7f35" class="la lb it bd lc ld le dn lf lg lh dp li lj lk ll lm ln lo lp lq lr ls lt lu lv bi translated"><code class="fe mp mq mr ms b">GridSearchCV</code>和<code class="fe mp mq mr ms b">RanomizedSearchCV</code>概述</h2><p id="b809" class="pw-post-body-paragraph lw lx it ly b lz ma ju mb mc md jx me lj mf mg mh ln mi mj mk lr ml mm mn mo im bi translated">因为新类的主要思想与GridSearch和RandomSearch相关，所以让我简单介绍一下它们是如何工作的。</p><p id="e0a2" class="pw-post-body-paragraph lw lx it ly b lz mt ju mb mc mu jx me lj mv mg mh ln mw mj mk lr mx mm mn mo im bi translated">GridSearch是一个详尽的强力估算工具。这意味着将使用交叉验证来训练超参数的所有组合。如果有100个可能的候选项，并且您正在进行5重交叉验证，则给定的模型将被训练500次(500次迭代)。当然，对于笨重的模型来说，这需要非常长的时间。</p><p id="72cb" class="pw-post-body-paragraph lw lx it ly b lz mt ju mb mc mu jx me lj mv mg mh ln mw mj mk lr mx mm mn mo im bi translated">RandomizedSearch试图通过对每次迭代中选择哪组参数做出“更聪明”的选择来控制迭代次数。它有一个额外的<code class="fe mp mq mr ms b">n_iter</code>参数直接控制这个过程。如果有1000个候选项并且<code class="fe mp mq mr ms b">n_iter</code>被设置为100，搜索将在第100次迭代后停止，并从这100个中返回最佳结果。这种随机选择过程导致训练时间大大缩短，但其性能不如GridSearch。</p><p id="a782" class="pw-post-body-paragraph lw lx it ly b lz mt ju mb mc mu jx me lj mv mg mh ln mw mj mk lr mx mm mn mo im bi translated">如果你想进一步了解它们，并看到它们的实际应用，请查看我关于这个主题的另一篇文章:</p><div class="my mz gp gr na nb"><a rel="noopener follow" target="_blank" href="/automatic-hyperparameter-tuning-with-sklearn-gridsearchcv-and-randomizedsearchcv-e94f53a518ee"><div class="nc ab fo"><div class="nd ab ne cl cj nf"><h2 class="bd iu gy z fp ng fr fs nh fu fw is bi translated">使用Sklearn GridSearchCV和RandomizedSearchCV自动调整超参数</h2><div class="ni l"><h3 class="bd b gy z fp ng fr fs nh fu fw dk translated">无需再次查看超参数即可调整模型</h3></div><div class="nj l"><p class="bd b dl z fp ng fr fs nh fu fw dk translated">towardsdatascience.com</p></div></div><div class="nk l"><div class="oa l nm nn no nk np ks nb"/></div></div></a></div></div><div class="ab cl nr ns hx nt" role="separator"><span class="nu bw bk nv nw nx"/><span class="nu bw bk nv nw nx"/><span class="nu bw bk nv nw"/></div><div class="im in io ip iq"><h2 id="5eb3" class="la lb it bd lc ld le dn lf lg lh dp li lj lk ll lm ln lo lp lq lr ls lt lu lv bi translated">什么是连续减半？</h2><p id="3fa7" class="pw-post-body-paragraph lw lx it ly b lz ma ju mb mc md jx me lj mf mg mh ln mi mj mk lr ml mm mn mo im bi translated">虽然GridSearch和RandomizedSearch都使用所有的训练数据来训练候选人，但是HalvingGridSearch和HalvingRandomSearch采用了一种不同的方法，称为<em class="ob">连续减半</em>。让我们看看它在HalvingGridSearch (HGS)方面意味着什么。</p><p id="df2f" class="pw-post-body-paragraph lw lx it ly b lz mt ju mb mc mu jx me lj mv mg mh ln mw mj mk lr mx mm mn mo im bi translated">HGS就像所有候选人之间的竞争(超参数组合)。在第一次迭代中，HGS用一小部分训练数据训练所有候选人。在下一次迭代中，只有表现最好的候选人才会被选中，他们将获得更多的竞争资源。因此，随着每一次迭代,“幸存”的候选者将被给予越来越多的资源(训练样本),直到最佳超参数集保持不变。</p><p id="abbc" class="pw-post-body-paragraph lw lx it ly b lz mt ju mb mc mu jx me lj mv mg mh ln mw mj mk lr mx mm mn mo im bi translated">现在，让我们更细化。上述过程的速度可以由两个参数控制— <code class="fe mp mq mr ms b">factor</code>和<code class="fe mp mq mr ms b">min_samples</code>。<code class="fe mp mq mr ms b">min_samples</code>取一个整数来指定在第一次迭代中使用的训练数据的样本数。所有候选人都在这个数据上被训练，并且在下一次迭代中<code class="fe mp mq mr ms b">min_samples</code>增加<code class="fe mp mq mr ms b">factor</code>，并且候选人的数量减少<code class="fe mp mq mr ms b">factor</code>。所有下一轮都以这种方式继续，直到找到最佳候选人。</p><p id="7380" class="pw-post-body-paragraph lw lx it ly b lz mt ju mb mc mu jx me lj mv mg mh ln mw mj mk lr mx mm mn mo im bi translated">通过一个例子让这个想法深入人心，假设我们在参数网格中有1000个样本和20个候选者。如果我们将<code class="fe mp mq mr ms b">min_samples</code>设置为20，并选择<code class="fe mp mq mr ms b">factor</code>为2，那么迭代将会这样展开:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="ab gu cl oc"><img src="../Images/e4df36aefa309c04ff45ee6d759cb0c3.png" data-original-src="https://miro.medium.com/v2/format:webp/1*7EhlgcNj29MIm97NDszlXw.png"/></div></figure><p id="6c84" class="pw-post-body-paragraph lw lx it ly b lz mt ju mb mc mu jx me lj mv mg mh ln mw mj mk lr mx mm mn mo im bi translated">将<code class="fe mp mq mr ms b">min_resources</code>设置为20，有20个候选，我们只能运行4次迭代，因为我们将在用尽所有样本之前用完所有候选。这意味着剩余的训练数据(1000 - 160 = 840)将被浪费，并且最佳候选仅通过仅在160行数据上进行训练来找到。</p><p id="10ac" class="pw-post-body-paragraph lw lx it ly b lz mt ju mb mc mu jx me lj mv mg mh ln mw mj mk lr mx mm mn mo im bi translated">同样，我们也可能在所有候选人都被试用之前就用完了样本。例如，假设我们有1000个样本和300个候选人。我们将<code class="fe mp mq mr ms b">min_samples</code>设置为50，并选择因子2:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="ab gu cl oc"><img src="../Images/b236023b908cae2cbf700ef5f4b5ad60.png" data-original-src="https://miro.medium.com/v2/format:webp/1*YXNUDnekWlsXoRPvPs4MbA.png"/></div></figure><p id="7ce2" class="pw-post-body-paragraph lw lx it ly b lz mt ju mb mc mu jx me lj mv mg mh ln mw mj mk lr mx mm mn mo im bi translated">如您所见，在第五次迭代中，我们没有足够的资源来进一步翻倍，我们只剩下18个最终候选项。这些最终候选人别无选择，只能在完整的数据集上接受训练，这与普通的老式网格搜索没有什么不同。这个问题在真实世界的数据集上更加明显。</p><p id="1362" class="pw-post-body-paragraph lw lx it ly b lz mt ju mb mc mu jx me lj mv mg mh ln mw mj mk lr mx mm mn mo im bi translated">例如，我们今天将使用的数据集有145，000个样本。因此，我们需要确保我们选择了这样一个<code class="fe mp mq mr ms b">factor</code>和<code class="fe mp mq mr ms b">min_samples</code>的组合，在最后一次迭代中，我们将得到尽可能多的未浪费的资源。</p><p id="01bd" class="pw-post-body-paragraph lw lx it ly b lz mt ju mb mc mu jx me lj mv mg mh ln mw mj mk lr mx mm mn mo im bi translated">这听起来确实有很多猜测，但幸运的是，您可以将<code class="fe mp mq mr ms b">exhaust</code>传递给<code class="fe mp mq mr ms b">min_samples</code>，这样最小数量的资源将被自动确定，以创建与<code class="fe mp mq mr ms b">factor</code>和大量候选人的最佳可能组合。例如，对于1000个样本和因子2，将<code class="fe mp mq mr ms b">min_samples</code>设置为<code class="fe mp mq mr ms b">exhaust</code>会将其设置为250，在我们进行每次迭代时，这将变成250、500、1000个样本。</p><p id="f544" class="pw-post-body-paragraph lw lx it ly b lz mt ju mb mc mu jx me lj mv mg mh ln mw mj mk lr mx mm mn mo im bi translated"><a class="ae kz" href="https://scikit-learn.org/stable/modules/grid_search.html#exhausting-the-resources" rel="noopener ugc nofollow" target="_blank">官方指南</a>称，耗尽样本数量肯定会导致参数选择更加稳健，但可能会多花一点时间。在接下来的部分中，我们将探索新的类比它们的同类好多少。</p></div><div class="ab cl nr ns hx nt" role="separator"><span class="nu bw bk nv nw nx"/><span class="nu bw bk nv nw nx"/><span class="nu bw bk nv nw"/></div><div class="im in io ip iq"><h2 id="2e91" class="la lb it bd lc ld le dn lf lg lh dp li lj lk ll lm ln lo lp lq lr ls lt lu lv bi translated">HalvingGridSearchCV与旧GridSearchCV的比较</h2><p id="bf7c" class="pw-post-body-paragraph lw lx it ly b lz ma ju mb mc md jx me lj mf mg mh ln mi mj mk lr ml mm mn mo im bi translated">为了比较这两个类，我将使用澳大利亚的<a class="ae kz" href="https://www.kaggle.com/jsphyg/weather-dataset-rattle-package" rel="noopener ugc nofollow" target="_blank">Rain</a>数据集来预测今天是否下雨。为了只关注手头的主题，我创建了包含所有必要预处理步骤的<code class="fe mp mq mr ms b">prep.py</code>文件:处理缺失值、缩放数字特征和编码分类变量(您可以在<a class="ae kz" href="https://gist.github.com/BexTuychiev/64f68715c6e5a0cafe2a7ce595a03eb3" rel="noopener ugc nofollow" target="_blank">GitHub gist</a>中获得该文件)。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ny nz l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="ab gu cl oc"><img src="../Images/412481a8a5a3e70efa8e67cdfeb95042.png" data-original-src="https://miro.medium.com/v2/format:webp/1*-0TO2LFfYl7EgmuDEsAMkA.png"/></div></figure><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ny nz l"/></div></figure><p id="5e9e" class="pw-post-body-paragraph lw lx it ly b lz mt ju mb mc mu jx me lj mv mg mh ln mw mj mk lr mx mm mn mo im bi translated">运行<code class="fe mp mq mr ms b">preprocess</code>函数会返回一个干净的、经过处理的特征和目标数组，我们将以传统的名称保存它们:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ny nz l"/></div></figure><p id="4e58" class="pw-post-body-paragraph lw lx it ly b lz mt ju mb mc mu jx me lj mv mg mh ln mw mj mk lr mx mm mn mo im bi translated">目标变量是<code class="fe mp mq mr ms b">RainToday</code>，作为基本估计量，我们将使用XGBoostClassifier。让我们首先通过用默认参数拟合来建立基础分数:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ny nz l"/></div></figure><p id="4a02" class="pw-post-body-paragraph lw lx it ly b lz mt ju mb mc mu jx me lj mv mg mh ln mw mj mk lr mx mm mn mo im bi translated">要计算ROC_AUC得分，应该对目标和预测进行编码，但它们不是:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ny nz l"/></div></figure><p id="e3d2" class="pw-post-body-paragraph lw lx it ly b lz mt ju mb mc mu jx me lj mv mg mh ln mw mj mk lr mx mm mn mo im bi translated">让我们用<code class="fe mp mq mr ms b">LabelEncoder</code>对它们进行编码:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ny nz l"/></div></figure><p id="d81d" class="pw-post-body-paragraph lw lx it ly b lz mt ju mb mc mu jx me lj mv mg mh ln mw mj mk lr mx mm mn mo im bi translated">我们的基本分数是0.73。现在，我们将尝试通过仅调整4个超参数来对此进行改进:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ny nz l"/></div></figure><blockquote class="od oe of"><p id="35cc" class="lw lx ob ly b lz mt ju mb mc mu jx me og mv mg mh oh mw mj mk oi mx mm mn mo im bi translated">如果你不熟悉XGBoost以及它的参数是什么，可以看看我在上面写的初学者友好的文章<a class="ae kz" rel="noopener" target="_blank" href="/beginners-guide-to-xgboost-for-classification-problems-50f75aac5390?source=your_stories_page-------------------------------------">。</a></p></blockquote><p id="e06a" class="pw-post-body-paragraph lw lx it ly b lz mt ju mb mc mu jx me lj mv mg mh ln mw mj mk lr mx mm mn mo im bi translated">候选人数为108人。首先，我们将使用五重交叉验证进行详尽的GridSearch，这是两个类中的默认设置。度量仍然是<code class="fe mp mq mr ms b">roc_auc</code>:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ny nz l"/></div></figure><p id="b606" class="pw-post-body-paragraph lw lx it ly b lz mt ju mb mc mu jx me lj mv mg mh ln mw mj mk lr mx mm mn mo im bi translated">一个多小时后，我们找到了最佳参数和分数:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ny nz l"/></div></figure><p id="d434" class="pw-post-body-paragraph lw lx it ly b lz mt ju mb mc mu jx me lj mv mg mh ln mw mj mk lr mx mm mn mo im bi translated">令人惊讶的是，<code class="fe mp mq mr ms b">roc_auc</code>的分数下降了一点。也许，我们没有提供足够好的参数网格。然而，为了便于比较，让我们使用<code class="fe mp mq mr ms b">HalvingGridSearchCV</code>进行同样的调优。它也是从<code class="fe mp mq mr ms b">sklearn.model_selection</code>进口的，你也应该进口<code class="fe mp mq mr ms b">enable_halving_search_cv</code>，因为它仍然是一个实验性的功能。</p><p id="1efc" class="pw-post-body-paragraph lw lx it ly b lz mt ju mb mc mu jx me lj mv mg mh ln mw mj mk lr mx mm mn mo im bi translated">如我所说，我们将把<code class="fe mp mq mr ms b">min_resources</code>设置为<code class="fe mp mq mr ms b">exhaust</code>，并选择Scikit-learn推荐的因子3:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ny nz l"/></div></figure><p id="0248" class="pw-post-body-paragraph lw lx it ly b lz mt ju mb mc mu jx me lj mv mg mh ln mw mj mk lr mx mm mn mo im bi translated">哇！运行时间显著减少——几乎比GridSearch快11倍。现在让我们来看看表演:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ny nz l"/></div></figure><p id="3020" class="pw-post-body-paragraph lw lx it ly b lz mt ju mb mc mu jx me lj mv mg mh ln mw mj mk lr mx mm mn mo im bi translated">如你所见，结果几乎相同。但是HGS比<code class="fe mp mq mr ms b">GridSearchCV</code>快了11倍！</p></div><div class="ab cl nr ns hx nt" role="separator"><span class="nu bw bk nv nw nx"/><span class="nu bw bk nv nw nx"/><span class="nu bw bk nv nw"/></div><div class="im in io ip iq"><h2 id="4243" class="la lb it bd lc ld le dn lf lg lh dp li lj lk ll lm ln lo lp lq lr ls lt lu lv bi translated">评估HalvingRandomSearchCV</h2><p id="a9a0" class="pw-post-body-paragraph lw lx it ly b lz ma ju mb mc md jx me lj mf mg mh ln mi mj mk lr ml mm mn mo im bi translated">这一次，让我们用更好的超参数创建一个更大的网格。</p><p id="b4f3" class="pw-post-body-paragraph lw lx it ly b lz mt ju mb mc mu jx me lj mv mg mh ln mw mj mk lr mx mm mn mo im bi translated">首先，<a class="ae kz" href="https://xgboost.readthedocs.io/en/latest/parameter.html" rel="noopener ugc nofollow" target="_blank"> XGBoost文档</a>说要考虑到类不平衡，我们应该调整<code class="fe mp mq mr ms b">scale_pos_weight</code>。结果表明，要找到最优值，我们应该用正类的和除以负类的和。让我们开始吧:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ny nz l"/></div></figure><p id="73fa" class="pw-post-body-paragraph lw lx it ly b lz mt ju mb mc mu jx me lj mv mg mh ln mw mj mk lr mx mm mn mo im bi translated">我们找到了<code class="fe mp mq mr ms b">scale_pos_weight</code>。现在，我们还修正了<code class="fe mp mq mr ms b">subsample</code>和<code class="fe mp mq mr ms b">colsample_bytree</code>的值，以避免过度拟合。这次让我们提供更多可能的值，因为模型可能受到了太多的约束。</p><p id="6cfa" class="pw-post-body-paragraph lw lx it ly b lz mt ju mb mc mu jx me lj mv mg mh ln mw mj mk lr mx mm mn mo im bi translated">上次，<code class="fe mp mq mr ms b">gamma</code>的最佳值被发现是1，这是它的范围的结尾，所以我们也应该扩展它:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ny nz l"/></div></figure><p id="da92" class="pw-post-body-paragraph lw lx it ly b lz mt ju mb mc mu jx me lj mv mg mh ln mw mj mk lr mx mm mn mo im bi translated">现在，我们有了一个相当大的网格，有将近3000个候选人。让我们看看<code class="fe mp mq mr ms b">HalvingRandomSearchCV</code>是怎么回事(通过将<code class="fe mp mq mr ms b">n_candidates</code>设置为<code class="fe mp mq mr ms b">exhaust</code>来耗尽该类中的所有可用资源):</p><blockquote class="od oe of"><p id="a1f5" class="lw lx ob ly b lz mt ju mb mc mu jx me og mv mg mh oh mw mj mk oi mx mm mn mo im bi translated">在看到HGS碾压GridSearch之后，我甚至懒得去比较<code class="fe mp mq mr ms b">HalvingRandomSearchCV</code>和普通的<code class="fe mp mq mr ms b">RandomizedSearchCV</code>。显然，连续减半比两种老方法都好。</p></blockquote><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ny nz l"/></div></figure><p id="2a96" class="pw-post-body-paragraph lw lx it ly b lz mt ju mb mc mu jx me lj mv mg mh ln mw mj mk lr mx mm mn mo im bi translated">对于一个相当大的网格，只需要10分钟就可以找到最好的分数。此外，这次我们看到了6%的分数增长，这是HGS和GridSearch都做不到的。</p><p id="6408" class="pw-post-body-paragraph lw lx it ly b lz mt ju mb mc mu jx me lj mv mg mh ln mw mj mk lr mx mm mn mo im bi translated">你印象深刻了吗？</p></div><div class="ab cl nr ns hx nt" role="separator"><span class="nu bw bk nv nw nx"/><span class="nu bw bk nv nw nx"/><span class="nu bw bk nv nw"/></div><div class="im in io ip iq"><h2 id="a853" class="la lb it bd lc ld le dn lf lg lh dp li lj lk ll lm ln lo lp lq lr ls lt lu lv bi translated">结论</h2><p id="8152" class="pw-post-body-paragraph lw lx it ly b lz ma ju mb mc md jx me lj mf mg mh ln mi mj mk lr ml mm mn mo im bi translated">在完成上述实验后，我已经下定决心要转到新的班级。</p><p id="624e" class="pw-post-body-paragraph lw lx it ly b lz mt ju mb mc mu jx me lj mv mg mh ln mw mj mk lr mx mm mn mo im bi translated">你应该注意到，我们只有通过调整新类的最基本设置才能达到如此高的性能。根据文档，这两个类都提供了更多的设置来控制速度和性能，所以也值得一试:</p><ul class=""><li id="aa1f" class="oj ok it ly b lz mt mc mu lj ol ln om lr on mo oo op oq or bi translated"><a class="ae kz" href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.HalvingGridSearchCV.html" rel="noopener ugc nofollow" target="_blank">halvinggridsearccv文档</a></li><li id="208d" class="oj ok it ly b lz os mc ot lj ou ln ov lr ow mo oo op oq or bi translated"><a class="ae kz" href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.HalvingRandomSearchCV.html" rel="noopener ugc nofollow" target="_blank"> HalvingRandomSearchCV文档</a></li><li id="762e" class="oj ok it ly b lz os mc ot lj ou ln ov lr ow mo oo op oq or bi translated"><a class="ae kz" href="https://scikit-learn.org/stable/modules/grid_search.html#exhausting-the-resources" rel="noopener ugc nofollow" target="_blank">官方Sklearn用户指南关于参数调整</a></li></ul><p id="a277" class="pw-post-body-paragraph lw lx it ly b lz mt ju mb mc mu jx me lj mv mg mh ln mw mj mk lr mx mm mn mo im bi translated">如果您感兴趣，XGBoost参数调整的官方指南:</p><ul class=""><li id="dcbf" class="oj ok it ly b lz mt mc mu lj ol ln om lr on mo oo op oq or bi translated"><a class="ae kz" href="https://xgboost.readthedocs.io/en/latest/parameter.html" rel="noopener ugc nofollow" target="_blank">XG boost参数概述</a></li><li id="33cf" class="oj ok it ly b lz os mc ot lj ou ln ov lr ow mo oo op oq or bi translated"><a class="ae kz" href="https://xgboost.readthedocs.io/en/latest/tutorials/param_tuning.html" rel="noopener ugc nofollow" target="_blank">XG boost参数整定注意事项</a></li></ul><p id="05cf" class="pw-post-body-paragraph lw lx it ly b lz mt ju mb mc mu jx me lj mv mg mh ln mw mj mk lr mx mm mn mo im bi translated">不要忘记进行你自己的实验并分享结果！</p></div><div class="ab cl nr ns hx nt" role="separator"><span class="nu bw bk nv nw nx"/><span class="nu bw bk nv nw nx"/><span class="nu bw bk nv nw"/></div><div class="im in io ip iq"><h1 id="5804" class="ox lb it bd lc oy oz pa lf pb pc pd li jz pe ka lm kc pf kd lq kf pg kg lu ph bi translated">如果你不知道接下来要读什么，这里，我为你挑选了一些:</h1><div class="my mz gp gr na nb"><a href="https://towardsdev.com/how-to-use-plotly-as-pandas-plotting-backend-123ff5378003" rel="noopener  ugc nofollow" target="_blank"><div class="nc ab fo"><div class="nd ab ne cl cj nf"><h2 class="bd iu gy z fp ng fr fs nh fu fw is bi translated">如何使用Plotly作为熊猫绘图后端</h2><div class="ni l"><h3 class="bd b gy z fp ng fr fs nh fu fw dk translated">无需学习新的库就可以制作互动的情节</h3></div><div class="nj l"><p class="bd b dl z fp ng fr fs nh fu fw dk translated">towardsdev.com</p></div></div><div class="nk l"><div class="pi l nm nn no nk np ks nb"/></div></div></a></div><div class="my mz gp gr na nb"><a rel="noopener follow" target="_blank" href="/intro-to-regularization-with-ridge-and-lasso-regression-with-sklearn-edcf4c117b7a"><div class="nc ab fo"><div class="nd ab ne cl cj nf"><h2 class="bd iu gy z fp ng fr fs nh fu fw is bi translated">介绍使用Sklearn的岭和套索回归正则化</h2><div class="ni l"><h3 class="bd b gy z fp ng fr fs nh fu fw dk translated">你还不如放弃线性回归</h3></div><div class="nj l"><p class="bd b dl z fp ng fr fs nh fu fw dk translated">towardsdatascience.com</p></div></div><div class="nk l"><div class="pj l nm nn no nk np ks nb"/></div></div></a></div><div class="my mz gp gr na nb"><a rel="noopener follow" target="_blank" href="/intro-to-scikit-learns-k-nearest-neighbors-classifier-and-regressor-4228d8d1cba6"><div class="nc ab fo"><div class="nd ab ne cl cj nf"><h2 class="bd iu gy z fp ng fr fs nh fu fw is bi translated">Scikit-learn的k近邻分类器和回归器简介</h2><div class="ni l"><h3 class="bd b gy z fp ng fr fs nh fu fw dk translated">了解掌握它们需要什么</h3></div><div class="nj l"><p class="bd b dl z fp ng fr fs nh fu fw dk translated">towardsdatascience.com</p></div></div><div class="nk l"><div class="pk l nm nn no nk np ks nb"/></div></div></a></div></div></div>    
</body>
</html>