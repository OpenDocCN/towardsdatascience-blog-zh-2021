<html>
<head>
<title>Nonlinear Optimization Using Halley’s Method</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">利用哈雷方法的非线性优化</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/nonlinear-optimization-using-halleys-method-2f2e7da4024?source=collection_archive---------25-----------------------#2021-08-04">https://towardsdatascience.com/nonlinear-optimization-using-halleys-method-2f2e7da4024?source=collection_archive---------25-----------------------#2021-08-04</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="f853" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph">优化。数据科学。分析。Python。</h2><div class=""/><div class=""><h2 id="b4fb" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">用Python从头开始构建Halley的方法优化器</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/ffa7fe3b34e98a99077eeb8459cfe42c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*uH-tSXGUa2BcUUav"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">由<a class="ae lh" href="https://unsplash.com/@volkanolmez?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">沃尔坎·奥尔梅斯</a>在<a class="ae lh" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>拍摄的照片</p></figure><h1 id="f2da" class="li lj it bd lk ll lm ln lo lp lq lr ls ki lt kj lu kl lv km lw ko lx kp ly lz bi translated">介绍</h1><p id="fc3c" class="pw-post-body-paragraph ma mb it mc b md me kd mf mg mh kg mi mj mk ml mm mn mo mp mq mr ms mt mu mv im bi translated">一些人认为哈雷方法是牛顿方法的延伸。关于它的发现的故事实际上相当精彩，但是我们把它留给另外一个阅读。</p><p id="083a" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">与其他优化算法相比，这种算法的优势在于它被认为收敛更快。然而，隐藏的成本是，它需要计算二阶导数函数。</p><p id="be3c" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">对于二阶导数很容易计算的方程来说，这个方程是非常有效和惊人的！</p><p id="d82b" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">要了解使用牛顿法解决这个问题的更多信息，请参考<a class="ae lh" href="https://medium.com/mlearning-ai/nonlinear-optimization-using-newtons-method-1e8ca91432d8" rel="noopener">使用牛顿法的非线性优化</a>。</p><h1 id="4d37" class="li lj it bd lk ll lm ln lo lp lq lr ls ki lt kj lu kl lv km lw ko lx kp ly lz bi translated"><strong class="ak">求根与优化算法</strong></h1><p id="02f3" class="pw-post-body-paragraph ma mb it mc b md me kd mf mg mh kg mi mj mk ml mm mn mo mp mq mr ms mt mu mv im bi translated">现在，我们将要做的是优化专家的常识，但是被大多数盲目应用这种方法的人遗忘了。</p><p id="441c" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">求根算法寻求找到一个函数的所谓临界点；使函数等于零(0)的函数。</p><p id="6a0a" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">牛顿和哈雷的方法都被认为是求根算法。微积分告诉我们，要找到一个函数的最佳点，我们需要计算它的一阶导数，并将其设置为0。</p><p id="d6b2" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated"><strong class="mc jd">因此</strong>:</p><ol class=""><li id="cff3" class="nb nc it mc b md mw mg mx mj nd mn ne mr nf mv ng nh ni nj bi translated">如果你将这些算法应用于一个基函数，输出将是它的根。</li><li id="22b8" class="nb nc it mc b md nk mg nl mj nm mn nn mr no mv ng nh ni nj bi translated">如果您将这些算法应用于函数的一阶导数，输出将是它的最佳点。</li></ol><h1 id="9010" class="li lj it bd lk ll lm ln lo lp lq lr ls ki lt kj lu kl lv km lw ko lx kp ly lz bi translated">通用优化算法</h1><p id="bf06" class="pw-post-body-paragraph ma mb it mc b md me kd mf mg mh kg mi mj mk ml mm mn mo mp mq mr ms mt mu mv im bi translated">假设我们的原始函数是:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi np"><img src="../Images/842c835668e8126c9289ac8aca30c0f4.png" data-original-src="https://miro.medium.com/v2/resize:fit:688/format:webp/1*s2Xl1pzBRFncyYh7OWTyXQ.png"/></div></figure><p id="fa21" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">为了找到最佳点，我们需要求解:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/00f932efab5c66f95e1fedea16d375a4.png" data-original-src="https://miro.medium.com/v2/resize:fit:402/format:webp/1*G6rjkYS7-BOh6nrzmZ1SCQ.png"/></div></figure><p id="5576" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">由于我们在做优化，我们将把哈雷方法应用于一阶导数。</p><p id="e33e" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">一般优化过程的特征在于迭代初始值以接近临界点。可以总结如下:</p><ol class=""><li id="a8da" class="nb nc it mc b md mw mg mx mj nd mn ne mr nf mv ng nh ni nj bi translated">为我们要寻找的根的函数选择一个初始值。</li><li id="f4da" class="nb nc it mc b md nk mg nl mj nm mn nn mr no mv ng nh ni nj bi translated">我们使用一个公式来更新这个初始值，这个公式根据我们应用的方法而不同。</li><li id="83f3" class="nb nc it mc b md nk mg nl mj nm mn nn mr no mv ng nh ni nj bi translated">使用更新后的值，计算我们的函数值。</li><li id="9c78" class="nb nc it mc b md nk mg nl mj nm mn nn mr no mv ng nh ni nj bi translated">重复步骤2-3，直到我们的函数(步骤3的输出)等于零(0)或一个非常接近所选阈值的值(被资源称为ε(<code class="fe nr ns nt nu b">eps</code>))</li></ol><h1 id="aa03" class="li lj it bd lk ll lm ln lo lp lq lr ls ki lt kj lu kl lv km lw ko lx kp ly lz bi translated"><strong class="ak">哈雷方法</strong></h1><p id="d1cc" class="pw-post-body-paragraph ma mb it mc b md me kd mf mg mh kg mi mj mk ml mm mn mo mp mq mr ms mt mu mv im bi translated">我们需要确定一个公式来更新前面章节中哈雷方法的初始值。这个公式有不同的版本，但是我发现这个更容易编码:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi nv"><img src="../Images/9e2de7f161e64d5229205699c88000ec.png" data-original-src="https://miro.medium.com/v2/resize:fit:868/format:webp/1*CeGeVylLVX9dIJpQ1VxgOw.png"/></div></figure><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi nw"><img src="../Images/f468f756b51bfa481aabacf6951a7e50.png" data-original-src="https://miro.medium.com/v2/resize:fit:630/format:webp/1*7Ls1aHLE4nI1nC6xp9FDBg.png"/></div></figure><p id="2971" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">正如我们所看到的，更新函数不仅需要估计函数的一阶导数，还需要估计它的二阶导数。也就是说，估计原函数一阶导数的一阶和二阶导数。它们相当于原函数的二阶和三阶导数。</p><p id="2dc9" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">所以我们有:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/e853cf6afa453677f876dcfb39abd27a.png" data-original-src="https://miro.medium.com/v2/resize:fit:624/format:webp/1*jUbPQqRGGFU2NssSrxpHsw.png"/></div></figure><p id="ede4" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">最后:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/49eb2c0f699c7da320a937439c460305.png" data-original-src="https://miro.medium.com/v2/resize:fit:254/format:webp/1*zxTfG7diDkUaxInh-2rujw.png"/></div></figure><p id="73a6" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">因此，应用我们的一般过程和更新哈雷方法的公式，我们有:</p><pre class="ks kt ku kv gt nz nu oa ob aw oc bi"><span id="1424" class="od lj it nu b gy oe of l og oh"># Function for Root Finding - This is the first derivative of the original function<br/>def f_0(x):<br/>    return 3*x**2 - 6*x -45</span><span id="2844" class="od lj it nu b gy oi of l og oh">#First Derivative for Root Function<br/>def f_1(x):<br/>    return 6*x - 6</span><span id="5c6c" class="od lj it nu b gy oi of l og oh">#Second Derivative for Root Function<br/>def f_2(x):<br/>    return 6</span></pre><p id="3720" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">初始化我们的变量和值:</p><pre class="ks kt ku kv gt nz nu oa ob aw oc bi"><span id="63d4" class="od lj it nu b gy oe of l og oh">initial_value = 50</span><span id="6536" class="od lj it nu b gy oi of l og oh">#Initialize to count total iterations<br/>iterations = 0</span><span id="9501" class="od lj it nu b gy oi of l og oh">#Initialize a container for target variable<br/>x_curr = initial_value</span><span id="1aa7" class="od lj it nu b gy oi of l og oh">#Setting epsilon - threshold (0.00001)<br/>epsilon = 0.00001</span><span id="7d91" class="od lj it nu b gy oi of l og oh">f = f_0(x_curr)</span></pre><p id="a790" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">现在，我们已经准备好了，我们可以开始我们的while循环，这是我们算法的核心:</p><pre class="ks kt ku kv gt nz nu oa ob aw oc bi"><span id="9370" class="od lj it nu b gy oe of l og oh">while (abs(f) &gt; epsilon):<br/>    <br/>    #Calculate function values<br/>    f = f_0(x_curr)<br/>    f_prime = f_1(x_curr)<br/>    f_double_prime = f_2(x_curr)<br/>    <br/>    #Update the value of the variable as long as the threshold has not been met<br/>    x_curr = x_curr - (2*f*f_prime)/(2*f_prime**2 - f*f_double_prime )<br/>    <br/>    #Update Iterations Count<br/>    iterations += 1<br/>    print(x_curr)</span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/513163b9fd02b248ea85102d739be282.png" data-original-src="https://miro.medium.com/v2/resize:fit:418/format:webp/1*JBgXF-AUXj8lfzDR7LObSg.png"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">我们可以看到，从初始值50开始，算法下降了一大半到17.62，经过6次迭代后最终收敛。高效！</p></figure><h1 id="8f1d" class="li lj it bd lk ll lm ln lo lp lq lr ls ki lt kj lu kl lv km lw ko lx kp ly lz bi translated">使用哈雷方法的求根函数</h1><p id="0fe9" class="pw-post-body-paragraph ma mb it mc b md me kd mf mg mh kg mi mj mk ml mm mn mo mp mq mr ms mt mu mv im bi translated">最后，让我们为Halley的方法创建一个通用函数，它可以为我们合并其他好的特性。</p><pre class="ks kt ku kv gt nz nu oa ob aw oc bi"><span id="fa20" class="od lj it nu b gy oe of l og oh">def halleys_method(root_func, first_prime, second_prime, eps, initial):<br/>    ''' Finds a root of a function using Halley's method.<br/>    <br/>    <br/>    Parameters:<br/>        root_func (function)    : The function which roots are being solved<br/>        first_prime (function)  : The first derivative of the root_func parameter<br/>        second_prime (function) : The second derivative of the root_func parameter<br/>        eps (float)             : Error threshold value close enough to zero (0)<br/>        initial (float)         : Initial value of the variable<br/>        <br/>    Returns:<br/>        The final value of the variable for which a local optimal value is found.<br/>    '''<br/>    #Initialize to count total iterations<br/>    iterations = 0</span><span id="95e9" class="od lj it nu b gy oi of l og oh">##Initialize a container for target variable<br/>    x_curr = initial<br/>    <br/>    #Initialize First Function Value<br/>    f = root_func(x_curr)<br/>    <br/>    #Update the variable<br/>    while (abs(f) &gt; eps):<br/>    <br/>        #Calculate function values<br/>        f = root_func(x_curr)<br/>        f_prime = first_prime(x_curr)<br/>        f_double_prime = second_prime(x_curr)</span><span id="5a46" class="od lj it nu b gy oi of l og oh">#Update the value of the variable as long as the threshold has not been met<br/>        x_curr = x_curr - (2*f*f_prime)/(2*f_prime**2 - f*f_double_prime )</span><span id="b906" class="od lj it nu b gy oi of l og oh">#Update Iterations Count<br/>        iterations += 1<br/>    <br/>    print(f"SUCCESS! Algorithm converged after {iterations} iterations. An optimum can be found at point: ")<br/>    <br/>    return x_curr</span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ok"><img src="../Images/ea0ec23bd40aa966c7eb6e936f6481dc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bVZRdVdEIZgrspGijB6ldA.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">测试我们的功能。令人惊奇不是吗？</p></figure><h1 id="6205" class="li lj it bd lk ll lm ln lo lp lq lr ls ki lt kj lu kl lv km lw ko lx kp ly lz bi translated">在SCIPY中使用内置函数</h1><p id="8559" class="pw-post-body-paragraph ma mb it mc b md me kd mf mg mh kg mi mj mk ml mm mn mo mp mq mr ms mt mu mv im bi translated">然而，对于那些想更方便地使用哈雷方法的人来说，我们可以使用Scipy中的方法。使用我们上面定义的函数:</p><pre class="ks kt ku kv gt nz nu oa ob aw oc bi"><span id="a586" class="od lj it nu b gy oe of l og oh">from scipy.optimize import newton</span><span id="bff3" class="od lj it nu b gy oi of l og oh">newton(func=f_0,x0=50,fprime=f_1,fprime2=f_2, tol=0.0001)</span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi ol"><img src="../Images/0a81f9f55738181e04aac88bdc70446f.png" data-original-src="https://miro.medium.com/v2/resize:fit:314/format:webp/1*an_03_Lv5KzIoyDgjnwQag.png"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">相同输出:)</p></figure><h1 id="2075" class="li lj it bd lk ll lm ln lo lp lq lr ls ki lt kj lu kl lv km lw ko lx kp ly lz bi translated">结束语</h1><p id="1309" class="pw-post-body-paragraph ma mb it mc b md me kd mf mg mh kg mi mj mk ml mm mn mo mp mq mr ms mt mu mv im bi translated">我们上面开发的函数对于大多数非线性优化问题来说是非常好的。与大多数非线性优化算法一样，哈雷方法收敛于我们所说的“局部最优”。这不同于“全局最优”，全局最优是整个方程的绝对最优点。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi om"><img src="../Images/95954a6bc391c6994621319123ac25b6.png" data-original-src="https://miro.medium.com/v2/resize:fit:692/format:webp/1*_E0NVKjXmk4TOd-MFcXQ8g.png"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">局部最优与全局最优</p></figure><p id="6c38" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">根据您对初始值的选择，算法会收敛到局部或全局最优值，通常是更接近初始值的值。因此，用一个极端负的初始值和一个极端正的值来尝试上面的算法。</p><p id="ac5f" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">最后，众所周知，Halley的方法收敛速度快，这也是它优于其他优化方法的明显优势之一。尝试一个极值，你可以看到该算法很快地在一个局部最优值附近徘徊。</p><p id="9491" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">完整代码可以在我的<a class="ae lh" href="https://github.com/francisadrianviernes/Financial-Analytics/blob/master/Nonlinear%20Optimization/Nonlinear%20Optimization%20Using%20Halley's%C2%A0Method.ipynb" rel="noopener ugc nofollow" target="_blank"> Github页面</a>找到。让我知道你的想法！</p></div></div>    
</body>
</html>