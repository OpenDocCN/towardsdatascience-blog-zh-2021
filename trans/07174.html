<html>
<head>
<title>How to Speed Up Your Python Code through PySpark</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如何通过PySpark加速您的Python代码</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/how-to-speed-up-your-python-code-through-pyspark-e3296e39da6?source=collection_archive---------19-----------------------#2021-06-29">https://towardsdatascience.com/how-to-speed-up-your-python-code-through-pyspark-e3296e39da6?source=collection_archive---------19-----------------------#2021-06-29</a></blockquote><div><div class="fc ik il im in io"/><div class="ip iq ir is it"><h2 id="476c" class="iu iv iw bd b dl ix iy iz ja jb jc dk jd translated" aria-label="kicker paragraph">环境设置</h2><div class=""/><div class=""><h2 id="318e" class="pw-subtitle-paragraph kc jf iw bd b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt dk translated">关于如何安装和运行Apache Spark和PySpark以提高代码性能的教程。</h2></div><figure class="kv kw kx ky gu kz gi gj paragraph-image"><div role="button" tabindex="0" class="la lb di lc bf ld"><div class="gi gj ku"><img src="../Images/881f06a5e00ea950cdaba61584d055e2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uuIwwhOH-PJK8sHbbPB51g.jpeg"/></div></div><p class="lg lh gk gi gj li lj bd b be z dk translated">图片由<a class="ae lk" href="https://pixabay.com/users/taken-336382/?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=1098059" rel="noopener ugc nofollow" target="_blank">拍摄</a>来自<a class="ae lk" href="https://pixabay.com/?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=1098059" rel="noopener ugc nofollow" target="_blank"> Pixabay </a></p></figure><p id="5d15" class="pw-post-body-paragraph ll lm iw ln b lo lp kg lq lr ls kj lt lu lv lw lx ly lz ma mb mc md me mf mg ip bi translated">当你处理巨大的数据集时，<strong class="ln jg">瓶颈不是你的代码</strong>(我希望如此…)，<strong class="ln jg">而是对你的数据集执行某些操作所耗费的时间</strong>。出于这个原因，开发一些可以加速代码的库是非常重要的。</p><p id="2280" class="pw-post-body-paragraph ll lm iw ln b lo lp kg lq lr ls kj lt lu lv lw lx ly lz ma mb mc md me mf mg ip bi translated"><strong class="ln jg">阿帕奇Spark或许能帮到你</strong>。Apache Spark是一个开源项目，相对于标准技术，它可以将工作负载加速100倍。它可以在分布式环境中工作(<strong class="ln jg">集群</strong>)，但也可以在本地使用，例如当您的机器中有多个处理器时。</p><p id="ef8e" class="pw-post-body-paragraph ll lm iw ln b lo lp kg lq lr ls kj lt lu lv lw lx ly lz ma mb mc md me mf mg ip bi translated">实际上，在Apache Spark中有两种类型的节点:主节点<strong class="ln jg">和许多工作节点</strong>，主节点是集群的主计算机。主服务器组织工作并在工人之间分配，然后检索结果。</p></div><div class="ab cl mh mi ia mj" role="separator"><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm"/></div><div class="ip iq ir is it"><p id="88e2" class="pw-post-body-paragraph ll lm iw ln b lo lp kg lq lr ls kj lt lu lv lw lx ly lz ma mb mc md me mf mg ip bi translated">在本教程中，我涉及以下几个方面:</p><ul class=""><li id="3b08" class="mo mp iw ln b lo lp lr ls lu mq ly mr mc ms mg mt mu mv mw bi translated">下载并安装Apache Spark</li><li id="75f6" class="mo mp iw ln b lo mx lr my lu mz ly na mc nb mg mt mu mv mw bi translated">安装PySpark以配置Python来与Apache Spark一起工作</li><li id="1ca8" class="mo mp iw ln b lo mx lr my lu mz ly na mc nb mg mt mu mv mw bi translated">运行一个简单示例</li></ul><h1 id="544f" class="nc nd iw bd ne nf ng nh ni nj nk nl nm kl nn km no ko np kp nq kr nr ks ns nt bi translated">下载并安装Apache Spark</h1><p id="2148" class="pw-post-body-paragraph ll lm iw ln b lo nu kg lq lr nv kj lt lu nw lw lx ly nx ma mb mc ny me mf mg ip bi translated">Apache Spark可以从其官方网站下载:</p><div class="nz oa gq gs ob oc"><a href="https://spark.apache.org/downloads.html" rel="noopener  ugc nofollow" target="_blank"><div class="od ab fp"><div class="oe ab of cl cj og"><h2 class="bd jg gz z fq oh fs ft oi fv fx jf bi translated">下载| Apache Spark</h2><div class="oj l"><h3 class="bd b gz z fq oh fs ft oi fv fx dk translated">注意，Spark 2.x是用Scala 2.11预建的，除了2.4.2版本是用Scala 2.12预建的。火花3.0+…</h3></div><div class="ok l"><p class="bd b dl z fq oh fs ft oi fv fx dk translated">spark.apache.org</p></div></div><div class="ol l"><div class="om l on oo op ol oq le oc"/></div></div></a></div><p id="e3b8" class="pw-post-body-paragraph ll lm iw ln b lo lp kg lq lr ls kj lt lu lv lw lx ly lz ma mb mc md me mf mg ip bi translated">你可以选择火花释放和包装时间。如果您没有任何特殊需求，可以下载Apache Hadoop的最新版本和预编译版本。下载后，您可以将其移动到您的首选目录，并用一个较短的名称重命名。</p><p id="81df" class="pw-post-body-paragraph ll lm iw ln b lo lp kg lq lr ls kj lt lu lv lw lx ly lz ma mb mc md me mf mg ip bi translated">Apache Spark可以通过<strong class="ln jg"> PySpark包</strong>与Python结合使用。</p><p id="94d6" class="pw-post-body-paragraph ll lm iw ln b lo lp kg lq lr ls kj lt lu lv lw lx ly lz ma mb mc md me mf mg ip bi translated">Apache Spark也需要安装Java。Java的兼容版本从8到11不等。不支持其他版本。</p><h1 id="4aad" class="nc nd iw bd ne nf ng nh ni nj nk nl nm kl nn km no ko np kp nq kr nr ks ns nt bi translated">安装PySpark</h1><p id="764a" class="pw-post-body-paragraph ll lm iw ln b lo nu kg lq lr nv kj lt lu nw lw lx ly nx ma mb mc ny me mf mg ip bi translated">现在您可以安装PySpark，例如通过<code class="fe or os ot ou b">pip</code>管理器:</p><pre class="kv kw kx ky gu ov ou ow ox aw oy bi"><span id="0f29" class="oz nd iw ou b gz pa pb l pc pd">pip install pyspark</span></pre><p id="c8c7" class="pw-post-body-paragraph ll lm iw ln b lo lp kg lq lr ls kj lt lu lv lw lx ly lz ma mb mc md me mf mg ip bi translated">安装完成后，您需要配置<code class="fe or os ot ou b">SPARK_HOME</code>并修改您的<code class="fe or os ot ou b">.bash_profile</code>或<code class="fe or os ot ou b">.profile</code>文件中的<code class="fe or os ot ou b">PATH</code>变量。该文件是隐藏的，位于您的主目录中。您可以打开它，并在文件末尾添加以下代码行:</p><pre class="kv kw kx ky gu ov ou ow ox aw oy bi"><span id="6c98" class="oz nd iw ou b gz pa pb l pc pd">export SPARK_HOME="<strong class="ou jg">/path/to/spark</strong>/spark"<br/>export PATH="$SPARK_HOME/python:$PATH"</span></pre><p id="d140" class="pw-post-body-paragraph ll lm iw ln b lo lp kg lq lr ls kj lt lu lv lw lx ly lz ma mb mc md me mf mg ip bi translated">您可以保存文件并启动终端。您可以输入以下命令:</p><pre class="kv kw kx ky gu ov ou ow ox aw oy bi"><span id="7c7f" class="oz nd iw ou b gz pa pb l pc pd">pyspark</span></pre><p id="d54f" class="pw-post-body-paragraph ll lm iw ln b lo lp kg lq lr ls kj lt lu lv lw lx ly lz ma mb mc md me mf mg ip bi translated"><code class="fe or os ot ou b">pyspark</code>终端启动。要退出它，只需写下<code class="fe or os ot ou b">quit()</code>并按回车键。</p><p id="e054" class="pw-post-body-paragraph ll lm iw ln b lo lp kg lq lr ls kj lt lu lv lw lx ly lz ma mb mc md me mf mg ip bi translated"><strong class="ln jg">作为选项，您可以配置</strong> <code class="fe or os ot ou b"><strong class="ln jg">pyspark</strong></code> <strong class="ln jg">使用Jupyter笔记本。</strong></p><p id="ec53" class="pw-post-body-paragraph ll lm iw ln b lo lp kg lq lr ls kj lt lu lv lw lx ly lz ma mb mc md me mf mg ip bi translated">在这种情况下，您可以使用<code class="fe or os ot ou b">findspark</code>包，它会为您搜索Spark在哪里。实际上，<code class="fe or os ot ou b">findspark</code>包会从你的概要文件中读取<code class="fe or os ot ou b">SPARK_HOME</code>目录。</p><p id="cc61" class="pw-post-body-paragraph ll lm iw ln b lo lp kg lq lr ls kj lt lu lv lw lx ly lz ma mb mc md me mf mg ip bi translated">您可以通过以下命令安装<code class="fe or os ot ou b">findspark</code>:</p><pre class="kv kw kx ky gu ov ou ow ox aw oy bi"><span id="03d2" class="oz nd iw ou b gz pa pb l pc pd">pip install findspark</span></pre><p id="ddab" class="pw-post-body-paragraph ll lm iw ln b lo lp kg lq lr ls kj lt lu lv lw lx ly lz ma mb mc md me mf mg ip bi translated">安装完成后，您可以启动Jupyter notebook，并在代码开头添加以下代码行:</p><pre class="kv kw kx ky gu ov ou ow ox aw oy bi"><span id="6753" class="oz nd iw ou b gz pa pb l pc pd">import findspark<br/>findspark.init()</span></pre><h1 id="c332" class="nc nd iw bd ne nf ng nh ni nj nk nl nm kl nn km no ko np kp nq kr nr ks ns nt bi translated">简单的例子</h1><p id="0e11" class="pw-post-body-paragraph ll lm iw ln b lo nu kg lq lr nv kj lt lu nw lw lx ly nx ma mb mc ny me mf mg ip bi translated">现在，您已经准备好运行您的第一个<code class="fe or os ot ou b">pyspark </code>示例了。首先，您可以创建一个<code class="fe or os ot ou b">SparkContext</code>，它对应于您的集群的主节点。您可以指定一些配置参数，例如应用程序名称(<code class="fe or os ot ou b">myproject</code>)和url ( <code class="fe or os ot ou b">local</code>):</p><pre class="kv kw kx ky gu ov ou ow ox aw oy bi"><span id="751b" class="oz nd iw ou b gz pa pb l pc pd">from pyspark import SparkContext, SparkConf</span><span id="faf2" class="oz nd iw ou b gz pe pb l pc pd">conf = SparkConf().setMaster("local").setAppName("myproject")<br/>sc = SparkContext.getOrCreate(conf=conf)</span></pre><p id="c9dd" class="pw-post-body-paragraph ll lm iw ln b lo lp kg lq lr ls kj lt lu lv lw lx ly lz ma mb mc md me mf mg ip bi translated"><code class="fe or os ot ou b">getOrCreate()</code>函数创建一个新的<code class="fe or os ot ou b">SparkContext</code>，如果它还不存在，否则它检索现有的。这是因为只能有一个运行中的<code class="fe or os ot ou b">SparkContext</code>实例。</p><p id="28df" class="pw-post-body-paragraph ll lm iw ln b lo lp kg lq lr ls kj lt lu lv lw lx ly lz ma mb mc md me mf mg ip bi translated">现在您可以创建一个Spark数据帧，它的行为很像一个SQL表。火花数据帧可从<code class="fe or os ot ou b">SparkContext</code>对象创建，如下所示:</p><pre class="kv kw kx ky gu ov ou ow ox aw oy bi"><span id="cf01" class="oz nd iw ou b gz pa pb l pc pd">from pyspark.sql import SparkSession</span><span id="bce5" class="oz nd iw ou b gz pe pb l pc pd">spark = SparkSession.builder.getOrCreate()</span></pre><p id="a3bd" class="pw-post-body-paragraph ll lm iw ln b lo lp kg lq lr ls kj lt lu lv lw lx ly lz ma mb mc md me mf mg ip bi translated">现在您可以使用<code class="fe or os ot ou b">spark</code>对象来读取CSV文件:</p><pre class="kv kw kx ky gu ov ou ow ox aw oy bi"><span id="3550" class="oz nd iw ou b gz pa pb l pc pd">df = spark.read.csv("/path/to/your/csv/file",inferSchema=True, header=True)</span></pre><p id="a55f" class="pw-post-body-paragraph ll lm iw ln b lo lp kg lq lr ls kj lt lu lv lw lx ly lz ma mb mc md me mf mg ip bi translated"><code class="fe or os ot ou b">inferSchema=True</code>参数允许自动识别数据类型，但需要更多时间。您可以通过<code class="fe or os ot ou b">show()</code>函数列出数据帧的第一行:</p><pre class="kv kw kx ky gu ov ou ow ox aw oy bi"><span id="2875" class="oz nd iw ou b gz pa pb l pc pd">df.show()</span></pre><p id="71b8" class="pw-post-body-paragraph ll lm iw ln b lo lp kg lq lr ls kj lt lu lv lw lx ly lz ma mb mc md me mf mg ip bi translated">结果看起来像一个SQL表。</p><p id="be72" class="pw-post-body-paragraph ll lm iw ln b lo lp kg lq lr ls kj lt lu lv lw lx ly lz ma mb mc md me mf mg ip bi translated">您可以通过经典的SQL查询来查询数据帧。为此，首先必须将新的数据帧注册到可用表列表中:</p><pre class="kv kw kx ky gu ov ou ow ox aw oy bi"><span id="73e6" class="oz nd iw ou b gz pa pb l pc pd">df_sql = df.createOrReplaceTempView('df_name')</span></pre><p id="5a5f" class="pw-post-body-paragraph ll lm iw ln b lo lp kg lq lr ls kj lt lu lv lw lx ly lz ma mb mc md me mf mg ip bi translated">其中<code class="fe or os ot ou b">df_name</code>是您希望在SQL中使用的表的名称。您可以列出所有可用的表格:</p><pre class="kv kw kx ky gu ov ou ow ox aw oy bi"><span id="7327" class="oz nd iw ou b gz pa pb l pc pd">print(spark.catalog.listTables())</span></pre><p id="02ad" class="pw-post-body-paragraph ll lm iw ln b lo lp kg lq lr ls kj lt lu lv lw lx ly lz ma mb mc md me mf mg ip bi translated">现在，您可以通过以下代码查询您的表:</p><pre class="kv kw kx ky gu ov ou ow ox aw oy bi"><span id="1d82" class="oz nd iw ou b gz pa pb l pc pd">query = 'SELECT count(*) FROM df_name'<br/>sc.sql(query)</span></pre><h1 id="07a1" class="nc nd iw bd ne nf ng nh ni nj nk nl nm kl nn km no ko np kp nq kr nr ks ns nt bi translated">摘要</h1><p id="fa96" class="pw-post-body-paragraph ll lm iw ln b lo nu kg lq lr nv kj lt lu nw lw lx ly nx ma mb mc ny me mf mg ip bi translated">在本教程中，我演示了如何在您的计算机上安装和运行Apache Spark和PySpark。此外，我还举例说明了如何将CSV文件读入Spark DataFrame。</p><p id="83a9" class="pw-post-body-paragraph ll lm iw ln b lo lp kg lq lr ls kj lt lu lv lw lx ly lz ma mb mc md me mf mg ip bi translated">如果你想了解我的研究和其他活动的最新情况，你可以在<a class="ae lk" href="https://twitter.com/alod83" rel="noopener ugc nofollow" target="_blank"> Twitter </a>、<a class="ae lk" href="https://www.youtube.com/channel/UC4O8-FtQqGIsgDW_ytXIWOg?view_as=subscriber" rel="noopener ugc nofollow" target="_blank"> Youtube </a>和<a class="ae lk" href="https://github.com/alod83" rel="noopener ugc nofollow" target="_blank"> Github </a>上关注我。</p><h1 id="cacb" class="nc nd iw bd ne nf ng nh ni nj nk nl nm kl nn km no ko np kp nq kr nr ks ns nt bi translated">相关著作</h1><div class="nz oa gq gs ob oc"><a rel="noopener follow" target="_blank" href="/how-to-load-huge-csv-datasets-in-python-pandas-d306e75ff276"><div class="od ab fp"><div class="oe ab of cl cj og"><h2 class="bd jg gz z fq oh fs ft oi fv fx jf bi translated">如何在Python Pandas中加载巨大的CSV数据集</h2><div class="oj l"><h3 class="bd b gz z fq oh fs ft oi fv fx dk translated">可能会出现这样的情况，您的硬盘中有一个巨大的CSV数据集，占用了4或5gb(甚至更多),而您…</h3></div><div class="ok l"><p class="bd b dl z fq oh fs ft oi fv fx dk translated">towardsdatascience.com</p></div></div><div class="ol l"><div class="pf l on oo op ol oq le oc"/></div></div></a></div><div class="nz oa gq gs ob oc"><a href="https://medium.com/geekculture/the-top-25-python-libraries-for-data-science-71c0eb58723d" rel="noopener follow" target="_blank"><div class="od ab fp"><div class="oe ab of cl cj og"><h2 class="bd jg gz z fq oh fs ft oi fv fx jf bi translated">面向数据科学的25大Python库</h2><div class="oj l"><h3 class="bd b gz z fq oh fs ft oi fv fx dk translated">你一生中至少应该尝试一次的Python库列表。</h3></div><div class="ok l"><p class="bd b dl z fq oh fs ft oi fv fx dk translated">medium.com</p></div></div><div class="ol l"><div class="pg l on oo op ol oq le oc"/></div></div></a></div><div class="nz oa gq gs ob oc"><a rel="noopener follow" target="_blank" href="/how-to-install-python-and-jupyter-notebook-onto-an-android-device-900009df743f"><div class="od ab fp"><div class="oe ab of cl cj og"><h2 class="bd jg gz z fq oh fs ft oi fv fx jf bi translated">如何在Android设备上安装Python和Jupyter Notebook</h2><div class="ok l"><p class="bd b dl z fq oh fs ft oi fv fx dk translated">towardsdatascience.com</p></div></div><div class="ol l"><div class="ph l on oo op ol oq le oc"/></div></div></a></div><h1 id="1b7a" class="nc nd iw bd ne nf ng nh ni nj nk nl nm kl nn km no ko np kp nq kr nr ks ns nt bi translated">参考</h1><div class="nz oa gq gs ob oc"><a href="https://medium.com/tinghaochen/how-to-install-pyspark-locally-94501eefe421" rel="noopener follow" target="_blank"><div class="od ab fp"><div class="oe ab of cl cj og"><h2 class="bd jg gz z fq oh fs ft oi fv fx jf bi translated">如何在本地安装PySpark</h2><div class="oj l"><h3 class="bd b gz z fq oh fs ft oi fv fx dk translated">在这里，我将一步一步地在您的笔记本电脑上安装pyspark。</h3></div><div class="ok l"><p class="bd b dl z fq oh fs ft oi fv fx dk translated">medium.com</p></div></div><div class="ol l"><div class="pi l on oo op ol oq le oc"/></div></div></a></div><div class="nz oa gq gs ob oc"><a href="https://www.sicara.ai/blog/2017-05-02-get-started-pyspark-jupyter-notebook-3-minutes" rel="noopener  ugc nofollow" target="_blank"><div class="od ab fp"><div class="oe ab of cl cj og"><h2 class="bd jg gz z fq oh fs ft oi fv fx jf bi translated">3分钟内开始使用PySpark和Jupyter笔记本| Sicara</h2><div class="oj l"><h3 class="bd b gz z fq oh fs ft oi fv fx dk translated">Apache Spark是大数据爱好者的必备品。简而言之，Spark是一个快速而强大的框架，它提供了一个…</h3></div><div class="ok l"><p class="bd b dl z fq oh fs ft oi fv fx dk translated">www.sicara.ai</p></div></div><div class="ol l"><div class="pj l on oo op ol oq le oc"/></div></div></a></div><div class="nz oa gq gs ob oc"><a rel="noopener follow" target="_blank" href="/pyspark-and-sparksql-basics-6cb4bf967e53"><div class="od ab fp"><div class="oe ab of cl cj og"><h2 class="bd jg gz z fq oh fs ft oi fv fx jf bi translated">PySpark和SparkSQL基础知识</h2><div class="oj l"><h3 class="bd b gz z fq oh fs ft oi fv fx dk translated">如何用Python编程实现Spark</h3></div><div class="ok l"><p class="bd b dl z fq oh fs ft oi fv fx dk translated">towardsdatascience.com</p></div></div><div class="ol l"><div class="pf l on oo op ol oq le oc"/></div></div></a></div></div></div>    
</body>
</html>