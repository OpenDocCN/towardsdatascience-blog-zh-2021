<html>
<head>
<title>How to make artificial intelligence articulate doubt</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如何让人工智能清晰地表达疑问</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/how-to-make-artificial-intelligence-articulate-doubt-9c2885a9e541?source=collection_archive---------42-----------------------#2021-08-13">https://towardsdatascience.com/how-to-make-artificial-intelligence-articulate-doubt-9c2885a9e541?source=collection_archive---------42-----------------------#2021-08-13</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><figure class="ip iq gp gr ir is gh gi paragraph-image"><div class="gh gi io"><img src="../Images/4e8f532b3935db6b34015e215b0e7c73.png" data-original-src="https://miro.medium.com/v2/resize:fit:1172/format:webp/1*p3KJHaNv_p4zU1NxU9_RZQ.png"/></div></figure><div class=""/><div class=""><h2 id="2d66" class="pw-subtitle-paragraph ju iw ix bd b jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl dk translated">影响人们生活的模特必须坦诚面对他们的疑虑。这种新方法给可解释的人工智能带来了平衡。T3】</h2></div><p id="cad3" class="pw-post-body-paragraph kn ko ix kp b kq kr jy ks kt ku kb kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">在<a class="ae lj" href="https://www.featurespace.com/" rel="noopener ugc nofollow" target="_blank">特征空间</a>，我们已经<a class="ae lj" href="https://arxiv.org/abs/2107.08756" rel="noopener ugc nofollow" target="_blank">发表了一个方法</a>来解释深度网络中的<strong class="kp iy">模型不确定性</strong>。我们的方法可以理解模型认为不符合其预测类别的输入数据中的混杂模式。这允许我们部署诚实和透明的模型，为决策提供平衡和完整的证据，而不仅仅是管理支持决策的证据。</p><p id="eb7e" class="pw-post-body-paragraph kn ko ix kp b kq kr jy ks kt ku kb kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">例如，在下图中，我们观察到包含在<em class="lk"> CelebA </em>数据集中的三幅名人图像。在所有情况下，名人都被标记为<em class="lk">“不笑”</em>，这可以使用广泛可用的用于图像处理任务的深度分类模型来轻松预测，如VGG16、ResNet50或EfficientNet。然而，模特们很难为这些照片中的一些提供<strong class="kp iy">自信的<em class="lk"> </em> </strong>分类。在下面的例子中，我们提出的方法强调了导致模型不确定的各种特征(右栏)。我们很容易注意到微笑弧、上唇弯曲和口腔走廊的存在，这些通常与微笑和咧嘴笑有关。</p><figure class="lm ln lo lp gt is gh gi paragraph-image"><div class="gh gi ll"><img src="../Images/89adde8d65c44d4c5058f95c89475fa9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1024/format:webp/1*_wfDhvrE7P3AG1y4OPd4kA.png"/></div><p class="lq lr gj gh gi ls lt bd b be z dk translated"><strong class="bd lu"> <em class="km">左</em></strong><em class="km">:CelebA数据集中预测类别标签为“没有笑容”的名人脸部图像。</em> <strong class="bd lu"> <em class="km">右</em> </strong> <em class="km">:由我们的方法高亮显示的像素(红色)导致模型对该预测不确定。作者图片。</em></p></figure><h2 id="a7e0" class="lv lw ix bd lx ly lz dn ma mb mc dp md kw me mf mg la mh mi mj le mk ml mm mn bi translated"><strong class="ak">不确定性和贝叶斯深度学习</strong></h2><p id="5543" class="pw-post-body-paragraph kn ko ix kp b kq mo jy ks kt mp kb kv kw mq ky kz la mr lc ld le ms lg lh li ij bi translated">我们在贝叶斯神经网络(BNNs)的基础上建立了我们的不确定性归因方法。该框架彻底处理了预测中不确定性的所有来源，包括源于<strong class="kp iy">模型选择和训练数据限制的不确定性，</strong>其表现为每个神经细胞的拟合权重参数的不确定性。点击查看<a class="ae lj" href="https://docs.aws.amazon.com/prescriptive-guidance/latest/ml-quantifying-uncertainty/welcome.html" rel="noopener ugc nofollow" target="_blank">中的BNNs概述。总而言之:</a></p><ol class=""><li id="8895" class="mt mu ix kp b kq kr kt ku kw mv la mw le mx li my mz na nb bi translated">神经元的每个拟合参数被捕获为一个<strong class="kp iy">概率分布<em class="lk"> </em> </strong>，该概率分布表示在给定训练数据的情况下，我们对其最佳<em class="lk">值缺乏确定性。</em></li><li id="b02d" class="mt mu ix kp b kq nc kt nd kw ne la nf le ng li my mz na nb bi translated">由于深度模型有大量参数，因此，我们在所有训练参数上有一个<strong class="kp iy">多元</strong>分布，称为<em class="lk">后验分布。</em></li><li id="0015" class="mt mu ix kp b kq nc kt nd kw ne la nf le ng li my mz na nb bi translated">当我们对一个新的数据点进行分类时，每个拟合参数值的合理组合都会给我们一个不同的分数！BNN提供给我们的不是一个单一的分数，而是一种可能分数的分布，称为<em class="lk">后验预测分布。</em></li><li id="bd56" class="mt mu ix kp b kq nc kt nd kw ne la nf le ng li my mz na nb bi translated">bnn通常返回后验预测分布的平均值作为它们对输出分数的估计。</li></ol><p id="62a2" class="pw-post-body-paragraph kn ko ix kp b kq kr jy ks kt ku kb kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">这些计算通常是近似的，并在引擎盖下运行，例如，当您将脱落层添加到您的神经模型架构中时。重要的是，对于我们希望分类的每一幅图像、一段文本或金融交易，我们可以检索代表分数应该是多少的分布。MNIST数字的一对示例分布如下所示。分布的平均值告诉我们分类有多确定，而方差告诉我们建模不确定性有多稳定。请注意，平均分数有时可能与由<em class="lk">最可能的</em>组模型参数产生的分数非常不同，这是非贝叶斯神经网络通常返回的分数。在第一个例子中，最可能模型与平均模型分数的偏差特别大。</p><figure class="lm ln lo lp gt is gh gi paragraph-image"><div role="button" tabindex="0" class="ni nj di nk bf nl"><div class="gh gi nh"><img src="../Images/64624925dd491ccf48d6bd90d170b4d4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xJtyu27uABN-ChxtageJ5w.png"/></div></div><p class="lq lr gj gh gi ls lt bd b be z dk translated">两个MNIST数字的潜在类别患病率的后验预测分布，最可能类别的BNN概率相似。<strong class="bd lu"> </strong>在<strong class="bd lu">顶部图像</strong>中，所示的分布(对于类别标签“8”)支持大范围的可能得分，事实上它是双峰的，有利于确定图像必须是“8”，或者确定它不可能是“8”。这意味着图像代表数据中的稀疏区域，在训练集中只有很少或没有样本。相反，<strong class="bd lu">底部图像</strong>中的分布在0.5附近达到峰值，并且相对较窄。这意味着训练集在特征空间的这一点上包含内在类别混合的证据——它已经看到了类似的例子，这些例子或者被标记为(不完整的)7，或者被标记为具有外围墨迹的杂散斑点的1。作者图片。</p></figure><h2 id="e3c6" class="lv lw ix bd lx ly lz dn ma mb mc dp md kw me mf mg la mh mi mj le mk ml mm mn bi translated"><strong class="ak">解释模型不确定性</strong></h2><p id="f2e0" class="pw-post-body-paragraph kn ko ix kp b kq mo jy ks kt mp kb kv kw mq ky kz la mr lc ld le ms lg lh li ij bi translated">为了解释预测中的不确定性，我们的方法建立在<strong class="kp iy">积分梯度</strong> (Sundararajan et al. 2017)的框架上，这是神经网络最广泛的解释技术之一。集成梯度通过对模型的输出倾向得分<strong class="kp iy"> p <em class="lk"> c </em> (x) </strong>的梯度进行集成来工作，对于类别<strong class="kp iy"> c. </strong>它沿着基准起始点和被解释的点之间的特征空间中的路径执行该集成。对于不确定性的解释，我们不积分模型分数的梯度，而是积分预测熵的梯度</p><p id="aa69" class="pw-post-body-paragraph kn ko ix kp b kq kr jy ks kt ku kb kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated"><strong class="kp iy">H(x)=-σc PC(x)。log pc(x) </strong></p><p id="140f" class="pw-post-body-paragraph kn ko ix kp b kq kr jy ks kt ku kb kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">该度量包含模型的总预测不确定性，包括源自后验预测分布的位置和宽度的分量。</p><p id="b4ed" class="pw-post-body-paragraph kn ko ix kp b kq kr jy ks kt ku kb kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">我们在公共图像基准上测试了集成熵梯度，包括MNIST和CelebA人脸数据集。不幸的是，由普通的综合梯度产生的熵解释被证明是糟糕的、令人困惑的，而且是一种敌对的解释。这是可以理解的，积分梯度需要一个基准特征向量作为路径积分的原点，而普通算法使用空白图像作为其基准。这有合理的动机来解释为什么分数高，因为空白图像可能对所有类别具有大致相等的模型分数；然而，围绕黑色图像的不确定性通常非常高。</p><p id="cbd8" class="pw-post-body-paragraph kn ko ix kp b kq kr jy ks kt ku kb kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">为了克服这个和其他问题(这些问题的细节在论文中)，我们的方法结合了两个最近的想法来产生干净和有信息的熵属性——这两个方法都利用了潜在的空间学习表示，使用了变分自动编码器。</p><ul class=""><li id="5922" class="mt mu ix kp b kq kr kt ku kw mv la mw le mx li nm mz na nb bi translated"><strong class="kp iy">综合反事实解释</strong> ( <a class="ae lj" href="https://arxiv.org/abs/2006.06848" rel="noopener ugc nofollow" target="_blank">安托万等人2021 </a> <strong class="kp iy">)。</strong>对于我们的路径积分的源图像，我们希望图像具有与测试图像相同的预测类别，但是预测熵为零。合成的反事实解释图像提供了这样的源图像，同时具有吸引人的属性，即源图像与测试图像非常相似——也就是说，增量限于那些增加不确定性的概念。</li><li id="7e36" class="mt mu ix kp b kq nc kt nd kw ne la nf le ng li nm mz na nb bi translated"><strong class="kp iy">分布内路径积分</strong> ( <a class="ae lj" href="https://genomebiology.biomedcentral.com/articles/10.1186/s13059-020-02055-7" rel="noopener ugc nofollow" target="_blank"> <strong class="kp iy"> Jha等人2020 </strong> </a> <strong class="kp iy"> ) </strong>积分梯度的一个问题是基准源和测试向量之间的路径可能偏离数据流形——通过不可能的图像探索轨迹。我们希望基准点和测试点之间的路径积分在数据流形内，即:两幅图像之间路径上的每个特征向量都代表了数据中可能合理出现的一些情况。</li></ul><p id="2934" class="pw-post-body-paragraph kn ko ix kp b kq kr jy ks kt ku kb kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">结合起来，这些想法产生了干净和相关的不确定性解释，即使是对困难的机器学习问题，如阅读人类表情。我们通过解释CelebA数据集中最不确定的图像来证明这一点，如微笑检测，拱形眉毛检测和眼袋检测。</p><figure class="lm ln lo lp gt is gh gi paragraph-image"><div role="button" tabindex="0" class="ni nj di nk bf nl"><div class="gh gi nn"><img src="../Images/98c1e0b996c8fe6e54eeaefcb297369d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vHWqvME9XgmqwSjzlQY_cg.png"/></div></div><p class="lq lr gj gh gi ls lt bd b be z dk translated"><em class="km">与预测“拱形眉毛”(上面两行)和“没有拱形眉毛”(下面两行)的现有方法相比，我们的方法的不确定性解释。作者图片。</em></p></figure><h2 id="eb7b" class="lv lw ix bd lx ly lz dn ma mb mc dp md kw me mf mg la mh mi mj le mk ml mm mn bi translated"><strong class="ak">结论</strong></h2><p id="8f4d" class="pw-post-body-paragraph kn ko ix kp b kq mo jy ks kt mp kb kv kw mq ky kz la mr lc ld le ms lg lh li ij bi translated">新兴的可解释人工智能领域通常专注于收集证据，证明<strong class="kp iy">支持已经做出的</strong>决定，就像律师在法庭上陈述案件一样。然而，大多数人工智能模型的操作更像专家证人，而不是律师说服法官。这就明确了对更加平衡的解释方法的需求，这种方法提供了阐明模糊性、不确定性和怀疑的细致入微的解释。我们的方法所做的是用丰富而简洁的解释来补充传统的分数归因方法，解释为什么数据点的分类是不确定的。随着机器学习被更广泛地用于为对生活有重大影响的法律、金融或医疗决策提供信息，对自动化预测背后的证据权重进行诚实和公开的评估对于维护道德标准至关重要。</p></div></div>    
</body>
</html>