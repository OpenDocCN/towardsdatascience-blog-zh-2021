<html>
<head>
<title>Part 3— Building a deep Q-network to play Gridworld — Learning Instability and Target Networks</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">第3部分——构建深度Q网络来玩grid world——学习不稳定性和目标网络</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/part-3-building-a-deep-q-network-to-play-gridworld-learning-instability-and-target-networks-fb399cb42616?source=collection_archive---------11-----------------------#2021-12-05">https://towardsdatascience.com/part-3-building-a-deep-q-network-to-play-gridworld-learning-instability-and-target-networks-fb399cb42616?source=collection_archive---------11-----------------------#2021-12-05</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><blockquote class="jn jo jp"><p id="c9b2" class="jq jr js jt b ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko ij bi translated">在本文中，让我们了解什么是学习不稳定性，这是深度强化学习代理的一个常见问题。我们将通过实现目标网络来解决这个问题</p></blockquote><p id="82a0" class="pw-post-body-paragraph jq jr iq jt b ju jv jw jx jy jz ka kb kp kd ke kf kq kh ki kj kr kl km kn ko ij bi translated">欢迎来到深度Q-网络教程的第三部分。这是<a class="ae ks" href="https://nandakishorej8.medium.com/part-1-building-a-deep-q-network-to-play-gridworld-deepminds-deep-q-networks-78842007c631" rel="noopener">第一部分</a>和<a class="ae ks" href="https://nandakishorej8.medium.com/part-2-building-a-deep-q-network-to-play-gridworld-catastrophic-forgetting-and-experience-6b2b000910d7" rel="noopener">第二部分</a>的延续。如果您还没有阅读这些，我强烈建议您阅读它们，因为本文中的许多代码和解释将与其中已经解释的直接相关。</p><p id="997c" class="pw-post-body-paragraph jq jr iq jt b ju jv jw jx jy jz ka kb kp kd ke kf kq kh ki kj kr kl km kn ko ij bi translated"><strong class="jt ir">到目前为止的第一部分！！</strong></p><ol class=""><li id="408f" class="kt ku iq jt b ju jv jy jz kp kv kq kw kr kx ko ky kz la lb bi translated">我们从理解什么是Q学习和用于更新Q学习的公式开始</li><li id="784c" class="kt ku iq jt b ju lc jy ld kp le kq lf kr lg ko ky kz la lb bi translated">后来我们看到了GridWorld游戏，并定义了它的状态、动作和奖励。</li><li id="f9a7" class="kt ku iq jt b ju lc jy ld kp le kq lf kr lg ko ky kz la lb bi translated">然后我们想出了一个强化学习的方法来赢得比赛</li><li id="60ea" class="kt ku iq jt b ju lc jy ld kp le kq lf kr lg ko ky kz la lb bi translated">我们学习了如何导入GridWorld环境和环境的各种模式</li><li id="dc8d" class="kt ku iq jt b ju lc jy ld kp le kq lf kr lg ko ky kz la lb bi translated">设计并建立了一个神经网络作为Q函数。</li><li id="6ac0" class="kt ku iq jt b ju lc jy ld kp le kq lf kr lg ko ky kz la lb bi translated">我们对RL代理进行了训练和测试，在解决静态网格世界问题上取得了很好的效果。但我们未能解决随机网格世界。</li></ol><p id="324f" class="pw-post-body-paragraph jq jr iq jt b ju jv jw jx jy jz ka kb kp kd ke kf kq kh ki kj kr kl km kn ko ij bi translated"><strong class="jt ir">第二部！！</strong></p><ol class=""><li id="4e9b" class="kt ku iq jt b ju jv jy jz kp kv kq kw kr kx ko ky kz la lb bi translated">我们学习了什么是灾难性遗忘，以及它如何影响DQN特工</li><li id="c0a9" class="kt ku iq jt b ju lc jy ld kp le kq lf kr lg ko ky kz la lb bi translated">我们通过实施经验回复解决了灾难性遗忘</li><li id="fe2c" class="kt ku iq jt b ju lc jy ld kp le kq lf kr lg ko ky kz la lb bi translated">我们看到DRL学习不稳定。</li></ol><p id="4b7c" class="pw-post-body-paragraph jq jr iq jt b ju jv jw jx jy jz ka kb kp kd ke kf kq kh ki kj kr kl km kn ko ij bi translated">在这篇文章中，我们将学习如何实现目标网络，以摆脱学习不稳定性</p><p id="377b" class="pw-post-body-paragraph jq jr iq jt b ju jv jw jx jy jz ka kb kp kd ke kf kq kh ki kj kr kl km kn ko ij bi translated"><strong class="jt ir">什么是学习不稳定？？</strong></p><p id="ef21" class="pw-post-body-paragraph jq jr iq jt b ju jv jw jx jy jz ka kb kp kd ke kf kq kh ki kj kr kl km kn ko ij bi translated">当Q-network的参数在每次移动后都被更新时，网络中存在不稳定的机会，因为奖励非常少(只有在赢或输时才会给出显著的奖励)。由于每一步都没有显著的回报，算法开始变得不稳定。</p><p id="0f78" class="pw-post-body-paragraph jq jr iq jt b ju jv jw jx jy jz ka kb kp kd ke kf kq kh ki kj kr kl km kn ko ij bi translated">例如，在任何状态下，移动“向上”将赢得游戏，因此获得+10作为奖励。我们的算法认为动作“向上”对于当前状态是好的，并更新其参数以预测该动作的高Q值。但是在下一场比赛中，网络预测高Q值为“向上”,这可能导致获得-10奖励。现在我们的算法认为这个动作是坏的，并更新它的参数。然后一些游戏后来上升可以导致获胜。这将导致混乱，并且预测的Q值将永远不会满足于合理的稳定值。这与我们在上一篇文章中讨论的灾难性遗忘非常相似。</p><p id="54c7" class="pw-post-body-paragraph jq jr iq jt b ju jv jw jx jy jz ka kb kp kd ke kf kq kh ki kj kr kl km kn ko ij bi translated"><strong class="jt ir">设备一个重复的Q-网络称为目标网络！！</strong></p><p id="9f8b" class="pw-post-body-paragraph jq jr iq jt b ju jv jw jx jy jz ka kb kp kd ke kf kq kh ki kj kr kl km kn ko ij bi translated">DeepMind设计的解决方案是将q网络复制成两份，每份都有自己的模型参数:“常规”q网络和一份名为<em class="js">目标网络</em>(象征性地表示为Q^-network，读作“q帽子”)。在开始时，在任何训练之前，目标网络与Q网络是相同的，但是它自己的参数在如何更新方面落后于常规的Q网络。</p><figure class="li lj lk ll gt lm gh gi paragraph-image"><div role="button" tabindex="0" class="ln lo di lp bf lq"><div class="gh gi lh"><img src="../Images/573691768e2afba9f872cd35d9e92cf8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*URcZDMRoxI1Tm-GeKIaNAA.png"/></div></div><p class="lt lu gj gh gi lv lw bd b be z dk translated">图1:目标网络的Q学习</p></figure><p id="1640" class="pw-post-body-paragraph jq jr iq jt b ju jv jw jx jy jz ka kb kp kd ke kf kq kh ki kj kr kl km kn ko ij bi translated">上图显示了对目标网络的Q-learning的一般概述。这是普通Q学习算法的一个相当简单的扩展，除了你有第二个Q网络叫做目标网络，它的预测Q值被用来反向传播和训练主Q网络。目标网络的参数不被训练，但是它们周期性地与Q网络的参数同步。想法是使用目标网络的Q值来训练Q网络将提高训练的稳定性。</p><p id="dd5e" class="pw-post-body-paragraph jq jr iq jt b ju jv jw jx jy jz ka kb kp kd ke kf kq kh ki kj kr kl km kn ko ij bi translated">使用目标网络的步骤如下</p><ol class=""><li id="711d" class="kt ku iq jt b ju jv jy jz kp kv kq kw kr kx ko ky kz la lb bi translated">用参数(权重)<em class="js"> θ(Q) </em>初始化Q网络(读作“θQ”)。</li><li id="068f" class="kt ku iq jt b ju lc jy ld kp le kq lf kr lg ko ky kz la lb bi translated">将目标网络初始化为Q网络的副本，但具有单独的参数<em class="js"> θ(T) </em>(读作“θT”)，并设置<em class="js"> θ(T) </em> = <em class="js"> θ(Q) </em>。</li><li id="3cb6" class="kt ku iq jt b ju lc jy ld kp le kq lf kr lg ko ky kz la lb bi translated">使用ε贪婪方法选择具有Q网络的Q值的动作a</li><li id="bd6c" class="kt ku iq jt b ju lc jy ld kp le kq lf kr lg ko ky kz la lb bi translated">观察采取行动a后状态s(t+1)的回报r(t+1)</li><li id="f3a8" class="kt ku iq jt b ju lc jy ld kp le kq lf kr lg ko ky kz la lb bi translated">如果剧集刚刚结束(即游戏是赢是输)，目标网络的Q值将被设置为<em class="js"> r(t </em> +1)，否则将被设置为<em class="js">r(t</em>+1)+<em class="js">γ</em>max<em class="js">Qθr</em>(<em class="js">S(t</em>+1))</li><li id="7674" class="kt ku iq jt b ju lc jy ld kp le kq lf kr lg ko ky kz la lb bi translated">通过Q网络反向传播目标网络的Q值。这里我们不使用Q网络的Q值，因为这会导致学习不稳定</li><li id="b4e1" class="kt ku iq jt b ju lc jy ld kp le kq lf kr lg ko ky kz la lb bi translated">每C次迭代，用Q-网络权重设置目标网络权重</li></ol><p id="9fad" class="pw-post-body-paragraph jq jr iq jt b ju jv jw jx jy jz ka kb kp kd ke kf kq kh ki kj kr kl km kn ko ij bi translated">让我们看看使用PyTorch实现目标网络</p><pre class="li lj lk ll gt lx ly lz ma aw mb bi"><span id="c559" class="mc md iq ly b gy me mf l mg mh">import copy<br/> <br/>model = torch.nn.Sequential(<br/>    torch.nn.Linear(l1, l2),<br/>    torch.nn.ReLU(),<br/>    torch.nn.Linear(l2, l3),<br/>    torch.nn.ReLU(),<br/>    torch.nn.Linear(l3,l4)<br/>)<br/> <br/>model2 = model2 = copy.deepcopy(model)        <strong class="ly ir"><em class="js">1</em></strong><br/>model2.load_state_dict(model.state_dict())    <strong class="ly ir"><em class="js">2</em></strong><br/>sync_freq = 50                                <strong class="ly ir"><em class="js">3</em></strong><br/><br/>loss_fn = torch.nn.MSELoss()<br/>learning_rate = 1e-3<br/>optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)<br/> </span></pre><ul class=""><li id="7dd7" class="kt ku iq jt b ju jv jy jz kp kv kq kw kr kx ko mi kz la lb bi translated"><strong class="jt ir"> <em class="js"> 1 </em> </strong>通过制作原始Q网络模型的相同副本来创建第二个模型</li><li id="ab38" class="kt ku iq jt b ju lc jy ld kp le kq lf kr lg ko mi kz la lb bi translated"><strong class="jt ir"> <em class="js"> 2 </em> </strong>复制原模型的参数</li><li id="3fd2" class="kt ku iq jt b ju lc jy ld kp le kq lf kr lg ko mi kz la lb bi translated"><strong class="jt ir"> <em class="js"> 3 </em> </strong>同步频率参数；每50步我们将把模型的参数复制到模型2中</li></ul><p id="dd35" class="pw-post-body-paragraph jq jr iq jt b ju jv jw jx jy jz ka kb kp kd ke kf kq kh ki kj kr kl km kn ko ij bi translated">让我们现在建立一个DQN与经验重放和目标网络</p><pre class="li lj lk ll gt lx ly lz ma aw mb bi"><span id="3b3c" class="mc md iq ly b gy me mf l mg mh">from collections import deque<br/>epochs = 5000<br/>losses = []<br/>mem_size = 1000<br/>batch_size = 200<br/>replay = deque(maxlen=mem_size)<br/>max_moves = 50<br/>h = 0<br/>sync_freq = 500                                    <strong class="ly ir"><em class="js">1</em></strong><br/>j=0<br/>for i in range(epochs):<br/>    game = Gridworld(size=4, mode='random')<br/>    state1_ = game.board.render_np().reshape(1,64) + np.random.rand(1,64)/100.0<br/>    state1 = torch.from_numpy(state1_).float()<br/>    status = 1<br/>    mov = 0<br/>    while(status == 1): <br/>        j+=1<br/>        mov += 1<br/>        qval = model(state1)<br/>        qval_ = qval.data.numpy()<br/>        if (random.random() &lt; epsilon):<br/>            action_ = np.random.randint(0,4)<br/>        else:<br/>            action_ = np.argmax(qval_)<br/>        <br/>        action = action_set[action_]<br/>        game.makeMove(action)<br/>        state2_ = game.board.render_np().reshape(1,64) + np.random.rand(1,64)/100.0<br/>        state2 = torch.from_numpy(state2_).float()<br/>        reward = game.reward()<br/>        done = True if reward &gt; 0 else False<br/>        exp =  (state1, action_, reward, state2, done)<br/>        replay.append(exp) <br/>        state1 = state2<br/>        <br/>        if len(replay) &gt; batch_size:<br/>            minibatch = random.sample(replay, batch_size)<br/>            state1_batch = torch.cat([s1 for (s1,a,r,s2,d) in minibatch])<br/>            action_batch = torch.Tensor([a for (s1,a,r,s2,d) in minibatch])<br/>            reward_batch = torch.Tensor([r for (s1,a,r,s2,d) in minibatch])<br/>            state2_batch = torch.cat([s2 for (s1,a,r,s2,d) in minibatch])<br/>            done_batch = torch.Tensor([d for (s1,a,r,s2,d) in minibatch])<br/>            Q1 = model(state1_batch) <br/>            with torch.no_grad():<br/>                Q2 = model2(state2_batch)        <strong class="ly ir"><em class="js">2</em></strong><br/>            Y = reward_batch + gamma * ((1-done_batch) * \<br/>            torch.max(Q2,dim=1)[0])<br/>            X = Q1.gather(dim=1,index=action_batch.long() \<br/>            .unsqueeze(dim=1)).squeeze()<br/>            loss = loss_fn(X, Y.detach())<br/>            print(i, loss.item())<br/>            clear_output(wait=True)<br/>            optimizer.zero_grad()<br/>            loss.backward()<br/>            losses.append(loss.item())<br/>            optimizer.step()<br/>            <br/>            if j % sync_freq == 0:               <strong class="ly ir"><em class="js">3</em></strong><br/>                model2.load_state_dict(model.state_dict())<br/>        if reward != -1 or mov &gt; max_moves:<br/>            status = 0<br/>            mov = 0<br/>        <br/>losses = np.array(losses)</span></pre><ul class=""><li id="3816" class="kt ku iq jt b ju jv jy jz kp kv kq kw kr kx ko mi kz la lb bi translated"><strong class="jt ir"> <em class="js"> 1 </em> </strong>设置目标模型参数与主DQN同步的更新频率</li><li id="92ef" class="kt ku iq jt b ju lc jy ld kp le kq lf kr lg ko mi kz la lb bi translated"><strong class="jt ir"> <em class="js"> 2 </em> </strong>使用目标网络获得下一状态的最大Q值</li><li id="95d6" class="kt ku iq jt b ju lc jy ld kp le kq lf kr lg ko mi kz la lb bi translated"><strong class="jt ir"> <em class="js"> 3 </em> </strong>将主模型参数复制到目标网络</li></ul><p id="5eb3" class="pw-post-body-paragraph jq jr iq jt b ju jv jw jx jy jz ka kb kp kd ke kf kq kh ki kj kr kl km kn ko ij bi translated">下面是目标网络的DQN的损失图</p><figure class="li lj lk ll gt lm gh gi paragraph-image"><div role="button" tabindex="0" class="ln lo di lp bf lq"><div class="gh gi mj"><img src="../Images/4baf70d1f692dfd5aad7ddfbfb3553b3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QdHsBxwvu8nN4e511iBcJg.png"/></div></div><p class="lt lu gj gh gi lv lw bd b be z dk translated">图2:目标网络的损耗图</p></figure><p id="5ab8" class="pw-post-body-paragraph jq jr iq jt b ju jv jw jx jy jz ka kb kp kd ke kf kq kh ki kj kr kl km kn ko ij bi translated">我们可以看到亏损有一个更稳定的下降趋势。试验超参数，例如经验重放缓冲区大小、批量大小、目标网络更新频率和学习速率。性能对这些超参数非常敏感。</p><p id="8ffb" class="pw-post-body-paragraph jq jr iq jt b ju jv jw jx jy jz ka kb kp kd ke kf kq kh ki kj kr kl km kn ko ij bi translated">当在1000个游戏上进行实验时，我们在准确率上比仅仅使用经验回放提高了3%。现在准确率在93%左右</p><p id="934d" class="pw-post-body-paragraph jq jr iq jt b ju jv jw jx jy jz ka kb kp kd ke kf kq kh ki kj kr kl km kn ko ij bi translated">这个项目的完整代码可以在这个<a class="ae ks" href="https://github.com/NandaKishoreJoshi/Reinforcement_Lerning/blob/main/RL_course/Ch3_Gridworld/Part%203%20-%20Deep%20Q-learning%20for%20GridWorld%20-%20Complete%20code.ipynb" rel="noopener ugc nofollow" target="_blank"> GIT </a>链接中找到</p><p id="1fbd" class="pw-post-body-paragraph jq jr iq jt b ju jv jw jx jy jz ka kb kp kd ke kf kq kh ki kj kr kl km kn ko ij bi translated">点击这里查看本文的第1部分:</p><p id="c30a" class="pw-post-body-paragraph jq jr iq jt b ju jv jw jx jy jz ka kb kp kd ke kf kq kh ki kj kr kl km kn ko ij bi translated"><a class="ae ks" href="https://nandakishorej8.medium.com/part-1-building-a-deep-q-network-to-play-gridworld-deepminds-deep-q-networks-78842007c631" rel="noopener">https://nandakishorej 8 . medium . com/part-1-building-a-deep-q-network-to-play-grid world-deep minds-deep-q-networks-78842007 c631</a></p><p id="57d1" class="pw-post-body-paragraph jq jr iq jt b ju jv jw jx jy jz ka kb kp kd ke kf kq kh ki kj kr kl km kn ko ij bi translated">点击这里查看本文的第2部分:</p><p id="acb2" class="pw-post-body-paragraph jq jr iq jt b ju jv jw jx jy jz ka kb kp kd ke kf kq kh ki kj kr kl km kn ko ij bi translated"><a class="ae ks" href="https://nandakishorej8.medium.com/part-2-building-a-deep-q-network-to-play-gridworld-catastrophic-forgetting-and-experience-6b2b000910d7" rel="noopener">https://nandakishorej 8 . medium . com/part-2-building-a-deep-q-network-to-play-grid world-灾变-遗忘-体验-6b2b000910d7 </a></p></div></div>    
</body>
</html>