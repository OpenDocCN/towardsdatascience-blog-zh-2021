<html>
<head>
<title>A Beginner’s Guide to Regression Analysis in Machine Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">机器学习中回归分析的初学者指南</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/a-beginners-guide-to-regression-analysis-in-machine-learning-8a828b491bbf?source=collection_archive---------0-----------------------#2021-04-10">https://towardsdatascience.com/a-beginners-guide-to-regression-analysis-in-machine-learning-8a828b491bbf?source=collection_archive---------0-----------------------#2021-04-10</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><figure class="ip iq gp gr ir is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi io"><img src="../Images/858442bd1d014d0aa1ab76eb85d53974.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mnx8PS2_YTKGKvP4xguvFA.png"/></div></div></figure><div class=""/><div class=""><h2 id="8f7e" class="pw-subtitle-paragraph jy ja jb bd b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp dk translated">回归分析用例子，插图，动画和备忘单解释。</h2></div><h1 id="9b53" class="kq kr jb bd ks kt ku kv kw kx ky kz la kh lb ki lc kk ld kl le kn lf ko lg lh bi translated">背景:</h1><p id="7989" class="pw-post-body-paragraph li lj jb lk b ll lm kc ln lo lp kf lq lr ls lt lu lv lw lx ly lz ma mb mc md ij bi translated">为了理解回归背后的动机，让我们考虑下面这个简单的例子。下面的散点图显示了从2001年到2012年美国大学毕业生的数量。</p><figure class="mf mg mh mi gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi me"><img src="../Images/13e05d092c000a09223e6708e614a242.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*x2cMaV5t46OKp56aoOt7tg.jpeg"/></div></div><p class="mj mk gj gh gi ml mm bd b be z dk translated">作者图片</p></figure><p id="5e5d" class="pw-post-body-paragraph li lj jb lk b ll mn kc ln lo mo kf lq lr mp lt lu lv mq lx ly lz mr mb mc md ij bi translated">现在根据现有的数据，如果有人问你2018年有多少硕士毕业的大学生呢？可以看出，拥有硕士学位的大学毕业生人数几乎是随年份线性增长的。因此，通过简单的视觉分析，我们可以粗略估计这个数字在200万到210万之间。让我们看看实际数字。下图描绘了从2001年到2018年的同一个变量。可以看出，我们预测的数字与实际值大致相当。</p><figure class="mf mg mh mi gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi ms"><img src="../Images/c88d9357952c1f0c13766952ba98609b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*g0TgDvCUAyNDMsmW7bRNfQ.jpeg"/></div></div><p class="mj mk gj gh gi ml mm bd b be z dk translated">作者图片</p></figure><p id="03b3" class="pw-post-body-paragraph li lj jb lk b ll mn kc ln lo mo kf lq lr mp lt lu lv mq lx ly lz mr mb mc md ij bi translated">因为这是一个更简单的问题(用一条线来拟合数据)，我们的大脑很容易做到这一点。将一个函数拟合到一组数据点的过程称为回归分析。</p><h1 id="2143" class="kq kr jb bd ks kt ku kv kw kx ky kz la kh lb ki lc kk ld kl le kn lf ko lg lh bi translated">什么是回归分析？</h1><p id="9009" class="pw-post-body-paragraph li lj jb lk b ll lm kc ln lo lp kf lq lr ls lt lu lv lw lx ly lz ma mb mc md ij bi translated">回归分析是估计因变量和自变量之间关系的过程。简而言之，它意味着在某种误差函数下，将选定的函数族中的一个函数拟合到采样数据。回归分析是用于预测的机器学习领域中最基本的工具之一。使用回归，您可以在可用数据上拟合一个函数，并尝试预测未来或持续数据点的结果。这种函数拟合有两个目的。</p><ol class=""><li id="0734" class="mt mu jb lk b ll mn lo mo lr mv lv mw lz mx md my mz na nb bi translated">您可以估计数据范围内的缺失数据(插值)</li><li id="2657" class="mt mu jb lk b ll nc lo nd lr ne lv nf lz ng md my mz na nb bi translated">您可以估计数据范围之外的未来数据(外推法)</li></ol><p id="23ac" class="pw-post-body-paragraph li lj jb lk b ll mn kc ln lo mo kf lq lr mp lt lu lv mq lx ly lz mr mb mc md ij bi translated">回归分析的一些实际例子包括预测给定房屋特征的房屋价格、预测SAT/GRE分数对大学录取的影响、基于输入参数预测销售、预测天气等。</p><p id="01a8" class="pw-post-body-paragraph li lj jb lk b ll mn kc ln lo mo kf lq lr mp lt lu lv mq lx ly lz mr mb mc md ij bi translated">让我们考虑一下之前大学毕业生的例子。</p><ol class=""><li id="8d6d" class="mt mu jb lk b ll mn lo mo lr mv lv mw lz mx md my mz na nb bi translated"><strong class="lk jc">插值:</strong>让我们假设我们可以访问一些稀疏的数据，我们知道每4年大学毕业生的数量，如下图所示。</li></ol><figure class="mf mg mh mi gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi me"><img src="../Images/38c6b9be3011c690e621d1f1b97849bf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4eVV0rx9n81pMOnEDW9Lqg.jpeg"/></div></div><p class="mj mk gj gh gi ml mm bd b be z dk translated">作者图片</p></figure><p id="a39a" class="pw-post-body-paragraph li lj jb lk b ll mn kc ln lo mo kf lq lr mp lt lu lv mq lx ly lz mr mb mc md ij bi translated">我们想估计一下在这之间的所有缺失年份的大学毕业生人数。我们可以通过对有限的可用数据点拟合一条线来做到这一点。这个过程叫做插值。</p><figure class="mf mg mh mi gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi ms"><img src="../Images/d124fda3e63cd2647a1c0b35be5c4005.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4XIWpLQXdZgiUXsWtHuibg.jpeg"/></div></div><p class="mj mk gj gh gi ml mm bd b be z dk translated"><strong class="bd nh">图4 </strong>:作者图片</p></figure><p id="a638" class="pw-post-body-paragraph li lj jb lk b ll mn kc ln lo mo kf lq lr mp lt lu lv mq lx ly lz mr mb mc md ij bi translated"><strong class="lk jc">推断</strong>:假设我们可以获得2001年到2012年的有限数据，我们想预测2013年到2018年的大学毕业生人数。</p><figure class="mf mg mh mi gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi me"><img src="../Images/13e05d092c000a09223e6708e614a242.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*x2cMaV5t46OKp56aoOt7tg.jpeg"/></div></div><p class="mj mk gj gh gi ml mm bd b be z dk translated">作者图片</p></figure><p id="a788" class="pw-post-body-paragraph li lj jb lk b ll mn kc ln lo mo kf lq lr mp lt lu lv mq lx ly lz mr mb mc md ij bi translated">可以看出，拥有硕士学位的大学毕业生人数几乎是随年份线性增长的。因此，对数据集拟合一条线是有意义的。用这12个点来拟合一条线，然后在未来的6个点上测试这条线的预测，可以看出预测非常接近。</p><figure class="mf mg mh mi gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi ni"><img src="../Images/a56bfb85f3baca68085b58263caddbe8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ee_S3W1O36MfwXBYO_uH5Q.png"/></div></div><p class="mj mk gj gh gi ml mm bd b be z dk translated">外推-预测未来看不见的价值-作者图片</p></figure><h2 id="cb6e" class="nj kr jb bd ks nk nl dn kw nm nn dp la lr no np lc lv nq nr le lz ns nt lg nu bi translated">从数学角度来说</h2><figure class="mf mg mh mi gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nv"><img src="../Images/4ea0baf293400868fe16944989c82cca.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tW8bZzZNt17N0lD0P-z7Vg.png"/></div></div></figure><h1 id="5edf" class="kq kr jb bd ks kt ku kv kw kx ky kz la kh lb ki lc kk ld kl le kn lf ko lg lh bi translated">回归分析的类型</h1><p id="7c12" class="pw-post-body-paragraph li lj jb lk b ll lm kc ln lo lp kf lq lr ls lt lu lv lw lx ly lz ma mb mc md ij bi translated">现在让我们来讨论一下进行回归的不同方法。基于函数族(f_beta)和使用的损失函数(l ),我们可以将回归分为以下几类。</p><h2 id="dc12" class="nj kr jb bd ks nk nl dn kw nm nn dp la lr no np lc lv nq nr le lz ns nt lg nu bi translated">1.线性回归</h2><p id="144f" class="pw-post-body-paragraph li lj jb lk b ll lm kc ln lo lp kf lq lr ls lt lu lv lw lx ly lz ma mb mc md ij bi translated">在线性回归中，目标是通过最小化每个数据点的均方误差的和来拟合超平面(2D数据点的线)。</p><p id="84f8" class="pw-post-body-paragraph li lj jb lk b ll mn kc ln lo mo kf lq lr mp lt lu lv mq lx ly lz mr mb mc md ij bi translated">从数学上来说，线性回归解决了以下问题</p><figure class="mf mg mh mi gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nw"><img src="../Images/63b31a4ae9516f0433ed4d278177e080.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*j3LRR2Z-g-r1vZgTqnmPAw.png"/></div></div></figure><p id="9c85" class="pw-post-body-paragraph li lj jb lk b ll mn kc ln lo mo kf lq lr mp lt lu lv mq lx ly lz mr mb mc md ij bi translated">因此，我们需要找到2个用β表示的变量，它们将线性函数f(参数化。).上图4中可以看到一个线性回归的例子，其中P=5。该图还显示了β_ 0 =-90.798和β_ 1 = 0.046的拟合线性函数</p><h2 id="779a" class="nj kr jb bd ks nk nl dn kw nm nn dp la lr no np lc lv nq nr le lz ns nt lg nu bi translated">2.多项式回归</h2><p id="240c" class="pw-post-body-paragraph li lj jb lk b ll lm kc ln lo lp kf lq lr ls lt lu lv lw lx ly lz ma mb mc md ij bi translated">线性回归假设因变量(y)和自变量(x)之间的关系是线性的。当数据点之间的关系不是线性时，它无法拟合数据点。多项式回归通过将m次多项式拟合到数据点来扩展线性回归的拟合能力。所考虑的函数越丰富，它的拟合能力就越好。从数学上讲，多项式回归解决了以下问题。</p><figure class="mf mg mh mi gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nx"><img src="../Images/c547387e5df2b2e7067d6aabf39f791e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rkiqZeZnIoVIifam5y-FEQ.png"/></div></div><p class="mj mk gj gh gi ml mm bd b be z dk translated">多项式回归的数学公式——作者图片</p></figure><p id="3ce0" class="pw-post-body-paragraph li lj jb lk b ll mn kc ln lo mo kf lq lr mp lt lu lv mq lx ly lz mr mb mc md ij bi translated">因此我们需要找到用β_ 0，…，β_ m表示的(m+1)个变量。可以看出，线性回归是二次多项式回归的一个特例。</p><p id="8f8e" class="pw-post-body-paragraph li lj jb lk b ll mn kc ln lo mo kf lq lr mp lt lu lv mq lx ly lz mr mb mc md ij bi translated">考虑下面一组绘制成散点图的数据点。如果我们使用线性回归，我们得到的拟合显然无法估计数据点。但是如果我们使用6次多项式回归，我们会得到更好的拟合，如下所示</p><div class="mf mg mh mi gt ab cb"><figure class="ny is nz oa ob oc od paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><img src="../Images/4b1c201b7af97e2f4ebe279dd911a9ea.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*yL67Ufrbs1mWkAcrPFMzCw.jpeg"/></div></figure><figure class="ny is nz oa ob oc od paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><img src="../Images/8e5b5264cc5384f957daf8c7bbcfca79.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*6v5ae_cuXa_7-tDcbo1W4Q.jpeg"/></div></figure><figure class="ny is nz oa ob oc od paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><img src="../Images/27a0b64b8b4bfa7a50b59413f8fa0a2b.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*ghD5_zDVZIfe-NMUwXJLGQ.jpeg"/></div><p class="mj mk gj gh gi ml mm bd b be z dk oe di of og translated"><strong class="bd nh">【左】</strong>数据散点图— <strong class="bd nh">【中心】</strong>对数据的线性回归— <strong class="bd nh">【右】</strong>6次多项式回归</p></figure></div><p id="35c9" class="pw-post-body-paragraph li lj jb lk b ll mn kc ln lo mo kf lq lr mp lt lu lv mq lx ly lz mr mb mc md ij bi translated">由于数据点在因变量和自变量之间没有线性关系，线性回归无法估计良好的拟合函数。另一方面，多项式回归能够捕捉非线性关系。</p><h2 id="aebf" class="nj kr jb bd ks nk nl dn kw nm nn dp la lr no np lc lv nq nr le lz ns nt lg nu bi translated">3.里脊回归</h2><p id="3f18" class="pw-post-body-paragraph li lj jb lk b ll lm kc ln lo lp kf lq lr ls lt lu lv lw lx ly lz ma mb mc md ij bi translated">岭回归解决了回归分析中的过度拟合问题。为了理解这一点，考虑与上面相同的例子。当在具有10个训练点的数据上拟合25次多项式时，可以看出它完美地拟合了红色数据点(下图中心)。但是这样做，它会损害中间的其他点(最后两个数据点之间的尖峰)。这一点可以从下图中看出。岭回归试图解决这个问题。它试图通过折衷训练点的拟合来最小化泛化误差。</p><div class="mf mg mh mi gt ab cb"><figure class="ny is nz oa ob oc od paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><img src="../Images/4b1c201b7af97e2f4ebe279dd911a9ea.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*yL67Ufrbs1mWkAcrPFMzCw.jpeg"/></div></figure><figure class="ny is nz oa ob oc od paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><img src="../Images/749ea0e6d8c0b61b5c8d5e55608e5cf0.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*KUXhQQg7x-2AZn3-GCeqOA.jpeg"/></div></figure><figure class="ny is nz oa ob oc od paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><img src="../Images/adad3df3ce71d9df2f085b1633a67d19.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*L7vEo8hAEdjF1VhEWfjJuQ.jpeg"/></div><p class="mj mk gj gh gi ml mm bd b be z dk oe di of og translated"><strong class="bd nh">【左】</strong>数据散点图— <strong class="bd nh">【中心】</strong>25次多项式回归— <strong class="bd nh">【右】</strong>25次多项式岭回归</p></figure></div><p id="b644" class="pw-post-body-paragraph li lj jb lk b ll mn kc ln lo mo kf lq lr mp lt lu lv mq lx ly lz mr mb mc md ij bi translated">从数学上讲，岭回归通过修改损失函数解决了以下问题。</p><figure class="mf mg mh mi gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi oh"><img src="../Images/23be93e4d789bb86de86d7dea5e1bf19.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*H22xduqYQEnfuC1IJvMJyg.png"/></div></div></figure><p id="38e2" class="pw-post-body-paragraph li lj jb lk b ll mn kc ln lo mo kf lq lr mp lt lu lv mq lx ly lz mr mb mc md ij bi translated">函数f(x)可以是线性的，也可以是多项式的。在没有岭回归的情况下，当函数过度拟合数据点时，学习到的权重往往相当高。岭回归通过在损失函数中引入权重(β)的缩放L2范数来限制正在学习的权重的范数，从而避免过拟合。因此，训练的模型在完美拟合数据点(学习的权重的大范数)和限制权重的范数之间进行权衡。比例常数α&gt; 0用于控制这种折衷。较小的alpha值将导致较高的范数权重和过拟合训练数据点。另一方面，大的alpha值将导致函数与训练数据点的拟合较差，但权重的范数非常小。仔细选择α的值将产生最佳的折衷。</p><h2 id="8567" class="nj kr jb bd ks nk nl dn kw nm nn dp la lr no np lc lv nq nr le lz ns nt lg nu bi translated">4.套索回归</h2><p id="da83" class="pw-post-body-paragraph li lj jb lk b ll lm kc ln lo lp kf lq lr ls lt lu lv lw lx ly lz ma mb mc md ij bi translated">套索回归类似于岭回归，因为两者都被用作正则项来防止训练数据点上的过度拟合。但是套索还有一个额外的好处。它加强了学习权重的稀疏性。</p><p id="d76d" class="pw-post-body-paragraph li lj jb lk b ll mn kc ln lo mo kf lq lr mp lt lu lv mq lx ly lz mr mb mc md ij bi translated">岭回归将学习到的权重的范数强制为小，产生一组总范数减小的权重。大多数权重(如果不是全部的话)将是非零的。另一方面，LASSO试图通过使大部分权重真正接近于零来找到一组权重。这产生了稀疏权重矩阵，其实现可以比非稀疏权重矩阵更节能，同时在拟合数据点方面保持相似的精度。</p><p id="fa9f" class="pw-post-body-paragraph li lj jb lk b ll mn kc ln lo mo kf lq lr mp lt lu lv mq lx ly lz mr mb mc md ij bi translated">下图试图在上面的同一个例子中形象化这个想法。使用岭回归和套索回归来拟合数据点，并且以升序绘制它们相应的拟合和权重。可以看出，套索回归中的大部分权重确实接近于零。</p><figure class="mf mg mh mi gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi oi"><img src="../Images/abf2715e19f885d19e03adc5c383ce40.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3tDfPkR_U1VSPUCYYV9QmA.jpeg"/></div></div></figure><figure class="mf mg mh mi gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi oi"><img src="../Images/a96006d894c0c302717685eed31e5a99.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OKyl-HOTMJd4pMOSkEFkWQ.jpeg"/></div></div><p class="mj mk gj gh gi ml mm bd b be z dk translated">作者图片</p></figure><p id="fc67" class="pw-post-body-paragraph li lj jb lk b ll mn kc ln lo mo kf lq lr mp lt lu lv mq lx ly lz mr mb mc md ij bi translated">从数学上讲，LASSO回归通过修改损失函数解决了以下问题。</p><figure class="mf mg mh mi gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi oj"><img src="../Images/383227a21e0cbc9d5eca24449207be7c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3uLgJx3x26o9GlgqK-Z7Ow.png"/></div></div></figure><p id="9429" class="pw-post-body-paragraph li lj jb lk b ll mn kc ln lo mo kf lq lr mp lt lu lv mq lx ly lz mr mb mc md ij bi translated">LASSO和岭回归的区别在于LASSO使用权重的L1范数而不是L2范数。损失函数中的这种L1范数倾向于增加学习到的权重的稀疏性。关于它如何加强稀疏性的更多细节可以在下面文章的章节<strong class="lk jc"> <em class="ok"> L1正则化</em> </strong>中找到。</p><div class="ip iq gp gr ir ol"><a rel="noopener follow" target="_blank" href="/types-of-regularization-in-machine-learning-eb5ce5f9bf50"><div class="om ab fo"><div class="on ab oo cl cj op"><h2 class="bd jc gy z fp oq fr fs or fu fw ja bi translated">机器学习中的正则化类型</h2><div class="os l"><h3 class="bd b gy z fp oq fr fs or fu fw dk translated">机器学习正则化初学者指南。</h3></div><div class="ot l"><p class="bd b dl z fp oq fr fs or fu fw dk translated">towardsdatascience.com</p></div></div><div class="ou l"><div class="ov l ow ox oy ou oz ix ol"/></div></div></a></div><p id="a78c" class="pw-post-body-paragraph li lj jb lk b ll mn kc ln lo mo kf lq lr mp lt lu lv mq lx ly lz mr mb mc md ij bi translated">常数α&gt; 0用于控制学习权重中拟合和稀疏度之间的折衷。较大的α值会导致较差的拟合，但会导致学习到的权重集更稀疏。另一方面，较小的alpha值导致训练数据点的紧密拟合(可能会导致过度拟合)，但权重集不太稀疏。</p><h2 id="39cc" class="nj kr jb bd ks nk nl dn kw nm nn dp la lr no np lc lv nq nr le lz ns nt lg nu bi translated">5.弹性网络回归</h2><p id="c454" class="pw-post-body-paragraph li lj jb lk b ll lm kc ln lo lp kf lq lr ls lt lu lv lw lx ly lz ma mb mc md ij bi translated">ElasticNet回归是脊和套索回归的组合。损失项包括权重的L1和L2范数以及它们各自的标度常数。它通常用于解决套索回归的局限性，例如非凸性。ElasticNet增加了权重的二次惩罚，使其主要为凸形。</p><p id="6197" class="pw-post-body-paragraph li lj jb lk b ll mn kc ln lo mo kf lq lr mp lt lu lv mq lx ly lz mr mb mc md ij bi translated">从数学上来说，ElasticNet回归通过修改损失函数解决了以下问题。</p><figure class="mf mg mh mi gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi pa"><img src="../Images/7855ef1b2460752808fe6e67809e1f2d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*c2xXFTNme6aksG7JKz4RGw.png"/></div></div></figure><h2 id="6679" class="nj kr jb bd ks nk nl dn kw nm nn dp la lr no np lc lv nq nr le lz ns nt lg nu bi translated">6.贝叶斯回归</h2><p id="5c0c" class="pw-post-body-paragraph li lj jb lk b ll lm kc ln lo lp kf lq lr ls lt lu lv lw lx ly lz ma mb mc md ij bi translated">对于上面讨论的回归(frequentists方法)，目标是找到一组解释数据的确定性权重值(β)。在贝叶斯回归中，我们不是为每个权重找到一个值，而是试图在假设先验的情况下找到这些权重的分布。</p><p id="f5b8" class="pw-post-body-paragraph li lj jb lk b ll mn kc ln lo mo kf lq lr mp lt lu lv mq lx ly lz mr mb mc md ij bi translated">因此，我们从权重的初始分布开始，并根据数据，利用贝叶斯定理将分布推向正确的方向，贝叶斯定理基于可能性和证据将先验分布与后验分布相关联。</p><figure class="mf mg mh mi gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi pb"><img src="../Images/fb138171271d0735c63d5b11fc48783e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3HNstMp2IXo9TyQOmkOmwQ.png"/></div></div></figure><p id="7152" class="pw-post-body-paragraph li lj jb lk b ll mn kc ln lo mo kf lq lr mp lt lu lv mq lx ly lz mr mb mc md ij bi translated">当我们有无限个数据点时，权重的后验分布在普通最小二乘解的解处变成一个脉冲，即方差接近零。</p><p id="f65d" class="pw-post-body-paragraph li lj jb lk b ll mn kc ln lo mo kf lq lr mp lt lu lv mq lx ly lz mr mb mc md ij bi translated">找到权重的分布而不是一组确定性值有两个目的</p><ol class=""><li id="2777" class="mt mu jb lk b ll mn lo mo lr mv lv mw lz mx md my mz na nb bi translated">它自然可以防止过度拟合的问题，因此可以作为一个正则项</li><li id="8dea" class="mt mu jb lk b ll nc lo nd lr ne lv nf lz ng md my mz na nb bi translated">它提供了信心和权重范围，这比只返回一个值更有逻辑意义。</li></ol><p id="a584" class="pw-post-body-paragraph li lj jb lk b ll mn kc ln lo mo kf lq lr mp lt lu lv mq lx ly lz mr mb mc md ij bi translated">让我们用数学方法来表述这个问题，并说明它的解决方法。</p><figure class="mf mg mh mi gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi pc"><img src="../Images/f308e64673615c806c4ce2fc3516cbef.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3opwYaIL7S7XcB4W7ehwdw.png"/></div></div></figure><p id="c647" class="pw-post-body-paragraph li lj jb lk b ll mn kc ln lo mo kf lq lr mp lt lu lv mq lx ly lz mr mb mc md ij bi translated">让我们对具有均值<strong class="lk jc"> μ </strong>和协方差<strong class="lk jc">σ</strong>的权重进行高斯先验，即</p><figure class="mf mg mh mi gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi pd"><img src="../Images/746c2c02fdf69692718f9d76ec9622cf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QQ40kokCVZDsDjI3lpHTVw.png"/></div></div></figure><p id="7a8d" class="pw-post-body-paragraph li lj jb lk b ll mn kc ln lo mo kf lq lr mp lt lu lv mq lx ly lz mr mb mc md ij bi translated">基于可用的数据D，我们更新这个分布。对于手头的问题，后验概率将是具有以下参数的高斯分布</p><figure class="mf mg mh mi gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi pe"><img src="../Images/253cb430b896310f1a0885c9b282dafc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NOFWBSBv1pJUXxkOtORAXQ.png"/></div></div></figure><p id="f406" class="pw-post-body-paragraph li lj jb lk b ll mn kc ln lo mo kf lq lr mp lt lu lv mq lx ly lz mr mb mc md ij bi translated">详细的数学解释可以在<a class="ae pf" href="https://cedar.buffalo.edu/~srihari/CSE574/Chap3/3.4-BayesianRegression.pdf" rel="noopener ugc nofollow" target="_blank">这里</a>找到</p><p id="a62f" class="pw-post-body-paragraph li lj jb lk b ll mn kc ln lo mo kf lq lr mp lt lu lv mq lx ly lz mr mb mc md ij bi translated">让我们通过观察顺序贝叶斯线性回归，一次更新一个数据点的权重分布，来直观地理解它。如下图</p><figure class="mf mg mh mi gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi pg"><img src="../Images/5110d706f892d5e523ba9b114549528b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ii-Uan8neJUpgH_-pYWQ3w.png"/></div></div><p class="mj mk gj gh gi ml mm bd b be z dk translated">贝叶斯回归基于输入数据(x，y)-作者提供的图片，在正确的方向上推动后验分布</p></figure><p id="b5ab" class="pw-post-body-paragraph li lj jb lk b ll mn kc ln lo mo kf lq lr mp lt lu lv mq lx ly lz mr mb mc md ij bi translated">随着每个数据点的包含，权重的分布变得更接近实际的基本分布。</p><p id="69ea" class="pw-post-body-paragraph li lj jb lk b ll mn kc ln lo mo kf lq lr mp lt lu lv mq lx ly lz mr mb mc md ij bi translated">下面的动画绘制了原始数据、预测的四分位间距、权重的边际后验分布以及考虑单个新数据点时每个时间步长的权重联合分布。可以看出，随着我们包括更多的点，四分位范围变得更窄(绿色阴影区域)，边际分布分布在两个权重参数周围，方差接近零，联合分布在实际权重处收敛。</p><figure class="mf mg mh mi gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi me"><img src="../Images/f09632ef61d8645ae807c36ce35d8e53.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*dbNBFq6FLyMqRxLqosBaSQ.gif"/></div></div><p class="mj mk gj gh gi ml mm bd b be z dk translated">动画贝叶斯回归——来源:<a class="ae pf" href="http://www.MLinGIFS.aqeel-anwar.com" rel="noopener ugc nofollow" target="_blank">www.MLinGIFS.aqeel-anwar.com</a></p></figure><h2 id="439a" class="nj kr jb bd ks nk nl dn kw nm nn dp la lr no np lc lv nq nr le lz ns nt lg nu bi translated">7.逻辑回归</h2><p id="da59" class="pw-post-body-paragraph li lj jb lk b ll lm kc ln lo lp kf lq lr ls lt lu lv lw lx ly lz ma mb mc md ij bi translated">在输出需要是给定输入的输出的条件概率的分类任务中，逻辑回归很方便。从数学角度来说，逻辑回归解决了以下问题</p><figure class="mf mg mh mi gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi pa"><img src="../Images/fc9de6947a38941bd0e0e5b3bfb21f59.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TaH5Z6JnOxloz9B6wvPsmA.png"/></div></div></figure><p id="1687" class="pw-post-body-paragraph li lj jb lk b ll mn kc ln lo mo kf lq lr mp lt lu lv mq lx ly lz mr mb mc md ij bi translated">考虑下面的例子，其中数据点属于两个类别之一:{0(红色)，1(黄色)}，如下面的散点图所示。</p><div class="mf mg mh mi gt ab cb"><figure class="ny is ph oa ob oc od paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><img src="../Images/5aa287953b3bdaa937df9c19bf003427.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*uUVC_q34eKpv6TBnF8Cf-w.jpeg"/></div></figure><figure class="ny is ph oa ob oc od paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><img src="../Images/f135823b99d5314af7e7636639b0e894.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*YzyOiG3VG-lMJYXoGpJzPA.jpeg"/></div><p class="mj mk gj gh gi ml mm bd b be z dk pi di pj og translated">[左]数据点散点图—[右]对以蓝色绘制的数据点进行逻辑回归</p></figure></div><p id="fad4" class="pw-post-body-paragraph li lj jb lk b ll mn kc ln lo mo kf lq lr mp lt lu lv mq lx ly lz mr mb mc md ij bi translated">逻辑回归在线性或多项式函数的输出端使用sigmoid函数，将(♾️-♾️)的输出映射到(0，1)。然后使用阈值(通常为0.5)将测试数据分为两类。</p><blockquote class="pk pl pm"><p id="1947" class="li lj ok lk b ll mn kc ln lo mo kf lq pn mp lt lu po mq lx ly pp mr mb mc md ij bi translated">这看起来像是逻辑回归不是回归而是一种分类算法。但事实并非如此。你可以在艾德里安的帖子里找到更多关于T2的信息。</p></blockquote><h1 id="73ee" class="kq kr jb bd ks kt ku kv kw kx ky kz la kh lb ki lc kk ld kl le kn lf ko lg lh bi translated">摘要</h1><p id="9a3f" class="pw-post-body-paragraph li lj jb lk b ll lm kc ln lo lp kf lq lr ls lt lu lv lw lx ly lz ma mb mc md ij bi translated">在本文中，我们研究了回归分析中的各种方法，它们的动机是什么以及如何使用它们。下面的表格和备忘单总结了上面讨论的不同方法。</p><figure class="mf mg mh mi gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi pq"><img src="../Images/0228fc31e8351d7937d8139f43fa9eda.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9t0WcZEzrL5DPG8WJsKYZA.png"/></div></div><p class="mj mk gj gh gi ml mm bd b be z dk translated">回归分析摘要-按作者分类的图片</p></figure><figure class="mf mg mh mi gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi pr"><img src="../Images/c23a3ebe1b4188c248ab7ec3183295e4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cZ8Fsltopg2ULtNdbFYtUQ.jpeg"/></div></div><p class="mj mk gj gh gi ml mm bd b be z dk translated">来源:http://www.cheatsheets.aqeel-anwar.com<a class="ae pf" href="http://www.cheatsheets.aqeel-anwar.com" rel="noopener ugc nofollow" target="_blank"/></p></figure><h1 id="1f5e" class="kq kr jb bd ks kt ku kv kw kx ky kz la kh lb ki lc kk ld kl le kn lf ko lg lh bi translated">奖金:</h1><p id="f2a1" class="pw-post-body-paragraph li lj jb lk b ll lm kc ln lo lp kf lq lr ls lt lu lv lw lx ly lz ma mb mc md ij bi translated">可以在下面的链接中找到这个主题和机器学习中许多其他重要主题的紧凑备忘单</p><div class="ip iq gp gr ir ol"><a href="https://medium.com/swlh/cheat-sheets-for-machine-learning-interview-topics-51c2bc2bab4f" rel="noopener follow" target="_blank"><div class="om ab fo"><div class="on ab oo cl cj op"><h2 class="bd jc gy z fp oq fr fs or fu fw ja bi translated">机器学习面试主题的备忘单</h2><div class="os l"><h3 class="bd b gy z fp oq fr fs or fu fw dk translated">ML面试的视觉备忘单(www.cheatsheets.aqeel-anwar.com)</h3></div><div class="ot l"><p class="bd b dl z fp oq fr fs or fu fw dk translated">medium.com</p></div></div><div class="ou l"><div class="ps l ow ox oy ou oz ix ol"/></div></div></a></div></div><div class="ab cl pt pu hu pv" role="separator"><span class="pw bw bk px py pz"/><span class="pw bw bk px py pz"/><span class="pw bw bk px py"/></div><div class="ij ik il im in"><p id="e07e" class="pw-post-body-paragraph li lj jb lk b ll mn kc ln lo mo kf lq lr mp lt lu lv mq lx ly lz mr mb mc md ij bi translated">如果这篇文章对你有帮助，欢迎鼓掌、分享和回复。如果想了解更多关于机器学习和数据科学的知识，关注我<a class="qa qb ep" href="https://medium.com/u/a7cc4f201fb5?source=post_page-----8a828b491bbf--------------------------------" rel="noopener" target="_blank"><strong class="lk jc">Aqeel an war</strong></a><strong class="lk jc">，或者在</strong><a class="ae pf" href="https://www.linkedin.com/in/aqeelanwarmalik/" rel="noopener ugc nofollow" target="_blank"><strong class="lk jc"><em class="ok">LinkedIn</em></strong></a><strong class="lk jc"><em class="ok">上联系我。</em> </strong></p></div></div>    
</body>
</html>