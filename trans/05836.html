<html>
<head>
<title>FAMD: How to generalize PCA to categorical and numerical data</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">FAMD:如何将主成分分析推广到分类和数值数据</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/famd-how-to-generalize-pca-to-categorical-and-numerical-data-2ddbeb2b9210?source=collection_archive---------7-----------------------#2021-05-25">https://towardsdatascience.com/famd-how-to-generalize-pca-to-categorical-and-numerical-data-2ddbeb2b9210?source=collection_archive---------7-----------------------#2021-05-25</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="caf0" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">让我们深入研究FAMD的理论和实现，这是一种非常有效(但却被误解)的技术，可以将PCA应用于所有类型的变量。</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/9e96801a62f6df00968cf5eb59e1f2d9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*dbDTboECkwwJWfq2"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">Mathilda Khoo 在<a class="ae kv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="3ac6" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">本文介绍了混合数据的<strong class="ky ir">因子分析(FAMD) </strong>，它将主成分分析(PCA)算法推广到包含数值和分类变量的数据集。为了让讲师准确理解这种技术，本文将深入FAMD背后的数学规则，并展示一些演示来证明重要的结果。如果你没有兴趣了解它为什么会起作用，可以直接上III). 3。以了解如何在您的程序中轻松实现它。最后，你可以在<a class="ae kv" href="https://medium.com/fifty-five-data-science/how-to-delete-bias-in-your-dataset-before-applying-a-clustering-ff1dfd14d179" rel="noopener">这篇文章</a>中看到这一理论的一个应用示例，其中FAMD在一个聚类用例中帮助删除数据集中的偏差。享受阅读吧！</p></div><div class="ab cl ls lt hu lu" role="separator"><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx"/></div><div class="ij ik il im in"><h1 id="c399" class="lz ma iq bd mb mc md me mf mg mh mi mj jw mk jx ml jz mm ka mn kc mo kd mp mq bi translated">I]导言</h1><p id="7e13" class="pw-post-body-paragraph kw kx iq ky b kz mr jr lb lc ms ju le lf mt lh li lj mu ll lm ln mv lp lq lr ij bi translated">主成分分析(PCA)是统计分析领域中广泛使用的技术。考虑通过<em class="mw"> P </em>变量描述的<em class="mw"> N </em>个数据点的初始数据集，其目标是通过寻找<em class="mw">K</em>(1≤<em class="mw">K</em>≤<em class="mw">P</em>主成分来减少表示每个数据点所需的维数。通过每次寻找<strong class="ky ir">尽可能解释惯性的轴</strong>，这些主成分被迭代计算，同时与之前的主成分正交(图1)。最后，我们可以通过<em class="mw"> K </em>主成分直接描述数据点。尽管它们更难解释，但它们使我们能够从原始数据集中保留尽可能多的信息，同时大大减少了变量的数量。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mx"><img src="../Images/224196c2923bbabe4fdf2a5b2620791f.png" data-original-src="https://miro.medium.com/v2/resize:fit:904/format:webp/1*84iwBfayVDjoppws0D2Piw.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><em class="my">图1:在图上，我们表示一个人群的体重和身高(居中并缩放后)。PCA获得的第一个分量(可以解释为“跨度”或“体积”)是在一个维度上表示这个特定数据集的最佳轴，因为当我们在这个轴上投影点时，我们丢失的信息量最少。(图片由作者提供)</em></p></figure><p id="9624" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在实践中，PCA通常用于减少存储信息所需的内存，或者有效地可视化数据，通常在平面图(二维)中。如果你对PCA不熟悉，并且这个快速演示感到模糊，我建议你在继续之前阅读<a class="ae kv" href="https://machine-learning-tutorial-abi.readthedocs.io/en/latest/content/unsupervised/pca.html" rel="noopener ugc nofollow" target="_blank"> <em class="mw">这篇文章</em> </a>。</p><p id="775f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">虽然这种算法对数值数据非常有效，但它不能直接考虑分类数据。在本文中，我们将介绍FAMD，这是一种考虑到数值变量和分类变量的主成分分析的推广，同时在最终成分的产生方面给予每个变量相似的重要性。</p><h1 id="20e4" class="lz ma iq bd mb mc mz me mf mg na mi mj jw nb jx ml jz nc ka mn kc nd kd mp mq bi translated">II]理论概念</h1><p id="fba0" class="pw-post-body-paragraph kw kx iq ky b kz mr jr lb lc ms ju le lf mt lh li lj mu ll lm ln mv lp lq lr ij bi translated">首先也是最重要的:在本文的其余部分，我们将讨论一个在ℝ取值的数值变量(例如，年龄、工资、身高等)。)，以及当分类变量在一系列<em class="mw"> m </em>个可能模态中取值时的分类变量，其中<em class="mw"> m </em> ≥2(例如，眼睛颜色、婚姻状况、国家、性别等。).</p><h2 id="b0fc" class="ne ma iq bd mb nf ng dn mf nh ni dp mj lf nj nk ml lj nl nm mn ln nn no mp np bi translated">1.相关系数和相关比率</h2><p id="1693" class="pw-post-body-paragraph kw kx iq ky b kz mr jr lb lc ms ju le lf mt lh li lj mu ll lm ln mv lp lq lr ij bi translated">两个数值变量<em class="mw"> X </em>和<em class="mw"> Y </em>之间的线性相关系数(又名皮尔逊相关)<em class="mw"> r(X，Y) </em>取[-1，1]中的值。接近1的值(分别为。-1)意味着这两个变量是正的(分别为。负)相关，接近0的值意味着变量之间不是线性相关的(图2)。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nq"><img src="../Images/02622e758f4af78950634eed28ca900f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_KKJOBPryjTDW3YVZ8QNYQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><em class="my">(图片作者)</em></p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nr"><img src="../Images/49dcb872a2bb226dcabedea5d52750ff.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lTI_fjvo-MV9GGTUKfwznA.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><em class="my">图2:不同变量的相关系数。(图片由作者提供)</em></p></figure><p id="fc21" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">为了得到两个变量之间相关性的直接指示，我们使用<strong class="ky ir">平方相关系数<em class="mw"> r (X，Y) </em> </strong>。它取[0，1]中的值，并且随着<em class="mw"> X </em>和<em class="mw"> Y </em>更明显地相关，它将变得越来越高。</p><p id="ab59" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">另一方面，<strong class="ky ir">分类变量<em class="mw"> A </em>和数值变量<em class="mw"> Y </em>之间的平方相关比𝜂 <em class="mw"> (A，Y) </em> </strong>也取0到1之间的值。解释该度量的直观方式如下:𝜂 <em class="mw"> (A，Y) </em>越接近1，<strong class="ky ir">对<em class="mw"> A </em>所采用的模态的了解越多，这就有力地表明了<em class="mw"> Y </em>所采用的值。</strong>例如，我们可以想象，在一个由身高<em class="mw"> H </em>、性别<em class="mw"> S </em>和发色<em class="mw"> C </em>、𝜂 <em class="mw"> (C，H) </em>描述的大型人口数据集上，这些变量应该接近于0(因为这些变量没有任何关联)，而𝜂 <em class="mw"> (S，H) </em>可能不可忽略(因为男性通常比女性高)。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nq"><img src="../Images/6d38425d00b2be76e51f1d917cedc947.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cmIdMrsRmSLF9dRlHNt3Tw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><em class="my">(图片作者)</em></p></figure><p id="2531" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">因此，<em class="mw"> r (X，Y) </em>和𝜂 <em class="mw"> (A，Y) </em>都表示与<em class="mw"> Y </em>的相关性，取[0，1]中的值，并随着变量(<em class="mw"> X </em>或<em class="mw"> A </em>)与<em class="mw"> Y </em>显著相关而变高。因此，我们可以在对数字变量或分类变量给予同等重视的同时对它们进行比较。</p><h2 id="a394" class="ne ma iq bd mb nf ng dn mf nh ni dp mj lf nj nk ml lj nl nm mn ln nn no mp np bi translated">2.惯性和投影惯性</h2><p id="0407" class="pw-post-body-paragraph kw kx iq ky b kz mr jr lb lc ms ju le lf mt lh li lj mu ll lm ln mv lp lq lr ij bi translated">现在让我们从变量的角度来看我们的数据集。我们考虑用<em class="mw"> N </em>维表示的<em class="mw"> P </em>变量(或列)<em class="mw"> (vⱼ) </em>(通过我们初始表的<em class="mw"> N </em>数据点)。我们假设<em class="mw"> P </em>变量为</p><ul class=""><li id="0734" class="ns nt iq ky b kz la lc ld lf nu lj nv ln nw lr nx ny nz oa bi translated">centered (mean( <em class="mw"> vⱼ </em> )=0):必要的，因为我们正在处理线性相关性</li><li id="f516" class="ns nt iq ky b kz ob lc oc lf od lj oe ln of lr nx ny nz oa bi translated">scaled (std( <em class="mw"> vⱼ </em> )=1):技术上不是强制性的，但是我们希望以类似的方式缩放所有变量，在计算惯性时给它们相同的初始权重。</li></ul><p id="201c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">☀ <em class="mw">并且稍微提醒一下:对于标量积</em> ⟨.,.⟩ <em class="mw">，我们将关联的范数和距离定义如下:</em></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nq"><img src="../Images/df8862895b8265d3076993b914771a82.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*K0uCkPu9KWv60ezROyk6dA.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><em class="my">(图片作者)</em></p></figure><p id="fb15" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们将变量<em class="mw"> (vⱼ) </em>的惯性定义为:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nq"><img src="../Images/8374146179b6af8e9990c84cd9f02eee.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hrUKg2Mf1MVtkr8LlKNPow.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><em class="my">(图片作者)</em></p></figure><p id="6bc6" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">其中<em class="mw"> pⱼ </em>是变量的相对权重。在大多数情况下，我们可以对所有变量取<em class="mw"> pⱼ </em> = 1，给它们相同的惯性影响。我们还可以定义<em class="mw"> (vⱼ) </em>在由矢量<em class="mw"> w </em>支撑的轴上的投影惯性:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi og"><img src="../Images/ef2e658f585ea90ad76bab26245bfbff.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4KhIxwxIjwEiTNns60f1jQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><em class="my">(图片作者)</em></p></figure><h2 id="dba3" class="ne ma iq bd mb nf ng dn mf nh ni dp mj lf nj nk ml lj nl nm mn ln nn no mp np bi translated">3.主成分分析算法的机理</h2><p id="bbd4" class="pw-post-body-paragraph kw kx iq ky b kz mr jr lb lc ms ju le lf mt lh li lj mu ll lm ln mv lp lq lr ij bi translated">PCA算法的工作步骤如下:</p><ol class=""><li id="79a9" class="ns nt iq ky b kz la lc ld lf nu lj nv ln nw lr oh ny nz oa bi translated">我们选择了多个组件<em class="mw">K</em>(1≤<em class="mw">K</em>≤<em class="mw">P</em>)</li><li id="1a67" class="ns nt iq ky b kz ob lc oc lf od lj oe ln of lr oh ny nz oa bi translated">对于从1到K的<em class="mw"> k </em>:</li></ol><ul class=""><li id="3c33" class="ns nt iq ky b kz la lc ld lf nu lj nv ln nw lr nx ny nz oa bi translated">我们将主成分<em class="mw"> Cₖ </em>定义为以ℝⁿ为中心并且与前面的主成分 <strong class="ky ir">正交的<strong class="ky ir">单位向量，其最大化变量<em class="mw">【vⱼ】</em></strong>(其中<em class="mw"> pⱼ </em> = 1用于所有变量)。一个快速的演示(在论文<strong class="ky ir"> <em class="mw"> (A) </em> </strong>的末尾给出)表明，这个定义相当于说，在每一步，<em class="mw"> Cₖ </em>最大化与数据集变量的平方相关:</strong></li></ul><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nq"><img src="../Images/967c6485921ac9ad76377fcdea4a928d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jffWePyfFUt081okERqfcQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><em class="my">(图片作者)</em></p></figure><h1 id="ad7f" class="lz ma iq bd mb mc mz me mf mg na mi mj jw nb jx ml jz nc ka mn kc nd kd mp mq bi translated">FAMD:理论与应用</h1><h2 id="9387" class="ne ma iq bd mb nf ng dn mf nh ni dp mj lf nj nk ml lj nl nm mn ln nn no mp np bi translated">1.FAMD背后的想法</h2><p id="1eff" class="pw-post-body-paragraph kw kx iq ky b kz mr jr lb lc ms ju le lf mt lh li lj mu ll lm ln mv lp lq lr ij bi translated">在本文的开始，你们中的一些人可能会想“为什么不在应用PCA算法之前简单地对分类变量进行一次性编码呢？”</p><p id="b366" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">☀ <em class="mw">一键编码将数据集中的分类数据转换为数字数据，为每个分类变量的每个模态提供一列，如果变量采用模态，则为1，否则为0(图3): </em></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oi"><img src="../Images/0ff9949b53789914b33ddd6de76125ac.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tMXdGi0izcEM0Ay0psbNgw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><em class="my">图3:两个分类变量的一键编码。(图片由作者提供)</em></p></figure><p id="99c7" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">正如我们所看到的，PCA最大化了所有柱在给定部件上的投影惯性(公平分布)。如果我们直接在包含一个热编码模态的表格上应用PCA，给予分类变量的惯性将固有地取决于变量可用的模态的数量，以及这些模态的概率。因此，<strong class="ky ir">不可能给所有初始变量一个类似的权重超过计算出的分量。</strong></p><blockquote class="oj ok ol"><p id="d66c" class="kw kx mw ky b kz la jr lb lc ld ju le om lg lh li on lk ll lm oo lo lp lq lr ij bi translated">相反，当搜索K个主成分时，FAMD算法希望给予所有变量<strong class="ky ir">完全相同的权重，无论是数字变量还是分类变量</strong>。为此，当vⱼ是数值时，它将迭代地寻找使r (vⱼ,Cₖ)最大化的分量Cₖ，当vⱼ是分类的时，它将迭代地寻找𝜂。</p></blockquote><p id="ab80" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">事实上，正如我们之前看到的，这两个量是可比较的，并且以相同的方式翻译相似的信息。利用由<em class="mw"> P </em>描述的初始数据集，数值变量<em class="mw"> (vⱼ)(1≤j≤P) </em>和<em class="mw"> Q </em>分类变量<em class="mw"> (vⱼ)(P+1≤j≤P+Q) </em>、<em class="mw">、Cₖ </em>将被定义如下:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi og"><img src="../Images/aa2a21ccf0e133a1443265e15fcf9871.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eGiUrrQp5I14MbJWLuu5vw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><em class="my">(图片作者)</em></p></figure><p id="8898" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">只剩下一个问题:我们如何获得这些部件？</p><h2 id="b85f" class="ne ma iq bd mb nf ng dn mf nh ni dp mj lf nj nk ml lj nl nm mn ln nn no mp np bi translated">2.定理</h2><p id="7aae" class="pw-post-body-paragraph kw kx iq ky b kz mr jr lb lc ms ju le lf mt lh li lj mu ll lm ln mv lp lq lr ij bi translated">我们来考虑一个<em class="mw"> P </em>数值变量<em class="mw"> (vⱼ) (1≤j≤P) </em>的表格；和<em class="mw"> Q </em>分类变量<em class="mw"> (vⱼ) (P+1≤j≤P+Q) </em>，它们各自呈现<em class="mw"> Mⱼ </em>模态。有了<em class="mw"> M </em> =∑ <em class="mw"> Mⱼ </em>，我们称之为<em class="mw"> T </em>大小为<em class="mw">n</em>*(<em class="mw">p</em>+<em class="mw">m</em>的表格，包含<em class="mw"> P </em>数值变量，紧挨着<em class="mw"> M </em>单热编码模态。我们对<em class="mw"> T </em>应用以下步骤:</p><p id="08ef" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">对于<em class="mw"> P </em>数值栏:</p><ul class=""><li id="0a60" class="ns nt iq ky b kz la lc ld lf nu lj nv ln nw lr nx ny nz oa bi translated">将它们缩放至1</li><li id="cee1" class="ns nt iq ky b kz ob lc oc lf od lj oe ln of lr nx ny nz oa bi translated">将它们居中</li></ul><p id="7c45" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">对于<em class="mw"> M </em>分类模态:</p><ul class=""><li id="8854" class="ns nt iq ky b kz la lc ld lf nu lj nv ln nw lr nx ny nz oa bi translated"><strong class="ky ir">我们用模态</strong>的概率<em class="mw"> μₘ </em>来划分每一列(一列中的数除以<em class="mw"> N </em>)</li><li id="6c6f" class="ns nt iq ky b kz ob lc oc lf od lj oe ln of lr nx ny nz oa bi translated">将它们居中</li></ul><p id="4276" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">最后，为了计算投影惯性，我们给数字列加权<strong class="ky ir"><em class="mw">【pₙᵤₘ】</em>= 1</strong>，给分类变量的模态<strong class="ky ir"><em class="mw">【pcₐₜ</em>=<em class="mw">【μₘ</em></strong>(模态的概率)。</p><p id="063f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这些在<em class="mw"> T </em>上的编码和权重的选择产生了FAMD的基本方程:对于在ℝⁿ:的<em class="mw"> w </em>中心向量</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nq"><img src="../Images/b936520f70f3c4ffc953c619897381f6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HHj1d98wGdkSVNlKn8uk4w.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><em class="my">(图片作者)</em></p></figure><p id="0822" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">虽然第一条线是直接获得的(与PCA中的推理相同)，但第二条线不是。对于比较好奇的讲师，全文演示<strong class="ky ir"> <em class="mw"> (B) </em> </strong>在本文最后呈现。</p><p id="cea5" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">用这种方法，我们能够把一个数值(相应地。分类)变量在轴上与其平方相关系数(分别为。平方相关比)与轴的支持向量！因此，每个变量——无论是分类变量还是数字变量——都会带来一个在0和1之间变化的投影惯性，这取决于它与向量的相关性。</p><h2 id="8591" class="ne ma iq bd mb nf ng dn mf nh ni dp mj lf nj nk ml lj nl nm mn ln nn no mp np bi translated">3.在实践中</h2><p id="6ac8" class="pw-post-body-paragraph kw kx iq ky b kz mr jr lb lc ms ju le lf mt lh li lj mu ll lm ln mv lp lq lr ij bi translated">FAMD算法由两部分组成:</p><ul class=""><li id="982a" class="ns nt iq ky b kz la lc ld lf nu lj nv ln nw lr nx ny nz oa bi translated">以适当的方式编码数据</li><li id="7fb9" class="ns nt iq ky b kz ob lc oc lf od lj oe ln of lr nx ny nz oa bi translated">迭代地寻找K个主成分</li></ul><p id="b050" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">主成分的研究以与PCA完全相同的方式进行。因此，我们所要做的就是应用一个经典的PCA算法(scikit-learn等。)在编码数据上。不幸的是，PCA的大多数程序不允许增加对每根柱子的惯性的考虑。为了克服这个困难，我们使用下面的结果:<em class="mw">用sqrt(μₘ(而不是μₘ)来划分模态的独热编码列将具有与应用适当加权相同的效果。</em>这一论断的证明也可以在本文末尾<strong class="ky ir"><em class="mw">【C】</em></strong>找到。</p><p id="406f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">总的来说，FAMD算法是这样的:</p><blockquote class="oj ok ol"><p id="533e" class="kw kx mw ky b kz la jr lb lc ld ju le om lg lh li on lk ll lm oo lo lp lq lr ij bi translated">a)标准标度数值变量(=获得z值)</p><p id="4d2f" class="kw kx mw ky b kz la jr lb lc ld ju le om lg lh li on lk ll lm oo lo lp lq lr ij bi translated">b)对于分类变量:</p><p id="50d2" class="kw kx mw ky b kz la jr lb lc ld ju le om lg lh li on lk ll lm oo lo lp lq lr ij bi translated">-获取一次性编码列</p><p id="a643" class="kw kx mw ky b kz la jr lb lc ld ju le om lg lh li on lk ll lm oo lo lp lq lr ij bi translated">-将每列除以其概率的平方根<strong class="ky ir">【sqrt(μₘ<em class="iq">)</em></strong></p><p id="8e9d" class="kw kx mw ky b kz la jr lb lc ld ju le om lg lh li on lk ll lm oo lo lp lq lr ij bi translated">-将列居中</p><p id="7ece" class="kw kx mw ky b kz la jr lb lc ld ju le om lg lh li on lk ll lm oo lo lp lq lr ij bi translated">c)对获得的表格应用PCA算法！</p></blockquote><p id="c65a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">备注:</p><ol class=""><li id="3346" class="ns nt iq ky b kz la lc ld lf nu lj nv ln nw lr oh ny nz oa bi translated">PCA算法通常自动使列居中，因此这部分在步骤a)或b)中是可选的</li><li id="8bae" class="ns nt iq ky b kz ob lc oc lf od lj oe ln of lr oh ny nz oa bi translated">将模态除以其概率的平方根可以这样解释:<em class="mw">我们给稀有模态更多的惯性，因为当模态非常稀有时，“取此模态”的信息更丰富。</em></li><li id="91d4" class="ns nt iq ky b kz ob lc oc lf od lj oe ln of lr oh ny nz oa bi translated">通过构造，分类变量<em class="mw"> vⱼ </em>和<em class="mw"> Mⱼ </em>模态可以生成最大的<em class="mw"> Mⱼ -1 </em>维度(因为所有列的总和总是给出一个1的向量)。因此，根据前面的符号，主成分的最大数量<em class="mw"> K </em>是<em class="mw"> P+M-Q. </em></li></ol><h1 id="aa77" class="lz ma iq bd mb mc mz me mf mg na mi mj jw nb jx ml jz nc ka mn kc nd kd mp mq bi translated">四]结论</h1><p id="f65a" class="pw-post-body-paragraph kw kx iq ky b kz mr jr lb lc ms ju le lf mt lh li lj mu ll lm ln mv lp lq lr ij bi translated">你有它！一种实现FAMD的简单方法，它将PCA推广到任何类型的数据集，同时对其所有变量给予相同的重视。</p><p id="de5f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">而且，因为我们对预编码数据应用了PCA算法，所以您可以使用PCA的所有分析工具(组件的惯性比、相关圆等)。)就像使用PCA一样。</p><p id="3d4b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我希望这有所帮助！如果您有任何问题，请告诉我:)</p></div><div class="ab cl ls lt hu lu" role="separator"><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx"/></div><div class="ij ik il im in"><h1 id="4787" class="lz ma iq bd mb mc md me mf mg mh mi mj jw mk jx ml jz mm ka mn kc mo kd mp mq bi translated">V]示威</h1><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi op"><img src="../Images/1745e57d91be6c49c31b4adc540f67ea.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YwetAUZ6dk6LFYWOANs0rw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><em class="my">(图片作者)</em></p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oq"><img src="../Images/29035471a8f0e2365a56fb1f2a69fa01.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TOmqTUR_TawqnT-O5iH5OQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><em class="my">(图片作者)</em></p></figure><h1 id="9a1f" class="lz ma iq bd mb mc mz me mf mg na mi mj jw nb jx ml jz nc ka mn kc nd kd mp mq bi translated">VI]参考资料</h1><p id="ac2f" class="pw-post-body-paragraph kw kx iq ky b kz mr jr lb lc ms ju le lf mt lh li lj mu ll lm ln mv lp lq lr ij bi translated">[1] G .萨波塔，<a class="ae kv" href="http://cedric.cnam.fr/~saporta/SAQQD.pdf" rel="noopener ugc nofollow" target="_blank"/>(1990)，意大利统计学会。意大利帕多瓦科学联盟。第62-72页。Ffhal-02513970f。</p><p id="9237" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">[2] J. Pagès，<a class="ae kv" href="http://www.numdam.org/article/RSA_2004__52_4_93_0.pdf" rel="noopener ugc nofollow" target="_blank">分析混合配料的因素</a> (2004年)，《应用统计杂志》，第52卷，第4期(2004年)，第93-111页</p><p id="1080" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">[3] F .休松和j .帕格斯，<a class="ae kv" href="https://lms.fun-mooc.fr/c4x/agrocampusouest/40001S03/asset/AnaDo_ACM_cours_slides.pdf" rel="noopener ugc nofollow" target="_blank">分析对应的倍数</a>，应用数学实验室，西部农业大学。</p></div></div>    
</body>
</html>