<html>
<head>
<title>Neural Network: The Dead Neuron</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">神经网络:死亡的神经元</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/neural-network-the-dead-neuron-eaa92e575748?source=collection_archive---------9-----------------------#2021-11-16">https://towardsdatascience.com/neural-network-the-dead-neuron-eaa92e575748?source=collection_archive---------9-----------------------#2021-11-16</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="34b1" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph">深度学习</h2><div class=""/><div class=""><h2 id="fb70" class="pw-subtitle-paragraph jw iz iq bd b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn dk translated">ReLU激活功能的最大缺点</h2></div><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ko"><img src="../Images/75d1422c06c1e078b9c12d3ea76ca113.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*JjMJaDM612TqlFg8"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">乔希·里默尔在<a class="ae le" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="9709" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi mb translated">为隐藏层选择激活功能并不是一件容易的事情。隐藏层的配置是一个非常活跃的研究主题，它只是没有任何关于给定数据集有多少神经元、多少层以及使用什么激活函数的理论。当时，由于其非线性，sigmoid是最受欢迎的激活函数。随着时间的推移，神经网络向更深层次的网络架构发展，这提出了消失梯度问题。纠正线性单位(ReLU)原来是隐藏层的激活功能的默认选项，因为它通过比sigmoid更大的梯度来解决消失梯度问题。</p></div><div class="ab cl mk ml hu mm" role="separator"><span class="mn bw bk mo mp mq"/><span class="mn bw bk mo mp mq"/><span class="mn bw bk mo mp"/></div><div class="ij ik il im in"><h2 id="9224" class="mr ms iq bd mt mu mv dn mw mx my dp mz lo na nb nc ls nd ne nf lw ng nh ni iw bi translated">消失梯度</h2><p id="6c58" class="pw-post-body-paragraph lf lg iq lh b li nj ka lk ll nk kd ln lo nl lq lr ls nm lu lv lw nn ly lz ma ij bi translated">消失梯度是训练深度神经网络时最大的挑战之一。这是一种深度神经网络无法将梯度从输出层反向传播回第一个隐藏层的情况。当您试图在其隐藏层上建立一个具有sigmoid激活函数的深度神经网络时，经常会发生这种情况。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi no"><img src="../Images/a36bcc1abb01e2f4060110f667c692ba.png" data-original-src="https://miro.medium.com/v2/resize:fit:1032/format:webp/1*ejJwG17GATqzsdbfjecC8Q.png"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">作者图片</p></figure><p id="4634" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">乙状结肠公式如下:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi np"><img src="../Images/1604c7c5a507943d867028c42e4f1019.png" data-original-src="https://miro.medium.com/v2/resize:fit:462/format:webp/0*066ete4g82n4RPyS.png"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">作者图片</p></figure><p id="90cf" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">它的导数是:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/2317ed8347f3f7c6b09d4d8e5bf74c47.png" data-original-src="https://miro.medium.com/v2/resize:fit:658/format:webp/0*RMmHZUThvkljgCxC.png"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">作者图片</p></figure><p id="a8e7" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">问题是sigmoid导数总是小于1。根据上面的公式，我们可以说当f(x) = 0.5时获得最大导数，因此f '(x)= 0.5 *(1–0.5)，也就是0.25。</p><p id="a9dc" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">现在想象一下，当你试图建立7层神经网络，每层都有一个sigmoid激活函数。无论损失函数提供的梯度是什么，如果我们将梯度乘以0.25，梯度将随着我们通过网络反向传播而变得越来越小，层数是7，这意味着我们用小于0的值将梯度乘以7次。</p><p id="60ce" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">这是最好的情况。在实际例子中，sigmoid的导数不会总是0.25。可能是0.1，0.06，0.04，也可能是0.001。那么第一个隐藏层的渐变如何呢？非常小，不是吗？</p><p id="887d" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">最简单的解决方案是用校正线性单元(ReLU)替换隐藏层上的激活函数。</p></div><div class="ab cl mk ml hu mm" role="separator"><span class="mn bw bk mo mp mq"/><span class="mn bw bk mo mp mq"/><span class="mn bw bk mo mp"/></div><div class="ij ik il im in"><h2 id="6df3" class="mr ms iq bd mt mu mv dn mw mx my dp mz lo na nb nc ls nd ne nf lw ng nh ni iw bi translated">整流线性单元</h2><p id="1620" class="pw-post-body-paragraph lf lg iq lh b li nj ka lk ll nk kd ln lo nl lq lr ls nm lu lv lw nn ly lz ma ij bi translated">ReLU被认为是深度学习的最大突破之一，因为ReLU使训练非常深度的神经网络成为可能。ReLU易于优化，因为它非常简单，计算成本低，并且类似于线性激活函数，但事实上，ReLU是一种非线性激活函数，允许学习数据中的复杂模式。线性激活函数和ReLU之间的唯一diﬀerence是ReLU将负值推至0。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi nr"><img src="../Images/4d4a4eb7b72aba07204f1154e364e13f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*7jcb7iDXTWfeVdtg.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">图片由M. Farid Landriandani拍摄</p></figure><p id="1df5" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">ReLU的公式如下:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/0838d1ed221b8e13f166ae0c487800de.png" data-original-src="https://miro.medium.com/v2/resize:fit:678/format:webp/0*J3n11peLbQujbHAr.png"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">作者图片</p></figure><p id="86e7" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">它的导数是:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/d5d088d94255410f20ef66261bc13342.png" data-original-src="https://miro.medium.com/v2/resize:fit:652/format:webp/0*YLmEpxTmv2_XjRU6.png"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">作者图片</p></figure><p id="00c0" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">ReLU在x &gt; 0时产生一个导数为1的线性，然后在x ≤ 0时产生一个导数为0的0。这使得通过ReLU神经元的导数在向前传播期间只要该神经元是活动的就保持很大。当训练更深的神经网络时，在隐藏层上使用ReLU被认为是解决消失梯度问题的一种可能的方法，因为导数是1或0。</p></div><div class="ab cl mk ml hu mm" role="separator"><span class="mn bw bk mo mp mq"/><span class="mn bw bk mo mp mq"/><span class="mn bw bk mo mp"/></div><div class="ij ik il im in"><h2 id="c388" class="mr ms iq bd mt mu mv dn mw mx my dp mz lo na nb nc ls nd ne nf lw ng nh ni iw bi translated">死亡神经元</h2><p id="4e7e" class="pw-post-body-paragraph lf lg iq lh b li nj ka lk ll nk kd ln lo nl lq lr ls nm lu lv lw nn ly lz ma ij bi translated">ReLU的缺点是他们不能从激活度为零的例子中学习。如果用0初始化整个神经网络并将ReLU放在隐藏层上，通常会发生这种情况。另一个原因是当大梯度流过时，ReLU神经元将更新其权重，并可能以大的负权重和偏差结束。如果发生这种情况，该神经元在正向传播期间将总是产生0，然后不管输入如何，流经该神经元的梯度将永远为零。</p><p id="b325" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">换句话说，这个神经元的权重永远不会再更新。这样的神经元可以被认为是死亡神经元，用生物学术语来说，这被认为是一种永久性的<strong class="lh ja">脑损伤</strong>。一个死亡的神经元可以被认为是自然的<strong class="lh ja">退出</strong>。但问题是，如果特定隐藏层中的每个神经元都死了，它会切断前一层的梯度，导致后一层的梯度为零。它可以通过使用较小的学习率来解决，这样大的梯度不会在ReLU神经元中设置大的负权重和偏差。另一个方法是使用泄漏ReLU，它允许活跃间期之外的神经元向后泄漏一些梯度。</p></div><div class="ab cl mk ml hu mm" role="separator"><span class="mn bw bk mo mp mq"/><span class="mn bw bk mo mp mq"/><span class="mn bw bk mo mp"/></div><div class="ij ik il im in"><h2 id="6962" class="mr ms iq bd mt mu mv dn mw mx my dp mz lo na nb nc ls nd ne nf lw ng nh ni iw bi translated">泄漏ReLU</h2><p id="2198" class="pw-post-body-paragraph lf lg iq lh b li nj ka lk ll nk kd ln lo nl lq lr ls nm lu lv lw nn ly lz ma ij bi translated">Leaky ReLU不是将负值推至0，而是通过将x乘以常数0.01，在负区域允许一些湖</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi nr"><img src="../Images/a5c3ceac0ff5b435dec7c1ebe73357c9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*DnnSlGbZpDng6Uz8.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">图片来自<a class="ae le" href="https://www.linkedin.com/in/landriandani/" rel="noopener ugc nofollow" target="_blank"> M. Farid Landriandani </a></p></figure><p id="5251" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">通过这样做，即使神经元有很大的负权重和偏差，仍然有可能通过层反向传播梯度。泄漏ReLU的公式如下:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/332f57e0f158f733208f0bea5808fc2b.png" data-original-src="https://miro.medium.com/v2/resize:fit:702/format:webp/0*yslqW5DAxNbT9B7b.png"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">作者图片</p></figure><p id="bd36" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">它的导数是:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi nv"><img src="../Images/4efb84024f04e03df4ef96aeb17ee900.png" data-original-src="https://miro.medium.com/v2/resize:fit:688/format:webp/0*9ZdCaVOPFq_dt4yv.png"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">作者图片</p></figure><p id="5eec" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">Leaky ReLU在x &gt; 0时产生导数为1的线性，然后在x ≤ 0时产生导数为0.01的0.01 * x。负区域中的常数也可以制成超参数，如<a class="ae le" href="http://arxiv.org/abs/1502.01852" rel="noopener ugc nofollow" target="_blank">深入研究整流器</a>中介绍的PReLU或参数ReLU激活功能所示。</p></div><div class="ab cl mk ml hu mm" role="separator"><span class="mn bw bk mo mp mq"/><span class="mn bw bk mo mp mq"/><span class="mn bw bk mo mp"/></div><div class="ij ik il im in"><h2 id="e289" class="mr ms iq bd mt mu mv dn mw mx my dp mz lo na nb nc ls nd ne nf lw ng nh ni iw bi translated">结论</h2><p id="fa6a" class="pw-post-body-paragraph lf lg iq lh b li nj ka lk ll nk kd ln lo nl lq lr ls nm lu lv lw nn ly lz ma ij bi translated">无论流行与否，ReLU都有一个缺点，叫做“T2”死神经元“T3”。这主要是由于流经网络的大负梯度导致ReLU神经元的大负权重。在前向和反向传播期间，该神经元将总是产生零，因此该神经元的权重将不再被更新，并且被认为是永远死亡的。作为一个自然的<strong class="lh ja">退出者</strong>，这可能很难，但是如果这发生在特定层的每个神经元上，它会切断前一层的梯度，导致后一层的梯度为零。作为替代，我们可以使用<strong class="lh ja">泄漏ReLU </strong>，当前馈期间的输出为零时，它至少具有0.01导数。</p></div><div class="ab cl mk ml hu mm" role="separator"><span class="mn bw bk mo mp mq"/><span class="mn bw bk mo mp mq"/><span class="mn bw bk mo mp"/></div><div class="ij ik il im in"><h2 id="88b4" class="mr ms iq bd mt mu mv dn mw mx my dp mz lo na nb nc ls nd ne nf lw ng nh ni iw bi translated">参考</h2><div class="nw nx gp gr ny nz"><a href="https://arxiv.org/abs/1803.08375#:~:text=We%20introduce%20the%20use%20of,function%20as%20their%20classification%20function" rel="noopener  ugc nofollow" target="_blank"><div class="oa ab fo"><div class="ob ab oc cl cj od"><h2 class="bd ja gy z fp oe fr fs of fu fw iz bi translated">使用校正线性单元(ReLU)的深度学习</h2><div class="og l"><h3 class="bd b gy z fp oe fr fs of fu fw dk translated">我们介绍了在深度神经网络(DNN)中使用校正线性单元(ReLU)作为分类函数</h3></div><div class="oh l"><p class="bd b dl z fp oe fr fs of fu fw dk translated">arxiv.org</p></div></div><div class="oi l"><div class="oj l ok ol om oi on ky nz"/></div></div></a></div><div class="nw nx gp gr ny nz"><a href="https://www.amazon.com/Deep-Learning-Adaptive-Computation-Machine-ebook/dp/B08FH8Y533/ref=sr_1_2?dchild=1&amp;keywords=deep+learning&amp;qid=1605266647&amp;s=digital-text&amp;sr=1-2" rel="noopener  ugc nofollow" target="_blank"><div class="oa ab fo"><div class="ob ab oc cl cj od"><h2 class="bd ja gy z fp oe fr fs of fu fw iz bi translated">深度学习(自适应计算和机器学习系列)</h2><div class="og l"><h3 class="bd b gy z fp oe fr fs of fu fw dk translated">Amazon.com:深度学习(自适应计算和机器学习系列)电子书:Goodfellow、Ian、Bengio、yo shua……</h3></div><div class="oh l"><p class="bd b dl z fp oe fr fs of fu fw dk translated">www.amazon.com</p></div></div><div class="oi l"><div class="oo l ok ol om oi on ky nz"/></div></div></a></div><div class="nw nx gp gr ny nz"><a href="https://www.amazon.com/Neural-Networks-Deep-Learning-Textbook/dp/3319944622/ref=sr_1_1?dchild=1&amp;keywords=Neural+Networks+and+Deep+Learning&amp;qid=1605266979&amp;sr=8-1" rel="noopener  ugc nofollow" target="_blank"><div class="oa ab fo"><div class="ob ab oc cl cj od"><h2 class="bd ja gy z fp oe fr fs of fu fw iz bi translated">神经网络和深度学习:教科书</h2><div class="og l"><h3 class="bd b gy z fp oe fr fs of fu fw dk translated">神经网络和深度学习:一本关于Amazon.com的教科书。*符合条件的优惠可享受免费*运输…</h3></div><div class="oh l"><p class="bd b dl z fp oe fr fs of fu fw dk translated">www.amazon.com</p></div></div><div class="oi l"><div class="op l ok ol om oi on ky nz"/></div></div></a></div><div class="nw nx gp gr ny nz"><a href="https://machinelearningmastery.com/how-to-fix-vanishing-gradients-using-the-rectified-linear-activation-function/" rel="noopener  ugc nofollow" target="_blank"><div class="oa ab fo"><div class="ob ab oc cl cj od"><h2 class="bd ja gy z fp oe fr fs of fu fw iz bi translated">如何使用ReLU - Machine Learning Mastery解决渐变消失问题</h2><div class="og l"><h3 class="bd b gy z fp oe fr fs of fu fw dk translated">消失梯度问题是一个不稳定行为的例子，当你训练一个深度神经…</h3></div><div class="oh l"><p class="bd b dl z fp oe fr fs of fu fw dk translated">machinelearningmastery.com</p></div></div><div class="oi l"><div class="oq l ok ol om oi on ky nz"/></div></div></a></div><div class="nw nx gp gr ny nz"><a href="http://arxiv.org/abs/1502.01852" rel="noopener  ugc nofollow" target="_blank"><div class="oa ab fo"><div class="ob ab oc cl cj od"><h2 class="bd ja gy z fp oe fr fs of fu fw iz bi translated">深入研究整流器:在ImageNet分类上超越人类水平的性能</h2><div class="og l"><h3 class="bd b gy z fp oe fr fs of fu fw dk translated">整流激活单元(整流器)对于最先进的神经网络是必不可少的。在这项工作中，我们研究…</h3></div><div class="oh l"><p class="bd b dl z fp oe fr fs of fu fw dk translated">arxiv.org</p></div></div><div class="oi l"><div class="or l ok ol om oi on ky nz"/></div></div></a></div></div></div>    
</body>
</html>