<html>
<head>
<title>Deep Emotion Recognition based on Facial Expressions</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">基于面部表情的深度情感识别</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/deep-emotion-recognition-based-on-facial-expressions-ed02fd7f8627?source=collection_archive---------20-----------------------#2021-06-03">https://towardsdatascience.com/deep-emotion-recognition-based-on-facial-expressions-ed02fd7f8627?source=collection_archive---------20-----------------------#2021-06-03</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="f1a6" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">第1部分:关于如何针对情感识别微调面部表情效率网络的教程🎯</h2></div><p id="94a2" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi lb translated">在机器人学硕士学位论文的启发下，我决定写一篇三部曲的文章。这一努力背后的愿望是分享我通过这次神奇的旅程获得的发现和知识。这三篇文章将主要采取循序渐进的编码教程的形式，同时在理论层面上解释已经做出的许多技术选择。</p><figure class="ll lm ln lo gt lp gh gi paragraph-image"><div role="button" tabindex="0" class="lq lr di ls bf lt"><div class="gh gi lk"><img src="../Images/6b3d5f548a480dc2a4f4fd0ef7573add.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*VlOjQ3TICAYY8-rr"/></div></div><p class="lw lx gj gh gi ly lz bd b be z dk translated"><a class="ae ma" href="https://unsplash.com/@tengyart?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">腾雅特</a>在<a class="ae ma" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="6bdc" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在我们开始实现之前，让我们稍微讨论一下<strong class="kh ir">面部表情识别</strong> (FER)领域:</p><blockquote class="mb mc md"><p id="044b" class="kf kg me kh b ki kj jr kk kl km ju kn mf kp kq kr mg kt ku kv mh kx ky kz la ij bi translated">FER是一个科学研究领域，研究试图从面部表情识别/推断情绪状态的技术和方法。在人类交流中，面部表情在推断情绪方面起着至关重要的作用，这可能有助于理解他人的意图。根据不同的调查[1，2]，<strong class="kh ir">言语成分只传达了人际交往的三分之一</strong>，<strong class="kh ir">非言语成分传达了三分之二</strong>。大多数与态度和感觉有关的信息存在于面部表情中。因此，面部表情被证明在整个信息交流过程中起着至关重要的作用。表情和情绪密不可分。埃克曼和弗里森在[3]引发了第一波<strong class="kh ir">基础情绪理论</strong>启发了对情绪表达的研究。他们使用了典型情绪面部表情的静态照片，并记录了有限的一组“基本”情绪(快乐)的识别和产生的某种程度的普遍性😀，惊喜<em class="iq">😮</em>，恐惧<em class="iq">😨</em>，厌恶<em class="iq">🤮</em>，悲伤<em class="iq">😭</em>和<em class="iq">愤怒😡</em>)。</p></blockquote><p id="6b6c" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这些<strong class="kh ir"> 6个类别</strong>也将用于我们的任务，并且<strong class="kh ir"> CK+48 </strong> [4]是选择的数据集，将帮助我们训练和评估我们的模型。整个实现发生在使用GPU加速的Google Colab上。所以，事不宜迟，让我们开始我们的任务吧…</p><h1 id="300b" class="mi mj iq bd mk ml mm mn mo mp mq mr ms jw mt jx mu jz mv ka mw kc mx kd my mz bi translated">数据预处理✂️</h1><p id="98f0" class="pw-post-body-paragraph kf kg iq kh b ki na jr kk kl nb ju kn ko nc kq kr ks nd ku kv kw ne ky kz la ij bi translated">a)首先，让我们导入必要的库，如下所示。</p><figure class="ll lm ln lo gt lp"><div class="bz fp l di"><div class="nf ng l"/></div></figure><p id="63d4" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">将数据集存储到Google Drive后，您必须挂载驱动器并转到包含数据集的目录。</p><figure class="ll lm ln lo gt lp"><div class="bz fp l di"><div class="nf ng l"/></div></figure><p id="0225" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">下一步是将我们的数据加载到内存中，这就是为什么我创建了两个辅助函数来简化事情。下面您可以看到描述其功能的文档字符串(为了避免文章中出现大量代码)。当然，你可以在我的<a class="ae ma" href="https://github.com/skanelo/Deep-Emotion-Recognition" rel="noopener ugc nofollow" target="_blank"> GitHub </a>账号上找到完整的代码。</p><figure class="ll lm ln lo gt lp"><div class="bz fp l di"><div class="nf ng l"/></div></figure><p id="b21a" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这是我们的数据分布图。</p><figure class="ll lm ln lo gt lp gh gi paragraph-image"><div class="gh gi nh"><img src="../Images/5f1747e484c590269e71afa8d45ccae8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1112/format:webp/1*Jq60t_vUsE2o92twa9WOOg.png"/></div><p class="lw lx gj gh gi ly lz bd b be z dk translated">作者图片</p></figure><p id="73f1" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir"> <em class="me">注:</em> </strong> <em class="me">理想情况下，我们应该有</em> <strong class="kh ir"> <em class="me">等分布的类</em> </strong> <em class="me">供我们训练，因为与“恐惧”的例子相比，模型更频繁地遇到“惊讶”的例子。最后，这将损害模型的性能，因为我们将在后面看到，大多数错误分类将与“悲伤”和“恐惧”这两个类别有关。</em></p><p id="16f1" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">b)我使用的数据集包括927幅形状为48×48×3的图像。之后，我们将数据集随机分为训练集(<em class="me"> 70% </em>)、验证集(<em class="me"> 15% </em>)和测试集(<em class="me"> 15% </em>)(一般来说，这被认为是拆分数据的一个很好的经验法则)。</p><figure class="ll lm ln lo gt lp"><div class="bz fp l di"><div class="nf ng l"/></div></figure><p id="4016" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">分割的指示打印如下所示。</p><pre class="ll lm ln lo gt ni nj nk nl aw nm bi"><span id="d091" class="nn mj iq nj b gy no np l nq nr">X_train has shape: (<strong class="nj ir">648</strong>, 48, 48, 3) y_train has shape: (<strong class="nj ir">648</strong>, 6)  X_valid has shape: (<strong class="nj ir">139</strong>, 48, 48, 3) y_valid has shape: (<strong class="nj ir">139</strong>, 6)  X_test has shape: (<strong class="nj ir">140</strong>, 48, 48, 3) y_test has shape: (<strong class="nj ir">140</strong>, 6)</span><span id="4183" class="nn mj iq nj b gy ns np l nq nr">X_train + X_valid + X_test = <strong class="nj ir">927</strong> samples in total</span></pre><p id="7ca2" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir"> <em class="me">注:</em> </strong> <em class="me">我们的验证和测试集从同一个分布中抽取是非常重要的。在这段</em> <a class="ae ma" href="https://www.youtube.com/watch?v=M3qpIzy4MQk&amp;ab_channel=DeepLearningAI" rel="noopener ugc nofollow" target="_blank"> <em class="me">视频</em> </a> <em class="me">中，吴恩达博士以一种引人注目(且易于消化)的方式解释了原因。问题是，我们的数据来自相同的分布(因为它是在实验室约束下创建的)，但一些类别之间的样本数量很大程度上不相等，再加上数据量相对较小，可能会损害模型的效率。理想情况下，我们应该以某种方式“控制”X_valid和X_test的</em> <strong class="kh ir"> <em class="me"> ~140个样本</em> </strong> <em class="me">，方法是从每个类别</em>  <em class="me">中抽取</em> <strong class="kh ir"> <em class="me"> ~23个不同的示例。</em></strong></p><p id="0770" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">c)此外，数据扩充将只用于训练样本，以便让模型遇到各种不同的例子，因为我们没有太多的例子可供我们使用，因此这将有助于它增强其预测性能。</p><figure class="ll lm ln lo gt lp"><div class="bz fp l di"><div class="nf ng l"/></div></figure><p id="3d58" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir"> <em class="me">注意:</em> </strong> <em class="me">并不是ImageDataGenerator类的所有参数都相关。对此我们必须小心。例如，垂直翻转对我们的任务没有意义，可能会损害模型的精度。</em></p><p id="c7e0" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">应用于训练样本的一些增强示例如下所示。</p><figure class="ll lm ln lo gt lp gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/5862429f21e04fd40e9088027355cd63.png" data-original-src="https://miro.medium.com/v2/resize:fit:1162/format:webp/1*kAU2Oeu2-d_pHRIHVYUh1g.png"/></div><p class="lw lx gj gh gi ly lz bd b be z dk translated">作者图片</p></figure><h1 id="38bb" class="mi mj iq bd mk ml mm mn mo mp mq mr ms jw mt jx mu jz mv ka mw kc mx kd my mz bi translated">模型创建和微调🚀</h1><p id="c4f8" class="pw-post-body-paragraph kf kg iq kh b ki na jr kk kl nb ju kn ko nc kq kr ks nd ku kv kw ne ky kz la ij bi translated">我创建了函数build_model，它加载了在ImageNet上预训练的EfficientNetB0模型(没有它的原始分类器),并在它的顶部添加了3层，如下图所示。只选择3层(而不是更复杂的东西)的想法来自于官方文档<a class="ae ma" href="http://Image classification via fine-tuning with EfficientNet" rel="noopener ugc nofollow" target="_blank"> Keras </a>展示了如何在图像分类上微调EfficientNet。此外，该函数使用Adam优化器、分类交叉熵损失函数和作为度量的准确性来编译模型。</p><figure class="ll lm ln lo gt lp"><div class="bz fp l di"><div class="nf ng l"/></div></figure><p id="79af" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在我的GitHub repo中，我上传了模型的概要以及模型的图形，这样<a class="ae ma" href="https://github.com/skanelo/Deep-Emotion-Recognition/tree/main/model%20summary" rel="noopener ugc nofollow" target="_blank">就可以看一下</a>。</p><p id="12b1" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">至于培训阶段，我使用了3个非常有用的回访:</p><ul class=""><li id="6543" class="nu nv iq kh b ki kj kl km ko nw ks nx kw ny la nz oa ob oc bi translated"><strong class="kh ir"> ModelCheckPoint: </strong>保存达到最佳验证精度的模型的权重。</li><li id="85e7" class="nu nv iq kh b ki od kl oe ko of ks og kw oh la nz oa ob oc bi translated"><strong class="kh ir">提前停止:</strong>如果连续15个时期验证精度没有提高，训练将被中断(该回调防止过度拟合)</li><li id="1973" class="nu nv iq kh b ki od kl oe ko of ks og kw oh la nz oa ob oc bi translated"><strong class="kh ir">reducelronplateaueu:</strong>这是一个调度程序，每当验证精度处于稳定状态时，它会将学习率降低一半。</li></ul><figure class="ll lm ln lo gt lp"><div class="bz fp l di"><div class="nf ng l"/></div></figure><p id="b51d" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">上述回调的组合确保了我们的模型不会过度拟合。当然，我们也可以通过我们的学习曲线观察到这一点，我们稍后会进行检查。</p><p id="d894" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir"> <em class="me">注:</em> </strong> <em class="me">至于批量，我试验了一个取值范围。对于批量大小为16和32的样本，训练时间增加了(这确实很重要，尤其是当数据集相当大时)，而验证准确性甚至没有比64个样本的批量更高。另一方面，一批128个样本当然加速了训练过程，然而，不仅验证不是更好，而且还观察到学习曲线的显著振荡(这可能妨碍模型的收敛)。</em></p><p id="71fa" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir"> <em class="me">注意:</em> </strong> <em class="me">两个</em> <strong class="kh ir"> <em class="me">我在建立这个模型的过程中意识到的非常重要的事情是:</em><strong class="kh ir"><em class="me">(1)D</em></strong><em class="me">尽管在大多数情况下像素归一化被应用于输入图像，EfficientNet仍然通过</em> <a class="ae ma" href="https://keras.io/api/applications/efficientnet/" rel="noopener ugc nofollow" target="_blank"> <em class="me">它的重新缩放层</em> </a> <em class="me">来实现这一点，因此，使用会损害模型的性能。</em><strong class="kh ir">【2】<em class="me">D</em></strong><em class="me">尽管模型将形状(224，224，3)的张量作为输入，但我意识到，当我修改输入层以接收形状(48，48，3)的张量而不是将数据集重新缩放到224x224时，可以获得更好的性能。我设法找到的唯一相关资料来源是阿德里安·罗斯布鲁克关于pyimagesearch.com的一篇文章。</em></strong></p><h1 id="46f1" class="mi mj iq bd mk ml mm mn mo mp mq mr ms jw mt jx mu jz mv ka mw kc mx kd my mz bi translated">学习曲线📈 📉</h1><p id="82f8" class="pw-post-body-paragraph kf kg iq kh b ki na jr kk kl nb ju kn ko nc kq kr ks nd ku kv kw ne ky kz la ij bi translated">最后，模型只需要46个时期就能收敛，训练过程从我们的回调中中断。相应的学习曲线如下所示。</p><figure class="ll lm ln lo gt lp gh gi paragraph-image"><div role="button" tabindex="0" class="lq lr di ls bf lt"><div class="gh gi oi"><img src="../Images/8dab417a24c70af14a565e0f6cc931e4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2lZLulTRnADFL89GVAW2Ng.png"/></div></div></figure><p id="8e0d" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">该模型似乎很好地符合新数据，具有相当小的泛化差距。训练和验证准确度都接近98-99%的准确度。但是，将自动加载模型最佳版本的权重。</p><h1 id="43be" class="mi mj iq bd mk ml mm mn mo mp mq mr ms jw mt jx mu jz mv ka mw kc mx kd my mz bi translated">最终评估🔮</h1><p id="08da" class="pw-post-body-paragraph kf kg iq kh b ki na jr kk kl nb ju kn ko nc kq kr ks nd ku kv kw ne ky kz la ij bi translated">最后，我们将在没有见过的样本上测试我们的模型。这个集合是从我们的初始数据集中抽取的<strong class="kh ir">测试集合</strong>。模型达到了<strong class="kh ir"><em class="me"/></strong>96.43%的准确率<em class="me">，</em>并且通过混淆矩阵来描述预测，以便我们可以彻底检查模型的错误分类。</p><figure class="ll lm ln lo gt lp gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/05c1e8741aff13592a0e640197678419.png" data-original-src="https://miro.medium.com/v2/resize:fit:1122/format:webp/1*fFvgevrG26Fti9Oq5LZd0Q.png"/></div><p class="lw lx gj gh gi ly lz bd b be z dk translated">作者图片</p></figure><p id="c692" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">从这个矩阵中你可以观察到，类“悲伤”本质上导致了这个问题。这是绝对合理的，虽然，由于数量很少的例子。此外，不要忘记，即使对一个人来说，表达悲伤、恐惧、厌恶或愤怒的面部表情也可能令人困惑，如果你检查以上面部表情的增强样本，并尝试自己预测每种表情的类别，这一点就会变得明显。你会意识到，即使对人类来说，这项任务也存在固有的困难。</p><p id="d349" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">您还可以看到模型的一些随机预测，右侧的条形图显示了每个预测的置信度。</p><figure class="ll lm ln lo gt lp gh gi paragraph-image"><div role="button" tabindex="0" class="lq lr di ls bf lt"><div class="gh gi ok"><img src="../Images/c156e49cbc1741c81cdf184c42b5df14.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0riIjatjJM37FNZfkmabwA.png"/></div></div><p class="lw lx gj gh gi ly lz bd b be z dk translated">作者图片</p></figure><h1 id="55e1" class="mi mj iq bd mk ml mm mn mo mp mq mr ms jw mt jx mu jz mv ka mw kc mx kd my mz bi translated">结论🏆</h1><p id="b8dc" class="pw-post-body-paragraph kf kg iq kh b ki na jr kk kl nb ju kn ko nc kq kr ks nd ku kv kw ne ky kz la ij bi translated">在本文中，我们看到了如何构建面部表情分类器来预测情绪。对于这个数据集(<strong class="kh ir"> CK+48 </strong>)，达到<strong class="kh ir"> <em class="me"> 96.43% </em> </strong>的准确性接近于最先进的性能，在这个三部曲的下一篇文章中，我们将应用可视化技术(GradCam)，通过将热图应用到原始图像上，揭示图像中对模型起关键作用的区域，以便进行推断。这些技术对于深度学习方法非常重要，因为它们有助于模型的可解释性和可解释性。因此，在我们的下一次会议上，我们将看到我们如何解释我们的模型的不准确性，以及我们如何可能改善它。</p><p id="f709" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">非常感谢您的宝贵时间！(完整的项目可以在<strong class="kh ir"/>这里找到<strong class="kh ir"> </strong> <a class="ae ma" href="https://github.com/skanelo/Deep-Emotion-Recognition" rel="noopener ugc nofollow" target="_blank"> <strong class="kh ir">)。</strong></a></p></div><div class="ab cl ol om hu on" role="separator"><span class="oo bw bk op oq or"/><span class="oo bw bk op oq or"/><span class="oo bw bk op oq"/></div><div class="ij ik il im in"><h1 id="fef9" class="mi mj iq bd mk ml os mn mo mp ot mr ms jw ou jx mu jz ov ka mw kc ow kd my mz bi translated">参考</h1><p id="0354" class="pw-post-body-paragraph kf kg iq kh b ki na jr kk kl nb ju kn ko nc kq kr ks nd ku kv kw ne ky kz la ij bi translated"><strong class="kh ir">【1】</strong>阿尔伯特·梅拉比安。没有文字的交流。沟通理论，第193-200页，2008年。</p><p id="a82f" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">凯瑟琳·考拉德，道格拉斯·坎宁安，海因里希·H·布尔霍夫和克里斯蒂安·沃尔雷文。mpi面部表情数据库——一个经验证的情感和会话面部表情数据库。PloS one，7(3)，2012。</p><p id="22c2" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">保罗·艾克曼，理查德·索伦森和华莱士·弗里森。面部表情中的泛文化因素。科学，164(3875):86–88，1969。</p><p id="154c" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">Patrick Lucey，Jeffrey F Cohn，Takeo Kanade，Jason Saragih，Zara Ambadar和Iain Matthews。扩展的cohn-kanade数据集(ck+):一个完整的动作单元和情感表达数据集。2010年ieee计算机学会计算机视觉和模式识别会议-研讨会，第94-101页。IEEE，2010年。</p></div></div>    
</body>
</html>