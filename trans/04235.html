<html>
<head>
<title>Understanding Dimension Reduction and Principal Component Analysis in R for Data Science</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">理解数据科学R中的降维和主成分分析</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/understanding-dimension-reduction-and-principal-component-analysis-in-r-e3fbd02b29ae?source=collection_archive---------10-----------------------#2021-04-10">https://towardsdatascience.com/understanding-dimension-reduction-and-principal-component-analysis-in-r-e3fbd02b29ae?source=collection_archive---------10-----------------------#2021-04-10</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="237f" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">什么是降维，我们如何使用R中的主成分分析来确定重要的特征</h2></div><p id="220f" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">当你在数据科学和分析领域工作时，处理高维数据是其中的一部分。您可能有一个包含600个甚至6000个变量的数据集，其中一些列在建模中被证明是重要的，而另一些是无关紧要的，一些是相互关联的(如体重和身高)，还有一些是完全相互独立的。我们非常清楚使用数以千计的特征对于我们的模型来说既乏味又不切实际，因此我们的目标是创建一个维数减少的数据集(全部不相关),以尽可能多地解释原始数据集中的变化。从高维数据到低维数据的转换需要我们提出a)统计解决方案和b)数据压缩活动，一种称为PCA(主成分分析)的技术。</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi lb"><img src="../Images/9a535c127865f5a7fcbffb5b88bd33c8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*pVACRr-AQSp1MLjj"/></div></div><p class="ln lo gj gh gi lp lq bd b be z dk translated">图片由<a class="ae lr" href="https://unsplash.com/@marcojodoin" rel="noopener ugc nofollow" target="_blank"> @marcojodoin </a>在【unsplash.com】T2上</p></figure><p id="e006" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在本文中，我们将讨论三个目标:</p><ol class=""><li id="b291" class="ls lt iq kh b ki kj kl km ko lu ks lv kw lw la lx ly lz ma bi translated">理解降维的必要性</li></ol><p id="5472" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">2.执行PCA并分析数据变化和数据组成</p><p id="eb03" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">3.确定在数据识别中最有用的特征</p><p id="a187" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir"><em class="mb">“PCA是机器学习中的一种无监督学习技术，它通过显示数据集中的最大变化(通过关注实际区分数据的变量)，将高排名矩阵的维度降低为低排名矩阵，从而捕捉原始数据的本质。”</em> </strong></p><p id="5639" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">PCA产生主成分(等于特征的数量)，这些主成分按照方差的顺序排列(PC1显示最大方差，PC2显示第二大方差，依此类推)。为了更详细地理解这一点，让我们使用r中的prcomp()函数来处理一个样本数据集。我使用了一个在<a class="ae lr" href="https://data.world/sdhilip/pizza-datasets" rel="noopener ugc nofollow" target="_blank"><strong class="kh ir">data . world</strong></a><strong class="kh ir">(一个开源平台)</strong>上可用的数据集，它为您提供了几种不同品牌的披萨的营养成分。我只保留了A、B &amp; C品牌的披萨。csv使我们的分析简单易懂。</p><p id="9d13" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">步骤1:导入库并将数据集读入表格</strong></p><p id="aeb6" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">确保您在当前目录中。csv被放置(使用getwd())或者能够使用setdw()将其更改为当前文件。</p><pre class="lc ld le lf gt mc md me mf aw mg bi"><span id="bbd5" class="mh mi iq md b gy mj mk l ml mm">library(dplyr)<br/>library(data.table)<br/>library(datasets)<br/>library(ggplot2)</span><span id="7c34" class="mh mi iq md b gy mn mk l ml mm">data &lt;- data.table(read.csv("Pizza.csv"))</span></pre><p id="301a" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">第二步:理解数据</strong></p><p id="7f66" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这些函数告诉你a)维数，即矩阵的行x列b)数据的前几行和c)变量的数据类型。</p><pre class="lc ld le lf gt mc md me mf aw mg bi"><span id="99a1" class="mh mi iq md b gy mj mk l ml mm">dim(data)<br/>head(data)<br/>str(data)</span></pre><figure class="lc ld le lf gt lg gh gi paragraph-image"><div class="gh gi mo"><img src="../Images/970501d6d810f399fa0f7a04acbf79b9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1248/format:webp/1*yaHNxbqeZS_8OESbmyfVng.png"/></div><p class="ln lo gj gh gi lp lq bd b be z dk translated">基本初始功能(图片由作者提供)</p></figure><p id="c7e1" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">第三步:获取主成分</strong></p><p id="1b70" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们希望让我们的目标变量为空，因为我们希望我们的分析告诉我们有不同类型的比萨饼品牌。一旦我们完成了这些并复制了数据集以保持原始值，我们将运行prcomp()。scale = TRUE，以便在分析前将变量缩放到单位方差。</p><pre class="lc ld le lf gt mc md me mf aw mg bi"><span id="fe8a" class="mh mi iq md b gy mj mk l ml mm">pizzas &lt;- copy(data)<br/>pizzas &lt;- pizza[, brand := NULL]</span><span id="c3d2" class="mh mi iq md b gy mn mk l ml mm">pca &lt;- prcomp(pizzas, scale. = TRUE)</span></pre><h2 id="8bd8" class="mh mi iq bd mp mq mr dn ms mt mu dp mv ko mw mx my ks mz na nb kw nc nd ne nf bi translated">关于prcomp()需要知道的重要事情是，它返回3样东西:</h2><ol class=""><li id="4100" class="ls lt iq kh b ki ng kl nh ko ni ks nj kw nk la lx ly lz ma bi translated"><strong class="kh ir"> x : </strong>存储数据的所有主要成分，我们可以使用这些成分来绘制图表，并了解PC之间的相关性。</li><li id="fbdd" class="ls lt iq kh b ki nl kl nm ko nn ks no kw np la lx ly lz ma bi translated"><strong class="kh ir"> sdev </strong>:计算标准差，了解每台电脑在数据上有多少变化</li><li id="00cb" class="ls lt iq kh b ki nl kl nm ko nn ks no kw np la lx ly lz ma bi translated"><strong class="kh ir">旋转:</strong>确定哪个加载分数对PCA图有最大影响，即最大坐标(绝对值)</li></ol><p id="6378" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">第四步:使用x </strong></p><p id="cb06" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">尽管目前我们的数据具有两个以上的维度，但我们可以使用x绘制我们的图表。通常，前几个PCs捕获最大方差，因此PC1和PC2绘制在下面以理解数据。</p><pre class="lc ld le lf gt mc md me mf aw mg bi"><span id="0cdb" class="mh mi iq md b gy mj mk l ml mm">pca_1_2 &lt;- data.frame(pca$x[, 1:2])<br/><br/>plot(pca$x[,1], pca$x[,2])</span></pre><figure class="lc ld le lf gt lg gh gi paragraph-image"><div class="gh gi mo"><img src="../Images/6a51c21ad1940dedcaae2a490e222a28.png" data-original-src="https://miro.medium.com/v2/resize:fit:1248/format:webp/1*8qoi5o7hpkE0mxz0_qnHtg.png"/></div><p class="ln lo gj gh gi lp lq bd b be z dk translated">PC1对比PC2图(图片由作者提供)</p></figure><p id="f5f2" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">该图清楚地向我们展示了前两个PCs如何根据定义它们的特征将数据分成三个聚类(或B &amp; C比萨饼品牌)。</p><p id="75d0" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">步骤5:使用sdev </strong></p><p id="25ef" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这里我们使用sdev的平方，并计算每台电脑的变化百分比。</p><pre class="lc ld le lf gt mc md me mf aw mg bi"><span id="f939" class="mh mi iq md b gy mj mk l ml mm">pca_var &lt;- pca$sdev^2</span><span id="4176" class="mh mi iq md b gy mn mk l ml mm">pca_var_perc &lt;- round(pca_var/sum(pca_var) * 100, 1)</span><span id="c121" class="mh mi iq md b gy mn mk l ml mm">barplot(pca_var_perc, main = "Variation Plot", xlab = "PCs", ylab = "Percentage Variance", ylim = c(0, 100))</span></pre><figure class="lc ld le lf gt lg gh gi paragraph-image"><div class="gh gi mo"><img src="../Images/e4a8d620ba9a8170d98c8cc22e27527a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1248/format:webp/1*F-coKHXFLIwUYy5wCimjJQ.png"/></div><p class="ln lo gj gh gi lp lq bd b be z dk translated">百分比变化图(图片由作者提供)</p></figure><p id="dda9" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这个柱状图告诉我们，PC1显示了几乎60%的数据变化，PC2显示了20 %, PC3显示了12%,其余的PC捕捉到的数据变化很少。</p><p id="c012" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">步骤6:使用旋转</strong></p><p id="134f" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这一部分解释了在区分比萨饼品牌时，哪些特征最重要；旋转将权重分配给特征(技术上称为负载), PC的“负载”数组称为特征向量。</p><pre class="lc ld le lf gt mc md me mf aw mg bi"><span id="7d63" class="mh mi iq md b gy mj mk l ml mm">PC1 &lt;- pca$rotation[,1]</span><span id="1def" class="mh mi iq md b gy mn mk l ml mm">PC1_scores &lt;- abs(PC1)</span><span id="8c00" class="mh mi iq md b gy mn mk l ml mm">PC1_scores_ordered &lt;- sort(PC1_scores, decreasing = TRUE)</span><span id="8cbf" class="mh mi iq md b gy mn mk l ml mm">names(PC1_scores_ordered)</span></pre><figure class="lc ld le lf gt lg gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/3185b7f1611165ac98a465bc4d4f9e86.png" data-original-src="https://miro.medium.com/v2/resize:fit:1196/format:webp/1*ZsVG9D2PrUmDinjMqxB0_Q.png"/></div><p class="ln lo gj gh gi lp lq bd b be z dk translated">(图片由作者提供)</p></figure><p id="e7d1" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们看到变量<em class="mb">‘cal’</em>(样品中每100克的卡路里量)是区分品牌的最重要特征，其次是<em class="mb">‘mois’</em>(样品中每100克的水含量)，依此类推。</p><p id="f8f9" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">第七步:利用两个最重要的特征区分品牌</strong></p><pre class="lc ld le lf gt mc md me mf aw mg bi"><span id="a1fc" class="mh mi iq md b gy mj mk l ml mm">ggplot(data, aes(x=data$cal, y=data$mois, color = data$brand)) + geom_point() + labs(title = “Pizza brands by two variables”)</span></pre><figure class="lc ld le lf gt lg gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/36afd05ef959b944fae1de1cf51cea52.png" data-original-src="https://miro.medium.com/v2/resize:fit:1132/format:webp/1*4R2En97WcSqy2ZMeWdRq7w.png"/></div><p class="ln lo gj gh gi lp lq bd b be z dk translated">通过权重最大的柱形成聚类(图片由作者提供)</p></figure><p id="d18e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">该图清楚地显示了如何代替数据集中给我们的8列，仅两列就足以理解我们有三种不同类型的比萨饼，从而使PCA成为一种成功的分析工具，将高维数据减少到较低的一列，用于建模和分析目的。</p><p id="9c62" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这篇文章背后的灵感来自于约翰·霍普斯金在Coursera上开设的数据科学专业的第三门课程和Josh Starmer的StatQuest。</p></div></div>    
</body>
</html>