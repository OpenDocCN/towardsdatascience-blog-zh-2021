<html>
<head>
<title>Explainable Deep Neural Networks</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">可解释的深度神经网络</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/explainable-deep-neural-networks-2f40b89d4d6f?source=collection_archive---------2-----------------------#2021-05-30">https://towardsdatascience.com/explainable-deep-neural-networks-2f40b89d4d6f?source=collection_archive---------2-----------------------#2021-05-30</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="684c" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">从隐藏层获得定性见解</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/ad2eab9a8b67f7929335fefa68baa87d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OJTOmZFPIkkUOZypqfzRjQ.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图片由诺尔·奥托(Pexels)提供</p></figure><blockquote class="kv"><p id="6a9f" class="kw kx iq bd ky kz la lb lc ld le lf dk translated">自然是一个无限的球体，它的中心无处不在，它的圆周无处不在。</p><p id="907f" class="kw kx iq bd ky kz lg lh li lj lk lf dk translated">B.帕</p></blockquote><p id="c354" class="pw-post-body-paragraph ll lm iq ln b lo lp jr lq lr ls ju lt lu lv lw lx ly lz ma mb mc md me mf lf ij bi mg translated"><span class="l mh mi mj bm mk ml mm mn mo di"> F </span> <em class="mp">还是有些年头，</em> <strong class="ln ir"> <em class="mp">黑盒机器学习</em> </strong> <em class="mp">一直被批评在从数据中提取知识方面的局限性。深度神经网络(DNNs)是最著名的“黑盒”算法之一。深度神经网络(DNNs)是当今使用最广泛和最成功的图像分类和处理技术。但是当涉及到结构化数据(最常见的数据问题)时，关于它的优势仍然存在争议。近年来，人们一直在努力开发</em> <strong class="ln ir"> <em class="mp">可解释的机器学习</em> </strong> <em class="mp">技术，目标是为算法提供更强有力的描述方法以及向用户提供附加信息，从而增强数据洞察力。在这项工作中，我们提出了一种新的方法，使深度神经网络在将编码数据转换为实际知识时更容易理解。在每个隐藏层表示中，将检查数据结构，以清楚地了解深度神经网络如何为分类目的转换数据。</em></p></div><div class="ab cl mq mr hu ms" role="separator"><span class="mt bw bk mu mv mw"/><span class="mt bw bk mu mv mw"/><span class="mt bw bk mu mv"/></div><div class="ij ik il im in"><h1 id="8146" class="mx my iq bd mz na nb nc nd ne nf ng nh jw ni jx nj jz nk ka nl kc nm kd nn no bi translated">深度学习的困惑</h1><p id="125a" class="pw-post-body-paragraph ll lm iq ln b lo np jr lq lr nq ju lt lu nr lw lx ly ns ma mb mc nt me mf lf ij bi translated">深度学习数学分析领域(Berner，J. et al. 2021)正在尝试使用数学方法来理解神经网络的神秘内部工作方式。这项研究的关键目标之一是理解神经网络在处理数据时实际上做了什么。深度神经网络(DNNs)在每一层转换数据，创建新的表示作为输出。在分类问题中，DNNs寻求将数据分成不同的类别，逐层改进这一过程，直到产生最终输出。根据流形假设(manifold hypothesis)，该假设提出自然数据在其嵌入空间中创建更低维的流形，该任务可以理解为在数据空间中分离更低维的流形(Fefferman C .，2016；奥拉赫c .，2014)。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nu"><img src="../Images/f346895e8c1524d7cf822dc63f5651a0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*C0-gKBHkWNMNLu_X1sosDQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><strong class="bd nv">图一</strong>。从瑞士卷上随机生成的点，到使用k-最近邻和MDS算法的低维嵌入。虽然初始数据分布是三维的，但最终的表示非常接近于一条连续的线(一维流形)。数据来自<a class="ae nw" href="https://chart-studio.plotly.com/~empet#/" rel="noopener ugc nofollow" target="_blank">https://chart-studio.plotly.com/~empet#/</a>。图片作者。</p></figure><p id="5b23" class="pw-post-body-paragraph ll lm iq ln b lo nx jr lq lr ny ju lt lu nz lw lx ly oa ma mb mc ob me mf lf ij bi translated">DNN层由一个<strong class="ln ir">实现函数</strong>、<em class="mp">φ</em>(一个仿射变换)和一个组件式的<strong class="ln ir">激活函数</strong>、<em class="mp"> ρ </em>链接。考虑图2所示的全连接前馈神经网络。网络架构可以通过定义层数<em class="mp"> N，L </em>，神经元数量，激活函数来描述。网络参数是权重矩阵W和偏置向量b。每一层的输出都是描述输入的新方式。这就是为什么它们被称为r <strong class="ln ir">表示</strong>的原因，因为它们本质上是输入数据的抽象。对于每层，<em class="mp">φ(x，θ)= Wx+b </em>，参数<em class="mp"> θ = (W，b) </em>。权重矩阵是<em class="mp"> W </em>，而偏置向量是<em class="mp"> b </em>。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oc"><img src="../Images/c221f95ea30055e7f6f659f6f5d9613d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*r3ABm5qWDrBZQQI4Djw7kA.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><strong class="bd nv">图二</strong>。具有架构的全连接前馈神经网络，a = ((3，4，3，1)，<em class="od"> ρ)。图片作者。</em></p></figure><p id="db05" class="pw-post-body-paragraph ll lm iq ln b lo nx jr lq lr ny ju lt lu nz lw lx ly oa ma mb mc ob me mf lf ij bi translated">激活函数在决定神经网络如何连接以及哪些信息从一层传递到下一层方面至关重要。最后，它控制信息交换，允许神经网络从数据中“学习”。他们学什么和为什么学仍然是一个困难的话题。一些研究人员甚至认为，DNN可以从数据中学习一些特别的东西，而且这项研究是由几个层次共享的。因此，一个层学习了一些东西，然后将一个表示传递给下一个层，下一个层学习其他东西的说法是部分正确的。</p><h1 id="f9f9" class="mx my iq bd mz na oe nc nd ne of ng nh jw og jx nj jz oh ka nl kc oi kd nn no bi translated">数据拓扑</h1><p id="70b7" class="pw-post-body-paragraph ll lm iq ln b lo np jr lq lr nq ju lt lu nr lw lx ly ns ma mb mc nt me mf lf ij bi translated">通过<strong class="ln ir">降维</strong>可视化高维数据表示是一种众所周知的用于检查深度学习模型的技术。在图3中，我们可以看到一个具有a= ((33，500，250，50，1)，<em class="mp"> ρ)架构的网络的层表示。</em></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oj"><img src="../Images/3a4e7bbac4bb837246a82097b21903a0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UAeucOpo2tMYXpxJDMHq8w.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><strong class="bd nv">图3 </strong>。使用DNN可视化二元分类问题中的高维表示。具有33列和91行的原始数据集。网络架构，a = ((33，500，250，50，1)，<em class="od"> ρ)。</em>用UMAP算法(均匀流形近似和投影)完成的表示的维数减少。不同的层试图在聚类中分离低维流形，以便将标签(黄色点)从其余数据点推开。深度网络的任务是尽可能地分离流形，以便隔离标签点。图片作者。</p></figure><p id="5b34" class="pw-post-body-paragraph ll lm iq ln b lo nx jr lq lr ny ju lt lu nz lw lx ly oa ma mb mc ob me mf lf ij bi translated">除了降维之外，还有其他可视化高维模型的技术。拓扑学分析空间中元素的连接信息，处理定性的几何信息。拓扑数据分析(TDA)使用范畴理论、代数拓扑和其他纯数学方法来实现对数据形式的实际调查[4]。高维数据集极大地限制了我们可视化它们的能力。这就是为什么TDA可以帮助我们提高可视化和分析信息的能力。最常观察到的数据拓扑包括连接的零部件、回路、空隙等。</p><p id="cade" class="pw-post-body-paragraph ll lm iq ln b lo nx jr lq lr ny ju lt lu nz lw lx ly oa ma mb mc ob me mf lf ij bi translated">在深度神经网络中分析高维数据表示时,<strong class="ln ir">维数灾难</strong>是一个主要挑战。在高维空间中，点非常分散，当DNN将数据从一个层传输到另一个具有不同维数的层时，点之间的欧氏距离以及点到子集的距离往往会增加。拓扑学是数学的一个分支，研究独立于所选坐标的几何对象的属性，这意味着即使对象被拉伸、弯曲或以其他方式变形，这些属性也不会改变。这使得拓扑对于分析深度神经网络表示特别有用，因为它允许分析数据的底层结构，而不受数据点的规模或方向的影响。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ok"><img src="../Images/b97e8ef922276e8b9e2bb05f52278670.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FLoZOlb-albmKkGtgrxDZA.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><strong class="bd nv">图四</strong>。定量拓扑数据分析。图片作者。</p></figure><p id="94d1" class="pw-post-body-paragraph ll lm iq ln b lo nx jr lq lr ny ju lt lu nz lw lx ly oa ma mb mc ob me mf lf ij bi translated">拓扑学避免了距离函数的量化值，而是用一个点到底层空间子集的<strong class="ln ir">【无限接近】</strong>的概念来代替它们【4】。</p><p id="f335" class="pw-post-body-paragraph ll lm iq ln b lo nx jr lq lr ny ju lt lu nz lw lx ly oa ma mb mc ob me mf lf ij bi translated">拓扑数据分析方法遵循一个基本程序:我们遇到一个<strong class="ln ir">拓扑空间</strong>(一个高维数据集的表示)<strong class="ln ir"> </strong>，我们需要找到它的<strong class="ln ir">基本组</strong>(数据关系为链接、循环或空洞)。但是我们的数据集是一个陌生的空间，很难看到显式的循环和关系。然后，我们寻找另一个空间，它是与我们的[4]同伦等价的T12，并且它的基本群更容易计算。由于两个空间都是同伦等价的，我们知道我们空间中的基本群与新空间中的基本群是同构的。这是一个无坐标的过程。一级连接信息与数据链路相关，二级连接信息与数据环路相关，三级连接信息与空隙相关(图4显示一级为绿色，二级为蓝色。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ok"><img src="../Images/e432e8345af41d3dca651939cd9a454e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Uen1_lu3S1Tc6Ozf-swtSw.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><strong class="bd nv">图5 </strong>。定性拓扑数据分析。映射器算法。图片作者。</p></figure><p id="8f56" class="pw-post-body-paragraph ll lm iq ln b lo nx jr lq lr ny ju lt lu nz lw lx ly oa ma mb mc ob me mf lf ij bi translated">拓扑数据分析通过直接可视化为高维数据的定性理解提供了定量方法和工具(Carlsson G .，2009)。一个例子是<strong class="ln ir">映射算法</strong>(图5)。定量数据分析将找到表示的基本组，定性数据分析将带来关于数据的其他定性见解。</p><p id="ac38" class="pw-post-body-paragraph ll lm iq ln b lo nx jr lq lr ny ju lt lu nz lw lx ly oa ma mb mc ob me mf lf ij bi translated"><strong class="ln ir">空间的拓扑特征</strong>是它的<strong class="ln ir">基本群</strong>。在TDA，这些签名是用来表征数据空间的定量元素。</p><p id="2226" class="pw-post-body-paragraph ll lm iq ln b lo nx jr lq lr ny ju lt lu nz lw lx ly oa ma mb mc ob me mf lf ij bi translated">为了直观地理解什么是数据拓扑，我们邀请您阅读这篇精彩的帖子:<a class="ae nw" href="https://www.quantamagazine.org/the-mathematical-shape-of-big-science-data-20131004" rel="noopener ugc nofollow" target="_blank">即将到来的事物的数学形状</a>来自Tang Yau Hoong，发表在Quanta杂志上。</p><blockquote class="kv"><p id="9699" class="kw kx iq bd ky kz lg lh li lj lk lf dk translated">作为推论，我们可以说拓扑数据分析是关于寻找连接。</p></blockquote><h1 id="f6b3" class="mx my iq bd mz na oe nc nd ne of ng nh jw ol jx nj jz om ka nl kc on kd nn no bi translated">理解深网表示的拓扑:一个真实的例子</h1><p id="dfd6" class="pw-post-body-paragraph ll lm iq ln b lo np jr lq lr nq ju lt lu nr lw lx ly ns ma mb mc nt me mf lf ij bi translated">在这篇文章中，我们提出对深度神经网络(DNNs)的表示使用拓扑数据分析可以帮助我们更好地理解这些网络如何运行以及可以从每一层收集到什么信息。为了证明这一点，我们将使用印度政府提供的数据集作为药物发现黑客马拉松的一部分，该数据集也可以在Kaggle上获得。该数据集包括一系列已经过抗Sars-CoV-2有效性测试的药物，以及从PubChem库中获得的每个分子的其他化学细节。最终数据集包括100个测试分子，每个分子有40个特征，包括pIC50值，pic 50值是半最大抑制浓度的负对数，用于衡量物质作为抑制剂的功效。这种分析的目的是根据这些分子对Sars-CoV-2的有效性开发一种分类方法。该数据集还包括6个盲分子，它们没有pIC50值，可用于预测任务。</p><p id="8d21" class="pw-post-body-paragraph ll lm iq ln b lo nx jr lq lr ny ju lt lu nz lw lx ly oa ma mb mc ob me mf lf ij bi translated">我们使用<a class="ae nw" href="https://giotto-ai.github.io/gtda-docs/0.4.0/library.html" rel="noopener ugc nofollow" target="_blank"> Giotto-tda </a>库来执行tda计算。库<code class="fe oo op oq or b">gtda</code>是用Python编写的高性能拓扑机器学习工具箱。在我看来，使用<code class="fe oo op oq or b">gtda</code>的真正优势在于它与<code class="fe oo op oq or b">scikit-learn</code>的直接集成以及对表格数据、时间序列、图表或图片的支持。<br/>这两种资产都使得构建机器学习管道变得简单。对于深度神经网络，我们使用<a class="ae nw" href="https://keras.io/" rel="noopener ugc nofollow" target="_blank"> Keras </a>库。</p><h2 id="62ff" class="os my iq bd mz ot ou dn nd ov ow dp nh lu ox oy nj ly oz pa nl mc pb pc nn pd bi translated">深度神经网络架构</h2><p id="f229" class="pw-post-body-paragraph ll lm iq ln b lo np jr lq lr nq ju lt lu nr lw lx ly ns ma mb mc nt me mf lf ij bi translated">在我们的实验中，我们使用了具有架构的全连接神经网络，<em class="mp"> a = ((33，500，250，50，1)，ρ) </em>。这是一个具有三个隐藏层的基本图形。我们用Keras functional API建立了网络，以使不同的实验更具可重复性。功能API可以处理具有非线性拓扑的模型。</p><pre class="kg kh ki kj gt pe or pf pg aw ph bi"><span id="1d82" class="os my iq or b gy pi pj l pk pl"><strong class="or ir"># clean is the pandas data frame with the data, <br/># and 'pIC50' is the label feature.</strong></span><span id="369d" class="os my iq or b gy pm pj l pk pl">input_dim = len(clean.drop(columns='pIC50').columns) <br/>model = Sequential()</span><span id="1d5f" class="os my iq or b gy pm pj l pk pl"><strong class="or ir">#The Dense function in Keras constructs a fully connected neural network layer, automatically initializing the weights as biases.</strong></span><span id="7afd" class="os my iq or b gy pm pj l pk pl"><strong class="or ir">#First hidden layer</strong></span><span id="e796" class="os my iq or b gy pm pj l pk pl">model.add(Dense(50, activation='relu',  <br/>          kernel_initializer='random_normal',<br/>          kernel_regularizer=regularizers.l2(0.05),    <br/>                             input_dim=input_dim)<br/>)</span><span id="9f9a" class="os my iq or b gy pm pj l pk pl"><strong class="or ir">#Second hidden layer</strong></span><span id="8baa" class="os my iq or b gy pm pj l pk pl">model.add(Dense(40, activation='relu',  <br/>          kernel_initializer='random_normal',<br/>          kernel_regularizer=regularizers.l2(0.05) ))</span><span id="c1a8" class="os my iq or b gy pm pj l pk pl"><strong class="or ir">#Third hidden layer</strong></span><span id="d8fb" class="os my iq or b gy pm pj l pk pl">model.add(Dense(20, activation='relu',  <br/>          kernel_initializer='random_normal',<br/>          kernel_regularizer=regularizers.l2(0.05) ))</span><span id="87e4" class="os my iq or b gy pm pj l pk pl"><strong class="or ir">#Output layer<br/></strong>model.add(Dense(1, activation='sigmoid',  <br/>          kernel_initializer='random_normal',<br/>          kernel_regularizer=regularizers.l2(0.05) ))</span><span id="1413" class="os my iq or b gy pm pj l pk pl">model.compile(optimizer='adam',   <br/>             loss='binary_crossentropy',<br/>             metrics=['accuracy'])</span></pre><p id="083c" class="pw-post-body-paragraph ll lm iq ln b lo nx jr lq lr ny ju lt lu nz lw lx ly oa ma mb mc ob me mf lf ij bi translated">如前所述，激活函数在将数据从一层映射到另一层时至关重要。我们感兴趣的激活函数有两种:可逆的(具有连续逆的连续函数)和不可逆的。<code class="fe oo op oq or b">tanh</code>、<code class="fe oo op oq or b">sigmoid</code>或<code class="fe oo op oq or b">softplus</code>是一级函数的例子，<code class="fe oo op oq or b">ReLU</code>是不可逆函数的例子。激活函数可以是可逆的，但是神经网络作为一个整体，即使具有可逆的激活函数，通常也是不可逆的。</p><p id="3c15" class="pw-post-body-paragraph ll lm iq ln b lo nx jr lq lr ny ju lt lu nz lw lx ly oa ma mb mc ob me mf lf ij bi translated">在这一点上，它是值得重述的。我们想在深层神经网络中研究表象的拓扑结构。因此，关键问题将是拓扑签名是否从一种表示保留到下一种表示。答案可以在范畴理论中找到:将一个范畴的对象(例如，德DNN表示)转换为另一个范畴的对象的构造是<strong class="ln ir">函子</strong>如果:</p><ul class=""><li id="2941" class="pn po iq ln b lo nx lr ny lu pp ly pq mc pr lf ps pt pu pv bi translated">它可以扩展到态射上的映射</li><li id="284f" class="pn po iq ln b lo pw lr px lu py ly pz mc qa lf ps pt pu pv bi translated">同时保留复合和同一性态射(Riehl E .，2017)。</li></ul><p id="0619" class="pw-post-body-paragraph ll lm iq ln b lo nx jr lq lr ny ju lt lu nz lw lx ly oa ma mb mc ob me mf lf ij bi translated">这种结构定义了类别之间的态射，称为<strong class="ln ir">函子</strong>。此类函子或构造将类别之间的态射定义为类别<em class="mp"> C </em>和<em class="mp"> D </em>之间的<em class="mp"> F : C → D </em>。因此，该任务仅限于确定从一层映射到另一层的数据是否是函子。函子描述了类别的等价性，因此一个类别中的对象可以被翻译成另一个类别中的对象并从另一个类别中重构。</p><p id="915f" class="pw-post-body-paragraph ll lm iq ln b lo nx jr lq lr ny ju lt lu nz lw lx ly oa ma mb mc ob me mf lf ij bi translated">为了检查DNN <strong class="ln ir"> </strong>中的<strong class="ln ir">功能</strong>，首先，我们分析我们的DNN架构的准确性和损失(图6)。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi qb"><img src="../Images/a702450575944e8b3590340741733ec8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*P-9ksYGMxX_TQaEbRiqLKA.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><strong class="bd nv">图6。</strong>我们DNN建筑的准确性和损失。图片作者。</p></figure><p id="6a6b" class="pw-post-body-paragraph ll lm iq ln b lo nx jr lq lr ny ju lt lu nz lw lx ly oa ma mb mc ob me mf lf ij bi translated">训练精度为0.916，训练精度为0.709。请记住，我们的数据集很小，因此准确性会受到影响。现在，我们计算每个表示的拓扑签名(也就是说，为每个图层表示找到基本组)。我们使用Vietori-Rips过滤[4]对<code class="fe oo op oq or b">gtda</code>库进行了这项工作。结果如图7所示。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi qc"><img src="../Images/4560761bf9bf36756532bf316ac60563.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2-E3NvmefOWMnp2le-1Qig.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><strong class="bd nv">图7 </strong>。图层表示的过滤。图片作者。</p></figure><p id="a333" class="pw-post-body-paragraph ll lm iq ln b lo nx jr lq lr ny ju lt lu nz lw lx ly oa ma mb mc ob me mf lf ij bi translated">为了理解该图，我们可以考虑以下情况:橙色点表示拓扑空间中“靠近”在一起的数据点，而绿色点表示“环”或点簇。靠近对角线的绿点可能被认为是噪声或不完全形成的环。橙色点还指示集合的大小，并可用作表征空间的定量方法，称为拓扑签名。在图7中，我们可以看到从输入层到第三个隐藏层的拓扑签名被保留，有四个不同的环和几个连接点。垂直线上还有一个橘色的点，和其他的分开。在二元分类任务中，我们期望在代表两个类别的垂直线上看到两个不同的点簇，否则该集合可能是不可分类的。</p><p id="3bec" class="pw-post-body-paragraph ll lm iq ln b lo nx jr lq lr ny ju lt lu nz lw lx ly oa ma mb mc ob me mf lf ij bi translated">这在输出图层中很明显，我们可以看到一个点明显远离其余的点，这意味着有两个类别，其中一个比另一个小得多(一个只有一个点，另一个有超过10个点)。然后，DNN将这些相连的点分开，直到她得到一个清晰的分离，即两点之间的最大“距离”或最小“接近度”。它对数据集进行了二进制分类，因此拓扑签名似乎是相同的，但在输出函数中，我们要求DNN对此进行操作以进行分类。所以表示之间的转换是<strong class="ln ir">函子</strong>:复合和同一性态射被保留。</p><p id="ea4b" class="pw-post-body-paragraph ll lm iq ln b lo nx jr lq lr ny ju lt lu nz lw lx ly oa ma mb mc ob me mf lf ij bi translated">我们重复了这个实验，改变了几个超参数。最有趣的结论是，函子性似乎更依赖于网络宽度，而不是激活函数的可逆性。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi qb"><img src="../Images/ec2bb02e7e097087b58f591207cee6ce.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WbGrb7LCSkoPDCB7uBBeiQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><strong class="bd nv">图8 </strong>。精度和损耗同<em class="od"> a = ((33，50，25，10，1)，ρ) </em></p></figure><p id="3c1f" class="pw-post-body-paragraph ll lm iq ln b lo nx jr lq lr ny ju lt lu nz lw lx ly oa ma mb mc ob me mf lf ij bi translated">我们实验了一个网络架构<em class="mp"> a = ((33，50，25，10，1)，ρ)。</em>精度和损耗与更广泛的架构非常相似(图8)。然后，我们执行了相同的Vietory-Rips过滤(图9)。我们看到拓扑签名与原始数据集大相径庭。在第一个隐藏层中，我们看到较少的循环在第二个隐藏层中完全消失(我们发现基本上是噪声)。从第二层开始，不映射拓扑签名。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi qd"><img src="../Images/2432cf80db15580ef18147d7913d9fde.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WHX37Gi6bDcFq4K14ZlmFg.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><strong class="bd nv">图九</strong>。层表示的Vietory-Rips过滤，a <em class="od"> = ((33，50，25，10，1)，ρ) </em>。图片作者。</p></figure><p id="af1c" class="pw-post-body-paragraph ll lm iq ln b lo nx jr lq lr ny ju lt lu nz lw lx ly oa ma mb mc ob me mf lf ij bi translated">改写:神经网络即使在更宽的架构下也能够分离连接点，这似乎是违反直觉的，因为它表明DNN不需要考虑拓扑签名来进行准确的分离。然而，通过相对简单的操作来实现这一点是可能的。DNN通过改变点之间的“接近度”来“操纵”数据的拓扑结构，这样就不会保留同态变换。</p><p id="cc7d" class="pw-post-body-paragraph ll lm iq ln b lo nx jr lq lr ny ju lt lu nz lw lx ly oa ma mb mc ob me mf lf ij bi translated">另一个话题是，在“学习”过程中，可能有必要考虑范畴的等价性。当我们考虑小数据集时，尽管我们获得了相对较高的准确性水平，但我们应该努力做得更好。拓扑数据分析为我们提供了在学习过程中双重测试网络性能的工具。</p><h2 id="d3e6" class="os my iq bd mz ot ou dn nd ov ow dp nh lu ox oy nj ly oz pa nl mc pb pc nn pd bi translated">定性信息</h2><p id="da5b" class="pw-post-body-paragraph ll lm iq ln b lo np jr lq lr nq ju lt lu nr lw lx ly ns ma mb mc nt me mf lf ij bi translated">我们已经看到了定量拓扑方法。但是拓扑学提供了非常有趣的定性理解工具。图5中介绍的Mapper算法是一种非常有趣的“可视化”算法，可以增强降维算法的性能。映射器算法可以自然地被视为一种聚类算法，它转换成具有有向边的图。在图10中，我们可以看到将mapper算法应用于具有架构<em class="mp"> a = ((33，500，250，50，1)，ρ) - </em>宽层<em class="mp"> -的网络中的层表示的结果。</em>节点用“pIC50”值的标度来着色(我们的标签特性)。尽管我们已经进行了二元分类(0表示负pIC50值，1表示正pIC50值)，我们现在对回归的观点感兴趣，以获得啤酒的洞察力。我们用<code class="fe oo op oq or b">gtda</code>库实现了mapper算法。制图者在选择超参数时的困难过滤、覆盖和聚类数据。</p><pre class="kg kh ki kj gt pe or pf pg aw ph bi"><span id="500c" class="os my iq or b gy pi pj l pk pl"><strong class="or ir">""" 1. Define filter function – can be any scikit-learn transformer. It is returning a selection of columns of the data """</strong></span><span id="2ce8" class="os my iq or b gy pm pj l pk pl">filter_func = Eccentricity(metric= 'euclidean')</span><span id="16f9" class="os my iq or b gy pm pj l pk pl"><strong class="or ir">""" 2. Define cover """</strong></span><span id="1811" class="os my iq or b gy pm pj l pk pl">cover = CubicalCover(n_intervals=20, overlap_frac=0.5)</span><span id="d76d" class="os my iq or b gy pm pj l pk pl"><strong class="or ir">""" 3. Choose clustering algorithm – default is DBSCAN """</strong></span><span id="6d28" class="os my iq or b gy pm pj l pk pl">clusterer = DBSCAN(eps=8,<br/>                   min_samples=2,<br/>                   metric='euclidean')</span><span id="d0bc" class="os my iq or b gy pm pj l pk pl"><strong class="or ir">""" 4. Initialise pipeline """</strong></span><span id="633e" class="os my iq or b gy pm pj l pk pl">pipe_mapper = make_mapper_pipeline(filter_func=filter_func,<br/>                                   cover=cover,<br/>                                   clusterer=clusterer, <br/>                                   verbose=False,<br/>                                   n_jobs=-1)</span><span id="a6af" class="os my iq or b gy pm pj l pk pl"><strong class="or ir">""" 5. Plot mapper """</strong></span><span id="4953" class="os my iq or b gy pm pj l pk pl">plotly_params = {"node_trace": {"marker_colorscale": "RdBu"}}</span><span id="73bf" class="os my iq or b gy pm pj l pk pl">fig = plot_static_mapper_graph(pipe_mapper,<br/>                               X,<br/>                               layout='fruchterman_reingold',<br/>                               color_variable =clean['pIC50'],<br/>                               node_scale = 20,   <br/>                               plotly_params=plotly_params<br/>)<br/>fig.show(config={'scrollZoom': True})</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi qe"><img src="../Images/595d8b78f62047b95c210929a3fc48f9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*K4Bib1PBVG6b8V6zg_ztLQ.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><strong class="bd nv">图10 </strong>。一个<em class="od"> = ((33，500，250，50，1)，ρ) </em>的图层表示映射算法。图片作者。</p></figure><p id="a490" class="pw-post-body-paragraph ll lm iq ln b lo nx jr lq lr ny ju lt lu nz lw lx ly oa ma mb mc ob me mf lf ij bi translated">我们在图10中看到了应用于输入、隐藏和输出层的映射器算法。输入层中的映射器显示了一些“循环”、几个不同长度的连通分量和一些非连通分量(孤立节点)。我们还可以在隐藏层1中看到循环，但在下面的层中看不到。在输入层，我们看到蓝点(pIC50的正值)和红点(pIC50的负值)完全混合在一起。有了这个图表，我们无法想象一个简单的方法来区分蓝点和红点。但输出层显示了深度神经网络是如何实现这种分离的:蓝色和红色的节点非常容易区分，因为蓝色位于连接组件的末端。我们可以很容易地切断联系，我们将有一个二元分类。我们可以看到，我们需要三个隐藏层来执行这种分离。如果我们看第三层，这种分离仍然是不可能的，所以另一个转换是必要的(输出层)。</p><blockquote class="kv"><p id="49be" class="kw kx iq bd ky kz lg lh li lj lk lf dk translated">DNN通过尝试连接尽可能多的数据点来解决分类问题。输出层通常包括一个主连接分支，以及一个隔离节点和一个双节点连接组件。与网络中的前几层相比，这一层中的点往往更具连通性。</p></blockquote><p id="8a49" class="pw-post-body-paragraph ll lm iq ln b lo lp jr lq lr ls ju lt lu lv lw lx ly lz ma mb mc md me mf lf ij bi translated">mapper算法的另一个有趣的特性是，您可以可视化每个节点，提取数据点，并直接从原始数据集转换见解(图11)。我们看到mapper算法将节点转换为数据点，特别是与分子绘图相关的特征。最后但同样重要的是，我们可以将输入数据集中的每个特征以及隐藏层中的每个神经元的图形可视化。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi qf"><img src="../Images/c2c40d302229d429070ae6a61252e6ad.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_i_2E4npij1sJl_QIr9wJg.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><strong class="bd nv">图11 </strong>。Mapper算法将节点转换为数据，在这种情况下转换为分子图。图片作者。</p></figure><p id="cee5" class="pw-post-body-paragraph ll lm iq ln b lo nx jr lq lr ny ju lt lu nz lw lx ly oa ma mb mc ob me mf lf ij bi translated">mapper的这一功能非常有用，例如，可以监控每个分子在每个层的图形中的位置。</p><h1 id="8c1d" class="mx my iq bd mz na oe nc nd ne of ng nh jw og jx nj jz oh ka nl kc oi kd nn no bi translated">结论</h1><p id="3e69" class="pw-post-body-paragraph ll lm iq ln b lo np jr lq lr nq ju lt lu nr lw lx ly ns ma mb mc nt me mf lf ij bi translated">在这项研究中，我们检查了使用神经网络进行图像分类和结构化数据分类。虽然已经有一些关于使用神经网络进行图像分类的研究，但是它们用于结构化数据分类的例子较少。我们已经展示了使用拓扑数据分析作为工具的潜力，以了解dnn在完成任务时使用的流程，并提高其性能。此外，我们发现，无论是否保留数据身份，DNNs都可以达到类似的准确性水平，尽管这只是在小数据集上进行的测试。当从数据中学习时，使用保留拓扑签名作为评估DNN准确性的度量是值得探索的。</p></div><div class="ab cl mq mr hu ms" role="separator"><span class="mt bw bk mu mv mw"/><span class="mt bw bk mu mv mw"/><span class="mt bw bk mu mv"/></div><div class="ij ik il im in"><p id="6f77" class="pw-post-body-paragraph ll lm iq ln b lo nx jr lq lr ny ju lt lu nz lw lx ly oa ma mb mc ob me mf lf ij bi translated">你可以在这里找到这篇文章<a class="ae nw" href="https://github.com/Javihaus/Explainable-Deep-Neural-Networks" rel="noopener ugc nofollow" target="_blank">中使用的代码。</a></p></div><div class="ab cl mq mr hu ms" role="separator"><span class="mt bw bk mu mv mw"/><span class="mt bw bk mu mv mw"/><span class="mt bw bk mu mv"/></div><div class="ij ik il im in"><p id="ed1b" class="pw-post-body-paragraph ll lm iq ln b lo nx jr lq lr ny ju lt lu nz lw lx ly oa ma mb mc ob me mf lf ij bi translated">Berner，j .，Grohs，p .，Kutyniok，g .，&amp; Petersen，P. (2021年)。深度学习的现代数学。<em class="mp"> ArXiv，abs/2105.04026 </em>。</p><p id="a0e4" class="pw-post-body-paragraph ll lm iq ln b lo nx jr lq lr ny ju lt lu nz lw lx ly oa ma mb mc ob me mf lf ij bi translated">C.奥拉(2014)。神经网络、流形和拓扑。<a class="ae nw" href="https://colah.github.io/posts/2014-03-NN-Manifolds-Topology/" rel="noopener ugc nofollow" target="_blank"> <em class="mp">陈少华博客</em> </a> <em class="mp">。</em></p><p id="cc59" class="pw-post-body-paragraph ll lm iq ln b lo nx jr lq lr ny ju lt lu nz lw lx ly oa ma mb mc ob me mf lf ij bi translated">C.Fefferman、S. Mitter和H. Narayanan (2016年)。检验多重假设。美国杂志。数学。社会主义者29, 4, 983–1049.</p><p id="fbd2" class="pw-post-body-paragraph ll lm iq ln b lo nx jr lq lr ny ju lt lu nz lw lx ly oa ma mb mc ob me mf lf ij bi translated">G.卡尔松。(2009)拓扑和数据。<em class="mp">公牛。阿米尔。数学。社会主义者46 , 255–308.</em></p><p id="6fc3" class="pw-post-body-paragraph ll lm iq ln b lo nx jr lq lr ny ju lt lu nz lw lx ly oa ma mb mc ob me mf lf ij bi translated">E.瑞尔。(2017)语境中的范畴论。<em class="mp">信使多佛出版物</em>。</p></div></div>    
</body>
</html>