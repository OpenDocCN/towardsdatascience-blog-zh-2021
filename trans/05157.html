<html>
<head>
<title>Creating ML Datasets with Ease using BigQuery and Dataflow</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用BigQuery和Dataflow轻松创建ML数据集</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/creating-ml-datasets-with-ease-using-bigquery-and-dataflow-121eef01f53a?source=collection_archive---------13-----------------------#2021-05-06">https://towardsdatascience.com/creating-ml-datasets-with-ease-using-bigquery-and-dataflow-121eef01f53a?source=collection_archive---------13-----------------------#2021-05-06</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="36b3" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">放弃内存处理，支持并行执行和水平扩展</h2></div><p id="a38e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">TL；DR:如果你正在处理大量数据，谷歌云平台(GCP)上的BigQuery和Dataflow可以提高你的效率，让你在为机器学习生成数据集时更加轻松。</p><p id="bc91" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">最近，初创公司<a class="ae lb" href="https://reviewr.ai/" rel="noopener ugc nofollow" target="_blank"> reviewr.ai </a>找到我，让我完成一项数据工程任务，包括为机器学习(ML)准备数据和训练ML模型。大公司显然可以让他们的数据科学家花上几天或几周的时间来完成这样的任务，因为他们必须与本地基础设施或云提供商合作，选择云提供商主要是因为该公司多年来一直在许可其office套件，与大公司不同，初创公司希望快速完成任务——速度很重要。与许多公司不同，初创公司在选择特定任务的基础设施时通常非常灵活。因此，我们同意应该在GCP上完成这项工作，在我看来，它仍然是大数据和ML的最佳云平台。因此，我没有花几天时间，而是在一个上午做了所有的数据准备，并在训练ML模型的时候休息了一个下午。</p><p id="dec4" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">本文不是关于任务的机器学习部分或ML模型的准确性。它是关于在机器学习开始之前必须发生的数据工程和预处理。大多数数据科学家可能会同意，这通常是花费大部分时间和精力的地方。</p><h2 id="79a6" class="lc ld iq bd le lf lg dn lh li lj dp lk ko ll lm ln ks lo lp lq kw lr ls lt lu bi translated">令人惊叹的任务</h2><p id="51ba" class="pw-post-body-paragraph kf kg iq kh b ki lv jr kk kl lw ju kn ko lx kq kr ks ly ku kv kw lz ky kz la ij bi translated">我被要求使用来自拥抱脸的<a class="ae lb" href="https://huggingface.co/datasets/amazon_us_reviews" rel="noopener ugc nofollow" target="_blank">亚马逊_美国_评论数据集来训练一个简单的NLP分类模型。它包含来自亚马逊网站的近1.51亿条顾客评论，总计约76 GB的数据。基于用户写的产品评论，该模型应该将评论分成特定的类别。类成员资格(标签)可以从数据集中包含的数据中导出。因为这个问题很标准，而且我们想要快速的结果，我在GCP上使用了</a><a class="ae lb" href="https://cloud.google.com/natural-language/automl/docs" rel="noopener ugc nofollow" target="_blank"> AutoML </a>。</p><p id="2e14" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">为了能够使用AutoML中的数据，我必须以某种方式从1 . 51亿个样本中选择最多100万个合适的样本，并将它们转换为<a class="ae lb" href="https://cloud.google.com/natural-language/automl/docs/prepare#csv" rel="noopener ugc nofollow" target="_blank">所需的格式</a>，即每个样本一个文本文件，其中包含评论文本以及一个索引CSV文件，其中包含所有样本的URIs和类别。听起来令人畏惧或耗时？都不是。</p><h2 id="23cb" class="lc ld iq bd le lf lg dn lh li lj dp lk ko ll lm ln ks lo lp lq kw lr ls lt lu bi translated">BigQuery:全进</h2><p id="71c8" class="pw-post-body-paragraph kf kg iq kh b ki lv jr kk kl lw ju kn ko lx kq kr ks ly ku kv kw lz ky kz la ij bi translated">让我们从获取数据副本开始。这些文件存储在AWS S3存储桶中，可以通过HTTP请求公开访问。<a class="ae lb" href="https://cloud.google.com/storage" rel="noopener ugc nofollow" target="_blank">在我们进一步处理它们之前，谷歌云存储</a> (GCS)是放置它们的合适地方。具体来说，我将把数据吸收到<a class="ae lb" href="https://cloud.google.com/bigquery" rel="noopener ugc nofollow" target="_blank"> BigQuery </a>中，其原因稍后将变得显而易见。有几种方法可以将数据传输到GCS——例如，使用bash脚本<code class="fe ma mb mc md b">wget</code> s或<code class="fe ma mb mc md b">curl</code> s文件，然后<code class="fe ma mb mc md b">gsutil cp</code> s循环地将它们传输到GCS，或者使用小的<a class="ae lb" href="https://github.com/jsarbach/data-engineering/blob/main/dataset-creation-bigquery-dataflow/dataset_to_gcs.py" rel="noopener ugc nofollow" target="_blank"> Python脚本</a>。</p><p id="f3d1" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我在<a class="ae lb" href="https://cloud.google.com/shell" rel="noopener ugc nofollow" target="_blank">云外壳</a>中运行这个程序，而不是在我的本地机器上，因为通过谷歌网络传输数据花费的时间要少得多。一旦所有文件都在GCS桶中，我只需使用<code class="fe ma mb mc md b">bq</code>将它们一次性加载到BigQuery 中，仍然来自云Shell:</p><pre class="me mf mg mh gt mi md mj mk aw ml bi"><span id="c913" class="lc ld iq md b gy mm mn l mo mp">bq load --source_format CSV --autodetect --field_delimiter tab <br/>--skip_leading_rows 1 --quote ' ' [DATASET_NAME].amazon_us_reviews gs://[BUCKET_NAME]/datasets/amazon-us-reviews/raw/*.tsv.gz</span></pre><p id="ee86" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">通过查看标志可以看出，我让BigQuery从数据中推断出<a class="ae lb" href="https://cloud.google.com/bigquery/docs/schemas" rel="noopener ugc nofollow" target="_blank">模式</a>，并将字段分隔符设置为制表符而不是逗号，将引号字符设置为none而不是双引号(“制表符分隔，没有引号和转义符”，如数据集描述所述)。BigQuery本身可以处理gzipped文件，所以我们不必先解压缩文件。</p><p id="0e6c" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">几分钟后，数据存储在BigQuery中，并准备好接受检查:</p><figure class="me mf mg mh gt mr gh gi paragraph-image"><div class="gh gi mq"><img src="../Images/4414eef4fda92e2042c14a74f61c4be8.png" data-original-src="https://miro.medium.com/v2/resize:fit:562/format:webp/1*ciq2QwUzN691vfcp08gbQQ.png"/></div></figure><p id="c3fb" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">附带说明:如果您有一个AWS帐户(因此可以创建一个<em class="mu">访问密钥ID </em>和一个<em class="mu">秘密访问密钥</em>，您可以使用<a class="ae lb" href="https://cloud.google.com/storage-transfer-service" rel="noopener ugc nofollow" target="_blank">存储传输服务</a>下载文件，甚至可以使用<a class="ae lb" href="https://cloud.google.com/bigquery-transfer/docs/s3-transfer" rel="noopener ugc nofollow" target="_blank"> BigQuery数据传输服务</a>将数据直接导入BigQuery。</p><p id="8072" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">接下来，让我们运行几个查询来感受一下数据。例如，我们可以询问每个产品组和星级有多少评论，以及星级在产品组中的百分比是多少:</p><figure class="me mf mg mh gt mr gh gi paragraph-image"><div role="button" tabindex="0" class="mw mx di my bf mz"><div class="gh gi mv"><img src="../Images/83a3162180767b5932909b57ce93ee80.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gR14cEmO3cCeYtJQjLstPQ.png"/></div></div></figure><p id="fdeb" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">或者:哪些产品类别平均获得的评论最长？</p><figure class="me mf mg mh gt mr gh gi paragraph-image"><div class="gh gi na"><img src="../Images/74e396661b8f7666f3b43bd4d5e9eeda.png" data-original-src="https://miro.medium.com/v2/resize:fit:974/format:webp/1*JXwYZwDcyF8p3GvDOTIB_Q.png"/></div><p class="nb nc gj gh gi nd ne bd b be z dk translated">顺便说一下，最短的是礼品卡，这可能不会让人感到意外。</p></figure><p id="b6bb" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如您所见，在1 . 51亿行上运行这些查询只需要几秒钟。因此，你可以尽情发挥你的创造力，在你提出问题的时候尽快分析数据。(但是，请记住，你是按照处理的数据量向<a class="ae lb" href="https://cloud.google.com/bigquery/pricing#analysis_pricing_models" rel="noopener ugc nofollow" target="_blank">收费的，除非你选择统一价格模式。)如果你想绘制结果，只需点击一下鼠标，就可以在</a><a class="ae lb" href="https://cloud.google.com/bigquery/docs/visualize-data-studio" rel="noopener ugc nofollow" target="_blank">数据工作室</a>中看到它们。</p><h2 id="0b1b" class="lc ld iq bd le lf lg dn lh li lj dp lk ko ll lm ln ks lo lp lq kw lr ls lt lu bi translated">从数据集中选择训练数据</h2><p id="41db" class="pw-post-body-paragraph kf kg iq kh b ki lv jr kk kl lw ju kn ko lx kq kr ks ly ku kv kw lz ky kz la ij bi translated">然而，我们不只是想分析数据。我们还希望为AutoML生成训练数据集，这意味着从总共1.51亿个样本中选择100万个样本并计算特征。我不想透露<em class="mu"> reviewr.ai </em>到底想让我做什么，所以让我们假设我们想从评论机构推断评级(1-5颗星)。理想情况下，在这种情况下，每个类有200，000个例子。</p><p id="835b" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">首先，我们将排除所有没有审核主体的审核(即<code class="fe ma mb mc md b">WHERE review_body IS NOT NULL</code>)。此外，我们必须排除具有相同review_body的行，因为基本事实中的分类可能会变得模糊，而且AutoML不允许重复。事实证明，有很多这样的例子:</p><figure class="me mf mg mh gt mr gh gi paragraph-image"><div class="gh gi nf"><img src="../Images/d789e82e2d2e44a77cfc25ba4e9cd297.png" data-original-src="https://miro.medium.com/v2/resize:fit:990/format:webp/1*ezZH0ueo9MDk88Ec9-zlLg.png"/></div><p class="nb nc gj gh gi nd ne bd b be z dk translated">厉害！</p></figure><p id="5f4f" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">为了进一步缩小结果范围，我们将关注大多数用户认为有帮助的评论。此外，我们将要求一个评论收到至少x票。在哪里设置阈值，x？让我们制作一个统计图，感受一下每个评分的投票数是如何分布的:</p><figure class="me mf mg mh gt mr gh gi paragraph-image"><div role="button" tabindex="0" class="mw mx di my bf mz"><div class="gh gi ng"><img src="../Images/8c74728c5923fdb639825decb13aab70.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*I_Je0HM-pJzAaTd2OwwmEg.png"/></div></div></figure><p id="c304" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如果我们将阈值设置在10到20之间，我们似乎仍然有足够的样本用于每一类，所以我选择15。这意味着我们只包括至少获得15票的评论，其中大多数被认为是有帮助的。当然，我们可以更进一步，要求每个产品类别都应该有平等的代表性。出于本文的目的，我们将它留在这里。因此，下面是最终定义数据集的查询:</p><figure class="me mf mg mh gt mr"><div class="bz fp l di"><div class="nh ni l"/></div></figure><p id="7bd2" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">运行一个快速的<code class="fe ma mb mc md b">SELECT class, COUNT(*) FROM ... GROUP BY class</code>确认我们每个类确实有200，000个样本。</p><p id="e2f6" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">一旦我们对我们的查询感到满意，我们就可以运行它并将结果具体化为一个表，或者将查询本身保存为一个视图。加载表将比查询视图更快(这基本上与运行查询本身是一样的)。尽管如此，我还是选择创建一个视图，因为我们只需要查询一次或几次，所以速度并不重要。该视图的优点是记录了数据集是如何创建的，并且如果您想要为另一个模型迭代创建新的数据集，您可以很容易地修改它并以不同的名称保存它。</p><p id="126d" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">那么，现在我们通过SQL查询得到了训练数据集的定义——我们如何将它转换成我们需要的格式(每个示例一个包含评论正文的文本文件，加上一个包含所有示例的URIs和类的CSV文件)？将查询结果导出为CSV文件，并编写Python脚本来逐行处理该文件？这将花费一两个永恒的时间(处理，而不是写剧本)。相反，让我们转向另一个专门用于这种工作负载的伟大工具:GCP上的<a class="ae lb" href="https://beam.apache.org/" rel="noopener ugc nofollow" target="_blank"> Apache Beam </a>和<a class="ae lb" href="https://cloud.google.com/dataflow" rel="noopener ugc nofollow" target="_blank"> Dataflow </a>。</p><h2 id="1540" class="lc ld iq bd le lf lg dn lh li lj dp lk ko ll lm ln ks lo lp lq kw lr ls lt lu bi translated">无与伦比的并行执行速度</h2><p id="b248" class="pw-post-body-paragraph kf kg iq kh b ki lv jr kk kl lw ju kn ko lx kq kr ks ly ku kv kw lz ky kz la ij bi translated">引用<a class="ae lb" href="https://beam.apache.org/get-started/beam-overview/" rel="noopener ugc nofollow" target="_blank">文档</a>，“Apache Beam是一个开源的统一模型，用于定义批处理和流数据并行处理管道”。Dataflow是GCP Apache Beam的一个完全托管的跑步者。通常的工作流程是这样的:编写您的管道(用Java或Python)，用一些元素在本地测试它以验证它的工作，然后将其部署到数据流以在云中大规模处理整个数据集。</p><p id="0655" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">Apache Beam有一组I/O连接器(“源”和“接收器”)，其中一个用于BigQuery，一个用于GCS。这意味着我们可以直接从BigQuery查询结果中读取，并将输出写入GCS bucket。从概念上讲，我们将要构建的管道如下所示:</p><figure class="me mf mg mh gt mr gh gi paragraph-image"><div class="gh gi nj"><img src="../Images/0d40bc37a3667b606441089f97cfa1d3.png" data-original-src="https://miro.medium.com/v2/resize:fit:996/format:webp/1*Nfv_3beE1w7gEaDhng0uHw.png"/></div></figure><p id="c458" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">管道从一个源(BigQuery查询结果，包含review_id、review_body和类)读取数据，并生成两个输出:第一个分支使用review_body创建100万个文本文件，第二个分支创建100万个包含URI ( <code class="fe ma mb mc md b">gs://...</code>)和类的字符串，然后将它们连接起来并将结果写入一个CSV文件。管道以每行为基础进行操作。也就是说，每个元素都是查询结果中的一行。</p><p id="b395" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">以下是完整的管道代码:</p><figure class="me mf mg mh gt mr"><div class="bz fp l di"><div class="nh ni l"/></div></figure><p id="d803" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">使用Apache Beam的好处在于，因为查询结果的每一行都可以独立处理，所以我们实现了大规模并行处理。正如您在下面看到的，Dataflow确实将工作线程的数量扩展到了527个，每秒处理大约2，200行:</p><figure class="me mf mg mh gt mr gh gi paragraph-image"><div role="button" tabindex="0" class="mw mx di my bf mz"><div class="gh gi nk"><img src="../Images/0c3d8a172dc0d050f118556b6bc8a6be.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7y74w9Vn-4STk0koZnze6w.png"/></div></div></figure><figure class="me mf mg mh gt mr gh gi paragraph-image"><div role="button" tabindex="0" class="mw mx di my bf mz"><div class="gh gi nl"><img src="../Images/6c8dc483f0df3dd1a8c4e0cac1e1a6ed.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nbsyVBCcr2XRx_F39XSHhg.png"/></div></div></figure><p id="dc76" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">工作所需的基础设施得到全面管理。在开始实际的管道执行之前，Dataflow需要花几分钟来配置基础设施(计算引擎)。作业成功完成(或失败)后，它会降速并删除虚拟机。</p><p id="4d46" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">由于并行执行，相当于72个CPU小时的工作仅用了大约18分钟就完成了。一个重要的注意事项:数据流扩展工作负载的程度可能会受到配额的限制。特别是，在新创建的GCP项目中，<em class="mu">Compute Engine API in-use IP addresses</em>配额(默认为每个区域8个)会限制为作业提供的计算实例数量(因为每个工作实例都需要一个IP地址)。因此，如果您发现现有配额过低，请务必提前请求增加配额。</p><p id="dd8a" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在我们有了:100万个文本文件和一个CSV，准备导入AutoML。</p><figure class="me mf mg mh gt mr gh gi paragraph-image"><div class="gh gi nm"><img src="../Images/84b1437fd56aeb2fc843be7fcf334222.png" data-original-src="https://miro.medium.com/v2/resize:fit:1120/format:webp/1*VfbDMYNh4U7N3qs-9ypwOQ.png"/></div></figure><figure class="me mf mg mh gt mr gh gi paragraph-image"><div role="button" tabindex="0" class="mw mx di my bf mz"><div class="gh gi nn"><img src="../Images/9531df0a53748aa519191a2c928a3eeb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*funhaV4DnP9Cvd0fbtABnw.png"/></div></div><p class="nb nc gj gh gi nd ne bd b be z dk translated">工作证明</p></figure><h2 id="b5c8" class="lc ld iq bd le lf lg dn lh li lj dp lk ko ll lm ln ks lo lp lq kw lr ls lt lu bi translated">外卖食品</h2><p id="936b" class="pw-post-body-paragraph kf kg iq kh b ki lv jr kk kl lw ju kn ko lx kq kr ks ly ku kv kw lz ky kz la ij bi translated">现在，准备这个数据集的麻烦已经消失在云中，让我们得出一些结论:</p><ul class=""><li id="f041" class="no np iq kh b ki kj kl km ko nq ks nr kw ns la nt nu nv nw bi translated"><strong class="kh ir">水平而不是垂直缩放。</strong>我认识的许多数据科学家都专注于垂直扩展。不符合记忆？让我们得到一个有更多内存的更大的实例。然而，在坚持内存处理的同时，仅仅投入更多的内存来解决问题，会失去并行工作负载的优势，从而在更短的时间内完成更多的工作。对于BigQuery中的数据分析，我不必花一秒钟考虑它需要多少内存——我只需打开BigQuery UI并开始查询。我的数据流工作的工人都是<a class="ae lb" href="https://cloud.google.com/compute/docs/machine-types" rel="noopener ugc nofollow" target="_blank"> n1-standard-1 </a>虚拟机，带有1个vCPU和3.75 GB的内存。唯一可能面临内存瓶颈的实例是在将一百万行写入文本文件之前收集并连接它们的工作人员。在这种情况下，产生的字符串使用了123 MB，所以根本没有达到限制。如果我们这样做，我们只会创建分片文件，而不是一个单一的文件。</li><li id="773c" class="no np iq kh b ki nx kl ny ko nz ks oa kw ob la nt nu nv nw bi translated">使用GCP的专业数据工程工具。为什么每次想使用Jupyter笔记本时都要麻烦地启动它并将数据加载到pandas中呢？相反，您可以将数据转储到BigQuery中，并在那里保存数据，使您可以随时查询数据。当Jupyter实例完成旋转，并且已经将所有数据加载到pandas数据帧中时，您可能已经完成了BigQuery中的数据分析。当然，你仍然可以使用Jupyter毕竟它对于数据科学任务有其优点。但是使用它来查询BigQuery和编写Apache Beam管道，而不是处理内存中的数据。顺便说一句:也有很多现成的<a class="ae lb" href="https://cloud.google.com/dataflow/docs/guides/templates/provided-templates" rel="noopener ugc nofollow" target="_blank">数据流模板</a>，所以你甚至可以不用写自己的管道。</li><li id="6330" class="no np iq kh b ki nx kl ny ko nz ks oa kw ob la nt nu nv nw bi translated"><strong class="kh ir">精益和可重用流程。</strong>使用BigQuery和Apache Beam的工作流是精简和灵活的。需要另一个具有不同功能的训练集吗？只需编写所需的查询，启动管道，20分钟后，您就已经将新数据集导入到AutoML中了。如果我们想用相同的数据集在TensorFlow中训练自己的模型呢？稍微修改一下Apache Beam管道，输出TFRecord文件，就可以了。</li><li id="b237" class="no np iq kh b ki nx kl ny ko nz ks oa kw ob la nt nu nv nw bi translated"><strong class="kh ir">高效利用基础设施。</strong>使用这些工具不仅可以节省时间，还可以降低基础设施的使用成本。由于BigQuery和Dataflow都是无服务器的，因此不存在未充分利用或过度利用的基础设施，也没有固定成本(当然，除了存储成本之外，存储成本在大多数情况下可以忽略不计)。考虑到工作负载的短暂性和所需的有限财政承诺，您可能会考虑使用GCP来完成这种数据工程任务，即使它不是您的默认主力。</li></ul></div></div>    
</body>
</html>