<html>
<head>
<title>The Power of Ensemble Learning and Data Augmentation (with the MNIST dataset)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">集成学习和数据扩充的力量(使用MNIST数据集)</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/the-power-of-ensemble-learning-and-data-augmentation-435c62e13c57?source=collection_archive---------30-----------------------#2021-07-01">https://towardsdatascience.com/the-power-of-ensemble-learning-and-data-augmentation-435c62e13c57?source=collection_archive---------30-----------------------#2021-07-01</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="3757" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">MNIST数据集上的完整代码示例，VGG16 | ResNet50 | FG-UNET |多数投票|单次可转移投票|即时决胜投票</h2></div><h1 id="d9db" class="kf kg iq bd kh ki kj kk kl km kn ko kp jw kq jx kr jz ks ka kt kc ku kd kv kw bi translated">目录</h1><ol class=""><li id="12be" class="kx ky iq kz b la lb lc ld le lf lg lh li lj lk ll lm ln lo bi translated"><a class="ae lp" href="#0a2f" rel="noopener ugc nofollow">简介</a>(MNIST数据集和目标)</li><li id="79b0" class="kx ky iq kz b la lq lc lr le ls lg lt li lu lk ll lm ln lo bi translated"><a class="ae lp" href="#2f65" rel="noopener ugc nofollow">合奏中包含的模特</a> <br/>一、<a class="ae lp" href="#2198" rel="noopener ugc nofollow"> VGG16 </a> <br/>二。<a class="ae lp" href="#a27e" rel="noopener ugc nofollow"> ResNet50 </a> <br/> iii。<a class="ae lp" href="#d6b5" rel="noopener ugc nofollow"> FG-UNET </a></li><li id="b36f" class="kx ky iq kz b la lq lc lr le ls lg lt li lu lk ll lm ln lo bi translated"><a class="ae lp" href="#98a9" rel="noopener ugc nofollow">数据扩充</a></li><li id="e66e" class="kx ky iq kz b la lq lc lr le ls lg lt li lu lk ll lm ln lo bi translated"><a class="ae lp" href="#0e40" rel="noopener ugc nofollow">集成学习方法</a> <br/>一、<a class="ae lp" href="#9e71" rel="noopener ugc nofollow">多数表决</a> <br/>二。<a class="ae lp" href="#081f" rel="noopener ugc nofollow">岭回归合奏</a> <br/> iii。<a class="ae lp" href="#cbd1" rel="noopener ugc nofollow">单一可转让票(STV) </a> <br/> iv。<a class="ae lp" href="#1671" rel="noopener ugc nofollow">即时决胜投票(IRV) </a></li><li id="c65d" class="kx ky iq kz b la lq lc lr le ls lg lt li lu lk ll lm ln lo bi translated"><a class="ae lp" href="#01ab" rel="noopener ugc nofollow">关键外卖</a></li></ol></div><div class="ab cl lv lw hu lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="ij ik il im in"><h1 id="0a2f" class="kf kg iq bd kh ki mc kk kl km md ko kp jw me jx kr jz mf ka kt kc mg kd kv kw bi translated">介绍</h1><p id="1ac0" class="pw-post-body-paragraph mh mi iq kz b la lb jr mj lc ld ju mk le ml mm mn lg mo mp mq li mr ms mt lk ij bi translated">作为这篇博客的第一篇文章，我决定从一些简单的东西开始——写一个我早期在Kaggle上做的关于数字识别的老项目。这个任务很简单(<strong class="kz ir">手写数字上的数字[0–9]识别</strong>)并且已经被ML社区很好地解决了，但是它是一个很好的玩具数据集来启动<a class="ae lp" href="https://www.kaggle.com/socathie" rel="noopener ugc nofollow" target="_blank">我的Kaggle投资组合</a>。</p><p id="9384" class="pw-post-body-paragraph mh mi iq kz b la mu jr mj lc mv ju mk le mw mm mn lg mx mp mq li my ms mt lk ij bi translated">要了解更多关于MNIST数据集的信息，您可以访问<a class="ae lp" href="https://www.tensorflow.org/datasets/catalog/mnist" rel="noopener ugc nofollow" target="_blank">它在TensorFlow数据集上的页面，这里是</a>。简而言之，MNIST数据集包含需要被分别分类成10个数字的手写数字的图像。</p><p id="a64f" class="pw-post-body-paragraph mh mi iq kz b la mu jr mj lc mv ju mk le mw mm mn lg mx mp mq li my ms mt lk ij bi translated">Kaggle有一个永久运行的<a class="ae lp" href="https://www.kaggle.com/c/digit-recognizer" rel="noopener ugc nofollow" target="_blank">玩具竞赛</a>，它为任何人提供了一个方便的平台，通过设置好的编程环境和模型评估来测试他们的数据科学技能。使用的度量是<strong class="kz ir">分类准确度</strong>，即正确图像预测的百分比。</p><p id="a506" class="pw-post-body-paragraph mh mi iq kz b la mu jr mj lc mv ju mk le mw mm mn lg mx mp mq li my ms mt lk ij bi translated">下面是<a class="ae lp" href="https://www.kaggle.com/c/digit-recognizer/data" rel="noopener ugc nofollow" target="_blank">数据描述</a>:</p><blockquote class="mz na nb"><p id="95ed" class="mh mi nc kz b la mu jr mj lc mv ju mk nd mw mm mn ne mx mp mq nf my ms mt lk ij bi translated">数据文件train.csv和test.csv包含手绘数字的灰度图像，从0到9。</p><p id="1e1a" class="mh mi nc kz b la mu jr mj lc mv ju mk nd mw mm mn ne mx mp mq nf my ms mt lk ij bi translated">每幅图像高28像素，宽28像素，总共784像素。每个像素都有一个与之关联的像素值，表示该像素的亮度或暗度，数字越大表示越暗。该像素值是0到255之间的整数，包括0和255。</p></blockquote><figure class="nh ni nj nk gt nl gh gi paragraph-image"><div role="button" tabindex="0" class="nm nn di no bf np"><div class="gh gi ng"><img src="../Images/d95cbf7d970455539710e2049ebbf991.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*kjDqAHZTKKH6VIpp"/></div></div><p class="ns nt gj gh gi nu nv bd b be z dk translated">手写数字仅供参考(照片由<a class="ae lp" href="https://unsplash.com/@popnzebra?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Pop &amp;斑马</a>在<a class="ae lp" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄)</p></figure><p id="d86e" class="pw-post-body-paragraph mh mi iq kz b la mu jr mj lc mv ju mk le mw mm mn lg mx mp mq li my ms mt lk ij bi translated">请注意，图像尺寸仅为28×28像素，这对于<a class="ae lp" href="https://www.tensorflow.org/api_docs/python/tf/keras/applications" rel="noopener ugc nofollow" target="_blank"> tf.keras.applications </a>中的许多模型来说太小了。在本文的其余部分，我们将假设数据已经被预处理成形状为<strong class="kz ir">(样本大小，32，32，1) </strong>的数组，方法是<a class="ae lp" href="https://numpy.org/doc/stable/reference/generated/numpy.pad.html" rel="noopener ugc nofollow" target="_blank">在图像</a>的每一侧填充2行2列零。Kaggle上的MNIST数据集有42，000个训练样本和28，000个测试样本。</p></div><div class="ab cl lv lw hu lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="ij ik il im in"><h1 id="2f65" class="kf kg iq bd kh ki mc kk kl km md ko kp jw me jx kr jz mf ka kt kc mg kd kv kw bi translated">这套服装中的模特</h1><h1 id="2198" class="kf kg iq bd kh ki kj kk kl km kn ko kp jw kq jx kr jz ks ka kt kc ku kd kv kw bi translated">一、VGG16(准确率98.80%)</h1><p id="30e5" class="pw-post-body-paragraph mh mi iq kz b la lb jr mj lc ld ju mk le ml mm mn lg mo mp mq li mr ms mt lk ij bi translated">这是在MNIST数据集上实现VGG16 (增加了数据)的完整的<a class="ae lp" href="https://www.kaggle.com/socathie/mnist-w-vgg16/" rel="noopener ugc nofollow" target="_blank"> Kaggle笔记本。</a></p><p id="883b" class="pw-post-body-paragraph mh mi iq kz b la mu jr mj lc mv ju mk le mw mm mn lg mx mp mq li my ms mt lk ij bi translated">VGG16是由<a class="ae lp" href="https://arxiv.org/abs/1409.1556" rel="noopener ugc nofollow" target="_blank"> Simonyan和Zisserman (2014) </a>作为提交给<a class="ae lp" href="https://www.image-net.org/challenges/LSVRC/2014/" rel="noopener ugc nofollow" target="_blank"> ILSVRC2014 </a>的，在<a class="ae lp" href="https://www.image-net.org/about.php" rel="noopener ugc nofollow" target="_blank"> ImageNet </a>中实现了92.7%的top-5测试准确率。数字16代表网络的层数。这个模型还有一个变种，<a class="ae lp" href="https://keras.io/api/applications/vgg/" rel="noopener ugc nofollow" target="_blank"> VGG19 </a>，它的网络改为19层。</p><p id="d2ca" class="pw-post-body-paragraph mh mi iq kz b la mu jr mj lc mv ju mk le mw mm mn lg mx mp mq li my ms mt lk ij bi translated">Tensorflow的Keras API有一个预训练的VGG16模型，它只接受224x224的输入大小。对于MNIST数据集，我们将使用Keras API创建一个输入大小为32x32的VGG16网络，并从头开始训练，如以下代码所示。</p><pre class="nh ni nj nk gt nw nx ny nz aw oa bi"><span id="cb83" class="ob kg iq nx b gy oc od l oe of">from tf.keras.applications import VGG16<br/>from tf.keras import Model<br/>from tf.keras.layers import Dense</span><span id="ccd1" class="ob kg iq nx b gy og od l oe of">vgg  = VGG16(include_top=False, weights=None, input_shape=(32,32,3), pooling="max")</span><span id="31dd" class="ob kg iq nx b gy og od l oe of">x = vgg.layers[-1].output<br/>x = Dense(10, activation='softmax', name='predictions')(x)<br/>model = Model(inputs=vgg.layers[0].output,outputs=x)</span></pre><p id="6c36" class="pw-post-body-paragraph mh mi iq kz b la mu jr mj lc mv ju mk le mw mm mn lg mx mp mq li my ms mt lk ij bi translated">10%的样品留作验证之用。该模型使用<a class="ae lp" href="https://keras.io/api/optimizers/adam/" rel="noopener ugc nofollow" target="_blank"> Adam优化器</a>对<a class="ae lp" href="https://www.tensorflow.org/api_docs/python/tf/keras/losses/CategoricalCrossentropy" rel="noopener ugc nofollow" target="_blank">分类交叉熵</a>进行100个时期的训练，使用<a class="ae lp" href="https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/EarlyStopping" rel="noopener ugc nofollow" target="_blank">提前停止</a>标准，即验证损失的耐心为10。</p><p id="ac54" class="pw-post-body-paragraph mh mi iq kz b la mu jr mj lc mv ju mk le mw mm mn lg mx mp mq li my ms mt lk ij bi translated">除了将在下面<a class="ae lp" href="#98a9" rel="noopener ugc nofollow">描述的数据扩充，VGG16在测试集上实现了98.80%的分类准确率。</a></p><h1 id="a27e" class="kf kg iq bd kh ki kj kk kl km kn ko kp jw kq jx kr jz ks ka kt kc ku kd kv kw bi translated">二。ResNet50 (99.17%的准确率)</h1><p id="ba5f" class="pw-post-body-paragraph mh mi iq kz b la lb jr mj lc ld ju mk le ml mm mn lg mo mp mq li mr ms mt lk ij bi translated">这是在MNIST数据集上实现ResNet50 (带有数据扩充)的完整<a class="ae lp" href="https://www.kaggle.com/socathie/mnist-w-resnet-and-data-augmentation/" rel="noopener ugc nofollow" target="_blank"> Kaggle笔记本。</a></p><p id="0ddd" class="pw-post-body-paragraph mh mi iq kz b la mu jr mj lc mv ju mk le mw mm mn lg mx mp mq li my ms mt lk ij bi translated">ResNet由<a class="ae lp" href="https://arxiv.org/abs/1512.03385" rel="noopener ugc nofollow" target="_blank">何等(2016) </a>提出，其一个变种在<a class="ae lp" href="https://image-net.org/challenges/LSVRC/2015/" rel="noopener ugc nofollow" target="_blank"> ILSVRC2015 </a>获得第一名。ResNet的特点是<em class="nc">跳过连接</em>，网络中跳过几层的捷径，旨在解决<a class="ae lp" rel="noopener" target="_blank" href="/the-vanishing-gradient-problem-69bf08b15484">消失梯度</a>的问题。数字50再次表示网络的层数。</p><p id="f208" class="pw-post-body-paragraph mh mi iq kz b la mu jr mj lc mv ju mk le mw mm mn lg mx mp mq li my ms mt lk ij bi translated">Tensorflow的Keras API也有一个预训练的ResNet50模型，它同样只接受224x224的输入大小。与VGG16类似，我们将使用下面的代码在MNIST数据集上从头开始训练模型。</p><pre class="nh ni nj nk gt nw nx ny nz aw oa bi"><span id="3676" class="ob kg iq nx b gy oc od l oe of">from tf.keras.applications import ResNet50</span><span id="5bc5" class="ob kg iq nx b gy og od l oe of">res  = ResNet50(include_top=False, weights=None, input_shape=(32,32,3), pooling="max")</span><span id="677c" class="ob kg iq nx b gy og od l oe of">x = res.layers[-1].output<br/>x = Dense(10, activation='softmax', name='predictions')(x)<br/>model = Model(inputs=res.layers[0].output,outputs=x)]</span></pre><p id="9116" class="pw-post-body-paragraph mh mi iq kz b la mu jr mj lc mv ju mk le mw mm mn lg mx mp mq li my ms mt lk ij bi translated">使用上述相同的训练参数，ResNet50在测试集上实现了99.17%的分类准确率。</p><h1 id="d6b5" class="kf kg iq bd kh ki kj kk kl km kn ko kp jw kq jx kr jz ks ka kt kc ku kd kv kw bi translated">三。FG-UNET (97.93%的准确率)</h1><p id="63e2" class="pw-post-body-paragraph mh mi iq kz b la lb jr mj lc ld ju mk le ml mm mn lg mo mp mq li mr ms mt lk ij bi translated">这是在MNIST数据集上实现FG-UNET的完整的<a class="ae lp" href="https://www.kaggle.com/socathie/mnist-w-fg-unet/" rel="noopener ugc nofollow" target="_blank"> Kaggle笔记本。</a></p><p id="d4ac" class="pw-post-body-paragraph mh mi iq kz b la mu jr mj lc mv ju mk le mw mm mn lg mx mp mq li my ms mt lk ij bi translated">我在为一个客户的卫星图像项目工作时遇到了FG-UNET，并发现它特别有趣。该模型基于<a class="ae lp" href="https://lmb.informatik.uni-freiburg.de/people/ronneber/u-net/#:~:text=The%20u%2Dnet%20is%20convolutional,and%20precise%20segmentation%20of%20images.&amp;text=U%2Dnet%20architecture%20(example%20for,on%20top%20of%20the%20box." rel="noopener ugc nofollow" target="_blank"> U-Net </a>架构，该架构由几个下采样和上采样路径组成，用于对生物医学图像进行逐像素分类。FG-UNET试图借用这种架构，同时通过添加完全连接的路径而不进行任何下采样或上采样来保留细粒度信息。</p><p id="ceec" class="pw-post-body-paragraph mh mi iq kz b la mu jr mj lc mv ju mk le mw mm mn lg mx mp mq li my ms mt lk ij bi translated">修改FG-UNET对MNIST数据集进行分类真的只是一个有趣的实验。特别是，我们不再需要按像素分类，因此修改了最终图层以展平2D影像并输出到10个类别的密集图层。具体的内核大小和使用的层可以在下面的代码中找到。</p><figure class="nh ni nj nk gt nl"><div class="bz fp l di"><div class="oh oi l"/></div></figure><p id="e7ec" class="pw-post-body-paragraph mh mi iq kz b la mu jr mj lc mv ju mk le mw mm mn lg mx mp mq li my ms mt lk ij bi translated">在没有数据扩充的情况下，FG-UNET在测试集上实现了97.93%的分类准确率。</p></div><div class="ab cl lv lw hu lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="ij ik il im in"><h1 id="98a9" class="kf kg iq bd kh ki mc kk kl km md ko kp jw me jx kr jz mf ka kt kc mg kd kv kw bi translated">数据扩充</h1><p id="e8f1" class="pw-post-body-paragraph mh mi iq kz b la lb jr mj lc ld ju mk le ml mm mn lg mo mp mq li mr ms mt lk ij bi translated">数据扩充是<a class="ae lp" href="https://journalofbigdata.springeropen.com/articles/10.1186/s40537-019-0197-0" rel="noopener ugc nofollow" target="_blank">一种在有限数据集下增强模型性能的成熟技术</a>。通过旋转、移动和剪切输入图像，网络将能够学习手写数字的更好的内部表示，而不是过度适应训练图像。</p><p id="0cf9" class="pw-post-body-paragraph mh mi iq kz b la mu jr mj lc mv ju mk le mw mm mn lg mx mp mq li my ms mt lk ij bi translated">Keras API有一个<a class="ae lp" href="https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/ImageDataGenerator" rel="noopener ugc nofollow" target="_blank"> ImageDataGenerator </a>，它可以方便地提供一个生成器，为模型生成增强的输入图像。</p><pre class="nh ni nj nk gt nw nx ny nz aw oa bi"><span id="5a81" class="ob kg iq nx b gy oc od l oe of">from tf.keras.preprocessing.image import ImageDataGenerator</span><span id="072c" class="ob kg iq nx b gy og od l oe of">gen = ImageDataGenerator(rotation_range=30, width_shift_range=2, height_shift_range=2, shear_range=30, validation_split=val_rate)</span></pre></div><div class="ab cl lv lw hu lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="ij ik il im in"><h1 id="0e40" class="kf kg iq bd kh ki mc kk kl km md ko kp jw me jx kr jz mf ka kt kc mg kd kv kw bi translated">集成学习方法</h1><p id="20a5" class="pw-post-body-paragraph mh mi iq kz b la lb jr mj lc ld ju mk le ml mm mn lg mo mp mq li mr ms mt lk ij bi translated">为什么要集成学习？嗯，<em class="nc">三个臭皮匠胜过一个</em>(这里是三个臭皮匠)。</p><p id="260f" class="pw-post-body-paragraph mh mi iq kz b la mu jr mj lc mv ju mk le mw mm mn lg mx mp mq li my ms mt lk ij bi translated">假设我们有上面的三个模型，它们的误差率分别为1.20%、0.83%和2.07%。他们未能正确预测的图像集可能不会完全重叠。一个模型可能难以区分1和7，而另一个模型可能难以区分3和8。如果我们能够以某种方式将所有三个模型的学习结合起来，我们应该会得到一个比这三个单独的模型更好的模型！</p><p id="9343" class="pw-post-body-paragraph mh mi iq kz b la mu jr mj lc mv ju mk le mw mm mn lg mx mp mq li my ms mt lk ij bi translated">我们如何组合这些模型？事实证明，有许多方法可以使用集成学习，我们将在下面讨论其中的几种。这里是Kaggle上的<a class="ae lp" href="https://www.kaggle.com/socathie/multiple-model-ensemble" rel="noopener ugc nofollow" target="_blank">全合奏学习笔记本</a>。</p><h1 id="9e71" class="kf kg iq bd kh ki kj kk kl km kn ko kp jw kq jx kr jz ks ka kt kc ku kd kv kw bi translated">一、多数表决(准确率99.35%)</h1><p id="ec89" class="pw-post-body-paragraph mh mi iq kz b la lb jr mj lc ld ju mk le ml mm mn lg mo mp mq li mr ms mt lk ij bi translated">最直接的合奏方法就是让模特们“投票”。如果三个模型中的两个或更多个预测了相同的数字，则它们是正确的可能性很高。</p><p id="ae42" class="pw-post-body-paragraph mh mi iq kz b la mu jr mj lc mv ju mk le mw mm mn lg mx mp mq li my ms mt lk ij bi translated">当没有多数时，多数表决的问题就出现了，特别是考虑到我们有10路分类。当没有多数时，有许多可能的解决方案，但这里我们使用一个简单的解决方案，如果没有多数，我们使用ResNet50，因为它具有最高的单一模型精度。</p><p id="9be5" class="pw-post-body-paragraph mh mi iq kz b la mu jr mj lc mv ju mk le mw mm mn lg mx mp mq li my ms mt lk ij bi translated">在28，000个测试样本中，只有842个(3.01%)没有一致的预测，需要投票。在842次“选举”中，只有37次(0.13%)没有获得多数票。这意味着该例外仅适用于极少数样本。</p><p id="dcd3" class="pw-post-body-paragraph mh mi iq kz b la mu jr mj lc mv ju mk le mw mm mn lg mx mp mq li my ms mt lk ij bi translated">通过多数投票，MNIST数据集上的分类准确率提高到99.35%，高于三个模型中的任何一个。</p><h1 id="081f" class="kf kg iq bd kh ki kj kk kl km kn ko kp jw kq jx kr jz ks ka kt kc ku kd kv kw bi translated">二。岭回归集成(99.33%的准确率)</h1><p id="5f3b" class="pw-post-body-paragraph mh mi iq kz b la lb jr mj lc ld ju mk le ml mm mn lg mo mp mq li mr ms mt lk ij bi translated">岭回归通常被称为<a class="ae lp" rel="noopener" target="_blank" href="/ridge-and-lasso-regression-a-complete-guide-with-python-scikit-learn-e20e34bcbf0b">一种正则化普通线性回归的技术</a>。但是，也可以通过将单个模型预测作为输入，将分类标签作为输出，使用岭回归来创建集成。</p><p id="7d25" class="pw-post-body-paragraph mh mi iq kz b la mu jr mj lc mv ju mk le mw mm mn lg mx mp mq li my ms mt lk ij bi translated">这三种模型的输出都是10位数的概率分布。通过连接3x10个概率，我们得到一个30维的向量<em class="nc"> X </em>。对于输出向量<em class="nc"> y </em>，我们简单地将类标签编码成<a class="ae lp" href="https://www.tensorflow.org/api_docs/python/tf/keras/utils/to_categorical" rel="noopener ugc nofollow" target="_blank">单热点向量</a>。通过对输入<em class="nc"> X </em>和输出<em class="nc"> y </em>进行岭回归拟合，回归器将预测10个类别的<em class="nc">伪</em>-概率向量(<em class="nc">伪</em>，因为该向量可能未被归一化)。</p><p id="cc02" class="pw-post-body-paragraph mh mi iq kz b la mu jr mj lc mv ju mk le mw mm mn lg mx mp mq li my ms mt lk ij bi translated">岭回归集成将MNIST数据集上的分类准确率提高到99.33%。</p><h1 id="cbd1" class="kf kg iq bd kh ki kj kk kl km kn ko kp jw kq jx kr jz ks ka kt kc ku kd kv kw bi translated">四。单一可转移投票(STV，99.36%的准确率)</h1><p id="b48d" class="pw-post-body-paragraph mh mi iq kz b la lb jr mj lc ld ju mk le ml mm mn lg mo mp mq li mr ms mt lk ij bi translated">对于接下来的两种集成方法，我们将使用一个叫做<a class="ae lp" href="https://pypi.org/project/pyrankvote/" rel="noopener ugc nofollow" target="_blank"> PyRankVote </a>的便捷模块。PyRankVote由Jon Tingvold于2019年6月创建，实施单一可转让投票(STV)、即时决胜投票(IRV)和优先集团投票(PBV)。</p><p id="3b3e" class="pw-post-body-paragraph mh mi iq kz b la mu jr mj lc mv ju mk le mw mm mn lg mx mp mq li my ms mt lk ij bi translated">在STV，每个模型按照预测概率的降序排列，为他们排序的选择投下一张“选票”。在每一轮，最不受欢迎的“候选人”被淘汰。只要有可能，投票会投给模型的首选，但是如果他们的首选被排除，那么投票会投给他们的第二、第三、第四选择，等等。该过程递归运行，直到剩下一个获胜者。</p><p id="6768" class="pw-post-body-paragraph mh mi iq kz b la mu jr mj lc mv ju mk le mw mm mn lg mx mp mq li my ms mt lk ij bi translated">使用PyRankVote，我们可以用几行代码实现STV:</p><pre class="nh ni nj nk gt nw nx ny nz aw oa bi"><span id="9a47" class="ob kg iq nx b gy oc od l oe of">import pyrankvote<br/>from pyrankvote import Candidate, Ballot</span><span id="ece9" class="ob kg iq nx b gy og od l oe of">candidates = [Candidate(i) for i <strong class="nx ir">in</strong> range(10)]<br/>ballots = []<br/>for j <strong class="nx ir">in</strong> range(3):<br/>    ballot = np.argsort(X_pred[i,:,j]) #input is the predicted probability distribution<br/>    ballot = np.flip(ballot) #flipping to descending order<br/>    <br/>    ballots.append(Ballot(ranked_candidates=[candidates[i] for i <strong class="nx ir">in</strong> ballot]))<br/>    <br/>election_result = pyrankvote.single_transferable_vote(candidates, ballots, number_of_seats=1)</span><span id="57bc" class="ob kg iq nx b gy og od l oe of">winners = election_result.get_winners()</span></pre><p id="f852" class="pw-post-body-paragraph mh mi iq kz b la mu jr mj lc mv ju mk le mw mm mn lg mx mp mq li my ms mt lk ij bi translated">STV在MNIST数据集的Kaggle测试集上取得了99.36%的分类准确率。</p><h1 id="1671" class="kf kg iq bd kh ki kj kk kl km kn ko kp jw kq jx kr jz ks ka kt kc ku kd kv kw bi translated">动词 （verb的缩写）即时决胜投票(IRV，99.35%的准确率)</h1><p id="64f7" class="pw-post-body-paragraph mh mi iq kz b la lb jr mj lc ld ju mk le ml mm mn lg mo mp mq li mr ms mt lk ij bi translated">与STV相似，每个模特也为IRV的一个排名列表投票。如果一名“候选人”在第一轮投票中获得简单多数(超过半数)，该候选人获胜。否则，最不受欢迎的“候选人”将被淘汰，而那些首先投票给最不受欢迎的候选人的人的选票将被计入他们的下一个选择。</p><p id="27f7" class="pw-post-body-paragraph mh mi iq kz b la mu jr mj lc mv ju mk le mw mm mn lg mx mp mq li my ms mt lk ij bi translated">根据我们使用<a class="ae lp" href="#9e71" rel="noopener ugc nofollow">多数投票</a>方法的经验，我们知道28，000个测试样本中只有37个没有简单多数胜出者。因此，只有这37个样本会受到IRV的影响。</p><p id="8b7d" class="pw-post-body-paragraph mh mi iq kz b la mu jr mj lc mv ju mk le mw mm mn lg mx mp mq li my ms mt lk ij bi translated">IRV获得了与多数投票法相同的99.35%的分类准确率。</p></div><div class="ab cl lv lw hu lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="ij ik il im in"><h1 id="01ab" class="kf kg iq bd kh ki mc kk kl km md ko kp jw me jx kr jz mf ka kt kc mg kd kv kw bi translated">关键外卖</h1><p id="7768" class="pw-post-body-paragraph mh mi iq kz b la lb jr mj lc ld ju mk le ml mm mn lg mo mp mq li mr ms mt lk ij bi translated">下表总结了所有型号的性能:</p><figure class="nh ni nj nk gt nl"><div class="bz fp l di"><div class="oh oi l"/></div></figure><p id="e067" class="pw-post-body-paragraph mh mi iq kz b la mu jr mj lc mv ju mk le mw mm mn lg mx mp mq li my ms mt lk ij bi translated">我们观察到集合模型性能都是相似的，范围在99.3%到99.4%之间。这篇文章的关键要点不是一种集成方法是否比另一种更好，而是所有的集成模型都可以比三个单独的模型中的任何一个达到更高的精度。</p><p id="0bec" class="pw-post-body-paragraph mh mi iq kz b la mu jr mj lc mv ju mk le mw mm mn lg mx mp mq li my ms mt lk ij bi translated">在学术研究中，由于发表的压力，我们经常专注于为特定的机器学习任务找到最佳模型，而独立的最佳表现模型的优雅更容易被期刊接受。在行业中，没有最终部署在最终产品中的模型的先前迭代通常被认为是沉没成本。如果资源允许，部署几个表现良好的模型的组合可能会比单个表现最好的模型产生更好的性能，并帮助公司更好地实现其业务目标。</p></div><div class="ab cl lv lw hu lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="ij ik il im in"><h1 id="da5f" class="kf kg iq bd kh ki mc kk kl km md ko kp jw me jx kr jz mf ka kt kc mg kd kv kw bi translated">参考</h1><p id="660f" class="pw-post-body-paragraph mh mi iq kz b la lb jr mj lc ld ju mk le ml mm mn lg mo mp mq li mr ms mt lk ij bi translated"><strong class="kz ir"> VGG16 </strong>:西蒙扬，k .，&amp;齐塞曼，A. (2014)。用于大规模图像识别的非常深的卷积网络。<em class="nc"> arXiv预印本arXiv:1409.1556 </em>。</p><p id="93fb" class="pw-post-body-paragraph mh mi iq kz b la mu jr mj lc mv ju mk le mw mm mn lg mx mp mq li my ms mt lk ij bi translated">何，王，张，徐，任，孙，孙(2016)。用于图像识别的深度残差学习。IEEE计算机视觉和模式识别会议论文集<em class="nc">(第770–778页)。</em></p><p id="c33e" class="pw-post-body-paragraph mh mi iq kz b la mu jr mj lc mv ju mk le mw mm mn lg mx mp mq li my ms mt lk ij bi translated"><strong class="kz ir"> FG-UNET </strong>:斯托伊安，a .，普莱恩，v .，英格拉达，j .，普贡，v .，&amp;德克森，D. (2019)。用高分辨率卫星图像时间序列和卷积神经网络制作土地覆盖图:业务系统的适应和限制。<em class="nc">遥感</em>，<em class="nc"> 11 </em> (17)，1986年。</p><p id="d828" class="pw-post-body-paragraph mh mi iq kz b la mu jr mj lc mv ju mk le mw mm mn lg mx mp mq li my ms mt lk ij bi translated"><strong class="kz ir">数据扩充</strong> : Shorten，c .，&amp; Khoshgoftaar，T. M. (2019)。面向深度学习的图像数据增强综述。<em class="nc">大数据杂志</em>，<em class="nc"> 6 </em> (1)，1–48。</p></div><div class="ab cl lv lw hu lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="ij ik il im in"><figure class="nh ni nj nk gt nl"><div class="bz fp l di"><div class="oj oi l"/></div></figure><p id="ca8e" class="pw-post-body-paragraph mh mi iq kz b la mu jr mj lc mv ju mk le mw mm mn lg mx mp mq li my ms mt lk ij bi translated"><em class="nc">关于作者:我是一名自由数据科学家和ML工程师。我为贵公司的数据科学和机器学习需求提供定制解决方案。更多信息请访问www.CathieSo.com</em><a class="ae lp" href="https://www.CathieSo.com" rel="noopener ugc nofollow" target="_blank"><em class="nc"/></a><em class="nc">。</em></p></div></div>    
</body>
</html>