<html>
<head>
<title>Activation functions you might have missed</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">你可能错过的激活功能</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/activation-functions-you-might-have-missed-79d72fc080a5?source=collection_archive---------8-----------------------#2021-09-10">https://towardsdatascience.com/activation-functions-you-might-have-missed-79d72fc080a5?source=collection_archive---------8-----------------------#2021-09-10</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="2340" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsdatascience.com/tagged/tips-and-tricks" rel="noopener" target="_blank">提示和技巧</a></h2><div class=""/><div class=""><h2 id="890d" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">你应该“嗖嗖”一下这些新发明，还是继续使用老掉牙的东西？</h2></div><p id="6b4a" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">如今，机器学习领域的科学进步速度是无与伦比的。很难跟上时代，除非是在一个狭窄的领域。时不时会冒出一篇新论文，声称已经取得了一些最先进的成果。这些新发明中的大多数从未成为默认的首选方法，有时是因为它们没有最初希望的那么好，但有时只是因为它们最终在新出版物的洪流中被淹没了。</p><p id="461f" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">错过一些金块是多么可惜啊！别害怕，我会保护你的。我最近浏览了一些相对较新的关于神经网络构建模块之一的论文:激活函数。让我们来看看几个最有前途的，看看他们为什么好，什么时候使用它们。但在此之前，我们将快速浏览一下常用的激活，以了解它们解决或产生了什么问题。如果您能区分PReLU和RReLU，请随意向下滚动前两个部分。</p><figure class="lo lp lq lr gt ls gh gi paragraph-image"><div class="gh gi ln"><img src="../Images/3698648e7f1f108a24db8e6a82486cad.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*vIXnyoIcpn1gTCGk.png"/></div></figure><h2 id="921f" class="lv lw it bd lx ly lz dn ma mb mc dp md la me mf mg le mh mi mj li mk ml mm iz bi translated">为什么还要激活？</h2><p id="c1f3" class="pw-post-body-paragraph kr ks it kt b ku mn kd kw kx mo kg kz la mp lc ld le mq lg lh li mr lk ll lm im bi translated">在每个神经网络的单元内，单元的输入与一些权重参数<em class="ms"> W </em>相乘，添加一个偏差<em class="ms"> b </em>，并且结果被馈送到一个函数中，称为<em class="ms">激活函数。</em>它的输出又是传递给下一层单元的单元输出。</p><figure class="lo lp lq lr gt ls gh gi paragraph-image"><div role="button" tabindex="0" class="mu mv di mw bf mx"><div class="gh gi mt"><img src="../Images/3495607a8678a747aac736ca95167952.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*g2L52bVb63WWGfIJJi96ig.png"/></div></div><p class="my mz gj gh gi na nb bd b be z dk translated">神经网络单元的内部。图片由作者提供。</p></figure><p id="6068" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">激活函数原则上可以是任何函数，只要它不是线性的。为什么？如果我们使用线性激活(包括一个身份函数，意味着根本没有激活)，我们的网络将有效地成为一个简单的线性回归模型，无论我们使用多少层和单元。这是因为线性组合的线性组合可以表示为一个线性方程。</p><figure class="lo lp lq lr gt ls gh gi paragraph-image"><div role="button" tabindex="0" class="mu mv di mw bf mx"><div class="gh gi nc"><img src="../Images/5492ee6c3e6610ff03605743a64b9a4d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5fYs-0Re8hkvLGkSzFQeyQ.png"/></div></div></figure><p id="fd5e" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">这种网络将具有有限的学习能力，因此需要引入非线性。</p><figure class="lo lp lq lr gt ls gh gi paragraph-image"><div class="gh gi ln"><img src="../Images/3698648e7f1f108a24db8e6a82486cad.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*vIXnyoIcpn1gTCGk.png"/></div></figure><h2 id="d884" class="lv lw it bd lx ly lz dn ma mb mc dp md la me mf mg le mh mi mj li mk ml mm iz bi translated">经典激活函数</h2><p id="7a93" class="pw-post-body-paragraph kr ks it kt b ku mn kd kw kx mo kg kz la mp lc ld le mq lg lh li mr lk ll lm im bi translated">让我们快速看一下五个最常用的激活函数。在这里，它们是使用<em class="ms"> numpy </em>实现的。</p><figure class="lo lp lq lr gt ls gh gi paragraph-image"><div role="button" tabindex="0" class="mu mv di mw bf mx"><div class="gh gi nd"><img src="../Images/b031bb210a992a6739e0a8f25eca7c0e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WiUaB2NvyemNSaUPSfIC2w.png"/></div></div><p class="my mz gj gh gi na nb bd b be z dk translated">经典激活函数:numpy实现。图片由作者提供。</p></figure><p id="d721" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">这是它们的样子:</p><figure class="lo lp lq lr gt ls gh gi paragraph-image"><div role="button" tabindex="0" class="mu mv di mw bf mx"><div class="gh gi ne"><img src="../Images/da71bde37c82c17e96f4504f568722d1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VwyzC0hq2BsMNeD__ZAyjA.png"/></div></div><p class="my mz gj gh gi na nb bd b be z dk translated">经典激活函数:图。图片由作者提供。</p></figure><p id="1cf4" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">让我简短地讨论一下它们。</p><p id="e37b" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">在历史上，sigmoid 或<strong class="kt jd">逻辑</strong>激活是第一个取代早期网络中阶跃函数的激活。根据科学，这大致是用于激活我们生物大脑中神经元的功能。这是一个游戏改变者，因为sigmoid的定义明确的非零导数允许使用梯度下降来训练神经网络。从那以后，sigmoid已经被网络隐藏层中的其他函数所取代，尽管它仍然被用作二元分类任务的最终预测层。</p><p id="9673" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated"><strong class="kt jd">双曲正切(tanh) </strong>在形状上与sigmoid非常相似，但是它取值在-1和1之间，而不是0和1之间。因此，它的输出更集中在零附近，这有助于加速收敛，尤其是在训练的早期。</p><p id="2506" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">然而，sigmoid和tanh都有一个共同的问题:它们都是饱和函数。当输入非常大或非常小时，斜率接近零，使得梯度消失，学习缓慢。因此需要非饱和激活。一个成功的故事属于r <strong class="kt jd">有限线性单元(ReLU) </strong>函数，它不会对正值饱和。它的计算速度很快，由于没有最大值，它防止了消失梯度问题。不过它有一个缺点，被称为<em class="ms">将死。</em>问题是ReLU对任何负值都输出零。如果网络的权重达到这样的值，当与输入相乘时，它们总是产生负值，那么整个重新激活的单元继续产生零。如果许多神经元像这样死亡，网络的学习能力就会受损。</p><p id="fa5c" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">为了缓解日益严重的ReLU问题，已经有人提议对ReLU进行一些升级。<strong class="kt jd"> Leaky ReLU </strong>对负值有一个小但非零的斜率，确保神经元不会死亡。这种激活函数的一些奇特变体包括<strong class="kt jd">随机化的泄漏ReLU (RReLU) </strong>，其中这个小斜率是在训练时随机选择的，或者<strong class="kt jd">参数化的泄漏ReLU(PReLU)</strong>，其中斜率被认为是网络的参数之一，并通过梯度下降来学习。</p><p id="4478" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">最后，<strong class="kt jd">指数线性单位(ELU) </strong>诞生了，击败了所有真正的变种。它吸收了所有世界的精华:负值的非零梯度消除了死亡神经元的问题，就像在leaky ReLU中一样，负值使输出更接近零，就像在tanh中一样，最重要的是，eLU在零附近是平滑的，这加快了收敛速度。但是它也有自己的问题:指数函数的使用使得计算相对较慢。</p><p id="5609" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">为了方便起见，以下是经典激活的概述:</p><figure class="lo lp lq lr gt ls gh gi paragraph-image"><div role="button" tabindex="0" class="mu mv di mw bf mx"><div class="gh gi nf"><img src="../Images/17393ab9c1bd934ef6f367699b7cf2e2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kepHzmGda8_cG9Ijg-jB0A.png"/></div></div><p class="my mz gj gh gi na nb bd b be z dk translated">经典激活函数的比较。由作者编译。</p></figure><p id="bb2c" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">现在让我们来看看最近的一些发明吧！</p><figure class="lo lp lq lr gt ls gh gi paragraph-image"><div class="gh gi ln"><img src="../Images/3698648e7f1f108a24db8e6a82486cad.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*vIXnyoIcpn1gTCGk.png"/></div></figure><h2 id="a487" class="lv lw it bd lx ly lz dn ma mb mc dp md la me mf mg le mh mi mj li mk ml mm iz bi translated">缩放的ELU (SELU)</h2><p id="a087" class="pw-post-body-paragraph kr ks it kt b ku mn kd kw kx mo kg kz la mp lc ld le mq lg lh li mr lk ll lm im bi translated">Klambauer等人在<a class="ae ng" href="https://arxiv.org/abs/1706.02515" rel="noopener ugc nofollow" target="_blank"> 2017年的一篇论文中介绍了<strong class="kt jd">比例ELU </strong>或<strong class="kt jd"> SELU </strong>激活。顾名思义，它是ELU的缩放版本，在下面的公式中选择了两个缩放常数，例如在TensorFlow和Pytorch实现中。</a></p><figure class="lo lp lq lr gt ls gh gi paragraph-image"><div role="button" tabindex="0" class="mu mv di mw bf mx"><div class="gh gi nh"><img src="../Images/9f2f135765848f423e2a8eeff1d04261.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZM8fwN05CFHOEUMsA3lU8Q.png"/></div></div></figure><figure class="lo lp lq lr gt ls gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/b37a8756605281c78a3c928e152e1e25.png" data-original-src="https://miro.medium.com/v2/resize:fit:838/format:webp/1*ZYtRGf_oGYaKiyGYmvnZ8w.png"/></div></figure><p id="e861" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">SELU函数有一个奇特的性质。该论文的作者表明，如果正确初始化，如果所有隐藏层都是SELU激活的，密集的前馈网络将会自我归一化。这意味着每个图层的输出将大致具有等于零的平均值和等于一的标准差，这有助于防止消失或爆炸梯度问题，并允许构建深度网络。该论文在UCI机器学习知识库、药物发现基准甚至天文学任务的120多个任务上评估了这种自规范化网络，发现它们明显优于传统的前馈网络。</p><figure class="lo lp lq lr gt ls gh gi paragraph-image"><div class="gh gi ln"><img src="../Images/3698648e7f1f108a24db8e6a82486cad.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*vIXnyoIcpn1gTCGk.png"/></div></figure><h2 id="7308" class="lv lw it bd lx ly lz dn ma mb mc dp md la me mf mg le mh mi mj li mk ml mm iz bi translated">高斯误差线性单位</h2><p id="4e13" class="pw-post-body-paragraph kr ks it kt b ku mn kd kw kx mo kg kz la mp lc ld le mq lg lh li mr lk ll lm im bi translated">Hendrycks &amp; Gimpel 在<a class="ae ng" href="https://arxiv.org/abs/1606.08415" rel="noopener ugc nofollow" target="_blank">2016年的一篇论文中提出了<strong class="kt jd">高斯误差线性单元</strong>，或<strong class="kt jd"> GELU，</strong>。该函数只是将其输入与该输入的正态分布累积密度函数相乘。由于这种计算非常慢，所以在实践中经常使用一种更快的近似值，这种近似值只有第四位小数不同。</a></p><figure class="lo lp lq lr gt ls gh gi paragraph-image"><div role="button" tabindex="0" class="mu mv di mw bf mx"><div class="gh gi nj"><img src="../Images/1fa94ba142beaaaee3d503abc1481026.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*spKSUI47qVH1txSmp1S3OQ.png"/></div></div></figure><figure class="lo lp lq lr gt ls gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/b46e1aea25cf30bce152cb496b37c88a.png" data-original-src="https://miro.medium.com/v2/resize:fit:844/format:webp/1*fYerzjJvlU0lDKx6V0RZtg.png"/></div></figure><p id="75b2" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">与ReLU系列的激活相反，GELU根据输入值对其进行加权，而不是根据符号对其进行阈值处理。作者针对ReLU和ELU函数评估了GELU激活，发现所有考虑的计算机视觉、自然语言处理和语音任务的性能都有所提高。</p><figure class="lo lp lq lr gt ls gh gi paragraph-image"><div class="gh gi ln"><img src="../Images/3698648e7f1f108a24db8e6a82486cad.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*vIXnyoIcpn1gTCGk.png"/></div></figure><h2 id="04d5" class="lv lw it bd lx ly lz dn ma mb mc dp md la me mf mg le mh mi mj li mk ml mm iz bi translated">嗖嗖</h2><p id="490e" class="pw-post-body-paragraph kr ks it kt b ku mn kd kw kx mo kg kz la mp lc ld le mq lg lh li mr lk ll lm im bi translated">由<a class="ae ng" href="https://arxiv.org/abs/1710.05941v1?source=post_page---------------------------" rel="noopener ugc nofollow" target="_blank"> Ramachandran等人</a>于2017年在谷歌大脑发明的<strong class="kt jd"> Swish </strong>激活功能非常简单:它只是将输入乘以自己的sigmoid。它在形状上非常类似于GELU函数。</p><figure class="lo lp lq lr gt ls gh gi paragraph-image"><div class="gh gi nl"><img src="../Images/0bff7f2f23b6244d9e731602a3e5e2eb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1084/format:webp/1*_PJSe6NhW_mjEkSkys9NTg.png"/></div></figure><figure class="lo lp lq lr gt ls gh gi paragraph-image"><div class="gh gi nm"><img src="../Images/93f2a2c7e278df67ebe49abe21c6785d.png" data-original-src="https://miro.medium.com/v2/resize:fit:842/format:webp/1*8esUdqiRw8jZSaZH00_e0g.png"/></div></figure><p id="eed6" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">该论文的作者注意到，尽管已经提出了许多其他激活，ReLU仍然是最广泛采用的，主要是因为使用这些新奇事物的收益不一致。因此，他们评估Swish的方法是，在针对ReLU进行优化的网络架构中，简单地将它作为ReLU的替代。他们发现了显著的性能提升，并建议使用Swish作为ReLU的替代方案。</p><p id="93b6" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">Swish的论文还包含了一个有趣的讨论，关于是什么让激活函数变好的。作者指出，上无界、下有界、非单调和平滑是Swish如此出色的原因。你可能已经注意到GELU也有所有这些属性，我们稍后将讨论的最后一个激活也是如此。看起来这是激活研究的方向。</p><figure class="lo lp lq lr gt ls gh gi paragraph-image"><div class="gh gi ln"><img src="../Images/3698648e7f1f108a24db8e6a82486cad.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*vIXnyoIcpn1gTCGk.png"/></div></figure><h2 id="4521" class="lv lw it bd lx ly lz dn ma mb mc dp md la me mf mg le mh mi mj li mk ml mm iz bi translated">米什</h2><p id="33ea" class="pw-post-body-paragraph kr ks it kt b ku mn kd kw kx mo kg kz la mp lc ld le mq lg lh li mr lk ll lm im bi translated">激活是迄今为止讨论过的发明中最新的一个。它是由Misra在<a class="ae ng" href="https://arxiv.org/abs/1908.08681" rel="noopener ugc nofollow" target="_blank">2019年的一篇论文</a>中提出的。米什受到了Swish的启发，并已被证明在各种计算机视觉任务中胜过它。</p><figure class="lo lp lq lr gt ls gh gi paragraph-image"><div role="button" tabindex="0" class="mu mv di mw bf mx"><div class="gh gi nn"><img src="../Images/93c3e13f0738c45d33c17bf3a0748fff.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*spYZHhtp1wI3kbT2yQ-b0Q.png"/></div></div></figure><figure class="lo lp lq lr gt ls gh gi paragraph-image"><div class="gh gi no"><img src="../Images/d399315f870bd4d5a8a2100203723957.png" data-original-src="https://miro.medium.com/v2/resize:fit:850/format:webp/1*rV12R9JwWiTXYo752-iymA.png"/></div></figure><p id="25d2" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">引用原始论文，Mish是“通过对使Swish如此有效的特征进行系统分析和实验而发现的”。Mish似乎是库存中最好的激活，但请记住，原始论文只在计算机视觉任务上测试了它。</p><figure class="lo lp lq lr gt ls gh gi paragraph-image"><div class="gh gi ln"><img src="../Images/3698648e7f1f108a24db8e6a82486cad.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*vIXnyoIcpn1gTCGk.png"/></div></figure><h2 id="46e2" class="lv lw it bd lx ly lz dn ma mb mc dp md la me mf mg le mh mi mj li mk ml mm iz bi translated">使用哪个激活？</h2><p id="580f" class="pw-post-body-paragraph kr ks it kt b ku mn kd kw kx mo kg kz la mp lc ld le mq lg lh li mr lk ll lm im bi translated">在他的精彩著作《用Scikit-Learn和TensorFlow实践机器学习》中，Geron陈述了以下一般规则:</p><blockquote class="np"><p id="0858" class="nq nr it bd ns nt nu nv nw nx ny lm dk translated">SELU &gt; ELU &gt;泄漏的ReLU &gt; ReLU</p></blockquote><p id="80a2" class="pw-post-body-paragraph kr ks it kt b ku nz kd kw kx oa kg kz la ob lc ld le oc lg lh li od lk ll lm im bi translated">但是有一些问题。如果网络的架构阻止它自我正常化，那么ELU可能是比SELU更好的选择。其次，如果速度是重要的，(漏)ReLU将是一个比缓慢的eLU更好的选择。但是，这本书没有讨论最近提出的激活。</p><p id="89f2" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">有一次，我和我的同事，一位前谷歌员工讨论我当时正在做的一个网络架构。他给我的第一条建议是用Swishes代替ReLUs。这并没有改变游戏规则，但尽管如此，性能还是提高了。</p><p id="108c" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">基于这一点和我的其他经验，我建议在选择激活时使用以下主观决策树，假设架构的其余部分是固定的。</p><figure class="lo lp lq lr gt ls gh gi paragraph-image"><div role="button" tabindex="0" class="mu mv di mw bf mx"><div class="gh gi oe"><img src="../Images/3a552646591dab6405fd87e5bfce7cbe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vIbn82CJN9rBi_xN0N1Q6Q.png"/></div></div><p class="my mz gj gh gi na nb bd b be z dk translated">如何选择激活，作者。</p></figure><figure class="lo lp lq lr gt ls gh gi paragraph-image"><div class="gh gi ln"><img src="../Images/3698648e7f1f108a24db8e6a82486cad.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*vIXnyoIcpn1gTCGk.png"/></div></figure><h2 id="fb63" class="lv lw it bd lx ly lz dn ma mb mc dp md la me mf mg le mh mi mj li mk ml mm iz bi translated">来源</h2><ul class=""><li id="4941" class="of og it kt b ku mn kx mo la oh le oi li oj lm ok ol om on bi translated">Geron A .，2019，第二版，使用Scikit-Learn和TensorFlow进行机器学习:构建智能系统的概念、工具和技术</li><li id="dc0a" class="of og it kt b ku oo kx op la oq le or li os lm ok ol om on bi translated"><a class="ae ng" href="https://deepdrive.pl" rel="noopener ugc nofollow" target="_blank"> deepdrive.pl </a>的图像分类教程(波兰语)</li><li id="3123" class="of og it kt b ku oo kx op la oq le or li os lm ok ol om on bi translated"><a class="ae ng" href="https://arxiv.org/abs/1606.08415" rel="noopener ugc nofollow" target="_blank"> 2016，丹·亨德里克斯&amp;凯文·金佩尔，高斯误差线性单位(GELUs) </a></li><li id="ca97" class="of og it kt b ku oo kx op la oq le or li os lm ok ol om on bi translated"><a class="ae ng" href="https://arxiv.org/abs/1710.05941v1?source=post_page---------------------------" rel="noopener ugc nofollow" target="_blank"> 2017，Prajit Ramachandran，Barret Zoph，Quoc V. Le，Swish:一个自门控激活函数</a></li><li id="fdaa" class="of og it kt b ku oo kx op la oq le or li os lm ok ol om on bi translated"><a class="ae ng" href="https://arxiv.org/abs/1706.02515" rel="noopener ugc nofollow" target="_blank"> 2017，君特·克兰鲍尔，托马斯·安特辛纳，安德里亚斯·迈尔，塞普·霍克雷特，自归一化神经网络</a></li><li id="f746" class="of og it kt b ku oo kx op la oq le or li os lm ok ol om on bi translated"><a class="ae ng" href="https://arxiv.org/abs/1908.08681" rel="noopener ugc nofollow" target="_blank"> 2019，迪甘塔·米斯拉，米什:一个自正则化的非单调激活函数</a></li></ul><figure class="lo lp lq lr gt ls gh gi paragraph-image"><div class="gh gi ln"><img src="../Images/3698648e7f1f108a24db8e6a82486cad.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*vIXnyoIcpn1gTCGk.png"/></div></figure><p id="220a" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">感谢阅读！</p><p id="4d86" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">如果你喜欢这篇文章，为什么不订阅电子邮件更新我的新文章呢？并且通过<a class="ae ng" href="https://michaloleszak.medium.com/membership" rel="noopener"> <strong class="kt jd">成为媒介会员</strong> </a>，可以支持我的写作，获得其他作者和我自己的所有故事的无限访问权限。</p><p id="5b7f" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">需要咨询？你可以问我任何事情，也可以在这里 为我预约1:1 <a class="ae ng" href="http://hiretheauthor.com/michal" rel="noopener ugc nofollow" target="_blank"> <strong class="kt jd">。</strong></a></p><p id="0f9c" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">你也可以试试我的其他文章。不能选择？从这些中选择一个:</p><div class="ot ou gp gr ov ow"><a rel="noopener follow" target="_blank" href="/monte-carlo-dropout-7fd52f8b6571"><div class="ox ab fo"><div class="oy ab oz cl cj pa"><h2 class="bd jd gy z fp pb fr fs pc fu fw jc bi translated">蒙特卡洛辍学</h2><div class="pd l"><h3 class="bd b gy z fp pb fr fs pc fu fw dk translated">用一个小技巧免费改善你的神经网络，获得模型不确定性估计作为奖励。</h3></div><div class="pe l"><p class="bd b dl z fp pb fr fs pc fu fw dk translated">towardsdatascience.com</p></div></div><div class="pf l"><div class="pg l ph pi pj pf pk lt ow"/></div></div></a></div><div class="ot ou gp gr ov ow"><a rel="noopener follow" target="_blank" href="/6-useful-probability-distributions-with-applications-to-data-science-problems-2c0bee7cef28"><div class="ox ab fo"><div class="oy ab oz cl cj pa"><h2 class="bd jd gy z fp pb fr fs pc fu fw jc bi translated">6有用的概率分布及其在数据科学问题中的应用</h2><div class="pd l"><h3 class="bd b gy z fp pb fr fs pc fu fw dk translated">带有示例和Python代码的实用概述。</h3></div><div class="pe l"><p class="bd b dl z fp pb fr fs pc fu fw dk translated">towardsdatascience.com</p></div></div><div class="pf l"><div class="pl l ph pi pj pf pk lt ow"/></div></div></a></div><div class="ot ou gp gr ov ow"><a rel="noopener follow" target="_blank" href="/the-learning-rate-finder-6618dfcb2025"><div class="ox ab fo"><div class="oy ab oz cl cj pa"><h2 class="bd jd gy z fp pb fr fs pc fu fw jc bi translated">学习率查找器</h2><div class="pd l"><h3 class="bd b gy z fp pb fr fs pc fu fw dk translated">无需昂贵的搜索即可快速到达最佳值的邻域。</h3></div><div class="pe l"><p class="bd b dl z fp pb fr fs pc fu fw dk translated">towardsdatascience.com</p></div></div><div class="pf l"><div class="pm l ph pi pj pf pk lt ow"/></div></div></a></div></div></div>    
</body>
</html>