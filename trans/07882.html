<html>
<head>
<title>Fine-Tuning “LaBSE” for a Sentiment Classification Task</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">针对情感分类任务微调“LaBSE”</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/fine-tuning-labse-for-a-sentiment-classification-task-56e34b74e655?source=collection_archive---------27-----------------------#2021-07-19">https://towardsdatascience.com/fine-tuning-labse-for-a-sentiment-classification-task-56e34b74e655?source=collection_archive---------27-----------------------#2021-07-19</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi jn"><img src="../Images/23a5feca6992ed0277d8406be2506b99.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*KTWhnSGvunTeisvt"/></div></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">照片由<a class="ae kc" href="https://unsplash.com/@denisseleon?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">丹尼斯·莱昂</a>在<a class="ae kc" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><p id="3133" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir">一些背景</strong></p><p id="0232" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">多语言语言模型(让我们称之为“MLM ”)由于其提供多语言单词嵌入(或句子、文档等)的能力，近来已经成为NLP领域的趋势。)在单个模型内。“预训练”是与MLMs一起出现的另一个术语，它告诉我们，模型已经在不同领域的大型语料库上训练过，因此我们不必从头开始再次训练它们，但我们可以针对所需的目标任务“微调”它们，同时利用来自预训练知识的“知识转移(转移学习)”。传销主要由谷歌、脸书、百度等科技巨头发布供公众使用，因为他们有资源来训练这些具有数百万、数十亿甚至数万亿参数的大型模型。LaBSE[1]就是Google发布的这样一个模型，基于BERT模型。</p><p id="ee06" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">LaBSE或“语言不可知的BERT句子嵌入”专注于双文本挖掘、句子/嵌入相似性任务。它使用“单词块”标记化，可以为109种语言生成句子嵌入([CLS]标记从模型的最终层嵌入表示句子嵌入)。虽然，他们还没有报道该模型在其他下游任务如分类或命名实体识别(NER)中的性能，也没有太多用于这类下游任务。LaBSE的架构是一个“双编码器”模型(带附加余量Softmax的双向双编码器)，这意味着它有两个基于“BERT-base”模型编码器的编码器模块。这两个编码器分别对源句子和目标句子进行编码，并提供给一个评分函数(余弦相似度)来对它们的相似度进行排序。LaBSE的训练损失函数就是建立在这个评分的基础上的，这个评分就是前面提到的“加性边际Softmax”。</p><p id="de73" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir">设置事物</strong></p><p id="770e" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">(<em class="lb">我会尽量保持简单，同时包括重要的观点</em>)LaBSE的官方或原始模型由作者发布到“tensor flow Hub”(<a class="ae kc" href="https://www.tensorflow.org/hub/" rel="noopener ugc nofollow" target="_blank">https://www.tensorflow.org/hub/</a>)，我将使用它。该模块依赖于Tensorflow (2.4.0+将是伟大的，我正在使用2.5.0)有其他必要的库，他们可以使用pip安装。</p><blockquote class="lc ld le"><p id="d266" class="kd ke lb kf b kg kh ki kj kk kl km kn lf kp kq kr lg kt ku kv lh kx ky kz la ij bi translated"><strong class="kf ir"> <em class="iq">注意:</em> </strong> <em class="iq">到目前为止(据我所知)Conda环境不支持tfhub模型+ GPU。如果您尝试使用这样的设置，它将总是(自动)退回到Tensorflow的CPU版本或抛出错误。因此，如果使用GPU，Conda应该不在考虑范围内。(</em><a class="ae kc" href="https://github.com/tensorflow/text/issues/644" rel="noopener ugc nofollow" target="_blank">)https://github.com/tensorflow/text/issues/644</a></p></blockquote><p id="0716" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">首先，需要安装一些必要的库，(我用的是Ubuntu机器)</p><pre class="li lj lk ll gt lm ln lo lp aw lq bi"><span id="977a" class="lr ls iq ln b gy lt lu l lv lw">!pip install tensorflow-hub</span><span id="14df" class="lr ls iq ln b gy lx lu l lv lw">!pip install tensorflow-text # Needed for loading universal-sentence-encoder-cmlm/multilingual-preprocess</span><span id="1301" class="lr ls iq ln b gy lx lu l lv lw">!pip install tf-models-official</span></pre><p id="c399" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们可以进口它们，</p><pre class="li lj lk ll gt lm ln lo lp aw lq bi"><span id="f6f0" class="lr ls iq ln b gy lt lu l lv lw">import tensorflow as tf</span><span id="81c0" class="lr ls iq ln b gy lx lu l lv lw">import tensorflow_hub as hub</span><span id="b753" class="lr ls iq ln b gy lx lu l lv lw">import tensorflow_text as text </span><span id="060c" class="lr ls iq ln b gy lx lu l lv lw">from official.nlp import optimization </span></pre><p id="ac50" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">显然，如果需要的话，你也应该导入其他的公共库(numpy，pandas，sklearn ),我不会在这里提到。</p><p id="3ccb" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">对于分类任务，我们可以使用预先训练的109种语言的任何标记数据集(或者也可以是不支持的，这没有关系！但是有一些性能下降)。我们要做的是对模型进行微调，即使用额外的数据集(与庞大的预训练数据集或语料库相比，较小的数据集)训练预训练模型，以便将模型微调到我们特定的分类(或任何其他)任务。作为起点，我们可以使用IMDb电影评论数据集。(<a class="ae kc" href="https://ai.stanford.edu/~amaas/data/sentiment/" rel="noopener ugc nofollow" target="_blank">https://ai.stanford.edu/~amaas/data/sentiment/</a>)。因此，我们的任务将变成“二元情感分类”任务。数据集由25k训练和25k测试数据组成。</p><pre class="li lj lk ll gt lm ln lo lp aw lq bi"><span id="a74f" class="lr ls iq ln b gy lt lu l lv lw">!wget -c <a class="ae kc" href="https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz" rel="noopener ugc nofollow" target="_blank">https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz</a> -O — | tar -xz</span><span id="9a06" class="lr ls iq ln b gy lx lu l lv lw">AUTOTUNE = tf.data.AUTOTUNE<br/>batch_size = 32 #8 #16<br/>seed = 42</span><span id="25c1" class="lr ls iq ln b gy lx lu l lv lw">raw_train_ds = tf.keras.preprocessing.text_dataset_from_directory(<br/> ‘aclImdb/train’,<br/> batch_size=batch_size,<br/> validation_split=0.2,<br/> subset=’training’,<br/> seed=seed)</span><span id="b7be" class="lr ls iq ln b gy lx lu l lv lw">class_names = raw_train_ds.class_names<br/>train_ds = raw_train_ds.cache().prefetch(buffer_size=AUTOTUNE)</span><span id="6719" class="lr ls iq ln b gy lx lu l lv lw">val_ds = tf.keras.preprocessing.text_dataset_from_directory(<br/> ‘aclImdb/train’,<br/> batch_size=batch_size,<br/> validation_split=0.2,<br/> subset=’validation’,<br/> seed=seed)</span><span id="768f" class="lr ls iq ln b gy lx lu l lv lw">val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)</span><span id="c8e0" class="lr ls iq ln b gy lx lu l lv lw">test_ds = tf.keras.preprocessing.text_dataset_from_directory(<br/> ‘aclImdb/test’,<br/> batch_size=batch_size)</span><span id="b6de" class="lr ls iq ln b gy lx lu l lv lw">test_ds = test_ds.cache().prefetch(buffer_size=AUTOTUNE)</span></pre><p id="f6a9" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们可以使用Tensorflow构建的数据管道作为输入数据。(<em class="lb">如果使用TF的早期版本，该功能可能无法完全或至少不能直接使用</em>。).接下来，我们需要“预处理”这些数据，然后输入到我们将要构建的模型中。之后，预处理的数据可以被编码或嵌入向量空间。为此，我们可以定义以下变量，我们将使用LaBSE的版本2(<a class="ae kc" href="https://tfhub.dev/google/LaBSE/2" rel="noopener ugc nofollow" target="_blank">https://tfhub.dev/google/LaBSE/2</a>，版本1是发布到TFhub的初始模型)</p><pre class="li lj lk ll gt lm ln lo lp aw lq bi"><span id="0f9b" class="lr ls iq ln b gy lt lu l lv lw">tfhub_handle_preprocess=”<a class="ae kc" href="https://tfhub.dev/google/universal-sentence-encoder-cmlm/multilingual-preprocess/2" rel="noopener ugc nofollow" target="_blank">https://tfhub.dev/google/universal-sentence-encoder-cmlm/multilingual-preprocess/2</a>"<br/>tfhub_handle_encoder=”<a class="ae kc" href="https://tfhub.dev/google/LaBSE/2" rel="noopener ugc nofollow" target="_blank">https://tfhub.dev/google/LaBSE/2</a>"</span></pre><figure class="li lj lk ll gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi ly"><img src="../Images/9c82a6afaba6f71a63d2ba5235e134f1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*23qZM7xCArY0P2UF"/></div></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">照片由<a class="ae kc" href="https://unsplash.com/@steve_j?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">斯蒂夫·约翰森</a>在<a class="ae kc" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><p id="1c4a" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir">建立模型</strong></p><p id="bd32" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">接下来，我们可以建立模型。下面，已经定义了一个函数来建立具有一些特定层的模型。</p><pre class="li lj lk ll gt lm ln lo lp aw lq bi"><span id="e069" class="lr ls iq ln b gy lt lu l lv lw">def build_classifier_model():<br/> text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name=’text’)<br/> preprocessing_layer = hub.KerasLayer(tfhub_handle_preprocess, name=’preprocessing’)<br/> encoder_inputs = preprocessing_layer(text_input)<br/> encoder = hub.KerasLayer(tfhub_handle_encoder, trainable=True, name=’LaBSE_encoder’)<br/> outputs = encoder(encoder_inputs)<br/> net = outputs[‘pooled_output’]<br/> net = tf.keras.layers.Dropout(0.1)(net)<br/> net = tf.keras.layers.Dense(1, name=’classifier’)(net)<br/> return tf.keras.Model(text_input, net)</span></pre><p id="4c8b" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">你可以注意到模型中有一个术语“pooled_outputs ”,它指的是本文前面提到的句子的[CLS]令牌表示。(另一种输出形式是‘sequence _ outputs’)。编码器层上的“trainable = True”参数或标志意味着，在不使用数据集进行微调的同时，我们也可以更新原始模型的权重/参数的权重。(这也被称为“全局微调”)。</p><pre class="li lj lk ll gt lm ln lo lp aw lq bi"><span id="299b" class="lr ls iq ln b gy lt lu l lv lw">encoder = hub.KerasLayer(tfhub_handle_encoder, trainable=True, name=’LaBSE_encoder’)<br/><br/>net = outputs[‘pooled_output’]</span></pre><p id="806c" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如果我们希望保持原始模型的权重不变，那么它将被称为“基于特征的微调”或“固定维度方法”(在文献中有不同的术语)。此外，在充当模型的分类器层的编码器(下降和密集)之后添加了一些附加层。这是在文献中经常发现的这种分类的组合。</p><p id="4094" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">对于优化器，Adam或AdamW(更多)是首选，我们可以像下面这样设置。基于数据集或任务，学习率可能会改变(在大多数情况下会降低)。Keras(<a class="ae kc" href="https://keras.io/guides/keras_tuner/" rel="noopener ugc nofollow" target="_blank">https://keras.io/guides/keras_tuner/</a>)或类似“Wandb”(<a class="ae kc" href="https://wandb.ai/site" rel="noopener ugc nofollow" target="_blank">https://wandb.ai/site</a>)的服务中已经提供的超参数优化方法也可用于寻找最佳参数，如学习速率和批量大小。</p><pre class="li lj lk ll gt lm ln lo lp aw lq bi"><span id="9afe" class="lr ls iq ln b gy lt lu l lv lw">from tensorflow_addons.optimizers import AdamW<br/>step = tf.Variable(0, trainable=False)<br/>schedule = tf.optimizers.schedules.PiecewiseConstantDecay(<br/> [1000], [5e-5,1e-5])<br/>lr = 1 * schedule(step)<br/>wd = lambda: 1e-6 * schedule(step)<br/>optimizer=AdamW(learning_rate=lr,weight_decay=wd)</span></pre><p id="aece" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">接下来，可以用mode.fit()方法对模型进行编译和训练。</p><pre class="li lj lk ll gt lm ln lo lp aw lq bi"><span id="aa86" class="lr ls iq ln b gy lt lu l lv lw">classifier_model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),<br/> optimizer=optimizer,<br/> metrics=tf.keras.metrics.BinaryAccuracy(threshold=0.0))</span><span id="9f47" class="lr ls iq ln b gy lx lu l lv lw">history = classifier_model.fit(train_ds,validation_data=val_ds,<br/>                               epochs=epochs, batch_size=32)</span></pre><p id="1101" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">时期的数量可以设置为3、4或5，这通常是足够的。我们也可以包括像“提前停止”这样的方法。对这样的大型模型使用“K-Fold交叉验证”并不常见。相反，我们可以对输入数据选择使用不同的随机种子多次运行这个模型。从模型中构建、运行和获得结果是相当容易的。经过训练的模型可以被保存并用于预测新的数据等。如张量流模型通常所做的那样。</p><p id="3712" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><em class="lb">在我看来，</em>与XLM-R、<em class="lb">或</em>等模型相比，LaBSE模型在文本分类等任务上可能表现不佳，因为LaBSE最初是为双文本挖掘或句子相似性任务而构建和训练的，为了获得更好的结果，可能需要更多的训练数据(微调数据)。LaBSE在文献中也没有太多用于分类任务(根据我的知识和论文本身对谷歌学术的38次引用<em class="lb">)</em>。对于这里的任务，我获得了略高于50%的准确度、精确度、召回率和f1分数。这也是用一些随机选择的超参数完成的，因此如果超参数也被改变/调整，结果可能会得到改善。(使用LaBSE版本1(<a class="ae kc" href="https://medium.com/swlh/language-agnostic-text-classification-with-labse-51a4f55dab77" rel="noopener">https://medium . com/swlh/language-agnostic-text-classification-with-LaBSE-51 a4 f 55 dab 77</a>)进行了一些类似的工作，但是使用了更大的训练数据集。)</p><p id="06c9" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">无论如何，我希望听到反馈，评论或其他人的经验。所以。请随意评论您的想法和建议。感谢阅读！！！</p><div class="lz ma gp gr mb mc"><a href="https://github.com/VinuraD" rel="noopener  ugc nofollow" target="_blank"><div class="md ab fo"><div class="me ab mf cl cj mg"><h2 class="bd ir gy z fp mh fr fs mi fu fw ip bi translated">VinuraD -概述</h2><div class="mj l"><h3 class="bd b gy z fp mh fr fs mi fu fw dk translated">阻止或报告基于Python的web应用程序，用于查看SDWAN网络隧道Python 1 2一个基于GoLang的程序，用于解析…</h3></div><div class="mk l"><p class="bd b dl z fp mh fr fs mi fu fw dk translated">github.com</p></div></div><div class="ml l"><div class="mm l mn mo mp ml mq jw mc"/></div></div></a></div></div><div class="ab cl mr ms hu mt" role="separator"><span class="mu bw bk mv mw mx"/><span class="mu bw bk mv mw mx"/><span class="mu bw bk mv mw"/></div><div class="ij ik il im in"><p id="0dc3" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir">参考文献</strong></p><p id="6bbd" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">[1] —语言不可知的伯特语句嵌入，冯，杨，丹尼尔Cer，纳文阿里瓦扎甘，，<a class="ae kc" href="https://arxiv.org/abs/2007.01852v1" rel="noopener ugc nofollow" target="_blank">arXiv:2007.01852 v1</a><strong class="kf ir">【cs .CL] </strong></p></div></div>    
</body>
</html>