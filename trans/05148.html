<html>
<head>
<title>How fast is reading Parquet file (with Arrow) vs. CSV with Pandas?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">熊猫读拼花文件(带箭头)vs. CSV多快？</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/how-fast-is-reading-parquet-file-with-arrow-vs-csv-with-pandas-2f8095722e94?source=collection_archive---------4-----------------------#2021-05-06">https://towardsdatascience.com/how-fast-is-reading-parquet-file-with-arrow-vs-csv-with-pandas-2f8095722e94?source=collection_archive---------4-----------------------#2021-05-06</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="78cc" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsdatascience.com/tagged/making-sense-of-big-data" rel="noopener" target="_blank">理解大数据</a></h2><div class=""/><div class=""><h2 id="a99e" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">重点研究使用PyArrow读取拼花文件与使用Pandas读取相同CSV文件的速度比较</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/bcb6f131f2945222f762610bb9bdc067.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WWGYMf5RXCV6tjsoozok2w.jpeg"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">图片来源:<a class="ae lh" href="https://pixabay.com/illustrations/motorcycle-speed-helmet-1690452/" rel="noopener ugc nofollow" target="_blank"> Pixabay </a></p></figure><h1 id="0b5b" class="li lj it bd lk ll lm ln lo lp lq lr ls ki lt kj lu kl lv km lw ko lx kp ly lz bi translated">为什么用拼花地板代替CSV？</h1><blockquote class="ma mb mc"><p id="a674" class="md me mf mg b mh mi kd mj mk ml kg mm mn mo mp mq mr ms mt mu mv mw mx my mz im bi translated"><strong class="mg jd">因为你可能希望读取大数据文件的速度比熊猫的内置函数快50倍！</strong></p></blockquote><p id="62f2" class="pw-post-body-paragraph md me it mg b mh mi kd mj mk ml kg mm na mo mp mq nb ms mt mu nc mw mx my mz im bi translated">逗号分隔值(CSV)是一种广泛用于数据分析的平面文件格式。它操作简单，在中小型数据环境中运行良好。然而，当您处理较大文件的数据时(也许还要为基于云的存储付费)，有一些很好的理由转向使用<a class="ae lh" href="https://www.stitchdata.com/columnardatabase/" rel="noopener ugc nofollow" target="_blank"> <strong class="mg jd">列数据存储原则</strong> </a>的文件格式。</p><p id="ceb4" class="pw-post-body-paragraph md me it mg b mh mi kd mj mk ml kg mm na mo mp mq nb ms mt mu nc mw mx my mz im bi translated"><a class="ae lh" href="https://parquet.apache.org/" rel="noopener ugc nofollow" target="_blank">阿帕奇拼花地板</a>是这些类型中最受欢迎的一种。下面的文章讨论了其中的一些优势(相对于使用传统的基于行的格式，例如平面CSV文件)。</p><div class="nd ne gp gr nf ng"><a href="https://blog.openbridge.com/how-to-be-a-hero-with-powerful-parquet-google-and-amazon-f2ae0f35ee04" rel="noopener  ugc nofollow" target="_blank"><div class="nh ab fo"><div class="ni ab nj cl cj nk"><h2 class="bd jd gy z fp nl fr fs nm fu fw jc bi translated">Apache Parquet:如何成为开源列数据格式的英雄</h2><div class="nn l"><h3 class="bd b gy z fp nl fr fs nm fu fw dk translated">Google BigQuery、Azure Data Lakes、Amazon Athena和Redshift Spectrum的Apache Parquet文件格式。</h3></div><div class="no l"><p class="bd b dl z fp nl fr fs nm fu fw dk translated">blog.openbridge.com</p></div></div><div class="np l"><div class="nq l nr ns nt np nu lb ng"/></div></div></a></div><p id="adb7" class="pw-post-body-paragraph md me it mg b mh mi kd mj mk ml kg mm na mo mp mq nb ms mt mu nc mw mx my mz im bi translated">简言之，</p><ul class=""><li id="1b46" class="nv nw it mg b mh mi mk ml na nx nb ny nc nz mz oa ob oc od bi translated">由于是面向列的，Parquet将所有高效的存储特性(例如，块、行组、列块)都带到了表中</li><li id="0135" class="nv nw it mg b mh oe mk of na og nb oh nc oi mz oa ob oc od bi translated">Apache Parquet是使用Google的分解和组装算法从头开始构建的</li><li id="73ac" class="nv nw it mg b mh oe mk of na og nb oh nc oi mz oa ob oc od bi translated">Parquet文件的设计考虑到了复杂的嵌套数据结构。</li><li id="e506" class="nv nw it mg b mh oe mk of na og nb oh nc oi mz oa ob oc od bi translated">Apache Parquet支持非常高效的压缩和编码方案。可以使用各种巧妙的方法来压缩一个parquet文件，例如(a)字典编码，(b)位打包，(c)游程编码。</li></ul><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi oj"><img src="../Images/d1101c2e66a853a1541abcddd56632dd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OSsUkX25F784zZVd8RLlgg.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated"><strong class="bd ok">图片来源</strong>:作者出品(自有版权)</p></figure><p id="2947" class="pw-post-body-paragraph md me it mg b mh mi kd mj mk ml kg mm na mo mp mq nb ms mt mu nc mw mx my mz im bi translated">当您必须从磁盘或云存储中存储和读取大型数据文件时，Parquet文件是一个很好的选择。对于用Python进行数据分析，我们都广泛使用熊猫。在本文中，我们将展示在读取大文件的内容时，与使用CSV文件和Pandas 相比，<strong class="mg jd">使用Parquet文件和Apache Arrow会给您带来令人印象深刻的速度优势。特别是，我们将讨论，</strong></p><ul class=""><li id="77b2" class="nv nw it mg b mh mi mk ml na nx nb ny nc nz mz oa ob oc od bi translated">文件大小</li><li id="cd61" class="nv nw it mg b mh oe mk of na og nb oh nc oi mz oa ob oc od bi translated">正在读取的列数</li><li id="d564" class="nv nw it mg b mh oe mk of na og nb oh nc oi mz oa ob oc od bi translated">文件的稀疏性(缺少值)</li></ul><h1 id="a3b6" class="li lj it bd lk ll lm ln lo lp lq lr ls ki lt kj lu kl lv km lw ko lx kp ly lz bi translated">阅读用箭头</h1><p id="9d1b" class="pw-post-body-paragraph md me it mg b mh ol kd mj mk om kg mm na on mp mq nb oo mt mu nc op mx my mz im bi translated">PyArrow是Apache Arrow框架的Python绑定(API)。<a class="ae lh" href="https://arrow.apache.org/" rel="noopener ugc nofollow" target="_blank">根据他们的网站</a>——<em class="mf">Apache Arrow是</em> <strong class="mg jd"> <em class="mf">内存分析</em> </strong> <em class="mf">的开发平台。它包含一系列技术，使大数据系统能够快速处理和移动数据。它为平面和分层数据指定了一种标准化的</em> <strong class="mg jd"> <em class="mf">语言无关的列内存格式</em> </strong> <em class="mf">，为现代硬件上的高效分析操作进行了组织</em></p><p id="0ac4" class="pw-post-body-paragraph md me it mg b mh mi kd mj mk ml kg mm na mo mp mq nb ms mt mu nc mw mx my mz im bi translated">这些特性使得Apache arrow成为增长最快的分布式内存数据分析框架之一。首先，它是一个理想的内存传输层，用于读取或写入Parquet文件中的数据。</p><p id="f3d6" class="pw-post-body-paragraph md me it mg b mh mi kd mj mk ml kg mm na mo mp mq nb ms mt mu nc mw mx my mz im bi translated">对Parquet文件使用PyArrow可以在读取大型数据文件的速度方面带来令人印象深刻的速度优势，并且一旦读取，内存中的对象可以很容易地转换为常规的Pandas数据帧。</p><p id="7118" class="pw-post-body-paragraph md me it mg b mh mi kd mj mk ml kg mm na mo mp mq nb ms mt mu nc mw mx my mz im bi translated">要了解更多关于PyArrow的完整特性，请参考Apache文档<a class="ae lh" href="https://arrow.apache.org/docs/python/" rel="noopener ugc nofollow" target="_blank">。</a></p><h1 id="4705" class="li lj it bd lk ll lm ln lo lp lq lr ls ki lt kj lu kl lv km lw ko lx kp ly lz bi translated">py arrow/拼花组合有多快？</h1><p id="ee2b" class="pw-post-body-paragraph md me it mg b mh ol kd mj mk om kg mm na on mp mq nb oo mt mu nc op mx my mz im bi translated">这篇文章的代码是<a class="ae lh" href="https://github.com/tirthajyoti/Machine-Learning-with-Python/blob/master/Pandas%20and%20Numpy/Read_data_various_sources/Pandas%20CSV%20vs.%20PyArrow%20parquet%20reading%20speed.ipynb" rel="noopener ugc nofollow" target="_blank"> <strong class="mg jd">在我的Github repo </strong> </a>这里。</p><h2 id="2b7d" class="oq lj it bd lk or os dn lo ot ou dp ls na ov ow lu nb ox oy lw nc oz pa ly iz bi translated">各种尺寸的CSV和拼花文件</h2><p id="3dca" class="pw-post-body-paragraph md me it mg b mh ol kd mj mk om kg mm na on mp mq nb oo mt mu nc op mx my mz im bi translated">首先，我们创建各种填充了随机生成的浮点数的CSV文件。我们还将它们转换成压缩的拼花文件。所有文件都有100列，但行数不同，因此文件大小也不同。</p><p id="c363" class="pw-post-body-paragraph md me it mg b mh mi kd mj mk ml kg mm na mo mp mq nb ms mt mu nc mw mx my mz im bi translated">完成此过程后，目录可能看起来像这样。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pb"><img src="../Images/3f35ca2f0240c36852d6bc26bf57b166.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*C731MXo94GZkOuZSASnEdg.png"/></div></div></figure><blockquote class="pc"><p id="933d" class="pd pe it bd pf pg ph pi pj pk pl mz dk translated">在大型数据文件的读取速度方面，对Parquet文件使用PyArrow可以带来令人印象深刻的速度优势</p></blockquote><h2 id="b2b8" class="oq lj it bd lk or pm dn lo ot pn dp ls na po ow lu nb pp oy lw nc pq pa ly iz bi translated">熊猫CSV vs. Arrow拼花阅读速度</h2><p id="c91b" class="pw-post-body-paragraph md me it mg b mh ol kd mj mk om kg mm na on mp mq nb oo mt mu nc op mx my mz im bi translated">现在，我们可以编写两小段代码来使用Pandas <code class="fe pr ps pt pu b">read_csv</code>和PyArrow的<code class="fe pr ps pt pu b">read_table</code>函数读取这些文件。我们还监控读取文件所需的时间，并以比率的形式对它们进行比较。结果如下所示，</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pv"><img src="../Images/1b0c7deb4a3373c24806a7de213c58dd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kR86kQM1Cz-iToEqS2ZbWg.png"/></div></div></figure><p id="4c40" class="pw-post-body-paragraph md me it mg b mh mi kd mj mk ml kg mm na mo mp mq nb ms mt mu nc mw mx my mz im bi translated">尽管趋势有所起伏，但很明显，PyArrow/Parquet组合在较大的文件中表现出色，即随着文件大小的增长，以Parquet格式存储数据并使用PyArrow读取会更有利/更快。</p><p id="0fb0" class="pw-post-body-paragraph md me it mg b mh mi kd mj mk ml kg mm na mo mp mq nb ms mt mu nc mw mx my mz im bi translated">对于一个100 MB的文件，该比率增长到大于10，即<strong class="mg jd">读取速度比</strong>快10倍！对于GB大小的文件，优势可能更大。</p><h2 id="d089" class="oq lj it bd lk or os dn lo ot ou dp ls na ov ow lu nb ox oy lw nc oz pa ly iz bi translated">使用Arrow读取少量列要快得多</h2><p id="1346" class="pw-post-body-paragraph md me it mg b mh ol kd mj mk om kg mm na on mp mq nb oo mt mu nc op mx my mz im bi translated">接下来，我们展示更酷的东西。通常，我们可能不需要从列存储文件中读取所有的列。例如，我们可以对数据应用一些过滤器，并只选择选定的数据进行实际的内存处理。</p><p id="2b4f" class="pw-post-body-paragraph md me it mg b mh mi kd mj mk ml kg mm na mo mp mq nb ms mt mu nc mw mx my mz im bi translated">现在，对于CSV文件或常规SQL数据库，这意味着我们从所有数据中选择特定的行。然而，对于列数据库，这实际上意味着选择特定的列。</p><p id="37fd" class="pw-post-body-paragraph md me it mg b mh mi kd mj mk ml kg mm na mo mp mq nb ms mt mu nc mw mx my mz im bi translated">让我们看看，当我们只从Parquet文件中读取一小部分列时，我们是否在读取速度方面增加了优势。这是我们分析的结果，</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pw"><img src="../Images/4f276359c36bd5768c41bcbfb4f6819e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*L8cuTmLwKmgRBttnGG6MAg.png"/></div></div></figure><p id="bee6" class="pw-post-body-paragraph md me it mg b mh mi kd mj mk ml kg mm na mo mp mq nb ms mt mu nc mw mx my mz im bi translated">当我们读取很小一部分列时，比如说&lt; 10 out of 100, the reading speed ratio becomes as large as &gt; 50，即<strong class="mg jd">，与常规的Pandas CSV文件读取</strong>相比，我们获得了50倍的加速。对于大部分列，加速逐渐减弱，并稳定在一个稳定的值。</p><blockquote class="pc"><p id="318c" class="pd pe it bd pf pg px py pz qa qb mz dk translated">当我们只从Parquet文件中读取一小部分列时，我们在读取速度方面增加了优势</p></blockquote><h2 id="6882" class="oq lj it bd lk or pm dn lo ot pn dp ls na po ow lu nb pp oy lw nc pq pa ly iz bi translated">PyArrow(拼花)读取速度随文件的稀疏性而变化</h2><p id="9623" class="pw-post-body-paragraph md me it mg b mh ol kd mj mk om kg mm na on mp mq nb oo mt mu nc op mx my mz im bi translated">接下来，我们看看稀疏性对Parquet文件读取速度的影响。在许多情况下，数据可能非常稀疏，即没有记录任何值。这在传感器驱动的数据分析中很常见，例如，各种传感器以不同的频率和间隔记录数据，并且数据矩阵的大部分被NaN值填充。</p><p id="a00a" class="pw-post-body-paragraph md me it mg b mh mi kd mj mk ml kg mm na mo mp mq nb ms mt mu nc mw mx my mz im bi translated">在我们的分析中，我们人为地将Numpy NaN值注入到一个固定大小的文件中，以Parquet格式保存它们，并使用PyArrow读取它们。结果如下所示。显然，<strong class="mg jd">稀疏文件被PyArrow读取的速度要比密集数据文件</strong>快得多。根据我们可能遇到的数据类型，这种行为可以为我们所用。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi qc"><img src="../Images/bbb3a4e136f10de743a12e61ad18f486.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YyV_ShovjuV5tc-3N59M3w.png"/></div></div></figure><h1 id="71ef" class="li lj it bd lk ll lm ln lo lp lq lr ls ki lt kj lu kl lv km lw ko lx kp ly lz bi translated">摘要</h1><p id="20ca" class="pw-post-body-paragraph md me it mg b mh ol kd mj mk om kg mm na on mp mq nb oo mt mu nc op mx my mz im bi translated">在本文中，我们重点分析了Pandas/CSV和Apache Arrow/Parquet组合的阅读速度。</p><p id="5856" class="pw-post-body-paragraph md me it mg b mh mi kd mj mk ml kg mm na mo mp mq nb ms mt mu nc mw mx my mz im bi translated">我们展示了Apache Arrow如何在读取速度上比Pandas CSV有显著优势，以及这如何随着数据集的大小而变化。</p><p id="d1b8" class="pw-post-body-paragraph md me it mg b mh mi kd mj mk ml kg mm na mo mp mq nb ms mt mu nc mw mx my mz im bi translated">我们还展示了对于这种类型的面向列的文件格式，读取一小部分列本来就更快。</p><p id="eab4" class="pw-post-body-paragraph md me it mg b mh mi kd mj mk ml kg mm na mo mp mq nb ms mt mu nc mw mx my mz im bi translated">最后，我们还通过Apache箭头展示了稀疏性对读取速度的影响。越稀疏的文件读取速度越快。</p></div><div class="ab cl qd qe hx qf" role="separator"><span class="qg bw bk qh qi qj"/><span class="qg bw bk qh qi qj"/><span class="qg bw bk qh qi"/></div><div class="im in io ip iq"><p id="8e1f" class="pw-post-body-paragraph md me it mg b mh mi kd mj mk ml kg mm na mo mp mq nb ms mt mu nc mw mx my mz im bi translated"><em class="mf">喜欢这篇文章吗？成为</em> <a class="ae lh" href="https://medium.com/@tirthajyoti/membership" rel="noopener"> <strong class="mg jd"> <em class="mf">中等会员</em> </strong> </a> <em class="mf">继续</em> <strong class="mg jd"> <em class="mf">无限制学习</em> </strong> <em class="mf">。如果你使用下面的链接，</em> <strong class="mg jd"> <em class="mf">，我会收到你的一部分会员费，而不会对你产生额外的费用</em> </strong> <em class="mf">。</em></p><div class="nd ne gp gr nf ng"><a href="https://medium.com/@tirthajyoti/membership" rel="noopener follow" target="_blank"><div class="nh ab fo"><div class="ni ab nj cl cj nk"><h2 class="bd jd gy z fp nl fr fs nm fu fw jc bi translated">通过我的推荐链接加入Medium—Tirthajyoti Sarkar</h2><div class="nn l"><h3 class="bd b gy z fp nl fr fs nm fu fw dk translated">作为一个媒体会员，你的会员费的一部分会给你阅读的作家，你可以完全接触到每一个故事…</h3></div><div class="no l"><p class="bd b dl z fp nl fr fs nm fu fw dk translated">medium.com</p></div></div><div class="np l"><div class="qk l nr ns nt np nu lb ng"/></div></div></a></div></div></div>    
</body>
</html>