<html>
<head>
<title>“Chain-linking” NLP tasks With Wav2Vec2 &amp; Transformers</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Wav2Vec2和变压器的“链式链接”NLP任务</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/chain-linking-nlp-tasks-with-wav2vec2-transformers-7297181db3a7?source=collection_archive---------19-----------------------#2021-03-19">https://towardsdatascience.com/chain-linking-nlp-tasks-with-wav2vec2-transformers-7297181db3a7?source=collection_archive---------19-----------------------#2021-03-19</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="e20f" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">从音频直接进入一系列基于文本的自然语言处理任务，如翻译、摘要和情感分析</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/4f37995dd1d9dd9fad873ccf76cd9783.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xq91N5qzjsLIS9m921E6nw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图表:蔡钦汉</p></figure><p id="bc60" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">拥抱脸的变形金刚库中增加了<a class="ae lr" href="https://huggingface.co/transformers/model_doc/wav2vec2.html" rel="noopener ugc nofollow" target="_blank"> Wav2Vec2 </a>模型，这是近几个月来NLP中更令人兴奋的发展之一。在那之前，如果你只有一个很长的音频片段，执行机器翻译或情感分析等任务并不容易。</p><p id="eaf5" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">但现在你可以一气呵成地将NLP任务的有趣组合联系起来:用Wav2Vec2转录音频片段，然后使用各种transformer模型来总结或翻译转录本。你能联系起来的NLP任务的可能排列和组合是相当令人难以置信的。</p><p id="64c7" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">当然，结果可能是不完整的。一些NLP任务，比如总结，本质上是很难解决的。</p><p id="e9cd" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">这篇文章是我之前的<a class="ae lr" rel="noopener" target="_blank" href="/transcribing-poetry-and-speeches-with-wav2vec2-2658c6510f06">关于Wav2Vec2试验</a>的后续，将概述2个试验:</p><ul class=""><li id="9d0b" class="ls lt iq kx b ky kz lb lc le lu li lv lm lw lq lx ly lz ma bi translated"><strong class="kx ir"> #1:语音转文本转翻译&amp;情感分析</strong></li><li id="edb8" class="ls lt iq kx b ky mb lb mc le md li me lm mf lq lx ly lz ma bi translated"><strong class="kx ir"> #2:语音转文本转摘要</strong></li></ul></div><div class="ab cl mg mh hu mi" role="separator"><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml"/></div><div class="ij ik il im in"><h1 id="9e8b" class="mn mo iq bd mp mq mr ms mt mu mv mw mx jw my jx mz jz na ka nb kc nc kd nd ne bi translated">回购、要求和参考</h1><p id="5369" class="pw-post-body-paragraph kv kw iq kx b ky nf jr la lb ng ju ld le nh lg lh li ni lk ll lm nj lo lp lq ij bi translated">试用所需的笔记本和音频文件在我的<a class="ae lr" href="https://github.com/chuachinhon/wav2vec2_transformers" rel="noopener ugc nofollow" target="_blank"> repo </a>里。此外，您还需要这些来运行笔记本电脑:</p><ul class=""><li id="72a9" class="ls lt iq kx b ky kz lb lc le lu li lv lm lw lq lx ly lz ma bi translated"><a class="ae lr" href="https://pypi.org/project/transformers/" rel="noopener ugc nofollow" target="_blank">变压器≥ 4.3 </a></li><li id="797f" class="ls lt iq kx b ky mb lb mc le md li me lm mf lq lx ly lz ma bi translated"><a class="ae lr" href="https://pypi.org/project/librosa/" rel="noopener ugc nofollow" target="_blank"> Librosa </a>(管理音频文件)</li></ul><p id="f9e5" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">通过Github上<a class="ae lr" href="https://github.com/huggingface/transformers/issues/10366" rel="noopener ugc nofollow" target="_blank">拥抱脸的机器学习工程师弗拉达利·吉克</a>的帖子，大多数笔记本中的代码已经更新，以使用更好的方法来转录长音频文件。</p><p id="6a5d" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在我早期的工作中，我使用<a class="ae lr" href="https://www.audacityteam.org/" rel="noopener ugc nofollow" target="_blank"> Audacity </a>手动将长音频剪辑分割成更小、更易管理的片段(长度超过90秒的音频剪辑往往会导致本地机器和Colab崩溃)。弗拉达利的代码通过使用Librosa将长音频剪辑转换成较短的固定“块”来消除这一步骤。</p><p id="7bdb" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在Hugging Face的model hub上有几个版本的Wav2Vec2模型。在这篇文章中，我将使用<a class="ae lr" href="https://huggingface.co/facebook/wav2vec2-large-960h-lv60-self" rel="noopener ugc nofollow" target="_blank">wav2 vec 2-large-960h-lv60-self</a>型号。</p></div><div class="ab cl mg mh hu mi" role="separator"><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml"/></div><div class="ij ik il im in"><h1 id="cdc1" class="mn mo iq bd mp mq mr ms mt mu mv mw mx jw my jx mz jz na ka nb kc nc kd nd ne bi translated">试验#1:转录+翻译+情感分析</h1><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nk"><img src="../Images/1fcd8586947c2976f6866550e1b87406.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-Mdy5zA3Iq_aVt8JwcPnuA.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">通过<a class="ae lr" href="https://www.youtube.com/watch?v=sZBquj94CGU" rel="noopener ugc nofollow" target="_blank">美国消费者新闻与商业频道电视台的YouTube </a>频道在右边截屏</p></figure><p id="4b31" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">对于这次试验，我选择了美国总统乔·拜登在2021年3月11/12日(取决于你在哪个时区)的<a class="ae lr" href="https://www.youtube.com/watch?v=JYBatFW-BP4" rel="noopener ugc nofollow" target="_blank">第一次黄金时间演讲</a>。</p><p id="f477" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">他的演讲大约有24分钟长，我通过25秒的组块将它传输到Wav2Vec2。你可以选择更长或更短的块。我发现25可以给出不错的结果。详细情况在我的回购中的<a class="ae lr" href="https://github.com/chuachinhon/wav2vec2_transformers/blob/main/notebooks/3.0_transcribe_translate_sentiment_analysis.ipynb" rel="noopener ugc nofollow" target="_blank"> notebook3.0 </a>里，这里不再赘述。</p><p id="fe79" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">演讲很复杂，众所周知，拜登一直在与口吃作斗争。剧本(直接下载<a class="ae lr" href="https://www.dropbox.com/s/aybqx2jex5cg202/biden_alt.txt?dl=0" rel="noopener ugc nofollow" target="_blank">这里</a>)的某些部分相当粗糙，尤其是接近结尾的部分:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nl"><img src="../Images/3f6d715380a42769f4a65cc591dd479f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6rO2ckeQksFReFfSZBLSLw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">点击此处下载原始Wav2Vec2抄本<a class="ae lr" href="https://www.dropbox.com/s/aybqx2jex5cg202/biden_alt.txt?dl=0" rel="noopener ugc nofollow" target="_blank"/></p></figure><p id="73d1" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">公平地说，在我看来，拜登说得非常清楚，这是Wav2Vec2模式陷入困境的一个例子。但是，在我2015年末的iMac上，这个模型只花了12分钟左右就生成了副本，这比我手动完成要快得多。</p><p id="a922" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">接下来，我想把拜登的演讲文本输入机器翻译模型，看看中文翻译的质量如何。我用了<a class="ae lr" href="https://huggingface.co/transformers/model_doc/marian.html" rel="noopener ugc nofollow" target="_blank">拥抱脸的MarianMT的实现</a>来做这个，因为我已经在之前的几次演讲中试过了<a class="ae lr" href="https://github.com/chuachinhon/practical_nlp/blob/master/notebooks/4.0_english_to_chinese_translate.ipynb" rel="noopener ugc nofollow" target="_blank">。</a></p><p id="cf82" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">不幸的是，结果几乎无法使用。点击此处下载完整的中文翻译<a class="ae lr" href="https://www.dropbox.com/s/lricsuk8xw3ry8d/biden_translated_alt.txt?dl=0" rel="noopener ugc nofollow" target="_blank">:</a></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nm"><img src="../Images/96336acdaab303869820e16dbc8cd415.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RvXEtp-ZNTzvLiG5YxTCEQ.png"/></div></div></figure><p id="8be5" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">从技术角度来看，“链式链接”过程工作得很好，在Wav2Vec2过程之后，您只需要大约10行额外的代码。</p><p id="d702" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">但是，当原始抄本中有问题的部分在没有彻底清理丢失或错误的单词以及正确的标点符号的情况下被输入到MarianMT模型中时，翻译的质量明显受到影响。我添加了句号(“.”)出现在Wav2Vec2中每段25秒的文字记录的末尾，但这显然没有捕捉到原始演讲中每个句子的正确开头和结尾。</p><p id="e2c0" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">因此，虽然看起来“链式链接”NLP任务可以节省大量时间，但一个领域中的问题可能会复合，导致更下游的任务的质量更低。</p><p id="677d" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">显然，清理原始的英文文本需要额外的时间。但我相信这样做会大大提高中文翻译的质量。至于准确的标点，目前在Wav2Vec2内还没有快捷的方法。但我怀疑这种模式的未来版本会解决这个问题，因为市场上的一些自动语音识别服务已经具备了“添加标点符号”的功能。</p><p id="4c83" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">接下来，我想尝试将情感分析应用到拜登的演讲中。语音到文本与情感分析的结合在政治或商业环境中非常有用，在这些环境中，快速了解说话者的情感可以影响某些决策。</p><p id="965d" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">原始文字被转换成一个简单的数据框架，然后我使用拥抱脸的变形金刚管道和Plotly生成一个“情绪结构”图，如下图所示。我已经试验这些情绪图表有一段时间了。我早期的实验可以在这里找到。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/4f37995dd1d9dd9fad873ccf76cd9783.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xq91N5qzjsLIS9m921E6nw.png"/></div></div></figure><p id="d674" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">这比尝试中文翻译要好得多，尽管我确信情感分析也将受益于彻底的清理和在原始英文抄本中包含适当的标点符号。</p><p id="cb84" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">从技术上来说，没有理由为什么一个人会只完成三项任务。您可以很容易地再编写几行代码，并将汇总作为连续的第四个NLP任务。</p><p id="da5b" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">但是，包括演讲在内的长文本文档的自动摘要仍然是一项极具挑战性的任务，即使对于transformer模型来说也是如此。在我看来，这一结果还不成熟。让我们在下一次试验中仔细观察一下。</p><h1 id="f30a" class="mn mo iq bd mp mq nn ms mt mu no mw mx jw np jx mz jz nq ka nb kc nr kd nd ne bi translated">试验#2:转录+总结</h1><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ns"><img src="../Images/fa0bc6de062c4c3bdd73f0c258626b12.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bB4tr6pMlTiZ_xzXCjn72w.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">通过新加坡总理办公室的<a class="ae lr" href="https://www.youtube.com/watch?v=4bUl9R2N90A" rel="noopener ugc nofollow" target="_blank"> YouTube频道截图</a>。</p></figure><p id="5fc4" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">为此，我选择了一个较短的音频剪辑，一个4分钟的视频，内容是新加坡总理李显龙在2019年的一次商业会议上回答一个关于民粹主义的问题。该剪辑集中在一个单一的问题，但有足够的复杂性，它是任何自动总结模型的挑战。</p><p id="ec95" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">详见我的回购中的<a class="ae lr" href="https://github.com/chuachinhon/wav2vec2_transformers/blob/main/notebooks/3.1_transcribe_summarise.ipynb" rel="noopener ugc nofollow" target="_blank">记事本3.1 </a>。</p><p id="c9d2" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">Wav2Vec2输出非常出色，如下图所示(或者在这里下载<a class="ae lr" href="https://www.dropbox.com/s/y1xp27ktd41js1o/pm_populism.txt?dl=0" rel="noopener ugc nofollow" target="_blank"/>)。这里和那里有一些小失误，但没有什么是你不能很快清理干净的。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nt"><img src="../Images/ce7a6797aaa63e10cd9d89493e988bc6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*H-rGzylpu7URa5GRw7w2UQ.png"/></div></div></figure><p id="9d38" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我运行了两个变压器模型的原始抄本，FB的“<a class="ae lr" href="https://huggingface.co/facebook/bart-large-cnn" rel="noopener ugc nofollow" target="_blank"> bart-large-cnn </a>”和谷歌的“<a class="ae lr" href="https://huggingface.co/google/pegasus-large" rel="noopener ugc nofollow" target="_blank"> pegasus-large </a>”。</p><p id="f063" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">这是来自Bart的结果:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nu"><img src="../Images/808da0623b3b48d45418be4bdfa29fa8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*08Y3kmNv7gcplR2vvJxJCA.png"/></div></div></figure><p id="5ba0" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">这是飞马座的总结:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nu"><img src="../Images/826841ebb00c5efd64029880fbb0545e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UrtslYGMvZon16GXilAjwQ.png"/></div></div></figure><p id="3f08" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">两个模型都很好地捕捉到了演讲者对民粹主义的宽泛定义，随后加入了他对新加坡如何试图避免这一问题的阐述。</p><p id="06e8" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">但FB-Bart模型没有捕捉到李先生评论第二部分的任何细节。与此同时，飞马模型从他评论的第一部分抓住了太多，而对后半部分抓得不够。在我看来，这两个版本都算不上一个好的总结，尽管公平地说，这两个模型都没有受过政治演讲的训练。</p><p id="c395" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">因此，我们再次看到，通过Wav2Vec2和transformers链接NLP任务在技术上是可行的，但结果并不总是令人满意。</p></div><div class="ab cl mg mh hu mi" role="separator"><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml"/></div><div class="ij ik il im in"><h1 id="1fc3" class="mn mo iq bd mp mq mr ms mt mu mv mw mx jw my jx mz jz na ka nb kc nc kd nd ne bi translated">结论</h1><p id="4ff8" class="pw-post-body-paragraph kv kw iq kx b ky nf jr la lb ng ju ld le nh lg lh li ni lk ll lm nj lo lp lq ij bi translated">对于在NLP工作的人来说，这是令人兴奋的时刻，Wav2Vec2看起来将开辟一个全新的可能性范围。随着拥抱脸<a class="ae lr" href="https://twitter.com/huggingface/status/1372180102977568770" rel="noopener ugc nofollow" target="_blank">发起冲刺，将Wav2Vec2扩展到其他语言</a>(英语之外)，NLP任务的“链式链接”范围只会越来越大。</p><p id="a3c9" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">但是考虑到Wav2Vec2现有的限制和许多NLP任务(如总结)中固有的困难，在这个过程中添加一个“暂停”按钮可能更明智。</p><p id="fe6c" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我的意思是，如果原始的Wav2Vec2转录物在被送到下游的其他NLP任务之前被清除，结果会更好。否则，成绩单上有问题的地方会变得复杂，导致其他地方的成绩低于标准。</p><p id="a49d" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">和往常一样，如果你在这篇文章或我之前的文章中发现了错误，请联系我:</p><ul class=""><li id="6429" class="ls lt iq kx b ky kz lb lc le lu li lv lm lw lq lx ly lz ma bi translated">推特:<a class="ae lr" href="https://medium.com/u/b3d8090c0aee?source=post_page-----aad7f2e1d0a0----------------------" rel="noopener">蔡锦鸿</a></li><li id="eb6c" class="ls lt iq kx b ky mb lb mc le md li me lm mf lq lx ly lz ma bi translated">领英:【www.linkedin.com/in/chuachinhon T2】</li></ul><p id="0e2c" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">这个帖子的回购，包含图表的数据和笔记本，可以在<a class="ae lr" href="https://github.com/chuachinhon/wav2vec2_transformers" rel="noopener ugc nofollow" target="_blank">这里</a>找到。</p></div></div>    
</body>
</html>