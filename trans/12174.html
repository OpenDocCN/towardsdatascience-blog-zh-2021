<html>
<head>
<title>Paper explained: A Simple Framework for Contrastive Learning of Visual Representations</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">论文解释:视觉表征对比学习的简单框架</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/paper-explained-a-simple-framework-for-contrastive-learning-of-visual-representations-6a2a63bfa703?source=collection_archive---------4-----------------------#2021-12-09">https://towardsdatascience.com/paper-explained-a-simple-framework-for-contrastive-learning-of-visual-representations-6a2a63bfa703?source=collection_archive---------4-----------------------#2021-12-09</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="d71c" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">查看 SimCLR 论文中提出的观点</h2></div><p id="70c4" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在这个故事中，我们将看看 SimCLR:在视觉任务的自我监督预训练中，将计算机视觉研究社区引向新高度的架构。</p><p id="5eb3" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">SimCLR 在 2020 年来自 Google Research 的陈等人的论文<a class="ae le" href="https://arxiv.org/pdf/2002.05709.pdf" rel="noopener ugc nofollow" target="_blank"> <em class="lf">【视觉表征对比学习的简单框架】</em></a>中提出。本文中的思想相对简单和直观，但也有一个新颖的损失函数，这是实现自我监督预训练的良好性能的关键。我尽量让文章简单，这样即使没有什么先验知识的读者也能理解。事不宜迟，我们开始吧！</p><figure class="lh li lj lk gt ll gh gi paragraph-image"><div role="button" tabindex="0" class="lm ln di lo bf lp"><div class="gh gi lg"><img src="../Images/d925cfe4987d6d4004a1ffe94238eae0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NjdVYtL4C2HmV1r22XIweg.png"/></div></div><p class="ls lt gj gh gi lu lv bd b be z dk translated">SimCLR 训练程序的说明。来源:<a class="ae le" href="https://github.com/google-research/simclr" rel="noopener ugc nofollow" target="_blank">【1】</a></p></figure><h1 id="e3a1" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">先决条件:计算机视觉的自我监督预培训</h1><p id="bd9e" class="pw-post-body-paragraph ki kj it kk b kl mo ju kn ko mp jx kq kr mq kt ku kv mr kx ky kz ms lb lc ld im bi translated">在我们深入研究 SimCLR 白皮书之前，有必要快速回顾一下自我监督的预培训到底是怎么回事。如果你一直在阅读我的其他自我监督学习的故事，或者你熟悉自我监督预培训，请随意跳过这一部分。</p><p id="4248" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">传统上，计算机视觉模型总是使用<strong class="kk iu">监督学习</strong>来训练。这意味着人类看着这些图像，并为它们创建了各种各样的标签，这样模型就可以学习这些标签的模式。例如，人类注释者可以为图像分配一个类标签，或者在图像中的对象周围绘制边界框。但是，任何接触过标注任务的人都知道，创建足够的训练数据集的工作量很大。</p><p id="b217" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">相比之下，<strong class="kk iu">自我监督学习不需要任何人为创造的标签</strong>。顾名思义，<strong class="kk iu">模特学会自我监督</strong>。在计算机视觉中，对这种自我监督进行建模的最常见方式是获取图像的不同裁剪或对其应用不同的增强，并通过模型传递修改后的输入。尽管图像包含相同的视觉信息，但看起来并不相同，<strong class="kk iu">我们让模型知道这些图像仍然包含相同的视觉信息</strong>，即相同的对象。<strong class="kk iu">这导致模型学习相同对象的相似潜在表示(输出向量)。</strong></p><p id="e98e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们可以稍后在这个预训练的模型上应用迁移学习。通常，这些模型然后在 10%的带有标签的数据上进行训练，以执行下游任务，如对象检测和语义分割。</p><h1 id="6c3e" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">用 SimCLR 学习图像相似度</h1><p id="2f14" class="pw-post-body-paragraph ki kj it kk b kl mo ju kn ko mp jx kq kr mq kt ku kv mr kx ky kz ms lb lc ld im bi translated">这篇论文的一个重要贡献是使用了<strong class="kk iu">数据扩充</strong>。SimCLR 创建成对的图像来学习相似性。如果我们两次输入相同的图像，就不会有学习效果。因此，每对图像都是通过对图像应用<strong class="kk iu">放大或变换来创建的。</strong></p><figure class="lh li lj lk gt ll gh gi paragraph-image"><div role="button" tabindex="0" class="lm ln di lo bf lp"><div class="gh gi mt"><img src="../Images/938c96c3c7e21763546d1217d71af5f3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Um_2B7Nh4Pq-bykxqfGKJQ.png"/></div></div><p class="ls lt gj gh gi lu lv bd b be z dk translated">不同的数据增强应用于狗的图像。来源:<a class="ae le" href="https://arxiv.org/pdf/2002.05709.pdf" rel="noopener ugc nofollow" target="_blank">【2】</a></p></figure><p id="600a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">从这篇论文的摘录中可以看出，作者应用了不同的增强，如调整大小、颜色失真、模糊、噪声等等。他们还从图像的不同部分获取作物，这对模型学习一致的表示很重要。可以将图像裁剪成全局和局部视图(完整图像和图像的裁剪部分)，或者可以使用相邻视图(从图像的不同部分进行两次裁剪)。每一对被公式化为<strong class="kk iu">正对，</strong>，即两个增强图像包含相同的对象。</p><p id="7da4" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">接下来，这些对被传递到<strong class="kk iu">卷积神经网络</strong>中，以创建每个图像的特征表示。在论文中，作者选择使用流行的 ResNet 架构进行实验。成对的图像总是成批地提供给模型。特别强调的是批量的大小，作者从 256 到 8192 不等。从该批次开始，应用数据扩充，导致批次大小加倍，因此从 512 到 16382 个输入图像。</p><p id="097e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">一旦 ResNet 计算出输入图像的矢量表示，该输出将被传送到<strong class="kk iu">投影头</strong>进行进一步处理。在本文中，这个投影头是一个带有一个隐藏层的 MLP。该 MLP 仅在训练和进一步细化输入图像的特征表示期间使用。</p><figure class="lh li lj lk gt ll gh gi paragraph-image"><div role="button" tabindex="0" class="lm ln di lo bf lp"><div class="gh gi mu"><img src="../Images/5d8634a2ab2b2d7748007222a3560420.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DjfAT0CBvObo49ckZzdiXg.png"/></div></div><p class="ls lt gj gh gi lu lv bd b be z dk translated">从原始输入图像到由 MLP 计算的表示的 SimCLR 训练过程。来源:<a class="ae le" href="https://github.com/google-research/simclr" rel="noopener ugc nofollow" target="_blank">【1】</a></p></figure><p id="4f5f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">一旦 MLP 计算完成，结果保留为损失函数的输入。<strong class="kk iu">sim clr 的学习目标是使同一图像的不同增强之间的一致性最大化</strong>。这意味着模型试图<strong class="kk iu">最小化包含相同对象的图像之间的距离</strong>和<strong class="kk iu">最大化包含非常不同的对象的图像之间的距离</strong>。这种机制也被称为<strong class="kk iu">对比学习</strong>。</p><p id="9210" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">SimCLR 论文的一个主要贡献是制定了其<strong class="kk iu"> NT-Xent 损失</strong>。NT-Xent 代表归一化温度标度交叉熵损失。这个新颖的损失函数具有一个特别理想的性质:<strong class="kk iu">不同的例子被有效地加权</strong>允许模型<strong class="kk iu">从彼此远离的矢量表示中更有效地学习</strong>，即使它们的原点是相同的图像。这些模型认为彼此非常不同的例子被称为<strong class="kk iu">硬否定</strong>。</p><p id="ad37" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这种损失有效地实现了相似图像的吸引，即，相似图像被学习为更紧密地映射在一起。</p><figure class="lh li lj lk gt ll gh gi paragraph-image"><div role="button" tabindex="0" class="lm ln di lo bf lp"><div class="gh gi mv"><img src="../Images/42967ebbe6be100b3c9505bb704b8ed0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RFILaYrPribmV3rP-lwA2w.png"/></div></div><p class="ls lt gj gh gi lu lv bd b be z dk translated">相似的图像相互吸引。来源:<a class="ae le" href="https://github.com/google-research/simclr" rel="noopener ugc nofollow" target="_blank">【1】</a></p></figure><h1 id="acfb" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">结果</h1><p id="d593" class="pw-post-body-paragraph ki kj it kk b kl mo ju kn ko mp jx kq kr mq kt ku kv mr kx ky kz ms lb lc ld im bi translated">一旦网络被完全训练，MLP 投影头被丢弃，仅卷积神经网络被用于评估。在他们的论文中，作者进行了不同的评估:</p><p id="fbfb" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">首先，他们在 ImageNet 数据集上测量 SimCLR 作为线性分类器的性能。他们的结果显示<strong class="kk iu"> SimCLR 执行所有其他自我监督的方法</strong>。</p><figure class="lh li lj lk gt ll gh gi paragraph-image"><div role="button" tabindex="0" class="lm ln di lo bf lp"><div class="gh gi mw"><img src="../Images/eb6c9cba22d2b3b0485ef9ea60f551a8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_C0on6-VkZQ8ZzPDoaapvQ.png"/></div></div><p class="ls lt gj gh gi lu lv bd b be z dk translated">SimCLR 和其他自监督方法对 ImageNet 线性分类的结果。来源:<a class="ae le" href="https://arxiv.org/pdf/2002.05709.pdf" rel="noopener ugc nofollow" target="_blank">【2】</a></p></figure><p id="e4e8" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">请记住，这些结果不再是最新的，因为具有更好性能的新方法已经出现。请随意阅读我的其他文章，在那里我回顾了其他自我监督的预培训模型。</p><p id="8053" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">其次，他们评估了 SimCLR 在不同图像数据集上的<strong class="kk iu">性能，对比了使用标签</strong>训练相同的 ResNet，即使用监督学习。同样，<strong class="kk iu"> SimCLR 表现非常好，在许多数据集上击败了监督训练方法</strong>。在同一张表中，他们还查看了用标记数据微调自我监督模型的结果。在这一行中，他们表明<strong class="kk iu"> SimCLR 在几乎所有数据集上的表现都优于监督训练方法</strong>。</p><figure class="lh li lj lk gt ll gh gi paragraph-image"><div role="button" tabindex="0" class="lm ln di lo bf lp"><div class="gh gi mx"><img src="../Images/96e31252bbf55b611552da17fb43d270.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hoHoiiNkCnf3SfJlN8T4DQ.png"/></div></div><p class="ls lt gj gh gi lu lv bd b be z dk translated">评估 SimCLR 与 ResNet 的监督培训。来源:<a class="ae le" href="https://arxiv.org/pdf/2002.05709.pdf" rel="noopener ugc nofollow" target="_blank">【2】</a></p></figure><h1 id="809e" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">包装它</h1><p id="7369" class="pw-post-body-paragraph ki kj it kk b kl mo ju kn ko mp jx kq kr mq kt ku kv mr kx ky kz ms lb lc ld im bi translated">在本文中，您了解了 SimCLR，这是一篇最流行的自我监督框架，概念简单，结果有希望。SimCLR 在不断改进，甚至还有这种架构的第二个版本。虽然我希望这个故事能让你对这篇论文有一个很好的初步了解，但是还有很多东西需要发现。因此，我会鼓励你自己阅读这篇论文，即使你是这个领域的新手。你必须从某个地方开始；)</p><p id="9f10" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如果你对论文中介绍的方法有更多的细节感兴趣，请随时在 Twitter 上给我留言，我的账户链接在我的媒体简介上。</p><p id="1ace" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我希望你喜欢这篇论文的解释。如果你对这篇文章有任何意见，或者如果你看到任何错误，请随时留下评论。</p><p id="0a34" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">最后但同样重要的是，如果你想在高级计算机视觉领域更深入地探索，考虑成为我的追随者</strong>。我试着每周发一篇文章，让你和其他人了解计算机视觉研究的最新进展。</p></div><div class="ab cl my mz hx na" role="separator"><span class="nb bw bk nc nd ne"/><span class="nb bw bk nc nd ne"/><span class="nb bw bk nc nd"/></div><div class="im in io ip iq"><p id="dba5" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">参考资料:</p><p id="6c1e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">[1] SimCLR GitHub 实现:【https://github.com/google-research/simclr T2】</p><p id="362b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">[2]陈，丁等:“视觉表征对比学习的一个简单框架。”<em class="lf">机器学习国际会议</em>。PMLR，2020 年。<a class="ae le" href="https://arxiv.org/pdf/2002.05709.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/2002.05709.pdf</a></p></div></div>    
</body>
</html>