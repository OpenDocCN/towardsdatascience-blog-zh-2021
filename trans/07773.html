<html>
<head>
<title>Intro to natural language processing — how to encode meaning of a word</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">自然语言处理导论——如何对单词的含义进行编码</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/intro-to-natural-language-processing-how-to-encode-meaning-of-a-word-b1742d4beca2?source=collection_archive---------11-----------------------#2021-07-16">https://towardsdatascience.com/intro-to-natural-language-processing-how-to-encode-meaning-of-a-word-b1742d4beca2?source=collection_archive---------11-----------------------#2021-07-16</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="405d" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsdatascience.com/tagged/hands-on-tutorials" rel="noopener" target="_blank">实践教程</a></h2><div class=""/><div class=""><h2 id="4f13" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">理解Word2Vec的数学和实现</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/0df221f8b2d355b4e81526ecd8177fa6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Er8v7OgfpPUrIwQbrWpn6g.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">作者图片</p></figure><p id="467c" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jd"> <em class="md">男人之于国王如同女人之于什么？</em> </strong></p><p id="4324" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">你如何训练一个机器学习算法来正确预测正确答案，“女王”？</p><p id="c2ab" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">在这篇文章中，我将谈论一种自然语言处理模型——word 2 vec——它可以用来进行这样的预测。</p></div><div class="ab cl me mf hx mg" role="separator"><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj"/></div><div class="im in io ip iq"><p id="cc6b" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">在Word2Vec模型中，词汇表中的每个单词都由一个向量表示。这个向量的长度是我们人类设定的一个超参数。在本文中我们用100。</p><p id="3ad1" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">因为每个单词都是100个数字的向量，所以你可以把每个单词想象成100维空间中的一个点。</p><p id="21f5" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">Word2Vec的最终目标是相似的单词具有相似的向量，从而在这个100维空间中更紧密地组合在一起。</p><p id="6996" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">那么，你如何训练一个算法，使得相似的单词最终有相似的向量呢？这如何帮助解决类比问题“男人对国王就像女人对什么？”</p></div><div class="ab cl me mf hx mg" role="separator"><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj"/></div><div class="im in io ip iq"><h1 id="e370" class="ml mm it bd mn mo mp mq mr ms mt mu mv ki mw kj mx kl my km mz ko na kp nb nc bi translated">第一部分。带梯度下降的训练字2Vec</h1><p id="e2f0" class="pw-post-body-paragraph lh li it lj b lk nd kd lm ln ne kg lp lq nf ls lt lu ng lw lx ly nh ma mb mc im bi translated">为了训练Word2Vec，你需要一个大的文本语料库。比如<a class="ae ni" href="https://en.wikipedia.org/wiki/Brown_Corpus" rel="noopener ugc nofollow" target="_blank">棕色文集</a>。NLTK已经编译了一个可供访问的语料库的<a class="ae ni" href="http://www.nltk.org/nltk_data/" rel="noopener ugc nofollow" target="_blank">列表。</a></p><p id="1e37" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">Word2Vec的核心是预测一个单词出现在另一个单词附近的概率(在一个句子中)。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi nj"><img src="../Images/033cd1d03facff27266dd4f14c9bd342.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LOBNMGDb-Cy7iVvQ2YmFmg.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">作者图片</p></figure><p id="dd54" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">上例:<em class="md">“…珍惜淡蓝色的小点，我们所知的唯一家园”</em>，“蓝”是<strong class="lj jd">中心</strong>字，“淡”是或<strong class="lj jd">上下文</strong>字外的一个<strong class="lj jd">。我们希望算法预测“苍白”是“蓝色”的上下文单词的概率注意:在这篇文章中，我将在</strong>单词和<strong class="lj jd">上下文</strong>单词之外互换使用<strong class="lj jd">。</strong></p><p id="2c40" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">我们计算在定义的窗口大小内的每个上下文单词的概率。如果窗口大小为1，我们希望预测紧接在中心单词之前和之后的单词的概率。如果窗口大小是2，我们想要预测紧接在中心单词之前和之后的2个单词的概率。</p><p id="cdd8" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">因此，对于算法，我们遍历语料库中的每个单词。在每个位置<em class="md"> t </em>处，给定位置<em class="md"> t </em>处的中心单词，我们计算窗口<em class="md"> w </em>内上下文单词的概率。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi nk"><img src="../Images/1acde0f8fc8524b4735f65c7df23cbf8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iOyKcnEpWLrzbnuQzI-JHw.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">作者图片</p></figure><p id="5636" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">好的模型将给出这些上下文单词的高概率，而坏的模型给出低概率。</p><p id="0109" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">实际值和预测值之间的差异被计入损失，在梯度下降中用于更新模型的参数(θ)。这就是许多监督学习算法的中心主题。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi nl"><img src="../Images/52286d041a470c21df34c450a9fef19b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1332/format:webp/1*bTIn4qGkg5TDdr7PW10-dA.png"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">梯度下降</p></figure><p id="bd73" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">在Word2Vec中，theta到底是什么？损失函数是什么？</p></div><div class="ab cl me mf hx mg" role="separator"><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj"/></div><div class="im in io ip iq"><h2 id="dcc8" class="nm mm it bd mn nn no dn mr np nq dp mv lq nr ns mx lu nt nu mz ly nv nw nb iz bi translated">Word2Vec的参数和损失函数</h2><p id="4f18" class="pw-post-body-paragraph lh li it lj b lk nd kd lm ln ne kg lp lq nf ls lt lu ng lw lx ly nh ma mb mc im bi translated">回想一下，每个单词都由100个数字组成的向量表示。还记得每个单词都可以作为中心词或上下文词。因此，<strong class="lj jd">语料库中的每个唯一单词由两个向量</strong>表示:一个向量作为中心单词，另一个向量作为上下文单词。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/7eda33e7159fdc3d58107282e5855198.png" data-original-src="https://miro.medium.com/v2/resize:fit:1356/format:webp/1*wGk_iLDYUVx0LLFlpD6tKA.png"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">参数θ</p></figure><p id="6603" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">Word2Vec的参数theta是语料库中所有唯一单词的中心和外部向量。theta的维数是词汇行和100列长度的2倍。</p><p id="94af" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">在图片中，我用小写的<em class="md"> v </em>表示一个单词的中心词向量，小写的<em class="md"> u </em>表示一个单词的上下文词向量。因此，当斑马是中心单词时，<em class="md"> v_zebra </em>是斑马的向量。当zebra是某个其他中心词的上下文词时，u_zebra 是zebra的向量。稍后，我将使用大写字母<em class="md"> U </em>来引用一个包含所有外部向量<em class="md"> u </em>的矩阵。</p><p id="c992" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">我们将使用的损失函数是负对数似然(NLL)损失。NLL用于许多机器学习算法，如逻辑回归。</p><p id="686e" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">NLL的计算公式为:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ny"><img src="../Images/9e1d7111ad113f5ff09e651ad4c7c66e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*af_yCHlJwRICJlRBxKMSyA.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">负对数似然损失函数</p></figure><p id="7e1d" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">我们对唯一单词<em class="md"> T、</em>的长度求和，并且在每个索引<em class="md"> t </em>处，我们对给定索引<em class="md"> t </em>处的中心单词的每个上下文单词(在窗口<em class="md"> w </em>内)的概率求和。</p><p id="5b47" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">因此，给定一个中心单词，丢失一个上下文单词是:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi nz"><img src="../Images/9ec590f8aa06c3e525393dc3e1ac65dc.png" data-original-src="https://miro.medium.com/v2/resize:fit:744/format:webp/1*d-vkuffqA3Wdo6spCHtN3Q.png"/></div></figure><p id="a2ad" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">你如何计算概率？我们使用Softmax方程。下面是在给定一个中心词(c)的情况下，如何计算上下文词(o)的概率。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi oa"><img src="../Images/8bb771022b8ea32605cd895ff0f7add4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bjxBryoTC923rNQ-Q0QrUA.png"/></div></div></figure><p id="17ed" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">命名符是上下文词<em class="md"> o </em>的上下文向量与中心词<em class="md"> c </em>的中心向量的点积的指数。分母是每个词汇单词的上下文向量与中心单词向量的点积的指数之和。哇，那是一口！</p><p id="c450" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">正如您可能已经猜到的那样，这个naive-softmax概率的计算成本很高，因为我们必须对整个词汇表求和。还有另一种更有效的方法来做亏损；稍后我会描述它。</p><p id="2bc8" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">在梯度下降法中，你对损失函数求关于参数的导数，以找到如何在每次迭代中更新参数。</p><p id="b991" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">对于给定的中心词，我们需要计算中心词向量的梯度以及所有上下文词向量的梯度。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ob"><img src="../Images/eda852432dbdde68093da07b6c3f140c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QFUeGPx-NAgHiyG9i1O9qg.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">计算梯度的损失函数导数</p></figure><p id="cfc7" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">我不会在这里求导，但是为了计算中心向量的梯度，你要取<em class="md"> U </em>和(y_hat减y)的点积。正如我提到的，U 是整个词汇的所有外部向量的矩阵。为了计算外部向量的梯度，你取中心向量和(y_hat减y)的点积。这种情况下的y_hat是<em class="md"> U </em>和中心向量的点积的Softmax。</p><p id="cc97" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">总的来说，算法是这样工作的:</p><p id="efea" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">对于每次迭代:</p><ol class=""><li id="a2cb" class="oc od it lj b lk ll ln lo lq oe lu of ly og mc oh oi oj ok bi translated">计算小批量(如50号)的损耗和梯度。</li><li id="968a" class="oc od it lj b lk ol ln om lq on lu oo ly op mc oh oi oj ok bi translated">像这样更新模型参数:</li></ol><p id="d406" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><code class="fe oq or os ot b">theta = theta — learning_rate * gradients</code></p><p id="18cb" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">为了计算尺寸为50的小批量的梯度，您迭代50次，并且在每次迭代期间，您:</p><ol class=""><li id="f09e" class="oc od it lj b lk ll ln lo lq oe lu of ly og mc oh oi oj ok bi translated">获取一个随机的中心词及其上下文词。</li><li id="dd90" class="oc od it lj b lk ol ln om lq on lu oo ly op mc oh oi oj ok bi translated">对于每个上下文单词，计算梯度(通过对损失函数求导)。</li><li id="014c" class="oc od it lj b lk ol ln om lq on lu oo ly op mc oh oi oj ok bi translated">聚合上下文单词的所有梯度。</li></ol><p id="df97" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">然后，小批量的聚集梯度被用于更新θ。</p><p id="47e3" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">这里有一个Github的要点和一些相关的代码。注意:我已经排除了一些部分，如解析语料库和Softmax方程的代码。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="ou ov l"/></div></figure><p id="09ab" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">我刚才描述的算法叫做跳格算法。在Skip-Gram中，给定一个中心词，预测上下文词的概率。还有另一种算法叫做CBOW，根据上下文预测中心词的概率。</p><p id="f312" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">正如我提到的，我们使用的损失函数计算起来很昂贵。还有另一种训练Word2Vec的技术叫做“负采样”在负采样中，不是交叉熵损失(即Softmax函数的负对数似然)，而是为一个真实对和几个噪声对训练二元逻辑回归。真正的对是一个中心词和它的上下文词之一。噪声对是相同的中心词和随机词。在负抽样中，损失函数更有效，因为你不是对整个词汇求和。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ow"><img src="../Images/b3e1f206c325c31165bf01955d92caf0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5KPXiw67UmKxtBC6L80VEw.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">负采样的损失fxn</p></figure><h1 id="0c2c" class="ml mm it bd mn mo ox mq mr ms oy mu mv ki oz kj mx kl pa km mz ko pb kp nb nc bi translated">第二部分。Word2Vec如何解决类比问题</h1><p id="8250" class="pw-post-body-paragraph lh li it lj b lk nd kd lm ln ne kg lp lq nf ls lt lu ng lw lx ly nh ma mb mc im bi translated">男人之于国王就像女人之于什么？</p><p id="1ca1" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">当你让Word2Vec算法来解决这个类比时，你实际上是在让它找出一个与“国王”和“女人”最相似，但与“男人”最不相似的词。这里的相似度是指两个向量的<strong class="lj jd">余弦相似度。</strong></p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pc"><img src="../Images/ae2e8925bcfdd6fc28e52268964a1420.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nmd9W2ZMtIrXLAl_xmVxSA.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated"><a class="ae ni" href="https://en.wikipedia.org/wiki/Cosine_similarity" rel="noopener ugc nofollow" target="_blank">https://en.wikipedia.org/wiki/Cosine_similarity</a></p></figure><p id="8146" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">因此，经过训练后，单词“女王”的向量与“国王”和“女人”的向量最相似，但与“男人”的向量不同</p><p id="aad4" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">此外，因为相似的单词在高维空间中聚集在一起，所以您可以将“queen”的向量想象为“king”的向量减去“man”的向量加上“woman”的向量。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi pd"><img src="../Images/a5ec3bd534081eb5ab84f5d22b80f8c1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/1*QzVJL0NAa6LLrmwZ20u3Xw.png"/></div></figure><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pe"><img src="../Images/006142d4fd57fc77151aeb322034aaf0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Rk9pMEME8wJN-PCmZzpUxQ.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">作者图片</p></figure><p id="0129" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">这就是我们如何训练计算机解决类比问题！</p></div><div class="ab cl me mf hx mg" role="separator"><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj"/></div><div class="im in io ip iq"><p id="d958" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">查找相似词和解决类比只是Word2Vec的两个基本应用。Word2Vec嵌入还可以用于更复杂的任务，比如情感分析。对于情感分析，特征将是句子中所有单词向量的平均值。标签当然是一种情绪(积极、消极、中性)。然后，您可以使用随机森林、Softmax回归甚至神经网络等分类器进行多类分类！</p><p id="1e2b" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">敬请关注NLP的更多文章！</p></div></div>    
</body>
</html>