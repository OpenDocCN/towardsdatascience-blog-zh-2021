<html>
<head>
<title>Wisdom of the Crowd -Voting Classifier, Bagging-Pasting, Random Forest and Extra Trees-</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">群体的智慧——投票分类器，装袋粘贴，随机森林和额外的树</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/wisdom-of-the-crowd-voting-classifier-bagging-pasting-random-forest-and-extra-trees-289ef991e723?source=collection_archive---------20-----------------------#2021-08-13">https://towardsdatascience.com/wisdom-of-the-crowd-voting-classifier-bagging-pasting-random-forest-and-extra-trees-289ef991e723?source=collection_archive---------20-----------------------#2021-08-13</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="5b7e" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">使用多种算法，集成学习，用python实现</h2></div><pre class="ki kj kk kl gt km kn ko kp aw kq bi"><span id="256b" class="kr ks it kn b gy kt ku l kv kw"><strong class="kn iu"><em class="kx">Table of Contents</em></strong><em class="kx"> <br/></em><strong class="kn iu">1. Introduction<br/>2. Voting Classifier<br/>3. Bagging and Pasting<br/>3.1. Out of Bag Evolution<br/>3.2. Random patches and Random Subspaces<br/>4. Random Forest<br/>5. Extremely Randomized Trees</strong></span></pre><figure class="ki kj kk kl gt kz gh gi paragraph-image"><div role="button" tabindex="0" class="la lb di lc bf ld"><div class="gh gi ky"><img src="../Images/c9767ed843a2d9bf357aab9859d49e90.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*Enxnh6OI253im0eC"/></div></div><p class="lg lh gj gh gi li lj bd b be z dk translated">在<a class="ae lk" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上<a class="ae lk" href="https://unsplash.com/@cliff_77?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Cliff Johnson </a>拍摄的照片</p></figure><h1 id="ce53" class="ll ks it bd lm ln lo lp lq lr ls lt lu jz lv ka lw kc lx kd ly kf lz kg ma mb bi translated">1.介绍</h1><p id="2261" class="pw-post-body-paragraph mc md it me b mf mg ju mh mi mj jx mk ml mm mn mo mp mq mr ms mt mu mv mw mx im bi translated">在大多数问题上，人们在做决定之前都会咨询别人的意见。在决定一个集体环境时，通常是多数人说了算。虽然即使在个人层面上也是如此，但一些公司在全球层面上调查许多事情。由一群人而不是一个专家做出的决定被称为<strong class="me iu">【群体的智慧】</strong>亚里士多德在他的著作《政治学》中首次使用了这一论点。这种方法集成到机器学习中的部分是集成学习。最简单的形式是训练多个模型并比较它们的结果来解决复杂的问题。目的是通过考虑泛化来提高监视下的准确率，并获得更成功的结果。本文通过最小化方法以及如何在python中实现方法来涉及决策机制。</p><h1 id="4e76" class="ll ks it bd lm ln lo lp lq lr ls lt lu jz lv ka lw kc lx kd ly kf lz kg ma mb bi translated">2.投票分类器</h1><p id="5f94" class="pw-post-body-paragraph mc md it me b mf mg ju mh mi mj jx mk ml mm mn mo mp mq mr ms mt mu mv mw mx im bi translated">投票分类器，顾名思义，是一种‘投票’——基于民主的分类。用一句话来解释，可以定义为多数表决。让我们假设我们用三种不同的算法来训练我们的分类问题——SVM、逻辑回归、决策树——我们在每一种算法中获得了不同的成功率。另一方面，投票分类器具体评估每个测试数据的结果，并决定支持投票占多数的一方。训练数据的测试结果如图1所示，不正确的预测与<em class="kx"> y_test </em>进行比较，并以红色显示。从准确性结果来看，可以看出巨大的差异。当然，没有预料到真实数据集会有这样的差异，但是图1。演示了投票分类器的工作原理。由于有3个分类器，投票分类器决定了其中至少有2个是相同的。这种基于民主多数投票的分类被称为<strong class="me iu">硬投票分类器</strong>。</p><figure class="ki kj kk kl gt kz gh gi paragraph-image"><div role="button" tabindex="0" class="la lb di lc bf ld"><div class="gh gi my"><img src="../Images/c09a87f53d4db1f0698c321450fbb828.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-U3qLDw2scW9mkd0luKcDw.png"/></div></div><p class="lg lh gj gh gi li lj bd b be z dk translated">图一。投票分类器是如何工作的？，作者图片</p></figure><p id="9049" class="pw-post-body-paragraph mc md it me b mf mz ju mh mi na jx mk ml nb mn mo mp nc mr ms mt nd mv mw mx im bi translated"><strong class="me iu">软投票分类器</strong>采用更加灵活的策略。就像在硬投票中，评估每个分类器的结果，但后退一步就会出现，也就是说，在进行预测时，它会观察分类器的概率值。例如，让我们考虑测试结果为</p><ul class=""><li id="7145" class="ne nf it me b mf mz mi na ml ng mp nh mt ni mx nj nk nl nm bi translated">SVM[0级=0.9，1级=0.1]，</li><li id="524c" class="ne nf it me b mf nn mi no ml np mp nq mt nr mx nj nk nl nm bi translated">决策树分类器[class-0 =0.4，class-1=0.6]，</li><li id="7c8f" class="ne nf it me b mf nn mi no ml np mp nq mt nr mx nj nk nl nm bi translated">逻辑回归[0类=0.35，1类=0.65]</li></ul><p id="56ff" class="pw-post-body-paragraph mc md it me b mf mz ju mh mi na jx mk ml nb mn mo mp nc mr ms mt nd mv mw mx im bi translated">如果我们用<strong class="me iu">硬投票分类器</strong>对其进行评估，因为有2个1类，而只有1个0类，硬投票分类器会判定为1类。如果用<strong class="me iu">软投票分类器</strong>来评价:</p><ul class=""><li id="c607" class="ne nf it me b mf mz mi na ml ng mp nh mt ni mx nj nk nl nm bi translated">0类的概率:0.33 * 0.9+0.33 * 0.4+0.33 * 0.35 = 0.5445</li><li id="82fe" class="ne nf it me b mf nn mi no ml np mp nq mt nr mx nj nk nl nm bi translated">第一类的概率:0.33 * 0.1+0.33 * 0.6+0.33 * 0.65 = 0.4455</li></ul><p id="4a9a" class="pw-post-body-paragraph mc md it me b mf mz ju mh mi na jx mk ml nb mn mo mp nc mr ms mt nd mv mw mx im bi translated">可以看出，与硬投票不同，软投票分类器的偏好将是0类。</p><p id="176f" class="pw-post-body-paragraph mc md it me b mf mz ju mh mi na jx mk ml nb mn mo mp nc mr ms mt nd mv mw mx im bi translated">现在，让我们用python实现上面提到的乳腺癌数据集:</p><figure class="ki kj kk kl gt kz"><div class="bz fp l di"><div class="ns nt l"/></div></figure><pre class="ki kj kk kl gt km kn ko kp aw kq bi"><span id="ce3d" class="kr ks it kn b gy kt ku l kv kw"><strong class="kn iu">OUT<br/>LogisticRegression() 0.9525539512498058<br/>DecisionTreeClassifier() 0.9227293898462972<br/>SVC(probability=True) 0.9437820214252446<br/>KNeighborsClassifier() 0.9455519329296692<br/>VotingClassifier(estimators=[('lr', LogisticRegression()),<br/>                             ('dt', DecisionTreeClassifier()),<br/>                             ('svc', SVC(probability=True)),<br/>                             ('knn', KNeighborsClassifier())]) 0.9490451793199813<br/>VotingClassifier(estimators=[('lr', LogisticRegression()),<br/>                             ('dt', DecisionTreeClassifier()),<br/>                             ('svc', SVC(probability=True)),<br/>                             ('knn', KNeighborsClassifier())],<br/>voting='soft') 0.9455364073901569</strong></span></pre><p id="4104" class="pw-post-body-paragraph mc md it me b mf mz ju mh mi na jx mk ml nb mn mo mp nc mr ms mt nd mv mw mx im bi translated">SVC中<em class="kx">‘概率’</em>的默认值为假。对于添加<code class="fe nu nv nw kn b">predict_proba()</code>方法，该值被定义为真。这是必要的。在投票分类器中，所有模型都用相同的数据集进行训练和测试。如图所示，软和硬的区别由<em class="kx">“投票”</em>决定。</p><h1 id="f098" class="ll ks it bd lm ln lo lp lq lr ls lt lu jz lv ka lw kc lx kd ly kf lz kg ma mb bi translated">3.装袋和粘贴</h1><p id="5362" class="pw-post-body-paragraph mc md it me b mf mg ju mh mi mj jx mk ml mm mn mo mp mq mr ms mt mu mv mw mx im bi translated">另一种方法bagging及其衍生方法根据训练数据从投票分类器中分离出来。训练数据集被分成子集，分类器用这些子集来训练。在培训后的测试过程中，民主发挥作用，通过硬投票做出决定。如果随机选择行的子集进行替换，这称为<strong class="me iu">引导聚合(Bagging) </strong>。为了解释表达式<em class="kx">“with replacement”</em>，对应于一个分类器中用于训练的特定数据被另一个分类器使用的表达式是with replacement。这个事件被称为<strong class="me iu">引导</strong>。背后的想法是基于统计的，它用于估计数量。通过将数据集随机划分为子样本来评估数据集。在该划分期间，在多个子集中具有相同数据的事件用上述with replacement解释，并在sklearn中用<code class="fe nu nv nw kn b">bootstrap=True</code>定义。</p><p id="f7e6" class="pw-post-body-paragraph mc md it me b mf mz ju mh mi na jx mk ml nb mn mo mp nc mr ms mt nd mv mw mx im bi translated">另一方面，如果随机选择行的子集而不替换，这被称为<strong class="me iu">粘贴</strong>。任何分类器在粘贴过程中使用的数据都不允许被另一个分类器使用。</p><p id="0ca3" class="pw-post-body-paragraph mc md it me b mf mz ju mh mi na jx mk ml nb mn mo mp nc mr ms mt nd mv mw mx im bi translated">由于在这些过程中使用了多个分类器，虽然<strong class="me iu">偏差</strong>变化不大，但它降低了模型的<strong class="me iu">方差</strong>，保护了模型不至于过拟合，模型的稳定性增加，变得更加通用。现在让我们用python实现这些提到的方法:</p><figure class="ki kj kk kl gt kz"><div class="bz fp l di"><div class="ns nt l"/></div></figure><pre class="ki kj kk kl gt km kn ko kp aw kq bi"><span id="8a8d" class="kr ks it kn b gy kt ku l kv kw"><strong class="kn iu">OUT<br/>bagging results [0.89473684 0.97368421 0.95767196]<br/>average of bagging: 0.9420310034345123<br/>********************************************<br/>pasting results [0.89473684 0.97368421 0.96296296]<br/>average of pasting: 0.9437946718648473<br/>********************************************</strong></span></pre><p id="03bb" class="pw-post-body-paragraph mc md it me b mf mz ju mh mi na jx mk ml nb mn mo mp nc mr ms mt nd mv mw mx im bi translated">可以看出，装袋和粘贴的区别是由bootstrap决定的。如果拾取数据时有替换，即引导，则称为<strong class="me iu">引导聚集(bagging) </strong>，如果拾取数据时没有替换，即没有引导，则称为<strong class="me iu">粘贴</strong>。</p><p id="7747" class="pw-post-body-paragraph mc md it me b mf mz ju mh mi na jx mk ml nb mn mo mp nc mr ms mt nd mv mw mx im bi translated">到目前为止，一直使用分类，但它也可以用于回归:</p><figure class="ki kj kk kl gt kz"><div class="bz fp l di"><div class="ns nt l"/></div></figure><pre class="ki kj kk kl gt km kn ko kp aw kq bi"><span id="225c" class="kr ks it kn b gy kt ku l kv kw"><strong class="kn iu">OUT<br/>numeric_cols Index(['wheelbase', 'carlength', 'carwidth', 'carheight', 'curbweight','enginesize', 'boreratio', 'stroke', 'compressionratio', 'horsepower','peakrpm', 'citympg','highwaympg'],<br/>dtype='object')<br/>categorical_cols Index(['fueltype', 'aspiration', 'drivewheel', 'enginetype', 'cylindernumber','fuelsystem'],dtype='object')<br/>bagging results [0.88065606 0.84712127 0.7973921  0.72010695]<br/>average of bagging: 0.8113190957164755<br/>********************************************</strong></span></pre><p id="d2ce" class="pw-post-body-paragraph mc md it me b mf mz ju mh mi na jx mk ml nb mn mo mp nc mr ms mt nd mv mw mx im bi translated">使用汽车价格数据集。分类列用<code class="fe nu nv nw kn b">OneHotEncoder</code>编码，大多数数字列用<code class="fe nu nv nw kn b">StandardScaler</code>标准化，一些数字列用宁滨分组。然后，我们获得的新数据集包含了<code class="fe nu nv nw kn b">max_features=0.4</code>，即40%的特征和<code class="fe nu nv nw kn b">max_sampeles=0.8</code>，即80%的数据。</p><h2 id="ab9b" class="kr ks it bd lm nx ny dn lq nz oa dp lu ml ob oc lw mp od oe ly mt of og ma oh bi translated">3.1.出袋进化</h2><p id="ca13" class="pw-post-body-paragraph mc md it me b mf mg ju mh mi mj jx mk ml mm mn mo mp mq mr ms mt mu mv mw mx im bi translated"><strong class="me iu">理论:</strong>如果数据集足够大，bootstrap=True，36.79%的数据根本不会被选择。</p><p id="0df1" class="pw-post-body-paragraph mc md it me b mf mz ju mh mi na jx mk ml nb mn mo mp nc mr ms mt nd mv mw mx im bi translated"><strong class="me iu">证明:</strong>提到了一些数据被选择并用于一个以上的带有装袋的分类器。没有被任何分类器使用的选择数据的概率是(1–1/m ),其中m是样本数。</p><p id="e814" class="pw-post-body-paragraph mc md it me b mf mz ju mh mi na jx mk ml nb mn mo mp nc mr ms mt nd mv mw mx im bi translated">让我们概括一下:不拾取任何分类器都不使用的数据的概率是(1–1/m)ᵐ.当“m”足够大时，统计上它相当于(1–1/e)= 0.6321。</p><p id="0478" class="pw-post-body-paragraph mc md it me b mf mz ju mh mi na jx mk ml nb mn mo mp nc mr ms mt nd mv mw mx im bi translated">这意味着63.21%的训练样本被每个分类器提取，36.79%的训练样本未被采样。这36.79%的样本被称为<strong class="me iu"/>。由于在训练期间没有“oob”数据，这些数据可用于评估(<em class="kx"> test_data </em>)。我们可以在Scikit学习库中实现它，如下所示:</p><figure class="ki kj kk kl gt kz"><div class="bz fp l di"><div class="ns nt l"/></div></figure><pre class="ki kj kk kl gt km kn ko kp aw kq bi"><span id="496e" class="kr ks it kn b gy kt ku l kv kw"><strong class="kn iu">OUT<br/>bagging oob results: [0.88811189 0.97183099 0.95774648 0.97183099]<br/>average of oob: 0.9473800847040283</strong></span></pre><p id="eb6a" class="pw-post-body-paragraph mc md it me b mf mz ju mh mi na jx mk ml nb mn mo mp nc mr ms mt nd mv mw mx im bi translated">这只能通过设置oob_score=True来实现。</p><h2 id="b1d6" class="kr ks it bd lm nx ny dn lq nz oa dp lu ml ob oc lw mp od oe ly mt of og ma oh bi translated">3.2.随机补丁和随机子空间</h2><p id="ac5f" class="pw-post-body-paragraph mc md it me b mf mg ju mh mi mj jx mk ml mm mn mo mp mq mr ms mt mu mv mw mx im bi translated">装袋的超参数:</p><ul class=""><li id="0427" class="ne nf it me b mf mz mi na ml ng mp nh mt ni mx nj nk nl nm bi translated"><code class="fe nu nv nw kn b">max_features:</code>决定选择多少/多少特征。Float表示百分比，integer表示数字。</li><li id="e9e8" class="ne nf it me b mf nn mi no ml np mp nq mt nr mx nj nk nl nm bi translated"><code class="fe nu nv nw kn b">max_samples:</code>决定采样多少数据。Float表示百分比，integer表示数字。</li><li id="4b9d" class="ne nf it me b mf nn mi no ml np mp nq mt nr mx nj nk nl nm bi translated"><code class="fe nu nv nw kn b">bootstrap:</code>布尔，真意味着有替换；假的意思是没有替换。</li><li id="5317" class="ne nf it me b mf nn mi no ml np mp nq mt nr mx nj nk nl nm bi translated"><code class="fe nu nv nw kn b">bootstrap_features:</code>布尔，True表示特征是用替换绘制的；False表示绘制的要素没有替换。</li></ul><p id="1374" class="pw-post-body-paragraph mc md it me b mf mz ju mh mi na jx mk ml nb mn mo mp nc mr ms mt nd mv mw mx im bi translated">所有训练实例(<code class="fe nu nv nw kn b">bootstrap=False</code>，<code class="fe nu nv nw kn b">max_samples</code>为高(默认值=1))，但采样特征(<code class="fe nu nv nw kn b">bootstrap_features=True</code>)被称为<strong class="me iu">随机子空间</strong>。</p><figure class="ki kj kk kl gt kz"><div class="bz fp l di"><div class="ns nt l"/></div></figure><pre class="ki kj kk kl gt km kn ko kp aw kq bi"><span id="3247" class="kr ks it kn b gy kt ku l kv kw"><strong class="kn iu">OUT<br/>randomsub 0.929824561403508</strong></span></pre><p id="7a5f" class="pw-post-body-paragraph mc md it me b mf mz ju mh mi na jx mk ml nb mn mo mp nc mr ms mt nd mv mw mx im bi translated">训练和特征都被采样，称为<strong class="me iu">随机面片</strong>。</p><figure class="ki kj kk kl gt kz"><div class="bz fp l di"><div class="ns nt l"/></div></figure><pre class="ki kj kk kl gt km kn ko kp aw kq bi"><span id="fa31" class="kr ks it kn b gy kt ku l kv kw"><strong class="kn iu">OUT<br/>patch 0.9649122807017544</strong></span></pre><p id="caf0" class="pw-post-body-paragraph mc md it me b mf mz ju mh mi na jx mk ml nb mn mo mp nc mr ms mt nd mv mw mx im bi translated">这些主要用于高维数据集，如图像数据集。图像中有大量的特征，成百上千的像素，通过使用这些方法可以获得更可靠的结果。</p><h1 id="88a1" class="ll ks it bd lm ln lo lp lq lr ls lt lu jz lv ka lw kc lx kd ly kf lz kg ma mb bi translated">4.随机森林</h1><p id="593b" class="pw-post-body-paragraph mc md it me b mf mg ju mh mi mj jx mk ml mm mn mo mp mq mr ms mt mu mv mw mx im bi translated">Bagging已经用分类器方法的相同数据集的不同训练子集训练了多次。如果要选择分类器<em class="kx"> DecisionTreeClassifier </em>，决策树的更优化版本<em class="kx"> RandomForestClassifier </em>也可以作为bagging的替代方案。</p><figure class="ki kj kk kl gt kz"><div class="bz fp l di"><div class="ns nt l"/></div></figure><pre class="ki kj kk kl gt km kn ko kp aw kq bi"><span id="5003" class="kr ks it kn b gy kt ku l kv kw"><strong class="kn iu">OUT<br/>random forest results: [0.92105263 0.97368421 0.95767196]<br/>average of rf: 0.9508029332590736<br/>********************************************<br/> bagging random forest results: [0.91578947 0.97368421 0.96296296]<br/>average of bagging rf: 0.9508122157244964<br/>********************************************</strong></span></pre><h1 id="aecb" class="ll ks it bd lm ln lo lp lq lr ls lt lu jz lv ka lw kc lx kd ly kf lz kg ma mb bi translated">5.额外的树</h1><p id="5315" class="pw-post-body-paragraph mc md it me b mf mg ju mh mi mj jx mk ml mm mn mo mp mq mr ms mt mu mv mw mx im bi translated">全名是<strong class="me iu">极度随机树</strong>，它建造多棵树以及随机森林。与其他不同，自举不提供随机性，而是在每个节点随机分裂节点。因为在每个节点上都提供了随机分割，所以方差非常低。</p><figure class="ki kj kk kl gt kz"><div class="bz fp l di"><div class="ns nt l"/></div></figure><pre class="ki kj kk kl gt km kn ko kp aw kq bi"><span id="9b04" class="kr ks it kn b gy kt ku l kv kw"><strong class="kn iu">OUT<br/>extra tree results: [0.91052632 0.98421053 0.96296296]<br/>average of extra tree: 0.9525666016894087</strong></span></pre><blockquote class="oi oj ok"><p id="1e83" class="mc md kx me b mf mz ju mh mi na jx mk ol nb mn mo om nc mr ms on nd mv mw mx im bi translated">比较树间方差，说的是决策树方差&gt;随机森林方差&gt;极度随机化树方差。</p></blockquote></div><div class="ab cl oo op hx oq" role="separator"><span class="or bw bk os ot ou"/><span class="or bw bk os ot ou"/><span class="or bw bk os ot"/></div><div class="im in io ip iq"><h2 id="a815" class="kr ks it bd lm nx ny dn lq nz oa dp lu ml ob oc lw mp od oe ly mt of og ma oh bi translated">回到指引点击<a class="ae lk" href="https://ibrahimkovan.medium.com/machine-learning-guideline-959da5c6f73d" rel="noopener">这里</a>。</h2><div class="ov ow gp gr ox oy"><a href="https://ibrahimkovan.medium.com/machine-learning-guideline-959da5c6f73d" rel="noopener follow" target="_blank"><div class="oz ab fo"><div class="pa ab pb cl cj pc"><h2 class="bd iu gy z fp pd fr fs pe fu fw is bi translated">机器学习指南</h2><div class="pf l"><h3 class="bd b gy z fp pd fr fs pe fu fw dk translated">本文旨在准备一个机器学习数据库，以便在一个视图中显示所有的机器学习标题。这个…</h3></div><div class="pg l"><p class="bd b dl z fp pd fr fs pe fu fw dk translated">ibrahimkovan.medium.com</p></div></div><div class="ph l"><div class="pi l pj pk pl ph pm le oy"/></div></div></a></div></div></div>    
</body>
</html>