<html>
<head>
<title>The Magic of Principal Component Analysis through Image Compression</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">通过图像压缩实现主成分分析的魔力</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/image-compression-with-pca-a0595f57940c?source=collection_archive---------14-----------------------#2021-10-24">https://towardsdatascience.com/image-compression-with-pca-a0595f57940c?source=collection_archive---------14-----------------------#2021-10-24</a></blockquote><div><div class="fc ii ij ik il im"/><div class="in io ip iq ir"><div class=""/><div class=""><h2 id="ddda" class="pw-subtitle-paragraph jr it iu bd b js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki dk translated">利用图像精美演示PCA</h2></div><figure class="kk kl km kn gu ko gi gj paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gi gj kj"><img src="../Images/a3fbfd34280c49e7dc86378b2ea33239.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*X4IrOz4Xi_O8Knvx"/></div></div><p class="kv kw gk gi gj kx ky bd b be z dk translated">Erik Mclean 在<a class="ae kz" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><h1 id="729e" class="la lb iu bd lc ld le lf lg lh li lj lk ka ll kb lm kd ln ke lo kg lp kh lq lr bi translated">什么是PCA？</h1><p id="f4d6" class="pw-post-body-paragraph ls lt iu lu b lv lw jv lx ly lz jy ma mb mc md me mf mg mh mi mj mk ml mm mn in bi translated">主成分分析(PCA)是一种<em class="mo">降维技术</em>，用于具有许多<em class="mo">连续</em>(数字)特征或维度的数据集。它使用线性代数来确定数据集最重要的特征。在识别出这些特征之后，你可以只使用这些特征来训练机器学习模型，并在不牺牲准确性的情况下提高性能。正如我的一个好朋友兼导师所说:</p><blockquote class="mp"><p id="857e" class="mq mr iu bd ms mt mu mv mw mx my mn dk translated">“PCA是你机器学习工具箱中的主力。”</p></blockquote><p id="8d84" class="pw-post-body-paragraph ls lt iu lu b lv na jv lx ly nb jy ma mb nc md me mf nd mh mi mj ne ml mm mn in bi translated">PCA找到具有最大方差的轴，并将这些点投影到该轴上。PCA使用线性代数中的一个概念，称为特征向量和特征值。<a class="ae kz" href="https://stats.stackexchange.com/questions/2691/making-sense-of-principal-component-analysis-eigenvectors-eigenvalues/140579" rel="noopener ugc nofollow" target="_blank"> <strong class="lu iv">栈交换</strong> </a>上有个帖子很漂亮的解释了一下。</p><h1 id="5c8d" class="la lb iu bd lc ld le lf lg lh li lj lk ka ll kb lm kd ln ke lo kg lp kh lq lr bi translated">图像压缩</h1><p id="b48d" class="pw-post-body-paragraph ls lt iu lu b lv lw jv lx ly lz jy ma mb mc md me mf mg mh mi mj mk ml mm mn in bi translated">PCA在用于压缩图像时得到了很好的演示。图像只不过是一个像素网格和一个颜色值。让我们将一个图像加载到一个数组中，并观察它的形状。我们将使用来自<code class="fe nf ng nh ni b">matplotlib</code>的<code class="fe nf ng nh ni b">imread</code>。</p><pre class="kk kl km kn gu nj ni nk bn nl nm bi"><span id="2610" class="nn lb iu ni b be no np l nq nr">import numpy as np<br/>import matplotlib.pyplot as plt<br/>from matplotlib.image import imread<br/><br/>image_raw = imread("cat.jpg")<br/>print(image_raw.shape)</span></pre><pre class="ns nj ni nk bn nl nm bi"><span id="7537" class="nn lb iu ni b be no np l nq nr">(3120, 4160, 3)</span></pre><pre class="ns nj ni nk bn nl nm bi"><span id="0dba" class="nn lb iu ni b be no np l nq nr">plt.figure(figsize=[12,8])<br/>plt.imshow(image_raw)</span></pre><figure class="kk kl km kn gu ko gi gj paragraph-image"><div class="gi gj nt"><img src="../Images/64998e42f2f2138de68617c67a452d15.png" data-original-src="https://miro.medium.com/v2/resize:fit:1252/format:webp/0*KjwQdfJxFnqHYnJc.png"/></div><p class="kv kw gk gi gj kx ky bd b be z dk translated">作者图片</p></figure><p id="3c1b" class="pw-post-body-paragraph ls lt iu lu b lv nu jv lx ly nv jy ma mb nw md me mf nx mh mi mj ny ml mm mn in bi translated">结果显示一个矩阵的大小<code class="fe nf ng nh ni b">(3120, 4160, 3)</code>。第一个是图像的高度，第二个是宽度，第三个是RGB值的三个通道。考虑到这张图片的维度数量，您可以看到与经典的表格数据集相比，这是相当大的，尤其是当我们想到3120个<em class="mo">列</em>时。</p><p id="c8f6" class="pw-post-body-paragraph ls lt iu lu b lv nu jv lx ly nv jy ma mb nw md me mf nx mh mi mj ny ml mm mn in bi translated">在我们继续之前，让我们把它改成灰度图像，去掉RGB值。</p><pre class="kk kl km kn gu nj ni nk bn nl nm bi"><span id="c3c7" class="nn lb iu ni b be no np l nq nr"># Show the new shape of the image<br/>image_sum = image_raw.sum(axis=2)<br/>print(image_sum.shape)<br/><br/># Show the max value at any point.  1.0 = Black, 0.0 = White<br/>image_bw = image_sum/image_sum.max()<br/>print(image_bw.max())</span></pre><pre class="ns nj ni nk bn nl nm bi"><span id="140b" class="nn lb iu ni b be no np l nq nr">(3120, 4160)<br/>1.0</span></pre><h1 id="0145" class="la lb iu bd lc ld le lf lg lh li lj lk ka ll kb lm kd ln ke lo kg lp kh lq lr bi translated">计算解释方差</h1><p id="9885" class="pw-post-body-paragraph ls lt iu lu b lv lw jv lx ly lz jy ma mb mc md me mf mg mh mi mj mk ml mm mn in bi translated">接下来，我们可以<code class="fe nf ng nh ni b">fit</code>使用Scikit-Learn中的PCA处理我们的灰度图像。在图像拟合之后，我们有了方法<code class="fe nf ng nh ni b">pca.explained_variance_ratio_,</code>，它返回由每个主成分解释的方差的百分比。利用<code class="fe nf ng nh ni b">np.cumsum</code>，我们可以累加每个分量的每个方差，直到它达到<code class="fe nf ng nh ni b">100%</code>。我们将把它画在一条线上，并显示解释方差的<code class="fe nf ng nh ni b">95%</code>在哪里。</p><pre class="kk kl km kn gu nj ni nk bn nl nm bi"><span id="c0b5" class="nn lb iu ni b be no np l nq nr">import numpy as np<br/>from sklearn.decomposition import PCA, IncrementalPCA<br/><br/>pca = PCA()<br/>pca.fit(image_bw)<br/><br/># Getting the cumulative variance<br/>var_cumu = np.cumsum(pca.explained_variance_ratio_)*100<br/><br/># How many PCs explain 95% of the variance?<br/>k = np.argmax(var_cumu&gt;95)<br/>print("Number of components explaining 95% variance: "+ str(k))<br/>#print("\n")<br/><br/>plt.figure(figsize=[10,5])<br/>plt.title('Cumulative Explained Variance explained by component')<br/>plt.ylabel('Cumulative Explained variance (%)')<br/>plt.xlabel('Principal components')<br/>plt.axvline(x=k, color="k", linestyle="--")<br/>plt.axhline(y=95, color="r", linestyle="--")<br/>ax = plt.plot(var_cumu)</span></pre><pre class="ns nj ni nk bn nl nm bi"><span id="73b3" class="nn lb iu ni b be no np l nq nr">Number of components explaining 95% variance: 54</span></pre><figure class="kk kl km kn gu ko gi gj paragraph-image"><div class="gi gj nz"><img src="../Images/97bbcde61a56c8aaf6fe31924973d4f8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1224/format:webp/0*ALNKB_tBfY61eiZY.png"/></div><p class="kv kw gk gi gj kx ky bd b be z dk translated">作者图片</p></figure><p id="47de" class="pw-post-body-paragraph ls lt iu lu b lv nu jv lx ly nv jy ma mb nw md me mf nx mh mi mj ny ml mm mn in bi translated">首先，我想指出一些事情。通过打印组件的长度，我们可以看到总共有<code class="fe nf ng nh ni b">3120</code>个组件，显示了组件的数量与图像宽度的关系。</p><pre class="kk kl km kn gu nj ni nk bn nl nm bi"><span id="96b8" class="nn lb iu ni b be no np l nq nr">len(pca.components_)</span></pre><pre class="ns nj ni nk bn nl nm bi"><span id="9a31" class="nn lb iu ni b be no np l nq nr">3120</span></pre><p id="8982" class="pw-post-body-paragraph ls lt iu lu b lv nu jv lx ly nv jy ma mb nw md me mf nx mh mi mj ny ml mm mn in bi translated">通过绘制这个图，我们可以看到曲线是如何戏剧性地向<code class="fe nf ng nh ni b">100%</code>加速，然后变平。疯狂的是，我们只需要用原来<code class="fe nf ng nh ni b">3120</code>组件的<code class="fe nf ng nh ni b">54</code>来解释图像中方差的<code class="fe nf ng nh ni b">95%</code>！这太不可思议了。</p><h1 id="48df" class="la lb iu bd lc ld le lf lg lh li lj lk ka ll kb lm kd ln ke lo kg lp kh lq lr bi translated">利用主成分分析降低维数</h1><p id="5bab" class="pw-post-body-paragraph ls lt iu lu b lv lw jv lx ly lz jy ma mb mc md me mf mg mh mi mj mk ml mm mn in bi translated">我们将使用来自<code class="fe nf ng nh ni b">IncrementalPCA</code>模块的<code class="fe nf ng nh ni b">fit_transform</code>方法，首先找到<code class="fe nf ng nh ni b">54</code>主成分，并转换和表示这些<code class="fe nf ng nh ni b">54</code>新成分中的数据。接下来，我们将使用<code class="fe nf ng nh ni b">inverse_transform</code>方法从这些<code class="fe nf ng nh ni b">54</code>组件中重建原始矩阵。最后，我们将绘制图像，从视觉上评估其质量。</p><pre class="kk kl km kn gu nj ni nk bn nl nm bi"><span id="8316" class="nn lb iu ni b be no np l nq nr">ipca = IncrementalPCA(n_components=k)<br/>image_recon = ipca.inverse_transform(ipca.fit_transform(image_bw))<br/><br/># Plotting the reconstructed image<br/>plt.figure(figsize=[12,8])<br/>plt.imshow(image_recon,cmap = plt.cm.gray)</span></pre><figure class="kk kl km kn gu ko gi gj paragraph-image"><div class="gi gj nt"><img src="../Images/d57d3075e4404bab16be8b25c1115bfb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1252/format:webp/0*r1NgkWfMPy1k0jl0.png"/></div><p class="kv kw gk gi gj kx ky bd b be z dk translated">作者图片</p></figure><p id="56b5" class="pw-post-body-paragraph ls lt iu lu b lv nu jv lx ly nv jy ma mb nw md me mf nx mh mi mj ny ml mm mn in bi translated">我们可以清楚地看到图像的质量降低了，但我们可以将其识别为原始图像。当PCA与机器学习模型(如图像分类)一起应用时，两种训练时间都显著减少，对新数据的预测时间产生几乎一样好的结果，但数据更少。</p><h1 id="0f9f" class="la lb iu bd lc ld le lf lg lh li lj lk ka ll kb lm kd ln ke lo kg lp kh lq lr bi translated">显示k维的其他值</h1><p id="727c" class="pw-post-body-paragraph ls lt iu lu b lv lw jv lx ly lz jy ma mb mc md me mf mg mh mi mj mk ml mm mn in bi translated">接下来，让我们对我们的图像的六个不同的k值进行迭代，显示在每个数字上逐渐提高的图像质量。我们将只去<code class="fe nf ng nh ni b">250</code>组件，仍然只是原始图像的一小部分。</p><pre class="kk kl km kn gu nj ni nk bn nl nm bi"><span id="5455" class="nn lb iu ni b be no np l nq nr">def plot_at_k(k):<br/>    ipca = IncrementalPCA(n_components=k)<br/>    image_recon = ipca.inverse_transform(ipca.fit_transform(image_bw))<br/>    plt.imshow(image_recon,cmap = plt.cm.gray)<br/><br/>ks = [10, 25, 50, 100, 150, 250]<br/><br/>plt.figure(figsize=[15,9])<br/><br/>for i in range(6):<br/>    plt.subplot(2,3,i+1)<br/>    plot_at_k(ks[i])<br/>    plt.title("Components: "+str(ks[i]))<br/><br/>plt.subplots_adjust(wspace=0.2, hspace=0.0)<br/>plt.show()</span></pre><figure class="kk kl km kn gu ko gi gj paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gi gj oa"><img src="../Images/3abf080ac52770b46e9ad55597034da0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*qdTLIyPFH-Tvery-.png"/></div></div><p class="kv kw gk gi gj kx ky bd b be z dk translated">作者图片</p></figure><h1 id="5185" class="la lb iu bd lc ld le lf lg lh li lj lk ka ll kb lm kd ln ke lo kg lp kh lq lr bi translated">结论</h1><p id="a1b5" class="pw-post-body-paragraph ls lt iu lu b lv lw jv lx ly lz jy ma mb mc md me mf mg mh mi mj mk ml mm mn in bi translated">就是这样！少到<code class="fe nf ng nh ni b">10</code>的组件甚至让我们分辨出图像是什么，在<code class="fe nf ng nh ni b">250</code>很难说出原始图像和PCA缩小图像之间的区别。</p><p id="f845" class="pw-post-body-paragraph ls lt iu lu b lv nu jv lx ly nv jy ma mb nw md me mf nx mh mi mj ny ml mm mn in bi translated">PCA是一个非常强大的工具，可以集成到您的工作流程中(通过<a class="ae kz" href="https://scikit-learn.org/stable/auto_examples/compose/plot_column_transformer_mixed_types.html#sphx-glr-auto-examples-compose-plot-column-transformer-mixed-types-py" rel="noopener ugc nofollow" target="_blank">管道</a>)，以大幅减少数据集中<em class="mo">维度</em>的数量，而不会丢失太多信息。请记住，PCA是为连续或数字数据使用而设计的。查看这篇文章，<a class="ae kz" rel="noopener" target="_blank" href="/pca-clearly-explained-how-when-why-to-use-it-and-feature-importance-a-guide-in-python-7c274582c37e"> PCA清楚地解释了</a>，了解更多细节和PCA背后的数学原理。感谢阅读，并享受！</p><p id="1231" class="pw-post-body-paragraph ls lt iu lu b lv nu jv lx ly nv jy ma mb nw md me mf nx mh mi mj ny ml mm mn in bi translated">如果你喜欢阅读这样的故事，并想支持我成为一名作家，可以考虑报名成为一名媒体成员。一个月5美元，让你可以无限制地访问成千上万篇文章。如果你使用<a class="ae kz" href="https://medium.com/@broepke/membership" rel="noopener">我的链接</a>注册，我会赚一小笔佣金，不需要你额外付费。</p></div></div>    
</body>
</html>