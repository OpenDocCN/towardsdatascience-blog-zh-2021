<html>
<head>
<title>Supervised Learning Algorithm Connections Explained</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">监督学习算法连接解释</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/supervised-learning-algorithm-connections-explained-733d2353b7ee?source=collection_archive---------15-----------------------#2021-09-20">https://towardsdatascience.com/supervised-learning-algorithm-connections-explained-733d2353b7ee?source=collection_archive---------15-----------------------#2021-09-20</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/ec58ed1aad38d5d4297187cb65a9a6df.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CtdRg5-cZFNy8-wIF2JvUw.jpeg"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">图片作者。</p></figure><div class=""/><div class=""><h2 id="07dc" class="pw-subtitle-paragraph kf jh ji bd b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw dk translated"><strong class="ak">机器学习模型嵌套——从维恩图到回归、决策树、支持向量机&amp;人工神经网络</strong></h2></div><p id="2289" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">你有没有想过复杂的机器学习技术是如何问世的？纯天才还是多年迭代的产物？让我们采用第二种选择，因为它会让我们都感觉好一些。更重要的是，它将帮助我们成为更好的机器学习实践者。</p><blockquote class="lt lu lv"><p id="9f2c" class="kx ky lw kz b la lb kj lc ld le km lf lx lh li lj ly ll lm ln lz lp lq lr ls im bi translated">“如果说我看得更远，那是因为我站在巨人的肩膀上”——<em class="ji">艾萨克·牛顿，1675 </em></p></blockquote><p id="4e17" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">在这篇文章中，<strong class="kz jj">我将解释监督机器学习技术是如何联系在一起的</strong>，简单的模型嵌套在更复杂的模型中，它们本身嵌入在更复杂的算法中。接下来的将不仅仅是模型的备忘单，不仅仅是监督方法的年表，它将用文字、方程和图表解释机器学习技术的主要家族之间的关系以及它们在偏差-方差权衡难题中的相对位置。</p><p id="3a4a" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">注意:关于无监督学习的例子，参见我的另一篇文章“从零开始的混合建模”，见R 。</p><figure class="mc md me mf gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi mb"><img src="../Images/4b57ba952faaaa8e192247c208798044.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SB0E_--2sevFcree4YJyRg.jpeg"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">嵌套模型的示例:错误的概率密度&lt; Linear regression &lt; Logistic regression &lt; Feedforward neural network &lt; Convolutional neural network (see text for details). Source: Russian doll photo by <em class="mg"> cottonbro </em>来自<a class="ae ma" href="https://www.pexels.com/" rel="noopener ugc nofollow" target="_blank"> <em class="mg"> pexels </em> </a>，由作者修改(文本添加在裁剪版本上)。</p></figure><h2 id="4da4" class="mh mi ji bd mj mk ml dn mm mn mo dp mp lg mq mr ms lk mt mu mv lo mw mx my mz bi translated">从维恩图到最简单的机器学习模型</h2><figure class="mc md me mf gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi na"><img src="../Images/8adb6d7c4580aa573fe9154db0483c00.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KEeF2ZtcRbg0LCmgYNROuA.jpeg"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">图片作者。</p></figure><p id="74d4" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">重构机器学习技术的历史让我们回到<strong class="kz jj">概率论</strong>。概率论可以从Kolmogorov公理或简单地从<strong class="kz jj">文氏图</strong>中推导出来。麦克道尔的《<em class="lw">破解编码访谈</em>》对此做了最好的解释。我们有两个事件A和b，两个圆圈的面积代表它们的概率。重叠的区域就是事件{A和B}。我们直接得到P(A和B) = P(A)×P(B给定A ),因为我们需要事件A发生以及事件B给定A已经发生。A和B可以互换，所以我们也有P(A和B) = P(B)×P(A给定B)。瞧，我们通过结合两个关系得到<strong class="kz jj">贝叶斯定理</strong>:P(A | B)= P(A)×P(B | A)/P(B)。自然，朴素贝叶斯分类器也随之而来。</p><p id="5e98" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">现在，事件{A或B}的概率是多少？在维恩图上，我们观察到它是A的面积和B的面积之和，然而，为了避免重复计算重叠面积，我们有P(A或B) = P(A) + P(B)-P(A和B)。正如我们现在将发现的，两个逻辑关系AND和OR导致二项式分布，然后导致正态分布，这将是线性回归的基础。</p><p id="8def" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">让我们深入探讨一下。<strong class="kz jj">二项式分布</strong>采用以下形式</p><figure class="mc md me mf gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nb"><img src="../Images/d40142ab55dd62acc7839285c630e069.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kiY4uMEjNluEse_WhbUBVQ.png"/></div></div></figure><p id="b814" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">在使用二项式系数的<em class="lw"> n </em>次独立伯努利试验中，准确获得<em class="lw"> k </em>次成功的概率是多少</p><figure class="mc md me mf gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nb"><img src="../Images/5200ca7d90f08629cc56f35421948d25.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Qf9d3NPtrQMshHeNnSUTFg.png"/></div></div></figure><p id="590d" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">由帕斯卡三角形描述(见上图)。乍一看，这看起来很复杂。然而，它可以很容易地从文氏图中推导出来。独立伯努利试验意味着事件a和b是独立的，因此b不以a为条件，a也不以b为条件。从上面可以得出，P(A和B) = P(A)×P(B)，或者，在本实验中，P(A <em class="lw"> k </em>次，而不是A ( <em class="lw"> n </em> - <em class="lw"> k </em>)次，它等于<em class="lw"> p^k </em>次(1- <em class="lw"> p </em> )^( <em class="lw"> n </em>如果我们拿一个硬币来说，有<em class="lw"> k </em>正面(事件A)和<em class="lw"> n </em> - <em class="lw"> k </em>反面(事件B，即不是A)，这种事件有不同的实现方式，用二项式系数来表示。举例来说，假设在3次试验中成功了2次，我们可能有{H，H，T}，{H，T，H}或{T，H，H}。所有那些路径都是互斥的，给出P(A '或B') = P(A') + P(B ')，或者3乘以<em class="lw"> p^k </em>乘以(1-<em class="lw">p</em>)^(<em class="lw">n</em>-<em class="lw">k</em>)，这就是二项分布！假设<em class="lw"> n </em>足够大，则<strong class="kz jj">正态分布</strong></p><figure class="mc md me mf gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nb"><img src="../Images/5cfe6c6489354684c15187ec5165befc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wndULUXuhEVTUlADTUdA0g.png"/></div></div></figure><p id="0286" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">是具有均值<em class="lw"> np </em>和方差<em class="lw"> np </em> (1- <em class="lw"> p </em>)的二项分布的良好近似(演示不在这篇短文的范围之内)。</p><figure class="mc md me mf gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nc"><img src="../Images/966bbe9a9d2f66fc4dd5ebcd80129aa3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mQejZHGdEClICESadKtzhA.jpeg"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">大<em class="mg"> n </em>和小<em class="mg"> p </em>的二项分布(离散)和正态分布(连续)。图片作者。</p></figure><p id="bd86" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">机器学习和统计的主力是<strong class="kz jj">线性回归</strong></p><figure class="mc md me mf gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nb"><img src="../Images/6a6d1be6de3bdc45949c739476a799c2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VhStTMnxUAhI-6TgA0E3xQ.png"/></div></div></figure><p id="b344" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">为了训练该模型，要最小化的误差函数是残差平方和(观察值和预测值之间的差)。正是高斯在19世纪早期成功地将最小二乘法与概率原理和正态分布(具有剩余高斯误差)联系起来。正态分布和线性回归之间的联系在线性回归的概率公式中变得很清楚:</p><figure class="mc md me mf gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nb"><img src="../Images/f3deaf48df875ec4c2d045bd914e646d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Dj4Cf05q8Wv08UJQGCwGRw.png"/></div></div></figure><p id="a3a8" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">请注意我们是如何从一个简单的维恩图达到这个阶段的！</p><figure class="mc md me mf gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nd"><img src="../Images/eebd81666d2f66fb8a9fc265688cd8e0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aVi4cD3vYEqOkUEfB4Zl2w.jpeg"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">噪声服从正态分布的线性回归。图片作者。</p></figure><figure class="mc md me mf gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ne"><img src="../Images/fe7a7cbf24d97140007b1f2a034c5bfa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*19M5rWys6AqgJXW6VyemOg.jpeg"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">基于熵的决策树概念。图片作者。</p></figure><p id="e69f" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">让我们回到概率论来发展我们成长之树的另一个分支(在这个隐含的剧透中没有双关语；参见下面的图表)。香农在1948年将<strong class="kz jj">熵</strong> <em class="lw"> H </em>定义为-p_i×log_2[p_i]之和，这是一个纯度的度量(也是一个惊奇的度量)。上两节课<em class="lw"> y </em> =(🔴,🔵)，集合的熵{🔴,🔴,🔵,🔵,🔴}为增加<em class="lw"> x </em>为-p(🔴)×log_2[p(🔴)]-p(🔵)×log_2[p(🔵)]或者这里-3/5×log _ 2[3/5]-2/5×log _ 2[2/5]≈0.97，非常不纯(集合纯时熵从0到相反情况下的1)。然后<strong class="kz jj">决策树</strong>包括在给定的<em class="lw"> x </em>处进行分裂，使得2个子集的组合熵最小化，这是在{{🔴,🔴}, { 🔵,🔵,🔴}}.这是通过<strong class="kz jj">信息增益</strong>的概念来实现的，信息增益是集合熵减去划分子集的熵的加权和。这里，0.97-(2/5×[-1×log _ 2(1)]+3/5×[-1/3×log _ 2(1/3)-2/3×log _ 2(2/3)])≈0.42，这表明由分裂造成的熵的实质性减少。为了清楚起见，决策树分裂的目标是最大化信息增益。这在Provost &amp; Fawcett的书《商业数据科学<em class="lw">》中有很好的解释。从那里，通过bagging(通过bootstrapping)和boosting，可以开发一组基于决策树的技术，包括<strong class="kz jj">随机森林</strong>和<strong class="kz jj"> Boosted树</strong>。</em></p><figure class="mc md me mf gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/422f44b6928fa3e5949b48404fdc2265.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8ecKihAIBIR9OEBzZeBXYA.jpeg"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">嵌套监督学习模型-第一部分。作者图片。</p></figure><h2 id="94d1" class="mh mi ji bd mj mk ml dn mm mn mo dp mp lg mq mr ms lk mt mu mv lo mw mx my mz bi translated">从线性回归到其他一切！</h2><p id="662d" class="pw-post-body-paragraph kx ky ji kz b la nf kj lc ld ng km lf lg nh li lj lk ni lm ln lo nj lq lr ls im bi translated">现在让我们从线性回归分支展开进化树。首先，结合线性回归和贝叶斯定理产生了几种机器学习技术。这在墨菲2012年的“<em class="lw">机器学习的概率视角</em>”中有很好的描述。让我们首先将贝叶斯定理改写为P(模型|观测值)= P(模型)×P(观测值)/ P(观测值)，或者换句话说，后验概率等于先验概率乘以似然概率，用边际函数归一化。线性回归以前被形式化为一个似然函数，忽略了均匀先验。<strong class="kz jj">正则化技术</strong>(最小化过度拟合)包括在<strong class="kz jj">脊</strong>模型的情况下从均匀先验分布移动到正态先验分布，在<strong class="kz jj">套索</strong>模型的情况下移动到拉普拉斯先验分布。这些技术对非线性回归特别有用；因此，让我们添加<strong class="kz jj">基函数展开</strong>的概念，以从线性回归移动到非线性回归，例如使用<strong class="kz jj">多项式回归</strong>:</p><figure class="mc md me mf gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nb"><img src="../Images/363246fec85ee5eb9cbc0ff1c1b6b92b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fq9-ErmlEy1C_sZAjLhFuA.png"/></div></div></figure><p id="4e7e" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">随着</p><figure class="mc md me mf gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nb"><img src="../Images/113dcefea94b4df6d8c7534a9fb00d9c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HYg7JapuYuYannxyeKl_zw.png"/></div></div></figure><p id="c3f5" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">下图说明了多项式回归以及通过山脊和套索进行正则化的作用。</p><figure class="mc md me mf gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nk"><img src="../Images/932a5f0d76fc1cc4b24bb28d5f47cb79.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_GWz-L3JmWQOPS1cOWIwjQ.jpeg"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">由脊或套索调整的过拟合多项式回归。图片作者。</p></figure><figure class="mc md me mf gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nl"><img src="../Images/765b89a83011016f023af0cbb402657d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WqAWE-Uv50MLpcdmgLh7EA.jpeg"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">最大边缘超平面和分离两类的SVM的边缘。图片作者。</p></figure><p id="1b24" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">仍然有两个机器学习家族要从线性回归中定义:支持向量机和人工神经网络。<strong class="kz jj">支持向量机</strong> (SVM)是一个线性模型，它通过一个所谓的最大间隔超平面将两个类别分开，最大间隔超平面可以写成满足以下等式的点集<em class="lw"> x </em>:</p><figure class="mc md me mf gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nb"><img src="../Images/eb18e8858ced889cceebb6056b8d8cbc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3gUara6kAxpRoC7u7GRtVQ.png"/></div></div></figure><p id="2576" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">将两类数据分开以使它们之间的距离尽可能大的两个平行超平面被定义为</p><figure class="mc md me mf gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nb"><img src="../Images/dc381bda47c4e87b6f226721ac0c14a1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jPsUlnE54wQ_fcf9B4vDUQ.png"/></div></div></figure><p id="fd5e" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">叫做利润。不用深入SVM的细节，我们马上就能看到它与线性回归的联系。</p><p id="77de" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">回想一下，线性回归被定义为正态分布的可能性。如果我们现在用伯努利分布(二项式分布的特殊情况)代替正态分布，我们得到<strong class="kz jj">逻辑回归</strong>:</p><figure class="mc md me mf gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nb"><img src="../Images/5ced3f46a4c1a004bc23a62857ef2eab.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GDZQSb1FtHl-DleXA1Kdow.png"/></div></div></figure><p id="da13" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">这里，在sigmoid函数中实现线性回归，以确保结果本质上是概率性的，即在0和1之间，具有类别而不是连续值。</p><figure class="mc md me mf gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nd"><img src="../Images/d0c2752d8206773950ead504352cade8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nuBkRQZHAzR5i5vgwlBhzg.jpeg"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">图片作者。</p></figure><p id="f560" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">现在，逻辑回归类似于一个人工神经元，连接几个神经元导致<strong class="kz jj">人工前馈神经网络</strong>。不同的特征排列和网络架构最终导致<strong class="kz jj">卷积神经网络</strong>、<strong class="kz jj">递归神经网络</strong>等深度学习模型。</p><figure class="mc md me mf gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nm"><img src="../Images/86970bba4acbc6da9338ff255c028d19.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MK-02WHBgQ2Wk35LeKatAw.jpeg"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">图片作者。</p></figure><figure class="mc md me mf gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/4686e7cab009eaba54f4c69ac6a81a91.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*J_lUswwqoN9PsZizRLXXXg.jpeg"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">嵌套监督学习模型-第二部分。图片作者。</p></figure><h2 id="1372" class="mh mi ji bd mj mk ml dn mm mn mo dp mp lg mq mr ms lk mt mu mv lo mw mx my mz bi translated">监督学习的进化树</h2><p id="22e1" class="pw-post-body-paragraph kx ky ji kz b la nf kj lc ld ng km lf lg nh li lj lk ni lm ln lo nj lq lr ls im bi translated">既然已经描述了所有的连接，让我们在最后一张图中概括所有的内容。就像生命的进化树一样，我们现在可以清楚地看到有监督的机器学习模型的嵌套。它有助于我们理解不同技术之间的联系，但最重要的是，它为我们在探索偏差/方差权衡时测试哪一系列模型提供了一些指导。</p><figure class="mc md me mf gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/af9a76d774dd6dd3707c6c6ca9b25c5c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*y_RRZTymPrn84Mu29l_x1w.jpeg"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">作为进化树的嵌套监督学习模型。图片作者。</p></figure><h2 id="64a3" class="mh mi ji bd mj mk ml dn mm mn mo dp mp lg mq mr ms lk mt mu mv lo mw mx my mz bi translated">参考</h2><p id="c645" class="pw-post-body-paragraph kx ky ji kz b la nf kj lc ld ng km lf lg nh li lj lk ni lm ln lo nj lq lr ls im bi translated">[1] G.L .麦克道尔，《破解编码访谈<em class="lw">》(2015)，第6版。，CareerCup，687页。</em></p><p id="2881" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">[2] K.P .墨菲，《<em class="lw">机器学习的概率视角</em>》(2012)，麻省理工学院出版社，1104页</p><p id="1f0f" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">[3] F. Provost &amp; T. Fawcett，《<em class="lw">商业数据科学</em>》(2013年)，奥赖利媒体，414页</p></div></div>    
</body>
</html>