<html>
<head>
<title>Deep In Singular Value Decomposition</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">深入奇异值分解</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/deep-in-singular-value-decomposition-98cfd9532241?source=collection_archive---------23-----------------------#2021-04-30">https://towardsdatascience.com/deep-in-singular-value-decomposition-98cfd9532241?source=collection_archive---------23-----------------------#2021-04-30</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="50b6" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph">从头做起</h2><div class=""/><div class=""><h2 id="9db8" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">数据科学中分解策略的概述，用代码解释。</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/03187c7b25a5eca3e18c8e118ec77cc1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cMeSk71TPJgduaW7CDN1uw.jpeg"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">(src =<a class="ae lh" href="https://pixabay.com/images/id-3926174/" rel="noopener ugc nofollow" target="_blank">https://pixabay.com/images/id-3926174/</a>)(懂吗？分解？)</p></figure><h1 id="b488" class="li lj it bd lk ll lm ln lo lp lq lr ls ki lt kj lu kl lv km lw ko lx kp ly lz bi translated">介绍</h1><p id="6390" class="pw-post-body-paragraph ma mb it mc b md me kd mf mg mh kg mi mj mk ml mm mn mo mp mq mr ms mt mu mv im bi translated">在数据科学领域工作时，我们经常会遇到计算机很难解释的特征。这通常是因为数据的维度太大，计算机无法估算。这可能会有问题，因为很难观察一个特征的数百个维度。这是人类的情况，当然也包括计算机。</p><p id="2a1e" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">幸运的是，这个问题有一个解决方案。在数据科学的世界中，为了使这些值更容易被统计、机器学习和分析策略所识别，使用所谓的分解是很常见的。在大多数情况下，程序员会使用一种叫做奇异值分解的分解策略。然而(SVD)，也有其他解决方案可以使用，如随机投影。在这两种解决方案中，SVD使用得更广泛，因为随机投影需要点之间合适的维度空间，因此有时会在某些特征上出现问题。出于这个原因，本文将全部讨论SVD以及用Python从头开始编写SVD。然而，如果你想学习更多关于随机投影的知识，以及它在数据科学中是如何运作的，我也有一篇文章可以让你阅读。</p><div class="nb nc gp gr nd ne"><a rel="noopener follow" target="_blank" href="/random-projection-and-its-role-in-data-science-f253dd66485b"><div class="nf ab fo"><div class="ng ab nh cl cj ni"><h2 class="bd jd gy z fp nj fr fs nk fu fw jc bi translated">随机投影及其在数据科学中的作用</h2><div class="nl l"><h3 class="bd b gy z fp nj fr fs nk fu fw dk translated">随机投影概述及其在现代数据科学实践中通常扮演的角色。</h3></div><div class="nm l"><p class="bd b dl z fp nj fr fs nk fu fw dk translated">towardsdatascience.com</p></div></div><div class="nn l"><div class="no l np nq nr nn ns lb ne"/></div></div></a></div></div><div class="ab cl nt nu hx nv" role="separator"><span class="nw bw bk nx ny nz"/><span class="nw bw bk nx ny nz"/><span class="nw bw bk nx ny"/></div><div class="im in io ip iq"><h1 id="2f4a" class="li lj it bd lk ll oa ln lo lp ob lr ls ki oc kj lu kl od km lw ko oe kp ly lz bi translated">关于奇异值分解的更多信息</h1><p id="cd84" class="pw-post-body-paragraph ma mb it mc b md me kd mf mg mh kg mi mj mk ml mm mn mo mp mq mr ms mt mu mv im bi translated">为了最小化特征的维数，SVD无疑是数据科学家最常用的方法。这是因为SVD是我们所能使用的最通用、最古老的分解方法。SVD是许多需要解释高维数据的模型的基础。如果不使用SVD，我们就不太可能拥有自动驾驶汽车或图像识别，因为这些功能太多了，计算机无法在不利用分解的情况下真正读取，而分解通常会发生在SVD上。</p><p id="ccd1" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">为什么不用随机投影？答案实际上很简单，尽管随机投影在技术上更有效，并且应该在计算机上运行得更快，但是对于计算机来说，SVD也很容易执行。考虑到这一点，大部分时间确实没有使用随机投影的好理由，使用随机投影的风险是数据的维度可能不够高维。这可能会导致严重的问题，因为您的要素会被分解，但并不代表原始数据。</p><h1 id="1c45" class="li lj it bd lk ll lm ln lo lp lq lr ls ki lt kj lu kl lv km lw ko lx kp ly lz bi translated">求矩阵的奇异值分解</h1><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi of"><img src="../Images/22d5c402d1b0c618356c89e17dddd367.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qDtiDqks-v2bhZOBSpKU8Q.png"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">(<a class="ae lh" href="https://en.wikipedia.org/wiki/Singular_value_decomposition#/media/File:Singular-Value-Decomposition.svg" rel="noopener ugc nofollow" target="_blank"> src =维基百科</a>)</p></figure><p id="a09d" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">为了弄清楚矩阵的奇异值分解，我们必须首先弄清楚我们到底要计算什么。奇异值分解可以表示为，V^t.是一个对角矩阵。现在让我们看看如何得到V矩阵。</p><p id="f503" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">v代表X转置X的特征向量集，当然我是数据科学家，所以我经常喜欢用代码思考。我们要写一个函数，用Julia编程语言来计算所有这些。</p><p id="0287" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">这个矩阵需要是正交的。不使用正交矩阵也可以做到这一点，但我们只是在函数中为此做了准备。在Julia中，我们可以使用基本模块LinearAlgebra，以便使用eigvecs()方法计算特征向量。我们可以用乘法运算符*将矩阵相乘。为了计算SVD的V，我们将转置的A乘以A，然后得到其特征向量。</p><pre class="ks kt ku kv gt og oh oi oj aw ok bi"><span id="a007" class="ol lj it oh b gy om on l oo op">using LinearAlgebra: eigvecs</span><span id="077d" class="ol lj it oh b gy oq on l oo op">V = eigvecs(transpose(a) * a)</span></pre><p id="af9d" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">接下来，我们将计算sigma。适马很简单，从原始矩阵中得到特征值，我们可以用线性代数中的八分函数来实现。得到这些特征值后，我们需要求它们的平方根。</p><pre class="ks kt ku kv gt og oh oi oj aw ok bi"><span id="cfdd" class="ol lj it oh b gy om on l oo op">using LinearAlgebra: eigvals<br/>Σ = eigvals(a)</span></pre><p id="6b7d" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">最后，我们需要找到我们的U矩阵。为此，我喜欢使用reduce()方法，以及这个迭代循环的水平连接:</p><pre class="ks kt ku kv gt og oh oi oj aw ok bi"><span id="68d4" class="ol lj it oh b gy om on l oo op">U = reduce(hcat,[W[:,i]/S[i] <strong class="oh jd">for</strong> i=1:size(W,2)])</span></pre><p id="6ac2" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">现在我们有了三个矩阵，我们可以简单地把它们代入奇异值分解公式。在数学中，我们可以这样表示:</p><p id="eddf" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">uσv^t</p><p id="9a02" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">在朱丽亚，我们把它变成了</p><pre class="ks kt ku kv gt og oh oi oj aw ok bi"><span id="781d" class="ol lj it oh b gy om on l oo op">U .* Σ .* transpose(V)</span></pre><h1 id="4c6b" class="li lj it bd lk ll lm ln lo lp lq lr ls ki lt kj lu kl lv km lw ko lx kp ly lz bi translated">结论</h1><p id="5855" class="pw-post-body-paragraph ma mb it mc b md me kd mf mg mh kg mi mj mk ml mm mn mo mp mq mr ms mt mu mv im bi translated">降维是数据科学家用来解释有太多属性需要观察的数据的基本策略。最流行的降维方法SVD的伟大之处在于，它遵循一些相对基本的线性代数概念，保持相对良好的性能，同时也非常有效。考虑到这一点，所有数据科学家可能都应该研究一下这种方法，它提供的关于数据的知识很可能会在数据科学工作中派上用场。降维的目标永远是标准形式，即数据或数学表达式的最简单版本。如果你想了解更多关于标准形式的内容，你可以在这里阅读我的另一篇文章:</p><div class="nb nc gp gr nd ne"><a rel="noopener follow" target="_blank" href="/what-on-earth-is-canonical-form-23cef915601d"><div class="nf ab fo"><div class="ng ab nh cl cj ni"><h2 class="bd jd gy z fp nj fr fs nk fu fw jc bi translated">“规范形式”到底是什么？</h2><div class="nl l"><h3 class="bd b gy z fp nj fr fs nk fu fw dk translated">计算中本征向量、值和整体分解的数学概述。</h3></div><div class="nm l"><p class="bd b dl z fp nj fr fs nk fu fw dk translated">towardsdatascience.com</p></div></div><div class="nn l"><div class="or l np nq nr nn ns lb ne"/></div></div></a></div><p id="a388" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">谢谢你看我的文章！我希望这是一篇信息丰富的文章，让读者更好地理解SVD及其在数据科学中的应用！</p></div></div>    
</body>
</html>