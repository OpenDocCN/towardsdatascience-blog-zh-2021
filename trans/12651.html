<html>
<head>
<title>The Naive Bayes classifier</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">朴素贝叶斯分类器</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/the-naive-bayes-classifier-how-it-works-e229e7970b84?source=collection_archive---------7-----------------------#2021-12-29">https://towardsdatascience.com/the-naive-bayes-classifier-how-it-works-e229e7970b84?source=collection_archive---------7-----------------------#2021-12-29</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="a446" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">通过简单的例子解释了朴素贝叶斯算法。</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="gh gi kl"><img src="../Images/8a93d7f5e79d0477c2b233b7a365f656.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CG0oXRqe8ojZ8SfRG2PAZw.png"/></div></div><p class="kx ky gj gh gi kz la bd b be z dk translated">作者图片</p></figure><p id="df96" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">内容:</strong></p><p id="4af8" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><em class="lb">简介</em></p><p id="6911" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><em class="lb"> 1。贝叶斯定理</em></p><p id="9a8c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><em class="lb"> 2。朴素贝叶斯分类器</em></p><p id="4369" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><em class="lb"> 3。一个简单的二元分类问题</em></p><p id="5e71" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><em class="lb"> 3.1 先验概率计算</em></p><p id="380e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><em class="lb"> 3.2 类条件概率计算</em></p><p id="af80" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><em class="lb"> 3.3 预测后验概率</em></p><p id="1db0" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><em class="lb"> 3.4 用连续数据处理特征</em></p><p id="035b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><em class="lb"> 3.5 处理不完整数据集</em></p><p id="fe6a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><em class="lb"> 4。使用 Scikit Learn 的朴素贝叶斯</em></p><p id="8048" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><em class="lb"> 4.1 处理混合特征</em></p><p id="8fa9" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><em class="lb"> 5。结论</em></p><p id="47b8" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">6。参考文献</p><p id="ec89" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">简介:</strong></p><p id="5169" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">分类算法试图预测分类目标变量的类别或标签。分类变量通常代表具有离散值的定性数据，如通过/失败或低/中/高等。在许多分类算法中，朴素贝叶斯分类器是最简单的分类算法之一。在其他应用程序中，朴素贝叶斯分类器通常用于大型文本数据集。本文的目的是解释朴素贝叶斯算法是如何工作的。朴素贝叶斯分类器基于贝叶斯定理，这将在下面讨论。</p><p id="86e4" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir"> 1.0 贝叶斯定理:</strong></p><p id="2fb8" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">假设进行了一项关于购买超高清电视的客户调查。调查结果以列联表的形式呈现如下:</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="gh gi lc"><img src="../Images/ebebced78a401ba0c301cee28b7db2c1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1050/format:webp/1*E8TeBZh-OAZfB3uvFWmk-A.png"/></div><p class="kx ky gj gh gi kz la bd b be z dk translated">表 1:列联表</p></figure><p id="76e0" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">为简单起见，列联表中的类别用字母表示如下:</p><p id="4269" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">答:计划购买</p><p id="b03d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">b:实际购买</p><p id="74d7" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">答`:不打算购买</p><p id="e397" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">b `:实际上没有购买</p><p id="a9be" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">基于上述符号，列联表中的信息也可以用文氏图的形式表示。</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="gh gi ld"><img src="../Images/15a422859ac8fb8d5dd14e5637c7c8a9.png" data-original-src="https://miro.medium.com/v2/resize:fit:940/format:webp/1*o1xXXTKNcLcLkzPN4jw7kw.png"/></div><p class="kx ky gj gh gi kz la bd b be z dk translated">作者图片</p></figure><p id="3ff0" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">P(A)计划购买的概率= 350/1100。</p><p id="e7d0" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">P(B)实际购买的概率= 450/1100</p><p id="83c3" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">P(A∩B)计划购买和实际购买的概率= 300/1100</p><p id="23a7" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">请注意，在计算 P(B)时，我们没有考虑客户是否打算购买。顾客计划购买的先验知识改变了顾客实际购买的可能性。这也被称为假设顾客计划购买的实际购买的条件概率。它被表示为 P(B|A)。</p><p id="1475" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">P(B|A) = 𝑃(𝐴∩𝐵)/𝑃(𝐴) = 300/350，即在计划购买的总实例中，实际购买了多少。</p><p id="3994" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这个等式可以改写为</p><p id="87bd" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi">𝑃(𝐴 ∩ 𝐵) = 𝑃(𝐵|𝐴)∗𝑃(𝐴)</p><p id="f2c4" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们也可以写出假设客户实际购买了 P(A|B) = 𝑃(𝐵∩𝐴)/ 𝑃(𝐵)= 300/450 的计划购买的条件概率。</p><p id="a0e8" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们可以将此改写为:</p><p id="6a7f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">𝑃(𝐵 ∩ 𝐴) = P(A|B)*P(B)</p><p id="bd2c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">由于 P(A∩B) = P(B∩A ),我们可以使上述两个方程的右边相等，从而得到:</p><p id="2879" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">𝑃(𝐵|𝐴)∗𝑃(𝐴)=𝑃(A|B)∗𝑃(B)</p><p id="9728" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这个等式可以改写成贝叶斯定理:</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="gh gi le"><img src="../Images/7a41dfe24ec93c6421600f432e6fe2ea.png" data-original-src="https://miro.medium.com/v2/resize:fit:442/format:webp/1*MSZehsC_gjQnJ6i6ZYntiQ.png"/></div></figure><p id="5430" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">如果 a 和 b 是独立事件，那么𝑃(𝐴∩𝐵)=𝑃(𝐴)÷𝑃(𝐵).</p><p id="68ad" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">因此，当 A 和 B 是独立事件时，P(A|B) = P(A)</p><p id="af9d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir"> 2.0 朴素贝叶斯分类器:</strong></p><p id="2f89" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">让我们数据集中的特征向量由𝑨 = (𝐴₁,𝐴₂,…，𝐴ₙ).目标向量是具有值𝐵ᵢ，i= 1，2，…k 的分类向量，其中 k 是目标向量的类别/标签总数。</p><p id="fd01" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">给定一个新的特征向量𝐴₁,𝐴₂,…，𝐴ₙ，朴素贝叶斯分类器预测概率 P(𝐵ᵢ| (𝐴₁,𝐴₂,…，𝐴ₙ)，i=1，2，…。k，即给定特征向量的目标向量的每个类别/标签的条件概率。这种条件概率也称为后验概率，可以用简洁的形式写成𝑃(𝐵ᵢ| 𝑨，i=1，2，…k。然后，朴素贝叶斯分类器将后验概率最高的类/标签 I 选为最可能的结果。</p><p id="0425" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">使用贝叶斯定理计算类别的后验概率:</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="gh gi lf"><img src="../Images/76667290887a709f8ce745da26ee58c3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1142/format:webp/1*lz71lvvCLbQUGvhra-cqbw.png"/></div></figure><p id="2f4f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在上面的等式中，分母 P(𝐴₁,𝐴₂,…，𝐴ₙ)对于所有类别𝐵ᵢ都是相同的，i= 1，2，…k。因此，为了计算类别的后验概率，我们可以忽略它，只需计算分子 P( <strong class="jp ir"> A </strong> | 𝐵ᵢ)和 p(𝐵ᵢ)I = 1，2…k 中的各项。下面将解释这些项的计算方法。</p><p id="fbf3" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">术语 P(𝐵ᵢ)也称为类别 I 的先验概率。设 m 是数据集中数据点的总数，𝑁Bⱼ是数据集中类别𝐵ⱼ的实例数，则对于第 j 个类别:</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="gh gi lg"><img src="../Images/7706f6a537ac27e9dc0ef5d6b07b4f73.png" data-original-src="https://miro.medium.com/v2/resize:fit:266/format:webp/1*Shp3VnUrrhkfvHxunvVpTw.png"/></div></figure><p id="1b5c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">因此，为了计算先验概率，我们需要计算数据集中每个类的实例数，并将其除以数据集中的数据点总数。</p><p id="a5d8" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">为了计算项 P( <strong class="jp ir"> A </strong> | 𝐵ⱼ)，朴素贝叶斯分类器假设特征彼此独立，即特征𝐴ₛ的出现不受特征𝐴ᵣ.的出现的影响这个假设并不适用于所有情况，因此这个分类器被命名为朴素贝叶斯分类器。</p><p id="15f6" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">特征独立性的假设产生:</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="gh gi lh"><img src="../Images/dd105205c94b1abfe409c0089b6aecfe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fjRPfDh_lybRELOHHF5P4g.png"/></div></div></figure><p id="e012" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">P(𝐴ₛ|𝐵ᵢ)也称为类条件概率。计算和存储类别条件概率是任何朴素贝叶斯分类器执行的关键任务之一。</p><p id="74b7" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">假设在一个数据集中有三个分类特征𝐴₁,𝐴₂和 A₃。𝐴₁要素有 2 个等级/标签，𝐴₂要素有 3 个等级/标签，A₃要素也有 2 个等级/标签。总的特征标号是 2+3+2=7。数据集中的目标变量是一个二元分类变量，即只有 2 个类别。对于这样的数据集，我们需要计算和存储 7*2=14 个类别条件概率。在预测阶段查找这些概率。在下一节中，将使用一个简单的示例来解释先验和类别条件概率以及后验概率的计算。</p><p id="ee73" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir"> 3.0 一个简单的二元分类问题:</strong></p><p id="952c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这个虚构的示例数据集包含与事故相关的不同情况的示例。目标变量 accident 是一个带有是/否值的二元分类变量。有 4 个分类特征:天气状况、道路状况、交通状况和发动机问题。</p><p id="2781" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">下面列出了与每个功能相关的类别/标签:</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="gh gi li"><img src="../Images/9f0aa5bd700bc58db6a742cfc6e4b119.png" data-original-src="https://miro.medium.com/v2/resize:fit:750/format:webp/1*-98BNMyw4765fFurY0BVJg.png"/></div><p class="kx ky gj gh gi kz la bd b be z dk translated">表 2:要素类</p></figure><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="gh gi lj"><img src="../Images/004524cf70f4c13bc73b8349ee44ee30.png" data-original-src="https://miro.medium.com/v2/resize:fit:1070/format:webp/1*6Vbud53EZ7OnnANcJ64FBA.png"/></div><p class="kx ky gj gh gi kz la bd b be z dk translated">表 3:示例数据</p></figure><p id="38a7" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir"> 3.1 先验概率计算:</strong></p><p id="57d3" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">有 10 个数据点(m = 10)。有 5 个分类/标签“是”(𝑁Accidentᵧₑₛ = 5)，5 个分类/标签“否”(𝑁Accidentₙₒ = 5)。可以使用第 2.0 节中的先验概率公式计算先验概率:</p><p id="86dd" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">𝑃(Accidentᵧₑₛ) = 5/10</p><p id="bbd8" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">𝑃(Accidentₙₒ) = 5/10</p><p id="2ec3" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir"> 3.2 类条件概率计算:</strong></p><p id="6320" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">首先根据目标标签(是/否)分割数据集。因为目标变量有 2 个类，所以我们得到 2 个子表。如果目标变量有 3 个类，我们将得到 3 个子表，每个类一个。</p><p id="9c8b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">以下两个表分别显示了目标类/标签“否”和“是”的数据集:</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="gh gi lk"><img src="../Images/69134bc9fc9dd06e76227dc040bd4c46.png" data-original-src="https://miro.medium.com/v2/resize:fit:1068/format:webp/1*ppNy2mexXrewA0MGXaQLAA.png"/></div><p class="kx ky gj gh gi kz la bd b be z dk translated">表 4:目标标签“否”的子表</p></figure><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="gh gi ll"><img src="../Images/9381646d11ea89428e53f2218e29e3f1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1066/format:webp/1*WyimkuJ1-JhduqdCoDtGpQ.png"/></div><p class="kx ky gj gh gi kz la bd b be z dk translated">表 5:目标标签“是”的子表</p></figure><p id="8a25" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">类别条件概率𝑃(𝑨 |𝐵ₙₒ和𝑃(𝑨 |𝐵ᵧₑₛ可以使用如下所示的表 4 和表 5 来计算:</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="gh gi lm"><img src="../Images/17817cca958b64d9deb3a2f60f384b4e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1346/format:webp/1*I1q6wFJanBlUXDErtdbOZQ.png"/></div><p class="kx ky gj gh gi kz la bd b be z dk translated">表 6:天气条件的分类条件概率</p></figure><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="gh gi ln"><img src="../Images/d44716764ecac38b0f41ddeaa43797a5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1348/format:webp/1*DBWzqXLthce5B7kRIsaxIQ.png"/></div><p class="kx ky gj gh gi kz la bd b be z dk translated">表 7:路况的等级条件概率</p></figure><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="gh gi lo"><img src="../Images/a96c262f2ed19b242ebcffa67c137aa9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1380/format:webp/1*VIi5nzt05TMNwmzscIR9_g.png"/></div><p class="kx ky gj gh gi kz la bd b be z dk translated">表 8:交通状况的类别条件概率</p></figure><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="gh gi ln"><img src="../Images/ed85facbfe75ed1b89b18f440f94f3c6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1348/format:webp/1*6uTAh3LroIVXbvFI9f6mmQ.png"/></div><p class="kx ky gj gh gi kz la bd b be z dk translated">表 9:引擎问题的类条件概率</p></figure><p id="d63e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">虽然类条件概率的计算很简单，但是条件概率应该被系统地组织，因为在预测阶段需要查找这些概率。</p><p id="4367" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir"> 3.3 预测后验概率:</strong></p><p id="1f58" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">假设我们现在有了一个新的特征向量:</p><p id="77d1" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">天气状况:下雨</p><p id="6045" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">路况:良好</p><p id="c775" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">交通状况:正常</p><p id="8d0b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">发动机问题:没有</p><p id="b341" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">任务是预测事故是否会发生？</p><p id="3ce0" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">使用第 2.0 节中的后验概率公式计算每个目标类别的后验概率。</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="gh gi lp"><img src="../Images/a9882879483933b42ca4243b12a2ef0f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1342/format:webp/1*bb3i9K8afiHsUa86vqAkDQ.png"/></div></figure><p id="8a9d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">注意，如第 2.0 节所述，分母的计算被省略。代入上表 6、7、8 和 9 中的先验概率和类别条件概率的值，我们得到:</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="gh gi lq"><img src="../Images/333506356664cf2dcb439ad4821f1e9e.png" data-original-src="https://miro.medium.com/v2/resize:fit:856/format:webp/1*_052Sqq4si4oeoPpUpC1zw.png"/></div></figure><p id="9c29" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">自𝑃(accidentₙₒ|𝐴ₙₑ𝓌)&gt; 𝑃(accidentᵧₑₛ|𝐴ₙₑ𝓌)的预言是偶然的= '不'。</p><p id="f583" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">概率可以通过归一化后验概率来获得:</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="gh gi lr"><img src="../Images/17ff28c96a0344e38028d6638ff95109.png" data-original-src="https://miro.medium.com/v2/resize:fit:812/format:webp/1*5Wc25r-LDyqg2PI00TCvdg.png"/></div></figure><p id="5313" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir"> 3.4 用连续数据处理特征:</strong></p><p id="4f4a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这个玩具示例中的数据集只有分类变量。如果数据集具有不断变化的要素会怎样？例如，假设我们有一个记录事故发生时温度的特征温度。这种特征的类条件概率的计算不能再基于上面解释的计数方法来计算。通常假设连续特征变量是正态分布的。对于正态分布，随机变量(x)在 x 和 x+dx 之间的概率由下式给出:</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="gh gi ls"><img src="../Images/b39addb63c14a65ae5407564ba740124.png" data-original-src="https://miro.medium.com/v2/resize:fit:498/format:webp/1*G6uL_DVLGIHcIBd24IrMKg.png"/></div></figure><p id="510d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">上式中的𝜇和𝜎是给定正态分布的均值和标准差。</p><p id="cce6" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">基于目标变量类的子表的创建类似于上面所解释的。计算子表(表 4 和表 5)后，我们计算并存储平均温度𝜇ₜₑₘₚₑᵣₐₜᵤᵣₑ|Accidentₙₒ、𝜇ₜₑₘₚₑᵣₐₜᵤᵣₑ|Accidentᵧₑₛ，以及来自 2 个子表的温度特征变量的标准偏差𝜎ₜₑₘₚₑᵣₐₜᵤᵣₑ|Accidentₙₒ和𝜎ₜₑₘₚₑᵣₐₜᵤᵣₑ|Accidentᵧₑₛ。</p><p id="9274" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在预测阶段，给定一个新的温度值，使用如上所示的正态分布曲线的分析形式计算类别条件概率:</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="gh gi lt"><img src="../Images/dc71c5845fd44c37f09787fce0ae1cf5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wdYzvdipkQRanBbKpKeoHw.png"/></div></div></figure><p id="aefc" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir"> 3.5 处理不完整数据集:</strong></p><p id="2aae" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在上述示例数据集中，存在足够的数据来计算所有类别条件概率。如果训练数据集中的目标类缺少特定的要素标注，该怎么办？例如，在我们的例子中，𝑃(𝑊eatherconditionᵣₐᵢₙ|accidentᵧₑₛ= 0。这将导致后验概率𝑃(accidentᵧₑₛ|𝐴ₙₑ𝓌= 0，即使其他类别的条件概率不为零。拉普拉斯校正用于处理这种情况。具有拉普拉斯校正的类条件概率的一般形式是:</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="gh gi lu"><img src="../Images/9d2340c18f3678e5014cec7dc9069e12.png" data-original-src="https://miro.medium.com/v2/resize:fit:1376/format:webp/1*PIFGKMzWQ_qOcWeLxTV7Nw.png"/></div></figure><p id="4f44" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">对于第 3.0 节中讨论的示例，n = 1 是事故=是时的要素类计数(即雨)。𝑁Accidentᵧₑₛ= 5 号。</p><p id="097b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">NFeature = 4(数据集中有 4 个要素)。</p><p id="9960" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">α是拉普拉斯校正因子。</p><p id="7ac1" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">拉普拉斯校正应用于所有类别的条件概率计算。从上式可以看出，对于特定的特征类，如果 n = 0，则类条件概率非零。</p><p id="8a3a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">对先验概率的拉普拉斯校正的形式是</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="gh gi lv"><img src="../Images/419496800cf8e3df0c0bc4ade87ff184.png" data-original-src="https://miro.medium.com/v2/resize:fit:766/format:webp/1*D-R9LPtxjPy6EtSee1tXHw.png"/></div></div></figure><p id="37be" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">同样对于 3.0 节中讨论的例子</p><p id="e24c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">m = 10(数据点的总数)。</p><p id="6e7a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">k = 2(目标变量类数量)。</p><p id="8d2e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">为了理解校正因子α的影响，考虑一种假设情况，其中:n = 1，NFeature = 4，𝑁Accidentᵧₑₛ= 60，m = 100，k = 2。</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="gh gi lw"><img src="../Images/5af053fe70c757f80a8248e99f2a294d.png" data-original-src="https://miro.medium.com/v2/resize:fit:912/format:webp/1*sz6lV8iPni9YMktiegCkmw.png"/></div><p class="kx ky gj gh gi kz la bd b be z dk translated">图 1</p></figure><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="gh gi lx"><img src="../Images/0a7b428a4251dd159e20437e17235523.png" data-original-src="https://miro.medium.com/v2/resize:fit:932/format:webp/1*xcDw43hSJjSkR3xHVh7RUQ.png"/></div><p class="kx ky gj gh gi kz la bd b be z dk translated">图 2</p></figure><p id="adcf" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">图 1 和图 2 分别显示了上述假设情况下类别条件概率和先验概率的变化。从图 1 中，我们可以看到，随着α的增加，类别条件概率趋向于 1/4 = 0.25。同样，从图 2 中，我们可以看到，随着α的增加，先验概率趋于 1/2 = 0.5。</p><p id="ebbe" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">通过让α趋于无穷大，这些极限值也可以从拉普拉斯校正方程中看出:</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="gh gi ly"><img src="../Images/027404781da9b40e0ec632902fc27a24.png" data-original-src="https://miro.medium.com/v2/resize:fit:874/format:webp/1*UsCrvfFF1XjxcLFaitRBNg.png"/></div></figure><p id="157d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">因此，随着校正因子的值增加，类别条件概率趋向于均匀的概率分布，每个特征具有相同的类别条件概率 1/NFeature。</p><p id="64f7" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">类似地，每个目标类别的先验概率趋向于均匀概率，每个类别具有相同的 1/k 的先验概率。对于大多数问题，值α通常选择为 1。</p><p id="8976" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir"> 4.0 使用 Scikit 学习的朴素贝叶斯:</strong></p><p id="b203" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">sklearn 中的 nave _ Bayes 模块支持不同版本的 nave Bayes 分类，例如高斯 nave Bayes(在 3.4 节中讨论)、用于分类特征的多项式 nave Bayes 以及其他版本。</p><p id="0321" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">本节使用 iris 数据集来说明 Scikit learn 中可用的高斯朴素贝叶斯分类器的用法。数据集可以在这里找到:<em class="lb"/><a class="ae lz" href="https://www.kaggle.com/uciml/iris?select=Iris.csv" rel="noopener ugc nofollow" target="_blank"><em class="lb">【https://www.kaggle.com/uciml/iris?select=Iris.csv】</em></a></p><p id="89b8" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">虹膜数据集是一个微小的数据集，由 4 个连续的特征向量组成，描述了鸢尾花家族的不同特征。有 3 个目标类别涉及三种鸢尾花。目的是为一组新的特征向量正确地预测花卉种类。</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="gh gi ll"><img src="../Images/4b183b268bc2a2b47d893e486bdcef2a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1066/format:webp/1*ZwQsOOiwVTUrjWUIWL6UEA.png"/></div></figure><figure class="km kn ko kp gt kq"><div class="bz fp l di"><div class="ma mb l"/></div></figure><p id="ae1f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">对于这个简单的数据集，高斯朴素贝叶斯分类器在预测花卉种类时达到了 0.96 的准确度分数。</p><p id="a8ee" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir"> 4.1 处理混合功能:</strong></p><p id="4100" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">如果数据集同时具有连续和分类特征。sklearn 的一个简单方法是使用宁滨将连续变量转换为分类变量。例如，通过定义冷、温和、热温度类别的温度范围，可以将温度等特征转换为分类变量。将所有要素转换为分类要素后，可以使用 sklearn 中 nave _ Bayes 模块中的多项式算法进行拟合和预测。</p><p id="c2a8" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir"> 5.0 结论:</strong></p><p id="18c3" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">朴素贝叶斯分类器是一个简单而通用的分类器。由于计算成本低，朴素贝叶斯分类器对于大型数据集非常有效。在性能方面，朴素贝叶斯分类器比许多其他分类器具有更好的性能。朴素贝叶斯分类器的一个主要缺点是固有的特征独立性假设。实际上，真实数据集中的要素很少是独立的。尽管有这个缺点，朴素贝叶斯分类器在初步理解数据时非常有用。</p><p id="df8c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir"> 6.0 参考文献:</strong></p><ol class=""><li id="c89a" class="mc md iq jp b jq jr ju jv jy me kc mf kg mg kk mh mi mj mk bi translated">基本商业统计概念和应用，M.L. Berenson，D.M. Levine，T.C. Krehbiel。</li><li id="80db" class="mc md iq jp b jq ml ju mm jy mn kc mo kg mp kk mh mi mj mk bi translated"><a class="ae lz" href="https://scikit-learn.org/stable/modules/naive_bayes.html" rel="noopener ugc nofollow" target="_blank">https://scikit-learn.org/stable/modules/naive_bayes.html</a></li></ol></div></div>    
</body>
</html>