<html>
<head>
<title>This Week’s Unboxing: Gradient Boosted Models’ “Black Box”</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">本周拆箱:渐变提升模型的“黑箱”</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/this-weeks-unboxing-gradient-boosted-models-black-box-138c3a0c6d80?source=collection_archive---------16-----------------------#2021-11-11">https://towardsdatascience.com/this-weeks-unboxing-gradient-boosted-models-black-box-138c3a0c6d80?source=collection_archive---------16-----------------------#2021-11-11</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="facd" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">使用R和LightGBM逐步重新计算隐藏的模型结果</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/13300be9454189edcd9fc7f1188912e4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BAyBPOOBoy4YTE3HCIVPGQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><p id="da35" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">梯度增强模型通常被称为机器学习算法的黑盒例子。在<a class="ae lu" href="https://en.wikipedia.org/wiki/Black_box#Computing_and_mathematics" rel="noopener ugc nofollow" target="_blank">维基百科</a>上，黑盒模型被定义为一个“<em class="lv">系统，可以根据其输入和输出[……]来查看，而无需了解其内部工作方式</em>。这实际上是对GBMs的准确描述吗？我会说，几乎没有。您可以查看像<a class="ae lu" href="https://github.com/dmlc/xgboost" rel="noopener ugc nofollow" target="_blank"> XGBoost </a>这样的开源包中的每一行代码，并导出详细的树结构。</p><p id="b3c0" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">争论的下一步是，所有的代码可能都是可用的，但它太复杂了，还不如不可用。我在一定程度上同意这种变化:这是一个复杂的盒子，当然…但这仍然不会使它变得不透明。</p><blockquote class="lw"><p id="4f09" class="lx ly it bd lz ma mb mc md me mf lt dk translated">我认为“黑箱”标签经常被用来证明没有投资去理解一个模型是如何工作的。</p></blockquote><p id="82b3" class="pw-post-body-paragraph ky kz it la b lb mg ju ld le mh jx lg lh mi lj lk ll mj ln lo lp mk lr ls lt im bi translated">让我们改变这一切！</p><h1 id="d110" class="ml mm it bd mn mo mp mq mr ms mt mu mv jz mw ka mx kc my kd mz kf na kg nb nc bi translated">宗旨</h1><p id="01a4" class="pw-post-body-paragraph ky kz it la b lb nd ju ld le ne jx lg lh nf lj lk ll ng ln lo lp nh lr ls lt im bi translated">在本帖中，我们将看看一个玩具示例，并重新计算梯度推进决策树所做的一切。嗯，所有的事情，<strong class="la iu">除了找到树分裂点</strong>。正如我们将看到的，找到分裂的地方是唯一棘手的一点，(毫无疑问，这是许多伟大创新的来源)，<strong class="la iu">其他一切都只是基本的计算</strong>。</p><p id="92aa" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我认为彻底查看结果很重要的主要原因是<strong class="la iu">更好地理解棘手的超参数是如何工作的</strong>。当然，有简单的超参数，比如<code class="fe ni nj nk nl b">max_depth</code>，但是无论你看多少可解释的图，在没有理解模型使用的迭代步骤的情况下，祝你好运概念化<code class="fe ni nj nk nl b">min_child_weight</code>做什么！这同样适用于<code class="fe ni nj nk nl b">learning_rate</code>。当然，我们可能有一个本能的想法，但是知道<strong class="la iu">单个参数到底做什么</strong>会让人放心。</p><p id="2ece" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">解释复杂模型有丰富的文献，如果你感兴趣，可以看看这本书。我们现在什么都没做。在这篇文章中，我们将看看模型如何得出预测，而不是如何解释这些预测。</p><p id="07c4" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这也是<strong class="la iu">不是GBDTs的通用教程</strong>。我假设我们知道如何准备数据、拟合模型、做预测、泊松回归如何使用对数函数等等。那里有很多指南和教程，你也可以看看我之前关于这个话题的帖子，它听起来非常小众，重点是曝光，但是当涉及到基础知识时，它会涉及很多细节。</p><p id="065b" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">也完全有可能是我弄错了。如果你认为我错过了什么，请给我发信息。</p><h1 id="2453" class="ml mm it bd mn mo mp mq mr ms mt mu mv jz mw ka mx kc my kd mz kf na kg nb nc bi translated">环境</h1><h2 id="3444" class="nm mm it bd mn nn no dn mr np nq dp mv lh nr ns mx ll nt nu mz lp nv nw nb nx bi translated">编程；编排</h2><p id="cfe6" class="pw-post-body-paragraph ky kz it la b lb nd ju ld le ne jx lg lh nf lj lk ll ng ln lo lp nh lr ls lt im bi translated">我们将使用R和<a class="ae lu" href="https://lightgbm.readthedocs.io/en/latest/index.html" rel="noopener ugc nofollow" target="_blank"> LightGBM </a>，我选择的增强树模型。不用说，大多数结果适用于XGBoost和其他GBDT包，如XGBoost。我们将需要其余的包:<code class="fe ni nj nk nl b">lightgbm</code>和<code class="fe ni nj nk nl b">data.table</code>。</p><h2 id="0bf6" class="nm mm it bd mn nn no dn mr np nq dp mv lh nr ns mx ll nt nu mz lp nv nw nb nx bi translated">开源代码库</h2><p id="58fc" class="pw-post-body-paragraph ky kz it la b lb nd ju ld le ne jx lg lh nf lj lk ll ng ln lo lp nh lr ls lt im bi translated">代码是一个很长的R脚本，可以在我的GitHub上找到。</p><h2 id="dc8f" class="nm mm it bd mn nn no dn mr np nq dp mv lh nr ns mx ll nt nu mz lp nv nw nb nx bi translated">模型目标</h2><p id="b4bc" class="pw-post-body-paragraph ky kz it la b lb nd ju ld le ne jx lg lh nf lj lk ll ng ln lo lp nh lr ls lt im bi translated">在我们生成数据之前，我们需要决定关注什么样的目标。我们在这里所做的当然也适用于其他地方。我<strong class="la iu">选了</strong> <a class="ae lu" href="https://en.wikipedia.org/wiki/Poisson_regression" rel="noopener ugc nofollow" target="_blank"> <strong class="la iu">泊松回归</strong> </a>的原因如下:</p><ul class=""><li id="bacd" class="ny nz it la b lb lc le lf lh oa ll ob lp oc lt od oe of og bi translated">我在保险行业工作，泊松分布在历史上很重要。</li><li id="5425" class="ny nz it la b lb oh le oi lh oj ll ok lp ol lt od oe of og bi translated">它不像其他目标和相应的指标那样被广泛记录。</li><li id="5dee" class="ny nz it la b lb oh le oi lh oj ll ok lp ol lt od oe of og bi translated">有一些棘手的半隐藏参数(我们很快就会看到……)。</li></ul><h2 id="472d" class="nm mm it bd mn nn no dn mr np nq dp mv lh nr ns mx ll nt nu mz lp nv nw nb nx bi translated">数据</h2><p id="b5b6" class="pw-post-body-paragraph ky kz it la b lb nd ju ld le ne jx lg lh nf lj lk ll ng ln lo lp nh lr ls lt im bi translated">玩具的例子非常简单，两个变量，<code class="fe ni nj nk nl b">var1</code>和<code class="fe ni nj nk nl b">var2</code>，每个变量有两个可能的值。对于4种可能的组合，我们生成特定λ的随机泊松数。</p><p id="977d" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">(这是一个非常简单的数据集，你永远不会使用GBT来模拟这种结构。然而，这将使重新计算变得容易/可能。一旦我们知道事物是如何计算的，我们就可以把它应用到任何现实生活中的问题。)</p><p id="9f31" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">如果您想了解结果，这是我用来生成数据集的代码:</p><pre class="kj kk kl km gt om nl on oo aw op bi"><span id="f1d9" class="nm mm it nl b gy oq or l os ot">set.seed(100)</span><span id="5307" class="nm mm it nl b gy ou or l os ot">data_curr &lt;- data.table(<br/>var1 = sample(c(0,1), prob = c(0.6, 0.4), size = 1000, replace = TRUE),<br/>var2 = sample(c(0,1), prob = c(0.8, 0.2), size = 1000, replace = TRUE))</span><span id="9dd2" class="nm mm it nl b gy ou or l os ot">var_impact &lt;- data.table(<br/>  var1 = c(0,1,0,1),<br/>  var2 = c(0,0,1,1),<br/>  lambda = c(0.3, 0.7, 1.3, 1.9)<br/>)</span><span id="3322" class="nm mm it nl b gy ou or l os ot">generate_claim_counts &lt;- function(dt, var_impact){<br/>  dt &lt;- merge.data.table(<br/>    x = dt, y = var_impact, by = c("var1", "var2"))<br/>  random_pois &lt;- as.numeric(<br/>    lapply(dt[,lambda], function(x){rpois(n  = 1, lambda = x)}))<br/>  dt[, target := random_pois]<br/>  return(dt)<br/>}</span><span id="78c6" class="nm mm it nl b gy ou or l os ot">data_curr &lt;- generate_claim_counts(data_curr, var_impact)</span></pre><p id="c915" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这是数据的样子:</p><pre class="kj kk kl km gt om nl on oo aw op bi"><span id="9258" class="nm mm it nl b gy oq or l os ot"><br/>      var1 var2 lambda target<br/>   1:    0    0    0.3      1<br/>   2:    0    0    0.3      1<br/>   3:    0    0    0.3      1<br/>  ---                        <br/> 998:    1    1    1.9      2<br/> 999:    1    1    1.9      1<br/>1000:    1    1    1.9      3<br/></span></pre><p id="8dde" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">每组的观察次数和目标总数:</p><pre class="kj kk kl km gt om nl on oo aw op bi"><span id="4c5e" class="nm mm it nl b gy oq or l os ot">    var1 var2   num_observations     sum_target<br/> 1:    0    0                457            125<br/> 2:    0    1                117            141<br/> 3:    1    0                340            232<br/> 4:    1    1                 86            179</span></pre><p id="7c96" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">目标的总期望值为0.677。</p><h1 id="182f" class="ml mm it bd mn mo mp mq mr ms mt mu mv jz mw ka mx kc my kd mz kf na kg nb nc bi translated">理论背景</h1><p id="98bd" class="pw-post-body-paragraph ky kz it la b lb nd ju ld le ne jx lg lh nf lj lk ll ng ln lo lp nh lr ls lt im bi translated">我找到的关于这个主题的最好的文档是XGBoost教程，我认为它很好地描述了渐变增强是如何工作的。这里只缺少一个部分:无论你在文档中哪里看到一个<code class="fe ni nj nk nl b">G</code>或<code class="fe ni nj nk nl b">g</code>，那就叫做渐变，<code class="fe ni nj nk nl b">H</code>或<code class="fe ni nj nk nl b">h</code>就是黑森(我们稍后会用到它)。</p><blockquote class="lw"><p id="2580" class="lx ly it bd lz ma mb mc md me mf lt dk translated">我敢打赌，如果你阅读并理解XGBoost教程，你会比大多数人更好地理解梯度提升决策树。</p></blockquote><p id="6b64" class="pw-post-body-paragraph ky kz it la b lb mg ju ld le mh jx lg lh mi lj lk ll mj ln lo lp mk lr ls lt im bi translated"><strong class="la iu">我强烈推荐阅读教程</strong>，但如果你不想，这里有一些我们会用到的快速要点:</p><ul class=""><li id="d3bf" class="ny nz it la b lb lc le lf lh oa ll ob lp oc lt od oe of og bi translated">该模型的目标是优化目标函数，包括损失和正则化项；</li><li id="f90a" class="ny nz it la b lb oh le oi lh oj ll ok lp ol lt od oe of og bi translated">梯度下降法实际上不使用似然函数，而是用关于预测的一阶和二阶导数来近似它，分别称为梯度和hessian</li><li id="d5d1" class="ny nz it la b lb oh le oi lh oj ll ok lp ol lt od oe of og bi translated">如果你知道要做哪个分裂，你可以根据你的梯度和hessian值计算新的预测；</li><li id="6b58" class="ny nz it la b lb oh le oi lh oj ll ok lp ol lt od oe of og bi translated">这是一个迭代过程，模型适合一棵树，该树将预测分配给数据中的每个观察值，然后保存这些预测的累积记录。</li></ul><h1 id="98b0" class="ml mm it bd mn mo mp mq mr ms mt mu mv jz mw ka mx kc my kd mz kf na kg nb nc bi translated">系统模型化</h1><h2 id="b070" class="nm mm it bd mn nn no dn mr np nq dp mv lh nr ns mx ll nt nu mz lp nv nw nb nx bi translated">运行模型</h2><p id="c7da" class="pw-post-body-paragraph ky kz it la b lb nd ju ld le ne jx lg lh nf lj lk ll ng ln lo lp nh lr ls lt im bi translated">是时候做一些建模了，下面是我们将使用的代码:</p><pre class="kj kk kl km gt om nl on oo aw op bi"><span id="dbcd" class="nm mm it nl b gy oq or l os ot">run_model &lt;- function(dtrain, learning_rate, num_iterations, <br/>                      min_sum_hessian, poisson_max_delta_step){<br/>  <br/>  param &lt;- list(<br/>    objective = "poisson",<br/>    num_iterations = num_iterations, <br/>    learning_rate = learning_rate,<br/>    min_sum_hessian = min_sum_hessian,<br/>    poisson_max_delta_step = poisson_max_delta_step)<br/>  <br/>  lgb_model &lt;- lgb.train(<br/>    params = param, <br/>    data = dtrain,<br/>    boosting = "gbdt",<br/>    verbose = 1)<br/>  <br/>  return(lgb_model)<br/>}</span></pre><p id="f8a2" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">请注意，我们在LightGBM中使用了<code class="fe ni nj nk nl b">“gbdt”</code> boosting方法，这是默认选项，以保持简单，并可在其他包中重现。</p><p id="e544" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">然后，我们设置LightGBM数据集:</p><pre class="kj kk kl km gt om nl on oo aw op bi"><span id="f47d" class="nm mm it nl b gy oq or l os ot">dtrain_data &lt;- as.matrix(data_curr[,.(var1, var2)])<br/>dtrain_label &lt;- as.matrix(data_curr[,.(target)])</span><span id="05a8" class="nm mm it nl b gy ou or l os ot">dtrain &lt;- lgb.Dataset(<br/>  data = dtrain_data,<br/>  label = dtrain_label,<br/>  categorical_feature = c(1,2))</span></pre><p id="e7b0" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">最后，训练模型，保存为<code class="fe ni nj nk nl b">lgb_model</code>:</p><pre class="kj kk kl km gt om nl on oo aw op bi"><span id="a7f3" class="nm mm it nl b gy oq or l os ot">lgb_model &lt;- run_model(<br/>    dtrain = dtrain, learning_rate = 0.3, num_iterations = 100,<br/>    min_sum_hessian = 0, poisson_max_delta_step = 0.6 )</span></pre><h2 id="ec1c" class="nm mm it bd mn nn no dn mr np nq dp mv lh nr ns mx ll nt nu mz lp nv nw nb nx bi translated">预测与实际</h2><p id="5b83" class="pw-post-body-paragraph ky kz it la b lb nd ju ld le ne jx lg lh nf lj lk ll ng ln lo lp nh lr ls lt im bi translated">让我们快速检查一下。在每个组中，我们期望预测与目标的平均值相同。(因为我们有杰出的群体，我们可以很容易地证实这一点。)</p><pre class="kj kk kl km gt om nl on oo aw op bi"><span id="72d1" class="nm mm it nl b gy oq or l os ot">data_curr[,predict := <br/>    predict(lgb_model,dtrain_data)]<br/>data_curr[,predict_raw := <br/>    predict(lgb_model,dtrain_data, rawscore = TRUE)]<br/>data_curr[,.(.N, mean_target = mean(target),<br/>    predict = predict[1], predict_raw = predict_raw[1]), <br/>    keyby = .(var1, var2)]</span></pre><p id="b73e" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">将返回:</p><pre class="kj kk kl km gt om nl on oo aw op bi"><span id="8471" class="nm mm it nl b gy oq or l os ot">    var1 var2   N  mean_target    predict  predict_raw<br/> 1:    0    0 457    0.2735230  0.2735230   -1.2963696<br/> 2:    0    1 117    1.2051282  1.2051282    0.1865859<br/> 3:    1    0 340    0.6823529  0.6823529   -0.3822083<br/> 4:    1    1  86    2.0813953  2.0813953    0.7330385</span></pre><p id="e41c" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这一点得到了证实。请注意，<code class="fe ni nj nk nl b">mean_target</code>列并不完全匹配相应的泊松lambda参数(从上到下分别是:0.3、1.2、0.7、2.0)，这是因为我们总共只生成了1k行，所以<a class="ae lu" href="https://en.wikipedia.org/wiki/Law_of_large_numbers" rel="noopener ugc nofollow" target="_blank">大数定律</a>还没有生效。(<code class="fe ni nj nk nl b">predict_raw</code>列供以后使用，那是指数变换前的原始预测，<code class="fe ni nj nk nl b">predict_raw</code> = log( <code class="fe ni nj nk nl b">predict</code>)。)</p><h1 id="94b8" class="ml mm it bd mn mo mp mq mr ms mt mu mv jz mw ka mx kc my kd mz kf na kg nb nc bi translated">追溯预测</h1><p id="58fe" class="pw-post-body-paragraph ky kz it la b lb nd ju ld le ne jx lg lh nf lj lk ll ng ln lo lp nh lr ls lt im bi translated">我们现在进入了激动人心的部分:理解模型如何准确地得出预测。</p><h2 id="edfe" class="nm mm it bd mn nn no dn mr np nq dp mv lh nr ns mx ll nt nu mz lp nv nw nb nx bi translated">获取树表</h2><p id="449e" class="pw-post-body-paragraph ky kz it la b lb nd ju ld le ne jx lg lh nf lj lk ll ng ln lo lp nh lr ls lt im bi translated">我们将依赖LightGBM的一个方便的功能:将拟合的树模型转换成一个易读的<code class="fe ni nj nk nl b">data.table</code>对象:</p><pre class="kj kk kl km gt om nl on oo aw op bi"><span id="d4e2" class="nm mm it nl b gy oq or l os ot">tree_chart &lt;- lgb.model.dt.tree(lgb_model)</span></pre><p id="2703" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">看看RStudio中的<code class="fe ni nj nk nl b">tree_chart</code>表，不幸的是，它不能很好地显示在屏幕上。该表由100棵树组成(这是我们传递的<code class="fe ni nj nk nl b">num_iterations</code>参数)，它们由<code class="fe ni nj nk nl b">tree_index</code>列标识。100棵树中的第一棵树看起来像这样:</p><pre class="kj kk kl km gt om nl on oo aw op bi"><span id="327c" class="nm mm it nl b gy oq or l os ot">   split_gain internal_value internal_count  leaf_value leaf_count<br/>1:  167.00688     -0.3900840           1000          NA         NA<br/>2:   26.41538     -0.4457929            797          NA         NA<br/>3:         NA             NA             NA -0.48820788        457<br/>4:         NA             NA             NA -0.38878219        340<br/>5:   30.85290     -0.1713648            203          NA         NA<br/>6:         NA             NA             NA -0.26164549        117<br/>7:         NA             NA             NA -0.04854109         86 </span></pre><p id="f44b" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">(当然还有许多其他列，但是因为我们有少量的组，所以拆分可以通过<code class="fe ni nj nk nl b">internal_count</code>和<code class="fe ni nj nk nl b">leaf_count</code>列来识别:第一行包含整个组，然后由<code class="fe ni nj nk nl b">var2</code>拆分，<code class="fe ni nj nk nl b">var2 = 0</code>从第2行开始，<code class="fe ni nj nk nl b">var2 = 1</code>从第5行开始，然后由<code class="fe ni nj nk nl b">var1</code>再次拆分。)</p><h2 id="d655" class="nm mm it bd mn nn no dn mr np nq dp mv lh nr ns mx ll nt nu mz lp nv nw nb nx bi translated">从哪里开始？</h2><p id="3e05" class="pw-post-body-paragraph ky kz it la b lb nd ju ld le ne jx lg lh nf lj lk ll ng ln lo lp nh lr ls lt im bi translated">如果您查看RStudio中的<code class="fe ni nj nk nl b">tree_chart</code>,您会注意到每棵树都由7个级别组成，第一行总是代表所有1k观测值的根节点。根节点的internal_value将是0，除了第一棵树，-0.3900840，正如我们在上面看到的。</p><p id="c2ed" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">嗯，这个值简单来说就是总体的总体期望值:exp(-0.3900840) = 0.677。基线预测将是所有观测值的总体平均值。这非常符合逻辑，我可以看到它是如何加快速度的，但这是我肯定不知道的事情。人们可以假设开始的预测将是0。</p><p id="5f6e" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">如果让<code class="fe ni nj nk nl b">verbose</code>开着，在训练模型时得到的第一组输出中有一个线索:</p><pre class="kj kk kl km gt om nl on oo aw op bi"><span id="6c50" class="nm mm it nl b gy oq or l os ot">[LightGBM] [Info] Start training from score -0.390084</span></pre><blockquote class="ov ow ox"><p id="391c" class="ky kz lv la b lb lc ju ld le lf jx lg oy li lj lk oz lm ln lo pa lq lr ls lt im bi translated"><strong class="la iu">第1课:</strong>模型将从总体平均值开始，作为所有观察值的基线预测。这个值放在第一棵树的internal_value的根节点中。后续的树在其根节点internal_value中具有0。</p></blockquote><p id="d92b" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">(这对于泊松目标来说是正确的，我假设使用期望值对于其他目标也是常见的，但不一定对所有目标都是如此。)</p><h2 id="d677" class="nm mm it bd mn nn no dn mr np nq dp mv lh nr ns mx ll nt nu mz lp nv nw nb nx bi translated">另外两个内部值</h2><p id="a6b9" class="pw-post-body-paragraph ky kz it la b lb nd ju ld le ne jx lg lh nf lj lk ll ng ln lo lp nh lr ls lt im bi translated">那另外两个<code class="fe ni nj nk nl b">internal_values</code>，-0.4457929和-0.1713648呢，我们在第一棵树上可以看到。根据<a class="ae lu" href="https://lightgbm.readthedocs.io/en/latest/pythonapi/lightgbm.create_tree_digraph.html" rel="noopener ugc nofollow" target="_blank">文档</a>(我们得用Python文档，R端没多少……)，这个<code class="fe ni nj nk nl b">internal_value</code>就是:“<em class="lv">如果这个节点是叶节点</em>会产生的原始预测值”。</p><p id="9937" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们不能再拖延了，是时候深入研究模型是如何计算预测的了。我将再次参考<a class="ae lu" href="https://xgboost.readthedocs.io/en/latest/tutorials/model.html" rel="noopener ugc nofollow" target="_blank"> XGBoost教程</a>了解详情，根据该教程，一个节点中的新预测值计算如下:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pb"><img src="../Images/3294f0d51f5483c9ca975298c10eda69.png" data-original-src="https://miro.medium.com/v2/resize:fit:290/format:webp/1*XM9gP0NAUpxdj34a8j-z8w.png"/></div></figure><p id="bd93" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这个公式我有点搞不清楚，不包括学习率，但这是大致思路。我们没有用正则化参数使问题复杂化，λ现在是0。</p><p id="93ba" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们将需要梯度和hessian值。如前所述，这些应该是对数似然函数的导数。我建议实际查看一下<a class="ae lu" href="https://github.com/microsoft/LightGBM/blob/4b1b412452218c5be5ac0f238454ec9309036798/src/objective/regression_objective.hpp" rel="noopener ugc nofollow" target="_blank">源代码</a>，你可能认为你可以自己推导出这些公式，但是偶尔会有一些有趣的事情。</p><p id="355b" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">对于泊松，这些是相关的线:</p><pre class="kj kk kl km gt om nl on oo aw op bi"><span id="28a8" class="nm mm it nl b gy oq or l os ot">gradients[i] = static_cast&lt;score_t&gt;(std::exp(score[i]) - label_[i]);       hessians[i] = static_cast&lt;score_t&gt;(std::exp(score[i] + max_delta_step_));</span></pre><p id="46c4" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">(在这个术语中，<code class="fe ni nj nk nl b">score</code>是预测值，<code class="fe ni nj nk nl b">label</code>是实际目标值。)</p><p id="ba22" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">让我们从第一个内部值-0.4457929开始，这个值是我们在有797个观察值的组中得到的。这是<code class="fe ni nj nk nl b">var2</code> = 0的组，目标总数为357。</p><p id="91a4" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们还需要一样东西:组中当前的预测。这<strong class="la iu">很符合逻辑，但是我在文档</strong>的任何地方都没有找到它:我们在上面看到的期望值-0.3900840，是在这一点上对组中所有观察值的预测。</p><blockquote class="ov ow ox"><p id="098c" class="ky kz lv la b lb lc ju ld le lf jx lg oy li lj lk oz lm ln lo pa lq lr ls lt im bi translated"><strong class="la iu">第2课</strong>:当计算迭代中下一步的梯度和hessian时，将使用它们在当前预测时的值。在第一步中，由模型确定的默认值将用于所有观察值。</p></blockquote><p id="f3c7" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">(除非你有一个<code class="fe ni nj nk nl b">init_score</code> (LGBM) / <code class="fe ni nj nk nl b">base_margin</code> (XGBoost)设置，但是我们真的没有时间去深究那方面，我推荐<a class="ae lu" rel="noopener" target="_blank" href="/how-to-handle-the-exposure-offset-with-boosted-trees-fd09cc946837">我另一篇关于此事的帖子</a>。)</p><p id="f374" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">事不宜迟，我们组的梯度计算如下:</p><pre class="kj kk kl km gt om nl on oo aw op bi"><span id="b372" class="nm mm it nl b gy oq or l os ot">g = 797 * exp(-0.3900840061) - 357</span></pre><p id="1e4b" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">梯度的含义很符合逻辑:它是这组中实际值和预测值之间的差异。</p><p id="8384" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">粗麻布的计算公式为:</p><pre class="kj kk kl km gt om nl on oo aw op bi"><span id="192f" class="nm mm it nl b gy oq or l os ot">h = 797 * exp(-0.3900840061 + 0.6)</span></pre><p id="4aaa" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">你看到的那个0.6，就是<code class="fe ni nj nk nl b">max_delta_step</code>参数。棘手的是这个参数默认为0。但是，XGBoost和LightGBM都将使用泊松目标强制其为正，并将默认值设置为0.7。根据XGBoost文档，该参数为“<em class="lv">用于保护优化</em>”。为了演示计算，我将该值更改为0.6。</p><blockquote class="ov ow ox"><p id="f6da" class="ky kz lv la b lb lc ju ld le lf jx lg oy li lj lk oz lm ln lo pa lq lr ls lt im bi translated"><strong class="la iu">第3课:</strong>对于泊松目标，默认的max_delta_step参数被添加到hessian计算中，其默认值为0.7，并且必须为正。</p></blockquote><p id="bf91" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">从<code class="fe ni nj nk nl b">g</code>和<code class="fe ni nj nk nl b">h</code>中，我们可以推导出额外的预测，<code class="fe ni nj nk nl b">internal_value</code>就像这样:</p><pre class="kj kk kl km gt om nl on oo aw op bi"><span id="2ecc" class="nm mm it nl b gy oq or l os ot">-(g / h) *  0.3 + -0.3900840061 = -0.4457929</span></pre><p id="920e" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">观察公式中0.3的学习率。</p><blockquote class="ov ow ox"><p id="4ae3" class="ky kz lv la b lb lc ju ld le lf jx lg oy li lj lk oz lm ln lo pa lq lr ls lt im bi translated"><strong class="la iu">第4课</strong>:当我们计算新迭代的预测时，我们需要用学习率乘以<code class="fe ni nj nk nl b">g/h</code>值。这就是学习率的作用。高学习率将意味着在一个步骤中添加更大部分的梯度。</p></blockquote><p id="7a0a" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">另一个有趣的地方是增加了第一个<code class="fe ni nj nk nl b">internal_value</code>的整体预测值，-0.3900840061。</p><blockquote class="ov ow ox"><p id="280c" class="ky kz lv la b lb lc ju ld le lf jx lg oy li lj lk oz lm ln lo pa lq lr ls lt im bi translated"><strong class="la iu">第5课</strong>:计算第一棵树中的节点/叶值时，总期望值被添加到默认公式中，并且仅在那里。</p></blockquote><p id="1c7a" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我认为这是一个有趣的技术细节，我认为这可能是LightGBM 特有的。<code class="fe ni nj nk nl b">internal_value</code>列有点不一致:第一个单元格包含预期值，并将用于计算，然而，正如我们将看到的，后面的内部值不会在任何地方使用。</p><p id="71b4" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">让我们快速计算另一个分支中的另一个<code class="fe ni nj nk nl b">internal_value</code>，以确保我们得到正确的公式。在这个组中，其中<code class="fe ni nj nk nl b">var2</code> = 1，我们有203个观察值，目标的总和是320。预测值与另一组相同。再现该值的公式:</p><pre class="kj kk kl km gt om nl on oo aw op bi"><span id="e0dd" class="nm mm it nl b gy oq or l os ot">g = 203 * exp(-0.3900840061) - 320<br/>h = 203 * exp(-0.3900840061 + 0.6)<br/>-(g / h) *  0.3 + -0.3900840061 = -0.1713648</span></pre><h2 id="7db7" class="nm mm it bd mn nn no dn mr np nq dp mv lh nr ns mx ll nt nu mz lp nv nw nb nx bi translated">四个叶值</h2><p id="04dd" class="pw-post-body-paragraph ky kz it la b lb nd ju ld le ne jx lg lh nf lj lk ll ng ln lo lp nh lr ls lt im bi translated">在<code class="fe ni nj nk nl b">internal_value</code>之后，让我们看看<code class="fe ni nj nk nl b">leaf_value</code>栏中有什么。</p><p id="e310" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">一旦我们知道了这个公式，它实际上就很简单了。我们唯一要考虑的是在这种情况下使用的基线预测。让我们以具有457个观察值和125个目标值的叶子为例。我们通过将数据按<code class="fe ni nj nk nl b">var2</code>分割，然后使用= 0组，再按<code class="fe ni nj nk nl b">var1</code>分割，然后再次使用= 0组来实现这一点。我们重新计算了上一节预测的-0.4457929。我们应该用那个数字，还是整体预测，-0.3900840061？</p><p id="a1a5" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">答案是，我们用总体期望值。</p><pre class="kj kk kl km gt om nl on oo aw op bi"><span id="fd8c" class="nm mm it nl b gy oq or l os ot">g = 457 * exp(-0.3900840061) - 125<br/>h = 457 * exp(-0.3900840061 + 0.6) <br/>-(g / h) * 0.3 + -0.3900840061 = -0.4882079</span></pre><blockquote class="ov ow ox"><p id="1253" class="ky kz lv la b lb lc ju ld le lf jx lg oy li lj lk oz lm ln lo pa lq lr ls lt im bi translated"><strong class="la iu">第6课</strong>:节点值对叶节点的预测计算没有影响，一旦分割完成，它就被“遗忘”了，我们只关注剩下的数据。</p></blockquote><p id="3843" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们可以重复同样的计算来复制另外3个<code class="fe ni nj nk nl b">leaf_values</code>。但是我们不要。</p><h2 id="50f9" class="nm mm it bd mn nn no dn mr np nq dp mv lh nr ns mx ll nt nu mz lp nv nw nb nx bi translated">第一棵树后的树</h2><p id="a2d2" class="pw-post-body-paragraph ky kz it la b lb nd ju ld le ne jx lg lh nf lj lk ll ng ln lo lp nh lr ls lt im bi translated">对于下面的树，我们可以使用类似的公式来计算额外的预测。唯一的区别是，我们不再添加整体期望值-0.3900840061。</p><p id="6d20" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">如果你对令人痛苦的手工计算感兴趣，可以看看我的GitHub上的代码。</p><h1 id="c195" class="ml mm it bd mn mo mp mq mr ms mt mu mv jz mw ka mx kc my kd mz kf na kg nb nc bi translated">分割收益</h1><p id="4c1b" class="pw-post-body-paragraph ky kz it la b lb nd ju ld le ne jx lg lh nf lj lk ll ng ln lo lp nh lr ls lt im bi translated">我们将在这里稍微转一下，看看模型是如何计算<code class="fe ni nj nk nl b">split_gains</code>的。这并不是理解预测所必需的。然而，你可能已经看到了所谓的<strong class="la iu">特性重要性图</strong>，其中的特性是基于某些东西进行排序的。嗯，通常情况下，这个东西是分配给特性的 <code class="fe ni nj nk nl b">split_gains</code> <strong class="la iu">的<strong class="la iu">总和，所以理解这些增益是什么可能会有所帮助。</strong></strong></p><h2 id="257e" class="nm mm it bd mn nn no dn mr np nq dp mv lh nr ns mx ll nt nu mz lp nv nw nb nx bi translated">如何计算</h2><p id="ea23" class="pw-post-body-paragraph ky kz it la b lb nd ju ld le ne jx lg lh nf lj lk ll ng ln lo lp nh lr ls lt im bi translated">第一个树的<code class="fe ni nj nk nl b">split_gain</code>列中有三个非NA值。</p><p id="525a" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我将再次参考<a class="ae lu" href="https://xgboost.readthedocs.io/en/latest/tutorials/model.html" rel="noopener ugc nofollow" target="_blank"> XGBoost教程</a>，即最后一个公式:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pc"><img src="../Images/0db08689212fd8f9bb6318a570f750b0.png" data-original-src="https://miro.medium.com/v2/resize:fit:944/format:webp/1*lTvlZ1Coa4ESk1WX8wWHCw.png"/></div></figure><p id="bbf1" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这几乎奏效了，但是由于某种原因，我不需要1/2乘数。</p><p id="4048" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在复制结果时对我有效的公式是这样的:</p><pre class="kj kk kl km gt om nl on oo aw op bi"><span id="ac24" class="nm mm it nl b gy oq or l os ot">split_gain &lt;- function(gradient_l, hessian_l, gradient_r, hessian_r, <br/>                       reg_lambda, reg_gamma){<br/>    return(((gradient_l^2 / (hessian_l+reg_lambda)) + <br/>        (gradient_r^2 / (hessian_r+reg_lambda)) - <br/>        ((gradient_l + gradient_r)^2 /    <br/>        (hessian_l+hessian_r+reg_lambda))) - reg_gamma) <br/>}</span></pre><p id="e763" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">让我们尝试复制表中的第一个split_gain，167.00688。在左边的分割中，我们将有一个目标357的计数797，在右边，目标320的计数203。这些是我们在进行两次internal_value拆分时已经计算过的值。</p><pre class="kj kk kl km gt om nl on oo aw op bi"><span id="b565" class="nm mm it nl b gy oq or l os ot">gradient_l = 797 * exp(-0.3900840061) - 357<br/>hessian_l = 797 * exp(-0.3900840061 + 0.6)<br/>gradient_r = 203 * exp(-0.3900840061) - 320<br/>hessian_r = 203 * exp(-0.3900840061 + 0.6)</span></pre><p id="38d1" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">将上面用<code class="fe ni nj nk nl b">reg_lambda</code> = 0和<code class="fe ni nj nk nl b">reg_gamma</code> = 0计算的4个值传递给<code class="fe ni nj nk nl b">split_gain</code>函数，我们确实得到了167.00688的<code class="fe ni nj nk nl b">split_gain</code>。</p><p id="5bd6" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">其他两个<code class="fe ni nj nk nl b">split_gains</code>也是如此。为了完整起见，这里有两个块:</p><pre class="kj kk kl km gt om nl on oo aw op bi"><span id="b876" class="nm mm it nl b gy oq or l os ot">gradient_l &lt;- 457 * exp(-0.3900840061) - 125<br/>hessian_l &lt;- 457 * exp(-0.3900840061 + 0.6)<br/>gradient_r &lt;- 340 * exp(-0.3900840061) - 232<br/>hessian_r &lt;- 340 * exp(-0.3900840061 + 0.6)</span></pre><p id="82db" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">和</p><pre class="kj kk kl km gt om nl on oo aw op bi"><span id="8375" class="nm mm it nl b gy oq or l os ot">gradient_l &lt;- 117 * exp(-0.3900840061) - 141<br/>hessian_l &lt;- 117 * exp(-0.3900840061 + 0.6)<br/>gradient_r &lt;- 86 * exp(-0.3900840061) - 179<br/>hessian_r &lt;- 86 * exp(-0.3900840061 + 0.6)</span></pre><p id="a9bd" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><em class="lv">(观察三个区块之间的数字是如何相加的，嗯……当然是这样。)</em></p><p id="9cec" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu">我想提醒大家注意一个有趣的细节</strong>:大的分割增益远远大于两个小的分割增益之和。这是为什么呢？在第一次大分割中，我们从一个总体期望值移动到两个<code class="fe ni nj nk nl b">internal_values</code>。但是在两个较小的分割中并没有真正使用这些信息。它们仍然从相同的总体期望值开始。但据推测，得到一个更好的整体适合。</p><p id="59ac" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这是怎么回事？</p><p id="f4db" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">答案是这些<code class="fe ni nj nk nl b">split_gains</code>仅仅是目标函数变化的近似值。</p><h2 id="1bda" class="nm mm it bd mn nn no dn mr np nq dp mv lh nr ns mx ll nt nu mz lp nv nw nb nx bi translated">分割收益背后的含义</h2><p id="ebbc" class="pw-post-body-paragraph ky kz it la b lb nd ju ld le ne jx lg lh nf lj lk ll ng ln lo lp nh lr ls lt im bi translated">现在，我必须承认，<strong class="la iu">我不完全确定这一点</strong>。请以开放的心态阅读，如果你认为有更好的解释，请告诉我。</p><p id="5577" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们知道<code class="fe ni nj nk nl b">split_gains</code>是如何精确计算的，让我们试着解释一下它们背后的含义。</p><p id="63cc" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们必须记住，这些分离增益应该接近目标函数的增加。由于我们现在没有处理正则化参数，这应该只是总对数似然的增加。</p><p id="24be" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">另一个线索是，第一次分裂的影响并没有真正反馈到后面的计算中。当我们到达一片叶子时，新的分数将不再依赖于先前的分裂，而只依赖于当前分配的预测。</p><p id="59e6" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">让我们再看一下树形图。如果我们用<code class="fe ni nj nk nl b">internal_count</code>来概括<code class="fe ni nj nk nl b">split_gains</code>:</p><pre class="kj kk kl km gt om nl on oo aw op bi"><span id="945a" class="nm mm it nl b gy oq or l os ot">tree_chart[!is.na(internal_count),.(split_count = .N, sum_split_gain = sum(split_gain)), by = internal_count]</span></pre><p id="d275" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们得到这张表:</p><pre class="kj kk kl km gt om nl on oo aw op bi"><span id="2034" class="nm mm it nl b gy oq or l os ot">   internal_count split_count sum_split_gain<br/>1:           1000         100   4.461004e+02<br/>2:            797          97   1.280125e+02<br/>3:            203          97   4.794666e+01<br/>4:            574           3   7.168127e-13<br/>5:            426           3   2.634893e-14</span></pre><p id="6541" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">(在100棵树中，除了3棵树之外，我们首先通过<code class="fe ni nj nk nl b">var2</code>在797和203组之间进行切割。)</p><p id="5695" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">现在，如果我们将internal_count不为1000的拆分的<code class="fe ni nj nk nl b">sum_split_gain</code>号相加(因此在导致<code class="fe ni nj nk nl b">leaf_values</code>的拆分处)，我们得到175.9591。</p><p id="8731" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">如果我们用所有1000次观察预测的初始总体预期值0.677计算对数似然函数值，我们得到-1150.381。(您可以通过使用<code class="fe ni nj nk nl b">dpois(target, predict, log = TRUE)</code>函数并累加这些值来轻松计算泊松对数似然。)模型预测的最终对数似然是-979.4902。涨幅？170.8909.</p><blockquote class="lw"><p id="5319" class="lx ly it bd lz ma mb mc md me mf lt dk translated">那是非常接近的。巧合吗？可能...</p></blockquote><p id="8de8" class="pw-post-body-paragraph ky kz it la b lb mg ju ld le mh jx lg lh mi lj lk ll mj ln lo lp mk lr ls lt im bi translated">这绝对是<code class="fe ni nj nk nl b">split_gains</code>应该代表的，目标函数的变化。然而，也有令人烦恼的不一致。首先，<code class="fe ni nj nk nl b">learning_rate</code>对<code class="fe ni nj nk nl b">split_gain</code>没有直接影响，第一棵树的<code class="fe ni nj nk nl b">split_gains</code>不会改变，不管你的<code class="fe ni nj nk nl b">learning_rate</code>是什么。但是它会改变实际对数似然的变化。</p><blockquote class="ov ow ox"><p id="7df8" class="ky kz lv la b lb lc ju ld le lf jx lg oy li lj lk oz lm ln lo pa lq lr ls lt im bi translated"><strong class="la iu">第7课</strong>:split _ gains近似表示拆分对目标函数的影响，但不一定做得很好。</p></blockquote><h1 id="cca2" class="ml mm it bd mn mo mp mq mr ms mt mu mv jz mw ka mx kc my kd mz kf na kg nb nc bi translated">这一切在哪里结束？</h1><p id="4d26" class="pw-post-body-paragraph ky kz it la b lb nd ju ld le ne jx lg lh nf lj lk ll ng ln lo lp nh lr ls lt im bi translated">好了，最后一点，这真的很酷。</p><p id="36e8" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">所有的<code class="fe ni nj nk nl b">leaf_values</code>现在都是已知的，我们知道模型如何精确地计算它们。(请记住，在第一棵树中，它还添加了总体期望值。)我们拿它们怎么办？</p><p id="f046" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">答案很简单，我们只需将树中的<code class="fe ni nj nk nl b">leaf_values</code>相加就可以得到预测。更准确地说，对于每次观察，每棵树上都有一片对应的叶子。将这些<code class="fe ni nj nk nl b">leaf_values</code>相加将等于该观察的最终原始预测(因此在我们的例子中，实际预测的日志)。</p><p id="6137" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们可以很容易地从<code class="fe ni nj nk nl b">tree_chart</code>中检查到这一点。我们知道每棵树都会将观察结果分成4组，每组的大小不同，所以我们可以简单地通过<code class="fe ni nj nk nl b">leaf_count</code>进行聚合。</p><pre class="kj kk kl km gt om nl on oo aw op bi"><span id="09d0" class="nm mm it nl b gy oq or l os ot">tree_chart[!is.na(leaf_count),.(sum_leaf_value = sum(leaf_value)), by = leaf_count]</span></pre><p id="1c7f" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">将返回:</p><pre class="kj kk kl km gt om nl on oo aw op bi"><span id="63a0" class="nm mm it nl b gy oq or l os ot">   leaf_count sum_leaf_value<br/>1:        457     -1.2963696<br/>2:        340     -0.3822083<br/>3:        117      0.1865859<br/>4:         86      0.7330385</span></pre><p id="98a2" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这正是模型所做的预测。</p><p id="abf4" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">(当然，在实际问题中，这不会像用<code class="fe ni nj nk nl b">leaf_count</code>求和那么简单。您将不得不进行大量的数据辩论，以确定某个观察值将落在每棵树的哪片叶子上。这将是困难的，但也是可能的。)</p><blockquote class="ov ow ox"><p id="1035" class="ky kz lv la b lb lc ju ld le lf jx lg oy li lj lk oz lm ln lo pa lq lr ls lt im bi translated"><strong class="la iu">第8课</strong>:如果你查找并累加所有树的leaf_values，你会得到每个观察的最终原始预测。如果在您的预测中没有使用所有的树，您也可以只将一定数量的树的结果相加。</p></blockquote><h1 id="2c33" class="ml mm it bd mn mo mp mq mr ms mt mu mv jz mw ka mx kc my kd mz kf na kg nb nc bi translated">摘要</h1><p id="091c" class="pw-post-body-paragraph ky kz it la b lb nd ju ld le ne jx lg lh nf lj lk ll ng ln lo lp nh lr ls lt im bi translated">我们了解了LightGBM:</p><ul class=""><li id="4888" class="ny nz it la b lb lc le lf lh oa ll ob lp oc lt od oe of og bi translated">计算内部值和叶值；</li><li id="25dd" class="ny nz it la b lb oh le oi lh oj ll ok lp ol lt od oe of og bi translated">做预测；</li><li id="3c5d" class="ny nz it la b lb oh le oi lh oj ll ok lp ol lt od oe of og bi translated">用分割增益逼近目标函数；</li><li id="da34" class="ny nz it la b lb oh le oi lh oj ll ok lp ol lt od oe of og bi translated">使用某些超参数；</li><li id="7575" class="ny nz it la b lb oh le oi lh oj ll ok lp ol lt od oe of og bi translated">调整泊松目标。</li></ul><div class="pd pe gp gr pf pg"><a href="https://matepocs.medium.com/membership" rel="noopener follow" target="_blank"><div class="ph ab fo"><div class="pi ab pj cl cj pk"><h2 class="bd iu gy z fp pl fr fs pm fu fw is bi translated">加入我的推荐链接-伴侣概念</h2><div class="pn l"><h3 class="bd b gy z fp pl fr fs pm fu fw dk translated">作为一个媒体会员，你的会员费的一部分会给你阅读的作家，你可以完全接触到每一个故事…</h3></div><div class="po l"><p class="bd b dl z fp pl fr fs pm fu fw dk translated">matepocs.medium.com</p></div></div><div class="pp l"><div class="pq l pr ps pt pp pu ks pg"/></div></div></a></div></div></div>    
</body>
</html>