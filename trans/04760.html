<html>
<head>
<title>Deduplication of customer data using fuzzy-scoring</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用模糊评分对客户数据进行重复数据删除</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/deduplication-of-customer-data-using-fuzzy-scoring-3f77bd3bb4dc?source=collection_archive---------28-----------------------#2021-04-25">https://towardsdatascience.com/deduplication-of-customer-data-using-fuzzy-scoring-3f77bd3bb4dc?source=collection_archive---------28-----------------------#2021-04-25</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="a612" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsdatascience.com/tagged/notes-from-industry" rel="noopener" target="_blank">行业笔记</a></h2><div class=""/><div class=""><h2 id="d832" class="pw-subtitle-paragraph jw iz iq bd b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn dk translated">使用Python处理复杂的客户数据</h2></div><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi ko"><img src="../Images/866e5fc34248348f68217b4970e45ba4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1248/format:webp/1*Yq5Hu3apPkKgWhb9auOVSg.png"/></div></figure><p id="537c" class="pw-post-body-paragraph kw kx iq ky b kz la ka lb lc ld kd le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ja">摘要:</strong></p><p id="1376" class="pw-post-body-paragraph kw kx iq ky b kz la ka lb lc ld kd le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">对于任何组织而言，数据管理始终是一项挑战，贯穿其规划、收集、处理、存储、管理、分析、可视化和最终解释的生命周期。为了成功管理和保护数据，需要团队之间进行大量规划和协作，以便将数据从一个阶段转移到下一个阶段。数据处理的几个主要步骤是争论、压缩、重复数据删除和加密。对于大多数以客户为中心的组织来说，获得唯一的客户列表将是一个最大的挑战，这不是因为存储成本降低，而是因为了解他们的客户是他们最大的资产。</p><p id="f726" class="pw-post-body-paragraph kw kx iq ky b kz la ka lb lc ld kd le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">因此，无论企业规模如何，他们都会花费大量时间和精力，通过线内重复数据删除或后处理重复数据删除来获取正确的数据[1]。本白皮书通过python和相关软件包使用模糊评分方法来讨论后处理重复数据删除。</p><p id="59ab" class="pw-post-body-paragraph kw kx iq ky b kz la ka lb lc ld kd le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ja">挑战:</strong></p><p id="6819" class="pw-post-body-paragraph kw kx iq ky b kz la ka lb lc ld kd le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">数据以各种方法收集，有/没有标准的数据收集表格，在不同的地方收集，但集中在一个地方。这些列表通常由第三方编制，没有标准格式，并且通常包含重复或“脏”数据。数据追溯到漫长的历史，包含不准确和不完整。</p><p id="d13e" class="pw-post-body-paragraph kw kx iq ky b kz la ka lb lc ld kd le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">当试图比较由不同收集方法编制的列表时，很快会遇到几个数据问题的挑战:</p><p id="f708" class="pw-post-body-paragraph kw kx iq ky b kz la ka lb lc ld kd le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">1.<strong class="ky ja">不明确的地址</strong>:根据数据收集的时间、地点和主要目的，它可能具有非结构化和非特定的地址、邮政编码甚至姓名。</p><p id="75a1" class="pw-post-body-paragraph kw kx iq ky b kz la ka lb lc ld kd le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">2.<strong class="ky ja">音译问题</strong>:将带有非罗马字符的地址翻译成英文文本时出现的不一致会导致地址和姓名的变化。</p><p id="41cd" class="pw-post-body-paragraph kw kx iq ky b kz la ka lb lc ld kd le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">3.<strong class="ky ja">联系信息复杂化</strong>:客户通常会有多个电子邮件地址和电话号码，当企业类型为B2B时，这个问题会变得更加突出，在B2B中，企业会有多个电话号码和电子邮件地址。</p><p id="59c6" class="pw-post-body-paragraph kw kx iq ky b kz la ka lb lc ld kd le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">4.<strong class="ky ja">最多重复值</strong>:常见的重复值是因为数据收集不正确，特别是在姓名、电子邮件、电话号码等项下。每种情况都必须得到适当的处理。</p><p id="7c1b" class="pw-post-body-paragraph kw kx iq ky b kz la ka lb lc ld kd le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ja">数据标准化:</strong></p><p id="6a68" class="pw-post-body-paragraph kw kx iq ky b kz la ka lb lc ld kd le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">开始重复数据消除之前的一个重要步骤是确保数据结构化、对齐并具有一致的格式。因此，遵循以下步骤是明智的:</p><p id="6d55" class="pw-post-body-paragraph kw kx iq ky b kz la ka lb lc ld kd le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">1.<strong class="ky ja">小写</strong>:高效最简单的一个步骤就是把所有东西都转换成小写，让名字和地址具有可比性。</p><p id="dc20" class="pw-post-body-paragraph kw kx iq ky b kz la ka lb lc ld kd le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">2.<strong class="ky ja">缩写</strong>:通过建立一个从互联网上获得的或定制的正在处理的数据的知识库，保持单词与缩写的一致性。示例:Street也表示为St .，St，Str。、字符串等。</p><p id="ef80" class="pw-post-body-paragraph kw kx iq ky b kz la ka lb lc ld kd le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">3.<strong class="ky ja">缺失值</strong>:评估每条记录的缺失值，如果有重大缺失值，则将其删除，不要继续处理，这样的记录质量将太低，无法帮助重复数据删除评分。</p><p id="7d82" class="pw-post-body-paragraph kw kx iq ky b kz la ka lb lc ld kd le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">4.<strong class="ky ja">不正确的国家名称</strong>:Python包“iso3166”包含所有国家的列表，帮助识别国家名称中不正确的国家名称。</p><p id="f2e0" class="pw-post-body-paragraph kw kx iq ky b kz la ka lb lc ld kd le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">5.<strong class="ky ja">不正确的邮政编码</strong>:‘pgeocode’是一个python库，包含所有国家的合法邮政编码，用于验证地址中的邮政编码。出于同样的目的，也有几个付费的API来帮助获得正确的邮政编码。</p><p id="7ce2" class="pw-post-body-paragraph kw kx iq ky b kz la ka lb lc ld kd le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ja">电话号码和电子邮件地址的验证:</strong></p><p id="c4a1" class="pw-post-body-paragraph kw kx iq ky b kz la ka lb lc ld kd le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">除了识别重复的电话号码之外，可以进行的附加验证是评估质量和标准化前缀为“+”或“0”或(国家代码)的格式。以下脚本有助于在几秒钟内标准化数百万个电话号码。可以通过付费API进行进一步的精确验证，该API有助于在线后处理重复数据删除期间的验证。</p><pre class="kp kq kr ks gt ls lt lu lv aw lw bi"><span id="fe8c" class="lx ly iq lt b gy lz ma l mb mc">def check_phone(phone, cc):<br/>    try: <br/>        if int(phone) &lt;1: return np.nan<br/>        if phone== np.nan: return np.nan<br/>    except:pass <br/>    <br/>    phone= str(phone)<br/>    cc= str(cc)</span><span id="550a" class="lx ly iq lt b gy md ma l mb mc">#print(phone, cc)<br/>    if len(phone)&gt;4 :<br/>        if phone != np.nan or cc != np.nan:<br/>            cc_len = len(cc)<br/>            if(phone[:1] == '+'):<br/>                if phone[1:1+cc_len] == cc:<br/>                    return phone<br/>                else: return phone<br/>            else:<br/>                cc_len = len(cc)<br/>                if phone[:1] == '0':<br/>                    if phone[1:1+cc_len]== cc:<br/>                        return phone.replace('0','+',1)<br/>                    else: <br/>                        return phone.replace('0','+'+cc,1)<br/>                else:<br/>                    if phone[0:cc_len]== cc:<br/>                        return '+'+phone<br/>                    else:return '+'+cc+phone<br/>    else: return np.nan</span><span id="7b1d" class="lx ly iq lt b gy md ma l mb mc"><br/># Function caller<br/>%time <br/>account[['calling_code','contact_Phone']].apply(lambda x : check_phone(x.contact_Phone, x.calling_code),axis = 1)</span></pre><p id="b5e4" class="pw-post-body-paragraph kw kx iq ky b kz la ka lb lc ld kd le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ja">重复的定义:</strong></p><p id="d821" class="pw-post-body-paragraph kw kx iq ky b kz la ka lb lc ld kd le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">定义数据中的哪些重复是该过程的一个重要方面。根据正在解决的问题，重复的定义会发生变化。对于最常见的客户数据，通常是姓名、邮政地址、电话号码和电子邮件地址。这些将形成一组字段，用于查找评分和帮助识别重复。很少有字段(如电话号码或电子邮件)会给出重复的明确指示，另一方面，姓名或地址的重复实际上可以表示实际上是分离的个体。因此，这种情况应该通过半自动/自动验证过程。</p><p id="ba18" class="pw-post-body-paragraph kw kx iq ky b kz la ka lb lc ld kd le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ja">最后的评分:</strong></p><p id="b23c" class="pw-post-body-paragraph kw kx iq ky b kz la ka lb lc ld kd le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">评分从基于城市/邮政编码与自身连接的表的自连接开始，然后使用模糊逻辑对其余列(如姓名、电子邮件和电话号码)进行评分。具有正确截止值的评分给出了数据中可能的重复列表，其余的将被丢弃。和往常一样，没有神奇的数字来确定临界值，需要几次迭代才能得出0-100之间的数字来定义数据的正确临界值。此外，每个列最好有不同的截止值，如下面的代码所示。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi ko"><img src="../Images/8a522f0030dc74263e36555445a95b9b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1248/format:webp/1*XGavi_7H2uOrNLxmcnk-VA.png"/></div><p class="me mf gj gh gi mg mh bd b be z dk translated">自联接后的结果表</p></figure><pre class="kp kq kr ks gt ls lt lu lv aw lw bi"><span id="583b" class="lx ly iq lt b gy lz ma l mb mc"># self-joining data based on the parameter<br/># Preparation of data for fuzzy<br/>def joiner(cntry , file , mcol):<br/>    df_string_all = account[(account.ShippingCountryCode == cntry)]<br/>    df_string_all = df_string_all[address_match_columns]<br/>    global dup<br/>    dup = pd.DataFrame()<br/>    parts = round(df_string_all.shape[0]/1000)<br/>    start_time = time.time()<br/>    if cntry == 'ie':<br/>        mcol = mcol.replace('PostalCode','City')</span><span id="9dc6" class="lx ly iq lt b gy md ma l mb mc">print(cntry.upper(), mcol)<br/>    total_uni = len(df_string_all[mcol].unique())<br/>    unique_col_value = df_string_all[mcol].unique()<br/>    rem = ['xxxxx','Nan','', 'NAN', 'nan', np.nan]<br/>    unique_col_value = [uni for uni in unique_col_value if uni not in rem]<br/>    <br/>    for i in range(1,parts+1): <br/>        my_list = unique_col_value[int(round(np.divide((1),parts)*total_uni*(i-1))):\<br/>   int(round(np.divide((1),parts)*total_uni*i))]<br/>        df_string = df_string_all[(df_string_all[mcol].isin(my_list))]<br/>        df_string = df_string.merge(df_string, on= mcol , how = 'left', suffixes=  ('1', '2'))<br/>        col_list = df_string.columns.sort_values().drop( ['Id1', 'Id2']).drop(mcol)</span><span id="dedc" class="lx ly iq lt b gy md ma l mb mc">df_string = df_string[(df_string.Id1 &lt; df_string.Id2)]<br/>        even = col_list[::2]<br/>        odd = col_list[1::2]<br/>        df_string = df_string[(df_string[['Name1' , 'Name2']].apply( lambda x:fuzz.token_sort_ratio(x['Name1'], x['Name2']), axis = 1) &gt; name_match_cutoff)]<br/>        if df_string.shape[0] &gt;0:<br/>            dup = dup.append(identifier(df_string, even, odd, mcol))<br/>            del df_string<br/>    del df_string_all<br/>    end_time = time.time()<br/>    print('Time taken for : ' ,cntry.upper() , mcol , round((end_time - start_time)/60,2) , ' minutes')<br/>    print('Duplicates for : ',cntry.upper() , mcol, dup.shape)<br/>    return dup</span></pre><p id="b301" class="pw-post-body-paragraph kw kx iq ky b kz la ka lb lc ld kd le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">下面的块根据用户定义的截止级别识别数据中的重复项:</p><pre class="kp kq kr ks gt ls lt lu lv aw lw bi"><span id="1a4a" class="lx ly iq lt b gy lz ma l mb mc">def identifier(df_string, col_even, col_odd, case):<br/>    for i in col_even: <br/>        for j in col_odd: <br/>            if(i[:-1] == j[:-1]):<br/>                new_col = i[:-1]+'_score'<br/>                df_string[new_col] = df_string.apply(lambda x: fuzz.token_sort_ratio(x[i], x[j]) , axis = 1)<br/>                df_string[new_col] = df_string.apply(lambda x: 0 if (pd.isnull(x[i]) | pd.isnull(x[j])) else x[new_col], axis=1)</span><span id="c622" class="lx ly iq lt b gy md ma l mb mc">col_score =  [k for k in df_string.columns if 'score' in k]<br/>    street_score = [k for k in col_score if 'Street' in k]<br/>    city_score = [k for k in col_score if 'City' in k] +[k for k in col_score if 'Post' in k]<br/>    <br/>    if case == 'Name': <br/>        duplicate_con = df_string[((df_string[street_score]&gt; street_match_cutoff).sum(axis= 1) &gt; 0) &amp;\<br/>                                            ((df_string[city_score]&gt; city_match_cutoff).sum(axis=1)&gt;0)]        <br/>    elif case == 'BillingStreet': <br/>        duplicate_con = df_string[((df_string[city_score]&gt; city_match_cutoff).sum(axis=1)&gt;0) &amp; \<br/>                                           (df_string['Name_score']&gt; name_match_cutoff)]<br/>    else: <br/>        duplicate_con = df_string[(df_string['Name_score'] &gt; name_match_cutoff) &amp; \<br/>                                            ((df_string[street_score] &gt; street_match_cutoff).sum(axis=1) &gt; 0) &amp; \<br/>                                          ((df_string[city_score]&gt; city_match_cutoff).sum(axis=1)&gt;0)]<br/>    if duplicate_con.shape[0] &gt;0: <br/>        duplicate_con['2Final_Score'] = round((duplicate_con[col_score].mean(axis = 1)))<br/>        duplicate_con['1Match_Case'] = case<br/>        duplicate_con[case+'1'] = duplicate_con[case]<br/>        duplicate_con[case+'2'] = duplicate_con[case]<br/>        duplicate_con[case+'_score'] = 100<br/>        duplicate_con= duplicate_con.drop(columns= case)<br/>        <br/>    return duplicate_con</span></pre><p id="4896" class="pw-post-body-paragraph kw kx iq ky b kz la ka lb lc ld kd le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">函数调用程序</p><pre class="kp kq kr ks gt ls lt lu lv aw lw bi"><span id="ed35" class="lx ly iq lt b gy lz ma l mb mc">country = list(account.ShippingCountryCode.unique())<br/>country = [e for e in country if e not in (['nan', np.nan])]<br/>duplicate = pd.DataFrame()<br/>duplicate_indi = pd.DataFrame()<br/>start = time.time()</span><span id="2870" class="lx ly iq lt b gy md ma l mb mc">for cntr in country: <br/>    file_name = 'account_'+ cntr<br/>    for cols in ['MailingPostalCode', 'PostalCode']:<br/>        duplicate = duplicate.append(joiner(cntr , file_name, cols))<br/>        <br/>end = time.time()<br/>print('Total Time taken:' , round((end - start)/60,2) , ' minutes')</span></pre><p id="1c87" class="pw-post-body-paragraph kw kx iq ky b kz la ka lb lc ld kd le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">通过LinkedIn与我进行有趣的对话:<a class="ae mi" href="http://www.linkedin.com/comm/mynetwork/discovery-see-all?usecase=PEOPLE_FOLLOWS&amp;followMember=shreepadahs" rel="noopener ugc nofollow" target="_blank">www.linkedin.com/comm/mynetwork/discovery-see-all?use case = PEOPLE _ FOLLOWS&amp;follow member = shreepadahs</a></p><p id="790e" class="pw-post-body-paragraph kw kx iq ky b kz la ka lb lc ld kd le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ja">参考文献:</strong></p><p id="8363" class="pw-post-body-paragraph kw kx iq ky b kz la ka lb lc ld kd le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">[1]<a class="ae mi" href="https://www.datacore.com/blog/inline-vs-post-process-deduplication-compression/" rel="noopener ugc nofollow" target="_blank">https://www . data core . com/blog/inline-vs-post-process-de duplication-compression/</a></p></div></div>    
</body>
</html>