<html>
<head>
<title>Training BPE, WordPiece, and Unigram Tokenizers from Scratch using Hugging Face</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用拥抱脸从头开始训练BPE、单词块和单字标记器</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/training-bpe-wordpiece-and-unigram-tokenizers-from-scratch-using-hugging-face-3dd174850713?source=collection_archive---------13-----------------------#2021-10-18">https://towardsdatascience.com/training-bpe-wordpiece-and-unigram-tokenizers-from-scratch-using-hugging-face-3dd174850713?source=collection_archive---------13-----------------------#2021-10-18</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="eca3" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">使用Hugging Face的tokenizers包比较SOTA标记化算法生成的标记</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/667de0fdf6488b41450dd06ab4dcb953.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*gMrHiJBIqBqkNMGE.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><p id="23a4" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">继续深入NLP的海洋，这篇文章是关于利用<strong class="la iu">humping Face的tokenizers包从头开始训练tokenizers。</strong></p><p id="40a4" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">标记化通常被认为是NLP的一个子领域，但它有自己的<a class="ae lu" href="https://dswharshit.substack.com/p/the-evolution-of-tokenization-byte" rel="noopener ugc nofollow" target="_blank">进化故事</a>，以及它是如何达到当前阶段的，它正在支撑最先进的NLP模型。</p><p id="1c36" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在我们开始训练和比较不同的记号赋予器的有趣部分之前，我想给你一个算法之间的关键区别的简要总结。</p><p id="46d1" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">主要区别在于<strong class="la iu">选择要合并的字符对</strong>和<strong class="la iu">合并策略</strong>，这些算法中的每一个用来生成最终的令牌集。</p><h1 id="5813" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">BPE——基于频率的模型</h1><ul class=""><li id="9e20" class="mn mo it la b lb mp le mq lh mr ll ms lp mt lt mu mv mw mx bi translated">字节对编码使用子字模式的频率将它们列入合并的候选名单。</li><li id="33af" class="mn mo it la b lb my le mz lh na ll nb lp nc lt mu mv mw mx bi translated">使用频率作为驱动因素的缺点是，最终可能会产生模糊的最终编码，这对新的输入文本可能没有用。</li><li id="ed0c" class="mn mo it la b lb my le mz lh na ll nb lp nc lt mu mv mw mx bi translated">在生成明确的令牌方面，它仍有改进的余地。</li></ul><h1 id="3a95" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">unigram——基于概率的模型</h1><ul class=""><li id="b946" class="mn mo it la b lb mp le mq lh mr ll ms lp mt lt mu mv mw mx bi translated">Unigram模型通过计算每个子词组合的可能性来解决合并问题，而不是选择最常见的模式。</li><li id="6be4" class="mn mo it la b lb my le mz lh na ll nb lp nc lt mu mv mw mx bi translated">它计算每个子词标记的概率，然后根据损失函数将其丢弃，该损失函数在本研究论文的<a class="ae lu" href="https://arxiv.org/pdf/1804.10959.pdf" rel="noopener ugc nofollow" target="_blank">中有所说明。</a></li><li id="8a5d" class="mn mo it la b lb my le mz lh na ll nb lp nc lt mu mv mw mx bi translated">基于损失值的特定阈值，您可以触发模型丢弃底部20–30%的子词标记。</li><li id="a338" class="mn mo it la b lb my le mz lh na ll nb lp nc lt mu mv mw mx bi translated">Unigram是一种完全概率算法，它根据概率选择字符对，并最终决定在每次迭代中合并(或不合并)。</li></ul><h1 id="ff1b" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">文字片</h1><ul class=""><li id="9fb5" class="mn mo it la b lb mp le mq lh mr ll ms lp mt lt mu mv mw mx bi translated">随着2018年BERT的发布，出现了一种新的子词标记化算法，称为WordPiece，可以认为是BPE和Unigram算法的中介。</li><li id="cbd3" class="mn mo it la b lb my le mz lh na ll nb lp nc lt mu mv mw mx bi translated">WordPiece也是一种贪婪算法，它利用似然性而不是计数频率来合并每次迭代中的最佳配对，但配对字符的选择是基于计数频率的。</li><li id="30a3" class="mn mo it la b lb my le mz lh na ll nb lp nc lt mu mv mw mx bi translated">因此，在选择要配对的字符方面，它类似于BPE，在选择要合并的最佳字符对方面，它类似于Unigram。</li></ul><p id="ef0b" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">考虑到算法上的差异，我尝试实现这些算法中的每一个(不是从头开始)，以比较它们各自生成的输出。</p><h1 id="2246" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">训练BPE、Unigram和WordPiece算法</h1><p id="7c50" class="pw-post-body-paragraph ky kz it la b lb mp ju ld le mq jx lg lh nd lj lk ll ne ln lo lp nf lr ls lt im bi translated">现在，为了对输出进行无偏见的比较，我不想使用预先训练的算法，因为这会将数据集的大小、质量和内容带入图片中。</p><p id="2d59" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">一种方法可能是使用研究论文从头开始编写这些算法，然后测试它们，这是一种很好的方法，以便真正理解每个算法的工作原理，但你可能会花费数周时间来完成这项工作。</p><p id="e974" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">相反，我使用了<a class="ae lu" href="https://huggingface.co/docs/tokenizers/python/latest/quicktour.html" rel="noopener ugc nofollow" target="_blank">拥抱脸的记号赋予器</a>包，它提供了所有当今最常用的记号赋予器的实现。它还允许我在我选择的数据集上从头开始训练这些模型，然后标记我们自己选择的输入字符串。</p><h2 id="30f2" class="ng lw it bd lx nh ni dn mb nj nk dp mf lh nl nm mh ll nn no mj lp np nq ml nr bi translated">训练数据集</h2><p id="2e49" class="pw-post-body-paragraph ky kz it la b lb mp ju ld le mq jx lg lh nd lj lk ll ne ln lo lp nf lr ls lt im bi translated">我选择了两个不同的数据集来训练这些模型，一个是来自古腾堡的免费书籍，作为一个小数据集，另一个是<a class="ae lu" href="https://blog.einstein.ai/the-wikitext-long-term-dependency-language-modeling-dataset/" rel="noopener ugc nofollow" target="_blank"> wikitext-103 </a>，它是516M的文本。</p><p id="4b3c" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在Colab中，您可以先下载数据集并解压缩(如果需要的话)，</p><pre class="kj kk kl km gt ns nt nu nv aw nw bi"><span id="cdcc" class="ng lw it nt b gy nx ny l nz oa">!wget <a class="ae lu" href="http://www.gutenberg.org/cache/epub/16457/pg16457.txt" rel="noopener ugc nofollow" target="_blank">http://www.gutenberg.org/cache/epub/16457/pg16457.txt</a></span><span id="67fc" class="ng lw it nt b gy ob ny l nz oa">!wget <a class="ae lu" href="https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-103-raw-v1.zip" rel="noopener ugc nofollow" target="_blank">https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-103-raw-v1.zip</a></span><span id="8a64" class="ng lw it nt b gy ob ny l nz oa">!unzip wikitext-103-raw-v1.zip</span></pre><h2 id="39ac" class="ng lw it bd lx nh ni dn mb nj nk dp mf lh nl nm mh ll nn no mj lp np nq ml nr bi translated">导入所需的模型和训练器</h2><p id="d15c" class="pw-post-body-paragraph ky kz it la b lb mp ju ld le mq jx lg lh nd lj lk ll ne ln lo lp nf lr ls lt im bi translated">浏览文档，你会发现这个包的主要API是类<code class="fe oc od oe nt b">Tokenizer.</code></p><p id="c473" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">然后，您可以用您选择的模型(BPE/ Unigram/ WordPiece)实例化任何记号化器。</p><p id="f1bc" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在这里，我导入了主类，所有我想测试的模型，以及它们的训练器，因为我想从头开始训练这些模型。</p><pre class="kj kk kl km gt ns nt nu nv aw nw bi"><span id="42ca" class="ng lw it nt b gy nx ny l nz oa">## importing the tokenizer and subword BPE trainer<br/>from tokenizers import Tokenizer<br/>from tokenizers.models import BPE, Unigram, WordLevel, WordPiece<br/>from tokenizers.trainers import BpeTrainer, WordLevelTrainer, \<br/>                                WordPieceTrainer, UnigramTrainer</span><span id="8165" class="ng lw it nt b gy ob ny l nz oa">## a pretokenizer to segment the text into words<br/>from tokenizers.pre_tokenizers import Whitespace</span></pre><h2 id="3f9e" class="ng lw it bd lx nh ni dn mb nj nk dp mf lh nl nm mh ll nn no mj lp np nq ml nr bi translated">自动化培训和令牌化的3步流程</h2><p id="e4de" class="pw-post-body-paragraph ky kz it la b lb mp ju ld le mq jx lg lh nd lj lk ll ne ln lo lp nf lr ls lt im bi translated">因为我需要为三个不同的模型执行有些相似的过程，所以我将这些过程分成三个函数。我只需要为每个模型调用这些函数，我在这里的工作就完成了。</p><p id="8052" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">那么，这些函数是什么样子的呢？</p><h2 id="9d1b" class="ng lw it bd lx nh ni dn mb nj nk dp mf lh nl nm mh ll nn no mj lp np nq ml nr bi translated">步骤1 —准备标记器</h2><p id="9806" class="pw-post-body-paragraph ky kz it la b lb mp ju ld le mq jx lg lh nd lj lk ll ne ln lo lp nf lr ls lt im bi translated">准备tokenizer需要我们用我们选择的模型实例化tokenizer类，但是因为我们有四个模型(还添加了一个简单的单词级算法)要测试，我们将编写if/else案例来用正确的模型实例化Tokenizer。</p><p id="19e3" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">为了在小型和大型数据集上训练实例化的记号赋予器，我们还需要实例化一个训练器，在我们的例子中，它们是<code class="fe oc od oe nt b"><a class="ae lu" href="https://huggingface.co/docs/tokenizers/python/latest/api/reference.html#tokenizers.trainers.BpeTrainer" rel="noopener ugc nofollow" target="_blank"><strong class="la iu">BpeTrainer</strong></a><strong class="la iu">, WordLevelTrainer, WordPieceTrainer, and UnigramTrainer.</strong></code></p><p id="5ea5" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">实例化和训练将需要我们指定一些特殊的令牌。这些是未知单词的标记和其他特殊标记，我们以后需要使用它们来增加我们的词汇。</p><p id="7186" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">您还可以在此指定其他训练参数的词汇大小或最小频率。</p><pre class="kj kk kl km gt ns nt nu nv aw nw bi"><span id="ed1e" class="ng lw it nt b gy nx ny l nz oa">unk_token = "&lt;UNK&gt;"  # token for unknown words<br/>spl_tokens = ["&lt;UNK&gt;", "&lt;SEP&gt;", "&lt;MASK&gt;", "&lt;CLS&gt;"]  # special tokens</span><span id="2411" class="ng lw it nt b gy ob ny l nz oa">def prepare_tokenizer_trainer(alg):<br/>    """<br/>    Prepares the tokenizer and trainer with unknown &amp; special tokens.<br/>    """<br/>    if alg == 'BPE':<br/>        tokenizer = Tokenizer(BPE(unk_token = unk_token))<br/>        trainer = BpeTrainer(special_tokens = spl_tokens)<br/>    elif alg == 'UNI':<br/>        tokenizer = Tokenizer(Unigram())<br/>        trainer = UnigramTrainer(unk_token= unk_token, special_tokens = spl_tokens)<br/>    elif alg == 'WPC':<br/>        tokenizer = Tokenizer(WordPiece(unk_token = unk_token))<br/>        trainer = WordPieceTrainer(special_tokens = spl_tokens)<br/>    else:<br/>        tokenizer = Tokenizer(WordLevel(unk_token = unk_token))<br/>        trainer = WordLevelTrainer(special_tokens = spl_tokens)<br/>    <br/>    tokenizer.pre_tokenizer = Whitespace()<br/>    return tokenizer, trainer</span></pre><p id="015d" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们还需要添加一个预标记器来将我们的输入拆分成单词，因为没有预标记器，我们可能会得到重叠几个单词的标记:例如，我们可以得到一个<code class="fe oc od oe nt b"><strong class="la iu">"there is"</strong></code>标记，因为这两个单词经常相邻出现。</p><blockquote class="of og oh"><p id="f720" class="ky kz oi la b lb lc ju ld le lf jx lg oj li lj lk ok lm ln lo ol lq lr ls lt im bi translated">使用预标记器将确保没有标记大于预标记器返回的单词。</p></blockquote><p id="b1d6" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">该函数将返回tokenizer及其trainer对象，可用于在数据集上训练模型。</p><p id="cb49" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这里，我们对所有模型使用相同的预标记器(<code class="fe oc od oe nt b">Whitespace</code>)。可以选择用<a class="ae lu" href="https://huggingface.co/docs/tokenizers/python/latest/api/reference.html#module-tokenizers.pre_tokenizers" rel="noopener ugc nofollow" target="_blank">其他</a>来测试。</p><h2 id="0975" class="ng lw it bd lx nh ni dn mb nj nk dp mf lh nl nm mh ll nn no mj lp np nq ml nr bi translated">步骤2 —培训分词器</h2><p id="3892" class="pw-post-body-paragraph ky kz it la b lb mp ju ld le mq jx lg lh nd lj lk ll ne ln lo lp nf lr ls lt im bi translated">准备好记号赋予者和训练者之后，我们就可以开始训练过程了。</p><p id="0310" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这里有一个函数，它将获取我们打算用来训练我们的标记器的文件以及算法标识符。</p><ul class=""><li id="02b6" class="mn mo it la b lb lc le lf lh om ll on lp oo lt mu mv mw mx bi translated"><code class="fe oc od oe nt b">‘WLV’</code> -词级算法</li><li id="833d" class="mn mo it la b lb my le mz lh na ll nb lp nc lt mu mv mw mx bi translated"><code class="fe oc od oe nt b">‘WPC’</code> -计件算法</li><li id="4331" class="mn mo it la b lb my le mz lh na ll nb lp nc lt mu mv mw mx bi translated"><code class="fe oc od oe nt b">‘BPE’</code> -字节对编码</li><li id="3e75" class="mn mo it la b lb my le mz lh na ll nb lp nc lt mu mv mw mx bi translated"><code class="fe oc od oe nt b">‘UNI’</code> -单字</li></ul><pre class="kj kk kl km gt ns nt nu nv aw nw bi"><span id="3bdc" class="ng lw it nt b gy nx ny l nz oa">def train_tokenizer(files, alg='WLV'):<br/>    """<br/>    Takes the files and trains the tokenizer.<br/>    """<br/>    tokenizer, trainer = prepare_tokenizer_trainer(alg)<br/>    tokenizer.train(files, trainer) # training the tokenzier<br/>    tokenizer.save("./tokenizer-trained.json")<br/>    tokenizer = Tokenizer.from_file("./tokenizer-trained.json")<br/>    return tokenizer</span></pre><p id="9019" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这是我们在训练分词器时需要调用的主要函数，它将首先准备分词器和训练器，然后用提供的文件开始训练分词器。</p><p id="77ec" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">训练之后，它将模型保存在一个JSON文件中，从该文件中加载它，并返回经过训练的标记器，开始对新输入进行编码。</p><h2 id="ced8" class="ng lw it bd lx nh ni dn mb nj nk dp mf lh nl nm mh ll nn no mj lp np nq ml nr bi translated">步骤3 —对输入字符串进行标记</h2><p id="1e9f" class="pw-post-body-paragraph ky kz it la b lb mp ju ld le mq jx lg lh nd lj lk ll ne ln lo lp nf lr ls lt im bi translated">最后一步是开始对新的输入字符串进行编码，并比较每个算法生成的标记。</p><p id="24e2" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这里，我们将编写一个嵌套的for循环，首先在较小的数据集上训练每个模型，然后在较大的数据集上训练，并标记输入字符串。</p><p id="674a" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu">输入字符串— </strong>“这是一个深度学习的记号化教程。标记化是深度学习NLP管道的第一步。我们将比较每个令牌化模型生成的令牌。很兴奋吗？！😍"</p><pre class="kj kk kl km gt ns nt nu nv aw nw bi"><span id="9e98" class="ng lw it nt b gy nx ny l nz oa">small_file = ['pg16457.txt']<br/>large_files = [f"./wikitext-103-raw/wiki.{split}.raw" for split in ["test", "train", "valid"]]</span><span id="95ac" class="ng lw it nt b gy ob ny l nz oa">for files in [small_file, large_files]:<br/>    print(f"========Using vocabulary from {files}=======")<br/>    for alg in ['WLV', 'BPE', 'UNI', 'WPC']:<br/>        trained_tokenizer = train_tokenizer(files, alg)<br/>        input_string = "This is a deep learning tokenization tutorial. Tokenization is the first step in a deep learning NLP pipeline. We will be comparing the tokens generated by each tokenization model. Excited much?!😍"<br/>        output = tokenize(input_string, trained_tokenizer)<br/>        tokens_dict[alg] = output.tokens<br/>        print("----", alg, "----")<br/>        print(output.tokens, "-&gt;", len(output.tokens))</span></pre><p id="caf8" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu"># #输出:</strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi op"><img src="../Images/38990062c8655cf96676639a1bda5f35.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*ly3w5X_Qobd0NwpG.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><h1 id="7c60" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">输出分析:</h1><p id="ccdb" class="pw-post-body-paragraph ky kz it la b lb mp ju ld le mq jx lg lh nd lj lk ll ne ln lo lp nf lr ls lt im bi translated">查看输出，您将看到令牌生成方式的差异，这反过来导致生成不同数量的令牌。</p><ul class=""><li id="212b" class="mn mo it la b lb lc le lf lh om ll on lp oo lt mu mv mw mx bi translated">一个简单的<strong class="la iu">单词级算法</strong>创建了35个标记，不管它是在哪个数据集上被训练的。</li><li id="b581" class="mn mo it la b lb my le mz lh na ll nb lp nc lt mu mv mw mx bi translated"><strong class="la iu"> BPE </strong>算法在较小数据集上训练时创建了55个令牌，在较大数据集上训练时创建了47个令牌。这表明，当在更大的数据集上训练时，它能够合并更多的字符对。</li><li id="f2ec" class="mn mo it la b lb my le mz lh na ll nb lp nc lt mu mv mw mx bi translated"><strong class="la iu"> Unigram模型</strong>为两个数据集创建了数量相似(68和67)的令牌。但是您可以看到生成的令牌之间的差异:</li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oq"><img src="../Images/ee146b63f6a9a74a2e7e518183296917.png" data-original-src="https://miro.medium.com/v2/resize:fit:856/format:webp/0*_a3UrSQKfUJVauJr.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><p id="40c0" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">对于更大的数据集，合并更接近于生成更适合编码我们经常使用的真实世界英语语言的标记。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi or"><img src="../Images/f73f8b7c47fc2647f5f4ed3ff09a15c7.png" data-original-src="https://miro.medium.com/v2/resize:fit:520/format:webp/0*fkSuiO4cwVCoOVkD.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><ul class=""><li id="3433" class="mn mo it la b lb lc le lf lh om ll on lp oo lt mu mv mw mx bi translated">在较小的数据集上训练时，WordPiece 创建了52个令牌，在较大的数据集上训练时，创建了48个令牌。生成的标记有两个##，表示标记用作前缀/后缀。</li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi os"><img src="../Images/e17e70a481025256be7df405d25d065e.png" data-original-src="https://miro.medium.com/v2/resize:fit:782/format:webp/0*Z5Ia4X67PrUxegdJ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">由作者制作的图像</p></figure><ul class=""><li id="79d5" class="mn mo it la b lb lc le lf lh om ll on lp oo lt mu mv mw mx bi translated">当在更大的数据集上训练时，这三种算法都生成了更少和更好的子词标记。</li></ul><h1 id="49fb" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">比较令牌</h1><p id="9c58" class="pw-post-body-paragraph ky kz it la b lb mp ju ld le mq jx lg lh nd lj lk ll ne ln lo lp nf lr ls lt im bi translated">为了比较标记，我将每个算法的输出存储在一个字典中，并将它转换成一个数据帧，以便更好地查看标记的差异。</p><p id="9c33" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">由于每个模型生成的令牌数量不同，我添加了一个<pad>令牌来使数据呈矩形并适合一个数据帧。</pad></p><p id="ce00" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><pad>基本上是数据帧中的nan。</pad></p><pre class="kj kk kl km gt ns nt nu nv aw nw bi"><span id="ac14" class="ng lw it nt b gy nx ny l nz oa">import pandas as pd</span><span id="5ba7" class="ng lw it nt b gy ob ny l nz oa">max_len = max(len(tokens_dict['UNI']), len(tokens_dict['WPC']), len(tokens_dict['BPE']))<br/>diff_bpe = max_len - len(tokens_dict['BPE'])<br/>diff_wpc = max_len - len(tokens_dict['WPC'])</span><span id="c848" class="ng lw it nt b gy ob ny l nz oa">tokens_dict['BPE'] = tokens_dict['BPE'] + ['&lt;PAD&gt;']*diff_bpe<br/>tokens_dict['WPC'] = tokens_dict['WPC'] + ['&lt;PAD&gt;']*diff_wpc</span><span id="980f" class="ng lw it nt b gy ob ny l nz oa">del tokens_dict['WLV']</span><span id="effc" class="ng lw it nt b gy ob ny l nz oa">df = pd.DataFrame(tokens_dict)<br/>df.head(10)</span></pre><p id="5337" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu"># #输出:</strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ot"><img src="../Images/ebbb9aa1d62b7db80422a6a7ff8ae3b1.png" data-original-src="https://miro.medium.com/v2/resize:fit:612/format:webp/0*whPmnGOyF_dgotat.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><p id="d98c" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">您还可以使用集合来查看标记的差异:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ou"><img src="../Images/a66f4e7cda0d0bf95db294dcfebc4efb.png" data-original-src="https://miro.medium.com/v2/resize:fit:740/format:webp/0*BHqvCgCX-Zke79i1.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ou"><img src="../Images/9dba36989c1ba529eba13fef0befcbc7.png" data-original-src="https://miro.medium.com/v2/resize:fit:740/format:webp/0*zrHjX-vYC5wd0B6i.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片作者图片</p></figure><p id="d266" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">所有的代码都可以在这个<a class="ae lu" href="https://colab.research.google.com/drive/10gwzRY55JqzgeEQOX6nwFs6bQ84-mB9f?usp=sharing" rel="noopener ugc nofollow" target="_blank"> Colab笔记本里找到。</a></p><h1 id="1649" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">结束想法和后续步骤！</h1><p id="49b5" class="pw-post-body-paragraph ky kz it la b lb mp ju ld le mq jx lg lh nd lj lk ll ne ln lo lp nf lr ls lt im bi translated">根据生成的标记的种类，WPC似乎确实生成了在英语中更常见的子词标记，但我并不认为这是一种观察结果。</p><p id="5a35" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这些算法彼此略有不同，但在开发一个像样的NLP模型方面做得有些类似。但是大部分性能取决于语言模型的用例、词汇量、速度和其他因素。</p><p id="e112" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这些算法的进一步发展是<a class="ae lu" href="https://arxiv.org/pdf/1808.06226.pdf" rel="noopener ugc nofollow" target="_blank"> SentencePiece算法</a>，这是一种解决整个标记化问题的有益方法，但这个问题的大部分通过HuggingFace得以缓解，甚至更好的是，他们在单个GitHub repo中实现了所有算法。</p><p id="c710" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这就结束了标记化算法，下一步是理解什么是嵌入，标记化如何在创建这些嵌入中发挥重要作用，以及它们如何影响模型的性能。</p></div><div class="ab cl ov ow hx ox" role="separator"><span class="oy bw bk oz pa pb"/><span class="oy bw bk oz pa pb"/><span class="oy bw bk oz pa"/></div><div class="im in io ip iq"><h1 id="53c8" class="lv lw it bd lx ly pc ma mb mc pd me mf jz pe ka mh kc pf kd mj kf pg kg ml mm bi translated">参考文献和注释</h1><p id="f86c" class="pw-post-body-paragraph ky kz it la b lb mp ju ld le mq jx lg lh nd lj lk ll ne ln lo lp nf lr ls lt im bi translated">如果你不同意我的分析或我在这篇文章中的任何工作，我强烈建议你查看这些资源，以准确理解每个算法的工作原理:</p><ol class=""><li id="ad07" class="mn mo it la b lb lc le lf lh om ll on lp oo lt ph mv mw mx bi translated"><a class="ae lu" href="https://arxiv.org/pdf/1804.10959.pdf" rel="noopener ugc nofollow" target="_blank">子词正则化:改进神经网络翻译模型</a>使用拥抱脸从头开始训练BPE、单词块和单字标记器</li><li id="f8eb" class="mn mo it la b lb my le mz lh na ll nb lp nc lt ph mv mw mx bi translated"><a class="ae lu" href="https://arxiv.org/pdf/1508.07909.pdf" rel="noopener ugc nofollow" target="_blank">具有子词单元的稀有词的神经机器翻译</a> —讨论基于BPE压缩算法的不同分割技术的研究论文。</li><li id="7845" class="mn mo it la b lb my le mz lh na ll nb lp nc lt ph mv mw mx bi translated"><a class="ae lu" href="https://huggingface.co/docs/tokenizers/python/latest/quicktour.html" rel="noopener ugc nofollow" target="_blank">拥抱脸的tokenizer包。</a></li></ol></div><div class="ab cl ov ow hx ox" role="separator"><span class="oy bw bk oz pa pb"/><span class="oy bw bk oz pa pb"/><span class="oy bw bk oz pa"/></div><div class="im in io ip iq"><h1 id="a344" class="lv lw it bd lx ly pc ma mb mc pd me mf jz pe ka mh kc pf kd mj kf pg kg ml mm bi translated">和我联系！</h1><p id="f523" class="pw-post-body-paragraph ky kz it la b lb mp ju ld le mq jx lg lh nd lj lk ll ne ln lo lp nf lr ls lt im bi translated">如果你想在数据科学或ML领域起步，请查看我的课程<a class="ae lu" href="https://www.wiplane.com/p/foundations-for-data-science-ml" rel="noopener ugc nofollow" target="_blank"> <strong class="la iu">数据科学基础&amp; ML </strong> </a>。</p><p id="eebc" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">如果你想看到更多这样的内容，并且你不是订阅者，可以考虑订阅<a class="ae lu" href="https://dswharshit.substack.com/" rel="noopener ugc nofollow" target="_blank">我的简讯</a>。</p><p id="9c1e" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">如有任何补充或建议，您可以通过以下方式联系我:</p><ul class=""><li id="8e96" class="mn mo it la b lb lc le lf lh om ll on lp oo lt mu mv mw mx bi translated"><a class="ae lu" href="https://www.youtube.com/channel/UCH-xwLTKQaABNs2QmGxK2bQ" rel="noopener ugc nofollow" target="_blank"> YouTube </a></li><li id="9467" class="mn mo it la b lb my le mz lh na ll nb lp nc lt mu mv mw mx bi translated"><a class="ae lu" href="https://twitter.com/dswharshit" rel="noopener ugc nofollow" target="_blank">推特</a></li><li id="c9bb" class="mn mo it la b lb my le mz lh na ll nb lp nc lt mu mv mw mx bi translated"><a class="ae lu" href="https://www.linkedin.com/in/tyagiharshit/" rel="noopener ugc nofollow" target="_blank">领英</a></li></ul></div></div>    
</body>
</html>