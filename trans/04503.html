<html>
<head>
<title>A Primer on the EM Algorithm</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">EM算法的初级读本</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/a-primer-on-the-em-algorithm-7bd60e9813e?source=collection_archive---------22-----------------------#2021-04-17">https://towardsdatascience.com/a-primer-on-the-em-algorithm-7bd60e9813e?source=collection_archive---------22-----------------------#2021-04-17</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="47be" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">克里斯蒂安·祖尼加博士</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="gh gi kl"><img src="../Images/da78dd1b1895f53d8c05b55818e6eaac.png" data-original-src="https://miro.medium.com/v2/resize:fit:720/1*OyXgise21a23D5JCss8Tlg.gif"/></div><p class="kt ku gj gh gi kv kw bd b be z dk translated">图一。用于混合模型的EM模型示例[1]</p></figure><p id="ab49" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">期望最大化(EM)算法是机器学习中用于估计模型参数的主要算法之一[2][3][4]。例如，它用于估计混合模型中的混合系数、均值和协方差，如图1所示。其目标是最大化似然p(X|θ),其中X是观测数据的矩阵，θ是模型参数的向量。这是最大似然估计，实际上对数似然ln p(X| θ)是最大的。最大化该函数的模型参数被认为是正确的模型参数。EM是梯度下降的有用替代，并且可能具有几个优点。</p><p id="6f86" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">1它更自然地实施约束，例如保持协方差矩阵正定。</p><p id="b0d8" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">2.在许多情况下，例如对于混合模型，它可能收敛得更快。</p><p id="33c4" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">3.它提供了一种处理缺失数据的方法。</p><p id="bf70" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">不幸的是，EM算法对初学者来说可能不容易理解。EM算法的第一步是假设存在帮助生成数据的隐藏变量Z。只有数据X被直接观察到。隐藏变量有助于将问题分成两个步骤，有时更简单，期望和最大化。通过对联合分布p(X，Z | θ)进行边缘化，可以将可能性写成隐藏变量Z的形式。总和是z的所有可能值。</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="gh gi kx"><img src="../Images/6c31ae15db16d92868c09885289c9343.png" data-original-src="https://miro.medium.com/v2/resize:fit:1126/format:webp/1*iyJ-PZTBBOnLJxNqNhcGGQ.png"/></div></div></figure><p id="fc54" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">推导EM算法需要理解三个主要概念:两个以上变量的Bayes定理、凹函数的Jensen不等式以及比较概率分布的Kullback-Leibler散度。</p><p id="086a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">贝叶斯定理在em算法中起着重要的作用。许多人都熟悉两个变量的定理，但是如上所示，三个变量会发生什么呢？虽然形式更复杂，但条件概率的定义仍然适用，它可以用联合分布p(X，Z，θ)来表示。</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="gh gi lc"><img src="../Images/df6347f469d4cfa36dc19d7396ae0b47.png" data-original-src="https://miro.medium.com/v2/resize:fit:1192/format:webp/1*e-rQ9XCixBNvt-ShxRxiWw.png"/></div></figure><p id="d10f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">无论变量的顺序如何，联合分布都是相同的，这是操纵所有条件概率的基础。在EM算法中，将使用X和θ来估计Z，因此条件概率p(Z | X，θ)很重要。使用上式中的联合分布，Z的后验分布可表示如下。</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="gh gi ld"><img src="../Images/8137a988a36321aa38a8bbc48cfccf4b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aLG-xkR1Yc0dQgyQGuc7yw.png"/></div></div></figure><p id="4f98" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">方程可以简化为p(X | θ) = p(X，θ)/p(θ)。</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="gh gi le"><img src="../Images/b40aec9912f052978f29c4dd7f475746.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SUU81VoYTDezYUZz9TVyJQ.png"/></div></div></figure><p id="b1d3" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">詹森不等式是获得EM算法所需的第二个结果。对于凹函数f(x)，f(x)的期望E[f(x)]，小于或等于先应用函数再取期望f(E[x])。简洁地说，E[f(x)] ≤ f(E[x])。</p><p id="ffc5" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">像对数一样，凹函数是那些向外“凸出”的函数，如图2所示。该函数总是大于或等于弦。数学上，对于x和y之间的任意点p，f(p) ≥ L(p)其中L(p)是在p处评估的线的值。点p(或x和y之间的任意点)可以通过p = w1 x + w2 y表示为x和y的加权组合。权重大于零，总和为1。利用直线的方程，值L(p)可以写成L(p) = w2 f(y) + (1- w2) f(x)。所以对于凹函数<em class="lf">f(w1 x+w2 y)≥w2 f(y)+(1-w2)f(x)</em>。</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="gh gi lg"><img src="../Images/8f2a9afa7b651b5d1453d9944bede4f9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rvB1pQjmpQRSlyiGutLM4Q.png"/></div></div><p class="kt ku gj gh gi kv kw bd b be z dk translated">图2凹函数位于任意弦之上。</p></figure><p id="3088" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">显示詹森不等式的下一步如下，因为期望可以被认为是一个加权和，概率分布给出了权重。一般来说，概率会给出两个以上的权重，但前面的结果可以归纳为许多权重。</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="gh gi lh"><img src="../Images/348b1d9252f0be6584c300f4e9cb5793.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0-Nu_TmqVhi39ISMoS0n-g.png"/></div></div></figure><p id="abac" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">要理解的最后一个概念是Kullback-Leibler散度，它是两个概率分布q(z)和p(z)之间不相似性的度量。当两个分布相等时，KL散度总是大于或等于零，q(z) = p(z)。它不是对称的，KL(p||q) ≠ KL(q||p)，所以它不是距离测度。当真实分布是p(z)时，它可以被解释为使用分布q(z)编码数据所需的额外比特数，反之亦然。</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="gh gi li"><img src="../Images/86696b8f22a29984a88ddf9eb8037568.png" data-original-src="https://miro.medium.com/v2/resize:fit:1344/format:webp/1*YiJB9_NzrfGp4PpXpCrRGA.png"/></div></figure><p id="89b4" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">掌握了这3个概念，EM算法就更容易理解了。隐藏变量Z也具有分布q(Z)。将q(z)作为q(z)/q(z) = 1引入对数似然中，并利用对数函数是凹的这一事实，詹森不等式给出了关于分布q(z)的期望。</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="gh gi lj"><img src="../Images/1d8776a36ab6f9268bc535dd5b29ee9b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8ItW4eT0TbCirm4c4lhuFQ.png"/></div></div></figure><p id="852f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这个不等式给出了对数似然的一个下界。对数似然和L(q，θ)之间的差异可以看作是q(z)和p(Z|X，θ)之间的KL散度。用p(X，Z| θ) = P(Z|X，θ)P(X| θ)的展开式来证明这一点。</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="gh gi lk"><img src="../Images/0c22a101acf047b86e202dc32f16f390.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*09bel7xnFcyG1gN65jlFBQ.png"/></div></div></figure><p id="fd59" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">当KL散度为零时，或者当q(z) = p(Z|X，θ)时，完全相等。</p><p id="f702" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">最大化对数似然可以通过最大化L(q，θ)来间接实现。有两种方法可以最大化L，即改变q或改变参数θ。独立地改变每一个更简单。顾名思义，EM算法有两个步骤，期望和最大化，并迭代进行。首先初始化参数。分布q(z)在期望步骤中更新，保持参数θ固定为θold。这仅仅涉及到设置q(z) = p(Z|X，θold)。然后更新参数θ以最大化L(q，θ),保持在最大化步骤中固定的q(z)。</p><p id="79be" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">期待</strong></p><p id="415a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">设q(z) = p(Z|X，θold)代入L。</p><p id="12b0" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">最大化</strong></p><p id="0b10" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">保持q(z)不变，改变θ，最大化L(p(Z|X，θold)，θ。</p><p id="2ca7" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这两个步骤保证总是增加可能性并且至少收敛到局部最大值。最大化步骤可以根据分布的形式以不同的方式进行。例如，在高斯混合模型中，最大似然估计的修改给出了分析结果。在其他情况下，可能没有解析解，但是求解最大化步骤可能仍然比原始问题简单。</p><p id="8396" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">参考文献</strong></p><p id="0a06" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">[1]https://en.wikipedia.org/wiki/<a class="ae ll" href="https://en.wikipedia.org/wiki/" rel="noopener ugc nofollow" target="_blank">期望% E2 % 80 %最大化_算法</a></p><p id="d4d9" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">【2】<a class="ae ll" href="https://en.wikipedia.org/wiki/Arthur_P._Dempster" rel="noopener ugc nofollow" target="_blank"/>邓普斯特；<a class="ae ll" href="https://en.wikipedia.org/wiki/Nan_Laird" rel="noopener ugc nofollow" target="_blank">莱尔德，n . m .</a>；鲁宾，D.B.  (1977)。“通过EM算法不完整数据的最大可能性”。<a class="ae ll" href="https://en.wikipedia.org/wiki/Journal_of_the_Royal_Statistical_Society,_Series_B" rel="noopener ugc nofollow" target="_blank"> <em class="lf">《英国皇家统计学会杂志》，B辑</em> </a>。<strong class="jp ir">39</strong>(1):1–38。<a class="ae ll" href="https://en.wikipedia.org/wiki/JSTOR_(identifier)" rel="noopener ugc nofollow" target="_blank">JSTOR</a>T18】2984875</p><p id="4f4b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">[3] Bishop模式识别和机器学习Springer 2006</p><p id="a878" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">[4]墨菲机器学习从概率角度看麻省理工学院出版社2012年</p></div></div>    
</body>
</html>