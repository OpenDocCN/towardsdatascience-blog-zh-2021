<html>
<head>
<title>EDA: Easy Data Augmentation Techniques for Boosting Performance on Text Classification Tasks</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">EDA:提高文本分类性能的简单数据扩充技术</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/eda-easy-data-augmentation-techniques-for-boosting-performance-on-text-classification-tasks-3e61a56d1332?source=collection_archive---------33-----------------------#2021-05-05">https://towardsdatascience.com/eda-easy-data-augmentation-techniques-for-boosting-performance-on-text-classification-tasks-3e61a56d1332?source=collection_archive---------33-----------------------#2021-05-05</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="53cf" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">研究论文解释</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/8a2595e6a61f14cc188c0d365f9f56d7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*u_nOwHNSJpRIXenIXGTphQ.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><p id="173b" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu">机器学习中的数据扩充</strong>是一种流行的技术，即使在数据可用性较低的情况下，也能使<strong class="la iu">稳健并推广ML模型</strong>。通过添加对现有数据稍加修改的副本或从现有数据中新创建的合成数据，它有助于增加原始数据的数量。当在低质量和大小的数据上训练机器学习模型时，添加各种数据大大有助于<strong class="la iu">减少过度拟合</strong>。</p><p id="6789" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">人们已经看到它在计算机视觉应用中工作得非常好，如图像分类、对象检测等——在这些应用中，我们现在已经有了一套变换函数，如旋转、剪切、裁剪等，这些函数很容易获得、研究过并已知可以工作(当然，很少需要注意)。来自Protago Labs Research、Dartmouth College和Georgetown University的这篇论文介绍了自然语言处理中的<strong class="la iu">数据扩充转换功能，特别关注文本分类任务</strong>。</p><p id="1868" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">有了这个背景和介绍，让我们直接开始研究提出的方法。</p><h2 id="a386" class="lu lv it bd lw lx ly dn lz ma mb dp mc lh md me mf ll mg mh mi lp mj mk ml mm bi translated">提议的转换功能</h2><ol class=""><li id="0784" class="mn mo it la b lb mp le mq lh mr ll ms lp mt lt mu mv mw mx bi translated"><em class="my">同义词替换(SR) </em> —从句子中随机选择n个不是停用词的单词。用随机选择的同义词替换这些单词。</li><li id="d3b6" class="mn mo it la b lb mz le na lh nb ll nc lp nd lt mu mv mw mx bi translated"><em class="my">随机插入(RI) </em> —在句子中找到一个非停用词的随机词的随机同义词。将同义词插入句子中任意位置。这样做n次。</li><li id="85f2" class="mn mo it la b lb mz le na lh nb ll nc lp nd lt mu mv mw mx bi translated"><em class="my">随机互换(RS) </em> —随机选择句子中的两个单词，互换它们的位置。这样做n次。</li><li id="0f1a" class="mn mo it la b lb mz le na lh nb ll nc lp nd lt mu mv mw mx bi translated"><em class="my">随机删除(RD) </em> —以概率p随机删除句子中的每个单词。</li></ol><p id="e527" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">作者选择噪音感应比作为句子长度的函数。假设是短文档不太容易受到处理噪声的影响，并且转换可能会导致类的可变性。其中as，长句可以吸收更多的噪音，同时保持原来的类标签。他们在一个句子中调整的字数定义为<strong class="la iu">n =αL；</strong>其中，L为单词级别的句子长度，α为参数，表示句子中需要改变的单词的百分比。他们摆弄α值{0.05，0.1，0.2，0.3，0.4，0.5}。下图显示了同样的情况—</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ne"><img src="../Images/cf913f2eb816b06e35b73207882000b5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Jes4hw033jDLtGhltvWcxQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">不同规模数据集的α-变化。图片来自<a class="ae nf" href="https://arxiv.org/pdf/1901.11196.pdf" rel="noopener ugc nofollow" target="_blank">来源</a></p></figure><p id="f6df" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">从图中可以清楚地看出，平均而言，随着要处理的单词百分比的增加，性能增益通常会下降一定的量，而与要增加的数据量无关。</p><h2 id="e3fb" class="lu lv it bd lw lx ly dn lz ma mb dp mc lh md me mf ll mg mh mi lp mj mk ml mm bi translated">实验和结果</h2><p id="6754" class="pw-post-body-paragraph ky kz it la b lb mp ju ld le mq jx lg lh ng lj lk ll nh ln lo lp ni lr ls lt im bi translated">他们选择了五个基准文本分类任务来评估所提出的系统—</p><ul class=""><li id="f43a" class="mn mo it la b lb lc le lf lh nj ll nk lp nl lt nm mv mw mx bi translated"><em class="my"> SST-2 </em> : <strong class="la iu">斯坦福情感</strong> Treebank v2 <strong class="la iu"> (SST2) </strong>斯坦福<strong class="la iu">数据集</strong>拥有Moview评论和相关情感。<a class="ae nf" href="https://www.kaggle.com/atulanandjha/stanford-sentiment-treebank-v2-sst2" rel="noopener ugc nofollow" target="_blank">参考文件</a></li><li id="bb8d" class="mn mo it la b lb mz le na lh nb ll nc lp nd lt nm mv mw mx bi translated"><em class="my"> CR </em>:关于<strong class="la iu">客户评论</strong>和相关标签的数据集。<a class="ae nf" href="https://dl.acm.org/doi/10.1145/1014052.1014073" rel="noopener ugc nofollow" target="_blank">参考文件</a></li><li id="79c3" class="mn mo it la b lb mz le na lh nb ll nc lp nd lt nm mv mw mx bi translated"><em class="my">subject</em>:<strong class="la iu">主观性/客观性</strong>带有相关情感的数据集。<a class="ae nf" href="https://dl.acm.org/doi/10.3115/1218955.1218990" rel="noopener ugc nofollow" target="_blank">参考论文</a></li><li id="f359" class="mn mo it la b lb mz le na lh nb ll nc lp nd lt nm mv mw mx bi translated"><em class="my"> TREC </em> : <strong class="la iu">问题类型分类</strong>定义了粗粒度和细粒度类别的数据集。<a class="ae nf" href="https://trec.nist.gov/data/qa.html" rel="noopener ugc nofollow" target="_blank">参考文献</a></li><li id="eaea" class="mn mo it la b lb mz le na lh nb ll nc lp nd lt nm mv mw mx bi translated"><em class="my"> PC </em> : <strong class="la iu"> Pro-Con </strong>是来自网络上用户生成内容的<strong class="la iu">情感分析</strong>目标数据集。<a class="ae nf" href="https://dl.acm.org/doi/10.5555/1599081.1599112" rel="noopener ugc nofollow" target="_blank">参考文件</a></li></ul><p id="5deb" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">作者对随机采样的不同大小的训练数据集进行实验，使用<strong class="la iu"> {500、2，000、5，000，所有可用数据}样本</strong>，因为他们假设他们的方法对较小的数据集更有帮助。</p><p id="0c9e" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">他们选择<a class="ae nf" href="https://en.wikipedia.org/wiki/Recurrent_neural_network" rel="noopener ugc nofollow" target="_blank"> <strong class="la iu"> RNNs </strong> </a>和<a class="ae nf" href="https://en.wikipedia.org/wiki/Convolutional_neural_network" rel="noopener ugc nofollow" target="_blank"> <strong class="la iu"> CNNs </strong> </a>作为他们选择有无EDA的分类模型。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/286fc0a5f634394e8136a45f691b09f1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1320/format:webp/1*IP-qkGhyGG37NsRKWGPBdQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">使用和不使用EDA的模型在五个文本分类任务中的平均性能(%)—<a class="ae nf" href="https://arxiv.org/pdf/1901.11196.pdf" rel="noopener ugc nofollow" target="_blank">图片来源</a></p></figure><p id="ff79" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">从上面的结果表中我们可以看出，与处理完整数据集(~0.8%)相比，小数据集(~3.0%)的增益相对较高。这是可能的，因为更少的数据意味着有限的模式和信息，这就是为什么增强在这种情况下会有更好的帮助，而不是当您使用大规模数据集进行训练时，其中已经有足够的模式和信息供模型学习。因此，在那个级别执行增强实际上不会提高性能数字。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi no"><img src="../Images/7d3254a7c81bc436bf74febe949563b1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_CNZzctTLwDUQ56ydS1QBg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">选择用于增强的数据集的百分比及其对准确性的影响。图片来自<a class="ae nf" href="https://arxiv.org/pdf/1901.11196.pdf" rel="noopener ugc nofollow" target="_blank">来源</a></p></figure><p id="097d" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们上面讨论的类似模式也可以在单独的数据集上看到。参考上图。</p><h2 id="711d" class="lu lv it bd lw lx ly dn lz ma mb dp mc lh md me mf ll mg mh mi lp mj mk ml mm bi translated">放大多少？</h2><p id="c896" class="pw-post-body-paragraph ky kz it la b lb mp ju ld le mq jx lg lh ng lj lk ll nh ln lo lp ni lr ls lt im bi translated">下一个重要的问题是，每个句子要生成多少个句子(naug)。如下图所示，作者对不同的增大尺寸值进行了试验。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi np"><img src="../Images/2f9cce2fba258b8efb14b504ad8d304d.png" data-original-src="https://miro.medium.com/v2/resize:fit:914/format:webp/1*Mf9tU4c8-LxM5Nr90xHu0g.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">不同数据大小的naug变量。图片来自<a class="ae nf" href="https://arxiv.org/pdf/1901.11196.pdf" rel="noopener ugc nofollow" target="_blank">来源</a></p></figure><p id="8348" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在较小的训练数据的情况下，更多的增加可能会导致过度拟合，从而大大提高性能。其中，对于较大的训练规模，具有较高的naug值似乎不像预期的那样工作，这仅仅是因为模型已经可以访问大量数据以进行适当的概括。经过大量实验后，他们推荐以下参数—</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/b9c5d63a1912006f92d19def5905eb4d.png" data-original-src="https://miro.medium.com/v2/resize:fit:502/format:webp/1*mvN9nh57YMaq0guYA8SYZQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">EDA的推荐参数。图片来自<a class="ae nf" href="https://arxiv.org/pdf/1901.11196.pdf" rel="noopener ugc nofollow" target="_blank">来源</a></p></figure></div><div class="ab cl nr ns hx nt" role="separator"><span class="nu bw bk nv nw nx"/><span class="nu bw bk nv nw nx"/><span class="nu bw bk nv nw"/></div><div class="im in io ip iq"><p id="6a8e" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我有一个同样的多语种字幕视频漫游，如果你喜欢看视频而不是文字(就像我:D一样)，一定要看看</p></div><div class="ab cl nr ns hx nt" role="separator"><span class="nu bw bk nv nw nx"/><span class="nu bw bk nv nw nx"/><span class="nu bw bk nv nw"/></div><div class="im in io ip iq"><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ny nz l"/></div></figure></div><div class="ab cl nr ns hx nt" role="separator"><span class="nu bw bk nv nw nx"/><span class="nu bw bk nv nw nx"/><span class="nu bw bk nv nw"/></div><div class="im in io ip iq"><p id="1cb2" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">如果你仍然对某件事感到困惑，一定要看报纸。另外，向作者问好，感谢他们的贡献。</p><blockquote class="oa ob oc"><p id="65ed" class="ky kz my la b lb lc ju ld le lf jx lg od li lj lk oe lm ln lo of lq lr ls lt im bi translated"><strong class="la iu"> <em class="it">论文标题:</em> </strong> <em class="it"> EDA:提升文本分类任务性能的简易数据增强技术</em></p><p id="8d89" class="ky kz my la b lb lc ju ld le lf jx lg od li lj lk oe lm ln lo of lq lr ls lt im bi translated"><strong class="la iu"> <em class="it">论文链接:</em></strong><a class="ae nf" href="https://arxiv.org/abs/1901.11196" rel="noopener ugc nofollow" target="_blank">【https://arxiv.org/abs/1901.11196】T21</a></p><p id="485f" class="ky kz my la b lb lc ju ld le lf jx lg od li lj lk oe lm ln lo of lq lr ls lt im bi translated"><strong class="la iu">代码链接</strong> : <a class="ae nf" href="http://github. com/jasonwei20/eda_nlp" rel="noopener ugc nofollow" target="_blank"> <em class="it"> http://github。com/jasonwei 20/EDA _ NLP</em></a></p><p id="fc89" class="ky kz my la b lb lc ju ld le lf jx lg od li lj lk oe lm ln lo of lq lr ls lt im bi translated"><strong class="la iu"> <em class="it">作者:</em> </strong> <a class="ae nf" href="https://arxiv.org/search/cs?searchtype=author&amp;query=Wei%2C+J" rel="noopener ugc nofollow" target="_blank"> <em class="it">魏</em></a><em class="it"/><a class="ae nf" href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zou%2C+K" rel="noopener ugc nofollow" target="_blank"><em class="it">【邹凯】</em></a></p></blockquote><p id="ff38" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">希望这本书值得你花时间去读！与你的朋友分享给对这些内容感兴趣的人。</p><p id="db8b" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><em class="my">谢谢。</em></p></div></div>    
</body>
</html>