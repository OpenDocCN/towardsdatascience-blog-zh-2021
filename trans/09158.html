<html>
<head>
<title>What Can You Do With GNNs</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">你能用GNNs做什么</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/what-can-you-do-with-gnns-5dbec638b525?source=collection_archive---------16-----------------------#2021-08-24">https://towardsdatascience.com/what-can-you-do-with-gnns-5dbec638b525?source=collection_archive---------16-----------------------#2021-08-24</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="2115" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">图形神经网络的操作、效用和优势</h2></div><p id="dc98" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">由于图形数据结构无处不在的特性，图形神经网络(GNN)越来越受欢迎。图表使我们能够在诸如(但不限于)生物学、社会学、生态学、视觉、教育、经济学等领域对许多不同的科学问题进行建模。此外，图形表示使我们能够处理大规模的非结构化数据。</p><p id="999a" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在本文中，我将展示如何在分类、聚类和可视化等任务中使用简单的GNN。我将使用一个GCN(图形卷积网络)运行的例子。这将为你提供巨大的直觉，将意识形态扩展到自己的领域。</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi lb"><img src="../Images/98e835adac456ab74c518b877dbb07b3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YECeOxlko9KoOJNw8RNm3A.jpeg"/></div></div><p class="ln lo gj gh gi lp lq bd b be z dk translated">艾莉娜·格鲁布尼亚克在<a class="ae lr" href="https://unsplash.com/s/photos/network?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><h1 id="e5bc" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">GNN的形式表示</h1><p id="02b8" class="pw-post-body-paragraph kf kg iq kh b ki mk jr kk kl ml ju kn ko mm kq kr ks mn ku kv kw mo ky kz la ij bi translated">任何GNN都可以表示为包含两个数学运算符的层，<strong class="kh ir">聚合函数</strong>和<strong class="kh ir">组合函数</strong>。使用<strong class="kh ir"> MPNN </strong>(消息传递神经网络)框架可以最好地理解这一点。</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi mp"><img src="../Images/c45ad354f3ab1d5279d0c1a571732f24.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hj_tH5CCOhayQQt-YhGu5A.png"/></div></div><p class="ln lo gj gh gi lp lq bd b be z dk translated">图:<strong class="bd mq">作者图</strong></p></figure><h2 id="9bd6" class="mr lt iq bd lu ms mt dn ly mu mv dp mc ko mw mx me ks my mz mg kw na nb mi nc bi translated">聚合</h2><p id="4eb1" class="pw-post-body-paragraph kf kg iq kh b ki mk jr kk kl ml ju kn ko mm kq kr ks mn ku kv kw mo ky kz la ij bi translated">如果我们考虑上面的示例图，聚合器函数专门用于组合邻域信息。更正式地说，聚合可以表示为:</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi nd"><img src="../Images/a665e7bc24fa66fa8b212849f6e8e55e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vz2VdLma2-J7EzgpHUpSTg.png"/></div></div><p class="ln lo gj gh gi lp lq bd b be z dk translated">作者引用的等式(<a class="ae lr" href="https://arxiv.org/pdf/1810.00826.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1810.00826.pdf</a>)</p></figure><p id="9e72" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">简单来说，第<strong class="kh ir"> k </strong>个GNN层的节点<strong class="kh ir"> v </strong>的邻域聚合是用层<strong class="kh ir"> k-1的邻居节点<strong class="kh ir"> u </strong>、<strong class="kh ir"> hᵤ </strong>的激活来表示的。</strong>v的邻居表示为<strong class="kh ir"> N(v) </strong>。在第一层<strong class="kh ir"> k-1=0 </strong>，即回退到节点特性。在第一层中，我们简单地聚集邻居的初始特征。在GCN的情况下，聚合器简单地是度归一化的平均值(每个消息通过<strong class="kh ir"> v </strong>和<strong class="kh ir"> u </strong>的度的乘积的平方根来归一化)。只要操作是顺序不变的(结果不会被混洗改变)，人们可以想到各种聚合器，例如max、mean、min等。</p><h2 id="32a7" class="mr lt iq bd lu ms mt dn ly mu mv dp mc ko mw mx me ks my mz mg kw na nb mi nc bi translated">结合</h2><p id="ba1e" class="pw-post-body-paragraph kf kg iq kh b ki mk jr kk kl ml ju kn ko mm kq kr ks mn ku kv kw mo ky kz la ij bi translated">邻居信息与节点本身的组合在下面的等式中正式表示。</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi ne"><img src="../Images/ccd22d4645634c72b5c8d4fb41c796a9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VBBkjKKPFvX2_HR4iiXqWw.png"/></div></div><p class="ln lo gj gh gi lp lq bd b be z dk translated">方程式由作者引用(<a class="ae lr" href="https://arxiv.org/pdf/1810.00826.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1810.00826.pdf</a>)</p></figure><p id="01ec" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这里可以使用不同的操作，例如连接、求和或元素池操作。不同的GNN架构依赖于不同的功能。GCN使用平均值，我们将在下面讨论。</p><p id="ee6f" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在上面的<strong class="kh ir">图</strong>图中，我们可以通过<code class="fe nf ng nh ni b">X1/(sqrt(7×2))</code>来聚合节点1到6的特征，X1是节点1和7的特征，2分别是节点6和1的度。对于每个节点，我们可以这样做。直观地说，我们可以认为这是每个节点通过平均其出度向其他节点传递消息，然后通过平均其入度接收其他节点的消息。因此得名<strong class="kh ir"> MPNN </strong>。</p><p id="ab0e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">对于邻接矩阵<strong class="kh ir"> A </strong>和度矩阵<strong class="kh ir"> D </strong>具有特征<strong class="kh ir"> X </strong>的图<strong class="kh ir"> G(V，E) </strong>，这可以通过<strong class="kh ir"> D^(-1/2)XAD^(-1/2) </strong>轻松实现。通常情况下，邻接矩阵加上<strong class="kh ir"> <em class="nj"> I </em> </strong>(单位矩阵)来体现节点自身的特征。在这种情况下，<strong class="kh ir"> A </strong>表示为<strong class="kh ir">—</strong>(A帽)并且<strong class="kh ir"> D </strong>被替换为<strong class="kh ir"> D帽</strong>，其中<strong class="kh ir"> D帽</strong>对应于<strong class="kh ir"> A帽</strong>。此时，我们已经在几个矩阵运算中执行了聚合和组合。得到的矩阵被馈送给可训练的可微分函数<strong class="kh ir"><em class="nj">【ɸ】</em></strong>，该函数通常是MLP(多层感知器)，即神经网络。</p><h2 id="983e" class="mr lt iq bd lu ms mt dn ly mu mv dp mc ko mw mx me ks my mz mg kw na nb mi nc bi translated">堆叠层</h2><p id="402a" class="pw-post-body-paragraph kf kg iq kh b ki mk jr kk kl ml ju kn ko mm kq kr ks mn ku kv kw mo ky kz la ij bi translated">我们讨论了在<strong class="kh ir"> GNN层</strong>中会发生什么，现在想象我们堆叠几个这样的层。这意味着我们在邻接矩阵上做更多的乘法。如果你熟悉随机游走，<strong class="kh ir"> D^(-1)A </strong>被称为转移矩阵。其用于幂迭代直到收敛，以找到从给定节点到另一个节点的随机行走概率。直观上，我们添加的GNN层数越多，聚合的跳数就越多。或者换句话说，在一层之后，我们有节点及其邻居的信息。当我们再次这样做时，邻居(他们有他们的邻居)被再次聚集。因此是2跳，依此类推。</p><blockquote class="nk"><p id="043e" class="nl nm iq bd nn no np nq nr ns nt la dk translated">示例时间！</p></blockquote><h1 id="94bd" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw nu jx me jz nv ka mg kc nw kd mi mj bi translated">PyTorch几何框架</h1><p id="709a" class="pw-post-body-paragraph kf kg iq kh b ki mk jr kk kl ml ju kn ko mm kq kr ks mn ku kv kw mo ky kz la ij bi translated">使用pytorch几何库可以很容易地实现gnn。在那里，您可以找到GNNs的许多实现和一个消息传递类，以供您自己的定制实现使用。请点击以下链接查看。</p><div class="nx ny gp gr nz oa"><a href="https://pytorch-geometric.readthedocs.io/en/latest" rel="noopener  ugc nofollow" target="_blank"><div class="ob ab fo"><div class="oc ab od cl cj oe"><h2 class="bd ir gy z fp of fr fs og fu fw ip bi translated">PyTorch几何文档- pytorch_geometric 1.7.2文档</h2><div class="oh l"><h3 class="bd b gy z fp of fr fs og fu fw dk translated">它由对图形和其他不规则结构进行深度学习的各种方法组成，也称为几何深度学习…</h3></div><div class="oi l"><p class="bd b dl z fp of fr fs og fu fw dk translated">py torch-geometric . readthedocs . io</p></div></div></div></a></div><h2 id="51d6" class="mr lt iq bd lu ms mt dn ly mu mv dp mc ko mw mx me ks my mz mg kw na nb mi nc bi translated">Cora数据集</h2><p id="484d" class="pw-post-body-paragraph kf kg iq kh b ki mk jr kk kl ml ju kn ko mm kq kr ks mn ku kv kw mo ky kz la ij bi translated">我们将使用受欢迎的Cora数据集，它由7类科学出版物组成。它通过引用连接，引用代表节点之间的边，这些节点是研究论文。</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi oj"><img src="../Images/29b6dc59bae89322af6bea2a39cfd839.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*l9fPSNCW8wPCi8HkQTwCNA.png"/></div></div><p class="ln lo gj gh gi lp lq bd b be z dk translated">作者图片</p></figure><p id="3b84" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">使用networkx的图形可视化产生了上面的图像。我们可以看到很少的颜色聚集在一起，但我们从任何一种满足感。因此，让我们降低特性的维度，并进行更多的探索。</p><h2 id="73cb" class="mr lt iq bd lu ms mt dn ly mu mv dp mc ko mw mx me ks my mz mg kw na nb mi nc bi translated">UMAP论特征</h2><p id="f5b2" class="pw-post-body-paragraph kf kg iq kh b ki mk jr kk kl ml ju kn ko mm kq kr ks mn ku kv kw mo ky kz la ij bi translated">解释数据的一个简单方法是查看那里有什么以及它们是如何放置的。UMAP是一个非常有用的多元学习工具，它让我们能够做到这一点。让我们想象一下。</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi oj"><img src="../Images/6229fd3779ed2a141735b5c2fb5cc3d6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*C25521rkToKFOWLlVPeeTA.png"/></div></div><p class="ln lo gj gh gi lp lq bd b be z dk translated">作者图片</p></figure><p id="9687" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们可以看到一些类的本地化，但并不完美。上述操作的简化代码如下(完整代码在文末)；</p><pre class="lc ld le lf gt ok ni ol om aw on bi"><span id="80d1" class="mr lt iq ni b gy oo op l oq or"># essential imports that will be needed throughout the blog<br/>import torch<br/>import torch.nn.functional as F<br/>from torch_geometric.datasets import Planetoid<br/>from torch_geometric.nn import GCNConv<br/>import matplotlib.pyplot as plt<br/>import seaborn as sns<br/>import umap<br/>import networkx as nx<br/>import numpy as np</span><span id="c18a" class="mr lt iq ni b gy os op l oq or">dataset = 'Cora'<br/>path = "./"<br/>dataset = Planetoid(path, dataset, transform=T.NormalizeFeatures())<br/>data = dataset[0]</span><span id="9812" class="mr lt iq ni b gy os op l oq or">embd = umap.UMAP().fit_transform(data.x.numpy())<br/>plt.figure(figsize=(10, 10))<br/>sns.scatterplot(x=embd.T[0], y=embd.T[1], hue=data.y.numpy(), palette=palette)<br/>plt.legend(bbox_to_anchor=(1,1), loc='upper left')</span></pre><p id="21d8" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们肯定不满意我们所看到的，所以让我们试试GCN，看看可视化。我的网络如下(由我从<a class="ae lr" href="https://github.com/rusty1s/pytorch_geometric/blob/master/examples/gcn.py" rel="noopener ugc nofollow" target="_blank"> pytorch几何github实例</a>修改而来)；</p><pre class="lc ld le lf gt ok ni ol om aw on bi"><span id="53d8" class="mr lt iq ni b gy oo op l oq or">class Net(torch.nn.Module):<br/>    def __init__(self):<br/>        super(Net, self).__init__()<br/>        self.conv1 = GCNConv(dataset.num_features, 16, cached=True)<br/>        self.conv2 = GCNConv(16, 16, cached=True)<br/>        <br/>        self.fc1 = torch.nn.Linear(16, dataset.num_classes)</span><span id="8af2" class="mr lt iq ni b gy os op l oq or">    def forward(self):<br/>        x, edge_index, edge_weight = data.x, data.edge_index,<br/>                                          data.edge_attr<br/>        x = self.conv1(x, edge_index, edge_weight)<br/>        x = F.relu(x)<br/>        x = F.dropout(x, training=self.training)<br/>        x = self.conv2(x, edge_index, edge_weight)<br/>        x = F.relu(x)<br/>        x = F.dropout(x, training=self.training)<br/>        x = self.fc1(x)<br/>        <br/>        return F.log_softmax(x, dim=1)<br/>  </span></pre><p id="5e43" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们可以使用以下代码对此进行训练:</p><pre class="lc ld le lf gt ok ni ol om aw on bi"><span id="2335" class="mr lt iq ni b gy oo op l oq or">device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')<br/>model, data = Net().to(device), data.to(device)<br/>optimizer = torch.optim.Adam([<br/>    dict(params=model.conv1.parameters(), weight_decay=5e-4),<br/>    dict(params=model.fc1.parameters(), weight_decay=5e-4),<br/>    dict(params=model.conv2.parameters(), weight_decay=0)<br/>], lr=0.01)</span><span id="aca1" class="mr lt iq ni b gy os op l oq or">def train():<br/>    model.train()<br/>    optimizer.zero_grad()<br/>    F.nll_loss(model()[data.train_mask],<br/>                   data.y[data.train_mask]).backward()<br/>    optimizer.step()</span></pre><p id="6643" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">注意，在Conv层2中缺少L2正则化子，这是GCN的作者凭经验决定的(【https://github.com/tkipf/gcn/issues/108】<a class="ae lr" href="https://github.com/tkipf/gcn/issues/108" rel="noopener ugc nofollow" target="_blank">)。</a></p><p id="c6dc" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">可视化后，输出如下所示；</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi oj"><img src="../Images/6e33e7e41e289a3624a9c2e2b96566c5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IvylYUwdbxQAkNyBdtt2ig.png"/></div></div><p class="ln lo gj gh gi lp lq bd b be z dk translated">作者图片</p></figure><p id="6a6e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们可以看到，不同的阶级有着非常明显的区别。这里，训练以<strong class="kh ir"> 0.7800 </strong>的测试精度结束。我们能再多操纵一下吗？让我们看看。</p><h2 id="095e" class="mr lt iq bd lu ms mt dn ly mu mv dp mc ko mw mx me ks my mz mg kw na nb mi nc bi translated">嵌入损失</h2><p id="a920" class="pw-post-body-paragraph kf kg iq kh b ki mk jr kk kl ml ju kn ko mm kq kr ks mn ku kv kw mo ky kz la ij bi translated">神经网络可以看作是连续的可微函数。分类本质上是学习预测的决策边界。点击此处了解更多关于决策界限的信息；</p><div class="nx ny gp gr nz oa"><a rel="noopener follow" target="_blank" href="/logistic-regression-and-decision-boundary-eab6e00c1e8"><div class="ob ab fo"><div class="oc ab od cl cj oe"><h2 class="bd ir gy z fp of fr fs og fu fw ip bi translated">逻辑回归和决策边界</h2><div class="oh l"><h3 class="bd b gy z fp of fr fs og fu fw dk translated">理解逻辑回归及其在分类中的效用</h3></div><div class="oi l"><p class="bd b dl z fp of fr fs og fu fw dk translated">towardsdatascience.com</p></div></div><div class="ot l"><div class="ou l ov ow ox ot oy ll oa"/></div></div></a></div><p id="f7d5" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">总之，如果我们强迫网络有更好的边界，我们可以有更好的可视化。这意味着，我们应该能够分别看到这些类。这在我们可视化集群数据时特别有用。我们能做的一件简单的事情是:</p><ol class=""><li id="414c" class="oz pa iq kh b ki kj kl km ko pb ks pc kw pd la pe pf pg ph bi translated">请GNN更紧密地嵌入类似的类</li><li id="f5a2" class="oz pa iq kh b ki pi kl pj ko pk ks pl kw pm la pe pf pg ph bi translated">请GNN进一步嵌入不同的类</li></ol><p id="7dfb" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">请注意，嵌入是网络的最终层输出或分类输出。在这种情况下，我们可以使用点积作为距离的度量。对于这种损失，我们准备如下的数据点对；</p><pre class="lc ld le lf gt ok ni ol om aw on bi"><span id="6ecd" class="mr lt iq ni b gy oo op l oq or">y_neg_pairs = []<br/>y_pos_pairs = []</span><span id="548e" class="mr lt iq ni b gy os op l oq or">data_idx = np.arange(len(data.x))<br/>for idx1, y1 in enumerate(data.y[data.train_mask].cpu().numpy()):<br/>    for idx2, y2 in enumerate(data.y[data.train_mask].cpu().numpy()):<br/>        if idx1 &gt; idx2 and y1!=y2:<br/>            y_neg_pairs.append([idx1, idx2])<br/>        if idx1 &gt; idx2 and y1==y2:<br/>            y_pos_pairs.append([idx1, idx2])</span><span id="791d" class="mr lt iq ni b gy os op l oq or">y_neg_pairs = np.array(y_neg_pairs)<br/>y_pos_pairs = np.array(y_pos_pairs)</span></pre><p id="c6c7" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们修改的损失函数如下:</p><pre class="lc ld le lf gt ok ni ol om aw on bi"><span id="398a" class="mr lt iq ni b gy oo op l oq or">model_out = model()[data.train_mask]<br/>    y_true = data.y[data.train_mask]<br/>    nllloss = F.nll_loss(model_out, y_true)</span><span id="848a" class="mr lt iq ni b gy os op l oq or">    #Negative loss<br/>    disloss_neg = F.logsigmoid(-1 * (model_out[y_neg_pairs.T[0]]*model_out[y_neg_pairs.T[1]])).sum(-1).mean()<br/>    <br/>    #Positive loss<br/>    disloss_pos = F.logsigmoid((model_out[y_pos_pairs.T[0]]*model_out[y_pos_pairs.T[1]])).sum(-1).mean()<br/>    <br/>    loss = 10 * nllloss - disloss_neg - disloss_pos</span></pre><p id="faac" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">请注意，我们处理点积的极性，并将其传递给logsigmoid，以获得基于点积的损耗。如果你感兴趣，这可以在GraphSAGE paper(<a class="ae lr" href="https://arxiv.org/abs/1706.02216" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1706.02216</a>)下研究。</p><p id="8c8f" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在我们的训练以损失<strong class="kh ir"> 0.7720 </strong>结束，比以前略有下降。让我们想象一下GNN和UMAP的输出。</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi oj"><img src="../Images/eb653ab637d8ba9a44e4df07a47ea768.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CrE_StjMN9cldTL26zZRjA.png"/></div></div><p class="ln lo gj gh gi lp lq bd b be z dk translated">作者图片</p></figure><p id="9945" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们可以看到集群现在更好了，噪音也稍微小了一些。尽管我们的准确度较低，但我们有更好的聚类分离。实际上，较小的测试损失是由于簇的不确定性。我们可以看到一些点自信地位于错误的颜色群中。这本质上是数据的性质决定的。</p><h2 id="6907" class="mr lt iq bd lu ms mt dn ly mu mv dp mc ko mw mx me ks my mz mg kw na nb mi nc bi translated">将该思想扩展到无监督聚类</h2><p id="c97e" class="pw-post-body-paragraph kf kg iq kh b ki mk jr kk kl ml ju kn ko mm kq kr ks mn ku kv kw mo ky kz la ij bi translated">当我们没有标签，只有特征和图形时，我们如何扩展这个想法。这已经在GraphSAGE中讨论过了。简单的想法是使用图拓扑将更近的节点嵌入更近的节点，反之亦然。代替我们的正对和负对，我们可以有直接连接对和随机对分别作为正对和负对。这在各个领域都显示了良好的效果，这是另一天的主题！😊</p><p id="2ae7" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我希望你喜欢这篇文章，我相信这对你的研究也会有用！</p><p id="e038" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">和其他文章一样，这篇文章也附带了<a class="ae lr" href="https://gist.github.com/anuradhawick/bd2eb3f4e5f9c8030f8125d97dc686ac" rel="noopener ugc nofollow" target="_blank">笔记本</a>！</p></div></div>    
</body>
</html>