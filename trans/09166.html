<html>
<head>
<title>Quantifying Model Capacity: The VC Dimension</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">量化模型容量:VC维度</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/quantifying-model-capacity-the-vc-dimension-d4eb76dd26f7?source=collection_archive---------24-----------------------#2021-08-24">https://towardsdatascience.com/quantifying-model-capacity-the-vc-dimension-d4eb76dd26f7?source=collection_archive---------24-----------------------#2021-08-24</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="85f9" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">VC维是一种用数学方法来表示模型容量的方法。它也有实际的机器学习用途。请继续阅读，了解更多信息。</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/64dbf8f67de4f2f57f1d64a0a28d9255.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*hX9xRfpjxBucc_h-"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">Ashkan Forouzani 在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="3c8d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">机器学习中的常识是，一些模型比其他模型具有更大的容量。例如，神经网络能够学习比线性模型多得多的各种功能。直觉上，这是有道理的。然而，拥有更高的容量在数学上意味着什么？模型容量如何影响测试集误差？幸运的是，有一个叫做“VC维度”的重要量回答了这些问题。</p><p id="5281" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我觉得理解VC维度最好的方法就是用例子，那就从一个开始吧。我们在2D的飞机上有两个点。我们有两种颜色，蓝色和红色。我们要问的问题是:我们的两个点是否有一种配置，使得无论我们赋予它们什么颜色，我们总能用一条线将蓝色和红色的点分开？让我们试一试。我们打算把这两点看成是这样:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi lv"><img src="../Images/89f2619776b9def18857d701490df829.png" data-original-src="https://miro.medium.com/v2/resize:fit:1296/format:webp/1*hlS6P4Ml4btvIDFUfz_OJw.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图片作者。</p></figure><p id="e575" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在，对于上述点的2 = 4种可能的颜色分配中的每一种，我们的目标是画一条线，使得所有的蓝色点在一边，所有的红色点在另一边。这很简单，我们可以展示所有4种颜色分配是如何完成的:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi lw"><img src="../Images/c16599db026fa1a73ebc9284e722b1a2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*i-9aeLYSheTiJbfVmqB3vQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图片作者。</p></figure><p id="d540" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">所以我们问题的答案是肯定的——存在两点的排列，因此有一种方法可以为所有可能的颜色分配分离颜色。在统计理论中，我们称之为<strong class="lb iu">粉碎</strong>。所以我们可以说<strong class="lb iu">两点可以用一条线</strong>击碎。</p><p id="4fbc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在我们问同样的问题，只是带着三点。三分可以用一条线击碎吗？让我们用这三个点的排列来试试:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi lx"><img src="../Images/5880353db488a1d393dadafa053e64ec.png" data-original-src="https://miro.medium.com/v2/resize:fit:1340/format:webp/1*dFn6MKKTyuFm-uNXTgiUsw.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图片作者。</p></figure><p id="5145" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这种排列有2 = 8种可能的颜色。我们研究了所有8种安排，我们发现确实有三点可以打破:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ly"><img src="../Images/ae394384c4c86d9e447b146d43875792.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*J5zcNYhAGI-Mv-xUjk63bg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图片作者。</p></figure><p id="1db5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在这一点上，我们应该澄清一些事情。看看这三个点的排列:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi lz"><img src="../Images/2e10f568c3332dd570a4afa0faaeb3c8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1332/format:webp/1*xIj5O9oZ2UIwcG1Mxx83zg.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图片作者。</p></figure><p id="2b89" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这种排列有一种颜色，因此无法用一条线将颜色分开:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ma"><img src="../Images/276ef9c0f4fd8ccc85a160ab4aa153a9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1320/format:webp/1*Xyrudog3VJX0MdEZ-1l7Xg.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图片作者。</p></figure><p id="9915" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">那么为什么我们说3分可以粉碎呢？这不是反例吗？重点是<strong class="lb iu">只要有一个</strong>排列的3点可以粉碎，我们就说3点可以粉碎。不是所有的3点排列都要求是可破坏的。</p><p id="bd9d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">最后，我们来考虑4点。在尝试了4个点的不同排列后，你会注意到，无论如何排列，这4个点总会有一种颜色，不能用一条线来区分。其中一种颜色是这样的:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mb"><img src="../Images/aee610e85a3c9537233192c6756aa9bf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1316/format:webp/1*HoCu1ldFQ_l-eRPcbQYZ8Q.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图片作者。</p></figure><p id="3918" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">换句话说，无论你怎么努力，都没有4点的排列是不可打破的。所以4分不能粉碎。</p><p id="3508" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在，我们已经介绍了粉碎的概念，我们可以谈谈风险资本的层面。简单来说，一组函数的<strong class="lb iu"> VC维就是可以粉碎的最高点数</strong>。在我们上面的例子中，因为一条线可以粉碎3个点，但不能粉碎4个点或更高，所以线的VC维是3。作为另一个例子，考虑所有常数分类器的集合。任何常量分类器甚至不能粉碎1个点，因为我们可以将该点的颜色设置为与分类器定义的常量颜色不同。因此，这个集合的VC维是0。另一个例子是所有圆的集合。我们可以对这个集合进行非常类似的分析，就像我们对所有线的集合所做的那样，以显示圆的VC维度是3(这是一个很好的练习，试试吧！).</p><p id="a7aa" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对于更复杂的分类器，我们不能仅仅通过画一些图，试图分离彩色点来计算VC维数。然而，根据经验，VC维度与模型的参数数量相关。例如，神经网络的VC维数与网络中的节点数和边数相关。更准确地说，已经表明，对于使用sigmoid激活函数的神经网络，VC维数最多为O(E * V)，其中E和V是网络中边和节点的数量。</p></div><div class="ab cl mc md hx me" role="separator"><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh"/></div><div class="im in io ip iq"><p id="8bba" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在我们已经看到了一些VC维的具体例子，我们能在高层次上解释一下什么是VC维吗？简单来说，VC维度衡量的是一个模型的容量。“容量”是指可以学习的不同分类的数量，它与可以分解的点的数量以及模型的有用性直接相关。想一想行量词和常量量词的区别。常数分类器甚至不能粉碎1个点(VCdim = 0)，它们只能预测所有数据点的是/否。不是很有用，而且容量低。相比之下，线可以粉碎3点(VCdim = 3)，使他们能够学习非平凡的模式。这更有用，容量也更高。然后我们有神经网络，其具有比线和常数分类器高得多的VC维数，因此具有高容量和有用性。这就是神经网络能学会下棋和开车，而线性模型和常数分类器不能的原因。</p><p id="e683" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们已经看到了如何计算VC维以及直观地计算它与模型容量的关系，但是它可以直接使用吗？事实证明，VC维将训练误差与测试误差联系起来。最著名的结果是这个不等式:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mj"><img src="../Images/1157e573b96c7c75aebe72433282f96b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TwxSnkzDIrs7Ec1Wqw4z8w.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">我们称之为“VC绑定”。图片作者。</p></figure><p id="200a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">n是训练集的大小，D是VC维数，1 — η是任意概率。这个不等式表明，在高概率下(将1-η设置为接近1)，测试误差与训练误差相差一个常数，该常数取决于VC维数和样本大小。这是一个极其有力的声明。机器学习的一个基本问题是，你不知道你在训练集的良好表现是否会转化为测试集。这个不等式提供了解决这个问题的方法。如果你知道你的模型的VC维，你可以使用上面的不等式来保证你的测试误差不会超过一定的水平。</p><p id="c8e8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">当然，一旦你代入D、N和η的值，你得到的界限可能太大而没有用。如果你的训练误差是10%的错误分类，而界限说你的测试误差会比50%的错误分类好，那并不能告诉你太多。这就是为什么我们在实践中很少听到VC维界限，因为通常测试误差超过VC界限。尽管如此，在你的脑海中保留这些类型的界限仍然是好的——因为它们仍然可能是有用的，并且因为在未来可能会找到更好的界限。</p><p id="abe6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">VC dimension的另一个用途是它从数学上证明了模型容量和性能之间的权衡。我们已经讨论过低模型容量如何阻止我们学习有趣的东西。然而，高模型容量(VC界中的高D)也不都是好的。它导致平方根下的项增加，从而导致测试误差中更多的方差。这演示了<strong class="lb iu">过度拟合</strong>是如何发生的——模型的容量不必要的高。在我们还想用神经网络这样的高容量模型的情况下，该怎么做？VC界也回答了这个问题:增加样本量n</p></div><div class="ab cl mc md hx me" role="separator"><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh"/></div><div class="im in io ip iq"><p id="4b21" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在本文中，我们讨论了粉碎和VC维的概念。我们看了几个简单的例子，通过一些分析可以找到VC维。然后，我们探讨了VC维度背后的直觉以及如何使用它，特别关注VC界限的后果。请留下任何问题/评论，感谢阅读！</p></div></div>    
</body>
</html>