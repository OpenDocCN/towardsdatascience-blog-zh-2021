<html>
<head>
<title>Text Exploration with Python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用Python进行文本探索</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/text-exploration-with-python-cb8ea710e07c?source=collection_archive---------13-----------------------#2021-06-11">https://towardsdatascience.com/text-exploration-with-python-cb8ea710e07c?source=collection_archive---------13-----------------------#2021-06-11</a></blockquote><div><div class="fc ii ij ik il im"/><div class="in io ip iq ir"><h2 id="9685" class="is it iu bd b dl iv iw ix iy iz ja dk jb translated" aria-label="kicker paragraph">文本探索循序渐进指南</h2><div class=""/><div class=""><h2 id="599b" class="pw-subtitle-paragraph ka jd iu bd b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr dk translated">用n-grams单词云探索隐藏在文本中的有趣见解</h2></div><figure class="kt ku kv kw gu kx gi gj paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="gi gj ks"><img src="../Images/4d34ea63a3829d390135e752835ff8c1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QvJIezGMRvFi5UK8PWiQOA.jpeg"/></div></div><p class="le lf gk gi gj lg lh bd b be z dk translated">詹妮弗·格里芬在<a class="ae li" href="https://unsplash.com/s/photos/interesting?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="7ce9" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">在文本分析中，文本探索一直是我最喜欢的过程。当我发现有趣的东西时，我总是很兴奋。我在学习期间做过两个文本分析项目，都是用主题建模来研究长文本中讨论的主题。</p><p id="32cf" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">在我做分析之前，我从来没有读过这篇文章，我也不应该这样做，因为这篇文章非常长，阅读它们来理解主题是不合理的。因此，我进行了文本探索，这给了我对文本内容的粗略想法，并让我知道我应该从主题建模模型中期待什么。</p><p id="6f7e" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">下面是我如何使用Word Cloud来探索我的一个学校项目中的文本的例子，你可以跳到下一部分的脚本和解释。</p><p id="53ff" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">如果你是一个视频人，这里的视频适合你。</p><figure class="kt ku kv kw gu kx"><div class="bz fq l di"><div class="mf mg l"/></div></figure></div><div class="ab cl mh mi hy mj" role="separator"><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm"/></div><div class="in io ip iq ir"><h1 id="1006" class="mo mp iu bd mq mr ms mt mu mv mw mx my kj mz kk na km nb kn nc kp nd kq ne nf bi translated">我的学校项目中的文本探索</h1><p id="b5f9" class="pw-post-body-paragraph lj lk iu ll b lm ng ke lo lp nh kh lr ls ni lu lv lw nj ly lz ma nk mc md me in bi translated">通过使用单词云来探索文本是了解<strong class="ll je">在文本</strong>中被频繁讨论的内容的一种完美而有趣的方式。例如，Kaggle的约会应用数据集包含用户对以下9个问题的回答[2]:</p><ol class=""><li id="fef2" class="nl nm iu ll b lm ln lp lq ls nn lw no ma np me nq nr ns nt bi translated">关于我/自我总结</li><li id="5862" class="nl nm iu ll b lm nu lp nv ls nw lw nx ma ny me nq nr ns nt bi translated">当前的目标/愿望</li><li id="c403" class="nl nm iu ll b lm nu lp nv ls nw lw nx ma ny me nq nr ns nt bi translated">我的黄金法则/我的特点</li><li id="6179" class="nl nm iu ll b lm nu lp nv ls nw lw nx ma ny me nq nr ns nt bi translated">我可能会在天赋上打败你</li><li id="c2fd" class="nl nm iu ll b lm nu lp nv ls nw lw nx ma ny me nq nr ns nt bi translated">我热衷的最后一场演出/爱好</li><li id="acbb" class="nl nm iu ll b lm nu lp nv ls nw lw nx ma ny me nq nr ns nt bi translated">完美的一天/时刻</li><li id="c78e" class="nl nm iu ll b lm nu lp nv ls nw lw nx ma ny me nq nr ns nt bi translated">一.价值/需求</li><li id="576d" class="nl nm iu ll b lm nu lp nv ls nw lw nx ma ny me nq nr ns nt bi translated">我愿意承认的最隐私的事情/秘密</li><li id="a4b9" class="nl nm iu ll b lm nu lp nv ls nw lw nx ma ny me nq nr ns nt bi translated">我在寻找/约会什么</li></ol><p id="e51b" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">问题以不同的顺序显示给每个用户，所以我们不能只提取某些问题的答案。</p><p id="6c1c" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">以下是从他们的回答中生成的词云。</p><figure class="kt ku kv kw gu kx gi gj paragraph-image"><div class="gi gj nz"><img src="../Images/bb71a27fd29797f43cd62213bee9a8f8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1080/format:webp/1*hsLMwunbgAeztrGeQ8SdWQ.png"/></div><p class="le lf gk gi gj lg lh bd b be z dk translated">从文本生成的词云。图片作者。</p></figure><p id="304a" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">上面的字云是清理后的文本<em class="oa">生成的字云(参考本文中处理和清理文本的步骤，</em><a class="ae li" rel="noopener" target="_blank" href="/text-processing-in-python-29e86ea4114c"><em class="oa">Python中的文本处理</em> </a> <em class="oa">)。</em>从云这个词中映入我眼帘的是“阳光一尘不染的心灵”和“永恒的阳光一尘不染”。这两个三元组似乎是一个短语的一部分，我以前从未见过这些术语，所以我想谷歌一下，看看它们是否有什么意思。</p><figure class="kt ku kv kw gu kx gi gj paragraph-image"><div class="gi gj ob"><img src="../Images/a36ab1859d37a4d8050ccb2a65ea82e2.png" data-original-src="https://miro.medium.com/v2/resize:fit:914/format:webp/1*E-GTEkcLCY78_lEB0PJUtg.png"/></div><p class="le lf gk gi gj lg lh bd b be z dk translated">纯洁心灵的永恒阳光。</p></figure><p id="d577" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">原来是一部名为《美丽心灵的永恒阳光》的剧。这大概就是问题5“我狂吃的最后一个节目/爱好”的答案。另一部被频繁提及的剧大概是《生活大爆炸》，在单词Cloud中显示为“生活大爆炸”。</p><blockquote class="oc"><p id="d595" class="od oe iu bd of og oh oi oj ok ol me dk translated">从这里我们能看到什么？</p></blockquote><blockquote class="on oo op"><p id="2a84" class="lj lk oa ll b lm oq ke lo lp or kh lr os ot lu lv ou ov ly lz ow ox mc md me in bi translated">《美丽心灵的永恒阳光》和《生活大爆炸》是当时最受欢迎的两部电视剧。</p></blockquote><p id="9bae" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">这种洞察力有意义吗？在进行进一步分析之前，我们现在还不能断定，但我们可以从这一陈述中作出假设。例如，当用户分享这些信息时，他们倾向于接受与他们观看同一系列的比赛。</p><p id="7965" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">我希望看到用户对他们的比赛有更多的期待。因此，我提取了包含单词“You”的每一行句子(您可以在本文中看到我是如何提取句子的，<a class="ae li" rel="noopener" target="_blank" href="/text-extraction-using-regular-expression-python-186369add656">使用正则表达式提取文本(Python) </a>)。</p><figure class="kt ku kv kw gu kx gi gj paragraph-image"><div class="gi gj nz"><img src="../Images/d288fe65c228b5c99a0e6379742c76ce.png" data-original-src="https://miro.medium.com/v2/resize:fit:1080/format:webp/1*KvSe0wSvUlM-nTrZmEoRHg.png"/></div><p class="le lf gk gi gj lg lh bd b be z dk translated">从提取的文本生成的词云。作者创建的图像。</p></figure><p id="4910" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">现在我们可以看到什么与“你”有关。或许我们可以忽略YouTube.com？无论如何，从上面的单词云，我们可以做出一个<strong class="ll je">假设</strong>，用户期待着结识有很强幽默感的新朋友，并希望建立长期关系。同样，来自词云的信息可能不完整，因为它只显示了频繁出现的短语，我们可以通过主题建模或其他自然语言技术来进一步证明这一点。</p><blockquote class="oc"><p id="21c9" class="od oe iu bd of og oh oi oj ok ol me dk translated">有意思？</p></blockquote></div><div class="ab cl mh mi hy mj" role="separator"><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm"/></div><div class="in io ip iq ir"><blockquote class="oc"><p id="04f0" class="od oe iu bd of og oh oi oj ok ol me dk translated">这是我过去项目的全部内容。现在让我们开始使用Python进行文本探索。</p></blockquote></div><div class="ab cl mh mi hy mj" role="separator"><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm"/></div><div class="in io ip iq ir"><h1 id="f5c9" class="mo mp iu bd mq mr ms mt mu mv mw mx my kj mz kk na km nb kn nc kp nd kq ne nf bi translated">数据集简介</h1><p id="3a6e" class="pw-post-body-paragraph lj lk iu ll b lm ng ke lo lp nh kh lr ls ni lu lv lw nj ly lz ma nk mc md me in bi translated">我最近在Kaggle上发现了一个有趣的数据集，觉得探索一下会很有趣。</p><p id="0329" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">我找到的文本数据是来自Kaggle的<a class="ae li" href="https://www.kaggle.com/hsankesara/medium-articles" rel="noopener ugc nofollow" target="_blank"> Medium Articles </a>数据集，其中包含337篇与机器学习、人工智能和数据科学相关的文章的作者、掌声、阅读时间、链接、标题和文本。</p><p id="3859" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">在下面的文本探索中，我将只使用文章的标题，来研究作者中有哪些流行的话题。</p><h1 id="0cfb" class="mo mp iu bd mq mr oy mt mu mv oz mx my kj pa kk na km pb kn nc kp pc kq ne nf bi translated">使用Python进行文本探索</h1><h2 id="a882" class="pd mp iu bd mq pe pf dn mu pg ph dp my ls pi pj na lw pk pl nc ma pm pn ne ja bi translated"><strong class="ak"> 1。导入库</strong></h2><pre class="kt ku kv kw gu po pp pq pr aw ps bi"><span id="32bf" class="pd mp iu pp b gz pt pu l pv pw">import re<br/>import pandas as pd</span><span id="9d0e" class="pd mp iu pp b gz px pu l pv pw"># text processing<br/>import nltk<br/>from nltk.tokenize import WordPunctTokenizer<br/>nltk.download('stopwords')<br/>from nltk.corpus import stopwords<br/>## needed for nltk.pos_tag function <br/># nltk.download('averaged_perceptron_tagger')<br/>nltk.download('wordnet')<br/>from nltk.stem import WordNetLemmatizer</span><span id="0463" class="pd mp iu pp b gz px pu l pv pw"># visualization<br/>import numpy as np<br/>import matplotlib.pyplot as plt<br/>%matplotlib inline<br/>import seaborn as sns<br/>from wordcloud import WordCloud</span></pre><h2 id="84c1" class="pd mp iu bd mq pe pf dn mu pg ph dp my ls pi pj na lw pk pl nc ma pm pn ne ja bi translated"><strong class="ak"> 2。导入数据</strong></h2><pre class="kt ku kv kw gu po pp pq pr aw ps bi"><span id="56c2" class="pd mp iu pp b gz pt pu l pv pw">df = pd.read_csv("articles.csv")<br/>print(df.shape)<br/>print(df.columns)<br/>df.head()</span></pre><figure class="kt ku kv kw gu kx gi gj paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="gi gj py"><img src="../Images/cd4b4594e199c77633bda9046032d501.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iEd9jo7FpZWWsIfWf9iqOw.png"/></div></div><p class="le lf gk gi gj lg lh bd b be z dk translated">数据框。图片作者。</p></figure><h2 id="e7b5" class="pd mp iu bd mq pe pf dn mu pg ph dp my ls pi pj na lw pk pl nc ma pm pn ne ja bi translated"><strong class="ak"> 3。文本处理</strong></h2><p id="1a53" class="pw-post-body-paragraph lj lk iu ll b lm ng ke lo lp nh kh lr ls ni lu lv lw nj ly lz ma nk mc md me in bi translated">这里的过程类似于我上一篇文章<a class="ae li" rel="noopener" target="_blank" href="/text-processing-in-python-29e86ea4114c">Python中的文本处理</a>中的过程。因此，我将在这里粘贴脚本，但跳过重复部分的解释，以避免冗余。</p><p id="f2a4" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated"><strong class="ll je"> a .标记化</strong></p><p id="d359" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">将标题拆分成一个令牌列表。</p><pre class="kt ku kv kw gu po pp pq pr aw ps bi"><span id="356d" class="pd mp iu pp b gz pt pu l pv pw"># change DataFrame columns into a list<br/>title = df['title'].values</span><span id="5ff1" class="pd mp iu pp b gz px pu l pv pw"># tokenize<br/>title_text = ""<br/>title_all = []<br/>for _ in title:<br/>    title_text += (_ + " ")<br/>    title_all.append(_)<br/>    <br/>word_punct_token = WordPunctTokenizer().tokenize(title_text)</span></pre><p id="711d" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">标题中有4099个标记。</p><p id="2b45" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated"><strong class="ll je"> b .正常化</strong></p><p id="06cd" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">移除不需要的令牌。</p><pre class="kt ku kv kw gu po pp pq pr aw ps bi"><span id="5753" class="pd mp iu pp b gz pt pu l pv pw">clean_token=[]<br/>for token in word_punct_token:<br/>    new_token = re.sub(r'[^a-zA-Z]+', '', token) # remove any value that are not alphabetical<br/>    if new_token != "" and len(new_token) &gt;= 2: # remove empty value and single character value<br/>        vowels=len([v for v in new_token if v in "aeiou"])<br/>        if vowels != 0: # remove line that only contains consonants<br/>            new_token = new_token.lower() # change to lower case<br/>            clean_token.append(new_token)</span><span id="b4cd" class="pd mp iu pp b gz px pu l pv pw"># Get the list of stop words<br/>stop_words = stopwords.words('english')<br/>stop_words.extend(["could","though","would","also","us"])</span><span id="8663" class="pd mp iu pp b gz px pu l pv pw"># Remove the stopwords from the list of tokens<br/>tokens = [x for x in clean_token if x not in stop_words]</span></pre><p id="c18e" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">在我们移除非字母值、单字符标记、只包含辅音和没有太多洞察力的停用词的标记后，还剩2214个标记。我们移除了将近一半的代币。</p><p id="a56d" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">本例中使用的数据集很小，因此删除这些标记不会显著提高模型的速度，但当我们分析一个巨大的数据集时，这将是至关重要的。</p><p id="059a" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated"><strong class="ll je"> c .位置标签和术语化</strong></p><p id="fadb" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">标出单词的词性，并相应地将单词还原成其基本形式。</p><pre class="kt ku kv kw gu po pp pq pr aw ps bi"><span id="f707" class="pd mp iu pp b gz pt pu l pv pw"># POS Tag every token and save into dataframe<br/>data_tagset = nltk.pos_tag(tokens)<br/>df_tagset = pd.DataFrame(data_tagset, columns=['Word', 'Tag'])</span><span id="a7eb" class="pd mp iu pp b gz px pu l pv pw"># to focus on nouns, adjective and verb<br/>tagset_allowed = ['NN','NNS','NNP','NNPS','JJ','JJR','JJS','VB','VBD','VBG','VBN','VBP','VBZ']<br/>new_tagset = df_tagset.loc[df_tagset['Tag'].isin(tagset_allowed)]<br/>text = [str(x) for x in new_tagset['Word']]<br/>tag =[x for x in new_tagset['Tag'] if x != '']</span></pre><p id="21f8" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">词类标签有30多种，但有意义的标签大多属于名词、形容词和动词的范畴。因此，我们可以从模型中过滤掉其他标签。</p><pre class="kt ku kv kw gu po pp pq pr aw ps bi"><span id="2688" class="pd mp iu pp b gz pt pu l pv pw"># Create lemmatizer object <br/>lemmatizer = WordNetLemmatizer()# Lemmatize each word and display the output<br/>lemmatize_text = []<br/>for word in text:<br/>    output = [word, lemmatizer.lemmatize(word, pos='n'),lemmatizer.lemmatize(word, pos='a'),lemmatizer.lemmatize(word, pos='v')]<br/>    lemmatize_text.append(output)# create DataFrame using original words and their lemma words<br/>df = pd.DataFrame(lemmatize_text, columns =['Word', 'Lemmatized Noun', 'Lemmatized Adjective', 'Lemmatized Verb'])</span><span id="ac90" class="pd mp iu pp b gz px pu l pv pw">df['Tag'] = tag</span></pre><figure class="kt ku kv kw gu kx gi gj paragraph-image"><div class="gi gj pz"><img src="../Images/c925fa667bd827618987804d20217c0e.png" data-original-src="https://miro.medium.com/v2/resize:fit:986/format:webp/1*5ZmWg0mC9ribGLiKnah8aA.png"/></div><p class="le lf gk gi gj lg lh bd b be z dk translated">词汇化后的数据框架。图片作者。</p></figure><p id="6d57" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">上面的脚本创建了三个列，存储了词汇化的名词、词汇化的形容词和词汇化的动词。当单词的标签是名词时，单词的基本形式将反映在词汇化的名词列中，形容词基本形式反映在词汇化的形容词列中，动词基本形式反映在词汇化的动词列中。</p><p id="399a" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">在这个阶段，每一类词类都被进一步划分为子类。根据[1]，名词进一步分为</p><ol class=""><li id="5f2d" class="nl nm iu ll b lm ln lp lq ls nn lw no ma np me nq nr ns nt bi translated">单数或复数名词(NN)，</li><li id="af93" class="nl nm iu ll b lm nu lp nv ls nw lw nx ma ny me nq nr ns nt bi translated">单数专有名词(NNP)，</li><li id="fc92" class="nl nm iu ll b lm nu lp nv ls nw lw nx ma ny me nq nr ns nt bi translated">复数专有名词(NNPS)，以及</li><li id="584d" class="nl nm iu ll b lm nu lp nv ls nw lw nx ma ny me nq nr ns nt bi translated">复数名词(NNS)。</li></ol><p id="fd31" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">形容词和动词也被进一步分成小类。当我们稍后想要根据组选择令牌时，这可能会增加一些工作量。因此，子类别将被其主类别所取代。</p><pre class="kt ku kv kw gu po pp pq pr aw ps bi"><span id="9d3c" class="pd mp iu pp b gz pt pu l pv pw"># replace with single character for simplifying<br/>df = df.replace(['NN','NNS','NNP','NNPS'],'n')<br/>df = df.replace(['JJ','JJR','JJS'],'a')<br/>df = df.replace(['VBG','VBP','VB','VBD','VBN','VBZ'],'v')</span></pre><p id="5988" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">然后，将使用下面的脚本创建一个新列“Lemmatized Word ”,其中包含单词的基本形式。</p><pre class="kt ku kv kw gu po pp pq pr aw ps bi"><span id="2b07" class="pd mp iu pp b gz pt pu l pv pw">'''<br/>define a function where take the lemmatized word when tagset is a noun, and take lemmatized adjectives when tagset is adjective<br/>'''<br/>df_lemmatized = df.copy()<br/>df_lemmatized['Tempt Lemmatized Word']=df_lemmatized['Lemmatized Noun'] + ' | ' + df_lemmatized['Lemmatized Adjective']+ ' | ' + df_lemmatized['Lemmatized Verb']</span><span id="21a3" class="pd mp iu pp b gz px pu l pv pw">lemma_word = df_lemmatized['Tempt Lemmatized Word']<br/>tag = df_lemmatized['Tag']<br/>i = 0<br/>new_word = []<br/>while i&lt;len(tag):<br/>    words = lemma_word[i].split('|')<br/>    if tag[i] == 'n':        <br/>        word = words[0]<br/>    elif tag[i] == 'a':<br/>        word = words[1]<br/>    elif tag[i] == 'v':<br/>        word = words[2]<br/>    new_word.append(word)<br/>    i += 1<br/>    <br/>df_lemmatized['Lemmatized Word']=new_word<br/>df_lemmatized.head()</span></pre><figure class="kt ku kv kw gu kx gi gj paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="gi gj qa"><img src="../Images/af084cd4b84738d7227044854e4c2a94.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*g7nKfDD90nzvqhX8T02JLA.png"/></div></div><p class="le lf gk gi gj lg lh bd b be z dk translated">在数据帧中创建的词汇。图片作者。</p></figure><p id="5ffe" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">文本处理的最后一步是将词条化的单词列转换成列表，供下一步处理。</p><pre class="kt ku kv kw gu po pp pq pr aw ps bi"><span id="63ac" class="pd mp iu pp b gz pt pu l pv pw">lemma_word = [str(x) for x in df_lemmatized['Lemmatized Word']]</span></pre><p id="dc6d" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">现在，我们准备创建单词云来探索文本！</p></div><div class="ab cl mh mi hy mj" role="separator"><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm"/></div><div class="in io ip iq ir"><h2 id="58d6" class="pd mp iu bd mq pe pf dn mu pg ph dp my ls pi pj na lw pk pl nc ma pm pn ne ja bi translated">4.文本探索</h2><p id="4b9b" class="pw-post-body-paragraph lj lk iu ll b lm ng ke lo lp nh kh lr ls ni lu lv lw nj ly lz ma nk mc md me in bi translated">通常情况下，我会按照递增的顺序用n-grams创建单词云。因此，我们将从一元模型开始，然后是二元模型和三元模型。</p><h2 id="87ec" class="pd mp iu bd mq pe pf dn mu pg ph dp my ls pi pj na lw pk pl nc ma pm pn ne ja bi translated">Unigram</h2><p id="4095" class="pw-post-body-paragraph lj lk iu ll b lm ng ke lo lp nh kh lr ls ni lu lv lw nj ly lz ma nk mc md me in bi translated">名词、形容词和动词都是有意义的，因此将为每个标签类别创建一个词云。</p><h2 id="1dbb" class="pd mp iu bd mq pe pf dn mu pg ph dp my ls pi pj na lw pk pl nc ma pm pn ne ja bi translated">a.名词</h2><pre class="kt ku kv kw gu po pp pq pr aw ps bi"><span id="bcd8" class="pd mp iu pp b gz pt pu l pv pw"># select only noun for word cloud<br/>tagset = df_lemmatized<br/>tagset_allowed = ['n']<br/>new_tagset = tagset.loc[tagset['Tag'].isin(tagset_allowed)]<br/>text = ' '.join(str(x) for x in new_tagset['Lemmatized Noun'])<br/>wordcloud = WordCloud(width = 1600, height = 800, max_words = 200, background_color = 'white').generate(text)<br/>plt.imshow(wordcloud, interpolation = 'bilinear')<br/>plt.axis("off")<br/>#plt.savefig('Vis/Noun_WordCloud.png') # if you want to save the WordCloud<br/>plt.show()</span></pre><figure class="kt ku kv kw gu kx gi gj paragraph-image"><div class="gi gj qb"><img src="../Images/5f330823a31c5548df3dd42a88a45538.png" data-original-src="https://miro.medium.com/v2/resize:fit:692/format:webp/1*CdNNI-jtHxprdNHRG7RVxQ.png"/></div><p class="le lf gk gi gj lg lh bd b be z dk translated">名词的词云。图片作者。</p></figure><p id="5a90" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">基于名词的词云，题目中使用频率较高的名词有<em class="oa">媒介、机器、网络、学习、智能、</em>和<em class="oa">数据科学</em>。</p><h2 id="85cb" class="pd mp iu bd mq pe pf dn mu pg ph dp my ls pi pj na lw pk pl nc ma pm pn ne ja bi translated">b.形容词</h2><pre class="kt ku kv kw gu po pp pq pr aw ps bi"><span id="a184" class="pd mp iu pp b gz pt pu l pv pw"># select only adjectives for word cloud<br/>tagset = df_lemmatized<br/>tagset_allowed = ['a']<br/>new_tagset = tagset.loc[tagset['Tag'].isin(tagset_allowed)]<br/>text = ' '.join(str(x) for x in new_tagset['Lemmatized Adjective'])<br/>wordcloud = WordCloud(width = 1600, height = 800, max_words = 200, background_color = 'white').generate(text)<br/>plt.imshow(wordcloud, interpolation = 'bilinear')<br/>plt.axis("off")<br/>#plt.savefig('Vis/Adjectives.png')<br/>plt.show()</span></pre><figure class="kt ku kv kw gu kx gi gj paragraph-image"><div class="gi gj qc"><img src="../Images/6b81e663df802e36147797c184988f79.png" data-original-src="https://miro.medium.com/v2/resize:fit:712/format:webp/1*oacgSpBQUD1h7iEZB2JWnQ.png"/></div><p class="le lf gk gi gj lg lh bd b be z dk translated">形容词的单词云。图片作者。</p></figure><p id="3294" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">题目中常用的形容词有<em class="oa">神经、深沉、人工、大、新</em>和<em class="oa">好玩</em>。</p><h2 id="25d9" class="pd mp iu bd mq pe pf dn mu pg ph dp my ls pi pj na lw pk pl nc ma pm pn ne ja bi translated">c.动词</h2><pre class="kt ku kv kw gu po pp pq pr aw ps bi"><span id="2f8d" class="pd mp iu pp b gz pt pu l pv pw"># select only verbs for word cloud<br/>tagset = df_lemmatized<br/>tagset_allowed = ['v']<br/>new_tagset = tagset.loc[tagset['Tag'].isin(tagset_allowed)]<br/>text = ' '.join(str(x) for x in new_tagset['Lemmatized Verb'])<br/>wordcloud = WordCloud(width = 1600, height = 800, max_words = 200, background_color = 'white').generate(text)<br/>plt.imshow(wordcloud, interpolation = 'bilinear')<br/>plt.axis("off")<br/>#plt.savefig('Vis/Adjectives_BeforeStemming.png')<br/>plt.show()</span></pre><figure class="kt ku kv kw gu kx gi gj paragraph-image"><div class="gi gj qd"><img src="../Images/5653abcca2cfe6b8e92755d66a1effe8.png" data-original-src="https://miro.medium.com/v2/resize:fit:710/format:webp/1*4oNEaM7qtMwWEJUPRAoD9g.png"/></div><p class="le lf gk gi gj lg lh bd b be z dk translated">动词的单词云。图片作者。</p></figure><p id="78bf" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">题目中经常出现的动词有<em class="oa">学习(learn)、使用、深入、理解、发生</em>和<em class="oa">面试</em>。在这个阶段，结合名词、形容词、动词的词云，大致可以看到经常讨论的话题。主题是神经网络、数据科学、机器学习、深度学习和人工智能。</p><p id="e0e8" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">然后，基于动词的词云，我们可以确定文章的目标。举个例子，</p><ol class=""><li id="b480" class="nl nm iu ll b lm ln lp lq ls nn lw no ma np me nq nr ns nt bi translated">使用、学习、解释、构建——包含这些关键词的文章是使用软件包、工具或算法的教程</li><li id="056c" class="nl nm iu ll b lm nu lp nv ls nw lw nx ma ny me nq nr ns nt bi translated">面试——带有这个关键词的文章是对面试的建议或指导</li></ol><h2 id="1c78" class="pd mp iu bd mq pe pf dn mu pg ph dp my ls pi pj na lw pk pl nc ma pm pn ne ja bi translated">二元模型</h2><p id="c37e" class="pw-post-body-paragraph lj lk iu ll b lm ng ke lo lp nh kh lr ls ni lu lv lw nj ly lz ma nk mc md me in bi translated">对于二元和三元词云，我们将需要使用<strong class="ll je">计数向量器</strong>来计算频率。</p><pre class="kt ku kv kw gu po pp pq pr aw ps bi"><span id="2e11" class="pd mp iu pp b gz pt pu l pv pw">#Using count vectoriser to view the frequency of bigrams<br/>tagset_allowed = ['a','n','v']<br/>new_tagset = df_lemmatized.loc[df_lemmatized['Tag'].isin(tagset_allowed)]<br/>text = [' '.join(str(x) for x in new_tagset['Lemmatized Word'])]</span><span id="f8c2" class="pd mp iu pp b gz px pu l pv pw">vectorizer = CountVectorizer(ngram_range=(2, 2))<br/>bag_of_words = vectorizer.fit_transform(text)<br/>vectorizer.vocabulary_<br/>sum_words = bag_of_words.sum(axis=0) <br/>words_freq = [(word, sum_words[0, idx]) for word, idx in vectorizer.vocabulary_.items()]<br/>words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)<br/>print (words_freq[:100])</span><span id="0d8d" class="pd mp iu pp b gz px pu l pv pw">#Generating wordcloud and saving as jpg image<br/>words_dict = dict(words_freq)<br/>WC_height = 800<br/>WC_width = 1600<br/>WC_max_words = 200<br/>wordCloud = WordCloud(max_words=WC_max_words, height=WC_height, width=WC_width,background_color = 'white')<br/>wordCloud.generate_from_frequencies(words_dict)<br/>plt.title('Most frequently occurring bigrams')<br/>plt.imshow(wordCloud, interpolation='bilinear')<br/>plt.axis("off")<br/>plt.show()<br/>wordCloud.to_file('wordcloud_bigram_title.jpg')</span></pre><figure class="kt ku kv kw gu kx gi gj paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="gi gj qe"><img src="../Images/0c65744722ea6363b16e700e866cf813.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oI_WXo8hSIzoR7eGf8Fl0g.jpeg"/></div></div><p class="le lf gk gi gj lg lh bd b be z dk translated">Bigram词云。图片作者。</p></figure><p id="4afe" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">从Bigram词云，我们可以看到比unigram更有意义的短语。比如强化学习，learn TensorFlow，raspberry pi，初学者指南，图像分割和硅谷。</p><p id="1a71" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">让我们看看用三元词云能找到什么。</p><h2 id="f220" class="pd mp iu bd mq pe pf dn mu pg ph dp my ls pi pj na lw pk pl nc ma pm pn ne ja bi translated">三元模型</h2><p id="16be" class="pw-post-body-paragraph lj lk iu ll b lm ng ke lo lp nh kh lr ls ni lu lv lw nj ly lz ma nk mc md me in bi translated">与二元模型相同，我们必须先用计数矢量器计算频率。</p><pre class="kt ku kv kw gu po pp pq pr aw ps bi"><span id="3daa" class="pd mp iu pp b gz pt pu l pv pw">#Using count vectoriser to view the frequency of trigrams<br/>vectorizer = CountVectorizer(ngram_range=(3, 3))<br/>bag_of_words = vectorizer.fit_transform(text)<br/>vectorizer.vocabulary_<br/>sum_words = bag_of_words.sum(axis=0) <br/>words_freq = [(word, sum_words[0, idx]) for word, idx in vectorizer.vocabulary_.items()]<br/>words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)<br/>print (words_freq[:100])</span><span id="6a9c" class="pd mp iu pp b gz px pu l pv pw">#Generating wordcloud and saving as jpg image<br/>words_dict = dict(words_freq)<br/>WC_height = 800<br/>WC_width = 1600<br/>WC_max_words = 200<br/>wordCloud = WordCloud(max_words=WC_max_words, height=WC_height, width=WC_width,background_color = 'white')<br/>wordCloud.generate_from_frequencies(words_dict)<br/>plt.title('Most frequently occurring trigrams')<br/>plt.imshow(wordCloud, interpolation='bilinear')<br/>plt.axis("off")<br/>plt.show()<br/>wordCloud.to_file('wordcloud_trigram_title.jpg')</span></pre><figure class="kt ku kv kw gu kx gi gj paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="gi gj qe"><img src="../Images/2c2da605fcaa0ef3653465abd39d250b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DSRTHGgFhxMm5euHX74oWA.jpeg"/></div></div><p class="le lf gk gi gj lg lh bd b be z dk translated">三元词云。图片作者。</p></figure><p id="b6cf" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">频繁阶段几乎类似于二元模型，除了“检测对象深度”、“识别深度学习”和“深度学习覆盆子”。前两个三元模型应该分别用于深度学习的对象检测和深度学习的图像识别，而第三个三元模型，我不确定。</p><blockquote class="oc"><p id="7ce3" class="od oe iu bd of og oh oi oj ok ol me dk translated">让我们找出它。</p></blockquote><pre class="qf qg qh qi qj po pp pq pr aw ps bi"><span id="b037" class="pd mp iu pp b gz pt pu l pv pw"># using the pre-process data<br/>title = df['title'].values<br/>keyline = []<br/>for line in title:<br/>    line = line.lower()<br/>    result = re.search(r"(^|[^a-z])" + "raspberry pi" + r"([^a-z]|$)", line)<br/>    if result != None:<br/>        keyline.append(line)</span></pre><figure class="kt ku kv kw gu kx gi gj paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="gi gj qk"><img src="../Images/d51d5f3da363c5d5a26910e0eff92485.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5_BdIuD7VDdqYjVSgzmm4g.png"/></div></div><p class="le lf gk gi gj lg lh bd b be z dk translated">片名包含“树莓派”。作者图片</p></figure><p id="3b74" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">Raspberry Pi用于检测具有深度学习的对象。</p><p id="2c24" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">重复的文章不会被删除，因为它们有不同的链接。</p></div><div class="ab cl mh mi hy mj" role="separator"><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm"/></div><div class="in io ip iq ir"><h1 id="f3d8" class="mo mp iu bd mq mr ms mt mu mv mw mx my kj mz kk na km nb kn nc kp nd kq ne nf bi translated">从媒体文章标题的词云透视</h1><p id="e195" class="pw-post-body-paragraph lj lk iu ll b lm ng ke lo lp nh kh lr ls ni lu lv lw nj ly lz ma nk mc md me in bi translated">从为媒体文章标题创建的词云中，我们可以看到大多数标题是由更多的<strong class="ll je">通用</strong>或<strong class="ll je">流行</strong>术语组成的，如机器学习、人工智能、深度学习和神经网络。这些都是当时的热门话题。但是，数据集不包含文章发布的时间。因此，我们无法确定词语的选择如何影响文章的浏览率。</p></div><div class="ab cl mh mi hy mj" role="separator"><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm"/></div><div class="in io ip iq ir"><h1 id="a561" class="mo mp iu bd mq mr ms mt mu mv mw mx my kj mz kk na km nb kn nc kp nd kq ne nf bi translated">结论</h1><p id="a93c" class="pw-post-body-paragraph lj lk iu ll b lm ng ke lo lp nh kh lr ls ni lu lv lw nj ly lz ma nk mc md me in bi translated">一般来说，如果与二元模型和三元模型相比，一元模型的平均出现频率会更高。因此，为了避免有意义的二元模型和三元模型被埋没，我们将分别为一元模型、二元模型和三元模型创建词云。</p><p id="e269" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">此外，二元模型和三元模型也有助于我们更好地理解文本，因为一元模型可能会令人困惑，当附加到一元模型的单词发生变化时，其含义可能会发生变化。比如低质好质。</p><p id="dfc1" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">这也引出了另一个问题，为什么在某些情况下文本探索是必不可少的。想象你正在浏览一家家具店的评论，如果你正在做<strong class="ll je">话题建模</strong>，你可能会得到人们正在讨论的<strong class="ll je">一般话题</strong>，像椅子、桌子、橱柜等等。通过<strong class="ll je">词云</strong>进行文本探索，你可能知道人们在评论中经常提到的<strong class="ll je">短语</strong>。例如，表质量低，交货慢等。</p><p id="d069" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">最后，虽然在我的例子中没有显示出来，但是我们可能会遇到这样的情况，当我们在文本探索过程中探索一些经常显示但没有意义的标记时，我们需要回到文本处理。移除它们可以揭示隐藏在它们下面的有洞察力的标记，并且进一步减少训练模型的时间。</p><blockquote class="oc"><p id="9db5" class="od oe iu bd of og oh oi oj ok ol me dk translated">字云只是文字探索的方法之一，可能不是最好的，但却是最好玩的方法！</p></blockquote></div><div class="ab cl mh mi hy mj" role="separator"><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm"/></div><div class="in io ip iq ir"><h2 id="040a" class="pd mp iu bd mq pe pf dn mu pg ph dp my ls pi pj na lw pk pl nc ma pm pn ne ja bi translated">一些旁注</h2><p id="2fcb" class="pw-post-body-paragraph lj lk iu ll b lm ng ke lo lp nh kh lr ls ni lu lv lw nj ly lz ma nk mc md me in bi translated">如果你对NLTK和SpaCy在文本处理上的区别感兴趣的话，<a class="ae li" rel="noopener" target="_blank" href="/text-processing-in-python-29e86ea4114c">Python中的文本处理</a>。</p><p id="eb79" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">如果您对从列表中提取包含关键字的句子感兴趣，<a class="ae li" rel="noopener" target="_blank" href="/text-extraction-using-regular-expression-python-186369add656">使用正则表达式(Python) </a>进行文本提取。</p><h1 id="a452" class="mo mp iu bd mq mr oy mt mu mv oz mx my kj pa kk na km pb kn nc kp pc kq ne nf bi translated">保持联系</h1><p id="458a" class="pw-post-body-paragraph lj lk iu ll b lm ng ke lo lp nh kh lr ls ni lu lv lw nj ly lz ma nk mc md me in bi translated">订阅<a class="ae li" href="https://www.youtube.com/channel/UCiMtx0qbILP41Ot-pkk6eJw" rel="noopener ugc nofollow" target="_blank"> YouTube </a></p><h1 id="ceb1" class="mo mp iu bd mq mr oy mt mu mv oz mx my kj pa kk na km pb kn nc kp pc kq ne nf bi translated">参考</h1><p id="e20d" class="pw-post-body-paragraph lj lk iu ll b lm ng ke lo lp nh kh lr ls ni lu lv lw nj ly lz ma nk mc md me in bi translated">[1] D. Juraksky和J. H. Martin，《词性和命名实体的<a class="ae li" href="https://web.stanford.edu/~jurafsky/slp3/8.pdf" rel="noopener ugc nofollow" target="_blank">序列标注》，载于</a><a class="ae li" href="https://web.stanford.edu/~jurafsky/slp3/" rel="noopener ugc nofollow" target="_blank"> <em class="oa">语音和语言处理</em> </a>，2020年，第4页。</p><p id="610b" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">[2] A. Kim和A. Escobedo-Land，“OkCupid Data for introductive Statistics and Data Science Courses”，<em class="oa">《统计教育杂志》，</em>第23卷，2015年07期。</p></div><div class="ab cl mh mi hy mj" role="separator"><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm"/></div><div class="in io ip iq ir"><p id="70e4" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">祝贺并感谢你阅读到最后。希望你喜欢这篇文章。 ☺️</p><figure class="kt ku kv kw gu kx gi gj paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="gi gj ql"><img src="../Images/266879739ab1d6bc5b40cf2a10d09b55.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TaeDzyHiLmiZ1iO0A2OEWQ.jpeg"/></div></div><p class="le lf gk gi gj lg lh bd b be z dk translated">伊恩·施耐德在<a class="ae li" href="https://unsplash.com/s/photos/passion?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure></div></div>    
</body>
</html>