<html>
<head>
<title>7 Steps to Design a Basic Neural Network (part 1 of 2)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">设计基本神经网络的7个步骤(第1部分，共2部分)</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/7-steps-to-design-a-basic-neural-network-part-1-of-2-ff0d391bf32b?source=collection_archive---------34-----------------------#2021-05-04">https://towardsdatascience.com/7-steps-to-design-a-basic-neural-network-part-1-of-2-ff0d391bf32b?source=collection_archive---------34-----------------------#2021-05-04</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="45cd" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">一个数学不太密集的，一步一步的指南，从头开始建立一个隐层神经网络</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/a06e22c073d13b916d73c7aab5b97774.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*7oc5lrwip3jku5dl"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图片由Lindsay Henwood在Unsplash上提供</p></figure><p id="dcbc" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">这篇由两部分组成的文章采用了一种更全面、更全面(是的，更少数学化)的方法来从头构建一个神经网络。用于完成网络的Python也包含在7个步骤的每一个中。</p><p id="0015" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx ir">第一部分:</strong> (1)定义网络结构，(2)初始化参数，(3)实现正向传播。<a class="ae lr" href="https://gabeverzino.medium.com/7-steps-to-design-a-basic-neural-network-part-2-of-2-792cb7273c3" rel="noopener"> <strong class="kx ir">第二部分</strong> </a> <strong class="kx ir"> : </strong> (4)估计成本，(5)实现反向传播，(6)更新参数，(7)进行预测。</p><h2 id="c0ff" class="ls lt iq bd lu lv lw dn lx ly lz dp ma le mb mc md li me mf mg lm mh mi mj mk bi translated">快速背景:为什么逻辑回归对于噪音数据集是不够的</h2><p id="1281" class="pw-post-body-paragraph kv kw iq kx b ky ml jr la lb mm ju ld le mn lg lh li mo lk ll lm mp lo lp lq ij bi translated">数据通常呈非线性分布，或者包含传统分类模型无法很好区分的异常边界。例如，假设我们想要对以下数据集中的红点和蓝点进行正确分类:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mq"><img src="../Images/767afd933594d226b9af16b49bddf3dd.png" data-original-src="https://miro.medium.com/v2/resize:fit:740/format:webp/1*vt4KQQdq2U88OSN4BPFkDA.jpeg"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="f3a6" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我们可以从逻辑回归开始——二元分类问题的常用模型。像往常一样，我们将导入必要的库，并使逻辑模型适合X变量(即上面点的X轴和Y轴坐标)和Y结果变量(即红色或蓝色点)。</p><pre class="kg kh ki kj gt mr ms mt mu aw mv bi"><span id="5193" class="ls lt iq ms b gy mw mx l my mz">import pandas as pd<br/>from sklearn.linear_model import LogisticRegression<br/>from sklearn.metrics import classification_report, confusion_matrix</span><span id="388b" class="ls lt iq ms b gy na mx l my mz">logmodel = LogisticRegression()<br/>logmodel.fit(X.T, Y.T)<br/>predictions = logmodel.predict(X.T)</span></pre><p id="39f9" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">应用逻辑回归模型的边界预测，我们可以立即看到一些错误分类的发生。</p><pre class="kg kh ki kj gt mr ms mt mu aw mv bi"><span id="fdc5" class="ls lt iq ms b gy mw mx l my mz">plot_decision_boundary(lambda x: logmodel.predict(x), X, Y)<br/>plt.title("Logistic Regression")</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nb"><img src="../Images/c9e95d8b86d1456f5ddc61f2ba2c539f.png" data-original-src="https://miro.medium.com/v2/resize:fit:810/format:webp/1*_iVTYYkdnwTG80e3Mt40VQ.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="c7b7" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">虽然大多数点被准确预测，但一些红点被归类为蓝色，反之亦然。这个模型有点太笼统，欠拟合数据，会导致<strong class="kx ir">偏高</strong>。运行快速分类报告和混淆矩阵显示，我们的逻辑模型的准确率约为87%。</p><pre class="kg kh ki kj gt mr ms mt mu aw mv bi"><span id="48fa" class="ls lt iq ms b gy mw mx l my mz">print(classification_report(predictions, Y.T))</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nc"><img src="../Images/caf70a534904215b287d7b6842b0d8fe.png" data-original-src="https://miro.medium.com/v2/resize:fit:846/format:webp/1*Uqh6lsliW7UU1JOV7BSG-A.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">分类报告(图片由作者提供)</p></figure><pre class="kg kh ki kj gt mr ms mt mu aw mv bi"><span id="a71a" class="ls lt iq ms b gy mw mx l my mz">conf_matrix = np.array(confusion_matrix(Y.T, predictions))<br/>pd.DataFrame(conf_matrix, index=['Purple','Red'],columns=['Predicted Purple','Predicted Red'])</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nd"><img src="../Images/a9a294eb083c43ee241a37d61be6c353.png" data-original-src="https://miro.medium.com/v2/resize:fit:520/format:webp/1*q2WL95E6z34FJuAbrqxKIg.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">困惑矩阵(图片由作者提供)</p></figure><p id="0a97" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">虽然这可能足以满足一些数据任务——基本的预测模型或快速的直觉检查——但神经网络在识别复杂模式(如上所述)和优化预测方面可以做得更好。</p><p id="c2bb" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">好吧，让我们开始吧！</p><h2 id="8ae1" class="ls lt iq bd lu lv lw dn lx ly lz dp ma le mb mc md li me mf mg lm mh mi mj mk bi translated">步骤1:定义网络结构</h2><p id="b8fe" class="pw-post-body-paragraph kv kw iq kx b ky ml jr la lb mm ju ld le mn lg lh li mo lk ll lm mp lo lp lq ij bi translated">人们将神经网络比作大脑中的神经元，因为它们看起来相似，并且都参与学习。事实是，与大脑神经元不同，神经网络双向“发送信息”，这种独特的双向对反向传播中发生的学习至关重要(在<a class="ae lr" href="https://gabeverzino.medium.com/7-steps-to-design-a-basic-neural-network-part-2-of-2-792cb7273c3" rel="noopener">第2部分</a>中有所介绍)。</p><p id="ebb7" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">考虑到这一点，我们首先要构建我们的神经网络。如下所示，网络包含节点和层(节点列)。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nf"><img src="../Images/36f997e2cce6c39e40816fd8fc6e61ec.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*h6bI3xqi9MHWorVeKJpDhQ.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">基本的一层神经网络(图片由作者提供，他的女朋友不同意他的笔迹)</p></figure><p id="4368" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx ir">输入层:</strong>从左到右，我们从输入层和2个节点开始，每个节点代表我们数据集中的一个变量(我们的X变量包含X轴和Y轴坐标数据，所以它变成了X1和X2)。</p><p id="0403" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx ir">隐藏层:</strong>在被称为前向传播的过程中，输入层数据被传输到隐藏层中的4个节点中的每一个。你在隐藏后选择的节点数量很重要；节点太少，您的模型可能会不足，节点太多，您的模型可能会过拟合，运行缓慢。根据StackExchange <a class="ae lr" href="https://stats.stackexchange.com/questions/181/how-to-choose-the-number-of-hidden-layers-and-nodes-in-a-feedforward-neural-netw#:~:text=There%20are%20many%20rule-of,size%20of%20the%20output%20layer." rel="noopener ugc nofollow" target="_blank">帖子</a>中解释的信息，我们将为隐藏层选择4个节点。</p><p id="19a4" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx ir">输出层:</strong>隐藏层的信息随后被传输到输出层，以进行我们的最终预测(即，点是蓝色还是红色？).这是通过完成本文后面提到的一些步骤(估计成本、反向传播和更新我们的参数)来实现的。</p><p id="bc40" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">因此，让我们创建这3层现在与他们相应的节点数量。</p><pre class="kg kh ki kj gt mr ms mt mu aw mv bi"><span id="6834" class="ls lt iq ms b gy mw mx l my mz">def layer_sizes(X, Y):<br/>    """Takes the arguments:<br/>        X = shape of feature variable data<br/>        Y = shape of the output data<br/>        <br/>        Returns:<br/>        n_x = input layer size<br/>        n_h = hidden layer size<br/>        n_y = output layer size"""<br/>    n_x = X.shape[0]<br/>    n_h = 4<br/>    n_y = Y.shape[0]<br/>    return(n_x, n_h, n_y)</span><span id="e756" class="ls lt iq ms b gy na mx l my mz">(n_x, n_h, n_y) = layer_sizes(X, Y)</span><span id="ccee" class="ls lt iq ms b gy na mx l my mz">print("The size of the input layer is: n_x = " + str(n_x))<br/>print("The size of the hidden layer is: n_h = " + str(n_h))<br/>print("The size of the output layer is: n_y = " + str(n_y))</span></pre><p id="b622" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">其输出是:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ng"><img src="../Images/f3a835d6f99da5f569924645f3e04369.png" data-original-src="https://miro.medium.com/v2/resize:fit:664/format:webp/1*8Wl4gbgYBWlMVw1Mj3HTvg.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">神经网络的层和节点(图片由作者提供)</p></figure><h2 id="6719" class="ls lt iq bd lu lv lw dn lx ly lz dp ma le mb mc md li me mf mg lm mh mi mj mk bi translated">第二步。初始化参数</h2><p id="24bd" class="pw-post-body-paragraph kv kw iq kx b ky ml jr la lb mm ju ld le mn lg lh li mo lk ll lm mp lo lp lq ij bi translated">现在我们已经定义了我们的网络结构，我们需要考虑构建将输入层数据传输到隐藏层的函数，以启动神经网络过程。<strong class="kx ir">然而，这些函数包含我们必须初始化(即设置值)的术语(称为参数)。</strong></p><p id="73c0" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">这是我们将在步骤3中使用的第一个函数。正向传播):</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nh"><img src="../Images/a42fdb66c51d2d0ba5fa73c0fd51b5d4.png" data-original-src="https://miro.medium.com/v2/resize:fit:296/format:webp/1*mqczDtPM6GgHu3Cv-meYdw.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">前向传播中使用的基本线性函数</p></figure><p id="18cb" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在这个函数中，我们已经有了我们的输入数据<strong class="kx ir"> x </strong>、该数据的权重(重要性)<strong class="kx ir"> w </strong>和我们的偏差项<strong class="kx ir"> b </strong>。<strong class="kx ir"> w </strong>和<strong class="kx ir"> b </strong>为参数。因为我们还没有参数的值，所以我们必须初始化它们。</p><p id="4e29" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我们如何以及为什么初始化这些参数与梯度下降有很大关系，梯度下降是神经网络行为的核心组成部分。所以让我们简单地聊一聊。</p><h2 id="5935" class="ls lt iq bd lu lv lw dn lx ly lz dp ma le mb mc md li me mf mg lm mh mi mj mk bi translated">梯度下降(快速注释)</h2><p id="b7e0" class="pw-post-body-paragraph kv kw iq kx b ky ml jr la lb mm ju ld le mn lg lh li mo lk ll lm mp lo lp lq ij bi translated">像许多非确定性模型一样，神经网络在随机梯度下降的前提下运行。在训练期间，这些算法采取增量步骤来最小化成本——反映模型整体表现如何的单个值(即预测值和实际值之间的误差)。他们采取的“学习步骤”取决于我们的<strong class="kx ir"> w </strong>(权重)和<strong class="kx ir"> b </strong>(偏差)。</p><p id="7db7" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">对于我们的模型，我们想要尽可能低的成本。将梯度下降过程想象成一个球滚下山坡，试图找到最低点(成本)。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ni"><img src="../Images/f7b03c587b5b6cef551e305c19b6b5a2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fH_9_fuwtAFgRAU-MJcRnA.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">梯度下降可视化(作者图片)</p></figure><p id="b29b" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在好的(不太好的)模型中，成本可能会停留在起点的较低点(<em class="ne">局部最小值</em>)，但不会处于绝对低点(<em class="ne">全局最小值</em>)。因此，因为我们(1)无法提前可视化我们的搜索空间，并且(2)不想以次优成本“困住”我们的模型，所以一个好的方法是用<strong class="kx ir">随机值</strong>初始化我们的参数<strong class="kx ir"> w </strong>(权重)和<strong class="kx ir"> b </strong>(偏差)。这确保了梯度下降从任何地方开始，而不会注入我们自己的偏见。</p><p id="b739" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">其次，我们希望参数的值<strong class="kx ir">小一些，例如，大约0.01。我们的选择有点像金发姑娘。太大的值和成本将在最小值附近振荡太多(即“爆炸梯度问题”)，零值基本上阻止我们的梯度更新任何权重。这被称为“未能打破对称性”，是神经元计算类似输出的结果，阻止了独立学习，并从本质上抵消了神经网络的优势。</strong></p><p id="62b8" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我们需要初始化的参数将跨越输入层到隐藏层(称它们为<strong class="kx ir"> W1 </strong>和<strong class="kx ir"> b1 </strong>)，然后隐藏层到输出层(称它们为<strong class="kx ir"> W2 </strong>和<strong class="kx ir"> b2 </strong>)。正如我们上面提到的，我们将把它们初始化为随机的小值。注意，偏置项<strong class="kx ir"> b1 </strong>和<strong class="kx ir"> b2 </strong>是向量，因为它们是通过<a class="ae lr" href="https://numpy.org/doc/stable/user/basics.broadcasting.html" rel="noopener ugc nofollow" target="_blank">广播</a>加到<strong class="kx ir"> w*x </strong>乘积上的，所以可以用大小为1的列初始化为零。</p><pre class="kg kh ki kj gt mr ms mt mu aw mv bi"><span id="3372" class="ls lt iq ms b gy mw mx l my mz">def initialize_parameters(n_x, n_h, n_y): <br/>    """Initialize the parameters for weight matrix w1, w2 and bias vectors b1, b2.<br/>       <br/>       Where: <br/>       n_x -- size of the input layer<br/>       n_h -- size of the hidden layer<br/>       n_y -- size of the output layer<br/>    <br/>       Returns:<br/>       params -- python dictionary containing your parameters:<br/>                        W1 -- weight matrix of shape (n_h, n_x)<br/>                        b1 -- bias vector of shape (n_h, 1)<br/>                        W2 -- weight matrix of shape (n_y, n_h)<br/>                        b2 -- bias vector of shape (n_y, 1)<br/>    """<br/>    np.random.seed(2)<br/>    <br/>    W1 = np.random.randn(n_h, n_x) * 0.01<br/>    b1 = np.zeros((n_h, 1))<br/>    W2 = np.random.randn(n_y, n_h) * 0.01<br/>    b2 = np.zeros((n_y, 1))<br/>    <br/>    assert (W1.shape == (n_h, n_x))<br/>    assert (b1.shape == (n_h, 1))<br/>    assert (W2.shape == (n_y, n_h))<br/>    assert (b2.shape == (n_y, 1))<br/>    <br/>    parameters = {'W1': W1,<br/>                  'b1': b1,<br/>                  'W2': W2,<br/>                  'b2': b2}<br/>    <br/>    return parameters</span></pre><p id="6e21" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">让我们看看这些参数会是什么样子:</p><pre class="kg kh ki kj gt mr ms mt mu aw mv bi"><span id="8814" class="ls lt iq ms b gy mw mx l my mz">parameters = initialize_parameters(n_x, n_h, n_y)<br/>print("W1 = " + str(parameters["W1"]))<br/>print("b1 = " + str(parameters["b1"]))<br/>print("W2 = " + str(parameters["W2"]))<br/>print("b2 = " + str(parameters["b2"]))</span></pre><p id="ba86" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">退货:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nj"><img src="../Images/5973720c9dfd8f78484dd1c1fc682acb.png" data-original-src="https://miro.medium.com/v2/resize:fit:876/format:webp/1*XcNjYLuGvSBBsWc3vGcz-g.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">用随机的小值初始化权重(图片由作者提供)</p></figure><h2 id="d701" class="ls lt iq bd lu lv lw dn lx ly lz dp ma le mb mc md li me mf mg lm mh mi mj mk bi translated">步骤3:正向传播</h2><p id="254a" class="pw-post-body-paragraph kv kw iq kx b ky ml jr la lb mm ju ld le mn lg lh li mo lk ll lm mp lo lp lq ij bi translated">初始化参数后，我们现在可以实现如上所述的线性函数(<strong class="kx ir"> z = w * x + b </strong>)。然而，单独的线性函数只会产生一个<a class="ae lr" href="https://stackoverflow.com/questions/9782071/why-must-a-nonlinear-activation-function-be-used-in-a-backpropagation-neural-net#:~:text=Input%20to%20networks%20is%20usually,and%20problems%20are%20non%2Dlinear.&amp;text=Non%2Dlinearity%20is%20needed%20in,of%20the%20weight%20and%20inputs." rel="noopener ugc nofollow" target="_blank">执行类似于</a>逻辑或线性回归的神经网络。因此，我们还必须将我们的<strong class="kx ir"> z </strong>输出传递给被称为<strong class="kx ir">激活函数</strong>的非线性函数。</p><p id="5309" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">这里的直觉是，我们有一个带有非线性决策边界的嘈杂数据集，那么为什么要应用线性模型呢？正是这些非线性激活函数将采取额外的步骤来转换我们的线性函数的输出(<strong class="kx ir"> z = w * x + b </strong>)，以便更好地对我们的非线性数据进行分类。</p><p id="225e" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我们可以在神经网络中使用多种激活函数，每种都有自己的优缺点。对于我们的神经网络，我们将在隐藏层选择一个<strong class="kx ir"> tanh </strong>激活函数，在输出层选择一个<strong class="kx ir"> sigmoid </strong>激活函数。Tanh计算-1和1之间的输出，激活的平均值为零。这具有自然地使我们的数据居中的效果，这允许在隐藏层之间学习以更快地计算。Sigmoid在输出层非常理想，特别是对于二进制分类问题，因为它计算0和1之间的输出。</p><p id="84ff" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">如果我们的<strong class="kx ir"> z </strong>输入非常大或非常小，那么sigmoid和tanh都有轻微的缺点；梯度(或斜率)变得非常小(有时接近零),可以减缓梯度下降。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nk"><img src="../Images/7558527f6e150a405b0b3df0f5fa7541.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*97F5I7pWFuc_eKY2mhe8vQ.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">1-隐层神经网络的z和激活函数(图片由作者提供)</p></figure><p id="6b03" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">让我们从上面显示的4个函数中解开细节。</p><ul class=""><li id="d7cd" class="nl nm iq kx b ky kz lb lc le nn li no lm np lq nq nr ns nt bi translated">第一个线性<strong class="kx ir"> z[1] </strong>函数计算所有输入层数据<strong class="kx ir"> a[0] </strong>(一种表示我们所有X特征数据的方式)和输入层权重<strong class="kx ir"> W[1] </strong>的乘积，并添加所有输入层偏差项<strong class="kx ir">b[1】</strong>。</li><li id="6059" class="nl nm iq kx b ky nu lb nv le nw li nx lm ny lq nq nr ns nt bi translated">激活函数<strong class="kx ir"> a[1] </strong>是<strong class="kx ir"> z[1] </strong>输出的双曲正切函数。</li><li id="f5d2" class="nl nm iq kx b ky nu lb nv le nw li nx lm ny lq nq nr ns nt bi translated">第二个线性<strong class="kx ir">z【2】</strong>函数计算所有隐藏层数据<strong class="kx ir">a【1】</strong>(一种表示我们所有初始激活函数的方式)和隐藏层权重<strong class="kx ir">W【2】</strong>的乘积，并添加所有隐藏层偏差项<strong class="kx ir">b【2】</strong>。</li><li id="3a07" class="nl nm iq kx b ky nu lb nv le nw li nx lm ny lq nq nr ns nt bi translated">激活函数<strong class="kx ir"> a[2] </strong>是<strong class="kx ir"> z[2] </strong>输出的sigmoid函数。</li></ul><p id="3c6b" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我们可以为<strong class="kx ir">z【1】</strong><strong class="kx ir">a【1】</strong><strong class="kx ir">z【2】</strong>和<strong class="kx ir">a【2】</strong>编写上述函数。然后我们可以将这些值存储到一个名为“cache”的字典中，以备后用。</p><pre class="kg kh ki kj gt mr ms mt mu aw mv bi"><span id="b3cc" class="ls lt iq ms b gy mw mx l my mz">def forward_propagation(X, parameters):<br/>    """Forward propagation will compute activations functions for <br/>    the hidden and output layer, producting outputs A1 and A2.<br/>    Arguments: Takes X data and parameter variables (W1, W2, b1, b2)<br/>    <br/>    """<br/>    <br/>    #retrieve each of the paramters from the dictionary<br/>    W1 = parameters['W1']<br/>    b1 = parameters['b1']<br/>    W2 = parameters['W2']<br/>    b2 = parameters['b2']<br/>    <br/>    Z1 = np.dot(W1,X) + b1<br/>    A1 = np.tanh(Z1)<br/>    Z2 = np.dot(W2, A1) + b2<br/>    A2 = sigmoid(Z2)<br/>    <br/>    cache = {"Z1": Z1,<br/>             "A1": A1,<br/>             "Z2": Z2,<br/>             "A2": A2}<br/>    <br/>    return A2, cache</span></pre><p id="287f" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">现在我们已经在缓存中定义了线性Z函数(<strong class="kx ir"> z[1] </strong>、<strong class="kx ir"> z[2] </strong>)和激活函数(<strong class="kx ir"> a[1] </strong>、<strong class="kx ir"> a[2] </strong>)，以备后用。</p><p id="2973" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">干得好，走了这么远！我们已经学会了如何创建一个神经网络结构，初始化参数，实现一些激活功能，是的，甚至还学了一点数学。</p><p id="ac2f" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在本帖的第2部分 中，我们将介绍构建神经网络模型的其余步骤:(4)定义成本，(5)实现反向传播，(6)更新参数，以及(7)进行预测。</p></div></div>    
</body>
</html>