<html>
<head>
<title>The Ultimate Guide to Clustering Algorithms and Topic Modeling</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">聚类算法和主题建模终极指南</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/the-ultimate-guide-to-clustering-algorithms-and-topic-modeling-3a65129df324?source=collection_archive---------27-----------------------#2021-07-27">https://towardsdatascience.com/the-ultimate-guide-to-clustering-algorithms-and-topic-modeling-3a65129df324?source=collection_archive---------27-----------------------#2021-07-27</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/75aa21494d2ae6e5722889f2c6560924.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OuXvHDCTt2YwlF5Ib5gydA.jpeg"/></div></div></figure><div class=""/><div class=""><h2 id="8e45" class="pw-subtitle-paragraph kb jd je bd b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks dk translated">第2部分，LDA模型的初学者指南</h2></div><p id="bb50" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">在我之前的帖子中，我介绍了聚类算法并详细讨论了K-Means算法，作为<strong class="kv jf"> <em class="lp">主题建模</em> </strong>系列的第一部分:</p><blockquote class="lq"><p id="5330" class="lr ls je bd lt lu lv lw lx ly lz lo dk translated"><a class="ae ma" rel="noopener" target="_blank" href="/wthe-ultimate-guide-to-clustering-algorithms-and-topic-modeling-4f7757c115">第一部分:K-Means入门指南</a></p><p id="f2f8" class="lr ls je bd lt lu lv lw lx ly lz lo dk translated">第2部分:LDA初学者指南(本文)</p><p id="3516" class="lr ls je bd lt lu lv lw lx ly lz lo dk translated">第3部分:使用K-Means和LDA进行主题建模(即将推出)</p></blockquote><p id="ff80" class="pw-post-body-paragraph kt ku je kv b kw mb kf ky kz mc ki lb lc md le lf lg me li lj lk mf lm ln lo im bi translated">我们不能在不介绍LDA模型的情况下讨论主题建模。LDA是<strong class="kv jf">潜在狄利克雷分配</strong>的简称。它是一个模型，主要用于理解文本数据集合中的潜在主题集，在协作过滤、基于内容的图像检索和生物信息学等方面也有其他应用。在本文中，我将讨论主题建模和LDA模型设置的更多细节。</p></div><div class="ab cl mg mh hx mi" role="separator"><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml"/></div><div class="im in io ip iq"><h2 id="b5d7" class="mn mo je bd mp mq mr dn ms mt mu dp mv lc mw mx my lg mz na nb lk nc nd ne nf bi translated">主题建模和术语</h2><p id="f143" class="pw-post-body-paragraph kt ku je kv b kw ng kf ky kz nh ki lb lc ni le lf lg nj li lj lk nk lm ln lo im bi translated">主题建模是在一组文本数据上寻找潜在的<strong class="kv jf"> <em class="lp">主题</em> </strong>的分析。研究主题有助于研究者理解文本中隐藏的语义结构。了解主题对于分类现有文本数据和生成新数据都很有用。在进一步详细讨论之前，我们需要指定一些在文本分析中使用的常规术语。根据<strong class="kv jf"> Blei、ng和Jordan 2003 </strong>(参考文献中的详细引用)，术语表述如下:</p><ul class=""><li id="687f" class="nl nm je kv b kw kx kz la lc nn lg no lk np lo nq nr ns nt bi translated"><strong class="kv jf"> <em class="lp">单词</em> </strong>是文本数据的基本单位，从词汇集{1，…V}中索引；</li><li id="8b79" class="nl nm je kv b kw nu kz nv lc nw lg nx lk ny lo nq nr ns nt bi translated">一个<strong class="kv jf"> <em class="lp">文档</em> </strong>是N个字的序列，定义为<strong class="kv jf"> w </strong> = (w_1，w_2，…w_N)，其中w_N是这个文档中的第N个字；</li><li id="68bf" class="nl nm je kv b kw nu kz nv lc nw lg nx lk ny lo nq nr ns nt bi translated">一个<strong class="kv jf"> <em class="lp">语料库</em> </strong>是M个文档的集合，定义为<strong class="kv jf"> D </strong> = ( <strong class="kv jf"> w_ </strong> 1、<strong class="kv jf"> w_ </strong> 2、…<strong class="kv jf">w _</strong>M)；</li><li id="6ec8" class="nl nm je kv b kw nu kz nv lc nw lg nx lk ny lo nq nr ns nt bi translated">一个<strong class="kv jf"> <em class="lp">题目</em> </strong>是分布词。LDA模型将语料库中的每个文档视为主题的混合。</li></ul><h2 id="9601" class="mn mo je bd mp mq mr dn ms mt mu dp mv lc mw mx my lg mz na nb lk nc nd ne nf bi translated">LDA模型设置</h2><p id="d967" class="pw-post-body-paragraph kt ku je kv b kw ng kf ky kz nh ki lb lc ni le lf lg nj li lj lk nk lm ln lo im bi translated">主题建模本质上是一个文本聚类问题。使用LDA模型，目标是通过研究语料库来估计两组分布:</p><ul class=""><li id="deb6" class="nl nm je kv b kw kx kz la lc nn lg no lk np lo nq nr ns nt bi translated">每个主题中单词的分布</li><li id="de33" class="nl nm je kv b kw nu kz nv lc nw lg nx lk ny lo nq nr ns nt bi translated">语料库中的主题分布</li></ul><p id="871c" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">LDA是一个三层的分层贝叶斯模型。与其深入研究数学细节，不如用一个例子来演示这个模型。</p><p id="de91" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">LDA模型为语料库中的每个文档生成主题分布。例如，文档可以分布在两个主题上:40%在主题“水果”上，60%在主题“蔬菜”上:</p><figure class="oa ob oc od gt iv gh gi paragraph-image"><div class="gh gi nz"><img src="../Images/b5a9963f075671513505b6961ff8a848.png" data-original-src="https://miro.medium.com/v2/resize:fit:582/format:webp/1*U1-h6_NWmdK15bMc7ka36A.png"/></div><p class="oe of gj gh gi og oh bd b be z dk translated">作者图片</p></figure><p id="7b21" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">每个主题都是词汇集{“甜的”、“胡萝卜”、“苹果”、“绿色”}中所有单词的分布。看题目，我们看到一些概率比较高的词。例如，直觉上，单词“绿色”在主题“蔬菜”中出现的概率比在主题“水果”中出现的概率高</p><figure class="oa ob oc od gt iv gh gi paragraph-image"><div class="gh gi oi"><img src="../Images/96c096acd42d034c19fe77b033866288.png" data-original-src="https://miro.medium.com/v2/resize:fit:838/format:webp/1*JaPJfHBgiLuqmNE_EiURMQ.png"/></div><p class="oe of gj gh gi og oh bd b be z dk translated">作者图片</p></figure><p id="4172" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">我们需要定义两个潜在变量<a class="ae ma" href="https://en.wikipedia.org/wiki/Latent_variable#:~:text=In%20statistics%2C%20latent%20variables%20(from,are%20observed%20(directly%20measured)." rel="noopener ugc nofollow" target="_blank"><strong class="kv jf"/></a><strong class="kv jf"/>，它们有助于从语料库到文档再到单词的文本生成过程。潜在变量是我们不能从数据中直接观察到的变量。然而，它揭示了数据中隐藏的结构，并且有利于建立概率模型。第一个潜在变量是θ，即主题在每个文档中的分布(40%“水果”，60%“蔬菜”)。第二个潜在变量是Z (Z ∈{1，2..T})，呈现每个词的主题。我们可以在下图中看到文本生成过程:</p><figure class="oa ob oc od gt iv gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/fc8ec8abbdb6ee7e27fdc01abf0e4ed5.png" data-original-src="https://miro.medium.com/v2/resize:fit:822/format:webp/1*GU8mJ9aAyBepcRkdNa4n3Q.png"/></div><p class="oe of gj gh gi og oh bd b be z dk translated">LDA中的文本生成。作者图片</p></figure><p id="f06b" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">对于每个文档d，都有一个主题分布θ_d。对于文档d (w_d i)中的每个单词I，它是基于主题分布θ_d、该单词di的主题Z以及主题Z_di上的单词分布来生成的。假设文档d中的第一个单词是“green”，它是通过首先指定一个主题分布生成的:60%“水果”，40%“蔬菜”(θ_d)。然后对于第一个单词，我们对主题“蔬菜”(Z_d1)进行采样。在主题“蔬菜”中，我们从主题分布中抽取单词“绿色”(w_d1)。</p><p id="6afa" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">在数学上，该过程可以被指定为下面的贝叶斯方程:</p><figure class="oa ob oc od gt iv gh gi paragraph-image"><div class="gh gi ok"><img src="../Images/e6a8062f790f64688e336d5c21ec8cee.png" data-original-src="https://miro.medium.com/v2/resize:fit:712/0*pI9fsVHSoUOs_bWB"/></div></figure><p id="e21c" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">结合潜在变量，可以在下面的图表和步骤中查看层次结构，以便更好地理解:</p><figure class="oa ob oc od gt iv gh gi paragraph-image"><div class="gh gi ol"><img src="../Images/0f926e15b3e7ec1e01baeb98240cf3f1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1232/format:webp/1*Af3fnlV_Q81sAmv51g0PSA.png"/></div><p class="oe of gj gh gi og oh bd b be z dk translated">作者图片</p></figure><p id="5c01" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">通过定义P(θ_d)，P(Z_dn|θ_d)，P(w_dn|z_dn)的概率分布，我们可以计算出联合分布P(w，Z，θ)，这是我们所拥有的观察语料库的概率。</p><p id="4b41" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">LDA中的狄利克雷来自于θ_d服从<a class="ae ma" href="https://en.wikipedia.org/wiki/Dirichlet_distribution" rel="noopener ugc nofollow" target="_blank">狄利克雷分布</a>。给定训练模型的语料库，我们可以使用EM算法进行参数估计，或者使用MCMC进行完全贝叶斯推理。本文将不涉及该模型的数学细节。如果有兴趣，可以参考2003年Blei，Ng，Jordan的论文。而且，<code class="fe om on oo op b">gensim</code>是在主题建模中应用LDA的常用Python库。参考<a class="ae ma" href="https://radimrehurek.com/gensim/" rel="noopener ugc nofollow" target="_blank">这篇文档</a>了解更多细节，我也会在下一篇讨论应用时展示。</p></div><div class="ab cl mg mh hx mi" role="separator"><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml"/></div><div class="im in io ip iq"><p id="d0e9" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">这都是为了这篇文章。在本系列的最后一篇文章中，我将比较和讨论K-Means和LDA在主题建模中的区别，并展示一个使用Python库在主题建模中应用这两种算法的例子。</p><p id="0b7a" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">感谢您的阅读！这是我所有博客帖子的列表。如果你感兴趣，可以去看看。</p><div class="is it gp gr iu oq"><a href="https://zzhu17.medium.com/my-blog-posts-gallery-ac6e01fe5cc3" rel="noopener follow" target="_blank"><div class="or ab fo"><div class="os ab ot cl cj ou"><h2 class="bd jf gy z fp ov fr fs ow fu fw jd bi translated">我的博客文章库</h2><div class="ox l"><h3 class="bd b gy z fp ov fr fs ow fu fw dk translated">我快乐的地方</h3></div><div class="oy l"><p class="bd b dl z fp ov fr fs ow fu fw dk translated">zzhu17.medium.com</p></div></div><div class="oz l"><div class="pa l pb pc pd oz pe ja oq"/></div></div></a></div><div class="is it gp gr iu oq"><a href="https://zzhu17.medium.com/membership" rel="noopener follow" target="_blank"><div class="or ab fo"><div class="os ab ot cl cj ou"><h2 class="bd jf gy z fp ov fr fs ow fu fw jd bi translated">阅读朱(以及媒体上成千上万的其他作家)的每一个故事</h2><div class="ox l"><h3 class="bd b gy z fp ov fr fs ow fu fw dk translated">作为一个媒体会员，你的会员费的一部分会给你阅读的作家，你可以完全接触到每一个故事…</h3></div><div class="oy l"><p class="bd b dl z fp ov fr fs ow fu fw dk translated">zzhu17.medium.com</p></div></div><div class="oz l"><div class="pf l pb pc pd oz pe ja oq"/></div></div></a></div><h2 id="3df0" class="mn mo je bd mp mq mr dn ms mt mu dp mv lc mw mx my lg mz na nb lk nc nd ne nf bi translated">参考</h2><p id="c5ac" class="pw-post-body-paragraph kt ku je kv b kw ng kf ky kz nh ki lb lc ni le lf lg nj li lj lk nk lm ln lo im bi translated"><em class="lp">[1]大卫·布雷、安德鲁·吴和迈克尔·乔丹。2003.潜在狄利克雷分配。j .马赫。学习。第3号决议，无效(2003年1月3日)，第993-1022号决议。</em></p></div></div>    
</body>
</html>