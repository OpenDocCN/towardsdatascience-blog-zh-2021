<html>
<head>
<title>The Ultimate Guide to Functional Programming for Big Data</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">大数据函数式编程终极指南</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/the-ultimate-guide-to-functional-programming-for-big-data-1e57b0d225a3?source=collection_archive---------12-----------------------#2021-07-11">https://towardsdatascience.com/the-ultimate-guide-to-functional-programming-for-big-data-1e57b0d225a3?source=collection_archive---------12-----------------------#2021-07-11</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="2c35" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">纯函数和惰性求值——分布式数据计算的症结</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/b96d90f647e302b3f4efd243d5348f93.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*NTuj6JBVxCzmI7Ln"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">纳斯蒂亚·杜尔希尔在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="2e52" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">Apache Spark已经成为当今大数据领域最常用的工具。它能够单独运行代码，将API扩展到Python、Scala、Java和更多工具。它可以用来查询数据集，其架构中最令人振奋的部分是能够对实时流数据进行分析，而无需显式地将其存储在任何地方。Spark源于Scala，被设计为一个分布式集群计算软件框架。从资源管理、多线程、任务分配到逻辑的实际运行，Spark无所不包。从最终用户的角度来看，它是一个分析工具，可以在几分钟内输入大量数据并得出所需的分析。但是，Spark是如何实现的呢？使用Spark处理大型数据集的一些核心原则是什么？</p><p id="1d0a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">要了解Spark的基础知识、其架构以及在大数据和云世界中的实施，请参考下面链接的故事。</p><div class="lv lw gp gr lx ly"><a href="https://medium.com/geekculture/unleash-the-spark-from-your-data-ba3227755e" rel="noopener follow" target="_blank"><div class="lz ab fo"><div class="ma ab mb cl cj mc"><h2 class="bd iu gy z fp md fr fs me fu fw is bi translated">释放数据的火花</h2><div class="mf l"><h3 class="bd b gy z fp md fr fs me fu fw dk translated">Apache Spark如何改变大数据游戏？</h3></div><div class="mg l"><p class="bd b dl z fp md fr fs me fu fw dk translated">medium.com</p></div></div><div class="mh l"><div class="mi l mj mk ml mh mm ks ly"/></div></div></a></div><h1 id="9a31" class="mn mo it bd mp mq mr ms mt mu mv mw mx jz my ka mz kc na kd nb kf nc kg nd ne bi translated">为什么是函数式编程？</h1><p id="8534" class="pw-post-body-paragraph kz la it lb b lc nf ju le lf ng jx lh li nh lk ll lm ni lo lp lq nj ls lt lu im bi translated">Spark是一种使用Scala开发的语言，Scala确实是一种函数式编程语言。函数式编程(通常缩写为FP)是通过积累纯函数、避免共享状态、可变数据和副作用来编写代码的过程。函数式编程是声明性的而不是命令性的，完整的应用程序流过不存储或修改输入的函数。另一方面，Python是一种过程编程语言。</p><p id="b97f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们看一个例子来理解什么是过程化编程，以及为什么函数式编程对于在分布式系统上运行更有意义。</p><pre class="kj kk kl km gt nk nl nm nn aw no bi"><span id="dc6d" class="np mo it nl b gy nq nr l ns nt"># Declare a list of Soccer Players<br/>players_log = ["Messi",<br/>"Ronaldo",<br/>"Maradona",<br/>"Messi",<br/>"Lewandowski",<br/>"Mbappe",<br/>"Ronaldo",<br/>"Messi",<br/>"Neymar"]</span><span id="b8e0" class="np mo it nl b gy nu nr l ns nt"># Variable to hold the count<br/>player_count = 0</span><span id="a1a9" class="np mo it nl b gy nu nr l ns nt"># Function to count number of times a player's name occurred<br/>def count_occurences(player_name):<br/>  global player_count<br/>  for player in players_log:<br/>    if player == player_name:<br/>      player_count = player_count + 1<br/>  return player_count</span><span id="36cc" class="np mo it nl b gy nu nr l ns nt"># First Run<br/>count_occurences("Messi")<br/>&gt;&gt;&gt; 3</span><span id="b395" class="np mo it nl b gy nu nr l ns nt"># Second Run<br/>count_occurences("Messi")<br/>&gt;&gt;&gt; 6</span></pre><p id="c1fd" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在上面的例子中我们观察到了什么？我们看到，当程序运行多次时，样本变量的计数不断变化。这个答案是不正确的，因为“梅西”在列表中出现的次数只有3次。</p><blockquote class="nv nw nx"><p id="ca0e" class="kz la ny lb b lc ld ju le lf lg jx lh nz lj lk ll oa ln lo lp ob lr ls lt lu im bi translated"><strong class="lb iu"> —为什么会这样？发生这种情况是因为我们调用了一个gloabl变量来存储玩家数量，而程序继续修改这个变量的值。<br/> <strong class="lb iu"> — </strong> <strong class="lb iu">可能的解决方案是什么？</strong>为了摆脱这一点，我们可以使用一个不实现全局变量的函数。下面我们来看一个实现。</strong></p></blockquote><pre class="kj kk kl km gt nk nl nm nn aw no bi"><span id="68dc" class="np mo it nl b gy nq nr l ns nt">def count_occurences(player_name, player_count):<br/>    for player in players_log:<br/>        if player == player_name:<br/>            player_count = player_count + 1<br/>    return player_count</span></pre><p id="09fb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在，每次这个函数被调用时，<em class="ny"> `player_count` </em>变量将被重置，它将总是给出准确的结果。</p><p id="eb06" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在并行系统上运行这段代码会发生什么？<br/> 在并行计算环境中，Spark将数据拆分到多台机器上。让我们想象一下，这个例子中玩家的输入是10，000，这个列表被拆分成15个系统。如果在上面的场景中实现这个逻辑，那么第一个系统必须在第二个系统启动之前完成它的执行，否则输出将被错误地组合。但是，如果机器不能同时工作，那么并行计算范例就失败了。</p><p id="c49e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为了解决程序代码的这个问题，我们在Spark中使用函数式编程。但是，在开始使用函数式代码之前，我们首先需要了解一些先决条件，即<strong class="lb iu">纯函数和惰性求值。</strong></p><p id="b43c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">那么，函数式编程将如何解决这个问题呢？看下面。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oc"><img src="../Images/bb7e2c4cf3edda0d9d5982ac66656676.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uwC0VrBz7Syl8tg6E3pJSw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">使用函数式编程方法运行相同程序的伪逻辑[Map — Shuffle — Reduce] |作者图片</p></figure><h1 id="78fd" class="mn mo it bd mp mq mr ms mt mu mv mw mx jz my ka mz kc na kd nb kf nc kg nd ne bi translated">纯函数及其在分布式系统中的应用</h1><p id="1476" class="pw-post-body-paragraph kz la it lb b lc nf ju le lf ng jx lh li nh lk ll lm ni lo lp lq nj ls lt lu im bi translated">编程术语中的纯函数，顾名思义，就是一类不改变给它们的输入的函数。在编程范例中，有三个关键特性定义了什么是纯函数。但是，首先让我们了解一下纯函数的起源。</p><blockquote class="od"><p id="84b1" class="oe of it bd og oh oi oj ok ol om lu dk translated">每个数学函数都是纯函数</p></blockquote><p id="f25e" class="pw-post-body-paragraph kz la it lb b lc on ju le lf oo jx lh li op lk ll lm oq lo lp lq or ls lt lu im bi translated">起源就是这么简单。所有的数学函数都是纯函数。它们接受输入，执行给定的操作并生成输出。没有中间步骤，没有对函数体中的任何变量进行修改或更改。请看下面一个乘以2的数学函数的例子。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi os"><img src="../Images/89e3c6a1b9df95bb3d074f0a158f2c7e.png" data-original-src="https://miro.medium.com/v2/resize:fit:792/format:webp/1*v9t-0U_D43YqgnbEyEagmQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">乘法运算|作者图片</p></figure><p id="2cdb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在同样的意义上，对于编程语言来说，纯功能的理论仍然是一样的。我们可以在下面的Scala代码中操作上面提到的相同逻辑。</p><pre class="kj kk kl km gt nk nl nm nn aw no bi"><span id="c674" class="np mo it nl b gy nq nr l ns nt"># Scala code to multiply by 2.<br/>def multiplyByTwo(i: Int): Int = {i*2}</span></pre><blockquote class="od"><p id="85dc" class="oe of it bd og oh ot ou ov ow ox lu dk translated">因为分布式系统同时运行许多函数，所以确保一个函数的执行不会对其作用域之外的变量产生副作用是很重要的。</p></blockquote><ul class=""><li id="a6c6" class="oy oz it lb b lc on lf oo li pa lm pb lq pc lu pd pe pf pg bi translated">输入要决定输出，输入的来源只有一个。不需要外部文件引用或额外的库。输入被实例化为函数，并产生输出。<strong class="lb iu">只要传递的输入参数的值相同，输出就永远不会改变。</strong></li><li id="9a41" class="oy oz it lb b lc ph lf pi li pj lm pk lq pl lu pd pe pf pg bi translated">该函数不会更改其输入，这意味着原始输入的值永远不会被修改。所有代码都在原始输入值的<strong class="lb iu">副本上运行。</strong></li><li id="4d2b" class="oy oz it lb b lc ph lf pi li pj lm pk lq pl lu pd pe pf pg bi translated">该函数除了计算输出之外不做任何事情。不需要读取任何文件，绝对不需要依赖函数范围之外的任何东西。<strong class="lb iu">接受输入，计算它，并返回输出。如果这个函数做了其他的事情，这就叫做副作用。</strong>而且纯函数应该是永远不会产生副作用的。</li></ul><p id="f55c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">纯函数法则:</strong></p><blockquote class="nv nw nx"><p id="469d" class="kz la ny lb b lc ld ju le lf lg jx lh nz lj lk ll oa ln lo lp ob lr ls lt lu im bi translated">1.函数的输入单独决定了它的输出。</p><p id="0ef0" class="kz la ny lb b lc ld ju le lf lg jx lh nz lj lk ll oa ln lo lp ob lr ls lt lu im bi translated">2.在函数的整个生命周期中，它不会改变输入。</p><p id="4aa9" class="kz la ny lb b lc ld ju le lf lg jx lh nz lj lk ll oa ln lo lp ob lr ls lt lu im bi translated">3.该函数不会产生意外的副作用。</p></blockquote><p id="0ed6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在让我们转向理解函数式编程的下一个重要概念——惰性求值。</p><h1 id="1e24" class="mn mo it bd mp mq mr ms mt mu mv mw mx jz my ka mz kc na kd nb kf nc kg nd ne bi translated">懒惰评估的概念</h1><p id="4b90" class="pw-post-body-paragraph kz la it lb b lc nf ju le lf ng jx lh li nh lk ll lm ni lo lp lq nj ls lt lu im bi translated">想象一下在数百个系统上同时运行的纯函数。请记住，所有这些纯函数的执行或输出都不能依赖于外部变量。如果一个业务逻辑需要在一个程序中完成10个任务，Spark将为这个任务创建10个纯函数。这也意味着所有10个输入的内存分配将是额外的，并且对于程序来说将变得昂贵，因为所有的纯函数都需要输入的副本。</p><blockquote class="od"><p id="b170" class="oe of it bd og oh oi oj ok ol om lu dk translated">Spark将单个任务划分到几个子功能中，这些子功能都是纯功能。</p></blockquote><figure class="pn po pp pq pr kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pm"><img src="../Images/f3b181f48f00d032ff9f3f373a186486.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qRHCVDgYafMcuRWBwML6OQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">Spark把主逻辑分成几个面向小运算的纯函数。每个纯函数都包含其输入参数| Image by Author的副本</p></figure><p id="df2b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">但是，这些单独的纯函数必然会占用大量内存来存储它们的输入参数。这可能会导致“内存不足”的错误。需要惰性求值来将程序从这些内存不足的错误中拯救出来。</p><blockquote class="nv nw nx"><p id="1345" class="kz la ny lb b lc ld ju le lf lg jx lh nz lj lk ll oa ln lo lp ob lr ls lt lu im bi translated">在Spark对该程序做任何事情之前，它会一步一步地指出需要什么功能和数据。这形成了懒惰评估的基础，并被称为<strong class="lb iu">有向无环图(DAG) </strong></p></blockquote><h1 id="9cc4" class="mn mo it bd mp mq mr ms mt mu mv mw mx jz my ka mz kc na kd nb kf nc kg nd ne bi translated">Spark中的有向无环图</h1><p id="61c7" class="pw-post-body-paragraph kz la it lb b lc nf ju le lf ng jx lh li nh lk ll lm ni lo lp lq nj ls lt lu im bi translated">与现代世界的大多数其他编程范例类似，DAG是一个图。它是一个图表，包含最终将应用于输入数据的轨迹或一系列操作。</p><ul class=""><li id="77b2" class="oy oz it lb b lc ld lf lg li ps lm pt lq pu lu pd pe pf pg bi translated"><strong class="lb iu"> Directed: </strong>每个操作都直接从一个节点连接到另一个节点。这些串联连接创建了一系列操作，其中旧节点在执行上顺序领先于新节点。</li><li id="a9d8" class="oy oz it lb b lc ph lf pi li pj lm pk lq pl lu pd pe pf pg bi translated"><strong class="lb iu">非循环的:</strong>在整个函数过程中发生的操作不是在一个循环中，这意味着，它离开后的一个状态永远不能恢复。</li><li id="569c" class="oy oz it lb b lc ph lf pi li pj lm pk lq pl lu pd pe pf pg bi translated"><strong class="lb iu"> Graph: </strong>从图论来说，它声明了一个顺序连接的<em class="ny">顶点</em>和<em class="ny">边</em>的组合。</li></ul><h2 id="4947" class="np mo it bd mp pv pw dn mt px py dp mx li pz qa mz lm qb qc nb lq qd qe nd qf bi translated">Spark如何构建DAG？</h2><p id="3218" class="pw-post-body-paragraph kz la it lb b lc nf ju le lf ng jx lh li nh lk ll lm ni lo lp lq nj ls lt lu im bi translated">基于所请求的根功能的性质，Spark通常按照以下步骤为任何给定的任务创建DAG:</p><ol class=""><li id="3f7b" class="oy oz it lb b lc ld lf lg li ps lm pt lq pu lu qg pe pf pg bi translated">用户提交一个Spark应用程序，驱动程序模块从Spark会话中接收这个请求。</li><li id="e5da" class="oy oz it lb b lc ph lf pi li pj lm pk lq pl lu qg pe pf pg bi translated">然后，驱动程序根据请求执行几项任务，帮助识别应用程序所需的转换和操作。</li><li id="a1e8" class="oy oz it lb b lc ph lf pi li pj lm pk lq pl lu qg pe pf pg bi translated">这些被识别的操作然后被安排在小的纯函数的逻辑流中，这种安排是DAG。</li></ol><p id="bae2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然后，DAG计划被转换为包含几个阶段的物理执行(下面显示了一个示例)。Spark完成的转换可以分为两大类:</p><ul class=""><li id="0ca1" class="oy oz it lb b lc ld lf lg li ps lm pt lq pu lu pd pe pf pg bi translated"><strong class="lb iu">窄变换</strong> —像map()和filter()这样的过程是窄变换。这些不需要跨分区的数据洗牌。</li><li id="c591" class="oy oz it lb b lc ph lf pi li pj lm pk lq pl lu pd pe pf pg bi translated"><strong class="lb iu">大范围转换</strong>—reduce by key之类的过程是大范围转换，因为它们需要通过分区进行数据洗牌。由于数据被混洗，结果显示在阶段边界上，不会干扰下一个纯函数的输入。</li></ul><p id="35c5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">DAG调度程序创建的所有任务都被连接起来，并通过捆绑包发送到运行该程序的集群。这标志着物理执行计划的完成。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi qh"><img src="../Images/470dce4be6aa9bf670c324460b9dd038.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oxJpJ4BDltJ_U5Damhhgkw@2x.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">Dag是给Spark功能代码的一系列指令，这些代码在惰性评估|作者图片期间运行</p></figure><p id="4bd4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">DAG旨在改进经典的Hadoop MapReduce操作，并为开发人员提供更大的灵活性。它允许一次执行多个查询，而Hadoop只支持两个查询(map和reduce)。DAG还允许运行SQL查询，具有高度容错性，并且比MapReduce更优化。</p><h1 id="c77c" class="mn mo it bd mp mq mr ms mt mu mv mw mx jz my ka mz kc na kd nb kf nc kg nd ne bi translated">在Spark中使用惰性评估的优势</h1><ol class=""><li id="5206" class="oy oz it lb b lc nf lf ng li qi lm qj lq qk lu qg pe pf pg bi translated"><strong class="lb iu">增加可管理性:</strong>当开发人员可以创建小的操作时，组织大的逻辑变得容易。它还通过分组操作减少了数据的传递次数。</li><li id="56e3" class="oy oz it lb b lc ph lf pi li pj lm pk lq pl lu qg pe pf pg bi translated"><strong class="lb iu">降低复杂度:</strong>编程中两个主要的运算复杂度是<strong class="lb iu"> <em class="ny">时间</em> </strong>和<strong class="lb iu"> <em class="ny">空间</em></strong>Spark可以克服这两个。因为所有的执行不会同时发生，所以节省了时间。该操作仅在需要数据时触发，因此空间仅在必要时使用，这样可以节省空间。</li><li id="a0b1" class="oy oz it lb b lc ph lf pi li pj lm pk lq pl lu qg pe pf pg bi translated"><strong class="lb iu">节省计算能力，提高速度:</strong> Spark函数通过驱动程序触发，在集群上运行。由于惰性求值仅在绝对必要时触发计算，它节省了驱动程序和集群之间不必要的行程，因此节省了功率并加快了执行过程，因为单个操作总是很小的。</li><li id="cc64" class="oy oz it lb b lc ph lf pi li pj lm pk lq pl lu qg pe pf pg bi translated"><strong class="lb iu">优化:</strong>在给定的时间内，运行的查询数量非常少，因此查询会得到优化。</li></ol><h1 id="06e5" class="mn mo it bd mp mq mr ms mt mu mv mw mx jz my ka mz kc na kd nb kf nc kg nd ne bi translated">惰性求值和函数式编程的应用</h1><p id="8b9b" class="pw-post-body-paragraph kz la it lb b lc nf ju le lf ng jx lh li nh lk ll lm ni lo lp lq nj ls lt lu im bi translated">让我们在相同的玩家名字列表上工作，通过下面的代码在Spark任务中练习PySpark和Lazy Evaluation。</p><pre class="kj kk kl km gt nk nl nm nn aw no bi"><span id="20a3" class="np mo it nl b gy nq nr l ns nt"># Import PySpark and create a sample App<br/>import pyspark<br/>sc = pyspark.SparkContext(appName="maps_and_lazy_evaluation_sample")</span><span id="b9a4" class="np mo it nl b gy nu nr l ns nt">players_log = ["Messi",<br/>"Ronaldo",<br/>"Maradona",<br/>"Messi",<br/>"Lewandowski",<br/>"Mbappe",<br/>"Ronaldo",<br/>"Messi",<br/>"Neymar"]</span><span id="8434" class="np mo it nl b gy nu nr l ns nt"># parallelize the log_of_players to use with Spark. The parallelize function distributes the input to multiple Spark clusters<br/>distributed_player_log = sc.parallelize(players_log)</span><span id="9374" class="np mo it nl b gy nu nr l ns nt"># Use the Map() Function to convert all names to lowercase<br/>distributed_player_log.map(convert_to_lowercase)<br/><strong class="nl iu">&gt;&gt;&gt; PythonRDD[1] at RDD at PythonRDD.scala:53</strong></span></pre><p id="d681" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">上面的map()函数几乎立即运行。这就是<strong class="lb iu">懒评的魔力。</strong> Spark实际上还没有执行这个映射。它会等到最后一刻才执行映射，如果不必要的话就不会执行。上面输出中的“RDD”指的是弹性分布式数据集(RDD)。这些是分布在运行集群中的容错数据集。这就是Spark存储数据的方式。为了让Spark实际运行map步骤，我们需要调用一个<strong class="lb iu">“动作”。</strong>一个可用的操作是collect方法。<strong class="lb iu"> collect()方法</strong>从所有正在运行的集群中获取结果，并将它们合并到主节点上的一个列表中。</p><pre class="kj kk kl km gt nk nl nm nn aw no bi"><span id="31ec" class="np mo it nl b gy nq nr l ns nt">distributed_player_log.map(convert_to_lowercase).collect()<br/><strong class="nl iu">&gt;&gt;&gt; ['messi',  'ronaldo',  'maradona',  'messi',  'lewandowski',  'mbappe',  'ronaldo',  'messi',  'neymar']</strong></span><span id="1d48" class="np mo it nl b gy nu nr l ns nt"># Running collect with a traditional Python Functional Programming technique [ Lambda Functions ]</span><span id="d905" class="np mo it nl b gy nu nr l ns nt">distributed_player_log.map(lambda x: x.lower()).collect()<br/><strong class="nl iu">&gt;&gt;&gt; ['messi',  'ronaldo',  'maradona',  'messi',  'lewandowski',  'mbappe',  'ronaldo',  'messi',  'neymar']</strong></span></pre><p id="4a94" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">要参考本文中提到的完整代码，请访问下面提到的链接:</p><div class="lv lw gp gr lx ly"><a href="https://github.com/rjrahul24/spark-ud-2002/blob/master/Functional_V_S_Procedural_Programming.ipynb" rel="noopener  ugc nofollow" target="_blank"><div class="lz ab fo"><div class="ma ab mb cl cj mc"><h2 class="bd iu gy z fp md fr fs me fu fw is bi translated">rjrahul24/spark-ud-2002</h2><div class="mf l"><h3 class="bd b gy z fp md fr fs me fu fw dk translated">项目内容是MapReduce和Spark上的应用。该存储库包含基于Udacity的Spark的项目…</h3></div><div class="mg l"><p class="bd b dl z fp md fr fs me fu fw dk translated">github.com</p></div></div><div class="mh l"><div class="ql l mj mk ml mh mm ks ly"/></div></div></a></div></div><div class="ab cl qm qn hx qo" role="separator"><span class="qp bw bk qq qr qs"/><span class="qp bw bk qq qr qs"/><span class="qp bw bk qq qr"/></div><div class="im in io ip iq"><h1 id="a744" class="mn mo it bd mp mq qt ms mt mu qu mw mx jz qv ka mz kc qw kd nb kf qx kg nd ne bi translated">结论</h1><p id="a98a" class="pw-post-body-paragraph kz la it lb b lc nf ju le lf ng jx lh li nh lk ll lm ni lo lp lq nj ls lt lu im bi translated">Spark是处理海量数据的健壮框架。它现在也能够执行机器学习任务，可以托管在所有云平台上。本文旨在介绍函数式编程的概念，以及它如何以积极的方式影响大数据系统，比过程化代码更好。Spark比我们在本教程中看到的要复杂得多。请务必参考链接的代码库，查看实现这些概念的笔记本。有关Spark和函数式编程的更多信息，请参考“参考”一节中的链接。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi qy"><img src="../Images/5add992761db4650e1bf013b51f0bf45.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*67hm2g_nzTZoBgpG"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">Joel Filipe 在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><h2 id="d8f9" class="np mo it bd mp pv pw dn mt px py dp mx li pz qa mz lm qb qc nb lq qd qe nd qf bi translated">关于我</h2><p id="886b" class="pw-post-body-paragraph kz la it lb b lc nf ju le lf ng jx lh li nh lk ll lm ni lo lp lq nj ls lt lu im bi translated">我是Rahul，目前在研究人工智能，在Xbox游戏上实现大数据分析。我在微软工作。除了专业工作之外，我还试图制定一个程序，来理解如何通过使用人工智能来改善世界上发展中国家的经济状况。</p><p id="d60e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我现在在纽约的哥伦比亚大学，你可以通过LinkedIn和Twitter与我联系。</p></div><div class="ab cl qm qn hx qo" role="separator"><span class="qp bw bk qq qr qs"/><span class="qp bw bk qq qr qs"/><span class="qp bw bk qq qr"/></div><div class="im in io ip iq"><h2 id="b6e9" class="np mo it bd mp pv pw dn mt px py dp mx li pz qa mz lm qb qc nb lq qd qe nd qf bi translated">[参考文献]</h2><ol class=""><li id="7498" class="oy oz it lb b lc nf lf ng li qi lm qj lq qk lu qg pe pf pg bi translated"><a class="ae ky" href="https://www.udacity.com/course/learn-spark-at-udacity--ud2002" rel="noopener ugc nofollow" target="_blank">https://www.udacity.com/course/learn-spark-at-udacity-ud 2002</a></li><li id="0a43" class="oy oz it lb b lc ph lf pi li pj lm pk lq pl lu qg pe pf pg bi translated"><a class="ae ky" href="https://stackoverflow.com/questions/57050728/count-occurrences-of-a-list-of-substrings-in-a-pyspark-df-column" rel="noopener ugc nofollow" target="_blank">https://stack overflow . com/questions/57050728/count-occurrences-of-a-list-of-substrings-in-a-py spark-df-column</a></li><li id="f5ea" class="oy oz it lb b lc ph lf pi li pj lm pk lq pl lu qg pe pf pg bi translated">https://en.wikipedia.org/wiki/Functional_programming<a class="ae ky" href="https://en.wikipedia.org/wiki/Functional_programming" rel="noopener ugc nofollow" target="_blank"/></li><li id="22f4" class="oy oz it lb b lc ph lf pi li pj lm pk lq pl lu qg pe pf pg bi translated"><a class="ae ky" href="https://www.geeksforgeeks.org/functional-programming-paradigm/" rel="noopener ugc nofollow" target="_blank">https://www . geeks forgeeks . org/functional-programming-paradigm/</a></li><li id="83d1" class="oy oz it lb b lc ph lf pi li pj lm pk lq pl lu qg pe pf pg bi translated"><a class="ae ky" href="https://spark.apache.org/" rel="noopener ugc nofollow" target="_blank">https://spark.apache.org/</a></li><li id="8c13" class="oy oz it lb b lc ph lf pi li pj lm pk lq pl lu qg pe pf pg bi translated"><a class="ae ky" href="https://spark.apache.org/docs/latest/api/python/_modules/pyspark/sql/functions.html" rel="noopener ugc nofollow" target="_blank">https://spark . Apache . org/docs/latest/API/python/_ modules/py spark/SQL/functions . html</a></li></ol></div></div>    
</body>
</html>