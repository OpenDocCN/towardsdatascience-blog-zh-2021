<html>
<head>
<title>Feature selection in Scikit-learn</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Scikit-learn中的功能选择</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/feature-selection-in-scikit-learn-dc005dcf38b7?source=collection_archive---------14-----------------------#2021-12-13">https://towardsdatascience.com/feature-selection-in-scikit-learn-dc005dcf38b7?source=collection_archive---------14-----------------------#2021-12-13</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/cafdb4d53c4fbc45db5b06163b3e761b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*PrHExWptvaFD2Zy8"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">由<a class="ae jg" href="https://unsplash.com/@mattpaul?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">马特·保罗·卡塔拉诺</a>在<a class="ae jg" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><h2 id="bc25" class="jh ji jj bd b dl jk jl jm jn jo jp dk jq translated" aria-label="kicker paragraph">Python中的机器学习</h2><div class=""/><div class=""><h2 id="1011" class="pw-subtitle-paragraph kp js jj bd b kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg dk translated">过滤特征的简单方法，以获得更简单、更快速的模型</h2></div><p id="d96d" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">当建立监督机器学习模型时，我们收集对预测结果潜在有用的特征。并不是所有的特性都有助于构建模型。过滤掉不可预测的特征并保持模型的精简性通常是可行的，这样模型更快，更容易向利益相关者解释，并且更容易生产。在本帖中，我们将学习一些简单的方法来剔除对预测结果没有帮助的特征，并选择贡献更大的特征。</p><figure class="me mf mg mh gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi md"><img src="../Images/d2651c11d0a156d69721d5404c9a8938.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*NbOr43AtZ7MCh16b"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">照片由<a class="ae jg" href="https://unsplash.com/@timmarshall?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">蒂姆·马歇尔</a>在<a class="ae jg" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>拍摄</p></figure></div><div class="ab cl mi mj hx mk" role="separator"><span class="ml bw bk mm mn mo"/><span class="ml bw bk mm mn mo"/><span class="ml bw bk mm mn"/></div><div class="im in io ip iq"><h1 id="2edd" class="mp mq jj bd mr ms mt mu mv mw mx my mz ky na kz nb lb nc lc nd le ne lf nf ng bi translated">📦 1.数据</h1><p id="fad0" class="pw-post-body-paragraph lh li jj lj b lk nh kt lm ln ni kw lp lq nj ls lt lu nk lw lx ly nl ma mb mc im bi translated">让我们首先加载必要的库，导入一个样本玩具数据，并将其划分为训练和测试数据集:</p><pre class="me mf mg mh gt nm nn no np aw nq bi"><span id="f83f" class="nr mq jj nn b gy ns nt l nu nv">import pandas as pd<br/>pd.options.display.max_rows = 20<br/># Used Scikit-learn version 1.0.1<br/>from sklearn.datasets import load_breast_cancer<br/>from sklearn.model_selection import train_test_split<br/>from sklearn.ensemble import GradientBoostingClassifier<br/>from sklearn.feature_selection import (RFE, SelectKBest, <br/>                                       SelectPercentile)<br/>from sklearn.metrics import roc_auc_score</span><span id="b2fc" class="nr mq jj nn b gy nw nt l nu nv"># Load data<br/>cancer = load_breast_cancer(as_frame=True)<br/>X = cancer['data']<br/>print(f"Features shape: {X.shape}")<br/>y = cancer['target']<br/>print(f"Target shape: {y.shape}\n")</span><span id="4488" class="nr mq jj nn b gy nw nt l nu nv"># Partition data<br/>X_train, X_test, y_train, y_test = train_test_split(<br/>    X, y, test_size=0.2, random_state=42<br/>)<br/>print(f"Training data - Features shape: {X_train.shape}")<br/>print(f"Training data - Target shape: {y_train.shape}\n")<br/>print(f"Test data - Features shape: {X_test.shape}")<br/>print(f"Test data - Target shape: {y_test.shape}")</span></pre><figure class="me mf mg mh gt iv gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/64c4fafe3907b2e1d04291ef2dfcc8d1.png" data-original-src="https://miro.medium.com/v2/resize:fit:804/format:webp/1*Ux81vnm2NEIqcMSUVHBRJA.png"/></div></figure><p id="48b7" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">我们将使用有30个特征的乳腺癌数据。</p></div><div class="ab cl mi mj hx mk" role="separator"><span class="ml bw bk mm mn mo"/><span class="ml bw bk mm mn mo"/><span class="ml bw bk mm mn"/></div><div class="im in io ip iq"><h1 id="483a" class="mp mq jj bd mr ms mt mu mv mw mx my mz ky na kz nb lb nc lc nd le ne lf nf ng bi translated">💻 2.基线模型</h1><p id="6413" class="pw-post-body-paragraph lh li jj lj b lk nh kt lm ln ni kw lp lq nj ls lt lu nk lw lx ly nl ma mb mc im bi translated">我们将使用ROC AUC(从这里开始的AUC)来评估模型。让我们通过使用所有特性构建一个简单的模型来了解基准性能:</p><pre class="me mf mg mh gt nm nn no np aw nq bi"><span id="b6af" class="nr mq jj nn b gy ns nt l nu nv">model0 = GradientBoostingClassifier(random_state=42)<br/>model0.fit(X_train, y_train)</span><span id="f0c3" class="nr mq jj nn b gy nw nt l nu nv">def get_roc_auc(model, X, y):<br/>    y_proba = model.predict_proba(X)[:,1]<br/>    return roc_auc_score(y, y_proba)</span><span id="794d" class="nr mq jj nn b gy nw nt l nu nv">print(f"Training data - ROC AUC: {get_roc_auc(model0, X_train, y_train):.4f}")<br/>print(f"Test data - ROC AUC: {get_roc_auc(model0, X_test, y_test):.4f}")</span></pre><figure class="me mf mg mh gt iv gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/dd07bdec83aede02e20b78fbf2d4e2f6.png" data-original-src="https://miro.medium.com/v2/resize:fit:618/format:webp/1*y7Ev9zuEzHzUKYeD6fuCKw.png"/></div></figure><p id="363b" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">测试数据集的AUC为0.9951。</p></div><div class="ab cl mi mj hx mk" role="separator"><span class="ml bw bk mm mn mo"/><span class="ml bw bk mm mn mo"/><span class="ml bw bk mm mn"/></div><div class="im in io ip iq"><h1 id="d737" class="mp mq jj bd mr ms mt mu mv mw mx my mz ky na kz nb lb nc lc nd le ne lf nf ng bi translated">📍 3.特征选择</h1><p id="9831" class="pw-post-body-paragraph lh li jj lj b lk nh kt lm ln ni kw lp lq nj ls lt lu nk lw lx ly nl ma mb mc im bi translated">我们将着眼于五种不同的方法来为有监督的机器学习问题进行特征选择。</p><h2 id="1680" class="nr mq jj bd mr nz oa dn mv ob oc dp mz lq od oe nb lu of og nd ly oh oi nf jp bi translated">📍 3.1.从要素重要性中过滤</h2><p id="0adf" class="pw-post-body-paragraph lh li jj lj b lk nh kt lm ln ni kw lp lq nj ls lt lu nk lw lx ly nl ma mb mc im bi translated">要素重要性显示了每个要素对预测的贡献程度。进行特征选择的一个简单方法是删除对模型贡献不大的特征。</p><pre class="me mf mg mh gt nm nn no np aw nq bi"><span id="c8a2" class="nr mq jj nn b gy ns nt l nu nv">imp = pd.DataFrame(model0.feature_importances_, index=X.columns, <br/>                   columns=['importance'])\<br/>        .sort_values('importance', ascending=False)<br/>imp</span></pre><figure class="me mf mg mh gt iv gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/691005fb944bac8ff4728415728e435d.png" data-original-src="https://miro.medium.com/v2/resize:fit:570/format:webp/1*vTQIHZmFJwsXTHoHvbLTVw.png"/></div></figure><p id="c025" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">通过使用阈值0.01，我们可以选择重要性超过阈值的特征，如下所示。或者，如果我们在寻找前n个特性，我们可以像这样调整语法:<code class="fe ok ol om nn b">imp[‘importance’].head(n).index</code>。</p><pre class="me mf mg mh gt nm nn no np aw nq bi"><span id="19cd" class="nr mq jj nn b gy ns nt l nu nv">imp_features = imp[imp['importance']&gt;0.01].index <br/>print(f"===== {len(imp_features)} features were selected =====")<br/>print(f"{', '.join(imp_features)}")</span></pre><figure class="me mf mg mh gt iv gh gi paragraph-image"><div class="gh gi on"><img src="../Images/c83f7400a0efa033755ec51e7115d898.png" data-original-src="https://miro.medium.com/v2/resize:fit:1112/format:webp/1*qfreohebIlLWGOTbPCFRVw.png"/></div></figure><p id="acb4" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">10个特征高于阈值。如果我们使用这10个特征，让我们检查模型的性能:</p><pre class="me mf mg mh gt nm nn no np aw nq bi"><span id="bc04" class="nr mq jj nn b gy ns nt l nu nv">model1 = GradientBoostingClassifier(random_state=42)<br/>model1.fit(X_train[imp_features], y_train)</span><span id="c3f3" class="nr mq jj nn b gy nw nt l nu nv">print(f"Training data - ROC AUC: {get_roc_auc(model1, X_train[imp_features], y_train):.4f}")<br/>print(f"Test data - ROC AUC: {get_roc_auc(model1, X_test[imp_features], y_test):.4f}")</span></pre><figure class="me mf mg mh gt iv gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/8437dc599071df200d8aa06264357bf4.png" data-original-src="https://miro.medium.com/v2/resize:fit:618/format:webp/1*7uxSUpZuP3pe4EeNso6N-Q.png"/></div></figure><p id="f79a" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">即使我们只使用了三分之一的功能，模型性能也没有显著下降。</p><h2 id="6c28" class="nr mq jj bd mr nz oa dn mv ob oc dp mz lq od oe nb lu of og nd ly oh oi nf jp bi translated">📍3.2.使用递归特征消除</h2><p id="1dd1" class="pw-post-body-paragraph lh li jj lj b lk nh kt lm ln ni kw lp lq nj ls lt lu nk lw lx ly nl ma mb mc im bi translated">在递归特征消除中，我们从所有特征开始，然后每次递归地丢弃最不重要的特征(<code class="fe ok ol om nn b">step=1</code>)，直到满足一个标准。我们将使用一个标准来保留10个特征。</p><pre class="me mf mg mh gt nm nn no np aw nq bi"><span id="e20a" class="nr mq jj nn b gy ns nt l nu nv">rfe = RFE(GradientBoostingClassifier(random_state=42), <br/>          n_features_to_select=10)<br/>rfe.fit(X_train, y_train)</span><span id="7021" class="nr mq jj nn b gy nw nt l nu nv">rfe_features = X_train.columns[rfe.support_]<br/>print(f"===== {len(rfe_features)} features were selected =====")<br/>print(f"{', '.join(rfe_features)}")</span></pre><figure class="me mf mg mh gt iv gh gi paragraph-image"><div class="gh gi oo"><img src="../Images/9864fb6d35880844b86bcdbc20cf524a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1140/format:webp/1*lP-DPPvKfL4vlHHAKBt7vQ.png"/></div></figure><p id="f7f9" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">让我们使用选定的功能运行模型，并检查其性能:</p><pre class="me mf mg mh gt nm nn no np aw nq bi"><span id="2702" class="nr mq jj nn b gy ns nt l nu nv">model2 = GradientBoostingClassifier(random_state=42)<br/>model2.fit(X_train[rfe_features], y_train)</span><span id="bae2" class="nr mq jj nn b gy nw nt l nu nv">print(f"Training data - ROC AUC: {get_roc_auc(model2, X_train[rfe_features], y_train):.4f}")<br/>print(f"Test data - ROC AUC: {get_roc_auc(model2, X_test[rfe_features], y_test):.4f}")</span></pre><figure class="me mf mg mh gt iv gh gi paragraph-image"><div class="gh gi op"><img src="../Images/f92eb4da4a0e51790d968016a76fd748.png" data-original-src="https://miro.medium.com/v2/resize:fit:612/format:webp/1*wLFRY9MejfhSS2zyRGK8tA.png"/></div></figure><p id="6e3e" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">性能和以前一模一样。让我们看看这种方法是否选择了与以前相同的功能:</p><pre class="me mf mg mh gt nm nn no np aw nq bi"><span id="8592" class="nr mq jj nn b gy ns nt l nu nv">imp_features.sort_values().equals(rfe_features.sort_values())</span></pre><figure class="me mf mg mh gt iv gh gi paragraph-image"><div class="gh gi oq"><img src="../Images/d8eaa03ff936719afe599f772c975191.png" data-original-src="https://miro.medium.com/v2/resize:fit:114/format:webp/1*mIzpB48FKXpRK0CblU7OHA.png"/></div></figure><p id="b94e" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">是的，它做到了！这并不奇怪，因为两者都是基于梯度推进分类器的特征重要性。这种方法也适用于提供特征重要性或系数的其他算法。使用系数时，请记住要素的比例会影响系数。</p><p id="d3c1" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">这种方法比前一种方法慢，因为它需要多次运行模型。加快速度的一个方法是增加<code class="fe ok ol om nn b">step</code>的大小，这样它可以更快地消除特征。</p><h2 id="23df" class="nr mq jj bd mr nz oa dn mv ob oc dp mz lq od oe nb lu of og nd ly oh oi nf jp bi translated">📍 3.用SelectKBest选择前n名</h2><p id="f4c1" class="pw-post-body-paragraph lh li jj lj b lk nh kt lm ln ni kw lp lq nj ls lt lu nk lw lx ly nl ma mb mc im bi translated">与前两种方法不同，这种方法和接下来的方法是模型不可知的。我们将根据分数选择10个最佳功能。<a class="ae jg" href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.f_classif.html#sklearn.feature_selection.f_classif" rel="noopener ugc nofollow" target="_blank"> F值</a>被用作分类的默认分数，然而，可以使用<a class="ae jg" href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectKBest.html" rel="noopener ugc nofollow" target="_blank">不同的分数</a>。</p><pre class="me mf mg mh gt nm nn no np aw nq bi"><span id="0a59" class="nr mq jj nn b gy ns nt l nu nv">kbest = SelectKBest(k=10)<br/>kbest.fit(X_train, y_train)</span><span id="7e4d" class="nr mq jj nn b gy nw nt l nu nv"># See selected features<br/>kbest_features = X_train.columns[kbest.get_support()]<br/>print(f"===== {len(kbest_features)} features were selected =====")<br/>print(f"{', '.join(kbest_features)}")</span></pre><figure class="me mf mg mh gt iv gh gi paragraph-image"><div class="gh gi or"><img src="../Images/3fc2fc96d688998d41bbef6b7640e30a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1124/format:webp/1*hJc66sOk4LISYFH2rheIuw.png"/></div></figure><p id="019c" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">现在，该用新选择的特性再次运行模型了:</p><pre class="me mf mg mh gt nm nn no np aw nq bi"><span id="3ef7" class="nr mq jj nn b gy ns nt l nu nv">model3 = GradientBoostingClassifier(random_state=42)<br/>model3.fit(X_train[kbest_features], y_train)</span><span id="bfa4" class="nr mq jj nn b gy nw nt l nu nv">print(f"Training data - ROC AUC: {get_roc_auc(model3, X_train[kbest_features], y_train):.4f}")<br/>print(f"Test data - ROC AUC: {get_roc_auc(model3, X_test[kbest_features], y_test):.4f}")</span></pre><figure class="me mf mg mh gt iv gh gi paragraph-image"><div class="gh gi os"><img src="../Images/c43768696d42c4155b8e44cd2c2ff757.png" data-original-src="https://miro.medium.com/v2/resize:fit:630/format:webp/1*KhsQLcpjEPc9ED_Kwda4gQ.png"/></div></figure><p id="1e58" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">模型性能与以前的方法相似。让我们来看看选择了哪些之前没有选择的功能:</p><pre class="me mf mg mh gt nm nn no np aw nq bi"><span id="8ffb" class="nr mq jj nn b gy ns nt l nu nv">[var for var in kbest_features if var not in rfe_features]</span></pre><figure class="me mf mg mh gt iv gh gi paragraph-image"><div class="gh gi ot"><img src="../Images/1345ac211094c0ed39119346a62ca63e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1254/format:webp/1*-i2x6nFE8EoszJ2tu-0k0w.png"/></div></figure><h2 id="d48f" class="nr mq jj bd mr nz oa dn mv ob oc dp mz lq od oe nb lu of og nd ly oh oi nf jp bi translated">📍 4.使用SelectPercentile选择前p个百分点</h2><p id="5bfe" class="pw-post-body-paragraph lh li jj lj b lk nh kt lm ln ni kw lp lq nj ls lt lu nk lw lx ly nl ma mb mc im bi translated">这是前一种方法的变体，它允许指定要选择的功能的百分比，而不是功能的数量。我们将选择前33%的功能来获得前10个功能:</p><pre class="me mf mg mh gt nm nn no np aw nq bi"><span id="96b1" class="nr mq jj nn b gy ns nt l nu nv">percentile = SelectPercentile(percentile=33)<br/>percentile.fit(X_train, y_train)</span><span id="9b0b" class="nr mq jj nn b gy nw nt l nu nv"># See selected features<br/>percentile_features = X_train.columns[percentile.get_support()]<br/>print(f"===== {len(percentile_features)} features were selected =====")<br/>print(f"{', '.join(percentile_features)}")</span></pre><figure class="me mf mg mh gt iv gh gi paragraph-image"><div class="gh gi ou"><img src="../Images/a0645b02a795e39557323de4b01dc08c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1122/format:webp/1*YCcCpsprRrv5kh11HbE9_A.png"/></div></figure><p id="9d50" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">现在，让我们检查模型性能:</p><pre class="me mf mg mh gt nm nn no np aw nq bi"><span id="e417" class="nr mq jj nn b gy ns nt l nu nv">model4 = GradientBoostingClassifier(random_state=42)<br/>model4.fit(X_train[percentile_features], y_train)</span><span id="7c23" class="nr mq jj nn b gy nw nt l nu nv">print(f"Training data - ROC AUC: {get_roc_auc(model4, X_train[percentile_features], y_train):.4f}")<br/>print(f"Test data - ROC AUC: {get_roc_auc(model4, X_test[percentile_features], y_test):.4f}")</span></pre><figure class="me mf mg mh gt iv gh gi paragraph-image"><div class="gh gi ov"><img src="../Images/3a31dc41c1bd992106caa310dc2eafb3.png" data-original-src="https://miro.medium.com/v2/resize:fit:632/format:webp/1*sC5w_KP8-qQWzM8peXsEfw.png"/></div></figure><p id="c9cd" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">性能和以前一样。让我们看看功能是否与前面的方法相同:</p><pre class="me mf mg mh gt nm nn no np aw nq bi"><span id="74a6" class="nr mq jj nn b gy ns nt l nu nv">percentile_features.sort_values().equals(kbest_features.sort_values())</span></pre><figure class="me mf mg mh gt iv gh gi paragraph-image"><div class="gh gi ow"><img src="../Images/c20e6aa1b7b2b6ddcb0ce08679a5da28.png" data-original-src="https://miro.medium.com/v2/resize:fit:118/format:webp/1*ocB-7lky3OwJAyWEPiIc0Q.png"/></div></figure><p id="f49d" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">这是有意义的，因为我们选择了百分点值来获得与以前相同数量的特征。</p><h2 id="3764" class="nr mq jj bd mr nz oa dn mv ob oc dp mz lq od oe nb lu of og nd ly oh oi nf jp bi translated">📍 5.结合多种方法</h2><p id="4334" class="pw-post-body-paragraph lh li jj lj b lk nh kt lm ln ni kw lp lq nj ls lt lu nk lw lx ly nl ma mb mc im bi translated">如果你不想只使用一种方法，我们可以基于多种方法的组合来进行特征选择。我们来看看所有方法的总结:</p><pre class="me mf mg mh gt nm nn no np aw nq bi"><span id="80b6" class="nr mq jj nn b gy ns nt l nu nv">selection = pd.DataFrame(index=X.columns)<br/>selection['imp'] = [var in imp_features for var in X.columns]<br/>selection['rfe'] = rfe.support_<br/>selection['kbest'] = kbest.get_support()<br/>selection['percentile'] = percentile.get_support()<br/>selection['sum'] = selection.sum(axis=1)<br/>selection.sort_values('sum', ascending=False, inplace=True)<br/>selection</span></pre><figure class="me mf mg mh gt iv gh gi paragraph-image"><div class="gh gi ox"><img src="../Images/59aa6175abf986ea578469923e50a7f2.png" data-original-src="https://miro.medium.com/v2/resize:fit:956/format:webp/1*eT5nZja_0aK_9gEXIBx9IA.png"/></div></figure><p id="6698" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">让我们检查一下<code class="fe ok ol om nn b">sum</code>列的分布:</p><pre class="me mf mg mh gt nm nn no np aw nq bi"><span id="8137" class="nr mq jj nn b gy ns nt l nu nv">pd.concat([selection['sum'].value_counts(normalize=True),<br/>           selection['sum'].value_counts()], axis=1, <br/>          keys=['prop', 'count'])</span></pre><figure class="me mf mg mh gt iv gh gi paragraph-image"><div class="gh gi oy"><img src="../Images/e941a5e622ca4230773d77257995c382.png" data-original-src="https://miro.medium.com/v2/resize:fit:344/format:webp/1*a3zZusCaVTaOaX1IoI6yhA.png"/></div></figure><p id="d940" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">该表告诉我们，16个特征没有被任何方法选择，6个特征被所有方法选择。由于<code class="fe ok ol om nn b">imp_features</code>和<code class="fe ok ol om nn b">rfe_features</code>是相同的，并且<code class="fe ok ol om nn b">kbest_features</code>和<code class="fe ok ol om nn b">percentile_features</code>包含相同的特征，所以看到我们在这里只看到偶数值:0，2，4也就不足为奇了。这意味着我们实际上是在聚合两种方法。</p><p id="edde" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">如果我们想更加小心，我们可以删除在两种方法中都没有选择的特性:</p><pre class="me mf mg mh gt nm nn no np aw nq bi"><span id="c152" class="nr mq jj nn b gy ns nt l nu nv">selected_features = selection[selection['sum']&gt;0].index<br/>print(f"===== {len(selected_features)} features were selected =====")<br/>print(f"{', '.join(selected_features)}")</span></pre><figure class="me mf mg mh gt iv gh gi paragraph-image"><div class="gh gi oz"><img src="../Images/d9c829b66e363158e63abdccd68cdcdc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1126/format:webp/1*VGYxYmQR3A2cCNzAMYXv_A.png"/></div></figure><p id="2749" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">我们现在已经选择了14个特征，是时候检查模型性能了:</p><pre class="me mf mg mh gt nm nn no np aw nq bi"><span id="8cbd" class="nr mq jj nn b gy ns nt l nu nv">model5 = GradientBoostingClassifier(random_state=42)<br/>model5.fit(X_train[selected_features], y_train)</span><span id="5c4c" class="nr mq jj nn b gy nw nt l nu nv">print(f"Training data - ROC AUC: {get_roc_auc(model5, X_train[selected_features], y_train):.4f}")<br/>print(f"Test data - ROC AUC: {get_roc_auc(model5, X_test[selected_features], y_test):.4f}")</span></pre><figure class="me mf mg mh gt iv gh gi paragraph-image"><div class="gh gi pa"><img src="../Images/4aef3606e067286900c32a222c453072.png" data-original-src="https://miro.medium.com/v2/resize:fit:626/format:webp/1*i0H0WFUuCtq0mWyCMOVeUA.png"/></div></figure><p id="fb6d" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">该性能与具有所有功能的基线模型相同，但我们仅使用了大约一半的功能。</p><p id="9c7e" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">瞧，这就是进行特征选择的5种简单方法。通过有效的特征选择，我们可以在不丧失预测能力的情况下建立更简单、更快速、更易于解释的模型。希望你能在下一个有监督的机器学习任务中使用这些方法。</p><figure class="me mf mg mh gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi pb"><img src="../Images/bbe5f055b103936b0173f226d39f471c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*i-TkEpoegnAxgo7_"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">马克·哈普尔在<a class="ae jg" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="9bb2" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><em class="pc">您想访问更多这样的内容吗？媒体会员可以无限制地访问媒体上的任何文章。如果你使用</em> <a class="ae jg" href="https://zluvsand.medium.com/membership" rel="noopener"> <em class="pc">我的推荐链接</em></a><em class="pc">成为会员，你的一部分会费会直接去支持我。</em></p></div><div class="ab cl mi mj hx mk" role="separator"><span class="ml bw bk mm mn mo"/><span class="ml bw bk mm mn mo"/><span class="ml bw bk mm mn"/></div><div class="im in io ip iq"><p id="f495" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">感谢您阅读这篇文章。如果你感兴趣，这里有我的一些其他帖子的链接:<br/> ◼️ <a class="ae jg" rel="noopener" target="_blank" href="/explaining-scikit-learn-models-with-shap-61daff21b12a">解释scikit-learn models with shap</a><br/>◼️️<a class="ae jg" rel="noopener" target="_blank" href="/k-nearest-neighbours-explained-52c910c035c5">k近邻解释</a> <br/> ◼️️ <a class="ae jg" rel="noopener" target="_blank" href="/logistic-regression-explained-7695f15d1b8b">逻辑回归解释</a> <br/> ◼️️ <a class="ae jg" rel="noopener" target="_blank" href="/comparing-random-forest-and-gradient-boosting-d7236b429c15">比较随机森林和梯度推进</a> <br/> ◼️️ <a class="ae jg" rel="noopener" target="_blank" href="/how-are-decision-trees-built-a8e5af57ce8?source=your_stories_page-------------------------------------">决策树是如何构建的？</a> <br/> ◼️️ <a class="ae jg" rel="noopener" target="_blank" href="/pipeline-columntransformer-and-featureunion-explained-f5491f815f?source=your_stories_page-------------------------------------">管道，ColumnTransformer和FeatureUnion说明</a>t30】◼️️<a class="ae jg" rel="noopener" target="_blank" href="/featureunion-columntransformer-pipeline-for-preprocessing-text-data-9dcb233dbcb6">feature union，ColumnTransformer &amp;管道用于预处理文本数据</a></p></div></div>    
</body>
</html>