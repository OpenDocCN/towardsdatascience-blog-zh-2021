<html>
<head>
<title>Higher-Order Functions with Spark 3.1</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Spark 3.1的高阶函数</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/higher-order-functions-with-spark-3-1-7c6cf591beaa?source=collection_archive---------9-----------------------#2021-07-26">https://towardsdatascience.com/higher-order-functions-with-spark-3-1-7c6cf591beaa?source=collection_archive---------9-----------------------#2021-07-26</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="71b8" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">在Spark SQL中处理数组。</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/e9064f9be3f3275b6500f6485ab13f05.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*W-Ek8C2nVYDOrIsyuHtI9A.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">唐纳德·詹纳蒂在<a class="ae ky" href="https://unsplash.com/s/photos/vla?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="283a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">复杂的数据结构，如数组、结构和映射在大数据处理中非常常见，尤其是在Spark中。每当我们希望在一列中表示每行上的多个值时，就会出现这种情况，在数组数据类型的情况下，这可以是一个值列表，在映射的情况下，这可以是一个键值对列表。</p><p id="9bee" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">从Spark 2.4开始，通过发布高阶函数(Hof)，对处理这些复杂数据类型的支持增加了。在本文中，我们将了解什么是高阶函数，如何有效地使用它们，以及在最近几个Spark和3.1.1版本中发布了哪些相关功能。对于代码，我们将使用Python API。</p><p id="3dbd" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">继我们在上一篇<a class="ae ky" rel="noopener" target="_blank" href="/spark-sql-102-aggregations-and-window-functions-9f829eaa7549">文章</a>中提到的聚合和窗口函数之后，HOFs是Spark SQL中另一组更高级的转换。</p><p id="3303" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们首先看看Spark提供的三种复杂数据类型之间的区别。</p><h2 id="27be" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated">数组类型</h2><pre class="kj kk kl km gt mo mp mq mr aw ms bi"><span id="8fa9" class="lv lw it mp b gy mt mu l mv mw">l = [(1, ['the', 'quick', 'braun', 'fox'])]</span><span id="dc01" class="lv lw it mp b gy mx mu l mv mw">df = spark.createDataFrame(l, schema=['id', 'words'])</span><span id="7dd5" class="lv lw it mp b gy mx mu l mv mw">df.printSchema()</span><span id="a3b6" class="lv lw it mp b gy mx mu l mv mw">root<br/> |-- id: long (nullable = true)<br/> |-- words: array (nullable = true)<br/> |    |-- element: string (containsNull = true)</span></pre><p id="19cc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在上面的示例中，我们有一个包含两列的数据帧，其中列<em class="my"> words </em>是数组类型，这意味着在数据帧的每一行上，我们可以表示一个值列表，并且该列表在每一行上可以有不同的大小。此外，数组的元素是有顺序的。重要的属性是数组在元素类型方面是同质的，这意味着所有元素必须具有相同的类型。要访问数组的元素，我们可以使用如下的索引:</p><pre class="kj kk kl km gt mo mp mq mr aw ms bi"><span id="ee66" class="lv lw it mp b gy mt mu l mv mw">df.withColumn('first_element', col('words')[0])</span></pre><h2 id="aed4" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated">结构类型</h2><p id="666d" class="pw-post-body-paragraph kz la it lb b lc mz ju le lf na jx lh li nb lk ll lm nc lo lp lq nd ls lt lu im bi translated"><em class="my"> StructType </em>用于将一些可能具有不同类型(不同于数组)的子字段组合在一起。每个子字段都有一个类型和一个名称，并且对于数据帧中的所有行都必须相同。可能出乎意料的是，一个结构中的子字段是有顺序的，所以比较两个具有相同字段但顺序不同的结构<em class="my"> s1==s2 </em>会导致<em class="my"> False </em>。</p><p id="d926" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">请注意数组和结构之间的基本区别:</p><ul class=""><li id="9330" class="ne nf it lb b lc ld lf lg li ng lm nh lq ni lu nj nk nl nm bi translated">数组:类型相同，允许每行有不同的大小</li><li id="7afb" class="ne nf it lb b lc nn lf no li np lm nq lq nr lu nj nk nl nm bi translated">结构:类型异构，每行都需要相同的模式</li></ul><h2 id="c1c4" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated">地图类型</h2><p id="ee39" class="pw-post-body-paragraph kz la it lb b lc mz ju le lf na jx lh li nb lk ll lm nc lo lp lq nd ls lt lu im bi translated">您可以将map类型视为前面两种类型的混合:array和struct。设想这样一种情况，每一行的模式都没有给出，您需要在每一行上支持不同数量的子字段。在这种情况下，您不能使用struct。但是使用数组对您来说并不是一个好的选择，因为每个元素都有一个名称和一个值(它实际上是一个键-值对),或者因为元素有不同的类型——这是map类型的一个很好的用例。使用map类型，您可以在每一行上存储不同数量的键-值对，但是每个键必须具有相同的类型，并且所有的值都必须是相同的类型(可以与键的类型不同)。配对的顺序很重要。</p><h1 id="3afe" class="ns lw it bd lx nt nu nv ma nw nx ny md jz nz ka mg kc oa kd mj kf ob kg mm oc bi translated">变换数组</h1><p id="8f3f" class="pw-post-body-paragraph kz la it lb b lc mz ju le lf na jx lh li nb lk ll lm nc lo lp lq nd ls lt lu im bi translated">在我们开始讨论转换数组之前，让我们先看看如何创建一个数组。第一种方法我们已经在上面看到了，我们从一个本地值列表中创建了数据帧。另一方面，如果我们已经有了一个数据帧，我们想将一些列组成一个数组，我们可以使用函数<a class="ae ky" href="https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.functions.array.html#pyspark.sql.functions.array" rel="noopener ugc nofollow" target="_blank"> <em class="my"> array() </em> </a>来实现这个目的。它允许您从其他现有的列中创建一个数组，因此如果您有列<em class="my"> a </em>、<em class="my"> b </em>、<em class="my"> c </em>，并且您希望将值放在一个数组中，而不是放在单独的列中，您可以这样做:</p><pre class="kj kk kl km gt mo mp mq mr aw ms bi"><span id="6659" class="lv lw it mp b gy mt mu l mv mw">df.withColumn('my_arr', array('a', 'b', 'c'))</span></pre><p id="9daa" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">除此之外，还有一些函数产生一个数组作为转换的结果。例如，函数<a class="ae ky" href="https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.functions.split.html#pyspark.sql.functions.split" rel="noopener ugc nofollow" target="_blank"> <em class="my"> split( </em> ) </a>会将一个字符串拆分成一个单词数组。另一个例子是<a class="ae ky" href="https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.functions.collect_list.html#pyspark.sql.functions.collect_list" rel="noopener ugc nofollow" target="_blank"><em class="my">collect _ list()</em></a>或<a class="ae ky" href="https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.functions.collect_set.html#pyspark.sql.functions.collect_set" rel="noopener ugc nofollow" target="_blank"><em class="my">collect _ set()</em></a>，它们都是聚合函数，也会产生一个数组。</p><p id="99d2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">实际上，将数组放入数据帧的最常见方式是从支持复杂数据结构的数据源(如Parquet)读取数据。在这种文件格式中，一些列可以存储为数组，因此Spark自然也会将它们读取为数组。</p><p id="b784" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在，当我们知道如何创建一个数组时，让我们看看数组是如何转换的。</p><p id="b5f1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">从Spark 2.4开始，有大量的函数用于数组转换。有关它们的完整列表，请查看PySpark <a class="ae ky" href="https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql.html#functions" rel="noopener ugc nofollow" target="_blank">文档</a>。例如，所有以<em class="my"> array_ </em>开头的函数都可以用于数组处理，您可以找到最小-最大值、对数组进行重复数据删除、排序、连接等等。接下来，还有<a class="ae ky" href="https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.functions.concat.html#pyspark.sql.functions.concat" rel="noopener ugc nofollow" target="_blank"><em class="my">concat()</em></a><a class="ae ky" href="https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.functions.flatten.html#pyspark.sql.functions.flatten" rel="noopener ugc nofollow" target="_blank"><em class="my">flatten()</em></a><a class="ae ky" href="https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.functions.shuffle.html#pyspark.sql.functions.shuffle" rel="noopener ugc nofollow" target="_blank"><em class="my">shuffle()</em></a><a class="ae ky" href="https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.functions.size.html#pyspark.sql.functions.size" rel="noopener ugc nofollow" target="_blank"><em class="my">size()</em></a><a class="ae ky" href="https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.functions.slice.html#pyspark.sql.functions.slice" rel="noopener ugc nofollow" target="_blank"><em class="my">【slice()</em></a><a class="ae ky" href="https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.functions.sort_array.html#pyspark.sql.functions.sort_array" rel="noopener ugc nofollow" target="_blank"><em class="my">sort _ array()</em></a>。正如你所看到的，API在这方面已经相当成熟，你可以用Spark中的数组做很多操作。</p><p id="c696" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">除了上述这些函数，还有一组函数将另一个函数作为参数，然后应用于数组的每个元素，这些函数被称为高阶函数(Hof)。了解它们很重要的一点是，在Python API中，从3.1.1开始就支持它们，而在Scala API中，它们是从3.0开始发布的。另一方面，对于SQL表达式，从2.4开始就可以使用了。</p><p id="51b3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">要查看一些具体示例，请考虑以下简单的数据框架:</p><pre class="kj kk kl km gt mo mp mq mr aw ms bi"><span id="91b9" class="lv lw it mp b gy mt mu l mv mw">l = [(1, ['prague', 'london', 'tokyo', None, 'sydney'])]</span><span id="b1da" class="lv lw it mp b gy mx mu l mv mw">df = spark.createDataFrame(l, ['id', 'cities'])</span><span id="b377" class="lv lw it mp b gy mx mu l mv mw">df.show(truncate=False)</span><span id="48eb" class="lv lw it mp b gy mx mu l mv mw">+---+-------------------------------------+<br/>|id |cities                               |<br/>+---+-------------------------------------+<br/>|1  |[prague, london, tokyo, null, sydney]|<br/>+---+-------------------------------------+</span></pre><p id="d9bf" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">假设我们想要完成这五个独立的任务:</p><ol class=""><li id="84de" class="ne nf it lb b lc ld lf lg li ng lm nh lq ni lu od nk nl nm bi translated">将每个城市的首字母转换成大写。</li><li id="8e51" class="ne nf it lb b lc nn lf no li np lm nq lq nr lu od nk nl nm bi translated">去掉数组中的空值。</li><li id="5cab" class="ne nf it lb b lc nn lf no li np lm nq lq nr lu od nk nl nm bi translated">检查是否有以字母t开头的元素。</li><li id="cd33" class="ne nf it lb b lc nn lf no li np lm nq lq nr lu od nk nl nm bi translated">检查数组中是否有空值。</li><li id="a0d6" class="ne nf it lb b lc nn lf no li np lm nq lq nr lu od nk nl nm bi translated">对数组中每个城市的字符数(长度)求和。</li></ol><p id="e7f3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这些是可以用HOFs解决的问题的一些典型例子。所以让我们一个一个来看:</p><h2 id="b87e" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated">改变</h2><p id="9772" class="pw-post-body-paragraph kz la it lb b lc mz ju le lf na jx lh li nb lk ll lm nc lo lp lq nd ls lt lu im bi translated">对于第一个问题，我们可以使用<a class="ae ky" href="https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.functions.transform.html#pyspark.sql.functions.transform" rel="noopener ugc nofollow" target="_blank"> <em class="my"> transform </em> </a> HOF，它只是采用一个匿名函数，将它应用于原始数组的每个元素，并返回另一个转换后的数组。语法如下:</p><pre class="kj kk kl km gt mo mp mq mr aw ms bi"><span id="3998" class="lv lw it mp b gy mt mu l mv mw">df \<br/>.withColumn('cities', transform('cities', lambda x: initcap(x))) \<br/>.show(truncate=False)</span><span id="076a" class="lv lw it mp b gy mx mu l mv mw">+---+-------------------------------------+<br/>|id |cities                               |<br/>+---+-------------------------------------+<br/>|1  |[Prague, London, Tokyo, null, Sydney]|<br/>+---+-------------------------------------+</span></pre><p id="ca80" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如您所见，<em class="my"> transform() </em>有两个参数，第一个是需要转换的数组，第二个是匿名函数。在这里，为了实现我们的转换，我们在匿名函数中使用了<a class="ae ky" href="https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.functions.initcap.html#pyspark.sql.functions.initcap" rel="noopener ugc nofollow" target="_blank"> <em class="my"> initcap() </em> </a>，它被应用于数组的每个元素——这正是<em class="my">转换</em> HOF允许我们做的。对于SQL表达式，可以按如下方式使用:</p><pre class="kj kk kl km gt mo mp mq mr aw ms bi"><span id="3f7d" class="lv lw it mp b gy mt mu l mv mw">df.selectExpr("id", "TRANSFORM(cities, x -&gt; INITCAP(x)) AS cities")</span></pre><p id="57df" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">注意，SQL中的匿名函数是用箭头(-&gt;)符号表示的。</p><h2 id="1f78" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated">过滤器</h2><p id="547a" class="pw-post-body-paragraph kz la it lb b lc mz ju le lf na jx lh li nb lk ll lm nc lo lp lq nd ls lt lu im bi translated">在第二个问题中，我们希望从数组中过滤出空值。这一点(以及任何其他过滤)可以使用<a class="ae ky" href="https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.functions.filter.html#pyspark.sql.functions.filter" rel="noopener ugc nofollow" target="_blank"> <em class="my">过滤器</em> </a> HOF来处理。它允许我们应用一个匿名函数，该函数对每个元素返回布尔值(<em class="my"> True </em> / <em class="my"> False </em>)，并且它将返回一个新数组，该数组只包含该函数返回<em class="my"> True </em>的元素:</p><pre class="kj kk kl km gt mo mp mq mr aw ms bi"><span id="fde0" class="lv lw it mp b gy mt mu l mv mw">df \<br/>.withColumn('cities', filter('cities', lambda x: x.isNotNull())) \<br/>.show(truncate=False)</span><span id="3381" class="lv lw it mp b gy mx mu l mv mw">+---+-------------------------------+<br/>|id |cities                         |<br/>+---+-------------------------------+<br/>|1  |[prague, london, tokyo, sydney]|<br/>+---+-------------------------------+</span></pre><p id="3eda" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这里，在匿名函数中我们调用PySpark函数<a class="ae ky" href="https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.Column.isNotNull.html#pyspark.sql.Column.isNotNull" rel="noopener ugc nofollow" target="_blank"> <em class="my"> isNotNull() </em> </a>。SQL语法如下所示:</p><pre class="kj kk kl km gt mo mp mq mr aw ms bi"><span id="6a6f" class="lv lw it mp b gy mt mu l mv mw">df.selectExpr("id", "FILTER(cities, x -&gt; x IS NOT NULL) AS cities")</span></pre><h2 id="9d03" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated">存在</h2><p id="a2f5" class="pw-post-body-paragraph kz la it lb b lc mz ju le lf na jx lh li nb lk ll lm nc lo lp lq nd ls lt lu im bi translated">在下一个问题中，我们希望检查数组是否包含满足某些特定条件的元素。请注意，这是一个更一般的例子，在这种情况下，我们希望检查某个特定元素的存在。例如，如果我们想检查数组是否包含城市<em class="my">布拉格</em>，我们可以只调用<a class="ae ky" href="https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.functions.array_contains.html#pyspark.sql.functions.array_contains" rel="noopener ugc nofollow" target="_blank"><em class="my">array _ contains</em></a>函数:</p><pre class="kj kk kl km gt mo mp mq mr aw ms bi"><span id="024f" class="lv lw it mp b gy mt mu l mv mw">df.withColumn('has_prague', array_contains('cities', 'prague'))</span></pre><p id="6170" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">另一方面，<a class="ae ky" href="https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.functions.exists.html#pyspark.sql.functions.exists" rel="noopener ugc nofollow" target="_blank"> <em class="my">存在</em> </a> HOF允许我们对每个元素应用更一般的条件。结果不再是像前两个Hof那样的数组，而只是<em class="my">真</em> / <em class="my">假</em>:</p><pre class="kj kk kl km gt mo mp mq mr aw ms bi"><span id="fbc7" class="lv lw it mp b gy mt mu l mv mw">df \<br/>.withColumn('has_t_city', <br/>  exists('cities', lambda x: x.startswith('t'))) \<br/>.show(truncate=False)</span><span id="150e" class="lv lw it mp b gy mx mu l mv mw">+---+-------------------------------------+----------+<br/>|id |cities                               |has_t_city|<br/>+---+-------------------------------------+----------+<br/>|1  |[prague, london, tokyo, null, sydney]|true      |<br/>+---+-------------------------------------+----------+</span></pre><p id="512c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这里在匿名函数中，我们使用了PySpark函数<a class="ae ky" href="https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.Column.startswith.html#pyspark.sql.Column.startswith" rel="noopener ugc nofollow" target="_blank"><em class="my">【starts with()</em></a>。</p><h2 id="cac4" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated">FORALL</h2><p id="3f8f" class="pw-post-body-paragraph kz la it lb b lc mz ju le lf na jx lh li nb lk ll lm nc lo lp lq nd ls lt lu im bi translated">在第四个问题中，我们希望验证数组中的所有元素是否都满足某些条件，在我们的示例中，我们希望检查它们是否都不为空:</p><pre class="kj kk kl km gt mo mp mq mr aw ms bi"><span id="7637" class="lv lw it mp b gy mt mu l mv mw">df \<br/>.withColumn('nulls_free',forall('cities', lambda x: x.isNotNull()))\<br/>.show(truncate=False)</span><span id="61e0" class="lv lw it mp b gy mx mu l mv mw">+---+-------------------------------------+----------+<br/>|id |cities                               |nulls_free|<br/>+---+-------------------------------------+----------+<br/>|1  |[prague, london, tokyo, null, sydney]|false     |<br/>+---+-------------------------------------+----------+</span></pre><p id="6325" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">正如你所看到的，对于所有的 来说<a class="ae ky" href="https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.functions.forall.html#pyspark.sql.functions.forall" rel="noopener ugc nofollow" target="_blank"> <em class="my">与<em class="my">存在</em>非常相似，但是现在我们正在检查条件是否对所有的元素都成立，之前我们至少要寻找一个。</em></a></p><h2 id="0bf0" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated">总计</h2><p id="edcf" class="pw-post-body-paragraph kz la it lb b lc mz ju le lf na jx lh li nb lk ll lm nc lo lp lq nd ls lt lu im bi translated">在最后一个问题中，我们希望对数组中每个单词的长度求和。这是一个例子，我们希望将整个数组简化为一个值，对于这种问题，我们可以使用HOF <a class="ae ky" href="https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.functions.aggregate.html#pyspark.sql.functions.aggregate" rel="noopener ugc nofollow" target="_blank"> <em class="my">聚合</em> </a>。</p><pre class="kj kk kl km gt mo mp mq mr aw ms bi"><span id="8fd7" class="lv lw it mp b gy mt mu l mv mw"><br/>df \<br/>.withColumn('cities', filter('cities', lambda x: x.isNotNull())) \<br/>.withColumn('cities_len', <br/>  aggregate('cities', lit(0), lambda x, y: x + length(y))) \<br/>.show(truncate=False)</span><span id="f32b" class="lv lw it mp b gy mx mu l mv mw">+---+-------------------------------+----------+<br/>|id |cities                         |cities_len|<br/>+---+-------------------------------+----------+<br/>|1  |[prague, london, tokyo, sydney]|23        |<br/>+---+-------------------------------+----------+</span></pre><p id="3b40" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">使用SQL:</p><pre class="kj kk kl km gt mo mp mq mr aw ms bi"><span id="b77c" class="lv lw it mp b gy mt mu l mv mw">df \<br/>.withColumn("cities", filter("cities", lambda x: x.isNotNull())) \<br/>.selectExpr(<br/>    "cities", <br/>    "AGGREGATE(cities, 0,(x, y) -&gt; x + length(y)) AS cities_len"<br/>)</span></pre><p id="c37a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如你所见，与之前的HOFs相比，语法稍微复杂了一些。<em class="my">集合</em>接受更多的参数，第一个仍然是我们想要转换的数组，第二个参数是我们想要开始的初始值。在我们的例子中，初始值是零(<em class="my"> lit(0) </em>)，我们将把每个城市的长度加进去。第三个参数是匿名函数，现在这个函数本身有两个参数——第一个参数(在我们的例子中是<em class="my"> x </em>)是运行缓冲区，我们将第二个参数表示的下一个元素的长度(在我们的例子中是<em class="my"> y </em>)添加到这个缓冲区中。</p><p id="e5cf" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">可选地，可以提供第四个参数，这是另一个转换最终结果的匿名函数。如果我们想要进行更复杂的聚合，这是很有用的，例如，如果我们想要计算平均长度，我们需要保留大约两个值，即<em class="my"> sum </em>和<em class="my"> count </em>，我们将在最后的转换中对它们进行如下划分:</p><pre class="kj kk kl km gt mo mp mq mr aw ms bi"><span id="fe36" class="lv lw it mp b gy mt mu l mv mw">(<br/>    df<br/>    .withColumn('cities', filter('cities', lambda x: x.isNotNull()))<br/>    .withColumn('cities_avg_len', <br/>        aggregate(<br/>            'cities', <br/>            struct(lit(0).alias('sum'), lit(0).alias('count')), <br/>            lambda x, y: struct(<br/>                (x.sum + length(y)).alias('sum'), <br/>                (x.count + 1).alias('count')<br/>            ),<br/>            lambda x: x.sum / x.count<br/>        )<br/>    )<br/>).show(truncate=False)</span><span id="1b08" class="lv lw it mp b gy mx mu l mv mw">+---+-------------------------------+--------------+<br/>|id |cities                         |cities_avg_len|<br/>+---+-------------------------------+--------------+<br/>|1  |[prague, london, tokyo, sydney]|5.75          |<br/>+---+-------------------------------+--------------+</span></pre><p id="cf54" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如您所见，这是一个更高级的示例，我们需要在聚合过程中保留两个值，我们使用具有两个子字段<em class="my"> sum </em>和<em class="my"> count的<a class="ae ky" href="https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.functions.struct.html#pyspark.sql.functions.struct" rel="noopener ugc nofollow" target="_blank"> <em class="my"> struct() </em> </a>来表示它们。</em>使用第一个匿名函数，我们计算所有长度的最终总和，以及元素总数。在第二个匿名函数中，我们只是将这两个值相除，得到最终的平均值。还要注意，在使用<em class="my"> aggregate </em>之前，我们首先过滤掉空值，因为如果我们在数组中保留空值，总和(以及平均值)将变成空值。</p><p id="a25f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">要查看聚合HOF与SQL表达式一起使用的另一个示例，请检查<a class="ae ky" href="https://stackoverflow.com/questions/59181802/how-to-count-the-trailing-zeroes-in-an-array-column-in-a-pyspark-dataframe-witho/59194663#59194663" rel="noopener ugc nofollow" target="_blank"> this </a> Stack Overflow问题。</p><p id="8fad" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">除了前面提到的这五个Hof，还有<a class="ae ky" href="https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.functions.zip_with.html#pyspark.sql.functions.zip_with" rel="noopener ugc nofollow" target="_blank"> <em class="my"> zip_with </em> </a>可以用来将两个数组合并成一个数组。除此之外，还有其他的Hof，如<a class="ae ky" href="https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.functions.map_filter.html#pyspark.sql.functions.map_filter" rel="noopener ugc nofollow" target="_blank"> <em class="my"> map_filter </em> </a>，<a class="ae ky" href="https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.functions.map_zip_with.html#pyspark.sql.functions.map_zip_with" rel="noopener ugc nofollow" target="_blank"> <em class="my"> map_zip_with </em> </a>，<a class="ae ky" href="https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.functions.transform_keys.html#pyspark.sql.functions.transform_keys" rel="noopener ugc nofollow" target="_blank"><em class="my">transform _ keys</em></a>，以及<a class="ae ky" href="https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.functions.transform_values.html#pyspark.sql.functions.transform_values" rel="noopener ugc nofollow" target="_blank"><em class="my">transform _ values</em></a>与地图一起使用，我们将在以后的文章中了解它们。</p><h1 id="4261" class="ns lw it bd lx nt nu nv ma nw nx ny md jz nz ka mg kc oa kd mj kf ob kg mm oc bi translated">结论</h1><p id="a343" class="pw-post-body-paragraph kz la it lb b lc mz ju le lf na jx lh li nb lk ll lm nc lo lp lq nd ls lt lu im bi translated">在本文中，我们介绍了高阶函数(Hof ),这是Spark 2.4中发布的一个特性。首先，只有SQL表达式支持它，但是从3.1.1开始，Python API也支持它。我们已经看到了五个Hof的例子，它们允许我们在Spark数组中转换、过滤、检查存在性和聚集元素。在HOFs发布之前，大多数问题都必须使用用户定义的函数来解决。然而，HOF方法在性能方面更有效，要查看一些性能基准，请参见我最近的另一篇文章，其中显示了一些具体的数字。</p></div></div>    
</body>
</html>