<html>
<head>
<title>The “Bias-Variance Trade-Off” Explained Practically (In Python)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">“偏差-方差权衡”实用解释(Python语言)</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/the-bias-variance-trade-off-explained-practically-in-python-48cf29d9e900?source=collection_archive---------11-----------------------#2021-07-31">https://towardsdatascience.com/the-bias-variance-trade-off-explained-practically-in-python-48cf29d9e900?source=collection_archive---------11-----------------------#2021-07-31</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="136a" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsdatascience.com/tagged/getting-started" rel="noopener" target="_blank">入门</a></h2><div class=""/><div class=""><h2 id="cc73" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">如果你曾经对偏差-方差权衡感到困惑，那是因为你总是阅读理论解释。这比看起来简单——借助于几行Python代码。</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/28224a0ae537760f0c6e9318aa74c3ef.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MFcu-N5SuTY4UF55YxZuOw.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">[作者图]</p></figure><p id="e8d5" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">“偏差-方差权衡”是数据科学面试中最常见的话题之一。尽管如此，许多候选人仍难以深入理解这个概念。我猜这是因为这个话题总是从完全理论的角度来解释。</p><p id="8558" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">然而，我相信理解一件事情的最好方法是自己去做——或者，更好的方法是，自己编写代码。</p><blockquote class="md"><p id="e132" class="me mf it bd mg mh mi mj mk ml mm mc dk translated">你不是真的懂，直到你不会编码！</p></blockquote><p id="aa56" class="pw-post-body-paragraph lh li it lj b lk mn kd lm ln mo kg lp lq mp ls lt lu mq lw lx ly mr ma mb mc im bi translated">在本文中，借助一些数据，我们将看到偏差-方差权衡在实践中意味着什么，以及如何在Python中计算它(从头开始和使用开箱即用的实现)。</p><h1 id="fc3c" class="ms mt it bd mu mv mw mx my mz na nb nc ki nd kj ne kl nf km ng ko nh kp ni nj bi translated">偏差-方差公式:从理论到实践</h1><p id="a960" class="pw-post-body-paragraph lh li it lj b lk nk kd lm ln nl kg lp lq nm ls lt lu nn lw lx ly no ma mb mc im bi translated">偏差-方差分解的通常定义是:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi np"><img src="../Images/c2d0895855a5e1fc6fb98489c3e8df91.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6Qid9KgPWPkGPeW0gePHRg.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">偏差-方差分解(θ是包含模型参数的向量)。[作者图]</p></figure><p id="55d3" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">其中MSE代表均方误差，θ代表模型的参数(例如，在线性回归中，θ是包含所有回归系数的向量)。</p><p id="8d9a" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jd">但是有一个问题:我们永远无法观测到θ </strong>的真实值。此外，在一些模型中，不可能明确地找到θ是什么。所以这个定义从实用的角度来看是相当没用的。</p><p id="ac54" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">但是，我们在现实生活中观察到的是地面真实，是目标变量在一些测试数据上的实现(通常称为<em class="nq"> y </em>)。因此，从我们的角度来看，用<em class="nq"> y </em>代替θ更有意义，并获得以下等式:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi np"><img src="../Images/8ad5c00e271cf8a239aa249767467d9c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aX9fXjlox88XN3XM-0swLA.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">偏差-方差分解(y是目标变量)。[作者图]</p></figure><p id="64a1" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">其中:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi np"><img src="../Images/ae3ac410123e201ff99a2b4a4dad3a31.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*V__Ad7sgp_sDKU8MOhVJQQ.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">图例[作者图]</p></figure><p id="8e40" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">这个公式更方便，因为在现实生活中，我们实际上知道所有这些量。</p><p id="11d4" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">此外，使用这个版本的公式，我们能够对MSE、方差和偏差给出更非正式的解释:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi np"><img src="../Images/ac9ed33049619a23673914aa11228a77.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9xjIBOvBmcPPtMI5GZGDpw.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">偏差-方差分解。MSE、方差和偏差分别测量的非正式定义。[作者图]</p></figure><p id="d516" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">这种分解经常被用来解释模型的结果是如何基于它的“灵活性”而变化的。</p><ul class=""><li id="d56c" class="nr ns it lj b lk ll ln lo lq nt lu nu ly nv mc nw nx ny nz bi translated"><strong class="lj jd">灵活性低</strong>。所有估计的模型都趋于相似，因此<strong class="lj jd">方差很小</strong>。另一方面,“平均模型”不够强大，无法接近地面真相，因此<strong class="lj jd">偏差较大</strong>。</li><li id="03a7" class="nr ns it lj b lk oa ln ob lq oc lu od ly oe mc nw nx ny nz bi translated"><strong class="lj jd">高灵活性</strong>。每个模型在很大程度上依赖于它被训练的特定观察，所以模型彼此非常不同，因此<strong class="lj jd">方差很大</strong>。然而，从长远来看，灵活性可以让你考虑到所有的小细节。因此，平均所有模型允许我们获得非常精确的“平均模型”，因此<strong class="lj jd">偏差很小</strong>。</li></ul><p id="ca62" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">这是权衡的本质:如果复杂度太低或太高，由于偏差或方差，我们将分别具有高的均方误差。关键是如何获得恰到好处的模型灵活性，以便同时最小化偏差和方差。</p><h1 id="659d" class="ms mt it bd mu mv mw mx my mz na nb nc ki nd kj ne kl nf km ng ko nh kp ni nj bi translated">但是我们所说的“平均模型”是什么意思呢？</h1><p id="6f2c" class="pw-post-body-paragraph lh li it lj b lk nk kd lm ln nl kg lp lq nm ls lt lu nn lw lx ly no ma mb mc im bi translated">此时，您可能想知道获得不同的模型(和一个“平均模型”)意味着什么。</p><p id="2632" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">假设您选择一个算法(例如Scikit-learn的<code class="fe of og oh oi b">DecisionTreeRegressor</code>)并如下设置其超参数:</p><pre class="ks kt ku kv gt oj oi ok ol aw om bi"><span id="d637" class="on mt it oi b gy oo op l oq or"><strong class="oi jd">from </strong>sklearn.tree <strong class="oi jd">import </strong>DecisionTreeRegressor</span><span id="5c4b" class="on mt it oi b gy os op l oq or">algo = DecisionTreeRegressor<!-- -->(min_samples_leaf = 10)</span></pre><p id="32bb" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">此时，您可以根据训练算法的特定训练数据集获得无限可能的模型。</p><p id="46ea" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">假设您可以绘制许多不同的训练数据集，并为每个数据集训练不同的模型。然后，你可以取每个模型预测的平均值:这就是我们所说的“平均模型”。</p><pre class="ks kt ku kv gt oj oi ok ol aw om bi"><span id="f663" class="on mt it oi b gy oo op l oq or"><strong class="oi jd">import </strong>pandas <strong class="oi jd">as </strong>pd</span><span id="034c" class="on mt it oi b gy os op l oq or"><strong class="oi jd"># initialize dataframe for storing predictions on test data</strong></span><span id="8fb9" class="on mt it oi b gy os op l oq or">preds_test = pd.DataFrame(index = y_test.index)<br/></span><span id="eb2b" class="on mt it oi b gy os op l oq or"><strong class="oi jd"># for each model: draw training dataset, fit model on training dataset and make predictions on test data</strong></span><span id="26d8" class="on mt it oi b gy os op l oq or"><strong class="oi jd">for </strong>i <strong class="oi jd">in range</strong>(1, n_models + 1):<br/>  X_train, y_train = draw_training_dataset()<br/>  model = algo.fit(X_train, y_train)<br/>  preds_test[f'Model {i}'] = model.predict(X_test)<br/></span><span id="8ee9" class="on mt it oi b gy os op l oq or"><strong class="oi jd"># calculate "average model"'s predictions</strong></span><span id="6454" class="on mt it oi b gy os op l oq or">mean_pred_test = preds_test.mean(axis = 1)</span></pre><p id="535d" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">现在我们有了所有需要的量，我们终于可以计算均方误差、(平方)偏差和方差了。</p><pre class="ks kt ku kv gt oj oi ok ol aw om bi"><span id="14b7" class="on mt it oi b gy oo op l oq or"><strong class="oi jd">from </strong>sklearn.metrics <strong class="oi jd">import </strong>mean_squared_error</span><span id="b4ae" class="on mt it oi b gy os op l oq or">mse = preds_test.apply(<strong class="oi jd">lambda </strong>pred_test: mean_squared_error(y_test, pred_test)).mean()<br/><br/>bias_squared = mean_squared_error(y_test, mean_pred_test)<br/><br/>variance = preds_test.apply(<strong class="oi jd">lambda </strong>pred_test: mean_squared_error(mean_pred_test, pred_test)).mean()</span></pre><p id="35f4" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">这是我们刚刚遵循的流程的概念性草图:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ot"><img src="../Images/c58aecd84d9e959a9ee82c6c79eb44f5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WhSn5iilLr4jcLIKKeWu4A.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">获得n个不同模型的绘图过程。注1:所有模型都用相同的算法和相同的超参数初始化。注2:测试数据在不同的模型之间共享。[作者图]</p></figure><p id="b77f" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">让我们看看3个模型和5个测试观察的输出示例:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ou"><img src="../Images/9ec3b9585f0e58466117d2149357b3c6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wAOFASmq1B7gOy8Y8Br_LQ.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">具有5个观察值和3个模型的测试集的偏差-方差计算示例。[作者图]</p></figure><h1 id="8899" class="ms mt it bd mu mv mw mx my mz na nb nc ki nd kj ne kl nf km ng ko nh kp ni nj bi translated">自举是你所需要的</h1><p id="9433" class="pw-post-body-paragraph lh li it lj b lk nk kd lm ln nl kg lp lq nm ls lt lu nn lw lx ly no ma mb mc im bi translated">但是有一个问题。</p><p id="5613" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">在上面的代码中，我们使用了一个虚构的函数<code class="fe of og oh oi b">draw_training_dataset()</code>。然而，在现实生活中，我们实际上无法从无限的样本空间中提取训练数据集。事实上，我们通常只有一个训练数据集。怎么补？</p><p id="e69c" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">诀窍是从我们的训练数据集中引导(即随机抽取替换)行。所以这个过程变成了:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ov"><img src="../Images/5a9fe6cf0c01d4af0ef7cc165f673bfc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2z9UkghPkQodfzbeT-UEkg.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">从同一训练数据集中获得n个不同模型的引导程序。注1:所有模型都用相同的算法和相同的超参数初始化。注2:测试数据在不同的模型之间共享。[作者图]</p></figure><p id="4b33" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">现在我们准备把所有的东西包装在一起，编写一个Python函数来计算估计量的偏差和方差。我们所需要做的就是取上面的代码片段，用引导程序替换函数<code class="fe of og oh oi b">draw_training_dataset()</code>。</p><p id="b05d" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">这是一个解决方案:</p><pre class="ks kt ku kv gt oj oi ok ol aw om bi"><span id="b466" class="on mt it oi b gy oo op l oq or"><strong class="oi jd">import </strong>numpy <strong class="oi jd">as </strong>np<br/><strong class="oi jd">import </strong>pandas <strong class="oi jd">as </strong>pd<br/><strong class="oi jd">from </strong>sklearn.metrics <strong class="oi jd">import </strong>mean_squared_error</span><span id="42fa" class="on mt it oi b gy os op l oq or"><strong class="oi jd">def </strong>bias_variance_estimate(<br/>  estimator, <br/>  X_train, <br/>  y_train, <br/>  X_test, <br/>  y_test, <br/>  bootstrap_rounds = 100<br/>):</span><span id="3a5a" class="on mt it oi b gy os op l oq or">  <strong class="oi jd"># initialize dataframe for storing predictions on test data</strong></span><span id="b534" class="on mt it oi b gy os op l oq or">  preds_test = pd.DataFrame(index = y_test.index)<br/></span><span id="27f8" class="on mt it oi b gy os op l oq or"><strong class="oi jd">  # for each round: draw bootstrap indices, train model on bootstrap data and make predictions on test data</strong></span><span id="0d0f" class="on mt it oi b gy os op l oq or"><strong class="oi jd">  for</strong> r <strong class="oi jd">in range</strong>(bootstrap_rounds):<br/>    boot = np.random.randint(len(y_train), size = len(y_train))<br/>    preds_test[f'Model {r}'] = estimator.fit(X_train.iloc[boot, :], y_train.iloc[boot]).predict(X_test)</span><span id="6a4c" class="on mt it oi b gy os op l oq or">        <br/>  <strong class="oi jd"># calculate "average model"'s predictions</strong></span><span id="e7a1" class="on mt it oi b gy os op l oq or">  mean_pred_test = preds_test.mean(axis = 1)<br/></span><span id="184a" class="on mt it oi b gy os op l oq or"><strong class="oi jd">  # compute and return: mse, squared bias and variance</strong></span><span id="a08b" class="on mt it oi b gy os op l oq or">  mse = preds_test.apply(<strong class="oi jd">lambda </strong>pred_test: mean_squared_error(y_test, pred_test)).mean()<br/><br/>  bias_squared = mean_squared_error(y_test, mean_pred_test)<br/><br/>  variance = preds_test.apply(<strong class="oi jd">lambda </strong>pred_test: mean_squared_error(mean_pred_test, pred_test)).mean()<br/>    <br/>  <strong class="oi jd">return </strong>mse, bias_squared, variance</span></pre><h1 id="6e57" class="ms mt it bd mu mv mw mx my mz na nb nc ki nd kj ne kl nf km ng ko nh kp ni nj bi translated">在真实数据上</h1><p id="0215" class="pw-post-body-paragraph lh li it lj b lk nk kd lm ln nl kg lp lq nm ls lt lu nn lw lx ly no ma mb mc im bi translated">为了看到偏差-方差分解在起作用，让我们将它用于一些真实数据:来自<a class="ae ow" href="https://www.kaggle.com/c/house-prices-advanced-regression-techniques/overview" rel="noopener ugc nofollow" target="_blank"> Kaggle </a>的房价数据集。数据集由79个预测值组成(如建筑类别、一般分区分类、与物业相连的街道的直线英尺数、以平方英尺为单位的地块大小等)，目标是预测最终销售价格(以千美元为单位)。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ox"><img src="../Images/a90170e5bbf33ae88a47878d4931d7c9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hOx7V30RIVdxNxxKB0CpOA.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">房价数据集。[作者图]</p></figure><p id="44a5" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">让我们用一个预测算法，例如Scikit-learn的<code class="fe of og oh oi b">DecisionTreeRegressor</code>,看看偏差和方差如何根据模型的灵活性而变化。</p><p id="bd11" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">但是模型的灵活性意味着什么呢？答案取决于具体的算法及其超参数。</p><p id="1c99" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">在<code class="fe of og oh oi b">DecisionTreeRegressor</code>的情况下，我们可以拿<code class="fe of og oh oi b">min_samples_leaf</code>来举例。这个超参数决定了可以在决策树的任何末端叶子中结束的样本的最小数量。因此:</p><ul class=""><li id="dbe4" class="nr ns it lj b lk ll ln lo lq nt lu nu ly nv mc nw nx ny nz bi translated">当<code class="fe of og oh oi b">min_samples_leaf</code>为<strong class="lj jd">高</strong>时，意味着我们会有很少的顶生叶，每个顶生叶包含很多样本。因此，<strong class="lj jd">模型并不灵活</strong>，因为它被迫把许多不同的样本放在一起。</li><li id="2ad3" class="nr ns it lj b lk oa ln ob lq oc lu od ly oe mc nw nx ny nz bi translated">当<code class="fe of og oh oi b">min_samples_leaf</code>为<strong class="lj jd">低</strong>时，表示树很深。<strong class="lj jd">该模型非常灵活</strong>，因为它允许对任意几个样本进行不同的预测。</li></ul><p id="219e" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">所以这个想法是对<code class="fe of og oh oi b">min_samples_leaf</code>的不同选择做一个偏差和方差的估计。</p><p id="2283" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">我们可以使用之前创建的函数。或者，我们可以使用库<a class="ae ow" href="http://rasbt.github.io/mlxtend/" rel="noopener ugc nofollow" target="_blank"><strong class="lj jd">mlx tend</strong></a><strong class="lj jd"/>中的函数<code class="fe of og oh oi b">bias_variance_decomp</code>(<a class="ae ow" href="http://rasbt.github.io/mlxtend/user_guide/evaluate/bias_variance_decomp/" rel="noopener ugc nofollow" target="_blank">这里</a>你可以找到一个写得很漂亮的文档)。</p><pre class="ks kt ku kv gt oj oi ok ol aw om bi"><span id="377a" class="on mt it oi b gy oo op l oq or"><strong class="oi jd">import </strong>pandas <strong class="oi jd">as </strong>pd<br/><strong class="oi jd">from </strong>mlxtend.evaluate <strong class="oi jd">import </strong>bias_variance_decomp</span><span id="cef1" class="on mt it oi b gy os op l oq or">out = pd.DataFrame(columns = ['MSE', 'Bias^2', 'Variance'])</span><span id="631b" class="on mt it oi b gy os op l oq or"><strong class="oi jd">for </strong>min_samples_leaf <strong class="oi jd">in</strong> list(range(1, 11)) + list(range(15, 105, 5)):</span><span id="7376" class="on mt it oi b gy os op l oq or">  model = DecisionTreeRegressor(min_samples_leaf = min_samples_leaf)<br/>    <br/>  mse, bias, variance = bias_variance_decomp(<br/>    model, <br/>    X_train.to_numpy(), y_train.to_numpy(),<br/>    X_test.to_numpy(), y_test.to_numpy(), <br/>    loss = 'mse'<br/>  )<br/>    <br/>  out.loc[min_samples_leaf, 'Bias^2'] = bias<br/>  out.loc[min_samples_leaf, 'Variance'] = variance<br/>  out.loc[min_samples_leaf, 'MSE'] = mse</span></pre><p id="a589" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">这是结果:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi oy"><img src="../Images/bf47dce9b2ee22424d3ef281cb5443c9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*k8T4P9ra7NqtOOLmxM-m0Q.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">min_samples_leaf不同值的估计偏差和方差。[作者图]</p></figure><p id="4fa6" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">这正是我们所预期的:当模型的灵活性增加时(即当<code class="fe of og oh oi b">min_samples_leaf</code>减少时)，偏差趋于减少，但方差趋于增加。这是权衡的本质。在这两个极端之间找到正确的平衡是任何数据科学家的使命。</p></div><div class="ab cl oz pa hx pb" role="separator"><span class="pc bw bk pd pe pf"/><span class="pc bw bk pd pe pf"/><span class="pc bw bk pd pe"/></div><div class="im in io ip iq"><p id="b864" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">感谢您的阅读！我希望本演练有助于您深入理解偏差-方差权衡。</p><p id="edf8" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">我感谢反馈和建设性的批评。如果你想谈论这篇文章或其他相关话题，可以发短信到<a class="ae ow" href="https://www.linkedin.com/in/samuelemazzanti/" rel="noopener ugc nofollow" target="_blank">我的Linkedin联系人</a>给我。</p></div></div>    
</body>
</html>