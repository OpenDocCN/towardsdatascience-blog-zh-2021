<html>
<head>
<title>KeyBERT: Keyword Extraction using BERT</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">KeyBERT:使用BERT提取关键词</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/keybert-keyword-extraction-using-bert-a6dc3dd38caf?source=collection_archive---------7-----------------------#2021-12-02">https://towardsdatascience.com/keybert-keyword-extraction-using-bert-a6dc3dd38caf?source=collection_archive---------7-----------------------#2021-12-02</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="53a6" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">解码NLP库—视觉效果和示例</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/dcfed5b3e2d6a2801e2d9be9aea95c17.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sadGAw2e6inZp2gmeoC2kA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">来自<a class="ae ky" href="https://unsplash.com/photos/W8KTS-mhFUE" rel="noopener ugc nofollow" target="_blank">源</a>的修改图像</p></figure><p id="b4c6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">关键词提取的任务是识别最能代表源文档的<strong class="lb iu">重要术语或短语。识别好的关键词不仅有助于准确描述文档的内容，还可以通过将关键词作为元数据存储到原始文档中来帮助<strong class="lb iu">加快信息检索</strong>。在这个领域已经做了很多研究。其中之一就是我们今天要讨论的。</strong></p><blockquote class="lv lw lx"><p id="8910" class="kz la ly lb b lc ld ju le lf lg jx lh lz lj lk ll ma ln lo lp mb lr ls lt lu im bi translated">在这篇博客中，我们将讨论一种流行的基于BERT的关键字提取技术/库，称为<strong class="lb iu"> KeyBERT </strong>。</p></blockquote><p id="82c1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="ly">没时间看完整个博客？然后看这个快速的&lt; 60秒的YouTube短片—</em></p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mc md l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><a class="ae ky" href="https://www.youtube.com/channel/UCoz8NrwgL7U9535VNc0mRPA" rel="noopener ugc nofollow" target="_blank">多看看这样的视频</a></p></figure><h1 id="ac62" class="me mf it bd mg mh mi mj mk ml mm mn mo jz mp ka mq kc mr kd ms kf mt kg mu mv bi translated">BERT概述</h1><p id="ff4c" class="pw-post-body-paragraph kz la it lb b lc mw ju le lf mx jx lh li my lk ll lm mz lo lp lq na ls lt lu im bi translated"><strong class="lb iu"> BERT </strong>又名变压器的双向编码器表示是一个仅编码器模型，旨在从未标记的文本中学习<strong class="lb iu">文本段的深度双向表示。它接受了两项任务的预训练——1 .<strong class="lb iu"> MLM </strong>(蒙面语言造型)2。<strong class="lb iu"> NSP </strong>(下一句预测)</strong></p><p id="3a27" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">其中MLM的任务使得模型能够基于在其上下文中出现的单词来学习输入中每个单词/记号的表示。NSP的任务是说，如果给两个句子，然后知道第二个句子是否跟在第一个后面。在这个预训练步骤之后，<strong class="lb iu">你可以在任何下游任务</strong>中对这个模型进行微调，比如分类等等。你可以在<a class="ae ky" rel="noopener" target="_blank" href="/understanding-masked-language-models-mlm-and-causal-language-models-clm-in-nlp-194c15f56a5"> MLM vs CLM </a>和<a class="ae ky" href="https://www.geeksforgeeks.org/understanding-bert-nlp/" rel="noopener ugc nofollow" target="_blank">BERT了解NLP中的BERT</a>中阅读更多关于掩蔽语言建模的内容。</p><h1 id="77a1" class="me mf it bd mg mh mi mj mk ml mm mn mo jz mp ka mq kc mr kd ms kf mt kg mu mv bi translated">基于BERT的关键词提取</h1><p id="10d3" class="pw-post-body-paragraph kz la it lb b lc mw ju le lf mx jx lh li my lk ll lm mz lo lp lq na ls lt lu im bi translated">因此<a class="ae ky" href="https://maartengr.github.io/KeyBERT/index.html" rel="noopener ugc nofollow" target="_blank"> KeyBERT </a>是一个关键字提取库，它利用BERT嵌入来获取最能代表底层文本文档的关键字。此外，这个库处理这项任务的方式——你可以说这是一种从给定文本中获取关键字的<strong class="lb iu">无监督提取方式。</strong></p><p id="b0ec" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在很高的层面上，KeyBERT的工作是下图所示的<em class="ly">—</em></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nb"><img src="../Images/31f1fadb311d0153ddd0c34ff482f163.png" data-original-src="https://miro.medium.com/v2/resize:fit:600/format:webp/1*N16cc_jCLz3lj15eQDr_PQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">KeyBERT Internals |作者图片</p></figure><p id="bfde" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">整个步骤由3个连续的步骤组成。首先，我们从<strong class="lb iu">开始，从提供给我们的用于提取关键词的底层文本语料库中提取n-grams </strong>。n元文法不过是字符串中n个连续记号的序列。作者使用CountVectorizer来获得候选n元文法的列表。CountVectorizer根据n元语法在原始文档中的出现频率对其进行排序。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nc"><img src="../Images/67f6448c6a529bed8b661810126445ac.png" data-original-src="https://miro.medium.com/v2/resize:fit:648/format:webp/1*muUbwsq3dbetgPqa25drQw.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">N-grams作为关键字|作者图片</p></figure><p id="79e4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在此之后，使用BERT模型将从上一步提取的所有n元语法转换为它们的高维表示。下一步是计算每个n元语法和原始文本文档之间的<strong class="lb iu">语义距离</strong>。<em class="ly">相似度越高，关键词与源文档的相关性和代表性就越强。</em></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nd"><img src="../Images/e77df1bc2187ac0d0e07f235c28f761b.png" data-original-src="https://miro.medium.com/v2/resize:fit:724/format:webp/1*U0AZpLM5XoHnAs37rK49cA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">确保提取的关键字|作者图片的相关性</p></figure><p id="80f3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">因此，在根据这些n元语法与底层文档的相关程度获得这些n元语法的排序列表后，下一步是根据 <a class="ae ky" href="https://www.youtube.com/watch?v=ykClwtoLER8" rel="noopener ugc nofollow" target="_blank"> <strong class="lb iu">【最大边距相关度】</strong> </a> <em class="ly">(检查时间戳的描述)</em> <strong class="lb iu">或最大和策略</strong>对它们进行重新排序。其中这背后的核心思想是<em class="ly">最小化n-gram到原始文档的距离，但同时最大化与其他候选n-gram的距离</em>。这确保了我们不会在最终的集合中输出相似含义的n元语法作为可能的关键字，也就是说<strong class="lb iu">确保多样性</strong>。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ne"><img src="../Images/870865d808896caa62cc1df493a0a8e4.png" data-original-src="https://miro.medium.com/v2/resize:fit:632/format:webp/1*uFfVoSmQDc71Mj7YUk46AQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">确保提取关键字的多样性|按作者分类的图片</p></figure><h1 id="b5f3" class="me mf it bd mg mh mi mj mk ml mm mn mo jz mp ka mq kc mr kd ms kf mt kg mu mv bi translated">我的想法</h1><p id="113d" class="pw-post-body-paragraph kz la it lb b lc mw ju le lf mx jx lh li my lk ll lm mz lo lp lq na ls lt lu im bi translated">看到使用上下文表示来获取相关关键词是很有趣的。虽然我觉得这将是很有趣的，看看它比非上下文化的表示好多少，以及平均起来这种方法如何优于早期基于图的关键字提取工作。但肯定的是，多元化步骤在这一研究领域发挥了至关重要的作用。</p><p id="c110" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果你感兴趣的话，可以随意阅读我不久前写的另一篇关于关键词提取的优美文章——<a class="ae ky" href="https://medium.com/mlearning-ai/10-popular-keyword-extraction-algorithms-in-natural-language-processing-8975ada5750c" rel="noopener">自然语言处理中10种流行的关键词提取算法</a></p><h1 id="7268" class="me mf it bd mg mh mi mj mk ml mm mn mo jz mp ka mq kc mr kd ms kf mt kg mu mv bi translated">Github项目</h1><p id="9a17" class="pw-post-body-paragraph kz la it lb b lc mw ju le lf mx jx lh li my lk ll lm mz lo lp lq na ls lt lu im bi translated"><a class="ae ky" href="https://maartengr.github.io/KeyBERT/index.html" rel="noopener ugc nofollow" target="_blank">https://maartengr.github.io/KeyBERT/index.html</a></p></div><div class="ab cl nf ng hx nh" role="separator"><span class="ni bw bk nj nk nl"/><span class="ni bw bk nj nk nl"/><span class="ni bw bk nj nk"/></div><div class="im in io ip iq"><p id="027b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我希望你喜欢读这篇文章。如果你愿意支持我成为一名作家，考虑注册<a class="ae ky" href="https://prakhar-mishra.medium.com/membership" rel="noopener">成为一名媒体成员</a>。每月只需5美元，你就可以无限制地使用Medium。<em class="ly"> </em>谢谢！</p></div></div>    
</body>
</html>