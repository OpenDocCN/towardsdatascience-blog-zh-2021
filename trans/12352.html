<html>
<head>
<title>Accelerating Computer Vision: How we scaled EfficientNet to IPU-POD Supercomputing Systems</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">加速计算机视觉:我们如何将EfficientNet扩展到IPU-波德超级计算系统</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/accelerating-computer-vision-how-we-scaled-efficientnet-to-ipu-pod-supercomputing-systems-e4e6b506b671?source=collection_archive---------27-----------------------#2021-12-15">https://towardsdatascience.com/accelerating-computer-vision-how-we-scaled-efficientnet-to-ipu-pod-supercomputing-systems-e4e6b506b671?source=collection_archive---------27-----------------------#2021-12-15</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="7ff4" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">更快的大规模图像处理</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/159863f879f5d7a44b47fee2367ef444.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*s0c0vcIDf9hsNshrufKZ5w.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图片作者。</p></figure><p id="017c" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">Graphcore研究展示了我们如何在Graphcore最新的超大规模<a class="ae lr" href="https://www.graphcore.ai/posts/the-next-big-thing-introducing-ipu-pod128-and-ipu-pod256" rel="noopener ugc nofollow" target="_blank"> IPU-POD128和IPU-POD256系统</a>上加速训练创新的计算机视觉模型EfficientNet-B4，在不到两个小时内达到收敛。</p><h1 id="96f2" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">为什么选择EfficientNet？</h1><p id="3112" class="pw-post-body-paragraph kv kw iq kx b ky mk jr la lb ml ju ld le mm lg lh li mn lk ll lm mo lo lp lq ij bi translated"><a class="ae lr" href="https://arxiv.org/abs/1905.11946" rel="noopener ugc nofollow" target="_blank"> EfficientNet系列模型</a> [1]展示了计算机视觉的最新发展水平，以相对较少的参数和失败实现了高任务性能。然而，它在实践中的采用受到以下事实的限制:传统的处理器架构不能利用使EfficientNet如此高效的许多属性。</p><p id="f3ac" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">例如，与标准卷积运算相比，EfficientNets中的深度方向或<a class="ae lr" href="https://arxiv.org/abs/2106.03640" rel="noopener ugc nofollow" target="_blank">组</a>卷积具有很高的表达能力和计算效率，但算术强度较低(计算与数据移动的比率)。由于内存和处理器内核之间的大量数据传输，这些类型的操作在GPU上的性能很差。由于IPU以内存为中心的架构，整个模型及其激活都可以保留在芯片上，从而缓解了这种昂贵的数据移动。</p><p id="551d" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">此外，IPU的MIMD(多指令、多数据)范式支持模型和训练过程的多维度上的细粒度并行性。这有助于实现高吞吐量，同时使用小卷积并并行处理非常少的数据样本。相比之下，具有SIMD(单指令，多数据)架构的GPU在利用并行性的方式上受到限制，迫使用户在他们的机器学习算法的设计和硬件上该算法可达到的吞吐量之间进行权衡。</p><p id="d6f9" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">IPU硬件上的这种加速为更多的创新者和人工智能从业者带来了机会，他们可以从EfficientNet models的高效率中受益。</p><p id="e190" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">Graphcore在GitHub上的例子提供了广泛的流行模型，任何人都可以在IPU上直接使用，包括EfficientNet模型家族。在这个练习中，我们考虑EfficientNet-B4，它通常用于人工智能领域来测试EfficientNet性能。</p><h1 id="9b26" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">优化吞吐量</h1><h2 id="5ff7" class="mp lt iq bd lu mq mr dn ly ms mt dp mc le mu mv me li mw mx mg lm my mz mi na bi translated">分发模型</h2><p id="27ae" class="pw-post-body-paragraph kv kw iq kx b ky mk jr la lb ml ju ld le mm lg lh li mn lk ll lm mo lo lp lq ij bi translated">有许多方法可以在一组IPU上分发模型，包括数据并行复制和<a class="ae lr" href="https://docs.graphcore.ai/projects/tf-model-parallelism/en/latest/pipelining.html#pipelining" rel="noopener ugc nofollow" target="_blank">管道模型并行</a>。给定EfficientNet-B4的大小和IPU-pod的规模，数据和模型并行性的某种组合可以获得最佳结果。Poplar软件堆栈使得在框架级别尝试不同的分布式设置变得容易，加快了我们对配置各种类型并行性的最佳方式的搜索。</p><p id="9f42" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">GitHub示例中EfficientNet-B4的默认配置通过四个IPU来传输模型。拥有多级管道意味着需要花费更多时间来填充和排空管道，从而降低了IPU的整体利用率。这也意味着需要更加注意平衡IPU之间的工作量。如果我们能够将流水线阶段的数量减少到两个，这些挑战是可以解决的。这也使我们能够将副本数量增加一倍。</p><p id="67cd" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">为了使EfficientNet-B4仅适用于两个IPU，我们采用了三种技术:</p><ul class=""><li id="fbe8" class="nb nc iq kx b ky kz lb lc le nd li ne lm nf lq ng nh ni nj bi translated">使用16位浮点运算和主权重</li><li id="9467" class="nb nc iq kx b ky nk lb nl le nm li nn lm no lq ng nh ni nj bi translated">减少本地批量大小</li><li id="fce9" class="nb nc iq kx b ky nk lb nl le nm li nn lm no lq ng nh ni nj bi translated">使用Graphcore Research的“让EfficientNet更高效”<a class="ae lr" rel="noopener" target="_blank" href="/how-we-made-efficientnet-more-efficient-61e1bf3f84b3">博客</a>和<a class="ae lr" href="https://arxiv.org/abs/2106.03640" rel="noopener ugc nofollow" target="_blank">论文</a>【2】中探索的G16版本的EfficientNet</li></ul><p id="041c" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">IPU本身支持16位和32位浮点数表示，允许用户灵活调整应用中各种张量的表示。在某些情况下，可能有必要存储FP32主重量，其中在整个训练过程中以完全精确的方式存储模型参数的副本。我们发现，对于EfficientNet，当使用FP16主权重并在权重更新中采用随机舍入时，性能仍然可以保持不变。</p><p id="9e27" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">由于存储激活所需的内存随批量大小而增加，因此考虑如何在不影响培训的情况下减少激活开销非常重要。激活重新计算是一种我们可以在需要的时候重新计算激活的方法。这提供了计算和内存之间的简单权衡。这种方法在Poplar中可用，并且可以通过管道API在框架级别轻松访问。使用EfficientNet-B4，我们可以使用激活重新计算来适应本地批处理大小3。与四级管道设置相比，这种配置允许并行处理更多的样本，因为我们有两倍多的副本来分发小批量。对于这项工作，我们使用组规范，一个批次独立的标准化方法。由于该模型没有任何批间依赖性，因此我们可以拥有任何适合内存的本地批大小，并简单地累积梯度以实现我们想要的任何全局批大小，从而允许我们在不影响模型的训练动态的情况下在内存中调整我们的激活。</p><p id="7c21" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">通过将卷积组的大小从1增加到16(并随后降低扩展比以补偿触发器和参数的增加)，我们降低了EfficientNet中MBConv模块的内存开销。这在Graphcore Research的“<a class="ae lr" href="https://arxiv.org/abs/2106.03640" rel="noopener ugc nofollow" target="_blank">让EfficientNet更高效</a>”论文中有更详细的探讨，并在我们的<a class="ae lr" rel="noopener" target="_blank" href="/how-we-made-efficientnet-more-efficient-61e1bf3f84b3">博客</a>中有总结。除了节省内存之外，该模型的这种变体还有增加ImageNet验证准确性的额外好处。</p><p id="8b5a" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">这三种技术允许我们仅在两个IPU上拟合模型，而不必将任何优化器状态卸载到流存储器，从而允许以高吞吐量进行训练。</p><h2 id="ba05" class="mp lt iq bd lu mq mr dn ly ms mt dp mc le mu mv me li mw mx mg lm my mz mi na bi translated">数据输入输出</h2><p id="1033" class="pw-post-body-paragraph kv kw iq kx b ky mk jr la lb ml ju ld le mm lg lh li mn lk ll lm mo lo lp lq ij bi translated">当在像IPU-POD这样强大的人工智能加速器系统上训练机器学习模型时，一个常见的瓶颈是从主机向模型提供足够的数据来处理。PopRun是一个命令行实用程序，它通过在多个程序实例上以分布式方式启动应用程序来帮助缓解这一瓶颈。每个实例的I/O由其对应的主机服务器管理，这允许我们将模型扩展到各种IPU-POD系统，而不受我们向模型提供数据的速度的限制。</p><p id="a7ca" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">虽然ImageNet数据集最初以INT-8表示，但作为预处理的一部分，它通常在主机上被转换为更高精度的浮点数据类型。一旦数据位于IPU上，通过应用这种转换，通信开销就会降低，因为输入数据可以以较低的精度流式传输到IPU。这有助于提高向模型提供数据的速率，从而进一步提高吞吐量。</p><p id="8bf8" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">IPU内部和之间的数据移动是在<a class="ae lr" href="https://share.vidyard.com/watch/9aDEUsLyYpJjRbGJyt94zd" rel="noopener ugc nofollow" target="_blank">批量同步并行</a> (BSP)执行方案下实现的。有了BSP，<em class="np">瓦片</em> (IPU处理器内核)在本地计算和与其他瓦片的数据交换之间交替，中间有一个同步步骤。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nq"><img src="../Images/2557925d30cf7f646c2cd6d5cee8324d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_lUi23A9vSN5TUEnG-DGnw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">批量同步并行(BSP)执行方案。图片作者。</p></figure><p id="8eee" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">这种执行方案使Poplar能够高效地并行处理数十万个图块。对于I/O受限的应用，跨越所有瓦片的天真BSP范例会抑制性能。Poplar通过允许一部分瓦片以异步方式专用于流数据，而剩余的计算瓦片在BSP范式下执行来解决这一问题。重叠I/O缓解了性能瓶颈，同时仍然允许高度可伸缩的执行方案。我们发现，将每个IPU 1472个切片中的32个分配给I/O重叠，并预取多达三批数据，结合上述方法，可以在所有系统规模下实现令人印象深刻的吞吐量。</p><h1 id="da2b" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">调整批量大小</h1><p id="acfc" class="pw-post-body-paragraph kv kw iq kx b ky mk jr la lb ml ju ld le mm lg lh li mn lk ll lm mo lo lp lq ij bi translated">要跨大型系统(如IPU-波德128和IPU-波德256)进行训练，使用大的全局批量是有益的。这使我们能够并行处理多个副本上的数据样本，同时确保每个副本都有足够的工作来保持效率。此外，为了分摊减少副本之间的梯度的成本，我们在副本之间通信和更新权重之前，在多个前向和后向通道上本地累积梯度，这被称为梯度累积<em class="np"> </em> (GA)。因此，全局批量大小是本地批量大小、副本数量和梯度累积计数的乘积。</p><p id="6613" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我们的基线来自“让EfficientNet更高效”，使用的全球批量为768。给定我们的本地批次大小为3，这将在IPU-POD256上每次训练迭代仅产生两个本地批次，未充分利用流水线设置，并且必须应对频繁的高成本重量更新。然而，简单地增加全局批量大小会导致泛化性能下降。这种现象在许多机器学习应用中很常见，<a class="ae lr" href="https://arxiv.org/abs/1804.07612" rel="noopener ugc nofollow" target="_blank">特别是在计算机视觉中</a> [3]。因此，我们寻求一个足够大的全球批量，以维持所有IPU-POD系统的高通量，但又足够小，以获得良好的统计效率。</p><p id="0900" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">众所周知，<a class="ae lr" href="https://arxiv.org/abs/1811.03600" rel="noopener ugc nofollow" target="_blank">优化器对大批量训练的鲁棒性有很大影响</a> [4]。最初的EfficientNet论文使用了RMSProp优化器，在我们的研究中，当增加批量时，它很难保持统计效率。<a class="ae lr" href="https://arxiv.org/abs/2011.00071" rel="noopener ugc nofollow" target="_blank">王帕尼奇等人</a>。[5]进行了类似的观察，并建议使用<a class="ae lr" href="https://arxiv.org/abs/1708.03888" rel="noopener ugc nofollow" target="_blank">分层自适应速率缩放</a> (LARS) [6]，这是一种已知的优化器，可以很好地训练大批量的视觉模型。LARS在逐层的基础上缩放学习率，以确保权重和权重更新之间的相似幅度。除了LARS，我们还采用了多项式衰减学习率计划，通常与优化器结合使用。</p><p id="3228" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">通过对批量大小、学习速率、预热时期的数量、动量系数和重量衰减进行超参数扫描，我们发现，与我们最初的EfficientNet-B4实施相比，我们可以实现6144的全局批量大小，而不会有任何性能损失。在此批量下，所有考虑的IPU容器都可以保持高梯度累积计数。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nr"><img src="../Images/82807c8ab7b904fb6a05fd0ce6fb538c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-ZL2PoOFWm5t2eFnm5mw8w.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图片作者。</p></figure><h1 id="9f26" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">绩效结果</h1><p id="47c6" class="pw-post-body-paragraph kv kw iq kx b ky mk jr la lb ml ju ld le mm lg lh li mn lk ll lm mo lo lp lq ij bi translated">借助灵活的超参数配置，我们现在能够在一系列IPU-POD系统上以高吞吐量训练EfficientNet-B4。根据最初的效率网论文，我们训练了350个纪元。虽然“让EfficientNet更高效”展示了EfficientNet系列模型在以其原始分辨率进行微调之前，可以在较低的图像分辨率下进行预调整，但我们保持了原始分辨率，以便与其他实施保持一致。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ns"><img src="../Images/048606ae95f192d43ec235895ca9715a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*P4_BCT3Q2WcFysSq9shjvA.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图片作者。</p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nt"><img src="../Images/b224d297da1e482907284d4134ddb783.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Nn3XWQ_1Xw6KRBjM7XinZw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图片作者。</p></figure><p id="842e" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">所有的实验都使用相同的基本机器学习超参数配置，通过根据IPU荚的大小缩放复制品的数量和梯度累积计数。结果都收敛了，验证准确率为82.54±0.13%。</p><p id="d64c" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">上述结果显示了EfficientNet在基于IPU的系统上的训练速度，并与领先的GPU硬件上的结果进行了对比。能够快速训练模型对于创新者快速迭代和测试新想法至关重要——我们可以在不到两个小时的时间内训练EfficientNet-B4，将创新速度从几天加快到几小时。</p><p id="33d2" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">这种加速表明，当不受传统处理器架构的限制时，许多计算机视觉应用可以更快地进行大规模训练。像EfficientNet这样的下一代计算机视觉模型，当与Graphcore的IPU-POD系统等新型硬件一起使用时，可以帮助加速大量与视觉相关的用例，从CT扫描分析和视频升级到故障诊断和保险索赔验证。</p><p id="53d0" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">这些配置现在<a class="ae lr" href="https://github.com/graphcore/examples/tree/master/vision/cnns/tensorflow1/training" rel="noopener ugc nofollow" target="_blank">可以在Graphcore的GitHub示例上</a>试用。</p><h1 id="8fbe" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">谢谢你</h1><p id="23c3" class="pw-post-body-paragraph kv kw iq kx b ky mk jr la lb ml ju ld le mm lg lh li mn lk ll lm mo lo lp lq ij bi translated">感谢<a class="ae lr" href="https://medium.com/@dominic.masters.gc" rel="noopener"> Dominic Masters </a>和Carlo Luschi，他们也为这项研究做出了贡献，感谢我们在Graphcore的其他同事的支持和见解。</p><h1 id="e68e" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">参考</h1><p id="324c" class="pw-post-body-paragraph kv kw iq kx b ky mk jr la lb ml ju ld le mm lg lh li mn lk ll lm mo lo lp lq ij bi translated">[1] M. Tan，Q. V. Le，<a class="ae lr" href="https://arxiv.org/abs/1905.11946" rel="noopener ugc nofollow" target="_blank"> EfficientNet:重新思考卷积神经网络的模型缩放</a>，arXiv 2019</p><p id="0b8a" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">[2] D. Masters，A. Labatie，Z. Eaton-Rosen，C. Luschi，<a class="ae lr" href="https://arxiv.org/abs/2106.03640" rel="noopener ugc nofollow" target="_blank">使效率网更有效率:探索独立于批处理的标准化、组卷积和降低分辨率训练</a>，arXiv 2021</p><p id="c9ce" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">[3] D. Masters，C. Luschi，<a class="ae lr" href="https://arxiv.org/abs/1804.07612" rel="noopener ugc nofollow" target="_blank">重温深度神经网络的小批量训练</a>，arXiv 2018</p><p id="b206" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">[4] C. J. Shallue，J. Lee，J. Antognini，J. Sohl-Dickstein，R. Frostig，G. E. Dahl，<a class="ae lr" href="https://arxiv.org/abs/1811.03600" rel="noopener ugc nofollow" target="_blank">测量数据并行性对神经网络训练的影响</a>，arXiv 2018</p><p id="f484" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">[5] A. Wongpanich，H. Pham，J. Demmel，M. Tan，Q. Le，Y. You，S. Kumar，<a class="ae lr" href="https://arxiv.org/abs/2011.00071" rel="noopener ugc nofollow" target="_blank">超级计算机规模的训练效率网络:一小时内83%的ImageNet Top-1准确性</a>，arXiv 2020</p><p id="3c20" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">[6] Y. You，I. Gitman，B. Ginsburg，<a class="ae lr" href="https://arxiv.org/abs/1708.03888" rel="noopener ugc nofollow" target="_blank">卷积网络的大批量训练</a>，arXiv 2017</p></div></div>    
</body>
</html>