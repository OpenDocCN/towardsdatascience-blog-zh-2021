<html>
<head>
<title>Implementing Part of Speech Tagging for English Words Using Viterbi Algorithm from Scratch</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">从零开始用 Viterbi 算法实现英语单词的词性标注</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/implementing-part-of-speech-tagging-for-english-words-using-viterbi-algorithm-from-scratch-9ded56b29133?source=collection_archive---------11-----------------------#2021-10-28">https://towardsdatascience.com/implementing-part-of-speech-tagging-for-english-words-using-viterbi-algorithm-from-scratch-9ded56b29133?source=collection_archive---------11-----------------------#2021-10-28</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="7ca0" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">词类是句子结构和意义的有用线索。以下是我们识别它们的方法。</h2></div><p id="3636" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">本文的完整代码可以在<a class="ae lb" href="https://github.com/cleopatra27/pos_tagging_viterbi" rel="noopener ugc nofollow" target="_blank">这里</a>找到</p><p id="62e8" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">词性标注是为文本中的每个单词分配词性的过程，这是一项消除歧义的任务，单词可能有多个词性，我们的目标是找到适合这种情况的正确标签。</p><p id="31fb" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们将使用一个经典的序列标记算法，<a class="ae lb" href="https://en.wikipedia.org/wiki/Hidden_Markov_model" rel="noopener ugc nofollow" target="_blank"> <strong class="kh ir">隐马尔可夫模型</strong> </a>来演示，序列标记是一个任务，其中我们给输入单词序列中的每个单词 x1 分配一个标签 y1，因此输出序列 Y 与输入序列 x 具有相同的长度。HMM 是一个基于扩充<a class="ae lb" href="https://en.wikipedia.org/wiki/Markov_chain" rel="noopener ugc nofollow" target="_blank">马尔可夫链</a>的概率序列模型。马尔可夫链做了一个很强的假设，如果我们想预测序列中的未来，唯一重要的是当前状态。例如，预测我下周写一篇文章的概率取决于我本周写一篇文章，仅此而已。你可以观看视频<a class="ae lb" href="https://www.youtube.com/watch?v=fX5bYmnHqqE&amp;t=15s" rel="noopener ugc nofollow" target="_blank">这里</a>更多的例子，我会说一个更详细的解释。</p><p id="0660" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">隐马尔可夫模型允许我们谈论观察到的事件——我们在 put 中看到的单词和隐藏事件——我们认为是概率模型中偶然因素的部分语音标签。</p><p id="a02f" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们将使用<a class="ae lb" href="https://en.wikipedia.org/wiki/Brown_Corpus" rel="noopener ugc nofollow" target="_blank"> Brown Corpus </a>来实现我们的 tagger，其中每个文件包含标记化单词的句子，后跟 POS 标签，每行包含一个句子。您可以在这里找到描述标签<a class="ae lb" href="http://korpus.uib.no/icame/manuals/BROWN/INDEX.HTM" rel="noopener ugc nofollow" target="_blank">的数据手册。注意，我们将使用二元模型 HMM 实现我们的 POS 标记器。</a></p><p id="3bce" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们可以在下面看到我们的数据样本:</p><pre class="lc ld le lf gt lg lh li lj aw lk bi"><span id="a351" class="ll lm iq lh b gy ln lo l lp lq">[('RB', 'manifestly'), ('VB', 'admit')]</span></pre><p id="3f19" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">首先，让我们创建一个生成 n 元文法的函数。</p><pre class="lc ld le lf gt lg lh li lj aw lk bi"><span id="533f" class="ll lm iq lh b gy ln lo l lp lq">def ngrams(self, text, n):<br/>    n_grams = []<br/>    for i in range(len(text)): n_grams.append(tuple(text[i: i + n]))<br/>    return n_grams</span></pre><h2 id="ce57" class="ll lm iq bd lr ls lt dn lu lv lw dp lx ko ly lz ma ks mb mc md kw me mf mg mh bi translated">转移概率</h2><figure class="lc ld le lf gt mj gh gi paragraph-image"><div class="gh gi mi"><img src="../Images/568d478e36dcd782764bd5f84361c80e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*kg8KOD9s6hLsEcNt3ibbfQ.png"/></div><p class="mm mn gj gh gi mo mp bd b be z dk translated">作者图片</p></figure><p id="15ea" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">一个 HMM 有两个组成部分，即<strong class="kh ir"> <em class="mq">跃迁概率</em> </strong> <em class="mq"> A </em>和<strong class="kh ir"> <em class="mq">发射概率</em> </strong> <em class="mq"> B </em>。</p><figure class="lc ld le lf gt mj gh gi paragraph-image"><div role="button" tabindex="0" class="ms mt di mu bf mv"><div class="gh gi mr"><img src="../Images/2e564bc0812a5fd3c89429de5b1b7425.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Yr8IhqcAkJZtuVm7lY6hdA@2x.jpeg"/></div></div><p class="mm mn gj gh gi mo mp bd b be z dk translated">作者图片</p></figure><p id="0d10" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">转移概率是给定前一个标签时标签出现的概率，例如，动词<strong class="kh ir"> <em class="mq">将</em> </strong>最有可能后跟另一种形式的动词，如<strong class="kh ir"> <em class="mq">舞</em> </strong>，因此它将具有高概率。我们可以使用上面的等式来计算这个概率，实现如下:</p><figure class="lc ld le lf gt mj"><div class="bz fp l di"><div class="mw mx l"/></div></figure><p id="c081" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这里，我们将二元模型的计数除以我们创建的每个二元模型的一元模型计数，并将其存储在<em class="mq"> transition_probabilities </em>字典中。</p><h2 id="1f5f" class="ll lm iq bd lr ls lt dn lu lv lw dp lx ko ly lz ma ks mb mc md kw me mf mg mh bi translated">排放概率</h2><figure class="lc ld le lf gt mj gh gi paragraph-image"><div class="gh gi mi"><img src="../Images/90f820a00dfbc6ea1c36aea4bb1722cc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*IJBxnR9TXyPEBrm6jAAxoQ.png"/></div><p class="mm mn gj gh gi mo mp bd b be z dk translated">作者图片</p></figure><p id="c1fd" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">发射概率是给定标签，它将与给定单词相关联的概率。我们可以使用上面的等式来计算这个概率，实现如下:</p><figure class="lc ld le lf gt mj"><div class="bz fp l di"><div class="mw mx l"/></div></figure><p id="71c2" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在这里，我们将该单词后面的标签的计数除以同一标签的计数，并将其存储在<em class="mq"> emission_probabilities </em>字典中。</p><p id="69b1" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">HMM taggers 做了两个进一步简化的假设。第一个假设是，一个单词出现的概率只取决于它自己的标签，而与相邻的单词和标签无关；第二个假设，二元模型假设，是一个标签的概率只取决于前一个标签，而不是整个标签序列。将这两个假设代入我们的 bigram 标记器，得到最可能的标记序列的以下等式:</p><figure class="lc ld le lf gt mj gh gi paragraph-image"><div role="button" tabindex="0" class="ms mt di mu bf mv"><div class="gh gi my"><img src="../Images/e32b9be8a1dac3a0aa5a6f73246a98bf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VzlW0_xPvtotqLkZNaxUUw.png"/></div></div><p class="mm mn gj gh gi mo mp bd b be z dk translated">作者图片</p></figure><h2 id="b06b" class="ll lm iq bd lr ls lt dn lu lv lw dp lx ko ly lz ma ks mb mc md kw me mf mg mh bi translated">解码 HMM</h2><p id="a471" class="pw-post-body-paragraph kf kg iq kh b ki mz jr kk kl na ju kn ko nb kq kr ks nc ku kv kw nd ky kz la ij bi translated">对于任何模型，例如包含隐藏变量(词性)的 HMM，确定对应于观察序列的隐藏变量序列的任务称为解码，这是使用<a class="ae lb" href="https://en.wikipedia.org/wiki/Viterbi_algorithm" rel="noopener ugc nofollow" target="_blank"> <strong class="kh ir"> <em class="mq">维特比算法</em> </strong> </a> <strong class="kh ir"> <em class="mq">完成的。</em> </strong></p><h2 id="e121" class="ll lm iq bd lr ls lt dn lu lv lw dp lx ko ly lz ma ks mb mc md kw me mf mg mh bi translated"><strong class="ak"> <em class="ne">维特比算法</em> </strong></h2><p id="632d" class="pw-post-body-paragraph kf kg iq kh b ki mz jr kk kl na ju kn ko nb kq kr ks nc ku kv kw nd ky kz la ij bi translated"><strong class="kh ir">维特比算法</strong>是一种<a class="ae lb" href="https://en.wikipedia.org/wiki/Dynamic_programming" rel="noopener ugc nofollow" target="_blank">动态规划</a> <a class="ae lb" href="https://en.wikipedia.org/wiki/Algorithm" rel="noopener ugc nofollow" target="_blank">算法</a>，用于获得最有<a class="ae lb" href="https://en.wikipedia.org/wiki/Likelihood_function" rel="noopener ugc nofollow" target="_blank">可能的</a>隐藏状态序列的<a class="ae lb" href="https://en.wikipedia.org/wiki/Maximum_a_posteriori_estimation" rel="noopener ugc nofollow" target="_blank">最大后验概率估计</a>—称为<strong class="kh ir">维特比路径</strong>——其产生一系列观察到的事件，特别是在<a class="ae lb" href="https://en.wikipedia.org/wiki/Markov_information_source" rel="noopener ugc nofollow" target="_blank">马尔可夫信息源</a>和<a class="ae lb" href="https://en.wikipedia.org/wiki/Hidden_Markov_model" rel="noopener ugc nofollow" target="_blank">隐藏马尔可夫模型</a> (HMM)的环境中。此外，一个很好的解释视频可以找到<a class="ae lb" href="https://www.youtube.com/watch?v=IqXdjdOgXPM&amp;t=498s" rel="noopener ugc nofollow" target="_blank">这里</a></p><p id="5b5e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">维特比解码有效地从指数多的可能性中确定最可能的路径。它通过查看我们的传输和发射概率，将这些概率相乘，然后找到最大概率，从而找到一个单词相对于所有标签的最高概率。我们将为未知概率定义一个默认值<strong class="kh ir"><em class="mq">0.000000000001</em></strong>。</p><p id="8f65" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们将从计算初始概率/该状态的开始概率开始，这是单词开始句子的概率，在我们的例子中，我们使用了“开始”标记</p><pre class="lc ld le lf gt lg lh li lj aw lk bi"><span id="288a" class="ll lm iq lh b gy ln lo l lp lq">def initial_probabilities(self, tag):<br/>    return self.transition_probabilities["START", tag]</span></pre><figure class="lc ld le lf gt mj"><div class="bz fp l di"><div class="mw mx l"/></div></figure><h2 id="18b2" class="ll lm iq bd lr ls lt dn lu lv lw dp lx ko ly lz ma ks mb mc md kw me mf mg mh bi translated"><strong class="ak">测试</strong></h2><p id="28b7" class="pw-post-body-paragraph kf kg iq kh b ki mz jr kk kl na ju kn ko nb kq kr ks nc ku kv kw nd ky kz la ij bi translated">为了测试我们的解决方案，我们将使用一个已经分解成单词的句子，如下所示:</p><pre class="lc ld le lf gt lg lh li lj aw lk bi"><span id="cc8f" class="ll lm iq lh b gy ln lo l lp lq">test_sent = ["We",<br/>            "have",<br/>            "learned",<br/>            "much",<br/>            "about",<br/>            "interstellar",<br/>            "drives",<br/>            "since",<br/>            "a",<br/>            "hundred",<br/>            "years",<br/>            "ago",<br/>            "that",<br/>            "is",<br/>            "all",<br/>            "I",<br/>            "can",<br/>            "tell",<br/>            "you",<br/>            "about",<br/>            "them",<br/>            ]</span><span id="af5e" class="ll lm iq lh b gy nf lo l lp lq">cleaned_test_sent = [self.clean(w) for w in test_sent]<br/>print(self.vertibi(cleaned_test_sent, all_tags))</span></pre><p id="6d10" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们的结果一:</p><pre class="lc ld le lf gt lg lh li lj aw lk bi"><span id="849a" class="ll lm iq lh b gy ln lo l lp lq">we,PPSS<br/>have,HV-HL<br/>learned,VBN<br/>much,AP-TL<br/>about,RB<br/>interstellar,JJ-HL<br/>drives,NNS<br/>since,IN<br/>a,AT<br/>hundred,CD<br/>years,NNS<br/>ago,RB<br/>that,CS<br/>is,BEZ-NC<br/>all,QL<br/>i,PPSS<br/>can,MD<br/>tell,VB-NC<br/>you,PPO-NC<br/>about,RP<br/>them,DTS</span></pre><p id="565d" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">根据我们的文档，这是正确的。</p><p id="17d4" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我期待听到反馈或问题。</p></div></div>    
</body>
</html>