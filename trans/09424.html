<html>
<head>
<title>Case Study 2: An Unsupervised Neural Attention Model for Aspect Extraction</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">案例研究2:用于特征提取的无监督神经注意模型</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/case-study-2-an-unsupervised-neural-attention-model-for-aspect-extraction-1c2c97b1380a?source=collection_archive---------22-----------------------#2021-09-01">https://towardsdatascience.com/case-study-2-an-unsupervised-neural-attention-model-for-aspect-extraction-1c2c97b1380a?source=collection_archive---------22-----------------------#2021-09-01</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="5f6f" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsdatascience.com/tagged/notes-from-industry" rel="noopener" target="_blank">行业笔记</a></h2><div class=""/><figure class="gl gn jx jy jz ka gh gi paragraph-image"><div role="button" tabindex="0" class="kb kc di kd bf ke"><div class="gh gi jw"><img src="../Images/b33c56bf4d0b74c9d9e431e9874bc5ab.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*YwDYKgOY7b94-cSm"/></div></div><p class="kh ki gj gh gi kj kk bd b be z dk translated">由<a class="ae kl" href="https://unsplash.com/@markuswinkler?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">马库斯·温克勒</a>在<a class="ae kl" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><p id="9235" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">十多年来，企业一直利用互联网的可达性来为其内容做广告。如果没有这种广告媒介，这些公司就很难获得他们想要的客户群。但随着社交网站的快速增长，互联网已经发展成为一个论坛，消费者可以根据他人在网上发布的产品反馈来评估产品和服务。对任何特定产品的评论决定了它在市场上的声誉。关于网上购物行为的各种研究表明，潜在客户在信任任何特定产品之前，平均至少会阅读四到五条评论。这就是为什么顾客评论对企业的运作至关重要。在这个案例研究中，我们将探索基于方面的抽取的概念，这对于在线评论是至关重要的。我们将它分为以下几个部分—</p><ul class=""><li id="037a" class="lk ll iq ko b kp kq kt ku kx lm lb ln lf lo lj lp lq lr ls bi translated"><strong class="ko ja">问题描述</strong></li><li id="720a" class="lk ll iq ko b kp lt kt lu kx lv lb lw lf lx lj lp lq lr ls bi translated"><strong class="ko ja">车型概述</strong></li><li id="4e3d" class="lk ll iq ko b kp lt kt lu kx lv lb lw lf lx lj lp lq lr ls bi translated"><strong class="ko ja">数据描述</strong></li><li id="798a" class="lk ll iq ko b kp lt kt lu kx lv lb lw lf lx lj lp lq lr ls bi translated"><strong class="ko ja">数据预处理</strong></li><li id="7068" class="lk ll iq ko b kp lt kt lu kx lv lb lw lf lx lj lp lq lr ls bi translated"><strong class="ko ja">模型架构</strong></li><li id="bb44" class="lk ll iq ko b kp lt kt lu kx lv lb lw lf lx lj lp lq lr ls bi translated"><strong class="ko ja">基线模型</strong></li><li id="265e" class="lk ll iq ko b kp lt kt lu kx lv lb lw lf lx lj lp lq lr ls bi translated"><strong class="ko ja">结论</strong></li><li id="6578" class="lk ll iq ko b kp lt kt lu kx lv lb lw lf lx lj lp lq lr ls bi translated"><strong class="ko ja">部署</strong></li><li id="815e" class="lk ll iq ko b kp lt kt lu kx lv lb lw lf lx lj lp lq lr ls bi translated"><strong class="ko ja">未来工作</strong></li><li id="c3fd" class="lk ll iq ko b kp lt kt lu kx lv lb lw lf lx lj lp lq lr ls bi translated"><strong class="ko ja">链接</strong></li><li id="2e3c" class="lk ll iq ko b kp lt kt lu kx lv lb lw lf lx lj lp lq lr ls bi translated"><strong class="ko ja">参考文献</strong></li></ul></div><div class="ab cl ly lz hu ma" role="separator"><span class="mb bw bk mc md me"/><span class="mb bw bk mc md me"/><span class="mb bw bk mc md"/></div><div class="ij ik il im in"><h1 id="8754" class="mf mg iq bd mh mi mj mk ml mm mn mo mp mq mr ms mt mu mv mw mx my mz na nb nc bi translated"><strong class="ak">问题描述</strong></h1><p id="816f" class="pw-post-body-paragraph km kn iq ko b kp nd kr ks kt ne kv kw kx nf kz la lb ng ld le lf nh lh li lj ij bi translated">情感分析是一种自然语言处理技术，用于确定给定文本是正面的、负面的还是中性的。当您希望从给定的文本块中推断出整体情感时，这种技术非常有用。这种技术的一个缺点是，如果人们希望了解客户对产品的哪个方面不满意，就必须手动筛选每个评论。这种形式的体力劳动非常耗时。在这种情况下，基于方面的情感分析是更好的选择。使用这种技术，我们可以通过将情感与评论的特定方面联系起来来分析评论。</p><p id="c1b6" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">在2017年由何，Wee Sun Lee，Hwee Tou Ng和Daniel Dahlmeier发表的题为“<a class="ae kl" href="https://www.aclweb.org/anthology/P17-1036.pdf" rel="noopener ugc nofollow" target="_blank">一种用于方面提取的无监督神经注意力模型</a>”的研究论文中，研究人员设计了一种无监督的深度神经网络，可以根据它们的方面对一组句子进行分类。这个模型限制了它自己只能识别每个输入的一个方面，并且没有将任何情感与这个方面相关联。任何经过适当训练以执行情感分析的模型都可以在评论被分离后应用于评论。在本案例研究中，我们将从头开始设计2017年研究论文中提到的模型。我们将使用Tensorflow 2.x .作为Python中用于模型构建和训练的后端框架。</p><h1 id="a447" class="mf mg iq bd mh mi ni mk ml mm nj mo mp mq nk ms mt mu nl mw mx my nm na nb nc bi translated">模型概述</h1><p id="a32b" class="pw-post-body-paragraph km kn iq ko b kp nd kr ks kt ne kv kw kx nf kz la lb ng ld le lf nh lh li lj ij bi translated">以前训练来执行这项任务的传统机器学习模型假设每个句子中出现的单词是独立的，上下文无关。这种假设导致这些模型的性能下降。单词嵌入是在2013年由托马斯·米科洛夫、伊利亚·苏茨基弗、程凯、格雷格·科拉多和杰弗里·迪恩发表的题为“<a class="ae kl" href="https://papers.nips.cc/paper/2013/file/9aa42b31882ec039965f3c4923ce901b-Paper.pdf" rel="noopener ugc nofollow" target="_blank">单词和短语的分布式表示及其组合性</a>”的论文中引入的。在这篇文章中，他们介绍了一个名为Word2Vec的模型。这个模型的目的是表明句子的上下文有多重要。使用这种模型，相似的单词被映射到相似方向的向量。同现单词在嵌入空间中彼此靠近。</p><p id="e3a8" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">在2014年由Dzmitry Bahdanau、Kyunghyun Cho和Yoshua Bengio撰写的题为“通过联合学习对齐和翻译”的神经机器翻译的研究论文中，注意力机制的概念被引入到语言翻译任务中。这项技术模仿了注意力的认知过程，允许任何生物选择并专注于相关的刺激。按照这种技术，文本的某些部分比整个文本具有更高的优先级。换句话说，注意力使模型能够在训练过程中强调重要的单词，而不强调无关的单词。这提高了模型发现更多一致方面的能力。研究人员给这个模型起的名字是基于注意力的方面提取(ABAE)。</p><h1 id="2ba4" class="mf mg iq bd mh mi ni mk ml mm nj mo mp mq nk ms mt mu nl mw mx my nm na nb nc bi translated">数据描述</h1><p id="b5f8" class="pw-post-body-paragraph km kn iq ko b kp nd kr ks kt ne kv kw kx nf kz la lb ng ld le lf nh lh li lj ij bi translated">我们将使用城市搜索语料库作为训练和测试的数据集，可以从<a class="ae kl" href="https://www.cs.cmu.edu/~mehrbod/RR/" rel="noopener ugc nofollow" target="_blank">这个</a>网站免费下载。研究论文中使用了相同的数据集来测试模型。数据集中有52，574条评论，其中只有3，400条被标记。本文考虑了六个方面。这些方面是食物，工作人员，氛围，价格，轶事，和杂项。研究人员选择未标记的数据作为训练集，选择标记的数据作为测试集。这些设置可以在研究者的谷歌驱动<a class="ae kl" href="https://drive.google.com/file/d/1qzbTiJ2IL5ATZYNMp2DRkHvbFYsnOVAQ/view" rel="noopener ugc nofollow" target="_blank">链接</a>上找到。</p></div><div class="ab cl ly lz hu ma" role="separator"><span class="mb bw bk mc md me"/><span class="mb bw bk mc md me"/><span class="mb bw bk mc md"/></div><div class="ij ik il im in"><h1 id="d074" class="mf mg iq bd mh mi mj mk ml mm mn mo mp mq mr ms mt mu mv mw mx my mz na nb nc bi translated">数据预处理</h1><p id="f4db" class="pw-post-body-paragraph km kn iq ko b kp nd kr ks kt ne kv kw kx nf kz la lb ng ld le lf nh lh li lj ij bi translated">加载测试集和训练集之后，我们需要对它们进行预处理。为了这个<br/>的目的，我们定义了preprocess()和complete_preprocess() <br/>函数。</p><figure class="nn no np nq gt ka"><div class="bz fp l di"><div class="nr ns l"/></div></figure><p id="75f9" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">使用这个函数，我们将评论中的所有单词转换成小写字母后进行分词。下一步是将这些令牌符号化。从本质上讲，通过将每个单词转换成其对应的词条，词条化有助于使您的数据矩阵更加稀疏。这样的词被称为是他们的规范形式或字典形式。此外，请注意，我们必须删除不会给评论增加任何价值的停用词。</p><figure class="nn no np nq gt ka"><div class="bz fp l di"><div class="nr ns l"/></div></figure><p id="037b" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">为了训练Word2Vec模型，我们需要以列表的形式提供输入。这意味着每个评论被转换成一个列表，整个集合是一个包含多个子列表的列表。这可以在split_list函数的帮助下完成。</p><p id="8367" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">所有评论都以字符串表示形式提供。深度神经网络模型无法理解字符串，因此，我们需要以数字格式对它们进行编码。这意味着检查中出现的每个令牌都应该映射到一个大于或等于1的唯一数字。所有字符串，包括数字8和9，当出现在评论中时都有唯一的映射。但是根据研究论文，在映射到相应的表示之前，所有的数字都应该表示为由<num>表示的单个标记。当将测试数据转换成相应的数字表示时，可以看到这个集合可能包含一些在训练集中不可用的单词。我们会在映射前用<unk>来表示这样的词。为了保持输入数据的一致性，我们需要将所有这些令牌列表转换成统一的长度。这可以在衬垫的帮助下完成。使用这种技术，通过在映射前预先计划或附加标记<pad>，所有的标记列表都被转换成相同的大小。在我们的例子中，我们将在每个列表前面加上<pad>标记。每个填充集的宽度等于该集中最长句子的长度。预留令牌的映射为:{' <pad> ':0，'<unk> ':1，'<num> ':2}。从这个映射中，我们可以推断出集合中出现的所有单词都应该得到3或更大的值。下面给出了一个标记化和填充表示(最大长度为4)的虚拟示例，说明输入在输入到模型之前应该是什么样子——</num></unk></pad></pad></pad></unk></num></p><pre class="nn no np nq gt nt nu nv nw aw nx bi"><span id="28a2" class="ny mg iq nu b gy nz oa l ob oc">"I ate 2 donuts" -&gt; ["I", "ate", "2", "donuts"] -&gt; [3, 4, 2, 5]</span><span id="8ce2" class="ny mg iq nu b gy od oa l ob oc">“I dislike donuts” -&gt; ["&lt;pad&gt;", "I", "dislike", "donuts"] -&gt;<br/>[0, 3, 6, 5]</span><span id="16e6" class="ny mg iq nu b gy od oa l ob oc">"I drank" -&gt; ["&lt;pad&gt;", "&lt;pad&gt;", "I", "drank"] -&gt; [0, 0, 3, 7]</span></pre></div><div class="ab cl ly lz hu ma" role="separator"><span class="mb bw bk mc md me"/><span class="mb bw bk mc md me"/><span class="mb bw bk mc md"/></div><div class="ij ik il im in"><h1 id="db4d" class="mf mg iq bd mh mi mj mk ml mm mn mo mp mq mr ms mt mu mv mw mx my mz na nb nc bi translated">模型架构</h1><p id="4282" class="pw-post-body-paragraph km kn iq ko b kp nd kr ks kt ne kv kw kx nf kz la lb ng ld le lf nh lh li lj ij bi translated">在前面提到的引入Word2Vec概念的论文中，研究人员还添加了一个称为负采样的概念，帮助我们更快地训练单词嵌入。通俗地说，负面样本是从训练集中所有可用评论的集合中随机选取的评论。这意味着，当我们将一篇综述作为输入(称为正样本或目标样本)时，我们应该挑选一组P篇综述与目标样本一起作为负样本。这里，P可以是一个小数字，例如5或6，或者甚至可以是一个更大的数字，例如21或22。请注意，我们并没有按照已经存在于训练集中的顺序将输入直接输入到模型中。这是为了应对训练数据的过度拟合。为了总结这个令人困惑的概念，让我仔细地将我的话重组如下——当P的值是5时，在一个时期的每一步中，我们用从训练集随机采样的1个评论作为正输入，用从这个相同的集替换随机采样的5个评论作为负样本来呈现模型。在这种情况下，假设批量大小为1。如果批量大小被设置为值32，则在一个时期的任何一个步骤中，我们将32*1=32个正样本与32*5=160个负样本一起输入到模型中。这是生成阳性和阴性样本的代码—</p><figure class="nn no np nq gt ka"><div class="bz fp l di"><div class="nr ns l"/></div></figure><p id="45f6" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">现在是详细了解ABSA的时候了。下面给出了该模型的架构。这里需要注意的重要一点是，该图仅描述了阳性样本的处理。</p><figure class="nn no np nq gt ka gh gi paragraph-image"><div class="gh gi oe"><img src="../Images/4f6f54f5faf733e5c7242b30fd4478a7.png" data-original-src="https://miro.medium.com/v2/resize:fit:906/format:webp/1*MMDKpfwPL9j--37EhONlfg.png"/></div><p class="kh ki gj gh gi kj kk bd b be z dk translated">基于注意力的情感分析器架构(图片取自纸张)</p></figure><p id="883f" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">使用从Word2Vec模型生成的所有单词嵌入，我们将它们组合成一个矩阵。这个矩阵将被视为嵌入矩阵。下一步是使用k-means聚类算法训练这个矩阵，并识别模型收敛后生成的聚类中心。形成的聚类中心的数目等于k的值，k是我们在训练过程之前手动馈入模型的超参数。k的值应该是我们希望使用ABSA模型识别的不同方面的数量。rₛ层的权重矩阵(在上图中表示为t)使用所有这些聚类中心的归一化形式进行初始化。这个T矩阵也被称为方面矩阵。</p><p id="4b59" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">正样本和负样本的最大区别在于，正样本被馈送到注意层，而后者没有。但是在进一步处理之前，所有的样本必须通过一个不可训练的嵌入层被转换成单词嵌入。该层使用在先前步骤之一中创建的嵌入矩阵来初始化。使用Tensorflow的子类化API，我们设计了多个自定义层来处理样本。</p><p id="1330" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">第一个自定义层的名称是“平均”。它可以计算评论中所有单词嵌入的平均值，用术语yₛ.表示</p><figure class="nn no np nq gt ka gh gi paragraph-image"><div class="gh gi of"><img src="../Images/03f57290f2e81fd18ed5812595bc5608.png" data-original-src="https://miro.medium.com/v2/resize:fit:342/format:webp/1*d8osexu6D3Nqi-Iebf9mbQ.png"/></div><p class="kh ki gj gh gi kj kk bd b be z dk translated">yₛ公式(图片取自纸张)</p></figure><p id="38a2" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">然后，正样本嵌入和yₛ值被馈送到关注层。术语dᵢ表示单词嵌入、矩阵m和yₛ.的转置之间的中间乘法这里，M是可训练的，并使用Glorot统一初始化器进行初始化。在这个初始化式中，样本是从特定范围内的均匀分布中抽取的。这个极限值的计算取决于这个矩阵的形状。通过在dᵢ上应用softmax，我们计算aᵢ，这是注意力权重。</p><figure class="nn no np nq gt ka gh gi paragraph-image"><div class="gh gi og"><img src="../Images/bb7969cc5bd5ae34abdcc7f714b1c89e.png" data-original-src="https://miro.medium.com/v2/resize:fit:404/format:webp/1*CnD-K3xGL2wo9LW2K1eAwA.png"/></div><p class="kh ki gj gh gi kj kk bd b be z dk translated">dᵢ和aᵢ的公式(图片取自纸张)</p></figure><p id="ce5f" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">下一个自定义图层名为WeightedSum。使用这一层，我们计算每个单词的嵌入和为每个单词计算的注意力权重之间的点积。这个点积用术语zₛ.来表示</p><figure class="nn no np nq gt ka gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/c86db994b16682b5ebbf122c4570b61b.png" data-original-src="https://miro.medium.com/v2/resize:fit:336/format:webp/1*V6xEzYevgbCI0QHsM8kOBw.png"/></div><p class="kh ki gj gh gi kj kk bd b be z dk translated">zₛ公式(图片取自纸张)</p></figure><p id="bb0f" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">使用Tensorflow的自然密集层，我们将zₛ乘以随机初始化的权重，然后将偏差项添加到其中。最重要的是，我们应用softmax。这一层在文中被称为pₜ。</p><figure class="nn no np nq gt ka gh gi paragraph-image"><div class="gh gi oi"><img src="../Images/7f5cee65ee37ae29e672beb31f440d13.png" data-original-src="https://miro.medium.com/v2/resize:fit:554/format:webp/1*xNdJWZuMlSWlfcF4YQoqdQ.png"/></div><p class="kh ki gj gh gi kj kk bd b be z dk translated">pₜ公式(图片取自纸张)</p></figure><p id="a6a6" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">下一步是通过自定义平均层传递负样本。我们得到的输出是一个包含每个样本平均值的列表。这个列表用zₙ.来表示最后一层是rₛ，我们之前提到过。下面给出的是rₛ和pₜ.之间的关系</p><figure class="nn no np nq gt ka gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/5a67e7cc99321e45d82e9954d199b8bd.png" data-original-src="https://miro.medium.com/v2/resize:fit:310/format:webp/1*sBFs3Pz9KjMsnaMNPMv-0Q.png"/></div><p class="kh ki gj gh gi kj kk bd b be z dk translated">rₛ公式(图片取自纸张)</p></figure><p id="fef1" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">自动编码器是用于压缩数据的无监督神经网络架构。输入和输出的大小相同。中间有一个瓶颈层，用于将数据压缩到所需的大小。从输入层到瓶颈层的部分称为编码器，从瓶颈层到输出层的部分称为解码器。这个网络的目标是在输出端重构被瓶颈层压缩的输入。通过训练模型，我们试图最小化重建误差。</p><figure class="nn no np nq gt ka gh gi paragraph-image"><div role="button" tabindex="0" class="kb kc di kd bf ke"><div class="gh gi ok"><img src="../Images/8ecd219e47aa895eb11ffed1829bf561.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2Ot6dQKfBuc4LgvqNP06PA.png"/></div></div><p class="kh ki gj gh gi kj kk bd b be z dk translated">Autoencoder模型(图片作者:<a class="ae kl" href="https://seongjuhong.com/author/pyramid19/" rel="noopener ugc nofollow" target="_blank"> Seongju Hong </a></p></figure><p id="5fb9" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">整个ABSA模型是一个定制的自动编码器。从输入层到pₜ层的部分是编码器，从pₜ层到rₛ层的部分是解码器。我们的目标是最小化rₛ层的重建误差。</p><p id="9928" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">在2016年由Mohit Iyyer，Anupam Guha，Snigdha Chaturvedi，Jordan Boyd-Graber和Hal Daumé III撰写的题为“<a class="ae kl" href="https://aclanthology.org/N16-1180.pdf" rel="noopener ugc nofollow" target="_blank">世仇家庭和前朋友:动态虚构关系的无监督学习</a>”的论文中，研究人员提到了对比最大利润损失的概念。如果负样本嵌入与重构嵌入相似，这个损失函数会严重地惩罚模型。这意味着损失函数试图最大化rₛ和zₛ之间的乘积，而它试图最小化rₛ和nᵢ(when nᵢ之间的乘积是来自列表zₙ).的元素我们使用一个名为HingeLoss的自定义层来执行这一步中必要的计算。损失函数的公式如下所示</p><figure class="nn no np nq gt ka gh gi paragraph-image"><div class="gh gi ol"><img src="../Images/115259f089b4764653b6090cd0925483.png" data-original-src="https://miro.medium.com/v2/resize:fit:788/format:webp/1*wFU0M5fW-j-SrC1zMYt7pw.png"/></div><p class="kh ki gj gh gi kj kk bd b be z dk translated">J(θ)的公式(图片取自纸张)</p></figure><p id="42d4" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">为了防止过度拟合，我们需要正则化模型。甚至为了这个目的，提到了定制的L2正则化。另外，请注意，我们应该使用方面矩阵的规范化形式进行正则化。使用自定义函数将正则化代码应用于密集rₛ图层。</p><figure class="nn no np nq gt ka gh gi paragraph-image"><div class="gh gi om"><img src="../Images/fbfe313db520c71f370bbdedf57b68c8.png" data-original-src="https://miro.medium.com/v2/resize:fit:604/format:webp/1*EXZor_lcg_YAlJJq3jaDxw.png"/></div><p class="kh ki gj gh gi kj kk bd b be z dk translated">U(θ)的公式(图片取自纸张)</p></figure><p id="685b" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">我们计划使用RMSProp作为训练这个模型的优化器。学习率为1e-02，ε值为1e-06。在训练该模型时，有必要设置clipnorm值。在我们的例子中，我们将其设置为10。这意味着梯度的L2范数被限制在给定值。我们正在为15个时期训练模型，并且在每个时期考虑182个批次。批量大小设置为1024，负采样率为20。当在任何时期中计算的损失小于先前记录的最小损失时，我们显示从每个方面得到的前50个单词及其相应的相似性得分。以下是培训代码—</p><figure class="nn no np nq gt ka"><div class="bz fp l di"><div class="nr ns l"/></div></figure><p id="3989" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">从印刷的文字，我们必须手动推断方面。相应地，我们还应该基于这些信息创建一个聚类图。现在是时候使用张量板绘制表示最小化的图形了。如你所见，模型收敛于损失值4.7。</p><figure class="nn no np nq gt ka gh gi paragraph-image"><div role="button" tabindex="0" class="kb kc di kd bf ke"><div class="gh gi on"><img src="../Images/5f3524830235d9124775f44c2b4c47d7.png" data-original-src="https://miro.medium.com/v2/resize:fit:766/format:webp/1*DPM0iDmjKukugYtYK08tuA.png"/></div></div><p class="kh ki gj gh gi kj kk bd b be z dk translated">张量板图(图片由作者提供)</p></figure><p id="afa1" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">现在是对测试集执行预测的时候了。我们将需要创建一个新的自定义模型使用以前制作的ABSA模型的层。对于这个新制作的模型，我们将只使用为ABSA模型中的正样本设计的输入层作为该模型的输入。该模型的输出是ABSA的pₜ层。</p><figure class="nn no np nq gt ka"><div class="bz fp l di"><div class="nr ns l"/></div></figure><p id="1401" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">研究人员根据这些方面对测试进行了筛选。他们只允许那些属于食物、员工或环境方面的评论。该模型不能在一次审查中识别多个方面。因此，即使那些包含多个样本的评论也被删除了。在这一步之后，最后的任务是执行预测和生成分类报告。正如你所看到的，ABSA模型的性能似乎是公平的！</p><figure class="nn no np nq gt ka gh gi paragraph-image"><div class="gh gi oo"><img src="../Images/1fb810241d797a8ce04df324458195d0.png" data-original-src="https://miro.medium.com/v2/resize:fit:938/format:webp/1*dj30sXQhJiiz4yD_oIKk_A.png"/></div><p class="kh ki gj gh gi kj kk bd b be z dk translated">ABSA分类报告(图片由作者提供)</p></figure></div><div class="ab cl ly lz hu ma" role="separator"><span class="mb bw bk mc md me"/><span class="mb bw bk mc md me"/><span class="mb bw bk mc md"/></div><div class="ij ik il im in"><h1 id="b12e" class="mf mg iq bd mh mi mj mk ml mm mn mo mp mq mr ms mt mu mv mw mx my mz na nb nc bi translated">基线模型</h1><p id="4cc2" class="pw-post-body-paragraph km kn iq ko b kp nd kr ks kt ne kv kw kx nf kz la lb ng ld le lf nh lh li lj ij bi translated">主题建模是一种无监督的方法，通过找到一些自然的项目组(在此上下文中称为主题)来对文档进行分类。这项技术已经使用了很长时间，因为它可以自动组织、理解、搜索和有效地汇总数据。最流行的主题建模算法是潜在狄利克雷分配(LDA)。这是基于基线模型方面的情感分析任务。我们将比较我们的ABSA模型的结果和使用LDA模型获得的结果。</p><p id="313d" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">一个文档可以是具有不同相似比例的多个主题的一部分。每个文档(在我们的例子中也称为评论)是一个单词列表。我们真正想弄清楚的是一个单词属于每个主题的概率。表格中的每一行代表一个不同的主题，每一列是数据集中出现的一个不同的单词。每个单元格包含单词(列)属于主题(行)的概率。</p><figure class="nn no np nq gt ka gh gi paragraph-image"><div class="gh gi op"><img src="../Images/4786e25a1dc95814193ab97bd081866f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1222/format:webp/0*LqDaFv6S6Qtzety2.png"/></div><p class="kh ki gj gh gi kj kk bd b be z dk translated">每个主题包含所有可用单词的概率分数(图片由Ria Kulshrestha 提供)</p></figure><p id="a53c" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">对于我们的任务，这些主题只是方面。因此，导出的主题数量等于我们分析所需的方面数量。关于单词的一个重要假设是这些单词的顺序和语法结构并不重要。这意味着单词是独立的，这样的假设会导致性能下降。无论如何，我们将在我们的训练集上训练LDA模型，批量大小设置为1024。下面给出了显示属于每个方面的前50个单词的代码。</p><figure class="nn no np nq gt ka"><div class="bz fp l di"><div class="nr ns l"/></div></figure><p id="9926" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">在打印分类报告时，我们可以观察到ABSA模型在方面提取任务方面的性能更好。</p><figure class="nn no np nq gt ka gh gi paragraph-image"><div class="gh gi oq"><img src="../Images/6a0576f4afc083993b1ba6c6ebd1b72d.png" data-original-src="https://miro.medium.com/v2/resize:fit:966/format:webp/1*qx6y7sIZ5xv66m_QJbuJow.png"/></div><p class="kh ki gj gh gi kj kk bd b be z dk translated">LDA分类报告(图片由作者提供)</p></figure></div><div class="ab cl ly lz hu ma" role="separator"><span class="mb bw bk mc md me"/><span class="mb bw bk mc md me"/><span class="mb bw bk mc md"/></div><div class="ij ik il im in"><h1 id="a349" class="mf mg iq bd mh mi mj mk ml mm mn mo mp mq mr ms mt mu mv mw mx my mz na nb nc bi translated">结论</h1><p id="49ba" class="pw-post-body-paragraph km kn iq ko b kp nd kr ks kt ne kv kw kx nf kz la lb ng ld le lf nh lh li lj ij bi translated">如您所见，基于神经网络的模型非常适合基于方面的情感分析任务。这种表现应该归功于注意机制。</p><h1 id="c257" class="mf mg iq bd mh mi ni mk ml mm nj mo mp mq nk ms mt mu nl mw mx my nm na nb nc bi translated"><strong class="ak">部署</strong></h1><p id="1e4f" class="pw-post-body-paragraph km kn iq ko b kp nd kr ks kt ne kv kw kx nf kz la lb ng ld le lf nh lh li lj ij bi translated">下面提到了部署的截图。同样的录像可以在<a class="ae kl" href="https://youtu.be/NylhrESz3UU" rel="noopener ugc nofollow" target="_blank">这里</a>观看。</p><figure class="nn no np nq gt ka gh gi paragraph-image"><div role="button" tabindex="0" class="kb kc di kd bf ke"><div class="gh gi or"><img src="../Images/75940f75283fd75025cf924b628a6f02.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Z6nA_kyUdTMMmwwoxkZ1Yw.png"/></div></div><p class="kh ki gj gh gi kj kk bd b be z dk translated">预测截图(图片由作者提供)</p></figure><h1 id="ab48" class="mf mg iq bd mh mi ni mk ml mm nj mo mp mq nk ms mt mu nl mw mx my nm na nb nc bi translated">未来的工作</h1><p id="5d34" class="pw-post-body-paragraph km kn iq ko b kp nd kr ks kt ne kv kw kx nf kz la lb ng ld le lf nh lh li lj ij bi translated">我们的模型只能识别包含不超过一个方面的评论。此外，当我们希望评估与任何给定方面相关的情绪时，我们需要使用其他模型。这一领域未来的研究应该集中在这两个问题上。</p><h1 id="702c" class="mf mg iq bd mh mi ni mk ml mm nj mo mp mq nk ms mt mu nl mw mx my nm na nb nc bi translated">链接</h1><blockquote class="os ot ou"><p id="d9e5" class="km kn ov ko b kp kq kr ks kt ku kv kw ow ky kz la ox lc ld le oy lg lh li lj ij bi translated">Github资源库:【https://github.com/Ashcom-git/case-study-2 T4】</p><p id="ec6c" class="km kn ov ko b kp kq kr ks kt ku kv kw ow ky kz la ox lc ld le oy lg lh li lj ij bi translated">领英:<a class="ae kl" href="https://www.linkedin.com/in/ashwin-michael-10b617142/" rel="noopener ugc nofollow" target="_blank">https://www.linkedin.com/in/ashwin-michael-10b617142/</a></p></blockquote><h1 id="cee9" class="mf mg iq bd mh mi ni mk ml mm nj mo mp mq nk ms mt mu nl mw mx my nm na nb nc bi translated">参考</h1><blockquote class="os ot ou"><p id="4aa3" class="km kn ov ko b kp kq kr ks kt ku kv kw ow ky kz la ox lc ld le oy lg lh li lj ij bi translated">应用根。[在线]地址:https://www.appliedaicourse.com/。</p><p id="442c" class="km kn ov ko b kp kq kr ks kt ku kv kw ow ky kz la ox lc ld le oy lg lh li lj ij bi translated">何、李维孙、吴惠头和丹尼尔·达尔梅尔(2017)。用于特征提取的无监督神经注意模型。计算语言学协会第55届年会会议录(第1卷:长论文)。</p><p id="9644" class="km kn ov ko b kp kq kr ks kt ku kv kw ow ky kz la ox lc ld le oy lg lh li lj ij bi translated">‌tomas·米科洛夫、伊利亚·苏茨基弗、陈开、格雷戈·科拉多和杰弗里·迪恩(2013年)。词和短语的分布式表示及其组合性。神经信息处理系统进展26 (NIPS 2013)。</p><p id="9e6b" class="km kn ov ko b kp kq kr ks kt ku kv kw ow ky kz la ox lc ld le oy lg lh li lj ij bi translated">Dzmitry Bahdanau、Kyunghyun Cho和Yoshua Bengio。(2015).通过联合学习对齐和翻译的神经机器翻译。第三届国际学习代表会议论文集。</p><p id="77a0" class="km kn ov ko b kp kq kr ks kt ku kv kw ow ky kz la ox lc ld le oy lg lh li lj ij bi translated">Mohit Iyyer、Anupam Guha、Snigdha Chaturvedi、Jordan Boyd-Graber和Hal Daumé III。(2016).世仇家庭和前朋友:动态虚构关系的无监督学习。计算语言学协会北美分会2016年会议论文集:人类语言技术。</p><p id="7973" class="km kn ov ko b kp kq kr ks kt ku kv kw ow ky kz la ox lc ld le oy lg lh li lj ij bi translated">陈燕琳(2019年)。如何使用文本建立ld a主题模型？【在线】中等。可从以下网址获得:https://medium . com/@ yanlinc/how-to-build-a-LDA-topic-model-using-from-text-601 CDC bfd 3 a 6。</p><p id="bfe2" class="km kn ov ko b kp kq kr ks kt ku kv kw ow ky kz la ox lc ld le oy lg lh li lj ij bi translated">苏珊·李(2018)。Python中的主题建模和潜在狄利克雷分配。【在线】中等。可从以下网址获得:https://towards data science . com/topic-modeling-and-latent-Dirichlet-allocation-in-python-9bf 156893 c 24。</p><p id="080c" class="km kn ov ko b kp kq kr ks kt ku kv kw ow ky kz la ox lc ld le oy lg lh li lj ij bi translated">Ria Kulshrestha (2020年)。潜在狄利克雷分配。【在线】中等。可从https://towards data science . com/latent-Dirichlet-allocation-LDA-9d 1 CD 064 FFA 2获取。</p><p id="5859" class="km kn ov ko b kp kq kr ks kt ku kv kw ow ky kz la ox lc ld le oy lg lh li lj ij bi translated">桑内·德·罗弗(2020)。方面，更好的话题？在亚马逊化妆品评论中应用无监督特征抽取。【在线】中等。可从以下网址获得:https://medium . com/@ sanne . de . ro ever/aspects-the-better-topics-applying-unsupervised-aspect-extraction-on-Amazon-cosmetics-reviews-9d 523747 f8e 5。</p></blockquote><p id="fd28" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi">‌</p><p id="b50e" class="pw-post-body-paragraph km kn iq ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi">‌</p></div></div>    
</body>
</html>