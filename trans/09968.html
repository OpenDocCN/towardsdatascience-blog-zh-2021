<html>
<head>
<title>High-quality sentence paraphraser using Transformers in NLP</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">自然语言处理中使用变形器的高质量句子解释器</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/high-quality-sentence-paraphraser-using-transformers-in-nlp-c33f4482856f?source=collection_archive---------6-----------------------#2021-09-20">https://towardsdatascience.com/high-quality-sentence-paraphraser-using-transformers-in-nlp-c33f4482856f?source=collection_archive---------6-----------------------#2021-09-20</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="569b" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">在定制数据集和T5大型模型上训练的开源解释工具</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/3c2fedfd55f85a7992c41d397f59dc9f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ruKquu6U4_zYXK0r538zag.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><blockquote class="ky kz la"><p id="f603" class="lb lc ld le b lf lg ju lh li lj jx lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated">注意:这个解释器被训练来解释简短的英语句子，最适合这些输入。</p></blockquote><h2 id="7932" class="ly lz it bd ma mb mc dn md me mf dp mg mh mi mj mk ml mm mn mo mp mq mr ms mt bi translated">投入</h2><p id="bbfa" class="pw-post-body-paragraph lb lc it le b lf mu ju lh li mv jx lk mh mw ln lo ml mx lr ls mp my lv lw lx im bi translated">我们程序的输入将是任何英语句子</p><pre class="kj kk kl km gt mz na nb nc aw nd bi"><span id="3e1c" class="ly lz it na b gy ne nf l ng nh"><strong class="na iu">Four private astronauts launched to orbit by Elon Musk’s SpaceX returned to Earth Saturday evening, splashing down into the ocean off the east coast of Florida after a three-day mission.</strong></span></pre><h2 id="f005" class="ly lz it bd ma mb mc dn md me mf dp mg mh mi mj mk ml mm mn mo mp mq mr ms mt bi translated">输出</h2><p id="06c3" class="pw-post-body-paragraph lb lc it le b lf mu ju lh li mv jx lk mh mw ln lo ml mx lr ls mp my lv lw lx im bi translated">输出将是<strong class="le iu">转述</strong>版本的同一句话。解释一个句子意味着，你创造一个新的句子，用一个不同的词语选择来表达与T4相同的意思。</p><pre class="kj kk kl km gt mz na nb nc aw nd bi"><span id="8784" class="ly lz it na b gy ne nf l ng nh"><strong class="na iu">After a three-day mission, four private astronauts sent by Elon Musk's SpaceX returned to Earth on Saturday evening, splashing down into the ocean off the east coast of Florida.</strong></span></pre><h1 id="88e2" class="ni lz it bd ma nj nk nl md nm nn no mg jz np ka mk kc nq kd mo kf nr kg ms ns bi translated">实际使用案例</h1><p id="9e39" class="pw-post-body-paragraph lb lc it le b lf mu ju lh li mv jx lk mh mw ln lo ml mx lr ls mp my lv lw lx im bi translated">从重写你以前的社交媒体文章到大学论文，当你没有很多文本分类模型的例子时，有几个解释器的用例。</p><h1 id="93c7" class="ni lz it bd ma nj nk nl md nm nn no mg jz np ka mk kc nq kd mo kf nr kg ms ns bi translated">资料组</h1><p id="b964" class="pw-post-body-paragraph lb lc it le b lf mu ju lh li mv jx lk mh mw ln lo ml mx lr ls mp my lv lw lx im bi translated">作为构建<a class="ae nt" href="https://questgen.ai/" rel="noopener ugc nofollow" target="_blank"> Questgen.ai </a>的一部分，我们创建了一个在<a class="ae nt" href="https://github.com/jwieting/para-nmt-50m" rel="noopener ugc nofollow" target="_blank"> ParaNMT </a>之上过滤的自定义数据集，以仅保留多样化的高质量释义。</p><p id="a1f9" class="pw-post-body-paragraph lb lc it le b lf lg ju lh li lj jx lk mh lm ln lo ml lq lr ls mp lu lv lw lx im bi translated">这里的多样性意味着句子对的选择使得在词序上有显著的差异，或者至少转述输出由于多个单词变化而不同。</p><h1 id="44cc" class="ni lz it bd ma nj nk nl md nm nn no mg jz np ka mk kc nq kd mo kf nr kg ms ns bi translated">怎么用？</h1><p id="88b1" class="pw-post-body-paragraph lb lc it le b lf mu ju lh li mv jx lk mh mw ln lo ml mx lr ls mp my lv lw lx im bi translated">如果你喜欢易于使用的<strong class="le iu">谷歌Colab笔记本</strong>，可以在<a class="ae nt" href="https://github.com/ramsrigouthamg/Questgen.ai/tree/master/NewModels/T5LargeParaphraser" rel="noopener ugc nofollow" target="_blank"> Questgen的Github Repo </a>找到。</p><h2 id="76c2" class="ly lz it bd ma mb mc dn md me mf dp mg mh mi mj mk ml mm mn mo mp mq mr ms mt bi translated">1.装置</h2><pre class="kj kk kl km gt mz na nb nc aw nd bi"><span id="4144" class="ly lz it na b gy ne nf l ng nh"><strong class="na iu">!pip install transformers==4.10.2<br/>!pip install sentencepiece==0.1.96</strong></span></pre><h2 id="006f" class="ly lz it bd ma mb mc dn md me mf dp mg mh mi mj mk ml mm mn mo mp mq mr ms mt bi translated">2.运行代码</h2><p id="0eeb" class="pw-post-body-paragraph lb lc it le b lf mu ju lh li mv jx lk mh mw ln lo ml mx lr ls mp my lv lw lx im bi translated">我们将使用上传到HuggingFace Transformers库中心的预训练模型来运行解释器。我们将使用不同的波束搜索解码策略，为释义输出提供最佳结果。</p><p id="b921" class="pw-post-body-paragraph lb lc it le b lf lg ju lh li lj jx lk mh lm ln lo ml lq lr ls mp lu lv lw lx im bi translated">更多的例子可以在上面提到的Google Colab演示中找到。</p><pre class="kj kk kl km gt mz na nb nc aw nd bi"><span id="c89f" class="ly lz it na b gy ne nf l ng nh">from transformers import AutoTokenizer, AutoModelForSeq2SeqLM</span><span id="8d43" class="ly lz it na b gy nu nf l ng nh">model = AutoModelForSeq2SeqLM.from_pretrained("<strong class="na iu">ramsrigouthamg/t5-large-paraphraser-diverse-high-quality</strong>")<br/>tokenizer = AutoTokenizer.from_pretrained("<strong class="na iu">ramsrigouthamg/t5-large-paraphraser-diverse-high-quality</strong>")</span><span id="cd3e" class="ly lz it na b gy nu nf l ng nh">import torch<br/>device = torch.device("cuda" if torch.cuda.is_available() else "cpu")<br/>print ("device ",device)<br/>model = model.to(device)</span><span id="2cb6" class="ly lz it na b gy nu nf l ng nh"><strong class="na iu"># Diverse Beam search</strong></span><span id="9760" class="ly lz it na b gy nu nf l ng nh">context = "<strong class="na iu">Once, a group of frogs was roaming around the forest in search of water.</strong>"<br/>text = "paraphrase: "+context + " &lt;/s&gt;"</span><span id="358d" class="ly lz it na b gy nu nf l ng nh">encoding = tokenizer.encode_plus(text,max_length =128, padding=True, return_tensors="pt")<br/>input_ids,attention_mask  = encoding["input_ids"].to(device), encoding["attention_mask"].to(device)</span><span id="bad8" class="ly lz it na b gy nu nf l ng nh">model.eval()<br/>diverse_beam_outputs = model.generate(<br/>    input_ids=input_ids,attention_mask=attention_mask,<br/>    max_length=128,<br/>    early_stopping=True,<br/>    num_beams=5,<br/>    num_beam_groups = 5,<br/>    num_return_sequences=5,<br/>    diversity_penalty = 0.70</span><span id="473a" class="ly lz it na b gy nu nf l ng nh">)</span><span id="6745" class="ly lz it na b gy nu nf l ng nh">print ("\n\n")<br/>print ("Original: ",context)<br/>for beam_output in diverse_beam_outputs:<br/>    sent = tokenizer.decode(beam_output, skip_special_tokens=True,clean_up_tokenization_spaces=True)<br/>    print (sent)</span></pre><p id="fa0a" class="pw-post-body-paragraph lb lc it le b lf lg ju lh li lj jx lk mh lm ln lo ml lq lr ls mp lu lv lw lx im bi translated">上述代码的输出是:</p><p id="3898" class="pw-post-body-paragraph lb lc it le b lf lg ju lh li lj jx lk mh lm ln lo ml lq lr ls mp lu lv lw lx im bi translated"><strong class="le iu">原文</strong>:有一次，一群青蛙在森林里四处游荡寻找水源。一群青蛙在森林里四处游荡寻找水源。一群青蛙在森林里四处游荡寻找水源。一次，一群青蛙在森林里四处游荡寻找水源。一群青蛙在森林里游荡寻找水源。一群青蛙在森林里四处游荡，再次寻找水源。</p></div><div class="ab cl nv nw hx nx" role="separator"><span class="ny bw bk nz oa ob"/><span class="ny bw bk nz oa ob"/><span class="ny bw bk nz oa"/></div><div class="im in io ip iq"><p id="3715" class="pw-post-body-paragraph lb lc it le b lf lg ju lh li lj jx lk mh lm ln lo ml lq lr ls mp lu lv lw lx im bi translated">就是这样！你有一个高质量的最先进的句子解释器，你可以在你的项目中使用。需要注意的是，转述者的输出并不总是完美的，因此人在回路中的系统可能是必要的。</p></div><div class="ab cl nv nw hx nx" role="separator"><span class="ny bw bk nz oa ob"/><span class="ny bw bk nz oa ob"/><span class="ny bw bk nz oa"/></div><div class="im in io ip iq"><p id="1cfe" class="pw-post-body-paragraph lb lc it le b lf lg ju lh li lj jx lk mh lm ln lo ml lq lr ls mp lu lv lw lx im bi translated">祝NLP探索愉快，如果你喜欢它的内容，请随时在Twitter上找到我。</p><p id="19d0" class="pw-post-body-paragraph lb lc it le b lf lg ju lh li lj jx lk mh lm ln lo ml lq lr ls mp lu lv lw lx im bi translated">如果你想学习使用变形金刚的现代自然语言处理，看看我的课程<a class="ae nt" href="https://www.udemy.com/course/question-generation-using-natural-language-processing/?referralCode=C8EA86A28F5398CBF763" rel="noopener ugc nofollow" target="_blank">使用自然语言处理的问题生成</a></p></div></div>    
</body>
</html>