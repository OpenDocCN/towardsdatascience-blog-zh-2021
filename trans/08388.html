<html>
<head>
<title>An Overview of Data Preprocessing: Features Enrichment, Automatic Feature Selection</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">数据预处理概述:特征浓缩、自动特征选择</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/an-overview-of-data-preprocessing-features-enrichment-automatic-feature-selection-60b0c12d75ad?source=collection_archive---------13-----------------------#2021-08-02">https://towardsdatascience.com/an-overview-of-data-preprocessing-features-enrichment-automatic-feature-selection-60b0c12d75ad?source=collection_archive---------13-----------------------#2021-08-02</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="cff0" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsdatascience.com/tagged/hands-on-tutorials" rel="noopener" target="_blank">实践教程</a></h2><div class=""/><div class=""><h2 id="2fd7" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">在一个视图中使用python实现的有用的要素工程方法</h2></div><p id="c04c" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">数据集应该适用于机器学习中训练的数据和算法做出的预测，以产生更成功的结果。查看数据集，可以发现有些要素比其他要素更重要，也就是说，它们对输出的影响更大。例如，当用数据集的对数值或其他数学运算(如平方根、指数运算)进行替换时，可以获得更好的结果。这里要区分的是选择适合模型和项目的数据预处理方法。这篇文章包含了不同的角度来看待数据集，以使算法更容易学习数据集。使用python应用程序，所有研究都变得更容易理解。</p><pre class="ln lo lp lq gt lr ls lt lu aw lv bi"><span id="8328" class="lw lx it ls b gy ly lz l ma mb">Table of Contents (TOC)<br/>1. Binning<br/>2. Polynomial &amp; Interaction Features<br/>3. Non-Linear Transform<br/>3.1. Log Transform<br/>3.2. Square Root Transform<br/>3.3. Exponential Transform<br/>3.4. Box-cox Transform<br/>3.5. Reciprocal Transform<br/>4. Automatic Feature Selection<br/>4.1. Analysis of Variance (ANOVA)<br/>4.2. Model-Based Feature Selection<br/>4.3. Iterative Feature Selection</span></pre><figure class="ln lo lp lq gt md gh gi paragraph-image"><div role="button" tabindex="0" class="me mf di mg bf mh"><div class="gh gi mc"><img src="../Images/0fa874c2ef96308a51cdb93117cda25a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*98gsWxelWfH8yoMi"/></div></div><p class="mk ml gj gh gi mm mn bd b be z dk translated"><a class="ae mo" href="https://unsplash.com/@tamara_photography?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">塔玛拉·加克</a>在<a class="ae mo" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><h1 id="302a" class="mp lx it bd mq mr ms mt mu mv mw mx my ki mz kj na kl nb km nc ko nd kp ne nf bi translated">1.扔掉</h1><p id="48d1" class="pw-post-body-paragraph kr ks it kt b ku ng kd kw kx nh kg kz la ni lc ld le nj lg lh li nk lk ll lm im bi translated">在上一篇文章中，解释了以算法可以处理的方式对分类数据进行数字化的方法。宁滨用于将数值数据转换为分类数据，从而使模型更加灵活。考虑到数字数据，创建了由用户确定的箱数。所有数据都被填入这些范围并被重命名。现在让我们将宁滨应用于数据集中的年龄列。</p><pre class="ln lo lp lq gt lr ls lt lu aw lv bi"><span id="bb60" class="lw lx it ls b gy ly lz l ma mb">import numpy as np<br/>import pandas as pd<br/>import matplotlib.pyplot as plt</span><span id="9403" class="lw lx it ls b gy nl lz l ma mb">IN[1]<br/>data=pd.read_csv('toy_dataset.csv')<br/>data[['Age']].describe()<br/><strong class="ls jd">OUT[2]</strong><br/><strong class="ls jd">count 150000.000000<br/>mean 44.950200<br/>std 11.572486<br/>min 25.000000<br/>25% 35.000000<br/>50% 45.000000<br/>75% 55.000000<br/>max 65.000000</strong></span><span id="38c3" class="lw lx it ls b gy nl lz l ma mb">IN[2]<br/>def binnings(col, number_of_bins,labels):<br/>    min_val = col.min()<br/>    max_val = col.max()<br/>    space = (max_val-min_val)/number_of_bins<br/>    bin_borders=[]<br/>    for i in range(number_of_bins+1):<br/>        bin_values = min_val+space*i<br/>        bin_borders.append(bin_values)<br/>    bin_borderss = pd.cut(col, bins=bin_borders,labels=labels,include_lowest=True)<br/>    return bin_borders,bin_borderss<br/>IN[3]<br/>labels=["young_1","young_2","young_3","young_4","young_5","young_6","young_7","young_8","young_9","young_10"]<br/>binnings(data['Age'],10,labels)<br/><strong class="ls jd">OUT[3]<br/>([25.0, 29.0, 33.0, 37.0, 41.0, 45.0, 49.0, 53.0, 57.0, 61.0, 65.0],<br/> 0         young_4<br/> 1         young_8<br/> 2         young_5<br/> 3         young_4<br/> 4         young_6<br/>            ...   <br/> 149995    young_6<br/> 149996    young_1<br/> 149997    young_1<br/> 149998    young_1<br/> 149999    young_3<br/> Name: Age, Length: 150000, dtype: category</strong></span></pre><p id="d506" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">通过将数据集中的年龄范围等间隔分成11个部分，创建了10个箱。每个范围都被赋予选定的标签(young_1…..young_10)并作为列添加到数据集中。现在，如果我们想向数据集添加一个新列:</p><pre class="ln lo lp lq gt lr ls lt lu aw lv bi"><span id="ae16" class="lw lx it ls b gy ly lz l ma mb">IN[4]<br/>data['binned_age']= binnings(data['Age'],10,labels)<br/>data</span></pre><figure class="ln lo lp lq gt md gh gi paragraph-image"><div class="gh gi nm"><img src="../Images/5117d5aab5618441cd18add674ed3424.png" data-original-src="https://miro.medium.com/v2/resize:fit:1092/format:webp/1*qJAWCbpOOkjhw2muQLEiGg.png"/></div><p class="mk ml gj gh gi mm mn bd b be z dk translated">图一。OUT[4]，图片由作者提供</p></figure><p id="2b18" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">现在让我们看看算法精度对我们创建的数据集的影响。</p><pre class="ln lo lp lq gt lr ls lt lu aw lv bi"><span id="f6c9" class="lw lx it ls b gy ly lz l ma mb">IN[5]<br/>x = np.random.rand(100, 1)<br/>y = 100 + 5 * x + np.random.randn(100, 1)<br/>plt.scatter(x,y)<br/>plt.xlabel('input')<br/>plt.ylabel('output')</span></pre><figure class="ln lo lp lq gt md gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/4da4e112e4faf2dcd1709a67a5fbd9ef.png" data-original-src="https://miro.medium.com/v2/resize:fit:778/format:webp/1*4xm6AGeb63VGILaPnnusFA.png"/></div><p class="mk ml gj gh gi mm mn bd b be z dk translated">图二。OUT[5]，图片由作者提供</p></figure><pre class="ln lo lp lq gt lr ls lt lu aw lv bi"><span id="d8ce" class="lw lx it ls b gy ly lz l ma mb">IN[6] <em class="no">without bin</em><br/>from sklearn.linear_model import LinearRegression<br/>from sklearn.model_selection import train_test_split<br/>x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2,random_state=2021)<br/>lr = LinearRegression()<br/>lr.fit(x_train,y_train)<br/>print("score=",lr.score(x_test,y_test))<br/><strong class="ls jd">OUT[6]<br/>score= 0.7120200116547827</strong></span></pre><p id="583c" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">现在，让我们创建条块，并用条块测试新数据集。</p><pre class="ln lo lp lq gt lr ls lt lu aw lv bi"><span id="c3b8" class="lw lx it ls b gy ly lz l ma mb">IN[7] <em class="no">create bins</em><br/>bins=np.linspace(x.min()-0.01,x.max()+0.01,9)<br/>print(bins)<br/>datas_to_bins = np.digitize(x, bins=bins,right=False)<br/>np.unique(datas_to_bins)<br/><strong class="ls jd">OUT[7]<br/>[-0.00374919  0.12264801  0.24904522  0.37544243  0.50183963  0.62823684 0.75463404  0.88103125  1.00742846]<br/>array([1, 2, 3, 4, 5, 6, 7, 8], dtype=int64)</strong></span><span id="bbad" class="lw lx it ls b gy nl lz l ma mb">IN[8] <em class="no">with bins</em><br/>from sklearn.preprocessing import OneHotEncoder<br/>encoder = OneHotEncoder(sparse=False)<br/>x_binned=encoder.fit_transform(datas_to_bins)<br/>x_binned_train,x_binned_test,y_binned_train,y_binned_test=train_test_split(x_binned,y,test_size=0.2,random_state=2021)<br/>lr.fit(x_binned_train,y_binned_train)<br/>print("score=",lr.score(x_binned_test,y_binned_test))<br/><strong class="ls jd">OUT[8]<br/>score= 0.7433952534198586</strong></span></pre><p id="338e" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">可以看出，当分成9个部分并分组为8个箱时，测试数据集中的成功率从71增加到74。</p><blockquote class="np nq nr"><p id="7b72" class="kr ks no kt b ku kv kd kw kx ky kg kz ns lb lc ld nt lf lg lh nu lj lk ll lm im bi translated">宁滨不会影响基于树的算法，因为它们使用拆分日期来训练模型。另一方面，它对于线性模型相当有效。</p></blockquote><h1 id="ec49" class="mp lx it bd mq mr ms mt mu mv mw mx my ki mz kj na kl nb km nc ko nd kp ne nf bi translated">2.多项式和交互特征</h1><p id="8d88" class="pw-post-body-paragraph kr ks it kt b ku ng kd kw kx nh kg kz la ni lc ld le nj lg lh li nk lk ll lm im bi translated">可以对数据集进行的另一项改进是添加交互要素和多项式要素。如果我们考虑上一节中创建的数据集和宁滨运算，可以创建各种数学配置来增强这一点。例如，让我们以宁滨数据为例，该数据从数值变量转换为分类变量，然后用OneHotEncoder转换回数值变量。我们使用宁滨对通过添加0和1之间的100个随机数据而创建的数据集进行了分组，现在让我们将入库数据集与正常数据集合并并创建一个新数据集，或将入库数据集与正常数据集相乘并将其添加到入库数据集，或将入库数据集划分为正常数据集并将其添加到入库数据集。让我们看看所有这些配置的线性回归和得分。</p><pre class="ln lo lp lq gt lr ls lt lu aw lv bi"><span id="6852" class="lw lx it ls b gy ly lz l ma mb">IN[9]<br/>x_combined=np.hstack([x_binned,x*x_binned])<br/>print(x_binned.shape)<br/>print(x.shape)<br/>print(x_combined.shape)<br/>x_combined_train,x_combined_test,y_combined_train,y_combined_test=train_test_split(x_combined,y,test_size=0.2,random_state=2021)<br/>lr.fit(x_combined_train,y_combined_train)<br/>print("score=",lr.score(x_combined_test,y_combined_test))<br/><strong class="ls jd">OUT[9]<br/>(100, 3)<br/>(100, 1)<br/>(100, 6)<br/>score= 0.7910475179261578</strong></span><span id="e8f0" class="lw lx it ls b gy nl lz l ma mb">IN[10]<br/>x_combined2=np.hstack([x_binned,x])<br/>x_combined2_train,x_combined2_test,y_combined2_train,y_combined2_test=train_test_split(x_combined2,y,test_size=0.2,random_state=2021)<br/>lr.fit(x_combined2_train,y_combined2_train)<br/>print("score=",lr.score(x_combined2_test,y_combined2_test))<br/><strong class="ls jd">OUT[10]<br/>score= 0.7203969392138159</strong></span><span id="f38c" class="lw lx it ls b gy nl lz l ma mb">IN[11]<br/>x_combined3=np.hstack([x_binned,x_binned/x])<br/>x_combined3_train,x_combined3_test,y_combined3_train,y_combined3_test=train_test_split(x_combined3,y,test_size=0.2,random_state=2021)<br/>lr.fit(x_combined3_train,y_combined3_train)<br/>print("score=",lr.score(x_combined3_test,y_combined3_test))<br/><strong class="ls jd">OUT[11]<br/>score= 0.7019604516773869</strong></span></pre><p id="06cb" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">丰富数据集的另一种方法是使用多项式要素。通过对多项式要素列中的数据进行指定次数的幂运算来扩展数据集。例如，当在多边形特征预处理中设置4度时，它很容易与sklearn库一起使用，4个新特征将被添加为x，x，x，x⁴.现在，让我们通过在同一数据集中添加多项式要素来观察结果。</p><pre class="ln lo lp lq gt lr ls lt lu aw lv bi"><span id="196e" class="lw lx it ls b gy ly lz l ma mb">IN[12]<br/>from sklearn.preprocessing import PolynomialFeatures<br/>poly = PolynomialFeatures(degree=4, include_bias=False)<br/>x_poly=poly.fit_transform(x)<br/>poly.get_feature_names()<br/><strong class="ls jd">OUT[12]<br/>['x0', 'x0^2', 'x0^3', 'x0^4']</strong></span><span id="f0e9" class="lw lx it ls b gy nl lz l ma mb">IN[13]<br/>x_poly_train,x_poly_test,y_poly_train,y_poly_test=train_test_split(x_poly,y,test_size=0.2,random_state=2021)<br/>lr.fit(x_poly_train,y_poly_train)<br/>print("score=",lr.score(x_poly_test,y_poly_test))<br/><strong class="ls jd">OUT[13]<br/>score= 0.7459793178415801</strong></span></pre><p id="7bb0" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">使用多项式和交互功能无法获得良好结果的原因是，数据集是随机创建的。这些方法在实际项目中经常使用，效率很高。</p><h1 id="3e36" class="mp lx it bd mq mr ms mt mu mv mw mx my ki mz kj na kl nb km nc ko nd kp ne nf bi translated">3.非线性变换</h1><p id="8342" class="pw-post-body-paragraph kr ks it kt b ku ng kd kw kx nh kg kz la ni lc ld le nj lg lh li nk lk ll lm im bi translated">数据集中的数值呈高斯分布这一事实非常有利于模型学习和进行预测。它可以通过一些数学运算将数据集转换为高斯分布。这就像从另一个角度看同一数据集，就像在同一信号的频率分析中使用傅立叶变换一样。相同的mat操作应用于该列中的所有数据。现在，在我们看一下这些方法之前，让我们准备一下我们将使用的列和qq_plot。</p><pre class="ln lo lp lq gt lr ls lt lu aw lv bi"><span id="e761" class="lw lx it ls b gy ly lz l ma mb">IN[14]<br/>data=pd.read_csv('CarPrice_Assignment.csv')<br/>data.columns<br/><strong class="ls jd">OUT[14]<br/>Index(['car_ID', 'symboling', 'CarName', 'fueltype', 'aspiration',<br/>       'doornumber', 'carbody', 'drivewheel', 'enginelocation', 'wheelbase', 'carlength', 'carwidth', 'carheight', 'curbweight', 'enginetype', 'cylindernumber', 'enginesize', 'fuelsystem', 'boreratio', 'stroke','compressionratio', 'horsepower', 'peakrpm', 'citympg', 'highwaympg', 'price'],<br/>      dtype='object')</strong></span><span id="2779" class="lw lx it ls b gy nl lz l ma mb">IN[15]   <em class="no">column we will use</em><br/>data['price'].describe()<br/><strong class="ls jd">OUT[15]<br/>count      205.000000<br/>mean     13276.710571<br/>std       7988.852332<br/>min       5118.000000<br/>25%       7788.000000<br/>50%      10295.000000<br/>75%      16503.000000<br/>max      45400.000000<br/>Name: price, dtype: float64</strong></span><span id="6a99" class="lw lx it ls b gy nl lz l ma mb">IN[16] <em class="no">create histogram and qq plot</em><br/>import scipy.stats as stat<br/>import pylab<br/>def qq_plot(data,feature):<br/>    plt.figure(figsize=(12,4))<br/>    plt.subplot(1,2,1)<br/>    data[feature].hist()<br/>    plt.title('histogram')<br/>    plt.subplot(1,2,2)<br/>    stat.probplot(data[feature],dist='norm',plot=pylab)<br/>    plt.show()</span><span id="ba53" class="lw lx it ls b gy nl lz l ma mb">IN[17]<br/>qq_plot(data,'price')</span></pre><figure class="ln lo lp lq gt md gh gi paragraph-image"><div role="button" tabindex="0" class="me mf di mg bf mh"><div class="gh gi nv"><img src="../Images/cbf1f7cb08a21bd23f51964d157faa00.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OMp2iY-sJpUKUWbgCgJ0cA.png"/></div></div><p class="mk ml gj gh gi mm mn bd b be z dk translated">图3。输出[17]，直方图(左)和概率图(右)，图片由作者提供</p></figure><h2 id="abae" class="lw lx it bd mq nw nx dn mu ny nz dp my la oa ob na le oc od nc li oe of ne iz bi translated">3.1.对数变换</h2><p id="c0dd" class="pw-post-body-paragraph kr ks it kt b ku ng kd kw kx nh kg kz la ni lc ld le nj lg lh li nk lk ll lm im bi translated">列中所有数据的对数。</p><pre class="ln lo lp lq gt lr ls lt lu aw lv bi"><span id="920a" class="lw lx it ls b gy ly lz l ma mb">IN[18]<br/>data['log'] = np.log(data['price'])<br/>qq_plot(data,'log')<br/>OUT[18]</span></pre><figure class="ln lo lp lq gt md gh gi paragraph-image"><div role="button" tabindex="0" class="me mf di mg bf mh"><div class="gh gi nv"><img src="../Images/5eab145f6b097485a3613962bbd92986.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*No6XoaleZcmHAog8ogLWkg.png"/></div></div><p class="mk ml gj gh gi mm mn bd b be z dk translated">图4。输出[18]，对数变换，直方图(左)和概率图(右)，作者图片</p></figure><h2 id="98d9" class="lw lx it bd mq nw nx dn mu ny nz dp my la oa ob na le oc od nc li oe of ne iz bi translated">3.2.平方根变换</h2><p id="4ffa" class="pw-post-body-paragraph kr ks it kt b ku ng kd kw kx nh kg kz la ni lc ld le nj lg lh li nk lk ll lm im bi translated">列中所有数据的平方根。</p><pre class="ln lo lp lq gt lr ls lt lu aw lv bi"><span id="be1e" class="lw lx it ls b gy ly lz l ma mb">IN[19]<br/>data['squareroot'] = data.price**(1/2)<br/>qq_plot(data,'squareroot')</span></pre><figure class="ln lo lp lq gt md gh gi paragraph-image"><div role="button" tabindex="0" class="me mf di mg bf mh"><div class="gh gi nv"><img src="../Images/c98cff73322a5445f945035ec613cd3b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1w96HlhdvEDqoIknCGDAGg.png"/></div></div><p class="mk ml gj gh gi mm mn bd b be z dk translated">图5。输出[19]，平方根变换，直方图(左)和概率图(右)，图片作者</p></figure><h2 id="586f" class="lw lx it bd mq nw nx dn mu ny nz dp my la oa ob na le oc od nc li oe of ne iz bi translated">3.3.指数变换</h2><p id="bebc" class="pw-post-body-paragraph kr ks it kt b ku ng kd kw kx nh kg kz la ni lc ld le nj lg lh li nk lk ll lm im bi translated">列中所有数据的用户选择的指数。</p><pre class="ln lo lp lq gt lr ls lt lu aw lv bi"><span id="907a" class="lw lx it ls b gy ly lz l ma mb">IN[20]<br/>data['exp'] = data.price**(1/1.5)<br/>qq_plot(data,'exp')</span></pre><figure class="ln lo lp lq gt md gh gi paragraph-image"><div role="button" tabindex="0" class="me mf di mg bf mh"><div class="gh gi nv"><img src="../Images/f554d5562176de6e1ee0781d727b7ad1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*K_14NhaYs0cO8pSXjhcAVw.png"/></div></div><p class="mk ml gj gh gi mm mn bd b be z dk translated">图6。OUT[20]，指数变换，直方图(左)和概率图(右)，作者图片</p></figure><h2 id="1223" class="lw lx it bd mq nw nx dn mu ny nz dp my la oa ob na le oc od nc li oe of ne iz bi translated">3.4.Boxcox变换</h2><p id="f5b3" class="pw-post-body-paragraph kr ks it kt b ku ng kd kw kx nh kg kz la ni lc ld le nj lg lh li nk lk ll lm im bi translated">它根据等式应用于列。</p><figure class="ln lo lp lq gt md gh gi paragraph-image"><div class="gh gi og"><img src="../Images/7b5f67bdde566f99a64c766e1aae7ee5.png" data-original-src="https://miro.medium.com/v2/resize:fit:544/format:webp/1*lfmfQfrZTq2m8Qv918liZw.png"/></div><p class="mk ml gj gh gi mm mn bd b be z dk translated">Box-cox方程，<a class="ae mo" href="https://www.statisticshowto.com/box-cox-transformation/" rel="noopener ugc nofollow" target="_blank">来源</a></p></figure><pre class="ln lo lp lq gt lr ls lt lu aw lv bi"><span id="ccd7" class="lw lx it ls b gy ly lz l ma mb">IN[21]<br/>data['boxcox'],parameters = stat.boxcox(data['price'])<br/>print(parameters)<br/>qq_plot(data,'price')</span></pre><figure class="ln lo lp lq gt md gh gi paragraph-image"><div role="button" tabindex="0" class="me mf di mg bf mh"><div class="gh gi nv"><img src="../Images/cbf1f7cb08a21bd23f51964d157faa00.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OMp2iY-sJpUKUWbgCgJ0cA.png"/></div></div><p class="mk ml gj gh gi mm mn bd b be z dk translated">图7。OUT[21]，box-cox变换，直方图(左)和概率图(右)，图片由作者提供</p></figure><h2 id="a2c0" class="lw lx it bd mq nw nx dn mu ny nz dp my la oa ob na le oc od nc li oe of ne iz bi translated">3.5.互易变换</h2><p id="ad64" class="pw-post-body-paragraph kr ks it kt b ku ng kd kw kx nh kg kz la ni lc ld le nj lg lh li nk lk ll lm im bi translated">列中的所有数据除以1。</p><pre class="ln lo lp lq gt lr ls lt lu aw lv bi"><span id="c41c" class="lw lx it ls b gy ly lz l ma mb">IN[22]<br/>data['reciprocal'] = 1/data.price<br/>qq_plot(data,'reciprocal')</span></pre><figure class="ln lo lp lq gt md gh gi paragraph-image"><div role="button" tabindex="0" class="me mf di mg bf mh"><div class="gh gi nv"><img src="../Images/d35907712128a9a642abea604f7b5a92.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NGPkglcQM3jZD5Hmkvmf0Q.png"/></div></div><p class="mk ml gj gh gi mm mn bd b be z dk translated">图8。OUT[22]，倒数变换，直方图(左)和概率图(右)，作者图片</p></figure><h1 id="5114" class="mp lx it bd mq mr ms mt mu mv mw mx my ki mz kj na kl nb km nc ko nd kp ne nf bi translated">4.自动特征选择</h1><p id="04ad" class="pw-post-body-paragraph kr ks it kt b ku ng kd kw kx nh kg kz la ni lc ld le nj lg lh li nk lk ll lm im bi translated">在上一节中，我们丰富了我们的特征并扩展了我们的数据集，但是由于这个操作将创建一个复杂的数据集，它可能会导致过度拟合。现在，让我们研究一下根据高维数据集或复杂数据集的要素重要性减少要素的方法。</p><h2 id="afe1" class="lw lx it bd mq nw nx dn mu ny nz dp my la oa ob na le oc od nc li oe of ne iz bi translated">4.1.方差分析——ANOVA</h2><p id="b5bc" class="pw-post-body-paragraph kr ks it kt b ku ng kd kw kx nh kg kz la ni lc ld le nj lg lh li nk lk ll lm im bi translated">每个特征与目标的关系被单独分析，并且以用户选择的速率与目标具有较少关系的特征被排除。这种特征-目标关系是根据p值确定的。首先消除具有高p值的要素。现在让我们导入乳腺癌数据集，然后应用线性回归和决策树算法。</p><pre class="ln lo lp lq gt lr ls lt lu aw lv bi"><span id="1f20" class="lw lx it ls b gy ly lz l ma mb">IN[22]<br/>from sklearn.datasets import load_breast_cancer<br/>data=load_breast_cancer()<br/>x=data.data<br/>y=data.target</span><span id="4144" class="lw lx it ls b gy nl lz l ma mb">IN[23]<br/>from sklearn.linear_model import LinearRegression<br/>from sklearn.model_selection import train_test_split<br/>x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2,random_state=2021)<br/>lr = LinearRegression()<br/>lr.fit(x_train,y_train)<br/>print("score=",lr.score(x_test,y_test))<br/><strong class="ls jd">OUT[23]<br/>score= 0.7494572559981934</strong></span><span id="3da5" class="lw lx it ls b gy nl lz l ma mb">IN[24]<br/>from sklearn.tree import DecisionTreeRegressor<br/>dt = DecisionTreeRegressor(random_state=42)<br/>dt.fit(x_train, y_train)<br/>print("score on test set: ", dt.score(x_test, y_test))<br/><strong class="ls jd">OUT[24]<br/>score on test set:  0.8115079365079365</strong></span></pre><p id="c92b" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">添加决策树回归器是为了查看特征的重要性:</p><pre class="ln lo lp lq gt lr ls lt lu aw lv bi"><span id="bbca" class="lw lx it ls b gy ly lz l ma mb">IN[25]<br/>print(dt.feature_importances_)<br/><strong class="ls jd">OUT[25]<br/>[0.         0.01755418 0.         0.         0.01690402 0.00845201<br/> 0.01173891 0.         0.         0.00375645 0.00725021 0.01126935<br/> 0.00907901 0.00991574 0.00223873 0.         0.         0.<br/> 0.         0.         0.00782594 0.0397066  0.         0.71559469<br/> 0.         0.         0.01979559 0.11891856 0.         0.        ]</strong></span><span id="772d" class="lw lx it ls b gy nl lz l ma mb">IN[26]<br/>np.argsort(dt.feature_importances_)<br/><strong class="ls jd">OUT[26]<br/>array([ 0, 25, 24, 22, 19, 18, 17, 16, 15, 28, 29,  2,  3,  8,  7, 14,  9, 10, 20,  5, 12, 13, 11,  6,  4,  1, 26, 21, 27, 23], dtype=int64)</strong></span></pre><p id="5e08" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">现在让我们应用方差分析:</p><pre class="ln lo lp lq gt lr ls lt lu aw lv bi"><span id="98a5" class="lw lx it ls b gy ly lz l ma mb">IN[27]<br/>from sklearn.feature_selection import SelectPercentile<br/>select = SelectPercentile(percentile=30)<br/>select.fit(x_train, y_train)<br/># transform training set<br/>x_train_selected = select.transform(x_train)<br/>print("X_train.shape: {}".format(x_train.shape))<br/>print("X_train_selected.shape: {}".format(x_train_selected.shape))<br/><strong class="ls jd">OUT[27]<br/>X_train.shape: (455, 30)<br/>X_train_selected.shape: (455, 9)</strong></span></pre><p id="bd79" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">百分比设置为30，以便选择30%的特征(9)。</p><p id="6d93" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">具有选定特征的线性回归:</p><pre class="ln lo lp lq gt lr ls lt lu aw lv bi"><span id="9dce" class="lw lx it ls b gy ly lz l ma mb">IN[28]<br/>x_test_selected = select.transform(x_test)<br/>lr.fit(x_train_selected, y_train)<br/>print("Score with only selected features: {:.3f}".format(<br/>lr.score(x_test_selected, y_test)))<br/><strong class="ls jd">OUT[28]<br/>Score with only selected features: 0.712</strong></span></pre><p id="bd1b" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">仅用9个特征获得0.712，而用30个特征获得0.749。现在让我们看看SelectPercentile选择了哪些功能:</p><pre class="ln lo lp lq gt lr ls lt lu aw lv bi"><span id="748f" class="lw lx it ls b gy ly lz l ma mb">IN[29]<br/>mask = select.get_support()<br/>print(mask)<br/><strong class="ls jd">OUT[29]<br/>[ True False  True  True False False  True  True False False False False False False False False False False False False  True False  True True False False False  True False False]</strong></span></pre><h2 id="9f63" class="lw lx it bd mq nw nx dn mu ny nz dp my la oa ob na le oc od nc li oe of ne iz bi translated">4.2.基于模型的特征选择</h2><p id="425d" class="pw-post-body-paragraph kr ks it kt b ku ng kd kw kx nh kg kz la ni lc ld le nj lg lh li nk lk ll lm im bi translated">它一次评估所有特性，并根据它们的相互作用选择特性。它根据用户设置的阈值选择重要性较高的内容。例如，如果选择阈值=中等，则选择50%的特征。Sklearn中阈值的默认值是均值。</p><pre class="ln lo lp lq gt lr ls lt lu aw lv bi"><span id="7dd1" class="lw lx it ls b gy ly lz l ma mb">IN[30]<br/>from sklearn.feature_selection import SelectFromModel<br/>selection = SelectFromModel(LinearRegression(), threshold="median")<br/>selection.fit(x_train, y_train)<br/>x_train_select = selection.transform(x_train)<br/>print("X_train.shape:",x_train.shape)<br/>print("X_train_l1.shape:",x_train_select.shape)<br/><strong class="ls jd">OUT[30]<br/>X_train.shape: (455, 30)<br/>X_train_l1.shape: (455, 15)</strong></span><span id="e3ce" class="lw lx it ls b gy nl lz l ma mb">IN[31]<br/>mask = select.get_support()<br/>print(mask)<br/><strong class="ls jd">OUT[31]<br/>[False False False False  True  True  True  True False  True False False False False  True  True  True  True  True  True False False False False True False False  True  True  True]</strong></span></pre><p id="1b76" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">具有选定特征的线性回归:</p><pre class="ln lo lp lq gt lr ls lt lu aw lv bi"><span id="3e2b" class="lw lx it ls b gy ly lz l ma mb">IN[32]<br/>x_test_select = selection.transform(x_test)<br/>print(lr.fit(x_train_select, y_train).score(x_test_select, y_test))<br/><strong class="ls jd">OUT[32]<br/>0.6919232554797755</strong></span></pre><h2 id="79f6" class="lw lx it bd mq nw nx dn mu ny nz dp my la oa ob na le oc od nc li oe of ne iz bi translated">4.3.迭代特征选择</h2><p id="80a6" class="pw-post-body-paragraph kr ks it kt b ku ng kd kw kx nh kg kz la ni lc ld le nj lg lh li nk lk ll lm im bi translated">它根据某个阈值以2种方式工作:第一种从0个特征开始，根据其重要性继续添加特征，直到达到阈值。第二个选择所有特征并逐个消除它们，直到阈值。顾名思义，<em class="no">递归特征消除(RFE) </em>选择所有特征，消除特征，直到指定条件。</p><pre class="ln lo lp lq gt lr ls lt lu aw lv bi"><span id="b818" class="lw lx it ls b gy ly lz l ma mb">IN[33]<br/>from sklearn.feature_selection import RFE<br/>from sklearn.linear_model import LogisticRegression<br/>select = RFE(LogisticRegression(),<br/>n_features_to_select=20)<br/>select.fit(x_train, y_train)<br/># visualize the selected features:<br/>mask = select.get_support()<br/>print(mask)<br/><strong class="ls jd">OUT[33]<br/>[ True  True False False  True  True  True  True  True False  True  True True  True False False False False False False  True  True  True False True  True  True  True  True  True]</strong></span></pre><p id="7ee8" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">条件设置为20个特征，从30个特征开始，一个接一个地消除特征，直到剩下20个特征。</p><pre class="ln lo lp lq gt lr ls lt lu aw lv bi"><span id="bf2e" class="lw lx it ls b gy ly lz l ma mb">IN[34]<br/>x_train_rfe= select.transform(x_train)<br/>x_test_rfe= select.transform(x_test)<br/>lr.fit(x_train_rfe, y_train)<br/>print("score:",lr.score(x_test_rfe, y_test))<br/><strong class="ls jd">OUT[34]<br/>score: 0.7140632795679898</strong></span></pre><h2 id="6489" class="lw lx it bd mq nw nx dn mu ny nz dp my la oa ob na le oc od nc li oe of ne iz bi translated">回到指引点击<a class="ae mo" href="https://ibrahimkovan.medium.com/machine-learning-guideline-959da5c6f73d" rel="noopener">这里</a>。</h2><div class="oh oi gp gr oj ok"><a href="https://ibrahimkovan.medium.com/machine-learning-guideline-959da5c6f73d" rel="noopener follow" target="_blank"><div class="ol ab fo"><div class="om ab on cl cj oo"><h2 class="bd jd gy z fp op fr fs oq fu fw jc bi translated">机器学习指南</h2><div class="or l"><h3 class="bd b gy z fp op fr fs oq fu fw dk translated">本文旨在准备一个机器学习数据库，以便在一个视图中显示所有的机器学习标题。这个…</h3></div><div class="os l"><p class="bd b dl z fp op fr fs oq fu fw dk translated">ibrahimkovan.medium.com</p></div></div><div class="ot l"><div class="ou l ov ow ox ot oy mi ok"/></div></div></a></div></div></div>    
</body>
</html>