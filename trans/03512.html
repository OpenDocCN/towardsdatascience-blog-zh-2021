<html>
<head>
<title>Beyond Single Point Estimations using Neural Networks</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用神经网络超越单点估计</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/estimate-of-a-probability-density-function-through-neural-networks-f0de20b49de5?source=collection_archive---------12-----------------------#2021-03-21">https://towardsdatascience.com/estimate-of-a-probability-density-function-through-neural-networks-f0de20b49de5?source=collection_archive---------12-----------------------#2021-03-21</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="779a" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">用Python估算未知数据生成过程的观测数据的PDF。</h2></div><p id="d091" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">机器学习算法通常估计未知数据生成过程的第一矩(即均值)。然而，超越分布拟合一阶矩的应用可以在各种不同的领域找到，如经济[2]，工程[3]和自然科学[4]。例如，观察金融市场并试图指导投资者的神经网络可以计算股票市场运动的概率以及运动的方向。为此，可以使用概率密度函数来计算连续随机变量范围出现的总概率。</p><p id="a64e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">用于密度估计的传统方法(例如这里的<a class="ae lb" href="https://medium.com/hal24k-techblog/a-guide-to-generating-probability-distributions-with-neural-networks-ffc4efacd6a4" rel="noopener">和</a>)是基于函数的统计模型的初步选择和随后对其参数的拟合。例如，神经网络输出的均值(μ)和标准差(σ)参数足以描述正态分布的密度函数:</p><figure class="ld le lf lg gt lh gh gi paragraph-image"><div class="gh gi lc"><img src="../Images/375abe9008b2dbdcd35ec5c45cc6c8f4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1196/format:webp/1*Fl4g6vWFbssTPGYOa35jEw@2x.png"/></div></figure><p id="b2cd" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">然而，什么id。没有关于数据生成过程的先验知识？Aristidis Likas [1]在早期工作中首次提出了一种基于贝叶斯统计的替代方法。这种简单但有效的方法不需要对可用数据进行任何假设，而是从神经网络的输出中提取概率密度函数，该神经网络用合适的数据库进行训练，该数据库包括原始数据和一些具有已知分布的特别创建的数据。不幸的是，最初的实现并不适合现代计算环境，因此，在本文中，我将演示如何在Python和TensorFlow的上下文中使用该方法。</p></div><div class="ab cl lk ll hu lm" role="separator"><span class="ln bw bk lo lp lq"/><span class="ln bw bk lo lp lq"/><span class="ln bw bk lo lp"/></div><div class="ij ik il im in"><h2 id="47b9" class="lr ls iq bd lt lu lv dn lw lx ly dp lz ko ma mb mc ks md me mf kw mg mh mi mj bi translated">设计数据生成流程</h2><p id="b26c" class="pw-post-body-paragraph kf kg iq kh b ki mk jr kk kl ml ju kn ko mm kq kr ks mn ku kv kw mo ky kz la ij bi translated">为了便于说明，我们可以创建一些简单的数据生成过程，如下图所示，其中使用单位指数Xs的条件正态分布生成结果。因此，每个个体都将具有以x为条件的唯一概率密度函数(PDF)。</p><pre class="ld le lf lg gt mp mq mr ms aw mt bi"><span id="6c18" class="lr ls iq mq b gy mu mv l mw mx">def hi_sample(N):<br/>    fx = lambda x: np.random.normal(loc = np.mean(x[:,0:-1],1), size=N)<br/>    X1 = np.random.exponential(1, size=N)<br/>    X2 = np.random.exponential(1, size=N)<br/>    Y =  fx(np.array([X1,X2]).T)<br/>    hi_data = [X1, X2, Y]<br/>    return np.array(hi_data).T<br/><br/>train_array = hi_sample(5000)</span></pre><p id="936c" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">生成的Y将具有以下分布:</p><figure class="ld le lf lg gt lh gh gi paragraph-image"><div role="button" tabindex="0" class="mz na di nb bf nc"><div class="gh gi my"><img src="../Images/7ec387bf1eaff502fb5f71fdab01e22a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*v3gyBU8mwLQhxrXGxleQJg@2x.png"/></div></div><p class="nd ne gj gh gi nf ng bd b be z dk translated">作者图片</p></figure></div><div class="ab cl lk ll hu lm" role="separator"><span class="ln bw bk lo lp lq"/><span class="ln bw bk lo lp lq"/><span class="ln bw bk lo lp"/></div><div class="ij ik il im in"><h2 id="3f94" class="lr ls iq bd lt lu lv dn lw lx ly dp lz ko ma mb mc ks md me mf kw mg mh mi mj bi translated"><strong class="ak">概率密度估计的基本方法</strong></h2><p id="7ffc" class="pw-post-body-paragraph kf kg iq kh b ki mk jr kk kl ml ju kn ko mm kq kr ks mn ku kv kw mo ky kz la ij bi translated">在这里，我们不讨论原著中的所有数学术语，而是强调用python实现所需的基本步骤:</p><ol class=""><li id="e4fc" class="nh ni iq kh b ki kj kl km ko nj ks nk kw nl la nm nn no np bi translated">用已知的数据生成过程(创建先验分布)扩充原始数据集，该过程来自参考PDF: <strong class="kh ir"> P(ref) </strong>。这里，我们假设扩充数据集中所有个体的结果将从标准正态分布中生成:</li></ol><pre class="ld le lf lg gt mp mq mr ms aw mt bi"><span id="07dc" class="lr ls iq mq b gy mu mv l mw mx">train_array_0 = train_array.copy()<br/>train_array_0[:,-1] = metropolis_hastings_normal(train_array_0[:,-1]) #regenerate the outcome f(x) using metropolis_hastings simulators with a standard normal distribution. <br/>train_data = [train_array,train_array_0]</span></pre><p id="0bcd" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">2.用二进制判别神经网络拟合扩充的数据集，该网络估计从<strong class="kh ir"> pr(X) </strong>或从目标PDF: <strong class="kh ir"> p(X) </strong>生成观察对<strong class="kh ir"> X:=(x，f(x)) </strong>的概率<strong class="kh ir">y(X)</strong>。它可以是这样一个简单的网络:</p><pre class="ld le lf lg gt mp mq mr ms aw mt bi"><span id="9fe3" class="lr ls iq mq b gy mu mv l mw mx">def simple_nn(dim_x):<br/>    input_x = tfkl.Input(shape=(dim_x+1))<br/>    combined_layer = tfkl.Dense(dim_x)(input_x)<br/>    for i in range(5):<br/>        combined_layer = tfkl.Dense(dim_x)(combined_layer)<br/>        combined_layer = tfkl.Dropout(0.1)(combined_layer)<br/>    out = tfkl.Dense(1, activation='sigmoid')(combined_layer)<br/>    model = tf.keras.Model(inputs = input_x, outputs=out)<br/>    model.compile(loss = 'binary_crossentropy',<br/>                  optimizer=tf.keras.optimizers.RMSprop(lr=0.001))<br/>    return model</span></pre><p id="acb6" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">3.使用贝叶斯理论计算目标概率密度函数:</p><figure class="ld le lf lg gt lh gh gi paragraph-image"><div class="gh gi lc"><img src="../Images/4eee02d760bda7015aa8b50d2ee2fd7d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1196/format:webp/1*exzXhy2K7_1mH0JmgS_sbA@2x.png"/></div></figure><p id="40a3" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">因此，用特意扩充的数据集训练NN产生了神经函数<strong class="kh ir"> y(X) </strong>，通过使用中间的任意PDF <strong class="kh ir"> pr(X) </strong>，可以从该神经函数中推断出未知的PDF <strong class="kh ir"> p(X) </strong>:</p><pre class="ld le lf lg gt mp mq mr ms aw mt bi"><span id="64d1" class="lr ls iq mq b gy mu mv l mw mx">conditional_y = pdf_function_i(x).numpy()<br/>reference_dense =  norm.pdf(x[:,-1]).reshape(-1, 1) #norm(loc = np.sum(x[:,0:-1],1)).pdf(x[:,-1]).reshape(-1, 1)<br/>conditional_dense = (conditional_y / np.clip(1 - conditional_y, a_min=1e-3, a_max=1.0))  * reference_dense</span></pre><p id="84e1" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在让我们用真实的PDF来绘制估计的PDF (conditional_dense ):</p><figure class="ld le lf lg gt lh gh gi paragraph-image"><div role="button" tabindex="0" class="mz na di nb bf nc"><div class="gh gi nq"><img src="../Images/eefd0e03ccfaa88b7187d18abfdf28e8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8k7JeDMt6XH28A5RA5b2CQ@2x.png"/></div></div><p class="nd ne gj gh gi nf ng bd b be z dk translated">条件正态分布的估计。作者图片</p></figure><p id="7367" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">注意这里每个单独的点都有一个以给定的x为条件的正态分布。</p><p id="9645" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">让我们看看从标准密度函数导出的其他几个条件分布的估计:</p><figure class="ld le lf lg gt lh gh gi paragraph-image"><div role="button" tabindex="0" class="mz na di nb bf nc"><div class="gh gi nr"><img src="../Images/d09d05bbfbcde0b19fc5c8af780177fc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Vf-teDs6pC5N8rdWpOOp3Q@2x.png"/></div></div><p class="nd ne gj gh gi nf ng bd b be z dk translated">条件均匀分布的估计。作者图片</p></figure><figure class="ld le lf lg gt lh gh gi paragraph-image"><div role="button" tabindex="0" class="mz na di nb bf nc"><div class="gh gi nr"><img src="../Images/1b8dcbb32ba14a3db718db04a8c1c6d9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bpzrDOFfZC8-Gvpz4Fo3Yw@2x.png"/></div></div><p class="nd ne gj gh gi nf ng bd b be z dk translated">条件指数分布的估计。作者图片</p></figure><p id="f518" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">使用相同的标准正态先验，我们能够从不同的原始数据生成过程中获得相当准确的单个pdf的估计。我们能进一步提高估计值吗？是的。在Leonardo Reyneri等人[5]的另一项工作中，作者建议使用初始估计的估计PDF<strong class="kh ir">p’(X)</strong>作为调整估计的先验。也就是说，我们重复第1步到第2步，用<strong class="kh ir">p’(X)，</strong>代替标准的正态先验，在第三步中，我们估计以下量:</p><figure class="ld le lf lg gt lh gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/242e568be34d7c6053337399b7c390d2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1216/format:webp/1*m0Ron2v5O_BUyFlL3ENfOA@2x.png"/></div></figure><p id="99e7" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">因此，我们可以得到PDF的原始估计的一步调整。这背后的直觉类似于自动编码器，其中如果神经网络不能区分估计的PDF和先前的PDF，则估计的PDF将是最佳的。</p></div><div class="ab cl lk ll hu lm" role="separator"><span class="ln bw bk lo lp lq"/><span class="ln bw bk lo lp lq"/><span class="ln bw bk lo lp"/></div><div class="ij ik il im in"><h2 id="6eb3" class="lr ls iq bd lt lu lv dn lw lx ly dp lz ko ma mb mc ks md me mf kw mg mh mi mj bi translated">参考</h2><p id="8010" class="pw-post-body-paragraph kf kg iq kh b ki mk jr kk kl ml ju kn ko mm kq kr ks mn ku kv kw mo ky kz la ij bi translated">[1] Likas，A. (2001年)。用人工神经网络估计概率密度。<em class="nt">计算机物理通信</em>，<em class="nt"> 135 </em> (2)，167–175。</p><p id="cd15" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">[2] Combes，c .，Dussauchoy，a .:用于拟合股票交易中开盘/收盘资产价格和收益的广义极值分布。施普林格运营研究6(1)，3–26(2006)</p><p id="20d5" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">[3] Zaharim，a .，Razali，A.M .，Abidin，R.Z .，Sopian，k .:马来西亚风速数据的统计分布拟合。欧洲科学研究杂志26(1)，6–12(2009)</p><p id="9bbb" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">[4] Wendell Cropper Jr .，p .，Anderson，P.J .:热带棕榈的种群动态:使用遗传算法进行逆参数估计。生态模型177，119–127(2004)</p><p id="445a" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">[5]雷诺里、科拉和范努齐(2011年6月)。通过神经网络估计概率密度函数。在<em class="nt">人工神经网络国际工作会议上</em>(第57-64页)。斯普林格，柏林，海德堡。</p></div></div>    
</body>
</html>