<html>
<head>
<title>FasterAI: a library to make smaller and faster neural networks</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">FasterAI:制作更小更快的神经网络的库</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/fasterai-a-library-to-make-smaller-and-faster-neural-networks-70c3ff2e2ba3?source=collection_archive---------16-----------------------#2021-11-18">https://towardsdatascience.com/fasterai-a-library-to-make-smaller-and-faster-neural-networks-70c3ff2e2ba3?source=collection_archive---------16-----------------------#2021-11-18</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="f2c9" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsdatascience.com/tagged/making-sense-of-big-data" rel="noopener" target="_blank">理解大数据</a></h2><div class=""/><div class=""><h2 id="4683" class="pw-subtitle-paragraph jw iz iq bd b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn dk translated">FasterAI包括一组用于神经网络的压缩技术，构建于Fastai和Pytorch之上</h2></div><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ko"><img src="../Images/981c79e9c3ae751913628ecbdf5bc324.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-_DXq4IDiPttPMpd6OPH9w.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">作者图片</p></figure><h1 id="7fd9" class="le lf iq bd lg lh li lj lk ll lm ln lo kf lp kg lq ki lr kj ls kl lt km lu lv bi translated">FasterAI简介</h1><p id="49e4" class="pw-post-body-paragraph lw lx iq ly b lz ma ka mb mc md kd me mf mg mh mi mj mk ml mm mn mo mp mq mr ij bi translated"><em class="ms"> →图书馆可以在这里找到</em><a class="ae mt" href="https://github.com/nathanhubens/fasterai" rel="noopener ugc nofollow" target="_blank"><em class="ms"/></a></p><p id="2b08" class="pw-post-body-paragraph lw lx iq ly b lz mu ka mb mc mv kd me mf mw mh mi mj mx ml mm mn my mp mq mr ij bi translated">FasterAI是一个项目，它通过使用<a class="ae mt" href="https://github.com/fastai/fastai" rel="noopener ugc nofollow" target="_blank"> fastai </a>库来使神经网络变得更小更快。这里实现的技术可以很容易地与普通Pytorch一起使用，但是我们的想法是以一种抽象和易于使用的方式来表达它们(<em class="ms"> à la </em> fastai)。</p><p id="4cd8" class="pw-post-body-paragraph lw lx iq ly b lz mu ka mb mc mv kd me mf mw mh mi mj mx ml mm mn my mp mq mr ij bi translated">在本文中，我们将通过一个用例来解释如何使用FasterAI。</p></div><div class="ab cl mz na hu nb" role="separator"><span class="nc bw bk nd ne nf"/><span class="nc bw bk nd ne nf"/><span class="nc bw bk nd ne"/></div><div class="ij ik il im in"><h2 id="69f5" class="ng lf iq bd lg nh ni dn lk nj nk dp lo mf nl nm lq mj nn no ls mn np nq lu iw bi translated"><em class="nr">准备好了吗？那我们开始吧！</em></h2><p id="ee53" class="pw-post-body-paragraph lw lx iq ly b lz ma ka mb mc md kd me mf mg mh mi mj mk ml mm mn mo mp mq mr ij bi translated">为了演示的目的，让我们先了解一些背景知识。假设我们想要在存储容量有限的移动设备上部署一个VGG16模型，并且我们的任务要求我们的模型运行足够快。众所周知，参数和速度效率不是VGG16的强项，但让我们看看我们能做些什么。</p><p id="a5f2" class="pw-post-body-paragraph lw lx iq ly b lz mu ka mb mc mv kd me mf mw mh mi mj mx ml mm mn my mp mq mr ij bi translated">我们先来检查一下VGG16的参数个数和推断时间。</p><pre class="kp kq kr ks gt ns nt nu nv aw nw bi"><span id="5fe8" class="ng lf iq nt b gy nx ny l nz oa">learn = Learner(dls, vgg16_bn(num_classes=10), metrics=accuracy)</span></pre><p id="a47f" class="pw-post-body-paragraph lw lx iq ly b lz mu ka mb mc mv kd me mf mw mh mi mj mx ml mm mn my mp mq mr ij bi translated">因此，VGG16有1.34亿个参数</p><pre class="kp kq kr ks gt ns nt nu nv aw nw bi"><span id="8a39" class="ng lf iq nt b gy nx ny l nz oa">Total parameters : 134,309,962</span></pre><p id="44c4" class="pw-post-body-paragraph lw lx iq ly b lz mu ka mb mc mv kd me mf mw mh mi mj mx ml mm mn my mp mq mr ij bi translated">并花费4.03 <em class="ms"> </em> ms对单幅图像进行推断。</p><pre class="kp kq kr ks gt ns nt nu nv aw nw bi"><span id="f70e" class="ng lf iq nt b gy nx ny l nz oa">4.03 ms ± 18.5 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)</span></pre><p id="5553" class="pw-post-body-paragraph lw lx iq ly b lz mu ka mb mc mv kd me mf mw mh mi mj mx ml mm mn my mp mq mr ij bi translated">啪！这超出了我们的部署能力，理想情况下，我们希望我们的模型只需要一半的容量…但是我们应该放弃吗？不，实际上有很多技术可以帮助我们减小模型的尺寸并提高速度！让我们看看如何用FasterAI应用它们。</p><p id="807e" class="pw-post-body-paragraph lw lx iq ly b lz mu ka mb mc mv kd me mf mw mh mi mj mx ml mm mn my mp mq mr ij bi translated">我们将首先训练我们的VGG16模型，以便对它的性能有一个基准。</p><pre class="kp kq kr ks gt ns nt nu nv aw nw bi"><span id="c3a7" class="ng lf iq nt b gy nx ny l nz oa">learn.fit_one_cycle(10, 1e-4)</span><span id="4cf5" class="ng lf iq nt b gy ob ny l nz oa">epoch | train_loss | valid_loss | accuracy | time<br/>0     | 2.016354   | 1.778865   | 0.3689170| 1:31<br/>1     | 1.777570   | 1.508860   | 0.5235670| 1:31 <br/>2     | 1.436139   | 1.421571   | 0.5691720| 1:32<br/>3     | 1.275864   | 1.118840   | 0.6300640| 1:31<br/>4     | 1.136620   | 0.994999   | 0.6878980| 1:31<br/>5     | 0.970474   | 0.824344   | 0.7396180| 1:31<br/>6     | 0.878756   | 0.764273   | 0.7656050| 1:32<br/>7     | 0.817084   | 0.710727   | 0.7819110| 1:31<br/>8     | 0.716041   | 0.625853   | 0.8048410| 1:31<br/>9     | 0.668815   | 0.605727   | 0.8109550| 1:31</span></pre><p id="54e1" class="pw-post-body-paragraph lw lx iq ly b lz mu ka mb mc mv kd me mf mw mh mi mj mx ml mm mn my mp mq mr ij bi translated">因此，我们希望我们的网络具有相当的精度，但参数更少，运行速度更快……我们将展示如何使用的第一项技术叫做<strong class="ly ja">知识蒸馏</strong></p></div><div class="ab cl mz na hu nb" role="separator"><span class="nc bw bk nd ne nf"/><span class="nc bw bk nd ne nf"/><span class="nc bw bk nd ne"/></div><div class="ij ik il im in"><h1 id="0641" class="le lf iq bd lg lh oc lj lk ll od ln lo kf oe kg lq ki of kj ls kl og km lu lv bi translated">知识蒸馏</h1><p id="32e0" class="pw-post-body-paragraph lw lx iq ly b lz ma ka mb mc md kd me mf mg mh mi mj mk ml mm mn mo mp mq mr ij bi translated">知识提炼是一种简单但非常有效的训练模型的方法。卡鲁阿纳等人于2006年推出了这一技术。背后的主要思想是用一个小模型(称为<strong class="ly ja">学生</strong>)来近似一个更大的高性能模型(称为<strong class="ly ja">老师</strong>)所学的函数。这可以通过使用大模型来伪标记数据来实现。这个想法最近已经被用来打破ImageNet上最先进的精确度。</p><p id="9274" class="pw-post-body-paragraph lw lx iq ly b lz mu ka mb mc mv kd me mf mw mh mi mj mx ml mm mn my mp mq mr ij bi translated">当我们训练模型进行分类时，我们通常使用softmax作为最后一层。这个softmax的特点是将低值逻辑值挤向0，将最高值逻辑值挤向1。这实际上完全丢失了所有的类间信息，或者有时被称为<em class="ms">黑暗知识</em>。这是有价值的信息，我们希望从老师那里传递给学生。</p><p id="18e4" class="pw-post-body-paragraph lw lx iq ly b lz mu ka mb mc mv kd me mf mw mh mi mj mx ml mm mn my mp mq mr ij bi translated">为此，我们仍然使用常规分类损失，但同时，我们将使用另一个损失，在老师的<em class="ms">软化</em>逻辑(我们的<em class="ms">软标签</em>)和学生的<em class="ms">软化</em>逻辑(我们的<em class="ms">软预测</em>)之间计算。这些软值是在使用soft-softmax时获得的，这样可以避免在其输出端挤压这些值。我们的实施遵循<a class="ae mt" href="http://cs230.stanford.edu/files_winter_2018/projects/6940224.pdf" rel="noopener ugc nofollow" target="_blank">本文</a>，培训的基本原则如下图所示:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi oh"><img src="../Images/cfe90c08bce47587e93014d1287ba824.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Z0t7GzrkI1-XLlkqlKLkBQ.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">作者图片</p></figure><p id="afa0" class="pw-post-body-paragraph lw lx iq ly b lz mu ka mb mc mv kd me mf mw mh mi mj mx ml mm mn my mp mq mr ij bi translated">要在FasterAI中使用知识蒸馏，您只需在训练学生模型时使用此回调函数:</p><pre class="kp kq kr ks gt ns nt nu nv aw nw bi"><span id="7643" class="ng lf iq nt b gy nx ny l nz oa"><em class="ms">KnowledgeDistillation(student, teacher)</em></span></pre><blockquote class="oi oj ok"><p id="31cb" class="lw lx ms ly b lz mu ka mb mc mv kd me ol mw mh mi om mx ml mm on my mp mq mr ij bi translated"><em class="iq">你只需要给回调函数你的学生学习者和你的教师学习者。在幕后，FasterAI将使用知识精华来制作您的火车模型。</em></p></blockquote><p id="d1dc" class="pw-post-body-paragraph lw lx iq ly b lz mu ka mb mc mv kd me mf mw mh mi mj mx ml mm mn my mp mq mr ij bi translated">首先要做的是找到一个老师，可以是任何模型，最好表现良好。我们将选择VGG19进行演示。为了确保它比我们的VGG16模型性能更好，让我们从预训练版本开始。</p><pre class="kp kq kr ks gt ns nt nu nv aw nw bi"><span id="6622" class="ng lf iq nt b gy nx ny l nz oa">teacher = cnn_learner(dls, vgg19_bn, metrics=accuracy)<br/>teacher.fit_one_cycle(3, 1e-4)</span><span id="35dd" class="ng lf iq nt b gy ob ny l nz oa">epoch | train_loss | valid_loss | accuracy | time<br/>0     | 0.249884   | 0.088749   | 0.9727390| 1:02<br/>1     | 0.201829   | 0.087495   | 0.9742680| 1:02<br/>2     | 0.261882   | 0.082631   | 0.9740130| 1:02</span></pre><p id="f9d5" class="pw-post-body-paragraph lw lx iq ly b lz mu ka mb mc mv kd me mf mw mh mi mj mx ml mm mn my mp mq mr ij bi translated">我们的老师有97.4%的准确率，这已经很不错了，它已经准备好保护学生了。因此，让我们创建我们的学生模型，并通过知识蒸馏回拨对其进行培训:</p><pre class="kp kq kr ks gt ns nt nu nv aw nw bi"><span id="2753" class="ng lf iq nt b gy nx ny l nz oa">student = Learner(dls, vgg16_bn(num_classes=10), metrics=accuracy)<br/>student.fit_one_cycle(10, 1e-4, cbs=KnowledgeDistillation(student, teacher))</span><span id="6f95" class="ng lf iq nt b gy ob ny l nz oa">epoch | train_loss | valid_loss | accuracy | time<br/>0     | 2.323744   | 2.102873   | 0.4109550| 2:16<br/>1     | 2.099557   | 2.441147   | 0.5714650| 2:16 <br/>2     | 1.829197   | 2.215419   | 0.6076430| 2:16<br/>3     | 1.617705   | 1.683477   | 0.6670060| 2:16<br/>4     | 1.364808   | 1.366435   | 0.7133760| 2:16<br/>5     | 1.257906   | 0.985063   | 0.7880250| 2:16<br/>6     | 1.087404   | 0.877424   | 0.8010190| 2:16<br/>7     | 0.949960   | 0.777630   | 0.8221660| 2:16<br/>8     | 0.868683   | 0.733206   | 0.8377070| 2:16<br/>9     | 0.756630   | 0.707806   | 0.8430570| 2:16</span></pre><p id="895f" class="pw-post-body-paragraph lw lx iq ly b lz mu ka mb mc mv kd me mf mw mh mi mj mx ml mm mn my mp mq mr ij bi translated">我们可以看到，老师的知识确实对学生有用，因为它明显超出了基线VGG16。</p><p id="1d73" class="pw-post-body-paragraph lw lx iq ly b lz mu ka mb mc mv kd me mf mw mh mi mj mx ml mm mn my mp mq mr ij bi translated">好了，现在我们能够从一个给定的模型中得到更多，这有点酷！通过一些实验，我们可以设计出比VGG16更小的模型，但能够达到与我们的基线相同的性能！您可以稍后尝试自己找到它，但现在让我们继续下一项技术！</p></div><div class="ab cl mz na hu nb" role="separator"><span class="nc bw bk nd ne nf"/><span class="nc bw bk nd ne nf"/><span class="nc bw bk nd ne"/></div><div class="ij ik il im in"><h1 id="ae92" class="le lf iq bd lg lh oc lj lk ll od ln lo kf oe kg lq ki of kj ls kl og km lu lv bi translated">稀疏化</h1><p id="91e3" class="pw-post-body-paragraph lw lx iq ly b lz ma ka mb mc md kd me mf mg mh mi mj mk ml mm mn mo mp mq mr ij bi translated">现在我们有了一个比基线表现更好的学生模型，我们有一些空间来压缩它。我们将从稀疏网络开始。</p><p id="b2fb" class="pw-post-body-paragraph lw lx iq ly b lz mu ka mb mc mv kd me mf mw mh mi mj mx ml mm mn my mp mq mr ij bi translated"><strong class="ly ja"> <em class="ms">注</em> </strong> <em class="ms">:通常情况下，使网络稀疏的过程称为剪枝。当参数实际上从网络中移除时，我们更喜欢使用术语修剪，这将在下一节中进行。</em></p><p id="e7f2" class="pw-post-body-paragraph lw lx iq ly b lz mu ka mb mc mv kd me mf mw mh mi mj mx ml mm mn my mp mq mr ij bi translated">在FasterAI中，稀疏化过程也是通过使用回调来管理的，回调将在训练期间用零替换模型中最不重要的参数。回调函数有各种各样的参数来调整您的稀疏化操作，让我们来看看它们:</p><pre class="kp kq kr ks gt ns nt nu nv aw nw bi"><span id="6b5f" class="ng lf iq nt b gy nx ny l nz oa"><em class="ms">SparsifyCallback(sparsity, granularity, method, criteria, sched_func)</em></span></pre><ul class=""><li id="ed38" class="oo op iq ly b lz mu mc mv mf oq mj or mn os mr ot ou ov ow bi translated"><strong class="ly ja">稀疏度</strong>:你想要的网络稀疏度的百分比</li><li id="41a0" class="oo op iq ly b lz ox mc oy mf oz mj pa mn pb mr ot ou ov ow bi translated"><strong class="ly ja">粒度</strong>:你希望在什么粒度上进行稀疏化操作(<code class="fe pc pd pe nt b">weight</code>、<code class="fe pc pd pe nt b">kernel</code>、<code class="fe pc pd pe nt b">filter</code>、…)</li><li id="4699" class="oo op iq ly b lz ox mc oy mf oz mj pa mn pb mr ot ou ov ow bi translated"><strong class="ly ja">方法</strong>:无论是<code class="fe pc pd pe nt b">local</code>还是<code class="fe pc pd pe nt b">global</code>，都会影响参数的选择，是在每一层单独选择(<code class="fe pc pd pe nt b">local</code>)还是在整个网络上选择(<code class="fe pc pd pe nt b">global</code>)。</li><li id="3575" class="oo op iq ly b lz ox mc oy mf oz mj pa mn pb mr ot ou ov ow bi translated"><strong class="ly ja">标准</strong>:用于选择移除哪些参数的标准(<code class="fe pc pd pe nt b">large_final</code>、<code class="fe pc pd pe nt b">movement</code>、…)</li><li id="488b" class="oo op iq ly b lz ox mc oy mf oz mj pa mn pb mr ot ou ov ow bi translated"><strong class="ly ja"> sched_func </strong>:你希望按照哪个调度进行稀疏化(目前支持:<a class="ae mt" href="https://docs.fast.ai/callback.html#Annealing-functions" rel="noopener ugc nofollow" target="_blank">fastai</a>的任意调度函数，即<code class="fe pc pd pe nt b">annealing_linear</code>，<code class="fe pc pd pe nt b">annealing_cos</code>，...而且还有经典的时间表如<code class="fe pc pd pe nt b">one_shot</code>、<code class="fe pc pd pe nt b">iterative</code>、<code class="fe pc pd pe nt b">annealing_gradual</code>、朱<a class="ae mt" href="https://openreview.net/pdf?id=Sy1iIDkPM" rel="noopener ugc nofollow" target="_blank">古普塔&amp;提出的时间表</a></li></ul><p id="1e64" class="pw-post-body-paragraph lw lx iq ly b lz mu ka mb mc mv kd me mf mw mh mi mj mx ml mm mn my mp mq mr ij bi translated">但是让我们回到我们的例子上来！</p><p id="7066" class="pw-post-body-paragraph lw lx iq ly b lz mu ka mb mc mv kd me mf mw mh mi mj mx ml mm mn my mp mq mr ij bi translated">这里，我们将使我们的网络40%稀疏，并删除整个过滤器，本地选择并基于<em class="ms"> L1 </em>规范。我们将以稍小的学习率进行训练，以温和地对待我们的网络，因为它已经被训练过了。所选择的调度是一个退火余弦，所以修剪开始和结束都很温和。</p><pre class="kp kq kr ks gt ns nt nu nv aw nw bi"><span id="310e" class="ng lf iq nt b gy nx ny l nz oa">student.fit(10, 1e-5, callbacks=[SparsifyCallback(40, granularity='filter', method='local', criteria=large_final, sched_func=annealing_cos)])</span><span id="d0b1" class="ng lf iq nt b gy ob ny l nz oa">Pruning of filter until a sparsity of 40%</span><span id="b505" class="ng lf iq nt b gy ob ny l nz oa">epoch | train_loss | valid_loss | accuracy | time<br/>0     | 0.584072   | 0.532074   | 0.8384710| 1:31<br/>1     | 0.583805   | 0.499353   | 0.8445860| 1:31 <br/>2     | 0.599410   | 0.527805   | 0.8364330| 1:32<br/>3     | 0.610081   | 0.544566   | 0.8280250| 1:31<br/>4     | 0.625637   | 0.543279   | 0.8298090| 1:31<br/>5     | 0.628777   | 0.563051   | 0.8196180| 1:31<br/>6     | 0.688617   | 0.617627   | 0.8000000| 1:32<br/>7     | 0.691044   | 0.629927   | 0.8010190| 1:31<br/>8     | 0.669935   | 0.576220   | 0.8140130| 1:31<br/>9     | 0.682428   | 0.562718   | 0.8239490| 1:31</span><span id="bdd6" class="ng lf iq nt b gy ob ny l nz oa">Sparsity at epoch 0: 0.98%<br/>Sparsity at epoch 1: 3.83%<br/>Sparsity at epoch 2: 8.25%<br/>Sparsity at epoch 3: 13.83%<br/>Sparsity at epoch 4: 20.01%<br/>Sparsity at epoch 5: 26.19%<br/>Sparsity at epoch 6: 31.76%<br/>Sparsity at epoch 7: 36.19%<br/>Sparsity at epoch 8: 39.02%<br/>Sparsity at epoch 9: 40.00%<br/>Final Sparsity: 40.00</span></pre><p id="99f7" class="pw-post-body-paragraph lw lx iq ly b lz mu ka mb mc mv kd me mf mw mh mi mj mx ml mm mn my mp mq mr ij bi translated">我们的网络现在有40%的滤波器完全由零组成，代价是2%的精度。显然，选择较高的稀疏度会使网络更难保持类似的精度。其他参数也可以广泛地改变我们的稀疏化过程的行为。例如，选择更细粒度的稀疏性通常会导致更好的结果，但在速度方面更难利用。</p><p id="4ab2" class="pw-post-body-paragraph lw lx iq ly b lz mu ka mb mc mv kd me mf mw mh mi mj mx ml mm mn my mp mq mr ij bi translated">我们可以仔细检查我们的模型确实被修剪了40%的参数。</p><pre class="kp kq kr ks gt ns nt nu nv aw nw bi"><span id="cee9" class="ng lf iq nt b gy nx ny l nz oa">Sparsity in Conv2d 2: 39.06%<br/>Sparsity in Conv2d 5: 39.06%<br/>Sparsity in Conv2d 9: 39.84%<br/>Sparsity in Conv2d 12: 39.84%<br/>Sparsity in Conv2d 16: 39.84%<br/>Sparsity in Conv2d 19: 39.84%<br/>Sparsity in Conv2d 22: 39.84%<br/>Sparsity in Conv2d 26: 39.84%<br/>Sparsity in Conv2d 29: 39.84%<br/>Sparsity in Conv2d 32: 39.84%<br/>Sparsity in Conv2d 36: 39.84%<br/>Sparsity in Conv2d 39: 39.84%<br/>Sparsity in Conv2d 42: 39.84%</span></pre><p id="1bf7" class="pw-post-body-paragraph lw lx iq ly b lz mu ka mb mc mv kd me mf mw mh mi mj mx ml mm mn my mp mq mr ij bi translated">我们没有确切的40%,因为当我们去掉完整的过滤器时，我们不一定有一个整数。</p><p id="b574" class="pw-post-body-paragraph lw lx iq ly b lz mu ka mb mc mv kd me mf mw mh mi mj mx ml mm mn my mp mq mr ij bi translated">现在让我们看看我们在速度方面获得了多少。因为我们删除了40%的卷积滤波器，我们应该期待疯狂的加速，对不对？</p><pre class="kp kq kr ks gt ns nt nu nv aw nw bi"><span id="3a1a" class="ng lf iq nt b gy nx ny l nz oa">4.02 ms ± 5.77 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)</span></pre><p id="3878" class="pw-post-body-paragraph lw lx iq ly b lz mu ka mb mc mv kd me mf mw mh mi mj mx ml mm mn my mp mq mr ij bi translated">实际上，没有。我们没有删除任何参数，我们只是用零代替了一些，记得吗？参数的数量仍然相同:</p><pre class="kp kq kr ks gt ns nt nu nv aw nw bi"><span id="d193" class="ng lf iq nt b gy nx ny l nz oa">Total parameters : 134,309,962</span></pre><p id="094e" class="pw-post-body-paragraph lw lx iq ly b lz mu ka mb mc mv kd me mf mw mh mi mj mx ml mm mn my mp mq mr ij bi translated">这将引导我们进入下一部分。</p></div><div class="ab cl mz na hu nb" role="separator"><span class="nc bw bk nd ne nf"/><span class="nc bw bk nd ne nf"/><span class="nc bw bk nd ne"/></div><div class="ij ik il im in"><h1 id="f69a" class="le lf iq bd lg lh oc lj lk ll od ln lo kf oe kg lq ki of kj ls kl og km lu lv bi translated">修剪</h1><blockquote class="oi oj ok"><p id="0701" class="lw lx ms ly b lz mu ka mb mc mv kd me ol mw mh mi om mx ml mm on my mp mq mr ij bi translated"><strong class="ly ja">重要提示</strong>:这目前仅支持全前馈模型，如VGG模型，因为更复杂的架构需要越来越困难且通常依赖于模型的实现。</p></blockquote><p id="e24c" class="pw-post-body-paragraph lw lx iq ly b lz mu ka mb mc mv kd me mf mw mh mi mj mx ml mm mn my mp mq mr ij bi translated">为什么我们没有看到任何加速度，即使我们去掉了一半的参数？这是因为我们的GPU本身不知道我们的矩阵是稀疏的，因此无法加速计算。最简单的办法，就是用<strong class="ly ja">物理地</strong>移除我们归零的参数。但是这种操作需要改变网络的架构。当移除层<em class="ms"> i+1，</em>中的过滤器时，它也影响层<em class="ms"> i+2，</em>中的内核，因为它们现在变得无用并且可以被移除。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi pf"><img src="../Images/b55ff6f2b8a9975bed5db1e5ea8081aa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EbXm3b223aiQ723FqhI9sw.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">作者图片</p></figure><p id="9d10" class="pw-post-body-paragraph lw lx iq ly b lz mu ka mb mc mv kd me mf mw mh mi mj mx ml mm mn my mp mq mr ij bi translated">这种修剪只有在我们事先将整个滤波器归零的情况下才有效，因为这是唯一可以相应地改变架构的情况。希望稀疏计算很快能在普通深度学习图书馆中可用，所以这一部分在未来将变得无用，但目前，这是我们能想到的最好的解决方案🤷</p><p id="5c07" class="pw-post-body-paragraph lw lx iq ly b lz mu ka mb mc mv kd me mf mw mh mi mj mx ml mm mn my mp mq mr ij bi translated">这是fasterai的样子:</p><pre class="kp kq kr ks gt ns nt nu nv aw nw bi"><span id="a072" class="ng lf iq nt b gy nx ny l nz oa">pruner = Pruner()<br/>pruned_model = pruner.prune_model(learn.model)</span></pre><blockquote class="oi oj ok"><p id="b517" class="lw lx ms ly b lz mu ka mb mc mv kd me ol mw mh mi om mx ml mm on my mp mq mr ij bi translated"><em class="iq">您只需传递其过滤器已被稀疏化的模型，FasterAI将负责移除它们。</em></p></blockquote><p id="444e" class="pw-post-body-paragraph lw lx iq ly b lz mu ka mb mc mv kd me mf mw mh mi mj mx ml mm mn my mp mq mr ij bi translated"><strong class="ly ja"> <em class="ms">注意</em> </strong> <em class="ms">:该操作应该是无损的，因为它只移除已经不再参与网络的过滤器。</em></p><p id="1d8b" class="pw-post-body-paragraph lw lx iq ly b lz mu ka mb mc mv kd me mf mw mh mi mj mx ml mm mn my mp mq mr ij bi translated">在我们的例子中，它给出了:</p><pre class="kp kq kr ks gt ns nt nu nv aw nw bi"><span id="18fb" class="ng lf iq nt b gy nx ny l nz oa">pruner = Pruner()<br/>pruned_model = pruner.prune_model(student.model)</span></pre><p id="5717" class="pw-post-body-paragraph lw lx iq ly b lz mu ka mb mc mv kd me mf mw mh mi mj mx ml mm mn my mp mq mr ij bi translated">现在，让我们看看我们的模型能够做些什么:</p><pre class="kp kq kr ks gt ns nt nu nv aw nw bi"><span id="1bd7" class="ng lf iq nt b gy nx ny l nz oa">Total parameters : 83,975,344</span></pre><p id="2374" class="pw-post-body-paragraph lw lx iq ly b lz mu ka mb mc mv kd me mf mw mh mi mj mx ml mm mn my mp mq mr ij bi translated">就速度而言:</p><pre class="kp kq kr ks gt ns nt nu nv aw nw bi"><span id="2fb6" class="ng lf iq nt b gy nx ny l nz oa">2.44 ms ± 3.51 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)</span></pre><p id="3efd" class="pw-post-body-paragraph lw lx iq ly b lz mu ka mb mc mv kd me mf mw mh mi mj mx ml mm mn my mp mq mr ij bi translated">耶！现在我们可以说话了！让我们再次检查我们的准确性没有改变，我们没有在某个地方出错:</p><pre class="kp kq kr ks gt ns nt nu nv aw nw bi"><span id="36e9" class="ng lf iq nt b gy nx ny l nz oa">Loss: 0.5641388, Accuracy: 0.8229</span></pre><p id="e074" class="pw-post-body-paragraph lw lx iq ly b lz mu ka mb mc mv kd me mf mw mh mi mj mx ml mm mn my mp mq mr ij bi translated">实际上我们还能做更多的事情！让我们继续前进！</p></div><div class="ab cl mz na hu nb" role="separator"><span class="nc bw bk nd ne nf"/><span class="nc bw bk nd ne nf"/><span class="nc bw bk nd ne"/></div><div class="ij ik il im in"><h1 id="fcd6" class="le lf iq bd lg lh oc lj lk ll od ln lo kf oe kg lq ki of kj ls kl og km lu lv bi translated">批量标准化折叠</h1><p id="f33e" class="pw-post-body-paragraph lw lx iq ly b lz ma ka mb mc md kd me mf mg mh mi mj mk ml mm mn mo mp mq mr ij bi translated">批量规范化折叠是一个非常容易实现和简单的想法。要点是批量标准化只不过是对每一层的输入数据进行标准化。此外，在推断时，用于这种标准化的批次统计是固定的。因此，我们可以通过改变卷积的权重将归一化过程直接合并到卷积中，并完全移除批量归一化层，这在参数和计算方面都是一个增益。更深入的解释，请看我的<a class="ae mt" href="https://medium.com/towards-data-science/speed-up-inference-with-batch-normalization-folding-8a45a83a89d8" rel="noopener">上一篇</a>帖子。</p><p id="a249" class="pw-post-body-paragraph lw lx iq ly b lz mu ka mb mc mv kd me mf mw mh mi mj mx ml mm mn my mp mq mr ij bi translated">这是如何与FasterAI一起使用:</p><pre class="kp kq kr ks gt ns nt nu nv aw nw bi"><span id="3a4a" class="ng lf iq nt b gy nx ny l nz oa">bn_folder = BN_Folder() <br/>bn_folder.fold(learn.model))</span></pre><p id="fd52" class="pw-post-body-paragraph lw lx iq ly b lz mu ka mb mc mv kd me mf mw mh mi mj mx ml mm mn my mp mq mr ij bi translated"><strong class="ly ja"> <em class="ms">注意</em> </strong> <em class="ms">:该操作也应该是无损的，因为它重新定义了卷积以考虑批量范数，因此是等价的。</em></p><p id="d7ea" class="pw-post-body-paragraph lw lx iq ly b lz mu ka mb mc mv kd me mf mw mh mi mj mx ml mm mn my mp mq mr ij bi translated">让我们用我们的模型来做吧！</p><pre class="kp kq kr ks gt ns nt nu nv aw nw bi"><span id="c015" class="ng lf iq nt b gy nx ny l nz oa">folded_model = bn_folder.fold(pruned_learner.model)</span></pre><p id="d535" class="pw-post-body-paragraph lw lx iq ly b lz mu ka mb mc mv kd me mf mw mh mi mj mx ml mm mn my mp mq mr ij bi translated">参数下降通常不太明显，尤其是在VGG这样的网络中，几乎所有参数都包含在FC层中，但是，任何增益都是值得的。</p><pre class="kp kq kr ks gt ns nt nu nv aw nw bi"><span id="d78a" class="ng lf iq nt b gy nx ny l nz oa">Total parameters : 83,970,260</span></pre><p id="0a8f" class="pw-post-body-paragraph lw lx iq ly b lz mu ka mb mc mv kd me mf mw mh mi mj mx ml mm mn my mp mq mr ij bi translated">既然我们删除了批处理规范化层，我们应该再次看到加速。</p><pre class="kp kq kr ks gt ns nt nu nv aw nw bi"><span id="8ac4" class="ng lf iq nt b gy nx ny l nz oa">2.27 ms ± 1.22 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)</span></pre><p id="06bd" class="pw-post-body-paragraph lw lx iq ly b lz mu ka mb mc mv kd me mf mw mh mi mj mx ml mm mn my mp mq mr ij bi translated">再一次，让我们仔细检查我们没有在某个地方搞砸:</p><pre class="kp kq kr ks gt ns nt nu nv aw nw bi"><span id="5354" class="ng lf iq nt b gy nx ny l nz oa">Loss: 0.5641388, Accuracy: 0.8229</span></pre><p id="2644" class="pw-post-body-paragraph lw lx iq ly b lz mu ka mb mc mv kd me mf mw mh mi mj mx ml mm mn my mp mq mr ij bi translated">我们还没完成呢！正如我们所知，对于VGG16，大部分参数都包含在全连接层中，因此我们应该对此有所作为，对吗？</p></div><div class="ab cl mz na hu nb" role="separator"><span class="nc bw bk nd ne nf"/><span class="nc bw bk nd ne nf"/><span class="nc bw bk nd ne"/></div><div class="ij ik il im in"><h1 id="6113" class="le lf iq bd lg lh oc lj lk ll od ln lo kf oe kg lq ki of kj ls kl og km lu lv bi translated">FC层因子分解</h1><p id="93fb" class="pw-post-body-paragraph lw lx iq ly b lz ma ka mb mc md kd me mf mg mh mi mj mk ml mm mn mo mp mq mr ij bi translated">事实上，我们可以分解我们的大的全连接层，并用两个更小的近似层来代替它们。其思路是对权重矩阵进行SVD分解，将原矩阵表示为3个矩阵的乘积:<em class="ms">uσVT</em>。其中<em class="ms">σ</em>是沿其对角线具有非负值的对角矩阵(奇异值)。然后，我们定义奇异值的值<em class="ms"> k </em>来保持和修改矩阵<em class="ms"> U </em>和<em class="ms"> VT </em>。结果将是初始矩阵的近似值。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi pg"><img src="../Images/2c46f62033decbd2447d4e1f9e6e9019.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Lro9sADIXJaEBGPi94GXEA.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">作者图片</p></figure><p id="c9b8" class="pw-post-body-paragraph lw lx iq ly b lz mu ka mb mc mv kd me mf mw mh mi mj mx ml mm mn my mp mq mr ij bi translated">在FasterAI中，要分解模型的全连接层，您需要执行以下操作:</p><pre class="kp kq kr ks gt ns nt nu nv aw nw bi"><span id="a32a" class="ng lf iq nt b gy nx ny l nz oa">FCD = FCDecomposer() <br/>decomposed_model = FCD.decompose(model, percent_removed)</span></pre><blockquote class="oi oj ok"><p id="f877" class="lw lx ms ly b lz mu ka mb mc mv kd me ol mw mh mi om mx ml mm on my mp mq mr ij bi translated"><em class="iq"/><code class="fe pc pd pe nt b"><em class="iq">percent_removed</em></code><em class="iq">对应去除奇异值的百分比(</em> k <em class="iq">值)。</em></p></blockquote><p id="4a47" class="pw-post-body-paragraph lw lx iq ly b lz mu ka mb mc mv kd me mf mw mh mi mj mx ml mm mn my mp mq mr ij bi translated"><strong class="ly ja"> <em class="ms">注</em> </strong> <em class="ms">:这次分解不确切，所以我们预计之后业绩会下降，需要进一步的再培训。</em></p><p id="8191" class="pw-post-body-paragraph lw lx iq ly b lz mu ka mb mc mv kd me mf mw mh mi mj mx ml mm mn my mp mq mr ij bi translated">在我们的示例中，如果我们只想保留其中的一半:</p><pre class="kp kq kr ks gt ns nt nu nv aw nw bi"><span id="6763" class="ng lf iq nt b gy nx ny l nz oa">fc_dec = FCDecomposer()<br/>dec_model = fc_dec.decompose(folded_model, percent_removed=0.5)</span></pre><p id="35a6" class="pw-post-body-paragraph lw lx iq ly b lz mu ka mb mc mv kd me mf mw mh mi mj mx ml mm mn my mp mq mr ij bi translated">我们现在有多少参数？</p><pre class="kp kq kr ks gt ns nt nu nv aw nw bi"><span id="25f4" class="ng lf iq nt b gy nx ny l nz oa">Total parameters : 61,430,022</span></pre><p id="52e9" class="pw-post-body-paragraph lw lx iq ly b lz mu ka mb mc mv kd me mf mw mh mi mj mx ml mm mn my mp mq mr ij bi translated">我们赢得了多少时间？</p><pre class="kp kq kr ks gt ns nt nu nv aw nw bi"><span id="ab2a" class="ng lf iq nt b gy nx ny l nz oa">2.11 ms ± 462 ns per loop (mean ± std. dev. of 7 runs, 100 loops each)</span></pre><p id="34d3" class="pw-post-body-paragraph lw lx iq ly b lz mu ka mb mc mv kd me mf mw mh mi mj mx ml mm mn my mp mq mr ij bi translated">然而，这种技术是近似的，所以它不是无损的，所以我们应该重新训练我们的网络来恢复它的性能。</p><pre class="kp kq kr ks gt ns nt nu nv aw nw bi"><span id="7b52" class="ng lf iq nt b gy nx ny l nz oa">final_learner = Learner(data, dec_model, metrics=[accuracy])<br/>final_learner.fit_one_cycle(5, 1e-5)</span><span id="2d2b" class="ng lf iq nt b gy ob ny l nz oa">epoch | train_loss | valid_loss | accuracy | time<br/>0     | 0.795416   | 0.759886   | 0.7729940| 0:51<br/>1     | 0.752566   | 0.701141   | 0.7943950| 0:51 <br/>2     | 0.700373   | 0.650178   | 0.8048410| 0:52<br/>3     | 0.604264   | 0.606801   | 0.8216560| 0:51<br/>4     | 0.545705   | 0.592318   | 0.8231850| 0:51</span></pre><p id="89fb" class="pw-post-body-paragraph lw lx iq ly b lz mu ka mb mc mv kd me mf mw mh mi mj mx ml mm mn my mp mq mr ij bi translated">这种操作通常对较新的架构用处不大，因为它们的全连接层中通常没有那么多参数。</p></div><div class="ab cl mz na hu nb" role="separator"><span class="nc bw bk nd ne nf"/><span class="nc bw bk nd ne nf"/><span class="nc bw bk nd ne"/></div><div class="ij ik il im in"><p id="3b58" class="pw-post-body-paragraph lw lx iq ly b lz mu ka mb mc mv kd me mf mw mh mi mj mx ml mm mn my mp mq mr ij bi translated">概括地说，我们在本文中看到了如何使用FasterAI来:</p><ol class=""><li id="fbcd" class="oo op iq ly b lz mu mc mv mf oq mj or mn os mr ph ou ov ow bi translated">让学生模型向教师模型学习(<strong class="ly ja">知识蒸馏</strong>)</li><li id="01ab" class="oo op iq ly b lz ox mc oy mf oz mj pa mn pb mr ph ou ov ow bi translated">使我们的网络稀疏化(<strong class="ly ja">稀疏化</strong>)</li><li id="dc05" class="oo op iq ly b lz ox mc oy mf oz mj pa mn pb mr ph ou ov ow bi translated">可选地，物理移除零滤波器(<strong class="ly ja">修剪</strong>)</li><li id="b95b" class="oo op iq ly b lz ox mc oy mf oz mj pa mn pb mr ph ou ov ow bi translated">移除批量定额层(<strong class="ly ja">批量规格化折叠</strong>)</li><li id="9023" class="oo op iq ly b lz ox mc oy mf oz mj pa mn pb mr ph ou ov ow bi translated">用较小的层来近似我们的大的全连接层(<strong class="ly ja">全连接层因式分解</strong>)</li></ol><p id="4360" class="pw-post-body-paragraph lw lx iq ly b lz mu ka mb mc mv kd me mf mw mh mi mj mx ml mm mn my mp mq mr ij bi translated">我们看到，通过应用这些，我们可以将VGG16模型的参数从<strong class="ly ja"> 134 </strong>百万减少到<strong class="ly ja"> 61 </strong>百万，并且还可以将推断从<strong class="ly ja"> 4.03 </strong>毫秒加速到<strong class="ly ja"> 2.11 </strong>毫秒，与基线相比，准确性没有任何下降(甚至实际上略有增加)。</p><p id="e7ac" class="pw-post-body-paragraph lw lx iq ly b lz mu ka mb mc mv kd me mf mw mh mi mj mx ml mm mn my mp mq mr ij bi translated">当然，这些技术可以与<a class="ae mt" href="https://pytorch.org/docs/stable/quantization.html" rel="noopener ugc nofollow" target="_blank">量化</a>或<a class="ae mt" href="https://pytorch.org/docs/stable/notes/amp_examples.html" rel="noopener ugc nofollow" target="_blank">混合精度训练</a>结合使用，Pytorch中已经提供了这些技术，可以进行更多的压缩和加速。</p><p id="0ca7" class="pw-post-body-paragraph lw lx iq ly b lz mu ka mb mc mv kd me mf mw mh mi mj mx ml mm mn my mp mq mr ij bi translated"><strong class="ly ja"> <em class="ms">注意</em> </strong> <em class="ms">:请记住，上面介绍的技术不是神奇的🧙‍♂️，所以不要期望每次都能看到200%的加速和压缩。您能实现什么在很大程度上取决于您正在使用的架构(有些已经通过设计实现了速度/参数效率)或它正在执行的任务(有些数据集非常简单，您可以删除几乎所有网络而不会看到性能下降)</em></p><p id="9d46" class="pw-post-body-paragraph lw lx iq ly b lz mu ka mb mc mv kd me mf mw mh mi mj mx ml mm mn my mp mq mr ij bi translated">仅此而已！感谢您的阅读，我希望您会喜欢FasterAI。我不认为它是完美的，你可能会发现很多错误。如果你有，请告诉我，我可以试着解决它们😌</p></div></div>    
</body>
</html>