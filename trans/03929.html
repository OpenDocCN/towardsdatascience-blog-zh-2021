<html>
<head>
<title>MMDetection Tutorial — An End2End State-of-the-art Object Detection Library</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">MMDetection教程——最先进的对象检测库</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/mmdetection-tutorial-an-end2end-state-of-the-art-object-detection-library-59064deeada3?source=collection_archive---------4-----------------------#2021-04-02">https://towardsdatascience.com/mmdetection-tutorial-an-end2end-state-of-the-art-object-detection-library-59064deeada3?source=collection_archive---------4-----------------------#2021-04-02</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="8a72" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">MMDetection为使用许多SOTA对象检测模型提供了巨大的潜力，例如FasterRCNN、DETR、VFNet等等！</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/e3d9257e83c8aba16fbfac43668b0855.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*hzbRT46dnEU0s5ly"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">照片由<a class="ae ky" href="https://unsplash.com/@alfonsmc10?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">阿尔方斯·莫拉莱斯</a>在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>拍摄</p></figure><p id="d760" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在进行一次物体探测比赛时，我通过其他一些参赛者偶然发现了<a class="ae ky" href="https://github.com/open-mmlab/mmdetection" rel="noopener ugc nofollow" target="_blank"> MMDetection </a>。而且，我对网上缺乏教程感到很有趣。MMDetection为大约30个对象检测模型的训练和推理提供了一个包装器！它提供了一个编码库和一个命令行界面，使得对象检测原型非常快速。实际上，我很惊讶地看到VFNet(最近的SOTA物体探测网络之一)已经在那里实现了。如果您不熟悉VarifocalNet，它提供了非常有竞争力的性能，尤其是与YoloV5相比(我见过它几次胜过YoloV5)。</p><p id="e673" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">所有的MMdetection模型都基于PyTorch，但是老实说，它使用的代码要少得多(我将在这里展示)。我发现使用这个库的最大好处是，一旦你完成了初始设置，你可以通过修改1-5行代码来非常容易地改变你正在使用的模型！所以它对快速实验非常有用。</p><p id="eff3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">以下是MMdetection支持的所有型号的列表:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi lv"><img src="../Images/38e5a2f9f12195c3e1ee49eabb73b0df.png" data-original-src="https://miro.medium.com/v2/resize:fit:876/format:webp/1*5Z8-b-xFiwPyzvkShCk80w.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">来源:<a class="ae ky" href="https://github.com/open-mmlab/mmdetection" rel="noopener ugc nofollow" target="_blank"> Github </a></p></figure><h2 id="0d9b" class="lw lx it bd ly lz ma dn mb mc md dp me li mf mg mh lm mi mj mk lq ml mm mn mo bi translated">让我们开始—安装所需的软件包</h2><p id="c85a" class="pw-post-body-paragraph kz la it lb b lc mp ju le lf mq jx lh li mr lk ll lm ms lo lp lq mt ls lt lu im bi translated">我们要做的第一件事是安装“mmcv-full ”,这是一个mm库，提供了我们需要的大部分东西。然后克隆mmdetection Github存储库并安装需求。请注意，这需要大约12分钟，所以有点耐心。</p><pre class="kj kk kl km gt mu mv mw mx aw my bi"><span id="82a1" class="lw lx it mv b gy mz na l nb nc">!pip install mmcv-full<br/>!git clone <a class="ae ky" href="https://github.com/open-mmlab/mmdetection.git" rel="noopener ugc nofollow" target="_blank">https://github.com/open-mmlab/mmdetection.git</a><br/>%cd mmdetection</span><span id="40e9" class="lw lx it mv b gy nd na l nb nc">!pip install -r requirements.txt</span></pre><p id="e487" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">接下来要做的是打开存储库文件并导航到“configs”目录。在configs目录中，您将找到每个受支持型号的文件。选择其中一个模型后，您将需要下载其预训练重量。为此，打开模型目录，然后打开相应的自述文件。</p><p id="2428" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在自述文件中，您将找到该模型各种实现的权重的所有链接。在本文中，我们将实现VFNet。它的自述，看起来是这样的:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ne"><img src="../Images/b869426ac32b8394bb5be256b0bc47b9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*N70Ej9Mhn-kYakb7cpkKTQ.png"/></div></div></figure><p id="88fe" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">你可以找到不同骨干的细节和他们的表现。让我们先用R-50，然后我们可以选择另一个。要下载预训练的重量，只需运行以下命令:</p><pre class="kj kk kl km gt mu mv mw mx aw my bi"><span id="8874" class="lw lx it mv b gy mz na l nb nc">!mkdir checkpoints<br/>!wget -c <a class="ae ky" href="https://openmmlab.oss-cn-hangzhou.aliyuncs.com/mmdetection/v2.0/vfnet/vfnet_r50_fpn_mdconv_c3-c5_mstrain_2x_coco/$CHECKPOINT\" rel="noopener ugc nofollow" target="_blank">https://openmmlab.oss-cn-hangzhou.aliyuncs.com/mmdetection/v2.0/vfnet/vfnet_r50_fpn_mdconv_c3-c5_mstrain_2x_coco/</a>vfnet_r50_fpn_mdconv_c3-c5_mstrain_2x_coco_20201027pth-6879c318.pth<a class="ae ky" href="https://openmmlab.oss-cn-hangzhou.aliyuncs.com/mmdetection/v2.0/vfnet/vfnet_r50_fpn_mdconv_c3-c5_mstrain_2x_coco/$CHECKPOINT\" rel="noopener ugc nofollow" target="_blank">\</a><br/>      -O checkpoints/vfnet_r50_fpn_mdconv_c3-c5_mstrain_2x_coco_20201027pth-6879c318.pth</span></pre><p id="ec5e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">wget将安装权重，O标志指定它们的安装位置。这些是可可预训练的重量。</p><h2 id="eb37" class="lw lx it bd ly lz ma dn mb mc md dp me li mf mg mh lm mi mj mk lq ml mm mn mo bi translated">进口</h2><pre class="kj kk kl km gt mu mv mw mx aw my bi"><span id="e045" class="lw lx it mv b gy mz na l nb nc">from mmcv import Config<br/>from mmdet.apis import set_random_seed<br/>from mmdet.datasets import build_dataset<br/>from mmdet.models import build_detector<br/>from mmdet.apis import train_detector, init_detector, inference_detector<br/>import torch </span></pre><h2 id="9587" class="lw lx it bd ly lz ma dn mb mc md dp me li mf mg mh lm mi mj mk lq ml mm mn mo bi translated">构建模型和数据集</h2><p id="15f0" class="pw-post-body-paragraph kz la it lb b lc mp ju le lf mq jx lh li mr lk ll lm ms lo lp lq mt ls lt lu im bi translated">接下来要做的是配置模型和数据集。这是大部分工作需要完成的地方，也是很多错误发生的地方。两种最常见的数据集格式是COCO和PASCAL VOC。您当然可以实现自己的格式，但这超出了本文的范围。</p><p id="79e4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">由于COCO是更常见的一种，我们将在这里使用它。如果你不熟悉这些格式，我建议你看看这篇伟大的文章<a class="ae ky" href="https://pub.towardsai.net/understanding-coco-and-pascal-voc-annotations-for-object-detection-bb8ffbbb36e3?source=search_post---------1" rel="noopener ugc nofollow" target="_blank">这里</a>。如果你想看看我们将要使用的数据集，你可以在<a class="ae ky" href="https://www.kaggle.com/sreevishnudamodaran/vinbigdata-coco-dataset-with-wbf-3x-downscaled" rel="noopener ugc nofollow" target="_blank">这里</a>找到它</p><p id="f849" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">首先，我们需要指向我们模型的配置文件(来自“configs”的那个)。然后我们需要指向数据集并提供一些其他属性。</p><pre class="kj kk kl km gt mu mv mw mx aw my bi"><span id="c89a" class="lw lx it mv b gy mz na l nb nc">cfg = Config.fromfile('./configs/vfnet/vfnet_r50_fpn_mstrain_2x_coco.py')<br/>cfg.dataset_type = 'CocoDataset'</span><span id="fb88" class="lw lx it mv b gy nd na l nb nc">cfg.classes = ("Aortic_enlargement", "Atelectasis", <br/>               "Calcification", "Cardiomegaly", <br/>               "Consolidation", "ILD", "Infiltration", <br/>               "Lung_Opacity", "Nodule/Mass", "Other_lesion", <br/>               "Pleural_effusion", "Pleural_thickening", <br/>               "Pneumothorax", "Pulmonary_fibrosis")<br/># number of classes</span><span id="f4ab" class="lw lx it mv b gy nd na l nb nc">cfg.model.bbox_head.num_classes = 14</span></pre><p id="b5d8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">接下来，我们需要指定每个训练、测试和验证数据集:</p><pre class="kj kk kl km gt mu mv mw mx aw my bi"><span id="310a" class="lw lx it mv b gy mz na l nb nc">PREFIX = '/kaggle/input/vinbigdata-coco-dataset-with-wbf-3x-downscaled/vinbigdata-coco-dataset-with-wbf-3x-downscaled/'</span><span id="bacb" class="lw lx it mv b gy nd na l nb nc">cfg.data.train.img_prefix = PREFIX<br/>cfg.data.train.classes = cfg.classes<br/>cfg.data.train.ann_file = PREFIX + 'train_annotations.json'<br/>cfg.data.train.type = 'CocoDataset'</span><span id="9440" class="lw lx it mv b gy nd na l nb nc">cfg.data.val.img_prefix = PREFIX<br/>cfg.data.val.classes = cfg.classes<br/>cfg.data.val.ann_file = PREFIX + 'val_annotations.json'<br/>cfg.data.val.type = 'CocoDataset'</span><span id="bd6b" class="lw lx it mv b gy nd na l nb nc">cfg.data.test.img_prefix = PREFIX<br/>cfg.data.test.classes = cfg.classes<br/>cfg.data.test.ann_file = PREFIX + 'val_annotations.json'<br/>cfg.data.test.type = 'CocoDataset'</span></pre><p id="8d03" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">由于测试文件实际上没有注释，我们现在只将它指向val注释。</p><p id="edb4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在，我们几乎完成了数据集，让我们做模型，然后建立两者。这一部分是关于指定您想要使用的超参数，例如:</p><pre class="kj kk kl km gt mu mv mw mx aw my bi"><span id="86e6" class="lw lx it mv b gy mz na l nb nc">cfg.optimizer.lr = 0.02 / 8<br/>cfg.lr_config.warmup = None<br/>cfg.log_config.interval = 600</span><span id="98d1" class="lw lx it mv b gy nd na l nb nc"># Change the evaluation metric since we use customized dataset.<br/>cfg.evaluation.metric = 'bbox'<br/># We can set the evaluation interval to reduce the evaluation times<br/>cfg.evaluation.interval = 3<br/># We can set the checkpoint saving interval to reduce the storage cost<br/>cfg.checkpoint_config.interval = 3</span><span id="17a3" class="lw lx it mv b gy nd na l nb nc"># Set seed thus the results are more reproducible<br/>cfg.seed = 0<br/>set_random_seed(0, deterministic=False)<br/>cfg.gpu_ids = range(1)</span><span id="2984" class="lw lx it mv b gy nd na l nb nc"><br/>cfg.load_from = 'vfnet_r50_fpn_mdconv_c3-c5_mstrain_2x_coco_20201027pth-6879c318.pth'<br/>cfg.work_dir = "../vinbig"</span><span id="6635" class="lw lx it mv b gy nd na l nb nc">cfg.runner.max_epochs = 12<br/>cfg.total_epochs = 12</span></pre><p id="95cc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在，我们可以构建数据集和模型:</p><pre class="kj kk kl km gt mu mv mw mx aw my bi"><span id="ea0a" class="lw lx it mv b gy mz na l nb nc">model = build_detector(cfg.model)<br/>datasets = [build_dataset(cfg.data.train)]</span></pre><p id="0b25" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然后我们可以开始训练:</p><pre class="kj kk kl km gt mu mv mw mx aw my bi"><span id="23f9" class="lw lx it mv b gy mz na l nb nc">train_detector(model, datasets[0], cfg, distributed=False, validate=True)</span></pre><h2 id="6eae" class="lw lx it bd ly lz ma dn mb mc md dp me li mf mg mh lm mi mj mk lq ml mm mn mo bi translated">推理</h2><p id="3314" class="pw-post-body-paragraph kz la it lb b lc mp ju le lf mq jx lh li mr lk ll lm ms lo lp lq mt ls lt lu im bi translated">在MMdetection中，您可以通过命令行进行推断，也可以通过inference_detector进行推断。我发现命令行更简单，所以让我们在这里使用它。</p><p id="54ac" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">但是首先，让我们创建之前忽略的测试注释文件。它本质上是一个JSON文件，带有空的注释和一个列出每个图像的图像属性的字典。</p><pre class="kj kk kl km gt mu mv mw mx aw my bi"><span id="5a01" class="lw lx it mv b gy mz na l nb nc">test_files = f"/kaggle/input/vinbigdata-512-image-dataset/vinbigdata/test"<br/>IMG_SIZE = 512<br/>ids = os.listdir(test_files)<br/>img_infos = []</span><span id="b3eb" class="lw lx it mv b gy nd na l nb nc">for i, _id in enumerate(ids):<br/>    if '.png' in _id:<br/>        img_infos.append({<br/>                    "license": 0,<br/>                    "url": 'null',<br/>                    "file_name": _id,<br/>                    "height": IMG_SIZE,<br/>                    "width": IMG_SIZE,<br/>                    "date_captured": 'null',<br/>                    "id": _id<br/>                })</span><span id="a790" class="lw lx it mv b gy nd na l nb nc">val_anno = '/kaggle/input/vinbigdata-coco-dataset-with-wbf-3x-downscaled/vinbigdata-coco-dataset-with-wbf-3x-downscaled/val_annotations.json'</span><span id="ee78" class="lw lx it mv b gy nd na l nb nc">with open(val_anno) as f:<br/>    val_anno_json = json.load(f)</span><span id="08f8" class="lw lx it mv b gy nd na l nb nc">val_anno_json.keys()<br/>val_anno_json['annotations']=[]<br/>val_anno_json['images'] = img_infos<br/>with open('./test_ann.json', 'w') as outfile:<br/>    json.dump(val_anno_json, outfile)</span></pre><p id="3625" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在，让我们运行推断命令。这个命令会有点长，因为我们需要指定配置。您可以通过将配置保存到一个文件并在命令行中使用该文件来缩短它，但我认为这样更容易快速更改。</p><pre class="kj kk kl km gt mu mv mw mx aw my bi"><span id="22b4" class="lw lx it mv b gy mz na l nb nc">classes_str='("Aortic_enlargement", "Atelectasis", "Calcification", "Cardiomegaly", "Consolidation", "ILD", "Infiltration", "Lung_Opacity", "Nodule/Mass", "Other_lesion", \<br/>"Pleural_effusion", "Pleural_thickening", "Pneumothorax", "Pulmonary_fibrosis")'</span><span id="f579" class="lw lx it mv b gy nd na l nb nc">MODEL = './configs/vfnet/vfnet_r50_fpn_mstrain_2x_coco.py'<br/>CHECKPOINT = 'checkpoints/vfnet_r50_fpn_mdconv_c3-c5_mstrain_2x_coco_20201027pth-6879c318.pth'</span><span id="f7f3" class="lw lx it mv b gy nd na l nb nc">!python mmdetection/tools/test.py\</span><span id="4f9f" class="lw lx it mv b gy nd na l nb nc">$MODEL\<br/> $CHECKPOINT\<br/> --cfg-options dataset_type='CocoDataset' classes=$classes_str\</span><span id="eeb7" class="lw lx it mv b gy nd na l nb nc">data.train.img_prefix=$cfg.data.train.img_prefix\<br/>data.train.classes=$classes_str\ data.train.ann_file=$cfg.data.train.ann_file\<br/>data.train.type='CocoDataset'\</span><span id="0b08" class="lw lx it mv b gy nd na l nb nc">data.val.img_prefix=$cfg.data.val.img_prefix\<br/>data.val.classes=$classes_str\ data.val.ann_file=$cfg.data.val.ann_file\ data.val.type='CocoDataset'\</span><span id="d7c4" class="lw lx it mv b gy nd na l nb nc">data.test.img_prefix=$cfg.data.test.img_prefix\ data.test.classes=$classes_str\<br/>data.test.ann_file='../test_ann.json'\<br/>data.test.type='CocoDataset'\<br/>model.bbox_head.num_classes='14' evaluation.metric='bbox' work_dir='../vinbig_output' load_from=$cfg.load_from total_epochs='1' --out results.pkl</span></pre><p id="512a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这与我们之前指定的训练配置基本相同，只是我们现在传递一个测试注释JSON，运行1个时期(推断)，并将输出结果保存到“results.pkl”。</p><p id="db81" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">就是这样！您可以从pickle文件加载预测，如下所示:</p><pre class="kj kk kl km gt mu mv mw mx aw my bi"><span id="8bfd" class="lw lx it mv b gy mz na l nb nc">import pickle</span><span id="1161" class="lw lx it mv b gy nd na l nb nc">with open('./results.pkl', 'rb') as f:<br/>    data = pickle.load(f)</span></pre><p id="da29" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">接下来的步骤就看你的了！</p><p id="92c7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">关于MMdetection最好的事情之一是，现在如果你想改变模型，你可以指向不同的配置文件，下载不同的检查点并运行相同的代码！如果你在其中一个步骤上卡住了，或者你想以不同的方式实现其中的一些，我建议查看他们的文档<a class="ae ky" href="https://github.com/open-mmlab/mmdetection" rel="noopener ugc nofollow" target="_blank">这里</a>。</p></div></div>    
</body>
</html>