<html>
<head>
<title>The Ultimate Guide to Clustering Algorithms and Topic Modeling</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">聚类算法和主题建模终极指南</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/wthe-ultimate-guide-to-clustering-algorithms-and-topic-modeling-4f7757c115?source=collection_archive---------14-----------------------#2021-07-23">https://towardsdatascience.com/wthe-ultimate-guide-to-clustering-algorithms-and-topic-modeling-4f7757c115?source=collection_archive---------14-----------------------#2021-07-23</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/75aa21494d2ae6e5722889f2c6560924.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OuXvHDCTt2YwlF5Ib5gydA.jpeg"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">戴维·巴洛在<a class="ae jg" href="https://unsplash.com/s/photos/unsupervised-machine-learning?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><div class=""/><div class=""><h2 id="7cfc" class="pw-subtitle-paragraph kg ji jj bd b kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx dk translated">第1部分:K均值初学者指南</h2></div><p id="7618" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">聚类是最常用的<strong class="la jk">无监督机器学习</strong>算法之一。您可以将聚类视为将未组织的数据点放入不同的类别，以便您可以了解更多关于数据结构的信息。聚类在从没有标签的数据中提取信息方面有多种应用。例如，公司根据客户的特征(如购买行为)对客户进行分类，以便更好地开展市场活动，制定定价策略以获取更多利润，等等。聚类算法也广泛用于自然语言处理(NLP)中，以从非结构化文本数据中提取信息，主题建模就是一个例子。</p><p id="814b" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">本系列文章旨在为读者提供两种常见但非常不同的聚类算法<strong class="la jk"> K均值</strong>和<strong class="la jk">潜在狄利克雷分配(LDA)</strong>及其在主题建模中的应用:</p><blockquote class="lu"><p id="e503" class="lv lw jj bd lx ly lz ma mb mc md lt dk translated">第1部分:K-Means初学者指南(本文)</p><p id="9a90" class="lv lw jj bd lx ly lz ma mb mc md lt dk translated"><a class="ae jg" rel="noopener" target="_blank" href="/the-ultimate-guide-to-clustering-algorithms-and-topic-modeling-3a65129df324">第2部分:LDA初学者指南</a></p><p id="8dba" class="lv lw jj bd lx ly lz ma mb mc md lt dk translated">第3部分:使用K-Means和LDA进行主题建模(即将推出)</p></blockquote><p id="45a9" class="pw-post-body-paragraph ky kz jj la b lb me kk ld le mf kn lg lh mg lj lk ll mh ln lo lp mi lr ls lt im bi translated">在本文中，我将介绍应用K-Means算法的细节。</p></div><div class="ab cl mj mk hx ml" role="separator"><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo"/></div><div class="im in io ip iq"><h2 id="2020" class="mq mr jj bd ms mt mu dn mv mw mx dp my lh mz na nb ll nc nd ne lp nf ng nh ni bi translated">什么是K-Means</h2><p id="73fd" class="pw-post-body-paragraph ky kz jj la b lb nj kk ld le nk kn lg lh nl lj lk ll nm ln lo lp nn lr ls lt im bi translated">K-Means是最简单的聚类算法之一，用于检测无组织数据点中的常见模式。该算法通过基于相似性识别接近的数据点，将所有数据点分类为K个聚类。K-Means简单地将相似性定义为在特征空间中测量的欧几里德距离。为了便于说明，使用下面的代码，我生成了三个二维的正态分布的点簇:</p><pre class="no np nq nr gt ns nt nu nv aw nw bi"><span id="ef66" class="mq mr jj nt b gy nx ny l nz oa">import numpy as np<br/>import matplotlib.pyplot as plt</span><span id="129b" class="mq mr jj nt b gy ob ny l nz oa">#generate random data samples from known clusters<br/>x = np.random.normal(0, 0.5, (100, 2)) + np.array([0, 1.2])<br/>y = np.random.normal(0, 0.5, (100, 2)) + np.array([-0.7, -0.7])<br/>z = np.random.normal(0, 0.5, (100, 2)) + np.array([0.7, -0.7])</span><span id="8bc5" class="mq mr jj nt b gy ob ny l nz oa">plt.scatter(*x.T)<br/>plt.scatter(*y.T)<br/>plt.scatter(*z.T)<br/>plt.title(‘Data Example with Only Two Features’)<br/>plt.xlabel(‘Feature 1’)<br/>plt.ylabel(‘Feature 2’);</span></pre><p id="4bdc" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">下图显示了<em class="oc">手动生成的</em>三组数据。由于样本数据只有两个特征，我们可以清楚地看到二维图形中哪些点彼此接近。在实际应用中，当特征空间增加到二维以上时，同样的直觉成立。</p><figure class="no np nq nr gt iv gh gi paragraph-image"><div class="gh gi od"><img src="../Images/88c21cba7f85131b7d18846a18405a6b.png" data-original-src="https://miro.medium.com/v2/resize:fit:806/format:webp/1*p1WBf3cGjAD0wB9y9INd3Q.png"/></div><p class="jc jd gj gh gi je jf bd b be z dk translated">作者图片</p></figure><p id="a57d" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">为了应用K-Means，研究人员首先需要确定聚类的数量。然后，该算法将把每个样本分配到离聚类中心的距离最小的聚类中。代码很简单:</p><pre class="no np nq nr gt ns nt nu nv aw nw bi"><span id="c587" class="mq mr jj nt b gy nx ny l nz oa">from sklearn.cluster import KMeans</span><span id="d160" class="mq mr jj nt b gy ob ny l nz oa">data = np.vstack((x,y,z))<br/>km = KMeans(n_clusters=3)<br/>km.fit(data)</span></pre><p id="c3be" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在这里，由于我们生成了数据，我们知道有三个集群。当你对数据知之甚少的时候，在实际应用中呢？本文将在介绍K-Means算法的数学细节后讨论这个问题的答案。</p></div><div class="ab cl mj mk hx ml" role="separator"><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo"/></div><div class="im in io ip iq"><h2 id="5166" class="mq mr jj bd ms mt mu dn mv mw mx dp my lh mz na nb ll nc nd ne lp nf ng nh ni bi translated"><strong class="ak">K-Means背后的数学</strong></h2><p id="e69d" class="pw-post-body-paragraph ky kz jj la b lb nj kk ld le nk kn lg lh nl lj lk ll nm ln lo lp nn lr ls lt im bi translated">K-Means算法预先确定聚类数K，然后分配一个聚类集合C = {C1，C2，…Ck}，使:</p><figure class="no np nq nr gt iv gh gi paragraph-image"><div class="gh gi oe"><img src="../Images/6325e345db6ef9bb6c89282d754975bc.png" data-original-src="https://miro.medium.com/v2/resize:fit:456/format:webp/1*Ea4id7f-hXX4NhAiS6OS2A.png"/></div></figure><p id="ab60" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">其中𝜇𝑘是群集Ck的点的中心:</p><figure class="no np nq nr gt iv gh gi paragraph-image"><div class="gh gi of"><img src="../Images/e30cf5a9dcc9a6623f012e318f90cea1.png" data-original-src="https://miro.medium.com/v2/resize:fit:304/format:webp/1*ein9Q16eWjEtZO2e0qn7Bw.png"/></div></figure><p id="0fe5" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">该算法的工作原理如下:</p><ol class=""><li id="bef2" class="og oh jj la b lb lc le lf lh oi ll oj lp ok lt ol om on oo bi translated">指定K个簇的数量。</li></ol><p id="2613" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">2.用随机值初始化每个聚类的中心点𝜇𝑘 (k ∈ K)。</p><p id="012e" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">3.计算每个数据点到每个聚类中心点的平方欧几里德距离。</p><figure class="no np nq nr gt iv gh gi paragraph-image"><div class="gh gi op"><img src="../Images/2ccd24670a78ad210b468419633b65bf.png" data-original-src="https://miro.medium.com/v2/resize:fit:168/format:webp/1*BPssma5KRLHduSWxdRFoBA.png"/></div><p class="jc jd gj gh gi je jf bd b be z dk translated">数据点和聚类中心之间的欧几里德距离</p></figure><p id="a0ec" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">4.将X_j分配给欧氏距离平方最小的最近聚类k:</p><figure class="no np nq nr gt iv gh gi paragraph-image"><div class="gh gi oq"><img src="../Images/99d7dc58816bdb65cabc5e6e3610a62a.png" data-original-src="https://miro.medium.com/v2/resize:fit:248/0*EI4WM6sesc1P5aoa"/></div></figure><p id="2fe7" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">5.通过取分配给聚类k的样本点的平均值来更新𝜇𝑘</p><p id="d2c8" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">6.重复步骤3至5，直到收敛。</p><p id="1b53" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">请注意，迭代步骤降低了目标函数，并且只有有限数量的点的可能分区，因此该算法保证收敛。然而，收敛的解决方案可能不是全局最优的。默认情况下，<code class="fe or os ot nt b">K-Means</code>使用不同的质心种子运行聚类算法十次，并取得度量中的最佳结果。</p></div><div class="ab cl mj mk hx ml" role="separator"><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo"/></div><div class="im in io ip iq"><h2 id="a5f0" class="mq mr jj bd ms mt mu dn mv mw mx dp my lh mz na nb ll nc nd ne lp nf ng nh ni bi translated"><strong class="ak">K均值指标</strong></h2><p id="aa65" class="pw-post-body-paragraph ky kz jj la b lb nj kk ld le nk kn lg lh nl lj lk ll nm ln lo lp nn lr ls lt im bi translated">如何确定像K-means这样的聚类模型的拟合优度？这个问题的答案对于在迭代步骤中找到最适合的模型是很重要的，并且在帮助研究人员决定聚类的数量方面起着重要的作用。聚类算法最常用的度量是<strong class="la jk">惯性</strong>和<strong class="la jk">轮廓</strong>。</p><p id="ba50" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la jk">惯性</strong></p><p id="54d5" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">惯性测量从每个数据点到其最终聚类中心的距离。对于每个集群，惯性由每个数据点X_j ∈ Ck和中心𝜇𝑘:之间的均方距离给出</p><figure class="no np nq nr gt iv gh gi paragraph-image"><div class="gh gi ou"><img src="../Images/c7282cf0a8e793afe32b1e1dcf0dab52.png" data-original-src="https://miro.medium.com/v2/resize:fit:404/format:webp/1*0yEQ7AGDHYdMDbLJUy8OkQ.png"/></div></figure><p id="42a1" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">对所有聚类的惯性求和后，总惯性用于比较不同K均值模型的性能:</p><figure class="no np nq nr gt iv gh gi paragraph-image"><div class="gh gi ov"><img src="../Images/8dc024a26c56f1f3d6b39fd2572ba1fd.png" data-original-src="https://miro.medium.com/v2/resize:fit:172/format:webp/1*cXpeF9vfw59FuZD9ltYWWg.png"/></div></figure><p id="0266" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">由惯性的定义可知，我们需要选择<strong class="la jk"> <em class="oc">最小化</em> </strong>总惯性的模型。</p><p id="ce93" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la jk">轮廓系数</strong></p><p id="5434" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">轮廓系数是可以在聚类算法中使用的另一个度量。对于每个数据点X_j，它计算X_j和相同聚类中所有其他点之间的平均距离。我们将其定义为a_j。然后，它会寻找X_j的下一个最近的聚类(对X_j进行分类的第二好的聚类)，并计算X_j与该聚类中所有点之间的平均距离作为b_j。点X_j的轮廓系数为:</p><figure class="no np nq nr gt iv gh gi paragraph-image"><div class="gh gi ow"><img src="../Images/b6aca111d04a13f8cc93743562482726.png" data-original-src="https://miro.medium.com/v2/resize:fit:250/format:webp/1*_J0M3aoBNGoixBkAwCT9Qg.png"/></div></figure><p id="9834" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">从上面的等式中，我们可以看到轮廓系数在-1和1之间，取决于a_j和b_j中哪一个更大:</p><figure class="no np nq nr gt iv gh gi paragraph-image"><div class="gh gi ox"><img src="../Images/d5af2a36da7aedd216eedfebe62bae7b.png" data-original-src="https://miro.medium.com/v2/resize:fit:352/0*gO3Hp512EoPN411i"/></div></figure><p id="1834" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">如果b_j大于a_j，说明模型已经将X_j聚类到最佳聚类中。与a_j相比，b_j越大，集群越好。否则，当a_j较大时，意味着该点可能在另一个簇中。a_j与b_j相比越大，值越接近-1。</p><p id="c091" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">轮廓系数给出了当计算聚类中所有数据点的平均值时每个聚类紧密程度的信息。当计算所有数据点的平均轮廓系数时，它测量模型性能。</p></div><div class="ab cl mj mk hx ml" role="separator"><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo"/></div><div class="im in io ip iq"><h2 id="e34e" class="mq mr jj bd ms mt mu dn mv mw mx dp my lh mz na nb ll nc nd ne lp nf ng nh ni bi translated"><strong class="ak">如何选择合适的集群数？</strong></h2><p id="fd45" class="pw-post-body-paragraph ky kz jj la b lb nj kk ld le nk kn lg lh nl lj lk ll nm ln lo lp nn lr ls lt im bi translated">不幸的是，我们不能解析地解决这个问题，但是在确定K时遵循一些一般规则是有用的。K的数目在数学上和实际上都是确定的。为了提供最佳模型，我们可以从K的不同选择中计算惯性，并选择最有效的一个。这时候肘部曲线就派上用场了。肘形曲线描绘了不同K的惯性。注意，随着K的增加，惯性总是减小。如果K等于总数据点的数量，惯性将为零(每个数据点是一个聚类)。我们可以使用肘曲线来检查下降速度，并选择在<strong class="la jk">肘点</strong>处的K，当过了这个点后，惯性下降得相当慢。</p><p id="a5af" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">使用上面生成的数据点和下面的代码，我们可以绘制肘形曲线:</p><pre class="no np nq nr gt ns nt nu nv aw nw bi"><span id="5cc0" class="mq mr jj nt b gy nx ny l nz oa">inertias = []</span><span id="9d8c" class="mq mr jj nt b gy ob ny l nz oa">for n_clusters in range(2, 15):<br/> km = KMeans(n_clusters=n_clusters).fit(data)<br/> inertias.append(km.inertia_)<br/> <br/>plt.plot(range(2, 15), inertias, ‘k’)<br/>plt.title(“Inertia vs Number of Clusters”)<br/>plt.xlabel(“Number of clusters”)<br/>plt.ylabel(“Inertia”);</span></pre><p id="9bf4" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">情节是这样的:</p><figure class="no np nq nr gt iv gh gi paragraph-image"><div class="gh gi oy"><img src="../Images/afc4c95c8a14fb42d3d71e2d9a1bfd52.png" data-original-src="https://miro.medium.com/v2/resize:fit:822/format:webp/1*bvF-6iPbrGG_ddtgPEhyeg.png"/></div><p class="jc jd gj gh gi je jf bd b be z dk translated">肘部曲线:作者图片</p></figure><p id="72c5" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">该图显示惯性随着K的增加而不断减小。当K = 3时，惯性下降速度有一个转折点，这就是我们要寻找的<strong class="la jk">拐点</strong>。请注意，当我们生成数据时，我们使用三种正态分布。因此，它与基于肘部曲线的结果相匹配。</p><p id="6024" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们还需要考虑使用肘形曲线决定的聚类数是否可行。我们应该选择易于解释和实际可行的K。例如，如果您的公司只有资源(劳动力和资本)来将客户分为三类，那么您应该将K设置为3，而不考虑肘曲线的建议。</p></div><div class="ab cl mj mk hx ml" role="separator"><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo"/></div><div class="im in io ip iq"><h2 id="0751" class="mq mr jj bd ms mt mu dn mv mw mx dp my lh mz na nb ll nc nd ne lp nf ng nh ni bi translated">在应用K均值之前</h2><p id="8948" class="pw-post-body-paragraph ky kz jj la b lb nj kk ld le nk kn lg lh nl lj lk ll nm ln lo lp nn lr ls lt im bi translated">为了获得更好的模型性能和更可靠的结果，我们需要用至少两个步骤对特征进行预处理:缩放和降维。</p><p id="0935" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la jk">秤的特点</strong></p><p id="db26" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">由于K-Means使用数据点之间的欧氏距离来定义聚类，因此所有要素都应进行缩放，以便其单位具有可比性。否则，来自不可比单位的某些特征差异可能会使结果产生偏差。我们可以用scikit-learn的<code class="fe or os ot nt b">StandardScaler</code> transformer来缩放数据。它将每个要素以其平均值为中心，并通过除以该要素的标准差对其进行缩放。</p><p id="758e" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la jk">执行降维</strong></p><p id="2379" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">正如您可能注意到的，聚类算法在计算上很复杂，并且复杂性随着特征的数量而快速增加。因此，在应用K-Means聚类算法之前降低数据的维数是非常常见的。</p><p id="b19b" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la jk">主成分分析(PCA) </strong>是一种广泛使用的算法，用于减少数据中的特征数量，同时保留尽可能多的信息。更多细节你可以参考<a class="ae jg" href="https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html" rel="noopener ugc nofollow" target="_blank">这份文件</a>。</p></div><div class="ab cl mj mk hx ml" role="separator"><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo"/></div><div class="im in io ip iq"><p id="ea53" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">K-Means是最简单和最流行的聚类算法，有多种使用案例。本文重点介绍它的数学细节、它使用的度量标准以及应用它时的建议。在接下来的文章中，我将介绍另一种聚类算法LDA，以及K-Means和LDA在主题建模中的应用。</p><p id="9ed2" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">感谢您的阅读！这是我所有博客帖子的列表。如果你感兴趣，可以去看看。</p><div class="is it gp gr iu oz"><a href="https://zzhu17.medium.com/my-blog-posts-gallery-ac6e01fe5cc3" rel="noopener follow" target="_blank"><div class="pa ab fo"><div class="pb ab pc cl cj pd"><h2 class="bd jk gy z fp pe fr fs pf fu fw ji bi translated">我的博客文章库</h2><div class="pg l"><h3 class="bd b gy z fp pe fr fs pf fu fw dk translated">我快乐的地方</h3></div><div class="ph l"><p class="bd b dl z fp pe fr fs pf fu fw dk translated">zzhu17.medium.com</p></div></div><div class="pi l"><div class="pj l pk pl pm pi pn ja oz"/></div></div></a></div><div class="is it gp gr iu oz"><a href="https://zzhu17.medium.com/membership" rel="noopener follow" target="_blank"><div class="pa ab fo"><div class="pb ab pc cl cj pd"><h2 class="bd jk gy z fp pe fr fs pf fu fw ji bi translated">阅读朱(以及媒体上成千上万的其他作家)的每一个故事</h2><div class="pg l"><h3 class="bd b gy z fp pe fr fs pf fu fw dk translated">作为一个媒体会员，你的会员费的一部分会给你阅读的作家，你可以完全接触到每一个故事…</h3></div><div class="ph l"><p class="bd b dl z fp pe fr fs pf fu fw dk translated">zzhu17.medium.com</p></div></div><div class="pi l"><div class="po l pk pl pm pi pn ja oz"/></div></div></a></div></div></div>    
</body>
</html>