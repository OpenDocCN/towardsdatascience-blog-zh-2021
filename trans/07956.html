<html>
<head>
<title>Transformers in NLP</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">NLP中的变压器</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/transformers-in-nlp-7c164291326d?source=collection_archive---------28-----------------------#2021-07-21">https://towardsdatascience.com/transformers-in-nlp-7c164291326d?source=collection_archive---------28-----------------------#2021-07-21</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="d262" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">伯特、罗伯塔和GPT-3</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/ae0127d3d2cfb967e3f32221032e9b0b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Wo-WRjz1SVp7j1K6-mHofw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><a class="ae ky" href="https://unsplash.com/photos/iy_MT2ifklc" rel="noopener ugc nofollow" target="_blank">https://unsplash.com/photos/iy_MT2ifklc</a></p></figure><h1 id="c042" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">介绍</h1><p id="00a4" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">与计算机视觉不同，在自然语言处理(NLP)中，预训练模型最近才变得广泛可用。部分由于文本数据集的稀缺，NLP的进展缓慢，直到2018年变压器(BERT)双向编码器表示的发布[1]。</p><p id="3262" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">BERT由Jacob Devlin和他的同事在Google AI Language发表[1]。</p><p id="cc05" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">BERT有两种预训练模型:基本模型和大型模型。虽然两者使用相同的架构，但前者包含1.1亿个参数，而后者包含3.45亿个参数。能够使用预先训练的模型并对其进行微调以适应不同的任务意味着即使在缺乏足够的数据来训练具有数百万个参数的模型的情况下，也可以避免模型过度拟合。较大的模型通常具有更好的性能[2]。</p><p id="4d8a" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">在微调BERT时，有三种可能性[2]:</p><ul class=""><li id="8c13" class="ms mt it lt b lu mn lx mo ma mu me mv mi mw mm mx my mz na bi translated">训练所有架构</li><li id="5214" class="ms mt it lt b lu nb lx nc ma nd me ne mi nf mm mx my mz na bi translated">训练一些层，冷冻其他层</li><li id="1432" class="ms mt it lt b lu nb lx nc ma nd me ne mi nf mm mx my mz na bi translated">冻结整个网络并将额外的层附加到模型上</li></ul><p id="f1d5" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">第三种方法用于下面的案例研究(食谱-&gt;烹饪)。</p><p id="1733" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">与其他神经网络架构一样，提供的训练数据量越大，它的性能就越好。因此，一个重要的收获是，运行更多代的BERT将导致更高的准确性，并有足够的数据。作为一个参考值，包含128 000字批量的任务可能需要500K到1M的训练步骤才能收敛。就所需的计算能力而言，它相当于使用多个最先进的GPU进行几天的训练[3]。</p><p id="1a45" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">BERT的另一个特点是它的双向方法。与以前的从左到右或者结合从左到右和从右到左训练来查看文本序列的努力相反[3]。</p><p id="806c" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">与递归神经网络和长短期记忆神经网络相比，BERT的一个优点是它可以并行化。这意味着它可以通过在多个GPU中训练来加速。在输入数据是文本序列的情况下，这意味着它可以一次接受多个标记作为输入[3]。</p><p id="89ca" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">变压器的两种后实现是鲁棒优化的伯特预训练方法(罗伯塔)和生成预训练变压器3 (GPT-3)。罗伯塔于2019年由脸书研究中心发布，在广泛使用的基准-通用语言理解评估上取得了最先进的结果。后来，在2020年，OpenAI发布了GPT-3。它的完整版本拥有1750亿个机器学习参数的容量。由于其非凡的成果，微软于2020年9月22日宣布授权GPT-3独家使用。使得它只能通过开放的API提供给公众。</p><h1 id="5a0c" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">案例研究(食谱-&gt;烹饪)</h1><p id="8e6b" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">使用BERT变换器，训练了一个从给定的配料列表中预测菜肴的模型。</p><p id="6f02" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">包含大约100，00 0个配方的Kaggle和Nature数据集用于训练转换器。每个食谱包括一个配料列表(图1)，以及相应的菜肴。一份按字母顺序排列的配料清单给了模型。数据集的80%用于训练，10%用于验证，10%用于测试。</p><p id="dc23" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">在训练模型之前，标记化步骤使用BERT标记化器将每个配方分成50个标记。初始变压器的所有参数都被冻结。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ng"><img src="../Images/7e68191cf9fdc10692a453a25b46e0b7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Pxc2fYqQw8HR5UvdDl0vsg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd nh">图1 </strong>编号。每份食谱的配料数量。食谱</p></figure><p id="0ca5" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">训练时使用了实现权重衰减为1e-3的Adam算法的优化器。负对数似然被用作损失函数。并且模型收敛需要40个时期。训练在来自Google Colab的P100 GPU中进行。</p><p id="d96f" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">模型的性能是用每种菜肴的精确度、召回率和f1分数来评估的。此外，所有菜系的f1总分。以及精确度、召回率和f1分数的宏观和加权平均值(图2)。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ni"><img src="../Images/d5bf7f86cc1aa59aff00efd345e8f2c8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hpWXaHOfi8YowWEpeCcmTA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd nh">图2 </strong>料理预测器模型性能。0-非洲、1-东亚、2-东欧、3-拉丁美洲、4-中东、5-北美、6-北欧、7-南亚、8-东南亚、9-南欧&amp;10-西欧</p></figure><p id="7198" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">对于北美、南欧、拉丁美洲和东亚菜系，检测准确率达到65%以上。</p><h1 id="4f39" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">参考</h1><p id="c9a2" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">[1]<a class="ae ky" href="https://en.wikipedia.org/wiki/BERT_(language_model)" rel="noopener ugc nofollow" target="_blank">https://en . Wikipedia . org/wiki/BERT _(language _ model)</a>【2021年7月21日访问】</p><p id="5454" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">[2]<a class="ae ky" href="https://www.analyticsvidhya.com/blog/2020/07/transfer-learning-for-nlp-fine-tuning-bert-for-text-classification/" rel="noopener ugc nofollow" target="_blank">https://www . analyticsvidhya . com/blog/2020/07/transfer-learning-for-NLP-fine-tuning-Bert-for-text-class ification/</a>【2021年7月21日访问】</p><p id="304b" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">[3]<a class="ae ky" rel="noopener" target="_blank" href="/bert-explained-state-of-the-art-language-model-for-nlp-f8b21a9b6270">https://towards data science . com/Bert-explained-state-of-art-language-model-for-NLP-F8 b 21 a9b 6270</a>【2021年7月21日获取】</p><p id="8187" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">[4]<a class="ae ky" href="https://ai.facebook.com/blog/roberta-an-optimized-method-for-pretraining-self-supervised-nlp-systems/" rel="noopener ugc nofollow" target="_blank">https://ai . Facebook . com/blog/Roberta-an-optimized-method-for-pre training-self-supervised-NLP-systems/</a>【2021年7月21日访问】</p><p id="0620" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">[5]<a class="ae ky" href="https://openai.com/blog/gpt-3-apps/" rel="noopener ugc nofollow" target="_blank">https://openai.com/blog/gpt-3-apps/</a>【2021年7月21日访问】</p></div></div>    
</body>
</html>