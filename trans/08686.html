<html>
<head>
<title>Curse of Dimensionality — A “Curse” to Machine Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">维度的诅咒——机器学习的“诅咒”</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/curse-of-dimensionality-a-curse-to-machine-learning-c122ee33bfeb?source=collection_archive---------1-----------------------#2021-08-11">https://towardsdatascience.com/curse-of-dimensionality-a-curse-to-machine-learning-c122ee33bfeb?source=collection_archive---------1-----------------------#2021-08-11</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="ddcc" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">维数灾难描述了不断增长的数据维数的爆炸性本质，以及数据处理和/或分析所需的计算工作量的指数级增长。这个术语首先是由Richard E. Bellman在动态编程领域引入的，用来解释与增加额外维度相关的欧几里得空间体积的增加。今天，这种现象出现在机器学习、数据分析、数据挖掘等领域。理论上，维度的增加可以为数据增加更多的信息，从而提高数据的质量，但实际上增加了数据分析过程中的噪声和冗余。</p><p id="b981" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">机器学习算法的行为——需要数据点和模型的准确性</strong></p><p id="6cde" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在机器学习中，对象的特征可以是定义它的属性或特征。每个特征代表一个维度，一组维度创建一个数据点。这表示定义机器学习算法要使用的数据点的特征向量。当我们说维数增加时，它意味着用于描述数据的特征数量的增加。例如，在乳腺癌研究领域，年龄、癌性结节的数量可以用作定义乳腺癌患者预后的特征。这些特征构成了特征向量的<em class="kl">维度</em>。但其他因素，如过去的手术，病史，肿瘤类型和其他类似的特征，有助于医生更好地确定预后。在这种情况下，通过添加功能，我们在理论上增加了数据的维度。</p><p id="f233" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">随着维度的增加，任何机器学习算法的良好性能所需的数据点数量都呈指数增长。原因是，对于任何给定的特征组合，我们需要更多数量的数据点，以使任何机器学习模型有效。例如，假设一个模型要表现良好，对于特征值的每个组合，我们至少需要10个数据点。如果我们假设有一个二元特征，那么对于它的21个唯一值(0和1 ),我们需要2 x 10 = 20个数据点。对于2个二元特征，我们将有2个唯一值，并且需要2 x 10 = 40个数据点。因此，对于k个二元特征，我们需要2ᵏ x 10个数据点。</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="gh gi km"><img src="../Images/d072fb752add58775e609ca9c239fe6c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*h95iP-Edtxb1RNFD4wrGuA.jpeg"/></div></div></figure><p id="aefa" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">Hughes (1968)在他的研究中得出结论，在训练样本数量固定的情况下，任何分类器的预测能力首先随着维度数量的增加而增加，但是在维度数量达到一定值之后，性能会恶化。因此，维数灾难现象也被称为休斯现象。</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="gh gi ky"><img src="../Images/a35bd15e3935d5f011f5fd4c4365cd4a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pcJdd2y924Xk61SXa7uYpw.jpeg"/></div></div><p class="kz la gj gh gi lb lc bd b be z dk translated">休斯原理的图示</p></figure><p id="864c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">维数灾难对距离函数的影响:</strong></p><p id="4827" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">对于任意一点a，我们假设distₘᵢₙ(A是a和它最近的邻居之间的最小距离，distₘₐₓ(A是a和最远的邻居之间的最大距离。</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="gh gi ld"><img src="../Images/22ac367bfb37f98cdbcc6f2f4b40a19b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*86t8eoE-ivg9iRgjYbxWrg.jpeg"/></div></div></figure><p id="42c4" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">也就是说，对于d维空间，给定n个随机点，distₘᵢₙ(a≈distₘₐₓ(a意义，任何给定的点对彼此等距。</p><p id="81a3" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">因此，当数据中的维数非常高时，任何基于包括KNN(k-最近邻)的距离度量的机器学习算法都容易失败。因此，在这样的算法中，维数可以被认为是一个“诅咒”。</p><p id="f6e8" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">维度诅咒的解决方案:</strong></p><p id="5e59" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">减少高维度影响的方法之一是在空间向量中使用不同的距离度量。人们可以探索使用<em class="kl">余弦相似度</em>来代替欧几里德距离。余弦相似度对高维数据的影响较小。然而，这种方法的使用也可以特定于问题的所需解决方案。</p><p id="cae8" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">其他方法:</p><p id="73fb" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">其他方法可以包括使用尺寸缩减。可以使用的一些技术有:</p><p id="88f5" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">1.正向特征选择:这种方法包括从所有给定的特征中挑选最有用的特征子集。</p><p id="9ad4" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">2.主成分分析/t-SNE:虽然这些方法有助于减少特征的数量，但它不一定能保持类别标签，从而使结果的解释成为一项艰巨的任务。</p><p id="a4a3" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">参考文献</strong>:</p><p id="feba" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">1.应用人工智能机器学习课程，<a class="ae le" href="https://www.appliedaicourse.com/course/11/applied-machine-learning-online-course" rel="noopener ugc nofollow" target="_blank">https://www . applied AI Course . com/Course/11/applied-Machine-Learning-online-Course</a></p><p id="b4dc" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">2.G. Hughes，“关于统计模式识别器的平均精度”，IEEE信息论汇刊第14卷第1期，第55-63页，1968年1月，doi:10.1109/tit . 1968 . 19861986087</p></div></div>    
</body>
</html>