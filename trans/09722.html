<html>
<head>
<title>Attention is all you need</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">你需要的只是关注</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/attention-please-85bd0abac41?source=collection_archive---------4-----------------------#2021-09-11">https://towardsdatascience.com/attention-please-85bd0abac41?source=collection_archive---------4-----------------------#2021-09-11</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="5350" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">解释机器翻译的变压器架构</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/a9d0d1ece35a8e56941409ff9e1e5a6f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PtpkqIHbbozkUnC1JgFzxQ.png"/></div></div></figure><h1 id="4ba3" class="ku kv it bd kw kx ky kz la lb lc ld le jz lf ka lg kc lh kd li kf lj kg lk ll bi translated">动机</h1><p id="f54e" class="pw-post-body-paragraph lm ln it lo b lp lq ju lr ls lt jx lu lv lw lx ly lz ma mb mc md me mf mg mh im bi translated">假设我们想把英语翻译成德语。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mi"><img src="../Images/6530bc0b084e2c4b94a37438ab2d8e57.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gXpG4m9_Z6Ezag-fHT9XCg.png"/></div></div><p class="mj mk gj gh gi ml mm bd b be z dk translated">(图片由作者提供)</p></figure><p id="bb58" class="pw-post-body-paragraph lm ln it lo b lp mn ju lr ls mo jx lu lv mp lx ly lz mq mb mc md mr mf mg mh im bi translated">我们可以清楚地看到，我们不能通过单独翻译每个单词来翻译这个句子。例如，英语单词“the”可以翻译成“der”或“die ”,这取决于与其相关的名词的性别。同样，单词“to”根本没有被翻译成德语，因为在德语句子中没有不定式。还有更多例子可以说明一个单词的上下文是如何影响其翻译的。</p><p id="f7b5" class="pw-post-body-paragraph lm ln it lo b lp mn ju lr ls mo jx lu lv mp lx ly lz mq mb mc md mr mf mg mh im bi translated">我们需要将整个输入句子的信息输入到我们的机器翻译模型中，这样它才能理解单词的上下文。</p><p id="beea" class="pw-post-body-paragraph lm ln it lo b lp mn ju lr ls mo jx lu lv mp lx ly lz mq mb mc md mr mf mg mh im bi translated">由于大多数机器翻译模型一次输出一个单词，我们也要给模型关于它已经翻译了哪些<strong class="lo iu">部分的信息</strong>。</p><p id="0466" class="pw-post-body-paragraph lm ln it lo b lp mn ju lr ls mo jx lu lv mp lx ly lz mq mb mc md mr mf mg mh im bi translated">过去，机器翻译主要是通过使用LSTM或GRU这样的递归神经网络来完成的。然而，他们很难学习单词之间的依赖关系，因为单词之间的计算步骤数随着距离的增加而增加，所以单词在句子中距离很远。</p><p id="7d71" class="pw-post-body-paragraph lm ln it lo b lp mn ju lr ls mo jx lu lv mp lx ly lz mq mb mc md mr mf mg mh im bi translated">为了解决这个问题，引入了变形金刚，它消除了重复现象，代之以注意机制。我将介绍著名论文<a class="ae ms" href="https://arxiv.org/pdf/1706.03762.pdf" rel="noopener ugc nofollow" target="_blank">“你所需要的只是注意力”</a>中提出的架构的内部运作。</p><h1 id="89c0" class="ku kv it bd kw kx ky kz la lb lc ld le jz lf ka lg kc lh kd li kf lj kg lk ll bi translated"><strong class="ak">编码器解码器</strong></h1><p id="2a2a" class="pw-post-body-paragraph lm ln it lo b lp lq ju lr ls lt jx lu lv lw lx ly lz ma mb mc md me mf mg mh im bi translated">transformer模型可以一次预测一个单词/单词。它将我们想要翻译的源句子和它已经翻译的句子部分作为输入。然后，变压器输出下一个字。</p><p id="440b" class="pw-post-body-paragraph lm ln it lo b lp mn ju lr ls mo jx lu lv mp lx ly lz mq mb mc md mr mf mg mh im bi translated">变压器有两个不同的部分，称为“编码器”和“解码器”。输入的句子被输入编码器，而已经翻译的部分被输入解码器，解码器也产生输出。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mt"><img src="../Images/317ada7b966c6f28d3384523ad8095eb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CtnnLy6ASlp34iaJXAwa-A.png"/></div></div></figure><h1 id="f47c" class="ku kv it bd kw kx ky kz la lb lc ld le jz lf ka lg kc lh kd li kf lj kg lk ll bi translated">注意机制</h1><p id="394c" class="pw-post-body-paragraph lm ln it lo b lp lq ju lr ls lt jx lu lv lw lx ly lz ma mb mc md me mf mg mh im bi translated">注意力机制是Transformer架构的核心，其灵感来自于人脑中的<a class="ae ms" href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.465.3727&amp;rep=rep1&amp;type=pdf" rel="noopener ugc nofollow" target="_blank">注意力</a>。想象你在一个聚会上。即使你的名字被淹没在其他的噪音中，你也能听出在房间的另一边有人在喊你的名字。你的大脑可以专注于它认为重要的事情，过滤掉所有不必要的信息。</p><p id="4780" class="pw-post-body-paragraph lm ln it lo b lp mn ju lr ls mo jx lu lv mp lx ly lz mq mb mc md mr mf mg mh im bi translated">在查询、键和值的帮助下，变压器中的注意力变得更加容易。</p><p id="6428" class="pw-post-body-paragraph lm ln it lo b lp mn ju lr ls mo jx lu lv mp lx ly lz mq mb mc md mr mf mg mh im bi translated"><strong class="lo iu">Key:</strong>Key是一个单词的标签，用来区分不同的单词。</p><p id="e3a2" class="pw-post-body-paragraph lm ln it lo b lp mn ju lr ls mo jx lu lv mp lx ly lz mq mb mc md mr mf mg mh im bi translated"><strong class="lo iu">查询:</strong>检查所有可用的键，并选择最匹配的一个。所以它代表了对特定信息的主动请求。</p><p id="e2fb" class="pw-post-body-paragraph lm ln it lo b lp mn ju lr ls mo jx lu lv mp lx ly lz mq mb mc md mr mf mg mh im bi translated"><strong class="lo iu">值:</strong>键和值总是成对出现。当查询匹配一个键时，不是键本身，而是单词的值被进一步传播。值是一个单词包含的信息。</p><p id="e97a" class="pw-post-body-paragraph lm ln it lo b lp mn ju lr ls mo jx lu lv mp lx ly lz mq mb mc md mr mf mg mh im bi translated">在Transformer架构中有三种不同的注意机制。一个是编码器和解码器之间的T2。这种类型的关注被称为<strong class="lo iu">交叉关注</strong>，因为键和值是由不同于查询的序列生成的。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mu"><img src="../Images/a62744672624a6e562af31966da3c84b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FICONTQIyD8db4fcx1rkZg.png"/></div></div><p class="mj mk gj gh gi ml mm bd b be z dk translated">(图片作者</p></figure><p id="2162" class="pw-post-body-paragraph lm ln it lo b lp mn ju lr ls mo jx lu lv mp lx ly lz mq mb mc md mr mf mg mh im bi translated">如果键、值和查询是从相同的序列中生成的，那么我们称之为<strong class="lo iu">自关注</strong>。在编码器和解码器中各有一种自我关注机制。</p><p id="3f3e" class="pw-post-body-paragraph lm ln it lo b lp mn ju lr ls mo jx lu lv mp lx ly lz mq mb mc md mr mf mg mh im bi translated">自我注意在下图中用紫色表示，交叉注意用红色表示。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mv"><img src="../Images/2119a2abdc2565d4f0dc1bc3da5bafea.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XQRJz6VDX_lw1RrabyEGyA.png"/></div></div><p class="mj mk gj gh gi ml mm bd b be z dk translated">(图片由作者提供)</p></figure><p id="acd9" class="pw-post-body-paragraph lm ln it lo b lp mn ju lr ls mo jx lu lv mp lx ly lz mq mb mc md mr mf mg mh im bi translated">"那么，在数字上是如何做到的呢？"，你可能会问。一种方法是使用成比例的点积注意力。</p><h1 id="6633" class="ku kv it bd kw kx ky kz la lb lc ld le jz lf ka lg kc lh kd li kf lj kg lk ll bi translated">比例点产品关注度</h1><p id="a912" class="pw-post-body-paragraph lm ln it lo b lp lq ju lr ls lt jx lu lv lw lx ly lz ma mb mc md me mf mg mh im bi translated">首先，我们必须注意，我们通过使用嵌入层将单词表示为向量。这个向量的维数可以变化。例如，小型的GPT-2记号赋予器使用每个单词/记号768的嵌入大小。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mw"><img src="../Images/c203f44f7d834e76182550547cfc5d93.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ynj1S5JKlg0msE-O8u66aQ.png"/></div></div><p class="mj mk gj gh gi ml mm bd b be z dk translated">(图片由作者提供)</p></figure><p id="9a38" class="pw-post-body-paragraph lm ln it lo b lp mn ju lr ls mo jx lu lv mp lx ly lz mq mb mc md mr mf mg mh im bi translated">从这些单词向量中，查询(<strong class="lo iu"> q </strong>)、键(<strong class="lo iu"> k </strong>)和值(<strong class="lo iu"> v </strong>)向量通过<a class="ae ms" rel="noopener" target="_blank" href="/illustrated-self-attention-2d627e33b20a">矩阵乘以学习矩阵</a>来计算。我们把这些矩阵叫做<strong class="lo iu"> M </strong>。</p><p id="784e" class="pw-post-body-paragraph lm ln it lo b lp mn ju lr ls mo jx lu lv mp lx ly lz mq mb mc md mr mf mg mh im bi translated">在以下示例中，矩阵的形状为(3，2)。3是单词向量的长度，2是一个查询、键或值向量的长度。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mx"><img src="../Images/6075ac10a0409811627ba86d1674cf46.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*imxjJ7DZ5e8Umdp1-NLquQ.png"/></div></div><p class="mj mk gj gh gi ml mm bd b be z dk translated">(图片由作者提供)</p></figure><p id="325f" class="pw-post-body-paragraph lm ln it lo b lp mn ju lr ls mo jx lu lv mp lx ly lz mq mb mc md mr mf mg mh im bi translated">我们将查询放在矩阵<strong class="lo iu"> Q </strong>中，将键放在矩阵<strong class="lo iu"> K </strong>中，将值放在矩阵<strong class="lo iu"> V </strong>中。</p><p id="0b9d" class="pw-post-body-paragraph lm ln it lo b lp mn ju lr ls mo jx lu lv mp lx ly lz mq mb mc md mr mf mg mh im bi translated">注意力是这样计算的:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi my"><img src="../Images/f562f831f43f463c35bbcbd72ec353ea.png" data-original-src="https://miro.medium.com/v2/resize:fit:1208/format:webp/1*n4yOevDyUexoKonNPGiMzg.png"/></div></div><p class="mj mk gj gh gi ml mm bd b be z dk translated">(图片由作者提供)</p></figure><p id="353b" class="pw-post-body-paragraph lm ln it lo b lp mn ju lr ls mo jx lu lv mp lx ly lz mq mb mc md mr mf mg mh im bi translated">为了简化，我们考虑3个键、3个值和1个查询。当在<strong class="lo iu"> Q </strong>和转置的<strong class="lo iu"> K，</strong>之间取点积时，这与在每个键和查询之间取标量积是一样的。标量积越大，键和查询之间的角度越小。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mz"><img src="../Images/64efc0485cd05f058ca8e44ecc7049b8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9bkh88lu4wKyyc8MbFOBUg.png"/></div></div><p class="mj mk gj gh gi ml mm bd b be z dk translated">(图片由作者提供)</p></figure><p id="6aec" class="pw-post-body-paragraph lm ln it lo b lp mn ju lr ls mo jx lu lv mp lx ly lz mq mb mc md mr mf mg mh im bi translated">然后，softmax函数缩放分数向量，使其总和等于1。然后分数向量乘以值向量。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi na"><img src="../Images/0a5fbbb1e51d700120100a30dd95bebd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eo6xB3KDY5zJf2F1wxcshQ.png"/></div></div><p class="mj mk gj gh gi ml mm bd b be z dk translated">(图片由作者提供)</p></figure><p id="81d7" class="pw-post-body-paragraph lm ln it lo b lp mn ju lr ls mo jx lu lv mp lx ly lz mq mb mc md mr mf mg mh im bi translated">使用这个过程，我们从值中选择更多的信息，其中键和查询更相似。</p><p id="f71d" class="pw-post-body-paragraph lm ln it lo b lp mn ju lr ls mo jx lu lv mp lx ly lz mq mb mc md mr mf mg mh im bi translated">为了提高变压器的性能，我们可以引入多头关注。</p><h1 id="d0f2" class="ku kv it bd kw kx ky kz la lb lc ld le jz lf ka lg kc lh kd li kf lj kg lk ll bi translated">多头注意力</h1><p id="05ce" class="pw-post-body-paragraph lm ln it lo b lp lq ju lr ls lt jx lu lv lw lx ly lz ma mb mc md me mf mg mh im bi translated">多头注意力意味着我们有多个并行运行的点积注意力机制。这有助于网络同时处理多条信息。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nb"><img src="../Images/57c9abca5c499522d416a97d91c1c014.png" data-original-src="https://miro.medium.com/v2/resize:fit:584/format:webp/1*LytXJROogCMEyq14_0WKag.png"/></div><p class="mj mk gj gh gi ml mm bd b be z dk translated">摘自论文<a class="ae ms" href="https://arxiv.org/pdf/1706.03762.pdf" rel="noopener ugc nofollow" target="_blank">“注意力是你所需要的一切”</a></p></figure><p id="a4b4" class="pw-post-body-paragraph lm ln it lo b lp mn ju lr ls mo jx lu lv mp lx ly lz mq mb mc md mr mf mg mh im bi translated">线性层是简单的学习权重矩阵。对于每个头部，我们通过不同的学习权重矩阵<strong class="lo iu"> W </strong>来缩放<strong class="lo iu"> V </strong>、<strong class="lo iu"> K </strong>和<strong class="lo iu"> Q </strong>。并且对于整个多头也有一个输出权重矩阵。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nc"><img src="../Images/97b9506637879cf39e3135910ba8a2dd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rdUC6RWy5gQwLA6CBDSs7A.png"/></div></div><p class="mj mk gj gh gi ml mm bd b be z dk translated">(图片由作者提供)</p></figure><h1 id="9b9d" class="ku kv it bd kw kx ky kz la lb lc ld le jz lf ka lg kc lh kd li kf lj kg lk ll bi translated">位置编码</h1><p id="a778" class="pw-post-body-paragraph lm ln it lo b lp lq ju lr ls lt jx lu lv lw lx ly lz ma mb mc md me mf mg mh im bi translated">因为转换器没有循环元素，所以当我们输入一个完整的句子时，转换器没有办法知道哪个单词在哪个位置。因此，我们必须对位置进行编码。一种方法是将不同频率的正弦波和余弦波附加到字向量上。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mv"><img src="../Images/aafd06f55bdfdfa2a7f5019e6619f562.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1UjbUtnHFKQazjrTcBXLgw.png"/></div></div><p class="mj mk gj gh gi ml mm bd b be z dk translated">(图片由作者提供)</p></figure><p id="38f7" class="pw-post-body-paragraph lm ln it lo b lp mn ju lr ls mo jx lu lv mp lx ly lz mq mb mc md mr mf mg mh im bi translated">由于每个位置都有唯一的值组合，因此变压器能够准确地学习位置。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nd"><img src="../Images/b6a29950ac790f24f73481fc9ddfa260.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9RJOP8_AgyIEHoP8d0bajQ.png"/></div></div><p class="mj mk gj gh gi ml mm bd b be z dk translated">(图片由作者提供)</p></figure><p id="d5d2" class="pw-post-body-paragraph lm ln it lo b lp mn ju lr ls mo jx lu lv mp lx ly lz mq mb mc md mr mf mg mh im bi translated">假设通过使用正弦波和余弦波作为位置编码，变换器应该能够知道超出训练样本大小的位置。</p><h1 id="add9" class="ku kv it bd kw kx ky kz la lb lc ld le jz lf ka lg kc lh kd li kf lj kg lk ll bi translated">剩余连接和图层标准化</h1><p id="a768" class="pw-post-body-paragraph lm ln it lo b lp lq ju lr ls lt jx lu lv lw lx ly lz ma mb mc md me mf mg mh im bi translated">我们还在我们的架构中引入了剩余连接。这是通过在图层之后将输入添加到输出来完成的。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ne"><img src="../Images/2d48c9bff8b4eb4954a82e2bcb6f40bb.png" data-original-src="https://miro.medium.com/v2/resize:fit:784/format:webp/1*Anwy8OztuE6Me9WxPPstIA.png"/></div><p class="mj mk gj gh gi ml mm bd b be z dk translated">(图片由作者提供)</p></figure><p id="5ece" class="pw-post-body-paragraph lm ln it lo b lp mn ju lr ls mo jx lu lv mp lx ly lz mq mb mc md mr mf mg mh im bi translated">它们用于<a class="ae ms" href="https://discuss.pytorch.org/t/intuition-for-residual-connections-in-transformer-layers/99851" rel="noopener ugc nofollow" target="_blank">允许梯度直接流经网络</a>，因此也被称为跳过连接。</p><p id="bf0b" class="pw-post-body-paragraph lm ln it lo b lp mn ju lr ls mo jx lu lv mp lx ly lz mq mb mc md mr mf mg mh im bi translated">执行层标准化以保持每个训练样本的<a class="ae ms" href="https://keras.io/api/layers/normalization_layers/layer_normalization/" rel="noopener ugc nofollow" target="_blank">平均值接近0，标准偏差接近1 </a>。这有助于<a class="ae ms" href="https://arxiv.org/pdf/2002.04745.pdf" rel="noopener ugc nofollow" target="_blank">稳定训练，从而减少训练时间</a>。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nf"><img src="../Images/aa57ded48489fdbbaf2e22147a0dcf74.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*23leZREjhJAditogUPezgw.png"/></div></div><p class="mj mk gj gh gi ml mm bd b be z dk translated">(图片由作者提供)</p></figure><h1 id="2c1d" class="ku kv it bd kw kx ky kz la lb lc ld le jz lf ka lg kc lh kd li kf lj kg lk ll bi translated"><strong class="ak">整体架构</strong></h1><p id="6ace" class="pw-post-body-paragraph lm ln it lo b lp lq ju lr ls lt jx lu lv lw lx ly lz ma mb mc md me mf mg mh im bi translated">下图是变压器的整体架构。编码器和解码器可以重复N次。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/a9d0d1ece35a8e56941409ff9e1e5a6f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PtpkqIHbbozkUnC1JgFzxQ.png"/></div></div><p class="mj mk gj gh gi ml mm bd b be z dk translated">(图片由作者提供)</p></figure><p id="a19e" class="pw-post-body-paragraph lm ln it lo b lp mn ju lr ls mo jx lu lv mp lx ly lz mq mb mc md mr mf mg mh im bi translated">我们还没有提到架构的前馈部分。这是一个点式前馈网络。它是一个简单的神经网络，具有相同的输入和输出维度。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ng"><img src="../Images/0fb2569956f3ec648e1f8815c5fd9884.png" data-original-src="https://miro.medium.com/v2/resize:fit:1050/format:webp/1*H9ooubLyHIgKH-UgWaWKDw.png"/></div><p class="mj mk gj gh gi ml mm bd b be z dk translated">(图片由作者提供)</p></figure><h1 id="46c5" class="ku kv it bd kw kx ky kz la lb lc ld le jz lf ka lg kc lh kd li kf lj kg lk ll bi translated">摘要</h1><p id="82a5" class="pw-post-body-paragraph lm ln it lo b lp lq ju lr ls lt jx lu lv lw lx ly lz ma mb mc md me mf mg mh im bi translated">Transformer architecture去掉了递归，代之以一种关注机制，这种机制使用查询来选择它需要的信息(值)，基于键提供的标签。如果键、值和查询是从同一个序列中生成的，就叫自注意。在交叉注意中，查询是由不同于键值对的序列生成的。多头注意力有助于变形金刚同时处理多件事情。</p><p id="5b19" class="pw-post-body-paragraph lm ln it lo b lp mn ju lr ls mo jx lu lv mp lx ly lz mq mb mc md mr mf mg mh im bi translated">关于源文本的信息被提供给编码器，关于目标句子的已翻译部分的信息被提供给解码器，解码器还输出下一个单词/单词。网络从以不同频率的正弦波和余弦波的形式提供的位置编码中学习句子中单词的顺序。</p><h1 id="c956" class="ku kv it bd kw kx ky kz la lb lc ld le jz lf ka lg kc lh kd li kf lj kg lk ll bi translated">作者相关文章</h1><div class="nh ni gp gr nj nk"><a rel="noopener follow" target="_blank" href="/how-you-can-use-gpt-j-9c4299dd8526"><div class="nl ab fo"><div class="nm ab nn cl cj no"><h2 class="bd iu gy z fp np fr fs nq fu fw is bi translated">如何使用GPT J</h2><div class="nr l"><h3 class="bd b gy z fp np fr fs nq fu fw dk translated">GPT J解释了3种简单的方法，你可以如何访问它</h3></div><div class="ns l"><p class="bd b dl z fp np fr fs nq fu fw dk translated">towardsdatascience.com</p></div></div><div class="nt l"><div class="nu l nv nw nx nt ny ks nk"/></div></div></a></div><div class="nh ni gp gr nj nk"><a rel="noopener follow" target="_blank" href="/backpropagation-in-neural-networks-6561e1268da8"><div class="nl ab fo"><div class="nm ab nn cl cj no"><h2 class="bd iu gy z fp np fr fs nq fu fw is bi translated">神经网络中的反向传播</h2><div class="nr l"><h3 class="bd b gy z fp np fr fs nq fu fw dk translated">从零开始的神经网络，包括数学和python代码</h3></div><div class="ns l"><p class="bd b dl z fp np fr fs nq fu fw dk translated">towardsdatascience.com</p></div></div><div class="nt l"><div class="nz l nv nw nx nt ny ks nk"/></div></div></a></div><h1 id="9186" class="ku kv it bd kw kx ky kz la lb lc ld le jz lf ka lg kc lh kd li kf lj kg lk ll bi translated">作者撰写的其他文章</h1><div class="nh ni gp gr nj nk"><a rel="noopener follow" target="_blank" href="/deep-q-learning-is-no-rocket-science-e34912f1864"><div class="nl ab fo"><div class="nm ab nn cl cj no"><h2 class="bd iu gy z fp np fr fs nq fu fw is bi translated">深度Q学习不是火箭科学</h2><div class="nr l"><h3 class="bd b gy z fp np fr fs nq fu fw dk translated">用pytorch解释和编码的深度Q和双Q学习</h3></div><div class="ns l"><p class="bd b dl z fp np fr fs nq fu fw dk translated">towardsdatascience.com</p></div></div><div class="nt l"><div class="oa l nv nw nx nt ny ks nk"/></div></div></a></div><div class="nh ni gp gr nj nk"><a rel="noopener follow" target="_blank" href="/einstein-index-notation-d62d48795378"><div class="nl ab fo"><div class="nm ab nn cl cj no"><h2 class="bd iu gy z fp np fr fs nq fu fw is bi translated">爱因斯坦指数符号</h2><div class="nr l"><h3 class="bd b gy z fp np fr fs nq fu fw dk translated">爱因斯坦求和、指数符号和数值</h3></div><div class="ns l"><p class="bd b dl z fp np fr fs nq fu fw dk translated">towardsdatascience.com</p></div></div><div class="nt l"><div class="ob l nv nw nx nt ny ks nk"/></div></div></a></div><div class="nh ni gp gr nj nk"><a rel="noopener follow" target="_blank" href="/snake-with-policy-gradients-deep-reinforcement-learning-5e6e921db054"><div class="nl ab fo"><div class="nm ab nn cl cj no"><h2 class="bd iu gy z fp np fr fs nq fu fw is bi translated">具有策略梯度的Snake深度强化学习</h2><div class="nr l"><h3 class="bd b gy z fp np fr fs nq fu fw dk translated">策略梯度深度强化学习在蛇游戏中的应用</h3></div><div class="ns l"><p class="bd b dl z fp np fr fs nq fu fw dk translated">towardsdatascience.com</p></div></div><div class="nt l"><div class="oc l nv nw nx nt ny ks nk"/></div></div></a></div><h2 id="2fd7" class="od kv it bd kw oe of dn la og oh dp le lv oi oj lg lz ok ol li md om on lk oo bi translated">想联系支持我？</h2><p id="6998" class="pw-post-body-paragraph lm ln it lo b lp lq ju lr ls lt jx lu lv lw lx ly lz ma mb mc md me mf mg mh im bi translated">领英<br/><a class="ae ms" href="https://www.linkedin.com/in/vincent-m%C3%BCller-6b3542214/" rel="noopener ugc nofollow" target="_blank">https://www.linkedin.com/in/vincent-m%C3%BCller-6b3542214/</a><br/>脸书<br/><a class="ae ms" href="https://www.facebook.com/profile.php?id=100072095823739" rel="noopener ugc nofollow" target="_blank">https://www.facebook.com/profile.php?id=100072095823739</a><br/>推特<br/><a class="ae ms" href="https://twitter.com/Vincent02770108" rel="noopener ugc nofollow" target="_blank">https://twitter.com/Vincent02770108</a><br/>中等<br/><a class="ae ms" href="https://medium.com/@Vincent.Mueller" rel="noopener">https://medium.com/@Vincent.Mueller</a><br/>成为中等会员并支持我(你的部分会员费直接归我)<br/><a class="ae ms" href="https://medium.com/@Vincent.Mueller/membership" rel="noopener">https://medium.com/@Vincent.Mueller/membership</a></p><h1 id="bfc7" class="ku kv it bd kw kx ky kz la lb lc ld le jz lf ka lg kc lh kd li kf lj kg lk ll bi translated">参考</h1><p id="4e9e" class="pw-post-body-paragraph lm ln it lo b lp lq ju lr ls lt jx lu lv lw lx ly lz ma mb mc md me mf mg mh im bi translated"><a class="ae ms" href="https://jalammar.github.io/illustrated-gpt2/" rel="noopener ugc nofollow" target="_blank">图文并茂的GPT-2 </a></p><p id="dcc1" class="pw-post-body-paragraph lm ln it lo b lp mn ju lr ls mo jx lu lv mp lx ly lz mq mb mc md mr mf mg mh im bi translated"><a class="ae ms" href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.465.3727&amp;rep=rep1&amp;type=pdf" rel="noopener ugc nofollow" target="_blank">人脑中的注意力</a></p><p id="9292" class="pw-post-body-paragraph lm ln it lo b lp mn ju lr ls mo jx lu lv mp lx ly lz mq mb mc md mr mf mg mh im bi translated"><a class="ae ms" href="https://arxiv.org/pdf/1706.03762.pdf" rel="noopener ugc nofollow" target="_blank">《注意力是你所需要的全部》论文</a></p><p id="69e6" class="pw-post-body-paragraph lm ln it lo b lp mn ju lr ls mo jx lu lv mp lx ly lz mq mb mc md mr mf mg mh im bi translated"><a class="ae ms" href="https://www.youtube.com/watch?v=iDulhoQ2pro" rel="noopener ugc nofollow" target="_blank"> Yannik Kilchers关于“关注是你所需要的一切</a>”的视频</p><p id="4790" class="pw-post-body-paragraph lm ln it lo b lp mn ju lr ls mo jx lu lv mp lx ly lz mq mb mc md mr mf mg mh im bi translated"><a class="ae ms" href="https://github.com/tensorflow/tensor2tensor" rel="noopener ugc nofollow" target="_blank">谷歌的变形金刚代码库</a></p><p id="a632" class="pw-post-body-paragraph lm ln it lo b lp mn ju lr ls mo jx lu lv mp lx ly lz mq mb mc md mr mf mg mh im bi translated"><a class="ae ms" href="https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html" rel="noopener ugc nofollow" target="_blank">py torch中的变压器</a></p><p id="c60b" class="pw-post-body-paragraph lm ln it lo b lp mn ju lr ls mo jx lu lv mp lx ly lz mq mb mc md mr mf mg mh im bi translated"><a class="ae ms" href="https://stats.stackexchange.com/questions/421935/what-exactly-are-keys-queries-and-values-in-attention-mechanisms" rel="noopener ugc nofollow" target="_blank">解释的键、查询和值</a></p><p id="f4cd" class="pw-post-body-paragraph lm ln it lo b lp mn ju lr ls mo jx lu lv mp lx ly lz mq mb mc md mr mf mg mh im bi translated"><a class="ae ms" rel="noopener" target="_blank" href="/illustrated-self-attention-2d627e33b20a">关键字、查询和值的数学运算</a></p><p id="4a14" class="pw-post-body-paragraph lm ln it lo b lp mn ju lr ls mo jx lu lv mp lx ly lz mq mb mc md mr mf mg mh im bi translated"><a class="ae ms" href="https://colab.research.google.com/github/tensorflow/tensor2tensor/blob/master/tensor2tensor/notebooks/hello_t2t.ipynb" rel="noopener ugc nofollow" target="_blank"> Tensor2Tensor笔记本</a></p><p id="5141" class="pw-post-body-paragraph lm ln it lo b lp mn ju lr ls mo jx lu lv mp lx ly lz mq mb mc md mr mf mg mh im bi translated"><a class="ae ms" href="https://stats.stackexchange.com/questions/321054/what-are-residual-connections-in-rnns" rel="noopener ugc nofollow" target="_blank">剩余连接上的堆栈溢出</a></p><p id="ac05" class="pw-post-body-paragraph lm ln it lo b lp mn ju lr ls mo jx lu lv mp lx ly lz mq mb mc md mr mf mg mh im bi translated"><a class="ae ms" href="https://arxiv.org/pdf/2002.04745.pdf" rel="noopener ugc nofollow" target="_blank">变压器架构中的层标准化</a></p><p id="0765" class="pw-post-body-paragraph lm ln it lo b lp mn ju lr ls mo jx lu lv mp lx ly lz mq mb mc md mr mf mg mh im bi translated">K <a class="ae ms" href="https://keras.io/api/layers/normalization_layers/layer_normalization/" rel="noopener ugc nofollow" target="_blank">纪元层归一化</a></p></div></div>    
</body>
</html>