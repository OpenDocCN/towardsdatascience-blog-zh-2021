<html>
<head>
<title>Fine-Tune a Transformer Model for Grammar Correction</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">微调用于语法纠正的转换器模型</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/fine-tune-a-transformer-model-for-grammar-correction-b5c8ca49cc26?source=collection_archive---------31-----------------------#2021-08-18">https://towardsdatascience.com/fine-tune-a-transformer-model-for-grammar-correction-b5c8ca49cc26?source=collection_archive---------31-----------------------#2021-08-18</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="8ca2" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">学习如何训练一个名为T5的转换器模型成为你自己的语法校正器</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/851f5cbb6532e2dbb73476fe384154d6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AvCUUTzltFbUBxNGaaU33w.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><p id="226e" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在本文中，我们将讨论如何训练一个最先进的转换器模型来执行语法纠正。我们将使用一个名为T5的模型，它目前在通用语言理解评估(GLUE)基准上的表现优于人类基准，使其成为现有的最强大的NLP模型之一。T5由Google AI创建，并向全世界发布，供任何人下载和使用。</p><p id="78f4" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在本教程中，我们将使用我自己的Python包<a class="ae lu" href="https://github.com/EricFillion/happy-transformer" rel="noopener ugc nofollow" target="_blank"> Happy Transformer </a>。快乐变形金刚建立在<a class="ae lu" href="https://github.com/huggingface/transformers" rel="noopener ugc nofollow" target="_blank">拥抱脸的变形金刚库</a>之上，只需几行代码就可以轻松实现和训练变形金刚模型。因此，理解本教程的内容不需要对NLP或Python有复杂的理解，即使我们将训练世界上最有能力的人工智能模型之一。</p><p id="a07d" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><em class="lv">向下滚动到“预训练模型”部分，了解如何下载和使用我训练并上传到Hugging Face的模型分发网络的语法纠正模型。</em></p><h1 id="0004" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">装置</h1><p id="90ec" class="pw-post-body-paragraph ky kz it la b lb mo ju ld le mp jx lg lh mq lj lk ll mr ln lo lp ms lr ls lt im bi translated">快乐变压器在PyPI上可用，因此可以pip安装。</p><pre class="kj kk kl km gt mt mu mv mw aw mx bi"><span id="6cd6" class="my lx it mu b gy mz na l nb nc">pip install happytransformer</span></pre><h1 id="bcc5" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">模型</h1><p id="91ff" class="pw-post-body-paragraph ky kz it la b lb mo ju ld le mp jx lg lh mq lj lk ll mr ln lo lp ms lr ls lt im bi translated">T5有几种不同的尺寸，我们将使用<a class="ae lu" href="https://huggingface.co/t5-base" rel="noopener ugc nofollow" target="_blank">基本模型</a>，它有2.2亿个参数。最大的<a class="ae lu" href="https://huggingface.co/t5-11b" rel="noopener ugc nofollow" target="_blank">可用模型有110亿个参数，而最小的</a><a class="ae lu" href="https://huggingface.co/t5-small" rel="noopener ugc nofollow" target="_blank">有6000万个参数。</a></p><p id="c436" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">T5是一个文本到文本的模型，意味着给定的文本，它根据输入生成一段独立的文本。因此，我们将从Happy Transformer导入一个名为HappyTextToText的类，我们将使用它来加载模型。我们将为第一个位置参数提供模型类型(T5 ),为第二个位置参数提供模型名称(t5-base)。</p><pre class="kj kk kl km gt mt mu mv mw aw mx bi"><span id="3d17" class="my lx it mu b gy mz na l nb nc">from happytransformer import HappyTextToText<br/> <br/>happy_tt = HappyTextToText("T5", "t5-base")</span></pre><h1 id="3a99" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">数据收集</h1><p id="0911" class="pw-post-body-paragraph ky kz it la b lb mo ju ld le mp jx lg lh mq lj lk ll mr ln lo lp ms lr ls lt im bi translated">我们将使用一个名为JFLEG的著名数据集来训练该模型。根据其拥抱脸页面上的<a class="ae lu" href="https://huggingface.co/datasets/jfleg" rel="noopener ugc nofollow" target="_blank">描述</a>，它是“开发和评估GEC系统流畅度的黄金标准基准”(GEC代表语法错误纠正。)此外，根据谷歌学术的说法，它的<a class="ae lu" href="https://arxiv.org/abs/1702.04066" rel="noopener ugc nofollow" target="_blank">论文</a>目前有<a class="ae lu" href="https://scholar.google.com/scholar?cites=14012499899893589917&amp;as_sdt=2005&amp;sciodt=0,5&amp;hl=en" rel="noopener ugc nofollow" target="_blank"> 106次引用</a>，这表明它在NLP社区内确实受到尊重[1]。它受CC BY-NC-SA 4.0许可证的约束，这意味着您必须提供其归属，不得将其用于商业目的，并对任何衍生应用相同的许可证*。</p><p id="7978" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><em class="lv">*这不是法律建议。阅读</em> <a class="ae lu" href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener ugc nofollow" target="_blank"> <em class="lv">完全许可</em> </a> <em class="lv">了解更多信息</em></p><p id="48b0" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">该数据集在Hugging Face的数据集分发网络上<a class="ae lu" href="https://huggingface.co/datasets/jfleg" rel="noopener ugc nofollow" target="_blank">可用</a>，并且可以使用他们的数据集库来访问。因为这个库是Happy Transformer的依赖项，所以我们不需要安装它，可以直接从库中导入一个名为load_dataset的函数。</p><pre class="kj kk kl km gt mt mu mv mw aw mx bi"><span id="c8c1" class="my lx it mu b gy mz na l nb nc">from datasets import load_dataset</span></pre><p id="0338" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">数据集的id是“jfleg ”,有两个分支“‌"validation”和“测试”我们将验证集用于训练，测试集用于评估。</p><pre class="kj kk kl km gt mt mu mv mw aw mx bi"><span id="673c" class="my lx it mu b gy mz na l nb nc">train_dataset = load_dataset("jfleg", split='validation[:]') </span><span id="4c45" class="my lx it mu b gy nd na l nb nc">eval_dataset = load_dataset("jfleg", split='test[:]')</span></pre><h1 id="a425" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">数据检查</h1><p id="2cd7" class="pw-post-body-paragraph ky kz it la b lb mo ju ld le mp jx lg lh mq lj lk ll mr ln lo lp ms lr ls lt im bi translated">我们刚刚成功下载了数据集。现在让我们通过迭代一些案例来探索它。训练和评估数据集都以相同的方式构造，并且具有两个特征，句子和修正。句子功能包含每个案例的单个字符串，而更正功能包含4个人工生成的更正列表。</p><pre class="kj kk kl km gt mt mu mv mw aw mx bi"><span id="67ad" class="my lx it mu b gy mz na l nb nc">for case in train_dataset["corrections"][:2]: <br/>    print(case)<br/>    print(case[0]) <br/>    print("--------------------------------------------------")</span></pre><p id="7128" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><em class="lv">结果:</em></p><p id="df8c" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><em class="lv">【所以我认为如果我们的祖先没有发展科学技术，我们就不会活着……】</em></p><p id="fde6" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">所以我认为如果我们的祖先没有发展科学技术，我们就不会活着。</p><p id="6f1a" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi">— — — — — — — — — — — — — — — — — — — — — — — — —</p><p id="4ae0" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><em class="lv">[‘不适用于汽车’，‘不要在车里使用。’，'车不能用'，‘不能用这辆车。’】</em></p><p id="b61a" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><em class="lv">不适用于汽车。</em></p><p id="d898" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi">— — — — — — — — — — — — — — — — — — — — — — — — —</p><h1 id="b3d7" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">数据预处理</h1><p id="e638" class="pw-post-body-paragraph ky kz it la b lb mo ju ld le mp jx lg lh mq lj lk ll mr ln lo lp ms lr ls lt im bi translated">现在，我们必须将处理成Happy Transformer的适当格式。我们需要将训练和评估数据组织成相同的格式，这是一个包含两列的CSV文件:输入和目标。输入列包含语法不正确的文本，而目标列包含来自目标列的文本的正确版本。</p><p id="cb15" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">下面是将数据处理成适当格式的代码。我们必须通过给每个输入添加相同的前缀来指定我们希望执行的任务。在这种情况下，我们将使用前缀“grammar:”。这样做是因为T5模型能够用单个模型执行多项任务，如<a class="ae lu" href="https://youtu.be/2tdUyez7a2s" rel="noopener ugc nofollow" target="_blank">翻译</a>和<a class="ae lu" href="https://www.vennify.ai/summarize-text-with-transformer-models/" rel="noopener ugc nofollow" target="_blank">总结</a>，并且为每项任务使用唯一的前缀，以便模型学习执行哪项任务。我们还需要跳过包含空白字符串的情况，以避免微调时出现错误。</p><pre class="kj kk kl km gt mt mu mv mw aw mx bi"><span id="6897" class="my lx it mu b gy mz na l nb nc">import csv<br/><br/>def generate_csv(csv_path, dataset):<br/>    with open(csv_path, 'w', newline='') as csvfile:<br/>        writter = csv.writer(csvfile)<br/>        writter.writerow(["input", "target"])<br/>        for case in dataset:<br/>     	    # Adding the task's prefix to input <br/>            input_text = "grammar: " + case["sentence"]<br/>            for correction in case["corrections"]:<br/>                # a few of the cases contain blank strings. <br/>                if input_text and correction:<br/>                    writter.writerow([input_text, correction])<br/>                    <br/><br/><br/>generate_csv("train.csv", train_dataset)<br/>generate_csv("eval.csv", eval_dataset)</span></pre><p id="a8d5" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们只是生成我们的训练和评估数据！我们总共生成了3016个训练样本和2988个评估样本。</p><h1 id="06d2" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">培训前评估</h1><p id="db7f" class="pw-post-body-paragraph ky kz it la b lb mo ju ld le mp jx lg lh mq lj lk ll mr ln lo lp ms lr ls lt im bi translated">我们将使用一个称为损失的通用指标来评估微调前后的模型。损失可以描述为模型的预测与正确答案相比有多“错误”。因此，如果微调后损失减少，那么这表明模型学习了。重要的是，我们使用单独的数据进行训练和评估，以表明该模型可以概括其获得的知识，以解决看不见的情况。</p><p id="809e" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><em class="lv">你还可以使用其他指标来评估语法纠正模型。其中最流行的一个叫GLEU，在这里可以了解更多</em><a class="ae lu" href="https://www.researchgate.net/publication/283810389_Ground_truth_for_grammatical_error_correction_metrics" rel="noopener ugc nofollow" target="_blank"><em class="lv"/></a><em class="lv"/>【2】<em class="lv">。Loss是用Happy Transformer实现的最简单的方法，所以我们将使用它。</em></p><p id="48a6" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">让我们在任何训练之前确定评估数据集上的模型损失。为此，我们将调用happy_tt的eval()方法，并提供包含评估数据的CSV路径。</p><pre class="kj kk kl km gt mt mu mv mw aw mx bi"><span id="ac64" class="my lx it mu b gy mz na l nb nc">before_result = happy_tt.eval("eval.csv")</span></pre><p id="cc0f" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">结果是一个dataclass对象，它只有一个名为loss的变量，我们可以如下所示对其进行隔离。</p><pre class="kj kk kl km gt mt mu mv mw aw mx bi"><span id="a917" class="my lx it mu b gy mz na l nb nc">print("Before loss:", before_result.loss)</span></pre><p id="3651" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><em class="lv">结果:</em>亏损前:1 . 54385 . 38383838661</p><h1 id="4729" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">培养</h1><p id="2390" class="pw-post-body-paragraph ky kz it la b lb mo ju ld le mp jx lg lh mq lj lk ll mr ln lo lp ms lr ls lt im bi translated">现在让我们训练模型。我们可以通过调用happy_tt的train()方法来实现。为了简单起见，我们将使用默认参数，而不是批量大小，我们将把批量大小增加到8。如果您遇到内存不足的错误，那么我建议您减少批量大小。您可以访问此<a class="ae lu" href="https://happytransformer.com/text-to-text/finetuning/" rel="noopener ugc nofollow" target="_blank">网页</a>了解如何修改各种参数，如学习率和时期数。</p><pre class="kj kk kl km gt mt mu mv mw aw mx bi"><span id="20c4" class="my lx it mu b gy mz na l nb nc">from happytransformer import TTTrainArgs </span><span id="3068" class="my lx it mu b gy nd na l nb nc">args = TTTrainArgs(batch_size=8) </span><span id="ab23" class="my lx it mu b gy nd na l nb nc">happy_tt.train("train.csv", args=args)</span></pre><h1 id="5857" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">培训后评估</h1><p id="9b1f" class="pw-post-body-paragraph ky kz it la b lb mo ju ld le mp jx lg lh mq lj lk ll mr ln lo lp ms lr ls lt im bi translated">像以前一样，让我们确定模型的损失。</p><pre class="kj kk kl km gt mt mu mv mw aw mx bi"><span id="5b23" class="my lx it mu b gy mz na l nb nc">before_loss = happy_tt.eval("eval.csv") </span><span id="37eb" class="my lx it mu b gy nd na l nb nc">print("After loss: ", before_loss.loss)</span></pre><p id="ecd4" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><em class="lv">结果:损失后:</em>0 . 54687 . 68686868661</p><p id="daf7" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这就对了，你可以看到损失减少了！但是，现在让我们通过提供示例来更定性地评估该模型。</p><h1 id="943a" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">推理</h1><p id="9f50" class="pw-post-body-paragraph ky kz it la b lb mo ju ld le mp jx lg lh mq lj lk ll mr ln lo lp ms lr ls lt im bi translated">现在让我们用这个模型来纠正我们将提供的例子的语法。为此，我们将使用快乐tt的generate_text()方法。我们还将使用一种称为<em class="lv">波束搜索</em>的算法进行生成。您可以在这个<a class="ae lu" href="https://happytransformer.com/text-to-text/settings/" rel="noopener ugc nofollow" target="_blank">网页</a>上查看您可以修改的不同文本生成参数，以及您可以用于通用算法的不同配置。</p><pre class="kj kk kl km gt mt mu mv mw aw mx bi"><span id="0e0a" class="my lx it mu b gy mz na l nb nc">from happytransformer import TTSettings</span><span id="0103" class="my lx it mu b gy nd na l nb nc">beam_settings = TTSettings(num_beams=5, min_length=1, max_length=20)</span></pre><h2 id="cd66" class="my lx it bd ly ne nf dn mc ng nh dp mg lh ni nj mi ll nk nl mk lp nm nn mm no bi translated">示例1</h2><pre class="kj kk kl km gt mt mu mv mw aw mx bi"><span id="01f9" class="my lx it mu b gy mz na l nb nc">example_1 = "grammar: This sentences, has bads grammar and spelling!" </span><span id="186a" class="my lx it mu b gy nd na l nb nc">result_1 = happy_tt.generate_text(example_1, args=beam_settings) </span><span id="1c51" class="my lx it mu b gy nd na l nb nc">print(result_1.text)</span></pre><p id="af97" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><em class="lv">结果:这个句子有糟糕的语法和拼写！</em></p><h2 id="5052" class="my lx it bd ly ne nf dn mc ng nh dp mg lh ni nj mi ll nk nl mk lp nm nn mm no bi translated">示例2</h2><pre class="kj kk kl km gt mt mu mv mw aw mx bi"><span id="e495" class="my lx it mu b gy mz na l nb nc">example_2 = "grammar: I am enjoys, writtings articles ons AI." </span><span id="bea2" class="my lx it mu b gy nd na l nb nc">result_2 = happy_tt.generate_text(example_2, args=beam_settings) </span><span id="2b38" class="my lx it mu b gy nd na l nb nc">print(result_2.text)</span></pre><p id="f49c" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">结果:我喜欢写关于人工智能的文章。</p><h1 id="4787" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">后续步骤</h1><p id="cce9" class="pw-post-body-paragraph ky kz it la b lb mo ju ld le mp jx lg lh mq lj lk ll mr ln lo lp ms lr ls lt im bi translated">有一些方法可以潜在地提高性能。我建议将一些评估案例转移到训练数据中，然后通过应用网格搜索之类的技术来优化超参数。然后，您可以将评估案例包括在训练集中，以使用您的最佳超参数集来微调最终模型。</p><p id="6e1c" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我还建议您应用基本的数据预处理。数据集中的某些案例包含多余的空间，如果不进行更正，模型将在不需要的时候生成空间。因此，您可以应用下面的代码来更正训练和评估数据的输入和输出文本。</p><pre class="kj kk kl km gt mt mu mv mw aw mx bi"><span id="2ea1" class="my lx it mu b gy mz na l nb nc">replacements = [<br/>  (" .", "."), <br/>  (" ,", ","),<br/>  (" '", "'"),<br/>  (" ?", "?"),<br/>  (" !", "!"),<br/>  (" :", "!"),<br/>  (" ;", "!"),<br/>  (" n't", "n't"),<br/>  (" v", "n't"),<br/>  ("2 0 0 6", "2006"),<br/>  ("5 5", "55"),<br/>  ("4 0 0", "400"),<br/>  ("1 7-5 0", "1750"),<br/>  ("2 0 %", "20%"),<br/>  ("5 0", "50"),<br/>  ("1 2", "12"),<br/>  ("1 0", "10"),<br/>  ('" ballast water', '"ballast water')<br/>]<br/><br/>def remove_excess_spaces(text):<br/>  for rep in replacements:<br/>    text = text.replace(rep[0], rep[1])<br/><br/>  return text</span></pre><p id="91f8" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">现在，在generate_csv()函数的底部进行以下更改。</p><pre class="kj kk kl km gt mt mu mv mw aw mx bi"><span id="d6e1" class="my lx it mu b gy mz na l nb nc">input_text = remove_excess_spaces(input_text)<br/>correction = remove_excess_spaces(correction)<br/>writter.writerow([input_text, correction])</span></pre><p id="2ca1" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">最后，您可以保存您的模型，并在其他时间加载它，如本<a class="ae lu" href="https://happytransformer.com/save-load-model/" rel="noopener ugc nofollow" target="_blank">网页</a>所述。</p><h1 id="b919" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">测试你的技能:</h1><p id="9027" class="pw-post-body-paragraph ky kz it la b lb mo ju ld le mp jx lg lh mq lj lk ll mr ln lo lp ms lr ls lt im bi translated">你可以微调一个语法纠正模型，上传到Hugging Face的模型分发网络，增强学习。特别是，我建议你考虑使用谷歌新发布的语法纠错数据集，名为<a class="ae lu" href="https://github.com/google-research-datasets/C4_200M-synthetic-dataset-for-grammatical-error-correction" rel="noopener ugc nofollow" target="_blank"> C4_200M语法纠错合成数据集</a> [3]*。然后，跟随这个<a class="ae lu" href="https://www.vennify.ai/upload-happy-transformer/" rel="noopener ugc nofollow" target="_blank">教程</a>在你训练完一个模特后，如何上传一个模特到拥抱脸的模特分销网络。</p><p id="8b42" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">如果您使用本教程中讨论的技术发布了一个模型，请给我发电子邮件(eric@vennify.ca)。我可能会发表一篇如何使用它的文章。</p><p id="3129" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><em class="lv">*许可证:</em> <a class="ae lu" href="https://creativecommons.org/licenses/by/4.0/" rel="noopener ugc nofollow" target="_blank"> <em class="lv">知识共享署名4.0国际</em> </a></p><h1 id="54e0" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">预训练模型</h1><p id="681a" class="pw-post-body-paragraph ky kz it la b lb mo ju ld le mp jx lg lh mq lj lk ll mr ln lo lp ms lr ls lt im bi translated">我<a class="ae lu" href="https://huggingface.co/vennify/t5-base-grammar-correction" rel="noopener ugc nofollow" target="_blank">在Hugging Face的模型发布网络上发布了</a>一个模型，使用了本教程中介绍的数据集和技术。我还应用了后续步骤部分中的建议。我在下面包含了演示如何使用它的代码。</p><pre class="kj kk kl km gt mt mu mv mw aw mx bi"><span id="a59a" class="my lx it mu b gy mz na l nb nc">happy_tt = HappyTextToText("T5", "vennify/t5-base-grammar-correction")<br/><br/>result = happy_tt.generate_text("grammar: I boughts ten apple.", args=beam_settings)</span><span id="8678" class="my lx it mu b gy nd na l nb nc">print(result.text)</span></pre><p id="36ff" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><em class="lv">结果:我买了十个苹果。</em></p><h1 id="385f" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">结论:</h1><p id="cb50" class="pw-post-body-paragraph ky kz it la b lb mo ju ld le mp jx lg lh mq lj lk ll mr ln lo lp ms lr ls lt im bi translated">我希望你把你学到的东西应用到训练你自己的模型上，然后通过在拥抱脸的模型发布网络上发布给全世界。通过这样做，您将帮助许许多多渴望实现高质量语法纠正模型的人。或者，您可能只是滚动到本文的底部，以了解如何实现预训练模型。无论如何，希望你学到了有用的东西，并保持快乐！</p><h1 id="2751" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">资源</h1><p id="6900" class="pw-post-body-paragraph ky kz it la b lb mo ju ld le mp jx lg lh mq lj lk ll mr ln lo lp ms lr ls lt im bi translated">快乐变形金刚的<a class="ae lu" href="https://github.com/EricFillion/happy-transformer" rel="noopener ugc nofollow" target="_blank"> GitHub页面</a></p><p id="ca68" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">订阅我的<a class="ae lu" href="https://www.youtube.com/channel/UC7-EWrr8YdcQgPPk76OiUVw?sub_confirmation=1" rel="noopener ugc nofollow" target="_blank"> YouTube频道</a>观看即将发布的语法纠正视频。</p><p id="3afc" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">加入Happy Transformer的<a class="ae lu" href="https://discord.com/invite/psVwe3wfTb" rel="noopener ugc nofollow" target="_blank"> Discord </a>社区进行交流，并向阅读过这篇文章并对NLP充满热情的人提问</p><p id="49ea" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">本教程中使用的<a class="ae lu" href="https://colab.research.google.com/drive/1SZWAe_ND1V_sbCEOCpLFL_HrgLtKHvLo?usp=sharing" rel="noopener ugc nofollow" target="_blank">代码</a></p><h1 id="0a60" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">参考</h1><p id="4f64" class="pw-post-body-paragraph ky kz it la b lb mo ju ld le mp jx lg lh mq lj lk ll mr ln lo lp ms lr ls lt im bi translated">[1] C .纳波莱斯，k .坂口，j .泰特劳特，<a class="ae lu" href="https://arxiv.org/abs/1702.04066" rel="noopener ugc nofollow" target="_blank"> JFLEG:一个流利度语料库和语法纠错的基准</a>，EACL 2017</p><p id="373c" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">[2] C .纳波莱斯，k .坂口，m .波斯特，j .泰特劳特，<strong class="la iu"> </strong> <a class="ae lu" href="https://www.researchgate.net/publication/283810389_Ground_truth_for_grammatical_error_correction_metrics" rel="noopener ugc nofollow" target="_blank">语法纠错度量的基础真理</a>，IJCNLP 2015</p><p id="e60d" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">[3] F. Stahlberg，S. Kumar，<a class="ae lu" href="https://aclanthology.org/2021.bea-1.4/" rel="noopener ugc nofollow" target="_blank">利用标记的讹误模型进行语法纠错的合成数据生成</a>，ACL 2021</p></div><div class="ab cl np nq hx nr" role="separator"><span class="ns bw bk nt nu nv"/><span class="ns bw bk nt nu nv"/><span class="ns bw bk nt nu"/></div><div class="im in io ip iq"><p id="8a8b" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><em class="lv">原载于2021年8月18日</em><a class="ae lu" href="https://www.vennify.ai/fine-tune-grammar-correction/" rel="noopener ugc nofollow" target="_blank"><em class="lv">https://www . venni fy . ai</em></a><em class="lv">。</em></p></div></div>    
</body>
</html>