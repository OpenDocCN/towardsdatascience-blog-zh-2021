<html>
<head>
<title>DINO: Emerging Properties in Self-Supervised Vision Transformers Summary</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">DINO:自监督视觉变压器中的新兴特性摘要</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/dino-emerging-properties-in-self-supervised-vision-transformers-summary-ab91df82cc3c?source=collection_archive---------7-----------------------#2021-05-10">https://towardsdatascience.com/dino-emerging-properties-in-self-supervised-vision-transformers-summary-ab91df82cc3c?source=collection_archive---------7-----------------------#2021-05-10</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="d75e" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">对脸书惊人的视觉变形金刚迪诺的深入总结</h2></div><p id="a7fb" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">DINO是脸书人工智能公司开发的一个新的自我监督系统，能够从未标记的数据中学习令人难以置信的表示。下面是一个视频，显示了它的注意力地图，我们看到该模型能够自动学习特定于类的特征，从而实现精确的无监督对象分割。这是在他们的论文<em class="lb">“自我监督视觉变形金刚的新兴特性”</em>中介绍的</p><figure class="lc ld le lf gt lg"><div class="bz fp l di"><div class="lh li l"/></div></figure><p id="5bfe" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这里有一个它如何工作的总结👇</p><figure class="lc ld le lf gt lg"><div class="bz fp l di"><div class="lj li l"/></div></figure><p id="33e4" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir"><em class="lb">TLDR；</em></strong></p><h1 id="2c7a" class="lk ll iq bd lm ln lo lp lq lr ls lt lu jw lv jx lw jz lx ka ly kc lz kd ma mb bi translated"><strong class="ak">网络:</strong></h1><p id="a8c6" class="pw-post-body-paragraph kf kg iq kh b ki mc jr kk kl md ju kn ko me kq kr ks mf ku kv kw mg ky kz la ij bi translated">这个网络通过一个叫做“自我升华”的过程来学习。有一个教师和学生网络都具有相同的架构，一个<strong class="kh ir">视觉转换器(ViT) </strong>。</p><p id="08fd" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">老师是一个<strong class="kh ir">动量老师</strong>这意味着它的权重是学生权重的指数加权平均值。在论文“<em class="lb">无监督视觉表征学习的动量对比”</em>中引入了动量老师，以便在老师和学生是相同的并且不管输入如何都输出相同的嵌入时防止模式崩溃。</p><p id="e5f9" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">教师权重的更新规则是:</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div class="gh gi mh"><img src="../Images/76647e5b6cbd12fa8a0306a6b951035e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1024/format:webp/1*UnDm6ZOQVhQzzgSuf2CR4w.png"/></div><p class="mk ml gj gh gi mm mn bd b be z dk translated">等式1:更新规则，其中θt和θs是教师和学生的权重(<a class="ae mo" href="https://arxiv.org/abs/2104.14294" rel="noopener ugc nofollow" target="_blank">来源</a>)</p></figure><p id="75bb" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在本文的训练期间，λ遵循从0.996到1的<strong class="kh ir">余弦规律。</strong></p><h1 id="42f8" class="lk ll iq bd lm ln lo lp lq lr ls lt lu jw lv jx lw jz lx ka ly kc lz kd ma mb bi translated">数据:</h1><p id="a5e6" class="pw-post-body-paragraph kf kg iq kh b ki mc jr kk kl md ju kn ko me kq kr ks mf ku kv kw mg ky kz la ij bi translated">正如在自我监督学习中常见的那样，对一幅图像进行不同的裁剪。小作物称为局部视图(&lt;50% of the image) and large crops( &gt;图像的50%)称为全局视图。</p><p id="08c2" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">所有作物都通过学生传递，而只有全局视图通过教师传递。<strong class="kh ir"> <em class="lb">这鼓励“地方到全球”的对应，训练学生从少量作物中插入上下文。</em> </strong>见图1。</p><p id="6e0c" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">颜色抖动、高斯模糊和曝晒的随机增强也应用在视图上，以使网络更加健壮。</p><h1 id="db61" class="lk ll iq bd lm ln lo lp lq lr ls lt lu jw lv jx lw jz lx ka ly kc lz kd ma mb bi translated">失败</h1><p id="70a2" class="pw-post-body-paragraph kf kg iq kh b ki mc jr kk kl md ju kn ko me kq kr ks mf ku kv kw mg ky kz la ij bi translated">教师和学生各自预测一维嵌入。应用softmax和交叉熵损失来使学生的分布与教师的分布相匹配</p><p id="cce5" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">Softmax就像一个归一化，它转换原始激活来表示每个特征相对于整体的存在程度。例如)[-2.3，4.2，0.9，2.6，6]--&gt;[0.00，0.14，0.01，0.03，0.83]因此，我们可以说最后一个功能的强度为83%，我们希望学生的也是如此。因此，我们要求我们的学生网络拥有与教师相同的功能比例。  <strong class="kh ir"> <em class="lb">具有较大上下文的教师预测学生也必须匹配的更多高级特征。</em> </strong></p><p id="e901" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">交叉熵损失试图使两个分布相同，就像在知识提取中一样。</p><p id="426e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这也可以看作是一个虚构的分类问题。我们要求我们的网络提出一个分类问题，这样网络就可以从局部视图中学习有意义的全局表示。T11】</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div class="gh gi mp"><img src="../Images/644617a830b992887a0cf3285ddb9949.png" data-original-src="https://miro.medium.com/v2/resize:fit:1264/format:webp/1*C1_JK8uZIg_zCapv9d0ITA.png"/></div><p class="mk ml gj gh gi mm mn bd b be z dk translated">图2:迪诺流量(<a class="ae mo" href="https://arxiv.org/abs/2104.14294" rel="noopener ugc nofollow" target="_blank">来源</a>)</p></figure><h1 id="63b0" class="lk ll iq bd lm ln lo lp lq lr ls lt lu jw lv jx lw jz lx ka ly kc lz kd ma mb bi translated">居中和锐化</h1><p id="ad4d" class="pw-post-body-paragraph kf kg iq kh b ki mc jr kk kl md ju kn ko me kq kr ks mf ku kv kw mg ky kz la ij bi translated">模式折叠有两种形式:不管输入是什么，模型输出在所有维度上都是相同的(即任何输入的输出都是相同的)，或者由一个维度支配。居中和锐化旨在防止这两种情况。</p><p id="a7ca" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">居中</strong>:从教师的原始激活中减去其指数移动平均值。简单来说就是:</p><p id="7abd" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><em class="lb">Logits = Logits-Logits _ mean</em></p><p id="2906" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir"> <em class="lb">这意味着当激活高于平均值时，激活有时必须为正，当低于平均值时，激活有时必须为负。</em> </strong>这防止了任何一个特征占主导地位，因为平均值将处于范围的中间。我们知道softmax给负数很低的值，给正数很高的值。</p><p id="dbe5" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">锐化<em class="lb">:锐化等同于对softmax应用一个温度，人为地使分布更加尖峰</em> </strong>，即放大微小的差异，从而有一个或一些高值和一些低值。这防止了所有的激活都是相同的值，因为小的差异被夸大了。这与不断改变高激活的中心协同作用。锐化也有助于学生获得更强的信号，它应该增加哪些功能。</p><p id="0add" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">DINO Psedudocode:</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="mr ms di mt bf mu"><div class="gh gi mq"><img src="../Images/43bf6db89dab11ac76a85f632b93eb82.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kUkf2ilf5KySJsdX9RcWQg.png"/></div></div><p class="mk ml gj gh gi mm mn bd b be z dk translated">图3: DINO Psedudocode( <a class="ae mo" href="https://arxiv.org/abs/2104.14294" rel="noopener ugc nofollow" target="_blank">来源</a>)</p></figure><h1 id="224e" class="lk ll iq bd lm ln lo lp lq lr ls lt lu jw lv jx lw jz lx ka ly kc lz kd ma mb bi translated">可视化:</h1><p id="38cd" class="pw-post-body-paragraph kf kg iq kh b ki mc jr kk kl md ju kn ko me kq kr ks mf ku kv kw mg ky kz la ij bi translated">这里有一些注意力地图显示，迪诺能够专注于图像中感兴趣的物体。这意味着DINO很好地理解了对象语义，它的注意力图看起来就像分段掩码。</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="mr ms di mt bf mu"><div class="gh gi mv"><img src="../Images/8a1f8b60adc229ed670262642ec59972.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PWhJqJFo5DIDYTuXLsUySg.png"/></div></div><p class="mk ml gj gh gi mm mn bd b be z dk translated">图4:注意力地图(<a class="ae mo" href="https://arxiv.org/abs/2104.14294" rel="noopener ugc nofollow" target="_blank">来源</a></p></figure><p id="8e4e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们还发现，恐龙的潜在空间甚至在动物群体中也有很好的分类，这意味着它的特征足够丰富，可以区分物体的微小差异。这使得它非常适合下游任务和迁移学习。</p><p id="dcf2" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">它在识别重复图像方面也做得很好，如下所示。</p><figure class="lc ld le lf gt lg"><div class="bz fp l di"><div class="lj li l"/></div></figure><h1 id="34a2" class="lk ll iq bd lm ln lo lp lq lr ls lt lu jw lv jx lw jz lx ka ly kc lz kd ma mb bi translated">总结:</h1><p id="a7fc" class="pw-post-body-paragraph kf kg iq kh b ki mc jr kk kl md ju kn ko me kq kr ks mf ku kv kw mg ky kz la ij bi translated">学生ViT学习在动量老师ViT的嵌入的交叉熵损失的监督下，从局部面片预测图像的全局特征，同时进行居中和锐化以防止模式崩溃…哇，这是一大堆术语！</p><p id="86c8" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi">— — — — — — — — — — — — — — — — — — — — — — — — — —</p><p id="c959" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">自监督视觉变压器中新兴特性的相关链接:</p><p id="9545" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">博客:<a class="ae mo" href="https://ai.facebook.com/blog/dino-paws-computer-vision-with-self-supervised-transformers-and-10x-more-efficient-training/" rel="noopener ugc nofollow" target="_blank">https://ai . Facebook . com/blog/dino-paws-computer-vision-with-self-supervised-transformers-and-10x-more-efficient-training/</a></p><p id="6559" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">GitHub:【https://github.com/facebookresearch/dino T4】</p><p id="e1aa" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">论文:<a class="ae mo" href="https://arxiv.org/abs/2104.14294" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/2104.14294</a></p><p id="ef98" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><a class="ae mo" href="https://twitter.com/schrep" rel="noopener ugc nofollow" target="_blank">https://twitter.com/schrep/status/1388189398496202752</a>的推文:<a class="ae mo" href="https://twitter.com/schrep/status/1388189398496202752" rel="noopener ugc nofollow" target="_blank">T11】</a></p><p id="010d" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">扬尼克:<a class="ae mo" href="https://www.youtube.com/watch?v=h3ij3F3cPIk&amp;t=2106s" rel="noopener ugc nofollow" target="_blank">https://www.youtube.com/watch?v=h3ij3F3cPIk&amp;t = 2106s</a></p></div></div>    
</body>
</html>