<html>
<head>
<title>PCA, LDA, and SVD: Model Tuning Through Feature Reduction for Transportation POI Classification</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">PCA、LDA和SVD:通过用于交通POI分类的特征缩减进行模型调整</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/pca-lda-and-svd-model-tuning-through-feature-reduction-for-transportation-poi-classification-8d20501ee255?source=collection_archive---------11-----------------------#2021-07-09">https://towardsdatascience.com/pca-lda-and-svd-model-tuning-through-feature-reduction-for-transportation-poi-classification-8d20501ee255?source=collection_archive---------11-----------------------#2021-07-09</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="0999" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">比较要素缩减方法以调整将POI记录分类为机场、火车站或公交车站的模型</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/b77ecba207b063116740e5f89af60556.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nHvi3nMS7tlpxbthx9DDXg.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">布雷特·乔丹在<a class="ae kv" href="https://unsplash.com/s/photos/reduce?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="b104" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir"> <em class="ls">注意</em> </strong> <em class="ls">:本文是关于使用安全图模式数据进行分类的系列文章中的第二篇。第一篇文章使用几个多类分类器(如高斯朴素贝叶斯分类器、决策树分类器、K-最近邻分类器和支持向量机分类器)来分析数据的初始分类，如公交车站、机场和火车站。第1部分可在此链接找到</em>:<em class="ls">:</em><a class="ae kv" rel="noopener" target="_blank" href="/poi-classification-using-visit-and-popularity-metrics-part-1-ae5e94f92077"><em class="ls">POI分类第1部分</em> </a></p><p id="f606" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">本文将通过使用各种特征约简算法来调整我们在本系列第一部分中使用的模型。这篇文章将作为本项目笔记本中代码片段的解释。该项目利用来自<a class="ae kv" href="https://www.safegraph.com/" rel="noopener ugc nofollow" target="_blank">安全图</a>的数据。SafeGraph是一家数据提供商，为数千家企业和类别提供POI数据。它向学术界免费提供数据。对于这个项目，我选择使用SafeGraph模式数据，以便将记录分类为各种POI。模式数据的模式可以在这里找到:<a class="ae kv" href="https://docs.safegraph.com/v4.0/docs/places-schema#section-patterns" rel="noopener ugc nofollow" target="_blank">模式信息</a>。如果你想看笔记本并自己运行代码，在这里看笔记本<a class="ae kv" href="https://colab.research.google.com/drive/1A6ZZ0WZX3v4N5sN4pU5Yz_gxx2gThVqd?usp=sharing" rel="noopener ugc nofollow" target="_blank">看</a></p><p id="ce92" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">什么是特征约简？</strong></p><p id="8697" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">真实世界的数据非常复杂，由许多要素组成，尤其是用于机器学习的数据。随着要素数量的增加，可视化数据变得越来越困难。此外，大量特征的存在导致更长的计算时间。因此，为了更好地可视化数据并更快地执行计算，我们必须通过组合变量来移除不需要的特征或降低特征的维度。</p></div><div class="ab cl lt lu hu lv" role="separator"><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly"/></div><div class="ij ik il im in"><p id="ac4d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在我们进入特征缩减概念的第一步之前，我们必须首先加载我们将用于这个项目的数据:加载数据的过程可以在<a class="ae kv" href="https://colab.research.google.com/drive/1A6ZZ0WZX3v4N5sN4pU5Yz_gxx2gThVqd?usp=sharing" rel="noopener ugc nofollow" target="_blank">笔记本</a>中找到，并且已经在系列的<a class="ae kv" rel="noopener" target="_blank" href="/poi-classification-using-visit-and-popularity-metrics-part-1-ae5e94f92077">第一部分中详细解释。就我们的目的而言，所需要的只是对所采取的步骤和结果数据框架的简要概述:</a></p><ol class=""><li id="187d" class="ma mb iq ky b kz la lc ld lf mc lj md ln me lr mf mg mh mi bi translated">删除不必要的列- ['parent_safegraph_place_id '，' placekey '，' safegraph_place_id '，' parent_placekey '，' parent_placekey '，' safegraph_brand_ids '，' brands '，' poi_cbg']</li><li id="c5db" class="ma mb iq ky b kz mj lc mk lf ml lj mm ln mn lr mf mg mh mi bi translated">创建地面实况列，将每个记录建立为机场、汽车站、机场或未知</li><li id="febe" class="ma mb iq ky b kz mj lc mk lf ml lj mm ln mn lr mf mg mh mi bi translated">删除未知记录以清除无法识别的记录</li><li id="7bc3" class="ma mb iq ky b kz mj lc mk lf ml lj mm ln mn lr mf mg mh mi bi translated">使用pyspark水平分解JSON字符串的列</li><li id="8ec2" class="ma mb iq ky b kz mj lc mk lf ml lj mm ln mn lr mf mg mh mi bi translated">阵列的水平分解列</li><li id="da8c" class="ma mb iq ky b kz mj lc mk lf ml lj mm ln mn lr mf mg mh mi bi translated">使用Sklearn LabelEncoder包转换类列</li></ol><p id="d4b1" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">作为这些转换的结果，输出的数据如下所示，并具有以下各列:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mo"><img src="../Images/5ebb29ef6eb008d343afe62246c8c71c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aUHnefLrSabnnybfKhFz5g.png"/></div></div></figure><p id="02d9" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir"> Raw_visit_counts: </strong>在日期范围内，我们的小组中对此兴趣点的访问次数。</p><p id="f1d3" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir"> Raw_visitor_counts: </strong>在日期范围内从我们的面板访问该兴趣点的独立访问者的数量。</p><p id="2df7" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir"> Distance_from_home: </strong>访客(我们已确定其住所的访客)离家的中间距离，以米为单位。</p><p id="efc6" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">中值_停留:</strong>中值最小停留时间，以分钟为单位。</p><p id="f57a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">分时段停留(分解为&lt; 5，5–10，11-20，21-60，61-120，121–240):</strong>键是分钟范围，值是在该持续时间内的访问次数</p><p id="7f27" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir"> Popularity_by_day(分解到周一至周日):</strong>一周中的某一天到日期范围内每天(当地时间)的访问次数的映射</p><p id="517f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir"> Popularity_by_hour(分解为Popularity _ 1-Popularity _ 24):</strong>将一天中的某个小时映射到当地时间日期范围内每小时的访问量。数组中的第一个元素对应于从午夜到凌晨1点的时间</p><p id="e86b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir"> Device_type(分解为ios和Android): </strong>使用Android和IOS的POI访客人数。仅显示至少包含2个设备的设备类型，包含少于5个设备的任何类别都报告为4</p></div><div class="ab cl lt lu hu lv" role="separator"><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly"/></div><div class="ij ik il im in"><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mp"><img src="../Images/4997fdb8e4ff7656a498b71ea22af5c3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zJUshIh3G9U_sI24CgaIhA.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><a class="ae kv" href="https://unsplash.com/@worthyofelegance?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Alex </a>在<a class="ae kv" href="https://unsplash.com/s/photos/divide?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="d8b5" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">既然已经获得了数据，我们可以使用特征约简来查看整体精度和各个类的精度是如何变化的。以下是高斯朴素贝叶斯模型、决策树模型和K-最近邻模型在使用原始数据集进行训练和测试时的热图和精度，以供参考:</p><p id="b276" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">高斯朴素贝叶斯:</strong></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mq"><img src="../Images/5ef11bc1090e21466a2d8e08e0124308.png" data-original-src="https://miro.medium.com/v2/resize:fit:606/format:webp/1*q8A_rXD93MXNKQBUi0qMnQ.png"/></div></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mr"><img src="../Images/612423f2cd64af93e7ff33dbd7a2e31a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1376/format:webp/1*kC93ugCNmhytjBx3Ig2doQ.png"/></div></div></figure><p id="7f3e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">决策树:</strong></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ms"><img src="../Images/234b50b3e0071e5f2919be8ed6485791.png" data-original-src="https://miro.medium.com/v2/resize:fit:920/format:webp/1*CBSnRWa093KXagVqmh5ELA.png"/></div></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mt"><img src="../Images/09106c3ce5c1729c21271fdd76c68171.png" data-original-src="https://miro.medium.com/v2/resize:fit:1354/format:webp/1*hb8H_1Lr4ToHi_Q4CcNajQ.png"/></div></figure><p id="eaee" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">K-最近邻分类器</strong></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mu"><img src="../Images/b05614e9c18caff661ac6a19789c52ba.png" data-original-src="https://miro.medium.com/v2/resize:fit:834/format:webp/1*OAbuR5B9AXCyOt-0yMjJwg.png"/></div></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mv"><img src="../Images/f4ba058ee2124965cdc3e199bd1b484d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1358/format:webp/1*HH0frkseJOHLjhM-s_iEOw.png"/></div></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mw"><img src="../Images/30fa175b28f9f2fa4a56fc8c6ce02315.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sEy5DSUZRQAwacftWXPW3w.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><a class="ae kv" href="https://unsplash.com/@jeremy0?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">杰里米·泽罗</a>在<a class="ae kv" href="https://unsplash.com/s/photos/divide?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure></div><div class="ab cl lt lu hu lv" role="separator"><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly"/></div><div class="ij ik il im in"><h1 id="44ce" class="mx my iq bd mz na nb nc nd ne nf ng nh jw ni jx nj jz nk ka nl kc nm kd nn no bi translated"><strong class="ak">使用主成分分析的特征减少</strong></h1><p id="8f59" class="pw-post-body-paragraph kw kx iq ky b kz np jr lb lc nq ju le lf nr lh li lj ns ll lm ln nt lp lq lr ij bi translated">PCA是一种降维方法，它采用具有大量特征的数据集，并将它们减少到几个基本特征。PCA通过执行以下步骤来查找给定数据集中的基础要素:</p><p id="d2e9" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">1.计算特征矩阵的协方差</p><p id="971b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">2.计算协方差矩阵的特征向量和特征值</p><p id="0874" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">3.按特征值降序排列特征向量</p><p id="2ad6" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">4.选择第k个特征向量。这个<strong class="ky ir"> k </strong>值将成为<strong class="ky ir"> k </strong>的新尺寸</p><p id="e745" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">5.将原始数据转换成<strong class="ky ir"> k </strong>尺寸</p><p id="1120" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这篇文章深入探讨了PCA背后的数学原理</p><p id="6727" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">sklearn的PCA包为我们执行这个过程。</p><p id="3b23" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在下面的代码片段中，我们将初始数据集的75个要素缩减为8个要素。</p><pre class="kg kh ki kj gt nu nv nw nx aw ny bi"><span id="2713" class="nz my iq nv b gy oa ob l oc od">from sklearn.decomposition import PCA</span><span id="5739" class="nz my iq nv b gy oe ob l oc od">pca = PCA(n_components=8)</span><span id="489f" class="nz my iq nv b gy oe ob l oc od">Principal_components=pca.fit_transform(transportation_df.drop([‘location_name’, ‘Class’], axis=1))</span><span id="78ec" class="nz my iq nv b gy oe ob l oc od">pca_df = pd.DataFrame(data = Principal_components, columns = [‘PC 1’, ‘PC 2’, ‘PC 3’, ‘PC 4’, ‘PC 5’, ‘PC 6’, ‘PC 7’, ‘PC 8’])</span><span id="54a7" class="nz my iq nv b gy oe ob l oc od">pca_df.head(3)</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi of"><img src="../Images/aa4959f467975d48cd1899b06d9b15c4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1230/format:webp/1*Ll1nZT0oHPOAR3CGHqzMAg.png"/></div></div></figure><p id="c9ce" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">此代码片段用于显示特征缩减算法适合的最佳特征数量。scree图显示理想值是<code class="fe og oh oi nv b">n = 2</code>，但是，我们将继续使用<code class="fe og oh oi nv b">n = 8</code>特征进行剩余的分析，因为它实际上为下面的分类器提供了比<code class="fe og oh oi nv b">n = 2</code>特征稍好的准确性</p><pre class="kg kh ki kj gt nu nv nw nx aw ny bi"><span id="2220" class="nz my iq nv b gy oa ob l oc od">PC_values = np.arange(pca.n_components_) + 1</span><span id="b1c6" class="nz my iq nv b gy oe ob l oc od">plt.plot(PC_values, pca.explained_variance_ratio_, ‘ro-’, linewidth=2)</span><span id="f3d9" class="nz my iq nv b gy oe ob l oc od">plt.title(‘Scree Plot’)</span><span id="20b6" class="nz my iq nv b gy oe ob l oc od">plt.xlabel(‘Principal Component’)</span><span id="755c" class="nz my iq nv b gy oe ob l oc od">plt.ylabel(‘Proportion of Variance Explained’)</span><span id="462e" class="nz my iq nv b gy oe ob l oc od">plt.show()</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/960f279fc95f8a073c0bf0ff4898327a.png" data-original-src="https://miro.medium.com/v2/resize:fit:672/format:webp/1*cMAZ3fnuWkPJdYFYVNgn5g.png"/></div></figure><p id="b0be" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">将类列添加回PCA数据帧</p><pre class="kg kh ki kj gt nu nv nw nx aw ny bi"><span id="794d" class="nz my iq nv b gy oa ob l oc od">PCA_df = pd.concat([pca_df, transportation_df[‘Class’]], axis = 1)</span><span id="7673" class="nz my iq nv b gy oe ob l oc od">PCA_df.head()</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ok"><img src="../Images/78a1f4d4904e9cc3f209d8a720e287a6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1308/format:webp/1*SqLtUlwkH4D7nPvDsMay0A.png"/></div></figure><p id="a6c1" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">将PC1和PC2绘制成不同类别的散点图</p><pre class="kg kh ki kj gt nu nv nw nx aw ny bi"><span id="981e" class="nz my iq nv b gy oa ob l oc od">PC_0 = PCA_df.where(PCA_df[‘Class’] == 0).dropna()[[‘PC 1’,’PC 2']]</span><span id="ce56" class="nz my iq nv b gy oe ob l oc od">PC_1 = PCA_df.where(PCA_df[‘Class’] == 1).dropna()[[‘PC 1’,’PC 2']]</span><span id="d339" class="nz my iq nv b gy oe ob l oc od">PC_2 = PCA_df.where(PCA_df[‘Class’] == 2).dropna()[[‘PC 1’,’PC 2']]</span><span id="0e82" class="nz my iq nv b gy oe ob l oc od">plt.scatter(PC_0[‘PC 1’],PC_0[‘PC 2’])</span><span id="1a45" class="nz my iq nv b gy oe ob l oc od">plt.scatter(PC_1[‘PC 1’],PC_1[‘PC 2’])</span><span id="f359" class="nz my iq nv b gy oe ob l oc od">plt.scatter(PC_2[‘PC 1’],PC_2[‘PC 2’])</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ol"><img src="../Images/ac8c07499ee5fb5ad29e7e9845cf0392.png" data-original-src="https://miro.medium.com/v2/resize:fit:742/format:webp/1*ZLcsbIjrhLNqIT2jNLyHiQ.png"/></div></figure><h1 id="eb75" class="mx my iq bd mz na om nc nd ne on ng nh jw oo jx nj jz op ka nl kc oq kd nn no bi translated"><strong class="ak">使用PCA的分类算法</strong></h1><p id="acaa" class="pw-post-body-paragraph kw kx iq ky b kz np jr lb lc nq ju le lf nr lh li lj ns ll lm ln nt lp lq lr ij bi translated">以下片段将展示如何使用高斯朴素贝叶斯、决策树和K-最近邻分类器来简化特征。</p><p id="76ba" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">将数据分为训练数据和测试数据</p><pre class="kg kh ki kj gt nu nv nw nx aw ny bi"><span id="34ea" class="nz my iq nv b gy oa ob l oc od">x_cols = []</span><span id="c6ae" class="nz my iq nv b gy oe ob l oc od">for item in list(PCA_df.columns):</span><span id="416f" class="nz my iq nv b gy oe ob l oc od">if(item != 'Class'):</span><span id="c005" class="nz my iq nv b gy oe ob l oc od">x_cols.append(item)</span><span id="9c6f" class="nz my iq nv b gy oe ob l oc od">X = PCA_df[x_cols]</span><span id="0aa9" class="nz my iq nv b gy oe ob l oc od">y = PCA_df['Class']</span><span id="e95b" class="nz my iq nv b gy oe ob l oc od">X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 0)</span></pre><p id="5d7c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">高斯朴素贝叶斯分类器:</strong></p><pre class="kg kh ki kj gt nu nv nw nx aw ny bi"><span id="d9af" class="nz my iq nv b gy oa ob l oc od">gnb = GaussianNB().fit(X_train, y_train)</span><span id="5b95" class="nz my iq nv b gy oe ob l oc od">gnb_predictions = gnb.predict(X_test)</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi or"><img src="../Images/62b61952cbaa3e6614c9b017f2944973.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iCa2Sfu59Tot_oSMOuff5g.png"/></div></div></figure><p id="8f4b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">将特征减少到n = 8个特征似乎有助于高斯朴素贝叶斯模型的整体模型准确性。预测率略好于原始数据集的准确率~26%。该模型将实际机场预测为预测列车，PCA数据帧的速率略高于初始数据帧(. 770 vs .802)。该模型很好地正确预测了火车站，但表现略差于初始数据(. 940 vs. .968)。该模型在正确预测公交车站方面稍差一些(0.009比0.00)，但这种准确性的降低会导致模型错误地预测所有公交车站记录。该模型预测机场的精度是原始数据集的两倍(. 59 vs .120)。我们可以从上面的热图中看到，大多数记录都被归类为火车站。</p><p id="eb78" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">决策树分类器:</strong></p><pre class="kg kh ki kj gt nu nv nw nx aw ny bi"><span id="cf08" class="nz my iq nv b gy oa ob l oc od">dtree_model = DecisionTreeClassifier(max_depth = 3).fit(X_train, y_train)</span><span id="d136" class="nz my iq nv b gy oe ob l oc od">dtree_predictions = dtree_model.predict(X_test)</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi os"><img src="../Images/bbd2180f05e1555245c3a9840c14a01d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hHijaEuW8z_UsLnwzTyj3g.png"/></div></div></figure><p id="8c11" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">决策树模型使用PCA数据帧的总体性能比使用原始数据帧的性能稍差(75%比73.93%)。该模型在正确预测机场方面表现非常好，事实上，比原始数据在这个特定模型中表现得更好(. 958 vs .912)。该模型在预测公交车站方面表现不佳，这与该模型对原始数据集的预测方式类似。该模型预测火车站的正确率低于原始数据集的正确率(. 387 vs. .553)。该模型倾向于将大多数记录预测为机场。</p><p id="82cd" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">K-最近邻:</strong></p><pre class="kg kh ki kj gt nu nv nw nx aw ny bi"><span id="720a" class="nz my iq nv b gy oa ob l oc od">knn = KNeighborsClassifier(n_neighbors = 22).fit(X_train, y_train)</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ot"><img src="../Images/0d6169cbb35f9146085b4d2a4c1b5cd6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vOg-BzMQcHoSWq_5zWOoRA.png"/></div></div></figure><p id="8f8c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">K-最近邻算法具有良好的总体精度，并且与使用原始数据集的算法一样好。该模型以类似于原始数据集的速率(0.931比0.937)准确预测机场记录。该模型准确地预测了公共汽车记录和火车记录，预测率与原始数据的预测率相同(分别为0.027比0.027，0.180比0.180)。在应用PCA算法之前，大部分数据被分类，因此，对于这个特定的模型，使用PCA来降低特征的维度似乎不是最佳选择。</p><p id="6757" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">从这三个模型及其结果中，我们可以看到PCA数据框架并没有提高决策树和K-最近邻等模型的准确性。唯一看到增长的模型是高斯朴素贝叶斯模型。这可能是由于模型没有考虑特征之间的相关性，这个问题用主成分解决了，因为主成分考虑了原始特征之间的相关性。因此，通过主分量的这种调节导致了稍好的精确度。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ou"><img src="../Images/995774318ab8cbc3fa16c29942286833.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7hbjjHu0T70VdHhyFYfVyw.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">Anthony Intraversato 在<a class="ae kv" href="https://unsplash.com/s/photos/divide?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><h1 id="43e6" class="mx my iq bd mz na om nc nd ne on ng nh jw oo jx nj jz op ka nl kc oq kd nn no bi translated">使用LDA的特征约简</h1><p id="e8a4" class="pw-post-body-paragraph kw kx iq ky b kz np jr lb lc nq ju le lf nr lh li lj ns ll lm ln nt lp lq lr ij bi translated">线性判别分析是另一种降维算法。下面是对LDA算法的深入研究:<a class="ae kv" href="https://medium.com/@srishtisawla/linear-discriminant-analysis-d38decf48105" rel="noopener"> LDA文章</a></p><p id="acc3" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">LDA通过以下步骤工作:</p><p id="88e4" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">1)计算不同要素的平均值之间的距离-这称为要素间方差</p><p id="99f0" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">2)计算每个要素的平均值和样本之间的距离-这称为要素内方差</p><p id="beae" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">3)构造低维空间以最大化特征间方差和最小化特征内方差</p><p id="5e0f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">sk learnlinear discriminant analysis软件包非常方便地为我们执行这一分析</p><pre class="kg kh ki kj gt nu nv nw nx aw ny bi"><span id="209f" class="nz my iq nv b gy oa ob l oc od">from sklearn.discriminant_analysis import LinearDiscriminantAnalysis</span><span id="efb9" class="nz my iq nv b gy oe ob l oc od">lda = LinearDiscriminantAnalysis(n_components=2, solver=’svd’)</span><span id="2f52" class="nz my iq nv b gy oe ob l oc od">X_lda = lda.fit_transform(transportation_df.drop([‘location_name’, ‘Class’], axis=1), transportation_df[‘Class’])</span><span id="0b5c" class="nz my iq nv b gy oe ob l oc od">lda_df = pd.DataFrame(data = X_lda, columns = [‘PC 1’, ‘PC 2’])</span><span id="3068" class="nz my iq nv b gy oe ob l oc od">lda_df.head()</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ov"><img src="../Images/b720194556820f3cb9e4a3cb5730af85.png" data-original-src="https://miro.medium.com/v2/resize:fit:282/format:webp/1*0E1zDbvIUbeU1AtQJROgMg.png"/></div></figure><p id="90b0" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">将类列添加到LDA数据帧:</p><pre class="kg kh ki kj gt nu nv nw nx aw ny bi"><span id="5509" class="nz my iq nv b gy oa ob l oc od">lda_df = pd.concat([lda_df, transportation_df[‘Class’]], axis = 1)</span><span id="9d24" class="nz my iq nv b gy oe ob l oc od">lda_df.head()</span></pre><p id="cc74" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">下面的代码片段将数据集中的不同类绘制成PC 1和PC 2的函数。从图中可以看出，LDA分析得出的降维结果与PCA分析得出的降维结果有很大不同。让我们看看这个特殊的降维算法的有效性</p><pre class="kg kh ki kj gt nu nv nw nx aw ny bi"><span id="c883" class="nz my iq nv b gy oa ob l oc od">PC_0 = lda_df.where(lda_df[‘Class’] == 0).dropna()[[‘PC 1’,’PC 2']]</span><span id="1985" class="nz my iq nv b gy oe ob l oc od">PC_1 = lda_df.where(lda_df[‘Class’] == 1).dropna()[[‘PC 1’,’PC 2']]</span><span id="c0ad" class="nz my iq nv b gy oe ob l oc od">PC_2 = lda_df.where(lda_df[‘Class’] == 2).dropna()[[‘PC 1’,’PC 2']]</span><span id="9ded" class="nz my iq nv b gy oe ob l oc od">plt.scatter(PC_0[‘PC 1’],PC_0[‘PC 2’])</span><span id="7fc5" class="nz my iq nv b gy oe ob l oc od">plt.scatter(PC_1[‘PC 1’],PC_1[‘PC 2’])</span><span id="8d95" class="nz my iq nv b gy oe ob l oc od">plt.scatter(PC_2[‘PC 1’],PC_2[‘PC 2’])</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ow"><img src="../Images/3d7e6f05d0406a69d4eaf4b60af39074.png" data-original-src="https://miro.medium.com/v2/resize:fit:728/format:webp/1*8p9B-57Tbhketl_UwqZzMA.png"/></div></figure><h1 id="9999" class="mx my iq bd mz na om nc nd ne on ng nh jw oo jx nj jz op ka nl kc oq kd nn no bi translated">使用LDA的分类算法</h1><p id="d601" class="pw-post-body-paragraph kw kx iq ky b kz np jr lb lc nq ju le lf nr lh li lj ns ll lm ln nt lp lq lr ij bi translated">以下片段将展示如何使用高斯朴素贝叶斯、决策树和K-最近邻分类器来简化特征。</p><p id="9f8e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">将数据分为训练数据和测试数据</p><pre class="kg kh ki kj gt nu nv nw nx aw ny bi"><span id="43c2" class="nz my iq nv b gy oa ob l oc od">x_cols = []</span><span id="4b51" class="nz my iq nv b gy oe ob l oc od">for item in list(lda_df.columns):</span><span id="b95b" class="nz my iq nv b gy oe ob l oc od">if(item != 'Class'):</span><span id="d20e" class="nz my iq nv b gy oe ob l oc od">x_cols.append(item)</span><span id="96c3" class="nz my iq nv b gy oe ob l oc od">X = lda_df[x_cols]</span><span id="36de" class="nz my iq nv b gy oe ob l oc od">y = lda_df['Class']</span><span id="23b6" class="nz my iq nv b gy oe ob l oc od">X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 0)</span></pre><p id="0126" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">高斯朴素贝叶斯分类器:</strong></p><pre class="kg kh ki kj gt nu nv nw nx aw ny bi"><span id="6ad1" class="nz my iq nv b gy oa ob l oc od">gnb = GaussianNB().fit(X_train, y_train)</span><span id="9a65" class="nz my iq nv b gy oe ob l oc od">gnb_predictions = gnb.predict(X_test)</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ox"><img src="../Images/3f9a2146829d7cbf7e86b32d3832243a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fM9HQdREZLNlH8sItTqNBw.png"/></div></div></figure><p id="def2" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">尽管高斯朴素贝叶斯模型背后的耻辱是效率低下，因为它无法将数据集的要素视为相互依赖，而是将它们和独立且不相关的要素(因此是模型的朴素本质)考虑在内，但该模型在降维后的数据上似乎表现得非常好。该模型在预测真实的机场记录(. 973)方面表现非常好，并且在预测真实的公交车站记录(. 064 vs. .009)方面做得更好。该模型在正确分类真实列车记录方面表现更差(. 968 vs. .035)。与模型使用原始数据的性能不同，模型现在预测大多数值为机场。这个模型预测的原因可以归结为数据的不平衡。由于数据主要由机场记录组成，该模型可能倾向于预测更多的机场记录</p><p id="da24" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">决策树模型:</strong></p><pre class="kg kh ki kj gt nu nv nw nx aw ny bi"><span id="6d34" class="nz my iq nv b gy oa ob l oc od">dtree_model = DecisionTreeClassifier(max_depth = 3).fit(X_train, y_train)</span><span id="f61c" class="nz my iq nv b gy oe ob l oc od">dtree_predictions = dtree_model.predict(X_test)</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi os"><img src="../Images/3e52c5921f7e7595b393d4537ca8f0ef.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*twtVox-ofMlLdkbloSvrIg.png"/></div></div></figure><p id="6b86" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">使用LDA数据帧的决策树模型的性能比使用原始数据帧的模型稍差(分别为0.703和0.75)。该模型在准确预测真实的机场记录方面稍差(0.91比0.9)。该模型不能用任何一个数据帧预测任何真实的公交车站记录，并且在正确分类真实的火车站记录时表现明显更差(0.292对0.553)。该模型显示了将大多数记录预测为机场的趋势，这可能归因于数据的不平衡性质。</p><p id="d6e4" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir"> K近邻:</strong></p><pre class="kg kh ki kj gt nu nv nw nx aw ny bi"><span id="b204" class="nz my iq nv b gy oa ob l oc od">knn = KNeighborsClassifier(n_neighbors = 22).fit(X_train, y_train)</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oy"><img src="../Images/f6411ec3dae70c5a344b728f4a12560c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XHgqF-kw2StImLkZFo5FvA.png"/></div></div></figure><p id="371b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">K-最近邻分类器使用降维的数据帧比原始数据帧执行得更好(. 703对. 673)。与原始数据帧(. 927 vs. .937)相比，该模型在预测具有LDA数据帧的真实机场记录方面稍差，但是它在正确预测正确的汽车站记录方面做得同样好。该模型在预测真实火车站记录时表现稍差(0.173比0.180)。虽然使用LDA数据帧的模型的性能与原始数据帧相同或稍差，但准确性提高了，因为与原始数据帧的情况相比，错误预测记录的比率在LDA数据帧中分布得更好。</p><p id="2f33" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">从对LDA数据帧进行的分析中，我们开始看到一种模式。随着特征约简的出现，不围绕数据的互连性质的分类算法的整体准确性会降低。这可以从决策树算法中看出，该算法以一种简单明了的方式工作，其核心思想是根据特征的变化选择一个特定的类。当特征的数量从75个特征减少到2个特征时，模型的精度下降。但是，特征约简有助于完全基于概率的模型或无监督模型(如高斯朴素贝叶斯模型和K-最近邻算法)的准确性。让我们看看这种趋势是否会延续到最后一种特征约简方法中。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oz"><img src="../Images/c5c157347d190e9b057d1bb8278687dd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*G-u51RBGa42fOtF7iGgEzQ.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">由<a class="ae kv" href="https://unsplash.com/@marekpiwnicki?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Marek Piwnicki </a>在<a class="ae kv" href="https://unsplash.com/s/photos/split?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>拍摄的照片</p></figure><h1 id="2ae8" class="mx my iq bd mz na om nc nd ne on ng nh jw oo jx nj jz op ka nl kc oq kd nn no bi translated"><strong class="ak">使用奇异值分解减少特征</strong></h1><p id="4cf0" class="pw-post-body-paragraph kw kx iq ky b kz np jr lb lc nq ju le lf nr lh li lj ns ll lm ln nt lp lq lr ij bi translated">奇异值分解是另一种降维算法。下面是对SVD算法的深入研究:<a class="ae kv" href="https://medium.com/the-andela-way/foundations-of-machine-learning-singular-value-decomposition-svd-162ac796c27d" rel="noopener"> SVD文章</a></p><p id="856c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">SVD算法的工作原理是将特征矩阵分解为三个矩阵，这三个矩阵通过以下公式组合在一起:</p><p id="d90c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir"> A = U𝚺V^T </strong></p><p id="96d6" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">其中<strong class="ky ir"> A </strong>是维数为(m×n)的特征矩阵，<strong class="ky ir"> U </strong>是维数为(m×m)的正交矩阵，<strong class="ky ir"> V </strong>是维数为(n×n)的正交矩阵，<strong class="ky ir"> 𝚺 </strong>是大小为(m×n)的非负矩形对角矩阵</p><p id="2799" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir"> sklearn </strong>包中的<code class="fe og oh oi nv b">TruncatedSVD</code>函数很好地执行了这种转换</p><pre class="kg kh ki kj gt nu nv nw nx aw ny bi"><span id="b194" class="nz my iq nv b gy oa ob l oc od">from sklearn.decomposition import TruncatedSVD</span><span id="717c" class="nz my iq nv b gy oe ob l oc od">svd = TruncatedSVD(n_components=2, algorithm=’randomized’,</span><span id="178d" class="nz my iq nv b gy oe ob l oc od">random_state=0)</span><span id="1799" class="nz my iq nv b gy oe ob l oc od">X_svd = svd.fit_transform(transportation_df.drop([‘location_name’, ‘Class’], axis=1))</span><span id="f8be" class="nz my iq nv b gy oe ob l oc od">svd_df = pd.DataFrame(data = X_svd, columns = [‘PC 1’, ‘PC 2’])</span><span id="d978" class="nz my iq nv b gy oe ob l oc od">svd_df.head(3)</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi pa"><img src="../Images/0fedf9e6c1bd08c3fe921e2ff9637e7e.png" data-original-src="https://miro.medium.com/v2/resize:fit:374/format:webp/1*_NtMSqqcOv2g6R5Wr8bsjg.png"/></div></figure><p id="1cf5" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">将类列添加到SVD数据帧中</p><pre class="kg kh ki kj gt nu nv nw nx aw ny bi"><span id="54b6" class="nz my iq nv b gy oa ob l oc od">svd_df = pd.concat([svd_df, transportation_df[‘Class’]], axis = 1)</span><span id="e190" class="nz my iq nv b gy oe ob l oc od">svd_df.head(3)</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi pb"><img src="../Images/15061dc5d606a1154ca97366ecaa1a00.png" data-original-src="https://miro.medium.com/v2/resize:fit:464/format:webp/1*Y-F4MLqF3u9ys89ZHVI8nQ.png"/></div></figure><p id="ad88" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">绘制PC1和PC2的各种类的函数图。下图似乎类似于通过PCA分析生成的图表。这显示了两种算法的输出之间的相似性。让我们看看分类算法在准确性上是否与PCA分析相似</p><pre class="kg kh ki kj gt nu nv nw nx aw ny bi"><span id="cd7f" class="nz my iq nv b gy oa ob l oc od">PC_0 = svd_df.where(svd_df[‘Class’] == 0).dropna()[[‘PC 1’,’PC 2']]</span><span id="ae43" class="nz my iq nv b gy oe ob l oc od">PC_1 = svd_df.where(svd_df[‘Class’] == 1).dropna()[[‘PC 1’,’PC 2']]</span><span id="55af" class="nz my iq nv b gy oe ob l oc od">PC_2 = svd_df.where(svd_df[‘Class’] == 2).dropna()[[‘PC 1’,’PC 2']]</span><span id="051d" class="nz my iq nv b gy oe ob l oc od">plt.scatter(PC_0[‘PC 1’],PC_0[‘PC 2’])</span><span id="5250" class="nz my iq nv b gy oe ob l oc od">plt.scatter(PC_1[‘PC 1’],PC_1[‘PC 2’])</span><span id="858f" class="nz my iq nv b gy oe ob l oc od">plt.scatter(PC_2[‘PC 1’],PC_2[‘PC 2’])</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi pc"><img src="../Images/98fdf20b54293c3e22ca156ae5d184f9.png" data-original-src="https://miro.medium.com/v2/resize:fit:716/format:webp/1*XK22u50MHdS7RlOPx-2Uiw.png"/></div></figure><h1 id="4497" class="mx my iq bd mz na om nc nd ne on ng nh jw oo jx nj jz op ka nl kc oq kd nn no bi translated">使用奇异值分解的分类算法</h1><p id="2721" class="pw-post-body-paragraph kw kx iq ky b kz np jr lb lc nq ju le lf nr lh li lj ns ll lm ln nt lp lq lr ij bi translated">以下片段将展示如何使用高斯朴素贝叶斯、决策树和K-最近邻分类器来简化特征。</p><p id="6299" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">将数据分为训练数据和测试数据</p><pre class="kg kh ki kj gt nu nv nw nx aw ny bi"><span id="bf25" class="nz my iq nv b gy oa ob l oc od">x_cols = []</span><span id="e317" class="nz my iq nv b gy oe ob l oc od">for item in list(svd_df.columns):</span><span id="c043" class="nz my iq nv b gy oe ob l oc od">if(item != ‘Class’):</span><span id="68a6" class="nz my iq nv b gy oe ob l oc od">x_cols.append(item)</span><span id="b712" class="nz my iq nv b gy oe ob l oc od">X = svd_df[x_cols]</span><span id="bc8c" class="nz my iq nv b gy oe ob l oc od">y = svd_df[‘Class’]</span><span id="c7a9" class="nz my iq nv b gy oe ob l oc od">X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 0)</span></pre><p id="8754" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">高斯朴素贝叶斯分类器:</strong></p><pre class="kg kh ki kj gt nu nv nw nx aw ny bi"><span id="ffd0" class="nz my iq nv b gy oa ob l oc od">gnb = GaussianNB().fit(X_train, y_train)</span><span id="393e" class="nz my iq nv b gy oe ob l oc od">gnb_predictions = gnb.predict(X_test)</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi pd"><img src="../Images/d0bc33b3b96f73687a24bca9f0d4550b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5DOtIGg3ESQogKVHB4Inxg.png"/></div></div></figure><p id="312c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">高斯朴素贝叶斯的热图显示，SVD数据帧的总体精度略好于原始数据帧的总体精度(. 265 vs. .319)。总的来说，该模型在用数据帧(. 140对. 059)和(0对. 009)预测实际机场记录和汽车站记录方面表现不佳。在这两种情况下，大多数记录都被分类为火车站，并且在这两种情况下，模型都非常好地预测了实际的火车站记录(. 968 vs. .968)。与所有使用高斯朴素贝叶斯模型的情况一样，无论是否使用特征约简，该模型都倾向于将大多数记录预测为火车站。这也是因为模型的性质，以及它如何将每个功能视为单独的功能，而不是相互关联的功能。</p><p id="ee39" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">决策树分类器:</strong></p><pre class="kg kh ki kj gt nu nv nw nx aw ny bi"><span id="9c37" class="nz my iq nv b gy oa ob l oc od">dtree_model = DecisionTreeClassifier(max_depth = 3).fit(X_train, y_train)</span><span id="49f3" class="nz my iq nv b gy oe ob l oc od">dtree_predictions = dtree_model.predict(X_test)</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi pe"><img src="../Images/ff902e1bdf9583ef9311bddd739f2cbb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*A6fWdk8wZ3yOiKbt81wt-Q.png"/></div></div></figure><p id="c208" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">使用SVD数据帧的决策树模型比使用原始数据帧的模型稍差(67.8比75)。该模型预测真实机场记录的速度比使用原始数据时的模型(0.91比0.9)稍差。该模型在缺乏对真实公交车站记录的预测方面没有改变，但在预测真实列车记录方面比原始数据集差得多(. 533 vs. .292)。该模型显示了将大多数记录分类为机场的趋势，因为数据中存在不平衡，并且大多数记录具有机场的初始地面真实性</p><p id="f017" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">K-最近邻分类器</strong></p><pre class="kg kh ki kj gt nu nv nw nx aw ny bi"><span id="abfc" class="nz my iq nv b gy oa ob l oc od">knn = KNeighborsClassifier(n_neighbors = 22).fit(X_train, y_train)</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi pf"><img src="../Images/333a1404a54cae6ef9d22033e15b73a5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cSThaoNdqCJOsTtiCmRjrQ.png"/></div></div></figure><p id="0893" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">使用SVD数据帧的K-最近邻模型比使用原始数据的模型表现稍差(分别为0.67和0.679)。像原始模型一样，SVD数据框架模型在预测实际机场记录方面做得很好(. 927 vs. .93)。实际公共汽车和火车站记录的预测精度也类似于原始数据的精度(0.027对. 027)和(. 180对. 173)。与以前的模型一样，数据集中的不平衡和大量的机场记录导致该模型将大多数记录预测为机场。</p><p id="7e44" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">上述分析证实了我们以前的理论。基于概率决策和无监督模型(如高斯朴素贝叶斯和K-最近邻模型)的算法在处理特征减少的数据时比需要大量特征以获得更好性能的模型(如决策树模型)表现得更好。</p><h1 id="3a7e" class="mx my iq bd mz na om nc nd ne on ng nh jw oo jx nj jz op ka nl kc oq kd nn no bi translated">结论:</h1><p id="c698" class="pw-post-body-paragraph kw kx iq ky b kz np jr lb lc nq ju le lf nr lh li lj ns ll lm ln nt lp lq lr ij bi translated">从我们对三种不同形式的特征约简进行的分析以及这些特征约简的数据帧在三种不同的多类分类器上的应用中，我们看到一种特定形式的特征约简比其他两种效果好得多，这就是LDA变换。LDA算法有助于提高所使用的大多数分类器的准确性，事实上，使高斯朴素贝叶斯模型的准确性提高了近30%。因此，该模型在减少安全图数据的特征和提供最佳结果方面是最有效的。</p><p id="548e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">本<a class="ae kv" href="https://colab.research.google.com/drive/1A6ZZ0WZX3v4N5sN4pU5Yz_gxx2gThVqd?usp=sharing" rel="noopener ugc nofollow" target="_blank">笔记本</a>向我们介绍了基于安全图模式访问数据使用特征缩减和POI类别分类进行模型调整的方法。我们的算法在对机场、火车站和公交车站进行分类时取得了一些成功，所有这些都基于停留时间、游客离家的距离、一天中每个小时的受欢迎程度以及一周中每一天的受欢迎程度。</p><p id="cb8b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">该系列的下一个笔记本(链接即将推出！)将深入研究使用深度学习分类器来执行相同的分析，并看看神经网络是否能提高预测精度</p><p id="89d7" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir"> <em class="ls">提问？</em> </strong></p><p id="f002" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我邀请你在<a class="ae kv" href="https://www.safegraph.com/academics" rel="noopener ugc nofollow" target="_blank"> SafeGraph社区</a>的<strong class="ky ir"> #safegraphdata </strong>频道问他们，这是一个面向数据爱好者的免费Slack社区。获得支持、共享您的工作或与GIS社区中的其他人联系。通过SafeGraph社区，学者们可以免费访问美国、英国和加拿大700多万家企业的数据。</p></div></div>    
</body>
</html>