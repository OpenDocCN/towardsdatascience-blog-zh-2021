<html>
<head>
<title>Tokyo 2020 Tweets Topic Modelling with Machine Learning, using Python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">东京2020使用Python通过机器学习进行主题建模</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/tokyo-2020-tweets-topic-modelling-with-machine-learning-using-python-fd82b46b46bc?source=collection_archive---------26-----------------------#2021-08-09">https://towardsdatascience.com/tokyo-2020-tweets-topic-modelling-with-machine-learning-using-python-fd82b46b46bc?source=collection_archive---------26-----------------------#2021-08-09</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><figure class="ip iq gp gr ir is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi io"><img src="../Images/6ee0ee910a412f092b7373d87e1c0415.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4ObIDiWXEkSwZ7VhBsTxGQ.jpeg"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated"><a class="ae jd" href="https://unsplash.com/@mcghavan?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">阿莱士·史密斯</a>在<a class="ae jd" href="https://unsplash.com/s/photos/olympics?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍照</p></figure><div class=""/><div class=""><h2 id="0697" class="pw-subtitle-paragraph kd jf jg bd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku dk translated">以下是如何使用机器学习按照主题对东京2020推文进行分类。</h2></div><p id="69c5" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx jh">不出所料，两周以来，一切都与奥运有关。</strong></p><p id="3b03" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">许多伟大的故事被创造出来，许多明星诞生，许多不可思议的事件发生。而且，在社交媒体<strong class="kx jh">时代，“如果你没有发布，那就从来没有发生过”。因为这个原因，很多人在推特上谈论奥运，分享他们的感受和想法。</strong></p><p id="74ba" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我们的目标是<strong class="kx jh">为推文的主题建模。</strong>这意味着，给定一条tweet作为输入，我们想要自动理解给定tweet的主题。</p><p id="4cef" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">以下是一些带有一般句子的例子:</p><pre class="lr ls lt lu gt lv lw lx ly aw lz bi"><span id="ccf3" class="ma mb jg lw b gy mc md l me mf">INPUT SENTENCE:<br/>I'm so angry, so I think I'm going to buy a <strong class="lw jh">cheeseburger</strong></span><span id="4f8f" class="ma mb jg lw b gy mg md l me mf">TOPIC:<br/><strong class="lw jh">cheeseburger</strong></span><span id="c44d" class="ma mb jg lw b gy mg md l me mf">INPUT SENTENCE:<br/>This fast food makes the best <strong class="lw jh">cheeseburger </strong>of the entire city</span><span id="f99d" class="ma mb jg lw b gy mg md l me mf">TOPIC: <br/><strong class="lw jh">cheeseburger</strong></span><span id="f725" class="ma mb jg lw b gy mg md l me mf">INPUT SENTENCE: <br/>I really like <strong class="lw jh">Coke</strong>, so I think I'm going to buy a couple of bottles.</span><span id="e2a1" class="ma mb jg lw b gy mg md l me mf">TOPIC:<br/><strong class="lw jh">Coke</strong></span></pre><p id="cb4c" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">当然，如果你看第二句话，另一个话题可能是“<strong class="kx jh">快餐”</strong>或“<strong class="kx jh">城市”</strong>。事实上，我们更可能有一个主题列表，并筛选出最有影响力的主题。</p><p id="a999" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我们开始吧！</p><h2 id="0ab6" class="ma mb jg bd mh mi mj dn mk ml mm dp mn le mo mp mq li mr ms mt lm mu mv mw mx bi translated">0.图书馆</h2><p id="7df0" class="pw-post-body-paragraph kv kw jg kx b ky my kh la lb mz kk ld le na lg lh li nb lk ll lm nc lo lp lq ij bi translated">如题，我们将使用<strong class="kx jh"> Python </strong>。特别是，我们将使用这些库:</p><figure class="lr ls lt lu gt is"><div class="bz fp l di"><div class="nd ne l"/></div></figure><h2 id="e737" class="ma mb jg bd mh mi mj dn mk ml mm dp mn le mo mp mq li mr ms mt lm mu mv mw mx bi translated">1.想法是:</h2><p id="7ab6" class="pw-post-body-paragraph kv kw jg kx b ky my kh la lb mz kk ld le na lg lh li nb lk ll lm nc lo lp lq ij bi translated">NLP任务与其他任务(如表格数据分类)的不同之处在于<strong class="kx jh">您需要将单词/句子转换成向量。<br/> </strong>所以你要做的第一件事就是把你的推文转换，表示到一个N维空间。</p><p id="d36e" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">通常，N非常大，因此您可能希望通过使用合适的方法(例如PCA)来降低<strong class="kx jh">维度</strong>。</p><p id="9898" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">一旦完成，你需要使用一个<strong class="kx jh">聚类</strong>模型，以一种无人监管的方式对这些推文进行分类。</p><p id="a63f" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">因此，你将得到k个分类的向量。最后一步是使用原始推文，并查看每个类别最常用的单词。<br/>这个会给你每个班的<strong class="kx jh">题目</strong>。因此，您将拥有数据集中所有tweets的主题<strong class="kx jh">。</strong></p><figure class="lr ls lt lu gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nf"><img src="../Images/0cd3291d2d26dbf3aee3a0a9975b26ed.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iwfiEjQ0WU16e9uCwPt2Hg.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">我制作的图像</p></figure><p id="8cc7" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">下面详细解释一下:</p><h2 id="bc12" class="ma mb jg bd mh mi mj dn mk ml mm dp mn le mo mp mq li mr ms mt lm mu mv mw mx bi translated">2.数据集:</h2><p id="6039" class="pw-post-body-paragraph kv kw jg kx b ky my kh la lb mz kk ld le na lg lh li nb lk ll lm nc lo lp lq ij bi translated">我在这里找到了数据集<a class="ae jd" href="https://www.kaggle.com/gpreda/tokyo-olympics-2020-tweets" rel="noopener ugc nofollow" target="_blank"/>，用熊猫导入，出于计算原因只选择了5000条推文:</p><figure class="lr ls lt lu gt is"><div class="bz fp l di"><div class="nd ne l"/></div></figure><h2 id="ec36" class="ma mb jg bd mh mi mj dn mk ml mm dp mn le mo mp mq li mr ms mt lm mu mv mw mx bi translated">3.从文本到矢量:</h2><p id="46ad" class="pw-post-body-paragraph kv kw jg kx b ky my kh la lb mz kk ld le na lg lh li nb lk ll lm nc lo lp lq ij bi translated">用于文本预处理的一个很好的模型是<strong class="kx jh"> BERT </strong>。一篇精彩的文章展示了为什么以及如何使用BERT来达到这个目的，这篇文章是<a class="ae jd" rel="noopener" target="_blank" href="/topic-modeling-with-bert-779f7db187e6"/>。以下是如何将你的文本转换成矢量:</p><figure class="lr ls lt lu gt is"><div class="bz fp l di"><div class="nd ne l"/></div></figure><p id="961e" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">让我们使用PCA*来绘制我们的向量:</p><ol class=""><li id="1fa1" class="ng nh jg kx b ky kz lb lc le ni li nj lm nk lq nl nm nn no bi translated"><strong class="kx jh">二维主成分分析:</strong></li></ol><figure class="lr ls lt lu gt is"><div class="bz fp l di"><div class="nd ne l"/></div></figure><figure class="lr ls lt lu gt is"><div class="bz fp l di"><div class="nd ne l"/></div></figure><p id="add6" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx jh"> 2。3D主成分分析:</strong></p><figure class="lr ls lt lu gt is"><div class="bz fp l di"><div class="nd ne l"/></div></figure><p id="9655" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">为了清楚地理解我们可能想要使用PCA分解方法的多少特征，让我们使用<strong class="kx jh">解释的方差比:</strong></p><figure class="lr ls lt lu gt is"><div class="bz fp l di"><div class="nd ne l"/></div></figure><p id="f4ff" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">好的，所以<strong class="kx jh"> 5个组件就足够了，当然是</strong>(记住，在Python中，你从0开始计数)。</p><blockquote class="np nq nr"><p id="2184" class="kv kw ns kx b ky kz kh la lb lc kk ld nt lf lg lh nu lj lk ll nv ln lo lp lq ij bi translated">* PCA是一种用于<strong class="kx jh">根据每个轴包含的信息减少数据维数的方法。</strong>我们正在使用它，因为我们的向量有700多个维度，它可能会干扰我们的过程。</p></blockquote><h2 id="fbc0" class="ma mb jg bd mh mi mj dn mk ml mm dp mn le mo mp mq li mr ms mt lm mu mv mw mx bi translated">4.分类和主题建模:</h2><p id="7807" class="pw-post-body-paragraph kv kw jg kx b ky my kh la lb mz kk ld le na lg lh li nb lk ll lm nc lo lp lq ij bi translated">如果我们想要精确，我们不应该谈论<strong class="kx jh">分类，</strong>因为它是一种受监督的方法。相反，我们正在以一种无人监督的方式对我们的数据进行聚类，因为我们根本没有标签。<br/>我们将使用的非常简单的方法是<a class="ae jd" href="https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html" rel="noopener ugc nofollow" target="_blank"> <strong class="kx jh"> sklearn K-Means。</strong> </a> <strong class="kx jh"> </strong>没有什么比这更复杂的了:</p><pre class="lr ls lt lu gt lv lw lx ly aw lz bi"><span id="1aa1" class="ma mb jg lw b gy mc md l me mf">from sklearn.cluster import KMeans <br/>cluster = KMeans(n_clusters=20, random_state=0).fit(pca_data)</span></pre><p id="e973" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">那么我们就用<strong class="kx jh"> TF-IDF* </strong>来了解哪些词出现的次数最多(但排除比较<strong class="kx jh">太多的</strong>像“这个”，或者“那个”或者“安”)。</p><p id="68f2" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">完整的代码可以在这里找到:</p><figure class="lr ls lt lu gt is"><div class="bz fp l di"><div class="nd ne l"/></div></figure><blockquote class="np nq nr"><p id="6f0f" class="kv kw ns kx b ky kz kh la lb lc kk ld nt lf lg lh nu lj lk ll nv ln lo lp lq ij bi translated">* TF-IDF是一种以智能方式计算单词在一类文档中出现的次数的方法。聪明的地方在于，如果这个词也出现在所有其他类中，它可能不会影响(像“The”或“an”)，因此，它不会被认为是“频繁的”。</p></blockquote><h2 id="02c0" class="ma mb jg bd mh mi mj dn mk ml mm dp mn le mo mp mq li mr ms mt lm mu mv mw mx bi translated">5.最终结果:</h2><p id="899a" class="pw-post-body-paragraph kv kw jg kx b ky my kh la lb mz kk ld le na lg lh li nb lk ll lm nc lo lp lq ij bi translated">下面是我们获取<strong class="kx jh">主题的方法:</strong></p><ol class=""><li id="a00f" class="ng nh jg kx b ky kz lb lc le ni li nj lm nk lq nl nm nn no bi translated">按照<strong class="kx jh"> TF-IDF </strong>的降序排列每一类中的单词</li><li id="eacc" class="ng nh jg kx b ky nw lb nx le ny li nz lm oa lq nl nm nn no bi translated">仅在具有最高TF-IDF 的类中考虑每个单词<strong class="kx jh">(单词的TF-IDF值可以根据我们考虑的类而改变)</strong></li><li id="089a" class="ng nh jg kx b ky nw lb nx le ny li nz lm oa lq nl nm nn no bi translated">保持<strong class="kx jh">最高TF-IDF值的字为代表整个类的字。</strong></li></ol><p id="8049" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">这是代码:</p><figure class="lr ls lt lu gt is"><div class="bz fp l di"><div class="nd ne l"/></div></figure><p id="3a1e" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">当然，这种方法并不完美，但确实有些有趣的地方。</p><p id="012c" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">这13节课都是关于biles的。5类是关于<strong class="kx jh">美国的。</strong>几乎每个班级都有一个特定的主题。</p><h2 id="bb8f" class="ma mb jg bd mh mi mj dn mk ml mm dp mn le mo mp mq li mr ms mt lm mu mv mw mx bi translated">考虑因素:</h2><p id="b1bf" class="pw-post-body-paragraph kv kw jg kx b ky my kh la lb mz kk ld le na lg lh li nb lk ll lm nc lo lp lq ij bi translated">当体育赛事发生时，我们会感到很亲近。整个国家在比赛的最后一秒屏住呼吸，分享胜利的喜悦或失败的悲伤。</p><p id="c73c" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">如果你喜欢这篇文章，你想知道更多关于机器学习的知识，或者你只是想问我一些你可以问的问题:</p><p id="5c6b" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">A.在<a class="ae jd" href="https://www.linkedin.com/in/pieropaialunga/" rel="noopener ugc nofollow" target="_blank"> <strong class="kx jh"> Linkedin </strong> </a>上关注我，在那里我发布我所有的故事<br/> B .订阅我的<a class="ae jd" href="https://piero-paialunga.medium.com/subscribe" rel="noopener"> <strong class="kx jh">简讯</strong> </a>。这会让你了解新的故事，并给你机会发短信给我，让我收到你所有的更正或疑问。<br/> C .成为<a class="ae jd" href="https://piero-paialunga.medium.com/membership" rel="noopener"> <strong class="kx jh">推荐会员</strong> </a>，这样你就不会有任何“本月最大数量的故事”，你可以阅读我(以及成千上万其他机器学习和数据科学顶级作家)写的任何关于最新可用技术的文章。</p></div></div>    
</body>
</html>