<html>
<head>
<title>Four Deep Learning Papers to Read in July 2021</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">2021年7月要读的四篇深度学习论文</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/four-deep-learning-papers-to-read-in-july-2021-e91c546d112d?source=collection_archive---------6-----------------------#2021-06-30">https://towardsdatascience.com/four-deep-learning-papers-to-read-in-july-2021-e91c546d112d?source=collection_archive---------6-----------------------#2021-06-30</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="8048" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated"><strong class="ak">从大规模深度RL到对抗性鲁棒性，SimCLR-v2 &amp;学习神经网络空间</strong></h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/558adb3f34f1deb85abc94bdab4c99cf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UoceB56eWw1I3WxEi5k_Eg.png"/></div></div></figure><p id="d3d2" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">欢迎来到七月版的【T2:机器学习拼贴】系列，在这里我提供了不同深度学习研究流的概述。那么什么是ML拼贴呢？简单地说，我为我最近最喜欢的一篇论文起草了一张幻灯片的视觉摘要。每一周。在月底，所有由此产生的视觉拼贴都被收集在一个摘要博客帖子中。因此，我希望给你一个视觉和直观的深入了解一些最酷的趋势。所以，废话不多说:这里是我在2021年6月读过的四篇最喜欢的论文，以及为什么我认为它们对深度学习的未来很重要。</p><h2 id="dbef" class="lr ls it bd lt lu lv dn lw lx ly dp lz ld ma mb mc lh md me mf ll mg mh mi mj bi translated"><strong class="ak">‘什么才是对政策有深度的演员的批评方法？</strong>大规模研究</h2><p id="29f0" class="pw-post-body-paragraph ku kv it kw b kx mk ju kz la ml jx lc ld mm lf lg lh mn lj lk ll mo ln lo lp im bi translated"><em class="mp">作者:Andrychowicz等人(2021) </em>📝<a class="ae lq" href="https://openreview.net/forum?id=nIAxjsniDzg" rel="noopener ugc nofollow" target="_blank">纸</a> |🤖<a class="ae lq" href="https://github.com/google-research/seed_rl/blob/master/mujoco/what_matters_in_on_policy_rl.ipynb" rel="noopener ugc nofollow" target="_blank">代码</a></p><p id="065a" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><strong class="kw iu">一段话总结:</strong>政策上的深度强化学习代理是出了名的难调。结果的再现可能具有挑战性，并可能导致类似“深度RL还不起作用”的陈述。有一点是肯定的:深度RL训练循环不像训练有监督的MNIST CNN分类器那样防弹。那么，如何弥合这一差距呢？Andrychowicz等人(2021年)首次尝试对DRL超参数稳健性进行实证研究:他们训练了25万名代理人&amp;评估50多个基本选择的超参数。这项研究是使用MuJoCo控制环境的子集和谷歌的<a class="ae lq" href="https://github.com/google-research/seed_rl" rel="noopener ugc nofollow" target="_blank"> Seed RL </a>框架进行的，该框架为培训分布式演员-评论家代理提供了一个通用基础设施。作者比较了不同的政策损失目标，网络架构，优化，正则化和优势估计技术。他们的结果证实了许多常见的DRL智慧:例如，使用PPO裁剪目标、tanh激活和训练单独的演员-评论家网络。但它们也强调了一系列新的见解:策略的初始化似乎相当重要，建议确保策略分布和观察的零均值和独立性。此外，观察值的平均归一化显著提高了性能。就我个人而言，我非常喜欢这种大规模的研究，并相信它们提供了一种有价值的资源，可以节省相当多的时间和令人头疼的调优问题。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mq"><img src="../Images/b332365c6e13073eea37dd4431a117ca.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MhLNTlSs2p9qFE0peKZeXQ.png"/></div></div><p class="mr ms gj gh gi mt mu bd b be z dk translated">ML-Collage [21/52]:作者的数字。|📝<a class="ae lq" href="https://openreview.net/forum?id=nIAxjsniDzg" rel="noopener ugc nofollow" target="_blank">论文</a></p></figure><h2 id="6de3" class="lr ls it bd lt lu lv dn lw lx ly dp lz ld ma mb mc lh md me mf ll mg mh mi mj bi translated"><strong class="ak">‘模拟中枢神经系统前部的初级视觉皮层提高了对图像扰动的鲁棒性’</strong></h2><p id="516e" class="pw-post-body-paragraph ku kv it kw b kx mk ju kz la ml jx lc ld mm lf lg lh mn lj lk ll mo ln lo lp im bi translated"><em class="mp">作者:达佩洛和马克斯等人(2020) </em> |📝<a class="ae lq" href="https://biorxiv.org/content/10.1101/2020.06.16.154542v2" rel="noopener ugc nofollow" target="_blank">纸张</a> |🤖<a class="ae lq" href="https://github.com/dicarlolab/vonenet" rel="noopener ugc nofollow" target="_blank">代码</a></p><p id="698e" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><strong class="kw iu">一段话总结:</strong>如何让卷积神经网络不那么容易受到对抗性攻击？Dapello和Marques等人(2020)从我们的视觉皮层中获得灵感，并将V1的经典Gabor滤波器模型与标准计算机视觉架构相结合。更具体地说，他们提出将具有加性高斯噪声的固定权重生物约束滤波器组作为标准CNN的前端。他们称这个前端为VOne块，它与1乘1瓶颈一起为标准类ResNet CNN的第一卷积层提供了一个替代。作者表明，这种简单的前端块大大提高了对白盒攻击的鲁棒性，同时保持了有竞争力的ImageNet性能。他们执行了一组消融实验来解开滤波器组和随机性的贡献。他们的实验表明，类V1特征和随机性以非平凡的方式有益地相互作用:将两种成分结合起来的总改善超过了它们各自贡献的总和。作者推测，随机性不仅在推理时使攻击不那么有效，而且在训练时促进了健壮特征的学习。最后，作者表明，中枢神经网络的对抗性鲁棒性与其V1脑反应预测性相关。这篇论文提供了一个如何将深度学习与经过实验验证的自下而上的神经科学建模相结合的美丽例子。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mv"><img src="../Images/03a2e048b08f8e261e8394c7e553eee7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wEWUs4f6BrwcMiTKbcERrw.png"/></div></div><p class="mr ms gj gh gi mt mu bd b be z dk translated">ML-Collage [22/52]:作者的数字。|📝<a class="ae lq" href="https://biorxiv.org/content/10.1101/2020.06.16.154542v2" rel="noopener ugc nofollow" target="_blank">论文</a></p></figure><h2 id="f295" class="lr ls it bd lt lu lv dn lw lx ly dp lz ld ma mb mc lh md me mf ll mg mh mi mj bi translated"><strong class="ak">‘大的自我监督模型是强的半监督学习器’</strong></h2><p id="8c17" class="pw-post-body-paragraph ku kv it kw b kx mk ju kz la ml jx lc ld mm lf lg lh mn lj lk ll mo ln lo lp im bi translated"><em class="mp">作者:陈等(2020) </em> |📝<a class="ae lq" href="https://arxiv.org/abs/2006.10029" rel="noopener ugc nofollow" target="_blank">纸</a> |🤖<a class="ae lq" href="https://github.com/google-research/simclr" rel="noopener ugc nofollow" target="_blank">代码</a></p><p id="7ea3" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">一段话总结:这些年来，我们已经看到了自我监督预培训的革命。这包括大型自然语言模型，如GPT，以及对比预训练的计算机视觉模型，如<a class="ae lq" href="https://arxiv.org/abs/2002.05709" rel="noopener ugc nofollow" target="_blank"> SimCLR-v1 </a>和<a class="ae lq" href="https://arxiv.org/abs/1911.05722" rel="noopener ugc nofollow" target="_blank"> MoCo </a>。一个关键的研究问题是如何充分利用未标记数据？我们如何仅使用无监督的信息来提取核心表示，这对下游任务是有用的？SimCLR-v1使用图像不同的增强以及对比损失来最大化/最小化正/负图像对的代表性相似性。之后，使用少量标记数据对预训练的架构进行微调。因此，未标记的数据以一种纯任务不可知的方式被使用。在SimCLR-v2中，Chen等人(2021)建议再次使用未标记的数据:在对网络进行微调后，仅使用未标记的示例将其预测提取到不同的架构中。因此，SimCLR-v2结合了无监督的与任务无关的预训练，调整&amp;特定于任务的提取。作者认为，这种对未标记示例的特定任务使用允许大型神经网络更好地传输它们的一般知识。较大的模型在“小标签数据体制”中表现出色，并且它们能够胜过那些纯粹使用监督分类损失训练的对应模型。最后，他们还表明，更深的投影头和第一个投影头的微调可以提高最终性能。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mw"><img src="../Images/49fde4462de043ed388e1cd3bdcddf78.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hK5kwWKluV8avXqDC7gQww.png"/></div></div><p class="mr ms gj gh gi mt mu bd b be z dk translated">ML-Collage [23/52]:作者的数字。|📝<a class="ae lq" href="https://arxiv.org/abs/2006.10029" rel="noopener ugc nofollow" target="_blank">论文</a></p></figure><h2 id="1e1d" class="lr ls it bd lt lu lv dn lw lx ly dp lz ld ma mb mc lh md me mf ll mg mh mi mj bi translated"><strong class="ak">‘学习神经网络子空间’</strong></h2><p id="fe04" class="pw-post-body-paragraph ku kv it kw b kx mk ju kz la ml jx lc ld mm lf lg lh mn lj lk ll mo ln lo lp im bi translated"><em class="mp">作者:沃茨曼等人(2021) </em> |📝<a class="ae lq" href="http://arxiv.org/abs/2102.10472" rel="noopener ugc nofollow" target="_blank">纸张</a> |🤖<a class="ae lq" href="https://github.com/apple/learning-subspaces" rel="noopener ugc nofollow" target="_blank">代码</a></p><p id="557a" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><strong class="kw iu">一段话总结:</strong>还有什么比训练单个神经网络更好的呢？在一次运行中训练性能良好的网络的整个子空间。Wortsman等人(2021)介绍了一种新的训练范例，它使得在5个步骤中训练神经网络的线性(或非线性)组合成为可能:1)独立初始化<em class="mp"> m </em>神经网络。2)从<em class="mp"> m-1 </em>单工中取样一个点。3)基于先前采样的点计算m个网络的(可能是非线性的)组合。4)根据一批数据计算“连接”神经网络的损失。5)通过线性/非线性组合操作传播梯度，执行反向传播以更新m个网络。作者表明，这允许在ImageNet上训练表现良好的网络的整个线条、曲线和简化。此外，他们引入了正则化器，通过最小化m个网络端点之间的余弦相似性来促进功能多样性。直观上，子空间端点之间的距离由此被最大化。这有助于子空间中点，其对于标签噪声是鲁棒的并且被很好地校准。一个假设的原因可能是中点提供了一个不太尖锐的最小值，这已被证明可以更好地概括<a class="ae lq" href="https://arxiv.org/pdf/1703.11008.pdf" rel="noopener ugc nofollow" target="_blank"> (Dziugaite和Roy，2017) </a>。如果你对Frankle等人(2020)的<a class="ae lq" href="https://arxiv.org/abs/1912.05671" rel="noopener ugc nofollow" target="_blank">线性模式连接</a>的发现感兴趣，那么你会喜欢这篇文章。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mx"><img src="../Images/52cd0b7d4cd7b6a61159bb64f1f3e221.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OG7Tr_v6s26sgU4QI6wqCg.png"/></div></div><p class="mr ms gj gh gi mt mu bd b be z dk translated">ML-Collage [24/52]:作者的数字。|📝<a class="ae lq" href="http://arxiv.org/abs/2102.10472" rel="noopener ugc nofollow" target="_blank">论文</a></p></figure></div><div class="ab cl my mz hx na" role="separator"><span class="nb bw bk nc nd ne"/><span class="nb bw bk nc nd ne"/><span class="nb bw bk nc nd"/></div><div class="im in io ip iq"><p id="2e39" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">这是这个月的🤗让我知道你最喜欢的论文是什么。如果你想获得一些每周ML拼贴输入，查看Twitter上的标签<a class="ae lq" href="https://twitter.com/hashtag/mlcollage" rel="noopener ugc nofollow" target="_blank"># ML collage</a>。你也可以在最后的总结中找到拼贴画📖博客帖子:</p><div class="nf ng gp gr nh ni"><a rel="noopener follow" target="_blank" href="/four-deep-learning-papers-to-read-in-june-2021-5570cc5213bb"><div class="nj ab fo"><div class="nk ab nl cl cj nm"><h2 class="bd iu gy z fp nn fr fs no fu fw is bi translated">2021年6月要读的四篇深度学习论文</h2><div class="np l"><p class="bd b dl z fp nn fr fs no fu fw dk translated">towardsdatascience.com</p></div></div><div class="nq l"><div class="nr l ns nt nu nq nv ks ni"/></div></div></a></div></div></div>    
</body>
</html>