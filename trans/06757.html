<html>
<head>
<title>Three Popular Feature Selection Methods for Efficient Machine Learning in Python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Python中高效机器学习的三种流行特征选择方法</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/four-popular-feature-selection-methods-for-efficient-machine-learning-in-python-fdd34762efdb?source=collection_archive---------13-----------------------#2021-06-18">https://towardsdatascience.com/four-popular-feature-selection-methods-for-efficient-machine-learning-in-python-fdd34762efdb?source=collection_archive---------13-----------------------#2021-06-18</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/94342e2206b5fa8c6e1b6e205d5b425d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*83pO21dpOoNK11qv"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">叶夫根尼·切尔卡斯基在<a class="ae jg" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><div class=""/><div class=""><h2 id="b5d8" class="pw-subtitle-paragraph kg ji jj bd b kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx dk translated">使用真实数据集执行特征选择方法，并在每个方法后检索所选特征</h2></div><p id="9dad" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">特征选择是机器学习最重要的部分之一。在现实世界的大多数数据集中，可能有许多要素。但并不是所有的特征都是某个机器学习算法所必需的。使用太多不必要的功能可能会导致很多问题。第一个肯定是计算成本。不必要的大数据集将花费不必要的长时间来运行算法。同时，它可能导致完全没有预料到的过拟合问题。</p><p id="c97b" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">有几种特征选择方法。我将在这里用python演示四种流行的特征选择方法。如果我们不得不从头开始执行，这将是一个漫长而耗时的过程。但是幸运的是，python具有强大的功能，使得使用几行代码来执行特性选择变得非常容易。</p><p id="be33" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在开始特征选择方法之前，请随意下载用于本练习的数据集:</p><div class="is it gp gr iu lu"><a href="https://github.com/rashida048/Machine-Learning-BU/blob/main/Feature_Selection.csv" rel="noopener  ugc nofollow" target="_blank"><div class="lv ab fo"><div class="lw ab lx cl cj ly"><h2 class="bd jk gy z fp lz fr fs ma fu fw ji bi translated">rashida 048/机器学习-BU</h2><div class="mb l"><h3 class="bd b gy z fp lz fr fs ma fu fw dk translated">在GitHub上创建一个帐户，为rashida048/Machine-Learning-BU开发做贡献。</h3></div><div class="mc l"><p class="bd b dl z fp lz fr fs ma fu fw dk translated">github.com</p></div></div></div></a></div><p id="a402" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">让我们首先导入数据集:</p><pre class="md me mf mg gt mh mi mj mk aw ml bi"><span id="3f72" class="mm mn jj mi b gy mo mp l mq mr">import pandas as pd<br/>import numpy as np<br/>df = pd.read_csv('Feature_Selection.csv')</span></pre><p id="c74c" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">数据集太大。它有108列和11933行。因此，我不能在这里显示截图。以下是一些栏目:</p><pre class="md me mf mg gt mh mi mj mk aw ml bi"><span id="cd42" class="mm mn jj mi b gy mo mp l mq mr">df.columns</span></pre><p id="4338" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">输出:</p><pre class="md me mf mg gt mh mi mj mk aw ml bi"><span id="f94e" class="mm mn jj mi b gy mo mp l mq mr">Index(['x.aidtst3', 'employ1', 'income2', 'weight2', 'height3', 'children','veteran3', 'blind', 'renthom1', 'sex1',<br/>       ...<br/>'x.denvst3', 'x.prace1', 'x.mrace1', 'x.exteth3', 'x.asthms1', 'x.michd', 'x.ltasth1', 'x.casthm1', 'x.state', 'havarth3'], dtype='object', length=108)</span></pre><p id="59ce" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">假设我们想要使用机器学习算法来预测‘hawarth 3’变量。这是X和y。</p><pre class="md me mf mg gt mh mi mj mk aw ml bi"><span id="fe58" class="mm mn jj mi b gy mo mp l mq mr">X= df.drop(columns=["havarth3"])<br/>y= df['havarth3']</span></pre><p id="4579" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">现在，我们将使用不同的方法找出哪些特征是预测“havarth3”变量的最佳方法。</p><h2 id="23ba" class="mm mn jj bd ms mt mu dn mv mw mx dp my lh mz na nb ll nc nd ne lp nf ng nh ni bi translated"><a class="ae jg" href="https://scikit-learn.org/stable/modules/feature_selection.html#univariate-feature-selection" rel="noopener ugc nofollow" target="_blank"> <strong class="ak">单变量特征选择</strong> </a></h2><p id="80ec" class="pw-post-body-paragraph ky kz jj la b lb nj kk ld le nk kn lg lh nl lj lk ll nm ln lo lp nn lr ls lt im bi translated">该方法基于单变量统计测试选择最佳特征。将用于此目的的函数是sklearn库中的SelectKBest函数。此函数移除除了顶部指定数量的特征之外的所有特征。在该数据集中，有107个要素。k值10仅用于保留10个特征。选择f_classif的score_function，它使用方差分析表中的F值来选择特征。还有另外两个分类选项。分别是chi2和mutual_info_classif。请随时检查他们自己。</p><p id="d14a" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这是流程。</p><p id="995e" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">从sklearn库中导入方法。</p><pre class="md me mf mg gt mh mi mj mk aw ml bi"><span id="65bb" class="mm mn jj mi b gy mo mp l mq mr">from sklearn.feature_selection import SelectKBest<br/>from sklearn.feature_selection import f_classif</span></pre><p id="1107" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">传递前面提到的score_function 'f_classif '和您希望在' SelectKBest '函数中保留的特征数，并将X和y拟合到函数中:</p><pre class="md me mf mg gt mh mi mj mk aw ml bi"><span id="d5f4" class="mm mn jj mi b gy mo mp l mq mr">uni = SelectKBest(score_func = f_classif, k = 10)<br/>fit = uni.fit(X, y)</span></pre><p id="8152" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这是该函数选择的10个特征:</p><pre class="md me mf mg gt mh mi mj mk aw ml bi"><span id="5070" class="mm mn jj mi b gy mo mp l mq mr">X.columns[fit.get_support(indices=True)].tolist()</span></pre><p id="5ff0" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">输出:</p><pre class="md me mf mg gt mh mi mj mk aw ml bi"><span id="c2bc" class="mm mn jj mi b gy mo mp l mq mr">['employ1',<br/> 'rmvteth4',<br/> 'genhlth',<br/> 'x.age.g',<br/> 'x.age80',<br/> 'x.ageg5yr',<br/> 'x.age65yr',<br/> 'x.rfhlth',<br/> 'x.phys14d',<br/> 'x.hcvu651']</span></pre><p id="ed4d" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">特征选择完成了！</p><h2 id="2dcb" class="mm mn jj bd ms mt mu dn mv mw mx dp my lh mz na nb ll nc nd ne lp nf ng nh ni bi translated"><strong class="ak">使用相关矩阵的特征选择</strong></h2><p id="6771" class="pw-post-body-paragraph ky kz jj la b lb nj kk ld le nk kn lg lh nl lj lk ll nm ln lo lp nn lr ls lt im bi translated">该过程计算所有特征与目标特征的相关性。基于这些相关值，选择特征。对于这个项目，阈值选择为0.2。如果特征与目标的相关性超过0.2，则选择该特征用于分类。</p><pre class="md me mf mg gt mh mi mj mk aw ml bi"><span id="8f2d" class="mm mn jj mi b gy mo mp l mq mr">cor = df.corr()<br/>cor_target = abs(cor["havarth3"])<br/>relevant_features = cor_target[cor_target &gt; 0.2]<br/>relevant_features.index</span></pre><p id="4beb" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">输出:</p><pre class="md me mf mg gt mh mi mj mk aw ml bi"><span id="0c12" class="mm mn jj mi b gy mo mp l mq mr">Index(['employ1', 'genhlth', 'x.age.g', 'x.age80', 'x.ageg5yr', 'x.age65yr', 'x.hcvu651', 'havarth3'],<br/>      dtype='object')</span></pre><p id="eb4e" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">请注意,“havarth3”变量也被选中。因为‘hawarth 3’变量与自身的相关性最高。因此，请记住在执行机器学习算法之前删除它。</p><h2 id="88db" class="mm mn jj bd ms mt mu dn mv mw mx dp my lh mz na nb ll nc nd ne lp nf ng nh ni bi translated">包装方法</h2><p id="c73e" class="pw-post-body-paragraph ky kz jj la b lb nj kk ld le nk kn lg lh nl lj lk ll nm ln lo lp nn lr ls lt im bi translated">这是一个有趣的方法。在这种方法中，使用一种机器学习方法来找到正确的特征。此方法使用p值。这里我将使用statsmodels库中的普通线性模型。我选择statsmodels库，因为它提供p值作为模型的一部分，而且我发现它很容易使用。</p><pre class="md me mf mg gt mh mi mj mk aw ml bi"><span id="095d" class="mm mn jj mi b gy mo mp l mq mr">import statsmodels.api as sm<br/>X_new = sm.add_constant(X)<br/>model = sm.OLS(y, X_new).fit()<br/>model.pvalues</span></pre><p id="0f93" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">输出:</p><pre class="md me mf mg gt mh mi mj mk aw ml bi"><span id="9d0a" class="mm mn jj mi b gy mo mp l mq mr">const        2.132756e-01<br/>x.aidtst3    6.269686e-01<br/>employ1      4.025786e-20<br/>income2      3.931291e-04<br/>weight2      2.122768e-01<br/>                 ...     <br/>x.asthms1    3.445036e-01<br/>x.michd      3.478433e-01<br/>x.ltasth1    3.081917e-03<br/>x.casthm1    9.802652e-01<br/>x.state      6.724318e-01<br/>Length: 108, dtype: float64</span></pre><p id="3275" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">看所有的p值。现在，基于p值，我们将逐个移除特征。我们将继续运行statsmodels库中的机器学习算法，在每次迭代中，我们将找到具有最高p值的特征。如果最高p值大于0.05，我们将移除该特征。将进行相同的过程，直到我们达到最高p值不再大于0.05的点。</p><pre class="md me mf mg gt mh mi mj mk aw ml bi"><span id="61f4" class="mm mn jj mi b gy mo mp l mq mr">selected_features = list(X.columns)<br/>pmax = 1<br/>while (len(selected_features)&gt;0):<br/>    p= []<br/>    X_new = X[selected_features]<br/>    X_new = sm.add_constant(X_new)<br/>    model = sm.OLS(y,X_new).fit()<br/>    p = pd.Series(model.pvalues.values[1:],index = selected_features)      <br/>    pmax = max(p)<br/>    feature_pmax = p.idxmax()<br/>    if(pmax&gt;0.05):<br/>        selected_features.remove(feature_pmax)<br/>    else:<br/>        break<br/>selected_features</span></pre><p id="1f5b" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">输出:</p><pre class="md me mf mg gt mh mi mj mk aw ml bi"><span id="286a" class="mm mn jj mi b gy mo mp l mq mr">['employ1',<br/> 'income2',<br/> 'blind',<br/> 'sex1',<br/> 'pneuvac4',<br/> 'diffwalk',<br/> 'diffdres',<br/> 'smoke100',<br/> 'rmvteth4',<br/> 'physhlth',<br/> 'menthlth',<br/> 'hlthpln1',<br/> 'genhlth',<br/> 'persdoc2',<br/> 'checkup1',<br/> 'addepev2',<br/> 'chcscncr',<br/> 'asthma3',<br/> 'qstlang',<br/> 'x.metstat',<br/> 'htin4',<br/> 'wtkg3',<br/> 'x.age.g',<br/> 'x.ageg5yr',<br/> 'x.age65yr',<br/> 'x.chldcnt',<br/> 'x.incomg',<br/> 'x.rfseat3',<br/> 'x.rfsmok3',<br/> 'x.urbstat',<br/> 'x.llcpwt2',<br/> 'x.rfhlth',<br/> 'x.imprace',<br/> 'x.wt2rake',<br/> 'x.strwt',<br/> 'x.phys14d',<br/> 'x.hcvu651',<br/> 'x.denvst3',<br/> 'x.prace1',<br/> 'x.mrace1',<br/> 'x.ltasth1']</span></pre><p id="02b0" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">所以，这些是选择的特征。</p><h2 id="7467" class="mm mn jj bd ms mt mu dn mv mw mx dp my lh mz na nb ll nc nd ne lp nf ng nh ni bi translated">结论</h2><p id="d9a5" class="pw-post-body-paragraph ky kz jj la b lb nj kk ld le nk kn lg lh nl lj lk ll nm ln lo lp nn lr ls lt im bi translated">我在我的一个项目中使用了所有四种方法，使用包装器方法得到了最好的结果。但并非所有项目都是如此。当您有很多像这个项目这样的特性时，尝试至少几种特性选择方法是一个好主意。不同的功能选择可能会给你完全不同的功能集。在这种情况下，使用另一种方法来确认是一个很好的解决方案。在机器学习中，收集正确的特征是成功的一半。</p><h2 id="c66c" class="mm mn jj bd ms mt mu dn mv mw mx dp my lh mz na nb ll nc nd ne lp nf ng nh ni bi translated">更多阅读:</h2><div class="is it gp gr iu lu"><a rel="noopener follow" target="_blank" href="/stochastic-gradient-descent-explanation-and-complete-implementation-from-scratch-a2c6a02f28bd"><div class="lv ab fo"><div class="lw ab lx cl cj ly"><h2 class="bd jk gy z fp lz fr fs ma fu fw ji bi translated">随机梯度下降:从头开始的解释和完整实现</h2><div class="mb l"><h3 class="bd b gy z fp lz fr fs ma fu fw dk translated">使用单个感知器</h3></div><div class="mc l"><p class="bd b dl z fp lz fr fs ma fu fw dk translated">towardsdatascience.com</p></div></div><div class="no l"><div class="np l nq nr ns no nt ja lu"/></div></div></a></div><div class="is it gp gr iu lu"><a rel="noopener follow" target="_blank" href="/exploratory-data-analysis-of-text-data-including-visualization-and-sentiment-analysis-e46dda3dd260"><div class="lv ab fo"><div class="lw ab lx cl cj ly"><h2 class="bd jk gy z fp lz fr fs ma fu fw ji bi translated">文本数据的探索性数据分析，包括可视化和情感分析</h2><div class="mc l"><p class="bd b dl z fp lz fr fs ma fu fw dk translated">towardsdatascience.com</p></div></div><div class="no l"><div class="nu l nq nr ns no nt ja lu"/></div></div></a></div><div class="is it gp gr iu lu"><a rel="noopener follow" target="_blank" href="/an-ultimate-cheat-sheet-for-numpy-bb1112b0488f"><div class="lv ab fo"><div class="lw ab lx cl cj ly"><h2 class="bd jk gy z fp lz fr fs ma fu fw ji bi translated">Numpy的终极备忘单</h2><div class="mb l"><h3 class="bd b gy z fp lz fr fs ma fu fw dk translated">日常工作所需的所有Numpy功能</h3></div><div class="mc l"><p class="bd b dl z fp lz fr fs ma fu fw dk translated">towardsdatascience.com</p></div></div><div class="no l"><div class="nv l nq nr ns no nt ja lu"/></div></div></a></div><div class="is it gp gr iu lu"><a rel="noopener follow" target="_blank" href="/an-ultimate-data-visualization-course-in-python-for-free-12a5da0a517b"><div class="lv ab fo"><div class="lw ab lx cl cj ly"><h2 class="bd jk gy z fp lz fr fs ma fu fw ji bi translated">免费的Python终极数据可视化课程</h2><div class="mb l"><h3 class="bd b gy z fp lz fr fs ma fu fw dk translated">学习比任何付费课程更多的可视化功能和技术</h3></div><div class="mc l"><p class="bd b dl z fp lz fr fs ma fu fw dk translated">towardsdatascience.com</p></div></div><div class="no l"><div class="nw l nq nr ns no nt ja lu"/></div></div></a></div><div class="is it gp gr iu lu"><a rel="noopener follow" target="_blank" href="/a-collection-of-advanced-data-visualization-in-matplotlib-and-seaborn-f08136172e14"><div class="lv ab fo"><div class="lw ab lx cl cj ly"><h2 class="bd jk gy z fp lz fr fs ma fu fw ji bi translated">Matplotlib和Seaborn中的高级数据可视化集合</h2><div class="mb l"><h3 class="bd b gy z fp lz fr fs ma fu fw dk translated">让你的故事更有趣</h3></div><div class="mc l"><p class="bd b dl z fp lz fr fs ma fu fw dk translated">towardsdatascience.com</p></div></div><div class="no l"><div class="nx l nq nr ns no nt ja lu"/></div></div></a></div></div></div>    
</body>
</html>