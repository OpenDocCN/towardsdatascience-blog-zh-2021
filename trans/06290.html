<html>
<head>
<title>Scene Text Detection And Recognition Using EAST And Tesseract</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">基于EAST和Tesseract的场景文本检测与识别</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/scene-text-detection-and-recognition-using-east-and-tesseract-6f07c249f5de?source=collection_archive---------7-----------------------#2021-06-06">https://towardsdatascience.com/scene-text-detection-and-recognition-using-east-and-tesseract-6f07c249f5de?source=collection_archive---------7-----------------------#2021-06-06</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="90fa" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsdatascience.com/tagged/hands-on-tutorials" rel="noopener" target="_blank">实践教程</a></h2><div class=""/><div class=""><h2 id="e388" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">使用EAST和Tesseract算法检测和识别给定自然场景图像的文本。</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/bfdb58ef7435fbfa61fad445fe0acb87.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fNhL1LJHXM7ViloEoi1CXA.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">图片由Paritosh Mahto提供</p></figure><pre class="ks kt ku kv gt lh li lj lk aw ll bi"><span id="a0e6" class="lm ln it li b gy lo lp l lq lr"><strong class="li jd">This Article Includes:<br/>1.Introduction<br/>2.Real World Problem<br/>    2.1 Description<br/>    2.2 Problem Statement<br/>    2.3 Bussiness Objectives and Constraints<br/>3.Datasets Available for Text Detection And Recognition<br/>    3.1 Dataset Overview &amp; Description<br/>4.Exploratory Data Analysis(EDA)<br/>5.Methods of text detection before deep learning era<br/>6.EAST (Efficient Accurate Scene Text Detector)<br/>7.Model Implementation<br/>8.Model Analysis &amp; Model Quantization<br/>9.Deployment<br/>10.Future Work<br/>11.Reference</strong></span></pre><h2 id="8318" class="lm ln it bd ls lt lu dn lv lw lx dp ly lz ma mb mc md me mf mg mh mi mj mk iz bi translated">1.介绍</h2><p id="102a" class="pw-post-body-paragraph ml mm it mn b mo mp kd mq mr ms kg mt lz mu mv mw md mx my mz mh na nb nc nd im bi translated">在这个数字化时代，从不同来源提取文本信息的需求在很大程度上已经上升。幸运的是，计算机视觉的最新进展使我们能够在减轻文本检测和其他文档分析和理解的负担方面取得长足进步。在计算机视觉中，将图像或扫描文档中的文本转换为机器可读格式的方法被称为<strong class="mn jd">光学字符识别(OCR) </strong>，这种格式可以在以后编辑、搜索并用于进一步处理。</p><p id="970e" class="pw-post-body-paragraph ml mm it mn b mo ne kd mq mr nf kg mt lz ng mv mw md nh my mz mh ni nb nc nd im bi translated"><strong class="mn jd">OCR的应用</strong></p><p id="1c93" class="pw-post-body-paragraph ml mm it mn b mo ne kd mq mr nf kg mt lz ng mv mw md nh my mz mh ni nb nc nd im bi translated"><strong class="mn jd">答:信息检索和自动数据输入</strong> - OCR在许多公司和机构中扮演着非常重要的角色，这些公司和机构需要处理、分析和转换成千上万的文档来执行日常操作。</p><p id="d790" class="pw-post-body-paragraph ml mm it mn b mo ne kd mq mr nf kg mt lz ng mv mw md nh my mz mh ni nb nc nd im bi translated">例如，在银行信息中，如账户详细信息，使用OCR可以很容易地从支票中提取金额。同样，在机场检查护照时，也可以使用OCR提取信息。其他例子是使用OCR从收据、发票、表格、报表、合同等中检索信息。</p><p id="5be0" class="pw-post-body-paragraph ml mm it mn b mo ne kd mq mr nf kg mt lz ng mv mw md nh my mz mh ni nb nc nd im bi translated"><strong class="mn jd"> b .车辆牌照识别</strong> - OCR也可用于识别车辆牌照，然后用于车辆跟踪、收费等。</p><p id="3d4f" class="pw-post-body-paragraph ml mm it mn b mo ne kd mq mr nf kg mt lz ng mv mw md nh my mz mh ni nb nc nd im bi translated"><strong class="mn jd"> c .自动驾驶汽车</strong>——OCR也可以用来为自动驾驶汽车建立模型。它可以帮助识别交通标志。如果没有这一点，自动驾驶汽车将对行人和道路上的其他车辆构成风险。</p><p id="7bdb" class="pw-post-body-paragraph ml mm it mn b mo ne kd mq mr nf kg mt lz ng mv mw md nh my mz mh ni nb nc nd im bi translated">在本文中，我们将讨论和实现OCR中使用的深度学习算法。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/d78af48ae04035209285e21fc9065be2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bITgfvkewAFxcAKdvbTUSg.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">图片由Paritosh Mahto提供</p></figure><blockquote class="nj nk nl"><p id="baa4" class="ml mm nm mn b mo ne kd mq mr nf kg mt nn ng mv mw no nh my mz np ni nb nc nd im bi translated"><strong class="mn jd">数字化</strong>-将文本、图片或声音转换成计算机可以处理的数字形式</p></blockquote><h2 id="a5a2" class="lm ln it bd ls lt lu dn lv lw lx dp ly lz ma mb mc md me mf mg mh mi mj mk iz bi translated">2.现实世界的问题</h2><h2 id="a354" class="lm ln it bd ls lt lu dn lv lw lx dp ly lz ma mb mc md me mf mg mh mi mj mk iz bi translated">2.1描述</h2><p id="47ea" class="pw-post-body-paragraph ml mm it mn b mo mp kd mq mr ms kg mt lz mu mv mw md mx my mz mh na nb nc nd im bi translated">因为我们现在熟悉文本检测和识别的各种应用。本文将讨论从<strong class="mn jd">自然场景</strong>图像中检测和识别文本。</p><p id="2a50" class="pw-post-body-paragraph ml mm it mn b mo ne kd mq mr nf kg mt lz ng mv mw md nh my mz mh ni nb nc nd im bi translated">因此，在我们的例子中，我们使用任何自然图像或场景(不特别是文档、牌照或车号),对于给定的图像/场景，我们希望通过边界框定位图像中的字符/单词/句子。之后，我们要识别任何语言的本地化文本。一般工作流程图如下所示:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/4cd09a48f2dff7782a817346e36a656d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*svhWMWuDh_CJUe8TVXksYQ.jpeg"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">图片由Paritosh Mahto提供</p></figure><blockquote class="nj nk nl"><p id="6432" class="ml mm nm mn b mo ne kd mq mr nf kg mt nn ng mv mw no nh my mz np ni nb nc nd im bi translated">上面使用的图像是为了显示整个任务。但是对于这个案例研究，我们将使用一个随机的自然场景作为输入图像。</p></blockquote><h2 id="3d07" class="lm ln it bd ls lt lu dn lv lw lx dp ly lz ma mb mc md me mf mg mh mi mj mk iz bi translated">2.2问题陈述</h2><p id="db23" class="pw-post-body-paragraph ml mm it mn b mo mp kd mq mr ms kg mt lz mu mv mw md mx my mz mh na nb nc nd im bi translated">对于给定的<strong class="mn jd">自然场景/图像</strong>,目标是通过绘制边界框来检测文本区域，之后，必须识别检测到的文本。</p><h2 id="cd0d" class="lm ln it bd ls lt lu dn lv lw lx dp ly lz ma mb mc md me mf mg mh mi mj mk iz bi translated">2.3业务目标和限制。</h2><ul class=""><li id="2725" class="nq nr it mn b mo mp mr ms lz ns md nt mh nu nd nv nw nx ny bi translated">自然场景图像中的文本可以是不同的语言、颜色、字体、大小、方向和形状。我们必须处理自然场景图像中的这些文本，这些自然场景图像表现出较高的多样性和可变性。</li><li id="95a9" class="nq nr it mn b mo nz mr oa lz ob md oc mh od nd nv nw nx ny bi translated">自然场景可能具有图案背景或形状与任何文本极其相似的物体，这在检测文本时会产生问题。</li><li id="ae91" class="nq nr it mn b mo nz mr oa lz ob md oc mh od nd nv nw nx ny bi translated">图像混乱(低质量/分辨率/多方向)</li><li id="926d" class="nq nr it mn b mo nz mr oa lz ob md oc mh od nd nv nw nx ny bi translated">实时检测、识别和翻译图像中的文本需要低延迟。</li></ul><h2 id="c6b0" class="lm ln it bd ls lt lu dn lv lw lx dp ly lz ma mb mc md me mf mg mh mi mj mk iz bi translated">3.可用于文本检测和识别的数据集</h2><p id="4661" class="pw-post-body-paragraph ml mm it mn b mo mp kd mq mr ms kg mt lz mu mv mw md mx my mz mh na nb nc nd im bi translated">有许多公开可用的数据集可用于此任务，下面列出了不同数据集的发行年份、图像编号、文本方向、语言和重要特征。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi oe"><img src="../Images/e99b7ef2e9fa7a6452728e2136b17745.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*ln9ezhIftDlhhR7C"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated"><em class="of">图像来源——纸张(场景文字检测与识别)</em></p></figure><p id="7d5d" class="pw-post-body-paragraph ml mm it mn b mo ne kd mq mr nf kg mt lz ng mv mw md nh my mz mh ni nb nc nd im bi translated">由于非结构化的文本、不同的方向等，所有的数据集可能无法很好地用于所有的深度学习模型。对于这项任务，我选择ICDAR 2015数据，因为它很容易获得足够数量的非商业用途的图像，这些图像中的文本是英文的，因为我是初学者，我想专注于理解解决这项任务的算法的工作。此外，图像很小，在这个数据集中有多向模糊，因此我可以用检测部分做更多的实验。</p><h2 id="4da3" class="lm ln it bd ls lt lu dn lv lw lx dp ly lz ma mb mc md me mf mg mh mi mj mk iz bi translated"><strong class="ak"> 3.1数据集概述&amp;描述</strong></h2><p id="c79f" class="pw-post-body-paragraph ml mm it mn b mo mp kd mq mr ms kg mt lz mu mv mw md mx my mz mh na nb nc nd im bi translated"><strong class="mn jd">数据来源- </strong> <a class="ae og" href="https://rrc.cvc.uab.es/?ch=4&amp;com=downloads" rel="noopener ugc nofollow" target="_blank">下载-附带场景文字-健壮阅读比赛(uab.es) </a></p><p id="74d5" class="pw-post-body-paragraph ml mm it mn b mo ne kd mq mr nf kg mt lz ng mv mw md nh my mz mh ni nb nc nd im bi translated">ICDAR-2015由国际会议文档分析与识别提供</p><p id="6a02" class="pw-post-body-paragraph ml mm it mn b mo ne kd mq mr nf kg mt lz ng mv mw md nh my mz mh ni nb nc nd im bi translated">名为“稳健阅读竞赛”的竞赛是名为“偶发场景文本-2015”的挑战之一，该数据集就是为该挑战提供的。</p><p id="63cc" class="pw-post-body-paragraph ml mm it mn b mo ne kd mq mr nf kg mt lz ng mv mw md nh my mz mh ni nb nc nd im bi translated"><strong class="mn jd">描述:</strong></p><ul class=""><li id="2aec" class="nq nr it mn b mo ne mr nf lz oh md oi mh oj nd nv nw nx ny bi translated">数据集在训练和测试集中可用，每个集合都有基本事实信息。它总共包含1，500幅图像，其中1，000幅用于训练，500幅用于测试。它还包含2，077个裁剪文本实例，包括200多个不规则文本样本。</li><li id="6e5e" class="nq nr it mn b mo nz mr oa lz ob md oc mh od nd nv nw nx ny bi translated">这些图像是从可佩戴的相机中获得的。</li></ul><h2 id="07f3" class="lm ln it bd ls lt lu dn lv lw lx dp ly lz ma mb mc md me mf mg mh mi mj mk iz bi translated">4.探索性数据分析</h2><ul class=""><li id="0307" class="nq nr it mn b mo mp mr ms lz ns md nt mh nu nd nv nw nx ny bi translated">下载数据后，所有文件的结构如下-</li></ul><pre class="ks kt ku kv gt lh li lj lk aw ll bi"><span id="de61" class="lm ln it li b gy lo lp l lq lr"><strong class="li jd">Data(main directory)<br/> |<br/> |<br/> |----- ICDAR2015<br/>           |<br/>           |<br/>           |-----train ( </strong>containing all image files<strong class="li jd"> )<br/>           |<br/>           |<br/>           |------train_gt ( </strong>containing texts and coordinates <strong class="li jd">)<br/>           |<br/>           |<br/>           |------test ( </strong>containing all image files<strong class="li jd"> )<br/>           |<br/>           |<br/>           |-----test ( </strong>containing texts and coordinates <strong class="li jd">)</strong></span></pre><ul class=""><li id="33ca" class="nq nr it mn b mo ne mr nf lz oh md oi mh oj nd nv nw nx ny bi translated">使用以下代码，可以观察到图像尺寸、通道数量等其他信息</li></ul><p id="aa4f" class="pw-post-body-paragraph ml mm it mn b mo ne kd mq mr nf kg mt lz ng mv mw md nh my mz mh ni nb nc nd im bi translated">对于训练图像</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="ok ol l"/></div></figure><p id="3381" class="pw-post-body-paragraph ml mm it mn b mo ne kd mq mr nf kg mt lz ng mv mw md nh my mz mh ni nb nc nd im bi translated">对于测试图像</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="ok ol l"/></div></figure><ul class=""><li id="cfca" class="nq nr it mn b mo ne mr nf lz oh md oi mh oj nd nv nw nx ny bi translated">我们还可以从条形图中得出结论，所有图像的高度和宽度都相同，即720和1280。</li></ul><p id="c27e" class="pw-post-body-paragraph ml mm it mn b mo ne kd mq mr nf kg mt lz ng mv mw md nh my mz mh ni nb nc nd im bi translated">对于训练图像</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi om"><img src="../Images/632945a69e1918e74432113ca147e7ca.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*8pgXFwxEyZNi0BhM"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">图片由Paritosh Mahto提供</p></figure><p id="58fa" class="pw-post-body-paragraph ml mm it mn b mo ne kd mq mr nf kg mt lz ng mv mw md nh my mz mh ni nb nc nd im bi translated">对于测试图像</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi om"><img src="../Images/a64f6096953347fc3389b10bdf47aaf5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*TXO0vCTROL2If5Al"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">图片由Paritosh Mahto提供</p></figure><ul class=""><li id="cc4b" class="nq nr it mn b mo ne mr nf lz oh md oi mh oj nd nv nw nx ny bi translated"><strong class="mn jd">在地面真实信息的帮助下绘制原始图像和具有边界框的图像</strong></li><li id="adfd" class="nq nr it mn b mo nz mr oa lz ob md oc mh od nd nv nw nx ny bi translated">对于训练图像</li></ul><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi on"><img src="../Images/d624355167e57efe2d9ce43100aa1010.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xJJpucv3oa4ZUR6N3rYf3A.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">图片来源ICDAR2015</p></figure><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi on"><img src="../Images/6fd49f4a12a5bd1e49fd5e89d0aa0ebf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mXOAx1taN7TO6HPLH4OSHA.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">图片来源ICDAR2015</p></figure><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi on"><img src="../Images/3dad9b52595deaafd3facb3da26bd70a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-ICfNEKiEAYKogBxf3-I-Q.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">图片来源ICDAR2015</p></figure><ul class=""><li id="cb26" class="nq nr it mn b mo ne mr nf lz oh md oi mh oj nd nv nw nx ny bi translated">对于测试图像</li></ul><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi on"><img src="../Images/72c93b2d60a021df6407368b6c97cfc7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YePxcxZcWyyOqnNAwMNk5A.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">图片来源ICDAR2015</p></figure><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi on"><img src="../Images/a8a898bb39ad90122a27b1e78cad2f62.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uaWcF0NqX5C8LH_DvGcETQ.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">图片来源ICDAR2015</p></figure><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi on"><img src="../Images/cf106eeed7b00e36e93a3e435018f5fe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OjtotzDNWalW5K553YiiKg.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">图片来源ICDAR2015</p></figure><p id="eb8f" class="pw-post-body-paragraph ml mm it mn b mo ne kd mq mr nf kg mt lz ng mv mw md nh my mz mh ni nb nc nd im bi translated"><strong class="mn jd">从EDA得出的结论</strong></p><ul class=""><li id="bb00" class="nq nr it mn b mo ne mr nf lz oh md oi mh oj nd nv nw nx ny bi translated">在ICDAR-15数据集中，所有图像都具有相似的大小(720x1280)和扩展名(。jpg)。</li><li id="a6e2" class="nq nr it mn b mo nz mr oa lz ob md oc mh od nd nv nw nx ny bi translated">训练集有1000幅图像，而测试集中有500幅图像。</li><li id="244f" class="nq nr it mn b mo nz mr oa lz ob md oc mh od nd nv nw nx ny bi translated">所有图像的高度和宽度是相同的，所以我们不需要平均高度和平均宽度。</li><li id="56b2" class="nq nr it mn b mo nz mr oa lz ob md oc mh od nd nv nw nx ny bi translated">在大多数图像中，所有文本都在小区域中，并且图像是模糊的。</li><li id="bdcb" class="nq nr it mn b mo nz mr oa lz ob md oc mh od nd nv nw nx ny bi translated">所有文本均为英语，少数文本也不可用，并*替换为“###”。</li><li id="b7d6" class="nq nr it mn b mo nz mr oa lz ob md oc mh od nd nv nw nx ny bi translated">大多数文本是单个单词，而不是单词和句子，而且单词是多向的。我们必须建立一个模型来预测这些模糊的文本。</li></ul><h2 id="feba" class="lm ln it bd ls lt lu dn lv lw lx dp ly lz ma mb mc md me mf mg mh mi mj mk iz bi translated">5.深度学习时代之前的文本检测方法</h2><p id="289c" class="pw-post-body-paragraph ml mm it mn b mo mp kd mq mr ms kg mt lz mu mv mw md mx my mz mh na nb nc nd im bi translated">正如问题陈述中提到的，我们必须首先定位图像中的文本，即首先检测文本，然后识别检测到的文本。现在对于检测，我们将尝试在深度学习时代之前用于文本检测的几种方法。</p><p id="10ac" class="pw-post-body-paragraph ml mm it mn b mo ne kd mq mr nf kg mt lz ng mv mw md nh my mz mh ni nb nc nd im bi translated"><strong class="mn jd">MSER(最大稳定极值区域)</strong></p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="ok ol l"/></div></figure><p id="e33c" class="pw-post-body-paragraph ml mm it mn b mo ne kd mq mr nf kg mt lz ng mv mw md nh my mz mh ni nb nc nd im bi translated"><strong class="mn jd">SWT(笔画宽度变换)</strong></p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="ok ol l"/></div></figure><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="ok ol l"/></div></figure><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="ok ol l"/></div></figure><p id="ca2b" class="pw-post-body-paragraph ml mm it mn b mo ne kd mq mr nf kg mt lz ng mv mw md nh my mz mh ni nb nc nd im bi translated">两种方法的所有输出都不是很清楚，在第一种方法中，我们可以观察到图像中没有文本的区域仍然用方框标记。同样在第二种方法中，文本没有被正确地检测到。</p><p id="fc5f" class="pw-post-body-paragraph ml mm it mn b mo ne kd mq mr nf kg mt lz ng mv mw md nh my mz mh ni nb nc nd im bi translated">还有很多其他深度学习算法用于文本检测和识别。在本文中，我们将讨论<strong class="mn jd"> EAST检测器</strong>，并尝试借助一篇关于<a class="ae og" href="https://arxiv.org/pdf/1704.03155.pdf" rel="noopener ugc nofollow" target="_blank"> EAST </a>算法的研究论文来实现它。为了识别，我们将尝试预先训练的模型<strong class="mn jd">宇宙魔方</strong>。</p><h2 id="9396" class="lm ln it bd ls lt lu dn lv lw lx dp ly lz ma mb mc md me mf mg mh mi mj mk iz bi translated">6.EAST(高效精确的场景文本检测器)</h2><p id="1d37" class="pw-post-body-paragraph ml mm it mn b mo mp kd mq mr ms kg mt lz mu mv mw md mx my mz mh na nb nc nd im bi translated">这是一种快速准确的场景文本检测方法，包括两个阶段:</p><p id="265f" class="pw-post-body-paragraph ml mm it mn b mo ne kd mq mr nf kg mt lz ng mv mw md nh my mz mh ni nb nc nd im bi translated">1.它使用完整的卷积网络(FCN)模型来直接生成基于像素的单词或文本行预测</p><p id="65e0" class="pw-post-body-paragraph ml mm it mn b mo ne kd mq mr nf kg mt lz ng mv mw md nh my mz mh ni nb nc nd im bi translated">2.生成文本预测(旋转矩形或四边形)后，输出被发送到非最大值抑制以产生最终结果。</p><p id="a90a" class="pw-post-body-paragraph ml mm it mn b mo ne kd mq mr nf kg mt lz ng mv mw md nh my mz mh ni nb nc nd im bi translated">管道如下所示:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi oo"><img src="../Images/9bf4498be56002dfbfef0c27d2843627.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mGMjN1Lo_N-QCorA3zmlAw.jpeg"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">图片由Paritosh Mahto提供</p></figure><p id="399e" class="pw-post-body-paragraph ml mm it mn b mo ne kd mq mr nf kg mt lz ng mv mw md nh my mz mh ni nb nc nd im bi translated"><strong class="mn jd">网络架构-(带PVANet) </strong></p><p id="fde9" class="pw-post-body-paragraph ml mm it mn b mo ne kd mq mr nf kg mt lz ng mv mw md nh my mz mh ni nb nc nd im bi translated">PVANet-它是用于对象检测的轻量级特征提取网络架构，在不损失准确性的情况下实现实时对象检测性能。</p><p id="3332" class="pw-post-body-paragraph ml mm it mn b mo ne kd mq mr nf kg mt lz ng mv mw md nh my mz mh ni nb nc nd im bi translated">该模型分为三个部分:词干特征提取、特征合并分支和输出层。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/b6310138295cc9768e56da652b728249.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2xBqhBIcvcauo_bBX2E-Bg.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">图片由Paritosh Mahto提供</p></figure><p id="c286" class="pw-post-body-paragraph ml mm it mn b mo ne kd mq mr nf kg mt lz ng mv mw md nh my mz mh ni nb nc nd im bi translated"><strong class="mn jd">一、特征提取器(PVANet) </strong></p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/3fa37fe4af7b23e7ab3c163c95e4892d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EY8p4ILpvdpuz4XYGo1fyA.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">图片由Paritosh Mahto提供</p></figure><p id="4298" class="pw-post-body-paragraph ml mm it mn b mo ne kd mq mr nf kg mt lz ng mv mw md nh my mz mh ni nb nc nd im bi translated">该部分可以是任何卷积神经网络，其具有在Imagenet数据上预先训练的卷积层和汇集层交织，例如PVANet、VGG16和RESNET50。从这个网络中，可以获得四级特征图f1、f2、f3和f4。因为我们正在提取特征，所以它被称为特征提取器。</p><p id="6d3e" class="pw-post-body-paragraph ml mm it mn b mo ne kd mq mr nf kg mt lz ng mv mw md nh my mz mh ni nb nc nd im bi translated"><strong class="mn jd">二。特征合并分支</strong></p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/a1f263cf272fe6af9f4fbb05045edf8c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fBkBiGSsfXU1UzOiiweHwQ.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">图片由Paritosh Mahto提供</p></figure><p id="c254" class="pw-post-body-paragraph ml mm it mn b mo ne kd mq mr nf kg mt lz ng mv mw md nh my mz mh ni nb nc nd im bi translated">在这一部分中，从特征提取器获得的特征图首先被馈送到非pooling层以使其大小加倍，然后在每个合并状态中与当前特征图连接。接下来，使用1×1卷积，其中conv瓶颈减少了通道的数量，并且也减少了计算量，接着使用3×3卷积来融合信息，以产生每个合并级的最终输出，如图2所示。</p><p id="9b84" class="pw-post-body-paragraph ml mm it mn b mo ne kd mq mr nf kg mt lz ng mv mw md nh my mz mh ni nb nc nd im bi translated">g和h的计算过程如下图所示</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi op"><img src="../Images/2827bdbbe0bb55ad70a979c8b7b1c913.png" data-original-src="https://miro.medium.com/v2/resize:fit:910/format:webp/0*X4xWDX8GR93U9OhK.png"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">图片来源—研究论文EAST</p></figure><p id="3122" class="pw-post-body-paragraph ml mm it mn b mo ne kd mq mr nf kg mt lz ng mv mw md nh my mz mh ni nb nc nd im bi translated">在哪里，</p><p id="9745" class="pw-post-body-paragraph ml mm it mn b mo ne kd mq mr nf kg mt lz ng mv mw md nh my mz mh ni nb nc nd im bi translated">gi是一个中间状态，是合并的基础</p><p id="4c47" class="pw-post-body-paragraph ml mm it mn b mo ne kd mq mr nf kg mt lz ng mv mw md nh my mz mh ni nb nc nd im bi translated">hi是合并的特征地图</p><p id="c781" class="pw-post-body-paragraph ml mm it mn b mo ne kd mq mr nf kg mt lz ng mv mw md nh my mz mh ni nb nc nd im bi translated"><strong class="mn jd">三世。输出层</strong></p><p id="1bbf" class="pw-post-body-paragraph ml mm it mn b mo ne kd mq mr nf kg mt lz ng mv mw md nh my mz mh ni nb nc nd im bi translated">来自合并状态的最终输出通过具有1个通道的1X1 Conv层，该层给出了范围从[0–1]的分数图。最终的输出也通过RBOX或QUAD几何体(关于这些的描述显示在下图中)给出了一个多通道几何体图。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi oq"><img src="../Images/20791c5da62f957ddd8305da72da5bf8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1056/format:webp/0*oHsGYOoBjjou1iyk.png"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">图片来源—研究论文EAST</p></figure><p id="d5a8" class="pw-post-body-paragraph ml mm it mn b mo ne kd mq mr nf kg mt lz ng mv mw md nh my mz mh ni nb nc nd im bi translated">关于分数图和几何图的细节将在实现时讨论。</p><h2 id="8138" class="lm ln it bd ls lt lu dn lv lw lx dp ly lz ma mb mc md me mf mg mh mi mj mk iz bi translated">7.履行</h2><p id="2d90" class="pw-post-body-paragraph ml mm it mn b mo mp kd mq mr ms kg mt lz mu mv mw md mx my mz mh na nb nc nd im bi translated">对于实现，我们将遵循上面显示的管道-</p><p id="6d0c" class="pw-post-body-paragraph ml mm it mn b mo ne kd mq mr nf kg mt lz ng mv mw md nh my mz mh ni nb nc nd im bi translated"><strong class="mn jd">步骤1-数据准备&amp;数据生成(数据流水线)</strong></p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi or"><img src="../Images/0807829f8a665469e7520935a4958c95.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0bWMjb2ICx6gi4YqQ0GnvQ.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">图片由Paritosh Mahto提供</p></figure><p id="7555" class="pw-post-body-paragraph ml mm it mn b mo ne kd mq mr nf kg mt lz ng mv mw md nh my mz mh ni nb nc nd im bi translated">在这一步中，我们必须进行数据准备，还必须构建一个生成器函数，该函数将给出一个图像数组(模型的输入),其中包含分数图(输出)和地理图(输出),正如您在上图中看到的多通道FCN的输出以及训练掩码。</p><p id="d2d7" class="pw-post-body-paragraph ml mm it mn b mo ne kd mq mr nf kg mt lz ng mv mw md nh my mz mh ni nb nc nd im bi translated"><strong class="mn jd">评分图:</strong></p><p id="12c1" class="pw-post-body-paragraph ml mm it mn b mo ne kd mq mr nf kg mt lz ng mv mw md nh my mz mh ni nb nc nd im bi translated">它表示该位置的预测几何图的置信度得分/级别。它位于范围[0，1]内。我们通过一个例子来理解一下:</p><p id="335f" class="pw-post-body-paragraph ml mm it mn b mo ne kd mq mr nf kg mt lz ng mv mw md nh my mz mh ni nb nc nd im bi translated">假设0.80是像素的得分图，这仅仅意味着对于该像素，我们有80%的把握它将具有预测的几何图，或者我们可以说该像素有80%的机会是预测的文本区域的一部分。</p><p id="5683" class="pw-post-body-paragraph ml mm it mn b mo ne kd mq mr nf kg mt lz ng mv mw md nh my mz mh ni nb nc nd im bi translated"><strong class="mn jd">地理地图:</strong></p><p id="fc79" class="pw-post-body-paragraph ml mm it mn b mo ne kd mq mr nf kg mt lz ng mv mw md nh my mz mh ni nb nc nd im bi translated">正如我们所知，随着得分图，我们还获得了一个多通道几何信息图作为输出。几何输出可以是RBOX或QUAD。下表显示了通道数量以及AABB、RBOX和QUAD的功能。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi oq"><img src="../Images/20791c5da62f957ddd8305da72da5bf8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1056/format:webp/0*oHsGYOoBjjou1iyk.png"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">图片来源—研究论文EAST</p></figure><p id="b5eb" class="pw-post-body-paragraph ml mm it mn b mo ne kd mq mr nf kg mt lz ng mv mw md nh my mz mh ni nb nc nd im bi translated"><strong class="mn jd">信箱:</strong></p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi os"><img src="../Images/b724cc2267aba7d787e40202a8f41905.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5uTsUndPUjfOmZPnYBvk2Q.jpeg"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">图片由Paritosh Mahto提供</p></figure><p id="ecd2" class="pw-post-body-paragraph ml mm it mn b mo ne kd mq mr nf kg mt lz ng mv mw md nh my mz mh ni nb nc nd im bi translated">从上图中我们可以观察到，对于RBOX，几何体使用四通道轴对齐的边界框(AABB) R和通道旋转角度θ。R的公式为g。四个通道表示4个距离，即从像素位置到矩形边界的距离，一个通道表示旋转角度，如下所示。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ot"><img src="../Images/515b9dab15649e808833dfc1bf8c8ea7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_Lp627hAnsKtdVnrtlkexw.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">图像源-纸张(场景文本检测和识别)</p></figure><p id="603e" class="pw-post-body-paragraph ml mm it mn b mo ne kd mq mr nf kg mt lz ng mv mw md nh my mz mh ni nb nc nd im bi translated"><strong class="mn jd">四元:</strong></p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ou"><img src="../Images/04cfe4a3a87da8253d412141121309bb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-all6xF6aY55Rp13yKchLw.jpeg"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">图片由Paritosh Mahto提供</p></figure><p id="3912" class="pw-post-body-paragraph ml mm it mn b mo ne kd mq mr nf kg mt lz ng mv mw md nh my mz mh ni nb nc nd im bi translated">对于四边形，我们使用8个数字来表示从四个顶点到每个像素位置的坐标位移。每个偏移距离包含δXi |δyi两个数字，几何输出包含8个通道。下面显示了一个示例</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/327a25bd7233f7b5da0a96637780d96c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vJgMtEhWH5Vjeh5Mch6VTQ.jpeg"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">图片由Paritosh Mahto提供</p></figure><p id="19b9" class="pw-post-body-paragraph ml mm it mn b mo ne kd mq mr nf kg mt lz ng mv mw md nh my mz mh ni nb nc nd im bi translated">在这个实现中，我们将只使用RBOX。</p><p id="9727" class="pw-post-body-paragraph ml mm it mn b mo ne kd mq mr nf kg mt lz ng mv mw md nh my mz mh ni nb nc nd im bi translated">对于生成器函数，我们必须遵循几个步骤</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/9e3b68f1d70db97ab0041b844dbaedea.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*L7g420gMVr64o1oSibO1EA.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">图片由Paritosh Mahto提供</p></figure><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/959e7db4b7f8528fa0888597bac4e0c2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*w4OHVbNZ5dtCljXoHov08w.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">图片由Paritosh MAhto提供</p></figure><p id="3c73" class="pw-post-body-paragraph ml mm it mn b mo ne kd mq mr nf kg mt lz ng mv mw md nh my mz mh ni nb nc nd im bi translated">所有的密码都在这里-</p><div class="ov ow gp gr ox oy"><a href="https://jovian.ai/paritosh/data-preparation-and-model-implt" rel="noopener  ugc nofollow" target="_blank"><div class="oz ab fo"><div class="pa ab pb cl cj pc"><h2 class="bd jd gy z fp pd fr fs pe fu fw jc bi translated">pari tosh/数据准备和模型实现- Jovian</h2><div class="pf l"><h3 class="bd b gy z fp pd fr fs pe fu fw dk translated">与paritosh合作编写数据准备和模型实施笔记本。</h3></div><div class="pg l"><p class="bd b dl z fp pd fr fs pe fu fw dk translated">jovian.ai</p></div></div><div class="ph l"><div class="pi l pj pk pl ph pm lb oy"/></div></div></a></div><p id="ce4a" class="pw-post-body-paragraph ml mm it mn b mo ne kd mq mr nf kg mt lz ng mv mw md nh my mz mh ni nb nc nd im bi translated">这里显示了从生成器函数输出的带有分数图、几何图和训练掩码的原始图像</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="ok ol l"/></div></figure><p id="524a" class="pw-post-body-paragraph ml mm it mn b mo ne kd mq mr nf kg mt lz ng mv mw md nh my mz mh ni nb nc nd im bi translated"><strong class="mn jd">第二步建模&amp;损失函数</strong></p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pn"><img src="../Images/1960b67609ddccc5effcfc3184ad22cf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6DDHFBZX-SA0Y_ePiv9pig.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">图片由Paritosh Mahto提供</p></figure><p id="d5d1" class="pw-post-body-paragraph ml mm it mn b mo ne kd mq mr nf kg mt lz ng mv mw md nh my mz mh ni nb nc nd im bi translated">在这一步中，我们将尝试使用Imagenet数据上预先训练的VGG16模型和ResNet50模型作为特征提取器来构建检测器架构。</p><p id="21be" class="pw-post-body-paragraph ml mm it mn b mo ne kd mq mr nf kg mt lz ng mv mw md nh my mz mh ni nb nc nd im bi translated"><strong class="mn jd">模型-1(VGG16作为特征提取器)</strong></p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/6ecb6eae5e5ae273ab56fc037869fd29.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cwWN2WwggmRdvO8RQPG7rQ.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">图片由Paritosh Mahto提供</p></figure><p id="1caa" class="pw-post-body-paragraph ml mm it mn b mo ne kd mq mr nf kg mt lz ng mv mw md nh my mz mh ni nb nc nd im bi translated">源代码-</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="ok ol l"/></div></figure><p id="bb72" class="pw-post-body-paragraph ml mm it mn b mo ne kd mq mr nf kg mt lz ng mv mw md nh my mz mh ni nb nc nd im bi translated">模型架构-</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="ok ol l"/></div></figure><p id="58ec" class="pw-post-body-paragraph ml mm it mn b mo ne kd mq mr nf kg mt lz ng mv mw md nh my mz mh ni nb nc nd im bi translated"><strong class="mn jd">模型-2(ResNet50作为特征提取器)</strong></p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="ok ol l"/></div></figure><p id="3946" class="pw-post-body-paragraph ml mm it mn b mo ne kd mq mr nf kg mt lz ng mv mw md nh my mz mh ni nb nc nd im bi translated">模型架构-</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="ok ol l"/></div></figure><h2 id="b913" class="lm ln it bd ls lt lu dn lv lw lx dp ly lz ma mb mc md me mf mg mh mi mj mk iz bi translated">损失函数</h2><p id="aef7" class="pw-post-body-paragraph ml mm it mn b mo mp kd mq mr ms kg mt lz mu mv mw md mx my mz mh na nb nc nd im bi translated">当我们处理图像数据时，IOU得分是经常使用的损失之一。但是这里我们主要有两个输出:分数图和几何图，所以我们必须计算两者的损失。</p><p id="4476" class="pw-post-body-paragraph ml mm it mn b mo ne kd mq mr nf kg mt lz ng mv mw md nh my mz mh ni nb nc nd im bi translated">总损失表示为:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi po"><img src="../Images/de4f0a45bd6fec2317cdcd1f1041ca22.png" data-original-src="https://miro.medium.com/v2/resize:fit:370/0*vCeF0lV_wsWeqxXi.JPEG"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">图片来源—研究论文EAST</p></figure><p id="a384" class="pw-post-body-paragraph ml mm it mn b mo ne kd mq mr nf kg mt lz ng mv mw md nh my mz mh ni nb nc nd im bi translated">Ls和Lg代表得分图和几何形状，λg衡量两个权重的重要性。在我们的实验中，我们设λg为1。</p><p id="2173" class="pw-post-body-paragraph ml mm it mn b mo ne kd mq mr nf kg mt lz ng mv mw md nh my mz mh ni nb nc nd im bi translated"><strong class="mn jd">为得分图损失</strong></p><p id="ddc0" class="pw-post-body-paragraph ml mm it mn b mo ne kd mq mr nf kg mt lz ng mv mw md nh my mz mh ni nb nc nd im bi translated">在该论文中，用于得分图的损失是二进制交叉熵损失，其对正负类别都有权重，如图2所示。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi pp"><img src="../Images/dc792b0552fc40adfae9fa7a039d96d5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1118/0*BqgRUd1dyOwYjArH.JPEG"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">图片来源—研究论文EAST</p></figure><p id="4e02" class="pw-post-body-paragraph ml mm it mn b mo ne kd mq mr nf kg mt lz ng mv mw md nh my mz mh ni nb nc nd im bi translated">但是，当使用实施骰子损失时</p><p id="9d87" class="pw-post-body-paragraph ml mm it mn b mo ne kd mq mr nf kg mt lz ng mv mw md nh my mz mh ni nb nc nd im bi translated"><strong class="mn jd">几何图丢失</strong></p><p id="65e5" class="pw-post-body-paragraph ml mm it mn b mo ne kd mq mr nf kg mt lz ng mv mw md nh my mz mh ni nb nc nd im bi translated">对于RBOX，损失的定义是</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi pq"><img src="../Images/8537089ad1775f3a12d951b0bfc2a76c.png" data-original-src="https://miro.medium.com/v2/resize:fit:462/0*Iz-octBUGTiL2cxN.JPEG"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">图片来源—研究论文EAST</p></figure><p id="a2d3" class="pw-post-body-paragraph ml mm it mn b mo ne kd mq mr nf kg mt lz ng mv mw md nh my mz mh ni nb nc nd im bi translated">第一个损耗是盒子损耗，对于这个损耗，IOU损耗被使用，因为它对于不同比例的物体是不变的。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi pr"><img src="../Images/bd7ac596937e5082882e4060bece9c90.png" data-original-src="https://miro.medium.com/v2/resize:fit:1074/0*x2odDozHCB5mAWvE.JPEG"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">图片来源—研究论文EAST</p></figure><p id="16b0" class="pw-post-body-paragraph ml mm it mn b mo ne kd mq mr nf kg mt lz ng mv mw md nh my mz mh ni nb nc nd im bi translated">对于旋转角度，损耗由下式给出</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi ps"><img src="../Images/ffe3369015b168a1de85f01005348988.png" data-original-src="https://miro.medium.com/v2/resize:fit:630/0*ulwVZBi4W42YWXQb.JPEG"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">图片来源—研究论文EAST</p></figure><p id="c0a3" class="pw-post-body-paragraph ml mm it mn b mo ne kd mq mr nf kg mt lz ng mv mw md nh my mz mh ni nb nc nd im bi translated">实现的代码如下所示:</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="ok ol l"/></div></figure><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="ok ol l"/></div></figure><p id="edac" class="pw-post-body-paragraph ml mm it mn b mo ne kd mq mr nf kg mt lz ng mv mw md nh my mz mh ni nb nc nd im bi translated"><strong class="mn jd">第三步模型训练</strong></p><p id="1f3d" class="pw-post-body-paragraph ml mm it mn b mo ne kd mq mr nf kg mt lz ng mv mw md nh my mz mh ni nb nc nd im bi translated">两个模型都用Adam优化器训练了30个时期，其他参数如下所示</p><p id="713b" class="pw-post-body-paragraph ml mm it mn b mo ne kd mq mr nf kg mt lz ng mv mw md nh my mz mh ni nb nc nd im bi translated"><strong class="mn jd">型号1 </strong></p><pre class="ks kt ku kv gt lh li lj lk aw ll bi"><span id="cff1" class="lm ln it li b gy lo lp l lq lr"><strong class="li jd">model_vgg.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001,amsgrad=True),loss= total_Loss())</strong></span></pre><p id="e4ab" class="pw-post-body-paragraph ml mm it mn b mo ne kd mq mr nf kg mt lz ng mv mw md nh my mz mh ni nb nc nd im bi translated">历元与损耗图:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pt"><img src="../Images/f48060c5e745ae9a716ee42400c9dadc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ls8nJQls2NFvcIOePGyMkw.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">图片由Paritosh Mahto提供</p></figure><p id="798f" class="pw-post-body-paragraph ml mm it mn b mo ne kd mq mr nf kg mt lz ng mv mw md nh my mz mh ni nb nc nd im bi translated"><strong class="mn jd">型号2 </strong></p><pre class="ks kt ku kv gt lh li lj lk aw ll bi"><span id="bfea" class="lm ln it li b gy lo lp l lq lr"><strong class="li jd">model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001,amsgrad=True),loss=total_Loss())</strong></span></pre><p id="6398" class="pw-post-body-paragraph ml mm it mn b mo ne kd mq mr nf kg mt lz ng mv mw md nh my mz mh ni nb nc nd im bi translated">历元与损耗图:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pu"><img src="../Images/7af1412c84cb25b38f47dc31c4da90df.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7lhS8rKWjmyNS-4rFIDQgg.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">图片由Paritosh Mahto提供</p></figure><p id="12f0" class="pw-post-body-paragraph ml mm it mn b mo ne kd mq mr nf kg mt lz ng mv mw md nh my mz mh ni nb nc nd im bi translated"><strong class="mn jd">第四步干涉管线</strong></p><p id="2960" class="pw-post-body-paragraph ml mm it mn b mo ne kd mq mr nf kg mt lz ng mv mw md nh my mz mh ni nb nc nd im bi translated">在训练之后，首先将几何图转换回边界框。然后，我们应用基于得分图的阈值来去除一些低置信度框。使用<strong class="mn jd">非最大抑制合并剩余的盒子。</strong></p><blockquote class="nj nk nl"><p id="e4f4" class="ml mm nm mn b mo ne kd mq mr nf kg mt nn ng mv mw no nh my mz np ni nb nc nd im bi translated">非最大抑制(NMS)是许多计算机视觉算法中使用的一种技术。这是一类从许多重叠的实体中选择一个实体(例如边界框)的算法。-来源<a class="ae og" href="https://whatdhack.medium.com/reflections-on-non-maximum-suppression-nms-d2fce148ef0a" rel="noopener">关于非最大抑制的思考(NMS)| subra ta Goswami | Medium</a></p></blockquote><p id="f920" class="pw-post-body-paragraph ml mm it mn b mo ne kd mq mr nf kg mt lz ng mv mw md nh my mz mh ni nb nc nd im bi translated">这里我们将使用本地感知的NMS。它将加权合并添加到标准NMS中。所谓加权合并，就是根据分数在输出框中合并高于某个阈值的2个iou。下面讨论实施时遵循的步骤</p><ol class=""><li id="cc08" class="nq nr it mn b mo ne mr nf lz oh md oi mh oj nd pv nw nx ny bi translated">首先，对几何图形进行排序，从最顶端开始。</li><li id="c548" class="nq nr it mn b mo nz mr oa lz ob md oc mh od nd pv nw nx ny bi translated">2.拿这一行中的下一个盒子，找出与前一个盒子相同的借据</li><li id="0961" class="nq nr it mn b mo nz mr oa lz ob md oc mh od nd pv nw nx ny bi translated">3.如果IOU &gt; threshold，通过按分数取加权平均值来合并2个框，否则保持原样。</li><li id="5829" class="nq nr it mn b mo nz mr oa lz ob md oc mh od nd pv nw nx ny bi translated">4.重复步骤2到3，直到所有的盒子都被迭代。</li><li id="216e" class="nq nr it mn b mo nz mr oa lz ob md oc mh od nd pv nw nx ny bi translated">5.最后在剩下的盒子上使用标准NMS。</li></ol><p id="dc1c" class="pw-post-body-paragraph ml mm it mn b mo ne kd mq mr nf kg mt lz ng mv mw md nh my mz mh ni nb nc nd im bi translated">实现的代码-</p><p id="13e2" class="pw-post-body-paragraph ml mm it mn b mo ne kd mq mr nf kg mt lz ng mv mw md nh my mz mh ni nb nc nd im bi translated"><strong class="mn jd">非最大抑制</strong></p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="ok ol l"/></div></figure><p id="7ef9" class="pw-post-body-paragraph ml mm it mn b mo ne kd mq mr nf kg mt lz ng mv mw md nh my mz mh ni nb nc nd im bi translated"><strong class="mn jd">检测模型的干扰管道</strong></p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="ok ol l"/></div></figure><p id="a2cf" class="pw-post-body-paragraph ml mm it mn b mo ne kd mq mr nf kg mt lz ng mv mw md nh my mz mh ni nb nc nd im bi translated">每个模型的输出:</p><p id="a665" class="pw-post-body-paragraph ml mm it mn b mo ne kd mq mr nf kg mt lz ng mv mw md nh my mz mh ni nb nc nd im bi translated">模型-1</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pw"><img src="../Images/1dab87dc4e40bb5821f5dbfa4711a497.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xaWmNlJRoE348DuwfPwBtA.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">图片来源ICDAR2015</p></figure><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pw"><img src="../Images/f4d8045809e61a2af961a8abbfa3b682.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*w3Y9l0CMl6arshkfcNEhhw.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">图片来源ICDAR2015</p></figure><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pw"><img src="../Images/15bf254b34e016251f785f163a57fefc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*j73Gs19LA3B70ncuBJsVmA.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">图片来源ICDAR2015</p></figure><p id="7df2" class="pw-post-body-paragraph ml mm it mn b mo ne kd mq mr nf kg mt lz ng mv mw md nh my mz mh ni nb nc nd im bi translated">模型-2</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pw"><img src="../Images/0330bb7246dc803af6f2ad11bb0c2b20.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*X-tXrqfLrRAiP5GReeVWQw.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">图片来源ICDAR2015</p></figure><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pw"><img src="../Images/af911598cca8f39ea6634e5e7125eee5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*G7vps58r95tcOVTjKcXwyw.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">图片来源ICDAR2015</p></figure><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pw"><img src="../Images/747b982d23ecc97588cbd2fbbce4b6c9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6CON1HVcy2Dt59MQX6A2gQ.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">图片来源ICDAR2015</p></figure><p id="0eaf" class="pw-post-body-paragraph ml mm it mn b mo ne kd mq mr nf kg mt lz ng mv mw md nh my mz mh ni nb nc nd im bi translated">如果我们比较两个模型的损失，那么我们可以得出结论，模型2 (resnet_east)表现良好。让我们分析一下model_2的性能。</p><h2 id="9064" class="lm ln it bd ls lt lu dn lv lw lx dp ly lz ma mb mc md me mf mg mh mi mj mk iz bi translated">8.模型分析和模型量化</h2><p id="98b1" class="pw-post-body-paragraph ml mm it mn b mo mp kd mq mr ms kg mt lz mu mv mw md mx my mz mh na nb nc nd im bi translated">正如我们已经看到的，模型2比模型1执行得更好，这里我们将对模型2的输出进行一些分析。首先，计算训练和测试目录中每个图像的损失，然后基于分布，通过查看每个训练和测试图像的损失的方框图，我们将选择两个阈值损失，最后，我们将数据分为三类，即最好、一般和最差。</p><p id="6715" class="pw-post-body-paragraph ml mm it mn b mo ne kd mq mr nf kg mt lz ng mv mw md nh my mz mh ni nb nc nd im bi translated">训练集和测试集的密度图-</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi px"><img src="../Images/7df782d100a0d076c5ba8fd990db9489.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KPYFyp-BeUra0RVqT7MgEw.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">图片由Paritosh Mahto提供</p></figure><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi px"><img src="../Images/5dd637584c12885d3ac3ef7f98e6855c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*m63_c1eDqGBEvHyM_c_x6A.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">图片由Paritosh Mahto提供</p></figure><p id="19c2" class="pw-post-body-paragraph ml mm it mn b mo ne kd mq mr nf kg mt lz ng mv mw md nh my mz mh ni nb nc nd im bi translated">将图像数据分为3类(最好、一般和最差)</p><ul class=""><li id="af92" class="nq nr it mn b mo ne mr nf lz oh md oi mh oj nd nv nw nx ny bi translated">将数据分为3类</li></ul><pre class="ks kt ku kv gt lh li lj lk aw ll bi"><span id="2204" class="lm ln it li b gy lo lp l lq lr">Best = if loss &lt; np.percentile(final_train.loss, 33)-----&gt;Threshold-1<br/>Average = if np.percentile(final_train.loss, 33)&lt; loss &lt; np.percentile(final_train.loss, 75)<br/>Worst = if loss &gt; np.percentile(final_train.loss, 75)--Threshold-2</span></pre><p id="de90" class="pw-post-body-paragraph ml mm it mn b mo ne kd mq mr nf kg mt lz ng mv mw md nh my mz mh ni nb nc nd im bi translated">每个类别的图片数量如下所示:</p><p id="2f36" class="pw-post-body-paragraph ml mm it mn b mo ne kd mq mr nf kg mt lz ng mv mw md nh my mz mh ni nb nc nd im bi translated">对于训练图像:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi py"><img src="../Images/4b40fab168a0aff0f0928ab5673c82bd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Wa5j3ZbDbxEWaIHAz84PCw.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">图片由Paritosh Mahto提供</p></figure><p id="6d17" class="pw-post-body-paragraph ml mm it mn b mo ne kd mq mr nf kg mt lz ng mv mw md nh my mz mh ni nb nc nd im bi translated">对于测试图像:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi py"><img src="../Images/34bff8dc31485f7d4f26bc2221fec56d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*m5odu3xSnp7t7YkPlRo7ow.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">图片由Paritosh Mahto提供</p></figure><ul class=""><li id="8f0e" class="nq nr it mn b mo ne mr nf lz oh md oi mh oj nd nv nw nx ny bi translated">从第一个图1中，对于列车数据，我们可以观察到33%的数据属于最佳类别，42%的数据属于一般类别，只有25%的数据属于最差类别。</li><li id="c1eb" class="nq nr it mn b mo nz mr oa lz ob md oc mh od nd nv nw nx ny bi translated">从第二个图I中，对于测试数据，我们可以观察到2.6%的数据属于最佳类别，13.2%的数据属于一般类别，84.2%的数据属于最差类别。</li><li id="3344" class="nq nr it mn b mo nz mr oa lz ob md oc mh od nd nv nw nx ny bi translated">从这些观察中，我们还可以得出结论，我们的模型对于测试图像(即新图像)可能表现不佳。</li></ul><p id="06ca" class="pw-post-body-paragraph ml mm it mn b mo ne kd mq mr nf kg mt lz ng mv mw md nh my mz mh ni nb nc nd im bi translated"><strong class="mn jd">模型量化</strong></p><p id="c4bc" class="pw-post-body-paragraph ml mm it mn b mo ne kd mq mr nf kg mt lz ng mv mw md nh my mz mh ni nb nc nd im bi translated">深度学习的量化是通过低位宽数的神经网络来逼近使用浮点数的神经网络的过程。这极大地降低了使用神经网络的内存需求和计算成本。量化后，原始模型和量化模型的大小如下所示-</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="ok ol l"/></div></figure><h2 id="5320" class="lm ln it bd ls lt lu dn lv lw lx dp ly lz ma mb mc md me mf mg mh mi mj mk iz bi translated">9.部署</h2><p id="e007" class="pw-post-body-paragraph ml mm it mn b mo mp kd mq mr ms kg mt lz mu mv mw md mx my mz mh na nb nc nd im bi translated">模型量化后，使用streamlit和Github选择并部署了float16量化模型。使用streamlit uploader函数，我创建了一个. jpg文件输入部分，您可以在其中给出原始图像数据，模型将给出图像，并在图像上显示检测到的文本。</p><p id="cc14" class="pw-post-body-paragraph ml mm it mn b mo ne kd mq mr nf kg mt lz ng mv mw md nh my mz mh ni nb nc nd im bi translated">网页链接-<a class="ae og" href="https://share.streamlit.io/paritoshmahto07/scene-text-detection-and-recognition-/main/app_2.py" rel="noopener ugc nofollow" target="_blank">https://share . streamlit . io/paritoshmahto 07/scene-text-detection-and-recognition-/main/app _ 2 . py</a></p><p id="b6fa" class="pw-post-body-paragraph ml mm it mn b mo ne kd mq mr nf kg mt lz ng mv mw md nh my mz mh ni nb nc nd im bi translated">部署视频-</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="pz ol l"/></div></figure><h2 id="cea8" class="lm ln it bd ls lt lu dn lv lw lx dp ly lz ma mb mc md me mf mg mh mi mj mk iz bi translated">10.未来的工作</h2><p id="1c1c" class="pw-post-body-paragraph ml mm it mn b mo mp kd mq mr ms kg mt lz mu mv mw md mx my mz mh na nb nc nd im bi translated">在这项任务中，我们的主要目标是了解任何检测模型的工作原理，并从头开始实现它。为了提高模型的性能，我们可以用大型数据集来训练我们的模型。我们还可以使用另一种识别模型来更好地识别文本。</p><p id="51af" class="pw-post-body-paragraph ml mm it mn b mo ne kd mq mr nf kg mt lz ng mv mw md nh my mz mh ni nb nc nd im bi translated"><strong class="mn jd">我的LinkedIn个人资料</strong></p><div class="ov ow gp gr ox oy"><a href="https://www.linkedin.com/in/paritosh07/" rel="noopener  ugc nofollow" target="_blank"><div class="oz ab fo"><div class="pa ab pb cl cj pc"><h2 class="bd jd gy z fp pd fr fs pe fu fw jc bi translated">Paritosh Mahto -学生应用人工智能课程| LinkedIn</h2><div class="pf l"><h3 class="bd b gy z fp pd fr fs pe fu fw dk translated">查看Paritosh Mahto在世界上最大的职业社区LinkedIn上的个人资料。Paritosh有一个工作列在…</h3></div><div class="pg l"><p class="bd b dl z fp pd fr fs pe fu fw dk translated">www.linkedin.com</p></div></div></div></a></div><p id="79ad" class="pw-post-body-paragraph ml mm it mn b mo ne kd mq mr nf kg mt lz ng mv mw md nh my mz mh ni nb nc nd im bi translated"><strong class="mn jd">我的Github个人资料</strong></p><div class="ov ow gp gr ox oy"><a href="https://github.com/paritoshMahto07" rel="noopener  ugc nofollow" target="_blank"><div class="oz ab fo"><div class="pa ab pb cl cj pc"><h2 class="bd jd gy z fp pd fr fs pe fu fw jc bi translated">paritoshMahto07 -概述</h2><div class="pf l"><h3 class="bd b gy z fp pd fr fs pe fu fw dk translated">paritoshMahto07有两个可用的存储库。在GitHub上关注他们的代码。</h3></div><div class="pg l"><p class="bd b dl z fp pd fr fs pe fu fw dk translated">github.com</p></div></div><div class="ph l"><div class="qa l pj pk pl ph pm lb oy"/></div></div></a></div><h2 id="1ca1" class="lm ln it bd ls lt lu dn lv lw lx dp ly lz ma mb mc md me mf mg mh mi mj mk iz bi translated">11.参考</h2><p id="e058" class="pw-post-body-paragraph ml mm it mn b mo mp kd mq mr ms kg mt lz mu mv mw md mx my mz mh na nb nc nd im bi translated">一.研究论文</p><div class="ov ow gp gr ox oy"><a href="https://arxiv.org/abs/1811.04256" rel="noopener  ugc nofollow" target="_blank"><div class="oz ab fo"><div class="pa ab pb cl cj pc"><h2 class="bd jd gy z fp pd fr fs pe fu fw jc bi translated">场景文本检测和识别:深度学习时代</h2><div class="pf l"><h3 class="bd b gy z fp pd fr fs pe fu fw dk translated">随着深度学习的兴起和发展，计算机视觉发生了巨大的变革和重塑。作为一个…</h3></div><div class="pg l"><p class="bd b dl z fp pd fr fs pe fu fw dk translated">arxiv.org</p></div></div></div></a></div><div class="ov ow gp gr ox oy"><a href="https://arxiv.org/abs/1704.03155" rel="noopener  ugc nofollow" target="_blank"><div class="oz ab fo"><div class="pa ab pb cl cj pc"><h2 class="bd jd gy z fp pd fr fs pe fu fw jc bi translated">EAST:一种高效准确的场景文本检测器</h2><div class="pf l"><h3 class="bd b gy z fp pd fr fs pe fu fw dk translated">以前的场景文本检测方法已经在各种基准测试中取得了令人满意的性能…</h3></div><div class="pg l"><p class="bd b dl z fp pd fr fs pe fu fw dk translated">arxiv.org</p></div></div></div></a></div><p id="48d5" class="pw-post-body-paragraph ml mm it mn b mo ne kd mq mr nf kg mt lz ng mv mw md nh my mz mh ni nb nc nd im bi translated">二。博客</p><div class="ov ow gp gr ox oy"><a href="https://www.programmersought.com/article/56252830437/" rel="noopener  ugc nofollow" target="_blank"><div class="oz ab fo"><div class="pa ab pb cl cj pc"><h2 class="bd jd gy z fp pd fr fs pe fu fw jc bi translated">一个高效准确的场景文本检测器-程序员寻求</h2><div class="pf l"><h3 class="bd b gy z fp pd fr fs pe fu fw dk translated">本文提出了一种快速准确的场景文本检测算法，只有两步。完整的卷积…</h3></div><div class="pg l"><p class="bd b dl z fp pd fr fs pe fu fw dk translated">www.programmersought.com</p></div></div><div class="ph l"><div class="qb l pj pk pl ph pm lb oy"/></div></div></a></div><div class="ov ow gp gr ox oy"><a href="https://theailearner.com/2019/10/19/efficient-and-accurate-scene-text-detector-east/" rel="noopener  ugc nofollow" target="_blank"><div class="oz ab fo"><div class="pa ab pb cl cj pc"><h2 class="bd jd gy z fp pd fr fs pe fu fw jc bi translated">高效准确的场景文本检测器(东)</h2><div class="pf l"><h3 class="bd b gy z fp pd fr fs pe fu fw dk translated">在文本检测领域引入深度学习之前，大多数文本分割都很难…</h3></div><div class="pg l"><p class="bd b dl z fp pd fr fs pe fu fw dk translated">theailearner.com</p></div></div><div class="ph l"><div class="qc l pj pk pl ph pm lb oy"/></div></div></a></div><p id="1a46" class="pw-post-body-paragraph ml mm it mn b mo ne kd mq mr nf kg mt lz ng mv mw md nh my mz mh ni nb nc nd im bi translated">三。Github代码源代码</p><div class="ov ow gp gr ox oy"><a href="https://github.com/kurapan/EAST" rel="noopener  ugc nofollow" target="_blank"><div class="oz ab fo"><div class="pa ab pb cl cj pc"><h2 class="bd jd gy z fp pd fr fs pe fu fw jc bi translated">库拉潘/东部</h2><div class="pf l"><h3 class="bd b gy z fp pd fr fs pe fu fw dk translated">这是基于argman的Tensorflow实现的EAST的Keras实现。原论文作者周…</h3></div><div class="pg l"><p class="bd b dl z fp pd fr fs pe fu fw dk translated">github.com</p></div></div><div class="ph l"><div class="qd l pj pk pl ph pm lb oy"/></div></div></a></div><div class="ov ow gp gr ox oy"><a href="https://github.com/solaris33/EAST-tf2" rel="noopener  ugc nofollow" target="_blank"><div class="oz ab fo"><div class="pa ab pb cl cj pc"><h2 class="bd jd gy z fp pd fr fs pe fu fw jc bi translated">Solaris 33/东部-tf2</h2><div class="pf l"><h3 class="bd b gy z fp pd fr fs pe fu fw dk translated">这是EAST的TensorFlow2 &amp; Keras实现:一个基于Keras的高效准确的场景文本检测器…</h3></div><div class="pg l"><p class="bd b dl z fp pd fr fs pe fu fw dk translated">github.com</p></div></div><div class="ph l"><div class="qe l pj pk pl ph pm lb oy"/></div></div></a></div><p id="fb1d" class="pw-post-body-paragraph ml mm it mn b mo ne kd mq mr nf kg mt lz ng mv mw md nh my mz mh ni nb nc nd im bi translated">四。模型量化</p><div class="ov ow gp gr ox oy"><a href="https://www.tensorflow.org/lite/performance/post_training_quantization" rel="noopener  ugc nofollow" target="_blank"><div class="oz ab fo"><div class="pa ab pb cl cj pc"><h2 class="bd jd gy z fp pd fr fs pe fu fw jc bi translated">训练后量化| TensorFlow Lite</h2><div class="pf l"><h3 class="bd b gy z fp pd fr fs pe fu fw dk translated">训练后量化是一种转换技术，可以减少模型大小，同时还可以改善CPU和硬件…</h3></div><div class="pg l"><p class="bd b dl z fp pd fr fs pe fu fw dk translated">www.tensorflow.org</p></div></div><div class="ph l"><div class="qf l pj pk pl ph pm lb oy"/></div></div></a></div><p id="469d" class="pw-post-body-paragraph ml mm it mn b mo ne kd mq mr nf kg mt lz ng mv mw md nh my mz mh ni nb nc nd im bi translated">动词 （verb的缩写）师徒制</p><div class="ov ow gp gr ox oy"><a href="https://www.appliedaicourse.com/" rel="noopener  ugc nofollow" target="_blank"><div class="oz ab fo"><div class="pa ab pb cl cj pc"><h2 class="bd jd gy z fp pd fr fs pe fu fw jc bi translated">应用课程</h2><div class="pf l"><h3 class="bd b gy z fp pd fr fs pe fu fw dk translated">我们知道转行是多么具有挑战性。我们的应用人工智能/机器学习课程被设计为整体学习…</h3></div><div class="pg l"><p class="bd b dl z fp pd fr fs pe fu fw dk translated">www.appliedaicourse.com</p></div></div><div class="ph l"><div class="qg l pj pk pl ph pm lb oy"/></div></div></a></div><p id="44a7" class="pw-post-body-paragraph ml mm it mn b mo ne kd mq mr nf kg mt lz ng mv mw md nh my mz mh ni nb nc nd im bi translated"><em class="nm">感谢阅读，祝你有美好的一天！</em>🙂</p></div></div>    
</body>
</html>