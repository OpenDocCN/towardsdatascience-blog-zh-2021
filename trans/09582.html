<html>
<head>
<title>What does word2vec actually learn?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">word2vec实际学的是什么？</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/what-does-word2vec-actually-learn-489f3f950388?source=collection_archive---------17-----------------------#2021-09-06">https://towardsdatascience.com/what-does-word2vec-actually-learn-489f3f950388?source=collection_archive---------17-----------------------#2021-09-06</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="71cf" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsdatascience.com/tagged/thoughts-and-theory" rel="noopener" target="_blank">思想和理论</a></h2><div class=""/><div class=""><h2 id="7760" class="pw-subtitle-paragraph jw iz iq bd b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn dk translated">以及如何从相似性函数中训练嵌入</h2></div><p id="f3db" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">用连续向量表示离散对象，即所谓的嵌入，是许多成功的机器学习解决方案的核心。这种优越性来自于这样一个事实，即与原始的离散对象不同，嵌入向量提供了一种紧凑的表示，它捕捉了原始对象之间的相似性。</p><p id="d8ba" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">在本文中，我们考虑著名的word2vec算法。Word2vec简单直观。在高层次上，它认为频繁出现的彼此接近的单词应该具有相似的向量表示。具体来说，该示例</p><p id="c23e" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated"><em class="lk">嵌入(男人)-嵌入(国王)~嵌入(女人)-嵌入(女王)</em></p><p id="0eb9" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">已经成为单词嵌入捕捉单词语义能力的典型代表。</p><p id="b0f7" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">然而，优化目标不能由明确定义的量来表示。作为比较，考虑使用矩阵分解学习单词嵌入。设D是由<em class="lk"> m </em>个文档和一个<em class="lk"> n </em>个唯一词的词汇组成的文本语料库。我们计算n乘M字，文档矩阵<em class="lk"> M </em>，其中<em class="lk">M【u，v】</em>记录了字<em class="lk"> u </em>在文档<em class="lk"> v </em>中出现的次数，见图1。</p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div class="gh gi ll"><img src="../Images/5c24747ae7716c010aa310c5df678f42.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/1*E0lbHg6SEPjgl7Vb4SpGng.gif"/></div><p class="lt lu gj gh gi lv lw bd b be z dk translated">图1: Word文档矩阵。克里斯托弗·卡尔·克林拍摄。来源:<a class="ae lx" href="https://commons.wikimedia.org/wiki/File:Topic_detection_in_a_document-word_matrix.gif" rel="noopener ugc nofollow" target="_blank">维基媒体</a></p></figure><p id="de8e" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">矩阵分解被定义为</p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div class="gh gi ly"><img src="../Images/1a72aae5a211edd337bd9f35b9122d57.png" data-original-src="https://miro.medium.com/v2/resize:fit:910/format:webp/1*0PjEv6t2hQ2MlQLf4Hu1Ig.png"/></div></figure><p id="f57b" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated"><em class="lk">在下文中，我们将稍微滥用符号，并用u表示单词u及其嵌入向量。</em></p><p id="c724" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">在这种情况下，我们知道对于一个嵌入了<em class="lk"> u </em>的单词，和一个嵌入了<em class="lk"> v </em>的文档，<em class="lk"> u </em>和<em class="lk"> v </em>的内积保存了单词<em class="lk"> u </em>在文档<em class="lk"> v </em>中出现了多少次的信息。嵌入维数<em class="lk"> d </em>越大，逼近性越好。不幸的是，word2vec模型的优化目标没有如此清晰的表述。word2vec中两个字向量的内积到底保存了什么？并且通过增加维度<em class="lk"> d </em>嵌入一定会变得更好吗？</p><p id="e0e0" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">Levy和Goldberg的一篇研究论文回答了这个问题[1]。在这篇文章中，我给出了[1]中的理论结果，然后展示了如何用它们来设计更一般的嵌入类。</p><h2 id="1625" class="lz ma iq bd mb mc md dn me mf mg dp mh kx mi mj mk lb ml mm mn lf mo mp mq iw bi translated">训练单词嵌入:word2vec</h2><p id="a9dc" class="pw-post-body-paragraph ko kp iq kq b kr mr ka kt ku ms kd kw kx mt kz la lb mu ld le lf mv lh li lj ij bi translated">让我们简单考虑一下负采样的word2vec是如何工作的。更全面的描述，我们参考<a class="ae lx" rel="noopener" target="_blank" href="/word2vec-from-scratch-with-numpy-8786ddd49e72">这篇文章</a>。设<em class="lk"> D </em>是由单词、语境对组成的语料库。在word2vec中，单词<em class="lk"> w </em>的上下文被定义为围绕<em class="lk"> w </em>的<em class="lk"> k </em>单词，其中<em class="lk"> k </em>通常是在5和15之间变化的小常数。我们想要学习单词嵌入，使得如果两个单词在语料库中频繁地同时出现，则它们的内积很大。</p><p id="3ff9" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">考虑一个单词<em class="lk"> w </em>并且让<em class="lk"> c </em>是其上下文中的一个单词。对于语料库中一起出现的词对<em class="lk"> (w，c) </em>，我们希望嵌入的内积最大化<em class="lk"> (w，c) </em>确实出现在语料库中的概率(表示为D=1)。概率由sigmoid函数建模:</p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div class="gh gi mw"><img src="../Images/71403ce733964ba32b9ce1ed849f0496.png" data-original-src="https://miro.medium.com/v2/resize:fit:1108/format:webp/1*ZRHCIBscwGWTQwJo6ghjhQ.png"/></div></figure><p id="6a91" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">上面有一个琐碎的解决方案，我们可以简单地把所有的内积任意变大。因此，我们也引入否定对，即在语料库中不共现的对，其目的是:</p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div class="gh gi mx"><img src="../Images/5acfabf89d0126a2997d1598f1139df9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1330/format:webp/1*dsbXxSrAvGSo-MMa_Y21Fw.png"/></div></figure><p id="16b2" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">该算法可以总结如下:</p><pre class="lm ln lo lp gt my mz na nb aw nc bi"><span id="f897" class="lz ma iq mz b gy nd ne l nf ng">Algorithm word2vec</span><span id="6e1d" class="lz ma iq mz b gy nh ne l nf ng">1. Assign a random <em class="lk">d</em>-dimensional vector to each word that appears in the corpus.</span><span id="3f0a" class="lz ma iq mz b gy nh ne l nf ng">2. Traverse the corpus and generate pairs of words that appear in it. These are the positive pairs.</span><span id="7637" class="lz ma iq mz b gy nh ne l nf ng">3. For each positive pair, generate <em class="lk">k</em> random pairs of words. These are the negative pairs.</span><span id="ed32" class="lz ma iq mz b gy nh ne l nf ng">4. Feed the inner products of the embedding vectors of positive and negative pairs into a binary classification model where the learnable parameters are the word embeddings.</span></pre><p id="916a" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">我们在语料库上运行上述算法几个时期，以保证学习过程收敛到最优。</p><h2 id="a103" class="lz ma iq bd mb mc md dn me mf mg dp mh kx mi mj mk lb ml mm mn lf mo mp mq iw bi translated">理论分析</h2><p id="673c" class="pw-post-body-paragraph ko kp iq kq b kr mr ka kt ku ms kd kw kx mt kz la lb mu ld le lf mv lh li lj ij bi translated">固定一个单词<em class="lk"> w </em>并考虑所有出现<em class="lk"> w </em>的配对的目标。设<em class="lk"> #(w，c) </em>为语料库中<em class="lk"> (w，c) </em>对的出现次数。我们可以把目标写成</p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/61f97abd95d7f25be9a59051163f7329.png" data-original-src="https://miro.medium.com/v2/resize:fit:1118/format:webp/1*a9gWZnOY7qEsKYEqXrlW5w.png"/></div></figure><p id="d256" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">其中第二个乘积在我们生成的负对之上。</p><p id="7f28" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">通过取目标的对数并观察每个否定词cN有机会被采样，我们得到:</p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div role="button" tabindex="0" class="nk nl di nm bf nn"><div class="gh gi nj"><img src="../Images/3aeeee6b730c6b4674d88e9c7622f521.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZSZZqGd9s-ExOEU5D_fctA.png"/></div></div></figure><p id="344a" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">我们来解释一下上面的内容。单词<em class="lk"> w </em>是固定的，我们考虑语料库中出现的所有单词上下文对(<em class="lk"> w，c) </em>，并且我们对<em class="lk"> k </em>否定对<em class="lk"> (w，cN) </em>进行采样，使得每个单词c以概率#(c)/|D|被采样。我们希望正对的内积是一个大的正数。对于负对，我们希望内积是一个绝对值很大的负数。</p><p id="bcb8" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">注意，语料库中的对数|D|是恒定的。因此，通过将上述表达式除以|D|,目标变为</p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div role="button" tabindex="0" class="nk nl di nm bf nn"><div class="gh gi no"><img src="../Images/68027f769fa8c6a7c3fa1976c2971b10.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yKZtZOAtrI1H7x9EXUjkIQ.png"/></div></div></figure><p id="8651" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">这已经给我们提供了一些直觉。目标是优化嵌入，使得它们反映正配对被采样的概率，而不是随机采样的配对。正对是以概率产生的</p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div class="gh gi np"><img src="../Images/bae6eeb0a3004862a4a2133f0b22043e.png" data-original-src="https://miro.medium.com/v2/resize:fit:212/format:webp/1*gp7gFwuCIzhOEbMcY5dTuA.png"/></div></figure><p id="91b7" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">对于负对，两个词被独立地随机抽样，每个词都有概率</p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/d8ee66eaed259c53472febd2f1bf57fc.png" data-original-src="https://miro.medium.com/v2/resize:fit:522/format:webp/1*cQKB4BXimevcSNwSUPlQGA.png"/></div></figure><p id="80b9" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">通过将内积设置为未知参数并求解相应的优化问题，我们可以找到内积的最优值:</p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/304288fce9ded493e89bec738d2579ea.png" data-original-src="https://miro.medium.com/v2/resize:fit:1128/format:webp/1*_dNP50tbBR6pP3h3rr6SOA.png"/></div></figure><p id="66b0" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">在上面的P(w，c)是单词对(w，c)出现的概率，P(w)是单词w在语料库中出现的边际概率。以上原来是一个在自然语言处理中广泛使用的词语关联度量，即<a class="ae lx" href="https://en.wikipedia.org/wiki/Pointwise_mutual_information" rel="noopener ugc nofollow" target="_blank"> <em class="lk">【点态互信息(PMI) </em> </a>度量。</p><p id="e2f8" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">这真是太神奇了！事实证明，word2vec本质上相当于矩阵分解，其中矩阵条目是词对之间的PMI得分。自80年代以来，PMI作为一种距离度量被用于NLP相关的任务[2]，远远早于单词嵌入概念的出现。</p><h2 id="5e24" class="lz ma iq bd mb mc md dn me mf mg dp mh kx mi mj mk lb ml mm mn lf mo mp mq iw bi translated">基于任意相似度函数的嵌入</h2><p id="af3a" class="pw-post-body-paragraph ko kp iq kq b kr mr ka kt ku ms kd kw kx mt kz la lb mu ld le lf mv lh li lj ij bi translated">现在很容易看出，我们可以简单地替换采样正负对的概率。我们只需要更新上面给出的word2vec算法中的第二步和第三步:</p><pre class="lm ln lo lp gt my mz na nb aw nc bi"><span id="1782" class="lz ma iq mz b gy nd ne l nf ng">Algorithm sim2vec:</span><span id="e820" class="lz ma iq mz b gy nh ne l nf ng">1. Assign a random <em class="lk">d</em>-dimensional vector to each word that appears in the corpus.</span><span id="243a" class="lz ma iq mz b gy nh ne l nf ng">2. <strong class="mz ja">Generate pairs of words according to a words similarity measure.</strong> These are the positive pairs.</span><span id="4996" class="lz ma iq mz b gy nh ne l nf ng">3. For each positive pair, <strong class="mz ja">generate <em class="lk">k</em> random pairs of words by the independent sampling of word pairs</strong>. These are the negative pairs.</span><span id="ef2d" class="lz ma iq mz b gy nh ne l nf ng">4. Feed the inner products of the embedding vectors of positive and negative pairs into a binary classification model where the learnable parameters are the word embeddings.</span></pre><p id="1b63" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">为什么这很有帮助？这给了我们更多的自由来分配配对的重要性。我们可以变得有创造性，考虑不同的相似性度量。例如，单词之间的Jaccard相似度定义如下:</p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/baa29c9a37db50b11621c641ecef2e0e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1306/format:webp/1*9nI9FaPAfwDGkqjrmeM97w.png"/></div></figure><p id="65e7" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">因此，如果w的存在意味着c也可能出现在文档中，则我们可以学习优化单词w和c彼此相似的目标的嵌入，反之亦然。在这种情况下，这一对(“凯拉”、“奈特莉”)的得分可能会高于(“数据”、“科学”)。目标变成了:</p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/baa96a34c1e7e94ca0bef6bcd6f2b33b.png" data-original-src="https://miro.medium.com/v2/resize:fit:630/format:webp/1*eiylto0Y97tZXn8NsDxJhw.png"/></div></figure><p id="5423" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">我们也可以模拟产生负对的概率。例如，Pr(w)可以是均匀分布，其中所有单词具有相同的被选择概率，而不管它们出现的频率如何。</p><h2 id="489d" class="lz ma iq bd mb mc md dn me mf mg dp mh kx mi mj mk lb ml mm mn lf mo mp mq iw bi translated">从分布中取样</h2><p id="cc89" class="pw-post-body-paragraph ko kp iq kq b kr mr ka kt ku ms kd kw kx mt kz la lb mu ld le lf mv lh li lj ij bi translated">如果我们可以计算并存储所有对(u，v)的相似性，那么根据相似性进行采样就变得简单了:只需将相似性分数作为权重来存储对，并使用类似于<a class="ae lx" href="https://numpy.org/doc/stable/reference/random/generated/numpy.random.choice.html" rel="noopener ugc nofollow" target="_blank"> numpy.random.choice </a>的算法进行采样。然而，这在计算上可能是不可行的。</p><p id="2530" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">有不同的方法来处理具有大量线对的问题。一般来说，我们希望仅使用那些具有高相似性得分的配对作为阳性配对。</p><ul class=""><li id="e83a" class="nu nv iq kq b kr ks ku kv kx nw lb nx lf ny lj nz oa ob oc bi translated">如果您的相似性度量主要基于计数，那么数据的子样本将保留最频繁的对，但许多不频繁的对将被过滤掉。例如，我们可以只考虑语料库中文档的子集。诸如(“数据”、“科学”)之类的常用词对将可能保留下来。但这可能不是这种情况(“凯拉”，“奈特利”)。</li><li id="f407" class="nu nv iq kq b kr od ku oe kx of lb og lf oh lj nz oa ob oc bi translated">对于每个对象，只考虑最近的<em class="lk"> t </em>个邻居。例如，我们可以使用scikit-learn的公开实现，它使用kd-trees这样的算法来加速相似性搜索。这些算法适用于维度不是很高的数据。否则，人们可以考虑像<a class="ae lx" rel="noopener" target="_blank" href="/understanding-locality-sensitive-hashing-49f6d1f6134">这样的方法，本地敏感散列</a>将产生相似的单词。对于像Jaccard相似性这样的度量来说尤其如此。</li></ul><h1 id="d21e" class="oi ma iq bd mb oj ok ol me om on oo mh kf op kg mk ki oq kj mn kl or km mq os bi translated">实用的实现</h1><p id="42d5" class="pw-post-body-paragraph ko kp iq kq b kr mr ka kt ku ms kd kw kx mt kz la lb mu ld le lf mv lh li lj ij bi translated">为了便于说明，我们实现了一个简单的解决方案，用于从文本语料库中学习文档嵌入。这个问题与训练单词嵌入的问题是正交的:我们根据文档包含的单词训练文档的向量表示。</p><p id="b094" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">我们考虑<a class="ae lx" href="https://www.kaggle.com/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews/" rel="noopener ugc nofollow" target="_blank"> IMDB情感分析数据集</a>。该数据集由用户的电影评论组成，每个评论都标有积极或消极的情绪。</p><ul class=""><li id="98a3" class="nu nv iq kq b kr ks ku kv kx nw lb nx lf ny lj nz oa ob oc bi translated">在对文本进行预处理之后，我们使用tf-idf编码将文档转换成向量，这样每个文档</li></ul><pre class="lm ln lo lp gt my mz na nb aw nc bi"><span id="888e" class="lz ma iq mz b gy nd ne l nf ng">from sklearn.feature_extraction.text import TfidfVectorizer<br/>tfv=TfidfVectorizer(min_df=0.00,ngram_range=(1,1))<br/>tf_reviews = tfv.fit_transform(reviews)</span></pre><p id="231b" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">参数min_df表示我们只考虑出现在至少0.1%的文档中的单词。本质上，这防止我们使用可能只出现在几个文档中的非常具体的单词。</p><ul class=""><li id="35b7" class="nu nv iq kq b kr ks ku kv kx nw lb nx lf ny lj nz oa ob oc bi translated">对于每个输入向量，找到其最近的邻居。这可以通过使用现成的软件包来实现，例如scikit-learn的K-nearest neighborhood，它返回以下各项的最近邻居:</li></ul><pre class="lm ln lo lp gt my mz na nb aw nc bi"><span id="f068" class="lz ma iq mz b gy nd ne l nf ng">from sklearn.neighbors import NearestNeighbors<br/>nn_model = NearestNeighbors(n_neighbors=t, algorithm='auto')<br/>nn_model.fit(tf_reviews)<br/>distances, indices = nn_model.kneighbors(tf_reviews[idx])</span></pre><ul class=""><li id="7086" class="nu nv iq kq b kr ks ku kv kx nw lb nx lf ny lj nz oa ob oc bi translated">计算生成的<em class="lk"> n*t </em>正对的相似度，在一个数组中对它们进行排序，并使用numpy.random.choice()根据它们的权重进行采样:</li></ul><pre class="lm ln lo lp gt my mz na nb aw nc bi"><span id="7538" class="lz ma iq mz b gy nd ne l nf ng">eps = 1e-3<br/>weights = [(1.0/(d+eps))**2 for d in distances[0][1:]]<br/>weights = [w/sum(weights) for w in weights]<br/>pos = np.random.choice(indices[0][1:], nr_pos_samples, p=weights)<br/>neg = np.random.choice(nr_reviews, nr_neg_samples)</span></pre><ul class=""><li id="da06" class="nu nv iq kq b kr ks ku kv kx nw lb nx lf ny lj nz oa ob oc bi translated">使用Keras生成器生成正负对:</li></ul><pre class="lm ln lo lp gt my mz na nb aw nc bi"><span id="6e62" class="lz ma iq mz b gy nd ne l nf ng">class PairsGen(tf.keras.utils.Sequence):<br/>    <br/>    def __init__(self, data, nr_pos, nr_neg, ...):<br/>        <br/>        """Initialization<br/>        :param data: the sparse matrix containing <br/>        :param nr_pos: the number of positive samples per word<br/>        :param nr_neg: the number of negative samples per pos. pair<br/>        ...<br/>        """<br/>        ...</span><span id="706f" class="lz ma iq mz b gy nh ne l nf ng">    def __getitem__(self, idx):<br/>       """<br/>       Sample self.nr_pos word pairs and self.nr_neg word pairs<br/>       """<br/>       .....</span></pre><ul class=""><li id="0a3f" class="nu nv iq kq b kr ks ku kv kx nw lb nx lf ny lj nz oa ob oc bi translated">将生成的对输入到具有嵌入层、计算内积的点层和具有sigmoid激活函数的输出层的浅层神经网络中:</li></ul><pre class="lm ln lo lp gt my mz na nb aw nc bi"><span id="d628" class="lz ma iq mz b gy nd ne l nf ng">Layer (type)                    Output Shape         Param #     Connected to                     <br/>==================================================================================================<br/>input_2 (InputLayer)            [(None, 2)]          0                                            <br/>__________________________________________________________________________________________________<br/>context_embedding (Embedding)   (None, 50)           2500000      tf.__operators__.getitem_3[0][0] <br/>__________________________________________________________________________________________________<br/>word_embedding (Embedding)      (None, 50)           2500000      tf.__operators__.getitem_2[0][0] <br/>__________________________________________________________________________________________________<br/>dot_1 (Dot)                     (None, 1)            0           context_embedding[0][0]          <br/>                                                                 word_embedding[0][0]             <br/>__________________________________________________________________________________________________<br/>flatten_1 (Flatten)             (None, 1)            0           dot_1[0][0]                      <br/>__________________________________________________________________________________________________<br/>dense_1 (Dense)                 (None, 1)            2           flatten_1[0][0]                  <br/>=====================================================================<br/>Total params: 5,000,002<br/>Trainable params: 5,000,002<br/>Non-trainable params: 0</span></pre><p id="0b9b" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">上述方法将训练嵌入:</p><pre class="lm ln lo lp gt my mz na nb aw nc bi"><span id="a190" class="lz ma iq mz b gy nd ne l nf ng">model.compile(optimizer='adam', loss= 'binary_crossentropy', <br/>              metrics=['accuracy', 'AUC'])<br/>model.fit(generator, epochs=10)<br/>Epoch 1/10<br/>10000/10000 [==============================] - 153s 15ms/step - loss: 0.6354 - accuracy: 0.6667 - auc: 0.5359<br/>Epoch 2/10<br/>10000/10000 [==============================] - 152s 15ms/step - loss: 0.6129 - accuracy: 0.6868 - auc: 0.6151<br/>Epoch 3/10<br/>10000/10000 [==============================] - 151s 15ms/step - loss: 0.5680 - accuracy: 0.7292 - auc: 0.6963<br/>  ...........<br/>Epoch 10/10<br/>10000/10000 [==============================] - 151s 15ms/step - loss: 0.3891 - accuracy: 0.8323 - auc: 0.8865</span></pre><p id="4a3b" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">然后，我们可以提取每个单词的嵌入层，并对文档进行聚类(类似于图1中的gif所示)。我们观察到两个集群中的情感分布非常不同:</p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div class="gh gi ll"><img src="../Images/d4eec5304e487ec02e60278ab93ca2cf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/1*f7t1AI6MFxsn_qJTvMi6Pw.png"/></div><p class="lt lu gj gh gi lv lw bd b be z dk translated">两个集群中的情感分布。图片作者。</p></figure><h2 id="258a" class="lz ma iq bd mb mc md dn me mf mg dp mh kx mi mj mk lb ml mm mn lf mo mp mq iw bi translated">密码</h2><p id="4488" class="pw-post-body-paragraph ko kp iq kq b kr mr ka kt ku ms kd kw kx mt kz la lb mu ld le lf mv lh li lj ij bi translated">上述内容的Python实现可在以下网站公开获得:<a class="ae lx" href="https://github.com/konstantinkutzkov/sim2vec" rel="noopener ugc nofollow" target="_blank">https://github.com/konstantinkutzkov/sim2vec</a></p></div><div class="ab cl ot ou hu ov" role="separator"><span class="ow bw bk ox oy oz"/><span class="ow bw bk ox oy oz"/><span class="ow bw bk ox oy"/></div><div class="ij ik il im in"><p id="a4e8" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">[1]奥马尔·利维，约夫·戈德堡。<a class="ae lx" href="https://papers.nips.cc/paper/2014/file/feab05aa91085b7a8012516bc3533958-Paper.pdf" rel="noopener ugc nofollow" target="_blank">神经字嵌入为隐式矩阵分解</a>。国家实施计划2014年:2177-2185</p><p id="97be" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">[2]肯尼斯·沃德·丘奇和帕特里克·汉克斯<a class="ae lx" href="https://aclanthology.org/P89-1010.pdf" rel="noopener ugc nofollow" target="_blank">词语联想规范、互信息和词典编纂</a>。计算语言学，16(1):22–29，1990。</p></div></div>    
</body>
</html>