<html>
<head>
<title>A deep dive into partitioning around medoids</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">深入探究水母周围的分区</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/a-deep-dive-into-partitioning-around-medoids-a77d9b888881?source=collection_archive---------13-----------------------#2021-08-20">https://towardsdatascience.com/a-deep-dive-into-partitioning-around-medoids-a77d9b888881?source=collection_archive---------13-----------------------#2021-08-20</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="fa65" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsdatascience.com/tagged/hands-on-tutorials" rel="noopener" target="_blank">实践教程</a></h2><div class=""/><div class=""><h2 id="0809" class="pw-subtitle-paragraph jw iz iq bd b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn dk translated">k-means的最终进化以及为什么你之前可能学错了</h2></div><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ko"><img src="../Images/32dd91a9b17ad7ceb77786c603643f41.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*02TF9l458bkpjtv7MOntCQ.jpeg"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated"><a class="ae le" href="https://unsplash.com/@alinnnaaaa?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">阿丽娜·格鲁布尼亚</a>在<a class="ae le" href="https://unsplash.com/photos/oyXis2kALVg?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="9011" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">在我作为一名数据科学家的工作中，我经常遇到简单算法不够用的问题，因为它们陷入了局部最优。这通常会导致开发过程中的许多挫折，因为您首先认为您的方法是有效的，但随后发现它并不一致，或者不是对您的所有数据集都有效。在我的k-means及其变体系列的最后一篇文章中，我将讨论k-medoids算法，通常也称为围绕medoids划分(PAM)。它具有基本确定性的优点，能够可靠地找到非常好的解决方案。这确实以更高的计算成本为代价，但是如果您的数据集不是非常大，如果您需要可靠的结果，它仍然是一个很好的候选。像往常一样，你也可以在我的<a class="ae le" href="https://github.com/MSHelm/algorithms-from-scratch" rel="noopener ugc nofollow" target="_blank"> GitHub </a>上找到这篇文章的所有代码。</p></div><div class="ab cl mb mc hu md" role="separator"><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg"/></div><div class="ij ik il im in"><h1 id="75c3" class="mi mj iq bd mk ml mm mn mo mp mq mr ms kf mt kg mu ki mv kj mw kl mx km my mz bi translated">k-medoids的直觉</h1><p id="6f9b" class="pw-post-body-paragraph lf lg iq lh b li na ka lk ll nb kd ln lo nc lq lr ls nd lu lv lw ne ly lz ma ij bi translated">与k-中位数一样，k-medoids也通常使用曼哈顿度量，但中心现在总是数据集中的实际点。我们现在计算中点，而不是质心，因此也就是中面。这增加了该方法的可解释性，因为数据中的代表性点总是可以被检索到。这经常与k-medians混淆(你可以在下面的文章中查看)，在k-medians中，中心点不需要是一个实际的对象。</p><div class="nf ng gp gr nh ni"><a rel="noopener follow" target="_blank" href="/use-this-clustering-method-if-you-have-many-outliers-5c99b4cd380d"><div class="nj ab fo"><div class="nk ab nl cl cj nm"><h2 class="bd ja gy z fp nn fr fs no fu fw iz bi translated">如果有许多异常值，请使用这种聚类方法</h2><div class="np l"><h3 class="bd b gy z fp nn fr fs no fu fw dk translated">稳健结果的k-中位数变化</h3></div><div class="nq l"><p class="bd b dl z fp nn fr fs no fu fw dk translated">towardsdatascience.com</p></div></div><div class="nr l"><div class="ns l nt nu nv nr nw ky ni"/></div></div></a></div><p id="c737" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">考虑以下示例集群，由二维的5个点组成:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/6a89d118d358147b6ae37af0ba093ab4.png" data-original-src="https://miro.medium.com/v2/resize:fit:326/format:webp/1*nLLLF_SEG5QQurVNqLDjfA.png"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">作者图片</p></figure><p id="1a09" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">因为中位数是在k-中位数中为每个维度单独计算的，所以中位数将是x = 3，y = 3。但是在数据集中不存在点(3，3)。</p><p id="817e" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">针对k-medoids实现了多种算法，最常见的算法还是Lloyd风格的算法(也称为Voronoi迭代)和围绕medoids的真实划分(PAM)。不幸的是，Lloyd风格的算法通常也被称为PAM，但这是不正确的，因为PAM的构建阶段(我们将在后面看到)与Lloyd非常不同。真正PAM的构建阶段是该算法成功的关键步骤，这也是为什么Llody风格的k-medoids通常比PAM得到更差的结果。</p></div><div class="ab cl mb mc hu md" role="separator"><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg"/></div><div class="ij ik il im in"><h1 id="db80" class="mi mj iq bd mk ml mm mn mo mp mq mr ms kf mt kg mu ki mv kj mw kl mx km my mz bi translated">k-medoids劳埃德风格</h1><p id="4796" class="pw-post-body-paragraph lf lg iq lh b li na ka lk ll nb kd ln lo nc lq lr ls nd lu lv lw ne ly lz ma ij bi translated">为了简单起见，让我们首先以Lloyd风格实现k-medoids，然后在此基础上构建真正的PAM。像往常一样，我们首先随机初始化中心，<br/>,但是现在中心的更新是完全不同的。</p><p id="d82f" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">更新步骤现在称为交换阶段。顾名思义，我们考虑将当前的medoid与其簇中的所有其他点进行交换。对于每个候选交换，我们计算总成本，即该簇中所有点到新medoid的距离之和。我们会记住所有成本较低且表现最佳的互换，即成本最低的互换。</p><p id="cadd" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">如果成本不能再降低，则算法终止。请注意，这并不意味着我们达到了一个全球最小值。因为我们只执行降低成本的步骤，所以如果算法没有在全局最小值“谷”内初始化，它就没有办法走出局部最小值并进入全局最小值。</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="ny nz l"/></div></figure></div><div class="ab cl mb mc hu md" role="separator"><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg"/></div><div class="ij ik il im in"><h1 id="59bd" class="mi mj iq bd mk ml mm mn mo mp mq mr ms kf mt kg mu ki mv kj mw kl mx km my mz bi translated">围绕水母的分割(PAM)</h1><p id="c28e" class="pw-post-body-paragraph lf lg iq lh b li na ka lk ll nb kd ln lo nc lq lr ls nd lu lv lw ne ly lz ma ij bi translated">最后是PAM算法。正如我之前已经暗示的，它有一个独特的构建阶段，可以确保非常好的初始化。下面的交换阶段与我们之前在Lloyd风格的k-medoids中实现的相同。</p><p id="d841" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">在构建阶段，选择第一个medoid作为具有最小成本的med oid，成本是到所有其他点的所有距离的总和。因此，第一个点是数据集的最中心点。</p><p id="8170" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">然后迭代地选择所有进一步的点。对于所有非中面体，我们计算选择该点作为下一个中面体的成本(也是从候选中面体到所有其他非中面体的距离之和)，然后选择具有最小成本的一个作为下一个中面体。</p><p id="d7c0" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">为了澄清这确实是真正PAM算法，您可以在这里查阅最初发明它的作者的<br/>论文或书籍。</p><p id="3488" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">正如人们立即看到的，这是计算昂贵的执行。在我们的实现中，我们将在每次迭代中计算所有的距离，一个不太昂贵的解决方案是只计算一次距离矩阵(并且只计算一个三角形，因为它是对称的)，然后只根据成本计算的需要对其进行索引。</p><p id="5a7b" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">这种算法的优点是详尽的构建阶段通常已经达到了非常好的聚类。在收敛之前，下面的交换阶段通常只执行几次。作者甚至指出，有时甚至可以忽略它，但仍然可以获得良好的分区。</p><p id="2878" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">在Lloyd style k-medoids和true PAM之间的交换阶段也有一些差异:Lloyd只考虑同一个集群内的交换，而PAM考虑所有当前的非medoids进行潜在的交换，而不管它们当前是否在同一个集群内。这增加了PAM的搜索空间，并有可能使它找到更好的解决方案。</p><p id="c754" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">PAM的另一个特性是它接近于确定性的，<br/>因为它在初始化时不使用随机元素，并且总是将所有点视为可能的下一个medoids。由于所考虑的两个medoids的成本之间可能存在联系，因此根据这些联系的解决方式，该算法并不是100%确定的(即，可以随机解决联系，或者根据点出现的顺序来解决)。)</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="ny nz l"/></div></figure></div><div class="ab cl mb mc hu md" role="separator"><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg"/></div><div class="ij ik il im in"><h1 id="4850" class="mi mj iq bd mk ml mm mn mo mp mq mr ms kf mt kg mu ki mv kj mw kl mx km my mz bi translated">算法之间的比较</h1><p id="1aa1" class="pw-post-body-paragraph lf lg iq lh b li na ka lk ll nb kd ln lo nc lq lr ls nd lu lv lw ne ly lz ma ij bi translated">在整个系列中，我们已经实现了许多不同的算法，让我们就运行时间和结果对它们进行一些比较。因为我们在base R中实现了所有东西，而没有利用矢量化，所以运行时间将比使用C或FORTRAN中构建的优化算法长得多。</p><h2 id="629c" class="oa mj iq bd mk ob oc dn mo od oe dp ms lo of og mu ls oh oi mw lw oj ok my iw bi translated">聚类结果</h2><p id="78ff" class="pw-post-body-paragraph lf lg iq lh b li na ka lk ll nb kd ln lo nc lq lr ls nd lu lv lw ne ly lz ma ij bi translated">让我们从可视化结果开始。当然,“相同”聚类的颜色在不同的算法之间可以不同，因为它们不知道哪个聚类属于哪个物种。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ol"><img src="../Images/bcbf1a3578a685e39edfbcce81114281.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Tu_cQEqyKafIdnI5rrS1TA.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">iris数据集上所有k均值变量的比较。所有算法都能找到大致正确的分割。请注意，颜色不需要匹配，因为它们彼此独立运行。图片作者。</p></figure><p id="20bd" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">除了k-中值之外，大多数算法都能找到或多或少正确的聚类。我们还看到PAM算法实际上根本不执行任何交换，这突出了它在构建阶段的优势！<br/>还要记住，如果比较PAM和Lloyd k-medoids之间的迭代次数，PAM每次迭代只执行一次交换，而Lloyd k-medoids对每个当前medoids执行一次交换，因此总交换次数为k *次迭代。</p><p id="6543" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">如果您想更多地了解可以用哪些客观指标来判断您的聚类结果，请查看我的文章中关于k-means:</p><div class="nf ng gp gr nh ni"><a rel="noopener follow" target="_blank" href="/a-deep-dive-into-k-means-f9a1ef2490f8"><div class="nj ab fo"><div class="nk ab nl cl cj nm"><h2 class="bd ja gy z fp nn fr fs no fu fw iz bi translated">对k-means的深入探究</h2><div class="nq l"><p class="bd b dl z fp nn fr fs no fu fw dk translated">towardsdatascience.com</p></div></div><div class="nr l"><div class="om l nt nu nv nr nw ky ni"/></div></div></a></div><h2 id="15d8" class="oa mj iq bd mk ob oc dn mo od oe dp ms lo of og mu ls oh oi mw lw oj ok my iw bi translated">运行时间</h2><p id="764d" class="pw-post-body-paragraph lf lg iq lh b li na ka lk ll nb kd ln lo nc lq lr ls nd lu lv lw ne ly lz ma ij bi translated">最后，让我们比较不同算法的运行时间，让我们也<br/>检查一下从R到FORTRAN的实现速度有多快:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi on"><img src="../Images/8c70548cd74991f03fe5e063eedf79d0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CsOWXGxq6nVrKdooSjf84A.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">我们实现的不同算法的运行时间比较。PAM的运行时间最高，其次是Lloyd风格的k-medoids。图片作者。</p></figure><p id="0b31" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">正如所料，PAM是最慢的算法，其次是Lloyd风格的k-medoids。因为其他线在标尺上非常接近，所以让我们来看看比率:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ol"><img src="../Images/bf30749d04b6a520dad77df564b6fada.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*F5Cm0xX262_n_juGlfiJSg.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">对数级运行时间的比较。与我在R. Image by author中的实现相比，我们可以观察到在FORTRAN中实现的基本k-means有很大的不同。</p></figure><p id="1211" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">我们的普通k均值算法比基本k均值算法慢4000倍！这个<br/>演示了如果用C++这样的低级语言更有效地实现一个算法，你可以获得巨大的性能增益。但是我们的目标不是效率，而是理解。</p><h1 id="f97e" class="mi mj iq bd mk ml oo mn mo mp op mr ms kf oq kg mu ki or kj mw kl os km my mz bi translated">摘要</h1><p id="f0c4" class="pw-post-body-paragraph lf lg iq lh b li na ka lk ll nb kd ln lo nc lq lr ls nd lu lv lw ne ly lz ma ij bi translated">恭喜你能走到这一步。有了PAM，您现在知道了一种非常复杂的聚类方法，它可以稳健地应用于许多数据集。由于其高计算成本，它可能不完全适合非常大的数据集。如果你是这种情况，那就去看看专门为此设计的算法，比如CLARA或CLARANS。这篇文章也总结了我关于k-means和相关聚类算法的迷你系列。请继续关注接下来的节目！</p></div></div>    
</body>
</html>