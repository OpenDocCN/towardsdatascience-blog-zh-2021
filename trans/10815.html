<html>
<head>
<title>Mathematical Understanding of Bias Variance Tradeoff</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">偏差方差权衡的数学理解</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/mathematical-understanding-of-bias-variance-tradeoff-9366dbc8439a?source=collection_archive---------24-----------------------#2021-10-19">https://towardsdatascience.com/mathematical-understanding-of-bias-variance-tradeoff-9366dbc8439a?source=collection_archive---------24-----------------------#2021-10-19</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="6986" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">从方程理解偏差-方差权衡</h2></div><p id="5e78" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们中的许多人已经在人工智能文献的不同地方读到了偏差和方差，但仍然有许多人努力用数学方程来解释它。每当人们建立一个模型时，他们总是对偏差-方差进行评论，以确定该模型是否可以在现实世界中使用以及其性能有多好。在本文中，我们将关注描述偏差-方差的数学方程，并尝试从数学角度理解这个方程的不同部分。让我首先强调一些对理解这个等式至关重要的假设:</p><h2 id="53a2" class="lb lc iq bd ld le lf dn lg lh li dp lj ko lk ll lm ks ln lo lp kw lq lr ls lt bi translated">假设</h2><ol class=""><li id="5e13" class="lu lv iq kh b ki lw kl lx ko ly ks lz kw ma la mb mc md me bi translated">我们在训练模型时使用假设集H。</li><li id="1806" class="lu lv iq kh b ki mf kl mg ko mh ks mi kw mj la mb mc md me bi translated">有一个实际的目标函数f，我们试图使用学习算法和数据集d来逼近它。这个函数f总是未知的。</li><li id="3bae" class="lu lv iq kh b ki mf kl mg ko mh ks mi kw mj la mb mc md me bi translated">当我们获取一个数据集D并对其应用学习算法时，我们将得到一个预测函数g，它试图逼近实际的目标函数f。</li><li id="8d6f" class="lu lv iq kh b ki mf kl mg ko mh ks mi kw mj la mb mc md me bi translated">有一个数据点x属于数据集d。</li></ol><p id="34a1" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们将在下面的等式中使用上述假设。</p><p id="d0d5" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">由于本文的主要重点是从数学角度理解偏差-方差方程，我们将直接查看该方程，而不深入其背后的推导过程(如果您想要理解推导过程，请查看参考文献)。</p><p id="52ff" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在深入解释之前，我们先来看看偏差和方差的等式:</p><figure class="ml mm mn mo gt mp gh gi paragraph-image"><div role="button" tabindex="0" class="mq mr di ms bf mt"><div class="gh gi mk"><img src="../Images/e4d04f79f4d368f95512b87d21919c48.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XbWd-4156MX_j_OuIAURkg.jpeg"/></div></div><p class="mw mx gj gh gi my mz bd b be z dk translated">作者图片</p></figure><h2 id="aad7" class="lb lc iq bd ld le lf dn lg lh li dp lj ko lk ll lm ks ln lo lp kw lq lr ls lt bi translated">说明</h2><p id="3522" class="pw-post-body-paragraph kf kg iq kh b ki lw jr kk kl lx ju kn ko na kq kr ks nb ku kv kw nc ky kz la ij bi translated">在上面的等式中，我们假设我们正在查看一个数据点x并找出错误。现在为了理解右手边，我们使用上面提到的假设。假设我们有一个假设集H(它表示我们在训练时可能会用到的函数类)。现在，假设我们通过获取数据集d并对其运行经验风险最小化(使用优化算法训练模型以最小化经验损失)来应用学习算法，以获得预测器函数g。在上面的等式中，g^D(x表示g明确地附属于数据集d，因为对于数据集d的不同实现，我们将获得不同的预测器函数。例如:假设我们有一个数据集D的10种不同实现:D1，D2，D3，…..，D10。现在，当我们取其中每一个并运行经验风险最小化时，我们将分别得到不同的预测值g1，g2，g3，…，g10。请注意，我只取了10个数据集，对应于此，有10个不同的预测器，只是为了解释，但可以有无限多的实现。</p><p id="3f87" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在想象我们有许多不同的预测值，用g_i表示，我们取它们的平均值得到g^bar(x).从数学上来说，g^bar(x)可以显示为比其他学习到的预测器更好，并且更接近实际的目标函数f。现在，上述等式中右手侧的第一部分简单地表示预测器函数g^D(x).的方差通过数据集d的单一实现(这简单地意味着我们明确拥有或给予我们的数据集),我们将获得一些预测值g^D(x，并且我们测量不同预测值在简单指示方差的平均预测值g^bar(x周围的分布。</p><p id="fa7c" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">理解上述等式的第二部分非常简单，我们试图测量我们的最佳平均预测器g^bar(x的预测相对于实际目标函数f(x)有多远。这种差异表明我们对数据点x的预测有偏差。</p><p id="6baf" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">实际上，当我们训练一个模型并测量误差时，我们永远无法访问这个平均预测值以及实际的目标函数，我们所拥有的是一些数据集，我们试图使用这些数据集来训练模型和一些基于我们建模决策的假设集。假设我们选择假设集作为神经网络来训练给定的数据集。训练之后，我们将得到一些预测函数g^D(i.e.，一种神经网络，使用它我们可以测量测试集中数据点的误差。从理论上讲，无论我们测量什么样的误差，都会有一部分归因于偏差，一部分归因于方差，还有一些不可约误差没有显示在上述等式中。</p><p id="7bea" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">看了上面的等式后，我们可以很快地将过拟合等概念联系起来，每当我们看到训练误差很低但测试误差很高时，我们很快就会说这是过拟合，这是低偏差和高方差的情况。这里，低偏差表示平均预测值非常接近实际目标函数f，而高方差表示我们的假设集中的预测值相对于平均预测值(即g^bar(x)过于分散，这意味着不同的数据集I将获得不同的预测值，并且它们彼此不接近。因此，为了解决这种情况，我们很快开始应用正则化技术，该技术通过对模型参数施加约束(尝试将L1和L2正则化公式联系起来),在理论上减小了假设集的大小。直观地说，由于这些约束，随着假设集的减少，方差开始减少，但偏差又开始增加。偏差增加的原因是，由于我们有一个受限制的假设集，平均预测值现在将会改变，并且将只依赖于存在于减少的假设集中的那些预测值，由于这些预测值，它与实际目标函数f的接近度可能会降低，这将导致偏差增加。</p><h2 id="31f8" class="lb lc iq bd ld le lf dn lg lh li dp lj ko lk ll lm ks ln lo lp kw lq lr ls lt bi translated">注意</h2><p id="29c9" class="pw-post-body-paragraph kf kg iq kh b ki lw jr kk kl lx ju kn ko na kq kr ks nb ku kv kw nc ky kz la ij bi translated">当你在文献中看到偏差-方差方程时，对数据点x有另一个期望，它只是表明我们报告了测试集中所有数据点的期望误差，但在本文中，我们只是试图理解关于一个数据点的方程。</p><p id="1437" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如果您想更深入地了解这个等式是如何推导出来的，请随意查看参考资料:</p><h2 id="799d" class="lb lc iq bd ld le lf dn lg lh li dp lj ko lk ll lm ks ln lo lp kw lq lr ls lt bi translated">参考</h2><p id="b9ed" class="pw-post-body-paragraph kf kg iq kh b ki lw jr kk kl lx ju kn ko na kq kr ks nb ku kv kw nc ky kz la ij bi translated"><a class="ae nd" href="https://www.youtube.com/watch?v=zrEyxfl2-a8&amp;list=PLD63A284B7615313A&amp;index=8" rel="noopener ugc nofollow" target="_blank">https://www.youtube.com/watch?v=zrEyxfl2-a8&amp;list = PLD 63 a 284 b 7615313 a&amp;index = 8</a></p></div></div>    
</body>
</html>