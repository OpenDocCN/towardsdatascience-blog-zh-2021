<html>
<head>
<title>How I achieved 90% accuracy on a text classification problem with ZERO preprocessing</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">我如何在一个没有预处理的文本分类问题上达到90%的准确率</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/how-i-achieved-90-accuracy-on-a-text-classification-problem-with-zero-preprocessing-6acfa96e8d2e?source=collection_archive---------7-----------------------#2021-03-27">https://towardsdatascience.com/how-i-achieved-90-accuracy-on-a-text-classification-problem-with-zero-preprocessing-6acfa96e8d2e?source=collection_archive---------7-----------------------#2021-03-27</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="24d4" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">使用Spark NLP充分利用BERT句子嵌入的能力</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/4345e3459f2141f08757842324a2ea43.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rSS5LYxOox9RFs20xLieOw.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">布雷特·乔丹在<a class="ae ky" href="https://unsplash.com/s/photos/symbol?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="a1ea" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在之前的<a class="ae ky" rel="noopener" target="_blank" href="/glove-elmo-bert-9dbbc9226934">帖子</a>中，我展示了不同的单词嵌入(GloVe，ELMo，BERT)如何用于文本分类任务。我们看到了捕捉上下文对于最大化准确性的重要性。</p><p id="d75b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们还探索了几种预处理文本以改善结果的方法。与此同时，我们想象了每一步是如何改变我们的原始文本的。</p><p id="535d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">今天我们把一切都扔出窗外！我将演示如何在没有任何预处理的情况下达到90%的分类准确率。你准备好了吗？我想保持这一个简短和甜蜜，但你可以在这里找到我的完整笔记本<a class="ae ky" href="https://github.com/ryancburke/AG_news" rel="noopener ugc nofollow" target="_blank"/>。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h2 id="09f9" class="mc md it bd me mf mg dn mh mi mj dp mk li ml mm mn lm mo mp mq lq mr ms mt mu bi translated">数据集</h2><p id="6a38" class="pw-post-body-paragraph kz la it lb b lc mv ju le lf mw jx lh li mx lk ll lm my lo lp lq mz ls lt lu im bi translated">我选择使用<a class="ae ky" href="http://groups.di.unipi.it/~gulli/AG_corpus_of_news_articles.html" rel="noopener ugc nofollow" target="_blank"> AG news </a>基准数据集。我从约翰·斯诺实验室的<a class="ae ky" href="https://github.com/JohnSnowLabs/spark-nlp-workshop/tree/master/tutorials/Certification_Trainings/Public/data" rel="noopener ugc nofollow" target="_blank">中恢复了<em class="na">训练</em>和<em class="na">测试</em>测试(所有NLP的东西都必须看参考)。该数据集分为四个平衡的类别，共有120，000行，如下所示。</a></p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nb nc l"/></div></figure><p id="21b1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">数据集被格式化为两列，类别和描述。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nb nc l"/></div></figure><h2 id="fa2a" class="mc md it bd me mf mg dn mh mi mj dp mk li ml mm mn lm mo mp mq lq mr ms mt mu bi translated">如何在没有预处理的情况下获得90%的准确率</h2><p id="d63f" class="pw-post-body-paragraph kz la it lb b lc mv ju le lf mw jx lh li mx lk ll lm my lo lp lq mz ls lt lu im bi translated">因为我希望这是一个简洁的帖子，所以我会让你参考我以前的<a class="ae ky" rel="noopener" target="_blank" href="/glove-elmo-bert-9dbbc9226934">文章</a>来了解如何在Colab中使用Spark NLP。这只是几段代码。或者，你可以在这里查看我关于那个项目的笔记本。</p><p id="dcd3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">作为一个简短的回顾，为了将句子嵌入放在有多酷的环境中，考虑下面我用于GloVe、ELMo和BERT单词嵌入的预处理步骤的概要:</p><ul class=""><li id="42bf" class="nd ne it lb b lc ld lf lg li nf lm ng lq nh lu ni nj nk nl bi translated">将原始文本转换为文档</li><li id="4ce8" class="nd ne it lb b lc nm lf nn li no lm np lq nq lu ni nj nk nl bi translated">将文档标记化以将其分解成单词</li><li id="9515" class="nd ne it lb b lc nm lf nn li no lm np lq nq lu ni nj nk nl bi translated">规范化标记以删除标点符号</li><li id="6ec8" class="nd ne it lb b lc nm lf nn li no lm np lq nq lu ni nj nk nl bi translated">删除停用词</li><li id="507d" class="nd ne it lb b lc nm lf nn li no lm np lq nq lu ni nj nk nl bi translated">将剩余的单词简化为它们的引理</li><li id="e1e7" class="nd ne it lb b lc nm lf nn li no lm np lq nq lu ni nj nk nl bi translated">然后我可以创建单词嵌入</li></ul><p id="7a14" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">使用<a class="ae ky" href="https://nlp.johnsnowlabs.com/2020/08/25/sent_bert_base_cased.html" rel="noopener ugc nofollow" target="_blank"> BERT </a>句子嵌入，唯一需要的步骤是将原始文本转换成文档。完整的管道可以在下面看到。在第一个块中，您可以看到<strong class="lb iu">描述</strong>列中的文本被使用<em class="na">文档组装器转换为<strong class="lb iu">文档</strong>。</em>这个<strong class="lb iu">文档</strong>列然后被用作<strong class="lb iu"> BERT句子嵌入</strong>的输入。最后，这些嵌入被用作<em class="na">分类器的输入。</em>就是这样！</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nb nc l"/></div></figure><h2 id="85b4" class="mc md it bd me mf mg dn mh mi mj dp mk li ml mm mn lm mo mp mq lq mr ms mt mu bi translated">结果</h2><p id="cfb2" class="pw-post-body-paragraph kz la it lb b lc mv ju le lf mw jx lh li mx lk ll lm my lo lp lq mz ls lt lu im bi translated">下面，您将找到一个示例，展示该模型如何在文本子集上执行。<strong class="lb iu">类别</strong>列有标签，而<strong class="lb iu">结果</strong>列有预测类别。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nb nc l"/></div></figure><p id="228f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然后，我们可以使用scikit-learn来计算我们的指标。如下图所示，总体<strong class="lb iu">准确率为90% </strong>！</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nb nc l"/></div></figure><h2 id="4632" class="mc md it bd me mf mg dn mh mi mj dp mk li ml mm mn lm mo mp mq lq mr ms mt mu bi translated">我们今天学了什么？</h2><p id="eb31" class="pw-post-body-paragraph kz la it lb b lc mv ju le lf mw jx lh li mx lk ll lm my lo lp lq mz ls lt lu im bi translated">我想和你分享一个简单而强大的工具来添加到你的NLP工具包中。没有任何预处理，这可能是相当耗时的，BERT句子嵌入用于获得我们的4个类别的优秀分类。</p><p id="1cb7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我把这个保持简短，只是为了向你介绍句子嵌入的力量。试试看，让我知道你的想法！</p></div></div>    
</body>
</html>