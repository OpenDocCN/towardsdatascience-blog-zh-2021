<html>
<head>
<title>About Sort in Spark 3.x</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">关于Spark 3.x中的排序</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/about-sort-in-spark-3-x-f3699cc31008?source=collection_archive---------11-----------------------#2021-06-27">https://towardsdatascience.com/about-sort-in-spark-3-x-f3699cc31008?source=collection_archive---------11-----------------------#2021-06-27</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="d89c" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">深入探究Spark SQL中的数据排序。</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/1e95e719b80e8be200731091379cd906.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UtT327LTu62cqTJ1PK-brg.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">由<a class="ae ky" href="https://unsplash.com/@iorni?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">iorni.com</a>在Unsplash上拍摄的照片</p></figure><p id="22f4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">排序数据是许多应用程序、ETL过程或各种数据分析中需要的非常重要的转换。Spark提供了几个函数来根据用户的特定用例对数据进行排序。在本文中，我们将描述这些函数，并进一步了解sort是如何工作的，以及它的结果是什么。我们还将运行一个简单的实验，看看排序对文件系统中已创建文件大小的影响。</p><h1 id="375e" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">对整个数据帧进行排序</h1><p id="92c3" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">对于整个数据帧的排序，有两个等价的函数<em class="ms"> orderBy() </em>和<em class="ms"> sort() </em>。它们之间真的没有什么区别，所以你会用哪一个真的是你个人喜好的问题。关于这些函数，需要了解的重要一点是，它们会导致混乱，因为数据需要在集群上重新组织才能达到所需的顺序。</p><p id="8241" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">此外，这些排序函数是转换，所以它们是懒惰的，不直接触发作业。然而，当您运行一个作业时——通过调用诸如<em class="ms"> write </em>之类的动作——您会注意到Spark运行了由排序引起的另一个作业。</p><p id="a8f3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">从3.0版本开始，发现它变得更加困难，因为启用了自适应查询执行(AQE)功能，并且它为每个阶段运行不同的作业，因此现在一个操作触发更多作业是很常见的事情。但是无论AQE是否启用，排序都将触发一个额外的作业，因为Spark需要估计排序列中数据的分布，以确定如何在分区之间创建边界。让我们看一个特殊的例子:</p><pre class="kj kk kl km gt mt mu mv mw aw mx bi"><span id="f3d8" class="my lw it mu b gy mz na l nb nc">(<br/>    df.orderBy('creation_date')<br/>    .write<br/>    .mode('overwrite')<br/>    .format('noop')<br/>    .save()<br/>)</span></pre><p id="b1b7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">运行上述查询后，如果我们检查Spark UI，我们将看到为该查询生成了两个作业。第一个作业(id为2)负责估计<em class="ms"> creation_date </em>列中的分布，并试图将其分成200个区间。这个数字200是由内部配置设置<em class="ms">spark . SQL . shuffle . partitions</em>给出的，可以更改为不同的数字。这也是下一个作业(id为3)有208个任务的原因，因为它分两个阶段运行，其中第一个阶段有8个任务，第二个阶段在重新分区后有200个任务:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nd"><img src="../Images/980dd437f81e1b51fdac0acc298a78b0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pDOAqYJyzdK6-kUroGryUA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><p id="3ba5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">关于数据排序的信息也可以从查询计划中推断出来。它由<em class="ms">排序</em>操作符表示，如果您在Spark UI的SQL选项卡中查看图形表示，您将会看到以下操作符:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ne"><img src="../Images/d3da17e291360640d53646fcee8e0322.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bdhPJv-dgOE5QEAoeXNJzg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><p id="5366" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">将鼠标悬停在它上面时，您还会看到一个较小的黑色矩形，其中包含一些附加信息，告诉您哪些列用于排序。布尔信息<em class="ms"> true/false </em>表示这种排序是全局的还是局部的。在这里，因为我们正在对整个数据帧进行排序，所以它说<em class="ms"> true </em>表示全局排序。</p><p id="54f7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如上所述，全局排序需要对数据进行重新分区，因此整个数据集将被打乱，这由位于<em class="ms">排序</em>之前的<em class="ms">交换</em>操作符表示:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nf"><img src="../Images/19cd327ffdad0e2c1b43f9a56bc001f9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cJ1azxOnzZuYbawHOnM5yw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><p id="ac17" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">关于分区的信息在黑色矩形中，它表示<em class="ms">range partitioning(creation _ date，200)。</em><em class="ms">range partitioning</em>是一种不同于<em class="ms"> hashpartitioning </em>的分区类型，当您想要实现<em class="ms"> hashpartitioning </em>时，也可以通过调用<em class="ms"> repartitionByRange() </em>来实现这种分配，repartitionByRange() 是<em class="ms"> repartition(col) </em>的对应物。</p><p id="a47d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这两种排序函数都可以与下列列转换结合使用:</p><ul class=""><li id="874a" class="ng nh it lb b lc ld lf lg li ni lm nj lq nk lu nl nm nn no bi translated"/><li id="f2f9" class="ng nh it lb b lc np lf nq li nr lm ns lq nt lu nl nm nn no bi translated"><a class="ae ky" href="https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.functions.asc_nulls_first.html#pyspark.sql.functions.asc_nulls_first" rel="noopener ugc nofollow" target="_blank"> asc_nulls_first(列名)</a></li><li id="00c3" class="ng nh it lb b lc np lf nq li nr lm ns lq nt lu nl nm nn no bi translated"><a class="ae ky" href="https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.functions.asc_nulls_last.html#pyspark.sql.functions.asc_nulls_last" rel="noopener ugc nofollow" target="_blank">ASC _ nulls _ last(col _ name)</a></li><li id="71c4" class="ng nh it lb b lc np lf nq li nr lm ns lq nt lu nl nm nn no bi translated">desc(列名)</li><li id="25d7" class="ng nh it lb b lc np lf nq li nr lm ns lq nt lu nl nm nn no bi translated"><a class="ae ky" href="https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.functions.desc_nulls_first.html#pyspark.sql.functions.desc_nulls_first" rel="noopener ugc nofollow" target="_blank"> desc_nulls_first(列名)</a></li><li id="2208" class="ng nh it lb b lc np lf nq li nr lm ns lq nt lu nl nm nn no bi translated"><a class="ae ky" href="https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.functions.desc_nulls_last.html#pyspark.sql.functions.desc_nulls_last" rel="noopener ugc nofollow" target="_blank">desc _空值_姓氏(列名)</a></li></ul><p id="1076" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这些函数只是用来定义空值应该放在哪里，以及应该使用升序还是降序。然后，我们可以将它称为排序函数的参数，如下所示:</p><pre class="kj kk kl km gt mt mu mv mw aw mx bi"><span id="f6cd" class="my lw it mu b gy mz na l nb nc">df.orderBy(desc('creation_date'))</span></pre><h1 id="1dc8" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">分类分区</h1><p id="f4c3" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">如果您不关心所有数据的全局排序，而是只需要对Spark集群上的每个分区进行排序，那么您可以使用<em class="ms">sortwithinspartitions()</em>，这也是一个数据帧转换，但与<em class="ms"> orderBy() </em>不同，它不会导致洗牌。该函数不会对数据进行重新分区，它将保持当前的分区，并且只会对这些分区进行排序:</p><pre class="kj kk kl km gt mt mu mv mw aw mx bi"><span id="6c2e" class="my lw it mu b gy mz na l nb nc"># each partition will be sorted by creation_date column<br/>df.sortWithinPartitions('creation_date')</span></pre><h1 id="84a5" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">分类桶</h1><p id="a0fe" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">创建分桶表时，有一个函数<em class="ms"> bucketBy </em>可以用来对桶进行排序:</p><pre class="kj kk kl km gt mt mu mv mw aw mx bi"><span id="b6e0" class="my lw it mu b gy mz na l nb nc">(<br/>  df<br/>  .write<br/>  .bucketBy(n, field1, field2, ...)<br/>  .<strong class="mu iu">sortBy</strong>(field1, field2, ...)<br/>  .option('path', output_path)<br/>  .saveAsTable(table_name)<br/>)</span></pre><p id="aa7c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">关于分桶和这个特定函数的更多细节，请查看我最近的文章<a class="ae ky" rel="noopener" target="_blank" href="/best-practices-for-bucketing-in-spark-sql-ea9f23f7dd53"><em class="ms">Spark SQL</em></a>中的分桶最佳实践。</p><h1 id="5e2b" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">对每个数据帧行上的数组进行排序</h1><p id="537f" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">另一个排序用例是Spark复杂数据类型的数组。数组包含有顺序的元素，Spark提供了改变顺序的函数:</p><ul class=""><li id="b600" class="ng nh it lb b lc ld lf lg li ni lm nj lq nk lu nl nm nn no bi translated"><a class="ae ky" href="https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.functions.array_sort.html#pyspark.sql.functions.array_sort" rel="noopener ugc nofollow" target="_blank">数组_排序</a></li><li id="1986" class="ng nh it lb b lc np lf nq li nr lm ns lq nt lu nl nm nn no bi translated"><a class="ae ky" href="https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.functions.sort_array.html#pyspark.sql.functions.sort_array" rel="noopener ugc nofollow" target="_blank"> sort_array </a></li></ul><p id="5d60" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这两个函数都按升序对数组进行排序，它们在处理空值的方式上有所不同。另外，<em class="ms"> sort_array </em>可以接受另一个参数<em class="ms"> asc=False </em>，通过这个参数可以对数组进行降序排序。对数组进行排序的一种更灵活的方式是SQL函数<a class="ae ky" href="https://spark.apache.org/docs/latest/api/sql/index.html#array_sort" rel="noopener ugc nofollow" target="_blank"> array_sort </a>，它可以将一个比较器作为附加参数。更多细节，请见我的另一篇<a class="ae ky" rel="noopener" target="_blank" href="/did-you-know-this-in-spark-sql-a7398bfcc41e">文章</a>，在那里我也描述了它并提供了一个特殊的例子。</p><h1 id="8c2c" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">与排序相关的Spark优化</h1><p id="a312" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">排序是一个非常昂贵的操作，因此一个好的优化是避免它，如果它不是必要的。Spark optimizer有一个规则<em class="ms">消除排序</em>，它就是这样做的。想象一个如下的例子:</p><pre class="kj kk kl km gt mt mu mv mw aw mx bi"><span id="626e" class="my lw it mu b gy mz na l nb nc">(<br/>  df<br/>  .orderBy('created_date')<br/>  .select('id', 'message', 'interaction_count')<br/>  .filter(col('interaction_count') &gt; 100)<br/>  .orderBy('interaction_count')<br/>)</span></pre><p id="a815" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在这里，我们可以看到，第一次按<em class="ms"> created_date </em>排序对最终输出没有影响，并且是多余的，因此规则<em class="ms"> EliminateSorts </em>将删除它。只要select或filter中的表达式是确定性的，就可以这样做。在<em class="ms">消除排序</em>规则中，不确定性表达式被跳过。例如，如果在select函数中使用<em class="ms"> rand() </em>，两个<em class="ms">排序</em>都将保留在查询计划中并被执行。</p><h1 id="dbba" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">排序对数据压缩的影响</h1><p id="b469" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">将数据保存到外部存储系统时，对数据进行排序也会产生影响。根据选择的文件格式，它将影响保存数据的最终压缩。让我们来看一个简单的基准测试，其中这一点非常明显。我们将获取一个数据集并多次保存，每次都使用不同的排序技术，最后检查所创建文件的大小。我们将使用的数据集表示社交网络上一年内发布的消息，它具有以下列和结构:</p><ul class=""><li id="a2a5" class="ng nh it lb b lc ld lf lg li ni lm nj lq nk lu nl nm nn no bi translated"><em class="ms"> profile_id </em> —社交网络上的个人资料/页面的标识符</li><li id="2814" class="ng nh it lb b lc np lf nq li nr lm ns lq nt lu nl nm nn no bi translated"><em class="ms"> created_time </em> —一个在个人资料上发布消息的时间戳</li><li id="fb8f" class="ng nh it lb b lc np lf nq li nr lm ns lq nt lu nl nm nn no bi translated"><em class="ms">创建_月份</em> —从<em class="ms">创建_时间</em>时间戳导出的月份</li><li id="85ed" class="ng nh it lb b lc np lf nq li nr lm ns lq nt lu nl nm nn no bi translated">许多其他具有复杂层次结构的字段对分析并不重要</li></ul><p id="0e81" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">数据集有301 204 753条记录，并且在<em class="ms"> profile_id </em>列中包含许多重复项，因为每个配置文件可以发布许多消息。数据集中配置文件的不同计数是410 836。在下面的7个查询中，我们将测试所有三个排序函数并检查最终大小。对于所有查询，我们将shuffle分区设置为2000，考虑到数据集的容量，这似乎是合理的。在前六种情况下，我们将以parquet文件格式保存数据，在最后一种情况下，我们将使用orc来查看它与最后一种情况下的parquet相比如何。</p><ol class=""><li id="e518" class="ng nh it lb b lc ld lf lg li ni lm nj lq nk lu nu nm nn no bi translated">作为参考，我们将首先应用无排序，然后随机分配数据:</li></ol><pre class="kj kk kl km gt mt mu mv mw aw mx bi"><span id="f670" class="my lw it mu b gy mz na l nb nc">(<br/>  df.repartition(n)<br/>  .write<br/>  .format('parquet')<br/>  .mode('overwrite')<br/>  .option('path', output_path_1)<br/>  .save()<br/>)</span></pre><p id="f50a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">2.接下来，我们将对<em class="ms"> created_time </em>列应用全局排序，因此所有发布的消息将根据它们发布的时间进行排序:</p><pre class="kj kk kl km gt mt mu mv mw aw mx bi"><span id="2c98" class="my lw it mu b gy mz na l nb nc">(<br/>  df.orderBy('created_time')<br/>  .write<br/>  .format('parquet')<br/>  .mode('overwrite')<br/>  .option('path', output_path_2)<br/>  .save()<br/>)</span></pre><p id="8f45" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">3.现在，我们将尝试首先按<em class="ms"> profile_id </em>列排序，然后按<em class="ms"> created_time </em>排序:</p><pre class="kj kk kl km gt mt mu mv mw aw mx bi"><span id="8e67" class="my lw it mu b gy mz na l nb nc">(<br/>  df.orderBy('profile_id', 'created_time')<br/>  .write<br/>  .format('parquet')<br/>  .mode('overwrite')<br/>  .option('path', output_path_3)<br/>  .save()<br/>)</span></pre><p id="8b59" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">4.在下一个测试中，我们还将应用按月分区，因为数据非常大，并且在实践中，您可能希望在文件系统中对其进行分区，以便可以使用分区过滤器更快地检索数据。请注意，我们将按两列进行重新分区:<em class="ms"> created_time </em>，然后是给定时间间隔内的一个随机值，通过这个值我们可以控制创建文件的数量。然后，每个创建的Spark分区按照<em class="ms">创建月</em>、<em class="ms">概要id、</em>和<em class="ms">创建时间</em>进行排序:</p><pre class="kj kk kl km gt mt mu mv mw aw mx bi"><span id="9f0e" class="my lw it mu b gy mz na l nb nc">(<br/>  df.repartition('created_month', (rand() * 200).cast('int'))<br/>  .sortWithinPartitions(<br/>      'created_month', 'profile_id', 'created_time'<br/>  )<br/>  .write<br/>  .partitionBy('created_month')<br/>  .format('parquet')<br/>  .mode('overwrite')<br/>  .option('path', output_path_4)<br/>  .save()<br/>)</span></pre><p id="e690" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">5.作为先前技术的替代方法，我们现在将按范围重新分区，这样我们可以避免分布的随机性:</p><pre class="kj kk kl km gt mt mu mv mw aw mx bi"><span id="5ba9" class="my lw it mu b gy mz na l nb nc">(<br/>  df.repartitionByRange('created_time')<br/>  .sortWithinPartitions(<br/>       'created_month', 'profile_id', 'created_time'<br/>   )<br/>  .write<br/>  .partitionBy('created_month')<br/>  .format('parquet')<br/>  .mode('overwrite')<br/>  .option('path', output_path_5)<br/>  .save()<br/>)</span></pre><p id="31df" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">6.接下来，我们还将测试存储桶，并根据<em class="ms"> profile_id </em>列将数据集分成200个存储桶。实际上，如果您想要将<em class="ms"> profile_id </em>列上的表与其他一些表(例如，包含每个概要文件的一些附加信息)连接起来，这可能会很有用。</p><p id="de0a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">使用<em class="ms"> sortBy </em>函数，每个存储桶将按照<em class="ms"> profile_id </em>进行排序。我们还希望保留按月进行的分区，并且在每个文件系统分区中，我们希望每个存储桶创建一个文件，以达到合理的文件数量。为此，我们通过列<em class="ms"> created_month </em>以及由bucketing函数生成的表达式对数据进行重新分区，您可以在下面看到该函数的定义。这个bucketing函数模仿了Spark的内部行为，关于它的更多细节请参见我最近的关于bucketing的文章。</p><pre class="kj kk kl km gt mt mu mv mw aw mx bi"><span id="5e5f" class="my lw it mu b gy mz na l nb nc">(<br/>  df.repartition('created_month', bucketing('profile_id', 200))<br/>  .write<br/>  .partitionBy('created_month')<br/>  .bucketBy(200, 'profile_id')<br/>  .sortBy('profile_id', 'created_time')<br/>  .format('parquet')<br/>  .mode('overwrite')<br/>  .option('path', output_path_6)<br/>  .saveAsTable('test_table')<br/>)</span><span id="6455" class="my lw it mu b gy nv na l nb nc">from pyspark.sql.functions import hash, when</span><span id="55da" class="my lw it mu b gy nv na l nb nc">def bucketing(col, buckets):<br/>  _mod = hash(col) % buckets<br/>  return when(_mod &lt; 0, (_mod + buckets) % buckets).otherwise(_mod)</span></pre><p id="4d0f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">7.最后，我们将测试最后一个查询的orc文件格式。所以这里我们运行相同的查询，但是使用<em class="ms">。格式(' orc') </em>。</p><p id="c6dc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对于所有的查询，我们使用Spark 3.0.1和Databricks runtime 7.3。现在，让我们在下图中查看每个已创建数据集的最终大小:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nw"><img src="../Images/4bcb5e507b11a0c9eb07034be8ec98a1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1DcfUJqsuQu0tY0wHmzmfg.png"/></div></div></figure><h1 id="5fec" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">分类文件的其他好处</h1><p id="bd43" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">除了更好的压缩之外，以parquet等文件格式排序的数据对于在排序列上使用过滤器的查询也是有益的，因为Spark将能够应用数据跳过技术。关于parquet中数据跳过的更多细节，请参见我的另一篇文章<a class="ae ky" rel="noopener" target="_blank" href="/notes-about-saving-data-with-spark-3-0-86ba85ca2b71">。</a></p><h1 id="d991" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">结论</h1><p id="5836" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">在本文中，我们讨论了Spark SQL中可用的不同排序技术。我们已经看到了局部排序和全局排序的区别。我们还提到，全局排序通常是一种开销很大的操作，因为它需要对数据进行重新分区，导致数据混乱，还需要一项额外的工作来估计数据的分布。</p><p id="9bf3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们还检查了排序对文件系统中创建的文件大小的影响。大小的差异可能非常大，在我们的示例中，从没有排序的346 GB到对两个特定列进行全局排序的214 GB不等。这里，字段的不同组合可能会导致更好的压缩。在实践中，全局排序可能不是最有用的设置，因为通常希望对大型表进行分区，甚至可能为了高效的连接而分桶。在这里，我们可以看到，与仅使用分区相比，分区和分桶一起使用导致了更小的大小。这很可能是因为在<em class="ms"> profile_id </em>列上的分桶将具有相同配置文件的记录放在一起，从而提高了压缩。在上一个示例中，我们还看到，orc数据集的大小略大于相应的拼花数据源的大小，但差异很小，两个数据集在大小上是相当的。</p></div></div>    
</body>
</html>