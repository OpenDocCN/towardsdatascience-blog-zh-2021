<html>
<head>
<title>Monitoring BERT Model Training with TensorBoard</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用 TensorBoard 监控 BERT 模型训练</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/monitor-bert-model-training-with-tensorboard-2f4c42b373ea?source=collection_archive---------11-----------------------#2021-12-24">https://towardsdatascience.com/monitor-bert-model-training-with-tensorboard-2f4c42b373ea?source=collection_archive---------11-----------------------#2021-12-24</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="4469" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">梯度流量和更新率</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/128e96f78f7f6e13137e8c7076b6afa2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6g2y1ATrLch1RuCNCoSQWw.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><a class="ae kv" href="https://unsplash.com/@tobiastu" rel="noopener ugc nofollow" target="_blank">https://unsplash.com/@tobiastu</a></p></figure><p id="a1b2" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在<a class="ae kv" href="https://medium.com/@alexml0123/deep-dive-into-the-code-of-bert-model-9f618472353e" rel="noopener">的上一篇文章</a>中，我们解释了 BERT 模型的所有构建组件。现在，我们将在 TensorBoard 中训练监控训练过程的模型，查看梯度流、更新参数比率、损失和评估指标。</p><p id="7006" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">为什么我们希望监控梯度流量和更新率，而不是简单地查看损失和评估指标？当我们开始对大量数据进行模型训练时，我们可能会在意识到之前运行许多次迭代，查看模型没有训练的损失和评估指标。在这里，查看梯度的大小和更新率，我们可以立即发现有问题，这为我们节省了时间和金钱。</p></div><div class="ab cl ls lt hu lu" role="separator"><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx"/></div><div class="ij ik il im in"><h2 id="8e11" class="lz ma iq bd mb mc md dn me mf mg dp mh lf mi mj mk lj ml mm mn ln mo mp mq mr bi translated">数据准备</h2><p id="b71b" class="pw-post-body-paragraph kw kx iq ky b kz ms jr lb lc mt ju le lf mu lh li lj mv ll lm ln mw lp lq lr ij bi translated">我们将使用来自<em class="mx"> sklearn </em>的【T20 个新闻组数据集(许可证:公共域/来源:<a class="ae kv" href="http://qwone.com/~jason/20Newsgroups/" rel="noopener ugc nofollow" target="_blank">http://qwone.com/~jason/20Newsgroups/</a>)在这个例子中有 4 个类别:<em class="mx"> alt .无神论</em>、<em class="mx"> talk.religion.misc </em>、<em class="mx"> comp.graphics </em>和<em class="mx"> sci.space. </em>我们用来自<em class="mx">变形金刚</em>的<em class="mx">bertokenizer</em>对数据进行标记化</p><pre class="kg kh ki kj gt my mz na nb aw nc bi"><span id="0d13" class="lz ma iq mz b gy nd ne l nf ng">categories = ['alt.atheism', 'talk.religion.misc', 'comp.graphics', 'sci.space']</span><span id="3172" class="lz ma iq mz b gy nh ne l nf ng">newsgroups_train = fetch_20newsgroups(subset='train', categories=categories)<br/>newsgroups_test = fetch_20newsgroups(subset='test', categories=categories)</span><span id="e7cc" class="lz ma iq mz b gy nh ne l nf ng">X_train = pd.DataFrame(newsgroups_train['data'])<br/>y_train = pd.Series(newsgroups_train['target'])</span><span id="45f6" class="lz ma iq mz b gy nh ne l nf ng">X_test = pd.DataFrame(newsgroups_test['data'])<br/>y_test = pd.Series(newsgroups_test['target'])</span><span id="7d6a" class="lz ma iq mz b gy nh ne l nf ng">BATCH_SIZE = 16</span><span id="70c1" class="lz ma iq mz b gy nh ne l nf ng">max_length = 256<br/>config = BertConfig.from_pretrained("bert-base-uncased")<br/>config.num_labels = len(y_train.unique())<br/>config.max_position_embeddings = max_length</span><span id="4c8b" class="lz ma iq mz b gy nh ne l nf ng">train_encodings = tokenizer(X_train[0].tolist(), truncation=True, padding=True, max_length=max_length)<br/>test_encodings = tokenizer(X_test[0].tolist(), truncation=True, padding=True, max_length=max_length)</span><span id="04ae" class="lz ma iq mz b gy nh ne l nf ng">class BertDataset(Dataset):</span><span id="3d8a" class="lz ma iq mz b gy nh ne l nf ng">def __init__(self, encodings, labels):<br/> self.encodings = encodings<br/> self.labels = labels</span><span id="db5a" class="lz ma iq mz b gy nh ne l nf ng">def __getitem__(self, idx):<br/> item = {key: torch.tensor(val[idx]).to(device) for key, val in <br/>self.encodings.items()}<br/> item[‘labels’] = torch.tensor(self.labels[idx]).to(device)<br/> return item</span><span id="25de" class="lz ma iq mz b gy nh ne l nf ng">def __len__(self):<br/> return len(self.labels)</span><span id="e500" class="lz ma iq mz b gy nh ne l nf ng">train_dataset = BertDataset(train_encodings, y_train)<br/>test_dataset = BertDataset(test_encodings, y_test)</span><span id="6435" class="lz ma iq mz b gy nh ne l nf ng">train_dataset_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)<br/>test_dataset_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE)</span><span id="9971" class="lz ma iq mz b gy nh ne l nf ng">for d in train_dataset_loader:<br/>    print(d)<br/>    break</span><span id="7dc2" class="lz ma iq mz b gy nh ne l nf ng"># output : <br/>{'input_ids': tensor([[ 101, 2013, 1024,  ...,    0,    0,    0],<br/>         [ 101, 2013, 1024,  ..., 1064, 1028,  102],<br/>         [ 101, 2013, 1024,  ...,    0,    0,    0],<br/>         ...,<br/>         [ 101, 2013, 1024,  ..., 2620, 1011,  102],<br/>         [ 101, 2013, 1024,  ..., 1012, 4012,  102],<br/>         [ 101, 2013, 1024,  ..., 3849, 2053,  102]], device='cuda:0'),<br/> 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],<br/>         [0, 0, 0,  ..., 0, 0, 0],<br/>         [0, 0, 0,  ..., 0, 0, 0],<br/>         ...,<br/>         [0, 0, 0,  ..., 0, 0, 0],<br/>         [0, 0, 0,  ..., 0, 0, 0],<br/>         [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0'),<br/> 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],<br/>         [1, 1, 1,  ..., 1, 1, 1],<br/>         [1, 1, 1,  ..., 0, 0, 0],<br/>         ...,<br/>         [1, 1, 1,  ..., 1, 1, 1],<br/>         [1, 1, 1,  ..., 1, 1, 1],<br/>         [1, 1, 1,  ..., 1, 1, 1]], device='cuda:0'),<br/> 'labels': tensor([3, 0, 2, 1, 0, 2, 2, 1, 1, 0, 1, 3, 3, 0, 2, 1], device='cuda:0')}</span></pre></div><div class="ab cl ls lt hu lu" role="separator"><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx"/></div><div class="ij ik il im in"><h2 id="3201" class="lz ma iq bd mb mc md dn me mf mg dp mh lf mi mj mk lj ml mm mn ln mo mp mq mr bi translated">张量板的使用</h2><p id="009a" class="pw-post-body-paragraph kw kx iq ky b kz ms jr lb lc mt ju le lf mu lh li lj mv ll lm ln mw lp lq lr ij bi translated">TensorBoard 允许我们编写和保存不同类型的数据，包括图像和标量，以供将来分析。首先让我们用 pip 安装 tensorboard:</p><blockquote class="ni nj nk"><p id="73ef" class="kw kx mx ky b kz la jr lb lc ld ju le nl lg lh li nm lk ll lm nn lo lp lq lr ij bi translated">pip 安装张量板</p></blockquote><p id="9fbc" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">要写入 TensorBoard，我们将使用来自<em class="mx">torch . utils . tensor board</em>的<em class="mx"> SummaryWriter </em></p><pre class="kg kh ki kj gt my mz na nb aw nc bi"><span id="1a02" class="lz ma iq mz b gy nd ne l nf ng">from torch.utils.tensorboard import SummaryWriter</span><span id="8f9a" class="lz ma iq mz b gy nh ne l nf ng"># SummaryWriter takes log directory as argument<br/>writer = SummaryWriter(‘tensorboard/runs/bert_experiment_1’)</span></pre><p id="32de" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">为了编写标量，我们使用:</p><pre class="kg kh ki kj gt my mz na nb aw nc bi"><span id="0883" class="lz ma iq mz b gy nd ne l nf ng">writer.add_scalar(‘loss/train’, loss, counter_train)</span></pre><p id="3893" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">需要变量<em class="mx"> counter_train </em>来知道向 TensorBoard 写入内容的步数。<br/>要写一个图像，我们将使用以下内容:</p><pre class="kg kh ki kj gt my mz na nb aw nc bi"><span id="5b1b" class="lz ma iq mz b gy nd ne l nf ng">writer.add_figure(“gradients”, myfig, global_step=counter_train, close=True, walltime=None)</span></pre></div><div class="ab cl ls lt hu lu" role="separator"><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx"/></div><div class="ij ik il im in"><h2 id="45ed" class="lz ma iq bd mb mc md dn me mf mg dp mh lf mi mj mk lj ml mm mn ln mo mp mq mr bi translated">模特培训</h2><p id="5cab" class="pw-post-body-paragraph kw kx iq ky b kz ms jr lb lc mt ju le lf mu lh li lj mv ll lm ln mw lp lq lr ij bi translated">现在让我们看看我们的训练函数</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="no np l"/></div></figure><p id="8244" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><em class="mx"> out_every </em>变量控制写入 TensorBoard 的频率，以步数衡量。我们还可以使用<em class="mx"> step_eval </em>变量，比在每个时期之后更频繁地进行评估。梯度流量和更新比率数字分别从<em class="mx"> plot_grad_flow </em>和<em class="mx"> plot_ratios </em>函数返回。</p><p id="a794" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们传递模型的参数，为了更好的可视化，我们可以决定用<em class="mx"> skip_prob </em>参数跳过一些层。我们写下每层梯度的平均值、最大值和标准偏差，忽略偏差层，因为它们不太有趣。如果你也想显示 bias 的渐变，你可以删除第 17 行的部分。我们还显示了每层中零梯度的百分比。需要强调的是，由于 BERT 使用 GeLU 而不是 ReLU 激活函数，这最后一个图可能不太有用，但如果您使用的是 ReLU 的不同模型，该模型存在<a class="ae kv" href="https://en.wikipedia.org/wiki/Rectifier_(neural_networks)" rel="noopener ugc nofollow" target="_blank">死亡神经元</a>问题，显示零梯度的百分比实际上是有帮助的。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="no np l"/></div></figure><p id="11b8" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><em class="mx"> plot_ratios </em>函数显示每个参数的更新/参数比率，这是一个标准化测量，因为更新除以参数值，并有助于了解您的神经网络如何学习。作为一个粗略的启发，这个值应该在<em class="mx"> 1e-3 </em>左右，如果低于这个值，学习率可能太低，否则太高。此外，您可以通过<em class="mx"> skip_prob </em>参数减少显示的图层。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="no np l"/></div></figure><p id="4903" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在模型训练期间，您可以使用以下命令启动 TensorBoard:</p><pre class="kg kh ki kj gt my mz na nb aw nc bi"><span id="a49a" class="lz ma iq mz b gy nd ne l nf ng">tensorboard --samples_per_plugin images=100 --logdir bert_experiment_1</span></pre><p id="51eb" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">否则，您可以创建一个<em class="mx">。bat </em>文件(在 Windows 上)以便更快地启动。创建一个新文件，例如<em class="mx">run _ tensor board</em>with<em class="mx">。bat </em>扩展并复制粘贴以下命令，相应修改<em class="mx"> path_to_anaconda_env，path_to_saved_results </em>和<em class="mx"> env_name </em>。然后点击文件启动 TensorBoard。</p><pre class="kg kh ki kj gt my mz na nb aw nc bi"><span id="d3c8" class="lz ma iq mz b gy nd ne l nf ng">cmd /k “cd path_to_anaconda_env\Scripts &amp; activate env_name &amp; cd path_to_saved_results\tensorboard\runs &amp; tensorboard — samples_per_plugin images=100 — logdir bert_experiment_1”</span></pre><p id="9ed8" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">定义<em class="mx">samples _ per _ plugin images = 100</em>我们显示这个任务中的所有图像，否则默认情况下 TensorBoard 只会显示其中的一部分。</p></div><div class="ab cl ls lt hu lu" role="separator"><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx"/></div><div class="ij ik il im in"><h2 id="a726" class="lz ma iq bd mb mc md dn me mf mg dp mh lf mi mj mk lj ml mm mn ln mo mp mq mr bi translated">结果</h2><p id="b024" class="pw-post-body-paragraph kw kx iq ky b kz ms jr lb lc mt ju le lf mu lh li lj mv ll lm ln mw lp lq lr ij bi translated">监控这些情节，我们可以很快发现事情是否如预期发展。例如，如果许多图层的渐变为零，您可能会遇到渐变消失的问题。同样，如果比率非常低或非常高，您可能希望立即深入了解，而不是等到训练结束或训练的几个时期。</p><p id="944e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">例如，通过下面的配置查看 240 步的比率和梯度，我们可以看到情况看起来很好，我们的训练进行得很好——我们可以期待最终的好结果。</p><pre class="kg kh ki kj gt my mz na nb aw nc bi"><span id="e515" class="lz ma iq mz b gy nd ne l nf ng">EPOCHS = 5<br/>optimizer = AdamW(model.parameters(), lr=3e-5, correct_bias=False)<br/>total_steps = len(train_dataset_loader) * EPOCHS<br/>scheduler = get_linear_schedule_with_warmup(<br/>                   optimizer,<br/>                   num_warmup_steps= 0.7 * total_steps,<br/>                   num_training_steps=total_steps<br/>                                            )</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nq"><img src="../Images/38cf3b4e50121a785f64d74b06656d71.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*b_aQB-SFZapGonBPYlxHbw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">比率更新，作者图片</p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nr"><img src="../Images/4e9b1e9378dabfd943d88d133c1717b7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yzK9MNpR9zuHb8QuqRIBjA.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">渐变流，按作者排列的图像</p></figure><p id="5279" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">事实上，最终结果是:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/56150ea9f695b313a30bd22c7c39af7d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1328/format:webp/1*i0DvIR4RzD8taVD0DlsV3g.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">训练/测试准确性，图片由作者提供</p></figure><p id="8f87" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">而如果我们将计划设置更改为:</p><pre class="kg kh ki kj gt my mz na nb aw nc bi"><span id="01c1" class="lz ma iq mz b gy nd ne l nf ng">scheduler = get_linear_schedule_with_warmup(<br/> optimizer,<br/> num_warmup_steps= 0.1 * total_steps,<br/> num_training_steps=total_steps<br/> )</span></pre><p id="a45a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们从图中注意到，在第 180 步，模型没有学习，我们可以停止训练以进一步研究。在这种情况下，将<em class="mx"> num_warmup_steps </em>设置为<em class="mx"> 0.1 * total_steps </em>使得学习率降低，并且在训练开始后不久变得非常小，并且我们最终得到消失的梯度，该梯度不会传播回网络的第一层，从而有效地停止了学习过程。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nt"><img src="../Images/3db81b9b26878091f06a498359dd4c88.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*f_WtnLzecN8KuUmvmykxyg.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">比率更新，作者图片</p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nu"><img src="../Images/24158756154667bfc1c01f489527946f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JY6s-wZiTkib8z5u2WEBSw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">渐变流，按作者排列的图像</p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nv"><img src="../Images/68a394e4859dabdf9c1183846d6faf6a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oWokjdgd1gDQcikJi7iAzw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">训练/测试准确性，图片由作者提供</p></figure><p id="ef79" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">你可以在<a class="ae kv" href="https://github.com/MLAlex1/bert_tensorboard" rel="noopener ugc nofollow" target="_blank">的这个</a> GitHub repo 中找到完整的代码，自己尝试一下。</p></div><div class="ab cl ls lt hu lu" role="separator"><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx"/></div><div class="ij ik il im in"><h2 id="b9b6" class="lz ma iq bd mb mc md dn me mf mg dp mh lf mi mj mk lj ml mm mn ln mo mp mq mr bi translated">结论</h2><p id="9a97" class="pw-post-body-paragraph kw kx iq ky b kz ms jr lb lc mt ju le lf mu lh li lj mv ll lm ln mw lp lq lr ij bi translated">在阅读完这篇文章和之前的文章之后，你应该拥有所有的工具，并且理解如何在你的项目中训练 Bert 模型！我在这里描述的是一些你想在培训期间监控的东西，但 TensorBoard 提供了其他功能，如<a class="ae kv" href="https://www.tensorflow.org/tensorboard/tensorboard_projector_plugin" rel="noopener ugc nofollow" target="_blank">嵌入投影仪</a>，你可以使用它来探索你的嵌入层，以及更多你可以在这里找到的功能<a class="ae kv" href="https://www.tensorflow.org/tensorboard/get_started" rel="noopener ugc nofollow" target="_blank"/>。</p><h2 id="44c6" class="lz ma iq bd mb mc md dn me mf mg dp mh lf mi mj mk lj ml mm mn ln mo mp mq mr bi translated">参考</h2><p id="5c44" class="pw-post-body-paragraph kw kx iq ky b kz ms jr lb lc mt ju le lf mu lh li lj mv ll lm ln mw lp lq lr ij bi translated">https://cs231n.github.io/neural-networks-3/</p></div></div>    
</body>
</html>