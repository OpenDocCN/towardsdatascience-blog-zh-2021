<html>
<head>
<title>Developing and Explaining Cross-Entropy from Scratch</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">从头开始发展和解释交叉熵</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/developing-and-explaining-cross-entropy-from-scratch-ba2eff9036dd?source=collection_archive---------33-----------------------#2021-07-29">https://towardsdatascience.com/developing-and-explaining-cross-entropy-from-scratch-ba2eff9036dd?source=collection_archive---------33-----------------------#2021-07-29</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="97cc" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">继续阅读，理解交叉熵背后的直觉，以及为什么机器学习算法试图最小化它。</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/789b3b629630bb45a881697782cbb91c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*7-wlZSTjw8kWlsH0"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">约尔根·哈兰在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="c8b5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">交叉熵是一个重要的概念。它通常在机器学习中用作成本函数——通常我们的目标是最小化交叉熵。但是为什么我们要最小化交叉熵，交叉熵到底是什么意思？让我们来回答这些问题。</p><p id="b9c4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">首先，我们需要充分理解信息和熵的概念。如果你想彻底理解这些概念，我写了一篇关于它们的详细文章<a class="ae ky" rel="noopener" target="_blank" href="/developing-the-concepts-of-information-and-entropy-from-scratch-11faca089d44">在这里</a>。总而言之，信息是衡量一个事件有多令人惊讶/低概率的指标。概率越低(越让人吃惊)，信息量越高。信息也可以解释为表示一个事件需要多少位。公式为I = -log_2(p) bits，其中I为信息，p为概率。熵是所有可能事件的平均值，所以熵的公式是E = -∑ p_i * log_2(p_i)。</p><p id="9ef5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">有了这个背景，我们可以继续讨论交叉熵。交叉熵提出的问题是:<strong class="lb iu">当我用不同的概率分布代替真实的概率分布时，信息/比特表示长度会发生什么变化？让我们来看一个具体的例子，看看这是什么意思。</strong></p><p id="a662" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">假设我们有一枚公平的硬币。真正的概率分布是正面，反面。头和尾事件的信息是相同的:I = -log_2(1/2) = 1位。熵是两个事件的信息的平均值，所以也是1比特。现在让我们假设，出于某种原因，我们认为硬币是不公平的(即使它实际上是公平的)。我们认为概率分布是正面，反面。在我们看来，正面比实际情况更罕见，反面更常见。因此，我们认为正面包含的信息比它实际包含的多，反面包含的信息少。从我们的角度来看，确切的信息量是I_heads = -log_2(1/4) = 2，I_tails = -log_2(3/4) = 0.42。</p><p id="add5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然而，真正的概率分布仍然是正面，反面。正因为如此，从我们的角度来看，翻转的期望信息将是* I _ heads+* I _ tails = * 2+* 0.42 = 1.21。这个数字就是交叉熵:通过假设与真实概率分布不同的概率分布计算出的平均信息。如果真实概率记为p_i，假设概率记为q_i，则交叉熵的公式为CE = -∑ p_i * log_2(q_i)。</p><p id="2665" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们从另一个角度来看这个例子，以建立更多的直觉。与其抛硬币，不如考虑一个只有两个字母的字母表:A和B。我和我的朋友都使用这个字母表，但我更喜欢字母A，他更喜欢字母B。假设我的A到B的字母用法是80/20，我朋友的是20/80。我想想出一个高效的二进制代码来表示这个字母表。因为我使用字母A的次数远远多于B，所以我将用较少的比特数表示A，用较多的比特数表示B。我的朋友也想出了一个代码，但是由于他用B比用A多，他的代码用A的比特多，用B的比特少。</p><p id="9247" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在，考虑当我试图用我的代码来表示他所说的话(他的语言)时会发生什么。因为他说了很多次B，所以我很多次都要用我的高比特位表示B，用我的低比特位A表示的机会不多。因此，我的代码的总比特使用量(在我朋友的语言中)将远远高于他自己的代码(有一个低位B和一个高位A)。同样，我朋友的代码在表示我的语言时会非常低效。那么这个例子有什么意义呢？我的代码需要我朋友的语言的位数是交叉熵。他的代码为他的语言使用的比特数是熵。如我们所见，<strong class="lb iu">交叉熵高于熵</strong>。</p><p id="de52" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们还可以改变字母使用分布。比如说我的信使用分布是60/40 A/B而不是80/20，我的朋友是40/60而不是20/80。现在，由于我们的发行版更接近了，使用我的优化代码来表示我朋友的语言不再像以前那样低效了。换句话说，交叉熵更小。你可以通过插入我们上面陈述的交叉熵公式来自己看到这一点。</p><p id="dae0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这表明了交叉熵的一个重要用途:比较两个概率分布。<strong class="lb iu">两个分布越接近，交叉熵就会越小</strong>。因此，如果我们的目标是使一个概率分布尽可能接近另一个，我们需要<strong class="lb iu">来最小化它们之间的交叉熵</strong>。这是怎么用的？在机器学习问题中，一个常见的模式是，我们有一个真实分布P和一个输出另一个分布Q的模型M。目标是找到M的正确参数，使P和Q之间的交叉熵尽可能小。一种流行的方法是最大化对数似然，这与最小化交叉熵是一回事。你可以在这里看到证明<a class="ae ky" href="https://en.wikipedia.org/wiki/Cross_entropy#Relation_to_log-likelihood" rel="noopener ugc nofollow" target="_blank"/>。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><p id="4f23" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在本文中，我们解释了什么是交叉熵，探索了一些交叉熵构建直觉的例子，并讨论了它在机器学习中的用法。我希望一切都很清楚，请留下您可能有的任何问题/意见。如果你对这类话题感兴趣，我打算再写一些关于重要理论机器学习概念的帖子，敬请关注！</p></div></div>    
</body>
</html>