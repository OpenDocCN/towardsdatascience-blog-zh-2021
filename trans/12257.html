<html>
<head>
<title>Text To Speech — Foundational Knowledge (Part 2)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">文本到语音—基础知识(第二部分)</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/text-to-speech-foundational-knowledge-part-2-4db2a3657335?source=collection_archive---------17-----------------------#2021-12-13">https://towardsdatascience.com/text-to-speech-foundational-knowledge-part-2-4db2a3657335?source=collection_archive---------17-----------------------#2021-12-13</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="edc0" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">训练、合成和实现最新的TTS算法所需的知识:利用ESPnet的机器学习音频从零到英雄系列的第2部分</h2></div><figure class="kf kg kh ki gt kj"><div class="bz fp l di"><div class="kk kl l"/></div><p class="km kn gj gh gi ko kp bd b be z dk translated">资料来源:Giphy</p></figure><h1 id="98ea" class="kq kr iq bd ks kt ku kv kw kx ky kz la jw lb jx lc jz ld ka le kc lf kd lg lh bi translated">背景:</h1><p id="5b27" class="pw-post-body-paragraph li lj iq lk b ll lm jr ln lo lp ju lq lr ls lt lu lv lw lx ly lz ma mb mc md ij bi translated">在第1部分中，我们通过一个简单的ESPnet TTS实现展示了快速合成的能力。不过，如果你和我一样，你可能想知道这一切是如何在幕后工作的！起初，我很难找到一篇完整的文章或资料来源来解释幕后发生的事情。这篇文章总结了大量的调查/研究论文、YouTube演讲视频和博客文章。如果没有这篇出色的<a class="ae me" href="https://arxiv.org/abs/2106.15561" rel="noopener ugc nofollow" target="_blank">调查论文</a>，这篇文章是不可能发表的，这篇论文提供了许多图片和文字。我试图在这里“综合”我的基础知识发现，希望这能让你更好地理解:</p><ul class=""><li id="1380" class="mf mg iq lk b ll mh lo mi lr mj lv mk lz ml md mm mn mo mp bi translated">基本音频术语和框架</li><li id="953c" class="mf mg iq lk b ll mq lo mr lr ms lv mt lz mu md mm mn mo mp bi translated">文本到语音算法的发展</li><li id="4bde" class="mf mg iq lk b ll mq lo mr lr ms lv mt lz mu md mm mn mo mp bi translated">深入研究声学模型和神经声码器算法</li><li id="8a65" class="mf mg iq lk b ll mq lo mr lr ms lv mt lz mu md mm mn mo mp bi translated">下一代端到端TTS</li></ul><h1 id="256f" class="kq kr iq bd ks kt ku kv kw kx ky kz la jw lb jx lc jz ld ka le kc lf kd lg lh bi translated">基本音频术语:</h1><h2 id="2ed5" class="mv kr iq bd ks mw mx dn kw my mz dp la lr na nb lc lv nc nd le lz ne nf lg ng bi translated">波形:</h2><p id="48a8" class="pw-post-body-paragraph li lj iq lk b ll lm jr ln lo lp ju lq lr ls lt lu lv lw lx ly lz ma mb mc md ij bi translated">计算机解释在固定时间帧内改变振幅的音频信号。每个样本通常具有65，536个值(16位)，质量以kHz为单位测量。波形中的循环分量是下面讨论的方法的主要分量。</p><figure class="kf kg kh ki gt kj"><div class="bz fp l di"><div class="nh kl l"/></div><p class="km kn gj gh gi ko kp bd b be z dk translated">来源:Giphy(音频波形)</p></figure><h2 id="034d" class="mv kr iq bd ks mw mx dn kw my mz dp la lr na nb lc lv nc nd le lz ne nf lg ng bi translated">音素:</h2><p id="7d00" class="pw-post-body-paragraph li lj iq lk b ll lm jr ln lo lp ju lq lr ls lt lu lv lw lx ly lz ma mb mc md ij bi translated">文本到语音引擎不直接将字符作为输入，而是音素，特别强调ARPA(英语),如在CMU字典中找到的。例如，绿色是G R IY1 N。其他语言可能有不同的格式，但用法相同。这就是为什么我们将输入文本(在训练期间)转换成音素，这些音素是区分语言中一个单词与另一个单词的不同声音单位。当你的输入被处理后，它会被一个训练有素的神经网络转换成音素，这个神经网络还会学习生成新单词的拼写。</p><h2 id="6b0e" class="mv kr iq bd ks mw mx dn kw my mz dp la lr na nb lc lv nc nd le lz ne nf lg ng bi translated">光谱图:</h2><p id="9902" class="pw-post-body-paragraph li lj iq lk b ll lm jr ln lo lp ju lq lr ls lt lu lv lw lx ly lz ma mb mc md ij bi translated">深度学习模型不直接将原始音频作为输入，因此音频被转换为频谱图，并且傅立叶变换源音频到时频域。变换过程在变换之前将声音信号的持续时间分割成较小的信号，然后将输出组合成单个视图。</p><figure class="kf kg kh ki gt kj gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/9a1a7e7b0c0be0c20708dee9412ccf5b.png" data-original-src="https://miro.medium.com/v2/resize:fit:704/format:webp/1*4wspnpaLfS5Caf1N2K-G7Q.png"/></div><p class="km kn gj gh gi ko kp bd b be z dk translated">示例光谱图(作者)</p></figure><p id="66bf" class="pw-post-body-paragraph li lj iq lk b ll mh jr ln lo mi ju lq lr nl lt lu lv nm lx ly lz nn mb mc md ij bi translated">上图中的颜色直观地展示了音频分贝。然而，我们可以看到，我们并没有从这个音频剪辑中捕捉到太多。</p><h2 id="229d" class="mv kr iq bd ks mw mx dn kw my mz dp la lr na nb lc lv nc nd le lz ne nf lg ng bi translated">梅尔光谱图:</h2><p id="8db1" class="pw-post-body-paragraph li lj iq lk b ll lm jr ln lo lp ju lq lr ls lt lu lv lw lx ly lz ma mb mc md ij bi translated">人类对声音频率的理解可能会因对频率的印象而大相径庭，大自然对声音的感知不是线性的。这就是开发<a class="ae me" href="https://www.sfu.ca/sonic-studio-webdav/handbook/Mel.html" rel="noopener ugc nofollow" target="_blank">梅尔秤</a>的原因。它的关键是在处理振幅(多大声)和频率(音高)的对数标度时考虑了分贝标度。把声音特征全部储存在梅尔的声谱图中。</p><figure class="kf kg kh ki gt kj gh gi paragraph-image"><div class="gh gi no"><img src="../Images/69f1efa6b8c13f0cb2991160779b54b1.png" data-original-src="https://miro.medium.com/v2/resize:fit:826/format:webp/1*uFGML8YCj8W2KxG-wPA_6A.png"/></div><p class="km kn gj gh gi ko kp bd b be z dk translated">Mel谱图示例(作者)</p></figure><p id="ed82" class="pw-post-body-paragraph li lj iq lk b ll mh jr ln lo mi ju lq lr nl lt lu lv nm lx ly lz nn mb mc md ij bi translated">我们可以看到，Mel-spectrogram提供了一个更清晰的图像，以分贝为单位进行测量，并针对输入TTS进行了优化。如果你想对数学有更深的理解，看看这个伟大的帖子<a class="ae me" rel="noopener" target="_blank" href="/audio-deep-learning-made-simple-part-2-why-mel-spectrograms-perform-better-aad889a93505">这里</a>。</p><h2 id="ad55" class="mv kr iq bd ks mw mx dn kw my mz dp la lr na nb lc lv nc nd le lz ne nf lg ng bi translated">音频评估指标:</h2><p id="1126" class="pw-post-body-paragraph li lj iq lk b ll lm jr ln lo lp ju lq lr ls lt lu lv lw lx ly lz ma mb mc md ij bi translated">ML工程师用来评估语音合成质量的最常见的数字指标是平均意见得分(MOS)，其范围从0到5，日常人类语音从4.5到4.8。要检查基准，你可以在这里看<a class="ae me" href="https://paperswithcode.com/sota/speech-synthesis-on-north-american-english" rel="noopener ugc nofollow" target="_blank"/>，但至少目前不包括所有的TTS算法。</p><figure class="kf kg kh ki gt kj gh gi paragraph-image"><div role="button" tabindex="0" class="nq nr di ns bf nt"><div class="gh gi np"><img src="../Images/e1dee8f83e947af003c7762a7ae3640b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WzQfuKxh88PxduZflSaZVA.png"/></div></div><p class="km kn gj gh gi ko kp bd b be z dk translated">来源:作者的平均意见得分</p></figure><h2 id="6aea" class="mv kr iq bd ks mw mx dn kw my mz dp la lr na nb lc lv nc nd le lz ne nf lg ng bi translated">自回归(AR):</h2><p id="19aa" class="pw-post-body-paragraph li lj iq lk b ll lm jr ln lo lp ju lq lr ls lt lu lv lw lx ly lz ma mb mc md ij bi translated">描述基于过去数据预测未来值的模型，其基本假设是未来<strong class="lk ir"> <em class="nu">将</em> </strong>与过去相似。在处理音频领域时，这是一个重要的注意事项，因为你需要知道扬声器在哪些单词上受过训练，以产生正确的输出音频。当在这两种算法之间工作时，还存在速度和质量的一般化折衷，其中自回归生成具有较低的速率，但是较高的质量和非自回归生成具有相反的效果。</p><h2 id="e8e9" class="mv kr iq bd ks mw mx dn kw my mz dp la lr na nb lc lv nc nd le lz ne nf lg ng bi translated">统计参数语音合成(SPSS):</h2><p id="51df" class="pw-post-body-paragraph li lj iq lk b ll lm jr ln lo lp ju lq lr ls lt lu lv lw lx ly lz ma mb mc md ij bi translated">一种解决传统拼接TTS问题的文本到语音转换方法。这种方法通过生成语音所需的声学参数，然后使用算法从生成的声学参数中恢复语音来合成语音。主流的两阶段方法框架是基于SPSS的。</p><h2 id="0c68" class="mv kr iq bd ks mw mx dn kw my mz dp la lr na nb lc lv nc nd le lz ne nf lg ng bi translated"><strong class="ak">主流2阶段框架:</strong></h2><p id="ab5a" class="pw-post-body-paragraph li lj iq lk b ll lm jr ln lo lp ju lq lr ls lt lu lv lw lx ly lz ma mb mc md ij bi translated">回顾一下，TTS已经从串联合成发展到参数合成，再到基于神经网络的合成，如<a class="ae me" rel="noopener" target="_blank" href="/text-to-speech-lifelike-speech-synthesis-demo-part-1-f991ffe9e41e">第1部分</a>所述。在第1部分中，我们分解了主流的两阶段方法，如下所述。</p><figure class="kf kg kh ki gt kj gh gi paragraph-image"><div role="button" tabindex="0" class="nq nr di ns bf nt"><div class="gh gi nv"><img src="../Images/c31d54e7cfb5b9fe256a7cefec18ec09.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HgzCZolqPcTNdRNOLKgG6w.png"/></div></div><p class="km kn gj gh gi ko kp bd b be z dk translated">来源:作者图片(主流2阶段高层架构)</p></figure><h2 id="9354" class="mv kr iq bd ks mw mx dn kw my mz dp la lr na nb lc lv nc nd le lz ne nf lg ng bi translated"><strong class="ak">文本到语音框架:</strong></h2><figure class="kf kg kh ki gt kj gh gi paragraph-image"><div role="button" tabindex="0" class="nq nr di ns bf nt"><div class="gh gi nw"><img src="../Images/40838da8f44563b2dfab75d4c2ec204e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*S2Hc0Sw_GhS7ug-lqME3Nw.png"/></div></div><p class="km kn gj gh gi ko kp bd b be z dk translated">来源:关于神经语音合成的调查</p></figure><p id="11a3" class="pw-post-body-paragraph li lj iq lk b ll mh jr ln lo mi ju lq lr nl lt lu lv nm lx ly lz nn mb mc md ij bi translated">上图显示了目前使用的五种不同类型的TTS框架。第三阶段和第四阶段是本文的主题。</p><p id="f8de" class="pw-post-body-paragraph li lj iq lk b ll mh jr ln lo mi ju lq lr nl lt lu lv nm lx ly lz nn mb mc md ij bi translated">当前主流的基于两阶段(声学模型+声码器)神经网络的模型已经显著提高了合成语音的质量。突出的方法(例如，Tacotron 2/FastSpeech 2)将首先从文本生成Mel谱图，然后使用诸如WaveNet的神经声码器从Mel谱图合成语音。目前还有一种朝着下一代完全端到端TTS模型的演进，我们将对此进行讨论。</p><h1 id="6449" class="kq kr iq bd ks kt ku kv kw kx ky kz la jw lb jx lc jz ld ka le kc lf kd lg lh bi translated">文本到语音的演变:</h1><figure class="kf kg kh ki gt kj gh gi paragraph-image"><div role="button" tabindex="0" class="nq nr di ns bf nt"><div class="gh gi nx"><img src="../Images/dd5e5ca8d0674923314c55f9546e74e9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kuraEvojNifPfqgykc9hLQ.png"/></div></div><p class="km kn gj gh gi ko kp bd b be z dk translated">来源:关于神经语音合成的调查</p></figure><p id="4a11" class="pw-post-body-paragraph li lj iq lk b ll mh jr ln lo mi ju lq lr nl lt lu lv nm lx ly lz nn mb mc md ij bi translated">上面这张侧面的网络图清楚地表明，利用神经网络的合成文本到语音(TTS)领域在过去几年中一直呈爆炸式增长，最近的趋势是如何从主流的两阶段(声学模型+声码器)转向下一代端到端模型。</p><p id="52f6" class="pw-post-body-paragraph li lj iq lk b ll mh jr ln lo mi ju lq lr nl lt lu lv nm lx ly lz nn mb mc md ij bi translated">在下面的数据流图中，我们可以看到不同的基于神经的TTS算法是如何从原始文本(字符)开始并生成波形的。上图和下图都非常有助于将算法的发展和相应的数据流联系起来。</p><figure class="kf kg kh ki gt kj gh gi paragraph-image"><div role="button" tabindex="0" class="nq nr di ns bf nt"><div class="gh gi ny"><img src="../Images/8b3a3e00507067b75e3aa3e70a91a7f6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rzi-bmYZWTUf8ycBf0tfkw.png"/></div></div><p class="km kn gj gh gi ko kp bd b be z dk translated">来源:关于神经语音合成的调查</p></figure></div><div class="ab cl nz oa hu ob" role="separator"><span class="oc bw bk od oe of"/><span class="oc bw bk od oe of"/><span class="oc bw bk od oe"/></div><div class="ij ik il im in"><h1 id="4f63" class="kq kr iq bd ks kt og kv kw kx oh kz la jw oi jx lc jz oj ka le kc ok kd lg lh bi translated">声学算法和神经声码器深入研究:</h1><figure class="kf kg kh ki gt kj"><div class="bz fp l di"><div class="ol kl l"/></div><p class="km kn gj gh gi ko kp bd b be z dk translated">资料来源:Giphy</p></figure><h1 id="e0bf" class="kq kr iq bd ks kt ku kv kw kx ky kz la jw lb jx lc jz ld ka le kc lf kd lg lh bi translated">声学算法:</h1><p id="2100" class="pw-post-body-paragraph li lj iq lk b ll lm jr ln lo lp ju lq lr ls lt lu lv lw lx ly lz ma mb mc md ij bi translated">当前声学算法使用的<strong class="lk ir"> <em class="nu">三种</em> </strong>主要架构类型有:</p><h2 id="25fc" class="mv kr iq bd ks mw mx dn kw my mz dp la lr na nb lc lv nc nd le lz ne nf lg ng bi translated">递归神经网络(RNN)</h2><figure class="kf kg kh ki gt kj gh gi paragraph-image"><div role="button" tabindex="0" class="nq nr di ns bf nt"><div class="gh gi om"><img src="../Images/2667b4e40571087881a577a2134c089b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lhFm46ytpyIozaSTHDhqPg.png"/></div></div><p class="km kn gj gh gi ko kp bd b be z dk translated">来源:<a class="ae me" href="https://arxiv.org/abs/1712.05884v2" rel="noopener ugc nofollow" target="_blank">通过根据Mel谱图预测调节WaveNet的自然TTS合成</a></p></figure><p id="e984" class="pw-post-body-paragraph li lj iq lk b ll mh jr ln lo mi ju lq lr nl lt lu lv nm lx ly lz nn mb mc md ij bi translated">RNN是用于诸如非自回归TacoTron 2的算法的声学模型框架，并且是递归的序列到序列特征预测网络，其关注于从输入字符序列预测Mel谱图帧的序列。<a class="ae me" href="https://paperswithcode.com/method/wavenet" rel="noopener ugc nofollow" target="_blank"> WaveNet </a>的修改版本根据预测的Mel谱图帧生成时域波形样本。TacoTron 2架构在改善语音质量方面比其他方法(如级联、参数和自回归TacoTron 1)向前迈进了一大步。TacoTron 2的架构如上图所示。</p><h2 id="1585" class="mv kr iq bd ks mw mx dn kw my mz dp la lr na nb lc lv nc nd le lz ne nf lg ng bi translated">卷积神经网络(CNN)</h2><p id="7d87" class="pw-post-body-paragraph li lj iq lk b ll lm jr ln lo lp ju lq lr ls lt lu lv lw lx ly lz ma mb mc md ij bi translated">像DeepVoice 3这样的算法利用全卷积网络结构进行语音合成，从字符生成Mel-spectro gram，并扩展到真实世界的多说话人数据集。这类声学算法类似于主流CNN通过对每类图像的训练来分类狗和猫，但在这种情况下，它的训练是在梅尔频谱图上进行的。DeepVoice 3通过使用更紧凑的序列到序列模型和直接预测Mel-spectrogram而不是复杂的语言特征，改进了以前的DeepVoice 1/2系统。</p><h2 id="34cc" class="mv kr iq bd ks mw mx dn kw my mz dp la lr na nb lc lv nc nd le lz ne nf lg ng bi translated">变压器</h2><figure class="kf kg kh ki gt kj gh gi paragraph-image"><div class="gh gi on"><img src="../Images/9681ac23a8a7bd83ed9c7e85ff5a49be.png" data-original-src="https://miro.medium.com/v2/resize:fit:1382/format:webp/1*leFk9OgCgd64G-bQMZoKdg.png"/></div><p class="km kn gj gh gi ko kp bd b be z dk translated">来源:<a class="ae me" href="https://www.microsoft.com/en-us/research/lab/microsoft-research-asia/articles/fastspeech-2-fast-and-high-quality-end-to-end-text-to-speech/" rel="noopener ugc nofollow" target="_blank">微软研究院博客FastSpeech2，作者:徐坦</a></p></figure><p id="d8d3" class="pw-post-body-paragraph li lj iq lk b ll mh jr ln lo mi ju lq lr nl lt lu lv nm lx ly lz nn mb mc md ij bi translated">基于转换器的(自我关注)声学算法，如FastSpeech 1/2，利用基于转换器的编码器-注意力-解码器架构从音素生成Mel频谱图。它们是<a class="ae me" rel="noopener" target="_blank" href="/transformer-neural-network-step-by-step-breakdown-of-the-beast-b3e096dc857f">转换网络</a>的衍生，后者是一种架构，旨在解决序列到序列的任务，同时轻松处理远程依赖性。与具有自回归编码器注意力解码器的其他模型(例如TacoTron 2)相比，基于变压器的算法通过前馈变压器网络显著加快了语音合成，从而并行生成Mel频谱图。最值得注意的是，该算法通过完全去除作为中间输出的Mel频谱图来简化输出，并且在推理过程中直接从文本生成语音波形，从而享有训练中完全端到端联合优化和推理中低延迟的好处。FastSpeech 2实现了比FastSpeech 1更好的语音质量，并且通过利用基于变换器的架构保持了快速、鲁棒和可控的语音合成的优点；这可以在上面的FastSpeech 2图中看到，重要的是要注意方差适配器部分是使用FastSpeech 2时与其他声学算法/框架相比的主要区别。</p><h1 id="698d" class="kq kr iq bd ks kt ku kv kw kx ky kz la jw lb jx lc jz ld ka le kc lf kd lg lh bi translated">神经声码器算法</h1><p id="9f50" class="pw-post-body-paragraph li lj iq lk b ll lm jr ln lo lp ju lq lr ls lt lu lv lw lx ly lz ma mb mc md ij bi translated">用于将声学模型输出声谱图转换成我们的目标音频波形(也称为合成语音)。非自回归模型最有前途，但不如自回归模型。</p><p id="dcd8" class="pw-post-body-paragraph li lj iq lk b ll mh jr ln lo mi ju lq lr nl lt lu lv nm lx ly lz nn mb mc md ij bi translated">基本上有四种主要类型的声码器:</p><ul class=""><li id="026e" class="mf mg iq lk b ll mh lo mi lr mj lv mk lz ml md mm mn mo mp bi translated"><strong class="lk ir">自回归:</strong> WaveNet是第一个基于神经网络的声码器，它扩展卷积以自回归生成波形点。这种方法的新颖之处在于它能够利用几乎没有关于输入音频信号的先验知识，而是依赖于端到端的学习。</li><li id="fb31" class="mf mg iq lk b ll mq lo mr lr ms lv mt lz mu md mm mn mo mp bi translated"><strong class="lk ir">基于流程:</strong>用一系列可逆映射转换概率密度的生成模型。这种方法有两个不同的框架，一个利用自回归转换，另一个<a class="ae me" href="https://arxiv.org/abs/2109.13675." rel="noopener ugc nofollow" target="_blank">双向</a>转换。</li><li id="b03b" class="mf mg iq lk b ll mq lo mr lr ms lv mt lz mu md mm mn mo mp bi translated"><strong class="lk ir">基于GAN:</strong>仿照典型的生成对抗网络(GANs)用于图像生成任务。任何一个GAN框架都是由一个用于数据生成的生成器和一个用于判断生成器数据的鉴别器组成的。我总是想到的类比是强盗和警察，强盗总是试图想出新的计划/袭击，而警察试图阻止他们。大多数当前的基于GAN的声码器将利用扩展卷积来增加感受野以模拟波形序列中的长相关性，并利用转置卷积来上采样条件信息以匹配波形长度。另一方面，鉴别器专注于设计模型以捕获波形特征，从而为发生器提供更好的引导信号。损失函数提高了对抗训练的稳定性和效率，并提高了音频质量。如下表所示，许多现代神经声码器都是基于GAN的，并将使用各种方法来实现发生器、鉴别器和损失函数。</li></ul><figure class="kf kg kh ki gt kj gh gi paragraph-image"><div role="button" tabindex="0" class="nq nr di ns bf nt"><div class="gh gi oo"><img src="../Images/0e6d4292b7c45e8e9cc15a5206e10130.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kbFB8K8HrEibqgyhecd7EA.png"/></div></div><p class="km kn gj gh gi ko kp bd b be z dk translated">来源:关于神经语音合成的调查</p></figure><p id="7be7" class="pw-post-body-paragraph li lj iq lk b ll mh jr ln lo mi ju lq lr nl lt lu lv nm lx ly lz nn mb mc md ij bi translated"><strong class="lk ir">基于扩散:</strong>利用声码器的去噪扩散概率模型，直觉上是数据和潜在分布之间的映射具有扩散过程和逆过程。目前的研究表明，这类声码器产生高质量的语音，但需要大量的时间进行推理。</p><p id="d7e5" class="pw-post-body-paragraph li lj iq lk b ll mh jr ln lo mi ju lq lr nl lt lu lv nm lx ly lz nn mb mc md ij bi translated">下图展示了所有类型的神经声码器及其架构。</p><figure class="kf kg kh ki gt kj gh gi paragraph-image"><div role="button" tabindex="0" class="nq nr di ns bf nt"><div class="gh gi op"><img src="../Images/935323955a9be9a9c3036c0ea45267d4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7kkT-lNZL_MZ2xa5_IYMkg.png"/></div></div><p class="km kn gj gh gi ko kp bd b be z dk translated">来源:关于神经语音合成的调查</p></figure><h1 id="e1ec" class="kq kr iq bd ks kt ku kv kw kx ky kz la jw lb jx lc jz ld ka le kc lf kd lg lh bi translated">下一代端到端TTS</h1><figure class="kf kg kh ki gt kj gh gi paragraph-image"><div role="button" tabindex="0" class="nq nr di ns bf nt"><div class="gh gi oq"><img src="../Images/f858f3adc598b324a9c56108e893e25b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*x72MMDKy1pfqNyIJ.png"/></div></div><p class="km kn gj gh gi ko kp bd b be z dk translated">来源:作者图片(下一代端到端TTS框架)</p></figure><p id="6d5c" class="pw-post-body-paragraph li lj iq lk b ll mh jr ln lo mi ju lq lr nl lt lu lv nm lx ly lz nn mb mc md ij bi translated">发展图中定义的TTS的当前研究和发展正朝着端到端TTS的方向发展，尽管与主流的两阶段方法相比，由于当前的一些质量限制和训练时间资源要求，它还没有达到临界质量。也就是说，与SPSS相比，端到端TTS有一些明显的优势:</p><ul class=""><li id="5db6" class="mf mg iq lk b ll mh lo mi lr mj lv mk lz ml md mm mn mo mp bi translated">大幅降低开发和部署成本。</li><li id="5efb" class="mf mg iq lk b ll mq lo mr lr ms lv mt lz mu md mm mn mo mp bi translated">拥有连接架构和端到端优化意味着减少当前主流2阶段方法中的错误传播。</li><li id="d887" class="mf mg iq lk b ll mq lo mr lr ms lv mt lz mu md mm mn mo mp bi translated">需要较少的人工注释和特性开发。</li><li id="6dc3" class="mf mg iq lk b ll mq lo mr lr ms lv mt lz mu md mm mn mo mp bi translated">传统的声学模型需要语言和声学特征之间的对齐，而基于序列到序列的神经模型通过注意或预测隐含地学习对齐</li><li id="8dcc" class="mf mg iq lk b ll mq lo mr lr ms lv mt lz mu md mm mn mo mp bi translated">持续时间联合，这更像是端到端的，并且需要更少的预处理</li><li id="7825" class="mf mg iq lk b ll mq lo mr lr ms lv mt lz mu md mm mn mo mp bi translated">随着神经网络建模能力的不断增强，语言特征被简化为仅仅是字符或音素序列，而声学特征已经从低维的、浓缩的</li></ul><figure class="kf kg kh ki gt kj gh gi paragraph-image"><div role="button" tabindex="0" class="nq nr di ns bf nt"><div class="gh gi or"><img src="../Images/44a5679fb8016fe416fef34e1d9904e1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qCS0BAT3HiVi48fmz0wjkw.png"/></div></div><p class="km kn gj gh gi ko kp bd b be z dk translated">来源:关于神经语音合成的调查</p></figure><p id="646f" class="pw-post-body-paragraph li lj iq lk b ll mh jr ln lo mi ju lq lr nl lt lu lv nm lx ly lz nn mb mc md ij bi translated">在上图中，我们可以看到当前提出的端到端TTS算法的概述，尽管不是所有的<em class="nu">都有开源代码。</em></p><p id="b158" class="pw-post-body-paragraph li lj iq lk b ll mh jr ln lo mi ju lq lr nl lt lu lv nm lx ly lz nn mb mc md ij bi translated">FastSpeech 2s被部署到微软Azure Managed TTS服务中，对我来说，这以一种应用的商业形式清楚地证明了该领域的未来状态。对我们来说幸运的是，开源的ESPnet 2有带对抗学习的条件变量自动编码器(<a class="ae me" href="https://arxiv.org/abs/2106.06103" rel="noopener ugc nofollow" target="_blank"> VITs </a>)可供使用，我计划在未来的帖子中实际介绍它。</p><h1 id="5069" class="kq kr iq bd ks kt ku kv kw kx ky kz la jw lb jx lc jz ld ka le kc lf kd lg lh bi translated">下一步是什么？</h1><p id="e8ed" class="pw-post-body-paragraph li lj iq lk b ll lm jr ln lo lp ju lq lr ls lt lu lv lw lx ly lz ma mb mc md ij bi translated">现在，我们已经在第1部分中使用EspNet 2做了一些简单的TTS，并且对声学算法、神经声码器和整个TTS架构有了很好的基础理解，让我们开始应用它吧！未来的文章将会介绍如何在ESPnet中进行语音增强、自动语音识别和训练我们的声音！</p><figure class="kf kg kh ki gt kj"><div class="bz fp l di"><div class="os kl l"/></div><p class="km kn gj gh gi ko kp bd b be z dk translated">资料来源:Giphy</p></figure><h1 id="dd37" class="kq kr iq bd ks kt ku kv kw kx ky kz la jw lb jx lc jz ld ka le kc lf kd lg lh bi translated">参考</h1><p id="de26" class="pw-post-body-paragraph li lj iq lk b ll lm jr ln lo lp ju lq lr ls lt lu lv lw lx ly lz ma mb mc md ij bi translated">如前所述，如果没有微软亚洲研究院团队在“<a class="ae me" href="https://arxiv.org/abs/2106.15561" rel="noopener ugc nofollow" target="_blank">神经语音合成调查</a>”中提供的精确措辞、描述和照片，这篇文章是不可能发表的。</p><ul class=""><li id="028e" class="mf mg iq lk b ll mh lo mi lr mj lv mk lz ml md mm mn mo mp bi translated"><a class="ae me" href="https://arxiv.org/abs/2106.15561" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/2106.15561</a></li><li id="dd0c" class="mf mg iq lk b ll mq lo mr lr ms lv mt lz mu md mm mn mo mp bi translated"><a class="ae me" href="https://speechresearch.github.io/fastspeech/" rel="noopener ugc nofollow" target="_blank">https://speechresearch.github.io/fastspeech/</a></li><li id="286b" class="mf mg iq lk b ll mq lo mr lr ms lv mt lz mu md mm mn mo mp bi translated"><a class="ae me" href="https://www.microsoft.com/en-us/research/blog/fastspeech-new-text-to-speech-model-improves-on-speed-accuracy-and-controllability/" rel="noopener ugc nofollow" target="_blank">https://www . Microsoft . com/en-us/research/blog/fast speech-new-text-to-speech-model-improves-on-speed-accuracy-and-control ability/</a></li><li id="25e6" class="mf mg iq lk b ll mq lo mr lr ms lv mt lz mu md mm mn mo mp bi translated"><a class="ae me" rel="noopener" target="_blank" href="/audio-deep-learning-made-simple-part-2-why-mel-spectrograms-perform-better-aad889a93505">https://towards data science . com/audio-deep-learning-made-simple-part-2-why-Mel-spectrograms-perform-better-aad 889 a 93505</a></li><li id="ff49" class="mf mg iq lk b ll mq lo mr lr ms lv mt lz mu md mm mn mo mp bi translated"><a class="ae me" href="https://theaisummer.com/text-to-speech/" rel="noopener ugc nofollow" target="_blank">https://theaisummer.com/text-to-speech/</a></li><li id="5610" class="mf mg iq lk b ll mq lo mr lr ms lv mt lz mu md mm mn mo mp bi translated"><a class="ae me" href="https://www.youtube.com/watch?v=knzT7M6qsl0" rel="noopener ugc nofollow" target="_blank">https://www.youtube.com/watch?v=knzT7M6qsl0</a></li></ul></div></div>    
</body>
</html>