<html>
<head>
<title>Generalized IoU loss for Object Detection with Torchvision</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">火炬视觉目标检测的广义IoU损失</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/generalized-iou-loss-for-object-detection-with-torchvision-9534029d1a89?source=collection_archive---------11-----------------------#2021-09-05">https://towardsdatascience.com/generalized-iou-loss-for-object-detection-with-torchvision-9534029d1a89?source=collection_archive---------11-----------------------#2021-09-05</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="237b" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">关于如何在Torchvision对象检测中使用GIoU自定义损失函数的教程</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ki"><img src="../Images/d251b5df101cabb38a3e5ac33fc4fcd1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1238/format:webp/1*Ro_ywFPkr5eK8PuV85qShA.png"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">使用GIoU损失函数后目标检测的改进(<a class="ae ku" href="https://images.unsplash.com/photo-1598896130238-5606c05ec207?ixid=MnwxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8&amp;ixlib=rb-1.2.1&amp;auto=format&amp;fit=crop&amp;w=334&amp;q=80" rel="noopener ugc nofollow" target="_blank">来源</a></p></figure><p id="cf90" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">在对象检测任务中，最常见的评估度量是IoU，这促使我们在训练中最小化这样的度量，看看我们是否取得了一些进展。Hamid Rezatofighi和他的同事表明，使用广义IoU ( <strong class="kx iu"> <em class="lr"> GIoU </em> </strong>)损失函数优于使用其他标准损失函数的最先进的对象检测方法。</p><p id="1484" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">我不想浪费你的时间解释什么是<strong class="kx iu"><em class="lr">IoU</em></strong><strong class="kx iu"><em class="lr">GIoU</em></strong>。如果你在这里，你可能对这些功能很熟悉。你可以在这里找到完整的描述。</p><p id="954d" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">在<a class="ae ku" href="https://github.com/pytorch/vision" rel="noopener ugc nofollow" target="_blank">火炬视觉</a>物体检测模型中，RCNN家族中的默认损失函数是<a class="ae ku" href="https://pytorch.org/docs/stable/generated/torch.nn.SmoothL1Loss.html" rel="noopener ugc nofollow" target="_blank">平滑L1损失函数</a>。模型中没有改变损失函数的选项，但是如果您对使用平滑L1损失不感兴趣，可以简单地定义您的自定义损失并将其替换为平滑损失。</p><h1 id="75c2" class="ls lt it bd lu lv lw lx ly lz ma mb mc jz md ka me kc mf kd mg kf mh kg mi mj bi translated">GIoU损失函数</h1><p id="8d14" class="pw-post-body-paragraph kv kw it kx b ky mk ju la lb ml jx ld le mm lg lh li mn lk ll lm mo lo lp lq im bi translated">我们计划计算以下GIoU:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="mq mr di ms bf mt"><div class="gh gi mp"><img src="../Images/2137b6912a7e726fe811eb494f7bda2f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AO24IoYBsnneViLzEqrS0A.png"/></div></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">IoU和GIoU(查看更多详情<a class="ae ku" href="https://giou.stanford.edu/GIoU.pdf" rel="noopener ugc nofollow" target="_blank">此处</a>)</p></figure><p id="b823" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">Torchvision提供了边界框的交集和并集计算，这使得计算GIoU变得非常容易。</p><p id="7b2d" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">我们可以通过从<code class="fe mu mv mw mx b">torchvision.ops.boxes</code>导入<code class="fe mu mv mw mx b">_box_inter_union</code>来直接计算盒子的交集和并集。</p><p id="d38b" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">为了计算包围盒子的最小凸形的面积，首先我们找到<strong class="kx iu"> <em class="lr"> C: </em> </strong>的坐标</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi my"><img src="../Images/b1d75d61b2c7b1917d9402e435323c29.png" data-original-src="https://miro.medium.com/v2/resize:fit:636/format:webp/1*g1d-GDMzgdqgxikbXAFmjg.png"/></div></figure><p id="4fbe" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">然后，我们计算它的面积(下面代码的第17行)。</p><p id="0da1" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated"><strong class="kx iu"> GIoU损失为:<em class="lr"> 1- GIoU </em> </strong> <em class="lr">。</em></p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mz na l"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">物体检测的GIoU损失函数(类似于<a class="ae ku" href="https://github.com/facebookresearch/fvcore/blob/master/fvcore/nn/giou_loss.py" rel="noopener ugc nofollow" target="_blank"> fvcore </a>)</p></figure><h1 id="24c1" class="ls lt it bd lu lv lw lx ly lz ma mb mc jz md ka me kc mf kd mg kf mh kg mi mj bi translated">火炬视觉中的自定义损失函数</h1><p id="b2e6" class="pw-post-body-paragraph kv kw it kx b ky mk ju la lb ml jx ld le mm lg lh li mn lk ll lm mo lo lp lq im bi translated">主RCNN框架在<code class="fe mu mv mw mx b"><a class="ae ku" href="https://github.com/pytorch/vision/blob/main/torchvision/models/detection/roi_heads.py" rel="noopener ugc nofollow" target="_blank">roi_heads.py</a></code>中完成，FastRCNN和MaskRCNN损失函数位于此处。因此，要使用您的自定义损失函数，您需要从<code class="fe mu mv mw mx b">roi_heads</code>导入它并用您的自定义函数替换它，正如我在下面代码的第22行中所做的:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mz na l"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">FastRCNN的自定义损失函数</p></figure><h1 id="6643" class="ls lt it bd lu lv lw lx ly lz ma mb mc jz md ka me kc mf kd mg kf mh kg mi mj bi translated">摘要</h1><p id="0a1b" class="pw-post-body-paragraph kv kw it kx b ky mk ju la lb ml jx ld le mm lg lh li mn lk ll lm mo lo lp lq im bi translated">在这篇短文中，您可以找到使用GIoU loss函数通过Torchvision进行物体检测所需的全部内容。</p><p id="7e8c" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated"><strong class="kx iu">参考文献:</strong></p><p id="8299" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">[1] <a class="ae ku" href="https://giou.stanford.edu/GIoU.pdf" rel="noopener ugc nofollow" target="_blank">并集上的广义交集:包围盒回归的度量和损失</a></p><div class="nb nc gp gr nd ne"><a href="https://sciencenotes.medium.com/membership" rel="noopener follow" target="_blank"><div class="nf ab fo"><div class="ng ab nh cl cj ni"><h2 class="bd iu gy z fp nj fr fs nk fu fw is bi translated">加入媒体阅读伟大的教程和故事！</h2><div class="nl l"><h3 class="bd b gy z fp nj fr fs nk fu fw dk translated">我写机器学习、深度学习和数据科学教程。升级阅读更多。</h3></div><div class="nm l"><p class="bd b dl z fp nj fr fs nk fu fw dk translated">sciencenotes.medium.com</p></div></div><div class="nn l"><div class="no l np nq nr nn ns ko ne"/></div></div></a></div><p id="10b2" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">杰瑞米·哈里斯，<a class="nt nu ep" href="https://medium.com/u/e8cce06956c9?source=post_page-----9534029d1a89--------------------------------" rel="noopener" target="_blank">拉胡尔·阿加瓦尔</a>，<a class="nt nu ep" href="https://medium.com/u/1d81a71197ab?source=post_page-----9534029d1a89--------------------------------" rel="noopener" target="_blank">大卫·麦克</a>，<a class="nt nu ep" href="https://medium.com/u/c7ad13abfbb7?source=post_page-----9534029d1a89--------------------------------" rel="noopener" target="_blank">库马尔·什里达尔</a></p></div></div>    
</body>
</html>