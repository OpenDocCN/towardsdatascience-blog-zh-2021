<html>
<head>
<title>Multi-GPUs and Custom Training Loops in TensorFlow 2</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">TensorFlow 2中的多GPU和自定义训练循环</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/multi-gpus-and-custom-training-loops-in-tensorflow-2-15b4b86b53bd?source=collection_archive---------13-----------------------#2021-06-01">https://towardsdatascience.com/multi-gpus-and-custom-training-loops-in-tensorflow-2-15b4b86b53bd?source=collection_archive---------13-----------------------#2021-06-01</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="df09" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">如何使用tf.distribute.MirroredStrategy在多个GPU上训练自定义训练循环模型的简明示例。</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/2ff3a52f608bb32ba9690cfc07cf9cdd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2wImCG-FQeg7qylIMW4dvA.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">使用<code class="fe kv kw kx ky b">tf.distribute.MirroredStrategy</code>在2个GPU上进行分布式训练【图片由作者提供】</p></figure><p id="0349" class="pw-post-body-paragraph kz la iq lb b lc ld jr le lf lg ju lh li lj lk ll lm ln lo lp lq lr ls lt lu ij bi translated">TensorFlow提供了许多关于如何执行分布式训练的优秀教程，尽管这些示例中的大多数严重依赖于Keras API，这可能会限制想要实现更复杂模型和训练过程的用户。本教程提供了如何在TensorFlow 2.4中将<code class="fe kv kw kx ky b">tf.distribute.MirroredStategy</code>与自定义训练循环一起使用的简明示例。为此，我们改编了由<a class="ae lv" href="https://keras.io/examples/generative/cyclegan/" rel="noopener ugc nofollow" target="_blank"> Keras </a>和<a class="ae lv" href="https://www.tensorflow.org/tutorials/generative/cyclegan" rel="noopener ugc nofollow" target="_blank"> TensorFlow </a>编写的CycleGAN [1]教程，并支持多GPU训练。</p><p id="7bfc" class="pw-post-body-paragraph kz la iq lb b lc ld jr le lf lg ju lh li lj lk ll lm ln lo lp lq lr ls lt lu ij bi translated">请记住，尽管本指南不是关于CycleGAN本身的，但与大多数关于该主题的现有教程相比，CycleGAN由于其(相对)复杂的损耗计算和培训程序而被用作示例。</p><p id="5702" class="pw-post-body-paragraph kz la iq lb b lc ld jr le lf lg ju lh li lj lk ll lm ln lo lp lq lr ls lt lu ij bi translated">本教程中使用的完整代码可在github.com/bryanlimy/tf2-cyclegan<a class="ae lv" href="https://github.com/bryanlimy/tf2-cyclegan" rel="noopener ugc nofollow" target="_blank">获得。</a></p><h2 id="bef1" class="lw lx iq bd ly lz ma dn mb mc md dp me li mf mg mh lm mi mj mk lq ml mm mn mo bi translated">摘要</h2><p id="c569" class="pw-post-body-paragraph kz la iq lb b lc mp jr le lf mq ju lh li mr lk ll lm ms lo lp lq mt ls lt lu ij bi translated">一般来说，TensorFlow 2中任何现有的自定义训练循环代码都可以通过6个步骤转换为与<code class="fe kv kw kx ky b"><a class="ae lv" href="https://www.tensorflow.org/api_docs/python/tf/distribute/Strategy" rel="noopener ugc nofollow" target="_blank">tf.distribute.Strategy</a></code>一起工作:</p><ol class=""><li id="c130" class="mu mv iq lb b lc ld lf lg li mw lm mx lq my lu mz na nb nc bi translated">初始化<code class="fe kv kw kx ky b">tf.distribute.MirroredStrategy</code></li><li id="a477" class="mu mv iq lb b lc nd lf ne li nf lm ng lq nh lu mz na nb nc bi translated">分发<code class="fe kv kw kx ky b">tf.data.Dataset</code></li><li id="eabe" class="mu mv iq lb b lc nd lf ne li nf lm ng lq nh lu mz na nb nc bi translated">每个副本的损失计算和聚合</li><li id="4f6b" class="mu mv iq lb b lc nd lf ne li nf lm ng lq nh lu mz na nb nc bi translated">用<code class="fe kv kw kx ky b">tf.distribute.MirroredStrategy</code>初始化模型、优化器和检查点</li><li id="1822" class="mu mv iq lb b lc nd lf ne li nf lm ng lq nh lu mz na nb nc bi translated">将计算分配给每个副本</li><li id="99f1" class="mu mv iq lb b lc nd lf ne li nf lm ng lq nh lu mz na nb nc bi translated">聚合返回值和指标</li></ol><h2 id="97f3" class="lw lx iq bd ly lz ma dn mb mc md dp me li mf mg mh lm mi mj mk lq ml mm mn mo bi translated">包装要求</h2><p id="210f" class="pw-post-body-paragraph kz la iq lb b lc mp jr le lf mq ju lh li mr lk ll lm ms lo lp lq mt ls lt lu ij bi translated">本教程需要以下Python包:</p><pre class="kg kh ki kj gt ni ky nj nk aw nl bi"><span id="00b3" class="lw lx iq ky b gy nm nn l no np">tqdm==4.61.0<br/>yapf==0.31.0<br/>matplotlib==3.4.2<br/>tensorflow==2.4.1<br/>tensorflow_addons==0.13.0<br/>tensorflow_datasets==4.3.0</span></pre><p id="3399" class="pw-post-body-paragraph kz la iq lb b lc ld jr le lf lg ju lh li lj lk ll lm ln lo lp lq lr ls lt lu ij bi translated">此外，还需要CUDA、cuDNN和NCCL。有关TensorFlow中GPU支持的更多信息，请参见<a class="ae lv" href="https://www.tensorflow.org/install/gpu" rel="noopener ugc nofollow" target="_blank">tensorflow.org/install/gpu</a>。如果您使用的是<code class="fe kv kw kx ky b">conda</code>虚拟环境，您可以使用以下命令安装上述库:</p><pre class="kg kh ki kj gt ni ky nj nk aw nl bi"><span id="f5b9" class="lw lx iq ky b gy nm nn l no np">conda install -c nvidia <em class="nq">cudatoolkit</em>=11.0 <em class="nq">cudnn</em> nccl</span></pre><p id="e3cb" class="pw-post-body-paragraph kz la iq lb b lc ld jr le lf lg ju lh li lj lk ll lm ln lo lp lq lr ls lt lu ij bi translated">您可以使用命令<code class="fe kv kw kx ky b">nvidia-smi</code>来检查系统中可用的GPU数量。例如，我使用的系统配有8个Nvidia GeForce 2080Ti GPUs:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nr"><img src="../Images/1bc825c309d5529efe455f709b6ac668.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OQlzN4JsVvkWV8ncKk9a_g.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><code class="fe kv kw kx ky b">nvidia-smi</code>输出【图片作者】</p></figure><h2 id="69b9" class="lw lx iq bd ly lz ma dn mb mc md dp me li mf mg mh lm mi mj mk lq ml mm mn mo bi translated">初始化<code class="fe kv kw kx ky b">tf.distribute.MirroredStrategy</code></h2><p id="35a0" class="pw-post-body-paragraph kz la iq lb b lc mp jr le lf mq ju lh li mr lk ll lm ms lo lp lq mt ls lt lu ij bi translated">我们首先导入所需的包:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ns nt l"/></div></figure><p id="220b" class="pw-post-body-paragraph kz la iq lb b lc ld jr le lf lg ju lh li lj lk ll lm ln lo lp lq lr ls lt lu ij bi translated">然后我们可以初始化<code class="fe kv kw kx ky b">tf.distribute.Strategy</code>:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ns nt l"/></div></figure><p id="c8b9" class="pw-post-body-paragraph kz la iq lb b lc ld jr le lf lg ju lh li lj lk ll lm ln lo lp lq lr ls lt lu ij bi translated">请注意，当<code class="fe kv kw kx ky b">devices=None</code>时，TensorFlow将自动检测所有可用设备。我更喜欢在每次运行开始时手动设置环境变量<code class="fe kv kw kx ky b">CUDA_VISIBLE_DEVICES</code>，而不是以编程方式设置<code class="fe kv kw kx ky b">devices</code>参数。请注意，<code class="fe kv kw kx ky b">nvidia-smi</code>中的GPU ID不一定与<code class="fe kv kw kx ky b">CUDA_VISIBLE_DEVICES</code>的编号相关。</p><p id="2b34" class="pw-post-body-paragraph kz la iq lb b lc ld jr le lf lg ju lh li lj lk ll lm ln lo lp lq lr ls lt lu ij bi translated">使用3个GPU(即<code class="fe kv kw kx ky b">CUDA_VISIBLE_DEVICES=0,1,2</code>)运行上述代码片段将会打印出以下内容:</p><pre class="kg kh ki kj gt ni ky nj nk aw nl bi"><span id="336c" class="lw lx iq ky b gy nm nn l no np">Number of devices: 3</span></pre><h2 id="0ef9" class="lw lx iq bd ly lz ma dn mb mc md dp me li mf mg mh lm mi mj mk lq ml mm mn mo bi translated">数据集设置</h2><p id="01ee" class="pw-post-body-paragraph kz la iq lb b lc mp jr le lf mq ju lh li mr lk ll lm ms lo lp lq mt ls lt lu ij bi translated">我们从<a class="ae lv" href="https://www.tensorflow.org/datasets" rel="noopener ugc nofollow" target="_blank"> TensorFlow数据集</a>加载<code class="fe kv kw kx ky b">horse2zebra</code>数据集，并以与<a class="ae lv" href="https://keras.io/examples/generative/cyclegan/" rel="noopener ugc nofollow" target="_blank"> Keras教程</a>相同的方式预处理4个数据集(<code class="fe kv kw kx ky b">train_horses</code>、<code class="fe kv kw kx ky b">train_zebras</code>、<code class="fe kv kw kx ky b">test_horses</code>和<code class="fe kv kw kx ky b">test_zebras</code>)，包括归一化和增强。此外，我们从测试集中提取前5个样本，并创建<code class="fe kv kw kx ky b">plot_ds</code>用于绘图目的。</p><p id="2f42" class="pw-post-body-paragraph kz la iq lb b lc ld jr le lf lg ju lh li lj lk ll lm ln lo lp lq lr ls lt lu ij bi translated">在为分布式培训构建<code class="fe kv kw kx ky b">tf.data.Dataset</code>时，记住以下几点很重要:</p><ul class=""><li id="b07a" class="mu mv iq lb b lc ld lf lg li mw lm mx lq my lu nu na nb nc bi translated">使用<code class="fe kv kw kx ky b">tf.data.Dataset.zip</code>压缩前，确保所有数据集具有相同数量的样本。<code class="fe kv kw kx ky b">horse2zebra</code>数据集由奇数匹马和斑马图像组成，因此我们必须调用下面的<code class="fe kv kw kx ky b">tf.data.Dataset.take</code>。</li><li id="b17a" class="mu mv iq lb b lc nd lf ne li nf lm ng lq nh lu nu na nb nc bi translated">用<code class="fe kv kw kx ky b">GLOBAL_BATCH_SIZE</code>而不是<code class="fe kv kw kx ky b">BATCH_SIZE</code>批处理每个数据集。</li><li id="aee2" class="mu mv iq lb b lc nd lf ne li nf lm ng lq nh lu nu na nb nc bi translated"><code class="fe kv kw kx ky b">tf.data.Dataset.cache</code>和<code class="fe kv kw kx ky b">tf.data.Dataset.prefetch</code>数据集，以确保在不停止计算的情况下向GPU提供足够的数据量。</li><li id="0805" class="mu mv iq lb b lc nd lf ne li nf lm ng lq nh lu nu na nb nc bi translated">可以查看TensorFlow的tf.data API 关于<a class="ae lv" href="https://www.tensorflow.org/guide/data_performance" rel="noopener ugc nofollow" target="_blank">更好性能的优秀教程。我还打算写一篇关于<code class="fe kv kw kx ky b">tf.data</code>的教程，包括高效写作和阅读<code class="fe kv kw kx ky b">TFRecords</code>，敬请关注。</a></li></ul><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ns nt l"/></div></figure><p id="85c3" class="pw-post-body-paragraph kz la iq lb b lc ld jr le lf lg ju lh li lj lk ll lm ln lo lp lq lr ls lt lu ij bi translated">在构建数据集之后，我们可以通过简单地调用<code class="fe kv kw kx ky b">strategy.experimental_distribute_dataset</code>方法来分发它们。请注意，我们没有分发<code class="fe kv kw kx ky b">plot_ds</code>,因为我们将在单个设备上执行推理。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ns nt l"/></div></figure><h2 id="0167" class="lw lx iq bd ly lz ma dn mb mc md dp me li mf mg mh lm mi mj mk lq ml mm mn mo bi translated">构建生成器和鉴别器</h2><p id="156d" class="pw-post-body-paragraph kz la iq lb b lc mp jr le lf mq ju lh li mr lk ll lm ms lo lp lq mt ls lt lu ij bi translated">发生器和鉴别器的架构与<a class="ae lv" href="https://keras.io/examples/generative/cyclegan/" rel="noopener ugc nofollow" target="_blank"> Keras教程</a>中的模型相同。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ns nt l"/></div></figure><h2 id="d955" class="lw lx iq bd ly lz ma dn mb mc md dp me li mf mg mh lm mi mj mk lq ml mm mn mo bi translated">每样本损失计算</h2><p id="b938" class="pw-post-body-paragraph kz la iq lb b lc mp jr le lf mq ju lh li mr lk ll lm ms lo lp lq mt ls lt lu ij bi translated"><code class="fe kv kw kx ky b">tf.losses</code>和<code class="fe kv kw kx ky b">tf.keras.losses</code>中的损失函数默认返回给定批次的平均损失。用分布式训练计算的平均损失应该是相对于全局批量大小，而不是每个复制品。因此，我将定义自己的损失计算，其中函数返回每个样本的损失。例如，如果标签张量具有形状<code class="fe kv kw kx ky b">(32, 256, 256, 3)</code>，那么返回的平均损失具有形状<code class="fe kv kw kx ky b">(32,)</code>。还需要一个额外的步骤来计算整体批次平均损失。</p><p id="6a63" class="pw-post-body-paragraph kz la iq lb b lc ld jr le lf lg ju lh li lj lk ll lm ln lo lp lq lr ls lt lu ij bi translated">为分布式训练编写损失函数时要记住的事情:</p><ul class=""><li id="817f" class="mu mv iq lb b lc ld lf lg li mw lm mx lq my lu nu na nb nc bi translated">验证损失函数的形状。</li><li id="e550" class="mu mv iq lb b lc nd lf ne li nf lm ng lq nh lu nu na nb nc bi translated">如果使用<code class="fe kv kw kx ky b"><a class="ae lv" href="https://www.tensorflow.org/api_docs/python/tf/keras/losses" rel="noopener ugc nofollow" target="_blank">tf.keras.losses</a></code>中的损失函数，损失减少必须是<code class="fe kv kw kx ky b">NONE</code>或<code class="fe kv kw kx ky b">SUM</code>中的一个。</li><li id="356d" class="mu mv iq lb b lc nd lf ne li nf lm ng lq nh lu nu na nb nc bi translated">请注意，<code class="fe kv kw kx ky b">tf.losses</code>和<code class="fe kv kw kx ky b">tf.keras.losses</code>中的一些函数，如<code class="fe kv kw kx ky b"><a class="ae lv" href="https://www.tensorflow.org/api_docs/python/tf/keras/losses/MSE" rel="noopener ugc nofollow" target="_blank">tf.losses.mean_squared_error</a></code>和<code class="fe kv kw kx ky b"><a class="ae lv" href="https://www.tensorflow.org/api_docs/python/tf/keras/losses/binary_crossentropy" rel="noopener ugc nofollow" target="_blank">tf.losses.binary_crossentropy</a></code>，会减少输出的最后一个维度，因此我们必须扩展标签和预测的最后一个维度，以确保在每样本减少之前正确的输出形状。请参考下面的功能<code class="fe kv kw kx ky b">BCE</code>。</li></ul><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ns nt l"/></div></figure><p id="539d" class="pw-post-body-paragraph kz la iq lb b lc ld jr le lf lg ju lh li lj lk ll lm ln lo lp lq lr ls lt lu ij bi translated">然后，我们可以定义CycleGAN中使用的损失函数。我添加了一个助手函数<code class="fe kv kw kx ky b">reduce_mean</code>，它返回全局平均损失。您也可以使用<code class="fe kv kw kx ky b"><a class="ae lv" href="https://www.tensorflow.org/api_docs/python/tf/nn/compute_average_loss" rel="noopener ugc nofollow" target="_blank">tf.nn.compute_average_loss</a></code>来执行相同的操作。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ns nt l"/></div></figure><h2 id="26d3" class="lw lx iq bd ly lz ma dn mb mc md dp me li mf mg mh lm mi mj mk lq ml mm mn mo bi translated">初始化模型、优化器和检查点</h2><p id="90e4" class="pw-post-body-paragraph kz la iq lb b lc mp jr le lf mq ju lh li mr lk ll lm ms lo lp lq mt ls lt lu ij bi translated">我们现在可以在<code class="fe kv kw kx ky b">tf.strategy.MirroredStrategy</code>范围内初始化我们的模型、优化器和<code class="fe kv kw kx ky b">tf.train.Checkpoint</code>。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ns nt l"/></div></figure><p id="fa7c" class="pw-post-body-paragraph kz la iq lb b lc ld jr le lf lg ju lh li lj lk ll lm ln lo lp lq lr ls lt lu ij bi translated">下面的内容与分布式培训教程无关，但是为了让生活更简单，我添加了<code class="fe kv kw kx ky b">Summary</code>类，用于简单的TensorBoard日志记录和一个绘制图像周期的辅助函数。这里我们创建一个目录<code class="fe kv kw kx ky b">OUTPUT_DIR='runs/'</code>来存储模型检查点和TensorBoard摘要。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ns nt l"/></div></figure><h2 id="952c" class="lw lx iq bd ly lz ma dn mb mc md dp me li mf mg mh lm mi mj mk lq ml mm mn mo bi translated">定制培训和测试步骤</h2><p id="fdea" class="pw-post-body-paragraph kz la iq lb b lc mp jr le lf mq ju lh li mr lk ll lm ms lo lp lq mt ls lt lu ij bi translated">这里的<code class="fe kv kw kx ky b">train_step</code>函数与<a class="ae lv" href="https://www.tensorflow.org/tutorials/generative/cyclegan" rel="noopener ugc nofollow" target="_blank">张量流教程</a>中的函数非常相似，除了它返回一个张量字典。利用分布式训练，每个复制品将单独调用<code class="fe kv kw kx ky b">train_step</code>。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ns nt l"/></div></figure><p id="f3d5" class="pw-post-body-paragraph kz la iq lb b lc ld jr le lf lg ju lh li lj lk ll lm ln lo lp lq lr ls lt lu ij bi translated"><code class="fe kv kw kx ky b">train_step</code>的输出示例:</p><pre class="kg kh ki kj gt ni ky nj nk aw nl bi"><span id="9adc" class="lw lx iq ky b gy nm nn l no np">{'loss_G/loss': &lt;tf.Tensor: shape=(), dtype=float32, numpy=0.99999964&gt;, 'loss_G/cycle': &lt;tf.Tensor: shape=(), dtype=float32, numpy=5.1983647&gt;, 'loss_G/identity': &lt;tf.Tensor: shape=(), dtype=float32, numpy=2.5991883&gt;, 'loss_G/total': &lt;tf.Tensor: shape=(), dtype=float32, numpy=8.797552&gt;, 'loss_F/loss': &lt;tf.Tensor: shape=(), dtype=float32, numpy=1.0&gt;, 'loss_F/cycle': &lt;tf.Tensor: shape=(), dtype=float32, numpy=3.6904013&gt;, 'loss_F/identity': &lt;tf.Tensor: shape=(), dtype=float32, numpy=1.8451972&gt;, 'loss_F/total': &lt;tf.Tensor: shape=(), dtype=float32, numpy=6.5355983&gt;, 'loss_X/loss': &lt;tf.Tensor: shape=(), dtype=float32, numpy=0.49949202&gt;, 'loss_Y/loss': &lt;tf.Tensor: shape=(), dtype=float32, numpy=0.49720365&gt;}</span></pre><p id="7843" class="pw-post-body-paragraph kz la iq lb b lc ld jr le lf lg ju lh li lj lk ll lm ln lo lp lq lr ls lt lu ij bi translated">然后我们调用<code class="fe kv kw kx ky b">strategy.run</code>将计算分布到所有副本上，每个副本计算并从<code class="fe kv kw kx ky b">train_step</code>返回相应的结果。由于我们的<code class="fe kv kw kx ky b">train_step</code>函数返回一个张量字典，因此，<code class="fe kv kw kx ky b">strategy.run</code>的返回值将成为一个张量字典。我添加了一个助手函数<code class="fe kv kw kx ky b">reduce_dict</code>，它简单地迭代字典中的每一项，并对每个键的所有值求和。</p><p id="54b6" class="pw-post-body-paragraph kz la iq lb b lc ld jr le lf lg ju lh li lj lk ll lm ln lo lp lq lr ls lt lu ij bi translated">分发计算时要记住的事情:</p><ul class=""><li id="dda1" class="mu mv iq lb b lc ld lf lg li mw lm mx lq my lu nu na nb nc bi translated">我们可以在<code class="fe kv kw kx ky b">tf.function</code>内外迭代数据集。</li><li id="9a9d" class="mu mv iq lb b lc nd lf ne li nf lm ng lq nh lu nu na nb nc bi translated">由于每个副本的结果都与全局批量大小有关，因此我们用<code class="fe kv kw kx ky b">strategy.reduce</code>计算<code class="fe kv kw kx ky b">SUM</code>而不是<code class="fe kv kw kx ky b">MEAN</code>。</li><li id="3af2" class="mu mv iq lb b lc nd lf ne li nf lm ng lq nh lu nu na nb nc bi translated">检查从<code class="fe kv kw kx ky b">train_step</code>返回的值(或您计划分配计算的任何函数)。例如，如果您的函数返回一个张量，那么从<code class="fe kv kw kx ky b">strategy.run</code>返回的值将是一个张量字典。</li></ul><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ns nt l"/></div></figure><p id="71ea" class="pw-post-body-paragraph kz la iq lb b lc ld jr le lf lg ju lh li lj lk ll lm ln lo lp lq lr ls lt lu ij bi translated">3个GPU上<code class="fe kv kw kx ky b">strategy.run(train_step, args=(x, y))</code>的输出示例(即<code class="fe kv kw kx ky b">CUDA_VISIBLE_DEVICES=0,1,2</code>):</p><pre class="kg kh ki kj gt ni ky nj nk aw nl bi"><span id="de5b" class="lw lx iq ky b gy nm nn l no np">{'loss_G/loss': PerReplica:{<br/>  0: &lt;tf.Tensor: shape=(), dtype=float32, numpy=0.33333334&gt;,<br/>  1: &lt;tf.Tensor: shape=(), dtype=float32, numpy=0.33333334&gt;,<br/>  2: &lt;tf.Tensor: shape=(), dtype=float32, numpy=0.33333334&gt;<br/>}, 'loss_G/cycle': PerReplica:{<br/>  0: &lt;tf.Tensor: shape=(), dtype=float32, numpy=1.2934016&gt;,<br/>  1: &lt;tf.Tensor: shape=(), dtype=float32, numpy=1.4167924&gt;,<br/>  2: &lt;tf.Tensor: shape=(), dtype=float32, numpy=1.2807639&gt;<br/>}, 'loss_G/identity': PerReplica:{<br/>  0: &lt;tf.Tensor: shape=(), dtype=float32, numpy=0.6467091&gt;,<br/>  1: &lt;tf.Tensor: shape=(), dtype=float32, numpy=0.70838636&gt;,<br/>  2: &lt;tf.Tensor: shape=(), dtype=float32, numpy=0.6403792&gt;<br/>}, 'loss_G/total': PerReplica:{<br/>  0: &lt;tf.Tensor: shape=(), dtype=float32, numpy=2.2734442&gt;,<br/>  1: &lt;tf.Tensor: shape=(), dtype=float32, numpy=2.458512&gt;,<br/>  2: &lt;tf.Tensor: shape=(), dtype=float32, numpy=2.2544765&gt;<br/>}, 'loss_F/loss': PerReplica:{<br/>  0: &lt;tf.Tensor: shape=(), dtype=float32, numpy=0.33333334&gt;,<br/>  1: &lt;tf.Tensor: shape=(), dtype=float32, numpy=0.33333334&gt;,<br/>  2: &lt;tf.Tensor: shape=(), dtype=float32, numpy=0.33333334&gt;<br/>}, 'loss_F/cycle': PerReplica:{<br/>  0: &lt;tf.Tensor: shape=(), dtype=float32, numpy=0.6893108&gt;,<br/>  1: &lt;tf.Tensor: shape=(), dtype=float32, numpy=1.1769139&gt;,<br/>  2: &lt;tf.Tensor: shape=(), dtype=float32, numpy=1.5197902&gt;<br/>}, 'loss_F/identity': PerReplica:{<br/>  0: &lt;tf.Tensor: shape=(), dtype=float32, numpy=0.34465796&gt;,<br/>  1: &lt;tf.Tensor: shape=(), dtype=float32, numpy=0.58845943&gt;,<br/>  2: &lt;tf.Tensor: shape=(), dtype=float32, numpy=0.75989777&gt;<br/>}, 'loss_F/total': PerReplica:{<br/>  0: &lt;tf.Tensor: shape=(), dtype=float32, numpy=1.3673022&gt;,<br/>  1: &lt;tf.Tensor: shape=(), dtype=float32, numpy=2.0987067&gt;,<br/>  2: &lt;tf.Tensor: shape=(), dtype=float32, numpy=2.6130214&gt;<br/>}, 'loss_X/loss': PerReplica:{<br/>  0: &lt;tf.Tensor: shape=(), dtype=float32, numpy=0.16675813&gt;,<br/>  1: &lt;tf.Tensor: shape=(), dtype=float32, numpy=0.16679758&gt;,<br/>  2: &lt;tf.Tensor: shape=(), dtype=float32, numpy=0.16684371&gt;<br/>}, 'loss_Y/loss': PerReplica:{<br/>  0: &lt;tf.Tensor: shape=(), dtype=float32, numpy=0.16681099&gt;,<br/>  1: &lt;tf.Tensor: shape=(), dtype=float32, numpy=0.1668036&gt;,<br/>  2: &lt;tf.Tensor: shape=(), dtype=float32, numpy=0.16669472&gt;<br/>}}</span></pre><p id="1aa8" class="pw-post-body-paragraph kz la iq lb b lc ld jr le lf lg ju lh li lj lk ll lm ln lo lp lq lr ls lt lu ij bi translated"><code class="fe kv kw kx ky b">reduce_dict</code>输出示例:</p><pre class="kg kh ki kj gt ni ky nj nk aw nl bi"><span id="bde1" class="lw lx iq ky b gy nm nn l no np">{'loss_G/loss': &lt;tf.Tensor: shape=(), dtype=float32, numpy=1.0&gt;, 'loss_G/cycle': &lt;tf.Tensor: shape=(), dtype=float32, numpy=3.990958&gt;, 'loss_G/identity': &lt;tf.Tensor: shape=(), dtype=float32, numpy=1.9954746&gt;, 'loss_G/total': &lt;tf.Tensor: shape=(), dtype=float32, numpy=6.986433&gt;, 'loss_F/loss': &lt;tf.Tensor: shape=(), dtype=float32, numpy=1.0&gt;, 'loss_F/cycle': &lt;tf.Tensor: shape=(), dtype=float32, numpy=3.386015&gt;, 'loss_F/identity': &lt;tf.Tensor: shape=(), dtype=float32, numpy=1.6930151&gt;, 'loss_F/total': &lt;tf.Tensor: shape=(), dtype=float32, numpy=6.07903&gt;, 'loss_X/loss': &lt;tf.Tensor: shape=(), dtype=float32, numpy=0.5003994&gt;, 'loss_Y/loss': &lt;tf.Tensor: shape=(), dtype=float32, numpy=0.5003093&gt;}</span></pre><p id="d5b8" class="pw-post-body-paragraph kz la iq lb b lc ld jr le lf lg ju lh li lj lk ll lm ln lo lp lq lr ls lt lu ij bi translated">测试步骤遵循相同的程序。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ns nt l"/></div></figure><h2 id="4a5f" class="lw lx iq bd ly lz ma dn mb mc md dp me li mf mg mh lm mi mj mk lq ml mm mn mo bi translated">训练循环</h2><p id="c890" class="pw-post-body-paragraph kz la iq lb b lc mp jr le lf mq ju lh li mr lk ll lm ms lo lp lq mt ls lt lu ij bi translated">剩下的工作就是以与任何典型的定制训练循环相同的方式构建训练和测试循环。我们保存一个检查点，每10个时期在TensorBoard上绘制5个样本。请注意，在恢复检查点时，必须在策略范围内调用<code class="fe kv kw kx ky b">checkpoint.read</code>。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ns nt l"/></div></figure><p id="65e5" class="pw-post-body-paragraph kz la iq lb b lc ld jr le lf lg ju lh li lj lk ll lm ln lo lp lq lr ls lt lu ij bi translated">一个时期后打印语句示例:</p><pre class="kg kh ki kj gt ni ky nj nk aw nl bi"><span id="167a" class="lw lx iq ky b gy nm nn l no np">Epoch 001/200<br/>Train: 100%|██████████████████████| 178/178 [12:29&lt;00:00,  4.21s/it]<br/>Test: 100%|█████████████████████████| 20/20 [00:24&lt;00:00,  1.22s/it]<br/>MAE(X, F(G(X))): 0.2599  MAE(X, F(X)): 0.2823<br/>MAE(Y, G(F(Y))): 0.2492  MAE(Y, G(Y)): 0.2793<br/>Elapse: 773.84s</span></pre><h2 id="1793" class="lw lx iq bd ly lz ma dn mb mc md dp me li mf mg mh lm mi mj mk lq ml mm mn mo bi translated">1个GPU对比3个GPU的结果</h2><p id="e449" class="pw-post-body-paragraph kz la iq lb b lc mp jr le lf mq ju lh li mr lk ll lm ms lo lp lq mt ls lt lu ij bi translated">为了验证我们可以在分布计算后获得类似的结果，我在1个GPU和3个GPU上用相同的超参数训练了2个模型。一般来说，我们可以用更大的批量来使用更大的学习速率，尽管为了简单起见，我将所有的学习速率都保持在<code class="fe kv kw kx ky b">2e-4</code>。平均而言，1个GPU完成一个纪元需要大约325秒，而3个GPU需要大约167秒。使用多个GPU时，分析输入管道和训练过程以确保适当的加速非常重要，分布式训练的开销可能会超过并行化的优势。</p><p id="1a29" class="pw-post-body-paragraph kz la iq lb b lc ld jr le lf lg ju lh li lj lk ll lm ln lo lp lq lr ls lt lu ij bi translated">下面的图和图像显示了200个时期后发生器和鉴别器的训练统计和结果。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nv"><img src="../Images/03dd58b23dea82e27a34785ce93dc2da.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*H4r4IDWs4S63q0H9Kvn6lA.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">上图:发生器和鉴别器的训练(实线)和测试(虚线)损失|下图:测试集上的循环和同一性平均绝对误差[图片由作者提供]</p></figure><div class="kg kh ki kj gt ab cb"><figure class="nw kk nx ny nz oa ob paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><img src="../Images/34ae1cfaf85cf20b3c9d2dfdce7ad11f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*uQ_O9a09LMnXaXYVb4tHUg.png"/></div></figure><figure class="nw kk oc ny nz oa ob paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><img src="../Images/ca8f81e5266b0dc00a48d7e745546b68.png" data-original-src="https://miro.medium.com/v2/resize:fit:1002/format:webp/1*g3f6y6JEHkhOea8K8dEQOg.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk od di oe of translated">x -&gt; G(x) -&gt; F(G(x))样本[图片由作者提供]</p></figure></div><div class="ab cb"><figure class="nw kk og ny nz oa ob paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><img src="../Images/3e960cee9cbc75760d9bf06986b6f7ab.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*jN8kbI4tOkvKczm4axLceA.png"/></div></figure><figure class="nw kk oh ny nz oa ob paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><img src="../Images/ff1f97099c4ee7873ab3c29a1e36d37c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1002/format:webp/1*d8hukQ_3mXoElSAWo80Bhw.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk od di oe of translated">y -&gt; F(y) -&gt; G(F(y))样本[图片由作者提供]</p></figure></div><p id="94ee" class="pw-post-body-paragraph kz la iq lb b lc ld jr le lf lg ju lh li lj lk ll lm ln lo lp lq lr ls lt lu ij bi translated">我们有了它，一个关于如何通过<code class="fe kv kw kx ky b">tf.distribute.MirroredStrategy</code>将多GPU训练与TensorFlow 2中的自定义训练循环集成的端到端示例。</p><p id="bad5" class="pw-post-body-paragraph kz la iq lb b lc ld jr le lf lg ju lh li lj lk ll lm ln lo lp lq lr ls lt lu ij bi translated">由于本教程的博客性质，一些编程风格相当糟糕(例如，大量使用全局变量，训练验证测试分割而不是训练测试分割，等等。).请查看<a class="ae lv" href="https://github.com/bryanlimy/tf2-cyclegan" rel="noopener ugc nofollow" target="_blank">github.com/bryanlimy/tf2-cyclegan</a>获取更完整的示例，包括正确的检查点保存和加载，以及TensorBoard集成。</p></div><div class="ab cl oi oj hu ok" role="separator"><span class="ol bw bk om on oo"/><span class="ol bw bk om on oo"/><span class="ol bw bk om on"/></div><div class="ij ik il im in"><h2 id="c21f" class="lw lx iq bd ly lz ma dn mb mc md dp me li mf mg mh lm mi mj mk lq ml mm mn mo bi translated">TensorFlow和Keras中有关分布式培训的其他参考资料</h2><ul class=""><li id="9dc1" class="mu mv iq lb b lc mp lf mq li op lm oq lq or lu nu na nb nc bi translated">TensorFlow - <a class="ae lv" href="https://www.tensorflow.org/guide/distributed_training" rel="noopener ugc nofollow" target="_blank">使用TensorFlow进行分布式训练</a></li><li id="7618" class="mu mv iq lb b lc nd lf ne li nf lm ng lq nh lu nu na nb nc bi translated">TensorFlow - <a class="ae lv" href="https://www.tensorflow.org/tutorials/distribute/keras" rel="noopener ugc nofollow" target="_blank">带Keras的分布式训练</a></li><li id="ec63" class="mu mv iq lb b lc nd lf ne li nf lm ng lq nh lu nu na nb nc bi translated">TensorFlow - <a class="ae lv" href="https://www.tensorflow.org/tutorials/distribute/custom_training" rel="noopener ugc nofollow" target="_blank">用tf.distribute.Strategy </a>定制训练</li><li id="6177" class="mu mv iq lb b lc nd lf ne li nf lm ng lq nh lu nu na nb nc bi translated">Keras - <a class="ae lv" href="https://keras.io/guides/distributed_training/" rel="noopener ugc nofollow" target="_blank">多GPU和分布式培训</a></li></ul></div><div class="ab cl oi oj hu ok" role="separator"><span class="ol bw bk om on oo"/><span class="ol bw bk om on oo"/><span class="ol bw bk om on"/></div><div class="ij ik il im in"><h2 id="36e7" class="lw lx iq bd ly lz ma dn mb mc md dp me li mf mg mh lm mi mj mk lq ml mm mn mo bi translated">参考</h2><p id="d6f6" class="pw-post-body-paragraph kz la iq lb b lc mp jr le lf mq ju lh li mr lk ll lm ms lo lp lq mt ls lt lu ij bi translated">[1]朱，严军，等.“利用循环一致对抗网络进行不成对的映象对映象翻译”IEEE计算机视觉国际会议论文集。2017.</p></div></div>    
</body>
</html>