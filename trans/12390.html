<html>
<head>
<title>Explain like I’m five: Backpropagation</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">像我五岁一样解释:反向传播</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/explain-like-im-five-backpropagation-38d1cec94ee5?source=collection_archive---------34-----------------------#2021-12-16">https://towardsdatascience.com/explain-like-im-five-backpropagation-38d1cec94ee5?source=collection_archive---------34-----------------------#2021-12-16</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><figure class="ip iq gp gr ir is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi io"><img src="../Images/ac25c85615574361f7b064a2063d7477.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8QpMpCoG48LOav6-o4MsTA.jpeg"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">尼古拉斯·卡佩罗在<a class="ae jd" href="https://unsplash.com/s/photos/graph?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片。</p></figure><div class=""/><div class=""><h2 id="3ec4" class="pw-subtitle-paragraph kd jf jg bd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku dk translated">对人工智能初学者主题的温和介绍</h2></div><p id="4a49" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我最近写了一篇关于<a class="ae jd" rel="noopener" target="_blank" href="/explain-like-im-five-how-an-artificial-neuronal-network-learns-8ea4715b6f4c">人工神经元网络如何学习</a>的短文。虽然这篇文章为您提供了主要概念的简要概述，但它并没有向您解释学习实际上是如何工作的。本文将尝试回答一些未解决的问题，并通过简单解释人工神经网络监督学习最常用的方法，带您深入兔子洞:<strong class="kx jh">“反向传播”</strong>。</p><h1 id="3d5a" class="lr ls jg bd lt lu lv lw lx ly lz ma mb km mc kn md kp me kq mf ks mg kt mh mi bi translated">什么是反向传播？</h1><p id="2103" class="pw-post-body-paragraph kv kw jg kx b ky mj kh la lb mk kk ld le ml lg lh li mm lk ll lm mn lo lp lq ij bi translated">反向传播是一种算法，它使人工神经网络根据它产生的预测误差进行学习。该算法可以总结为三个步骤:</p><ol class=""><li id="454b" class="mo mp jg kx b ky kz lb lc le mq li mr lm ms lq mt mu mv mw bi translated">输入数据通过网络向前传递，人工神经网络根据其初始权重进行<strong class="kx jh">预测</strong>。</li><li id="a037" class="mo mp jg kx b ky mx lb my le mz li na lm nb lq mt mu mv mw bi translated">然后将网络的输出与期望的结果(也称为目标值)进行比较。两者之间的差异被认为是网络的<strong class="kx jh">误差</strong>。</li><li id="382e" class="mo mp jg kx b ky mx lb my le mz li na lm nb lq mt mu mv mw bi translated">然后误差通过网络反向传递，神经元连接的权重根据它们对误差的影响进行<strong class="kx jh">调整</strong>。</li></ol><p id="a4f1" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">通过这种方式，反向传播算法保证了每次学习的输出都接近期望的输出。</p><h1 id="4be1" class="lr ls jg bd lt lu lv lw lx ly lz ma mb km mc kn md kp me kq mf ks mg kt mh mi bi translated">我们如何测量人工神经网络误差？</h1><p id="9c11" class="pw-post-body-paragraph kv kw jg kx b ky mj kh la lb mk kk ld le ml lg lh li mm lk ll lm mn lo lp lq ij bi translated">反向传播解决了人工神经网络的所有错误。该误差由所谓的“<strong class="kx jh">损失函数</strong>计算，该函数有时也称为“成本函数”。损失函数将实际值与神经网络计算的值进行比较，并计算它们的差值。这种差异就表示为网络损耗。根据网络要解决的问题，必须使用不同的损失函数。</p><p id="94b6" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx jh">二元分类</strong>问题是建模问题，其中示例被分配给两个标签中的一个(即是或否)。应用于二元分类问题的流行损失函数是二元交叉熵损失、铰链损失和平方铰链损失函数。</p><p id="70b9" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx jh">多类分类</strong>问题是指将样本分配给两个以上标签中的一个(即动物的图像识别)。这类问题常用的损失函数有多类交叉熵损失、稀疏多类交叉熵损失和 Kullback Leibler 散度损失函数。</p><p id="dd6e" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx jh">对于回归问题</strong>(即时间序列预测)最常用的损失函数有均方误差、均方对数误差、平均绝对误差和平均绝对百分比误差。</p><p id="324f" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">如果你想更深入地研究其中的任何一个，我强烈推荐你阅读杰森·布朗利关于“<a class="ae jd" href="https://machinelearningmastery.com/how-to-choose-loss-functions-when-training-deep-learning-neural-networks/" rel="noopener ugc nofollow" target="_blank"> <em class="nc">在训练深度学习神经网络</em> </a>时如何选择损失函数”的博客。虽然博客不仅给你更详细的解释，作者还包括实际的代码片段，我总是觉得非常有帮助。</p><h1 id="534c" class="lr ls jg bd lt lu lv lw lx ly lz ma mb km mc kn md kp me kq mf ks mg kt mh mi bi translated">如何根据误差调整权重？</h1><p id="9f74" class="pw-post-body-paragraph kv kw jg kx b ky mj kh la lb mk kk ld le ml lg lh li mm lk ll lm mn lo lp lq ij bi translated">反向传播算法本身基于所谓的“<strong class="kx jh">梯度下降</strong>技术，这是一种旨在优化特定函数的算法。在人工神经网络的情况下，梯度下降技术用于调整人工神经网络的权重以最小化损失函数。</p><p id="9f3a" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">为了最小化损失函数，该算法测量函数的梯度并在相反方向更新权重(w <em class="nc"> ij) </em>，使得网络的损失移动到函数的平坦区域。重复该过程，直到算法通过找到损失函数的<strong class="kx jh">局部最小值</strong>而实现最低误差(<em class="nc"> E </em>)。</p><figure class="ne nf ng nh gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nd"><img src="../Images/9c103c9316a28248e2f2a2ccb0933e9b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0qkoztCO57j22HPy3QqbQQ.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">权重以与渐变相反的方向更新。图片作者。</p></figure><p id="8fb9" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">然而，大多数人工神经网络是多维的，这意味着它们可以有多个局部最小值。你可以想象这一点，想象一下下面的三维图中展示的风景。</p><p id="f677" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">红色的山顶是我们网络误差最高的起点。图形边缘的紫色区域分别代表该模型的最佳局部最小值。因此，例如，如果梯度下降，算法在损失景观最陡的山的后面向下移动，它将迅速找到一个最优的局部最小值。我们可以得出结论，无论算法朝哪个方向移动，都会找到一个最优设置——对吗？</p><figure class="ne nf ng nh gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi ni"><img src="../Images/d171595ebb2f66f7c8bd023edf74d8b5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*2DwY_hXhQWwd4dwX.jpg"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">来自 wikimedia.org<a class="ae jd" href="https://commons.wikimedia.org/wiki/File:Contour3D.jpg" rel="noopener ugc nofollow" target="_blank">的山农鲍林的三维等高线图</a></p></figure><p id="059b" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">不幸的是，它没有那么防弹。让我们假设算法选择沿着山的前面走，向图的中间移动，发现自己在一个山谷中。因此，误差将开始变平，表明算法已经找到局部最小值。由于误差将再次开始增加，当算法继续沿着山谷的山坡向上走时，看起来找到的局部最小值是最优的。然而，实际上，该算法只找到了(潜在的)<strong class="kx jh">许多不代表最优的局部最小值</strong>中的一个。</p><p id="9dee" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">你们中的一些人现在会问:为什么不简单地告诉梯度下降，走哪条路？这是一个有效的问题，如果可能的话，这将解决安的许多问题。然而，根据所选择的损失函数、网络的规模和复杂性，我们可能<strong class="kx jh">不可能</strong>提前模拟人工神经网络的损失维度。</p><p id="0de8" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">如您所见，使用梯度下降算法可能会有一些陷阱。根据手头的问题和您选择的设置，您可以通过调整反向传播算法本身来尝试减轻一些问题。如果你有兴趣了解更多，我可以向你推荐这篇<a class="ae jd" rel="noopener" target="_blank" href="/gradient-descent-algorithm-and-its-variants-10f652806a3">文章</a>。</p><h1 id="6b51" class="lr ls jg bd lt lu lv lw lx ly lz ma mb km mc kn md kp me kq mf ks mg kt mh mi bi translated">结论</h1><p id="647b" class="pw-post-body-paragraph kv kw jg kx b ky mj kh la lb mk kk ld le ml lg lh li mm lk ll lm mn lo lp lq ij bi translated">简而言之，反向传播是一种技术，可用于根据人工神经元网络产生的误差，通过相应地调整网络的权重来训练人工神经元网络。</p><p id="51f6" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">误差就是人工神经网络的预测值和给定的目标值之间的差异。误差的计算取决于用于反向传播的损失函数。损失函数的类型可以变化，这取决于人工神经网络要解决的问题。</p><p id="947a" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在反向传播期间，使用所谓的梯度下降技术来调整权重，梯度下降技术是一种优化损失函数的算法。为此，该算法在相反方向上更新权重<em class="nc"> </em>，以便网络的损耗随着每次学习而减少。重复该过程，直到找到局部最小值，这表示网络损耗被最小化到最优。然而，取决于网络的复杂性以及手头的问题，由于可能存在多个局部最小值，找到最优局部最小值可能是棘手的。</p><h1 id="4b50" class="lr ls jg bd lt lu lv lw lx ly lz ma mb km mc kn md kp me kq mf ks mg kt mh mi bi translated">来源</h1><ol class=""><li id="6bf6" class="mo mp jg kx b ky mj lb mk le nj li nk lm nl lq mt mu mv mw bi translated">梁和海金。1991.复杂的反向传播算法，2101–2104。新泽西:电气和电子工程师协会</li><li id="44b4" class="mo mp jg kx b ky mx lb my le mz li na lm nb lq mt mu mv mw bi translated">布朗利。2019.“训练深度学习神经网络时如何选择损失函数”。最后更改 2019 年 1 月 30 日。<a class="ae jd" href="https://machinelearningmastery.com/how-to-choose-loss-functions-when-training-deep-learning-neural-networks/" rel="noopener ugc nofollow" target="_blank">https://machine learning mastery . com/how-to-choose-loss-functions-when-training-deep-learning-neural-networks/</a></li><li id="81b0" class="mo mp jg kx b ky mx lb my le mz li na lm nb lq mt mu mv mw bi translated">金内布洛克。1994.Neuronale Netze: Grundlagen，Anwendungen，Beispiele，19–162 页。奥尔登堡:德·格鲁埃特。</li></ol></div></div>    
</body>
</html>