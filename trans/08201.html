<html>
<head>
<title>Mining Opinions to Understand Customer Trends: Part 1 of 2</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">挖掘意见以了解客户趋势:第1部分，共2部分</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/mining-opinions-to-understand-customer-trends-part-1-of-2-e531793228b7?source=collection_archive---------45-----------------------#2021-07-27">https://towardsdatascience.com/mining-opinions-to-understand-customer-trends-part-1-of-2-e531793228b7?source=collection_archive---------45-----------------------#2021-07-27</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="70b5" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">用PyTorch微调预训练变压器</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/9a82c57728e9d588f50cb8abf454229e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*DJ5SgrSzdwJvA_Qp"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">ahin Sezer diner在<a class="ae kv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="1f48" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">随着互联网上可用信息的丰富，挖掘意见以了解客户对你的品牌、产品和服务的感受已经成为一个重要的成功指标。公司在投资人工智能以从客户头脑中获得有价值的见解时不会退缩。</p><blockquote class="ls"><p id="bfbc" class="lt lu iq bd lv lw lx ly lz ma mb lr dk translated">“你最不满意的顾客是你最大的学习来源”~比尔·盖茨</p></blockquote><p id="195d" class="pw-post-body-paragraph kw kx iq ky b kz mc jr lb lc md ju le lf me lh li lj mf ll lm ln mg lp lq lr ij bi translated">为了挖掘客户的意见，公司经常利用自然语言处理(NLP)的进步，在这篇博客中，我想探索一些可以用来解决手头问题的NLP的关键概念。</p><h2 id="38c7" class="mh mi iq bd mj mk ml dn mm mn mo dp mp lf mq mr ms lj mt mu mv ln mw mx my mz bi translated">目标</h2><p id="900e" class="pw-post-body-paragraph kw kx iq ky b kz na jr lb lc nb ju le lf nc lh li lj nd ll lm ln ne lp lq lr ij bi translated">在这个由2部分组成的系列中，我使用2017年改变NLP面貌的模型Transformers，使用亚马逊评论分析不同产品领域的客户情绪。除了Transformers，我还探索了并行计算、迁移学习和交互式仪表板的概念。</p><blockquote class="nf ng nh"><p id="04ad" class="kw kx ni ky b kz la jr lb lc ld ju le nj lg lh li nk lk ll lm nl lo lp lq lr ij bi translated">本系列的第1部分着重于微调预先训练好的变压器，第2部分详细介绍了我在Jupyter笔记本上创建交互式仪表盘的实验。</p></blockquote></div><div class="ab cl nm nn hu no" role="separator"><span class="np bw bk nq nr ns"/><span class="np bw bk nq nr ns"/><span class="np bw bk nq nr"/></div><div class="ij ik il im in"><h1 id="b863" class="nt mi iq bd mj nu nv nw mm nx ny nz mp jw oa jx ms jz ob ka mv kc oc kd my od bi translated">1.在我们编码之前…</h1><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oe"><img src="../Images/339ddfd0f9a219ab8f8351c42dcc624f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XdXs4MuJFOYuLIkKHb9mbQ.png"/></div></div></figure><h2 id="728d" class="mh mi iq bd mj mk ml dn mm mn mo dp mp lf mq mr ms lj mt mu mv ln mw mx my mz bi translated">1a。变压器-ing NLP</h2><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi of"><img src="../Images/c67123e515ee5be3468f08df6dc8e718.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*cEyTHiTYiVDg0Dy6"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><a class="ae kv" href="https://unsplash.com/@samule?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Samule孙</a>在<a class="ae kv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍照</p></figure><blockquote class="ls"><p id="4081" class="lt lu iq bd lv lw og oh oi oj ok lr dk translated">Transformer是一种NLP编码器-解码器架构，它使用多头自关注机制来并行处理输入序列。</p></blockquote><p id="ac40" class="pw-post-body-paragraph kw kx iq ky b kz mc jr lb lc md ju le lf me lh li lj mf ll lm ln mg lp lq lr ij bi translated">让我们分解上面的句子，并为每个部分开发一个直觉:</p><ul class=""><li id="a9f6" class="ol om iq ky b kz la lc ld lf on lj oo ln op lr oq or os ot bi translated"><strong class="ky ir">“NLP”:自然语言处理或NLP </strong>是机器学习中的一个领域，它帮助机器从人类语言中获取意义。NLP的应用范围从理解语言(如总结文本、社交媒体监控)到生成语言(如为图片创建字幕)，有时两者同时进行(如语言翻译、聊天机器人)。</li><li id="6015" class="ol om iq ky b kz ou lc ov lf ow lj ox ln oy lr oq or os ot bi translated">"<strong class="ky ir">编码器-解码器架构</strong>":计算机/机器无法理解文字，因此我们以数字的形式给它输入语言。<strong class="ky ir">编码</strong>意味着将数据转换成编码信息(在我们的例子中是数字向量，也就是隐藏状态)，而<strong class="ky ir">解码</strong>意味着将编码信息转换成可理解的语言。解码器的输出将取决于模型的目标，例如，在将英语翻译成印地语时，解码器会将编码的消息转换成印地语。</li><li id="2ab7" class="ol om iq ky b kz ou lc ov lf ow lj ox ln oy lr oq or os ot bi translated">"<strong class="ky ir">自我关注机制</strong>":简单的编码器-解码器架构的缺点是，由于<a class="ae kv" rel="noopener" target="_blank" href="/the-vanishing-gradient-problem-69bf08b15484">消失梯度问题</a>，它不能熟练地管理长句子序列。为了解决这个问题，引入了<strong class="ky ir">注意机制</strong>。目的是允许NLP模型在解码每个时间步长时更多地关注输入序列的相关部分。有<a class="ae kv" rel="noopener" target="_blank" href="/attention-and-its-different-forms-7fc3674d14dc">种不同类型的注意力</a>，变形金刚使用的那种叫做<a class="ae kv" rel="noopener" target="_blank" href="/illustrated-self-attention-2d627e33b20a">自我注意力</a>。它通过利用上下文向量来捕捉句子中单词之间的上下文关系。</li></ul><blockquote class="nf ng nh"><p id="6ae5" class="kw kx ni ky b kz la jr lb lc ld ju le nj lg lh li nk lk ll lm nl lo lp lq lr ij bi translated">注意是理解变压器和NLP最重要的概念之一，在这里阅读更多关于它的内容！</p></blockquote><ul class=""><li id="b255" class="ol om iq ky b kz la lc ld lf on lj oo ln op lr oq or os ot bi translated">"<strong class="ky ir">多头注意力</strong>":自我注意力过程并行重复多次，每次都称为一个头，因此得名"多头"注意力。多头注意力允许嵌入学习每个单词意思的不同方面。一种思考方式是，一座建筑可以用它的高度、宽度、颜色、位置等来描述。而拥有不同的描述，让最终的画面更加丰富。</li><li id="1de3" class="ol om iq ky b kz ou lc ov lf ow lj ox ln oy lr oq or os ot bi translated">"<strong class="ky ir">并行处理输入序列</strong>":变形金刚的前身按顺序(一个单词接一个单词，从左到右或从右到左)编码输入，不允许模型利用GPU并行计算的魔力。假设Transformer架构为每个时间步长创建上下文向量，则单词相互独立，可以并行处理。该属性还允许模型中的双向性，即每个单词预测都考虑当前单词两侧的上下文。(图一)</li></ul><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oz"><img src="../Images/993be8152d734aded0daa4404e4ecbdb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1218/format:webp/1*FiBKAU6_Zd1Wioci_Z1khQ.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图1:变形金刚在编码过程中从两个方向考虑上下文(图片由作者提供)</p></figure><blockquote class="nf ng nh"><p id="4e01" class="kw kx ni ky b kz la jr lb lc ld ju le nj lg lh li nk lk ll lm nl lo lp lq lr ij bi translated">变形金刚在NLP中是一股强大的力量，有大量文章解释了<a class="ae kv" href="https://arxiv.org/pdf/1706.03762.pdf" rel="noopener ugc nofollow" target="_blank">原始论文</a>。上面的简短描述并没有很好地体现建筑的复杂之美——我鼓励你多读一些关于它的内容。这里有一篇这样有用的<a class="ae kv" rel="noopener" target="_blank" href="/transformers-89034557de14">文章</a>。</p></blockquote><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi pa"><img src="../Images/c4442baa24204f78faae19010660a52e.png" data-original-src="https://miro.medium.com/v2/resize:fit:960/format:webp/1*LTds9cZnUbCo26ixI7wyuw.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图2:变压器架构(<a class="ae kv" href="https://arxiv.org/pdf/1706.03762.pdf" rel="noopener ugc nofollow" target="_blank">来源</a>)</p></figure><h2 id="6899" class="mh mi iq bd mj mk ml dn mm mn mo dp mp lf mq mr ms lj mt mu mv ln mw mx my mz bi translated">1b。将学习转化为救援</h2><p id="b919" class="pw-post-body-paragraph kw kx iq ky b kz na jr lb lc nb ju le lf nc lh li lj nd ll lm ln ne lp lq lr ij bi translated">当我们开始向机器学习模型教授人类语言时，我们理解得相当简单的人类语言似乎具有无限的复杂性。找到捕捉所有细微差别的数据集来制作高级NLP模型是一项艰巨的任务，一旦我们找到了必要的数据量，模型的实际训练就变成了一项计算开销很大的任务。为了解决这个问题，我们使用迁移学习。</p><blockquote class="nf ng nh"><p id="6e7f" class="kw kx ni ky b kz la jr lb lc ld ju le nj lg lh li nk lk ll lm nl lo lp lq lr ij bi translated">在<!-- -->迁移学习中，我们重用一个预先训练的模型作为另一个模型的起点。潜在的想法是，在第一个任务中学到的特征足够普遍，可以重新用于第二个任务。</p></blockquote><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi pb"><img src="../Images/4ccb2b9aedee77bce2d739fed741fbad.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*I-21t26L9Fxa_Hsv704vCg.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图3:迁移学习的图解(图片由作者提供)</p></figure><p id="4073" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">2018年，谷歌彻底改变了NLP格局(再次！)通过从变压器释放双向编码器表示(<a class="ae kv" href="https://arxiv.org/abs/1810.04805" rel="noopener ugc nofollow" target="_blank"> BERT </a>)。BERT本质上是“变形金刚”的“编码器”部分，它是在维基百科语料库上训练出来的(意思是很多很多的数据！).</p><blockquote class="ls"><p id="5106" class="lt lu iq bd lv lw lx ly lz ma mb lr dk translated">来自BERT的迁移学习显示了2018年对11个不同下游任务的异常预测。</p></blockquote><p id="2e36" class="pw-post-body-paragraph kw kx iq ky b kz mc jr lb lc md ju le lf me lh li lj mf ll lm ln mg lp lq lr ij bi translated">BERT的目的是制造一个健壮的编码器，它可以嵌入单词，同时从两个方向考虑句子的上下文。(当前单词前后的单词)唉，它就是这么做的！使用来自BERT的预训练表示显著地增加了NLP模型预测的准确性，并且甚至更显著地减少了达到该准确性所需的计算开销。</p><blockquote class="nf ng nh"><p id="f240" class="kw kx ni ky b kz la jr lb lc ld ju le nj lg lh li nk lk ll lm nl lo lp lq lr ij bi translated">经过预先训练的语言模型已经成为自然语言处理研究中的一项重要内容，在过去的三年中取得了许多突破。<a class="ae kv" href="https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf" rel="noopener ugc nofollow" target="_blank"> GPT-2 </a>、<a class="ae kv" href="https://arxiv.org/pdf/2005.14165.pdf" rel="noopener ugc nofollow" target="_blank"> GPT-3 </a>、<a class="ae kv" href="https://arxiv.org/pdf/1910.10683.pdf" rel="noopener ugc nofollow" target="_blank"> T5 </a>、<a class="ae kv" href="https://arxiv.org/pdf/1907.11692.pdf" rel="noopener ugc nofollow" target="_blank">罗伯塔</a>和<a class="ae kv" href="https://arxiv.org/abs/1906.08237" rel="noopener ugc nofollow" target="_blank"> XLNet </a>是其他一些流行的语言模型。</p></blockquote><h2 id="2166" class="mh mi iq bd mj mk ml dn mm mn mo dp mp lf mq mr ms lj mt mu mv ln mw mx my mz bi translated">1c。并行计算的力量</h2><p id="6b33" class="pw-post-body-paragraph kw kx iq ky b kz na jr lb lc nb ju le lf nc lh li lj nd ll lm ln ne lp lq lr ij bi translated">随着数据可用性的扩展和体系结构的复杂性，给你的神经网络增加新的层次对你的CPU来说是一项艰巨的任务。让“工作”变得更容易的一个简单方法是利用多个CPU内核。这有助于CPU并行处理进程，减少周转时间。</p><p id="0611" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">加快处理速度的更好方法是使用<a class="ae kv" rel="noopener" target="_blank" href="/what-is-a-gpu-and-do-you-need-one-in-deep-learning-718b9597aa0d">GPU</a>。CPU并行计算线程的能力受限于内核的数量，然而，GPU可以同时处理多个计算。事实证明，GPU对于ML模型非常有价值，尤其是对于深度学习。</p><p id="630c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">GPU可能不适合所有人，但不用担心，ML社区会支持你！网上有多种选择可以使用免费的GPU！在这篇博客中，我将利用<a class="ae kv" rel="noopener" target="_blank" href="/getting-started-with-google-colab-f2fff97f594c"> Google Colab </a>的<strong class="ky ir"> NVIDIA </strong> GPU。</p><blockquote class="nf ng nh"><p id="7200" class="kw kx ni ky b kz la jr lb lc ld ju le nj lg lh li nk lk ll lm nl lo lp lq lr ij bi translated">对于NVIDIA GPUs，我们可以使用<a class="ae kv" href="https://blogs.nvidia.com/blog/2012/09/10/what-is-cuda-2/" rel="noopener ugc nofollow" target="_blank"> CUDA </a>来创建一个简单的解决方案，以允许在我们的模型中进行并行处理</p></blockquote></div><div class="ab cl nm nn hu no" role="separator"><span class="np bw bk nq nr ns"/><span class="np bw bk nq nr ns"/><span class="np bw bk nq nr"/></div><div class="ij ik il im in"><h1 id="402f" class="nt mi iq bd mj nu nv nw mm nx ny nz mp jw oa jx ms jz ob ka mv kc oc kd my od bi translated">2.现在，让我们编码</h1><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oe"><img src="../Images/515fd6ec224dcc79d760cab5906e72cc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8BSnlIRSklyYfDXoH9g8GA.png"/></div></div></figure><p id="5e21" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">上面讨论的概念可以用来创建一个ML模型来挖掘意见和理解当前的客户情绪。我将这个过程分为4个步骤:</p><ul class=""><li id="1add" class="ol om iq ky b kz la lc ld lf on lj oo ln op lr oq or os ot bi translated">数据准备</li><li id="5987" class="ol om iq ky b kz ou lc ov lf ow lj ox ln oy lr oq or os ot bi translated">迁移学习</li><li id="3fe9" class="ol om iq ky b kz ou lc ov lf ow lj ox ln oy lr oq or os ot bi translated">微调和培训</li><li id="976b" class="ol om iq ky b kz ou lc ov lf ow lj ox ln oy lr oq or os ot bi translated">预测！</li></ul><p id="9d25" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我将使用来自<a class="ae kv" href="https://www.cs.jhu.edu/~mdredze/datasets/sentiment/" rel="noopener ugc nofollow" target="_blank">多领域情感数据集(版本2.0) </a>的“未处理的”tar文件。该数据集包含过去几十年中多个类别的亚马逊产品评论。</p><h2 id="f058" class="mh mi iq bd mj mk ml dn mm mn mo dp mp lf mq mr ms lj mt mu mv ln mw mx my mz bi translated">2a。数据准备</h2><p id="ff7e" class="pw-post-body-paragraph kw kx iq ky b kz na jr lb lc nb ju le lf nc lh li lj nd ll lm ln ne lp lq lr ij bi translated">数据以XML格式呈现，首先需要转换成Pandas数据框架。我们可以用美味的汤来做到这一点。</p><blockquote class="nf ng nh"><p id="e890" class="kw kx ni ky b kz la jr lb lc ld ju le nj lg lh li nk lk ll lm nl lo lp lq lr ij bi translated">“Beautiful Soup是一个Python库，用于从HTML和XML文件中提取数据。它有助于导航、搜索和修改解析树”</p></blockquote><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="pc pd l"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">代码块1 :使用Beautiful Soup将XML数据转换成Pandas DataFrame</p></figure><p id="f24f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们现在将使用BERT词汇表将文本转换成标记。<a class="ae kv" href="https://github.com/huggingface/pytorch-pretrained-BERT" rel="noopener ugc nofollow" target="_blank"> PyTorch-Pretrained-BERT </a>库为所有的BERT模型提供了令牌。点击阅读更多关于他们<a class="ae kv" rel="noopener" target="_blank" href="/bert-to-the-rescue-17671379687f">的信息。</a></p><p id="0612" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">每个模型都有一个“最大序列”限制，我们需要将我们的令牌截断到伯特的限制，即512。我们还将添加一个标记来区分句子(“CLS”)和填充(“填充”)，以确保所有句子都具有相同的长度。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="pc pd l"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><strong class="ak">代码块2 </strong>:创建一个处理器来标记数据集</p></figure><p id="0a41" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们现在将创建一个<a class="ae kv" rel="noopener" target="_blank" href="/how-to-use-datasets-and-dataloader-in-pytorch-for-custom-text-data-270eed7f7c00">数据加载器</a>来遍历数据。数据加载器支持数据的自动批处理，在我们希望并行处理数据操作时非常有用。在将文本放入数据加载器时，我们对数据集的每一行进行标记，并用相应的标签加载它。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="pc pd l"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><strong class="ak">代码块3 </strong>:创建一个带有文本标记和相应标签的数据加载器</p></figure><h2 id="2845" class="mh mi iq bd mj mk ml dn mm mn mo dp mp lf mq mr ms lj mt mu mv ln mw mx my mz bi translated">2b。迁移学习</h2><p id="b539" class="pw-post-body-paragraph kw kx iq ky b kz na jr lb lc nb ju le lf nc lh li lj nd ll lm ln ne lp lq lr ij bi translated">对于迁移学习，我采用了来自<a class="ae kv" href="https://docs.google.com/presentation/d/1fIhGikFPnb7G5kr58OvYC3GN4io7MznnM0aAgadvJfc/edit#slide=id.g5a8fba10ed_1_204" rel="noopener ugc nofollow" target="_blank">NAACL 2019迁移学习</a>教程的预训练模型。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="pc pd l"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><strong class="ak">代码块4 </strong>:变压器型号</p></figure><p id="cf8e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在模型的顶部，我们将为业务问题添加一个分类器头。在我们的例子中，我们希望预测“积极”或“消极”的客户反应——因此，我们的模型将有两个输出类。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="pc pd l"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><strong class="ak">代码块</strong> <strong class="ak"> 5 </strong>:模型分类头</p></figure><h2 id="58f0" class="mh mi iq bd mj mk ml dn mm mn mo dp mp lf mq mr ms lj mt mu mv ln mw mx my mz bi translated">2c。模型微调和培训</h2><p id="f576" class="pw-post-body-paragraph kw kx iq ky b kz na jr lb lc nb ju le lf nc lh li lj nd ll lm ln ne lp lq lr ij bi translated">我们现在需要创建一个函数来训练数据，并创建一个评估函数来评估每个时期的验证数据。我们还需要为模型优化定义一个配置。定义一个单独的字典来微调模型允许我们在迭代超参数时进行简单的修改。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="pc pd l"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><strong class="ak">代码块</strong>T22】6:定义列车功能</p></figure><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="pc pd l"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><strong class="ak">代码块7 </strong>:定义评估函数</p></figure><p id="fc68" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">PyTorch的Ignite 库只需要几行代码就可以非常方便地进行培训和评估。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="pc pd l"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><strong class="ak">代码块8 </strong>:训练模型</p></figure><p id="da65" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">可以使用定义的赋值器来检查模型的准确性。预训练模型使我们能够通过几千个例子达到92%的准确率。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi pe"><img src="../Images/17dae57a9ef8ab9382b4e0c37f202a77.png" data-original-src="https://miro.medium.com/v2/resize:fit:1372/format:webp/1*v__gQk8wKv1M4KwF-ZHwVQ.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图4:测试集上的模型准确性</p></figure><h2 id="a11c" class="mh mi iq bd mj mk ml dn mm mn mo dp mp lf mq mr ms lj mt mu mv ln mw mx my mz bi translated">2d。预测！</h2><p id="102b" class="pw-post-body-paragraph kw kx iq ky b kz na jr lb lc nb ju le lf nc lh li lj nd ll lm ln ne lp lq lr ij bi translated">为了预测样本，我们将首先像代码块2一样标记数据，然后将其输入到模型中。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="pc pd l"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><strong class="ak">代码块9: </strong>定义预测功能</p></figure><p id="73d2" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">上述函数可直接用于输出单个句子的肯定和否定类别。在这个博客的第2部分，我们将使用这个函数进行批量预测。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi pf"><img src="../Images/7a4ed69f008fca1d6206c2c0f5721480.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ji4-SeFL_Q7UMqf8TzXJnA.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图5 <strong class="bd pg"> : </strong>单输入的模型预测</p></figure></div><div class="ab cl nm nn hu no" role="separator"><span class="np bw bk nq nr ns"/><span class="np bw bk nq nr ns"/><span class="np bw bk nq nr"/></div><div class="ij ik il im in"><h1 id="9790" class="nt mi iq bd mj nu nv nw mm nx ny nz mp jw oa jx ms jz ob ka mv kc oc kd my od bi translated">结论</h1><p id="7d7e" class="pw-post-body-paragraph kw kx iq ky b kz na jr lb lc nb ju le lf nc lh li lj nd ll lm ln ne lp lq lr ij bi translated">概括地说，我们学习了如何使用漂亮的Soup读取XML文件，使用PyTorch数据加载器创建迭代器，使用BERT标记文本，并在预训练的模型上使用迁移学习来创建情感分析的基础。一个分类器也被添加到基本模型中，使其能够预测积极和消极的情绪。</p><p id="e41c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">查看“<a class="ae kv" href="https://arxiv.org/pdf/1706.03762.pdf" rel="noopener ugc nofollow" target="_blank">注意力是你所需要的全部</a>”这篇文章，阅读更多关于变形金刚的内容。请参考<a class="ae kv" href="https://github.com/VidushiBhatia/Mining-Opinions-using-Transformers-PyTorch/blob/main/Mining_Opinions_to_Predict_Customer_Trends_using_Transformers_PyTorch.ipynb" rel="noopener ugc nofollow" target="_blank">源代码</a>了解更多细节，如果您有任何问题/建议，请随时联系我！</p><blockquote class="ls"><p id="5d9c" class="lt lu iq bd lv lw lx ly lz ma mb lr dk translated">在这篇博客的第二部分，我将创建一个交互式仪表板，它将根据变压器模型的预测绘制实时趋势！</p></blockquote></div></div>    
</body>
</html>