<html>
<head>
<title>Scaling Flower with Multiprocessing</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">多重处理缩放花</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/scaling-flower-with-multiprocessing-a0bc7b7aace0?source=collection_archive---------17-----------------------#2021-06-21">https://towardsdatascience.com/scaling-flower-with-multiprocessing-a0bc7b7aace0?source=collection_archive---------17-----------------------#2021-06-21</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="9da5" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsdatascience.com/tagged/making-sense-of-big-data" rel="noopener" target="_blank">理解大数据</a></h2><div class=""/><div class=""><h2 id="7a55" class="pw-subtitle-paragraph jw iz iq bd b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn dk translated">了解如何使用Flower框架和PyTorch的多处理在本地扩展您的联邦学习实验。</h2></div><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ko"><img src="../Images/2b4e52af35037e61128a28172d817e3d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*10wrVU4gGfFWyBVO"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">由<a class="ae le" href="https://unsplash.com/@lensinkmitchel?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">米切尔·林辛克</a>在<a class="ae le" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><h1 id="a341" class="lf lg iq bd lh li lj lk ll lm ln lo lp kf lq kg lr ki ls kj lt kl lu km lv lw bi translated">联合学习简介:</h1><p id="47df" class="pw-post-body-paragraph lx ly iq lz b ma mb ka mc md me kd mf mg mh mi mj mk ml mm mn mo mp mq mr ms ij bi translated">随着技术的发展，最近的技术产生越来越多的数据，大量收集这些数据以训练准确的模型变得越来越容易获得。然而，这引起了隐私问题，为了确保对他们的保护，人们目前根据他们的居住地受到许多法律的保护(例如欧洲的<a class="ae le" href="https://gdpr-info.eu/" rel="noopener ugc nofollow" target="_blank"> GDPR </a>)。当涉及个人数据时，不能盲目应用传统的机器学习方法，即在单个点积累数据来训练模型。</p><p id="639a" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated">为了解决这个问题，谷歌在2016年发布了一种新的训练模型的范式，称为联邦学习，并将其应用于其谷歌键盘应用[ <a class="ae le" href="http://arxiv.org/abs/1610.05492" rel="noopener ugc nofollow" target="_blank"> 1a </a> ] [ <a class="ae le" href="http://arxiv.org/abs/1602.05629" rel="noopener ugc nofollow" target="_blank"> 1b </a> ]。它的引入是为了利用他们用来训练模型的公开可用数据集和用户将产生的私有数据之间的域差异问题。</p><p id="70d2" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated">正如联合学习手册[ <a class="ae le" href="https://www.morganclaypool.com/doi/abs/10.2200/S00960ED2V01Y201910AIM043" rel="noopener ugc nofollow" target="_blank"> 2 </a> ]中所指出的，为了让这种范式发挥作用，它需要遵守4个主要原则，即:</p><blockquote class="my mz na"><p id="f6a2" class="lx ly nb lz b ma mt ka mc md mu kd mf nc mv mi mj nd mw mm mn ne mx mq mr ms ij bi translated"><em class="iq"/>至少有两个实体想要训练一个模型，拥有自己的数据并准备使用。</p><p id="41d4" class="lx ly nb lz b ma mt ka mc md mu kd mf nc mv mi mj nd mw mm mn ne mx mq mr ms ij bi translated">在培训期间，数据不会离开其原始所有者。</p><p id="ea28" class="lx ly nb lz b ma mt ka mc md mu kd mf nc mv mi mj nd mw mm mn ne mx mq mr ms ij bi translated">模型可以通过受保护的方式从一个实体转移到另一个实体。</p><p id="1621" class="lx ly nb lz b ma mt ka mc md mu kd mf nc mv mi mj nd mw mm mn ne mx mq mr ms ij bi translated">得到的模型性能是用单一实体拥有的所有数据训练的理想模型的良好近似。</p></blockquote><p id="95db" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated">最后一点也是在告诉我们，联合学习并不能一直适用。它最大的缺点是，至少在目前，联合学习对来自内部的攻击很敏感[ <a class="ae le" href="http://arxiv.org/abs/1811.12470" rel="noopener ugc nofollow" target="_blank"> 3 </a> ]，不能保证收敛[ <a class="ae le" href="http://arxiv.org/abs/1810.07766" rel="noopener ugc nofollow" target="_blank"> 4 </a> ]，并且需要足够多的客户端来实现它的结果[ <a class="ae le" href="http://arxiv.org/abs/1902.01046" rel="noopener ugc nofollow" target="_blank"> 5 </a>。然而，当正确应用时，它可以产生通过常规手段无法获得的模型，如Google和他们的Google Keyboard。</p><p id="f403" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated">到目前为止，只有少数框架可以实现它，因为这是一个相当新的概念。TensorFlow开发了自己的版本，名为<a class="ae le" href="https://www.tensorflow.org/federated" rel="noopener ugc nofollow" target="_blank"> TensorFlow Federated </a>。PyTorch还没有自己的实现，但是它们确实存在兼容的框架，比如OpenMined开发的<a class="ae le" href="https://github.com/OpenMined/PySyft" rel="noopener ugc nofollow" target="_blank"> PySyft </a>和<a class="ae le" href="https://flower.dev/" rel="noopener ugc nofollow" target="_blank"> Flower </a>，这将是本文的重点。</p><h1 id="d180" class="lf lg iq bd lh li lj lk ll lm ln lo lp kf lq kg lr ki ls kj lt kl lu km lv lw bi translated"><strong class="ak">为什么要用花:</strong></h1><p id="8774" class="pw-post-body-paragraph lx ly iq lz b ma mb ka mc md me kd mf mg mh mi mj mk ml mm mn mo mp mq mr ms ij bi translated">Flower是联邦学习的最新框架，创建于2020年。与TensorFlow Federated和PySyft链接到单个框架相反，Flower可以通过设计与它们一起使用。它侧重于提供有效应用联合学习的工具，并允许您专注于培训本身。用Flower实现一个基本的联邦设置非常简单(20行代码就足够了),将集中式代码改编成联邦代码所需的重写工作非常少。</p><p id="ab39" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated">此外，兼容设备的范围也非常广泛:从移动设备到Raspberry Pi、服务器等等。其架构还允许多达1000个客户端的可扩展性，如他们的论文[ <a class="ae le" href="http://arxiv.org/abs/2007.14390" rel="noopener ugc nofollow" target="_blank"> 6 </a> ]所示。总的来说，这是一个非常好的实验框架。</p><h1 id="1c7d" class="lf lg iq bd lh li lj lk ll lm ln lo lp kf lq kg lr ki ls kj lt kl lu km lv lw bi translated"><strong class="ak">GPU问题:</strong></h1><p id="f4f1" class="pw-post-body-paragraph lx ly iq lz b ma mb ka mc md me kd mf mg mh mi mj mk ml mm mn mo mp mq mr ms ij bi translated">如果您想在本地模拟一个联邦学习设置，只要CPU允许，扩展到尽可能多的客户机是非常容易的。对于基本款来说，CPU绰绰有余，不需要在GPU上进行拓展训练。然而，当使用更大的模型或更大的数据集时，您可能希望转移到GPU，以便大大提高训练速度。</p><p id="f108" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated">这就是在扩展您的联邦设置时可能会遇到问题的地方。事实上，与其他一些框架不同，Flower的目标是允许从研究/原型到生产的轻松部署，因此他们将客户视为独立的过程。此外，当访问GPU时，CUDA会自动分配固定数量的内存，以便在请求更多内存之前有足够的空间来处理。</p><p id="fb32" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated">然而，这个内存根本不能被释放，至少在进程退出之前不能。这意味着，如果您正在启动100个客户端，并且每轮对其中的10个进行采样，并且正在使用GPU，则每次客户端访问它时，都会有剩余的内存无法释放，并且随着新客户端被采样，剩余的内存会不断增加。从长远来看，你的GPU需要和客户端启动一样多的内存。</p><p id="b4df" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated">下面是一个简短的代码片段，它显示了这个问题，在Google联合实验室上运行:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi nf"><img src="../Images/6d83b29116e9eb97dff5051387994587.png" data-original-src="https://miro.medium.com/v2/resize:fit:944/format:webp/1*jVZdqGq50sjncX0JW7E5Gw.png"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">监控内存显示，即使在清空PyTorch使用的内存后，还有7%的剩余内存。图片作者。</p></figure><h1 id="ce12" class="lf lg iq bd lh li lj lk ll lm ln lo lp kf lq kg lr ki ls kj lt kl lu km lv lw bi translated"><strong class="ak">如何解决问题:</strong></h1><p id="7852" class="pw-post-body-paragraph lx ly iq lz b ma mb ka mc md me kd mf mg mh mi mj mk ml mm mn mo mp mq mr ms ij bi translated">你可能遇到过的这个问题很容易解决。由于在访问内存的进程被释放之前，内存是不会被释放的，所以我们只需要将需要访问GPU的那部分代码封装在一个子进程中，等待它被终止，直到我们可以继续执行我们的程序。多重处理是解决方案，我将向您展示如何使用PyTorch和Flower来实现。</p><p id="e610" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated">由于这个例子是基于Flower文档中的<a class="ae le" href="https://flower.dev/docs/quickstart_pytorch.html" rel="noopener ugc nofollow" target="_blank">快速入门Pytorch教程</a>，我强烈建议在继续之前查看一下，因为它展示了基础知识。</p><h1 id="119c" class="lf lg iq bd lh li lj lk ll lm ln lo lp kf lq kg lr ki ls kj lt kl lu km lv lw bi translated"><strong class="ak">助手文件</strong></h1><p id="295c" class="pw-post-body-paragraph lx ly iq lz b ma mb ka mc md me kd mf mg mh mi mj mk ml mm mn mo mp mq mr ms ij bi translated">首先，我们将为自己构建一个<em class="nb"> flower_helpers.py </em>文件，我们将在其中放置一些函数和类，这些函数和类稍后会派上用场。从进口开始，我们有:</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="ng nh l"/></div></figure><p id="cbab" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated">基本导入:用于CIFAR10工作的torch导入和一个flower策略导入，因为我们需要对用例的FedAvg策略稍作修改。然后，我们定义要在其上计算训练和测试步骤的设备:</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="ng nh l"/></div></figure><p id="31ab" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated">接下来，我们需要定义如何加载数据:</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="ng nh l"/></div></figure><p id="d669" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated">一个简单的CNN模型来自“py torch:60分钟闪电战”:</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="ng nh l"/></div></figure><p id="e22c" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated">到目前为止，从最初的花卉教程没有什么变化，从现在开始事情会变得不同。因为我们不能将模型保存在客户端的内存中，所以我们需要定义一种方法来获取模型的权重，以便客户端可以跟踪它们。为此，我们从flower教程中移走了<code class="fe ni nj nk nl b">get_parameters</code>和<code class="fe ni nj nk nl b">set_parameters</code>函数，并将它们放在我们的助手文件中:</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="ng nh l"/></div></figure><p id="b407" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated">现在，我们可以定义每次客户想要训练其模型时将被调用的训练函数:</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="ng nh l"/></div></figure><p id="84dc" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated">这个函数有三个参数，我们想要训练的本地历元的数量，全局模型的新参数和一个返回字典，该字典将作为我们的返回值向客户端返回更新的模型，本地数据集的大小和我们想要包括的其他度量，如损失或准确性。测试功能也是如此:</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="ng nh l"/></div></figure><p id="90e8" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated">最后，我们需要定义我们的定制策略。这些在快速入门教程中没有提到，但是策略是决定服务器如何聚集新权重、如何评估客户端等的类。最基本的策略是FedAvg(用于联合平均[ <a class="ae le" href="http://arxiv.org/abs/1602.05629" rel="noopener ugc nofollow" target="_blank"> 1b </a> ]),我们将使用它来实现我们自己的策略。Flower已经为您提供了一种方法，通过FedAvg策略的初始参数来定义您想要用来评估您的模型的客户数量，但是这只适用于每轮之间进行的评估。</p><p id="fcb1" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated">事实上，在最后一轮之后，flower服务器将执行最后一个评估步骤，对所有可用的客户端进行采样，以验证模型的性能。在真实情况下，这不会是一个问题，但在我们的情况下，这可能会适得其反，我们希望特别避免可能涉及GPU内存需求溢出的情况。</p><p id="a030" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated">这就是为什么我们将在本教程中只在服务器端执行评估，并且我们将删除此功能。这是通过策略的<code class="fe ni nj nk nl b">configure_evaluate </code>方法完成的，我们需要覆盖它:</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="ng nh l"/></div></figure><h1 id="8ad8" class="lf lg iq bd lh li lj lk ll lm ln lo lp kf lq kg lr ki ls kj lt kl lu km lv lw bi translated"><strong class="ak">客户端文件</strong></h1><p id="5aea" class="pw-post-body-paragraph lx ly iq lz b ma mb ka mc md me kd mf mg mh mi mj mk ml mm mn mo mp mq mr ms ij bi translated">我们已经完成了助手文件，现在我们可以切换到客户端并创建<em class="nb"> client.py </em>。从进口开始:</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="ng nh l"/></div></figure><p id="2200" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated">下一步是实现我们自己的客户端类，这样它就可以连接到flower服务器。我们需要从NumpyClient flower类派生并实现4个方法，即<code class="fe ni nj nk nl b">get_parameters</code>、<code class="fe ni nj nk nl b">set_parameters</code>、<code class="fe ni nj nk nl b">fit</code>和<code class="fe ni nj nk nl b">evaluate</code>。我们还将添加一个名为<code class="fe ni nj nk nl b">parameters</code>的属性，在这里我们将跟踪模型的权重:</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="ng nh l"/></div></figure><p id="9488" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated"><code class="fe ni nj nk nl b">get_parameters</code>和<code class="fe ni nj nk nl b">set_parameters</code>很简单，它们只是一个getter和一个setter:</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="ng nh l"/></div></figure><p id="c26d" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated">然后<code class="fe ni nj nk nl b">fit</code>方法是模型被训练的地方，它接收两个参数:来自全局模型的新参数和包含当前回合的配置的配置字典。在<code class="fe ni nj nk nl b">fit</code>内部，我们将启动一个子进程，这样我们就可以使用GPU而不用担心内存延迟:</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="ng nh l"/></div></figure><p id="ef44" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated">如您所见，该函数返回最新更新的参数、本地数据集的大小和一个字典(这里为空),其中可能包含不同的指标。最后我们有<code class="fe ni nj nk nl b">evaluate</code>方法，类似于<code class="fe ni nj nk nl b">fit</code>但用于评估。在我们的情况下，我们可以选择简单地实现最低要求，因为我们不会评估我们的客户。但是我将在这里给出完整的实现:</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="ng nh l"/></div></figure><p id="d235" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated">我们只需将所有这些打包在<code class="fe ni nj nk nl b">main</code>中，设置<code class="fe ni nj nk nl b">spawning</code>方式来创建新的子流程(不是Python下的默认方式)，并在本地端口8080上启动我们的客户端:</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="ng nh l"/></div></figure><h1 id="3241" class="lf lg iq bd lh li lj lk ll lm ln lo lp kf lq kg lr ki ls kj lt kl lu km lv lw bi translated"><strong class="ak">服务器文件</strong></h1><p id="d8c6" class="pw-post-body-paragraph lx ly iq lz b ma mb ka mc md me kd mf mg mh mi mj mk ml mm mn mo mp mq mr ms ij bi translated">客户端完成后，我们现在可以向服务器前进，我们将简单地称之为<em class="nb"> server.py </em>！在最初的教程中，启动服务器需要一行代码！但是在这里，我们将执行服务器端评估，并使用自定义策略，因此情况略有不同。从进口开始:</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="ng nh l"/></div></figure><p id="d79d" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated">首先，我们需要定义在服务器端评估模型的方式，并将函数封装在<code class="fe ni nj nk nl b">get_eval_fn</code>中，告诉服务器如何检索函数。该评估与我对客户端给出的评估几乎相同，您实际上可以合并其中的一部分:</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="ng nh l"/></div></figure><p id="2e81" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated">然后我们可以启动<code class="fe ni nj nk nl b">__main__</code>，加载参数并设置spawn方法:</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="ng nh l"/></div></figure><p id="c1b8" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated">然后我们得到一个新的网络，这样我们就可以初始化联邦循环的权重:</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="ng nh l"/></div></figure><p id="59bf" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated">最后，定义策略并在端口8080上启动服务器:</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="ng nh l"/></div></figure><h1 id="d19f" class="lf lg iq bd lh li lj lk ll lm ln lo lp kf lq kg lr ki ls kj lt kl lu km lv lw bi translated"><strong class="ak"> Bash文件</strong></h1><p id="37df" class="pw-post-body-paragraph lx ly iq lz b ma mb ka mc md me kd mf mg mh mi mj mk ml mm mn mo mp mq mr ms ij bi translated">剩下唯一要做的就是启动我们的服务器和客户端！我们编写了一个很好的bash文件，所以我们只需运行它就可以开始实验了:</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="ng nh l"/></div></figure><h1 id="d798" class="lf lg iq bd lh li lj lk ll lm ln lo lp kf lq kg lr ki ls kj lt kl lu km lv lw bi translated"><strong class="ak">运行中</strong></h1><p id="7067" class="pw-post-body-paragraph lx ly iq lz b ma mb ka mc md me kd mf mg mh mi mj mk ml mm mn mo mp mq mr ms ij bi translated">现在，只要在您的终端中运行<code class="fe ni nj nk nl b">./run.sh</code>，一旦您将它转换成可执行文件(<code class="fe ni nj nk nl b">chmod u+x run.sh)</code>，您应该会看到下面的输出:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi nm"><img src="../Images/aa5b13adfa95aa5e18db6c177345b009.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IoirRIcaYv4zJpSwBe9kkA.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">运行脚本的终端输出。图片作者。</p></figure><p id="4ae1" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated">打开一个新终端并使用<code class="fe ni nj nk nl b">nvtop</code>命令，我们可以实时监控我们的GPU使用情况:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi nn"><img src="../Images/f6d4a2233b60e673b2793e6b9b0b2846.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Wr_uNLlv4WRvt4DfGKnEuA.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">使用nvtop时的GPU内存使用情况，蓝色表示GPU计算使用情况，黄色表示内存使用情况。图片作者。</p></figure><p id="8ce2" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated">我们可以看到，我们的客户端正在正确地生成子流程，并且每当它们完成训练时，内存都会被释放。</p><p id="72f1" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated">如果你得到一个由<em class="nb">“没有名为backports.lzma的模块”</em>引起的错误，你可以用<code class="fe ni nj nk nl b">poetry add backports.lzma</code>命令添加这个包。</p><p id="e7e7" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated">如果出于某种原因，您收到一个错误，告诉您客户端无法连接，请确定在客户端尝试连接之前，服务器有足够的时间进行设置。另一个原因可能是由于<a class="ae le" href="https://stackoverflow.com/questions/57599354/python-not-able-to-connect-to-grpc-channel-failed-to-connect-to-all-addresse" rel="noopener ugc nofollow" target="_blank"> GRPC和Python </a>的一个已知错误，您可以尝试在您的服务器和客户端文件中添加以下行:</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="ng nh l"/></div></figure><p id="c9d5" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated">所有的代码都可以在<a class="ae le" href="https://github.com/matturche/flower_scaling_example" rel="noopener ugc nofollow" target="_blank"> GitHub </a>上获得。现在，您可以启动CPU允许的任意数量的客户端，并根据需要管理GPU内存。本教程到此结束。希望对你有用，不要犹豫留下反馈！</p><h1 id="bf00" class="lf lg iq bd lh li lj lk ll lm ln lo lp kf lq kg lr ki ls kj lt kl lu km lv lw bi translated"><strong class="ak">更进一步</strong></h1><p id="9e27" class="pw-post-body-paragraph lx ly iq lz b ma mb ka mc md me kd mf mg mh mi mj mk ml mm mn mo mp mq mr ms ij bi translated">当然，这只是一个工作区的演示，还没有准备好进行真正的联邦实验，但是如果你需要更进一步，你可以尝试制作你自己的联邦数据集(现在我们为所有客户端加载相同的数据)或者使用像<a class="ae le" href="https://leaf.cmu.edu/" rel="noopener ugc nofollow" target="_blank"> LEAF </a>这样的基准。用tqdm包装培训和测试步骤，以获得更好的反馈，包括更详细的报告(精确度、召回率、F1分数……)。添加一种通过加密或差分隐私保护模型的方法。诸如此类。你也可以查看更多的<a class="ae le" href="https://flower.dev/docs/#user-guide" rel="noopener ugc nofollow" target="_blank">花卉教程</a>来更好地掌握框架的可能性。</p><p id="7674" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated">最后，Flower的团队在他们最近的<a class="ae le" href="https://www.youtube.com/watch?v=MTUKPP4f-uI" rel="noopener ugc nofollow" target="_blank">峰会</a>上讨论了扩展问题，似乎虚拟客户端管理器的发布将允许解决这个问题，甚至通过允许每轮使用数千个客户端来进一步改进扩展，同时仍然考虑可用资源。</p><h1 id="5aff" class="lf lg iq bd lh li lj lk ll lm ln lo lp kf lq kg lr ki ls kj lt kl lu km lv lw bi translated"><strong class="ak">参考文献</strong></h1><pre class="kp kq kr ks gt no nl np nq aw nr bi"><span id="f91d" class="ns lg iq nl b gy nt nu l nv nw">[1a] J. Konečný, H. B. McMahan, F. X. Yu, P. Richtárik, A. T. Suresh, and D. Bacon, <a class="ae le" href="http://arxiv.org/abs/1610.05492" rel="noopener ugc nofollow" target="_blank">Federated Learning: Strategies for Improving Communication Efficiency</a> (2017), arXiv:1610.05492 [cs]</span><span id="8411" class="ns lg iq nl b gy nx nu l nv nw">[1b] H. B. McMahan, E. Moore, D. Ramage, S. Hampson, and B. A. y Arcas, <a class="ae le" href="http://arxiv.org/abs/1602.05629" rel="noopener ugc nofollow" target="_blank">Communication-Efficient Learning of Deep Networks from Decentralized Data</a> (2017), arXiv:1602.05629 [cs]</span><span id="f805" class="ns lg iq nl b gy nx nu l nv nw">[2] Q. Yang, Y. Liu, Y. Cheng, Y. Kang, T. Chen, and H. Yu, “<a class="ae le" href="https://www.morganclaypool.com/toc/aim/1/1" rel="noopener ugc nofollow" target="_blank">Federated Learning</a> (2019), Synthesis Lectures on Artificial Intelligence and Machine Learning, vol. 13, no. 3, pp. 1–207</span><span id="768e" class="ns lg iq nl b gy nx nu l nv nw">[3] A. N. Bhagoji, S. Chakraborty, P. Mittal, and S. Calo, <a class="ae le" href="http://arxiv.org/abs/1811.12470" rel="noopener ugc nofollow" target="_blank">Analyzing Federated Learning through an Adversarial Lens</a> (2019) arXiv:1811.12470 [cs, stat]</span><span id="4210" class="ns lg iq nl b gy nx nu l nv nw">[4] C. Yu et al., <a class="ae le" href="http://arxiv.org/abs/1810.07766" rel="noopener ugc nofollow" target="_blank">Distributed Learning over Unreliable Networks</a> (2019), arXiv:1810.07766 [cs]</span><span id="03bf" class="ns lg iq nl b gy nx nu l nv nw">[5] K. Bonawitz et al., <a class="ae le" href="http://arxiv.org/abs/1902.01046" rel="noopener ugc nofollow" target="_blank">Towards Federated Learning at Scale: System Design</a> (2019), arXiv:1902.01046 [cs, stat]</span><span id="0ca5" class="ns lg iq nl b gy nx nu l nv nw">[6] D. J. Beutel et al., <a class="ae le" href="http://arxiv.org/abs/2007.14390" rel="noopener ugc nofollow" target="_blank">Flower: A Friendly Federated Learning Research Framework</a> (2021), arXiv:2007.14390 [cs, stat]</span></pre></div></div>    
</body>
</html>