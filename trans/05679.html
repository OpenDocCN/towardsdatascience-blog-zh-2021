<html>
<head>
<title>29 Pytorch Snippets to Speed Up Your Machine Learning Cycle</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">29个Pytorch片段加速您的机器学习周期</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/29-pytorch-snippets-to-speed-up-your-machine-learning-cycle-38e659e26b12?source=collection_archive---------29-----------------------#2021-05-20">https://towardsdatascience.com/29-pytorch-snippets-to-speed-up-your-machine-learning-cycle-38e659e26b12?source=collection_archive---------29-----------------------#2021-05-20</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/5fd01fd8a827ab51bc2b91ed9c813f9e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*4vu8nj6_Ge9OBh2u"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">阿里·沙阿·拉哈尼在<a class="ae jg" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><div class=""/><div class=""><h2 id="4189" class="pw-subtitle-paragraph kg ji jj bd b kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx dk translated">从创建张量到编写神经网络</h2></div><h1 id="4de8" class="ky kz jj bd la lb lc ld le lf lg lh li kp lj kq lk ks ll kt lm kv ln kw lo lp bi translated">为什么片段很重要</h1><p id="e8c2" class="pw-post-body-paragraph lq lr jj ls b lt lu kk lv lw lx kn ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">我非常支持代码片段来创建比传统机器学习管道更快的迭代周期。Pytorch已经成为我的机器学习工具堆栈的重要组成部分有一段时间了，所以我决定将这篇文章写给<strong class="ls jk"> <em class="mm">分享一些我最喜欢的代码片段，以帮助你建立Pytorch模型。</em> </strong></p></div><div class="ab cl mn mo hx mp" role="separator"><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms"/></div><div class="im in io ip iq"><h2 id="119a" class="mu kz jj bd la mv mw dn le mx my dp li lz mz na lk md nb nc lm mh nd ne lo nf bi translated">1.打印Pytorch版本</h2><pre class="ng nh ni nj gt nk nl nm nn aw no bi"><span id="28da" class="mu kz jj nl b gy np nq l nr ns">import torch<br/>print(torch.version.__version__)</span><span id="3034" class="mu kz jj nl b gy nt nq l nr ns"># Output</span><span id="2e28" class="mu kz jj nl b gy nt nq l nr ns">1.7.1</span></pre><h2 id="baf8" class="mu kz jj bd la mv mw dn le mx my dp li lz mz na lk md nb nc lm mh nd ne lo nf bi translated">2.从n数组的嵌套列表中创建张量</h2><pre class="ng nh ni nj gt nk nl nm nn aw no bi"><span id="3da0" class="mu kz jj nl b gy np nq l nr ns">example_list = [1,2,3]<br/>x = torch.tensor(example_list)<br/>print(x)</span><span id="1337" class="mu kz jj nl b gy nt nq l nr ns"># Output</span><span id="975f" class="mu kz jj nl b gy nt nq l nr ns">tensor([1, 2, 3])</span></pre><h2 id="609c" class="mu kz jj bd la mv mw dn le mx my dp li lz mz na lk md nb nc lm mh nd ne lo nf bi translated">3.克隆张量</h2><pre class="ng nh ni nj gt nk nl nm nn aw no bi"><span id="26b5" class="mu kz jj nl b gy np nq l nr ns">y = x.clone()<br/>print(y)</span><span id="db2d" class="mu kz jj nl b gy nt nq l nr ns"># Output</span><span id="046b" class="mu kz jj nl b gy nt nq l nr ns">tensor([1, 2, 3])</span></pre><h2 id="d2ae" class="mu kz jj bd la mv mw dn le mx my dp li lz mz na lk md nb nc lm mh nd ne lo nf bi translated">4.获取大小和形状</h2><pre class="ng nh ni nj gt nk nl nm nn aw no bi"><span id="ca98" class="mu kz jj nl b gy np nq l nr ns">x = torch.randn((10,10))<br/>print(x.size())<br/># or<br/>print(x.shape)</span><span id="f83b" class="mu kz jj nl b gy nt nq l nr ns"># Output</span><span id="a4f7" class="mu kz jj nl b gy nt nq l nr ns">torch.Size([10, 10])<br/>torch.Size([10, 10])</span></pre><h2 id="e936" class="mu kz jj bd la mv mw dn le mx my dp li lz mz na lk md nb nc lm mh nd ne lo nf bi translated">5.沿着维度连接张量</h2><pre class="ng nh ni nj gt nk nl nm nn aw no bi"><span id="6c28" class="mu kz jj nl b gy np nq l nr ns">tensor_seq = [torch.randn(1,2),torch.randn(1,2)]<br/>x = torch.cat(tensor_seq, dim=0)<br/>print(x)</span><span id="8056" class="mu kz jj nl b gy nt nq l nr ns"># Output</span><span id="29e5" class="mu kz jj nl b gy nt nq l nr ns">tensor([[-0.4298,  1.3190],<br/>        [ 0.3904, -1.4962]])</span></pre><h2 id="0d1f" class="mu kz jj bd la mv mw dn le mx my dp li lz mz na lk md nb nc lm mh nd ne lo nf bi translated">6.将张量整形为维度:(1，任意)</h2><pre class="ng nh ni nj gt nk nl nm nn aw no bi"><span id="b0b1" class="mu kz jj nl b gy np nq l nr ns">x = torch.randn(10,2)<br/>y = x.view(1,-1)<br/>print(y.shape)</span><span id="b5c8" class="mu kz jj nl b gy nt nq l nr ns"># Output</span><span id="c7f8" class="mu kz jj nl b gy nt nq l nr ns">torch.Size([1, 20])</span></pre><h2 id="6477" class="mu kz jj bd la mv mw dn le mx my dp li lz mz na lk md nb nc lm mh nd ne lo nf bi translated">7.交换张量的特定维度</h2><pre class="ng nh ni nj gt nk nl nm nn aw no bi"><span id="6310" class="mu kz jj nl b gy np nq l nr ns">x = torch.randn(1,2)<br/>print(x)<br/>y = x.transpose(0,1)<br/>print(y)</span><span id="c44d" class="mu kz jj nl b gy nt nq l nr ns"># Output</span><span id="a6d3" class="mu kz jj nl b gy nt nq l nr ns">tensor([[-0.3085,  0.9356]])<br/><br/>tensor([[-0.3085],<br/>        [ 0.9356]])</span></pre><h2 id="920d" class="mu kz jj bd la mv mw dn le mx my dp li lz mz na lk md nb nc lm mh nd ne lo nf bi translated">8.向张量添加轴</h2><pre class="ng nh ni nj gt nk nl nm nn aw no bi"><span id="b024" class="mu kz jj nl b gy np nq l nr ns">x = torch.randn(2,2)<br/>print(x.shape)<br/>y = x.unsqueeze(dim=0)                      <br/>print(y.shape)</span><span id="073d" class="mu kz jj nl b gy nt nq l nr ns"># Output</span><span id="1ebd" class="mu kz jj nl b gy nt nq l nr ns">torch.Size([2, 2])<br/>torch.Size([1, 2, 2])</span></pre><h2 id="2627" class="mu kz jj bd la mv mw dn le mx my dp li lz mz na lk md nb nc lm mh nd ne lo nf bi translated">9.删除尺寸为1的所有尺寸</h2><pre class="ng nh ni nj gt nk nl nm nn aw no bi"><span id="2304" class="mu kz jj nl b gy np nq l nr ns">x = torch.randn(10,10,1)<br/>print(x.shape)<br/>y = x.squeeze()<br/>print(y.shape)</span><span id="ad55" class="mu kz jj nl b gy nt nq l nr ns"># Output</span><span id="43a5" class="mu kz jj nl b gy nt nq l nr ns">torch.Size([10, 10, 1])</span><span id="5027" class="mu kz jj nl b gy nt nq l nr ns">torch.Size([10, 10])</span></pre><h2 id="f8c7" class="mu kz jj bd la mv mw dn le mx my dp li lz mz na lk md nb nc lm mh nd ne lo nf bi translated">10.矩阵乘法</h2><pre class="ng nh ni nj gt nk nl nm nn aw no bi"><span id="f124" class="mu kz jj nl b gy np nq l nr ns">A = torch.ones(2,2)<br/>B = torch.randn(2,2)<br/>print(A)<br/>print(B)<br/>print(A.mm(B))</span><span id="572d" class="mu kz jj nl b gy nt nq l nr ns"># Output</span><span id="711b" class="mu kz jj nl b gy nt nq l nr ns">tensor([[1., 1.],<br/>        [1., 1.]])<br/>tensor([[ 0.5804,  1.2500],<br/>        [-0.8334,  1.1711]])<br/><br/>tensor([[-0.2530,  2.4212],<br/>        [-0.2530,  2.4212]])</span></pre><h2 id="5983" class="mu kz jj bd la mv mw dn le mx my dp li lz mz na lk md nb nc lm mh nd ne lo nf bi translated">11.矩阵向量乘法</h2><pre class="ng nh ni nj gt nk nl nm nn aw no bi"><span id="bb2e" class="mu kz jj nl b gy np nq l nr ns">A = torch.tensor([[1,2],[3,4]])<br/>x = torch.tensor([1,2])<br/>print(A.mv(x))</span><span id="6a90" class="mu kz jj nl b gy nt nq l nr ns"># Output</span><span id="02c7" class="mu kz jj nl b gy nt nq l nr ns">tensor([ 7, 10])</span></pre><h2 id="0365" class="mu kz jj bd la mv mw dn le mx my dp li lz mz na lk md nb nc lm mh nd ne lo nf bi translated">12.矩阵转置</h2><pre class="ng nh ni nj gt nk nl nm nn aw no bi"><span id="b83a" class="mu kz jj nl b gy np nq l nr ns">x = torch.randn(1,2)<br/>print(x)<br/>x = x.t()<br/>print(x)</span><span id="c9bc" class="mu kz jj nl b gy nt nq l nr ns">tensor([[0.1167, 0.4135]])<br/><br/>tensor([[0.1167],<br/>        [0.4135]])</span></pre><h2 id="ee92" class="mu kz jj bd la mv mw dn le mx my dp li lz mz na lk md nb nc lm mh nd ne lo nf bi translated">13.检查cuda可用性</h2><pre class="ng nh ni nj gt nk nl nm nn aw no bi"><span id="b012" class="mu kz jj nl b gy np nq l nr ns">print(<!-- -->torch.cuda.is_available())</span><span id="3155" class="mu kz jj nl b gy nt nq l nr ns"># Output</span><span id="32a3" class="mu kz jj nl b gy nt nq l nr ns">True</span></pre><h2 id="07be" class="mu kz jj bd la mv mw dn le mx my dp li lz mz na lk md nb nc lm mh nd ne lo nf bi translated">14.将张量数据从CPU移动到GPU并返回新对象</h2><pre class="ng nh ni nj gt nk nl nm nn aw no bi"><span id="4ae4" class="mu kz jj nl b gy np nq l nr ns">x = torch.randn(2,2)<br/>print(x)<br/>x = x.cuda()<br/>print(x)</span><span id="df0f" class="mu kz jj nl b gy nt nq l nr ns"># Output</span><span id="fd23" class="mu kz jj nl b gy nt nq l nr ns">tensor([[-1.0331, -3.2458],<br/>        [ 0.0226,  1.3091]])<br/><br/>tensor([[-1.0331, -3.2458],<br/>        [ 0.0226,  1.3091]], device='cuda:0')</span></pre><h2 id="74d0" class="mu kz jj bd la mv mw dn le mx my dp li lz mz na lk md nb nc lm mh nd ne lo nf bi translated">15.将张量数据从GPU移动到CPU</h2><pre class="ng nh ni nj gt nk nl nm nn aw no bi"><span id="2f11" class="mu kz jj nl b gy np nq l nr ns">x = torch.randn(2,2).cuda()<br/>print(x)<br/>x = x.cpu()<br/>print(x)</span><span id="98b8" class="mu kz jj nl b gy nt nq l nr ns"># Output</span><span id="619e" class="mu kz jj nl b gy nt nq l nr ns">tensor([[ 0.4664, -1.7070],<br/>        [ 1.7160,  0.0263]], device='cuda:0')<br/><br/>tensor([[ 0.4664, -1.7070],<br/>        [ 1.7160,  0.0263]])</span></pre><h2 id="e028" class="mu kz jj bd la mv mw dn le mx my dp li lz mz na lk md nb nc lm mh nd ne lo nf bi translated">16.与设备无关的代码和模块化</h2><pre class="ng nh ni nj gt nk nl nm nn aw no bi"><span id="d536" class="mu kz jj nl b gy np nq l nr ns">device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")<br/>print(device)</span><span id="d9ae" class="mu kz jj nl b gy nt nq l nr ns"># Output</span><span id="32cb" class="mu kz jj nl b gy nt nq l nr ns">device(type='cuda', index=0)</span></pre><h2 id="bcdf" class="mu kz jj bd la mv mw dn le mx my dp li lz mz na lk md nb nc lm mh nd ne lo nf bi translated">17.将张量复制到设备(gpu、cpu)</h2><pre class="ng nh ni nj gt nk nl nm nn aw no bi"><span id="d5e1" class="mu kz jj nl b gy np nq l nr ns">x = x.to(device)<br/>print(x)</span><span id="42bb" class="mu kz jj nl b gy nt nq l nr ns"># Output</span><span id="912b" class="mu kz jj nl b gy nt nq l nr ns">tensor([[ 0.4664, -1.7070],<br/>        [ 1.7160,  0.0263]], device='cuda:0')</span></pre><h2 id="316b" class="mu kz jj bd la mv mw dn le mx my dp li lz mz na lk md nb nc lm mh nd ne lo nf bi translated">18.检查它是否是Pytorch张量</h2><pre class="ng nh ni nj gt nk nl nm nn aw no bi"><span id="3f97" class="mu kz jj nl b gy np nq l nr ns">print(<!-- -->torch.is_tensor(x))</span><span id="f75d" class="mu kz jj nl b gy nt nq l nr ns"># Output</span><span id="fd2c" class="mu kz jj nl b gy nt nq l nr ns">True</span></pre><h2 id="aece" class="mu kz jj bd la mv mw dn le mx my dp li lz mz na lk md nb nc lm mh nd ne lo nf bi translated">19.检查它是否是Pytorch存储对象</h2><pre class="ng nh ni nj gt nk nl nm nn aw no bi"><span id="543a" class="mu kz jj nl b gy np nq l nr ns">print(<!-- -->torch.is_storage(x))</span><span id="a598" class="mu kz jj nl b gy nt nq l nr ns"># Output</span><span id="6c73" class="mu kz jj nl b gy nt nq l nr ns">False</span></pre><h2 id="7081" class="mu kz jj bd la mv mw dn le mx my dp li lz mz na lk md nb nc lm mh nd ne lo nf bi translated">20.获取输入张量中的元素总数</h2><pre class="ng nh ni nj gt nk nl nm nn aw no bi"><span id="3bc1" class="mu kz jj nl b gy np nq l nr ns">x = torch.randn(2,2) # 4 elements<br/>torch.numel(x)</span><span id="f0f0" class="mu kz jj nl b gy nt nq l nr ns"># Output</span><span id="81e5" class="mu kz jj nl b gy nt nq l nr ns">4</span></pre><h2 id="ae1b" class="mu kz jj bd la mv mw dn le mx my dp li lz mz na lk md nb nc lm mh nd ne lo nf bi translated">21.获取给定大小的单位矩阵</h2><pre class="ng nh ni nj gt nk nl nm nn aw no bi"><span id="0fb6" class="mu kz jj nl b gy np nq l nr ns">size = 5<br/>print(torch.eye(size))</span><span id="94be" class="mu kz jj nl b gy nt nq l nr ns"># Output</span><span id="9a5e" class="mu kz jj nl b gy nt nq l nr ns">tensor([[1., 0., 0., 0., 0.],<br/>        [0., 1., 0., 0., 0.],<br/>        [0., 0., 1., 0., 0.],<br/>        [0., 0., 0., 1., 0.],<br/>        [0., 0., 0., 0., 1.]])</span></pre><h2 id="65ad" class="mu kz jj bd la mv mw dn le mx my dp li lz mz na lk md nb nc lm mh nd ne lo nf bi translated">22.从numpy数组到torch张量的转换</h2><pre class="ng nh ni nj gt nk nl nm nn aw no bi"><span id="6c49" class="mu kz jj nl b gy np nq l nr ns">x = np.random.rand(2,2)<br/>print(torch.from_numpy(x))</span><span id="8aad" class="mu kz jj nl b gy nt nq l nr ns"># Output</span><span id="9021" class="mu kz jj nl b gy nt nq l nr ns">tensor([[0.7407, 0.8823],<br/>        [0.0352, 0.5823]], dtype=torch.float64)</span></pre><h2 id="37d7" class="mu kz jj bd la mv mw dn le mx my dp li lz mz na lk md nb nc lm mh nd ne lo nf bi translated">23.基本的线性间距(比如numpy的np.linspace)</h2><pre class="ng nh ni nj gt nk nl nm nn aw no bi"><span id="158f" class="mu kz jj nl b gy np nq l nr ns">print(np.linspace(1,5,10))<br/>print(torch.linspace(1, 5, steps=10))</span><span id="d2aa" class="mu kz jj nl b gy nt nq l nr ns"># Output</span><span id="2b53" class="mu kz jj nl b gy nt nq l nr ns">[1.         1.44444444 1.88888889 2.33333333 2.77777778 3.22222222<br/> 3.66666667 4.11111111 4.55555556 5.        ] # numpy<br/><br/><br/>tensor([1.0000, 1.4444, 1.8889, 2.3333, 2.7778, 3.2222, 3.6667, 4.1111, 4.5556,<br/>        5.0000]) # pytorch</span></pre><h2 id="ad51" class="mu kz jj bd la mv mw dn le mx my dp li lz mz na lk md nb nc lm mh nd ne lo nf bi translated">24.对数间距</h2><pre class="ng nh ni nj gt nk nl nm nn aw no bi"><span id="1a28" class="mu kz jj nl b gy np nq l nr ns">torch.logspace(start=-10, end=10, steps=15) #logarithmic spacing</span><span id="f3c1" class="mu kz jj nl b gy nt nq l nr ns"># Output</span><span id="da61" class="mu kz jj nl b gy nt nq l nr ns">tensor([1.0000e-10, 2.6827e-09, 7.1969e-08, 1.9307e-06, 5.1795e-05, 1.3895e-03,<br/>        3.7276e-02, 1.0000e+00, 2.6827e+01, 7.1969e+02, 1.9307e+04, 5.1795e+05,<br/>        1.3895e+07, 3.7276e+08, 1.0000e+10])</span></pre><h2 id="257d" class="mu kz jj bd la mv mw dn le mx my dp li lz mz na lk md nb nc lm mh nd ne lo nf bi translated">25.将Pytorch张量分割成小块</h2><pre class="ng nh ni nj gt nk nl nm nn aw no bi"><span id="94d3" class="mu kz jj nl b gy np nq l nr ns">x = torch.linspace(1,10,10)<br/>print(torch.chunk(x,chunks=5))</span><span id="4387" class="mu kz jj nl b gy nt nq l nr ns"># Output</span><span id="87e0" class="mu kz jj nl b gy nt nq l nr ns">(tensor([1., 2.]),<br/> tensor([3., 4.]),<br/> tensor([5., 6.]),<br/> tensor([7., 8.]),<br/> tensor([ 9., 10.]))</span></pre><h2 id="1370" class="mu kz jj bd la mv mw dn le mx my dp li lz mz na lk md nb nc lm mh nd ne lo nf bi translated">26.基本神经网络</h2><pre class="ng nh ni nj gt nk nl nm nn aw no bi"><span id="9798" class="mu kz jj nl b gy np nq l nr ns">import torch<br/>import torch.nn as nn<br/>import torch<br/><br/>class NeuralNet(nn.Module):<br/>    def __init__(self):<br/>        super(NeuralNet, self).__init__()<br/>        self.fc1 = nn.Linear(1,1)<br/>        self.relu = nn.ReLU()<br/>        <br/>    <br/>    def forward(self,x):<br/>        x = self.fc1(x)<br/>        x = self.relu(x)<br/>        <br/>        return x<br/>net = NeuralNet()<br/>net</span><span id="2a1c" class="mu kz jj nl b gy nt nq l nr ns"># Output</span><span id="f9f3" class="mu kz jj nl b gy nt nq l nr ns">NeuralNet(<br/>  (fc1): Linear(in_features=1, out_features=1, bias=True)<br/>  (relu): ReLU()<br/>)</span></pre><h2 id="8151" class="mu kz jj bd la mv mw dn le mx my dp li lz mz na lk md nb nc lm mh nd ne lo nf bi translated">27.创建张量来保存用于训练神经网络的输入和输出</h2><pre class="ng nh ni nj gt nk nl nm nn aw no bi"><span id="ae71" class="mu kz jj nl b gy np nq l nr ns">x = torch.linspace(-10, 10, 2000).view(-1,1)<br/>y = torch.square(x)</span></pre><h2 id="045a" class="mu kz jj bd la mv mw dn le mx my dp li lz mz na lk md nb nc lm mh nd ne lo nf bi translated">28.加载神经网络并设置损失函数和优化器</h2><pre class="ng nh ni nj gt nk nl nm nn aw no bi"><span id="0e36" class="mu kz jj nl b gy np nq l nr ns">model = NeuralNet()<br/>criterion = torch.nn.MSELoss()<br/>optimizer = torch.optim.SGD(model.parameters(), lr=1e-6)</span></pre><h2 id="b5c6" class="mu kz jj bd la mv mw dn le mx my dp li lz mz na lk md nb nc lm mh nd ne lo nf bi translated">29.训练循环，每10个时期打印一次输出</h2><pre class="ng nh ni nj gt nk nl nm nn aw no bi"><span id="9208" class="mu kz jj nl b gy np nq l nr ns">epochs = 50<br/>for t in range(epochs):<br/>    # Forward pass: computing prediction<br/>    y_pred = model(x)<br/><br/>    # Compute the loss and printing ever 10 iterations<br/>    loss = criterion(y_pred, y)<br/>    if t % 10 == 9:<br/>        print(t, loss.item())<br/><br/>    # Zero gradients, perform a backward pass, and update the weights.<br/>    optimizer.zero_grad()<br/>    loss.backward()<br/>    optimizer.step()</span><span id="47a4" class="mu kz jj nl b gy nt nq l nr ns"># Output</span><span id="86d1" class="mu kz jj nl b gy nt nq l nr ns">9 1987.289306640625<br/>19 1986.6630859375<br/>29 1986.0374755859375<br/>39 1985.412353515625<br/>49 1984.78759765625</span></pre></div><div class="ab cl mn mo hx mp" role="separator"><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms"/></div><div class="im in io ip iq"><h1 id="ede7" class="ky kz jj bd la lb nu ld le lf nv lh li kp nw kq lk ks nx kt lm kv ny kw lo lp bi translated">结论</h1><p id="4506" class="pw-post-body-paragraph lq lr jj ls b lt lu kk lv lw lx kn ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">我认为这样的代码片段很容易获得，这很有帮助。这使得整个迭代周期更快，也更令人愉快，因为我们不必不断地寻找如何在我们选择的框架中执行我们想要的每个任务。</p><p id="a8fb" class="pw-post-body-paragraph lq lr jj ls b lt nz kk lv lw oa kn ly lz ob mb mc md oc mf mg mh od mj mk ml im bi translated">我想在这里提到两个我认为对Pytorch中的基本代码非常有用的资源:</p><div class="is it gp gr iu oe"><a href="https://pytorch.org/tutorials/beginner/ptcheat.html" rel="noopener  ugc nofollow" target="_blank"><div class="of ab fo"><div class="og ab oh cl cj oi"><h2 class="bd jk gy z fp oj fr fs ok fu fw ji bi translated">PyTorch备忘单- PyTorch教程1.8.1+cu102文档</h2><div class="ol l"><h3 class="bd b gy z fp oj fr fs ok fu fw dk translated">编辑描述</h3></div><div class="om l"><p class="bd b dl z fp oj fr fs ok fu fw dk translated">pytorch.org</p></div></div></div></a></div><div class="is it gp gr iu oe"><a href="https://github.com/Apress/pytorch-recipes" rel="noopener  ugc nofollow" target="_blank"><div class="of ab fo"><div class="og ab oh cl cj oi"><h2 class="bd jk gy z fp oj fr fs ok fu fw ji bi translated">apress/py torch-食谱</h2><div class="ol l"><h3 class="bd b gy z fp oj fr fs ok fu fw dk translated">该知识库附有Pradeepta Mishra的PyTorch食谱(Apress，2019)。使用下载压缩文件…</h3></div><div class="om l"><p class="bd b dl z fp oj fr fs ok fu fw dk translated">github.com</p></div></div><div class="on l"><div class="oo l op oq or on os ja oe"/></div></div></a></div><p id="1f53" class="pw-post-body-paragraph lq lr jj ls b lt nz kk lv lw oa kn ly lz ob mb mc md oc mf mg mh od mj mk ml im bi translated">因此，请确保您的Pytorch代码片段随时可用，并享受编写代码的乐趣！</p></div><div class="ab cl mn mo hx mp" role="separator"><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms"/></div><div class="im in io ip iq"><p id="980d" class="pw-post-body-paragraph lq lr jj ls b lt nz kk lv lw oa kn ly lz ob mb mc md oc mf mg mh od mj mk ml im bi translated">如果你喜欢这篇文章，请在<a class="ae jg" href="https://twitter.com/LucasEnkrateia" rel="noopener ugc nofollow" target="_blank"> Twitter </a>、<a class="ae jg" href="https://www.linkedin.com/in/lucas-soares-969044167/" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>上联系我，并在<a class="ae jg" href="https://lucas-soares.medium.com" rel="noopener"> Medium </a>上关注我。谢谢，下次再见！:)</p></div><div class="ab cl mn mo hx mp" role="separator"><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms"/></div><div class="im in io ip iq"><h1 id="1162" class="ky kz jj bd la lb nu ld le lf nv lh li kp nw kq lk ks nx kt lm kv ny kw lo lp bi translated">参考</h1><ul class=""><li id="ab30" class="ot ou jj ls b lt lu lw lx lz ov md ow mh ox ml oy oz pa pb bi translated"><a class="ae jg" href="https://pytorch.org/tutorials/beginner/ptcheat.html" rel="noopener ugc nofollow" target="_blank"> Pytorch备忘单</a></li><li id="87ba" class="ot ou jj ls b lt pc lw pd lz pe md pf mh pg ml oy oz pa pb bi translated"><a class="ae jg" href="https://github.com/Apress/pytorch-recipes" rel="noopener ugc nofollow" target="_blank"> Pytorch食谱</a></li></ul></div></div>    
</body>
</html>