<html>
<head>
<title>Word Embedding Techniques: Word2Vec and TF-IDF Explained</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">单词嵌入技术:Word2Vec和TF-IDF讲解</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/word-embedding-techniques-word2vec-and-tf-idf-explained-c5d02e34d08?source=collection_archive---------1-----------------------#2021-07-22">https://towardsdatascience.com/word-embedding-techniques-word2vec-and-tf-idf-explained-c5d02e34d08?source=collection_archive---------1-----------------------#2021-07-22</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="be1c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><em class="kl">需要让这些词对机器学习或深度学习算法有意义。因此，它们必须用数字来表示。诸如One Hot Encoding、TF-IDF、Word2Vec、FastText之类的算法使单词能够以数学方式表示为用于解决此类问题的单词嵌入技术。</em></p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="gh gi km"><img src="../Images/f60bd1e95e4ce854573a7ea200125a98.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WLMFjPdcA2Fh5DsZ_ahAFg.jpeg"/></div></div><p class="ky kz gj gh gi la lb bd b be z dk translated">Camille Orgel在<a class="ae lc" href="https://unsplash.com/photos/SKcTKYNRvHY" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><h1 id="ebda" class="ld le iq bd lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma bi translated">单词嵌入</h1><p id="1a5f" class="pw-post-body-paragraph jn jo iq jp b jq mb js jt ju mc jw jx jy md ka kb kc me ke kf kg mf ki kj kk ij bi translated">单词嵌入技术用于以数学方式表示单词。One Hot Encoding、TF-IDF、Word2Vec、FastText是常用的单词嵌入方法。根据数据处理的状态、大小和目的，这些技术中的一种(在某些情况下是几种)是首选的。</p><h2 id="66b5" class="mg le iq bd lf mh mi dn lj mj mk dp ln jy ml mm lr kc mn mo lv kg mp mq lz mr bi translated">*一个热编码</h2><p id="6976" class="pw-post-body-paragraph jn jo iq jp b jq mb js jt ju mc jw jx jy md ka kb kc me ke kf kg mf ki kj kk ij bi translated">用数字表示数据的最基本的技术之一是热编码技术[1]。在这种方法中，按照唯一单词总数的大小创建一个向量。向量的值被分配，使得属于其索引的每个单词的值是1，而其他的是0。作为一个例子，可以研究图1。</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="gh gi ms"><img src="../Images/9ac273312876116ce754abd6ae8143a6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CKO2BWiiU8WxYl74A2XXkQ.png"/></div></div><p class="ky kz gj gh gi la lb bd b be z dk translated">图一。一个热编码的样本</p></figure><p id="414a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在图1中，名为“X”的列由3个不同的单词组成。当对该列应用一个热编码时，创建了表示每个表达式的3个不同的列(换句话说，为每行创建了3个单位向量)。对应于每行中单词的列用值1填充，其他的用0填充。因此，这些表达被数字化了。它通常用于没有太多语言数据多样性的情况，并且不需要表示数据之间的语义和统计关系。</p><h2 id="b8b3" class="mg le iq bd lf mh mi dn lj mj mk dp ln jy ml mm lr kc mn mo lv kg mp mq lz mr bi translated">* TF-IDF</h2><p id="cac7" class="pw-post-body-paragraph jn jo iq jp b jq mb js jt ju mc jw jx jy md ka kb kc me ke kf kg mf ki kj kk ij bi translated">TF-IDF是一种统计方法，用于确定文档中单词的数学意义[2]。矢量化过程类似于热编码。或者，对应于该单词的值被赋予TF-IDF值而不是1。TF-IDF值通过将TF和IDF值相乘获得。作为一个例子，让我们找到由1个句子组成的3个文档的TF-IDF值。</p><blockquote class="mt mu mv"><p id="c6bd" class="jn jo kl jp b jq jr js jt ju jv jw jx mw jz ka kb mx kd ke kf my kh ki kj kk ij bi translated">[他是沃尔特]，</p><p id="6d96" class="jn jo kl jp b jq jr js jt ju jv jw jx mw jz ka kb mx kd ke kf my kh ki kj kk ij bi translated">[他是威廉]，</p><p id="1087" class="jn jo kl jp b jq jr js jt ju jv jw jx mw jz ka kb mx kd ke kf my kh ki kj kk ij bi translated">[他不是彼得或九月]</p></blockquote><p id="6e49" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在上面的例子中，“他”在所有3个文档中使用，“是”在2个文档中使用，“或”只在一个文档中使用。根据这些，我们分别求出TF，然后求出IDF值。</p><ul class=""><li id="f752" class="mz na iq jp b jq jr ju jv jy nb kc nc kg nd kk ne nf ng nh bi translated"><strong class="jp ir"> <em class="kl"> TF(词频)</em> </strong></li></ul><p id="13ef" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">用最简单的术语来说，术语频率是文档中目标术语的数量与文档中术语总数的比率。如果根据上面的例子计算TF值，它将是</p><p id="a21c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi">[0.33, 0.33, 0.33],</p><p id="789c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi">[0.33, 0.33, 0.33],</p><p id="d376" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi">[0.20, 0.20, 0.20, 0.20, 0.20]</p><ul class=""><li id="d455" class="mz na iq jp b jq jr ju jv jy nb kc nc kg nd kk ne nf ng nh bi translated"><strong class="jp ir"> <em class="kl"> IDF ( </em>逆文档频率<em class="kl"> ) </em> </strong></li></ul><p id="aaaa" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">IDF值是文档总数与目标术语出现的文档数之比的对数。在这个阶段，这个术语在文档中出现多少次并不重要。确定是否通过就足够了。在这个例子中，要取的对数的底值被确定为10。但是，使用不同的值没有问题。</p><p id="2669" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">“他”:Log(3/3)= 0，</p><p id="3a81" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">“是”:Log(3/2):0.1761，</p><p id="061b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">“或者，彼得，.”:对数(3/1) : 0.4771</p><p id="1928" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">因此，获得了TF和IDF值。如果使用这些值创建矢量化，首先会为每个文档创建一个向量，该向量由等于所有文档中唯一单词数量的元素组成(在本例中，有8个术语)。在这个阶段，有一个问题需要解决。如术语“he”所示，由于IDF值为0，因此TF-IDF值也将为零。但是，在矢量化过程中没有包含在文档中的单词(例如，第一句中没有包含短语“Peter”)将被赋值为0。为了避免混淆，对TF-IDF值进行平滑处理以进行矢量化。最常见的方法是在获得的值上加1。根据目的，可以在以后对这些值应用规范化。如果矢量化过程是根据上述内容创建的；</p><p id="e77f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi">[1. , 1.1761 , 1.4771 , 0. , 0. , 0. , 0. , 0.],</p><p id="3e19" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi">[1. , 1.1761 , 0. , 1.4771 , 0. , 0. , 0. , 0.],</p><p id="66ab" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi">[1. , 0. , 0. , 0. , 1.4771 , 1.4771, 1.4771 , 1.4771],</p><h2 id="7ae7" class="mg le iq bd lf mh mi dn lj mj mk dp ln jy ml mm lr kc mn mo lv kg mp mq lz mr bi translated">* Word2Vec</h2><p id="f644" class="pw-post-body-paragraph jn jo iq jp b jq mb js jt ju mc jw jx jy md ka kb kc me ke kf kg mf ki kj kk ij bi translated">Word2vec是另一种常用的单词嵌入技术。扫描整个语料库，并通过确定目标单词更经常出现的单词来执行向量创建过程[3]。这样，单词之间的语义接近度也就显现出来了。例如，让序列中的每个字母..x y A z w..,..x y B z k..和..x l C d m…代表一个单词。在这种情况下，word_A会比word_C更接近word_B，当在向量构成中考虑到这种情况时，单词之间的语义接近程度就用数学来表示了。</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="gh gi ni"><img src="../Images/dd90445cd51630e324da85449e6d1ce2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8n9olHBg0vIYCRR1eAlLDA.png"/></div></div><p class="ky kz gj gh gi la lb bd b be z dk translated">图二。单词相似度</p></figure><p id="3b23" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">图2显示了Word2Vec中最常用的图像之一。这些单词之间的语义接近度是向量值彼此之间的数学接近度。经常举的一个例子是“国王-男人+女人=王后”这个等式。这里发生的情况是，作为向量彼此相减和相加的结果而获得的向量值等于对应于“queen”表达式的向量。可以理解，单词“king”和“queen”彼此非常相似，但向量差异仅因其性别而产生。</p><p id="f3a5" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在Word2Vec方法中，与One Hot Encoding和TF-IDF方法不同，执行无监督学习过程。通过人工神经网络对未标记的数据进行训练，以创建生成单词向量的Word2Vec模型。与其他方法不同，向量大小不像语料库中唯一单词的数量那么多。向量的大小可以根据语料库的大小和项目的类型来选择。这对于非常大的数据尤其有益。例如，如果我们假设在大型语料库中有300 000个唯一单词，当用一个热编码执行向量创建时，为每个单词创建300 000大小的向量，其中只有一个元素的值为1，其他元素的值为0。然而，通过在Word2Vec侧选择向量大小300(它可以根据用户的选择或多或少)，避免了不必要的大尺寸向量操作。</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="gh gi nj"><img src="../Images/ecca6b45e7bdef08c4e758cd28c27141.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*a5VtmlgdNAZD_KbvaUpqgQ.png"/></div></div><p class="ky kz gj gh gi la lb bd b be z dk translated">图3。Word2Vec模型中单词的矢量化</p></figure><p id="7eff" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">“Royal”的矢量化可以在图3中看到。如果用一个热编码将五个单词句子中的单词“Royal”矢量化，则获得第一个向量表达式(输入向量)。可以看出，这个向量的大小和句子中的总字数一样多。但是，如果矢量化过程是用Word2Vec完成的，这一次将创建一个包含三个单位[5，12，2]的向量。</p><p id="75fa" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">ka ggle(<a class="ae lc" href="https://www.kaggle.com/anu0012/hotel-review" rel="noopener ugc nofollow" target="_blank">https://www.kaggle.com/anu0012/hotel-review</a>)中的酒店评论数据集用于应用Word2Vec模型训练。作为例子给出的所有代码都可以在<a class="ae lc" href="https://github.com/ademakdogan/word2vec_generator" rel="noopener ugc nofollow" target="_blank">这里</a>找到。</p><figure class="kn ko kp kq gt kr"><div class="bz fp l di"><div class="nk nl l"/></div></figure><p id="5955" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">因为区分大小写，所以所有单词都转换成小写。然后，清除特殊字符和停用词。nltk库用于无效单词。如果需要，这些单词也可以完全手动确定。执行这些操作之前的一个例句如下。</p><p id="933f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><em class="kl">“我丈夫和我在这家酒店住过几次。虽然不是最高档的酒店，但我们喜欢这样的事实，我们可以步行大约英里到芬威。它很干净，工作人员也很通融。我唯一的抱怨是浴室里的风扇很吵，而且当你开灯的时候它会自动运转，我们也尽可能地让灯保持关闭。我们住过收费较高的酒店，包括网络和早餐。会再次留在那里。”</em></p><p id="99d0" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">数据预处理后会出现的新情况如下。</p><p id="bbb8" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><em class="kl">“丈夫住过的酒店时代虽然最华丽的酒店爱事实步行英里芬威清洁员工住宿投诉范浴室噪音去自动关灯试着保持轻多可能我们住过的酒店收费较高互联网早餐包括住宿”</em></p><p id="adce" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这些过程完成后，进行Word2Vec训练。培训期间使用的参数:</p><p id="71eb" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir"> min_count : </strong>目标词在语料库中出现的最小次数。特别是对于非常大的复合病毒，保持这个限制较高会增加更多的成功率。但是，对于小型数据集，保持较小的大小会更准确。</p><p id="4037" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">窗口:</strong>直接影响目标表达式向量计算的是相邻词的数量。比如“他是一个很好的人。”对于window =1，单词“a”和“good”在“very”单词向量的形成中是有效的。当window = 2时，单词“是”、“一个”、“好”和“人”在创建“非常”单词向量时是有效的。</p><p id="1146" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir"> size : </strong>它是为每个元素创建的向量的大小。</p><p id="bd77" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir"> alpha : </strong>初始学习率</p><p id="2f90" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir"> min_alpha : </strong>是训练时学习率会线性下降的最小值。</p><p id="d5c1" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir"> sg : </strong>指定训练算法将如何工作。如果sg的值为1，则使用skip-gram算法，否则使用CBOW算法。</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="gh gi nm"><img src="../Images/1bdeaed4fabe83afba9473d2eea3184a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VTu7IlEOcoqs4B5Xs711Ag.png"/></div></div><p class="ky kz gj gh gi la lb bd b be z dk translated">图4。Skip-gram vs CBOW</p></figure><p id="77af" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">CBOW(连续单词包)和Skip-gram算法之间的区别可以在图4中看到。在使用CBOW算法的训练中，与目标单词相邻的单词作为输入给出，而目标单词本身作为输出获得。在skip-gram算法中，目标单词本身作为输入给出，相邻单词作为输出获得。</p><p id="438b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">工作人员:</strong>培训可以并行进行。用于此目的的内核数量可以通过workers参数来确定。</p><p id="0048" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">如果您想通过使用作为训练结果获得的模型来查看单词“great”的向量；</p><pre class="kn ko kp kq gt nn no np nq aw nr bi"><span id="a60e" class="mg le iq no b gy ns nt l nu nv">w2v_model["great"]<br/>&gt;&gt;&gt;array([ 3.03658217e-01, -1.56424701e-01, -8.23674500e-01,<br/>.<br/>.<br/>.</span><span id="cf99" class="mg le iq no b gy nw nt l nu nv">-1.36196673e-01,  8.55127215e-01, -7.31807232e-01,  1.36362463e-01],<br/>      dtype=float32)</span><span id="281c" class="mg le iq no b gy nw nt l nu nv">print(w2v_model["great"].shape)<br/>&gt;&gt;&gt;(300,)</span></pre><p id="e562" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">最接近“伟大”、“可怕”、“波士顿”、“代客”的10个词如下。</p><pre class="kn ko kp kq gt nn no np nq aw nr bi"><span id="c042" class="mg le iq no b gy ns nt l nu nv">w2v_model.wv.most_similar(positive=["great"])<br/>&gt;&gt;&gt;[('excellent', 0.8094755411148071),<br/> ('fantastic', 0.7735758423805237),<br/> ('perfect', 0.7473931312561035),<br/> ('wonderful', 0.7063912153244019),<br/> ('good', 0.7039040327072144),<br/> ('amazing', 0.6384587287902832),<br/> ('loved', 0.6266685724258423),<br/> ('nice', 0.6253951787948608),<br/> ('awesome', 0.6186609268188477),<br/> ('enjoyed', 0.5889394283294678)]<br/>---------------------------------<br/>w2v_model.wv.most_similar(positive=["terrible"])<br/>&gt;&gt;&gt;[('bad', 0.5275813341140747),<br/> ('poor', 0.504431962966919),<br/> ('horrible', 0.4722219705581665),<br/> ('awful', 0.42389577627182007),<br/> ('worst', 0.40153956413269043),<br/> ('dirty', 0.3467090427875519),<br/> ('disgusting', 0.32588857412338257),<br/> ('horrendous', 0.3157917261123657),<br/> ('lousy', 0.30114778876304626),<br/> ('uncomfortable', 0.3005620837211609)]<br/>---------------------------------<br/>w2v_model.wv.most_similar(positive=["boston"])<br/>&gt;&gt;&gt;[('chicago', 0.519180417060852),<br/> ('seattle', 0.5126588940620422),<br/> ('dc', 0.4830571711063385),<br/> ('bostons', 0.4459514617919922),<br/> ('copley', 0.4455355107784271),<br/> ('city', 0.44090309739112854),<br/> ('newbury', 0.4349810481071472),<br/> ('fenway', 0.4237935543060303),<br/> ('philadelphia', 0.40892332792282104),<br/> ('denver', 0.39840811491012573)]<br/>---------------------------------<br/>w2v_model.wv.most_similar(positive=["valet"])<br/>&gt;&gt;&gt;[('parking', 0.7374086380004883),<br/> ('car', 0.6263512969017029),<br/> ('garage', 0.6224508285522461),<br/> ('retrieving', 0.5173929929733276),<br/> ('self', 0.5013973712921143),<br/> ('inandout', 0.4847780168056488),<br/> ('selfpark', 0.47603434324264526),<br/> ('fee', 0.47458043694496155),<br/> ('per', 0.4741314947605133),<br/> ('parked', 0.4707031846046448)]</span></pre><h2 id="502c" class="mg le iq bd lf mh mi dn lj mj mk dp ln jy ml mm lr kc mn mo lv kg mp mq lz mr bi translated">*快速文本</h2><p id="bf47" class="pw-post-body-paragraph jn jo iq jp b jq mb js jt ju mc jw jx jy md ka kb kc me ke kf kg mf ki kj kk ij bi translated">FastText算法的工作逻辑类似于Word2Vec，但最大的不同是它在训练时也使用了N元词[4]。虽然这增加了模型的大小和处理时间，但它也赋予了模型预测单词的不同变化的能力。例如，假设单词“Windows”在训练数据集中，我们希望在训练结束后获得单词“Wndows”的向量。如果将Word2Vec模型用于这些操作，将会给出一个错误，即字典中不存在单词“Wndows ”,并且不会返回任何向量。但是，如果FastText模型用于此过程，则vector将返回，单词“Windows”将是最接近的单词之一。如上所述，训练中不仅包括单词本身，还包括N-gram变体(例如单词“Windows”-&gt; Win、ind、ndo、dow、ows的3-gram表达式)。尽管FastText模型目前在许多不同的领域中使用，但它经常是首选，尤其是在OCR工作中需要单词嵌入技术时。特别是与其他不能容忍最轻微OCR错误的技术相比，FastText在获取不直接在其自身词汇表中的偶数单词的向量方面提供了很大的优势。正因如此，在可能出现用词错误的问题上，它比其他备选方案领先一步。</p><p id="2562" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">上述矢量化方法是当今最常用的技术。每一种都有不同的用途。在需要进行单词矢量化的研究中，应该首先确定问题，然后根据这个问题优先选择矢量化技术。事实上，每种技术都有不同的优势。</p><p id="3d88" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">除此之外，还有语境表征，如ELMO和伯特[5]。这些问题将在下一篇文章中讨论。</p><h2 id="6f84" class="mg le iq bd lf mh mi dn lj mj mk dp ln jy ml mm lr kc mn mo lv kg mp mq lz mr bi translated">开源代码库</h2><p id="348a" class="pw-post-body-paragraph jn jo iq jp b jq mb js jt ju mc jw jx jy md ka kb kc me ke kf kg mf ki kj kk ij bi translated">所有代码都可以在<a class="ae lc" href="https://github.com/ademakdogan/word2vec_generator" rel="noopener ugc nofollow" target="_blank">https://github.com/ademakdogan/word2vec_generator</a>找到。在这个项目中，word2vec训练可以根据任何所需csv文件中确定的列自动完成。将作为操作结果创建的word2vec模型保存在“model”文件夹下。下面是用法示例。这个项目也可以在docker上运行。src/training.py中的参数可以根据数据大小进行优化。</p><pre class="kn ko kp kq gt nn no np nq aw nr bi"><span id="4087" class="mg le iq no b gy ns nt l nu nv">python3 src/w2vec.py -c &lt;/Users/.../data.csv&gt; -t &lt;target_column_name&gt;</span></pre><p id="6349" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">Github:</strong>【https://github.com/ademakdogan】T4</p><p id="636a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">领英:</strong><a class="ae lc" href="https://www.linkedin.com/in/adem-akdo%C4%9Fan-948334177/" rel="noopener ugc nofollow" target="_blank">https://www.linkedin.com/in/adem-akdo%C4%9Fan-948334177/</a></p><h1 id="035b" class="ld le iq bd lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma bi translated">参照符号</h1><p id="f677" class="pw-post-body-paragraph jn jo iq jp b jq mb js jt ju mc jw jx jy md ka kb kc me ke kf kg mf ki kj kk ij bi translated">[1]史蒂文斯，S. S. (1946)。《论测量的尺度》。<em class="kl">科学，新系列</em>，103.2684，677–680。</p><p id="1158" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">[2]会泽明子(2003)。“TF-IDF测量的信息论观点”。<em class="kl">信息处理与管理</em>。<strong class="jp ir"> 39 </strong> (1)，45–65。<a class="ae lc" href="https://en.wikipedia.org/wiki/Doi_(identifier)" rel="noopener ugc nofollow" target="_blank">doi</a>:<a class="ae lc" href="https://doi.org/10.1016%2FS0306-4573%2802%2900021-3" rel="noopener ugc nofollow" target="_blank">10.1016/s 0306–4573(02)00021–3</a></p><p id="7b8d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">[3]托马斯·米科洛夫等人(2013年)。“向量空间中单词表示的有效估计”。<a class="ae lc" href="https://en.wikipedia.org/wiki/ArXiv_(identifier)" rel="noopener ugc nofollow" target="_blank">arXiv</a>:<a class="ae lc" href="https://arxiv.org/abs/1301.3781" rel="noopener ugc nofollow" target="_blank">1301.3781</a></p><p id="c7ad" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">[4]阿曼德·朱林，爱德华·格雷夫，皮奥特·博亚诺斯基，托马斯·米科洛夫，(2017)。“有效文本分类的锦囊妙计”，会议:计算语言学协会欧洲分会第15届会议论文集:第2卷。</p><p id="d888" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">[5]德夫林·雅各布，常明伟，李·肯顿，图塔诺娃·克里斯蒂娜，(2018)。“BERT:用于语言理解的深度双向转换器的预训练”</p></div></div>    
</body>
</html>