<html>
<head>
<title>Under the Hood of Gradient Boosting and Its Python Implementation</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">梯度增强及其Python实现</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/under-the-hood-of-gradient-boosting-and-its-python-implementation-99cc63efd24d?source=collection_archive---------9-----------------------#2021-10-24">https://towardsdatascience.com/under-the-hood-of-gradient-boosting-and-its-python-implementation-99cc63efd24d?source=collection_archive---------9-----------------------#2021-10-24</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="0a31" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph">助推技术</h2><div class=""/><div class=""><h2 id="946e" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">机器学习中的助推算法——第三部分</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/faed59202649d8f936bae5443a024cd5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QC0djMPoZ_amSlZDjh9fHQ.jpeg"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">唐纳德·詹纳蒂在<a class="ae lh" href="https://unsplash.com/?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="8242" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">到目前为止，我们已经在<a class="ae lh" rel="noopener" target="_blank" href="/introduction-to-boosted-trees-2692b6653b53">第一部分</a>中讨论了升压的一般含义和一些重要的技术术语。我们还在第2部分的<a class="ae lh" rel="noopener" target="_blank" href="/how-do-you-implement-adaboost-with-python-a76427b0fa7a">中讨论了AdaBoost(自适应增强)的Python实现。</a></p><p id="190e" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">今天我们来讨论另一个重要的boosting算法:<strong class="lk jd">梯度Boosting </strong>。它是AdaBoost的一个很好的替代方案，有时甚至超过AdaBoost。</p><p id="7cbe" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">梯度增强使用<strong class="lk jd">梯度下降算法</strong>，它试图通过梯度下降来最小化误差(残差)。</p><p id="10c9" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">就像其他提升技术一样，在梯度提升的训练过程中，通过纠正先前树的错误，每个新树都被添加到集成中。</p><p id="96e7" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">与AdaBoost相反，梯度增强中的每个新树都适合于由先前树的预测产生的<strong class="lk jd"> <em class="me">残差</em> </strong>。</p><p id="21e4" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">有一个增强版本的梯度增强，称为<strong class="lk jd"> XGBoost(极限梯度增强)</strong>，将在第4部分讨论。</p><h1 id="ef45" class="mf mg it bd mh mi mj mk ml mm mn mo mp ki mq kj mr kl ms km mt ko mu kp mv mw bi translated">残差</h1><p id="2d64" class="pw-post-body-paragraph li lj it lk b ll mx kd ln lo my kg lq lr mz lt lu lv na lx ly lz nb mb mc md im bi translated">在讨论梯度增强是如何工作的之前，我们想了解一下<em class="me">残差</em>背后的想法。</p><p id="40e0" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">数学上，模型中的残差可以定义为实际值和预测值之间的差异。</p><pre class="ks kt ku kv gt nc nd ne nf aw ng bi"><span id="fe2e" class="nh mg it nd b gy ni nj l nk nl"><strong class="nd jd">Residual = Actual value — Predicted value</strong></span></pre><p id="c8f3" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">残差可以是正数，也可以是负数。</p><h1 id="071b" class="mf mg it bd mh mi mj mk ml mm mn mo mp ki mq kj mr kl ms km mt ko mu kp mv mw bi translated">梯度增强的手动实现</h1><p id="a609" class="pw-post-body-paragraph li lj it lk b ll mx kd ln lo my kg lq lr mz lt lu lv na lx ly lz nb mb mc md im bi translated">为了理解梯度增强是如何工作的，我们需要手动实现该算法。为此，我们使用<a class="ae lh" href="https://drive.google.com/file/d/1Kees3lk-Zo7AsrYz7svcj8Hnbr6gHok6/view?usp=sharing" rel="noopener ugc nofollow" target="_blank">加州房价数据集</a>做了一个回归任务。请注意，梯度增强也适用于分类任务。</p><p id="b828" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">数据集的前几行如下所示:</p><pre class="ks kt ku kv gt nc nd ne nf aw ng bi"><span id="d252" class="nh mg it nd b gy ni nj l nk nl">import pandas as pd</span><span id="cda9" class="nh mg it nd b gy nm nj l nk nl">df = pd.read_csv('cali_housing.csv')<br/>df.head()</span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi nn"><img src="../Images/f85327a647aff978eae47eb275c1f158.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Wq0URFINkP-O-0HGYNGEaw.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">(图片由作者提供)</p></figure><p id="0f2d" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">目标列是最后一列(MedHouseVal)。因此，我们将X(特征矩阵)和y(目标列)定义如下:</p><pre class="ks kt ku kv gt nc nd ne nf aw ng bi"><span id="2696" class="nh mg it nd b gy ni nj l nk nl">X = df.drop(columns='MedHouseVal')<br/>y = df['MedHouseVal']</span></pre><p id="5ceb" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">然后，我们为X和y创建训练集和测试集</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="no np l"/></div></figure><p id="64f2" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">现在，我们准备好手动实现梯度推进。</p><p id="33cf" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><strong class="lk jd">第一步:</strong>在集成中训练初始决策树。这棵树被称为基础学习者。用<strong class="lk jd"> max_depth=1 </strong>调节。所以，这棵树被专门称为决策树桩。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="no np l"/></div></figure><p id="0c84" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><strong class="lk jd">第二步:</strong>对第一棵树进行预测。</p><pre class="ks kt ku kv gt nc nd ne nf aw ng bi"><span id="feb1" class="nh mg it nd b gy ni nj l nk nl">tree_1_pred = tree_1.predict(X_train)</span></pre><p id="4c3d" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><strong class="lk jd">第三步:</strong>计算第一棵树的预测残差。</p><pre class="ks kt ku kv gt nc nd ne nf aw ng bi"><span id="cd27" class="nh mg it nd b gy ni nj l nk nl">tree_1_residuals = y_train - tree_1_pred</span></pre><p id="c3b7" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><strong class="lk jd">第四步:</strong>在第一棵树的残差上训练第二棵树，进行预测，计算残差。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="no np l"/></div></figure><p id="476f" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">同样，我们可以根据第二棵树的残差训练第三棵树，进行预测并计算残差。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="no np l"/></div></figure><p id="08af" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">这是第三次迭代的结尾。我们继续构建新的树，直到残差接近0。整个过程可能包含由<strong class="lk jd"> n_estimators </strong>定义的数百或数千次迭代(这将在后面讨论)。</p><p id="e71e" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">第三次迭代后，我们可以计算RMSE值。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="no np l"/></div></figure><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/8426b774d0462d1009a1f26b15fe0ef0.png" data-original-src="https://miro.medium.com/v2/resize:fit:382/format:webp/1*_-wxCftnxdIuqAcmz2VM3g.png"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">(图片由作者提供)</p></figure><p id="76d8" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">y单位的近似值为0.86。我们可以通过增加迭代次数来最小化这个值。但是，我们不能手动操作。Scikit-learn提供了以下特殊类来轻松实现梯度增强。</p><ul class=""><li id="9088" class="nr ns it lk b ll lm lo lp lr nt lv nu lz nv md nw nx ny nz bi translated"><a class="ae lh" href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html" rel="noopener ugc nofollow" target="_blank"><strong class="lk jd">【GradientBoostingRegressor()</strong></a><strong class="lk jd"/>—用于回归</li><li id="4bb4" class="nr ns it lk b ll oa lo ob lr oc lv od lz oe md nw nx ny nz bi translated"><a class="ae lh" href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html" rel="noopener ugc nofollow" target="_blank"><strong class="lk jd">GradientBoostingClassifier()</strong></a>—用于分类</li></ul><h1 id="374a" class="mf mg it bd mh mi mj mk ml mm mn mo mp ki mq kj mr kl ms km mt ko mu kp mv mw bi translated">Scikit-learn梯度增强的实现</h1><p id="7352" class="pw-post-body-paragraph li lj it lk b ll mx kd ln lo my kg lq lr mz lt lu lv na lx ly lz nb mb mc md im bi translated">现在，我们使用Scikit-learn的<strong class="lk jd">GradientBoostingRegressor()</strong>类在同一数据集上训练梯度增强模型。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="no np l"/></div></figure><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi of"><img src="../Images/67def8656644a4193a2a51cafcc6dc67.png" data-original-src="https://miro.medium.com/v2/resize:fit:370/format:webp/1*G-KH4bIsTL8TAYI8ejUeeA.png"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">(图片由作者提供)</p></figure><p id="dd31" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">在使用100次迭代后，RMSE值显著降低！然而，这也取决于<strong class="lk jd">最大深度</strong>和<strong class="lk jd">学习率</strong>。让我们测量每个超参数的影响。</p><h1 id="50c7" class="mf mg it bd mh mi mj mk ml mm mn mo mp ki mq kj mr kl ms km mt ko mu kp mv mw bi translated">测量n估计量的效果</h1><p id="ec69" class="pw-post-body-paragraph li lj it lk b ll mx kd ln lo my kg lq lr mz lt lu lv na lx ly lz nb mb mc md im bi translated">在1到500的范围内，我们将测量<strong class="lk jd"> n_estimators </strong>的效果，并绘制由梯度推进模型给出的测试RMSE值。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="no np l"/></div></figure><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi og"><img src="../Images/4a41282071b5c19ef328cc3e5046ac9e.png" data-original-src="https://miro.medium.com/v2/resize:fit:812/format:webp/1*Y0ee--KvU6ZZrvhCFOtP3A.png"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">(图片由作者提供)</p></figure><p id="485c" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">随着估计器数量的增加，RMSE值逐渐减小，并在400次迭代后保持不变。最好选择300到500之间的值。如果选择更高的值，模型将需要很长时间来完成训练过程。</p><h1 id="8857" class="mf mg it bd mh mi mj mk ml mm mn mo mp ki mq kj mr kl ms km mt ko mu kp mv mw bi translated">测量max_depth的效果</h1><p id="52af" class="pw-post-body-paragraph li lj it lk b ll mx kd ln lo my kg lq lr mz lt lu lv na lx ly lz nb mb mc md im bi translated">在1到5的范围内，我们将测量<strong class="lk jd"> max_depth </strong>的效果，并绘制由梯度推进模型给出的测试RMSE值。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="no np l"/></div></figure><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/4b216bfc559aa5260386eaf9d921dc78.png" data-original-src="https://miro.medium.com/v2/resize:fit:840/format:webp/1*MSABQ5hGC-VkLAHtvpdRbg.png"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">(图片由作者提供)</p></figure><p id="e842" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><strong class="lk jd">最大深度</strong>的最佳值为2。</p><h1 id="e958" class="mf mg it bd mh mi mj mk ml mm mn mo mp ki mq kj mr kl ms km mt ko mu kp mv mw bi translated">衡量学习率的影响</h1><p id="979f" class="pw-post-body-paragraph li lj it lk b ll mx kd ln lo my kg lq lr mz lt lu lv na lx ly lz nb mb mc md im bi translated">在0.1到1.0的范围内，我们将测量<strong class="lk jd"> learning_rate </strong>的效果，并绘制梯度增强模型给出的测试RMSE值。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="no np l"/></div></figure><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi oi"><img src="../Images/e3b8114d577e4bb3d6b29500ba8d6ca7.png" data-original-src="https://miro.medium.com/v2/resize:fit:830/format:webp/1*zQIkM7IYVKbXZeHjS4WpvA.png"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">(图片由作者提供)</p></figure><p id="486a" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">最好的学习率是0.4或0.7。</p><h1 id="7f55" class="mf mg it bd mh mi mj mk ml mm mn mo mp ki mq kj mr kl ms km mt ko mu kp mv mw bi translated">使用随机搜索找到最佳超参数值</h1><p id="c839" class="pw-post-body-paragraph li lj it lk b ll mx kd ln lo my kg lq lr mz lt lu lv na lx ly lz nb mb mc md im bi translated">在这里，我们用<a class="ae lh" rel="noopener" target="_blank" href="/python-implementation-of-grid-search-and-random-search-for-hyperparameter-optimization-2d6a82ebf75c"> <strong class="lk jd">随机搜索</strong> </a> <strong class="lk jd"> </strong>一下子找到最优的超参数值。我们也可以使用网格搜索，但这需要几个小时才能完成。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="no np l"/></div></figure><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi oj"><img src="../Images/089b938fe7df841e65e54f3129ba7efb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*y2kIuA93IhdMmUxIchUC9w.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">(图片由作者提供)</p></figure><p id="15e6" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">您可以将这些超参数值与之前逐个获得的值进行比较。</p><h1 id="f2b3" class="mf mg it bd mh mi mj mk ml mm mn mo mp ki mq kj mr kl ms km mt ko mu kp mv mw bi translated">摘要</h1><p id="dc1a" class="pw-post-body-paragraph li lj it lk b ll mx kd ln lo my kg lq lr mz lt lu lv na lx ly lz nb mb mc md im bi translated">到目前为止，我们已经完成了2个boosting算法。在梯度增强中，RMSE值受<strong class="lk jd"> n_estimators </strong>、<strong class="lk jd"> max_depth </strong>和<strong class="lk jd"> learning_rate </strong>的值影响很大。</p><p id="6a2f" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">最佳超参数值将根据您拥有的数据集和许多其他因素而变化。</p><p id="a162" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">有时，梯度增强可以胜过AdaBoost。但是，梯度推进比随机森林好得多。</p><p id="1b62" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">在<a class="ae lh" href="https://rukshanpramoditha.medium.com/unlock-the-power-of-xgboost-738536b9f36f" rel="noopener">第4部分</a>中，我们将讨论<strong class="lk jd"> XGBoost(极限梯度增强)</strong>，一个梯度增强的增强版本。下一个故事再见。祝大家学习愉快！</p></div><div class="ab cl ok ol hx om" role="separator"><span class="on bw bk oo op oq"/><span class="on bw bk oo op oq"/><span class="on bw bk oo op"/></div><div class="im in io ip iq"><p id="0b0b" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我的读者可以通过下面的链接注册成为会员，以获得我写的每个故事的全部信息，我将收到你的一部分会员费。</p><div class="or os gp gr ot ou"><a href="https://rukshanpramoditha.medium.com/membership" rel="noopener follow" target="_blank"><div class="ov ab fo"><div class="ow ab ox cl cj oy"><h2 class="bd jd gy z fp oz fr fs pa fu fw jc bi translated">通过我的推荐链接加入Medium</h2><div class="pb l"><h3 class="bd b gy z fp oz fr fs pa fu fw dk translated">作为一个媒体会员，你的会员费的一部分会给你阅读的作家，你可以完全接触到每一个故事…</h3></div><div class="pc l"><p class="bd b dl z fp oz fr fs pa fu fw dk translated">rukshanpramoditha.medium.com</p></div></div><div class="pd l"><div class="pe l pf pg ph pd pi lb ou"/></div></div></a></div><p id="4d1f" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">非常感谢你一直以来的支持！</p><p id="0873" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">特别感谢Unsplash上的<strong class="lk jd"> Donald Giannatti </strong>，<strong class="lk jd"> </strong>为我提供了这篇文章的封面图片。</p><p id="458d" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><a class="pj pk ep" href="https://medium.com/u/f90a3bb1d400?source=post_page-----99cc63efd24d--------------------------------" rel="noopener" target="_blank">鲁克山普拉莫迪塔</a><br/><strong class="lk jd">2021–10–24</strong></p></div></div>    
</body>
</html>