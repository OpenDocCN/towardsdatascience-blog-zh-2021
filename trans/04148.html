<html>
<head>
<title>Logistic Regression From Scratch in Python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Python中从头开始的逻辑回归</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/logistic-regression-from-scratch-in-python-ec66603592e2?source=collection_archive---------0-----------------------#2021-04-08">https://towardsdatascience.com/logistic-regression-from-scratch-in-python-ec66603592e2?source=collection_archive---------0-----------------------#2021-04-08</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="e053" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">从零开始的机器学习:第5部分</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/a45556c06d1f52f4563787f66198aab9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eMimR6WxLvcctZsvQ8UPIQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><p id="96da" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在本文中，我们将实现最常用的分类算法，称为逻辑回归。首先，我们将理解<strong class="la iu"> Sigmoid </strong>函数、<strong class="la iu">假设</strong>函数、<strong class="la iu">决策边界</strong>、<strong class="la iu">测井损失函数</strong>并对它们进行编码。</p><p id="1257" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">之后，我们将应用<strong class="la iu">梯度下降</strong>算法来寻找参数<code class="fe lu lv lw lx b">weights</code>和<code class="fe lu lv lw lx b">bias</code>。最后，我们将测量<strong class="la iu">准确度</strong>和<strong class="la iu">绘制线性可分离数据集和非线性可分离数据集的决策边界</strong>。</p><p id="7d6f" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们将使用Python NumPy和Matplotlib来实现这一切。</p><div class="ly lz gp gr ma mb"><a rel="noopener follow" target="_blank" href="/polynomial-regression-in-python-b69ab7df6105"><div class="mc ab fo"><div class="md ab me cl cj mf"><h2 class="bd iu gy z fp mg fr fs mh fu fw is bi translated">用Python从头开始实现多项式回归</h2><div class="mi l"><h3 class="bd b gy z fp mg fr fs mh fu fw dk translated">从零开始的机器学习:第4部分</h3></div><div class="mj l"><p class="bd b dl z fp mg fr fs mh fu fw dk translated">towardsdatascience.com</p></div></div><div class="mk l"><div class="ml l mm mn mo mk mp ks mb"/></div></div></a></div></div><div class="ab cl mq mr hx ms" role="separator"><span class="mt bw bk mu mv mw"/><span class="mt bw bk mu mv mw"/><span class="mt bw bk mu mv"/></div><div class="im in io ip iq"><h1 id="44d0" class="mx my it bd mz na nb nc nd ne nf ng nh jz ni ka nj kc nk kd nl kf nm kg nn no bi translated">符号—</h1><ul class=""><li id="e9ae" class="np nq it la b lb nr le ns lh nt ll nu lp nv lt nw nx ny nz bi translated"><code class="fe lu lv lw lx b">n</code>→特征数量</li><li id="11a5" class="np nq it la b lb oa le ob lh oc ll od lp oe lt nw nx ny nz bi translated"><code class="fe lu lv lw lx b">m</code>→训练实例数量</li><li id="eeb6" class="np nq it la b lb oa le ob lh oc ll od lp oe lt nw nx ny nz bi translated"><code class="fe lu lv lw lx b">X</code>→形状输入数据矩阵(<code class="fe lu lv lw lx b">m</code> x <code class="fe lu lv lw lx b">n</code>)</li><li id="daf5" class="np nq it la b lb oa le ob lh oc ll od lp oe lt nw nx ny nz bi translated"><code class="fe lu lv lw lx b">y</code>→真/目标值(<strong class="la iu">只能是0或1</strong>)</li><li id="df30" class="np nq it la b lb oa le ob lh oc ll od lp oe lt nw nx ny nz bi translated"><code class="fe lu lv lw lx b">x(i), y(i)</code>→带培训示例</li><li id="80c5" class="np nq it la b lb oa le ob lh oc ll od lp oe lt nw nx ny nz bi translated"><code class="fe lu lv lw lx b">w</code> →形状的权重(参数)(<code class="fe lu lv lw lx b">n</code> x 1)</li><li id="34e1" class="np nq it la b lb oa le ob lh oc ll od lp oe lt nw nx ny nz bi translated"><code class="fe lu lv lw lx b">b</code> →bias(参数)，一个可以<a class="ae of" href="https://numpy.org/doc/stable/user/basics.broadcasting.html" rel="noopener ugc nofollow" target="_blank">广播的实数</a>。</li><li id="aedb" class="np nq it la b lb oa le ob lh oc ll od lp oe lt nw nx ny nz bi translated"><code class="fe lu lv lw lx b">y_hat</code>(带帽子的y)→假设(<strong class="la iu">输出0和1 </strong>之间的值)</li></ul></div><div class="ab cl mq mr hx ms" role="separator"><span class="mt bw bk mu mv mw"/><span class="mt bw bk mu mv mw"/><span class="mt bw bk mu mv"/></div><div class="im in io ip iq"><p id="7975" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们将进行<strong class="la iu">二进制分类</strong>，因此<code class="fe lu lv lw lx b">y</code>(真/目标)的值将为0或1。</p><p id="b9da" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">例如，假设我们有一个乳腺癌数据集，其中<code class="fe lu lv lw lx b">X</code>是肿瘤大小，<code class="fe lu lv lw lx b">y</code>是肿块是恶性(癌变)还是良性(非癌变)。每当病人来访时，你的工作就是告诉他/她根据肿瘤的大小，肿块是恶性的还是良性的。这种情况下只有两个类。</p><p id="c8eb" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">所以，<code class="fe lu lv lw lx b">y</code>要么是0，要么是1。</p><h1 id="3385" class="mx my it bd mz na og nc nd ne oh ng nh jz oi ka nj kc oj kd nl kf ok kg nn no bi translated">逻辑回归</h1><p id="5a17" class="pw-post-body-paragraph ky kz it la b lb nr ju ld le ns jx lg lh ol lj lk ll om ln lo lp on lr ls lt im bi translated">让我们使用以下随机生成的数据作为一个激励性的例子来理解逻辑回归。</p><pre class="kj kk kl km gt oo lx op oq aw or bi"><span id="aaa2" class="os my it lx b gy ot ou l ov ow"><strong class="lx iu">from sklearn.datasets import make_classification</strong></span><span id="4cbb" class="os my it lx b gy ox ou l ov ow"><strong class="lx iu">X, y = make_classification(n_features=2, n_redundant=0, <br/>                           n_informative=2, random_state=1, <br/>                           n_clusters_per_class=1)</strong></span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oy"><img src="../Images/e97d0412e8fada27960337dc61156a6f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IN4WEJKa8OqJ51lCi1SvFQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><p id="7c73" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">有2个特性，<code class="fe lu lv lw lx b">n</code> =2。有两类，蓝色和绿色。</p><p id="4c34" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">对于二进制分类问题，我们自然希望我们的假设(<code class="fe lu lv lw lx b">y_hat</code>)函数输出0到1之间的值，这意味着从0到1的所有实数。</p><p id="69d8" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">所以，我们想选择一个函数，它把所有的输入压缩在0和1之间。一个这样的函数是Sigmoid或逻辑函数。</p><h1 id="53cb" class="mx my it bd mz na og nc nd ne oh ng nh jz oi ka nj kc oj kd nl kf ok kg nn no bi translated"><strong class="ak">乙状结肠或逻辑函数</strong></h1><p id="d1de" class="pw-post-body-paragraph ky kz it la b lb nr ju ld le ns jx lg lh ol lj lk ll om ln lo lp on lr ls lt im bi translated">正如我们在下图的y轴上看到的，Sigmoid函数将其所有输入(x轴上的值)压缩在0和1之间。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oz"><img src="../Images/e0c18247b05dc65977787c52081d0620.png" data-original-src="https://miro.medium.com/v2/resize:fit:532/format:webp/1*VQ7Sg5-9G0BrMDLR0GJgGA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">资料来源:吴恩达</p></figure><p id="190d" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">该函数的输入范围是所有实数的集合，输出范围在0和1之间。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pa"><img src="../Images/0c28f71eac1cfb0204498ad9ebe4e664.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*O1Yl4WplwlvVVUR_.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">乙状结肠函数；来源:<a class="ae of" href="https://www.google.com/url?sa=i&amp;url=https%3A%2F%2Fen.wikipedia.org%2Fwiki%2FSigmoid_function&amp;psig=AOvVaw2nq5uAehtbrwbJxkVG5Klg&amp;ust=1617881288361000&amp;source=images&amp;cd=vfe&amp;ved=0CAIQjRxqFwoTCNjO282D7O8CFQAAAAAdAAAAABAD" rel="noopener ugc nofollow" target="_blank">维基百科</a></p></figure><p id="6b5a" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们可以看到，随着<code class="fe lu lv lw lx b">z</code>向正无穷大增加，输出越来越接近1，随着<code class="fe lu lv lw lx b">z</code>向负无穷大减少，输出越来越接近0。</p><pre class="kj kk kl km gt oo lx op oq aw or bi"><span id="0c48" class="os my it lx b gy ot ou l ov ow"><strong class="lx iu">def sigmoid(z):<br/>    return 1.0/(1 + np.exp(-z))</strong></span></pre><h1 id="2062" class="mx my it bd mz na og nc nd ne oh ng nh jz oi ka nj kc oj kd nl kf ok kg nn no bi translated">假设</h1><p id="3bf4" class="pw-post-body-paragraph ky kz it la b lb nr ju ld le ns jx lg lh ol lj lk ll om ln lo lp on lr ls lt im bi translated">对于线性回归，我们有一个假设<code class="fe lu lv lw lx b">y_hat = w.X +b</code>，其输出范围是所有实数的集合。</p><p id="036c" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">现在，对于逻辑回归，我们的假设是——<code class="fe lu lv lw lx b">y_hat = sigmoid(w.X + b)</code>，其输出范围在0和1之间，因为通过应用sigmoid函数，我们总是输出0和1之间的数字。</p><p id="2d66" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><code class="fe lu lv lw lx b">y_hat</code> =</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pb"><img src="../Images/34a510a12dbeecf903a053ec035adbdc.png" data-original-src="https://miro.medium.com/v2/resize:fit:412/format:webp/1*eH9JgxMBUl-6D8MtixckkA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">逻辑回归假设；<a class="ae of" href="https://www.google.com/url?sa=i&amp;url=https%3A%2F%2Fneuralrepo.net%2F2019%2F07%2F07%2Fsigmoid-neuron-logistic-regression-from-scratch%2F&amp;psig=AOvVaw26nBIbMpK8ZeFZVkA0ndO9&amp;ust=1617919651464000&amp;source=images&amp;cd=vfe&amp;ved=0CAIQjRxqFwoTCIjhk8eS7e8CFQAAAAAdAAAAABAD" rel="noopener ugc nofollow" target="_blank">来源</a></p></figure><blockquote class="pc pd pe"><p id="4232" class="ky kz pf la b lb lc ju ld le lf jx lg pg li lj lk ph lm ln lo pi lq lr ls lt im bi translated">z = w.X +b</p></blockquote><p id="3931" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">现在，你可能会奇怪，有很多连续函数输出0到1之间的值。为什么我们只选择物流功能，而不选择其他功能？实际上，有一类更广泛的算法叫做广义线性模型，这是它的一个特例。给定我们的一组假设，Sigmoid函数会很自然地从中脱离出来。</p></div><div class="ab cl mq mr hx ms" role="separator"><span class="mt bw bk mu mv mw"/><span class="mt bw bk mu mv mw"/><span class="mt bw bk mu mv"/></div><div class="im in io ip iq"><h1 id="3794" class="mx my it bd mz na nb nc nd ne nf ng nh jz ni ka nj kc nk kd nl kf nm kg nn no bi translated">损失/成本函数</h1><p id="b410" class="pw-post-body-paragraph ky kz it la b lb nr ju ld le ns jx lg lh ol lj lk ll om ln lo lp on lr ls lt im bi translated">对于每个参数化机器学习算法，我们需要一个损失函数，我们希望最小化该函数(找到其全局最小值)，以确定最佳参数(<code class="fe lu lv lw lx b">w</code>和<code class="fe lu lv lw lx b">b</code>)，这将帮助我们做出最佳预测。</p><p id="00b8" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">对于线性回归，我们用均方差作为损失函数。但这是一个回归问题。</p><p id="cc2a" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">对于一个二元分类问题，我们需要能够输出<code class="fe lu lv lw lx b">y</code>是<strong class="la iu"> 1 </strong>的概率(比如肿瘤是良性的)，然后我们就可以确定<code class="fe lu lv lw lx b">y</code>是<strong class="la iu"> 0 </strong>(肿瘤是恶性的)<strong class="la iu"> </strong>的概率或者反之。</p><p id="7982" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">所以，我们假设我们的假设(<code class="fe lu lv lw lx b">y_hat</code>)输出的值在0和1之间，是<code class="fe lu lv lw lx b">y</code>为1的概率，那么<code class="fe lu lv lw lx b">y</code>为0的概率将是<code class="fe lu lv lw lx b">(1-y_hat)</code>。</p><blockquote class="pc pd pe"><p id="7079" class="ky kz pf la b lb lc ju ld le lf jx lg pg li lj lk ph lm ln lo pi lq lr ls lt im bi translated">记住<code class="fe lu lv lw lx b">y</code>只是0或者1。<code class="fe lu lv lw lx b">y_hat</code>是介于0和1之间的数字。</p></blockquote><p id="02eb" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">更正式的说法是，给定<code class="fe lu lv lw lx b">X</code>，用<code class="fe lu lv lw lx b">w</code>和<code class="fe lu lv lw lx b">b</code>参数化的y=1的概率是<code class="fe lu lv lw lx b">y_hat</code>(假设)。那么，逻辑上给定<code class="fe lu lv lw lx b">X</code>，用<code class="fe lu lv lw lx b">w</code>和<code class="fe lu lv lw lx b">b</code>参数化的y=0的概率应该是<code class="fe lu lv lw lx b">1-y_hat</code>。这可以写成—</p><blockquote class="pj"><p id="21b3" class="pk pl it bd pm pn po pp pq pr ps lt dk translated"><code class="fe lu lv lw lx b">P(y = 1 | X; w, b) = </code> y_hat</p><p id="7c2b" class="pk pl it bd pm pn po pp pq pr ps lt dk translated"><code class="fe lu lv lw lx b">P(y = 0 | X; w, b) = (1-y_hat)</code></p></blockquote><p id="040e" class="pw-post-body-paragraph ky kz it la b lb pt ju ld le pu jx lg lh pv lj lk ll pw ln lo lp px lr ls lt im bi translated">然后，基于我们的假设，我们可以使用上述两个方程计算参数的对数似然，从而确定我们必须最小化的损失函数。以下是二元Coss-熵损失或对数损失函数—</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi py"><img src="../Images/d238abd4e147ea2e8d27b4b94a3e62d9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*yNTgEBnoJzF-dWez.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">二元交叉熵损失函数；资料来源:吴恩达</p></figure><p id="119e" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">供参考— <a class="ae of" href="https://stats.stackexchange.com/questions/304988/understanding-the-logistic-regression-and-likelihood" rel="noopener ugc nofollow" target="_blank">了解逻辑回归和可能性</a></p><p id="a479" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><code class="fe lu lv lw lx b">J(w,b)</code>是训练集的总成本/损失，而<code class="fe lu lv lw lx b">L</code>是第I个训练示例的成本。</p><pre class="kj kk kl km gt oo lx op oq aw or bi"><span id="3019" class="os my it lx b gy ot ou l ov ow"><strong class="lx iu">def loss(y, y_hat):<br/>    loss = -np.mean(y*(np.log(y_hat)) - (1-y)*np.log(1-y_hat))<br/>    return loss</strong></span></pre><p id="aadf" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">通过查看损失函数，我们可以看到，当我们正确预测时，即当y=0且y_hat=0或y=1且y_hat=1时，损失函数接近<strong class="la iu"> 0 </strong>，而如果我们预测错误，即当y=0但y_hat=1或y=1但y_hat=1时，损失函数接近<strong class="la iu">无穷大</strong>。</p><h1 id="2f6a" class="mx my it bd mz na og nc nd ne oh ng nh jz oi ka nj kc oj kd nl kf ok kg nn no bi translated">梯度下降</h1><p id="21d2" class="pw-post-body-paragraph ky kz it la b lb nr ju ld le ns jx lg lh ol lj lk ll om ln lo lp on lr ls lt im bi translated">现在我们知道了假设函数和损失函数，我们需要做的就是使用梯度下降算法找到我们参数的最优值，就像这样(<code class="fe lu lv lw lx b">lr</code>→学习速率)—</p><blockquote class="pj"><p id="2b56" class="pk pl it bd pm pn po pp pq pr ps lt dk translated">w := w-lr*dw</p><p id="cfd1" class="pk pl it bd pm pn po pp pq pr ps lt dk translated">b := b-lr*db</p></blockquote><p id="2659" class="pw-post-body-paragraph ky kz it la b lb pt ju ld le pu jx lg lh pv lj lk ll pw ln lo lp px lr ls lt im bi translated">其中，<code class="fe lu lv lw lx b">dw</code>是损失函数相对于<code class="fe lu lv lw lx b">w</code>的偏导数，<code class="fe lu lv lw lx b">db</code>是损失函数相对于<code class="fe lu lv lw lx b">b</code>的偏导数。</p><blockquote class="pc pd pe"><p id="49ed" class="ky kz pf la b lb lc ju ld le lf jx lg pg li lj lk ph lm ln lo pi lq lr ls lt im bi translated">dw = (1/m)*(y_hat — y)。X</p><p id="e366" class="ky kz pf la b lb lc ju ld le lf jx lg pg li lj lk ph lm ln lo pi lq lr ls lt im bi translated">db = (1/m)*(y_hat — y)</p></blockquote><p id="6848" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">让我们写一个函数<code class="fe lu lv lw lx b">gradients</code>来计算<code class="fe lu lv lw lx b">dw</code>和<code class="fe lu lv lw lx b">db</code>。</p><p id="b2df" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">请参见注释(#)。</p><pre class="kj kk kl km gt oo lx op oq aw or bi"><span id="d0b6" class="os my it lx b gy ot ou l ov ow"><strong class="lx iu">def gradients(X, y, y_hat):</strong><br/>    <br/>    # X --&gt; Input.<br/>    # y --&gt; true/target value.<br/>    # y_hat --&gt; hypothesis/predictions.<br/>    # w --&gt; weights (parameter).<br/>    # b --&gt; bias (parameter).<br/>    <br/>    # m-&gt; number of training examples.<br/>    <strong class="lx iu">m = X.shape[0]</strong><br/>    <br/>    # Gradient of loss w.r.t weights.<br/>    <strong class="lx iu">dw = (1/m)*np.dot(X.T, (y_hat - y))</strong><br/>    <br/>    # Gradient of loss w.r.t bias.<br/>    <strong class="lx iu">db = (1/m)*np.sum((y_hat - y)) <br/>    <br/>    return dw, db</strong></span></pre></div><div class="ab cl mq mr hx ms" role="separator"><span class="mt bw bk mu mv mw"/><span class="mt bw bk mu mv mw"/><span class="mt bw bk mu mv"/></div><div class="im in io ip iq"><h1 id="aac2" class="mx my it bd mz na nb nc nd ne nf ng nh jz ni ka nj kc nk kd nl kf nm kg nn no bi translated">判别边界</h1><p id="2b54" class="pw-post-body-paragraph ky kz it la b lb nr ju ld le ns jx lg lh ol lj lk ll om ln lo lp on lr ls lt im bi translated">现在，我们想知道我们的假设(<code class="fe lu lv lw lx b">y_hat</code>)将如何预测y=1还是y=0。我们定义假设的方式是给定<code class="fe lu lv lw lx b">X</code>并且由<code class="fe lu lv lw lx b">w</code>和<code class="fe lu lv lw lx b">b</code>参数化的<code class="fe lu lv lw lx b">y</code>为1的概率。</p><p id="f6d4" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">所以，我们会说它会预测—</p><blockquote class="pj"><p id="7843" class="pk pl it bd pm pn po pp pq pr ps lt dk translated">当<code class="fe lu lv lw lx b">y_hat ≥ 0.5</code>时y=1</p><p id="1c50" class="pk pl it bd pm pn po pp pq pr ps lt dk translated"><code class="fe lu lv lw lx b">y_hat &lt; 0.5</code>时y=0</p></blockquote><p id="e518" class="pw-post-body-paragraph ky kz it la b lb pt ju ld le pu jx lg lh pv lj lk ll pw ln lo lp px lr ls lt im bi translated">查看sigmoid函数的图表，我们看到——</p><blockquote class="pj"><p id="af24" class="pk pl it bd pm pn po pp pq pr ps lt dk translated">y_hat ≥ 0.5，<code class="fe lu lv lw lx b">z</code>或w.X + b ≥ 0</p><p id="c059" class="pk pl it bd pm pn po pp pq pr ps lt dk translated">y_hat &lt; 0.5，z或w.X + b &lt; 0</p></blockquote><p id="9fd7" class="pw-post-body-paragraph ky kz it la b lb pt ju ld le pu jx lg lh pv lj lk ll pw ln lo lp px lr ls lt im bi translated">也就是说，我们预测—</p><blockquote class="pj"><p id="7ddb" class="pk pl it bd pm pn po pp pq pr ps lt dk translated">当w.X + b ≥ 0时，y=1</p><p id="44f2" class="pk pl it bd pm pn po pp pq pr ps lt dk translated">当w.X + b &lt; 0时，y=0</p></blockquote><p id="c9c9" class="pw-post-body-paragraph ky kz it la b lb pt ju ld le pu jx lg lh pv lj lk ll pw ln lo lp px lr ls lt im bi translated">所以，<code class="fe lu lv lw lx b"><strong class="la iu">w.X + b = 0</strong></code>将是我们的决策边界。</p><blockquote class="pc pd pe"><p id="bf56" class="ky kz pf la b lb lc ju ld le lf jx lg pg li lj lk ph lm ln lo pi lq lr ls lt im bi translated">以下用于绘制决策边界的代码仅在<code class="fe lu lv lw lx b">X</code>中只有两个特征时有效。</p></blockquote><pre class="kj kk kl km gt oo lx op oq aw or bi"><span id="4e96" class="os my it lx b gy ot ou l ov ow"><strong class="lx iu">def plot_decision_boundary(X, w, b):</strong><br/>    <br/>    # X --&gt; Inputs<br/>    # w --&gt; weights<br/>    # b --&gt; bias<br/>    <br/>    # The Line is y=mx+c<br/>    # So, Equate mx+c = w.X + b<br/>    # Solving we find m and c<br/>    <strong class="lx iu">x1 = [min(X[:,0]), max(X[:,0])]<br/>    m = -w[0]/w[1]<br/>    c = -b/w[1]<br/>    x2 = m*x1 + c</strong><br/>    <br/>    # Plotting<br/>    <strong class="lx iu">fig = plt.figure(figsize=(10,8))<br/>    plt.plot(X[:, 0][y==0], X[:, 1][y==0], "g^")<br/>    plt.plot(X[:, 0][y==1], X[:, 1][y==1], "bs")<br/>    plt.xlim([-2, 2])<br/>    plt.ylim([0, 2.2])<br/>    plt.xlabel("feature 1")<br/>    plt.ylabel("feature 2")<br/>    plt.title('Decision Boundary')</strong></span><span id="261f" class="os my it lx b gy ox ou l ov ow">    <strong class="lx iu">plt.plot(x1, x2, 'y-')</strong></span></pre><h1 id="cde4" class="mx my it bd mz na og nc nd ne oh ng nh jz oi ka nj kc oj kd nl kf ok kg nn no bi translated">归一化函数</h1><p id="1e9b" class="pw-post-body-paragraph ky kz it la b lb nr ju ld le ns jx lg lh ol lj lk ll om ln lo lp on lr ls lt im bi translated">函数来规范化输入。请参见注释(#)。</p><pre class="kj kk kl km gt oo lx op oq aw or bi"><span id="385e" class="os my it lx b gy ot ou l ov ow"><strong class="lx iu">def normalize(X):</strong><br/>    <br/>    # X --&gt; Input.<br/>    <br/>    # m-&gt; number of training examples<br/>    # n-&gt; number of features <br/>    <strong class="lx iu">m, n = X.shape</strong><br/>    <br/>    # Normalizing all the n features of X.<br/>    <strong class="lx iu">for i in range(n):<br/>        X = (X - X.mean(axis=0))/X.std(axis=0)</strong><br/>        <br/> <strong class="lx iu">   return X</strong></span></pre><h1 id="b824" class="mx my it bd mz na og nc nd ne oh ng nh jz oi ka nj kc oj kd nl kf ok kg nn no bi translated">训练功能</h1><p id="6118" class="pw-post-body-paragraph ky kz it la b lb nr ju ld le ns jx lg lh ol lj lk ll om ln lo lp on lr ls lt im bi translated"><code class="fe lu lv lw lx b">train</code>该功能包括初始化权重和偏差以及带有小批量梯度下降的训练循环。</p><p id="2641" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">请参见注释(#)。</p><pre class="kj kk kl km gt oo lx op oq aw or bi"><span id="436b" class="os my it lx b gy ot ou l ov ow"><strong class="lx iu">def train(X, y, bs, epochs, lr):</strong><br/>    <br/>    # X --&gt; Input.<br/>    # y --&gt; true/target value.<br/>    # bs --&gt; Batch Size.<br/>    # epochs --&gt; Number of iterations.<br/>    # lr --&gt; Learning rate.<br/>        <br/>    # m-&gt; number of training examples<br/>    # n-&gt; number of features <br/>    <strong class="lx iu">m, n = X.shape</strong><br/>    <br/>    # Initializing weights and bias to zeros.<br/>    <strong class="lx iu">w = np.zeros((n,1))<br/>    b = 0</strong><br/>    <br/>    # Reshaping y.<br/>    <strong class="lx iu">y = y.reshape(m,1)</strong><br/>    <br/>    # Normalizing the inputs.<br/>    <strong class="lx iu">x = normalize(X)</strong><br/>    <br/>    # Empty list to store losses.<br/>    <strong class="lx iu">losses = []</strong><br/>    <br/>    # Training loop.<br/>   <strong class="lx iu"> for epoch in range(epochs):<br/>        for i in range((m-1)//bs + 1):</strong><br/>            <br/>            # Defining batches. SGD.<br/>            <strong class="lx iu">start_i = i*bs<br/>            end_i = start_i + bs<br/>            xb = X[start_i:end_i]<br/>            yb = y[start_i:end_i]</strong><br/>            <br/>            # Calculating hypothesis/prediction.<br/><strong class="lx iu">            y_hat = sigmoid(np.dot(xb, w) + b)<br/></strong>            <br/>            # Getting the gradients of loss w.r.t parameters.<br/><strong class="lx iu">            dw, db = gradients(xb, yb, y_hat)<br/></strong>            <br/>            # Updating the parameters.<br/>            <strong class="lx iu">w -= lr*dw<br/>            b -= lr*db</strong><br/>        <br/>        # Calculating loss and appending it in the list.<br/>        <strong class="lx iu">l = loss(y, sigmoid(np.dot(X, w) + b))<br/>        losses.append(l)</strong><br/>        <br/>    # returning weights, bias and losses(List).<br/>  <strong class="lx iu">  return w, b, losses</strong></span></pre><h1 id="747e" class="mx my it bd mz na og nc nd ne oh ng nh jz oi ka nj kc oj kd nl kf ok kg nn no bi translated">预测功能</h1><p id="a722" class="pw-post-body-paragraph ky kz it la b lb nr ju ld le ns jx lg lh ol lj lk ll om ln lo lp on lr ls lt im bi translated">请参见注释(#)。</p><pre class="kj kk kl km gt oo lx op oq aw or bi"><span id="0c7b" class="os my it lx b gy ot ou l ov ow"><strong class="lx iu">def predict(X):</strong><br/>    <br/>    # X --&gt; Input.<br/>    <br/>    # Normalizing the inputs.<br/>   <strong class="lx iu"> x = normalize(X)</strong><br/>    <br/>    # Calculating presictions/y_hat.<br/><strong class="lx iu">    preds = sigmoid(np.dot(X, w) + b)<br/></strong>    <br/>    # Empty List to store predictions.<br/><strong class="lx iu">    pred_class = []</strong></span><span id="7762" class="os my it lx b gy ox ou l ov ow">    # if y_hat &gt;= 0.5 --&gt; round up to 1<br/>    # if y_hat &lt; 0.5 --&gt; round up to 1<br/><strong class="lx iu">    pred_class = [1 if i &gt; 0.5 else 0 for i in preds]<br/></strong>    <br/><strong class="lx iu">    return np.array(pred_class)</strong></span></pre><h1 id="5fd7" class="mx my it bd mz na og nc nd ne oh ng nh jz oi ka nj kc oj kd nl kf ok kg nn no bi translated">训练和绘制决策边界</h1><pre class="kj kk kl km gt oo lx op oq aw or bi"><span id="bdd5" class="os my it lx b gy ot ou l ov ow"># Training <br/><strong class="lx iu">w, b, l = train(X, y, bs=100, epochs=1000, lr=0.01)</strong></span><span id="a8e0" class="os my it lx b gy ox ou l ov ow"># Plotting Decision Boundary<br/><strong class="lx iu">plot_decision_boundary(X, w, b)</strong></span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oy"><img src="../Images/d3686a2a84eec41f9f028c3b1d4a472a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YCLhUNKkq9jBtAKcwKBoDw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><h1 id="bb6e" class="mx my it bd mz na og nc nd ne oh ng nh jz oi ka nj kc oj kd nl kf ok kg nn no bi translated">计算精度</h1><p id="8061" class="pw-post-body-paragraph ky kz it la b lb nr ju ld le ns jx lg lh ol lj lk ll om ln lo lp on lr ls lt im bi translated">我们检查有多少例子是正确的，然后除以例子的总数。</p><pre class="kj kk kl km gt oo lx op oq aw or bi"><span id="e986" class="os my it lx b gy ot ou l ov ow"><strong class="lx iu">def accuracy(y, y_hat):<br/>    accuracy = np.sum(y == y_hat) / len(y)<br/>    return accuracy</strong></span><span id="cfc9" class="os my it lx b gy ox ou l ov ow"><strong class="lx iu">accuracy(X, y_hat=predict(X))</strong><br/>&gt;&gt; 1.0</span></pre><p id="0243" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们得到了100%的准确率。我们可以从上面的决策边界图中看到，我们能够完美地分离绿色和蓝色类。</p></div><div class="ab cl mq mr hx ms" role="separator"><span class="mt bw bk mu mv mw"/><span class="mt bw bk mu mv mw"/><span class="mt bw bk mu mv"/></div><div class="im in io ip iq"><h1 id="49e5" class="mx my it bd mz na nb nc nd ne nf ng nh jz ni ka nj kc nk kd nl kf nm kg nn no bi translated">非线性可分数据的检验</h1><p id="06d5" class="pw-post-body-paragraph ky kz it la b lb nr ju ld le ns jx lg lh ol lj lk ll om ln lo lp on lr ls lt im bi translated">让我们测试一下代码中不可线性分离的数据。</p><pre class="kj kk kl km gt oo lx op oq aw or bi"><span id="d6f2" class="os my it lx b gy ot ou l ov ow"><strong class="lx iu">from sklearn.datasets import make_moons</strong></span><span id="dbe6" class="os my it lx b gy ox ou l ov ow"><strong class="lx iu">X, y = make_moons(n_samples=100, noise=0.24)</strong></span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oy"><img src="../Images/d0cbd3d3c247ad811a7a07578b3135d7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dwe0mnOPHwbuw_sA7fV3sg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><pre class="kj kk kl km gt oo lx op oq aw or bi"><span id="a17e" class="os my it lx b gy ot ou l ov ow"># Training <br/><strong class="lx iu">w, b, l = train(X, y, bs=100, epochs=1000, lr=0.01)</strong></span><span id="de43" class="os my it lx b gy ox ou l ov ow"># Plotting Decision Boundary<br/><strong class="lx iu">plot_decision_boundary(X, w, b)</strong></span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oy"><img src="../Images/5feb8a0e1c0e524ef77b48ed193a328b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2l2d32mvg2Pk-1tEK1SnRQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><p id="57a1" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">由于逻辑回归只是一个线性分类器，我们能够画出一条像样的直线，能够尽可能多地将蓝色和绿色彼此分开。</p><p id="cd4d" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">让我们检查一下这个的准确性—</p><pre class="kj kk kl km gt oo lx op oq aw or bi"><span id="0312" class="os my it lx b gy ot ou l ov ow"><strong class="lx iu">accuracy(y, predict(X))</strong><br/>&gt;&gt; 0.87</span></pre><p id="4ead" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">87 %的准确率。还不错。</p></div><div class="ab cl mq mr hx ms" role="separator"><span class="mt bw bk mu mv mw"/><span class="mt bw bk mu mv mw"/><span class="mt bw bk mu mv"/></div><div class="im in io ip iq"><h1 id="c6eb" class="mx my it bd mz na nb nc nd ne nf ng nh jz ni ka nj kc nk kd nl kf nm kg nn no bi translated">重要见解</h1><p id="bceb" class="pw-post-body-paragraph ky kz it la b lb nr ju ld le ns jx lg lh ol lj lk ll om ln lo lp on lr ls lt im bi translated">当我使用我的代码训练数据时，我总是在损失列表中得到NaN值。</p><p id="e2a5" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">后来我发现我没有把我的输入正常化，这就是我的损失充满了失败的原因。</p><p id="7f32" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">如果你在训练中得到NaN值或溢出——</p><ul class=""><li id="afe2" class="np nq it la b lb lc le lf lh pz ll qa lp qb lt nw nx ny nz bi translated">规范化您的数据— <code class="fe lu lv lw lx b">X</code>。</li><li id="b473" class="np nq it la b lb oa le ob lh oc ll od lp oe lt nw nx ny nz bi translated">降低你的学习速度。</li></ul></div><div class="ab cl mq mr hx ms" role="separator"><span class="mt bw bk mu mv mw"/><span class="mt bw bk mu mv mw"/><span class="mt bw bk mu mv"/></div><div class="im in io ip iq"><p id="3652" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">感谢阅读。对于问题、评论、顾虑，请在回复部分进行讨论。更多的ML从零开始即将推出。</p><p id="6518" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu">看看从零开始学习的机器系列— </strong></p><ul class=""><li id="5195" class="np nq it la b lb lc le lf lh pz ll qa lp qb lt nw nx ny nz bi translated">第一部分:<a class="ae of" href="https://medium.com/analytics-vidhya/linear-regression-from-scratch-in-python-b6501f91c82d?source=your_stories_page-------------------------------------" rel="noopener"><strong class="la iu">Python中从零开始的线性回归</strong> </a></li><li id="72ee" class="np nq it la b lb oa le ob lh oc ll od lp oe lt nw nx ny nz bi translated">第二部分:<a class="ae of" rel="noopener" target="_blank" href="/locally-weighted-linear-regression-in-python-3d324108efbf?source=your_stories_page-------------------------------------"><strong class="la iu">Python中的局部加权线性回归</strong> </a></li><li id="83dc" class="np nq it la b lb oa le ob lh oc ll od lp oe lt nw nx ny nz bi translated">第三部分:<a class="ae of" rel="noopener" target="_blank" href="/normal-equation-in-python-the-closed-form-solution-for-linear-regression-13df33f9ad71?source=your_stories_page-------------------------------------"> <strong class="la iu">使用Python的正规方程:线性回归的封闭解</strong> </a></li><li id="cce1" class="np nq it la b lb oa le ob lh oc ll od lp oe lt nw nx ny nz bi translated">第四部分:<a class="ae of" rel="noopener" target="_blank" href="/polynomial-regression-in-python-b69ab7df6105"><strong class="la iu">Python中从零开始的多项式回归</strong> </a></li></ul></div></div>    
</body>
</html>