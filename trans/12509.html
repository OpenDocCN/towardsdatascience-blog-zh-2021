<html>
<head>
<title>Fisher’s Linear Discriminant: Intuitively Explained</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">费雪线性判别式:直观解释</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/fishers-linear-discriminant-intuitively-explained-52a1ba79e1bb?source=collection_archive---------0-----------------------#2021-12-22">https://towardsdatascience.com/fishers-linear-discriminant-intuitively-explained-52a1ba79e1bb?source=collection_archive---------0-----------------------#2021-12-22</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="054f" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph">机器学习基础</h2><div class=""/><div class=""><h2 id="e706" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">线性判别分析的基础 Fisher 线性判别的直观解释</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/89742acb7922b1b97ca0fb5002bc9aaa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*_Uxdn1HRXys0CLWI"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">塞缪尔·伯克在<a class="ae lh" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="fbf4" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">LDA 是一种广泛使用的基于 Fisher 线性判别式的降维技术。这些概念是机器学习理论的基础。在本文中，我将介绍一个使用 Fisher 线性判别式的分类器示例，并推导出 Fisher 准则的最佳解决方案。最后，我将 LDA 作为一种降维技术与 PCA 进行了比较。</p><h1 id="5d5c" class="me mf it bd mg mh mi mj mk ml mm mn mo ki mp kj mq kl mr km ms ko mt kp mu mv bi translated">介绍</h1><p id="edac" class="pw-post-body-paragraph li lj it lk b ll mw kd ln lo mx kg lq lr my lt lu lv mz lx ly lz na mb mc md im bi translated">训练分类器包括找到最佳分离数据的权重向量。如何定义这种分离是分类器不同的原因。在<a class="ae lh" rel="noopener" target="_blank" href="/derivation-of-least-squares-regressor-and-classifier-708be1358fe9">最小二乘分类器</a>中，我们找到最小化均方误差的向量，并通过优化算法(如随机梯度下降)进行优化。在 Fisher 的线性判别式中，我们试图根据分布来分离数据，而不是根据每个数据点来调整权重向量。</p><h1 id="7221" class="me mf it bd mg mh mi mj mk ml mm mn mo ki mp kj mq kl mr km ms ko mt kp mu mv bi translated">费希尔线性判别式</h1><p id="235e" class="pw-post-body-paragraph li lj it lk b ll mw kd ln lo mx kg lq lr my lt lu lv mz lx ly lz na mb mc md im bi translated">为了理解线性判别分析，我们需要首先理解费希尔的线性判别。</p><p id="e6ac" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">Fisher 的线性判别式可以用作监督学习分类器。给定标记的数据，分类器可以找到一组权重来绘制决策边界，从而对数据进行分类。<strong class="lk jd"> Fisher 线性判别式试图找到最大化投影数据类别之间的间隔的向量。</strong>最大化<strong class="lk jd">“</strong>分离”可能会有歧义。Fisher 线性判别式遵循的标准是最大化投影均值的距离，最小化投影类内方差。</p><h1 id="1f32" class="me mf it bd mg mh mi mj mk ml mm mn mo ki mp kj mq kl mr km ms ko mt kp mu mv bi translated">我们的使命</h1><p id="87a8" class="pw-post-body-paragraph li lj it lk b ll mw kd ln lo mx kg lq lr my lt lu lv mz lx ly lz na mb mc md im bi translated">让我们设置一个问题，如果你能完成它，你就会理解费希尔的线性判别式！</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi nb"><img src="../Images/b70e630ac534fff2d33c26f666e4680e.png" data-original-src="https://miro.medium.com/v2/resize:fit:662/format:webp/1*Kn2Jf0QkNtmwgz253hdIIw.png"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">作者图片</p></figure><p id="5cc2" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">这里有两个二元高斯模型，它们具有相同的协方差矩阵和不同的均值。<strong class="lk jd">我们希望找到最能分离数据投影的向量。</strong>让我们画一个随机向量，并绘制投影。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi nc"><img src="../Images/c4cd20375245c0f23f01400b682c70b5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1244/format:webp/1*e2xzPGfWCQjGugupJgcEww.png"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">作者图片</p></figure><p id="ba29" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">请记住，我们看到的是数据在向量(权重向量和数据矩阵的点积)上的投影，而不是决策边界。数据在这个随机权重向量上的投影可以绘制成直方图(右图)。正如您在将数据投影到矢量上并绘制直方图时所看到的，这两类数据没有很好地分开。我们的目标是找到右边图像中两个分布的最佳分割线。</p><p id="ea66" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">为了分离这两个分布，我们可以首先尝试最大化投影平均值之间的距离，这意味着平均而言，这两个分布彼此尽可能远。让我们在两个平均值之间画一条线，并将投影的直方图绘制到这条线上。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi nd"><img src="../Images/442b69c340a81cf387b47e2f149c1b80.png" data-original-src="https://miro.medium.com/v2/resize:fit:1232/format:webp/1*1inGh5D86sPK5Dce_XcbVA.png"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">作者图片</p></figure><p id="beac" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">这已经好多了，但是数据的投影还没有完全分离。为了完全分离它们，Fisher 线性判别式在最大化均值间投影的同时最小化投影的类内方差。它试图最大化我们之前讨论的方法来分离它们，但是也试图使分布尽可能紧密。这样可以更好地分离，如下图所示。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ne"><img src="../Images/3140883b2f17efac2a2360573d97a21d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Zj8OBbYnyBZZib8jquWkIA.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">作者图片</p></figure><p id="3876" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">如你所见，数据的投影被很好地分开了。我们可以从权重向量中取一个正交向量来创建决策边界。决策边界告诉我们，在边界的任何一边，数据都可以被预测为一个或另一个类。对于具有相同协方差矩阵的多元高斯分布，这产生了最佳分类器。</p><h1 id="2f92" class="me mf it bd mg mh mi mj mk ml mm mn mo ki mp kj mq kl mr km ms ko mt kp mu mv bi translated">费希尔标准</h1><p id="4837" class="pw-post-body-paragraph li lj it lk b ll mw kd ln lo mx kg lq lr my lt lu lv mz lx ly lz na mb mc md im bi translated">既然我把问题形象化了，我们就可以用方程的形式正式表达这些概念。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi nf"><img src="../Images/7b8c11fc312549f951f953e2ce1c8404.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Pcdg3Bk6gQWZ88taXcbhvA.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">作者图片</p></figure><p id="3d15" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">最大化费希尔准则包括找到最大化上述等式的权重向量。最大化这个方程相当于我们之前看到的。最大化意味着最大化分子(投影均值之间的距离)和最小化分母(类内方差)。我们可以在等式中代入均值和协方差，并将费雪准则改写如下。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ng"><img src="../Images/e1c5658edcc80337fdd8572151533bc6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Dq8Kf5Ppjh506_AZjV8i2Q.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">作者图片</p></figure><p id="ca23" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">上述等式在权重向量 w 的分子和分母中具有二次形式。我们可以通过微分和等于零来使 J 相对于 w 最大化。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi nh"><img src="../Images/6d49908b3fc9c44b3cfc76f4461b3223.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WnL85Rl2Nfh_KQLNQxrD_A.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">作者图片</p></figure><p id="50e6" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我们不关心标量，因为我们总是可以在以后规格化向量。上面突出显示的项是标量，因此被吸收到常数α或最后一行的比例符号中。由此，我们知道，当权重向量 w 与上述表达式成比例时，它使费雪准则最大化。在前面的例子中，我用这个比例找到了费雪的判别线性方向。</p><h1 id="511e" class="me mf it bd mg mh mi mj mk ml mm mn mo ki mp kj mq kl mr km ms ko mt kp mu mv bi translated">线性判别分析(LDA)</h1><p id="d549" class="pw-post-body-paragraph li lj it lk b ll mw kd ln lo mx kg lq lr my lt lu lv mz lx ly lz na mb mc md im bi translated">之前，我们将数据投影到权重向量上，并绘制了直方图。这种从 2D 空间到直线的投影降低了数据的维数，这就是 LDA。LDA 使用 Fisher 线性判别式来降低数据的维数，同时最大化类别之间的分离。它通过最大化均值之间的距离和最小化类内方差来做到这一点。</p><h1 id="3fa3" class="me mf it bd mg mh mi mj mk ml mm mn mo ki mp kj mq kl mr km ms ko mt kp mu mv bi translated">PCA 与 LDA</h1><p id="3bbf" class="pw-post-body-paragraph li lj it lk b ll mw kd ln lo mx kg lq lr my lt lu lv mz lx ly lz na mb mc md im bi translated">那么 LDA 与其他降维技术相比如何呢？另一种非常常见的降维方法是 PCA，它可以最大限度地将信息量传递到更小的维度上。PCA 使用通过奇异值分解找到的主成分，而不是 Fisher 的线性鉴别方向。主成分是最大化投影数据变化的方向(这不考虑数据的类别)。LDA 会考虑数据中的类别，而 PCA 则不会。</p><p id="fca4" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">选择什么样的降维技术需要你正确理解这些技术和你正在处理的问题。希望这篇文章对你的第一部分有所帮助。</p><h1 id="0c44" class="me mf it bd mg mh mi mj mk ml mm mn mo ki mp kj mq kl mr km ms ko mt kp mu mv bi translated"><strong class="ak">结论</strong></h1><p id="86ac" class="pw-post-body-paragraph li lj it lk b ll mw kd ln lo mx kg lq lr my lt lu lv mz lx ly lz na mb mc md im bi translated">在这篇文章中，我解释了 Fisher 的线性判别式，以及如何将它用作分类器和降维。我强调费希尔的线性判别式试图在低维空间中最大化类的分离。这与 PCA 等其他降维技术有着本质的不同，它不考虑数据的类成员关系。</p><h1 id="5bf7" class="me mf it bd mg mh mi mj mk ml mm mn mo ki mp kj mq kl mr km ms ko mt kp mu mv bi translated">支持我</h1><p id="f4fc" class="pw-post-body-paragraph li lj it lk b ll mw kd ln lo mx kg lq lr my lt lu lv mz lx ly lz na mb mc md im bi translated">希望这对你有所帮助，如果你喜欢它，你可以<a class="ae lh" href="https://medium.com/@diegounzuetaruedas" rel="noopener">跟随我！ </a></p><p id="5a9e" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">您也可以成为<a class="ae lh" href="https://diegounzuetaruedas.medium.com/membership" rel="noopener"> <strong class="lk jd">中级会员</strong> </a> <strong class="lk jd"> </strong>使用我的推荐链接，访问我的所有文章以及更多:<a class="ae lh" href="https://diegounzuetaruedas.medium.com/membership" rel="noopener">https://diegounzuetaruedas.medium.com/membership</a></p><h1 id="3468" class="me mf it bd mg mh mi mj mk ml mm mn mo ki mp kj mq kl mr km ms ko mt kp mu mv bi translated">你可能喜欢的其他文章</h1><p id="8f4b" class="pw-post-body-paragraph li lj it lk b ll mw kd ln lo mx kg lq lr my lt lu lv mz lx ly lz na mb mc md im bi translated"><a class="ae lh" rel="noopener" target="_blank" href="/convolutional-layers-vs-fully-connected-layers-364f05ab460b">卷积层 vs 全连接层</a></p><p id="684c" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><a class="ae lh" rel="noopener" target="_blank" href="/fourier-transforms-an-intuitive-visualisation-ba186c7380ee">傅立叶变换:直观的可视化</a></p></div></div>    
</body>
</html>