<html>
<head>
<title>K-Means Clustering in R</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">R中的k-均值聚类</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/k-means-clustering-in-r-feb4a4740aa?source=collection_archive---------9-----------------------#2021-03-23">https://towardsdatascience.com/k-means-clustering-in-r-feb4a4740aa?source=collection_archive---------9-----------------------#2021-03-23</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div class="gh gi jn"><img src="../Images/66cc2149bb19ee6db785be729974c080.png" data-original-src="https://miro.medium.com/v2/resize:fit:1258/format:webp/1*ETHMUpPiolfvPpFsT_cWvQ.png"/></div><p class="ju jv gj gh gi jw jx bd b be z dk translated">作者的情节</p></figure><h1 id="1901" class="jy jz iq bd ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv bi translated">介绍</h1><p id="8b16" class="pw-post-body-paragraph kw kx iq ky b kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi lu translated"><span class="l lv lw lx bm ly lz ma mb mc di">K</span>-均值聚类是机器学习中最流行的无监督学习方法之一。该算法有助于根据元素之间的距离从“n”个元素中识别“k”个可能的组(聚类)。</p><p id="e193" class="pw-post-body-paragraph kw kx iq ky b kz md lb lc ld me lf lg lh mf lj lk ll mg ln lo lp mh lr ls lt ij bi translated">更详细的解释可能是:算法找出你的数据中每个元素之间的距离，然后找出质心的数量，将元素分配到最近的质心以形成聚类，最终目标是使每个聚类的大小尽可能小。</p><p id="dffe" class="pw-post-body-paragraph kw kx iq ky b kz md lb lc ld me lf lg lh mf lj lk ll mg ln lo lp mh lr ls lt ij bi translated">K-means可以以多种方式使用。例如，客户细分、保险欺诈检测、文档分类等。</p><p id="0c8a" class="pw-post-body-paragraph kw kx iq ky b kz md lb lc ld me lf lg lh mf lj lk ll mg ln lo lp mh lr ls lt ij bi translated">关于K-means算法的一个常见问题是它是否能处理非数字数据。简单的回答是否定的，因为算法使用的是观测值之间的距离。但是，有许多算法可以帮助将非数字要素转换为数字要素，这将允许您将K-means算法应用于您的数据。例如,“node2vec”模型——一种用于图形表示学习的算法框架——可以将数据转换为嵌入，这种嵌入以后可以用于K-means。</p><p id="524d" class="pw-post-body-paragraph kw kx iq ky b kz md lb lc ld me lf lg lh mf lj lk ll mg ln lo lp mh lr ls lt ij bi translated">另一个有趣的事实是，与大多数无监督学习算法不同，您必须对调整参数做出经验决策(或不断尝试不同的参数，直到您达到满意的状态)，您可以以某种方式对K-means执行超参数调整。我将在这篇博客中演示如何做到这一点。</p><h1 id="c49e" class="jy jz iq bd ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv bi translated">个案研究</h1><h2 id="7bc2" class="mi jz iq bd ka mj mk dn ke ml mm dp ki lh mn mo km ll mp mq kq lp mr ms ku mt bi translated">Loda数据</h2><p id="6691" class="pw-post-body-paragraph kw kx iq ky b kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">在这个案例研究中，我使用了R中的“iris”数据集。</p><pre class="mu mv mw mx gt my mz na nb aw nc bi"><span id="bfb4" class="mi jz iq mz b gy nd ne l nf ng">data("iris")<br/>?iris</span></pre><blockquote class="nh ni nj"><p id="b446" class="kw kx nk ky b kz md lb lc ld me lf lg nl mf lj lk nm mg ln lo nn mh lr ls lt ij bi translated">Edgar Anderson的iris数据描述<br/>这个著名的(Fisher的或Anderson的)iris数据集分别给出了3种Iris的50朵花的变量萼片长和宽以及花瓣长和宽的厘米测量值。这些物种是刚毛鸢尾、杂色鸢尾和海滨鸢尾。</p><p id="64a0" class="kw kx nk ky b kz md lb lc ld me lf lg nl mf lj lk nm mg ln lo nn mh lr ls lt ij bi translated">来源<br/> Fisher，R. A. (1936)分类问题中多重测量的使用。优生学年鉴，7，第二部分，179-188页。<br/>这些数据是由安德森·埃德加(1935)收集的。加斯佩半岛的鸢尾，美国鸢尾协会公报，59，2–5。</p><p id="3001" class="kw kx nk ky b kz md lb lc ld me lf lg nl mf lj lk nm mg ln lo nn mh lr ls lt ij bi translated">参考文献<br/>《新的S语言》。沃兹沃斯&amp;布鲁克斯/科尔。(将iris3作为iris。)</p></blockquote><figure class="mu mv mw mx gt jr gh gi paragraph-image"><div class="gh gi no"><img src="../Images/7aa6d323b0ef7828563171d25afc32d4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1062/format:webp/1*IGbYKbbexMcaB4pXwAC4dQ.png"/></div><p class="ju jv gj gh gi jw jx bd b be z dk translated">数据预览</p></figure><p id="6128" class="pw-post-body-paragraph kw kx iq ky b kz md lb lc ld me lf lg lh mf lj lk ll mg ln lo lp mh lr ls lt ij bi translated">在执行K-means算法之前，我首先检查了标签，以查看这个数据集中有多少个分类。</p><pre class="mu mv mw mx gt my mz na nb aw nc bi"><span id="ec06" class="mi jz iq mz b gy nd ne l nf ng">&gt; levels(iris$Species)<br/>[1] "setosa"     "versicolor" "virginica"</span></pre><p id="8d9b" class="pw-post-body-paragraph kw kx iq ky b kz md lb lc ld me lf lg lh mf lj lk ll mg ln lo lp mh lr ls lt ij bi translated">然后我把标签从原始数据中分离出来。这样，我们可以将数据视为新的、未标记的数据，用于无监督学习目的。</p><pre class="mu mv mw mx gt my mz na nb aw nc bi"><span id="fbc0" class="mi jz iq mz b gy nd ne l nf ng">iris1 &lt;- iris[,-5]<br/>iris_label &lt;- iris[,5]</span></pre><p id="e029" class="pw-post-body-paragraph kw kx iq ky b kz md lb lc ld me lf lg lh mf lj lk ll mg ln lo lp mh lr ls lt ij bi translated">由于虹膜数据中的所有变量都是数字，所以我不必做任何数据预处理。我假装以前从未看到过这个虹膜数据，所以我想知道模型的最佳K是什么(我在开始时提到的超参数调整)。</p><h2 id="a934" class="mi jz iq bd ka mj mk dn ke ml mm dp ki lh mn mo km ll mp mq kq lp mr ms ku mt bi translated">找到K</h2><p id="0263" class="pw-post-body-paragraph kw kx iq ky b kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">在这一步中，我们需要知道两个术语，“wss”和“肘规则”，以帮助我们找到质心的最佳数量。</p><p id="940e" class="pw-post-body-paragraph kw kx iq ky b kz md lb lc ld me lf lg lh mf lj lk ll mg ln lo lp mh lr ls lt ij bi translated"><strong class="ky ir"> WSS </strong> : <em class="nk">质心内的总距离</em>。由于K-means算法的目标是保持每个聚类的大小尽可能小，因此小的wss表示每个数据点都接近其最近的质心，或者说模型返回了良好的结果。</p><p id="88e9" class="pw-post-body-paragraph kw kx iq ky b kz md lb lc ld me lf lg lh mf lj lk ll mg ln lo lp mh lr ls lt ij bi translated"><strong class="ky ir">肘规则/方法</strong> : <em class="nk">用于确定数据集</em>中聚类数量的试探法。你首先绘制出wss分数与K数的关系，因为随着K数的增加，wss总会下降；然而，每个k值之间的下降幅度会逐渐减小，曲线看起来就像一只蜷曲的手臂。这样一来，你其次需要找出哪个点落在手肘上。</p><pre class="mu mv mw mx gt my mz na nb aw nc bi"><span id="695e" class="mi jz iq mz b gy nd ne l nf ng"># to reproduce the results<br/># you have to use set.seed()<br/>set.seed(1)<br/>wss&lt;- NULL<br/>for (i in 1:10){<br/>  fit = kmeans(iris1,centers = i)<br/>  wss = c(wss, fit$tot.withinss)<br/>}</span><span id="4815" class="mi jz iq mz b gy np ne l nf ng">plot(1:10, wss, type = "o")</span></pre><figure class="mu mv mw mx gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="nr ns di nt bf nu"><div class="gh gi nq"><img src="../Images/e46477749573052c52277e14fdc7dffe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PyGU5JqI2wh-DhESED2w9A.png"/></div></div><p class="ju jv gj gh gi jw jx bd b be z dk translated">WSS对K</p></figure><figure class="mu mv mw mx gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="nr ns di nt bf nu"><div class="gh gi nv"><img src="../Images/36b18974900ee633c9194f9ddc68f328.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7vni5BC1-H_2_yUo7YCYLA.png"/></div></div><p class="ju jv gj gh gi jw jx bd b be z dk translated">阅读图并应用肘方法</p></figure><p id="509a" class="pw-post-body-paragraph kw kx iq ky b kz md lb lc ld me lf lg lh mf lj lk ll mg ln lo lp mh lr ls lt ij bi translated">根据上面的图表，我们可以知道k = 3可能是答案。如果你还记得，原始数据确实有三个不同的物种。到目前为止，我们做得很好！然而，你必须记住，在现实世界中，数据往往既不完美，也没有标签。这些群体之间的关系比我们这里的要复杂得多。因此您可能会发现多个K可以满足您需求。然后，你将不得不一个一个地尝试，研究这些群体，并通过做更深入的研究来证明你的答案。</p><h2 id="f0b2" class="mi jz iq bd ka mj mk dn ke ml mm dp ki lh mn mo km ll mp mq kq lp mr ms ku mt bi translated">拟合和绘图</h2><p id="5e2d" class="pw-post-body-paragraph kw kx iq ky b kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">然后，我拟合了一个k = 3的K均值模型，并用“fpc”包绘制了聚类图。</p><pre class="mu mv mw mx gt my mz na nb aw nc bi"><span id="5054" class="mi jz iq mz b gy nd ne l nf ng">fit &lt;- kmeans(iris[,-5], 3)</span><span id="c6d4" class="mi jz iq mz b gy np ne l nf ng">library(fpc)<br/>plotcluster(iris[,-5],fit$cluster,pointsbyclvecd=FALSE)</span></pre><figure class="mu mv mw mx gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="nr ns di nt bf nu"><div class="gh gi nw"><img src="../Images/847eb9dbd83221970ff12dc315184c44.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0TUhrrkB1_ixiaTcCa29HQ.png"/></div></div><p class="ju jv gj gh gi jw jx bd b be z dk translated">k = 3时的聚类图</p></figure><p id="133e" class="pw-post-body-paragraph kw kx iq ky b kz md lb lc ld me lf lg lh mf lj lk ll mg ln lo lp mh lr ls lt ij bi translated">注意:当pointsbyclvecd = TRUE时，图形的数据点将由更合理的符号表示，如字母数字。当该参数设置为FALSE时，符号将变得不那么“合理”，就像这个博客的特色图片所显示的那样。</p><h2 id="9ac3" class="mi jz iq bd ka mj mk dn ke ml mm dp ki lh mn mo km ll mp mq kq lp mr ms ku mt bi translated">估价</h2><p id="cce2" class="pw-post-body-paragraph kw kx iq ky b kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">当您在R中应用K-means算法时，该函数将帮助您同时生成模型的多个统计数据，包括TSS、BSS和我们在上面讨论过的WSS。</p><p id="6665" class="pw-post-body-paragraph kw kx iq ky b kz md lb lc ld me lf lg lh mf lj lk ll mg ln lo lp mh lr ls lt ij bi translated"><strong class="ky ir"> TSS </strong>:代表总平方和，即数据点与数据整体平均值的总距离。</p><p id="5c21" class="pw-post-body-paragraph kw kx iq ky b kz md lb lc ld me lf lg lh mf lj lk ll mg ln lo lp mh lr ls lt ij bi translated"><strong class="ky ir"> BSS </strong>:是每个聚类到数据全局均值的距离之和。</p><p id="fc5e" class="pw-post-body-paragraph kw kx iq ky b kz md lb lc ld me lf lg lh mf lj lk ll mg ln lo lp mh lr ls lt ij bi translated">并且，<strong class="ky ir"> <em class="nk"> TSS = BSS + WSS </em> </strong>。</p><p id="f0ed" class="pw-post-body-paragraph kw kx iq ky b kz md lb lc ld me lf lg lh mf lj lk ll mg ln lo lp mh lr ls lt ij bi translated">这里，我们可以用另一种方法来评估你适合的模型。由于我一再提到我们希望WSS尽可能小，因此，从理论上讲，BSS与TSS的高比率是我们所寻求的。</p><pre class="mu mv mw mx gt my mz na nb aw nc bi"><span id="5ed3" class="mi jz iq mz b gy nd ne l nf ng">&gt; fit$betweenss/fit$totss<br/>[1] 0.8842753</span></pre><p id="cadf" class="pw-post-body-paragraph kw kx iq ky b kz md lb lc ld me lf lg lh mf lj lk ll mg ln lo lp mh lr ls lt ij bi translated">如果你想测试你的模型的准确性，我是这样做的:</p><pre class="mu mv mw mx gt my mz na nb aw nc bi"><span id="0bec" class="mi jz iq mz b gy nd ne l nf ng"># First, relabel the data with the cluster number</span><span id="3357" class="mi jz iq mz b gy np ne l nf ng">iris$cluster = fit$cluster</span><span id="96a3" class="mi jz iq mz b gy np ne l nf ng">for (i in 1:length(iris$Species)){<br/>  if (iris$cluster[i] == 1){<br/>    iris$label[i] = "setosa"<br/>  } else if (iris$cluster[i] == 3){<br/>    iris$label[i] = "versicolor"<br/>  } else {<br/>    iris$label[i] = "virginica"<br/>  }<br/>}</span><span id="49b0" class="mi jz iq mz b gy np ne l nf ng"># Second, calculate the accuracy score<br/>&gt; mean(iris$label ==iris$Species)<br/>[1] 0.8933333</span></pre><p id="7be5" class="pw-post-body-paragraph kw kx iq ky b kz md lb lc ld me lf lg lh mf lj lk ll mg ln lo lp mh lr ls lt ij bi translated">有时，如果您有多个组，那么很难用for循环正确地重新标记数据。或者您只是想浏览来自同一个集群的数据，因为您的原始数据没有标记。您可以根据聚类数对它们进行二次抽样。我认为这也是一种处理你的数据的通用方法。您可以根据需要修改代码。</p><pre class="mu mv mw mx gt my mz na nb aw nc bi"><span id="8a53" class="mi jz iq mz b gy nd ne l nf ng">subsample &lt;- list()<br/>for(i in 1:3){<br/>  subsample[[i]]&lt;- iris[fit$cluster==i,]<br/>}</span><span id="7e6f" class="mi jz iq mz b gy np ne l nf ng">&gt; table(subsample[[1]]$Species)<br/>setosa versicolor  virginica <br/>50          0          0 </span><span id="274e" class="mi jz iq mz b gy np ne l nf ng">&gt; table(subsample[[2]]$Species)<br/>setosa versicolor  virginica <br/>0          2         36 </span><span id="8545" class="mi jz iq mz b gy np ne l nf ng">&gt; table(subsample[[3]]$Species)<br/>setosa versicolor  virginica <br/>0         48         14</span><span id="beaa" class="mi jz iq mz b gy np ne l nf ng">&gt; 134/150<br/>[1] 0.8933333</span></pre><h1 id="2a75" class="jy jz iq bd ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv bi translated">结论</h1><p id="25ff" class="pw-post-body-paragraph kw kx iq ky b kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">作为最流行的无监督学习算法之一，K-means可以帮助我们研究和发现未标记数据中的复杂关系，而这些关系很可能被我们忽略。</p><p id="23ed" class="pw-post-body-paragraph kw kx iq ky b kz md lb lc ld me lf lg lh mf lj lk ll mg ln lo lp mh lr ls lt ij bi translated">在这篇博客中，我讨论了在R中拟合K-means模型，找到最佳K，并评估该模型。我也谈到了计算标记数据的准确度分数。</p><p id="b7cc" class="pw-post-body-paragraph kw kx iq ky b kz md lb lc ld me lf lg lh mf lj lk ll mg ln lo lp mh lr ls lt ij bi translated"><strong class="ky ir"> <em class="nk">请随时与我联系</em></strong><a class="ae nx" href="https://www.linkedin.com/in/jinhangjiang/" rel="noopener ugc nofollow" target="_blank"><strong class="ky ir"><em class="nk">LinkedIn</em></strong></a><strong class="ky ir"><em class="nk">。</em>T13】</strong></p><h1 id="52d4" class="jy jz iq bd ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv bi translated"><strong class="ak">相关博客</strong></h1><p id="aeae" class="pw-post-body-paragraph kw kx iq ky b kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated"><a class="ae nx" rel="noopener" target="_blank" href="/use-r-to-calculate-boilerplate-for-accounting-analysis-f4a5b64e9b0d"> <strong class="ky ir"> <em class="nk">用R计算会计分析的样板文件</em> </strong> </a></p><p id="ca32" class="pw-post-body-paragraph kw kx iq ky b kz md lb lc ld me lf lg lh mf lj lk ll mg ln lo lp mh lr ls lt ij bi translated"><a class="ae nx" rel="noopener" target="_blank" href="/linear-regression-analysis-in-r-fdd59295d4a8"> <strong class="ky ir"> <em class="nk">线性回归分析中的R </em> </strong> </a></p><p id="888c" class="pw-post-body-paragraph kw kx iq ky b kz md lb lc ld me lf lg lh mf lj lk ll mg ln lo lp mh lr ls lt ij bi translated"><a class="ae nx" href="https://medium.com/analytics-vidhya/analyzing-disease-co-occurrence-using-networkx-gephi-and-node2vec-53941da35a0f" rel="noopener"> <strong class="ky ir"> <em class="nk">使用NetworkX、Gephi和Node2Vec </em> </strong> </a>分析疾病共现</p></div></div>    
</body>
</html>