<html>
<head>
<title>Why is Least Squares So Special?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">为什么最小二乘这么特别？</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/why-least-squares-regression-whats-with-the-l1-norm-criterion-51e8b7f905eb?source=collection_archive---------27-----------------------#2021-07-15">https://towardsdatascience.com/why-least-squares-regression-whats-with-the-l1-norm-criterion-51e8b7f905eb?source=collection_archive---------27-----------------------#2021-07-15</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="cc42" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated"><em class="kf">为什么没人谈‘绝对损失’？</em></h2></div><p id="a2d1" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">在这个故事中，我们将解决为什么我们通常更喜欢最小化偏移的平方而不是它们的绝对值的问题，至于简短的回答，这主要是因为它更容易<strong class="ki ir"/>，它更稳定<strong class="ki ir"/>，它更强大<strong class="ki ir"/>。</p><p id="af66" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">回想一下，对于OLS(普通最小二乘法)，我们寻求最小化</p><figure class="ld le lf lg gt lh gh gi paragraph-image"><div class="gh gi lc"><img src="../Images/cf7786acde7763fedfb3e86b04dcfb63.png" data-original-src="https://miro.medium.com/v2/resize:fit:552/format:webp/1*fp5n7JAOCCiDpdjr3mlJwg.png"/></div></figure><p id="f1c9" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">问题是，为什么我们不把</p><figure class="ld le lf lg gt lh gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi lk"><img src="../Images/abd72cc6a15d5c4101226fefbd68e0ff.png" data-original-src="https://miro.medium.com/v2/resize:fit:562/format:webp/1*cUlf9q6LsAUpS3gwIwc2BA.png"/></div></div></figure><p id="004c" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">先说我们简答的第一点。最小化前者更容易。特别是，最小化涉及平方偏移的成本函数的系数向量θ是通过一个封闭形式的表达式找到的，这也是我最近一篇文章的主题(你可以在这里查看<a class="ae lp" href="https://essamamin99.medium.com/ordinary-least-squares-ols-deriving-the-normal-equation-8da168d740c" rel="noopener"/>)。)相反，对于最小化涉及绝对偏移的成本函数的系数，没有“封闭形式”。我们能做的最好的事情也许是达成一个迭代方案，它能帮助我们最终在数值上收敛到属于最佳拟合超平面的系数。</p><p id="1dad" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">最小化前一个成本函数更容易的事实源于这样一个事实，即<em class="lq"> y = f (x) </em>有一个很好的导数，即<em class="lq"> y' = 2f(x)。f '(x) </em>与<em class="lq"> y' = |f(x)|f '(x) /f(x) </em>那是因为<em class="lq"> y = |f(x)|。</em>对于后者，<em class="lq"> </em>绝对值在微分后保持不变，这使得求和表达式等于零(尤其是涉及求和的情况下)变得非常重要。</p><figure class="ld le lf lg gt lh"><div class="bz fp l di"><div class="lr ls l"/></div><p class="lt lu gj gh gi lv lw bd b be z dk translated">吉菲的扎克·加利费安纳基斯反应</p></figure><p id="308c" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">我们简短回答的第二点是稳定。两个成本函数都承诺存在一个系数向量θ，该向量在全局上最小化从超平面到数据集的偏移。这是两个函数都是凸的直接结果(有自己的证明)。</p><p id="7702" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">问题在于，涉及绝对失调的成本函数可能有点过了，因为它实际上可以提供无限数量的不同系数向量θ，所有这些向量都可以最小化成本函数。同时，对于OLS方法，总是有一个唯一的系数向量θ使成本函数最小化(除非数据集非常小，在这种情况下两个问题都简化为插值)。)这两个事实是从L1范数的<br/> <strong class="ki ir">非严格</strong>凸性和L2范数的严格凸性中推导出来的，两者也都有自己的证明。</p><p id="ff83" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">如果你想看到与我们的论点一致的东西，考虑为一些随机数据集绘制两个成本函数(你可以考虑拟合类似于<em class="lq"> y=ax </em>的东西来使它变得简单)，然后根据你的数据集，你可能会看到类似这样的东西。</p><figure class="ld le lf lg gt lh gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi lx"><img src="../Images/95b9bc4fadc27f807834488b44e7b773.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WaYf1OcgX5RTcMuNjsvpHQ.png"/></div></div><p class="lt lu gj gh gi lv lw bd b be z dk translated">L1 vs L2损失-作者图片。</p></figure><p id="2de1" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">最小值明显不同，对于平方损耗，最小值是一个唯一点，而对于L1损耗，最小值是无穷多个点。</p><p id="a334" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">我们简短回答中的最后一点是关于L2损失的回归比L1损失的回归在统计上更有说服力。让我们首先强调这样一个事实，即在某种意义上，两种选择都可能是真正最优的，特别是，如果我们假设误差(偏移)遵循<strong class="ki ir">正态分布</strong>，那么最小化L2损失与寻找最大似然估计<strong class="ki ir"> ( </strong>)相一致，这主要用于统计推断。)另一方面，如果我们假设误差遵循<strong class="ki ir">拉普拉斯分布</strong>，那么最小化L1损失与找到最大似然估计相符。当然，这两个事实都有自己的证明。</p><p id="6627" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">下图显示了两种分布的曲线图(在2D)。红色和蓝色分别对应正态和拉普拉斯分布。</p><figure class="ld le lf lg gt lh gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi ly"><img src="../Images/80ffef5ed6ab9110900fe71de5530580.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rVMfXyR_-D1iJmUohB_phg.png"/></div></div><p class="lt lu gj gh gi lv lw bd b be z dk translated">正态分布与拉普拉斯分布-作者图片。</p></figure><p id="f0fc" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">现在的问题是，我们应该相信误差服从哪个分布，即使是近似的。答案是，这通常是正态分布，因为我们通常认为误差是独立的(不以任何方式相互影响)，这使得它们随着数据集的增长而接近正态分布，这要归功于中心极限定理。事实上，你可能只听说过正态分布，因为它在现实生活中出现的频率更高。</p><p id="dfc1" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated"><strong class="ki ir">原来如此？使用L1标准是没有意义的？答案肯定是<strong class="ki ir">没有</strong>。事实上，使用L1范数准则进行回归是一件真实的事情，也可以按需使用。如果你不知道，它通常也被称为最小绝对偏差(缩写为LAD)。在一种情况下，LAD似乎优于OLS，特别是当数据集包含许多异常或异常值时，因为OLS会在尽量减少损失的同时大肆渲染这些异常或异常值，因为对它们的偏移量求平方会产生很大一部分误差，结果是一个位于实际数据集和异常值之间的超平面，如果你没有理解我在这里试图说的话，请仔细观察</strong></p><figure class="ld le lf lg gt lh gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi lz"><img src="../Images/f403e0ff4a41605d4b85ba6d8f636507.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*TXo0mM3Pq8YmXoKrkgDBqA.gif"/></div></div><p class="lt lu gj gh gi lv lw bd b be z dk translated">小伙子对OLS-GIF和作者的项目。</p></figure><p id="142d" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">此处的数据集包括左侧表格中的8个点，黄线(yₐ)is归因于LAD，白线归因于OLS。数据集不包含任何异常值，这两条线似乎是合理的。当然除了我们每次计算时，LAD的意见略有不同(迭代格式每次从不同的初始点开始，LAD不保证有唯一的极小值)。</p><p id="2c6f" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">现在，如果我们把点(9，6)换成(9，50)会发生什么</p><figure class="ld le lf lg gt lh gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi ma"><img src="../Images/ccab1bc8f068d2bdc48a4a589bc3862a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vS0TTubw2b3D_irmenNjhg.jpeg"/></div></div><p class="lt lu gj gh gi lv lw bd b be z dk translated">小伙子对OLS-GIF和作者的项目。</p></figure><p id="938b" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">对于LAD线来说没什么大不了的，同时OLS很容易被诱惑牺牲整个数据集，只为了离群值，这证实了LAD比OLS更健壮的事实。</p><p id="96bb" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">如果您想自己尝试一下，请访问:</p><div class="mb mc gp gr md me"><a href="http://mathemati.herokuapp.com/LeastAbsoluteErrors" rel="noopener  ugc nofollow" target="_blank"><div class="mf ab fo"><div class="mg ab mh cl cj mi"><h2 class="bd ir gy z fp mj fr fs mk fu fw ip bi translated">最小绝对偏差</h2><div class="ml l"><p class="bd b dl z fp mj fr fs mk fu fw dk translated">mathemati.herokuapp.com</p></div></div></div></a></div><p id="d6a3" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">希望这有助于你理解为什么我们通常选择OLS，下次再见。</p><p id="990b" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">参考资料:</p><p id="562f" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">[1]:吉菲。<em class="lq">https://giphy.com/gifs/reaction-BmmfETghGOPrW</em>扎克·加利费安纳基斯<a class="ae lp" href="https://giphy.com/gifs/reaction-BmmfETghGOPrW" rel="noopener ugc nofollow" target="_blank">反应</a></p><p id="a5b6" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">[2]:维基百科。最大似然估计https://en.wikipedia.org/wiki/Maximum_likelihood_estimation<a class="ae lp" href="https://en.wikipedia.org/wiki/Maximum_likelihood_estimation" rel="noopener ugc nofollow" target="_blank">T4</a></p><p id="17b9" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">[3]:维基百科，<em class="lq">最小一乘偏差</em>T10】https://en.wikipedia.org/wiki/Least_absolute_deviations</p><p id="76e2" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">[4]:维基百科，正态分布<a class="ae lp" href="https://en.wikipedia.org/wiki/Normal_distribution" rel="noopener ugc nofollow" target="_blank">https://en.wikipedia.org/wiki/Normal_distribution</a></p></div></div>    
</body>
</html>