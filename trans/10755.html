<html>
<head>
<title>A Step By Step Implementation of Principal Component Analysis</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">主成分分析的逐步实现</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/a-step-by-step-implementation-of-principal-component-analysis-5520cc6cd598?source=collection_archive---------2-----------------------#2021-10-18">https://towardsdatascience.com/a-step-by-step-implementation-of-principal-component-analysis-5520cc6cd598?source=collection_archive---------2-----------------------#2021-10-18</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="afeb" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">一个分步教程，解释PCA的工作原理，并从头开始用python实现它</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi kf"><img src="../Images/17dd5f55e480f8e438f67221294cbc50.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/1*1PcCOEf8b0e-aZkz9wxpHg.gif"/></div><p class="kn ko gj gh gi kp kq bd b be z dk translated">作者图片</p></figure><h1 id="a5f1" class="kr ks iq bd kt ku kv kw kx ky kz la lb jw lc jx ld jz le ka lf kc lg kd lh li bi translated">介绍</h1><p id="d504" class="pw-post-body-paragraph lj lk iq ll b lm ln jr lo lp lq ju lr ls lt lu lv lw lx ly lz ma mb mc md me ij bi translated">主成分分析(PCA)是一种常用的降维方法。它的工作原理是计算主成分并执行基变换。它保留最大方差方向的数据。减少的特征彼此不相关。这些特征可用于无监督聚类和分类。为了降低维数，自动编码器是另一种常用的方法。但是，自动编码器的潜在空间不一定是不相关的。而PCA保证所有特征彼此不相关。</p><p id="6029" class="pw-post-body-paragraph lj lk iq ll b lm mf jr lo lp mg ju lr ls mh lu lv lw mi ly lz ma mj mc md me ij bi translated">首先，PCA计算协方差矩阵。然后我们找到协方差矩阵的特征向量和特征值。之后，我们沿着特征向量投影数据。如果原始数据的维数为<strong class="ll ir"> n </strong>，我们可以将维数减少到<strong class="ll ir"> k，</strong>使得<strong class="ll ir"> k≤ n. </strong></p><p id="24a9" class="pw-post-body-paragraph lj lk iq ll b lm mf jr lo lp mg ju lr ls mh lu lv lw mi ly lz ma mj mc md me ij bi translated">在本教程中，我们将从头开始实现PCA，并理解每一步的意义。</p><h1 id="120a" class="kr ks iq bd kt ku kv kw kx ky kz la lb jw lc jx ld jz le ka lf kc lg kd lh li bi translated">履行</h1><p id="7bdc" class="pw-post-body-paragraph lj lk iq ll b lm ln jr lo lp lq ju lr ls lt lu lv lw lx ly lz ma mb mc md me ij bi translated">首先，导入库。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="mk ml l"/></div></figure><h2 id="a78f" class="mm ks iq bd kt mn mo dn kx mp mq dp lb ls mr ms ld lw mt mu lf ma mv mw lh mx bi translated">步骤1:创建随机数据</h2><p id="bc8b" class="pw-post-body-paragraph lj lk iq ll b lm ln jr lo lp lq ju lr ls lt lu lv lw lx ly lz ma mb mc md me ij bi translated">通过从多元正态分布中随机抽取样本来创建数据。我们将从二维数据开始。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="mk ml l"/></div></figure><p id="bda7" class="pw-post-body-paragraph lj lk iq ll b lm mf jr lo lp mg ju lr ls mh lu lv lw mi ly lz ma mj mc md me ij bi translated">意思是，mu是:[10，13]。</p><p id="7929" class="pw-post-body-paragraph lj lk iq ll b lm mf jr lo lp mg ju lr ls mh lu lv lw mi ly lz ma mj mc md me ij bi translated">协方差矩阵，sigma为:[[3.5 -1.8]，[-1.8，3.5]]。</p><p id="81d1" class="pw-post-body-paragraph lj lk iq ll b lm mf jr lo lp mg ju lr ls mh lu lv lw mi ly lz ma mj mc md me ij bi translated">这是数据的散点图:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="mz na di nb bf nc"><div class="gh gi my"><img src="../Images/1026ee8c77b8d007501ae38bf4a92d0d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4iqj8AuRmvWqrxR8rL-biQ.png"/></div></div><p class="kn ko gj gh gi kp kq bd b be z dk translated">原始数据散点图</p></figure><h2 id="cf30" class="mm ks iq bd kt mn mo dn kx mp mq dp lb ls mr ms ld lw mt mu lf ma mv mw lh mx bi translated">步骤2:均值居中/归一化数据</h2><p id="76ef" class="pw-post-body-paragraph lj lk iq ll b lm ln jr lo lp lq ju lr ls lt lu lv lw lx ly lz ma mb mc md me ij bi translated">在PCA之前，我们将数据标准化/规范化。</p><p id="35dc" class="pw-post-body-paragraph lj lk iq ll b lm mf jr lo lp mg ju lr ls mh lu lv lw mi ly lz ma mj mc md me ij bi translated">通常，进行归一化是为了使所有要素的比例相同。例如，对于房价预测数据集，我们有不同的功能。价格以美元为单位，而面积以平方为单位。制成现在，该算法将认为数值越高越重要。因此，我们需要在同一范围内对所有特征进行归一化。</p><p id="df56" class="pw-post-body-paragraph lj lk iq ll b lm mf jr lo lp mg ju lr ls mh lu lv lw mi ly lz ma mj mc md me ij bi translated">进行均值居中以确保第一主分量在最大方差的方向上。</p><p id="76f3" class="pw-post-body-paragraph lj lk iq ll b lm mf jr lo lp mg ju lr ls mh lu lv lw mi ly lz ma mj mc md me ij bi translated">我们将通过从所有特征或通道中减去平均值来进行均值居中。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="mk ml l"/></div></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="mz na di nb bf nc"><div class="gh gi nd"><img src="../Images/3da8b80392df368c670d167efb919120.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5uXioHa2YoyOWN-7TomSLw.png"/></div></div><p class="kn ko gj gh gi kp kq bd b be z dk translated">原始数据(左)和平均值居中数据(右)的散点图</p></figure><h2 id="4e3a" class="mm ks iq bd kt mn mo dn kx mp mq dp lb ls mr ms ld lw mt mu lf ma mv mw lh mx bi translated">步骤3:计算协方差矩阵</h2><p id="a5d9" class="pw-post-body-paragraph lj lk iq ll b lm ln jr lo lp lq ju lr ls lt lu lv lw lx ly lz ma mb mc md me ij bi translated">现在，我们计算所有特征维度的协方差。每个协方差矩阵都是对称的和半正定的。它有正交的特征向量。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="mk ml l"/></div></figure><p id="131d" class="pw-post-body-paragraph lj lk iq ll b lm mf jr lo lp mg ju lr ls mh lu lv lw mi ly lz ma mj mc md me ij bi translated">协方差矩阵的大小将是(2×2)。</p><h2 id="d10d" class="mm ks iq bd kt mn mo dn kx mp mq dp lb ls mr ms ld lw mt mu lf ma mv mw lh mx bi translated">步骤4:计算协方差矩阵的特征向量</h2><p id="23ab" class="pw-post-body-paragraph lj lk iq ll b lm ln jr lo lp lq ju lr ls lt lu lv lw lx ly lz ma mb mc md me ij bi translated">现在，我们执行协方差矩阵的特征分解，我们将得到特征向量和特征值。(特征值/向量的数量将与特征/通道的数量相同。)</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="mk ml l"/></div></figure><p id="8f95" class="pw-post-body-paragraph lj lk iq ll b lm mf jr lo lp mg ju lr ls mh lu lv lw mi ly lz ma mj mc md me ij bi translated">每个特征向量代表一个方差方向。对应于最大特征值的特征向量将给出最大方差的方向。这是第一个主成分。然后，对应于第二大特征值的特征向量将给出第二大方差的方向。这是第二个主成分。诸如此类。</p><p id="070c" class="pw-post-body-paragraph lj lk iq ll b lm mf jr lo lp mg ju lr ls mh lu lv lw mi ly lz ma mj mc md me ij bi translated">因此，我们需要根据特征值的降序对特征向量进行排序。记住，所有的特征向量都是相互正交的。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="mk ml l"/></div></figure><h2 id="1801" class="mm ks iq bd kt mn mo dn kx mp mq dp lb ls mr ms ld lw mt mu lf ma mv mw lh mx bi translated">步骤5:计算解释的方差并选择N个分量</h2><p id="ba13" class="pw-post-body-paragraph lj lk iq ll b lm ln jr lo lp lq ju lr ls lt lu lv lw lx ly lz ma mb mc md me ij bi translated">我们可以基于我们想要的压缩程度来选择顶部的<strong class="ll ir"> <em class="ne"> k </em> </strong>特征向量。</p><p id="3b89" class="pw-post-body-paragraph lj lk iq ll b lm mf jr lo lp mg ju lr ls mh lu lv lw mi ly lz ma mj mc md me ij bi translated">选择组件数量的最佳方法是计算每个特征的解释方差。我们通过将特征值除以所有特征值的总和来计算解释方差。然后，我们取所有特征值的累积和。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="mk ml l"/></div></figure><p id="acaf" class="pw-post-body-paragraph lj lk iq ll b lm mf jr lo lp mg ju lr ls mh lu lv lw mi ly lz ma mj mc md me ij bi translated">这里的特征值是:[5.50，1.72]。</p><p id="27e9" class="pw-post-body-paragraph lj lk iq ll b lm mf jr lo lp mg ju lr ls mh lu lv lw mi ly lz ma mj mc md me ij bi translated">特征值之和为:7.22</p><p id="b9f7" class="pw-post-body-paragraph lj lk iq ll b lm mf jr lo lp mg ju lr ls mh lu lv lw mi ly lz ma mj mc md me ij bi translated">解释方差为:[0.76，0.23]</p><p id="9ef8" class="pw-post-body-paragraph lj lk iq ll b lm mf jr lo lp mg ju lr ls mh lu lv lw mi ly lz ma mj mc md me ij bi translated">累计解释方差为:[0.76，0.99]</p><p id="53a0" class="pw-post-body-paragraph lj lk iq ll b lm mf jr lo lp mg ju lr ls mh lu lv lw mi ly lz ma mj mc md me ij bi translated">因此，当我们有更高维度的数据时，我们通常以这样一种方式获取<strong class="ll ir"> <em class="ne"> k </em> </strong>分量，从而得到0.95或更大的解释方差。</p><p id="4bd5" class="pw-post-body-paragraph lj lk iq ll b lm mf jr lo lp mg ju lr ls mh lu lv lw mi ly lz ma mj mc md me ij bi translated">在本文中，我们将选择这两个组件来解释PCA的工作原理。</p><h2 id="dfc4" class="mm ks iq bd kt mn mo dn kx mp mq dp lb ls mr ms ld lw mt mu lf ma mv mw lh mx bi translated">步骤6:使用特征向量转换数据</h2><p id="d20d" class="pw-post-body-paragraph lj lk iq ll b lm ln jr lo lp lq ju lr ls lt lu lv lw lx ly lz ma mb mc md me ij bi translated">现在，我们将我们的数据与特征向量进行点积，以获得我们的数据在这些特征向量方向上的投影。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="mk ml l"/></div></figure><p id="e84a" class="pw-post-body-paragraph lj lk iq ll b lm mf jr lo lp mg ju lr ls mh lu lv lw mi ly lz ma mj mc md me ij bi translated">这是转换数据的散点图:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="mz na di nb bf nc"><div class="gh gi nf"><img src="../Images/9b38bda8c8ef216c9e65b00100c707f9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EC-PPZ8zfRptedy4kgd6Ng.png"/></div></div><p class="kn ko gj gh gi kp kq bd b be z dk translated">原始数据的散点图(左)，平均中心数据(中)，PCA后的转换数据(右)</p></figure><p id="569d" class="pw-post-body-paragraph lj lk iq ll b lm mf jr lo lp mg ju lr ls mh lu lv lw mi ly lz ma mj mc md me ij bi translated">在散点图中，我们可以看到PCA后，y轴是方差最大的方向。例如，如果我们将10维数据简化为2维数据，我们将得到沿两个垂直方向的具有最大方差的投影。</p><h2 id="24c5" class="mm ks iq bd kt mn mo dn kx mp mq dp lb ls mr ms ld lw mt mu lf ma mv mw lh mx bi translated">步骤7:反转PCA并重建原始数据</h2><p id="7f60" class="pw-post-body-paragraph lj lk iq ll b lm ln jr lo lp lq ju lr ls lt lu lv lw lx ly lz ma mb mc md me ij bi translated">我们还可以通过取特征向量的转置与变换数据的点积来重构原始数据。请记住，我们在开始时从数据中减去平均值，以使数据居中。所以，现在我们需要加上平均值。</p><p id="5342" class="pw-post-body-paragraph lj lk iq ll b lm mf jr lo lp mg ju lr ls mh lu lv lw mi ly lz ma mj mc md me ij bi translated">所有的特征向量都是相互正交的。所以，当我们取特征向量的点积时，我们得到一个单位矩阵。</p><blockquote class="ng nh ni"><p id="c4a1" class="lj lk ne ll b lm mf jr lo lp mg ju lr nj mh lu lv nk mi ly lz nl mj mc md me ij bi translated">X = X —平均值</p><p id="3862" class="lj lk ne ll b lm mf jr lo lp mg ju lr nj mh lu lv nk mi ly lz nl mj mc md me ij bi translated">pca_X = X * V</p><p id="ae82" class="lj lk ne ll b lm mf jr lo lp mg ju lr nj mh lu lv nk mi ly lz nl mj mc md me ij bi translated">recon _ X =(PCA _ X * V ')+mean =((X * V)* V ')+mean =(X *(V * V ')+mean =(X * I)+mean = X+mean</p></blockquote><p id="9944" class="pw-post-body-paragraph lj lk iq ll b lm mf jr lo lp mg ju lr ls mh lu lv lw mi ly lz ma mj mc md me ij bi translated"><em class="ne">PCA _ X:PCA后的变换数据</em></p><p id="5ed6" class="pw-post-body-paragraph lj lk iq ll b lm mf jr lo lp mg ju lr ls mh lu lv lw mi ly lz ma mj mc md me ij bi translated"><em class="ne"> recon_X:重建数据</em></p><p id="da3d" class="pw-post-body-paragraph lj lk iq ll b lm mf jr lo lp mg ju lr ls mh lu lv lw mi ly lz ma mj mc md me ij bi translated"><em class="ne"> V:特征向量矩阵</em></p><p id="3691" class="pw-post-body-paragraph lj lk iq ll b lm mf jr lo lp mg ju lr ls mh lu lv lw mi ly lz ma mj mc md me ij bi translated">平均值:每个维度/通道的平均值</p><p id="bea3" class="pw-post-body-paragraph lj lk iq ll b lm mf jr lo lp mg ju lr ls mh lu lv lw mi ly lz ma mj mc md me ij bi translated">所以，我们可以从k个分量重建X。</p><blockquote class="ng nh ni"><p id="f098" class="lj lk ne ll b lm mf jr lo lp mg ju lr nj mh lu lv nk mi ly lz nl mj mc md me ij bi translated"><em class="iq">如果原始数据的形状为:num_samp x N. </em></p><p id="d239" class="lj lk ne ll b lm mf jr lo lp mg ju lr nj mh lu lv nk mi ly lz nl mj mc md me ij bi translated"><em class="iq">那么，协方差矩阵的形状将是:N×N。特征向量的形状也将是N×N。</em></p><p id="a4f0" class="lj lk ne ll b lm mf jr lo lp mg ju lr nj mh lu lv nk mi ly lz nl mj mc md me ij bi translated"><em class="iq">如果我们选择顶部d个特征向量，我们得到形状的特征向量:N×k。</em></p><p id="f183" class="lj lk ne ll b lm mf jr lo lp mg ju lr nj mh lu lv nk mi ly lz nl mj mc md me ij bi translated"><em class="iq">现在，我们将原始数据与前d个特征向量进行点积。因此，在使用PCA转换数据后，我们将得到如下形状:num_samp x k. </em></p><p id="d6ee" class="lj lk ne ll b lm mf jr lo lp mg ju lr nj mh lu lv nk mi ly lz nl mj mc md me ij bi translated"><em class="iq">当我们将重构数据时，我们用顶部d个特征向量的转置来取pca数据的点积。因此，我们将有形状:</em></p><p id="6e7f" class="lj lk ne ll b lm mf jr lo lp mg ju lr nj mh lu lv nk mi ly lz nl mj mc md me ij bi translated"><em class="iq">(num _ samp x k)*(N x k)' =(num _ samp x N)。</em></p></blockquote><p id="d673" class="pw-post-body-paragraph lj lk iq ll b lm mf jr lo lp mg ju lr ls mh lu lv lw mi ly lz ma mj mc md me ij bi translated">这是重建数据的散点图:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="mz na di nb bf nc"><div class="gh gi nf"><img src="../Images/536ca52b98be9c75c3800bbec83e08f6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*my9BxXXnkldzfsSMR8-7TA.png"/></div></div><p class="kn ko gj gh gi kp kq bd b be z dk translated">原始数据的散点图(左)，平均中心数据(中)，反转PCA后的重建数据(右)</p></figure><p id="d047" class="pw-post-body-paragraph lj lk iq ll b lm mf jr lo lp mg ju lr ls mh lu lv lw mi ly lz ma mj mc md me ij bi translated">您还可以计算重建损失:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="mk ml l"/></div></figure><p id="5b72" class="pw-post-body-paragraph lj lk iq ll b lm mf jr lo lp mg ju lr ls mh lu lv lw mi ly lz ma mj mc md me ij bi translated">在这种情况下，重建损失为:2.6426840324903897e-32。</p><p id="32af" class="pw-post-body-paragraph lj lk iq ll b lm mf jr lo lp mg ju lr ls mh lu lv lw mi ly lz ma mj mc md me ij bi translated">它非常低，因为我们使用了所有组件来重建数据。</p><p id="b81c" class="pw-post-body-paragraph lj lk iq ll b lm mf jr lo lp mg ju lr ls mh lu lv lw mi ly lz ma mj mc md me ij bi translated">完整的代码库可从以下网址获得:</p><div class="nm nn gp gr no np"><a href="https://github.com/AdityaDutt/PCATutorial/blob/main/PCA_tutorial.ipynb" rel="noopener  ugc nofollow" target="_blank"><div class="nq ab fo"><div class="nr ab ns cl cj nt"><h2 class="bd ir gy z fp nu fr fs nv fu fw ip bi translated">pcatuarial/PCA _ tutorial . ipynb位于主AdityaDutt/pcatuarial</h2><div class="nw l"><h3 class="bd b gy z fp nu fr fs nv fu fw dk translated">实现PCA的一步一步的教程。通过在…上创建帐户，为adityadut/pcatuality的发展做出贡献</h3></div><div class="nx l"><p class="bd b dl z fp nu fr fs nv fu fw dk translated">github.com</p></div></div><div class="ny l"><div class="nz l oa ob oc ny od kl np"/></div></div></a></div><p id="636f" class="pw-post-body-paragraph lj lk iq ll b lm mf jr lo lp mg ju lr ls mh lu lv lw mi ly lz ma mj mc md me ij bi translated">在本教程中，我们没有降低维度。但是我们可以取前N个特征向量并计算其与原始数据的点积来获得PCA特征。其他都一样。</p><p id="c788" class="pw-post-body-paragraph lj lk iq ll b lm mf jr lo lp mg ju lr ls mh lu lv lw mi ly lz ma mj mc md me ij bi translated">您可以签出这个库中的代码来减少特性。</p><h1 id="1605" class="kr ks iq bd kt ku kv kw kx ky kz la lb jw lc jx ld jz le ka lf kc lg kd lh li bi translated">外卖食品</h1><ol class=""><li id="c5e2" class="oe of iq ll b lm ln lp lq ls og lw oh ma oi me oj ok ol om bi translated">PCA是一种正交线性变换。</li><li id="72eb" class="oe of iq ll b lm on lp oo ls op lw oq ma or me oj ok ol om bi translated">PCA给出不相关的特征。(自动编码器将给出相关的特征，它们也可以模拟非线性数据)</li><li id="16cd" class="oe of iq ll b lm on lp oo ls op lw oq ma or me oj ok ol om bi translated">协方差矩阵是对称且半正定的。</li><li id="d109" class="oe of iq ll b lm on lp oo ls op lw oq ma or me oj ok ol om bi translated">协方差矩阵的特征向量彼此正交。</li><li id="9c8b" class="oe of iq ll b lm on lp oo ls op lw oq ma or me oj ok ol om bi translated">PCA可以被反转以重建数据。</li><li id="b319" class="oe of iq ll b lm on lp oo ls op lw oq ma or me oj ok ol om bi translated">我们还可以使用PCA来检测异常值。当我们将使用k个分量重建数据时，k≤n，其中n是原始维度，异常值将给出更高的重建损失。我们可以计算离群值和非离群值的重建损失直方图。然后选择一个阈值来丢弃异常值。</li><li id="fe0e" class="oe of iq ll b lm on lp oo ls op lw oq ma or me oj ok ol om bi translated">PCA特征也可以用于聚类和分类。我们可以将这些简化的特征输入神经网络。</li></ol></div><div class="ab cl os ot hu ou" role="separator"><span class="ov bw bk ow ox oy"/><span class="ov bw bk ow ox oy"/><span class="ov bw bk ow ox"/></div><div class="ij ik il im in"><blockquote class="ng nh ni"><p id="275f" class="lj lk ne ll b lm mf jr lo lp mg ju lr nj mh lu lv nk mi ly lz nl mj mc md me ij bi translated">我希望这篇文章对你有用。</p><p id="cb7a" class="lj lk ne ll b lm mf jr lo lp mg ju lr nj mh lu lv nk mi ly lz nl mj mc md me ij bi translated"><em class="iq">如果您有关于PCA和使用PCA进行异常值检测的其他问题，并且需要帮助，请告诉我。我将在以后的文章中尝试介绍它们。</em></p></blockquote><p id="3275" class="pw-post-body-paragraph lj lk iq ll b lm mf jr lo lp mg ju lr ls mh lu lv lw mi ly lz ma mj mc md me ij bi translated"><strong class="ll ir"> <em class="ne">非常感谢您的阅读！</em>T15】🙂</strong></p></div></div>    
</body>
</html>