<html>
<head>
<title>Connecting Naive Bayes and Logistic Regression: Binary Classification</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">连接朴素贝叶斯和逻辑回归:二元分类</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/connecting-naive-bayes-and-logistic-regression-binary-classification-ce69e527157f?source=collection_archive---------4-----------------------#2021-12-24">https://towardsdatascience.com/connecting-naive-bayes-and-logistic-regression-binary-classification-ce69e527157f?source=collection_archive---------4-----------------------#2021-12-24</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="21e0" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">概率、似然和张量流概率库</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/130875c91c50a417818410309954af86.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0FCFeF4X_xZQkbP0w3IAng.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">给企鹅分类【照片由<a class="ae ky" href="https://unsplash.com/@ncx1701d?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">马丁·韦特斯坦</a>在<a class="ae ky" href="https://unsplash.com/s/photos/penguins?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄】</p></figure><p id="8ad5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">朴素贝叶斯和逻辑回归都是非常常用的分类器，在这篇文章中，我们将试图找到并理解这些分类器之间的联系。我们还将使用<a class="ae ky" href="https://allisonhorst.github.io/palmerpenguins/" rel="noopener ugc nofollow" target="_blank">帕尔默企鹅数据集</a>来看一个例子，该数据集在 CC-0 许可下可用。你希望从这篇文章中学到什么？</p><ul class=""><li id="6957" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated">概率和可能性基础。</li><li id="8a86" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">朴素贝叶斯分类器是如何工作的？</li><li id="e78d" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">区分和生成分类器。</li><li id="173e" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">朴素贝叶斯和逻辑回归之间的联系。</li></ul><p id="fac6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这里使用的所有公式都来自我的笔记本，链接和其他有用的参考资料都在参考资料部分。我们开始吧！</p></div><div class="ab cl mj mk hx ml" role="separator"><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo"/></div><div class="im in io ip iq"><h2 id="5810" class="mq mr it bd ms mt mu dn mv mw mx dp my li mz na nb lm nc nd ne lq nf ng nh ni bi translated">概率和可能性:</h2><p id="f241" class="pw-post-body-paragraph kz la it lb b lc nj ju le lf nk jx lh li nl lk ll lm nm lo lp lq nn ls lt lu im bi translated">每个概率分布都有一个连续分布的概率密度函数(PDF ),如高斯分布(或离散分布的概率质量函数或 PMF，如二项式分布),表示样本(某点)取特定值的概率。该函数通常由<em class="no"> P </em> ( <em class="no"> y </em> | <em class="no"> θ </em>表示，其中<em class="no"> y </em>是样本值，<em class="no"> θ </em>是描述 PDF/PMF 的参数。当一个以上的样本彼此独立抽取时，我们可以写成—</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi np"><img src="../Images/878e4b96f2f59ab7b63382db23fa4c4c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1330/format:webp/1*LFj4Kz5OXpnSGkCWCauEHg.jpeg"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">情商。1:独立绘制样本的 PDF。</p></figure><p id="2637" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">当我们知道分布(和相应的参数<em class="no"> θ </em>)并想要推导出<em class="no">y</em>s 时，我们在计算中考虑 PDF。这里我们认为<em class="no"> θ </em>对于不同的样本是固定的(已知的),并且我们想要推导出不同的<em class="no"> y </em>。似然函数是相同的，但是有所改变。这里<em class="no"> y </em>是已知的，而<em class="no"> θ </em>是我们想要确定的变量。至此，我们可以简单介绍一下最大似然估计(MLE)的思想。</p><p id="4a94" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="no">极大似然估计:</em> MLE 与数据的建模密切相关；如果我们观察数据点<em class="no"> y </em> 1，<em class="no"> y </em> 2，…，<em class="no"> yn </em>并假设它们来自一个由<em class="no"> θ </em>参数化的分布，那么似然性由<em class="no"> L </em> ( <em class="no"> y </em> 1，<em class="no"> y </em> 2，…、<em class="no">yn</em>|<em class="no">θ</em>)；对于 MLE，我们的任务是估计最大似然的θ。对于独立观察，我们可以将最佳θ的表达式写成如下所示—</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nq"><img src="../Images/3421e6710198f7ffbace6c3e16587bb7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aq6Zci-WEsG6t0zKfNoauA.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">情商。2:最大化可能性</p></figure><p id="04b7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">优化的惯例是最小化一个函数；因此，最大化似然归结为最小化负对数似然。这些是我们稍后将会用到的概念，现在让我们转到朴素贝叶斯分类器。</p></div><div class="ab cl mj mk hx ml" role="separator"><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo"/></div><div class="im in io ip iq"><h2 id="e30f" class="mq mr it bd ms mt mu dn mv mw mx dp my li mz na nb lm nc nd ne lq nf ng nh ni bi translated">朴素贝叶斯分类器:</h2><p id="1959" class="pw-post-body-paragraph kz la it lb b lc nj ju le lf nk jx lh li nl lk ll lm nm lo lp lq nn ls lt lu im bi translated">朴素贝叶斯分类器是<em class="no">生成分类器</em>的一个例子，而逻辑回归是<em class="no">鉴别分类器</em>的一个例子。但是我们所说的生成性和辨别力是什么意思呢？</p><p id="f108" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">判别分类器:</strong>一般来说，分类问题可以简单地认为是预测给定输入向量的类别标签<em class="no">p</em>(<em class="no">C _ k</em>|<em class="no">x</em>)。在判别模型中，我们假设<em class="no">p</em>(<em class="no">C _ k</em>|<em class="no">x</em>)的某种函数形式，并直接从训练数据中估计参数。</p><p id="3a47" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">生成分类器:</strong>在生成模型中，我们估计<em class="no">p</em>(<em class="no">x</em>|<em class="no">C _ k</em>)的参数，即每个类的输入的概率分布以及类先验<em class="no"> p </em> ( <em class="no"> C_k </em>)。两者都用在贝叶斯定理中计算<em class="no">p</em>(<em class="no">C _ k</em>|<em class="no">x</em>)。</p><p id="9e00" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们已经将类别标签定义为<em class="no"> C_k </em>，让我们在此基础上定义贝叶斯定理——</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nr"><img src="../Images/f5e44184ba924f46659a88910e5971a9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_2pNqUTYOtMWEoAnDJXzow.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">情商。3:贝叶斯定理，x 是训练数据 C_k 代表类别标签。</p></figure><p id="acb1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="no"> p </em> ( <em class="no"> x </em>)可以认为是一个归一化常数；<em class="no">p</em>(<em class="no">x</em>)=∑<em class="no">p</em>(<em class="no">x</em>|<em class="no">C _ k</em>)<em class="no">p</em>(<em class="no">C _ k</em>)，∑超过类标签<em class="no"> k </em>。</p><p id="eaec" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们考虑一个一般化的场景，其中我们的数据有<em class="no"> d </em>个特征和<em class="no"> K </em>个类，那么上面的等式可以写成—</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ns"><img src="../Images/f730257f41b8460e0cc3a5b94db364da.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pahvxAKw-go34PlRK79r5A.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">情商。4:同 Eq。3 但是对于具有 d 特征的数据。</p></figure><p id="abcc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">给定类<em class="no"> Y </em>  <em class="no">，通过“天真地”假设每个数据特征<em class="no"> X_i </em>是相互条件独立的<strong class="lb iu">，可以极大地简化类条件概率项。现在我们重写等式(2.2)，如下所示</strong></em></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nt"><img src="../Images/04823a2a473f976628810b2ae22235d6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VDoTQB3C4-zkDAsyaXQnow.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">情商。5:同 eq。但是对于给定的类 y，数据特征有条件地相互独立。</p></figure><p id="cdbb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这是朴素贝叶斯分类器的基本方程。一旦估计了类别先验分布和类别条件密度，朴素贝叶斯分类器模型就可以为新的数据输入<em class="no"> X </em>做出类别预测<em class="no"> Y </em> ^ (y hat)</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nu"><img src="../Images/02d92e2f938d6f48125a068e65b5fc6b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*f6lc7WcsOft2QHY6GgBl2w.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">情商。6:对新数据点的类别预测。</p></figure><p id="a73e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">由于我们这里的分母<em class="no"> p </em> ( <em class="no"> X_ </em> 1，<em class="no"> X_ </em> 2，…，<em class="no"> X_d </em>)对于给定的输入是常数。我们可以用最大后验概率(MAP)估计来估计<em class="no">p</em>(<em class="no">Y</em>=<em class="no">c _ k</em>)和<em class="no">p</em>(<em class="no">X _ I</em>|<em class="no">Y</em>=<em class="no">c _ k</em>)。前者则是训练集中类别的<em class="no">相对频率</em>。这些在我们后面做编码部分的时候会更加清晰。</p><p id="26e7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">不同的朴素贝叶斯分类器的区别主要在于它们对<em class="no">p</em>(<em class="no">X _ I</em>|<em class="no">Y</em>=<em class="no">c _ k</em>)的分布所做的假设。这对于 MAP 估计非常重要，例如，如果我们假设类别条件是单变量高斯分布，则需要确定的参数是均值和标准差。要估计的参数数量取决于特征和类别。</p></div><div class="ab cl mj mk hx ml" role="separator"><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo"/></div><div class="im in io ip iq"><h2 id="0b82" class="mq mr it bd ms mt mu dn mv mw mx dp my li mz na nb lm nc nd ne lq nf ng nh ni bi translated">连接朴素贝叶斯和逻辑回归:</h2><p id="75b1" class="pw-post-body-paragraph kz la it lb b lc nj ju le lf nk jx lh li nl lk ll lm nm lo lp lq nn ls lt lu im bi translated">代替上述具有<em class="no"> K </em>类的朴素贝叶斯分类器的一般化情况，我们简单地考虑 2 个类，即<em class="no"> Y </em>现在是布尔型(0/1，真/假)。由于逻辑回归(LogReg)是一种判别算法，它从假设<em class="no">P</em>(<em class="no">Y</em>|<em class="no">X</em>)的函数形式开始。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nv"><img src="../Images/e23e3ce27a24d010667bcf10075ca5fa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QcjHWG6FBKhZ44ZRbWH6aw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">情商。7:用于逻辑回归任务的 Sigmoid 函数。</p></figure><p id="c27b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">正如所料，逻辑回归的条件类分布的函数形式基本上是 sigmoid 函数。在另一篇<a class="ae ky" rel="noopener" target="_blank" href="/logit-of-logistic-regression-understanding-the-fundamentals-f384152a33d1">文章</a>中，我详细讨论了如何从线性回归开始实现逻辑回归。可以使用训练数据导出参数(权重<em class="no"> w </em>)。我们可以拓展情商。7，对于 Y(类标签)为布尔型—</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nw"><img src="../Images/46351a5e7efa497d8269bd45deeff609.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eCTL8QrrqVnRws0fQjwD3A.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">情商。8:同 Eq。7，但明确是为二进制分类编写的。</p></figure><p id="10d6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对于给定的 X，要指定 Y=0，我们施加一个简单的条件如下—</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nx"><img src="../Images/0380c7297d5417ff89d647c630871803.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*raU_VL86zuOq6XSk0EkZDg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">在 Eq 上加一个条件。8，并在第二步中对两边应用自然对数。</p></figure><p id="8293" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="no">我们将使用高斯朴素贝叶斯(GNB)分类器，恢复 P(Y|X)的形式，并将其与逻辑回归结果进行比较。</em></p></div><div class="ab cl mj mk hx ml" role="separator"><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo"/></div><div class="im in io ip iq"><h2 id="2bbc" class="mq mr it bd ms mt mu dn mv mw mx dp my li mz na nb lm nc nd ne lq nf ng nh ni bi translated">高斯朴素贝叶斯作为二元分类器；</h2><p id="f39c" class="pw-post-body-paragraph kz la it lb b lc nj ju le lf nk jx lh li nl lk ll lm nm lo lp lq nn ls lt lu im bi translated">在高斯朴素贝叶斯(GNB)分类器中，我们将假设类条件分布<em class="no">p</em>(<em class="no">X _ I</em>|<em class="no">Y</em>=<em class="no">c _ k</em>)是单变量高斯分布。让我们明确地写出假设—</p><ol class=""><li id="8fb7" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ny mb mc md bi translated"><em class="no"> Y </em>有一个布尔形式(即 0/1，真/假)，它由伯努利分布控制。</li><li id="833d" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ny mb mc md bi translated">既然是 GNB，对于类条件句<em class="no">P</em>(<em class="no">X _ I</em>|<em class="no">Y</em>=<em class="no">c _ k</em>)我们假设一元高斯。</li><li id="4914" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ny mb mc md bi translated">对于所有<em class="no"> i </em>和<em class="no">j</em>≦<em class="no">I</em>，<em class="no"> X_i </em>，<em class="no"> X_j </em>都是有条件独立给定<em class="no"> Y </em>。</li></ol><p id="7453" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们用贝叶斯法则写出<em class="no">P</em>(<em class="no">Y</em>= 1 |<em class="no">X</em>)</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nz"><img src="../Images/34f92ea462ce0d0f0c92db9ea98f7e8d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*U4xv71URJIlzjGiXY69-xA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">情商。9:类似于 Eq。4，但只为 Y=1 而写。</p></figure><p id="b559" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们可以通过引入指数和自然对数来进一步简化这个方程，如下所示—</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oa"><img src="../Images/8a2260e630ba1fd835734ade2c304fcb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bmBTeNZO8_Ob-y6wrjdxgA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">情商。10:由等式简化而来。用一种更适合与逻辑回归比较的形式来写它。</p></figure><p id="8bbb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们假设<em class="no">p</em>(<em class="no">y</em>= 1 |<em class="no">x</em>)=<em class="no">π，</em>⟹<em class="no">p</em>(<em class="no">y</em>= 0 |<em class="no">x</em>)= 1-<em class="no">π</em>并且还使用给定类别标签<em class="no"> </em>的数据点的条件独立性来将上面的等式重写如下——</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ob"><img src="../Images/635dc0b18dfdd579ae14ead8b343e1f7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*P1bhJlbZ_f7mbyGxm-oFgA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">情商。11:来自 Eq。10 我们假设<em class="oc">P</em>(<em class="oc">Y</em>= 1 |<em class="oc">X</em>)=<em class="oc">π的先验分布，并使用条件独立性。</em></p></figure><p id="1e06" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对于类别条件<em class="no">P</em>(<em class="no">X _ I</em>|<em class="no">Y</em>=<em class="no">c _ k</em>)我们假设<strong class="lb iu">单变量高斯函数</strong>带参数<em class="no">N</em>(<em class="no">μ_ { ik】</em>，<em class="no"> σ_i </em>)，即标准偏差为我们将用它来简化上面等式的分母。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi od"><img src="../Images/c77370941a6c1e72b37d2176d947ef41.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cox2atiDsFfxtjvW6sl5eQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">情商。12:简化等式中的分母。11 假设类条件句由单变量高斯函数描述。</p></figure><p id="9996" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们可以将该表达式用于之前的等式(等式)。11)以更简洁的形式写出 Y=1 的后验分布，如下所示</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oe"><img src="../Images/53368b838cb299211504d459358cdf93.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Cs6Mh3bUouNx50YrMSDZfQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">情商。13:使用等式中的表达式。12 回到 Eq。11</p></figure><p id="7730" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">把<em class="no">P</em>(<em class="no">Y</em>= 1 |<em class="no">X</em>)写成这种形式，让我们有了直接和 Eq 比较的可能。8，即逻辑回归的条件类分布的函数形式，我们根据高斯均值和标准差获得参数(权重和偏差)，如下所示—</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi of"><img src="../Images/20e0d8a6f32cc8a4984e3700655da2bf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4wA3ttTQScn_ICNflBV9yg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">情商。14:以高斯平均值和标准偏差表示的逻辑回归参数。</p></figure><p id="971b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这里，我们从高斯朴素贝叶斯分布出发，得出了逻辑回归的生成公式，并且，对于给定的二元分类问题，我们还可以找到这两种分布的参数之间的关系。</p></div><div class="ab cl mj mk hx ml" role="separator"><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo"/></div><div class="im in io ip iq"><h2 id="250c" class="mq mr it bd ms mt mu dn mv mw mx dp my li mz na nb lm nc nd ne lq nf ng nh ni bi translated">实现高斯朴素贝叶斯；循序渐进:</h2><p id="3ba8" class="pw-post-body-paragraph kz la it lb b lc nj ju le lf nk jx lh li nl lk ll lm nm lo lp lq nn ls lt lu im bi translated">理解上述步骤的最佳方式是实现它们，为此，我将使用<a class="ae ky" href="https://github.com/allisonhorst/palmerpenguins" rel="noopener ugc nofollow" target="_blank"> Palmer Penguin 数据集</a>，它与 Iris 数据集非常相似。让我们从导入必要的库和加载数据集开始。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi og"><img src="../Images/a8f4fb841fbf70d1b116afd00d5ee293.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gf2wVcm0T0_zKZEE37afnA.png"/></div></div></figure><p id="d094" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这是一个数据集，包括 3 种不同的企鹅‘阿德利’、‘巴布亚’和‘下颚带’，以及一些特征，如喙长、鳍长等。都是给定的。我们可以看到企鹅类基于两个参数“喙长度”和“喙深度”的分布，使用简单的一行代码如下—</p><pre class="kj kk kl km gt oh oi oj ok aw ol bi"><span id="1120" class="mq mr it oi b gy om on l oo op">penguin_nonan_df = penguin.dropna(how=’any’, axis=0, inplace=False)<br/>sns_fgrid=sns.FacetGrid(penguin_nonan_df, hue=”species”, height=6).map(plt.scatter, “bill_length_mm”, “bill_depth_mm”).add_legend()</span><span id="59c2" class="mq mr it oi b gy oq on l oo op">plt.xlabel(‘Bill Length (mm)’, fontsize=12)</span><span id="3d94" class="mq mr it oi b gy oq on l oo op">plt.ylabel(‘Bill Depth (mm)’, fontsize=12)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi or"><img src="../Images/b7e07d63a58257674944e800a94d5e74.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1UIZjC5mUyb-byFSDyDFbA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图 1:基于 2 个不同参数的不同企鹅种类的分布。(来源:作者笔记本)</p></figure><p id="7c9c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为了简单起见，我们将只选择这两个特性，而不是使用所有可用的特性。</p><pre class="kj kk kl km gt oh oi oj ok aw ol bi"><span id="c54d" class="mq mr it oi b gy om on l oo op">penguin_nonan_selected_df = penguin_nonan_df[['species', 'bill_length_mm', 'bill_depth_mm']]</span><span id="7f79" class="mq mr it oi b gy oq on l oo op">X=penguin_nonan_selected_df.drop(['species'], axis=1)</span><span id="22f4" class="mq mr it oi b gy oq on l oo op">Y=penguin_nonan_selected_df['species']</span><span id="5ad7" class="mq mr it oi b gy oq on l oo op">from sklearn.model_selection import train_test_split</span><span id="cad0" class="mq mr it oi b gy oq on l oo op">X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=30, stratify=Y)</span><span id="3af1" class="mq mr it oi b gy oq on l oo op">print (X_train.shape, y_train.shape, X_test.shape)</span><span id="af2a" class="mq mr it oi b gy oq on l oo op">&gt;&gt;&gt; (266, 2) (266,) (67, 2)</span><span id="4b57" class="mq mr it oi b gy oq on l oo op">X_train = X_train.to_numpy()<br/>X_test = X_test.to_numpy()</span><span id="5c98" class="mq mr it oi b gy oq on l oo op">y_train = y_train.to_numpy()<br/>y_test = y_test.to_numpy()</span></pre><p id="4fc2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">要从头开始构建朴素贝叶斯分类器，首先，我们需要定义类的先验分布，在这种情况下，我们将计算唯一类的数量，然后除以样本总数，以获得类的先验分布。我们可以定义如下—</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="os ot l"/></div></figure><p id="ffde" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们打印基于训练样本的先验分布—</p><pre class="kj kk kl km gt oh oi oj ok aw ol bi"><span id="3454" class="mq mr it oi b gy om on l oo op">prior = get_prior(y_train)</span><span id="dce4" class="mq mr it oi b gy oq on l oo op">print (prior)</span><span id="7beb" class="mq mr it oi b gy oq on l oo op">print (‘check prior probs: ‘, prior.probs,)</span><span id="b61e" class="mq mr it oi b gy oq on l oo op">&gt;&gt;&gt; [0.43984962 0.20300752 0.35714286]<br/>tfp.distributions.Categorical("Categorical", batch_shape=[], event_shape=[], dtype=int32)<br/>&gt;&gt;&gt; check prior probs:  tf.Tensor([0.43984962 0.20300752 0.35714286], shape=(3,), dtype=float64)</span></pre><p id="f8d8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">定义先验分布后，我们将定义类别条件分布，正如我之前所述，对于高斯 NB，我们将假设单变量高斯分布，如下式所示。记住，对于我们的问题，我们有 2 个特性和 3 个类。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ou"><img src="../Images/40879ad5a242c8fa6e13fdfaedfc01cc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Lp3el1YSrs-vBeXhiN8y7w.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">情商。15:高斯分布作为类条件</p></figure><p id="2321" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为了定义类别条件，我们需要知道均值和方差，对于正态分布，它们可以很容易地计算出来，查看这里的<a class="ae ky" href="https://www.statlect.com/fundamentals-of-statistics/normal-distribution-maximum-likelihood" rel="noopener ugc nofollow" target="_blank"/>。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ov"><img src="../Images/2b4248c2f3e00a5620c017209ae892df.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8bWMQKwb9PmI5JMw_V2p1w.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">情商。16:等式中类别条件分布的均值和方差。15:</p></figure><p id="2ee5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们把它们放在一起，定义类条件函数。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="os ot l"/></div></figure><p id="5b8f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">再次检查代码块，理解并领会它确实给了我们 Eq。15 使用等式的平均值和方差。16.我们还可以打印来检查训练数据的均值和方差—</p><pre class="kj kk kl km gt oh oi oj ok aw ol bi"><span id="fb23" class="mq mr it oi b gy om on l oo op">class_conditionals, mu, sigma2 = class_conditionals_MLE(X_train, y_train)</span><span id="df91" class="mq mr it oi b gy oq on l oo op">print (class_conditionals)</span><span id="8243" class="mq mr it oi b gy oq on l oo op">print (‘check mu and variance: ‘, ‘\n’)</span><span id="7c6b" class="mq mr it oi b gy oq on l oo op">print (‘mu: ‘, mu, )</span><span id="8fae" class="mq mr it oi b gy oq on l oo op">print (‘sigma2: ‘, sigma2, )</span><span id="115d" class="mq mr it oi b gy oq on l oo op"># batch shape : 3 classes, event shape : 2 features</span><span id="5bb0" class="mq mr it oi b gy oq on l oo op">&gt;&gt;&gt; tfp.distributions.MultivariateNormalDiag("MultivariateNormalDiag", batch_shape=[3], event_shape=[2], dtype=float64)</span><span id="5cc9" class="mq mr it oi b gy oq on l oo op">&gt;&gt;&gt; check mu and variance:<br/>&gt;&gt;&gt; mu:  [ListWrapper([39.017094017093996, 18.389743589743592]), ListWrapper([49.12592592592592, 18.479629629629624]), ListWrapper([48.063157894736854, 15.058947368421052])]<br/>&gt;&gt;&gt; sigma2:  [[7.205861640733441, 1.5171597633136102], [10.038216735253773, 1.1042146776406032], [9.476642659279785, 0.9687357340720222]]</span></pre><p id="5517" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">最后，我们在测试集上进行预测。因此，我们逐个样本地选择，并使用来自先验概率和类别条件的信息。这里我们构建一个函数来表示 Eq。5 和情商。这是我们在文章开头写的。</p><pre class="kj kk kl km gt oh oi oj ok aw ol bi"><span id="bd0d" class="mq mr it oi b gy om on l oo op">def predict_class(prior_dist, class_cond_dist, x):</span><span id="3cf9" class="mq mr it oi b gy oq on l oo op">“””</span><span id="bf6d" class="mq mr it oi b gy oq on l oo op">We will use prior distribution (P(Y|C)), class-conditional distribution(P(X|Y)),</span><span id="ea18" class="mq mr it oi b gy oq on l oo op">and test data-set with shape (batch_shape, 2).</span><span id="45d5" class="mq mr it oi b gy oq on l oo op">“””</span><span id="605f" class="mq mr it oi b gy oq on l oo op">   y = np.zeros((x.shape[0]), dtype=int)</span><span id="2e89" class="mq mr it oi b gy oq on l oo op">   for i, train_point in enumerate(x):</span><span id="00e9" class="mq mr it oi b gy oq on l oo op">       likelihood = tf.cast(class_cond_dist.prob(train_point), dtype=tf.float32) #class_cond_dist.prob has dtype float64</span><span id="6f20" class="mq mr it oi b gy oq on l oo op">       prior_prob = tf.cast(prior_dist.probs, dtype=tf.float32)</span><span id="2782" class="mq mr it oi b gy oq on l oo op">       numerator = likelihood * prior_prob</span><span id="b63d" class="mq mr it oi b gy oq on l oo op">       denominator = tf.reduce_sum(numerator)</span><span id="94b7" class="mq mr it oi b gy oq on l oo op">       P = tf.math.divide(numerator, denominator) # till eq. 5</span><span id="d1e6" class="mq mr it oi b gy oq on l oo op">       #print (‘check posterior shape: ‘, P.shape)</span><span id="b079" class="mq mr it oi b gy oq on l oo op">       Y = tf.argmax(P) # exact similar to np.argmax [get the class]<br/>       # back to eq. 6</span><span id="bc23" class="mq mr it oi b gy oq on l oo op">       y[i] = int(Y)</span><span id="1e02" class="mq mr it oi b gy oq on l oo op">   return y</span></pre><p id="a29c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们可以预测测试数据点的类别，并将它们与原始标签进行比较</p><pre class="kj kk kl km gt oh oi oj ok aw ol bi"><span id="860a" class="mq mr it oi b gy om on l oo op">predictions = predict_class(prior, class_conditionals, X_test)</span></pre><p id="a98c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为测试数据点绘制决策边界(使用等高线图)将产生下图</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ow"><img src="../Images/28e6feab1ed4384c2024145fa65e1198.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aw7hJjQOL7iIdCLpms9-PQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图 2:使用 GNB 分类器的测试数据集的决策区域。(来源:作者笔记本)</p></figure><p id="603b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">以上所有任务都可以用 Sklearn 完成，只需要几行代码—</p><pre class="kj kk kl km gt oh oi oj ok aw ol bi"><span id="a5e9" class="mq mr it oi b gy om on l oo op">from sklearn.naive_bayes import GaussianNB</span><span id="56d4" class="mq mr it oi b gy oq on l oo op">sklearn_GNB = GaussianNB()<br/>sklearn_GNB.fit(X_train, y_train)</span><span id="b679" class="mq mr it oi b gy oq on l oo op">predictions_sklearn = sklearn_GNB.predict(X_test)</span></pre><p id="0461" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">了解这些基本算法的幕后工作总是好的，我们可以比较与我们之前获得的参数非常接近的拟合参数—</p><pre class="kj kk kl km gt oh oi oj ok aw ol bi"><span id="639d" class="mq mr it oi b gy om on l oo op">print ('variance:',  sklearn_GNB.var_)</span><span id="d9ec" class="mq mr it oi b gy oq on l oo op">print ('mean: ', sklearn_GNB.theta_)</span><span id="d07a" class="mq mr it oi b gy oq on l oo op">&gt;&gt;&gt; variance: [[ 7.20586167  1.51715979]<br/> [10.03821677  1.10421471]<br/> [ 9.47664269  0.96873576]]<br/>mean:  [[39.01709402 18.38974359]<br/> [49.12592593 18.47962963]<br/> [48.06315789 15.05894737]]</span></pre><p id="054e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们绘制决策区域，以便与我们的硬编码 GNB 分类器进行比较—</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ow"><img src="../Images/b6feb1d6fe38a5d5633a12bce7db1fa4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RxzBVvRbl6yaTmOlW9-PHg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图 3:与图 2 相同，但结果来自使用 Sklearn 库。(来源:作者笔记本)</p></figure><p id="be10" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">他们看起来和预期的非常相似。让我们移动到明确连接 GNB 和逻辑回归分类器的二元分类问题。</p></div><div class="ab cl mj mk hx ml" role="separator"><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo"/></div><div class="im in io ip iq"><h2 id="4d50" class="mq mr it bd ms mt mu dn mv mw mx dp my li mz na nb lm nc nd ne lq nf ng nh ni bi translated">来自 GNB 的逻辑回归参数:</h2><p id="0d49" class="pw-post-body-paragraph kz la it lb b lc nj ju le lf nk jx lh li nl lk ll lm nm lo lp lq nn ls lt lu im bi translated">如前所述，为了将朴素贝叶斯和逻辑回归联系起来，我们会想到二元分类。由于企鹅数据集中有 3 个类，首先，我们将问题转化为一个 vs rest 分类器，然后确定逻辑回归参数。同样关于 GNB 作为二元分类器的推导，我们将使用<em class="no">相同的σ_i 用于两个不同的类</em>(没有 k 依赖性)。这里我们会找到 6 个参数。4 为<em class="no"> μ_{ik} </em> &amp; 2 为<em class="no"> σ_i </em>。这排除了使用我们之前使用的类条件函数的可能性，因为在标准差公式中使用了<em class="no"> μ_{ik} </em>，并且均值取决于类和特征。所以我们要写一个函数，用梯度下降型优化来学习类独立标准差。这是主要的编码任务。</p><p id="3f0b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在所有这些之前，我们需要对标签进行二值化，为此，我们将对标签为 2 的样本使用标签 1。</p><pre class="kj kk kl km gt oh oi oj ok aw ol bi"><span id="512c" class="mq mr it oi b gy om on l oo op">class_types_dict = {'Adelie':0, 'Chinstrap':1, 'Gentoo':2}</span><span id="cbf7" class="mq mr it oi b gy oq on l oo op">y_train_cat = np.vectorize(class_types_dict.get)(y_train)</span><span id="d216" class="mq mr it oi b gy oq on l oo op">y_train_cat_binary = y_train_cat</span><span id="d4c5" class="mq mr it oi b gy oq on l oo op">y_train_cat_binary[np.where(y_train_cat_binary == 2)] = 1 <br/># gentoo == chinstrap</span><span id="a0d6" class="mq mr it oi b gy oq on l oo op">y_test_cat = np.vectorize(class_types_dict.get)(y_test)</span><span id="b57b" class="mq mr it oi b gy oq on l oo op">y_test_cat_binary = np.array(y_test_cat)</span><span id="1b14" class="mq mr it oi b gy oq on l oo op">y_test_cat_binary[np.where(y_test_cat_binary == 2)] = 1</span><span id="25c0" class="mq mr it oi b gy oq on l oo op">print (‘check shapes: ‘, y_train_cat_binary.shape, y_test_cat_binary.shape, y_train_cat_binary.dtype, ‘\n’, y_train_cat.shape)</span><span id="beb9" class="mq mr it oi b gy oq on l oo op">&gt;&gt;&gt; check shapes:  (266,) (67,) int64<br/>&gt;&gt;&gt; (266,)</span></pre><p id="e7ba" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们可以绘制数据分布图，如下图所示—</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ow"><img src="../Images/552b78475998a70cf25a90051cf9441f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZZTq7vNEUXFZdvZu310JDQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图 4:对巴布亚企鹅和下颚带企鹅使用相同的分类标签，将问题转化为二元分类。(来源:作者笔记本)。</p></figure><p id="0a0b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们可以使用先验分布函数来获得这两个类别的类别标签先验</p><pre class="kj kk kl km gt oh oi oj ok aw ol bi"><span id="c1ff" class="mq mr it oi b gy om on l oo op">prior_binary = get_prior(y_train_cat_binary)</span><span id="f70e" class="mq mr it oi b gy oq on l oo op">print (prior_binary.probs, print (type(prior_binary)))</span><span id="37e1" class="mq mr it oi b gy oq on l oo op">&gt;&gt;&gt; [0.43984962 0.56015038]<br/>&lt;class 'tensorflow_probability.python.distributions.categorical.Categorical'&gt;<br/>tf.Tensor([0.43984962 0.56015038], shape=(2,), dtype=float64) None</span></pre><p id="ff4b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">与之前的先验分布相比，我们会看到 Adelie penguin 类具有相同的先验(0.43)，并预期增加 Gentoo 和 Chinstrap 先验分布(≈ 0.20 + 0.35)。</p><p id="4a2b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为了学习标准差，我们将在给定数据和标签的情况下，使用梯度下降来最小化负对数似然。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="os ot l"/></div></figure><p id="4c6e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">一旦训练结束，我们就可以检索参数—</p><pre class="kj kk kl km gt oh oi oj ok aw ol bi"><span id="3acd" class="mq mr it oi b gy om on l oo op">print(class_conditionals_binary.loc.numpy())</span><span id="4ae8" class="mq mr it oi b gy oq on l oo op">print (class_conditionals_binary.covariance().numpy())</span></pre><p id="299b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">使用类条件句，我们可以绘制如下等高线——</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ox"><img src="../Images/e3885bd6f9ceea112912cc7f486dbc1d.png" data-original-src="https://miro.medium.com/v2/resize:fit:994/format:webp/1*C5Wq-XffTVUtJD_maoztOg.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图 5:2 类[0，1]的类条件轮廓。(来源:作者笔记本)。</p></figure><p id="0a52" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们也可以画出二元 GNB 分类器的判定边界——</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oy"><img src="../Images/c26cc4a32069598a81ca8f74d2a607bf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1152/format:webp/1*YON5sxQLixUYMUfSb14SfA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图 6:使用 GNB 分类器的二元分类的判定边界。</p></figure><p id="c37c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">一旦我们有了均值和对角协方差矩阵，我们就可以为逻辑回归寻找参数了。权重和偏差参数是使用等式中的均值和协方差导出的。14，让我们再重写一遍—</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oz"><img src="../Images/6b4b23ab3ab36d28621daba0fa0eb9df.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fGf2oUxPyMIhZM30iJTRmw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">情商。17:用均值和协方差矩阵表示的逻辑回归参数。</p></figure><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="os ot l"/></div></figure><p id="0ceb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">就这样，我们到了帖子的末尾！涵盖了许多重要的基本概念，希望这对您有所帮助。</p></div><div class="ab cl mj mk hx ml" role="separator"><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo"/></div><div class="im in io ip iq"><h2 id="ac2f" class="mq mr it bd ms mt mu dn mv mw mx dp my li mz na nb lm nc nd ne lq nf ng nh ni bi translated">参考资料:</h2><p id="16b3" class="pw-post-body-paragraph kz la it lb b lc nj ju le lf nk jx lh li nl lk ll lm nm lo lp lq nn ls lt lu im bi translated">[1] <a class="ae ky" href="https://allisonhorst.github.io/palmerpenguins/" rel="noopener ugc nofollow" target="_blank">企鹅数据集</a>，CC-0 许可，可自由改编。</p><p id="dc42" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">[2]机器学习的书；卡内基梅隆大学的汤姆·米切尔；<a class="ae ky" href="http://www.cs.cmu.edu/%7Etom/NewChapters.html" rel="noopener ugc nofollow" target="_blank">新章节</a>。</p><p id="9827" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">[3]本帖使用的完整笔记本。<a class="ae ky" href="https://github.com/suvoooo/Machine_Learning/blob/master/NB_LogisticReg.ipynb" rel="noopener ugc nofollow" target="_blank"> GitHub 链接</a>。</p><p id="a040" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">保持坚强！！</p></div></div>    
</body>
</html>