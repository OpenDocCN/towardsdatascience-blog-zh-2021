<html>
<head>
<title>SVM with Scikit-Learn: What You Should Know</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">SVM与Scikit-学习:你应该知道什么</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/svm-with-scikit-learn-what-you-should-know-780f1bc99e4a?source=collection_archive---------7-----------------------#2021-07-25">https://towardsdatascience.com/svm-with-scikit-learn-what-you-should-know-780f1bc99e4a?source=collection_archive---------7-----------------------#2021-07-25</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="4c0e" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">为什么LinearSVC和具有线性核的SVC不是同一个函数？</h2></div><p id="6a88" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">为了在scikit-learn中创建线性SVM模型，有两个来自同一个模块<code class="fe le lf lg lh b">svm</code> : <code class="fe le lf lg lh b">SVC</code>和<code class="fe le lf lg lh b">LinearSVC</code>的函数。由于我们想要创建一个具有线性内核的SVM模型，并且我们可以在函数<code class="fe le lf lg lh b">LinearSVC</code>的名称中读取<code class="fe le lf lg lh b">Linear</code>，我们自然选择使用这个函数。但事实证明，我们也可以将<code class="fe le lf lg lh b">SVC</code>与参数<code class="fe le lf lg lh b">kernel='linear'</code>一起使用。</p><p id="d6ec" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在的问题是:<code class="fe le lf lg lh b">LinearSVC</code>和<code class="fe le lf lg lh b">SVC(kernel='linear’)</code>是同一个功能吗？如果不是，哪一个是真正的SVM模式？</p><p id="2c3a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">(如果你不知道什么是SVM，那么本文末尾有一个链接，可以非常直观地定义一个SVM。)</p><figure class="lj lk ll lm gt ln gh gi paragraph-image"><div role="button" tabindex="0" class="lo lp di lq bf lr"><div class="gh gi li"><img src="../Images/10cf7a39d7cbf20658f60d5eab4b8fd9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*tOAHh4bLj3ZRqX9x"/></div></div><p class="lu lv gj gh gi lw lx bd b be z dk translated">乔纳森·博尔巴在<a class="ae ly" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><h1 id="13d3" class="lz ma it bd mb mc md me mf mg mh mi mj jz mk ka ml kc mm kd mn kf mo kg mp mq bi translated">经验方法</h1><p id="34b2" class="pw-post-body-paragraph ki kj it kk b kl mr ju kn ko ms jx kq kr mt kt ku kv mu kx ky kz mv lb lc ld im bi translated">为了找出差异，可以有两种不同的方法:</p><ul class=""><li id="0a4e" class="mw mx it kk b kl km ko kp kr my kv mz kz na ld nb nc nd ne bi translated">理论方法:我们可以阅读文档，找出与这些函数相关的数学公式的不同之处</li><li id="390b" class="mw mx it kk b kl nf ko ng kr nh kv ni kz nj ld nb nc nd ne bi translated">经验方法:我们可以对这两个函数做一些测试，我们可以看看它们的结果是否有任何不同</li></ul><p id="847c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们从经验方法开始。</p><h2 id="a3a6" class="nk ma it bd mb nl nm dn mf nn no dp mj kr np nq ml kv nr ns mn kz nt nu mp nv bi translated">数据集</h2><p id="3d3d" class="pw-post-body-paragraph ki kj it kk b kl mr ju kn ko ms jx kq kr mt kt ku kv mu kx ky kz mv lb lc ld im bi translated">为了直观显示差异，数据应该简单。并且有必要测试数据中的不同行为。这里有一个将帮助我们:这个数据集有两个线性可分的类；然而，有一个异常点(与真实的类1)实际上接近于类1。让我们看看这两个函数将如何处理这个异常点。</p><pre class="lj lk ll lm gt nw lh nx ny aw nz bi"><span id="7f78" class="nk ma it lh b gy oa ob l oc od">X=np.array([1,3,4,5,6,9,20,21,23,24]).reshape(-1,1)</span><span id="b316" class="nk ma it lh b gy oe ob l oc od">y=np.array([-1,-1,-1,-1,-1,1,1,1,1,1])</span><span id="01f6" class="nk ma it lh b gy oe ob l oc od">plt.scatter(X,y)</span></pre><figure class="lj lk ll lm gt ln gh gi paragraph-image"><div role="button" tabindex="0" class="lo lp di lq bf lr"><div class="gh gi of"><img src="../Images/cbf7d0ce2d7d3f5a16293cdf3135ef78.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2ZJasVTZMtrttJnuBbGQZw.png"/></div></div><p class="lu lv gj gh gi lw lx bd b be z dk translated">作者图片</p></figure><h2 id="c8a4" class="nk ma it bd mb nl nm dn mf nn no dp mj kr np nq ml kv nr ns mn kz nt nu mp nv bi translated">线性模型</h2><p id="ad81" class="pw-post-body-paragraph ki kj it kk b kl mr ju kn ko ms jx kq kr mt kt ku kv mu kx ky kz mv lb lc ld im bi translated">现在我们可以应用<code class="fe le lf lg lh b">LinearSVC</code>模型:</p><pre class="lj lk ll lm gt nw lh nx ny aw nz bi"><span id="a28f" class="nk ma it lh b gy oa ob l oc od">from sklearn.svm import LinearSVC<br/>linsvc<!-- --> = LinearSVC(C=1)<br/>linsvc<!-- -->.fit(X,y)</span></pre><p id="6171" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们现在可以用橙色线来显示一些测试数据的预测:</p><pre class="lj lk ll lm gt nw lh nx ny aw nz bi"><span id="f2cd" class="nk ma it lh b gy oa ob l oc od">X_test=np.linspace(-1, 30, 300).reshape(-1,1)<br/>plt.scatter(X,y)<br/>plt.scatter(X_test,<!-- -->linsvc<!-- -->.predict(X_test),marker="_")</span></pre><figure class="lj lk ll lm gt ln gh gi paragraph-image"><div role="button" tabindex="0" class="lo lp di lq bf lr"><div class="gh gi of"><img src="../Images/dc00551f7ba5ca9019fd1ae645c845c6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GQl4dB0oS66kbdRJOao1LQ.png"/></div></div><p class="lu lv gj gh gi lw lx bd b be z dk translated">作者图片</p></figure><p id="3191" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">目前，我们看不到任何奇怪的东西:分类似乎是正确的。为了测试模型，让我们使用超参数<code class="fe le lf lg lh b">C</code>的一些极值。让我们回忆一下<code class="fe le lf lg lh b">C</code>的含义:</p><ul class=""><li id="6847" class="mw mx it kk b kl km ko kp kr my kv mz kz na ld nb nc nd ne bi translated">对于<code class="fe le lf lg lh b">C</code>的大值，误分类会被认为是大损失。在我们的例子中，<code class="fe le lf lg lh b">C=1</code>可以被认为是大的，因为异常点被(正确地)分类为1，而它更接近于1类。</li><li id="03ee" class="mw mx it kk b kl nf ko ng kr nh kv ni kz nj ld nb nc nd ne bi translated">对于<code class="fe le lf lg lh b">C</code>的小值，错误分类将被容忍:理论上，异常点应被分类为-1…，让我们看看会发生什么</li></ul><pre class="lj lk ll lm gt nw lh nx ny aw nz bi"><span id="0cc8" class="nk ma it lh b gy oa ob l oc od">linsvc <!-- -->= LinearSVC(C=0.001)<br/>svm_lin.fit(X,y)</span></pre><p id="8466" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们得到以下结果:发生了什么？</p><figure class="lj lk ll lm gt ln gh gi paragraph-image"><div role="button" tabindex="0" class="lo lp di lq bf lr"><div class="gh gi of"><img src="../Images/3bcf497c6756d242a4f9ee96b24b102b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*d7MjnDRnpOlC2-IfI-z21g.png"/></div></div><p class="lu lv gj gh gi lw lx bd b be z dk translated">作者图片</p></figure><p id="ba67" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">不是(理论上)错误分类异常点，而是很多-1被错误分类。</p><p id="c283" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">也许我们对理论的理解是错误的？让我们用<code class="fe le lf lg lh b">SVC</code>做一些测试…</p><h2 id="ce16" class="nk ma it bd mb nl nm dn mf nn no dp mj kr np nq ml kv nr ns mn kz nt nu mp nv bi translated">带SVC的模型</h2><p id="685d" class="pw-post-body-paragraph ki kj it kk b kl mr ju kn ko ms jx kq kr mt kt ku kv mu kx ky kz mv lb lc ld im bi translated">使用相同的数据集，我们可以拟合一个<code class="fe le lf lg lh b">SVC</code>模型:</p><pre class="lj lk ll lm gt nw lh nx ny aw nz bi"><span id="74b3" class="nk ma it lh b gy oa ob l oc od">from sklearn.svm import SVC</span><span id="ac40" class="nk ma it lh b gy oe ob l oc od">linsvc = SVC(kernel = 'linear',C=0.01)</span></pre><p id="0604" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们得到了:</p><figure class="lj lk ll lm gt ln gh gi paragraph-image"><div role="button" tabindex="0" class="lo lp di lq bf lr"><div class="gh gi of"><img src="../Images/dba1765de312612d3bf3d548846861f1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9iiSxMczOYqBLWMP0s9TwQ.png"/></div></div><p class="lu lv gj gh gi lw lx bd b be z dk translated">作者图片</p></figure><p id="e02a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这更符合我们对超参数<code class="fe le lf lg lh b">C</code>在SVM模型中如何工作的理解。</p><h1 id="5d88" class="lz ma it bd mb mc md me mf mg mh mi mj jz mk ka ml kc mm kd mn kf mo kg mp mq bi translated">理论方法</h1><h2 id="8211" class="nk ma it bd mb nl nm dn mf nn no dp mj kr np nq ml kv nr ns mn kz nt nu mp nv bi translated">要验证的数学公式</h2><p id="262c" class="pw-post-body-paragraph ki kj it kk b kl mr ju kn ko ms jx kq kr mt kt ku kv mu kx ky kz mv lb lc ld im bi translated">既然我们可以从经验上看到这两个函数之间的差异，那么让我们更深入地从理论上寻找一些差异。</p><p id="02ec" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">根据各种来源(<a class="ae ly" href="https://scikit-learn.org/stable/modules/svm.html#mathematical-formulation" rel="noopener ugc nofollow" target="_blank"> scikit-learn </a>，<a class="ae ly" href="https://en.wikipedia.org/wiki/Support-vector_machine" rel="noopener ugc nofollow" target="_blank"> Wikipedia </a>，<a class="ae ly" href="https://www.cs.princeton.edu/courses/archive/spring16/cos495/slides/ML_basics_lecture4_SVM_I.pdf" rel="noopener ugc nofollow" target="_blank">普林斯顿SVM讲座</a>，<a class="ae ly" href="https://see.stanford.edu/materials/aimlcs229/cs229-notes3.pdf" rel="noopener ugc nofollow" target="_blank">斯坦福SVM讲座</a>)，损失函数如下(铰链损失):</p><figure class="lj lk ll lm gt ln gh gi paragraph-image"><div role="button" tabindex="0" class="lo lp di lq bf lr"><div class="gh gi og"><img src="../Images/f79e7799d352cee21f4b5f89ae4bd527.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IMXqmmotk-TDV8BHMX1tJg.png"/></div></div></figure><p id="e7f8" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们可以注意到正则化超过了β项，即<strong class="kk iu">不包括</strong>截距。</p><p id="c557" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">由于<code class="fe le lf lg lh b">LinearSVC</code>函数来自<code class="fe le lf lg lh b">LIBLINEAR</code>库，我们可以阅读<a class="ae ly" href="https://github.com/cjlin1/liblinear" rel="noopener ugc nofollow" target="_blank">文档</a>中损失函数定义为:</p><pre class="lj lk ll lm gt nw lh nx ny aw nz bi"><span id="c80c" class="nk ma it lh b gy oa ob l oc od">min_w w^Tw/2 + C \sum max(0, 1- y_i w^Tx_i)^2</span></pre><p id="a6f0" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">w项包括截距，我们可以这样写</p><figure class="lj lk ll lm gt ln gh gi paragraph-image"><div role="button" tabindex="0" class="lo lp di lq bf lr"><div class="gh gi oh"><img src="../Images/4b77d8b00d9f6704c6d66ff51c024d00.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*72GBzbeTj_m3Zl4pTXBUvQ.png"/></div></div></figure><h2 id="b86f" class="nk ma it bd mb nl nm dn mf nn no dp mj kr np nq ml kv nr ns mn kz nt nu mp nv bi translated">模型中的系数</h2><p id="b8f6" class="pw-post-body-paragraph ki kj it kk b kl mr ju kn ko ms jx kq kr mt kt ku kv mu kx ky kz mv lb lc ld im bi translated">现在，我们可以回到python代码，使用以下代码从两个模型中获取系数:</p><pre class="lj lk ll lm gt nw lh nx ny aw nz bi"><span id="4f5e" class="nk ma it lh b gy oa ob l oc od">print(linsvc.coef_)<br/>print(linsvc.intercept_)</span></pre><p id="27e5" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">对于<code class="fe le lf lg lh b">LinearSVC</code>，截距为零，而在<code class="fe le lf lg lh b">SVC</code>中，截距没有被规则化。我们可以通过这张图来比较它们</p><figure class="lj lk ll lm gt ln gh gi paragraph-image"><div role="button" tabindex="0" class="lo lp di lq bf lr"><div class="gh gi oi"><img src="../Images/67577f1a70e2e6800c5c0ee7040e68ef.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bP5qEmzG6mqsEGK5Yfatmg.png"/></div></div></figure><h1 id="b244" class="lz ma it bd mb mc md me mf mg mh mi mj jz mk ka ml kc mm kd mn kf mo kg mp mq bi translated">结论</h1><p id="291b" class="pw-post-body-paragraph ki kj it kk b kl mr ju kn ko ms jx kq kr mt kt ku kv mu kx ky kz mv lb lc ld im bi translated">由于各种包(python，R，spark)的出现，创建机器学习模型似乎变得越来越容易。然而，理解实现的函数背后的真正含义是很重要的。</p><p id="ac69" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在SVM的例子中，我们可以看到来自<code class="fe le lf lg lh b">LIBLINEAR</code>库的<code class="fe le lf lg lh b">LinearSVC</code>使用了一个损失函数，它不同于在来自<code class="fe le lf lg lh b">LIBSVM</code>库的<code class="fe le lf lg lh b">SVC</code>中实现的通常形式。对于高维，截距项的重要性会降低，但是对于小维，正则化截距项会导致不正确的结果。</p></div></div>    
</body>
</html>