<html>
<head>
<title>Do Vision Transformers See Like Convolutional Neural Networks? (Paper Explained)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">视觉变形器看起来像卷积神经网络吗？(论文解释)</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/do-vision-transformers-see-like-convolutional-neural-networks-paper-explained-91b4bd5185c8?source=collection_archive---------2-----------------------#2021-10-09">https://towardsdatascience.com/do-vision-transformers-see-like-convolutional-neural-networks-paper-explained-91b4bd5185c8?source=collection_archive---------2-----------------------#2021-10-09</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="c1bd" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsdatascience.com/tagged/thoughts-and-theory" rel="noopener" target="_blank">思想和理论</a></h2><div class=""/><p id="0b19" class="pw-post-body-paragraph jz ka it kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw im bi translated">视觉变形金刚(ViT)近年来势头越来越猛。本文将解释论文<em class="kx">“视觉变形金刚看起来像卷积神经网络吗？”</em> ( <a class="ae ky" href="https://arxiv.org/abs/2108.08810" rel="noopener ugc nofollow" target="_blank"> Raghu等人，2021 </a>)由Google Research和Google Brain发布，并探讨常规使用的CNN和Vision Transformer的区别。</p></div><div class="ab cl kz la hx lb" role="separator"><span class="lc bw bk ld le lf"/><span class="lc bw bk ld le lf"/><span class="lc bw bk ld le"/></div><div class="im in io ip iq"><h1 id="097e" class="lg lh it bd li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md bi translated">本文摘要和博客内容</h1><p id="17e4" class="pw-post-body-paragraph jz ka it kb b kc me ke kf kg mf ki kj kk mg km kn ko mh kq kr ks mi ku kv kw im bi translated">本文有六个中心摘要:ResNet ( <a class="ae ky" href="https://openaccess.thecvf.com/content_cvpr_2016/html/He_Deep_)) as a representative of CNN-based networks. Residual_Learning_CVPR_2016_paper.html" rel="noopener ugc nofollow" target="_blank"> He et al .，2016 </a>)和ViT为代表的基于CNN的网络。</p><ol class=""><li id="f4e1" class="mj mk it kb b kc kd kg kh kk ml ko mm ks mn kw mo mp mq mr bi translated"><strong class="kb jd"> <em class="kx">与CNN</em></strong>相比，ViT在浅层和深层获得的表征之间具有更大的相似性</li><li id="6d02" class="mj mk it kb b kc ms kg mt kk mu ko mv ks mw kw mo mp mq mr bi translated"><strong class="kb jd"> <em class="kx">与CNN不同，ViT从浅层获得全局表示，但从浅层获得的局部表示也很重要。</em>T15】</strong></li><li id="6b76" class="mj mk it kb b kc ms kg mt kk mu ko mv ks mw kw mo mp mq mr bi translated"><strong class="kb jd"><em class="kx">ViT中的Skip连接甚至比CNN(ResNet)中的影响更大，并对表示的性能和相似性产生实质性影响。</em>T19】</strong></li><li id="a2f5" class="mj mk it kb b kc ms kg mt kk mu ko mv ks mw kw mo mp mq mr bi translated"><strong class="kb jd"> <em class="kx"> ViT比ResNet </em> </strong>保留更多的空间信息</li><li id="7f1b" class="mj mk it kb b kc ms kg mt kk mu ko mv ks mw kw mo mp mq mr bi translated"><strong class="kb jd"> <em class="kx"> ViT可以学习大量数据的高质量中间表征</em> </strong></li><li id="3ea6" class="mj mk it kb b kc ms kg mt kk mu ko mv ks mw kw mo mp mq mr bi translated"><strong class="kb jd"> <em class="kx"> MLP混频器的表示更接近ViT而不是ResNet </em> </strong></li></ol><p id="aa89" class="pw-post-body-paragraph jz ka it kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw im bi translated">在这篇博客中，我将首先简要回顾ResNet和ViT的结构，它们是基于CNN的模型的代表性示例，然后进一步研究本文中描述的所获得的表示的差异。</p><h2 id="225c" class="mx lh it bd li my mz dn lm na nb dp lq kk nc nd lu ko ne nf ly ks ng nh mc iz bi translated">ResNet基础知识</h2><p id="cfa7" class="pw-post-body-paragraph jz ka it kb b kc me ke kf kg mf ki kj kk mg km kn ko mh kq kr ks mi ku kv kw im bi translated">ResNet是计算机视觉(CV)任务的流行模型。如下面的图2所示，ResNet的加权传播端使用跳过一层权重的skip连接进行求和。具有跳跃连接的求和过程减轻了诸如梯度消失的问题，并且允许比先前网络更深的层。</p><figure class="nj nk nl nm gt nn gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/1986fde825bdccde4f646ed54edcdda7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1136/format:webp/1*uC2m-XicjqtCOSjk_zkegg.png"/></div><p class="nq nr gj gh gi ns nt bd b be z dk translated">ResNet架构来自<a class="ae ky" href="https://openaccess.thecvf.com/content_cvpr_2016/html/He_Deep_)) as a representative of CNN-based networks. Residual_Learning_CVPR_2016_paper.html" rel="noopener ugc nofollow" target="_blank">何等，2016 </a></p></figure><h1 id="b65d" class="lg lh it bd li lj nu ll lm ln nv lp lq lr nw lt lu lv nx lx ly lz ny mb mc md bi translated">视觉转换器(ViT)基础知识</h1><p id="08f4" class="pw-post-body-paragraph jz ka it kb b kc me ke kf kg mf ki kj kk mg km kn ko mh kq kr ks mi ku kv kw im bi translated">首先，让我们来看看视觉变压器(ViT)中使用的变压器编码器。</p><h2 id="f97d" class="mx lh it bd li my mz dn lm na nb dp lq kk nc nd lu ko ne nf ly ks ng nh mc iz bi translated">变压器</h2><p id="5ebe" class="pw-post-body-paragraph jz ka it kb b kc me ke kf kg mf ki kj kk mg km kn ko mh kq kr ks mi ku kv kw im bi translated">变压器是在论文《注意力是你所需要的全部》(<a class="ae ky" href="https://arxiv.org/abs/1706.03762" rel="noopener ugc nofollow" target="_blank"> Vaswani et al .，2017 </a>)中提出的模型。它是一个使用称为自我关注的机制的模型，这种机制既不是CNN，也不是LSTM，并建立了Transformer模型，以显著优于现有的方法。结果比现有的方法好得多。</p><p id="d5e1" class="pw-post-body-paragraph jz ka it kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw im bi translated">注意下图中标有多头注意的部分是Transformer的核心部分，但它也像ResNet一样使用了skip-joining。</p><figure class="nj nk nl nm gt nn gh gi paragraph-image"><div class="gh gi nz"><img src="../Images/fba7630f203f8ae7b39025b175fd5bc5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1258/format:webp/1*Tux9sHmid8uV2EGdo5u5UQ.png"/></div><p class="nq nr gj gh gi ns nt bd b be z dk translated">变压器架构。来自<a class="ae ky" href="https://arxiv.org/abs/1706.03762" rel="noopener ugc nofollow" target="_blank">瓦斯瓦尼等人，2017 </a></p></figure><p id="f955" class="pw-post-body-paragraph jz ka it kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw im bi translated">变压器中使用的注意机制使用了三个变量:<strong class="kb jd"> <em class="kx"> Q </em> </strong>(查询)、<strong class="kb jd"> <em class="kx"> K </em> </strong>(键)、和<strong class="kb jd"> <em class="kx"> V </em> </strong>(值)。简单地说，它计算一个查询标记(标记:类似于一个单词)和一个键标记的注意力权重，并乘以与每个键相关联的值。简而言之，它计算查询令牌和密钥令牌之间的关联(注意力权重),并乘以与每个密钥相关联的值。</p><figure class="nj nk nl nm gt nn gh gi paragraph-image"><div role="button" tabindex="0" class="ob oc di od bf oe"><div class="gh gi oa"><img src="../Images/0a6936795e79a155f3b0fd016d9458c5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qbIed45YC54m3TaDWFFcUw.png"/></div></div><p class="nq nr gj gh gi ns nt bd b be z dk translated">(单头)自我关注</p></figure><p id="ea53" class="pw-post-body-paragraph jz ka it kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw im bi translated">将<strong class="kb jd"> Q，K，V </strong>计算定义为单头，多头注意机制定义如下。上图中的(单头)注意机制原样使用了<strong class="kb jd"> <em class="kx"> Q </em> </strong>和<strong class="kb jd"> <em class="kx"> K </em> </strong>。尽管如此，在多头注意力机制中，每个头都有其投影矩阵<strong class="kb jd"><em class="kx"/></strong><strong class="kb jd"><em class="kx"/></strong><strong class="kb jd"><em class="kx">【w_i^v</em></strong>，并且它们使用使用这些矩阵投影的特征值来计算注意力权重。</p><figure class="nj nk nl nm gt nn gh gi paragraph-image"><div role="button" tabindex="0" class="ob oc di od bf oe"><div class="gh gi oa"><img src="../Images/8119338168c7017164702365a94b14a5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xUXvP-zO9tJ8kJUyqyeN0Q.png"/></div></div><p class="nq nr gj gh gi ns nt bd b be z dk translated">多头注意力</p></figure><p id="9e8e" class="pw-post-body-paragraph jz ka it kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw im bi translated">如果这个注意机制中使用的<strong class="kb jd"> <em class="kx"> Q，K，V </em> </strong>都是从同一个输入计算出来的，则专门称为自我注意。另一方面，Transformer解码器的上部不是一个“自我”注意力机制，因为它使用来自编码器的<strong class="kb jd"> <em class="kx"> Q </em> </strong>和来自解码器的<strong class="kb jd"> <em class="kx"> K </em> </strong>和<strong class="kb jd"> <em class="kx"> V </em> </strong>来计算注意力。</p><p id="daad" class="pw-post-body-paragraph jz ka it kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw im bi translated">实际应用的图像如下图所示。该图示出了使用单词“making”作为查询为每个关键令牌计算的注意力权重的可视化。转换器使用多头自关注机制传播到后面的层，每个头学习不同的依赖关系。下图中的关键词是彩色的，代表每个头部的注意力权重。</p><figure class="nj nk nl nm gt nn gh gi paragraph-image"><div role="button" tabindex="0" class="ob oc di od bf oe"><div class="gh gi of"><img src="../Images/f5d1496553211e8cc09241ea569b8bc1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zecJbaAAWYHMnTjUaD8mKw.png"/></div></div><p class="nq nr gj gh gi ns nt bd b be z dk translated">注意力权重可视化。从<a class="ae ky" href="https://arxiv.org/abs/1706.03762" rel="noopener ugc nofollow" target="_blank"> Vaswani等人，2017 </a>引用的图像，我已经注释过了。</p></figure><p id="de1c" class="pw-post-body-paragraph jz ka it kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw im bi translated">###视觉转换器(ViT)</p><p id="6247" class="pw-post-body-paragraph jz ka it kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw im bi translated">视觉转换器(ViT)是一种将转换器应用于图像分类任务的模型，于2020年10月提出(<a class="ae ky" href="https://arxiv.org/abs/2010.11929" rel="noopener ugc nofollow" target="_blank"> Dosovitskiy等人2020 </a>)。模型架构与原始的Transformer几乎相同，但有一点不同，它允许将图像作为输入处理，就像自然语言处理一样。</p><figure class="nj nk nl nm gt nn gh gi paragraph-image"><div role="button" tabindex="0" class="ob oc di od bf oe"><div class="gh gi og"><img src="../Images/b628464a5943cd01b5337dcc14baf936.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jvrltrmKP61ywMC2nSq3zw.png"/></div></div><p class="nq nr gj gh gi ns nt bd b be z dk translated">视觉转换器架构。图片引自<a class="ae ky" href="https://arxiv.org/abs/2010.11929" rel="noopener ugc nofollow" target="_blank"> Dosovitskiy et al. 2020 </a>，我已经做了注释。</p></figure><p id="a7fd" class="pw-post-body-paragraph jz ka it kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw im bi translated">首先，ViT将图像分成N个“小块”,例如16x16。由于面片本身是3D数据(高x宽x通道数)，它们不能由处理语言(2D)的转换器直接处理，因此它将它们展平并进行线性投影，以将其转换为2D数据。因此，每个补丁都可以被视为一个令牌，可以输入到转换器中。</p><p id="98b3" class="pw-post-body-paragraph jz ka it kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw im bi translated">此外，ViT采用先预训练后微调的策略。ViT使用包含3亿张图像的数据集JFT-300M进行预训练，然后在ImageNet等下游任务上进行微调。ViT是第一个在ImageNet上实现SotA性能的纯变形金刚模型，这导致了对变形金刚应用于计算机视觉任务的研究大幅增加。</p><p id="0bed" class="pw-post-body-paragraph jz ka it kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw im bi translated">但是，训练ViT需要大量的数据。变压器在数据较少的情况下不太准确，但在数据较多的情况下变得更加准确，并且在JFT-300M上进行预训练时表现优于CNN。更多详情请参考原论文。</p><figure class="nj nk nl nm gt nn gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/0b407017a1de2cea70e38e7ec491b8c3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1054/format:webp/1*5vr8qt6bSLf5idLFOL4lGA.png"/></div><p class="nq nr gj gh gi ns nt bd b be z dk translated">视觉转换结果。(<a class="ae ky" href="https://arxiv.org/abs/2010.11929" rel="noopener ugc nofollow" target="_blank"> Dosovitskiy等人2020 </a></p></figure></div><div class="ab cl kz la hx lb" role="separator"><span class="lc bw bk ld le lf"/><span class="lc bw bk ld le lf"/><span class="lc bw bk ld le"/></div><div class="im in io ip iq"><h1 id="139b" class="lg lh it bd li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md bi translated">根据我们得到的表达式比较ResNet和ViT</h1><p id="946f" class="pw-post-body-paragraph jz ka it kb b kc me ke kf kg mf ki kj kk mg km kn ko mh kq kr ks mi ku kv kw im bi translated">到目前为止，我们已经看到了ResNet和ViT的概述，两者在图像识别任务中都可以有不错的表现，但是<strong class="kb jd"> <em class="kx">它们之间有什么区别呢？</em> </strong>在论文《视觉变形金刚看起来像卷积神经网络吗？作者对其进行了研究。</p><p id="8f81" class="pw-post-body-paragraph jz ka it kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw im bi translated">让我们仔细看看介绍中提到的以下六点。</p><ol class=""><li id="1ee1" class="mj mk it kb b kc kd kg kh kk ml ko mm ks mn kw mo mp mq mr bi translated"><strong class="kb jd"> <em class="kx">与CNN</em></strong>相比，ViT在浅层和深层获得的表征之间具有更大的相似性</li><li id="ecf6" class="mj mk it kb b kc ms kg mt kk mu ko mv ks mw kw mo mp mq mr bi translated"><strong class="kb jd"> <em class="kx">与CNN不同，ViT从浅层获得全局表示，但从浅层获得的局部表示也很重要。</em>T19】</strong></li><li id="b720" class="mj mk it kb b kc ms kg mt kk mu ko mv ks mw kw mo mp mq mr bi translated">ViT中的Skip连接甚至比CNN(ResNet)中的更有影响，并且实质上影响表示的性能和相似性。T3】</li><li id="c7b5" class="mj mk it kb b kc ms kg mt kk mu ko mv ks mw kw mo mp mq mr bi translated"><strong class="kb jd"> <em class="kx"> ViT比ResNet </em> </strong>保留更多的空间信息</li><li id="a056" class="mj mk it kb b kc ms kg mt kk mu ko mv ks mw kw mo mp mq mr bi translated"><strong class="kb jd"> <em class="kx"> ViT可以学习大量数据的高质量中间表征</em> </strong></li><li id="7601" class="mj mk it kb b kc ms kg mt kk mu ko mv ks mw kw mo mp mq mr bi translated"><strong class="kb jd"> <em class="kx"> MLP混频器的表示更接近ViT而不是ResNet </em> </strong></li></ol></div><div class="ab cl kz la hx lb" role="separator"><span class="lc bw bk ld le lf"/><span class="lc bw bk ld le lf"/><span class="lc bw bk ld le"/></div><div class="im in io ip iq"><h2 id="4bed" class="mx lh it bd li my mz dn lm na nb dp lq kk nc nd lu ko ne nf ly ks ng nh mc iz bi translated"><strong class="ak"> <em class="oi"> 1。与CNN</em></strong>相比，ViT在浅层和深层获得的表示之间有更多的相似性</h2><p id="0054" class="pw-post-body-paragraph jz ka it kb b kc me ke kf kg mf ki kj kk mg km kn ko mh kq kr ks mi ku kv kw im bi translated">ViT和ResNet的主要区别之一是初始层的大视野。</p><figure class="nj nk nl nm gt nn gh gi paragraph-image"><div role="button" tabindex="0" class="ob oc di od bf oe"><div class="gh gi oa"><img src="../Images/0ec3c138c6e40df1f4170dca9833349f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7yMDoSCCpRpbmZ5G96hVOA.png"/></div></div><p class="nq nr gj gh gi ns nt bd b be z dk translated">相对于输入图像的感受野大小。</p></figure><p id="56e6" class="pw-post-body-paragraph jz ka it kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw im bi translated">CNN(ResNet)只有固定大小的内核视场(大小3或7)。具体来说，CNN是通过一层一层地重复“卷积”内核周围的信息来逐渐扩大视野的。相比之下，ViT使用自我关注机制，允许模型即使在最低层也具有整个视野。这样，根据网络的结构，视野是不同的。</p><p id="de89" class="pw-post-body-paragraph jz ka it kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw im bi translated">下图是ViT的实际视野(自我注意机制的有效距离)。在浅层，有一些像CNN这样局部视野的部分，但也有很多头部具有全局视野。</p><figure class="nj nk nl nm gt nn gh gi paragraph-image"><div role="button" tabindex="0" class="ob oc di od bf oe"><div class="gh gi oa"><img src="../Images/60f532d9c488145bde1895b51b8cc642.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Iex410ArRTkeuFRKlj-SYQ.png"/></div></div><p class="nq nr gj gh gi ns nt bd b be z dk translated">注意距离。该数据引自<a class="ae ky" href="https://arxiv.org/abs/2010.11929" rel="noopener ugc nofollow" target="_blank"> Dosovitskiy等人2020年</a></p></figure><p id="0cb9" class="pw-post-body-paragraph jz ka it kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw im bi translated">那么，ResNet和ViT在各层深度获得的表象，结构上有什么区别呢？为了找到答案，作者在下图(图1)中绘制了每一层获得的表示的相似性。</p><figure class="nj nk nl nm gt nn gh gi paragraph-image"><div role="button" tabindex="0" class="ob oc di od bf oe"><div class="gh gi oj"><img src="../Images/9cebda97262e8a2138480eeb8d00b7b1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QzTg3dVdf7p7_u6tDSEzjQ.png"/></div></div><p class="nq nr gj gh gi ns nt bd b be z dk translated"><a class="ae ky" href="https://arxiv.org/abs/2108.08810" rel="noopener ugc nofollow" target="_blank">拉古等人，2021年</a></p></figure><p id="9bbc" class="pw-post-body-paragraph jz ka it kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw im bi translated">在上图中，他们使用一种称为CKA相似性的测量方法绘制了每个图层所获得的表示的相似性(我不会深入CKA相似性的技术细节，因此如果您想了解更多信息，请参考原始论文。)图形的对角线分量因为与自身相似，自然高，但是我们再来看看图形的其他部分。</p><p id="86a2" class="pw-post-body-paragraph jz ka it kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw im bi translated">首先，在ViT(左边两个)中，我们可以看到整体的着色表明，无论层的深度如何，都在获得相似的表征。另一方面，在CNN(右边两个)中，我们注意到在浅层和深层获得的表征之间没有相似性。这可能是因为在ViT中，我们从一开始就获得全局表示，而在CNN中，我们需要传播层来获得全局表示。</p><p id="19da" class="pw-post-body-paragraph jz ka it kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw im bi translated">ViT和ResNet之间的相似性绘制在下图中(图2)。</p><figure class="nj nk nl nm gt nn gh gi paragraph-image"><div role="button" tabindex="0" class="ob oc di od bf oe"><div class="gh gi ok"><img src="../Images/3db861979ca025d4cbb3cca0372a4c1e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YRIIFeqTVImoejCquiUkXg.png"/></div></div><p class="nq nr gj gh gi ns nt bd b be z dk translated"><a class="ae ky" href="https://arxiv.org/abs/2108.08810" rel="noopener ugc nofollow" target="_blank">拉古等人，2021年</a></p></figure><p id="be21" class="pw-post-body-paragraph jz ka it kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw im bi translated">我们可以看到ViT的1到40层和ResNet的1到70层的相似度很高。所以ResNet需要70层来获得ViT需要40层来获得的表示。这意味着，在浅层获得一种表示的方法是非常不同的。此外，ViT深层与ResNet深层的相似度较低。所以图像的抽象表示在ViT和ResNet之间有很大的不同。</p><p id="ffa1" class="pw-post-body-paragraph jz ka it kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw im bi translated">顺便提一下，一些研究的动机是，由于自我注意图的相似性，ViT不会受益于加深(<a class="ae ky" href="https://arxiv.org/abs/2103.11886" rel="noopener ugc nofollow" target="_blank">周等，2021 </a>)。该研究关注头部之间的高度多样性，并提出了一种称为重新注意的机制，该机制引入了一个学习参数来混合不同头部之间的特征。他们使用它取得了良好的效果(DeepViT)。</p><figure class="nj nk nl nm gt nn gh gi paragraph-image"><div class="gh gi ol"><img src="../Images/4ed0b92059caa165f6772f0492fd273a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1296/format:webp/1*msR5VqroI5EL18s2rIQtnQ.png"/></div><p class="nq nr gj gh gi ns nt bd b be z dk translated">自我关注和再关注</p></figure><figure class="nj nk nl nm gt nn gh gi paragraph-image"><div class="gh gi om"><img src="../Images/15bf8ad733ba88f8d27f8bb03877d3da.png" data-original-src="https://miro.medium.com/v2/resize:fit:942/format:webp/1*4XHGiZnzkm8KuiowxNSBXw.png"/></div><p class="nq nr gj gh gi ns nt bd b be z dk translated">使用再注意的深度得益于加深(<a class="ae ky" href="https://arxiv.org/abs/2103.11886" rel="noopener ugc nofollow" target="_blank">周等，2021 </a>)</p></figure></div><div class="ab cl kz la hx lb" role="separator"><span class="lc bw bk ld le lf"/><span class="lc bw bk ld le lf"/><span class="lc bw bk ld le"/></div><div class="im in io ip iq"><h2 id="eab4" class="mx lh it bd li my mz dn lm na nb dp lq kk nc nd lu ko ne nf ly ks ng nh mc iz bi translated"><strong class="ak"> <em class="oi"> 2。与CNN不同，ViT从浅层获得全局表示，但从浅层获得的局部表示也很重要</em> </strong></h2><p id="83b1" class="pw-post-body-paragraph jz ka it kb b kc me ke kf kg mf ki kj kk mg km kn ko mh kq kr ks mi ku kv kw im bi translated">下图(图3)是用JFT-300M (3亿张图片)预训练，用ImageNet(130万张图片)微调后，自我注意机制的有效距离(5000个数据的自我注意距离的平均值)。</p><figure class="nj nk nl nm gt nn gh gi paragraph-image"><div role="button" tabindex="0" class="ob oc di od bf oe"><div class="gh gi on"><img src="../Images/785bd51984dfcc7b30f2f7a8533a7295.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ws9-rUrEXCiZuFyTEva6bg.png"/></div></div><p class="nq nr gj gh gi ns nt bd b be z dk translated"><a class="ae ky" href="https://arxiv.org/abs/2108.08810" rel="noopener ugc nofollow" target="_blank">拉古等人，2021年</a></p></figure><p id="a6c6" class="pw-post-body-paragraph jz ka it kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw im bi translated">在浅层(encoder_block0，1)中，我们可以看到模型同时获得了局部和全局表示，而在深层(encoder_block22，23，30，31)，所有表示都具有全局视图。</p><p id="212f" class="pw-post-body-paragraph jz ka it kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw im bi translated">正如我们在视觉转换器的描述中看到的，训练ViT需要大量的数据(如JFT-300M)，如果数据不足，精度就会下降。下图(图4)显示了这种情况下的相似之处。</p><figure class="nj nk nl nm gt nn gh gi paragraph-image"><div role="button" tabindex="0" class="ob oc di od bf oe"><div class="gh gi on"><img src="../Images/7cb22296adbf8cdaf3fa3677c3ab7f62.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oMICr47_DQ-yCfx9PoXNpg.png"/></div></div><p class="nq nr gj gh gi ns nt bd b be z dk translated"><a class="ae ky" href="https://arxiv.org/abs/2108.08810" rel="noopener ugc nofollow" target="_blank"> Raghu等人，2021年</a></p></figure><p id="7571" class="pw-post-body-paragraph jz ka it kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw im bi translated">如果我们比较图3和图4，我们可以看到，当数据集很小时，我们无法在浅层获得局部表示。从这个结果以及“ViT在数据量很小时达不到精度”的事实，我们可以看出，用足够数据训练的ViT所获得的“局部表示”对精度有显著的影响。</p><p id="7a43" class="pw-post-body-paragraph jz ka it kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw im bi translated">但是数据量和所需的表示之间有什么关系呢？下图(图12)说明了这一点。</p><figure class="nj nk nl nm gt nn gh gi paragraph-image"><div role="button" tabindex="0" class="ob oc di od bf oe"><div class="gh gi oo"><img src="../Images/467feede9052ea4f77b5ced49368cc8d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qcar6dp3yCkiFRF5PtruQA.png"/></div></div><p class="nq nr gj gh gi ns nt bd b be z dk translated"><a class="ae ky" href="https://arxiv.org/abs/2108.08810" rel="noopener ugc nofollow" target="_blank">拉古等人，2021年</a></p></figure><p id="e2db" class="pw-post-body-paragraph jz ka it kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw im bi translated">对于约10%数据的浅层表示，与使用所有数据获得的表示的相似性在某种程度上增加了。然而，对于深层表示，即使有30%的数据，相似度也低于0.2。由此，我们可以说，有助于准确性的深层表示法只能通过大量数据来学习。前面提到局部表示很重要，但似乎在深层可以获得的全局表示也很重要。</p><p id="b9c4" class="pw-post-body-paragraph jz ka it kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw im bi translated">虽然这里没有具体说明，但是实验很可能是用JFT-300M进行的，所以即使我们说总数据的3%，仍然有10M左右的数据量(大概是ImageNet的10倍)。在我看来，30%的数据(100M)就足以获得浅层应该获得的局部表示，如果数据更多，就有可能获得全局表示中重要的东西。</p></div><div class="ab cl kz la hx lb" role="separator"><span class="lc bw bk ld le lf"/><span class="lc bw bk ld le lf"/><span class="lc bw bk ld le"/></div><div class="im in io ip iq"><h2 id="ff93" class="mx lh it bd li my mz dn lm na nb dp lq kk nc nd lu ko ne nf ly ks ng nh mc iz bi translated"><strong class="ak"> <em class="oi"> 3。ViT中的跳过连接甚至比CNN(ResNet)中的更有影响力，并且实质上影响表现的性能和相似性</em> </strong></h2><p id="a79b" class="pw-post-body-paragraph jz ka it kb b kc me ke kf kg mf ki kj kk mg km kn ko mh kq kr ks mi ku kv kw im bi translated">接下来，让我们看看跳过连接和习得表达相似性之间的关系。如下图所示(图8)。</p><figure class="nj nk nl nm gt nn gh gi paragraph-image"><div role="button" tabindex="0" class="ob oc di od bf oe"><div class="gh gi on"><img src="../Images/e4239d499d36147fbe8382b6cc7972e8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1P1TkPsy7V7sCHDP7gdMQg.png"/></div></div><p class="nq nr gj gh gi ns nt bd b be z dk translated"><a class="ae ky" href="https://arxiv.org/abs/2108.08810" rel="noopener ugc nofollow" target="_blank">拉古等人，2021年</a></p></figure><p id="3668" class="pw-post-body-paragraph jz ka it kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw im bi translated">在该图所示的实验中，我们计算当消除层<strong class="kb jd"> <em class="kx"> i </em> </strong>的跳过连接时所获得的表示的相似度。将该图与图1的左侧(ViT)相比较，可以看到在消除了跳过连接的层<strong class="kb jd"> <em class="kx"> i </em> </strong>之后，所获得的表示的类似趋势发生了剧烈变化。换句话说，跳过连接对制图表达传播有重大影响，当它被消除时，图层的相似性会发生显著变化。顺便说一句，当在中间层消除跳跃连接时，精度下降约4%。</p><p id="d9cd" class="pw-post-body-paragraph jz ka it kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw im bi translated">接下来，我们来看看跳过连接在信息传播中是如何发挥作用的。请看下图(图7)。</p><figure class="nj nk nl nm gt nn gh gi paragraph-image"><div role="button" tabindex="0" class="ob oc di od bf oe"><div class="gh gi op"><img src="../Images/4bf38683466f18162355283edac0fbef.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TCQAVwYd4oPlIIQ7TgZEsw.png"/></div></div><p class="nq nr gj gh gi ns nt bd b be z dk translated"><a class="ae ky" href="https://arxiv.org/abs/2108.08810" rel="noopener ugc nofollow" target="_blank">拉古等人，2021年</a></p></figure><p id="3427" class="pw-post-body-paragraph jz ka it kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw im bi translated">在图7的左面板中，<strong class="kb jd">|<em class="kx">| | z _ I |/| f(z _ I)| |</em>|</strong>，比值为$ <strong class="kb jd"> <em class="kx"> z_i </em> </strong>，输入信息到$i$层中的自我注意，而<strong class="kb jd"> <em class="kx"> f(z_i) </em> </strong>，经自我注意和多层网络变换后的特征值<strong class="kb jd"><em class="kx">【z _ I</em></strong>，被标绘为)比率越大，通过跳跃连接传播的信息越多；左边的图7显示了类标记通过初始层中的跳跃连接进行传播，而图像通过自我注意和多层网络进行传播。在更深的层面上，趋势是相反的。</p><p id="073f" class="pw-post-body-paragraph jz ka it kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw im bi translated">右图显示了与ResNet的对比。绿线为ResNet，而ViT值较大，说明通过跳跃结的信息传播起主要作用。</p><p id="fa05" class="pw-post-body-paragraph jz ka it kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw im bi translated">虽然论文中没有具体提到，但跳过连接在信息传播中起着重要作用，这一事实可能导致在图8中消除中间层的跳过连接时精度显著下降。</p></div><div class="ab cl kz la hx lb" role="separator"><span class="lc bw bk ld le lf"/><span class="lc bw bk ld le lf"/><span class="lc bw bk ld le"/></div><div class="im in io ip iq"><h2 id="5a9a" class="mx lh it bd li my mz dn lm na nb dp lq kk nc nd lu ko ne nf ly ks ng nh mc iz bi translated"><strong class="ak"> <em class="oi"> 4。ViT比ResNet </em> </strong>保留更多的空间信息</h2><p id="3c04" class="pw-post-body-paragraph jz ka it kb b kc me ke kf kg mf ki kj kk mg km kn ko mh kq kr ks mi ku kv kw im bi translated">接下来，我们来比较一下ViT和ResNet保留了多少位置信息。看下图。</p><figure class="nj nk nl nm gt nn gh gi paragraph-image"><div role="button" tabindex="0" class="ob oc di od bf oe"><div class="gh gi oq"><img src="../Images/e9f6f7595ba80edcc164bf5c0900f7b1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Q_qvlRV3PgHyvi4_0Q5wrQ.png"/></div></div><p class="nq nr gj gh gi ns nt bd b be z dk translated"><a class="ae ky" href="https://arxiv.org/abs/2108.08810" rel="noopener ugc nofollow" target="_blank">拉古等人，2021 </a></p></figure><p id="c452" class="pw-post-body-paragraph jz ka it kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw im bi translated">在本实验中，作者通过绘制输入图像的补片与特定位置的最终图层特征图之间的CKA相似性，测试了哪些ViT和ResNet保留了位置信息。如果它保留位置信息，那么在某个位置与输入图像的片的相似度应该仅在对应于特征图中该位置的位置处高。</p><p id="1e25" class="pw-post-body-paragraph jz ka it kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw im bi translated">首先，我们来看看ViT(上，中)。不出所料，在最后一层，对应位置的相似度较高。这意味着ViT传播表示，同时保留位置信息。接下来，我们来看看ResNet(底排)。在这种情况下，不相关位置的相似性更高，表明它没有保留位置信息。</p><p id="0976" class="pw-post-body-paragraph jz ka it kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw im bi translated">这种趋势上的差异很可能是因为网络结构的不同。看下图(图取自<a class="ae ky" href="http://Wang et al., 2021" rel="noopener ugc nofollow" target="_blank">王等2021 </a>)。</p><figure class="nj nk nl nm gt nn gh gi paragraph-image"><div role="button" tabindex="0" class="ob oc di od bf oe"><div class="gh gi or"><img src="../Images/3f187df32a6d7c7d92da3a559b899e02.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2-h_oXk5-qZGSZSY4cxqAQ.png"/></div></div><p class="nq nr gj gh gi ns nt bd b be z dk translated"><a class="ae ky" href="http://Wang et al., 2021" rel="noopener ugc nofollow" target="_blank">王等，2021 </a></p></figure><p id="b724" class="pw-post-body-paragraph jz ka it kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw im bi translated">ResNet和其他基于CNN的图像分类网络以降低的分辨率传播表示。比如ResNet有五个阶段，每个阶段的分辨率减半，这样最终的特征图大小为1/32 * 1/32(上图左图)。另一方面，ViT首先标记为16x16大小，这会降低该区域的分辨率，但它会以该分辨率传播到最终层。因此，ViT比ResNet更有可能保留位置信息。然而，首先，图像分类任务不需要位置信息来进行分类决策，因此不能说ViT比ResNet有优势，因为位置信息被保留了。</p><p id="49d7" class="pw-post-body-paragraph jz ka it kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw im bi translated">此外，在最近的研究中，像ResNet这样逐渐降低分辨率的策略经常被用于视觉变压器相关的研究中。例如，右上角显示的金字塔视觉转换器。Transformer系统使用自我关注，占用的内存大小与图像大小的四次方成比例增加。这使得难以处理大分辨率，但是通过使用逐渐降低分辨率的策略，如在CNN系统中，可以在第一层中处理高分辨率信息，同时节省存储器。</p></div><div class="ab cl kz la hx lb" role="separator"><span class="lc bw bk ld le lf"/><span class="lc bw bk ld le lf"/><span class="lc bw bk ld le"/></div><div class="im in io ip iq"><h2 id="3cd9" class="mx lh it bd li my mz dn lm na nb dp lq kk nc nd lu ko ne nf ly ks ng nh mc iz bi translated"><strong class="ak"> <em class="oi"> 5。ViT可以学习大量数据的高质量中间表征</em> </strong></h2><p id="2788" class="pw-post-body-paragraph jz ka it kb b kc me ke kf kg mf ki kj kk mg km kn ko mh kq kr ks mi ku kv kw im bi translated">接下来，让我们看看中间层表示的质量。下图(图13)展示了这个实验。</p><figure class="nj nk nl nm gt nn gh gi paragraph-image"><div role="button" tabindex="0" class="ob oc di od bf oe"><div class="gh gi os"><img src="../Images/ae9fc26617f2b3fb931ad1fdefbb48d1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ndFxa8MQah8R9e5MoJvBCg.png"/></div></div><p class="nq nr gj gh gi ns nt bd b be z dk translated"><a class="ae ky" href="https://arxiv.org/abs/2108.08810" rel="noopener ugc nofollow" target="_blank">拉古等人，2021年</a></p></figure><p id="71e7" class="pw-post-body-paragraph jz ka it kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw im bi translated">在这个实验中，作者试图看看他们是否可以使用中间层的表示来用线性模型进行分类。简单模型(如线性模型)的精度越高，图层的表现就越好。</p><p id="f98b" class="pw-post-body-paragraph jz ka it kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw im bi translated">首先，我们来看看数据集的大小和得到的表示之间的关系(左图)。这里，我们比较了包含130万张图片的ImageNet(虚线)和包含3亿张图片的JFT-300M(实线)的实验结果。正如你所看到的，在JFT-300M上训练的表现更好，这是一个巨大的数据集。接下来是包括ResNet在内的车型对比。可以看出，较大的模型产生更好的表示。</p><p id="45fa" class="pw-post-body-paragraph jz ka it kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw im bi translated">作为旁注，在右图中，基于ResNet的模型的精度在接近最终层时突然增加。这是为什么呢？</p><p id="7c68" class="pw-post-body-paragraph jz ka it kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw im bi translated">Frosst及其同事的一项研究提供了一个提示(<a class="ae ky" href="https://arxiv.org/abs/1902.01889" rel="noopener ugc nofollow" target="_blank"> Frosst et al .，2019 </a>)。他们在ResNet的中间层引入了带有温度项的软最近邻损失，并研究了它的行为。软最近邻损失通过类别指示特征的纠缠状态。较大的软最近邻损失值表示按类划分的要素相互交织，而较小的值表示按类划分的要素相互分离。</p><figure class="nj nk nl nm gt nn gh gi paragraph-image"><div class="gh gi ot"><img src="../Images/7ed46d81641f4bbda2130bf59b0f7862.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lVKrXynqSKfFY_cVmLy7JQ.png"/></div><p class="nq nr gj gh gi ns nt bd b be z dk translated">软最近邻损失</p></figure><figure class="nj nk nl nm gt nn gh gi paragraph-image"><div class="gh gi ot"><img src="../Images/293f7cc7e6a64a818de8dc5694b70427.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oiCUd1PEQ26t8FfILwh1ZQ.png"/></div><p class="nq nr gj gh gi ns nt bd b be z dk translated">值越小，分类越好。(<a class="ae ky" href="https://arxiv.org/abs/1902.01889" rel="noopener ugc nofollow" target="_blank">弗罗斯特等人，2019 </a>)</p></figure><p id="7f65" class="pw-post-body-paragraph jz ka it kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw im bi translated">[软最近邻损失]</p><p id="5ab2" class="pw-post-body-paragraph jz ka it kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw im bi translated">下图显示了ResNet的每个块中的软最近邻损失的值。众所周知，这是一个高性能的图像分类网络，但除了最后一层之外，它没有分离每一类的特征。在我看来，ResNet的这一特性可能是最后一层附近精度快速提升的原因，如图13所示。</p><figure class="nj nk nl nm gt nn gh gi paragraph-image"><div class="gh gi ou"><img src="../Images/8b057dba7a33fd4042bb333e0bf4271f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1112/format:webp/1*C1H1qdrwuK9OED_Sg1Vong.png"/></div><p class="nq nr gj gh gi ns nt bd b be z dk translated">在最后一层，软最近邻损失的值很小，表明要素是按类别分开的。(<a class="ae ky" href="https://arxiv.org/abs/1902.01889" rel="noopener ugc nofollow" target="_blank">弗罗斯特等人，2019年</a>)</p></figure></div><div class="ab cl kz la hx lb" role="separator"><span class="lc bw bk ld le lf"/><span class="lc bw bk ld le lf"/><span class="lc bw bk ld le"/></div><div class="im in io ip iq"><h2 id="c768" class="mx lh it bd li my mz dn lm na nb dp lq kk nc nd lu ko ne nf ly ks ng nh mc iz bi translated"><strong class="ak"> <em class="oi"> 6。MLP-混频器的表示更接近ViT而不是ResNet </em> </strong></h2><p id="6945" class="pw-post-body-paragraph jz ka it kb b kc me ke kf kg mf ki kj kk mg km kn ko mh kq kr ks mi ku kv kw im bi translated">最近，代替变压器，使用多层感知器(MLPs)，即具有密集层的网络，已经提出了高度精确的图像分类模型。一个典型的例子是称为MLP混频器的网络(<a class="ae ky" href="https://arxiv.org/abs/2105.01601" rel="noopener ugc nofollow" target="_blank"> Tolstikhin等人，2021 </a>)。这个网络的结构如下图所示。</p><figure class="nj nk nl nm gt nn gh gi paragraph-image"><div role="button" tabindex="0" class="ob oc di od bf oe"><div class="gh gi ov"><img src="../Images/cdf634cc03bf71580b319561bc2b28a2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rQztplhaosvRes53CqU1Mg.png"/></div></div><p class="nq nr gj gh gi ns nt bd b be z dk translated">(<a class="ae ky" href="https://arxiv.org/abs/2105.01601" rel="noopener ugc nofollow" target="_blank">托尔斯泰欣等人，2021年</a>)</p></figure><p id="ac2b" class="pw-post-body-paragraph jz ka it kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw im bi translated">MLP混合器是一种系统，它使用MLP1在小块之间混合信息，然后使用MLP2在小块内混合信息，然后通过堆积组合两者的块来传播。这种MLP混频器可以达到与ViT相同或更高的精度。下图以与前面相同的方式比较了MLP混频器的表示。将该图与图1和图2进行比较，作者认为总的趋势与ViT相似。</p><p id="c0b7" class="pw-post-body-paragraph jz ka it kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw im bi translated">MLP混合器通过将图像划分为类似ViT的面片来传播图像，因此它在结构上比ResNet更接近ViT。这种结构可能是造成这种结果的原因。</p><figure class="nj nk nl nm gt nn gh gi paragraph-image"><div role="button" tabindex="0" class="ob oc di od bf oe"><div class="gh gi ow"><img src="../Images/07f2c1c4829a37043de03bc68532218e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*x2YhfgZ4ZfNmHiD5rXcj8w.png"/></div></div><p class="nq nr gj gh gi ns nt bd b be z dk translated">Raghu等人，2021年</p></figure><figure class="nj nk nl nm gt nn gh gi paragraph-image"><div role="button" tabindex="0" class="ob oc di od bf oe"><div class="gh gi ox"><img src="../Images/cd7a3c5ca3377cb2f43b0353c2a31593.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3r8-jfQkrkieT33Hb8fqoQ.png"/></div></div><p class="nq nr gj gh gi ns nt bd b be z dk translated"><a class="ae ky" href="https://arxiv.org/abs/2108.08810" rel="noopener ugc nofollow" target="_blank"> Raghu等人，2021年</a></p></figure></div><div class="ab cl kz la hx lb" role="separator"><span class="lc bw bk ld le lf"/><span class="lc bw bk ld le lf"/><span class="lc bw bk ld le"/></div><div class="im in io ip iq"><h1 id="f15c" class="lg lh it bd li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md bi translated">结论</h1><p id="1640" class="pw-post-body-paragraph jz ka it kb b kc me ke kf kg mf ki kj kk mg km kn ko mh kq kr ks mi ku kv kw im bi translated">在这篇文章中，我详细研究了ViT和CNN之间的差异。概括来说，这里有两者之间的一些差异。变形金刚将继续在计算机视觉领域发挥重要影响。希望这篇文章能帮助你了解变形金刚。</p><ol class=""><li id="9ad1" class="mj mk it kb b kc kd kg kh kk ml ko mm ks mn kw mo mp mq mr bi translated"><strong class="kb jd"> <em class="kx">与CNN</em></strong>相比，ViT在浅层和深层获得的表征之间具有更大的相似性</li><li id="9637" class="mj mk it kb b kc ms kg mt kk mu ko mv ks mw kw mo mp mq mr bi translated"><strong class="kb jd"> <em class="kx">与CNN不同，ViT从浅层获得全局表征，但从浅层获得的局部表征也很重要。</em> </strong></li><li id="737b" class="mj mk it kb b kc ms kg mt kk mu ko mv ks mw kw mo mp mq mr bi translated"><strong class="kb jd"><em class="kx">ViT中的Skip连接甚至比CNN(ResNet)中的影响更大，并对表示的性能和相似性产生实质性影响。</em>T11】</strong></li><li id="9411" class="mj mk it kb b kc ms kg mt kk mu ko mv ks mw kw mo mp mq mr bi translated"><strong class="kb jd"> <em class="kx"> ViT比ResNet </em> </strong>保留更多的空间信息</li><li id="818e" class="mj mk it kb b kc ms kg mt kk mu ko mv ks mw kw mo mp mq mr bi translated"><strong class="kb jd"> <em class="kx"> ViT可以学习大量数据的高质量中间表征</em> </strong></li><li id="6e10" class="mj mk it kb b kc ms kg mt kk mu ko mv ks mw kw mo mp mq mr bi translated"><strong class="kb jd"><em class="kx">MLP-混频器的表征更接近ViT而不是ResNet </em> </strong></li></ol><p id="b998" class="pw-post-body-paragraph jz ka it kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw im bi">— — — — — — — — — — — — — — — — — — –</p><h1 id="4060" class="lg lh it bd li lj nu ll lm ln nv lp lq lr nw lt lu lv nx lx ly lz ny mb mc md bi translated">🌟我每周发布时事通讯！请订阅！🌟</h1><div class="oy oz gp gr pa pb"><a href="https://www.getrevue.co/profile/akiratosei" rel="noopener  ugc nofollow" target="_blank"><div class="pc ab fo"><div class="pd ab pe cl cj pf"><h2 class="bd jd gy z fp pg fr fs ph fu fw jc bi translated">阿基拉的机器学习新闻- Revue</h2><div class="pi l"><h3 class="bd b gy z fp pg fr fs ph fu fw dk translated">由Akira的机器学习新闻-由Akihiro FUJII:制造工程师/机器学习工程师/硕士…</h3></div><div class="pj l"><p class="bd b dl z fp pg fr fs ph fu fw dk translated">www.getrevue.co</p></div></div><div class="pk l"><div class="pl l pm pn po pk pp no pb"/></div></div></a></div><p id="52f0" class="pw-post-body-paragraph jz ka it kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw im bi">— — — — — — — — — — — — — — — — — — — — — — — — — — — — — — —</p><h1 id="da53" class="lg lh it bd li lj nu ll lm ln nv lp lq lr nw lt lu lv nx lx ly lz ny mb mc md bi translated">其他博客</h1><div class="oy oz gp gr pa pb"><a rel="noopener follow" target="_blank" href="/machine-learning-2020-summary-84-interesting-papers-articles-45bd45c0d35b"><div class="pc ab fo"><div class="pd ab pe cl cj pf"><h2 class="bd jd gy z fp pg fr fs ph fu fw jc bi translated">机器学习2020摘要:84篇有趣的论文/文章</h2><div class="pi l"><h3 class="bd b gy z fp pg fr fs ph fu fw dk translated">在这篇文章中，我总共展示了2020年发表的84篇我觉得特别有趣的论文和文章…</h3></div><div class="pj l"><p class="bd b dl z fp pg fr fs ph fu fw dk translated">towardsdatascience.com</p></div></div><div class="pk l"><div class="pq l pm pn po pk pp no pb"/></div></div></a></div><div class="oy oz gp gr pa pb"><a rel="noopener follow" target="_blank" href="/recent-developments-and-views-on-computer-vision-x-transformer-ed32a2c72654"><div class="pc ab fo"><div class="pd ab pe cl cj pf"><h2 class="bd jd gy z fp pg fr fs ph fu fw jc bi translated">计算机视觉x变形金刚的最新发展和看法</h2><div class="pi l"><h3 class="bd b gy z fp pg fr fs ph fu fw dk translated">《变形金刚》和CNN的区别，为什么《变形金刚》很重要，它的弱点是什么。</h3></div><div class="pj l"><p class="bd b dl z fp pg fr fs ph fu fw dk translated">towardsdatascience.com</p></div></div><div class="pk l"><div class="pr l pm pn po pk pp no pb"/></div></div></a></div><div class="oy oz gp gr pa pb"><a href="https://medium.com/analytics-vidhya/reach-and-limits-of-the-supermassive-model-gpt-3-5012a6ddff00" rel="noopener follow" target="_blank"><div class="pc ab fo"><div class="pd ab pe cl cj pf"><h2 class="bd jd gy z fp pg fr fs ph fu fw jc bi translated">超大质量模型GPT-3的到达和极限</h2><div class="pi l"><h3 class="bd b gy z fp pg fr fs ph fu fw dk translated">在这篇博文中，我将从技术上解释GPT 3号，GPT 3号取得了什么，GPT 3号没有取得什么…</h3></div><div class="pj l"><p class="bd b dl z fp pg fr fs ph fu fw dk translated">medium.com</p></div></div><div class="pk l"><div class="ps l pm pn po pk pp no pb"/></div></div></a></div></div></div>    
</body>
</html>