<html>
<head>
<title>AdaHessian: a second order optimizer for deep learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">AdaHessian:用于深度学习的二阶优化器</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/adahessian-a-second-order-optimizer-for-deep-learning-2fc76b29bcbb?source=collection_archive---------24-----------------------#2021-08-09">https://towardsdatascience.com/adahessian-a-second-order-optimizer-for-deep-learning-2fc76b29bcbb?source=collection_archive---------24-----------------------#2021-08-09</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="de97" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsdatascience.com/tagged/thoughts-and-theory" rel="noopener" target="_blank">思想和理论</a></h2><div class=""/><figure class="gl gn ka kb kc kd gh gi paragraph-image"><div role="button" tabindex="0" class="ke kf di kg bf kh"><div class="gh gi jz"><img src="../Images/80eef667a84777e20481adf8d8c4ab71.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WcE1J87pZUsOjzTFPumBGw.png"/></div></div><p class="kk kl gj gh gi km kn bd b be z dk translated">阿达赫森在向最小值前进。(图片由作者提供)</p></figure><p id="f610" class="pw-post-body-paragraph ko kp it kq b kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">深度学习中使用的优化器大多是(随机)梯度下降法。他们只考虑损失函数的梯度。相比之下，二阶方法也考虑了损失函数的曲率。有了它，就可以计算更好的更新步长(至少在理论上)。只有少数二阶方法可用于深度学习，其中之一是姚等人在2020年发表的<a class="ae lm" href="https://arxiv.org/pdf/2006.00719.pdf" rel="noopener ugc nofollow" target="_blank"> AdaHessian </a>。作者提供了PyTorch实现。</p><p id="23ff" class="pw-post-body-paragraph ko kp it kq b kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">在其最基本的形式中，二阶方法需要计算包含N×N个元素的Hessian矩阵，其中N是神经网络中参数(权重)的数量。这对于大多数模型来说是不可行的。AdaHessian包含了一些有趣的技术来解决这个问题。然而，乍看起来它们可能很复杂。本文将介绍这些技术，它们可以有效地计算Hessian矩阵的近似值。</p><h1 id="52ee" class="ln lo it bd lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk bi translated">动机</h1><p id="67dc" class="pw-post-body-paragraph ko kp it kq b kr ml kt ku kv mm kx ky kz mn lb lc ld mo lf lg lh mp lj lk ll im bi translated">梯度下降仅使用其一阶导数来模拟损失函数:该算法在负梯度的方向上采取足够小的步长(由学习速率控制),从而损失值降低。损失函数的曲率信息(在Hessian矩阵中收集的二阶导数)被忽略。</p><p id="0267" class="pw-post-body-paragraph ko kp it kq b kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">图1显示了当前位置的损失函数(绿色)和梯度(红色)。左图显示了梯度与损失函数局部精确匹配的情况。右图显示了当向负梯度方向移动时损失函数向上的情况。虽然在左图中应用较大的更新步长可能有意义，但在右图中需要较小的更新步长，以避免超过最小值。</p><figure class="mr ms mt mu gt kd gh gi paragraph-image"><div class="gh gi mq"><img src="../Images/e270c01e1fa088a4318c1bacca4c1130.png" data-original-src="https://miro.medium.com/v2/resize:fit:1080/format:webp/1*wachQcq2_6uIfSxjyFeysg.png"/></div><p class="kk kl gj gh gi km kn bd b be z dk translated">图1:损失函数f(w)(绿色)及其在w=-1处的梯度(红色)。(图片由作者提供)</p></figure><h1 id="0865" class="ln lo it bd lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk bi translated">牛顿更新步骤</h1><p id="b44b" class="pw-post-body-paragraph ko kp it kq b kr ml kt ku kv mm kx ky kz mn lb lc ld mo lf lg lh mp lj lk ll im bi translated">牛顿的方法就是这样处理的:它同时考虑了当前位置的梯度和曲率。它用二次模型对损失函数f(w)(w是模型的参数或权重)进行局部建模。更新步骤推导如下(公式见图2):</p><ul class=""><li id="98cb" class="mv mw it kq b kr ks kv kw kz mx ld my lh mz ll na nb nc nd bi translated">使用泰勒多项式计算二次模型m</li><li id="0928" class="mv mw it kq b kr ne kv nf kz ng ld nh lh ni ll na nb nc nd bi translated">g是梯度，H是海森矩阵</li><li id="1bf5" class="mv mw it kq b kr ne kv nf kz ng ld nh lh ni ll na nb nc nd bi translated">为了找到模型m的最小值，计算模型的导数，将其设置为零，并求解更新步长δw</li><li id="6b14" class="mv mw it kq b kr ne kv nf kz ng ld nh lh ni ll na nb nc nd bi translated">这给出了牛顿更新，其中负梯度被Hessian的逆缩放和旋转</li></ul><figure class="mr ms mt mu gt kd gh gi paragraph-image"><div class="gh gi nj"><img src="../Images/35e7f6c482ccd602fe53b8b79de74d93.png" data-original-src="https://miro.medium.com/v2/resize:fit:1252/format:webp/1*j_RT4a2KuE2DLOolN9ZRDg.png"/></div><p class="kk kl gj gh gi km kn bd b be z dk translated">图2:牛顿法的导出更新步骤。(图片由作者提供)</p></figure><p id="a0d2" class="pw-post-body-paragraph ko kp it kq b kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">梯度和Hessian都可以用PyTorch这样的深度学习框架来计算，这是应用牛顿方法所需的一切。然而，对于具有N个参数的神经网络模型，Hessian矩阵由N×N个元素组成，这对于典型模型是不可行的。AdaHessian用一个对角矩阵来近似Hessian矩阵，它只包含N个元素(与梯度向量大小相同)。</p><h1 id="962e" class="ln lo it bd lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk bi translated">计算黑森的对角线</h1><p id="1d94" class="pw-post-body-paragraph ko kp it kq b kr ml kt ku kv mm kx ky kz mn lb lc ld mo lf lg lh mp lj lk ll im bi translated">我们现在有了牛顿更新公式，我们把海森近似法限制在一个对角矩阵上。让我们看看它是如何计算的。</p><h2 id="68af" class="nk lo it bd lp nl nm dn lt nn no dp lx kz np nq mb ld nr ns mf lh nt nu mj iz bi translated">哈钦森方法</h2><p id="8901" class="pw-post-body-paragraph ko kp it kq b kr ml kt ku kv mm kx ky kz mn lb lc ld mo lf lg lh mp lj lk ll im bi translated">图3示出了Hutchinson方法的公式，其计算Hessian的对角元素:</p><ul class=""><li id="baaf" class="mv mw it kq b kr ks kv kw kz mx ld my lh mz ll na nb nc nd bi translated">通过为每个元素抛硬币来创建一个随机向量z，并为头部设置+1，为尾部设置-1，因此在2D的例子中，z可以是(1，-1)</li><li id="2a31" class="mv mw it kq b kr ne kv nf kz ng ld nh lh ni ll na nb nc nd bi translated">计算矩阵向量乘积</li><li id="c008" class="mv mw it kq b kr ne kv nf kz ng ld nh lh ni ll na nb nc nd bi translated">将z元素(用⊙表示)乘以上一步的结果z⊙(H z)</li><li id="e65e" class="mv mw it kq b kr ne kv nf kz ng ld nh lh ni ll na nb nc nd bi translated">计算期望值(或在实际实现中使用z向量的多个实例的平均值)</li></ul><figure class="mr ms mt mu gt kd gh gi paragraph-image"><div class="gh gi nv"><img src="../Images/49dd8890543eb3775e3b29a8d8825684.png" data-original-src="https://miro.medium.com/v2/resize:fit:502/format:webp/1*x8kTKjFjsHnxHW1_w-yodg.png"/></div><p class="kk kl gj gh gi km kn bd b be z dk translated">图3: Hutchinson计算h对角线的方法</p></figure><p id="01ed" class="pw-post-body-paragraph ko kp it kq b kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">这乍一看很奇怪。我们已经需要H来获得H的对角元素——这听起来不是很聪明。但事实证明我们只需要知道H z(一个向量)的结果，这个结果很容易计算，所以我们从来不需要知道全Hessian矩阵的实际元素。</p><p id="e35f" class="pw-post-body-paragraph ko kp it kq b kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">但是哈钦森的方法是如何工作的呢？当写下2D的例子时(图4 ),很容易看出。逐元素乘法意味着逐行乘以向量。检查z⊙(H z)的结果显示其中有z₁(和z₂)的项和z₁ z₂的项。在计算多次试验的z⊙(H z)时，z₁(和z₂)总是+1，而z₁ z₂在50%的试验中给出+1，在另外50%的试验中给出-1(只需写下所有可能的乘积:1 1，1 (-1)，(-1) 1，(-1) (-1))。当计算多次试验的平均值时，包含z₁ z₂的项趋向于零，留给我们的是h的对角元素的向量</p><figure class="mr ms mt mu gt kd gh gi paragraph-image"><div class="gh gi nw"><img src="../Images/fc254def4854bb4160ed7a9adfa3200a.png" data-original-src="https://miro.medium.com/v2/resize:fit:968/format:webp/1*MwcYUr4hiDKZCBeZX6gf6Q.png"/></div><p class="kk kl gj gh gi km kn bd b be z dk translated">图4:哈钦森在2D案例中的方法。(图片由作者提供)</p></figure><h2 id="66e1" class="nk lo it bd lp nl nm dn lt nn no dp lx kz np nq mb ld nr ns mf lh nt nu mj iz bi translated">矩阵矢量积</h2><p id="c1ce" class="pw-post-body-paragraph ko kp it kq b kr ml kt ku kv mm kx ky kz mn lb lc ld mo lf lg lh mp lj lk ll im bi translated">只剩下一个问题:我们没有在Hutchinson方法中使用的Hessian矩阵H。然而，正如已经提到的，我们实际上并不需要黑森。如果我们有H z的结果就足够了。它是在PyTorch的自动微分功能的帮助下计算的:如果我们采用PyTorch已经计算的梯度，乘以z，并对参数向量w进行微分，我们会得到与直接计算H z相同的结果。因此，我们可以在不知道H的元素的情况下计算H z。图5显示了为什么这是正确的:再次对梯度求导得到Hessian，z被视为常数。</p><figure class="mr ms mt mu gt kd gh gi paragraph-image"><div role="button" tabindex="0" class="ke kf di kg bf kh"><div class="gh gi nx"><img src="../Images/bddbceebd9da8df6bafd545b63ba085d.png" data-original-src="https://miro.medium.com/v2/resize:fit:600/format:webp/1*mKeEcn2Wp891zjD_C_EB4Q.png"/></div></div><p class="kk kl gj gh gi km kn bd b be z dk translated">图5:我们用PyTorch的自动微分法(左)和Hutchinson方法所需的矩阵向量乘积H z(右)计算的结果相等。(图片由作者提供)</p></figure><h1 id="78f0" class="ln lo it bd lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk bi translated">把所有的放在一起</h1><p id="4f3d" class="pw-post-body-paragraph ko kp it kq b kr ml kt ku kv mm kx ky kz mn lb lc ld mo lf lg lh mp lj lk ll im bi translated">我们现在有了Hessian的梯度和对角线值，所以我们可以直接应用牛顿更新步骤。然而，AdaHessian遵循与Adam优化器类似的方法:它在多个时间步长上求平均值，以获得对梯度和Hessian的更平滑的估计。</p><p id="dcdf" class="pw-post-body-paragraph ko kp it kq b kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">单个更新步骤如下所示:</p><ul class=""><li id="ca4b" class="mv mw it kq b kr ks kv kw kz mx ld my lh mz ll na nb nc nd bi translated">计算当前梯度</li><li id="e3dd" class="mv mw it kq b kr ne kv nf kz ng ld nh lh ni ll na nb nc nd bi translated">计算当前对角Hessian近似</li><li id="e83c" class="mv mw it kq b kr ne kv nf kz ng ld nh lh ni ll na nb nc nd bi translated">通过将其与早期值混合来平均梯度</li><li id="3a28" class="mv mw it kq b kr ne kv nf kz ng ld nh lh ni ll na nb nc nd bi translated">通过将它与早期的值混合来平均Hessian，并注意对角线元素是正的(否则，我们可能会以上升的方向结束移动)</li><li id="14dc" class="mv mw it kq b kr ne kv nf kz ng ld nh lh ni ll na nb nc nd bi translated">计算牛顿更新步骤，包括学习率参数，以避免过大的步骤导致发散</li></ul><p id="a65c" class="pw-post-body-paragraph ko kp it kq b kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">在图6中，一个简单的二次函数用梯度下降和AdaHessian最小化。这里，AdaHessian是不带动量使用的。在这个玩具示例中，与梯度下降相比，它收敛得更快，并且不需要调整学习速率。当然，最小化二次函数有利于二阶优化器。</p><figure class="mr ms mt mu gt kd gh gi paragraph-image"><div role="button" tabindex="0" class="ke kf di kg bf kh"><div class="gh gi ny"><img src="../Images/9cbdbec3cf84f4756ba43c8ac7731317.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kBd3Xs9sKjWnsLksYARlRQ.png"/></div></div><p class="kk kl gj gh gi km kn bd b be z dk translated">图6:二次函数最小值的三个步骤。左:具有适当学习速率的梯度下降。右图:使用对角海森近似的牛顿方法(右图)。(图片由作者提供)</p></figure><h1 id="717e" class="ln lo it bd lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk bi translated">结论</h1><p id="56a8" class="pw-post-body-paragraph ko kp it kq b kr ml kt ku kv mm kx ky kz mn lb lc ld mo lf lg lh mp lj lk ll im bi translated">AdaHessian通过(1)使用对角线近似，(2)利用Hutchinson的方法和(PyTorch中包含的自动微分函数解决了计算Hessian的任务。</p><p id="48b6" class="pw-post-body-paragraph ko kp it kq b kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">链接:</p><ul class=""><li id="b5d5" class="mv mw it kq b kr ks kv kw kz mx ld my lh mz ll na nb nc nd bi translated"><a class="ae lm" href="https://arxiv.org/pdf/2006.00719.pdf" rel="noopener ugc nofollow" target="_blank">论文</a></li><li id="9dc2" class="mv mw it kq b kr ne kv nf kz ng ld nh lh ni ll na nb nc nd bi translated"><a class="ae lm" href="https://github.com/amirgholami/adahessian" rel="noopener ugc nofollow" target="_blank">原PyTorch实现</a></li><li id="f1fe" class="mv mw it kq b kr ne kv nf kz ng ld nh lh ni ll na nb nc nd bi translated"><a class="ae lm" href="https://github.com/davda54/ada-hessian" rel="noopener ugc nofollow" target="_blank">另一个PyTorch实现，具有简化的接口</a></li><li id="0a99" class="mv mw it kq b kr ne kv nf kz ng ld nh lh ni ll na nb nc nd bi translated"><a class="ae lm" href="https://github.com/githubharald/analyze_ada_hessian" rel="noopener ugc nofollow" target="_blank">对2D函数进行分析的代码</a></li><li id="e68b" class="mv mw it kq b kr ne kv nf kz ng ld nh lh ni ll na nb nc nd bi translated"><a class="ae lm" href="https://gist.github.com/githubharald/201196d445373dacbfda60e1942db2ca" rel="noopener ugc nofollow" target="_blank">代码分析所描述的Hessian计算</a></li></ul></div></div>    
</body>
</html>