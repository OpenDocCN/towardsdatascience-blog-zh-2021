<html>
<head>
<title>Graph Neural Networks: A learning journey since 2008 — Graph Convolutional Network</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">图形神经网络:2008年以来的学习之旅——图形卷积网络</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/graph-neural-networks-a-learning-journey-since-2008-graph-convolution-network-aadd77e91606?source=collection_archive---------15-----------------------#2021-12-01">https://towardsdatascience.com/graph-neural-networks-a-learning-journey-since-2008-graph-convolution-network-aadd77e91606?source=collection_archive---------15-----------------------#2021-12-01</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><p id="73f7" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">图形卷积网络(GCN)在数学上很难理解，但是让我们跟随我的第四篇文章，一步一步地分解GCN</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ko"><img src="../Images/4be60b82e84a059d81d5089e2220cc01.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EHwkYRvp9NcspgRdCWLhyQ.jpeg"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">约翰·罗德恩·卡斯蒂略在<a class="ae le" href="https://unsplash.com/photos/rQqWOHZ96OM" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的图片</p></figure><div class="lf lg gp gr lh li"><a href="https://medium.com/@stefanobosisio1/membership" rel="noopener follow" target="_blank"><div class="lj ab fo"><div class="lk ab ll cl cj lm"><h2 class="bd iu gy z fp ln fr fs lo fu fw is bi translated">通过我的推荐链接加入Medium-Stefano Bosisio</h2><div class="lp l"><h3 class="bd b gy z fp ln fr fs lo fu fw dk translated">作为一个媒体会员，你的会员费的一部分会给你阅读的作家，你可以完全接触到每一个故事…</h3></div><div class="lq l"><p class="bd b dl z fp ln fr fs lo fu fw dk translated">medium.com</p></div></div><div class="lr l"><div class="ls l lt lu lv lr lw ky li"/></div></div></a></div><p id="6778" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我以前关于图形和ML的帖子:</p><ul class=""><li id="0927" class="lx ly it js b jt ju jx jy kb lz kf ma kj mb kn mc md me mf bi translated"><a class="ae le" rel="noopener" target="_blank" href="/graph-neural-networks-a-learning-journey-since-2008-part-1-7df897834df9?source=your_stories_page----------------------------------------">图形神经网络:2008年以来的学习之旅——第一部分</a></li><li id="49e2" class="lx ly it js b jt mg jx mh kb mi kf mj kj mk kn mc md me mf bi translated"><a class="ae le" rel="noopener" target="_blank" href="/graph-neural-networks-a-learning-journey-since-2008-part-2-22dbf7a3b0d?source=your_stories_page----------------------------------------">图形神经网络:2008年以来的学习之旅——第二部分</a></li><li id="cf75" class="lx ly it js b jt mg jx mh kb mi kf mj kj mk kn mc md me mf bi translated"><a class="ae le" rel="noopener" target="_blank" href="/graph-neural-networks-a-learning-journey-since-2008-deep-walk-e424e716070a?source=your_stories_page----------------------------------------">图形神经网络:2008年以来的学习之旅——深度行走</a></li><li id="599a" class="lx ly it js b jt mg jx mh kb mi kf mj kj mk kn mc md me mf bi translated"><a class="ae le" rel="noopener" target="_blank" href="/graph-neural-networks-a-learning-journey-since-2008-python-deep-walk-29c3e31432f?source=your_stories_page----------------------------------------">图形神经网络:2008年以来的学习之旅——Python&amp;深度行走</a></li></ul><p id="9af8" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在我们之前的文章中，我们看到了斯卡塞利的图形神经网络思想[1–4]如何彻底改变了构建典型ML问题的方式。自2008年以来的几年中，图上的ML已经成为一个热门话题，越来越完善和改进了Scarselli的第一个开创性方法。上一次我们看到了Perozzi的DeepWalk方法[5]对GNN的改进，社区第一次开始讨论“节点嵌入”[6–8]。</p><p id="bc25" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">今天要给大家展示一个非常流行的算法:图卷积网络[9]。这篇文章涵盖了2014年至2017年间的出版物。在这3年里，科学家们投入了大量的精力将著名的卷积神经网络方法应用于图形。事实上，为什么卷积在图像上非常有效？为什么不能直接应用到图中？如何将图转化为卷积项？有什么数学方法可以让我们达到这个目的吗？</p><p id="9d04" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在这篇文章中，我们将涵盖GCN的理论方面，从定义卷积运算到图形卷积。下一次，我们将介绍Kipf GCN算法的Tensorflow实现。</p><h1 id="6270" class="ml mm it bd mn mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni bi translated">卷积什么？</h1><p id="6542" class="pw-post-body-paragraph jq jr it js b jt nj jv jw jx nk jz ka kb nl kd ke kf nm kh ki kj nn kl km kn im bi translated">神经网络是一个连续的<em class="no">仿射变换</em>【15，16】:给定一个输入向量，网络将其乘以一些矩阵，生成一个输出。输入向量可以是1D信号、图像、视频，并且通常是多维数组。该阵列具有不同的轴，例如，图像具有专用于RGB通道的轴，另一个轴定义高度，另一个轴定义宽度。仿射变换中不使用轴，这里卷积开始发挥重要作用，以帮助神经网络输出产品。离散卷积[17]是一种<em class="no">线性变换，</em>，其中输入特征图和卷积核相乘</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi np"><img src="../Images/b17ba7d82994e2de29d61fc9d1aa6713.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hZ0_uobhDHrbt79MDEQWSg.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">图1:图像卷积的例子。A)3×3像素大小的核乘以给定图像(输入特征图，例如10×10)中的重叠区域。橙色阴影区域定义了与卷积(黑色)内核相乘的输入要素地图区域。b)对于重叠的3x3区域，点积返回1个单值。基于步距值，卷积继续下一个输入区域(例如，这里我们将卷积核移动3个像素)。作者图片</p></figure><p id="14dd" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">每个重叠的输入特征地图片是具有滑动卷积核的点积，返回最终的输出图像/矢量。为了固定一些关于卷积的一般概念，我们可以用数学方法将卷积定义为积分:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/7a3e8e695cec84db3943d2f1bc3ce935.png" data-original-src="https://miro.medium.com/v2/resize:fit:1044/format:webp/1*FMLS1uOiVFmOs1G3UO2VMw.png"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">等式1:卷积积分。在时域中，在输入特征矩阵f上执行的卷积是输入f的域上的积分</p></figure><p id="f010" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">其中<em class="no"> f </em>为输入特征图，<em class="no">圆星</em>为时域卷积运算，<em class="no"> g </em>为输入卷积核。卷积是在输入特征映射域τ上的积分，由<em class="no"> dτ </em>定义(例如，这可以是立方体、由图像定义的平面、向量等)，<em class="no"> g(t- τ) </em>是卷积核，其在输入特征映射上滑动τ步<em class="no"> f(τ) </em>。这个积分可以转换成一个和，得到一个离散的卷积。这里，内核可以基于以下参数化:</p><ul class=""><li id="ff74" class="lx ly it js b jt ju jx jy kb lz kf ma kj mb kn mc md me mf bi translated"><em class="no"> n </em>输出特征地图的数量</li><li id="e09f" class="lx ly it js b jt mg jx mh kb mi kf mj kj mk kn mc md me mf bi translated"><em class="no"> m </em>输入特征地图的数量</li><li id="39fc" class="lx ly it js b jt mg jx mh kb mi kf mj kj mk kn mc md me mf bi translated"><em class="no"> k </em>轴上定义的ⱼthe内核尺寸<em class="no"> j </em></li></ul><p id="8a38" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">卷积层沿轴<em class="no"> j </em>的输出大小<em class="no"> oⱼ </em>受以下因素影响:</p><ul class=""><li id="1e68" class="lx ly it js b jt ju jx jy kb lz kf ma kj mb kn mc md me mf bi translated"><em class="no">I</em>ⱼ<em class="no">t31】输入沿轴尺寸<em class="no"> j </em></em></li><li id="9c46" class="lx ly it js b jt mg jx mh kb mi kf mj kj mk kn mc md me mf bi translated"><em class="no"> kⱼ </em>沿轴线的内核尺寸<em class="no"> j </em></li><li id="f2f7" class="lx ly it js b jt mg jx mh kb mi kf mj kj mk kn mc md me mf bi translated"><em class="no"> sⱼ </em>沿轴的步幅<em class="no"> j </em></li><li id="223a" class="lx ly it js b jt mg jx mh kb mi kf mj kj mk kn mc md me mf bi translated"><em class="no"> pⱼ </em>沿轴补零<em class="no"> j </em></li></ul><p id="af83" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我们可能会发现这些术语不时出现在我们的卷积码中，为了使它更清楚:</p><ul class=""><li id="0a1b" class="lx ly it js b jt ju jx jy kb lz kf ma kj mb kn mc md me mf bi translated">步幅是内核的两个连续位置之间的距离</li><li id="b2c6" class="lx ly it js b jt mg jx mh kb mi kf mj kj mk kn mc md me mf bi translated">零填充是添加到图像中的零的附加“帧”,通常是为了使图像大小达到2的幂</li></ul><p id="9c6a" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">卷积与神经网络配合得很好，提供了输入信号的线性变换。然而，卷积只有且仅当:</p><ul class=""><li id="51e4" class="lx ly it js b jt ju jx jy kb lz kf ma kj mb kn mc md me mf bi translated">可以在网格上描述输入信号(例如向量、图像、3D立方体)</li><li id="bd98" class="lx ly it js b jt mg jx mh kb mi kf mj kj mk kn mc md me mf bi translated">输入具有主要的局部统计(例如，一组像素主导图像的信息内容)</li></ul><p id="9d99" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">当处理3D网格或社交媒体数据，即图形时，这是一个很大的问题。这些对象没有基于网格的数学域，因此卷积不能将图形作为输入信号进行处理。然而，数学可以帮助我们，提供一个绝妙的解决方案。</p><h2 id="ab79" class="nr mm it bd mn ns nt dn mr nu nv dp mv kb nw nx mz kf ny nz nd kj oa ob nh oc bi translated">图表:数学见解</h2><p id="4458" class="pw-post-body-paragraph jq jr it js b jt nj jv jw jx nk jz ka kb nl kd ke kf nm kh ki kj nn kl km kn im bi translated">为了在图上应用卷积，第一个技巧是获得地图<em class="no">位置</em> [10]的新定义。在图像情况下，局部性可以被认为是主要的局部统计。在图<em class="no">中，G=(N，E) </em>(或<em class="no"> G=(V，E) </em>根据Kipf的论文【9】)，给定阈值δ，局部性可以基于节点的邻域来表示:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi od"><img src="../Images/1a3c90d059f855f79a0d08b22b2eec26.png" data-original-src="https://miro.medium.com/v2/resize:fit:940/format:webp/1*eRUavAVDaMMFBWOkdP5XgA.png"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">等式2:图中的局部性。给定节点特征必须满足的阈值，我们可以定义节点j的局部性，查看满足给定特征值Wij阈值的所有节点I</p></figure><p id="bdf2" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在给定一个节点<em class="no"> j </em>的情况下，邻域局部性可以被表示为其特征<em class="no"> W </em> ᵢⱼare大于给定阈值的所有那些节点<em class="no"> i </em>。</p><p id="3836" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">由此，有可能得到该图的紧凑表示，其中节点的邻域平均值可以用作节点特征值，如图2所示</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi oe"><img src="../Images/eb7207f862c1ad47fdfce4b25621346d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4plGqEfhzUfFfHCX7uu71w.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">图2:图中的位置。通过等式2定义局部性可以通过将其特征与其所有相邻(红色圆圈)特征的值进行平均来获得红色节点的隐藏表示。图片由作者提供，灵感来自吴<em class="of">等人</em>【18】</p></figure><p id="6ddc" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">对于平均局部性，卷积滤波器可以接受邻域主导统计，输入层的大小最大为<em class="no"> O(S*n) </em>，其中<em class="no"> S </em>是给定<em class="no"> n </em>个节点的邻域大小的平均值。</p><p id="f62b" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">第二个基本技巧是从图的数学表示中提取属性，拉普拉斯矩阵<em class="no">L</em>【19】。一个拉普拉斯定义为<em class="no"> L = D — A </em>，其中<em class="no"> D </em>为度矩阵，<em class="no"> A </em>为邻接矩阵。<em class="no"> D </em>矩阵指出每个节点有多少个连接，而<em class="no"> A </em>哪些节点相互连接(图3)</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi og"><img src="../Images/342db98f7e4d41dc29f696e3b86de625.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*V6CCXA2zBB6wv0WUKKGY7w.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">图3左边的示例图，每个节点的度D矩阵，邻接矩阵A和拉普拉斯L，其中L=D-A。</p></figure><p id="d4fe" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">因此，Laplacian是描述图以及节点的连接如何影响图的动力学的一种伟大的、简洁的和高度信息化的方式。为了进一步欣赏和理解拉普拉斯在图上的作用，我们可以看到一个经典的例子，它解释了热量如何在给定的图上扩散。给定一个热变量<em class="no">φ</em>，其中<em class="no">φᵢ</em>是节点<em class="no"> i </em>和节点<em class="no"> j </em>中<em class="no">φ</em>ⱼ的热量，从经典物理学中我们知道从<em class="no"> i </em>传递到<em class="no"> j </em>的热量与材料热容<em class="no"> κ </em>成正比，如<em class="no">κ(φᵢ—φⱼ)</em>。因此，热量随时间在曲线图中的演变可以通过以下等式来描述:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/1a7bbe34781d22d4285ec28071cd5aa1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1238/format:webp/1*P1Dc9AJ6wW_xxsSRGyRRBw.png"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">方程3:图上的热量方程。节点I和j之间的热传递可以容易地分解成图的拉普拉斯算子</p></figure><p id="ade7" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><a class="ae le" href="https://en.wikipedia.org/wiki/Laplacian_matrix#Example_of_the_operator_on_a_grid" rel="noopener ugc nofollow" target="_blank">这个等式可以用Python代码实现，我们可以得到热扩散:</a></p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi oi"><img src="../Images/a4ce6366deb8298797206bdf72be89fe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1120/1*gz2hyrcSSJG9MtDzmQLe3w.gif"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">图4:通过拉普拉斯热方程在图上的热扩散。随着时间的推移，热量在节点的邻居之间平稳地扩散。图片由<a class="ae le" href="https://commons.wikimedia.org/w/index.php?title=User:Ctralie&amp;action=edit&amp;redlink=1" rel="noopener ugc nofollow" target="_blank"> Ctralie </a>、<a class="ae le" href="https://creativecommons.org/licenses/by-sa/3.0/" rel="noopener ugc nofollow" target="_blank">知识共享许可</a>、<a class="ae le" href="https://en.wikipedia.org/wiki/Laplacian_matrix#/media/File:Graph_Laplacian_Diffusion_Example.gif" rel="noopener ugc nofollow" target="_blank">链接</a></p></figure><p id="86bc" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">正如你所看到的，拉普拉斯矩阵L告诉我们热量如何从(本例中为三个)起始点均匀地扩散到整个图表中。</p><h2 id="e8b6" class="nr mm it bd mn ns nt dn mr nu nv dp mv kb nw nx mz kf ny nz nd kj oa ob nh oc bi translated">填补空白:拉普拉斯和傅立叶变换</h2><p id="745e" class="pw-post-body-paragraph jq jr it js b jt nj jv jw jx nk jz ka kb nl kd ke kf nm kh ki kj nn kl km kn im bi translated"><em class="no"> L </em>可以表示为对称正交矩阵(等式4)</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/d475a5712124e8037068e9f9b356edb1.png" data-original-src="https://miro.medium.com/v2/resize:fit:682/format:webp/1*_ByXQxqHKBdHwo4AARht1w.png"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">等式4:拉普拉斯算子可以表示为对称正交矩阵</p></figure><p id="6e19" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">其中<em class="no"> I </em>是单位矩阵，<em class="no"> -1/2 </em>是度矩阵<em class="no"> D </em>的幂，<em class="no"> A </em>是邻接矩阵。对称正交L矩阵是半正定的，这保证了它有实的特征向量和特征值。</p><p id="02a9" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">该条件允许通过频谱分解(等式5)来表达<em class="no"> L </em></p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi ok"><img src="../Images/0b1ea37258947579e8407041abdf352f.png" data-original-src="https://miro.medium.com/v2/resize:fit:394/format:webp/1*0Z_3_jI5PVYw6m7IAgcJoQ.png"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">等式5:拉普拉斯矩阵的频谱分解</p></figure><p id="e7cb" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">其中<em class="no"> U </em>是正交矩阵基，<em class="no">λ</em>是具有正特征值的对角矩阵，<em class="no"> T </em>幂代表转置。<em class="no">uλ</em>构成实数域中的标准正交基。由此可见,<em class="no"> L </em>的每个特征向量可以与图中的相应节点相关联。如果节点改变位置，那么特征向量将相应地重新排列— <em class="no">排列不变。</em>其次，<em class="no"> L </em>的本征函数可以进一步重排为复指数。</p><p id="994f" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">此时，你的脑海中应该会响起一个铃声。<a class="ae le" href="https://czxttkl.com/2018/10/07/eulers-formula/" rel="noopener ugc nofollow" target="_blank">通过欧拉公式，复指数可视为时域中的一系列正弦曲线</a>。这意味着傅立叶变换和拉普拉斯特征值之间存在关系。在傅立叶域中，卷积(等式1)是输入信号<em class="no"> x </em>和卷积核gϑ:之间的乘法</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ol"><img src="../Images/3ff8cd8e1abdd3ba6a443dd0a8986c67.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0UycjTw0MaeSGmRLiQT0hQ.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">等式6:输入信号的拉普拉斯算子(图)和傅立叶域中的卷积之间的关键关系</p></figure><p id="0c1c" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">其中<em class="no"> F </em>代表傅立叶变换，<em class="no"> U </em>是归一化图拉普拉斯的特征向量的矩阵(等式5)。这意味着拉普拉斯算子定义了傅立叶基，因此它返回输入图的傅立叶基视图，因此拉普拉斯算子可用于计算图上的卷积。具体而言，通过计算图的拉普拉斯算子的特征向量，可以在给定图的频谱上计算卷积及其权重[20]。</p><h1 id="e749" class="ml mm it bd mn mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni bi translated">Thomas Kipf和Max Welling在2017年发表的《图形卷积网络》</h1><p id="09fc" class="pw-post-body-paragraph jq jr it js b jt nj jv jw jx nk jz ka kb nl kd ke kf nm kh ki kj nn kl km kn im bi translated">现在，所有这些数学框架都很棒，我们可以考虑将拉普拉斯谱分解应用于一个图，将其传递给一些神经网络层和激活函数，工作就完成了。不幸的是，通过拉普拉斯算子的卷积在计算上是不可行的，并且非常昂贵。为了解决这个问题，Kipf和Welling以及Hammond在2011年的论文中提出了傅立叶卷积滤波器gϑ:的切比雪夫多项式<em class="no"> Tₖ(x) </em>的截断展开</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi om"><img src="../Images/20f379e5eac54be00c7973468fe9b6ec.png" data-original-src="https://miro.medium.com/v2/resize:fit:572/format:webp/1*eoOTQFQr71ldIVSUi-JwEg.png"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">方程7:用切比雪夫多项式逼近卷积滤波器</p></figure><p id="96f9" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">其中<em class="no">θ’</em>表示切比雪夫多项式系数的向量，<em class="no"> Tₖ </em>是切比雪夫多项式的递归公式，λ是基于图拉普拉斯矩阵的最大特征值的重定标常数。将等式7并入等式7，可以获得卷积的最终切比雪夫近似:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi on"><img src="../Images/99c77555d269b733d1aab748ccc09a8d.png" data-original-src="https://miro.medium.com/v2/resize:fit:780/format:webp/1*YuUVZeJUcYnjCb6oRfS_Iw.png"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">等式8:图形卷积的切比雪夫近似</p></figure><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi oo"><img src="../Images/4e1fefd0d5e7aca8e47ed259040ee397.png" data-original-src="https://miro.medium.com/v2/resize:fit:594/format:webp/1*rZrqKFfS6XdEgAbdh8_z3Q.png"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">等式9:重新缩放的拉普拉斯算子</p></figure><p id="46cb" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">其中tilded L是重新缩放的图拉普拉斯算子(等式9)。该表达式是<em class="no"> K </em>局部化的，即它是拉普拉斯算子中的第<em class="no">K</em>阶切比雪夫多项式，因此它仅依赖于距离中心节点最多<em class="no"> K </em>步的节点<em class="no"> N </em>(重新定义图的局部性)。然后计算复杂度降低到<em class="no"> O(E) </em>，所以依赖于边数。</p><p id="8c18" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">给定等式。8上图卷积网络可以通过堆叠多个卷积层来实现，每层之后是逐点非线性。因此，对于给定的层<em class="no"> l+1 </em>，图的逐层传播将是:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi op"><img src="../Images/4a6947ae3581e16d30b4e36231fccdf6.png" data-original-src="https://miro.medium.com/v2/resize:fit:968/format:webp/1*9m1J-fCwmVShh8aYixsPmQ.png"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">等式10:GCN的逐层传播规则</p></figure><p id="cf31" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">其中<em class="no"> H </em>是第<em class="no">l</em>或<em class="no">l+1</em>层的激活矩阵，σ是类似于<em class="no"> ReLu </em>的激活函数，<em class="no"> W </em>是特定层的可训练权重矩阵。这是图卷积神经网络的核心公式，由Kipf在2017年提出。现在让我们看看空手道俱乐部的Python实现！</p><h1 id="37ab" class="ml mm it bd mn mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni bi translated">空手道俱乐部和GCN</h1><p id="8e09" class="pw-post-body-paragraph jq jr it js b jt nj jv jw jx nk jz ka kb nl kd ke kf nm kh ki kj nn kl km kn im bi translated">首先，让我们直接从<code class="fe oq or os ot b">networkx</code> Python库加载空手道俱乐部:</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="ou ov l"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">图5:导入库并创建空手道幼崽</p></figure><p id="4a05" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">其次，我们可以创建图邻接矩阵<code class="fe oq or os ot b">adj</code>和一个简单的一键编码图特征矩阵<code class="fe oq or os ot b">X</code> <em class="no"> : </em></p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="ou ov l"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">图6:创建空手道俱乐部邻接表和特征(一键编码矩阵)</p></figure><p id="33e7" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">然后，我们将创建度矩阵<code class="fe oq or os ot b">deg</code>，通过一个自连接邻接矩阵<code class="fe oq or os ot b">adj_self</code>，即带有自项连接的邻接矩阵:</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="ou ov l"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">图7:邻接自连接矩阵和归一化度矩阵</p></figure><p id="b42f" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">现在，是时候实现GCN方法了，如图8所示。最初，我们需要定义ReLu函数，这只是<code class="fe oq or os ot b">np.maximum(input_value, 0)</code>。其次，我们可以定义GCN中的逐层传播规则，即等式10。该函数将邻接矩阵、度矩阵、图形特征矩阵和第I层权重作为输入。最后，初始化层权重，设置2层——但是你可以设置任意多的层——给GCN一个机会！</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="ou ov l"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">图8: GCN实现:ReLu函数、分层传播、权重初始化和模型运行。</p></figure><p id="9f75" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">请继续关注我们的下一篇文章:我们将深入研究Kipf使用Tensorflow的GCN实现。</p></div><div class="ab cl ow ox hx oy" role="separator"><span class="oz bw bk pa pb pc"/><span class="oz bw bk pa pb pc"/><span class="oz bw bk pa pb"/></div><div class="im in io ip iq"><p id="53f2" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">如果有任何问题或意见，请随时给我发电子邮件，地址是:stefanobosisio1@gmail.com，或者直接在Medium这里。</p><h1 id="b704" class="ml mm it bd mn mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni bi translated">文献学</h1><ol class=""><li id="9068" class="lx ly it js b jt nj jx nk kb pd kf pe kj pf kn pg md me mf bi translated">《网页排序的图形神经网络》。<em class="no">2005年IEEE/WIC/ACM网络智能国际会议(WI'05) </em>。IEEE，2005年。</li><li id="4f8c" class="lx ly it js b jt mg jx mh kb mi kf mj kj mk kn pg md me mf bi translated">戈德堡，安德鲁v，和克里斯哈里森。"计算最短路径:搜索符合图论."<em class="no">汽水</em>。第五卷。2005.</li><li id="ec5f" class="lx ly it js b jt mg jx mh kb mi kf mj kj mk kn pg md me mf bi translated">《网页排序的图形神经网络》。<em class="no">2005年IEEE/WIC/ACM网络智能国际会议(WI'05) </em>。IEEE，2005年。</li><li id="4304" class="lx ly it js b jt mg jx mh kb mi kf mj kj mk kn pg md me mf bi translated">《图形神经网络模型》<em class="no"> IEEE神经网络汇刊</em>20.1(2008):61–80。</li><li id="46b5" class="lx ly it js b jt mg jx mh kb mi kf mj kj mk kn pg md me mf bi translated">佩罗齐、布莱恩、拉米·艾尔弗和史蒂文·斯基纳。"深度行走:社交表征的在线学习."第20届ACM SIGKDD知识发现和数据挖掘国际会议论文集。2014.</li><li id="7e99" class="lx ly it js b jt mg jx mh kb mi kf mj kj mk kn pg md me mf bi translated">《向量空间中单词表征的有效估计》arXiv预印本arXiv:1301.3781  (2013)</li><li id="7447" class="lx ly it js b jt mg jx mh kb mi kf mj kj mk kn pg md me mf bi translated">唐、雷、。"通过潜在社会维度的关系学习."<em class="no">第15届ACM SIGKDD知识发现和数据挖掘国际会议论文集</em>。2009.</li><li id="591b" class="lx ly it js b jt mg jx mh kb mi kf mj kj mk kn pg md me mf bi translated">唐、雷、。“利用社交媒体网络进行分类。”<em class="no">数据挖掘和知识发现</em>23.3(2011):447–478。</li><li id="cde7" class="lx ly it js b jt mg jx mh kb mi kf mj kj mk kn pg md me mf bi translated">基普夫，托马斯n，和马克斯韦林。"图卷积网络的半监督分类."<em class="no"> arXiv预印本arXiv:1609.02907 </em> (2016)。</li><li id="fa0e" class="lx ly it js b jt mg jx mh kb mi kf mj kj mk kn pg md me mf bi translated">琼·布鲁纳等着《图上的谱网络和深局部连通网络》第二届学习代表国际会议，ICLR 。第2014卷。2014.</li><li id="ab4e" class="lx ly it js b jt mg jx mh kb mi kf mj kj mk kn pg md me mf bi translated">阿特伍德，詹姆斯和唐·陶斯利。"扩散卷积神经网络."<em class="no">神经信息处理系统的进展</em>。2016.</li><li id="bf33" class="lx ly it js b jt mg jx mh kb mi kf mj kj mk kn pg md me mf bi translated">迪费拉德、米歇尔、泽维尔·布列松和皮埃尔·范德盖恩斯特。"具有快速局部谱滤波的图上的卷积神经网络."神经信息处理系统进展29(2016):3844–3852。</li><li id="20db" class="lx ly it js b jt mg jx mh kb mi kf mj kj mk kn pg md me mf bi translated">学习分子指纹的图形卷积网络。<em class="no"> arXiv预印本arXiv:1509.09292 </em> (2015)。</li><li id="fb7c" class="lx ly it js b jt mg jx mh kb mi kf mj kj mk kn pg md me mf bi translated">哈蒙德、大卫·k、皮埃尔·范德盖恩斯特和雷米·格里邦瓦尔。"通过谱图论研究图上的小波."<em class="no">应用和计算谐波分析</em>30.2(2011):129–150。</li><li id="048d" class="lx ly it js b jt mg jx mh kb mi kf mj kj mk kn pg md me mf bi translated">学习仿射变换。<em class="no">模式识别</em>32.10(1999):1783–1799。</li><li id="09dc" class="lx ly it js b jt mg jx mh kb mi kf mj kj mk kn pg md me mf bi translated">李，文静，和唐利。"仿射不变匹配的Hopfield神经网络."IEEE神经网络汇刊12.6(2001):1400–1410。</li><li id="dbad" class="lx ly it js b jt mg jx mh kb mi kf mj kj mk kn pg md me mf bi translated">离散傅立叶变换和卷积的算法。斯普林格，1989年。</li><li id="6f00" class="lx ly it js b jt mg jx mh kb mi kf mj kj mk kn pg md me mf bi translated">吴，，等，“图神经网络综述”<em class="no"> IEEE神经网络和学习系统汇刊</em>32.1(2020):4–24。</li><li id="f167" class="lx ly it js b jt mg jx mh kb mi kf mj kj mk kn pg md me mf bi translated">《拉普拉斯矩阵》<a class="ae le" href="https://mathworld." rel="noopener ugc nofollow" target="_blank"> <em class="no"> https://mathworld。</em> </a> <em class="no">沃尔夫拉姆。com/ </em> (1999)。</li><li id="5513" class="lx ly it js b jt mg jx mh kb mi kf mj kj mk kn pg md me mf bi translated">Singh，Rahul，Abhishek Chakraborty和B. S. Manoj。"基于有向拉普拉斯的图形傅立叶变换."<em class="no"> 2016国际信号处理与通信会议(SPCOM) </em>。IEEE，2016。</li></ol></div></div>    
</body>
</html>