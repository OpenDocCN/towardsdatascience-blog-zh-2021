<html>
<head>
<title>Reinforcement Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">强化学习</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/reinforcement-learning-fda8ff535bb6?source=collection_archive---------12-----------------------#2021-06-12">https://towardsdatascience.com/reinforcement-learning-fda8ff535bb6?source=collection_archive---------12-----------------------#2021-06-12</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="c3a5" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsdatascience.com/tagged/thoughts-and-theory" rel="noopener" target="_blank">思想和理论</a></h2><div class=""/><div class=""><h2 id="3873" class="pw-subtitle-paragraph jw iz iq bd b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn dk translated">对这种特殊形式的机器学习的历史、现代和未来应用的回顾</h2></div><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ko"><img src="../Images/68b93c46fd7fe4043049fc24e7262dab.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Y0TDuXNyywjqqr5l5GkMQQ.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">强化学习(<a class="ae le" href="https://cdn.iconscout.com/icon/premium/png-512-thumb/reinforcement-learning-2040769-1721120.png" rel="noopener ugc nofollow" target="_blank">图片</a>由<a class="ae le" href="https://iconscout.com/" rel="noopener ugc nofollow" target="_blank">图标上的平面图标显示</a>授权给克里斯·马奥尼)</p></figure><h1 id="8a20" class="lf lg iq bd lh li lj lk ll lm ln lo lp kf lq kg lr ki ls kj lt kl lu km lv lw bi translated">内容</h1><p id="afd9" class="pw-post-body-paragraph lx ly iq lz b ma mb ka mc md me kd mf mg mh mi mj mk ml mm mn mo mp mq mr ms ij bi translated">1.<a class="ae le" href="#b1da" rel="noopener ugc nofollow">简介</a> <br/> 2。<a class="ae le" href="#5554" rel="noopener ugc nofollow">历史发展(1992年以前)</a> <br/> — 2.1。<a class="ae le" href="#ca1d" rel="noopener ugc nofollow">并行开发</a><br/>———2 . 1 . 1。<a class="ae le" href="#703d" rel="noopener ugc nofollow">试错学习</a> <br/> — — 2.1.2。<a class="ae le" href="#757c" rel="noopener ugc nofollow">最优控制问题</a><br/>———2 . 1 . 3。<a class="ae le" href="#4b10" rel="noopener ugc nofollow">时间差异学习方法</a> <br/> — 2.2。<a class="ae le" href="#51b2" rel="noopener ugc nofollow">综合发展</a> <br/> 3。<a class="ae le" href="#7383" rel="noopener ugc nofollow">现代发展(1992年后)<br/> </a> — 3.1。<a class="ae le" href="#70c7" rel="noopener ugc nofollow">桌游发展</a> <br/> — 3.2。电脑游戏的发展。<a class="ae le" href="#19c2" rel="noopener ugc nofollow">当前动态</a> <br/> 5。<a class="ae le" href="#e66f" rel="noopener ugc nofollow">未来发展</a> <br/> 6。<a class="ae le" href="#fd37" rel="noopener ugc nofollow">结论</a> <br/> 7。<a class="ae le" href="#4bed" rel="noopener ugc nofollow">参考文献</a></p><h1 id="b1da" class="lf lg iq bd lh li lj lk ll lm ln lo lp kf lq kg lr ki ls kj lt kl lu km lv lw bi translated">1.介绍</h1><p id="bf68" class="pw-post-body-paragraph lx ly iq lz b ma mb ka mc md me kd mf mg mh mi mj mk ml mm mn mo mp mq mr ms ij bi translated">强化学习并不是一个新概念，而是在70多年的学术严谨中发展成熟的。从根本上说，强化学习是一种机器学习的方法，通过这种方法，算法可以在给定的环境中做出决定并采取行动，并通过反复试错的行动来学习做出什么样的适当决定。在20世纪90年代的研究中，强化学习的学术论述追求三个并行的研究“线索”(试错法、最优控制和时间差)。然后，强化学习能够继续掌握下棋、下围棋和无数电子游戏。强化学习的现代应用使企业能够优化、控制和监控各自的流程，达到惊人的准确度和精细度。因此，强化学习的未来既令人兴奋又令人着迷，因为该研究旨在提高算法的可解释性、可问责性和可信度。</p><h1 id="5554" class="lf lg iq bd lh li lj lk ll lm ln lo lp kf lq kg lr ki ls kj lt kl lu km lv lw bi translated">2.历史发展(1992年以前)</h1><p id="1b23" class="pw-post-body-paragraph lx ly iq lz b ma mb ka mc md me kd mf mg mh mi mj mk ml mm mn mo mp mq mr ms ij bi translated">萨顿和巴尔托(<a class="ae le" href="#2338" rel="noopener ugc nofollow"> 2018 </a>)讨论了强化学习的三个“线索”:1)通过试错法学习；2)最优控制问题；和3)时间差异学习方法。这些线索在20世纪80年代交织在一起之前，由研究人员独立进行，导致了我们今天所知道的强化学习的概念。</p><h2 id="ca1d" class="mt lg iq bd lh mu mv dn ll mw mx dp lp mg my mz lr mk na nb lt mo nc nd lv iw bi translated">2.1.并行发展</h2><p id="703d" class="pw-post-body-paragraph lx ly iq lz b ma mb ka mc md me kd mf mg mh mi mj mk ml mm mn mo mp mq mr ms ij bi translated"><strong class="lz ja"> 2.1.1。通过反复试验学习</strong></p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ne"><img src="../Images/7183a686cc929a6c505f11fcaccd4ba3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wkZMtjhzEStBQSa1rR9Q0w.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">桑代克效应定律(<a class="ae le" href="https://www.researchgate.net/profile/Dyar-Jabbary/publication/329428575_Edward_L_Thorndike_Trial_and_Error_Theory/links/5c3bcfaf458515a4c7247ad8/Edward-L-Thorndike-Trial-and-Error-Theory" rel="noopener ugc nofollow" target="_blank">图片</a>由<a class="ae le" href="https://www.researchgate.net/profile/Dyar-Jabbary" rel="noopener ugc nofollow" target="_blank">迪亚尔·贾巴里</a>在<a class="ae le" href="https://www.researchgate.net/" rel="noopener ugc nofollow" target="_blank">研究之门</a>上拍摄)</p></figure><p id="c57c" class="pw-post-body-paragraph lx ly iq lz b ma nf ka mc md ng kd mf mg nh mi mj mk ni mm mn mo nj mq mr ms ij bi translated">在观察动物智力时，桑代克(<a class="ae le" href="#b219" rel="noopener ugc nofollow"> 1911 </a>)将试错法定义为“效果法则”,它与给定情况下产生的满意或不适的感觉相关联。当明斯基(<a class="ae le" href="#f99b" rel="noopener ugc nofollow"> 1954 </a>)假设使用SNARCs(随机神经模拟强化计算器)时，这一概念被整合到机器学习模拟中，当明斯基(<a class="ae le" href="#acc3" rel="noopener ugc nofollow"> 1961 </a>)解决“信用分配问题”时，这一概念被进一步具体化；也就是如何在许多可能产生成功的决策中分配成功的功劳。计算试错过程的研究被推广到模式识别(<a class="ae le" href="#aa51" rel="noopener ugc nofollow">Clark&amp;Farley 1955</a>；<a class="ae le" href="#5036" rel="noopener ugc nofollow"> Farley &amp; Clark 1954 </a>)之前，通过使用错误信息更新连接权重来适应监督学习(<a class="ae le" href="#b35d" rel="noopener ugc nofollow">Rosenblatt 1962</a>；<a class="ae le" href="#0b4f" rel="noopener ugc nofollow"> Widrow &amp;霍夫1960 </a>)。由于传统的试错法和错误损失函数之间的区别模糊，在整个20世纪60年代和70年代，很少有出版物专门讨论强化学习。约翰·安德瑞就是这一领域的一名研究者，他开发了斯特拉系统(<a class="ae le" href="#86da" rel="noopener ugc nofollow">安德瑞1963 </a>)，该系统通过与环境的互动进行学习，还开发了具有“内心独白”的机器(<a class="ae le" href="#7fac" rel="noopener ugc nofollow">安德瑞&amp;卡辛1969 </a>)，以及后来可以向老师学习的机器(<a class="ae le" href="#6cee" rel="noopener ugc nofollow">安德瑞1977 </a>)。不幸的是，正如萨顿&amp;巴尔托(<a class="ae le" href="#2338" rel="noopener ugc nofollow"> 2018 </a>)所讨论的，安德烈的开创性研究并不为人所知，也没有对后续的强化学习研究产生很大影响。</p><p id="757c" class="pw-post-body-paragraph lx ly iq lz b ma nf ka mc md ng kd mf mg nh mi mj mk ni mm mn mo nj mq mr ms ij bi translated">2.1.2。最优控制的问题</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi nk"><img src="../Images/4ce377a6719422e673d7ce670c90eac3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*Mb_CPHFb3j4EiuHk.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">最优控制的问题(<a class="ae le" href="https://miro.medium.com/max/3076/1*ZC9qItK9wI0F6BwSVYMQGg.png" rel="noopener">图像</a>由<a class="ae le" href="https://pradyyadav.medium.com/" rel="noopener"> Pradyumna Yadav </a>对<a class="ae le" href="https://medium.com/analytics-vidhya" rel="noopener"> AnalyticsVidhya </a></p></figure><p id="4b57" class="pw-post-body-paragraph lx ly iq lz b ma nf ka mc md ng kd mf mg nh mi mj mk ni mm mn mo nj mq mr ms ij bi translated">对“最优控制”的研究始于20世纪50年代，被定义为“一个控制器，以最小化动态系统随时间推移的行为的测量”(<a class="ae le" href="#2338" rel="noopener ugc nofollow">萨顿&amp;巴尔托2018 </a>)。Bellman ( <a class="ae le" href="#b611" rel="noopener ugc nofollow"> 1957a </a>)在Hamilton ( <a class="ae le" href="#5617" rel="noopener ugc nofollow"> 1833 </a>、<a class="ae le" href="#4be1" rel="noopener ugc nofollow"> 1834 </a>)和Jacobi ( <a class="ae le" href="#b90b" rel="noopener ugc nofollow"> 1866 </a>)的工作基础上开发了一种专门用于强化学习的方法，该方法使用动态系统的状态动态定义函数方程，并返回最优值函数。这种“最优回报函数”现在通常被称为贝尔曼方程，从技术上来说，这是一类用于解决控制问题的方法，也是贝尔曼的书<em class="nl">动态规划</em> ( <a class="ae le" href="#b611" rel="noopener ugc nofollow">贝尔曼1957a </a>)的主要焦点。贝尔曼继续介绍了马尔可夫决策过程(<a class="ae le" href="#83da" rel="noopener ugc nofollow">1957 b</a>)——他将其定义为“最优控制问题的离散随机版本”——霍华德(<a class="ae le" href="#1d73" rel="noopener ugc nofollow"> 1960 </a>)利用该过程定义了马尔可夫决策过程的政策迭代方法。在20世纪60年代和70年代，在“最优控制”领域没有太多的研究；然而，Bryson ( <a class="ae le" href="#0bb1" rel="noopener ugc nofollow"> 1996 </a>)指出，自20世纪80年代以来，在诸如部分可观测的马尔可夫决策过程及其应用、近似方法、异步方法和动态规划的现代处理等领域发表了许多论文。</p><p id="4b10" class="pw-post-body-paragraph lx ly iq lz b ma nf ka mc md ng kd mf mg nh mi mj mk ni mm mn mo nj mq mr ms ij bi translated"><strong class="lz ja"> 2.1.3。时间差异学习方法</strong></p><p id="51dd" class="pw-post-body-paragraph lx ly iq lz b ma nf ka mc md ng kd mf mg nh mi mj mk ni mm mn mo nj mq mr ms ij bi translated">时间差分学习从数学微分中获得灵感，旨在从一组已知变量中获得预测。但一个重要的区别是，它是通过从价值函数的当前估计值进行引导来实现的。它类似于Monde Carlo方法(<a class="ae le" href="#10d5" rel="noopener ugc nofollow"> Hammersley 1964 </a>)，但它能够在最终结果已知之前，在后期迭代阶段调整最终预测(蒙特卡罗无法做到)；如Sutton ( <a class="ae le" href="#9cc0" rel="noopener ugc nofollow"> 1988 </a>)举例说明的，在给定的星期天，为下一个星期六创建预报，然后在实际的星期六天气已知之前，在星期五更新星期六预报。</p><p id="51b2" class="pw-post-body-paragraph lx ly iq lz b ma nf ka mc md ng kd mf mg nh mi mj mk ni mm mn mo nj mq mr ms ij bi translated">时间差异法的起源植根于动物学习心理学(<a class="ae le" href="#b219" rel="noopener ugc nofollow"> Thorndike 1911 </a>)，尤其是“次级强化物”的概念。实际上，当次级强化物(如刺激物)与初级强化物(如食物或疼痛)配对时，次级强化物具有与初级强化物相似的性质。明斯基(<a class="ae le" href="#f99b" rel="noopener ugc nofollow"> 1954 </a>)是第一个意识到时间差异方法对于强化学习的重要性的人，尽管萨缪尔(<a class="ae le" href="#161b" rel="noopener ugc nofollow"> 1959 </a>)似乎是第二个意识到这种重要性的人，但是他在他的工作中并没有提到明斯基。Minsky ( <a class="ae le" href="#acc3" rel="noopener ugc nofollow"> 1961 </a>)在Samuel ( <a class="ae le" href="#161b" rel="noopener ugc nofollow"> 1959 </a>)的工作基础上，加强了这个概念对强化学习理论的重要性。当Klopf ( <a class="ae le" href="#0367" rel="noopener ugc nofollow"> 1972 </a>，<a class="ae le" href="#ede0" rel="noopener ugc nofollow"> 1975 </a>)研究大系统中的强化学习时，时间差分法和试错法变得纠缠不清，这些强化学习由大系统的单个子组件概念化，每个子组件都有自己的兴奋性输入作为奖励，抑制性输入作为惩罚，并且每个子组件都可以相互强化。</p><h2 id="7331" class="mt lg iq bd lh mu mv dn ll mw mx dp lp mg my mz lr mk na nb lt mo nc nd lv iw bi translated">2.2.综合发展</h2><p id="d49a" class="pw-post-body-paragraph lx ly iq lz b ma mb ka mc md me kd mf mg mh mi mj mk ml mm mn mo mp mq mr ms ij bi translated">利用Klopf的观点，Sutton ( <a class="ae le" href="#0197" rel="noopener ugc nofollow"> 1978a </a>、<a class="ae le" href="#af3e" rel="noopener ugc nofollow"> 1978b </a>、<a class="ae le" href="#560b" rel="noopener ugc nofollow"> 1978c </a>、<a class="ae le" href="#2e0a" rel="noopener ugc nofollow"> 1984 </a>)进一步发展了与动物学习理论的联系，并进一步探索了学习受时间连续预测变化驱动的规则。这项工作实际上为强化学习的研究打开了学术闸门，随后几年见证了许多有影响力的发展，例如:</p><ul class=""><li id="1a62" class="nm nn iq lz b ma nf md ng mg no mk np mo nq ms nr ns nt nu bi translated">经典条件作用的心理模型(巴尔托&amp;萨顿<a class="ae le" href="#7602" rel="noopener ugc nofollow"> 1981a </a>、<a class="ae le" href="#51ac" rel="noopener ugc nofollow"> 1981b </a>、<a class="ae le" href="#b8eb" rel="noopener ugc nofollow">1982</a>；萨顿&amp;巴尔托<a class="ae le" href="#8c25" rel="noopener ugc nofollow"> 1981a </a>，<a class="ae le" href="#fbb5" rel="noopener ugc nofollow">1981 b</a>)；</li><li id="05b8" class="nm nn iq lz b ma nv md nw mg nx mk ny mo nz ms nr ns nt nu bi translated">经典条件作用的神经元模型(<a class="ae le" href="#3cb4" rel="noopener ugc nofollow">Klopf 1988</a>；<a class="ae le" href="#39d1" rel="noopener ugc nofollow">特索罗1986</a>)；</li><li id="b132" class="nm nn iq lz b ma nv md nw mg nx mk ny mo nz ms nr ns nt nu bi translated">经典条件作用的时差模型(萨顿和巴尔托<a class="ae le" href="#b95a" rel="noopener ugc nofollow"> 1987 </a>，<a class="ae le" href="#e4d6" rel="noopener ugc nofollow">1990</a>)；</li><li id="e621" class="nm nn iq lz b ma nv md nw mg nx mk ny mo nz ms nr ns nt nu bi translated">应用于极点平衡问题的“演员-评论家架构”(<a class="ae le" href="#1330" rel="noopener ugc nofollow">巴尔托等人，1983</a>)；</li><li id="c76c" class="nm nn iq lz b ma nv md nw mg nx mk ny mo nz ms nr ns nt nu bi translated">“演员-评论家架构”的扩展以及与反向传播神经网络技术的集成(<a class="ae le" href="#1c4b" rel="noopener ugc nofollow">Anderson 1986</a>；<a class="ae le" href="#2e0a" rel="noopener ugc nofollow">萨顿1984</a>)；</li><li id="c2b8" class="nm nn iq lz b ma nv md nw mg nx mk ny mo nz ms nr ns nt nu bi translated">将时间差学习从控制决策中分离出来，并引入<code class="fe oa ob oc od b">lambda()</code>属性(<a class="ae le" href="#9cc0" rel="noopener ugc nofollow"> Sutton 1988 </a>)。</li></ul><p id="8208" class="pw-post-body-paragraph lx ly iq lz b ma nf ka mc md ng kd mf mg nh mi mj mk ni mm mn mo nj mq mr ms ij bi translated">当Watkins ( <a class="ae le" href="#8279" rel="noopener ugc nofollow"> 1989 </a>)开发了Q-Learning(<a class="ae le" href="#8279" rel="noopener ugc nofollow">Watkins 1989</a>)时，强化学习起源的三个“线程”最终被联合起来，该Q-Learning由Werbos ( <a class="ae le" href="#1ce7" rel="noopener ugc nofollow"> 1987 </a>)加强，他主张试错学习和动态编程的融合。在这之后，研究人员探索了强化学习的自动化电子实现，并能够取得一些惊人的结果。</p><h1 id="7383" class="lf lg iq bd lh li lj lk ll lm ln lo lp kf lq kg lr ki ls kj lt kl lu km lv lw bi translated">3.现代发展(1992年后)</h1><h2 id="70c7" class="mt lg iq bd lh mu mv dn ll mw mx dp lp mg my mz lr mk na nb lt mo nc nd lv iw bi translated">3.1.棋盘游戏的发展</h2><p id="db6e" class="pw-post-body-paragraph lx ly iq lz b ma mb ka mc md me kd mf mg mh mi mj mk ml mm mn mo mp mq mr ms ij bi translated">1992年，Tesauro能够在他开发的TD-Gammon程序中实现这些概念(<a class="ae le" href="#d705" rel="noopener ugc nofollow"> Tesauro 1994 </a>)，该程序能够在双陆棋游戏中达到“大师级别”。研究已经转向尝试将这一成功应用于国际象棋游戏(Baxter等人<a class="ae le" href="#ea1a" rel="noopener ugc nofollow"> 2000 </a>，<a class="ae le" href="#3a4a" rel="noopener ugc nofollow">2001</a>；<a class="ae le" href="#b96f" rel="noopener ugc nofollow"> Thrun 1995 </a>)。IBM开发DeepBlue的唯一目的就是下棋。然而，它遭受了维数灾难，因为算法花费了太多的时间来计算，它无法在未来'看'得足够远，并且它因非常糟糕的开局移动而臭名昭著(<a class="ae le" href="#c52b" rel="noopener ugc nofollow"> Szita 2012 </a>)。然而，深蓝在1997年战胜了一位世界冠军。<a class="ae le" href="#433d" rel="noopener ugc nofollow">征1997</a>；<a class="ae le" href="#ab52" rel="noopener ugc nofollow">新生儿2000</a>；<a class="ae le" href="#e14f" rel="noopener ugc nofollow">桑托1997 </a>。</p><p id="08e3" class="pw-post-body-paragraph lx ly iq lz b ma nf ka mc md ng kd mf mg nh mi mj mk ni mm mn mo nj mq mr ms ij bi translated">研究人员接着进行了一个更大甚至更复杂的游戏:围棋。虽然做了很多尝试，试图学习如何玩这个游戏(布兹<a class="ae le" href="#5d4f" rel="noopener ugc nofollow"> 2006a </a>，<a class="ae le" href="#4e1a" rel="noopener ugc nofollow">2006 b</a>；<a class="ae le" href="#5061" rel="noopener ugc nofollow"> Bouzy &amp;赫尔姆斯泰特2004</a>；库仑<a class="ae le" href="#a9bd" rel="noopener ugc nofollow"> 2007a </a>、<a class="ae le" href="#e71e" rel="noopener ugc nofollow">2007 b</a>；<a class="ae le" href="#6110" rel="noopener ugc nofollow">达尔1999</a>；<a class="ae le" href="#0eaa" rel="noopener ugc nofollow">盖利&amp;银2008</a>；<a class="ae le" href="#30eb" rel="noopener ugc nofollow">盖利等人2006年</a>；<a class="ae le" href="#398b" rel="noopener ugc nofollow"> Schraudolph等人2001年</a>；<a class="ae le" href="#71b9" rel="noopener ugc nofollow"> Silver et al. 2008 </a>)，没有人能够战胜世界冠军，直到谷歌的AlphaGo在2016年夺冠(<a class="ae le" href="#bff9" rel="noopener ugc nofollow">Borowiec 2016</a>；<a class="ae le" href="#b94f" rel="noopener ugc nofollow">莫耶2016 </a>)。DeepBlue和AlphaGo的区别在于，DeepBlue利用并行化的基于树的搜索方法和定制的硬件进步，有效地使用“蛮力”来计算赢得游戏所需的所有步骤(<a class="ae le" href="#bff9" rel="noopener ugc nofollow">boro wiec 2016</a>；<a class="ae le" href="#ab52" rel="noopener ugc nofollow">新生儿2000</a>)；但是这种方法在围棋中是不可能的，因为有太多的走法和可能的组合，计算开销是不可能的。相比之下，AlphaGo利用了蒙特卡洛模拟、蒙特卡洛树搜索、贝叶斯优化和实际观看世界冠军之前的比赛的组合(<a class="ae le" href="#fe01" rel="noopener ugc nofollow">陈等人2018</a>；<a class="ae le" href="#a0cc" rel="noopener ugc nofollow">福2016</a>；<a class="ae le" href="#66bf" rel="noopener ugc nofollow">惠勒2017 </a>)，以便建立一个足够强大的模型，并且不需要对未来的移动进行蛮力计算。这些发展标志着强化学习领域的重大进步，并使整个世界能够看到这种特殊的机器学习技术的一些可能性。</p><h2 id="c35a" class="mt lg iq bd lh mu mv dn ll mw mx dp lp mg my mz lr mk na nb lt mo nc nd lv iw bi translated">3.2.电脑游戏的发展</h2><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi oe"><img src="../Images/5a94946e9d5e4f634b2088423ebd93ee.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*HeeglLFN53ExCZRi"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">雅达利游戏(<a class="ae le" href="https://scontent-syd2-1.xx.fbcdn.net/v/t1.18169-9/14344753_357669837897331_4452619001162145046_n.jpg?_nc_cat=101&amp;ccb=1-3&amp;_nc_sid=e3f864&amp;_nc_ohc=CLFxqaOcxuQAX-Isp3w&amp;_nc_ht=scontent-syd2-1.xx&amp;oh=b1a89d5cef7db2909bf902c037c01d1c&amp;oe=60CA8BFB" rel="noopener ugc nofollow" target="_blank">图片</a>由脸书<a class="ae le" href="https://www.facebook.com/atari2600system" rel="noopener ugc nofollow" target="_blank">雅达利2600 </a></p></figure><p id="782f" class="pw-post-body-paragraph lx ly iq lz b ma nf ka mc md ng kd mf mg nh mi mj mk ni mm mn mo nj mq mr ms ij bi translated">随着二十世纪八十年代和九十年代儿童的成长，许多人玩雅达利电子游戏。它们于1977年发布，包含526款游戏，包括经典游戏，如Pong、Breakout和Space Invaders。Atari模拟器为强化学习算法提供了一个理想的环境来学习如何玩游戏(<a class="ae le" href="#9d1e" rel="noopener ugc nofollow">Hausknecht et al . 2014</a>；<a class="ae le" href="#c1dc" rel="noopener ugc nofollow">凯泽等人2019</a>；<a class="ae le" href="#f4d8" rel="noopener ugc nofollow"> Mnih等人2013 </a>)由于其基于像素的显示和简单的控制选项。玩了一段时间游戏后，研究人员观察到算法使用了一些令人印象深刻的技术；比如在《突围》中，它能够钻一个洞，以便用更少的努力赢得比赛。这表明了强化学习的力量，特别是它能够学习某些规则和实践，而这些规则和实践不是以编程方式告诉模型的(<a class="ae le" href="#aa00" rel="noopener ugc nofollow"> Berges等人和</a>；<a class="ae le" href="#0db0" rel="noopener ugc nofollow">帕特尔等人2019</a>；<a class="ae le" href="#8340" rel="noopener ugc nofollow">两分钟论文2015 </a>。许多其他计算机游戏也看到了强化学习的发展，包括像Snake，Flappy Bird，Angry Birds和Candy Crush这样的游戏。自Atari游戏以来，现代计算机游戏已经取得了实质性的进步，并且它们提供了更加复杂和动态的环境，这为强化学习的应用提供了大量的学习机会。</p><h1 id="19c2" class="lf lg iq bd lh li lj lk ll lm ln lo lp kf lq kg lr ki ls kj lt kl lu km lv lw bi translated">4.当前的发展</h1><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi of"><img src="../Images/f11495767616033dcece28b13d538854.png" data-original-src="https://miro.medium.com/v2/resize:fit:1142/format:webp/1*J10k53qF9IAMA0jDYs-eXA.png"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">红绿灯自动化(<a class="ae le" href="https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQ-IXKdWEQVI7OS8KykSSsYqhskLg-AqdZ_D7FDOhUKk3WBvrnOcPdhhU66G3ElP1PV_eY&amp;usqp=CAU" rel="noopener ugc nofollow" target="_blank">图片</a>由<a class="ae le" href="https://arxiv.org/pdf/1803.11115.pdf" rel="noopener ugc nofollow" target="_blank">小源等人</a>在<a class="ae le" href="https://arxiv.org/" rel="noopener ugc nofollow" target="_blank"> Arxiv </a>上拍摄)</p></figure><p id="fcfe" class="pw-post-body-paragraph lx ly iq lz b ma nf ka mc md ng kd mf mg nh mi mj mk ni mm mn mo nj mq mr ms ij bi translated">虽然强化学习的历史令人着迷，但现代强化学习的应用确实令人兴奋。在最近的一篇文章中，Garychl ( <a class="ae le" href="#8f43" rel="noopener ugc nofollow"> 2018 </a>)列举了一些强化学习在当今工业中应用的例子；包括:计算机集群中的资源管理、交通灯控制、机器人、网络系统配置、化学、广告和游戏。洛里卡(<a class="ae le" href="#639d" rel="noopener ugc nofollow"> 2017 </a>)将这些机会整合为三个主要的应用主题，这些主题可以应用于任何行业的任何业务:优化(如流程规划、产量管理、供应链)、控制(如自动驾驶汽车、工厂自动化、风力涡轮机控制)以及监控和维护(如质量控制、预测性维护、库存监控)。所有这些应用程序都提供了一个环境、一个动作、一个响应和一种优化方法，因此为强化学习算法应用于这些特定情况提供了机会。</p><p id="248b" class="pw-post-body-paragraph lx ly iq lz b ma nf ka mc md ng kd mf mg nh mi mj mk ni mm mn mo nj mq mr ms ij bi translated">很简单，当企业面临以下任何场景时，它们应该寻求在自己的人工智能战略中实施强化学习(<a class="ae le" href="#aa85" rel="noopener ugc nofollow"> Chahill 2017 </a>):</p><ul class=""><li id="62fc" class="nm nn iq lz b ma nf md ng mg no mk np mo nq ms nr ns nt nu bi translated">他们使用模拟，因为系统或过程太复杂(或太危险)，无法通过试错法来教授机器；或者</li><li id="e916" class="nm nn iq lz b ma nv md nw mg nx mk ny mo nz ms nr ns nt nu bi translated">他们正在处理大的国家空间；或者</li><li id="788a" class="nm nn iq lz b ma nv md nw mg nx mk ny mo nz ms nr ns nt nu bi translated">他们正在寻求通过优化运营效率和提供决策支持来增强人工分析和领域专家的能力。</li></ul><h1 id="e66f" class="lf lg iq bd lh li lj lk ll lm ln lo lp kf lq kg lr ki ls kj lt kl lu km lv lw bi translated">5.未来发展</h1><p id="39dc" class="pw-post-body-paragraph lx ly iq lz b ma mb ka mc md me kd mf mg mh mi mj mk ml mm mn mo mp mq mr ms ij bi translated">强化学习的未来是一个非常主观的讨论，在这个讨论中，许多人可能会有不同的观点。强化学习的未来有两个主要方向，可以概括如下:</p><ol class=""><li id="6115" class="nm nn iq lz b ma nf md ng mg no mk np mo nq ms og ns nt nu bi translated">未来是光明的，强化学习被广泛采用和实施，并继续对人类产生积极影响；或者</li><li id="88b9" class="nm nn iq lz b ma nv md nw mg nx mk ny mo nz ms og ns nt nu bi translated">未来是严峻的，人们反对强化学习在他们生活中的逐步渗透，人工智能的未来发展集中在强化学习以外的技术上。</li></ol><p id="7de8" class="pw-post-body-paragraph lx ly iq lz b ma nf ka mc md ng kd mf mg nh mi mj mk ni mm mn mo nj mq mr ms ij bi translated">强化学习领域所取得的惊人进步及其对我们社会的积极影响表明，这一趋势将会持续下去，并将延续到未来。戈德布(<a class="ae le" href="#2f56" rel="noopener ugc nofollow"> 2018 </a>)讨论了强化学习的光明前景，而基尔希特(<a class="ae le" href="#7f0e" rel="noopener ugc nofollow"> 2019 </a>)讨论了强化学习将继续取得实质性进展，并将使企业的日常生活变得更加轻松和高效。在这两种选择中，这种光明的未来是最有可能的。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi oh"><img src="../Images/37fda215c77a545b562a92ee08b97f6a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fco19DOJvH_bfPVhV385rQ.jpeg"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">自动驾驶汽车(<a class="ae le" href="https://www.shutterstock.com/image-photo/young-woman-reading-book-autonomous-car-671755273" rel="noopener ugc nofollow" target="_blank">图片</a>由<a class="ae le" href="https://www.shutterstock.com/g/chombosan" rel="noopener ugc nofollow" target="_blank"> metamorworks </a>在<a class="ae le" href="https://www.shutterstock.com/" rel="noopener ugc nofollow" target="_blank"> Shutterstock </a>授权给Chrism Mahoney)</p></figure><p id="f4d4" class="pw-post-body-paragraph lx ly iq lz b ma nf ka mc md ng kd mf mg nh mi mj mk ni mm mn mo nj mq mr ms ij bi translated">然而，如果不讨论人工智能兴起背后的一些社会担忧，以及一些好莱坞电影对人工智能产生的一些焦虑，那将是不负责任的。Frye ( <a class="ae le" href="#2a59" rel="noopener ugc nofollow"> 2019 </a>)围绕AI &amp;特别是强化学习讨论了一些令人担忧的问题，提到强化学习是不安全的，因为任务规范(难以精确指定AI智能体应该执行什么任务)，以及不安全的探索(智能体从试错中学习，这意味着它必须先犯错误才能知道不做什么)，这可能导致我们的同胞发生事故和受伤(或更糟)。特别是如果我们考虑自动驾驶汽车的例子。Knight ( <a class="ae le" href="#d483" rel="noopener ugc nofollow"> 2017 </a>)明确探索了这个例子，重点关注Nvidia自动驾驶汽车，它不是通过给它的任何编程命令，而是通过观察其他司机来学习驾驶。虽然这本身是一个令人印象深刻的壮举，但潜在的问题是，创造者不知道计算机如何或为什么做出决定，这无疑会将人们的生命置于危险之中。因此，在强化学习被更广泛的社区广泛接受和采用之前，还需要做一些改进。那些领域是对创作者的可解释性和对用户的责任性。一旦这两个问题得到整改，那么人工智能和强化学习无疑会变得更加值得信赖。</p><h1 id="fd37" class="lf lg iq bd lh li lj lk ll lm ln lo lp kf lq kg lr ki ls kj lt kl lu km lv lw bi translated">6.结论</h1><p id="3da3" class="pw-post-body-paragraph lx ly iq lz b ma mb ka mc md me kd mf mg mh mi mj mk ml mm mn mo mp mq mr ms ij bi translated">强化学习自20世纪50年代出现以来已经走过了漫长的道路；它在发展和成熟的道路上还有很长的路要走。从20世纪90年代的理论和概念进步来看，强化学习已经征服了国际象棋、围棋和无数电子计算机游戏。强化学习也开始在商业和工业中出现，并且在我们现代社会日益增长的挑战中继续被证明是有益和有用的。强化学习的未来将很快以许多许多不同的方式渗透到我们的日常生活中；但在此之前，它的可解释性、可问责性和可信度等一些基本问题都得到了纠正。尽管如此，强化学习的未来似乎是漫长而光明的，我们将继续从这个强大的人工智能领域看到许多伟大的事情。</p><h1 id="4bed" class="lf lg iq bd lh li lj lk ll lm ln lo lp kf lq kg lr ki ls kj lt kl lu km lv lw bi translated">7.参考</h1><p id="1c4b" class="pw-post-body-paragraph lx ly iq lz b ma mb ka mc md me kd mf mg mh mi mj mk ml mm mn mo mp mq mr ms ij bi translated">安德森，C. 1986，<em class="nl">用多层联结主义系统学习和解决问题(适应性，策略学习，神经网络，强化学习)</em>，<em class="nl"> </em>马萨诸塞大学阿姆赫斯特分校博士论文。</p><p id="86da" class="pw-post-body-paragraph lx ly iq lz b ma nf ka mc md ng kd mf mg nh mi mj mk ni mm mn mo nj mq mr ms ij bi translated">Andreae，J. 1963，“Stella:一个学习机器的方案”，<em class="nl">国际会计师联合会会议录卷</em>，第1卷，第2号，<strong class="lz ja"> </strong>第497–502页，ISSN:1474–6670，DOI:10.1016/s 1474–6670(17)69682–4。</p><p id="6cee" class="pw-post-body-paragraph lx ly iq lz b ma nf ka mc md ng kd mf mg nh mi mj mk ni mm mn mo nj mq mr ms ij bi translated">安德烈，J. 1977，<em class="nl">用可教的机器思考</em>，ISBN:学术出版社，伦敦。</p><p id="7fac" class="pw-post-body-paragraph lx ly iq lz b ma nf ka mc md ng kd mf mg nh mi mj mk ni mm mn mo nj mq mr ms ij bi translated">Andreae，J. &amp; Cashin，P. 1969，“具有独白的学习机器”，<em class="nl">国际人机研究杂志</em>，第1卷，第1期，<strong class="lz ja"> </strong>第1–20页，ISSN:0020–7373，DOI:10.1016/s 0020–7373(69)80008–8。</p><p id="7602" class="pw-post-body-paragraph lx ly iq lz b ma nf ka mc md ng kd mf mg nh mi mj mk ni mm mn mo nj mq mr ms ij bi translated">巴尔托，A. &amp;萨顿，R. 1981a，<em class="nl">适应性智能的目标寻求成分:初步评估</em>。</p><p id="51ac" class="pw-post-body-paragraph lx ly iq lz b ma nf ka mc md ng kd mf mg nh mi mj mk ni mm mn mo nj mq mr ms ij bi translated">巴尔托，A. &amp;萨顿，R. 1981b，“地标学习:联想搜索的一个例证”，<em class="nl">生物控制论</em>，第42卷，第1期，<strong class="lz ja"> </strong>第1-8页，ISSN:0340-1200，DOI: 10.1007/BF00335152。</p><p id="b8eb" class="pw-post-body-paragraph lx ly iq lz b ma nf ka mc md ng kd mf mg nh mi mj mk ni mm mn mo nj mq mr ms ij bi translated">巴尔托，A. &amp;萨顿，R. 1982，“通过神经元样适应性元素模拟经典条件作用中的预期反应”，<em class="nl">行为大脑研究</em>，第4卷，第3期，<strong class="lz ja"> </strong>第221–35页，ISSN:0166–4328，DOI:10.1016/0166–4328(82)90001–8。</p><p id="1330" class="pw-post-body-paragraph lx ly iq lz b ma nf ka mc md ng kd mf mg nh mi mj mk ni mm mn mo nj mq mr ms ij bi translated">巴尔托，a .，萨顿，R. &amp;安德森，C. 1983，“能解决困难的学习控制问题的神经元样自适应元件”，<em class="nl"> IEEE系统、人与控制论汇刊</em>，第SMC-13卷，第5期，<strong class="lz ja"> </strong>第834–46页，ISSN:0018–9472，DOI:10.11109/台积电。19804.888686666007</p><p id="ea1a" class="pw-post-body-paragraph lx ly iq lz b ma nf ka mc md ng kd mf mg nh mi mj mk ni mm mn mo nj mq mr ms ij bi translated">Baxter，j .，Tridgell，A. &amp; Weaver，L. 2000，“利用时间差学习下棋”，<em class="nl">机器学习</em>，第40卷第3期，<strong class="lz ja"> </strong>第243页，ISSN:0885–6125，DOI: 10.1023/A:1007634325138。</p><p id="3a4a" class="pw-post-body-paragraph lx ly iq lz b ma nf ka mc md ng kd mf mg nh mi mj mk ni mm mn mo nj mq mr ms ij bi translated">Baxter，j .，Tridgell，A. &amp; Weaver，L. 2001，“强化学习与国际象棋”，<em class="nl">学习玩游戏的机器</em>，第91–116页。</p><p id="b611" class="pw-post-body-paragraph lx ly iq lz b ma nf ka mc md ng kd mf mg nh mi mj mk ni mm mn mo nj mq mr ms ij bi translated">贝尔曼，R. 1957a，<em class="nl">动态规划</em>，ISBN: 069107951x，普林斯顿大学出版社。</p><p id="83da" class="pw-post-body-paragraph lx ly iq lz b ma nf ka mc md ng kd mf mg nh mi mj mk ni mm mn mo nj mq mr ms ij bi translated">Bellman，R. 1957b，“一个马尔可夫决策过程”，<em class="nl">数学与力学杂志</em>，第6卷第5期，<strong class="lz ja"> </strong>第679–84页，ISSN: 00959057。</p><p id="aa00" class="pw-post-body-paragraph lx ly iq lz b ma nf ka mc md ng kd mf mg nh mi mj mk ni mm mn mo nj mq mr ms ij bi translated">伯杰斯，v .、饶，P. &amp; Pryzant，R. nd，“雅达利突围的强化学习”，<em class="nl">斯坦福大学</em>，&lt;<a class="ae le" href="https://cs.stanford.edu/~rpryzant/data/rl/paper.pdf" rel="noopener ugc nofollow" target="_blank">https://cs.stanford.edu/~rpryzant/data/rl/paper.pdf</a>&gt;。</p><p id="bff9" class="pw-post-body-paragraph lx ly iq lz b ma nf ka mc md ng kd mf mg nh mi mj mk ni mm mn mo nj mq mr ms ij bi translated">Borowiec，S. 2016，<em class="nl">alpha Go Seals 4-1战胜围棋特级大师Lee Sedol </em>《卫报》查看2020年5月31日，&lt;<a class="ae le" href="https://www.fbe.hku.hk/f/page/75261/Reading%201_AlphaGo%20seals%204-1%20Victory%20over%20Go%20Grandmaster%20Lee%20Sedol.pdf" rel="noopener ugc nofollow" target="_blank">https://www . FBE . hku . hk/f/page/75261/Reading % 201 _ alpha Go % 20 Seals % 204-1% 20 Victory % 20 over % 20Go % 20g rand master % 20 Lee % 20 Sedol . pdf【T12</a></p><p id="5d4f" class="pw-post-body-paragraph lx ly iq lz b ma nf ka mc md ng kd mf mg nh mi mj mk ni mm mn mo nj mq mr ms ij bi translated">Bouzy，B. 2006a，“将浅的和选择性的全局树搜索与蒙特卡罗用于围棋相关联”，<em class="nl">计算机和游戏</em>，第3846卷，第67-80页，DOI: 10.1007/11674399_5，施普林格柏林海德堡，柏林，海德堡。</p><p id="4e1a" class="pw-post-body-paragraph lx ly iq lz b ma nf ka mc md ng kd mf mg nh mi mj mk ni mm mn mo nj mq mr ms ij bi translated">Bouzy，B. 2006b，“蒙特卡罗围棋的移动修剪技术”，<em class="nl">计算机游戏进展</em>，第4250卷，第104-19页，DOI: 10.1007/11922155_8。</p><p id="5061" class="pw-post-body-paragraph lx ly iq lz b ma nf ka mc md ng kd mf mg nh mi mj mk ni mm mn mo nj mq mr ms ij bi translated">Bouzy，b .和Helmstetter，B. 2004，“蒙特卡罗围棋发展”，<em class="nl">计算机游戏的进步</em>，第159–74页，施普林格。</p><p id="0bb1" class="pw-post-body-paragraph lx ly iq lz b ma nf ka mc md ng kd mf mg nh mi mj mk ni mm mn mo nj mq mr ms ij bi translated">布赖森，A. 1996，“最优控制”，<em class="nl"> IEEE控制系统</em>，第16卷第3期，<strong class="lz ja"> </strong>第26–33页，ISSN:1066–033 x，DOI: 10.1109/37.506395。</p><p id="aa85" class="pw-post-body-paragraph lx ly iq lz b ma nf ka mc md ng kd mf mg nh mi mj mk ni mm mn mo nj mq mr ms ij bi translated">Chahill，D. 2017，<em class="nl">为什么强化学习可能是复杂工业系统的最佳Ai技术</em>，查看2020年5月31日，&lt;<a class="ae le" href="https://www.bons.ai/blog/ai-reinforcement-learning-strategy-industrial-systems" rel="noopener ugc nofollow" target="_blank">https://www . bons . Ai/blog/Ai-Reinforcement-Learning-strategy-Industrial-Systems</a>&gt;。</p><p id="fe01" class="pw-post-body-paragraph lx ly iq lz b ma nf ka mc md ng kd mf mg nh mi mj mk ni mm mn mo nj mq mr ms ij bi translated">Chen，y .，Huang，a .，Wang，z .，Antonoglou，I .，Schrittwieser，J. &amp; Silver，D. 2018，《Alphago中的贝叶斯优化》，<em class="nl">，</em>，ISSN:2331–8422，&lt; <a class="ae le" href="https://arxiv.org/pdf/1812.06855.pdf" rel="noopener ugc nofollow" target="_blank">，</a> &gt;。</p><p id="aa51" class="pw-post-body-paragraph lx ly iq lz b ma nf ka mc md ng kd mf mg nh mi mj mk ni mm mn mo nj mq mr ms ij bi translated">Clark，W. &amp; Farley，B. 1955，“自组织系统中模式识别的一般化”，<em class="nl">1955年3月1-3日西方联合计算机会议论文集</em>，第86-91页，DOI: 10.1145/1455292.1455309，&lt;【https://dl.acm.org/doi/abs/10.1145/1455292.1455309】&gt;。</p><p id="a9bd" class="pw-post-body-paragraph lx ly iq lz b ma nf ka mc md ng kd mf mg nh mi mj mk ni mm mn mo nj mq mr ms ij bi translated">库隆，R. 2007a，“计算围棋游戏中走法的Elo等级”，【https://hal.inria.fr/inria-00149859/document】电脑游戏工作坊、&lt;、<a class="ae le" href="https://hal.inria.fr/inria-00149859/document" rel="noopener ugc nofollow" target="_blank">、</a>、&gt;。</p><p id="e71e" class="pw-post-body-paragraph lx ly iq lz b ma nf ka mc md ng kd mf mg nh mi mj mk ni mm mn mo nj mq mr ms ij bi translated">Coulom，R. 2007b，“蒙特卡罗树搜索中的有效选择性和备份算子”，载于H. Herik，P. Ciancarini和H. Donkers。(编辑。)，<em class="nl">电脑与游戏</em>，第4630卷，第72–83页，施普林格。</p><p id="6110" class="pw-post-body-paragraph lx ly iq lz b ma nf ka mc md ng kd mf mg nh mi mj mk ni mm mn mo nj mq mr ms ij bi translated">达尔，F. 1999，“Honte，一个使用神经网络的围棋程序”，<em class="nl">学习玩游戏的机器</em>，第205–23页，&lt;<a class="ae le" href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.50.2676&amp;rep=rep1&amp;type=pdf" rel="noopener ugc nofollow" target="_blank">http://citeseerx.ist.psu.edu/viewdoc/download?doi = 10 . 1 . 1 . 50 . 2676&amp;rep = re P1&amp;type = pdf</a>&gt;。</p><p id="5036" class="pw-post-body-paragraph lx ly iq lz b ma nf ka mc md ng kd mf mg nh mi mj mk ni mm mn mo nj mq mr ms ij bi translated">法利，B. &amp;克拉克，W. 1954，“用数字计算机模拟自组织系统”，<em class="nl">信息理论IRE专业组汇刊</em>，第4卷，第4期，<strong class="lz ja"> </strong>第76–84页，ISSN:2168–2690，DOI:10.11109/tit . 1991996</p><p id="2a59" class="pw-post-body-paragraph lx ly iq lz b ma nf ka mc md ng kd mf mg nh mi mj mk ni mm mn mo nj mq mr ms ij bi translated">Frye，C. 2019，<em class="nl">现实世界中强化学习的危险</em>，查看2020年5月31日，&lt;<a class="ae le" href="https://faculty.ai/blog/the-dangers-of-reinforcement-learning-in-the-real-world/" rel="noopener ugc nofollow" target="_blank">https://faculty . ai/blog/The-Dangers-of-Reinforcement-Learning-in-The-Real-World/</a>&gt;。</p><p id="a0cc" class="pw-post-body-paragraph lx ly iq lz b ma nf ka mc md ng kd mf mg nh mi mj mk ni mm mn mo nj mq mr ms ij bi translated">傅，M. 2016，“Alphago与蒙特卡罗树搜索:仿真优化视角”，载于<em class="nl"> 2016冬季仿真会议</em>，IEEE，第659–70页，&lt;<a class="ae le" href="https://doi-org.ezproxy.lib.uts.edu.au/10.1109/WSC.2016.7822130" rel="noopener ugc nofollow" target="_blank">https://doi-org . ez proxy . lib . uts . edu . au/10.1109/WSC . 2016.7822130</a>&gt;。</p><p id="8f43" class="pw-post-body-paragraph lx ly iq lz b ma nf ka mc md ng kd mf mg nh mi mj mk ni mm mn mo nj mq mr ms ij bi translated">Garychl 2018，<em class="nl">强化学习在现实世界中的应用</em>，查看2020年5月31日，&lt;<a class="ae le" rel="noopener" target="_blank" href="/applications-of-reinforcement-learning-in-real-world-1a94955bcd12">https://towardsdatascience . com/Applications-of-Reinforcement-Learning-in-Real-World-1a 94955 BCD 12</a>&gt;。</p><p id="0eaa" class="pw-post-body-paragraph lx ly iq lz b ma nf ka mc md ng kd mf mg nh mi mj mk ni mm mn mo nj mq mr ms ij bi translated">Gelly，s .和Silver，D. 2008，“在9 X 9计算机围棋中达到大师水平”，载于https://www.aaai.org/Papers/AAAI/2008/AAAI08-257.pdf&gt;<em class="nl">AAAI</em>第8卷第1537-40页。</p><p id="30eb" class="pw-post-body-paragraph lx ly iq lz b ma nf ka mc md ng kd mf mg nh mi mj mk ni mm mn mo nj mq mr ms ij bi translated">Gelly，s .、Wang，y .、Munos，R. &amp; Teytaud，O. 2006年，<em class="nl">用蒙特卡罗围棋</em>、&lt;、、&gt;中的模式修改Uct。</p><p id="2f56" class="pw-post-body-paragraph lx ly iq lz b ma nf ka mc md ng kd mf mg nh mi mj mk ni mm mn mo nj mq mr ms ij bi translated">戈德布，C. 2018，<em class="nl">强化学习的光明前景</em>，查看31/5/2020，&lt;<a class="ae le" href="https://medium.com/apteo/the-bright-future-of-reinforcement-learning-a66173694f88" rel="noopener">https://medium . com/apteo/The-Bright-Future-of-Reinforcement-Learning-a 66173694 f88</a>&gt;。</p><p id="5617" class="pw-post-body-paragraph lx ly iq lz b ma nf ka mc md ng kd mf mg nh mi mj mk ni mm mn mo nj mq mr ms ij bi translated">汉密尔顿，W. 1833，<em class="nl">关于通过特征函数</em>的系数表达光的路径的一般方法，&amp;行星，ISBN:p . d .哈代印刷。</p><p id="4be1" class="pw-post-body-paragraph lx ly iq lz b ma nf ka mc md ng kd mf mg nh mi mj mk ni mm mn mo nj mq mr ms ij bi translated">汉密尔顿，W. 1834，<em class="nl">关于以前应用于光学的一般数学方法在动力学中的应用</em>，ISBN:p . d .哈代印刷。</p><p id="10d5" class="pw-post-body-paragraph lx ly iq lz b ma nf ka mc md ng kd mf mg nh mi mj mk ni mm mn mo nj mq mr ms ij bi translated">哈默斯利，J. 1964，<em class="nl">蒙特卡罗方法</em>，ISBN: Methuen，伦敦。</p><p id="9d1e" class="pw-post-body-paragraph lx ly iq lz b ma nf ka mc md ng kd mf mg nh mi mj mk ni mm mn mo nj mq mr ms ij bi translated">Hausknecht，m .、Lehman，j .、Miikkulainen，R. &amp; Stone，P. 2014，“一种用于一般Atari游戏的神经进化方法”，<em class="nl"> IEEE计算智能和游戏中的人工智能汇刊</em>，第6卷，第4期，<strong class="lz ja"> </strong>第355–66页，ISSN:1943–068 x，DOI:10.11109/TCI AIG . 2013 . 20133106</p><p id="1d73" class="pw-post-body-paragraph lx ly iq lz b ma nf ka mc md ng kd mf mg nh mi mj mk ni mm mn mo nj mq mr ms ij bi translated">霍华德，R. 1960，“动态规划和马尔可夫过程”。</p><p id="b90b" class="pw-post-body-paragraph lx ly iq lz b ma nf ka mc md ng kd mf mg nh mi mj mk ni mm mn mo nj mq mr ms ij bi translated">Jacobi，K. 1866年，<em class="nl">在克莱布什的《科学知识分子学术讨论会》之前。</em></p><p id="c1dc" class="pw-post-body-paragraph lx ly iq lz b ma nf ka mc md ng kd mf mg nh mi mj mk ni mm mn mo nj mq mr ms ij bi translated">Kaiser，l .，Babaeizadeh，m .，Milos，p .，Osinski，b .，Campbell，r .，Czechowski，k .，Erhan，d .，Finn，c .，Kozakowski，p .，Levine，s .，Mohiuddin，a .，Sepassi，r .，Tucker，G. &amp; Michalewski，H. 2019，“基于模型的强化学习用于Atari”，载于arXiv.org<em class="nl">ICLR</em>，<a class="ae le" href="https://arxiv.org/pdf/1903.00374.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1903.00374.pdf</a>&gt;。</p><p id="e711" class="pw-post-body-paragraph lx ly iq lz b ma nf ka mc md ng kd mf mg nh mi mj mk ni mm mn mo nj mq mr ms ij bi translated">金，D. 1997，《卡斯帕罗夫对深蓝:终极人对机器挑战》，<em class="nl">机器挑战，特拉法尔加广场</em>。</p><p id="7f0e" class="pw-post-body-paragraph lx ly iq lz b ma nf ka mc md ng kd mf mg nh mi mj mk ni mm mn mo nj mq mr ms ij bi translated">Kirschte，M. 2019，<em class="nl">对强化学习有什么期待？</em>，2020年5月31日查看，&lt;<a class="ae le" rel="noopener" target="_blank" href="/what-to-expect-from-reinforcement-learning-a22e8c16f40c">https://towards data science . com/what-to-expect-from-reinforcement-learning-a 22 e 8 c 16 f 40 c</a>&gt;。</p><p id="0367" class="pw-post-body-paragraph lx ly iq lz b ma nf ka mc md ng kd mf mg nh mi mj mk ni mm mn mo nj mq mr ms ij bi translated">Klopf，A. 1972，<em class="nl">大脑功能和适应系统:一种异质理论</em>。</p><p id="ede0" class="pw-post-body-paragraph lx ly iq lz b ma nf ka mc md ng kd mf mg nh mi mj mk ni mm mn mo nj mq mr ms ij bi translated">Klopf，A. 1975，“自然智能与人工智能的比较”，<em class="nl"> ACM SIGART Bulletin </em>，第52期，<strong class="lz ja"> </strong>第11–3页，ISSN:0163–5719，DOI:10.1145/1045236.104236 . 1045237。</p><p id="3cb4" class="pw-post-body-paragraph lx ly iq lz b ma nf ka mc md ng kd mf mg nh mi mj mk ni mm mn mo nj mq mr ms ij bi translated">Klopf，A. 1988，“经典条件作用的神经元模型”，<em class="nl">心理生物学</em>，第16卷第2期，<strong class="lz ja"> </strong>第85–125页，ISSN:0889–6313，DOI:10.3758/BF 0333113。</p><p id="d483" class="pw-post-body-paragraph lx ly iq lz b ma nf ka mc md ng kd mf mg nh mi mj mk ni mm mn mo nj mq mr ms ij bi translated">骑士，W. 2017，<em class="nl">Ai核心的黑暗秘密</em>，2020年5月31日查看，&lt;<a class="ae le" href="https://www.technologyreview.com/2017/04/11/5113/the-dark-secret-at-the-heart-of-ai/" rel="noopener ugc nofollow" target="_blank">https://www . technology review . com/2017/04/11/5113/The-Dark-Secret-at-The-Heart-of-Ai/</a>&gt;。</p><p id="433d" class="pw-post-body-paragraph lx ly iq lz b ma nf ka mc md ng kd mf mg nh mi mj mk ni mm mn mo nj mq mr ms ij bi translated">Levy，S. 1997，“人与机器”，<em class="nl">新闻周刊</em>，第129卷第18期，<strong class="lz ja"> </strong>第50–6页，ISSN: 00289604。</p><p id="639d" class="pw-post-body-paragraph lx ly iq lz b ma nf ka mc md ng kd mf mg nh mi mj mk ni mm mn mo nj mq mr ms ij bi translated">Lorica，B. 2017，<em class="nl">强化学习在工业中的实际应用:强化学习的商业和工业应用概述</em>，查看2020年5月31日，&lt;<a class="ae le" href="https://www.oreilly.com/radar/practical-applications-of-reinforcement-learning-in-industry/" rel="noopener ugc nofollow" target="_blank">https://www . oreilly . com/radar/Practical-Applications-of-Reinforcement-Learning-in-Industry/</a>&gt;。</p><p id="f99b" class="pw-post-body-paragraph lx ly iq lz b ma nf ka mc md ng kd mf mg nh mi mj mk ni mm mn mo nj mq mr ms ij bi translated">明斯基，M. 1954，<em class="nl">神经模拟强化系统理论及其对脑模型问题的应用</em>，<em class="nl"> </em>普林斯顿大学博士论文。</p><p id="acc3" class="pw-post-body-paragraph lx ly iq lz b ma nf ka mc md ng kd mf mg nh mi mj mk ni mm mn mo nj mq mr ms ij bi translated">明斯基，M. 1961，“迈向人工智能的步骤”，<em class="nl">IRE会议录</em>，第49卷，第1期，<strong class="lz ja"> </strong>第8-30页，ISSN:0096-8390，DOI:10.1109/Jr proc . 1961 . 2877771 . 1961 . 287771</p><p id="f4d8" class="pw-post-body-paragraph lx ly iq lz b ma nf ka mc md ng kd mf mg nh mi mj mk ni mm mn mo nj mq mr ms ij bi translated">Mnih，v .、Kavukcuoglu，k .、Silver，d .、Graves，a .、Antonoglou，I .、Wierstra，d .、Riedmiller，M. 2013，《用深度强化学习玩雅达利》，<em class="nl">ArXiv.org</em>、&lt;<a class="ae le" href="https://arxiv.org/pdf/1312.5602.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1312.5602.pdf</a>&gt;。</p><p id="b94f" class="pw-post-body-paragraph lx ly iq lz b ma nf ka mc md ng kd mf mg nh mi mj mk ni mm mn mo nj mq mr ms ij bi translated">Moyer，C. 2016，<em class="nl">谷歌的Alphago如何击败一个围棋世界冠军</em>，《大西洋》，查看31/May/2020，&lt;<a class="ae le" href="https://www.theatlantic.com/technology/archive/2016/03/the-invisible-opponent/475611/" rel="noopener ugc nofollow" target="_blank">https://www . thealantic . com/technology/archive/2016/03/The-invisible-opposite/475611/</a>&gt;。</p><p id="ab52" class="pw-post-body-paragraph lx ly iq lz b ma nf ka mc md ng kd mf mg nh mi mj mk ni mm mn mo nj mq mr ms ij bi translated">新生儿，M. 2000，《深蓝对Ai的贡献》，<em class="nl">数学与人工智能年鉴</em>，第28卷第1期，<strong class="lz ja"> </strong>第27–30页，ISSN:1012–2443，DOI: 10.1023/A:1018939819265。</p><p id="0db0" class="pw-post-body-paragraph lx ly iq lz b ma nf ka mc md ng kd mf mg nh mi mj mk ni mm mn mo nj mq mr ms ij bi translated">Patel，d .、Hazan，h .、Saunders，d .、Siegelmann，H. &amp; Kozma，R. 2019，“应用于Atari Breakout Game的转换到脉冲神经元网络平台后增强学习策略的改进鲁棒性”，<em class="nl">神经网络</em>，第120卷，第108–15页，ISSN:0893–6080。</p><p id="b35d" class="pw-post-body-paragraph lx ly iq lz b ma nf ka mc md ng kd mf mg nh mi mj mk ni mm mn mo nj mq mr ms ij bi translated">罗森布拉特，F. 1962，<em class="nl">神经动力学原理:感知机和大脑机制理论</em>，ISBN:斯巴达图书公司，华盛顿特区</p><p id="161b" class="pw-post-body-paragraph lx ly iq lz b ma nf ka mc md ng kd mf mg nh mi mj mk ni mm mn mo nj mq mr ms ij bi translated">Samuel a . 1959，“使用跳棋游戏进行机器学习的一些研究”，<em class="nl"> IBM研发杂志</em>，第3卷第3期，<strong class="lz ja"> </strong>第210–29页，ISSN:0018–8646，DOI: 10.1147/rd.33.0210</p><p id="e14f" class="pw-post-body-paragraph lx ly iq lz b ma nf ka mc md ng kd mf mg nh mi mj mk ni mm mn mo nj mq mr ms ij bi translated">Santo，B. 1997，“IBM为人机复赛调整深蓝”，<em class="nl">电子工程时报</em>，第946期，<strong class="lz ja"> </strong>第111–2页，ISSN:0192–1541。</p><p id="398b" class="pw-post-body-paragraph lx ly iq lz b ma nf ka mc md ng kd mf mg nh mi mj mk ni mm mn mo nj mq mr ms ij bi translated">Schraudolph，n .，Dayan，P. &amp; Sejnowski，T. 2001，“通过时间差分方法学习评估围棋位置”，<em class="nl">博弈中的计算智能</em>，第77–98页，Springer，<a class="ae le" href="https://snl.salk.edu/~schraudo/pubs/SchDaySej01.pdf" rel="noopener ugc nofollow" target="_blank">https://snl.salk.edu/~schraudo/pubs/SchDaySej01.pdf</a>&gt;。</p><p id="71b9" class="pw-post-body-paragraph lx ly iq lz b ma nf ka mc md ng kd mf mg nh mi mj mk ni mm mn mo nj mq mr ms ij bi translated">Silver，d .、Sutton，r .和m .üller，2008，“基于样本的学习和带有永久和短暂记忆的搜索”，载于<em class="nl">国际机器学习会议</em>，第968–75页。</p><p id="0197" class="pw-post-body-paragraph lx ly iq lz b ma nf ka mc md ng kd mf mg nh mi mj mk ni mm mn mo nj mq mr ms ij bi translated">萨顿，R. 1978a，<em class="nl">学习理论对大脑单通道理论的支持</em>，ISBN。</p><p id="af3e" class="pw-post-body-paragraph lx ly iq lz b ma nf ka mc md ng kd mf mg nh mi mj mk ni mm mn mo nj mq mr ms ij bi translated">萨顿，R. 1978b，“单通道理论:学习的神经元理论”，<em class="nl">大脑理论通讯</em>，第3卷，第3期，<strong class="lz ja"> </strong>第72–4页。</p><p id="560b" class="pw-post-body-paragraph lx ly iq lz b ma nf ka mc md ng kd mf mg nh mi mj mk ni mm mn mo nj mq mr ms ij bi translated">萨顿，R. 1978c，<em class="nl">经典和工具条件作用中的期望的统一理论</em>，ISBN。</p><p id="2e0a" class="pw-post-body-paragraph lx ly iq lz b ma nf ka mc md ng kd mf mg nh mi mj mk ni mm mn mo nj mq mr ms ij bi translated">萨顿，R. 1984，<em class="nl">强化学习中的时间学分分配</em>，<em class="nl"> </em>博士论文，ProQuest学位论文出版。</p><p id="9cc0" class="pw-post-body-paragraph lx ly iq lz b ma nf ka mc md ng kd mf mg nh mi mj mk ni mm mn mo nj mq mr ms ij bi translated">Sutton，R. 1988，“通过时间差异的方法学习预测”，<em class="nl">机器学习</em>，第3卷第1期，<strong class="lz ja"> </strong>第9–44页，ISSN:0885–6125，DOI: 10.1023/A:1022633531479。</p><p id="8c25" class="pw-post-body-paragraph lx ly iq lz b ma nf ka mc md ng kd mf mg nh mi mj mk ni mm mn mo nj mq mr ms ij bi translated">萨顿，R. &amp;巴尔托，A. 1981a，“一个构建和使用其世界内部模型的适应性网络”，<em class="nl">认知和大脑理论</em>，第4卷，第3期，<strong class="lz ja"> </strong>第217–46页。</p><p id="fbb5" class="pw-post-body-paragraph lx ly iq lz b ma nf ka mc md ng kd mf mg nh mi mj mk ni mm mn mo nj mq mr ms ij bi translated">萨顿，r .和巴尔托，A. 1981b年b，“走向适应网络的现代理论:期望和预测”，<em class="nl">心理评论</em>，第88卷，第2期，<strong class="lz ja"> </strong>第135-70页，ISSN:0033-295 x，DOI:10.1037/0033-295 x . 88 . 2 . 135</p><p id="b95a" class="pw-post-body-paragraph lx ly iq lz b ma nf ka mc md ng kd mf mg nh mi mj mk ni mm mn mo nj mq mr ms ij bi translated">萨顿，r .和巴尔托，A. 1987，“经典条件作用的时间差模型”，载于<em class="nl">认知科学学会第九届年会会议录</em>，华盛顿州西雅图，第355–78页。</p><p id="e4d6" class="pw-post-body-paragraph lx ly iq lz b ma nf ka mc md ng kd mf mg nh mi mj mk ni mm mn mo nj mq mr ms ij bi translated">萨顿，r .和巴尔托，A. 1990，“巴甫洛夫强化的时间导数模型”，m .加布里埃尔和j .摩尔(编辑)，<em class="nl">学习和计算神经科学:适应性网络的基础</em>，第497–537页，麻省剑桥麻省理工学院出版社。</p><p id="2338" class="pw-post-body-paragraph lx ly iq lz b ma nf ka mc md ng kd mf mg nh mi mj mk ni mm mn mo nj mq mr ms ij bi translated">萨顿，r .和巴尔托，A. 2018，<em class="nl">强化学习:导论</em>，第二版。，ISBN: 9780262039246，麻省理工学院出版社，马萨诸塞州剑桥，&lt;<a class="ae le" href="https://web.stanford.edu/class/psych209/Readings/SuttonBartoIPRLBook2ndEd.pdf" rel="noopener ugc nofollow" target="_blank">https://web . Stanford . edu/class/psych 209/Readings/suttonbartoiprlbook 2 nded . pdf</a>&gt;。</p><p id="c52b" class="pw-post-body-paragraph lx ly iq lz b ma nf ka mc md ng kd mf mg nh mi mj mk ni mm mn mo nj mq mr ms ij bi translated">Szita，I. 2012，“游戏中的强化学习”，载于M. Wiering和M. van Otterlo(编辑。)，<em class="nl">强化学习:最先进的</em>，第539–77页，DOI:10.1007/978–3–642–27645–3 _ 17，施普林格柏林海德堡，柏林，海德堡，&lt;<a class="ae le" href="https://doi.org/10.1007/978-3-642-27645-3_17" rel="noopener ugc nofollow" target="_blank">https://doi.org/10.1007/978-3-642-27645-3_17</a>&gt;。</p><p id="39d1" class="pw-post-body-paragraph lx ly iq lz b ma nf ka mc md ng kd mf mg nh mi mj mk ni mm mn mo nj mq mr ms ij bi translated">Tesauro，G. 1986，“经典条件作用的简单神经模型”，<em class="nl">生物控制论</em>，第55卷，第2–3期，<strong class="lz ja"> </strong>第187–200页，ISSN:0340–1200，DOI: 10.1007/BF00341933。</p><p id="d705" class="pw-post-body-paragraph lx ly iq lz b ma nf ka mc md ng kd mf mg nh mi mj mk ni mm mn mo nj mq mr ms ij bi translated">Tesauro，G. 1994，“Td-Gammon，一个自学的双陆棋程序，实现大师级的游戏”，<em class="nl">神经计算</em>，第6卷，第2期，<strong class="lz ja"> </strong>第215–9页，ISSN:0899–7667。</p><p id="b219" class="pw-post-body-paragraph lx ly iq lz b ma nf ka mc md ng kd mf mg nh mi mj mk ni mm mn mo nj mq mr ms ij bi translated">桑代克，E. 1911，<em class="nl">动物智力:实验研究</em>，ISBN: 9780765804822，麦克米伦公司。</p><p id="b96f" class="pw-post-body-paragraph lx ly iq lz b ma nf ka mc md ng kd mf mg nh mi mj mk ni mm mn mo nj mq mr ms ij bi translated">Thrun，S. 1995，“学习下棋”，载于<em class="nl">神经信息处理系统进展</em>，第1069–76页，&lt;<a class="ae le" href="http://papers.neurips.cc/paper/1007-learning-to-play-the-game-of-chess.pdf" rel="noopener ugc nofollow" target="_blank">http://papers . neur IPS . cc/paper/1007-学习下棋. pdf </a> &gt;。</p><p id="8340" class="pw-post-body-paragraph lx ly iq lz b ma nf ka mc md ng kd mf mg nh mi mj mk ni mm mn mo nj mq mr ms ij bi translated">TwoMinutePapers 2015，<em class="nl"> Google Deepmind的深度Q-Learning玩雅达利突围</em>，2020年5月31日查看，&lt;<a class="ae le" href="https://www.youtube.com/watch?v=V1eYniJ0Rnk" rel="noopener ugc nofollow" target="_blank">https://www.youtube.com/watch?v=V1eYniJ0Rnk</a>&gt;。</p><p id="8279" class="pw-post-body-paragraph lx ly iq lz b ma nf ka mc md ng kd mf mg nh mi mj mk ni mm mn mo nj mq mr ms ij bi translated">沃特金斯，C. 1989，<em class="nl">从延迟奖励中学习</em>，<em class="nl"> </em>博士论文，<a class="ae le" href="https://www.researchgate.net/publication/33784417_Learning_From_Delayed_Rewards" rel="noopener ugc nofollow" target="_blank">https://www . research gate . net/publication/33784417 _ Learning _ From _ Delayed _ Rewards</a>&gt;。</p><p id="1ce7" class="pw-post-body-paragraph lx ly iq lz b ma nf ka mc md ng kd mf mg nh mi mj mk ni mm mn mo nj mq mr ms ij bi translated">Werbos，P. 1987，“构建和理解适应性系统:工厂自动化和大脑研究的统计/数值方法”，<em class="nl"> IEEE系统、人和控制论汇刊</em>，第17卷第1期，<strong class="lz ja"> </strong>第7-20页，ISSN:0018-9472，DOI: 10.1109/TSMC.1987.289329，&lt; <a class="ae le" href="https://www.aaai.org/Papers/Symposia/Fall/1993/FS-93-02/FS93-02-003.pdf" rel="noopener ugc nofollow" target="_blank"> https://www.aaai</a></p><p id="66bf" class="pw-post-body-paragraph lx ly iq lz b ma nf ka mc md ng kd mf mg nh mi mj mk ni mm mn mo nj mq mr ms ij bi translated">惠勒，T. 2017，<em class="nl">alpha go Zero——它是如何和为什么工作的</em>，2020年5月31日观看，&lt;http://tim.hibal.org/blog/alpha-zero-how-and-why-it-works/&gt;T2。</p><p id="0b4f" class="pw-post-body-paragraph lx ly iq lz b ma nf ka mc md ng kd mf mg nh mi mj mk ni mm mn mo nj mq mr ms ij bi translated">Widrow，b .和Hoff，M. 1960，“自适应开关电路”，<em class="nl"> 1960年Wescon会议记录第四部分</em>，第94–104页，麻省理工学院出版社，马萨诸塞州剑桥。</p><p id="5cee" class="pw-post-body-paragraph lx ly iq lz b ma nf ka mc md ng kd mf mg nh mi mj mk ni mm mn mo nj mq mr ms ij bi translated">维基百科2020，<em class="nl">雅达利2600 </em>，浏览2020年5月31日，&lt;<a class="ae le" href="https://en.wikipedia.org/wiki/Atari_2600" rel="noopener ugc nofollow" target="_blank">https://en.wikipedia.org/wiki/Atari_2600</a>&gt;。</p></div></div>    
</body>
</html>