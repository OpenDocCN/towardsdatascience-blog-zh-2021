<html>
<head>
<title>Building a Python Code Generator</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">构建Python代码生成器</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/building-a-python-code-generator-4b476eec5804?source=collection_archive---------10-----------------------#2021-03-22">https://towardsdatascience.com/building-a-python-code-generator-4b476eec5804?source=collection_archive---------10-----------------------#2021-03-22</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="5302" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">使用转换器将英语问题语句转换成Python代码</h2></div><p id="bf93" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">NLP技术可以用来生成实际的代码吗？我们离一个用AI写软件的世界还有多远？</p><p id="246b" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在这篇博客中，我尝试构建一个python代码生成器，它可以将简单的英语问题语句转换成相应的python代码。</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi lb"><img src="../Images/71a368f97a050184f465af963cfabd95.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qtnvh5MFsdsqmyjtjRC5jQ.jpeg"/></div></div><p class="ln lo gj gh gi lp lq bd b be z dk translated">图1:生成python代码的转换器。😉(原始图像来自<a class="ae lr" href="https://unsplash.com/photos/liAwyJ64wHE" rel="noopener ugc nofollow" target="_blank"> unsplash </a></p></figure><p id="b1a2" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们通过把它作为一个<strong class="kh ir">序列到序列(Seq2Seq) </strong>的学习问题来处理这个问题。在这里，我们的英语句子将是我们的<em class="ls">输入或SRC </em>序列，我们的Python代码将是我们的<em class="ls">输出或TRG </em>序列。</p><p id="d56a" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在过去的几年里，变压器已经成为解决Seq2Seq问题的主导架构。大多数当今的SoA模型，如BERT或GPT-3，都在内部使用转换器。我们今天所知的变形金刚是由谷歌在他们的<a class="ae lr" href="https://arxiv.org/abs/1706.03762" rel="noopener ugc nofollow" target="_blank"> <strong class="kh ir">“注意力是你所需要的一切”</strong> </a>论文中首次介绍的。我们在博客中讨论的“英语到Python”模型也在本文中找到了它的动机。</p><p id="ebb9" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在我们开始解决问题之前，让我们先简要回顾一下变压器。</p><h1 id="cfef" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated"><strong class="ak">变形金刚</strong></h1><figure class="lc ld le lf gt lg gh gi paragraph-image"><div class="gh gi ml"><img src="../Images/5ed1410b84fff902cde05202c1516e0a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1106/format:webp/1*vBMXkVbcXozv-BfYRyHDSg.png"/></div><p class="ln lo gj gh gi lp lq bd b be z dk translated">图2:变形金刚(图片来自<a class="ae lr" href="https://d2l.ai/chapter_attention-mechanisms/transformer.html" rel="noopener ugc nofollow" target="_blank"> d2l.ai </a>)</p></figure><p id="f9ed" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">变压器可以从三个方面来理解:</p><ol class=""><li id="9d08" class="mm mn iq kh b ki kj kl km ko mo ks mp kw mq la mr ms mt mu bi translated">将输入序列编码成状态表示向量的编码器。</li><li id="2a95" class="mm mn iq kh b ki mv kl mw ko mx ks my kw mz la mr ms mt mu bi translated">一种关注机制，使我们的Transformer模型能够关注顺序输入流的正确方面。这在编码器和解码器中重复使用，以帮助它们将输入数据置于上下文中。</li><li id="b528" class="mm mn iq kh b ki mv kl mw ko mx ks my kw mz la mr ms mt mu bi translated">解码器，对状态表示向量进行解码，以生成目标输出序列。</li></ol><p id="723d" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在我之前的<a class="ae lr" href="https://ai.plainenglish.io/lets-pay-attention-to-transformers-a1c2dc566dbd" rel="noopener ugc nofollow" target="_blank"> <strong class="kh ir">博客</strong> </a>中，我已经详细解释了这些组件中的每一个，并附有代码演练。</p><p id="2b99" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在让我们看看如何将数据输入到转换器中。</p><h1 id="581a" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated">了解培训数据</h1><p id="89b0" class="pw-post-body-paragraph kf kg iq kh b ki na jr kk kl nb ju kn ko nc kq kr ks nd ku kv kw ne ky kz la ij bi translated">我们将使用定制的<a class="ae lr" href="https://drive.google.com/file/d/1rHb0FQ5z5ZpaY2HpyFGY6CeyDG0kTLoO/view" rel="noopener ugc nofollow" target="_blank"> <strong class="kh ir">数据集</strong> </a>由<a class="ae lr" href="https://theschoolof.ai/" rel="noopener ugc nofollow" target="_blank"><strong class="kh ir">AI(TSAI)</strong></a><strong class="kh ir"/>学院策划来训练我们的模型。这个数据集包含大约5000个数据点，其中每个数据点包含一个英文问题语句及其对应的Python代码。可以关注我的<a class="ae lr" href="https://github.com/divyam96/English-to-Python-Converter" rel="noopener ugc nofollow" target="_blank"> <strong class="kh ir">项目回购</strong> </a>了解如何解析数据。</p><p id="9366" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">样本数据点:</strong></p><p id="dc68" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><em class="ls">英文语句:“</em>写一个将两个数相加的函数<em class="ls">”</em></p><p id="0b7d" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><em class="ls"> Python代码:</em></p><pre class="lc ld le lf gt nf ng nh ni aw nj bi"><span id="dce4" class="nk lu iq ng b gy nl nm l nn no">def add_two_numbers (num1 ,num2 ):<br/>    sum =num1 +num2 <br/>    return sum</span></pre><p id="498e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这里的"<em class="ls">英文语句</em>"是我们的输入或SRC序列，而"<em class="ls"> Python代码</em>"是我们的输出或用于训练的TRG序列。</p><h1 id="3674" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated">将数据符号化</h1><p id="319d" class="pw-post-body-paragraph kf kg iq kh b ki na jr kk kl nb ju kn ko nc kq kr ks nd ku kv kw ne ky kz la ij bi translated">我们的输入(SRC)和输出(TRG)序列以单个字符串的形式存在，需要进一步标记才能发送到transformer模型中。</p><p id="8dcd" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">为了标记输入(SRC)序列，我们使用了<strong class="kh ir"> </strong> <a class="ae lr" href="https://spacy.io/api/tokenizer" rel="noopener ugc nofollow" target="_blank"> <strong class="kh ir">空间</strong> </a>。这在PyTorch的<a class="ae lr" href="https://torchtext.readthedocs.io/en/latest/data.html#torchtext.data.Field" rel="noopener ugc nofollow" target="_blank">torch text . data . field</a><strong class="kh ir">中默认实现。</strong>我们将使用torchtext.data.Field通过spacy来标记我们的数据。</p><pre class="lc ld le lf gt nf ng nh ni aw nj bi"><span id="bd4b" class="nk lu iq ng b gy nl nm l nn no">Input = data.Field(tokenize = 'spacy',<br/>            init_token='&lt;sos&gt;', <br/>            eos_token='&lt;eos&gt;', <br/>            lower=<strong class="ng ir">True</strong>)</span></pre><p id="f5d0" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">为了对我们的输出(TRG)序列进行标记化，我们使用了基于Python源代码<a class="ae lr" href="https://docs.python.org/3/library/tokenize.html" rel="noopener ugc nofollow" target="_blank">标记化器</a>构建的自定义标记化器。Python的tokenizer为每个标记返回几个属性。我们只提取元组形式的令牌类型和相应的字符串属性(即(token_type_int，token_string))作为最终令牌。</p><p id="a137" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><em class="ls">标记化输入(SRC): </em></p><pre class="lc ld le lf gt nf ng nh ni aw nj bi"><span id="5135" class="nk lu iq ng b gy nl nm l nn no">SRC = [' ', 'write', 'a', 'python', 'function', 'to', 'add', 'two', 'user', 'provided', 'numbers', 'and', 'return', 'the', 'sum']</span></pre><p id="e8a6" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><em class="ls">符号化输出(TRG): </em></p><pre class="lc ld le lf gt nf ng nh ni aw nj bi"><span id="11c0" class="nk lu iq ng b gy nl nm l nn no">TRG = [(57, 'utf-8'), (1, 'def'), (1, 'add_two_numbers'), (53, '('), (1, 'num1'), (53, ','), (1, 'num2'), (53, ')'), (53, ':'), (4, '\n'), (5, '    '), (1, 'sum'), (53, '='), (1, 'num1'), (53, '+'), (1, 'num2'), (4, '\n'), (1, 'return'), (1, 'sum'), (4, ''), (6, ''), (0, '')]</span></pre><h1 id="149e" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated">数据扩充</h1><p id="9b02" class="pw-post-body-paragraph kf kg iq kh b ki na jr kk kl nb ju kn ko nc kq kr ks nd ku kv kw ne ky kz la ij bi translated">由于我们的数据集仅包含5000个数据点，我们利用数据扩充来增加数据集的大小。在标记python代码时，我们随机屏蔽某些变量的名称(用‘var _ 1’、‘var _ 2’等),以确保我们训练的模型不仅仅关注变量的命名方式，而且实际上试图理解python代码的内在逻辑和语法。</p><p id="1008" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">例如，考虑下面的程序。</p><pre class="lc ld le lf gt nf ng nh ni aw nj bi"><span id="8763" class="nk lu iq ng b gy nl nm l nn no">def add_two_numbers (num1 ,num2 ):<br/>    sum =num1 +num2 <br/>    return sum</span></pre><p id="39ba" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们可以替换上面的一些变量来创建新的数据点。以下是有效的扩充。</p><p id="9316" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi">1.</p><pre class="lc ld le lf gt nf ng nh ni aw nj bi"><span id="10ca" class="nk lu iq ng b gy nl nm l nn no">def add_two_numbers (var_1 ,num2 ):<br/>    sum =var_1 +num2 <br/>    return sum</span></pre><p id="2567" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi">2.</p><pre class="lc ld le lf gt nf ng nh ni aw nj bi"><span id="e453" class="nk lu iq ng b gy nl nm l nn no">def add_two_numbers (num1 ,var_1 ):<br/>   sum =num1 +var_1 <br/>   return sum<!-- --> </span></pre><p id="9106" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi">3.</p><pre class="lc ld le lf gt nf ng nh ni aw nj bi"><span id="2de7" class="nk lu iq ng b gy nl nm l nn no">def add_two_numbers (var_1 ,var_2 ):<br/>    sum = var_1 + var_2 <br/>    return sum</span></pre><p id="189e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在上面的例子中，我们使用随机变量替换技术将一个数据点扩展为另外3个数据点。</p><p id="8cff" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们在生成TRG令牌时实现我们的扩充。</p><p id="ef8c" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">当随机选择变量来屏蔽时，我们避免关键字文字(<em class="ls"> keyword.kwlist </em>)、控制结构(如下面的<em class="ls"> skip_list </em>所示)和对象属性。我们将所有需要跳过的文字添加到<em class="ls"> skip_list中。</em></p><figure class="lc ld le lf gt lg"><div class="bz fp l di"><div class="np nq l"/></div></figure><p id="b72b" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们现在使用Pytorch的<a class="ae lr" href="https://torchtext.readthedocs.io/en/latest/data.html#torchtext.data.Field" rel="noopener ugc nofollow" target="_blank"> torchtext.data.Field </a> <strong class="kh ir">来应用我们的扩充和标记化。</strong></p><pre class="lc ld le lf gt nf ng nh ni aw nj bi"><span id="e255" class="nk lu iq ng b gy nl nm l nn no">Output = data.Field(tokenize = augment_tokenize_python_code,<br/>                    init_token='&lt;sos&gt;', <br/>                    eos_token='&lt;eos&gt;', <br/>                    lower=<strong class="ng ir">False</strong>)</span></pre><p id="bb82" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们的测试<em class="ls">应用标记化后的标记化输出(TRG) </em>:</p><pre class="lc ld le lf gt nf ng nh ni aw nj bi"><span id="c820" class="nk lu iq ng b gy nl nm l nn no">TRG = [(57, 'utf-8'), (1, 'def'), (1, 'add_two_numbers'), (53, '('), (1, 'num1'), (53, ','), (1, 'var_1'), (53, ')'), (53, ':'), (4, '\n'), (5, '    '), (1, 'sum'), (53, '='), (1, 'num1'), (53, '+'), (1, 'var_1'), (4, '\n'), (1, 'return'), (1, 'sum'), (4, ''), (6, ''), (0, '')]</span></pre><h1 id="5e2d" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated">喂养数据</h1><p id="5386" class="pw-post-body-paragraph kf kg iq kh b ki na jr kk kl nb ju kn ko nc kq kr ks nd ku kv kw ne ky kz la ij bi translated">为了将数据输入到我们的模型中，我们首先使用Pytorch的<a class="ae lr" href="https://torchtext.readthedocs.io/en/latest/data.html#torchtext.data.BucketIterator" rel="noopener ugc nofollow" target="_blank">torch text . data . bucket iterator</a>创建批处理。这确保了具有相似长度的输入一起留在单个批次中，以便于训练。然后，我们将标记化输入(SRC)批次送入编码器，并在解码器中使用标记化输出(TRG)批次。我们的目标是使用编码器的标记化英语输入(SRC)来预测通过解码器的标记化Python输出(TRG)。然后，标记化的预测通过Python源代码标记化器的<a class="ae lr" href="https://docs.python.org/3/library/tokenize.html#tokenize.untokenize" rel="noopener ugc nofollow" target="_blank">取消标记化</a>函数取消标记化。</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/fc3fa0e86be2c1915a56eb151405bcc1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1012/format:webp/1*gnd1JZoyIX8wUacL7DZyxA.png"/></div><p class="ln lo gj gh gi lp lq bd b be z dk translated">图3:编码器的输入和解码器的输出。(图片来自<a class="ae lr" href="https://arxiv.org/abs/1706.03762" rel="noopener ugc nofollow" target="_blank">谷歌arxiv研究</a>)</p></figure><h1 id="c416" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated">损失函数</h1><p id="0a99" class="pw-post-body-paragraph kf kg iq kh b ki na jr kk kl nb ju kn ko nc kq kr ks nd ku kv kw ne ky kz la ij bi translated">我们在数据集中使用了扩充来屏蔽变量文字。这意味着我们的模型可以预测特定变量的各种值，只要预测在代码中保持一致，所有这些值都是正确的。这将意味着我们的训练标签不是非常确定，因此以概率<em class="ls"> 1- smooth_eps </em>将它们视为正确，否则视为不正确将更有意义。这就是标签平滑的作用。通过添加<a class="ae lr" href="https://arxiv.org/abs/1906.02629" rel="noopener ugc nofollow" target="_blank">标签平滑</a>到<a class="ae lr" href="https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html" rel="noopener ugc nofollow" target="_blank">交叉熵</a>中，我们确保了模型在预测一些可以通过增强来替代的变量时不会变得过于自信。</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi ns"><img src="../Images/a4ed840d90f3ecb46fd3c4c428ddb3fa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oHROlXfKksEZWxbV4Uqdww.png"/></div></div><p class="ln lo gj gh gi lp lq bd b be z dk translated">图4:标签平滑(来源:<a class="ae lr" href="https://www.deeplearningbook.org/" rel="noopener ugc nofollow" target="_blank">深度学习</a>，Goodfellow等人2016)</p></figure><p id="931a" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在我们所有的组件都设置好了，我们可以使用反向传播来训练我们的模型。我们将数据集分为训练数据和验证数据。我们的模型被训练，直到我们的验证损失不再进一步改善。</p><p id="ad29" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">值得注意的是，与不使用标签平滑的模型相比，标签平滑会导致更高的损失值。但这是意料之中的，因为我们不打算确定我们的标签预测。对于变量尤其如此，因为只要预测在整个目标代码序列中是一致的，就可以有多个正确的选项。</p><p id="f137" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">完整的训练代码请参考我的<a class="ae lr" href="https://github.com/divyam96/English-to-Python-Converter/blob/main/English_to_Python.ipynb" rel="noopener ugc nofollow" target="_blank">笔记本</a>。</p><h1 id="a9bc" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated">样本结果</h1><p id="c438" class="pw-post-body-paragraph kf kg iq kh b ki na jr kk kl nb ju kn ko nc kq kr ks nd ku kv kw ne ky kz la ij bi translated"><em class="ls">输入:</em>“程序按关键字排序字典列表”<br/>T5】输出:</p><pre class="lc ld le lf gt nf ng nh ni aw nj bi"><span id="5a67" class="nk lu iq ng b gy nl nm l nn no">var_1 ={'Nikhil':{'roll':24 ,'marks':17 },<br/>'Akshat':{'roll':54 ,'marks':12 },<br/>'Akash':{'roll':15 },'marks':15 }}<br/>sort_key ='marks'<br/>res ='marks'<br/>res =var_2 (test_dict .items (),key =lambda x :x [1 ][sort_key ])<br/>print ("The sorted dictionary by marks is : "+str (res ))</span></pre><p id="a108" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><em class="ls">输入:</em>“列表奇数元素求和函数”<br/> <em class="ls">输出</em>:</p><pre class="lc ld le lf gt nf ng nh ni aw nj bi"><span id="8a82" class="nk lu iq ng b gy nl nm l nn no">def sum_odd_elements (l :list ):<br/>    return sum ([i for i in l if i %2 ==1 ])</span></pre><p id="5944" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><em class="ls">输入:</em>程序反转一串<em class="ls"><br/>输出</em>:</p><pre class="lc ld le lf gt nf ng nh ni aw nj bi"><span id="6c01" class="nk lu iq ng b gy nl nm l nn no">var_1 ='Today is bad day'<br/>var_1 [::-1 ]</span></pre><p id="c14d" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">更多样本请看我的<a class="ae lr" href="https://github.com/divyam96/English-to-Python-Converter" rel="noopener ugc nofollow" target="_blank">项目回购</a>。</p><p id="fd71" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们已经成功训练了一个模型，它能够将简单的问题陈述(英语)转换成相应的python代码。看起来我们离人工智能代替人类编写软件的时代不远了。软件开发人员当心！</p><h1 id="8d8b" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated">参考知识库</h1><div class="nt nu gp gr nv nw"><a href="https://github.com/divyam96/English-to-Python-Converter" rel="noopener  ugc nofollow" target="_blank"><div class="nx ab fo"><div class="ny ab nz cl cj oa"><h2 class="bd ir gy z fp ob fr fs oc fu fw ip bi translated">divyam 96/英语-Python转换器</h2><div class="od l"><h3 class="bd b gy z fp ob fr fs oc fu fw dk translated">这是一个尝试使用变压器和自我关注，以便将英文描述转换成Python代码…</h3></div><div class="oe l"><p class="bd b dl z fp ob fr fs oc fu fw dk translated">github.com</p></div></div><div class="of l"><div class="og l oh oi oj of ok ll nw"/></div></div></a></div><div class="nt nu gp gr nv nw"><a href="https://github.com/bentrevett/pytorch-seq2seq" rel="noopener  ugc nofollow" target="_blank"><div class="nx ab fo"><div class="ny ab nz cl cj oa"><h2 class="bd ir gy z fp ob fr fs oc fu fw ip bi translated">bentrevett/pytorch-seq2seq</h2><div class="od l"><h3 class="bd b gy z fp ob fr fs oc fu fw dk translated">本报告包含的教程涵盖了使用…理解和实现序列到序列(seq2seq)模型</h3></div><div class="oe l"><p class="bd b dl z fp ob fr fs oc fu fw dk translated">github.com</p></div></div><div class="of l"><div class="ol l oh oi oj of ok ll nw"/></div></div></a></div><h1 id="e636" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated">参考</h1><p id="f550" class="pw-post-body-paragraph kf kg iq kh b ki na jr kk kl nb ju kn ko nc kq kr ks nd ku kv kw ne ky kz la ij bi translated">[1]阿斯顿·张(Aston Zhang)和扎卡里·c·利普顿(Zachary C. Lipton)以及李牧和亚历山大·j·斯莫拉(Alexander J. Smola)，<a class="ae lr" href="https://d2l.ai" rel="noopener ugc nofollow" target="_blank">潜入深度学习</a> (2020)。</p><p id="e325" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">[2] Ashish Vaswani，Noam Shazeer，Niki Parmar，Jakob Uszkoreit，Llion Jones，Aidan N. Gomez，Lukasz Kaiser，Illia Polosukhin，<a class="ae lr" href="https://arxiv.org/abs/1706.03762" rel="noopener ugc nofollow" target="_blank"/>(2017)，第31届神经信息处理系统会议(NIPS 2017)，美国加利福尼亚州长滩</p><p id="1aaa" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">[3] Rafael Müller，Simon Kornblith，Geoffrey Hinton，<a class="ae lr" href="https://arxiv.org/pdf/1906.02629.pdf" rel="noopener ugc nofollow" target="_blank">标签平滑在什么情况下有帮助？</a> (2019)，第33届神经信息处理系统会议(NeurIPS 2019)，加拿大温哥华</p><p id="d4de" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">[4]伊恩·古德费勒(Ian Goodfellow)与约舒阿·本吉奥(Yoshua Bengio)和亚伦·库维尔(Aaron Smith)，<a class="ae lr" href="https://www.deeplearningbook.org/" rel="noopener ugc nofollow" target="_blank">深度学习书籍</a> (2016)，麻省理工学院出版社。</p></div></div>    
</body>
</html>