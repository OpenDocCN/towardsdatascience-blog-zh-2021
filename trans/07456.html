<html>
<head>
<title>ASMNet: a Lightweight Deep Neural Network for Face Alignment and Pose Estimation</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">ASMNet:一种用于人脸对齐和姿态估计的轻量级深度神经网络</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/asmnet-a-lightweight-deep-neural-network-for-face-alignment-and-pose-estimation-9e9dfac07094?source=collection_archive---------23-----------------------#2021-07-07">https://towardsdatascience.com/asmnet-a-lightweight-deep-neural-network-for-face-alignment-and-pose-estimation-9e9dfac07094?source=collection_archive---------23-----------------------#2021-07-07</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="e50a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">本文解释了ASMNet，这是一个轻量级卷积神经网络(CNN ),用于面部标志点检测(也称为面部对齐)和野外面部姿态估计。</p><p id="0863" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">代码和预先训练好的模型可以在Github <a class="ae kl" href="https://github.com/aliprf/ASMNet" rel="noopener ugc nofollow" target="_blank">这里</a>获得。你也可以在这里阅读<a class="ae kl" href="https://openaccess.thecvf.com/content/CVPR2021W/AMFG/html/Fard_ASMNet_A_Lightweight_Deep_Neural_Network_for_Face_Alignment_and_CVPRW_2021_paper.html" rel="noopener ugc nofollow" target="_blank">原文。</a></p><h1 id="a8d0" class="km kn iq bd ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj bi translated">介绍</h1><p id="9afe" class="pw-post-body-paragraph jn jo iq jp b jq lk js jt ju ll jw jx jy lm ka kb kc ln ke kf kg lo ki kj kk ij bi translated">面部标志点检测是许多面部图像分析和应用中的基本任务。它对于人脸图像对齐、人脸识别、姿态估计和表情识别至关重要。已经提出了几种方法用于面部标志点检测，例如基于约束局部模型的方法[1，2]，AAM [3，4]，部分模型[5]，以及基于深度学习(DL)的方法[6，7]。尽管基于DL的方法被认为是最先进的方法，但是对于具有大姿态变化的人脸，人脸标志点检测仍然具有挑战性。因此，实现高精度的代价是计算复杂性的增加和效率的下降。</p><p id="757a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">此外，包含在特征中的信息分层地分布在整个深度神经网络中。更具体地，虽然较低层包含关于边缘和角的信息，因此更适合于定位任务，例如面部标志点检测和姿态估计，但是较深层包含更适合于分类任务的更抽象的信息。受多任务学习思想的启发，我们设计了我们的CNN模型以及相关的损失函数来同时学习多个相关的任务。</p><p id="43d0" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">最近的方法集中于提高精确度，这通常通过引入新的层、增加参数的数量和更长的推断时间来实现。这些方法在桌面和服务器应用中是准确和成功的，但随着物联网、移动设备和机器人技术的发展，人们越来越需要更准确和高效的算法。</p><p id="160b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们提出了一种新的网络结构，它受MobileNetV2的启发，专门设计用于面部标志点检测，重点是在不损失太多准确性的情况下使网络变得浅而小。为了实现这个目标，我们提出了一个新的损失函数，它使用ASM作为辅助损失，并使用多任务学习来提高准确率。图1描绘了我们提出的想法的一般框架。我们用具有挑战性的300W [8]数据集和野外更宽的面部标志(WFLW) [9]数据集测试了我们提出的方法。我们的实验结果表明，在网络规模比MobileNetV2小2倍的情况下，面部标志点检测和姿态估计的精度与最先进的方法相当。</p></div><div class="ab cl lp lq hu lr" role="separator"><span class="ls bw bk lt lu lv"/><span class="ls bw bk lt lu lv"/><span class="ls bw bk lt lu"/></div><div class="ij ik il im in"><h1 id="2c47" class="km kn iq bd ko kp lw kr ks kt lx kv kw kx ly kz la lb lz ld le lf ma lh li lj bi translated">ASM网络</h1><figure class="mc md me mf gt mg gh gi paragraph-image"><div class="gh gi mb"><img src="../Images/c531291027f99e76fc52542105c77d37.png" data-original-src="https://miro.medium.com/v2/resize:fit:746/format:webp/1*skgznH9I1csP-y9poHYFAg.png"/></div><p class="mj mk gj gh gi ml mm bd b be z dk translated">图1: ASM网络(图片来自作者)</p></figure><p id="9252" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们设计了一个比MobileNetV2 [33]小两倍的网络，在参数和FLOPs数量方面都是如此。在设计ASMNet时，我们只使用MobileNetV2 [33]的前15个块，而主架构有16个块。然而，创建一个浅层网络最终会降低系统的最终精度。为了避免这个问题，我们有目的地添加了一些新的层。图1显示了ASMNet的架构。</p><p id="b92c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">除此之外，在CNN中，较低层具有诸如边缘和角之类的特征，这些特征更适合于诸如地标定位和姿态估计之类的任务，而较深层包含更适合于诸如图像分类和图像检测之类的任务的更抽象的特征。因此，为相关任务训练网络同时建立了可以提高每个任务的性能的协同作用。</p><p id="d554" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">因此，我们设计了一个多任务CNN来检测面部标志，同时估计面部的姿态(俯仰、滚动和偏航)。为了使用不同图层的功能，我们创建了块1-批量-归一化、块3-批量-归一化、块6-批量-归一化、块10-批量-归一化以及最后块13-批量-归一化的快捷方式。我们使用全局平均池层将这些快捷方式中的每一个连接到MobileNetV2的块15的输出，块15-add。最后，我们连接所有的全局平均池层。这种架构使我们能够使用网络不同层中可用的功能，同时保持触发器的数量较少。换句话说，由于最初的MobileNetV2是为图像分类任务而设计的，其中需要更抽象的特征，因此它可能不适合面部对齐任务，因为面部对齐任务既需要在较深层中可用的抽象特征，也需要在较低层中可用的特征，例如边缘和拐角。</p><p id="d1ec" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">此外，我们向网络中添加了另一个相关的任务。如图1所示，所提出的网络预测2个不同的输出:面部标志点(网络的主要输出)，以及面部姿态。虽然这两个任务之间的相关性和协同作用可以产生更准确的结果，但我们也希望我们的轻量级ASMNet能够预测人脸姿态，以便它可以用于更多的应用程序。</p></div><div class="ab cl lp lq hu lr" role="separator"><span class="ls bw bk lt lu lv"/><span class="ls bw bk lt lu lv"/><span class="ls bw bk lt lu"/></div><div class="ij ik il im in"><h1 id="eb22" class="km kn iq bd ko kp lw kr ks kt lx kv kw kx ly kz la lb lz ld le lf ma lh li lj bi translated">ASM辅助损失函数</h1><p id="d8e9" class="pw-post-body-paragraph jn jo iq jp b jq lk js jt ju ll jw jx jy lm ka kb kc ln ke kf kg lo ki kj kk ij bi translated">我们首先回顾主动形状模型(ASM)算法，然后解释我们基于ASM定制的损失函数，它提高了网络的准确性。</p><h2 id="f5db" class="mn kn iq bd ko mo mp dn ks mq mr dp kw jy ms mt la kc mu mv le kg mw mx li my bi translated">活动形状模型检查</h2><p id="c8b9" class="pw-post-body-paragraph jn jo iq jp b jq lk js jt ju ll jw jx jy lm ka kb kc ln ke kf kg lo ki kj kk ij bi translated">活动形状模型是形状对象的统计模型。每个形状被表示为<em class="mz"> n </em>个点以及<em class="mz"> S </em>集合在等式中定义。1在以下:</p><figure class="mc md me mf gt mg gh gi paragraph-image"><div role="button" tabindex="0" class="nb nc di nd bf ne"><div class="gh gi na"><img src="../Images/c0e8b1d83891148a8c760c56a2f7e876.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KaFi5hI6zdvvyLIIlAMnoA.png"/></div></div></figure><p id="1783" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">为了简化问题和学习形状分量，将主分量分析(PCA)应用于从一组K个训练形状样本计算的协方差矩阵。一旦建立了模型，任何训练样本的近似值都可以使用等式计算。2:</p><figure class="mc md me mf gt mg gh gi paragraph-image"><div class="gh gi nf"><img src="../Images/2126c50f45b7faf0a31af5ed38ef4044.png" data-original-src="https://miro.medium.com/v2/resize:fit:430/format:webp/1*nEMvXPnFxygv7TbaadUINQ.png"/></div></figure><p id="683d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">因此，可变形模型的一组参数由向量b定义，从而通过改变向量的元素，改变模型的形状。考虑b的第I个参数的统计方差(即本征值)为λi，为了保证应用ASM后生成的图像与地面真实情况比较接近，通常将向量b的参数bi限定为3√λi [7]。此约束确保生成的形状与原始训练集中的形状相似。因此，在应用了这个约束后，我们根据等式创建了一个新的形状<br/>。3:</p><figure class="mc md me mf gt mg gh gi paragraph-image"><div class="gh gi ng"><img src="../Images/7ee9cfecf3cb88399c055881024f46f4.png" data-original-src="https://miro.medium.com/v2/resize:fit:824/format:webp/1*5JVPdT315jh9KmXE7g4XdQ.png"/></div></figure><p id="6a4a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">其中b̃是受约束的b。我们还根据等式定义ASM算子。4:</p><figure class="mc md me mf gt mg gh gi paragraph-image"><div role="button" tabindex="0" class="nb nc di nd bf ne"><div class="gh gi nh"><img src="../Images/c0d31bc08dc56829b3008c8696793e44.png" data-original-src="https://miro.medium.com/v2/resize:fit:1286/format:webp/1*LfTQR5iqYMPm0nMMb6BfyA.png"/></div></div></figure><p id="0443" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><em class="mz"> ASM </em>使用等式将每个输入点(Px i，Py i)转换为新点(Aix，Aiy)。1、2和3。</p><figure class="mc md me mf gt mg gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/98d41d2a5372fa0460f0124644ff749f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1120/format:webp/1*02kGVJBJN4zg4SiGMWu1Jg.png"/></div><p class="mj mk gj gh gi ml mm bd b be z dk translated">图2:asmlos(图片由作者提供)</p></figure><h2 id="8663" class="mn kn iq bd ko mo mp dn ks mq mr dp kw jy ms mt la kc mu mv le kg mw mx li my bi translated">ASM辅助损失</h2><p id="2da2" class="pw-post-body-paragraph jn jo iq jp b jq lk js jt ju ll jw jx jy lm ka kb kc ln ke kf kg lo ki kj kk ij bi translated">我们描述了两种不同任务的损失函数。这些任务负责面部标志点检测和姿态估计。</p><p id="394b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">人脸标志点检测任务</strong>:人脸标志点检测常用的损失函数是均方误差(MSE)。我们提出了一个新的损失函数，它包括MSE作为主要损失，以及利用ASM提高网络精度的辅助损失，称为ASM-LOSS。</p><p id="9157" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">所提出的ASM损失指导网络首先学习面部标志点的平滑分布。换句话说，在训练过程中，损失函数将预测的面部标志点与它们相应的地面实况以及使用ASM生成的地面实况的平滑版本进行比较。考虑到这一点，在训练的早期阶段，与主要损失(即MSE)相比，我们为ASM损失设置了更大的权重，因为平滑的面部标志点的变化比原始标志点低得多，并且作为经验法则，更容易被CNN学习。然后，通过逐渐降低ASM-LOSS的权重，我们引导网络更多地关注原始标志点。在实践中，我们发现这种方法，也可以被认为是迁移学习，效果很好，可以得到更准确的模型。</p><p id="8e0c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们还发现，虽然人脸姿态估计严重依赖于人脸对齐，但它也可以在平滑的人脸标志点的帮助下获得良好的精度。换句话说，如果面部标志点检测任务的性能是可接受的，这意味着网络可以预测面部标志，使得面部的整个形状是正确的，则姿态估计可以达到良好的精度。因此，使用平滑的标志点和使用ASM-LOSS的训练网络将导致姿态估计任务的更高精度。</p><p id="9479" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">考虑对于训练集中的每个图像，在称为G的集合中存在n个标志点，使得(Gxi，Gyi)是第I个标志点的坐标。类似地，预测集P包含n个点，使得(Px i，Py i)是第I个标志点的预测坐标。</p><figure class="mc md me mf gt mg gh gi paragraph-image"><div class="gh gi nj"><img src="../Images/fcc21f900c594921541e53d8e1e96156.png" data-original-src="https://miro.medium.com/v2/resize:fit:882/format:webp/1*_kA08f_8R-8wFCoTgvdGEQ.png"/></div></figure><p id="51fe" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们对训练集应用PCA并计算特征向量和特征值。然后，我们计算集合A，它包含n个点，每个点都是G中对应点的变换，根据等式应用ASM运算符<br/>。4:</p><figure class="mc md me mf gt mg gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/eda1c4d53be446e2ef525c7300fa4f05.png" data-original-src="https://miro.medium.com/v2/resize:fit:838/format:webp/1*atqE1O7K7lMbXxJaHlZ-rA.png"/></div></figure><p id="534f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们定义主要的面部标志点损失，Eq。7、作为<br/>地面实况(G)和<br/>预测标志点(P)之间的均方差</p><figure class="mc md me mf gt mg gh gi paragraph-image"><div class="gh gi nl"><img src="../Images/97af455410b98ac5ab709ac401246e0c.png" data-original-src="https://miro.medium.com/v2/resize:fit:844/format:webp/1*n1nryHkVwR--2D0Gx3RbKA.png"/></div></figure><p id="76c6" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">其中N是训练集中图像的总数，Gij = (Gix，Giy)表示训练集中第j个样本的第I个界标。我们将ASM损失计算为ASM点(Aset)和预测标志点(Pset)之间的误差，使用等式。8:</p><figure class="mc md me mf gt mg gh gi paragraph-image"><div class="gh gi nm"><img src="../Images/f07ee6817e733e39ab2018aee2a93d17.png" data-original-src="https://miro.medium.com/v2/resize:fit:852/format:webp/1*8LLFjoqEm7GRw85oJYbYgw.png"/></div></figure><p id="ee3e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">最后，我们根据等式计算面部标志任务的总损失。9:</p><figure class="mc md me mf gt mg gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/84d1a67e5fc0307e5fff786d99098f11.png" data-original-src="https://miro.medium.com/v2/resize:fit:748/format:webp/1*xQIIXG_UhV6GjevR2JhKoA.png"/></div></figure><p id="840f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">主成分分析的准确性严重依赖于ASM点(Aset)，这意味着主成分分析越准确，地面实况(G)和ASM点(Aset)之间的差异就越小。更详细地说，通过降低PCA的准确度，生成的ASM点(Aset)将更类似于平均点集，平均点集是训练集中所有地面真实人脸对象的平均值。因此，预测Aset中的点比Gset中的点更容易，因为后者的变化低于前者的变化。我们使用这个特征来设计我们的损失函数，使得我们首先引导网络学习平滑的标志点的分布——这更容易学习——并且通过降低ASM损失的权重来逐渐硬化问题。我们使用等式将α定义为ASM-减重。10:</p><figure class="mc md me mf gt mg gh gi paragraph-image"><div class="gh gi no"><img src="../Images/32a558fee1286b8853feac048c6211ab.png" data-original-src="https://miro.medium.com/v2/resize:fit:764/format:webp/1*xWejBdMBj-hXc5LsY9efXg.png"/></div></figure><p id="8a81" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">其中I是历元数，l是训练历元的总数。如等式所示。9，在训练开始时，α的值较高，这意味着我们更加重视ASM-LOSS。因此，网络更关注于预测更简单的任务，并且收敛得更快。然后，在总周期的三分之一之后，我们将α减少到1，并同等重视主要MSE损失ASM损失。最后，在总时期的三分之二之后，通过将<br/> α减小到0.5，我们将网络导向预测主要地面事实，同时考虑使用ASM作为辅助生成的平滑点。</p><p id="98c9" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">姿态估计任务:</strong>我们使用均方差来计算头部姿态估计任务的损失。情商。11定义了损失函数<em class="mz"> Lpose </em>，其中偏航(yp)、俯仰(pp)和滚转(rp)是预测姿态，而<em class="mz"> yt </em>、<em class="mz"> pt </em>和<em class="mz"> rt </em>是相应的地面实况。</p><figure class="mc md me mf gt mg gh gi paragraph-image"><div class="gh gi np"><img src="../Images/cbfcbaefa20c51cf8a92ae49c6579d0f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1330/format:webp/1*XIBRY9LZbzwYUHm256piyw.png"/></div></figure></div><div class="ab cl lp lq hu lr" role="separator"><span class="ls bw bk lt lu lv"/><span class="ls bw bk lt lu lv"/><span class="ls bw bk lt lu"/></div><div class="ij ik il im in"><h1 id="92c6" class="km kn iq bd ko kp lw kr ks kt lx kv kw kx ly kz la lb lz ld le lf ma lh li lj bi translated">实施细节</h1><p id="7513" class="pw-post-body-paragraph jn jo iq jp b jq lk js jt ju ll jw jx jy lm ka kb kc ln ke kf kg lo ki kj kk ij bi translated">代码可以在Github上找到。关于代码的所有文档也是可用的。</p><p id="b14f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">安装需求</strong></p><p id="848b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">为了运行代码，您需要安装python &gt;= 3.5。可以使用以下命令安装运行代码所需的要求和库:</p><pre class="mc md me mf gt nq nr ns nt aw nu bi"><span id="21ae" class="mn kn iq nr b gy nv nw l nx ny">pip install -r requirements.txt</span></pre><p id="e37f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">使用预先训练好的模型</strong></p><p id="62d4" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">您可以使用以下文件中的代码测试和使用预训练模型:<a class="ae kl" href="https://github.com/aliprf/ASMNet/blob/master/main.py" rel="noopener ugc nofollow" target="_blank">https://github.com/aliprf/ASMNet/blob/master/main.py</a></p><pre class="mc md me mf gt nq nr ns nt aw nu bi"><span id="f50b" class="mn kn iq nr b gy nv nw l nx ny">tester = Test()<br/>  tester.test_model(ds_name=DatasetName.w300,<br/>                     pretrained_model_path='./pre_trained_models/ASMNet/ASM_loss/ASMNet_300W_ASMLoss.h5')</span></pre><h2 id="aa8e" class="mn kn iq bd ko mo mp dn ks mq mr dp kw jy ms mt la kc mu mv le kg mw mx li my bi translated">从零开始的培训网络</h2><p id="abe7" class="pw-post-body-paragraph jn jo iq jp b jq lk js jt ju ll jw jx jy lm ka kb kc ln ke kf kg lo ki kj kk ij bi translated"><strong class="jp ir">准备数据</strong></p><p id="54c0" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">数据需要规范化，以npy格式保存。</p><p id="3888" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">常设仲裁法院的创建</strong></p><p id="fe1a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">您可以使用pca_utility.py类来创建特征值、特征向量和均值向量:</p><pre class="mc md me mf gt nq nr ns nt aw nu bi"><span id="1a46" class="mn kn iq nr b gy nv nw l nx ny">pca_calc = PCAUtility()<br/>    pca_calc.create_pca_from_npy(dataset_name=DatasetName.w300,<br/>                                 labels_npy_path='./data/w300/normalized_labels/',<br/>                                 pca_percentages=90)</span></pre><p id="e167" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">培训</strong></p><p id="c7c0" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">培训实现位于train.py类中。您可以使用以下代码开始培训:</p><pre class="mc md me mf gt nq nr ns nt aw nu bi"><span id="7a32" class="mn kn iq nr b gy nv nw l nx ny">trainer = Train(arch=ModelArch.ASMNet,<br/>                    dataset_name=DatasetName.w300,<br/>                    save_path='./',<br/>                    asm_accuracy=90)</span></pre></div><div class="ab cl lp lq hu lr" role="separator"><span class="ls bw bk lt lu lv"/><span class="ls bw bk lt lu lv"/><span class="ls bw bk lt lu"/></div><div class="ij ik il im in"><h1 id="adca" class="km kn iq bd ko kp lw kr ks kt lx kv kw kx ly kz la lb lz ld le lf ma lh li lj bi translated">结果</h1><p id="2832" class="pw-post-body-paragraph jn jo iq jp b jq lk js jt ju ll jw jx jy lm ka kb kc ln ke kf kg lo ki kj kk ij bi translated">ASMNet可以以非常高的视觉精度执行人脸对齐和姿态估计。样本图片请访问我的GitHub或原始论文。</p></div><div class="ab cl lp lq hu lr" role="separator"><span class="ls bw bk lt lu lv"/><span class="ls bw bk lt lu lv"/><span class="ls bw bk lt lu"/></div><div class="ij ik il im in"><h1 id="d107" class="km kn iq bd ko kp lw kr ks kt lx kv kw kx ly kz la lb lz ld le lf ma lh li lj bi translated">结论</h1><p id="5cdf" class="pw-post-body-paragraph jn jo iq jp b jq lk js jt ju ll jw jx jy lm ka kb kc ln ke kf kg lo ki kj kk ij bi translated">我们提出了ASMNet，一种轻量级的CNN架构，具有多任务学习功能，用于面部标志点检测和姿态估计。我们提出了使用ASM辅助的损失函数来提高网络精度。我们使用MobileNetV2的一小部分构建了我们的网络(称为ASMNet)。所提出的ASMNet架构比MobileNetV2小大约2倍，而精度保持在相同的rat</p></div><div class="ab cl lp lq hu lr" role="separator"><span class="ls bw bk lt lu lv"/><span class="ls bw bk lt lu lv"/><span class="ls bw bk lt lu"/></div><div class="ij ik il im in"><h1 id="fe27" class="km kn iq bd ko kp lw kr ks kt lx kv kw kx ly kz la lb lz ld le lf ma lh li lj bi translated">请将此作品引用为:</h1><pre class="mc md me mf gt nq nr ns nt aw nu bi"><span id="9ad2" class="mn kn iq nr b gy nv nw l nx ny">@inproceedings{fard2021asmnet,<br/>        title={ASMNet: A Lightweight Deep Neural Network for Face Alignment and Pose Estimation},<br/>        author={Fard, Ali Pourramezan and Abdollahi, Hojjat and Mahoor, Mohammad},<br/>        booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},<br/>        pages={1521--1530},<br/>        year={2021}<br/>  }</span></pre></div><div class="ab cl lp lq hu lr" role="separator"><span class="ls bw bk lt lu lv"/><span class="ls bw bk lt lu lv"/><span class="ls bw bk lt lu"/></div><div class="ij ik il im in"><h2 id="ad10" class="mn kn iq bd ko mo mp dn ks mq mr dp kw jy ms mt la kc mu mv le kg mw mx li my bi translated">参考</h2><p id="4ea7" class="pw-post-body-paragraph jn jo iq jp b jq lk js jt ju ll jw jx jy lm ka kb kc ln ke kf kg lo ki kj kk ij bi translated">[1] A. Asthana、S. Zafeiriou、S. Cheng和M. Pantic。约束局部模型的鲁棒判别响应图拟合。IEEE计算机视觉和模式识别会议论文集，第3444–3451页，2013年。</p><p id="e044" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">[2] D. Cristinacce和T. F. Cootes。约束局部模型下的特征检测和跟踪。在Bmvc，第1卷，第3页。Citeseer，2006年。</p><p id="3ec1" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">[3]库茨、爱德华兹和泰勒。主动外观模型。在欧洲计算机视觉会议上，第484-498页。斯普林格，1998年。</p><p id="70a0" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">[4]马丁斯、卡塞罗和巴蒂斯塔。通过2.5维主动外观模型的生成式人脸对齐。计算机视觉和图像理解，117(3):250–268，2013。</p><p id="0285" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">[5] X .朱和d .拉马南。人脸检测、姿态估计、<br/>和野外地标定位。2012年IEEE计算机视觉和模式识别会议，2879-2886页，2012年6月。</p><p id="fd3b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">[6]张俊杰、单绍良、简明和陈。用于实时人脸对齐的粗到细自动编码器网络(cfan)。在欧洲计算机视觉会议上，第1-16页。斯普林格，2014。</p><p id="4a09" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">[7]张志军、罗培平、李春春和唐晓明。基于深度多任务学习的人脸标志点检测。在欧洲计算机视觉会议上，第94-108页。斯普林格，2014。</p><p id="73ab" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">[8] C .萨戈纳斯、G. Tzimiropoulos、S. Zafeiriou和M. Pantic。野外300张脸挑战:第一次面部标志定位挑战。IEEE计算机视觉研讨会国际会议论文集，397–403页，2013年。</p><p id="2d1b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">[9]吴文伟、钱春弦、杨树国、王庆、蔡玉英和周庆。一种边界感知的人脸对齐算法。IEEE计算机视觉和模式识别会议论文集，第2129-2138页，2018年。</p></div></div>    
</body>
</html>