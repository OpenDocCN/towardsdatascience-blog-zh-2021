<html>
<head>
<title>About Post-Decision States Again</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">关于后决策状态</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/about-post-decision-states-again-5725e5c15d90?source=collection_archive---------34-----------------------#2021-06-23">https://towardsdatascience.com/about-post-decision-states-again-5725e5c15d90?source=collection_archive---------34-----------------------#2021-06-23</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="b92d" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsdatascience.com/tagged/thoughts-and-theory" rel="noopener" target="_blank">思想和理论</a></h2><div class=""/><div class=""><h2 id="6416" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">论强化学习中状态-动作对和决策后状态的(不那么)细微差别。</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/802916c44e6990ac561ab75f93e122ac.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*VbiuRpDWa28jkTPj"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">照片由<a class="ae lh" href="https://unsplash.com/@rayhennessy?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">雷·轩尼诗</a>在<a class="ae lh" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><p id="5c42" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我之前关于后决策状态的文章没有得到太多关注，所以我决定再写一篇(我猜是关于狐狸和陷阱的)。</p><p id="6844" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">在上一篇文章中，我为强化学习中状态-动作对和决策后状态之间的相似性做了一个案例。简而言之，决策后状态是状态和行动的结合(例如，在添加标记后的<em class="me">，但是在</em>之前的<em class="me">)，这是世界再次开始运动之前的一种中间状态。</em></p><p id="02db" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">然而，在关注相似之处的同时，我也掩盖了这两个概念之间的一些重要差异。留下他们未提及的感觉未完成。这篇文章着重于区分后决策状态和状态-行为对，给出了一个更完整的观点。</p><h1 id="ec65" class="mf mg it bd mh mi mj mk ml mm mn mo mp ki mq kj mr kl ms km mt ko mu kp mv mw bi translated">什么是决策后状态？</h1><p id="70dd" class="pw-post-body-paragraph li lj it lk b ll mx kd ln lo my kg lq lr mz lt lu lv na lx ly lz nb mb mc md im bi translated">当然，我不能从我离开的地方跳回去，所以让我们总结一下后决策状态的概念(详细描述见Powell [1])。正如介绍中所说，这是采取行动后的问题状态，但在新的随机信息到来之前(这将引导我们进入下一个预决策状态)。通常，根据当前状态和可行的行动，您可能会达到决策后状态<code class="fe nc nd ne nf b">S'⊆S</code> <em class="me"> </em>的子集。通过分解从一个状态到另一个状态的转换函数，可以更好地理解这个概念。</p><p id="efa8" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">通常，我们有一个转移函数<code class="fe nc nd ne nf b">f</code>，它将当前状态<code class="fe nc nd ne nf b">s_t</code>、选择的动作<code class="fe nc nd ne nf b">a_t</code>和外部信息<code class="fe nc nd ne nf b">ω_t</code>引导到下一个状态<code class="fe nc nd ne nf b">s_t+1</code>:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi ng"><img src="../Images/b424650f0a3db479adfa916d52a360c6.png" data-original-src="https://miro.medium.com/v2/resize:fit:696/format:webp/1*oXhH79dZ86cRrAkotfPlYA.png"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">从一种状态到另一种状态的转换</p></figure><p id="0639" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">这个转移函数包含一个<strong class="lk jd">确定性成分和一个随机成分</strong>，我们可以明确区分。首先，我们可以确定性地从预决策状态<code class="fe nc nd ne nf b">s_t</code>前进到后决策状态<code class="fe nc nd ne nf b">s_t^a</code>:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi nh"><img src="../Images/c0e542a858a652824f9dfdad0eaed6cc.png" data-original-src="https://miro.medium.com/v2/resize:fit:662/format:webp/1*82WP8vQyx_e-kadSBc-kUQ.png"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">从状态-动作对到决策后状态的确定性转换</p></figure><p id="903a" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">第二，我们可以从随机过程中采样<code class="fe nc nd ne nf b">ω_t</code>，并从决策后状态移动到下一个决策前状态<code class="fe nc nd ne nf b">s_t+1</code>:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi nh"><img src="../Images/621f95d01c012646e901d76c8003ff01.png" data-original-src="https://miro.medium.com/v2/resize:fit:662/format:webp/1*hUoAsZVzeJ-zYn83vvqIzw.png"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">从决策后状态到下一个决策前状态的随机转换</p></figure><p id="38db" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">除了计算Q值<code class="fe nc nd ne nf b">Q(s_t,a_t)</code>，我们还可以计算决策后状态的Q值<code class="fe nc nd ne nf b">Q(s_t^a)</code>，提供一个更清晰的视角。此外，我们通常基于决策后的状态来设计功能:再订购后的库存水平、移动后的网格位置等。简单地说:决策后状态包含了我们拥有的最新信息。</p><h1 id="a709" class="mf mg it bd mh mi mj mk ml mm mn mo mp ki mq kj mr kl ms km mt ko mu kp mv mw bi translated">井字游戏</h1><p id="ce3e" class="pw-post-body-paragraph li lj it lk b ll mx kd ln lo my kg lq lr mz lt lu lv na lx ly lz nb mb mc md im bi translated">在井字游戏中——上一篇文章中使用的例子——状态-动作对和决策后状态实际上是相同的。只有最新的主板配置才是最重要的，而不是为了达到这个目标而采取的措施。决策后状态实际上是状态-动作对的串联:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/a818ef3f48823bd1f5e4584c603ee0cd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1306/format:webp/1*3mqxuYGI2xDCzkws0tyGJQ.png"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">查看给定井字游戏状态下的可用操作。绿色玩家可以采取三个动作或者(等价地)达到三个决策后状态。[图片由作者提供]</p></figure><p id="0a1f" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">实际上，状态-动作对和决策后状态在这里是等价的。你最后采取的行动并不重要，重要的是棋盘的当前状态。在当前状态下，您要么采取3个允许动作中的1个(即，3个状态-动作对)，要么评估可能达到的3个决策后状态。</p><p id="f097" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">接下来的两个例子说明了概念并不总是如此相似。</p><h1 id="4bc9" class="mf mg it bd mh mi mj mk ml mm mn mo mp ki mq kj mr kl ms km mt ko mu kp mv mw bi translated">悬崖漫步</h1><p id="f75e" class="pw-post-body-paragraph li lj it lk b ll mx kd ln lo my kg lq lr mz lt lu lv na lx ly lz nb mb mc md im bi translated">为了显示状态-动作对不同的情况，考虑<a class="ae lh" rel="noopener" target="_blank" href="/walking-off-the-cliff-with-off-policy-reinforcement-learning-7fdbcdfe31ff">悬崖行走问题</a>。在SARSA或Q-learning中，你可以用<code class="fe nc nd ne nf b">Q(s,a)</code>值填充一个查找表，用<code class="fe nc nd ne nf b">s</code>表示网格上的位置，用<code class="fe nc nd ne nf b">a</code>表示可行的移动(<em class="me">左、右、下、上</em>)。相比之下，决策后状态将只是网格位置。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi nj"><img src="../Images/72257f02f984858e683045ab1db65f49.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*waxDA-SoYKW_a__akHjb-A.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">状态-动作对和决策后状态的Q值之间的比较。对于每四个状态-动作对，我们只需要存储一个决策后状态的值。[图片由作者提供]</p></figure><p id="f0d3" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">请注意，在处理状态-动作对时，我们有48*4=192个Q值，但只有48个决策后值。到底有什么区别？</p><p id="f640" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">让我们仔细看看突出显示的单幅图块。它可以从四个相邻的图块到达，每个图块都有自己的状态-动作对。然而，对于剩余的轨迹，你如何在给定的瓷砖上着陆实际上并不重要——重要的是到达目标的剩余步骤数。</p><p id="482e" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">决策后状态巧妙地抓住了这一现象。不考虑前面的图块，我们只需要一个<strong class="lk jd">单个后决策值</strong>来预测剩余的步数。让四个不同的<code class="fe nc nd ne nf b">Q(s,a)</code>值指向单个图块实际上是一个非常低效的解决方案。观察到——为了达到相同的精度——我们需要比决策后实现多四倍的观察值。</p><p id="9ef5" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">只有四个动作，我们或许可以承受打击，但如果我们有1000个动作呢？对于大多数RL问题来说，将计算工作量减少1000倍将是非常有益的。决策后状态是这里首选的建模选择。</p><h1 id="665f" class="mf mg it bd mh mi mj mk ml mm mn mo mp ki mq kj mr kl ms km mt ko mu kp mv mw bi translated">多臂土匪</h1><p id="0e11" class="pw-post-body-paragraph li lj it lk b ll mx kd ln lo my kg lq lr mz lt lu lv na lx ly lz nb mb mc md im bi translated">在RL教科书的例子中，<a class="ae lh" rel="noopener" target="_blank" href="/a-minimal-working-example-for-discrete-policy-gradients-in-tensorflow-2-0-d6a0d6b1a6d7">多臂匪徒</a>有点古怪，因为它本质上是一个<em class="me">无状态</em>问题。我们有<em class="me"> n </em>台老虎机，拉一下杠杆，我们又回到了同一套老虎机。如果没有问题状态，我们如何计算决策后状态？</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi nk"><img src="../Images/33cd621463fb7fbbd4a8c6fbd0d53ee5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*bS6vYsz2EtNnsIsa"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">由<a class="ae lh" href="https://unsplash.com/@carltraw?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">卡尔·劳</a>在<a class="ae lh" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="5d67" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">实际上，我现在不想讨论这个哲学问题。更有趣的是，没有直接奖励。只有在拉动杠杆后<em class="me">我们才能观察到<strong class="lk jd">随机回报</strong>，这意味着它被嵌入到外生变量<code class="fe nc nd ne nf b">ω</code>中。我们如何将它纳入决策后状态？</em></p><p id="d340" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">恐怕严格来说我们不能。如果后决策状态必须在状态空间的子集中<code class="fe nc nd ne nf b">S'⊆S</code>并且<code class="fe nc nd ne nf b">S</code>不存在——或者，充其量，它保持一个单一的、不变的状态——我们不能导出任何(有意义的)后决策状态。相比之下，国家-行动对出现在<code class="fe nc nd ne nf b">(s,a) ∈ S × A</code>中，这是我们<em class="me">可以</em>处理的事情。</p><p id="3ac9" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">在这种情况下，状态-动作对优于决策后状态。即使我们有实际的状态要处理，我们也必须记住区分奖励所采取的行动；我们不能简单地把所有的观察堆到一个单一的决策后状态。</p><p id="1dc4" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">正如您所看到的，选择最佳的建模方法并不总是微不足道的！</p></div><div class="ab cl nl nm hx nn" role="separator"><span class="no bw bk np nq nr"/><span class="no bw bk np nq nr"/><span class="no bw bk np nq"/></div><div class="im in io ip iq"><h1 id="06c2" class="mf mg it bd mh mi ns mk ml mm nt mo mp ki nu kj mr kl nv km mt ko nw kp mv mw bi translated">外卖食品</h1><ul class=""><li id="f26b" class="nx ny it lk b ll mx lo my lr nz lv oa lz ob md oc od oe of bi translated">在许多情况下，决策后状态和状态-动作对可以很容易地互换，但建模的含义可能是实质性的。</li><li id="7892" class="nx ny it lk b ll og lo oh lr oi lv oj lz ok md oc od oe of bi translated">如果如何达到给定状态并不重要，那么使用决策后状态。</li><li id="d169" class="nx ny it lk b ll og lo oh lr oi lv oj lz ok md oc od oe of bi translated">如果奖励是随机的，并且在选择行动时是未知的，那么使用状态-行动对。</li></ul><p id="5cb0" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">对更多背景感兴趣？查看我之前关于后决策状态的文章:</p><div class="ol om gp gr on oo"><a rel="noopener follow" target="_blank" href="/what-are-post-decision-states-and-what-do-they-want-from-us-9e02105b7f40"><div class="op ab fo"><div class="oq ab or cl cj os"><h2 class="bd jd gy z fp ot fr fs ou fu fw jc bi translated">什么是后决策状态？他们想从我们这里得到什么？</h2><div class="ov l"><p class="bd b dl z fp ot fr fs ou fu fw dk translated">towardsdatascience.com</p></div></div><div class="ow l"><div class="ox l oy oz pa ow pb lb oo"/></div></div></a></div><h2 id="8034" class="pc mg it bd mh pd pe dn ml pf pg dp mp lr ph pi mr lv pj pk mt lz pl pm mv iz bi translated">参考</h2><p id="58f0" class="pw-post-body-paragraph li lj it lk b ll mx kd ln lo my kg lq lr mz lt lu lv na lx ly lz nb mb mc md im bi translated">[1]鲍威尔，W. B. (2007)。<em class="me">近似动态规划:解决维数灾难</em>。约翰·威利&amp;的儿子们。</p></div></div>    
</body>
</html>