<html>
<head>
<title>Building a Residual Network with PyTorch</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用PyTorch构建剩余网络</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/building-a-residual-network-with-pytorch-df2f6937053b?source=collection_archive---------17-----------------------#2021-08-28">https://towardsdatascience.com/building-a-residual-network-with-pytorch-df2f6937053b?source=collection_archive---------17-----------------------#2021-08-28</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="e46a" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">当网络变得非常深的时候</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/2f428dc8952f28a3ad0be1e912664cdf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*g_XefR_YiUTfqjPOBxnCzg.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图片来自Unsplash。</p></figure><p id="08e5" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">自动驾驶、人脸检测和许多计算机应用的成功都归功于深度神经网络。然而，许多人可能没有意识到，计算机视觉进步的繁荣是由于一种特定类型的架构:残余网络。事实上，导致这个人工智能主导世界的最先进的结果只有在残余块的发明下才成为可能——这是一个简单而优雅的概念，导致了创建真正“深度”网络的飞跃。</p><p id="d35c" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">本文深入研究了残差网络背后的直觉和PyTorch中的实现，以训练ResNets执行图像分类任务。</p><h1 id="f394" class="lu lv it bd lw lx ly lz ma mb mc md me jz mf ka mg kc mh kd mi kf mj kg mk ml bi translated">在变‘深’之前，退化问题是什么？</h1><p id="8efd" class="pw-post-body-paragraph ky kz it la b lb mm ju ld le mn jx lg lh mo lj lk ll mp ln lo lp mq lr ls lt im bi translated">理论上，具有更多变量的更深的网络对于近似困难的任务(例如图像理解)是更好的函数。然而，经验测试表明，传统的深度网络更难训练，表现甚至比浅网络更差。我们称之为<em class="mr">退化</em>问题。</p><p id="5ec9" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这种现象是不直观的，因为假设我们有两个相同层数的网络，第二个网络在前面增加了<em class="mr"> x </em>层，最坏的情况应该是第一个<em class="mr"> x </em>层输出与原始输入相同的映射，因此具有相同的性能。</p><p id="7dae" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">据推测，较差的性能是由于与原始输入的相同映射在过程中被遗忘，因此在21世纪初，网络(如VGG-16)最多只有10-20层。</p><h1 id="110d" class="lu lv it bd lw lx ly lz ma mb mc md me jz mf ka mg kc mh kd mi kf mj kg mk ml bi translated"><strong class="ak">残留架构</strong></h1><p id="416c" class="pw-post-body-paragraph ky kz it la b lb mm ju ld le mn jx lg lh mo lj lk ll mp ln lo lp mq lr ls lt im bi translated">残差网络是一种简单直接的方法，通过创建一种称为跳过连接的快捷方式来馈送原始输入，并在网络的几个堆叠层之后将其与输出特征相结合，从而解决上述<em class="mr">退化</em>问题。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ms"><img src="../Images/1eda82004698e0dc2c3aad6e636c878a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6HDuqhUzP92iXhHoS0Wl3w.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图一。一个简单的剩余块。来源:<a class="ae mt" href="https://arxiv.org/abs/1512.03385" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1512.03385</a></p></figure><p id="f46a" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">形式上，如图1所示。，给定堆叠层的输入为<em class="mr"> x </em>，网络层为函数<em class="mr"> F </em>，输出<em class="mr"> y </em>如下:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mu"><img src="../Images/838604667bfb0df5b162f2a473f503a2.png" data-original-src="https://miro.medium.com/v2/resize:fit:208/1*M5dc10Uu6fOVrpM25ERMhw.gif"/></div></figure><p id="323d" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">当<em class="mr"> F(x) </em>和<em class="mr"> x </em>的尺寸不匹配时，可以简单地在跳接过程中进行线性投影来改变<em class="mr"> x </em>的尺寸。</p><p id="ae00" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们将上面的整个管道称为一个剩余块，我们可以有多个剩余块来构建一个更深的网络，而没有原来的<em class="mr">退化</em>问题。</p><h1 id="9c67" class="lu lv it bd lw lx ly lz ma mb mc md me jz mf ka mg kc mh kd mi kf mj kg mk ml bi translated">计算环境</h1><h2 id="1c84" class="mv lv it bd lw mw mx dn ma my mz dp me lh na nb mg ll nc nd mi lp ne nf mk ng bi translated">图书馆</h2><p id="8786" class="pw-post-body-paragraph ky kz it la b lb mm ju ld le mn jx lg lh mo lj lk ll mp ln lo lp mq lr ls lt im bi translated">整个程序是通过PyTorch库(包括torchvision)构建的。以下代码导入所有库:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nh ni l"/></div></figure><h2 id="74b5" class="mv lv it bd lw mw mx dn ma my mz dp me lh na nb mg ll nc nd mi lp ne nf mk ng bi translated">资料组</h2><p id="9a35" class="pw-post-body-paragraph ky kz it la b lb mm ju ld le mn jx lg lh mo lj lk ll mp ln lo lp mq lr ls lt im bi translated">为了展示残差网络的能力，我们在两个数据集上进行测试:更简单的<a class="ae mt" href="https://gas.graviti.com/dataset/hellodataset/MNIST/?utm_medium=0828Taying_2" rel="noopener ugc nofollow" target="_blank"> MNIST </a>数据集，包括6万张从0到9的手写数字图像，以及更复杂的<a class="ae mt" href="https://gas.graviti.com/dataset/graviti/CIFAR10?utm_medium=0828Taying_2" rel="noopener ugc nofollow" target="_blank"> CIFAR-10 </a>数据集。<br/> <br/>通常在测试过程中，人们可能会引用多个数据集，无论是出于研究目的还是为了查看哪个模型更通用。因此，当所有数据集被组织到一个平台中时，这是非常方便的。幸运的是，一家名为<a class="ae mt" href="https://graviti.com?utm_medium=0828Taying_2" rel="noopener ugc nofollow" target="_blank"> Graviti </a>的年轻初创公司提出在其平台上托管许多臭名昭著的数据集。人们可以简单地直接下载它们来执行进一步的训练和测试。</p><h2 id="3dd7" class="mv lv it bd lw mw mx dn ma my mz dp me lh na nb mg ll nc nd mi lp ne nf mk ng bi translated">硬件要求</h2><p id="512f" class="pw-post-body-paragraph ky kz it la b lb mm ju ld le mn jx lg lh mo lj lk ll mp ln lo lp mq lr ls lt im bi translated">最好在GPU上训练神经网络，因为它们可以显著提高训练速度。但是，如果只有CPU可用，您仍然可以测试程序。在我们的例子中，一个更简单的基于残差块的网络(只有几个网络)应该可以在两种类型的设备上运行，而更复杂的模型(如ResNet-152)将更适合在GPU上运行。要让您的程序自己决定硬件，只需使用以下代码:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nh ni l"/></div></figure><h1 id="eda8" class="lu lv it bd lw lx ly lz ma mb mc md me jz mf ka mg kc mh kd mi kf mj kg mk ml bi translated">构建剩余块</h1><p id="b7f3" class="pw-post-body-paragraph ky kz it la b lb mm ju ld le mn jx lg lh mo lj lk ll mp ln lo lp mq lr ls lt im bi translated">本节提供PyTorch教程，介绍在输入和输出维数相同的卷积神经网络上可以创建的最简单类型的残差块。</p><p id="f241" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">人们可以使用PyTorch <em class="mr"> nn来创建它。模块</em>如下:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nh ni l"/></div></figure><h1 id="358e" class="lu lv it bd lw lx ly lz ma mb mc md me jz mf ka mg kc mh kd mi kf mj kg mk ml bi translated">使用预先存在的ResNet模型</h1><p id="4a4b" class="pw-post-body-paragraph ky kz it la b lb mm ju ld le mn jx lg lh mo lj lk ll mp ln lo lp mq lr ls lt im bi translated">一些利用剩余架构的网络已经在像ImageNet这样的大数据集下被证明是成功的。Torchvision提供了网络的检查点和体系结构，如在其库中预构建的ResNet-34、ResNet-50和ResNet-152。人们可以通过以下方式简单地检索他们的模型:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nh ni l"/></div></figure><p id="0da0" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">但是，如果要将网络微调到非ImageNet的数据集，则更新ResNet的最终图层非常重要，因为最终的独热向量等于数据集的类数。</p><h1 id="033c" class="lu lv it bd lw lx ly lz ma mb mc md me jz mf ka mg kc mh kd mi kf mj kg mk ml bi translated">结果</h1><p id="e5ab" class="pw-post-body-paragraph ky kz it la b lb mm ju ld le mn jx lg lh mo lj lk ll mp ln lo lp mq lr ls lt im bi translated">我们对我们的网络进行了50个历元的训练，对于ResNet-34和ResNet-152，我们可以在MNIST数据集上轻松实现约99%的准确率，在CIFAR-10数据集上轻松实现90%的准确率。</p><p id="8bdb" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">基于何等人的原始论文的结果，我们还可以看到，残差架构在ImageNet数据集上的性能明显优于和具有相同层数但没有残差架构的网络。</p><p id="f0a2" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><em class="mr">这些结果可以直接从论文</em> <a class="ae mt" href="https://arxiv.org/abs/1512.03385" rel="noopener ugc nofollow" target="_blank"> <em class="mr">这里</em> </a> <em class="mr">中检索出来。</em></p><h1 id="e43f" class="lu lv it bd lw lx ly lz ma mb mc md me jz mf ka mg kc mh kd mi kf mj kg mk ml bi translated">结论</h1><p id="a8b8" class="pw-post-body-paragraph ky kz it la b lb mm ju ld le mn jx lg lh mo lj lk ll mp ln lo lp mq lr ls lt im bi translated">由何等人创建的剩余结构可以说是近年来计算机视觉神经网络发展中最伟大的发明之一。今天几乎所有的网络，甚至是卷积网络之外的网络，都有类似于更好更深的网络的概念。</p><p id="c088" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这种简单而优雅的方法创造了无数的可能性，推动了机器对人类世界理解的前沿。</p><p id="3b2c" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><em class="mr">感谢您坚持到现在</em>🙏<em class="mr">！</em> <em class="mr">我会在计算机视觉/深度学习的不同领域发布更多内容。一定要看看我关于计算机视觉方法的其他文章！如果你对Graviti平台感兴趣，也可以随时加入</em> <a class="ae mt" href="https://discord.gg/sF9zQTbB" rel="noopener ugc nofollow" target="_blank"> <em class="mr">不和谐</em> </a> <em class="mr">频道！</em></p></div></div>    
</body>
</html>