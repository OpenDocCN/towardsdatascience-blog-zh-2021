<html>
<head>
<title>Evaluating All Possible Combinations of Hyperparameters -Grid Search-</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">评估所有可能的超参数组合-网格搜索-</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/evaluating-all-possible-combinations-of-hyperparameters-grid-search-e41c7044e8e?source=collection_archive---------21-----------------------#2021-08-08">https://towardsdatascience.com/evaluating-all-possible-combinations-of-hyperparameters-grid-search-e41c7044e8e?source=collection_archive---------21-----------------------#2021-08-08</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="7289" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">一个视图中有/没有Sklearn库的超参数组合</h2></div><pre class="ki kj kk kl gt km kn ko kp aw kq bi"><span id="2948" class="kr ks it kn b gy kt ku l kv kw"><strong class="kn iu"><em class="kx">Table of Contents</em></strong><em class="kx"> <br/></em><strong class="kn iu">1. Introduction<br/>2. Grid Search without Sklearn Library<br/>3. Grid Search with Sklearn Library<br/>4. Grid Search with Validation Dataset<br/>5. Grid Search with Cross-Validation<br/>6. Customized Grid Search<br/>7. Different Cross-Validation types in Grid Search<br/>8. Nested Cross-Validation<br/>9. Summary</strong></span></pre><figure class="ki kj kk kl gt kz gh gi paragraph-image"><div role="button" tabindex="0" class="la lb di lc bf ld"><div class="gh gi ky"><img src="../Images/490633a989edce105a9c7c1f9ba4f4ac.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*eMj-XqGOm6vXdgot"/></div></div><p class="lg lh gj gh gi li lj bd b be z dk translated">在<a class="ae lk" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上<a class="ae lk" href="https://unsplash.com/@tekton_tools?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Tekton </a>拍摄的照片</p></figure><h1 id="a052" class="ll ks it bd lm ln lo lp lq lr ls lt lu jz lv ka lw kc lx kd ly kf lz kg ma mb bi translated">1.介绍</h1><p id="9875" class="pw-post-body-paragraph mc md it me b mf mg ju mh mi mj jx mk ml mm mn mo mp mq mr ms mt mu mv mw mx im bi translated">每个项目的模型和预处理都是独立的。超参数根据数据集进行调整，对每个项目使用相同的超参数会影响结果的准确性。例如，在逻辑回归算法中有不同的超参数，如<em class="kx">、【求解器】、【C】、【惩罚】</em>，这些超参数的不同组合会给出不同的结果。类似地，支持向量机也有可调参数，如gamma值、C值，它们的组合也会给出不同的结果。这些算法的超参数可在sklearn网站上获得。开发人员的目标是设计一个具有概化和高精度的模型，因此，检测超参数的最佳组合对于提高精度非常重要。</p><p id="ada3" class="pw-post-body-paragraph mc md it me b mf my ju mh mi mz jx mk ml na mn mo mp nb mr ms mt nc mv mw mx im bi translated">本文涉及评估超表的所有组合，以提高模型的准确性和结果准确性的可靠性。</p><h1 id="2b0f" class="ll ks it bd lm ln lo lp lq lr ls lt lu jz lv ka lw kc lx kd ly kf lz kg ma mb bi translated">2.没有Sklearn库的网格搜索</h1><p id="6775" class="pw-post-body-paragraph mc md it me b mf mg ju mh mi mj jx mk ml mm mn mo mp mq mr ms mt mu mv mw mx im bi translated">用户要求评估的组合通过Sklearn库中的<em class="kx"> GridSearchCV </em>进行测试。事实上，该模型适合每一个单独的组合，揭示了最好的结果和参数。例如，当我们考虑LogisticRegression时，如果为<em class="kx"> C </em>选择4个不同的值，为<em class="kx">惩罚</em>选择2个不同的值，则该模型将拟合8次，并且每次的结果将表示。现在让我们在不使用sklearn库的情况下在癌症数据集上创建网格搜索:</p><pre class="ki kj kk kl gt km kn ko kp aw kq bi"><span id="3b18" class="kr ks it kn b gy kt ku l kv kw">IN[1]<br/>cancer=load_breast_cancer()<br/>cancer_data   =cancer.data<br/>cancer_target =cancer.target</span><span id="58f2" class="kr ks it kn b gy nd ku l kv kw">IN[2]<br/>x_train,x_test,y_train,y_test=train_test_split(cancer_data,cancer_target,test_size=0.2,random_state=2021)</span><span id="6f03" class="kr ks it kn b gy nd ku l kv kw">best_lr=0<br/>for C in [0.001,0.1,1,10]:<br/>    for penalty in ['l1','l2']:<br/>        lr=LogisticRegression(solver='saga',C=C,penalty=penalty)<br/>        lr.fit(x_train,y_train)<br/>        lr_score=lr.score(x_test,y_test)<br/>        print("C: ",C,"penalty:",penalty,'acc  {:.3f}'.format(lr_score))<br/>        if lr_score&gt;best_lr:<br/>            best_lr=lr_score<br/>            best_lr_combination=(C,penalty)</span><span id="b834" class="kr ks it kn b gy nd ku l kv kw">print("best score LogisticRegression",best_lr)<br/>print("C and penalty",best_lr_combination)<br/><strong class="kn iu">OUT[2]<br/>C:  0.001 penalty: l1 acc:0.912<br/>C:  0.001 penalty: l2 acc:0.895<br/>C:  0.1 penalty: l1 acc:0.904<br/>C:  0.1 penalty: l2 acc:0.904<br/>C:  1 penalty: l1 acc:0.895<br/>C:  1 penalty: l2 acc:0.904<br/>C:  10 penalty: l1 acc:0.904<br/>C:  10 penalty: l2 acc:0.904<br/>best score LogisticRegression 0.9122807017543859<br/>C and penalty (0.001, 'l1')</strong></span></pre><blockquote class="ne nf ng"><p id="f755" class="mc md kx me b mf my ju mh mi mz jx mk nh na mn mo ni nb mr ms nj nc mv mw mx im bi translated">逻辑回归的所有超参数和更多参数可通过此<a class="ae lk" href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html" rel="noopener ugc nofollow" target="_blank">链接</a>访问。</p></blockquote><p id="6723" class="pw-post-body-paragraph mc md it me b mf my ju mh mi mz jx mk ml na mn mo mp nb mr ms mt nc mv mw mx im bi translated">可以看出，为每个组合创建了准确度值。开发人员可以通过选择超参数的最佳组合来提高模型的准确性。OUT[2]表示超参数的最佳组合是C=0.001，罚值=L1。</p><p id="f01b" class="pw-post-body-paragraph mc md it me b mf my ju mh mi mz jx mk ml na mn mo mp nb mr ms mt nc mv mw mx im bi translated">让我们使用支持向量分类器和决策树分类器创建相同的过程。</p><pre class="ki kj kk kl gt km kn ko kp aw kq bi"><span id="e53e" class="kr ks it kn b gy kt ku l kv kw">IN[3]<br/>#SVC<br/>best_svc=0<br/>for gamma in [0.001,0.1,1,100]:<br/>    for C in[0.01,0.1,1,100]:<br/>        svm=SVC(gamma=gamma,C=C)<br/>        svm.fit(x_train,y_train)<br/>        score=svm.score(x_test,y_test)<br/>        #print("gamma:",gamma,"C:",C,"acc",score)<br/>        if score&gt;best_svc:<br/>            best_svc=score<br/>            best_svc_combination=(gamma, C)</span><span id="d45a" class="kr ks it kn b gy nd ku l kv kw">print("best score SVM",best_svc)<br/>print("gamma and C",best_svc_combination)<br/><strong class="kn iu">OUT[3]<br/>best score SVM 0.9210526315789473<br/>gamma and C (0.001, 100)</strong></span><span id="a971" class="kr ks it kn b gy nd ku l kv kw">IN[4]<br/>#DT<br/>best_dt=0<br/>for max_depth in [1,2,3,5,7,9,11,13,15]:<br/>    dt = DecisionTreeClassifier(max_depth=max_depth, random_state=2021)<br/>    dt.fit(x_train,y_train)<br/>    dt_score=dt.score(x_test,y_test)<br/>    #print("max_depth:",max_depth,dt_score)<br/>    if dt_score&gt;best_dt:<br/>        best_dt=dt_score<br/>        best_dt_depth=(max_depth)<br/>        <br/>print("best dt_score:",best_dt)<br/>print("best dt depth:", best_dt_depth)<br/><strong class="kn iu">OUT[4]<br/>best dt_score: 0.9473684210526315<br/>best dt depth: 3</strong></span></pre><blockquote class="ne nf ng"><p id="16c3" class="mc md kx me b mf my ju mh mi mz jx mk nh na mn mo ni nb mr ms nj nc mv mw mx im bi translated">决策树分类器的所有超参数和更多参数可以从这个<a class="ae lk" href="https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html" rel="noopener ugc nofollow" target="_blank">链接</a>访问，支持向量分类器可以从这个<a class="ae lk" href="https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html" rel="noopener ugc nofollow" target="_blank">链接</a>访问。</p></blockquote><p id="7b6f" class="pw-post-body-paragraph mc md it me b mf my ju mh mi mz jx mk ml na mn mo mp nb mr ms mt nc mv mw mx im bi translated">OUT[3]表示SVC的最佳组合是gamma=0.001，C=100。OUT[4]表示DTC的最佳组合是max_depth=3。</p><h1 id="ff2e" class="ll ks it bd lm ln lo lp lq lr ls lt lu jz lv ka lw kc lx kd ly kf lz kg ma mb bi translated">3.使用Sklearn库进行网格搜索</h1><p id="1f0a" class="pw-post-body-paragraph mc md it me b mf mg ju mh mi mj jx mk ml mm mn mo mp mq mr ms mt mu mv mw mx im bi translated">让我们使用sklearn库做同样的事情:</p><pre class="ki kj kk kl gt km kn ko kp aw kq bi"><span id="cfa9" class="kr ks it kn b gy kt ku l kv kw">IN[5]<br/>param_grid_lr = {'C': [0.001,0.1,1,10],'penalty': ['l1','l2']}</span><span id="cb9e" class="kr ks it kn b gy nd ku l kv kw">gs_lr=GridSearchCV(LogisticRegression(solver='saga'),param_grid_lr)</span><span id="8ca6" class="kr ks it kn b gy nd ku l kv kw">x_train,x_test,y_train,y_test=train_test_split(cancer_data,<br/>cancer_target,test_size=0.2,random_state=2021)</span><span id="8bd2" class="kr ks it kn b gy nd ku l kv kw">gs_lr.fit(x_train,y_train)<br/>test_score=gs_lr.score(x_test,y_test)<br/>print("test score:",test_score)<br/>print("best combination: ",gs_lr.best_params_)<br/>print("best score: ", gs_lr.best_score_)<br/>print("best all parameters:",gs_lr.best_estimator_)<br/>print("everything ",gs_lr.cv_results_)<br/><strong class="kn iu">OUT[5]<br/>test score: 0.9122807017543859<br/>best combination:  {'C': 0.001, 'penalty': 'l1'}<br/>best score:  0.9054945054945055<br/>best all parameters: LogisticRegression(C=0.001, penalty='l1', solver='saga')</strong></span></pre><p id="1b1d" class="pw-post-body-paragraph mc md it me b mf my ju mh mi mz jx mk ml na mn mo mp nb mr ms mt nc mv mw mx im bi translated">如上所述，使用<em class="kx"> train_test_split </em>分割数据集。通过使用<em class="kx"> GridSearchCV </em>，使用具有各种超参数组合的逻辑回归算法对训练数据集进行了训练。可以看出，准确率和最佳参数同上。<em class="kx"> GridSearchCV </em>有很多属性，所有这些都可以在sklearn <a class="ae lk" href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html" rel="noopener ugc nofollow" target="_blank">网站</a>上找到。</p><h1 id="0164" class="ll ks it bd lm ln lo lp lq lr ls lt lu jz lv ka lw kc lx kd ly kf lz kg ma mb bi translated">4.使用验证数据集进行网格搜索</h1><p id="4a43" class="pw-post-body-paragraph mc md it me b mf mg ju mh mi mj jx mk ml mm mn mo mp mq mr ms mt mu mv mw mx im bi translated">在以前的研究中，数据被分为测试集和训练集。用所有组合尝试训练数据集，并将最高比率应用于测试数据集。不过在这个<a class="ae lk" rel="noopener" target="_blank" href="/increasing-model-reliability-model-selection-cross-validation-1ce0bf506cd">环节</a>中说明了，用random拆分<em class="kx"> train_test_split </em>是一种赌博，不一定能给出可靠的结果。现在，在将数据分为训练集和测试集以增加可靠性之后，让我们将训练数据集分为训练集和验证集。让我们用训练数据集训练模型，用验证数据进行评估，在为模型确定最合适的超参数后，将其应用于最初分配的测试数据集。即使精度值较低，模型也会更一般化。这比假的高精度更可取。</p><pre class="ki kj kk kl gt km kn ko kp aw kq bi"><span id="34ef" class="kr ks it kn b gy kt ku l kv kw">IN[6]<br/>x_valtrain,x_test,y_valtrain,y_test=train_test_split(cancer_data,<br/>cancer_target,test_size=0.2,random_state=2021)</span><span id="2cfb" class="kr ks it kn b gy nd ku l kv kw">x_train,x_val,y_train,y_val=train_test_split(x_valtrain,y_valtrain,<br/>test_size=0.2,random_state=2021)</span><span id="7869" class="kr ks it kn b gy nd ku l kv kw">param_grid_lr = {'C': [0.001,0.1,1,10],'penalty': ['l1','l2']}<br/>gs_lr=GridSearchCV(LogisticRegression(solver='saga'),param_grid_lr)<br/>gs_lr.fit(x_train,y_train)<br/>val_score=gs_lr.score(x_val,y_val)</span><span id="4083" class="kr ks it kn b gy nd ku l kv kw">print("val score:",val_score)<br/>print("best parameters: ",gs_lr.best_params_)<br/>print("best score: ", gs_lr.best_score_)</span><span id="d6c2" class="kr ks it kn b gy nd ku l kv kw">new_lr=LogisticRegression(solver='saga', C=0.001, penalty='l2').fit(x_valtrain,y_valtrain)<br/>test_score=new_lr.score(x_test,y_test)<br/>print("test score", test_score)<br/><strong class="kn iu">OUT[6]<br/>val score: 0.9010989010989011<br/>best parameters:  {'C': 0.001, 'penalty': 'l2'}<br/>best score:  0.9092465753424659<br/>test score 0.9035087719298246</strong></span></pre><p id="f0fc" class="pw-post-body-paragraph mc md it me b mf my ju mh mi mz jx mk ml na mn mo mp nb mr ms mt nc mv mw mx im bi translated">用训练数据集(x_train，y_train)对其进行训练，用验证数据集(x_val，y_val)对其进行评估，并确定最佳组合。然后，使用训练数据集+验证数据集的最佳组合创建新模型，使用更多数据，最后，使用在第一次分裂中分配的测试数据集对其进行评估。</p><p id="d87a" class="pw-post-body-paragraph mc md it me b mf my ju mh mi mz jx mk ml na mn mo mp nb mr ms mt nc mv mw mx im bi translated">同样的过程可以不使用sklearn或者按照上面的模板进行其他算法。</p><figure class="ki kj kk kl gt kz gh gi paragraph-image"><div role="button" tabindex="0" class="la lb di lc bf ld"><div class="gh gi nk"><img src="../Images/8b896c2cf43593942ad6a0c41dc3fbf4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*5W3L3QbFEfl21jz3"/></div></div><p class="lg lh gj gh gi li lj bd b be z dk translated"><a class="ae lk" href="https://unsplash.com/@jeshoots?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">JESHOOTS.COM</a>在<a class="ae lk" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍照</p></figure><h1 id="0f60" class="ll ks it bd lm ln lo lp lq lr ls lt lu jz lv ka lw kc lx kd ly kf lz kg ma mb bi translated">5.交叉验证网格搜索</h1><p id="3146" class="pw-post-body-paragraph mc md it me b mf mg ju mh mi mj jx mk ml mm mn mo mp mq mr ms mt mu mv mw mx im bi translated">数据集分为训练集和测试集。通过交叉验证将训练数据集分离为训练集+验证集。让我们在不使用sklearn库来理解系统的情况下实现它:</p><pre class="ki kj kk kl gt km kn ko kp aw kq bi"><span id="f9b3" class="kr ks it kn b gy kt ku l kv kw">IN[7]<br/>x_valtrain,x_test,y_valtrain,y_test=train_test_split(cancer_data,<br/>cancer_target,test_size=0.2,random_state=2021)</span><span id="9a72" class="kr ks it kn b gy nd ku l kv kw">best_lr=0<br/>for C in [0.001,0.1,1,10]:<br/>    for penalty in ['l1','l2']:<br/>        lr=LogisticRegression(solver='saga',C=C,penalty=penalty)<br/>        cv_scores=cross_val_score(lr,x_valtrain,y_valtrain,cv=5)<br/>        mean_score=np.mean(cv_scores)<br/>        if mean_score&gt;best_lr:<br/>            best_lr=mean_score<br/>            best_lr_parameters=(C,penalty)</span><span id="b3ad" class="kr ks it kn b gy nd ku l kv kw">print("best score LogisticRegression",best_lr)<br/>print("C and penalty",best_lr_parameters)<br/>print("**************************************")</span><span id="89c1" class="kr ks it kn b gy nd ku l kv kw">new_cv_lr=LogisticRegression(solver='saga',C=0.001,penalty='l1').fit(x_valtrain,y_valtrain)<br/>new_cv_score=new_cv_lr.score(x_test,y_test)<br/>print('test accuracy:',new_cv_score)<br/><strong class="kn iu">OUT[7]<br/>best score LogisticRegression 0.9054945054945055<br/>C and penalty (0.001, 'l1')<br/>**************************************<br/>test accuracy: 0.9122807017543859</strong></span></pre><p id="a712" class="pw-post-body-paragraph mc md it me b mf my ju mh mi mz jx mk ml na mn mo mp nb mr ms mt nc mv mw mx im bi translated">将<em class="kx"> x_valtrain </em>(训练+验证)数据集分割为CV=5的值，并将最初分配的测试数据应用于具有最佳参数的重建模型。</p><p id="2fef" class="pw-post-body-paragraph mc md it me b mf my ju mh mi mz jx mk ml na mn mo mp nb mr ms mt nc mv mw mx im bi translated">同样的过程可以应用于sklearn库:</p><pre class="ki kj kk kl gt km kn ko kp aw kq bi"><span id="be6c" class="kr ks it kn b gy kt ku l kv kw">IN[8]<br/>param_grid_lr = {'C': [0.001,0.1,1,10,100],'penalty': ['l1','l2']}</span><span id="a4a4" class="kr ks it kn b gy nd ku l kv kw">gs_lr=GridSearchCV(LogisticRegression(solver='saga'),param_grid_lr,<br/>cv=5)</span><span id="4bea" class="kr ks it kn b gy nd ku l kv kw">x_valtrain,x_test,y_valtrain,y_test=train_test_split(cancer_data,<br/>cancer_target,test_size=0.2,random_state=2021)</span><span id="8f76" class="kr ks it kn b gy nd ku l kv kw">gs_lr.fit(x_valtrain,y_valtrain)<br/>gs_lr_score=gs_lr.score(x_test,y_test)<br/>print('test acc:',gs_lr_score)<br/>print("best parameters: ",gs_lr.best_params_)<br/>print("best score: ", gs_lr.best_score_)<br/><strong class="kn iu">OUT[8]<br/>test acc: 0.9122807017543859<br/>best parameters:  {'C': 0.001, 'penalty': 'l1'}<br/>best score:  0.9054945054945055<br/>best all parameters LogisticRegression(C=0.001, penalty='l1', solver='saga')</strong></span></pre><p id="68d0" class="pw-post-body-paragraph mc md it me b mf my ju mh mi mz jx mk ml na mn mo mp nb mr ms mt nc mv mw mx im bi translated">可以看出，获得了相同的结果和相同的最佳参数。</p><h1 id="b7a0" class="ll ks it bd lm ln lo lp lq lr ls lt lu jz lv ka lw kc lx kd ly kf lz kg ma mb bi translated">6.定制网格搜索</h1><p id="f06f" class="pw-post-body-paragraph mc md it me b mf mg ju mh mi mj jx mk ml mm mn mo mp mq mr ms mt mu mv mw mx im bi translated">只要允许，参数组合是可能的。有些参数不能相互组合。例如，当<em class="kx">解算器在LogisticRegression中选择了:【saga】</em>时，可以应用<em class="kx">【L1】</em><em class="kx">【L2】</em><em class="kx">【elastic net】</em>，但对于解算器:<em class="kx">【lbfgs】</em>，只能应用<em class="kx">【L2】</em>(或<em class="kx">【无】</em>)。使用GridSearch可以克服这个缺点，如下所示:</p><pre class="ki kj kk kl gt km kn ko kp aw kq bi"><span id="b416" class="kr ks it kn b gy kt ku l kv kw">IN[9]<br/>x_valtrain,x_test,y_valtrain,y_test=train_test_split(cancer_data,<br/>cancer_target,test_size=0.2,random_state=2021)</span><span id="1f43" class="kr ks it kn b gy nd ku l kv kw">param_grid_lr=[{'solver':['saga'],'C':[0.1,1,10],'penalty':['elasticnet','l1','l2']},<br/>               {'solver':['lbfgs'],'C':[0.1,1,10],'penalty':['l2']}]</span><span id="9715" class="kr ks it kn b gy nd ku l kv kw">gs_lr = GridSearchCV(LogisticRegression(),param_grid_lr,cv=5)<br/>gs_lr.fit(x_valtrain,y_valtrain)<br/>gs_lr_score=gs_lr.score(x_test,y_test)<br/>print("test score:",gs_lr_score)<br/>print("best parameters: ",gs_lr.best_params_)<br/>print("best score: ", gs_lr.best_score_)<br/><strong class="kn iu">OUT[9]<br/>test score: 0.9210526315789473<br/>best parameters:  {'C': 1, 'penalty': 'l2', 'solver': 'lbfgs'}<br/>best score:  0.9516483516483516</strong></span></pre><p id="ff57" class="pw-post-body-paragraph mc md it me b mf my ju mh mi mz jx mk ml na mn mo mp nb mr ms mt nc mv mw mx im bi translated">根据选定的最佳参数创建新模型，并通过<em class="kx"> GridSearchCV </em>应用测试数据。</p><h1 id="ef36" class="ll ks it bd lm ln lo lp lq lr ls lt lu jz lv ka lw kc lx kd ly kf lz kg ma mb bi translated">7.网格搜索中的不同交叉验证类型</h1><p id="c2d2" class="pw-post-body-paragraph mc md it me b mf mg ju mh mi mj jx mk ml mm mn mo mp mq mr ms mt mu mv mw mx im bi translated">到目前为止，交叉验证已经被实现为k-fold，但是也可以应用不同的交叉验证方法:</p><pre class="ki kj kk kl gt km kn ko kp aw kq bi"><span id="706b" class="kr ks it kn b gy kt ku l kv kw">IN[10]<br/>x_valtrain,x_test,y_valtrain,y_test=train_test_split(cancer_data,<br/>cancer_target,test_size=0.2,random_state=2021)</span><span id="7dec" class="kr ks it kn b gy nd ku l kv kw">param_grid_lr=[{'solver':['saga'],'C':[0.1,1,10],'penalty':['elasticnet','l1','l2']},<br/>               {'solver':['lbfgs'],'C':[0.1,1,10],'penalty':['l2']}]</span><span id="00a2" class="kr ks it kn b gy nd ku l kv kw">IN[11]<br/>gs_lr_loo = GridSearchCV(LogisticRegression(),param_grid_lr,cv=LeaveOneOut())<br/>gs_lr_loo.fit(x_valtrain,y_valtrain)<br/>gs_lr_loo_score=gs_lr_loo.score(x_test,y_test)</span><span id="875f" class="kr ks it kn b gy nd ku l kv kw">print("loo-test score:",gs_lr_loo_score)<br/>print("loo-best parameters: ",gs_lr_loo.best_params_)<br/>print("**********************************************")<br/><strong class="kn iu">OUT[11]<br/>loo-test score: 0.9122807017543859<br/>loo-best parameters:  {'C': 0.1, 'penalty': 'l2', 'solver': 'lbfgs'}<br/>**********************************************</strong></span><span id="ceb2" class="kr ks it kn b gy nd ku l kv kw">IN[12]<br/>skf = StratifiedKFold(n_splits=5)<br/>gs_lr_skf = GridSearchCV(LogisticRegression(),param_grid_lr,cv=skf)<br/>gs_lr_skf.fit(x_valtrain,y_valtrain)<br/>gs_lr_skf_score=gs_lr_skf.score(x_test,y_test)</span><span id="3360" class="kr ks it kn b gy nd ku l kv kw">print("skf-test score:",gs_lr_skf_score)<br/>print("skf-best parameters: ",gs_lr_skf.best_params_)<br/>print("**********************************************")<br/><strong class="kn iu">OUT[12]<br/>skf-test score: 0.9210526315789473<br/>skf-best parameters:  {'C': 1, 'penalty': 'l2', 'solver': 'lbfgs'}<br/>**********************************************</strong></span><span id="598a" class="kr ks it kn b gy nd ku l kv kw">IN[13]<br/>rkf = RepeatedKFold(n_splits=5, n_repeats=5, random_state=2021)<br/>gs_lr_rkf= GridSearchCV(LogisticRegression(),param_grid_lr,cv=rkf)<br/>gs_lr_rkf.fit(x_valtrain,y_valtrain)<br/>gs_lr_rkf_score=gs_lr_rkf.score(x_test,y_test)</span><span id="5743" class="kr ks it kn b gy nd ku l kv kw">print("rkf-test score:",gs_lr_rkf_score)<br/>print("rkf-best parameters: ",gs_lr_rkf.best_params_)<br/>print("**********************************************")<br/><strong class="kn iu">OUT[13]<br/>rkf-test score: 0.9298245614035088<br/>rkf-best parameters:  {'C': 10, 'penalty': 'l2', 'solver': 'lbfgs'}<br/>**********************************************</strong></span></pre><p id="dbf3" class="pw-post-body-paragraph mc md it me b mf my ju mh mi mz jx mk ml na mn mo mp nb mr ms mt nc mv mw mx im bi translated">当<em class="kx">用<em class="kx"> C=10 </em>和<em class="kx">罚值= L2</em>重复折叠</em>获得最高精度值时，确定精度值与之接近的所有其他结果的C值是不同的。</p><h1 id="7d4c" class="ll ks it bd lm ln lo lp lq lr ls lt lu jz lv ka lw kc lx kd ly kf lz kg ma mb bi translated">8.嵌套交叉验证</h1><p id="b0ec" class="pw-post-body-paragraph mc md it me b mf mg ju mh mi mj jx mk ml mm mn mo mp mq mr ms mt mu mv mw mx im bi translated">到目前为止，测试数据已经被<em class="kx"> train_test_split </em>分离，训练数据已经被分离成一个训练集和一个交叉验证的验证集。为了进一步推广这种方法，我们还可以通过交叉验证来拆分测试数据:</p><pre class="ki kj kk kl gt km kn ko kp aw kq bi"><span id="bd8f" class="kr ks it kn b gy kt ku l kv kw">IN[14]<br/>param_grid_lr=[{'solver':['saga'],'C':[0.1,1,10],'penalty':['elasticnet','l1','l2']},<br/>               {'solver':['lbfgs'],'C':[0.1,1,10],'penalty':['l2']}]</span><span id="1236" class="kr ks it kn b gy nd ku l kv kw">gs=GridSearchCV(LogisticRegression(),param_grid_lr,cv=5)<br/>nested_scores=cross_val_score(gs,cancer.data,cancer.target,cv=5)</span><span id="da15" class="kr ks it kn b gy nd ku l kv kw">print("nested acc",nested_scores)<br/>print("Average acc: ", nested_scores.mean())<br/><strong class="kn iu">OUT[14]<br/>nested acc [0.94736842 0.93859649 0.94736842 0.9122807  0.92920354]<br/>Average acc:  0.9349635149821456</strong></span></pre><p id="8164" class="pw-post-body-paragraph mc md it me b mf my ju mh mi mz jx mk ml na mn mo mp nb mr ms mt nc mv mw mx im bi translated">对于<em class="kx">解算器='saga' </em>，有3x3个组合，对于<em class="kx">解算器='lbfgs' </em>，有3x1个组合。模型在内部交叉验证中拟合了5次，在外部交叉验证中拟合了5次。</p><p id="df1d" class="pw-post-body-paragraph mc md it me b mf my ju mh mi mz jx mk ml na mn mo mp nb mr ms mt nc mv mw mx im bi translated">所以，模型的总拟合数为9x5x5 + 3x5x5 = 300。</p><h1 id="463d" class="ll ks it bd lm ln lo lp lq lr ls lt lu jz lv ka lw kc lx kd ly kf lz kg ma mb bi translated">9.摘要</h1><figure class="ki kj kk kl gt kz gh gi paragraph-image"><div role="button" tabindex="0" class="la lb di lc bf ld"><div class="gh gi nl"><img src="../Images/333256e784891b0b783a9afb825edd73.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tZHW6V_BtA8CBLps5kWFhQ.png"/></div></div><p class="lg lh gj gh gi li lj bd b be z dk translated">图一。标题摘要，按作者分类的图像</p></figure><blockquote class="ne nf ng"><p id="489d" class="mc md kx me b mf my ju mh mi mz jx mk nh na mn mo ni nb mr ms nj nc mv mw mx im bi translated">网格搜索和交叉验证的缺点是需要很长时间来拟合几十个模型。n_jobs值可以由用户设置，并且可以分配要使用的CPU内核数量。如果设置了n_jobs=-1，则使用所有可用的CPU核心。</p></blockquote><h2 id="b227" class="kr ks it bd lm nm nn dn lq no np dp lu ml nq nr lw mp ns nt ly mt nu nv ma nw bi translated">回到指引点击<a class="ae lk" href="https://ibrahimkovan.medium.com/machine-learning-guideline-959da5c6f73d" rel="noopener">此处</a>。</h2><div class="nx ny gp gr nz oa"><a href="https://ibrahimkovan.medium.com/machine-learning-guideline-959da5c6f73d" rel="noopener follow" target="_blank"><div class="ob ab fo"><div class="oc ab od cl cj oe"><h2 class="bd iu gy z fp of fr fs og fu fw is bi translated">机器学习指南</h2><div class="oh l"><h3 class="bd b gy z fp of fr fs og fu fw dk translated">本文旨在准备一个机器学习数据库，以便在一个视图中显示所有的机器学习标题。这个…</h3></div><div class="oi l"><p class="bd b dl z fp of fr fs og fu fw dk translated">ibrahimkovan.medium.com</p></div></div><div class="oj l"><div class="ok l ol om on oj oo le oa"/></div></div></a></div></div></div>    
</body>
</html>