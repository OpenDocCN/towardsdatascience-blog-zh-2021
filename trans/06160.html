<html>
<head>
<title>ML From Scratch: Linear, Polynomial, and Regularized Regression Models</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">ML从零开始:线性、多项式和正则化回归模型</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/ml-from-scratch-linear-polynomial-and-regularized-regression-models-725672336076?source=collection_archive---------15-----------------------#2021-06-02">https://towardsdatascience.com/ml-from-scratch-linear-polynomial-and-regularized-regression-models-725672336076?source=collection_archive---------15-----------------------#2021-06-02</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="a67d" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">通过在python中从头实现线性模型，深入了解回归中使用的线性模型。</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/94b724dc920a1fbc0d8265d53e539a19.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*l9fGdY2RLGRsIYwb_P1_jg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">封面照片——卢克·纽曼拍摄</p></figure><p id="4959" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在这个新的系列中，我通过编写干净的、可重用的、有良好文档记录的带有测试用例的代码来提高我的编码技能和习惯。这是我以面向对象的方式从头开始实现线性、多项式、脊形、套索和ElasticNet回归的系列文章的第一部分。</p><p id="9abc" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们将从一个简单的LinearRegression类开始，然后在它的基础上以类似于Scikit-Learn的简单风格创建一个完整的线性模型模块。我的实现绝不是最优的解决方案，只是为了增加我们对机器学习的理解。</p><p id="8929" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在这个库中，你会找到这个博客中的所有代码，包括每个类和函数的测试用例。</p><p id="d1b4" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">要查看Github资源库，请访问<a class="ae lu" href="https://github.com/lukenew2/mlscratch" rel="noopener ugc nofollow" target="_blank">此处</a>。</p><h2 id="39e2" class="lv lw it bd lx ly lz dn ma mb mc dp md lh me mf mg ll mh mi mj lp mk ml mm mn bi translated">装置</h2><p id="271c" class="pw-post-body-paragraph ky kz it la b lb mo ju ld le mp jx lg lh mq lj lk ll mr ln lo lp ms lr ls lt im bi translated">为了使用类和函数进行测试，创建一个虚拟环境并pip安装项目。</p><pre class="kj kk kl km gt mt mu mv mw aw mx bi"><span id="d3df" class="lv lw it mu b gy my mz l na nb">    $ pip install mlscratch==0.0.1</span></pre><p id="3942" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">要从Github下载本地存储库中的所有源代码，请创建一个虚拟环境，并在您的终端中运行以下命令。</p><pre class="kj kk kl km gt mt mu mv mw aw mx bi"><span id="9d62" class="lv lw it mu b gy my mz l na nb">    $ git clone https://github.com/lukenew2/mlscratch<br/>    $ cd mlscratch<br/>    $ python setup.py install</span></pre><h1 id="0f1c" class="nc lw it bd lx nd ne nf ma ng nh ni md jz nj ka mg kc nk kd mj kf nl kg mm nm bi translated">线性回归</h1><p id="927b" class="pw-post-body-paragraph ky kz it la b lb mo ju ld le mp jx lg lh mq lj lk ll mr ln lo lp ms lr ls lt im bi translated">通常，任何人在他们的数据科学之旅中学习的第一个模型是<em class="nn">线性回归</em>。简单地说，线性回归模型通过计算每个自变量的参数权重加上一个称为<em class="nn">偏差项</em>(也称为<em class="nn">截距项</em>)的常数，来表示因变量标量变量<strong class="la iu"> y </strong>和自变量<strong class="la iu"> X </strong>之间的关系。</p><p id="41ff" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们通过将特征权重向量<strong class="la iu">θ、</strong>乘以独立变量<strong class="la iu"> X </strong>来进行预测。</p><p id="3476" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">就是这样！这就是线性回归模型的全部内容。让我们看看我们是如何训练它的。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/5320d3839244c6c618caf712ead55758.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dL94dVVamfEruO4V5yq2kQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图一。线性回归——卢克·纽曼</p></figure><p id="c14c" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">训练模型意味着找到最适合训练数据集的参数。为此，我们需要能够衡量模型与数据的吻合程度。对于线性回归，我们找到最小化均方误差(MSE)的<strong class="la iu">θ</strong>的值。有多种优化算法可以做到这一点，所以我们来看几个。</p><h2 id="5ee4" class="lv lw it bd lx ly lz dn ma mb mc dp md lh me mf mg ll mh mi mj lp mk ml mm mn bi translated">普通最小二乘法</h2><p id="d344" class="pw-post-body-paragraph ky kz it la b lb mo ju ld le mp jx lg lh mq lj lk ll mr ln lo lp ms lr ls lt im bi translated">我们要从头开始编码的第一种方法叫做<em class="nn">普通最小二乘法</em> (OLS)。OLS计算出<strong class="la iu"> X </strong>的伪逆，并将其乘以目标值<strong class="la iu"> y </strong>。在python中，这个方法很容易使用<strong class="la iu"> scipy.linalg.lstsq() </strong>来实现，Scikit-Learn的<strong class="la iu"> LinearRegression() </strong>类使用的就是这个函数。</p><p id="13bd" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们将尝试通过使用<strong class="la iu"> fit() </strong>和<strong class="la iu"> predict() </strong>方法，使我们的库类似于Scikit-Learn的库。我们还将赋予它属性<strong class="la iu"> coef_ </strong>，它是在训练期间计算的参数。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="no np l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">模块1。回归. py</p></figure><p id="7848" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">要编写<strong class="la iu"> fit() </strong>方法，我们只需向我们的特征数组添加一个偏差项，并使用函数<strong class="la iu"> scipy.linalg.lstsq() </strong>执行OLS。我们将计算出的参数系数存储在我们的属性<strong class="la iu"> coef_ </strong>中，然后返回self的一个实例。<strong class="la iu"> predict() </strong>方法甚至更简单。我们所做的就是为偏差项的每个实例加1，然后取特征矩阵和我们计算的参数的点积。</p><p id="4a2f" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">当特征的数量变得非常大时，普通的最小二乘法会变得非常慢。在这些情况下，最好使用另一种称为<em class="nn">梯度下降</em>的优化方法。</p><h2 id="4687" class="lv lw it bd lx ly lz dn ma mb mc dp md lh me mf mg ll mh mi mj lp mk ml mm mn bi translated">批量梯度下降</h2><p id="da38" class="pw-post-body-paragraph ky kz it la b lb mo ju ld le mp jx lg lh mq lj lk ll mr ln lo lp ms lr ls lt im bi translated">梯度下降是一种通用优化算法，通过对参数进行微调来搜索最优解。首先，用随机值填充<strong class="la iu">θ</strong>(这种方法称为<em class="nn">随机初始化</em>)。然后，调整参数，直到算法通过向降低成本函数的方向行进而收敛到最小解。在我们的情况下，这意味着减少MSE。</p><p id="2bdd" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">为了沿着降低成本函数的方向行进，你需要计算包含成本函数所有偏导数的梯度向量。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nq"><img src="../Images/d9c2ffad2d3f9725a7969b85294d822a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RZtENANVritFnhfksrklaQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">等式1。成本函数的梯度向量</p></figure><p id="c8d3" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">计算完梯度向量后，在梯度向量指向的相反方向上更新参数。这就是学习率 (η)发挥作用的地方。学习速度决定了你朝那个方向迈出的步伐的大小。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nr"><img src="../Images/d434363d3b45f757159aa6b35787d94e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hu0ALdnAIqNArvqDo6FHow.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">等式2。梯度下降步骤</p></figure><p id="d5e7" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><em class="nn">批量梯度下降</em>是梯度下降的一个版本，我们在每一步计算整个数据集的梯度向量。这意味着批量梯度下降不适用于非常大的训练集，因为它必须加载整个数据集来计算下一步。如果你的数据集非常大，你可能想使用<em class="nn">随机梯度下降</em>或<em class="nn">小批量梯度下降</em>，但我们不会在这里涵盖这些。只要知道他们的存在。</p><p id="efa8" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">让我们编辑当前模块，以包括使用批量梯度下降进行训练的选项。因为我们还将创建一个Ridge、Lasso和ElasticNet类，所以我们将创建一个所有回归变量都可以继承的基本Regression()类。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="no np l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">模块2。回归. py</p></figure><p id="9a7a" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">现在，我们有了一个全功能的线性回归模型，能够使用批量梯度下降和普通最小二乘法进行训练。但是，如果我们的数据不是一条直线呢？如果是这样的话，我们就需要一个更复杂的模型来拟合非线性数据，比如多项式回归。</p><h1 id="c56a" class="nc lw it bd lx nd ne nf ma ng nh ni md jz nj ka mg kc nk kd mj kf nl kg mm nm bi translated">多项式回归</h1><p id="dd01" class="pw-post-body-paragraph ky kz it la b lb mo ju ld le mp jx lg lh mq lj lk ll mr ln lo lp ms lr ls lt im bi translated">使用线性模型对非线性数据进行建模的一种简单方法是添加每个特征的幂作为新特征，然后在这个扩展的特征集上训练模型。这种技术被称为多项式回归。</p><p id="ba99" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">当您有多个特性时，这种技术能够发现特性之间的关系，因为您将所有特性的组合添加到给定的程度。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ns"><img src="../Images/430dba78f1e16234a25df387c5d496c9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FxG7SIRBLHXXhF-o2yBk7w.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图二。多项式回归——卢克·纽曼</p></figure><p id="af10" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">好了，现在我们知道多项式回归与线性回归相同，只是我们在训练之前向数据集添加了多项式要素。我们不是创建一个单独的<strong class="la iu">多项式回归()</strong>类，而是添加一个预处理类，它可以在训练之前转换您的数据。这样，一旦我们建立了正则化的线性模型，它们也将能够执行多项式回归。</p><p id="440e" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们将以类似于Scikit-Learn的预处理类的方式对其进行编码。它将有一个<strong class="la iu"> fit() </strong>、<strong class="la iu"> transform() </strong>和<strong class="la iu"> fit_transform() </strong>方法。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="no np l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">模块3。预处理. py</p></figure><p id="7175" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">如果此时尝试执行多项式回归，可能会在训练过程中出错。这是因为您使用梯度下降作为训练算法。当我们变换要素以添加多项式项时，如果我们使用梯度下降等迭代训练算法，则规范化数据非常重要。如果我们不这样做，我们就有遭遇<em class="nn">爆炸梯度</em>的风险。</p><h2 id="b513" class="lv lw it bd lx ly lz dn ma mb mc dp md lh me mf mg ll mh mi mj lp mk ml mm mn bi translated">数据标准化</h2><p id="c03d" class="pw-post-body-paragraph ky kz it la b lb mo ju ld le mp jx lg lh mq lj lk ll mr ln lo lp ms lr ls lt im bi translated">有许多方法可以标准化数据，但是我们将坚持使用最简单的一种。通过减去平均值并将每个实例除以每个特征的标准偏差，我们有效地<em class="nn">标准化了</em>我们的数据。这意味着我们所有的特征都以平均值0和单位方差为中心。</p><p id="3834" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">让我们将一个名为<strong class="la iu"> StandardScaler() </strong>的类添加到我们的预处理. py模块中。我们将在这里包含一个<strong class="la iu"> inverse_transform() </strong>方法，以防我们需要在数据被标准化后将其恢复到原始状态。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="no np l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">模块4。预处理. py</p></figure><p id="c9e7" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">使用我们的代码，多项式回归可以分三步实现。首先，向数据集中添加多项式要素。然后，使用<strong class="la iu"> StandardScaler() </strong>标准化我们的特征。最后，用OLS或批量梯度下降训练我们的线性回归模型。</p><h1 id="c68a" class="nc lw it bd lx nd ne nf ma ng nh ni md jz nj ka mg kc nk kd mj kf nl kg mm nm bi translated">正则化线性模型</h1><p id="5d33" class="pw-post-body-paragraph ky kz it la b lb mo ju ld le mp jx lg lh mq lj lk ll mr ln lo lp ms lr ls lt im bi translated">使用多项式回归，我们很容易通过将degree参数设置得太高来过度拟合数据集。减少过度拟合的一种方法是正则化模型(即约束它):模型的自由度越少，它就越难过度拟合数据。</p><p id="70e9" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">对于线性模型，正则化它们的一种方法是约束参数权重。让我们从零开始，通过编码<em class="nn">脊</em>、<em class="nn">套索</em>和<em class="nn">弹性</em>、T6】网回归，来看看实现这一点的三种不同方式。</p><h2 id="9805" class="lv lw it bd lx ly lz dn ma mb mc dp md lh me mf mg ll mh mi mj lp mk ml mm mn bi translated">山脉</h2><p id="772d" class="pw-post-body-paragraph ky kz it la b lb mo ju ld le mp jx lg lh mq lj lk ll mr ln lo lp ms lr ls lt im bi translated">岭回归是线性回归的正则化形式，其中我们将正则化项添加到成本函数中，等于参数权重的L2范数的一半。使用岭回归，我们的成本函数看起来像:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/300d69db7dbec92e57f2b803876e0e8b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1358/format:webp/1*cKKTMKDc7mQRba25QCbJUA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">等式3。岭回归成本函数</p></figure><p id="db82" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">其中α控制您希望添加到模型中的正则化程度。为了使用梯度下降从头实现这一点，我们还需要正则化项的偏导数。很简单，就是α乘以参数权重。</p><p id="9366" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">需要注意的是，正则项sum从i=1开始，而不是0。这是因为我们没有正则化偏差项。</p><p id="19ef" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">好了，我们知道了向regression.py模块添加Ridge所需的一切。首先，我们将创建一个计算L2范数和梯度向量的类。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="no np l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">模块5。正则化. py</p></figure><p id="4ba4" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">当我们调用这个类时，它将表现为一个函数并为我们计算正则项，当我们调用它的<strong class="la iu"> grad() </strong>方法时，它将计算梯度向量正则项。现在，我们只是使用我们的助手类来计算我们的基本回归类中梯度下降过程中的正则项。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="no np l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">模块6。回归. py</p></figure><p id="7a0d" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在我们的基本回归类的<strong class="la iu"> fit() </strong>方法中，我们将正则化项添加到我们的误差度量MSE中，并将导数添加到梯度向量中。</p><p id="1e82" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">接下来，我们更新了LinearRegression类，以包含self.regularization和self . regulation . grad属性，通过快速使用lambda函数，这些属性的值为0。这意味着使用线性回归时没有正则化。</p><p id="6e9b" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">最后，我们创建我们的岭回归类，它继承了基本回归类，给它相同的属性和方法。我们唯一需要改变的是<strong class="la iu"> __init__() </strong>方法。</p><p id="ac58" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们给它两个属性，alpha和self。正则化。Alpha用于控制正则化和self的数量。正则化等于我们的l2 _正则化类，它计算我们在梯度下降中使用的惩罚项。</p><h2 id="92eb" class="lv lw it bd lx ly lz dn ma mb mc dp md lh me mf mg ll mh mi mj lp mk ml mm mn bi translated">套索</h2><p id="5abf" class="pw-post-body-paragraph ky kz it la b lb mo ju ld le mp jx lg lh mq lj lk ll mr ln lo lp ms lr ls lt im bi translated">最小绝对收缩和选择算子(Lasso)回归的实现方式与岭回归完全相同，只是它增加了一个等于L1范数的正则化惩罚。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/aeeaa8ce0d22c4586a089b10cc08d8ff.png" data-original-src="https://miro.medium.com/v2/resize:fit:1300/format:webp/1*h8_nCcC9D_yDI7YWLru6MQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">等式4。Lasso回归成本函数</p></figure><p id="a75d" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">因为L1范数在0处不可微，所以我们用等于符号函数的次梯度向量来计算梯度上的正则化罚值。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nv"><img src="../Images/2eaa25eb5fda7a66b4bc2ea7012c8191.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*d_dWd3FRkpNs81DPsnZ7iw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">等式5。Lasso回归次梯度向量</p></figure><p id="6739" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">要从头开始完成Lasso回归的实现，我们所要做的就是创建一个正则化类来帮助计算惩罚项，然后创建Lasso回归类。</p><p id="a2d3" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">首先，我们将L1正则化类添加到regularization.py模块中。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="no np l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">模块7。正则化. py</p></figure><p id="d23c" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">其次，我们将创建一个新的Lasso类，它也将从我们的regression.py模块中的基本回归类继承。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="no np l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">模块8。回归. py</p></figure><p id="d2cb" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">Lasso回归实现了一种要素选择形式，因为它倾向于消除最不重要的要素(即将其设置为零)。</p><h2 id="1fc1" class="lv lw it bd lx ly lz dn ma mb mc dp md lh me mf mg ll mh mi mj lp mk ml mm mn bi translated">弹性网</h2><p id="ea75" class="pw-post-body-paragraph ky kz it la b lb mo ju ld le mp jx lg lh mq lj lk ll mr ln lo lp ms lr ls lt im bi translated">弹性网实现了对成本函数和梯度向量的脊和套索正则化项的简单混合。混合比<em class="nn"> r </em>决定了每项包含多少。当<em class="nn"> r </em> = 0时，弹性网相当于脊线，当<em class="nn"> r </em> = 1时，相当于套索。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nw"><img src="../Images/bf2b41ab9c2c2d6ab699f8b8282e7b83.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7MJ2g9NtPUWfhoj4N3mckQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">等式6。弹性净成本函数</p></figure><p id="f185" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">正如我们从头开始编写Lasso Regression时所做的那样，我们创建了一个正则化类来计算惩罚，并使ElasticNet类从基本Regression类继承。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="no np l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">模块9。正则化. py</p></figure><p id="bfe7" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">现在让我们编写ElasticNet回归类。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="no np l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">模块10。回归. py</p></figure><p id="4b52" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">让我们快速地提一下你什么时候应该使用我们从头编码的每个模型。一般来说，使用稍微正则化的正则化线性模型几乎总是更可取的。</p><p id="118c" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">山脊线是一个很好的默认值，但是如果您预计只有少数要素有用，则应使用套索或弹性网，因为它们具有要素选择属性。当要素的数量大于训练实例的数量或者当多个要素高度相关时，Lasso可能会表现不稳定。因此，通常优选弹性网。</p><h1 id="b372" class="nc lw it bd lx nd ne nf ma ng nh ni md jz nj ka mg kc nk kd mj kf nl kg mm nm bi translated">包裹</h1><p id="4588" class="pw-post-body-paragraph ky kz it la b lb mo ju ld le mp jx lg lh mq lj lk ll mr ln lo lp ms lr ls lt im bi translated">这就完成了机器从头学习系列的这一部分。我们已经实现了用于回归的最常见的线性模型。</p><p id="58ee" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我怎么强调都不为过，从头开始编写机器学习模型将会成倍地增加你的ML知识，同时也增加你在python和面向对象编程方面的技能。</p><p id="7015" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">对于扩展这个项目的想法，我想包括小批量和随机梯度下降作为训练每个模型的选项。此外，实现某种形式的早期停止将进一步增加我们的机器学习知识。</p><p id="8b5e" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">一如既往，感谢阅读和所有的反馈非常感谢！</p></div><div class="ab cl nx ny hx nz" role="separator"><span class="oa bw bk ob oc od"/><span class="oa bw bk ob oc od"/><span class="oa bw bk ob oc"/></div><div class="im in io ip iq"><p id="6fb0" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在本系列的下一部分中，我们将构建我们的库，从头开始编写用于分类的线性模型，包括logistic和softmax回归。</p><p id="0d0d" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">如果您喜欢这里的内容，请关注我！:-) </p></div></div>    
</body>
</html>