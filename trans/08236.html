<html>
<head>
<title>Word2Vec Explained</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Word2Vec解释道</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/word2vec-explained-49c52b4ccb71?source=collection_archive---------0-----------------------#2021-07-29">https://towardsdatascience.com/word2vec-explained-49c52b4ccb71?source=collection_archive---------0-----------------------#2021-07-29</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="3958" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">解释Word2Vec的直观性&amp;用Python实现它</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/df1544e91725e90485842757f3b99b27.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*opkJFwNbcnezyRnh"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">这张照片是由<a class="ae ky" href="https://unsplash.com/photos/GkinCd2enIY" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>的Raphael Schaller 拍摄的</p></figure></div><div class="ab cl kz la hx lb" role="separator"><span class="lc bw bk ld le lf"/><span class="lc bw bk ld le lf"/><span class="lc bw bk ld le"/></div><div class="im in io ip iq"><p id="5a04" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated"><strong class="li iu">目录</strong></p><ul class=""><li id="3478" class="mc md it li b lj lk lm ln lp me lt mf lx mg mb mh mi mj mk bi translated">介绍</li><li id="db11" class="mc md it li b lj ml lm mm lp mn lt mo lx mp mb mh mi mj mk bi translated">什么是单词嵌入？</li><li id="3fb6" class="mc md it li b lj ml lm mm lp mn lt mo lx mp mb mh mi mj mk bi translated">Word2Vec架构<br/> - CBOW(连续单词包)模型<br/> -连续跳格模型</li><li id="051d" class="mc md it li b lj ml lm mm lp mn lt mo lx mp mb mh mi mj mk bi translated">实现<br/> -数据<br/> -需求<br/> -导入数据<br/> -数据预处理<br/> -嵌入<br/> -嵌入的PCA</li><li id="41e2" class="mc md it li b lj ml lm mm lp mn lt mo lx mp mb mh mi mj mk bi translated">结束语</li><li id="6c6b" class="mc md it li b lj ml lm mm lp mn lt mo lx mp mb mh mi mj mk bi translated">资源</li></ul><h1 id="766a" class="mq mr it bd ms mt mu mv mw mx my mz na jz nb ka nc kc nd kd ne kf nf kg ng nh bi translated">介绍</h1><p id="337d" class="pw-post-body-paragraph lg lh it li b lj ni ju ll lm nj jx lo lp nk lr ls lt nl lv lw lx nm lz ma mb im bi translated">Word2Vec是NLP领域的最新突破。<a class="ae ky" href="https://en.wikipedia.org/wiki/Tomas_Mikolov" rel="noopener ugc nofollow" target="_blank"> Tomas Mikolov </a>捷克计算机科学家，目前是CIIRC ( <a class="ae ky" href="https://en.wikipedia.org/wiki/Czech_Institute_of_Informatics,_Robotics_and_Cybernetics" rel="noopener ugc nofollow" target="_blank">捷克信息、机器人和控制论研究所</a>)的研究员，是word2vec研究和实现的主要贡献者之一。单词嵌入是解决自然语言处理中许多问题不可缺少的一部分。它们描述了人类如何理解机器的语言。你可以把它们想象成文本的矢量表示。Word2Vec是一种常见的生成单词嵌入的方法，具有多种应用，如文本相似性、推荐系统、情感分析等。</p><h1 id="1e4f" class="mq mr it bd ms mt mu mv mw mx my mz na jz nb ka nc kc nd kd ne kf nf kg ng nh bi translated">什么是单词嵌入？</h1><p id="24a9" class="pw-post-body-paragraph lg lh it li b lj ni ju ll lm nj jx lo lp nk lr ls lt nl lv lw lx nm lz ma mb im bi translated">在进入word2vec之前，让我们先了解一下什么是单词嵌入。了解这一点很重要，因为word2vec的整体结果和输出将是与通过算法传递的每个唯一单词相关联的嵌入。</p><p id="89b0" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">单词嵌入是一种将单个单词转换成单词的数字表示(向量)的技术。其中每个单词被映射到一个向量，然后这个向量以类似于神经网络的方式被学习。向量试图捕捉该单词相对于整个文本的各种特征。这些特征可以包括单词的语义关系、定义、上下文等。有了这些数字表示，你可以做很多事情，比如识别单词之间的相似或相异。</p><p id="52ee" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">显然，这些是机器学习各个方面的输入。机器不能处理原始形式的文本，因此将文本转换成嵌入将允许用户将嵌入馈送到经典的机器学习模型。最简单的嵌入是文本数据的一个热编码，其中每个向量将被映射到一个类别。</p><pre class="kj kk kl km gt nn no np nq aw nr bi"><span id="888e" class="ns mr it no b gy nt nu l nv nw">For example: </span><span id="0735" class="ns mr it no b gy nx nu l nv nw">have = [1, 0, 0, 0, 0, 0, ... 0]<br/>a    = [0, 1, 0, 0, 0, 0, ... 0]<br/>good = [0, 0, 1, 0, 0, 0, ... 0]<br/>day  = [0, 0, 0, 1, 0, 0, ... 0] ...</span></pre><p id="67a4" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">然而，像这样的简单嵌入有多种限制，因为它们不能捕获单词的特征，并且它们可能非常大，这取决于语料库的大小。</p><h1 id="a1fa" class="mq mr it bd ms mt mu mv mw mx my mz na jz nb ka nc kc nd kd ne kf nf kg ng nh bi translated">Word2Vec架构</h1><p id="d329" class="pw-post-body-paragraph lg lh it li b lj ni ju ll lm nj jx lo lp nk lr ls lt nl lv lw lx nm lz ma mb im bi translated">Word2Vec的有效性来自于它能够将相似单词的向量组合在一起。给定一个足够大的数据集，Word2Vec可以根据单词在文本中的出现次数对单词的含义做出强有力的估计。这些估计产生了与语料库中其他单词的单词关联。例如，像“国王”和“王后”这样的词彼此非常相似。当对单词嵌入进行代数运算时，你可以找到单词相似性的近似。例如,“国王”的2维嵌入向量-“男人”的2维嵌入向量+“女人”的2维嵌入向量产生了与“皇后”的嵌入向量非常接近的向量。注意，下面的值是任意选择的。</p><pre class="kj kk kl km gt nn no np nq aw nr bi"><span id="85f3" class="ns mr it no b gy nt nu l nv nw">King    -    Man    +    Woman    =    Queen<br/>[5,3]   -    [2,1]  +    [3, 2]   =    [6,4]  </span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/64d1e0100d4c8ff7bcc87af25e68db07.png" data-original-src="https://miro.medium.com/v2/resize:fit:1128/format:webp/1*hnu-NqrK3C7wmYWcKXpb-Q.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">你可以看到King和Queen这两个词的位置很接近。(图片由作者提供)</p></figure><p id="8ca7" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">word2vec的成功主要得益于两种架构。跳跃图和CBOW架构。</p><h2 id="00ac" class="ns mr it bd ms nz oa dn mw ob oc dp na lp od oe nc lt of og ne lx oh oi ng oj bi translated">连续单词袋</h2><p id="eed9" class="pw-post-body-paragraph lg lh it li b lj ni ju ll lm nj jx lo lp nk lr ls lt nl lv lw lx nm lz ma mb im bi translated">这种架构非常类似于前馈神经网络。这种模型架构本质上试图从上下文单词列表中预测目标单词。这个模型背后的直觉非常简单:给定一个短语<code class="fe ok ol om no b">"Have a great day"</code>，我们将选择我们的目标词为“a”，我们的上下文词为[“have”、“great”、“day”]。这个模型要做的是利用上下文单词的分布式表示来尝试和预测目标单词。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi on"><img src="../Images/a5dade10c48d2d444d58f6660b347497.png" data-original-src="https://miro.medium.com/v2/resize:fit:784/format:webp/1*_8Ul4ICaCtmZWPrWqH32Ow.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">CBOW架构。图片取自<a class="ae ky" href="https://arxiv.org/pdf/1301.3781.pdf" rel="noopener ugc nofollow" target="_blank">向量空间中单词表示的有效估计</a></p></figure><h2 id="a16f" class="ns mr it bd ms nz oa dn mw ob oc dp na lp od oe nc lt of og ne lx oh oi ng oj bi translated">连续跳格模型</h2><p id="a0a6" class="pw-post-body-paragraph lg lh it li b lj ni ju ll lm nj jx lo lp nk lr ls lt nl lv lw lx nm lz ma mb im bi translated">skip-gram模型是一个简单的神经网络，具有一个经过训练的隐藏层，以便在输入单词出现时预测给定单词出现的概率。直觉上，你可以想象跳格模型是CBOW模型的对立面。在这种架构中，它将当前单词作为输入，并尝试准确预测当前单词前后的单词。该模型本质上试图学习和预测指定输入单词周围的上下文单词。基于评估该模型准确性的实验，发现在给定大范围的词向量的情况下，预测质量提高了，然而这也增加了计算复杂度。该过程可以直观地描述如下。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oo"><img src="../Images/6f310cc3ab794079984e7f082091f75e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*M6UxaLSbNMeoDFWRN_kPeQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">为skip-gram模型生成训练数据的示例。窗口大小为3。图片由作者提供</p></figure><p id="ee74" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">如上所述，给定一些文本语料库，在一些滚动窗口上选择目标单词。训练数据由该目标单词和窗口中所有其他单词的成对组合组成。这是神经网络的最终训练数据。一旦模型被训练，我们基本上可以产生一个单词成为给定目标的上下文单词的概率。下图显示了skip-gram模型的神经网络体系结构。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi op"><img src="../Images/afed5871d067d415236163297a524ea9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UYAkOS9JQwdozQjCzttuow.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">跳格模型架构(图片由作者提供)</p></figure><p id="9ce9" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">语料库可以表示为大小为N的向量，其中N中的每个元素对应于语料库中的一个单词。在训练过程中，我们有一对目标和上下文单词，输入数组中除目标单词外的所有元素都为0。目标字将等于1。隐藏层将学习每个单词的嵌入表示，产生d维嵌入空间。输出层是具有softmax激活功能的密集层。输出层基本上会产生一个与输入大小相同的向量，向量中的每个元素都由一个概率组成。这个概率指示了目标单词和语料库中的关联单词之间的相似性。</p><p id="8468" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">对于这两个模型的更详细的概述，我强烈推荐阅读概述这些结果的原始论文<a class="ae ky" href="https://arxiv.org/pdf/1301.3781.pdf" rel="noopener ugc nofollow" target="_blank">这里</a>。</p><h1 id="8815" class="mq mr it bd ms mt mu mv mw mx my mz na jz nb ka nc kc nd kd ne kf nf kg ng nh bi translated">履行</h1><p id="2bba" class="pw-post-body-paragraph lg lh it li b lj ni ju ll lm nj jx lo lp nk lr ls lt nl lv lw lx nm lz ma mb im bi translated">我将展示如何使用word2vec来生成单词嵌入，并通过<a class="ae ky" href="https://pyshark.com/principal-component-analysis-in-python/" rel="noopener ugc nofollow" target="_blank"> PCA </a>使用这些嵌入来查找相似的单词和嵌入的可视化。</p><h2 id="9aa6" class="ns mr it bd ms nz oa dn mw ob oc dp na lp od oe nc lt of og ne lx oh oi ng oj bi translated"><strong class="ak">数据</strong></h2><p id="9c46" class="pw-post-body-paragraph lg lh it li b lj ni ju ll lm nj jx lo lp nk lr ls lt nl lv lw lx nm lz ma mb im bi translated">出于本教程的目的，我们将使用莎士比亚数据集。你可以在这里找到我在本教程中使用的文件<a class="ae ky" href="https://github.com/vatsal220/medium_articles/blob/main/w2v/data/shakespeare.txt" rel="noopener ugc nofollow" target="_blank"/>，它包括了莎士比亚为他的剧本写的所有台词。</p><h2 id="a0dc" class="ns mr it bd ms nz oa dn mw ob oc dp na lp od oe nc lt of og ne lx oh oi ng oj bi translated">要求</h2><pre class="kj kk kl km gt nn no np nq aw nr bi"><span id="65c2" class="ns mr it no b gy nt nu l nv nw">nltk==3.6.1<br/>node2vec==0.4.3<br/>pandas==1.2.4<br/>matplotlib==3.3.4<br/>gensim==4.0.1<br/>scikit-learn=0.24.1</span></pre><p id="2c18" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated"><strong class="li iu">注意:</strong>因为我们正在使用NLTK，你可能需要下载下面的语料库来完成本教程的剩余部分。这可以通过以下命令轻松完成:</p><pre class="kj kk kl km gt nn no np nq aw nr bi"><span id="44f7" class="ns mr it no b gy nt nu l nv nw">import nltk<br/>nltk.download('stopwords')<br/>nltk.download('punkt')</span></pre><h2 id="8f75" class="ns mr it bd ms nz oa dn mw ob oc dp na lp od oe nc lt of og ne lx oh oi ng oj bi translated">输入数据</h2><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oq or l"/></div></figure><p id="cf2c" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated"><strong class="li iu">注意:</strong>将<code class="fe ok ol om no b"><strong class="li iu">PATH</strong></code>变量更改为您正在处理的数据的路径。</p><h2 id="3cbe" class="ns mr it bd ms nz oa dn mw ob oc dp na lp od oe nc lt of og ne lx oh oi ng oj bi translated">预处理数据</h2><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oq or l"/></div></figure><p id="6cf7" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated"><strong class="li iu">停用词过滤注释</strong></p><ul class=""><li id="5203" class="mc md it li b lj lk lm ln lp me lt mf lx mg mb mh mi mj mk bi translated">请注意，从这些行中删除的停用词是现代词汇。应用程序和数据对于清理单词所需的预处理策略的类型非常重要。</li><li id="44fb" class="mc md it li b lj ml lm mm lp mn lt mo lx mp mb mh mi mj mk bi translated">在我们的场景中，单词“you”或“yourself”将出现在停用词中，并从行中删除，但是由于这是莎士比亚文本数据，因此不会使用这些类型的单词。相反，“你”或“你自己”可能是有用的删除。保持对这些类型的微小变化的热情，因为它们对好模型和差模型的性能产生了巨大的差异。</li><li id="f742" class="mc md it li b lj ml lm mm lp mn lt mo lx mp mb mh mi mj mk bi translated">出于这个例子的目的，在识别不同世纪的停用词时，我不会涉及太多细节，但是请注意，您应该这样做。</li></ul><h2 id="faa0" class="ns mr it bd ms nz oa dn mw ob oc dp na lp od oe nc lt of og ne lx oh oi ng oj bi translated">把…嵌入</h2><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oq or l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi os"><img src="../Images/d1e220a9f6983f3d7e84c6613002f6f0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1164/format:webp/1*RSuePgt7WsS9qIrj88DuIA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">莎士比亚资料中与thou最相似的单词(图片由作者提供)</p></figure><h2 id="534b" class="ns mr it bd ms nz oa dn mw ob oc dp na lp od oe nc lt of og ne lx oh oi ng oj bi translated">嵌入的主成分分析</h2><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oq or l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ot"><img src="../Images/0b774ea61c2c8e5d5269fb87e48a04b3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*E8jo0k46xpXQOSJ4Ibo0jw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">彼此相似的单词将被放置在彼此更靠近的地方。图片由作者提供</p></figure><p id="3b91" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">Tensorflow对word2vec模型做了非常漂亮、直观和用户友好的表示。我强烈建议您探索它，因为它允许您与word2vec的结果进行交互。链接在下面。</p><div class="ou ov gp gr ow ox"><a href="https://projector.tensorflow.org/" rel="noopener  ugc nofollow" target="_blank"><div class="oy ab fo"><div class="oz ab pa cl cj pb"><h2 class="bd iu gy z fp pc fr fs pd fu fw is bi translated">嵌入式投影仪-高维数据的可视化</h2><div class="pe l"><h3 class="bd b gy z fp pc fr fs pd fu fw dk translated">可视化高维数据。</h3></div><div class="pf l"><p class="bd b dl z fp pc fr fs pd fu fw dk translated">projector.tensorflow.org</p></div></div><div class="pg l"><div class="ph l pi pj pk pg pl ks ox"/></div></div></a></div><h1 id="fd9a" class="mq mr it bd ms mt mu mv mw mx my mz na jz nb ka nc kc nd kd ne kf nf kg ng nh bi translated">结束语</h1><p id="bcc1" class="pw-post-body-paragraph lg lh it li b lj ni ju ll lm nj jx lo lp nk lr ls lt nl lv lw lx nm lz ma mb im bi translated">词嵌入是解决自然语言处理中许多问题的重要组成部分，它描述了人类如何理解机器语言。给定一个大的文本语料库，word2vec产生一个与语料库中的每个单词相关联的嵌入向量。这些嵌入的结构使得具有相似特征的单词彼此非常接近。CBOW(连续单词包)和skip-gram模型是与word2vec相关的两个主要架构。给定一个输入单词，skip-gram将尝试预测输入上下文中的单词，而CBOW模型将采用各种单词并尝试预测缺失的单词。</p><p id="5008" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">我还写过node2vec，它使用word2vec生成给定网络的节点嵌入。你可以在这里读到它。</p><div class="ou ov gp gr ow ox"><a rel="noopener follow" target="_blank" href="/node2vec-explained-db86a319e9ab"><div class="oy ab fo"><div class="oz ab pa cl cj pb"><h2 class="bd iu gy z fp pc fr fs pd fu fw is bi translated">Node2Vec解释</h2><div class="pe l"><h3 class="bd b gy z fp pc fr fs pd fu fw dk translated">用Python解释和实现Node2Vec白皮书</h3></div><div class="pf l"><p class="bd b dl z fp pc fr fs pd fu fw dk translated">towardsdatascience.com</p></div></div><div class="pg l"><div class="pm l pi pj pk pg pl ks ox"/></div></div></a></div><h1 id="c1c3" class="mq mr it bd ms mt mu mv mw mx my mz na jz nb ka nc kc nd kd ne kf nf kg ng nh bi translated">资源</h1><ul class=""><li id="f048" class="mc md it li b lj ni lm nj lp pn lt po lx pp mb mh mi mj mk bi translated"><a class="ae ky" href="https://arxiv.org/pdf/1301.3781.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1301.3781.pdf</a></li><li id="0197" class="mc md it li b lj ml lm mm lp mn lt mo lx mp mb mh mi mj mk bi translated"><a class="ae ky" href="https://www.kdnuggets.com/2019/02/word-embeddings-nlp-applications.html" rel="noopener ugc nofollow" target="_blank">https://www . kdnugges . com/2019/02/word-embeddings-NLP-applications . html</a></li><li id="d5a4" class="mc md it li b lj ml lm mm lp mn lt mo lx mp mb mh mi mj mk bi translated">【https://wiki.pathmind.com/word2vec T4】</li><li id="2403" class="mc md it li b lj ml lm mm lp mn lt mo lx mp mb mh mi mj mk bi translated"><a class="ae ky" href="https://projector.tensorflow.org/" rel="noopener ugc nofollow" target="_blank">https://projector.tensorflow.org/</a></li></ul></div><div class="ab cl kz la hx lb" role="separator"><span class="lc bw bk ld le lf"/><span class="lc bw bk ld le lf"/><span class="lc bw bk ld le"/></div><div class="im in io ip iq"><p id="f2f0" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">如果您喜欢阅读这篇文章，请考虑关注我的后续文章，了解其他数据科学材料以及解决数据科学不同领域相关问题的材料(如word2vec)。这里有一些我写的其他文章，我想你可能会喜欢。</p><div class="ou ov gp gr ow ox"><a rel="noopener follow" target="_blank" href="/bayesian-a-b-testing-explained-344a6df88c1a"><div class="oy ab fo"><div class="oz ab pa cl cj pb"><h2 class="bd iu gy z fp pc fr fs pd fu fw is bi translated">贝叶斯A/B测试解释</h2><div class="pf l"><p class="bd b dl z fp pc fr fs pd fu fw dk translated">towardsdatascience.com</p></div></div><div class="pg l"><div class="pq l pi pj pk pg pl ks ox"/></div></div></a></div><div class="ou ov gp gr ow ox"><a rel="noopener follow" target="_blank" href="/recommendation-systems-explained-a42fc60591ed"><div class="oy ab fo"><div class="oz ab pa cl cj pb"><h2 class="bd iu gy z fp pc fr fs pd fu fw is bi translated">推荐系统解释</h2><div class="pe l"><h3 class="bd b gy z fp pc fr fs pd fu fw dk translated">用Python解释和实现基于内容的协同过滤和混合推荐系统</h3></div><div class="pf l"><p class="bd b dl z fp pc fr fs pd fu fw dk translated">towardsdatascience.com</p></div></div><div class="pg l"><div class="pr l pi pj pk pg pl ks ox"/></div></div></a></div><div class="ou ov gp gr ow ox"><a rel="noopener follow" target="_blank" href="/monte-carlo-method-explained-8635edf2cf58"><div class="oy ab fo"><div class="oz ab pa cl cj pb"><h2 class="bd iu gy z fp pc fr fs pd fu fw is bi translated">蒙特卡罗方法解释</h2><div class="pe l"><h3 class="bd b gy z fp pc fr fs pd fu fw dk translated">在这篇文章中，我将向你介绍、解释和实现蒙特卡罗方法。这种模拟方法是一种…</h3></div><div class="pf l"><p class="bd b dl z fp pc fr fs pd fu fw dk translated">towardsdatascience.com</p></div></div><div class="pg l"><div class="ps l pi pj pk pg pl ks ox"/></div></div></a></div><div class="ou ov gp gr ow ox"><a rel="noopener follow" target="_blank" href="/markov-chain-explained-210581d7a4a9"><div class="oy ab fo"><div class="oz ab pa cl cj pb"><h2 class="bd iu gy z fp pc fr fs pd fu fw is bi translated">马尔可夫链解释道</h2><div class="pe l"><h3 class="bd b gy z fp pc fr fs pd fu fw dk translated">在本文中，我将解释并提供马尔可夫链的python实现。这篇文章不会是一篇深刻的…</h3></div><div class="pf l"><p class="bd b dl z fp pc fr fs pd fu fw dk translated">towardsdatascience.com</p></div></div><div class="pg l"><div class="pt l pi pj pk pg pl ks ox"/></div></div></a></div></div></div>    
</body>
</html>