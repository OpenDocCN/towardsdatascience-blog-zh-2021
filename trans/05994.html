<html>
<head>
<title>Pattern Recognition and the Fundamental Methods of Machine Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">模式识别和机器学习的基本方法</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/under-the-hood-of-modern-machine-and-deep-learning-d76fc1249467?source=collection_archive---------6-----------------------#2021-05-29">https://towardsdatascience.com/under-the-hood-of-modern-machine-and-deep-learning-d76fc1249467?source=collection_archive---------6-----------------------#2021-05-29</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/81c5a73fb6ee9921372fda08b6beeabd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JYpPuT4ShPHXKg_BOlekyg.jpeg"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">现代深度和机器学习“在引擎盖下”有许多经典的方法。内森·范·埃格蒙德在<a class="ae jg" href="https://unsplash.com/s/photos/under-the-hood?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><h2 id="dcf3" class="jh ji jj bd b dl jk jl jm jn jo jp dk jq translated" aria-label="kicker paragraph">在机器和深度学习的引擎盖下</h2><div class=""/><div class=""><h2 id="bd0b" class="pw-subtitle-paragraph kp js jj bd b kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg dk translated">经典ML方法综述</h2></div><p id="acb8" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">现代深度学习方法每天都会产生非凡的结果。为了理解它的基础，最好也理解经典的模式识别和机器学习方法。在2020/21年的冬季学期，我教了一门关于这个主题的课，这是完全可以作为开放存取材料获得的。你可以在Zenodo 上找到PDF格式的<a class="ae jg" href="https://doi.org/10.5281/zenodo.4429576" rel="noopener ugc nofollow" target="_blank">幻灯片，在github </a>上找到相应的<a class="ae jg" href="https://github.com/akmaier/pr-slides" rel="noopener ugc nofollow" target="_blank"> TeX源文件。所有材料都通过许可证</a>在<a class="ae jg" href="https://creativecommons.org/licenses/by/4.0/" rel="noopener ugc nofollow" target="_blank"> CC 4.0下获得许可。在这篇文章中，我将对所有材料进行概述，并对每一章进行简短介绍，同时提供相应的视频和文字记录链接。</a></p><figure class="me mf mg mh gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi md"><img src="../Images/95b25516545381618b7e6c00ec15fbee.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wDX3hto3-Skv_sABpLVlRQ.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">经典模式识别将学习过程分为训练和测试阶段。这对于任何有监督的、无监督的或强化学习方法都是一样的。图片来源:<a class="ae jg" href="https://commons.wikimedia.org/wiki/File:PatternRecognitionSystem.svg" rel="noopener ugc nofollow" target="_blank">维基共享</a>下CC BY 4.0</p></figure><h1 id="0e9f" class="mi mj jj bd mk ml mm mn mo mp mq mr ms ky mt kz mu lb mv lc mw le mx lf my mz bi translated">第1章—简介</h1><p id="cd80" class="pw-post-body-paragraph lh li jj lj b lk na kt lm ln nb kw lp lq nc ls lt lu nd lw lx ly ne ma mb mc im bi translated">在第一章中，我们回顾了机器学习和模式识别的基本原理。尼曼的模式识别管道使用每个机器学习系统中基本出现的基本步骤引入了这一思想，并从数据处理的角度引入了对监督、非监督和强化学习系统有效的概念。基于这些观察，给出了详细的课程概述，并介绍了基本概念，如贝叶斯定理，最佳分类和评估措施。</p><p id="6a15" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">模式识别流水线— <a class="ae jg" href="https://youtu.be/8cZ-ljrSaEw" rel="noopener ugc nofollow" target="_blank">视频</a> — <a class="ae jg" href="https://medium.com/codex/an-introduction-to-pattern-recognition-adb34cd5d7bd" rel="noopener">抄本</a> <br/>模式识别公设— <a class="ae jg" href="https://youtu.be/lkf47tCvdbI" rel="noopener ugc nofollow" target="_blank">视频</a> — <a class="ae jg" href="https://medium.com/codex/pattern-recognition-basics-assumptions-strategies-for-classifier-evaluation-987186d5072" rel="noopener">抄本</a> <br/>详细课程概述— <a class="ae jg" href="https://youtu.be/BROB96fXtPI" rel="noopener ugc nofollow" target="_blank">视频</a> — <a class="ae jg" href="https://medium.com/codex/pattern-recognition-the-big-picture-29bc556f3c42" rel="noopener">抄本</a> <br/>贝叶斯定理— <a class="ae jg" href="https://youtu.be/NNigmXxPbGw" rel="noopener ugc nofollow" target="_blank">视频</a> — <a class="ae jg" href="https://medium.com/codex/how-do-i-compute-probabilities-for-things-that-i-cannot-observe-5503bde33ad9" rel="noopener">抄本</a> <br/>最优分类器— <a class="ae jg" href="https://youtu.be/2MN_7DpPDkY" rel="noopener ugc nofollow" target="_blank">视频</a> — <a class="ae jg" href="https://medium.com/codex/the-optimal-classifier-b8444c264852" rel="noopener">抄本</a> <br/>评价措施</p><figure class="me mf mg mh gt iv gh gi paragraph-image"><div class="gh gi nf"><img src="../Images/de0faab1da1ba398ec520e364daf64b7.png" data-original-src="https://miro.medium.com/v2/resize:fit:618/1*vIOOWjq5-FN8oau5Xy8fFA.gif"/></div><p class="jc jd gj gh gi je jf bd b be z dk translated">高斯钟形曲线通常用于模拟类别分布。图片来源:<a class="ae jg" href="https://commons.wikimedia.org/wiki/File:SmallDisplacedGaussianWF.gif" rel="noopener ugc nofollow" target="_blank">维基共享资源</a></p></figure><h1 id="cf36" class="mi mj jj bd mk ml mm mn mo mp mq mr ms ky mt kz mu lb mv lc mw le mx lf my mz bi translated">第2章——阶级分布和阶级界限</h1><p id="8d88" class="pw-post-body-paragraph lh li jj lj b lk na kt lm ln nb kw lp lq nc ls lt lu nd lw lx ly ne ma mb mc im bi translated">机器学习方法的一个关键要素是如何对底层分类问题建模。在本章中，我们介绍了逻辑函数(也称为sigmoid函数)，它是两个概率分布相交的一般结果，即两个类别之间的决策边界。如果使用高斯钟形曲线对类进行建模，则它们之间的决策边界通常会产生二次函数。对于指数族的所有分布，我们可以观察到类别之间的判定边界是线性的，如果它们共享相同的分散参数，例如高斯分布的相同协方差矩阵。基于这些观察，我们可以证明，我们可以使用嵌入到逻辑函数中的回归问题来直接估计类之间的决策边界。本章以朴素贝叶斯分类器的描述结束，该分类器假设所有类的对角协方差矩阵。</p><p id="187f" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">逻辑函数— <a class="ae jg" href="https://www.youtube.com/watch?v=UdtR5ZOMFWE" rel="noopener ugc nofollow" target="_blank">视频</a> — <a class="ae jg" href="https://medium.com/dataseries/an-introduction-to-the-sigmoid-function-cd654877efa3" rel="noopener">抄本</a> <br/>高斯人&amp;他们的交集— <a class="ae jg" href="https://youtu.be/gdxtzDFfR8E" rel="noopener ugc nofollow" target="_blank">视频</a> — <a class="ae jg" href="https://medium.com/dataseries/gaussians-their-intersections-the-logistic-function-9cd75492b1a0" rel="noopener">抄本</a> <br/>指数家庭— <a class="ae jg" href="https://youtu.be/euPnbXQuppk" rel="noopener ugc nofollow" target="_blank">视频</a> — <a class="ae jg" href="https://medium.com/dataseries/what-do-gaussians-poisson-and-binomial-probability-mass-function-have-in-common-12588f51500f" rel="noopener">抄本</a>—<br/>逻辑回归&amp;损失函数— <a class="ae jg" href="https://youtu.be/dZQNlV9DCQo" rel="noopener ugc nofollow" target="_blank">视频</a> — <a class="ae jg" href="https://medium.com/codex/classification-is-just-another-regression-problem-f324760bc695" rel="noopener">抄本</a> <br/>逻辑回归—函数拟合— <a class="ae jg" href="https://youtu.be/hv0R8jktQug" rel="noopener ugc nofollow" target="_blank">视频</a> — <a class="ae jg" href="https://medium.com/codex/how-to-fit-a-decision-boundary-to-data-using-the-logistic-function-6925d7721953" rel="noopener"/></p><figure class="me mf mg mh gt iv gh gi paragraph-image"><div class="gh gi ng"><img src="../Images/61f6e71a3dd5cc8cb53c35ca7f43b3fe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/1*PoGPSvaipBtSsAKFh-LxOg.gif"/></div><p class="jc jd gj gh gi je jf bd b be z dk translated">并不是每一个低维度的投影都能够保留类别信息(这里是:颜色)。图片来源:<a class="ae jg" href="https://commons.wikimedia.org/wiki/File:Radon_transform_sinogram.gif" rel="noopener ugc nofollow" target="_blank">维基共享资源</a></p></figure><h1 id="9051" class="mi mj jj bd mk ml mm mn mo mp mq mr ms ky mt kz mu lb mv lc mw le mx lf my mz bi translated">第3章—类最佳特征变换</h1><p id="67b0" class="pw-post-body-paragraph lh li jj lj b lk na kt lm ln nb kw lp lq nc ls lt lu nd lw lx ly ne ma mb mc im bi translated">在本章中，我们将讨论特征转换及其对后续分类任务的影响。如果我们能够将所有类别转换到一个空间中，在该空间中，它们共享它们的分散参数，则所有决策边界都将是线性的，并且随后的分类任务将更容易解决。线性判别分析(LDA)是一种变换，它假设情况是这样的，并确定进入这种分离超空间的最佳投影。</p><p id="7e09" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">线性判别分析简介— <a class="ae jg" href="https://youtu.be/LoegZ_-z7h8" rel="noopener ugc nofollow" target="_blank">视频</a> — <a class="ae jg" href="https://medium.com/codex/if-your-problem-is-too-hard-linearise-it-a4bcfcbe507f" rel="noopener">抄本</a> <br/>线性判别分析—属性— <a class="ae jg" href="https://youtu.be/zTsZdXMKbS4" rel="noopener ugc nofollow" target="_blank">视频</a> — <a class="ae jg" href="https://medium.com/codex/how-many-dimensions-do-we-need-in-a-linearly-separating-space-fd760380dac7" rel="noopener">抄本</a> <br/>约束、凸优化&amp;正则化— <a class="ae jg" href="https://youtu.be/1kiKF1p6btY" rel="noopener ugc nofollow" target="_blank">视频</a> — <a class="ae jg" href="https://medium.com/codex/what-the-heck-are-lagrange-multipliers-and-can-they-hurt-me-1e142f45791" rel="noopener">抄本</a> <br/>降秩形式的LDA—<a class="ae jg" href="https://youtu.be/6YSDM1hCO6M" rel="noopener ugc nofollow" target="_blank">视频</a> — <a class="ae jg" href="https://medium.com/p/9fd71959bf9c/edit" rel="noopener">抄本</a> <br/> LDA —原公式— <a class="ae jg" href="https://youtu.be/XnhY66H-OGs" rel="noopener ugc nofollow" target="_blank">视频</a> —</p><figure class="me mf mg mh gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nh"><img src="../Images/69451a53a7f2dd3300036394bd78a40b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Fq4UInzakgo4cL5fvJBHcQ.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">即使对于高度非线性的问题，我们能找到最优和唯一的决策边界吗？内核技巧有助于解决这个问题。图片来源:<a class="ae jg" href="https://commons.wikimedia.org/wiki/File:Kernel_Machine.png" rel="noopener ugc nofollow" target="_blank">维基共享资源</a></p></figure><h1 id="f83d" class="mi mj jj bd mk ml mm mn mo mp mq mr ms ky mt kz mu lb mv lc mw le mx lf my mz bi translated">第四章——最优且唯一的分离超平面</h1><p id="ad67" class="pw-post-body-paragraph lh li jj lj b lk na kt lm ln nb kw lp lq nc ls lt lu nd lw lx ly ne ma mb mc im bi translated">在这一章中，我们研究是否能找到唯一的最优决策边界。为了做到这一点，我们首先要重温几个基本的数学原理。正则化是一种数学工具，它允许我们找到唯一的解决方案，即使是对于高度不适定的问题。为了使用这个技巧，我们回顾了规范以及它们如何被用来指导回归问题。Rosenblatt的感知器和多层感知器(也称为人工神经网络)天生就有这种不适定性。作为一种补救措施，通常可以采用适当的规范和正规化。在大多数情况下，产生的优化问题是使用梯度下降程序解决的。有趣的是，规范的使用也会影响到最终梯度的方向。有了这些新的理论见解，我们接着引入支持向量机的概念。他们使用对偶，即原始问题到对偶变量的映射，即拉格朗日乘数，来解决问题。这允许找到全局最优的、唯一的解决方案。支持向量公式的一个副作用是，原始问题也可以被映射到高维空间，其中非线性决策边界变成线性边界。这被称为内核空间。使用Mercer定理，我们探索了不同的选择来构建这样一个空间，包括序列核。此外，核技巧可以用于机器学习中的其他众所周知的概念，例如主成分分析(PCA ),并且它甚至可以与流形学习相结合，从而产生拉普拉斯支持向量机。</p><p id="1abd" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">正则化回归— <a class="ae jg" href="https://youtu.be/1yXuIpv2IW8" rel="noopener ugc nofollow" target="_blank">视频</a> — <a class="ae jg" href="https://medium.com/codex/if-regularization-doesnt-work-you-are-not-using-enough-of-it-ae837091e118" rel="noopener">抄本</a> <br/>规范和单位球— <a class="ae jg" href="https://youtu.be/WSzA9YgKeEs" rel="noopener ugc nofollow" target="_blank">视频</a> — <a class="ae jg" href="https://medium.com/codex/if-you-want-to-understand-norms-you-have-to-get-them-by-their-balls-a8894c034a18" rel="noopener">抄本</a> <br/>规范依赖回归— <a class="ae jg" href="https://youtu.be/H6wqBwasWHQ" rel="noopener ugc nofollow" target="_blank">视频</a> — <a class="ae jg" href="https://medium.com/codex/linear-regression-norm-based-regularization-from-closed-form-solutions-to-non-linear-problems-40621c342294" rel="noopener">抄本</a> <br/>罗森布拉特的感知器— <a class="ae jg" href="https://youtu.be/uclj81wBdxQ" rel="noopener ugc nofollow" target="_blank">视频</a> — <a class="ae jg" href="https://medium.com/codex/in-the-beginning-was-the-perceptron-through-them-all-nets-were-made-51bd9c6bff9f" rel="noopener">抄本</a> <br/>多层感知器— <a class="ae jg" href="https://youtu.be/FONoPLXmPPY" rel="noopener ugc nofollow" target="_blank">视频</a> — <a class="ae jg" href="https://medium.com/codex/i-am-your-father-deep-net-the-multi-layer-perceptron-9ac61e1577cc" rel="noopener">抄本【T23 <br/>支持向量机的概念— </a><a class="ae jg" href="https://youtu.be/KMjDEymy5wc" rel="noopener ugc nofollow" target="_blank">视频</a> — <a class="ae jg" href="https://medium.com/codex/a-simple-introduction-to-support-vector-machines-6fd330110293" rel="noopener">抄本</a> <br/>优化中的对偶— <a class="ae jg" href="https://youtu.be/h5Up5FzCvWE" rel="noopener ugc nofollow" target="_blank">视频</a> — <a class="ae jg" href="https://medium.com/codex/the-other-side-of-optimality-d0f07dedbe95" rel="noopener">抄本</a> <br/>支持向量机—优化— <a class="ae jg" href="https://youtu.be/dQi9OyQah-I" rel="noopener ugc nofollow" target="_blank">视频</a> — <a class="ae jg" href="https://medium.com/codex/how-to-train-your-svm-1641a090a063" rel="noopener">抄本</a> <br/>梅塞定理和核SVM — <a class="ae jg" href="https://youtu.be/Zi3Fu0M3QFI" rel="noopener ugc nofollow" target="_blank">视频</a> — <a class="ae jg" href="https://medium.com/codex/can-my-kernel-really-be-implemented-as-a-transform-c38a3a85e9e9" rel="noopener">抄本</a> <br/>核PCA和序列核— 【T55</p><figure class="me mf mg mh gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ni"><img src="../Images/6e2c55bb0859c7f32afba985085527c3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*qLboYaamaw5sPhJikKEE2A.gif"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">使用期望最大化算法可以稳健地估计高斯混合模型的参数。图片来源:<a class="ae jg" href="https://commons.wikimedia.org/wiki/File:KernelDensityGaussianAnimated.gif" rel="noopener ugc nofollow" target="_blank">维基共享资源</a></p></figure><h1 id="4e8a" class="mi mj jj bd mk ml mm mn mo mp mq mr ms ky mt kz mu lb mv lc mw le mx lf my mz bi translated">第5章——期望最大化、隐藏信息及其应用</h1><p id="409e" class="pw-post-body-paragraph lh li jj lj b lk na kt lm ln nb kw lp lq nc ls lt lu nd lw lx ly ne ma mb mc im bi translated">模式识别中的另一个关键问题是估计隐藏的、不可观察的信息。期望最大化算法是解决这类问题的通用方法。我们首先推导出高斯混合的情况下，其次是该方法的一般推导。<br/>独立分量分析也揭示了一种隐藏的信息。它基于独立分量的混合可能产生高斯分布的观察。因此，ICA使用这种先验知识，通过最大化变换分量的非高斯性来解开混合信息。ICA的典型应用是盲源分离，例如几个声源的分离。</p><p id="13b3" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">高斯混合EM算法— <a class="ae jg" href="https://youtu.be/rZXfFBkJ0aA" rel="noopener ugc nofollow" target="_blank">视频</a> — <a class="ae jg" href="https://medium.com/codex/how-to-estimate-gaussians-and-their-mixtures-1b2faf31d8a5" rel="noopener">抄本</a> <br/>期望最大化算法— <a class="ae jg" href="https://youtu.be/NaSORPKoij8" rel="noopener ugc nofollow" target="_blank">视频</a> — <a class="ae jg" href="https://medium.com/codex/on-hidden-information-and-how-to-estimate-it-638d97f5c9fb" rel="noopener">抄本</a><br/>EM算法应用实例— <a class="ae jg" href="https://youtu.be/J2X-760nj5w" rel="noopener ugc nofollow" target="_blank">视频</a> — <a class="ae jg" href="https://medium.com/codex/on-hidden-biases-in-mri-segmentation-56acfb3bef72" rel="noopener">抄本</a> <br/>独立分量分析简介— <a class="ae jg" href="https://youtu.be/ku6cfaqXTCo" rel="noopener ugc nofollow" target="_blank">视频</a> — <a class="ae jg" href="https://medium.com/codex/blind-source-separation-in-a-nutshell-a5e9dbf924a7" rel="noopener">抄本</a> <br/>独立分量分析与高斯性— <a class="ae jg" href="https://youtu.be/Zw3mCi0aAO8" rel="noopener ugc nofollow" target="_blank">视频</a></p><figure class="me mf mg mh gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi md"><img src="../Images/f6fbb284623d364722e71140603031bc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EGoK7SNPhwFwRKJzBHM6Wg.jpeg"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">训练数据很贵。我们能做什么来处理有限的数据？图片由<a class="ae jg" href="https://pixabay.com/de/users/1820796-1820796/?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=2070384" rel="noopener ugc nofollow" target="_blank"> 1820796 </a>来自<a class="ae jg" href="https://pixabay.com/de/?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=2070384" rel="noopener ugc nofollow" target="_blank"> Pixabay </a></p></figure><h1 id="bacf" class="mi mj jj bd mk ml mm mn mo mp mq mr ms ky mt kz mu lb mv lc mw le mx lf my mz bi translated">第6章——有限数据及其处理方法</h1><p id="65cf" class="pw-post-body-paragraph lh li jj lj b lk na kt lm ln nb kw lp lq nc ls lt lu nd lw lx ly ne ma mb mc im bi translated">在这最后一章中，我们重温选择最佳分类方法的一般问题。“没有免费的午餐”——定理证明这样的方法在一般情况下是不存在的。然而，偏差-方差权衡告诉我们，只有先验知识才能同时减少两个误差源。一般来说，在几乎所有的模式识别问题中，我们都必须使用有限的数据源。为了估计有效的误差估计，必须使用重采样方法。基于对重采样的观察，boosting制定了一种方法，旨在重新加权来自先前训练迭代的错误分类实例，以提高通用分类器的性能。有趣的是，这种方法实际上等同于使用指数损失函数。最后，我们来看看boosting在Viola-Jones人脸检测算法中最流行的应用。</p><p id="ad88" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">没有免费的午餐定理&amp;偏差-方差权衡— <a class="ae jg" href="https://youtu.be/ibjN4G6ahCM" rel="noopener ugc nofollow" target="_blank">视频</a> — <a class="ae jg" href="https://medium.com/codex/the-cake-is-a-lie-6ba31c1be921" rel="noopener">抄本</a> <br/>有限数据上的性能度量— <a class="ae jg" href="https://youtu.be/RlxilgP25eo" rel="noopener ugc nofollow" target="_blank">视频</a> — <a class="ae jg" href="https://medium.com/codex/how-to-estimate-out-of-distribution-errors-without-collecting-new-data-b4c3a4d307ca" rel="noopener">抄本</a>—T13】Adaboost的概念— <a class="ae jg" href="https://youtu.be/SiB6K2TImYk" rel="noopener ugc nofollow" target="_blank">视频</a> — <a class="ae jg" href="https://medium.com/codex/can-the-wisdom-of-the-crowd-also-improve-machine-learning-14b19596db9d" rel="noopener">抄本</a> <br/> Adaboost &amp;指数损失— <a class="ae jg" href="https://youtu.be/fwoshNG5QpI" rel="noopener ugc nofollow" target="_blank">视频</a> — <a class="ae jg" href="https://medium.com/codex/which-loss-function-does-adaboost-actually-optimise-6c645527127f" rel="noopener">抄本</a> <br/>维奥拉-琼斯算法— <a class="ae jg" href="https://youtu.be/gAYJga9YVEc" rel="noopener ugc nofollow" target="_blank"/></p><p id="335a" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">如果你喜欢这篇文章，你可以在这里找到更多的文章，或者看看我们的讲座。如果你想在未来了解更多的文章、视频和研究，我也会很感激关注<a class="ae jg" href="https://www.youtube.com/c/AndreasMaierTV" rel="noopener ugc nofollow" target="_blank"> YouTube </a>、<a class="ae jg" href="https://twitter.com/maier_ak" rel="noopener ugc nofollow" target="_blank"> Twitter </a>、<a class="ae jg" href="https://www.facebook.com/andreas.maier.31337" rel="noopener ugc nofollow" target="_blank">脸书</a>或<a class="ae jg" href="https://www.linkedin.com/in/andreas-maier-a6870b1a6/" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>。本文以<a class="ae jg" href="https://creativecommons.org/licenses/by/4.0/deed.de" rel="noopener ugc nofollow" target="_blank"> Creative Commons 4.0归属许可</a>发布，如果引用，可以转载和修改。如果你有兴趣从视频讲座中获得文字记录，试试<a class="ae jg" href="http://autoblog.tf.fau.de/" rel="noopener ugc nofollow" target="_blank">自动博客</a>。</p></div></div>    
</body>
</html>