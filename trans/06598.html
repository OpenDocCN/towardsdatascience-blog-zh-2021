<html>
<head>
<title>Build Better Decision Trees with Pruning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">通过修剪构建更好的决策树</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/build-better-decision-trees-with-pruning-8f467e73b107?source=collection_archive---------3-----------------------#2021-06-14">https://towardsdatascience.com/build-better-decision-trees-with-pruning-8f467e73b107?source=collection_archive---------3-----------------------#2021-06-14</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="a125" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">通过限制最大深度和修剪减少决策树的过度拟合和复杂性</h2></div><p id="b0e8" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">作者:<a class="ae le" href="https://www.linkedin.com/in/edkrueger/" rel="noopener ugc nofollow" target="_blank">爱德华·克鲁格</a>、<a class="ae le" href="https://www.linkedin.com/in/sheetal-bongale/" rel="noopener ugc nofollow" target="_blank">希德尔·邦戈尔</a>和<a class="ae le" href="https://www.linkedin.com/in/dougaf/" rel="noopener ugc nofollow" target="_blank">道格拉斯·富兰克林</a>。</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi lf"><img src="../Images/bc541952784ba9573b34d7241fd83b64.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SGzVKXT4cxeKVxZtCjd52g.jpeg"/></div></div><p class="lr ls gj gh gi lt lu bd b be z dk translated">照片由Ales Krivec在Unsplash上拍摄</p></figure><p id="9e46" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu"> <em class="lv">在另一篇文章</em> </strong> <em class="lv">中，我们讨论了决策树或CART算法的基本概念，以及在回归或分类问题中使用决策树的优势和局限性。</em></p><p id="7fdf" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><em class="lv">点击这里阅读更多内容:</em></p><div class="lw lx gp gr ly lz"><a rel="noopener follow" target="_blank" href="/learn-how-decision-trees-are-grown-22bc3d22fb51"><div class="ma ab fo"><div class="mb ab mc cl cj md"><h2 class="bd iu gy z fp me fr fs mf fu fw is bi translated">了解决策树是如何生长的</h2><div class="mg l"><p class="bd b dl z fp me fr fs mf fu fw dk translated">towardsdatascience.com</p></div></div><div class="mh l"><div class="mi l mj mk ml mh mm lp lz"/></div></div></a></div><p id="1dda" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">有关决策树的视频<strong class="kk iu">介绍</strong>，请查看本<strong class="kk iu"> 8分钟课程</strong>:</p><figure class="lg lh li lj gt lk"><div class="bz fp l di"><div class="mn mo l"/></div></figure><p id="9b51" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">在本文中，</strong>我们将重点关注:</p><ul class=""><li id="6628" class="mp mq it kk b kl km ko kp kr mr kv ms kz mt ld mu mv mw mx bi translated"><strong class="kk iu">决策树中的过度拟合</strong></li><li id="4e8a" class="mp mq it kk b kl my ko mz kr na kv nb kz nc ld mu mv mw mx bi translated"><strong class="kk iu">限制最大深度</strong>如何<strong class="kk iu">防止</strong>决策树过拟合</li><li id="69c8" class="mp mq it kk b kl my ko mz kr na kv nb kz nc ld mu mv mw mx bi translated"><strong class="kk iu">成本复杂性修剪</strong>如何能够<strong class="kk iu">防止过度适应</strong>决策树</li><li id="9aaa" class="mp mq it kk b kl my ko mz kr na kv nb kz nc ld mu mv mw mx bi translated">在<strong class="kk iu"> Python </strong>中实现一棵<strong class="kk iu">全树</strong>、一棵<strong class="kk iu">有限最大深度树</strong>和一棵<strong class="kk iu">修剪树</strong></li><li id="21b1" class="mp mq it kk b kl my ko mz kr na kv nb kz nc ld mu mv mw mx bi translated">修剪的<strong class="kk iu">优点</strong>和<strong class="kk iu">限制</strong></li></ul><p id="86aa" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><em class="lv">下面使用的</em> <strong class="kk iu"> <em class="lv">代码</em> </strong> <em class="lv">在本</em> <a class="ae le" href="https://github.com/edkrueger/tree-demos" rel="noopener ugc nofollow" target="_blank"> <strong class="kk iu"> <em class="lv"> GitHub资源库</em> </strong> </a> <em class="lv">中可用。</em></p><h1 id="e249" class="nd ne it bd nf ng nh ni nj nk nl nm nn jz no ka np kc nq kd nr kf ns kg nt nu bi translated">过度拟合和决策树</h1><p id="f5d4" class="pw-post-body-paragraph ki kj it kk b kl nv ju kn ko nw jx kq kr nx kt ku kv ny kx ky kz nz lb lc ld im bi translated">决策树容易过度拟合。如果我们允许决策树增长到其最大深度，它将总是过度适应训练数据。</p><p id="efef" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">当决策树被训练为完全适合训练数据集中的所有样本时，该决策树是过度适合的。您可以调整一些参数，如<a class="ae le" href="https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html" rel="noopener ugc nofollow" target="_blank"> min_samples_leaf </a>，以最小化默认过拟合。这种类型的调整被称为<strong class="kk iu">预修剪，</strong>但是超出了本文的范围。</p><p id="4552" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">你让你的树长得越深，决策规则的序列就变得越复杂。给一棵树指定一个最大深度<strong class="kk iu">可以简化它并防止过度拟合。</strong></p><p id="61b1" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们来看看一个没有使用Python修剪的<strong class="kk iu">完整决策树</strong>:</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi oa"><img src="../Images/5ade15d88e4a634e429ee73d6d541f72.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5dvwxyQuYLkDKz6dnj82lg.png"/></div></div></figure><p id="bdc4" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这些ipynb单元包含导入、数据文件的路径以及构建和交叉验证树模型所需的变量。</p><p id="c102" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在让我们用scikit-learn来种出我们的第一棵树。</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi ob"><img src="../Images/541c214017167c25ec0ae0f6feb148d0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GSN8piSpaW4dsO9sHNGoAw.png"/></div></div></figure><p id="231f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在这里，我们创建了<code class="fe oc od oe of b">full_tree</code>，一个scikit-learns <code class="fe oc od oe of b">DecisionTreeClassifier</code>的实例。然后，我们将模型与训练数据进行拟合。</p><p id="2a50" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">接下来，我们使用matplotlib来确定可视化的大小，使用scikit-learn的<code class="fe oc od oe of b">plot_tree</code>来绘制我们的树。</p><p id="6a1d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">注意这棵树的大小。这个二元分类器中有多少规则？</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi og"><img src="../Images/e48d4412533b5ea585d141ec7c8df058.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*zYIOk0An6y2_6AkU.png"/></div></div></figure><p id="fd9c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如果你数的是“18”，你答对了。下面是这棵树遇到<strong class="kk iu">测试数据</strong>时的分类报告。</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi oh"><img src="../Images/e88c040c2b12c312556e8e46d687bdc4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*m6r0NN67Osy55060z5OpaA.png"/></div></div></figure><p id="1bcf" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">注意精确度，91。在我们构建下两棵树时，请记住这一点。</p><p id="b459" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">默认情况下，树会一直生长，直到每个叶节点中的所有点都来自同一个类。换句话说，直到每片叶子都<strong class="kk iu">纯</strong>。</p><p id="e6c8" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">怎样才能产生一个规则更少结果更好的树？</p><h1 id="6afd" class="nd ne it bd nf ng nh ni nj nk nl nm nn jz no ka np kc nq kd nr kf ns kg nt nu bi translated"><strong class="ak">如何用最优最大深度简化决策树</strong></h1><p id="a924" class="pw-post-body-paragraph ki kj it kk b kl nv ju kn ko nw jx kq kr nx kt ku kv ny kx ky kz nz lb lc ld im bi translated">现在让我们建立一棵树，并限制其最大深度。</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi oi"><img src="../Images/8c79ed4869f75cd40f3b01ddf6454527.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kV5qT7hswW87k-sDcJY1sQ.png"/></div></div></figure><p id="d441" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在上面的第一个单元格中，我们找到了整个树的深度，并将其保存为<code class="fe oc od oe of b">max_depth</code>。我们这样做是为了从1 → <code class="fe oc od oe of b">max_depth</code>建立一个网格搜索。</p><p id="6ae1" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这种网格搜索建立深度范围为1 → 7的树，并比较每棵树的训练精度，以找到产生最高训练精度的深度。</p><p id="e65d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">最精确的树的深度为4，如下图所示。</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi og"><img src="../Images/07cfdd0df5fd05e618caec828769552a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*kOhodGDxLYF7ccTZ.png"/></div></div></figure><p id="d245" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这棵树有10条规则。这意味着它是一个比完整树更简单的模型。</p><p id="8187" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">它在测试数据上的表现如何？</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi oj"><img src="../Images/2cd41bb7d34d0d6212d2dcc131887584.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WifkqnJpFKV_AIsWD64O9g.png"/></div></div></figure><p id="596c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">看那个！规则更少，准确率更高。当我们限制我们的最大深度时，我们确实建造了一个更好的模型。</p><p id="3e99" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">然而，有时设置一个最大深度可能会过于苛刻，导致拟合不足。</p><p id="1dfc" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们如何以更精确的方式调整我们的树模型？</p><h1 id="2335" class="nd ne it bd nf ng nh ni nj nk nl nm nn jz no ka np kc nq kd nr kf ns kg nt nu bi translated">什么是修剪？</h1><p id="181c" class="pw-post-body-paragraph ki kj it kk b kl nv ju kn ko nw jx kq kr nx kt ku kv ny kx ky kz nz lb lc ld im bi translated"><strong class="kk iu">修剪</strong>是一种用于减少过度拟合的技术。修剪还通过删除最弱的规则来简化决策树。修剪通常分为:</p><ul class=""><li id="4d24" class="mp mq it kk b kl km ko kp kr mr kv ms kz mt ld mu mv mw mx bi translated"><strong class="kk iu">预修剪</strong>(提前停止)在树完成对训练集的分类之前停止树，</li><li id="3c7c" class="mp mq it kk b kl my ko mz kr na kv nb kz nc ld mu mv mw mx bi translated"><strong class="kk iu">后期修剪</strong>允许树对训练集进行完美的分类，然后对树进行修剪。</li></ul><p id="1df4" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在本文中，我们将重点关注<strong class="kk iu">后期修剪</strong>。</p><p id="4131" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">修剪从一棵未修剪的树开始，获取一系列子树(修剪过的树)，并通过交叉验证选择最佳的一棵。</p><p id="9711" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">修剪应确保以下几点:</p><ul class=""><li id="424b" class="mp mq it kk b kl km ko kp kr mr kv ms kz mt ld mu mv mw mx bi translated">子树是最优的，这意味着它在交叉验证的训练集上具有最高的准确性。(采油树可以针对对工程师最重要的任何参数进行优化，但并不总是精确的)</li><li id="0f26" class="mp mq it kk b kl my ko mz kr na kv nb kz nc ld mu mv mw mx bi translated">对最优子树的搜索在计算上应该是易处理的。</li></ul><p id="842f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在scikit-learns <code class="fe oc od oe of b">DecisionTreeClassifier</code>中，<code class="fe oc od oe of b">ccp_alpha</code>是<strong class="kk iu">成本复杂度参数。</strong></p><p id="cd9a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">本质上，修剪递归地找到具有“最弱链接”的节点最弱的链路由有效α来表征，其中具有最小有效α的节点首先被修剪。</p><p id="f4e7" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">数学上，树T的<strong class="kk iu">成本复杂性度量</strong>由下式给出:</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi ok"><img src="../Images/4c32b1fb788284a88b6edf704dae847d.png" data-original-src="https://miro.medium.com/v2/resize:fit:684/format:webp/0*Sil0WT-eMuYYcL3j.png"/></div></figure><ul class=""><li id="cb55" class="mp mq it kk b kl km ko kp kr mr kv ms kz mt ld mu mv mw mx bi translated"><strong class="kk iu"> R(T) </strong> —叶节点的总训练误差</li><li id="cecf" class="mp mq it kk b kl my ko mz kr na kv nb kz nc ld mu mv mw mx bi translated"><strong class="kk iu"> |T| </strong> —叶节点数</li><li id="6e3e" class="mp mq it kk b kl my ko mz kr na kv nb kz nc ld mu mv mw mx bi translated"><strong class="kk iu"> α </strong> —复杂度参数(整数)</li></ul><p id="86e2" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">随着alpha的增加，更多的树被修剪，这增加了树叶的总杂质。</p><p id="c896" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如果只是试图减少训练误差<strong class="kk iu"> R(T) </strong>，会导致树相对更大(叶节点更多)，造成过拟合。</p><p id="c228" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">成本复杂性修剪生成一系列树，其中子树<strong class="kk iu"> Tₜ </strong>的成本复杂性度量为:</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi ol"><img src="../Images/836bbcfd2793f2125e2451b2055118eb.png" data-original-src="https://miro.medium.com/v2/resize:fit:742/format:webp/0*wEuCZGHBBCkgV2V_.png"/></div></figure><p id="1d55" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">参数<strong class="kk iu"> α </strong>通过控制叶节点的数量来降低树的复杂度，最终减少过拟合。</p><blockquote class="om on oo"><p id="343c" class="ki kj lv kk b kl km ju kn ko kp jx kq op ks kt ku oq kw kx ky or la lb lc ld im bi translated"><em class="it">最终选择哪个子树取决于α。如果</em><strong class="kk iu"><em class="it">α= 0</em></strong><em class="it">，那么将选择最大的树，因为复杂度惩罚项实质上被丢弃了。当α接近无穷大时，将选择大小为1的树，即单个根节点。</em></p></blockquote><p id="fc18" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">为了了解什么样的<code class="fe oc od oe of b">ccp_alpha</code>值将有助于减小树的大小，scikit-learn提供了一个函数<code class="fe oc od oe of b">cost_complexity_pruning_path</code>，它返回修剪过程中每一步的有效alphas和相应的总叶子杂质。</p><p id="c515" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们构建最终的树模型，看看它的表现如何。</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi ob"><img src="../Images/f917dbc34631d205585709d8d48043ba.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*y8Rq4HaoX75s-wacEGTZZQ.png"/></div></div></figure><p id="4439" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">上面的每个<code class="fe oc od oe of b">ccp_alpha</code>代表一个最优子树。我们再次构建网格搜索来比较不同的树。这里，网格搜索比较每个最优子树的训练精度。</p><p id="834c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们看到最准确的子树是由<code class="fe oc od oe of b">ccp_alpha</code>，0.0059340658…</p><p id="6db6" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在让我们画出这棵修剪过的树。</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi og"><img src="../Images/16a5e1ab7f30e4b4acbabb82a8b5a986.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*hLGNdT-P5RYu5blK.png"/></div></div></figure><p id="c26f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这个模型只包含5条规则！</p><p id="85b9" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">与以前的决策树图相比，修剪后的模型不太复杂，更容易解释，也更容易理解。</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi os"><img src="../Images/ae1047b8f77bacc2e8a5c793fa667685.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*CyOtw8kou0gqo0lk.png"/></div></div></figure><p id="19b2" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">使用最大深度限制树的一半规则，我们已经达到了相同的精度。我们又一次改进了我们的模型！这一次，我们在保持性能的同时降低了复杂性。</p><h1 id="f317" class="nd ne it bd nf ng nh ni nj nk nl nm nn jz no ka np kc nq kd nr kf ns kg nt nu bi translated">修剪决策树的优势</h1><ul class=""><li id="4820" class="mp mq it kk b kl nv ko nw kr ot kv ou kz ov ld mu mv mw mx bi translated">修剪降低了最终树的复杂性，从而减少了过度拟合。</li><li id="80a2" class="mp mq it kk b kl my ko mz kr na kv nb kz nc ld mu mv mw mx bi translated">可解释性——修剪过的树更短，更简单，也更容易解释。</li></ul><h1 id="3f22" class="nd ne it bd nf ng nh ni nj nk nl nm nn jz no ka np kc nq kd nr kf ns kg nt nu bi translated">修剪的局限性</h1><p id="db0d" class="pw-post-body-paragraph ki kj it kk b kl nv ju kn ko nw jx kq kr nx kt ku kv ny kx ky kz nz lb lc ld im bi translated">类似于套索正则化，没有真正的劣势。然而，修剪带来了很高的计算成本。</p><h2 id="e4d7" class="ow ne it bd nf ox oy dn nj oz pa dp nn kr pb pc np kv pd pe nr kz pf pg nt ph bi translated"><strong class="ak">资源</strong></h2><ul class=""><li id="630a" class="mp mq it kk b kl nv ko nw kr ot kv ou kz ov ld mu mv mw mx bi translated">scikit-学习文档为<strong class="kk iu">c</strong><a class="ae le" href="https://scikit-learn.org/stable/auto_examples/tree/plot_cost_complexity_pruning.html#sphx-glr-auto-examples-tree-plot-cost-complexity-pruning-py" rel="noopener ugc nofollow" target="_blank">T5】ost复杂性修剪 </a> <strong class="kk iu">。</strong></li><li id="0d95" class="mp mq it kk b kl my ko mz kr na kv nb kz nc ld mu mv mw mx bi translated">PennState STAT 508 |应用数据挖掘和统计学习<a class="ae le" href="https://online.stat.psu.edu/stat508/lesson/11/11.8/11.8.2" rel="noopener ugc nofollow" target="_blank">https://online.stat.psu.edu/stat508/lesson/11/11.8/11.8.2</a></li></ul></div><div class="ab cl pi pj hx pk" role="separator"><span class="pl bw bk pm pn po"/><span class="pl bw bk pm pn po"/><span class="pl bw bk pm pn"/></div><div class="im in io ip iq"><h2 id="9fed" class="ow ne it bd nf ox oy dn nj oz pa dp nn kr pb pc np kv pd pe nr kz pf pg nt ph bi translated">进一步阅读</h2><p id="29fb" class="pw-post-body-paragraph ki kj it kk b kl nv ju kn ko nw jx kq kr nx kt ku kv ny kx ky kz nz lb lc ld im bi translated">要继续学习相关概念，请查看这篇由<a class="pp pq ep" href="https://medium.com/u/a3b0b8410b61?source=post_page-----8f467e73b107--------------------------------" rel="noopener" target="_blank"> Aliaksandr Kazlou </a>撰写的关于随机森林和过度适配的文章。</p><div class="lw lx gp gr ly lz"><a rel="noopener follow" target="_blank" href="/one-common-misconception-about-random-forest-and-overfitting-47cae2e2c23b"><div class="ma ab fo"><div class="mb ab mc cl cj md"><h2 class="bd iu gy z fp me fr fs mf fu fw is bi translated">关于随机森林和过度适应的一个常见误解</h2><div class="pr l"><h3 class="bd b gy z fp me fr fs mf fu fw dk translated">自举、多数投票规则和100%训练准确率的悖论</h3></div><div class="mg l"><p class="bd b dl z fp me fr fs mf fu fw dk translated">towardsdatascience.com</p></div></div><div class="mh l"><div class="ps l mj mk ml mh mm lp lz"/></div></div></a></div></div></div>    
</body>
</html>