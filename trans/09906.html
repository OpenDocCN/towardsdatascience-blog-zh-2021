<html>
<head>
<title>Equivalence of Regularised Least Squares and Maximising the Posterior Probability Function</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">正则化最小二乘与最大化后验概率函数的等价性</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/equivalence-of-regularised-least-squares-and-maximising-the-posterior-probability-function-32082479849a?source=collection_archive---------21-----------------------#2021-09-17">https://towardsdatascience.com/equivalence-of-regularised-least-squares-and-maximising-the-posterior-probability-function-32082479849a?source=collection_archive---------21-----------------------#2021-09-17</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="1ddf" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph">机器学习推导</h2><div class=""/><div class=""><h2 id="4ae7" class="pw-subtitle-paragraph jw iz iq bd b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn dk translated">为什么正则化最小二乘等价于最大后验解，是正态分布数据的最优算法</h2></div><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ko"><img src="../Images/63456eca41dc960b2bc6b3ebd88dfacb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*dcdYeaoRWBxlCrsA"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">在<a class="ae le" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的<a class="ae le" href="https://unsplash.com/@thisisengineering?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> ThisisEngineering RAEng </a></p></figure><p id="8ed7" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">本文比较蒂科诺夫L2正则化和最大后验概率函数。这是一个相当代数密集，所以我建议拿起笔和纸，尝试自己！</p><h1 id="3f35" class="mb mc iq bd md me mf mg mh mi mj mk ml kf mm kg mn ki mo kj mp kl mq km mr ms bi translated"><strong class="ak">偏差方差困境</strong></h1><p id="23db" class="pw-post-body-paragraph lf lg iq lh b li mt ka lk ll mu kd ln lo mv lq lr ls mw lu lv lw mx ly lz ma ij bi translated">许多机器学习模型的一个问题是它们倾向于过度拟合数据。过度拟合是机器学习中的一种现象，发生在模型对训练数据学习得太好，使其概括能力变差的时候。</p><p id="4b64" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">学习机的<strong class="lh ja">偏差</strong>和<strong class="lh ja">方差</strong>之间有一个微妙的平衡。</p><p id="6ddb" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated"><strong class="lh ja">偏差:</strong>学习机器对未知数据进行归纳的能力</p><p id="e026" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated"><strong class="lh ja">方差:</strong>当我们改变随机机器被训练的数据时，随机机器的预测的期望偏差</p><p id="7386" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">一台性能良好的机器将具有高偏差和低方差。为了获得更高的偏差，我们需要一台复杂的机器，能够对看不见的数据进行归纳。然而，机器必须足够简单，方差不会增加太多。</p><h1 id="5100" class="mb mc iq bd md me mf mg mh mi mj mk ml kf mm kg mn ki mo kj mp kl mq km mr ms bi translated"><strong class="ak">蒂科诺夫L2正规化</strong></h1><p id="73f3" class="pw-post-body-paragraph lf lg iq lh b li mt ka lk ll mu kd ln lo mv lq lr ls mw lu lv lw mx ly lz ma ij bi translated">一种增加机器复杂性而不增加方差的方法是调整机器。</p><p id="9044" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">正规化试图将训练过程中的过度适应最小化。当机器超载时，它们的重量会急剧增加。L2正则化惩罚权重的大小以减少过度拟合。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi my"><img src="../Images/992b721b30e0e57db73b750879d6fa32.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TKpq2_lC3JG8ncsiPNlaxQ.png"/></div></div></figure><p id="cef4" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">L2正则化的工作原理是，首先向最小二乘误差添加一项，惩罚权重w的大小。相对于w求微分并找到最小误差，得到上面所示的伪逆解，其中γ是L2正则化项。</p><h1 id="fbaa" class="mb mc iq bd md me mf mg mh mi mj mk ml kf mm kg mn ki mo kj mp kl mq km mr ms bi translated"><strong class="ak">最大后验概率</strong></h1><p id="c2f5" class="pw-post-body-paragraph lf lg iq lh b li mt ka lk ll mu kd ln lo mv lq lr ls mw lu lv lw mx ly lz ma ij bi translated">根据贝叶斯定理，后验概率与先验乘似然成正比。</p><p id="2f0f" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">高斯分布似然和共轭先验的表达式如下所示。似然和先验概率都是多元高斯的形式。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi mz"><img src="../Images/4cfdbf2ba992a778976dd45478a08e3c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1038/format:webp/1*ZVJLQ74QsUDpbqIeFH5JYA.png"/></div></figure><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi na"><img src="../Images/680d5e037323cda535fe0f5603458afb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1160/format:webp/1*x8aPYYCN7u7U4CH4QZOYyg.png"/></div></figure><p id="69a1" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">其中多元高斯函数如下所示:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi nb"><img src="../Images/63518249170899b2e1d811aa57226e17.png" data-original-src="https://miro.medium.com/v2/resize:fit:1220/format:webp/1*0F3L-ymECxbjOhmmEKOp9A.png"/></div></figure><p id="e1a3" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">由数据、平均值和协方差矩阵参数化。</p><p id="87e6" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">通过取似然和先验的乘积，然后取自然对数并最大化，可以找到产生最大后验概率的权重。对数函数用于简化，由于它是单调函数，我们希望最大化它，因此它不会影响最大化的结果。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi nc"><img src="../Images/3417a45a216699afed6bdeec79dd5ecf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZLJcGM0Y7Xw3JXlB1ETJ9A.png"/></div></div></figure><p id="323c" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">后验概率函数结果对数的上方。因此，最大化后验概率相当于最小化下面的等式。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi nd"><img src="../Images/8d15711b2a5e98200741850dad4d9e8d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1084/format:webp/1*wRFEOOCWbDcRTkVbX-sHtg.png"/></div></figure><p id="5312" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">当最小化时，该等式与本文开头所示的正则化最小二乘法中所示的误差函数相同。因此，通过最大化后验概率函数来寻找最佳权重等价于最小化正则化最小二乘函数。</p><h1 id="8c17" class="mb mc iq bd md me mf mg mh mi mj mk ml kf mm kg mn ki mo kj mp kl mq km mr ms bi translated"><strong class="ak">结论</strong></h1><p id="0ebe" class="pw-post-body-paragraph lf lg iq lh b li mt ka lk ll mu kd ln lo mv lq lr ls mw lu lv lw mx ly lz ma ij bi translated">最小化正则化最小二乘误差相当于最大化后验概率。对于正态分布数据和无限数据，正则化最小二乘算法是最优的。这就是这个算法如此强大的原因，也是它一直沿用至今的原因。</p><h2 id="d432" class="ne mc iq bd md nf ng dn mh nh ni dp ml lo nj nk mn ls nl nm mp lw nn no mr iw bi translated">支持我👏</h2><p id="83d0" class="pw-post-body-paragraph lf lg iq lh b li mt ka lk ll mu kd ln lo mv lq lr ls mw lu lv lw mx ly lz ma ij bi translated">希望这对你有帮助，如果你喜欢它，你可以<a class="ae le" href="https://medium.com/@diegounzuetaruedas" rel="noopener"> <strong class="lh ja">跟我来！</strong> </a></p><p id="1628" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">您也可以成为<a class="ae le" href="https://diegounzuetaruedas.medium.com/membership" rel="noopener"> <strong class="lh ja">中级会员</strong> </a> <strong class="lh ja"> </strong>使用我的推荐链接，访问我的所有文章和更多:<a class="ae le" href="https://diegounzuetaruedas.medium.com/membership" rel="noopener">https://diegounzuetaruedas.medium.com/membership</a></p><h2 id="74f1" class="ne mc iq bd md nf ng dn mh nh ni dp ml lo nj nk mn ls nl nm mp lw nn no mr iw bi translated">你可能喜欢的其他文章</h2><p id="4a40" class="pw-post-body-paragraph lf lg iq lh b li mt ka lk ll mu kd ln lo mv lq lr ls mw lu lv lw mx ly lz ma ij bi translated"><a class="ae le" rel="noopener" target="_blank" href="/differentiable-generator-networks-an-introduction-5a9650a24823">可微发电机网络:简介</a></p><p id="2ebe" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated"><a class="ae le" rel="noopener" target="_blank" href="/fourier-transforms-an-intuitive-visualisation-ba186c7380ee">傅立叶变换:直观的可视化</a></p><h1 id="d665" class="mb mc iq bd md me mf mg mh mi mj mk ml kf mm kg mn ki mo kj mp kl mq km mr ms bi translated"><strong class="ak">参考文献</strong></h1><p id="f315" class="pw-post-body-paragraph lf lg iq lh b li mt ka lk ll mu kd ln lo mv lq lr ls mw lu lv lw mx ly lz ma ij bi translated">主教，哥伦比亚特区，2006年。模式识别和机器学习</p></div></div>    
</body>
</html>