<html>
<head>
<title>Data Formats for Training in TensorFlow: Parquet, Petastorm, Feather, and More</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">TensorFlow中用于训练的数据格式:拼花地板、Petastorm、羽毛等</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/data-formats-for-training-in-tensorflow-parquet-petastorm-feather-and-more-e55179eeeb72?source=collection_archive---------11-----------------------#2021-05-25">https://towardsdatascience.com/data-formats-for-training-in-tensorflow-parquet-petastorm-feather-and-more-e55179eeeb72?source=collection_archive---------11-----------------------#2021-05-25</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="f055" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsdatascience.com/tagged/making-sense-of-big-data" rel="noopener" target="_blank">理解大数据</a></h2><div class=""/><div class=""><h2 id="8ba0" class="pw-subtitle-paragraph jw iz iq bd b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn dk translated">通过统一不同类型的机器学习工作负载的文件格式来简化数据管理</h2></div><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ko"><img src="../Images/d9d4e1b4d0cf0f16de9657cd593055ee.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*dkpcVKOkuQUZ3z_f"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">由<a class="ae le" href="https://unsplash.com/@maksimshutov?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">马克西姆·舒托夫</a>在<a class="ae le" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><p id="814f" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">机器学习是关于数据的。为了成功训练一个复杂的模型，你需要一个<em class="mb">高质量的</em>训练数据集；一个足够大、标注准确并正确表示现实世界中数据样本分布的数据集。然而，适当的数据管理同样重要。通过<em class="mb">数据管理</em>，我们指的是数据存储的方式和位置、数据访问的方式以及数据在开发生命周期中经历的转换。这篇文章的重点是用于存储训练数据的<em class="mb">文件格式</em>以及它对模型训练的影响。在定义机器学习项目时，选择文件格式是您需要做出的许多重要决定之一。</p><p id="9296" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">这个职位由四个部分组成。在第一部分中，我们将确定一些我们希望文件格式具有的属性。在第二部分中，我们将回顾潜在的文件格式，并根据我们发现的所需属性对它们进行评估。接下来，我们将调查将这些格式流式传输到TensorFlow培训课程的不同选项。在最后一节中，我们将描述我们运行的一些实验，以测试其中的一些选项。</p><p id="5802" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">在我们开始之前，让我们先做好准备。在本帖中，我们将假设我们的训练数据集的大小需要将其存储在分布式存储系统中，例如数百TB。在下面的例子中，我们将使用亚马逊S3进行数据存储，但是原则同样适用于任何其他分布式存储系统。在开发周期中，数据可能会被几个消费者访问，用于各种目的，包括数据创建、数据分析、模型训练等等。我们的重点将是TensorFlow中的培训，尽管我们要说的大部分内容也适用于其他培训框架。</p><p id="3f02" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">这篇文章的基本假设是我们的数据需要一种文件格式，即我们需要一个或多个文件用于:1。将单个数据样本的所有元素/列/特征组合在一起，以及2。将多个数据样本分组在一起。当然，人们可以设想这样的场景，其中人们可以选择将所有样本的所有元素保存在单独的文件中，并以它们的原始格式保存。然而，在许多情况下，特别是如果文件很小(例如几千字节)，这种策略可能会严重降低数据访问的运行时性能。</p><p id="fd02" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">这篇文章的目的是强调选择文件格式时一些重要的考虑因素，并讨论一些目前可用的不同选项。我们将提到许多格式以及许多软件开发框架和工具。这些提及应该<em class="mb">而不是</em>被解释为认可。适合您的选择可能基于广泛的考虑，其中一些可能超出了本讨论的范围。这篇文章将包括一些简单的代码片段。这些仅用于演示目的，不应视为最佳实施。</p><p id="f1f3" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">机器学习工具的前景是极其动态的。我们将提到的一些格式和工具将继续发展，我们将做出的一些评论可能在您阅读本文时已经过时。注意跟踪新版本和新工具的发布，并确保根据最新的可用信息做出设计决策。</p><p id="9c5b" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">请不要犹豫，与我联系，提出任何意见或纠正。</p><h1 id="463a" class="mc md iq bd me mf mg mh mi mj mk ml mm kf mn kg mo ki mp kj mq kl mr km ms mt bi translated">文件格式的期望属性</h1><p id="4d5b" class="pw-post-body-paragraph lf lg iq lh b li mu ka lk ll mv kd ln lo mw lq lr ls mx lu lv lw my ly lz ma ij bi translated">为了便于讨论，让我们来看看TensorFlow中用于培训的一种常见文件格式，即<em class="mb"> TFRecord </em>格式。</p><h2 id="a680" class="mz md iq bd me na nb dn mi nc nd dp mm lo ne nf mo ls ng nh mq lw ni nj ms iw bi translated">TFRecord格式</h2><p id="dfad" class="pw-post-body-paragraph lf lg iq lh b li mu ka lk ll mv kd ln lo mw lq lr ls mx lu lv lw my ly lz ma ij bi translated"><a class="ae le" href="https://www.tensorflow.org/tutorials/load_data/tfrecord" rel="noopener ugc nofollow" target="_blank"> <em class="mb"> TFRecord </em> </a>是一种基于<a class="ae le" href="https://developers.google.com/protocol-buffers/" rel="noopener ugc nofollow" target="_blank">协议缓冲区</a>的格式，专门设计用于TensorFlow。一个<em class="mb"> TFRecord </em>文件由序列化的二进制样本序列组成。每个样本代表一个<a class="ae le" href="https://www.tensorflow.org/api_docs/python/tf/train/Example?version=nightly" rel="noopener ugc nofollow" target="_blank"> tf.train.Example </a>，它又代表一个字符串到值映射的字典。<em class="mb"> TFRecord </em>格式的顺序特性有助于提高数据流吞吐量。特别是，不需要下载并打开一个完整的文件来开始遍历其内容。此外，<a class="ae le" href="https://www.tensorflow.org/guide/data#consuming_tfrecord_data" rel="noopener ugc nofollow" target="_blank"> TensorFlow tf.data </a>模块包括高度优化的<a class="ae le" href="https://www.tensorflow.org/api_docs/python/tf/data/TFRecordDataset" rel="noopener ugc nofollow" target="_blank"> TFRecordDataset </a>类，用于基于<em class="mb"> TFRecord </em>文件中存储的数据构建输入管道。然而，<em class="mb"> TFRecord </em>格式并非没有缺点。这里我们列举几个:</p><p id="8b3d" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated"><strong class="lh ja">单一目的</strong>:TF record<em class="mb">格式不太可能满足开发管道上任何其他数据消费者的需求。开发团队通常以不同的格式维护他们的数据，并专门为培训目的以<em class="mb"> TFRecord </em>格式创建数据的派生。存储数据的多个副本并不理想，尤其是当数据集很大时。除了数据存储的额外成本，这意味着每次核心数据集发生变化时，都需要生成一个新的<em class="mb"> TFRecord </em>副本。我们可能希望根据培训课程的发现更新核心数据记录，在这种情况下，我们还需要维护生成的<em class="mb"> TFRecord </em>记录与核心数据格式中相应条目之间的映射。</em></p><p id="d713" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated"><strong class="lh ja">提取部分记录数据</strong>:当我们的训练模型只需要<em class="mb"> TFRecord </em>元素的子集时，<em class="mb"> TFRecord </em>格式出现了另一个问题。例如，假设我们有一个多头模型，它对输入图像执行不同类型的像素级分割。数据集中的每个记录包含对应于模型的多个头部的多个地面实况图像。现在我们决定训练一个只有一个头部的模型。简单的解决方案是输入完整的<em class="mb"> TFRecord </em>，忽略不必要的字段。然而，提取和解析无关数据的需求会严重影响我们输入管道的吞吐量，从而影响我们训练的速度。为了降低管道中的瓶颈风险，我们需要创建一个额外的数据副本，其中只包含与当前特定培训相关的内容。这进一步加剧了我们上面讨论的数据重复问题。</p><p id="0763" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated"><strong class="lh ja">记录过滤</strong>:有时我们对遍历具有特定条目值的记录感兴趣。简单的方法是遍历所有数据，简单地删除任何不匹配我们的过滤器的记录(例如，使用<a class="ae le" href="https://www.tensorflow.org/api_docs/python/tf/data/Dataset?version=nightly#filter" rel="noopener ugc nofollow" target="_blank"> tf.filter </a>)。和以前一样，这会给数据输入管道带来相当大的开销，并导致训练循环严重不足。在之前的帖子中(这里是<a class="ae le" href="https://medium.com/@julsimon/making-amazon-sagemaker-and-tensorflow-work-for-you-893365184233" rel="noopener">这里是</a>和<a class="ae le" href="https://medium.com/@julsimon/deep-dive-on-tensorflow-training-with-amazon-sagemaker-and-amazon-s3-12038828075c" rel="noopener">这里是</a>，我们展示了根据输入图像是否包含粉色汽车来过滤输入图像的需求。我们提出的解决方案是将不同的数据类存储在不同的文件中。每当需要过滤时，我们可以只遍历与过滤器相关的文件。然而，这个解决方案要求我们能够预测我们将需要应用的过滤器，这并不总是可能的。如果我们需要应用一个我们没有预料到的过滤器，我们可以用一个额外的分区重新创建数据，或者使用丢弃不匹配样本的简单方法。这两个选项都不是最优的。</p><p id="8e8b" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated"><strong class="lh ja">数据转换</strong>:典型的输入数据管道可能包括对输入数据的多种操作，包括数据扭曲、扩充、批处理等等。TensorFlow提供了一组内置的数据处理操作，可以通过<a class="ae le" href="https://www.tensorflow.org/api_docs/python/tf/data/Dataset#map" rel="noopener ugc nofollow" target="_blank"> tf.data.Dataset.map </a>函数添加到输入数据管道计算图中。但是，您可能会发现，使用TensorFlow无法高效地实现所需的数据处理。一种选择是使用<a class="ae le" href="https://www.tensorflow.org/api_docs/python/tf/py_function" rel="noopener ugc nofollow" target="_blank"> tf.py_function </a>应用原生python代码块，但是由于<a class="ae le" href="https://wiki.python.org/moin/GlobalInterpreterLock" rel="noopener ugc nofollow" target="_blank"> Python全局解释器锁</a> (GIL)，这可能会限制您的数据吞吐量性能。许多用例要求在进入张量流计算图之前，使用一种能够在原生python中应用操作的格式。</p><h2 id="b1fb" class="mz md iq bd me na nb dn mi nc nd dp mm lo ne nf mo ls ng nh mq lw ni nj ms iw bi translated">格式要求</h2><p id="a3c9" class="pw-post-body-paragraph lf lg iq lh b li mu ka lk ll mv kd ln lo mw lq lr ls mx lu lv lw my ly lz ma ij bi translated">基于上面的讨论，让我们在我们的文件格式中编译一些我们正在寻找的属性的列表。考虑到存储我们数据的多个副本会增加成本和复杂性，我们将从现在开始限制自己只存储一个数据副本，也就是说，我们必须选择一种能够满足所有数据消费者需求的格式。该列表并不打算包含所有内容。应考虑基于特定用例的附加要求。</p><ul class=""><li id="aeab" class="nk nl iq lh b li lj ll lm lo nm ls nn lw no ma np nq nr ns bi translated"><strong class="lh ja">分布式存储</strong>:文件格式和支持库必须支持在分布式存储设置中存储大型数据集的选项，如亚马逊S3、HDFS等。</li><li id="354b" class="nk nl iq lh b li nt ll nu lo nv ls nw lw nx ma np nq nr ns bi translated"><strong class="lh ja">软件生态系统</strong>:我们需要一个强大的图书馆生态系统，为分析和操作数据提供一系列工具。</li><li id="3051" class="nk nl iq lh b li nt ll nu lo nv ls nw lw nx ma np nq nr ns bi translated"><strong class="lh ja">列支持</strong>:文件格式和支持库必须支持有效提取每个数据样本的特征子集。</li><li id="7a99" class="nk nl iq lh b li nt ll nu lo nv ls nw lw nx ma np nq nr ns bi translated"><strong class="lh ja">行过滤</strong>:文件格式和支持库应支持对样本特征值的高效过滤。</li><li id="f4ee" class="nk nl iq lh b li nt ll nu lo nv ls nw lw nx ma np nq nr ns bi translated"><strong class="lh ja"> TensorFlow集成</strong>:格式必须有助于数据从存储器高效地流入TensorFlow培训课程。</li></ul><p id="64b0" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">我们在讨论中忽略的一个培训需求是<strong class="lh ja">数据洗牌</strong>。通常的做法是在每次遍历(历元)之前混洗训练数据。如果我们能够随机访问数据集中的任何样本，数据重组就很容易了。然而，这种对单个样本的随机访问是以牺牲性能为代价的，尤其是在分布式存储环境中。取而代之的是，人们必须求助于用于混洗的其他机制，包括以下各项的组合:在数据创建期间混洗样本、在构成完整数据集的文件列表上混洗、在多个数据集之间进行<a class="ae le" href="https://www.tensorflow.org/api_docs/python/tf/data/Dataset#interleave" rel="noopener ugc nofollow" target="_blank">交错</a>，以及在训练期间使用适当大的混洗缓冲区(例如，参见<a class="ae le" href="https://www.tensorflow.org/api_docs/python/tf/data/Dataset#shuffle" rel="noopener ugc nofollow" target="_blank"> tf.data.Dataset.shuffle </a>)。我们选择不将混排作为文件格式的要求，因为在我们看来，在我们提出的培训场景中，这一挑战将存在于任何文件格式选择中。</p><p id="f664" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">在下一节中，我们将评估几种用于培训的文件格式的兼容性。在下一节中，我们将讨论将它们流式传输到TensorFlow的选项。</p><h1 id="e2ed" class="mc md iq bd me mf mg mh mi mj mk ml mm kf mn kg mo ki mp kj mq kl mr km ms mt bi translated">文件格式调查:镶木地板、花瓣和羽毛</h1><p id="8572" class="pw-post-body-paragraph lf lg iq lh b li mu ka lk ll mv kd ln lo mw lq lr ls mx lu lv lw my ly lz ma ij bi translated">对深度学习的数据格式的全面调查超出了本文的范围。我们将把讨论限制在几个强有力的候选人身上。更广泛的调查比比皆是。这里有一个例子:</p><div class="ny nz gp gr oa ob"><a rel="noopener follow" target="_blank" href="/guide-to-file-formats-for-machine-learning-columnar-training-inferencing-and-the-feature-store-2e0c3d18d4f9"><div class="oc ab fo"><div class="od ab oe cl cj of"><h2 class="bd ja gy z fp og fr fs oh fu fw iz bi translated">机器学习文件格式指南:列式、训练、推理和特征存储</h2><div class="oi l"><h3 class="bd b gy z fp og fr fs oh fu fw dk translated">TLDR；大多数机器学习模型都是使用文件中的数据进行训练的。这篇文章是流行文件格式的指南…</h3></div><div class="oj l"><p class="bd b dl z fp og fr fs oh fu fw dk translated">towardsdatascience.com</p></div></div><div class="ok l"><div class="ol l om on oo ok op ky ob"/></div></div></a></div><p id="4421" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">您还可以查看我最近的一篇帖子，它涵盖了相对较新的<em class="mb"/><a class="ae le" href="https://github.com/tmbdev/webdataset" rel="noopener ugc nofollow" target="_blank"><em class="mb">web dataset</em></a><em class="mb"/>格式，这是一个有趣的解决方案，专门面向希望尽可能以最原始的形式维护数据的开发人员。</p><p id="6cf2" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">我们将根据我们在上面选择的指标来衡量每种格式，如下图所示，其中总结了我们对<em class="mb"> TFRecord </em>格式的评估。我们用绿色表示完全支持，黄色表示部分支持，红色表示不支持。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi oq"><img src="../Images/e0c3d41f16485e7b456809bc66b7fdc5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RIzxkwquS1pZFf2P9g15mw.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">TFRecord评估</p></figure><h2 id="8938" class="mz md iq bd me na nb dn mi nc nd dp mm lo ne nf mo ls ng nh mq lw ni nj ms iw bi translated">阿帕奇拼花地板</h2><p id="fdfe" class="pw-post-body-paragraph lf lg iq lh b li mu ka lk ll mv kd ln lo mw lq lr ls mx lu lv lw my ly lz ma ij bi translated">更引人注目的文件格式选项之一是<a class="ae le" href="http://parquet.apache.org/" rel="noopener ugc nofollow" target="_blank"> <em class="mb"> Apache Parquet </em> </a>。<em class="mb"> Apache Parquet </em>有一个广泛的软件生态系统，它有多个框架和工具支持各种各样的数据处理操作。它在数据分析师中尤其受欢迎。将我们的培训建立在相同的形式上的可能性是很有吸引力的。</p><p id="a5a3" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated"><em class="mb"> Parquet </em>格式的成功主要归功于它是一种<a class="ae le" href="http://en.wikipedia.org/wiki/Column-oriented_DBMS" rel="noopener ugc nofollow" target="_blank">列存储</a>格式。与CSV或<em class="mb"> TFRecord </em>等其他格式相反，在列数据格式<em class="mb">中，数据的列</em>被存储在一起。数据集中的每个文件都被划分为行块，每个行块包含多个样本。在行块内，样本数据根据列存储，即所有样本的第一字段的值首先出现，然后是所有样本的第二字段的值，依此类推。格式的列性质有助于高效的数据分析，因为可以在列的子集上进行查询，而不需要加载整个数据记录。此外，将列组合在一起可以更有效地压缩数据，从而降低存储成本。查看<a class="ae le" rel="noopener" target="_blank" href="/understanding-apache-parquet-7197ba6462a9">这篇文章</a>了解更多关于列数据存储的优势。</p><p id="564d" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">下图总结了我们对拼花地板的评估:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi or"><img src="../Images/408da82796163fa25fee7c130312d87b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BKR8QquoHb3XAIxkfkjEbQ.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">拼花地板评估</p></figure><p id="d4aa" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">作为一种列格式，<em class="mb"> Parquet </em>能够有效地提取数据列的子集。此外，我们还可以利用格式的分栏特性，通过以下方式促进行过滤:1 .首先提取我们正在过滤的列，然后2。仅对匹配过滤器的行提取其余的列。然而，由于有效的数据提取可能依赖于提取完整行块的列数据，因此不清楚这种方法是否能很好地执行。因此我们用黄色标记了这个能力。更高效的过滤方法可能需要在数据集创建期间根据类别将样本分离到不同的<em class="mb"> Parquet </em>文件中，如上所述。在<a class="ae le" href="https://spark.apache.org/docs/latest/api/python/" rel="noopener ugc nofollow" target="_blank"> pyspark </a>中，这可以使用<a class="ae le" href="https://spark.apache.org/docs/2.4.0/api/python/pyspark.html?highlight=partitionby#pyspark.RDD.partitionBy" rel="noopener ugc nofollow" target="_blank"> partitionBy </a>功能来完成。</p><p id="cf3a" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated"><strong class="lh ja"> Parquet数据集创建</strong>:在下面的代码块中，我们演示了使用<a class="ae le" href="https://spark.apache.org/docs/latest/api/python/" rel="noopener ugc nofollow" target="_blank"> pyspark </a>库从流行的<a class="ae le" href="https://www.cs.toronto.edu/~kriz/cifar.html" rel="noopener ugc nofollow" target="_blank"> Cifar10 </a>数据集创建一个<em class="mb"> Parquet </em>数据集。其他库也支持拼花地板的创建，包括熊猫和pyarrow。</p><pre class="kp kq kr ks gt os ot ou ov aw ow bi"><span id="f433" class="mz md iq ot b gy ox oy l oz pa">from tensorflow.keras import datasets<br/>from pyspark.sql import SparkSession, Row<br/>from pyspark.sql.types import StructType, \<br/>        StructField, IntegerType, BinaryType</span><span id="3235" class="mz md iq ot b gy pb oy l oz pa">def cifar_to_parquet():<br/>    schema = StructType(<br/>                [StructField("image", BinaryType(), True),<br/>                 StructField("label", IntegerType(), True)])<br/>    (data, labels), _ = datasets.cifar10.load_data()<br/>    labels = labels.flatten().tolist()<br/>    num_procs = 4 <em class="mb"># set the number of parallel processes<br/>    </em>spark = SparkSession.builder\<br/>        .master('local[{num_procs}]'.format(num_procs=num_procs))\<br/>        .getOrCreate()<br/>    sc = spark.sparkContext</span><span id="105e" class="mz md iq ot b gy pb oy l oz pa">    num_samples = len(labels)<br/>    output_url = 'file:///tmp/parquet'</span><span id="11d1" class="mz md iq ot b gy pb oy l oz pa">    def row_generator(i):<br/>        return {<br/>            'image': bytearray(data[i].tobytes()),<br/>            'label': labels[i],<br/>        }</span><span id="36e6" class="mz md iq ot b gy pb oy l oz pa">    <em class="mb"># optionally configure the size of row blocks<br/>    # blockSize = 1024 * 1024 * 16 # 16 MB<br/>    # sc._jsc.hadoopConfiguration()\<br/>    #     .setInt("parquet.block.size", blockSize)<br/>    </em>rows_rdd = sc.parallelize(range(num_samples))\<br/>        .map(row_generator)\<br/>        .map(lambda x: Row(**x))</span><span id="a025" class="mz md iq ot b gy pb oy l oz pa">    spark.createDataFrame(rows_rdd, schema)\<br/>        .write.mode('overwrite')\<br/>        .parquet(output_url)</span></pre><p id="aafa" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">虽然组成数据集的文件数量、每个文件的大小以及每个行块的大小会对数据加载的性能产生有意义的影响，但控制这些参数有时会有点棘手。在pyspark中，可以使用<em class="mb"> sc设置行块大小。_jsc.hadoopConfiguration </em>如上面代码块中的<em class="mb"> </em>注释所示(默认值为128 MB)，文件的大小和数量取决于并行进程的数量和<a class="ae le" href="https://spark.apache.org/docs/2.4.0/api/python/pyspark.sql.html?highlight=partition#pyspark.sql.DataFrame.coalesce" rel="noopener ugc nofollow" target="_blank"> coalesce </a>函数。</p><p id="7a5a" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">通过适当的配置，该脚本可以修改为直接写入亚马逊S3(例如，参见此处的<a class="ae le" href="https://routdeepak.medium.com/writing-to-aws-s3-from-spark-91e85d09724b" rel="noopener"/>和此处的<a class="ae le" href="https://sparkbyexamples.com/spark/spark-read-write-parquet-file-from-amazon-s3/" rel="noopener ugc nofollow" target="_blank"/>)。</p><h2 id="e597" class="mz md iq bd me na nb dn mi nc nd dp mm lo ne nf mo ls ng nh mq lw ni nj ms iw bi translated">佩塔斯托姆</h2><p id="5985" class="pw-post-body-paragraph lf lg iq lh b li mu ka lk ll mv kd ln lo mw lq lr ls mx lu lv lw my ly lz ma ij bi translated">petastorm库的创建有一个特定的目标，即统一开发管道上所有数据消费者使用的数据集(见<a class="ae le" href="https://www.infoq.com/presentations/petastorm-ml-pipelines/" rel="noopener ugc nofollow" target="_blank">这里</a>)。尽管petastorm抽象了底层存储格式，但使用的默认格式是<em class="mb"> Apache Parquet </em>。更准确地说，petastorm通过在<a class="ae le" href="https://petastorm.readthedocs.io/en/latest/api.html#module-petastorm.unischema" rel="noopener ugc nofollow" target="_blank"> Unischema </a>结构中提供额外的模式信息、支持多维数据和支持数据压缩编解码器来扩展<em class="mb"> Apache Parquet </em>。此后，我们将假设使用扩展的<em class="mb"> Parquet </em>作为底层格式，并滥用术语<em class="mb"> petastorm </em>来指代库本身以及库创建的文件格式。为了区分这两者，我们在提到<em class="mb"> Petastorm </em>格式时将使用大写。使用<em class="mb"> Parquet </em>作为底层格式意味着<em class="mb"> Petastorm </em>享有我们上面讨论的与使用列格式相关的所有好处。然而，使用petastorm引入的<em class="mb"> Parquet </em>扩展有一个警告:数据集需要由petastorm库创建，开发管道上的所有数据消费者都需要petastorm库来正确读取和解析数据。这个要求有些限制，因为<em class="mb"> Parquet </em>格式最吸引人的属性之一是其用于访问和操作数据的广泛的软件生态系统。正是因为这个原因，我们在下面的评估图中用黄色标记了<em class="mb">软件生态系统</em>:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi pc"><img src="../Images/410fd4587888386e944f1c502eef8e37.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tAv2ZSZsG7p-gSeQRvSbFA.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">Petastorm评估</p></figure><p id="a7a5" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">Petastorm包括许多其他引人注目的特性，包括对行组索引和n元语法的支持。你可以在这里和这里了解更多关于petastorm <a class="ae le" href="https://eng.uber.com/petastorm/" rel="noopener ugc nofollow" target="_blank">的信息。</a></p><p id="f3ac" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated"><strong class="lh ja"> Petastorm数据集创建</strong>:PETA storm中的数据集创建与pyspark中的<em class="mb"> Parquet </em>创建非常相似。主要区别在于使用了<a class="ae le" href="https://petastorm.readthedocs.io/en/latest/api.html#module-petastorm.unischema" rel="noopener ugc nofollow" target="_blank"> Unischema </a>和包装数据集创建的<em class="mb"> materialize_dataset </em>上下文。</p><pre class="kp kq kr ks gt os ot ou ov aw ow bi"><span id="b7c0" class="mz md iq ot b gy ox oy l oz pa">from petastorm.codecs import CompressedImageCodec, \<br/>        NdarrayCodec, ScalarCodec<br/>from petastorm.etl.dataset_metadata import materialize_dataset<br/>from petastorm.unischema import Unischema,\<br/>        UnischemaField, dict_to_spark_row<br/>from pyspark.sql import SparkSession<br/>from pyspark.sql.types import IntegerType</span><span id="d7b2" class="mz md iq ot b gy pb oy l oz pa">def cifar_to_peta():<br/>    MySchema = Unischema('MySchema', [<br/>        UnischemaField('image', np.uint8,<br/>                       (32,32,3), NdarrayCodec(), False),<br/>        UnischemaField('label', np.uint8,<br/>                       (), ScalarCodec(IntegerType()), False),<br/>    ])<br/>    (data, labels), _ = datasets.cifar10.load_data()<br/>    labels = labels.flatten().tolist()<br/>    num_procs = 4 <em class="mb"># set the number of parallel processes<br/>    </em>spark = SparkSession.builder.\<br/>        master('local[{num_procs}]'.format(num_procs=num_procs))\<br/>        .getOrCreate()<br/>    sc = spark.sparkContext</span><span id="34a6" class="mz md iq ot b gy pb oy l oz pa">    num_samples = 100<em class="mb">#len(labels)<br/>    </em>output_url = 'file:///tmp/petastorm'<br/>    rowgroup_size_mb = 128</span><span id="98e0" class="mz md iq ot b gy pb oy l oz pa">    def row_generator(i):<br/>        return {<br/>            'image': data[i],<br/>            'label': np.uint8(labels[i]),<br/>        }</span><span id="9e2a" class="mz md iq ot b gy pb oy l oz pa">    <em class="mb"># Wrap dataset materialization portion.<br/>    # Will take care of setting up spark environment variables as<br/>    # well as save petastorm specific metadata<br/>    </em>with materialize_dataset(spark, output_url,<br/>                             MySchema, rowgroup_size_mb):<br/>        rows_rdd = sc.parallelize(range(num_samples)) \<br/>            .map(row_generator) \<br/>            .map(lambda x: dict_to_spark_row(MySchema, x))</span><span id="69a0" class="mz md iq ot b gy pb oy l oz pa">        spark.createDataFrame(rows_rdd, <br/>                              MySchema.as_spark_schema()) \<br/>            .write \<br/>            .mode('overwrite') \<br/>            .parquet(output_url)</span></pre><p id="00cf" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">行块大小由<em class="mb"> rowgroup_size_mb </em>参数在petastorm中确定。在这个例子中，我们没有利用petastorm中包含的编解码器支持。当使用大数据元素时，使用编解码器支持可以显著压缩数据并节省存储成本。</p><h2 id="fd27" class="mz md iq bd me na nb dn mi nc nd dp mm lo ne nf mo ls ng nh mq lw ni nj ms iw bi translated">羽毛</h2><p id="9c05" class="pw-post-body-paragraph lf lg iq lh b li mu ka lk ll mv kd ln lo mw lq lr ls mx lu lv lw my ly lz ma ij bi translated"><a class="ae le" href="https://arrow.apache.org/docs/python/feather.html" rel="noopener ugc nofollow" target="_blank">羽毛文件格式</a>是我们考虑的另一种纵列格式，因为它包含在<a class="ae le" href="https://www.tensorflow.org/io" rel="noopener ugc nofollow" target="_blank"> TensorFlow I/O </a>中(更多信息见下文)。虽然<em class="mb"> Feather </em>和<em class="mb"> Parquet </em>之间有许多相似之处，但是它们不同的底层实现也有许多细微的差异。两者之间的一个很好的比较，包括场景中<em class="mb">羽毛</em>可能是更好的选择，可以在<a class="ae le" href="https://ursalabs.org/blog/2020-feather-v2/#:~:text=Parquet%20is%20fast%20to%20read,is%20even%20faster%20to%20read.&amp;text=In%20the%20case%20of%20Feather,converting%20to%20R%20or%20pandas." rel="noopener ugc nofollow" target="_blank">这里</a>找到。区分<em class="mb">羽化</em>格式的版本1和版本2很重要。版本2支持更多的数据类型以及不同类型的数据压缩。每当你遇到<em class="mb">羽</em>的评论，切记它可能是基于格式的版本1。</p><p id="d103" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">羽毛和T21的主要区别在于软件生态系统的范围。尽管它得到了诸如<a class="ae le" href="https://arrow.apache.org/docs/python/feather.html" rel="noopener ugc nofollow" target="_blank"> pyarrow </a>和<a class="ae le" href="https://pythontic.com/pandas/serialization/feather" rel="noopener ugc nofollow" target="_blank"> pandas </a>等库的支持，但在撰写本文时，<em class="mb"> Feather </em>格式远没有<em class="mb"> Parquet </em>那么流行，而且支持框架的数量也非常有限。我们在下图中总结了我们的<em class="mb">羽</em>文件格式评估:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi pc"><img src="../Images/410fd4587888386e944f1c502eef8e37.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tAv2ZSZsG7p-gSeQRvSbFA.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">羽毛评价</p></figure><p id="f6eb" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">作为列格式，对于我们如何选择对<em class="mb">列支持</em>和<em class="mb">行过滤</em>属性进行评级，存在与上述相同的考虑因素。</p><p id="116d" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated"><strong class="lh ja">羽毛数据集创建</strong>:在下面的代码块中，我们演示了使用<a class="ae le" href="https://arrow.apache.org/docs/python/feather.html" rel="noopener ugc nofollow" target="_blank"> pyarrow </a>创建一个<em class="mb">羽毛</em>文件。<em class="mb">羽毛</em>创作支持也被内置到<a class="ae le" href="https://pythontic.com/pandas/serialization/feather" rel="noopener ugc nofollow" target="_blank">熊猫</a>库中。在之前的数据集创建中，流程并行化是数据创建代码不可或缺的一部分，与此相反，我们在这里演示了单个文件的创建。完整的解决方案需要生成多个进程，负责创建构成完整数据集的文件的不相交子集。</p><pre class="kp kq kr ks gt os ot ou ov aw ow bi"><span id="f068" class="mz md iq ot b gy ox oy l oz pa">from tensorflow.keras import datasets<br/>import pyarrow as pa<br/>from pyarrow.feather import write_feather</span><span id="ba35" class="mz md iq ot b gy pb oy l oz pa">def cifar_to_feather():<br/>    (data, labels), _ = datasets.cifar10.load_data()<br/>    data = [data[i].flatten() for i in range(data.shape[0])]<br/>    labels = labels.flatten()<br/>    table = pa.Table.from_arrays([data,labels], ['data','labels'])<br/>    write_feather(table, '/tmp/df.feather', chunksize=10000)</span><span id="f63a" class="mz md iq ot b gy pb oy l oz pa">write_feather(table, '/tmp/df.feather', chunksize=10000)</span></pre><p id="59f6" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">在上面的例子中，行块的大小由参数<em class="mb"> chunksize </em>决定。注意，与前面的格式创建相反，我们通过每个块的记录数而不是每个块的内存量来确定大小。</p><h1 id="c373" class="mc md iq bd me mf mg mh mi mj mk ml mm kf mn kg mo ki mp kj mq kl mr km ms mt bi translated">使用替代文件格式的TensorFlow培训</h1><p id="e78e" class="pw-post-body-paragraph lf lg iq lh b li mu ka lk ll mv kd ln lo mw lq lr ls mx lu lv lw my ly lz ma ij bi translated">现在，我们将注意力转向上面列出的最后一个要求，文件格式与TensorFlow培训的兼容性。无论您选择的文件格式有多么吸引人，如果您不能将其应用到培训课程中，或者输入流的速度不能满足您的需求，那么您就又回到了起点。在本节中，我们将探索一些工具，用于在TensorFlow中使用上述文件格式进行培训。在接下来的部分中，我们将在一些实验中测量输入流的速度。我们的讨论是基于<a class="ae le" href="https://www.tensorflow.org/" rel="noopener ugc nofollow" target="_blank"> TensorFlow </a>的2.4.1版本、<a class="ae le" href="https://www.tensorflow.org/io" rel="noopener ugc nofollow" target="_blank"> TensorFlow I/O </a>的0.17.1版本和<a class="ae le" href="https://petastorm.readthedocs.io/en/latest/index.html#" rel="noopener ugc nofollow" target="_blank"> petastorm </a>的0.11.0版本。</p><h2 id="e3b3" class="mz md iq bd me na nb dn mi nc nd dp mm lo ne nf mo ls ng nh mq lw ni nj ms iw bi translated">张量流数据集API</h2><p id="4c8a" class="pw-post-body-paragraph lf lg iq lh b li mu ka lk ll mv kd ln lo mw lq lr ls mx lu lv lw my ly lz ma ij bi translated">在一个典型的TensorFlow应用程序中，我们定义了一个<a class="ae le" href="https://www.tensorflow.org/api_docs/python/tf/data/Dataset" rel="noopener ugc nofollow" target="_blank"> tf.data.Dataset </a>，它表示一系列数据样本，并将其送入训练循环。建立一个<a class="ae le" href="https://www.tensorflow.org/api_docs/python/tf/data/Dataset" rel="noopener ugc nofollow" target="_blank"> tf.data.Dataset </a>包括定义数据源和对数据应用转换。源可以是存储在内存或文件中的数据。查看<a class="ae le" href="https://www.tensorflow.org/guide/data" rel="noopener ugc nofollow" target="_blank">此处</a>了解更多关于如何在TensorFlow中创建数据集的信息。既然我们已经选择了<em class="mb"> TFRecord </em>格式作为我们的参考点，那么让我们从回顾如何将<em class="mb"> TFRecord </em>文件输入到TensorFlow培训会话开始。</p><p id="72be" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated"><strong class="lh ja"> TFRecordDataset: </strong>一个<a class="ae le" href="https://www.tensorflow.org/api_docs/python/tf/data/TFRecordDataset" rel="noopener ugc nofollow" target="_blank"> TFRecordDataset </a>将一列<em class="mb"> TFRecord </em>文件作为输入，并产生一系列序列化的<em class="mb"> TFRecord </em>数据样本。之后通常会有一个<a class="ae le" href="https://www.tensorflow.org/api_docs/python/tf/data/Dataset?version=nightly#map" rel="noopener ugc nofollow" target="_blank"> tf.data.Dataset.map </a>调用，在这个调用中会对每个样本进行解析。下面的代码块演示了从以<em class="mb"> TFRecord </em>格式存储的<a class="ae le" href="https://www.cs.toronto.edu/~kriz/cifar.html" rel="noopener ugc nofollow" target="_blank"> Cifar10 </a>数据创建<a class="ae le" href="https://www.tensorflow.org/api_docs/python/tf/data/TFRecordDataset" rel="noopener ugc nofollow" target="_blank"> TFRecordDataset </a>:</p><pre class="kp kq kr ks gt os ot ou ov aw ow bi"><span id="600b" class="mz md iq ot b gy ox oy l oz pa">import tensorflow as tf<br/>def get_dataset():<br/>    autotune = tf.data.experimental.AUTOTUNE<br/>    def parse(example_proto):<br/>        feature_description = {<br/>            'image': tf.io.FixedLenFeature([], tf.string),<br/>            'label': tf.io.FixedLenFeature([], tf.int64)}<br/>        features = tf.io.parse_single_example(example_proto,          <br/>                                         feature_description)<br/>        image = tf.io.decode_raw(features['image'], tf.uint8)<br/>        image = tf.reshape(image, [32, 32, 3])<br/>        return image, label<br/><br/>    records = tf.data.Dataset.list_files(&lt;path_to_files&gt;+'/*')<br/>    ds = tf.data.TFRecordDataset(records, <br/>                     num_parallel_reads=autotune)<br/>    ds = ds.map(parse, num_parallel_calls=autotune)<br/>    return ds</span></pre><p id="c848" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">我们对<a class="ae le" href="https://www.tensorflow.org/api_docs/python/tf/data/TFRecordDataset" rel="noopener ugc nofollow" target="_blank"> TFRecordDataset </a>的体验总体上是积极的；拉取和解析文件的底层机制似乎相当可靠，我们很少发现自己在管道的这一部分遇到瓶颈。我们还发现该API非常健壮，在各种各样的培训环境中都表现良好。</p><p id="489d" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">TensorFlow有几个其他的tf.data.Dataset类用于直接从文件加载数据，包括<a class="ae le" href="https://www.tensorflow.org/api_docs/python/tf/data/FixedLengthRecordDataset" rel="noopener ugc nofollow" target="_blank"> FixedLengthRecordDataset </a>和<a class="ae le" href="https://www.tensorflow.org/api_docs/python/tf/data/TextLineDataset" rel="noopener ugc nofollow" target="_blank"> TextLineDataset </a>。从与任何现有类都不匹配的文件格式构建TensorFlow数据集需要更多的创造力。这里我们将按照复杂程度的升序来提到三个选项。</p><p id="d0a8" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated"><strong class="lh ja">从内存源</strong>创建数据集:第一个选项是下载并解析Python中的文件(在TensorFlow之外)并将数据样本从内存源加载到TensorFlow中。一种方法是使用<a class="ae le" href="https://www.tensorflow.org/api_docs/python/tf/data/Dataset#from_generator" rel="noopener ugc nofollow" target="_blank">TF . data . dataset . from _ generator</a>，如下面的伪代码块所示。</p><pre class="kp kq kr ks gt os ot ou ov aw ow bi"><span id="97e8" class="mz md iq ot b gy ox oy l oz pa">import tensorflow as tf<br/>def get_custom_ds(file_names):<br/>    def my_generator():<br/>        for f in file_names:<br/>            # download f<br/>            samples = ... # parse samples from f<br/>            for sample in samples:<br/>                yield sample</span><span id="a85b" class="mz md iq ot b gy pb oy l oz pa">    return tf.data.Dataset.from_generator(<br/>                  my_generator,<br/>                  output_types=[tf.uint8,tf.uint8],<br/>                  output_shapes=[[32,32,3],[]])</span></pre><p id="6d37" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated"><strong class="lh ja">解析TensorFlow </strong>中的文件:第二种选择是依赖TensorFlow的<a class="ae le" href="https://www.tensorflow.org/api_docs/python/tf/io" rel="noopener ugc nofollow" target="_blank"> tf.io </a>模块来下载和解析文件。与之前的解决方案相比，这里的文件管理是TensorFlow执行图的一部分。下面是一种使用<a class="ae le" href="https://www.tensorflow.org/api_docs/python/tf/data/Dataset#list_files" rel="noopener ugc nofollow" target="_blank">TF . data . dataset . list _ files</a>和<a class="ae le" href="https://www.tensorflow.org/api_docs/python/tf/data/Dataset#interleave" rel="noopener ugc nofollow" target="_blank">TF . data . dataset . interleave</a>API的方法:</p><pre class="kp kq kr ks gt os ot ou ov aw ow bi"><span id="c06b" class="mz md iq ot b gy ox oy l oz pa">import tensorflow as tf<br/>def get_custom_ds():<br/>    autotune = tf.data.experimental.AUTOTUNE<br/>    filenames = tf.data.Dataset.list_files(&lt;path_to_files&gt;+'/*',<br/>                           shuffle=True)<br/>    def make_ds(path):<br/>        bytestring = tf.io.read_file(path)<br/>        samples = ... # parse bytestring using tf functions<br/>        return tf.data.Dataset.from_tensor_slices(samples)</span><span id="b2c0" class="mz md iq ot b gy pb oy l oz pa">    ds = filenames.interleave(make_ds, <br/>                         num_parallel_calls=autotune,<br/>                         deterministic=False)<br/>    return ds</span></pre><p id="c4e7" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated"><strong class="lh ja">创建自定义数据集类</strong>:我们提到的最后一个选项是创建一个新的tf.data.Dataset类，专门用于处理您的数据格式。这种选择需要最高超的技术。根据数据输入流的速度来衡量，它还提供了最高的潜在回报。实现这一点的一种方法是修改TensorFlow C++代码，<a class="ae le" href="https://www.tensorflow.org/install/source" rel="noopener ugc nofollow" target="_blank">从源代码</a>重新构建TensorFlow。例如，可以克隆TFRecordDataset实现，只覆盖与解析格式特别相关的代码部分。通过这种方式，人们希望能够享受与TFRecordDataset相同的性能优势。这种方法的缺点是需要维护一个专门的TensorFlow版本。特别是，每次升级到TensorFlow的新版本时，您都需要重新构建您的自定义解决方案。请注意，自定义数据集类创建也可以在TensorFlow I/O中实现，而不是在TensorFlow中实现，如<a class="ae le" rel="noopener" target="_blank" href="/how-to-build-a-custom-dataset-for-tensorflow-1fe3967544d8">这篇</a>帖子中所述。</p><p id="d5d7" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">虽然上述任何解决方案都可以进行适当的调整以最大限度地提高性能，但这并不总是那么容易。更糟糕的是，您可能会发现理想的配置(例如，底层系统流程的数量)可能会因培训环境的不同而有很大差异。从这个意义上来说，使用一个专用的数据集类(比如TFRecordDataset)比我们已经描述过的定制解决方案有更大的优势。接下来我们将看到的两个解决方案将使用专门为我们选择的文件格式设计的数据集类。</p><h2 id="db34" class="mz md iq bd me na nb dn mi nc nd dp mm lo ne nf mo ls ng nh mq lw ni nj ms iw bi translated">张量流输入/输出</h2><p id="12f9" class="pw-post-body-paragraph lf lg iq lh b li mu ka lk ll mv kd ln lo mw lq lr ls mx lu lv lw my ly lz ma ij bi translated"><a class="ae le" href="http://TensorFlow I/O" rel="noopener ugc nofollow" target="_blank"> TensorFlow I/O </a> (tfio)是TensorFlow的一个扩展包，增加了对TensorFlow中不包含的许多文件系统和文件格式的支持。特别是，tfio定义了用于创建基于<em class="mb"> Feather </em>格式的数据集的<a class="ae le" href="https://www.tensorflow.org/io/api_docs/python/tfio/arrow/ArrowFeatherDataset" rel="noopener ugc nofollow" target="_blank">tfio . arrow . arrow Feather dataset</a>类和用于创建基于<em class="mb"> Parquet </em>格式的数据集的<a class="ae le" href="https://www.tensorflow.org/io/api_docs/python/tfio/v0/IODataset#from_parquet" rel="noopener ugc nofollow" target="_blank">tfio . v 0 . io dataset . from _ Parquet</a>函数。</p><p id="267a" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated"><strong class="lh ja">tensor flow I/O Feather Dataset</strong>:<a class="ae le" href="https://www.tensorflow.org/io/api_docs/python/tfio/arrow/ArrowFeatherDataset" rel="noopener ugc nofollow" target="_blank">tfio . Arrow . Arrow Feather Dataset</a>类只是设计用于支持<a class="ae le" href="https://arrow.apache.org/" rel="noopener ugc nofollow" target="_blank"> Apache Arrow </a>格式的<a class="ae le" href="https://www.tensorflow.org/io/api_docs/python/tfio/arrow" rel="noopener ugc nofollow" target="_blank">API集合</a>中的一个。要全面了解tfio <a class="ae le" href="https://arrow.apache.org/" rel="noopener ugc nofollow" target="_blank"> Apache Arrow </a>产品，请务必查看<a class="ae le" href="https://blog.tensorflow.org/2019/08/tensorflow-with-apache-arrow-datasets.html" rel="noopener ugc nofollow" target="_blank">这个</a>博客。在下面的代码块中，我们演示了基于我们在上面创建的以<em class="mb"> Feather </em>格式存储的Cifar10数据的<a class="ae le" href="https://www.tensorflow.org/io/api_docs/python/tfio/arrow/ArrowFeatherDataset" rel="noopener ugc nofollow" target="_blank">tfio . arrow . arrow Feather dataset</a>的使用。</p><pre class="kp kq kr ks gt os ot ou ov aw ow bi"><span id="8fb9" class="mz md iq ot b gy ox oy l oz pa">import tensorflow as tf<br/>import tensorflow_io.arrow as arrow_io</span><span id="d5b6" class="mz md iq ot b gy pb oy l oz pa">def get_dataset():<br/>    filenames = &lt;list of feather files&gt;<br/>    ds = arrow_io.ArrowFeatherDataset(filenames, <br/>                          columns=(0, 1),<br/>                          output_types=(tf.uint8, tf.uint8),<br/>                          output_shapes=([32*32*3,], []),<br/>                          batch_mode='auto')<br/>    ds = ds.unbatch()<br/>    return ds</span></pre><p id="8b04" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">通过将<em class="mb"> batch_mode </em>参数设置为<em class="mb">‘auto’</em>，我们选择让数据集返回<em class="mb"> Parquet </em>行块。因此，我们应用的第一个调用是<a class="ae le" href="https://www.tensorflow.org/api_docs/python/tf/data/Dataset#unbatch" rel="noopener ugc nofollow" target="_blank">解锁</a>记录，以便返回单个样本。这种策略应该比单独读取样本产生更好的性能。</p><p id="ea35" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">我们发现，如果将<a class="ae le" href="https://www.tensorflow.org/io/api_docs/python/tfio/arrow/ArrowFeatherDataset" rel="noopener ugc nofollow" target="_blank">tfio . arrow . arrow feather dataset</a>与<a class="ae le" href="https://www.tensorflow.org/api_docs/python/tf/data/Dataset#interleave" rel="noopener ugc nofollow" target="_blank">TF . data . dataset . interleave</a>结合使用，吞吐量性能会有所提高:</p><pre class="kp kq kr ks gt os ot ou ov aw ow bi"><span id="48fb" class="mz md iq ot b gy ox oy l oz pa">import tensorflow as tf<br/>import tensorflow_io as tfio</span><span id="f7bc" class="mz md iq ot b gy pb oy l oz pa">def get_dataset():<br/>    autotune = tf.data.experimental.AUTOTUNE<br/>    filenames = tf.data.Dataset.list_files(&lt;path_to_files&gt;+'/*',<br/>                           shuffle=True)<br/>    def make_ds(file):<br/>        ds = arrow_io.ArrowFeatherDataset(<br/>                     [file], [0,1],<br/>                     output_types=(tf.uint8, tf.uint8),<br/>                     output_shapes=([32*32*3,], []),<br/>                     batch_mode='auto')<br/>        return ds<br/><em class="mb">    </em>ds = filenames.interleave(make_ds,       <br/>                              num_parallel_calls=autotune,<br/>                              deterministic=False)<br/>    ds = ds.unbatch()    <br/>    return ds</span></pre><p id="4bea" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated"><strong class="lh ja"> TensorFlow I/O拼花数据集</strong>:与<em class="mb"> Feather </em>数据集类相反，<a class="ae le" href="https://www.tensorflow.org/io/api_docs/python/tfio/v0/IODataset#from_parquet" rel="noopener ugc nofollow" target="_blank"> from_parquet </a>函数接收单个<em class="mb">拼花</em>文件。但是，我们可以通过在以<em class="mb"> Parquet </em>格式存储的Cifar10数据集上使用<a class="ae le" href="https://www.tensorflow.org/api_docs/python/tf/data/Dataset#interleave" rel="noopener ugc nofollow" target="_blank">TF . data . dataset . interleave</a>来克服这一限制，如下所示:</p><pre class="kp kq kr ks gt os ot ou ov aw ow bi"><span id="cb7d" class="mz md iq ot b gy ox oy l oz pa">import tensorflow as tf<br/>import tensorflow_io as tfio</span><span id="2c6e" class="mz md iq ot b gy pb oy l oz pa">def get_dataset():<br/>    autotune = tf.data.experimental.AUTOTUNE<br/>    filenames = tf.data.Dataset.list_files(&lt;path_to_files&gt;+'/*',<br/>                           shuffle=True)<br/>    def parquet_ds(file):<br/>        ds = tfio.IODataset.from_parquet(file, <br/>               {'image': tf.string, <br/>                'label': tf.int32})<br/>        return ds<br/><em class="mb">    </em>ds = filenames.interleave(parquet_ds,       <br/>                              num_parallel_calls=autotune,<br/>                              deterministic=False)<br/>    def parse(example):<br/>        image = tf.io.decode_raw(example['image'], tf.uint8)<br/>        image = tf.reshape(image, [32, 32, 3])<br/>        label = example['label']<br/>        return image, label<br/>    ds = ds.map(parse,num_parallel_calls=autotune)<br/>    <br/>    return ds</span></pre><h2 id="3491" class="mz md iq bd me na nb dn mi nc nd dp mm lo ne nf mo ls ng nh mq lw ni nj ms iw bi translated">佩塔斯托姆</h2><p id="651d" class="pw-post-body-paragraph lf lg iq lh b li mu ka lk ll mv kd ln lo mw lq lr ls mx lu lv lw my ly lz ma ij bi translated">petastorm库<a class="ae le" href="https://petastorm.readthedocs.io/en/latest/readme_include.html#tensorflow-api" rel="noopener ugc nofollow" target="_blank"> TensorFlow API </a>定义了<em class="mb"> make_petastorm_dataset </em>函数，用于从petastorm阅读器(petastorm.reader.Reader)创建TensorFlow tf.data.Dataset。这个数据集的源可以是<em class="mb"> Petastorm </em>格式，也可以是raw <em class="mb"> Parquet </em>格式。为了从<em class="mb"> Petastorm </em>格式的数据集中读取，我们使用make_reader API创建读取器。为了从一个<em class="mb"> Parquet </em>格式的数据集中读取数据，我们使用make_batch_reader API创建读取器。这里的表<a class="ae le" href="https://petastorm.readthedocs.io/en/latest/readme_include.html#non-petastorm-parquet-stores" rel="noopener ugc nofollow" target="_blank">中描述了两个阅读器之间的一些微妙差异</a>。请注意，从<em class="mb"> Petastorm </em>格式创建的TensorFlow tf.data.Dataset返回单个数据样本的序列，而从raw <em class="mb"> Parquet </em>格式创建的TensorFlow tf.data.Dataset返回批量数据样本，其大小由<em class="mb"> Parquet </em>行组大小决定。</p><p id="6ab6" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">在下面的代码块中，我们演示了如何使用<em class="mb">make _ PETA storm _ dataset</em>API从以<em class="mb"> Petastorm </em>格式存储的Cifar10数据创建TensorFlow tf.data.Dataset。</p><pre class="kp kq kr ks gt os ot ou ov aw ow bi"><span id="447d" class="mz md iq ot b gy ox oy l oz pa">from petastorm import make_reader<br/>from petastorm.tf_utils import make_petastorm_dataset</span><span id="8e42" class="mz md iq ot b gy pb oy l oz pa">def get_dataset():<br/>    with make_reader('&lt;path to data&gt;') as reader:<br/>        ds = make_petastorm_dataset(reader)<br/>    return ds</span></pre><p id="a282" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">在下面的代码块中，我们演示了如何使用<em class="mb">make _ PETA storm _ dataset</em>API从以<em class="mb"> Parquet </em>格式存储的Cifar10数据创建TensorFlow tf.data.Dataset。</p><pre class="kp kq kr ks gt os ot ou ov aw ow bi"><span id="c490" class="mz md iq ot b gy ox oy l oz pa">from petastorm import make_batch_reader<br/>from petastorm.tf_utils import make_petastorm_dataset</span><span id="3629" class="mz md iq ot b gy pb oy l oz pa">def get_dataset():<br/>    autotune = tf.data.experimental.AUTOTUNE<br/>    with make_batch_reader('&lt;path to data&gt;') as reader:<br/>        ds = make_petastorm_dataset(reader)<br/>    ds = ds.unbatch()<br/>    def parse(example):<br/>        image, label = example<br/>        image = tf.io.decode_raw(image, tf.uint8)<br/>        image = tf.reshape(image, [32, 32, 3])<br/>        return image, label<br/>    ds = ds.map(parse,num_parallel_calls=autotune)<br/>    return ds</span></pre><p id="1e53" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">注意我们如何使用<a class="ae le" href="https://www.tensorflow.org/api_docs/python/tf/data/Dataset#unbatch" rel="noopener ugc nofollow" target="_blank"> unbatch </a>例程来返回单个样本。</p><h1 id="b7e7" class="mc md iq bd me mf mg mh mi mj mk ml mm kf mn kg mo ki mp kj mq kl mr km ms mt bi translated">结果</h1><p id="42eb" class="pw-post-body-paragraph lf lg iq lh b li mu ka lk ll mv kd ln lo mw lq lr ls mx lu lv lw my ly lz ma ij bi translated">在这一部分，我们分享几个实验的结果。所有实验都在一个c5.2xlarge <a class="ae le" href="https://aws.amazon.com/ec2/instance-types/" rel="noopener ugc nofollow" target="_blank"> Amazon EC2实例类型</a>(有8个vCPUs)上运行，TensorFlow版本为2.4.1，TensorFlow I/O版本为0.17.1，petastorm版本为0.11.0。实验分为两部分。首先，我们尝试了以我们讨论过的文件格式存储的Cifar10数据输入TensorFlow会话的不同方法。我们创建了数据的多个副本，以便人为地夸大数据集。对于这些实验，我们选择将训练批量设置为1024。</p><p id="3964" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">为了评估样本记录的大小如何影响相对性能，我们运行了第二组测试，其中我们向每个Cifar10数据样本添加了一个大小为2 MB的随机字节数组。对于这些实验，我们选择将训练批量设置为16。</p><p id="c487" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">对于所有实验，数据集被分成大小为100-200 MB的基础文件。</p><p id="c7b3" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">因为我们只对测量训练数据吞吐量感兴趣，所以我们选择放弃构建训练模型，而是直接在TensorFlow数据集上迭代，如下面的代码块所示。</p><pre class="kp kq kr ks gt os ot ou ov aw ow bi"><span id="ddb9" class="mz md iq ot b gy ox oy l oz pa">import time<br/>ds = get_dataset().batch(batch_size)<br/>round = 0<br/>start_time = time.time()<br/>for x in ds:<br/>    round = round + 1<br/>    if round % 100 == 0:<br/>        print("round {}: epoch time: {}".<br/>                format(round, time.time() - start_time))<br/>        start_time = time.time()<br/>    if round == 2000:<br/>        break</span></pre><p id="1226" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">注意，在petastorm的情况下，数据集遍历必须移动到petastorm阅读器上下文中。</p><p id="dbb3" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">我们分享这些结果的目的是让你知道你的出发点是什么，以及可能需要的优化工作量。我们强烈建议不要从这些结果中得出任何关于您自己的使用案例的结论，原因如下:</p><ol class=""><li id="1b18" class="nk nl iq lh b li lj ll lm lo nm ls nn lw no ma pd nq nr ns bi translated">解决方案的性能可能会因模型、数据集和培训环境的不同而有很大差异。</li><li id="6fd6" class="nk nl iq lh b li nt ll nu lo nv ls nw lw nx ma pd nq nr ns bi translated">对于给定的用例，每个解决方案的性能将根据如何配置格式和TensorFlow数据集的具体情况而有所不同，包括:每个文件的大小、行组的大小、压缩方案的使用、工作进程的数量、每个样本批次的大小等等。</li><li id="7030" class="nk nl iq lh b li nt ll nu lo nv ls nw lw nx ma pd nq nr ns bi translated">实验通过直接迭代TensorFlow数据集来测量解决方案的最大吞吐量。在实践中，吞吐量需要足够高，以保持培训会话繁忙。换句话说，只要我们没有遇到IO瓶颈，我们可能会非常满意低于最大值的吞吐量。</li><li id="3246" class="nk nl iq lh b li nt ll nu lo nv ls nw lw nx ma pd nq nr ns bi translated">当你读到这篇文章的时候，我用过的库的版本可能已经过时了。较新版本的库可能包含影响其性能的更改。</li><li id="4dcb" class="nk nl iq lh b li nt ll nu lo nv ls nw lw nx ma pd nq nr ns bi translated">最后但并非最不重要的是，我的实验中出现错误的可能性。当你找到他们时，请给我写信。</li></ol><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi pe"><img src="../Images/1b661c54b92934da0112f5a386ff67f5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yCpmARbVGX5xCrGxMNN_8w.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">测试结果(每步秒数越少越好)</p></figure><p id="0b08" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">结果证明了<em class="mb"> TFRecord </em>格式和<em class="mb"> TFRecordDataset </em>的实力。特别是，我们可以看到它们在不同环境下的稳定性。与此同时，其他一些实验产生的训练量相比之下并不逊色。虽然我们发现结果令人鼓舞，但在我们测试的两个案例中，必须注意替代解决方案的不稳定性。在一个场景中表现良好的解决方案(与<em class="mb"> TFRecord </em>格式相比)在另一个场景中似乎会失效。看起来，与使用<em class="mb"> TFRecord </em>格式<em class="mb">相比，</em>将其他格式用于不同的培训设置可能需要一些额外的工作。鉴于我们在本帖中讨论的替代格式的优势，这种努力可能是值得的。</p><p id="c492" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">另一个感兴趣的指标是与每个选项相关的CPU利用率。在最近的一篇博客文章中，我们讨论了由于一个或多个CPU达到最大利用率而导致培训管道中出现瓶颈的可能性。在担心这种可能性的情况下，明智的做法是评估与数据存储相关的不同决策如何影响加载和解析数据的CPU利用率。其中包括文件格式的选择、数据压缩(和解压缩)、TensorFlow数据集创建方法等等。在下面的图表中，我们列出了Cifar10实验中三个性能最高的解决方案的平均CPU利用率。CPU利用率是从<a class="ae le" href="https://aws.amazon.com/cloudwatch/" rel="noopener ugc nofollow" target="_blank">亚马逊CloudWatch </a>的<a class="ae le" href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/viewing_metrics_with_cloudwatch.html#ec2-cloudwatch-metrics" rel="noopener ugc nofollow" target="_blank">亚马逊EC2实例指标</a>中获取的。由于我们使用的实例有8个vCPUs，因此最大利用率为800%。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi pf"><img src="../Images/957d6ba12e7a44cca5c0b3e9415d450a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1322/format:webp/1*7qKKI037H5MhEC5AMNx2ew.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">CPU利用率(800%之外—越低越好)</p></figure><h1 id="0073" class="mc md iq bd me mf mg mh mi mj mk ml mm kf mn kg mo ki mp kj mq kl mr km ms mt bi translated">摘要</h1><p id="1d77" class="pw-post-body-paragraph lf lg iq lh b li mu ka lk ll mv kd ln lo mw lq lr ls mx lu lv lw my ly lz ma ij bi translated">尽管TensorFlow明显偏向于<em class="mb"> TFRecord </em>格式，但它的一些属性仍不尽如人意。虽然<em class="mb"> TFRecord </em>格式对于训练来说很棒，但是它不太可能适合开发管道中数据的任何其他消费者。在本帖中，我们讨论了几种更好地解决整体项目需求的格式，并评估了它们与TensorFlow培训的兼容性。虽然可能需要一些努力来支持不同的训练场景，但是我们的结论是有合法的替代方法来代替<em class="mb"> TFRecord </em>格式。</p></div></div>    
</body>
</html>