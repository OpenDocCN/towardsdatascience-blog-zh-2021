<html>
<head>
<title>5 Outlier Detection Techniques that every “Data Enthusiast” Must Know</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">每个“数据爱好者”都必须知道的5种异常检测技术</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/5-outlier-detection-methods-that-every-data-enthusiast-must-know-f917bf439210?source=collection_archive---------3-----------------------#2021-06-12">https://towardsdatascience.com/5-outlier-detection-methods-that-every-data-enthusiast-must-know-f917bf439210?source=collection_archive---------3-----------------------#2021-06-12</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="ec1a" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">异常检测方法(视觉和代码)</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/0d03a17a93f69a937285673b3dde2cd6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*T6XguKbmYeiHLp0jWckxVw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">来自<a class="ae ky" href="https://unsplash.com/photos/W8KTS-mhFUE" rel="noopener ugc nofollow" target="_blank">源</a>的修改图像</p></figure><p id="41b5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">异常值</strong>是那些与总体的<a class="ae ky" rel="noopener" target="_blank" href="/8-types-of-sampling-techniques-b21adcdd2124">样本</a>中的其他数据点有强烈<em class="lv">(不同属性)</em>差异的观察值。在这篇博客中，我们将经历<em class="lv"> 5种离群点检测技术，每个“数据爱好者”</em> <strong class="lb iu"> <em class="lv">都必须</em> </strong> <em class="lv"> </em> <strong class="lb iu"> <em class="lv">知道</em> </strong> <em class="lv">。但在此之前，让我们看看并了解离群值的来源。</em></p><h2 id="2bb5" class="lw lx it bd ly lz ma dn mb mc md dp me li mf mg mh lm mi mj mk lq ml mm mn mo bi translated">数据集中异常值的可能来源是什么？</h2><p id="b5a5" class="pw-post-body-paragraph kz la it lb b lc mp ju le lf mq jx lh li mr lk ll lm ms lo lp lq mt ls lt lu im bi translated">数据集中可能存在异常值有多种原因，如人为错误<em class="lv">(错误的数据输入)</em>、测量错误<em class="lv">(系统/工具错误)</em>、数据操作错误<em class="lv">(错误的数据预处理错误)</em>、采样错误<em class="lv">(从不同来源创建样本)等</em>。重要的是，检测和处理这些异常值对于学习一个健壮的和可推广的机器学习系统是重要的。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mu"><img src="../Images/216e333a32c3bbb7f256fc4ca1bdd60c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1348/format:webp/1*7TEP_-AnRpzVbhvxKs1JJQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">z得分</p></figure><p id="2d6f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu"> Z值</strong> <em class="lv">(也称标准值)</em>是统计学中的一个重要概念，表示<strong class="lb iu">某一点距离均值</strong>有多远。通过应用z变换，我们移动分布，使其平均值为单位标准差<strong class="lb iu">0。<em class="lv">例如，Z值为2意味着数据点距离平均值有2个标准偏差。</em></strong></p><p id="aef8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="lv">此外，任何数据点的z分数都可以按如下所示计算— </em></p><blockquote class="mv"><p id="6350" class="mw mx it bd my mz na nb nc nd ne lu dk translated">z得分(I)=(x(I)-平均值)/标准差</p></blockquote><p id="0caf" class="pw-post-body-paragraph kz la it lb b lc nf ju le lf ng jx lh li nh lk ll lm ni lo lp lq nj ls lt lu im bi translated">它假设数据呈正态分布，因此数据点的百分比位于-/+1标准偏差之间。是~68%，-/+2 stdev。是大约95%和-/+3标准偏差。就是~99.7%。因此，如果<strong class="lb iu"> Z值是&gt; 3，我们可以安全地将该点标记为异常值。</strong> <em class="lv">参考下图</em></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/af4297e345900250c5da95ff1a368129.png" data-original-src="https://miro.medium.com/v2/resize:fit:1064/format:webp/1*tRDusoSDEMmPbBLdjWsfjw.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">带有异常标记的正态分布|图片由作者提供</p></figure><p id="2d9d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="lv">你可以使用python来实现它，如下所示— </em></p><pre class="kj kk kl km gt nl nm nn no aw np bi"><span id="bdff" class="lw lx it nm b gy nq nr l ns nt"><em class="lv">import numpy as np</em></span><span id="f62b" class="lw lx it nm b gy nu nr l ns nt"><em class="lv">data = [1, 2, 3, 2, 1, 100, 1, 2, 3, 2, 1]<br/></em><em class="lv">threshold = 3</em></span><span id="830d" class="lw lx it nm b gy nu nr l ns nt"><em class="lv">mean = np.mean(data)<br/>std = np.std(data)</em></span><span id="a3b1" class="lw lx it nm b gy nu nr l ns nt"><em class="lv">z_score_outlier = [i for</em><em class="lv"> </em><em class="lv">i in</em><em class="lv"> </em><em class="lv">data if (i-mean)/std &gt; threshold]<br/>print (z_score_outlier)<br/>&gt;&gt; 100   (outlier)</em></span></pre><p id="39cb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="lv">在Python中也可以使用Scikit learn和Scipy提供的内置函数。(链接在</em> <strong class="lb iu"> <em class="lv">资源部分</em> </strong> <em class="lv"> ) </em></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nv"><img src="../Images/9dc1d57c393dd05cfaa46b4962fb5349.png" data-original-src="https://miro.medium.com/v2/resize:fit:1330/format:webp/1*_0qfCzQok0nykAq-nvzFLw.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">力线</p></figure><p id="290a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在<strong class="lb iu">局部异常因素</strong> (LOF)中，这个想法围绕着<strong class="lb iu">局部区域</strong>的概念。这里，我们计算并比较焦点的<strong class="lb iu">局部密度和它的邻居的局部密度。</strong>如果我们发现焦点的局部密度与其邻居相比非常低，这将暗示焦点在该空间中是孤立的，并且是潜在的异常值。该算法取决于超参数K，该超参数K决定了在计算局部密度时要考虑的邻居数量。该值介于空间中的0 <em class="lv">(无邻居)</em>和总点数<em class="lv">(所有点都是邻居)</em>之间。</p><p id="a0a0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">局部密度函数定义为平均可达距离的倒数<em class="lv">，其中<strong class="lb iu">平均可达距离</strong>定义为<em class="lv"> </em>从焦点到邻居中所有点的平均距离<em class="lv"> </em>。</em></p><blockquote class="mv"><p id="dc75" class="mw mx it bd my mz na nb nc nd ne lu dk translated"><strong class="ak"> LOF =邻居的平均局部密度/焦点的局部密度</strong></p></blockquote><p id="a17e" class="pw-post-body-paragraph kz la it lb b lc nf ju le lf ng jx lh li nh lk ll lm ni lo lp lq nj ls lt lu im bi translated">如果，</p><ul class=""><li id="175d" class="nw nx it lb b lc ld lf lg li ny lm nz lq oa lu ob oc od oe bi translated"><em class="lv"> LOF ≈ 1密度与邻国相似</em></li><li id="3056" class="nw nx it lb b lc of lf og li oh lm oi lq oj lu ob oc od oe bi translated"><em class="lv"> LOF &lt; 1密度高于邻国(正常点)</em></li><li id="fc9e" class="nw nx it lb b lc of lf og li oh lm oi lq oj lu ob oc od oe bi translated"><em class="lv"> LOF &gt; 1密度比邻国低(异常)</em></li></ul><p id="10e0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">此外，下图显示了空间中样本焦点<em class="lv">(深蓝色)</em>的LOF和局部密度的计算。这里K=3 <em class="lv">(邻居)</em>，d <em class="lv">(距离)</em>可以计算为欧几里德，曼哈顿等。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ok"><img src="../Images/0b8ce7aa38db6b6b5504269f421c28ae.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*uHqPCeV6msFprmumPMfg8Q.jpeg"/></div></figure><p id="9365" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="lv">您可以使用python的Scikit-learn库来实现它，如下所示— </em></p><pre class="kj kk kl km gt nl nm nn no aw np bi"><span id="7888" class="lw lx it nm b gy nq nr l ns nt"><em class="lv">from sklearn.neighbors import LocalOutlierFactor</em></span><span id="21f9" class="lw lx it nm b gy nu nr l ns nt"><em class="lv">data = [[1, 1], [2, 2.1], [1, 2], [2, 1], [50, 35], [2, 1.5]]</em></span><span id="6cf5" class="lw lx it nm b gy nu nr l ns nt"><em class="lv">lof = LocalOutlierFactor(n_neighbors=2, metric='manhattan')<br/>prediction = lof.fit_predict(data)<br/>&gt;&gt; [ 1,  1,  1,  1, -1,  1]</em></span></pre><p id="ee2f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="lv">请随意探索超参数调谐库等。(链接在</em> <strong class="lb iu"> <em class="lv">资源部分</em> </strong> <em class="lv"> ) </em></p></div><div class="ab cl ol om hx on" role="separator"><span class="oo bw bk op oq or"/><span class="oo bw bk op oq or"/><span class="oo bw bk op oq"/></div><div class="im in io ip iq"><blockquote class="mv"><p id="6f39" class="mw mx it bd my mz na nb nc nd ne lu dk translated">此外，如果你想继续享受阅读与数据科学和机器学习相关的精彩文章，你可以通过我的<a class="ae ky" href="https://prakhar-mishra.medium.com/membership" rel="noopener">推荐链接</a>:)购买中级会员资格</p></blockquote></div><div class="ab cl ol om hx on" role="separator"><span class="oo bw bk op oq or"/><span class="oo bw bk op oq or"/><span class="oo bw bk op oq"/></div><div class="im in io ip iq"><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi os"><img src="../Images/aff7592cd8ee109163a5812f3bf41fd1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1338/format:webp/1*kk-nWhhFcdoh1sOkiIz6gQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">ABOD和凸包方法</p></figure><p id="9f4f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我已经有一篇关于离群点检测的几何模型的非常详细的博客文章，其中我主要关注于<strong class="lb iu">基于角度的技术<em class="lv"> (ABOD) </em> </strong> <em class="lv">和</em> <strong class="lb iu">基于深度的技术<em class="lv">(凸包)</em> </strong> <em class="lv">。你可以在这里查看ABOD</em><a class="ae ky" href="https://blog.paperspace.com/outlier-detection-with-abod/" rel="noopener ugc nofollow" target="_blank"><em class="lv"/></a><em class="lv">。下图显示了检测异常值的两种技术的图示视图— </em></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ot"><img src="../Images/eafca3cd74d401b2c46b69d0077967a8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FmuzM72cKEb9LgHjA_2LYg.png"/></div></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nv"><img src="../Images/d9a2e8edae3b7b5ff518e7323c065955.png" data-original-src="https://miro.medium.com/v2/resize:fit:1330/format:webp/1*8Pyw1IhSh62o2xtMyJ8G0w.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">隔离森林</p></figure><p id="bbd7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">隔离森林</strong>是一种基于树的算法，它试图根据决策边界<em class="lv">(就像我们对决策树一样)</em>的概念找出离群值。这里的想法是<strong class="lb iu">继续在随机阈值和特征上分割数据，直到每个点都变得孤立</strong> <em class="lv">(这就像在数据集上过度拟合决策树)。</em>一旦隔离完成，我们就把在这个过程中很早就被隔离的点分出来。我们将这些点标记为潜在的异常值。如果您直观地看到这一点，那么一个点离多数越远，就越容易被隔离，而隔离属于一个组的点将需要更多的切割来隔离每个点。</p><p id="610d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="lv">如果你在下图中看到，我们随机选择特征和值到我们切割的地方。经过4次切割后，我们能够分离出异常点，这意味着这些节点会在我们的树构建阶段很早就出现。</em></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ou"><img src="../Images/0a5766db8d2a37970b3c0632ef7ad835.png" data-original-src="https://miro.medium.com/v2/resize:fit:1110/format:webp/1*-4zTo64uS8wpMmcYhEGtqg.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">隔离森林流程|作者图片</p></figure><p id="22ac" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="lv">您可以使用python的Scikit-learn库来实现它，如下所示— </em></p><pre class="kj kk kl km gt nl nm nn no aw np bi"><span id="69db" class="lw lx it nm b gy nq nr l ns nt"><em class="lv">from sklearn.ensemble import IsolationForest</em></span><span id="d3ab" class="lw lx it nm b gy nu nr l ns nt"><em class="lv">data = [[1, 1], [2, 2.1], [1, 2], [2, 1], [50, 35], [2, 1.5]]</em></span><span id="c3d1" class="lw lx it nm b gy nu nr l ns nt"><em class="lv">iforest = IsolationForest(n_estimators=5)<br/>iforest.fit(data)</em></span><span id="59c0" class="lw lx it nm b gy nu nr l ns nt"><em class="lv">actual_data = [[1, 1.5]]<br/>iforest.predict(actual_data)<br/>&gt;&gt; 1   (Normal)</em></span><span id="52dd" class="lw lx it nm b gy nu nr l ns nt"><em class="lv">outlier_data = [[45, 55]]<br/>iforest.predict(outlier_data)<br/>&gt;&gt; -1   (Outlier)</em></span></pre><p id="d489" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="lv">请随意探索超参数调谐库等。(链接在</em> <strong class="lb iu"> <em class="lv">资源部分</em> </strong> <em class="lv"> ) </em></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ov"><img src="../Images/bed5cd2513388c1d6e04768a71fc9895.png" data-original-src="https://miro.medium.com/v2/resize:fit:1328/format:webp/1*v-mTnwzLtwgWw3qDKv4cOg.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">自动编码器</p></figure><p id="d0c0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">自动编码器</strong>是一种神经网络架构，被训练来重现输入本身。它包括两个可训练的组件，即编码器和解码器。其中编码器的目标是学习输入的潜在表示<em class="lv">(原始维度到低维度)</em>，解码器的目标是学习从该潜在表示<em class="lv">(低维度到原始维度)</em>中重构输入。因此，为了让autoencoder正常工作，这两个组件应该针对各自的任务进行优化。</p><p id="0f76" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="lv">自动编码器广泛用于检测异常情况</em>。这种工作方式背后的一个典型直觉是，如果特征空间中的一个点远离大多数点<em class="lv">(这意味着它拥有不同的属性，例如——狗图像聚集在特征空间的某个部分周围，而牛图像离该集群相当远)，</em>在这种情况下，自动编码器学习狗分布<em class="lv">(因为与牛相比，狗图像的数量将非常高——这就是为什么它是异常的，因此模型将主要专注于学习狗集群)。</em>这意味着，模型将能够或多或少地正确重新生成狗的图像，导致低损失值，而对于牛的图像，它将生成高损失<em class="lv">(因为这是它第一次看到的奇怪的东西，并且它已经学习的权重主要是重建狗的图像)。</em>我们使用这些重建损失值作为异常得分，因此得分越高，输入异常的几率就越高。</p><p id="caf3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在下图中—将<strong class="lb iu">绿点视为狗</strong>，将<strong class="lb iu">蓝点视为牛</strong>。我们用这种输入分布来训练我们的系统，并期望它输出相同的结果。但我们可以看到，该模型或多或少完美地重新生成了黄色圆点，但不太适合蓝色圆点<em class="lv">(因为它不在黄色圆点的相同特征范围内)</em></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ow"><img src="../Images/03d68e3fb86178ab57ebc0fc9bf134d1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1222/format:webp/1*A-svrs24XIzxMvC87J6HEg.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">Autoencoder |图片作者培训流程</p></figure><p id="9c85" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在下图中——在推理过程中，当我们给我们的模型一个黄点时，它能够重新生成误差较小的点<em class="lv">(正常点的信号)</em>，而对于蓝色点，它返回较高的误差<em class="lv">(异常/异常值的信号)</em>，因为无法重新生成相同的点。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ox"><img src="../Images/b9872db8627adbd9b9d575d19d2e6914.png" data-original-src="https://miro.medium.com/v2/resize:fit:926/format:webp/1*-807Vam9Fjxyod5UVXP3Jg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">自动编码器的推理流程|按作者排序的图像</p></figure><p id="e8f1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">你可以使用python的PyOD库来实现它。我推荐你关注这篇 <a class="ae ky" href="https://h1ros.github.io/posts/anomaly-detection-by-auto-encoder-deep-learning-in-pyod/" rel="noopener ugc nofollow" target="_blank"> <em class="lv">的博文</em> </a> <em class="lv">同样适用。(链接在</em> <strong class="lb iu"> <em class="lv">资源部分</em> </strong> <em class="lv"> ) </em></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oy"><img src="../Images/fb00018df7c1a361003e932b856edf19.png" data-original-src="https://miro.medium.com/v2/resize:fit:1238/format:webp/1*TqRZmYuTeXjU-52gaFUakg.png"/></div></figure><p id="4beb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在使用入度数 (ODIN)的<strong class="lb iu">异常值检测中，我们计算每个数据点</strong>的<strong class="lb iu">入度。这里，入度被定义为该点所属的最近邻集的数量。该值越高，该点属于空间中某个密集区域的置信度越高。然而，另一方面，较小的值意味着它不是许多最近邻集的一部分，在空间中是孤立的。你可以把这种方法看作是KNN的逆方法。</strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oz"><img src="../Images/d1f8ec8ac72312bedc2cc83761886f7e.png" data-original-src="https://miro.medium.com/v2/resize:fit:786/format:webp/1*z73ozy-zCMMW1v9dmeGddg.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">使用度数内数字(ODIN) |图像的离群点检测</p></figure><p id="f7f7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在该图中，我们设置k的值<em class="lv">(最近邻点)</em> =3，红点仅属于蓝点的一个最近邻集，而所有其他点都是多于一个最近邻集的一部分。因此，我们得出结论，红点是一个离群值。</p><p id="52ac" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="lv">你可以使用python的包离群库来实现它，如下所示— </em></p><pre class="kj kk kl km gt nl nm nn no aw np bi"><span id="3051" class="lw lx it nm b gy nq nr l ns nt"><em class="lv">import package_outlier as po</em></span><span id="2541" class="lw lx it nm b gy nu nr l ns nt"><em class="lv">data = [[1, 1], [2, 2.1], [1, 2], [2, 1], [50, 35], [2, 1.5]]</em></span><span id="3400" class="lw lx it nm b gy nu nr l ns nt"><em class="lv">result = po.LocalOutlierFactorOutlier(data)<br/></em><em class="lv">print (result)</em></span></pre><p id="fb5c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="lv">请随意探索超参数调谐库等。(链接在</em> <strong class="lb iu"> <em class="lv">资源部分</em> </strong> <em class="lv"> ) </em></p><blockquote class="mv"><p id="98c2" class="mw mx it bd my mz na nb nc nd ne lu dk translated">另外，如果你对研究论文感兴趣，你可以<a class="ae ky" href="https://medium.com/analytics-vidhya/summarizing-nlp-research-papers-dbd12965aa0a" rel="noopener">查看我写的一些研究论文摘要</a>。</p></blockquote><h1 id="a80e" class="pa lx it bd ly pb pc pd mb pe pf pg me jz ph ka mh kc pi kd mk kf pj kg mn pk bi translated">资源</h1><ol class=""><li id="d0ae" class="nw nx it lb b lc mp lf mq li pl lm pm lq pn lu po oc od oe bi translated">https://pypi.org/project/pyod/</li><li id="5bb6" class="nw nx it lb b lc of lf og li oh lm oi lq oj lu po oc od oe bi translated">sci kit-Learn:<a class="ae ky" href="https://scikit-learn.org/stable/index.html" rel="noopener ugc nofollow" target="_blank">https://scikit-learn.org/stable/index.html</a></li><li id="2442" class="nw nx it lb b lc of lf og li oh lm oi lq oj lu po oc od oe bi translated">包装异常值:<a class="ae ky" href="https://pypi.org/project/package-outlier/" rel="noopener ugc nofollow" target="_blank">https://pypi.org/project/package-outlier/</a></li></ol></div><div class="ab cl ol om hx on" role="separator"><span class="oo bw bk op oq or"/><span class="oo bw bk op oq or"/><span class="oo bw bk op oq"/></div><div class="im in io ip iq"><p id="7364" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我希望你喜欢读这篇文章。如果你愿意支持我成为一名作家，可以考虑注册<a class="ae ky" href="https://prakhar-mishra.medium.com/membership" rel="noopener">成为</a>的媒体成员。每月只需5美元，你就可以无限制地使用Medium。</p><p id="3d4d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这篇博客到此为止。感谢您的宝贵时间！</p></div></div>    
</body>
</html>