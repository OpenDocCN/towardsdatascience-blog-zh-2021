<html>
<head>
<title>Using PettingZoo with RLlib for Multi-Agent Deep Reinforcement Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用带RLlib的PettingZoo进行多智能体深度强化学习</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/using-pettingzoo-with-rllib-for-multi-agent-deep-reinforcement-learning-5ff47c677abd?source=collection_archive---------4-----------------------#2021-09-25">https://towardsdatascience.com/using-pettingzoo-with-rllib-for-multi-agent-deep-reinforcement-learning-5ff47c677abd?source=collection_archive---------4-----------------------#2021-09-25</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/></div><div class="ab cl jn jo hu jp" role="separator"><span class="jq bw bk jr js jt"/><span class="jq bw bk jr js jt"/><span class="jq bw bk jr js"/></div><div class="ij ik il im in"><p id="4b7c" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated"><strong class="jw ir">免责声明</strong></p><p id="cdfa" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">对于PettingZoo和RLlib的新版本，本教程不再是最新的。自从这篇文章发表以来，PettingZoo经历了一些重大的修改，现在是<a class="ae ks" href="https://farama.org/" rel="noopener ugc nofollow" target="_blank"> Farama基金会</a>的一部分。有关最新文档和教程，请参见https://pettingzoo.farama.org/的<a class="ae ks" href="https://pettingzoo.farama.org/" rel="noopener ugc nofollow" target="_blank">。</a></p></div><div class="ab cl jn jo hu jp" role="separator"><span class="jq bw bk jr js jt"/><span class="jq bw bk jr js jt"/><span class="jq bw bk jr js"/></div><div class="ij ik il im in"><h2 id="0907" class="kt ku iq bd kv kw kx dn ky kz la dp lb kf lc ld le kj lf lg lh kn li lj lk ll bi translated">通过RLlib强化学习库使用PettingZoo多代理环境的教程</h2><p id="57cc" class="pw-post-body-paragraph ju jv iq jw b jx lm jz ka kb ln kd ke kf lo kh ki kj lp kl km kn lq kp kq kr ij bi translated">感谢<a class="lr ls ep" href="https://medium.com/u/bb68bf291304?source=post_page-----5ff47c677abd--------------------------------" rel="noopener" target="_blank">尤里·普洛特金</a>、<a class="lr ls ep" href="https://medium.com/u/84b22428198a?source=post_page-----5ff47c677abd--------------------------------" rel="noopener" target="_blank">罗汉·波特达尔</a>、<a class="lr ls ep" href="https://medium.com/u/8f380a12a0a6?source=post_page-----5ff47c677abd--------------------------------" rel="noopener" target="_blank">本·布莱克</a>和<a class="lr ls ep" href="https://medium.com/u/8457ef414ca8?source=post_page-----5ff47c677abd--------------------------------" rel="noopener" target="_blank">卡安·奥兹多格鲁</a>，他们各自创作或编辑了本文的大部分内容。</p><p id="8558" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">本教程概述了如何使用<a class="ae ks" href="https://docs.ray.io/en/master/rllib.html#rllib-index" rel="noopener ugc nofollow" target="_blank"> RLlib </a> Python库和<a class="ae ks" href="https://github.com/PettingZoo-Team/PettingZoo" rel="noopener ugc nofollow" target="_blank"> PettingZoo </a>环境进行多智能体深度强化学习。关于使用PettingZoo环境的更多细节可以在最近的博客文章中找到。在下面的文章中，我们将介绍如何使用两个PettingZoo环境来训练和评估RLlib策略:</p><p id="63d6" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">活塞球——不存在非法行为，所有特工同时行动</p><p id="6ab5" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated"><a class="ae ks" href="https://www.pettingzoo.ml/classic/leduc_holdem" rel="noopener ugc nofollow" target="_blank"> Leduc Hold'em </a> —非法行动掩蔽，基于回合的行动</p><p id="27ae" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">宠物动物园和滑雪球</p><p id="8b35" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">PettingZoo是为多代理强化学习模拟开发的Python库。当前的软件提供了一个标准的API，在使用其他著名的开源强化学习库的环境中进行训练。你可以把这个库想象成类似于OpenAI的<a class="ae ks" href="https://gym.openai.com/" rel="noopener ugc nofollow" target="_blank"> Gym </a>库，然而，它是为多代理强化学习而定制的。与其他类似，API的基本用法如下:</p><pre class="lt lu lv lw gt lx ly lz ma aw mb bi"><span id="9a27" class="kt ku iq ly b gy mc md l me mf">from pettingzoo.butterfly import pistonball_v5</span><span id="0727" class="kt ku iq ly b gy mg md l me mf">env = pistonball_v5.env()<br/> env.reset()<br/> for agent in env.agent_iter():<br/> observation, reward, done, info = env.last()<br/> action = policy(observation, agent)<br/> env.step(action)</span></pre><p id="f85f" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated"><a class="ae ks" href="https://www.pettingzoo.ml/butterfly/pistonball" rel="noopener ugc nofollow" target="_blank"> Pistonball </a>是一个合作的PettingZoo环境，如下图所示:</p><figure class="lt lu lv lw gt mi gh gi paragraph-image"><div role="button" tabindex="0" class="mj mk di ml bf mm"><div class="gh gi mh"><img src="../Images/b50d25839045363118121cd2d3b81825.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*ZBS0do1EE3ayKGk-.gif"/></div></div><p class="mp mq gj gh gi mr ms bd b be z dk translated">作者图片</p></figure><p id="b6dc" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">环境的<em class="mt">目标</em>是训练活塞协同工作，尽可能快地把球移到左边。</p><p id="34ab" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">每个活塞都作为一个独立的代理，由一个策略<em class="mt"> π </em>控制，这个策略是用函数逼近技术训练的，比如神经网络(因此是深度强化学习)。每个代理的观察空间是每个活塞上方和侧面的一个窗口(见下文)。</p><figure class="lt lu lv lw gt mi gh gi paragraph-image"><div role="button" tabindex="0" class="mj mk di ml bf mm"><div class="gh gi mu"><img src="../Images/12fbcde9da50960de93b2bcd795ded9d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*AWgGxj9mneFJWDKX"/></div></div><p class="mp mq gj gh gi mr ms bd b be z dk translated">作者图片</p></figure><p id="06d1" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">我们假设完全可观察，策略<em class="mt"> π </em>返回一个动作，用于将活塞从+4像素升高或降低到-4像素(图像尺寸为84x84像素)。</p><p id="edca" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">随着每个活塞的动作，环境输出两个全局奖励</p><p id="407d" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">(δx/xₑ)* 100+τt</p><p id="0ea0" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">其中<em class="mt">δx</em>是球的x位置的变化，<em class="mt"> Xₑ </em>是球的起始位置，<em class="mt"> τ </em> <strong class="jw ir"> </strong>是时间惩罚(默认值为0.1)乘以时间长度<em class="mt"> t </em>。</p><p id="8ce7" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">关于宠物动物园环境的更多细节，请查看以下<a class="ae ks" href="https://www.pettingzoo.ml/butterfly/pistonball" rel="noopener ugc nofollow" target="_blank">描述</a>。</p><p id="5c8a" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated"><strong class="jw ir">代码</strong></p><p id="764d" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">让我们更详细地浏览一下代码。</p><p id="8630" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">首先，为了运行强化学习环境，我们导入所需的库:</p><pre class="lt lu lv lw gt lx ly lz ma aw mb bi"><span id="4dbb" class="kt ku iq ly b gy mc md l me mf">from ray import tune</span><span id="2527" class="kt ku iq ly b gy mg md l me mf">from ray.rllib.models import ModelCatalog</span><span id="bf0e" class="kt ku iq ly b gy mg md l me mf">from ray.rllib.models.torch.torch_modelv2 import TorchModelV2</span><span id="2f16" class="kt ku iq ly b gy mg md l me mf">from ray.tune.registry import register_env</span><span id="4612" class="kt ku iq ly b gy mg md l me mf">from ray.rllib.env.wrappers.pettingzoo_env import ParallelPettingZooEnv</span><span id="1627" class="kt ku iq ly b gy mg md l me mf">from pettingzoo.butterfly import pistonball_v5</span><span id="0525" class="kt ku iq ly b gy mg md l me mf">import supersuit as ss</span><span id="29eb" class="kt ku iq ly b gy mg md l me mf">import torch</span><span id="1c10" class="kt ku iq ly b gy mg md l me mf">from torch import nn</span></pre><p id="6459" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">多主体强化学习环境需要分布式训练。为了设置环境，我们使用开源库<a class="ae ks" href="https://docs.ray.io/en/master/index.html" rel="noopener ugc nofollow" target="_blank"> Ray </a>。Ray是为构建分布式应用程序提供通用API而开发的框架。<a class="ae ks" href="https://docs.ray.io/en/master/tune/index.html" rel="noopener ugc nofollow" target="_blank"> Tune </a>是一个构建在Ray之上的库，用于分布式强化学习中的可伸缩超参数调优。因此，我们只使用Tune在<a class="ae ks" href="https://docs.ray.io/en/master/rllib.html" rel="noopener ugc nofollow" target="_blank"> RLlib </a>中执行一次训练运行。由于我们将需要使用一个定制模型来训练我们的策略<em class="mt"> π </em>，我们首先在RLlib的ModelCatalog中注册这个模型。为了创建自定义模型，我们从RLlib中继承了TorchModelV2类的子类。</p><p id="95b0" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">为了将PettingZoo环境与Tune一起使用，我们首先使用<em class="mt"> register_env </em>函数注册环境。<em class="mt">parallelpettingzooeenv</em>是一个包装器，用于PettingZoo环境，如Pistonball，以与RLlib的多代理API接口。SuperSuit是一个为健身房和宠物动物园环境提供预处理功能的库，我们将在下面看到。</p><p id="2c19" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">最初，我们在Pytorch中创建一个卷积神经网络模型来训练我们的策略<em class="mt"> π </em>:</p><pre class="lt lu lv lw gt lx ly lz ma aw mb bi"><span id="2a76" class="kt ku iq ly b gy mc md l me mf">class CNNModelV2(TorchModelV2, nn.Module):</span><span id="7f65" class="kt ku iq ly b gy mg md l me mf">def __init__(self, obs_space, act_space, num_outputs, *args, **kwargs):</span><span id="5632" class="kt ku iq ly b gy mg md l me mf">TorchModelV2.__init__(self, obs_space, act_space, num_outputs, *args, **kwargs)</span><span id="cf84" class="kt ku iq ly b gy mg md l me mf">nn.Module.__init__(self)</span><span id="1908" class="kt ku iq ly b gy mg md l me mf">self.model = nn.Sequential(</span><span id="3efc" class="kt ku iq ly b gy mg md l me mf">nn.Conv2d( 3, 32, [8, 8], stride=(4, 4)),</span><span id="69dc" class="kt ku iq ly b gy mg md l me mf">nn.ReLU(),</span><span id="628e" class="kt ku iq ly b gy mg md l me mf">nn.Conv2d( 32, 64, [4, 4], stride=(2, 2)),</span><span id="bfa5" class="kt ku iq ly b gy mg md l me mf">nn.ReLU(),</span><span id="2216" class="kt ku iq ly b gy mg md l me mf">nn.Conv2d( 64, 64, [3, 3], stride=(1, 1)),</span><span id="517e" class="kt ku iq ly b gy mg md l me mf">nn.ReLU(),</span><span id="2e1f" class="kt ku iq ly b gy mg md l me mf">nn.Flatten(),</span><span id="e666" class="kt ku iq ly b gy mg md l me mf">(nn.Linear(3136,512)),</span><span id="c3c3" class="kt ku iq ly b gy mg md l me mf">nn.ReLU(),</span><span id="4273" class="kt ku iq ly b gy mg md l me mf">)</span><span id="6585" class="kt ku iq ly b gy mg md l me mf">self.policy_fn = nn.Linear(512, num_outputs)</span><span id="600e" class="kt ku iq ly b gy mg md l me mf">self.value_fn = nn.Linear(512, 1)</span><span id="f3d1" class="kt ku iq ly b gy mg md l me mf">def forward(self, input_dict, state, seq_lens):</span><span id="8b40" class="kt ku iq ly b gy mg md l me mf">model_out = self.model(input_dict[“obs”].permute(0, 3, 1, 2))</span><span id="ab1a" class="kt ku iq ly b gy mg md l me mf">self._value_out = self.value_fn(model_out)</span><span id="8596" class="kt ku iq ly b gy mg md l me mf">return self.policy_fn(model_out), state</span><span id="d0a5" class="kt ku iq ly b gy mg md l me mf">def value_function(self):</span><span id="81de" class="kt ku iq ly b gy mg md l me mf">return self._value_out.flatten()</span></pre><p id="7d5a" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">你可以在这里找到更多关于如何使用RLlib库<a class="ae ks" href="https://docs.ray.io/en/master/rllib-models.html#custom-models-implementing-your-own-forward-logic" rel="noopener ugc nofollow" target="_blank">的定制模型的信息。然后我们需要定义一个函数来创建和返回环境:</a></p><pre class="lt lu lv lw gt lx ly lz ma aw mb bi"><span id="0da2" class="kt ku iq ly b gy mc md l me mf">def env_creator(args):</span><span id="e364" class="kt ku iq ly b gy mg md l me mf">env = pistonball_v4.parallel_env(n_pistons=20, local_ratio=0, time_penalty=-0.1, continuous=True, random_drop=True, random_rotate=True, ball_mass=0.75, ball_friction=0.3, ball_elasticity=1.5, max_cycles=125)</span><span id="362c" class="kt ku iq ly b gy mg md l me mf">env = ss.color_reduction_v0(env, mode=’B’)</span><span id="9eb5" class="kt ku iq ly b gy mg md l me mf">env = ss.dtype_v0(env, ‘float32’)</span><span id="efe0" class="kt ku iq ly b gy mg md l me mf">env = ss.resize_v0(env, x_size=84, y_size=84)</span><span id="cb0f" class="kt ku iq ly b gy mg md l me mf">env = ss.frame_stack_v1(env, 3)</span><span id="f2dd" class="kt ku iq ly b gy mg md l me mf">env = ss.normalize_obs_v0(env, env_min=0, env_max=1)</span><span id="4e83" class="kt ku iq ly b gy mg md l me mf">return env</span></pre><p id="e288" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">我们使用PettingZoo的<a class="ae ks" href="https://www.pettingzoo.ml/api" rel="noopener ugc nofollow" target="_blank">并行API </a>来创建环境。函数的参数控制环境的属性和行为。我们另外使用SuperSuit的包装函数进行预处理操作。</p><p id="22fe" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">第一个函数用于将环境产生的全色观察图像转换为灰度图像，以降低计算复杂度和成本。随后，我们将像素图像数据类型从uint8转换为float32，将其缩减为84x84网格表示，并归一化像素强度。最后，为了测量球的方向变化δX(用于计算总奖励)，将3个连续的观察帧堆叠在一起，以提供一种简单的学习策略。</p><p id="ccd8" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">这些预处理操作的更详细的解释可以在之前的<a class="ae ks" rel="noopener" target="_blank" href="/multi-agent-deep-reinforcement-learning-in-15-lines-of-code-using-pettingzoo-e0b963c0820b">教程</a>中找到。</p><p id="f937" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">在定义了模型和环境之后，我们可以使用配置字典中的参数，使用tune.run()函数运行训练器。你可以在这里详细了解这些超参数<a class="ae ks" href="https://medium.com/aureliantactics/ppo-hyperparameters-and-ranges-6fc2d29bccbe" rel="noopener">。值得注意的是，我们实例化了一个多代理特定配置，其中我们使用字典映射来指定我们的策略:</a></p><p id="0634" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">policy_id字符串→元组(policy_cls，obs_space，act_space，config)。</p><p id="8f28" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">policy_mapping_fn是一个将agent_ids映射到policy_ids的函数。在我们的特殊情况下，我们使用参数共享来训练活塞，如之前的<a class="ae ks" rel="noopener" target="_blank" href="/multi-agent-deep-reinforcement-learning-in-15-lines-of-code-using-pettingzoo-e0b963c0820b">教程</a>中所述，即所有活塞都由id为‘policy _ 0’的同一策略<em class="mt"> π </em>控制。</p><pre class="lt lu lv lw gt lx ly lz ma aw mb bi"><span id="2771" class="kt ku iq ly b gy mc md l me mf">if __name__ == “__main__”:</span><span id="6dfa" class="kt ku iq ly b gy mg md l me mf">env_name = “pistonball_v4”</span><span id="1bc8" class="kt ku iq ly b gy mg md l me mf">register_env(env_name, lambda config: ParallelPettingZooEnv(env_creator(config)))</span><span id="1885" class="kt ku iq ly b gy mg md l me mf">test_env = ParallelPettingZooEnv(env_creator({}))</span><span id="14e3" class="kt ku iq ly b gy mg md l me mf">obs_space = test_env.observation_space</span><span id="8a4b" class="kt ku iq ly b gy mg md l me mf">act_space = test_env.action_space</span><span id="d126" class="kt ku iq ly b gy mg md l me mf">ModelCatalog.register_custom_model(“CNNModelV2”, CNNModelV2)</span><span id="f8d3" class="kt ku iq ly b gy mg md l me mf">def gen_policy(i):</span><span id="5238" class="kt ku iq ly b gy mg md l me mf">config = {</span><span id="e00a" class="kt ku iq ly b gy mg md l me mf">“model”: {</span><span id="20b8" class="kt ku iq ly b gy mg md l me mf">“custom_model”: “CNNModelV2”,</span><span id="da95" class="kt ku iq ly b gy mg md l me mf">},</span><span id="ff5f" class="kt ku iq ly b gy mg md l me mf">“gamma”: 0.99,</span><span id="cc93" class="kt ku iq ly b gy mg md l me mf">}</span><span id="ee28" class="kt ku iq ly b gy mg md l me mf">return (None, obs_space, act_space, config)</span><span id="861a" class="kt ku iq ly b gy mg md l me mf">policies = {“policy_0”: gen_policy(0)}</span><span id="d94e" class="kt ku iq ly b gy mg md l me mf">policy_ids = list(policies.keys())</span><span id="9689" class="kt ku iq ly b gy mg md l me mf">tune.run(</span><span id="b42b" class="kt ku iq ly b gy mg md l me mf">“PPO”,</span><span id="cacf" class="kt ku iq ly b gy mg md l me mf">name=”PPO”,</span><span id="4b65" class="kt ku iq ly b gy mg md l me mf">stop={“timesteps_total”: 5000000},</span><span id="178f" class="kt ku iq ly b gy mg md l me mf">checkpoint_freq=10,</span><span id="f539" class="kt ku iq ly b gy mg md l me mf">local_dir=”~/ray_results/”+env_name,</span><span id="a3fc" class="kt ku iq ly b gy mg md l me mf">config={</span><span id="5be3" class="kt ku iq ly b gy mg md l me mf"># Environment specific</span><span id="7202" class="kt ku iq ly b gy mg md l me mf">“env”: env_name,</span><span id="2e1c" class="kt ku iq ly b gy mg md l me mf"># General</span><span id="c0b3" class="kt ku iq ly b gy mg md l me mf">“log_level”: “ERROR”,</span><span id="b3f6" class="kt ku iq ly b gy mg md l me mf">“framework”: “torch”,</span><span id="66ae" class="kt ku iq ly b gy mg md l me mf">“num_gpus”: 1,</span><span id="9356" class="kt ku iq ly b gy mg md l me mf">“num_workers”: 4,</span><span id="563f" class="kt ku iq ly b gy mg md l me mf">“num_envs_per_worker”: 1,</span><span id="94a7" class="kt ku iq ly b gy mg md l me mf">“compress_observations”: False,</span><span id="893c" class="kt ku iq ly b gy mg md l me mf">“batch_mode”: ‘truncate_episodes’,</span><span id="ffb3" class="kt ku iq ly b gy mg md l me mf"># ‘use_critic’: True,</span><span id="96c4" class="kt ku iq ly b gy mg md l me mf">‘use_gae’: True,</span><span id="5063" class="kt ku iq ly b gy mg md l me mf">“lambda”: 0.9,</span><span id="7914" class="kt ku iq ly b gy mg md l me mf">“gamma”: .99,</span><span id="f64d" class="kt ku iq ly b gy mg md l me mf"># “kl_coeff”: 0.001,</span><span id="36f4" class="kt ku iq ly b gy mg md l me mf"># “kl_target”: 1000.,</span><span id="dd12" class="kt ku iq ly b gy mg md l me mf">“clip_param”: 0.4,</span><span id="79b8" class="kt ku iq ly b gy mg md l me mf">‘grad_clip’: None,</span><span id="af2c" class="kt ku iq ly b gy mg md l me mf">“entropy_coeff”: 0.1,</span><span id="608f" class="kt ku iq ly b gy mg md l me mf">‘vf_loss_coeff’: 0.25,</span><span id="563d" class="kt ku iq ly b gy mg md l me mf">“sgd_minibatch_size”: 64,</span><span id="1b99" class="kt ku iq ly b gy mg md l me mf">“num_sgd_iter”: 10, # epoc</span><span id="cb3d" class="kt ku iq ly b gy mg md l me mf">‘rollout_fragment_length’: 512,</span><span id="c814" class="kt ku iq ly b gy mg md l me mf">“train_batch_size”: 512*4,</span><span id="b035" class="kt ku iq ly b gy mg md l me mf">‘lr’: 2e-05,</span><span id="585c" class="kt ku iq ly b gy mg md l me mf">“clip_actions”: True,</span><span id="a578" class="kt ku iq ly b gy mg md l me mf"># Method specific</span><span id="ff8f" class="kt ku iq ly b gy mg md l me mf">“multiagent”: {</span><span id="9374" class="kt ku iq ly b gy mg md l me mf">“policies”: policies,</span><span id="8ce9" class="kt ku iq ly b gy mg md l me mf">“policy_mapping_fn”: (</span><span id="abbc" class="kt ku iq ly b gy mg md l me mf">lambda agent_id: policy_ids[0]),</span><span id="876d" class="kt ku iq ly b gy mg md l me mf">},</span><span id="fc42" class="kt ku iq ly b gy mg md l me mf">},</span><span id="0c77" class="kt ku iq ly b gy mg md l me mf">)</span></pre><p id="f2da" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">现在，我们可以看到我们训练过的策略在环境中自动执行。在训练期间，根据checkpoint_freq指定的频率，在每个检查点保存策略。默认情况下，RLlib将检查点存储在~/ray_results中。我们首先指定检查点的路径:</p><pre class="lt lu lv lw gt lx ly lz ma aw mb bi"><span id="a83c" class="kt ku iq ly b gy mc md l me mf">checkpoint_path = “foo”</span></pre><p id="3149" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">然后，我们可以加载并恢复我们训练过的策略<em class="mt"> π </em>，并在每个时间步渲染环境时使用该策略来选择动作。我们将渲染后的视频保存为GIF格式:</p><pre class="lt lu lv lw gt lx ly lz ma aw mb bi"><span id="1f46" class="kt ku iq ly b gy mc md l me mf">with open(params_path, “rb”) as f:</span><span id="b071" class="kt ku iq ly b gy mg md l me mf">config = pickle.load(f)</span><span id="94ff" class="kt ku iq ly b gy mg md l me mf"># num_workers not needed since we are not training</span><span id="2357" class="kt ku iq ly b gy mg md l me mf">del config[‘num_workers’]</span><span id="f8e1" class="kt ku iq ly b gy mg md l me mf">del config[‘num_gpus’]</span><span id="516e" class="kt ku iq ly b gy mg md l me mf">ray.init(num_cpus=8, num_gpus=1)</span><span id="3d4a" class="kt ku iq ly b gy mg md l me mf">PPOagent = PPOTrainer(env=env_name, config=config)</span><span id="81f2" class="kt ku iq ly b gy mg md l me mf">PPOagent.restore(checkpoint_path)</span><span id="09de" class="kt ku iq ly b gy mg md l me mf">reward_sum = 0</span><span id="1970" class="kt ku iq ly b gy mg md l me mf">frame_list = []</span><span id="5df9" class="kt ku iq ly b gy mg md l me mf">i = 0</span><span id="7e6f" class="kt ku iq ly b gy mg md l me mf">env.reset()</span><span id="6bec" class="kt ku iq ly b gy mg md l me mf">for agent in env.agent_iter():</span><span id="2696" class="kt ku iq ly b gy mg md l me mf">observation, reward, done, info = env.last()</span><span id="703e" class="kt ku iq ly b gy mg md l me mf">reward_sum += reward</span><span id="b327" class="kt ku iq ly b gy mg md l me mf">if done:</span><span id="20c3" class="kt ku iq ly b gy mg md l me mf">action = None</span><span id="bbdb" class="kt ku iq ly b gy mg md l me mf">else:</span><span id="beca" class="kt ku iq ly b gy mg md l me mf">action, _, _ = PPOagent.get_policy(“policy_0”).compute_single_action(observation)</span><span id="0d65" class="kt ku iq ly b gy mg md l me mf">env.step(action)</span><span id="7448" class="kt ku iq ly b gy mg md l me mf">i += 1</span><span id="61f6" class="kt ku iq ly b gy mg md l me mf">if i % (len(env.possible_agents)+1) == 0:</span><span id="292c" class="kt ku iq ly b gy mg md l me mf">frame_list.append(PIL.Image.fromarray(env.render(mode=’rgb_array’)))</span><span id="651a" class="kt ku iq ly b gy mg md l me mf">env.close()</span><span id="eaf4" class="kt ku iq ly b gy mg md l me mf">print(reward_sum)</span><span id="f9a2" class="kt ku iq ly b gy mg md l me mf">frame_list[0].save(“out.gif”, save_all=True, append_images=frame_list[1:], duration=3, loop=0)</span></pre><p id="7981" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">解决环境的gif可以在上面看到。</p><p id="5800" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">本教程中详细介绍的整个培训代码可以在<a class="ae ks" href="https://github.com/PettingZoo-Team/PettingZoo/blob/master/tutorials/rllib_pistonball.py" rel="noopener ugc nofollow" target="_blank">这里</a>找到。并且渲染的代码可以在<a class="ae ks" href="https://github.com/PettingZoo-Team/PettingZoo/blob/master/tutorials/render_rllib_pistonball.py" rel="noopener ugc nofollow" target="_blank">这里</a>找到。对于代码库的任何改进或本文遇到的任何问题，请在PettingZoo <a class="ae ks" href="https://github.com/PettingZoo-Team/PettingZoo" rel="noopener ugc nofollow" target="_blank">资源库</a>中创建一个问题。</p><p id="8d70" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated"><strong class="jw ir"> Leduc德州扑克</strong></p><p id="e60c" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">Leduc Hold'em是AI研究中流行的一种扑克变种详细介绍<a class="ae ks" href="https://ai.plainenglish.io/building-a-poker-ai-part-8-leduc-holdem-and-a-more-generic-cfr-algorithm-in-python-9dd036bf9a30" rel="noopener ugc nofollow" target="_blank">这里</a>和<a class="ae ks" href="https://www.pettingzoo.ml/classic/leduc_holdem" rel="noopener ugc nofollow" target="_blank">这里</a>；我们将使用双人模式。这种环境是值得注意的，因为它是一个纯粹的回合制游戏，有些动作是非法的(例如，需要动作屏蔽)。</p><p id="4aa5" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">首先，我们需要的初始代码看起来非常相似:</p><pre class="lt lu lv lw gt lx ly lz ma aw mb bi"><span id="bfbd" class="kt ku iq ly b gy mc md l me mf">from copy import deepcopy</span><span id="15b5" class="kt ku iq ly b gy mg md l me mf">import os</span><span id="4ed3" class="kt ku iq ly b gy mg md l me mf">import ray</span><span id="769c" class="kt ku iq ly b gy mg md l me mf">from ray import tune</span><span id="80bd" class="kt ku iq ly b gy mg md l me mf">from ray.rllib.agents.registry import get_agent_class</span><span id="4f08" class="kt ku iq ly b gy mg md l me mf">from ray.rllib.env import PettingZooEnv</span><span id="57da" class="kt ku iq ly b gy mg md l me mf">from pettingzoo.classic import leduc_holdem_v2</span><span id="7790" class="kt ku iq ly b gy mg md l me mf">from ray.rllib.models import ModelCatalog</span><span id="380d" class="kt ku iq ly b gy mg md l me mf">from ray.tune.registry import register_env</span><span id="4e04" class="kt ku iq ly b gy mg md l me mf">from gym.spaces import Box</span><span id="bfd9" class="kt ku iq ly b gy mg md l me mf">from ray.rllib.agents.dqn.dqn_torch_model import DQNTorchModel</span><span id="7a27" class="kt ku iq ly b gy mg md l me mf">from ray.rllib.models.torch.fcnet import FullyConnectedNetwork as TorchFC</span><span id="0c8b" class="kt ku iq ly b gy mg md l me mf">from ray.rllib.utils.framework import try_import_torch</span><span id="8fca" class="kt ku iq ly b gy mg md l me mf">from ray.rllib.utils.torch_ops import FLOAT_MAX</span><span id="d752" class="kt ku iq ly b gy mg md l me mf">torch, nn = try_import_torch()</span></pre><p id="b695" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">通过参数动作屏蔽使用动作屏蔽初始化合适的策略，如下所示:</p><pre class="lt lu lv lw gt lx ly lz ma aw mb bi"><span id="7b88" class="kt ku iq ly b gy mc md l me mf">class TorchMaskedActions(DQNTorchModel):</span><span id="6db0" class="kt ku iq ly b gy mg md l me mf">“””PyTorch version of above ParametricActionsModel.”””</span><span id="7628" class="kt ku iq ly b gy mg md l me mf">def __init__(self,</span><span id="9291" class="kt ku iq ly b gy mg md l me mf">obs_space,</span><span id="2855" class="kt ku iq ly b gy mg md l me mf">action_space,</span><span id="3976" class="kt ku iq ly b gy mg md l me mf">num_outputs,</span><span id="0a50" class="kt ku iq ly b gy mg md l me mf">model_config,</span><span id="8b1b" class="kt ku iq ly b gy mg md l me mf">name,</span><span id="205e" class="kt ku iq ly b gy mg md l me mf">**kw):</span><span id="2a55" class="kt ku iq ly b gy mg md l me mf">DQNTorchModel.__init__(self, obs_space, action_space, num_outputs,</span><span id="e8dd" class="kt ku iq ly b gy mg md l me mf">model_config, name, **kw)</span><span id="de53" class="kt ku iq ly b gy mg md l me mf">obs_len = obs_space.shape[0]-action_space.n</span><span id="cc1a" class="kt ku iq ly b gy mg md l me mf">orig_obs_space = Box(shape=(obs_len,), low=obs_space.low[:obs_len], high=obs_space.high[:obs_len])</span><span id="59ab" class="kt ku iq ly b gy mg md l me mf">self.action_embed_model = TorchFC(orig_obs_space, action_space, action_space.n, model_config, name + “_action_embed”)</span><span id="a8ce" class="kt ku iq ly b gy mg md l me mf">def forward(self, input_dict, state, seq_lens):</span><span id="1c6a" class="kt ku iq ly b gy mg md l me mf"># Extract the available actions tensor from the observation.</span><span id="dc49" class="kt ku iq ly b gy mg md l me mf">action_mask = input_dict[“obs”][“action_mask”]</span><span id="5570" class="kt ku iq ly b gy mg md l me mf"># Compute the predicted action embedding</span><span id="4755" class="kt ku iq ly b gy mg md l me mf">action_logits, _ = self.action_embed_model({</span><span id="b62f" class="kt ku iq ly b gy mg md l me mf">“obs”: input_dict[“obs”][‘observation’]</span><span id="3acd" class="kt ku iq ly b gy mg md l me mf">})</span><span id="0e2a" class="kt ku iq ly b gy mg md l me mf"># turns probit action mask into logit action mask</span><span id="2235" class="kt ku iq ly b gy mg md l me mf">inf_mask = torch.clamp(torch.log(action_mask), -1e10, FLOAT_MAX)</span><span id="d2f1" class="kt ku iq ly b gy mg md l me mf">return action_logits + inf_mask, state</span><span id="d885" class="kt ku iq ly b gy mg md l me mf">def value_function(self):</span><span id="932a" class="kt ku iq ly b gy mg md l me mf">return self.action_embed_model.value_function()</span></pre><p id="9e4c" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">Leduc Hold'em中的渲染功能类似于Pistonball，使用以下代码片段:</p><pre class="lt lu lv lw gt lx ly lz ma aw mb bi"><span id="47b3" class="kt ku iq ly b gy mc md l me mf">import ray</span><span id="980c" class="kt ku iq ly b gy mg md l me mf">import pickle5 as pickle</span><span id="05a1" class="kt ku iq ly b gy mg md l me mf">from ray.tune.registry import register_env</span><span id="18d2" class="kt ku iq ly b gy mg md l me mf">from ray.rllib.agents.dqn import DQNTrainer</span><span id="196b" class="kt ku iq ly b gy mg md l me mf">from pettingzoo.classic import leduc_holdem_v4</span><span id="0feb" class="kt ku iq ly b gy mg md l me mf">import supersuit as ss</span><span id="5655" class="kt ku iq ly b gy mg md l me mf">from ray.rllib.env.wrappers.pettingzoo_env import PettingZooEnv</span><span id="035a" class="kt ku iq ly b gy mg md l me mf">import PIL</span><span id="c113" class="kt ku iq ly b gy mg md l me mf">from ray.rllib.models import ModelCatalog</span><span id="6a37" class="kt ku iq ly b gy mg md l me mf">import numpy as np</span><span id="a8bd" class="kt ku iq ly b gy mg md l me mf">import os</span><span id="fe05" class="kt ku iq ly b gy mg md l me mf">from ray.rllib.agents.registry import get_agent_class</span><span id="8a95" class="kt ku iq ly b gy mg md l me mf">from copy import deepcopy</span><span id="27dc" class="kt ku iq ly b gy mg md l me mf">import argparse</span><span id="ddd3" class="kt ku iq ly b gy mg md l me mf">from pathlib import Path</span><span id="4c5f" class="kt ku iq ly b gy mg md l me mf">from rllib_leduc_holdem import TorchMaskedActions</span><span id="96bc" class="kt ku iq ly b gy mg md l me mf">os.environ[“SDL_VIDEODRIVER”] = “dummy”</span><span id="d9a6" class="kt ku iq ly b gy mg md l me mf">parser = argparse.ArgumentParser(description=’Render pretrained policy loaded from checkpoint’)</span><span id="316d" class="kt ku iq ly b gy mg md l me mf">parser.add_argument(“checkpoint_path”, help=”Path to the checkpoint. This path will likely be something like this: `~/ray_results/pistonball_v4/PPO/PPO_pistonball_v4_660ce_00000_0_2021–06–11_12–30–57/checkpoint_000050/checkpoint-50`”)</span><span id="9736" class="kt ku iq ly b gy mg md l me mf">args = parser.parse_args()</span><span id="4b1e" class="kt ku iq ly b gy mg md l me mf">checkpoint_path = os.path.expanduser(args.checkpoint_path)</span><span id="06d6" class="kt ku iq ly b gy mg md l me mf">params_path = Path(checkpoint_path).parent.parent/”params.pkl”</span><span id="11a7" class="kt ku iq ly b gy mg md l me mf">alg_name = “DQN”</span><span id="5ca6" class="kt ku iq ly b gy mg md l me mf">ModelCatalog.register_custom_model(</span><span id="d27e" class="kt ku iq ly b gy mg md l me mf">“pa_model”, TorchMaskedActions)</span><span id="290f" class="kt ku iq ly b gy mg md l me mf"># function that outputs the environment you wish to register.</span><span id="061e" class="kt ku iq ly b gy mg md l me mf">def env_creator():</span><span id="1db0" class="kt ku iq ly b gy mg md l me mf">env = leduc_holdem_v4.env()</span><span id="832d" class="kt ku iq ly b gy mg md l me mf">return env</span><span id="83ed" class="kt ku iq ly b gy mg md l me mf">num_cpus = 1</span><span id="8ba7" class="kt ku iq ly b gy mg md l me mf">config = deepcopy(get_agent_class(alg_name)._default_config)</span><span id="2a54" class="kt ku iq ly b gy mg md l me mf">register_env(“leduc_holdem”,</span><span id="4958" class="kt ku iq ly b gy mg md l me mf">lambda config: PettingZooEnv(env_creator()))</span><span id="d111" class="kt ku iq ly b gy mg md l me mf">env = (env_creator())</span><span id="bbc7" class="kt ku iq ly b gy mg md l me mf"># obs_space = env.observation_space</span><span id="46e9" class="kt ku iq ly b gy mg md l me mf"># print(obs_space)</span><span id="42bc" class="kt ku iq ly b gy mg md l me mf"># act_space = test_env.action_space</span><span id="adbd" class="kt ku iq ly b gy mg md l me mf">with open(params_path, “rb”) as f:</span><span id="8979" class="kt ku iq ly b gy mg md l me mf">config = pickle.load(f)</span><span id="b91a" class="kt ku iq ly b gy mg md l me mf"># num_workers not needed since we are not training</span><span id="f5a5" class="kt ku iq ly b gy mg md l me mf">del config[‘num_workers’]</span><span id="96b4" class="kt ku iq ly b gy mg md l me mf">del config[‘num_gpus’]</span><span id="eac8" class="kt ku iq ly b gy mg md l me mf">ray.init(num_cpus=8, num_gpus=0)</span><span id="fe7b" class="kt ku iq ly b gy mg md l me mf">DQNAgent = DQNTrainer(env=”leduc_holdem”, config=config)</span><span id="d434" class="kt ku iq ly b gy mg md l me mf">DQNAgent.restore(checkpoint_path)</span><span id="781f" class="kt ku iq ly b gy mg md l me mf">reward_sums = {a:0 for a in env.possible_agents}</span><span id="074c" class="kt ku iq ly b gy mg md l me mf">i = 0</span><span id="27a9" class="kt ku iq ly b gy mg md l me mf">env.reset()</span><span id="68f1" class="kt ku iq ly b gy mg md l me mf">for agent in env.agent_iter():</span><span id="525f" class="kt ku iq ly b gy mg md l me mf">observation, reward, done, info = env.last()</span><span id="aa52" class="kt ku iq ly b gy mg md l me mf">obs = observation[‘observation’]</span><span id="c850" class="kt ku iq ly b gy mg md l me mf">reward_sums[agent] += reward</span><span id="a87b" class="kt ku iq ly b gy mg md l me mf">if done:</span><span id="6e5d" class="kt ku iq ly b gy mg md l me mf">action = None</span><span id="b0be" class="kt ku iq ly b gy mg md l me mf">else:</span><span id="71bb" class="kt ku iq ly b gy mg md l me mf">print(DQNAgent.get_policy(agent))</span><span id="8330" class="kt ku iq ly b gy mg md l me mf">policy = DQNAgent.get_policy(agent)</span><span id="fc8f" class="kt ku iq ly b gy mg md l me mf">batch_obs = {</span><span id="a073" class="kt ku iq ly b gy mg md l me mf">‘obs’:{</span><span id="282f" class="kt ku iq ly b gy mg md l me mf">‘observation’: np.expand_dims(observation[‘observation’], 0),</span><span id="31c7" class="kt ku iq ly b gy mg md l me mf">‘action_mask’: np.expand_dims(observation[‘action_mask’],0)</span><span id="cdd0" class="kt ku iq ly b gy mg md l me mf">}</span><span id="1507" class="kt ku iq ly b gy mg md l me mf">}</span><span id="11e3" class="kt ku iq ly b gy mg md l me mf">batched_action, state_out, info = policy.compute_actions_from_input_dict(batch_obs)</span><span id="e598" class="kt ku iq ly b gy mg md l me mf">single_action = batched_action[0]</span><span id="868a" class="kt ku iq ly b gy mg md l me mf">action = single_action</span><span id="6cfe" class="kt ku iq ly b gy mg md l me mf">env.step(action)</span><span id="a815" class="kt ku iq ly b gy mg md l me mf">i += 1</span><span id="5e92" class="kt ku iq ly b gy mg md l me mf">env.render()</span><span id="78a0" class="kt ku iq ly b gy mg md l me mf">print(“rewards:”)</span><span id="2774" class="kt ku iq ly b gy mg md l me mf">print(reward_sums)</span></pre><p id="9b36" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">与之前类似，本教程中使用的完整训练代码可以在<a class="ae ks" href="https://github.com/PettingZoo-Team/PettingZoo/blob/master/tutorials/rllib_leduc_holdem.py" rel="noopener ugc nofollow" target="_blank">这里</a>找到，渲染代码可以在<a class="ae ks" href="https://github.com/PettingZoo-Team/PettingZoo/blob/master/tutorials/render_rllib_leduc_holdem.py" rel="noopener ugc nofollow" target="_blank">这里</a>找到。如果您对这段代码有任何改进，或者对本文有任何问题，请在PettingZoo <a class="ae ks" href="https://github.com/PettingZoo-Team/PettingZoo" rel="noopener ugc nofollow" target="_blank">资源库</a>上创建一个问题，我们很乐意提供帮助。</p></div></div>    
</body>
</html>