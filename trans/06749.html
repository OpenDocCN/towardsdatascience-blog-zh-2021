<html>
<head>
<title>Understanding Optimization Algorithms in Machine Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">理解机器学习中的优化算法</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/understanding-optimization-algorithms-in-machine-learning-edfdb4df766b?source=collection_archive---------5-----------------------#2021-06-18">https://towardsdatascience.com/understanding-optimization-algorithms-in-machine-learning-edfdb4df766b?source=collection_archive---------5-----------------------#2021-06-18</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="973c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">机器学习中两种重要优化技术背后的数学</p><h1 id="6caf" class="kl km iq bd kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li bi translated"><strong class="ak">目录:</strong></h1><ol class=""><li id="50fb" class="lj lk iq jp b jq ll ju lm jy ln kc lo kg lp kk lq lr ls lt bi translated"><a class="ae lu" href="#c601" rel="noopener ugc nofollow">简介</a></li><li id="14ab" class="lj lk iq jp b jq lv ju lw jy lx kc ly kg lz kk lq lr ls lt bi translated"><a class="ae lu" href="#a4db" rel="noopener ugc nofollow">最大值和最小值</a></li><li id="188a" class="lj lk iq jp b jq lv ju lw jy lx kc ly kg lz kk lq lr ls lt bi translated"><a class="ae lu" href="#c06b" rel="noopener ugc nofollow">梯度下降</a></li><li id="1247" class="lj lk iq jp b jq lv ju lw jy lx kc ly kg lz kk lq lr ls lt bi translated"><a class="ae lu" href="#414b" rel="noopener ugc nofollow">学习率</a></li><li id="4cbb" class="lj lk iq jp b jq lv ju lw jy lx kc ly kg lz kk lq lr ls lt bi translated"><a class="ae lu" href="#b0a7" rel="noopener ugc nofollow">逻辑回归中的梯度下降</a></li><li id="b44f" class="lj lk iq jp b jq lv ju lw jy lx kc ly kg lz kk lq lr ls lt bi translated"><a class="ae lu" href="#88aa" rel="noopener ugc nofollow">随机梯度下降</a></li><li id="4cd4" class="lj lk iq jp b jq lv ju lw jy lx kc ly kg lz kk lq lr ls lt bi translated"><a class="ae lu" href="#3598" rel="noopener ugc nofollow">结论</a></li><li id="ffdb" class="lj lk iq jp b jq lv ju lw jy lx kc ly kg lz kk lq lr ls lt bi translated"><a class="ae lu" href="#7c1a" rel="noopener ugc nofollow">参考文献</a></li></ol><figure class="mb mc md me gt mf gh gi paragraph-image"><div role="button" tabindex="0" class="mg mh di mi bf mj"><div class="gh gi ma"><img src="../Images/65255469d2ed09f489ceead5e11eb5b7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*g1gwa-B__DR-PmRMdFKKsA.jpeg"/></div></div><p class="mm mn gj gh gi mo mp bd b be z dk translated">作者图片</p></figure></div><div class="ab cl mq mr hu ms" role="separator"><span class="mt bw bk mu mv mw"/><span class="mt bw bk mu mv mw"/><span class="mt bw bk mu mv"/></div><div class="ij ik il im in"><ol class=""><li id="c601" class="lj lk iq jp b jq jr ju jv jy mx kc my kg mz kk lq lr ls lt bi translated"><strong class="jp ir">简介</strong></li></ol><p id="c0af" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">优化是我们迭代地训练模型的过程，其结果是最大和最小的函数评估。获得更好的结果是机器学习中最重要的现象之一。</p><p id="8dbe" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们为什么要优化我们的机器学习模型？我们通过改变每一步中的超参数来比较每一次迭代的结果，直到我们达到最佳结果。我们创建了一个错误率更低的精确模型。我们可以使用不同的方法来优化模型。在本文中，我们来讨论两个重要的优化算法:<strong class="jp ir">梯度下降和随机梯度下降算法</strong>；它们是如何在机器学习模型中使用的，以及它们背后的数学。</p><p id="a4db" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir"> 2。最大值和最小值</strong></p><p id="989c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">最大值是给定范围内函数的最大值，最小值是最小值。我们将它们表示如下:</p><figure class="mb mc md me gt mf gh gi paragraph-image"><div role="button" tabindex="0" class="mg mh di mi bf mj"><div class="gh gi na"><img src="../Images/5be4a10b8691882c52f7e6833e3e1171.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eKd1KhJnYP1z85RvmKGy9A.png"/></div></div><p class="mm mn gj gh gi mo mp bd b be z dk translated">最小值和最大值(图片由作者提供)</p></figure><p id="c634" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><em class="nb">全局最大值和最小值</em>:分别是函数在整个定义域上的最大值和最小值</p><p id="0a91" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><em class="nb">局部最大值和最小值</em>:分别是函数在给定范围内的最大值和最小值。</p><p id="a73a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">只能有一个全局最小值和最大值，但是可以有多个局部最小值和最大值。</p><p id="c06b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir"> 3。梯度下降</strong></p><p id="b751" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">梯度下降是一种优化算法，它能找出可微函数的局部极小值。它是最小化给定函数的最小化算法。</p><p id="9032" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">让我们看看梯度下降的几何直观:</p><figure class="mb mc md me gt mf gh gi paragraph-image"><div role="button" tabindex="0" class="mg mh di mi bf mj"><div class="gh gi nc"><img src="../Images/4e33213798df7de98c769f7757b08987.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*j-eksZQCi-uQzfdT0DuFkw.png"/></div></div><p class="mm mn gj gh gi mo mp bd b be z dk translated">Y=X的斜率(图片作者提供)</p></figure><p id="51b0" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">让我们举一个抛物线的例子，Y=X</p><p id="c782" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这里，最小值是原点(0，0)。这里的斜率是Tanθ。因此，当0 </p><figure class="mb mc md me gt mf gh gi paragraph-image"><div role="button" tabindex="0" class="mg mh di mi bf mj"><div class="gh gi nd"><img src="../Images/02ba8613bdb3db31a34e16de63ee6d1e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VbnsxNqgKyoRWbd3uwqMQg.png"/></div></div><p class="mm mn gj gh gi mo mp bd b be z dk translated">点向最小值移动的斜率(图片由作者提供)</p></figure><p id="4978" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">图中的一个重要观察结果是斜率在最小值时从正变到负。当我们越接近最小值，斜率越小。</p><p id="4683" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">那么，梯度下降算法是如何工作的呢？</p><p id="c807" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">目的:计算函数Y=X的X*-局部最小值。</p><ul class=""><li id="603f" class="lj lk iq jp b jq jr ju jv jy mx kc my kg mz kk ne lr ls lt bi translated">随机选取一个初始点X₀</li><li id="8478" class="lj lk iq jp b jq lv ju lw jy lx kc ly kg lz kk ne lr ls lt bi translated">在X₀.计算X₁ = X₀-r[df/dx]r是学习率(我们将在<a class="ae lu" href="#414b" rel="noopener ugc nofollow">学习率</a>部分讨论<em class="nb"> r </em>)。让我们取r=1。这里，df/dx只不过是<em class="nb">梯度。</em></li><li id="ad44" class="lj lk iq jp b jq lv ju lw jy lx kc ly kg lz kk ne lr ls lt bi translated">在X₁.计算X₂ = X₁-r[df/dx]</li><li id="4560" class="lj lk iq jp b jq lv ju lw jy lx kc ly kg lz kk ne lr ls lt bi translated">计算所有的分数:X₁，X₂，X₃，…。xᵢxᵢ-₁</li><li id="e3ee" class="lj lk iq jp b jq lv ju lw jy lx kc ly kg lz kk ne lr ls lt bi translated">计算局部最小值的一般公式:Xᵢ = (Xᵢ-₁)-r[df/dx)在Xᵢ-₁</li><li id="b4d6" class="lj lk iq jp b jq lv ju lw jy lx kc ly kg lz kk ne lr ls lt bi translated">当(Xᵢ — Xᵢ-₁)很小时，即当Xᵢ-₁、Xᵢ收敛时，我们停止迭代并宣布X* = Xᵢ</li></ul><p id="414b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir"> 4。学习率</strong></p><p id="32e0" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">学习率是一个超参数或调谐参数，它决定了在函数中向最小值移动时每次迭代的步长。比如初始步r = 0.1，下一步就可以取r=0.01。同样，当我们进一步迭代时，它可以按指数规律减少。在深度学习中使用更有效。</p><p id="78ad" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">如果我们保持r值不变，会发生什么:</p><figure class="mb mc md me gt mf gh gi paragraph-image"><div role="button" tabindex="0" class="mg mh di mi bf mj"><div class="gh gi nf"><img src="../Images/4d69cf79722eae1970cc96b05d600680.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CvhYcnY1Fy4SnvgKv84h1w.png"/></div></div><p class="mm mn gj gh gi mo mp bd b be z dk translated">振荡问题(图片由作者提供)</p></figure><p id="9676" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在上面的例子中，我们取r=1。当我们计算Xᵢ，Xᵢ+₁，Xᵢ+₂,….求局部最小值X*，我们可以看到它在X = -0.5和X = 0.5之间振荡。</p><p id="d4f1" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">当我们保持r不变时，我们最终会遇到一个<em class="nb">振荡问题</em>。因此，我们必须在每次迭代中减少“r”值。随着迭代步长的增加，减小<em class="nb"> r </em>的值。</p><p id="9d3e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">重要提示:</strong>超参数决定偏差-方差权衡。当<em class="nb"> r </em>值较低时，可能会使模型过拟合并导致高方差。当<em class="nb"> r </em>值较高时，可能会使模型欠拟合并导致高偏差。我们可以用<em class="nb">交叉验证</em>技术找到正确的<em class="nb"> r </em>值。用不同的学习率绘制图表，检查每个值的训练损失，选择损失最小的一个。</p><p id="b0a7" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir"> 5。逻辑回归中的梯度下降</strong></p><p id="e47a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">应用sigmoid函数后，逻辑回归中最佳平面的公式为:</p><figure class="mb mc md me gt mf gh gi paragraph-image"><div role="button" tabindex="0" class="mg mh di mi bf mj"><div class="gh gi ng"><img src="../Images/ff981ad90b19528293986ff471a86645.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JkhP7Rqt_eJics53cebwtw.png"/></div></div><p class="mm mn gj gh gi mo mp bd b be z dk translated">最优平面—逻辑回归(图片由作者提供)</p></figure><p id="a6e0" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">对逻辑回归应用梯度下降算法:</p><figure class="mb mc md me gt mf gh gi paragraph-image"><div role="button" tabindex="0" class="mg mh di mi bf mj"><div class="gh gi nh"><img src="../Images/c94c6dabfbfea50873650235300a2d0e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SltrqZ8yexNXqg0FqVEHvA.png"/></div></div><p class="mm mn gj gh gi mo mp bd b be z dk translated">逻辑回归中的梯度下降(图片由作者提供)</p></figure><p id="d582" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们将计算W₀、W₁、w₂……、Wᵢ-₁、Wᵢ去找W*。当(Wᵢ-₁-Wᵢ)很小时，即当wᵢ-₁、wᵢ收敛时，我们宣布W* = Wᵢ</p><p id="2c49" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">梯度下降的<strong class="jp ir">缺点</strong>:</p><p id="555b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">当n(数据点的数量)很大时，<em class="nb"> k </em>次迭代计算最佳向量所花费的时间变得非常长。</p><p id="7c79" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">时间复杂度:O(kn)</p><p id="9e61" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这个问题用随机梯度下降法解决，在下一节讨论。</p><p id="88aa" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir"> 5。随机梯度下降(SGD) </strong></p><p id="4e2f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在SGD中，我们不使用所有的数据点，而是使用它的一个样本来计算函数的局部最小值。随机基本上就是概率的意思。所以我们从人群中随机选择点。</p><ul class=""><li id="eb34" class="lj lk iq jp b jq jr ju jv jy mx kc my kg mz kk ne lr ls lt bi translated"><strong class="jp ir">逻辑回归中的SGD</strong></li></ul><figure class="mb mc md me gt mf gh gi paragraph-image"><div role="button" tabindex="0" class="mg mh di mi bf mj"><div class="gh gi ni"><img src="../Images/fe09bf532e2118e287a646deb437aa3c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SEMyKC77RmPb3k2cLMW3Kg.png"/></div></div><p class="mm mn gj gh gi mo mp bd b be z dk translated">逻辑回归中的随机梯度下降(图片由作者提供)</p></figure><p id="9b8b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这里，<em class="nb"> m </em>是从总体中随机选取的数据样本，<em class="nb"> n </em></p><p id="9e1b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">时间复杂度:O(km)。m远小于n。因此，与梯度下降法相比，计算时间更短。</p><p id="3598" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir"> 6。结论</strong></p><p id="8e3d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在本文中，我们讨论了优化算法，如梯度下降和随机梯度下降及其在逻辑回归中的应用。SGD是机器学习中最重要的优化算法。它主要用于逻辑回归和线性回归。它在深度学习中被扩展为Adam，Adagrad。</p><p id="7c1a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir"> 7。参考文献</strong></p><p id="e2d7" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">[1]最大值和最小值:<a class="ae lu" href="https://en.wikipedia.org/wiki/Maxima_and_minima" rel="noopener ugc nofollow" target="_blank">https://en.wikipedia.org/wiki/Maxima_and_minima</a></p><p id="9f27" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">[2]梯度下降:<a class="ae lu" href="https://en.wikipedia.org/wiki/Gradient_descent" rel="noopener ugc nofollow" target="_blank">https://en.wikipedia.org/wiki/Gradient_descent</a></p></div></div>    
</body>
</html>