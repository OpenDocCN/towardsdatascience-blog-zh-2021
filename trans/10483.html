<html>
<head>
<title>Simple Model Stacking, Explained and Automated</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">简单的模型堆叠，解释和自动化</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/simple-model-stacking-explained-and-automated-1b54e4357916?source=collection_archive---------6-----------------------#2021-10-06">https://towardsdatascience.com/simple-model-stacking-explained-and-automated-1b54e4357916?source=collection_archive---------6-----------------------#2021-10-06</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="693e" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsdatascience.com/tagged/hands-on-tutorials" rel="noopener" target="_blank">实践教程</a></h2><div class=""/><div class=""><h2 id="e107" class="pw-subtitle-paragraph jw iz iq bd b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn dk translated">在这个循序渐进的教程中，通过叠加来提高模型性能</h2></div><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ko"><img src="../Images/7e1d4ef44ed440bef1d9b92f8e85e950.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*O2LGE45iYWeHHkKA"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">由<a class="ae le" href="https://unsplash.com/@lastnameeaster?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> La-Rel复活节</a>在<a class="ae le" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><h1 id="297e" class="lf lg iq bd lh li lj lk ll lm ln lo lp kf lq kg lr ki ls kj lt kl lu km lv lw bi translated">模型堆叠概述</h1><p id="4bfe" class="pw-post-body-paragraph lx ly iq lz b ma mb ka mc md me kd mf mg mh mi mj mk ml mm mn mo mp mq mr ms ij bi translated">在模型堆叠中，我们不使用单个模型进行预测，而是使用几个不同的模型进行预测，然后使用这些预测作为更高级元模型的特征。它尤其适用于不同类型的低水平学习者，他们都对元模型有不同的贡献。模型堆栈可以用许多方法构建，使用堆栈没有一种“正确”的方法。它可以比今天的例子更复杂，有多个级别、权重、平均值等。我们今天将制作的基本模型堆栈如下所示:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi mt"><img src="../Images/a6dd72afb85e3b690b22f6c359d1b5d0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0qQTUDfImZYQBsyn9F6dpw.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">具有原始训练特征的模型堆叠-图片由作者提供</p></figure><p id="58f1" class="pw-post-body-paragraph lx ly iq lz b ma mu ka mc md mv kd mf mg mw mi mj mk mx mm mn mo my mq mr ms ij bi translated">在我们的堆栈中，我们将使用一系列中间模型对我们的训练数据进行非泄漏预测，然后将这些作为特征与元模型上的原始训练特征结合使用。</p><p id="eb86" class="pw-post-body-paragraph lx ly iq lz b ma mu ka mc md mv kd mf mg mw mi mj mk mx mm mn mo my mq mr ms ij bi translated">如果这听起来很复杂，不要被吓住。我将向您展示一种无痛的方法来自动化这个过程，包括选择最佳元模型、选择最佳堆栈模型、将所有数据放在一起，并对您的测试数据进行最终预测。</p><p id="372a" class="pw-post-body-paragraph lx ly iq lz b ma mu ka mc md mv kd mf mg mw mi mj mk mx mm mn mo my mq mr ms ij bi translated">今天，我们使用位于Kaggle上的<a class="ae le" href="https://www.kaggle.com/harlfoxem/housesalesprediction" rel="noopener ugc nofollow" target="_blank"> King County Housing </a>数据集来处理一个回归问题。</p><h1 id="31be" class="lf lg iq bd lh li lj lk ll lm ln lo lp kf lq kg lr ki ls kj lt kl lu km lv lw bi translated">起始位置</h1><blockquote class="mz na nb"><p id="49d3" class="lx ly nc lz b ma mu ka mc md mv kd mf nd mw mi mj ne mx mm mn nf my mq mr ms ij bi translated">健全性检查:本文不涉及任何预处理或模型调优。您的数据集应该是干净的，随时可以使用，如果需要，您的潜在模型应该进行超参数调整。您还应该将您的数据拆分为一个训练/测试集或一个训练/验证/测试集。这篇文章还假设读者对交叉验证有基本的了解。</p></blockquote><p id="720c" class="pw-post-body-paragraph lx ly iq lz b ma mu ka mc md mv kd mf mg mw mi mj mk mx mm mn mo my mq mr ms ij bi translated">我们首先用任何可选的超参数实例化我们调优的潜在模型。一个有趣且成功的堆栈的关键是<em class="nc">越多越好— </em>尽可能多地尝试您想要的模型！你<em class="nc">可以</em>节省时间，如果你事先对你所有的潜在模型进行交叉验证的抽查，以避免那些明显不能提供信息的模型。但是您可能会惊讶于一个糟糕的基本模型对一个堆栈的贡献有多大。请记住，您添加到可能性中的每个潜在模型都需要时间。但是在您的堆栈中可以尝试的模型数量没有限制，我们教程的关键是我们最终将只使用最好的模型，而不必手动选择任何模型。</p><p id="21ee" class="pw-post-body-paragraph lx ly iq lz b ma mu ka mc md mv kd mf mg mw mi mj mk mx mm mn mo my mq mr ms ij bi translated">对于我们的例子，我将只实例化五个潜在的模型。我们开始吧:</p><pre class="kp kq kr ks gt ng nh ni nj aw nk bi"><span id="04a1" class="nl lg iq nh b gy nm nn l no np">from sklearn.linear_model import LinearRegression, Ridge<br/>from sklearn.svm import SVR<br/>from sklearn.neighbors import KNeighborsRegressor<br/>import xgboost as xgb</span><span id="c5b7" class="nl lg iq nh b gy nq nn l no np">svr = SVR(gamma = 'scale', kernel = 'linear', C=10, epsilon=.05)</span><span id="222e" class="nl lg iq nh b gy nq nn l no np">ridge = Ridge(random_state = randomstate, tol=1e-3, normalize=False, solver='auto')</span><span id="2a73" class="nl lg iq nh b gy nq nn l no np">neighbor = KNeighborsRegressor(n_neighbors = 11)</span><span id="d38f" class="nl lg iq nh b gy nq nn l no np">linreg = LinearRegression()</span><span id="0d05" class="nl lg iq nh b gy nq nn l no np">xgbr = xgb.XGBRegressor(n_estimators=1000, eval_metric='mae', max_depth = 7,eta = .1, min_child_weight = 5, colsample_bytree = .4, reg_lambda = 50)</span></pre><p id="1dc3" class="pw-post-body-paragraph lx ly iq lz b ma mu ka mc md mv kd mf mg mw mi mj mk mx mm mn mo my mq mr ms ij bi translated">接下来，我们将创建空列表来存储每个潜在的训练集预测(我们将以非泄漏的方式获得)。您需要为每个潜在模型列出一个列表，一致的命名约定将有助于保持有序。</p><pre class="kp kq kr ks gt ng nh ni nj aw nk bi"><span id="8e0c" class="nl lg iq nh b gy nm nn l no np">svr_yhat, ridge_yhat, neighbor_yhat, linreg_yhat, xgbr_yhat = [], [], [], [], []</span></pre><p id="bacd" class="pw-post-body-paragraph lx ly iq lz b ma mu ka mc md mv kd mf mg mw mi mj mk mx mm mn mo my mq mr ms ij bi translated">最后，我们将实例化的模型和它们的空预测列表放到一个存储字典中，我们将在各种堆栈函数中使用它。每个字典条目的格式是“标签”:[模型实例，预测列表]，像这样:</p><pre class="kp kq kr ks gt ng nh ni nj aw nk bi"><span id="5039" class="nl lg iq nh b gy nm nn l no np">models_dict = {'SVR' : [svr, svr_yhat], <br/>                'Ridge' : [ridge, ridge_yhat],  <br/>                'KNN' : [neighbor, neighbor_yhat], <br/>                'Linear Regression' : [linreg, linreg_yhat], <br/>                'XGB' : [xgbr, xgbr_yhat]}</span></pre><blockquote class="mz na nb"><p id="552f" class="lx ly nc lz b ma mu ka mc md mv kd mf nd mw mi mj ne mx mm mn nf my mq mr ms ij bi translated">健全性检查:为以下函数准备数组格式的训练/测试集。X要素应该是(n，m)的数组形状，其中n是样本数，m是要素数，y目标应该是(n，)的数组。如果它们是数据帧，用np.array(df)转换它们</p></blockquote><h1 id="7723" class="lf lg iq bd lh li lj lk ll lm ln lo lp kf lq kg lr ki ls kj lt kl lu km lv lw bi translated">获得非折叠预测</h1><p id="acf4" class="pw-post-body-paragraph lx ly iq lz b ma mb ka mc md me kd mf mg mh mi mj mk ml mm mn mo mp mq mr ms ij bi translated">有一个超出折叠或超出样本的预测意味着什么？在模型堆叠中，我们使用对训练数据本身做出的预测来训练元模型。正确包含这些预测的方法是将我们的训练数据分成多个折叠，就像交叉验证一样，并使用剩余的折叠对每个折叠进行预测。通过这种方式，我们将拥有针对我们的训练数据的全套预测，但没有任何<em class="nc">数据泄漏</em>，如果我们只是简单地训练，然后在同一套数据上进行预测，就会出现这种情况。</p><p id="e88f" class="pw-post-body-paragraph lx ly iq lz b ma mu ka mc md mv kd mf mg mw mi mj mk mx mm mn mo my mq mr ms ij bi translated">这是我们的第一个函数，用来得到我们的非折叠预测。这个函数改编自<a class="ae le" href="https://machinelearningmastery.com/out-of-fold-predictions-in-machine-learning/" rel="noopener ugc nofollow" target="_blank">机器学习大师</a>上的大折叠预测教程:</p><pre class="kp kq kr ks gt ng nh ni nj aw nk bi"><span id="1eba" class="nl lg iq nh b gy nm nn l no np">from sklearn.model_selection import KFold</span><span id="abd0" class="nl lg iq nh b gy nq nn l no np">def train_oof_predictions(x, y, models, verbose=True):<br/>    '''Function to perform Out-Of-Fold predictions on train data<br/>    returns re-ordered predictors x, re-ordered target y, and model dictionary with filled predictors<br/>    Parameters:<br/>    x: training predictors<br/>    y: training targets<br/>    models: dictionary of models in form of model name : [instantiated model, predictors list]<br/>    verbose: if True, prints status update as the function works<br/>    '''<br/>    <br/>    # instantiate a KFold with 10 splits<br/>    kfold = KFold(n_splits=10, shuffle=True, random_state=randomstate)<br/>    <br/>    # prepare lists to hold the re-ordered x and y values<br/>    data_x, data_y  = [], []<br/>    <br/>    # run the following block for each of the 10 kfold splits<br/>    for train_ix, test_ix in kfold.split(x, y):<br/>    <br/>        if verbose: print("\nStarting a new fold\n")<br/>    <br/>        if verbose: print("Creating splits")<br/>        #create this fold's training and test sets<br/>        train_X, test_X = x[train_ix], x[test_ix] <br/>        train_y, test_y = y[train_ix], y[test_ix]<br/>    <br/>        if verbose: print("Adding x and y to lists\n")<br/>        # add the data that is used in this fold to the re-ordered lists<br/>        data_x.extend(test_X)<br/>        data_y.extend(test_y)<br/>    <br/>        # run each model on this kfold and add the predictors to the model's running predictors list<br/>        for item in models:<br/>            <br/>            label = item # get label for reporting purposes<br/>            model = models[item][0] # get the model to use on the kfold<br/>        <br/>            # fit and make predictions <br/>            if verbose: print("Running",label,"on this fold")<br/>            model.fit(train_X, train_y) # fit to the train set for the kfold<br/>            predictions = model.predict(test_X) # fit on the out-of-fold set<br/>            models[item][1].extend(predictions) # add predictions to the model's running predictors list<br/>    <br/>    return data_x, data_y, models</span></pre><p id="0dce" class="pw-post-body-paragraph lx ly iq lz b ma mu ka mc md mv kd mf mg mw mi mj mk mx mm mn mo my mq mr ms ij bi translated">现在，我们已经准备好了，使用我们之前制作的模型字典，来获得非折叠预测。该函数默认为verbose，并将提供有关其进度的状态更新。请记住，如果您的堆栈中有大量数据集或大量模型要尝试，那么获得OOF预测可能需要很长时间！</p><p id="f938" class="pw-post-body-paragraph lx ly iq lz b ma mu ka mc md mv kd mf mg mw mi mj mk mx mm mn mo my mq mr ms ij bi translated">运行折叠外预测功能:</p><pre class="kp kq kr ks gt ng nh ni nj aw nk bi"><span id="f1fe" class="nl lg iq nh b gy nm nn l no np">data_x, data_y, trained_models = train_oof_predictions(X_train, y_train, models_dict)</span></pre><blockquote class="mz na nb"><p id="e4eb" class="lx ly nc lz b ma mu ka mc md mv kd mf nd mw mi mj ne mx mm mn nf my mq mr ms ij bi translated">健全性检查:检查这个折叠函数的一致输出。字典中的所有yhats都应该返回普通的数字列表，没有任何数组。</p></blockquote><p id="4d02" class="pw-post-body-paragraph lx ly iq lz b ma mu ka mc md mv kd mf mg mw mi mj mk mx mm mn mo my mq mr ms ij bi translated">我们现在有了data_x和data_y，它们是与我们的x_train和y_train相同的数据，但是<em class="nc">被重新排序以匹配势的yhat预测</em>的顺序。我们返回的trained_models字典包含对整个训练集和每个潜在模型的预测。</p><h1 id="1fe4" class="lf lg iq bd lh li lj lk ll lm ln lo lp kf lq kg lr ki ls kj lt kl lu km lv lw bi translated">运行堆栈选择器</h1><p id="40fd" class="pw-post-body-paragraph lx ly iq lz b ma mb ka mc md me kd mf mg mh mi mj mk ml mm mn mo mp mq mr ms ij bi translated">堆栈选择器时间到了。这是我们的下一个函数。这个是基于大卫·戴尔<a class="ae le" href="https://datascience.stackexchange.com/questions/937/does-scikit-learn-have-a-forward-selection-stepwise-regression-algorithm" rel="noopener ugc nofollow" target="_blank">写的特征选择向前向后选择器:</a></p><pre class="kp kq kr ks gt ng nh ni nj aw nk bi"><span id="16d0" class="nl lg iq nh b gy nm nn l no np">from sklearn.model_selection import cross_validate</span><span id="fd36" class="nl lg iq nh b gy nq nn l no np">def model_selector(X, y, meta_model, models_dict, model_label, verbose=True):<br/>    <br/>    """ <br/>    Perform a forward model selection based on MAE improvement<br/>    Parameters:<br/>        X - baseline X_train with all features<br/>        y - baseline y_train with all targets<br/>        meta_model - meta_model to be trained<br/>        models_dict - dictionary of models in format of model name : [model object, out-of-fold predictions]<br/>        label - the label for the current meta model<br/>        verbose - whether to print the sequence of inclusions(True recommended)<br/>    Returns: list of selected models, best MAE <br/>    """</span><span id="3b4e" class="nl lg iq nh b gy nq nn l no np">print("\n\nRunning model selector for ", model_label)<br/>    included_models = []<br/>     <br/>    while True:<br/>        changed=False<br/>        <br/>        # forward step<br/>        <br/>        if verbose: print("\nNEW ROUND - Setting up score charts")<br/>        excluded_models = list(set(models_dict.keys())-set(included_models)) # make a list of the current excluded_models<br/>        if verbose: print("Included models: {}".format(included_models))<br/>        if verbose: print("Exluded models: {}".format(excluded_models))<br/>        new_mae = pd.Series(index=excluded_models) # make a series where the index is the current excluded_models<br/>        <br/>        current_meta_x = np.array(X)<br/>        <br/>        if len(included_models) &gt; 0:<br/>            for included in included_models:<br/>                included = np.array(models_dict[included][1]).reshape((len(models_dict[included][1]), 1))<br/>                current_meta_x = np.hstack((current_meta_x, included))</span><span id="bcf9" class="nl lg iq nh b gy nq nn l no np"># score the current model<br/>        scores = cross_validate(meta_model, current_meta_x, y, cv=5, n_jobs=-1, scoring=('neg_mean_absolute_error'))<br/>        starting_mae = round(scores['test_score'].mean(),3)<br/>        if verbose: print("Starting mae: {}\n".format(starting_mae))<br/>        <br/>       <br/>        for excluded in excluded_models:  # for each item in the excluded_models list:<br/>            <br/>            new_yhat = np.array(models_dict[excluded][1]).reshape(-1, 1) # get the current item's predictions<br/>            meta_x = np.hstack((current_meta_x, new_yhat)) # add the predictions to the meta set<br/>            <br/>            # score the current item<br/>            scores = cross_validate(meta_model, meta_x, y, cv=5, n_jobs=-1, scoring=('neg_mean_absolute_error'))<br/>            mae = round(scores['test_score'].mean(),3)<br/>            if verbose: print("{} score: {}".format(excluded, mae))<br/>            <br/>            new_mae[excluded] = mae # append the mae to the series field<br/>        <br/>        best_mae = new_mae.max() # evaluate best mae of the excluded_models in this round<br/>        if verbose: print("Best mae: {}\n".format(best_mae))<br/>        <br/>        if best_mae &gt; starting_mae:  # if the best mae is better than the initial mae<br/>            best_feature = new_mae.idxmax()  # define this as the new best feature<br/>            included_models.append(str(best_feature)) # append this model name to the included list<br/>            changed=True # flag that we changed it<br/>            if verbose: print('Add  {} with mae {}\n'.format(best_feature, best_mae))<br/>        else: changed = False<br/>        <br/>        if not changed:<br/>            break<br/>            <br/>    print(model_label, "model optimized")<br/>    print('resulting models:', included_models)<br/>    print('MAE:', starting_mae)<br/>    <br/>    return included_models, starting_mae</span></pre><blockquote class="mz na nb"><p id="b34d" class="lx ly nc lz b ma mu ka mc md mv kd mf nd mw mi mj ne mx mm mn nf my mq mr ms ij bi translated">健全性检查:我的功能是对平均绝对误差进行评分，但是您可以根据您最喜欢的评分标准进行编辑，比如R2或RMSE。</p></blockquote><p id="7ef6" class="pw-post-body-paragraph lx ly iq lz b ma mu ka mc md mv kd mf mg mw mi mj mk mx mm mn mo my mq mr ms ij bi translated">我们将最终使用我们的每个潜在模型作为函数的元模型来运行这个函数。当我们运行这个函数时，我们向它发送从out-of-fold函数中得到的data_x和data_y，还有一个实例化的模型作为元模型，以及我们的字典和所有out-of-fold预测。然后，该函数使用折叠外预测作为特征运行向前选择。</p><p id="fc32" class="pw-post-body-paragraph lx ly iq lz b ma mu ka mc md mv kd mf mg mw mi mj mk mx mm mn mo my mq mr ms ij bi translated">对于被尝试作为元模型的模型，我们在训练集上获得基线分数(使用CV)。对于每个其他潜在模型，我们迭代地将潜在的yhat预测附加到特征集，并使用该附加特征对元模型重新评分。如果我们的元模型得分随着任何特征的增加而提高，那么单个最佳得分潜力的预测将被永久地附加到特征集，并且提高的得分将成为基线。然后，该函数循环，再次尝试添加尚未在堆栈中的每个潜在值，直到没有潜在值可以提高分数。然后，该函数报告这个元模型的最佳包含模型，以及获得的最佳分数。</p><blockquote class="mz na nb"><p id="f17c" class="lx ly nc lz b ma mu ka mc md mv kd mf nd mw mi mj ne mx mm mn nf my mq mr ms ij bi translated">健全性检查:编写这个模型选择器是为了使用CV在训练集上进行优化。如果你足够幸运有一个验证集，你可以重写并执行选择。它会快得多——但要小心过度拟合！</p></blockquote><p id="9224" class="pw-post-body-paragraph lx ly iq lz b ma mu ka mc md mv kd mf mg mw mi mj mk mx mm mn mo my mq mr ms ij bi translated">我们如何为这个任务选择元模型？有趣的部分来了——我们将尝试我们所有的模型作为元模型。请记住，此功能可能需要很长时间才能运行。将verbose设置为True会给出频繁的进度报告。</p><p id="4c7f" class="pw-post-body-paragraph lx ly iq lz b ma mu ka mc md mv kd mf mg mw mi mj mk mx mm mn mo my mq mr ms ij bi translated">我们制作了一个字典来存储我们将从测试中得到的所有分数。然后，我们在每个经过训练的模型上运行堆栈选择器，将该模型用作元模型:</p><pre class="kp kq kr ks gt ng nh ni nj aw nk bi"><span id="cd31" class="nl lg iq nh b gy nm nn l no np"># Set up a scoring dictionary to hold the model stack selector results<br/>scores = {}<br/>scores['Model'] = []<br/>scores['MAE'] = []<br/>scores['Included'] = []</span><span id="624e" class="nl lg iq nh b gy nq nn l no np"># Run the model stack selector for each model in our trained_models</span><span id="c799" class="nl lg iq nh b gy nq nn l no np">for model in trained_models:<br/>    <br/>    meta_model = trained_models[model][0]</span><span id="4496" class="nl lg iq nh b gy nq nn l no np">    resulting_models, best_mae = model_selector(data_x, data_y,  meta_model, trained_models, label, verbose=True)<br/>    <br/>    scores['Model'].append(model)<br/>    scores['MAE'].append(best_mae)<br/>    scores['Included'].append(resulting_models)</span></pre><p id="7fcd" class="pw-post-body-paragraph lx ly iq lz b ma mu ka mc md mv kd mf mg mw mi mj mk mx mm mn mo my mq mr ms ij bi translated">之后，我们将得到每个模型作为元模型的表现分数，以及它最好的附加部分。我们将字典转换成数据框架，并根据分数标准对结果进行排序:</p><pre class="kp kq kr ks gt ng nh ni nj aw nk bi"><span id="b14e" class="nl lg iq nh b gy nm nn l no np"># Look at the scores of our model combinations</span><span id="8fb6" class="nl lg iq nh b gy nq nn l no np">best_model = pd.DataFrame(scores).reset_index(drop=True)<br/>best_model.sort_values('MAE', ascending=False)<br/></span></pre><p id="ec41" class="pw-post-body-paragraph lx ly iq lz b ma mu ka mc md mv kd mf mg mw mi mj mk mx mm mn mo my mq mr ms ij bi translated">现在，我们可以确切地看到哪个元模型和堆叠模型执行得最好。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/78dbae36a4d3940430e63b11697a0464.png" data-original-src="https://miro.medium.com/v2/resize:fit:858/format:webp/1*-S4PJ4eJUKzdrGMWRAmNcQ.png"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">作者图片</p></figure><h1 id="9bca" class="lf lg iq bd lh li lj lk ll lm ln lo lp kf lq kg lr ki ls kj lt kl lu km lv lw bi translated">把所有的放在一起</h1><p id="7ff0" class="pw-post-body-paragraph lx ly iq lz b ma mb ka mc md me kd mf mg mh mi mj mk ml mm mn mo mp mq mr ms ij bi translated">我们快完成了！很快我们将在我们的测试集上进行预测。我们已经选择了一个元模型(可能是在堆栈选择器上表现最好的一个！)和我们将包括的堆叠模型。</p><p id="c640" class="pw-post-body-paragraph lx ly iq lz b ma mu ka mc md mv kd mf mg mw mi mj mk mx mm mn mo my mq mr ms ij bi translated">在我们将元模型与堆叠数据相匹配之前，在测试集上尝试没有添加特性的模型，这样您就可以比较堆叠模型的改进了！在原始训练/测试集上拟合和预测元模型。这是我们期望用堆叠模型打破的基线。</p><pre class="kp kq kr ks gt ng nh ni nj aw nk bi"><span id="e46e" class="nl lg iq nh b gy nm nn l no np"># Check our meta model on the original train/test set only</span><span id="b60a" class="nl lg iq nh b gy nq nn l no np"># Instantiate the chosen meta model<br/>meta_model =  SVR(gamma = 'scale', kernel = 'linear', C=10, epsilon=.05)</span><span id="5dc4" class="nl lg iq nh b gy nq nn l no np">meta_model.fit(X_train, y_train)<br/>predictions = meta_model.predict(X_test)</span><span id="a868" class="nl lg iq nh b gy nq nn l no np">pred_exp = np.exp(predictions)<br/>actual = np.exp(y_test)</span><span id="5c80" class="nl lg iq nh b gy nq nn l no np">print("MAE: ",int(mean_absolute_error(pred_exp, actual)))<br/>print("RMSE:",int(np.sqrt(mean_squared_error(pred_exp, actual))))<br/>print(("R2:",r2_score(pred_exp, actual)*100)</span><span id="e6cc" class="nl lg iq nh b gy nq nn l no np"><em class="nc">Output:<br/>MAE:  53130<br/>RMSE: 82427<br/>R2: 86.93570728212</em></span></pre><p id="e785" class="pw-post-body-paragraph lx ly iq lz b ma mu ka mc md mv kd mf mg mw mi mj mk mx mm mn mo my mq mr ms ij bi translated">后退一步，将我们将在堆栈中使用的所有模型放入我们的原始训练数据集。我们这样做是因为我们将在测试集上进行预测(与单个模型相同！)并将预测作为元模型的特征添加到我们的测试集中。</p><pre class="kp kq kr ks gt ng nh ni nj aw nk bi"><span id="2065" class="nl lg iq nh b gy nm nn l no np">print("Fitting Models")<br/>linreg.fit(X_train, y_train)<br/>xgbr.fit(X_train, y_train)<br/>knn.fit(X_train, y_train)</span></pre><p id="fa56" class="pw-post-body-paragraph lx ly iq lz b ma mu ka mc md mv kd mf mg mw mi mj mk mx mm mn mo my mq mr ms ij bi translated">现在我们准备模型堆栈。首先，手动创建一个列表，只存储我们在最终堆栈中使用的模型的折叠外预测。我们从先前使用折叠外预测函数生成的“trained_models”字典中获得这些预测:</p><pre class="kp kq kr ks gt ng nh ni nj aw nk bi"><span id="cc91" class="nl lg iq nh b gy nm nn l no np">yhat_predics = [trained_models['XGB'][1], trained_models['Linear Regression'][1], trained_models['KNN'][1]]</span></pre><p id="3d01" class="pw-post-body-paragraph lx ly iq lz b ma mu ka mc md mv kd mf mg mw mi mj mk mx mm mn mo my mq mr ms ij bi translated">又到了一个功能的时间了。这一个接受我们重新排序的train data_x和包含yhat预测的列表，并把它们放在一个单一的元train集合中。我们在一个函数中这样做，因为稍后我们将对测试数据再次这样做。</p><pre class="kp kq kr ks gt ng nh ni nj aw nk bi"><span id="6ab8" class="nl lg iq nh b gy nm nn l no np">def create_meta_dataset(data_x, items):<br/>    '''Function that takes in a data set and list of predictions, and forges into one dataset<br/>    parameters:<br/>    data_x - original data set<br/>    items - list of predictions<br/>    returns: stacked data set<br/>    '''<br/>    <br/>    meta_x = data_x<br/>    <br/>    for z in items:<br/>        z = np.array(z).reshape((len(z), 1))<br/>        meta_x = np.hstack((meta_x, z))<br/>        <br/>    return meta_x</span></pre><p id="3dc2" class="pw-post-body-paragraph lx ly iq lz b ma mu ka mc md mv kd mf mg mw mi mj mk mx mm mn mo my mq mr ms ij bi translated">现在调用这个函数传递data_x和预测列表，为我们的元模型创建一个元训练集:</p><pre class="kp kq kr ks gt ng nh ni nj aw nk bi"><span id="ca29" class="nl lg iq nh b gy nm nn l no np"># create the meta data set using the oof predictions<br/>meta_X_train = create_meta_dataset(data_x, yhat_predics)</span></pre><p id="f39b" class="pw-post-body-paragraph lx ly iq lz b ma mu ka mc md mv kd mf mg mw mi mj mk mx mm mn mo my mq mr ms ij bi translated">元训练集由我们的原始预测特征以及堆叠模型的非折叠预测作为附加特征组成。</p><p id="d41f" class="pw-post-body-paragraph lx ly iq lz b ma mu ka mc md mv kd mf mg mw mi mj mk mx mm mn mo my mq mr ms ij bi translated">制作一个列表来保存拟合的模型实例；我们将在下一个也是最后一个函数中用到它。确保这与您为元堆栈器列出的yhat预测顺序相同！</p><pre class="kp kq kr ks gt ng nh ni nj aw nk bi"><span id="ff3e" class="nl lg iq nh b gy nm nn l no np">final_models = [xgbr, linreg, knn]</span></pre><p id="bb51" class="pw-post-body-paragraph lx ly iq lz b ma mu ka mc md mv kd mf mg mw mi mj mk mx mm mn mo my mq mr ms ij bi translated">我们最后一次活动的时间到了。该函数接受我们的测试集和拟合模型，使用拟合模型对测试集进行预测，然后将这些预测作为特征添加到测试集。它发回一个完整的meta_x集合，供元模型进行预测。</p><pre class="kp kq kr ks gt ng nh ni nj aw nk bi"><span id="35d7" class="nl lg iq nh b gy nm nn l no np">def stack_prediction(X_test, final_models): <br/>    '''takes in a test set and a list of fitted models.<br/>    Fits each model in the list on the test set and stores it in a predictions list. Then sends the test set and the predictions to the create_meta_dataset to be combined<br/>    Returns: combined meta test set<br/>    Parameters:<br/>    X_test - testing dataset<br/>    final_models - list of fitted models<br/>    '''<br/>    predictions = []<br/>    <br/>    for item in final_dict:<br/>        print(item)<br/>        preds = item.predict(X_test).reshape(-1,1)<br/>        predictions.append(preds)<br/>    <br/>    meta_X = create_meta_dataset(X_test, predictions)<br/>        <br/>    return meta_X</span></pre><p id="89f3" class="pw-post-body-paragraph lx ly iq lz b ma mu ka mc md mv kd mf mg mw mi mj mk mx mm mn mo my mq mr ms ij bi translated">这里我们调用函数，发送测试集和拟合模型:</p><pre class="kp kq kr ks gt ng nh ni nj aw nk bi"><span id="ac30" class="nl lg iq nh b gy nm nn l no np">meta_X_test = stack_prediction(X_test, final_models)</span></pre><h2 id="47e9" class="nl lg iq bd lh ns nt dn ll nu nv dp lp mg nw nx lr mk ny nz lt mo oa ob lv iw bi translated">最终模型评估</h2><p id="3824" class="pw-post-body-paragraph lx ly iq lz b ma mb ka mc md me kd mf mg mh mi mj mk ml mm mn mo mp mq mr ms ij bi translated">关键时刻终于来了。使用堆叠模型进行预测的时间到了！</p><pre class="kp kq kr ks gt ng nh ni nj aw nk bi"><span id="8947" class="nl lg iq nh b gy nm nn l no np"># fit the meta model to the Train meta dataset<br/># There is no data leakage in the meta dataset since we did all of our predictions out-of-sample!<br/>meta_model.fit(meta_X_train, data_y)</span><span id="4633" class="nl lg iq nh b gy nq nn l no np"># predict on the meta test set<br/>predictions = meta_model.predict(meta_X_test)</span><span id="a9c6" class="nl lg iq nh b gy nq nn l no np">pred_exp = np.exp(predictions)<br/>actual = np.exp(y_test)</span><span id="49f8" class="nl lg iq nh b gy nq nn l no np">print("MAE: ",int(mean_absolute_error(pred_exp, actual)))<br/>print("RMSE:",int(np.sqrt(mean_squared_error(pred_exp, actual))))<br/>print(("R2:",r2_score(pred_exp, actual)*100)</span><span id="31d8" class="nl lg iq nh b gy nq nn l no np"><em class="nc">Output:<br/>MAE:  47205<br/>RMSE: 73973<br/>R2: 90.03816032670765</em></span></pre><p id="55c7" class="pw-post-body-paragraph lx ly iq lz b ma mu ka mc md mv kd mf mg mw mi mj mk mx mm mn mo my mq mr ms ij bi translated">在我们的堆栈示例中，我们将测试集上的MAE从53130减少到47205，提高了11.15%。</p><p id="da16" class="pw-post-body-paragraph lx ly iq lz b ma mu ka mc md mv kd mf mg mw mi mj mk mx mm mn mo my mq mr ms ij bi translated">我希望你能看到你的模型分数的提高，并能看到尝试堆叠模型的效用！我相信您已经学到了一些有价值的工具，可以添加到自动化模型选择和堆叠的工具包中。</p><p id="8df0" class="pw-post-body-paragraph lx ly iq lz b ma mu ka mc md mv kd mf mg mw mi mj mk mx mm mn mo my mq mr ms ij bi translated">参考资料:</p><ol class=""><li id="0f41" class="oc od iq lz b ma mu md mv mg oe mk of mo og ms oh oi oj ok bi translated">如何在机器学习中使用非折叠预测</li></ol><p id="1287" class="pw-post-body-paragraph lx ly iq lz b ma mu ka mc md mv kd mf mg mw mi mj mk mx mm mn mo my mq mr ms ij bi translated">2.<a class="ae le" href="https://datascience.stackexchange.com/questions/937/does-scikit-learn-have-a-forward-selection-stepwise-regression-algorithm" rel="noopener ugc nofollow" target="_blank">大卫·戴尔的前向-后向特征选择</a></p></div></div>    
</body>
</html>