<html>
<head>
<title>Siamese NN Recipes with Keras</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">暹罗NN食谱与Keras</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/siamese-nn-recipes-with-keras-72f6a26deb64?source=collection_archive---------10-----------------------#2021-09-07">https://towardsdatascience.com/siamese-nn-recipes-with-keras-72f6a26deb64?source=collection_archive---------10-----------------------#2021-09-07</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="b451" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">用于语义相似性任务的具有Keras和BERT的实用暹罗神经网络配方</h2></div><p id="e36d" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我已经享受了一段时间在我的工作不同的NLU任务的暹罗网络。在本文中，我将与Keras分享一些快速的方法，将Glove vectors或BERT作为文本矢量器。我们将关注语义相似度计算。语义相似性基本上是确定一组文本是否相关的任务。语义相似度通常是在一对文本段之间计算的。在这篇文章中，我将举例说明如何比较两个文本。</p><p id="c9b7" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">连体网络是具有两个或更多输入的神经网络(通常输入的数量是两个，否则必须定义三向距离函数)。我们对输入文本进行编码，然后将编码后的向量提供给距离层。最后，我们在距离层上运行一个分类层。距离可以是余弦距离、L1距离、指数负曼哈顿距离和任何其他距离函数。这是一个作为黑盒的连体网络:</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi lb"><img src="../Images/05c3f786dac54b91267d4578b415a5cd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5QAucYflZOfxPsnE0blkrg.png"/></div></div><p class="ln lo gj gh gi lp lq bd b be z dk translated">更高层次的暹罗NN。如果2个输入和NN个输出在语义上相关，我们就给它们</p></figure><p id="dbf5" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">可以通过LSTM、通用编码器或BERT对输入文本进行编码。根据带有单词嵌入的文本编码，该体系结构如下图所示:</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi lb"><img src="../Images/77d789026e99a5784b15b0e85d2f3ed6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jyPZCDVLuvW4X_K-jXEJ3g.png"/></div></div><p class="ln lo gj gh gi lp lq bd b be z dk translated">详细的暹罗建筑。我们首先用LSTM/伯特对输入句子进行编码，然后将编码后的向量对送入距离层。</p></figure><p id="508c" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这是带有LSTM和预训练嵌入层的Keras配方:</p><pre class="lc ld le lf gt lr ls lt lu aw lv bi"><span id="c533" class="lw lx iq ls b gy ly lz l ma mb">from keras import backend as K</span><span id="9ca1" class="lw lx iq ls b gy mc lz l ma mb">first_sent_in = Input(shape=(MAX_LEN,))<br/>second_sent_in = Input(shape=(MAX_LEN,))</span><span id="8a37" class="lw lx iq ls b gy mc lz l ma mb">embedding_layer =  Embedding(input_dim=n_words+1, output_dim=embed_size, embeddings_initializer=Constant(embedding_matrix), input_length=MAX_LEN, trainable=True, mask_zero=True)</span><span id="eefb" class="lw lx iq ls b gy mc lz l ma mb">first_sent_embedding = embedding_layer(first_sent_in)<br/>second_sent_embedding = embedding_layer(second_sent_in)</span><span id="7f7e" class="lw lx iq ls b gy mc lz l ma mb">lstm =  Bidirectional(LSTM(units=256, return_sequences=False))</span><span id="5fff" class="lw lx iq ls b gy mc lz l ma mb">first_sent_encoded = lstm(first_sent_embedding)<br/>second_sent_encoded = lstm(second_sent_embedding)</span><span id="2138" class="lw lx iq ls b gy mc lz l ma mb">l1_norm = lambda x: 1 - K.abs(x[0] - x[1])</span><span id="80a5" class="lw lx iq ls b gy mc lz l ma mb">merged = Lambda(function=l1_norm, output_shape=lambda x: x[0], name='L1_distance')([first_sent_encoded, second_sent_encoded])</span><span id="778c" class="lw lx iq ls b gy mc lz l ma mb">predictions = Dense(1, activation='sigmoid', name='classification_layer')(merged)</span><span id="c8a2" class="lw lx iq ls b gy mc lz l ma mb">model = Model([first_sent_in, second_sent_in], predictions)<br/>model.compile(loss = 'binary_crossentropy', optimizer = "adam", metrics=["accuracy"])<br/>print(model.summary())</span><span id="5bcd" class="lw lx iq ls b gy mc lz l ma mb">model.fit([fsents, ssents], labels, validation_split=0.1, epochs = 20,shuffle=True, batch_size = 256)</span></pre><p id="e864" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">模型摘要应该是这样的:</p><pre class="lc ld le lf gt lr ls lt lu aw lv bi"><span id="b222" class="lw lx iq ls b gy ly lz l ma mb">Layer (type)                    Output Shape         Param #     Connected to                     <br/>==================================================================================================<br/>input_1 (InputLayer)            [(None, 200)]        0                                            <br/>__________________________________________________________________________________________________<br/>input_2 (InputLayer)            [(None, 200)]        0                                            <br/>__________________________________________________________________________________________________<br/>embedding (Embedding)           (None, 200, 100)     1633800     input_1[0][0]                    <br/>                                                                 input_2[0][0]                    <br/>__________________________________________________________________________________________________<br/>bidirectional (Bidirectional)   (None, 512)          731136      embedding[0][0]                  <br/>                                                                 embedding[1][0]                  <br/>__________________________________________________________________________________________________<br/>L1_distance (Lambda)            (None, 512)          0           bidirectional[0][0]              <br/>                                                                 bidirectional[1][0]              <br/>__________________________________________________________________________________________________<br/>classification_layer (Dense)    (None, 1)            1026        L1_distance[0][0]                <br/>==================================================================================================<br/>Total params: 2,365,962<br/>Trainable params: 2,365,962<br/>Non-trainable params: 0<br/>______________________________</span></pre><p id="2b49" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">两个输入层输入要比较的两个文本。然后，我们将输入单词馈送到嵌入层，以获得每个输入单词的单词嵌入。之后，我们分别将第一句话的嵌入向量馈送到LSTM层，将第二句话的嵌入向量馈送到LSTM层，并获得第一文本和第二文本的密集表示(用变量<em class="md"> first_sent_encoded </em>和<em class="md"> second_sent_encoded </em>表示)。现在是棘手的部分，合并层。合并层输入第一个文本和第二个文本的密集表示，并计算它们之间的距离。如果你查看模型摘要的第四层，你会看到<em class="md">L1 _距离(λ)</em>(这一层在技术上是一个Keras层)，接受两个输入，它们都是LSTM层的输出。结果是一个512维的向量，我们将这个向量提供给分类器。结果是一个一维向量，它是0或1，因为我在做相似或不相似的二元分类。在分类层，我用<strong class="kh ir"> sigmoid </strong>压缩了512维距离向量(因为我很喜欢这个激活函数:)，我还用<strong class="kh ir"> binary_crossentory </strong>编译了模型，因为这也是一个二进制分类任务。</p><p id="9502" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在这个配方中，使用了<strong class="kh ir">L1-距离</strong>来计算编码向量之间的距离。您可以使用余弦距离或任何其他距离。我特别喜欢l1距离，因为它的<em class="md">不像函数</em>那么平滑。这同样适用于sigmoid函数，它为<em class="md">提供了语言神经网络所需的非线性</em>。</p><p id="a4d6" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">伯特的食谱有点不同。我们去掉了嵌入+ LSTM层，取而代之的是伯特层(因为伯特向量包含了足够多的顺序性！):</p><pre class="lc ld le lf gt lr ls lt lu aw lv bi"><span id="c57a" class="lw lx iq ls b gy ly lz l ma mb">fsent_inputs = Input(shape=(MAX_L,), dtype="int32")                       fsent_encoded = bert_model(fsent_inputs)                       fsent_encoded = fsent_encoded[1]                                               </span><span id="eac9" class="lw lx iq ls b gy mc lz l ma mb">ssent_inputs = Input(shape=(150,), dtype="int32")                       ssent_encoded = bert_model(ssent_inputs)                       ssent_encoded = ssent_encoded[1]                                               </span><span id="fb95" class="lw lx iq ls b gy mc lz l ma mb">merged =concatenate([fsent_encoded, ssent_encoded])                                               predictions = Dense(1, activation='sigmoid', name='classification_layer')(merged)                                               </span><span id="6071" class="lw lx iq ls b gy mc lz l ma mb">model = Model([fsent_inputs, ssent_inputs], predictions)                                               adam = keras.optimizers.Adam(learning_rate=2e-6,epsilon=1e-08)                                           </span><span id="289a" class="lw lx iq ls b gy mc lz l ma mb">model.compile(loss="binary_crossentropy", metrics=["accuracy"], optimizer="adam")</span></pre><p id="50c6" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这里，我们再次分别向BERT层提供一对文本输入，并获得它们的编码<em class="md"> fsent-encoded </em>和<em class="md"> ssent_encoded </em>。我们使用<strong class="kh ir">【CLS】</strong>标记的嵌入，它捕获句子的平均表示。(BERT层有2个输出，第一个是<strong class="kh ir"> CLS </strong>令牌的向量，第二个是<em class="md"> (MAX_LEN，768) </em>的向量。第二个输出为输入句子的每个标记提供了一个向量。我们通过调用<em class="md">fsent _ encoded = fsent _ encoded[1]</em>和<em class="md">ssent _ encoded = ssent _ encoded[1]</em>来使用第一个输入。优化器也是Adam，但学习率略有不同(我们降低lr以防止BERT表现过激和过度拟合。如果我们不加以阻止，伯特很快就会过度适应)。损失也是二元交叉熵，因为我们在做二元分类任务。基本上我用BERT层替换了嵌入层+ LSTM，其余的架构是相同的。</p></div><div class="ab cl me mf hu mg" role="separator"><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj"/></div><div class="ij ik il im in"><p id="b41d" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">亲爱的读者，您已经阅读完了这篇实用文章。我的目的是提供一个快速的工作解决方案，以及解释如何暹罗网络的工作。如果你愿意，你可以把辍学层放在几个地方，自己做实验。玩暹罗网络很令人兴奋，就我个人而言，它们可能是我最喜欢的架构。我希望你喜欢暹罗NN，并在你的工作中使用。在新架构会议之前，感谢您的关注，敬请关注！</p></div></div>    
</body>
</html>