<html>
<head>
<title>How to Use Reinforcement Learning to Recommend Content</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如何利用强化学习推荐内容</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/how-to-use-reinforcement-learning-to-recommend-content-6d7f9171b956?source=collection_archive---------9-----------------------#2021-08-23">https://towardsdatascience.com/how-to-use-reinforcement-learning-to-recommend-content-6d7f9171b956?source=collection_archive---------9-----------------------#2021-08-23</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="0636" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">谷歌的研究人员开发了一个新的RL框架。</h2></div><p id="a865" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">强化学习理论上是最有效的深度学习方法之一。然而，实际上它并不处理复杂的问题。谷歌开发的RL框架RecSim ，允许优化复杂的推荐系统。</p><figure class="ld le lf lg gt lh gh gi paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="gh gi lc"><img src="../Images/b30a853bfbf78eaa3444523f984ef33c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EM9TykLHdedkFpzF0viVFQ.png"/></div></div><p class="lo lp gj gh gi lq lr bd b be z dk translated">图1:强化学习框架。图片作者。</p></figure><p id="bb0f" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">谷歌和UT奥斯汀的研究人员创建了一个更强大的RL框架，允许动态用户功能，并处理一些RL的技术问题。虽然这种方法计算量很大，但它提供了一个允许优化或推荐引擎的离线开发环境。</p><p id="6b40" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">让我们深入了解一下这种方法是如何工作的…</p><h1 id="35cc" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">技术TLDR</h1><p id="c483" class="pw-post-body-paragraph kf kg iq kh b ki mk jr kk kl ml ju kn ko mm kq kr ks mn ku kv kw mo ky kz la ij bi translated">RecSim是一个利用强化学习(RL)的模拟环境构建器。该方法由“模拟器”模块控制，该模块负责对用户/文档进行采样，并迭代地训练推荐代理。该方法支持与用户的顺序交互，并允许工程师对环境进行大量定制。</p><p id="9aa9" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这篇论文和其他一些有用的资源在评论中有链接。</p><h1 id="3bdd" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">好的，很多了。它实际上是如何工作的？</h1><p id="9995" class="pw-post-body-paragraph kf kg iq kh b ki mk jr kk kl ml ju kn ko mm kq kr ks mn ku kv kw mo ky kz la ij bi translated">让我们慢一点，真正理解这个方法是如何工作的。</p><h2 id="35d2" class="mp lt iq bd lu mq mr dn ly ms mt dp mc ko mu mv me ks mw mx mg kw my mz mi na bi translated">强化学习的背景</h2><p id="c897" class="pw-post-body-paragraph kf kg iq kh b ki mk jr kk kl ml ju kn ko mm kq kr ks mn ku kv kw mo ky kz la ij bi translated">强化学习(RL)是一个框架，它涉及通过重复模拟来训练代理人做出决策。简而言之，代理做出决定，从模拟器获得反馈，调整其决定，然后再次尝试。重复这个过程，直到损失函数被优化。</p><figure class="ld le lf lg gt lh gh gi paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="gh gi nb"><img src="../Images/a33c5e290b596f79a037a7208e66a857.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iOhRFAnZW0vmavnrQ1EDcw.png"/></div></div><p class="lo lp gj gh gi lq lr bd b be z dk translated">图2:雅达利突围:2013年第一款RL游戏。图片作者——<a class="ae lb" href="https://en.m.wikipedia.org/wiki/File:Atari_breakout.jpg" rel="noopener ugc nofollow" target="_blank">src</a>。</p></figure><p id="65d5" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现代RL算法首次应用于雅达利游戏《突围》(图2)。代理人控制底部的桨，并通过用球打破障碍来最大化得分。仅仅通过大量的尝试，RL就可以成为一个近乎完美的玩家。从那以后，由于RL能够掌握定义良好的任务，如游戏和NLP，因此受到了很多关注。</p><p id="a3d2" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">但是，与任何机器学习算法一样，它也有其局限性:</p><ul class=""><li id="bf67" class="nc nd iq kh b ki kj kl km ko ne ks nf kw ng la nh ni nj nk bi translated"><strong class="kh ir"> RL不好概括。</strong>如果引入了新的功能或决策，it部门通常很难适应。</li><li id="7b3c" class="nc nd iq kh b ki nl kl nm ko nn ks no kw np la nh ni nj nk bi translated">RL在组合决策空间上不能很好地扩展。因此，如果我们有很多可能的决定，比如在网飞的主屏幕上推荐很多电影，RL会努力处理大量可能的配置。</li><li id="b76e" class="nc nd iq kh b ki nl kl nm ko nn ks no kw np la nh ni nj nk bi translated"><strong class="kh ir"> RL不处理低信噪比数据。</strong> RL是一个非常强大的模型，可以从数据中学习错综复杂的规则和关系——如果有噪声特征，RL将拟合噪声。</li><li id="7497" class="nc nd iq kh b ki nl kl nm ko nn ks no kw np la nh ni nj nk bi translated"><strong class="kh ir"> RL不处理长时间范围。</strong>与上面的要点类似，如果我们想要优化一个长期决策，有很多机会去适应噪声，所以如果给RL一个复杂的优化任务，它可能会过度适应。</li></ul><p id="4c73" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">所以，RL在明确定义的问题空间中确实很强大，但是我们如何让它推广到更复杂的问题呢？</p><h2 id="58ee" class="mp lt iq bd lu mq mr dn ly ms mt dp mc ko mu mv me ks mw mx mg kw my mz mi na bi translated">RecSim框架</h2><p id="dc4e" class="pw-post-body-paragraph kf kg iq kh b ki mk jr kk kl ml ju kn ko mm kq kr ks mn ku kv kw mo ky kz la ij bi translated">由谷歌和UT奥斯汀的研究人员开发的RecSim可能会解决我们在推荐引擎方面的问题。在这里，推荐包括向用户提供内容——想想Spotify、网飞、YouTube等。</p><figure class="ld le lf lg gt lh gh gi paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="gh gi nq"><img src="../Images/664e29a408ffb0bd518c5810c6e13d44.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3y8rYnUOJjBsMCJRB6GYDQ.png"/></div></div><p class="lo lp gj gh gi lq lr bd b be z dk translated">图3:简化的RecSim框架。图片作者——<a class="ae lb" href="https://arxiv.org/pdf/1909.04847.pdf" rel="noopener ugc nofollow" target="_blank">src</a>。</p></figure><p id="1016" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如图3所示，RecSim中有两种类型的组件:模型(蓝色)和数据(黑色)。从第一个代理-环境周期开始，我们…</p><ol class=""><li id="5dc0" class="nc nd iq kh b ki kj kl km ko ne ks nf kw ng la nr ni nj nk bi translated"><strong class="kh ir">创建用户和文档样本(内容)。这些样本来自工程师指定的分布。</strong></li><li id="f261" class="nc nd iq kh b ki nl kl nm ko nn ks no kw np la nr ni nj nk bi translated"><strong class="kh ir">获得代理的推荐。</strong><em class="ns">推荐代理</em>从前面的步骤中获取用户和文档，并且<strong class="kh ir">产生一个</strong> <strong class="kh ir">策略</strong> —一个例子是向给定用户显示一组电影。</li><li id="a0d0" class="nc nd iq kh b ki nl kl nm ko nn ks no kw np la nr ni nj nk bi translated"><strong class="kh ir">模拟用户的决策。</strong><em class="ns">用户选择模型</em>接受来自代理的推荐，并基于先前的例子，将估计用户的行为——假设45%的用户开始看电影。</li><li id="2cfd" class="nc nd iq kh b ki nl kl nm ko nn ks no kw np la nr ni nj nk bi translated"><strong class="kh ir">向代理和用户传递反馈。</strong>在确定了用户的行为之后，我们将这些信息传递给代理，这样它就可以改进它的预测。我们还将信息传递给用户数据，这样我们就可以更新长期的用户偏好，比如最喜欢的流派。</li><li id="e719" class="nc nd iq kh b ki nl kl nm ko nn ks no kw np la nr ni nj nk bi translated"><strong class="kh ir">重复步骤1-4，直到停止标准</strong>。常见的停止标准是固定的迭代次数或最低精度。</li></ol><p id="2095" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">注意，<em class="ns">推荐代理</em>和<em class="ns">用户选择模型</em>都可以是复杂的神经网络。对于文中的例子，推荐代理是一个深度神经网络，用户选择模型是一个多项式逻辑。</p><h2 id="8802" class="mp lt iq bd lu mq mr dn ly ms mt dp mc ko mu mv me ks mw mx mg kw my mz mi na bi translated">个案研究</h2><p id="6dce" class="pw-post-body-paragraph kf kg iq kh b ki mk jr kk kl ml ju kn ko mm kq kr ks mn ku kv kw mo ky kz la ij bi translated">RecSim在各种使用案例中都取得了成功，但本文中引用了三个核心示例。</p><p id="8280" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">第一个是使用RecSim为用户发现潜在的(隐藏的)状态，并优化长期点击率(CTR)。潜在状态的一个例子是电影类型偏好。</p><p id="009f" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这是一个典型的探索与利用的权衡。我们需要获得一些关于用户的数据来提供建议，但我们也希望尽快做出明智的决定来帮助我们的用户。对于有强烈内容偏好的用户，RecSim能够显示出比贪婪算法提高51%的点击率。</p><figure class="ld le lf lg gt lh gh gi paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="gh gi nt"><img src="../Images/122f4f078a7f26fb43134f9eab657d2e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZgXVarA9GSwsmerLV3h09Q.png"/></div></div><p class="lo lp gj gh gi lq lr bd b be z dk translated">图4:单一文档与候选文档推荐。图片作者。</p></figure><p id="c8ec" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">研究人员解决的第二个问题是处理候选推荐，即组合决策空间(图4)。</p><p id="8faf" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">虽然没有给出准确性分数，但通过使用一些基本假设，作者能够证明RecSim框架的有效性。他们还注意到,“最优”算法的计算量非常大，并且大部分增益可以通过使用时间差异学习来获得，而不是完全成熟的RL算法。</p><p id="f4c0" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">因此，虽然我们没有解决组合决策空间的问题，但RecSim提供了一些比普通RL计算效率更高的解决方案。</strong></p><p id="e00e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">最后，在第三个案例研究中，研究人员试图解决长时间跨度的问题。为了解决这个问题，作者反复向一组用户给出相同的建议，并观察他们偏好的变化。<strong class="kh ir">通过隔离单个建议的影响，该模型甚至能够在低信噪比的环境中估计功能的真实影响。</strong></p><h1 id="2223" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">摘要</h1><p id="1830" class="pw-post-body-paragraph kf kg iq kh b ki mk jr kk kl ml ju kn ko mm kq kr ks mn ku kv kw mo ky kz la ij bi translated">现在你知道了！</p><p id="c6d2" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">总之，RecSim创建了一个模拟框架来优化推荐代理。它允许组合决策空间和长时间的视野。</p><p id="71ae" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如果您从事为用户生成推荐的业务，并且有足够的数据和计算资源，RecSim可能是RL最健壮的推荐框架。</p><h1 id="22e0" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">实施说明</h1><ul class=""><li id="5e57" class="nc nd iq kh b ki mk kl ml ko nu ks nv kw nw la nh ni nj nk bi translated">RecSim框架被封装在OpenAI gym中，以利用当前的RL库。</li><li id="31a1" class="nc nd iq kh b ki nl kl nm ko nn ks no kw np la nh ni nj nk bi translated">该方法是有效的，但仍可从未来的开发中受益。</li><li id="c2ec" class="nc nd iq kh b ki nl kl nm ko nn ks no kw np la nh ni nj nk bi translated">RecSim附带了几个默认的学习环境，但是因为这些环境可能是特定于领域的，所以鼓励开发人员构建自己的环境。</li><li id="0a62" class="nc nd iq kh b ki nl kl nm ko nn ks no kw np la nh ni nj nk bi translated">RecSim允许可配置的用户状态，比如主题关联性。</li><li id="f15d" class="nc nd iq kh b ki nl kl nm ko nn ks no kw np la nh ni nj nk bi translated">虽然这不是RecSim的直接贡献，但该库支持动态用户特性，这是RL的一个相对较新的特性</li></ul></div><div class="ab cl nx ny hu nz" role="separator"><span class="oa bw bk ob oc od"/><span class="oa bw bk ob oc od"/><span class="oa bw bk ob oc"/></div><div class="ij ik il im in"><p id="6977" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><em class="ns">感谢阅读！我会再写38篇文章，把学术研究带到DS行业。查看我的评论，链接到这篇文章的主要来源以及一些有用的资源。</em></p></div></div>    
</body>
</html>