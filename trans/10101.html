<html>
<head>
<title>Complete Guide to Spark and PySpark Setup for Data Science</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">数据科学Spark和PySpark设置完整指南</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/complete-guide-to-spark-and-pyspark-setup-for-data-science-374ecd8d1eea?source=collection_archive---------9-----------------------#2021-09-24">https://towardsdatascience.com/complete-guide-to-spark-and-pyspark-setup-for-data-science-374ecd8d1eea?source=collection_archive---------9-----------------------#2021-09-24</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="3975" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">完成关于如何为数据科学设置<strong class="ak"> Spark的A-Z，包括通过<strong class="ak"> PySpark </strong>将<strong class="ak"> Spark </strong>与<strong class="ak"> Scala </strong>和<strong class="ak"> Python </strong>一起使用，以及与<strong class="ak"> Jupyter笔记本</strong> s集成</strong></h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/1d672a3bb9475ba64827e65bae97f866.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*mS3HUYY4t-oLFAX_"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">照片由<a class="ae kv" href="https://www.pexels.com/@rakicevic-nenad-233369?utm_content=attributionCopyText&amp;utm_medium=referral&amp;utm_source=pexels" rel="noopener ugc nofollow" target="_blank"> Rakicevic Nenad </a>从<a class="ae kv" href="https://www.pexels.com/photo/man-with-fireworks-769525/?utm_content=attributionCopyText&amp;utm_medium=referral&amp;utm_source=pexels" rel="noopener ugc nofollow" target="_blank"> Pexels </a>拍摄</p></figure><h2 id="378c" class="kw kx iq bd ky kz la dn lb lc ld dp le lf lg lh li lj lk ll lm ln lo lp lq lr bi translated">介绍</h2><p id="22a5" class="pw-post-body-paragraph ls lt iq lu b lv lw jr lx ly lz ju ma lf mb mc md lj me mf mg ln mh mi mj mk ij bi translated">Apache <strong class="lu ir"> Spark </strong>是一个用于大数据处理的统一分析引擎，内置了用于流、SQL、机器学习和图形处理的模块。它正迅速成为数据科学家调查大数据的事实上的工具。</p><p id="bd2d" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated">与大多数数据科学家类似，Python一直是我的首选编程语言，从通过Scrapy和Selenium等网络抓取工具收集数据，到与熊猫的数据争论，以及使用Pytorch和Tensorflow等Python中所有优秀的库进行机器学习/深度学习。</p><p id="56dc" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated">然而，数据每天都在迅速变得越来越大，预计这种增长在未来几年将会加速，特别是随着社交媒体和互联网在全球范围内的使用不断扩大。</p><p id="0a9a" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated">当处理大文件并试图对这些文件进行操作时，Python开始遇到困难。我知道有一些可行的解决方案，比如Dask，它是Python的并行计算库，但是在使用Spark之后，我觉得它是最好的解决方案，原因如下:</p><ol class=""><li id="c0d9" class="mq mr iq lu b lv ml ly mm lf ms lj mt ln mu mk mv mw mx my bi translated"><strong class="lu ir">速度</strong> — Spark具有Catalyst Optimizer等特性，它使用过滤和索引等技术来确保SQL类型查询等任务以最高效的顺序执行。</li><li id="0574" class="mq mr iq lu b lv mz ly na lf nb lj nc ln nd mk mv mw mx my bi translated"><strong class="lu ir">社区支持</strong> — Spark在全球拥有庞大的追随者，并且发展迅速，不断增加更多的工具和功能。</li><li id="9281" class="mq mr iq lu b lv mz ly na lf nb lj nc ln nd mk mv mw mx my bi translated"><strong class="lu ir"> MLflow </strong> — Spark与MLflow集成，MLflow是一款出色的开源MLOps工具，用于管理从初始调查和模型参数记录到在生产中存储和部署模型的机器学习生命周期。</li><li id="8a42" class="mq mr iq lu b lv mz ly na lf nb lj nc ln nd mk mv mw mx my bi translated">我真的很喜欢Databricks，它是一个在云上使用Python和Spark的平台。他们的笔记本电脑环境非常适合团队协作，从MLOps的角度来看，他们的托管MLflow添加确实有助于更快、更高效地将数据科学模型投入生产。</li></ol><p id="5bd0" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated">我希望您现在急于加入Spark，将这个优秀的大数据工具添加到您的武器库中。下面我列出了先决条件，然后一步一步地介绍如何安装Spark和PySpark。</p><h1 id="4b57" class="ne kx iq bd ky nf ng nh lb ni nj nk le jw nl jx li jz nm ka lm kc nn kd lq no bi translated">Spark的最佳语言:Scala Vs Python</h1><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi np"><img src="../Images/be7e0011e2c20a2ff41963159468bbb9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*08v-4ac7hbcUHS5O"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">照片由<a class="ae kv" href="https://www.pexels.com/photo/competition-dispute-goats-37323/?utm_content=attributionCopyText&amp;utm_medium=referral&amp;utm_source=pexels" rel="noopener ugc nofollow" target="_blank">像素</a>的<a class="ae kv" href="https://www.pexels.com/@pixabay?utm_content=attributionCopyText&amp;utm_medium=referral&amp;utm_source=pexels" rel="noopener ugc nofollow" target="_blank">像素</a>拍摄</p></figure><p id="96e7" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated">Spark使用Scala作为默认编程语言。然而，使用PySpark，我们也可以通过Python使用Spark。使用Spark和Scala的主要好处是性能效率，特别是在流方面，Spark/Scala比Python发达得多。然而，与你很可能已经非常熟悉的Python相比，你必须考虑学习一门新语言(比如Scala)的时间成本。</p><p id="b3bb" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated">我认为Python是一种很好的基础语言，可以通过包和库(如PySpark)来添加功能，我个人采用的方法是将Python作为我的主要语言，并通过PySpark(在这种情况下)或PyJulia(在Julia语言的情况下)导入额外的功能。我知道这导致了一些效率的损失，但我觉得我可以在python脚本中包含我的整个数据科学项目这一事实在尝试在复杂的管道中生产我的模型时有很大的帮助，在复杂的管道中增加多种语言之间的额外复杂性可能是一件令人头痛的事情。</p><p id="39c4" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated">不过，我在下面展示了两种方法，从Spark/Scala安装开始，到PySpark结束，这样您就可以选择在涉及Spark的数据科学项目中更喜欢哪种方法。</p><h1 id="a42b" class="ne kx iq bd ky nf ng nh lb ni nj nk le jw nl jx li jz nm ka lm kc nn kd lq no bi translated">两种方法的先决条件</h1><p id="3ff9" class="pw-post-body-paragraph ls lt iq lu b lv lw jr lx ly lz ju ma lf mb mc md lj me mf mg ln mh mi mj mk ij bi translated">我假设您使用的是Linux发行版，无论是在专用的Linux机器上还是通过WSL或VirtualBox，只要您使用的是Linux环境，这都无关紧要。此外，您应该已经安装了Jupyter，并在您的本地机器上进行编码等工作。</p><h1 id="fbb2" class="ne kx iq bd ky nf ng nh lb ni nj nk le jw nl jx li jz nm ka lm kc nn kd lq no bi translated">Scala设置的火花</h1><h2 id="b06f" class="kw kx iq bd ky kz la dn lb lc ld dp le lf lg lh li lj lk ll lm ln lo lp lq lr bi translated">关于兼容版本的重要说明</h2><p id="1cdc" class="pw-post-body-paragraph ls lt iq lu b lv lw jr lx ly lz ju ma lf mb mc md lj me mf mg ln mh mi mj mk ij bi translated">scala、spark、hadoop和sbt的版本兼容至关重要。每一个的最新版本不一定能一起工作。我目前的设置使用下面的版本，它们都可以很好地配合使用。</p><p id="0da4" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated"><code class="fe nq nr ns nt b">spark=2.4.4</code><br/><code class="fe nq nr ns nt b">scala=2.13.1</code><br/><code class="fe nq nr ns nt b">hadoop=2.7</code><br/><code class="fe nq nr ns nt b">sbt=1.3.5</code><br/>T4】</p><h1 id="ab5a" class="ne kx iq bd ky nf ng nh lb ni nj nk le jw nl jx li jz nm ka lm kc nn kd lq no bi translated">步骤1:安装Java</h1><p id="3963" class="pw-post-body-paragraph ls lt iq lu b lv lw jr lx ly lz ju ma lf mb mc md lj me mf mg ln mh mi mj mk ij bi translated">如果你在你的终端中输入<code class="fe nq nr ns nt b">which java</code>,这会告诉你你的Java安装存储在哪里，如果你已经安装了的话。如果你没有安装它，它不会返回任何东西。</p><blockquote class="nu nv nw"><p id="e866" class="ls lt nx lu b lv ml jr lx ly mm ju ma ny mn mc md nz mo mf mg oa mp mi mj mk ij bi translated">如果你想检查你正在使用的某个东西的版本，比如Python等，这个"<strong class="lu ir"> which" </strong>命令通常是很好的选择。</p></blockquote><p id="024a" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated">值得注意的是，Java是一个有些独特的安装，如果您键入<code class="fe nq nr ns nt b">which java</code>并且它已经安装好了，您应该会得到如下所示的内容:</p><p id="245f" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated"><code class="fe nq nr ns nt b">/usr/bin/java</code></p><p id="c1bc" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated">然而，这是一个指向<code class="fe nq nr ns nt b">/etc/alternatives/java</code>的符号链接，如下所示。</p><blockquote class="nu nv nw"><p id="57e4" class="ls lt nx lu b lv ml jr lx ly mm ju ma ny mn mc md nz mo mf mg oa mp mi mj mk ij bi translated">符号链接总是有一个箭头指向它们所象征的位置，如下图所示。</p></blockquote><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ob"><img src="../Images/502434b127b5f061a6a07d4b81873d3b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QgWUBjyuqbXsLzdZz1NTUw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="85a6" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated">这个<code class="fe nq nr ns nt b">/etc/alternatives/java</code>是另一个符号链接，它反过来指向<code class="fe nq nr ns nt b">/usr/lib/jvm/java-8-openjdk-amd64/jre/bin/java</code>，这是用于运行Java的实际文件。这种结构对于Java的运行方式是必要的，但是当试图强制将某个版本作为默认版本时要小心。</p><p id="6bfe" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated">我们需要Java版本8来正确地与Spark一起工作，因此如果你已经有了错误的版本，比如版本11，那么卸载并重新安装，在你的终端中运行下面的代码行。<br/> <code class="fe nq nr ns nt b">sudo apt-get remove openjdk*</code> <br/> <code class="fe nq nr ns nt b">sudo apt-get install openjdk-8-jdk</code> <br/> <code class="fe nq nr ns nt b">sudo apt-get install openjdk-8-jre</code></p><p id="593d" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated">接下来安装java只需运行下面一行:<br/> <code class="fe nq nr ns nt b">sudo apt-get install openjdk-8-jdk</code></p><p id="e108" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated">您现在可以通过运行<br/> <code class="fe nq nr ns nt b">update-java-alternatives --list</code>来检查您的Linux系统上当前所有的Java版本</p><p id="052d" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated">要将您想要的java版本设置为默认版本，在本例中是版本8，然后运行下面需要root权限的命令:<br/> <code class="fe nq nr ns nt b">sudo update-java-alternatives --set /path/to/java/version8</code></p><p id="307c" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated">最后加上下面一行。bashrc/。zshrc视情况而定。<br/> <code class="fe nq nr ns nt b">export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64</code></p><p id="7f9a" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated">现在，您应该已经在系统上成功安装了Java 8，我们现在可以开始安装Spark了。</p><h1 id="ec58" class="ne kx iq bd ky nf ng nh lb ni nj nk le jw nl jx li jz nm ka lm kc nn kd lq no bi translated">火花</h1><p id="31a8" class="pw-post-body-paragraph ls lt iq lu b lv lw jr lx ly lz ju ma lf mb mc md lj me mf mg ln mh mi mj mk ij bi translated">从下面的链接下载Spark安装文件。<a class="ae kv" href="https://spark.apache.org/downloads.html" rel="noopener ugc nofollow" target="_blank">https://spark.apache.org/downloads.html</a></p><p id="dc1c" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated">然后通过运行<br/>提取:<code class="fe nq nr ns nt b">tar xvf spark-1.3.1-bin-hadoop2.6.tgz</code></p><p id="000f" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated">接下来将这个文件移动到<code class="fe nq nr ns nt b">/usr/local/spark</code> <br/> <code class="fe nq nr ns nt b">mv spark-1.3.1-bin-hadoop2.6 /usr/local/spark</code></p><p id="7350" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated">最后加上下面几行。巴沙尔或者。zshrc取决于您运行的是什么shell。这只是意味着当你启动未来的终端时，它会自动将下面的火花路径添加到路径变量中，以便可以使用。<br/> <code class="fe nq nr ns nt b">export PATH=$PATH:/usr/local/spark/bin</code> <br/> <code class="fe nq nr ns nt b">export SPARK_HOME=/usr/local/spark</code></p><p id="28fd" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated">请记住，如果不使用下面的命令，当前的终端将不会更新。因此，您可以运行这个命令，当前的shell将更新或打开一个新的终端，它将自动使用新的设置。</p><p id="10ba" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated"><code class="fe nq nr ns nt b">source ~/.zshrc</code></p><h1 id="b0e8" class="ne kx iq bd ky nf ng nh lb ni nj nk le jw nl jx li jz nm ka lm kc nn kd lq no bi translated">Hadoop</h1><p id="7437" class="pw-post-body-paragraph ls lt iq lu b lv lw jr lx ly lz ju ma lf mb mc md lj me mf mg ln mh mi mj mk ij bi translated">从下面的链接下载二进制文件:<a class="ae kv" href="https://spark.apache.org/downloads.html" rel="noopener ugc nofollow" target="_blank">https://hadoop.apache.org/releases.html</a><br/><br/>解压文件</p><p id="b354" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated"><code class="fe nq nr ns nt b">tar xzvf hadoop-3.2.1.tar.gz</code></p><p id="8bf3" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated">将提取的文件移动到您想要存储Hadoop安装的位置。我建议创建一个名为hadoop的通用文件夹，并将下载的文件移动到其中，而不是名为hadoop_version_xx的文件夹，因为这意味着一旦我们设置了PATH等，以后如果我们更改为新版本的hadoop，通用Hadoop文件夹的路径将保持不变。(这通常是安装这样的库的好方法，你可以在将来更新版本号，但不需要更新路径变量等)<br/> <code class="fe nq nr ns nt b">mv hadoop-3.2.1/* ~/FILE_PATH_EXAMPLE/spark_scala/software/hadoop/</code></p><p id="57ec" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated">在bash文件中添加Hadoop和Java路径。bashrc/。zshrc以类似于我们在路径中添加Spark的方式，在。bashrc/。zshrc文件。</p><p id="a005" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated"><code class="fe nq nr ns nt b">export HADOOP_HOME=~/FILE_PATH_TO_HADOOP/hadoop</code><br/><code class="fe nq nr ns nt b">export HADOOP_CLASSPATH=$HADOOP_CLASSPATH:~/FILE_PATH_TO_HADOOP/hadoop/share/hadoop/common/lib</code><br/><code class="fe nq nr ns nt b">export HADOOP_CONF_DIR=~/FILE_PATH_TO_HADOOP/hadoop/etc/hadoop</code><br/><code class="fe nq nr ns nt b">export HADOOP_MAPRED_HOME=~/FILE_PATH_TO_HADOOP/hadoop</code><br/><code class="fe nq nr ns nt b">export HADOOP_COMMON_HOME=~/FILE_PATH_TO_HADOOP/hadoop</code><br/><code class="fe nq nr ns nt b">export HADOOP_HDFS_HOME=~/FILE_PATH_TO_HADOOP/hadoop</code><br/><code class="fe nq nr ns nt b">export YARN_HOME=~/FILE_PATH_TO_HADOOP/hadoop</code><br/><code class="fe nq nr ns nt b">export PATH=$PATH:~/FILE_PATH_TO_HADOOP/hadoop/bin</code></p><h1 id="99eb" class="ne kx iq bd ky nf ng nh lb ni nj nk le jw nl jx li jz nm ka lm kc nn kd lq no bi translated">斯卡拉</h1><p id="ae0e" class="pw-post-body-paragraph ls lt iq lu b lv lw jr lx ly lz ju ma lf mb mc md lj me mf mg ln mh mi mj mk ij bi translated">要安装scala，只需使用<strong class="lu ir"> wget </strong>获取它，然后使用<strong class="lu ir"> dpkg </strong>包管理器解包，如下所示。</p><p id="bcd9" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated"><code class="fe nq nr ns nt b">sudo wget www.scala-lang.org/files/archive/scala-2.13.1.deb</code> <br/> <code class="fe nq nr ns nt b">sudo dpkg -i scala-2.13.1.deb</code></p><blockquote class="nu nv nw"><p id="def1" class="ls lt nx lu b lv ml jr lx ly mm ju ma ny mn mc md nz mo mf mg oa mp mi mj mk ij bi translated">注意，当我尝试使用apt包管理器时，我在使用scala时看不到终端中的命令，所以我使用dpkg包管理器，如下所示</p></blockquote><p id="0278" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated">如果你已经通过apt安装了，并且在终端中启动scala后出现了同样的命令不可见的错误，那么为了解决这个问题，请完全删除scala，并使用dpkg安装它(不要使用apt):<br/><code class="fe nq nr ns nt b">sudo apt-get remove scala-library scala</code><br/><code class="fe nq nr ns nt b">sudo wget www.scala-lang.org/files/archive/scala-2.13.1.deb</code><br/><code class="fe nq nr ns nt b">sudo dpkg -i scala-2.13.1.deb</code></p><h1 id="1096" class="ne kx iq bd ky nf ng nh lb ni nj nk le jw nl jx li jz nm ka lm kc nn kd lq no bi translated">SBT</h1><p id="76ff" class="pw-post-body-paragraph ls lt iq lu b lv lw jr lx ly lz ju ma lf mb mc md lj me mf mg ln mh mi mj mk ij bi translated">只需运行下面的命令来安装简单的构建工具【SBT】<br/><code class="fe nq nr ns nt b">sudo apt-get update</code><br/><code class="fe nq nr ns nt b">sudo apt-get install sbt</code></p><p id="b6f9" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated">如果使用上述命令从回购中获取SBT有困难，那么只需从网站<a class="ae kv" href="https://www.scala-sbt.org/download.html" rel="noopener ugc nofollow" target="_blank">https://www.scala-sbt.org/download.html</a>手动下载，并按如下方式提取:</p><p id="b702" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated"><code class="fe nq nr ns nt b">tar xvf sbt-1.3.5.tgz</code></p><p id="bb75" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated"><code class="fe nq nr ns nt b">mv sbt /usr/share/sbt</code></p><p id="97dc" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated">最后添加下面一行。bashrc/。zshrc视情况:<br/> <code class="fe nq nr ns nt b">Export PATH=$PATH:/usr/local/sbt/bin</code></p><h1 id="e7e6" class="ne kx iq bd ky nf ng nh lb ni nj nk le jw nl jx li jz nm ka lm kc nn kd lq no bi translated">检查安装</h1><p id="8f48" class="pw-post-body-paragraph ls lt iq lu b lv lw jr lx ly lz ju ma lf mb mc md lj me mf mg ln mh mi mj mk ij bi translated">首先确保。bashrc/。zshrc通过运行下面的命令来使用我们最新的更新。(或者，您可以简单地退出当前终端，打开一个新的终端，它将自动使用最新的终端。bashrc/。zshrc文件。)</p><p id="8227" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated"><code class="fe nq nr ns nt b">Command source ~/.bashrc</code></p><h1 id="add9" class="ne kx iq bd ky nf ng nh lb ni nj nk le jw nl jx li jz nm ka lm kc nn kd lq no bi translated">检查Java的安装</h1><p id="1e2b" class="pw-post-body-paragraph ls lt iq lu b lv lw jr lx ly lz ju ma lf mb mc md lj me mf mg ln mh mi mj mk ij bi translated">您可以通过运行<code class="fe nq nr ns nt b">java -version</code>进行检查，这应该会返回以下结果:<br/> <code class="fe nq nr ns nt b">openjdk version "1.8.0_212"</code></p><h1 id="6108" class="ne kx iq bd ky nf ng nh lb ni nj nk le jw nl jx li jz nm ka lm kc nn kd lq no bi translated">检查Hadoop的安装</h1><p id="d95b" class="pw-post-body-paragraph ls lt iq lu b lv lw jr lx ly lz ju ma lf mb mc md lj me mf mg ln mh mi mj mk ij bi translated">你可以通过运行<code class="fe nq nr ns nt b">hadoop version</code>来检查(注意这次<code class="fe nq nr ns nt b">-</code>版本前没有)。这将返回您正在使用的hadoop版本，如下所示:<br/> <code class="fe nq nr ns nt b">hadoop 2.7.3</code></p><h1 id="0695" class="ne kx iq bd ky nf ng nh lb ni nj nk le jw nl jx li jz nm ka lm kc nn kd lq no bi translated">检查火花的安装</h1><ol class=""><li id="0f20" class="mq mr iq lu b lv lw ly lz lf oc lj od ln oe mk mv mw mx my bi translated">cd到apache-spark的安装目录，然后使用<code class="fe nq nr ns nt b">ls</code>命令列出所有文件/目录。</li><li id="2b00" class="mq mr iq lu b lv mz ly na lf nb lj nc ln nd mk mv mw mx my bi translated">寻找一个我们可以使用的文本文件，比如README.md或CHANGES.txt</li><li id="e4d1" class="mq mr iq lu b lv mz ly na lf nb lj nc ln nd mk mv mw mx my bi translated">在终端中输入spark-shell，spark应该会在scala提示符下启动，如下所示:</li></ol><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi of"><img src="../Images/b8d19894a6de003d7624389b12f2387c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9u96OxJx46ErqQbqHaxWHA.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="e2a4" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated">作为测试命令，输入下面一行，用您选择的任何文本文件替换README.md。</p><p id="9b28" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated"><code class="fe nq nr ns nt b">val rdd = sc.textFile("README.md")</code></p><p id="e209" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated">接下来输入<code class="fe nq nr ns nt b">rdd.count()</code>，它将返回文件中的行数，如下所示。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi og"><img src="../Images/c65e1ce77a0ce29fe593eb4ddeb107e2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bD60fZkMLRWvNTTb38r-Eg.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="0442" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated">恭喜你现在可以在Scala中使用Spark-Shell了！我们现在将继续安装PySpark，这将使我们能够在Python环境中使用Spark，如Jupyter笔记本上的Python内核等。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oh"><img src="../Images/d4b105b78ddb33a8bf45d9e4e3dabacc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*i8cnrFGtEt23uIbu"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">照片由<a class="ae kv" href="https://www.pexels.com/@ninauhlikova?utm_content=attributionCopyText&amp;utm_medium=referral&amp;utm_source=pexels" rel="noopener ugc nofollow" target="_blank"> Nina Uhlíková </a>从<a class="ae kv" href="https://www.pexels.com/photo/person-standing-on-hand-rails-with-arms-wide-open-facing-the-mountains-and-clouds-725255/?utm_content=attributionCopyText&amp;utm_medium=referral&amp;utm_source=pexels" rel="noopener ugc nofollow" target="_blank"> Pexels </a>拍摄</p></figure><h1 id="3025" class="ne kx iq bd ky nf ng nh lb ni nj nk le jw nl jx li jz nm ka lm kc nn kd lq no bi translated">Python设置的spark(PySpark)</h1><blockquote class="nu nv nw"><p id="c7e5" class="ls lt nx lu b lv ml jr lx ly mm ju ma ny mn mc md nz mo mf mg oa mp mi mj mk ij bi translated">注意PySpark目前与Python 3.8不兼容，因此为了确保它能够正常工作，我们安装了Python 3.7，并使用该版本的Python创建了一个虚拟环境，我们将在其中运行PySpark。</p></blockquote><p id="29b6" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated">要在Linux系统上安装Python 3.7作为Python的附加版本，只需运行:</p><p id="353e" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated"><code class="fe nq nr ns nt b">sudo apt update</code> <br/> <code class="fe nq nr ns nt b">sudo apt install software-properties-common</code> <br/> <code class="fe nq nr ns nt b">sudo add-apt-repository ppa:deadsnakes/ppa</code> <br/></p><blockquote class="nu nv nw"><p id="4152" class="ls lt nx lu b lv ml jr lx ly mm ju ma ny mn mc md nz mo mf mg oa mp mi mj mk ij bi translated">请注意，要在系统上将软件包安装到这个特定版本的python中，请确保将python版本指定为pip命令的一部分，例如<code class="fe nq nr ns nt b">/usr/bin/python3.7 -m pip install pandas</code></p></blockquote><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oi"><img src="../Images/3288778a7c564fa9f5c6e2750f703706.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9NIJVZmk-wsfzIzCa-gWQA.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="e51a" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated">接下来创建一个名为pyspark_env的新虚拟环境，并让它使用这个新安装的Python3.7版本:</p><p id="01e0" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated"><code class="fe nq nr ns nt b">mkvirtualenv -p /usr/bin/python3.7 pyspark_env</code></p><blockquote class="nu nv nw"><p id="371f" class="ls lt nx lu b lv ml jr lx ly mm ju ma ny mn mc md nz mo mf mg oa mp mi mj mk ij bi translated"><em class="iq">(请注意，我使用</em><strong class="lu ir"><em class="iq">virtualenvwrapper</em></strong><em class="iq">来创建我的python虚拟环境，并强烈推荐它作为保持您的虚拟环境良好维护的好方法。更多详情请看此链接</em><a class="ae kv" href="https://virtualenvwrapper.readthedocs.io/en/latest/" rel="noopener ugc nofollow" target="_blank"><em class="iq">https://virtualenvwrapper.readthedocs.io/en/latest/</em></a><em class="iq">)</em></p></blockquote><p id="2f76" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated">在创建新的虚拟环境时，它应该会自动激活它，你应该会在你的提示符下看到它的名字，如下所示。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oj"><img src="../Images/9901230c5a7e09c36674dd9a238b4b1d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2kTDLO513xSKpOiWZzALWQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="6045" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated">如果它没有自动激活，那么只需使用下面的命令来激活环境(这是另一个方便的命令，是我前面提到的virtualenv包装库的一部分)。</p><p id="b45f" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated"><code class="fe nq nr ns nt b">workon pyspark_env</code></p><p id="d783" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated">现在，在新的虚拟环境中，我们可以通过以下命令简单地安装PySpark:</p><p id="090a" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated"><code class="fe nq nr ns nt b">pip install pyspark</code></p><p id="81d6" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated">最后添加下面一行。bashrc/。zshrc视情况而定:</p><p id="53fd" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated"><code class="fe nq nr ns nt b">export PYTHONPATH=$SPARK_HOME/python:$PYTHONPATH</code><br/>T1】</p><p id="1146" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated">现在，如果你简单地在你的终端中输入pyspark，你应该启动spark，就像我们之前在Scala中做的那样，只是这次你不会得到Scala的提示，而是可以像平常一样使用Python，并且拥有PySpark功能。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ok"><img src="../Images/eee66b18e834460c6dc23f3c2a81fff8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LQn3UFVnlRY3Iq4xCzWTAA.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="566a" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated">例如，如果您已经将pandas安装到我们之前安装的Python3.7中，您可以导入pandas并创建数据帧，如下所示:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ol"><img src="../Images/e25352eae00064b1cdafc1559026889d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Sd0wyS6UcJJ_hMLDTlNCEg.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><h1 id="de6e" class="ne kx iq bd ky nf ng nh lb ni nj nk le jw nl jx li jz nm ka lm kc nn kd lq no bi translated">PySpark和Jupyter</h1><p id="ac2f" class="pw-post-body-paragraph ls lt iq lu b lv lw jr lx ly lz ju ma lf mb mc md lj me mf mg ln mh mi mj mk ij bi translated">最后一步是在Jupyter中启用PySpark。为此，首先要确保pyspark_env环境处于活动状态。然后安装ipykernel，如果你还没有像下面这样安装它。(需要注意的是，我们不需要指定pip3等，因为我们在活动环境pyspark_env中，它会自动将包安装到该环境使用的python版本中。</p><p id="ef40" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated"><code class="fe nq nr ns nt b">ipip install ipykernel</code></p><p id="58ca" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated">接下来，通过以下方式将虚拟环境添加到Jupyter:</p><p id="b4e8" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated"><code class="fe nq nr ns nt b">ipython -m ipykernel install --user --name=pyspark_env</code></p><p id="9154" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated">如果您现在启动Jupyter，您应该会看到pyspark_env列在右边的内核中，如下所示:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi om"><img src="../Images/a3aba4ab59bfe4bd190fe29a4ec09645.png" data-original-src="https://miro.medium.com/v2/resize:fit:684/format:webp/1*Z93Nl0TPux99c4iEii-gsQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="94f4" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated">在新笔记本中，作为一个简单的测试，尝试制作一个PySpark数据框架。首先从pyspark.sql导入SparkSession，如下所示:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi on"><img src="../Images/23e5fe62e902f92a1d6e80fecfdf1de3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*O-2rIDm5NcpYcI5rYb4rwA.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="f55a" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated">接下来构建一个标准的Spark会话，如下所示:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oo"><img src="../Images/3df671e718c6afa8ee8c552567e6b1ad.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8Q3c6MyS5SRFw_SJQtfHig.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="0679" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated">最后，创建一个简单的PySpark数据框架，以确保其正常工作:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi op"><img src="../Images/0c771a426979f5b6d0ed16408866de93.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DbubIBpBsNXy_4NJ55oPxw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="2f82" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated">您也可以在这些单元格中编写标准的python代码，下面的示例将Python F字符串添加到我们创建PySpark数据帧的同一个单元格中。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oq"><img src="../Images/0b144b5581a9354402c03da3fcc2abdd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9e-74tveVPJfunBMj40Yrw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="b032" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated">成功！！您现在可以在同一个Jupyter笔记本中编写python和PySpark代码了！</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi or"><img src="../Images/eb190f86fc73f57d279639c9266bf38f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*Q--g6W0EyoCgfU52"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">来自<a class="ae kv" href="https://www.pexels.com/photo/woman-raising-her-hands-up-while-sitting-on-floor-with-macbook-pro-on-lap-3813341/?utm_content=attributionCopyText&amp;utm_medium=referral&amp;utm_source=pexels" rel="noopener ugc nofollow" target="_blank"> Pexels </a>的<a class="ae kv" href="https://www.pexels.com/@olly?utm_content=attributionCopyText&amp;utm_medium=referral&amp;utm_source=pexels" rel="noopener ugc nofollow" target="_blank"> Andrea Piacquadio </a>的照片</p></figure><h1 id="931c" class="ne kx iq bd ky nf ng nh lb ni nj nk le jw nl jx li jz nm ka lm kc nn kd lq no bi translated">摘要</h1><p id="4268" class="pw-post-body-paragraph ls lt iq lu b lv lw jr lx ly lz ju ma lf mb mc md lj me mf mg ln mh mi mj mk ij bi translated">总之，我已经给出了如何安装<strong class="lu ir"> Spark </strong>和<strong class="lu ir"> Scala </strong>的详细说明，使你能够在你的终端上使用spark-shell。我还演示了如何使用定制的python3.7虚拟环境安装<strong class="lu ir"> PySpark </strong>，以确保没有兼容性问题，这使您能够使用PySpark命令在您的终端中打开一个<strong class="lu ir"> PySpark会话。最后，我展示了如何将<strong class="lu ir"> PySpark功能添加到您的jupyter笔记本</strong>中，您现在可以在相同的单元中编写python和PySpark代码，并根据需要使用这两种语言。</strong></p><p id="5e61" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated">我希望您喜欢阅读我的指南，并且您已经准备好在未来的数据科学项目中探索<strong class="lu ir"> Spark </strong>和<strong class="lu ir"> PySpark </strong>。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi or"><img src="../Images/07aa8c879e91fc7128447b4070e70c81.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*-zsIzhk4Ru5U_hx3"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">照片由<a class="ae kv" href="https://www.pexels.com/@pixabay?utm_content=attributionCopyText&amp;utm_medium=referral&amp;utm_source=pexels" rel="noopener ugc nofollow" target="_blank"> Pixabay </a>从<a class="ae kv" href="https://www.pexels.com/photo/dark-light-long-exposure-men-48801/?utm_content=attributionCopyText&amp;utm_medium=referral&amp;utm_source=pexels" rel="noopener ugc nofollow" target="_blank">像素</a>拍摄</p></figure></div></div>    
</body>
</html>