<html>
<head>
<title>Building a Mask R-CNN from scratch in TensorFlow and Keras</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">在TensorFlow和Keras中从头开始构建一个面具R-CNN</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/building-a-mask-r-cnn-from-scratch-in-tensorflow-and-keras-c49c72acc272?source=collection_archive---------2-----------------------#2021-03-30">https://towardsdatascience.com/building-a-mask-r-cnn-from-scratch-in-tensorflow-and-keras-c49c72acc272?source=collection_archive---------2-----------------------#2021-03-30</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="0ac8" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">讲解如何建立一个基本的面具R-CNN，以学习为目的，没有喧嚣。</h2></div><p id="07b9" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi lb translated"><span class="l lc ld le bm lf lg lh li lj di">如果</span>你曾经想在TensorFlow中从头实现一个Mask R-CNN，你很可能找到了<a class="ae lk" href="https://github.com/matterport/Mask_RCNN/tree/master/mrcnn" rel="noopener ugc nofollow" target="_blank"> Matterport的实现</a>。这是一个伟大的，如果你只想<em class="ll">使用</em>一个面具R-CNN。然而，由于它非常健壮和复杂，很难彻底理解它的每一点。更大的问题是，它不能运行新版本的TensorFlow。</p><p id="6b6d" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如果你有机会使用PyTorch实现，最常用的是<a class="ae lk" href="https://github.com/facebookresearch/detectron2" rel="noopener ugc nofollow" target="_blank"> Detectron2 </a>，它也很难理解，因为它很复杂。</p><p id="1499" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">从所有关于Mask R-CNN如何工作的描述来看，实现起来似乎总是非常容易，但不知为何你还是找不到很多实现。</p><figure class="lm ln lo lp gt lq"><div class="bz fp l di"><div class="lr ls l"/></div></figure><p id="59a0" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">本文的目的是理解R-CNN掩码的基础，以及如何实现它。(假人用口罩R-CNN？)因此，我的实现缺少了原始实现的一些重要部分，因为这主要是为了理解R-CNN是如何构建的。</p><p id="e265" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><em class="ll">*在我的实现中，基础模型不是特征金字塔网络(FPN)，ROI Align也没有实现。</em></p><p id="dea1" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">为了训练和测试，我从<a class="ae lk" href="https://wiki.cancerimagingarchive.net/display/Public/LIDC-IDRI" rel="noopener ugc nofollow" target="_blank"> LIDC-IDRI </a>公共肺部CT扫描数据集生成了一个玩具数据集，目标是分割非常(可笑？)简单的形状。为此，我放置了不同大小的心脏，并开始进行CT扫描，这些是我的面罩R-CNN必须找到、分类和分割的对象。</p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div role="button" tabindex="0" class="lu lv di lw bf lx"><div class="gh gi lt"><img src="../Images/48abcac696e40636e90a3f42f4ef854f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BHing80sa30uUzHYk_qpSQ.png"/></div></div><p class="ma mb gj gh gi mc md bd b be z dk translated">这就是R-CNN完整的面具的样子。绿色虚线方框标记了不同的神经网络。因此，整个模型由4个神经网络模型组成。图片作者。</p></figure><p id="495b" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi lb translated"><span class="l lc ld le bm lf lg lh li lj di"> 1。</span>第一步，要有骨干模型。这是一个简单的分类器模型。在我的例子中，它是一个多类标签分类器，在matterport的例子中，它是一个带有ResNet101主干的预训练FPN。当训练掩模R-CNN时，我们永远不会使用这个网络的预测，我们只需要一个内层<strong class="kh ir">特征图</strong>。在分类器网络内部生成的那些特征地图将成为我们的<strong class="kh ir">区域提议网络(RPN)的输入。</strong></p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div class="gh gi me"><img src="../Images/8082558b383a6fd060e09376b57bec28.png" data-original-src="https://miro.medium.com/v2/resize:fit:1230/format:webp/1*jBFII3x2MvqNVc4E7T0QoA.png"/></div><p class="ma mb gj gh gi mc md bd b be z dk translated">我们的分类器是从输入到标签的keras模型，但是我们也保存了从输入到featuremap的keras模型(featuremap_model)。在训练分类器之后，该特征映射模型将从输入图像生成特征映射。图片作者。</p></figure><p id="1476" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi lb translated"><span class="l lc ld le bm lf lg lh li lj di"> 2。</span>第二步是RPN。我认为这可能是网络中最复杂的部分。RPN需要找到感兴趣的区域(ROI)。为此，它预测了4个坐标，以及每个锚点的标签。什么是<strong class="kh ir">主播？</strong></p><p id="0dcb" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">锚点是预定义的不同大小的矩形，覆盖整个输入图像。我们根据我们的对象选择锚的大小/比例。<br/>我们将选择与我们的对象重叠的对象，RPN的工作是移动它们以更好地重叠，并调整它们的大小以更好地适应(预测的4个坐标)。为此，RPN还需要识别哪些锚代表对象，即哪些锚是前景锚，哪些锚是背景锚(预测的2个数字)。</p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div role="button" tabindex="0" class="lu lv di lw bf lx"><div class="gh gi mf"><img src="../Images/f67edb85f13c3a6e31ec8e658f84c76f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*P9dhL5Ytk34gmHrd6czLtg.png"/></div></div><p class="ma mb gj gh gi mc md bd b be z dk translated">选择固定数量的主播进行训练。为此，我们需要计算每个锚点和每个真实边界框之间的交集(IoU)。我们总是用相同数量的锚来训练，然而，在每个输入的情况下，前景锚的数量是不同的。(例如，如果我们在图像上有一个小对象，我们将有一个前景锚，如果我们有两个较大的对象，我们将有最大提议计数/2个前景锚。)图片由作者提供。</p></figure><p id="9751" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">因此，我们选择我们的建议计数(PC)，它告诉我们有多少RPN的预测将对损失函数有贡献。(请记住，RPN预测每个主播台！)<br/>以我的toydataset为例，我们的背景(BG)远远多于前景(FG)，所以我的PC只有20。即使我在CT扫描上有一个大明星和一颗大心脏，我只有大约10个前景锚，我们需要一个平衡的训练。(10 FG / 10 BG)如果你有更多的多事输入图像，你的PC会更大。</p><figure class="lm ln lo lp gt lq"><div class="bz fp l di"><div class="mg ls l"/></div><p class="ma mb gj gh gi mc md bd b be z dk translated">请记住，RPN模型预测每个锚点的增量值和fg/bg标签。</p></figure><p id="d108" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">对于这20个锚，我们知道真相标签和真相增量。δ是4个坐标:(δx，δy，δW，δH)。前两个显示了我们需要滑动锚的中心点多少，后两个显示了我们需要改变锚的宽度和高度多少。</p><p id="58bb" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">RPN损失由class_loss + boundingbox_loss组成。PC个预测对class_loss有贡献，FG个预测对boundingbox_loss有贡献。</p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div role="button" tabindex="0" class="lu lv di lw bf lx"><div class="gh gi lt"><img src="../Images/f187aba654b3c6a25c535cc37902d0e6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wpU7qcRSLeEjbUSsA9-D0A.png"/></div></div><p class="ma mb gj gh gi mc md bd b be z dk translated">RPN损失是class_loss和bbox_loss的总和。class_loss是一个简单的稀疏分类交叉熵，bbox_loss是一个光滑L1函数。背景锚点不会导致bbox丢失，因为我们只需要移动已经重叠的锚点。图片作者。</p></figure><figure class="lm ln lo lp gt lq"><div class="bz fp l di"><div class="mg ls l"/></div><p class="ma mb gj gh gi mc md bd b be z dk translated">在训练期间，我们简单地使用rpnloss作为classloss + deltaloss之和。如果有必要，我们可以使用损失权重。</p></figure><figure class="lm ln lo lp gt lq"><div class="bz fp l di"><div class="mg ls l"/></div></figure><p id="11ca" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在，我们选择我们预测的锚作为前景，并将其与相应的预测增量一起移动。所以我们会有NumberOfForegrounds * 4个坐标，这些是我们的ROI。我们从特征图中剪切这些区域，并将它们调整到相同的大小:这些是<strong class="kh ir">提议。</strong>这些将是我们的类、boxrefinement头和maskhead的输入。(这些神经网络模型需要有一个固定的输入形状，为此，我们总是可以用零填充我们的预测。)</p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div class="gh gi mh"><img src="../Images/20bf35fa5f4b97226895d9cca2e0968f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1204/format:webp/1*XuTZmD4r-OlzAI7RCR-pjQ.png"/></div><p class="ma mb gj gh gi mc md bd b be z dk translated">图片作者。</p></figure><p id="03c1" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">因为我们最多有两个对象是输入图像，所以我们的RPN很有可能移动了不同的锚点来与同一个对象重叠。我们将使用非最大抑制来过滤掉高度重叠的提议，因为它们是不必要的。我们只会保留最好的。</p><p id="a77d" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi lb translated"><span class="l lc ld le bm lf lg lh li lj di"> 3。</span>最后一步是“头部”模型，这些可以并行训练。<br/>在我们有了我们的建议之后，我们需要一个神经网络来进一步细化包围盒坐标，并且预测分类标签。因此，它不仅会预测一个区域是前景还是背景，而且如果它是前景，它会对它进行分类(在我的toydatasets案例中是心形或星形)。就像在RPN的情况下，只有前景建议会造成盒损失，并且任意固定数量的建议classlabel会造成类损失。</p><figure class="lm ln lo lp gt lq"><div class="bz fp l di"><div class="mg ls l"/></div></figure><p id="1da0" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们不仅为每个建议预测一组盒精化坐标，而是三组。三是班级人数+1为背景。</p><p id="48a9" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">所以在我们的例子中，我们预测每个提议的3*4坐标。我们将基于我们在同一网络中预测的类来选择适当的盒细化坐标。</p><figure class="lm ln lo lp gt lq"><div class="bz fp l di"><div class="mg ls l"/></div><p class="ma mb gj gh gi mc md bd b be z dk translated">在训练过程中，我们简单地使用chloss作为classloss + deltaloss的总和。如果有必要，我们可以使用损失权重。我们使用一个新的提议计数，因为我们对RPN提议使用非最大值抑制，以避免提议重叠。正因为如此，我们将只剩下很少(&lt; 5)的前景框，我们希望有一个平衡的训练。(大约5FG-5BG提案)</p></figure><p id="a064" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">maskhead是一个卷积网络，末端有一个上采样层。它仅对接收到的ROI(而不是整个图像)预测一个遮罩。</p><figure class="lm ln lo lp gt lq"><div class="bz fp l di"><div class="mg ls l"/></div></figure><p id="9156" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">最后一个卷积层的滤波器数目等于类的数目。当检查预测掩码时，我们需要使用过滤器，对应于classlabel。因此，如果我们的标签是:0:star，1:heart，并且我们的输入图像上有一颗心，我们将从filter维度中获取第一个轴。</p><figure class="lm ln lo lp gt lq"><div class="bz fp l di"><div class="mg ls l"/></div></figure><p id="4cec" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">正如我之前提到的，这些头部模型可以并行训练，但是，如果没有分类预测，你无法预测一个面具。(与预测的情况一样，您没有基础真实类标签，因此您将需要预测类标签来选择合适的掩膜。)</p><p id="4aff" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi lb translated">这就是了，我们有一个正常工作的R-CNN。希望这篇文章有助于理解基础知识，以便您可以实现自己的，甚至是改进的Mask R-CNN。</p><p id="f66b" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">你可以在https://github.com/rajkifranciska/maskrcnn-from-scratch<br/><a class="ae lk" href="https://github.com/rajkifranciska/maskrcnn-from-scratch" rel="noopener ugc nofollow" target="_blank"/>找到整个实现和toydataset生成文件</p><p id="7ea5" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如果这篇文章有帮助，请引用我的话:</p><p id="7f0d" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><a class="ae lk" href="http://twitter.com/misc" rel="noopener ugc nofollow" target="_blank"> @misc </a> {rajki_2021，<br/>title = {在TensorFlow和Keras中从头开始构建一个面具R-CNN }，journal={Medium}，<br/> author={Rajki，Franciska}，<br/> year={2021}，month={Mar}}</p></div><div class="ab cl mi mj hu mk" role="separator"><span class="ml bw bk mm mn mo"/><span class="ml bw bk mm mn mo"/><span class="ml bw bk mm mn"/></div><div class="ij ik il im in"><p id="5e06" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">[1].瓦利德·阿卜杜拉<br/> <strong class="kh ir">掩膜R-CNN用于Keras和Tensorflow上的物体检测和实例分割</strong>，2017<br/><a class="ae lk" href="https://github.com/matterport/Mask_RCNN" rel="noopener ugc nofollow" target="_blank">https://github.com/matterport/Mask_RCNN</a></p><p id="ceb6" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">[2.]吴雨欣，亚历山大·基里洛夫，弗朗西斯科·马萨，卢万延，罗斯·吉斯克<br/> <strong class="kh ir">侦探2 </strong>，2019<br/>https://github.com/facebookresearch/detectron2</p><p id="0e2e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">[3].阿玛托三世，SG；麦克伦南，G；比达乌，L；McNitt-Gray，MF；迈耶河；美联社里夫斯；赵，乙；阿伯勒博士；亨施克，CI；埃里克·霍夫曼；卡泽罗尼，EA；麦克马洪；范·比克，EJR；Yankelevitz，D；Biancardi，AM；淡而无味，PH；布朗，密西西比州；恩格尔曼，RM；拉德拉赫，葛；Max，D；Pais清，DPY罗伯茨，RY；阿肯色州史密斯；斯塔基，A；巴特拉，P；卡利久里，P；法鲁奇，阿里；Gladish，GW；裘德，CM；Munden佩特科夫斯卡，我；、乐；施瓦茨，LH；孙达拉姆湾；多德，勒；费尼莫尔，C；Gur，D；彼得里克，N；弗莱曼，J；柯比，J；休斯湾；卡斯特利，AV；Gupte，S；萨拉姆，M；医学博士希斯；MH库恩；Dharaiya，E；伯恩斯河；Fryd，DS；萨尔加尼科夫，M；阿南德五世；Shreter，U；Vastagh，S；克罗夫特，由；克拉克唱片公司。<br/>【http://doi.org/10.7937/K9/TCIA.2015.LO9QL9SX】肿瘤影像存档，2015<br/>T24</p></div></div>    
</body>
</html>