<html>
<head>
<title>Black-box and White-Box Models towards Explainable AI</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">面向可解释人工智能的黑盒和白盒模型</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/black-box-and-white-box-models-towards-explainable-ai-172d45bfc512?source=collection_archive---------13-----------------------#2021-06-23">https://towardsdatascience.com/black-box-and-white-box-models-towards-explainable-ai-172d45bfc512?source=collection_archive---------13-----------------------#2021-06-23</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="758f" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph">第一部分 | <a class="ae ep" rel="noopener" target="_blank" href="/symbolic-vs-subsymbolic-ai-paradigms-for-ai-explainability-6e3982c6948a">第二部分</a> |可解释的人工智能——第三部分</h2><div class=""/><div class=""><h2 id="51f2" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">使用模型属性、局部逻辑表示和全局逻辑表示从黑盒模型中生成解释</h2></div><div class="kr ks kt ku gt ab cb"><figure class="kv kw kx ky kz la lb paragraph-image"><div role="button" tabindex="0" class="lc ld di le bf lf"><img src="../Images/1991153585485a86571e89547e339710.png" data-original-src="https://miro.medium.com/v2/resize:fit:616/0*GX_D5KAD2h40M11h"/></div></figure><figure class="kv kw li ky kz la lb paragraph-image"><div role="button" tabindex="0" class="lc ld di le bf lf"><img src="../Images/88fdfa079cc32c8faecc284f449255d1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1386/0*aJXoIo0Fa8bByTrk"/></div><p class="lj lk gj gh gi ll lm bd b be z dk ln di lo lp translated">图一。照片由<a class="ae lq" href="https://unsplash.com/@donovan_valdivia?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Andrew“Donovan”valdi via</a>在<a class="ae lq" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a> |和图2拍摄。照片由<a class="ae lq" href="https://unsplash.com/@kelli_mcclintock?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">凯利·麦克林托克</a>在<a class="ae lq" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure></div><h1 id="27ea" class="lr ls it bd lt lu lv lw lx ly lz ma mb ki mc kj md kl me km mf ko mg kp mh mi bi translated">快速回顾:XAI和国家安全委员会</h1><p id="7ef0" class="pw-post-body-paragraph mj mk it ml b mm mn kd mo mp mq kg mr ms mt mu mv mw mx my mz na nb nc nd ne im bi translated"><strong class="ml jd">可解释的人工智能(XAI) </strong>处理开发人工智能模型，这些模型本质上对人类来说更容易理解，包括用户、开发者、决策者和执法人员。<strong class="ml jd">神经符号计算(NSC) </strong>处理子符号学习算法与符号推理方法的结合。因此，我们可以断言，神经符号计算是可解释人工智能下的一个子领域。NSC也是最适用的方法之一，因为它依赖于现有方法和模型的结合。</p><figure class="kr ks kt ku gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="lc ld di le bf lf"><div class="gh gi nf"><img src="../Images/0a65d942e5b29d0741e232cde0700b50.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lFbqqQ5UsCtmRrowjthNuA.png"/></div></div><p class="lj lk gj gh gi ll lm bd b be z dk translated">图3。符号人工智能vs次符号人工智能(作者提供图片)</p></figure><blockquote class="ng"><p id="5f10" class="nh ni it bd nj nk nl nm nn no np ne dk translated">如果说<strong class="ak">可解释性</strong>是指<strong class="ak">用<strong class="ak">人类语言有意义地描述</strong>事物的能力。换句话说，它</strong>是将<strong class="ak">原始信息(数据)<strong class="ak">映射成对人类有意义的<strong class="ak">符号表示</strong>的可能性(例如英文文本)</strong></strong></p></blockquote><p id="9b36" class="pw-post-body-paragraph mj mk it ml b mm nq kd mo mp nr kg mr ms ns mu mv mw nt my mz na nu nc nd ne im bi translated">通过从子符号中提取符号，我们可以使这些子符号变得可以解释。XAI和国家安全委员会都试图让次符号系统更容易解释。NSC 更多的是关于将子符号映射到符号，通过逻辑设计的可解释性:对子符号学习表示的符号推理。XAI没有那么具体，更多的是关于所有细微差别的可解释性，即使可解释性被包裹在不可解释的模型中。如果从子符号中提取符号意味着可解释性，那么<strong class="ml jd"> XAI包括NSC </strong>。</p><blockquote class="nv nw nx"><p id="866f" class="mj mk ny ml b mm nz kd mo mp oa kg mr ob oc mu mv od oe my mz of og nc nd ne im bi translated">让我们通过一个示例来看看这些NSC:</p></blockquote><h2 id="6c85" class="oh ls it bd lt oi oj dn lx ok ol dp mb ms om on md mw oo op mf na oq or mh iz bi translated">神经符号概念学习者</h2><p id="d568" class="pw-post-body-paragraph mj mk it ml b mm mn kd mo mp mq kg mr ms mt mu mv mw mx my mz na nb nc nd ne im bi translated">Mao等人提出了一种新的NSC模型，即神经符号概念学习器，它遵循以下步骤:</p><figure class="kr ks kt ku gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="lc ld di le bf lf"><div class="gh gi os"><img src="../Images/437865d215b2291c8659451e5f1a33e7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4wvKUA3qSIquTeyoMSqQSg.png"/></div></div><p class="lj lk gj gh gi ll lm bd b be z dk translated">图4。神经符号概念学习者(图由<a class="ae lq" href="https://arxiv.org/pdf/1904.12584.pdf" rel="noopener ugc nofollow" target="_blank">毛等</a></p></figure><ul class=""><li id="3cae" class="ot ou it ml b mm nz mp oa ms ov mw ow na ox ne oy oz pa pb bi translated">图像分类器学习从图像或文本片段中提取子符号(数字)表示。</li><li id="9551" class="ot ou it ml b mm pc mp pd ms pe mw pf na pg ne oy oz pa pb bi translated">然后，每个子符号表示都与一个人类可以理解的符号相关联。</li><li id="b843" class="ot ou it ml b mm pc mp pd ms pe mw pf na pg ne oy oz pa pb bi translated">然后，符号推理器检查符号表示的嵌入相似性</li><li id="06b0" class="ot ou it ml b mm pc mp pd ms pe mw pf na pg ne oy oz pa pb bi translated">训练继续进行，直到通过更新表示使推理机的输出精度最大化。</li></ul><h1 id="4549" class="lr ls it bd lt lu lv lw lx ly lz ma mb ki mc kj md kl me km mf ko mg kp mh mi bi translated">白盒与黑盒模型</h1><p id="b7df" class="pw-post-body-paragraph mj mk it ml b mm mn kd mo mp mq kg mr ms mt mu mv mw mx my mz na nb nc nd ne im bi translated">人工智能模型可以是(I)白盒或(ii)黑盒。</p><ul class=""><li id="7be9" class="ot ou it ml b mm nz mp oa ms ov mw ow na ox ne oy oz pa pb bi translated">白盒模型可以通过设计来解释。因此，它不需要额外的能力来解释。</li><li id="97db" class="ot ou it ml b mm pc mp pd ms pe mw pf na pg ne oy oz pa pb bi translated">黑箱模型本身是无法解释的。因此，为了使一个黑盒模型可解释，我们必须采用几种技术从模型的内部逻辑或输出中提取解释。</li></ul><p id="6744" class="pw-post-body-paragraph mj mk it ml b mm nz kd mo mp oa kg mr ms oc mu mv mw oe my mz na og nc nd ne im bi translated">黑盒模型可以用</p><ol class=""><li id="fa86" class="ot ou it ml b mm nz mp oa ms ov mw ow na ox ne ph oz pa pb bi translated"><strong class="ml jd">模型属性:</strong>展示模型或其预测的特定属性，如(a)对属性变化的敏感性或(b)负责给定决策的模型组件(如神经元或节点)的识别。</li><li id="178d" class="ot ou it ml b mm pc mp pd ms pe mw pf na pg ne ph oz pa pb bi translated"><strong class="ml jd">局部逻辑:</strong>单个决策或预测背后的内在逻辑的表示。</li><li id="9875" class="ot ou it ml b mm pc mp pd ms pe mw pf na pg ne ph oz pa pb bi translated"><strong class="ml jd">全局逻辑</strong>:整个内部逻辑的表现。</li></ol><p id="2e29" class="pw-post-body-paragraph mj mk it ml b mm nz kd mo mp oa kg mr ms oc mu mv mw oe my mz na og nc nd ne im bi translated">因此，下图显示了人工智能模型在可解释性方面的子类别:</p><figure class="kr ks kt ku gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="lc ld di le bf lf"><div class="gh gi pi"><img src="../Images/6f6101abd8dc0aac554e54ae53592f50.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cM0wVkhUro1MJsSO_Vby3Q.png"/></div></div><p class="lj lk gj gh gi ll lm bd b be z dk translated">图5。简单易懂的人工智能分类法</p></figure><h1 id="0d0a" class="lr ls it bd lt lu lv lw lx ly lz ma mb ki mc kj md kl me km mf ko mg kp mh mi bi translated">基于规则的可解释性与基于案例的可解释性</h1><p id="6442" class="pw-post-body-paragraph mj mk it ml b mm mn kd mo mp mq kg mr ms mt mu mv mw mx my mz na nb nc nd ne im bi translated">除了可解释模型的逻辑区别之外，我们还确定了两种常见的解释类型，所有上述模型都可以采用这两种类型来提供解释:</p><p id="cb89" class="pw-post-body-paragraph mj mk it ml b mm nz kd mo mp oa kg mr ms oc mu mv mw oe my mz na og nc nd ne im bi translated"><strong class="ml jd">基于规则的解释</strong>:基于规则的可解释性依赖于生成一套“形式化的逻辑规则，阐明给定模型的内部逻辑”。</p><figure class="kr ks kt ku gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="lc ld di le bf lf"><div class="gh gi pj"><img src="../Images/3e3964a2c37e3d8713a3c7c7283a029d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VFuJLvj4L46TOg_L5zbxJQ.png"/></div></div><p class="lj lk gj gh gi ll lm bd b be z dk translated">图6。决策树可以很容易地公式化，以提供基于规则的解释(图由<a class="ae lq" href="https://arxiv.org/pdf/1805.10820.pdf" rel="noopener ugc nofollow" target="_blank"> Guidotti等人</a></p></figure><p id="ed56" class="pw-post-body-paragraph mj mk it ml b mm nz kd mo mp oa kg mr ms oc mu mv mw oe my mz na og nc nd ne im bi translated"><strong class="ml jd">基于案例的解释</strong>:基于规则的可解释性依赖于提供有价值的输入输出对(积极的和消极的)来提供模型内部逻辑的直觉。基于案例的解释依赖于人从这些对中推断逻辑的能力。</p><figure class="kr ks kt ku gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="lc ld di le bf lf"><div class="gh gi pk"><img src="../Images/af7ea200b9601114a50e62c42c391a88.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LxMyCvdF5ANXd8u08PZFaA.png"/></div></div><p class="lj lk gj gh gi ll lm bd b be z dk translated">图7。基于案例的解释示例(作者图)(图片来自<a class="ae lq" href="https://unsplash.com/" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>)</p></figure><blockquote class="nv nw nx"><p id="4378" class="mj mk ny ml b mm nz kd mo mp oa kg mr ob oc mu mv od oe my mz of og nc nd ne im bi translated"><strong class="ml jd">基于规则与基于案例的学习算法比较示例:</strong></p></blockquote><p id="f49d" class="pw-post-body-paragraph mj mk it ml b mm nz kd mo mp oa kg mr ms oc mu mv mw oe my mz na og nc nd ne im bi translated">假设我们的模型需要学习如何做苹果派的食谱。我们有蓝莓派、芝士蛋糕、牧羊人派的食谱，还有一份简单的蛋糕食谱。基于规则的学习方法试图为制作所有类型的甜点提出一套通用规则(即，渴望方法)，而基于案例的学习方法则根据需要概括信息，以涵盖特定的任务。因此，它会在可用数据中寻找与苹果派最相似的甜点。然后，它会尝试在类似的食谱上做一些小的改动。</p><h2 id="c917" class="oh ls it bd lt oi oj dn lx ok ol dp mb ms om on md mw oo op mf na oq or mh iz bi translated">XAI:设计白盒模型</h2><p id="2757" class="pw-post-body-paragraph mj mk it ml b mm mn kd mo mp mq kg mr ms mt mu mv mw mx my mz na nb nc nd ne im bi translated">包括基于规则和基于案例的学习系统，我们有四类主要的白盒设计:</p><ul class=""><li id="61c4" class="ot ou it ml b mm nz mp oa ms ov mw ow na ox ne oy oz pa pb bi translated">手工制作的专家系统；</li><li id="dcde" class="ot ou it ml b mm pc mp pd ms pe mw pf na pg ne oy oz pa pb bi translated"><strong class="ml jd">基于规则的学习系统:</strong>从数据中学习逻辑规则的算法，如归纳逻辑编程、决策树等；</li><li id="6a00" class="ot ou it ml b mm pc mp pd ms pe mw pf na pg ne oy oz pa pb bi translated"><strong class="ml jd">基于案例的学习系统</strong>:基于案例推理的算法。他们利用例子、案例、先例和/或反例来解释系统输出；和</li><li id="2549" class="ot ou it ml b mm pc mp pd ms pe mw pf na pg ne oy oz pa pb bi translated"><strong class="ml jd">嵌入符号&amp;提取系统</strong>:更多的生物启发算法，如<strong class="ml jd">神经符号计算</strong>。</li></ul><p id="b13f" class="pw-post-body-paragraph mj mk it ml b mm nz kd mo mp oa kg mr ms oc mu mv mw oe my mz na og nc nd ne im bi translated">在本系列接下来的部分中，我们将有这些方法的实际例子。</p><h1 id="1616" class="lr ls it bd lt lu lv lw lx ly lz ma mb ki mc kj md kl me km mf ko mg kp mh mi bi translated">最终注释</h1><p id="653c" class="pw-post-body-paragraph mj mk it ml b mm mn kd mo mp mq kg mr ms mt mu mv mw mx my mz na nb nc nd ne im bi translated">在本帖中，我们:</p><p id="5ab4" class="pw-post-body-paragraph mj mk it ml b mm nz kd mo mp oa kg mr ms oc mu mv mw oe my mz na og nc nd ne im bi translated">1 —简要介绍了XAI和NSC之间的差异和相似之处；</p><p id="85fb" class="pw-post-body-paragraph mj mk it ml b mm nz kd mo mp oa kg mr ms oc mu mv mw oe my mz na og nc nd ne im bi translated">2 —定义和比较黑盒和白盒模型；</p><p id="7ccc" class="pw-post-body-paragraph mj mk it ml b mm nz kd mo mp oa kg mr ms oc mu mv mw oe my mz na og nc nd ne im bi translated">3 —使黑盒模型可解释的方法(模型属性、局部逻辑、全局逻辑)；</p><p id="c235" class="pw-post-body-paragraph mj mk it ml b mm nz kd mo mp oa kg mr ms oc mu mv mw oe my mz na og nc nd ne im bi translated">4-比较基于规则的解释和基于案例的解释，并举例说明。</p><p id="0459" class="pw-post-body-paragraph mj mk it ml b mm nz kd mo mp oa kg mr ms oc mu mv mw oe my mz na og nc nd ne im bi translated">在下一篇文章中，我们将介绍市场上可解释工作的库和技术，并将使用其中一些库从黑盒模型和白盒模型中提取解释。</p><h1 id="b749" class="lr ls it bd lt lu lv lw lx ly lz ma mb ki mc kj md kl me km mf ko mg kp mh mi bi translated">订阅邮件列表获取更多内容</h1><p id="49e3" class="pw-post-body-paragraph mj mk it ml b mm mn kd mo mp mq kg mr ms mt mu mv mw mx my mz na nb nc nd ne im bi translated">如果你想获得我在Google Colab上的其他教程文章的代码，并尽早获得我的最新内容，可以考虑订阅:✉️的邮件列表</p><blockquote class="nv nw nx"><p id="2df8" class="mj mk ny ml b mm nz kd mo mp oa kg mr ob oc mu mv od oe my mz of og nc nd ne im bi translated"><a class="ae lq" href="https://medium.us4.list-manage.com/subscribe?u=bf0e9524ea3d765ba10131675&amp;id=52221072de" rel="noopener ugc nofollow" target="_blank"> <em class="it">现在就订阅</em> </a></p></blockquote><p id="d4fc" class="pw-post-body-paragraph mj mk it ml b mm nz kd mo mp oa kg mr ms oc mu mv mw oe my mz na og nc nd ne im bi translated">如果你对深度学习感兴趣，也可以看看我的人工智能内容指南:</p><div class="pl pm gp gr pn po"><a href="https://blog.orhangaziyalcin.com/a-guide-to-my-content-on-artificial-intelligence-c70c9b4a3b17" rel="noopener  ugc nofollow" target="_blank"><div class="pp ab fo"><div class="pq ab pr cl cj ps"><h2 class="bd jd gy z fp pt fr fs pu fu fw jc bi translated">我的人工智能内容指南</h2><div class="pv l"><h3 class="bd b gy z fp pt fr fs pu fu fw dk translated">帮助您轻松浏览我的内容的指南。</h3></div><div class="pw l"><p class="bd b dl z fp pt fr fs pu fu fw dk translated">blog.orhangaziyalcin.com</p></div></div><div class="px l"><div class="py l pz qa qb px qc lg po"/></div></div></a></div><p id="03ee" class="pw-post-body-paragraph mj mk it ml b mm nz kd mo mp oa kg mr ms oc mu mv mw oe my mz na og nc nd ne im bi translated">如果你正在阅读这篇文章，我确信我们有着相似的兴趣，并且现在/将来会从事相似的行业。那么我们就通过 <a class="ae lq" href="https://linkedin.com/in/orhangaziyalcin/" rel="noopener ugc nofollow" target="_blank"> <em class="ny"> Linkedin </em> </a> <em class="ny">来连线吧！请不要犹豫发送联系请求！</em><a class="ae lq" href="https://linkedin.com/in/orhangaziyalcin/" rel="noopener ugc nofollow" target="_blank"><em class="ny">Orhan g . yaln—Linkedin</em></a></p><h1 id="323c" class="lr ls it bd lt lu lv lw lx ly lz ma mb ki mc kj md kl me km mf ko mg kp mh mi bi translated">参考</h1><p id="785d" class="pw-post-body-paragraph mj mk it ml b mm mn kd mo mp mq kg mr ms mt mu mv mw mx my mz na nb nc nd ne im bi translated">Guidotti，r .，Monreale，a .，Ruggieri，s .，Pedreschi，d .，Turini，f .，&amp; Giannotti，F. (2018年)。<em class="ny">黑盒决策系统的基于局部规则的解释</em>。<a class="ae lq" href="http://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing" rel="noopener ugc nofollow" target="_blank">www . propublica . org/article/machine-bias-risk-assessments-in-criminal-pending</a></p><p id="86de" class="pw-post-body-paragraph mj mk it ml b mm nz kd mo mp oa kg mr ms oc mu mv mw oe my mz na og nc nd ne im bi translated">毛，甘，c，柯利，p，特南鲍姆，J. B .，，吴，J. (2019)。神经符号概念学习者:从自然监督中解读场景、词语和句子。<em class="ny">第七届国际学习代表大会，ICLR 2019 </em>。<a class="ae lq" href="http://nscl.csail.mit.edu" rel="noopener ugc nofollow" target="_blank">http://nscl.csail.mit.edu</a></p><p id="e14c" class="pw-post-body-paragraph mj mk it ml b mm nz kd mo mp oa kg mr ms oc mu mv mw oe my mz na og nc nd ne im bi translated">Sovrano，F. (2021)，现代人工智能对可解释性的需求，博洛尼亚大学信息学院</p><figure class="kr ks kt ku gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="lc ld di le bf lf"><div class="gh gi qd"><img src="../Images/43ea8c1489acb22feb6d8e2b70d65e57.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*beyOIMWqO647Jq9aPf1eOA.png"/></div></div><p class="lj lk gj gh gi ll lm bd b be z dk translated">图一。Andrew "Donovan" Valdivia 在<a class="ae lq" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a> |和图2上拍摄的照片。照片由<a class="ae lq" href="https://unsplash.com/@kelli_mcclintock?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">凯利·麦克林托克</a>在<a class="ae lq" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure></div></div>    
</body>
</html>