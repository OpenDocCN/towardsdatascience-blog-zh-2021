<html>
<head>
<title>The Creative Side of Vision Transformers</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">视觉变形金刚的创意一面</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/the-creative-side-of-vision-transformers-e3efa7c4b859?source=collection_archive---------22-----------------------#2021-09-14">https://towardsdatascience.com/the-creative-side-of-vision-transformers-e3efa7c4b859?source=collection_archive---------22-----------------------#2021-09-14</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="c381" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">视觉变形器生成图像并绘制照片</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/53d665551f61dffd07b1d44ac77d9c41.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*c9P_BGZPx9dbKLMwqHbsrw.png"/></div></div></figure><p id="18e1" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">什么是创造力？最受认可的定义如下:</p><p id="81a3" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><em class="lq">“创造力是创造新奇事物的能力”</em></p><p id="fef3" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">它被认为是人类最重要和不可替代的特性之一。但如果这是如此特殊的特征，神经网络就不可能模仿它，不是吗？不完全是。今天，我们在创建具有生成模型的创造性人工智能方面面临着一些特殊的问题，主要被称为生成对抗网络(GANs)。这些被深度学习之父之一Yann Le Cunn认为是本世纪在AI领域最重要的突破[8]。</p><h1 id="51ce" class="lr ls it bd lt lu lv lw lx ly lz ma mb jz mc ka md kc me kd mf kf mg kg mh mi bi translated">生成对抗网络</h1><p id="86d6" class="pw-post-body-paragraph ku kv it kw b kx mj ju kz la mk jx lc ld ml lf lg lh mm lj lk ll mn ln lo lp im bi translated">生成对抗网络(GANs) [3]由Ian Goodfellow在2014年提出，并立即用于执行壮观的和从未探索过的任务。</p><p id="4980" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">第一个也是最广泛使用的应用是生成新图像，正如我在以前的文章《<a class="ae mo" rel="noopener" target="_blank" href="/how-does-an-ai-imagine-the-universe-d1d01139b50a">人工智能如何想象宇宙》中所展示的那样？</a>【5】生成如下行星和天体的图像:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mp"><img src="../Images/1f1055fdcede8e71e6de0141deee2ad7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2uYM3_uhF33rFpdfdnfcag.jpeg"/></div></div><p class="mq mr gj gh gi ms mt bd b be z dk translated">图片由<a class="ae mo" href="https://www.linkedin.com/in/davide-coccomini/" rel="noopener ugc nofollow" target="_blank">作者</a></p></figure><p id="67a0" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">所有这些图像都代表了由神经网络生成的不存在的对象，动物、人以及所有形状和类型的对象也是如此。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mu"><img src="../Images/fe191cac6a6b977bd3b6ce9646089d84.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*d1ePubYV6dKwjpPww3XObw.jpeg"/></div></div><p class="mq mr gj gh gi ms mt bd b be z dk translated">使用<a class="ae mo" href="https://github.com/lucidrains/lightweight-gan" rel="noopener ugc nofollow" target="_blank">轻量级GAN </a>生成的图像</p></figure><p id="0a4e" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">这些网络最近被证明能够执行非常有用的任务，例如提高照片的分辨率，或者有时非常有趣，例如让史蒂夫·鲍尔默和小罗伯特·唐尼合唱布鲁诺·马斯的《Uptown Funk！</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mv mw l"/></div><p class="mq mr gj gh gi ms mt bd b be z dk translated">作者使用Wombo.ai制作的视频<a class="ae mo" href="https://www.linkedin.com/in/davide-coccomini/" rel="noopener ugc nofollow" target="_blank"/></p></figure><p id="05f3" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">尽管乍一看这似乎很吸引人，而且完全无害，但这背后隐藏着严重的影响和危险。这些模型还每天被用来欺骗人们或破坏他们的声誉，例如通过所谓的deepfakes。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mx mw l"/></div></figure><p id="0189" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">现在有了这项技术，就可以把源演员的脸转置到目标演员的脸上，使其看起来、说话或动作都和源演员一模一样。一个视频deepfake可以让人们相信一位政治领导人发表了特别危险的言论，加剧了人们之间的仇恨，或将一个人置于他或她从未经历过的不恰当的环境中，或者这些视频可以在司法程序中代表伪造的证据。</p><p id="795b" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">像所有的工具一样，它没有好坏之分，它总是取决于你如何使用它。</p><p id="9e5e" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">但是在深入本文的核心之前，对生成性对抗网络的内部工作原理做一点解释是合适的。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi my"><img src="../Images/bd6c2f6f19e4503ab3a44ac3351c019b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*HMVA-HEsP_GGNRhL54QEXQ.gif"/></div></div><p class="mq mr gj gh gi ms mt bd b be z dk translated">图片由<a class="ae mo" href="https://www.linkedin.com/in/davide-coccomini/" rel="noopener ugc nofollow" target="_blank">作者</a>来自<a class="ae mo" href="https://medium.com/r?url=https%3A%2F%2Ftowardsdatascience.com%2Fhow-does-an-ai-imagine-the-universe-d1d01139b50a" rel="noopener">人工智能如何想象宇宙？</a></p></figure><p id="95b6" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">GANs由两个不同的网络组成，通常是两个卷积网络，一个生成器和一个鉴别器，它们是相互对照的。生成器查看输入数据，试图生成新的可信图像，以欺骗鉴别器。另一方面，鉴别器试图理解给定的图像是生成的还是原始的。当生成器在生成图像方面变得足够好以至于骗过鉴别器时，除了输入数据中存在的例子之外，它还可以用于创建其他可信的例子。</p><h1 id="832e" class="lr ls it bd lt lu lv lw lx ly lz ma mb jz mc ka md kc me kd mf kf mg kg mh mi bi translated">视觉变形金刚</h1><p id="62a2" class="pw-post-body-paragraph ku kv it kw b kx mj ju kz la mk jx lc ld ml lf lg lh mm lj lk ll mn ln lo lp im bi translated">在进入文章核心之前，最后一点题外话。对于那些从未听说过视觉变形金刚的人来说，这是一种正在彻底改变计算机视觉领域的新架构。它基于自我注意的机制，该机制使用通过将输入图像分割成小块并将它们投影到线性空间而获得的向量。基于视觉转换器的架构是当今计算机视觉中最有前途的方法之一，并取得了惊人的成果。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi my"><img src="../Images/e10d295451f10a0fbbe29fc68bd835c4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*ZIX43B4Op1nIMVbkpV7-Yw.gif"/></div></div><p class="mq mr gj gh gi ms mt bd b be z dk translated">图片由<a class="ae mo" href="https://www.linkedin.com/in/davide-coccomini/" rel="noopener ugc nofollow" target="_blank">作者</a>从<a class="ae mo" rel="noopener" target="_blank" href="/transformers-an-exciting-revolution-from-text-to-videos-dc70a15e617b">上看变形金刚、时代先锋和关注</a></p></figure><p id="c80a" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><a class="ae mo" rel="noopener" target="_blank" href="/transformers-an-exciting-revolution-from-text-to-videos-dc70a15e617b">关于视觉变形金刚的深入解释，建议你去看看我之前的文章。</a></p><p id="5761" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">但是如果视觉变形金刚这么棒，难道不值得用它们来做GANs吗？</p><h1 id="e66d" class="lr ls it bd lt lu lv lw lx ly lz ma mb jz mc ka md kc me kd mf kf mg kg mh mi bi translated">TransGAN:两个纯变形金刚可以组成一个强大的GAN，而且可以扩大规模！</h1><p id="32d9" class="pw-post-body-paragraph ku kv it kw b kx mj ju kz la mk jx lc ld ml lf lg lh mm lj lk ll mn ln lo lp im bi translated">这项研究旨在回答这样一个问题:不使用卷积网络能否创建GAN？gan通常依赖卷积网络作为生成器和鉴别器。相反，在这种架构中，只使用视觉转换器，这对于鉴别器来说非常简单，因为它是一个普通的分类器，必须判断图像是否是伪造的，但生成器呢？在这种情况下，情况更加复杂，因为依赖于自关注机制的变压器需要大量的计算资源，因此以期望的分辨率直接生成图像可能很快变得过于昂贵。</p><p id="46a4" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">因此，发生器的架构由一系列变换编码器组组成。每一个都以增加的分辨率接收图像，但是在减少的通道数量上，以便降低复杂性。换句话说，生成器开始处理带有许多通道的小图像，然后处理带有较少通道的较大图像。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mz"><img src="../Images/ab5fd2af93e3bc06e33ec17d56ca6b79.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*Ny_StDhWTKnbX89BameB-w.gif"/></div></div><p class="mq mr gj gh gi ms mt bd b be z dk translated">图片由<a class="ae mo" href="https://www.linkedin.com/in/davide-coccomini/" rel="noopener ugc nofollow" target="_blank">作者</a>基于<a class="ae mo" href="https://arxiv.org/pdf/2102.07074.pdf" rel="noopener ugc nofollow" target="_blank"> TransGAN:两个纯变形金刚可以做一个强GAN，并且可以放大</a></p></figure><p id="55a9" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">例如，如果我们想要在3个通道上生成一个32×32像素的图像，则最初会生成一个代表简单噪声、大小为8×8的图像，就像在经典GAN中一样。这由一系列的变换编码器进行变换，过一会儿，进行放大，将其变换为16x16，并减少通道的数量。这个过程被进一步重复，直到达到32×32的目标分辨率。最后，由最后一组编码器生成的矢量被转换回小块，以构成输出图像。</p><p id="dd59" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">因此，有了TransGANs [1]，就有可能以一种计算上可持续的方式生成图像，而不使用卷积网络，但是有一个小细节仍然需要研究:<strong class="kw iu">放大是如何执行的？</strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi na"><img src="../Images/79625d7dcc7f4610ed6121637ecf350d.png" data-original-src="https://miro.medium.com/v2/resize:fit:998/1*6z21tzyESoUU-w4F7uj90Q.gif"/></div><p class="mq mr gj gh gi ms mt bd b be z dk translated">图片由<a class="ae mo" href="https://www.linkedin.com/in/davide-coccomini/" rel="noopener ugc nofollow" target="_blank">作者</a>基于<a class="ae mo" href="https://arxiv.org/pdf/1609.05158.pdf" rel="noopener ugc nofollow" target="_blank">的实时单幅图像和视频超分辨率采用高效的亚像素卷积神经网络</a></p></figure><p id="82c9" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">在增加分辨率的同时减少通道所采用的策略如图所示，包括将每个像素“列”转换成单个超像素。所有超像素被组合在一起以获得单个更高分辨率的图像。</p><p id="4faf" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">为了通过实现更高分辨率的输出来进一步提高性能，最后的变压器编码器被<strong class="kw iu">网格变压器模块</strong>所取代。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi my"><img src="../Images/a141eda1ff5a7b891870d4f2c94fc4ef.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*Zp-8Nrj2ff-MtZbR5T1QgA.gif"/></div></div><p class="mq mr gj gh gi ms mt bd b be z dk translated">图片由<a class="ae mo" href="https://www.linkedin.com/in/davide-coccomini/" rel="noopener ugc nofollow" target="_blank">作者</a>基于<a class="ae mo" href="https://arxiv.org/pdf/2102.07074.pdf" rel="noopener ugc nofollow" target="_blank"> TransGAN:两个纯变压器可以组成一个强大的GAN，并且可以按比例放大</a></p></figure><p id="3f79" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">在所谓的网格自注意中，不是计算给定标记和所有其他标记之间的对应关系，而是将全尺寸特征图划分成许多不重叠的网格，并且在每个局部网格内计算标记交互。</p><p id="ccda" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">TransGAN能够在STL-10上建立新的最先进水平，并在CelebA和CIFAR-10等其他数据集上匹配基于卷积网络的架构的结果。</p><p id="c237" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">结果在许多情况下都非常出色，激发了人们对基于视觉变形金刚的新GAN变体的兴趣！</p><h1 id="a6ef" class="lr ls it bd lt lu lv lw lx ly lz ma mb jz mc ka md kc me kd mf kf mg kg mh mi bi translated">油漆变压器</h1><p id="ea08" class="pw-post-body-paragraph ku kv it kw b kx mj ju kz la mk jx lc ld ml lf lg lh mm lj lk ll mn ln lo lp im bi translated">另一个基于视觉变形金刚的有趣且有创造性的作品是油漆变形金刚[2]。这个模型能够把一幅普通的画变成一幅画！</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nb"><img src="../Images/9326c11e67c7c0d1f126b916b1aac1ee.png" data-original-src="https://miro.medium.com/v2/resize:fit:800/1*njiYH026QR12SlWH4LP67w.gif"/></div><p class="mq mr gj gh gi ms mt bd b be z dk translated">图片来自<a class="ae mo" href="https://github.com/wzmsltw/PaintTransformer" rel="noopener ugc nofollow" target="_blank">官方油漆变形金刚库</a></p></figure><p id="0df3" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">这里的问题是作为给定图像的一系列笔划的预测来处理的，以便以非真实感的方式重新创建它。</p><p id="e176" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">该过程是渐进的，在每一步，模型并行预测多个笔画，以最小化当前画布和目标图像之间的差异。两个模块用于实现这一点:<strong class="kw iu">笔划预测器</strong>和<strong class="kw iu">笔划渲染器</strong>。</p><p id="5a83" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">笔划渲染器负责将一系列笔划作为输入，并将它们放在画布上。笔画预测器是架构的核心，其任务是学习预测生成图形所需的笔画。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi my"><img src="../Images/7051ebd86bed8582f15a432ad6b968a0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*QMt4ZdtCM3Dc3oHMqD731Q.gif"/></div></div><p class="mq mr gj gh gi ms mt bd b be z dk translated">图片由<a class="ae mo" href="https://www.linkedin.com/in/davide-coccomini/" rel="noopener ugc nofollow" target="_blank">作者</a>基于<a class="ae mo" href="https://arxiv.org/pdf/2108.03798.pdf" rel="noopener ugc nofollow" target="_blank">绘制转换器:带有笔画预测的前馈神经绘制</a></p></figure><p id="f7b1" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">整个培训过程是自我监督的，<a class="ae mo" rel="noopener" target="_blank" href="/self-supervised-learning-in-vision-transformers-30ff9be928c">一种创新的和更可持续的培训方法</a> [7]。首先，生成一些随机的背景笔画，并由笔画渲染器放置在空画布上，从而获得中间画布。随后，随机的前景笔划被生成并放置在前一个画布上，以从另一个笔划渲染器获得目标画布。预测器接收中间画布和目标画布，并试图生成要放置在中间画布上的潜在前景笔画。预测的笔画然后被放置在中间画布上，并且笔画和目标画布都被用于计算与原始笔画的差异，并且因此损失被最小化。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi my"><img src="../Images/d97d4275bcc498e1d3525f4e9db6ac4c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*CYYv6hWebMw_RFPzq61EcA.gif"/></div></div><p class="mq mr gj gh gi ms mt bd b be z dk translated">图片由<a class="ae mo" href="https://www.linkedin.com/in/davide-coccomini/" rel="noopener ugc nofollow" target="_blank">作者</a>基于<a class="ae mo" href="https://arxiv.org/pdf/2108.03798.pdf" rel="noopener ugc nofollow" target="_blank">绘制转换器:带有笔画预测的前馈神经绘制</a></p></figure><p id="262a" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">笔画预测器包含两个CNN，它们负责从两个输入画布中初步提取特征。这些笔画首先通过一个转换器编码器，然后通过一个转换器解码器输出所需的笔画。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nc"><img src="../Images/28aa2ad468a1633b8cd15fbafb9d91cc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5KbPzuPp_iYGnU-1wgYmHw.png"/></div></div><p class="mq mr gj gh gi ms mt bd b be z dk translated">来自<a class="ae mo" href="https://arxiv.org/pdf/2108.03798.pdf" rel="noopener ugc nofollow" target="_blank">绘制转换器的图像:带有笔划预测的前馈神经绘制</a></p></figure><p id="f484" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">由于这种架构，现在有可能将任何照片变成一幅具有不同和特殊效果的画！</p><h1 id="56a9" class="lr ls it bd lt lu lv lw lx ly lz ma mb jz mc ka md kc me kd mf kf mg kg mh mi bi translated"><strong class="ak">结论</strong></h1><p id="f758" class="pw-post-body-paragraph ku kv it kw b kx mj ju kz la mk jx lc ld ml lf lg lh mm lj lk ll mn ln lo lp im bi translated">有人说，除了创造性的工作，人工智能将能够做任何事情。今天，这种说法不再可靠。他们正在让现实变得不那么清晰，伪造的图像和视频很难与原始图像和视频区分开来，他们正在制作照片，给照片上色，假装自己是画家，以及许多其他有趣的应用。现代深度学习技术正在展示使用人工智能可能获得的奇迹，而我们只是触及了这个迷人领域的表面。</p><h1 id="a021" class="lr ls it bd lt lu lv lw lx ly lz ma mb jz mc ka md kc me kd mf kf mg kg mh mi bi translated">参考资料和见解</h1><p id="c358" class="pw-post-body-paragraph ku kv it kw b kx mj ju kz la mk jx lc ld ml lf lg lh mm lj lk ll mn ln lo lp im bi translated">[1]“江一帆等人”。<a class="ae mo" href="https://arxiv.org/abs/2102.07074" rel="noopener ugc nofollow" target="_blank"> TransGAN:两个纯变压器可以组成一个强GAN，并且可以放大</a></p><p id="eab6" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">[2]《刘松华等著》。"<a class="ae mo" href="https://arxiv.org/pdf/2108.03798.pdf" rel="noopener ugc nofollow" target="_blank">绘制转换器:带有笔画预测的前馈神经绘制</a>"</p><p id="400e" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">[3]“伊恩·j·古德费勒等人”。"<a class="ae mo" href="https://arxiv.org/abs/1406.2661" rel="noopener ugc nofollow" target="_blank">生成性对抗网络</a>"</p><p id="675a" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">[4]“阿列克谢·多索维茨基等人”。"<a class="ae mo" href="https://arxiv.org/abs/2010.11929" rel="noopener ugc nofollow" target="_blank">一幅图像相当于16x16个字:大规模图像识别的变形金刚</a>"</p><p id="a0be" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">[5]《大卫·柯考米尼》。"<a class="ae mo" rel="noopener" target="_blank" href="/how-does-an-ai-imagine-the-universe-d1d01139b50a">人工智能如何想象宇宙？</a></p><p id="e1ee" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">[6]《大卫·柯考米尼》。"<a class="ae mo" rel="noopener" target="_blank" href="/transformers-an-exciting-revolution-from-text-to-videos-dc70a15e617b">关于变形金刚，时间创造者和注意力</a>"</p><p id="e833" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">[7]《大卫·柯考米尼》。"<a class="ae mo" rel="noopener" target="_blank" href="/self-supervised-learning-in-vision-transformers-30ff9be928c">视觉变形金刚中的自我监督学习</a>"</p><p id="e10c" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">[8]《凯尔·威格斯》。"<a class="ae mo" href="https://venturebeat.com/2019/12/26/gan-generative-adversarial-network-explainer-ai-machine-learning/" rel="noopener ugc nofollow" target="_blank">生成性对抗网络:什么是gan以及它们是如何进化的</a>"</p><p id="1df9" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">[9]“大卫·科科米尼等人”。"<a class="ae mo" href="https://arxiv.org/abs/2107.02612" rel="noopener ugc nofollow" target="_blank">结合EfficientNet和视觉转换器进行视频深度防伪检测</a>"</p></div></div>    
</body>
</html>