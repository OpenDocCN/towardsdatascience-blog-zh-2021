<html>
<head>
<title>Scorecard Development for Finance Industry Using PyCaret — Part 2</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用PyCaret为金融行业开发记分卡—第2部分</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/scorecard-development-for-finance-industry-using-pycaret-part-2-142a7b2b82c8?source=collection_archive---------35-----------------------#2021-07-04">https://towardsdatascience.com/scorecard-development-for-finance-industry-using-pycaret-part-2-142a7b2b82c8?source=collection_archive---------35-----------------------#2021-07-04</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="cb66" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">使用轻编码工作流评估分类模型</h2></div><p id="dcbd" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在<a class="ae lb" rel="noopener" target="_blank" href="/scorecard-development-for-finance-industry-using-pycaret-part-1-8277c761160a">第1部分</a>中，我解释了如何使用PyCaret框架来利用模型开发/培训。在这一部分中，我们将重点介绍如何评估开发的模型，并将其与“训练/测试”和“超时验证数据集”进行比较。在PyCaret库中，一行代码就可以获得所有必要的模型评估指标，这将有助于最终确定模型。</p><figure class="ld le lf lg gt lh gh gi paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="gh gi lc"><img src="../Images/4be5b6200aec38f3cdd2d4c3043880cb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xidVpjIrnRadHPLTtjWI7w.jpeg"/></div></div><p class="lo lp gj gh gi lq lr bd b be z dk translated">照片由Ameen Fahmy (Unsplash)拍摄</p></figure><p id="84d6" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">模型评估可以大致分为<strong class="kh ir">稳定性</strong>和<strong class="kh ir">预测能力</strong>。而<strong class="kh ir">稳定性</strong>和<strong class="kh ir">预测力</strong>在其下有不同的成分。模型评估的不同指标解释如下。</p><figure class="ld le lf lg gt lh gh gi paragraph-image"><div class="gh gi ls"><img src="../Images/ab085536dc436ae25363c5e46e4fdd78.png" data-original-src="https://miro.medium.com/v2/resize:fit:1028/format:webp/1*e--A6A_xc4FjKll_UEd6_g.png"/></div><p class="lo lp gj gh gi lq lr bd b be z dk translated">模型评估的关键指标</p></figure><p id="74c2" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">群体稳定性指数</strong> —群体稳定性指数衡量<strong class="kh ir">开发</strong>和<strong class="kh ir">验证</strong> / <strong class="kh ir">测试数据集</strong>之间的分布差异。它总是取正值，指数值越高，差值就越大。</p><p id="3ab2" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">(开发% —验证%) *日志(开发% /验证%)*100</p><p id="1856" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><em class="lt">经验法则</em>:数值&lt; 10表示PSI为绿色(分布之间几乎没有差异)。10到25之间的值被视为琥珀色(这意味着在分布中观察到一些变化，需要调查)。PSI &gt; 25表示开发和验证数据集之间的分布不同。</p><p id="7ad2" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在上一部分中，我已经保存了scr_train和scr_test文件，其中包含客户列表、得分变量、事件的预测值、实际事件和模型中重要的重要特征。现在，为了获得模型，我们将使用以下代码:</p><pre class="ld le lf lg gt lu lv lw lx aw ly bi"><span id="b31d" class="lz ma iq lv b gy mb mc l md me"><strong class="lv ir">from</strong> <strong class="lv ir">sklearn.metrics</strong> <strong class="lv ir">import</strong> roc_auc_score,balanced_accuracy_score, f1_score, accuracy_score<br/><strong class="lv ir">from</strong> <strong class="lv ir">itertools</strong> <strong class="lv ir">import</strong> combinations, chain<br/><strong class="lv ir">from</strong> <strong class="lv ir">pandas._libs.lib</strong> <strong class="lv ir">import</strong> is_integer<br/><strong class="lv ir">from</strong> <strong class="lv ir">pycaret.classification</strong> <strong class="lv ir">import</strong> *<br/><strong class="lv ir">import</strong> <strong class="lv ir">matplotlib.patches</strong> <strong class="lv ir">as</strong> <strong class="lv ir">patches</strong><br/><strong class="lv ir">import</strong> <strong class="lv ir">matplotlib.ticker</strong> <strong class="lv ir">as</strong> <strong class="lv ir">mtick</strong><br/><strong class="lv ir">import</strong> <strong class="lv ir">matplotlib.pyplot</strong> <strong class="lv ir">as</strong> <strong class="lv ir">plt</strong><br/><strong class="lv ir">from</strong> <strong class="lv ir">scipy</strong> <strong class="lv ir">import</strong> stats<br/><strong class="lv ir">import</strong> <strong class="lv ir">seaborn</strong> <strong class="lv ir">as</strong> <strong class="lv ir">sns</strong><br/><strong class="lv ir">import</strong> <strong class="lv ir">sweetviz</strong> <strong class="lv ir">as</strong> <strong class="lv ir">sv</strong><br/><strong class="lv ir">import</strong> <strong class="lv ir">pandasql</strong> <strong class="lv ir">as</strong> <strong class="lv ir">ps</strong><br/><strong class="lv ir">import</strong> <strong class="lv ir">pandas</strong> <strong class="lv ir">as</strong> <strong class="lv ir">pd</strong><br/><strong class="lv ir">import</strong> <strong class="lv ir">numpy</strong> <strong class="lv ir">as</strong> <strong class="lv ir">np</strong><br/><em class="lt"># import shap</em><br/><strong class="lv ir">import</strong> <strong class="lv ir">math</strong></span><span id="beca" class="lz ma iq lv b gy mf mc l md me"><strong class="lv ir">def</strong> psi(X,Y):<br/>    <br/>    X['pentile'] = pd.qcut(X['score'], 5, labels=<strong class="lv ir">False</strong>) + 1<br/><br/>    <em class="lt">##Finding the boundary conditions for each pentile</em><br/>    X_tile = pd.DataFrame(X.groupby("pentile").agg({"score": [np.min, np.max]})).reset_index()<br/>    X_tile.columns = ['pentile','min','max']<br/><br/>    <em class="lt">##Fixing lowest and highest value for min and max respectively</em><br/>    X_tile.loc[0, 'min'] = -10000<br/>    X_tile.loc[4, 'max'] = 10000<br/>    <br/>    <em class="lt">##joining based on pentile conditions</em><br/>    sqlcode2 = '''<br/>    select c.pentile, c.cnt as X_count, c.X_tot, d.cnt as Y_cnt, d.Y_tot<br/>    from<br/>        (select a.*, b.*<br/>        from<br/>            (select b.pentile, count(*) as cnt<br/>            from X a<br/>            left join X_tile b<br/>            on a.score&gt;=b.min and a.score&lt;=b.max<br/>            group by b.pentile) a<br/>        cross join<br/>            (select count(*) as X_tot from X) b ) c<br/>    left join<br/>        (select a.*, b.*<br/>        from<br/>            (select b.pentile, count(*) as cnt<br/>            from Y a<br/>            left join X_tile b<br/>            on a.score&gt;=b.min and a.score&lt;=b.max<br/>            group by b.pentile) a<br/>        cross join<br/>                (select count(*) as Y_tot from Y) b ) d<br/>    on c.pentile=d.pentile<br/>    '''<br/>    <br/>    psi_stg0 = ps.sqldf(sqlcode2,locals()) <br/>    psi_stg0['X_perc'] = psi_stg0['X_count']/psi_stg0['X_tot']<br/>    psi_stg0['Y_perc'] = psi_stg0['Y_cnt']/psi_stg0['Y_tot']<br/>    psi_stg1 = psi_stg0.drop(['X_count', 'X_tot', 'Y_cnt','Y_tot'], axis=1)<br/><br/>    <em class="lt">##Final PSI calculation</em><br/>    psi_stg1['psi'] = (psi_stg1['X_perc'] - psi_stg1['Y_perc'])*np.log((psi_stg1['X_perc']/psi_stg1['Y_perc']))*100<br/>    psi_stg2 = pd.merge(psi_stg1, X_tile,  how='left', left_on=['pentile'], right_on = ['pentile'])<br/>    psi_stg2.loc[0, 'min'] = 'low'<br/>    psi_stg2.loc[4, 'max'] = 'high'<br/>    psi_stg2['score_band'] = psi_stg2['min'].astype(str) + "-" + psi_stg2['max'].astype(str)<br/>    psi = pd.DataFrame(psi_stg2[['score_band','X_perc','Y_perc','psi']])<br/>    <br/>    <strong class="lv ir">return</strong> psi</span><span id="5ea7" class="lz ma iq lv b gy mf mc l md me">psi_train_test = psi(scr_train, scr_test)<br/>psi_train_test = psi_train_test.rename(columns={'score_band': 'score_band', 'X_perc': 'scr_train_perc', 'Y_perc': 'scr_test_perc', 'psi': 'psi'})<br/>psi_train_test['scr_train_%']=round(psi_train_test['scr_train_perc']*100,2)<br/>psi_train_test['scr_test_%']=round(psi_train_test['scr_test_perc']*100,2)<br/>psi_train_test['psi']=round(psi_train_test['psi'],2)<br/>psi_train_test1=psi_train_test[['score_band','scr_train_%','scr_test_%','psi']]<br/>print(psi_train_test1)<br/>print('PSI - scr_train vs scr_test: ' + str(round(sum(psi_train_test['psi']),2)))</span></pre><figure class="ld le lf lg gt lh gh gi paragraph-image"><div class="gh gi mg"><img src="../Images/383c370bb8e842225e64056d4a26432b.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*Ye69gN9gOYJ3yJCEy-OWMw.png"/></div><p class="lo lp gj gh gi lq lr bd b be z dk translated">scr_train和scr_test之间的PSI分布结果相似</p></figure><pre class="ld le lf lg gt lu lv lw lx aw ly bi"><span id="87c5" class="lz ma iq lv b gy mb mc l md me"><strong class="lv ir"># To plot the PSI graph-</strong></span><span id="30e4" class="lz ma iq lv b gy mf mc l md me"><strong class="lv ir">from</strong>  <strong class="lv ir">matplotlib.ticker</strong> <strong class="lv ir">import</strong> PercentFormatter<br/>psi_table=psi_train_test[['score_band','scr_train_perc','scr_test_perc']]<br/>psi_table = psi_table.melt('score_band', var_name='cols',  value_name='% population')<br/>g = sns.factorplot(x="score_band", y="% population", hue='cols', data=psi_table)<br/>g.set(ylim=(0, .50))<br/>g.ax.set_title('Population Stability', size = 18 )<br/>g.ax.yaxis.set_major_formatter(PercentFormatter(1))<br/>g.savefig('PSI.png')</span></pre><figure class="ld le lf lg gt lh gh gi paragraph-image"><div class="gh gi mh"><img src="../Images/03b6c56b71dcfda99f07202b6a1b9ece.png" data-original-src="https://miro.medium.com/v2/resize:fit:872/format:webp/1*BSX7YP9osCgEtxu2shLcFw.png"/></div><p class="lo lp gj gh gi lq lr bd b be z dk translated">分数分布的图形表示</p></figure><p id="35f0" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">特性稳定性指数</strong> —这在特性/特征/变量水平上测量开发和验证/测试数据之间的差异。如果psi为琥珀色或红色，在特征水平上检查分布差异以理解导致这种变化的变量列表是很重要的。</p><p id="5328" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">一旦我们研究了模型的稳定性，下一步就是根据预测能力来评估模型的强度。使用PyCaret，在一行代码中，我们将得到一个不同的评估矩阵。让我们看看下面提到的例子。</p><pre class="ld le lf lg gt lu lv lw lx aw ly bi"><span id="92ab" class="lz ma iq lv b gy mb mc l md me">evaluate_model(gbc_custom,use_train_data= <strong class="lv ir">True</strong>) #this would give the result on the train data</span><span id="b71f" class="lz ma iq lv b gy mf mc l md me">evaluate_model(gbc_custom) #this would give the result on the test data</span><span id="fa23" class="lz ma iq lv b gy mf mc l md me"># we had saved our model as gbc_custom ( refer to the previous part)</span></pre><p id="4da4" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">曲线下面积(AUC) </strong> —该值越高，模型在区分事件和非事件方面越好。</p><figure class="ld le lf lg gt lh gh gi paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="gh gi mi"><img src="../Images/8e5984e76df5dcf10d068f96c6d3c810.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cjIFQaAW7VFAyCOrNye3-g.png"/></div></div><p class="lo lp gj gh gi lq lr bd b be z dk translated">火车的AUC。顶部有不同的选项卡，包含不同的评估指标。</p></figure><figure class="ld le lf lg gt lh gh gi paragraph-image"><div class="gh gi mj"><img src="../Images/2141e790175bb7baf45029aed75b89f9.png" data-original-src="https://miro.medium.com/v2/resize:fit:964/format:webp/1*kzy-rJAOCf9wADQtl0m9nw.png"/></div><p class="lo lp gj gh gi lq lr bd b be z dk translated">AUC测试</p></figure><figure class="ld le lf lg gt lh gh gi paragraph-image"><div class="gh gi mk"><img src="../Images/d6e91fc6c1937a8daebbc2cec411bf1a.png" data-original-src="https://miro.medium.com/v2/resize:fit:854/format:webp/1*7tNCee6D25R3Qfqf38TUNQ.png"/></div><p class="lo lp gj gh gi lq lr bd b be z dk translated">跨事件变量类别的精度/召回/F1/支持</p></figure><div class="ld le lf lg gt ab cb"><figure class="ml lh mm mn mo mp mq paragraph-image"><img src="../Images/51298c076be671d56976ff5a72572ec3.png" data-original-src="https://miro.medium.com/v2/resize:fit:810/format:webp/1*SiC8DarreJuu0-tgim6dYA.png"/></figure><figure class="ml lh mr mn mo mp mq paragraph-image"><img src="../Images/e48f5867f4e3a569d966ba53af351e6a.png" data-original-src="https://miro.medium.com/v2/resize:fit:912/format:webp/1*5GKjh0eHH04GUWsnxE1ifQ.png"/><p class="lo lp gj gh gi lq lr bd b be z dk ms di mt mu translated">训练和测试数据的混淆矩阵</p></figure></div><div class="ab cb"><figure class="ml lh mv mn mo mp mq paragraph-image"><img src="../Images/2cfb986117b2c1f1416276a2027cd9d5.png" data-original-src="https://miro.medium.com/v2/resize:fit:956/format:webp/1*8OsDSBElGVBW7aTszu0AtA.png"/></figure><figure class="ml lh mw mn mo mp mq paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><img src="../Images/f3fc14448953921cbb52a78fa7d2b945.png" data-original-src="https://miro.medium.com/v2/resize:fit:1012/format:webp/1*jx8XZyJQIf2f5XOA9bzc9A.png"/></div><p class="lo lp gj gh gi lq lr bd b be z dk mx di my mu translated">学习曲线和提升图</p></figure></div><div class="ab cb"><figure class="ml lh mz mn mo mp mq paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><img src="../Images/956c5489e5cf5c422b1dc1f8a0872b52.png" data-original-src="https://miro.medium.com/v2/resize:fit:1234/format:webp/1*1dbp4gR5V2mZ7uMoUBh6pQ.png"/></div></figure><figure class="ml lh na mn mo mp mq paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><img src="../Images/73fccad2e757f753daa837d370e70205.png" data-original-src="https://miro.medium.com/v2/resize:fit:484/format:webp/1*58VM6JaKRfW0qmJ75C8IxA.png"/></div><p class="lo lp gj gh gi lq lr bd b be z dk nb di nc mu translated">特征重要性图和模型的超参数</p></figure></div><p id="23ba" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">一旦我们研究了所有这些评估指标，下一步将是创建一个<strong class="kh ir">收益矩阵</strong>来决定客户定位的分界点。</p><pre class="ld le lf lg gt lu lv lw lx aw ly bi"><span id="865e" class="lz ma iq lv b gy mb mc l md me">def weighted_qcut(values, weights, q, **kwargs):<br/>    'Return weighted quantile cuts from a given series, values.'<br/>    if is_integer(q):<br/>        quantiles = np.linspace(0, 1, q + 1)<br/>    else:<br/>        quantiles = q<br/>    order = weights.iloc[values.argsort()].cumsum()<br/>    bins = pd.cut(order / order.iloc[-1], quantiles, **kwargs)<br/>    return bins.sort_index()</span><span id="d526" class="lz ma iq lv b gy mf mc l md me">def gains_matrix(input_gm,target):<br/>    if 'SamplingWeight' not in input_gm.columns:<br/>        input_gm['SamplingWeight'] = 1<br/>    input_gm['mevent']=input_gm[target]<br/>    input_gm['deciles'] = weighted_qcut(input_gm['score'], input_gm['SamplingWeight'], 20, labels=False)</span><span id="0f4a" class="lz ma iq lv b gy mf mc l md me">sqlcode3 = '''<br/>    select deciles, mevent, sum(samplingweight) as count<br/>    from input_gm<br/>    group by deciles, mevent<br/>    '''<br/>    gainsfreq = ps.sqldf(sqlcode3,locals())<br/>    <br/>    transpose = pd.DataFrame(gainsfreq.pivot_table(index=['deciles'], columns='mevent', aggfunc=sum, fill_value=0).reset_index())<br/>    transpose.columns = ['deciles','count_0','count_1']<br/>    <br/>    transpose.sort_values(by=['deciles'], ascending=False, inplace=True)</span><span id="42b3" class="lz ma iq lv b gy mf mc l md me">transpose['cum_0'] = transpose['count_0'].cumsum()<br/>    transpose['cum_1'] = transpose['count_1'].cumsum()<br/>    <br/>    transpose['percent_cum_0'] = (transpose['cum_0']/np.sum(transpose.count_0))*100<br/>    transpose['percent_cum_1'] = (transpose['cum_1']/np.sum(transpose.count_1))*100<br/>    <br/>    <br/>    transpose['event_rate'] = (transpose['count_1']/(transpose['count_0']+transpose['count_1']))*100<br/>    transpose['cum_event_rate'] = (transpose['cum_1']/(transpose['cum_0']+transpose['cum_1']))*100<br/>    <br/>    transpose['cum_separation'] = transpose['percent_cum_1']-transpose['percent_cum_0']<br/>    <br/>    sqlcode4 = '''<br/>    select deciles, min(score) as score<br/>    from input_gm<br/>    group by deciles<br/>    '''<br/>    score = ps.sqldf(sqlcode4,locals())<br/>    <br/>    result = pd.DataFrame(pd.merge(score, transpose , on='deciles', how='outer'))<br/>    resultn = result.sort_values('deciles', ascending=False)<br/>    resultn['score_band'] = resultn["deciles"].tolist()[::-1]<br/>    resultn['score_band'] = resultn["score_band"]+1<br/>    resultn= resultn.drop(columns=['deciles']) <br/>    return resultn</span><span id="d238" class="lz ma iq lv b gy mf mc l md me">train_gain=gains_matrix(scr_train,'default.payment.next.month')<br/>test_gain=gains_matrix(scr_test,'default.payment.next.month')</span></pre><figure class="ld le lf lg gt lh gh gi paragraph-image"><div class="gh gi nd"><img src="../Images/a7a1e65d33739721ffc598b47fcac6a5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1252/format:webp/1*eLqGUtrNOOjPcWdhf8e-5A.png"/></div><p class="lo lp gj gh gi lq lr bd b be z dk translated">训练的增益矩阵—有20个分数段</p></figure><p id="55ae" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">等级排序</strong> —在整个评分范围内，事件发生率应理想地单调递增或递减——评分越高，非违约率越高，评分越低，非违约率越低。</p><pre class="ld le lf lg gt lu lv lw lx aw ly bi"><span id="5e59" class="lz ma iq lv b gy mb mc l md me">rank_train=train_gain[['score_band','event_rate']]<br/>rank_test=test_gain[['score_band','event_rate']]</span><span id="ac5e" class="lz ma iq lv b gy mf mc l md me">rank_train=rank_train.rename(columns={'event_rate': 'train'})<br/>rank_test=rank_test.rename(columns={'event_rate': 'test'})<br/>rank_table = pd.DataFrame.merge(rank_train, rank_test,on=['score_band'],how='outer')</span><span id="d7e0" class="lz ma iq lv b gy mf mc l md me">rank_table = rank_table.melt('score_band', var_name='cols',  value_name='event_rate')<br/>g = sns.factorplot(x="score_band", y="event_rate", hue='cols', data=rank_table,grid=False).ax.set_title("Rank Ordering Comparison")<br/>from matplotlib.ticker import FuncFormatter<br/>def to_percent(y, position):<br/>    s = str(y)<br/>    return s + '%'<br/>formatter = FuncFormatter(to_percent)<br/>plt.gca().yaxis.set_major_formatter(formatter)<br/>plt.show()<br/>g.figure.savefig('Rank Orderging.png')</span></pre><figure class="ld le lf lg gt lh gh gi paragraph-image"><div class="gh gi ne"><img src="../Images/68c8765c4904243cdea836f61ef52d88.png" data-original-src="https://miro.medium.com/v2/resize:fit:936/format:webp/1*KCOwKsBhs31p1eLywQZ3CA.png"/></div><p class="lo lp gj gh gi lq lr bd b be z dk translated">培训和测试中分数范围内的事件率</p></figure><p id="5fc1" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><em class="lt">经验法则</em>:就模型实现而言，可以考虑以下阈值(但它可能因行业和用例而异):</p><div class="ld le lf lg gt ab cb"><figure class="ml lh nf mn mo mp mq paragraph-image"><img src="../Images/d403cc8563f55d82b1960993daf987f6.png" data-original-src="https://miro.medium.com/v2/resize:fit:560/format:webp/1*qWqJt8TUzsdbmmzp19Siig.png"/></figure><figure class="ml lh ng mn mo mp mq paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><img src="../Images/41fec1b4c06995c93dbd0dd175a4f30b.png" data-original-src="https://miro.medium.com/v2/resize:fit:998/format:webp/1*mL_j5oCs1fkVFapnNLh2OA.png"/></div><p class="lo lp gj gh gi lq lr bd b be z dk nh di ni mu translated">模型实施标准—经验法则</p></figure></div><p id="51db" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这就把我们带到了第2部分的结尾。在最后一部分，我们将讨论<strong class="kh ir">偏差和模型可解释性</strong>。</p></div></div>    
</body>
</html>