<html>
<head>
<title>Self-Talk: Obtain Knowledge From Text Generation Transformer Models</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">自言自语:从文本生成转换器模型中获取知识</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/self-talk-obtain-knowledge-from-text-generation-transformer-models-918277dbfc8b?source=collection_archive---------19-----------------------#2021-08-07">https://towardsdatascience.com/self-talk-obtain-knowledge-from-text-generation-transformer-models-918277dbfc8b?source=collection_archive---------19-----------------------#2021-08-07</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/5f4fe0159e2c1acbac37c395d34cc4b4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NR7_-PRnpR3QE34egs-Ihg.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">作者图片</p></figure><div class=""/><div class=""><h2 id="1ee7" class="pw-subtitle-paragraph kf jh ji bd b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw dk translated">有了变形金刚模型，谁还需要字典？</h2></div><p id="6d5f" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">文本生成转换器模型确实令人印象深刻。当OpenAI认为其名为GPT-2的模型过于危险而不能发布时，它们首次引起了公众的注意。他们不可避免地发布了这个模型，包括它的最大版本，你现在只需要几行代码就可以使用。从那以后，这些型号在尺寸和性能方面都有了很大的发展。现在，OpenAI的最新模型，称为GPT-3，可以<a class="ae lt" href="https://arxiv.org/abs/2005.14165" rel="noopener ugc nofollow" target="_blank">执行基本运算</a>并生成<a class="ae lt" href="https://www.theguardian.com/commentisfree/2020/sep/08/robot-wrote-this-article-gpt-3" rel="noopener ugc nofollow" target="_blank">现实新闻文章</a>。</p><p id="50d9" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">本文将关注文本生成转换模型的最新应用之一——知识生成。从高层次来看，这些模型相当简单；他们试图继续你提供给他们的文本。现在，如果你向模型提出一个问题呢？嗯，模型将继续文本，因此，通常会尝试回答问题。通过这样做，他们将利用他们在培训中学到的知识来产生信息。到本文结束时，您将知道如何用最少的Python代码实现最先进的人工智能模型来执行这项任务。</p><figure class="lv lw lx ly gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi lu"><img src="../Images/ec82d8e1be8bb167d942eb4c2ad437cc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*jnJUZbrAvr3oIdIt.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">作者图片</p></figure><h1 id="6246" class="lz ma ji bd mb mc md me mf mg mh mi mj ko mk kp ml kr mm ks mn ku mo kv mp mq bi translated">发现</h1><p id="47fd" class="pw-post-body-paragraph kx ky ji kz b la mr kj lc ld ms km lf lg mt li lj lk mu lm ln lo mv lq lr ls im bi translated">艾伦人工智能研究所(AI2)第一个发现了变形金刚模型的这种应用，并将其命名为自言自语。我的本科顶点项目团队独立发现了一种方法，与AI2在得知他们的论文之前提出的方法非常相似。因此，我从我的顶点项目和AI2的论文中得到了一些见解来分享。</p><h1 id="43b1" class="lz ma ji bd mb mc md me mf mg mh mi mj ko mk kp ml kr mm ks mn ku mo kv mp mq bi translated">基本实现</h1><p id="ea92" class="pw-post-body-paragraph kx ky ji kz b la mr kj lc ld ms km lf lg mt li lj lk mu lm ln lo mv lq lr ls im bi translated">在本教程中，我们将实现一个稍微简化的自言自语版本。我们将使用我自己的<a class="ae lt" href="https://github.com/EricFillion/happy-transformer" rel="noopener ugc nofollow" target="_blank">快乐变形金刚</a> Python包，它是在<a class="ae lt" href="https://github.com/huggingface/transformers" rel="noopener ugc nofollow" target="_blank">拥抱脸的变形金刚库</a>之上的一个包装器。Happy Transformer允许您用几行代码实现和训练Transformer模型——包括我们将在本教程中使用的文本生成模型。</p><h1 id="5d2e" class="lz ma ji bd mb mc md me mf mg mh mi mj ko mk kp ml kr mm ks mn ku mo kv mp mq bi translated">装置</h1><p id="f3d6" class="pw-post-body-paragraph kx ky ji kz b la mr kj lc ld ms km lf lg mt li lj lk mu lm ln lo mv lq lr ls im bi translated">首先，PyPI上提供了Happy Transformer，因此我们可以用一行代码来安装它。</p><pre class="lv lw lx ly gt mw mx my mz aw na bi"><span id="6dd3" class="nb ma ji mx b gy nc nd l ne nf">pip install happytransformer</span></pre><h1 id="f54a" class="lz ma ji bd mb mc md me mf mg mh mi mj ko mk kp ml kr mm ks mn ku mo kv mp mq bi translated">下载模型</h1><p id="006d" class="pw-post-body-paragraph kx ky ji kz b la mr kj lc ld ms km lf lg mt li lj lk mu lm ln lo mv lq lr ls im bi translated">现在，我们将从Happy Transformer中导入一个名为HappyGeneration的类来下载一个文本生成模型。</p><pre class="lv lw lx ly gt mw mx my mz aw na bi"><span id="4d56" class="nb ma ji mx b gy nc nd l ne nf">from happytransformer import HappyGeneration</span></pre><p id="4509" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">从这里，我们可以加载一个名为GPT-尼奥的GPT-3的完全开源版本。目前有三种不同尺寸的GPT-尼奥模型可以在拥抱脸的模型分销网络上买到。我们将使用第二大(或者第二小，如果你感到悲观)模型，它有1.3B个参数。这里有一个<a class="ae lt" href="https://huggingface.co/EleutherAI" rel="noopener ugc nofollow" target="_blank">链接</a>到其他型号，如果你想使用它们。</p><p id="cf15" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">要创建HappyGeneration对象，我们必须为其第一个位置参数提供模型类型，为其第二个位置参数提供模型名称。在这种情况下，型号类型为“GPT-尼奥”，型号名称为“EleutherAI/gpt-neo-1.3B”，如型号的<a class="ae lt" href="https://huggingface.co/EleutherAI/gpt-neo-1.3B" rel="noopener ugc nofollow" target="_blank">网页</a>左上方所示。</p><pre class="lv lw lx ly gt mw mx my mz aw na bi"><span id="0cb9" class="nb ma ji mx b gy nc nd l ne nf">happy_gen = HappyGeneration("GPT-NEO", "EleutherAI/gpt-neo-1.3B")</span></pre><h1 id="97c1" class="lz ma ji bd mb mc md me mf mg mh mi mj ko mk kp ml kr mm ks mn ku mo kv mp mq bi translated">文本生成算法</h1><p id="8690" class="pw-post-body-paragraph kx ky ji kz b la mr kj lc ld ms km lf lg mt li lj lk mu lm ln lo mv lq lr ls im bi translated">您可以使用不同的文本生成算法来生成文本。现在，本文的目的不是深入解释不同的文本生成算法，而是描述如何实现自言自语。在我的团队测试的四种不同的文本生成算法中，我们发现“波束搜索”算法是最有效的。这个算法是确定性的，这意味着每次运行它，都会得到相同的结果。</p><p id="e158" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">要修改设置，我们必须导入一个名为GENSettings的类。然后，我们可以修改各种参数来选择和修改我们希望使用的算法。</p><pre class="lv lw lx ly gt mw mx my mz aw na bi"><span id="bb89" class="nb ma ji mx b gy nc nd l ne nf">from happytransformer import GENSettings</span><span id="6265" class="nb ma ji mx b gy ng nd l ne nf">beam_settings = GENSettings(num_beams=5, max_length=50, no_repeat_ngram_size=3)</span></pre><p id="e6fd" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">AI2使用了一种叫做top-p抽样的算法，p值为0.5来生成答案。与波束搜索不同，该算法是不确定的，这意味着每次运行它时，它都会产生不同的文本。</p><pre class="lv lw lx ly gt mw mx my mz aw na bi"><span id="071f" class="nb ma ji mx b gy nc nd l ne nf">top_p_settings = GENSettings(do_sample=True, top_k=0, top_p=0.5, max_length=50)</span></pre><h1 id="ad55" class="lz ma ji bd mb mc md me mf mg mh mi mj ko mk kp ml kr mm ks mn ku mo kv mp mq bi translated">获取知识</h1><p id="1f63" class="pw-post-body-paragraph kx ky ji kz b la mr kj lc ld ms km lf lg mt li lj lk mu lm ln lo mv lq lr ls im bi translated">我们现在可以开始创造知识了！让我们创建一个问题，然后使用文本生成模型来回答它。为此，我们将调用happy_gen的generate()方法。</p><pre class="lv lw lx ly gt mw mx my mz aw na bi"><span id="02cd" class="nb ma ji mx b gy nc nd l ne nf">question = "What is a dog?" </span><span id="0865" class="nb ma ji mx b gy ng nd l ne nf">result = happy_gen.generate_text(question, args=beam_settings) </span><span id="eae3" class="nb ma ji mx b gy ng nd l ne nf">print(result.text)</span></pre><p id="70b9" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated"><em class="nh">结果:狗是犬科动物的一员，包括狗、狼、郊狼、狐狸和豺。狗已经被驯化了一万多年。他们首先被驯化</em></p><p id="7d79" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">还不错！现在，如果您使用top-p设置来代替，那么每次您执行推理时，都会得到一个原始的结果。这里有一个使用top-p算法的例子。</p><pre class="lv lw lx ly gt mw mx my mz aw na bi"><span id="92c7" class="nb ma ji mx b gy nc nd l ne nf">result = happy_gen.generate_text(question, args=top_p_settings) </span><span id="24bd" class="nb ma ji mx b gy ng nd l ne nf">print(result.text)</span></pre><p id="ca92" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated"><em class="nh">结果:狗是一种驯养的动物，被饲养来作为伴侣，通常是为了狩猎、守卫或工作。狗通常被称为“宠物”或“伙伴”,因为它们通常是为“T7”而饲养的</em></p><h1 id="4814" class="lz ma ji bd mb mc md me mf mg mh mi mj ko mk kp ml kr mm ks mn ku mo kv mp mq bi translated">包括上下文</h1><figure class="lv lw lx ly gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/408512617d5474298dc57c4c8dac5820.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*LhX343C1gqtkGFbP.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">作者图片</p></figure><p id="069a" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">如果可能的话，我建议你在你的输入中添加额外的上下文。例如，假设您要求模型定义一个恰好是同音异义词的单词。然后，该模型可能会产生一个不适合您的上下文的单词含义的定义。下面是在问题前添加和不添加上下文的情况下生成知识的两个示例。</p><pre class="lv lw lx ly gt mw mx my mz aw na bi"><span id="0d6f" class="nb ma ji mx b gy nc nd l ne nf">input_no_context = "What is a library?" </span><span id="f930" class="nb ma ji mx b gy ng nd l ne nf">result_no_context = happy_gen.generate_text(input_no_context, args=beam_settings) </span><span id="a7d3" class="nb ma ji mx b gy ng nd l ne nf">print(result_no_context.text)</span></pre><p id="5bb8" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">结果:图书馆是书籍和其他材料的集合，可用于各种目的，如研究、教学或娱乐。图书馆和书店的区别是什么？书店是一个</p><pre class="lv lw lx ly gt mw mx my mz aw na bi"><span id="fada" class="nb ma ji mx b gy nc nd l ne nf">input_with_context = "Happy Transformer is an open-souce Python package. What is a library?" </span><span id="8321" class="nb ma ji mx b gy ng nd l ne nf">result_with_context = happy_gen.generate_text(input_with_context, args=beam_settings) </span><span id="464e" class="nb ma ji mx b gy ng nd l ne nf">print(result_with_context.text)</span></pre><p id="e825" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated"><em class="nh">结果:库是可重用代码片段的集合，它们可以一起用来解决一个共同的问题。例如，一个库可以是一组在程序中一起使用来执行特定任务的函数。</em></p><p id="bce2" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">如您所见，通过提供上下文，我们帮助模型缩小了“库”的含义所以，我建议你在进行自我对话时，在你的输入中加入上下文。现在，我们来讨论一下如何自动生成问题。</p><h1 id="395d" class="lz ma ji bd mb mc md me mf mg mh mi mj ko mk kp ml kr mm ks mn ku mo kv mp mq bi translated">自动生成问题</h1><p id="e781" class="pw-post-body-paragraph kx ky ji kz b la mr kj lc ld ms km lf lg mt li lj lk mu lm ln lo mv lq lr ls im bi translated">假设给你一段文字，你希望通过问语言模型问题来自动生成额外的背景知识。你必须理解潜在的文本，才能提出语法正确且相关的质量问题。可以应用文本生成转换器模型来解决这个问题。</p><p id="c0d5" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">我的顶点项目团队使用命名实体识别来识别上下文中的名词，然后为每个名词创建一个问题。例如，如果单词“香蕉”在文本中，那么问题“什么是香蕉？”会产生。我们发现这种方法是有效的，但AI2的团队提出了一种更复杂的方法，使用文本生成模型来生成问题。</p><p id="f321" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">我们将讨论如何使用AI2提出的方法来生成问题。他们精心制作了提示，然后使用文本生成模型来继续提示以生成问题。例如，他们将自言自语应用于一项名为Winograd Schema Challenge的挑战，简单来说，就是预测一个模糊代词所指的名词。因此，例如，给定句子“锤子不适合工具箱，因为<strong class="kz jj">它</strong>太大了”，模型必须确定<strong class="kz jj">它</strong>是指“锤子”还是“工具箱”以下是AI2在挑战中使用的部分提示。[1]</p><figure class="lv lw lx ly gt iv"><div class="bz fp l di"><div class="ni nj l"/></div></figure><p id="a8fd" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">然后，他们将提示附加到上下文中，并让模型产生文本。通过产生文本，模型将潜在地产生与上下文相关的可行问题。下面是这个过程的代码示例。我们将使用p值为0.2的top-p采样，最多生成6个令牌——正如AI2在论文中所建议的那样。</p><pre class="lv lw lx ly gt mw mx my mz aw na bi"><span id="39ea" class="nb ma ji mx b gy nc nd l ne nf">question_generation_settings = GENSettings(do_sample=True, top_k=0, top_p=0.2, max_length=6)</span><span id="a1b3" class="nb ma ji mx b gy ng nd l ne nf">context = "The hammer did not fit into the toolbox because it is too big"</span><span id="7d04" class="nb ma ji mx b gy ng nd l ne nf">prompt = "What is the definition of"</span><span id="b498" class="nb ma ji mx b gy ng nd l ne nf">input = context + prompt</span><span id="ebf0" class="nb ma ji mx b gy ng nd l ne nf">q_g_result = happy_gen.generate_text(input, args=question_generation_settings)</span><span id="2737" class="nb ma ji mx b gy ng nd l ne nf">print(q_g_result.text)</span></pre><p id="cf2d" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated"><em class="nh">结果:一个工具箱？答:</em></p><p id="01cc" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">现在，我们可以用下面一行代码隔离生成的问题。</p><pre class="lv lw lx ly gt mw mx my mz aw na bi"><span id="7c16" class="nb ma ji mx b gy nc nd l ne nf"># Get the location of the question mark if it exists.<br/># Results in -1 if not present <br/>q_m_location = q_g_result.text.find("?")</span><span id="0957" class="nb ma ji mx b gy ng nd l ne nf">full_question= ""</span><span id="3c0b" class="nb ma ji mx b gy ng nd l ne nf">question_ending = ""</span><span id="a40d" class="nb ma ji mx b gy ng nd l ne nf">if q_m_location != -1:<br/>   question_ending = q_g_result.text[:q_m_location+1]<br/>   full_question = question_prompt + question_ending<br/>   print(full_question)<br/>   <br/>else:<br/>  print("question not generated. Try a different prompt.")</span></pre><p id="82cd" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated"><em class="nh">结果:奖杯的定义是什么？</em></p><h1 id="eb5c" class="lz ma ji bd mb mc md me mf mg mh mi mj ko mk kp ml kr mm ks mn ku mo kv mp mq bi translated">应答前缀</h1><p id="3b6f" class="pw-post-body-paragraph kx ky ji kz b la mr kj lc ld ms km lf lg mt li lj lk mu lm ln lo mv lq lr ls im bi translated">AI2为每个问题前缀手动创建了一个答案前缀，以帮助模型回答问题。下面的图表显示了问题和答案前缀的各种组合。[1]</p><figure class="lv lw lx ly gt iv"><div class="bz fp l di"><div class="ni nj l"/></div></figure><p id="ac71" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">请注意，每个回答提示都有一个下划线来表示其主题。我们可以用下面的代码很容易地提取出主语，因为主语只是问题的结尾而不是问号。</p><pre class="lv lw lx ly gt mw mx my mz aw na bi"><span id="efab" class="nb ma ji mx b gy nc nd l ne nf">subject = question_ending[:-1] </span><span id="8ec1" class="nb ma ji mx b gy ng nd l ne nf">print(subject)</span></pre><p id="a81e" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated"><em class="nh">结果:一个工具箱</em></p><p id="e9ce" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">提示可以分为三个部分:主题前的文本、主题和主题后的文本。因此，这些组件可以按如下所示进行组合。</p><pre class="lv lw lx ly gt mw mx my mz aw na bi"><span id="6d5b" class="nb ma ji mx b gy nc nd l ne nf">answer_prefix = " The definition of" + subject + " is" </span><span id="53a1" class="nb ma ji mx b gy ng nd l ne nf">print(answer_prefix)</span></pre><p id="5b1f" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated"><em class="nh">结果:工具箱的定义是</em></p><h1 id="4c93" class="lz ma ji bd mb mc md me mf mg mh mi mj ko mk kp ml kr mm ks mn ku mo kv mp mq bi translated">把所有东西放在一起</h1><p id="002b" class="pw-post-body-paragraph kx ky ji kz b la mr kj lc ld ms km lf lg mt li lj lk mu lm ln lo mv lq lr ls im bi translated">我们现在已经了解了为任意文本生成背景信息所需的一切。下面是将我们创建的各种组件组合起来形成文本生成模型的最终输入的最后一个例子。</p><pre class="lv lw lx ly gt mw mx my mz aw na bi"><span id="2229" class="nb ma ji mx b gy nc nd l ne nf">final_input = context + full_question + answer_prefix </span><span id="af76" class="nb ma ji mx b gy ng nd l ne nf">print(final_input)</span></pre><p id="e4c6" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated"><em class="nh">结果:</em>锤子太大，放不进工具箱。工具箱的定义是什么？工具箱的定义是</p><p id="6f2c" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">AI2建议使用p值为0.5的top-p采样来生成10个令牌。让我们来定义这些设置。</p><pre class="lv lw lx ly gt mw mx my mz aw na bi"><span id="0261" class="nb ma ji mx b gy nc nd l ne nf">answer_generation_settings = GENSettings(do_sample=True, top_k=0, top_p=0.5, max_length=10)</span></pre><p id="5ee3" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">我们现在拥有了生成最终结果所需的一切。</p><pre class="lv lw lx ly gt mw mx my mz aw na bi"><span id="327e" class="nb ma ji mx b gy nc nd l ne nf">answer_suffix = happy_gen.generate_text(final_input, args=answer_generation_settings).text</span><span id="9d2d" class="nb ma ji mx b gy ng nd l ne nf">print(answer_suffix)</span></pre><p id="8fc2" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated"><em class="nh">结果:一个存放和运输工具的盒子。是</em></p><p id="043c" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">最后一步是结合我们的前缀和后缀的答案。</p><pre class="lv lw lx ly gt mw mx my mz aw na bi"><span id="1091" class="nb ma ji mx b gy nc nd l ne nf">final_result = answer_prefix + answer_suffix </span><span id="1dad" class="nb ma ji mx b gy ng nd l ne nf">print(final_result)</span></pre><p id="4b1a" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated"><em class="nh">结果:工具箱的定义是存放和运输工具的盒子。是</em></p><p id="1f3f" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">那挺好的！</p><p id="a1f8" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated"><em class="nh">注:您可能希望应用基础数据后处理，以便仅提取第一句话。</em></p><h1 id="6349" class="lz ma ji bd mb mc md me mf mg mh mi mj ko mk kp ml kr mm ks mn ku mo kv mp mq bi translated">差异</h1><p id="1319" class="pw-post-body-paragraph kx ky ji kz b la mr kj lc ld ms km lf lg mt li lj lk mu lm ln lo mv lq lr ls im bi translated">本文概述的方法和AI2概述的方法之间的主要区别是每个上下文生成的背景信息量。对于每个语境，他们至少使用了5个问题前缀。然后，对于每个问题前缀，他们生成5个问题。最后，对于每个问题，他们都有10个答案。这意味着，对于每一个语境，他们至少产生了250个答案——这是大量的背景信息！</p><h1 id="b666" class="lz ma ji bd mb mc md me mf mg mh mi mj ko mk kp ml kr mm ks mn ku mo kv mp mq bi translated">结论</h1><p id="a838" class="pw-post-body-paragraph kx ky ji kz b la mr kj lc ld ms km lf lg mt li lj lk mu lm ln lo mv lq lr ls im bi translated">就是这样！您刚刚学习了如何使用Transformer模型生成背景信息。我相信这项技术可以进一步改进并应用于其他任务。例如，在生成问题和答案时，也许可以应用微调来帮助提高模型的性能。此外，我最近发表了一篇文章,概述了一个你可以自由追求的与自我对话相关的研究想法。我期待着阅读NLP研究人员如何改进和应用自言自语！</p><h1 id="c918" class="lz ma ji bd mb mc md me mf mg mh mi mj ko mk kp ml kr mm ks mn ku mo kv mp mq bi translated">资源</h1><p id="249b" class="pw-post-body-paragraph kx ky ji kz b la mr kj lc ld ms km lf lg mt li lj lk mu lm ln lo mv lq lr ls im bi translated"><a class="ae lt" href="https://www.youtube.com/channel/UC7-EWrr8YdcQgPPk76OiUVw?sub_confirmation=1" rel="noopener ugc nofollow" target="_blank">订阅</a>我的YouTube频道或注册我的<a class="ae lt" href="https://www.vennify.ai/" rel="noopener ugc nofollow" target="_blank">时事通讯</a>以获取更多关于自言自语的内容！</p><p id="6f34" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">本教程中使用的<a class="ae lt" href="https://colab.research.google.com/drive/1_JfIY1xkks3v0-9mx0CbB27oeaFDk7ye?usp=sharing" rel="noopener ugc nofollow" target="_blank">代码</a></p><h1 id="69d1" class="lz ma ji bd mb mc md me mf mg mh mi mj ko mk kp ml kr mm ks mn ku mo kv mp mq bi translated">参考</h1><p id="0021" class="pw-post-body-paragraph kx ky ji kz b la mr kj lc ld ms km lf lg mt li lj lk mu lm ln lo mv lq lr ls im bi translated">[1] V. Shwartz，P. West，R. Bras，C. Bhagavatula1和Y. Choi，<a class="ae lt" href="https://arxiv.org/abs/2004.05483" rel="noopener ugc nofollow" target="_blank">用自言自语进行无监督常识性问题回答</a> (2020)，EMNLP 2020</p><h1 id="ed48" class="lz ma ji bd mb mc md me mf mg mh mi mj ko mk kp ml kr mm ks mn ku mo kv mp mq bi translated">归因</h1><p id="1279" class="pw-post-body-paragraph kx ky ji kz b la mr kj lc ld ms km lf lg mt li lj lk mu lm ln lo mv lq lr ls im bi translated"><a class="ae lt" href="https://www.linkedin.com/in/ted-brownlow-7043ab168/" rel="noopener ugc nofollow" target="_blank"> Ted Brownlow </a>、<a class="ae lt" href="https://www.linkedin.com/in/will-macdonald-211923175/" rel="noopener ugc nofollow" target="_blank"> Will Macdonald </a>和<a class="ae lt" href="https://www.linkedin.com/in/ryleywells78rs88/" rel="noopener ugc nofollow" target="_blank"> Ryley Wells </a>都是我的顶点项目团队的成员。</p></div><div class="ab cl nk nl hx nm" role="separator"><span class="nn bw bk no np nq"/><span class="nn bw bk no np nq"/><span class="nn bw bk no np"/></div><div class="im in io ip iq"><p id="b73a" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated"><em class="nh">原载于2021年8月7日</em><a class="ae lt" href="https://www.vennify.ai/self-talk-transformer-model/" rel="noopener ugc nofollow" target="_blank"><em class="nh">https://www . vennify . ai</em></a><em class="nh">。</em></p></div></div>    
</body>
</html>