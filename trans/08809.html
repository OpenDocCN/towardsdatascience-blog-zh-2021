<html>
<head>
<title>Sentiment Analysis with Deep Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">深度学习的情感分析</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/how-to-train-a-deep-learning-sentiment-analysis-model-4716c946c2ea?source=collection_archive---------6-----------------------#2021-08-14">https://towardsdatascience.com/how-to-train-a-deep-learning-sentiment-analysis-model-4716c946c2ea?source=collection_archive---------6-----------------------#2021-08-14</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="55b2" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">如何训练自己的高性能情绪分析模型</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/5ace650d63c3062be352bde818911b2a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*yzLt_DN5_mpdm-tW"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">照片由<a class="ae kv" href="https://unsplash.com/@pietrozj?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Pietro Jeng </a>在<a class="ae kv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><h1 id="291a" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">目标</h1><p id="4446" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">情感分析是自然语言处理中的一种技术，用于识别与文本相关联的情感。情感分析的常见用例包括监控客户在社交媒体上的反馈、品牌和活动监控。</p><p id="e0d8" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">在本文中，我们将探讨如何利用预先训练好的HuggingFace模型，在自定义数据集上训练自己的情感分析模型。我们还将研究如何在CPU和GPU环境中高效地对微调后的模型执行单次和批量预测。如果您正在寻找一个开箱即用的情感分析模型，请查看我以前的文章<a class="ae kv" href="https://medium.com/@edwin.tan/sentiment-analysis-in-python-with-3-lines-of-code-9382a649c23d" rel="noopener">如何用python </a>执行情感分析，其中只有3行代码。</p><h1 id="7007" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">装置</h1><pre class="kg kh ki kj gt mp mq mr ms aw mt bi"><span id="b904" class="mu kx iq mq b gy mv mw l mx my">pip install transformers<br/>pip install fast_ml==3.68<br/>pip install datasets</span></pre><h1 id="6d7d" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">导入包</h1><pre class="kg kh ki kj gt mp mq mr ms aw mt bi"><span id="ff76" class="mu kx iq mq b gy mv mw l mx my">import numpy as np<br/>import pandas as pd<br/>from fast_ml.model_development import train_valid_test_split<br/>from transformers import Trainer, TrainingArguments, AutoConfig, AutoTokenizer, AutoModelForSequenceClassification<br/>import torch<br/>from torch import nn<br/>from torch.nn.functional import softmax<br/>from sklearn.metrics import classification_report<br/>from sklearn.preprocessing import LabelEncoder<br/>import datasets</span></pre><p id="8289" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">启用GPU加速器(如果可用)。</p><pre class="kg kh ki kj gt mp mq mr ms aw mt bi"><span id="3406" class="mu kx iq mq b gy mv mw l mx my">DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")<br/>print (f'Device Availble: {DEVICE}')</span></pre><h1 id="8ab4" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">数据准备</h1><p id="1a6c" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">我们将使用一个电子商务数据集，其中包含文本评论和女装评级。</p><pre class="kg kh ki kj gt mp mq mr ms aw mt bi"><span id="0439" class="mu kx iq mq b gy mv mw l mx my">df = pd.read_csv('/kaggle/input/womens-ecommerce-clothing-reviews/Womens Clothing E-Commerce Reviews.csv')<br/>df.drop(columns = ['Unnamed: 0'], inplace = True)<br/>df.head()</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mz"><img src="../Images/ae222bb441826392eede4a67dfb305ad.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*ytZSE45JnP5elQSW"/></div></div></figure><p id="707f" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">我们只对<code class="fe na nb nc mq b">Review Text</code>和<code class="fe na nb nc mq b">Rating</code>列感兴趣。<code class="fe na nb nc mq b">Review Text</code>列用作模型的输入变量，而<code class="fe na nb nc mq b">Rating</code>列是我们的目标变量，其值从1(最不利)到5(最有利)。</p><p id="dc38" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">为了清楚起见，让我们在每个整数评级后面附加“星”或“星”。</p><pre class="kg kh ki kj gt mp mq mr ms aw mt bi"><span id="33c7" class="mu kx iq mq b gy mv mw l mx my">df_reviews = df.loc[:, ['Review Text', 'Rating']].dropna()<br/>df_reviews['Rating'] = df_reviews['Rating'].apply(lambda x: f'{x} Stars' if x != 1 else f'{x} Star')</span></pre><p id="8d73" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">这是现在数据的样子，1，2，3，4，5颗星是我们的类标签。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nd"><img src="../Images/0a9a1d35b6c742b8ad7b22f0f9268062.png" data-original-src="https://miro.medium.com/v2/resize:fit:678/0*yxn9i0tcsDGsjxZF"/></div></figure><p id="0a8a" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">让我们使用Sklearn的<code class="fe na nb nc mq b">LabelEncoder</code>对评级进行编码。</p><pre class="kg kh ki kj gt mp mq mr ms aw mt bi"><span id="553c" class="mu kx iq mq b gy mv mw l mx my">le = LabelEncoder()<br/>df_reviews['Rating'] = le.fit_transform(df_reviews['Rating'])<br/>df_reviews.head()</span></pre><p id="e2ac" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">请注意，<code class="fe na nb nc mq b">Rating</code>列已经从文本转换为整数列。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ne"><img src="../Images/efd319bdc5d2bb60dde697f201e30c7b.png" data-original-src="https://miro.medium.com/v2/resize:fit:674/0*jXCrTOa-EkuP11lQ"/></div></figure><p id="3027" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated"><code class="fe na nb nc mq b">Rating</code>栏中的数字范围从0到4。这些是将用于训练模型的类标签的类id。每个类id对应一个等级。</p><pre class="kg kh ki kj gt mp mq mr ms aw mt bi"><span id="549f" class="mu kx iq mq b gy mv mw l mx my">print (le.classes_)</span><span id="9eb8" class="mu kx iq mq b gy nf mw l mx my">&gt;&gt; ['1 Star' '2 Stars' '3 Stars' **'4 Stars'** '5 Stars']</span></pre><p id="6e1d" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">列表的位置索引是类id (0到4 ),该位置的值是原始评级。例如，在位置号3，类别id是“3 ”,它对应于类别标签“4星”。</p><p id="10a7" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">让我们将数据分别按照80%、10%和10%的比例拆分为训练、验证和测试。</p><pre class="kg kh ki kj gt mp mq mr ms aw mt bi"><span id="0545" class="mu kx iq mq b gy mv mw l mx my">(train_texts, train_labels,<br/> val_texts, val_labels,<br/> test_texts, test_labels) = train_valid_test_split(df_reviews, target = 'Rating', train_size=0.8, valid_size=0.1, test_size=0.1)</span></pre><p id="33e5" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">将熊猫系列的评论文本转换成句子列表。</p><pre class="kg kh ki kj gt mp mq mr ms aw mt bi"><span id="b8ab" class="mu kx iq mq b gy mv mw l mx my">train_texts = train_texts['Review Text'].to_list()<br/>train_labels = train_labels.to_list()<br/>val_texts = val_texts['Review Text'].to_list()<br/>val_labels = val_labels.to_list()<br/>test_texts = test_texts['Review Text'].to_list()<br/>test_labels = test_labels.to_list()</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ng"><img src="../Images/c7edc7c6d68ac4e8946b544baa21e8b7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*5zrf4UaQtt1FucWL"/></div></div></figure><p id="d185" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">创建一个<code class="fe na nb nc mq b">DataLoader</code>类，用于在训练和推理阶段处理和加载数据。</p><pre class="kg kh ki kj gt mp mq mr ms aw mt bi"><span id="5907" class="mu kx iq mq b gy mv mw l mx my">class DataLoader(torch.utils.data.Dataset):<br/>    def __init__(self, sentences=None, labels=None):<br/>        self.sentences = sentences<br/>        self.labels = labels<br/>        self.tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')<br/>        <br/>        if bool(sentences):<br/>            self.encodings = self.tokenizer(self.sentences,<br/>                                            truncation = True,<br/>                                            padding = True)<br/>        <br/>    def __getitem__(self, idx):<br/>        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}<br/>        <br/>        if self.labels == None:<br/>            item['labels'] = None<br/>        else:<br/>            item['labels'] = torch.tensor(self.labels[idx])<br/>        return item</span><span id="1e3f" class="mu kx iq mq b gy nf mw l mx my">    def __len__(self):<br/>        return len(self.sentences)<br/>    <br/>    <br/>    def encode(self, x):<br/>        return self.tokenizer(x, return_tensors = 'pt').to(DEVICE)</span></pre><p id="b17d" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">让我们来看看<code class="fe na nb nc mq b">DataLoader</code>的运行情况。</p><pre class="kg kh ki kj gt mp mq mr ms aw mt bi"><span id="0342" class="mu kx iq mq b gy mv mw l mx my">train_dataset = DataLoader(train_texts, train_labels)<br/>val_dataset = DataLoader(val_texts, val_labels)<br/>test_dataset = DataLoader(test_texts, test_labels)</span></pre><p id="6e6c" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated"><code class="fe na nb nc mq b">DataLoader</code>初始化一个预训练的标记器，并对输入句子进行编码。我们可以通过使用<code class="fe na nb nc mq b">__getitem__</code>函数从<code class="fe na nb nc mq b">DataLoader</code>中获得一条记录。下面是输入句子进行标记化后的结果。</p><pre class="kg kh ki kj gt mp mq mr ms aw mt bi"><span id="7639" class="mu kx iq mq b gy mv mw l mx my">print (train_dataset.__getitem__(0))</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nh"><img src="../Images/32739ae8478489dbcca919f19127f7e9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*b0kZRfly8ctAQlK5"/></div></div></figure><p id="931b" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">输出数据是一个由3个键值对组成的字典</p><ul class=""><li id="bb0d" class="ni nj iq lq b lr mk lu ml lx nk mb nl mf nm mj nn no np nq bi translated"><code class="fe na nb nc mq b">input_ids</code>:这包含一个整数张量，其中每个整数代表原始句子中的单词。<code class="fe na nb nc mq b">tokenizer</code>步骤将单个单词转换成由整数表示的符号。第一个记号<code class="fe na nb nc mq b">101</code>是句子的开始记号，而<code class="fe na nb nc mq b">102</code>记号是句子的结束记号。请注意，有许多尾随零，这是由于在<code class="fe na nb nc mq b">tokenizer</code>步骤中应用于句子的填充。</li><li id="c901" class="ni nj iq lq b lr nr lu ns lx nt mb nu mf nv mj nn no np nq bi translated"><code class="fe na nb nc mq b">attention_mask</code>:这是一个二进制值数组。<code class="fe na nb nc mq b">attention_mask</code>的每个位置对应<code class="fe na nb nc mq b">input_ids</code>中相同位置的一个令牌。<code class="fe na nb nc mq b">1</code>表示应该关注给定位置的令牌，<code class="fe na nb nc mq b">0</code>表示给定位置的令牌是填充值。</li><li id="69fa" class="ni nj iq lq b lr nr lu ns lx nt mb nu mf nv mj nn no np nq bi translated"><code class="fe na nb nc mq b">labels</code>:这是目标标签</li></ul><h1 id="d99d" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">定义评估指标</h1><p id="9842" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">我们希望在培训阶段定期对模型性能进行评估。为此，我们需要一个度量计算函数，它接受一个元组<code class="fe na nb nc mq b">(prediction, label)</code>作为参数，并返回一个度量字典:<code class="fe na nb nc mq b">{'metric1':value1,</code> metric2 <code class="fe na nb nc mq b">:value2}</code>。</p><pre class="kg kh ki kj gt mp mq mr ms aw mt bi"><span id="6d9a" class="mu kx iq mq b gy mv mw l mx my">f1 = datasets.load_metric('f1')<br/>accuracy = datasets.load_metric('accuracy')<br/>precision = datasets.load_metric('precision')<br/>recall = datasets.load_metric('recall')</span><span id="0ec5" class="mu kx iq mq b gy nf mw l mx my">def compute_metrics(eval_pred):<br/>    metrics_dict = {}<br/>    predictions, labels = eval_pred<br/>    predictions = np.argmax(predictions, axis=1)<br/>    <br/>    metrics_dict.update(f1.compute(predictions = predictions, references = labels, average = 'macro'))<br/>    metrics_dict.update(accuracy.compute(predictions = predictions, references = labels))<br/>    metrics_dict.update(precision.compute(predictions = predictions, references = labels, average = 'macro'))<br/>    metrics_dict.update(recall.compute(predictions = predictions, references = labels, average = 'macro'))</span><span id="b93a" class="mu kx iq mq b gy nf mw l mx my">    return metrics_dict</span></pre><h1 id="35ce" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">培养</h1><p id="1210" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">接下来，我们从预训练的检查点配置实例化一个<code class="fe na nb nc mq b">distilbert-base-uncased</code>模型。</p><pre class="kg kh ki kj gt mp mq mr ms aw mt bi"><span id="1e07" class="mu kx iq mq b gy mv mw l mx my">id2label = {idx:label for idx, label in enumerate(le.classes_)}<br/>label2id = {label:idx for idx, label in enumerate(le.classes_)}</span><span id="824e" class="mu kx iq mq b gy nf mw l mx my">config = AutoConfig.from_pretrained('distilbert-base-uncased',<br/>                                    num_labels = 5,<br/>                                    id2label = id2label,<br/>                                    label2id = label2id)</span><span id="500c" class="mu kx iq mq b gy nf mw l mx my">model = AutoModelForSequenceClassification.from_config(config)</span></pre><ul class=""><li id="96a3" class="ni nj iq lq b lr mk lu ml lx nk mb nl mf nm mj nn no np nq bi translated"><code class="fe na nb nc mq b">num_labels</code>:类别数</li><li id="0e2b" class="ni nj iq lq b lr nr lu ns lx nt mb nu mf nv mj nn no np nq bi translated"><code class="fe na nb nc mq b">id2label</code>:将类别id映射到类别标签的字典<code class="fe na nb nc mq b">{0: '1 Star', 1: '2 Stars', 2: '3 Stars', 3: '4 Stars', 4: '5 Stars'}</code></li><li id="12d5" class="ni nj iq lq b lr nr lu ns lx nt mb nu mf nv mj nn no np nq bi translated"><code class="fe na nb nc mq b">label2id</code>:将类别标签映射到类别id的映射字典<code class="fe na nb nc mq b">{'1 Star': 0, '2 Stars': 1, '3 Stars': 2, '4 Stars': 3, '5 Stars': 4}</code></li></ul><p id="ee75" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">让我们检查一下模型配置。<code class="fe na nb nc mq b">id2label</code>和<code class="fe na nb nc mq b">label2id</code>字典已经合并到配置中。我们可以在推理过程中从模型的配置中检索这些字典，以便为预测的类id找出相应的类标签。</p><pre class="kg kh ki kj gt mp mq mr ms aw mt bi"><span id="d452" class="mu kx iq mq b gy mv mw l mx my">print (config)</span><span id="7ff0" class="mu kx iq mq b gy nf mw l mx my">&gt;&gt; DistilBertConfig {<br/>  "activation": "gelu",<br/>  "architectures": [<br/>    "DistilBertForMaskedLM"<br/>  ],<br/>  "attention_dropout": 0.1,<br/>  "dim": 768,<br/>  "dropout": 0.1,<br/>  "hidden_dim": 3072,<br/>  "id2label": {<br/>    "0": "1 Star",<br/>    "1": "2 Stars",<br/>    "2": "3 Stars",<br/>    "3": "4 Stars",<br/>    "4": "5 Stars"<br/>  },<br/>  "initializer_range": 0.02,<br/>  "label2id": {<br/>    "1 Star": 0,<br/>    "2 Stars": 1,<br/>    "3 Stars": 2,<br/>    "4 Stars": 3,<br/>    "5 Stars": 4<br/>  },<br/>  "max_position_embeddings": 512,<br/>  "model_type": "distilbert",<br/>  "n_heads": 12,<br/>  "n_layers": 6,<br/>  "pad_token_id": 0,<br/>  "qa_dropout": 0.1,<br/>  "seq_classif_dropout": 0.2,<br/>  "sinusoidal_pos_embds": false,<br/>  "tie_weights_": true,<br/>  "transformers_version": "4.6.1",<br/>  "vocab_size": 30522<br/>}</span></pre><p id="9d35" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">我们还可以使用以下方法来检查模型架构</p><pre class="kg kh ki kj gt mp mq mr ms aw mt bi"><span id="0876" class="mu kx iq mq b gy mv mw l mx my">print (model)</span></pre><p id="dbcc" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">设置训练参数。</p><pre class="kg kh ki kj gt mp mq mr ms aw mt bi"><span id="e1fc" class="mu kx iq mq b gy mv mw l mx my">training_args = TrainingArguments(<br/>    output_dir='/kaggle/working/results',<br/>    num_train_epochs=10,<br/>    per_device_train_batch_size=64,<br/>    per_device_eval_batch_size=64,<br/>    warmup_steps=500,<br/>    weight_decay=0.05,<br/>    report_to='none',<br/>    evaluation_strategy='steps',<br/>    logging_dir='/kagge/working/logs',<br/>    logging_steps=50)</span></pre><ul class=""><li id="e89c" class="ni nj iq lq b lr mk lu ml lx nk mb nl mf nm mj nn no np nq bi translated"><code class="fe na nb nc mq b">report_to</code>支持将训练工件和结果记录到mlflow、tensorboard、azure_ml等平台</li><li id="9808" class="ni nj iq lq b lr nr lu ns lx nt mb nu mf nv mj nn no np nq bi translated"><code class="fe na nb nc mq b">per_device_train_batch_size</code>是训练期间每个TPU/GPU/CPU的批量大小。如果您的设备面临内存不足的问题，请降低此值</li><li id="b013" class="ni nj iq lq b lr nr lu ns lx nt mb nu mf nv mj nn no np nq bi translated"><code class="fe na nb nc mq b">per_device_eval_batch_size</code>是评估期间每个TPU/GPU/CPU的批量大小。如果您的设备面临内存不足的问题，请降低此值</li><li id="df46" class="ni nj iq lq b lr nr lu ns lx nt mb nu mf nv mj nn no np nq bi translated"><code class="fe na nb nc mq b">logging_step</code>确定培训期间进行指标评估的频率</li></ul><p id="5dde" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">实例化<code class="fe na nb nc mq b">Trainer</code>。在引擎盖下，<code class="fe na nb nc mq b">Trainer</code>基于给定的训练参数、模型、数据集和指标运行训练和评估循环。</p><pre class="kg kh ki kj gt mp mq mr ms aw mt bi"><span id="e8a0" class="mu kx iq mq b gy mv mw l mx my">trainer = Trainer(<br/>    model=model,<br/>    args=training_args,<br/>    train_dataset=train_dataset,<br/>    eval_dataset=val_dataset,<br/>    compute_metrics=compute_metrics)</span></pre><p id="ce81" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">开始训练吧！</p><pre class="kg kh ki kj gt mp mq mr ms aw mt bi"><span id="c763" class="mu kx iq mq b gy mv mw l mx my">trainer.train()</span></pre><p id="44d7" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">每50步进行一次评估。我们可以通过改变<code class="fe na nb nc mq b">TrainingArguments</code>中的<code class="fe na nb nc mq b">logging_steps</code>参数来改变求值的间隔。除了默认的训练和验证损失指标之外，我们还获得了之前在<code class="fe na nb nc mq b">compute_metric</code>函数中定义的额外指标。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nw"><img src="../Images/420392b17c4b541944b2354350aa6b97.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*bFkJuC8q0huoj2Kx"/></div></div></figure><h1 id="8fb9" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">估价</h1><p id="a315" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">让我们在测试集上评估我们的训练。</p><pre class="kg kh ki kj gt mp mq mr ms aw mt bi"><span id="5e6b" class="mu kx iq mq b gy mv mw l mx my">eval_results = trainer.predict(test_dataset)</span></pre><p id="80fc" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated"><code class="fe na nb nc mq b">Trainer</code>的<code class="fe na nb nc mq b">predict</code>函数返回3项:</p><ol class=""><li id="fb7f" class="ni nj iq lq b lr mk lu ml lx nk mb nl mf nm mj nx no np nq bi translated">原始预测分数的数组</li></ol><pre class="kg kh ki kj gt mp mq mr ms aw mt bi"><span id="31bd" class="mu kx iq mq b gy mv mw l mx my">print (test_results.predictions)</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ny"><img src="../Images/48ec66dadc58fe8d1ea94f3721de6678.png" data-original-src="https://miro.medium.com/v2/resize:fit:922/0*EKzTfiEaBTjwU2Qo"/></div></div></figure><p id="ce99" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">2.地面真实标签id</p><pre class="kg kh ki kj gt mp mq mr ms aw mt bi"><span id="1248" class="mu kx iq mq b gy mv mw l mx my">print (test_results.label_ids)<br/><br/>&gt;&gt; [1 1 4 ... 4 3 1]</span></pre><p id="432c" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">3.韵律学</p><pre class="kg kh ki kj gt mp mq mr ms aw mt bi"><span id="3d84" class="mu kx iq mq b gy mv mw l mx my">print (test_results.metrics)<br/><br/>&gt;&gt; {'test_loss': 0.9638910293579102,<br/>		'test_f1': 0.28503729426950286,<br/>		'test_accuracy': 0.5982339955849889,<br/>		'test_precision': 0.2740061405117546,<br/>		'test_recall': 0.30397183356136337,<br/>		'test_runtime': 5.7367,<br/>		'test_samples_per_second': 394.826,<br/>		'test_mem_cpu_alloc_delta': 0,<br/>		'test_mem_gpu_alloc_delta': 0,<br/>		'test_mem_cpu_peaked_delta': 0,<br/>		'test_mem_gpu_peaked_delta': 348141568}</span></pre><p id="1126" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">模型预测函数输出未标准化的概率得分。为了找到类别概率，我们对未标准化的分数取一个软最大值。具有最高类别概率的类别被认为是预测类别。我们可以通过取类概率的argmax来找到它。我们之前存储在模型配置中的<code class="fe na nb nc mq b">id2label</code>属性可用于将类别id (0-4)映射到类别标签(1星，2星..).</p><pre class="kg kh ki kj gt mp mq mr ms aw mt bi"><span id="4f69" class="mu kx iq mq b gy mv mw l mx my">label2id_mapper = model.config.id2label<br/>proba = softmax(torch.from_numpy(test_results.predictions))<br/>pred = [label2id_mapper[i] for i in torch.argmax(proba, dim = -1).numpy()]<br/>actual = [label2id_mapper[i] for i in test_results.label_ids]</span></pre><p id="6af9" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">我们使用Sklearn的<code class="fe na nb nc mq b">classification_report</code>来获得精确度、召回率、f1和准确度分数。</p><pre class="kg kh ki kj gt mp mq mr ms aw mt bi"><span id="c9c2" class="mu kx iq mq b gy mv mw l mx my">class_report = classification_report(actual, pred, output_dict = True)<br/>pd.DataFrame(class_report)</span></pre><h1 id="a845" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">保存模型</h1><pre class="kg kh ki kj gt mp mq mr ms aw mt bi"><span id="5a66" class="mu kx iq mq b gy mv mw l mx my">trainer.save_model('/kaggle/working/sentiment_model')</span></pre><h1 id="2ef2" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">推理</h1><p id="8a3d" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">在本节中，我们将了解如何加载已定型模型并对其执行预测。让我们在另一个笔记本上测试一下这个推论。</p><h1 id="6b1d" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">设置</h1><pre class="kg kh ki kj gt mp mq mr ms aw mt bi"><span id="9df3" class="mu kx iq mq b gy mv mw l mx my">import pandas as pd<br/>import numpy as np<br/>from transformers import Trainer, TrainingArguments, AutoConfig, AutoTokenizer, AutoModelForSequenceClassification<br/>import torch<br/>from torch import nn<br/>from torch.nn.functional import softmax</span></pre><p id="e12c" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">这种推断可以在GPU或CPU环境中工作。如果可用，在您的环境中启用GPU。</p><pre class="kg kh ki kj gt mp mq mr ms aw mt bi"><span id="a585" class="mu kx iq mq b gy mv mw l mx my">DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")<br/>print (f'Device Availble: {DEVICE}')</span></pre><p id="e0c0" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">这与我们在培训阶段使用的<code class="fe na nb nc mq b">DataLoader</code>相同</p><pre class="kg kh ki kj gt mp mq mr ms aw mt bi"><span id="4dcb" class="mu kx iq mq b gy mv mw l mx my">class DataLoader(torch.utils.data.Dataset):<br/>    def __init__(self, sentences=None, labels=None):<br/>        self.sentences = sentences<br/>        self.labels = labels<br/>        self.tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')<br/>        <br/>        if bool(sentences):<br/>            self.encodings = self.tokenizer(self.sentences,<br/>                                            truncation = True,<br/>                                            padding = True)<br/>        <br/>    def __getitem__(self, idx):<br/>        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}<br/>        <br/>        if self.labels == None:<br/>            item['labels'] = None<br/>        else:<br/>            item['labels'] = torch.tensor(self.labels[idx])<br/>        return item</span><span id="1547" class="mu kx iq mq b gy nf mw l mx my">    def __len__(self):<br/>        return len(self.sentences)<br/>    <br/>    <br/>    def encode(self, x):<br/>        return self.tokenizer(x, return_tensors = 'pt').to(DEVICE)</span></pre><h1 id="09bf" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">创建模型类</h1><p id="6548" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated"><code class="fe na nb nc mq b">SentimentModel</code>类帮助初始化模型，包含分别用于单个和批量预测的<code class="fe na nb nc mq b">predict_proba</code>和<code class="fe na nb nc mq b">batch_predict_proba</code>方法。<code class="fe na nb nc mq b">batch_predict_proba</code>使用HuggingFace的<code class="fe na nb nc mq b">Trainer</code>进行批量评分。</p><pre class="kg kh ki kj gt mp mq mr ms aw mt bi"><span id="2a61" class="mu kx iq mq b gy mv mw l mx my">class SentimentModel():<br/>    <br/>    def __init__(self, model_path):<br/>        <br/>        self.model = AutoModelForSequenceClassification.from_pretrained(model_path).to(DEVICE)<br/>        args =  TrainingArguments(output_dir='/kaggle/working/results', per_device_eval_batch_size=64)<br/>        self.batch_model = Trainer(model = self.model, args= args)<br/>        self.single_dataloader = DataLoader()<br/>        <br/>    def batch_predict_proba(self, x):<br/>        <br/>        predictions = self.batch_model.predict(DataLoader(x))<br/>        logits = torch.from_numpy(predictions.predictions)<br/>        <br/>        if DEVICE == 'cpu':<br/>            proba = torch.nn.functional.softmax(logits, dim = 1).detach().numpy()<br/>        else:<br/>            proba = torch.nn.functional.softmax(logits, dim = 1).to('cpu').detach().numpy()</span><span id="c4f4" class="mu kx iq mq b gy nf mw l mx my">        return proba<br/>        <br/>        <br/>    def predict_proba(self, x):<br/>        <br/>        x = self.single_dataloader.encode(x).to(DEVICE)<br/>        predictions = self.model(**x)<br/>        logits = predictions.logits<br/>        <br/>        if DEVICE == 'cpu':<br/>            proba = torch.nn.functional.softmax(logits, dim = 1).detach().numpy()<br/>        else:<br/>            proba = torch.nn.functional.softmax(logits, dim = 1).to('cpu').detach().numpy()</span><span id="47b3" class="mu kx iq mq b gy nf mw l mx my">        return proba</span></pre><h1 id="2561" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">数据准备</h1><p id="3cda" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">让我们加载一些样本数据</p><pre class="kg kh ki kj gt mp mq mr ms aw mt bi"><span id="427a" class="mu kx iq mq b gy mv mw l mx my">df = pd.read_csv('/kaggle/input/womens-ecommerce-clothing-reviews/Womens Clothing E-Commerce Reviews.csv')<br/>df.drop(columns = ['Unnamed: 0'], inplace = True)<br/>df_reviews = df.loc[:, ['Review Text', 'Rating']].dropna()<br/>df_reviews['Rating'] = df_reviews['Rating'].apply(lambda x: f'{x} Stars' if x != 1 else f'{x} Star')<br/>df_reviews.head()</span></pre><p id="71e0" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">我们将创建两组数据。一个用于批量评分，另一个用于单次评分。</p><pre class="kg kh ki kj gt mp mq mr ms aw mt bi"><span id="63ae" class="mu kx iq mq b gy mv mw l mx my">batch_sentences = df_reviews.sample(n = 10000, random_state = 1)['Review Text'].to_list()<br/>single_sentence = df_reviews.sample(n = 1, random_state = 1)['Review Text'].to_list()[0]</span></pre><h1 id="9c3a" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">预测</h1><p id="dd89" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">实例化模型</p><pre class="kg kh ki kj gt mp mq mr ms aw mt bi"><span id="1159" class="mu kx iq mq b gy mv mw l mx my">sentiment_model = SentimentModel('../input/fine-tune-huggingface-sentiment-analysis/sentiment_model')</span></pre><p id="3d43" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">使用<code class="fe na nb nc mq b">predict_proba</code>方法预测单个句子。</p><pre class="kg kh ki kj gt mp mq mr ms aw mt bi"><span id="c283" class="mu kx iq mq b gy mv mw l mx my">single_sentence_probas = sentiment_model.predict_proba(single_sentence)<br/>id2label = sentiment_model.model.config.id2label<br/>predicted_class_label = id2label[np.argmax(single_sentence_probas)]</span><span id="49b3" class="mu kx iq mq b gy nf mw l mx my">print (predicted_class_label)<br/>&gt;&gt; 5 Stars</span></pre><p id="79fe" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">使用<code class="fe na nb nc mq b">batch_predict_proba</code>方法对一批句子进行预测。</p><pre class="kg kh ki kj gt mp mq mr ms aw mt bi"><span id="a112" class="mu kx iq mq b gy mv mw l mx my">batch_sentence_probas = sentiment_model.batch_predict_proba(batch_sentences)<br/>predicted_class_labels = [id2label[i] for i in np.argmax(batch_sentence_probas, axis = -1)]</span></pre><h1 id="093b" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">推理速度</h1><p id="74a0" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">让我们比较一下<code class="fe na nb nc mq b">predict_proba</code>和<code class="fe na nb nc mq b">batch_predict_proba</code>方法之间的推理速度</p><p id="0d04" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">对于CPU和GPU环境中的10k样本数据。我们将对10k个样本进行迭代，<code class="fe na nb nc mq b">predict_proba</code>每次进行一次预测，同时使用<code class="fe na nb nc mq b">batch_predict_proa</code>方法对所有10k个样本进行评分，无需迭代。</p><pre class="kg kh ki kj gt mp mq mr ms aw mt bi"><span id="65fd" class="mu kx iq mq b gy mv mw l mx my">%%time<br/>for sentence in batch_sentences:<br/>    single_sentence_probas = sentiment_model.predict_proba(sentence)</span><span id="01ac" class="mu kx iq mq b gy nf mw l mx my">%%time<br/>batch_sentence_probas = sentiment_model.batch_predict_proba(batch_sentences)</span></pre><p id="d449" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated"><strong class="lq ir"> GPU环境</strong></p><p id="5b3c" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">遍历<code class="fe na nb nc mq b">predict_proba</code>大约需要2分钟，而对于10k样本数据<code class="fe na nb nc mq b">batch_predict_proba</code>大约需要30秒。批量预测比在GPU环境下使用单次预测快了差不多4倍。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nz"><img src="../Images/e8b9905dcc34d47822111ac7f1fbb0ef.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*7B8IjpGzyJoTNFcO"/></div></div></figure><p id="af5a" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated"><strong class="lq ir"> CPU环境</strong></p><p id="0fdd" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">在CPU环境中，<code class="fe na nb nc mq b">predict_proba</code>花费了大约14分钟，而<code class="fe na nb nc mq b">batch_predict_proba</code>花费了大约40分钟，几乎是原来的3倍。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oa"><img src="../Images/85b78aa8e89e2f316a791cc8d5447d20.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*7vbmPf6z6ms_OQNg"/></div></div></figure><p id="1f49" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">因此，对于大型数据集，如果您有GPU，请使用<code class="fe na nb nc mq b">batch_predict_proba</code>。如果您无法访问GPU，您最好使用<code class="fe na nb nc mq b">predict_proba</code>遍历数据集。</p><h1 id="9e0d" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">结论</h1><p id="6a78" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">在本文中，我们研究了:</p><ul class=""><li id="519d" class="ni nj iq lq b lr mk lu ml lx nk mb nl mf nm mj nn no np nq bi translated">如何利用预训练的HuggingFace模型来训练自己的深度学习情感分析模型</li><li id="984f" class="ni nj iq lq b lr nr lu ns lx nt mb nu mf nv mj nn no np nq bi translated">如何创建单项和批量评分预测方法</li><li id="24ef" class="ni nj iq lq b lr nr lu ns lx nt mb nu mf nv mj nn no np nq bi translated">CPU和GPU环境中单次和批量评分的推断速度</li></ul><p id="f814" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">这篇文章的笔记本可以在这里找到:</p><ul class=""><li id="4be1" class="ni nj iq lq b lr mk lu ml lx nk mb nl mf nm mj nn no np nq bi translated"><a class="ae kv" href="https://www.kaggle.com/edwintyh/huggingface-sentiment-analysis-inference" rel="noopener ugc nofollow" target="_blank">训练</a></li><li id="31b3" class="ni nj iq lq b lr nr lu ns lx nt mb nu mf nv mj nn no np nq bi translated"><a class="ae kv" href="https://www.kaggle.com/edwintyh/huggingface-sentiment-analysis-inference" rel="noopener ugc nofollow" target="_blank">推论</a></li></ul></div><div class="ab cl ob oc hu od" role="separator"><span class="oe bw bk of og oh"/><span class="oe bw bk of og oh"/><span class="oe bw bk of og"/></div><div class="ij ik il im in"><p id="0e87" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated"><a class="ae kv" href="https://medium.com/@edwin.tan/membership" rel="noopener">加入Medium </a>阅读更多这样的故事。</p></div></div>    
</body>
</html>