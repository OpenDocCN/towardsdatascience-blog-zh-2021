<html>
<head>
<title>Beginner’s guide for feature selection</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">功能选择初学者指南</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/beginners-guide-for-feature-selection-by-a-beginner-cd2158c5c36a?source=collection_archive---------15-----------------------#2021-05-08">https://towardsdatascience.com/beginners-guide-for-feature-selection-by-a-beginner-cd2158c5c36a?source=collection_archive---------15-----------------------#2021-05-08</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="5ddf" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">特征选择的不同方法以及为什么每个人都要为特征选择而烦恼；通过比较不同的方法来选择最优的特征选择方法。</h2></div><p id="25c7" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">处理大型数据集时，由于存在大量要素，建模运行起来可能会非常耗时。一个模型有数百个特征并不罕见。那么剔除不相关的和不合格的特征就很关键了。这就是特征选择的概念发挥作用的时候了。在本文中，我将尝试介绍一些广泛使用的特性选择技术，并演示其中的一些。</p><p id="1d6f" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">特征选择对于建立计算高效的模型是极其重要的步骤。这方面有很多技巧。让我们从定义特征选择的过程开始。</p><blockquote class="lb"><p id="1091" class="lc ld iq bd le lf lg lh li lj lk la dk translated">特征选择是选择最相关的预测特征的子集用于机器学习模型建立的过程。</p></blockquote><blockquote class="ll lm ln"><p id="4115" class="kf kg lo kh b ki lp jr kk kl lq ju kn lr ls kq kr lt lu ku kv lv lw ky kz la ij bi translated">特征消除通过剔除冗余特征和不能提供太多洞察力的特征来帮助模型更好地执行。它在计算能力上是经济的，因为需要训练的功能更少。结果更易于解释，并且通过检测共线要素减少了过度拟合的机会，如果方法使用得当，还可以提高模型的准确性。</p></blockquote><p id="d0d5" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir"> <em class="lo">特征选择方法:</em> </strong></p><figure class="ly lz ma mb gt mc gh gi paragraph-image"><div class="gh gi lx"><img src="../Images/b71a3a986adf5851537492dd3da7da46.png" data-original-src="https://miro.medium.com/v2/resize:fit:696/format:webp/1*5d8LRp6bIzYivL-JO9WTBQ.png"/></div><p class="mf mg gj gh gi mh mi bd b be z dk translated">特征选择方法的亮点，按作者分类的图片</p></figure><h1 id="f6c0" class="mj mk iq bd ml mm mn mo mp mq mr ms mt jw mu jx mv jz mw ka mx kc my kd mz na bi translated"><em class="nb"> 1。过滤方式:</em></h1><p id="fbb4" class="pw-post-body-paragraph kf kg iq kh b ki nc jr kk kl nd ju kn ko ne kq kr ks nf ku kv kw ng ky kz la ij bi translated"><em class="lo">探测能力:★☆☆|速度:★★★★ </em></p><figure class="ly lz ma mb gt mc gh gi paragraph-image"><div role="button" tabindex="0" class="ni nj di nk bf nl"><div class="gh gi nh"><img src="../Images/5fc7fdad96ae58a528228150e402eed3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*CjwW8-7CLrSXLBmB"/></div></div><p class="mf mg gj gh gi mh mi bd b be z dk translated">从可用的布景中挑选最好的，照片由<a class="ae nm" href="https://unsplash.com/@uxindo?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> UX印度尼西亚</a>在<a class="ae nm" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><p id="31b8" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这不是机器学习方法。它根据要素的属性过滤要素。这种方法是“模型不可知”的，即性能不依赖于所用的模型。</p><p id="7f3f" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这种方法应该用于初步筛选。它可以检测恒定的、重复的和相关的特征。通常在减少功能方面不是最好的表现。也就是说，这应该是减少要素的第一步，因为它根据所使用的方法来处理要素的多重共线性。</p><figure class="ly lz ma mb gt mc gh gi paragraph-image"><div role="button" tabindex="0" class="ni nj di nk bf nl"><div class="gh gi nn"><img src="../Images/991fcb72f4c83e73d02c72398ba8ccad.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MO2JGos1gZnyfQueFrzzsw.png"/></div></div><p class="mf mg gj gh gi mh mi bd b be z dk translated">使用过滤方法的特征选择过程，按作者分类的图像</p></figure><p id="a02b" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这方面的几个例子:</p><ol class=""><li id="8fc2" class="no np iq kh b ki kj kl km ko nq ks nr kw ns la nt nu nv nw bi translated"><em class="lo">单变量选择(ANOVA: </em>方差分析<em class="lo"> ) </em></li><li id="cbeb" class="no np iq kh b ki nx kl ny ko nz ks oa kw ob la nt nu nv nw bi translated"><em class="lo">卡方</em></li><li id="b161" class="no np iq kh b ki nx kl ny ko nz ks oa kw ob la nt nu nv nw bi translated"><em class="lo">基于皮尔森相关性</em></li><li id="6912" class="no np iq kh b ki nx kl ny ko nz ks oa kw ob la nt nu nv nw bi translated"><em class="lo">线性判别分析(LDA): </em>线性判别分析用于寻找特征的线性组合，其表征或分离两个或更多类别的分类变量</li></ol><h1 id="5112" class="mj mk iq bd ml mm mn mo mp mq mr ms mt jw mu jx mv jz mw ka mx kc my kd mz na bi translated"><em class="nb"> 2。包装方法:</em></h1><p id="57f0" class="pw-post-body-paragraph kf kg iq kh b ki nc jr kk kl nd ju kn ko ne kq kr ks nf ku kv kw ng ky kz la ij bi translated"><em class="lo">探测能力:★★★☆ |速度:★☆☆☆</em></p><figure class="ly lz ma mb gt mc gh gi paragraph-image"><div role="button" tabindex="0" class="ni nj di nk bf nl"><div class="gh gi oc"><img src="../Images/db57d6928092c083b04a5ea33c9ebc52.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*tlKDI9LytRTU74Ce"/></div></div><p class="mf mg gj gh gi mh mi bd b be z dk translated">机器学习的胜利，照片由<a class="ae nm" href="https://unsplash.com/@possessedphotography?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">附身摄影</a>在<a class="ae nm" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a></p></figure><p id="13ca" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这种方法使用机器学习算法。这种方法的性能取决于所选的模型和底层数据。通常可以建议最佳的特征子集。尝试不同的功能子集以找出最佳功能。通常计算量非常大。可以检测特征之间的相互作用。</p><p id="d809" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在特征消除方面可能是最好的性能。对于大型数据集，包装器非常慢。</p><figure class="ly lz ma mb gt mc gh gi paragraph-image"><div role="button" tabindex="0" class="ni nj di nk bf nl"><div class="gh gi od"><img src="../Images/d7d5a9110957d644e9ff25a24e65c6b4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8Q2Qh2E-g7dRcHwb50l1Rg.png"/></div></div><p class="mf mg gj gh gi mh mi bd b be z dk translated">使用包装方法的特征选择过程，由作者创建的图像</p></figure><p id="ad7d" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这方面的几个例子:</p><ol class=""><li id="4bb6" class="no np iq kh b ki kj kl km ko nq ks nr kw ns la nt nu nv nw bi translated"><em class="lo">正向选择</em></li><li id="c2fa" class="no np iq kh b ki nx kl ny ko nz ks oa kw ob la nt nu nv nw bi translated"><em class="lo">向后选择</em></li><li id="44fe" class="no np iq kh b ki nx kl ny ko nz ks oa kw ob la nt nu nv nw bi translated"><em class="lo">穷举搜索</em></li></ol><h1 id="f725" class="mj mk iq bd ml mm mn mo mp mq mr ms mt jw mu jx mv jz mw ka mx kc my kd mz na bi translated"><strong class="ak"> <em class="nb"> 3。嵌入方法:</em> </strong></h1><p id="7482" class="pw-post-body-paragraph kf kg iq kh b ki nc jr kk kl nd ju kn ko ne kq kr ks nf ku kv kw ng ky kz la ij bi translated"><em class="lo">探测能力:★★★☆☆ |速度:★★★☆☆ </em></p><figure class="ly lz ma mb gt mc gh gi paragraph-image"><div role="button" tabindex="0" class="ni nj di nk bf nl"><div class="gh gi oe"><img src="../Images/e70914f9766f3151eb25b3683a163779.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yXzDlS6xfBga8ZMEK8V8Hw.jpeg"/></div></div><p class="mf mg gj gh gi mh mi bd b be z dk translated">人机协同工作，图片来自<a class="ae nm" href="https://pixabay.com/photos/connection-hand-human-robot-touch-3308188/" rel="noopener ugc nofollow" target="_blank"> pixabay </a></p></figure><p id="4eb9" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">构建模型时执行特征选择。通常比包装方法的计算成本低。通常提供两全其美的结果，通常是更现实的方法。</p><figure class="ly lz ma mb gt mc gh gi paragraph-image"><div role="button" tabindex="0" class="ni nj di nk bf nl"><div class="gh gi of"><img src="../Images/cb8c4acbd53b55a11809931d2b60a40d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wHpQhfv2J9IcabwujU9ytg.png"/></div></div><p class="mf mg gj gh gi mh mi bd b be z dk translated">使用嵌入式方法的特征选择过程，按作者排序的图像</p></figure><p id="0a70" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这方面的几个例子:</p><ol class=""><li id="8c21" class="no np iq kh b ki kj kl km ko nq ks nr kw ns la nt nu nv nw bi translated"><em class="lo">拉索</em></li><li id="bd6d" class="no np iq kh b ki nx kl ny ko nz ks oa kw ob la nt nu nv nw bi translated"><em class="lo">带脊套索(使用ElasticNet规则化特征)</em></li><li id="e31c" class="no np iq kh b ki nx kl ny ko nz ks oa kw ob la nt nu nv nw bi translated"><em class="lo">基于树的选择。</em></li><li id="2a0a" class="no np iq kh b ki nx kl ny ko nz ks oa kw ob la nt nu nv nw bi translated"><em class="lo">回归系数(特征必须标准化)。</em></li></ol><p id="fdad" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">超参数调整对于该方法非常重要，这是该方法的人工干预部分。</p><h1 id="0e5e" class="mj mk iq bd ml mm mn mo mp mq mr ms mt jw mu jx mv jz mw ka mx kc my kd mz na bi translated"><em class="nb"> 4。混合方法:</em></h1><p id="3dd7" class="pw-post-body-paragraph kf kg iq kh b ki nc jr kk kl nd ju kn ko ne kq kr ks nf ku kv kw ng ky kz la ij bi translated"><em class="lo">探测能力:★★★☆☆ |速度:★★★☆ </em></p><p id="5831" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">上述所有技术的融合。这种方法比包装器方法计算量少，并且具有良好的性能。</p><figure class="ly lz ma mb gt mc gh gi paragraph-image"><div role="button" tabindex="0" class="ni nj di nk bf nl"><div class="gh gi og"><img src="../Images/421772a6da8bcb2feb48400cf1c9a1f3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YcQmBq1Ct61-tnengNuvrQ.png"/></div></div><p class="mf mg gj gh gi mh mi bd b be z dk translated">使用混合方法的特征选择过程，按作者排序</p></figure><p id="541b" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这方面的几个例子:</p><ol class=""><li id="e563" class="no np iq kh b ki kj kl km ko nq ks nr kw ns la nt nu nv nw bi translated"><em class="lo">功能洗牌</em></li><li id="5eae" class="no np iq kh b ki nx kl ny ko nz ks oa kw ob la nt nu nv nw bi translated"><em class="lo">递归特征消除</em></li><li id="9efc" class="no np iq kh b ki nx kl ny ko nz ks oa kw ob la nt nu nv nw bi translated"><em class="lo">递归特性添加</em></li></ol><p id="06c9" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">带走消息:</strong></p><ol class=""><li id="8113" class="no np iq kh b ki kj kl km ko nq ks nr kw ns la nt nu nv nw bi translated">过滤方法通过特征相对于目标变量的相关性基于特征的相关性对特征进行排序，而包装方法通过实际训练特征子集上的模型来测量特征子集的有用性。</li><li id="ba34" class="no np iq kh b ki nx kl ny ko nz ks oa kw ob la nt nu nv nw bi translated">与包装器方法相比，过滤器方法要快得多，因为它们实际上并不训练模型。而包装器方法则不然，这使得它们计算量很大，有时甚至无法执行。</li><li id="3861" class="no np iq kh b ki nx kl ny ko nz ks oa kw ob la nt nu nv nw bi translated">过滤方法使用选定的统计方法进行特征选择，而包装方法执行交叉验证以确定特征的最佳子集。</li><li id="e3f5" class="no np iq kh b ki nx kl ny ko nz ks oa kw ob la nt nu nv nw bi translated">由于要素的属性，过滤方法可能无法找到要素的最佳子集，但是包装方法通常可以更经常地提供要素的最佳子集。</li><li id="be02" class="no np iq kh b ki nx kl ny ko nz ks oa kw ob la nt nu nv nw bi translated">包装器方法倾向于使模型更容易过度拟合，对于那些方法，测试训练分离是必须的。</li></ol></div><div class="ab cl oh oi hu oj" role="separator"><span class="ok bw bk ol om on"/><span class="ok bw bk ol om on"/><span class="ok bw bk ol om"/></div><div class="ij ik il im in"><p id="2f9e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">为了演示这些技术，我对“金县住房数据集”执行了基于特征间相关性的特征选择、方差分析、正向选择、RFE和套索技术。</p><p id="0107" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">为了使这篇文章简短，不偏离主题，我将简要介绍一下我为准备数据所采取的步骤。</p><ol class=""><li id="0af8" class="no np iq kh b ki kj kl km ko nq ks nr kw ns la nt nu nv nw bi translated">分类特征是OneHotEncoded。</li><li id="6034" class="no np iq kh b ki nx kl ny ko nz ks oa kw ob la nt nu nv nw bi translated">我从“id”栏中删除了重复项。</li></ol><p id="99ad" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">2.在NaN中填入0，并在“waterfront”、“view”、“yr _ renovated”和“sqft_basement”功能中将其他错误输入填入0。</p><p id="8549" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">3.转换适当的数据类型。</p><p id="4867" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">4.从数据中剔除异常值。</p><p id="3538" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">5.将数据缩放至最小最大缩放器(此缩放器单独缩放和转换每个要素，使其位于数据集的给定范围内，例如，介于0和1之间)以用于lasso。</p><p id="975b" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">包含这部作品的笔记本可以在GitHub上的<code class="fe oo op oq or b">Feature_Selection.ipynb</code>找到<a class="ae nm" href="https://github.com/tamjid-ahsan/beginners-guide-for-feature-selection" rel="noopener ugc nofollow" target="_blank">这里的</a>。</p></div><div class="ab cl oh oi hu oj" role="separator"><span class="ok bw bk ol om on"/><span class="ok bw bk ol om on"/><span class="ok bw bk ol om"/></div><div class="ij ik il im in"><ol class=""><li id="2fb0" class="no np iq kh b ki kj kl km ko nq ks nr kw ns la nt nu nv nw bi translated">过滤方法:</li></ol><ul class=""><li id="fe68" class="no np iq kh b ki kj kl km ko nq ks nr kw ns la os nu nv nw bi translated"><em class="lo">基于皮尔森相关性</em></li></ul><p id="31e0" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我使用这个函数来获得相关的特征</p><figure class="ly lz ma mb gt mc"><div class="bz fp l di"><div class="ot ou l"/></div></figure><p id="b359" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在清理的数据集上运行时，结果如下:</p><pre class="ly lz ma mb gt ov or ow ox aw oy bi"><span id="036e" class="oz mk iq or b gy pa pb l pc pd">correlated features:  1<br/>correlated features:  {'sqft_above'}</span></pre><p id="910d" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这意味着“sqft_above”功能与其他功能相关，应该删除。</p><ul class=""><li id="f382" class="no np iq kh b ki kj kl km ko nq ks nr kw ns la os nu nv nw bi translated"><em class="lo">单变量选择(ANOVA) </em></li></ul><p id="67b9" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我使用这一行代码来执行ANOVA。</p><figure class="ly lz ma mb gt mc"><div class="bz fp l di"><div class="ot ou l"/></div></figure><p id="ae18" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我根据r的平方对结果进行了排序。结果如下。</p><figure class="ly lz ma mb gt mc gh gi paragraph-image"><div class="gh gi pe"><img src="../Images/9cdb67e9d05ba3873bc849c4c36b03e9.png" data-original-src="https://miro.medium.com/v2/resize:fit:920/format:webp/1*j2XiVyA6-yaxqdKhR_oLCA.png"/></div><p class="mf mg gj gh gi mh mi bd b be z dk translated">作者图片</p></figure><p id="e734" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">由此，我对模型中包含的最重要的特性有了一个想法。如<code class="fe oo op oq or b">grade</code>、<code class="fe oo op oq or b">sqft_living</code>、<code class="fe oo op oq or b">zipcode</code>等。</p><figure class="ly lz ma mb gt mc gh gi paragraph-image"><div class="gh gi pf"><img src="../Images/92ede6602fbaf2468e6dd64716e56b8d.png" data-original-src="https://miro.medium.com/v2/resize:fit:990/format:webp/1*ypowmd_T7RbzUru0ezFrgg.png"/></div><p class="mf mg gj gh gi mh mi bd b be z dk translated">作者图片</p></figure><p id="867f" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">绘制时，我也能根据它们的p值感觉到它们的重要性。<code class="fe oo op oq or b">grade</code>和<code class="fe oo op oq or b">condition</code>具有很高的p值。</p><p id="8874" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">2.包装方法:</p><ul class=""><li id="c1bf" class="no np iq kh b ki kj kl km ko nq ks nr kw ns la os nu nv nw bi translated">预选</li></ul><p id="1ea7" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我在OneHotEncoded数据帧上使用这个函数进行前向选择。</p><figure class="ly lz ma mb gt mc"><div class="bz fp l di"><div class="ot ou l"/></div></figure><p id="929f" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">然后我运行了下面的代码。</p><pre class="ly lz ma mb gt ov or ow ox aw oy bi"><span id="c725" class="oz mk iq or b gy pa pb l pc pd">model = forward_selected(df_model_processed_ohe, 'price')<br/>print(model.model.formula)<br/>print(model.rsquared_adj)<br/>model.summary()</span></pre><p id="ecaa" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">结果给出了从最重要到最不重要的特性。</p><pre class="ly lz ma mb gt ov or ow ox aw oy bi"><span id="bc83" class="oz mk iq or b gy pa pb l pc pd">price ~ sqft_living + yr_built + sqft_lot + sqft_living15 + zipcode_98004 + grade_9 + grade_8 + grade_10 + grade_7 + zipcode_98023 + zipcode_98033 + zipcode_98040 + zipcode_98092 + zipcode_98042 + zipcode_98003 + zipcode_98058 + zipcode_98038 + zipcode_98030 + zipcode_98031 + zipcode_98055 + zipcode_98002 + zipcode_98198 + zipcode_98032 + zipcode_98178 + zipcode_98168 + zipcode_98022 + zipcode_98112 + view_4 + zipcode_98199 + zipcode_98115 + zipcode_98103 + zipcode_98117 + zipcode_98119 + zipcode_98105 + zipcode_98107 + zipcode_98109 + zipcode_98116 + zipcode_98102 + zipcode_98122 + zipcode_98052 + zipcode_98006 + zipcode_98005 + zipcode_98053 + zipcode_98136 + zipcode_98144 + zipcode_98008 + zipcode_98029 + condition_5 + view_2 + zipcode_98188 + view_3 + zipcode_98027 + zipcode_98007 + zipcode_98074 + zipcode_98075 + zipcode_98034 + zipcode_98125 + zipcode_98039 + zipcode_98126 + zipcode_98177 + grade_11 + zipcode_98133 + zipcode_98118 + sqft_basement + condition_4 + yr_renovated + view_1 + zipcode_98155 + waterfront_1 + zipcode_98072 + zipcode_98011 + zipcode_98065 + zipcode_98028 + bathrooms + zipcode_98106 + floors + zipcode_98108 + zipcode_98077 + zipcode_98146 + zipcode_98056 + zipcode_98059 + zipcode_98045 + zipcode_98019 + zipcode_98166 + zipcode_98014 + zipcode_98024 + zipcode_98010 + condition_3 + zipcode_98148 + zipcode_98070 + grade_5 + bedrooms + sqft_lot15 + 1</span><span id="ede5" class="oz mk iq or b gy pg pb l pc pd">r_sq: 0.8315104663680916</span></pre><p id="cf9a" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这还会返回用于检查同质性的图。</p><figure class="ly lz ma mb gt mc gh gi paragraph-image"><div role="button" tabindex="0" class="ni nj di nk bf nl"><div class="gh gi ph"><img src="../Images/ea7d2a4c06e7e712f996ba3d0936125b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iXViC0icF7crOZqiXSOj_w.png"/></div></div><p class="mf mg gj gh gi mh mi bd b be z dk translated">作者图片</p></figure><p id="b0d3" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我可以看到，残差图远非完美，模型中有明显的偏差。</p><p id="a91a" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">3.嵌入式方法:</p><p id="a8a0" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我为这个演示表演了套索。使用sklearn.linear_model中的LassoCV</p><figure class="ly lz ma mb gt mc"><div class="bz fp l di"><div class="ot ou l"/></div></figure><p id="5949" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">一开始有95个特征。上面代码的结果是:</p><pre class="ly lz ma mb gt ov or ow ox aw oy bi"><span id="e440" class="oz mk iq or b gy pa pb l pc pd">Model r squared 0.8307881074614486<br/>Number of Selected Features: 91</span></pre><p id="b151" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">Lasso将特征集减少到91。这些功能已被删除- `sqft_lot15 ' 、` zipcode_98070 ' 、` zipcode_98148 '、` grade_8 '。</p><p id="1994" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">4.混合方法:</p><p id="635a" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">为了演示这一点，我使用了来自scikit-learn、sklearn.svm和sklearn.feature_selection模块的SVR和RFE。如果在RFE“n _ features _ to _ select”中没有传递任何内容，则选择一半的特征。我为这个特性选择使用了下面的代码。</p><figure class="ly lz ma mb gt mc"><div class="bz fp l di"><div class="ot ou l"/></div></figure><p id="596a" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这样的结果是:</p><pre class="ly lz ma mb gt ov or ow ox aw oy bi"><span id="8e08" class="oz mk iq or b gy pa pb l pc pd">Model r squred: 0.776523847362918<br/>number of selected feature 47</span></pre><p id="3332" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">为模型选择特征更像是一门艺术，其中使用的判断力很重要。建模中经常用到的一句话是<em class="lo">垃圾输入垃圾输出</em>。对于特征选择也是如此。我们在根据选定的特性建模时必须小心，因为<em class="lo">有时候越少越好</em>。</p><p id="c695" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在上面的演示中，他们中的一些人表现良好，而一些人表现不佳。这取决于数据科学家根据分析的目标为他们的模型选择最佳数字。也就是说，这些技术对于任何一个高效的数据科学家来说都很方便。</p><p id="4370" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">另一种方法是用于降维的主成分分析。它在保留属性的同时减少了特征，但是牺牲了模型的推理。</p></div><div class="ab cl oh oi hu oj" role="separator"><span class="ok bw bk ol om on"/><span class="ok bw bk ol om on"/><span class="ok bw bk ol om"/></div><div class="ij ik il im in"><p id="a406" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">今天到此为止。下次见！</p></div></div>    
</body>
</html>