<html>
<head>
<title/>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1/>
<blockquote>原文：<a href="https://towardsdatascience.com/gradient-boosting-is-one-of-the-most-effective-ml-techniques-out-there-af6bfd0df342?source=collection_archive---------7-----------------------#2021-04-04">https://towardsdatascience.com/gradient-boosting-is-one-of-the-most-effective-ml-techniques-out-there-af6bfd0df342?source=collection_archive---------7-----------------------#2021-04-04</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/121accb71b570dce79e38db879341669.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6w9E9tck6V9DOujGwTBxcw.png"/></div></div></figure></div><div class="ab cl jc jd hx je" role="separator"><span class="jf bw bk jg jh ji"/><span class="jf bw bk jg jh ji"/><span class="jf bw bk jg jh"/></div><div class="im in io ip iq"><h1 id="e4e2" class="jj jk jl bd jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh bi translated">为什么助推有效</h1><p id="ae8c" class="pw-post-body-paragraph ki kj jl kk b kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf im bi translated">梯度推进是最有效的ML技术之一。在这篇文章中，我来看看为什么升压工作。TL；DL Boosting通过拟合残差中的模式来纠正以前学习者的错误。</p><h1 id="5227" class="jj jk jl bd jm jn lg jp jq jr lh jt ju jv li jx jy jz lj kb kc kd lk kf kg kh bi translated">助推</h1><p id="4023" class="pw-post-body-paragraph ki kj jl kk b kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf im bi translated">在这篇文章中，我将着眼于boosting，重点是为这项技术的工作原理建立一个直觉。大多数从事数据科学和机器学习的人都知道梯度推进是最强大和最有效的算法之一。它仍然是Kaggle comps中最成功的ML技术之一，并在实践中广泛用于各种用例。</p><p id="2d51" class="pw-post-body-paragraph ki kj jl kk b kl ll kn ko kp lm kr ks kt ln kv kw kx lo kz la lb lp ld le lf im bi translated">为了建立增强的直觉，我将使用Scikit learn决策树实现来构建一个简单的增强器。不言而喻，用于梯度增强的go to技术是优秀的XGboost包。这篇文章应该有助于你理解为什么boosting在预测建模问题中如此有效。</p><p id="35c8" class="pw-post-body-paragraph ki kj jl kk b kl ll kn ko kp lm kr ks kt ln kv kw kx lo kz la lb lp ld le lf im bi translated">在高层次上，boosting属于ML算法的集合家族。增强包括顺序训练弱学习者——弱学习者是低偏差估计者——来预测一些结果。有趣的是，每个学习者并不预测最初的目标。相反，每个学习者试图预测前一个学习者的错误。在实践中，这意味着学习者1将试图直接预测目标结果，而学习者2将试图预测学习者1的残差。这个预测残差的过程一直持续到最终的学习者。然后，可以通过对所有个体学习者求和来做出最终预测。这是一种非常有效的预测事物的方法，但是直觉并不总是很清楚。我希望在这篇文章中让这种直觉更容易理解。</p><p id="94f4" class="pw-post-body-paragraph ki kj jl kk b kl ll kn ko kp lm kr ks kt ln kv kw kx lo kz la lb lp ld le lf im bi translated">让我们从导入一些依赖项开始</p><pre class="lr ls lt lu gt lv lw lx ly aw lz bi"><span id="a029" class="ma jk jl lw b gy mb mc l md me">from sklearn.tree import DecisionTreeRegressor<br/>import matplotlib.pyplot <strong class="lw lq">as</strong> plt<br/>import seaborn <strong class="lw lq">as</strong> sns<br/>import pandas <strong class="lw lq">as</strong> pd<br/>import numpy <strong class="lw lq">as</strong> np<br/><br/>sns<strong class="lw lq">.</strong>set_style<strong class="lw lq">(</strong>"whitegrid"<strong class="lw lq">)</strong></span><span id="422a" class="ma jk jl lw b gy mf mc l md me">n <strong class="lw lq">=</strong> 100<br/>X <strong class="lw lq">=</strong> np<strong class="lw lq">.</strong>linspace<strong class="lw lq">(</strong>0<strong class="lw lq">,</strong> 10<strong class="lw lq">,</strong> n<strong class="lw lq">)</strong> <br/>y <strong class="lw lq">=</strong> X<strong class="lw lq">**</strong>2 <strong class="lw lq">+</strong> 10 <strong class="lw lq">-</strong> <strong class="lw lq">(</strong>20 <strong class="lw lq">*</strong> np<strong class="lw lq">.</strong>random<strong class="lw lq">.</strong>random<strong class="lw lq">(</strong>n<strong class="lw lq">))</strong><br/>X <strong class="lw lq">=</strong> X<strong class="lw lq">[:,</strong> np<strong class="lw lq">.</strong>newaxis<strong class="lw lq">]</strong><br/><br/>plt<strong class="lw lq">.</strong>figure<strong class="lw lq">(</strong>figsize<strong class="lw lq">=(</strong>15<strong class="lw lq">,</strong> 4<strong class="lw lq">))</strong><br/>plt<strong class="lw lq">.</strong>scatter<strong class="lw lq">(</strong>X<strong class="lw lq">,</strong> y<strong class="lw lq">,</strong> alpha<strong class="lw lq">=.</strong>7<strong class="lw lq">);</strong></span></pre><figure class="lr ls lt lu gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi mg"><img src="../Images/e14e03018cc36feb4807edab0e2b910e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Nprs76mTWNnAUa7L082gKg.png"/></div></div><p class="mh mi gj gh gi mj mk bd b be z dk translated"><em class="ml">作者图片</em></p></figure><p id="0b5c" class="pw-post-body-paragraph ki kj jl kk b kl ll kn ko kp lm kr ks kt ln kv kw kx lo kz la lb lp ld le lf im bi translated">下面的函数使用决策树创建一个增强的回归学习器。决策树是最受欢迎和最有效的集成学习器之一(尽管从技术上来说，你可以提升任何算法)。然而决策树是一个不错的选择，因为1)它们训练迅速，2)它们可以模拟非线性。</p><p id="397e" class="pw-post-body-paragraph ki kj jl kk b kl ll kn ko kp lm kr ks kt ln kv kw kx lo kz la lb lp ld le lf im bi translated">我已经尽量保持下面的功能超级简单。<code class="fe mm mn mo lw b">many_trees</code>返回决策树列表，<code class="fe mm mn mo lw b">boost</code>通过首先用tree-0预测目标结果<code class="fe mm mn mo lw b">y</code>，然后从tree-1- <em class="mp"> n </em>预测残差，并<code class="fe mm mn mo lw b">predict</code>迭代拟合的决策树列表，并返回每个树的预测，从而依次拟合决策树。<code class="fe mm mn mo lw b">plot_fits</code>是一个便利函数，它对<em class="mp"> n </em>棵树的预测求和，并返回拟合线和残差。</p><h1 id="29f5" class="jj jk jl bd jm jn lg jp jq jr lh jt ju jv li jx jy jz lj kb kc kd lk kf kg kh bi translated">使用决策树实现助推</h1><pre class="lr ls lt lu gt lv lw lx ly aw lz bi"><span id="83b3" class="ma jk jl lw b gy mb mc l md me"><strong class="lw lq">def</strong> <strong class="lw lq">many_trees(</strong>n_trees<strong class="lw lq">,</strong> clf<strong class="lw lq">=False,</strong> <strong class="lw lq">**</strong>kwargs<strong class="lw lq">):</strong><br/>    trees <strong class="lw lq">=</strong> <strong class="lw lq">[</strong>DecisionTreeRegressor<strong class="lw lq">(**</strong>kwargs<strong class="lw lq">)</strong> <strong class="lw lq">for</strong> i <strong class="lw lq">in</strong> <em class="mp">range</em><strong class="lw lq">(</strong>n_trees<strong class="lw lq">)]</strong><br/>    <strong class="lw lq">return</strong> trees<br/><br/><strong class="lw lq">def</strong> <strong class="lw lq">boost(</strong>trees<strong class="lw lq">,</strong> X<strong class="lw lq">,</strong> y<strong class="lw lq">):</strong><br/>    fitted <strong class="lw lq">=</strong> <strong class="lw lq">[]</strong><br/>    <strong class="lw lq">for</strong> tree <strong class="lw lq">in</strong> trees<strong class="lw lq">:</strong><br/>        tree<strong class="lw lq">.</strong>fit<strong class="lw lq">(</strong>X<strong class="lw lq">,</strong> y<strong class="lw lq">)</strong><br/>        yhat <strong class="lw lq">=</strong> tree<strong class="lw lq">.</strong>predict<strong class="lw lq">(</strong>X<strong class="lw lq">)</strong><br/>        y <strong class="lw lq">=</strong> <strong class="lw lq">(</strong>y<strong class="lw lq">-</strong>yhat<strong class="lw lq">)</strong> <br/>        fitted<strong class="lw lq">.</strong>append<strong class="lw lq">(</strong>tree<strong class="lw lq">)</strong><br/>    <strong class="lw lq">return</strong> fitted<br/>        <br/><strong class="lw lq">def</strong> <strong class="lw lq">predict(</strong>trees<strong class="lw lq">,</strong> X<strong class="lw lq">):</strong><br/>    <strong class="lw lq">return</strong> np<strong class="lw lq">.</strong>array<strong class="lw lq">([</strong>tree<strong class="lw lq">.</strong>predict<strong class="lw lq">(</strong>X<strong class="lw lq">)</strong> <strong class="lw lq">for</strong> tree <strong class="lw lq">in</strong> trees<strong class="lw lq">]).</strong>T</span></pre><p id="444a" class="pw-post-body-paragraph ki kj jl kk b kl ll kn ko kp lm kr ks kt ln kv kw kx lo kz la lb lp ld le lf im bi translated">随着boosting functonality的实现，我们现在可以适应树。鉴于预测问题的简单性，我将通过将每棵树的最大深度设置为1来使学习者变得非常脆弱。当预测y时，这将每个树限制为X的一个分裂。</p><pre class="lr ls lt lu gt lv lw lx ly aw lz bi"><span id="0c25" class="ma jk jl lw b gy mb mc l md me">learners <strong class="lw lq">=</strong> many_trees<strong class="lw lq">(</strong>30<strong class="lw lq">,</strong> max_depth<strong class="lw lq">=</strong>1<strong class="lw lq">,</strong> clf<strong class="lw lq">=False)</strong><br/>fitted <strong class="lw lq">=</strong> boost<strong class="lw lq">(</strong>learners<strong class="lw lq">,</strong> X<strong class="lw lq">,</strong> y<strong class="lw lq">,</strong> clf<strong class="lw lq">=False)</strong><br/>boosted_yhat <strong class="lw lq">=</strong> predict<strong class="lw lq">(</strong>fitted<strong class="lw lq">,</strong> X<strong class="lw lq">)</strong><br/><br/>xfit <strong class="lw lq">=</strong> np<strong class="lw lq">.</strong>linspace<strong class="lw lq">(</strong>0<strong class="lw lq">,</strong> 10<strong class="lw lq">,</strong> 100<strong class="lw lq">).</strong>reshape<strong class="lw lq">(-</strong>1<strong class="lw lq">,</strong> 1<strong class="lw lq">)</strong><br/><em class="mp">  </em><br/><strong class="lw lq">def</strong> <strong class="lw lq">plot_fits(</strong>n_trees<strong class="lw lq">,</strong> row<strong class="lw lq">):</strong><br/>    preds_t <strong class="lw lq">=</strong> boosted_yhat<strong class="lw lq">[:,</strong> <strong class="lw lq">:</strong>n_trees<strong class="lw lq">]</strong><br/>    boosted_pred <strong class="lw lq">=</strong> preds_t<strong class="lw lq">.</strong>sum<strong class="lw lq">(</strong>1<strong class="lw lq">)</strong><br/>    res <strong class="lw lq">=</strong> boosted_pred<strong class="lw lq">-</strong>y<br/>    axes<strong class="lw lq">[</strong>row<strong class="lw lq">,</strong> 0<strong class="lw lq">].</strong>plot<strong class="lw lq">(</strong>xfit<strong class="lw lq">,</strong> boosted_pred<strong class="lw lq">,</strong> c<strong class="lw lq">=</strong>'red'<strong class="lw lq">)</strong><br/>    axes<strong class="lw lq">[</strong>row<strong class="lw lq">,</strong> 0<strong class="lw lq">].</strong>scatter<strong class="lw lq">(</strong>X<strong class="lw lq">,</strong> y<strong class="lw lq">)</strong><br/>    axes<strong class="lw lq">[</strong>row<strong class="lw lq">,</strong> 0<strong class="lw lq">].</strong>set_title<strong class="lw lq">(</strong>f"Fit after {n_trees} trees"<strong class="lw lq">,</strong> fontsize<strong class="lw lq">=</strong>15<strong class="lw lq">)</strong><br/>    axes<strong class="lw lq">[</strong>row<strong class="lw lq">,</strong> 1<strong class="lw lq">].</strong>scatter<strong class="lw lq">(</strong>sample_ix<strong class="lw lq">,</strong> res<strong class="lw lq">,</strong> alpha<strong class="lw lq">=</strong>0.7<strong class="lw lq">)</strong><br/>    axes<strong class="lw lq">[</strong>row<strong class="lw lq">,</strong> 1<strong class="lw lq">].</strong>plot<strong class="lw lq">(</strong>res<strong class="lw lq">,</strong> color<strong class="lw lq">=</strong>'r'<strong class="lw lq">,</strong> alpha<strong class="lw lq">=</strong>0.7<strong class="lw lq">)</strong><br/>    axes<strong class="lw lq">[</strong>row<strong class="lw lq">,</strong> 1<strong class="lw lq">].</strong>set_title<strong class="lw lq">(</strong>f"Residuals after {n_trees} trees"<strong class="lw lq">,</strong> fontsize<strong class="lw lq">=</strong>15<strong class="lw lq">)</strong></span></pre><h1 id="8830" class="jj jk jl bd jm jn lg jp jq jr lh jt ju jv li jx jy jz lj kb kc kd lk kf kg kh bi translated">就在残差里！</h1><p id="e1b8" class="pw-post-body-paragraph ki kj jl kk b kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf im bi translated">提升的工作原理是将后续模型与前一个模型的残差进行拟合。通常，绘制残差图是模型评估过程的一部分。通常你会检查残差以确保它们是随机的，并且没有明显的模式。如果残差中存在模式，则表明您缺少关于目标变量的关键信息，并且数据拟合不足。本质上，残差中的模式表示关于X和y之间关系的信息，这些信息<em class="mp">可以</em>建模。</p><p id="1a1a" class="pw-post-body-paragraph ki kj jl kk b kl ll kn ko kp lm kr ks kt ln kv kw kx lo kz la lb lp ld le lf im bi translated">Boosting通过拟合模型序列的残差来利用这种洞察力。这就是为什么我们在boosting中使用弱学习器——例如高度再分类的决策树——<strong class="kk lq">,我们希望每个学习器对数据进行欠填充，从而为下一个学习器提供纠正错误的机会</strong>(可能使用与前一个学习器不同的样本和特征空间)。</p><p id="f91e" class="pw-post-body-paragraph ki kj jl kk b kl ll kn ko kp lm kr ks kt ln kv kw kx lo kz la lb lp ld le lf im bi translated">我们可以在下面的图中看到这个过程。每幅图都显示了来自连续提升树的数据的拟合线。在面板1中，我们显示了第一个预测，很容易看出这在残差中留下了清晰的模式。在下一个面板中，我们显示了五棵提升树后的拟合度。这种提升为模型提供了更大的灵活性来适应数据，但它仍然在数据中留下了清晰的可利用模式。在接下来的四个面板中，我们增加了增强预测的数量，在最后一个面板中，您可以看到残差开始变得非常随机，并且该线似乎与数据相当吻合。</p><pre class="lr ls lt lu gt lv lw lx ly aw lz bi"><span id="d37c" class="ma jk jl lw b gy mb mc l md me">fig<strong class="lw lq">,</strong> axes <strong class="lw lq">=</strong> plt<strong class="lw lq">.</strong>subplots<strong class="lw lq">(</strong>nrows<strong class="lw lq">=</strong>6<strong class="lw lq">,</strong> ncols<strong class="lw lq">=</strong>2<strong class="lw lq">,</strong> figsize<strong class="lw lq">=(</strong>20<strong class="lw lq">,</strong>25<strong class="lw lq">))</strong><br/>plot_fits<strong class="lw lq">(</strong>1<strong class="lw lq">,</strong> 0<strong class="lw lq">)</strong><br/>plot_fits<strong class="lw lq">(</strong>5<strong class="lw lq">,</strong> 1<strong class="lw lq">)</strong><br/>plot_fits<strong class="lw lq">(</strong>15<strong class="lw lq">,</strong> 2<strong class="lw lq">)</strong><br/>plot_fits<strong class="lw lq">(</strong>20<strong class="lw lq">,</strong> 3<strong class="lw lq">)</strong><br/>plot_fits<strong class="lw lq">(</strong>25<strong class="lw lq">,</strong> 4<strong class="lw lq">)</strong><br/>plot_fits<strong class="lw lq">(</strong>30<strong class="lw lq">,</strong> 5<strong class="lw lq">)</strong><br/><br/>fig<strong class="lw lq">.</strong>tight_layout<strong class="lw lq">()</strong></span></pre><figure class="lr ls lt lu gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/121accb71b570dce79e38db879341669.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6w9E9tck6V9DOujGwTBxcw.png"/></div></div><p class="mh mi gj gh gi mj mk bd b be z dk translated"><em class="ml">作者图片</em></p></figure><p id="07d8" class="pw-post-body-paragraph ki kj jl kk b kl ll kn ko kp lm kr ks kt ln kv kw kx lo kz la lb lp ld le lf im bi translated">通过添加每个连续的预测集并绘制直线来绘制每个拟合数据也很有帮助。</p><pre class="lr ls lt lu gt lv lw lx ly aw lz bi"><span id="71d3" class="ma jk jl lw b gy mb mc l md me">plt<strong class="lw lq">.</strong>figure<strong class="lw lq">(</strong>figsize<strong class="lw lq">=(</strong>15<strong class="lw lq">,</strong> 4<strong class="lw lq">))</strong><br/><br/>pred <strong class="lw lq">=</strong> 0<br/><strong class="lw lq">for</strong> i <strong class="lw lq">in</strong> <em class="mp">range</em><strong class="lw lq">(</strong><em class="mp">len</em><strong class="lw lq">(</strong>learners<strong class="lw lq">)):</strong><br/>    pred <strong class="lw lq">+=</strong> boosted_yhat<strong class="lw lq">[:,</strong> i<strong class="lw lq">]</strong><br/>    plt<strong class="lw lq">.</strong>plot<strong class="lw lq">(</strong>xfit<strong class="lw lq">,</strong> pred<strong class="lw lq">)</strong><br/>plt<strong class="lw lq">.</strong>plot<strong class="lw lq">(</strong>xfit<strong class="lw lq">,</strong> predict<strong class="lw lq">(</strong>learners<strong class="lw lq">,</strong> X<strong class="lw lq">).</strong>sum<strong class="lw lq">(</strong>1<strong class="lw lq">))</strong><br/>plt<strong class="lw lq">.</strong>xlabel<strong class="lw lq">(</strong>"X"<strong class="lw lq">)</strong><br/>plt<strong class="lw lq">.</strong>ylabel<strong class="lw lq">(</strong>"y"<strong class="lw lq">);</strong><br/><br/>plt<strong class="lw lq">.</strong>scatter<strong class="lw lq">(</strong>X<strong class="lw lq">,</strong> y<strong class="lw lq">,</strong> alpha<strong class="lw lq">=.</strong>4<strong class="lw lq">)</strong></span></pre><figure class="lr ls lt lu gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi mq"><img src="../Images/711a5cf73d0a20c5e9d9c531bdf4eacd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XHcWiYPAHYhEfT9M0tT71g.png"/></div></div><p class="mh mi gj gh gi mj mk bd b be z dk translated"><em class="ml">作者图片</em></p></figure><p id="811e" class="pw-post-body-paragraph ki kj jl kk b kl ll kn ko kp lm kr ks kt ln kv kw kx lo kz la lb lp ld le lf im bi translated">很明显，除了这篇文章中所描述的，还有很多东西需要提升，但是我认为对这种技术起作用的核心原因有一个直观的理解是很有用的，希望这篇文章已经把这一点说清楚了！最后一个想法是，当您使用Boosting时，您需要仔细验证您的模型，因为这种方法很容易过度拟合。例如，如果我们将树的最大深度增加到4，我们可以观察到模型开始拟合单个数据点，而不是数据的总体趋势。</p><pre class="lr ls lt lu gt lv lw lx ly aw lz bi"><span id="33a2" class="ma jk jl lw b gy mb mc l md me">learners <strong class="lw lq">=</strong> many_trees<strong class="lw lq">(</strong>30<strong class="lw lq">,</strong> max_depth<strong class="lw lq">=</strong>4<strong class="lw lq">,</strong> clf<strong class="lw lq">=False)</strong><br/>fitted <strong class="lw lq">=</strong> boost<strong class="lw lq">(</strong>learners<strong class="lw lq">,</strong> X<strong class="lw lq">,</strong> y<strong class="lw lq">,</strong> clf<strong class="lw lq">=False)</strong><br/>boosted_yhat <strong class="lw lq">=</strong> predict<strong class="lw lq">(</strong>fitted<strong class="lw lq">,</strong> X<strong class="lw lq">)</strong><br/><br/>plt<strong class="lw lq">.</strong>figure<strong class="lw lq">(</strong>figsize<strong class="lw lq">=(</strong>15<strong class="lw lq">,</strong> 4<strong class="lw lq">))</strong><br/>plt<strong class="lw lq">.</strong>scatter<strong class="lw lq">(</strong>X<strong class="lw lq">,</strong> y<strong class="lw lq">,</strong> alpha<strong class="lw lq">=.</strong>4<strong class="lw lq">)</strong><br/>pred <strong class="lw lq">=</strong> 0<br/><strong class="lw lq">for</strong> i <strong class="lw lq">in</strong> <em class="mp">range</em><strong class="lw lq">(</strong><em class="mp">len</em><strong class="lw lq">(</strong>learners<strong class="lw lq">)):</strong><br/>    pred <strong class="lw lq">+=</strong> boosted_yhat<strong class="lw lq">[:,</strong> i<strong class="lw lq">]</strong><br/>    plt<strong class="lw lq">.</strong>plot<strong class="lw lq">(</strong>xfit<strong class="lw lq">,</strong> pred<strong class="lw lq">)</strong><br/>plt<strong class="lw lq">.</strong>plot<strong class="lw lq">(</strong>xfit<strong class="lw lq">,</strong> predict<strong class="lw lq">(</strong>learners<strong class="lw lq">,</strong> X<strong class="lw lq">).</strong>sum<strong class="lw lq">(</strong>1<strong class="lw lq">))</strong><br/>plt<strong class="lw lq">.</strong>xlabel<strong class="lw lq">(</strong>"X"<strong class="lw lq">)</strong><br/>plt<strong class="lw lq">.</strong>ylabel<strong class="lw lq">(</strong>"y"<strong class="lw lq">);</strong><br/><br/>sample_ix <strong class="lw lq">=</strong> np<strong class="lw lq">.</strong>arange<strong class="lw lq">(</strong>X<strong class="lw lq">.</strong>shape<strong class="lw lq">[</strong>0<strong class="lw lq">])</strong><br/>plt<strong class="lw lq">.</strong>figure<strong class="lw lq">(</strong>figsize<strong class="lw lq">=(</strong>15<strong class="lw lq">,</strong> 4<strong class="lw lq">))</strong><br/>plt<strong class="lw lq">.</strong>scatter<strong class="lw lq">(</strong>sample_ix<strong class="lw lq">,</strong> pred<strong class="lw lq">-</strong>y<strong class="lw lq">)</strong><br/>plt<strong class="lw lq">.</strong>plot<strong class="lw lq">(</strong>pred<strong class="lw lq">-</strong>y<strong class="lw lq">,</strong> color<strong class="lw lq">=</strong>'r'<strong class="lw lq">,</strong> alpha<strong class="lw lq">=</strong>0.4<strong class="lw lq">)</strong></span></pre><figure class="lr ls lt lu gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi mr"><img src="../Images/d88af9700cf75121f22715b2ad858289.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3_03RAYwSL3g2RUAjRAY-g.png"/></div></div><p class="mh mi gj gh gi mj mk bd b be z dk translated"><em class="ml">作者图片</em></p></figure><p id="6a0d" class="pw-post-body-paragraph ki kj jl kk b kl ll kn ko kp lm kr ks kt ln kv kw kx lo kz la lb lp ld le lf im bi translated">该模型已经学会在给定x的情况下完美地预测y，而不是创建随机分布的残差集。这看起来可能很好，但实际上该模型已经成为一个查找表，并且没有学会数据的基本结构。因此，在训练boosting模型时要做的一件好事是绘制残差图，这将使您清楚地了解您的模型是否过度拟合。</p><p id="2414" class="pw-post-body-paragraph ki kj jl kk b kl ll kn ko kp lm kr ks kt ln kv kw kx lo kz la lb lp ld le lf im bi translated">希望这对你有用。如有任何问题或想法，请联系下方。</p></div></div>    
</body>
</html>