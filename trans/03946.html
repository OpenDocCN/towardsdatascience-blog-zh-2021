<html>
<head>
<title>Augmentation Methods Using Albumentations And PyTorch</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用白蛋白和PyTorch的扩增方法</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/augmentation-methods-using-albumentations-and-pytorch-35cd135382f8?source=collection_archive---------21-----------------------#2021-04-02">https://towardsdatascience.com/augmentation-methods-using-albumentations-and-pytorch-35cd135382f8?source=collection_archive---------21-----------------------#2021-04-02</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="8e55" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph">增强管道</h2><div class=""/><div class=""><h2 id="2a0d" class="pw-subtitle-paragraph jw iz iq bd b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn dk translated">讨论最新的增强研究和使用所讨论的方法的新颖实现</h2></div><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ko"><img src="../Images/6c8e92a355c97bab8662862b98cef646.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-P2p3ho-r1KAHdhmGPag3Q.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">作者图片</p></figure><p id="d713" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">自从深度神经网络在20世纪90年代末成名以来，有限的数据一直是一块绊脚石。缺少数据会导致<em class="ma">过拟合，</em>尤其是对于具有大量参数的架构。幸运的是，增强对于数据有限的机器学习任务来说是一个启示。它提供了两个主要优势。</p><p id="2a52" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">首先，您可以通过创建原始数据集的多个扩充副本来增加数据量，这些副本也可用于平衡倾斜的数据集。</p><p id="70ea" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">第二，它迫使模型对于样本中的不相关特征是不变的，例如，人脸检测任务中的背景。这有助于模型更好地概括。</p><p id="2942" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">在本帖中，我们将探索最新的数据扩充方法，以及使用所讨论方法的新颖实现。我们将主要讨论两种方法，自动增强和随机增强。所以，让我们从简单介绍这两种方法开始，然后继续讨论实现。</p><h1 id="9de7" class="mb mc iq bd md me mf mg mh mi mj mk ml kf mm kg mn ki mo kj mp kl mq km mr ms bi translated">自动增强</h1><p id="2c8b" class="pw-post-body-paragraph le lf iq lg b lh mt ka lj lk mu kd lm ln mv lp lq lr mw lt lu lv mx lx ly lz ij bi translated">在“<a class="ae my" href="https://arxiv.org/abs/1805.09501" rel="noopener ugc nofollow" target="_blank"> <strong class="lg ja">中介绍的自动增强:从数据中学习增强策略</strong> </a>”试图自动选择应用于样本的变换的类型和幅度。他们提出了一种强化学习方法，通过离散化搜索问题来找到有效的数据扩充策略。所提出的方法找到增强策略<strong class="lg ja"> S </strong>，其具有关于变换、变换的幅度以及使用这些变换的概率的信息。</p><p id="5185" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">让我们看看算法的一些细节。</p><h2 id="4602" class="mz mc iq bd md na nb dn mh nc nd dp ml ln ne nf mn lr ng nh mp lv ni nj mr iw bi translated">搜索空间</h2><p id="690c" class="pw-post-body-paragraph le lf iq lg b lh mt ka lj lk mu kd lm ln mv lp lq lr mw lt lu lv mx lx ly lz ij bi translated">策略<strong class="lg ja"> S </strong>包含五个子策略，每个子策略有两个变换，每个变换有一个幅度和一个概率参数。</p><blockquote class="nk nl nm"><p id="fef1" class="le lf ma lg b lh li ka lj lk ll kd lm nn lo lp lq no ls lt lu np lw lx ly lz ij bi translated"><strong class="lg ja">注意</strong>这里的子策略对转换的顺序很敏感，如果应用程序的顺序颠倒了，则被视为不同的子策略。</p></blockquote><p id="39c1" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">有16种增强转换可供控制器选择(将在实现部分讨论)。每个变换的幅度和应用概率的范围分别被离散成10和11个均匀间隔的值。</p><h2 id="be72" class="mz mc iq bd md na nb dn mh nc nd dp ml ln ne nf mn lr ng nh mp lv ni nj mr iw bi translated">搜索算法</h2><p id="240e" class="pw-post-body-paragraph le lf iq lg b lh mt ka lj lk mu kd lm ln mv lp lq lr mw lt lu lv mx lx ly lz ij bi translated">搜索算法有两个组成部分:一个控制器，这是一个递归神经网络(RNN)，和训练算法，这是最接近的政策优化算法。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/e4db32822c56865059314b3b9a663148.png" data-original-src="https://miro.medium.com/v2/resize:fit:1142/format:webp/1*5z7T7hyOdcHKQ-bO1FwF4Q.png"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">来源:自动增强纸</p></figure><p id="afa7" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">控制器(RNN)从搜索空间预测增强策略<strong class="lg ja"> S </strong>，然后预测的策略用于训练小得多的子模型。子模型在通过对训练集应用5个预测子策略而生成的扩充数据上被训练。对于小批量中的每个示例，随机选择5个子策略中的一个来扩充图像。然后，子模型在一个单独的保留验证集上进行评估，以测量精确度<strong class="lg ja"> R </strong>，它被用作训练RNN控制器的奖励信号。</p><p id="7410" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">你可以在这里阅读全文<a class="ae my" href="https://arxiv.org/abs/1805.09501" rel="noopener ugc nofollow" target="_blank">了解所有细节。</a></p><p id="ac28" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">AutoAugment是第一批有效实现数据扩充自动化方法的论文之一，并显示出比以前的自动化方法好得多的结果。然而，单独的优化过程和非常大的搜索空间具有很高的计算成本。此外，该方法作出了一个强有力的假设，即来自一个较小网络的结果可以转移到一个大得多的网络。事实证明，这一假设并不完全正确，RandAugment论文中指出，该论文提出了一种更快且在某些情况下性能更好的数据扩充方法，我们将在接下来讨论该方法。</p><h1 id="6328" class="mb mc iq bd md me mf mg mh mi mj mk ml kf mm kg mn ki mo kj mp kl mq km mr ms bi translated">随机扩增</h1><p id="bc24" class="pw-post-body-paragraph le lf iq lg b lh mt ka lj lk mu kd lm ln mv lp lq lr mw lt lu lv mx lx ly lz ij bi translated">像AutoAugment这样的大多数已学习的增强算法创建一个代理任务，该代理任务在较小的网络上为最佳增强策略进行优化，如前一部分所述。这种方法有一些需要解决的基本缺陷。在“<a class="ae my" href="https://arxiv.org/abs/1909.13719" rel="noopener ugc nofollow" target="_blank"> <strong class="lg ja"> RandAugment:具有减少的搜索空间的实用自动数据扩充</strong> </a>”中介绍的RandAugment试图绕过这种代理任务，并且还极大地减少了搜索空间，以减轻训练过程的计算负荷。该方法将参数的数量减少到两个，<strong class="lg ja"> N </strong>和<strong class="lg ja"> M </strong>(将在随后的小节中详细讨论)，并在训练过程中将它们视为超参数。</p><h2 id="2a4b" class="mz mc iq bd md na nb dn mh nc nd dp ml ln ne nf mn lr ng nh mp lv ni nj mr iw bi translated">搜索空间</h2><p id="5390" class="pw-post-body-paragraph le lf iq lg b lh mt ka lj lk mu kd lm ln mv lp lq lr mw lt lu lv mx lx ly lz ij bi translated">RandAugment不是学习策略，而是以1/ <strong class="lg ja"> K </strong>的统一概率从一组<strong class="lg ja"> K </strong>转换中随机挑选一个转换。要挑选的转换数量由超参数<strong class="lg ja"> N </strong>给出，这将搜索空间中的总策略减少到<strong class="lg ja"> K pow N </strong>。学习增强方法的主要好处来自增加样本的多样性，RandAugment采用的均匀概率方法保持了样本的多样性，但搜索空间显著减小。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi nr"><img src="../Images/5f2feeb04e48a96087a7d3d002c14ebb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WH4yNuDLWYd6A3qKwBqUYQ.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">资料来源:RandAugment Paper</p></figure><p id="128e" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">最后，类似于自动增强的每个变换的幅度范围被离散成由<strong class="lg ja"> M </strong>给出的10个均匀间隔的值，然而，与已知的方法相反，所有的变换都遵循用于<strong class="lg ja"> M </strong>的相同的时间表，这将幅度的搜索空间减少到仅仅一个。这是因为增强的最佳幅度取决于模型和训练集的大小，更大的网络需要更大的数据失真(请参考上图)，这在直觉上是有意义的。该论文还指出，更大的数据集需要更大的数据失真，这似乎很奇怪，作者提出的一个假设是，小数据集的激进增强可能会导致低信噪比。在任何情况下，基于模型大小和训练集大小，所有变换都遵循最佳幅度的明确模式，这暴露了使用较小网络来估计所有任务的变换幅度的已有增强方法中的基本缺陷。</p><h2 id="8512" class="mz mc iq bd md na nb dn mh nc nd dp ml ln ne nf mn lr ng nh mp lv ni nj mr iw bi translated">搜索算法</h2><p id="f33f" class="pw-post-body-paragraph le lf iq lg b lh mt ka lj lk mu kd lm ln mv lp lq lr mw lt lu lv mx lx ly lz ij bi translated">RandAugment使用简单的网格搜索来寻找<strong class="lg ja"> N </strong>和<strong class="lg ja"> M </strong>的最优值，这是通过为两个超参数选择一组值并在单独的验证集上测试这些组的所有排列来完成的。最后，选择对验证集有最佳改进的组合。可以对每个样品、每个批次或每个时期执行该程序。</p><p id="e107" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">现在，我们已经对算法有了相当好的理解，让我们向实现前进。如前所述，RandAugment是一种更好的算法，但是，AutoAugment论文中的一些关键发现可以与RandAugment结合使用，以进一步降低计算成本。</p><p id="08aa" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">你可以从<a class="ae my" href="https://github.com/Varun9213/Blog_machine_learing/blob/main/Augmentaion_blog_3.ipynb" rel="noopener ugc nofollow" target="_blank">这里</a>下载这篇文章的笔记本，笔记本上有所有必要的代码来设置增强算法，并在fastai提供的一小部分Imagenet数据集上用Resnet50进行测试。然而，在这篇文章中，我们将只关注笔记本中与增强相关的部分。</p><h1 id="13fb" class="mb mc iq bd md me mf mg mh mi mj mk ml kf mm kg mn ki mo kj mp kl mq km mr ms bi translated">履行</h1><p id="87e6" class="pw-post-body-paragraph le lf iq lg b lh mt ka lj lk mu kd lm ln mv lp lq lr mw lt lu lv mx lx ly lz ij bi translated">让我们从笔记本上的自定义RandAugment函数开始，然后对其进行分解。</p><pre class="kp kq kr ks gt ns nt nu nv aw nw bi"><span id="4666" class="mz mc iq nt b gy nx ny l nz oa">def randAugment(N, M, p, mode="all", cut_out = False):</span><span id="c38d" class="mz mc iq nt b gy ob ny l nz oa">  <strong class="nt ja"># Magnitude(M) search space  </strong></span><span id="81ae" class="mz mc iq nt b gy ob ny l nz oa">  shift_x = np.linspace(0,150,10)<br/>  shift_y = np.linspace(0,150,10)<br/>  rot = np.linspace(0,30,10)<br/>  shear = np.linspace(0,10,10)<br/>  sola = np.linspace(0,256,10)<br/>  post = [4,4,5,5,6,6,7,7,8,8]<br/>  cont = [np.linspace(-0.8,-0.1,10),np.linspace(0.1,2,10)]<br/>  bright = np.linspace(0.1,0.7,10)<br/>  shar = np.linspace(0.1,0.9,10)<br/>  cut = np.linspace(0,60,10)</span><span id="9918" class="mz mc iq nt b gy ob ny l nz oa"><strong class="nt ja">  # Transformation search space</strong></span><span id="76a7" class="mz mc iq nt b gy ob ny l nz oa">  Aug =[#0 - geometrical<br/>        A.ShiftScaleRotate(shift_limit_x=shift_x[M], rotate_limit=0,   shift_limit_y=0, shift_limit=shift_x[M], p=p),<br/>        A.ShiftScaleRotate(shift_limit_y=shift_y[M], rotate_limit=0, shift_limit_x=0, shift_limit=shift_y[M], p=p),<br/>        A.IAAAffine(rotate=rot[M], p=p),<br/>        A.IAAAffine(shear=shear[M], p=p),<br/>        A.InvertImg(p=p),<br/>        #5 - Color Based<br/>        A.Equalize(p=p),<br/>        A.Solarize(threshold=sola[M], p=p),<br/>        A.Posterize(num_bits=post[M], p=p),<br/>        A.RandomContrast(limit=[cont[0][M], cont[1][M]], p=p),<br/>        A.RandomBrightness(limit=bright[M], p=p),<br/>        A.IAASharpen(alpha=shar[M], lightness=shar[M], p=p)]</span><span id="7ee1" class="mz mc iq nt b gy ob ny l nz oa"><strong class="nt ja">  # Sampling from the Transformation search space</strong></span><span id="7119" class="mz mc iq nt b gy ob ny l nz oa">  if mode == "geo": <br/>    ops = np.random.choice(Aug[0:5], N)<br/>  elif mode == "color": <br/>    ops = np.random.choice(Aug[5:], N)<br/>  else:<br/>    ops = np.random.choice(Aug, N)<br/>  <br/>  if cut_out:<br/>    ops.append(A.Cutout(num_holes=8, max_h_size=int(cut[M]),   max_w_size=int(cut[M]), p=p))</span><span id="a6af" class="mz mc iq nt b gy ob ny l nz oa">  transforms = A.Compose(ops)<br/>  return transforms, ops</span></pre><p id="03ba" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">该功能大致可分为以下两部分。</p><h2 id="4bd9" class="mz mc iq bd md na nb dn mh nc nd dp ml ln ne nf mn lr ng nh mp lv ni nj mr iw bi translated">量值搜索空间</h2><p id="2911" class="pw-post-body-paragraph le lf iq lg b lh mt ka lj lk mu kd lm ln mv lp lq lr mw lt lu lv mx lx ly lz ij bi translated">本节将星等范围离散为10个均匀分布的值。</p><h2 id="4073" class="mz mc iq bd md na nb dn mh nc nd dp ml ln ne nf mn lr ng nh mp lv ni nj mr iw bi translated">变换搜索空间</h2><p id="ec8f" class="pw-post-body-paragraph le lf iq lg b lh mt ka lj lk mu kd lm ln mv lp lq lr mw lt lu lv mx lx ly lz ij bi translated">"<strong class="lg ja"> Aug </strong>"是变换搜索空间，算法可以从中以均匀的概率挑选出<strong class="lg ja"> N个</strong>变换。请花点时间看看这些单独的转换做了什么，以便更好地创建这个搜索空间。</p><blockquote class="nk nl nm"><p id="eac4" class="le lf ma lg b lh li ka lj lk ll kd lm nn lo lp lq no ls lt lu np lw lx ly lz ij bi translated">注意该功能有三种模式“<strong class="lg ja">地理</strong>”、<strong class="lg ja">颜色</strong>、<strong class="lg ja">全部</strong>”。“地理”模式适用于基于颜色的特征定义图像中感兴趣对象的任务，例如火灾探测。类似地,“颜色”模式适用于形状定义感兴趣对象的任务，例如人脸检测、汽车检测等。然而，“全部”模式使用所有的转换。</p></blockquote><p id="6a7f" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">AutoAugment论文中的作者表明，不同的数据集对正向和负向的不同变换都很敏感。因此,“没有免费的午餐”定理也适用于数据扩充，你需要为每个项目调整算法。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi oc"><img src="../Images/a0cc1cdc34bd61c9d96f627f40aca089.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Gjzi4A83XtT7k0FhyNb_Ag.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">来源:自动增强纸</p></figure><blockquote class="nk nl nm"><p id="5af2" class="le lf ma lg b lh li ka lj lk ll kd lm nn lo lp lq no ls lt lu np lw lx ly lz ij bi translated"><strong class="lg ja">注意</strong>上表中给出的幅度范围是针对PIL库中的函数，然而，我们在randAugment()函数中使用白蛋白，主要是因为白蛋白库使用的open cv比PIL快。您必须进行实验，并为您喜欢使用的库计算出每个变换的幅度范围。</p></blockquote><p id="c11b" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">现在让我们来看看训练期间的转变幅度的时间表。</p><pre class="kp kq kr ks gt ns nt nu nv aw nw bi"><span id="b2b3" class="mz mc iq nt b gy nx ny l nz oa">def train_model(model, criterion, optimizer, scheduler, <strong class="nt ja">aug_mode="all"</strong>, <strong class="nt ja">cut_out=False</strong>, <strong class="nt ja">max_M=9</strong>, num_epochs=25):</span><span id="3282" class="mz mc iq nt b gy ob ny l nz oa">  best_model_wts = copy.deepcopy(model.state_dict())<br/>  best_acc = 0.0</span><span id="a2ed" class="mz mc iq nt b gy ob ny l nz oa"><strong class="nt ja">  N=2;M=0;p=0.5</strong></span><span id="00cd" class="mz mc iq nt b gy ob ny l nz oa">  for epoch in range(num_epochs):<br/>        print('Epoch {}/{}'.format(epoch, num_epochs - 1))<br/>        print('-' * 10)<br/>    <br/>        <strong class="nt ja">transforms, ops = randAugment(N=N, M=M, p=p, mode=aug_mode, cut_out=cut_out)</strong><br/>        dataloaders, dataset_sizes = create_dataloaders(root, train_df, valid_df, label_dict, bs=32, <strong class="nt ja">transforms=transforms</strong>)</span><span id="7dd5" class="mz mc iq nt b gy ob ny l nz oa">        # Each epoch has a training and validation phase<br/>        for phase in ['train', 'val']:<br/>            if phase == 'train':<br/>                model.train()  # Set model to training mode<br/>            else:<br/>                model.eval()   # Set model to evaluate mode</span><span id="038e" class="mz mc iq nt b gy ob ny l nz oa">            running_loss = 0.0<br/>            running_corrects = 0</span><span id="01a7" class="mz mc iq nt b gy ob ny l nz oa">            # Iterate over data.<br/>            for inputs, labels in dataloaders[phase]:<br/>                inputs = inputs.to(device)<br/>                labels = labels.to(device)</span><span id="b65a" class="mz mc iq nt b gy ob ny l nz oa">                # zero the parameter gradients<br/>                optimizer.zero_grad()</span><span id="4050" class="mz mc iq nt b gy ob ny l nz oa">                # forward<br/>                # track history if only in train<br/>                with torch.set_grad_enabled(phase == 'train'):<br/>                    outputs = model(inputs)<br/>                    preds = torch.argmax(input = outputs, dim = 1)<br/>                    loss = criterion(outputs, labels)</span><span id="93ea" class="mz mc iq nt b gy ob ny l nz oa">                    # backward + optimize only if in training phase<br/>                    if phase == 'train':<br/>                        loss.backward()<br/>                        optimizer.step()</span><span id="014d" class="mz mc iq nt b gy ob ny l nz oa">                # statistics<br/>                running_loss += loss.item() * inputs.size(0)<br/>                running_corrects += torch.sum(preds ==          torch.argmax(input = labels, dim = 1))<br/>            if phase == 'train':<br/>                scheduler.step()</span><span id="c740" class="mz mc iq nt b gy ob ny l nz oa">            epoch_loss = running_loss / dataset_sizes[phase]<br/>            epoch_acc = running_corrects.double() /  dataset_sizes[phase]</span><span id="bb28" class="mz mc iq nt b gy ob ny l nz oa">                print('{} Loss: {:.4f} Acc: {:.4f}'.format(<br/>                phase, epoch_loss, epoch_acc))</span><span id="0f6c" class="mz mc iq nt b gy ob ny l nz oa">            # deep copy the model<br/>            if phase == 'val' and epoch_acc &gt; best_acc:<br/>                best_acc = epoch_acc<br/>                best_model_wts = copy.deepcopy(model.state_dict())<br/>            <strong class="nt ja">elif phase == 'val' and epoch_acc &lt; best_acc:<br/>                stp = int((9-M)*((best_acc-epoch_acc)/best_acc))<br/>                M += max(1,stp)<br/>                M = min(M, max_M)<br/>                if M &lt; max_M:<br/>                  print("Augmentaion Magnitude Changed To : {}\n".format(M))</strong></span><span id="91e1" class="mz mc iq nt b gy ob ny l nz oa">    print('Best val Acc: {:4f}'.format(best_acc))</span><span id="0ece" class="mz mc iq nt b gy ob ny l nz oa">    # load best model weights<br/>    model.load_state_dict(best_model_wts)<br/>    return model</span></pre><p id="edb4" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">在上面的代码块中，与时间表相关的代码行被加粗。</p><p id="802d" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">在这个<strong class="lg ja"> </strong>示例中，我为<strong class="lg ja"> M </strong>使用了预定的时间表，每当历元精度低于验证集上的最佳精度时，<strong class="lg ja"> M </strong>就递增。根据最大可能值(9)和当前值<strong class="lg ja"> M </strong>之间的差值计算出<strong class="lg ja"> M </strong>将增加的数量，当前值由两个精度和最佳精度之间的差值的比率缩放。</p><blockquote class="nk nl nm"><p id="ee4b" class="le lf ma lg b lh li ka lj lk ll kd lm nn lo lp lq no ls lt lu np lw lx ly lz ij bi translated"><strong class="lg ja">注意</strong>该时间表不会产生额外成本，但是非常严格，RandAugment paper在每批之后对N和M进行网格搜索，以找到它们的最佳值。然而，即使与自动增强相比计算成本大幅降低，对于大多数机器学习项目来说，网格搜索对于具有适度设置和单个gpu的人来说仍然是不可行的。</p></blockquote><p id="cf5a" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">此实现中使用的时间表不是唯一可用的时间表，您可以试验每个子策略的转换数量<strong class="lg ja"> N </strong>以及这些转换的概率<strong class="lg ja"> p. </strong>增强不是一门精确的科学，每个锁都有不同的密钥，您需要通过试验和创造性思维来找到匹配的密钥。</p><h1 id="f22c" class="mb mc iq bd md me mf mg mh mi mj mk ml kf mm kg mn ki mo kj mp kl mq km mr ms bi translated">参考</h1><p id="c613" class="pw-post-body-paragraph le lf iq lg b lh mt ka lj lk mu kd lm ln mv lp lq lr mw lt lu lv mx lx ly lz ij bi translated">自动增强纸:<a class="ae my" href="https://arxiv.org/abs/1805.09501" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1805.09501</a></p><p id="e7a1" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">RandAugment论文:<a class="ae my" href="https://arxiv.org/abs/1909.13719" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1909.13719</a></p></div></div>    
</body>
</html>