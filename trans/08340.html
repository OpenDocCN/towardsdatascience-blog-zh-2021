<html>
<head>
<title>Natural Language Processing — Dependency Parsing</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">自然语言处理——依存句法分析</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/natural-language-processing-dependency-parsing-cf094bbbe3f7?source=collection_archive---------3-----------------------#2021-08-01">https://towardsdatascience.com/natural-language-processing-dependency-parsing-cf094bbbe3f7?source=collection_archive---------3-----------------------#2021-08-01</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="c88c" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsdatascience.com/tagged/hands-on-tutorials" rel="noopener" target="_blank">实践教程</a></h2><div class=""/><div class=""><h2 id="1859" class="pw-subtitle-paragraph jw iz iq bd b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn dk translated">使用spaCy、NLTK和Stanford CoreNLP和Stanza进行依赖解析的不同方法</h2></div><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ko"><img src="../Images/a8556bfb7f37d65f6a9c5162f076b231.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*QolbwbdqxGSmKzuu"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">照片由<a class="ae le" href="https://unsplash.com/@siora18?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Siora摄影</a>在<a class="ae le" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>拍摄</p></figure><h1 id="61c4" class="lf lg iq bd lh li lj lk ll lm ln lo lp kf lq kg lr ki ls kj lt kl lu km lv lw bi translated">什么是依存解析？</h1><p id="b6db" class="pw-post-body-paragraph lx ly iq lz b ma mb ka mc md me kd mf mg mh mi mj mk ml mm mn mo mp mq mr ms ij bi translated">依存句法分析是分析句子的语法结构，找出相关词以及它们之间的关系类型的过程。</p><p id="4d2a" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated">每种关系:</p><ol class=""><li id="d195" class="my mz iq lz b ma mt md mu mg na mk nb mo nc ms nd ne nf ng bi translated">有一个<strong class="lz ja">头</strong> <em class="nh"> </em>和一个修改<strong class="lz ja">头</strong>的<strong class="lz ja">从属</strong>。</li><li id="95af" class="my mz iq lz b ma ni md nj mg nk mk nl mo nm ms nd ne nf ng bi translated">根据<strong class="lz ja">头</strong>和<strong class="lz ja">被从属</strong>之间的从属性质进行标注。这些标签可以在<a class="ae le" href="https://universaldependencies.org/u/dep/" rel="noopener ugc nofollow" target="_blank">通用依赖关系</a>中找到。</li></ol><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/4ece5b9b7e4535cbf54812e82164e0b6.png" data-original-src="https://miro.medium.com/v2/resize:fit:656/format:webp/1*NrOGJABfU_dp2qOasutr_Q.png"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">两个词之间的简单依存关系</p></figure><p id="f361" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated">在上图中，<em class="nh">车</em> <strong class="lz ja"> <em class="nh"> </em> </strong>和<em class="nh">黑</em>之间存在关系，因为<em class="nh">黑</em>修饰了<em class="nh">车</em>的含义。这里，<em class="nh">车</em>充当<strong class="lz ja">头</strong>而<em class="nh">黑</em>是<strong class="lz ja">头的从属</strong>。这里关系的本质是<strong class="lz ja"> amod </strong>，代表“形容词修饰语”。它是修饰名词的形容词或形容词短语。</p><h1 id="ccf4" class="lf lg iq bd lh li lj lk ll lm ln lo lp kf lq kg lr ki ls kj lt kl lu km lv lw bi translated">履行</h1><p id="88d1" class="pw-post-body-paragraph lx ly iq lz b ma mb ka mc md me kd mf mg mh mi mj mk ml mm mn mo mp mq mr ms ij bi translated">在Python中实现依赖解析有不同的方法。在本文中，我们将研究三种方法。</p><h2 id="76ec" class="no lg iq bd lh np nq dn ll nr ns dp lp mg nt nu lr mk nv nw lt mo nx ny lv iw bi translated">方法1:使用空间</h2><p id="64bc" class="pw-post-body-paragraph lx ly iq lz b ma mb ka mc md me kd mf mg mh mi mj mk ml mm mn mo mp mq mr ms ij bi translated">spaCy是一个用于自然语言处理的开源Python库。</p><p id="fb82" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated">要开始，首先安装spaCy并加载所需的语言模型。</p><pre class="kp kq kr ks gt nz oa ob oc aw od bi"><span id="a0b5" class="no lg iq oa b gy oe of l og oh">pip install -U pip setuptools wheel<br/>pip install -U spacy<br/>python -m spacy download en_core_web_sm</span></pre><p id="fedd" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated"><strong class="lz ja"> en_core_web_sm </strong>是spaCy现有最小的英文型号，大小为12MB。参考<a class="ae le" href="https://spacy.io/models/en" rel="noopener ugc nofollow" target="_blank"> spaCy英国型号</a>查看其他可用型号。</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="oi oj l"/></div></figure><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ok"><img src="../Images/529dadd2b1290c7f07d0defaac972eee.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BIWcMmbtcQz2cnrZOJu2Nw.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">使用空间进行依存解析</p></figure><p id="7513" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated">spaCy还提供了一个名为<strong class="lz ja"> displaCy </strong>的内置依赖可视化工具，可以用来生成句子的依赖图。</p><p id="97ca" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated"><code class="fe ol om on oa b">displacy.render()</code>功能将生成句子的可视化。</p><blockquote class="oo op oq"><p id="0485" class="lx ly nh lz b ma mt ka mc md mu kd mf or mv mi mj os mw mm mn ot mx mq mr ms ij bi translated"><strong class="lz ja">注意:</strong>如果运行的是Jupyter Notebook或者Google Colab中的代码，使用<code class="fe ol om on oa b">render()</code>函数中的<code class="fe ol om on oa b">jupyter = True</code>。</p></blockquote><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ou"><img src="../Images/6b0cd457c50ff418f7dc14b3ea136a1e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mfcStHLTzMZC1evPaSJaag.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">依赖关系可视化的输出</p></figure><h2 id="14b7" class="no lg iq bd lh np nq dn ll nr ns dp lp mg nt nu lr mk nv nw lt mo nx ny lv iw bi translated">方法2:将NLTK与斯坦福CoreNLP一起使用</h2><p id="becd" class="pw-post-body-paragraph lx ly iq lz b ma mb ka mc md me kd mf mg mh mi mj mk ml mm mn mo mp mq mr ms ij bi translated">由斯坦福NLP小组创建的CoreNLP提供了Java语言的NLP工具。这个Java库可以和NLTK一起使用来解析Python中的依赖关系。</p><p id="1d9b" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated">第一步，从<a class="ae le" href="https://stanfordnlp.github.io/CoreNLP/download.html" rel="noopener ugc nofollow" target="_blank"> CoreNLP网站</a>下载斯坦福CoreNLP zip文件和斯坦福CoreNLP模型jar文件。</p><p id="5cc0" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated">运行这三个命令将下载所需的库并解压缩zip文件。</p><pre class="kp kq kr ks gt nz oa ob oc aw od bi"><span id="a9dc" class="no lg iq oa b gy oe of l og oh">wget <a class="ae le" href="https://nlp.stanford.edu/software/stanford-corenlp-4.2.2.zip" rel="noopener ugc nofollow" target="_blank">https://nlp.stanford.edu/software/stanford-corenlp-4.2.2.zip</a><br/>wget <a class="ae le" href="https://nlp.stanford.edu/software/stanford-corenlp-4.2.2-models-english.jar" rel="noopener ugc nofollow" target="_blank">https://nlp.stanford.edu/software/stanford-corenlp-4.2.2-models-english.jar</a><br/>unzip /content/stanford-corenlp-4.2.2.zip</span></pre><p id="202c" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated">一旦下载了这些库，我们就可以从NLTK导入StanfordDependencyParser。</p><pre class="kp kq kr ks gt nz oa ob oc aw od bi"><span id="2b50" class="no lg iq oa b gy oe of l og oh">from nltk.parse.stanford import StanfordDependencyParser</span></pre><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="oi oj l"/></div></figure><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ov"><img src="../Images/fe457591e4994574456839e495653558.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gakpXaH5miMKZLua-a82-A.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">使用NLTK和Stanford CoreNLP进行依存解析</p></figure><p id="a7f9" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated">为了可视化CoreNLP生成的依赖关系，我们可以使用<code class="fe ol om on oa b">dependency.nx_graph()</code>函数提取一个有标签和有方向的NetworkX图形对象，或者使用<code class="fe ol om on oa b">dependency.to_dot()</code>函数在图形描述语言中生成一个点定义。使用<a class="ae le" href="http://graphs.grevian.org/graph" rel="noopener ugc nofollow" target="_blank"> GraphViz </a>可以将点定义可视化为图形。</p><p id="80d9" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated"><strong class="lz ja">使用NetworkX可视化</strong></p><pre class="kp kq kr ks gt nz oa ob oc aw od bi"><span id="1b85" class="no lg iq oa b gy oe of l og oh"># importing networkx<br/>import networkx as nx<br/>import matplotlib.pyplot as plt</span><span id="eb2b" class="no lg iq oa b gy ow of l og oh"># Using reverse() to reverse the direction of edges as nx_graph() returns inverted edges<br/>G = dependency.nx_graph().reverse()</span><span id="2a1f" class="no lg iq oa b gy ow of l og oh"># nx_graph() returns numeric node labels starting from 1<br/># Create a dictionary to map numeric nodes and words in the sentence<br/>words = sentence.split(" ")<br/>labels = {index + 1: words[index] for index in range(len(words))}</span><span id="80aa" class="no lg iq oa b gy ow of l og oh">nx.draw(G, with_labels=True, labels=labels, node_size=2500, node_color='#B5EAD7', font_size=10)</span></pre><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi ox"><img src="../Images/bba23ce4d1477b24ec3c05fc8d419ec6.png" data-original-src="https://miro.medium.com/v2/resize:fit:892/format:webp/1*TeFVGbmlzR45CfRV7MAfyQ.png"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">从NetworkX对象创建的依赖关系图</p></figure><p id="41d9" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated"><strong class="lz ja">使用点定义可视化</strong></p><pre class="kp kq kr ks gt nz oa ob oc aw od bi"><span id="204e" class="no lg iq oa b gy oe of l og oh">!pip install graphviz</span><span id="ffd9" class="no lg iq oa b gy ow of l og oh">from graphviz import Source<br/>dot_def = dependency.to_dot()</span><span id="449e" class="no lg iq oa b gy ow of l og oh">'''<br/>The below lines require graphviz executables to be installed to your system. If that does not work, print the dot_def object and paste the output on http://graphs.grevian.org/graph/<br/>'''<br/>source = Source(dot_def, filename="dependency_graph", format="png")<br/>source.view()</span></pre><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi oy"><img src="../Images/005decc2c9076a10de96d5315a84139b.png" data-original-src="https://miro.medium.com/v2/resize:fit:532/0*1vOZtc8unoB2h8Cl"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">从点定义创建的依赖关系图</p></figure><h2 id="c001" class="no lg iq bd lh np nq dn ll nr ns dp lp mg nt nu lr mk nv nw lt mo nx ny lv iw bi translated">方法3:使用节</h2><p id="b2a5" class="pw-post-body-paragraph lx ly iq lz b ma mb ka mc md me kd mf mg mh mi mj mk ml mm mn mo mp mq mr ms ij bi translated">斯坦福大学NLP小组也开发了Stanza。它提供了一个可以定制的神经网络NLP管道和一个基于Stanford CoreNLP包的Python包装器，使得在不下载jar文件的情况下使用CoreNLP特性变得更加容易。</p><p id="f0e6" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated">要开始，请安装Stanza</p><pre class="kp kq kr ks gt nz oa ob oc aw od bi"><span id="6bb3" class="no lg iq oa b gy oe of l og oh">pip install stanza</span></pre><p id="18a8" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated">下一步是导入stanza并下载所需的语言模型。您可以在此查看所有可用型号<a class="ae le" href="https://stanfordnlp.github.io/stanza/available_models.html" rel="noopener ugc nofollow" target="_blank">。</a></p><pre class="kp kq kr ks gt nz oa ob oc aw od bi"><span id="aa7c" class="no lg iq oa b gy oe of l og oh">import stanza<br/>stanza.download('en')</span></pre><p id="da58" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated">使用<code class="fe ol om on oa b">stanza.Pipeline()</code>函数初始化神经管道。第一个参数是要使用的语言。可以传递一个可选参数<code class="fe ol om on oa b">processors</code>，该参数可以是一个字典或逗号分隔的字符串，以配置处理器在流水线中使用。</p><pre class="kp kq kr ks gt nz oa ob oc aw od bi"><span id="67d0" class="no lg iq oa b gy oe of l og oh">nlp = stanza.Pipeline('en',<br/>                     processors = 'tokenize,mwt,pos,lemma,depparse')</span></pre><p id="6d65" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated">您可以找到<a class="ae le" href="https://stanfordnlp.github.io/stanza/pipeline.html#processors" rel="noopener ugc nofollow" target="_blank">管道上所有处理器和处理器</a>的列表。有些处理器可能需要在流水线中的其他处理器之前，否则它们将无法工作。例如，<code class="fe ol om on oa b">pos</code>处理器需要<code class="fe ol om on oa b">tokenize</code>和<code class="fe ol om on oa b">mwt</code>处理器，所以如果我们想使用<code class="fe ol om on oa b">pos</code>处理器，也需要在流水线中使用这两个处理器。</p><p id="1e0a" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated">我们现在将通过管道传递我们的句子，并将所有结果存储在<code class="fe ol om on oa b">doc</code>变量中。</p><pre class="kp kq kr ks gt nz oa ob oc aw od bi"><span id="a2a2" class="no lg iq oa b gy oe of l og oh">doc = nlp(sentence)</span></pre><p id="80c3" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated">如果我们打印<code class="fe ol om on oa b">doc.sentences</code>，我们将看到通过管道传递的每个句子的列表。每个列表包含所有标记信息和语言特征的结果。</p><p id="1f4c" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated">例如，列表中的每一项都是一个对象，具有表示<em class="nh">词条</em>、<em class="nh">通用词性</em>、<em class="nh">树库专有词性</em>、<em class="nh">形态特征</em>、<em class="nh">中心词索引</em>、<em class="nh">在句子中的位置</em>等属性。</p><pre class="kp kq kr ks gt nz oa ob oc aw od bi"><span id="065a" class="no lg iq oa b gy oe of l og oh">{<br/>    "id": 1,   -----&gt; index of word in the sentence, starting from 1<br/>    "text": "Deemed", -----&gt; actual word<br/>    "lemma": "Deem",  -----&gt; lemma of the word<br/>    "upos": "VERB", -----&gt; universal POS<br/>    "xpos": "VBN", -----&gt; treebank specific POS<br/>    "feats": "Tense=Past|VerbForm=Part",-----&gt; morphological feature<br/>    "head": 2, -----&gt; index of the head<br/>    "deprel": "amod", -----&gt; dependency relationship nature<br/>    "start_char": 0, -----&gt; start index of word in the sentence<br/>    "end_char": 6  -----&gt; end index of word in the sentence<br/>}</span></pre><p id="b629" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated">对于<code class="fe ol om on oa b">doc</code>对象中的所有句子，我们可以调用<code class="fe ol om on oa b">print_dependencies()</code>函数。这个函数将打印具有三个值的元组——令牌、头部的索引和关系性质。</p><blockquote class="oo op oq"><p id="4a37" class="lx ly nh lz b ma mt ka mc md mu kd mf or mv mi mj os mw mm mn ot mx mq mr ms ij bi translated"><strong class="lz ja">注意:</strong>在我们的例子中，因为我们只有一个句子，所以我们使用<code class="fe ol om on oa b">doc.sentences[0]</code>直接访问它</p></blockquote><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="oi oj l"/></div></figure><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi oz"><img src="../Images/9f949f89e51e88b7b1b9612908ea221a.png" data-original-src="https://miro.medium.com/v2/resize:fit:680/format:webp/1*I8S9-6mbqeovqKzDnI6FPA.png"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">print_dependencies()函数的输出</p></figure><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi pa"><img src="../Images/c4b43e6de3da320ecf0a76841f1f6d87.png" data-original-src="https://miro.medium.com/v2/resize:fit:1092/format:webp/1*YP7Ey_dqnHJ_8iOW--h5uQ.png"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">使用Stanza进行依赖解析</p></figure><p id="ba98" class="pw-post-body-paragraph lx ly iq lz b ma mt ka mc md mu kd mf mg mv mi mj mk mw mm mn mo mx mq mr ms ij bi translated">因此，在本文中，我们看到了用Python实现文本依赖解析的三种方法。感谢您的阅读！:)如果对实现有疑问，可以在<a class="ae le" href="https://www.linkedin.com/in/shivaneej/" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>上联系我。</p><h1 id="2d73" class="lf lg iq bd lh li lj lk ll lm ln lo lp kf lq kg lr ki ls kj lt kl lu km lv lw bi translated">参考</h1><ol class=""><li id="393f" class="my mz iq lz b ma mb md me mg pb mk pc mo pd ms nd ne nf ng bi translated">空间依赖解析器—<a class="ae le" href="https://spacy.io/usage/linguistic-features#dependency-parse" rel="noopener ugc nofollow" target="_blank">https://spacy.io/usage/linguistic-features#dependency-parse</a></li><li id="d204" class="my mz iq lz b ma ni md nj mg nk mk nl mo nm ms nd ne nf ng bi translated">斯坦福·科伦普—<a class="ae le" href="https://stanfordnlp.github.io/CoreNLP/index.html" rel="noopener ugc nofollow" target="_blank">https://stanfordnlp.github.io/CoreNLP/index.html</a></li><li id="4704" class="my mz iq lz b ma ni md nj mg nk mk nl mo nm ms nd ne nf ng bi translated">小节依赖解析器—<a class="ae le" href="https://stanfordnlp.github.io/stanza/depparse.html" rel="noopener ugc nofollow" target="_blank">https://stanfordnlp.github.io/stanza/depparse.html</a></li></ol></div></div>    
</body>
</html>