<html>
<head>
<title>Decision Trees Under the Hood: Partitioning Data</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">引擎盖下的决策树:划分数据</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/decision-trees-under-the-hood-partitioning-data-b8b3922f5738?source=collection_archive---------24-----------------------#2021-12-16">https://towardsdatascience.com/decision-trees-under-the-hood-partitioning-data-b8b3922f5738?source=collection_archive---------24-----------------------#2021-12-16</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="8a7f" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">决策树如何为类和数值预测拆分数据</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/8b385229fd7d72bf5f918d1cd3fe6e56.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*FxDOdxG5Rb-cbBX5"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">里卡多·戈麦斯·安吉尔在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="2f45" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">人类总是用“如果”来做二元决策..然后..否则..”决策过程的结构。如果下雨，带把伞；如果它能飞，它就是一只鸟；如果这只鸟听起来嘎嘎叫，那它就是一只鸭子；如果动物有四条腿…我们需要更多的信息，那里有许多四条腿的动物。</p><p id="fcd8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">决策树使用相关特征(也称为变量)以类似的方式重复分割数据。</p><p id="58ea" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们以两种四条腿的动物——狗和猫为例。闭着眼睛，有人可以根据猫和狗独特的(看不见的)特征，如体重和声音，来区分它们。</p><p id="c106" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">假设你在用体重:体重高于 10 公斤的是狗，低于 10 公斤的是猫。但是你可以想象，有些狗和猫不能仅仅通过体重来区分，因为有些狗可能太小，有些猫可能太大。在这些情况下，你可以使用另一个特征:它们听起来像什么——喵还是汪？这一特征清楚地区分了剩下的不能单独用体重来区分的动物。</p><p id="011b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在，如果我给你带来一种新动物，让你闭上眼睛猜它是狗还是猫，你首先会问它的重量是否大于 10 公斤。如果是，那么你很确定这是一只狗，但是为了进一步确认，你问了第二个问题:它听起来像什么——喵还是汪？</p><p id="bc94" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">但是如果你改变特征的顺序，先问动物听起来像什么呢？狗不会说“喵”，猫也不会发出“汪”的声音。这意味着如果你使用声音作为区别特征，你可以 100%正确地区分两只动物，而不用询问体重或任何其他变量。</p><p id="408d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这说明有些特性比其他特性更重要。决策树根据哪个功能最能分割数据来决定先问哪个问题。</p><p id="a3ad" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">总之，本例中有三个关键信息:</p><p id="0e60" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">(1)您可以基于数字阈值(例如&gt;或&lt; 10 千克体重)或分类特征(例如喵或汪汪)来分割数据；</p><p id="f467" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">(2)一些变量比其他变量更重要(在这个例子中，声音比体重更重要)；</p><p id="9a29" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">(3)有些数据因为特征鲜明(蜥蜴不会飞，所以容易拆分(比如鸟类 vs 蜥蜴！)还有一些比较难。也就是说，两组之间的差异越大(即不同的特征)，就越容易将它们分开。</p><p id="aeb2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">基础知识说完了，现在让我们进入技术细节。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h1 id="3054" class="mc md it bd me mf mg mh mi mj mk ml mm jz mn ka mo kc mp kd mq kf mr kg ms mt bi translated">决策树是如何工作的？</h1><p id="0a0f" class="pw-post-body-paragraph kz la it lb b lc mu ju le lf mv jx lh li mw lk ll lm mx lo lp lq my ls lt lu im bi translated">在引擎盖下，决策树工作流程从选择分割数据的最佳特征开始。然后，使用上一节中描述的阈值或分类特征对数据进行分区。</p><p id="6250" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">完成第一次划分后，生成的树的深度为<em class="mz">1</em>。如果存在不同类别混合的剩余数据(即，不能仅通过重量区分的狗和猫的混合)，则使用下一个最佳特征进行第二次分割。</p><p id="4e7f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这个过程基本上是重复的，直到所有的数据都被清楚地分开，或者你用完了所有的特性。如果所有样本数据被清楚地分成各自的目标类别(例如，所有动物被分成猫和狗)，则一些特征可能保持不被使用；并非所有功能都是必需的。</p><p id="b942" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">生长的树现在是代表数据(也称为训练数据)的模型。对于一个新的预测，您所做的就是通过您刚刚创建的决策树逐个运行观察的特征。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h1 id="eaf2" class="mc md it bd me mf mg mh mi mj mk ml mm jz mn ka mo kc mp kd mq kf mr kg ms mt bi translated">那么数据到底是如何分区的呢？</h1><p id="e504" class="pw-post-body-paragraph kz la it lb b lc mu ju le lf mv jx lh li mw lk ll lm mx lo lp lq my ls lt lu im bi translated">让我们先来理解一个我们会反复使用的关键术语——“杂质”。</p><p id="b6c3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果一个要素正确地分隔了数据集中的两个类，这就是纯分割。在我们的例子中，如果<em class="mz">动物的声音</em>可以将所有的狗和所有的猫分开，那么得到的分割数据就是纯净的。但是，如果<em class="mz">动物体重</em>能够正确分离 90%的数据，就意味着 10%的数据有猫狗混杂。所以他们不纯洁。随着深度的增加，决策树算法将这种杂质最小化。</p><p id="323c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在分类中，有两个标准用于分割数据:基尼系数和熵。</p><h2 id="d200" class="na md it bd me nb nc dn mi nd ne dp mm li nf ng mo lm nh ni mq lq nj nk ms nl bi translated">a)基尼系数不纯</h2><p id="8894" class="pw-post-body-paragraph kz la it lb b lc mu ju le lf mv jx lh li mw lk ll lm mx lo lp lq my ls lt lu im bi translated"><em class="mz">基尼系数</em>衡量样本中错误阶层的比例。因此，基尼系数为 0.5 意味着所得数据中两个阶层的比例相等(50%-50%)——这是衡量杂质的最高标准。同样，基尼系数为 0.0 意味着一个阶级的情况为 0%，而另一个阶级的情况为 100%。基尼系数的值介于 0 和 0.5 之间，其中 0 表示绝对纯分裂，0.5 表示最差的不纯分裂。</p><p id="a85e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">基尼系数的数学表达式是:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nm"><img src="../Images/85e1a6f5687e7cfdde39a132d97ce8bd.png" data-original-src="https://miro.medium.com/v2/resize:fit:900/0*JA95LwI51QiPxMl3"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">这里 j 代表目标类(猫或狗)，p 是节点中类的比例(狗的数量和猫的数量)。(图片由作者提供)</p></figure><p id="763a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">因此，在 100 只动物的样本中，如果不纯节点有 60 只狗和 40 只猫，那么基尼不纯将是:</p><p id="fcea" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><code class="fe nn no np nq b">Gini = 1 - {(60/100)^2 + (40/100)^2} = 0.48</code></p><h2 id="e26e" class="na md it bd me nb nc dn mi nd ne dp mm li nf ng mo lm nh ni mq lq nj nk ms nl bi translated">b)熵/信息增益</h2><p id="bed3" class="pw-post-body-paragraph kz la it lb b lc mu ju le lf mv jx lh li mw lk ll lm mx lo lp lq my ls lt lu im bi translated">术语<em class="mz">熵</em>大概来源于物理学(热力学第二定律)；它是系统的物理属性，用来衡量无序状态。在信息论中,( Claude Shannon)设计了这个概念来测量从消息传输到接收的丢失信息量。</p><p id="4b6e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">它的数学公式是:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nm"><img src="../Images/3ae8b828d5ed0825aa8fd097ee04d40e.png" data-original-src="https://miro.medium.com/v2/resize:fit:900/0*44LYLD6ml9t3X42L"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">熵的数学公式。(图片由作者提供)</p></figure><p id="146c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">符号的含义与基尼中的相同。然而，熵的值介于 0-1 之间。</p><p id="2f8b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">上述相同数据的熵可计算如下:</p><pre class="kj kk kl km gt nr nq ns nt aw nu bi"><span id="cb20" class="na md it nq b gy nv nw l nx ny">Entropy = - {(60/100)*log2(0.6) + (40/100)*log2(0.4)} <br/>= 0.52</span></pre><p id="c903" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">请注意，分类特征使用每个类别分割目标，而数字/连续特征的分割候选是特征中的唯一值。有时，算法<a class="ae ky" href="https://spark.apache.org/docs/1.3.0/mllib-decision-tree.html" rel="noopener ugc nofollow" target="_blank">首先对值进行排序</a>，然后选择每个唯一值进行拆分，以找到最佳拆分。</p><h1 id="36a0" class="mc md it bd me mf nz mh mi mj oa ml mm jz ob ka mo kc oc kd mq kf od kg ms mt bi translated">那么如何从测量杂质到决定分裂(或不分裂)？</h1><p id="3b06" class="pw-post-body-paragraph kz la it lb b lc mu ju le lf mv jx lh li mw lk ll lm mx lo lp lq my ls lt lu im bi translated">我们已经看到了决策树如何在幕后工作，以及算法如何选择使用杂质标准来分割数据。现在，我们如何从测量杂质转向决定:</p><p id="d22b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">(a)是否要进一步分裂</p><p id="1d60" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">(b)使用哪个特征进行分割</p><p id="46f2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">杂质测量用于计算所谓的<em class="mz">信息增益</em>。它是分割前后杂质的差异，由每个节点中各自的样本大小加权。如果分裂后杂质减少，信息增益应该为正，因此算法将继续进一步分裂数据。本质上:</p><pre class="kj kk kl km gt nr nq ns nt aw nu bi"><span id="7c24" class="na md it nq b gy nv nw l nx ny">Information gain <br/>= Information in the parent node <br/>  – information in the left child node <br/>  – information in the right child node</span></pre><p id="78a7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在用于计算的数学公式中:</p><pre class="kj kk kl km gt nr nq ns nt aw nu bi"><span id="c24c" class="na md it nq b gy nv nw l nx ny">Information gain <br/>= (parent_sample*parent_gini <br/>   - left_sample*left_gini <br/>   - right_sample*right_gini)/parent_sample</span></pre><p id="17ca" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们用下面的假设例子，用上面的公式计算信息增益:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oe"><img src="../Images/446071035163cc6ffbf145ca12780f41.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*Kvni7h5vbCwTTMgM"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">计算信息增益的决策树。(图片由作者提供)</p></figure><pre class="kj kk kl km gt nr nq ns nt aw nu bi"><span id="e0fa" class="na md it nq b gy nv nw l nx ny"># data<br/>parent_sample, parent_gini = 212, 0.496<br/>left_sample, left_gini = 99, 0.367<br/>right_sample, right_gini = 113, 0.314</span><span id="c4e1" class="na md it nq b gy of nw l nx ny"># information gain calculation<br/>= (212*0.496 - 99*0.367 - 113*0.314)/212<br/>= 0.15725</span></pre><p id="47f7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">所以在假设的例子中，第一次分离后的信息增益是 0.15。如果进一步的分裂将继续给出所需的信息增益，树将继续分裂，除非您通过超参数调整来限制它的增长以避免过度拟合。</p><h1 id="b0d8" class="mc md it bd me mf nz mh mi mj oa ml mm jz ob ka mo kc oc kd mq kf od kg ms mt bi translated">回归呢？</h1><p id="0527" class="pw-post-body-paragraph kz la it lb b lc mu ju le lf mv jx lh li mw lk ll lm mx lo lp lq my ls lt lu im bi translated">在回归中，<em class="mz">方差</em>是代替杂质的度量。它衡量数据相对于平均值的可变性。自然地，较大的方差意味着数据点是分散的，远离平均值并且彼此远离。</p><p id="09fa" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">它的数学公式是:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi og"><img src="../Images/92c4aca517e6882e8bea08a779f0552a.png" data-original-src="https://miro.medium.com/v2/resize:fit:532/0*wfcqMgpmG4KVeUuJ"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">计算回归中的方差。(图片由作者提供)</p></figure><p id="4df8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">旁注:如果你听说过<em class="mz">标准差</em>，那只是方差的平方根。</p><p id="76b4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">类似于分类，选择用于分类和数字特征的分裂候选。分割完成后，算法会比较分割前后的方差，只要方差减小，分割就会继续。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h1 id="6152" class="mc md it bd me mf mg mh mi mj mk ml mm jz mn ka mo kc mp kd mq kf mr kg ms mt bi translated">摘要</h1><ul class=""><li id="ed22" class="oh oi it lb b lc mu lf mv li oj lm ok lq ol lu om on oo op bi translated"><em class="mz">决策树</em>根据数据的特征和算法对数据进行分割，只要<em class="mz">杂质</em>减少，分割就会继续。</li><li id="4f80" class="oh oi it lb b lc oq lf or li os lm ot lq ou lu om on oo op bi translated">计算杂质有两种主要方法:<em class="mz">基尼&amp;熵。</em></li><li id="1991" class="oh oi it lb b lc oq lf or li os lm ot lq ou lu om on oo op bi translated">分类特征和数字特征的分裂候选被不同地选择——对于分类特征:每个类别；对于数字特征:每个唯一值。</li><li id="ae26" class="oh oi it lb b lc oq lf or li os lm ot lq ou lu om on oo op bi translated">只要<em class="mz">信息增益</em>是有利的，所有特征都用尽或者树的生长通过修剪被限制，分割就继续。</li><li id="b66f" class="oh oi it lb b lc oq lf or li os lm ot lq ou lu om on oo op bi translated">回归树使用<em class="mz">方差</em>作为决定是否划分数据的度量。</li></ul><p id="cd16" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">感谢阅读。请随意<a class="ae ky" href="https://mab-datasc.medium.com/subscribe" rel="noopener">订阅</a>以获得我即将发布的文章的通知，或者通过<a class="ae ky" href="https://twitter.com/DataEnthus" rel="noopener ugc nofollow" target="_blank"> Twitter </a>或<a class="ae ky" href="https://www.linkedin.com/in/mab-alam/" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>与我联系。</p></div></div>    
</body>
</html>