<html>
<head>
<title>Run Heavy Prefect Workflows at Lightning Speed with Dask</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">借助Dask，以闪电般的速度运行繁重的级长工作流</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/scaling-your-prefect-workflow-to-the-cloud-2dec4e0b213b?source=collection_archive---------24-----------------------#2021-09-22">https://towardsdatascience.com/scaling-your-prefect-workflow-to-the-cloud-2dec4e0b213b?source=collection_archive---------24-----------------------#2021-09-22</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="3658" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">借助Dask和Coiled实现云原生工作流自动化</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/8359d0415fe5d3e6eb35cb969fbee741.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2cRddvrbmPZ0jca8eu_M8A.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图片由比利·胡恩通过<a class="ae ky" href="http://www.unsplash.com" rel="noopener ugc nofollow" target="_blank">unsplash.com</a>拍摄</p></figure><p id="517d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">Prefect是一个流行的用于自动化工作流程编排的开源Python库。当运行即时可用的Prefect时，工作流的<em class="lv">编排</em>在云中完成，而代码的实际<em class="lv">计算</em>在本地机器上完成。这意味着您的完美云工作流受到您的机器资源的限制。</p><p id="eb82" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这篇博文将向你展示:</p><ol class=""><li id="b585" class="lw lx it lb b lc ld lf lg li ly lm lz lq ma lu mb mc md me bi translated">如何使用DaskExecutor在本地完美工作流中利用并行性</li><li id="74ae" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu mb mc md me bi translated">何时以及如何将DaskExecutor连接到云计算集群，以超越本地机器的限制。</li><li id="72ec" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu mb mc md me bi translated">如何使用Prefect ResourceManager构建一个工作流，仅在必要时将计算委托给云中的集群。</li></ol><p id="4400" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">你也可以用这个完美的Python脚本直接进入代码。</p><h1 id="6b5e" class="mk ml it bd mm mn mo mp mq mr ms mt mu jz mv ka mw kc mx kd my kf mz kg na nb bi translated">通过并行计算加快处理速度</h1><p id="259f" class="pw-post-body-paragraph kz la it lb b lc nc ju le lf nd jx lh li ne lk ll lm nf lo lp lq ng ls lt lu im bi translated">完美的工作流使用流程和任务构建数据管道，这些流程和任务由执行者编排。提督的默认执行器按顺序运行任务。这对于简单的工作流来说没什么问题，但这意味着您的计算可能会比需要的速度慢，因为它们没有充分利用可用资源。</p><p id="cda8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">作为一名数据科学家或工程师，您会希望通过切换到<a class="ae ky" href="https://docs.prefect.io/orchestration/flow_config/executors.html#daskexecutor" rel="noopener ugc nofollow" target="_blank"> DaskExecutor </a>来优化性能。这将利用您的本地机器的多个核心，加速计算繁重的任务，如加入，洗牌和机器学习作业。</p><pre class="kj kk kl km gt nh ni nj nk aw nl bi"><span id="6347" class="nm ml it ni b gy nn no l np nq"># create a temporary local Dask Executor <br/>executor = DaskExecutor()</span></pre><p id="f868" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">当达到机器的内存极限时，可以将DaskExecutor连接到云计算资源，以便在多台机器上分配工作。一种方法是旋转一个盘绕的集群，并在那里运行您的完美流，方法是:</p><ul class=""><li id="3e1d" class="lw lx it lb b lc ld lf lg li ly lm lz lq ma lu nr mc md me bi translated">在远程集群上运行您的整个完美工作流，例如通过<a class="ae ky" href="https://docs.coiled.io/user_guide/examples/prefect.html#running-all-prefect-tasks-on-coiled" rel="noopener ugc nofollow" target="_blank">使用盘绕集群作为您的DaskExecutor </a>，</li><li id="5cdf" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu nr mc md me bi translated">在远程集群上运行特定的完美任务，例如通过<a class="ae ky" href="https://docs.coiled.io/user_guide/examples/prefect.html#calling-coiled-from-a-prefect-task" rel="noopener ugc nofollow" target="_blank">在特定的计算密集型任务</a>中旋转盘绕的集群，</li><li id="554c" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu nr mc md me bi translated">仅当数据集的大小超过某个阈值时，才在远程集群<em class="lv">上运行特定的提督任务，例如通过使用提督资源管理器对象内的盘绕集群</em></li></ul><p id="dd26" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">上述成卷文件的链接包括前两种方法的例子；下一节将向您展示如何编写一个完美的脚本，当数据集的大小超过某个阈值时，将计算委托给一个盘绕的集群。</p><p id="fad7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="lv">免责声明:我在Coiled工作，是一名数据科学传播者。</em><a class="ae ky" href="http://coiled.io/" rel="noopener ugc nofollow" target="_blank"><em class="lv">Coiled</em></a><em class="lv">由</em><a class="ae ky" href="https://dask.org/" rel="noopener ugc nofollow" target="_blank"><em class="lv">Dask</em></a><em class="lv">的最初作者Matthew Rocklin创立，是一个面向分布式计算的开源Python库。</em></p><h1 id="9499" class="mk ml it bd mm mn mo mp mq mr ms mt mu jz mv ka mw kc mx kd my kf mz kg na nb bi translated">使用Dask的自动化完美ETL管道</h1><p id="c2be" class="pw-post-body-paragraph kz la it lb b lc nc ju le lf nd jx lh li ne lk ll lm nf lo lp lq ng ls lt lu im bi translated">下面的代码示例将Github归档数据集从JSON转换为Parquet，并将其写入云存储，每当数据集变得太大而无法在本地处理时，就利用Coiled的计算资源来这样做。这意味着您可以在云上转换整个75GB的数据集，而不必将其下载到您的机器上。你可以在这里找到完整的Python脚本<a class="ae ky" href="https://github.com/coiled/coiled-resources/blob/main/prefect-with-coiled/prefect-coiled-conditional.py" rel="noopener ugc nofollow" target="_blank"/>。</p><p id="f490" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">ETL工作流看起来会像这样:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ns"><img src="../Images/5861b07b6d6e0cc2d0dce321c762a51a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vbU43f8__-wC31wmOqu2Dg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><h1 id="0f7b" class="mk ml it bd mm mn mo mp mq mr ms mt mu jz mv ka mw kc mx kd my kf mz kg na nb bi translated">定义您的完美任务运行</h1><p id="6cb1" class="pw-post-body-paragraph kz la it lb b lc nc ju le lf nd jx lh li ne lk ll lm nf lo lp lq ng ls lt lu im bi translated">让我们从定义我们想要运行的任务开始。如果你习惯于定义一个提督任务，可以随意向下滚动到下一部分，将你的提督工作流程连接到Coiled。</p><p id="b79d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们按照任务运行的顺序来定义任务。我们将从Prefect任务开始，该任务将创建我们想要从Github存档中获取的文件名列表。关于我们如何构建这些代码的更多背景信息，请看一下这个Jupyter笔记本。</p><pre class="kj kk kl km gt nh ni nj nk aw nl bi"><span id="d537" class="nm ml it ni b gy nn no l np nq">@task <br/>def create_list(start_date, end_date, format="%d-%m-%Y"): <br/>    start = datetime.datetime.strptime(start_date, format) <br/>    end = datetime.datetime.strptime(end_date, format) <br/>    date_generated = [start + datetime.timedelta(days=x) for x in range(0, (end-start).days)] <br/>    prefix = "https://data.gharchive.org/" <br/>    filenames = [] </span><span id="bbb3" class="nm ml it ni b gy nt no l np nq">    for date in date_generated: <br/>        for hour in range(1,24): <br/>            filenames.append(prefix + date.strftime("%Y-%m-%d") + '-' + str(hour) + '.json.gz') <br/>  <br/>    return filenames</span></pre><p id="7968" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">接下来，让我们定义一个任务，这个任务将决定我们的流将旋转的集群的类型。我们将使用<code class="fe nu nv nw ni b">len(filenames)</code>作为数据集大小的代理，您也可以考虑其他方法来估计内存中数据的大小。</p><pre class="kj kk kl km gt nh ni nj nk aw nl bi"><span id="f04a" class="nm ml it ni b gy nn no l np nq">@task <br/>def determine_cluster_type(filenames): <br/>    if len(filenames) &gt; 100: <br/>        return "coiled" <br/>    return "local"</span></pre><p id="b84a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们还需要一个任务来获取在<code class="fe nu nv nw ni b">filenames</code>列表中指定的数据...</p><pre class="kj kk kl km gt nh ni nj nk aw nl bi"><span id="ca23" class="nm ml it ni b gy nn no l np nq">@task <br/>def get_github_data(filenames): <br/>    records = db.read_text(filenames).map(ujson.loads) <br/>    push_events = records.filter(lambda record: record["type"] == "PushEvent") return push_events</span></pre><p id="3c87" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">…一个将原始JSON数据转换成表格数据帧格式的<code class="fe nu nv nw ni b">Task </code>...</p><pre class="kj kk kl km gt nh ni nj nk aw nl bi"><span id="c852" class="nm ml it ni b gy nn no l np nq">@task<br/>def to_dataframe(push_events): <br/>  def process(record): <br/>    try: <br/>      for commit in record["payload"]["commits"]: <br/>        yield { <br/>          "user": record["actor"]["login"], <br/>          "repo": record["repo"]["name"], <br/>          "created_at": record["created_at"], <br/>          "message": commit["message"], <br/>          "author": commit["author"]["name"], }   <br/>    except KeyError: <br/>      pass </span><span id="f92e" class="nm ml it ni b gy nt no l np nq">  processed = push_events.map(process) <br/>  df = processed.flatten().to_dataframe() <br/>  return df</span></pre><p id="bd43" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">…和一个<code class="fe nu nv nw ni b">Task</code>将展平的数据帧作为拼花文件写入我们的S3存储桶。</p><pre class="kj kk kl km gt nh ni nj nk aw nl bi"><span id="2564" class="nm ml it ni b gy nn no l np nq">@task<br/>def to_parquet(df, path): <br/>  df.to_parquet(path, engine='fastparquet', compression='lz4' )</span></pre><h1 id="7db2" class="mk ml it bd mm mn mo mp mq mr ms mt mu jz mv ka mw kc mx kd my kf mz kg na nb bi translated">具有完美资源管理器的Dask集群设置</h1><p id="448f" class="pw-post-body-paragraph kz la it lb b lc nc ju le lf nd jx lh li ne lk ll lm nf lo lp lq ng ls lt lu im bi translated">太棒了，我们已经定义了流程中的所有任务。下一步是定义我们的流可以使用的两种集群类型:<code class="fe nu nv nw ni b">local </code>用于当数据集小到足以在本地处理时，而<code class="fe nu nv nw ni b">coiled </code>则用于其他情况。</p><p id="aab2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">使用临时云计算资源时，您需要确保这些资源得到正确的实例化、使用和清理，以避免错误和不必要的成本。我们将通过定义所需的<code class="fe nu nv nw ni b">__init__</code>、<code class="fe nu nv nw ni b">setup</code>和<code class="fe nu nv nw ni b">close</code>块，使用提督<code class="fe nu nv nw ni b"><a class="ae ky" href="https://docs.prefect.io/core/idioms/resource-manager.html" rel="noopener ugc nofollow" target="_blank">ResourceManager</a></code>对象来完成这项工作。确保在<code class="fe nu nv nw ni b">__init__ </code>定义中包含任何想要传递给集群的关键字参数。这个块创建Dask集群，包括Dask调度程序，并将其连接到您的本地客户机。</p><pre class="kj kk kl km gt nh ni nj nk aw nl bi"><span id="9fcb" class="nm ml it ni b gy nn no l np nq"># Define a ResourceManager object <br/>@resource_manager <br/>class DaskCluster: </span><span id="6dbd" class="nm ml it ni b gy nt no l np nq">  def __init__(self, cluster_type="local", n_workers=None, software=None, account=None, name=None): <br/>    self.cluster_type = cluster_type <br/>    self.n_workers = n_workers <br/>    self.software = software <br/>    self.account = account <br/>    self.name = name </span><span id="e6c0" class="nm ml it ni b gy nt no l np nq">  def setup(self):  <br/>    if self.cluster_type == "local": <br/>      return Client(processes=False) <br/>    elif self.cluster_type == "coiled": <br/>      cluster = coiled.Cluster(<br/>                  name = self.name, <br/>                  software = self.software, <br/>                  n_workers = self.n_workers, <br/>                  account = self.account) <br/>      return Client(cluster) </span><span id="8b7f" class="nm ml it ni b gy nt no l np nq">  def cleanup(self, client): <br/>    client.close() <br/>    if self.cluster_type == "coiled": <br/>      client.cluster.close()</span></pre><h1 id="6528" class="mk ml it bd mm mn mo mp mq mr ms mt mu jz mv ka mw kc mx kd my kf mz kg na nb bi translated">构建完美的Dask数据工程流程</h1><p id="4b31" class="pw-post-body-paragraph kz la it lb b lc nc ju le lf nd jx lh li ne lk ll lm nf lo lp lq ng ls lt lu im bi translated">现在您已经定义了您的<code class="fe nu nv nw ni b">Tasks</code>和<code class="fe nu nv nw ni b">ResourceManager</code>，下一步是告诉Prefect这些任务是如何相互关联的，以及您将需要它们如何运行。我们还将定义一些可以根据用户运行的流程进行调整的<code class="fe nu nv nw ni b"> Parameters</code>。</p><pre class="kj kk kl km gt nh ni nj nk aw nl bi"><span id="f39a" class="nm ml it ni b gy nn no l np nq"># Build Prefect Flow <br/>with Flow(name="Github ETL Test") as flow: </span><span id="d31d" class="nm ml it ni b gy nt no l np nq"># define parameters <br/>  n_workers = Parameter("n_workers", default=4) <br/>  software = Parameter("software", default='coiled-examples/prefect') <br/>  account = Parameter("account", default=None) <br/>  name = Parameter("name", default='cluster-name') <br/>  start_date = Parameter("start_date", default="01-01-2015") <br/>  end_date = Parameter("end_date", default="31-12-2015") </span><span id="955a" class="nm ml it ni b gy nt no l np nq"># build flow <br/>  filenames = create_list(start_date=start_date, end_date=end_date) <br/>  cluster_type = determine_cluster_type(filenames) </span><span id="09c1" class="nm ml it ni b gy nt no l np nq"># use ResourceManager object <br/>  with DaskCluster(<br/>      cluster_type=cluster_type, <br/>      n_workers=n_workers, <br/>      software=software, <br/>      account=account, <br/>      name=name ) as client: <br/>    push_events = get_github_data(filenames) <br/>    df = to_dataframe(push_events) <br/>    to_parquet(df)</span></pre><p id="05f9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">干得好！您现在已经准备好运行您的完美流程了。继续尝试使用各种参数值<code class="fe nu nv nw ni b">end_date</code>来查看条件盘绕式集群起转的效果:将<code class="fe nu nv nw ni b">end_date</code>设置为“06-01-2015”之后的任何值都会将您的计算委托给一个集群。</p><p id="724b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">您还可以通过将可选的<code class="fe nu nv nw ni b">Parameter</code>值传递给<code class="fe nu nv nw ni b">flow.run()</code>来定制Dask集群，例如<code class="fe nu nv nw ni b">n_workers</code>和集群<code class="fe nu nv nw ni b">name</code>。如果您想调整Dask集群的其他特性，比如Dask worker和Dask scheduler内存或空闲超时，您必须在<code class="fe nu nv nw ni b">DaskCluster</code>的<code class="fe nu nv nw ni b">__init__</code>块中包含<a class="ae ky" href="https://docs.coiled.io/user_guide/cluster_creation.html" rel="noopener ugc nofollow" target="_blank">相应的关键字参数</a>。</p><pre class="kj kk kl km gt nh ni nj nk aw nl bi"><span id="f769" class="nm ml it ni b gy nn no l np nq"># Run flow with parameters <br/>flow.run(<br/>    parameters=dict(<br/>        end_date="02-01-2015", <br/>        n_workers=15,  <br/>        name="prefect-on-coiled"<br/>    ) <br/>)</span></pre><h1 id="9ee6" class="mk ml it bd mm mn mo mp mq mr ms mt mu jz mv ka mw kc mx kd my kf mz kg na nb bi translated">完美的Dask工作流程编排摘要</h1><p id="5af2" class="pw-post-body-paragraph kz la it lb b lc nc ju le lf nd jx lh li ne lk ll lm nf lo lp lq ng ls lt lu im bi translated">在本文中，我们讨论了何时以及如何使用Dask和Coiled在本地或云中的完美工作流中利用并行性。然后，我们构建了一个完美的数据工程工作流，每当数据集的大小超过某个阈值时，它就将计算委托给Dask集群。</p><div class="nx ny gp gr nz oa"><a href="https://coiled.io/blog/common-dask-mistakes/" rel="noopener  ugc nofollow" target="_blank"><div class="ob ab fo"><div class="oc ab od cl cj oe"><h2 class="bd iu gy z fp of fr fs og fu fw is bi translated">使用Dask时要避免的常见错误</h2><div class="oh l"><h3 class="bd b gy z fp of fr fs og fu fw dk translated">第一次使用Dask可能是一个陡峭的学习曲线。经过多年的建设Dask和引导人们通过…</h3></div><div class="oi l"><p class="bd b dl z fp of fr fs og fu fw dk translated">coiled.io</p></div></div><div class="oj l"><div class="ok l ol om on oj oo ks oa"/></div></div></a></div><p id="9073" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">主要要点:</p><ul class=""><li id="ec30" class="lw lx it lb b lc ld lf lg li ly lm lz lq ma lu nr mc md me bi translated">您可以使用基于云的集群，如Coiled，将您的完美工作流扩展到大于内存的数据集</li><li id="aa84" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu nr mc md me bi translated">您可以编写自己的代码，以便Dask集群只在真正需要的时候才启动</li></ul><p id="f5d5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在LinkedIn 上关注我<a class="ae ky" href="https://www.linkedin.com/in/richard-pelgrim/" rel="noopener ugc nofollow" target="_blank">获取更多类似内容！</a></p></div><div class="ab cl op oq hx or" role="separator"><span class="os bw bk ot ou ov"/><span class="os bw bk ot ou ov"/><span class="os bw bk ot ou"/></div><div class="im in io ip iq"><p id="1de4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="lv">原载于2021年9月22日</em><a class="ae ky" href="https://coiled.io/blog/big-data-workflow-automation-with-prefect-and-coiled/" rel="noopener ugc nofollow" target="_blank"><em class="lv">https://coiled . io</em></a><em class="lv">。</em></p></div></div>    
</body>
</html>