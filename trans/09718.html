<html>
<head>
<title>GPT-4 Will Have 100 Trillion Parameters — 500x the Size of GPT-3</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">GPT 4号将拥有100万亿个参数，是GPT 3号的500倍</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/gpt-4-will-have-100-trillion-parameters-500x-the-size-of-gpt-3-582b98d82253?source=collection_archive---------0-----------------------#2021-09-11">https://towardsdatascience.com/gpt-4-will-have-100-trillion-parameters-500x-the-size-of-gpt-3-582b98d82253?source=collection_archive---------0-----------------------#2021-09-11</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="906e" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph">人工智能|新闻</h2><div class=""/><div class=""><h2 id="86ad" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">大型神经网络有什么限制吗？</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/768c145105dbeb1aa8478f5882104484.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*PSxUk3lW6R325xnE"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">桑德罗·卡塔琳娜在<a class="ae lh" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><h2 id="f509" class="li lj it bd lk ll lm dn ln lo lp dp lq lr ls lt lu lv lw lx ly lz ma mb mc iz bi translated">更新:<a class="ae lh" href="https://thealgorithmicbridge.substack.com/p/gpt-4-in-10-keys" rel="noopener ugc nofollow" target="_blank"> GPT-4出局</a>。</h2></div><div class="ab cl md me hx mf" role="separator"><span class="mg bw bk mh mi mj"/><span class="mg bw bk mh mi mj"/><span class="mg bw bk mh mi"/></div><div class="im in io ip iq"><p id="4228" class="pw-post-body-paragraph mk ml it mm b mn mo kd mp mq mr kg ms lr mt mu mv lv mw mx my lz mz na nb nc im bi translated">OpenAI的诞生是为了应对实现人工通用智能(AGI)的挑战——一种能够做人类可以做的任何事情的人工智能。</p><p id="078e" class="pw-post-body-paragraph mk ml it mm b mn mo kd mp mq mr kg ms lr mt mu mv lv mw mx my lz mz na nb nc im bi translated">这种技术将改变我们所知的世界。如果使用得当，它可以造福于我们所有人，但如果落入坏人之手，它可能成为最具毁灭性的武器。这就是OpenAI接手这个任务的原因。为了确保每个人都能平等受益<a class="ae lh" href="https://openai.com/blog/introducing-openai/" rel="noopener ugc nofollow" target="_blank"/>:“我们的目标是以最有可能造福全人类的方式推进数字智能。”</p><p id="11a2" class="pw-post-body-paragraph mk ml it mm b mn mo kd mp mq mr kg ms lr mt mu mv lv mw mx my lz mz na nb nc im bi translated">然而，这个问题的严重性使它成为人类有史以来最大的科研项目。尽管计算机科学和人工智能取得了很大的进步，但没有人知道如何解决这个问题，也不知道什么时候会发生。</p><p id="7a55" class="pw-post-body-paragraph mk ml it mm b mn mo kd mp mq mr kg ms lr mt mu mv lv mw mx my lz mz na nb nc im bi translated">一些人认为深度学习不足以实现AGI。伯克利的计算机科学教授、人工智能先驱Stuart Russell<a class="ae lh" href="https://www.ft.com/content/c96e43be-b4df-11e9-8cb2-799a3a8cf37b" rel="noopener ugc nofollow" target="_blank">认为</a>“专注于原始的计算能力完全没有抓住要点[……]我们不知道如何让一台机器变得真正智能——即使它有宇宙那么大。”</p><p id="5ff4" class="pw-post-body-paragraph mk ml it mm b mn mo kd mp mq mr kg ms lr mt mu mv lv mw mx my lz mz na nb nc im bi translated">相比之下，OpenAI相信，基于大型数据集并在大型计算机上训练的大型神经网络是实现AGI的最佳途径。OpenAI的首席技术官格雷格·布罗克曼在接受金融时报采访时说:“我们认为谁拥有最大的计算机，谁就能获得最大的利益。”</p><p id="2018" class="pw-post-body-paragraph mk ml it mm b mn mo kd mp mq mr kg ms lr mt mu mv lv mw mx my lz mz na nb nc im bi translated">这就是他们所做的。他们开始训练越来越大的模型，以唤醒深度学习中隐藏的力量。朝着这个方向迈出的第一步是GPT和T2的发布。这些大型语言模型将为这场秀的主角<a class="ae lh" href="https://arxiv.org/abs/2005.14165" rel="noopener ugc nofollow" target="_blank"> GPT-3 </a>奠定基础。一个比GPT-2大100倍的语言模型，有1750亿个参数。</p><p id="f21d" class="pw-post-body-paragraph mk ml it mm b mn mo kd mp mq mr kg ms lr mt mu mv lv mw mx my lz mz na nb nc im bi translated">GPT-3是当时创造的最大的神经网络——并且仍然是最大的<em class="nd">密集</em>神经网络。它的语言专业知识和无数的能力让大多数人感到惊讶。尽管一些专家仍然持怀疑态度，但是大型语言模型已经让人感觉很奇怪了。对于OpenAI的研究人员来说，这是一个巨大的飞跃，他们加强了他们的信念，并让我们相信AGI是深度学习的一个问题。</p></div><div class="ab cl md me hx mf" role="separator"><span class="mg bw bk mh mi mj"/><span class="mg bw bk mh mi mj"/><span class="mg bw bk mh mi"/></div><div class="im in io ip iq"><h1 id="018c" class="ne lj it bd lk nf ng nh ln ni nj nk lq ki nl kj lu kl nm km ly ko nn kp mc no bi translated">神圣的三位一体——算法、数据和计算机</h1><p id="0d0e" class="pw-post-body-paragraph mk ml it mm b mn np kd mp mq nq kg ms lr nr mu mv lv ns mx my lz nt na nb nc im bi translated">OpenAI相信<a class="ae lh" href="https://www.gwern.net/Scaling-hypothesis#scaling-hypothesis" rel="noopener ugc nofollow" target="_blank">缩放假说</a>。给定一个可扩展的算法，在这种情况下是变压器——GPT家族背后的基本架构——可能有一条通往AGI的直接路径，包括基于该算法训练越来越大的模型。</p><p id="db2b" class="pw-post-body-paragraph mk ml it mm b mn mo kd mp mq mr kg ms lr mt mu mv lv mw mx my lz mz na nb nc im bi translated">但是大型模型只是AGI拼图的一部分。训练它们需要大量的数据集和大量的计算能力。</p><p id="4584" class="pw-post-body-paragraph mk ml it mm b mn mo kd mp mq mr kg ms lr mt mu mv lv mw mx my lz mz na nb nc im bi translated">当机器学习社区开始揭示无监督学习的潜力时，数据不再是瓶颈。这与生成语言模型和少量任务转移一起，解决了OpenAI的“大数据集”问题。</p><p id="d8b2" class="pw-post-body-paragraph mk ml it mm b mn mo kd mp mq mr kg ms lr mt mu mv lv mw mx my lz mz na nb nc im bi translated">他们只需要巨大的计算资源来训练和部署他们的模型，他们会很好地去。这也是他们<a class="ae lh" href="https://openai.com/blog/microsoft/" rel="noopener ugc nofollow" target="_blank">在2019年与微软</a>合作的原因。他们授权给这家大型科技公司，这样他们就可以在商业上使用OpenAI的一些模型，以换取他们所需的云计算基础设施和强大的GPU。</p><p id="7a38" class="pw-post-body-paragraph mk ml it mm b mn mo kd mp mq mr kg ms lr mt mu mv lv mw mx my lz mz na nb nc im bi translated">但是GPU并不是专门为训练神经网络而构建的。游戏行业开发了这些用于图形处理的芯片，而人工智能行业只是利用了它适合并行计算的优势。OpenAI想要最好的模型和最好的数据集，他们也想要最好的计算机芯片。GPU是不够的。</p><p id="f0d3" class="pw-post-body-paragraph mk ml it mm b mn mo kd mp mq mr kg ms lr mt mu mv lv mw mx my lz mz na nb nc im bi translated">许多公司也意识到了这一点，并开始在不损失效率或容量的情况下，构建内部专用芯片来训练神经网络。但是像OpenAI这样的纯软件公司，很难做到硬件设计和制作一体化。这就是为什么他们走了另一条路线:使用第三方AI专用芯片。</p><p id="c6f6" class="pw-post-body-paragraph mk ml it mm b mn mo kd mp mq mr kg ms lr mt mu mv lv mw mx my lz mz na nb nc im bi translated">这里是大脑系统进入场景的地方。这家芯片公司已经在2019年建造了<a class="ae lh" href="http://ever" rel="noopener ugc nofollow" target="_blank">有史以来最大的芯片</a>来训练大型神经网络。现在他们又做到了，OpenAI将很好地利用这一令人惊叹的工程。</p></div><div class="ab cl md me hx mf" role="separator"><span class="mg bw bk mh mi mj"/><span class="mg bw bk mh mi mj"/><span class="mg bw bk mh mi"/></div><div class="im in io ip iq"><h1 id="551e" class="ne lj it bd lk nf ng nh ln ni nj nk lq ki nl kj lu kl nm km ly ko nn kp mc no bi translated">一个芯片和一个模型——WSE二号和GPT四号</h1><p id="2db6" class="pw-post-body-paragraph mk ml it mm b mn np kd mp mq nq kg ms lr nr mu mv lv ns mx my lz nt na nb nc im bi translated">两周前，《连线》杂志<a class="ae lh" href="https://www.wired.com/story/cerebras-chip-cluster-neural-networks-ai/" rel="noopener ugc nofollow" target="_blank">发表了一篇文章</a>披露了两条重要消息。</p><p id="d4f2" class="pw-post-body-paragraph mk ml it mm b mn mo kd mp mq mr kg ms lr mt mu mv lv mw mx my lz mz na nb nc im bi translated">首先，脑波强化器又制造了<a class="ae lh" href="https://www.wired.com/story/power-ai-startup-built-really-big-chip/" rel="noopener ugc nofollow" target="_blank"/>市场上最大的芯片，晶圆级引擎二号(WSE-2)。它每边约22厘米，有2.6万亿个晶体管。相比之下，特斯拉的<a class="ae lh" href="https://youtu.be/j0z4FweCy4M" rel="noopener ugc nofollow" target="_blank">全新</a>训练瓦片有1.25万亿个晶体管。</p><p id="c986" class="pw-post-body-paragraph mk ml it mm b mn mo kd mp mq mr kg ms lr mt mu mv lv mw mx my lz mz na nb nc im bi translated">脑波强化器找到了一种有效浓缩计算能力的方法，因此WSE-2拥有85万个核心——计算单元——而典型的GPU只有几百个。他们还利用一种新颖的冷却系统解决了加热问题，并成功创建了高效的I/O数据流。</p><p id="421a" class="pw-post-body-paragraph mk ml it mm b mn mo kd mp mq mr kg ms lr mt mu mv lv mw mx my lz mz na nb nc im bi translated">像WSE-2这样超级专业、超级昂贵、超级强大的芯片没有多少用处。训练大型神经网络就是其中之一。因此脑波强化器与OpenAI对话。</p><p id="8592" class="pw-post-body-paragraph mk ml it mm b mn mo kd mp mq mr kg ms lr mt mu mv lv mw mx my lz mz na nb nc im bi translated">下面是第二条新闻。脑波强化器的首席执行官安德鲁·费尔德曼对《连线》说:“从与OpenAI的交谈中，GPT-4将有大约100万亿个参数。[……]那要好几年才能准备好。”</p><p id="c7c4" class="pw-post-body-paragraph mk ml it mm b mn mo kd mp mq mr kg ms lr mt mu mv lv mw mx my lz mz na nb nc im bi translated">自GPT 3以来，人们对OpenAI及其下一个版本有很多期待。现在我们知道它将在几年后问世，而且会非常大。它的大小将超过x500 GPT 3号。你没看错:<em class="nd"> x500。</em></p><p id="e0c8" class="pw-post-body-paragraph mk ml it mm b mn mo kd mp mq mr kg ms lr mt mu mv lv mw mx my lz mz na nb nc im bi translated">GPT-4将比去年震惊世界的语言模型大500倍。</p></div><div class="ab cl md me hx mf" role="separator"><span class="mg bw bk mh mi mj"/><span class="mg bw bk mh mi mj"/><span class="mg bw bk mh mi"/></div><div class="im in io ip iq"><h1 id="ca3a" class="ne lj it bd lk nf ng nh ln ni nj nk lq ki nl kj lu kl nm km ly ko nn kp mc no bi translated">我们能从GPT-4中期待什么？</h1><p id="09d3" class="pw-post-body-paragraph mk ml it mm b mn np kd mp mq nq kg ms lr nr mu mv lv ns mx my lz nt na nb nc im bi translated">100万亿参数很多。为了理解这个数字有多大，让我们把它与我们的大脑进行比较。大脑有大约800-1000亿个神经元(GPT-3的数量级)和大约100万亿个突触。</p><p id="eef4" class="pw-post-body-paragraph mk ml it mm b mn mo kd mp mq mr kg ms lr mt mu mv lv mw mx my lz mz na nb nc im bi translated">GPT-4将拥有和大脑拥有的突触一样多的参数。</p><p id="7018" class="pw-post-body-paragraph mk ml it mm b mn mo kd mp mq mr kg ms lr mt mu mv lv mw mx my lz mz na nb nc im bi translated">这样一个神经网络的庞大规模可能会带来我们只能想象的GPT-3的质的飞跃。用目前的<a class="ae lh" rel="noopener" target="_blank" href="/software-3-0-how-prompting-will-change-the-rules-of-the-game-a982fbfe1e0">提示方法</a>，我们甚至可能无法测试系统的全部潜力。</p><p id="4487" class="pw-post-body-paragraph mk ml it mm b mn mo kd mp mq mr kg ms lr mt mu mv lv mw mx my lz mz na nb nc im bi translated">然而，将人工神经网络与大脑进行比较是一件棘手的事情。这种比较似乎是公平的，但那只是因为我们假设人工神经元至少松散地基于生物神经元。<a class="ae lh" href="https://www.cell.com/neuron/fulltext/S0896-6273(21)00501-8?_returnURL=https%3A%2F%2Flinkinghub.elsevier.com%2Fretrieve%2Fpii%2FS0896627321005018%3Fshowall%3Dtrue" rel="noopener ugc nofollow" target="_blank">发表在《神经元》杂志上的一项最新研究</a>提出了相反的观点。他们发现，至少需要5层神经网络来模拟单个生物神经元的行为。每一个生物神经元大约有1000个人工神经元。</p><p id="6293" class="pw-post-body-paragraph mk ml it mm b mn mo kd mp mq mr kg ms lr mt mu mv lv mw mx my lz mz na nb nc im bi translated">但是，即使GPT 4号没有我们的大脑强大，它肯定会留下一些惊喜。与GPT-3不同，它可能不仅仅是一个语言模型。OpenAI的首席科学家Ilya Sutskever在2020年12月关于多模态的文章中暗示了这一点:</p><blockquote class="nu nv nw"><p id="655e" class="mk ml nd mm b mn mo kd mp mq mr kg ms nx mt mu mv ny mw mx my nz mz na nb nc im bi translated">“2021年，语言模型将开始意识到视觉世界。光是文字就能表达关于这个世界的大量信息，但它是不完整的，因为我们也生活在一个视觉世界里。”</p></blockquote><p id="f4c8" class="pw-post-body-paragraph mk ml it mm b mn mo kd mp mq mr kg ms lr mt mu mv lv mw mx my lz mz na nb nc im bi translated">我们已经用<a class="ae lh" href="https://openai.com/blog/dall-e/" rel="noopener ugc nofollow" target="_blank"> DALL E </a>看到了一些，这是GPT 3(120亿个参数)的一个较小版本，专门针对文本-图像对进行了训练。OpenAI当时说，“通过语言操纵视觉概念现在已经触手可及。”</p><p id="a718" class="pw-post-body-paragraph mk ml it mm b mn mo kd mp mq mr kg ms lr mt mu mv lv mw mx my lz mz na nb nc im bi translated">OpenAI一直在不停地开发GPT 3号的隐藏能力。达尔E是GPT-3的一个特例，非常像<a class="ae lh" href="https://openai.com/blog/openai-codex/" rel="noopener ugc nofollow" target="_blank">抄本</a>。但它们不是绝对的改进，更像是特例。GPT-4承诺更多。它承诺像DALL E(文本图像)和Codex(编码)这样的专家系统的深度与像GPT-3(通用语言)这样的通用系统的宽度相结合。</p><p id="c0bf" class="pw-post-body-paragraph mk ml it mm b mn mo kd mp mq mr kg ms lr mt mu mv lv mw mx my lz mz na nb nc im bi translated">其他类似人类的特征呢，比如推理或常识？在这方面，山姆·奥尔特曼说他们不确定，但他保持“乐观”</p><p id="0d28" class="pw-post-body-paragraph mk ml it mm b mn mo kd mp mq mr kg ms lr mt mu mv lv mw mx my lz mz na nb nc im bi translated">问题很多，回答很少。没有人知道AGI是否有可能。没有人知道如何建造它。没有人知道更大的神经网络是否会越来越接近它。但有一点是不可否认的:GPT 4号将是一个值得关注的东西。</p></div><div class="ab cl md me hx mf" role="separator"><span class="mg bw bk mh mi mj"/><span class="mg bw bk mh mi mj"/><span class="mg bw bk mh mi"/></div><div class="im in io ip iq"><p id="ba46" class="pw-post-body-paragraph mk ml it mm b mn mo kd mp mq mr kg ms lr mt mu mv lv mw mx my lz mz na nb nc im bi translated"><em class="nd">订阅</em> <a class="ae lh" href="https://thealgorithmicbridge.substack.com/" rel="noopener ugc nofollow" target="_blank"> <strong class="mm jd">算法桥</strong> </a> <em class="nd">。弥合算法和人之间的鸿沟。关于与你生活相关的人工智能的时事通讯。</em></p><p id="334c" class="pw-post-body-paragraph mk ml it mm b mn mo kd mp mq mr kg ms lr mt mu mv lv mw mx my lz mz na nb nc im bi translated"><em class="nd">您也可以直接支持我在Medium上的工作，并通过使用我的推荐链接</em> <a class="ae lh" href="https://albertoromgar.medium.com/membership" rel="noopener"> <strong class="mm jd">这里</strong> </a>成为会员来获得无限制的访问权限！<em class="nd"> :) </em></p></div></div>    
</body>
</html>