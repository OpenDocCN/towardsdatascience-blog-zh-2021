<html>
<head>
<title>Train a neural net for semantic segmentation in 50 lines of code, with Pytorch</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用 Pytorch 在 50 行代码中训练一个用于语义分段的神经网络</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/train-neural-net-for-semantic-segmentation-with-pytorch-in-50-lines-of-code-830c71a6544f?source=collection_archive---------0-----------------------#2021-12-03">https://towardsdatascience.com/train-neural-net-for-semantic-segmentation-with-pytorch-in-50-lines-of-code-830c71a6544f?source=collection_archive---------0-----------------------#2021-12-03</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="7add" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">如何用不到 50 行代码(排除导入的话 40 行)训练一个语义切分的神经网络？这里的目标是给出如何使用内置的 Torchvision 神经网络(DeepLabV3)在 PyTorch 中训练语义分割神经网络的最快最简单的概述。</p><p id="236a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">代码可用:<a class="ae kl" href="https://github.com/sagieppel/Train-Semantic-Segmentation-Net-with-Pytorch-In-50-Lines-Of-Code" rel="noopener ugc nofollow" target="_blank">https://github . com/sagie ppel/Train-Semantic-Segmentation-Net-with-py torch-In-50-Lines-Of-Code</a></p></div><div class="ab cl km kn hu ko" role="separator"><span class="kp bw bk kq kr ks"/><span class="kp bw bk kq kr ks"/><span class="kp bw bk kq kr"/></div><div class="ij ik il im in"><p id="4554" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">语义分割的目标是获取图像并识别属于特定类别的区域。这是通过卷积神经网络处理图像来完成的，卷积神经网络输出每像素一类的映射。这些类别是以一组数字的形式给出的。例如，在这种情况下，我们将使用带有三个类的<a class="ae kl" href="https://zenodo.org/record/3697452/" rel="noopener ugc nofollow" target="_blank"> LabPics V1 </a>数据集(如下图所示):</p><figure class="ku kv kw kx gt ky gh gi paragraph-image"><div role="button" tabindex="0" class="kz la di lb bf lc"><div class="gh gi kt"><img src="../Images/8f9e7dc6c941989783f90af8a56de531.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bC19gh9oq8yx8saKSWhVJQ.jpeg"/></div></div><p class="lf lg gj gh gi lh li bd b be z dk translated">图像，相应的分割掩模:黑色(0) =背景，灰色(1) =空容器，白色(2) =填充区域。图片由作者提供。</p></figure><p id="ad61" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">0 类:不是血管(黑色)，<br/>1 类:血管的空区域(灰色)，<br/>2 类:血管的填充区域(白色)。</p><p id="4f62" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">网络的目标是接收一幅图像，并预测每个像素的三个类别之一。</p></div><div class="ab cl km kn hu ko" role="separator"><span class="kp bw bk kq kr ks"/><span class="kp bw bk kq kr ks"/><span class="kp bw bk kq kr"/></div><div class="ij ik il im in"><p id="e9d6" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">第一步从这里下载<a class="ae kl" href="https://zenodo.org/record/3697452/" rel="noopener ugc nofollow" target="_blank"> LabPics </a>数据集:<a class="ae kl" href="https://zenodo.org/record/3697452" rel="noopener ugc nofollow" target="_blank">https://zenodo.org/record/3697452/files/LabPicsV1.zip?下载=1 </a></p><p id="3e2a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">你还需要安装<a class="ae kl" href="https://pytorch.org/" rel="noopener ugc nofollow" target="_blank"> Pytorch </a>和 OpenCV 来读取图像。</p><p id="2388" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">OpenCV 可以通过以下方式安装:</p><p id="9832" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><code class="fe lj lk ll lm b">pip install opencv-python</code></p></div><div class="ab cl km kn hu ko" role="separator"><span class="kp bw bk kq kr ks"/><span class="kp bw bk kq kr ks"/><span class="kp bw bk kq kr"/></div><div class="ij ik il im in"><p id="d564" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">首先，让我们导入包并定义主要的训练参数:</p><pre class="ku kv kw kx gt ln lm lo lp aw lq bi"><span id="5ad1" class="lr ls iq lm b gy lt lu l lv lw">import os<br/>import numpy as np<br/>import cv2<br/>import torchvision.models.segmentation<br/>import torch<br/>import torchvision.transforms as tf</span><span id="d02f" class="lr ls iq lm b gy lx lu l lv lw">Learning_Rate=1e-5</span><span id="927a" class="lr ls iq lm b gy lx lu l lv lw">width=height=800 # image width and height<br/>batchSize=3</span></pre><p id="fe0d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><em class="ly"> Learning_Rate </em>:训练时梯度下降的步长。</p><p id="b2ca" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><em class="ly">宽度</em>和<em class="ly">高度</em>是用于训练的图像尺寸。训练过程中的所有图像都将调整到这个大小。</p><p id="0640" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><em class="ly"> batchSize: i </em> s 将用于每次训练迭代的图像数量。</p><p id="b7e3" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><em class="ly">批次大小*宽度*高度</em> t 将与训练的内存需求成比例。根据您的硬件，可能有必要使用较小的批处理大小来避免内存不足的问题。</p><p id="17d9" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">请注意，由于我们只使用单一的图像大小进行训练，因此训练后的网络很可能仅限于使用这种大小的图像。在大多数情况下，你想做的是改变每个训练批次之间的大小。</p></div><div class="ab cl km kn hu ko" role="separator"><span class="kp bw bk kq kr ks"/><span class="kp bw bk kq kr ks"/><span class="kp bw bk kq kr"/></div><div class="ij ik il im in"><p id="8ab4" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">接下来，我们创建数据集中所有图像的列表:</p><pre class="ku kv kw kx gt ln lm lo lp aw lq bi"><span id="79b9" class="lr ls iq lm b gy lt lu l lv lw">TrainFolder="LabPics/Simple/Train//"<br/>ListImages=os.listdir(os.path.join(TrainFolder, "Image")) </span></pre><p id="f7c9" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">Were <em class="ly"> TrainFolder </em>是 LabPics 数据集的 train 文件夹。<br/>图片存储在 TrainFolder 的“image”子文件夹中。</p></div><div class="ab cl km kn hu ko" role="separator"><span class="kp bw bk kq kr ks"/><span class="kp bw bk kq kr ks"/><span class="kp bw bk kq kr"/></div><div class="ij ik il im in"><p id="4477" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">接下来，我们定义一组将使用 TorchVision 变换模块在图像上执行的变换:</p><pre class="ku kv kw kx gt ln lm lo lp aw lq bi"><span id="93d8" class="lr ls iq lm b gy lt lu l lv lw">transformImg=tf.Compose([tf.ToPILImage(),tf.Resize((height,width)), tf.ToTensor(),tf.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))])</span><span id="0cd6" class="lr ls iq lm b gy lx lu l lv lw">transformAnn=tf.Compose([tf.ToPILImage(),tf.Resize((height,width)), tf.ToTensor()])</span></pre><p id="2de0" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这定义了一组将应用于图像和注释映射的转换。这包括转换为 PIL 格式，这是转换的标准格式。调整大小并转换为 PyTorch 格式。对于图像，我们还通过减去平均值并除以像素强度的标准偏差来归一化图像中的像素强度。预先计算大量图像的平均值和偏差。</p></div><div class="ab cl km kn hu ko" role="separator"><span class="kp bw bk kq kr ks"/><span class="kp bw bk kq kr ks"/><span class="kp bw bk kq kr"/></div><div class="ij ik il im in"><p id="c793" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">接下来，我们创建一个函数，该函数允许我们加载一个随机图像和相应的用于训练的注释图:</p><pre class="ku kv kw kx gt ln lm lo lp aw lq bi"><span id="6874" class="lr ls iq lm b gy lt lu l lv lw">def ReadRandomImage(): </span><span id="9d4a" class="lr ls iq lm b gy lx lu l lv lw">  idx=np.random.randint(0,len(ListImages)) # Pick random image </span><span id="ff08" class="lr ls iq lm b gy lx lu l lv lw">  Img=cv2.imread(os.path.join(TrainFolder, "Image",ListImages[idx]))</span><span id="4931" class="lr ls iq lm b gy lx lu l lv lw">  Filled =  cv2.imread(os.path.join(TrainFolder,   "Semantic/16_Filled", ListImages[idx].replace("jpg","png")),0)       <br/> <br/>  Vessel =  cv2.imread(os.path.join(TrainFolder, "Semantic/1_Vessel", ListImages[idx].replace("jpg","png")),0)</span><span id="a304" class="lr ls iq lm b gy lx lu l lv lw">  AnnMap = np.zeros(Img.shape[0:2],np.float32) # Segmentation map  <br/>  if Vessel is not None:  AnnMap[ Vessel == 1 ] = 1    <br/>  if Filled is not None:  AnnMap[ Filled  == 1 ] = 2</span><span id="e412" class="lr ls iq lm b gy lx lu l lv lw">  Img=transformImg(Img)<br/>  AnnMap=transformAnn(AnnMap)</span><span id="064a" class="lr ls iq lm b gy lx lu l lv lw">  return Img,AnnMap</span></pre><p id="2ab9" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在第一部分中，我们从图像列表中选择一个随机索引，并加载对应于该索引的图像。</p><pre class="ku kv kw kx gt ln lm lo lp aw lq bi"><span id="5970" class="lr ls iq lm b gy lt lu l lv lw">idx = np.random.randint(0,len(ListImages)) # Pick random image</span><span id="3882" class="lr ls iq lm b gy lx lu l lv lw">Img = cv2.imread(os.path.join(TrainFolder, "Image",ListImages[idx]))</span></pre><p id="8605" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">接下来，我们要加载图像的注释遮罩:</p><pre class="ku kv kw kx gt ln lm lo lp aw lq bi"><span id="e996" class="lr ls iq lm b gy lt lu l lv lw">Filled =  cv2.imread(os.path.join(TrainFolder,   "Semantic/16_Filled", ListImages[idx].replace("jpg","png")),0)       <br/> <br/>Vessel =  cv2.imread(os.path.join(TrainFolder, "Semantic/1_Vessel", ListImages[idx].replace("jpg","png")),0)</span></pre><p id="90ce" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这些注释被存储为覆盖属于特定类别(填充/脉管)的区域的图像/遮罩。每个类别掩码存储在单独的<em class="ly">中。png </em>图像文件。其中属于该类的像素值为 1(灰色)，其他像素值为 0(黑色)。</p><figure class="ku kv kw kx gt ky gh gi paragraph-image"><div role="button" tabindex="0" class="kz la di lb bf lc"><div class="gh gi lz"><img src="../Images/68278c39e8b1aefe14e876c0b8e8800b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*I04eoi4F_NQUaUoqni7iNg.jpeg"/></div></div><p class="lf lg gj gh gi lh li bd b be z dk translated">图像、填充和血管区域的遮罩以及统一的注释图(灰色=1，白色=2，黑色=0)。作者的形象。</p></figure><p id="48f2" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">为了训练网络，我们需要创建一个分割图，其中属于空血管区域的像素值为 1(灰色)，属于填充区域的像素值为 2(白色)，其余为 0(黑色)。</p><p id="616a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">首先，我们创建一个在图像形状中充满零的分割图:</p><pre class="ku kv kw kx gt ln lm lo lp aw lq bi"><span id="4481" class="lr ls iq lm b gy lt lu l lv lw">AnnMap = np.zeros(Img.shape[0:2],np.float32)</span></pre><p id="6d88" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">接下来，我们将在血管掩模中具有值 1 的所有像素设置为在分割掩模中具有值 1。并且在填充掩模中值为 1 的所有像素在分割掩模中具有值 2:</p><pre class="ku kv kw kx gt ln lm lo lp aw lq bi"><span id="df89" class="lr ls iq lm b gy lt lu l lv lw">if Vessel is not None:  AnnMap[ Vessel == 1 ] = 1    <br/>if Filled is not None:  AnnMap[ Filled  == 1 ] = 2 </span></pre><p id="f51c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">其中"<em class="ly"> AnnMap[ Filled == 1 ] = 2" </em>表示填充遮罩中值为 1 的每个位置在 AnnMap 中都将得到值 2。</p><p id="3cb2" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">如果没有血管和填充类的注释文件(如果图像中没有出现类，就会出现这种情况)，cv2.imread 将返回 None，并且遮罩将被忽略。</p><p id="7605" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">最后，我们使用前面定义的转换将注释转换成 PyTorch 格式:</p><pre class="ku kv kw kx gt ln lm lo lp aw lq bi"><span id="92ba" class="lr ls iq lm b gy lt lu l lv lw">Img=transformImg(Img)<br/>AnnMap=transformAnn(AnnMap)</span></pre></div><div class="ab cl km kn hu ko" role="separator"><span class="kp bw bk kq kr ks"/><span class="kp bw bk kq kr ks"/><span class="kp bw bk kq kr"/></div><div class="ij ik il im in"><p id="0543" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">为了训练，我们需要使用一批图像。这意味着几幅图像以 4D 矩阵的形式堆叠在一起。我们使用以下函数创建批处理:</p><pre class="ku kv kw kx gt ln lm lo lp aw lq bi"><span id="732e" class="lr ls iq lm b gy lt lu l lv lw">def LoadBatch(): # Load batch of images<br/>    images = torch.zeros([batchSize,3,height,width])<br/>    ann = torch.zeros([batchSize, height, width])<br/>    <br/>    for i in range(batchSize):<br/>        images[i],ann[i]=ReadRandomImage()<br/>    <br/>    return images, ann</span></pre><p id="f317" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">第一部分创建一个空的 4d 矩阵，它将存储具有维度的图像:[批次大小，通道，高度，宽度]，其中通道是图像的层数；这对于 RGB 图像是 3，对于注释图是 1。</p><p id="517e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">下一部分使用我们之前定义的 ReadRandomImage()将一组图像和注释加载到空矩阵中。</p><pre class="ku kv kw kx gt ln lm lo lp aw lq bi"><span id="8f8c" class="lr ls iq lm b gy lt lu l lv lw">for i in range(batchSize):<br/>        images[i],ann[i]=ReadRandomImage()</span></pre></div><div class="ab cl km kn hu ko" role="separator"><span class="kp bw bk kq kr ks"/><span class="kp bw bk kq kr ks"/><span class="kp bw bk kq kr"/></div><div class="ij ik il im in"><p id="2ebc" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">现在我们可以加载数据了，是时候加载神经网络了:</p><pre class="ku kv kw kx gt ln lm lo lp aw lq bi"><span id="bfc4" class="lr ls iq lm b gy lt lu l lv lw">device = torch.device(‘cuda’) if torch.cuda.is_available() else torch.device(‘cpu’)</span><span id="f8fd" class="lr ls iq lm b gy lx lu l lv lw">Net = torchvision.models.segmentation.deeplabv3_resnet50(pretrained=True)</span><span id="fdff" class="lr ls iq lm b gy lx lu l lv lw">Net.classifier[4] = torch.nn.Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1)) # Change final layer to 3 classes</span><span id="03e8" class="lr ls iq lm b gy lx lu l lv lw">Net=Net.to(device)</span><span id="86e5" class="lr ls iq lm b gy lx lu l lv lw">optimizer=torch.optim.Adam(params=Net.parameters(),lr=Learning_Rate,weight_decay=Weight_Decay) # Create adam optimizer</span></pre><p id="c90e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">第一部分是识别计算机是否有 GPU 或 CPU。如果有 Cuda GPU，培训将在 GPU 上进行:</p><pre class="ku kv kw kx gt ln lm lo lp aw lq bi"><span id="4b63" class="lr ls iq lm b gy lt lu l lv lw">device = torch.device(‘cuda’) if torch.cuda.is_available() else torch.device(‘cpu’)</span></pre><p id="9dfe" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">对于任何实际的数据集，使用 CPU 进行训练都是极其缓慢的。</p><p id="23c4" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">接下来，我们加载深度实验室网络语义分割:</p><pre class="ku kv kw kx gt ln lm lo lp aw lq bi"><span id="6cc4" class="lr ls iq lm b gy lt lu l lv lw"><br/>Net = torchvision.models.segmentation.deeplabv3_resnet50(pretrained=True)</span></pre><p id="44b2" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">包含许多有用的语义分割模型，如 UNET 和 FCN。我们选择 Deeplabv3，因为它是最好的语义分割网络之一。通过设置<em class="ly"> pretrained=True </em>，我们在 COCO 数据集上加载预训练的权重。当学习一个新问题时，从预先训练的模型开始总是更好，因为它允许网络使用以前的经验并更快地收敛。</p><p id="f065" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们可以通过编写以下内容来查看我们刚刚加载的网络的所有层:</p><p id="dfaf" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><em class="ly">打印(网)</em></p><blockquote class="ma mb mc"><p id="70c8" class="jn jo ly jp b jq jr js jt ju jv jw jx md jz ka kb me kd ke kf mf kh ki kj kk ij bi">….</p><p id="77db" class="jn jo ly jp b jq jr js jt ju jv jw jx md jz ka kb me kd ke kf mf kh ki kj kk ij bi translated">(1): Conv2d(256，256，kernel_size=(3，3)，stride=(1，1)，padding=(1，1)，bias = False)<br/>(2):batch norm 2d(256，eps=1e-05，momentum=0.1，affine=True，track _ running _ stats = True)<br/>(3):ReLU()<br/>(4):conv 2d(256，21，kernel_size=(1，1)，stride=(1，1))</p></blockquote><p id="1538" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这将按使用顺序打印层的网络。网络的最后一层是具有 256 层输入和 21 层输出的卷积层。21 代表输出类的数量。因为我们的数据集中只有 3 个类，所以我们想用一个具有 3 个输出的新卷积层来替换它:</p><pre class="ku kv kw kx gt ln lm lo lp aw lq bi"><span id="6b5d" class="lr ls iq lm b gy lt lu l lv lw">Net.classifier[4] = torch.nn.Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1)) </span></pre><p id="0341" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">公平地说，这部分是可选的，因为具有 21 个输出类的网络可以简单地通过忽略剩余的 18 个类来预测 3 个类。但这样更优雅。</p><p id="b69f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">接下来，我们将网络加载到我们的 GPU 或 CPU 设备中:</p><pre class="ku kv kw kx gt ln lm lo lp aw lq bi"><span id="323b" class="lr ls iq lm b gy lt lu l lv lw">Net=Net.to(device)</span></pre><p id="3b3b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">最后，我们加载一个优化器:</p><pre class="ku kv kw kx gt ln lm lo lp aw lq bi"><span id="70c9" class="lr ls iq lm b gy lt lu l lv lw">optimizer=torch.optim.Adam(params=Net.parameters(),lr=Learning_Rate) # Create adam optimizer</span></pre><p id="451b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">优化器将在训练的反向传播步骤中控制梯度率。亚当优化器是最快的优化器之一。</p></div><div class="ab cl km kn hu ko" role="separator"><span class="kp bw bk kq kr ks"/><span class="kp bw bk kq kr ks"/><span class="kp bw bk kq kr"/></div><div class="ij ik il im in"><p id="e259" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">最后，我们开始训练循环:</p><pre class="ku kv kw kx gt ln lm lo lp aw lq bi"><span id="c35f" class="lr ls iq lm b gy lt lu l lv lw">for itr in range(20000): # Training loop<br/>   images,ann=LoadBatch() <br/>   <br/>   images=torch.autograd.Variable(images,requires_grad=False)<br/>   .to(device)    <br/>   <br/>   ann = torch.autograd.Variable(ann,requires_grad=False).to(device)              <br/>   <br/>   Pred=Net(images)[‘out’] # make prediction</span></pre><p id="5eea" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">LoadBatch 是前面定义的，加载批量的图像和标注图。<em class="ly">图像</em>和<em class="ly"> ann </em>将存储加载的图像和注释。</p><p id="a57f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">torch . autograded . variable:将数据转换成网络可以使用的梯度变量。我们设置<em class="ly"> Requires_grad=False </em>,因为我们不想将渐变应用于图像，只应用于网络的层。<em class="ly"> to(device) </em>将张量复制到与网络相同的设备(GPU/CPU)中。</p><p id="3d7d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">最后，我们将图像输入网络，得到预测。</p><pre class="ku kv kw kx gt ln lm lo lp aw lq bi"><span id="02ea" class="lr ls iq lm b gy lt lu l lv lw">Pred=Net(images)[‘out’] # make prediction</span></pre></div><div class="ab cl km kn hu ko" role="separator"><span class="kp bw bk kq kr ks"/><span class="kp bw bk kq kr ks"/><span class="kp bw bk kq kr"/></div><div class="ij ik il im in"><p id="9d05" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">一旦我们做出了预测，我们就可以将其与真实(地面实况)注释进行比较，并计算损失:</p><pre class="ku kv kw kx gt ln lm lo lp aw lq bi"><span id="7931" class="lr ls iq lm b gy lt lu l lv lw">criterion = torch.nn.CrossEntropyLoss() # Set loss function<br/>Loss=criterion(Pred,ann.long()) # Calculate cross entropy loss<br/>Loss.backward() # Backpropogate loss<br/>Optimizer.step() # Apply gradient descent change to weight</span></pre><p id="3c64" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">首先，我们定义损失函数。我们使用标准的交叉熵损失:</p><pre class="ku kv kw kx gt ln lm lo lp aw lq bi"><span id="7b1c" class="lr ls iq lm b gy lt lu l lv lw">criterion = torch.nn.CrossEntropyLoss()</span></pre><p id="f637" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们使用此函数来计算使用预测和真实注释的损失:</p><pre class="ku kv kw kx gt ln lm lo lp aw lq bi"><span id="6d18" class="lr ls iq lm b gy lt lu l lv lw">Loss=criterion(Pred,ann.long())</span></pre><p id="7e0f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">一旦我们计算出损失，我们就可以应用反向传播并改变净重。</p><pre class="ku kv kw kx gt ln lm lo lp aw lq bi"><span id="f8ac" class="lr ls iq lm b gy lt lu l lv lw">Loss.backward() # Backpropogate loss<br/>Optimizer.step() # Apply gradient descent change to weight</span></pre></div><div class="ab cl km kn hu ko" role="separator"><span class="kp bw bk kq kr ks"/><span class="kp bw bk kq kr ks"/><span class="kp bw bk kq kr"/></div><div class="ij ik il im in"><p id="3bc6" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这涵盖了整个训练阶段，但是我们还需要保存训练好的模型。否则，一旦程序停止，它就会丢失。<br/>保存非常耗时，所以我们希望每 1000 步保存一次:</p><pre class="ku kv kw kx gt ln lm lo lp aw lq bi"><span id="a6d4" class="lr ls iq lm b gy lt lu l lv lw">if itr % 1000 == 0: <br/>   print(“Saving Model” +str(itr) + “.torch”)<br/>   torch.save(Net.state_dict(), str(itr) + “.torch”)</span></pre><p id="e23e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在运行这个脚本大约 3000 步之后，网络应该会给出不错的结果。</p><p id="fd84" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">完整代码可在此处找到:</p><div class="mg mh gp gr mi mj"><a href="https://github.com/sagieppel/Train-Semantic-Segmentation-Net-with-Pytorch-In-50-Lines-Of-Code" rel="noopener  ugc nofollow" target="_blank"><div class="mk ab fo"><div class="ml ab mm cl cj mn"><h2 class="bd ir gy z fp mo fr fs mp fu fw ip bi translated">GitHub-sagieppel/Train-Semantic-Segmentation-Net-with-py torch-In-50-line Of-Code:Train neural…</h2><div class="mq l"><h3 class="bd b gy z fp mo fr fs mp fu fw dk translated">用 pytorch 在不到 50 行代码中训练语义分割的神经网络(deep lab V3)</h3></div><div class="mr l"><p class="bd b dl z fp mo fr fs mp fu fw dk translated">github.com</p></div></div><div class="ms l"><div class="mt l mu mv mw ms mx ld mj"/></div></div></a></div><p id="38ad" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">总共 50 行代码，不包括空格，40 行代码，不包括导入:-)</p></div><div class="ab cl km kn hu ko" role="separator"><span class="kp bw bk kq kr ks"/><span class="kp bw bk kq kr ks"/><span class="kp bw bk kq kr"/></div><div class="ij ik il im in"><p id="ad60" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">最后，一旦网络被训练，我们想申请分割真实的图像，看看结果。我们使用一个单独的推理脚本来实现这一点，该脚本使用训练网络来分割图像:</p><pre class="ku kv kw kx gt ln lm lo lp aw lq bi"><span id="7dfe" class="lr ls iq lm b gy lt lu l lv lw">import cv2<br/>import torchvision.models.segmentation<br/>import torch<br/>import torchvision.transforms as tf<br/>import matplotlib.pyplot as plt</span><span id="6bb7" class="lr ls iq lm b gy lx lu l lv lw">modelPath = "3000.torch"  # Path to trained model<br/>imagePath = "test.jpg"  # Test image<br/>height=width=900</span><span id="6bcb" class="lr ls iq lm b gy lx lu l lv lw">transformImg = tf.Compose([tf.ToPILImage(), tf.Resize((height, width)), tf.ToTensor(),tf.Normalize((0.485, 0.456, 0.406),(0.229, 0.224, 0.225))])  <br/><br/>device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu') <br/>Net = torchvision.models.segmentation.deeplabv3_resnet50(pretrained=True)  </span><span id="4c19" class="lr ls iq lm b gy lx lu l lv lw">Net.classifier[4] = torch.nn.Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1)) </span><span id="f2cf" class="lr ls iq lm b gy lx lu l lv lw">Net = Net.to(device)  # Set net to GPU or CPU</span><span id="9dd1" class="lr ls iq lm b gy lx lu l lv lw">Net.load_state_dict(torch.load(modelPath)) # Load trained model</span><span id="1f20" class="lr ls iq lm b gy lx lu l lv lw">Net.eval() # Set to evaluation mode</span><span id="1636" class="lr ls iq lm b gy lx lu l lv lw">Img = cv2.imread(imagePath) # load test image</span><span id="7d15" class="lr ls iq lm b gy lx lu l lv lw">height_orgin , widh_orgin ,d = Img.shape # Get image original size </span><span id="98f9" class="lr ls iq lm b gy lx lu l lv lw">plt.imshow(Img[:,:,::-1])  # Show imageplt.show()<br/>Img = transformImg(Img)  # Transform to pytorch</span><span id="e4d0" class="lr ls iq lm b gy lx lu l lv lw">Img = torch.autograd.Variable(Img, requires_grad=False).to(device).unsqueeze(0)</span><span id="7324" class="lr ls iq lm b gy lx lu l lv lw">with torch.no_grad():<br/>    Prd = Net(Img)['out']  # Run net</span><span id="f85c" class="lr ls iq lm b gy lx lu l lv lw"># resize to orginal size<br/>Prd = tf.Resize((height_orgin,widh_orgin))(Prd[0])</span><span id="6337" class="lr ls iq lm b gy lx lu l lv lw">#Convert probability to class map<br/>seg = torch.argmax(Prd, 0).cpu().detach().numpy()  </span><span id="c241" class="lr ls iq lm b gy lx lu l lv lw">plt.imshow(seg)  # display image<br/>plt.show()</span></pre><p id="0c04" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这里的大部分代码与训练脚本相同，只有几处不同:</p><pre class="ku kv kw kx gt ln lm lo lp aw lq bi"><span id="4466" class="lr ls iq lm b gy lt lu l lv lw">Net.load_state_dict(torch.load(modelPath)) # Load trained model</span></pre><p id="3bfb" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">从<em class="ly">模型路径</em>中的文件加载我们之前训练并保存的网络</p><pre class="ku kv kw kx gt ln lm lo lp aw lq bi"><span id="c3b3" class="lr ls iq lm b gy lt lu l lv lw">Net.eval()</span></pre><p id="b11a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">将网络从培训模式转换为评估模式。这主要意味着不会计算批量标准化统计数据。</p><pre class="ku kv kw kx gt ln lm lo lp aw lq bi"><span id="04c2" class="lr ls iq lm b gy lt lu l lv lw">with torch.no_grad():</span></pre><p id="d497" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这意味着网络运行时不收集梯度。梯度仅与训练相关，并且收集它们是资源密集型的。</p><p id="d35c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">请注意，<em class="ly"> Pred </em>中的输出将被映射为每幅图像 3 个通道，每个通道代表 3 个类别之一的非标准化概率。为了找到每个像素所属的类，我们使用 argmax 函数取具有 3 的最高值的通道(类):</p><pre class="ku kv kw kx gt ln lm lo lp aw lq bi"><span id="65d9" class="lr ls iq lm b gy lt lu l lv lw">seg = torch.argmax(Prd[0], 0)</span></pre><p id="4c31" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们对输出图中的每个像素都这样做，并为每个像素获得 3 个类中的一个。</p><p id="184e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">结果是:</p><p id="0500" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">输入图像:</p><figure class="ku kv kw kx gt ky gh gi paragraph-image"><div class="gh gi my"><img src="../Images/82b88244605cf53e44391cf35d4be3d4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*o-6Ap6mycKC-GmKlOlxMWQ.png"/></div></figure><p id="52d2" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">输出预测:</p><figure class="ku kv kw kx gt ky gh gi paragraph-image"><div class="gh gi my"><img src="../Images/ac6c4d867cd90ed679cb5625b0ea8896.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*rXYLeC3eZXLh03ZoXOPdCg.png"/></div></figure></div></div>    
</body>
</html>