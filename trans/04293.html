<html>
<head>
<title>Deep Learning Techniques for Text Classification</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">文本分类的深度学习技术</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/deep-learning-techniques-for-text-classification-78d9dc40bf7c?source=collection_archive---------2-----------------------#2021-04-12">https://towardsdatascience.com/deep-learning-techniques-for-text-classification-78d9dc40bf7c?source=collection_archive---------2-----------------------#2021-04-12</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="3990" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">使用Word2Vec对您常用的深度学习架构评估基于TCN和集成的模型的性能</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/7f16bab179eaf45fda01e6fc34ae6bde.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TeyJDOo8zo_vdbT94CYsyw.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">安妮·斯普拉特在<a class="ae kv" href="https://unsplash.com/s/photos/documents?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><h1 id="57fb" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">A.介绍</h1><h2 id="4430" class="lo kx iq bd ky lp lq dn lc lr ls dp lg lt lu lv li lw lx ly lk lz ma mb lm mc bi translated">A.1 .背景和动机</h2><p id="d92c" class="pw-post-body-paragraph md me iq mf b mg mh jr mi mj mk ju ml lt mm mn mo lw mp mq mr lz ms mt mu mv ij bi mw translated"><span class="l mx my mz bm na nb nc nd ne di"> T </span> ext classification是NLP中最流行的任务之一，它允许程序根据预定义的类对自由文本文档进行分类。这些课程可以基于主题、流派或情感。今天，大量数字文档的出现使得文本分类任务变得更加重要，尤其是对于公司来说，以最大化其工作流程甚至利润。</p><p id="00d0" class="pw-post-body-paragraph md me iq mf b mg nf jr mi mj ng ju ml lt nh mn mo lw ni mq mr lz nj mt mu mv ij bi translated">近年来，自然语言处理在文本分类方面的研究已经达到了最先进的水平(SOTA)。它已经取得了惊人的成果，表明深度学习方法是执行此类任务的尖端技术。</p><p id="8958" class="pw-post-body-paragraph md me iq mf b mg nf jr mi mj ng ju ml lt nh mn mo lw ni mq mr lz nj mt mu mv ij bi translated">因此，需要评估SOTA深度学习模型在文本分类方面的性能，这不仅是学术目的，也是人工智能从业者或专业人士在类似项目中需要指导和基准的必要条件。</p><h2 id="6094" class="lo kx iq bd ky lp lq dn lc lr ls dp lg lt lu lv li lw lx ly lk lz ma mb lm mc bi translated">A.2目标</h2><p id="d1e1" class="pw-post-body-paragraph md me iq mf b mg mh jr mi mj mk ju ml lt mm mn mo lw mp mq mr lz ms mt mu mv ij bi translated">实验将在<strong class="mf ir">五个</strong>文本分类数据集上评估一些流行的深度学习模型的性能，如<strong class="mf ir">前馈</strong>、<strong class="mf ir">递归</strong>、<strong class="mf ir">卷积</strong>和<strong class="mf ir">集成</strong> - <strong class="mf ir">基于</strong>神经网络。我们将在两个独立的特征提取之上构建每个模型，以捕捉文本中的信息。</p><p id="c1bb" class="pw-post-body-paragraph md me iq mf b mg nf jr mi mj ng ju ml lt nh mn mo lw ni mq mr lz nj mt mu mv ij bi translated">结果显示:</p><ul class=""><li id="852b" class="nk nl iq mf b mg nf mj ng lt nm lw nn lz no mv np nq nr ns bi translated"><strong class="mf ir">单词嵌入的鲁棒性</strong>作为所有模型的特征提取器，以做出更好的最终预测。</li><li id="3afb" class="nk nl iq mf b mg nt mj nu lt nv lw nw lz nx mv np nq nr ns bi translated"><strong class="mf ir">基于集合的</strong>和<strong class="mf ir">时间卷积神经网络</strong>在实现良好性能甚至与最先进的基准模型竞争方面的有效性。</li></ul><h1 id="8ddb" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">B.实验</h1><h2 id="6d2b" class="lo kx iq bd ky lp lq dn lc lr ls dp lg lt lu lv li lw lx ly lk lz ma mb lm mc bi translated">B.1 .数据集</h2><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/7785ae39b81d4995d189856c1d151b5d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AnRF4NzLpes_aTVMFQOjWA.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><strong class="bd nz">表1。</strong>标记化后的数据集统计。<strong class="bd nz"> *CV </strong>代表交叉验证。这意味着原始数据集没有标准的训练/测试分割。因此，我们使用10倍CV。</p></figure><ul class=""><li id="0f20" class="nk nl iq mf b mg nf mj ng lt nm lw nn lz no mv np nq nr ns bi translated"><strong class="mf ir">先生</strong>。电影评论——将评论分为正面或负面[1]。<em class="oa"> </em> <a class="ae kv" href="https://www.cs.cornell.edu/people/pabo/movie-review-data/" rel="noopener ugc nofollow" target="_blank"> <em class="oa">链接</em> </a></li><li id="b0ed" class="nk nl iq mf b mg nt mj nu lt nv lw nw lz nx mv np nq nr ns bi translated"><strong class="mf ir">subject</strong>。主观性——将句子分为主观和客观两类[2]。<a class="ae kv" href="https://www.cs.cornell.edu/people/pabo/movie-review-data/" rel="noopener ugc nofollow" target="_blank"> <em class="oa">链接</em> </a></li><li id="2d80" class="nk nl iq mf b mg nt mj nu lt nv lw nw lz nx mv np nq nr ns bi translated"><strong class="mf ir"> TREC </strong>。文本检索会议——将问题分为六类(人、位置、数字信息等)。) [3].<a class="ae kv" href="https://cogcomp.seas.upenn.edu/Data/QA/QC/" rel="noopener ugc nofollow" target="_blank"> <em class="oa">链接</em> </a></li><li id="ff35" class="nk nl iq mf b mg nt mj nu lt nv lw nw lz nx mv np nq nr ns bi translated"><strong class="mf ir"> CR </strong>。客户评论—对产品评论进行分类(相机、MP3等。)作为肯定或否定[4]。<a class="ae kv" href="https://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html" rel="noopener ugc nofollow" target="_blank"> <em class="oa">链接</em> </a></li><li id="c3b2" class="nk nl iq mf b mg nt mj nu lt nv lw nw lz nx mv np nq nr ns bi translated"><strong class="mf ir"> MPQA </strong>。多视角问答——观点极性检测[5]。<a class="ae kv" href="http://mpqa.cs.pitt.edu/" rel="noopener ugc nofollow" target="_blank"> <em class="oa">链接</em> </a></li><li id="a3cd" class="nk nl iq mf b mg nt mj nu lt nv lw nw lz nx mv np nq nr ns bi translated">为了方便起见，我们在这里准备了pickle格式的数据集<a class="ae kv" href="https://github.com/diardanoraihan/Text_Classification_Capstone/tree/main/0_data" rel="noopener ugc nofollow" target="_blank"/>。</li></ul><h2 id="bb68" class="lo kx iq bd ky lp lq dn lc lr ls dp lg lt lu lv li lw lx ly lk lz ma mb lm mc bi translated">B.2 .提议的模型</h2><h2 id="f1c9" class="lo kx iq bd ky lp lq dn lc lr ls dp lg lt lu lv li lw lx ly lk lz ma mb lm mc bi translated"><strong class="ak"> B.2.1 .时间卷积网络(TCN) </strong></h2><p id="7609" class="pw-post-body-paragraph md me iq mf b mg mh jr mi mj mk ju ml lt mm mn mo lw mp mq mr lz ms mt mu mv ij bi translated">白等人[6]提出了一个通用的时间卷积网络(TCN)作为扩张因果版本的CNN。它是递归架构的强大替代，可以处理长输入序列，而不会遭受消失或爆炸梯度问题。如果您想了解更多关于模型块的信息，您可以参考[6]和[7]中的实现。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ob"><img src="../Images/3e39b89676cef6e3413bada0f92563bc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8hqRDaoYsfQHpMBuK7yHhg.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><strong class="bd nz">图一。TCN的元素建筑。(来源:[6])</strong></p></figure><p id="7dab" class="pw-post-body-paragraph md me iq mf b mg nf jr mi mj ng ju ml lt nh mn mo lw ni mq mr lz nj mt mu mv ij bi translated">提议中的TCN模型是受Kaggle上的大师之一Christof Henkel [8]的启发。该模型包括:</p><ul class=""><li id="66b9" class="nk nl iq mf b mg nf mj ng lt nm lw nn lz no mv np nq nr ns bi translated">内核大小为3、膨胀因子为1、2和4的两个TCN块堆叠在一起。</li><li id="8fa4" class="nk nl iq mf b mg nt mj nu lt nv lw nw lz nx mv np nq nr ns bi translated">第一TCN块包含128个滤波器，第二块使用64个滤波器。输入特征将基于单词嵌入。</li><li id="29aa" class="nk nl iq mf b mg nt mj nu lt nv lw nw lz nx mv np nq nr ns bi translated">每个块的结果将采取序列的形式。</li><li id="54d2" class="nk nl iq mf b mg nt mj nu lt nv lw nw lz nx mv np nq nr ns bi translated">最终的序列然后被传递到两个不同的全局池层。</li><li id="3528" class="nk nl iq mf b mg nt mj nu lt nv lw nw lz nx mv np nq nr ns bi translated">接下来，两个结果被连接并传递到16个神经元的密集层，并传递到输出。</li></ul><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oc"><img src="../Images/7952d72f6d8a67aa93ea8a06011df94e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WdPg6W4QEPyBYh7jgkim4A.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><strong class="bd nz">图二。</strong>提议的TCN模式。</p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi od"><img src="../Images/b61981c22562e2e5ecf0aa7bce834ef1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*v4brRUPM2eDLJ8C0UGdIJw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><strong class="bd nz">表二。</strong>TCN模型的超参数。</p></figure><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="oe of l"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">Python和TensorFlow 2中TCN模型的实现。</p></figure><h2 id="09af" class="lo kx iq bd ky lp lq dn lc lr ls dp lg lt lu lv li lw lx ly lk lz ma mb lm mc bi translated"><strong class="ak">b . 2.2 . CNN-GRU合奏</strong></h2><p id="e80d" class="pw-post-body-paragraph md me iq mf b mg mh jr mi mj mk ju ml lt mm mn mo lw mp mq mr lz ms mt mu mv ij bi translated">K.Kowsari等人[9]介绍了一种新的分类深度学习技术，称为随机多模型深度学习(RMDL)。该模型可用于任何分类任务。下图说明了使用深度RNN、深度CNN和深度前馈神经网络(DNN)的架构。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi og"><img src="../Images/7d05dc4bb1e8575c6c0868c45f744f77.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NXUX3L_lVv9Q8ypQvbYwVQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><strong class="bd nz">图3。</strong>对RMDL架构进行分类。(来源:[9])</p></figure><p id="07a3" class="pw-post-body-paragraph md me iq mf b mg nf jr mi mj ng ju ml lt nh mn mo lw ni mq mr lz nj mt mu mv ij bi translated">在这个项目中，我们通过将<strong class="mf ir"> 1D CNN </strong>与单个<strong class="mf ir">双向GRU (BiGRU) </strong>相结合，实现了一个基于集成学习的模型。</p><ul class=""><li id="655e" class="nk nl iq mf b mg nf mj ng lt nm lw nn lz no mv np nq nr ns bi translated">1D CNN已经被证明在文本分类上工作得很好，尽管只有很少的参数调整[10]。</li><li id="ea08" class="nk nl iq mf b mg nt mj nu lt nv lw nw lz nx mv np nq nr ns bi translated">另一方面，BiGRU通过获取序列中较早和较晚的信息，可以很好地处理时态数据。</li></ul><p id="4e80" class="pw-post-body-paragraph md me iq mf b mg nf jr mi mj ng ju ml lt nh mn mo lw ni mq mr lz nj mt mu mv ij bi translated">我们将在实验中看到这种组合如何影响模型精度。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oh"><img src="../Images/10fa6599e119520f1db35a933ce60ad1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DFgjVswRPRmmoMfwfKpJ2g.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><strong class="bd nz">图4。</strong>提出了CNN和BiGRU相结合的集成学习模型。</p></figure><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="oe of l"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">集合CNN-GRU模型在Python和TensorFlow 2中的实现</p></figure><h2 id="c267" class="lo kx iq bd ky lp lq dn lc lr ls dp lg lt lu lv li lw lx ly lk lz ma mb lm mc bi translated">B.2.3 .其他模型</h2><p id="f077" class="pw-post-body-paragraph md me iq mf b mg mh jr mi mj mk ju ml lt mm mn mo lw mp mq mr lz ms mt mu mv ij bi translated">为了比较性能，我们还将评估其他受欢迎的型号，例如:</p><ul class=""><li id="0888" class="nk nl iq mf b mg nf mj ng lt nm lw nn lz no mv np nq nr ns bi translated"><strong class="mf ir"> SNN。</strong>一个浅层神经网络。</li></ul><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oi"><img src="../Images/f0cb8624c433691ca509468e231420e9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qIa58hHgIT8KPlmDtjfNZA.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><strong class="bd nz">表3。</strong>SNN模型的超参数。</p></figure><ul class=""><li id="7bf7" class="nk nl iq mf b mg nf mj ng lt nm lw nn lz no mv np nq nr ns bi translated"><strong class="mf ir"> edRVFL。</strong>集成深度随机向量函数链接神经网络。</li></ul><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oj"><img src="../Images/e91296a5587587bcd1c7810024f80339.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NAfuNkRUxuvH5Rk7B5bhDA.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><strong class="bd nz">表4。</strong>edr vfl模型的超参数。</p></figure><ul class=""><li id="0568" class="nk nl iq mf b mg nf mj ng lt nm lw nn lz no mv np nq nr ns bi translated"><strong class="mf ir"> 1D有线电视新闻网。</strong>我们的<strong class="mf ir">基线模型</strong>代表具有一维卷积和池层的神经网络。</li></ul><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ok"><img src="../Images/4a0bafadb7e6d91b5aaf9a09e548986f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lO3pqQGKplAd-yOffR_dRg.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">表5。1D CNN模型的超参数</p></figure><ul class=""><li id="3ca6" class="nk nl iq mf b mg nf mj ng lt nm lw nn lz no mv np nq nr ns bi translated"><strong class="mf ir">(堆叠)BiGRU/BiLSTM。</strong>双向门控循环单位/长短期记忆。它的堆叠版本意味着我们向网络添加了另一个双向块。</li></ul><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oi"><img src="../Images/f367ee88a3bcddf8b85b04ec4a23409b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kRzBpokrA-AiTSUe22lIkQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><strong class="bd nz">表6。</strong>big ru/bil STM模型的超参数</p></figure><h2 id="19f2" class="lo kx iq bd ky lp lq dn lc lr ls dp lg lt lu lv li lw lx ly lk lz ma mb lm mc bi translated">B.2.4 .模型摘要及其特征提取</h2><p id="4e22" class="pw-post-body-paragraph md me iq mf b mg mh jr mi mj mk ju ml lt mm mn mo lw mp mq mr lz ms mt mu mv ij bi translated">总而言之，我们将在五个文本分类数据集上使用两种不同的特征提取来构建深度学习模型，如下所示:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ol"><img src="../Images/c0fdda1c74a7d38ca0d6d36853961794.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RSZEqKBIO8vInaSACH8RCg.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><strong class="bd nz">表7</strong>项目中使用的模型和特征提取的变体。<strong class="bd nz"> * </strong> <em class="om">对所有模型来说，训练过程都是通过提前停止来完成的，模型会在超出训练数据之前停止训练</em>。</p></figure><ul class=""><li id="e7e8" class="nk nl iq mf b mg nf mj ng lt nm lw nn lz no mv np nq nr ns bi translated"><strong class="mf ir">我们——兰德。</strong>该模型使用嵌入层，在该层中，单词向量在训练期间被随机初始化和校正</li><li id="961b" class="nk nl iq mf b mg nt mj nu lt nv lw nw lz nx mv np nq nr ns bi translated"><strong class="mf ir">我们——静。</strong>该模型使用名为<strong class="mf ir"> Word2Vec </strong>的预训练单词嵌入，具有300维向量。向量在训练期间保持静态。使用一般正态分布随机初始化未知单词的向量。</li><li id="62e1" class="nk nl iq mf b mg nt mj nu lt nv lw nw lz nx mv np nq nr ns bi translated"><strong class="mf ir">我们——动态。</strong>同上，但向量在训练期间被修改，而不是静态的。</li><li id="9264" class="nk nl iq mf b mg nt mj nu lt nv lw nw lz nx mv np nq nr ns bi translated"><strong class="mf ir"> WE-avg。</strong>该模型使用来自预训练单词嵌入的向量的平均值来获得输入上下文。因此，输入特征的大小将与word 2 vec 300中使用的向量维数的大小相同。</li><li id="4f4d" class="nk nl iq mf b mg nt mj nu lt nv lw nw lz nx mv np nq nr ns bi translated"><strong class="mf ir">字袋(蝴蝶结)。</strong>在将文本输入模型之前，它将文本表示为文档中出现的单词数。我们将使用四个单词评分选项:<em class="oa">二进制</em>、<em class="oa">计数</em>、<em class="oa">频率</em>和<em class="oa"> TF-IDF </em>。</li></ul><p id="62f6" class="pw-post-body-paragraph md me iq mf b mg nf jr mi mj ng ju ml lt nh mn mo lw ni mq mr lz nj mt mu mv ij bi translated">这项工作中使用的基准是:</p><ul class=""><li id="1a2b" class="nk nl iq mf b mg nf mj ng lt nm lw nn lz no mv np nq nr ns bi translated"><strong class="mf ir">CNN-多频道</strong> (Yoon Kim，2014)【10】</li><li id="ccbe" class="nk nl iq mf b mg nt mj nu lt nv lw nw lz nx mv np nq nr ns bi translated"><strong class="mf ir"> SuBiLSTM </strong>(悉达多梵天，2018) [11]</li><li id="845a" class="nk nl iq mf b mg nt mj nu lt nv lw nw lz nx mv np nq nr ns bi translated"><strong class="mf ir">SuBiLSTM-并列</strong>(悉达多梵天，2018) [11]</li><li id="0fd1" class="nk nl iq mf b mg nt mj nu lt nv lw nw lz nx mv np nq nr ns bi translated"><strong class="mf ir"> USE_T+CNN </strong> (Cer et al .，2018) [12]</li></ul><h1 id="b60d" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">C.估价</h1><h2 id="191b" class="lo kx iq bd ky lp lq dn lc lr ls dp lg lt lu lv li lw lx ly lk lz ma mb lm mc bi translated">结果</h2><p id="e2fa" class="pw-post-body-paragraph md me iq mf b mg mh jr mi mj mk ju ml lt mm mn mo lw mp mq mr lz ms mt mu mv ij bi translated">我们将使用准确性和排名作为比较指标。将根据每个数据集的准确性来计算等级。在有平局的情况下，我们平均他们的排名。</p><p id="f261" class="pw-post-body-paragraph md me iq mf b mg nf jr mi mj ng ju ml lt nh mn mo lw ni mq mr lz nj mt mu mv ij bi translated"><strong class="mf ir">表8 </strong>显示了各型号性能的最终对比。我们还包括SOTA基准模型(在底部)以供进一步观察。注意，我们只包括使用单词袋和平均单词嵌入的模型的最佳结果(SNN和edRVFL)，</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="oe of l"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><strong class="ak">表八</strong>针对基准提出的深度学习模型。</p></figure><p id="d5c5" class="pw-post-body-paragraph md me iq mf b mg nf jr mi mj ng ju ml lt nh mn mo lw ni mq mr lz nj mt mu mv ij bi translated">从<strong class="mf ir">表8 </strong>中，我们可以计算出5个数据集上模型相对于<strong class="mf ir">基线</strong> ( <strong class="mf ir"> 1D CNN-rand </strong>)的平均精度差如下:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="oe of l"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><strong class="ak">图5。</strong>在5个数据集上，模型相对于基线的平均精度差</p></figure><p id="778b" class="pw-post-body-paragraph md me iq mf b mg nf jr mi mj ng ju ml lt nh mn mo lw ni mq mr lz nj mt mu mv ij bi translated">在<strong class="mf ir">图5 </strong>中，绿色条代表基准模型。紫色条描绘了超过基线的前六个建议模型。最后，红色柱是具有最低精确度的建议模型。减号(-)表示在以基线为参考的所有数据集中，该模型的精度远低于较高的模型。</p><p id="c39f" class="pw-post-body-paragraph md me iq mf b mg nf jr mi mj ng ju ml lt nh mn mo lw ni mq mr lz nj mt mu mv ij bi translated">从那里，我们可以计算平均等级值，并将结果可视化，如下所示:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="oe of l"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><strong class="ak">图六。</strong>每个模型相对于基准的平均等级值</p></figure><h2 id="21e5" class="lo kx iq bd ky lp lq dn lc lr ls dp lg lt lu lv li lw lx ly lk lz ma mb lm mc bi translated">C.2讨论</h2><h2 id="1bfc" class="lo kx iq bd ky lp lq dn lc lr ls dp lg lt lu lv li lw lx ly lk lz ma mb lm mc bi translated">C.2.1. BoW与单词嵌入</h2><p id="d968" class="pw-post-body-paragraph md me iq mf b mg mh jr mi mj mk ju ml lt mm mn mo lw mp mq mr lz ms mt mu mv ij bi translated">尽管有如此多的超参数调整，这个实验中带有BoW的模型不能做太多。大量的文本数据会使BoW的词汇变得广泛。因此，输入要素将是稀疏形式，通过许多零来表示一位信息。这种文本表示使模型更难训练以获得更好的结果。除非我们指定的词汇量不够大或者使用的语料库很小，否则BoW不是一个可靠的选择。</p><p id="150d" class="pw-post-body-paragraph md me iq mf b mg nf jr mi mj ng ju ml lt nh mn mo lw ni mq mr lz nj mt mu mv ij bi translated">另一方面，当使用单词嵌入时，模型表现得更好。通过仅取Word2Vec的平均值来获得N维特征输入，该模型可以在精确度上有高达10%的非常陡峭的增加。例如，在TREC数据集中，edRVFL和SNN都从75.2和76.2突然跳到83.6和85.8。这些结果证明了单词嵌入作为默认特征提取器的重要性。</p><h2 id="aa46" class="lo kx iq bd ky lp lq dn lc lr ls dp lg lt lu lv li lw lx ly lk lz ma mb lm mc bi translated">C.2.2随机、静态和动态</h2><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="oe of l"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><strong class="ak">图7。</strong>不同单词嵌入方式之间的平均准确率。</p></figure><p id="3eee" class="pw-post-body-paragraph md me iq mf b mg nf jr mi mj ng ju ml lt nh mn mo lw ni mq mr lz nj mt mu mv ij bi translated"><strong class="mf ir">图7 </strong>说明了不同的单词嵌入模式对模型性能的影响。正如所料，使用预先训练的Word2Vec的静态单词嵌入总是表现得更好。与随机模式相比，静态模式可以帮助任何模型更准确地预测类别，平均准确率提高3%。</p><p id="3621" class="pw-post-body-paragraph md me iq mf b mg nf jr mi mj ng ju ml lt nh mn mo lw ni mq mr lz nj mt mu mv ij bi translated">动态向量表示模型将微调由Word2Vec向量初始化的参数，以学习每个任务的有意义的上下文。理想情况下，它会比静态方法产生更好的性能。然而，情况并非总是如此。虽然模型还可以改进，但变化不大。在某些情况下，一个模型甚至可以有较低的准确性。</p><p id="c782" class="pw-post-body-paragraph md me iq mf b mg nf jr mi mj ng ju ml lt nh mn mo lw ni mq mr lz nj mt mu mv ij bi translated">在<strong class="mf ir">图7 </strong>中，动态模式略微降低了TREC和MPQA数据集上的整体模型性能。在<strong class="mf ir">表8 </strong>中，尽管BiGRU-dynamic在SUBJ数据集上比其静态版本提供了更好的性能，但它在其他数据集上的性能有所下降。这是因为向量调整到一个特定的数据集，该数据集可能会过度拟合并改变从Word2Vec派生的原始上下文。</p><h2 id="e844" class="lo kx iq bd ky lp lq dn lc lr ls dp lg lt lu lv li lw lx ly lk lz ma mb lm mc bi translated">c . 2.3 TCN与RNN模式</h2><p id="5f2d" class="pw-post-body-paragraph md me iq mf b mg mh jr mi mj mk ju ml lt mm mn mo lw mp mq mr lz ms mt mu mv ij bi translated">如果我们使用单词嵌入，TCN比RNN的模型如LSTM或GRU更有效。在五个数据集的四个中，TCN以优异的精确度超过了所有RNN架构。在另一个数据集上，TCN精度仍然很高，接近最高。TCN-静态和-动态作为顶级模型，其次是BiLSTM-静态、big ru-静态和堆叠big ru-静态。</p><p id="b229" class="pw-post-body-paragraph md me iq mf b mg nf jr mi mj ng ju ml lt nh mn mo lw ni mq mr lz nj mt mu mv ij bi translated">简而言之，TCN是最好的模型，不仅与RNN家族相比，而且在捕捉信息以做出稳定预测方面也与其他模型相比。在这个实验中，唯一能挑战TCN的模型是基于系综的模型。</p><h2 id="491e" class="lo kx iq bd ky lp lq dn lc lr ls dp lg lt lu lv li lw lx ly lk lz ma mb lm mc bi translated">C.2.4整体与单一模型</h2><p id="d5a5" class="pw-post-body-paragraph md me iq mf b mg mh jr mi mj mk ju ml lt mm mn mo lw mp mq mr lz ms mt mu mv ij bi translated">正如所料，集成模型通常在几乎所有的分类任务中都优于单个模型。集合模型的静态版本在5个数据集的3个中提供了更好的性能。集成学习的关键是候选模型需要被证明在给定的任务中工作良好。在这种情况下，1D CNN和BiRNN是结合用于文本分类的很好的模型。这一结果鼓励我们在未来尝试将一个有效的模型(如TCN)与其他现有的良好深度学习模型相结合。</p><h2 id="a0a2" class="lo kx iq bd ky lp lq dn lc lr ls dp lg lt lu lv li lw lx ly lk lz ma mb lm mc bi translated">C.2.5 .表现最佳的模型</h2><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oi"><img src="../Images/34373ba9d40fb033ef232451424a881d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*s2I8Cp7lkx9mCn4omgH49w.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><strong class="bd nz">表9。</strong>本论文排名前六的深度学习模型。</p></figure><p id="5fa0" class="pw-post-body-paragraph md me iq mf b mg nf jr mi mj ng ju ml lt nh mn mo lw ni mq mr lz nj mt mu mv ij bi translated">最后，<strong class="mf ir">表9 </strong>总结了这一系列实验中的最佳模型。我们使用<strong class="mf ir">图5 </strong>中的平均准确度裕度和<strong class="mf ir">图6 </strong>中的平均等级值来比较<strong class="mf ir">前六个用于文本分类的执行模型</strong>。我们可以看到，静态版本的TCN和集合模型脱颖而出，成为最好的。</p><p id="af1e" class="pw-post-body-paragraph md me iq mf b mg nf jr mi mj ng ju ml lt nh mn mo lw ni mq mr lz nj mt mu mv ij bi translated">接下来，TCN动力紧随其后，成为最佳车型，跻身前三名。最终，TCN和基于集成的模型在执行文本分类任务时优于其他配置，使它们成为未来应用和研究的最佳推荐架构。</p><h1 id="6d44" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">D.结论和未来工作</h1><h2 id="b798" class="lo kx iq bd ky lp lq dn lc lr ls dp lg lt lu lv li lw lx ly lk lz ma mb lm mc bi translated">D.1 .结论</h2><p id="f691" class="pw-post-body-paragraph md me iq mf b mg mh jr mi mj mk ju ml lt mm mn mo lw mp mq mr lz ms mt mu mv ij bi translated">该项目展示了一个全面的实验，重点是在五个文本分类数据集上使用两种不同的特征提取来构建深度学习模型。总之，以下是基本的见解:</p><ul class=""><li id="8f6a" class="nk nl iq mf b mg nf mj ng lt nm lw nn lz no mv np nq nr ns bi translated">任何建立在单词嵌入之上的模型都会使模型表现得非常好。</li><li id="0838" class="nk nl iq mf b mg nt mj nu lt nv lw nw lz nx mv np nq nr ns bi translated">使用预先训练的单词嵌入，例如Word2Vec，可以在很大程度上提高模型精度。</li><li id="6e8d" class="nk nl iq mf b mg nt mj nu lt nv lw nw lz nx mv np nq nr ns bi translated">TCN是递归架构的一个很好的替代方案，已经被证明在文本数据分类中是有效的。</li><li id="e648" class="nk nl iq mf b mg nt mj nu lt nv lw nw lz nx mv np nq nr ns bi translated">与单独训练的单一模型相比，基于集成学习的模型可以帮助做出更好的预测。</li><li id="6b74" class="nk nl iq mf b mg nt mj nu lt nv lw nw lz nx mv np nq nr ns bi translated">TCN和集合CNN-GRU模型是我们在这一系列文本分类任务中获得的性能最好的算法。</li></ul><h2 id="9fb8" class="lo kx iq bd ky lp lq dn lc lr ls dp lg lt lu lv li lw lx ly lk lz ma mb lm mc bi translated">D.2 .对未来工作的建议</h2><p id="6db2" class="pw-post-body-paragraph md me iq mf b mg mh jr mi mj mk ju ml lt mm mn mo lw mp mq mr lz ms mt mu mv ij bi translated">我们对未来的实验提出如下建议:</p><ul class=""><li id="9226" class="nk nl iq mf b mg nf mj ng lt nm lw nn lz no mv np nq nr ns bi translated"><strong class="mf ir">基于集合的TCN模型。</strong>在基于集成的学习中，使用TCN结合其他良好的模型(如1D CNN和BiGRU)来执行文本分类任务，以查看它是否能够挑战基准</li><li id="11a3" class="nk nl iq mf b mg nt mj nu lt nv lw nw lz nx mv np nq nr ns bi translated"><strong class="mf ir">内核大小和过滤器。</strong>通过在CNN或TCN中使用更多或更少的过滤器将内核大小扩展到1到10之间来探索这两个超参数，以查看它如何影响模型性能。</li><li id="e1d9" class="nk nl iq mf b mg nt mj nu lt nv lw nw lz nx mv np nq nr ns bi translated"><strong class="mf ir">更深的人脉。任何具有更多隐藏层的神经网络通常会在任何任务中做得更好。探索CNN、RNN和TCN的更深版本，看看它如何影响现有的性能。</strong></li><li id="c6eb" class="nk nl iq mf b mg nt mj nu lt nv lw nw lz nx mv np nq nr ns bi translated">使用手套和快速文本。探索其他预训练的单词嵌入选项，如具有静态和动态模式的GloVe和FastText，并将结果与Word2Vec进行比较。</li></ul><p id="59de" class="pw-post-body-paragraph md me iq mf b mg nf jr mi mj ng ju ml lt nh mn mo lw ni mq mr lz nj mt mu mv ij bi translated">谢谢你，</p><p id="1edf" class="pw-post-body-paragraph md me iq mf b mg nf jr mi mj ng ju ml lt nh mn mo lw ni mq mr lz nj mt mu mv ij bi translated">diardano Raihan<br/><a class="ae kv" href="https://www.linkedin.com/in/diardanoraihan/" rel="noopener ugc nofollow" target="_blank">LinkedIn简介</a></p><blockquote class="on oo op"><p id="29ea" class="md me oa mf b mg nf jr mi mj ng ju ml oq nh mn mo or ni mq mr os nj mt mu mv ij bi translated"><strong class="mf ir"> <em class="iq">注</em> </strong> : <em class="iq">你看到的一切都记载在</em> <a class="ae kv" href="https://github.com/diardanoraihan/Text_Classification_Capstone/" rel="noopener ugc nofollow" target="_blank"> <strong class="mf ir"> <em class="iq">我的GitHub资源库</em> </strong> </a> <em class="iq">。对于那些对完整代码感兴趣的人，请访问👍。</em></p></blockquote></div><div class="ab cl ot ou hu ov" role="separator"><span class="ow bw bk ox oy oz"/><span class="ow bw bk ox oy oz"/><span class="ow bw bk ox oy"/></div><div class="ij ik il im in"><h1 id="bef8" class="kw kx iq bd ky kz pa lb lc ld pb lf lg jw pc jx li jz pd ka lk kc pe kd lm ln bi translated">参考</h1><ul class=""><li id="88fa" class="nk nl iq mf b mg mh mj mk lt pf lw pg lz ph mv np nq nr ns bi translated">[1] B. Pang，L. Lee，“看见星星:利用阶级关系进行情绪分类的等级量表”，美国癌症学会2005年会议录<em class="oa">。</em></li><li id="e5cb" class="nk nl iq mf b mg nt mj nu lt nv lw nw lz nx mv np nq nr ns bi translated">[2] B. Pang，L. Lee，“情感教育:使用基于最小割的主观性摘要进行情感分析”，<em class="oa">计算语言学协会第42届会议论文集(ACL'04) </em>，2004年。</li><li id="3486" class="nk nl iq mf b mg nt mj nu lt nv lw nw lz nx mv np nq nr ns bi translated">[3] X .李，d .罗思，“学习疑问量词”，载于《中国林业学报》2002年第02期第19期。</li><li id="b047" class="nk nl iq mf b mg nt mj nu lt nv lw nw lz nx mv np nq nr ns bi translated">[4] M .胡，b .刘，“客户评论的挖掘与汇总”，<em class="oa">《学报》2004年04期</em>。</li><li id="1f4e" class="nk nl iq mf b mg nt mj nu lt nv lw nw lz nx mv np nq nr ns bi translated">[5] J. Wiebe，T. Wilson，C. Cardie，“用语言注释观点和情感的表达”，<em class="oa">语言资源和评估</em>，39(2):165–210，2005。</li><li id="99bd" class="nk nl iq mf b mg nt mj nu lt nv lw nw lz nx mv np nq nr ns bi translated">[6] S. bai，J. Kolter，V. Koltun，“序列建模通用卷积和递归网络的实证评估”，<em class="oa"> arXiv </em>，2018年4月。</li><li id="3e2d" class="nk nl iq mf b mg nt mj nu lt nv lw nw lz nx mv np nq nr ns bi translated">[7] P. Rémy，《凯拉斯·TCN》，<em class="oa">GitHub</em><a class="ae kv" href="https://github.com/philipperemy/keras-tcn" rel="noopener ugc nofollow" target="_blank">https://github.com/philipperemy/keras-tcn</a>，1月。2021.</li><li id="a0aa" class="nk nl iq mf b mg nt mj nu lt nv lw nw lz nx mv np nq nr ns bi translated">[8] C. Henkel，《时态卷积网络》，<em class="oa"> Kaggle </em>，<a class="ae kv" href="https://www.kaggle.com/christofhenkel/temporal-convolutional-network," rel="noopener ugc nofollow" target="_blank">https://www . ka ggle . com/christofhenkel/时态卷积网络，</a>2021年2月。</li><li id="1dfc" class="nk nl iq mf b mg nt mj nu lt nv lw nw lz nx mv np nq nr ns bi translated">[9] K. Kowsari，M. Heidarysafa，D. E. Brown，K. J. Meimandi，L. E. Barnes，“用于分类的随机多模型深度学习”，<em class="oa"> arXiv </em>，2018年4月。</li><li id="b8f0" class="nk nl iq mf b mg nt mj nu lt nv lw nw lz nx mv np nq nr ns bi translated">[10] Y. Kim，“用于句子分类的卷积神经网络”，<em class="oa">计算语言学协会</em>，2014年10月。</li><li id="9fc4" class="nk nl iq mf b mg nt mj nu lt nv lw nw lz nx mv np nq nr ns bi translated">[11] S. Brahma，《使用后缀双向LSTM的改进句子建模》，<em class="oa"> arXiv </em>，2018年9月。</li><li id="c487" class="nk nl iq mf b mg nt mj nu lt nv lw nw lz nx mv np nq nr ns bi translated">[12] D. Cer，Y. Yang，S. Kong，N. Hua，N. Limtiaco，R. S. John，N. Constant，M. Guajardo-Cespedes，S. Yuan，C. Tar，Y. Sung，B. Strope，R. Kurzweil，《通用句子编码器》，<em class="oa"> arXiv </em>，2018年4月。</li></ul></div></div>    
</body>
</html>