<html>
<head>
<title>Understanding Random Forest’s hyperparameters with images</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用图像理解随机森林的超参数</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/understanding-random-forests-hyperparameters-with-images-9b53fce32cb3?source=collection_archive---------20-----------------------#2021-10-04">https://towardsdatascience.com/understanding-random-forests-hyperparameters-with-images-9b53fce32cb3?source=collection_archive---------20-----------------------#2021-10-04</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="696c" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">理解随机森林ML模型中超参数的影响</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/d4fb5bffb98927570e7ee5c7bfaaa87e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-mGADAcbLmaQT-YK5N5LCQ.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">由<a class="ae ky" href="https://unsplash.com/@gerandeklerk?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Geran de Klerk </a>在<a class="ae ky" href="https://unsplash.com/s/photos/forest?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>拍摄的照片</p></figure></div><div class="ab cl kz la hx lb" role="separator"><span class="lc bw bk ld le lf"/><span class="lc bw bk ld le lf"/><span class="lc bw bk ld le"/></div><div class="im in io ip iq"><h2 id="8779" class="lg lh it bd li lj lk dn ll lm ln dp lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated">关于随机森林</h2><p id="ef28" class="pw-post-body-paragraph mc md it me b mf mg ju mh mi mj jx mk lp ml mm mn lt mo mp mq lx mr ms mt mu im bi translated">决策树是一种解决问题的分布式算法。它试图通过将决策的每一步二进制化来模拟人类的思维过程。所以，在每一步，算法都在真或假之间选择前进。</p><p id="76ef" class="pw-post-body-paragraph mc md it me b mf mv ju mh mi mw jx mk lp mx mm mn lt my mp mq lx mz ms mt mu im bi translated">该算法简单，但非常强大，因此广泛应用于机器学习模型中。然而，决策树的一个问题是它很难概括一个问题。该算法学习如何决定一个给定的数据集，以至于当我们想用它来处理新数据时，它无法给我们最好的答案。</p><p id="67b2" class="pw-post-body-paragraph mc md it me b mf mv ju mh mi mw jx mk lp mx mm mn lt my mp mq lx mz ms mt mu im bi translated">为了解决这个问题，创建了一种新型的决策树算法，通过收集在同一数据集的各种变化上训练的许多树，并使用投票或平均系统来组合它们，并为每个数据点确定最佳结果。这就是随机森林的概念。</p><blockquote class="na"><p id="0848" class="nb nc it bd nd ne nf ng nh ni nj mu dk translated">随机森林是由树结构分类器(…)独立同分布随机向量的集合组成的分类器，并且每棵树为输入x处最流行的类别投一个单位票。</p></blockquote><h2 id="0fb9" class="lg lh it bd li lj nk dn ll lm nl dp lo lp nm lr ls lt nn lv lw lx no lz ma mb bi translated">创建简单的模型</h2><p id="37f9" class="pw-post-body-paragraph mc md it me b mf mg ju mh mi mj jx mk lp ml mm mn lt mo mp mq lx mr ms mt mu im bi translated">创建模型相当简单。正如你们中的许多人可能知道的，实际的模型实例、拟合和预测只需要几行代码就可以完成。然而，困难的部分通常是准备数据和调整模型。</p><p id="1e93" class="pw-post-body-paragraph mc md it me b mf mv ju mh mi mw jx mk lp mx mm mn lt my mp mq lx mz ms mt mu im bi translated">要调整模型，我们必须将超参数从默认值更改为能给我们带来最佳结果的值。我们的目标是更好地理解随机森林中每个超参数的作用，以便在需要时更好地修改它们。</p><p id="953e" class="pw-post-body-paragraph mc md it me b mf mv ju mh mi mw jx mk lp mx mm mn lt my mp mq lx mz ms mt mu im bi translated">以下是我将在本例中使用的导入和数据集:来自<code class="fe np nq nr ns b">sklearn</code>的wines数据集。</p><pre class="kj kk kl km gt nt ns nu nv aw nw bi"><span id="1068" class="lg lh it ns b gy nx ny l nz oa"># Data<br/>import pandas as pd<br/>from sklearn.datasets import load_wine</span><span id="1c1e" class="lg lh it ns b gy ob ny l nz oa"># Data split<br/>from sklearn.model_selection import train_test_split</span><span id="ca91" class="lg lh it ns b gy ob ny l nz oa"># Model<br/>from sklearn.ensemble import RandomForestClassifier</span><span id="3fc9" class="lg lh it ns b gy ob ny l nz oa"># Visualize Tree<br/>from sklearn.tree import export_graphviz</span><span id="1f95" class="lg lh it ns b gy ob ny l nz oa"># Load dataset<br/>df = load_wine()</span><span id="1e94" class="lg lh it ns b gy ob ny l nz oa"># variables<br/>X = pd.DataFrame(df.data, columns=df.feature_names)<br/># target<br/>y = df.target</span><span id="0f2c" class="lg lh it ns b gy ob ny l nz oa"># Split train and test<br/>X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=12)</span></pre><p id="9b21" class="pw-post-body-paragraph mc md it me b mf mv ju mh mi mw jx mk lp mx mm mn lt my mp mq lx mz ms mt mu im bi translated">以下是具有默认值的模型:</p><pre class="kj kk kl km gt nt ns nu nv aw nw bi"><span id="b408" class="lg lh it ns b gy nx ny l nz oa"># Instantiate class. Using random_state=2 for you to be able to reproduce the same result</span><span id="1e36" class="lg lh it ns b gy ob ny l nz oa">rf = RandomForestClassifier(<em class="oc">n_estimators=100</em>, <em class="oc">criterion='gini'</em>, <em class="oc">max_depth=None</em>, <em class="oc">min_samples_split=2</em>, <em class="oc">min_samples_leaf=1</em>, <em class="oc">min_weight_fraction_leaf=0.0</em>, <em class="oc">max_features='auto'</em>, <em class="oc">max_leaf_nodes=None</em>, <em class="oc">min_impurity_decrease=0.0</em>, <em class="oc">bootstrap=True</em>, <em class="oc">oob_score=False</em>, <em class="oc">n_jobs=None</em>, <strong class="ns iu"><em class="oc">random_state=2</em></strong>, <em class="oc">verbose=0</em>, <em class="oc">warm_start=False</em>, <em class="oc">class_weight=None</em>, <em class="oc">ccp_alpha=0.0</em>, <em class="oc">max_samples=None</em>)</span></pre><p id="2476" class="pw-post-body-paragraph mc md it me b mf mv ju mh mi mw jx mk lp mx mm mn lt my mp mq lx mz ms mt mu im bi translated">我们来训练一下，生成一张图。</p><pre class="kj kk kl km gt nt ns nu nv aw nw bi"><span id="2117" class="lg lh it ns b gy nx ny l nz oa"># Fit the model<br/>rf.fit(X_train,y_train)</span><span id="0687" class="lg lh it ns b gy ob ny l nz oa"># Extract one of the trees from the model<br/>tree = rf.estimators_[99]</span><span id="3e91" class="lg lh it ns b gy ob ny l nz oa"># Export as dot file<br/>export_graphviz(tree, out_file='tree.dot',<br/>feature_names = df.feature_names, class_names = df.target_names, rounded = True, proportion = False, precision = 2, filled = True)</span><span id="691a" class="lg lh it ns b gy ob ny l nz oa"># Convert to png using system command (requires Graphviz)<br/>from subprocess import call<br/>call(['dot', '-Tpng', 'tree.dot', '-o', 'tree.png', '-Gdpi=90'])</span><span id="b925" class="lg lh it ns b gy ob ny l nz oa"># Display in jupyter notebook<br/>from IPython.display import Image<br/>Image(filename = 'tree.png')</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi od"><img src="../Images/d1c7571bafde26ea4330ca1088733a36.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pv4j5D8he4eIjg6WI3vBIg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图1:具有所有默认值的随机森林模型。图片由作者提供。</p></figure><p id="2a14" class="pw-post-body-paragraph mc md it me b mf mv ju mh mi mw jx mk lp mx mm mn lt my mp mq lx mz ms mt mu im bi translated">好的，很好。现在，在继续之前，让我们排列我们正在使用的名称。</p><ul class=""><li id="24d2" class="oe of it me b mf mv mi mw lp og lt oh lx oi mu oj ok ol om bi translated"><strong class="me iu">节点</strong>是我们有分裂的时候。</li><li id="281a" class="oe of it me b mf on mi oo lp op lt oq lx or mu oj ok ol om bi translated"><strong class="me iu">分支</strong>是一个决策路径【例如酒精=真&gt;色调=真&gt;结束(叶子)】</li><li id="8879" class="oe of it me b mf on mi oo lp op lt oq lx or mu oj ok ol om bi translated"><strong class="me iu">叶子</strong>是树枝的最后一格。</li></ul><p id="f515" class="pw-post-body-paragraph mc md it me b mf mv ju mh mi mw jx mk lp mx mm mn lt my mp mq lx mz ms mt mu im bi translated">也就是说，在图1中注意到的第一个有趣的事情是，当“基尼”指标值为0.0时，树枝才到达叶子。该指标是一个衡量分割质量的函数。<code class="fe np nq nr ns b">sklearn</code>支持信息增益的“基尼”或“熵”。当数字达到0时，我们不能获得更多的信息，因为叶子现在是纯的。当你查看<em class="oc">值</em>【类0数量，类1数量，类2数量】时，一个纯叶节点被确认。如果预测的<em class="oc">类</em>中的数是唯一大于0的数，则该叶是纯的<em class="oc">(例如[0，1，0]和class_1 =纯节点)</em>。</p><p id="567b" class="pw-post-body-paragraph mc md it me b mf mv ju mh mi mw jx mk lp mx mm mn lt my mp mq lx mz ms mt mu im bi translated">我们的目标是更好地理解改变超参数如何改变叶子、节点和分支的数量。因此，我将一次改变一个主要的超参数，并绘制结果。</p><h2 id="e6ff" class="lg lh it bd li lj lk dn ll lm ln dp lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated">最大功能</h2><p id="9454" class="pw-post-body-paragraph mc md it me b mf mg ju mh mi mj jx mk lp ml mm mn lt mo mp mq lx mr ms mt mu im bi translated">寻找最佳分割时要考虑的特征数量。默认值为“auto”，使用“sqrt”，但它有其他选项，如“log2”或一个有趣的可能性，即输入一个介于0和1之间的浮点数，这将是每次分割时使用的功能的百分比。如果你有10个特性，使用<code class="fe np nq nr ns b">max_feature=0.2</code>，它会考虑20%的特性，也就是2。</p><p id="4a19" class="pw-post-body-paragraph mc md it me b mf mv ju mh mi mw jx mk lp mx mm mn lt my mp mq lx mz ms mt mu im bi translated">通常不是所有的特征都那么重要，所以这是一个很好的在<code class="fe np nq nr ns b">GridSearchCV</code>中测试的超参数，你可以试着从0.3，0.4这样的值开始。这里的数字越小，方差越小，但偏差越大。对于较高的数字，您有更多的机会将最佳特征用于分割，因此您将减少偏差，但增加方差。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi os"><img src="../Images/c71162a011577e9e313028b77b87bad3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ixt33SZd0EoXY0eNTcSuNg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图二:max_features对比。作者图片</p></figure><h2 id="3e1a" class="lg lh it bd li lj lk dn ll lm ln dp lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated">最大深度</h2><p id="9959" class="pw-post-body-paragraph mc md it me b mf mg ju mh mi mj jx mk lp ml mm mn lt mo mp mq lx mr ms mt mu im bi translated">这个超参数将限制树可以向下生长的最大分裂数量。</p><pre class="kj kk kl km gt nt ns nu nv aw nw bi"><span id="f221" class="lg lh it ns b gy nx ny l nz oa"># Instantiate class<br/>rf = RandomForestClassifier(max_depth = 2)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ot"><img src="../Images/3b3cd32bf5334fea4f8158786a545bb8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1244/format:webp/1*6EKFDlzURVRfcRkFJPQ-nQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图3:最大深度=2的RF模型</p></figure><p id="520f" class="pw-post-body-paragraph mc md it me b mf mv ju mh mi mw jx mk lp mx mm mn lt my mp mq lx mz ms mt mu im bi translated">因此，正如我们选择了<code class="fe np nq nr ns b">max_depth=2</code>，这意味着它只能分裂两次，使第三行的结果方块成为树叶。请注意，<em class="oc">基尼系数</em>指标仅适用于一个方块。事实上，它对结果没有太大的影响，因为在这个模型中有100个不同的树(估计量)。即使深度限制为1，它仍然预测了三个类。它必须与其他超参数一起使用。</p><h2 id="c9e6" class="lg lh it bd li lj lk dn ll lm ln dp lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated"><strong class="ak">分割的最小样本数</strong></h2><p id="c175" class="pw-post-body-paragraph mc md it me b mf mg ju mh mi mj jx mk lp ml mm mn lt mo mp mq lx mr ms mt mu im bi translated">给定节点中能够拆分的最小样本数。</p><p id="2720" class="pw-post-body-paragraph mc md it me b mf mv ju mh mi mw jx mk lp mx mm mn lt my mp mq lx mz ms mt mu im bi translated">再看图3。看到左侧有42个样本。让我们把<code class="fe np nq nr ns b">min_samples_split</code> <strong class="me iu"> </strong>设为50，看看会发生什么。</p><pre class="kj kk kl km gt nt ns nu nv aw nw bi"><span id="bedd" class="lg lh it ns b gy nx ny l nz oa">rf = RandomForestClassifier(min_samples_split=50, random_state=2)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ou"><img src="../Images/fd3832c8d2c223d6b2b9c193e2aafc21.png" data-original-src="https://miro.medium.com/v2/resize:fit:806/format:webp/1*somipLwWTAgWzWCEhMZerg.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><p id="c6c3" class="pw-post-body-paragraph mc md it me b mf mv ju mh mi mw jx mk lp mx mm mn lt my mp mq lx mz ms mt mu im bi translated">不出所料，左边的树枝没有生长。因此，这是修剪树并迫使它在达到节点纯度之前给出分类的另一种方式。</p><h2 id="4533" class="lg lh it bd li lj lk dn ll lm ln dp lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated">最大叶节点数</h2><p id="463f" class="pw-post-body-paragraph mc md it me b mf mg ju mh mi mj jx mk lp ml mm mn lt mo mp mq lx mr ms mt mu im bi translated">它决定了你的树的最大叶子数。</p><pre class="kj kk kl km gt nt ns nu nv aw nw bi"><span id="40b5" class="lg lh it ns b gy nx ny l nz oa">rf = RandomForestClassifier(max_leaf_nodes=3, random_state=2)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ov"><img src="../Images/73aeb3ec6005d619fe406415e8622e86.png" data-original-src="https://miro.medium.com/v2/resize:fit:980/format:webp/1*7ZkddXRE8PfOFNbWweQSTg.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图4: max_leaf_nodes=3。图片由作者提供。</p></figure><p id="52b5" class="pw-post-body-paragraph mc md it me b mf mv ju mh mi mw jx mk lp mx mm mn lt my mp mq lx mz ms mt mu im bi translated">这里，我们看到分类器为每个可预测的类(类0、1或2)创建了一个叶子。</p><h2 id="7257" class="lg lh it bd li lj lk dn ll lm ln dp lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated">每片叶子的最小样本数</h2><p id="81c5" class="pw-post-body-paragraph mc md it me b mf mg ju mh mi mj jx mk lp ml mm mn lt mo mp mq lx mr ms mt mu im bi translated">叶子需要的样本数。这意味着，如果叶片上的样本数在另一次分割后低于该数量，则不会进行处理。</p><pre class="kj kk kl km gt nt ns nu nv aw nw bi"><span id="cbbe" class="lg lh it ns b gy nx ny l nz oa">rf = RandomForestClassifier(min_samples_leaf=20, random_state=2)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ow"><img src="../Images/41e65b189126167202beac9f8e272b8f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1316/format:webp/1*1upulTe7h1KZgY_VnnCcOw.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图5: main_samples_leaf=20。图片由作者提供。</p></figure><p id="be73" class="pw-post-body-paragraph mc md it me b mf mv ju mh mi mw jx mk lp mx mm mn lt my mp mq lx mz ms mt mu im bi translated">在图5中可以看到，每片叶子中的样本数高于20。当我们的模型过度拟合时，我们可以尝试调整这个超参数，无论是否与<code class="fe np nq nr ns b">max_depth</code>结合，并强制做出更早的决定，这可能有助于推广预测。</p><h2 id="0e17" class="lg lh it bd li lj lk dn ll lm ln dp lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated">复杂性成本修剪</h2><p id="73cf" class="pw-post-body-paragraph mc md it me b mf mg ju mh mi mj jx mk lp ml mm mn lt mo mp mq lx mr ms mt mu im bi translated">另一种修剪树的方法是使用<code class="fe np nq nr ns b">ccp_alpha</code>超参数，它是复杂性成本参数。该算法将通过计算复杂度成本在树之间进行选择，具有较小值的数量被认为较弱，因此它们被修剪。一旦复杂性成本的最小值高于<code class="fe np nq nr ns b">ccp_alpha</code>，修剪就停止。<code class="fe np nq nr ns b">ccp_alpha</code>的值越大，修剪的节点数越多。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ox"><img src="../Images/8cf94d0750a8a4d54c0904d2ea200b88.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*85dzE3VA8GqsZqR79AahIw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图6:CCP阿尔法数的差异。图片由作者提供。</p></figure><h2 id="7bcc" class="lg lh it bd li lj lk dn ll lm ln dp lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated">在你走之前</h2><p id="250d" class="pw-post-body-paragraph mc md it me b mf mg ju mh mi mj jx mk lp ml mm mn lt mo mp mq lx mr ms mt mu im bi translated">本材料的目的是让您直观地了解改变随机森林模型中的每个超参数将如何影响您的结果。</p><ul class=""><li id="618d" class="oe of it me b mf mv mi mw lp og lt oh lx oi mu oj ok ol om bi translated"><code class="fe np nq nr ns b">n_estimators</code>:估计数。你拥有的越多，你应该有一个更准确的结果，但它在计算能力方面更昂贵。</li><li id="904f" class="oe of it me b mf on mi oo lp op lt oq lx or mu oj ok ol om bi translated"><code class="fe np nq nr ns b">criterion</code> <strong class="me iu"> : </strong>在<em class="oc">基尼</em>或<em class="oc">熵</em>之间选择。两者将寻求相同的结果，即节点纯度。</li><li id="5496" class="oe of it me b mf on mi oo lp op lt oq lx or mu oj ok ol om bi translated">树越大，越有可能过度拟合。RF模型通常会尽量减少这种情况，但如果您的模型过度拟合，这个超参数可能会是一个有趣的玩法。</li><li id="61f7" class="oe of it me b mf on mi oo lp op lt oq lx or mu oj ok ol om bi translated">这个和上面那个一起工作。这是分裂到另一个分支所需的最小样本。</li><li id="2ff1" class="oe of it me b mf on mi oo lp op lt oq lx or mu oj ok ol om bi translated"><code class="fe np nq nr ns b">max_leaf_nodes</code>:可以强制树少叶子。</li><li id="9097" class="oe of it me b mf on mi oo lp op lt oq lx or mu oj ok ol om bi translated"><code class="fe np nq nr ns b">ccp_alpha</code>:另一种修剪树的方法，基于复杂性成本的计算。</li><li id="f48d" class="oe of it me b mf on mi oo lp op lt oq lx or mu oj ok ol om bi translated">参见GitHub中的<a class="ae ky" href="https://github.com/gurezende/Studying/tree/master/Python/RandomForest" rel="noopener ugc nofollow" target="_blank">代码，这里是</a>。</li></ul><h2 id="6df7" class="lg lh it bd li lj lk dn ll lm ln dp lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated">跟我来</h2><p id="ea6a" class="pw-post-body-paragraph mc md it me b mf mg ju mh mi mj jx mk lp ml mm mn lt mo mp mq lx mr ms mt mu im bi translated">如果你对这些内容感兴趣，请点击那个按钮告诉我应该写些什么，并关注我。</p><div class="oy oz gp gr pa pb"><a href="https://medium.com/gustavorsantos" rel="noopener follow" target="_blank"><div class="pc ab fo"><div class="pd ab pe cl cj pf"><h2 class="bd iu gy z fp pg fr fs ph fu fw is bi translated">古斯塔夫·桑托斯</h2><div class="pi l"><h3 class="bd b gy z fp pg fr fs ph fu fw dk translated">让我们做出更好的决定。数据驱动的决策。我用Python，R，Excel，SQL创建数据科学项目。</h3></div><div class="pj l"><p class="bd b dl z fp pg fr fs ph fu fw dk translated">medium.com</p></div></div><div class="pk l"><div class="pl l pm pn po pk pp ks pb"/></div></div></a></div><h2 id="31a0" class="lg lh it bd li lj lk dn ll lm ln dp lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated">参考</h2><p id="0fb6" class="pw-post-body-paragraph mc md it me b mf mg ju mh mi mj jx mk lp ml mm mn lt mo mp mq lx mr ms mt mu im bi translated"><a class="ae ky" href="https://www.stat.berkeley.edu/~breiman/randomforest2001.pdf" rel="noopener ugc nofollow" target="_blank">https://www.stat.berkeley.edu/~breiman/randomforest2001.pdf</a></p><div class="oy oz gp gr pa pb"><a href="https://scikit-learn.org/stable/modules/tree.html#minimal-cost-complexity-pruning" rel="noopener  ugc nofollow" target="_blank"><div class="pc ab fo"><div class="pd ab pe cl cj pf"><h2 class="bd iu gy z fp pg fr fs ph fu fw is bi translated">1.10.决策树</h2><div class="pi l"><h3 class="bd b gy z fp pg fr fs ph fu fw dk translated">决策树(DTs)是一种用于分类和回归的非参数监督学习方法。目标是…</h3></div><div class="pj l"><p class="bd b dl z fp pg fr fs ph fu fw dk translated">scikit-learn.org</p></div></div><div class="pk l"><div class="pq l pm pn po pk pp ks pb"/></div></div></a></div><p id="7ff6" class="pw-post-body-paragraph mc md it me b mf mv ju mh mi mw jx mk lp mx mm mn lt my mp mq lx mz ms mt mu im bi translated"><a class="ae ky" href="https://scikit-learn.org/stable/modules/tree.html#minimal-cost-complexity-pruning" rel="noopener ugc nofollow" target="_blank">https://sci kit-learn . org/stable/modules/tree . html #最小成本复杂性修剪</a></p><p id="6b84" class="pw-post-body-paragraph mc md it me b mf mv ju mh mi mw jx mk lp mx mm mn lt my mp mq lx mz ms mt mu im bi translated"><a class="ae ky" href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html?highlight=random%20forest%20classifier#sklearn.ensemble.RandomForestClassifier" rel="noopener ugc nofollow" target="_blank">随机森林文档</a></p><div class="oy oz gp gr pa pb"><a rel="noopener follow" target="_blank" href="/random-forest-hyperparameters-and-how-to-fine-tune-them-17aee785ee0d"><div class="pc ab fo"><div class="pd ab pe cl cj pf"><h2 class="bd iu gy z fp pg fr fs ph fu fw is bi translated">随机森林:超参数以及如何微调它们</h2><div class="pi l"><h3 class="bd b gy z fp pg fr fs ph fu fw dk translated">如何优化最常用的机器学习模型之一</h3></div><div class="pj l"><p class="bd b dl z fp pg fr fs ph fu fw dk translated">towardsdatascience.com</p></div></div><div class="pk l"><div class="pr l pm pn po pk pp ks pb"/></div></div></a></div></div></div>    
</body>
</html>