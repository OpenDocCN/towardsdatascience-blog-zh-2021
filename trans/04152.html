<html>
<head>
<title>Neural Network Optimizers Made Simple: Core algorithms and why they are needed</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">神经网络优化器变得简单:核心算法和为什么需要它们</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/neural-network-optimizers-made-simple-core-algorithms-and-why-they-are-needed-7fd072cd2788?source=collection_archive---------4-----------------------#2021-04-08">https://towardsdatascience.com/neural-network-optimizers-made-simple-core-algorithms-and-why-they-are-needed-7fd072cd2788?source=collection_archive---------4-----------------------#2021-04-08</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="0414" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsdatascience.com/tagged/hands-on-tutorials" rel="noopener" target="_blank">动手教程</a>，直观的深度学习</h2><div class=""/><div class=""><h2 id="5965" class="pw-subtitle-paragraph jw iz iq bd b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn dk translated">一个温和的指导，用简单的英语介绍梯度下降优化器使用的基本技术，如SGD，Momentum，RMSProp，Adam和其他优化器</h2></div><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ko"><img src="../Images/18c65d7ad0206ca093ffadc0c16e3d1b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*m2ujn5zPfzBEjRVc"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">照片由<a class="ae le" href="https://unsplash.com/@stacksvrollin?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">乔治·斯塔克波尔</a>在<a class="ae le" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><p id="abc3" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">优化器是神经网络架构的关键组件。在训练期间，它们在帮助网络学会做出越来越好的预测方面发挥着关键作用。</p><p id="c08c" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">他们通过找到最佳的模型参数集来做到这一点，如权重和偏差，以便模型可以为他们正在解决的问题产生最佳输出。</p><p id="afb5" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">大多数神经网络最常用的优化技术是梯度下降。</p><p id="ea66" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">大多数流行的深度学习库，如Pytorch和Keras，都有大量基于梯度下降的内置优化器，如随机梯度下降(SGD)、Adadelta、Adagrad、RMSProp、Adam等。</p><p id="3127" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">为什么有这么多不同的优化算法？我们如何决定选择哪一个？</p><p id="680a" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">如果你阅读每一个的文档，它们描述了一个如何更新模型参数的公式。每个公式是什么意思，有什么意义？</p><p id="4555" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">在我们准备深入研究数学之前，我这篇文章的目标是提供一些总体背景，并对每种算法如何适用有一些直觉。事实上，我不会在这里讨论公式本身，而是将它留给另一篇文章。</p><p id="33f7" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">如果你对一般的神经网络架构感兴趣，我有一些你可能会喜欢的文章。</p><ol class=""><li id="bb63" class="mb mc iq lh b li lj ll lm lo md ls me lw mf ma mg mh mi mj bi translated"><a class="ae le" rel="noopener" target="_blank" href="/batch-norm-explained-visually-how-it-works-and-why-neural-networks-need-it-b18919692739">批处理规范—其工作原理</a> <em class="mk">(显著稳定和加速模型训练的必要深度学习层)</em></li><li id="8aff" class="mb mc iq lh b li ml ll mm lo mn ls mo lw mp ma mg mh mi mj bi translated"><a class="ae le" rel="noopener" target="_blank" href="/differential-and-adaptive-learning-rates-neural-network-optimizers-and-schedulers-demystified-2edc589fa2c9">差分和自适应学习率</a> ( <em class="mk">优化器和调度器如何用于增强模型训练和调整超参数</em>)</li><li id="4c7d" class="mb mc iq lh b li ml ll mm lo mn ls mo lw mp ma mg mh mi mj bi translated"><a class="ae le" rel="noopener" target="_blank" href="/image-captions-with-deep-learning-state-of-the-art-architectures-3290573712db">图像字幕架构</a> <em class="mk">(具有图像特征编码器、序列解码器和注意力的多模态CNN和RNN架构)</em></li></ol><h1 id="9a0c" class="mq mr iq bd ms mt mu mv mw mx my mz na kf nb kg nc ki nd kj ne kl nf km ng nh bi translated">梯度下降优化综述</h1><h2 id="6473" class="ni mr iq bd ms nj nk dn mw nl nm dp na lo nn no nc ls np nq ne lw nr ns ng iw bi translated">损失曲线</h2><p id="741d" class="pw-post-body-paragraph lf lg iq lh b li nt ka lk ll nu kd ln lo nv lq lr ls nw lu lv lw nx ly lz ma ij bi translated">让我们从梯度下降算法在工作中的典型3D图片开始。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/ebaf69f42d3ec2d885559d3aac48d77a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1286/0*eumF-TG7eLkUhi4-"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">梯度下降损失曲线(<a class="ae le" href="https://upload.wikimedia.org/wikipedia/commons/5/5b/Gradient_descent_method.png" rel="noopener ugc nofollow" target="_blank">来源</a></p></figure><p id="3f96" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">此图显示了具有两个权重参数的网络:</p><ul class=""><li id="8521" class="mb mc iq lh b li lj ll lm lo md ls me lw mf ma nz mh mi mj bi translated">水平面有两个轴，分别代表重量w1和w2。</li><li id="a0f2" class="mb mc iq lh b li ml ll mm lo mn ls mo lw mp ma nz mh mi mj bi translated">纵轴显示了每个权重组合的损失值</li></ul><p id="cb20" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">换句话说，曲线的形状显示了神经网络的“损失情况”。它绘制了不同权重值的损失，而我们保持输入数据集固定。</p><p id="7336" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">蓝线描绘了优化过程中梯度下降算法的轨迹:</p><ul class=""><li id="a9e5" class="mb mc iq lh b li lj ll lm lo md ls me lw mf ma nz mh mi mj bi translated">它首先为两个权重选择一些随机值，然后计算损失值。</li><li id="824a" class="mb mc iq lh b li ml ll mm lo mn ls mo lw mp ma nz mh mi mj bi translated">在每次迭代中，当它更新它的权重值，导致更低的损失(希望如此)时，它沿着曲线移动到更低的点</li><li id="b72b" class="mb mc iq lh b li ml ll mm lo mn ls mo lw mp ma nz mh mi mj bi translated">最后，它到达它的目标，也就是曲线底部损失最低的地方。</li></ul><h2 id="fc21" class="ni mr iq bd ms nj nk dn mw nl nm dp na lo nn no nc ls np nq ne lw nr ns ng iw bi translated">计算梯度</h2><p id="75d0" class="pw-post-body-paragraph lf lg iq lh b li nt ka lk ll nu kd ln lo nv lq lr ls nw lu lv lw nx ly lz ma ij bi translated">该算法基于在该点的损失曲线的梯度和学习率因子来更新权重。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi oa"><img src="../Images/13cb9cc8dffc3070313977bf32ebc2ae.png" data-original-src="https://miro.medium.com/v2/resize:fit:1080/format:webp/0*7V74xgxBiTHRmut6.png"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">梯度下降参数更新(图片由作者提供)</p></figure><p id="6915" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">梯度测量斜率，是垂直方向的变化(<em class="mk">D1</em>)除以水平方向的变化(<em class="mk"> dW </em>)。这意味着陡坡的坡度大，缓坡的坡度小。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi ob"><img src="../Images/4c3ad773001e9c047095afde61ad8084.png" data-original-src="https://miro.medium.com/v2/resize:fit:1112/format:webp/0*TjTulb8miBH_4VdK.png"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">计算渐变(图片由作者提供)</p></figure><h2 id="1090" class="ni mr iq bd ms nj nk dn mw nl nm dp na lo nn no nc ls np nq ne lw nr ns ng iw bi translated">实践中的梯度下降</h2><p id="47d3" class="pw-post-body-paragraph lf lg iq lh b li nt ka lk ll nu kd ln lo nv lq lr ls nw lu lv lw nx ly lz ma ij bi translated">这些损失曲线有助于理解梯度下降的概念。然而，我们应该认识到这是一个理想化的场景，而不是现实的场景:</p><ul class=""><li id="bbce" class="mb mc iq lh b li lj ll lm lo md ls me lw mf ma nz mh mi mj bi translated">上图显示了一条平滑的凸形曲线。现实中，曲线非常坎坷。</li></ul><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/4480d009ca7070e3e3fa64f74c848729.png" data-original-src="https://miro.medium.com/v2/resize:fit:776/format:webp/0*5Ayc0YNqJDoo940C.png"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">神经网络损失图(<a class="ae le" href="https://arxiv.org/pdf/1712.09913.pdf" rel="noopener ugc nofollow" target="_blank">来源</a>，经郝莉许可)</p></figure><ul class=""><li id="7ad1" class="mb mc iq lh b li lj ll lm lo md ls me lw mf ma nz mh mi mj bi translated">其次，我们不会只有两个参数。往往有几千万或者几亿，在你的脑子里是不可能把那个形象化甚至想象出来的。</li></ul><p id="26f3" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">在每一次迭代中，梯度下降的工作原理是“向各个方向寻找它可以下降的最佳坡度”。那么，当最佳斜率(此时)不是最佳方向时，会发生什么呢？</p><ul class=""><li id="31a2" class="mb mc iq lh b li lj ll lm lo md ls me lw mf ma nz mh mi mj bi translated">如果景观向一个方向陡峭倾斜，但最低点在更平缓的斜坡方向，该怎么办？</li><li id="ea1a" class="mb mc iq lh b li ml ll mm lo mn ls mo lw mp ma nz mh mi mj bi translated">或者如果周围的地形相当平坦呢？</li><li id="9a93" class="mb mc iq lh b li ml ll mm lo mn ls mo lw mp ma nz mh mi mj bi translated">或者，如果它掉进了一个深沟，它是如何爬出来的？</li></ul><p id="b82b" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">这些是给它带来困难的曲线的一些例子。接下来让我们看看这些。</p><h1 id="fc02" class="mq mr iq bd ms mt mu mv mw mx my mz na kf nb kg nc ki nd kj ne kl nf km ng nh bi translated">梯度下降优化的挑战</h1><h2 id="a621" class="ni mr iq bd ms nj nk dn mw nl nm dp na lo nn no nc ls np nq ne lw nr ns ng iw bi translated">局部最小值</h2><p id="0b37" class="pw-post-body-paragraph lf lg iq lh b li nt ka lk ll nu kd ln lo nv lq lr ls nw lu lv lw nx ly lz ma ij bi translated">在典型的损失曲线中，除了全局最小值之外，你可能还有许多局部最小值。因为梯度下降被设计成保持向下，一旦下降到局部最小值，它发现很难再爬上斜坡。所以它可能会停留在那里，而没有达到全局最小值。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi od"><img src="../Images/e99ae295c6fbc99c29500bf5f0740a3a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1138/format:webp/1*B5B28RUrgXbxdz-f-PHhRQ.png"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">局部最小值和全局最小值(<a class="ae le" href="https://upload.wikimedia.org/wikipedia/commons/6/68/Extrema_example_original.svg" rel="noopener ugc nofollow" target="_blank">来源</a></p></figure><h2 id="7609" class="ni mr iq bd ms nj nk dn mw nl nm dp na lo nn no nc ls np nq ne lw nr ns ng iw bi translated">鞍点</h2><p id="7d87" class="pw-post-body-paragraph lf lg iq lh b li nt ka lk ll nu kd ln lo nv lq lr ls nw lu lv lw nx ly lz ma ij bi translated">另一个关键挑战是“鞍点”的出现。这是在对应于一个参数的一个方向上曲线处于局部最小值的点。另一方面，在对应于另一参数的第二方向上，曲线处于局部最大值。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi oe"><img src="../Images/29db2f5dfcf832d74e0a997510c7fe66.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*YgSaU3ugLX3YRvRu"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">鞍点(<a class="ae le" href="https://upload.wikimedia.org/wikipedia/commons/4/40/Saddle_point.png" rel="noopener ugc nofollow" target="_blank">来源</a></p></figure><p id="95c4" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">使鞍点变得棘手的是鞍点周围的区域通常相当平坦，就像一个高原。这意味着梯度接近于零。这导致优化器在第一个参数的方向上围绕鞍点振荡，而不能在第二个参数的方向上沿着斜坡下降。</p><p id="cf47" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">因此，梯度下降错误地认为它已经找到了最小值。</p><h2 id="2032" class="ni mr iq bd ms nj nk dn mw nl nm dp na lo nn no nc ls np nq ne lw nr ns ng iw bi translated">沟壑</h2><p id="fb34" class="pw-post-body-paragraph lf lg iq lh b li nt ka lk ll nu kd ln lo nv lq lr ls nw lu lv lw nx ly lz ma ij bi translated">梯度下降也很难穿越峡谷。这是一个狭长的山谷，向一个方向陡峭地倾斜。山谷的两侧)和轻轻地(即。沿着山谷)在第二方向。通常这个峡谷通向最低处。由于难以导航，这种形状也被称为病理弯曲。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi of"><img src="../Images/c08c388958d8ff324d487a9c410104f6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1100/format:webp/1*_ARatpzvoAt1MVLfFej4Mg.png"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">沟壑(经詹姆斯·马腾斯许可，改编自<a class="ae le" href="https://icml.cc/Conferences/2010/papers/458.pdf" rel="noopener ugc nofollow" target="_blank">来源</a>)</p></figure><p id="45fb" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">把这想象成一个狭窄的河谷，从山上缓缓向下倾斜，直到它在一个湖中结束。你要做的是迅速向下游的山谷方向移动。然而，梯度下降非常容易沿着山谷的两侧来回反弹，并朝着河流的方向非常缓慢地移动。</p><p id="d5f2" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">尽管他们继续使用梯度下降作为核心，但优化算法已经在普通梯度下降的基础上开发了一系列改进，以应对这些挑战。</p><h1 id="13f9" class="mq mr iq bd ms mt mu mv mw mx my mz na kf nb kg nc ki nd kj ne kl nf km ng nh bi translated">梯度下降的第一个改进—随机梯度下降(SGD)</h1><p id="e384" class="pw-post-body-paragraph lf lg iq lh b li nt ka lk ll nu kd ln lo nv lq lr ls nw lu lv lw nx ly lz ma ij bi translated">梯度下降通常意味着“整批梯度下降”，其中损失和梯度是使用数据集中的所有项目计算的。</p><p id="12bc" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">相反，小批量随机梯度下降为每次训练迭代随机选择数据集的子集。</p><p id="db21" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">这种随机性有助于我们探索损失景观。</p><p id="f0d5" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">前面我们提到过，损失曲线是在保持输入数据集不变的情况下，通过改变模型参数获得的。但是，如果通过在每个小批量中选择不同的数据样本来改变输入，损失值和梯度也会改变。换句话说，通过改变输入数据集，每个小批量的损失曲线略有不同。</p><p id="9e7f" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">因此，即使你在一个小批次中被困在风景中的某个地方，你可能会在下一个小批次中看到不同的风景，这让你继续前进。这可以防止算法卡在景观的特定部分，尤其是在训练的早期阶段。</p><h1 id="65b6" class="mq mr iq bd ms mt mu mv mw mx my mz na kf nb kg nc ki nd kj ne kl nf km ng nh bi translated">梯度下降的第二个改进—动量</h1><h2 id="604c" class="ni mr iq bd ms nj nk dn mw nl nm dp na lo nn no nc ls np nq ne lw nr ns ng iw bi translated">动态调整更新量</h2><p id="ab9b" class="pw-post-body-paragraph lf lg iq lh b li nt ka lk ll nu kd ln lo nv lq lr ls nw lu lv lw nx ly lz ma ij bi translated">梯度下降的一个棘手方面是处理陡坡。因为那里的坡度很大，当你真的想慢慢地、小心翼翼地前进时，你可以迈出一大步。这可能会导致来回跳动，从而减慢训练速度。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi og"><img src="../Images/6268781284081dd21f5cb0d3ce2c48aa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ffor6dqODlQs8iCqbFSJ-Q.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">(图片由作者提供)</p></figure><p id="4fad" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">理想情况下，您希望动态地改变更新的幅度，以便能够对周围环境的变化做出响应。如果坡很陡，你要减速。如果斜坡非常平坦，你可能想要加速等等。</p><p id="131f" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">使用梯度下降，您可以根据梯度和学习率更新每一步的权重。因此，要修改更新的大小，您可以做两件事:</p><ul class=""><li id="5743" class="mb mc iq lh b li lj ll lm lo md ls me lw mf ma nz mh mi mj bi translated">调整渐变</li><li id="cf06" class="mb mc iq lh b li ml ll mm lo mn ls mo lw mp ma nz mh mi mj bi translated">调整学习速度</li></ul><h2 id="72d3" class="ni mr iq bd ms nj nk dn mw nl nm dp na lo nn no nc ls np nq ne lw nr ns ng iw bi translated">动量vs新加坡元</h2><p id="5144" class="pw-post-body-paragraph lf lg iq lh b li nt ka lk ll nu kd ln lo nv lq lr ls nw lu lv lw nx ly lz ma ij bi translated">动量是在ie上面做前者的一种方式。调整渐变。</p><p id="7d58" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">使用SGD，我们只查看当前的梯度，而忽略我们所走路径上所有过去的梯度。这意味着，如果损失曲线突然出现异常，你的轨迹可能会偏离轨道。</p><p id="e683" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">另一方面，当使用动量时，你让过去的梯度引导你的整体方向，这样你就能保持在正确的方向上。这让你可以利用到那时为止你所看到的周围景观的知识，并有助于抑制损失曲线中异常值的影响。</p><ol class=""><li id="3b6b" class="mb mc iq lh b li lj ll lm lo md ls me lw mf ma mg mh mi mj bi translated">第一个问题是你能追溯到过去多久？越往后走，受异常的影响就越小。</li><li id="f7f7" class="mb mc iq lh b li ml ll mm lo mn ls mo lw mp ma mg mh mi mj bi translated">其次，从过去开始的每一个梯度都同样重要吗？不久前的事情应该比遥远过去的事情更重要，这是有道理的。因此，如果环境的变化不是暂时的异常，而是真正的结构性变化，那么你确实需要对此做出反应，并逐渐改变你的路线。</li></ol><p id="fe13" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">动量算法使用梯度的指数移动平均值，而不是当前梯度值。</p><h2 id="4aac" class="ni mr iq bd ms nj nk dn mw nl nm dp na lo nn no nc ls np nq ne lw nr ns ng iw bi translated">利用动量穿越峡谷</h2><p id="a252" class="pw-post-body-paragraph lf lg iq lh b li nt ka lk ll nu kd ln lo nv lq lr ls nw lu lv lw nx ly lz ma ij bi translated">动量帮助您解决病理曲率的狭窄峡谷问题，其中一个权重参数的梯度非常高，而另一个参数的梯度非常低。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/68121fc1607920d36c14ba1fb39b2b46.png" data-original-src="https://miro.medium.com/v2/resize:fit:1114/format:webp/0*HOLmd4ZqWFcqP1TZ.png"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">冲力帮助你穿越峡谷(经詹姆斯·马腾斯许可，由<a class="ae le" href="https://icml.cc/Conferences/2010/papers/458.pdf" rel="noopener ugc nofollow" target="_blank">来源</a>修改)</p></figure><p id="2d0f" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">通过使用动量，你可以抑制SGD可能出现的曲折振荡。</p><ul class=""><li id="f059" class="mb mc iq lh b li lj ll lm lo md ls me lw mf ma nz mh mi mj bi translated">对于具有陡坡的第一个参数，大的坡度导致从山谷的一侧到另一侧呈“之”字形。然而，在下一步中，这被反方向的“曲折”抵消了。</li><li id="1c0c" class="mb mc iq lh b li ml ll mm lo mn ls mo lw mp ma nz mh mi mj bi translated">另一方面，对于第二参数，来自第一步骤的小更新被第二步骤的小更新加强，因为它们在相同的方向上。这是你想去的山谷的方向。</li></ul><p id="3c52" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">使用不同公式的动量优化算法的一些示例如下:</p><ul class=""><li id="246b" class="mb mc iq lh b li lj ll lm lo md ls me lw mf ma nz mh mi mj bi translated">带动量的SGD</li><li id="1da1" class="mb mc iq lh b li ml ll mm lo mn ls mo lw mp ma nz mh mi mj bi translated">涅斯托罗夫加速梯度</li></ul><h1 id="41d8" class="mq mr iq bd ms mt mu mv mw mx my mz na kf nb kg nc ki nd kj ne kl nf km ng nh bi translated"><strong class="ak">梯度下降的第三个改进——修改学习率(基于梯度)</strong></h1><p id="13af" class="pw-post-body-paragraph lf lg iq lh b li nt ka lk ll nu kd ln lo nv lq lr ls nw lu lv lw nx ly lz ma ij bi translated">如上所述，修改参数更新量的第二种方式是通过调整学习速率。</p><p id="c752" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">到目前为止，从一次迭代到下一次迭代，我们一直保持学习速率不变。其次，梯度更新对所有参数使用相同的学习速率。</p><p id="27d8" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">然而，正如我们已经看到的，不同参数的梯度之间可能有很大的变化。一个参数可能有一个陡坡，而另一个有一个缓坡。</p><p id="f3f4" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">我们可以利用这一点来调整每个参数的学习速率。我们可以利用过去的梯度(分别针对每个参数)来选择该参数的学习速率。</p><p id="aac2" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">有一些优化算法可以做到这一点，使用稍微不同的技术，如Adagrad，Adadelta，RMS Prop。</p><p id="a62b" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">例如，Adagrad将过去的梯度平方并相加，对它们进行平均加权。RMSProp也对过去的梯度求平方，但是使用它们的指数移动平均值，因此给予最近的梯度更多的重要性。</p><p id="8fab" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">现在，通过平方梯度，它们都变成正的。方向一致。这否定了我们讨论动量的抵消效应，梯度方向相反。</p><p id="de39" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">这意味着对于一个斜率很陡的参数，梯度很大，梯度的平方也很大，并且总是正的，所以它们积累得很快。为了抑制这种情况，该算法通过将累积的平方梯度除以一个较大的因子来计算学习率。这使得它在陡坡上减速。</p><p id="187a" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">类似地，对于较浅的斜率，累积较小，因此该算法将累积的平方除以较小的因子来计算学习率。这提高了缓坡的学习速度。</p><p id="cdb6" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">一些优化器算法结合了这两种方法——如上修改学习率，以及使用动量来修改梯度。亚当和它的许多变种，羔羊。</p><h1 id="c393" class="mq mr iq bd ms mt mu mv mw mx my mz na kf nb kg nc ki nd kj ne kl nf km ng nh bi translated"><strong class="ak">梯度下降的第四个改进——修改学习率(根据你的训练进度)</strong></h1><p id="4f6a" class="pw-post-body-paragraph lf lg iq lh b li nt ka lk ll nu kd ln lo nv lq lr ls nw lu lv lw nx ly lz ma ij bi translated">在前面的部分中，基于参数的梯度修改了学习率。此外，我们可以根据训练过程的进度调整学习速度。学习率是基于训练时期设置的，并且与该点的模型参数无关。</p><p id="c416" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">这其实根本不是优化人员做的。事实上，它是被称为调度器的神经网络的一个独立组件。我提到这一点是为了完整性，并显示与我们已经讨论过的优化技术的关系，但在这里不会进一步讨论它们。</p><h1 id="8355" class="mq mr iq bd ms mt mu mv mw mx my mz na kf nb kg nc ki nd kj ne kl nf km ng nh bi translated"><strong class="ak">结论</strong></h1><p id="aed8" class="pw-post-body-paragraph lf lg iq lh b li nt ka lk ll nu kd ln lo nv lq lr ls nw lu lv lw nx ly lz ma ij bi translated">我们现在了解了基于梯度下降的优化器所使用的基本技术，为什么使用它们，以及它们之间的关系。</p><p id="508f" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">这使我们能够更深入地研究许多具体的优化算法，并详细了解它们是如何工作的。我希望很快在另一篇文章中讨论这个问题…</p><p id="26e0" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">最后，如果你喜欢这篇文章，你可能也会喜欢我关于变形金刚、音频深度学习和地理定位机器学习的其他系列。</p><div class="oi oj gp gr ok ol"><a rel="noopener follow" target="_blank" href="/transformers-explained-visually-part-1-overview-of-functionality-95a6dd460452"><div class="om ab fo"><div class="on ab oo cl cj op"><h2 class="bd ja gy z fp oq fr fs or fu fw iz bi translated">直观解释的变压器(第1部分):功能概述</h2><div class="os l"><h3 class="bd b gy z fp oq fr fs or fu fw dk translated">NLP变形金刚的简明指南，以及为什么它们比rnn更好，用简单的英语。注意力如何帮助…</h3></div><div class="ot l"><p class="bd b dl z fp oq fr fs or fu fw dk translated">towardsdatascience.com</p></div></div><div class="ou l"><div class="ov l ow ox oy ou oz ky ol"/></div></div></a></div><div class="oi oj gp gr ok ol"><a rel="noopener follow" target="_blank" href="/audio-deep-learning-made-simple-part-1-state-of-the-art-techniques-da1d3dff2504"><div class="om ab fo"><div class="on ab oo cl cj op"><h2 class="bd ja gy z fp oq fr fs or fu fw iz bi translated">音频深度学习变得简单(第一部分):最新技术</h2><div class="os l"><h3 class="bd b gy z fp oq fr fs or fu fw dk translated">颠覆性深度学习音频应用和架构世界的温和指南。以及为什么我们都需要…</h3></div><div class="ot l"><p class="bd b dl z fp oq fr fs or fu fw dk translated">towardsdatascience.com</p></div></div><div class="ou l"><div class="pa l ow ox oy ou oz ky ol"/></div></div></a></div><div class="oi oj gp gr ok ol"><a rel="noopener follow" target="_blank" href="/leveraging-geolocation-data-for-machine-learning-essential-techniques-192ce3a969bc"><div class="om ab fo"><div class="on ab oo cl cj op"><h2 class="bd ja gy z fp oq fr fs or fu fw iz bi translated">利用地理位置数据进行机器学习:基本技术</h2><div class="os l"><h3 class="bd b gy z fp oq fr fs or fu fw dk translated">简明的地理空间数据特征工程和可视化指南</h3></div><div class="ot l"><p class="bd b dl z fp oq fr fs or fu fw dk translated">towardsdatascience.com</p></div></div><div class="ou l"><div class="pb l ow ox oy ou oz ky ol"/></div></div></a></div><p id="0c87" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">让我们继续学习吧！</p></div></div>    
</body>
</html>