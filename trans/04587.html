<html>
<head>
<title>Faster, smoother, smaller, more accurate and more robust face alignment models on CPU</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">CPU上更快、更平滑、更小、更准确和更鲁棒的人脸对齐模型</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/faster-smoother-smaller-more-accurate-and-more-robust-face-alignment-models-d8cc867efc5?source=collection_archive---------7-----------------------#2021-04-20">https://towardsdatascience.com/faster-smoother-smaller-more-accurate-and-more-robust-face-alignment-models-d8cc867efc5?source=collection_archive---------7-----------------------#2021-04-20</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="294f" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsdatascience.com/tagged/thoughts-and-theory" rel="noopener" target="_blank">思想和理论</a></h2><div class=""/><div class=""><h2 id="e6c5" class="pw-subtitle-paragraph jw iz iq bd b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn dk translated">人脸标志检测器的训练策略、实现优化和新的评价方法。</h2></div><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ko"><img src="../Images/a71ea1bc6a4e023f814da15c7996ba17.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3Et97MeRGltEwZ-Zh_eU6g.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">使用和不使用GTX训练的正面人脸图像的DAN、ERT和LBF方法的推论(图片由作者提供)。</p></figure><p id="f82c" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">在你的人脸分析项目或应用中，是否使用人脸地标检测作为核心组件？你是否使用流行的计算机视觉库中的人脸对齐方法，如<strong class="lg ja">OpenCV</strong>【1】或<strong class="lg ja">Dlib</strong>【2】？如果你想改进你的人脸对齐模型，在这篇文章中，我将解释如何使它们更快、更平滑、更小、更准确、更健壮，特别是对于人脸跟踪应用。此外，我在我的<a class="ae mb" href="https://gitlab.com/visualhealth/vhpapers/real-time-facealignment" rel="noopener ugc nofollow" target="_blank"> Gitlab知识库</a>中提供了一组已经为<em class="ma"> OpenCV </em>和<em class="ma"> Dlib </em>改进的模型，该知识库是为我们在<strong class="lg ja">实时图像处理杂志</strong> (Springer)上发表的文章而创建的:</p><blockquote class="mc md me"><p id="2a35" class="le lf ma lg b lh li ka lj lk ll kd lm mf lo lp lq mg ls lt lu mh lw lx ly lz ij bi translated">Alvarez Casado，c .，Bordallo Lopez，M. <br/>实时人脸对齐:评估方法、训练策略和实现优化。<br/>施普林格实时图像处理杂志，2021年</p></blockquote></div><div class="ab cl mi mj hu mk" role="separator"><span class="ml bw bk mm mn mo"/><span class="ml bw bk mm mn mo"/><span class="ml bw bk mm mn"/></div><div class="ij ik il im in"><p id="ec1c" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi mp translated">ace对齐在大多数人脸分析系统中是一个至关重要的组件。它着重于识别图像或视频中人脸的几个关键点的位置。尽管在流行的计算机视觉库中，如<em class="ma"> OpenCV </em>或<em class="ma"> Dlib </em>中，有几种方法和模型可供开发人员使用，但他们仍然面临着照明不足、极端头部姿势或遮挡等挑战，特别是当他们受到实时应用需求的限制时。</p><p id="e523" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">如前所述，我们最近发表了一篇期刊文章，其中我们提出了一套基于数据增强和软件优化技术的最佳训练策略和实施规则，这些规则有助于改进属于几个用于人脸对齐的<strong class="lg ja">实时算法</strong>的各种模型。我们将它们命名为<strong class="lg ja">一般培训扩展</strong> ( <strong class="lg ja"> GTX </strong>)策略。此外，我们还提出了一组扩展的评估指标，允许新的评估来缓解在实时跟踪上下文和真实场景中发现的典型问题。实验结果表明，使用我们提出的技术生成的模型更快、更小、更准确，在特定的挑战性条件下更鲁棒，在跟踪系统中更平滑，如下一个视频所示。</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="my mz l"/></div></figure><p id="73f0" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">这种训练策略显示出适用于不同类型的设备和算法，使它们在学术和工业应用中通用。</p><p id="9688" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">为了评估我们的方法，我们专注于三种被认为是不同应用的最先进的人脸对齐方法，<strong class="lg ja"> LBF </strong>(局部二进制特征)[3]用于<em class="ma"> OpenCV </em>，<strong class="lg ja"> ERT </strong>(回归树的集合)[4]用于<em class="ma"> Dlib </em>和<strong class="lg ja"> DAN </strong>(深度对齐网络)[5]，可在pip (Python)中获得。LBF和ERT方法，也称为快速人脸对齐方法，基于级联回归器，而DAN算法基于深度学习方法。我们使用由多饼地标方案定义的68个地标位置。</p><p id="40b9" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated"><strong class="lg ja">评估度量和基准<br/> </strong>面部对齐的标准度量通常包括那些与基础事实注释相比时与预测质量相关的度量。这主要包括<strong class="lg ja">准确度</strong>和<strong class="lg ja">误差</strong>，在某些情况下还有<strong class="lg ja">失败率</strong>。此外，通常计算<strong class="lg ja">计算效率</strong>，通常以每个面的毫秒数表示，以及每秒处理的帧数(fps)。</p><p id="fa4a" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">问题在于，这些度量没有测量在大多数面部对齐模型中观察到的若干现象，例如:</p><ul class=""><li id="7c3b" class="na nb iq lg b lh li lk ll ln nc lr nd lv ne lz nf ng nh ni bi translated"><strong class="lg ja">抖动</strong>:恼人的帧间噪声和微小变化。这种影响可能是由缺乏训练样本、具有高方差的不精确注释、不干净的数据集或者在例如初始化阶段使用某种(伪)随机信息的非确定性推理方法产生的。</li><li id="d524" class="na nb iq lg b lh nj lk nk ln nl lr nm lv nn lz nf ng nh ni bi translated"><strong class="lg ja">人脸对齐灵敏度</strong>:大部分人脸对齐算法依赖人脸检测包围盒进行形状初始化。面部检测器在连续的帧中并不总是一致的。此外，大多数人脸对齐模型是用一个人脸检测器的人脸矩形来训练的。</li></ul><p id="6583" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">基于这些现象，我们定义了<strong class="lg ja">三个新颖的度量</strong>，并使用它们来评估一个模型在跟踪应用期间有多好的性能。我们建议计算两个连续帧估计形状之间的帧间误差:</p><ul class=""><li id="2fdc" class="na nb iq lg b lh li lk ll ln nc lr nd lv ne lz nf ng nh ni bi translated"><strong class="lg ja">地标均方位移</strong>度量(laMSD)，计算地标在静态人脸下如何沿视频移动。</li><li id="47c9" class="na nb iq lg b lh nj lk nk ln nl lr nm lv nn lz nf ng nh ni bi translated"><strong class="lg ja">归一化抖动灵敏度均方误差</strong> (NJS-MSE_σ <strong class="lg ja"> </strong>)是通过使用基于水平和垂直轴上的小中心偏移的面部矩形的随机变化来推断一组参考图像上的面部标志而计算的度量。误差测量产生评估度量，以比较面部对准模型的<strong class="lg ja"> <em class="ma">抖动鲁棒性</em> </strong>。</li><li id="9a59" class="na nb iq lg b lh nj lk nk ln nl lr nm lv nn lz nf ng nh ni bi translated"><strong class="lg ja">归一化人脸检测灵敏度均方误差</strong> (NFDS-MSE_σ <strong class="lg ja"> </strong>)。对于此度量，随机变化基于水平和垂直轴上的面部矩形中心偏移以及面部边界框的大小(宽度和高度)的随机变化。这产生了一个评估度量，用于比较我们的面部对准模型与<strong class="lg ja"> <em class="ma">不同面部检测器</em> </strong>的鲁棒性。</li></ul><p id="4aeb" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">为了评估我们的模型，我们在文献中先前提出的完善的标准基准和我们精心设计的一组基准的组合中测试它们。大多数相关工作主要关注两个性能指标，<strong class="lg ja">准确度</strong>和<strong class="lg ja">效率</strong>，但我们认为它们不是评价人脸对齐模型的唯一关键指标。我们相信这种基准组合有助于描述在无约束环境下运行的实时应用程序中更加<strong class="lg ja">客观</strong>和<strong class="lg ja">现实的行为。这些基准描述如下:</strong></p><ul class=""><li id="a2fd" class="na nb iq lg b lh li lk ll ln nc lr nd lv ne lz nf ng nh ni bi translated"><strong class="lg ja">野外常见的</strong> <strong class="lg ja">基准</strong>(300-W)【6】，用以测试一般精度和误差作为与其他机型比较的参考。</li><li id="08fa" class="na nb iq lg b lh nj lk nk ln nl lr nm lv nn lz nf ng nh ni bi translated"><strong class="lg ja">抖动基准</strong>，重点测量模型的抖动效果</li><li id="5b46" class="na nb iq lg b lh nj lk nk ln nl lr nm lv nn lz nf ng nh ni bi translated"><strong class="lg ja">特定领域基准</strong>，侧重于测试特定领域的模型，如弱光条件、极端倾斜的头部、强背光场景等</li></ul><p id="50a9" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated"><strong class="lg ja">训练策略</strong> <br/>主要思想是实现“更好”的人脸对齐模型。传统上，这是指更精确的模型。但如前所述，我们通过评估实时应用和真实场景中的其他关键方面，特别是在跟踪地标时，扩展了这一概念。</p><p id="6283" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">为了实现这一目标，我们建议在培训过程中执行几条规则，例如:</p><ul class=""><li id="4d86" class="na nb iq lg b lh li lk ll ln nc lr nd lv ne lz nf ng nh ni bi translated"><strong class="lg ja">基于图像处理的数据扩充</strong>:几何和颜色变换。</li><li id="52dc" class="na nb iq lg b lh nj lk nk ln nl lr nm lv nn lz nf ng nh ni bi translated"><strong class="lg ja">基于统计操作的数据扩充</strong>:图像归一化、噪声注入、初始化变化、异常值去除。</li><li id="cd09" class="na nb iq lg b lh nj lk nk ln nl lr nm lv nn lz nf ng nh ni bi translated"><strong class="lg ja">添加领域特定数据</strong>:我们研究属于已证明在基本模型中失败的特定条件的图像集合。这些图像用一个缓慢而精确的模型和一个教师<br/>学生建筑来注释。在我们之前的<a class="ae mb" href="https://www.semanticscholar.org/paper/Face-alignment%3A-improving-the-accuracy-of-fast-data-Casado-L%C3%B3pez/717f526e371992cc439244ff229d5fc0f880c747" rel="noopener ugc nofollow" target="_blank">出版物</a>中可以看到关于这项技术的更多细节。</li></ul><p id="e2ea" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated"><strong class="lg ja">实现和并行化</strong> <br/>我们已经描述了从串行优化到并行化策略的一系列实现技术。从性能及其可能的精度折衷的角度分析了这些解决方案。</p><p id="c59d" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">为了测试这些建议的优化，我们实现了自己的C++版本的LBF方法。因此，我们可以将我们自己的实现与<em class="ma"> OpenCV </em>实现进行比较，例如相同的算法，并在<em class="ma"> OpenCV </em>和<em class="ma"> Dlib </em>的实现版本中执行一些其他优化。</p><p id="6949" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">在文章中，我们解释了如何加快训练和推理过程。在训练中，我们通常使用<em class="ma">OpenMP</em>【7】来加速算法拓扑允许的循环。更准确地说，我们在LBF算法中加速了局部特征映射函数的学习，因为如图1所示，每个界标都是独立于其余的。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi no"><img src="../Images/e106810fbd1db87cbdb1ffe993d9dbdf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_jxhni2DpPYR6K97o4HNqA.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">图。1(图片由作者提供):LBF方法中的训练过程。学习过程遵循级联式拓扑。局部特征的线性回归最小化当前形状和目标地面真实形状之间的距离。</p></figure><p id="00a6" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">在训练我们的面部对齐模型之后，我们通过主要应用三种策略来加速推断:</p><ul class=""><li id="d03b" class="na nb iq lg b lh li lk ll ln nc lr nd lv ne lz nf ng nh ni bi translated"><strong class="lg ja">每个地标计算的并行化</strong>(局部特征映射功能)。</li><li id="eb8a" class="na nb iq lg b lh nj lk nk ln nl lr nm lv nn lz nf ng nh ni bi translated"><strong class="lg ja">加速在上述过程中提取的高维特征</strong>和全局线性回归或转移矩阵w的相乘。我们应用三种不同的优化技术，如图2所示。</li><li id="1c78" class="na nb iq lg b lh nj lk nk ln nl lr nm lv nn lz nf ng nh ni bi translated"><strong class="lg ja">对生成的模型进行量化。</strong>我们将模型的数据从浮点数转换成短整数。这提高了推断的速度，尤其是在具有缓存和内存限制的设备上，例如移动设备。</li></ul><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi np"><img src="../Images/5419d506749514f653d07db4c76c7701.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lq00VBvt9_pGzTAljMyUKg.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">图2(图片由作者提供):稀疏线性投影W t的乘法，在面部对齐<br/>过程的每个阶段使用3种不同的技术计算。</p></figure><p id="da8d" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated"><strong class="lg ja">对比评估</strong> <br/>为了更好地理解之前的规则集以改进我们的人脸对齐模型，我们从误差、模型大小、计算时间以及跟踪模式和挑战场景中的性能等方面，对最终模型中的训练策略和实现优化技术的影响进行了实证评估和分析。作为GTX训练策略的结果，我们获得了一组被标记为LBF_gtx模型的增强模型，这些模型将与LBF_base模型和其他互补模型进行比较。</p><p id="2934" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated"><strong class="lg ja">精度与尺寸和速度对比:</strong></p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi nq"><img src="../Images/95d925005677ba8e53feb7f9f346885e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-hSj1vKVNMh7Jcma9lyuOw.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">300瓦全套测试的平均误差(%)和失败图像(%)。在中间一栏中，每个型号的大小以兆字节(MB)为单位。在上一栏中，基准测试中的平均计算时间是以每张脸毫秒数为单位的。在评估的LBF模型中，“q”表示量化模型，“float”是原始模型，它们都已经集成到我们的C++管道中。DAN模型是原始论文中提供的原始模型。并且通过Dlib库使用ERT模型，比较库提供的模型和我们自己训练的ERT模型。OpenCV LBF模型通过OpenCV库使用。索引1、2、3和4表示OpenCV默认模型与不同的人脸检测器一起使用，包括板载的哈尔卡斯卡德人脸检测器和DNN人脸检测器。该推断是使用配备2.6 GHz英特尔酷睿i7处理器的笔记本电脑做出的(图片由作者提供)。</p></figure><p id="0e37" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated"><strong class="lg ja">我们的抖动基准测试中的抖动评估:</strong></p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi nr"><img src="../Images/b4e15854039ecbe45d26bdca2e72c7c2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-ixWebUBWe-UlA6gMS7LqA.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">在我们自行设计的抖动基准测试中，总抖动量和每帧抖动量按每帧laMSD增长排序。该推断是使用配备2.6 GHz英特尔酷睿i7处理器的笔记本电脑得出的。我们的LBF模型是浮动版本，以便与OpenCV one(图片由作者提供)进行公平的比较。</p></figure><p id="c219" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated"><strong class="lg ja">特定于领域的性能指标评测:</strong></p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ns"><img src="../Images/374e6398bfed7955d42bf9eaad8810f8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kuxS22JvHdH-gi7ompkaaA.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">在使用领域特定的未标记数据和师生方案训练基础模型<strong class="bd nt">之后，在300W全套和领域特定的测试子集上的平均误差(%)和失败图像(%)(图片由作者提供)。</strong></p></figure><p id="850e" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated"><strong class="lg ja">模型量化的影响:</strong></p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi nu"><img src="../Images/27fa63e9f10d4a11555a609b5824bcd5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9f8hQBWTNyXOUZpA7vuF3g.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">对于不同的<br/>硬件设备，量化在准确性(300 w-公共数据集)、速度和大小方面的影响(图片由作者提供)。</p></figure><h1 id="4ddf" class="nv nw iq bd nx ny nz oa ob oc od oe of kf og kg oh ki oi kj oj kl ok km ol om bi translated"><strong class="ak">定性结果</strong></h1><p id="7a0e" class="pw-post-body-paragraph le lf iq lg b lh on ka lj lk oo kd lm ln op lp lq lr oq lt lu lv or lx ly lz ij bi translated">为了查看建议的培训策略和实施优化的影响，我创建了一些视频来观察它们的效果。</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="os mz l"/></div></figure><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="my mz l"/></div></figure><h1 id="66df" class="nv nw iq bd nx ny nz oa ob oc od oe of kf og kg oh ki oi kj oj kl ok km ol om bi translated"><strong class="ak">结论</strong></h1><p id="ebdc" class="pw-post-body-paragraph le lf iq lg b lh on ka lj lk oo kd lm ln op lp lq lr oq lt lu lv or lx ly lz ij bi translated">本文的结果显示了一组优化和训练策略在人脸对齐系统环境中的影响，该人脸对齐系统旨在集成到在台式计算机和移动设备中实时运行的解决方案中。在没有任何算法开发的情况下，我们展示了训练、实施和系统设计的巧妙策略如何在准确性、速度、模型大小或挑战性条件下的失败率方面对模型的性能产生巨大影响。</p><p id="5f13" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">感谢阅读。我希望你从阅读这篇文章中学到的和我写这篇文章时学到的一样多。干杯！</p><p id="f216" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">这项工作与 <a class="ot ou ep" href="https://medium.com/u/f20d4f60ca7e?source=post_page-----d8cc867efc5--------------------------------" rel="noopener" target="_blank"> <strong class="lg ja">米盖尔·博尔达洛</strong> </a> <strong class="lg ja">合作完成。</strong></p><ul class=""><li id="d96c" class="na nb iq lg b lh li lk ll ln nc lr nd lv ne lz nf ng nh ni bi translated">奥卢大学助理教授兼讲师Miguel Bordallo López博士。<a class="ae mb" href="https://sites.google.com/view/miguelbordallo" rel="noopener ugc nofollow" target="_blank">https://sites.google.com/view/miguelbordallo</a></li></ul><p id="45df" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated"><strong class="lg ja">更多详情请见:</strong></p><blockquote class="mc md me"><p id="5f90" class="le lf ma lg b lh li ka lj lk ll kd lm mf lo lp lq mg ls lt lu mh lw lx ly lz ij bi translated">Alvarez Casado，c .，Bordallo Lopez，M. <br/>实时人脸对齐:评估方法、训练策略和实现优化。<br/>施普林格实时图像处理杂志，2021</p></blockquote><p id="4554" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated"><strong class="lg ja">从以下位置下载模型:<br/> </strong>从文章的<a class="ae mb" href="https://www.google.com/url?q=https%3A%2F%2Fgitlab.com%2Fvisualhealth%2Fvhpapers%2Freal-time-facealignment&amp;sa=D&amp;sntz=1&amp;usg=AFQjCNFSpOaKHL10u4WecuzO6SytAeMGOg" rel="noopener ugc nofollow" target="_blank"> GitLab资源库</a>中下载代码、模型、基准数据集和示例视频，文章中生成的模型现在是<a class="ae mb" href="https://www.google.com/url?q=https%3A%2F%2Fgithub.com%2Fdavisking%2Fdlib-models&amp;sa=D&amp;sntz=1&amp;usg=AFQjCNEe9SzwKWueU1Zk6cbjq4RjbRlBEA" rel="noopener ugc nofollow" target="_blank"> Dlib库模型库</a>中的标准模型。</p><ul class=""><li id="647a" class="na nb iq lg b lh li lk ll ln nc lr nd lv ne lz nf ng nh ni bi translated">我们的LBF_gtx和ERT_gtx车型:<a class="ae mb" href="https://drive.google.com/drive/folders/1t1fRQfTaL1-XgGA1JSzuvLSXsitZ6Scj" rel="noopener ugc nofollow" target="_blank">我们的车型</a></li><li id="0ad4" class="na nb iq lg b lh nj lk nk ln nl lr nm lv nn lz nf ng nh ni bi translated">原始和默认OpenCV LBF模型:<a class="ae mb" href="https://raw.githubusercontent.com/kurnianggoro/GSOC2017/master/data/lbfmodel.yaml" rel="noopener ugc nofollow" target="_blank"> OpenCV LBF模型</a></li><li id="fe3f" class="na nb iq lg b lh nj lk nk ln nl lr nm lv nn lz nf ng nh ni bi translated">原丹型号:<a class="ae mb" href="https://drive.google.com/drive/folders/168tC2OxS5DjyaiuDy_JhIV3eje8K_PLJ" rel="noopener ugc nofollow" target="_blank">丹型号</a></li><li id="b693" class="na nb iq lg b lh nj lk nk ln nl lr nm lv nn lz nf ng nh ni bi translated">原始和默认Dlib ERT模型:<a class="ae mb" href="https://github.com/davisking/dlib-models" rel="noopener ugc nofollow" target="_blank"> Dlib ERT模型</a></li></ul></div><div class="ab cl mi mj hu mk" role="separator"><span class="ml bw bk mm mn mo"/><span class="ml bw bk mm mn mo"/><span class="ml bw bk mm mn"/></div><div class="ij ik il im in"><h2 id="2957" class="ov nw iq bd nx ow ox dn ob oy oz dp of ln pa pb oh lr pc pd oj lv pe pf ol iw bi translated"><strong class="ak">参考文献</strong></h2><p id="0f91" class="pw-post-body-paragraph le lf iq lg b lh on ka lj lk oo kd lm ln op lp lq lr oq lt lu lv or lx ly lz ij bi translated">[1] OpenCV库:<a class="ae mb" href="https://opencv.org/" rel="noopener ugc nofollow" target="_blank"/><br/>【2】Dlib库:<a class="ae mb" href="http://dlib.net/" rel="noopener ugc nofollow" target="_blank"/><br/>【3】s . Ren，X. Cao，Y. Wei，J. Sun，“通过回归局部二值特征实现3000 FPS的人脸对齐”，<em class="ma"> 2014年IEEE计算机视觉和模式识别会议</em>，美国俄亥俄州哥伦布市，2014年，第1685–1692页，doi: 10.1109/CVPR.2014 “使用回归树集合的一毫秒人脸对齐”，<em class="ma"> 2014年IEEE计算机视觉和模式识别会议</em>，美国俄亥俄州哥伦布，2014年，第1867-1874页，doi:10.1109/cvpr . 2014.241 .<br/>[5]m . Kowalski，J. Naruniec和T. Trzcinski，“深度对齐网络:用于鲁棒人脸对齐的卷积神经网络”，<em class="ma"> 2017年 “野外300人面临的挑战:数据库和结果”。图像和视觉计算(IMAVIS)，面部标志定位“野外”特刊。2016.<br/>【7】OpenMP库:<a class="ae mb" href="https://www.openmp.org/" rel="noopener ugc nofollow" target="_blank">https://www.openmp.org/</a></em></p></div></div>    
</body>
</html>