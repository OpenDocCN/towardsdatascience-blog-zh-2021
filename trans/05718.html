<html>
<head>
<title>Gradient Descent Algorithm — a deep dive</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">梯度下降算法——深度探索</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/gradient-descent-algorithm-a-deep-dive-cf04e8115f21?source=collection_archive---------0-----------------------#2021-05-22">https://towardsdatascience.com/gradient-descent-algorithm-a-deep-dive-cf04e8115f21?source=collection_archive---------0-----------------------#2021-05-22</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="9342" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">梯度下降法为机器学习和深度学习技术奠定了基础。让我们来探索它是如何工作的，何时使用它，以及它对各种功能的表现如何。</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/5f355f29d222ac7399d4f150611b2833.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SMGMxLZndmnSY5vlzHwU6w.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">来自<a class="ae kv" href="https://pixabay.com/" rel="noopener ugc nofollow" target="_blank"> Pixabay </a>的<a class="ae kv" href="https://pixabay.com/users/johnsonmartin-724525/" rel="noopener ugc nofollow" target="_blank"> JohnsonMartin </a>图片</p></figure></div><div class="ab cl kw kx hu ky" role="separator"><span class="kz bw bk la lb lc"/><span class="kz bw bk la lb lc"/><span class="kz bw bk la lb"/></div><div class="ij ik il im in"><h1 id="5658" class="ld le iq bd lf lg lh li lj lk ll lm ln jw lo jx lp jz lq ka lr kc ls kd lt lu bi translated">1.介绍</h1><p id="f0ca" class="pw-post-body-paragraph lv lw iq lx b ly lz jr ma mb mc ju md me mf mg mh mi mj mk ml mm mn mo mp mq ij bi translated"><strong class="lx ir">梯度下降</strong> (GD)是一种迭代一阶优化算法，用于寻找给定函数的局部最小值/最大值。这种方法通常用于<em class="mr">机器学习</em> (ML)和<em class="mr">深度学习</em> (DL)，以最小化成本/损失函数(例如，在线性回归中)。由于其重要性和易于实现，这种算法通常在几乎所有机器学习课程的开始时教授。</p><p id="a007" class="pw-post-body-paragraph lv lw iq lx b ly ms jr ma mb mt ju md me mu mg mh mi mv mk ml mm mw mo mp mq ij bi translated">然而，它的使用不仅限于ML/DL，它还被广泛应用于以下领域:</p><ul class=""><li id="cbd6" class="mx my iq lx b ly ms mb mt me mz mi na mm nb mq nc nd ne nf bi translated">控制工程(机器人，化学等。)</li><li id="c4a3" class="mx my iq lx b ly ng mb nh me ni mi nj mm nk mq nc nd ne nf bi translated">电脑游戏</li><li id="a979" class="mx my iq lx b ly ng mb nh me ni mi nj mm nk mq nc nd ne nf bi translated">机械工程</li></ul><p id="35f1" class="pw-post-body-paragraph lv lw iq lx b ly ms jr ma mb mt ju md me mu mg mh mi mv mk ml mm mw mo mp mq ij bi translated">这就是为什么今天我们将深入探讨一阶梯度下降算法的数学、实现和行为。我们将直接导航自定义(成本)函数以找到其最小值，因此不会像典型的ML教程中那样有底层数据——我们将在函数的形状方面更加灵活。</p><p id="2ebe" class="pw-post-body-paragraph lv lw iq lx b ly ms jr ma mb mt ju md me mu mg mh mi mv mk ml mm mw mo mp mq ij bi translated">这种方法是在现代计算机时代之前提出的，其间有一个密集的开发，导致了它的许多改进版本，但在本文中，我们将使用Python中实现的基本/普通梯度下降。</p><h1 id="f94d" class="ld le iq bd lf lg nl li lj lk nm lm ln jw nn jx lp jz no ka lr kc np kd lt lu bi translated">2.功能需求</h1><p id="e2ad" class="pw-post-body-paragraph lv lw iq lx b ly lz jr ma mb mc ju md me mf mg mh mi mj mk ml mm mn mo mp mq ij bi translated">梯度下降算法并不适用于所有函数。有两个具体要求。功能必须是:</p><ul class=""><li id="5568" class="mx my iq lx b ly ms mb mt me mz mi na mm nb mq nc nd ne nf bi translated"><strong class="lx ir">可微</strong></li><li id="c3cb" class="mx my iq lx b ly ng mb nh me ni mi nj mm nk mq nc nd ne nf bi translated"><strong class="lx ir">凸面</strong></li></ul><p id="43c4" class="pw-post-body-paragraph lv lw iq lx b ly ms jr ma mb mt ju md me mu mg mh mi mv mk ml mm mw mo mp mq ij bi translated">首先，它必须是<strong class="lx ir">可微的</strong>是什么意思？如果一个函数是可微的，那么它对定义域中的每个点都有导数——不是所有的函数都满足这些标准。首先，让我们看一些满足这个标准的函数的例子:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nq"><img src="../Images/d450056f7b920b3aac7b1d02bba698ad.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cZt-P3GriY4Qva--IsbXig.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">可微函数的例子；作者图片</p></figure><p id="bfe6" class="pw-post-body-paragraph lv lw iq lx b ly ms jr ma mb mt ju md me mu mg mh mi mv mk ml mm mw mo mp mq ij bi translated">典型的不可微函数有一个台阶，一个尖点或一个不连续点:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nq"><img src="../Images/7dde8e2174e8e364771c95b2cabe3f89.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*C3eHe2Kt1cFiq7METjx51w.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">不可微函数的例子；作者图片</p></figure><p id="9ed3" class="pw-post-body-paragraph lv lw iq lx b ly ms jr ma mb mt ju md me mu mg mh mi mv mk ml mm mw mo mp mq ij bi translated">下一个要求— <strong class="lx ir">函数必须是凸的</strong>。对于一元函数，这意味着连接两个函数点的线段位于曲线上或曲线上方(不与曲线相交)。如果是，这意味着它有一个局部最小值，而不是一个全局最小值。</p><p id="a859" class="pw-post-body-paragraph lv lw iq lx b ly ms jr ma mb mt ju md me mu mg mh mi mv mk ml mm mw mo mp mq ij bi translated">数学上，对于位于函数曲线上的两点x₁、x₂，该条件表示为:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/5e1fccb86f85830cef79496dc5fd1c1f.png" data-original-src="https://miro.medium.com/v2/resize:fit:686/1*JS6lMwXqsos9XfDxIwLVPQ.gif"/></div></figure><p id="442e" class="pw-post-body-paragraph lv lw iq lx b ly ms jr ma mb mt ju md me mu mg mh mi mv mk ml mm mw mo mp mq ij bi translated">其中，λ表示剖面线上的点的位置，其值必须介于0(左点)和1(右点)之间，例如，λ=0.5表示位置在中间。</p><p id="2827" class="pw-post-body-paragraph lv lw iq lx b ly ms jr ma mb mt ju md me mu mg mh mi mv mk ml mm mw mo mp mq ij bi translated">下面有两个带有示例剖面线的函数。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nq"><img src="../Images/8e73381b3f58d7c7d59ae7eede407352.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OfzsEguVgdG15jgT2i__9g.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">示例性凸函数和非凸函数；作者图片</p></figure><p id="8250" class="pw-post-body-paragraph lv lw iq lx b ly ms jr ma mb mt ju md me mu mg mh mi mv mk ml mm mw mo mp mq ij bi translated">从数学上检查一元函数是否凸的另一种方法是计算二阶导数，并检查其值是否总是大于0。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/0771949c4b93c178756f21896d36c671.png" data-original-src="https://miro.medium.com/v2/resize:fit:172/1*7FwLtRGZ40bcPweVTjEHfQ.gif"/></div></figure><p id="429e" class="pw-post-body-paragraph lv lw iq lx b ly ms jr ma mb mt ju md me mu mg mh mi mv mk ml mm mw mo mp mq ij bi translated">我们来做一个简单的例子(<em class="mr">警告:前方微积分！</em>)。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nt nu l"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">GIF via <a class="ae kv" href="https://media.giphy.com/media/l4FGni1RBAR2OWsGk/giphy.gif" rel="noopener ugc nofollow" target="_blank"> giphy </a></p></figure><p id="d450" class="pw-post-body-paragraph lv lw iq lx b ly ms jr ma mb mt ju md me mu mg mh mi mv mk ml mm mw mo mp mq ij bi translated">让我们研究一个简单的二次函数，由下式给出:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nv"><img src="../Images/5ddc118cb79745891db8896b88353e9e.png" data-original-src="https://miro.medium.com/v2/resize:fit:280/1*22dKCTjXsXKuxqY22thzjA.gif"/></div></figure><p id="bcb9" class="pw-post-body-paragraph lv lw iq lx b ly ms jr ma mb mt ju md me mu mg mh mi mv mk ml mm mw mo mp mq ij bi translated">它一阶和二阶导数是:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nw"><img src="../Images/c94e87f821a499fbb0d3a9bdf58f58fb.png" data-original-src="https://miro.medium.com/v2/resize:fit:468/1*1R99vybJW0N5NbXSIbqPCw.gif"/></div></figure><p id="a8dc" class="pw-post-body-paragraph lv lw iq lx b ly ms jr ma mb mt ju md me mu mg mh mi mv mk ml mm mw mo mp mq ij bi translated">因为二阶导数总是大于0，所以我们的函数是严格凸的。</p><p id="2d4e" class="pw-post-body-paragraph lv lw iq lx b ly ms jr ma mb mt ju md me mu mg mh mi mv mk ml mm mw mo mp mq ij bi translated">也可以使用<strong class="lx ir">准凸函数</strong>和梯度下降算法。然而，通常它们有所谓的<strong class="lx ir">鞍点</strong>(也称为极大极小点)，算法可能会在这里卡住(我们将在本文后面演示)。准凸函数的一个例子是:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/c83ea3aef89181735aa50ef09e5571f5.png" data-original-src="https://miro.medium.com/v2/resize:fit:312/1*QNe9PmM5oWAJzB9qaXAhrg.gif"/></div></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/df15719aa52cf8d45be679736ea5d151.png" data-original-src="https://miro.medium.com/v2/resize:fit:494/1*mRLyNnccAkn0kG80fi_OXw.gif"/></div></figure><p id="bf4b" class="pw-post-body-paragraph lv lw iq lx b ly ms jr ma mb mt ju md me mu mg mh mi mv mk ml mm mw mo mp mq ij bi translated">让我们在这里停一会儿。我们看到，在x=0和x=1.5时，一阶导数等于零。这些位置是函数极值(最小值或最大值)的候选位置，那里的斜率为零。但是首先我们要先检查二阶导数。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nz"><img src="../Images/dc822ab9293831a985303531757c6d7e.png" data-original-src="https://miro.medium.com/v2/resize:fit:538/1*A-J_4W3NhCskacOipTGpcA.gif"/></div></figure><p id="f60a" class="pw-post-body-paragraph lv lw iq lx b ly ms jr ma mb mt ju md me mu mg mh mi mv mk ml mm mw mo mp mq ij bi translated">对于<em class="mr"> x=0 </em>和<em class="mr"> x=1 </em>，该表达式的值为零。这些位置被称为拐点——曲率改变符号的地方——意味着它从凸变到凹，或者反之亦然。通过分析这个等式，我们得出结论:</p><ul class=""><li id="ba71" class="mx my iq lx b ly ms mb mt me mz mi na mm nb mq nc nd ne nf bi translated">对于x &lt;0: function is convex</li><li id="f11d" class="mx my iq lx b ly ng mb nh me ni mi nj mm nk mq nc nd ne nf bi translated">for 0<x function="" is="" concave="" derivative=""/></li><li id="3576" class="mx my iq lx b ly ng mb nh me ni mi nj mm nk mq nc nd ne nf bi translated">for x&gt; 1:函数再次凸</li></ul><p id="71a5" class="pw-post-body-paragraph lv lw iq lx b ly ms jr ma mb mt ju md me mu mg mh mi mv mk ml mm mw mo mp mq ij bi translated">现在我们看到点<em class="mr"> x=0 </em>的一阶和二阶导数都等于0，这意味着这是一个鞍点，而点x=1.5是一个全局最小值。</p><p id="4193" class="pw-post-body-paragraph lv lw iq lx b ly ms jr ma mb mt ju md me mu mg mh mi mv mk ml mm mw mo mp mq ij bi translated">让我们看看这个函数的图形。根据之前的计算，鞍点在<em class="mr"> x=0 </em>处，最小值在<em class="mr"> x=1.5 </em>处。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oa"><img src="../Images/7e0bff387f1efbc8cd7bb07c1ec0626d.png" data-original-src="https://miro.medium.com/v2/resize:fit:974/format:webp/1*JOP6J4wouRuMzla0pBLJbA.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">具有鞍点的半凸函数；作者图片</p></figure><p id="b215" class="pw-post-body-paragraph lv lw iq lx b ly ms jr ma mb mt ju md me mu mg mh mi mv mk ml mm mw mo mp mq ij bi translated">对于多元函数来说，检查一个点是否是鞍点最合适的方法是计算一个Hessian矩阵，这涉及到一些更复杂的计算，超出了本文的范围。</p><p id="5248" class="pw-post-body-paragraph lv lw iq lx b ly ms jr ma mb mt ju md me mu mg mh mi mv mk ml mm mw mo mp mq ij bi translated">二元函数中鞍点的例子如下所示。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ob"><img src="../Images/91f3b594f7d0f40cada067cf48551e66.png" data-original-src="https://miro.medium.com/v2/resize:fit:176/1*Hp5bGkPu6F3xbTwMXdLHtw.gif"/></div></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/ea03b828873d5a27bda069fcc1e3f2be.png" data-original-src="https://miro.medium.com/v2/resize:fit:1012/format:webp/1*NuFFldoihRtpC8A0LDuylQ.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">Nicoguaro，CC BY 3.0，通过维基共享</p></figure><h1 id="e9b7" class="ld le iq bd lf lg nl li lj lk nm lm ln jw nn jx lp jz no ka lr kc np kd lt lu bi translated">3.梯度</h1><p id="5930" class="pw-post-body-paragraph lv lw iq lx b ly lz jr ma mb mc ju md me mf mg mh mi mj mk ml mm mn mo mp mq ij bi translated">在开始编写代码之前，还有一件事必须解释一下——什么是梯度。直观上，它是曲线在指定方向上给定点的斜率。</p><p id="d8a6" class="pw-post-body-paragraph lv lw iq lx b ly ms jr ma mb mt ju md me mu mg mh mi mv mk ml mm mw mo mp mq ij bi translated">在<strong class="lx ir">为一元函数</strong>的情况下，它只是在选定点的<strong class="lx ir">一阶导数。在<strong class="lx ir">是多元函数</strong>的情况下，它是在每个主方向(沿着可变轴)上导数</strong>的<strong class="lx ir">向量。因为我们只对一个轴上的斜率感兴趣，而不关心其他的，所以这些导数叫做<strong class="lx ir">偏导数</strong>。</strong></p><p id="020b" class="pw-post-body-paragraph lv lw iq lx b ly ms jr ma mb mt ju md me mu mg mh mi mv mk ml mm mw mo mp mq ij bi translated">n维函数f(x)在给定点p的梯度定义如下:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi od"><img src="../Images/c68ae67d4065dfa35df1f56b3478c5bb.png" data-original-src="https://miro.medium.com/v2/resize:fit:294/1*duhfS4ufOrjtXJw_rHg_6Q.gif"/></div></figure><p id="4655" class="pw-post-body-paragraph lv lw iq lx b ly ms jr ma mb mt ju md me mu mg mh mi mv mk ml mm mw mo mp mq ij bi translated">倒三角是所谓的<em class="mr"> nabla </em>符号，你读它“del”。为了更好地理解如何计算，让我们手动计算一个示例性的二维函数。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oe"><img src="../Images/8907e94586823d8ee2cbe107533a01d1.png" data-original-src="https://miro.medium.com/v2/resize:fit:274/1*z-6cHPM1q_HtB4HrAXZ-FQ.gif"/></div></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi of"><img src="../Images/335ece453b6a00bc7a1e89a95586cb8c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fWc_cUNDEn4LpS6aissyXA.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">3D绘图；作者图片</p></figure><p id="d42d" class="pw-post-body-paragraph lv lw iq lx b ly ms jr ma mb mt ju md me mu mg mh mi mv mk ml mm mw mo mp mq ij bi translated">假设我们对点p(10，10)处的梯度感兴趣:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi og"><img src="../Images/7b496aeadcddf848ce27622b5edb0118.png" data-original-src="https://miro.medium.com/v2/resize:fit:462/1*HJAz03MhHAFI-l3AeLrSYg.gif"/></div></figure><p id="a1b5" class="pw-post-body-paragraph lv lw iq lx b ly ms jr ma mb mt ju md me mu mg mh mi mv mk ml mm mw mo mp mq ij bi translated">因此:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/b6d4a58629d6735e2a9377eaef3abc79.png" data-original-src="https://miro.medium.com/v2/resize:fit:264/1*MMZM3DhpwFmSr9mkc5fZXg.gif"/></div></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oi"><img src="../Images/ea44a9d1f5c93c1131b8ba46fef01d64.png" data-original-src="https://miro.medium.com/v2/resize:fit:282/1*4kVChbyOJL49FDICIiGPnQ.gif"/></div></figure><p id="c315" class="pw-post-body-paragraph lv lw iq lx b ly ms jr ma mb mt ju md me mu mg mh mi mv mk ml mm mw mo mp mq ij bi translated">通过查看这些值，我们得出结论，y轴上的斜率要陡两倍。</p><h1 id="f2c2" class="ld le iq bd lf lg nl li lj lk nm lm ln jw nn jx lp jz no ka lr kc np kd lt lu bi translated">4.梯度下降算法</h1><p id="c08a" class="pw-post-body-paragraph lv lw iq lx b ly lz jr ma mb mc ju md me mf mg mh mi mj mk ml mm mn mo mp mq ij bi translated">梯度下降算法使用当前位置的梯度迭代计算下一个点，缩放它(通过学习率)并从当前位置减去获得的值(进行一步)。它减去了这个值，因为我们想要最小化这个函数(最大化它就是相加)。这个过程可以写成:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/f1d9dfeec6c07df7b029158842d57e4d.png" data-original-src="https://miro.medium.com/v2/resize:fit:330/1*GixQ9i6cQSvlfoe_XZdcog.gif"/></div></figure><p id="f391" class="pw-post-body-paragraph lv lw iq lx b ly ms jr ma mb mt ju md me mu mg mh mi mv mk ml mm mw mo mp mq ij bi translated">有一个重要的参数<strong class="lx ir"> η </strong>，它缩放梯度，从而控制步长。在机器学习中，它被称为<strong class="lx ir">学习率</strong>，对性能有很大的影响。</p><ul class=""><li id="7f5b" class="mx my iq lx b ly ms mb mt me mz mi na mm nb mq nc nd ne nf bi translated">学习率越小，GD收敛的时间越长，或者在达到最优点之前可能达到最大迭代</li><li id="17db" class="mx my iq lx b ly ng mb nh me ni mi nj mm nk mq nc nd ne nf bi translated">如果学习率太大，算法可能不会收敛到最优点(跳跃)，甚至完全发散。</li></ul><p id="67f8" class="pw-post-body-paragraph lv lw iq lx b ly ms jr ma mb mt ju md me mu mg mh mi mv mk ml mm mw mo mp mq ij bi translated">综上所述，梯度下降法的步骤是:</p><ol class=""><li id="1c06" class="mx my iq lx b ly ms mb mt me mz mi na mm nb mq ok nd ne nf bi translated">选择一个起点(初始化)</li><li id="e97d" class="mx my iq lx b ly ng mb nh me ni mi nj mm nk mq ok nd ne nf bi translated">计算该点的梯度</li><li id="504f" class="mx my iq lx b ly ng mb nh me ni mi nj mm nk mq ok nd ne nf bi translated">在与梯度相反的方向上按比例步进(目标:最小化)</li><li id="840e" class="mx my iq lx b ly ng mb nh me ni mi nj mm nk mq ok nd ne nf bi translated">重复第2点和第3点，直到满足其中一个标准:</li></ol><ul class=""><li id="6753" class="mx my iq lx b ly ms mb mt me mz mi na mm nb mq nc nd ne nf bi translated">已达到最大迭代次数</li><li id="988a" class="mx my iq lx b ly ng mb nh me ni mi nj mm nk mq nc nd ne nf bi translated">步长小于公差(由于缩放或小梯度)。</li></ul><p id="29a9" class="pw-post-body-paragraph lv lw iq lx b ly ms jr ma mb mt ju md me mu mg mh mi mv mk ml mm mw mo mp mq ij bi translated">下面是梯度下降算法的示例性实现(具有步长跟踪):</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ol nu l"/></div></figure><p id="439a" class="pw-post-body-paragraph lv lw iq lx b ly ms jr ma mb mt ju md me mu mg mh mi mv mk ml mm mw mo mp mq ij bi translated">这个函数有5个参数:</p><p id="4010" class="pw-post-body-paragraph lv lw iq lx b ly ms jr ma mb mt ju md me mu mg mh mi mv mk ml mm mw mo mp mq ij bi translated">1.在我们的例子中，我们手动定义它，但是在实践中，它通常是一个随机的初始化</p><p id="1700" class="pw-post-body-paragraph lv lw iq lx b ly ms jr ma mb mt ju md me mu mg mh mi mv mk ml mm mw mo mp mq ij bi translated">2.<strong class="lx ir">梯度函数</strong> n -必须提前指定</p><p id="6479" class="pw-post-body-paragraph lv lw iq lx b ly ms jr ma mb mt ju md me mu mg mh mi mv mk ml mm mw mo mp mq ij bi translated">3.<strong class="lx ir">学习率</strong> -步长的比例因子</p><p id="a4ca" class="pw-post-body-paragraph lv lw iq lx b ly ms jr ma mb mt ju md me mu mg mh mi mv mk ml mm mw mo mp mq ij bi translated">4.最大迭代次数</p><p id="0176" class="pw-post-body-paragraph lv lw iq lx b ly ms jr ma mb mt ju md me mu mg mh mi mv mk ml mm mw mo mp mq ij bi translated">5.有条件停止算法的容差(在这种情况下，默认值为0.01)</p><h1 id="1240" class="ld le iq bd lf lg nl li lj lk nm lm ln jw nn jx lp jz no ka lr kc np kd lt lu bi translated">5.示例1 —二次函数</h1><p id="62b1" class="pw-post-body-paragraph lv lw iq lx b ly lz jr ma mb mc ju md me mf mg mh mi mj mk ml mm mn mo mp mq ij bi translated">让我们用一个简单的二次函数来定义:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi om"><img src="../Images/e709e881405fbd3a625c15205d6ee9bd.png" data-original-src="https://miro.medium.com/v2/resize:fit:296/1*759WQyYJhCOmTg9-bUyajw.gif"/></div></figure><p id="564d" class="pw-post-body-paragraph lv lw iq lx b ly ms jr ma mb mt ju md me mu mg mh mi mv mk ml mm mw mo mp mq ij bi translated">因为梯度函数是一元函数，所以它是:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi on"><img src="../Images/a5d43654dd013459eb8a64fce05fe257.png" data-original-src="https://miro.medium.com/v2/resize:fit:234/1*XdVinZof2X_-kKO0NzLT7g.gif"/></div></figure><p id="f954" class="pw-post-body-paragraph lv lw iq lx b ly ms jr ma mb mt ju md me mu mg mh mi mv mk ml mm mw mo mp mq ij bi translated">让我们用Python写这些函数:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ol nu l"/></div></figure><p id="e090" class="pw-post-body-paragraph lv lw iq lx b ly ms jr ma mb mt ju md me mu mg mh mi mv mk ml mm mw mo mp mq ij bi translated">对于这个函数，通过取0.1的学习率和x=9的起点，我们可以很容易地手工计算每一步。让我们开始前三步:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oo"><img src="../Images/9b0a15b8650a916c21362ea196dc5566.png" data-original-src="https://miro.medium.com/v2/resize:fit:592/1*l4ghz3vtdMDgc0XPFHwAZw.gif"/></div></figure><p id="39dd" class="pw-post-body-paragraph lv lw iq lx b ly ms jr ma mb mt ju md me mu mg mh mi mv mk ml mm mw mo mp mq ij bi translated">python函数由以下函数调用:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ol nu l"/></div></figure><p id="b0c1" class="pw-post-body-paragraph lv lw iq lx b ly ms jr ma mb mt ju md me mu mg mh mi mv mk ml mm mw mo mp mq ij bi translated">下面的动画显示了学习率为0.1和0.8时GD算法所采取的步骤。如你所见，对于较小的学习速率，随着算法接近最小值，步长逐渐变小。对于更大的学习速率，它是在收敛之前从一边跳到另一边。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi op"><img src="../Images/c863a36b31401e38542445671b08b930.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*v5bc1TzeMKpzTAgorjOoHQ.gif"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">GD为小学习率和大学习率采取的前10个步骤；作者图片</p></figure><p id="752f" class="pw-post-body-paragraph lv lw iq lx b ly ms jr ma mb mt ju md me mu mg mh mi mv mk ml mm mw mo mp mq ij bi translated">不同学习速率下的轨迹、迭代次数和最终收敛结果(在公差范围内)如下所示:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oq"><img src="../Images/a551f3afa620dec4e3eb4b35e305ad21.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GR914FuA4pVTTXEpVDJ2Ng.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">不同学习速度的结果；作者图片</p></figure><h1 id="1d94" class="ld le iq bd lf lg nl li lj lk nm lm ln jw nn jx lp jz no ka lr kc np kd lt lu bi translated"><strong class="ak"> 6。示例2 —具有鞍点的函数</strong></h1><p id="c010" class="pw-post-body-paragraph lv lw iq lx b ly lz jr ma mb mc ju md me mf mg mh mi mj mk ml mm mn mo mp mq ij bi translated">现在让我们看看这个算法将如何处理我们之前用数学方法研究过的半凸函数。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/bc093bf87b05b2a98abcd2130d303001.png" data-original-src="https://miro.medium.com/v2/resize:fit:312/1*o2y5n16SH8C_kuI0OCApJw.gif"/></div></figure><p id="7f96" class="pw-post-body-paragraph lv lw iq lx b ly ms jr ma mb mt ju md me mu mg mh mi mv mk ml mm mw mo mp mq ij bi translated">下面是两种学习速度和两种不同起点的结果。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi or"><img src="../Images/3edd57cadecf74ec18155e066047d97f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dA6jHOoKa-uNo1sJO2XogA.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">GD试图逃离一个鞍点；作者图片</p></figure><p id="54ed" class="pw-post-body-paragraph lv lw iq lx b ly ms jr ma mb mt ju md me mu mg mh mi mv mk ml mm mw mo mp mq ij bi translated">下面是一个学习率为0.4，起点为<em class="mr"> x=-0.5 </em>的动画。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi os"><img src="../Images/4dbfe5bd46abd95534e2c05652a64b7b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1080/1*Z3wjHCFAehYXYG9lOiqiEw.gif"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">GD试图逃离一个鞍点的动画；作者图片</p></figure><p id="a596" class="pw-post-body-paragraph lv lw iq lx b ly ms jr ma mb mt ju md me mu mg mh mi mv mk ml mm mw mo mp mq ij bi translated">现在你可以看到，鞍点的存在对GD等一阶梯度下降算法提出了真正的挑战，并且不能保证获得全局最小值。二阶算法更好地处理这些情况(例如，牛顿-拉夫森方法)。</p><p id="4c44" class="pw-post-body-paragraph lv lw iq lx b ly ms jr ma mb mt ju md me mu mg mh mi mv mk ml mm mw mo mp mq ij bi translated">鞍点的研究以及如何逃离它们是正在进行的研究的主题，并且提出了各种解决方案。例如，金池和m .乔丹提出了一种扰动梯度下降算法——你可以在<a class="ae kv" href="https://bair.berkeley.edu/blog/2017/08/31/saddle-efficiency/" rel="noopener ugc nofollow" target="_blank">他们的博客文章</a>中找到细节。</p><h1 id="40e7" class="ld le iq bd lf lg nl li lj lk nm lm ln jw nn jx lp jz no ka lr kc np kd lt lu bi translated">7.摘要</h1><p id="bd60" class="pw-post-body-paragraph lv lw iq lx b ly lz jr ma mb mc ju md me mf mg mh mi mj mk ml mm mn mo mp mq ij bi translated">在本文中，我们检查了梯度体面算法如何工作，何时可以使用它，以及使用它时有哪些常见的挑战。我希望这将是一个很好的起点，让你探索更先进的基于梯度的优化方法，如动量或内斯特罗夫(加速)梯度下降，RMSprop，ADAM或二阶方法，如牛顿-拉尔夫森算法。</p></div></div>    
</body>
</html>