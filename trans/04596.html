<html>
<head>
<title>Interpreting Semantic Text Similarity from Transformer Models</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">从变压器模型解释语义文本相似性</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/interpreting-semantic-text-similarity-from-transformer-models-ba1b08e6566c?source=collection_archive---------16-----------------------#2021-04-20">https://towardsdatascience.com/interpreting-semantic-text-similarity-from-transformer-models-ba1b08e6566c?source=collection_archive---------16-----------------------#2021-04-20</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="b3fd" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsdatascience.com/tagged/hands-on-tutorials" rel="noopener" target="_blank">实践教程</a></h2><div class=""/><div class=""><h2 id="16fa" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">我们能想象搜索所使用的上下文吗？</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/c5b67a4065b757941b4f5ece02ce4423.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*gipNSEQAswTllQ1p"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">照片由<a class="ae lh" href="https://unsplash.com/@hjkp?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">亨利·珀克斯</a>在<a class="ae lh" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>拍摄</p></figure><p id="313e" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">使用基于transformer的模型搜索文本文档非常棒；如今，使用<a class="ae lh" href="https://huggingface.co/" rel="noopener ugc nofollow" target="_blank"> huggingface </a>库很容易实现，而且结果往往令人印象深刻。最近，我想了解<em class="me">为什么</em>返回一个给定的结果——我最初的想法是各种论文和博客帖子，这些论文和博客帖子都与挖掘变形金刚内部的注意力机制有关，这似乎有点复杂。在这篇文章中，我测试了一个非常简单的方法，当使用一些简单的向量数学进行上下文搜索时，可以一瞥这些模型获得的上下文相似性。让我们试一试。</p><p id="735d" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">为了这篇文章的目的，我将使用来自<code class="fe mf mg mh mi b"><a class="ae lh" href="https://github.com/UKPLab/sentence-transformers" rel="noopener ugc nofollow" target="_blank">sentence-transformers</a></code>库的一个模型，它已经被专门优化用于进行语义文本相似性搜索。该模型实质上为传递给它的每个句子创建了1024维嵌入，然后可以通过相应的两个向量之间的余弦相似性来计算两个这样的句子之间的相似性。假设我们有两个问题<em class="me"> A </em>和<em class="me"> B </em>，分别嵌入到1024维向量<strong class="lk jd"> <em class="me"> A </em> </strong>和<strong class="lk jd"> <em class="me"> B </em> </strong>中，那么句子之间的余弦相似度计算如下:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi mj"><img src="../Images/89feeb4ebb9a9c65d8961da466f40b18.png" data-original-src="https://miro.medium.com/v2/resize:fit:272/1*L7CGRKfkKqxTLi57AWk6yA.gif"/></div></figure><p id="69e5" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">也就是说，余弦相似度为1意味着问题是相同的(角度为0)，余弦相似度为-1意味着问题非常不同。出于演示的目的，我嵌入了来自<a class="ae lh" href="https://allenai.org/data/arc-classification" rel="noopener ugc nofollow" target="_blank"> ARC问题分类数据集</a>的1700个问题。完整的笔记本可以在google colab <a class="ae lh" href="https://colab.research.google.com/drive/1XVR1BRbrpLQkMOo7fKHPz778OFMASWWl?usp=sharing" rel="noopener ugc nofollow" target="_blank">这里</a>找到。在下面的代码片段中可以看到进行句子嵌入的基本部分:</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="mk ml l"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">作者代码片段</p></figure><p id="e09a" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">这样，我们可以很容易地在我们的问题数据库中进行搜索；假设我们有一个包含1700个问题的数据库，我们使用上面的代码片段将其嵌入到一个1700×1024的矩阵中。第一步是L2归一化每一行——这实质上意味着我们归一化每个问题向量，使其长度为1，这简化了我们之前的等式，使得<strong class="lk jd"> <em class="me"> A </em> </strong>和<strong class="lk jd"> <em class="me"> B </em> </strong>之间的余弦相似度只是两个向量的点积。根据前面代码片段中创建的嵌入，我们可以假设数据集中的第一个问题是我们的查询，并尝试从其余问题中找到最匹配的条目:</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="mk ml l"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">作者代码片段</p></figure><p id="7be2" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">在我的样本数据集中，第一个问题(查询)是“<em class="me">哪个因素最有可能导致一个人发烧？</em>”而确定的最相似的问题是“<em class="me">哪个最能解释为什么一个被细菌感染的人可能会发烧？</em>”。这是很好的搭配🙌—两个句子都与一个发烧的人有关。然而，我们如何知道算法选择特定匹配的原因不仅仅是因为它们都以单词“Which”开头？</p><p id="4831" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">需要记住的是，通过设计，transformer模型实际上为我们的句子中的每个<em class="me">标记</em>输出了一个1024维向量——这些标记嵌入被平均汇集以生成我们的<em class="me">句子</em>嵌入。因此，为了获得关于用于在我们的搜索查询中找到匹配的上下文的更多信息，我们可以计算我们的查询中的每个标记和我们的搜索匹配之间的余弦距离，并绘制结果2D矩阵:</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="mk ml l"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">作者代码片段</p></figure><p id="d3bb" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">这导致了下面的图:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi mm"><img src="../Images/a73554d277203f81eaaa1b9625eae1f8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*M2FjLr6Pi6XW7z3ne5_oig.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">作者创作的情节</p></figure><p id="4615" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">现在我们可以看到查询中的每个标记和最佳搜索结果中的每个标记之间的余弦相似性。很明显，实际上“发烧”关键词被挑选出来，并且是导致搜索结果的“语义语境”的主要部分，然而，同样明显的是，有附加的成分进入语义语境，例如“开发a”和“具有a”与高余弦相似性分数相结合，并且关键词“人”也被挑选出来，而在两个句子中都存在的首词“which”不太重要。</p><p id="1eab" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">这种计算所有标记嵌入之间的余弦相似性的简单技术提供了对每个标记对最终相似性得分的贡献的洞察，因此是解释模型在返回给定搜索结果时正在做什么的快速方式。必须注意的是，在进行实际搜索时，我们在计算不同句子嵌入之间的余弦相似度之前，取所有标记嵌入的平均值，这与我们在这里所做的不同——即使如此，查看查询中的每个标记嵌入如何与最佳匹配中的标记嵌入对齐，可以深入了解是什么组成了用于语义搜索的那些句子嵌入。</p></div></div>    
</body>
</html>