<html>
<head>
<title>A Comprehensive Python Implementation of GloVe</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">GloVe的全面Python实现</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/a-comprehensive-python-implementation-of-glove-c94257c2813d?source=collection_archive---------5-----------------------#2021-04-07">https://towardsdatascience.com/a-comprehensive-python-implementation-of-glove-c94257c2813d?source=collection_archive---------5-----------------------#2021-04-07</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="0d32" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">在单台机器上训练完整的手套模型</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/6002aeea9cdd37c9be9f58cdf0ce31f1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*YLUgNHRIQMqcdiJT"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">照片由<a class="ae kv" href="https://unsplash.com/@cookiethepom?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Cookie在<a class="ae kv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的Pom </a>拍摄</p></figure><p id="fa88" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">作为一名NLP数据科学家，我经常阅读各种主题的论文，包括单词向量、rnn和变压器。看论文很好玩，给我一种掌握了广泛技术的错觉。但是在复制它们的时候，困难出现了。据我所知，很多NLP学习者都碰到过和我一样的情况。因此，我决定开始一系列的文章，集中讨论如何实现经典的NLP论文。为此，我还创建了GitHub库T4。</p><p id="cdb7" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">本帖是本系列的第一篇，根据原纸再现手套模型。如前所述，重点纯粹在于实现。关于底层理论的更多信息，请参考<a class="ae kv" href="https://nlp.stanford.edu/pubs/glove.pdf" rel="noopener ugc nofollow" target="_blank">原文</a>。</p><p id="c002" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">根据这篇论文，手套模型是用单台机器训练的。<a class="ae kv" href="https://github.com/stanfordnlp/GloVe" rel="noopener ugc nofollow" target="_blank">发布的代码</a>是用C写的，对于NLP学习者来说可能有些陌生。因此，我对该模型进行了全面的Python实现，这符合只用一台机器训练大量词汇的目标。下面几节将逐步介绍实现细节。完整代码是<a class="ae kv" href="https://github.com/pengyan510/nlp-paper-implementation/tree/master/glove" rel="noopener ugc nofollow" target="_blank">这里是</a>。</p><h1 id="2f84" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">第0步:准备</h1><h2 id="e81b" class="mk lt iq bd lu ml mm dn ly mn mo dp mc lf mp mq me lj mr ms mg ln mt mu mi mv bi translated">培训用数据</h2><p id="6771" class="pw-post-body-paragraph kw kx iq ky b kz mw jr lb lc mx ju le lf my lh li lj mz ll lm ln na lp lq lr ij bi translated">对于这个项目，我使用<a class="ae kv" href="http://mattmahoney.net/dc/textdata.html" rel="noopener ugc nofollow" target="_blank"> Text8数据集</a>作为训练数据。要获得它，我们可以使用gensim下载器:</p><pre class="kg kh ki kj gt nb nc nd ne aw nf bi"><span id="037c" class="mk lt iq nc b gy ng nh l ni nj">import gensim.downloader as api</span><span id="aacf" class="mk lt iq nc b gy nk nh l ni nj">dataset = api.load("text8")</span></pre><p id="1bf2" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">数据集是一个列表的列表，其中每个子列表是一个代表一个句子的单词列表。我们只想要所有单词的列表，所以用itertools将其展平:</p><pre class="kg kh ki kj gt nb nc nd ne aw nf bi"><span id="dcf1" class="mk lt iq nc b gy ng nh l ni nj">import itertools</span><span id="38ae" class="mk lt iq nc b gy nk nh l ni nj">corpus = list(itertools.chain.from_iterable(dataset))</span></pre><p id="6ca6" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">好了，现在我们有了训练语料。</p><h2 id="ad1e" class="mk lt iq bd lu ml mm dn ly mn mo dp mc lf mp mq me lj mr ms mg ln mt mu mi mv bi translated">存储参数</h2><p id="d3d7" class="pw-post-body-paragraph kw kx iq ky b kz mw jr lb lc mx ju le lf my lh li lj mz ll lm ln na lp lq lr ij bi translated">在处理机器学习模型时，总是有大量的参数需要配置，如数据文件路径、批量大小、单词嵌入大小等。如果管理不好，这些参数会产生大量开销。根据我的经验，我发现最好的方法是将它们都存储在一个名为config.yaml的yaml文件中。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nl nm l"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">从config.yaml加载配置的代码段</p></figure><p id="1fab" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">然后对于代码的其余部分，我们可以使用参数config.batch_size、config.learning_rate来代替硬编码的值，这也使代码更好。</p><p id="46cc" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这就是所有需要的准备工作。让我们开始手套模型的实际两步训练吧！</p><h1 id="c0b0" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">步骤1:计算共现配对</h1><h2 id="faa8" class="mk lt iq bd lu ml mm dn ly mn mo dp mc lf mp mq me lj mr ms mg ln mt mu mi mv bi translated">创造词汇</h2><p id="5c48" class="pw-post-body-paragraph kw kx iq ky b kz mw jr lb lc mx ju le lf my lh li lj mz ll lm ln na lp lq lr ij bi translated">为了统计共现对，我们首先需要确定词汇。以下是对词汇表的一些要求:</p><ul class=""><li id="60dc" class="nn no iq ky b kz la lc ld lf np lj nq ln nr lr ns nt nu nv bi translated">它是出现在语料库中的一组标记。</li><li id="5289" class="nn no iq ky b kz nw lc nx lf ny lj nz ln oa lr ns nt nu nv bi translated">每个令牌都映射到一个整数。</li><li id="fcde" class="nn no iq ky b kz nw lc nx lf ny lj nz ln oa lr ns nt nu nv bi translated">如果一个标记不属于语料库，它应该被表示为一个未知标记，或“unk”。</li><li id="6d19" class="nn no iq ky b kz nw lc nx lf ny lj nz ln oa lr ns nt nu nv bi translated">为了对共现对进行计数，只需要记号的子集，例如前k个最频繁的记号。</li></ul><p id="314a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">为了以结构化的方式满足这些需求，创建了一个词汇表类。该类有四个字段:</p><ul class=""><li id="05b1" class="nn no iq ky b kz la lc ld lf np lj nq ln nr lr ns nt nu nv bi translated"><strong class="ky ir"> token2index: </strong>将令牌映射到索引的字典。索引从0开始，每次添加一个以前看不到的令牌时递增1。</li><li id="5f14" class="nn no iq ky b kz nw lc nx lf ny lj nz ln oa lr ns nt nu nv bi translated"><strong class="ky ir"> index2token: </strong>将索引映射到令牌的字典。</li><li id="2beb" class="nn no iq ky b kz nw lc nx lf ny lj nz ln oa lr ns nt nu nv bi translated"><strong class="ky ir"> token_counts: </strong>一个列表，其中<em class="ob"> i </em> th值是索引为<em class="ob"> i. </em>的令牌的计数</li><li id="a7af" class="nn no iq ky b kz nw lc nx lf ny lj nz ln oa lr ns nt nu nv bi translated"><strong class="ky ir"> _unk_token: </strong>一个整数，用作未知令牌的索引。默认值为-1。</li></ul><p id="e404" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">它还定义了以下方法:</p><ul class=""><li id="620a" class="nn no iq ky b kz la lc ld lf np lj nq ln nr lr ns nt nu nv bi translated"><strong class="ky ir"> add(token): </strong>向词汇表中添加一个新的token。如果以前未见过，则会生成一个新的索引。令牌的计数也会更新。</li><li id="ce7c" class="nn no iq ky b kz nw lc nx lf ny lj nz ln oa lr ns nt nu nv bi translated"><strong class="ky ir"> get_index(token): </strong>返回令牌的索引。</li><li id="a07a" class="nn no iq ky b kz nw lc nx lf ny lj nz ln oa lr ns nt nu nv bi translated"><strong class="ky ir"> get_token(index): </strong>返回索引对应的令牌。</li><li id="5daa" class="nn no iq ky b kz nw lc nx lf ny lj nz ln oa lr ns nt nu nv bi translated"><strong class="ky ir"> get_topk_subset(k): </strong>用前k个最频繁出现的标记创建一个新词汇。</li><li id="47d8" class="nn no iq ky b kz nw lc nx lf ny lj nz ln oa lr ns nt nu nv bi translated"><strong class="ky ir"> shuffle(): </strong>随机打乱所有的记号，使得记号和索引之间的映射被随机化。需要这种方法的原因将在我们实际计算共现配对时揭示。</li></ul><p id="acf4" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">了解了这一点，我们现在可以看看代码:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nl nm l"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">词汇课</p></figure><p id="e1c3" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">对于类的实现，我利用了Python的dataclass特性。有了这个特性，我只需要定义带类型注释的字段，就会自动为我生成__init__()方法。我还可以在定义字段时为它们设置默认值。例如，通过设置default_factory=dict，token2index默认为一个空dict。有关dataclass的更多信息，请参考官方文档。</p><p id="b51b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在我们有了词汇课，剩下的问题是:我们如何使用它？基本上有两种用例:</p><ul class=""><li id="2cf6" class="nn no iq ky b kz la lc ld lf np lj nq ln nr lr ns nt nu nv bi translated">从语料库中创建一个词汇表，该词汇表由前k个最频繁出现的标记组成。</li><li id="25c7" class="nn no iq ky b kz nw lc nx lf ny lj nz ln oa lr ns nt nu nv bi translated">在对共现对进行计数时，使用创建的词汇表将语料库(一个标记列表)转换为整数索引。</li></ul><p id="cc3b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我创建了另一个类，矢量器，来协调这两个用例。它只有一个字段，<strong class="ky ir"> vocab </strong>，指的是从语料库中创建的词汇。它有两种方法:</p><ul class=""><li id="86f9" class="nn no iq ky b kz la lc ld lf np lj nq ln nr lr ns nt nu nv bi translated"><strong class="ky ir"> from_corpus(corpus，vocab_size): </strong>这是一个类方法。首先，通过添加语料库中的所有标记来创建词汇表。然后，选择顶部最频繁出现的单词来创建新词汇。这个词汇表被打乱并用于实例化矢量器实例。洗牌的原因后面会解释。</li><li id="5cac" class="nn no iq ky b kz nw lc nx lf ny lj nz ln oa lr ns nt nu nv bi translated"><strong class="ky ir">向量化(语料库):</strong>将给定的语料库(一列标记)转换成一列索引。</li></ul><p id="541e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">完整代码如下:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nl nm l"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">矢量器类</p></figure><h2 id="c2a6" class="mk lt iq bd lu ml mm dn ly mn mo dp mc lf mp mq me lj mr ms mg ln mt mu mi mv bi translated">扫描上下文窗口</h2><p id="0177" class="pw-post-body-paragraph kw kx iq ky b kz mw jr lb lc mx ju le lf my lh li lj mz ll lm ln na lp lq lr ij bi translated">现在我们有了将所有单词转换成索引的矢量器，剩下的任务是扫描所有的上下文窗口并计算所有可能的共现对。因为共生矩阵是稀疏的，所以使用计数器来计数配对是合理的。关键是(单词<em class="ob"> i </em>的索引，单词<em class="ob"> j </em>的索引)，其中单词<em class="ob"> j </em>出现在单词<em class="ob"> i </em>的上下文中。该值是表示计数的浮点数。但是，如果使用这种策略，可能会出现两个问题。</p><p id="16b6" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">问题1:如果我们在一次扫描中计算所有同时出现的对，我们可能会耗尽内存，因为不同的数量(单词<em class="ob"> i </em>的索引，单词<em class="ob"> j </em>的索引)可能非常大。</strong></p><p id="dc9b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">解决方案:</strong>相反，我们可以在多次扫描中计数同时出现的配对。在每次扫描中，我们将单词<em class="ob"> i </em>的索引限制在一个小范围内，从而大大减少了不同对的数量。假设词汇表有100，000个不同的标记。如果我们在一次扫描中计数所有的对，不同对的数量可以多达10个⁰.相反，我们可以在10次扫描中计数所有对。在第一次扫描中，我们将word <em class="ob"> i </em>的索引限制在0到9999之间；在第二次扫描中，我们将其限制在10000和19999之间；在第三次扫描中，我们将其限制在20000到29999之间，以此类推。每次扫描完成后，我们将计数保存到磁盘。现在，在每次扫描中，不同对的数量可以和10⁹一样大，是原始数量的十分之一。</p><p id="e783" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这种方法背后的思想是，我们不是在一次扫描中计算整个共生矩阵，而是将矩阵分成10个更小的矩形，然后依次计算它们。下图形象化了这个想法。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oc"><img src="../Images/86d5de1f5a5dcc6ab9fbae50093b4c4e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NoBlq_1X-Nzg1vEjQTMmag.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">左:一次扫描计数|右:多次扫描计数</p></figure><p id="d9e9" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这种方法是可扩展的，因为随着词汇表大小的增加，我们总是可以增加扫描次数来减少内存使用。主要缺点是，如果使用单台机器，运行时间也会增加。然而，由于扫描之间没有依赖关系，因此可以使用Spark轻松实现并行化。但是这超出了我们的范围。</p><p id="7301" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">同样，在这一点上，洗牌的原因可以被揭开。当我们创建具有最频繁记号的词汇表时，这些记号的索引是有序的。索引0对应最频繁的令牌，索引1对应第二频繁的令牌，等等。如果我们继续100，000个令牌的示例，在第一次扫描中，我们将计数10000个最频繁的令牌对，不同对的数量将是巨大的。而在剩余的扫描中，不同对的数量会小得多。这导致扫描之间的内存使用不平衡。通过改变词汇表，不同的词汇对在扫描中均匀分布，并且平衡了内存使用。</p><p id="ac78" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">问题2:从问题1的解决方案继续，我们如何将每次扫描的计数保存到磁盘？最明显的方法是在扫描之间将(单词<em class="ob"> i </em>的索引，单词<em class="ob"> j </em>的索引，计数)三元组写入一个共享的文本文件。但是以后使用这个文件进行训练会涉及太多的开销。</strong></p><p id="4ea6" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">解决方案:</strong>有一个python库，h5py，提供了到HDF5二进制格式的python接口。它使您能够存储大量的数字数据，并轻松地操作它们，就像它们是真正的NumPy数组一样。关于这个库的更多细节，请查看它的<a class="ae kv" href="https://docs.h5py.org/en/stable/" rel="noopener ugc nofollow" target="_blank">文档</a>。</p><p id="faa1" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">和以前一样，我创建了一个CooccurrenceEntries类，它使用建议的解决方案进行计数并将结果保存到磁盘。该类有两个字段:</p><ul class=""><li id="6c73" class="nn no iq ky b kz la lc ld lf np lj nq ln nr lr ns nt nu nv bi translated"><strong class="ky ir">矢量器:</strong>从语料库中创建的矢量器实例。</li><li id="7cd5" class="nn no iq ky b kz nw lc nx lf ny lj nz ln oa lr ns nt nu nv bi translated"><strong class="ky ir">矢量化_语料库:</strong>单词索引列表。这是使用矢量器对原始语料库(单词列表)进行矢量化的结果。</li></ul><p id="e1db" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">它有两种主要方法:</p><ul class=""><li id="042b" class="nn no iq ky b kz la lc ld lf np lj nq ln nr lr ns nt nu nv bi translated"><strong class="ky ir"> setup(corpus，vectorizer): </strong>这是一个用于创建CooccurrenceEntries实例的类方法。通过在语料库上调用矢量器的矢量化方法来生成矢量化_语料库。</li><li id="64d8" class="nn no iq ky b kz nw lc nx lf ny lj nz ln oa lr ns nt nu nv bi translated"><strong class="ky ir"> build(window_size，num_partitions，chunk_size，output_directory= " . "):</strong>该方法统计num_partitions扫描中同时出现的对，并将结果写入输出目录。chunk_size参数用于使用HDF5格式将数据保存在块中。分块保存的原因将在模型训练部分讨论。简而言之，它用于更快地生成训练批次。</li></ul><p id="025f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">实现如下:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nl nm l"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">CooccurrenceEntrie class</p></figure></div><div class="ab cl od oe hu of" role="separator"><span class="og bw bk oh oi oj"/><span class="og bw bk oh oi oj"/><span class="og bw bk oh oi"/></div><div class="ij ik il im in"><p id="9beb" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">通过抽象词汇、矢量器、共现条目，计算共现对并保存到磁盘的代码很简单:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nl nm l"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">用于创建培训数据的代码段</p></figure><h1 id="eb78" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">第二步。训练手套模型</h1><h2 id="a667" class="mk lt iq bd lu ml mm dn ly mn mo dp mc lf mp mq me lj mr ms mg ln mt mu mi mv bi translated">从HDF5数据集加载批次</h2><p id="e1f2" class="pw-post-body-paragraph kw kx iq ky b kz mw jr lb lc mx ju le lf my lh li lj mz ll lm ln na lp lq lr ij bi translated">我们首先需要批量加载HDF5数据集中的数据。因为数据可以像存储在NumPy矩阵中一样被检索，所以最简单的方法是使用PyTorch数据加载器。但是加载每一批都涉及到许多dataset[i]形式的调用，其中dataset是一个h5py。数据集实例。这涉及到许多IO调用，可能会非常慢。</p><p id="998b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">解决方法是加载h5py。数据集逐块存储到内存中。每个加载的块都是内存中的一个纯NumPy ndarray，所以我们可以使用PyTorch的Dataloader对它进行批处理迭代。现在，所需的IO调用数量等于块的数量，这要小得多。</p><p id="d52c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这种方法的一个缺点是完全随机的混洗是不可能的，因为包含来自不同区块的数据的批将永远不会生成。因此，为了获得更大的随机性，我们可以以随机顺序加载块，并将DataLoader的shuffle参数设置为True。</p><p id="2609" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">创建HDF5DataLoader类是为了加载批处理。它有五个字段:</p><ul class=""><li id="4d07" class="nn no iq ky b kz la lc ld lf np lj nq ln nr lr ns nt nu nv bi translated"><strong class="ky ir">file path:</strong>HD F5文件的路径。</li><li id="e3ad" class="nn no iq ky b kz nw lc nx lf ny lj nz ln oa lr ns nt nu nv bi translated"><strong class="ky ir">数据集_名称:</strong>h5py的名称。文件中的数据集。</li><li id="d730" class="nn no iq ky b kz nw lc nx lf ny lj nz ln oa lr ns nt nu nv bi translated"><strong class="ky ir">批量大小:</strong>训练批量大小。</li><li id="3f77" class="nn no iq ky b kz nw lc nx lf ny lj nz ln oa lr ns nt nu nv bi translated"><strong class="ky ir">设备:</strong>训练设备，可以是cpu或gpu。</li><li id="6407" class="nn no iq ky b kz nw lc nx lf ny lj nz ln oa lr ns nt nu nv bi translated"><strong class="ky ir">数据集:</strong>h5py。文件中的数据集实例。</li></ul><p id="abf3" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">它有两种方法:</p><ul class=""><li id="c6d5" class="nn no iq ky b kz la lc ld lf np lj nq ln nr lr ns nt nu nv bi translated"><strong class="ky ir"> open(): </strong>这个方法打开HDF5文件并定位数据集。实际的阅读并不发生在这里。</li><li id="24d2" class="nn no iq ky b kz nw lc nx lf ny lj nz ln oa lr ns nt nu nv bi translated"><strong class="ky ir"> iter_batches(): </strong>该方法以随机顺序加载块，并创建PyTorch数据加载器来迭代块中的批。</li></ul><p id="aea9" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">代码如下所示。需要注意的一点是，CooccurrenceDataset只是PyTorch数据集的一个子类，用于索引数据。因为没什么特别的，所以省略了。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nl nm l"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">HDF5DataLoader类</p></figure><h2 id="9007" class="mk lt iq bd lu ml mm dn ly mn mo dp mc lf mp mq me lj mr ms mg ln mt mu mi mv bi translated">编码手套模型</h2><p id="a6b2" class="pw-post-body-paragraph kw kx iq ky b kz mw jr lb lc mx ju le lf my lh li lj mz ll lm ln na lp lq lr ij bi translated">用PyTorch实现手套模型很简单。我们在__init__()中定义了两个权重矩阵和两个偏置向量。注意，我们在创建嵌入时设置sparse=True，因为梯度更新本质上是稀疏的。在forward()中，返回平均批次损失。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nl nm l"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">手套班</p></figure><h2 id="73d3" class="mk lt iq bd lu ml mm dn ly mn mo dp mc lf mp mq me lj mr ms mg ln mt mu mi mv bi translated">训练手套模型</h2><p id="ff3b" class="pw-post-body-paragraph kw kx iq ky b kz mw jr lb lc mx ju le lf my lh li lj mz ll lm ln na lp lq lr ij bi translated">模型训练遵循标准PyTorch训练程序。唯一不同的是，我们使用定制的HDF5Loader来生成批处理，而不是PyTorch的DataLoader。以下是培训代码:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nl nm l"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">模型定型的代码段</p></figure></div><div class="ab cl od oe hu of" role="separator"><span class="og bw bk oh oi oj"/><span class="og bw bk oh oi oj"/><span class="og bw bk oh oi"/></div><div class="ij ik il im in"><p id="da92" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">唷，我们已经完成了完整的实现。恭喜你！</p><p id="b666" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">接下来，我们来训练模型，看看结果！</p><h1 id="82e1" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">第三步。结果</h1><p id="d477" class="pw-post-body-paragraph kw kx iq ky b kz mw jr lb lc mx ju le lf my lh li lj mz ll lm ln na lp lq lr ij bi translated">对于Text8数据集，训练一个历元大约需要80分钟。我为这个模型训练了20个纪元，花了不止一天的时间来完成。学习曲线看起来很有希望，如果培训继续下去，损失似乎会进一步减少。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ok"><img src="../Images/06a44aaea86b0a180da204ecb3ba7e8e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*JZBL0HY67nJg4PlNw0Qe3Q.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">学习曲线图</p></figure><p id="ee1c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们也可以做一些单词相似度的任务，看看单词向量是如何表现的。在这里，我使用了gensim的KeyedVectors类，它允许您在不编写最近邻或余弦相似性代码的情况下做到这一点。这里的相似度评价码是<a class="ae kv" href="https://github.com/pengyan510/nlp-paper-implementation/blob/master/glove/src/evaluate.py" rel="noopener ugc nofollow" target="_blank"/>。有关KeyedVectors的详细信息，请参考<a class="ae kv" href="https://radimrehurek.com/gensim/models/keyedvectors.html#what-can-i-do-with-word-vectors" rel="noopener ugc nofollow" target="_blank">文档</a>。</p><p id="d234" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">运行一些简单的相似性任务会显示以下结果:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ol"><img src="../Images/911938a127a4f13b5fb1d4aa3fd730b8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LCA8OAv8d9tLCjTqD2S07w.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">一些简单相似性任务的结果</p></figure><p id="1026" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">正如我们所看到的，其中一些是有意义的，如“计算机”和“游戏”，“联合”和“国家”；有些人没有。但这足以说明问题。在更大的数据集上进行更多时期的训练应该会改善结果。</p><h1 id="eeb2" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">摘要</h1><p id="8764" class="pw-post-body-paragraph kw kx iq ky b kz mw jr lb lc mx ju le lf my lh li lj mz ll lm ln na lp lq lr ij bi translated">手套纸写得很好，很好理解。然而，当谈到实现时，一路上有许多陷阱和困难，尤其是当您考虑到内存问题时。经过相当大的努力，我们最终得到了一个令人满意的在单台机器上训练的解决方案。</p><p id="626b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">正如我在开始时所说，我将继续实施更多的NLP文件，并与您分享我的第一手经验。希望你喜欢手套的实现，并在下一个帖子中看到你！</p></div></div>    
</body>
</html>