<html>
<head>
<title>Clustering sentence embeddings to identify intents in short text</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">聚类句子嵌入识别短文本意图</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/clustering-sentence-embeddings-to-identify-intents-in-short-text-48d22d3bf02e?source=collection_archive---------0-----------------------#2021-10-19">https://towardsdatascience.com/clustering-sentence-embeddings-to-identify-intents-in-short-text-48d22d3bf02e?source=collection_archive---------0-----------------------#2021-10-19</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="552a" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsdatascience.com/tagged/notes-from-industry" rel="noopener" target="_blank">行业笔记</a></h2><div class=""/><div class=""><h2 id="bdc6" class="pw-subtitle-paragraph jw iz iq bd b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn dk translated">UMAP + HDBSCAN的超参数调整以确定未标记文本数据中的聚类数</h2></div><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ko"><img src="../Images/e672f216688d6a59db4db8b96c5ac04a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*JyN1hyX28oxaA8Q8"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">由<a class="ae le" href="https://unsplash.com/@m15ky?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">迈克·提尼翁</a>在<a class="ae le" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="83d8" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated"><strong class="lh ja"><em class="mb">TL；DR </em> </strong> <em class="mb">聚类短文本消息的无监督学习问题可以转化为约束优化问题来自动调优UMAP + HDBSCAN超参数。</em><a class="ae le" href="https://github.com/dborrelli/chat-intents" rel="noopener ugc nofollow" target="_blank"><em class="mb">chat intents</em></a><em class="mb">包使得这个调优过程很容易实现。</em></p><h1 id="93d1" class="mc md iq bd me mf mg mh mi mj mk ml mm kf mn kg mo ki mp kj mq kl mr km ms mt bi translated">介绍</h1><p id="18bb" class="pw-post-body-paragraph lf lg iq lh b li mu ka lk ll mv kd ln lo mw lq lr ls mx lu lv lw my ly lz ma ij bi translated">用户对话交互可以成为如何改进产品或服务的巨大信息源。理解人们为什么联系客服也是自动化部分或全部回复的重要的第一步(例如，使用聊天机器人)。有几种方法来分析对话交互数据以提取有用的见解，并且通常通过讨论的主题、情感和意图来表征交互。</p><p id="8a85" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">从想要改进产品或服务的角度来看，确定意图特别有用，因为它回答了这样一个问题:为什么人们首先伸出手来？然而，利用用户消息意图的一个主要障碍是，确定它通常被视为一个分类问题。这意味着您通常需要已经有了大量的标记数据才能开始。例如，<a class="ae le" href="https://docs.microsoft.com/en-us/azure/cognitive-services/luis/luis-concept-intent" rel="noopener ugc nofollow" target="_blank">微软的路易斯</a>和<a class="ae le" href="https://cloud.google.com/dialogflow/es/docs/intents-overview" rel="noopener ugc nofollow" target="_blank">谷歌的Dialogflow </a>都是从这样一个前提开始的，你要么可以使用预先构建的域标记数据，要么你已经有了标记数据。</p><p id="619f" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">但是，如果您还没有任何带标签的数据，并且您认为任何公开可用的带标签的数据都不相关(这是经常发生的情况)，该怎么办呢？除了无监督学习问题的挑战之外，包含意图的消息通常非常短(少于15个单词)。我最近接受了这个挑战，还有一个额外的障碍:我们总共只有大约1000个样本。我立刻想起了几年前看到的一些明智的建议:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi mz"><img src="../Images/caba6eda7dee36868143e4127ddf86e9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Z1wQZis8xH2UOKarFi42PA.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">来源:https://twitter.com/RichardSocher/status/840333380130553856</p></figure><p id="91c6" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">在对我的特殊问题思考了一会儿，并尝试了几次把它当作一个无监督的学习问题，但毫无结果之后，我最终手动标记了这些数据(大约花了一周……)。手动标注让我对数据有了有益的理解和直觉。但与此同时，这让我非常好奇，想知道是否有一种方法可以更自动化地实现那些被标记的意图。这篇文章将提供一种我学到的方法，可以自动对短文本消息数据进行聚类，以识别和提取意图。</p><h1 id="f884" class="mc md iq bd me mf mg mh mi mj mk ml mm kf mn kg mo ki mp kj mq kl mr km ms mt bi translated">定义目标</h1><p id="afae" class="pw-post-body-paragraph lf lg iq lh b li mu ka lk ll mv kd ln lo mw lq lr ls mx lu lv lw my ly lz ma ij bi translated">在我们进一步讨论之前，让我们先定义一下我们要做什么。在这里我有兴趣回答这个问题:</p><blockquote class="na nb nc"><p id="4d5e" class="lf lg mb lh b li lj ka lk ll lm kd ln nd lp lq lr ne lt lu lv nf lx ly lz ma ij bi translated">给定用户和公司代表之间的一组未标记的对话，有没有办法自动获得用户意图的有用标记？</p></blockquote><p id="4547" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">由于这是一个无人监管的问题，标记意图可能相当主观，我不指望能够找到一个完美的解决方案。但是，类似于<a class="ae le" href="https://github.com/pandas-profiling/pandas-profiling" rel="noopener ugc nofollow" target="_blank"> auto-EDA库</a>并不详尽，但在面对新数据时可以提供一个有用的起点，我们可以在进行耗时的手动标记之前做些什么来提供初步的见解吗？自动化的结果可能已经足够好了，使人们从一周或更长时间的手动标记数据中节省下来。或者，它可以通过提供一个有用的起点来加速标记过程。</p><h1 id="6051" class="mc md iq bd me mf mg mh mi mj mk ml mm kf mn kg mo ki mp kj mq kl mr km ms mt bi translated">数据</h1><p id="4d3e" class="pw-post-body-paragraph lf lg iq lh b li mu ka lk ll mv kd ln lo mw lq lr ls mx lu lv lw my ly lz ma ij bi translated">显然，我无法分享激发本文灵感的原始数据集，所以我开始尽可能地寻找公开可用的类似资料。虽然有几个对话数据集已经标记了意图，但其中许多的主要限制是所表示的意图数量很少(通常约10个)。拥有少量的意图或类会使问题过于简单。尽管手工标注过程本身是主观的，但我发现对于我正在处理的数据来说，很容易有超过50种不同的意图。对于现实世界的应用程序来说，这似乎很常见。</p><p id="2d32" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">幸运的是，PolyAI团队发布了<a class="ae le" href="https://github.com/PolyAI-LDN/task-specific-datasets" rel="noopener ugc nofollow" target="_blank"> banking77数据集</a>，其中包含了77种不平等表示的意图:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ng"><img src="../Images/4eaf37ffb2eb470a982c465e3a023e3c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JUYy3iOL9VUEmAPxFL3k1w.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">来自bank77数据集的样本数据。图片由作者提供。</p></figure><p id="9136" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">完整数据集包含77个意图的训练数据集中的10，0003条消息。最大和最小类别计数分别为187和35。</p><p id="3819" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">为了更接近我之前面临的挑战，我将从训练集中的10，000个样本中随机抽取1，000个样本:</p><pre class="kp kq kr ks gt nh ni nj nk aw nl bi"><span id="a974" class="nm md iq ni b gy nn no l np nq">data_sample = data_full.sample(1000, random_state=123)</span></pre><p id="0f29" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">请注意，尽管该数据集对于本练习的演示目的很有用，但这仍然有些人为，并且您将在真实环境中面临额外的挑战。例如，你首先需要确定在一个完整的对话序列中什么信息或句子实际上与意图相关，以及处理随机的系统错误信息、打字错误和无意义的信息。<a class="ae le" href="https://medium.com/airbnb-engineering/discovering-and-classifying-in-app-message-intent-at-airbnb-6a55f5400a0c" rel="noopener">这篇关于发现和分类AirBnB信息意图的文章</a>触及了一些现实世界的挑战。</p><h1 id="f764" class="mc md iq bd me mf mg mh mi mj mk ml mm kf mn kg mo ki mp kj mq kl mr km ms mt bi translated">尝试主题建模</h1><p id="4d8c" class="pw-post-body-paragraph lf lg iq lh b li mu ka lk ll mv kd ln lo mw lq lr ls mx lu lv lw my ly lz ma ij bi translated">有几种方法可以解决像这样的无监督学习问题。<a class="ae le" href="https://en.wikipedia.org/wiki/Topic_model" rel="noopener ugc nofollow" target="_blank">面对这个问题，第一个想到的方法就是主题建模</a>。这是一种用来发现文档集合中潜在主题的技术。</p><p id="15c6" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">可以使用许多算法来执行主题建模，但一个非常常见的算法是<a class="ae le" href="https://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf" rel="noopener ugc nofollow" target="_blank">潜在狄利克雷分配(LDA) </a>。LDA是一种生成概率模型，它假设每个文档由固定数量的主题分布组成，每个主题由单词分布组成。当尝试使用LDA(和许多其他主题建模算法)时，一个很大的挑战是决定实际使用多少主题，这是一个必要的模型超参数。显然，如果这是我们希望从分析中得到的，那么这就是一个问题。<a class="ae le" href="http://svn.aksw.org/papers/2015/WSDM_Topic_Evaluation/public.pdf" rel="noopener ugc nofollow" target="_blank">连贯性</a>是一种通过测量每个主题中单词的相似程度来评估所学主题质量的方法，连贯性分数越高越好。<a class="ae le" href="https://radimrehurek.com/gensim/index.html" rel="noopener ugc nofollow" target="_blank"> Gensim </a>，一个非常流行的主题建模库，让计算<a class="ae le" href="https://radimrehurek.com/gensim/models/coherencemodel.html" rel="noopener ugc nofollow" target="_blank">模型一致性</a>变得很容易。不幸的是，对于我们在这里处理的短文，使用coherence选择多少主题并不明显:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi nr"><img src="../Images/a9d6bf5f544408b0ebe7f3d750b60e1a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RNG8Rkaz9NVNFRIGd7TUDw.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">LDA相干性作为应用于bank77数据集样本的主题数量的函数。图片由作者提供。</p></figure><p id="3d5d" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">似乎增加主题的数量会继续增加这个数据集的一致性，这对于我们选择多少主题几乎没有指导意义。</p><p id="de32" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">更具挑战性的是，主题模型可能很难解释。例如，考虑以下确定的主题:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ns"><img src="../Images/d856d9b4b73fbba443abd495909823d0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*a-knX4k59l-e2DiRFvn7eQ.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">从bank77数据集的样本中提取的LDA主题。图片由作者提供。</p></figure><p id="5e3b" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">虽然有些话题有意义，但许多很难解读。关于皮尤研究中心工作的系列文章很好地解决了解释主题模型的挑战。</p><p id="50fd" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">最终，最大的问题是意图比主题更微妙。LDA和其他主题建模方法的一个局限性是，它们将文档中的词汇表视为一个单词包，其中的顺序无关紧要。这对于较长的文档(和较大的语料库)很有用，在这些文档中，识别同现的单词可以很好地描述主题。此外，主题的数量通常相对较少，而且主题相当独特。然而，短文本意图带来了挑战，例如两个短语具有几乎相同的单词但意图非常不同，或者具有相同的意图但几乎没有共同的单词。这严重限制了标准主题建模方法在短文本中识别意图的有效性。</p><h1 id="d992" class="mc md iq bd me mf mg mh mi mj mk ml mm kf mn kg mo ki mp kj mq kl mr km ms mt bi translated">聚类嵌入</h1><p id="d94e" class="pw-post-body-paragraph lf lg iq lh b li mu ka lk ll mv kd ln lo mw lq lr ls mx lu lv lw my ly lz ma ij bi translated">除了主题建模，聚类是解决无监督学习问题的另一种非常常见的方法。为了能够对文本数据进行聚类，我们需要做出多个决定，包括如何处理数据以及使用什么算法。</p><h2 id="a6c9" class="nm md iq bd me nt nu dn mi nv nw dp mm lo nx ny mo ls nz oa mq lw ob oc ms iw bi translated">选择嵌入</h2><p id="50be" class="pw-post-body-paragraph lf lg iq lh b li mu ka lk ll mv kd ln lo mw lq lr ls mx lu lv lw my ly lz ma ij bi translated">首先，有必要用数字表示我们的文本数据。一种方法是创建每个单词的嵌入或向量表示，用于聚类。<a class="ae le" rel="noopener" target="_blank" href="/introduction-to-word-embeddings-4cf857b12edc">这篇文章</a>很好地概述了嵌入单词的各种方式。由于每个消息由几个单词组成，一种选择是简单地对每个消息中所有单词的单个单词嵌入进行平均。对于某些应用程序来说，这已经足够好了，但是最好是直接计算完整句子的嵌入，以便更有效地考虑含义。特别是考虑到每条消息有多短，这将有助于避免上述主题建模算法的一些缺陷。</p><p id="6573" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">事实证明，有许多方法可以找到完整消息或句子的单一向量表示。<a class="ae le" rel="noopener" target="_blank" href="/document-embedding-techniques-fed3e7a6a25d">本文</a>对实现这一目标的各种方法进行了概述。Google的<a class="ae le" href="https://tfhub.dev/google/universal-sentence-encoder/4" rel="noopener ugc nofollow" target="_blank">通用句子编码器</a> (USE)，由<a class="ae le" href="https://arxiv.org/abs/1803.11175" rel="noopener ugc nofollow" target="_blank"> Cer等人</a>于2018年首次发布，是一种流行的句子嵌入模型。使用模型在各种数据上进行训练，包括维基百科、网络新闻、网络问答页面和论坛，它在句子语义相似性任务上表现良好。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi od"><img src="../Images/63817e8f52901222672112cdd33af752.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ah3CX8s0g2nzVWAoiT0whA.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">通用句子编码器模型。来源:tensor flow Hub<a class="ae le" href="https://tfhub.dev/google/universal-sentence-encoder/4" rel="noopener ugc nofollow" target="_blank">https://tfhub.dev/google/universal-sentence-encoder/4</a></p></figure><p id="ec46" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">2019年，Reimers和Gurevych发表了一篇<a class="ae le" href="https://arxiv.org/pdf/1908.10084.pdf" rel="noopener ugc nofollow" target="_blank">论文</a>，介绍了句子-BERT，这是一种“对预训练的BERT网络的修改，使用连体和三元网络结构来导出语义上有意义的句子嵌入，可以使用余弦相似性进行比较”。他们还发布了一个<a class="ae le" href="https://github.com/UKPLab/sentence-transformers" rel="noopener ugc nofollow" target="_blank"> Python实现</a>，使得下载和使用许多不同的<a class="ae le" href="https://www.sbert.net/docs/pretrained_models.html" rel="noopener ugc nofollow" target="_blank">预训练模型</a>变得容易。</p><p id="821e" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">鉴于我们的数据集如此之小，在这里使用预先训练的模型是更可取的。对于这个分析，我将比较四个预训练的句子嵌入模型的结果:USE和三个不同的句子-BERT模型(<strong class="lh ja"> all-mpnet-base-v2 </strong>、<strong class="lh ja"> all-MiniLM-L6-v2 </strong>和<strong class="lh ja">all-distilloberta-v1</strong>)。</p><p id="9ae4" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">将我们的信息转换成句子嵌入就很简单了:</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="oe of l"/></div></figure><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi og"><img src="../Images/db7aa94f1b5b9d6bcd164c659b99c295.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6vXExYk6VefbaxEkN70Z_w.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">使用各种预训练的嵌入模型嵌入形状。图片由作者提供。</p></figure><h2 id="3bd4" class="nm md iq bd me nt nu dn mi nv nw dp mm lo nx ny mo ls nz oa mq lw ob oc ms iw bi translated">降维</h2><p id="36bd" class="pw-post-body-paragraph lf lg iq lh b li mu ka lk ll mv kd ln lo mw lq lr ls mx lu lv lw my ly lz ma ij bi translated">如上所述，我们所有的句子嵌入都有很高的维数(每个大于500个特征)。维数灾难的一个表现是，聚类所需的距离度量(如欧几里德和曼哈顿)在如此高的维数上变得毫无意义(更多细节，请参见Aggarwal等人的“<a class="ae le" href="https://bib.dbvis.de/uploadedFiles/155.pdf" rel="noopener ugc nofollow" target="_blank">关于高维空间中距离度量的惊人行为</a>”)。虽然一些句子转换器预训练模型是以保留一些距离度量的有用性的方式创建的，但是在聚类之前进行维度缩减将极大地改善结果。(为了向自己证明这一点，我<a class="ae le" href="https://github.com/dborrelli/chat-intents/blob/main/notebooks/03-clustering_without_dim_reduction.ipynb" rel="noopener ugc nofollow" target="_blank">在没有先降维的情况下，使用不同的聚类算法和嵌入简单地探索了一下</a>。)</p><p id="f35d" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">由<a class="ae le" href="https://arxiv.org/pdf/1802.03426.pdf" rel="noopener ugc nofollow" target="_blank">麦金尼斯等人</a>于2020年提出的一致流形逼近和投影降维(<a class="ae le" href="https://umap-learn.readthedocs.io/en/latest/" rel="noopener ugc nofollow" target="_blank"> UMAP </a>)作为一种降维技术，已经迅速普及。UMAP比t-SNE更快、更具可扩展性，同时也更好地保留了数据的全局结构。这使得它既可用于可视化，也可作为聚类前的预处理降维步骤。我们将在这里使用它。</p><h2 id="4ca6" class="nm md iq bd me nt nu dn mi nv nw dp mm lo nx ny mo ls nz oa mq lw ob oc ms iw bi translated">选择聚类分析算法</h2><p id="8ed7" class="pw-post-body-paragraph lf lg iq lh b li mu ka lk ll mv kd ln lo mw lq lr ls mx lu lv lw my ly lz ma ij bi translated">Scikit-learn文档中有一个<a class="ae le" href="https://scikit-learn.org/stable/modules/clustering.html#overview-of-clustering-methods" rel="noopener ugc nofollow" target="_blank">有用的概述</a>，介绍了它支持的许多不同的聚类算法以及每种算法何时性能最佳。对于我们当前的应用，最好是使用一种不需要预先指定聚类数的算法，并且还可以容忍噪声数据。基于密度的算法在这里是一个很好的选择，因为它们不需要指定簇的数量，并且与簇的形状无关。具有噪声的应用的基于层次密度的空间聚类(HDBSCAN)已经变得流行，因为它具有比DBSCAN更少且更直观的超参数，并且对可变密度聚类是鲁棒的。HDBSCAN文档提供了不同聚类算法的<a class="ae le" href="https://hdbscan.readthedocs.io/en/latest/comparing_clustering_algorithms.html#comparing-python-clustering-algorithms" rel="noopener ugc nofollow" target="_blank">有益比较</a>。HDBSCAN对当前的问题最有效，所以在本文中我们将重点关注它。</p><h2 id="3439" class="nm md iq bd me nt nu dn mi nv nw dp mm lo nx ny mo ls nz oa mq lw ob oc ms iw bi translated">从UMAP + HDBSCAN生成集群</h2><p id="b9bc" class="pw-post-body-paragraph lf lg iq lh b li mu ka lk ll mv kd ln lo mw lq lr ls mx lu lv lw my ly lz ma ij bi translated">为了主题建模的目的，至少有两个包(可能更多)可以将UMAP和HDBSCAN链接在一起:Top2Vec ( <a class="ae le" href="https://github.com/ddangelov/Top2Vec" rel="noopener ugc nofollow" target="_blank"> github </a>和<a class="ae le" href="https://arxiv.org/pdf/2008.09470.pdf" rel="noopener ugc nofollow" target="_blank"> paper </a>)和BERTopic ( <a class="ae le" href="https://github.com/MaartenGr/BERTopic" rel="noopener ugc nofollow" target="_blank"> github </a>和<a class="ae le" rel="noopener" target="_blank" href="/topic-modeling-with-bert-779f7db187e6"> article </a>)。然而，两个包中使用的默认超参数对于像当前这样的具有短文本和小语料库的问题(大多数数据最终被分类为噪声，并且总共只找到三个聚类)并不工作良好。为了更容易适应我们当前的意图提取问题，我们将直接使用UMAP和HDBSCAN软件包进行超参数调整。</p><p id="bae5" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">UMAP有几个超参数来控制它如何执行降维，但最重要的两个是<code class="fe oh oi oj ni b">n_neighbors</code>和<code class="fe oh oi oj ni b">n_components</code>。<code class="fe oh oi oj ni b">n_neighbors</code>参数控制UMAP如何平衡数据中的<a class="ae le" href="https://umap-learn.readthedocs.io/en/latest/parameters.html#n-neighbors" rel="noopener ugc nofollow" target="_blank">局部与全局结构</a>。该参数控制UMAP学习流形结构的邻域的大小，因此较低的值<code class="fe oh oi oj ni b">n_neighbors</code>将更多地关注非常局部的结构。<code class="fe oh oi oj ni b">n_components</code>参数控制输入数据降维后最终嵌入数据的维度。不幸的是，没有地面真实标签，没有明显的方法来挑选最佳UMAP参数。这里我们有标签，我们将在最后使用它来确定我们做得有多好。但是这项工作的重点是当我们有未标记的数据时确定一种使用的方法。在安杰洛夫的Top2Vec论文中，他提到<code class="fe oh oi oj ni b">n_neighbors</code> = 15和<code class="fe oh oi oj ni b">n_components</code> = 5最适合他的下游任务，但这不太可能总是适用于任何数据集。</p><p id="fc3f" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">HDBSCAN还有几个重要的超参数，但要考虑的最重要的一个<a class="ae le" href="https://hdbscan.readthedocs.io/en/latest/parameter_selection.html#selecting-min-cluster-size" rel="noopener ugc nofollow" target="_blank">是<code class="fe oh oi oj ni b">min_cluster_size</code>。直观地说，这控制了您想要视为集群的最小分组。此外，<code class="fe oh oi oj ni b">min_samples</code>参数控制聚类的保守程度，如果未指定，该参数默认为等于<code class="fe oh oi oj ni b">min_cluster_size</code>。它越大，作为噪声/异常值丢弃的点就越多。解耦这两个超参数并使<code class="fe oh oi oj ni b">min_samples</code>小于<code class="fe oh oi oj ni b">min_cluster_size</code>将通过将它们与它们最相似的相邻聚类合并来基本上保留将被标记为异常值的点。如果我们试图发现集群的数量，这并不是我们想要发生的事情。因此，这里我只考虑直接修改<code class="fe oh oi oj ni b">min_cluster_size</code>参数:</a></p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="oe of l"/></div></figure><p id="86b1" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">注意，UMAP是一种<a class="ae le" href="https://umap-learn.readthedocs.io/en/latest/reproducibility.html" rel="noopener ugc nofollow" target="_blank">随机算法</a>，使用随机性来加速逼近步骤并执行优化。因此，我们将随机种子状态设置为一个常量值，以获得给定的一组UMAP超参数的一致结果。</p><h2 id="51d0" class="nm md iq bd me nt nu dn mi nv nw dp mm lo nx ny mo ls nz oa mq lw ob oc ms iw bi translated">定义评分函数</h2><p id="45e2" class="pw-post-body-paragraph lf lg iq lh b li mu ka lk ll mv kd ln lo mw lq lr ls mx lu lv lw my ly lz ma ij bi translated">我们现在有了一个管道，其中有三个我们想要调优的超参数(<code class="fe oh oi oj ni b">n_neighbors</code>、<code class="fe oh oi oj ni b">n_components</code>和<code class="fe oh oi oj ni b">min_cluster_size</code>)。接下来，我们需要决定如何实际评估我们的聚类，以选择最佳超参数。虽然通常与各种聚类算法一起使用，但轮廓分数对于基于密度的算法(如DBSCAN和HDBSCAN)来说并不是一个很好的验证度量，因为它假设所有点都被分配到一个组中，并且不能适当地处理噪声/异常值。<a class="ae le" href="https://epubs.siam.org/doi/pdf/10.1137/1.9781611973440.96" rel="noopener ugc nofollow" target="_blank">基于密度的聚类验证(DBCV) </a>已经被一些人提出并用于调整HDBSCAN超参数。虽然它可能在一些应用中运行良好，但对于当前的问题，它倾向于使用较少的聚类数，而将过多的样本放在“噪声”类别中。</p><p id="119a" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">相反，我们将利用有用的<code class="fe oh oi oj ni b">probabilities_</code> HDBSCAN属性，该属性来自文档:</p><blockquote class="na nb nc"><p id="55f1" class="lf lg mb lh b li lj ka lk ll lm kd ln nd lp lq lr ne lt lu lv nf lx ly lz ma ij bi translated">每个样本作为其指定分类成员的强度。噪声点概率为零；聚类中的点的赋值与它们作为聚类的一部分存在的程度成比例。</p></blockquote><p id="e06d" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated"><a class="ae le" rel="noopener" target="_blank" href="/how-to-cluster-in-high-dimensions-4ef693bacc6">这篇由Nikolay Oskolkov撰写的文章</a>提供了一个非常直观且符合逻辑的解决方案，只需将我们想要最小化的成本函数定义为:</p><pre class="kp kq kr ks gt nh ni nj nk aw nl bi"><span id="031e" class="nm md iq ni b gy nn no l np nq">Cost = percent of dataset with &lt; 5% cluster label confidence</span></pre><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="oe of l"/></div></figure><p id="5aca" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">这将有助于确保我们将尽可能多的数据点分配给实际的聚类，而不是将它们标记为噪声。但是，是什么阻止我们设置超参数，使每个单独的点成为一个“集群”，或者仅仅是一个巨大的集群呢？</p><p id="dcd3" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated"><strong class="lh ja">这里我们必须使用一些领域知识来应用约束。</strong>对于这个问题，根据我对这类数据的经验，我预计至少会有30个标签，但不会超过100个。所以我们的目标函数变成了一个约束优化问题:</p><pre class="kp kq kr ks gt nh ni nj nk aw nl bi"><span id="bd5f" class="nm md iq ni b gy nn no l np nq">minimize(Cost = percent of dataset with &lt; 5% cluster label      <br/>                confidence)</span><span id="0373" class="nm md iq ni b gy ok no l np nq">Subject to: 30 &lt; num_clusters &lt; 100</span></pre><h2 id="969e" class="nm md iq bd me nt nu dn mi nv nw dp mm lo nx ny mo ls nz oa mq lw ob oc ms iw bi translated">随机超参数搜索</h2><p id="3f36" class="pw-post-body-paragraph lf lg iq lh b li mu ka lk ll mv kd ln lo mw lq lr ls mx lu lv lw my ly lz ma ij bi translated">在当前数据集大小仅为1，000个样本的情况下，对于给定的一组输入，生成聚类并对其进行评分仍然需要大约3秒钟。尝试进行例如10×10×10超参数搜索空间的全网格搜索将花费几乎一个小时。较大的数据集需要更长的时间。我关心找到正确的超参数，但不太关心<em class="mb">T4。执行随机搜索而不是全网格搜索是一个非常有效的替代策略:</em></p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="oe of l"/></div></figure><p id="e0c9" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">运行对100个随机选择的超参数值的搜索得到:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ol"><img src="../Images/d04d426cb6c6a250c1a8d9f57ac265f7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Jqrf2abqK44LmlMEQg2BiQ.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">UMAP+HDBSCAN随机搜索超参数空间的结果。图片由作者提供。</p></figure><p id="e43e" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">我们看到，成本最低的运行总共也只有不到10个集群。聚类数在30和100之间的第一个条目有59个聚类，成本为0.140(即大约14%的数据被标记为异常值或低置信度)。跑步也只用了6分钟。还不错。</p><h2 id="1cb7" class="nm md iq bd me nt nu dn mi nv nw dp mm lo nx ny mo ls nz oa mq lw ob oc ms iw bi translated">基于超点的贝叶斯优化</h2><p id="033e" class="pw-post-body-paragraph lf lg iq lh b li mu ka lk ll mv kd ln lo mw lq lr ls mx lu lv lw my ly lz ma ij bi translated">随机搜索超参数空间工作得相当好，但是有一个更好的选择:贝叶斯优化。在这里，我们将利用流行的<a class="ae le" href="https://github.com/hyperopt/hyperopt" rel="noopener ugc nofollow" target="_blank">超远视包</a>来做到这一点。如果你不熟悉hyperopt和贝叶斯优化，<a class="ae le" href="https://medium.com/district-data-labs/parameter-tuning-with-hyperopt-faa86acdfdce" rel="noopener">这篇文章</a>提供了一个很好的概述。</p><p id="a272" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">首先，定义我们想要最小化的目标函数。如果聚类的数量落在期望的范围之外，则通过添加惩罚项将优化约束包括在目标函数中:</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="oe of l"/></div></figure><p id="0f63" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">然后使用树形结构Parzen估计器(TPE)算法在超参数搜索空间上最小化目标函数:</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="oe of l"/></div></figure><p id="d397" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">在我们的参数空间上运行具有100个最大评估的贝叶斯搜索产生比随机搜索稍好的结果:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi om"><img src="../Images/14d2fcb4cc81664b3c8bfec47e402a3d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RVU9YBvq0ZyaZc9NF4dSig.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">对UMAP+HDBSCAN的超参数空间进行贝叶斯搜索的结果。图片由作者提供。</p></figure><p id="5ad0" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">然后，使用来自多个不同模型的嵌入来运行管道就很容易了:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi on"><img src="../Images/7ff91d1e9b540ef4cd159c13dbddef96.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UDgFJ7ww5Q2TPQ8b8FqnNg.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">此处考虑的每个模型的最佳超参数。图片由作者提供。</p></figure><p id="c5dc" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">在这一点上，我们可以做更多的事情，如可视化集群或手动检查其中一些以确保它们有意义。但最终，我们试图从最佳分析管道中找到“最佳”聚类结果。如果我们相信损失函数，那么选择损失最低的配置是有意义的。在上面尝试的组合中，似乎我们应该使用句子转换器#1 ( <strong class="lh ja"> all-mpnet-base-v2 </strong>)，它使用<code class="fe oh oi oj ni b">n_neighbors</code> = 6、<code class="fe oh oi oj ni b">n_comonents</code> = 9和<code class="fe oh oi oj ni b">min_cluster_size</code> = 6生成了55个集群。</p><h2 id="882d" class="nm md iq bd me nt nu dn mi nv nw dp mm lo nx ny mo ls nz oa mq lw ob oc ms iw bi translated">评估绩效，了解基本事实标签</h2><p id="e52c" class="pw-post-body-paragraph lf lg iq lh b li mu ka lk ll mv kd ln lo mw lq lr ls mx lu lv lw my ly lz ma ij bi translated">在这种情况下，我们碰巧也知道基本事实标签，因此我们可以看到我们的损失函数与性能的相关性。我们可以手动检查模型在一些地面真实集群上的表现:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi oo"><img src="../Images/33894f05a2ab3a4976179bca54e06ac0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Y7lI8kITItNWNzaVqznTEQ.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">四种不同模型的单一基本事实类别的示例结果。图片由作者提供。</p></figure><p id="a181" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">如上所示，所有模型在将<code class="fe oh oi oj ni b">card_about_to_expire</code>基本事实组中的大部分消息放在同一个集群中时表现相对较好。至少对于这个类别来说，第一句话-transformer模型似乎在正确地将所有消息分配到同一个集群方面表现突出。</p><p id="5d0a" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">除了手动检查所有组，我们还可以定量评估模型性能。评估文本聚类的两个常用指标是<a class="ae le" href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.normalized_mutual_info_score.html" rel="noopener ugc nofollow" target="_blank">归一化互信息</a>和<a class="ae le" href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.adjusted_rand_score.html" rel="noopener ugc nofollow" target="_blank">调整后的Rand指数</a>。这两个指标的值都在0到1之间，越大越好。计算考虑中的四个模型的最佳超参数的这些度量得出:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi op"><img src="../Images/957e2f728c668a3167c187f153910204.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Bvhtf52xeJVgBOttRjoSlg.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">四个模型结果的比较和对地面真实标签的评估。图片由作者提供。</p></figure><p id="2ec8" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">与我们之前的结论一致，句子转换器#1实际上表现最好，ARI为0.46，NMI为0.81。然而，某些其他模型的性能排序并不遵循其成本函数值的预期顺序。因此，我们对超参数调整的评分方法并不完美，但显然对当前的应用仍然有用。</p><h2 id="00c3" class="nm md iq bd me nt nu dn mi nv nw dp mm lo nx ny mo ls nz oa mq lw ob oc ms iw bi translated">自动聚类标记</h2><p id="8105" class="pw-post-body-paragraph lf lg iq lh b li mu ka lk ll mv kd ln lo mw lq lr ls mx lu lv lw my ly lz ma ij bi translated">为了使结果更有帮助，我们还可以自动将描述性标签应用到我们找到的分类中。Liu等人的论文提供了一种有趣的方法，通过从每个聚类中的短语中提取最常见的动作-对象对作为聚类标签(例如“book-flight”)。我们在这里考虑的bank77数据集比那篇论文中的数据集稍微复杂一些，但是我们可以做一些类似的事情。在这里，我们将连接最常见的动词、直接宾语和每个簇中的前两个名词。spaCy 包有一个强大的语法依赖解析器，我们可以用它来做这件事:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi oq"><img src="../Images/6fb96ce34905058530f4f977ce15fc3a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*W90goEDe_QfWQETyOLTj3A.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">对一个例句应用spaCy的句法依存解析器的结果。图片由作者提供。</p></figure><p id="c892" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">我们可以编写一个简单的函数来提取每个分类的标签:</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="oe of l"/></div></figure><p id="a9da" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">将这些标签应用到我们的最佳模型找到的每个聚类中，产生了我们的最终结果:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi or"><img src="../Images/7855186da44aff83f8a895ae44c57883.png" data-original-src="https://miro.medium.com/v2/resize:fit:634/format:webp/1*miOSW12hHFGqUb8p9eUeSQ.png"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">带有计数和描述性标签的提取分类的摘要。图片由作者提供。</p></figure><p id="a6c1" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">使用我们的调整方法，我们已经自动提取了描述性标签，并将其应用于数据集中的55个分类。</p><p id="98e5" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">因为在这种情况下，我们知道每个文档的基本事实标签，所以我们还可以检查一些文档，以查看我们的派生标签与基本事实标签的匹配程度:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi os"><img src="../Images/9f0366ff691480509691613ad2231a83.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*U2qRwRUz8yLzG9pXrUn-Iw.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">原始数据的样本，带有派生的描述性标签和原始的基本事实标签(“类别”字段)。图片由作者提供。</p></figure><p id="ede5" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">它们并不完美，但提取的标签与地面类别标签非常匹配！</p><h1 id="49fb" class="mc md iq bd me mf mg mh mi mj mk ml mm kf mn kg mo ki mp kj mq kl mr km ms mt bi translated">摘要</h1><p id="4c28" class="pw-post-body-paragraph lf lg iq lh b li mu ka lk ll mv kd ln lo mw lq lr ls mx lu lv lw my ly lz ma ij bi translated">在这篇文章中，我概述了一个利用领域知识创建约束优化问题的框架，以自动调整UMAP和HDBSCAN超参数。这使我们能够轻松地对短文本文档进行聚类，并应用描述性标签。本文的重点是小数据集，但是同样的方法也可以应用于更大的数据集。在决定或需要完成耗时的手动标注之前，聚类结果在很短的时间内提供了对未标注文本数据的有用见解。</p><p id="990d" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">本文中的所有代码示例，以及我为简化这些概念的应用而创建的chatintents python包，都可以在这里找到:</p><div class="ot ou gp gr ov ow"><a href="https://github.com/dborrelli/chat-intents" rel="noopener  ugc nofollow" target="_blank"><div class="ox ab fo"><div class="oy ab oz cl cj pa"><h2 class="bd ja gy z fp pb fr fs pc fu fw iz bi translated">GitHub - dborrelli/chat-intents:聚集句子嵌入以提取消息意图</h2><div class="pd l"><h3 class="bd b gy z fp pb fr fs pc fu fw dk translated">集群句子嵌入以提取消息意图</h3></div><div class="pe l"><p class="bd b dl z fp pb fr fs pc fu fw dk translated">github.com</p></div></div><div class="pf l"><div class="pg l ph pi pj pf pk ky ow"/></div></div></a></div><p id="3c26" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">感谢阅读。如果你觉得这篇文章很有帮助，请在下面留言或联系LinkedIn。</p></div></div>    
</body>
</html>