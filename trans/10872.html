<html>
<head>
<title>Differentiable Generator Networks: an Introduction</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">可微发电机网络:导论</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/differentiable-generator-networks-an-introduction-5a9650a24823?source=collection_archive---------21-----------------------#2021-10-21">https://towardsdatascience.com/differentiable-generator-networks-an-introduction-5a9650a24823?source=collection_archive---------21-----------------------#2021-10-21</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="08dc" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph">深度学习基础</h2><div class=""/><div class=""><h2 id="3ff1" class="pw-subtitle-paragraph jw iz iq bd b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn dk translated">VAEs、gan及其挑战介绍</h2></div><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi ko"><img src="../Images/b22e47dd2f89067084b00b583e9f7718.png" data-original-src="https://miro.medium.com/v2/resize:fit:1274/format:webp/1*9wUgA8V5Pj-REc4p5rs7Xg.png"/></div><p class="kw kx gj gh gi ky kz bd b be z dk translated">图片来自<a class="ae la" href="https://thispersondoesnotexist.com/" rel="noopener ugc nofollow" target="_blank">https://thispersondoesnotexist.com/</a>(开源)</p></figure><h1 id="5a61" class="lb lc iq bd ld le lf lg lh li lj lk ll kf lm kg ln ki lo kj lp kl lq km lr ls bi translated">介绍</h1><p id="06d9" class="pw-post-body-paragraph lt lu iq lv b lw lx ka ly lz ma kd mb mc md me mf mg mh mi mj mk ml mm mn mo ij bi translated">训练生成模型比训练分类器或回归器要困难得多。当在监督学习问题中训练分类器时，我们知道训练示例的输入和输出之间的最佳映射。然而，训练生成模型涉及优化标准，这是棘手的。</p><p id="970e" class="pw-post-body-paragraph lt lu iq lv b lw mp ka ly lz mq kd mb mc mr me mf mg ms mi mj mk mt mm mn mo ij bi translated">在本文中，我将介绍生成模型的基础知识，以及两个核心可微分生成网络的功能。这些是变分自动编码器(VAEs)和生成敌对网络(GANs)。</p><h1 id="7d6f" class="lb lc iq bd ld le lf lg lh li lj lk ll kf lm kg ln ki lo kj lp kl lq km lr ls bi translated">生成模型</h1><p id="559f" class="pw-post-body-paragraph lt lu iq lv b lw lx ka ly lz ma kd mb mc md me mf mg mh mi mj mk ml mm mn mo ij bi translated">在进入可微分的发电机网络之前，我们可以回顾一下生成模型。</p><p id="2c53" class="pw-post-body-paragraph lt lu iq lv b lw mp ka ly lz mq kd mb mc mr me mf mg ms mi mj mk mt mm mn mo ij bi translated">生成模型的最终目标是生成与训练数据尽可能相似的合成数据。总之，<strong class="lv ja">这都是关于在我们的训练数据p(x) </strong>上建模一个概率密度函数。给定一组训练数据，生成模型试图构建一个函数，该函数允许我们确定任何数据点属于训练集的可能性有多大。这种密度允许生成模型做有趣的事情，比如评估新生成的数据点p(x|y=y)的概率，或者从密度中采样以生成新数据。</p><h1 id="679d" class="lb lc iq bd ld le lf lg lh li lj lk ll kf lm kg ln ki lo kj lp kl lq km lr ls bi translated">生成模型:GMM</h1><p id="cab3" class="pw-post-body-paragraph lt lu iq lv b lw lx ka ly lz ma kd mb mc md me mf mg mh mi mj mk ml mm mn mo ij bi translated">高斯混合模型是一种非常简单的生成方法。GMM通过期望最大化来学习，通过迭代地评估对数似然来最大化模型的性能。在以后的文章中，我将更详细地介绍他们的学习。</p><p id="8d97" class="pw-post-body-paragraph lt lu iq lv b lw mp ka ly lz mq kd mb mc mr me mf mg ms mi mj mk mt mm mn mo ij bi translated">GMM假设数据的分布由一组高斯分布组成。他们通过在训练数据上拟合K个高斯pdf来近似训练数据空间，有点像k-mean聚类，但是在每个聚类上拟合高斯分布，而不是识别类成员。通过一组高斯分布来近似训练数据空间，我们获得了以下优点:</p><ul class=""><li id="084c" class="mu mv iq lv b lw mp lz mq mc mw mg mx mk my mo mz na nb nc bi translated">您可以将新样本的概率表示为K个高斯分布的加权和</li><li id="ac5f" class="mu mv iq lv b lw nd lz ne mc nf mg ng mk nh mo mz na nb nc bi translated">您可以对分布进行采样，以生成属于每个分类的数据</li></ul><p id="2dff" class="pw-post-body-paragraph lt lu iq lv b lw mp ka ly lz mq kd mb mc mr me mf mg ms mi mj mk mt mm mn mo ij bi translated">生成模型的全部要点是在我们的训练数据空间<em class="ni"> p(x) </em>上构建我们的密度函数。GMM是通过一组高斯密度来近似这个潜在空间的。通过一组高斯近似潜在空间是有利的，因为它允许我们评估该空间中的任何新样本，我们可以用它来了解新数据样本是否很好地符合我们现有的数据。此外，通过具有完全定义的潜在空间，可以对每个聚类进行采样，生成新的合成数据。</p><h1 id="9fea" class="lb lc iq bd ld le lf lg lh li lj lk ll kf lm kg ln ki lo kj lp kl lq km lr ls bi translated">可微分发电机网络</h1><p id="106f" class="pw-post-body-paragraph lt lu iq lv b lw lx ka ly lz ma kd mb mc md me mf mg mh mi mj mk ml mm mn mo ij bi translated">当生成发电机网络时，我们想要可微分的模型，<strong class="lv ja">学习潜在变量Z </strong>(来自随机分布)<strong class="lv ja">到我们的数据分布X. </strong>之间的映射</p><p id="0916" class="pw-post-body-paragraph lt lu iq lv b lw mp ka ly lz mq kd mb mc mr me mf mg ms mi mj mk mt mm mn mo ij bi translated">可微分生成器网络假设每个样本携带的信息可以在比数据空间更简单的空间中表示。即使数据生活在高维空间(比如高分辨率图像)，图像中的信息也可以用更简单的流形来表达。假设我们有手写数字的图片，即使训练数据存在于高维空间中(图像可能是32×32像素),每个图像背后的潜在信息也要小得多。所有可能的手写数字的空间比所有可能的32×32像素图像小得多。</p><p id="4187" class="pw-post-body-paragraph lt lu iq lv b lw mp ka ly lz mq kd mb mc mr me mf mg ms mi mj mk mt mm mn mo ij bi translated">生成模型在潜在变量Z上构建低维潜在空间，然后使用它来近似训练数据空间。有两种主要的方法<strong class="lv ja">来利用这个潜在空间产生新的数据:</strong></p><ul class=""><li id="5bf3" class="mu mv iq lv b lw mp lz mq mc mw mg mx mk my mo mz na nb nc bi translated">将潜在变量Z转换成新样本X(来自p(x)的近似样本)</li><li id="a125" class="mu mv iq lv b lw nd lz ne mc nf mg ng mk nh mo mz na nb nc bi translated">将Z转换为X上的分布(生成一个函数来逼近p(x ),然后从中采样)</li></ul><p id="5b70" class="pw-post-body-paragraph lt lu iq lv b lw mp ka ly lz mq kd mb mc mr me mf mg ms mi mj mk mt mm mn mo ij bi translated">因此，生成模型要么生成新样本(从Z生成X)，要么生成分布，其中该子空间中的所有样本都代表训练数据(从Z生成X上的分布)。</p><p id="00d8" class="pw-post-body-paragraph lt lu iq lv b lw mp ka ly lz mq kd mb mc mr me mf mg ms mi mj mk mt mm mn mo ij bi translated">根据您使用的创成式模型，模型将以上述两种方式之一生成数据。在本文接下来的部分中，我将讨论具体的模型架构，以及如何使用这些架构来生成合成数据。</p><h1 id="1fde" class="lb lc iq bd ld le lf lg lh li lj lk ll kf lm kg ln ki lo kj lp kl lq km lr ls bi translated">可变自动编码器</h1><h2 id="21f4" class="nj lc iq bd ld nk nl dn lh nm nn dp ll mc no np ln mg nq nr lp mk ns nt lr iw bi translated">优化阀门</h2><p id="96d8" class="pw-post-body-paragraph lt lu iq lv b lw lx ka ly lz ma kd mb mc md me mf mg mh mi mj mk ml mm mn mo ij bi translated">GMM和VAEs都试图在训练数据X上建立一个分布，然后可以对其进行采样以生成新数据(从Z生成X上的分布)。然而，vae并不在我们的训练数据X上构建直接的PDF，相反，它们利用前面陈述的假设，并且学习我们的潜在变量Z和我们的训练数据X(其中Z中的潜在空间比我们的数据X的空间简单得多)上的分布之间的映射。</p><p id="c970" class="pw-post-body-paragraph lt lu iq lv b lw mp ka ly lz mq kd mb mc mr me mf mg ms mi mj mk mt mm mn mo ij bi translated">在VAEs中，我们选择一个非常简单的潜在变量Z的分布(通常是正态分布),形式为N(0，I)。通过这样做，VAEs假设数据的变化是高斯型的。以前面的手写数字为例，如果您想到所有可能的手写“3”，所有可能的手写“3”之间的变化将大致为高斯型。实际上，这通常是一个很好的假设，因为传感器数据中的噪声通常是高斯噪声。</p><p id="b515" class="pw-post-body-paragraph lt lu iq lv b lw mp ka ly lz mq kd mb mc mr me mf mg ms mi mj mk mt mm mn mo ij bi translated">我们希望找到数据的分布<em class="ni"> p(x) </em>来最大化训练数据的概率。根据概率乘积和求和规则，我们可以将<em class="ni"> p(x) </em>表示如下:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/9c09ff3373a65d6f21e05a4b7a4286c5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1344/format:webp/1*craRTGSzAKYy31h0dkN1Uw.png"/></div></figure><p id="61db" class="pw-post-body-paragraph lt lu iq lv b lw mp ka ly lz mq kd mb mc mr me mf mg ms mi mj mk mt mm mn mo ij bi translated">其中<em class="ni"> p(x|z，θ) </em>是后验概率(给定我们的潜在样本Z和我们的模型参数θ的情况下，我们的训练数据X的概率)，而<em class="ni"> p(z) </em>是先验分布，我们假设该先验分布是具有零均值和恒等协方差矩阵的高斯分布。</p><p id="6322" class="pw-post-body-paragraph lt lu iq lv b lw mp ka ly lz mq kd mb mc mr me mf mg ms mi mj mk mt mm mn mo ij bi translated">目标是找到最大化该概率的一组权重θ，最大化<em class="ni"> p(x) </em>给出:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="nw nx di ny bf nz"><div class="gh gi nv"><img src="../Images/d992e05e2cc07476599ecb37418ed361.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yplnsF1-ic3z2_KFhycg6w.png"/></div></div></figure><p id="76ad" class="pw-post-body-paragraph lt lu iq lv b lw mp ka ly lz mq kd mb mc mr me mf mg ms mi mj mk mt mm mn mo ij bi translated">由此产生的表达式称为ELBO(证据下限),理解其术语非常重要。</p><p id="f08b" class="pw-post-body-paragraph lt lu iq lv b lw mp ka ly lz mq kd mb mc mr me mf mg ms mi mj mk mt mm mn mo ij bi translated">第一项是给定我们的潜在变量(我们希望最大化这个变量)时，我们的数据的概率的期望值。这告诉我们，在给定z的情况下，模型是否正在产生X的良好样本。当假设后验概率为高斯时，最大化此项实际上相当于最小化自动编码器的均方误差。</p><p id="ffd8" class="pw-post-body-paragraph lt lu iq lv b lw mp ka ly lz mq kd mb mc mr me mf mg ms mi mj mk mt mm mn mo ij bi translated">第二项是后验概率和先验概率之间的Kullback-Leibler散度。最小化这一项本质上是试图使我们的后验概率尽可能接近均值为零且sigma为1的高斯分布。同样，我们假设数据中的变化是高斯型的，因此我们试图迫使后验概率朝着这个形状，同时仍然试图产生代表我们训练数据的良好样本。</p><h2 id="c530" class="nj lc iq bd ld nk nl dn lh nm nn dp ll mc no np ln mg nq nr lp mk ns nt lr iw bi translated">VAEs架构</h2><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi oa"><img src="../Images/ab40df958b8315f721be4e764a8cb7c3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1328/format:webp/1*vy__pw0yYiG2bcNdN6Z9uw.png"/></div><p class="kw kx gj gh gi ky kz bd b be z dk translated">作者图片</p></figure><p id="2e7c" class="pw-post-body-paragraph lt lu iq lv b lw mp ka ly lz mq kd mb mc mr me mf mg ms mi mj mk mt mm mn mo ij bi translated">VAEs的结构分为两部分，编码器和解码器。编码器将输入数据的维度降低到二维。这些维度用于参数化我们的潜在空间p(z)的正态分布。然后，可以对该分布进行采样，这些样本被用作解码器的输入。解码器试图重建训练数据空间中对应于潜在空间中的样本的数据点。</p><p id="fae5" class="pw-post-body-paragraph lt lu iq lv b lw mp ka ly lz mq kd mb mc mr me mf mg ms mi mj mk mt mm mn mo ij bi translated">因此，VAEs将输入数据映射到z上的一个潜在空间。然后，解码器将该潜在空间映射回x上的图像空间。对潜在分布<em class="ni"> p(z) </em>进行采样，并将输入通过解码器基本上允许我们从<em class="ni"> p(x)进行采样。这是我们的目标！</em></p><p id="6a27" class="pw-post-body-paragraph lt lu iq lv b lw mp ka ly lz mq kd mb mc mr me mf mg ms mi mj mk mt mm mn mo ij bi translated">当训练VAEs时，由于这些自始至终都是可微分的函数，通过使用上面定义的ELBO损失函数，可以像任何其他神经网络结构一样，通过反向传播和您喜欢的优化器来更新网络的权重。</p><h1 id="d21d" class="lb lc iq bd ld le lf lg lh li lj lk ll kf lm kg ln ki lo kj lp kl lq km lr ls bi translated">生成对抗网络</h1><h2 id="9fc9" class="nj lc iq bd ld nk nl dn lh nm nn dp ll mc no np ln mg nq nr lp mk ns nt lr iw bi translated">甘斯理论</h2><p id="d8a5" class="pw-post-body-paragraph lt lu iq lv b lw lx ka ly lz ma kd mb mc md me mf mg mh mi mj mk ml mm mn mo ij bi translated">到目前为止，我们已经看到VAEs能够从潜在分布<em class="ni"> p(z) </em>生成训练数据<em class="ni"> p(x) </em>的分布。gan生成数据的方式不同。他们不是生成分布，而是生成可能属于该分布的样本。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="nw nx di ny bf nz"><div class="gh gi ob"><img src="../Images/d23302e4eff8f141731a393291a22458.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AVc8nasFhyf3NcmKPaYknw.png"/></div></div><p class="kw kx gj gh gi ky kz bd b be z dk translated">作者图片</p></figure><p id="6925" class="pw-post-body-paragraph lt lu iq lv b lw mp ka ly lz mq kd mb mc mr me mf mg ms mi mj mk mt mm mn mo ij bi translated">GANs由一个发生器和一个鉴别器组成。这是两个可微分的函数，一起作为一个大模型工作。这两种模式是对立的，每一种模式都想实现与另一种模式相反的目标，它们本质上是竞争的。生成器获取随机变量Z，并将其映射到我们的数据空间x中的生成样本。鉴别器从训练数据和生成器生成的图像中获取图像，并尝试分类哪些图像属于训练集，哪些图像是假的。<strong class="lv ja">生成器的目标是骗过鉴别器，鉴别器的目标是正确识别生成的图像。</strong></p><p id="e84e" class="pw-post-body-paragraph lt lu iq lv b lw mp ka ly lz mq kd mb mc mr me mf mg ms mi mj mk mt mm mn mo ij bi translated">如您所见，这种生成图像的方式与VAEs生成图像的方式有着根本的不同。我们并没有产生一个密度函数<em class="ni"> p(x) </em>，而是产生了类似于<em class="ni"> p(x) </em>的样本。</p><p id="d42f" class="pw-post-body-paragraph lt lu iq lv b lw mp ka ly lz mq kd mb mc mr me mf mg ms mi mj mk mt mm mn mo ij bi translated">这两个函数都是可微的，因此可以一起优化。它们像任何其他神经网络一样通过反向传播进行训练。我不会在这篇文章中讨论它们的损失函数，但也许在以后的文章中会讨论。</p><h2 id="de8a" class="nj lc iq bd ld nk nl dn lh nm nn dp ll mc no np ln mg nq nr lp mk ns nt lr iw bi translated">然而，gan有一些问题:</h2><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/c0ac8c05a8c516112be24c52849d8c5b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1240/format:webp/1*lMUSdN2_e0FcVI_EGHYD6g.png"/></div><p class="kw kx gj gh gi ky kz bd b be z dk translated">图片来自<a class="ae la" href="https://thispersondoesnotexist.com/" rel="noopener ugc nofollow" target="_blank">https://thispersondoesnotexist.com/</a>(开源)</p></figure><p id="c82b" class="pw-post-body-paragraph lt lu iq lv b lw mp ka ly lz mq kd mb mc mr me mf mg ms mi mj mk mt mm mn mo ij bi translated">众所周知，GANs倾向于不稳定的训练，导致输出没有意义。看上面的图片，这些人是不存在的，是从GANs生成的样本。正如你所看到的，虽然这个模型很接近真实的人脸，但我相信你可以找出每张图片的一些问题。</p><p id="4c05" class="pw-post-body-paragraph lt lu iq lv b lw mp ka ly lz mq kd mb mc mr me mf mg ms mi mj mk mt mm mn mo ij bi translated">GANs很难训练，因为有四个主要障碍</p><ul class=""><li id="5112" class="mu mv iq lv b lw mp lz mq mc mw mg mx mk my mo mz na nb nc bi translated">你的GAN可能永远不会收敛到最优解。这是因为鉴别器和发生器之间的梯度可能具有相反的符号，因此您的参数可能会卡在鞍点。</li><li id="7007" class="mu mv iq lv b lw nd lz ne mc nf mg ng mk nh mo mz na nb nc bi translated">没有什么可以阻止生成网络学习单一类型的数据。例如，如果生成动物的图片，模型可以学习只生成狗的图片而不生成其他的。</li><li id="fce5" class="mu mv iq lv b lw nd lz ne mc nf mg ng mk nh mo mz na nb nc bi translated">因为GANs生成样本，而不是生成分布，然后从中采样，我们失去了Z和x之间的平滑映射。这可能使GANs在某些情况下不如VAEs有用。</li><li id="f5f6" class="mu mv iq lv b lw nd lz ne mc nf mg ng mk nh mo mz na nb nc bi translated">没有什么可以阻止生成器学习复制训练数据。</li></ul><p id="68e4" class="pw-post-body-paragraph lt lu iq lv b lw mp ka ly lz mq kd mb mc mr me mf mg ms mi mj mk mt mm mn mo ij bi translated">深度卷积gan(dcgan)通过提出以下建议解决了其中一些问题:</p><ul class=""><li id="a9a4" class="mu mv iq lv b lw mp lz mq mc mw mg mx mk my mo mz na nb nc bi translated">人们不应该使用池层，发现这些层的性能比在生成器中使用步进卷积层更差。</li><li id="faf6" class="mu mv iq lv b lw nd lz ne mc nf mg ng mk nh mo mz na nb nc bi translated">卷积图层后使用批量归一化。</li><li id="81fb" class="mu mv iq lv b lw nd lz ne mc nf mg ng mk nh mo mz na nb nc bi translated">避免使用过多的密集层，而是使用更多的卷积层，使模型更深入。</li><li id="af62" class="mu mv iq lv b lw nd lz ne mc nf mg ng mk nh mo mz na nb nc bi translated">发生器中应使用ReLu激活功能，鉴别器中应使用LeakyReLu。</li></ul><p id="62c2" class="pw-post-body-paragraph lt lu iq lv b lw mp ka ly lz mq kd mb mc mr me mf mg ms mi mj mk mt mm mn mo ij bi translated">DCGANs的论文是在一系列实验后得出这些结论的。自1980年以来，DCGANs一直被广泛用于在各种领域生成数据。</p><h1 id="2430" class="lb lc iq bd ld le lf lg lh li lj lk ll kf lm kg ln ki lo kj lp kl lq km lr ls bi translated">结论</h1><p id="dc56" class="pw-post-body-paragraph lt lu iq lv b lw lx ka ly lz ma kd mb mc md me mf mg mh mi mj mk ml mm mn mo ij bi translated">生成模型极难训练，因为训练中涉及的映射是难以处理的。生成模型要么学习生成新的数据样本，要么学习生成训练数据的分布。在本文中，我将介绍VAEs(生成发行版)和gan(生成数据样本)。我概述了这些是如何工作的，以及这些是如何产生数据的根本不同的方法。理解这一点将使您在选择应用哪个模型时做出更好的决定。</p><h2 id="35e5" class="nj lc iq bd ld nk nl dn lh nm nn dp ll mc no np ln mg nq nr lp mk ns nt lr iw bi translated">支持我</h2><p id="55c6" class="pw-post-body-paragraph lt lu iq lv b lw lx ka ly lz ma kd mb mc md me mf mg mh mi mj mk ml mm mn mo ij bi translated">希望这对你有所帮助，如果你喜欢，你可以<a class="ae la" href="https://medium.com/@diegounzuetaruedas" rel="noopener"> <strong class="lv ja">关注我！</strong>T3】</a></p><p id="74bf" class="pw-post-body-paragraph lt lu iq lv b lw mp ka ly lz mq kd mb mc mr me mf mg ms mi mj mk mt mm mn mo ij bi translated">你也可以通过我的推荐链接成为<strong class="lv ja">的媒体会员</strong>，访问我所有的文章以及更多:<a class="ae la" href="https://diegounzuetaruedas.medium.com/membership" rel="noopener">https://diegounzuetaruedas.medium.com/membership</a></p><h2 id="20c8" class="nj lc iq bd ld nk nl dn lh nm nn dp ll mc no np ln mg nq nr lp mk ns nt lr iw bi translated">你可能喜欢的其他文章</h2><p id="179a" class="pw-post-body-paragraph lt lu iq lv b lw lx ka ly lz ma kd mb mc md me mf mg mh mi mj mk ml mm mn mo ij bi translated"><a class="ae la" rel="noopener" target="_blank" href="/these-are-not-real-clothes-af58154a98c2"> VAEs:从潜像分布中间接取样</a></p><p id="de85" class="pw-post-body-paragraph lt lu iq lv b lw mp ka ly lz mq kd mb mc mr me mf mg ms mi mj mk mt mm mn mo ij bi translated"><a class="ae la" rel="noopener" target="_blank" href="/kernel-methods-a-simple-introduction-4a26dcbe4ebd">内核方法:简单介绍</a></p><p id="6c50" class="pw-post-body-paragraph lt lu iq lv b lw mp ka ly lz mq kd mb mc mr me mf mg ms mi mj mk mt mm mn mo ij bi translated"><a class="ae la" rel="noopener" target="_blank" href="/kalman-filtering-a-simple-introduction-df9a84307add">卡尔曼滤波:简单介绍</a></p></div></div>    
</body>
</html>