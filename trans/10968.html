<html>
<head>
<title>Causal Discovery</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">因果发现</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/causal-discovery-6858f9af6dcb?source=collection_archive---------0-----------------------#2021-10-26">https://towardsdatascience.com/causal-discovery-6858f9af6dcb?source=collection_archive---------0-----------------------#2021-10-26</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="6956" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">使用Python从数据中学习因果关系</h2></div><p id="eaff" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这是关于因果关系的三篇系列文章的最后一篇。之前的帖子里介绍了<a class="ae le" rel="noopener" target="_blank" href="/causality-an-introduction-f8a3f6ac4c4a"><strong class="kk iu"/></a>因果关系的“新科学”【1】，讨论了<a class="ae le" rel="noopener" target="_blank" href="/causal-inference-962ae97cefda"> <strong class="kk iu">因果推断</strong> </a> <strong class="kk iu"> </strong>的话题。本文的重点是一个相关的想法，<strong class="kk iu">因果发现</strong>。我将从描述什么是因果发现开始，给出它如何工作的草图，并以一个使用Python的具体例子结束。</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi lf"><img src="../Images/6f91ba7bed469bcda8b305f9e16658af.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*u7lyX4e_ngmUun8Yf-DU8A.png"/></div></div><p class="lr ls gj gh gi lt lu bd b be z dk translated">因果发现的总体目标:将数据转化为因果模型。图片作者。</p></figure></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h1 id="622a" class="mc md it bd me mf mg mh mi mj mk ml mm jz mn ka mo kc mp kd mq kf mr kg ms mt bi translated">什么是因果发现？</h1><p id="6bf0" class="pw-post-body-paragraph ki kj it kk b kl mu ju kn ko mv jx kq kr mw kt ku kv mx kx ky kz my lb lc ld im bi translated">在<a class="ae le" rel="noopener" target="_blank" href="/causal-inference-962ae97cefda"> <strong class="kk iu">上一篇</strong> </a>中，我讨论了<strong class="kk iu">因果推断</strong>，旨在<strong class="kk iu">回答涉及因果</strong>的问题。虽然因果推理是一个强大的工具，但它<em class="mz">需要</em>一把钥匙来操作。也就是说，它需要一个因果模型。然而，通常在现实世界中，我们可能无法确定哪些变量是互为因果的。这就是因果发现能有所帮助的地方。</p><p id="3a9d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">因果发现</strong>旨在<strong class="kk iu">从数据中推断出因果结构</strong>。换句话说，给定一个数据集，<em class="mz">导出</em>一个描述它的因果模型。</p><h1 id="daf4" class="mc md it bd me mf na mh mi mj nb ml mm jz nc ka mo kc nd kd mq kf ne kg ms mt bi translated">它是如何工作的？</h1><p id="ee72" class="pw-post-body-paragraph ki kj it kk b kl mu ju kn ko mv jx kq kr mw kt ku kv mx kx ky kz my lb lc ld im bi translated">没有标准的因果发现方法。事实上，有各种各样的技术可供使用，其中任何两种可能彼此没有什么相似之处。我在这里的目标是勾画不同算法的共同主题，并给出一些关键思想的味道。</p><p id="9a39" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">正如我在本系列的<a class="ae le" rel="noopener" target="_blank" href="/causality-an-introduction-f8a3f6ac4c4a"> <strong class="kk iu">第一篇</strong> </a>中提到的，因果发现是<strong class="kk iu">逆问题</strong>的一个例子。这就像根据冰块在厨房台面上留下的水坑来预测它的形状一样。</p><p id="9377" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">显然，这是一个难题，因为任何形状都可能产生同一个水坑。将这与因果关系联系起来，水坑就像是嵌入数据中的统计关联，而冰块就像是潜在的因果模型。</p><h2 id="7a87" class="nf md it bd me ng nh dn mi ni nj dp mm kr nk nl mo kv nm nn mq kz no np ms nq bi translated">因果发现的4个常见假设</h2><p id="9827" class="pw-post-body-paragraph ki kj it kk b kl mu ju kn ko mv jx kq kr mw kt ku kv mx kx ky kz my lb lc ld im bi translated">解决逆问题的通常方法是对你试图揭示的东西做出假设。这缩小了可能的解决方案的范围，有希望使问题可以解决。</p><p id="9961" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">因果发现算法有四个共同的假设。在参考文献<a class="ae le" href="https://arxiv.org/abs/1803.04929" rel="noopener ugc nofollow" target="_blank">【3】</a>中可以找到对这些假设的很好的讨论。</p><ol class=""><li id="f2b4" class="nr ns it kk b kl km ko kp kr nt kv nu kz nv ld nw nx ny nz bi translated"><strong class="kk iu">无环性</strong> —因果结构可以用DAG (G)来表示</li><li id="6b5c" class="nr ns it kk b kl oa ko ob kr oc kv od kz oe ld nw nx ny nz bi translated"><strong class="kk iu">马尔可夫性质</strong> —当以它们的父节点为条件时，所有节点都独立于它们的非子节点</li><li id="2183" class="nr ns it kk b kl oa ko ob kr oc kv od kz oe ld nw nx ny nz bi translated"><strong class="kk iu">忠实度</strong> —真实基础分布中的所有条件独立性<em class="mz"> p </em>用G表示</li><li id="5485" class="nr ns it kk b kl oa ko ob kr oc kv od kz oe ld nw nx ny nz bi translated"><strong class="kk iu">充分性</strong>—G中的任何一对节点都没有共同的外因</li></ol><p id="5d59" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">尽管这些假设有助于缩小可能模型的数量，但它们并没有完全解决问题。这就是因果发现的一些技巧有用的地方。</p><h1 id="4263" class="mc md it bd me mf na mh mi mj nb ml mm jz nc ka mo kc nd kd mq kf ne kg ms mt bi translated">因果发现的3个技巧</h1><p id="ed5f" class="pw-post-body-paragraph ki kj it kk b kl mu ju kn ko mv jx kq kr mw kt ku kv mx kx ky kz my lb lc ld im bi translated">如前所述，没有一种单一的因果发现方法能支配所有其他方法。尽管大多数方法都使用上述假设(甚至更多)，但不同算法的细节可能会有很大差异。</p><p id="a2ce" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">下图给出了基于以下技巧的算法分类。这绝不是一个详尽的概述。更确切地说，这是我从引用的参考资料[ <a class="ae le" href="https://www.frontiersin.org/articles/10.3389/fgene.2019.00524/full" rel="noopener ugc nofollow" target="_blank"> 2 </a>、<a class="ae le" href="https://fentechsolutions.github.io/CausalDiscoveryToolbox/html/causality.html#module-cdt.causality.graph" rel="noopener ugc nofollow" target="_blank"> 4 </a>、<a class="ae le" href="https://www.sciencedirect.com/science/article/abs/pii/0165188988900553?via%3Dihub" rel="noopener ugc nofollow" target="_blank"> 5 </a> ]中偶然发现的算法集合。</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi of"><img src="../Images/6efb8fea6623b268467482655be03975.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*McKFnNEhh8_0_FOJC2EabQ.png"/></div></div><p class="lr ls gj gh gi lt lu bd b be z dk translated">基于[ <a class="ae le" href="https://www.frontiersin.org/articles/10.3389/fgene.2019.00524/full" rel="noopener ugc nofollow" target="_blank"> 2 </a>、<a class="ae le" href="https://fentechsolutions.github.io/CausalDiscoveryToolbox/html/causality.html#module-cdt.causality.graph" rel="noopener ugc nofollow" target="_blank"> 4 </a>、<a class="ae le" href="https://www.sciencedirect.com/science/article/abs/pii/0165188988900553?via%3Dihub" rel="noopener ugc nofollow" target="_blank"> 5 </a> ]的因果发现方法的狭义分类。图片作者。</p></figure><h2 id="5cbf" class="nf md it bd me ng nh dn mi ni nj dp mm kr nk nl mo kv nm nn mq kz no np ms nq bi translated"><strong class="ak">招数1:条件独立性测试</strong></h2><p id="8cc6" class="pw-post-body-paragraph ki kj it kk b kl mu ju kn ko mv jx kq kr mw kt ku kv mx kx ky kz my lb lc ld im bi translated">这些最早的因果发现算法之一是以其作者Peter Spirtes和Clark Glymour命名的<strong class="kk iu"> PC算法</strong>。这种算法(以及其他类似的算法)使用了这样一种思想:两个统计上独立的变量<strong class="kk iu"/><strong class="kk iu">没有因果联系</strong>。PC算法说明了第一个技巧。下图给出了算法的概要。关于PC算法的更多细节，我推荐参考文献[<a class="ae le" href="https://www.frontiersin.org/articles/10.3389/fgene.2019.00524/full" rel="noopener ugc nofollow" target="_blank">2</a>&amp;[<a class="ae le" href="https://www.jstatsoft.org/article/view/v047i11" rel="noopener ugc nofollow" target="_blank">6</a>]中的讨论。</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi og"><img src="../Images/a75aec4a2be1a28bbfa51fd6b7eb8417.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZpHPc9XC3qJJvZmx4M0FIA.png"/></div></div><p class="lr ls gj gh gi lt lu bd b be z dk translated">PC算法概要<a class="ae le" href="https://www.frontiersin.org/articles/10.3389/fgene.2019.00524/full" rel="noopener ugc nofollow" target="_blank"> 2 </a>、<a class="ae le" href="https://www.jstatsoft.org/article/view/v047i11" rel="noopener ugc nofollow" target="_blank"> 6 </a>。图片作者。</p></figure><p id="ac9c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">第一步是使用数据集中的每个变量形成一个完全连通的无向图。接下来，如果相应的变量是独立的，则删除边。然后，连接的边经受条件独立性测试，例如上图中以中间节点为条件的底部和最右侧节点的独立性测试(步骤2)。</p><p id="a726" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如果对一个变量的条件化消除了依赖性，则该变量被添加到这两个变量的分离集中。根据图表的大小，条件独立性测试将继续进行(即更多变量的条件)，直到不再有候选测试。</p><p id="3c0e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">接下来，碰撞器(即X → Y ← Z)基于节点对的分离集被定向。最后，剩余的边基于2个约束被定向，1)没有新的v-结构，以及2)不能形成定向的环。</p><h2 id="2308" class="nf md it bd me ng nh dn mi ni nj dp mm kr nk nl mo kv nm nn mq kz no np ms nq bi translated"><strong class="ak">招数二:图形空间的贪婪搜索</strong></h2><p id="8800" class="pw-post-body-paragraph ki kj it kk b kl mu ju kn ko mv jx kq kr mw kt ku kv mx kx ky kz my lb lc ld im bi translated">这个技巧有三个主要元素:一个<strong class="kk iu">图</strong>，一个<strong class="kk iu">图空间</strong>，以及一个<strong class="kk iu">贪婪搜索</strong>。</p><p id="b880" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">图</strong>是由边(线)连接的顶点(圆)组成的数学结构。因果模型通常使用一种特殊的图，称为有向无环图(DAG)。图和Dag在<a class="ae le" rel="noopener" target="_blank" href="/causality-an-introduction-f8a3f6ac4c4a"> <strong class="kk iu">第一帖</strong> </a>中讨论过。简而言之，DAG是一种顶点之间的边带有箭头并且不存在循环的图。</p><p id="843f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">一个<strong class="kk iu">图空间</strong>是<strong class="kk iu">图的集合</strong>。这只是一种形式化的奇特方式，即对于给定数量的顶点和边，有许多可能的图。例如，一个有2个顶点和1条边的DAG可以采用以下形式:A → B或B → A。请注意，这是前面冰块逆问题的另一个例子。然而，不是多个冰块可能产生相同的水坑，而是多个Dag可以具有相同数量的节点。</p><p id="b257" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">最后，<strong class="kk iu">贪婪搜索</strong>是一种导航空间的方式，这样你总是<strong class="kk iu">根据当地环境</strong>向<em class="mz">看起来</em>最有利的方向移动。这就像在森林里迷路了，试图通过向开阔地带移动来走出去。这种方法的问题是，你不能保证找到穿过空间的最佳路径，如下图所示。</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi oh"><img src="../Images/f247a0729a8e1e3b4203e85b46b2d076.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*V_M6a9LgVK_fwvaB0p4Ddg.png"/></div></div><p class="lr ls gj gh gi lt lu bd b be z dk translated">说明贪婪的搜索可能会出错。在这里，当试图走出森林时，对向开阔区域移动的贪婪搜索将导致次优路径。图片作者。</p></figure><p id="ea4b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">虽然贪婪搜索不能保证最优解，但对于大多数问题来说，可能Dag的空间如此之大，以至于很难找到真正的最优解。</p><p id="d281" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">贪婪等价搜索(GES) </strong>算法使用了这个技巧。GES从一个空图开始，迭代地添加有向边，从而最大化模型适应度(即分数)的改进。一个示例分数是贝叶斯信息标准(BIC) [ <a class="ae le" href="https://www.frontiersin.org/articles/10.3389/fgene.2019.00524/full" rel="noopener ugc nofollow" target="_blank"> 2 </a> ]。</p><h2 id="18b7" class="nf md it bd me ng nh dn mi ni nj dp mm kr nk nl mo kv nm nn mq kz no np ms nq bi translated"><strong class="ak">招数3:利用不对称</strong></h2><p id="cf2f" class="pw-post-body-paragraph ki kj it kk b kl mu ju kn ko mv jx kq kr mw kt ku kv mx kx ky kz my lb lc ld im bi translated">正如<a class="ae le" rel="noopener" target="_blank" href="/causality-an-introduction-f8a3f6ac4c4a"> <strong class="kk iu">第一帖</strong> </a>中所说，因果关系的一个根本属性是不对称。a可能导致B，但B可能不会导致a。有大量算法利用这一思想在因果模型候选之间进行选择。我将把这些不对称归类为三种常见的味道: <strong class="kk iu">时间、复杂性和功能性</strong>。在描述这些不对称之前，重要的是要注意它们并不相互排斥。也就是说，算法可以混合搭配不同的口味。</p><p id="dd65" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">时间不对称对我们来说是很自然的。它的理念是<strong class="kk iu">原因发生在结果</strong>之前。这是诸如<strong class="kk iu">格兰杰因果关系</strong>等观点的核心。尽管格兰杰因果关系不足以说明因果关系，但它利用了因果关系的概念。它在两个变量的情况下(例如X和Y)通过在给定Y和X的过去信息的情况下量化预测Y的增益来做到这一点，这与单独Y的过去信息相反[ <a class="ae le" href="https://www.sciencedirect.com/science/article/abs/pii/0165188988900553?via%3Dihub" rel="noopener ugc nofollow" target="_blank"> 5 </a> ]。</p><p id="f0fc" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">复杂性不对称</strong>遵循奥卡姆剃刀原理，即<strong class="kk iu">模型越简单越好</strong>。换句话说，如果你有几个候选模型可以选择，这个想法就是选择最简单的一个。量化简单性(或复杂性)的一种方法是Kolmogorov复杂性。</p><p id="e3d1" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">功能不对称</strong>假设<strong class="kk iu">更符合</strong>关系<strong class="kk iu">的模型是更好的候选模型</strong>。例如，给定两个变量X和Y，<strong class="kk iu">非线性加性噪声模型(NANM) </strong>在X和Y之间执行非线性回归，例如y = f(x) + n，其中n =噪声/残差，在两个方向上。如果潜在原因(例如x)与噪声项(例如n)无关，则该模型(即因果关系)被接受。关于NANM的更多细节可以在参考文献[ <a class="ae le" href="https://webdav.tuebingen.mpg.de/causality/NIPS2008-Hoyer.pdf" rel="noopener ugc nofollow" target="_blank"> 7 </a> ]中找到。</p><h1 id="774f" class="mc md it bd me mf na mh mi mj nb ml mm jz nc ka mo kc nd kd mq kf ne kg ms mt bi translated">示例:通过人口普查数据发现因果关系</h1><p id="d3d1" class="pw-post-body-paragraph ki kj it kk b kl mu ju kn ko mv jx kq kr mw kt ku kv mx kx ky kz my lb lc ld im bi translated">我们现在从理论转向一个具体的例子。这个例子使用了<a class="ae le" href="https://fentechsolutions.github.io/CausalDiscoveryToolbox/html/index.html" rel="noopener ugc nofollow" target="_blank">因果发现工具箱</a>，一个用于因果发现的Python库[ <a class="ae le" href="https://fentechsolutions.github.io/CausalDiscoveryToolbox/html/index.html" rel="noopener ugc nofollow" target="_blank"> 4 </a> ]。在这个例子中，我们从和以前一样的来源查看人口普查数据。</p><p id="dbf5" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在<a class="ae le" rel="noopener" target="_blank" href="/causal-inference-962ae97cefda">之前的例子</a>中，我们假设收入有两个原因:年龄和教育。然而，<em class="mz">我们怎么能肯定这个假设站得住脚呢？</em>在本例中，我们探索了包含更多变量并利用数据驱动方法的替代因果模型:因果发现。示例代码和数据可以在<a class="ae le" href="https://github.com/ShawhinT/YouTube-Blog/tree/main/causal_discovery" rel="noopener ugc nofollow" target="_blank"> GitHub repo </a>中找到。</p><p id="55a1" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">首先，我们加载必要的库和数据。如果您没有这些库，请查看回购中的<a class="ae le" href="https://github.com/ShawhinT/YouTube-Blog/blob/main/causal_discovery/requirements.txt" rel="noopener ugc nofollow" target="_blank"> requirements.txt </a>。数据也在回购中。</p><pre class="lg lh li lj gt oi oj ok ol aw om bi"><span id="e1df" class="nf md it oj b gy on oo l op oq"># import libraries<br/>import pickle<br/>import cdt<br/>import networkx as nx<br/>import matplotlib.pyplot as plt</span><span id="6fec" class="nf md it oj b gy or oo l op oq"># load data<br/>df = pickle.load( open( "df_causal_discovery.p", "rb") )</span></pre><p id="4268" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">接下来，我们需要为我们的因果模型构建一个框架。这基本上是前面讨论的PC算法的第二步。换句话说，进行成对独立性检验，被认为是统计相关的变量得到一个双向边。</p><pre class="lg lh li lj gt oi oj ok ol aw om bi"><span id="c81d" class="nf md it oj b gy on oo l op oq"># Get skeleton graph<br/># initialize graph lasso<br/>glasso = cdt.independence.graph.Glasso()</span><span id="4995" class="nf md it oj b gy or oo l op oq"># apply graph lasso to data<br/>skeleton = glasso.predict(df)</span><span id="cdf0" class="nf md it oj b gy or oo l op oq"># visualize network<br/>fig = plt.figure(figsize=(15,10))<br/>nx.draw_networkx(skeleton, font_size=18, font_color='r')</span></pre><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi os"><img src="../Images/a617d7520733df62ffc78618c31b26dd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CeZyqHF5T-KkEsDkgSHZgQ.png"/></div></div><p class="lr ls gj gh gi lt lu bd b be z dk translated">骨架图。图片作者。</p></figure><p id="ea2a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">注意我们有7个不同的变量。之前我们只考虑年龄，学历(hasGraduateDegree)，收入(greaterThan50k)。仅从骨架中，我们可以看到存在于这三个变量之间的统计相关性。</p><p id="5dbe" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在是因果发现的时候了。在这里，我们尝试了3种不同的算法:PC，GES和LiNGAM。</p><pre class="lg lh li lj gt oi oj ok ol aw om bi"><span id="a628" class="nf md it oj b gy on oo l op oq"># Use causal discovery to get causal models</span><span id="aa88" class="nf md it oj b gy or oo l op oq"># PC algorithm<br/>model_pc <strong class="oj iu">=</strong> cdt<strong class="oj iu">.</strong>causality<strong class="oj iu">.</strong>graph<strong class="oj iu">.</strong>PC()<br/>graph_pc <strong class="oj iu">=</strong> model_pc<strong class="oj iu">.</strong>predict(df, skeleton)<br/><br/><em class="mz"># visualize network</em><br/>fig<strong class="oj iu">=</strong>plt<strong class="oj iu">.</strong>figure(figsize<strong class="oj iu">=</strong>(15,10))<br/>nx<strong class="oj iu">.</strong>draw_networkx(graph_pc, font_size<strong class="oj iu">=</strong>18, font_color<strong class="oj iu">=</strong>'r')</span></pre><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi ot"><img src="../Images/d48d871e8f02a027a3df969d3d50c466.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*23OxW5136ZgsjZVy6EtX2A.png"/></div></div><p class="lr ls gj gh gi lt lu bd b be z dk translated">从PC算法输出因果图。图片作者。</p></figure><p id="8c4c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">PC给出了一个稍微合理的结果。我们在上一个例子中使用的简单的3变量DAG是<em class="mz">几乎</em>嵌入在这个图中。年龄和hasGraduateDegree之间的箭头颠倒了，这是有问题的，因为这表明拥有研究生学位对一个人的年龄有因果关系！</p><p id="f563" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">它还暗示了额外的因果关系。也就是说，hasGraduateDegree，greaterThan50k，age和isFemale都对每周工作时间有单向因果影响，这本身对inRelationship有因果影响。接下来，我们看看GES。</p><pre class="lg lh li lj gt oi oj ok ol aw om bi"><span id="e0c1" class="nf md it oj b gy on oo l op oq"># GES algorithm<em class="mz"><br/></em>model_ges <strong class="oj iu">=</strong> cdt<strong class="oj iu">.</strong>causality<strong class="oj iu">.</strong>graph<strong class="oj iu">.</strong>GES()<br/>graph_ges <strong class="oj iu">=</strong> model_ges<strong class="oj iu">.</strong>predict(df, skeleton)<br/><br/><em class="mz"># visualize network</em><br/>fig<strong class="oj iu">=</strong>plt<strong class="oj iu">.</strong>figure(figsize<strong class="oj iu">=</strong>(15,10))<br/>nx<strong class="oj iu">.</strong>draw_networkx(graph_ges, font_size<strong class="oj iu">=</strong>18, font_color<strong class="oj iu">=</strong>'r')</span></pre><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi ou"><img src="../Images/651e540f800c49f8319735036aa8f98a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XETzhDx2a44PpDW5hhDPPw.png"/></div></div><p class="lr ls gj gh gi lt lu bd b be z dk translated">从GES算法输出因果图。图片作者。</p></figure><p id="6c7e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">该图似乎与PC结果基本一致。值得注意的区别是，以前源于isWhite的双向边现在是单向的。回想一下GES的工作方式是有道理的。</p><p id="28b4" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">最后，我们将LiNGAM应用于数据，该数据使用我称之为线性模型之间的功能不对称来推断因果关系[ <a class="ae le" href="https://www.jmlr.org/papers/volume7/shimizu06a/shimizu06a.pdf" rel="noopener ugc nofollow" target="_blank"> 9 </a> ]。</p><pre class="lg lh li lj gt oi oj ok ol aw om bi"><span id="365a" class="nf md it oj b gy on oo l op oq"># LiNGAM Algorithm<br/>model_lingam <strong class="oj iu">=</strong> cdt<strong class="oj iu">.</strong>causality<strong class="oj iu">.</strong>graph<strong class="oj iu">.</strong>LiNGAM()<br/>graph_lingam <strong class="oj iu">=</strong> model_lingam<strong class="oj iu">.</strong>predict(df)<br/><br/><em class="mz"># visualize network</em><br/>fig<strong class="oj iu">=</strong>plt<strong class="oj iu">.</strong>figure(figsize<strong class="oj iu">=</strong>(15,10))<br/>nx<strong class="oj iu">.</strong>draw_networkx(graph_lingam, font_size<strong class="oj iu">=</strong>18, font_color<strong class="oj iu">=</strong>'r')</span></pre><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi ov"><img src="../Images/f04e2eb27f92d025699b5d4f33120f8d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vxs1_13BkorvIK-TO9Xesg.png"/></div></div><p class="lr ls gj gh gi lt lu bd b be z dk translated">从LiNGAM算法输出因果图。图片作者。</p></figure><p id="3b33" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这个结果与前两个有很大的不同。许多边缘方向已经翻转，例如在大于50k的<strong class="kk iu">和具有渐变度的<strong class="kk iu">之间的方向。性能不佳可能是由于违反了算法对线性效应的假设。这里的大多数变量都是二元的，因此线性结构方程可能不合适。</strong></strong></p><figure class="lg lh li lj gt lk"><div class="bz fp l di"><div class="ow ox l"/></div></figure><h1 id="2cff" class="mc md it bd me mf na mh mi mj nb ml mm jz nc ka mo kc nd kd mq kf ne kg ms mt bi translated">结论</h1><p id="d38b" class="pw-post-body-paragraph ki kj it kk b kl mu ju kn ko mv jx kq kr mw kt ku kv mx kx ky kz my lb lc ld im bi translated">关于因果关系的三部分系列到此结束。在这篇文章中，我试图从因果发现中勾勒出大的想法，同时仍然为实际应用提供足够的细节。尽管我无法在一篇简短的博客文章中对因果发现进行全面的回顾，但我已经尝试在下面的<strong class="kk iu">资源</strong>部分整理了一份好的参考资料列表。尽管还很年轻，但因果发现是一个充满希望的领域，可能有助于弥合机器和人类知识之间的差距。</p><div class="oy oz gp gr pa pb"><a rel="noopener follow" target="_blank" href="/causal-inference-962ae97cefda"><div class="pc ab fo"><div class="pd ab pe cl cj pf"><h2 class="bd iu gy z fp pg fr fs ph fu fw is bi translated">因果推理</h2><div class="pi l"><h3 class="bd b gy z fp pg fr fs ph fu fw dk translated">用Python回答因果问题</h3></div><div class="pj l"><p class="bd b dl z fp pg fr fs ph fu fw dk translated">towardsdatascience.com</p></div></div><div class="pk l"><div class="pl l pm pn po pk pp lp pb"/></div></div></a></div></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h1 id="bb01" class="mc md it bd me mf mg mh mi mj mk ml mm jz mn ka mo kc mp kd mq kf mr kg ms mt bi translated">资源</h1><p id="4099" class="pw-post-body-paragraph ki kj it kk b kl mu ju kn ko mv jx kq kr mw kt ku kv mx kx ky kz my lb lc ld im bi translated"><strong class="kk iu">更多关于因果关系</strong> : <a class="ae le" href="https://shawhin.medium.com/understanding-causal-effects-37a054b2ec3b" rel="noopener">因果关系概述</a> | <a class="ae le" rel="noopener" target="_blank" href="/causality-an-introduction-f8a3f6ac4c4a">因果关系:简介</a> | <a class="ae le" rel="noopener" target="_blank" href="/causal-inference-962ae97cefda">因果推断</a> | <a class="ae le" rel="noopener" target="_blank" href="/causal-discovery-6858f9af6dcb">因果发现</a></p><p id="d84f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">连接</strong> : <a class="ae le" href="https://shawhint.github.io/" rel="noopener ugc nofollow" target="_blank">我的网站</a> | <a class="ae le" href="https://calendly.com/shawhintalebi" rel="noopener ugc nofollow" target="_blank">预定电话</a> | <a class="ae le" href="https://shawhint.github.io/connect.html" rel="noopener ugc nofollow" target="_blank">邮件我</a></p><p id="1e0b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">社交</strong>:<a class="ae le" href="https://www.youtube.com/channel/UCa9gErQ9AE5jT2DZLjXBIdA" rel="noopener ugc nofollow" target="_blank">YouTube</a>|<a class="ae le" href="https://www.linkedin.com/in/shawhintalebi/" rel="noopener ugc nofollow" target="_blank">LinkedIn</a>|<a class="ae le" href="https://twitter.com/ShawhinT" rel="noopener ugc nofollow" target="_blank">Twitter</a></p><p id="e857" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">支持</strong> : <a class="ae le" href="https://www.buymeacoffee.com/shawhint?source=about_page-------------------------------------" rel="noopener ugc nofollow" target="_blank">给我买杯咖啡</a> ☕️ | <a class="ae le" href="https://shawhin.medium.com/membership?source=about_page-------------------------------------" rel="noopener">成为会员</a> ⭐️</p><div class="oy oz gp gr pa pb"><a href="https://shawhin.medium.com/membership" rel="noopener follow" target="_blank"><div class="pc ab fo"><div class="pd ab pe cl cj pf"><h2 class="bd iu gy z fp pg fr fs ph fu fw is bi translated">通过我的推荐链接加入媒体——肖文·塔莱比</h2><div class="pi l"><h3 class="bd b gy z fp pg fr fs ph fu fw dk translated">⭐️成为会员使用我的推荐链接，并获得充分的媒体上的每一篇文章。您的会员费直接…</h3></div><div class="pj l"><p class="bd b dl z fp pg fr fs ph fu fw dk translated">shawhin.medium.com</p></div></div><div class="pk l"><div class="pq l pm pn po pk pp lp pb"/></div></div></a></div></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><p id="dce0" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">《为什么之书:因果的新科学》朱迪亚·珀尔著</p><p id="023d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">[2]基于图形模型的因果发现方法综述。https://doi.org/10.3389/fgene.2019.00524<a class="ae le" href="https://www.frontiersin.org/articles/10.3389/fgene.2019.00524/full" rel="noopener ugc nofollow" target="_blank"/></p><p id="c843" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">[3]基于ML的因果发现:<a class="ae le" href="https://arxiv.org/abs/1803.04929" rel="noopener ugc nofollow" target="_blank">arXiv:1803.04929</a><strong class="kk iu">【stat。ML] </strong>(好的引物)</p><p id="9283" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">[4]因果发现工具箱文档:<a class="ae le" href="https://fentechsolutions.github.io/CausalDiscoveryToolbox/html/index.html" rel="noopener ugc nofollow" target="_blank">https://fentechsolutions . github . io/CausalDiscoveryToolbox/html/index . html</a></p><p id="58d9" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">[5]格兰杰因果关系:<a class="ae le" href="https://doi.org/10.1016/0165-1889(88)90055-3" rel="noopener ugc nofollow" target="_blank">https://doi . org/10.1016/0165-1889(88)90055-3</a></p><p id="d76d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">[6]因果推断用R包pcalg:<a class="ae le" href="https://www.jstatsoft.org/article/view/v047i11" rel="noopener ugc nofollow" target="_blank">http://dx.doi.org/10.18637/jss.v047.i11</a>(好的引子)</p><p id="573b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">[7]非线性加性噪声模型<a class="ae le" href="https://webdav.tuebingen.mpg.de/causality/NIPS2008-Hoyer.pdf" rel="noopener ugc nofollow" target="_blank">论文</a></p><p id="2869" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">[8] UCI机器学习知识库:<a class="ae le" href="https://archive.ics.uci.edu/ml/datasets/census+income" rel="noopener ugc nofollow" target="_blank">数据来源</a></p><p id="ce4f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">[9] LiNGAM <a class="ae le" href="https://www.cs.helsinki.fi/group/neuroinf/lingam/JMLR06.pdf" rel="noopener ugc nofollow" target="_blank">论文</a></p><p id="e379" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">[10]摘自纳西姆·尼古拉斯·塔勒布<a class="ae le" href="https://amzn.to/3vJAUtM" rel="noopener ugc nofollow" target="_blank">反脆弱的例子(付费链接)</a></p></div></div>    
</body>
</html>