<html>
<head>
<title>Predicting Wine Prices with Tuned Gradient Boosted Trees</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用调谐梯度提升树预测葡萄酒价格</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/predicting-wine-prices-with-tuned-gradient-boosted-trees-9ab5ebd0b85e?source=collection_archive---------40-----------------------#2021-09-27">https://towardsdatascience.com/predicting-wine-prices-with-tuned-gradient-boosted-trees-9ab5ebd0b85e?source=collection_archive---------40-----------------------#2021-09-27</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="3221" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">利用Optuna寻找最佳超参数组合</h2></div><h1 id="0b0b" class="ki kj it bd kk kl km kn ko kp kq kr ks jz kt ka ku kc kv kd kw kf kx kg ky kz bi translated">什么是超参数调谐？</h1><p id="dcbb" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">许多流行的机器学习库使用超参数的概念。这些可以被认为是机器学习模型的<strong class="lc iu">配置设置</strong>或<strong class="lc iu">控制</strong>。虽然在拟合模型的过程中学习或求解了许多参数(比如回归系数)，但有些输入需要数据科学家预先指定值。这些是超参数，然后用于建立和训练模型。</p><p id="9882" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">梯度推进决策树的一个例子是决策树的深度。较高的值可能会产生更复杂的树，可以提取某些关系，而较小的树可能能够更好地进行概括，并避免过度拟合我们的结果，这可能会导致预测未知数据时出现问题。这只是超参数的一个例子，许多模型都有许多这样的输入，它们都必须由数据科学家定义，或者使用代码库提供的默认值。</p><p id="2b02" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">这可能看起来很困难— <strong class="lc iu"> <em class="mb">我们如何知道哪种超参数组合会产生最准确的模型呢？</em> </strong>手动调优(寻找最佳组合)可能需要很长时间，并且覆盖很小的样本空间。这里将介绍的一种方法是使用<a class="ae mc" href="https://github.com/optuna/optuna" rel="noopener ugc nofollow" target="_blank"> Optuna </a>来自动完成一些工作。可以指定超参数的范围，而不是手动测试组合，Optuna进行了一项研究，以确定给定时间限制下的最佳组合。</p><h1 id="b5b1" class="ki kj it bd kk kl km kn ko kp kq kr ks jz kt ka ku kc kv kd kw kf kx kg ky kz bi translated">数据集概述</h1><p id="135b" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">为了演示Optuna和hyperparameter调优，我们将使用一个包含来自<a class="ae mc" href="https://www.kaggle.com/mysarahmadbhat/wine-tasting" rel="noopener ugc nofollow" target="_blank"> Kaggle </a>的葡萄酒评级和价格的数据集。给定一瓶红酒的一些输入特征——比如地区、点数和品种——使用超参数调优，我们能在多大程度上预测葡萄酒的价格？</p><figure class="me mf mg mh gt mi gh gi paragraph-image"><div role="button" tabindex="0" class="mj mk di ml bf mm"><div class="gh gi md"><img src="../Images/0f730463847727e61ffdb1dcc9e208d3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*VRdWEnmppY0ZlZTc.png"/></div></div><p class="mp mq gj gh gi mr ms bd b be z dk translated">数据集概述<a class="ae mc" href="https://datastud.dev/media/optuna_hyperparameter/dataset_overview.png" rel="noopener ugc nofollow" target="_blank">点击查看全尺寸版本</a></p></figure><p id="6364" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">在我们的数据中加载几行代码，并进行训练/测试分割:</p><pre class="me mf mg mh gt mt mu mv mw aw mx bi"><span id="60a8" class="my kj it mu b gy mz na l nb nc"># Read in data from local csv<br/>df = pd.read_csv('winemag-data-130k-v2.csv')<br/><br/># Choose just a few features for demonstration, infer categorical features<br/>feature_cols = ['country', 'points', 'province', 'region_1', 'region_2', 'taster_name', 'variety', 'winery']<br/>cat_features = [col for col in feature_cols if df[col].dtype == 'object']<br/>for col in cat_features:<br/>    df[col] = df[col].fillna('Other')<br/>target_col = 'price'<br/><br/># Train test split<br/>train_df, test_df = train_test_split(df, test_size=0.3, shuffle=False)<br/><br/>train_x = train_df.loc[:, feature_cols]<br/>train_y = train_df.loc[:, target_col]<br/><br/>test_x = test_df.loc[:, feature_cols]<br/>test_y = test_df.loc[:, target_col]</span></pre><h1 id="317a" class="ki kj it bd kk kl km kn ko kp kq kr ks jz kt ka ku kc kv kd kw kf kx kg ky kz bi translated">模特培训</h1><h2 id="3b0c" class="my kj it bd kk nd ne dn ko nf ng dp ks lj nh ni ku ln nj nk kw lr nl nm ky nn bi translated">基线模型</h2><p id="f85d" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">为了知道我们的超参数优化是否有帮助，我们将训练几个基线模型。第一种是采用简单的平均价格。使用这种方法的结果是79%的平均绝对百分比误差——不是很好，希望一些机器学习模型可以改善我们的预测！</p><p id="9d49" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">第二个基线是用默认参数训练我们的模型(使用<a class="ae mc" href="https://catboost.ai/docs/concepts/python-reference_catboostregressor.html" rel="noopener ugc nofollow" target="_blank"> Catboost库</a>)。下面是几行代码。这击败了我们的基线简单均值预测，但我们能通过进一步优化做得更好吗？</p><pre class="me mf mg mh gt mt mu mv mw aw mx bi"><span id="42f0" class="my kj it mu b gy mz na l nb nc"># Train a model with default parameters and score<br/>model = CatBoostRegressor(loss_function = 'RMSE', eval_metric='RMSE', verbose=False, cat_features=cat_features, random_state=42)<br/>default_train_score = np.mean(eda.cross_validate_custom(train_x, train_y, model, mean_absolute_percentage_error))<br/>print('Training with default parameters results in a training score of {:.3f}.'.format(default_train_score))</span><span id="acb5" class="my kj it mu b gy no na l nb nc">Output: Training with default parameters results in a training score of 0.298.</span></pre><h1 id="a8db" class="ki kj it bd kk kl km kn ko kp kq kr ks jz kt ka ku kc kv kd kw kf kx kg ky kz bi translated">超参数优化模型</h1><h2 id="627a" class="my kj it bd kk nd ne dn ko nf ng dp ks lj nh ni ku ln nj nk kw lr nl nm ky nn bi translated">设置优化研究</h2><p id="5ed8" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">为了使用优化的超参数创建我们的模型，我们创建了Optuna所谓的研究——这允许我们定义具有超参数范围的试验，并优化最佳组合。</p><p id="a25d" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">您将在下面的代码中看到，我们用一个试验对象定义了一个目标函数，它根据我们定义的范围建议超参数。然后，我们创建研究并进行优化，让Optuna完成它的工作。</p><pre class="me mf mg mh gt mt mu mv mw aw mx bi"><span id="e07e" class="my kj it mu b gy mz na l nb nc">def objective(trial):<br/><br/>    # Define parameter dictionary used to build catboost model<br/>    params = {<br/>        'loss_function': 'RMSE',<br/>        'eval_metric': 'RMSE',<br/>        'verbose': False,<br/>        'cat_features': cat_features,<br/>        'random_state': 42,<br/>        'learning_rate': trial.suggest_float('learning_rate', 0.001, 0.2),<br/>        'depth': trial.suggest_int('depth', 2, 12),<br/>        'n_estimators': trial.suggest_int('n_estimators', 100, 1000, step=50)<br/>    }<br/>    <br/>    # Build and score model<br/>    clf = CatBoostRegressor(**params)<br/>    score = np.mean(eda.cross_validate_custom(train_x, train_y, clf, mean_absolute_percentage_error))<br/><br/>    return score</span></pre><h2 id="f4ed" class="my kj it bd kk nd ne dn ko nf ng dp ks lj nh ni ku ln nj nk kw lr nl nm ky nn bi translated">查看结果</h2><p id="ace1" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">Optuna将最佳结果存储在我们的学习对象中。运行以下程序可以让我们访问最佳试用和查看培训结果。</p><pre class="me mf mg mh gt mt mu mv mw aw mx bi"><span id="fb95" class="my kj it mu b gy mz na l nb nc"># Grab best trial from optuna study<br/>best_trial_optuna = study.best_trial<br/>print('Best score {:.3f}. Params {}'.format(best_trial_optuna.value, best_trial_optuna.params))</span><span id="ff59" class="my kj it mu b gy no na l nb nc">Output: Best score 0.288. Params {'learning_rate': 0.0888813729642258 'depth': 12 'n_estimators': 800}</span></pre><h2 id="ed0e" class="my kj it bd kk nd ne dn ko nf ng dp ks lj nh ni ku ln nj nk kw lr nl nm ky nn bi translated">与默认参数比较</h2><p id="2219" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">将训练结果与我们初始运行的默认参数进行快速比较，显示出良好的迹象。您将看到优化的模型具有更好的训练拟合度(在这种情况下，分数是百分比误差，因此越低=越好)。</p><pre class="me mf mg mh gt mt mu mv mw aw mx bi"><span id="572b" class="my kj it mu b gy mz na l nb nc"># Compare best trial vs. default parameters<br/>print('Default parameters resulted in a score of {:.3f} vs. Optuna hyperparameter optimization score of {:.3f}.'.format(default_train_score, best_trial_optuna.value))</span><span id="03d8" class="my kj it mu b gy no na l nb nc">Output: Default parameters resulted in a score of 0.298 vs. Optuna hyperparameter optimization score of 0.288.</span></pre><h2 id="677d" class="my kj it bd kk nd ne dn ko nf ng dp ks lj nh ni ku ln nj nk kw lr nl nm ky nn bi translated">分析优化趋势</h2><p id="bf12" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">一个很好的例子是平行坐标图。这使我们能够观察试验并分析潜在趋势的超参数。如果我们发现有趣的优化，我们可能希望在审查结果后运行新的研究，允许我们搜索额外的超参数空间。</p><pre class="me mf mg mh gt mt mu mv mw aw mx bi"><span id="f626" class="my kj it mu b gy mz na l nb nc"># Visualize results to spot any hyperparameter trends<br/>plot_parallel_coordinate(study)</span></pre><figure class="me mf mg mh gt mi gh gi paragraph-image"><div role="button" tabindex="0" class="mj mk di ml bf mm"><div class="gh gi np"><img src="../Images/538cc9e49852639ca52c7b1fe2de00bb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*eLfR3hU03VN43Ssj.png"/></div></div><p class="mp mq gj gh gi mr ms bd b be z dk translated">平行坐标图<a class="ae mc" href="https://datastud.dev/media/optuna_hyperparameter/paralell_coordinates_plot.png" rel="noopener ugc nofollow" target="_blank">点击查看全尺寸版本</a></p></figure><p id="b7a5" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">您可以在左侧看到成本指标(越低=越好)。沿着黑线(最佳试验)，您会注意到深度越高效果越好，学习率在测试值的中间，并且有更多的估计值。鉴于这些发现，我们可以重新运行一项研究，缩小这些值的范围，并潜在地扩大其他值的范围——例如深度可能增加到我们的上限以上，或者添加额外的超参数进行调整。</p><h1 id="d36b" class="ki kj it bd kk kl km kn ko kp kq kr ks jz kt ka ku kc kv kd kw kf kx kg ky kz bi translated">比较测试结果</h1><p id="7d21" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">最后一步是比较测试结果。第一步是观察我们的简单平均预测基线在测试集上的表现。</p><pre class="me mf mg mh gt mt mu mv mw aw mx bi"><span id="1ad3" class="my kj it mu b gy mz na l nb nc"># Run baseline model (default predicting mean)<br/>preds_baseline = np.zeros_like(test_y)<br/>preds_baseline = np.mean(train_y) + preds_baseline<br/>baseline_model_score = mean_absolute_percentage_error(test_y, preds_baseline)<br/>print('Baseline score (mean) is {:.2f}.'.format(baseline_model_score))</span><span id="037b" class="my kj it mu b gy no na l nb nc">Output: Baseline score (mean) is 0.79.</span></pre><p id="5387" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">下一步是查看我们默认超参数模型的测试结果:</p><pre class="me mf mg mh gt mt mu mv mw aw mx bi"><span id="071e" class="my kj it mu b gy mz na l nb nc"># Rerun default model on full training set and score on test set<br/>simple_model = model.fit(train_x, train_y)<br/>simple_model_score = mean_absolute_percentage_error(test_y, model.predict(test_x))<br/>print('Default parameter model score is {:.2f}.'.format(simple_model_score))</span><span id="f48d" class="my kj it mu b gy no na l nb nc">Output: Default parameter model score is 0.30.</span></pre><p id="1b04" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">比简单地用平均值作为预测要好很多。我们的超参数优化解决方案能在测试集上做得更好吗？</p><pre class="me mf mg mh gt mt mu mv mw aw mx bi"><span id="54df" class="my kj it mu b gy mz na l nb nc"># Rerun optimized model on full training set and score on test set<br/>params = best_trial_optuna.params<br/>params['loss_function'] = 'RMSE'<br/>params['eval_metric'] ='RMSE'<br/>params['verbose'] = False<br/>params['cat_features'] = cat_features<br/>params['random_state'] = 42<br/>opt_model = CatBoostRegressor(**params)<br/>opt_model.fit(train_x, train_y)<br/>opt_model_score = mean_absolute_percentage_error(test_y, opt_model.predict(test_x))<br/>print('Optimized model score is {:.2f}.'.format(opt_model_score))</span><span id="188e" class="my kj it mu b gy no na l nb nc">Output: Optimized model score is 0.29.</span></pre><p id="48f7" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">我们能够通过超参数优化改进我们的模型！我们只在一个小空间里搜索了几次，但是改进了我们的成本度量，得到了更好的分数(误差降低了1%)。</p><p id="6571" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated"><em class="mb">所有的例子和文件都可以在</em> <a class="ae mc" href="https://github.com/bstuddard/python-examples/tree/master/hyperparameter-optimization/optuna" rel="noopener ugc nofollow" target="_blank"> <em class="mb"> Github </em> </a> <em class="mb">上找到。</em></p></div><div class="ab cl nq nr hx ns" role="separator"><span class="nt bw bk nu nv nw"/><span class="nt bw bk nu nv nw"/><span class="nt bw bk nu nv"/></div><div class="im in io ip iq"><p id="2a7f" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated"><em class="mb">原发布于</em><a class="ae mc" href="https://datastud.dev/posts/wine-optuna-hyperparameter" rel="noopener ugc nofollow" target="_blank"><em class="mb">https://data stud . dev</em></a><em class="mb">。</em></p></div></div>    
</body>
</html>