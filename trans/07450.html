<html>
<head>
<title>What is a Variational Autoencoder?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">什么是变分自动编码器？</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/what-is-a-variational-autoencoder-9b41bd63f65e?source=collection_archive---------17-----------------------#2021-07-07">https://towardsdatascience.com/what-is-a-variational-autoencoder-9b41bd63f65e?source=collection_archive---------17-----------------------#2021-07-07</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="ca97" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsdatascience.com/tagged/hands-on-tutorials" rel="noopener" target="_blank">实践教程</a></h2><div class=""/><div class=""><h2 id="c9ac" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">代码生成机器学习快速入门指南</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/198d4ee46f320f40c0ddf8f23b6226b4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tGDyLyPv9J13ZMLYhE2J9Q.jpeg"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated"><a class="ae lh" href="https://unsplash.com/@wqqq14?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">围棋熊</a>在<a class="ae lh" href="https://unsplash.com/s/photos/pagoda?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍照</p></figure><h1 id="1d4d" class="li lj it bd lk ll lm ln lo lp lq lr ls ki lt kj lu kl lv km lw ko lx kp ly lz bi translated">介绍</h1><p id="c05a" class="pw-post-body-paragraph ma mb it mc b md me kd mf mg mh kg mi mj mk ml mm mn mo mp mq mr ms mt mu mv im bi translated">生成机器学习在各种情况下都是有帮助的。例如，一种情况是有某种封闭形式或准封闭形式的解，但计算量很大。在这种情况下，神经网络可以提供具有可接受准确度的及时解决方案。然而，为了训练这样的网络，数据必须被聚集，并且在大多数情况下可能是稀缺的。一个很好的例子是奇异期权的定价。如果你想了解更多关于这个例子的信息，我会留下一个SSRN论文的链接…</p><div class="mw mx gp gr my mz"><a href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3881993" rel="noopener  ugc nofollow" target="_blank"><div class="na ab fo"><div class="nb ab nc cl cj nd"><h2 class="bd jd gy z fp ne fr fs nf fu fw jc bi translated">从波动表面跳到奇异期权价格</h2><div class="ng l"><h3 class="bd b gy z fp ne fr fs nf fu fw dk translated">撰写日期:2021年7月7日期权定价的波动特征方法(VFA)需要一个基础模型…</h3></div><div class="nh l"><p class="bd b dl z fp ne fr fs nf fu fw dk translated">papers.ssrn.com</p></div></div></div></a></div><p id="e72c" class="pw-post-body-paragraph ma mb it mc b md ni kd mf mg nj kg mi mj nk ml mm mn nl mp mq mr nm mt mu mv im bi translated">这个问题的解决方案可以通过变型自动编码器在生成式机器学习中找到。有了这样一个模型，我们可以从一组有代表性的群体数据中学习，并生成新的样本，这些样本可以作为更多的数据用于训练。本文将首先考虑自动编码器，给出一个直观的例子，并使用Python和Tensorflow的函数式API，通过本文展示的代码过渡到变化的自动编码器。</p></div><div class="ab cl nn no hx np" role="separator"><span class="nq bw bk nr ns nt"/><span class="nq bw bk nr ns nt"/><span class="nq bw bk nr ns"/></div><div class="im in io ip iq"><h1 id="48ff" class="li lj it bd lk ll nu ln lo lp nv lr ls ki nw kj lu kl nx km lw ko ny kp ly lz bi translated">自动编码器</h1><h2 id="6115" class="nz lj it bd lk oa ob dn lo oc od dp ls mj oe of lu mn og oh lw mr oi oj ly iz bi translated">前馈神经网络和图表</h2><p id="e92e" class="pw-post-body-paragraph ma mb it mc b md me kd mf mg mh kg mi mj mk ml mm mn mo mp mq mr ms mt mu mv im bi translated">理解变型自动编码器的最佳方式是首先理解标准自动编码器。如果你不熟悉标准的神经网络架构，我建议你看看这些文章，让你跟上速度…</p><ul class=""><li id="9adc" class="ok ol it mc b md ni mg nj mj om mn on mr oo mv op oq or os bi translated"><a class="ae lh" href="https://medium.com/swlh/artificial-intelligence-bootcamp-8745d61a9d25" rel="noopener">人工智能训练营</a></li><li id="837a" class="ok ol it mc b md ot mg ou mj ov mn ow mr ox mv op oq or os bi translated"><a class="ae lh" rel="noopener" target="_blank" href="/linear-regression-v-s-neural-networks-cd03b29386d4">线性回归vs .神经网络</a></li></ul><p id="5b4a" class="pw-post-body-paragraph ma mb it mc b md ni kd mf mg nj kg mi mj nk ml mm mn nl mp mq mr nm mt mu mv im bi translated">假设你在神经网络领域有基础，我们可以继续学习自动编码器的结构。</p><p id="b476" class="pw-post-body-paragraph ma mb it mc b md ni kd mf mg nj kg mi mj nk ml mm mn nl mp mq mr nm mt mu mv im bi translated">通常，前馈神经网络将具有输入层、在与输入相同的、更高维或更低维的空间中的一定数量的隐藏层、以及在相对于隐藏层和输入的更低维空间的输出层中的一些目标。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi oy"><img src="../Images/69399c2b09dc430b0c265743edb34be8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2HZgTnr6t9Yg8McisN8ofg.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">作者图片:前馈神经网络架构</p></figure><p id="0df4" class="pw-post-body-paragraph ma mb it mc b md ni kd mf mg nj kg mi mj nk ml mm mn nl mp mq mr nm mt mu mv im bi translated">这个概念相对简单，任何有构建神经网络经验的人可能会立即提出这个普遍性的反例，这不仅仅是公平的，然而，我离题了。此示例的目的是突出神经网络的每个组件的维度，因为自动编码器具有使其成为自动编码器的特定结构。</p><h2 id="30c0" class="nz lj it bd lk oa ob dn lo oc od dp ls mj oe of lu mn og oh lw mr oi oj ly iz bi translated">自动编码器和图表</h2><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi oy"><img src="../Images/09db02a63d76b5e3ca591da160220ba1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vxlQuetbLDo6_zEID5I0jg.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">作者图片:Autoencoder architechture</p></figure><p id="24e1" class="pw-post-body-paragraph ma mb it mc b md ni kd mf mg nj kg mi mj nk ml mm mn nl mp mq mr nm mt mu mv im bi translated">自动编码器接受输入(无论是波动点的网格、图像的像素还是时间序列路径的签名),并将其压缩到称为潜在空间的低维表示。从这个低维表示，潜在空间，目标是通过重建误差的反向传播来重建输入。</p><p id="f9af" class="pw-post-body-paragraph ma mb it mc b md ni kd mf mg nj kg mi mj nk ml mm mn nl mp mq mr nm mt mu mv im bi translated">这可以从类似于任何种类的维度缩减方法(即主成分分析和路径签名)的角度来看。</p><p id="f511" class="pw-post-body-paragraph ma mb it mc b md ni kd mf mg nj kg mi mj nk ml mm mn nl mp mq mr nm mt mu mv im bi translated">至于语义，自动编码器可以分解为两个模型:</p><ul class=""><li id="46a5" class="ok ol it mc b md ni mg nj mj om mn on mr oo mv op oq or os bi translated"><strong class="mc jd">编码器:</strong>将输入压缩到一个潜在空间，由上图中的<em class="oz">输入</em>和<em class="oz">隐藏层</em>组成</li><li id="6765" class="ok ol it mc b md ot mg ou mj ov mn ow mr ox mv op oq or os bi translated"><strong class="mc jd">解码器:</strong>将原始输入的潜在表示作为输入，并尝试重建原始输入</li></ul><h2 id="00ce" class="nz lj it bd lk oa ob dn lo oc od dp ls mj oe of lu mn og oh lw mr oi oj ly iz bi translated">自动编码器Python代码</h2><p id="941c" class="pw-post-body-paragraph ma mb it mc b md me kd mf mg mh kg mi mj mk ml mm mn mo mp mq mr ms mt mu mv im bi translated">关于在Tensorflow中实现自动编码器的资源并不多，所以我在下面提供了一个要点，作为使用Tensorflow的功能API开发您自己的自动编码器的指南。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="pa pb l"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">Tensorflow中的编码器模型</p></figure><pre class="ks kt ku kv gt pc pd pe pf aw pg bi"><span id="f8e2" class="nz lj it pd b gy ph pi l pj pk">Model: "encoder"<br/>_________________________________________________________________<br/>Layer (type)                 Output Shape              Param #   <br/>=================================================================<br/>encoder_input (InputLayer)   (None, 10)                0         <br/>_________________________________________________________________<br/>intermediate_layer_1 (Dense) (None, 10)                110       <br/>_________________________________________________________________<br/>intermediate_layer_2 (Dense) (None, 10)                110       <br/>_________________________________________________________________<br/>latent_space (Dense)         (None, 5)                 55        <br/>=================================================================<br/>Total params: 275<br/>Trainable params: 275<br/>Non-trainable params: 0<br/>_________________________________________________________________</span></pre><p id="4009" class="pw-post-body-paragraph ma mb it mc b md ni kd mf mg nj kg mi mj nk ml mm mn nl mp mq mr nm mt mu mv im bi translated">现在我们需要一个解码器…</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="pa pb l"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">Tensorflow中的解码器模型</p></figure><pre class="ks kt ku kv gt pc pd pe pf aw pg bi"><span id="b3f1" class="nz lj it pd b gy ph pi l pj pk">Model: "decoder"<br/>_________________________________________________________________<br/>Layer (type)                 Output Shape              Param #   <br/>=================================================================<br/>decoder_input (InputLayer)   (None, 5)                 0         <br/>_________________________________________________________________<br/>intermediate_layer_3 (Dense) (None, 10)                60        <br/>_________________________________________________________________<br/>latent_space (Dense)         (None, 10)                110       <br/>=================================================================<br/>Total params: 170<br/>Trainable params: 170<br/>Non-trainable params: 0<br/>_________________________________________________________________</span></pre><p id="567a" class="pw-post-body-paragraph ma mb it mc b md ni kd mf mg nj kg mi mj nk ml mm mn nl mp mq mr nm mt mu mv im bi translated">现在我们有了编码器和解码器，我们可以将它们编译成一个模型，称为(令人震惊的)自动编码器。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="pa pb l"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">Tensorflow中的自动编码器模型</p></figure><pre class="ks kt ku kv gt pc pd pe pf aw pg bi"><span id="de7a" class="nz lj it pd b gy ph pi l pj pk">Model: "autoencoder"<br/>_________________________________________________________________<br/>Layer (type)                 Output Shape              Param #   <br/>=================================================================<br/>encoder_input (InputLayer)   (None, 10)                0         <br/>_________________________________________________________________<br/>encoder (Model)              (None, 5)                 275       <br/>_________________________________________________________________<br/>decoder (Model)              (None, 10)                170       <br/>=================================================================<br/>Total params: 445<br/>Trainable params: 445<br/>Non-trainable params: 0<br/>_________________________________________________________________</span></pre><p id="7012" class="pw-post-body-paragraph ma mb it mc b md ni kd mf mg nj kg mi mj nk ml mm mn nl mp mq mr nm mt mu mv im bi translated">这就是全部了！我们已经使用Python和Tensorflow成功构建了一个动态自动编码器。现在我们已经有了自动编码器的坚实基础，让我们看看为什么可变自动编码器如此有用。</p></div><div class="ab cl nn no hx np" role="separator"><span class="nq bw bk nr ns nt"/><span class="nq bw bk nr ns nt"/><span class="nq bw bk nr ns"/></div><div class="im in io ip iq"><h1 id="009d" class="li lj it bd lk ll nu ln lo lp nv lr ls ki nw kj lu kl nx km lw ko ny kp ly lz bi translated">可变自动编码器</h1><h2 id="003c" class="nz lj it bd lk oa ob dn lo oc od dp ls mj oe of lu mn og oh lw mr oi oj ly iz bi translated">简介和图表</h2><p id="0e0d" class="pw-post-body-paragraph ma mb it mc b md me kd mf mg mh kg mi mj mk ml mm mn mo mp mq mr ms mt mu mv im bi translated">顾名思义，它和标准自动编码器的唯一区别是<em class="oz">变型</em>组件…</p><p id="ee90" class="pw-post-body-paragraph ma mb it mc b md ni kd mf mg nj kg mi mj nk ml mm mn nl mp mq mr nm mt mu mv im bi translated">自动编码器在潜在空间中具有原始输入的离散表示。离散，意味着它们本质上不是概率性的，它们是数学上确定的结果。</p><p id="3121" class="pw-post-body-paragraph ma mb it mc b md ni kd mf mg nj kg mi mj nk ml mm mn nl mp mq mr nm mt mu mv im bi translated">变分自动编码器保持潜在空间中的分布。如果听起来令人困惑，那是因为它确实如此。让我们来看一下这个体系结构的示意图，来阐明这个概念…</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi oy"><img src="../Images/f1b2454ef53106254026974cb1daf062.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zGGSZZCoW2FhcYCBOVnPXA.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">作者图片:可变自动编码器架构</p></figure><p id="423b" class="pw-post-body-paragraph ma mb it mc b md ni kd mf mg nj kg mi mj nk ml mm mn nl mp mq mr nm mt mu mv im bi translated">其中图中的<em class="oz"> X </em>表示正向传递中的输入实例。我们可以看到，潜在空间由潜在变量的分布组成，这些变量与编码器得出的平均值和标准偏差直接相关(回想一下我们的自动编码器的结构，这仍然成立)。</p><h2 id="d273" class="nz lj it bd lk oa ob dn lo oc od dp ls mj oe of lu mn og oh lw mr oi oj ly iz bi translated">潜在变量分布</h2><p id="973b" class="pw-post-body-paragraph ma mb it mc b md me kd mf mg mh kg mi mj mk ml mm mn mo mp mq mr ms mt mu mv im bi translated">在变分自动编码器的情况下，编码器产生负责构建潜在变量分布的条件均值和标准偏差。没有这些条件均值和标准偏差，解码器将没有用于重构原始输入的参考框架。</p><p id="8a5c" class="pw-post-body-paragraph ma mb it mc b md ni kd mf mg nj kg mi mj nk ml mm mn nl mp mq mr nm mt mu mv im bi translated">为了创建分布本身，我们添加了一个ε项(正态分布),它乘以条件标准差并与条件均值相加。这是为了绕过通过随机节点的反向传播问题。虽然超出了本文的范围，但是如果你对我们为什么必须使用这个<em class="oz">重新参数化技巧</em>感兴趣，你可以看下面的<a class="ae lh" href="https://stats.stackexchange.com/questions/199605/how-does-the-reparameterization-trick-for-vaes-work-and-why-is-it-important" rel="noopener ugc nofollow" target="_blank">帖子</a>。</p><blockquote class="pl pm pn"><p id="b6ae" class="ma mb oz mc b md ni kd mf mg nj kg mi po nk ml mm pp nl mp mq pq nm mt mu mv im bi translated">这开始变得非常技术性，所以请记住最初的目标:生成代表一些原始数据的新样本。</p></blockquote><h2 id="e675" class="nz lj it bd lk oa ob dn lo oc od dp ls mj oe of lu mn og oh lw mr oi oj ly iz bi translated">重建误差和KL损失</h2><p id="072d" class="pw-post-body-paragraph ma mb it mc b md me kd mf mg mh kg mi mj mk ml mm mn mo mp mq mr ms mt mu mv im bi translated">坚持这种潜在变量分布的概念意味着我们可以生成代表用于训练变分自动编码器的集合的新样本。然而，潜在分布的形状并不令人鼓舞。这在基本意义上是有问题的——我们希望从分布中提取值，并将这些提取值传递给解码器以生成新的数据样本，但是，如果我们不能确定我们在采样中捕获了整个变化的自动编码器训练空间，则生成的数据将会有偏差。</p><p id="0ad7" class="pw-post-body-paragraph ma mb it mc b md ni kd mf mg nj kg mi mj nk ml mm mn nl mp mq mr nm mt mu mv im bi translated">幸运的是，变分自动编码器的损失函数有两个分量:重建损失和KL损失。重建损失与标准自动编码器相同。KL损失促使潜在空间中的分布形状趋于正态。这<em class="oz">极其</em>重要，尤其是如果我们希望生成新的数据样本——如果潜在空间中的分布是正态的，我们可以简单地从正态分布中采样，并将这些值传递给解码器以生成新的输入(<em class="oz">这是一件大事</em>)。</p><h2 id="ccbe" class="nz lj it bd lk oa ob dn lo oc od dp ls mj oe of lu mn og oh lw mr oi oj ly iz bi translated">为什么变型自动编码器很棘手</h2><p id="85ad" class="pw-post-body-paragraph ma mb it mc b md me kd mf mg mh kg mi mj mk ml mm mn mo mp mq mr ms mt mu mv im bi translated">不幸的是，这可能是变型自动编码器变得非常难以工作的地方。当开发上述波动率表面用例的变分自动编码器时，花费了<em class="oz">两周</em>来找到平衡重建和KL损失的适当方法。如果重构误差被优先考虑，则生成的样本将会非常差，但是输入的重构将会很成功。如果KL损失被优先化，则生成的样本将是奇妙的，但是实际上没有意义，因为原始输入的重建被完全放弃了。这就是为什么取得平衡是绝对重要的。如果主要的努力是为一个特定的用例构造可变的自动编码器，我鼓励你阅读我的论文，因为它更详细地展示了(并提供了参考)用于平衡这些误差的过程。</p><h2 id="88e3" class="nz lj it bd lk oa ob dn lo oc od dp ls mj oe of lu mn og oh lw mr oi oj ly iz bi translated">变分自动编码器Python代码</h2><p id="20b1" class="pw-post-body-paragraph ma mb it mc b md me kd mf mg mh kg mi mj mk ml mm mn mo mp mq mr ms mt mu mv im bi translated">同样，Python中没有多少变分自动编码器的实现。这可以部分归因于实现的困难和缺乏用例。尽管如此，我还是提供了下面的代码，用于动态实现不同的自动编码器，而不考虑用例…</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="pa pb l"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">张量流中的变分编码器模型</p></figure><p id="2e64" class="pw-post-body-paragraph ma mb it mc b md ni kd mf mg nj kg mi mj nk ml mm mn nl mp mq mr nm mt mu mv im bi translated">您会注意到，变分编码器使用标准偏差的对数表示，而不是标准偏差本身(这也显示在示例函数中，数学上我们使用标准偏差)。这有助于KL损失项的收敛。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="pa pb l"/></div></figure><p id="fa7e" class="pw-post-body-paragraph ma mb it mc b md ni kd mf mg nj kg mi mj nk ml mm mn nl mp mq mr nm mt mu mv im bi translated">就像以前一样，变分编码器和变分解码器可以组合成一个单一的模型:变分自动编码器。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="pa pb l"/></div></figure><pre class="ks kt ku kv gt pc pd pe pf aw pg bi"><span id="b051" class="nz lj it pd b gy ph pi l pj pk">Model: "VAE"<br/>_________________________________________________________________<br/>Layer (type)                 Output Shape              Param #   <br/>=================================================================<br/>encoder_input (InputLayer)   (None, 10)                0         <br/>_________________________________________________________________<br/>encoder (Model)              (None, 5)                 220       <br/>_________________________________________________________________<br/>decoder (Model)              (None, 10)                170       <br/>=================================================================<br/>Total params: 390<br/>Trainable params: 390<br/>Non-trainable params: 0<br/>_________________________________________________________________</span></pre></div><div class="ab cl nn no hx np" role="separator"><span class="nq bw bk nr ns nt"/><span class="nq bw bk nr ns nt"/><span class="nq bw bk nr ns"/></div><div class="im in io ip iq"><h1 id="4915" class="li lj it bd lk ll nu ln lo lp nv lr ls ki nw kj lu kl nx km lw ko ny kp ly lz bi translated">结论</h1><p id="afb3" class="pw-post-body-paragraph ma mb it mc b md me kd mf mg mh kg mi mj mk ml mm mn mo mp mq mr ms mt mu mv im bi translated">本文的目标是通过示例代码让您了解可变自动编码器，以便您可以开发自己的生成式机器学习模型。我们首先从简单的前馈神经网络的背景来看标准的自动编码器。离散潜在空间的概念然后扩展到概率潜在空间，自动编码器和变分自动编码器之间的主要区别。最后，我们考虑了变分自动编码器的一些更具技术性的方面，并在训练空间中分解了一些关于平衡优化的问题。</p><p id="94d2" class="pw-post-body-paragraph ma mb it mc b md ni kd mf mg nj kg mi mj nk ml mm mn nl mp mq mr nm mt mu mv im bi translated">这是机器学习的一个特殊领域，我在最近的研究项目中花了很多时间。如果有任何关于特定代码或任何模型的问题，我很乐意回答。</p><div class="mw mx gp gr my mz"><a href="https://github.com/RomanMichaelPaolucci" rel="noopener  ugc nofollow" target="_blank"><div class="na ab fo"><div class="nb ab nc cl cj nd"><h2 class="bd jd gy z fp ne fr fs nf fu fw jc bi translated">RomanMichaelPaolucci -概述</h2><div class="ng l"><h3 class="bd b gy z fp ne fr fs nf fu fw dk translated">定量金融，数学和计算机科学阻止或报告算法的关键阶段的解决方案…</h3></div><div class="nh l"><p class="bd b dl z fp ne fr fs nf fu fw dk translated">github.com</p></div></div><div class="pr l"><div class="ps l pt pu pv pr pw lb mz"/></div></div></a></div></div></div>    
</body>
</html>