<html>
<head>
<title>Performance And Explainability With EBM</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">EBM的性能和可解释性</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/performance-and-explainability-with-ebm-c5d363e5f82?source=collection_archive---------40-----------------------#2021-04-13">https://towardsdatascience.com/performance-and-explainability-with-ebm-c5d363e5f82?source=collection_archive---------40-----------------------#2021-04-13</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="39ae" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsdatascience.com/tagged/model-interpretability" rel="noopener" target="_blank">模型可解释性</a></h2><div class=""/><div class=""><h2 id="48e0" class="pw-subtitle-paragraph jw iz iq bd b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn dk translated">可解释的助推机器是如何收获这两者的</h2></div><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ko"><img src="../Images/971ee15d8a1e18cfe1267f6147e5bbb7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Jozmoro2tFFSVHRCKRkHJw.jpeg"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated"><a class="ae le" href="https://unsplash.com/@alternateskate?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">轮滑</a>在<a class="ae le" href="/?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片。作者的所有其他数字。</p></figure><p id="f795" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">从那时起，人们不得不在有效模型和可解释模型之间做出选择。一方面，像逻辑回归这样的简单模型是可以解释的，但是在表现上有所滞后。另一方面，像提升树或神经网络这样的复杂模型达到了令人难以置信的准确性，但却难以理解。</p><p id="3d45" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">自2012年以来，来自微软的研究人员研究并实现了一种打破规则的算法:<a class="ae le" href="https://github.com/interpretml/interpret" rel="noopener ugc nofollow" target="_blank">可解释的助推机器</a> (EBM)。EBM是唯一没有这种性能与可解释性比率曲线的算法。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi mb"><img src="../Images/2ee86b1906132a733b0f71d91448ce2a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1x7NqZK-D40kdfcZ6-xHrQ.png"/></div></div></figure><p id="5074" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">有了EBM，当可解释性是一个需求时，你不必牺牲性能，你可以用顶级性能免费得到解释。循证医学的一个重要方面是它是自然可解释的。我们不谈论用像Lime、Shap或GramCam这样的方法来解释复杂的模型:EBM本质上是可以解释的。人们可以直接看到模型如何使用每个特性。例如，参见一个变量的全局解释图:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi mc"><img src="../Images/dae073cf6dc45626a396bd7526e7e86c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BT9J9W8fjSwLyljXofpnmw.png"/></div></div></figure><p id="c03c" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">蓝线<strong class="lh ja">是</strong>模型如何使用特性，而不是估计。这些图表对于理解模型的内部工作是非常宝贵的。它们对调试也有很大的帮助:如果模型在某个地方过度拟合了一个特性，您可以立即看到它(您看到这里的过度拟合了吗？).这些都是用石灰或Shap很难发现的问题。有人会说这是数据探索阶段的目标。不幸的是，在这一阶段很容易遗漏一些东西，所以依赖模型本身是很好的。</p><p id="0836" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">那么EBM是如何工作的呢？这是不是好得难以置信？</p><p id="da56" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">我们将会看到，由于几种技术的巧妙结合，这是可能的。</p><h1 id="9812" class="md me iq bd mf mg mh mi mj mk ml mm mn kf mo kg mp ki mq kj mr kl ms km mt mu bi translated">广义可加模型</h1><p id="0894" class="pw-post-body-paragraph lf lg iq lh b li mv ka lk ll mw kd ln lo mx lq lr ls my lu lv lw mz ly lz ma ij bi translated">先说让EBM可解释的基础。EBM是一种<em class="na">广义加法模型</em> (GAM)，由<em class="na"> Trevor Hastie </em>和<em class="na"> Robert Tibshirani </em>形式化。这个模型家族的完整描述可以在他们的书<a class="ae le" href="https://web.stanford.edu/~hastie/ElemStatLearn/" rel="noopener ugc nofollow" target="_blank">统计学习要素</a>的第9章中找到。GAM是满足以下公式的任何模型:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi nb"><img src="../Images/174f1fd256ebf5008ed50e46996e4b2b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*h-9xKhwWQmrNdXTSRReCMA.png"/></div></div></figure><p id="2b6b" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">这些<em class="na"> f </em>函数被命名为<em class="na">形状函数</em>。<em class="na"> g </em>功能就是<em class="na">链接功能</em>。您可能注意到这与线性回归非常相似:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi nc"><img src="../Images/2c7ac3b4b818d024e3203efb264754b6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*L2uBY3Z-9h_qPUmCrFOf4Q.png"/></div></div></figure><p id="31d5" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">你是对的！线性回归也是博弈，其中:</p><ul class=""><li id="1b02" class="nd ne iq lh b li lj ll lm lo nf ls ng lw nh ma ni nj nk nl bi translated">形状函数是(与权重的)乘法。</li><li id="2939" class="nd ne iq lh b li nm ll nn lo no ls np lw nq ma ni nj nk nl bi translated">链接功能就是身份。</li></ul><p id="ad49" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">类似地，逻辑回归也是一个GAM，其中链接函数是<a class="ae le" href="https://en.wikipedia.org/wiki/Logit" rel="noopener ugc nofollow" target="_blank"> logit </a>。</p><p id="e0da" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">gam的数学定义定义了为什么它们是可解释的:画出形状函数，看看模型如何使用每个特征。就这么简单。上一节的图就是:一个形状函数的图。</p><h1 id="f74b" class="md me iq bd mf mg mh mi mj mk ml mm mn kf mo kg mp ki mq kj mr kl ms km mt mu bi translated">比GAM更好:GA M</h1><p id="30c5" class="pw-post-body-paragraph lf lg iq lh b li mv ka lk ll mw kd ln lo mx lq lr ls my lu lv lw mz ly lz ma ij bi translated">所以游戏可以通过设计来解释。然而，它们有一个明显的缺点:它们单独使用每个特性，忽略了它们之间的任何联系。即使是一个单独的决策树也可以结合特征进行预测。请注意，根据我的经验，这种简单的方法可以提供非常好的性能。</p><p id="f2aa" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">EBM为GAMs带来了第一个改进，它还在附加项上使用了成对的特性:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi nr"><img src="../Images/9b9607fbfeffda17c0ae06bc3816804b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*g--Z5NzSuZDK8c6Xfd5aig.png"/></div></div></figure><p id="1659" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">这样的模型叫做GA M: <em class="na">有相互作用的广义模型</em>(不对，定义没有错误。为一个算法找一个名字并不总是容易的)。</p><p id="328f" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">关键是，使用两个特征的形状函数仍然是可以解释的:它们不是以直线、曲线或台阶的形式呈现，而是以热图的形式显示。当然，它们更难解释，但它们确实如此。另一方面，这允许组合几个特征并提高性能。</p><p id="2e29" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">在我们继续讨论EBM对GAMs的改进之前，让我们看看如何进行预测。</p><h1 id="2ef9" class="md me iq bd mf mg mh mi mj mk ml mm mn kf mo kg mp ki mq kj mr kl ms km mt mu bi translated">循证医学推理</h1><p id="a735" class="pw-post-body-paragraph lf lg iq lh b li mv ka lk ll mw kd ln lo mx lq lr ls my lu lv lw mz ly lz ma ij bi translated">EBM的另一个优点是它的推理在计算方面是轻的。这是生产环境中预测延迟必须尽可能小的一点。</p><p id="05ce" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">下图显示了预测的所有步骤:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ns"><img src="../Images/1ccc429ee444879f4063ceea066190cd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*39lY6cCCCo9BBluqcLcPxw.png"/></div></div></figure><p id="a407" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">计算密集型部分是箱柜查找和加法！</p><p id="09b3" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">第一步包括连续变量的离散化和分类变量的映射。连续变量被分割成箱。在宁滨步骤中，特征被映射到索引值。每个指数都与一个分数相关联。对每个特征都这样做。最终，预测是所有分数加上截距/偏差的总和。多类分类，每类一分。根据任务，可能会使用后处理(链接功能):</p><ul class=""><li id="a7e1" class="nd ne iq lh b li lj ll lm lo nf ls ng lw nh ma ni nj nk nl bi translated">对于回归，总和直接是预测值。</li><li id="1a74" class="nd ne iq lh b li nm ll nn lo no ls np lw nq ma ni nj nk nl bi translated">对于分类，得分最高的类是预测类。</li><li id="f916" class="nd ne iq lh b li nm ll nn lo no ls np lw nq ma ni nj nk nl bi translated">对于类别概率，对每个类别的分数应用软最大值</li></ul><p id="1d34" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">让我们通过一个例子来遵循这些步骤。下图突出显示了使用两个特征的模型对数据集第一行的预测:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi nt"><img src="../Images/0147c21679415daf3408b2d512723b44.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yfo6xB6_5nyXqoD6QuRT-g.png"/></div></div></figure><p id="ef59" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">这些步骤都是GAM所期望的。然而，最初的宁滨和计分部分更有趣:它们是形状函数，但独立于任何算法或公式。它们可以代表任何形状的函数:样条、决策树，甚至神经网络。</p><p id="af3f" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">让我们看看这种离散化是从哪里来的。</p><h1 id="c8b5" class="md me iq bd mf mg mh mi mj mk ml mm mn kf mo kg mp ki mq kj mr kl ms km mt mu bi translated">拟合形状函数</h1><p id="730a" class="pw-post-body-paragraph lf lg iq lh b li mv ka lk ll mw kd ln lo mx lq lr ls my lu lv lw mz ly lz ma ij bi translated">EBM培训部分结合使用提升树和套袋。一个好的定义可能是<em class="na">袋装浅树</em>。该算法的核心使用提升树，如下图所示:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi nu"><img src="../Images/c06832bea906dd19b170428bb7af7f8b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UauNE65dd_vgiaaf4veuhw.png"/></div></div></figure><p id="7987" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">浅树以助推的方式训练。这些是小树(默认情况下最多有3片叶子)。此外，提升过程是特定的:每个树只针对一个特征进行训练。在每一轮提升过程中，针对每个特征一个接一个地训练树。它确保:</p><ul class=""><li id="ae89" class="nd ne iq lh b li lj ll lm lo nf ls ng lw nh ma ni nj nk nl bi translated">模型是可加的。</li><li id="65ca" class="nd ne iq lh b li nm ll nn lo no ls np lw nq ma ni nj nk nl bi translated">每个形状函数只使用一个特征。</li></ul><p id="6caf" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">这是算法的基础，但其他技术可以进一步提高性能:</p><ul class=""><li id="ef02" class="nd ne iq lh b li lj ll lm lo nf ls ng lw nh ma ni nj nk nl bi translated">Bagging，在这个基础估计值之上。</li><li id="5cfa" class="nd ne iq lh b li nm ll nn lo no ls np lw nq ma ni nj nk nl bi translated">可选装袋，用于每个增压步骤。默认情况下它是禁用的，因为它大大增加了训练时间。</li><li id="53e8" class="nd ne iq lh b li nm ll nn lo no ls np lw nq ma ni nj nk nl bi translated">成对互动。</li></ul><p id="940e" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">根据任务的不同，第三种技术可以极大地提高性能:一旦用单个特征训练了一个模型，就进行第二次训练(使用相同的训练程序)，但使用成对的特征。配对选择使用专用算法，避免尝试所有可能的组合(当有许多特征时，这是不可行的)。</p><p id="b4b0" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">最后，经过所有这些步骤，我们有一个树合奏。这些树被离散化，只需用输入要素的所有可能值运行它们。这很容易，因为所有特征都是离散化的。因此，预测值的最大数量是每个要素的条柱数量。最后，这数千棵树被简化为宁滨和每个特征的得分向量。</p><p id="c39b" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">所以这些向量是成千上万只存在几分钟的树的结果。一旦我们建造了它们，我们就不再需要它们了！</p><h1 id="f324" class="md me iq bd mf mg mh mi mj mk ml mm mn kf mo kg mp ki mq kj mr kl ms km mt mu bi translated">结论</h1><p id="7116" class="pw-post-body-paragraph lf lg iq lh b li mv ka lk ll mw kd ln lo mx lq lr ls my lu lv lw mz ly lz ma ij bi translated">人们不得不在准确性和可解释性之间做出选择的时代已经过去了。EBMs可以像boosted树一样有效，同时又像逻辑回归一样容易解释。</p><p id="d631" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">EBM属于广义可加模型家族。他们使用以允许简单推断和解释的方式编码的增强树。</p><p id="6ac6" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">EBM是微软<a class="ae le" href="https://github.com/interpretml/interpret" rel="noopener ugc nofollow" target="_blank"> InterpretML </a>项目的一部分。我鼓励任何从事表格数据工作的数据科学家尝试一下。我们在几个任务上取得了与XGBoost相当的性能，同时清楚地了解了模型如何使用每个特性。如果你在一个领域工作，其中可解释性是强制性的，或者运行时性能是关键，你应该尝试一下。</p></div><div class="ab cl nv nw hu nx" role="separator"><span class="ny bw bk nz oa ob"/><span class="ny bw bk nz oa ob"/><span class="ny bw bk nz oa"/></div><div class="ij ik il im in"><p id="7a46" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated"><em class="na">原载于2021年4月13日</em><a class="ae le" href="https://blog.oakbits.com/ebm-algorithm.html" rel="noopener ugc nofollow" target="_blank"><em class="na">【https://blog.oakbits.com】</em></a><em class="na">。</em></p></div></div>    
</body>
</html>