<html>
<head>
<title>Spark SQL 102 — Aggregations and Window Functions</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Spark SQL 102 —聚合和窗口函数</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/spark-sql-102-aggregations-and-window-functions-9f829eaa7549?source=collection_archive---------5-----------------------#2021-06-30">https://towardsdatascience.com/spark-sql-102-aggregations-and-window-functions-9f829eaa7549?source=collection_archive---------5-----------------------#2021-06-30</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="19b2" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">Spark中的分析函数，适合初学者。</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/03b628ee486a82c879683be1cdb86ea6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5OyiWiN3TWBJKuGZnKHxzw.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">波格丹一世·卡伦科在<a class="ae ky" href="https://unsplash.com/s/photos/windows?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="2daf" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">数据聚合是许多数据分析中的重要步骤。这是一种如何减少数据集并计算各种指标、统计数据和其他特征的方法。一个相关但稍微高级一些的主题是窗口函数，它允许基于具有所谓框架的窗口对数据计算其他分析和排序函数。</p><p id="b311" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这是最近的<a class="ae ky" rel="noopener" target="_blank" href="/a-decent-guide-to-dataframes-in-spark-3-0-for-beginners-dcc2903345a5">文章</a>的延续，在文章中我们描述了什么是数据帧以及Spark SQL中的转换一般是如何工作的。在这里，我们将深入研究聚合和窗口函数，它们是两组特定的转换，它们密切相关，但正如我们将看到的，它们之间有一个重要的区别，这是很好理解的。</p><p id="b044" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对于代码，我们将使用PySpark API，这是撰写本文时的最新版本3 . 1 . 2(2021年6月)。</p><h1 id="2f61" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">整个数据帧的聚合</h1><p id="c4b0" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">让我们从最简单的聚合开始，这是一种将整个数据集简化为一个数字的计算。这可能类似于数据帧中的总行数或某个特定列中值的总和/平均值。为此，我们可以直接在数据帧上使用<em class="ms"> agg() </em>函数，并以逗号分隔的方式将聚合函数作为参数传递:</p><pre class="kj kk kl km gt mt mu mv mw aw mx bi"><span id="16b0" class="my lw it mu b gy mz na l nb nc">from pyspark.sql.functions import count, sum</span><span id="4899" class="my lw it mu b gy nd na l nb nc">df.agg(count('*'))</span><span id="f142" class="my lw it mu b gy nd na l nb nc">df.agg(count('*'), sum('price'))</span><span id="7bd4" class="my lw it mu b gy nd na l nb nc">df.agg(<br/>  count('*').alias('number_of_rows'),<br/>  sum('price').alias('total_price')<br/>)</span></pre><p id="91fa" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">请注意，第一个示例的输出是一个具有单行和单列的数据帧，它只是一个由数据帧表示的数字。在第二个示例中，输出是一个具有单行和两列的数据帧，每个聚合函数对应一列。在最后一个示例中，我们可以看到每个聚合也可以使用<em class="ms"> alias() </em>函数进行重命名。</p><h1 id="491b" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">基于组的聚合</h1><p id="0b63" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">通常，我们需要计算聚合，不是针对整个数据帧，而是针对每组行单独计算，其中组被定义为在特定列中具有相同值的行。例如，假设一个信用卡交易数据集，其中每一行都是唯一的交易，但不同的行可能属于同一个用户(持卡人)。这里，为每个用户单独计算聚合可能是有用的，对于这种聚合，我们可以使用<em class="ms"> groupBy </em>转换:</p><pre class="kj kk kl km gt mt mu mv mw aw mx bi"><span id="2b8b" class="my lw it mu b gy mz na l nb nc">(<br/>  df<br/>  .groupBy('user_id')<br/>  .agg(count('*').alias('number_of_transactions'))<br/>)</span></pre><p id="0f1b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">同样，我们在这里使用的是<em class="ms"> agg </em>函数，我们可以传入任何聚合函数，例如<em class="ms"> count </em>、<em class="ms"> countDistinct </em>、<em class="ms"> sum </em>、<em class="ms"> avg </em> / <em class="ms"> mean </em>、<em class="ms"> min </em>、<em class="ms"> max </em>、<em class="ms"> first </em>、<em class="ms"> last </em>、<em class="ms"> collect_list </em>、<em class="ms">注意，如果我们不重命名聚合的结果，它将有一个默认名称，在<em class="ms"> count </em>函数的情况下是<em class="ms"> count(1) </em>。</em></p><p id="ca81" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">或者，我们可以直接在<em class="ms"> groupBy </em>之后调用聚合函数，如下所示</p><pre class="kj kk kl km gt mt mu mv mw aw mx bi"><span id="4e02" class="my lw it mu b gy mz na l nb nc">(<br/>  df<br/>  .groupBy('user_id')<br/>  .count()<br/>)</span></pre><p id="5706" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">注意，使用这种语法的缺点是不能使用<em class="ms">别名()</em>直接重命名聚合的结果，因为这里count函数返回一个数据帧，所以<em class="ms">别名</em>将应用于整个数据帧。因此重命名必须由另一个转换来处理，比如带有ColumnRenamed('count '，' new_name') 。同样，这里你一次只能调用一个聚集函数，而带有<em class="ms"> agg </em>的语法允许你同时调用任意多个函数。其他在<em class="ms">分组后被</em>调用的功能参见<a class="ae ky" href="https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql.html#grouping" rel="noopener ugc nofollow" target="_blank">文档</a>。</p><p id="e6d5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这些<em class="ms"> groupBy </em>转换的一个重要属性是，输出数据帧将只包含在<em class="ms"> groupBy() </em>中指定为参数的列和聚合结果。所以如果我们调用<em class="ms"> df.groupBy('user_id ')。count() </em>，无论df有多少字段，输出都只有两列，分别是<em class="ms"> user_id </em>和<em class="ms"> count </em>。此外，输出数据帧所代表的行数将更少，或者在边际情况下，与原始<em class="ms"> df </em>中相同。边际情况对应于分组列的所有值都不同的情况，因此每个组只有一行。正如我们将看到的，这将与窗口函数不同。</p><h1 id="71b3" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">基于带框架的窗口的聚合</h1><p id="ebd3" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">窗口函数是一组函数，也可以在一组行上调用，就像我们在前面的例子中看到的那样。然而，还是有一些不同之处。首先，在调用窗口函数之后，数据集不会被缩减-所有行和所有列都将在输出数据帧中，并且计算将被添加到新列中。将应用该函数的行组再次由特定列(或列的列表)给出，对于该特定列，行具有相同的值，并且该组被称为<em class="ms">窗口</em>。此外，窗口函数更加灵活，因为有时您不想将函数应用于整个窗口，而是只应用于窗口中行的子集，即所谓的帧。最后，窗口也可以排序，因为一些功能(所谓的排序功能)需要它。让我们看看窗口函数的语法:</p><pre class="kj kk kl km gt mt mu mv mw aw mx bi"><span id="b215" class="my lw it mu b gy mz na l nb nc">from pyspark.sql import Window</span><span id="6c81" class="my lw it mu b gy nd na l nb nc">w = Window().partitionBy('user_id')</span><span id="3d3d" class="my lw it mu b gy nd na l nb nc">df.withColumn('number_of_transactions', count('*').over(w))</span></pre><p id="a437" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">正如你所看到的，我们首先使用函数<em class="ms">partition by()</em>定义窗口——这类似于<em class="ms"> groupBy() </em>，所有在指定列中具有相同值的行(这里是<em class="ms"> user_id </em>)将形成一个窗口。然后我们向数据帧添加一个新列，在这里我们调用指定窗口的窗口函数。</p><h2 id="27f8" class="my lw it bd lx ne nf dn mb ng nh dp mf li ni nj mh lm nk nl mj lq nm nn ml no bi translated">排名功能</h2><p id="a405" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">这是一组特定的窗口函数，需要对窗口进行排序。作为一个具体的例子，考虑函数<a class="ae ky" href="https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.functions.row_number.html#pyspark.sql.functions.row_number" rel="noopener ugc nofollow" target="_blank"> <em class="ms"> row_number() </em> </a>，它告诉您窗口中的行数:</p><pre class="kj kk kl km gt mt mu mv mw aw mx bi"><span id="d3d0" class="my lw it mu b gy mz na l nb nc">from pyspark.sql.functions import row_number</span><span id="f795" class="my lw it mu b gy nd na l nb nc">w = Window.partitionBy('user_id').orderBy('transaction_date')</span><span id="0311" class="my lw it mu b gy nd na l nb nc">df.withColumn('r', row_number().over(w))</span></pre><p id="921d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">其他排名功能例如有<a class="ae ky" href="https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.functions.rank.html#pyspark.sql.functions.rank" rel="noopener ugc nofollow" target="_blank"> <em class="ms">排名</em> </a> <em class="ms"> () </em>或<a class="ae ky" href="https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.functions.dense_rank.html#pyspark.sql.functions.dense_rank" rel="noopener ugc nofollow" target="_blank"> <em class="ms">密集_排名</em> </a> <em class="ms"> () </em>。</p><h2 id="1915" class="my lw it bd lx ne nf dn mb ng nh dp mf li ni nj mh lm nk nl mj lq nm nn ml no bi translated">指定框架</h2><p id="a2a4" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">如上所述，一些功能可以应用于窗口中的行的子集。一个典型的用例是计算值的累积和，其中帧将指定我们希望从窗口的开始直到当前行应用该函数。此外，很明显，帧中各行的顺序很重要，因为如果各行的顺序不同，累积和将具有不同的形状。可以使用以下两个函数之一来指定帧:</p><ul class=""><li id="6b42" class="np nq it lb b lc ld lf lg li nr lm ns lq nt lu nu nv nw nx bi translated"><a class="ae ky" href="https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.Window.rowsBetween.html#pyspark.sql.Window.rowsBetween" rel="noopener ugc nofollow" target="_blank"><em class="ms"/></a></li><li id="18a8" class="np nq it lb b lc ny lf nz li oa lm ob lq oc lu nu nv nw nx bi translated"><a class="ae ky" href="https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.Window.rangeBetween.html#pyspark.sql.Window.rangeBetween" rel="noopener ugc nofollow" target="_blank"> <em class="ms"> rangeBetween() </em> </a></li></ul><p id="937a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这两个函数都有两个参数:帧的开始和结束，它们可以指定如下:</p><ul class=""><li id="156f" class="np nq it lb b lc ld lf lg li nr lm ns lq nt lu nu nv nw nx bi translated">window . unbounded前进，window . unbounded跟随-从头到尾的整个窗口</li><li id="7c3a" class="np nq it lb b lc ny lf nz li oa lm ob lq oc lu nu nv nw nx bi translated">Window.unboundedPreceding，Window.currentRow从窗口的开头到当前行，这用于累计和</li><li id="9c7e" class="np nq it lb b lc ny lf nz li oa lm ob lq oc lu nu nv nw nx bi translated">使用数值，例如，0表示<em class="ms">当前值</em>，但是其他值的含义可以根据帧功能<em class="ms">行之间</em> / <em class="ms">范围之间</em>而不同。</li></ul><p id="d9d6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为了理解和<em class="ms">之间的<em class="ms">行和</em>之间的<em class="ms">范围之间的区别，让我们看看下面的例子，其中我们有三列，<em class="ms"> user_id </em>、<em class="ms"> activity </em>和<em class="ms"> day </em>，我们想对每个用户的活动求和:</em></em></p><pre class="kj kk kl km gt mt mu mv mw aw mx bi"><span id="02fa" class="my lw it mu b gy mz na l nb nc">df.withColumn('activity_sum', sum('activity').over(w))</span></pre><p id="5994" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们将按<em class="ms">日</em>对窗口进行排序，并使用间隔(-1，0)来指定帧。我们将会看到，这两个函数的区间有不同的含义。在图像的右侧，您可以看到每种情况下的求和结果(为了简单起见，我们在图像中只为id为100的用户显示一个窗口):</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi od"><img src="../Images/2bac0ef09d556eec29671becdf133b79.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WYO-zRP1SlrzGqT4S_5Jvw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><p id="4288" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在<em class="ms"> rowsBetween </em>的情况下，在每一行上，我们对当前行和前一行(如果存在)的活动求和，这就是interval (-1，0)的含义。另一方面，在<em class="ms"> rangeBetween </em>的情况下，在每一行上，我们首先需要通过从<em class="ms"> day </em>列中的值减去值1来计算将要求和的行的范围。例如，在第3行上，我们有7–1 = 6导致间隔(6，7)，我们应该合计所有符合此间隔的<em class="ms">日</em>列中的值的行，在我们的例子中，它只是当前行，因为没有<em class="ms">日</em> =6的行。</p><p id="7e99" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如您所见，在<em class="ms"> rangeBetween </em>的情况下，我们排序所依据的列需要是某种数字类型，因为Spark需要对该列中的值进行一些运算。在之间的<em class="ms">行中不存在这种限制。</em></p><p id="d8c1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">要提及其他一些窗口功能，请参见例如:</p><ul class=""><li id="dec2" class="np nq it lb b lc ld lf lg li nr lm ns lq nt lu nu nv nw nx bi translated"><a class="ae ky" href="https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.functions.lead.html#pyspark.sql.functions.lead" rel="noopener ugc nofollow" target="_blank"> <em class="ms">引()</em> </a></li><li id="4390" class="np nq it lb b lc ny lf nz li oa lm ob lq oc lu nu nv nw nx bi translated"><a class="ae ky" href="https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.functions.lag.html#pyspark.sql.functions.lag" rel="noopener ugc nofollow" target="_blank"> <em class="ms">【滞后()】</em> </a></li><li id="b928" class="np nq it lb b lc ny lf nz li oa lm ob lq oc lu nu nv nw nx bi translated"><a class="ae ky" href="http://ntile(n)" rel="noopener ugc nofollow" target="_blank">T5】ntile()T7】</a></li><li id="d7e3" class="np nq it lb b lc ny lf nz li oa lm ob lq oc lu nu nv nw nx bi translated"><a class="ae ky" href="https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.functions.nth_value.html#pyspark.sql.functions.nth_value" rel="noopener ugc nofollow" target="_blank"> <em class="ms">【第n _ value()】</em></a></li><li id="66c5" class="np nq it lb b lc ny lf nz li oa lm ob lq oc lu nu nv nw nx bi translated"><a class="ae ky" href="https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.functions.cume_dist.html#pyspark.sql.functions.cume_dist" rel="noopener ugc nofollow" target="_blank"><em class="ms">【cume _ dist()</em></a></li></ul><h2 id="8508" class="my lw it bd lx ne nf dn mb ng nh dp mf li ni nj mh lm nk nl mj lq nm nn ml no bi translated">值得知道的重要事情</h2><ul class=""><li id="df0f" class="np nq it lb b lc mn lf mo li oe lm of lq og lu nu nv nw nx bi translated">对窗口进行排序会改变帧— <strong class="lb iu">这可能不直观</strong> —对经过排序的窗口计算<em class="ms"> sum </em>会导致与未经排序的窗口相比不同的结果。更多细节，请查看我的另一篇<a class="ae ky" rel="noopener" target="_blank" href="/did-you-know-this-in-spark-sql-a7398bfcc41e">文章</a>，我在那里展示了一个例子。</li><li id="7161" class="np nq it lb b lc ny lf nz li oa lm ob lq oc lu nu nv nw nx bi translated">这两种转换，<em class="ms"> groupBy </em>和<em class="ms"> window </em>都需要特定的分区，如果分区不存在，它们将导致洗牌——数据需要以这样的方式重新组织，即一个组/窗口中的所有行都需要放在一个分区中。然而，不同之处在于，使用<em class="ms"> groupBy </em>，Spark将首先部分聚合数据，然后混洗缩减的数据集，而<em class="ms">窗口</em>将混洗整个数据集。</li><li id="2314" class="np nq it lb b lc ny lf nz li oa lm ob lq oc lu nu nv nw nx bi translated">如果我们不向<em class="ms"> partitionBy </em>函数传递任何参数，并将窗口指定为<em class="ms"> w = Window()。partitionBy() </em>，整个数据集将变成一个大窗口，所有数据将被混洗到单个分区，这可能会导致性能问题，因为所有数据都将放在集群中的单个节点上。</li></ul><h1 id="42a6" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">结论</h1><p id="db4f" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">在本文中，我们讨论了聚合和窗口函数，它们是非常常用的转换，尤其是在数据分析师中。我们讨论了调用<em class="ms"> groupBy </em>和<em class="ms"> Window.partitionBy </em>的区别，我们已经看到了如何在窗口上指定框架的不同选项。</p></div></div>    
</body>
</html>