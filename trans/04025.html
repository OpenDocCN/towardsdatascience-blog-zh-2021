<html>
<head>
<title>Beginner guide to Variational Autoencoders (VAE) with PyTorch Lightning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用PyTorch Lightning的可变自动编码器(VAE)初学者指南</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/beginner-guide-to-variational-autoencoders-vae-with-pytorch-lightning-13dbc559ba4b?source=collection_archive---------5-----------------------#2021-04-05">https://towardsdatascience.com/beginner-guide-to-variational-autoencoders-vae-with-pytorch-lightning-13dbc559ba4b?source=collection_archive---------5-----------------------#2021-04-05</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><figure class="gl gn jr js jt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi jq"><img src="../Images/f79c2b2c134a4b1d4715d556f8ecd995.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*2fXMSzNx4icARvvl"/></div></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">凯利·西克玛在<a class="ae kf" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="149c" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这篇博客文章是一个迷你系列的一部分，该系列讨论了使用可变自动编码器构建PyTorch深度学习项目的不同方面。</p><blockquote class="le lf lg"><p id="1832" class="kg kh lh ki b kj kk kl km kn ko kp kq li ks kt ku lj kw kx ky lk la lb lc ld im bi translated"><a class="ae kf" rel="noopener" target="_blank" href="/beginner-guide-to-variational-autoencoders-vae-with-pytorch-lightning-13dbc559ba4b">第1部分</a>:数学基础与实现<br/> <a class="ae kf" rel="noopener" target="_blank" href="/beginner-guide-to-variational-autoencoders-vae-with-pytorch-lightning-part-2-6b79ad697c79">第2部分</a>:用PyTorch Lightning <br/> <a class="ae kf" rel="noopener" target="_blank" href="/beginner-guide-to-variational-autoencoders-vae-with-pytorch-lightning-part-3-9d686d0d85d9#1921-b4c73b02c87">增压第3部分</a>:卷积VAE、继承与单元测试<br/> <a class="ae kf" rel="noopener" target="_blank" href="/building-a-vae-playground-with-streamlit-aa88a3394c04">第4部分</a> : Streamlit Web App与部署</p></blockquote><p id="39d9" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">autoencoder是一种无监督的神经网络架构，旨在寻找数据的低维表示。在这篇博客文章中，我将介绍variable Autoencoder的一个简单实现，它是auto encoder的一个有趣的变体，允许生成数据。</p><blockquote class="le lf lg"><p id="ab33" class="kg kh lh ki b kj kk kl km kn ko kp kq li ks kt ku lj kw kx ky lk la lb lc ld im bi translated">我不能创造的东西，我不理解——理查德·费曼</p></blockquote><p id="2e06" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">当我开始这个项目时，我有两个主要目标:</p><p id="fa72" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="ki iu"> 1。</strong> <strong class="ki iu">练习将数学概念翻译成代码</strong> <br/>使用预先构建的模型和常用的神经网络层只能做到这一步。将数学方程翻译成可执行代码是一项重要的技能，在学习如何使用深度学习库时，这是一个非常好的实践。实现像VAE这样的简单架构对于理解实验室新出的最新模型大有帮助！</p><p id="10a5" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="ki iu"> 2。</strong> <strong class="ki iu">学习PyTorch闪电</strong> <br/> PyTorch闪电一直是我想学很久的东西。它是PyTorch的一个非常有用的扩展，极大地简化了训练模型所需的大量过程和样板代码。利用这个项目作为学习PyTorch Lightning的平台，帮助我在实习中有信心将其应用到其他项目中。</p><p id="1331" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="ki iu">变分自动编码器和表示学习</strong></p><p id="174a" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在浏览代码之前，我们首先需要了解自动编码器是做什么的，以及为什么它非常有用。自动编码器通过学习数据的低维表示来工作，并尝试使用该低维数据来重新创建原始数据。</p><p id="5b8c" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">对于这个项目，我们将使用MNIST数据集。每个图像为28×28像素宽，可以表示为784维向量。正如大多数图像所预期的那样，许多像素共享相同的信息，并且彼此相关。这也意味着这些像素中的信息在很大程度上是冗余的，可以压缩相同数量的信息。数据的这种压缩形式是相同数据在更小的向量空间中的表示，该向量空间也称为潜在空间。给定一个特定的数据集，自动编码器试图找到最能反映底层数据的数据的潜在空间。</p><figure class="lm ln lo lp gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi ll"><img src="../Images/c6483ad71727e82a8eec00727a838250.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tkIphjdt9mbKvKQ7DnTPuQ.png"/></div></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">自动编码器架构—作者图片</p></figure><ol class=""><li id="3336" class="lq lr it ki b kj kk kn ko kr ls kv lt kz lu ld lv lw lx ly bi translated"><strong class="ki iu">编码器</strong></li><li id="ef8b" class="lq lr it ki b kj lz kn ma kr mb kv mc kz md ld lv lw lx ly bi translated"><strong class="ki iu">解码器</strong></li><li id="9cd3" class="lq lr it ki b kj lz kn ma kr mb kv mc kz md ld lv lw lx ly bi translated"><strong class="ki iu">潜在空间/瓶颈</strong></li><li id="cf84" class="lq lr it ki b kj lz kn ma kr mb kv mc kz md ld lv lw lx ly bi translated"><strong class="ki iu">重建损失</strong></li></ol><p id="fa37" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们看看自动编码器是如何为单个数据点工作的。首先，编码器部分试图将图像中的信息强行送入瓶颈。从数学上讲，这可以看作是从R⁷⁸⁴到r·⁴(瓶颈维度)的一个非常复杂的函数。维度低得多的瓶颈确保了信息将被压缩。接下来，解码器尝试使用该压缩信息来重建原始数据。根据压缩的潜在表示，解码器试图重建原始数据点。正如预期的那样，重建将不会是相同的，并且模型将基于原始数据和重建数据之间的差异而受到惩罚。对于普通的自动编码器，损失函数将是L2-诺姆损失。最终，经过训练后，编码器应该能够将信息压缩成仍然有用的表示形式，并保留原始数据点的大部分结构。</p><h1 id="d741" class="me mf it bd mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu mv mw mx my mz na nb bi translated">可变自动编码器</h1><p id="0bd2" class="pw-post-body-paragraph kg kh it ki b kj nc kl km kn nd kp kq kr ne kt ku kv nf kx ky kz ng lb lc ld im bi translated">既然我们已经了解了自动编码器是做什么的，让我们看看它们不太确定的表亲变分自动编码器(VAE)。vae通常用于数据生成，而不是数据压缩。由于这一关键差异，架构和功能与普通的自动编码器略有不同。普通自动编码器的问题是数据可能被映射到一个不规则的向量空间。不规则的潜在空间降低了模型很好地推广到看不见的例子的能力。做到这一点的一种方法是执行正则化，这可以防止过度拟合，并在模型具有异常结构时惩罚模型。这平衡了模型压缩信息的能力和生成新数据的能力。</p><figure class="lm ln lo lp gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi nh"><img src="../Images/788f2d8d02dc4e7c4c3645d4cdd92553.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CO4ov4T97rVY4cGaFtfgkQ.png"/></div></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">不规则的概率分布可能包含解码器性能较差的“口袋”,因为很少有例子被映射到该区域——按作者的图像</p></figure><p id="8568" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">VAEs不是将信息编码成向量，而是将信息编码成概率空间。这意味着，我们假设数据是从一个先验概率分布中产生的，然后尝试学习如何从这个概率分布中推导出数据。这个概率分布将是一个没有协方差的多元正态分布(N~(μ，σ))。这允许潜在的概率分布由2个n大小的向量来表示，一个用于平均值，另一个用于方差。</p><figure class="lm ln lo lp gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi ni"><img src="../Images/10ce333be8a0a34146c7709e99d14754.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nwCCP5cqlsu5-mpJ6qmcxw.png"/></div></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">VAE建筑——作者图片</p></figure><p id="a043" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们先来看一下前传。数据点通过编码器，但现在将被编码成两个矢量，而不是一个。这两个向量定义了一个概率分布，我们可以从这个概率分布中取样。从这个分布中取样给我们一个数据点的潜在表示。然后，该表示通过解码器获得重新创建的数据点。</p><h1 id="b270" class="me mf it bd mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu mv mw mx my mz na nb bi translated">VAE正规化</h1><p id="22dc" class="pw-post-body-paragraph kg kh it ki b kj nc kl km kn nd kp kq kr ne kt ku kv nf kx ky kz ng lb lc ld im bi translated">如前所述，VAE的另一个重要方面是确保潜在空间的规律性。在此之前，让我们先定义一些术语:</p><ul class=""><li id="f88d" class="lq lr it ki b kj kk kn ko kr ls kv lt kz lu ld nj lw lx ly bi translated"><strong class="ki iu">先验— P(Z) <br/> </strong>先验表示所有数据的底层分布。先验通常是标准正态分布N(0，I ),因为它简单且高度规则。</li><li id="55ce" class="lq lr it ki b kj lz kn ma kr mb kv mc kz md ld nj lw lx ly bi translated"><strong class="ki iu">后验— P(X|Z) <br/> </strong>后验表示给定特定数据点的条件分布<strong class="ki iu">，</strong>本质上，这是数据点被编码成的分布。后验是N(μ，σ)的正态分布，其中σ是对角协方差矩阵。</li></ul><p id="e2d3" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">为了正则化后验分布，我们分配一个代价函数，惩罚模型偏离先验分布。这个成本函数是Kullback-Leibler散度(KL-Divergence ),它度量两个概率分布之间的差异。</p><figure class="lm ln lo lp gt ju gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/78d38145cee25c48c58c898f17cab2a5.png" data-original-src="https://miro.medium.com/v2/resize:fit:956/format:webp/1*w2hatPx0wIxVW9poXdP8Dw.png"/></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">连续概率分布的KL散度—维基百科</p></figure><p id="25da" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">对于许多分布，积分可能难以求解，但对于一个分布(先验)是<strong class="ki iu">标准正态</strong>而另一个分布(后验)有一个<strong class="ki iu">对角协方差矩阵</strong>的特殊情况，KL-散度损失有一个闭合解。</p><figure class="lm ln lo lp gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi nl"><img src="../Images/2f240e940e7e3e4409101190e0cfdccc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*J-sY_acZUq22yOgXYvCWdw.png"/></div></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">高斯和标准高斯之间的KL散度—维基百科</p></figure><p id="1a84" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">当训练VAE时，损失函数由<strong class="ki iu">重建损失</strong>和<strong class="ki iu"> KL发散损失组成。</strong>KL-Divergence的正则化确保后验分布始终是正则的，并且从后验分布进行采样允许生成有意义和有用的数据点。</p><figure class="lm ln lo lp gt ju"><div class="bz fp l di"><div class="nm nn l"/></div></figure></div><div class="ab cl no np hx nq" role="separator"><span class="nr bw bk ns nt nu"/><span class="nr bw bk ns nt nu"/><span class="nr bw bk ns nt"/></div><div class="im in io ip iq"><h1 id="9b51" class="me mf it bd mg mh nv mj mk ml nw mn mo mp nx mr ms mt ny mv mw mx nz mz na nb bi translated">代码段</h1><p id="1360" class="pw-post-body-paragraph kg kh it ki b kj nc kl km kn nd kp kq kr ne kt ku kv nf kx ky kz ng lb lc ld im bi translated">现在让我们进入代码，看看PyTorch中的一切是如何组合在一起的！</p><h2 id="a8a5" class="oa mf it bd mg ob oc dn mk od oe dp mo kr of og ms kv oh oi mw kz oj ok na ol bi translated">初始化</h2><figure class="lm ln lo lp gt ju"><div class="bz fp l di"><div class="nm nn l"/></div></figure><p id="9959" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">初始化相当简单，编码器和解码器本质上与普通自动编码器的架构相同。主要区别在于，有两个额外的层将瓶颈转化为μ和σ向量。在这种情况下，这一层没有压缩，但这是一个可以调整的设计选择。</p><h2 id="f19b" class="oa mf it bd mg ob oc dn mk od oe dp mo kr of og ms kv oh oi mw kz oj ok na ol bi translated">重新参数化技巧</h2><p id="004b" class="pw-post-body-paragraph kg kh it ki b kj nc kl km kn nd kp kq kr ne kt ku kv nf kx ky kz ng lb lc ld im bi translated">实施VAE模型的另一个基本步骤是重新参数化技巧。如果仔细观察架构，从μ和σ向量生成潜在表示涉及采样操作。采样操作的问题在于，它是一个随机过程，梯度不能反向传播回μ和σ矢量。</p><figure class="lm ln lo lp gt ju gh gi paragraph-image"><div class="gh gi om"><img src="../Images/9a8258074f9ae3dbd5583e2c0072bef2.png" data-original-src="https://miro.medium.com/v2/resize:fit:948/format:webp/1*XODTykmSyRwepZjEKbpNXg.png"/></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">将x解构为确定性部分μ和σ以及随机部分z</p></figure><p id="3901" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">⊙ —逐元素乘法运算符</p><p id="9024" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">为此，我们利用了重新参数化技巧，它允许我们<strong class="ki iu">分离操作</strong>的随机和确定部分。其工作原理是，我们从标准正态分布N(0，I)中进行采样，并使用μ和σ矢量对其进行变换。</p><figure class="lm ln lo lp gt ju"><div class="bz fp l di"><div class="nm nn l"/></div></figure><p id="d173" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">总的来说，这允许对μ和σ向量进行梯度更新，这将允许VAE的编码器层从训练过程中学习。</p><h2 id="a94d" class="oa mf it bd mg ob oc dn mk od oe dp mo kr of og ms kv oh oi mw kz oj ok na ol bi translated">把所有的东西放在一起！</h2><figure class="lm ln lo lp gt ju"><div class="bz fp l di"><div class="nm nn l"/></div></figure><p id="8b70" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">正向传递现在只是编码和解码步骤，其间有重新参数化/采样操作。</p><p id="b631" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">需要注意的一件重要事情是，数据被编码为<code class="fe on oo op oq b">log_var</code>而不是<code class="fe on oo op oq b">variance</code>。<code class="fe on oo op oq b">log_var</code>矢量是由许多线性图层生成的，因此，矢量的值将从[-∞，∞]开始。由于方差不能为负，我们取指数，这样方差将有一个合适的范围[0，∞]。</p><p id="c534" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在下一部分，我们将看看PyTorch Lightning如何简化整个过程，并更详细地探索模型及其输出(插值)!</p><p id="9c49" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">请随意查看GitHub上的完整代码，非常感谢您的任何反馈！</p><p id="6ec9" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="ki iu">VAEs入门指南</strong> <a class="ae kf" rel="noopener" target="_blank" href="/beginner-guide-to-variational-autoencoders-vae-with-pytorch-lightning-part-2-6b79ad697c79"> <strong class="ki iu">第二部分</strong> </a></p><p id="0b28" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">github:<a class="ae kf" href="https://github.com/reoneo97/vae-playground" rel="noopener ugc nofollow" target="_blank">https://github.com/reoneo97/vae-playground</a><br/>领英:<a class="ae kf" href="https://www.linkedin.com/in/reo-neo/" rel="noopener ugc nofollow" target="_blank">https://www.linkedin.com/in/reo-neo/</a></p><h1 id="7121" class="me mf it bd mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu mv mw mx my mz na nb bi translated"><strong class="ak">有用链接:</strong></h1><p id="1159" class="pw-post-body-paragraph kg kh it ki b kj nc kl km kn nd kp kq kr ne kt ku kv nf kx ky kz ng lb lc ld im bi translated">(1)了解变分自动编码器(VAEs)</p><p id="d780" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">非常有用的资源，尤其是如果你想深入研究VAEs的数学方面。</p><div class="or os gp gr ot ou"><a rel="noopener follow" target="_blank" href="/understanding-variational-autoencoders-vaes-f70510919f73"><div class="ov ab fo"><div class="ow ab ox cl cj oy"><h2 class="bd iu gy z fp oz fr fs pa fu fw is bi translated">了解变分自动编码器(VAEs)</h2><div class="pb l"><h3 class="bd b gy z fp oz fr fs pa fu fw dk translated">逐步建立导致VAEs的推理。</h3></div><div class="pc l"><p class="bd b dl z fp oz fr fs pa fu fw dk translated">towardsdatascience.com</p></div></div><div class="pd l"><div class="pe l pf pg ph pd pi jz ou"/></div></div></a></div><p id="f0ad" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">(2)变型自动编码器——Arxiv洞察</p><p id="da3e" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">精心制作的视频介绍了VAE的基本知识和机制，同时在结尾部分介绍了许多关于VAEs的最新研究。</p><p id="0c19" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><a class="ae kf" href="https://www.youtube.com/watch?v=9zKuYvjFFS8" rel="noopener ugc nofollow" target="_blank">https://www.youtube.com/watch?v=9zKuYvjFFS8</a></p></div></div>    
</body>
</html>