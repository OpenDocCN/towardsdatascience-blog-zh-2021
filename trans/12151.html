<html>
<head>
<title>11 Different Uses of Dimensionality Reduction</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">降维的 11 种不同用途</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/11-different-uses-of-dimensionality-reduction-4325d62b4fa6?source=collection_archive---------12-----------------------#2021-12-08">https://towardsdatascience.com/11-different-uses-of-dimensionality-reduction-4325d62b4fa6?source=collection_archive---------12-----------------------#2021-12-08</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="65fe" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">整个 ML 都是降维及其应用。让我们看看他们的行动吧！</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/94df47a415fb3cda8f3249d8c376c2aa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GJ2rrsjBI9bKCKPfuI9tYA.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">由<a class="ae ky" href="https://unsplash.com/@nika_benedictova?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">卡尼·贝内迪克托娃</a>在<a class="ae ky" href="https://unsplash.com/?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>拍摄的照片</p></figure><p id="e976" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi lv translated"><span class="l lw lx ly bm lz ma mb mc md di">“D</span><strong class="lb iu">维度还原”</strong>是谷歌搜索引擎中的热门关键词。这是因为整个机器学习充满了降维及其应用。是时候用实例来看看他们的行动了！</p><p id="0102" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">降维是一种无监督的机器学习技术，可以应用于您的输入数据，而无需标签列。用专业术语来说，你的数据中变量(也称为特征或属性)的数量称为数据的<strong class="lb iu"><em class="me"/></strong>。如果你的数据有 3 个变量，你的数据的维数是 3。这意味着数据中的任何一点都可以在三维空间中绘制出来。</p><p id="dbe6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">真实世界的数据有许多变量，这意味着它的维数非常高。通常，这种数据被称为 n 维数据。“n”等于变量的数量，它可以是 10、100、1000 或甚至超过 10，000。</p><p id="e6df" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">随着数据集中变量数量的增加，我们需要减少变量数量，以获得本文即将讨论的优势。这是降维发生的地方。但是请记住，当您减少数据集中的变量数量时，您将丢失原始数据中的某些百分比的可变性。</p><p id="b044" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">因此，降维的有效定义是:</p><blockquote class="mf mg mh"><p id="6a4c" class="kz la me lb b lc ld ju le lf lg jx lh mi lj lk ll mj ln lo lp mk lr ls lt lu im bi translated">降维是减少高维数据中变量的数量，同时尽可能保留原始数据中的可变性的过程。它或者找到少于原始变量数的一组新变量，或者只保留少于原始变量数的最重要的变量。我们应该在要保留的变量数量和原始数据集中的可变性损失之间进行权衡。</p></blockquote><p id="bdeb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">降维的技术有很多。我不打算详细讨论它们，因为我在以前的帖子中已经讨论了 11 种这样的技术。如今，更多地强调降维的不同用途或优势。</p><h1 id="0916" class="ml mm it bd mn mo mp mq mr ms mt mu mv jz mw ka mx kc my kd mz kf na kg nb nc bi translated">降维的用途</h1><p id="dec6" class="pw-post-body-paragraph kz la it lb b lc nd ju le lf ne jx lh li nf lk ll lm ng lo lp lq nh ls lt lu im bi translated">降维的最终用途是获得更少的变量。这间接影响了降维的其他用途。</p><p id="0729" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们用实例逐一讨论。</p><h2 id="40f7" class="ni mm it bd mn nj nk dn mr nl nm dp mv li nn no mx lm np nq mz lq nr ns nb nt bi translated">高维数据的可视化可以通过降维来实现</h2><p id="8dd3" class="pw-post-body-paragraph kz la it lb b lc nd ju le lf ne jx lh li nf lk ll lm ng lo lp lq nh ls lt lu im bi translated">在 EDA(探索性数据分析)中，数据可视化扮演着重要的角色。但是我们经常很难可视化高维数据。这是因为我们只熟悉 2D 图或三维图，它们分别只适用于二维或三维数据。我们无法想象更高维度的情节，如 4D 或 5D 情节！</p><p id="912f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们可以应用降维方法(通常是 PCA)将高维数据转换成可以在 2D 或 3D 图中绘制的 2 维或 3 维数据。</p><p id="5724" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">作为一个例子，考虑有 30 个变量的<em class="me">乳腺癌</em>数据集。所以，数据的维数是 30。在 EDA 中，您需要绘制这些数据，以找到一些重要的模式。你怎么能这样做？显然，您不能在 30D 绘图中绘制数据！您需要将原始数据转换为二维或三维数据，然后制作 2D 和三维图，如下图所示。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nu"><img src="../Images/c95fb3f9de077ee210febbbd20297c5e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QG7_zWgqwHJGKtOsvPYByw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">(图片由作者提供)</p></figure><p id="3b7c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">当您将原始数据转换为二维数据时，仅捕获了原始数据中 63.24%的可变性。当您将原始数据转换为三维数据时，仅捕获了原始数据中 72.64%的可变性。但是，您能够绘制出近似捕捉原始数据的图，并在数据中找到一些模式。</p><p id="b780" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这个应用的详细指南可以在<a class="ae ky" rel="noopener" target="_blank" href="/principal-component-analysis-pca-with-scikit-learn-1e84a0c731b0">这里</a>(我写的)。</p><h2 id="17ff" class="ni mm it bd mn nj nk dn mr nl nm dp mv li nn no mx lm np nq mz lq nr ns nb nt bi translated">可以应用维数减少来减轻过度拟合的问题</h2><p id="5071" class="pw-post-body-paragraph kz la it lb b lc nd ju le lf ne jx lh li nf lk ll lm ng lo lp lq nh ls lt lu im bi translated">过度拟合是数据科学家在建模时面临的最糟糕的问题。降维是可用于减轻机器学习模型中过拟合的技术之一。</p><p id="b752" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在，你可能会想它是如何工作的。降维会找到较少数量的变量，或者从模型中删除最不重要的变量。这将降低模型的复杂性，并消除数据中的一些噪声。这样，维数减少有助于减轻过度拟合。</p><p id="26cc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这个应用的详细指南可以在<a class="ae ky" rel="noopener" target="_blank" href="/how-to-mitigate-overfitting-with-dimensionality-reduction-555b755b3d66">这里</a>找到(我写的)。</p><h2 id="6dcb" class="ni mm it bd mn nj nk dn mr nl nm dp mv li nn no mx lm np nq mz lq nr ns nb nt bi translated">降维会自动消除多重共线性</h2><p id="bf81" class="pw-post-body-paragraph kz la it lb b lc nd ju le lf ne jx lh li nf lk ll lm ng lo lp lq nh ls lt lu im bi translated">并非数据中的所有变量都是独立的。一些输入变量可能与数据集中的其他输入变量相关。这被称为<strong class="lb iu"> <em class="me">多重共线性</em> </strong>，它会对回归和分类模型的性能产生负面影响。</p><p id="3c63" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">当我们应用降维时，它利用了变量之间存在的多重共线性。例如，PCA 将高度相关的变量组合成一组新的不相关变量。主成分分析可以自动消除数据中的多重共线性！</p><p id="682f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在，看下面两张热图。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nv"><img src="../Images/c6be192450eb0d4aef4ddbb1b527cb8d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*O6MgztmbDO5gDk2RnuHTZQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">(图片由作者提供)</p></figure><p id="fae5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">左边的热图显示了有 30 个变量的<em class="me">乳腺癌</em>数据集中变量的相关系数。您可以看到数据集中有高度相关的变量。在对数据集应用 PCA 后，我们将变量的数量减少到 6 个，同时保持原始数据集中 88.76%的可变性。这 6 个变量的相关系数由右图的热图显示。你可以看到那些变量现在是<em class="me">而不是</em>相关。因此，这意味着我们已经通过应用 PCA(一种降维技术)成功地消除了多重共线性。</p><p id="8592" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这个应用的详细指南可以在<a class="ae ky" rel="noopener" target="_blank" href="/how-do-you-apply-pca-to-logistic-regression-to-remove-multicollinearity-10b7f8e89f9b">这里</a>找到(我写的)。</p><h2 id="7b79" class="ni mm it bd mn nj nk dn mr nl nm dp mv li nn no mx lm np nq mz lq nr ns nb nt bi translated">降维用于因子分析</h2><p id="8974" class="pw-post-body-paragraph kz la it lb b lc nd ju le lf ne jx lh li nf lk ll lm ng lo lp lq nh ls lt lu im bi translated">因子分析是一种降维技术。这使用主成分方法，除了一个显著的例外，几乎与 PCA 相同。因子分析发现一些重要的变量，称为<em class="me">因子</em>，它们不在原始数据集中。这些因素是通过分析主成分得出的。这也需要领域知识。</p><p id="8bef" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这个应用的详细指南可以在<a class="ae ky" rel="noopener" target="_blank" href="/factor-analysis-on-women-track-records-data-with-r-and-python-6731a73cd2e0">这里</a>找到(我写的)。</p><h2 id="1d79" class="ni mm it bd mn nj nk dn mr nl nm dp mv li nn no mx lm np nq mz lq nr ns nb nt bi translated">降维可以用于图像压缩</h2><p id="21b6" class="pw-post-body-paragraph kz la it lb b lc nd ju le lf ne jx lh li nf lk ll lm ng lo lp lq nh ls lt lu im bi translated">降维减少了数据集的大小，同时尽可能保持原始数据的可变性。一种类似的方法可以用于图像压缩。因此，在图像压缩中，我们减少图像的像素数量，同时尽可能保持原始图像的质量。</p><p id="c33c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这里，我们将图像中的总像素视为该图像的维数。现在，看下面三张图片。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nw"><img src="../Images/c7abdd29e4a295eef4f88fe5ad355ed8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Xg9peIQfXuiy29uL4GFEmQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">(图片由作者提供)</p></figure><p id="5df7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">左图是 784 维的原图。另外两个图像是压缩图像。中间的图像有 184 个维度。维数从 784 减少到 184，同时保持 96%的图像质量。右图只有 10 个维度。该图像中仅捕获了 40%的数据。</p><p id="f122" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这个应用的详细指南可以在<a class="ae ky" rel="noopener" target="_blank" href="/image-compression-using-principal-component-analysis-pca-253f26740a9f">这里</a>找到(我写的)。</p><p id="5747" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">上面的例子是为了压缩灰度图像。我们还可以使用降维来压缩 RGB 彩色图像。这里有一个例子。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/6d389841677eb93bf613c252abbde48e.png" data-original-src="https://miro.medium.com/v2/resize:fit:886/format:webp/1*QnWiusn7iJlcZXM9JESgog.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">花意象比较(作者图片)</p></figure><ul class=""><li id="7938" class="ny nz it lb b lc ld lf lg li oa lm ob lq oc lu od oe of og bi translated"><strong class="lb iu">左上:</strong>400 维的原始花卉图像。</li><li id="91b1" class="ny nz it lb b lc oh lf oi li oj lm ok lq ol lu od oe of og bi translated"><strong class="lb iu">右上:</strong>只有 40 维的压缩花图像。维数减少了 10 倍，同时保持了原始图像 91.24%的质量！我们仍然可以识别图像中的重要部分。</li><li id="96f2" class="ny nz it lb b lc oh lf oi li oj lm ok lq ol lu od oe of og bi translated"><strong class="lb iu">左下:</strong>只有 25 维的压缩花图像。维数减少了 16 倍，同时保持了原始图像 83.41%的质量！我们仍然可以识别图像中的重要部分。</li><li id="8211" class="ny nz it lb b lc oh lf oi li oj lm ok lq ol lu od oe of og bi translated"><strong class="lb iu">右下:</strong>不做特征缩放应用 PCA 后的花朵图像输出。转换后的数据不能代表原始数据。我在这里向您展示了在应用 PCA 之前进行特征缩放的重要性。</li></ul><p id="c834" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这个应用的详细指南可以在<a class="ae ky" rel="noopener" target="_blank" href="/rgb-color-image-compression-using-principal-component-analysis-fce3f48dfdd0">这里</a>找到(我写的)。</p><h2 id="0259" class="ni mm it bd mn nj nk dn mr nl nm dp mv li nn no mx lm np nq mz lq nr ns nb nt bi translated">降维减少了模型的训练时间</h2><p id="762e" class="pw-post-body-paragraph kz la it lb b lc nd ju le lf ne jx lh li nf lk ll lm ng lo lp lq nh ls lt lu im bi translated">高维空间中的数据点更有可能彼此远离。这是因为高维空间中有大量的可用空间。训练时，算法会对这些数据点进行计算。当维数很高，数据点之间距离很远时，计算会极其缓慢。因此，机器学习和深度学习算法将无法在高维数据上高效地训练模型。因此，在我们将数据输入算法之前，应该对数据应用降维技术。</p><h2 id="6d87" class="ni mm it bd mn nj nk dn mr nl nm dp mv li nn no mx lm np nq mz lq nr ns nb nt bi translated">在训练模型时，降维节省了大量的计算资源</h2><p id="2392" class="pw-post-body-paragraph kz la it lb b lc nd ju le lf ne jx lh li nf lk ll lm ng lo lp lq nh ls lt lu im bi translated">这很简单！由于降维通过简化计算减少了模型的训练时间，因此训练这些模型所需的计算资源将非常少。因此，降维在训练模型时自动节省了大量的计算资源。</p><h2 id="4420" class="ni mm it bd mn nj nk dn mr nl nm dp mv li nn no mx lm np nq mz lq nr ns nb nt bi translated">降维降低了模型的复杂性</h2><p id="a451" class="pw-post-body-paragraph kz la it lb b lc nd ju le lf ne jx lh li nf lk ll lm ng lo lp lq nh ls lt lu im bi translated">只要简单模型足够强大，能够做出准确的预测，简单模型就比复杂模型好。它们也很容易解读。通过只保留数据中最重要的变量或减少变量的数量，降维降低了模型的复杂性。</p><h2 id="974a" class="ni mm it bd mn nj nk dn mr nl nm dp mv li nn no mx lm np nq mz lq nr ns nb nt bi translated">降维提高了模型的准确性</h2><p id="2f51" class="pw-post-body-paragraph kz la it lb b lc nd ju le lf ne jx lh li nf lk ll lm ng lo lp lq nh ls lt lu im bi translated">数据的噪声去除可以发生在降维中，因为它去除了相关变量和最不重要的变量。这将进一步提高模型的准确性。</p><h2 id="bf7e" class="ni mm it bd mn nj nk dn mr nl nm dp mv li nn no mx lm np nq mz lq nr ns nb nt bi translated">降维将非线性数据转换成线性可分的形式</h2><p id="eb5c" class="pw-post-body-paragraph kz la it lb b lc nd ju le lf ne jx lh li nf lk ll lm ng lo lp lq nh ls lt lu im bi translated">大多数真实世界的数据不是线性可分的。在这种情况下，线性超平面(2d 空间中的直线或 3d 或更高维空间中的适当平面)无法对非线性数据进行分类。为了使线性超平面能够对非线性数据进行分类，我们应该将非线性数据转换成线性形式。</p><p id="5a53" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">一种称为<strong class="lb iu">核 PCA </strong>(也称为正常 PCA 的非线性形式)的降维方法可用于将非线性数据转换为线性可分离形式。内核 PCA 分两步运行。</p><ul class=""><li id="220c" class="ny nz it lb b lc ld lf lg li oa lm ob lq oc lu od oe of og bi translated">输入数据被馈送到算法中的核函数(通常是“rbf”核),并被投影到一个新的高维空间中，在该空间中，非线性数据变得可线性分离。</li><li id="4c9c" class="ny nz it lb b lc oh lf oi li oj lm ok lq ol lu od oe of og bi translated">然后，将普通 PCA 应用于从上一步骤投影的较高维数据，以将数据恢复到原始的较低维空间中。</li></ul><p id="b651" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在，看下面的情节。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi om"><img src="../Images/8b2292051c9d1b04ae45dce0c1424e71.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2IsSRZv4keNgt4S-bKfWFg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">(图片由作者提供)</p></figure><p id="5526" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">原始数据绘制在左侧的图像中。你可以清楚地看到，原始数据是非线性的，不能用一个线性超平面(本例中是直线)来分隔。应用核主成分分析后的数据绘制在右图中。此时，转换后的数据是线性可分的。</p><h2 id="2696" class="ni mm it bd mn nj nk dn mr nl nm dp mv li nn no mx lm np nq mz lq nr ns nb nt bi translated">降维可用于压缩神经网络结构</h2><p id="62c4" class="pw-post-body-paragraph kz la it lb b lc nd ju le lf ne jx lh li nf lk ll lm ng lo lp lq nh ls lt lu im bi translated">这可以通过使用一种称为<strong class="lb iu"> <em class="me">自动编码器</em> </strong>的特殊神经网络架构来实现，该架构将高维输入数据压缩到较低的维度。换句话说，它找到输入数据的压缩表示。</p><p id="0dad" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">自动编码器有两个功能，也称为运算符:</p><ul class=""><li id="f730" class="ny nz it lb b lc ld lf lg li oa lm ob lq oc lu od oe of og bi translated"><strong class="lb iu">编码器:</strong>这是一个非线性函数，它将输入数据转换成一个叫做<em class="me">潜在向量</em>的低维形式。</li><li id="5730" class="ny nz it lb b lc oh lf oi li oj lm ok lq ol lu od oe of og bi translated"><strong class="lb iu">解码器:</strong>这也是一个非线性函数，以潜向量为输入，构造另一个与原始输入非常相似的输出。目标是最小化<em class="me">重建误差</em>。</li></ul><p id="ba97" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在，请看下图中的自动编码器。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi on"><img src="../Images/681c6360e46ad22a04e8cb4e00bdcb2b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1K1cINrvxTI5EU4IKroyUQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">(图片由作者提供)</p></figure><p id="83cc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">编码器将 X (784-dim)作为输入，并将其转换为较低维的潜在向量(184-dim ),解码器将其作为输入。解码器的输出与原始输入 X 非常相似，但并不完全相同。输入和输出之间的不相似性由应该尽可能保持最小的<em class="me">重构误差</em>或<em class="me">损失函数</em>来测量。</p><h1 id="768f" class="ml mm it bd mn mo mp mq mr ms mt mu mv jz mw ka mx kc my kd mz kf na kg nb nc bi translated">摘要</h1><p id="16d3" class="pw-post-body-paragraph kz la it lb b lc nd ju le lf ne jx lh li nf lk ll lm ng lo lp lq nh ls lt lu im bi translated">整个 ML 充满了具有极其有用的优点的降维。有许多降维技术。你在这篇文章中已经听到了一些。PCA 是最常用的，它是一种线性方法。因子分析是 PCA 的一个特例。核 PCA 是一种可以应用于非线性数据的非线性方法。自动编码器的工作方式类似于 PCA，但应该用于非常大的数据集和神经网络应用程序。</p><p id="e9ff" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">降维方法的详细解释可以在<a class="ae ky" rel="noopener" target="_blank" href="/11-dimensionality-reduction-techniques-you-should-know-in-2021-dcb9500d388b">这里</a>(我写的)。</p></div><div class="ab cl oo op hx oq" role="separator"><span class="or bw bk os ot ou"/><span class="or bw bk os ot ou"/><span class="or bw bk os ot"/></div><div class="im in io ip iq"><p id="8bc5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">今天的帖子到此结束。</p><blockquote class="mf mg mh"><p id="f839" class="kz la me lb b lc ld ju le lf lg jx lh mi lj lk ll mj ln lo lp mk lr ls lt lu im bi translated">你以为整个 ML 都是降维及其应用？</p></blockquote><p id="a8aa" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">如果你有任何反馈请告诉我</strong>。</p><p id="5e69" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="me">我希望你喜欢阅读这篇文章。如果你愿意支持我成为一名作家，请考虑</em> <a class="ae ky" href="https://rukshanpramoditha.medium.com/membership" rel="noopener"> <strong class="lb iu"> <em class="me">注册会员</em> </strong> </a> <em class="me">以获得无限制的媒体访问权限。它只需要每月 5 美元，我会收到你的会员费的一部分。</em></p><div class="ov ow gp gr ox oy"><a href="https://rukshanpramoditha.medium.com/membership" rel="noopener follow" target="_blank"><div class="oz ab fo"><div class="pa ab pb cl cj pc"><h2 class="bd iu gy z fp pd fr fs pe fu fw is bi translated">通过我的推荐链接加入 Medium</h2><div class="pf l"><h3 class="bd b gy z fp pd fr fs pe fu fw dk translated">作为一个媒体会员，你的会员费的一部分会给你阅读的作家，你可以完全接触到每一个故事…</h3></div><div class="pg l"><p class="bd b dl z fp pd fr fs pe fu fw dk translated">rukshanpramoditha.medium.com</p></div></div><div class="ph l"><div class="pi l pj pk pl ph pm ks oy"/></div></div></a></div><p id="1f28" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">非常感谢你一直以来的支持！下一个故事再见。祝大家学习愉快！</p></div><div class="ab cl oo op hx oq" role="separator"><span class="or bw bk os ot ou"/><span class="or bw bk os ot ou"/><span class="or bw bk os ot"/></div><div class="im in io ip iq"><h2 id="a8a4" class="ni mm it bd mn nj nk dn mr nl nm dp mv li nn no mx lm np nq mz lq nr ns nb nt bi translated">接下来阅读(推荐)——我写的！</h2><p id="927a" class="pw-post-body-paragraph kz la it lb b lc nd ju le lf ne jx lh li nf lk ll lm ng lo lp lq nh ls lt lu im bi translated">寻找不同的方法来减少数据集的大小，同时尽可能地保持变化。</p><div class="ov ow gp gr ox oy"><a rel="noopener follow" target="_blank" href="/11-dimensionality-reduction-techniques-you-should-know-in-2021-dcb9500d388b"><div class="oz ab fo"><div class="pa ab pb cl cj pc"><h2 class="bd iu gy z fp pd fr fs pe fu fw is bi translated">2021 年你应该知道的 11 种降维技术</h2><div class="pf l"><h3 class="bd b gy z fp pd fr fs pe fu fw dk translated">减少数据集的大小，同时尽可能保留变化</h3></div><div class="pg l"><p class="bd b dl z fp pd fr fs pe fu fw dk translated">towardsdatascience.com</p></div></div><div class="ph l"><div class="pn l pj pk pl ph pm ks oy"/></div></div></a></div><p id="30ba" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">了解主成分分析(PCA)的幕后工作原理。</p><div class="ov ow gp gr ox oy"><a href="https://rukshanpramoditha.medium.com/eigendecomposition-of-a-covariance-matrix-with-numpy-c953334c965d" rel="noopener follow" target="_blank"><div class="oz ab fo"><div class="pa ab pb cl cj pc"><h2 class="bd iu gy z fp pd fr fs pe fu fw is bi translated">具有 NumPy 的协方差矩阵的特征分解</h2><div class="pf l"><h3 class="bd b gy z fp pd fr fs pe fu fw dk translated">对于主成分分析(PCA)</h3></div><div class="pg l"><p class="bd b dl z fp pd fr fs pe fu fw dk translated">rukshanpramoditha.medium.com</p></div></div><div class="ph l"><div class="po l pj pk pl ph pm ks oy"/></div></div></a></div><p id="6f3c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">一站式解决您关于 PCA 的大部分问题。</p><div class="ov ow gp gr ox oy"><a href="https://rukshanpramoditha.medium.com/principal-component-analysis-18-questions-answered-4abd72041ccd" rel="noopener follow" target="_blank"><div class="oz ab fo"><div class="pa ab pb cl cj pc"><h2 class="bd iu gy z fp pd fr fs pe fu fw is bi translated">主成分分析—回答了 18 个问题</h2><div class="pf l"><h3 class="bd b gy z fp pd fr fs pe fu fw dk translated">一站式解决关于 PCA 的大部分问题</h3></div><div class="pg l"><p class="bd b dl z fp pd fr fs pe fu fw dk translated">rukshanpramoditha.medium.com</p></div></div><div class="ph l"><div class="pp l pj pk pl ph pm ks oy"/></div></div></a></div></div><div class="ab cl oo op hx oq" role="separator"><span class="or bw bk os ot ou"/><span class="or bw bk os ot ou"/><span class="or bw bk os ot"/></div><div class="im in io ip iq"><p id="ad02" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">特别感谢 Unsplash 上的<a class="ae ky" href="https://unsplash.com/@nika_benedictova?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">卡尼·贝内迪克托娃</a> <strong class="lb iu"> </strong>，<strong class="lb iu"> </strong>为我提供了这篇文章的精美封面图片。</p><p id="15ef" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><a class="pq pr ep" href="https://medium.com/u/f90a3bb1d400?source=post_page-----4325d62b4fa6--------------------------------" rel="noopener" target="_blank">鲁克山·普拉莫迪塔</a><br/><strong class="lb iu">2021–12–08<br/></strong>(最后编辑时间:2022–04–26)</p></div></div>    
</body>
</html>