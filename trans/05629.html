<html>
<head>
<title>Understanding the Expressive Power of Neural Networks</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">理解神经网络的表达能力</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/understanding-the-expressive-power-of-neural-networks-d4918c9e98da?source=collection_archive---------21-----------------------#2021-05-19">https://towardsdatascience.com/understanding-the-expressive-power-of-neural-networks-d4918c9e98da?source=collection_archive---------21-----------------------#2021-05-19</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="ece8" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsdatascience.com/tagged/hands-on-tutorials" rel="noopener" target="_blank">实践教程</a></h2><div class=""/><p id="3b48" class="pw-post-body-paragraph jz ka it kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw im bi translated">我一直对神经网络似乎可以学习任何东西的能力感到惊讶。AlphaGo、谷歌翻译和特斯拉Autopilot都使用神经网络，但用于完全不同的任务。这怎么可能呢？事实证明，有一种叫做<strong class="kb jd">通用逼近定理</strong>的东西证明，在一定条件下，神经网络可以以任意精度逼近任何函数。这个定理的完整陈述和证明需要一些高等数学来理解，但是我们来看一个更简单的版本，建立一些直觉。首先，我们将使用一个通用的激活函数来构建任意函数。然后，我们将演示如何用神经网络来表示我们构建的任何函数。</p><p id="5186" class="pw-post-body-paragraph jz ka it kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw im bi translated"><strong class="kb jd">第一部分:从激活函数到任意函数</strong></p><p id="dcc9" class="pw-post-body-paragraph jz ka it kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw im bi translated">通用逼近定理的完整陈述涉及向量值函数。为了简化，我们将考虑标量函数。我们的目标是逼近任意函数y = f(x)，其中x和y是实数。回想一下高中微积分，任何这样的函数都可以用矩形来近似，如下所示。</p><figure class="ky kz la lb gt lc gh gi paragraph-image"><div class="gh gi kx"><img src="../Images/c78e64d381415364cc3c63df435beb51.png" data-original-src="https://miro.medium.com/v2/resize:fit:1136/format:webp/1*Es0IZBzpvG5TkLKY01fy7g.png"/></div><p class="lf lg gj gh gi lh li bd b be z dk translated">y = x的矩形近似。</p></figure><p id="21db" class="pw-post-body-paragraph jz ka it kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw im bi translated">矩形越窄、越多，近似值就越好:</p><figure class="ky kz la lb gt lc gh gi paragraph-image"><div class="gh gi lj"><img src="../Images/2a3c53a510a66d4686ee5e9231942282.png" data-original-src="https://miro.medium.com/v2/resize:fit:1156/format:webp/1*yB2UICKKS7k10kRNSJTo2Q.png"/></div><p class="lf lg gj gh gi lh li bd b be z dk translated">更多的矩形，更好的近似。图片作者。</p></figure><p id="8995" class="pw-post-body-paragraph jz ka it kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw im bi translated">因此，如果我们能创建任意宽度和高度的矩形，并把它们放在我们想要的任何地方，我们就能逼近任何函数y = f(x)。现在我们应该怎么做呢？</p><p id="08c9" class="pw-post-body-paragraph jz ka it kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw im bi translated">我们的第一个提示来自神经网络中常用的激活函数，tanh函数。k = 1的函数y = tanh(kx)如下所示:</p><figure class="ky kz la lb gt lc gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi lk"><img src="../Images/1a6ab796c87123af81b5690eadef2379.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*o4Lj5pGCYkylcbPP6FUbTw.png"/></div></div><p class="lf lg gj gh gi lh li bd b be z dk translated">y = tanh(x)。图片作者。</p></figure><p id="771f" class="pw-post-body-paragraph jz ka it kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw im bi translated">随着k的增加，请注意图形的对角线部分变得越来越垂直。这里是y = tanh(10x):</p><figure class="ky kz la lb gt lc gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi lp"><img src="../Images/f7901cc06be6531c31f78cdb58abc1cd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cL8UzVdkPOE4VUcj-Yl2fQ.png"/></div></div><p class="lf lg gj gh gi lh li bd b be z dk translated">y = tanh(10x)。图片作者。</p></figure><p id="58c7" class="pw-post-body-paragraph jz ka it kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw im bi translated">这里是y = tanh(1000x):</p><figure class="ky kz la lb gt lc gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi lp"><img src="../Images/3266ee09a02ab3925e6f766040188ca4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CY8YNZgwJHySr67qUgAz2Q.png"/></div></div><p class="lf lg gj gh gi lh li bd b be z dk translated">y = tanh(1000x)。图片作者。</p></figure><p id="d863" class="pw-post-body-paragraph jz ka it kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw im bi translated">我们现在有一个矩形的左边缘。但是仍然有一些问题:我们希望图的水平部分是0，而不是-1和1，而且我们还没有构造右边。幸运的是，有一种方法可以同时解决这些问题。请注意，如果我们反转(取负值)双曲正切函数并向右移动1，我们会得到这个函数:</p><figure class="ky kz la lb gt lc gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi lp"><img src="../Images/3e1745248222be7fcd8d1672443652b1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VmbGsBArkOecpWQICY4D0g.png"/></div></div><p class="lf lg gj gh gi lh li bd b be z dk translated">y = -tanh(1000(x-1))。图片作者。</p></figure><p id="9123" class="pw-post-body-paragraph jz ka it kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw im bi translated">现在，如果我们将这两个函数相加，我们得到:</p><figure class="ky kz la lb gt lc gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi lk"><img src="../Images/48c661947fd5e552d5667ee023ec3179.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*p75SJEr_wfR9xkLnVpQ_QQ.png"/></div></div><p class="lf lg gj gh gi lh li bd b be z dk translated">y = tanh(1000 x)-tanh(1000(x-1))。图片作者。</p></figure><p id="8247" class="pw-post-body-paragraph jz ka it kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw im bi translated">我们已经实现了制作矩形的目标！现在让我们花一些时间来观察矩形函数的属性。</p><ul class=""><li id="6489" class="lq lr it kb b kc kd kg kh kk ls ko lt ks lu kw lv lw lx ly bi translated">首先，除了矩形所在的地方(这里是区间(0，1))，矩形函数在任何地方都是0。这意味着我们可以添加任意多的其他矩形函数，只要没有矩形相互重叠。</li><li id="fe10" class="lq lr it kb b kc lz kg ma kk mb ko mc ks md kw lv lw lx ly bi translated">第二，我们可以任意调整矩形的宽度。例如，如果我们希望宽度为2，我们的函数将是y = tanh(1000 x)-tanh(1000(x-2))。</li><li id="a641" class="lq lr it kb b kc lz kg ma kk mb ko mc ks md kw lv lw lx ly bi translated">第三，我们可以任意调整矩形的高度。如果我们想要高度为3，我们的函数将是y =(3/2)tanh(1000 x)—(3/2)tanh(1000(x-1))。</li><li id="65ff" class="lq lr it kb b kc lz kg ma kk mb ko mc ks md kw lv lw lx ly bi translated">最后，我们可以调整矩形在x轴上的位置。如果我们想要一个从x = -1而不是x = 0开始的矩形，我们的函数应该是y = tanh(1000(x+1)) — tanh(1000x)。</li></ul><p id="9c4e" class="pw-post-body-paragraph jz ka it kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw im bi translated">我们可以用下面的规则来概括这些观察结果。如果我们想要一个矩形，其左侧为x = s，高度为h，宽度为w，近似精度为k(k越高越精确),我们的等式为:</p><blockquote class="me"><p id="b641" class="mf mg it bd mh mi mj mk ml mm mn kw dk translated">y =(h/2)tanh(k(x-s))—(h/2)tanh(k(x-s-w))</p></blockquote><p id="f385" class="pw-post-body-paragraph jz ka it kb b kc mo ke kf kg mp ki kj kk mq km kn ko mr kq kr ks ms ku kv kw im bi translated">例如，对于一个左侧为x = -2，高度为2，宽度为3，精度为1000的矩形，一旦您插入数字，等式就是y =-tanh(1000(x+2))+tanh(1000(x-1))。见下文:</p><figure class="ky kz la lb gt lc gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi mt"><img src="../Images/f2633e913de52888fd9ee1e572bf7d40.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hQNjaSEvlt2DwEXc4nST3w.png"/></div></div><p class="lf lg gj gh gi lh li bd b be z dk translated">y =-tanh(1000(x+2))+tanh(1000(x-1))。图片作者。</p></figure><p id="9b78" class="pw-post-body-paragraph jz ka it kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw im bi translated">现在我们有了逼近任何函数所需的工具。例如，我们可以使用三个矩形来粗略地逼近从0到3的恒等函数:</p><figure class="ky kz la lb gt lc gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi mu"><img src="../Images/faf16c9147ccdb557d3fa6e3509f7092.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2hdxL2aSUEYJSOPkFEEsBQ.png"/></div></div><p class="lf lg gj gh gi lh li bd b be z dk translated">y =[(1/2)tanh(1000 x)—(1/2)tanh(1000(x-1))]+[tanh(1000(x-1))—tanh(1000(x-2))]+[(3/2)tanh(1000(x-2))—(3/2)tanh(1000(x-3))]。图片作者。</p></figure><p id="7446" class="pw-post-body-paragraph jz ka it kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw im bi translated">如果我们想做得更好，我们可以使用更多的矩形:</p><figure class="ky kz la lb gt lc gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi mv"><img src="../Images/580ea373ac36b24dd9d9cacc27db9cf3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XbnKpUFPU8yyu4GuVmsIzw.png"/></div></div><p class="lf lg gj gh gi lh li bd b be z dk translated">y =[(0.5/2)tanh(1000 x)—(0.5/2)tanh(1000(x-0.5))]+[(1/2)tanh(1000(x-0.5))—(1/2)tanh(1000(x-1))]+[(1.5/2)tanh(1000(x-1))—(1.5/2)tanh(1000(x-1))]+图片作者。</p></figure><p id="9496" class="pw-post-body-paragraph jz ka it kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw im bi translated"><strong class="kb jd">第二部分:从任意矩形到神经网络</strong></p><p id="5ba5" class="pw-post-body-paragraph jz ka it kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw im bi translated">下一步是找到一种方法，通过神经网络来表示我们的矩形。这被证明是非常简单的。让我们再来看看我们为任意矩形找到的公式:y =(h/2)tanh(k(x-s))—(h/2)tanh(k(x-s-w))。使用具有一个隐藏层的网络，我们可以表示这个函数:</p><figure class="ky kz la lb gt lc gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi mw"><img src="../Images/7d74a552c554dca2ec763ecbd0569a66.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DmX9wYx8CdvW1Srho7ZFHA.png"/></div></div><p class="lf lg gj gh gi lh li bd b be z dk translated">图片作者。</p></figure><p id="d13d" class="pw-post-body-paragraph jz ka it kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw im bi translated">该网络接收一个输入x，并有一个针对输入层的偏置项(用正方形表示)。隐藏层有两个节点，都有tanh激活，没有偏置项。单输出节点没有激活功能。整个网络的权重如图所示。您可以自己验证所有的权重相加，输出确实是我们的矩形函数。</p><p id="f71f" class="pw-post-body-paragraph jz ka it kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw im bi translated">最后一个问题——如果我们想要不止一个矩形呢？解决方案很简单:对于每个额外的矩形，我们向隐藏层添加两个节点，并向输入和输出层添加适当的权重/连接。这意味着每个矩形需要两个隐藏节点来代表我们的网络。例如，我们上面看到的身份函数的六个矩形近似将有十二个隐藏节点。</p><p id="449e" class="pw-post-body-paragraph jz ka it kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw im bi translated">把所有的东西放在一起，我们现在有一种方法，用神经网络来表示任何函数y = f(x)和x，y实数。这样做的方法是将函数分解成一些矩形(矩形越多，逼近越好)，并为每个矩形找到将在我们的广义矩形公式中使用的参数h、k、s和w。然后，我们可以使用如上所示的找到的参数构建神经网络，对每个矩形/参数集使用两个隐藏节点。</p><p id="53e5" class="pw-post-body-paragraph jz ka it kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw im bi translated"><strong class="kb jd">限制</strong></p><p id="11e7" class="pw-post-body-paragraph jz ka it kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw im bi translated">在这一点上，你可能会问——那么是什么阻碍了呢？如果任何功能都可以用神经网络来表示，为什么我们没有完美的自动驾驶汽车？为什么人工智能不跑来跑去解决世界上所有的问题？简而言之，问题在于神经网络很难训练。仅仅因为您知道应该有一组参数很好地逼近您给定网络架构的功能，并不意味着有任何好的方法来找到这些参数。训练神经网络最常用的方法是梯度下降法，这种方法在实际中对于某些任务(例如图像分类)非常有效，但是不能保证梯度下降法能够为任意函数找到正确的参数。这是一个棘手的问题，在未来我会写一些关于它。</p><p id="8f14" class="pw-post-body-paragraph jz ka it kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw im bi translated">请随时留下您的任何问题/意见！让我知道你还想看什么机器学习话题。</p></div></div>    
</body>
</html>