<html>
<head>
<title>An Introduction to Reinforcement Learning: the K-Armed Bandit</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">强化学习导论:K臂强盗</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/an-introduction-to-reinforcement-learning-the-k-armed-bandit-3399593e68e8?source=collection_archive---------31-----------------------#2021-06-18">https://towardsdatascience.com/an-introduction-to-reinforcement-learning-the-k-armed-bandit-3399593e68e8?source=collection_archive---------31-----------------------#2021-06-18</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="bc0e" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">k臂土匪问题引入了很多强化学习的思想。多读点了解一下。</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/27edd3350f8f0f86bd5942523af8fd6c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*Y3fpmudsaw93vnpo"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><a class="ae ky" href="https://unsplash.com/@riverse?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">亲爱的</a>在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="9b55" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">最近有很多关于强化学习(RL)的宣传，这是理所当然的。自从DeepMind发表论文“<a class="ae ky" href="https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf" rel="noopener ugc nofollow" target="_blank">用深度强化学习玩雅达利</a>”以来，已经出现了许多有希望的结果，其中最著名的可能是AlphaGo。然而，在我们能够理解这些模型如何工作之前，我们需要理解强化学习的一些基本原理。我认为对这些概念最好的介绍是通过一个简单得多的问题——所谓的“k臂土匪问题”。</p><h1 id="8c38" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">问题是</h1><p id="d923" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">首先，我们来定义一下什么是k臂土匪问题。“k臂强盗”这个词让人想起《星球大战》中波巴·费特和章鱼杂交的画面，但幸运的是事情没有那么疯狂。相反，你可以把强盗想象成一个老虎机，而k个手臂中的每一个都是一个杠杆。拉动k杆中的一个会给你一个正的回报。在大多数强化学习问题中，你的目标是<strong class="lb iu">最大化你的总回报，给定固定数量(比如1000)的拉力</strong>。你唯一的先验信息是每个杠杆的平均回报不随时间变化。这里的最佳策略是什么？</p><p id="f5af" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果我们知道每个杠杆的平均回报，这个问题就简单了。我们总是拉着平均回报最高的杠杆。所以一个可能的方法是首先使用一些拉力来计算出哪一个杠杆有最高的平均回报，然后只拉那个杠杆。</p><p id="2e90" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">应该怎么搞清楚哪个杠杆最好？我们需要多次尝试所有的杠杆，因为即使每个杠杆的平均回报是固定的，每次拉动杠杆都会得到不同的回报。一旦我们多次尝试每个杠杆，我们可以取所有拉力的平均值作为真实回报的近似值。换句话说，一种可能的算法是:</p></div><div class="ab cl ms mt hx mu" role="separator"><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx"/></div><div class="im in io ip iq"><p id="23d0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">算法1 </strong></p><p id="8a26" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">拉动每根杠杆n次，平均每根杠杆的n次回报。</p><p id="3498" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">平均值最高的杠杆就是最好的杠杆。从现在开始只拉那个。</p></div><div class="ab cl ms mt hx mu" role="separator"><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx"/></div><div class="im in io ip iq"><p id="54d1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这就有一个问题，就是我们不知道如何设置n，比如说我们用n = 3。对于其中一个杠杆，我们拉3次，我们得到14、15和16的回报，平均下来是15。然而，那个杠杆的真实平均回报是5——我们只是非常幸运地拉了三次。现在我们有麻烦了，因为我们总是假设这个杠杆比它实际上要好得多。我们可以使n变大，这将使我们更加确信我们的平均值是准确的。然而，由于我们的时间步数有限，我们不想浪费杠杆拉动。因此，设置大的n也有问题。</p><p id="161c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们可以完全避免固定一个数字n。相反，我们保留了每个杠杆的运行平均回报。另一种可能的算法是:</p></div><div class="ab cl ms mt hx mu" role="separator"><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx"/></div><div class="im in io ip iq"><p id="4bec" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">算法二</strong></p><p id="5ab4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在做任何拉动之前，将每个杠杆的移动平均值设置为0。</p><p id="c561" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对于每个时间步长:</p><ul class=""><li id="34ec" class="mz na it lb b lc ld lf lg li nb lm nc lq nd lu ne nf ng nh bi translated">选择运行平均值最高的杠杆。如果有平局，在平局的杠杆之间随机选择。</li><li id="7c44" class="mz na it lb b lc ni lf nj li nk lm nl lq nm lu ne nf ng nh bi translated">拉动我们刚刚选择的杠杆，观察回报，并用回报修改该杠杆的运行平均值。</li></ul></div><div class="ab cl ms mt hx mu" role="separator"><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx"/></div><div class="im in io ip iq"><p id="d698" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">从理论上讲，这种算法的优势在于，它总是越来越接近每个杠杆的真实平均回报。没有每根杠杆n的拉动次数，在此之后我们停止学习。可惜行不通。在第一个时间步，这个算法将拉动一个随机杠杆，得到一个正的回报。然后，因为所有其他运行平均值都设置为0，它将永远从第一个时间步长继续拉动杠杆。它永远不会碰到其他杠杆。</p><p id="4f31" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">解决这个问题的一个办法是使用另一个参数epsilon，它控制我们是拉具有最高运行平均值的杠杆，还是拉随机的杠杆。我们的算法变成了:</p></div><div class="ab cl ms mt hx mu" role="separator"><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx"/></div><div class="im in io ip iq"><p id="1300" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">算法三</strong></p><p id="d304" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在做任何拉动之前，将每个杠杆的移动平均值设置为0。</p><p id="c80b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">将epsilon设置为[0，1]中的一个数字。</p><p id="2cbb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对于每个时间步长:</p><ul class=""><li id="24cc" class="mz na it lb b lc ld lf lg li nb lm nc lq nd lu ne nf ng nh bi translated">在[0，1]中得到一个随机数(来自均匀分布)x。</li><li id="88d2" class="mz na it lb b lc ni lf nj li nk lm nl lq nm lu ne nf ng nh bi translated">如果x &gt;ε，选择运行平均值最高的杠杆。如果有平局，在平局的杠杆之间随机选择。</li><li id="78ab" class="mz na it lb b lc ni lf nj li nk lm nl lq nm lu ne nf ng nh bi translated">如果x </li><li id="42e1" class="mz na it lb b lc ni lf nj li nk lm nl lq nm lu ne nf ng nh bi translated">拉动我们刚刚选择的杠杆，观察回报，并用回报修改该杠杆的运行平均值。</li></ul></div><div class="ab cl ms mt hx mu" role="separator"><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx"/></div><div class="im in io ip iq"><p id="f17b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这个算法确实有效。通过x的随机性，最终我们将被迫尝试所有的杠杆足够的次数，以获得所有杠杆的精确运行平均值。然而，我们必须决定ε是什么。如果我们将epsilon设置得更接近1，算法将更均匀地尝试杠杆，从而得到更好的估计，但总回报会更低。如果我们将epsilon设置得更接近0，算法将更多地关注几个杠杆，导致不太准确的估计，但可能会有更高的回报。</p><h1 id="814c" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">RL视角</h1><p id="1d26" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">现在让我们把所有这些放在强化学习的术语中。首先，我们注意到我们想要得到每个杠杆的真实平均回报的精确估计。我们称每个杠杆的真实平均回报为问题的<strong class="lb iu">行动价值函数</strong>。为什么叫这个？嗯，在我们的问题中，我们可以采取的可能的<strong class="lb iu">行动</strong>是拉动每个杠杆。每拉一次杠杆，就有一个真实的平均奖励值。更正式地说，动作值函数由<strong class="lb iu"> Q(a) </strong>表示。<strong class="lb iu"> Q </strong>是函数，<strong class="lb iu"> a </strong>代表可能的动作。我们不知道<strong class="lb iu"> Q </strong>，所以我们试着估计一下，记为<strong class="lb iu"> Q* </strong>。我们的希望是Q*将接近Q。</p><p id="0c58" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们也想出了一些具体的算法来解决这个土匪问题。算法1有两个独立的部分。第一部分试图通过使用每个杠杆n次拉动的样本平均值来近似动作值函数(求Q*)。第二部分用Q*来决定拉哪个杠杆。在高层次上，算法的第一部分，寻找Q*，被称为<strong class="lb iu">探索</strong>。算法的第二部分，使用由Q*确定的最佳动作/拉动，称为<strong class="lb iu">利用</strong>。我们在设置该算法的参数n时遇到了一些问题。同样，我们很难决定如何在勘探和开采之间划分时间间隔。这是强化学习中的一个常见主题。</p><p id="0dee" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们把总是利用<strong class="lb iu">贪婪算法的算法叫做</strong>。贪婪算法总是试图最大化每个时间步长的回报；它不探索。我们在算法2中看到了这个问题。因为没有进行探索，所以在短期内贪婪算法可能工作得很好，但从长期来看，它几乎肯定是次优的。</p><p id="8ef5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们提出的另一个算法(算法3)使用结合了参数ε的移动平均值。艾司隆有效地控制了勘探和开采之间的平衡。如果ε接近0，大部分时间我们都会在做剥削，或者贪婪算法。正因为如此，这类算法被称为“<strong class="lb iu"> epsilon-greedy </strong>”，因为epsilon控制着我们想要有多贪婪。ε-贪婪算法是非常常用的，包括在本文开头链接的Atari论文中。</p><p id="954c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们回顾一下我们已经讲过的内容。首先，我们解释了k臂土匪问题和几种可能的解决方案。我们注意到，逼近<strong class="lb iu">动作值函数</strong>是解决问题的重要一步。我们看到了<strong class="lb iu">平衡探索和利用的困难</strong>，这是强化学习问题中的一个常见主题。最后，我们讨论了<strong class="lb iu">贪婪</strong>和<strong class="lb iu">ε贪婪</strong>算法，并讨论了它们如何绑定到探索/开发框架中。强化学习是一个相当广泛的话题，但是我希望这篇文章能够阐明一些基本概念。</p></div></div>    
</body>
</html>