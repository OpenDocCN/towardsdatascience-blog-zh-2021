<html>
<head>
<title>Transfer Learning and Data Augmentation applied to the Simpsons Image Dataset</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">应用于辛普森图像数据集的迁移学习和数据扩充</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/transfer-learning-and-data-augmentation-applied-to-the-simpsons-image-dataset-e292716fbd43?source=collection_archive---------12-----------------------#2021-03-31">https://towardsdatascience.com/transfer-learning-and-data-augmentation-applied-to-the-simpsons-image-dataset-e292716fbd43?source=collection_archive---------12-----------------------#2021-03-31</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="7fc5" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsdatascience.com/tagged/making-sense-of-big-data" rel="noopener" target="_blank">理解大数据</a></h2><div class=""/><div class=""><h2 id="54ee" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">使用Tensorflow和Keras的深度学习应用</h2></div><h1 id="9627" class="kr ks it bd kt ku kv kw kx ky kz la lb ki lc kj ld kl le km lf ko lg kp lh li bi translated">1.介绍</h1><p id="3eba" class="pw-post-body-paragraph lj lk it ll b lm ln kd lo lp lq kg lr ls lt lu lv lw lx ly lz ma mb mc md me im bi translated">在机器学习(ML)的理想场景中，有大量带标签的训练实例，它们与测试数据<a class="ae mf" href="https://arxiv.org/abs/1911.02685" rel="noopener ugc nofollow" target="_blank">【1】</a>共享相同的分布。但是，在某些情况下，收集这些数据可能需要大量资源，或者不切实际。因此，迁移学习成为一种有用的方法。它包括通过从不同但相关的领域转移信息来增加模型的学习能力。换句话说，它放松了训练和测试数据独立同分布的假设<a class="ae mf" href="https://link.springer.com/chapter/10.1007/978-3-030-01424-7_27" rel="noopener ugc nofollow" target="_blank">【2】</a>。只有当要学习的特征对两个任务都通用时，这种方法才有效。另一种处理有限数据的方法是使用数据扩充(DA)。它包括应用一套变换来膨胀数据集。传统的ML算法非常依赖于特征工程，而深度学习(DL)专注于通过无监督或半监督的特征学习方法和分层特征提取来学习数据。DL通常需要大量的数据来进行有效的训练，这使得它成为TL和DA的强有力的候选对象。</p><p id="4224" class="pw-post-body-paragraph lj lk it ll b lm mg kd lo lp mh kg lr ls mi lu lv lw mj ly lz ma mk mc md me im bi translated">我们的任务是对一系列带标签的图像进行分类。由于数据集较小，我们面临两个问题:有效学习数据中的模式的挑战和过度拟合的高概率。我们从零开始实现一个卷积神经网络(CNN)模型作为基准模型。接下来，遵循TL的原理，我们在ImageNet数据集上使用一个预先训练好的卷积神经网络(<a class="ae mf" href="https://arxiv.org/abs/1610.02357" rel="noopener ugc nofollow" target="_blank">【3】</a>，<a class="ae mf" href="https://keras.io/api/applications/xception/" rel="noopener ugc nofollow" target="_blank">【4】</a>)。我们移除了它的顶层，以包含我们自己的适合我们的问题规范的深层结构。因此，预训练的CNN在整个新模型中充当特征提取层。通过这种方法，我们解决了这两个问题:我们大大减少了对大量训练数据的需求，同时也减少了过度拟合。我们还进行了第二个实验，通过应用一套增强图像大小和质量的技术来增加我们的训练数据。这种方法被定义为数据扩充(DA ),它是一种正则化技术。虽然它保留了标签，但它也使用转换来增加数据集，以添加更多不变的示例<a class="ae mf" href="https://arxiv.org/pdf/1708.06020.pdf" rel="noopener ugc nofollow" target="_blank">【5】</a>。</p><p id="3870" class="pw-post-body-paragraph lj lk it ll b lm mg kd lo lp mh kg lr ls mi lu lv lw mj ly lz ma mk mc md me im bi translated">在本文中，我们使用辛普森一家的角色数据集<a class="ae mf" href="https://www.kaggle.com/alexattia/the-simpsons-characters-dataset" rel="noopener ugc nofollow" target="_blank">【6】</a>。我们对数据集进行了过滤，仅包含包含100张以上图像的类(字符)。在训练、验证和测试数据集之间进行拆分后，数据集的最终大小如下:12411幅图像用于训练，3091幅图像用于验证，950幅图像用于测试。</p><p id="a6da" class="pw-post-body-paragraph lj lk it ll b lm mg kd lo lp mh kg lr ls mi lu lv lw mj ly lz ma mk mc md me im bi translated">和往常一样，代码可以在我的<a class="ae mf" href="https://github.com/luisroque/deep-learning-articles" rel="noopener ugc nofollow" target="_blank"> GitHub </a>上获得。</p><p id="73e3" class="pw-post-body-paragraph lj lk it ll b lm mg kd lo lp mh kg lr ls mi lu lv lw mj ly lz ma mk mc md me im bi translated">本文属于使用TensorFlow进行深度学习的系列文章:</p><ul class=""><li id="bb48" class="ml mm it ll b lm mg lp mh ls mn lw mo ma mp me mq mr ms mt bi translated"><a class="ae mf" rel="noopener" target="_blank" href="/transfer-learning-and-data-augmentation-applied-to-the-simpsons-image-dataset-e292716fbd43">迁移学习和数据增强应用于辛普森图像数据集</a></li><li id="171e" class="ml mm it ll b lm mu lp mv ls mw lw mx ma my me mq mr ms mt bi translated"><a class="ae mf" rel="noopener" target="_blank" href="/generating-text-with-recurrent-neural-networks-based-on-the-work-of-f-pessoa-1e804d88692d">基于F. Pessoa的工作用递归神经网络生成文本</a></li><li id="3a1a" class="ml mm it ll b lm mu lp mv ls mw lw mx ma my me mq mr ms mt bi translated"><a class="ae mf" rel="noopener" target="_blank" href="/neural-machine-translation-using-a-seq2seq-architecture-and-attention-eng-to-por-fe3cc4191175">使用Seq2Seq架构和注意力的神经机器翻译(ENG to POR) </a></li><li id="be73" class="ml mm it ll b lm mu lp mv ls mw lw mx ma my me mq mr ms mt bi translated"><a class="ae mf" rel="noopener" target="_blank" href="/residual-networks-in-computer-vision-ee118d3be68f">残差网络从无到有应用于计算机视觉</a></li></ul><h1 id="49c3" class="kr ks it bd kt ku kv kw kx ky kz la lb ki lc kj ld kl le km lf ko lg kp lh li bi translated">2.数据预处理</h1><p id="b392" class="pw-post-body-paragraph lj lk it ll b lm ln kd lo lp lq kg lr ls lt lu lv lw lx ly lz ma mb mc md me im bi translated">尽管数据集的大小很小，CNN可以有效地学习，但它足够大，我们在加载和转换它时会有内存问题。我们使用数据生成器向不同的模型提供实时数据。生成器函数是一种特殊类型的函数，它返回一个惰性迭代器，也就是说，它们不将内容存储在内存中。在创建生成器时，我们应用一个转换来规范化我们的数据，在训练和验证数据集之间分割它们，并定义一个32的批处理大小。</p><pre class="mz na nb nc gt nd ne nf ng aw nh bi"><span id="610c" class="ni ks it ne b gy nj nk l nl nm">import tensorflow as tf<br/>from tensorflow.keras.models import  Sequential, Model<br/>import numpy as np<br/>import os<br/>import pandas as pd<br/>from sklearn.metrics import confusion_matrix<br/>import matplotlib.pyplot as plt<br/>%matplotlib inline<br/>import seaborn as sns<br/>from tensorflow.keras import Input, layers<br/>from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D<br/>import tensorflow_hub as hub<br/>from tensorflow.keras.layers import Dropout, BatchNormalization<br/>from sklearn.model_selection import train_test_split<br/>from tensorflow.keras.preprocessing.image import ImageDataGenerator</span><span id="2957" class="ni ks it ne b gy nn nk l nl nm">directory_train = "./data/simpsons_data_split/train/"<br/>directory_test = "./data/simpsons_data_split/test/"</span><span id="8a47" class="ni ks it ne b gy nn nk l nl nm">def get_ImageDataGenerator(validation_split=None):<br/>    image_generator = ImageDataGenerator(rescale=(1/255.),<br/>                                         validation_split=validation_split)<br/>    return image_generator</span><span id="8b33" class="ni ks it ne b gy nn nk l nl nm">image_gen_train = get_ImageDataGenerator(validation_split=0.2)</span><span id="6eb2" class="ni ks it ne b gy nn nk l nl nm">def get_generator(image_data_generator, directory, train_valid=None, seed=None):<br/>    train_generator = image_data_generator.flow_from_directory(directory, batch_size=32, class_mode='categorical', target_size=(299,299), subset=train_valid, seed=seed)    <br/>    return train_generator</span><span id="4a65" class="ni ks it ne b gy nn nk l nl nm">train_generator = get_generator(image_gen_train, directory_train, train_valid='training', seed=1)<br/>validation_generator = get_generator(image_gen_train, directory_train, train_valid='validation')</span><span id="7766" class="ni ks it ne b gy nn nk l nl nm">Found 12411 images belonging to 19 classes.<br/>Found 3091 images belonging to 19 classes.</span><span id="63a2" class="ni ks it ne b gy nn nk l nl nm">image_gen_test = get_ImageDataGenerator(validation_split=None)<br/>test_generator = get_generator(image_gen_test, directory_test)</span><span id="1822" class="ni ks it ne b gy nn nk l nl nm">Found 950 images belonging to 19 classes.</span></pre><p id="a0d7" class="pw-post-body-paragraph lj lk it ll b lm mg kd lo lp mh kg lr ls mi lu lv lw mj ly lz ma mk mc md me im bi translated">我们可以遍历生成器来获得一组图像，其大小等于上面定义的批处理大小。</p><pre class="mz na nb nc gt nd ne nf ng aw nh bi"><span id="47a3" class="ni ks it ne b gy nj nk l nl nm">target_labels = next(os.walk(directory_train))[1]<br/><br/>target_labels.sort()<br/><br/>batch = next(train_generator)<br/>batch_images = np.array(batch[0])<br/>batch_labels = np.array(batch[1])<br/><br/>target_labels = np.asarray(target_labels)<br/><br/>plt.figure(figsize=(15,10))<br/>for n, i in enumerate(np.arange(10)):<br/>    ax = plt.subplot(3,5,n+1)<br/>    plt.imshow(batch_images[i])<br/>    plt.title(target_labels[np.where(batch_labels[i]==1)[0][0]])<br/>    plt.axis('off')</span></pre><figure class="mz na nb nc gt no gh gi paragraph-image"><div class="ab gu cl np"><img src="../Images/0cdf46b60b630e06f32d51af4ec660b6.png" data-original-src="https://miro.medium.com/v2/format:webp/1*i0JuBbQ4x05YlnIgh7_93Q.png"/></div><p class="ns nt gj gh gi nu nv bd b be z dk translated">图1:由训练生成器生成的一组图像。</p></figure><h1 id="1b37" class="kr ks it bd kt ku kv kw kx ky kz la lb ki lc kj ld kl le km lf ko lg kp lh li bi translated">3.基准模型</h1><p id="f02f" class="pw-post-body-paragraph lj lk it ll b lm ln kd lo lp lq kg lr ls lt lu lv lw lx ly lz ma mb mc md me im bi translated">我们定义一个简单的CNN作为基准模型。它结合使用2D卷积层(对图像执行空间卷积)和最大池操作。接下来是具有128个单元和ReLU激活功能的密集层和速率为0.5的下降层。最后，最后一层产生我们的网络的输出，其单元数量等于目标标签的数量，并使用softmax激活函数。该模型是用Adam优化器编译的，具有默认设置和分类交叉熵损失。</p><pre class="mz na nb nc gt nd ne nf ng aw nh bi"><span id="f62e" class="ni ks it ne b gy nj nk l nl nm">def get_benchmark_model(input_shape):<br/>    x = Input(shape=input_shape)<br/>    h = Conv2D(32, padding='same', kernel_size=(3,3), activation='relu')(x)<br/>    h = Conv2D(32, padding='same', kernel_size=(3,3), activation='relu')(x)<br/>    h = MaxPooling2D(pool_size=(2,2))(h)<br/>    h = Conv2D(64, padding='same', kernel_size=(3,3), activation='relu')(h)<br/>    h = Conv2D(64, padding='same', kernel_size=(3,3), activation='relu')(h)<br/>    h = MaxPooling2D(pool_size=(2,2))(h)<br/>    h = Conv2D(128, kernel_size=(3,3), activation='relu')(h)<br/>    h = Conv2D(128, kernel_size=(3,3), activation='relu')(h)<br/>    h = MaxPooling2D(pool_size=(2,2))(h)<br/>    h = Flatten()(h)<br/>    h = Dense(128, activation='relu')(h)<br/>    h = Dropout(.5)(h)<br/>    output = Dense(target_labels.shape[0], activation='softmax')(h)<br/><br/>    model = tf.keras.Model(inputs=x, outputs=output)<br/>    <br/>    model.compile(optimizer='adam',<br/>             loss='categorical_crossentropy',<br/>             metrics=['accuracy'])<br/>    return model</span></pre><p id="b5a0" class="pw-post-body-paragraph lj lk it ll b lm mg kd lo lp mh kg lr ls mi lu lv lw mj ly lz ma mk mc md me im bi translated">下面，你可以找到我们的模型的概要，详细列出了定义的层和每层的训练参数数量。</p><pre class="mz na nb nc gt nd ne nf ng aw nh bi"><span id="ec0b" class="ni ks it ne b gy nj nk l nl nm">benchmark_model = get_benchmark_model((299, 299, 3))<br/>benchmark_model.summary()</span><span id="f63c" class="ni ks it ne b gy nn nk l nl nm">Model: "model"<br/>_________________________________________________________________<br/>Layer (type)                 Output Shape              Param #   <br/>=================================================================<br/>input_1 (InputLayer)         [(None, 299, 299, 3)]     0         <br/>_________________________________________________________________<br/>conv2d_1 (Conv2D)            (None, 299, 299, 32)      896       <br/>_________________________________________________________________<br/>max_pooling2d (MaxPooling2D) (None, 149, 149, 32)      0         <br/>_________________________________________________________________<br/>conv2d_2 (Conv2D)            (None, 149, 149, 64)      18496     <br/>_________________________________________________________________<br/>conv2d_3 (Conv2D)            (None, 149, 149, 64)      36928     <br/>_________________________________________________________________<br/>max_pooling2d_1 (MaxPooling2 (None, 74, 74, 64)        0         <br/>_________________________________________________________________<br/>conv2d_4 (Conv2D)            (None, 72, 72, 128)       73856     <br/>_________________________________________________________________<br/>conv2d_5 (Conv2D)            (None, 70, 70, 128)       147584    <br/>_________________________________________________________________<br/>max_pooling2d_2 (MaxPooling2 (None, 35, 35, 128)       0         <br/>_________________________________________________________________<br/>flatten (Flatten)            (None, 156800)            0         <br/>_________________________________________________________________<br/>dense (Dense)                (None, 128)               20070528  <br/>_________________________________________________________________<br/>dropout (Dropout)            (None, 128)               0         <br/>_________________________________________________________________<br/>dense_1 (Dense)              (None, 19)                2451      <br/>=================================================================<br/>Total params: 20,350,739<br/>Trainable params: 20,350,739<br/>Non-trainable params: 0<br/>_________________________________________________________________</span></pre><p id="2e64" class="pw-post-body-paragraph lj lk it ll b lm mg kd lo lp mh kg lr ls mi lu lv lw mj ly lz ma mk mc md me im bi translated">我们使用回调来训练基准CNN模型，以在验证准确性在10个时期内没有提高的情况下更早地停止训练过程。这个数字应该更小，但它用于显示模型很快开始过度拟合数据。</p><pre class="mz na nb nc gt nd ne nf ng aw nh bi"><span id="70c0" class="ni ks it ne b gy nj nk l nl nm">def train_model(model, train_gen, valid_gen, epochs):<br/>    train_steps_per_epoch = train_gen.n // train_gen.batch_size<br/>    val_steps = valid_gen.n // valid_gen.batch_size<br/>    <br/>    earlystopping = tf.keras.callbacks.EarlyStopping(patience=10)<br/>    history = model.fit(train_gen, <br/>                        steps_per_epoch = train_steps_per_epoch,<br/>                        epochs=epochs,<br/>                        validation_data=valid_gen, <br/>                        callbacks=[earlystopping])<br/>    <br/>    return history</span></pre><p id="d752" class="pw-post-body-paragraph lj lk it ll b lm mg kd lo lp mh kg lr ls mi lu lv lw mj ly lz ma mk mc md me im bi translated">每次我们使用一个生成器，都需要在馈给一个模型之前进行重置；否则，我们将丢失大量数据。</p><pre class="mz na nb nc gt nd ne nf ng aw nh bi"><span id="84f6" class="ni ks it ne b gy nj nk l nl nm">train_generator = get_generator(image_gen_train, directory_train, train_valid='training')<br/>validation_generator = get_generator(image_gen_train, directory_train, train_valid='validation')</span><span id="0a03" class="ni ks it ne b gy nn nk l nl nm">Found 12411 images belonging to 19 classes.<br/>Found 3091 images belonging to 19 classes.</span><span id="538d" class="ni ks it ne b gy nn nk l nl nm">history_benchmark = train_model(benchmark_model, train_generator, validation_generator, 50)</span><span id="12fd" class="ni ks it ne b gy nn nk l nl nm">Epoch 1/50<br/>387/387 [==============================] - 747s 2s/step - loss: 2.8358 - accuracy: 0.1274 - val_loss: 2.4024 - val_accuracy: 0.2436<br/>Epoch 2/50<br/>387/387 [==============================] - 728s 2s/step - loss: 2.3316 - accuracy: 0.2758 - val_loss: 1.9895 - val_accuracy: 0.4170<br/>[...]<br/>Epoch 14/50<br/>387/387 [==============================] - 719s 2s/step - loss: 0.3846 - accuracy: 0.8612 - val_loss: 2.4831 - val_accuracy: 0.5962<br/>Epoch 15/50<br/>387/387 [==============================] - 719s 2s/step - loss: 0.3290 - accuracy: 0.8806 - val_loss: 2.5545 - val_accuracy: 0.5930</span></pre><p id="aef3" class="pw-post-body-paragraph lj lk it ll b lm mg kd lo lp mh kg lr ls mi lu lv lw mj ly lz ma mk mc md me im bi translated">下图显示了训练和验证数据集的准确性和损失随时间(时期)的演变。显然，我们的模型过度拟合了数据，因为训练的准确性接近90%，并且验证数据集的损失实际上在过去的时期中不断增加。这也是训练中纪元数量减少的原因。</p><pre class="mz na nb nc gt nd ne nf ng aw nh bi"><span id="c55d" class="ni ks it ne b gy nj nk l nl nm">plt.figure(figsize=(15,5))<br/>plt.subplot(121)<br/>plt.plot(history_benchmark.history['accuracy'])<br/>plt.plot(history_benchmark.history['val_accuracy'])<br/>plt.title('Accuracy vs. epochs')<br/>plt.ylabel('Accuracy')<br/>plt.xlabel('Epoch')<br/>plt.legend(['Training', 'Validation'], loc='lower right')<br/><br/>plt.subplot(122)<br/>plt.plot(history_benchmark.history['loss'])<br/>plt.plot(history_benchmark.history['val_loss'])<br/>plt.title('Loss vs. epochs')<br/>plt.ylabel('Loss')<br/>plt.xlabel('Epoch')<br/>plt.legend(['Training', 'Validation'], loc='upper right')<br/>plt.show()</span></pre><figure class="mz na nb nc gt no gh gi paragraph-image"><div class="ab gu cl np"><img src="../Images/a7cca4fb75b2840c0a04d713dd189337.png" data-original-src="https://miro.medium.com/v2/format:webp/1*PmQDAnRE-0uuL9eYiM9DuQ.png"/></div><p class="ns nt gj gh gi nu nv bd b be z dk translated">图2:基准模型在几个时期内的准确性和损失演变。</p></figure><p id="6260" class="pw-post-body-paragraph lj lk it ll b lm mg kd lo lp mh kg lr ls mi lu lv lw mj ly lz ma mk mc md me im bi translated">我们可以通过使基准模型适合我们的测试数据集来评估它。结果如下所示。</p><pre class="mz na nb nc gt nd ne nf ng aw nh bi"><span id="de88" class="ni ks it ne b gy nj nk l nl nm">test_steps = test_generator.n // test_generator.batch_size<br/>benchmark_test_loss, benchmark_test_acc = benchmark_model.evaluate(test_generator, steps=test_steps)<br/>print('\nTest dataset:')<br/>print("Loss: {}".format(benchmark_test_loss))<br/>print("Accuracy: {}".format(benchmark_test_acc))</span><span id="f51b" class="ni ks it ne b gy nn nk l nl nm">29/29 [==============================] - 9s 304ms/step - loss: 2.5011 - accuracy: 0.6272<br/><br/>Test dataset:<br/>Loss: 2.5011332035064697<br/>Accuracy: 0.6271551847457886</span></pre><h1 id="23de" class="kr ks it bd kt ku kv kw kx ky kz la lb ki lc kj ld kl le km lf ko lg kp lh li bi translated">4.预先训练的CNN</h1><p id="4431" class="pw-post-body-paragraph lj lk it ll b lm ln kd lo lp lq kg lr ls lt lu lv lw lx ly lz ma mb mc md me im bi translated">对于预训练的模型，我们使用Xception架构(<a class="ae mf" href="https://arxiv.org/abs/1610.02357" rel="noopener ugc nofollow" target="_blank">【3】</a>，<a class="ae mf" href="https://keras.io/api/applications/xception/" rel="noopener ugc nofollow" target="_blank">【4】</a>)，在<code class="fe nw nx ny ne b">keras.applications</code>模块中实现的深度CNN。我们已经加载了预训练的参数(在ImageNet数据集上学习的)。我们使用预训练的CNN作为大的特征提取层，我们用针对我们的多类分类任务的一组未训练的层来扩展它。这就是翻译原则被有效应用的地方。</p><pre class="mz na nb nc gt nd ne nf ng aw nh bi"><span id="3dfd" class="ni ks it ne b gy nj nk l nl nm">feature_extractor = tf.keras.applications.Xception(weights="imagenet")</span><span id="d7b0" class="ni ks it ne b gy nn nk l nl nm">feature_extractor.summary()</span><span id="d9cd" class="ni ks it ne b gy nn nk l nl nm">Model: "xception"<br/>__________________________________________________________________________________________________<br/>Layer (type)                    Output Shape         Param #     Connected to                     <br/>==================================================================================================<br/>input_2 (InputLayer)            [(None, 299, 299, 3) 0                                            <br/>__________________________________________________________________________________________________<br/>[...]     <br/>avg_pool (GlobalAveragePooling2 (None, 2048)         0           block14_sepconv2_act[0][0]       <br/>__________________________________________________________________________________________________<br/>predictions (Dense)             (None, 1000)         2049000     avg_pool[0][0]                   <br/>==================================================================================================<br/>Total params: 22,910,480<br/>Trainable params: 22,855,952<br/>Non-trainable params: 54,528<br/>_________________________________________________________________________________________________</span><span id="bd63" class="ni ks it ne b gy nn nk l nl nm">def remove_head(feature_extractor_model):<br/>    model_input = feature_extractor_model.input<br/>    output = feature_extractor_model.get_layer(name='avg_pool').output<br/>    model = tf.keras.Model(inputs=model_input, outputs=output)<br/>    return model</span><span id="5733" class="ni ks it ne b gy nn nk l nl nm">feature_extractor = remove_head(feature_extractor)<br/>feature_extractor.summary()</span><span id="0b7a" class="ni ks it ne b gy nn nk l nl nm">Model: "model_1"<br/>__________________________________________________________________________________________________<br/>Layer (type)                    Output Shape         Param #     Connected to                     <br/>==================================================================================================<br/>input_2 (InputLayer)            [(None, 299, 299, 3) 0                                            <br/>__________________________________________________________________________________________________<br/>[...]    <br/>avg_pool (GlobalAveragePooling2 (None, 2048)         0           block14_sepconv2_act[0][0]       <br/>==================================================================================================<br/>Total params: 20,861,480<br/>Trainable params: 20,806,952<br/>Non-trainable params: 54,528<br/>__________________________________________________________________________________________________</span><span id="3db6" class="ni ks it ne b gy nn nk l nl nm">def add_new_classifier_head(feature_extractor_model):<br/>    model = Sequential([<br/>        feature_extractor_model,<br/>        Dense(128, activation='relu'),<br/>        Dropout(.5),<br/>        Dense(target_labels.shape[0], activation='softmax')<br/>    ])<br/>    <br/>    return model</span></pre><p id="beb0" class="pw-post-body-paragraph lj lk it ll b lm mg kd lo lp mh kg lr ls mi lu lv lw mj ly lz ma mk mc md me im bi translated">下面，我们可以看到添加到我们的模型头部的层。</p><pre class="mz na nb nc gt nd ne nf ng aw nh bi"><span id="b523" class="ni ks it ne b gy nj nk l nl nm">new_model = add_new_classifier_head(feature_extractor)<br/>new_model.summary()</span><span id="438f" class="ni ks it ne b gy nn nk l nl nm">Model: "sequential"<br/>_________________________________________________________________<br/>Layer (type)                 Output Shape              Param #   <br/>=================================================================<br/>model_1 (Functional)         (None, 2048)              20861480  <br/>_________________________________________________________________<br/>dense_2 (Dense)              (None, 128)               262272    <br/>_________________________________________________________________<br/>dropout_1 (Dropout)          (None, 128)               0         <br/>_________________________________________________________________<br/>dense_3 (Dense)              (None, 19)                2451      <br/>=================================================================<br/>Total params: 21,126,203<br/>Trainable params: 21,071,675<br/>Non-trainable params: 54,528<br/>_________________________________________________________________</span><span id="2655" class="ni ks it ne b gy nn nk l nl nm">def freeze_pretrained_weights(model):<br/>    model.get_layer(name='model_1').trainable=False<br/>    <br/>    model.compile(optimizer='adam',<br/>                 loss='categorical_crossentropy',<br/>                 metrics=['accuracy'])<br/>    return model</span></pre><p id="a218" class="pw-post-body-paragraph lj lk it ll b lm mg kd lo lp mh kg lr ls mi lu lv lw mj ly lz ma mk mc md me im bi translated">我们将预先训练的CNN参数冻结为不可训练的——我们可以看到在我们的新模型中有超过2000万个不可训练的参数。与基准模型相比，这也导致每个时期的训练时间更短。</p><pre class="mz na nb nc gt nd ne nf ng aw nh bi"><span id="aafe" class="ni ks it ne b gy nj nk l nl nm">frozen_new_model = freeze_pretrained_weights(new_model)<br/>frozen_new_model.summary()</span><span id="2127" class="ni ks it ne b gy nn nk l nl nm">Model: "sequential"<br/>_________________________________________________________________<br/>Layer (type)                 Output Shape              Param #   <br/>=================================================================<br/>model_1 (Functional)         (None, 2048)              20861480  <br/>_________________________________________________________________<br/>dense_2 (Dense)              (None, 128)               262272    <br/>_________________________________________________________________<br/>dropout_1 (Dropout)          (None, 128)               0         <br/>_________________________________________________________________<br/>dense_3 (Dense)              (None, 19)                2451      <br/>=================================================================<br/>Total params: 21,126,203<br/>Trainable params: 264,723<br/>Non-trainable params: 20,861,480<br/>_________________________________________________________________</span><span id="0179" class="ni ks it ne b gy nn nk l nl nm">def train_model(model, train_gen, valid_gen, epochs):<br/>    train_steps_per_epoch = train_gen.n // train_gen.batch_size<br/>    val_steps = valid_gen.n // valid_gen.batch_size<br/>    <br/>    history = model.fit(train_gen, <br/>                        steps_per_epoch = train_steps_per_epoch,<br/>                        epochs=epochs,<br/>                        validation_data=valid_gen)<br/>    <br/>    return history</span><span id="8950" class="ni ks it ne b gy nn nk l nl nm">history_frozen_new_model = train_model(frozen_new_model, train_generator, validation_generator, 50)</span><span id="d2b6" class="ni ks it ne b gy nn nk l nl nm">Epoch 1/50<br/>387/387 [==============================] - 564s 1s/step - loss: 2.6074 - accuracy: 0.1943 - val_loss: 2.0344 - val_accuracy: 0.4232<br/>Epoch 2/50<br/>387/387 [==============================] - 561s 1s/step - loss: 2.0173 - accuracy: 0.3909 - val_loss: 1.7743 - val_accuracy: 0.5118<br/>[...]<br/>Epoch 49/50<br/>387/387 [==============================] - 547s 1s/step - loss: 0.4772 - accuracy: 0.8368 - val_loss: 1.7771 - val_accuracy: 0.6137<br/>Epoch 50/50<br/>387/387 [==============================] - 547s 1s/step - loss: 0.4748 - accuracy: 0.8342 - val_loss: 1.7402 - val_accuracy: 0.6215</span></pre><p id="95f9" class="pw-post-body-paragraph lj lk it ll b lm mg kd lo lp mh kg lr ls mi lu lv lw mj ly lz ma mk mc md me im bi translated">我们在没有使用回调的情况下运行了更长时间的模型。我们没有看到同样的过度拟合模式，因为超过90%的砝码被冻结。事实上，我们可以运行少量的时期，因为在10个时期后我们看不到真正的改进。</p><pre class="mz na nb nc gt nd ne nf ng aw nh bi"><span id="a529" class="ni ks it ne b gy nj nk l nl nm">plt.figure(figsize=(15,5))<br/>plt.subplot(121)<br/>plt.plot(history_frozen_new_model.history['accuracy'])<br/>plt.plot(history_frozen_new_model.history['val_accuracy'])<br/>plt.title('Accuracy vs. epochs')<br/>plt.ylabel('Accuracy')<br/>plt.xlabel('Epoch')<br/>plt.legend(['Training', 'Validation'], loc='lower right')<br/><br/>plt.subplot(122)<br/>plt.plot(history_frozen_new_model.history['loss'])<br/>plt.plot(history_frozen_new_model.history['val_loss'])<br/>plt.title('Loss vs. epochs')<br/>plt.ylabel('Loss')<br/>plt.xlabel('Epoch')<br/>plt.legend(['Training', 'Validation'], loc='upper right')<br/>plt.show()</span></pre><figure class="mz na nb nc gt no gh gi paragraph-image"><div class="ab gu cl np"><img src="../Images/afa3450a088c1abb3b558d256ca616b4.png" data-original-src="https://miro.medium.com/v2/format:webp/1*f10gnoeZKrRbD_O76Ztz9A.png"/></div><p class="ns nt gj gh gi nu nv bd b be z dk translated">图3:TL模型在几个时期内的准确性和损失演变。</p></figure><p id="808e" class="pw-post-body-paragraph lj lk it ll b lm mg kd lo lp mh kg lr ls mi lu lv lw mj ly lz ma mk mc md me im bi translated">将我们的新模型与测试数据集进行拟合，结果是精度略有提高(略高于1%)。</p><pre class="mz na nb nc gt nd ne nf ng aw nh bi"><span id="5de8" class="ni ks it ne b gy nj nk l nl nm">test_generator = get_generator(image_gen_test, directory_test)<br/>new_model_test_loss, new_model_test_acc = frozen_new_model.evaluate(test_generator, steps=test_steps)<br/>print('\nTest dataset')<br/>print("Loss: {}".format(new_model_test_loss))<br/>print("Accuracy: {}".format(new_model_test_acc))</span><span id="f61f" class="ni ks it ne b gy nn nk l nl nm">Found 950 images belonging to 19 classes.<br/>29/29 [==============================] - 33s 1s/step - loss: 1.6086 - accuracy: 0.6390<br/><br/>Test dataset<br/>Loss: 1.6085671186447144<br/>Accuracy: 0.639008641242981</span></pre><h1 id="b99a" class="kr ks it bd kt ku kv kw kx ky kz la lb ki lc kj ld kl le km lf ko lg kp lh li bi translated">5.数据扩充</h1><p id="551a" class="pw-post-body-paragraph lj lk it ll b lm ln kd lo lp lq kg lr ls lt lu lv lw lx ly lz ma mb mc md me im bi translated">正如我们在上面看到的，DA是一组用于膨胀数据集同时减少过度拟合的方法。我们关注由几何和光度变换组成的通用DA(参见<a class="ae mf" href="https://arxiv.org/pdf/1708.06020.pdf" rel="noopener ugc nofollow" target="_blank">【5】</a>了解更多关于这些和其他方法的信息)。几何变换改变了图像的几何形状，使得CNN不会因位置和方向的改变而改变。另一方面，光度变换通过调整图像的颜色通道，使CNN不受颜色和光照变化的影响。</p><pre class="mz na nb nc gt nd ne nf ng aw nh bi"><span id="6773" class="ni ks it ne b gy nj nk l nl nm">def get_ImageDataGenerator_augmented(validation_split=None):<br/>    image_generator = ImageDataGenerator(rescale=(1/255.),<br/>                                        rotation_range=40,<br/>                                        width_shift_range=0.2,<br/>                                        height_shift_range=0.2,<br/>                                        shear_range=0.2,<br/>                                        zoom_range=0.1,<br/>                                        brightness_range=[0.8,1.2],<br/>                                        horizontal_flip=True,<br/>                                        validation_split=validation_split)<br/>    return image_generator</span><span id="ecd4" class="ni ks it ne b gy nn nk l nl nm">image_gen_train_aug = get_ImageDataGenerator_augmented(validation_split=0.2)</span><span id="d0ba" class="ni ks it ne b gy nn nk l nl nm">train_generator_aug = get_generator(image_gen_train_aug, directory_train, train_valid='training', seed=1)<br/>validation_generator_aug = get_generator(image_gen_train_aug, directory_train, train_valid='validation')</span><span id="bad4" class="ni ks it ne b gy nn nk l nl nm">Found 12411 images belonging to 19 classes.<br/>Found 3091 images belonging to 19 classes.</span><span id="29ed" class="ni ks it ne b gy nn nk l nl nm">train_generator = get_generator(image_gen_train, directory_train, train_valid='training', seed=1)</span><span id="3a8b" class="ni ks it ne b gy nn nk l nl nm">Found 12411 images belonging to 19 classes.</span></pre><p id="2986" class="pw-post-body-paragraph lj lk it ll b lm mg kd lo lp mh kg lr ls mi lu lv lw mj ly lz ma mk mc md me im bi translated">我们可以显示原始图像和增强图像进行比较。请注意图像的几何变化，如翻转、垂直和水平方向的平移或缩放，以及光度测定，在一些图像的亮度变化中可见。</p><pre class="mz na nb nc gt nd ne nf ng aw nh bi"><span id="0dfe" class="ni ks it ne b gy nj nk l nl nm">batch = next(train_generator)<br/>batch_images = np.array(batch[0])<br/>batch_labels = np.array(batch[1])<br/><br/>aug_batch = next(train_generator_aug)<br/>aug_batch_images = np.array(aug_batch[0])<br/>aug_batch_labels = np.array(aug_batch[1])<br/><br/>plt.figure(figsize=(16,5))<br/>plt.suptitle("original images", fontsize=16)<br/>for n, i in enumerate(np.arange(10)):<br/>    ax = plt.subplot(2, 5, n+1)<br/>    plt.imshow(batch_images[i])<br/>    plt.title(target_labels[np.where(batch_labels[i]==1)[0][0]])<br/>    plt.axis('off')<br/>plt.figure(figsize=(16,5))<br/>plt.suptitle("Augmented images", fontsize=16)<br/>for n, i in enumerate(np.arange(10)):<br/>    ax = plt.subplot(2, 5, n+1)<br/>    plt.imshow(aug_batch_images[i])<br/>    plt.title(target_labels[np.where(batch_labels[i]==1)[0][0]])<br/>    plt.axis('off')</span></pre><figure class="mz na nb nc gt no gh gi paragraph-image"><div class="ab gu cl np"><img src="../Images/ef0367823de27a3f7e29806ed2ba8838.png" data-original-src="https://miro.medium.com/v2/format:webp/1*cQN-13sbz3GjVU4-XUCDUg.png"/></div></figure><figure class="mz na nb nc gt no gh gi paragraph-image"><div class="ab gu cl np"><img src="../Images/bb1bbb72dafd60279145fc311c0b967c.png" data-original-src="https://miro.medium.com/v2/format:webp/1*WIWjUbXh06cIAAi0yh-avA.png"/></div><p class="ns nt gj gh gi nu nv bd b be z dk translated">图4:一组没有任何变换的图像和相应的增强图像之间的比较。</p></figure><pre class="mz na nb nc gt nd ne nf ng aw nh bi"><span id="2c3a" class="ni ks it ne b gy nj nk l nl nm">train_generator_aug = get_generator(image_gen_train_aug, directory_train, train_valid='training')</span><span id="937b" class="ni ks it ne b gy nn nk l nl nm">Found 12411 images belonging to 19 classes.</span></pre><p id="c8a5" class="pw-post-body-paragraph lj lk it ll b lm mg kd lo lp mh kg lr ls mi lu lv lw mj ly lz ma mk mc md me im bi translated">扩充的数据集现在被提供给我们上面定义的定制模型(不使用预先训练的权重)。</p><pre class="mz na nb nc gt nd ne nf ng aw nh bi"><span id="0692" class="ni ks it ne b gy nj nk l nl nm">benchmark_model_aug = benchmark_model<br/>benchmark_model_aug.summary()</span><span id="0722" class="ni ks it ne b gy nn nk l nl nm">Model: "model"<br/>_________________________________________________________________<br/>Layer (type)                 Output Shape              Param #   <br/>=================================================================<br/>input_1 (InputLayer)         [(None, 299, 299, 3)]     0         <br/>_________________________________________________________________<br/>conv2d_1 (Conv2D)            (None, 299, 299, 32)      896       <br/>_________________________________________________________________<br/>max_pooling2d (MaxPooling2D) (None, 149, 149, 32)      0         <br/>_________________________________________________________________<br/>conv2d_2 (Conv2D)            (None, 149, 149, 64)      18496     <br/>_________________________________________________________________<br/>conv2d_3 (Conv2D)            (None, 149, 149, 64)      36928     <br/>_________________________________________________________________<br/>max_pooling2d_1 (MaxPooling2 (None, 74, 74, 64)        0         <br/>_________________________________________________________________<br/>conv2d_4 (Conv2D)            (None, 72, 72, 128)       73856     <br/>_________________________________________________________________<br/>conv2d_5 (Conv2D)            (None, 70, 70, 128)       147584    <br/>_________________________________________________________________<br/>max_pooling2d_2 (MaxPooling2 (None, 35, 35, 128)       0         <br/>_________________________________________________________________<br/>flatten (Flatten)            (None, 156800)            0         <br/>_________________________________________________________________<br/>dense (Dense)                (None, 128)               20070528  <br/>_________________________________________________________________<br/>dropout (Dropout)            (None, 128)               0         <br/>_________________________________________________________________<br/>dense_1 (Dense)              (None, 19)                2451      <br/>=================================================================<br/>Total params: 20,350,739<br/>Trainable params: 20,350,739<br/>Non-trainable params: 0<br/>_________________________________________________________________</span><span id="983b" class="ni ks it ne b gy nn nk l nl nm">history_augmented = train_model(benchmark_model_aug, train_generator_aug, validation_generator_aug, epochs=150)</span><span id="e709" class="ni ks it ne b gy nn nk l nl nm">Epoch 1/150<br/>387/387 [==============================] - 748s 2s/step - loss: 2.1520 - accuracy: 0.3649 - val_loss: 1.8956 - val_accuracy: 0.4426<br/>Epoch 2/150<br/>387/387 [==============================] - 749s 2s/step - loss: 1.8233 - accuracy: 0.4599 - val_loss: 1.6556 - val_accuracy: 0.5273<br/>[...]<br/>Epoch 149/150<br/>387/387 [==============================] - 753s 2s/step - loss: 0.2859 - accuracy: 0.9270 - val_loss: 0.6202 - val_accuracy: 0.8609<br/>Epoch 150/150<br/>387/387 [==============================] - 753s 2s/step - loss: 0.2830 - accuracy: 0.9259 - val_loss: 0.6289 - val_accuracy: 0.8622</span></pre><p id="0234" class="pw-post-body-paragraph lj lk it ll b lm mg kd lo lp mh kg lr ls mi lu lv lw mj ly lz ma mk mc md me im bi translated">过度拟合显然不再是一个问题。训练可以运行更长的时期，因为它显示了度量的一致改进。不过，我们可以在大约第70个纪元时停止学习过程，但是我们扩展了这个过程，以表明DA实际上可以减少过度拟合的可能性。此外，请注意，我们增加了模型识别属于每个字符的数据中的特征的能力，因为验证集的准确性显著提高(超过86%)。</p><pre class="mz na nb nc gt nd ne nf ng aw nh bi"><span id="1a7a" class="ni ks it ne b gy nj nk l nl nm">plt.figure(figsize=(15,5))<br/>plt.subplot(121)<br/>plt.plot(history_augmented.history['accuracy'])<br/>plt.plot(history_augmented.history['val_accuracy'])<br/>plt.title('Accuracy vs. epochs')<br/>plt.ylabel('Accuracy')<br/>plt.xlabel('Epoch')<br/>plt.legend(['Training', 'Validation'], loc='lower right')<br/><br/>plt.subplot(122)<br/>plt.plot(history_augmented.history['loss'])<br/>plt.plot(history_augmented.history['val_loss'])<br/>plt.title('Loss vs. epochs')<br/>plt.ylabel('Loss')<br/>plt.xlabel('Epoch')<br/>plt.legend(['Training', 'Validation'], loc='upper right')<br/>plt.show()</span></pre><figure class="mz na nb nc gt no gh gi paragraph-image"><div class="ab gu cl np"><img src="../Images/c22d7a09e8450a27d4df0d468f452a89.png" data-original-src="https://miro.medium.com/v2/format:webp/1*rHptMhZ-cCbYg_vXdLEUrg.png"/></div><p class="ns nt gj gh gi nu nv bd b be z dk translated">图5:使用DA的定制模型在几个时期内的精度和损耗变化。</p></figure><p id="1e38" class="pw-post-body-paragraph lj lk it ll b lm mg kd lo lp mh kg lr ls mi lu lv lw mj ly lz ma mk mc md me im bi translated">在我们的测试集上用增强的数据拟合我们的定制模型导致了超过91%的显著增加的准确性。</p><pre class="mz na nb nc gt nd ne nf ng aw nh bi"><span id="ee3d" class="ni ks it ne b gy nj nk l nl nm">test_generator = get_generator(image_gen_test, directory_test)<br/>augmented_model_test_loss, augmented_model_test_acc = benchmark_model_aug.evaluate(test_generator, steps=test_steps)<br/>print('\nTest dataset')<br/>print("Loss: {}".format(augmented_model_test_loss))<br/>print("Accuracy: {}".format(augmented_model_test_acc))</span><span id="6102" class="ni ks it ne b gy nn nk l nl nm">Found 950 images belonging to 19 classes.<br/>29/29 [==============================] - 9s 307ms/step - loss: 0.4446 - accuracy: 0.9106<br/><br/>Test dataset<br/>Loss: 0.44464701414108276<br/>Accuracy: 0.9105603694915771</span></pre><h1 id="953c" class="kr ks it bd kt ku kv kw kx ky kz la lb ki lc kj ld kl le km lf ko lg kp lh li bi translated">6.结果</h1><p id="9128" class="pw-post-body-paragraph lj lk it ll b lm ln kd lo lp lq kg lr ls lt lu lv lw lx ly lz ma mb mc md me im bi translated">最后，我们可以比较基准模型、使用TL原则定义的预训练模型和带有扩充数据的定制模型之间的训练、验证和测试指标。结果表明，TL方法仅略微优于基准模型。这可能是由于模型最初被训练的数据(域)的性质以及它如何转移到Simpsons字符域。另一方面，使用扩充数据的方法能够更有效地捕捉数据中的模式，将测试集中的准确率提高到91%以上。</p><pre class="mz na nb nc gt nd ne nf ng aw nh bi"><span id="5220" class="ni ks it ne b gy nj nk l nl nm">benchmark_train_loss = history_benchmark.history['loss'][-1]<br/>benchmark_valid_loss = history_benchmark.history['val_loss'][-1]<br/>benchmark_train_acc = history_benchmark.history['accuracy'][-1]<br/>benchmark_valid_acc = history_benchmark.history['val_accuracy'][-1]<br/><br/>new_model_train_loss = history_frozen_new_model.history['loss'][-1]<br/>new_model_valid_loss = history_frozen_new_model.history['val_loss'][-1]<br/>new_model_train_acc = history_frozen_new_model.history['accuracy'][-1]<br/>new_model_valid_acc = history_frozen_new_model.history['val_accuracy'][-1]<br/><br/>augmented_model_train_loss = history_augmented.history['loss'][-1]<br/>augmented_model_valid_loss = history_augmented.history['val_loss'][-1]<br/>augmented_model_train_acc = history_augmented.history['accuracy'][-1]<br/>augmented_model_valid_acc = history_augmented.history['val_accuracy'][-1]</span><span id="13b1" class="ni ks it ne b gy nn nk l nl nm">comparison = pd.DataFrame([['Training loss', benchmark_train_loss, new_model_train_loss, augmented_model_train_loss],<br/>                          ['Training accuracy', benchmark_train_acc, new_model_train_acc, augmented_model_train_acc],<br/>                          ['Validation loss', benchmark_valid_loss, new_model_valid_loss, augmented_model_valid_loss],<br/>                          ['Validation accuracy', benchmark_valid_acc, new_model_valid_acc, augmented_model_valid_acc],<br/>                          ['Test loss', benchmark_test_loss, new_model_test_loss, augmented_model_test_loss],<br/>                          ['Test accuracy', benchmark_test_acc, new_model_test_acc, augmented_model_test_acc]],<br/>                           columns=['Metric', 'Benchmark CNN', 'Transfer Learning CNN', 'Custom CNN w/ Data Augmentation'])<br/>comparison.index=['']*6<br/>comparison</span></pre><figure class="mz na nb nc gt no gh gi paragraph-image"><div class="ab gu cl np"><img src="../Images/43203a71a7967de6335b5e11fc0754da.png" data-original-src="https://miro.medium.com/v2/format:webp/1*DNMK80hr5RclbUs5wOUirw.png"/></div><p class="ns nt gj gh gi nu nv bd b be z dk translated">表1:测试的3个模型的比较结果。带有DA的定制CNN产生了最好的结果——测试准确率超过91%。</p></figure><p id="3ac3" class="pw-post-body-paragraph lj lk it ll b lm mg kd lo lp mh kg lr ls mi lu lv lw mj ly lz ma mk mc md me im bi translated">为了说明使用DA的自定义CNN输出，我们绘制了测试集中随机图像预测的分类分布。</p><pre class="mz na nb nc gt nd ne nf ng aw nh bi"><span id="fc58" class="ni ks it ne b gy nj nk l nl nm">test_generator = get_generator(image_gen_test, directory_test, seed=123)<br/>predictions = benchmark_model_aug.predict(test_generator)</span><span id="9bae" class="ni ks it ne b gy nn nk l nl nm">Found 950 images belonging to 19 classes.</span><span id="1750" class="ni ks it ne b gy nn nk l nl nm">test_generator = get_generator(image_gen_test, directory_test, seed=123)<br/>batches = []<br/>for i in range(1):<br/>    batches.append(next(test_generator))<br/><br/>batch_images = np.vstack([b[0] for b in batches])<br/>batch_labels = np.concatenate([b[1].astype(np.int32) for b in batches])</span><span id="a200" class="ni ks it ne b gy nn nk l nl nm">Found 950 images belonging to 19 classes.</span><span id="bdbc" class="ni ks it ne b gy nn nk l nl nm">fig, axes = plt.subplots(3, 2, figsize=(16, 17))<br/>fig.subplots_adjust(hspace = 0.4, wspace=0.8)<br/>axes = axes.ravel()<br/><br/>for i in range(3):<br/><br/>    inx = np.random.choice(batch_images.shape[0], 1, replace=False)[0]<br/><br/>    axes[0+i*2].imshow(batch_images[inx])<br/>    axes[0+i*2].get_xaxis().set_visible(False)<br/>    axes[0+i*2].get_yaxis().set_visible(False)<br/>    axes[0+i*2].text(60., -8, target_labels[np.where(batch_labels[inx]==1)[0][0]], <br/>                    horizontalalignment='center')<br/>    axes[1+i*2].barh(np.arange(len(predictions[inx])),predictions[inx])<br/>    axes[1+i*2].set_yticks(np.arange(len(predictions[inx])))<br/>    axes[1+i*2].set_yticklabels(target_labels)<br/>    axes[1+i*2].set_title(f"Categorical distribution. Model prediction: {target_labels[np.argmax(predictions[inx])]}")<br/>    <br/>plt.show()</span></pre><figure class="mz na nb nc gt no gh gi paragraph-image"><div role="button" tabindex="0" class="oa ob di oc bf od"><div class="gh gi nz"><img src="../Images/e47c27de376d42997ada65ec650a6224.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qLFqC8ohJa94ACABdnf56g.png"/></div></div><p class="ns nt gj gh gi nu nv bd b be z dk translated">图6:随机图像(在左边)和由定制CNN产生的预测的相应分类分布，带有DA ( <em class="oe">在右边</em>)</p></figure><h1 id="ae0f" class="kr ks it bd kt ku kv kw kx ky kz la lb ki lc kj ld kl le km lf ko lg kp lh li bi translated">7.结论</h1><p id="d35b" class="pw-post-body-paragraph lj lk it ll b lm ln kd lo lp lq kg lr ls lt lu lv lw lx ly lz ma mb mc md me im bi translated">我们使用两种不同的方法解决了小数据集和高过拟合可能性的问题。首先，我们加载了一个预先训练好的模型，从中我们删除了顶层，并添加了我们的任务所需的一组特定的层。第二种方法测试了使用DA技术来扩充我们的数据集。我们的结果表明，第二种方法能够克服有限的数据和过拟合问题，产生非常有趣的指标。</p><p id="414c" class="pw-post-body-paragraph lj lk it ll b lm mg kd lo lp mh kg lr ls mi lu lv lw mj ly lz ma mk mc md me im bi translated">可以通过在不同类型的数据集中测试更多的架构或预先训练的网络来进一步研究TL原则。在这种情况下，面部识别任务的预训练模型可能会产生有趣的结果。关于DA方法，它可以通过应用更复杂的变换来扩展。</p><p id="3951" class="pw-post-body-paragraph lj lk it ll b lm mg kd lo lp mh kg lr ls mi lu lv lw mj ly lz ma mk mc md me im bi translated">保持联系:<a class="ae mf" href="https://www.linkedin.com/in/luisbrasroque/" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a></p><h1 id="de35" class="kr ks it bd kt ku kv kw kx ky kz la lb ki lc kj ld kl le km lf ko lg kp lh li bi translated">8.参考</h1><p id="46f3" class="pw-post-body-paragraph lj lk it ll b lm ln kd lo lp lq kg lr ls lt lu lv lw lx ly lz ma mb mc md me im bi translated"><a class="ae mf" href="https://arxiv.org/abs/1911.02685" rel="noopener ugc nofollow" target="_blank">【1】</a>——【庄等，2020】庄，f，齐，z，段，k，，d，朱，y，朱，h，熊，h，何，Q. (2020)。迁移学习综述。</p><p id="bb56" class="pw-post-body-paragraph lj lk it ll b lm mg kd lo lp mh kg lr ls mi lu lv lw mj ly lz ma mk mc md me im bi translated"><a class="ae mf" href="https://link.springer.com/chapter/10.1007/978-3-030-01424-7_27" rel="noopener ugc nofollow" target="_blank">【2】</a>——【谭等，2018】谭，孙，孔，张，杨，刘(2018)。深度迁移学习综述。</p><p id="b81c" class="pw-post-body-paragraph lj lk it ll b lm mg kd lo lp mh kg lr ls mi lu lv lw mj ly lz ma mk mc md me im bi translated"><a class="ae mf" href="https://arxiv.org/abs/1610.02357" rel="noopener ugc nofollow" target="_blank">【3】</a>——【乔莱，2017】乔莱，F. (2017)。例外:具有深度可分卷积的深度学习。</p><p id="a33e" class="pw-post-body-paragraph lj lk it ll b lm mg kd lo lp mh kg lr ls mi lu lv lw mj ly lz ma mk mc md me im bi translated"><a class="ae mf" href="https://keras.io/api/applications/xception/" rel="noopener ugc nofollow" target="_blank">【4】</a>——<a class="ae mf" href="https://keras.io/api/applications/xception/" rel="noopener ugc nofollow" target="_blank">https://keras.io/api/applications/xception/</a></p><p id="5412" class="pw-post-body-paragraph lj lk it ll b lm mg kd lo lp mh kg lr ls mi lu lv lw mj ly lz ma mk mc md me im bi translated"><a class="ae mf" href="https://arxiv.org/pdf/1708.06020.pdf" rel="noopener ugc nofollow" target="_blank">【5】</a>——<a class="ae mf" href="https://arxiv.org/pdf/1708.06020.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1708.06020.pdf</a></p><p id="ff30" class="pw-post-body-paragraph lj lk it ll b lm mg kd lo lp mh kg lr ls mi lu lv lw mj ly lz ma mk mc md me im bi translated"><a class="ae mf" href="https://www.kaggle.com/alexattia/the-simpsons-characters-dataset" rel="noopener ugc nofollow" target="_blank">【6】</a>——<a class="ae mf" href="https://www.kaggle.com/alexattia/the-simpsons-characters-dataset" rel="noopener ugc nofollow" target="_blank">https://www . ka ggle . com/Alex attia/the-Simpsons-characters-dataset</a></p></div></div>    
</body>
</html>