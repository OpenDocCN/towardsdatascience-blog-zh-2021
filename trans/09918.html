<html>
<head>
<title>Learning Disentangled Representations with Invertible(Flow-based) Interpretation Networks</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用可逆的(基于流的)解释网络学习解纠缠的表示</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/learning-disentangled-representations-with-invertible-flow-based-interpretation-networks-9954554a28d2?source=collection_archive---------11-----------------------#2021-09-18">https://towardsdatascience.com/learning-disentangled-representations-with-invertible-flow-based-interpretation-networks-9954554a28d2?source=collection_archive---------11-----------------------#2021-09-18</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="86dc" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">什么是解开的表征？我们如何使用基于流的生成模型来学习任何任意模型的不纠缠的表示？</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/128a6658582dc778e9c3ddf376058f0e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*D9oLFmNzmqJHK4VZ5EOgcw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图1:IIN网络可以应用于任意的现有模型。IIN采用由任意模型学习的表示z，并将其分解成更小的因子，使得每个因子学习表示一个生成概念。图片来源:[1]。</p></figure><p id="4467" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">解开的表示在处理许多下游任务时会很有用，并有助于提高模型的健壮性和通用性。在本帖中，我们将探讨如何从使用基于流的生成模型的任意预训练模型所学习到的表示中学习解开的表示。具体来说，我们将研究埃塞等人在论文《用于解释潜在表征的解开可逆解释网络》中提出的可逆解释网络(IIN)。艾尔。[1].我们将看到IIN背后的想法，它们是如何工作的，它们的用途是什么。我们还将简要了解一下该文件所取得的成果。</p><h1 id="ddd8" class="lr ls iq bd lt lu lv lw lx ly lz ma mb jw mc jx md jz me ka mf kc mg kd mh mi bi translated">解开的表象</h1><p id="51a8" class="pw-post-body-paragraph kv kw iq kx b ky mj jr la lb mk ju ld le ml lg lh li mm lk ll lm mn lo lp lq ij bi translated">一个不纠缠的表征是这样的，其中单个潜在单位的变化对一个生成因素的变化敏感，而对其他因素的变化保持不变[2]。换句话说，给定一个解开的表征，潜在单位的变化将导致一个生成因素的变化，反之亦然。</p><p id="0d30" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">解开的表征在多种任务中是有用的，例如新样本生成、新颖性检测、学习压缩的表征等。良好的表示有助于模型的健壮性和通用性。除此之外，在需要知识转移的任务中，在学习的表示可以帮助模型快速收敛的情况下，如转移学习或领域适应，解开的表示也有帮助。</p><h1 id="8744" class="lr ls iq bd lt lu lv lw lx ly lz ma mb jw mc jx md jz me ka mf kc mg kd mh mi bi translated">基于流程的生成模型</h1><p id="8341" class="pw-post-body-paragraph kv kw iq kx b ky mj jr la lb mk ju ld le ml lg lh li mm lk ll lm mn lo lp lq ij bi translated">当我们观察VAEs和GAN这样的生成模型时，他们都没有明确地了解真实的数据分布p( <strong class="kx ir"> x </strong>)，因为p(<strong class="kx ir">x</strong>)=∫p(|<strong class="kx ir">z</strong>)p(<strong class="kx ir">z</strong>)d<strong class="kx ir">z</strong>通常是难以处理的，因为我们不可能对<strong class="kx ir"> z ~ p(z) </strong>的所有值进行积分。另一方面，基于流的生成模型能够通过使用标准化流来克服这个问题。</p><p id="c226" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx ir">标准化流程</strong>:标准化流程通过应用一系列双射变换，一个在另一个之上，将简单的分布变换为更复杂的分布。双射函数也有一个反函数，这意味着我们可以计算正向和反向。</p><p id="7a6e" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">基于流的生成模型只是一系列标准化的流，一个堆叠在另一个之上。因为变换函数是可逆的，所以基于流的模型也是可逆的(<strong class="kx ir"> x → z </strong>和<strong class="kx ir"> z →x) </strong>。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mo"><img src="../Images/1f3bdbafabbaa7cdbce163b1d7a8beb8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3GRq6GCnItJiAVzIIsHZyg.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><strong class="bd mp">情商。1: A流程</strong></p></figure><p id="e132" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">这里，<strong class="kx ir">z0</strong>是初始分布，<strong class="kx ir"> x = z_k </strong>是我们要学习的最终分布而f_i(。)是一系列双射变换函数。</p><p id="4e68" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">基于流量的模型允许我们直接优化数据的对数似然性，如下所示:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mq"><img src="../Images/e6c5d423b5d3a4a20e5e2de7a476a6fc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*E7FVKJtTmX4wprpClJfZzA.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><strong class="bd mp">情商。2:对数似然计算。</strong></p></figure><p id="e126" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">这里π_ 0(z0)是初始分布。通常，选择变换使得对数行列式易于计算。</p><p id="427c" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">要了解更多关于基于流程的生成模型及其工作原理，请查看这篇令人惊叹的博客<a class="ae mr" href="https://lilianweng.github.io/lil-log/2018/10/13/flow-based-deep-generative-models.html" rel="noopener ugc nofollow" target="_blank">帖子</a>！</p><h1 id="1a2e" class="lr ls iq bd lt lu lv lw lx ly lz ma mb jw mc jx md jz me ka mf kc mg kd mh mi bi translated">用于学习非纠缠表示的可逆解释网络</h1><p id="487e" class="pw-post-body-paragraph kv kw iq kx b ky mj jr la lb mk ju ld le ml lg lh li mm lk ll lm mn lo lp lq ij bi translated">论文[1]背后的主要思想是学习非线性映射，该非线性映射将来自任意预训练模型的学习到的表示转换到一个空间，在该空间中，新的表示可以分解成因子，使得每个因子的表示对应于一个人类可理解的语义概念。为此，作者提出了一种基于流的可逆网络<strong class="kx ir"> T </strong>，该网络学习从已学习的larent空间到新的、不纠缠的空间的映射(<strong class="kx ir">图1 </strong>，即:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ms"><img src="../Images/0d7746afe4bc982f936bb780158ccda1.png" data-original-src="https://miro.medium.com/v2/resize:fit:368/format:webp/1*6xUVJYeSeVCjtEzrkFFKxg.png"/></div></figure><p id="b533" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">并且由于<strong class="kx ir"> T </strong>是可逆的，所以反过来也是可能的:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mt"><img src="../Images/eb91eef1260872d1d7d9d061f6360c99.png" data-original-src="https://miro.medium.com/v2/resize:fit:458/format:webp/1*wdL4T7qsofXfyKshj_JOnQ.png"/></div></figure><p id="dca6" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">为了获得一个清晰且可解释的空间，作者建议将<strong class="kx ir"> Z~(读作Z颚化符)</strong>分解为<strong class="kx ir"> K+1 </strong>个因子，其中每个因子代表一个可解释的概念。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mu"><img src="../Images/0114f57811fa4557a56ae19813456ae4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bqzk_4T1D9X8z5UDEj_JAQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><strong class="bd mp">z ~的因式分解。z~被分解成k个因子，其中每个z ~ k是N_k维的。所有k的N_k总和为n。</strong></p></figure><p id="55dc" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">这里使用基于流的可逆网络是有意义的，因为我们能够从T: <strong class="kx ir"> z →z~ </strong>以及<strong class="kx ir"> z~ → z </strong>学习映射。这意味着，可逆模型<strong class="kx ir"> T </strong>也应该允许我们通过修改解开空间中的单个因素<strong class="kx ir"> z~_k </strong>来有意义地修改原始潜在表示<strong class="kx ir"> z </strong>。</p><p id="4c0d" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">实现因子<strong class="kx ir"> z~_k </strong>的解纠缠意味着对于所有k，每个<strong class="kx ir"> z~_k </strong>的分布彼此独立，并且所有<strong class="kx ir"> z~_k </strong>的联合分布可以写成</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mv"><img src="../Images/cfaaba5e749bdb4d9a20b7fe5f01b665.png" data-original-src="https://miro.medium.com/v2/resize:fit:1208/format:webp/1*6qm5BcqQkz8OtF6zqiDbQQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">情商。2:p(z ~)的因式分解。</p></figure><p id="0ada" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">每个因子z ~ k被模拟为一个单位高斯，因此联合分布成为</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mw"><img src="../Images/2a51cbb2eaa641493bbe28589fc8b2cb.png" data-original-src="https://miro.medium.com/v2/resize:fit:732/format:webp/1*gcugZSaux-PHbGrOOU92Hg.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">情商。3:联合分布p(z~)</p></figure><p id="689c" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">为了有解纠缠的因素，我们需要满足两个条件:<br/><strong class="kx ir">【I)</strong>每个z~_k应该只对一个可解释的概念敏感<br/> <strong class="kx ir"> ii) </strong> z~_k应该对它不代表的所有其他概念不变。</p><p id="a724" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">这是通过查看成对的图像(<strong class="kx ir"> x </strong> _a，<strong class="kx ir"> x_b </strong>)来实现的。每个语义概念<strong class="kx ir"> F ∈ {1，…，K} </strong>都用<strong class="kx ir"> z~_F </strong>来表示。使用从p(<strong class="kx ir">x</strong>a，<strong class="kx ir">x</strong>b | F)中提取的图像对(<strong class="kx ir">x</strong>a，<strong class="kx ir">x</strong>b)来训练每个<strong class="kx ir"> z~_F </strong>。除了<strong class="kx ir"> F ∈ {1，…，K}，</strong>之外的所有其他语义概念的表示都是通过剩余因子<strong class="kx ir"> z~_0来学习的。</strong></p><p id="8d15" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">给定共享相同概念的一对图像<strong class="kx ir"> F </strong>和它们的表示<strong class="kx ir"> z~^a </strong>和<strong class="kx ir"> z~^b </strong>(来自可逆模型的图像的表示)，我们需要确保两个图像的f因子的表示是相似的。为此，将图像b的F因子作为以图像a的F因子为中心的高斯:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mx"><img src="../Images/96faf73b4be0f5a698ee7940122ca60f.png" data-original-src="https://miro.medium.com/v2/resize:fit:958/format:webp/1*J9p_rbStJ2rNvIwvQyemdg.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><strong class="bd mp">情商。4 </strong></p></figure><p id="a4bb" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">正相关因子σ_ab ∈ (0，1)控制<strong class="kx ir"> z~^a_F </strong>和<strong class="kx ir"> z~^b_F </strong>有多接近/相似。</p><p id="2403" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx ir"> ii) </strong>所有其他因素的表示是不变的(无相关性),并被建模为单位高斯分布:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi my"><img src="../Images/e1f68cd2b129c728cd0c2216f89525cf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1364/format:webp/1*CKa5Fm0RI5Soko5jUiyPWw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><strong class="bd mp">情商。5 </strong></p></figure><p id="c895" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">为了训练流动模型，我们最大化图像对(<strong class="kx ir"> z </strong> _a，<strong class="kx ir"> z </strong> _b)的原始表示的可能性。该对的可能性计算如下</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mz"><img src="../Images/2086f473bd9a62994f44b2732da9c57d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1068/format:webp/1*AmaY7RMcFzjssWSPPI5oPQ.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><strong class="bd mp">情商。6 </strong></p></figure><p id="b71d" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">记录日志并应用Eqs。2、4和5到等式6，我们得到损失函数:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi na"><img src="../Images/35ed5e9dbe52db22af2762bc9d33182c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nrnxuFlY1MPZOFECXFCDsg.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><strong class="bd mp">情商。7:一个图像对的丢失</strong></p></figure><p id="0040" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在哪里，</p><p id="da13" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">红框中的项对应于p(z^a的所有因子的对数似然)，<br/>蓝框中的项对应于除p(z^b之外的所有因子的对数似然)，<br/>绿框中的项对应于图像b的因子f的对数似然，</p><p id="9dd6" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">T( <strong class="kx ir"> z </strong> ^j_k) = z~_k</p><p id="f88d" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">总损失是图像对在所有因子F上的损失:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nb"><img src="../Images/6fe7657d93f7d57b31521c9c0ddb62cb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1318/format:webp/1*Z19h4jZ3tmX59COJuznGbg.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><strong class="bd mp">情商。8:全损耗功能</strong></p></figure><h2 id="a032" class="nc ls iq bd lt nd ne dn lx nf ng dp mb le nh ni md li nj nk mf lm nl nm mh nn bi translated">估计每个因素的维度</h2><p id="875d" class="pw-post-body-paragraph kv kw iq kx b ky mj jr la lb mk ju ld le ml lg lh li mm lk ll lm mn lo lp lq ij bi translated">在IIN模型中，我们将潜在的<strong class="kx ir"> z~ </strong>分解为K+1个因子(残差+ K个因子)，其中K个因子中的每一个都可以具有不同的维度，条件是所有维度的总和等于<strong class="kx ir"> z~ </strong>的总和。</p><p id="8a01" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">通过计算每个因素的分数来估计每个因素的维度。因子f的分数被计算为所有图像对之间的原始表示的相关性的总和(<strong class="kx ir"> z </strong> ^a，<strong class="kx ir"> z </strong> ^b) ~ p( <strong class="kx ir"> z </strong> ^a，<strong class="kx ir"> z </strong> ^b|F)，即，</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi no"><img src="../Images/eeca37b8dbbd96e2d86fc85f09db1a33.png" data-original-src="https://miro.medium.com/v2/resize:fit:1078/format:webp/1*Sd_bEFNy_sV9408RrROLEA.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><strong class="bd mp">情商。9:每个因素的得分。</strong></p></figure><p id="8105" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">剩余因子被赋予分数<strong class="kx ir"> N </strong>(尺寸<strong class="kx ir"> z </strong>)。那么每个因素的维数估计为:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi np"><img src="../Images/24b63f5e08741fecacea3d30c2b14d3f.png" data-original-src="https://miro.medium.com/v2/resize:fit:576/format:webp/1*mMWGHbI81SAbPrZgCx3ljw.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><strong class="bd mp">情商。10:每个因子的估计维度。</strong></p></figure><h1 id="f225" class="lr ls iq bd lt lu lv lw lx ly lz ma mb jw mc jx md jz me ka mf kc mg kd mh mi bi translated">结果</h1><h2 id="88bc" class="nc ls iq bd lt nd ne dn lx nf ng dp mb le nh ni md li nj nk mf lm nl nm mh nn bi translated">交换解开的因素</h2><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/66d52014e672c58ff75d4f20f2c33d5e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1102/format:webp/1*Nv2jzgKdXY-fwZR4bI1GLg.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图2:将目标图像(最左边一列)的残差(<strong class="bd mp"> z~ </strong> _0)与目标图像(最上面一行)的动物类别因子(<strong class="bd mp"> z~ </strong> _1)相结合。图片来源:[1]。</p></figure><p id="8d5b" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在图2中，将目标图像(最左边的列)的残差(<strong class="kx ir"> z~ </strong> _0)与目标图像(最上面的行)的动物类别因子(<strong class="kx ir"> z~ </strong> _1)相结合，导致在结果图像中动物类型的转移，同时保持源图像的姿态。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/4974ef32fb837dc9db3be13ac3b0ca55.png" data-original-src="https://miro.medium.com/v2/resize:fit:938/format:webp/1*Kw0Bv5Qs0MreChN-h1Q8SA.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图3:沿着语义轴F → F~沿着微笑因子插值。图像来源:[1]</p></figure><p id="1942" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在CelebA数据集上沿着微笑的语义轴进行插值时，可以看到该模型能够控制微笑的数量，同时保持其他因素相对恒定。关于如何执行插值的更多信息可以在[1]中找到。</p><h1 id="d787" class="lr ls iq bd lt lu lv lw lx ly lz ma mb jw mc jx md jz me ka mf kc mg kd mh mi bi translated">密码</h1><p id="7cee" class="pw-post-body-paragraph kv kw iq kx b ky mj jr la lb mk ju ld le ml lg lh li mm lk ll lm mn lo lp lq ij bi translated">论文的代码可以在这里找到<a class="ae mr" href="https://github.com/CompVis/iin" rel="noopener ugc nofollow" target="_blank">。</a></p><h1 id="8d71" class="lr ls iq bd lt lu lv lw lx ly lz ma mb jw mc jx md jz me ka mf kc mg kd mh mi bi translated">结论</h1><p id="e974" class="pw-post-body-paragraph kv kw iq kx b ky mj jr la lb mk ju ld le ml lg lh li mm lk ll lm mn lo lp lq ij bi translated">在本文中，我们讨论了什么是无纠缠表示及其用途，然后简要地研究了基于流的生成模型是如何工作的。我们还深入研究了可逆解释网络。IIN是一个强大的网络，可以在任何预训练的网络上使用，以学习从原始表示到解开的表示的映射。利用基于流的模型的可逆性，该模型还可以用于任意修改解开的因子<strong class="kx ir"> z~ </strong> _k，以实现原始表示中语义上有意义的改变。</p><p id="976e" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">页（page的缩写）s:如果您发现任何错误/问题，请留下评论，我将很乐意修复它们！:)</p></div><div class="ab cl ns nt hu nu" role="separator"><span class="nv bw bk nw nx ny"/><span class="nv bw bk nw nx ny"/><span class="nv bw bk nw nx"/></div><div class="ij ik il im in"><h1 id="307c" class="lr ls iq bd lt lu nz lw lx ly oa ma mb jw ob jx md jz oc ka mf kc od kd mh mi bi translated">参考</h1><p id="1202" class="pw-post-body-paragraph kv kw iq kx b ky mj jr la lb mk ju ld le ml lg lh li mm lk ll lm mn lo lp lq ij bi translated">[1] Esser，Patrick，Robin Rombach和Bjorn Ommer。"一个用于解释潜在表征的解开的可逆解释网络."IEEE/CVF计算机视觉和模式识别会议文集。2020.</p><p id="9fb8" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">[2]本吉奥、库维尔和文森特。表征学习:回顾与新观点。在<em class="oe"> IEEE模式分析汇刊&amp;机器智能</em>，2013。</p></div></div>    
</body>
</html>