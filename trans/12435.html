<html>
<head>
<title>Generalized Attention Mechanism: BigBird’s Theoretical Foundation and General Transformers Models</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">广义注意机制:大鸟的理论基础和一般变形金刚模型</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/generalized-attention-mechanism-bigbirds-theoretical-foundation-and-general-transformers-models-9fb87bdac3b2?source=collection_archive---------11-----------------------#2021-12-19">https://towardsdatascience.com/generalized-attention-mechanism-bigbirds-theoretical-foundation-and-general-transformers-models-9fb87bdac3b2?source=collection_archive---------11-----------------------#2021-12-19</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><figure class="gl gn jr js jt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi jq"><img src="../Images/f8f8355a58a490ad1339a64f391fee7d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HYAd1aal5Et1A-Qzs6VAtQ.jpeg"/></div></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">美国宇航局在<a class="ae kf" href="https://unsplash.com/s/photos/network?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>拍摄的照片</p></figure><p id="87cc" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">作为一个小背景，<a class="ae kf" href="https://arxiv.org/abs/2007.14062" rel="noopener ugc nofollow" target="_blank"> <strong class="ki iu"> BigBird </strong> </a>是<a class="ae kf" href="https://research.google/pubs/pub49533/" rel="noopener ugc nofollow" target="_blank"> Google Research </a>最近发布的一个模型，它可以处理比以前可能的更长的文本序列。</p><figure class="lf lg lh li gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi le"><img src="../Images/fc8f4f3453dd6d9a7ffa755bf1fe4f3b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OxfgriCpOHlVSG0jX8YuwA.jpeg"/></div></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">BigBird 在处理了一整天的长文本序列后(<a class="ae kf" href="https://www.flickr.com/photos/62424772@N08/11504478233" rel="noopener ugc nofollow" target="_blank">BigBird</a>BY<a class="ae kf" href="https://www.flickr.com/photos/62424772@N08" rel="noopener ugc nofollow" target="_blank">J . Fei nberg</a>CC BY 2.0)</p></figure><p id="dbf7" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">以下是这种模式值得大肆宣传的几个原因。我喜欢这些属性甚至不是自然语言处理中出现的常见主题，但它显示了它的潜力:</p><ol class=""><li id="b76f" class="lj lk it ki b kj kk kn ko kr ll kv lm kz ln ld lo lp lq lr bi translated">正是<a class="ae kf" href="https://en.wikipedia.org/wiki/Turing_completeness" rel="noopener ugc nofollow" target="_blank">图灵完成</a>。</li><li id="6445" class="lj lk it ki b kj ls kn lt kr lu kv lv kz lw ld lo lp lq lr bi translated">它在基因组学研究方面取得了最先进的成果。</li><li id="8593" class="lj lk it ki b kj ls kn lt kr lu kv lv kz lw ld lo lp lq lr bi translated">它在使用变形金刚处理长文本序列方面击败了其他 SotA 模型，比如曾经主宰场景的<a class="ae kf" href="https://arxiv.org/abs/2004.05150" rel="noopener ugc nofollow" target="_blank"> Longformer </a>。</li></ol><p id="1cad" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">但是除了它的能力有多深远之外，这篇论文本身读起来非常有趣。作者深入研究了该模型的数学基础，以证明该设计的合理性，以及他们如何解释该模型的优越性能。</p><p id="7c0a" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在本文中没有得到足够讨论的一个概念是<strong class="ki iu"> <em class="lx">广义注意机制</em> </strong>。</p><h2 id="b055" class="ly lz it bd ma mb mc dn md me mf dp mg kr mh mi mj kv mk ml mm kz mn mo mp mq bi translated">变形金刚通常是如何解释的</h2><p id="c667" class="pw-post-body-paragraph kg kh it ki b kj mr kl km kn ms kp kq kr mt kt ku kv mu kx ky kz mv lb lc ld im bi translated">这个概念本身只不过是观看变形金刚模型的另一种方式。通常，当我们谈论变形金刚模型和它们的<strong class="ki iu">注意力机制</strong>时，它们被想象成这样:</p><figure class="lf lg lh li gt ju gh gi paragraph-image"><div class="gh gi mw"><img src="../Images/7be317d6bb75521e50d49e76a1f06fdd.png" data-original-src="https://miro.medium.com/v2/resize:fit:764/format:webp/1*i15dbiO-PJRr4TXIt1J6pA.png"/></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">作者使用<a class="ae kf" href="https://github.com/jessevig/bertviz" rel="noopener ugc nofollow" target="_blank"> BertViz </a>进行可视化</p></figure><p id="e510" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">变形金刚从标题为<a class="ae kf" href="https://papers.nips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf" rel="noopener ugc nofollow" target="_blank"> <strong class="ki iu"> <em class="lx">注意</em> </strong> <em class="lx">的论文开始，这就是你所需要的全部</em> </a> <em class="lx"> </em>，这可能是今天 NLP 中引用最多的一篇论文。中心思想是这样的:<strong class="ki iu"> <em class="lx">对一个句子中的一个标记的分析包括查看其他标记的整个集合</em> </strong> <em class="lx">，</em>，除了对每个标记进行基于注意力的加权。</p><p id="beeb" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">你很难找到变形金刚模型的其他可视化形式。有一整套像<a class="ae kf" href="https://github.com/jessevig/bertviz" rel="noopener ugc nofollow" target="_blank"> <strong class="ki iu"> BertViz </strong> </a>这样的工具和库，可以让你像这样制作变形金刚模型的可视化。</p><p id="42eb" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">但是，如果我告诉你，这些可视化并没有公正的变形金刚的真实本性呢？这些流行的可视化方法忽略了关于注意力机制本质的核心属性。</p><p id="7a29" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这篇论文建议我们，思考变形金刚如何处理序列的正确方式是把它看作<strong class="ki iu"> <em class="lx">图形运算</em> </strong>。</p><h1 id="552b" class="mx lz it bd ma my mz na md nb nc nd mg ne nf ng mj nh ni nj mm nk nl nm mp nn bi translated">广义注意机制</h1><p id="2805" class="pw-post-body-paragraph kg kh it ki b kj mr kl km kn ms kp kq kr mt kt ku kv mu kx ky kz mv lb lc ld im bi translated"><strong class="ki iu"> <em class="lx">广义注意机制</em> </strong>背后的思想是，我们应该把序列上的注意机制看作图形操作。</p><figure class="lf lg lh li gt ju gh gi paragraph-image"><div class="gh gi no"><img src="../Images/5a433bb3678092a6c0ca1228e7a80634.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*Q1LytkMkmuI4dVeTnuwoTQ.png"/></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">来自 Avinava Dubey 的谷歌人工智能博客 BigBird</p></figure><p id="d2e5" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><a class="ae kf" href="https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf" rel="noopener ugc nofollow" target="_blank"> <strong class="ki iu"> <em class="lx">关注是你所需要的全部</em> </strong> </a>背后的中心思想是，模型在处理每个令牌的同时，关注序列中的每一个其他令牌。那么，如果我们把记号看作图上的节点，注意每一个其他的记号意味着该节点与图中的每一个其他节点相连。</p><p id="4bb5" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">换句话说，传统的变形金刚模型可以另外表示为一个<strong class="ki iu"> <em class="lx">全连通图</em> </strong>。事实上，它们是<strong class="ki iu"> <em class="lx">加权的</em> </strong>全连通图，其中权重是我们大肆宣传的关注分数。</p><figure class="lf lg lh li gt ju gh gi paragraph-image"><div class="gh gi np"><img src="../Images/008211165916e887d5c778a67c16f3d9.png" data-original-src="https://miro.medium.com/v2/resize:fit:698/format:webp/1*UYWciJx251ZGrJm7Nyq0yQ.png"/></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">这篇<a class="ae kf" href="https://www.researchgate.net/figure/a-Undirected-fully-connected-weighted-Graph-G-b-Adjacency-Matrix-of-G_fig1_320116922" rel="noopener ugc nofollow" target="_blank">论文</a>中的加权全连通图示例。</p></figure><p id="0f4b" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这种替代方案，<strong class="ki iu"><em class="lx"/></strong>用图论的方法来观察变压器如何处理序列中的记号是非常强大的，因为我们可以直接应用数学家们已经开发了几个世纪的图论中的强大工具。</p><p id="f5e6" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这是大鸟设计的核心指导思想。</p><h1 id="9c32" class="mx lz it bd ma my mz na md nb nc nd mg ne nf ng mj nh ni nj mm nk nl nm mp nn bi translated">全连通图有多痛苦</h1><p id="b38c" class="pw-post-body-paragraph kg kh it ki b kj mr kl km kn ms kp kq kr mt kt ku kv mu kx ky kz mv lb lc ld im bi translated">BigBird 是出于处理长文本序列的需要。如果你以前用过变形金刚模型，这可能是 NLP 中的一个明显的问题。您会直接知道即使模型有数十亿个参数，最多也只有 1024 个标记。</p><p id="8518" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我对变形金刚形象化的另一个问题是:</p><figure class="lf lg lh li gt ju gh gi paragraph-image"><div class="gh gi mw"><img src="../Images/7be317d6bb75521e50d49e76a1f06fdd.png" data-original-src="https://miro.medium.com/v2/resize:fit:764/format:webp/1*i15dbiO-PJRr4TXIt1J6pA.png"/></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">作者使用<a class="ae kf" href="https://github.com/jessevig/bertviz" rel="noopener ugc nofollow" target="_blank"> BertViz </a>进行可视化</p></figure><p id="024c" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">或者这些:</p><figure class="lf lg lh li gt ju gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/3eb8bf89f566fc2fd033d0a8d68f4ad6.png" data-original-src="https://miro.medium.com/v2/resize:fit:816/format:webp/1*7IL_CuoKOTKRHUdpOPjq5Q.png"/></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">出自原来的<a class="ae kf" href="https://arxiv.org/pdf/1810.04805.pdf" rel="noopener ugc nofollow" target="_blank">伯特论文</a>。</p></figure><p id="c67d" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">是<strong class="ki iu"> <em class="lx">他们</em> </strong> <strong class="ki iu"> <em class="lx">没有恰当地捕捉到变形金刚模型背后的</em> </strong>处理有多激烈。</p><p id="1a92" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">一旦你把它们想成是完全连通的图，这就变得更加清楚了。</p><figure class="lf lg lh li gt ju gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/69d7e6695234223448f1f77c97ef6180.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/1*LblecaKOW3pOFhn11_X_Ng.png"/></div><p class="kb kc gj gh gi kd ke bd b be z dk translated"><a class="ae kf" href="https://upload.wikimedia.org/wikipedia/commons/d/dd/20-simplex_graph.png" rel="noopener ugc nofollow" target="_blank">汤姆·鲁恩</a>，公共领域，通过维基共享</p></figure><p id="699f" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这是一个只有 20 个节点的完整图！你能想象 1024 个节点的完整图形会是什么样子吗，更不用说 BigBird 能够处理 4000 多个令牌了？</p><p id="ed13" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">上图中的每条边代表一个计算，上图中所有计算<strong class="ki iu"> </strong>的集合只针对一层。</p><h1 id="8902" class="mx lz it bd ma my mz na md nb nc nd mg ne nf ng mj nh ni nj mm nk nl nm mp nn bi translated">利用图论:图稀疏化</h1><p id="6761" class="pw-post-body-paragraph kg kh it ki b kj mr kl km kn ms kp kq kr mt kt ku kv mu kx ky kz mv lb lc ld im bi translated">幸运的是，在计算机科学和数学的其他领域，这是一个古老的问题。BigBird 建议做的事情本质上是利用这些工具，当涉及到具有大量节点的图形时，我们看到这些工具正在其他领域中使用。</p><blockquote class="ns nt nu"><p id="173f" class="kg kh lx ki b kj kk kl km kn ko kp kq nv ks kt ku nw kw kx ky nx la lb lc ld im bi translated">这种将自我关注视为完全连通图的观点允许我们利用现有的图论来帮助降低其复杂性。降低自我关注的二次复杂度的问题现在可以被看作是一个<strong class="ki iu">图稀疏化问题</strong>——来自<a class="ae kf" href="https://arxiv.org/pdf/2007.14062.pdf" rel="noopener ugc nofollow" target="_blank"> BigBird 论文</a>。</p></blockquote><p id="2438" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">以下是对 BigBird 使用的方法的综合介绍:</p><figure class="lf lg lh li gt ju gh gi paragraph-image"><div class="gh gi no"><img src="../Images/38074642074ad7baa269451e1663f65c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*EWvJRjx-gw47j4yL48n_Dg.png"/></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">来自 Avinava Dubey 的谷歌人工智能博客文章</p></figure><p id="0823" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">下面的矩阵是<strong class="ki iu">邻接矩阵</strong>。它们是将图形数据结构表示为具有数值的矩阵的有用方式，其中每个位置的正值表示两个节点之间的连接。</p><figure class="lf lg lh li gt ju gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/20a5b798aa3ec392d981fe339aaddea8.png" data-original-src="https://miro.medium.com/v2/resize:fit:800/format:webp/1*KR0leu4nKkw4nFt1Q82ptg.png"/></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">邻接矩阵的例子，来自 nl.wikipedia 的<a class="ae kf" href="https://upload.wikimedia.org/wikipedia/commons/3/3b/AdjacencyMatrix.png" rel="noopener ugc nofollow" target="_blank"> Yepke，</a><a class="ae kf" href="https://creativecommons.org/licenses/by-sa/3.0" rel="noopener ugc nofollow" target="_blank"> CC BY-SA 3.0 </a>，通过维基共享</p></figure><p id="bbd2" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们现在知道 BigBird 的指导原则是<strong class="ki iu">稀疏化变形金刚</strong>模型的计算图，这样处理更长的序列是一个更现实的目标。</p><p id="fff2" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们需要一个比原始全连通图简单得多的图的设计，它仍然保留了节点之间有意义的连接。</p><p id="18f7" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">以下是设计此最佳图表的步骤。</p><h2 id="43b7" class="ly lz it bd ma mb mc dn md me mf dp mg kr mh mi mj kv mk ml mm kz mn mo mp mq bi translated"><strong class="ak"> 1。鄂尔多斯-雷尼模型:最简单的随机图</strong></h2><figure class="lf lg lh li gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi nz"><img src="../Images/81d61ba8d5813eaae5501e5336af4129.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*p4J56ZA_mzHc0dx6Wd9w9Q.png"/></div></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">来自<a class="ae kf" href="https://www.researchgate.net/figure/Erdoes-Renyi-model-of-random-graph-evolution_fig10_313854183" rel="noopener ugc nofollow" target="_blank"> ResearchGate </a>的一个帖子</p></figure><p id="37da" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这只是随机和独立地挑选连接节点的边的概念。如果问题是我们有太多的连接，那么这就是随机丢弃大多数连接的方法。</p><p id="cf06" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这种方法实际上比预期的更有效<strong class="ki iu">,因为两个变得不连接的令牌仍然可能共享信号</strong>,因为它们可能具有它们都连接到的其他节点。由于变压器需要多层处理，这些连接可以复合足够多的令牌，以充分地相互连接。</p><blockquote class="ns nt nu"><p id="b9a9" class="kg kh lx ki b kj kk kl km kn ko kp kq nv ks kt ku nw kw kx ky nx la lb lc ld im bi translated">因此，<strong class="ki iu">这样的随机图在光谱上接近完整图</strong>，并且它的第二特征值(邻接矩阵的)离第一特征值相当远。这一特性导致图中随机游走的快速混合时间，这非正式地表明信息可以在任意一对节点之间快速流动——来自论文。</p></blockquote><h2 id="dadd" class="ly lz it bd ma mb mc dn md me mf dp mg kr mh mi mj kv mk ml mm kz mn mo mp mq bi translated">2.瓦特和斯特罗加兹模型:参照的局部性</h2><p id="cfed" class="pw-post-body-paragraph kg kh it ki b kj mr kl km kn ms kp kq kr mt kt ku kv mu kx ky kz mv lb lc ld im bi translated">一般来说，单词之间的接近度是自然语言处理中的一个重要因素。直观地说，一个给定标记的大部分信息都可以被它周围的人找到。</p><p id="be5d" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">一个简单的随机图错过了这种结构，因为它固有地在整个图中产生相等的稀疏度——紧邻一个标记的单词被认为是数千个标记之外的单词的概率相等。</p><p id="ce5a" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">因此，解决方案是考虑一种“滑动窗口”方法:</p><figure class="lf lg lh li gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi oa"><img src="../Images/cc0d4295ede556a291c79ab912cb7dc1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*npPS6USvmooaDXFYhXOvYA.png"/></div></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">来自<a class="ae kf" href="https://arxiv.org/pdf/2004.05150v2.pdf" rel="noopener ugc nofollow" target="_blank">龙前纸业。</a></p></figure><p id="de6e" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">因为我们知道单词周围的记号在分析单词时肯定会很重要，<strong class="ki iu">我们简单地保证对每个记号的 n 个最近的记号的全部关注。这实际上是以前在为长文档应用变形时提出的，尤其是在<a class="ae kf" href="https://arxiv.org/pdf/2004.05150v2.pdf" rel="noopener ugc nofollow" target="_blank"> Longformer </a>中。</strong></p><p id="ef1a" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">虽然这不是一个完美的类比，但我认为可以将其比作网络中的集线器:</p><figure class="lf lg lh li gt ju gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/954959299a8db430d60f2c3dd9dd5f0a.png" data-original-src="https://miro.medium.com/v2/resize:fit:800/format:webp/1*89x4fcYSYRiA392InZOQsA.png"/></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">来自<a class="ae kf" href="https://commons.wikimedia.org/wiki/File:Scale-free_network_sample.png" rel="noopener ugc nofollow" target="_blank">马克西姆</a>、<a class="ae kf" href="https://creativecommons.org/licenses/by-sa/3.0" rel="noopener ugc nofollow" target="_blank"> CC BY-SA 3.0 </a>的本地枢纽示例，通过维基共享</p></figure><p id="541b" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">通过这些窗口，我们围绕每个令牌创建了<strong class="ki iu">注意力</strong>集群。结合简单的随机图方法，这些注意力的<strong class="ki iu">“本地中枢”</strong>被随机连接到整个文档中的一些标记。这使我们既能实现图的稀疏化，又能在分析中保持一定的局部性。</p><p id="1f81" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">然而，作者指出，简单的随机图和滑动窗口图的组合不足以让该模型显示 BERT 级性能。</p><p id="eee8" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">毫不奇怪，这种方法很难赶上 BERT 的性能，因为稀疏图包含的分析量本质上比全连通图少。但下面的最后一个组件显然是核心组件，至少在经验上，它允许 BigBird 显示出类似于 BERT 的性能，尽管它很稀疏。</p><h2 id="533f" class="ly lz it bd ma mb mc dn md me mf dp mg kr mh mi mj kv mk ml mm kz mn mo mp mq bi translated">3.全局令牌</h2><p id="6059" class="pw-post-body-paragraph kg kh it ki b kj mr kl km kn ms kp kq kr mt kt ku kv mu kx ky kz mv lb lc ld im bi translated">之前我们注意到:</p><blockquote class="ns nt nu"><p id="6f7c" class="kg kh lx ki b kj kk kl km kn ko kp kq nv ks kt ku nw kw kx ky nx la lb lc ld im bi translated">…[<strong class="ki iu">]两个未连接的节点</strong> ]仍然可能共享信号，因为它们可能都连接到其他节点。…这些连接可以复合到足以使令牌彼此<strong class="ki iu">充分连接</strong>。</p><p id="42e3" class="kg kh lx ki b kj kk kl km kn ko kp kq nv ks kt ku nw kw kx ky nx la lb lc ld im bi translated">“因此，<strong class="ki iu">这种随机图在光谱上接近完整图</strong>……这非正式地表明信息可以在任何一对节点之间快速流动”</p></blockquote><p id="4760" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们打了一个比方，滑动注意力窗口创造了局部注意力中心。</p><p id="a63e" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">对于全局令牌，我们指定一个令牌列表，让<strong class="ki iu">处理文档中的所有令牌</strong>。虽然 BERT 被设计成每个令牌照顾其他令牌，但是 BigBird 被设计成只有少数指定数量的令牌这样做。</p><figure class="lf lg lh li gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi ob"><img src="../Images/e76ea07ae2909df4e85563e3439ee8f6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XvhDrCp1_CVHgRXwlyJkhA.png"/></div></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">完整的大鸟来自<a class="ae kf" href="https://huggingface.co/blog/big-bird" rel="noopener ugc nofollow" target="_blank"> HuggingFace 的博客</a></p></figure><p id="42aa" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这是有用的，有几个原因，其中一些作者没有明确指出:</p><ol class=""><li id="2185" class="lj lk it ki b kj kk kn ko kr ll kv lm kz ln ld lo lp lq lr bi translated">全局记号(类似于 BERT 中的[CLS]记号)通常非常有用，因为虽然它们只是序列的另一个记号，但是它们本身能够表示完整的输入序列。如果没有全局标记，就很难生成一个表示整个文档的向量。</li><li id="c2c9" class="lj lk it ki b kj ls kn lt kr lu kv lv kz lw ld lo lp lq lr bi translated">虽然作者没有明确提到这一点，但我相信这是另一个非常重要的结果。关于未连接的令牌(由于随机图的构造)仍然通过跳过相互邻居来共享信号的想法，<strong class="ki iu"> <em class="lx">全局令牌保证这是真的</em> </strong>。在添加全局令牌之前，对彼此非常重要的两个令牌实际上可能是断开的，并且只能共享由于跳过太多节点而显著减弱的信号。<strong class="ki iu">现在有了全局令牌，它们被保证最多相距两跳</strong>，因为全局令牌被保证是图中的共同节点。</li></ol><p id="5930" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">最后，下面是作者提供的最终图表，它很好地总结了这一点:</p><figure class="lf lg lh li gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi oc"><img src="../Images/c843c1e75ffe88377d2fe4ca64e33552.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mu6nZOkMs8gVhrsOiJGsyQ.png"/></div></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">出自<a class="ae kf" href="https://arxiv.org/pdf/2007.14062.pdf" rel="noopener ugc nofollow" target="_blank">大鸟论文</a>。</p></figure><p id="4c2c" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">单纯看 BigBird 注意力的最后一张图，即使有很多空连接，直观上也不会让我们觉得太在意它们——好像没有它们处理就完全没问题了。这有点像原始全连通图的信息最大化缩减。</p><p id="f269" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">但随着这种减少，BigBird 能够处理的序列远远超过以前的 1024 个限制，甚至达到 4096 个，而不会像完全连接的同行那样损失太多性能。</p><p id="04f8" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我想很多读者可能也会看最后一张图，并想知道他们如何能够在 GPU 中表示这样的计算图。这是一个完全不同的话题，作者利用了非常聪明的方法“阻断稀疏注意力”来做到这一点。这是另一个话题，一个彻底的解释(用代码！)可以在一篇<a class="ae kf" href="https://huggingface.co/blog/big-bird" rel="noopener ugc nofollow" target="_blank"> Huggingface 的博文</a>中找到。</p><p id="52d8" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">感谢阅读！</p></div></div>    
</body>
</html>