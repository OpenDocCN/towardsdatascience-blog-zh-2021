<html>
<head>
<title>Basics of Deep Learning: Backpropagation</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">深度学习的基础:反向传播</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/basics-of-deep-learning-backpropagation-e2aeb435727?source=collection_archive---------26-----------------------#2021-08-08">https://towardsdatascience.com/basics-of-deep-learning-backpropagation-e2aeb435727?source=collection_archive---------26-----------------------#2021-08-08</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="80ae" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">从头开始反向传播的分步实践教程</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/b9e3b62951323a3bbf260213cbab872a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*xDjIwV7oxxaRlubF"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">劳伦·里奇蒙的照片</p></figure><p id="f963" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我现在已经研究深度学习有一段时间了，我成为了PyTorch或TensorFlow等当前深度学习框架的超级粉丝。然而，随着我越来越习惯这样简单但强大的工具，深度学习中核心概念的基础，如反向传播，开始淡出。我相信回到基础总是好的，我想做一个详细的实践教程来理清事情。</p><div class="ls lt gp gr lu lv"><a href="https://colab.research.google.com/drive/10sYc0tB2dw_jsw9D61SwliN1nqugyjP1?usp=sharing" rel="noopener  ugc nofollow" target="_blank"><div class="lw ab fo"><div class="lx ab ly cl cj lz"><h2 class="bd ir gy z fp ma fr fs mb fu fw ip bi translated">MNIST分类的逐步反向传播算法</h2><div class="mc l"><h3 class="bd b gy z fp ma fr fs mb fu fw dk translated">林炳成</h3></div><div class="md l"><p class="bd b dl z fp ma fr fs mb fu fw dk translated">colab.research.google.com</p></div></div><div class="me l"><div class="mf l mg mh mi me mj kp lv"/></div></div></a></div><h1 id="6e58" class="mk ml iq bd mm mn mo mp mq mr ms mt mu jw mv jx mw jz mx ka my kc mz kd na nb bi translated">介绍</h1><p id="233e" class="pw-post-body-paragraph kw kx iq ky b kz nc jr lb lc nd ju le lf ne lh li lj nf ll lm ln ng lp lq lr ij bi translated">深度学习的基本过程是使用学习到的权重执行网络定义的操作。比如著名的卷积神经网络(CNN)就是乘法、加法等。像素强度值具有由网络设计的这种规则。然后，如果我们想要分类图片是狗还是猫，我们应该以某种方式得到操作后的二进制结果，以告诉1是狗，0是猫。</p><p id="d275" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">当我们训练网络时，我们只是简单地更新权重，以便输出结果变得更接近答案。换句话说，有了一个训练有素的网络，我们可以正确地将图像分类到它真正属于的类别。这就是反向传播的用武之地。我们计算梯度并逐步更新权重以满足目标。目标函数(又名损失函数)是我们如何量化答案和预测之间的差异。通过一个简单且可微的目标函数，我们可以很容易地找到全局最小值。然而，在大多数情况下，这并不是一个微不足道的过程。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nh"><img src="../Images/c2e7cf61d0f3033255e7776032e1d05e.png" data-original-src="https://miro.medium.com/v2/resize:fit:700/format:webp/0*tpoXIDiVbvfit3B-.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图片来源<a class="ae kv" href="https://en.wikipedia.org/wiki/Gradient_descent" rel="noopener ugc nofollow" target="_blank">维基百科</a></p></figure><h1 id="7552" class="mk ml iq bd mm mn mo mp mq mr ms mt mu jw mv jx mw jz mx ka my kc mz kd na nb bi translated">链式法则</h1><p id="96e8" class="pw-post-body-paragraph kw kx iq ky b kz nc jr lb lc nd ju le lf ne lh li lj nf ll lm ln ng lp lq lr ij bi translated">没有链式法则，你就无法谈论反向传播。链规则使您能够以简单的方式计算局部梯度。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ni"><img src="../Images/3a63c27c7c09434c51520366b901b4ab.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TMMsrk8LpBqZztEgI4C7yg.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">局部梯度的计算(费，2017)</p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nj"><img src="../Images/d79c8260b06cb0a2bbcfbd33f2f0fb80.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0T12s8OunqB5sy1Z-OuKHw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">反向传播的例子(费，2017)</p></figure><p id="5b75" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这是一个简单的反向传播的例子。正如我们之前讨论的，输入数据是上面的<em class="nk"> x </em>、<em class="nk"> y </em>和<em class="nk"> z </em>。圆形节点是操作，它们构成了功能<em class="nk"> f </em>。由于我们需要知道每个输入变量对输出结果的影响，给定<em class="nk"> x </em>、<em class="nk"> y </em>或<em class="nk"> z </em>的<em class="nk"> f </em>的偏导数就是我们想要得到的梯度。然后，通过链规则，我们可以反向传播梯度，并获得每个局部梯度，如上图所示。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nl"><img src="../Images/d0f67dbb903ff4f6d1c25cadb8f3f93a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8Eg6UYLdIaJS6sE5P7r9SQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">矢量化反向传播示例(费-李非，2017年)</p></figure><p id="0428" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">由于我们将在实际实现中进行更多的矢量化计算，这里有一个例子，函数<em class="nk"> f </em>是L2范数。L2范数的梯度正好是输入值的两倍，即上面的<em class="nk"> 2q </em>。那么，给定<em class="nk"> W </em>的<em class="nk"> q </em>的偏导数将是<em class="nk"> 2q </em>和<em class="nk"> x </em>转置的内积。还有，同样给定的<em class="nk"> x </em>会是<em class="nk"> W </em>转置和<em class="nk"> 2q </em>的内积，而为什么<em class="nk"> W </em>转置是因为<em class="nk"> xᵢ </em>的每个偏导数都给定了<em class="nk"> W </em>的列向量。</p><h1 id="93a0" class="mk ml iq bd mm mn mo mp mq mr ms mt mu jw mv jx mw jz mx ka my kc mz kd na nb bi translated">激活功能</h1><p id="761e" class="pw-post-body-paragraph kw kx iq ky b kz nc jr lb lc nd ju le lf ne lh li lj nf ll lm ln ng lp lq lr ij bi translated">在深度学习中，如果没有激活函数，层之间的一组线性操作终究只是一个大的线性函数。非线性激活函数进一步增加了模型的复杂性。我将介绍一个基本的激活函数及其导数，来计算我们反向传播的梯度。</p><h2 id="2a6c" class="nm ml iq bd mm nn no dn mq np nq dp mu lf nr ns mw lj nt nu my ln nv nw na nx bi translated">乙状结肠的</h2><div class="kg kh ki kj gt ab cb"><figure class="ny kk nz oa ob oc od paragraph-image"><img src="../Images/92cb2932ebd0125e9719f815b29687b4.png" data-original-src="https://miro.medium.com/v2/resize:fit:540/format:webp/1*Xc6knxVezyPOxfx7PcL__Q.png"/></figure><figure class="ny kk oe oa ob oc od paragraph-image"><img src="../Images/b414f16fbe62a861597ea477521ff76f.png" data-original-src="https://miro.medium.com/v2/resize:fit:814/format:webp/1*Dq_LSfWDChAYf9mPsL0TnA.png"/></figure></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi of"><img src="../Images/88436f0e2dd264c812ba5de19efb6b23.png" data-original-src="https://miro.medium.com/v2/resize:fit:800/0*qTdsQDwaKSV87U3_"/></div></figure><h2 id="68cc" class="nm ml iq bd mm nn no dn mq np nq dp mu lf nr ns mw lj nt nu my ln nv nw na nx bi translated">双曲正切</h2><div class="kg kh ki kj gt ab cb"><figure class="ny kk og oa ob oc od paragraph-image"><img src="../Images/b5448385617b2968fadb8555383fde3e.png" data-original-src="https://miro.medium.com/v2/resize:fit:522/format:webp/1*S23qOOmrkqmDyhy1BhBRuA.png"/></figure><figure class="ny kk oh oa ob oc od paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><img src="../Images/91fce5775ea43b9e1283924a7c825d93.png" data-original-src="https://miro.medium.com/v2/resize:fit:1144/format:webp/1*QgSH-DJkVtwbnRR9sisuZg.png"/></div></figure></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oi"><img src="../Images/b41a69038de8d9c08963ae4ed340801f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1136/0*0KsXiF9_YdjQUWHi"/></div></figure><h2 id="b345" class="nm ml iq bd mm nn no dn mq np nq dp mu lf nr ns mw lj nt nu my ln nv nw na nx bi translated">整流线性单位</h2><div class="kg kh ki kj gt ab cb"><figure class="ny kk oj oa ob oc od paragraph-image"><img src="../Images/50fc9195cf8640ab8a90af98942b9d8e.png" data-original-src="https://miro.medium.com/v2/resize:fit:618/format:webp/1*HInGhhCQHBnWyMEWBmypIQ.png"/></figure><figure class="ny kk ok oa ob oc od paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><img src="../Images/0d34620ef4f63fb3ed591de126850a65.png" data-original-src="https://miro.medium.com/v2/resize:fit:776/format:webp/1*4ih58ZZgX3ZEB-71Lt0llA.png"/></div></figure></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ol"><img src="../Images/786b1ac580089c0275a8d45af187605f.png" data-original-src="https://miro.medium.com/v2/resize:fit:964/0*h_AVCR9wlpl9GZqo"/></div></figure><h1 id="ece2" class="mk ml iq bd mm mn mo mp mq mr ms mt mu jw mv jx mw jz mx ka my kc mz kd na nb bi translated">目标函数</h1><p id="0e13" class="pw-post-body-paragraph kw kx iq ky b kz nc jr lb lc nd ju le lf ne lh li lj nf ll lm ln ng lp lq lr ij bi translated">在训练神经网络时，量化预测与答案的接近程度的有效方法是非常重要的。为了执行反向传播和更新影响输出预测的所有相关权重，需要可微分的目标函数(又名损失函数)。我将介绍两个目标函数，称为均方误差(MSE)和交叉熵损失函数。</p><h2 id="48bc" class="nm ml iq bd mm nn no dn mq np nq dp mu lf nr ns mw lj nt nu my ln nv nw na nx bi translated">均方误差</h2><p id="5b27" class="pw-post-body-paragraph kw kx iq ky b kz nc jr lb lc nd ju le lf ne lh li lj nf ll lm ln ng lp lq lr ij bi translated">MSE是当今最通用的损失项，常用于预测数值。它计算了预测和事实之间的平均平方距离。最终激活层通常遵循线性或ReLU。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi om"><img src="../Images/cba4bb8d06e53e57226c8e479d6dcdb2.png" data-original-src="https://miro.medium.com/v2/resize:fit:618/0*uImq4xGVRPohJPKv"/></div></figure><h2 id="617d" class="nm ml iq bd mm nn no dn mq np nq dp mu lf nr ns mw lj nt nu my ln nv nw na nx bi translated">交叉熵</h2><p id="9d92" class="pw-post-body-paragraph kw kx iq ky b kz nc jr lb lc nd ju le lf ne lh li lj nf ll lm ln ng lp lq lr ij bi translated">交叉熵通常用于从多个类别中预测单个标签。对于最终激活函数，它通常遵循softmax，使得输出概率之和为1，并且它提供了对损失项的非常简单的推导，如下所示。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi on"><img src="../Images/2f3689020cfde1f04ad5d5c8b10fbcdd.png" data-original-src="https://miro.medium.com/v2/resize:fit:908/0*5Esc0swdW0GLixeD"/></div></figure><h1 id="0edc" class="mk ml iq bd mm mn mo mp mq mr ms mt mu jw mv jx mw jz mx ka my kc mz kd na nb bi translated">反向传播</h1><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oo"><img src="../Images/7988eca993620d642eaa3a718d2a7ea9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mLWOlqAf6jzODvUjYD0mVg.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="9aab" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">对于如上所述的全连接网络，在反向传播中只需要考虑三件事。来自右侧的通过梯度，从激活函数的导数计算的局部梯度，以及关于权重和左侧输入的通过梯度。</p><p id="c407" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">第一个梯度来自损失项，通过如上所述的这些项的推导，我们可以开始从右向左传递梯度。从每一层，我们首先计算关于激活层的梯度。然后，该梯度与输入值(<em class="nk">z’</em>)的内积将是相对于我们的权重的梯度。此外，权重梯度的内积(<em class="nk"> w </em>)将是下一个向左通过的梯度。</p><p id="c943" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">重复这个简单的过程是我们成功反向传播所需要的！</p><h1 id="3891" class="mk ml iq bd mm mn mo mp mq mr ms mt mu jw mv jx mw jz mx ka my kc mz kd na nb bi translated">参考</h1><p id="d272" class="pw-post-body-paragraph kw kx iq ky b kz nc jr lb lc nd ju le lf ne lh li lj nf ll lm ln ng lp lq lr ij bi translated">[1]费-李非，<a class="ae kv" href="http://cs231n.stanford.edu/2017/index.html" rel="noopener ugc nofollow" target="_blank"> CS231n:用于视觉识别的卷积神经网络</a>，2017 <br/> [2] Stacey Ronaghan，<a class="ae kv" rel="noopener" target="_blank" href="/deep-learning-which-loss-and-activation-functions-should-i-use-ac02f1c56aa8">深度学习:我应该使用哪些损失和激活函数？</a>，2018</p></div></div>    
</body>
</html>