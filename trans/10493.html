<html>
<head>
<title>Understanding The Accuracy-Interpretability Trade-Off</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">理解准确性和可解释性的权衡</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/accuracy-interpretability-trade-off-8d055ed2e445?source=collection_archive---------16-----------------------#2021-10-06">https://towardsdatascience.com/accuracy-interpretability-trade-off-8d055ed2e445?source=collection_archive---------16-----------------------#2021-10-06</a></blockquote><div><div class="fc ii ij ik il im"/><div class="in io ip iq ir"><div class=""/><div class=""><h2 id="bb88" class="pw-subtitle-paragraph jr it iu bd b js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki dk translated">讨论机器学习中模型准确性和可解释性之间的权衡</h2></div><figure class="kk kl km kn gu ko gi gj paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gi gj kj"><img src="../Images/65ae2b00e9eae9d002447412fb72abd2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*c7e8wYaLrh9HUlEDlH5veA.jpeg"/></div></div><p class="kv kw gk gi gj kx ky bd b be z dk translated">Abel Y Costa 在<a class="ae kz" href="https://unsplash.com/s/photos/table?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><h2 id="1899" class="la lb iu bd lc ld le dn lf lg lh dp li lj lk ll lm ln lo lp lq lr ls lt lu lv bi translated">介绍</h2><p id="395e" class="pw-post-body-paragraph lw lx iu ly b lz ma jv mb mc md jy me lj mf mg mh ln mi mj mk lr ml mm mn mo in bi translated">在我之前的一个角色中，我讨论了机器学习环境中参数化和非参数化方法之间的<a class="ae kz" rel="noopener" target="_blank" href="/parametric-vs-non-parametric-methods-2cea475da1a">差异。</a></p><p id="c509" class="pw-post-body-paragraph lw lx iu ly b lz mp jv mb mc mq jy me lj mr mg mh ln ms mj mk lr mt mm mn mo in bi translated">参数方法对数据和要估计的函数之间的关系做出假设，因此它们通常是不灵活的。例如，我们可以<strong class="ly iv">假设</strong>函数<em class="mu"> f </em>是线性的，并使用简单的线性回归，这在只能估计<em class="mu"> f </em>的线性形状的意义上可以认为是不灵活的。</p><p id="b746" class="pw-post-body-paragraph lw lx iu ly b lz mp jv mb mc mq jy me lj mr mg mh ln ms mj mk lr mt mm mn mo in bi translated">另一方面，非参数方法更加灵活，因为它们能够“考虑”函数<em class="mu"> f </em>更多可能的形状。</p></div><div class="ab cl mv mw hy mx" role="separator"><span class="my bw bk mz na nb"/><span class="my bw bk mz na nb"/><span class="my bw bk mz na"/></div><div class="in io ip iq ir"><h2 id="5a36" class="la lb iu bd lc ld le dn lf lg lh dp li lj lk ll lm ln lo lp lq lr ls lt lu lv bi translated">模型可解释性</h2><p id="785e" class="pw-post-body-paragraph lw lx iu ly b lz ma jv mb mc md jy me lj mf mg mh ln mi mj mk lr ml mm mn mo in bi translated">当我们估计一个未知函数<em class="mu"> f </em>时，我们最感兴趣的是执行<a class="ae kz" rel="noopener" target="_blank" href="/inference-vs-prediction-b719da908000">推理或预测</a>(有时两者都有)。</p><p id="6ff9" class="pw-post-body-paragraph lw lx iu ly b lz mp jv mb mc mq jy me lj mr mg mh ln ms mj mk lr mt mm mn mo in bi translated">当在推理设置中工作时，我们通常将模型视为白盒，因为我们试图理解输入X和输出变量y之间的关系。<strong class="ly iv">不太灵活的方法往往更容易解释，因此更适合进行推理</strong>。</p><p id="ab37" class="pw-post-body-paragraph lw lx iu ly b lz mp jv mb mc mq jy me lj mr mg mh ln ms mj mk lr mt mm mn mo in bi translated">另一方面，<strong class="ly iv">更灵活的方法</strong>(如支持向量机或Boosting)能够为未知函数<em class="mu"> f </em>估计更复杂的形状，但更难解释<strong class="ly iv">。这意味着这种方法可能不太适合推理设置。函数的形状越复杂，就越难理解预测变量(X)和目标变量y之间的关系</strong></p></div><div class="ab cl mv mw hy mx" role="separator"><span class="my bw bk mz na nb"/><span class="my bw bk mz na nb"/><span class="my bw bk mz na"/></div><div class="in io ip iq ir"><h2 id="4b0a" class="la lb iu bd lc ld le dn lf lg lh dp li lj lk ll lm ln lo lp lq lr ls lt lu lv bi translated">模型精度</h2><p id="6cd2" class="pw-post-body-paragraph lw lx iu ly b lz ma jv mb mc md jy me lj mf mg mh ln mi mj mk lr ml mm mn mo in bi translated">现在，如果我们对执行预测感兴趣，我们需要训练一个能够尽可能准确地预测目标变量的模型。在预测设置中，我们可以将模型视为黑盒，在某种意义上，我们并不真正关心估计的<em class="mu"> f </em>的形状，只要它能够导致准确的预测。</p><p id="2cd9" class="pw-post-body-paragraph lw lx iu ly b lz mp jv mb mc mq jy me lj mr mg mh ln ms mj mk lr mt mm mn mo in bi translated">在预测设置中，我们并不真正关心模型的可解释性，我们最感兴趣的是模型的性能。因此，更灵活(但更难解释)的方法是<strong class="ly iv">通常是</strong>(这是这里的关键词——在下一节中会有更多的介绍！)更适合这类用例。</p></div><div class="ab cl mv mw hy mx" role="separator"><span class="my bw bk mz na nb"/><span class="my bw bk mz na nb"/><span class="my bw bk mz na"/></div><div class="in io ip iq ir"><h2 id="f195" class="la lb iu bd lc ld le dn lf lg lh dp li lj lk ll lm ln lo lp lq lr ls lt lu lv bi translated">过度拟合</h2><p id="b30c" class="pw-post-body-paragraph lw lx iu ly b lz ma jv mb mc md jy me lj mf mg mh ln mi mj mk lr ml mm mn mo in bi translated">同样需要强调的是<strong class="ly iv">更灵活的模型并不能保证比相对不复杂的模型更好的结果</strong>。在某些情况下，与更灵活的模型相比，不太灵活的模型可能会产生模型性能。</p><p id="105d" class="pw-post-body-paragraph lw lx iu ly b lz mp jv mb mc mq jy me lj mr mg mh ln ms mj mk lr mt mm mn mo in bi translated">这是因为更灵活的模型容易出现一种叫做<strong class="ly iv">过度拟合</strong>的现象。当模型过于紧密地跟随训练数据的噪声和误差时，它是过度拟合的，因此它不能很好地推广到新的、看不见的数据点。</p></div><div class="ab cl mv mw hy mx" role="separator"><span class="my bw bk mz na nb"/><span class="my bw bk mz na nb"/><span class="my bw bk mz na"/></div><div class="in io ip iq ir"><h2 id="78f3" class="la lb iu bd lc ld le dn lf lg lh dp li lj lk ll lm ln lo lp lq lr ls lt lu lv bi translated">最后的想法</h2><p id="f311" class="pw-post-body-paragraph lw lx iu ly b lz ma jv mb mc md jy me lj mf mg mh ln mi mj mk lr ml mm mn mo in bi translated">在今天的文章中，我们讨论了机器学习环境中模型准确性和模型可解释性之间的权衡。</p><p id="6a12" class="pw-post-body-paragraph lw lx iu ly b lz mp jv mb mc mq jy me lj mr mg mh ln ms mj mk lr mt mm mn mo in bi translated">不太灵活的模型更容易解释，因此更适合我们最感兴趣的理解输入和输出之间关系的推理环境。另一方面，更灵活的模型更难解释，但结果可能更准确。</p><p id="3ed9" class="pw-post-body-paragraph lw lx iu ly b lz mp jv mb mc mq jy me lj mr mg mh ln ms mj mk lr mt mm mn mo in bi translated">根据我们正在处理的问题，我们可能必须选择最适合我们用例的模型。然而，我们应该记住，在大多数情况下，我们必须在模型准确性和模型可解释性之间找到最佳平衡点。</p></div><div class="ab cl mv mw hy mx" role="separator"><span class="my bw bk mz na nb"/><span class="my bw bk mz na nb"/><span class="my bw bk mz na"/></div><div class="in io ip iq ir"><p id="1b77" class="pw-post-body-paragraph lw lx iu ly b lz mp jv mb mc mq jy me lj mr mg mh ln ms mj mk lr mt mm mn mo in bi translated"><a class="ae kz" href="https://gmyrianthous.medium.com/membership" rel="noopener"> <strong class="ly iv">成为会员</strong> </a> <strong class="ly iv">阅读介质上的每一个故事。你的会员费直接支持我和你看的其他作家。</strong></p></div><div class="ab cl mv mw hy mx" role="separator"><span class="my bw bk mz na nb"/><span class="my bw bk mz na nb"/><span class="my bw bk mz na"/></div><div class="in io ip iq ir"><p id="676e" class="pw-post-body-paragraph lw lx iu ly b lz mp jv mb mc mq jy me lj mr mg mh ln ms mj mk lr mt mm mn mo in bi translated"><strong class="ly iv">你可能也会喜欢</strong></p><div class="nc nd gq gs ne nf"><a rel="noopener follow" target="_blank" href="/inference-vs-prediction-b719da908000"><div class="ng ab fp"><div class="nh ab ni cl cj nj"><h2 class="bd iv gz z fq nk fs ft nl fv fx it bi translated">推断和预测的区别是什么</h2><div class="nm l"><h3 class="bd b gz z fq nk fs ft nl fv fx dk translated">在统计学习的背景下讨论预测和推断的区别</h3></div><div class="nn l"><p class="bd b dl z fq nk fs ft nl fv fx dk translated">towardsdatascience.com</p></div></div><div class="no l"><div class="np l nq nr ns no nt kt nf"/></div></div></a></div></div><div class="ab cl mv mw hy mx" role="separator"><span class="my bw bk mz na nb"/><span class="my bw bk mz na nb"/><span class="my bw bk mz na"/></div><div class="in io ip iq ir"><div class="kk kl km kn gu nf"><a rel="noopener follow" target="_blank" href="/parametric-vs-non-parametric-methods-2cea475da1a"><div class="ng ab fp"><div class="nh ab ni cl cj nj"><h2 class="bd iv gz z fq nk fs ft nl fv fx it bi translated">机器学习中的参数方法与非参数方法</h2><div class="nm l"><h3 class="bd b gz z fq nk fs ft nl fv fx dk translated">讨论机器学习中参数方法和非参数方法的区别</h3></div><div class="nn l"><p class="bd b dl z fq nk fs ft nl fv fx dk translated">towardsdatascience.com</p></div></div><div class="no l"><div class="nu l nq nr ns no nt kt nf"/></div></div></a></div></div><div class="ab cl mv mw hy mx" role="separator"><span class="my bw bk mz na nb"/><span class="my bw bk mz na nb"/><span class="my bw bk mz na"/></div><div class="in io ip iq ir"><div class="kk kl km kn gu nf"><a rel="noopener follow" target="_blank" href="/supervised-vs-unsupervised-learning-bf2eab13f288"><div class="ng ab fp"><div class="nh ab ni cl cj nj"><h2 class="bd iv gz z fq nk fs ft nl fv fx it bi translated">监督与非监督学习</h2><div class="nm l"><h3 class="bd b gz z fq nk fs ft nl fv fx dk translated">讨论机器学习中监督学习、非监督学习和半监督学习的主要区别</h3></div><div class="nn l"><p class="bd b dl z fq nk fs ft nl fv fx dk translated">towardsdatascience.com</p></div></div><div class="no l"><div class="nv l nq nr ns no nt kt nf"/></div></div></a></div></div></div>    
</body>
</html>