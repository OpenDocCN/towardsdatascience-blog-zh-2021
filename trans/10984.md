# 深度学习优化理论—简介

> 原文：[https://towardsdatascience.com/deep-learning-optimization-theory-introduction-148b3504b20f?source=collection_archive---------16-----------------------#2021-10-26](https://towardsdatascience.com/deep-learning-optimization-theory-introduction-148b3504b20f?source=collection_archive---------16-----------------------#2021-10-26)

## 理解深度学习中的优化理论对于实现进步至关重要。这篇文章介绍了研究它的实验和理论方法。

在过去十年中，深度学习在学术界和工业界都蓬勃发展。几十年来众所周知难以解决的现实世界和学术问题，如计算机视觉、自然语言处理和玩游戏，现在正通过深度学习方法得到高度成功的解决。

然而，尽管深度学习取得了这些重大进步，但对其成功的理论理解仍然滞后。我们的理解范围从基于实验的传统智慧和直觉到对玩具问题的更深入分析，这些玩具问题并不类似于真实世界深度学习架构的真正复杂性。这篇博客旨在强调从挑战传统智慧的深度学习理论研究中获得的有趣经验。

一般来说，深度学习由以下三个支柱组成:

1.  **表达能力** —我们可以用神经网络表达什么功能？就一个神经网络所能代表的功能集而言，它对另一个神经网络的效率如何？现代神经网络存在哪些归纳偏差？
2.  **优化**——对于给定的任务，我们如何找到神经网络的最佳权重？能保证找到最好的吗？我们能多快做到？
3.  **推广** —为什么一个训练集的解决方案可以很好地推广到一个未知的测试集？我们能限制泛化误差吗？

这篇文章将重点讨论优化。假设读者熟悉 SGD、深度学习和一些优化背景。

**深度学习理论——优化**

凸函数的最优化被认为是数学中一个成熟的领域。因此，可以使用成熟的工具和理论来回答上一段中描述的优化问题。然而，复杂的非凸函数的优化很难分析。既然深度神经网络(是的，线性网络也是)的优化是非凸的，我们如何尝试回答这些问题呢？人们可能会寻找广泛的经验证据来证明 SGD 在现实世界问题上收敛于全局极小值。另一个可能会寻找一个严格的理论解释其收敛和条件，它发生了。由于这两种方法，实验的和理论的帮助推动了我们的知识，在这篇博文中，我将展示这两种方法的代表性作品。我要介绍的第一种方法是实验方法。

**DL 中的优化—一种实验方法**

Goodfellow 等人的作品“[定性表征神经网络优化问题](https://arxiv.org/pdf/1412.6544.pdf)”。艾尔。提出了一种简单的工具来解决上述针对特定实际网络和问题的优化问题。让我们假设你训练你的网络收敛到全局最优(训练损失接近零)。现在，取初始化时的权重和收敛时的权重，并简单地在初始权重和收敛权重之间的参数空间中的**线上的一系列点处评估您的损失函数。更正式地说:**

![](../Images/64cbe8b09942f96adddb69f8f9b9573e.png)

沿着θi(初始化时的权重)到θc(收敛时的权重)之间的线采样θa。

因为我们怀疑损失函数是高度非凸的，所以很难想象损失函数沿着这条线的行为。令人惊讶的是，在 MNIST 上训练的前馈神经网络上执行该实验揭示了以下行为:

![](../Images/1a1d38780a96787a045fc9af5e6d3b26.png)

在 MNIST 数据集上训练的具有 ReLU 激活的全连接网络上的线性插值实验。这表明目标函数在模型的初始参数和目标参数之间的线内是平滑的。图片[来源](https://arxiv.org/pdf/1412.6544.pdf)。

不直观地，我们观察到这条线上的损失函数是连续的单调递减函数。这表明，虽然损失函数本身是非凸的，但有一条“几乎凸的”路径通向收敛点。此外，其他架构的其他实验(CNN/RNN)揭示了类似的现象。请阅读[论文](https://arxiv.org/pdf/1412.6544.pdf)了解更多详情。如果你有兴趣自己尝试一下，可以用[**我为它实现**](https://github.com/OmriKaduri/nn-linear-path-experiments/blob/main/MNIST-Linear-Experiments.ipynb) 。

既然我们知道从初始权重到收敛存在一条平滑单调递减的路径，有人可能会问 SGD 遵循这条线性路径吗？为了回答这个问题，我们正在寻找一种方法来评估“如果 SGD 被投射到生产线上，它是如何沿着生产线前进的”。此外，在那些权重不在 1D 子空间上的情况下，我们想要评估“它们离线有多远”。我们用 **α** 表示沿线的距离，用 **β** 表示离该线的距离。在深入研究α和β的数学定义之前，请注意以下几点:

![](../Images/07c01f29983d4131becb8522e20b5a32.png)

水平轴— α(沿线性路径的进度— 0 表示初始，900 表示最终模型)。垂直轴— β(时间 **T** 时 SGD 重量与线性路径的距离)。图片[来源](https://arxiv.org/pdf/1412.6544.pdf)。

有趣的是，似乎随着 SGD 的发展，它有一个对称的发散和收敛模式。起初，它从它那里发散，然后在几乎完全对称的情况下，收敛回它那里。

让我们快速描述一下这些参数的数学。如果你只关心直觉，跳过下面的段落，或者如果你想自己运行这个实验，就参考 [**我的实现**](https://github.com/OmriKaduri/nn-linear-path-experiments/blob/main/MNIST-Linear-Experiments.ipynb) 。

让我们将 **u** 定义为从θi 指向θc 的单位向量，我们可以将θi 到θc 之间的线性路径描述为:θi + α(t)*u，其中α(t)是一个标量，表示“线上已经取得了多少进展”，定义为:

![](../Images/f1092688ea3885401cb128a45dddd978.png)

对于时间步长 t，计算 SGD 从初始化到收敛在直线上的投影进度。

现在我们有了α，它指定了我们沿着这条线前进的幅度。缺少的分量是离它的距离。让我们定义另一个单位向量 **v** ,它指向从θ_ t 到线上计划进度的方向。

![](../Images/49f13271a43f3628b790eeca75c39383.png)

对于时间戳 t，β是 SGD 的权重的范数(**距离**)，来自它们在从初始化到收敛的线路上的预计进度。

这就是重现上述实验所需的全部内容！

**这种现象适用于现代神经网络吗？**

今天回顾 2014 年的那些结果，在深度学习领域取得巨大进展后，人们可能会要求重新审视原始论文中观察到的现象。事实上， [Jonathan Frankle](https://arxiv.org/abs/2012.06898) 在 2020 年对现代神经网络架构和数据集进行了实验，发现情况可能不如简单数据集上的简单架构好。

![](../Images/cf0db0b2ded8a6d77d8ecee5a1676c57.png)

在 ImageNet 数据集上使用 ResNet-50 重新访问线性插值实验。从初始到收敛，损耗不再单调递减。图片[来源](https://arxiv.org/abs/2012.06898)。

令人惊讶的是，在现代 NN 架构中，他没有发现这种现象发生。更奇怪的是，他发现在 ImageNet 上使用 ResNet-50 进行的这个实验导致了上面显示的奇特观察结果。沿着初始权重到收敛之间的线，损失保持几乎相同的值，然后少量增加并下降到收敛。

**一种可能的解释(固执己见)**

郝力等人的作品[“可视化神经网络的损失景观](https://arxiv.org/abs/1712.09913)”。艾尔。提供了一种可视化损失函数曲率的方法。使用他们的可视化方法(我将在以后的帖子中使用！)他们探讨了不同的网络架构如何影响损失情况。令人惊讶的是，将跳跃连接添加到深度神经网络中，可以显著减少从混沌到“几乎凸起”的景观，如下图所示。

![](../Images/4c4f43e31ffafcc7f38239b25aed5d06.png)

对用 ResNet-50 观察到的现象的一种可能的(自以为是的)解释。如果我们假设 ResNet 的跳跃连接使损失函数“凸化”,它可以解释“在突然下降到全局最小值之前损失不变”的现象。[图片来源](https://arxiv.org/pdf/1712.09913.pdf)。

这一变化可能会弥合 Goodfellow 等人的现象。艾尔。发现于 2014 年，弗兰克尔发现于 2020 年。在图像的右侧，很明显，在初始化权重(假设在平坦区域的某处)和收敛权重之间有一个大的平坦插值区域。此外，在整个插值过程中，损失值变化不大。

总结一下**实验方法**，可以说它们给了我们一种关于优化问题难度的乐观感。这表明问题没有我们想象的那么难。我们如何严格证明(或反驳)这个假设？

**DL 中的优化—一种理论方法**

虽然实验方法产生了有助于我们将深度学习推向有趣的应用用途的传统智慧，但它并没有为我们所观察到的现象提供合理的解释，正如上一节中对现代架构的重新实验所看到的那样。具体来说，我将介绍一种方法，旨在更好地理解损失情况，从而了解其优化特征。另一种方法是理解梯度下降本身的轨迹，我将在以后的文章中介绍。

**亏损景观有多好看？**

Itay Safran 和 Ohad Shamir 的工作“[关于过度指定的神经网络](https://arxiv.org/pdf/1511.04210.pdf)中初始盆地的质量”，试图通过模拟(非凸)损失函数的几何结构来解决这个问题。这种方法的动机是，希望我们会发现**损失图**便于优化(更具体地说，是本地搜索算法，如 SGD)。我们之前看到的关于经验上存在的单调递减路径的观察鼓励研究人员解释这一现象。

具体来说，这项工作提供了以下定理:给定一个全连通网络，它在初始化和收敛时的权重，以及一个凸损失函数，存在一个从该初始化点开始并以相同损失值(作为收敛点)的**结束的**单调递减路径**。为了证明该定理，主要的重要假设是**全零预测器的损失小于初始化时的损失**。**

尽管现代神经网络不支持这种假设(由于权重初始化技术的最新进展)，但他们的结果非常有趣，因为它与我们之前看到的经验结果一致。他们证明，在他们的假设下，有一个“优化的简单方法”。

虽然为他们的定理提供充分的证明和微妙的细节肯定超出了一篇博客文章的范围，但让我们试着理解一下总体思路。他们证明中的主要方法是为从初始到收敛的连续路径中的每个点创建**，在单调递减路径中创建相应的点。**它们是如何构造对应点的？它们为平方损失的特殊情况提供了直觉:

![](../Images/0271ac3f76dc758a7e3fd4fa0c603edb.png)

对具有 **m** 个样本的数据集，在由权重 **W** 参数化的神经网络**N**上评估的平方损失。

由于该目标下神经网络的最后一层是线性的，请注意，最后一层乘以某个标量 **c** ，对应于乘以网络的输出。因此，给定一些权重 **W(λ)** (其中0≤ **λ** ≤1 定义了初始化到收敛之间的路径)，我们可以将最后一层乘以标量 c≥0，并接收相应的权重(用 W’表示)，目的是:

![](../Images/9cc3678ceea922369dcc3c4d67959139.png)

**的相应目标 W(λ)** 通过将其最后一层乘以标量 **c.**

注意，对于 c=0，这个目标是 Y 的平均值。对于 c=∞，它也是∞的。它基本上是说(利用[中值定理](https://en.wikipedia.org/wiki/Intermediate_value_theorem))我们可以调 c 得到从 Y 的均值到∞的任意客观值。现在剩下的就是为每个 W(λ)找到它对应的 c，这样我们就得到期望的单调递减路径！

**视觉回顾**

理解这个想法可能很有挑战性。为了让你更容易理解，我制作了(使用伟大的 [manim](https://github.com/ManimCommunity/manim) 工具，源自 [3B1B](https://github.com/3b1b/manim) )一个简短的 GIF 来说明这个证明的想法:

从初始化到收敛，从连续路径构建相应的单调递减路径的可视化。视频来源:[我](https://omrikaduri.github.io/)。

可视化从蓝色曲线开始，表示从初始到收敛的连续但非凸的曲线。然后，它突出显示了“全零预测值假设”,并从中投射光线来构建单调递减路径。

**结论**

我们理解深度学习模型优化的第一步总体上是乐观的。以前的优化知识告诉我们，优化高维非线性和非凸问题是一个众所周知的难题。然而，在真实世界模型上的实验(FC/CNN/RNN)提供了乐观的结果。此外，对损失情况的理论分析也很乐观。在某种意义上，这两种方法(实验的和理论的)都表明，我们用 SGD 在深度学习中解决的问题可能比人们想象的要容易得多。

然而，跳到 2020 年，我们观察到用现代架构和数据集重新审视这些实验揭示了不同的现象。虽然从实践者的角度来看，这些早期的发现是令人鼓舞的，但是仍然需要 SGD 收敛背后的理论。幸运的是，研究人员近年来取得了显著的成果，其中一些采用了不同的景观方法，我将在未来的帖子中进行回顾。

**确认**

我要感谢 [Nadav Cohen](https://www.cohennadav.com/) 在特拉维夫大学教授了一门关于深度学习理论的伟大课程。