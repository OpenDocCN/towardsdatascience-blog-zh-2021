<html>
<head>
<title>Intuitive Maths and Code behind Self-Attention Mechanism of Transformers</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">变压器自我关注机制背后的直观数学和代码</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/intuitive-maths-and-code-behind-self-attention-mechanism-of-transformers-for-dummies-7dfc28a30aaa?source=collection_archive---------12-----------------------#2021-05-13">https://towardsdatascience.com/intuitive-maths-and-code-behind-self-attention-mechanism-of-transformers-for-dummies-7dfc28a30aaa?source=collection_archive---------12-----------------------#2021-05-13</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="724f" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">这篇博文将深入关注机制的本质细节，并使用python从头开始创建一个关注机制</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/458b5df08f5ec30387a13bbd840eb5c2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Sy8nZbVHBBixWIBZWHNuCg.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">罗伯特·卡茨基在<a class="ae kv" href="https://unsplash.com/s/visual/fceccb05-6ab7-4e4e-afa9-bf4fa2cb4427?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="8060" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在开始这篇博文之前，我强烈推荐你访问我之前关于变形金刚概述的博文。为了充分利用这个博客，请按以下顺序查看我以前的博客。</p><ol class=""><li id="c13a" class="ls lt iq ky b kz la lc ld lf lu lj lv ln lw lr lx ly lz ma bi translated"><a class="ae kv" href="https://machinelearningmarvel.in/transformers-you-just-need-attention/" rel="noopener ugc nofollow" target="_blank"> <strong class="ky ir">变形金刚——你只需要关注。</strong> </a></li><li id="6b58" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated"><a class="ae kv" href="https://machinelearningmarvel.in/intuitive-maths-and-code-behind-self-attention-mechanism-of-transformers-for-dummies/" rel="noopener ugc nofollow" target="_blank"> <strong class="ky ir">变形金刚自我关注机制背后的直观数学和代码。</strong>T11】</a></li><li id="4b3b" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated"><a class="ae kv" href="https://machinelearningmarvel.in/concepts-about-positional-encoding-you-might-not-know-about/" rel="noopener ugc nofollow" target="_blank"> <strong class="ky ir">关于位置编码的概念你可能不知道。</strong>T15】</a></li></ol><p id="d44d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这篇博文将深入关注机制的本质细节，并使用python从头开始创建一个关注机制。代码和直观的数学解释密不可分。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mg"><img src="../Images/555b31ac3581a9d4567f32f9ee22352c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RNijjCpNQCBBQyxu3jd31Q.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><a class="ae kv" href="https://arxiv.org/abs/1706.03762" rel="noopener ugc nofollow" target="_blank">变压器编码器部分</a></p></figure><p id="1ceb" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们要学什么？</p><ol class=""><li id="7ff1" class="ls lt iq ky b kz la lc ld lf lu lj lv ln lw lr lx ly lz ma bi translated">注意机制概念</li><li id="0b30" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">自我注意机制所涉及的步骤(直观的数学理论和代码)</li></ol><ul class=""><li id="d59c" class="ls lt iq ky b kz la lc ld lf lu lj lv ln lw lr mh ly lz ma bi translated">输入预处理</li><li id="b4ca" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr mh ly lz ma bi translated">查询、键和值矩阵的角色</li><li id="df01" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr mh ly lz ma bi translated">标度注意分数的概念</li></ul><p id="d776" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">3.多头注意力机制</p><p id="d6aa" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">所以没有任何进一步的拖延，让我们开始吧。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="mi mj l"/></div></figure><h1 id="520f" class="mk ml iq bd mm mn mo mp mq mr ms mt mu jw mv jx mw jz mx ka my kc mz kd na nb bi translated"><strong class="ak">注意机制概念</strong></h1><p id="60fb" class="pw-post-body-paragraph kw kx iq ky b kz nc jr lb lc nd ju le lf ne lh li lj nf ll lm ln ng lp lq lr ij bi translated">正如上一篇文章中所讨论的，当一个句子通过注意机制时会发生什么。例如，假设我们有一个句子“<strong class="ky ir">他拿起帽子并仔细检查了它</strong>”，注意机制通过记住每个单词如何与句子中的其他单词相关来创建每个单词的表示(嵌入)。在上面的句子中，注意机制对句子的理解达到了可以将单词“<strong class="ky ir">it”</strong>与“<strong class="ky ir">hat”</strong>联系起来，而不能与“<strong class="ky ir">He”</strong>联系起来。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nh"><img src="../Images/fa0099ba5d9d4645086c5314e9674e98.png" data-original-src="https://miro.medium.com/v2/resize:fit:1292/format:webp/1*KLpqs-02YDJU8Yn0uOjM5Q.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">A</p></figure><h1 id="b487" class="mk ml iq bd mm mn mo mp mq mr ms mt mu jw mv jx mw jz mx ka my kc mz kd na nb bi translated">自我注意机制涉及的步骤</h1><h2 id="95f8" class="ni ml iq bd mm nj nk dn mq nl nm dp mu lf nn no mw lj np nq my ln nr ns na nt bi translated">1.以正确的格式获取输入:-</h2><p id="9a14" class="pw-post-body-paragraph kw kx iq ky b kz nc jr lb lc nd ju le lf ne lh li lj nf ll lm ln ng lp lq lr ij bi translated">我们现在都知道文本输入不适合被转换器/计算机解释。因此，我们用数字向量来表示文本中的每个单词。让我们为句子创建嵌入，例如:- <strong class="ky ir">“这是本书”</strong>，并且让我们假设嵌入维度为5。所以对于每个单词，我们有一个长度为5的向量，如下所示。</p><h2 id="d25e" class="ni ml iq bd mm nj nk dn mq nl nm dp mu lf nn no mw lj np nq my ln nr ns na nt bi translated"><strong class="ak">输入到变压器</strong></h2><pre class="kg kh ki kj gt nu nv nw nx aw ny bi"><span id="fa4b" class="ni ml iq nv b gy nz oa l ob oc">print(f”Shape is :- {np.random.randn(3,5).shape}”)<br/>X=np.random.randn(3,5)<br/>X</span></pre><p id="35c1" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">输出:- </strong></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi od"><img src="../Images/2a1162182d2bb06b24018c051cc5a4bd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*t62ezk-tnn_fsS4X0s-KBg.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><strong class="bd oe">输出(</strong>作者图片)</p></figure><p id="96a9" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">从上面的输入矩阵，我们将创建一对新的矩阵，即关键，查询和值矩阵。矩阵在注意机制中起着至关重要的作用。让我们看看如何？</p><h2 id="7a2f" class="ni ml iq bd mm nj nk dn mq nl nm dp mu lf nn no mw lj np nq my ln nr ns na nt bi translated">2.获取查询、键和值矩阵</h2><p id="82d0" class="pw-post-body-paragraph kw kx iq ky b kz nc jr lb lc nd ju le lf ne lh li lj nf ll lm ln ng lp lq lr ij bi translated">首先，我们需要查询、键和值权重矩阵。目前，我们已经随机地初始化了它，但是实际上，像神经网络中的任何其他权重一样，这些是参数，并且是在训练过程中学习到的。最终使用最佳权重。假设这些权重是最佳权重，如代码所示。下图总结了我们在代码部分要做的事情</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi of"><img src="../Images/340a9f6a3cf51624ab2fbd24a5daad57.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*W4WNAxZMCmMneHUr.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">下面的代码片段<strong class="bd oe"> ( </strong>图片由作者提供)将遵循此步骤中涉及的整体操作</p></figure><h2 id="50bc" class="ni ml iq bd mm nj nk dn mq nl nm dp mu lf nn no mw lj np nq my ln nr ns na nt bi translated">查询矩阵的优化权重</h2><pre class="kg kh ki kj gt nu nv nw nx aw ny bi"><span id="3e5e" class="ni ml iq nv b gy nz oa l ob oc">weight_of_query=np.random.randn(5,3)<br/>weight_of_query</span></pre><p id="2c0a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">输出:- </strong></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi og"><img src="../Images/6b69725c0e1f679eedb9c809185618a3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1014/format:webp/1*0dyeuXpSK7djdJqaTuGuug.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><strong class="bd oe">输出(</strong>作者图片)</p></figure><h2 id="326f" class="ni ml iq bd mm nj nk dn mq nl nm dp mu lf nn no mw lj np nq my ln nr ns na nt bi translated">关键矩阵的优化权重</h2><pre class="kg kh ki kj gt nu nv nw nx aw ny bi"><span id="1d95" class="ni ml iq nv b gy nz oa l ob oc">weight_of_key=np.random.randn(5,3)<br/>weight_of_key</span></pre><p id="8c25" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">输出:-</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/61a9567aa91e80c51d2c66d139c50dd8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1056/format:webp/1*lUVMfYkLGJAhEn1IBaJ6Pw.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><strong class="bd oe">输出(</strong>图片由作者提供)</p></figure><h2 id="3bca" class="ni ml iq bd mm nj nk dn mq nl nm dp mu lf nn no mw lj np nq my ln nr ns na nt bi translated">价值矩阵的优化权重</h2><pre class="kg kh ki kj gt nu nv nw nx aw ny bi"><span id="bde7" class="ni ml iq nv b gy nz oa l ob oc">weight_of_values=np.random.randn(5,3)<br/>weight_of_values</span></pre><p id="6551" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">输出:-</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oi"><img src="../Images/4d10ce91e7553378404e30be3c48f271.png" data-original-src="https://miro.medium.com/v2/resize:fit:1282/format:webp/1*CCNa9zxXprUr-IN4UO8-eg.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><strong class="bd oe">输出(</strong>图片由作者提供)</p></figure><p id="fd00" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">然后，这些权重将乘以我们的输入矩阵(X ),这将给出我们最终的键、查询和值矩阵</p><h2 id="f91c" class="ni ml iq bd mm nj nk dn mq nl nm dp mu lf nn no mw lj np nq my ln nr ns na nt bi translated">关键矩阵的计算</h2><pre class="kg kh ki kj gt nu nv nw nx aw ny bi"><span id="d419" class="ni ml iq nv b gy nz oa l ob oc">Key=np.matmul(X,weight_of_key)<br/>Key</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oj"><img src="../Images/95041deae20bd8ec386ed2ebeac2a604.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4kCp_o9H08oo0T13LZYx2A.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><strong class="bd oe">输出(</strong>图片由作者提供)</p></figure><h2 id="490a" class="ni ml iq bd mm nj nk dn mq nl nm dp mu lf nn no mw lj np nq my ln nr ns na nt bi translated">查询矩阵的计算</h2><pre class="kg kh ki kj gt nu nv nw nx aw ny bi"><span id="190a" class="ni ml iq nv b gy nz oa l ob oc">Query=np.matmul(X,weight_of_query)<br/>Query</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ok"><img src="../Images/02735c48b7b018a608d21d2c9b39d82d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UjoFFLFgd2-t3Uc0qISZgg.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><strong class="bd oe">输出(</strong>图片由作者提供)</p></figure><h2 id="31be" class="ni ml iq bd mm nj nk dn mq nl nm dp mu lf nn no mw lj np nq my ln nr ns na nt bi translated">价值矩阵的计算</h2><pre class="kg kh ki kj gt nu nv nw nx aw ny bi"><span id="8046" class="ni ml iq nv b gy nz oa l ob oc">Values=np.matmul(X,weight_of_values)<br/>Values</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ol"><img src="../Images/cea3553b179542f48917ae38c19a3eb2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0TZPgFAFK1CV9rcTEyPenQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><strong class="bd oe">输出(</strong>图片由作者提供)</p></figure><p id="7fb2" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">查询、键和值矩阵中的第一行表示单词“<strong class="ky ir"> This </strong>”的查询、键和值向量，其他单词依此类推。到目前为止，查询、键和值矩阵可能没有多大意义。让我们看看自我关注机制是如何通过使用<strong class="ky ir">查询、键和值</strong>向量来发现每个单词如何与句子中的其他单词相关，从而创建每个单词的表示(嵌入)的。</p><h2 id="cb05" class="ni ml iq bd mm nj nk dn mq nl nm dp mu lf nn no mw lj np nq my ln nr ns na nt bi translated">3.标度注意力分数</h2><p id="5d92" class="pw-post-body-paragraph kw kx iq ky b kz nc jr lb lc nd ju le lf ne lh li lj nf ll lm ln ng lp lq lr ij bi translated">标度注意力分数的公式</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi om"><img src="../Images/a0809787e2709f82956a03a9066b84ef.png" data-original-src="https://miro.medium.com/v2/resize:fit:548/format:webp/0*IvN0iPZMRmJXrn5U.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">标度注意力公式</p></figure><pre class="kg kh ki kj gt nu nv nw nx aw ny bi"><span id="b120" class="ni ml iq nv b gy nz oa l ob oc">dimension=5<br/>Scores=np.matmul(Query,Key.T)/np.sqrt(dimension)<br/>Scores</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ol"><img src="../Images/62d1e50744297cb2d41995b4cf151cbe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*D3uo9K_XkzgbEtX8lRJEhA.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><strong class="bd oe">输出(</strong>图片由作者提供)</p></figure><p id="7688" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">Q.K(转置)中发生的是查询和键矩阵之间的点积，点积定义了相似性，如下图所示。</p><p id="6eb9" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">注:——下图中的数字都是为了便于解释而编造的，不相加。</strong></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi on"><img src="../Images/d1fde44f7d836dcd361466be85470f12.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*UOdiGuG4CvqM3q-U.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">我们如何用数学方法计算注意力分数？<strong class="bd oe"> ( </strong>图片由作者提供)</p></figure><p id="306a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">所以在查询向量<strong class="ky ir"> q1(This) </strong>和所有关键向量<strong class="ky ir"> k1(This)，k2(is)，k3(book) </strong>之间有一个点积。这个计算告诉我们查询向量<strong class="ky ir"> q1(This) </strong>如何关联/类似于密钥矩阵<strong class="ky ir"> k1(This)、k2(is)、k3(book)中的每个向量。</strong>同样，如果我们关注最终的输出矩阵，我们可以看到，如对角线矩阵所示，句子中的每个单词与其自身的关联度都高于其他任何单词。这是因为点积值更高。其次，单词<strong class="ky ir">“This”</strong>与<strong class="ky ir">“book”</strong>更相关，如上图中用红色突出显示的。如代码的最后一部分所示，我们将<strong class="ky ir"> Q.K(转置)</strong>除以<strong class="ky ir"> sqrt(维度)</strong>。这是一种标准化步骤，目的是使梯度稳定。</p><p id="a8fe" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">下面代码中的Softmax有助于将它设置在0到1的范围内，并分配概率值。</p><pre class="kg kh ki kj gt nu nv nw nx aw ny bi"><span id="4676" class="ni ml iq nv b gy nz oa l ob oc">from scipy.special import softmax<br/>Softmax_attention_scores=np.array([softmax(x) for x in Scores])<br/>Softmax_attention_scores</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oo"><img src="../Images/c0a10a3e88473d635b8482f08ff86b53.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-qQmUz4seOPZsr-SqU6zpw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><strong class="bd oe">输出(</strong>图片由作者提供)</p></figure><p id="ff28" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">上面的矩阵是中间softmax标度的注意力得分矩阵，其中每行对应于序列中每个单词的中间注意力得分/概率得分。它显示了每个单词与其他单词在概率上的关系。为了得到最终的注意力向量，我们将把上面的分数乘以价值矩阵并求和。总结出“这个”这个词对应的三个注意力向量。</p><p id="d382" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在下面的代码片段中，softmax_attention_scores[0][0]是该特定单词的权重，values[0]是对应于单词“<strong class="ky ir"> This </strong>”的值向量，以此类推。</p><pre class="kg kh ki kj gt nu nv nw nx aw ny bi"><span id="476b" class="ni ml iq nv b gy nz oa l ob oc">Softmax_attention_scores[0][0]*Values[0]+\<br/>Softmax_attention_scores[0][1]*Values[1]+\<br/>Softmax_attention_scores[0][2]*Values[2]</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi op"><img src="../Images/3679c4b01bbacb3345d554b8d5f0529c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sCy-H63AoR1Vk4s2WFQezA.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><strong class="bd oe">输出(</strong>图片由作者提供)</p></figure><p id="0c7f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">类似地，我们可以计算其他单词如is和book的关注度。这就是自我关注的机制。接下来，我们将研究多头注意机制，它的基本原理来自自我注意机制。</p><h1 id="be47" class="mk ml iq bd mm mn mo mp mq mr ms mt mu jw mv jx mw jz mx ka my kc mz kd na nb bi translated">多头注意力机制:-</h1><p id="9b46" class="pw-post-body-paragraph kw kx iq ky b kz nc jr lb lc nd ju le lf ne lh li lj nf ll lm ln ng lp lq lr ij bi translated">简而言之，多头注意力机制只不过是多个自我注意力机制串联在一起。如果我们将每个自我关注流/过程表示为一个<strong class="ky ir">头</strong>，那么我们将通过将所有自我关注机制串联在一起而得到一个多头关注机制。</p><p id="9982" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">当我们在即将到来的博客文章中动手操作时，我们会看到每个编码器的输出都有一个512的维度。总共有8个头。因此，所发生的是，每个自我注意模块被制作成使得它向(句子中的单词数，64)维矩阵给出输出。当所有这些维度被连接时，我们将看到最终的矩阵将是(句子中的单词数，(64*8)=512)维。最后一步是将连接的头部与权重矩阵相乘(假设权重矩阵已经在该过程中被训练过),这将是我们的多头注意力的输出。</p><p id="c6f5" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在我们的下一篇博文中，我们将讨论变形金刚的拥抱脸实现，再见。如果你觉得这很有帮助，请随意查看我关于变形金刚的其他博文</p><ol class=""><li id="c5ff" class="ls lt iq ky b kz la lc ld lf lu lj lv ln lw lr lx ly lz ma bi translated"><a class="ae kv" href="https://machinelearningmarvel.in/transformers-you-just-need-attention/" rel="noopener ugc nofollow" target="_blank"> <strong class="ky ir">变形金刚——你只需要关注。</strong>T3】</a></li><li id="a96f" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated"><a class="ae kv" href="https://machinelearningmarvel.in/intuitive-maths-and-code-behind-self-attention-mechanism-of-transformers-for-dummies/" rel="noopener ugc nofollow" target="_blank"> <strong class="ky ir">变形金刚自我关注机制背后的直观数学和代码。</strong> </a></li><li id="4bee" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated"><a class="ae kv" href="https://machinelearningmarvel.in/concepts-about-positional-encoding-you-might-not-know-about/" rel="noopener ugc nofollow" target="_blank"> <strong class="ky ir">关于位置编码你可能不知道的概念。</strong>T11】</a></li></ol><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="oq mj l"/></div></figure></div></div>    
</body>
</html>