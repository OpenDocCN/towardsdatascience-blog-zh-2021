<html>
<head>
<title>A hands-on demo of analyzing big data with Spark</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用Spark分析大数据的实践演示</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/a-hands-on-demo-of-analyzing-big-data-with-spark-68cb6600a295?source=collection_archive---------6-----------------------#2021-06-16">https://towardsdatascience.com/a-hands-on-demo-of-analyzing-big-data-with-spark-68cb6600a295?source=collection_archive---------6-----------------------#2021-06-16</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="540c" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsdatascience.com/tagged/making-sense-of-big-data" rel="noopener" target="_blank">理解大数据</a></h2><div class=""/><div class=""><h2 id="d6ff" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">扫描一部小说，计算圆周率，对五千万行进行回归</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/7a2d414e64a6a0ecbc2c00b05a10496c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*Z9f9uam0E9soLDq6"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">由<a class="ae lh" href="https://unsplash.com/@bertsz?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">伯特·布瑞尔</a>在<a class="ae lh" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>拍摄的照片</p></figure><p id="df72" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">云服务公司Domo估计，2020年，WhatsApp用户每分钟发送4170万条信息，网飞播放40.4万小时的视频，24万美元在Venmo上易手，6.9万人在LinkedIn上申请工作。在这些数据中，是那些公司用来理解现在、预测未来，并最终在高度竞争的市场中生存下来的模式。</p><p id="db7b" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">但是，如何从如此庞大的数据集中提取洞察力，以至于当你试图将它们加载到<code class="fe me mf mg mh b">pandas</code>时，你的笔记本电脑会死机？当一个数据集的行数超过美国家庭在50年内的收入中值[1]美元时，我们<em class="mi">可以</em>前往BestBuy.com，将他们的电脑按“最贵”排序，并支付一些现金购买一台昂贵的机器。</p><p id="3200" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><strong class="lk jd">或者我们可以试试</strong> <a class="ae lh" href="https://spark.apache.org/" rel="noopener ugc nofollow" target="_blank"> <strong class="lk jd">阿帕奇Spark </strong> </a> <strong class="lk jd">。</strong></p><p id="6e92" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">在这篇文章结束时，你会明白为什么你不需要昂贵的硬件来分析大规模数据集——因为你已经做到了。在统计一部小说中每个字母的出现频率、手工计算<em class="mi"> π </em>以及处理一个有5000万行的数据帧之前，我们会先讲一下什么是Spark。</p><h1 id="5a6c" class="mj mk it bd ml mm mn mo mp mq mr ms mt ki mu kj mv kl mw km mx ko my kp mz na bi translated">目录</h1><ol class=""><li id="34c1" class="nb nc it lk b ll nd lo ne lr nf lv ng lz nh md ni nj nk nl bi translated">大数据的分析框架</li><li id="d7dd" class="nb nc it lk b ll nm lo nn lr no lv np lz nq md ni nj nk nl bi translated">统计小说中的字母频率</li><li id="92fc" class="nb nc it lk b ll nm lo nn lr no lv np lz nq md ni nj nk nl bi translated">计算<em class="mi"> π </em></li><li id="2b44" class="nb nc it lk b ll nm lo nn lr no lv np lz nq md ni nj nk nl bi translated">火花数据帧和机器学习</li></ol><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi nr"><img src="../Images/d881b125f103370b42836a5cbc54dd93.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*8xEPzAG8DEi38is5"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">照片由<a class="ae lh" href="https://unsplash.com/@fabioha?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">法比奥</a>在<a class="ae lh" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><h1 id="7e87" class="mj mk it bd ml mm mn mo mp mq mr ms mt ki mu kj mv kl mw km mx ko my kp mz na bi translated">大数据的分析框架</h1><p id="d9f0" class="pw-post-body-paragraph li lj it lk b ll nd kd ln lo ne kg lq lr ns lt lu lv nt lx ly lz nu mb mc md im bi translated"><strong class="lk jd"> Spark是一个处理海量数据的框架。</strong>它的工作方式是<strong class="lk jd"> <em class="mi">将你的数据</em> </strong>划分成子集，<strong class="lk jd"> <em class="mi">将子集</em> </strong>分发到工作节点(无论它们是笔记本电脑上的<a class="ae lh" href="https://unix.stackexchange.com/questions/88283/so-what-are-logical-cpu-cores-as-opposed-to-physical-cpu-cores" rel="noopener ugc nofollow" target="_blank">逻辑CPU核心</a>还是集群中的整个机器)，然后<strong class="lk jd"> <em class="mi">协调</em> </strong>工作节点来分析数据。本质上，Spark是一种“分而治之”的策略。</p><p id="a9d8" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">一个简单的类比有助于形象化这种方法的价值。假设我们想统计图书馆的藏书数量。“昂贵的计算机”方法将是教一些人尽可能快地数书，训练他们多年来在冲刺时准确计数。虽然看起来很有趣，但这种方法并没有多大用处——即使是奥林匹克短跑运动员也只能跑这么快，如果你的点书员受伤或决定转行，你就没那么幸运了！</p><p id="a5d3" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">与此同时，Spark方法是随机找100个人，给每个人分配图书馆的一个区域，让他们清点所在区域的书籍，然后把他们的答案加在一起。这种方法更具可扩展性，容错性更强，成本更低……而且看起来可能还很有趣。</p><p id="0e8c" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">Spark的主要数据类型是<a class="ae lh" href="https://sparkbyexamples.com/spark-rdd-tutorial/" rel="noopener ugc nofollow" target="_blank"> <strong class="lk jd">弹性分布式数据集(RDD) </strong> </a>。RDD是分布在许多地方的数据的抽象，就像实体“沃尔玛”是世界上数百万人的抽象一样。使用rdd感觉就像在内存中操作一个简单的数组，即使底层数据可能分布在多台机器上。</p><h2 id="f661" class="nv mk it bd ml nw nx dn mp ny nz dp mt lr oa ob mv lv oc od mx lz oe of mz iz bi translated">入门指南</h2><p id="f5e9" class="pw-post-body-paragraph li lj it lk b ll nd kd ln lo ne kg lq lr ns lt lu lv nt lx ly lz nu mb mc md im bi translated">Spark主要是用Scala编写的，但是也可以在Java、Python、R和SQL中使用。我们将使用<a class="ae lh" href="https://spark.apache.org/docs/latest/api/python/" rel="noopener ugc nofollow" target="_blank"> PySpark </a>，Spark的Python接口。要安装PySpark，请在终端中键入<code class="fe me mf mg mh b">pip install pyspark</code>。你可能还需要安装或更新Java。当您可以在终端中键入<code class="fe me mf mg mh b">pyspark</code>并看到类似这样的内容时，您就知道一切都已经设置好了。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="og oh l"/></div></figure><p id="6790" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">本文中的其余代码可以在Spark终端中运行，或者在单独的Python或Jupyter实例中运行。我将在Jupyter中运行剩余的代码，这样我们就可以使用非常方便的<code class="fe me mf mg mh b">%%timeit</code> <a class="ae lh" href="https://ipython.readthedocs.io/en/stable/interactive/magics.html" rel="noopener ugc nofollow" target="_blank"> IPython神奇命令</a>来测量代码块的速度。</p><p id="1eec" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">下面是一个小小的PySpark演示。我们首先手动定义<code class="fe me mf mg mh b">SparkSession</code>来启动到Spark的连接。(如果您在PySpark终端中，这已经为您完成了。)然后我们创建一个数组的RDD，可视化前两个数字，并打印出最大值。通过<code class="fe me mf mg mh b">.getNumPartitions</code>，我们看到Spark将我们的数组分配给了我机器上的八个逻辑核心。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="og oh l"/></div></figure><p id="e5c0" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">有了这些基础知识，我们就可以开始利用Spark处理大型数据集了。由于您可能没有任何TB或Pb大小的数据集可供分析，因此我们需要有点创造性。先说一本小说。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi oi"><img src="../Images/d394293a36eefd888695035562829771.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*nSqSk_wg1rWDkx6H"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">Arif Riyanto 在<a class="ae lh" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><h1 id="649d" class="mj mk it bd ml mm mn mo mp mq mr ms mt ki mu kj mv kl mw km mx ko my kp mz na bi translated">统计小说中的字母频率</h1><p id="96f8" class="pw-post-body-paragraph li lj it lk b ll nd kd ln lo ne kg lq lr ns lt lu lv nt lx ly lz nu mb mc md im bi translated"><a class="ae lh" href="https://www.gutenberg.org/" rel="noopener ugc nofollow" target="_blank">古腾堡计划</a>是一个公共领域<a class="ae lh" href="https://fairuse.stanford.edu/overview/public-domain/welcome/" rel="noopener ugc nofollow" target="_blank">书籍的在线知识库</a>，所以我们可以从那里提取一本书进行分析。让我们读一读列夫·托尔斯泰的《战争与和平》——我一直想读一读，或者至少知道字母表中每个字母出现的频率！[3]</p><p id="a414" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">下面我们用<a class="ae lh" href="https://www.crummy.com/software/BeautifulSoup/bs4/doc/" rel="noopener ugc nofollow" target="_blank">美汤</a> Python库从小说的网页中获取HTML，整理段落，然后追加到一个列表中。然后我们删除第一个<em class="mi"> 383 </em>段落，它们只是目录！我们剩下11，186个段落，从4个字符到4381个字符不等。(那串一类的人物，不过用<em class="mi">战争与和平</em>，也许小说人物也是如此。)</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="og oh l"/></div></figure><p id="616b" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">尽管<em class="mi">战争与和平</em>是一部大型小说，但我们看到<code class="fe me mf mg mh b">pandas</code>仍然可以毫无问题地处理高级指标——line 38在我的笔记本电脑上几乎可以立即运行。</p><p id="8e51" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">但是当我们开始问更难的问题时，比如整本书中每个字母的出现频率，我们会开始注意到Spark的性能有了实质性的提高。这是因为<strong class="lk jd">这些段落可以彼此独立地处理</strong>；Spark会一次处理几个段落，而Python和<code class="fe me mf mg mh b">pandas</code>会逐个处理。</p><p id="a114" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">像以前一样，我们开始我们的Spark会话，并为我们的段落创建一个RDD。我们还加载了<code class="fe me mf mg mh b">Counter</code>，一个为计数而优化的内置Python类，以及<code class="fe me mf mg mh b">reduce</code>，稍后我们将使用它来演示基本的Python方法。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="og oh l"/></div></figure><p id="7fea" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">然后<a class="ae lh" href="https://en.wikipedia.org/wiki/Map_(higher-order_function)" rel="noopener ugc nofollow" target="_blank">用<code class="fe me mf mg mh b">rdd.map(Counter)</code>把<code class="fe me mf mg mh b">Counter</code>命令</a>映射到每一段。请注意，除非我们添加<code class="fe me mf mg mh b">.take(2)</code>来输出前两个结果，否则什么都不会发生——Spark执行<a class="ae lh" href="https://en.wikipedia.org/wiki/Lazy_evaluation" rel="noopener ugc nofollow" target="_blank">惰性评估</a>，这是一种优化查询链的方法，直到实际需要返回一个结果。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="og oh l"/></div></figure><p id="6f9f" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><code class="fe me mf mg mh b">rdd.map(Counter)</code>为我们提供了一个新的RDD，其中包含每一段的字母频率，但我们实际上想要整本书的字母频率。幸运的是，我们可以通过简单地将<code class="fe me mf mg mh b">Counter</code>对象加在一起来做到这一点。</p><p id="8642" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我们使用<code class="fe me mf mg mh b">.reduce</code>方法执行从多元素RDD到单个输出的<a class="ae lh" href="https://en.wikipedia.org/wiki/Fold_(higher-order_function)" rel="noopener ugc nofollow" target="_blank">缩减</a>，传递一个匿名加法函数来指定如何折叠RDD。[4]结果是一个<code class="fe me mf mg mh b">Counter</code>对象。然后，我们通过使用它的<code class="fe me mf mg mh b">.most_common</code>方法打印出十个最常见的字符来完成我们的分析。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="og oh l"/></div></figure><p id="e2fc" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">获胜者是…太空！这是相同的频率，但在一个很好的柱状图中。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/eb5c0a8ec0089ca4e61b1104d1b458f7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OL9yzTB0pqQYtIrdiGHM3Q.png"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">作者图片</p></figure><p id="2767" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">使用Spark值得吗？让我们通过计时我们的任务在Spark和基本Python中运行的时间来结束这一部分。我们可以使用<code class="fe me mf mg mh b">%%timeit</code> <a class="ae lh" href="https://ipython.readthedocs.io/en/stable/interactive/magics.html" rel="noopener ugc nofollow" target="_blank"> IPython magic命令</a>在<em class="mi">分离</em> Jupyter笔记本单元格，看看我们的方法如何比较。</p><p id="b80f" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">在Spark中:</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="og oh l"/></div></figure><p id="e2d6" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">在基本Python中:</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="og oh l"/></div></figure><p id="529a" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">使用Spark，我们比使用基本Python快67.7%。太棒了。现在我需要决定如何利用这多出来的半秒钟空闲时间。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ok"><img src="../Images/69e45426ccfe2f893d5c1dab3fb1fab0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*lX5eAz6eeJM4Ccu8"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">照片由<a class="ae lh" href="https://unsplash.com/@priscilladupreez?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">普里西拉·杜·普里兹</a>在<a class="ae lh" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>拍摄</p></figure><h1 id="5a8c" class="mj mk it bd ml mm mn mo mp mq mr ms mt ki mu kj mv kl mw km mx ko my kp mz na bi translated">计算pi</h1><p id="2d33" class="pw-post-body-paragraph li lj it lk b ll nd kd ln lo ne kg lq lr ns lt lu lv nt lx ly lz nu mb mc md im bi translated">关于如何使用随机数计算圆周率，有很多很棒的教程。简单的总结就是我们在(0，0)和(1，1)之间生成随机的x-y坐标，然后计算那些落在半径为1的圆内的点的比例。然后，我们可以通过将这个比例乘以4来求解<em class="mi"> π </em>。在下面10，000个点的可视化中，我们将蓝色点的数量除以总点数，得到<em class="mi"> π </em> /4。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ol"><img src="../Images/70836738e710e2e89a0c49286fdd59f8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-T7BUVJTtK1euUkj8uDI_w.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">作者图片</p></figure><p id="8fd0" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我们生成的点越多，我们对<em class="mi"> π </em>的估计就越准确。这是Spark的理想用例，因为生成的点都是独立的。与其分析预先存在的数据，不如使用我们的worker节点<em class="mi">分别生成数千个点</em>和<em class="mi">来计算落在圆内的那些点的比例。</em>然后我们可以取我们的比例的平均值，作为我们对<em class="mi"> π </em>的最终估计。</p><p id="acbc" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">这是我们让每个工人运行的函数。我试图在1)每次生成一个点(内存低，但速度慢)和2)一次生成所有样本，然后计算比例(有效，但可能达到内存限制)之间取得平衡。我发现的一个体面的解决方案是将<code class="fe me mf mg mh b">n_points</code>分成几个<em class="mi">块</em>，计算每个块在圆内的点数比例，然后在最后得到比例的平均值。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="og oh l"/></div></figure><p id="7b26" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">现在让我们创建一个RDD并将<code class="fe me mf mg mh b">calculate_pi</code>映射到每个元素，然后取每个元素的估计值<em class="mi"> π </em>的平均值。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="og oh l"/></div></figure><p id="d830" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我们对圆周率的估计还不算太差！再次使用<code class="fe me mf mg mh b">%%timeit</code>,我们看到在我的机器上，基本Python需要大约3.67秒，而Spark需要0.95秒，提高了74%。不错！</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi om"><img src="../Images/799cc8d0b6374f42a64c7a0907761b67.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*cPdCD40fD3W5tyzN"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">维克多·加西亚在<a class="ae lh" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><h1 id="3384" class="mj mk it bd ml mm mn mo mp mq mr ms mt ki mu kj mv kl mw km mx ko my kp mz na bi translated">火花数据帧和机器学习</h1><p id="32dd" class="pw-post-body-paragraph li lj it lk b ll nd kd ln lo ne kg lq lr ns lt lu lv nt lx ly lz nu mb mc md im bi translated">让我们再做一个例子，这次使用Spark在rdd之上提供的一个很好的抽象。在类似于<code class="fe me mf mg mh b">pandas</code>的语法中，我们可以使用<a class="ae lh" href="https://spark.apache.org/docs/latest/sql-programming-guide.html" rel="noopener ugc nofollow" target="_blank"> Spark数据帧</a>对太大而不适合<code class="fe me mf mg mh b">pandas</code> df的数据执行操作。然后我们可以用我们的数据框架训练机器学习模型。</p><h2 id="ffd0" class="nv mk it bd ml nw nx dn mp ny nz dp mt lr oa ob mv lv oc od mx lz oe of mz iz bi translated">火花数据帧</h2><p id="216d" class="pw-post-body-paragraph li lj it lk b ll nd kd ln lo ne kg lq lr ns lt lu lv nt lx ly lz nu mb mc md im bi translated">您可能没有一个有几百万行的数据帧，所以我们需要生成一个。因为根据定义，我们试图创建一个太大而不适合<code class="fe me mf mg mh b">pandas</code>的数据集，所以我们需要分段生成数据帧，反复保存CSV以便稍后用PySpark接收。</p><p id="3211" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我们做5000万行吧，只是为了好玩。我们将生成50个CSV，每个CSV有1，000，000行。我们的数据将包括四名学生的考试成绩，以及他们前一天花在学习和跳舞上的时间。这些是致力于大数据的学生，他们每人将参加大约1250万次考试！</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="og oh l"/></div></figure><p id="1357" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我们现在将从CSV创建一个Spark数据帧，可视化模式，并打印出行数。但是在定义Spark会话时，我们首先需要添加一个配置选项——为了运行下面的代码块，我需要将<a class="ae lh" href="https://researchcomputing.princeton.edu/computational-hardware/hadoop/spark-memory" rel="noopener ugc nofollow" target="_blank">驱动程序内存</a>加倍，以避免Spark崩溃！我们将使用<code class="fe me mf mg mh b">.config('spark.driver.memory', '2g')</code>线路来实现这一点。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="og oh l"/></div></figure><p id="9e83" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">现在让我们分析我们的数据。我们将从计算每个人的行数开始。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="og oh l"/></div></figure><p id="f3ff" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">各1250万次考试……不可思议。现在让我们找出每个人学习时间、跳舞时间和考试成绩的平均值。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="og oh l"/></div></figure><p id="9582" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">不出所料，人与人之间没有差异，因为我们在生成数据时没有指定任何差异。平均值也正好在数据指定范围的中间:0-8代表小时，0-100代表分数。</p><p id="c537" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">为了使这些值更好更全面，我们实际上需要创建一个<a class="ae lh" href="https://docs.databricks.com/spark/latest/spark-sql/udf-python.html" rel="noopener ugc nofollow" target="_blank">用户定义函数(UDF) </a>。对于我们可以应用于groupby操作的函数，我们将加载<code class="fe me mf mg mh b">pandas_udf</code>和<code class="fe me mf mg mh b">PandasUDFType</code>。我们还导入了<code class="fe me mf mg mh b">DoubleType</code>，它引用了函数返回值的数据类型。在定义了我们的函数之后，我们使用<code class="fe me mf mg mh b">spark.udf.register</code>使它对我们的Spark环境可用。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="og oh l"/></div></figure><p id="6189" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">现在我们可以将它应用到我们的数据框架中。注意，由于我们使用了<code class="fe me mf mg mh b">.agg</code>，我们需要传入一个字典，其中包含我们想要应用UDF的列。我没有打出<code class="fe me mf mg mh b">{'study': 'round_mean', 'dance': 'round_mean', 'score': 'round_mean'}</code>，而是用了字典理解来装腔作势。虽然这可能是相同的击键次数…</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="og oh l"/></div></figure><p id="f694" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">干净多了！UDF是一个强大的工具，可以对PySpark中没有的数据进行计算。</p><h2 id="b800" class="nv mk it bd ml nw nx dn mp ny nz dp mt lr oa ob mv lv oc od mx lz oe of mz iz bi translated">机器学习</h2><p id="9b87" class="pw-post-body-paragraph li lj it lk b ll nd kd ln lo ne kg lq lr ns lt lu lv nt lx ly lz nu mb mc md im bi translated">最后，让我们对数据进行线性回归。在训练模型时，您可能习惯于将特征向量分布在数据帧的各个列中，每个特征一个。例如，在<code class="fe me mf mg mh b">sklearn</code>，我们可以直接在<code class="fe me mf mg mh b">score</code>列和<code class="fe me mf mg mh b">['study', 'dance']</code>列上安装模型。</p><p id="0788" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><strong class="lk jd">然而，Spark期望一行的整个特征向量驻留在一列中。</strong>因此，我们将使用<code class="fe me mf mg mh b">VectorAssembler</code>将我们的<code class="fe me mf mg mh b">study</code>和<code class="fe me mf mg mh b">dance</code>值转换成一个新的2元素向量的<code class="fe me mf mg mh b">features</code>列。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="og oh l"/></div></figure><p id="96ca" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">现在我们实际上符合我们的模型。我们将数据分成训练集和测试集，然后使用<code class="fe me mf mg mh b">LinearRegression</code>类找到<code class="fe me mf mg mh b">features</code>和<code class="fe me mf mg mh b">score</code>之间的关系。最后，在计算测试集上的<a class="ae lh" href="https://en.wikipedia.org/wiki/Root-mean-square_deviation" rel="noopener ugc nofollow" target="_blank"> RMSE </a>之前，我们可视化我们的系数及其p值。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="og oh l"/></div></figure><p id="f3e9" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">在我们完全伪造的数据中，看起来学习对考试成绩有相当大的影响，而舞蹈……没有那么大。有了这么多数据，我们的p值都是“零”也就不足为奇了——有了5000万行，所有这一切表明系数并不完全为零。看看RMSE:我们模型的预测平均比实际分数低6.12分！自动生成的数据总是那么干净漂亮。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi on"><img src="../Images/3dc796ec00efa24869f2380be72832c4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*Kc6Yvn9O-1NyWJYZ"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">照片由<a class="ae lh" href="https://unsplash.com/@ameenfahmy_?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Ameen Fahmy </a>在<a class="ae lh" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><h1 id="3a6e" class="mj mk it bd ml mm mn mo mp mq mr ms mt ki mu kj mv kl mw km mx ko my kp mz na bi translated">结论</h1><p id="7071" class="pw-post-body-paragraph li lj it lk b ll nd kd ln lo ne kg lq lr ns lt lu lv nt lx ly lz nu mb mc md im bi translated">这篇文章深入探讨了如何使用Spark处理大数据。我们从分布式计算的概述开始，然后计算托尔斯泰的<em class="mi">战争与和平</em>中每个字母的频率，用随机生成的数字估计<em class="mi"> π </em>，最后分析一个5000万行的Spark数据帧。这篇文章有望为你成为谷歌下一个炙手可热的数据工程师打下基础。</p><p id="915c" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">如果你有兴趣了解更多，兔子洞会更深！在分析方面，有针对网络应用的<a class="ae lh" href="https://www.knowledgehut.com/tutorials/apache-spark-tutorial/graph-processing-with-graphframes" rel="noopener ugc nofollow" target="_blank">图形处理</a>和针对连续数据流的<a class="ae lh" href="https://www.knowledgehut.com/tutorials/apache-spark-tutorial/continous-apps-structured-streaming" rel="noopener ugc nofollow" target="_blank">结构化流</a>。在工程方面，通过更好地<a class="ae lh" href="https://stackoverflow.com/questions/24622108/apache-spark-the-number-of-cores-vs-the-number-of-executors" rel="noopener ugc nofollow" target="_blank">配置驱动程序和工人的数量和大小</a>，有大量的优化可以挤出来——我们只是在一台机器上使用默认数量的工人节点，但是当<a class="ae lh" href="https://spark.apache.org/docs/latest/cluster-overview.html" rel="noopener ugc nofollow" target="_blank">在多台机器上运行Spark</a>时，情况会变得复杂得多。类似地，分配合适的内存量对于避免Spark崩溃或<a class="ae lh" href="http://site.clairvoyantsoft.com/understanding-resource-allocation-configurations-spark-application/" rel="noopener ugc nofollow" target="_blank">让其他应用程序挨饿</a>至关重要。</p><p id="5fdb" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">但是，一旦你掌握了Spark的细微差别，建立模型来发现Venmo上的欺诈行为，或者为网飞用户确定完美的下一场演出，或者帮助LinkedIn上的人找到下一份工作，都将是一小步。当你有工具去理解永无止境的数据时，这个世界就是你的了。</p><p id="9025" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">最好，<br/>马特</p><h1 id="2cea" class="mj mk it bd ml mm mn mo mp mq mr ms mt ki mu kj mv kl mw km mx ko my kp mz na bi translated">脚注</h1><h2 id="b3c1" class="nv mk it bd ml nw nx dn mp ny nz dp mt lr oa ob mv lv oc od mx lz oe of mz iz bi translated">1.介绍</h2><p id="683f" class="pw-post-body-paragraph li lj it lk b ll nd kd ln lo ne kg lq lr ns lt lu lv nt lx ly lz nu mb mc md im bi translated">根据<a class="ae lh" href="https://www.census.gov/library/publications/2020/demo/p60-270.html" rel="noopener ugc nofollow" target="_blank">Census.gov</a>的数据，2019年的家庭收入中值为68703美元。将这个乘以50得到343万，比我们在这篇文章中分析的一些数据要小。</p><h2 id="c322" class="nv mk it bd ml nw nx dn mp ny nz dp mt lr oa ob mv lv oc od mx lz oe of mz iz bi translated">2.大数据的分析框架</h2><p id="797a" class="pw-post-body-paragraph li lj it lk b ll nd kd ln lo ne kg lq lr ns lt lu lv nt lx ly lz nu mb mc md im bi translated">我不得不深入兔子洞去了解Spark具体将任务分配给什么硬件。您的计算机有几个具有独立处理能力的物理<a class="ae lh" href="https://www.computerhope.com/jargon/c/core.htm" rel="noopener ugc nofollow" target="_blank"> CPU内核</a>。举例来说，这就是你如何在YouTube播放时输入Word doc并翻阅照片。</p><p id="d279" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我们可以更进一步，尽管一个物理CPU内核可以通过两个或更多个<a class="ae lh" href="https://unix.stackexchange.com/questions/88283/so-what-are-logical-cpu-cores-as-opposed-to-physical-cpu-cores" rel="noopener ugc nofollow" target="_blank">“逻辑”内核</a>之间的<a class="ae lh" href="https://en.wikipedia.org/wiki/Hyper-threading" rel="noopener ugc nofollow" target="_blank">超线程</a>同时处理多个任务。逻辑内核充当独立的内核，每个内核处理自己的任务，但它们实际上只是同一个物理内核。诀窍在于，物理内核可以以令人难以置信的速度在逻辑内核之间切换，利用任务停机时间(例如，在你输入搜索词后等待YouTube发回数据)来挤进更多计算。</p><p id="8b85" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">当在本地机器上运行时，Spark将任务分配给计算机上的所有逻辑核心，除非您另外指定。默认情况下，Spark为每个内核留出<a class="ae lh" href="https://stackoverflow.com/questions/26562033/how-to-set-apache-spark-executor-memory" rel="noopener ugc nofollow" target="_blank"> 512 MB，并将数据平均分配给每个内核。可以用<code class="fe me mf mg mh b">sc.defaultParallelism</code>查看逻辑核的数量。</a></p><h2 id="dce6" class="nv mk it bd ml nw nx dn mp ny nz dp mt lr oa ob mv lv oc od mx lz oe of mz iz bi translated">3.统计小说中的字母频率</h2><p id="2138" class="pw-post-body-paragraph li lj it lk b ll nd kd ln lo ne kg lq lr ns lt lu lv nt lx ly lz nu mb mc md im bi translated">在这篇文章的早期草稿中，我自己动手为一部“小说”生成了文本。我看了一些<a class="ae lh" href="https://loremipsum.io/" rel="noopener ugc nofollow" target="_blank"><em class="mi">lorem ipsum</em></a>Python包，但是有点不一致；我发现了非常有趣的<a class="ae lh" href="https://baconipsum.com/json-api/" rel="noopener ugc nofollow" target="_blank"> Bacon Ipsum API </a>但是不想用几千段的请求淹没它。下面的代码使用随机字符串生成一部10万段的“小说”，或者说8.9倍的《战争与和平》只有区区11186段。原来写小说比我想象的要容易得多！</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="og oh l"/></div></figure><h2 id="16fc" class="nv mk it bd ml nw nx dn mp ny nz dp mt lr oa ob mv lv oc od mx lz oe of mz iz bi translated">4.统计小说中的字母频率</h2><p id="44f4" class="pw-post-body-paragraph li lj it lk b ll nd kd ln lo ne kg lq lr ns lt lu lv nt lx ly lz nu mb mc md im bi translated">您可以很容易地用更复杂的函数或者您已经提前定义的函数来减少rdd。但是对于很多大数据处理来说，操作通常非常简单——将元素加在一起，通过某个阈值进行过滤——所以在使用这些函数的一两次之外显式定义这些函数有点大材小用。</p></div></div>    
</body>
</html>