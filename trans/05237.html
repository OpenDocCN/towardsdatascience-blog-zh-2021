<html>
<head>
<title>Dissecting ML Models With NoPdb</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用NoPdb剖析ML模型</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/dissecting-ml-models-with-nopdb-6ff4651fb131?source=collection_archive---------19-----------------------#2021-05-08">https://towardsdatascience.com/dissecting-ml-models-with-nopdb-6ff4651fb131?source=collection_archive---------19-----------------------#2021-05-08</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="844f" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsdatascience.com/tagged/hands-on-tutorials" rel="noopener" target="_blank">实践教程</a></h2><div class=""/><div class=""><h2 id="7bdf" class="pw-subtitle-paragraph jw iz iq bd b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn dk translated">编程Python调试器NoPdb指南，以及可视化视觉转换器(ViT)注意力的应用</h2></div><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ko"><img src="../Images/2d6972628aa4420b82b11f6598ecb894.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kbJO3yJaKEach6PBQHfhtw.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">图片作者。</p></figure><p id="d67d" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">调试机器学习模型与调试“传统”代码非常不同。在深度神经网络中，我们必须处理大的特征图和权重矩阵，这些通常看起来没有意义。随着<em class="ma"> ML可解释性</em>的日益重要，已经设计出了几种分析这些内部表示的方法，但是在实践中，获得它们并不总是简单明了的。像<a class="ae mb" href="https://pymotw.com/3/pdb/index.html" rel="noopener ugc nofollow" target="_blank"> Pdb </a>这样的经典调试器可能会有所帮助，但是用它来进行可视化和分析至少是不方便的。</p><p id="2773" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">虽然PyTorch等一些框架通过允许将<a class="ae mb" href="https://pytorch.org/tutorials/beginner/former_torchies/nnft_tutorial.html#forward-and-backward-function-hooks" rel="noopener ugc nofollow" target="_blank">挂钩</a>附加到网络的层来解决这一问题，但这仅适用于我们感兴趣的功能作为特定层的<em class="ma">输入</em>或<em class="ma">输出</em>的情况。如果我们想要访问在某个函数中只作为<em class="ma">局部变量</em>可用的信息——例如在现在无处不在的变形金刚的许多实现中的<em class="ma">注意力权重</em>——我们就不走运了。</p><p id="f2fb" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">还是我们？</p><h1 id="fb6f" class="mc md iq bd me mf mg mh mi mj mk ml mm kf mn kg mo ki mp kj mq kl mr km ms mt bi translated">认识NoPdb</h1><p id="fc25" class="pw-post-body-paragraph le lf iq lg b lh mu ka lj lk mv kd lm ln mw lp lq lr mx lt lu lv my lx ly lz ij bi translated"><a class="ae mb" href="https://github.com/cifkao/nopdb" rel="noopener ugc nofollow" target="_blank"> <strong class="lg ja"> NoPdb </strong> </a>(声明:我是作者)是一个<strong class="lg ja">非交互式Python调试器</strong>。与标准Python调试器Pdb不同，NoPdb没有交互式用户界面，但可以使用方便的上下文管理器对其进行<strong class="lg ja">编程</strong><em class="ma">—</em>——以在给定的代码段运行时执行特定的操作。例如，我们可以使用它轻松地从其他人的代码深处获取一个局部变量，并保存它以供以后分析，甚至可以动态地修改它的值，看看会发生什么。</p><p id="2796" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">我们将在这里使用的基本功能由<code class="fe mz na nb nc b">nopdb.capture_call()</code>和<code class="fe mz na nb nc b">nopdb.capture_calls()</code>提供(参见<a class="ae mb" href="https://nopdb.readthedocs.io/en/stable/getting-started.html#capturing-function-calls" rel="noopener ugc nofollow" target="_blank">文档</a>)。这些上下文管理器允许捕获关于给定函数调用的有用信息，包括参数、局部变量、返回值和堆栈跟踪。一个更强大的上下文管理器是<code class="fe mz na nb nc b"><a class="ae mb" href="https://nopdb.readthedocs.io/en/stable/getting-started.html#setting-breakpoints" rel="noopener ugc nofollow" target="_blank">nopdb.breakpoint()</a></code>，当到达给定的代码行时，它允许执行用户定义的动作(例如计算表达式)。</p><h1 id="2e52" class="mc md iq bd me mf mg mh mi mj mk ml mm kf mn kg mo ki mp kj mq kl mr km ms mt bi translated">解剖视觉转换器</h1><p id="09eb" class="pw-post-body-paragraph le lf iq lg b lh mu ka lj lk mv kd lm ln mw lp lq lr mx lt lu lv my lx ly lz ij bi translated">为了查看NoPdb的运行情况，我们将把它应用到一个<a class="ae mb" href="https://arxiv.org/abs/2010.11929" rel="noopener ugc nofollow" target="_blank"> <strong class="lg ja">视觉转换器</strong> </a> ( <strong class="lg ja"> ViT </strong>)。ViT是最近提出的完全基于<a class="ae mb" href="https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)" rel="noopener ugc nofollow" target="_blank"> Transformer </a>架构的图像分类模型。如下图所示，其主要思想相当简单:将输入图像分割成小块，让每个小块通过一个线性层，然后对这个“小块嵌入”序列应用一个标准的Transformer编码器。为了进行分类，使用一个常见的技巧:我们在输入序列的开头添加一个特殊的<code class="fe mz na nb nc b">[class]</code>标记，并在编码器输出的相应(第一个)位置附加一个分类头(单层MLP)。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi nd"><img src="../Images/58dffa0445c2607956eed7b6026cc341.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uIe4yYUp3KtFUA3A8azPeA.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">视觉转换器(ViT)。作者绘图。</p></figure><p id="3ed6" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">虽然我们不太关心这篇文章中架构的细节，但我们确实需要知道模型的每一层都包含一个<em class="ma">注意机制</em>，它为<em class="ma">的每一对输入位置</em>(即图像块，加上<code class="fe mz na nb nc b">[class]</code>令牌)计算一个权重(一种相似性得分)。就像我们现在要做的，可视化这些权重可以给我们一个线索，告诉我们图像的哪些部分对模型最重要。</p><p id="9cb5" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">我们将使用来自<a class="ae mb" href="https://github.com/rwightman/pytorch-image-models" rel="noopener ugc nofollow" target="_blank"> pytorch-image-models </a>库的<code class="fe mz na nb nc b">timm</code>包中预先训练好的ViT，并且我们将跟踪<a class="ae mb" href="https://colab.research.google.com/github/cifkao/nopdb/blob/main/docs/pytorch_tutorial.ipynb" rel="noopener ugc nofollow" target="_blank">这个Colab笔记本</a>(笔记本最重要的部分都包含在这里)。</p><h2 id="2620" class="ne md iq bd me nf ng dn mi nh ni dp mm ln nj nk mo lr nl nm mq lv nn no ms iw bi translated">运行ViT</h2><p id="734f" class="pw-post-body-paragraph le lf iq lg b lh mu ka lj lk mv kd lm ln mw lp lq lr mx lt lu lv my lx ly lz ij bi translated">让我们首先安装<code class="fe mz na nb nc b">timm</code>计算机视觉包，以及<a class="ae mb" href="https://github.com/cifkao/nopdb" rel="noopener ugc nofollow" target="_blank"> NoPdb </a>:</p><pre class="kp kq kr ks gt np nc nq nr aw ns bi"><span id="a80d" class="ne md iq nc b gy nt nu l nv nw">pip install timm==0.4.5 nopdb==0.1</span></pre><p id="0df9" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">加载预训练模型很容易:</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="nx ny l"/></div></figure><p id="0c35" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">现在，我们将加载一个图像并将其提供给模型。让我们试试这张我在瑞士拍的照片:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ko"><img src="../Images/632c24e51b27621ec1e831390fbb7252.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*s8swKnzyeRiSfced7c-bIw.jpeg"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">瑞士阿姆登的奶牛。作者照片。</p></figure><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="nx ny l"/></div></figure><p id="65f4" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">模型返回的是所有类的<em class="ma">逻辑值</em>(前softmax值)。在<a class="ae mb" href="https://colab.research.google.com/github/cifkao/nopdb/blob/main/docs/pytorch_tutorial.ipynb" rel="noopener ugc nofollow" target="_blank">笔记本</a>里，我写了一个小函数<code class="fe mz na nb nc b">predict()</code>，打印出<em class="ma">最有可能</em>的类及其<em class="ma">概率</em>。打电话给<code class="fe mz na nb nc b">predict(input)</code>给:</p><pre class="kp kq kr ks gt np nc nq nr aw ns bi"><span id="b820" class="ne md iq nc b gy nt nu l nv nw">alp     0.7936609983444214<br/>ox      0.1110275536775589<br/>valley  0.029854662716388702<br/>oxcart  0.008171545341610909<br/>ibex    0.008044715970754623</span></pre><h2 id="c1ef" class="ne md iq bd me nf ng dn mi nh ni dp mm ln nj nk mo lr nl nm mq lv nn no ms iw bi translated">视觉化注意力</h2><p id="f98f" class="pw-post-body-paragraph le lf iq lg b lh mu ka lj lk mv kd lm ln mw lp lq lr mx lt lu lv my lx ly lz ij bi translated">现在让我们看看模型的内部！ViT由12个<code class="fe mz na nb nc b">blocks</code>组成，每个包含一个<code class="fe mz na nb nc b">attn</code>层；这是计算注意力权重的地方:</p><pre class="kp kq kr ks gt np nc nq nr aw ns bi"><span id="207c" class="ne md iq nc b gy nt nu l nv nw">VisionTransformer(<br/>  (patch_embed): PatchEmbed(<br/>    (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))<br/>  )<br/>  (pos_drop): Dropout(p=0.0, inplace=False)<br/>  (<strong class="nc ja">blocks</strong>): <strong class="nc ja">ModuleList</strong>(<br/>    (0): Block(<br/>      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)<br/>      (<strong class="nc ja">attn</strong>): <strong class="nc ja">Attention</strong>(<br/>        (qkv): Linear(in_features=768, out_features=2304, bias=True)<br/>        (attn_drop): Dropout(p=0.0, inplace=False)<br/>        (proj): Linear(in_features=768, out_features=768, bias=True)<br/>        (proj_drop): Dropout(p=0.0, inplace=False)<br/>      )<br/>      (drop_path): Identity()<br/>      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)<br/>      (mlp): Mlp(<br/>        (fc1): Linear(in_features=768, out_features=3072, bias=True)<br/>        (act): GELU()<br/>        (fc2): Linear(in_features=3072, out_features=768, bias=True)<br/>        (drop): Dropout(p=0.0, inplace=False)<br/>      )<br/>    )<br/>    ...<br/>  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)<br/>  (pre_logits): Identity()<br/>  (head): Linear(in_features=768, out_features=1000, bias=True)<br/>)</span></pre><p id="78e5" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">假设我们想将注意力可视化在第5个区块，即<code class="fe mz na nb nc b">model.blocks[4]</code>。查看 <code class="fe mz na nb nc b"><a class="ae mb" href="https://github.com/rwightman/pytorch-image-models/blob/v0.4.5/timm/models/vision_transformer.py#L168-L180" rel="noopener ugc nofollow" target="_blank">Attention</a></code> <a class="ae mb" href="https://github.com/rwightman/pytorch-image-models/blob/v0.4.5/timm/models/vision_transformer.py#L168-L180" rel="noopener ugc nofollow" target="_blank">层</a>的<a class="ae mb" href="https://github.com/rwightman/pytorch-image-models/blob/v0.4.5/timm/models/vision_transformer.py#L168-L180" rel="noopener ugc nofollow" target="_blank">代码，我们可以发现一个名为<code class="fe mz na nb nc b">attn</code>的变量，它正是我们要寻找的注意力张量:</a></p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi nz"><img src="../Images/c8df10dffa6ff14b351ce4b32b9e40b8.png" data-original-src="https://miro.medium.com/v2/resize:fit:986/format:webp/1*VDANDWVQ_rT_gyqn5G7_rQ.png"/></div></figure><p id="b9a0" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">为了得到它的值，我们将使用<code class="fe mz na nb nc b">nopdb.capture_call()</code>上下文管理器来捕获对注意力层的<code class="fe mz na nb nc b">forward</code>方法的调用:</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="nx ny l"/></div></figure><p id="e3bc" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">瞧——<code class="fe mz na nb nc b">attn_call</code>对象现在包含了一堆关于调用的有用信息，包括所有局部变量的值！让我们看看它们是什么:</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="nx ny l"/></div></figure><p id="526f" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">检查<code class="fe mz na nb nc b">attn_call.locals['attn']</code>，我们可以看到它是一个形状为[1，12，197，197]的张量，其中1是批量大小，12是注意头的数量，197是图像块的数量+ 1用于<code class="fe mz na nb nc b">[class]</code>令牌(记住，注意机制为每对<em class="ma">位置</em>计算一个权重)。</p><p id="7502" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">我们可以用不同的方法来分析这些注意力矩阵，但为了简单起见，我选择直观地显示每个补丁平均获得了多少注意力(针对每个注意力头):</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="nx ny l"/></div></figure><p id="ffaf" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">(助手功能<code class="fe mz na nb nc b">plot_weights</code>，只是显示图像，并根据其权重调整每个面片的亮度，可以在<a class="ae mb" href="https://colab.research.google.com/github/cifkao/nopdb/blob/main/docs/pytorch_tutorial.ipynb" rel="noopener ugc nofollow" target="_blank">笔记本</a>中找到。)</p><p id="d007" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">调用<code class="fe mz na nb nc b">plot_attention(input, attn_call.locals[‘attn’][0])</code>为12个注意力头中的每一个产生一个图。以下是其中的一些:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi oa"><img src="../Images/a9e34687e6066b25f4463431404edddb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4qlVxzzXCzLYxsf1ZeyXNg.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">第五个变形金刚模块中的平均注意力权重(对比度增加以便更好地观看)。每个补丁越亮，注意力权重越高。图片作者。</p></figure><p id="782a" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">我们可以看到，一些头部倾向于主要关注图像中的特定对象，如奶牛(头部8)或天空(头部12)，一些头部到处看(头部2)，一些头部主要关注一个看似随机的斑块，如背景中的一座山的一部分(头部3)。</p><p id="d542" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">请记住，这只是一个有限的例子。我们可以通过使用<a class="ae mb" href="https://arxiv.org/abs/2005.00928" rel="noopener ugc nofollow" target="_blank"> <em class="ma">注意力卷展栏</em>或<em class="ma">注意力流</em> </a>更进一步，这是估计单个输入面片如何对输出做出贡献的更好方法，我们可以以几乎相同的方式利用NoPdb。</p><h2 id="284d" class="ne md iq bd me nf ng dn mi nh ni dp mm ln nj nk mo lr nl nm mq lv nn no ms iw bi translated">调整重量</h2><p id="43e7" class="pw-post-body-paragraph le lf iq lg b lh mu ka lj lk mv kd lm ln mw lp lq lr mx lt lu lv my lx ly lz ij bi translated">NoPdb可以做的另一件事是将代码<em class="ma">插入到函数中。这意味着我们不仅可以捕获变量，还可以修改它们！我选了一个有点傻的例子来说明这一点:我们将在所有层中使用前softmax注意力权重，并将它们乘以3。(这就像应用一个低的<em class="ma"> softmax温度</em>，使分布更加“峰值化”。)我们当然可以通过<em class="ma">编辑<code class="fe mz na nb nc b">timm</code>包的代码</em>来做到这一点，但是我们需要重新加载包和模型，这可能会很繁琐，特别是如果我们需要重复做的话。另一方面，NoPdb允许我们快速地进行更改，而不需要重新加载。</em></p><p id="2ee4" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">我们再来看看<code class="fe mz na nb nc b">Attention.forward()</code>:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi nz"><img src="../Images/c8df10dffa6ff14b351ce4b32b9e40b8.png" data-original-src="https://miro.medium.com/v2/resize:fit:986/format:webp/1*VDANDWVQ_rT_gyqn5G7_rQ.png"/></div></figure><p id="a911" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">我们希望将<code class="fe mz na nb nc b">attn = attn * 3</code>放在softmax之前，即第174行。我们将通过在这一行设置一个“断点”并让它执行这条语句来实现这一点。(注意，NoPdb断点实际上并不停止执行；相反，它只是执行一些我们给它的代码。)我们也要像之前一样捕捉第5块的局部变量。</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="nx ny l"/></div></figure><p id="d6a6" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">请注意，我们没有指定行号(174)，而是指定了该行的实际<em class="ma">代码</em>:<code class="fe mz na nb nc b">line='attn = attn.softmax(dim=-1)'</code>—这只是一个方便的特性，并且<code class="fe mz na nb nc b">line=174</code>(如同在传统调试器中一样)也可以工作。另外，请注意，由于我们将函数指定为<code class="fe mz na nb nc b">Attention.forward</code>(而不是<code class="fe mz na nb nc b">model.blocks[4].attn.forward</code>)，断点将在每个关注层的<em class="ma">处触发。</em></p><p id="0b2b" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">让我们看看这是如何改变预测的:</p><pre class="kp kq kr ks gt np nc nq nr aw ns bi"><span id="7dc9" class="ne md iq nc b gy nt nu l nv nw">balloon     0.2919192612171173<br/>alp         0.12357209622859955<br/>valley      0.049703165888786316<br/>parachute   0.0346514955163002<br/>airship     0.019190486520528793</span></pre><p id="6040" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">我们捕捉到的注意力模式:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi oa"><img src="../Images/7b6b2212702de916f7f272a26aea0a03.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rdcA09bAW7UMMyIa6qpkNg.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">和上面一样的情节，但是调整了注意力的权重。图片作者。</p></figure><h1 id="d177" class="mc md iq bd me mf mg mh mi mj mk ml mm kf mn kg mo ki mp kj mq kl mr km ms mt bi translated">关于其他框架的说明</h1><p id="f2c3" class="pw-post-body-paragraph le lf iq lg b lh mu ka lj lk mv kd lm ln mw lp lq lr mx lt lu lv my lx ly lz ij bi translated">虽然这篇文章关注的是PyTorch，但是NoPdb不依赖于任何特定的框架，可以用于任何Python 3代码。</p><p id="83f3" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">也就是说，一些框架，如TensorFlow或JAX，将模型编译成计算图或直接编译成计算内核，然后在Python之外执行，NoPdb无法访问。幸运的是，在大多数情况下，我们可以出于调试目的禁用该功能:</p><ul class=""><li id="d434" class="ob oc iq lg b lh li lk ll ln od lr oe lv of lz og oh oi oj bi translated">在TensorFlow 2.x中，我们可以在执行模型之前调用<code class="fe mz na nb nc b"><a class="ae mb" href="https://www.tensorflow.org/api_docs/python/tf/config/run_functions_eagerly" rel="noopener ugc nofollow" target="_blank">tf.config.run_functions_eagerly(True)</a></code>。(注意，这不适用于为TensorFlow 1.x编写的模型，这些模型被显式编译为图形。)</li><li id="b38e" class="ob oc iq lg b lh ok lk ol ln om lr on lv oo lz og oh oi oj bi translated">在JAX，我们可以使用<code class="fe mz na nb nc b"><a class="ae mb" href="https://jax.readthedocs.io/en/latest/jax.html#jax.disable_jit" rel="noopener ugc nofollow" target="_blank">disable_jit()</a></code>上下文管理器。</li></ul><h1 id="d176" class="mc md iq bd me mf mg mh mi mj mk ml mm kf mn kg mo ki mp kj mq kl mr km ms mt bi translated">一些NoPdb链接</h1><ul class=""><li id="b9ff" class="ob oc iq lg b lh mu lk mv ln op lr oq lv or lz og oh oi oj bi translated"><a class="ae mb" href="https://colab.research.google.com/github/cifkao/nopdb/blob/main/docs/pytorch_tutorial.ipynb" rel="noopener ugc nofollow" target="_blank"> Colab本帖笔记本</a></li><li id="4396" class="ob oc iq lg b lh ok lk ol ln om lr on lv oo lz og oh oi oj bi translated"><a class="ae mb" href="https://github.com/cifkao/nopdb" rel="noopener ugc nofollow" target="_blank"> GitHub库</a></li><li id="13c6" class="ob oc iq lg b lh ok lk ol ln om lr on lv oo lz og oh oi oj bi translated"><a class="ae mb" href="https://nopdb.readthedocs.io/en/stable/" rel="noopener ugc nofollow" target="_blank">单据</a></li></ul></div></div>    
</body>
</html>