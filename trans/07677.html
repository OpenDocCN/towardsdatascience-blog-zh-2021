<html>
<head>
<title>A Step-by-Step Guide to Speech Recognition and Audio Signal Processing in Python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Python中语音识别和音频信号处理的分步指南</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/a-step-by-step-guide-to-speech-recognition-and-audio-signal-processing-in-python-136e37236c24?source=collection_archive---------1-----------------------#2021-07-14">https://towardsdatascience.com/a-step-by-step-guide-to-speech-recognition-and-audio-signal-processing-in-python-136e37236c24?source=collection_archive---------1-----------------------#2021-07-14</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="9782" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">向机器教授人类词汇的科学</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/601a3cbd3b1bcb1740c90bd1b3c50392.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*82MXLrMIerUMbwrT"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">凯利·西克玛在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="93e9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">言语是人类交流的主要形式，也是理解行为和认知的重要组成部分。人工智能中的语音识别是一种部署在计算机程序上的技术，使它们能够理解所说的话。</p><blockquote class="lv"><p id="962a" class="lw lx it bd ly lz ma mb mc md me lu dk translated">声音和图像、视频一样，也是人类通过感觉器官感知的模拟信号。</p></blockquote><p id="41b2" class="pw-post-body-paragraph kz la it lb b lc mf ju le lf mg jx lh li mh lk ll lm mi lo lp lq mj ls lt lu im bi translated">对于机器来说，要消耗这些信息，需要将其存储为数字信号，并通过软件进行分析。从模拟到数字的转换包括以下两个过程:</p><ol class=""><li id="3a1e" class="mk ml it lb b lc ld lf lg li mm lm mn lq mo lu mp mq mr ms bi translated"><strong class="lb iu">采样:</strong>是用来将时变(随时间变化)信号s(t)转换为实数x(n)的离散级数的过程。采样周期(Ts)是一个术语，定义两个连续离散样本之间的间隔。采样频率(fs = 1/Ts)是采样周期的倒数。常见的采样频率为8 kHz、16 kHz和44.1 kHz。1 Hz的采样速率意味着每秒一个样本，因此高采样速率意味着更好的信号质量。</li><li id="ae0b" class="mk ml it lb b lc mt lf mu li mv lm mw lq mx lu mp mq mr ms bi translated"><strong class="lb iu">量化:</strong>这是将采样产生的每一个实数用一个近似值代替以获得有限精度(定义在一个比特范围内)的过程。在大多数情况下，每个样本16比特用于表示单个量化样本。因此，原始音频样本通常具有-215到215的信号范围，尽管在分析期间，为了更简单的验证和模型训练，这些值被标准化到范围(-1，1)。样本分辨率总是以每个样本的位数来衡量。</li></ol><p id="d12e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">通用语音识别系统旨在执行下述任务，并可轻松与<a class="ae ky" rel="noopener" target="_blank" href="/clean-architecture-of-analyzing-data-7e689da7dd4a">标准数据分析架构</a>相关联:</p><ol class=""><li id="de28" class="mk ml it lb b lc ld lf lg li mm lm mn lq mo lu mp mq mr ms bi translated">捕获人类给出的语音(单词、句子、短语)。你可以将此视为任何通用机器学习工作流程的数据采集部分。</li><li id="0a62" class="mk ml it lb b lc mt lf mu li mv lm mw lq mx lu mp mq mr ms bi translated">转换音频，使其机器就绪。这个过程是数据预处理部分，在这里我们清除数据的特征，以便机器对其进行处理。</li><li id="283a" class="mk ml it lb b lc mt lf mu li mv lm mw lq mx lu mp mq mr ms bi translated">对获取的数据应用自然语言处理(NLP)来理解语音内容。</li><li id="ffcf" class="mk ml it lb b lc mt lf mu li mv lm mw lq mx lu mp mq mr ms bi translated">合成已识别的单词，以帮助机器说出类似的方言。</li></ol><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi my"><img src="../Images/ed490630d62e6bf5c572efb179c4fcc6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7bL1LloYyA_cekQS6_8Wzw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">语音(音频)信号的处理|作者提供的图像</p></figure><p id="c8df" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们用相应的伪代码逐一详细地介绍所有这些步骤和过程。此外，在我们开始之前，下面是一个完整代码库的链接，可以方便地与本教程一起阅读。</p><div class="mz na gp gr nb nc"><a href="https://github.com/rjrahul24/ai-with-python-series/tree/main/10.%20AI%20in%20Speech%20Recognition" rel="noopener  ugc nofollow" target="_blank"><div class="nd ab fo"><div class="ne ab nf cl cj ng"><h2 class="bd iu gy z fp nh fr fs ni fu fw is bi translated">rjrahul 24/ai-与python-系列</h2><div class="nj l"><h3 class="bd b gy z fp nh fr fs ni fu fw dk translated">Python系列教程，旨在学习人工智能概念。这一系列教程从…</h3></div><div class="nk l"><p class="bd b dl z fp nh fr fs ni fu fw dk translated">github.com</p></div></div><div class="nl l"><div class="nm l nn no np nl nq ks nc"/></div></div></a></div><h1 id="369a" class="nr ns it bd nt nu nv nw nx ny nz oa ob jz oc ka od kc oe kd of kf og kg oh oi bi translated">步骤1:读取音频信号文件</h1><p id="2de5" class="pw-post-body-paragraph kz la it lb b lc oj ju le lf ok jx lh li ol lk ll lm om lo lp lq on ls lt lu im bi translated"><strong class="lb iu"><em class="oo">Python中的文件I/O(SciPy . io):</em></strong>SciPy有很多在Python中执行文件操作的方法。包含方法<em class="oo"> read(文件名[，mmap]) </em>和<em class="oo"> write(文件名，速率，数据)</em>的I/O模块用于读取. wav文件并以. wav文件的形式写入NumPy数组。我们将使用这些方法读取和写入声音(音频)文件格式。</p><p id="40c9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">启动语音识别算法的第一步是创建一个可以读取包含音频文件的系统。wav、. mp3等。)并理解这些文件中的信息。Python有一些库，我们可以用它们来读取这些文件，并对它们进行分析。该步骤的目的是将音频信号可视化为结构化的数据点。</p><ul class=""><li id="69a3" class="mk ml it lb b lc ld lf lg li mm lm mn lq mo lu op mq mr ms bi translated"><strong class="lb iu">记录:</strong>记录是我们给算法作为输入的文件。然后，该算法对该输入进行处理，以分析其内容并建立语音识别模型。这可能是一个保存的文件或一个现场录音，Python允许两者。</li><li id="515e" class="mk ml it lb b lc mt lf mu li mv lm mw lq mx lu op mq mr ms bi translated"><strong class="lb iu">采样:</strong>记录的所有信号都以数字化方式存储。这些数字签名对软件来说很难处理，因为机器只能理解数字输入。<strong class="lb iu"> <em class="oo">采样</em> </strong>是用于将这些数字信号转换成离散数字形式的技术。采样以一定的频率进行，并产生数字信号。频率水平的选择取决于人类对声音的感知。例如，选择高频率意味着人类对该音频信号的感知是连续的。</li></ul><pre class="kj kk kl km gt oq or os ot aw ou bi"><span id="e2dd" class="ov ns it or b gy ow ox l oy oz"># Using IO module to read Audio Files<br/>from scipy.io import wavfile<br/>freq_sample, sig_audio = wavfile.read("/content/Welcome.wav")</span><span id="e9b3" class="ov ns it or b gy pa ox l oy oz"># Output the parameters: Signal Data Type, Sampling Frequency and Duration<br/>print('\nShape of Signal:', sig_audio.shape)<br/>print('Signal Datatype:', sig_audio.dtype)<br/>print('Signal duration:', round(sig_audio.shape[0] / float(freq_sample), 2), 'seconds')<br/><strong class="or iu"><em class="oo">&gt;&gt;&gt; Shape of Signal: (645632,) <br/>&gt;&gt;&gt; Signal Datatype: int16 <br/>&gt;&gt;&gt; Signal duration: 40.35 seconds</em></strong></span><span id="7146" class="ov ns it or b gy pa ox l oy oz"># Normalize the Signal Value and Plot it on a graph<br/>pow_audio_signal = sig_audio / np.power(2, 15)<br/>pow_audio_signal = pow_audio_signal [:100]<br/>time_axis = 1000 * np.arange(0, len(pow_audio_signal), 1) / float(freq_sample)</span><span id="809a" class="ov ns it or b gy pa ox l oy oz">plt.plot(time_axis, pow_audio_signal, color='blue')</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pb"><img src="../Images/7927243af8b958b76f65ca7caa9973ad.png" data-original-src="https://miro.medium.com/v2/resize:fit:788/format:webp/1*y8qr8Ysh0yMNBiugUhsWZw.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><p id="ec6a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="oo">这是输入文件的声音振幅相对于播放持续时间的表示。我们已经成功地从音频中提取了数字数据。wav)文件。</em></p><h1 id="6dda" class="nr ns it bd nt nu nv nw nx ny nz oa ob jz oc ka od kc oe kd of kf og kg oh oi bi translated">步骤2:转换音频</h1><p id="a554" class="pw-post-body-paragraph kz la it lb b lc oj ju le lf ok jx lh li ol lk ll lm om lo lp lq on ls lt lu im bi translated">我们在第一部分中所做的音频信号表示代表一个<strong class="lb iu">时域音频信号。</strong>显示声波相对于时间的强度(响度或振幅)。振幅= 0的部分表示静音。</p><blockquote class="lv"><p id="f9a2" class="lw lx it bd ly lz ma mb mc md me lu dk translated">就声音工程学而言，振幅= 0是指当环境中没有其他声音时，静止或运动的空气粒子发出的声音。</p></blockquote><p id="f497" class="pw-post-body-paragraph kz la it lb b lc mf ju le lf mg jx lh li mh lk ll lm mi lo lp lq mj ls lt lu im bi translated"><strong class="lb iu">频域表示:</strong>为了更好的理解一个音频信号，有必要通过频域来看。音频信号的这种表示将为我们提供信号中不同频率存在的细节。傅立叶变换是一种数学概念，可用于将连续信号从其原始时域状态转换到频域状态。我们将使用Python中的傅立叶变换(FT)将音频信号转换为以频率为中心的表示。</p><p id="e28b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">Python中的傅立叶变换:</strong>傅立叶变换是一个数学概念，可以分解这个信号，带出各个频率。这对于理解所有组合在一起形成我们听到的声音的频率是至关重要的。傅立叶变换(FT)给出了信号中存在的所有频率，还显示了每个频率的幅度。</p><blockquote class="pc pd pe"><p id="c2b0" class="kz la oo lb b lc ld ju le lf lg jx lh pf lj lk ll pg ln lo lp ph lr ls lt lu im bi translated">所有音频信号都是由许多单频声波的集合组成，这些声波一起传播，并在运动介质(例如房间)中产生干扰。捕捉声音本质上是捕捉这些波在空间中产生的振幅。</p></blockquote><p id="63e0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu"> NumPy (np.fft.fft): </strong>这个NumPy函数允许我们计算一维离散傅立叶变换。该函数使用快速傅立叶变换(FFT)算法将给定序列转换为离散傅立叶变换(DFT)。在我们正在处理的文件中，我们有一个从音频文件中提取的幅度序列，它最初是从连续信号中采样的。我们将使用该函数将该时域信号转换为离散的频域信号。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pi"><img src="../Images/6c3367c960f079c7726d51c5d5d17c88.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TpXzUaECTsf7yf-YQX6kAQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">音频信号从时域到频域的转换|图片由作者提供</p></figure><p id="617b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在让我们浏览一些代码来实现对音频信号的傅立叶变换，目的是用声音的强度(分贝(dB))来表示声音</p><pre class="kj kk kl km gt oq or os ot aw ou bi"><span id="acda" class="ov ns it or b gy ow ox l oy oz"># Working on the same input file<br/># Extracting the length and the half-length of the signal to input to the foruier transform<br/>sig_length = len(sig_audio)<br/>half_length = np.ceil((sig_length + 1) / 2.0).astype(np.int)</span><span id="d4e1" class="ov ns it or b gy pa ox l oy oz"># We will now be using the Fourier Transform to form the frequency domain of the signal<br/>signal_freq = np.fft.fft(sig_audio)</span><span id="5ce3" class="ov ns it or b gy pa ox l oy oz"># Normalize the frequency domain and square it<br/>signal_freq = abs(signal_freq[0:half_length]) / sig_length<br/>signal_freq **= 2<br/>transform_len = len(signal_freq)</span><span id="df67" class="ov ns it or b gy pa ox l oy oz"># The Fourier transformed signal now needs to be adjusted for both even and odd cases<br/>if sig_length % 2:<br/>  signal_freq[1:transform_len] *= 2<br/>else:<br/>  signal_freq[1:transform_len-1] *= 2</span><span id="bfe8" class="ov ns it or b gy pa ox l oy oz"># Extract the signal's strength in decibels (dB)<br/>exp_signal = 10 * np.log10(signal_freq)<br/>x_axis = np.arange(0, half_length, 1) * (freq_sample / sig_length) / 1000.0</span><span id="3abd" class="ov ns it or b gy pa ox l oy oz">plt.plot(x_axis, exp_signal, color='green', linewidth=1)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pj"><img src="../Images/e4faf0017dd41589d2447f3c3811df6e.png" data-original-src="https://miro.medium.com/v2/resize:fit:794/format:webp/1*uagxwCrxz25gk9Af2PU5Bw.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><p id="b15a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="oo">这样，我们就能够对音频输入文件进行傅立叶变换，并随后看到音频的频域(频率对信号强度)表示。</em></p><h1 id="205d" class="nr ns it bd nt nu nv nw nx ny nz oa ob jz oc ka od kc oe kd of kf og kg oh oi bi translated">步骤3:从语音中提取特征</h1><p id="f2ff" class="pw-post-body-paragraph kz la it lb b lc oj ju le lf ok jx lh li ol lk ll lm om lo lp lq on ls lt lu im bi translated">一旦语音从时域信号转换为频域信号，下一步就是将频域数据转换为可用的特征向量。在开始之前，我们必须了解一个叫做MFCC的新概念。</p><h2 id="6a3c" class="ov ns it bd nt pk pl dn nx pm pn dp ob li po pp od lm pq pr of lq ps pt oh pu bi translated"><strong class="ak">梅尔频率倒谱系数</strong></h2><p id="5915" class="pw-post-body-paragraph kz la it lb b lc oj ju le lf ok jx lh li ol lk ll lm om lo lp lq on ls lt lu im bi translated">MFCC是一种旨在从音频信号中提取特征的技术。它使用MEL标度来划分音频信号的频带，然后从每个单独的频带中提取系数，从而在频率之间创建分离。MFCC使用离散余弦变换(DCT)来执行这个操作。MEL标度建立在人类对声音的感知上，即人脑如何处理音频信号并区分不同的频率。下面让我们来看看梅尔标度的形成。</p><ul class=""><li id="562b" class="mk ml it lb b lc ld lf lg li mm lm mn lq mo lu op mq mr ms bi translated"><strong class="lb iu">人声声音感知:</strong>成年人的基本听力范围为85 Hz至255 Hz，并且这可以进一步区分性别(男性为85Hz至180 Hz，女性为165 Hz至255 Hz)。在这些基频之上，还有人耳处理的谐波。谐波是基频的倍数。这些是简单的乘法器，例如，100 Hz频率的二次谐波将是200 Hz，三次谐波将是300 Hz，等等。</li></ul><blockquote class="pc pd pe"><p id="e91d" class="kz la oo lb b lc ld ju le lf lg jx lh pf lj lk ll pg ln lo lp ph lr ls lt lu im bi translated">人类的粗略听觉范围是20Hz到20KHz，这种声音感知也是非线性的。与高频声音相比，我们可以更好地分辨低频声音。例如，我们可以清楚地说出100赫兹和200赫兹信号之间的区别，但不能区分15000赫兹和15100赫兹。为了产生不同频率的音调，我们可以使用上面的程序或者使用这个<a class="ae ky" href="http://www.szynalski.com/tone-generator/" rel="noopener ugc nofollow" target="_blank">工具。</a></p></blockquote><ul class=""><li id="1b90" class="mk ml it lb b lc ld lf lg li mm lm mn lq mo lu op mq mr ms bi translated"><strong class="lb iu">梅尔标度:</strong> Stevens、Volkmann和Newmann在1937年提出了一种向世界介绍梅尔标度的方法。它是一种音高音阶(具有不同音高水平的音频信号的音阶),由人类根据它们的距离相等来判断。它基本上是一个源自人类感知的尺度。例如，如果你接触到两个相距很远的声源，大脑会感知到这两个声源之间的距离，而不会实际看到它们。这个标度是基于我们人类如何用听觉测量音频信号距离。因为我们的感知是非线性的，这个尺度上的距离随着频率而增加。</li><li id="503d" class="mk ml it lb b lc mt lf mu li mv lm mw lq mx lu op mq mr ms bi translated"><strong class="lb iu">梅尔间隔滤波器组:</strong>为了计算每个频带的功率(强度)，第一步是区分可用的不同特征频带(由MFCC完成)。一旦进行了这些分离，我们就使用滤波器组在频率中创建分区并将它们分开。可以使用分区的任何指定频率来创建滤波器组。随着频率的增加，滤波器组中滤波器之间的间距呈指数增长。在代码部分，我们将了解如何分离频带。</li></ul><h2 id="8fed" class="ov ns it bd nt pk pl dn nx pm pn dp ob li po pp od lm pq pr of lq ps pt oh pu bi translated"><strong class="ak">MFCC和滤波器组的数学</strong></h2><p id="d245" class="pw-post-body-paragraph kz la it lb b lc oj ju le lf ok jx lh li ol lk ll lm om lo lp lq on ls lt lu im bi translated">MFCC和滤波器组的创建都是由音频信号的性质所激发，并受到人类感知声音的方式的影响。但是这种处理也需要大量的数学计算，这些计算在其实现过程中是幕后进行的。Python直接为我们提供了构建滤波器和对声音执行MFCC功能的方法，但让我们看一下这些函数背后的数学原理。</p><blockquote class="pc pd pe"><p id="f5c6" class="kz la oo lb b lc ld ju le lf lg jx lh pf lj lk ll pg ln lo lp ph lr ls lt lu im bi translated">进入该处理的三个离散数学模型是<strong class="lb iu">离散余弦变换(DCT) </strong>，用于滤波器组系数的去相关，也称为声音白化，以及<strong class="lb iu">高斯混合模型——隐马尔可夫模型(GMMs-HMMs) </strong>，它是<strong class="lb iu">自动语音识别(ASR)算法的标准</strong>。</p></blockquote><p id="f977" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">虽然，在今天，当计算成本下降时(由于云计算)，深度学习语音系统对噪声不太敏感，被用于这些技术之上。</p><blockquote class="lv"><p id="ec71" class="lw lx it bd ly lz ma mb mc md me lu dk translated">DCT是一种线性变换算法，因此它会排除许多有用的信号，因为声音是高度非线性的。</p></blockquote><pre class="pv pw px py pz oq or os ot aw ou bi"><span id="72cf" class="ov ns it or b gy ow ox l oy oz"># Installing and importing necessary libraries<br/>pip install python_speech_features<br/>from python_speech_features import mfcc, logfbank<br/>sampling_freq, sig_audio = wavfile.read("Welcome.wav")</span><span id="282c" class="ov ns it or b gy pa ox l oy oz"><em class="oo"># We will now be taking the first 15000 samples from the signal for analysis<br/></em>sig_audio = sig_audio[:15000]</span><span id="fadf" class="ov ns it or b gy pa ox l oy oz"><em class="oo"># Using MFCC to extract features from the signal<br/></em>mfcc_feat = mfcc(sig_audio, sampling_freq)<br/>print('\nMFCC Parameters\nWindow Count =', mfcc_feat.shape[0])<br/>print('Individual Feature Length =', mfcc_feat.shape[1])<br/><strong class="or iu"><em class="oo">&gt;&gt;&gt; MFCC Parameters Window Count = 93 <br/>&gt;&gt;&gt; Individual Feature Length = 13</em></strong></span><span id="a29a" class="ov ns it or b gy pa ox l oy oz">mfcc_feat = mfcc_feat.T<br/>plt.matshow(mfcc_feat)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi qa"><img src="../Images/0cb8f6c6da424208ea4cfd4bfeb83b73.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tqqK8Wt7vYV246kYD1TOYA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><em class="qb">每段下面的第一条水平黄线是基频，处于最强状态。黄线上方是谐波，它们之间的频率距离相同。|作者图片</em></p></figure><pre class="kj kk kl km gt oq or os ot aw ou bi"><span id="b52b" class="ov ns it or b gy ow ox l oy oz"><em class="oo"># Generating filter bank features<br/></em>fb_feat = logfbank(sig_audio, sampling_freq)<br/>print('\nFilter bank\nWindow Count =', fb_feat.shape[0])<br/>print('Individual Feature Length =', fb_feat.shape[1])<br/><strong class="or iu"><em class="oo">&gt;&gt;&gt; Filter bank Window Count = 93<br/>&gt;&gt;&gt; Individual Feature Length = 26</em></strong></span><span id="37d1" class="ov ns it or b gy pa ox l oy oz">fb_feat = fb_feat.T<br/>plt.matshow(fb_feat)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi qc"><img src="../Images/ceae3fab9bfcd937a14029848cfd0544.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NROIoVKEJBK5matAFANC5g.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><p id="4dfa" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果我们看到这两个分布，很明显，低频和高频声音分布在第二个图像中是分开的。</p><blockquote class="pc pd pe"><p id="6a64" class="kz la oo lb b lc ld ju le lf lg jx lh pf lj lk ll pg ln lo lp ph lr ls lt lu im bi translated">MFCC连同滤波器组的应用是分离高频和低频信号的好算法。这加快了分析过程，因为我们可以将声音信号分成两个或多个独立的片段，并根据它们的频率单独进行分析。</p></blockquote><h1 id="8250" class="nr ns it bd nt nu nv nw nx ny nz oa ob jz oc ka od kc oe kd of kf og kg oh oi bi translated">第四步:识别口语单词</h1><p id="9ca4" class="pw-post-body-paragraph kz la it lb b lc oj ju le lf ok jx lh li ol lk ll lm om lo lp lq on ls lt lu im bi translated">语音识别是理解人类声音并在机器中将其转录为文本的过程。有几个库可用于处理语音到文本，即Bing语音、Google语音、Houndify、IBM语音到文本等。我们将使用谷歌语音库将语音转换成文本。</p><h2 id="26eb" class="ov ns it bd nt pk pl dn nx pm pn dp ob li po pp od lm pq pr of lq ps pt oh pu bi translated"><strong class="ak">谷歌语音API </strong></h2><p id="962c" class="pw-post-body-paragraph kz la it lb b lc oj ju le lf ok jx lh li ol lk ll lm om lo lp lq on ls lt lu im bi translated">关于谷歌语音API的更多信息可以从<a class="ae ky" href="https://cloud.google.com/speech-to-text" rel="noopener ugc nofollow" target="_blank">谷歌云页面</a>和<a class="ae ky" href="https://pypi.org/project/SpeechRecognition/" rel="noopener ugc nofollow" target="_blank">语音识别</a> PyPi页面中阅读。Google Speech API能够实现的几个关键特性是语音的自适应。这意味着API理解语音的领域。例如，货币、地址、年份都被规定到语音到文本的转换中。在算法中定义了特定于域的类别，这些类别识别输入语音中的这些出现。在当前的工作环境中，API既可以处理现场预录制的文件，也可以处理麦克风上的现场录音。在下一节中，我们将通过麦克风输入来分析现场语音。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi qd"><img src="../Images/9299250c3ef70270733a4da299ab5f4e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vsuNqx_hld-1gaOMdsphhQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">Google Speech API的架构流程|作者图片</p></figure><ol class=""><li id="2a0a" class="mk ml it lb b lc ld lf lg li mm lm mn lq mo lu mp mq mr ms bi translated"><strong class="lb iu">使用麦克风:</strong>py audio开源包允许我们通过连接的麦克风直接录制音频，并使用Python进行实时分析。PyAudio的安装会因操作系统而异(安装说明在下面的代码部分提到)。</li><li id="c134" class="mk ml it lb b lc mt lf mu li mv lm mw lq mx lu mp mq mr ms bi translated"><strong class="lb iu">麦克风类:</strong>的实例。microphone()类可以与语音识别器一起使用，直接在工作目录中记录音频。要检查麦克风在系统中是否可用，请使用list_microphone_names静态方法。要使用任何列出的可用麦克风，请使用device_index方法(实现如下面的代码所示)</li><li id="6319" class="mk ml it lb b lc mt lf mu li mv lm mw lq mx lu mp mq mr ms bi translated"><strong class="lb iu">捕获麦克风输入:</strong>listen()函数用于捕获麦克风的输入。所选麦克风接收的所有声音信号都存储在调用listen()函数的变量中。这种方法持续记录，直到检测到无声(0振幅)信号。</li><li id="258e" class="mk ml it lb b lc mt lf mu li mv lm mw lq mx lu mp mq mr ms bi translated"><strong class="lb iu">降低环境噪音:</strong>任何功能环境都容易产生环境噪音，从而妨碍录音。因此，Recognizer类中的adjust_for_ambient_noise()方法有助于自动消除录音中的环境噪音。</li><li id="bfc1" class="mk ml it lb b lc mt lf mu li mv lm mw lq mx lu mp mq mr ms bi translated"><strong class="lb iu">声音识别:</strong>下面的语音识别工作流程解释了信号处理后的部分，其中API执行诸如语义和语法纠正的任务，理解声音域、口语，并最终通过将语音转换为文本来创建输出。下面我们还将看到使用Microphone类实现Google的语音识别API。</li></ol><pre class="kj kk kl km gt oq or os ot aw ou bi"><span id="8973" class="ov ns it or b gy ow ox l oy oz"># Install the SpeechRecognition and pipwin classes to work with the Recognizer() class<br/>pip install SpeechRecognition<br/>pip install pipwin</span><span id="fe11" class="ov ns it or b gy pa ox l oy oz"># Below are a few links that can give details about the PyAudio class we will be using to read direct microphone input into the Jupyter Notebook<br/># <a class="ae ky" href="https://anaconda.org/anaconda/pyaudio" rel="noopener ugc nofollow" target="_blank">https://anaconda.org/anaconda/pyaudio</a><br/># <a class="ae ky" href="https://www.lfd.uci.edu/~gohlke/pythonlibs/#pyaudio" rel="noopener ugc nofollow" target="_blank">https://www.lfd.uci.edu/~gohlke/pythonlibs/#pyaudio</a><br/># To install PyAudio, Run in the Anaconda Terminal CMD: conda install -c anaconda pyaudio<br/># Pre-requisite for running PyAudio installation - Microsoft Visual C++ 14.0 or greater will be required. Get it with "Microsoft C++ Build Tools" : <a class="ae ky" href="https://visualstudio.microsoft.com/visual-cpp-build-tools/" rel="noopener ugc nofollow" target="_blank">https://visualstudio.microsoft.com/visual-cpp-build-tools/</a><br/># To run PyAudio on Colab, please install PyAudio.whl in your local system and give that path to colab for installation</span><span id="ac60" class="ov ns it or b gy pa ox l oy oz">pip install pyaudio<br/>import speech_recognition as speech_recog</span><span id="6c84" class="ov ns it or b gy pa ox l oy oz"># Creating a recording object to store input<br/>rec = speech_recog.Recognizer()</span><span id="182d" class="ov ns it or b gy pa ox l oy oz"># Importing the microphone class to check availabiity of microphones<br/>mic_test = speech_recog.Microphone()</span><span id="01e8" class="ov ns it or b gy pa ox l oy oz"># List the available microphones<br/>speech_recog.Microphone.list_microphone_names()</span><span id="6bf7" class="ov ns it or b gy pa ox l oy oz"># We will now directly use the microphone module to capture voice input. Specifying the second microphone to be used for a duration of 3 seconds. The algorithm will also adjust given input and clear it of any ambient noise</span><span id="ba68" class="ov ns it or b gy pa ox l oy oz">with speech_recog.Microphone(device_index=1) as source: <br/>    rec.adjust_for_ambient_noise(source, duration=3)<br/>    print("Reach the Microphone and say something!")<br/>    audio = rec.listen(source)</span><span id="6630" class="ov ns it or b gy pa ox l oy oz"><strong class="or iu"><em class="oo">&gt;&gt;&gt; Reach the Microphone and say something!</em></strong></span><span id="f931" class="ov ns it or b gy pa ox l oy oz"># Use the recognize function to transcribe spoken words to text<br/>try:<br/>    print("I think you said: \n" + rec.recognize_google(audio))<br/>except Exception as e:<br/>    print(e)<br/><strong class="or iu"><em class="oo">&gt;&gt;&gt; I think you said: <br/>&gt;&gt;&gt; hi hello hello hello</em></strong></span></pre><p id="8b44" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们的语音识别和声音分析文章到此结束。我仍然建议您浏览参考资料部分提到的链接和故事顶部链接的代码库，以便能够在每一步都遵循它。</p><blockquote class="pc pd pe"><p id="a23a" class="kz la oo lb b lc ld ju le lf lg jx lh pf lj lk ll pg ln lo lp ph lr ls lt lu im bi translated">这是一个总结！</p></blockquote><h1 id="6418" class="nr ns it bd nt nu nv nw nx ny nz oa ob jz oc ka od kc oe kd of kf og kg oh oi bi translated">结论</h1><p id="4407" class="pw-post-body-paragraph kz la it lb b lc oj ju le lf ok jx lh li ol lk ll lm om lo lp lq on ls lt lu im bi translated">语音识别是一个人工智能概念，允许机器听人的声音，并从中转录文本。尽管本质上很复杂，但围绕语音识别的用例是很多的。从帮助不同能力的用户访问计算到自动应答机器，自动语音识别(ASR)算法目前正在被许多行业使用。本章简要介绍了声音分析工程，并展示了一些处理音频的基本技巧。虽然不详细，但它将有助于创建一个语音分析在人工智能世界中如何工作的整体画面。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi qe"><img src="../Images/c0f38695949f39a166d67541a75a8705.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*xX2jQKUaC7tVotnV"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><a class="ae ky" href="https://unsplash.com/@suanmoo?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> 수안 최 </a>在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍照</p></figure></div><div class="ab cl qf qg hx qh" role="separator"><span class="qi bw bk qj qk ql"/><span class="qi bw bk qj qk ql"/><span class="qi bw bk qj qk"/></div><div class="im in io ip iq"><h2 id="4243" class="ov ns it bd nt pk pl dn nx pm pn dp ob li po pp od lm pq pr of lq ps pt oh pu bi translated">关于我</h2><p id="d815" class="pw-post-body-paragraph kz la it lb b lc oj ju le lf ok jx lh li ol lk ll lm om lo lp lq on ls lt lu im bi translated">我是Rahul，目前在研究人工智能，在Xbox游戏上实现大数据分析。我在微软工作。除了专业工作之外，我还试图制定一个程序，来理解如何通过使用人工智能来改善世界上发展中国家的经济状况。</p><p id="a99b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我现在在纽约的哥伦比亚大学，你可以通过<a class="ae ky" href="https://www.linkedin.com/in/rjrahul24/" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>和<a class="ae ky" href="https://twitter.com/rjrahul24" rel="noopener ugc nofollow" target="_blank"> Twitter </a>与我联系。</p><h2 id="5d4d" class="ov ns it bd nt pk pl dn nx pm pn dp ob li po pp od lm pq pr of lq ps pt oh pu bi translated">[参考文献]</h2><ol class=""><li id="3410" class="mk ml it lb b lc oj lf ok li qm lm qn lq qo lu mp mq mr ms bi translated"><a class="ae ky" href="https://www.ibm.com/cloud/learn/speech-recognition" rel="noopener ugc nofollow" target="_blank">https://www.ibm.com/cloud/learn/speech-recognition</a></li><li id="ba5c" class="mk ml it lb b lc mt lf mu li mv lm mw lq mx lu mp mq mr ms bi translated"><a class="ae ky" href="https://www.sciencedirect.com/topics/engineering/speech-recognition" rel="noopener ugc nofollow" target="_blank">https://www . science direct . com/topics/engineering/speech-recognition</a></li><li id="2915" class="mk ml it lb b lc mt lf mu li mv lm mw lq mx lu mp mq mr ms bi translated"><a class="ae ky" href="https://signalprocessingsociety.org/publications-resources/blog/what-are-benefits-speech-recognition-technology" rel="noopener ugc nofollow" target="_blank">https://signalprocessingsociety . org/publications-resources/blog/what-are-benefits-speech-recognition-technology</a></li><li id="642b" class="mk ml it lb b lc mt lf mu li mv lm mw lq mx lu mp mq mr ms bi translated"><a class="ae ky" href="https://www.analyticsvidhya.com/blog/2021/01/introduction-to-automatic-speech-recognition-and-natural-language-processing/" rel="noopener ugc nofollow" target="_blank">https://www . analyticsvidhya . com/blog/2021/01/自动语音识别和自然语言处理简介/ </a></li><li id="dfd9" class="mk ml it lb b lc mt lf mu li mv lm mw lq mx lu mp mq mr ms bi translated"><a class="ae ky" href="https://cloud.google.com/speech-to-text" rel="noopener ugc nofollow" target="_blank">https://cloud.google.com/speech-to-text</a></li><li id="8966" class="mk ml it lb b lc mt lf mu li mv lm mw lq mx lu mp mq mr ms bi translated"><a class="ae ky" href="https://www.analyticsvidhya.com/blog/2021/06/mfcc-technique-for-speech-recognition/" rel="noopener ugc nofollow" target="_blank">https://www . analyticsvidhya . com/blog/2021/06/MFCC-technique-for-speech-recognition/</a></li><li id="dedb" class="mk ml it lb b lc mt lf mu li mv lm mw lq mx lu mp mq mr ms bi translated">【https://www.google.com/intl/en/chrome/demos/speech.html T4】</li><li id="5df1" class="mk ml it lb b lc mt lf mu li mv lm mw lq mx lu mp mq mr ms bi translated">https://OCW . MIT . edu/courses/electrical-engineering-and-computer-science/6-345-automatic-speech-recognition-spring-2003/lecture-notes/lecture 5 . pdf</li><li id="139c" class="mk ml it lb b lc mt lf mu li mv lm mw lq mx lu mp mq mr ms bi translated"><a class="ae ky" rel="noopener" target="_blank" href="/understanding-audio-data-fourier-transform-fft-spectrogram-and-speech-recognition-a4072d228520">https://towards data science . com/understanding-audio-data-Fourier-transform-FFT-spectrogram-and-speech-recognition-a 4072d 228520</a></li><li id="c55b" class="mk ml it lb b lc mt lf mu li mv lm mw lq mx lu mp mq mr ms bi translated"><a class="ae ky" href="https://haythamfayek.com/2016/04/21/speech-processing-for-machine-learning.html" rel="noopener ugc nofollow" target="_blank">https://haythamfayek . com/2016/04/21/speech-processing-for-machine-learning . html</a></li></ol></div></div>    
</body>
</html>