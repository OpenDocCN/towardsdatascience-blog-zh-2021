<html>
<head>
<title>Build a Classification Model for a Practical Use Case</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">为实际用例建立分类模型</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/build-a-classification-model-for-a-practical-use-case-670a1bb8dd11?source=collection_archive---------21-----------------------#2021-06-21">https://towardsdatascience.com/build-a-classification-model-for-a-practical-use-case-670a1bb8dd11?source=collection_archive---------21-----------------------#2021-06-21</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="77d2" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">具有核心技巧和松弛变量支持向量机深度挖掘</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/68368a744c64248bac57eedd38245384.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ObxCnY68f_1NXfylY3LbMA.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">在<a class="ae ky" href="https://unsplash.com/s/photos/night-sky?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上由<a class="ae ky" href="https://unsplash.com/@24ameer?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Ameer Basheer </a>拍摄的照片</p></figure><p id="b522" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">数据科学家你好！我希望你们都过得很好。上次我们讨论了如何用核心概念从零开始构建一个回归模型到一个实际用例。今天，我计划向您提供对分类模型的高级理解，并通过实践经验深入了解实际场景。</p><p id="138e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在本文中，您将了解到:</p><ul class=""><li id="e136" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated">支持向量机</li><li id="bf7e" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">拉格朗日乘数</li><li id="3929" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">松弛变量</li><li id="7eed" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">内核技巧</li><li id="fc13" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">超参数调谐</li><li id="279e" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">类别不平衡处理</li><li id="c87e" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">准确度测量</li></ul></div><div class="ab cl mj mk hx ml" role="separator"><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo"/></div><div class="im in io ip iq"><blockquote class="mq mr ms"><p id="b925" class="kz la mt lb b lc ld ju le lf lg jx lh mu lj lk ll mv ln lo lp mw lr ls lt lu im bi translated"><strong class="lb iu"> <em class="it">用例:</em> </strong> <em class="it">银行营销数据集:与一家葡萄牙银行机构的直接营销活动(电话)相关。目标是预测客户是否会认购定期存款。</em></p></blockquote><p id="49e7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我在这里使用的数据集来自<a class="ae ky" href="http://archive.ics.uci.edu/ml/datasets/Bank+Marketing" rel="noopener ugc nofollow" target="_blank"> UCI机器学习库</a>，它可以公开用于研究。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mx"><img src="../Images/8c6d284b52452ffb8981293d4964ab45.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DHew1ceYWuYLcJHPWd1iVQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">数据集来源:<a class="ae ky" href="https://archive.ics.uci.edu/ml/datasets/bank+marketing#" rel="noopener ugc nofollow" target="_blank">https://archive.ics.uci.edu/ml/datasets/bank+marketing#</a></p></figure><h1 id="36a4" class="my mz it bd na nb nc nd ne nf ng nh ni jz nj ka nk kc nl kd nm kf nn kg no np bi translated">1.问题定义</h1><p id="f732" class="pw-post-body-paragraph kz la it lb b lc nq ju le lf nr jx lh li ns lk ll lm nt lo lp lq nu ls lt lu im bi translated">在我们着手做任何事情之前，我们需要了解问题和影响决策的事实。所以根据给定的用例，这是一个分类问题。我们需要预测特定的客户是否愿意将他/她的钱存入银行。</p><p id="a93a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">作为数据科学家，我们的总体目标应该是，通过识别影响营销活动成功的主要因素并预测营销活动对特定客户是否会成功，来提高营销活动的效率。</p><p id="e0b1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">因此，作为第一步，我们需要找出我们在数据集中有什么(+它的复杂性)以及可以直接或间接影响我们预测的特定事实。<strong class="lb iu"> </strong>然后我们需要像往常一样做预处理。</p><p id="c7b5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我假设你已经熟悉了我之前的<a class="ae ky" rel="noopener" target="_blank" href="/build-a-machine-learning-model-to-a-practical-use-case-from-scratch-3a8dc65ab6f1?gi=313b9d219b09">文章</a>中的预处理步骤。我已经更深入地解释过了。因此，您将能够轻松地对该数据集进行预处理，直至添加PCA。</p><p id="dfe1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">此外，您可以参考我在<strong class="lb iu">资源部分</strong>下的python笔记本，一步一步地了解这个过程。</p><p id="9b43" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">预处理完成后，我们需要拟合一个模型。所以根据给定的用例，这是一个分类问题。有如此多的分类技术可用。一些非常受欢迎的是:</p><ol class=""><li id="1b9c" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu nv mb mc md bi translated">k-最近邻(KNN)-通过具有最小距离的多数表决原则确定数据点的类别。</li><li id="65f8" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu nv mb mc md bi translated">朴素贝叶斯——基于贝叶斯定理的概率算法。</li><li id="0d7a" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu nv mb mc md bi translated">决策树—遵循一组决策规则(if-then-else条件)来可视化数据并根据条件对其进行分类。</li></ol><p id="317f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">但是你不能盲目地将任何模型应用于给定的数据集。在应用模型之前，您需要了解问题环境、数据集复杂性、每个模型的特点和缺点，以及许多额外的东西。如果您用一些领域级别的理解(+您的经验)正确地分析它，您将能够为您的数据集找到一个更好的适合模型。</p><p id="a8a6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在这里，我选择了SVM模式。我将用一些数学概念在更深的层次上解释SVM。如果你赶时间，跳过下面的解释，直接跳到实际的实现(第6节)。</p><h1 id="332c" class="my mz it bd na nb nc nd ne nf ng nh ni jz nj ka nk kc nl kd nm kf nn kg no np bi translated">2.支持向量机解释</h1><p id="1112" class="pw-post-body-paragraph kz la it lb b lc nq ju le lf nr jx lh li ns lk ll lm nt lo lp lq nu ls lt lu im bi translated">支持向量机是机器学习中最流行的分类技术之一，它直接受到一个叫做统计学习理论的数学概念的启发。</p><p id="7fc5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">它是一种监督学习技术，主要应用于<strong class="lb iu">线性可分的</strong>数据集(特征)。同样，目标(y)应该是明确的。当y是连续的，我们可以使用支持向量回归。</p><p id="c28e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">线性可分的意思是，我们应该能够找到一条线性直线或者一个线性超平面，能够把数据点完全分成两类。</p><p id="486a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">关键思想是找到尽可能广泛地将一个类与另一个类分开的<strong class="lb iu">最优超平面</strong>。</p><blockquote class="mq mr ms"><p id="67cd" class="kz la mt lb b lc ld ju le lf lg jx lh mu lj lk ll mv ln lo lp mw lr ls lt lu im bi translated">注意——我们只能讨论两类的线性可分性。但是如何处理多类分类呢？等到最后！</p></blockquote><p id="f54c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果数据集不是线性可分的，您应该使用非线性转换或松弛变量使它们可分(至少在某种程度上)。我将在几分钟后解释什么是松弛变量。</p><p id="c52b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">SVM的主要直觉是识别边界向量。那么什么是边界呢？我用图表来解释一下。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nw"><img src="../Images/63aba5ab96b392178337cb5e076e949a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Xfr_nUDh4tjpxdMGenBD1g.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">对SVM的主要直觉(图片由作者提供)</p></figure><p id="8aaf" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在线性可分离的数据集中，我们可以找到分隔两个类的边界。(这里我取正和负)这些位于边界线上的向量称为支持向量。</p><p id="57a0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">SVM总能找到线性可分模式的最佳超平面。这个边界被称为决策边界。(在神经网络中，这个判定边界是任意选择的。但是在SVM，它是以一种最佳的方式被挑选出来的。)</p><p id="f8dc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在SVM，我们讨论如何绘制这些线条，使它们具有最宽的宽度，将阳性样本与阴性样本分开。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nx"><img src="../Images/46e89e4e34d2ecacab3777ce40ff59e4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ebmUTTZKo_OF0CLwDPFTjg.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">通过最大化宽度来绘制决策边界(图片由作者提供)</p></figure><p id="d012" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">所以这3条线可以用下面的等式来表示。</p><ul class=""><li id="a9bd" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated">wx+b = 0——用于决策边界</li><li id="1d57" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">Wx+b=-1 —适用于1级边界</li><li id="bdd7" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">Wx+b=+1 —用于2级边界</li></ul><p id="b9c4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">阳性样本位于<strong class="lb iu"> Wx+b≥ +1 </strong>侧，阴性样本位于<strong class="lb iu"> Wx+b≤ -1 </strong>侧。</p><p id="0c7d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果我们为正样本引入变量y=+1，为负样本引入变量y=-1，那么我们可以得到该条件的一般形式。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ny"><img src="../Images/ebc43374fa82af5470cb288c47bf6755.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*06m7lXQDl9oQ6u0F5hLymw.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">条件的一般形式(作者提供的图片)</p></figure><p id="8d79" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果满足这个条件，所有正负数据点都将在边界线后面。(完美区分两个类别)</p><p id="f33a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">当我们确定w和b时，我们应该考虑最大宽度。让我们找出宽度。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nz"><img src="../Images/60ce752a143b5e321767e426254d9ddf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3mZlX_aHDiZQCd3b5zFb4g.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">寻找宽度(作者图片)</p></figure><p id="d3f1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们想最大化宽度。为了最大化宽度，我们需要最小化|W|或1/2|W|(这不会改变优化函数，但在计算梯度时只会导致更整洁的解决方案)</p><p id="cb0a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">所以最终我们需要在<code class="fe oa ob oc od b">y(Wx+b)-1=0</code>的约束下最小化1/2|W|</p><p id="3c44" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果根据给定的条件，我们需要最小化某个东西，我们拥有的唯一定理是<strong class="lb iu">拉格朗日定理</strong>。所以还是来应用一下吧。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oe"><img src="../Images/477a290a017099e8d19cae46d4e1cd0f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5HILf0ezh8opovTCGOypaw.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">拉格朗日定理的应用(图片由作者提供)</p></figure><p id="f47e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在解完方程并将数值代入前面的方程A后，我们可以找到最佳宽度。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi of"><img src="../Images/d44e937eeb3cd88b754452cdf771a407.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QACebr1yGc0273SjAMmMEA.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">最佳宽度(图片由作者提供)</p></figure><p id="aa44" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">最佳权重向量和偏差可以这样导出。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi og"><img src="../Images/63429466180c373aaa7ee0271ef33b9f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xKulYEwGFTOdyG78T_4dMQ.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">最佳权重向量和偏差(图片由作者提供)</p></figure><p id="75bb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在我们知道了最佳权重向量和偏差。我们可以用它来预测给定的数据点。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oh"><img src="../Images/84dd182629f6cbe69b3bdd68c53165af.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3vn9nI-1GKwOc3NEkTVFEw.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">给定数据点的预测(图片由作者提供)</p></figure><p id="dabf" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">所以到现在为止，我们一直在讨论线性可分数据。如果我们找不到数据之间的线性分离，我们要么将它们转换成线性可分离状态，要么我们需要使用松弛变量技术对这个非线性可分离数据应用SVM模型。</p><h1 id="670e" class="my mz it bd na nb nc nd ne nf ng nh ni jz nj ka nk kc nl kd nm kf nn kg no np bi translated">3.松弛变量的使用</h1><p id="124d" class="pw-post-body-paragraph kz la it lb b lc nq ju le lf nr jx lh li ns lk ll lm nt lo lp lq nu ls lt lu im bi translated">实际上，大多数数据集不会是线性可分的。结果，将没有办法满足我们先前导出的约束。缓解这个问题的一个方法是通过引入松弛变量来放松一些约束。</p><p id="f83c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">因此，我们在这里做的是，计算与决策裕度不同的数据点之间的距离，并定义一个罚函数。该罚函数控制对误分类的支持。(由于违反了某些约束，这将导致多少数量被错误地分类。)</p><p id="ad1f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们对落在边界之外的数据点施加惩罚。惩罚参数的实际作用是，它温和地惩罚位于决策边界错误一侧的点。这个损失(ξ)被称为<strong class="lb iu">松弛变量</strong>。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oi"><img src="../Images/5f9d7fc169c486894aa53734432fa508.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*d7OhtFgRMF_uA_3oEyAeIA.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">引入惩罚(图片由作者提供)</p></figure><p id="c432" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">ξ = 0表示数据点在正确的边界上或边界内。<br/>对于决策边界上的数据点，ξ = 1。<br/>落在错误一侧的数据点ξ ≥ 1。(j和k被错误分类)</p><p id="e728" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在我们必须最小化:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oj"><img src="../Images/f8cbb3268b0d689debdd592fe6e122c1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*c_QP981SdB7dGa2ByQ11_w.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">最小化新目标(图片由作者提供)</p></figure><p id="8d99" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">参数<em class="mt"> C </em>是<em class="mt">惩罚强度</em>，它指定了我们有多在乎错误。它也被称为正则化参数，我们用它来调整我们的模型。</p><p id="9d46" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果C很小，SVM就变得很松散，可能会牺牲一些点来获得一个简单的解。这将导致未命中分类的增加。如果C非常大，SVM会变得非常严格(强),并试图获得超平面右侧的所有数据点。这将导致模型过拟合。所以在选择C的值时，你应该更加关注。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ok"><img src="../Images/e05b5218025e23f040533fd19b5d2d1b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oAdi8qTgclY4QKopJ6sfMg.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><em class="ol">惩罚</em>力量C vs分类行为(图片由作者提供)</p></figure><h1 id="511a" class="my mz it bd na nb nc nd ne nf ng nh ni jz nj ka nk kc nl kd nm kf nn kg no np bi translated">4.用于多类分类的SVM</h1><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi om"><img src="../Images/2cb18398c45245fbe8633a72c8ac9627.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6CC32uMSCDc6tIl6YFhOmQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">二元对多类分类(图片由作者提供)</p></figure><p id="e2a6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">SVM通常用于二元分类。它本身不支持多类分类。因此相同的二元分类原理被用于多类分类。可以看到有两种方法。</p><ol class=""><li id="50c1" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu nv mb mc md bi translated"><strong class="lb iu"> <em class="mt">一对一</em>方法— </strong>将多类问题分解为多个二元分类案例。</li><li id="7c20" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu nv mb mc md bi translated"><strong class="lb iu"> <em class="mt">一对多</em>方法<em class="mt"> — </em> </strong>细分设置为每个类一个二元分类器。连续地，某一类与所有其他类相区别。</li></ol><p id="1578" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们举一个简单的例子来进一步说明。这里，目标是四个可能的类中的一个:{红色、绿色、蓝色、黄色}。</p><p id="b125" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在<strong class="lb iu">一对一</strong>方法中，我们创建了一组二元分类器，每个分类器代表一个对:</p><ul class=""><li id="8b6d" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated">OvO二元分类器1:红色对绿色</li><li id="5bb4" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">OvO二元分类器2:红色对蓝色</li><li id="5473" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">OvO二元分类器3:红色vs黄色</li><li id="b852" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">OvO二元分类器4:绿色对蓝色</li><li id="53a8" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">OvO二元分类器5:绿色对黄色</li><li id="2e39" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">OvO二元分类器6:蓝色vs黄色</li></ul><p id="f350" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">一对一多类分类所需的分类器数量可以用下面的公式(n为类的数量)来检索:<code class="fe oa ob oc od b">n*(n-1)/2</code></p><p id="9ec8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在<strong class="lb iu">一对一</strong>分类器中，仅涉及创建四个二元分类器:</p><ul class=""><li id="a692" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated">OvR二元分类器1:红色对{绿色、蓝色、黄色}</li><li id="908c" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">OvR二元分类器2:绿色vs {红色、蓝色、黄色}</li><li id="3c64" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">OvR二元分类器3:蓝色vs {红色、绿色、黄色}</li><li id="c181" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">OvR二元分类器4:黄色vs {红色、绿色、蓝色}</li></ul><p id="8d0f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">因此所需的分类器等于类别的数量。这两者都可以通过使用scikit-learn库轻松实现。</p><pre class="kj kk kl km gt on od oo op aw oq bi"><span id="94fb" class="or mz it od b gy os ot l ou ov">from sklearn.svm import SVC<br/>from sklearn.multiclass import OneVsOneClassifier<br/>from sklearn.multiclass import OneVsRestClassifier</span><span id="789f" class="or mz it od b gy ow ot l ou ov"># define model one-vs-one<br/>clf1 = OneVsOneClassifier(SVC());</span><span id="0318" class="or mz it od b gy ow ot l ou ov"># define model one-vs-rest<br/>clf2 = OneVsRestClassifier(SVC());</span></pre><p id="7e56" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">最好的是什么？最好的方法是根据问题的具体情况而定。所以要明智地使用它。尽管如此，我还是会给出一些赞成和反对的意见。</p><p id="e31f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">一对一</strong></p><ul class=""><li id="0d4f" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated">pro不容易在数据集中造成不平衡。</li><li id="09b6" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">con仅对每对类将主数据集分割成一个二元分类。</li></ul><p id="f88a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">一比一休息</strong></p><ul class=""><li id="eb9d" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated">pro需要更少的分类器，使其成为更快的选项。</li><li id="de49" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">con——由于大量的类实例，处理大型数据集具有挑战性。</li></ul><h1 id="c131" class="my mz it bd na nb nc nd ne nf ng nh ni jz nj ka nk kc nl kd nm kf nn kg no np bi translated">5.内核技巧</h1><p id="2b2f" class="pw-post-body-paragraph kz la it lb b lc nq ju le lf nr jx lh li ns lk ll lm nt lo lp lq nu ls lt lu im bi translated">内核是适用于所有SVM氏症的窍门。这是一种优化技术，使SVM更快，并导致高度的普遍性。这是一个非常简单但更强大的概念。</p><p id="784b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">核函数的思想是使操作能够在输入空间中执行，而不是在潜在的高维特征空间中执行。因此，内积不需要在特征空间中计算。</p><p id="fdd1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">迷茫？让我用图表来解释一下。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi om"><img src="../Images/68f3a6c141518700b8e0f8e77e341d8d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*01ZNOIJMZY5RBmZp1MJytw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">从输入空间到特征空间的数据转换(图片由作者提供)</p></figure><p id="e517" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们使用函数(φ)将非线性可分数据转换到高维空间，在该空间中我们可以找到线性可分的决策状态。</p><p id="cf17" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对于线性可分数据，我们已经推导出宽度为W =σαy x<br/>,但是现在使用变换函数，它变成W =σαyφ(x ),最终我们的Q最优宽度函数将是:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ox"><img src="../Images/2f15bf9f8c48a118c70ce3bc93098fda.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kD01NlZwfM-u6oCZHMa5YA.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">新的最佳宽度函数(图片由作者提供)</p></figure><p id="2d8a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这里的问题是我们需要分别计算事物的数量。</p><ul class=""><li id="60fd" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated">φ(xi)</li><li id="9ed8" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">φ(xj)</li><li id="0273" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">两者的点积[φ(xi).φ(xj)]</li></ul><p id="4b7c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为了降低计算复杂度，我们可以引入另一个函数为:</p><pre class="kj kk kl km gt on od oo op aw oq bi"><span id="9e40" class="or mz it od b gy os ot l ou ov">k(xi,xj) =φ(xi)*φ(xj)</span></pre><p id="e59a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">该函数接受原始低维空间中的输入，并返回高维空间中已转换向量的点积。(直接进行转换，而不是显式地将转换应用于<em class="mt"> </em> <strong class="lb iu"> x </strong> i和<strong class="lb iu"> xj </strong>并获得点积)</p><p id="70d1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这就是所谓的<strong class="lb iu">内核技巧</strong>。因此，该函数通过大幅降低计算复杂度，在SVM及其性能中起着关键作用。</p><p id="9986" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对于大型数据集(如-10k、1M……)，您不需要对所有数据点逐一进行转换并获得点积，但您可以简单地将此核函数应用于所选数据点，以获得相同的结果，这将有助于降低复杂性。</p><blockquote class="mq mr ms"><p id="29e5" class="kz la mt lb b lc ld ju le lf lg jx lh mu lj lk ll mv ln lo lp mw lr ls lt lu im bi translated"><strong class="lb iu">重要提示—核函数应该是对称的。</strong></p></blockquote><p id="8809" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">最常用的内核类型列表如下所示。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oy"><img src="../Images/679df617943a201b239ed83ec9e2e7f1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FuB-eiPiwWxS_czoTV6C9A.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">内核类型(图片由作者提供)</p></figure><p id="a20a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">最流行的核函数类型是<strong class="lb iu"> RBF。</strong>因为它具有沿整个x轴的局部和有限响应。然而，需要记住的一个关键点是，当我们将数据映射到一个更高维度时，我们很有可能会过度拟合模型。因此，选择正确的核函数(+选择正确的参数)和正则化非常重要。</p><h1 id="45fd" class="my mz it bd na nb nc nd ne nf ng nh ni jz nj ka nk kc nl kd nm kf nn kg no np bi translated">6.将SVM应用于预处理数据集</h1><p id="e9f2" class="pw-post-body-paragraph kz la it lb b lc nq ju le lf nr jx lh li ns lk ll lm nt lo lp lq nu ls lt lu im bi translated">好吧！我希望我已经涵盖了SVM中的大部分核心概念。让我们回到我们的问题。现在我们需要将SVM模型应用到我们的数据集。编写代码比理解概念要容易得多。但是如果你盲目地应用它，那只是一种浪费。在实现任何模型之前，您需要更好地理解基本概念。我打算写更多关于不同模型的文章，并将它们应用于实际用例。希望它也有助于提高你的知识。</p><p id="a47e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">拿够了。来做实现吧。</p><blockquote class="mq mr ms"><p id="53c8" class="kz la mt lb b lc ld ju le lf lg jx lh mu lj lk ll mv ln lo lp mw lr ls lt lu im bi translated">我假设您已经在我上一篇文章的帮助下完成了PCA之前的所有预处理。</p></blockquote><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oz"><img src="../Images/c11996261e7e9d46263466a27963c3e2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3EfB44hh5-t0E6hmusgdqA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">预处理数据集到PCA(图片由作者提供)</p></figure><p id="829d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">scikit-learn库提供了支持向量分类器来实现SVM模型。</p><pre class="kj kk kl km gt on od oo op aw oq bi"><span id="a0a0" class="or mz it od b gy os ot l ou ov">from sklearn import svm<br/>svc = svm.SVC(kernel='rbf', C=1.2, gamma=0.5)</span></pre><p id="46ba" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">你可以像往常一样得到预测。</p><pre class="kj kk kl km gt on od oo op aw oq bi"><span id="06de" class="or mz it od b gy os ot l ou ov">predictions = svc.predict(X_test_pca)<br/>y_hat = pd.DataFrame(predictions, columns=["predicted"])</span></pre><h1 id="d948" class="my mz it bd na nb nc nd ne nf ng nh ni jz nj ka nk kc nl kd nm kf nn kg no np bi translated">7.准确性测量和评估</h1><p id="cc54" class="pw-post-body-paragraph kz la it lb b lc nq ju le lf nr jx lh li ns lk ll lm nt lo lp lq nu ls lt lu im bi translated">在回归分析中，我们有像均方误差、R值这样的精度指标..等等。但是在分类问题中，分数在某些情况下可能是误导的(例如不平衡的分类问题)</p><p id="1180" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们用总预测中的正确预测来衡量准确性。可能会有这样的情况，一些类完全没有被识别(忽略)，但由于其他类的影响，给出了超过90%的准确率。它们在少数类中占主导地位，并使准确性成为衡量模型性能的不可靠指标。</p><p id="d3ea" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">因此，建议使用以下一些技术，因为它们为所有类别提供了良好的度量，并且是分类任务中最常用的指标。</p><ul class=""><li id="d72a" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated">混淆矩阵</li><li id="4469" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">精确召回和<strong class="lb iu"> F1得分</strong></li><li id="b240" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">AUC - ROC曲线</li></ul><h2 id="fa53" class="or mz it bd na pa pb dn ne pc pd dp ni li pe pf nk lm pg ph nm lq pi pj no pk bi translated">混淆矩阵</h2><p id="2673" class="pw-post-body-paragraph kz la it lb b lc nq ju le lf nr jx lh li ns lk ll lm nt lo lp lq nu ls lt lu im bi translated">混淆矩阵提供了一个更有洞察力的图像，显示了哪些类被正确和错误地预测，最重要的是，显示了正在犯的错误的类型。它克服了单独使用分类精度的局限性。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi om"><img src="../Images/68d6578226472e62dc131e87e99346e5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Z-ZZoonx4FRuHFnOQPUp7w.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">困惑矩阵(图片由作者提供)</p></figure><p id="8b67" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">以下是需要记住的重要术语。(<strong class="lb iu">真</strong>更好，因为模型正确预测了它们)如果这两个类是正和负:</p><ol class=""><li id="4ded" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu nv mb mc md bi translated"><strong class="lb iu">真正值(TP): </strong>实际值是正值，它们被预测为正值</li><li id="0068" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu nv mb mc md bi translated"><strong class="lb iu">真负值(TN): </strong>实际值是负值，它们被预测为负值</li><li id="7f64" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu nv mb mc md bi translated"><strong class="lb iu">误报(FP): </strong>实际值为负值，预测值为正值。(也称为I型错误)</li><li id="cb57" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu nv mb mc md bi translated"><strong class="lb iu">假阴性(FN): </strong>实际值为正，预测值为负。(也称为第二类错误)</li></ol><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pl"><img src="../Images/9d4e0fe7c38a94fedf275356b0780287.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Krni-ve61EP73Bslb2FOvg.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">解读困惑矩阵(图片由作者提供)</p></figure><p id="1ec6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们看看实现。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="pm pn l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi po"><img src="../Images/7a9ee61d9662009c1a4dfce0407f4cfa.png" data-original-src="https://miro.medium.com/v2/resize:fit:764/format:webp/1*1dUxCkSQjBBF0Hcw28sF2w.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">混淆矩阵实现(图片由作者提供)</p></figure><p id="536b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如您所见，一个类(0)得到了非常好的预测，而另一个却没有。这是因为阶级不平衡。将在几分钟后看到如何克服这一点。</p><h2 id="72f6" class="or mz it bd na pa pb dn ne pc pd dp ni li pe pf nk lm pg ph nm lq pi pj no pk bi translated">F1分数</h2><p id="323f" class="pw-post-body-paragraph kz la it lb b lc nq ju le lf nr jx lh li ns lk ll lm nt lo lp lq nu ls lt lu im bi translated">F1分数是精确度和召回率的调和平均值。(传达了精准和召回之间的平衡。)我们先来看看什么是精准和召回。</p><ul class=""><li id="ce06" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated">精确度—从预测的阳性中，有多少是实际阳性</li><li id="978e" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">回忆—从实际阳性中，有多少被正确分类</li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pp"><img src="../Images/42c9c1aff40f42942a63aea8d45d7e6a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xCWVOmGc2ivUzocDVQM2gg.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">方程式(图片由作者提供)</p></figure><p id="c8df" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">F1分数通常比准确性更有用，尤其是如果你的班级分布不均匀。如果假阳性和假阴性具有相似的成本，则准确性最好。如果误报和漏报的代价相差很大，最好同时看精度和召回率。这就是为什么我们使用F1的分数。</p><pre class="kj kk kl km gt on od oo op aw oq bi"><span id="6399" class="or mz it od b gy os ot l ou ov">from sklearn.metrics import f1_score</span><span id="efbe" class="or mz it od b gy ow ot l ou ov">f1_score(y_test,y_hat)</span></pre><p id="5ea9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">F1的分数为1时被认为是完美的，而当分数为0时，这个模型就是一个彻底的失败。Scikit-learn的分类报告精确地提供了所有的分数。</p><pre class="kj kk kl km gt on od oo op aw oq bi"><span id="df9c" class="or mz it od b gy os ot l ou ov">from sklearn.metrics import classification_report</span><span id="49d0" class="or mz it od b gy ow ot l ou ov">print(classification_report(y_test,y_hat))</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pq"><img src="../Images/202b805fb9f9591359afbc1342b6b1c0.png" data-original-src="https://miro.medium.com/v2/resize:fit:932/format:webp/1*OQ85xnRoVqlNs8EecSsXcA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">分类报告(图片由作者提供)</p></figure><blockquote class="mq mr ms"><p id="fa86" class="kz la mt lb b lc ld ju le lf lg jx lh mu lj lk ll mv ln lo lp mw lr ls lt lu im bi translated">对于AUC-ROC曲线，我将单独写一篇文章，因为它需要更深入地解释。(+带有成本函数)但是如果你好奇，你可以参考我的Colab笔记本，因为我把所有代码都放在里面了。</p></blockquote><h1 id="06bd" class="my mz it bd na nb nc nd ne nf ng nh ni jz nj ka nk kc nl kd nm kf nn kg no np bi translated">8.处理阶级不平衡</h1><p id="a24a" class="pw-post-body-paragraph kz la it lb b lc nq ju le lf nr jx lh li ns lk ll lm nt lo lp lq nu ls lt lu im bi translated">你可能会注意到有一个类在预测中占主导地位。那是因为阶级不平衡。我们将类不平衡定义为分类数据集类的比例不均衡。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pr"><img src="../Images/ef1055375974bf4e28493797c6cb7cb2.png" data-original-src="https://miro.medium.com/v2/resize:fit:824/format:webp/1*qeuOxq19TpZUc-HbwqpThQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">目标中的类不平衡(图片由作者提供)</p></figure><p id="10fb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果我们检查数据集，我们可以发现这两个类之间的巨大差异。这在大多数实际情况下都会发生，如垃圾邮件检测、欺诈分析等。为了避免这种情况，我们可以使用不同的技术。我将简单解释两种不同的方法。如果你想进一步了解它，请告诉我，我也会就此写一篇单独的文章。</p><blockquote class="mq mr ms"><p id="79a7" class="kz la mt lb b lc ld ju le lf lg jx lh mu lj lk ll mv ln lo lp mw lr ls lt lu im bi translated">请记住，我们需要将这些技术应用于预处理过的数据集，因为它们也被视为模型。另一件事是，我们只对训练数据进行采样。</p></blockquote><h2 id="617f" class="or mz it bd na pa pb dn ne pc pd dp ni li pe pf nk lm pg ph nm lq pi pj no pk bi translated">过采样</h2><p id="d6cc" class="pw-post-body-paragraph kz la it lb b lc nq ju le lf nr jx lh li ns lk ll lm nt lo lp lq nu ls lt lu im bi translated">通过这样做，它将按照多数类比例对少数类比例进行重新采样。</p><p id="669c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">SMOTE是一种非常著名的过采样技术。(合成少数过采样技术。)它通过利用<strong class="lb iu">k-最近邻</strong>算法来创建合成数据。这就是为什么我们需要在数据预处理之后使用它。</p><p id="cea0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">SMOTE最初从少数类中选取随机数据开始，然后设置数据的k个最近邻。然后在随机数据和随机选择的k-最近邻之间产生合成数据。</p><p id="1d11" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">SMOTE有不同的变体/形式，例如边界SMOTE、SVM SMOTE(使用SVM算法)、自适应合成采样(ADASYN)…等等。让我们应用默认形式的SMOTE。</p><pre class="kj kk kl km gt on od oo op aw oq bi"><span id="474d" class="or mz it od b gy os ot l ou ov">from imblearn.over_sampling import SMOTE</span><span id="1ada" class="or mz it od b gy ow ot l ou ov">sm = SMOTE(random_state=101)<br/>X_train2, y_train2 = sm.fit_resample(X_train_pca, y_train)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ps"><img src="../Images/1f98202aa87abbebc0240e2d816997fb.png" data-original-src="https://miro.medium.com/v2/resize:fit:846/format:webp/1*9jbGPmH48L6rnBYI50VzGg.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">应用SMOTE后(图片由作者提供)</p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pt"><img src="../Images/aed2d3a34101cb60159048ec953813fc.png" data-original-src="https://miro.medium.com/v2/resize:fit:954/format:webp/1*KBCuAR7A1ImIED1OXHh-vg.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">应用SMOTE后的分类报告(图片由作者提供)</p></figure><p id="07be" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">它为少数民族班级带来了一点进步，但是我对结果有点担心，因为我期望得到更高的结果。没关系！总有改进的余地。如果你有时间，尝试一些不同的方法，如SVM SMOTE，并把结果放在评论部分。</p><h2 id="de20" class="or mz it bd na pa pb dn ne pc pd dp ni li pe pf nk lm pg ph nm lq pi pj no pk bi translated">欠采样</h2><p id="0e2d" class="pw-post-body-paragraph kz la it lb b lc nq ju le lf nr jx lh li ns lk ll lm nt lo lp lq nu ls lt lu im bi translated">通过这样做，它将减少多数阶级的比例，直到人数与少数阶级相似。</p><p id="2a94" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">最流行的欠采样技术之一是侥幸成功。它也是基于k-NN算法。它所做的是当属于不同类别的两个点在分布中彼此非常接近时，该算法消除较大类别的数据点，从而试图平衡分布。</p><p id="6ea2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这项技术有三个版本。</p><ul class=""><li id="b9df" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated"><strong class="lb iu">接近失误1 — </strong>通过计算较大分布和三个最接近的较小分布之间的平均最小距离来平衡数据。</li><li id="4b84" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated"><strong class="lb iu">接近失误2 — </strong>通过计算较大分布和三个最远的较小分布之间的平均最小距离来选择示例。</li><li id="e4f3" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated"><strong class="lb iu">接近未命中3 — </strong>考虑较小的类实例，并存储m个邻居。然后取该分布与较大分布之间的距离，并消除最大距离。</li></ul><p id="aed6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">您可以尝试一些不同的方法/排序，并找出最适合您的数据集和模型的方法。这里我使用了版本3和两个最近的邻居。</p><pre class="kj kk kl km gt on od oo op aw oq bi"><span id="3ba9" class="or mz it od b gy os ot l ou ov"># Undersample imbalanced dataset with NearMiss-3<br/>from imblearn.under_sampling import NearMiss</span><span id="e663" class="or mz it od b gy ow ot l ou ov">undersample = NearMiss(version=3, n_neighbors_ver3=2)<br/># transform the dataset</span><span id="3b9a" class="or mz it od b gy ow ot l ou ov">X_train_undermiss, y_train_undermiss  = undersample.fit_resample(X_train_pca, y_train)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pu"><img src="../Images/f1cbc691f97c7d38581e7e8c32acd53e.png" data-original-src="https://miro.medium.com/v2/resize:fit:808/format:webp/1*R29hevLzifGKDGhIf6D3fw.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">应用未遂事故后(图片由作者提供)</p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pv"><img src="../Images/46824b5c1cd4d7ab5b0642c2e87290c3.png" data-original-src="https://miro.medium.com/v2/resize:fit:922/format:webp/1*UTjspYoWvhk2v40IplFLcA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">应用未遂事件后的分类报告(图片由作者提供)</p></figure><p id="76e1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">虽然精确度下降了，但它为两个类别提供了相当好的分类。</p><p id="1d30" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">另一种良好的欠采样技术是Tomek链接(T-Links ),它是压缩最近邻或简称CNN的一种改进。(不要与卷积神经网络混淆)它寻找样本集合的子集，这不会导致模型性能的损失。这种方法比k-最近邻算法更有效，因为它减少了内存需求。</p><pre class="kj kk kl km gt on od oo op aw oq bi"><span id="4987" class="or mz it od b gy os ot l ou ov">from imblearn.under_sampling import TomekLinks<br/>undersample = TomekLinks()<br/>X_train_under, y_train_under  = undersample.fit_resample(X_train_pca, y_train)</span></pre><h1 id="1610" class="my mz it bd na nb nc nd ne nf ng nh ni jz nj ka nk kc nl kd nm kf nn kg no np bi translated">9.超参数调谐</h1><p id="0309" class="pw-post-body-paragraph kz la it lb b lc nq ju le lf nr jx lh li ns lk ll lm nt lo lp lq nu ls lt lu im bi translated">为了找到超参数的最佳配置，我们可以使用<strong class="lb iu"> GridSerch </strong>。这是一种调整技术，用于确定给定模型的最佳值。如你所知，在我们的SVM模型中，我们使用了两个超参数C和γ。但是没有办法预先知道这些超参数的最佳值是什么。</p><p id="3b11" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">所以，我们需要尝试所有可能的值，才能知道最优值。手动完成这项工作需要大量的时间和精力，因此我们使用网格搜索来自动完成这项工作，它会为我们找到最佳参数。</p><pre class="kj kk kl km gt on od oo op aw oq bi"><span id="0490" class="or mz it od b gy os ot l ou ov">from sklearn.model_selection import GridSearchCV<br/>from sklearn.svm import SVC</span><span id="2e65" class="or mz it od b gy ow ot l ou ov"># defining parameter range<br/>param_grid = {'C': [0.1, 1, 10, 100, 1000], <br/>              'gamma': [1, 0.1, 0.01, 0.001, 0.0001],<br/>              'kernel': ['rbf']}</span><span id="6659" class="or mz it od b gy ow ot l ou ov">#grid search<br/>grid = GridSearchCV(SVC(),param_grid,n_jobs=-1,verbose=3,refit = True)</span><span id="c2a7" class="or mz it od b gy ow ot l ou ov"># fitting the model for grid search<br/>grid.fit(X_train_pca, y_train.values.ravel())</span></pre><p id="d2fc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">n_jobs参数允许GridSearch在幕后使用Python的多处理模块。设置n_jobs = -1会导致使用机器上所有可用的内核，这将提高网格搜索性能。</p><blockquote class="mq mr ms"><p id="c34c" class="kz la mt lb b lc ld ju le lf lg jx lh mu lj lk ll mv ln lo lp mw lr ls lt lu im bi translated">注意—在联合实验室中，我们只有2个内核。所以还需要一些时间。在我的情况下，它会持续1个半小时。</p></blockquote><p id="3edb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这里将测试不同的参数组合，并最终找到最佳参数。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pw"><img src="../Images/87af6686093c41f489f604037546997b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FaH6fPPkaisWvbX1SYwqwg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">不同网格的性能+时间分析(图片由作者提供)</p></figure><pre class="kj kk kl km gt on od oo op aw oq bi"><span id="e401" class="or mz it od b gy os ot l ou ov"># print best parameter after tuning<br/>print(grid.best_params_)<br/>  <br/># print how our model looks after hyper-parameter tuning<br/>print(grid.best_estimator_)</span></pre><p id="3237" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">微调T-连锁SVM后的结果如下所示。它在所有实例中提供了最好的准确性，但是在识别第二(1)类时仍然有一些缺点。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi px"><img src="../Images/721ff0f47c38501b4632ebcdb2b69b2c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*z3FhnjMfDeyUMz1fZnySMw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">T型链接的网格搜索优化SVM(图片由作者提供)</p></figure><p id="004c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">最后，我将向您展示这种情况下两种最佳技术的准确性比较。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi py"><img src="../Images/581ebbc71b634fc5bf5209db67e3c756.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fliUkRAjuOt1w5fbIgJ1kg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">最终对比(图片由作者提供)</p></figure><h1 id="9680" class="my mz it bd na nb nc nd ne nf ng nh ni jz nj ka nk kc nl kd nm kf nn kg no np bi translated">10.资源</h1><ul class=""><li id="fdd6" class="lv lw it lb b lc nq lf nr li pz lm qa lq qb lu ma mb mc md bi translated">完整的合作实验室Python笔记本</li></ul><div class="qc qd gp gr qe qf"><a href="https://colab.research.google.com/drive/1EcVY0jQsO6uKzTMlCPK1wt-T6eEYNDzA?usp=sharing" rel="noopener  ugc nofollow" target="_blank"><div class="qg ab fo"><div class="qh ab qi cl cj qj"><h2 class="bd iu gy z fp qk fr fs ql fu fw is bi translated">基于SVM的银行营销数据集分类</h2><div class="qm l"><h3 class="bd b gy z fp qk fr fs ql fu fw dk translated">由Yasas Sandeepa创作</h3></div><div class="qn l"><p class="bd b dl z fp qk fr fs ql fu fw dk translated">colab.research.google.com</p></div></div><div class="qo l"><div class="qp l qq qr qs qo qt ks qf"/></div></div></a></div><ul class=""><li id="deee" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated">前一篇文章</li></ul><div class="qc qd gp gr qe qf"><a rel="noopener follow" target="_blank" href="/build-a-machine-learning-model-to-a-practical-use-case-from-scratch-3a8dc65ab6f1"><div class="qg ab fo"><div class="qh ab qi cl cj qj"><h2 class="bd iu gy z fp qk fr fs ql fu fw is bi translated">从零开始建立一个机器学习模型到实际使用案例</h2><div class="qn l"><p class="bd b dl z fp qk fr fs ql fu fw dk translated">towardsdatascience.com</p></div></div><div class="qo l"><div class="qu l qq qr qs qo qt ks qf"/></div></div></a></div><h1 id="b341" class="my mz it bd na nb nc nd ne nf ng nh ni jz nj ka nk kc nl kd nm kf nn kg no np bi translated">结论</h1><p id="f292" class="pw-post-body-paragraph kz la it lb b lc nq ju le lf nr jx lh li ns lk ll lm nt lo lp lq nu ls lt lu im bi translated">我们在这里讨论了支持向量机背后的原理的深层解释。我们学习了<strong class="lb iu">松弛变量</strong>和<strong class="lb iu">内核技巧</strong>，它们帮助我们高效地解决非线性问题。此外，我们还探索了如何使用这个SVM模型处理<strong class="lb iu">多类分类</strong>。</p><p id="2147" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们通过理解如何从零开始实现SVM和<strong class="lb iu">正确评估模型的准确性度量</strong>来掩盖实际场景。不仅如此，我们还通过修复<strong class="lb iu">类不平衡</strong>和使用<strong class="lb iu">网格搜索</strong>微调我们的模型来增加我们模型的准确性。我们已经看到了许多视觉表现，这有助于我们更好地理解所有的核心概念。</p><p id="69ae" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">所以我希望你对SVM模式及其实施有一个清晰的了解。我会在另一篇有趣的文章中再见到你。在那之前，注意安全！快乐学习！❤️</p></div></div>    
</body>
</html>