<html>
<head>
<title>Data Analysis on Health Passport Tweets</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">健康护照推文的数据分析</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/data-analysis-on-health-passport-tweets-6732660324f3?source=collection_archive---------26-----------------------#2021-09-22">https://towardsdatascience.com/data-analysis-on-health-passport-tweets-6732660324f3?source=collection_archive---------26-----------------------#2021-09-22</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="d77b" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsdatascience.com/tagged/hands-on-tutorials" rel="noopener" target="_blank">实践教程</a></h2><div class=""/><div class=""><h2 id="2c3b" class="pw-subtitle-paragraph jw iz iq bd b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn dk translated">通过自然语言处理和数据可视化，发现关于健康护照的推文必须告诉我们什么</h2></div><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ko"><img src="../Images/f73e31565a1e29a1cccb5ff7a1521735.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XYBZ5teb4-nmyg5iyZ-bPA.jpeg"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated"><a class="ae le" href="https://www.pexels.com/photo/internet-typography-technology-travel-8383888/" rel="noopener ugc nofollow" target="_blank">来源</a></p></figure><p id="7cfd" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">健康护照又称绿色护照、卫生护照或疫苗护照，是2019年开始的<a class="ae le" href="https://en.wikipedia.org/wiki/COVID-19_pandemic" rel="noopener ugc nofollow" target="_blank"> <em class="mb">【新冠肺炎】</em> </a> <em class="mb"> </em>的成果。健康护照在世界范围内引起了广泛的争议。当各国政府认为这是限制病毒传播的解决方案时，许多人和团体坚决反对，认为这是对人权的侵犯。因此，我决定在谈论健康护照的推特上训练我的数据分析技能，特别是自然语言处理(NLP)和数据可视化。</p></div><div class="ab cl mc md hu me" role="separator"><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh"/></div><div class="ij ik il im in"><p id="7b29" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">为了开始这个项目，创建一个<a class="ae le" href="https://developer.twitter.com/en/apply-for-access" rel="noopener ugc nofollow" target="_blank"> <em class="mb"> twitter开发者帐户</em> </a>并获取密钥和令牌以通过twitter检索数据是很重要的。本文不会详细讨论提取tweets的步骤；然而，值得一提的是，提取的推文只有英文，使用的搜索查询如下:</p><pre class="kp kq kr ks gt mj mk ml mm aw mn bi"><span id="a8dd" class="mo mp iq mk b gy mq mr l ms mt">search_query = """ “vaccine passport” OR “vaccine pass” OR <br/>                   “pass sanitaire” OR “sanitary pass” OR <br/>                   “sanitary passport” OR “health pass” OR <br/>                   “covid passport” OR “covid pass” <br/>                   -filter:retweets -filter:media """</span></pre><p id="bfe8" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">该search_query用于搜索包含一个或多个上述关键字的任何推文，不包括转发和媒体推文。</p><p id="d9ba" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">创建的数据集如下所示:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi mu"><img src="../Images/e8ca73c0031f8450bf8758505071e969.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FG_n92eLPNk_q8zvxNPX6A.png"/></div></div></figure><p id="622d" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">对于这个项目，我对以下三个栏目感兴趣:</p><ul class=""><li id="3b00" class="mv mw iq lh b li lj ll lm lo mx ls my lw mz ma na nb nc nd bi translated">user_location栏——查看每个国家的推文数量</li><li id="9a24" class="mv mw iq lh b li ne ll nf lo ng ls nh lw ni ma na nb nc nd bi translated">日期——查看推文的日期</li><li id="5268" class="mv mw iq lh b li ne ll nf lo ng ls nh lw ni ma na nb nc nd bi translated">文本——通过一元、二元和三元模型进行文本分析</li></ul><h1 id="bc7b" class="nj mp iq bd nk nl nm nn no np nq nr ns kf nt kg nu ki nv kj nw kl nx km ny nz bi translated">日期分析</h1><p id="8ccd" class="pw-post-body-paragraph lf lg iq lh b li oa ka lk ll ob kd ln lo oc lq lr ls od lu lv lw oe ly lz ma ij bi translated">日期列的分析是一项相当简单的任务。为了验证推文的日期，使用以下代码行从日期列中删除了时间:</p><pre class="kp kq kr ks gt mj mk ml mm aw mn bi"><span id="4a02" class="mo mp iq mk b gy mq mr l ms mt">tweets['date'] = pd.to_datetime(tweets['date']).dt.date</span></pre><p id="e587" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">然后，使用<a class="ae le" href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.value_counts.html" rel="noopener ugc nofollow" target="_blank"><em class="mb">value _ counts()</em></a>方法显示唯一日期的计数。</p><pre class="kp kq kr ks gt mj mk ml mm aw mn bi"><span id="d138" class="mo mp iq mk b gy mq mr l ms mt">tweets['date'].value_counts()</span></pre><p id="aafe" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated"><em class="mb">输出:</em></p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi of"><img src="../Images/cd2db5b2488a17e7743618c3dfddd0e2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xan9_4kPknK89OVmPYV7aA.png"/></div></div></figure><h1 id="7d65" class="nj mp iq bd nk nl nm nn no np nq nr ns kf nt kg nu ki nv kj nw kl nx km ny nz bi translated">位置分析</h1><p id="3196" class="pw-post-body-paragraph lf lg iq lh b li oa ka lk ll ob kd ln lo oc lq lr ls od lu lv lw oe ly lz ma ij bi translated">位置分析的目的是通过计算每个国家的推文数量来获得推文来源的总体概况。为了实现这一点，有必要对user_location列进行一些预处理。另外，<a class="ae le" href="https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.html" rel="noopener ugc nofollow" target="_blank"><em class="mb">matplotlib . py plot</em></a>和<a class="ae le" href="https://geopandas.org/" rel="noopener ugc nofollow" target="_blank"> <em class="mb"> geopandas </em> </a>用于以饼状图和地理空间图的形式可视化结果。</p><h2 id="3e7d" class="mo mp iq bd nk og oh dn no oi oj dp ns lo ok ol nu ls om on nw lw oo op ny iw bi translated">预处理用户位置</h2><p id="c993" class="pw-post-body-paragraph lf lg iq lh b li oa ka lk ll ob kd ln lo oc lq lr ls od lu lv lw oe ly lz ma ij bi translated">预处理user_location列是从该列中找到的数据中提取国家名称的一项重要任务。</p><p id="5315" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">user_location列中的一些数据没有任何意义，例如'<em class="mb">莱昂内尔·梅西的奖杯室'</em>和'<em class="mb">你在哪里'</em>，因此，所做的第一步是删除任何不是位置的内容。这是通过使用<a class="ae le" href="https://spacy.io/" rel="noopener ugc nofollow" target="_blank"> <em class="mb">空间</em> </a>库的命名实体识别实现的。</p><pre class="kp kq kr ks gt mj mk ml mm aw mn bi"><span id="f988" class="mo mp iq mk b gy mq mr l ms mt"><em class="mb"># NLP<br/></em>import spacy<br/>nlp = spacy.load('en_core_web_sm')</span><span id="e94d" class="mo mp iq mk b gy oq mr l ms mt"><em class="mb"># create a list of raw locations - i.e. locations entered by users<br/></em>raw_locations = tweets.user_location.unique().tolist()</span><span id="076a" class="mo mp iq mk b gy oq mr l ms mt"><em class="mb"># replace nan by "" - the first element of the list is nan<br/></em>raw_locations[0] = ""</span><span id="5f7c" class="mo mp iq mk b gy oq mr l ms mt"><em class="mb"># locations list will only include relevant locations<br/></em>locations = []</span><span id="6be9" class="mo mp iq mk b gy oq mr l ms mt"><em class="mb"># search for relevant locations and add them to the locations list<br/></em>for loc in raw_locations:<br/>  text = ""<br/>  loc_ = nlp(loc)<br/>  for ent in loc_.ents:<br/>    if ent.label_ == "GPE":<br/>      text = text + " " + ent.text<br/>      locations.append(text)<br/>    else:<br/>      continue</span></pre><p id="f1df" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">上述代码用于删除所有在位置方面没有意义的内容；然而，user_location列不仅包括国家名称，还包括城市和州，例如'<em class="mb"> London' </em>或'<em class="mb"> New York，NY '。</em>因此，我决定使用<a class="ae le" href="https://geopy.readthedocs.io/en/stable/" rel="noopener ugc nofollow" target="_blank"> <em class="mb"> geopy </em> </a>库从城市和州名中获取国名。</p><pre class="kp kq kr ks gt mj mk ml mm aw mn bi"><span id="a9d3" class="mo mp iq mk b gy mq mr l ms mt"><em class="mb"># Geocoding Webservices<br/></em>from geopy.geocoders import Nominatim</span><span id="ca04" class="mo mp iq mk b gy oq mr l ms mt"><em class="mb"># NLP<br/></em>import spacy<br/>nlp = spacy.load('en_core_web_sm')</span><span id="e143" class="mo mp iq mk b gy oq mr l ms mt"><em class="mb"># Get the country name from cities' and states' names<br/></em>countries = []</span><span id="c845" class="mo mp iq mk b gy oq mr l ms mt">for loc in locations:<br/>  geolocator = Nominatim(user_agent = "geoapiExercises")<br/>  location = geolocator.geocode(loc, timeout=10000)<br/>  <br/>  if location == None:<br/>    countries.append(loc)<br/>    continue<br/>  <br/>  location_ = nlp(location.address)</span><span id="c495" class="mo mp iq mk b gy oq mr l ms mt">  if "," in location_.text:<br/><em class="mb">    # Example of a location_.text: "New York, United States"<br/>    # get the name after the last ","<br/></em>    countries.append(location_.text.split(",")[-1])<br/>  else:<br/>    countries.append(location.address)</span></pre><p id="6b63" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">为了显示国家名称，我使用了<a class="ae le" href="https://numpy.org/doc/stable/reference/generated/numpy.unique.html" rel="noopener ugc nofollow" target="_blank"> <em class="mb"> np.unique() </em> </a>方法。</p><pre class="kp kq kr ks gt mj mk ml mm aw mn bi"><span id="62e3" class="mo mp iq mk b gy mq mr l ms mt">np.unique(np.array(countries))</span></pre><p id="6e66" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated"><em class="mb">输出:</em></p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi or"><img src="../Images/20d8285cea61f164fdebc17b8182d51f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EC-sFQvqtnfDus26BW-bJg.png"/></div></div></figure><p id="2138" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">你可以明显地注意到，有些结果是用英语以外的语言显示的，有些是用“/”或“-”分隔的多种语言显示的。另外，一些结果仍然没有指示国家名称，例如'<em class="mb">多伦多哈尔格萨'</em>和'<em class="mb">底特律拉斯维加斯'</em>。我通过删除文本中的“/”和“-”来解决这些问题，并使用这些图标后的姓氏。我还用相关的国家名称手动替换了一些地名。最后，我使用了<a class="ae le" href="https://pypi.org/project/googletrans/" rel="noopener ugc nofollow" target="_blank"> <em class="mb"> googletrans </em> </a>库，将非英文的国家名称自动翻译成英文。请注意，我保留了包含不同国家城市的位置(例如<em class="mb">‘London Bxl Paris’)</em>)不变。以下是上述步骤的完整代码:</p><pre class="kp kq kr ks gt mj mk ml mm aw mn bi"><span id="f5b7" class="mo mp iq mk b gy mq mr l ms mt"><em class="mb"># Translation</em><br/>from googletrans import Translator</span><span id="cf3a" class="mo mp iq mk b gy oq mr l ms mt"><em class="mb"># get the last name only when "/" or "-" is found <br/># "/" and "-" separates names in different languages<br/></em>countries = [country.split("/")[-1] for country in countries]<br/>countries = [country.split("-")[-1] for country in countries]</span><span id="b12c" class="mo mp iq mk b gy oq mr l ms mt"><em class="mb"># remove white space found at the beginning of a string</em><br/>countries = [country[1:] if country[0] == " " else country for country in countries]</span><span id="8534" class="mo mp iq mk b gy oq mr l ms mt"><em class="mb"># Manually replace locations to their relevant country name</em><br/>countries = ['United States' if country in <br/>                ["LA", "Detroit Las Vegas", "Atlanta Seattle"] <br/>                   else country for country in countries]</span><span id="c4c0" class="mo mp iq mk b gy oq mr l ms mt">countries = ['Canada' if country in<br/>                ["Calgary Mohkinstis", "Calgary Mohkinstis Alberta"]<br/>                   else country for country in countries]</span><span id="2a01" class="mo mp iq mk b gy oq mr l ms mt"><em class="mb"># translate countries in foreign language to english</em><br/>translator = Translator()<br/>countries = [translator.translate(country).text <br/>              for country in countries]</span><span id="d3a5" class="mo mp iq mk b gy oq mr l ms mt">print(len(countries))<br/>print(np.unique(np.array(countries)))</span></pre><p id="b403" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated"><em class="mb">输出:</em></p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi os"><img src="../Images/9cf23a8e286b5675e739c4d5c784702d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*E0U-VZpiS29PsBgJjZbBEA.png"/></div></div></figure><p id="fc40" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">不幸的是，仍然有一些国家的名称没有被正确翻译，因此，我不得不手动用它们的英文版本名称来替换它们。</p><pre class="kp kq kr ks gt mj mk ml mm aw mn bi"><span id="e536" class="mo mp iq mk b gy mq mr l ms mt"><em class="mb"># for those that were not translated properly add them manually<br/># unknown to be added to others later: joke To L Poo l  ދިވެހިރާއްޖެ</em></span><span id="16e1" class="mo mp iq mk b gy oq mr l ms mt">countries = ['Germany'  if country == "Deutschland" <br/>               else country for country in countries]</span><span id="8ab5" class="mo mp iq mk b gy oq mr l ms mt">countries = ['Spain'  if country == "España" <br/>               else country for country in countries]</span><span id="ca03" class="mo mp iq mk b gy oq mr l ms mt">countries = ['Iceland'  if country == "Ísland" <br/>               else country for country in countries]</span><span id="5625" class="mo mp iq mk b gy oq mr l ms mt">countries = ['Greece'  if country == "Ελλάς" <br/>               else country for country in countries]</span><span id="1390" class="mo mp iq mk b gy oq mr l ms mt">countries = ['Ukraine'  if country == "Україна" <br/>               else country for country in countries]</span><span id="b2f7" class="mo mp iq mk b gy oq mr l ms mt">countries = ['Iran'  if country == "ایران" <br/>               else country for country in countries]</span><span id="8a6e" class="mo mp iq mk b gy oq mr l ms mt">countries = ['Japan'  if country == "日本" <br/>               else country for country in countries]</span><span id="fe7d" class="mo mp iq mk b gy oq mr l ms mt">countries = ['Svizra'  if country == "Switzerland" <br/>               else country for country in countries]</span><span id="672d" class="mo mp iq mk b gy oq mr l ms mt">countries = ['Polska'  if country == "Poland" <br/>               else country for country in countries]</span><span id="07a1" class="mo mp iq mk b gy oq mr l ms mt">countries = ["The Democratic People's Republic of Korea"  <br/>               if country == "조선민주주의인민공화국" <br/>                 else country for country in countries]</span></pre><p id="727b" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">最后，我创建了两个字典:1) <em class="mb"> countries_values </em>字典，它将所有国家的名称存储为键，将每个国家的推文数量存储为值；2)<em class="mb">main _ countries</em>字典，它存储推文数量最多的国家，并将所有其他国家归入名为<em class="mb">【其他】</em>的键。</p><pre class="kp kq kr ks gt mj mk ml mm aw mn bi"><span id="524d" class="mo mp iq mk b gy mq mr l ms mt">from collections import Counter</span><span id="0db8" class="mo mp iq mk b gy oq mr l ms mt"><em class="mb"># Use Counter to create a dictionary with all countries <br/># and their equivalent number of tweets<br/></em>countries_values = Counter(countries)</span><span id="c8d4" class="mo mp iq mk b gy oq mr l ms mt"><em class="mb"># Create dictionary of the countries having the most tweets <br/># "others" key represent all other countries<br/></em>main_countries = {'others': 0}<br/>other_countries = []<br/>for key, val in countries_values.items():<br/>  if val &gt;= 20:<br/>    main_countries[key] = val<br/>  else:<br/>    main_countries["others"] += val<br/>    other_countries.append(key)</span></pre><h2 id="8b5e" class="mo mp iq bd nk og oh dn no oi oj dp ns lo ok ol nu ls om on nw lw oo op ny iw bi translated">使用matplotlib.pyplot和geopandas绘制结果</h2><p id="1c1f" class="pw-post-body-paragraph lf lg iq lh b li oa ka lk ll ob kd ln lo oc lq lr ls od lu lv lw oe ly lz ma ij bi translated">在预处理user_location列以获得作为国家名称的位置后，我决定使用<a class="ae le" href="https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.html" rel="noopener ugc nofollow" target="_blank"><em class="mb">matplotlib . py plot</em></a>和<a class="ae le" href="https://geopandas.org/" rel="noopener ugc nofollow" target="_blank"> <em class="mb"> geopandas </em> </a>以地图和饼状图的形式显示结果。绘制饼图时，只需使用之前创建的字典<em class="mb">‘main _ countries’</em>即可。但是，要显示显示每个国家的tweets数量的地图，创建地理数据框是很重要的。</p><p id="8f1e" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated"><strong class="lh ja"> <em class="mb">创建地理数据框架</em> </strong></p><p id="1ec8" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">创建地理数据框架的第一步是从存储每个国家的iso_alpha3代码的<em class="mb">‘countries _ values’</em>字典创建数据框架。<a class="ae le" href="https://pypi.org/project/pycountry/" rel="noopener ugc nofollow" target="_blank"> <em class="mb"> pycountry </em> </a>库用于获取iso代码。</p><pre class="kp kq kr ks gt mj mk ml mm aw mn bi"><span id="2c71" class="mo mp iq mk b gy mq mr l ms mt"><em class="mb"># Library to get the iso codes of the countries <br/></em>import pycountry</span><span id="a9e5" class="mo mp iq mk b gy oq mr l ms mt"><em class="mb"># create a DataFrame of country names and their codes<br/></em>df_countries = pd.DataFrame()<br/>df_countries["country_name"] = list(countries_values.keys())<br/>df_countries["country_value"] = list(countries_values.values())</span><span id="964d" class="mo mp iq mk b gy oq mr l ms mt">def get_cntry_code(column):<br/>  CODE=[]<br/>  for country in column:<br/>    try:<br/>      code=pycountry.countries.get(name=country).alpha_3<br/><em class="mb">      # .alpha_3 means 3-letter country code<br/>      # .alpha_2 means 2-letter country code<br/>      </em>CODE.append(code)<br/>    except:<br/>      CODE.append('None')<br/>  return CODE</span><span id="291d" class="mo mp iq mk b gy oq mr l ms mt"><em class="mb"># create a column for code<br/></em>df_countries["country_code"] = <br/>  get_cntry_code(df_countries.country_name)</span><span id="0362" class="mo mp iq mk b gy oq mr l ms mt">df_countries.head()</span></pre><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ot"><img src="../Images/0b5a11cf306e3905a4c924b0594541ef.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ax9AgTllguGtVi5LzRhqbQ.png"/></div></div></figure><p id="e3bc" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">第二步是使用<a class="ae le" href="https://geopandas.org/" rel="noopener ugc nofollow" target="_blank"> <em class="mb"> geopandas </em> </a>库加载世界地理数据框架，如下所示:</p><pre class="kp kq kr ks gt mj mk ml mm aw mn bi"><span id="fa88" class="mo mp iq mk b gy mq mr l ms mt"><em class="mb"># Geospatial Data<br/></em>import geopandas</span><span id="291c" class="mo mp iq mk b gy oq mr l ms mt">world = geopandas.read_file(<br/>  geopandas.datasets.get_path('naturalearth_lowres'))</span><span id="ef3f" class="mo mp iq mk b gy oq mr l ms mt">world</span></pre><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ou"><img src="../Images/670ba09d273148067136cd5dc578cd2b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LP4MGRfjy5ovgPsQmMEDwg.png"/></div></div></figure><p id="3289" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">值得注意的是，一些国家的iso_a3被设置为-99，因此，我不得不手动更新代码，如下所示:</p><pre class="kp kq kr ks gt mj mk ml mm aw mn bi"><span id="8237" class="mo mp iq mk b gy mq mr l ms mt">world.loc[world['name'] == 'France', 'iso_a3'] = 'FRA'<br/>world.loc[world['name'] == 'Norway', 'iso_a3'] = 'NOR'<br/>world.loc[world['name'] == 'N. Cyprus', 'iso_a3'] = 'CYP'<br/>world.loc[world['name'] == 'Somaliland', 'iso_a3'] = 'SOM'<br/>world.loc[world['name'] == 'Kosovo', 'iso_a3'] = 'RKS'</span></pre><p id="df42" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">为了最终确定地理数据框架，世界地理数据框架和国家数据框架<em class="mb"> (countries_df </em>)被合并到“<em class="mb">国家代码</em>”(即“iso_a3”)。</p><pre class="kp kq kr ks gt mj mk ml mm aw mn bi"><span id="1bf5" class="mo mp iq mk b gy mq mr l ms mt"><em class="mb"># rename columns to merge the DataFrames<br/></em>world = world.rename(columns={"iso_a3": "country_code"})</span><span id="0631" class="mo mp iq mk b gy oq mr l ms mt">df_merged = pd.merge(world, df_countries, <br/>                      on="country_code", how='outer')</span><span id="5175" class="mo mp iq mk b gy oq mr l ms mt"><em class="mb"># fill empty value with zero<br/># any country with no tweet will have a value of 0<br/></em>df_merged.country_value = df_merged.country_value.fillna(0)</span></pre><p id="c9dd" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated"><strong class="lh ja"> <em class="mb">绘制地图和饼状图</em> </strong></p><p id="35e7" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">下面的代码显示了绘制地图和饼图的步骤。</p><pre class="kp kq kr ks gt mj mk ml mm aw mn bi"><span id="988a" class="mo mp iq mk b gy mq mr l ms mt"><em class="mb"># Geospatial Data<br/></em>import geopandas<br/><em class="mb"># Data Visualisation<br/></em>import matplotlib.pyplot as plt<br/>from mpl_toolkits.axes_grid1 import make_axes_locatable</span><span id="755c" class="mo mp iq mk b gy oq mr l ms mt">fig = plt.figure(figsize=(15,10), facecolor='#eef3f8')</span><span id="71db" class="mo mp iq mk b gy oq mr l ms mt">ax1 = fig.add_axes([0, 0, 1, 1]) <em class="mb"># the map</em><br/>ax2 = fig.add_axes([0, 0.1, 0.2, 0.2]) <em class="mb"># the pie chart</em></span><span id="f970" class="mo mp iq mk b gy oq mr l ms mt">divider = divider = make_axes_locatable(ax1)<br/>cax = divider.append_axes("right", size="2%", pad=0) <em class="mb"># legend of map</em></span><span id="3883" class="mo mp iq mk b gy oq mr l ms mt"><em class="mb">### MAP ###<br/></em>df_merged = df_merged[(df_merged.name != "Antarctica") &amp; <br/>                       (df_merged.name != "Fr. S. Antarctic Lands")]</span><span id="422d" class="mo mp iq mk b gy oq mr l ms mt">df_merged.to_crs(epsg=4326, inplace=True)</span><span id="5099" class="mo mp iq mk b gy oq mr l ms mt">df_merged.plot(column='country_value', cmap='Greens', <br/>                linewidth=1.0, ax=ax1, edgecolor='0.8', <br/>                  legend=True, cax=cax)</span><span id="6b64" class="mo mp iq mk b gy oq mr l ms mt"><em class="mb"># remove axis<br/></em>ax1.axis('off')</span><span id="0db6" class="mo mp iq mk b gy oq mr l ms mt"><em class="mb"># add a title of the map<br/></em>font_t1 = {'fontsize':'20', 'fontweight':'bold', 'color':'#065535'}</span><span id="f359" class="mo mp iq mk b gy oq mr l ms mt">ax1.set_title('NUMBER OF TWEETS PER COUNTRY', <br/>                fontdict=font_t1, pad=24)</span><span id="0387" class="mo mp iq mk b gy oq mr l ms mt"><em class="mb">### PIE CHART ###<br/></em>total = len(countries) <em class="mb"># 1241</em></span><span id="2a78" class="mo mp iq mk b gy oq mr l ms mt"><em class="mb"># Sort the dictionary - this is only for the sake of visualisation<br/></em>sorted_main_countries = dict(sorted(main_countries.items(),<br/>                              key=lambda x: x[1]))</span><span id="f0b2" class="mo mp iq mk b gy oq mr l ms mt"><em class="mb"># get labels for pie chart<br/></em>labels = list(sorted_main_countries.keys())</span><span id="eb66" class="mo mp iq mk b gy oq mr l ms mt"><em class="mb"># get percentages for pie chart<br/></em>percentages = [(val/total)*100 <br/>                 for val in list(sorted_main_countries.values())]</span><span id="423d" class="mo mp iq mk b gy oq mr l ms mt"><em class="mb"># get theme colors to be coherent with that of the map's colors<br/></em>theme = plt.get_cmap('Greens')<br/>ax2.set_prop_cycle("color", [theme(1. * i / len(percentages)) <br/>                              for i in range(len(percentages))])</span><span id="896c" class="mo mp iq mk b gy oq mr l ms mt">wedgeprops = {'linewidth':1, 'edgecolor':'white'}</span><span id="04c7" class="mo mp iq mk b gy oq mr l ms mt">_, texts, autotexts = ax2.pie(percentages, labels=labels, <br/>                               labeldistance=1.1, autopct='%.0f%%',<br/>                                 radius=1.8, wedgeprops=wedgeprops)</span><span id="2c66" class="mo mp iq mk b gy oq mr l ms mt"><em class="mb"># set color for the autotext<br/></em>for auto in autotexts:<br/>  auto.set_color('black')</span><span id="502a" class="mo mp iq mk b gy oq mr l ms mt"><em class="mb"># set color of the labels<br/></em>for text in texts:<br/>  text.set_color('#909994') </span><span id="a745" class="mo mp iq mk b gy oq mr l ms mt"><em class="mb"># add a title for the pie chart<br/></em>font_t2 = {'fontsize':'12', 'fontweight':'bold', 'color':'#065535'}</span><span id="238e" class="mo mp iq mk b gy oq mr l ms mt">ax2.set_title('Percent of Tweets by Country', fontdict=font_t2,<br/>                pad=60, style='oblique')</span><span id="75f4" class="mo mp iq mk b gy oq mr l ms mt"><em class="mb"># save figure<br/></em>plt.savefig("no_of_tweets_per_country.png", <br/>              bbox_inches = 'tight', facecolor='#eef3f8')</span><span id="a0d8" class="mo mp iq mk b gy oq mr l ms mt">plt.show()</span></pre><p id="3b42" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated"><em class="mb">最终结果:</em></p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ov"><img src="../Images/9c402e5c2bbe1bffb729708440c28fab.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GV7J22db4S2Bb67HFoi7pA.png"/></div></div></figure><p id="5fd8" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">结果表明，大多数推文来自英语国家<em class="mb">(美国、英国、加拿大、澳大利亚和南非)</em>。这背后的原因可能是因为收集的推文只有英文。如果选择其他语言，结果可能会有所不同。法国是应用卫生通行证最严格的欧洲国家之一，反对“卫生通行证”概念的示威活动经常发生，这可以解释为什么法国在不是英语国家的情况下仍名列前茅。</p><h1 id="5bd6" class="nj mp iq bd nk nl nm nn no np nq nr ns kf nt kg nu ki nv kj nw kl nx km ny nz bi translated">文本分析</h1><p id="0102" class="pw-post-body-paragraph lf lg iq lh b li oa ka lk ll ob kd ln lo oc lq lr ls od lu lv lw oe ly lz ma ij bi translated">文本分析的重点是获取推文的一元词、二元词和三元词。单字通过世界云可视化，二元和三元显示在条形图上。这些都是通过应用<a class="ae le" rel="noopener" target="_blank" href="/tf-idf-for-document-ranking-from-scratch-in-python-on-real-world-dataset-796d339a4089"> TF_IDF </a>(词频-逆文档频率)实现的。我们的目标是一瞥推特中最常用的术语，因此，希望能帮助我们总体上理解推特用户的立场。</p><h2 id="891a" class="mo mp iq bd nk og oh dn no oi oj dp ns lo ok ol nu ls om on nw lw oo op ny iw bi translated">文本预处理</h2><p id="a02d" class="pw-post-body-paragraph lf lg iq lh b li oa ka lk ll ob kd ln lo oc lq lr ls od lu lv lw oe ly lz ma ij bi translated">在获取一元词、二元词和三元词之前，进行文本预处理是很重要的。下面，我列出了预处理文本所采取的步骤以及本节末尾显示的完整代码</p><p id="195e" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">1-转换为小写</p><p id="7557" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">2-删除URL</p><p id="bbc9" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">3-将俚语转换成它们原来的形式(为了实现这一点，我不得不抓取这个<a class="ae le" href="https://www.noslang.com/dictionary/1/" rel="noopener ugc nofollow" target="_blank"> <em class="mb">网站</em></a>——网页抓取的细节将不在本文中提及)</p><p id="55ed" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">4-删除提及</p><p id="891b" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">5-删除标点符号</p><p id="fb4d" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">6-引理</p><p id="0728" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">7-删除停用词</p><p id="6093" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">8-去掉数字(表情符号翻译成数字。为了避免进入前n个字母的数字，我决定去掉数字。然而，人们可以决定保留它们并使用表情符号作为分析的一部分)</p><p id="2eb5" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">9-删除国家和城市的名字(为了避免国家或城市的名字出现在前n个字母中，我决定将它们从推文中删除)</p><p id="f742" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated"><strong class="lh ja"> <em class="mb">文本预处理的全部代码:</em> </strong></p><pre class="kp kq kr ks gt mj mk ml mm aw mn bi"><span id="d605" class="mo mp iq mk b gy mq mr l ms mt"><em class="mb"># NLP<br/></em>import spacy<br/>import regex as re<br/>import string</span><span id="5a68" class="mo mp iq mk b gy oq mr l ms mt">nlp = spacy.load('en_core_web_sm')</span><span id="096b" class="mo mp iq mk b gy oq mr l ms mt">tweets["text_processed"] = ""</span><span id="5b5d" class="mo mp iq mk b gy oq mr l ms mt"><em class="mb"># convert to lower case<br/></em>tweets["text_processed"] = tweets["text"].apply(lambda x:<br/>                             str.lower(x))</span><span id="af81" class="mo mp iq mk b gy oq mr l ms mt"><em class="mb"># remove URL<br/></em>tweets["text_processed"] = tweets["text_processed"].apply(lambda x:<br/>                             re.sub(<br/>                              r"(?:\@|http?\://|https?\://|www)\S+",<br/>                                  ' ', x))</span><span id="21f0" class="mo mp iq mk b gy oq mr l ms mt"><em class="mb"># convert slang<br/># load slang dictionary<br/></em>with open('slangdict.json') as json_file:<br/>  slang_dict=json.load(json_file)</span><span id="3750" class="mo mp iq mk b gy oq mr l ms mt">for i in range(len(tweets["text_processed"])):<br/>  txt = ""<br/>  doc = nlp(tweets["text_processed"].iloc[i])<br/>  <br/>  for token in doc:<br/>    if token.text in list(slang_dict.keys()):<br/>      txt = txt + " " + slang_dict[token.text]<br/>    else:<br/>      txt = txt + " " + token.text<br/>  tweets["text_processed"].iloc[i] = txt</span><span id="2389" class="mo mp iq mk b gy oq mr l ms mt"><em class="mb"># remove mentions<br/></em>def remove_entities(text, entity_list):</span><span id="4b6a" class="mo mp iq mk b gy oq mr l ms mt">  for separator in string.punctuation:<br/>    if separator not in entity_list:<br/>      text = text.replace(separator,' ')<br/>  words = []<br/>  for word in text.split():<br/>    word = word.strip()<br/>    if word:<br/>      if word[0] not in entity_list:<br/>        words.append(word)</span><span id="416a" class="mo mp iq mk b gy oq mr l ms mt">  return ' '.join(words)</span><span id="d2bd" class="mo mp iq mk b gy oq mr l ms mt">tweets["text_processed"] = tweets["text_processed"].apply(lambda x:<br/>                             remove_entities(x, ["@"]))</span><span id="8a3a" class="mo mp iq mk b gy oq mr l ms mt"><em class="mb"># remove punctuation<br/></em>tweets["text_processed"] = tweets["text_processed"].apply(lambda x:<br/>                             re.sub(r'[^\w\s]',' ', x))</span><span id="13b6" class="mo mp iq mk b gy oq mr l ms mt"><em class="mb"># lemmatize<br/></em>def lemmatize(sentence):</span><span id="0af8" class="mo mp iq mk b gy oq mr l ms mt">  doc = nlp(sentence) <em class="mb"># tokenize the text and produce a Doc Object</em><br/>  lemmas = [token.lemma_ for token in doc]<br/>  <br/>  return " ".join(lemmas)</span><span id="4430" class="mo mp iq mk b gy oq mr l ms mt">tweets["text_processed"] = tweets["text_processed"].apply(lambda x:<br/>                             lemmatize(x))</span><span id="4038" class="mo mp iq mk b gy oq mr l ms mt"><em class="mb"># remove stopwords<br/></em>def remove_stopwords(sentence):</span><span id="1576" class="mo mp iq mk b gy oq mr l ms mt">  doc = nlp(sentence) <em class="mb"># tokenize the text and produce a Doc Object<br/>  </em>all_stopwords = nlp.Defaults.stop_words<br/>  doc_tokens = [token.text for token in doc]<br/>  tokens_without_sw = [word for word in doc_tokens <br/>                        if not word in all_stopwords]<br/>  <br/>  return " ".join(tokens_without_sw)</span><span id="e9b0" class="mo mp iq mk b gy oq mr l ms mt">tweets["text_processed"] = tweets["text_processed"].apply(lambda x:<br/>                             remove_stopwords(x))</span><span id="b5a6" class="mo mp iq mk b gy oq mr l ms mt"><em class="mb"># remove -PRON- a result of lemmatization<br/></em>tweets["text_processed"] = tweets["text_processed"].apply(lambda x:<br/>                             re.sub('-PRON-', " ", x))</span><span id="3436" class="mo mp iq mk b gy oq mr l ms mt"><em class="mb"># remove numbers<br/></em>tweets["text_processed"] = tweets["text_processed"].apply(lambda x:<br/>                             re.sub(r'[0-9]', " ", x))</span><span id="3393" class="mo mp iq mk b gy oq mr l ms mt"><em class="mb"># remove country and city names<br/></em>def remove_country_city(sentence):</span><span id="9ca5" class="mo mp iq mk b gy oq mr l ms mt">  doc = nlp(sentence)</span><span id="d2a5" class="mo mp iq mk b gy oq mr l ms mt">  return (" ".join([ent.text for ent in doc if not ent.ent_type_]))</span><span id="efb3" class="mo mp iq mk b gy oq mr l ms mt">tweets["text_processed"] = tweets["text_processed"].apply(lambda x:<br/>                             remove_country_city(x) <br/>                               if pd.isna(x) != True else x)</span></pre><p id="8c7c" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated"><strong class="lh ja"> <em class="mb">从单字创建词云</em> </strong></p><p id="c31f" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">如下所示，我决定创建一个世界云来显示推文中最突出的单词。我通过使用<a class="ae le" href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html" rel="noopener ugc nofollow" target="_blank"><em class="mb">TfidfVectorizer</em></a><em class="mb">，</em>实现了这一点，其中默认的ngram_range是(1，1)，意思是unigram。</p><pre class="kp kq kr ks gt mj mk ml mm aw mn bi"><span id="56a4" class="mo mp iq mk b gy mq mr l ms mt">from PIL import Image<br/>import numpy as np<br/>from sklearn.feature_extraction.text import TfidfVectorizer<br/>import matplotlib.pyplot as plt<br/>from wordcloud import WordCloud</span><span id="ac7b" class="mo mp iq mk b gy oq mr l ms mt">vectorizor = TfidfVectorizer(stop_words='english')<br/>vecs = vectorizor.fit_transform(tweets.text_processed)<br/>feature_names = vectorizor.get_feature_names()<br/>dense = vecs.todense()<br/>l = dense.tolist()<br/>df = pd.DataFrame(l, columns=feature_names)</span><span id="40e0" class="mo mp iq mk b gy oq mr l ms mt"><em class="mb"># mask is the image used to reshape the cloud<br/></em>mask = np.array(Image.open('./images/syringe44_.jpeg'))</span><span id="0071" class="mo mp iq mk b gy oq mr l ms mt">word_cloud = WordCloud(collocations=False, background_color='white',<br/>                         max_words=200, width=3000,<br/>                           height=2000, colormap='viridis',<br/>                             mask=mask).generate_from_frequencies(<br/>                                                   df.T.sum(axis=1))</span><span id="6ca7" class="mo mp iq mk b gy oq mr l ms mt">plt.figure(figsize=[15,10])<br/>plt.imshow(word_cloud)<br/>plt.axis("off")<br/>plt.show()</span><span id="d167" class="mo mp iq mk b gy oq mr l ms mt">word_cloud.to_file("tfidf_ps_word_cloud.png")</span></pre><p id="44cd" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated"><em class="mb">词云:</em></p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ow"><img src="../Images/c345167ab1012fd567241c9a512457ad.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*G0XMG5X_m2tl8GWzCH7FVg.png"/></div></div></figure><p id="7c46" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">不出所料，<em class="mb">【护照】</em><em class="mb">【疫苗】</em><em class="mb">【接种】</em><em class="mb">【通行证】</em><em class="mb">【covid】</em>等词汇非常丰富。这很正常，因为这是推文的主题；此外，提取tweets时的搜索查询集中在这些关键字上。然而，如果我们更仔细地观察单词cloud，我们可以注意到对进一步分析有用的其他感兴趣的单词，如<em class="mb">‘抗议’</em><em class="mb">【停止】</em><em class="mb">【强制】</em><em class="mb">【拒绝】</em><em class="mb">【正确】</em>和<em class="mb">【强制】</em>。</p><p id="1355" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">在Unigrams中，每个单词的出现被认为是独立于它前面的那个单词的，这并不总是使它成为文本分析的最佳选择。因此，我决定进一步检查二元模型和三元模型，看看它们是否能提供更多的见解。</p><p id="f53c" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated"><strong class="lh ja"> <em class="mb">获取并可视化二元模型和三元模型</em> </strong></p><p id="b7f8" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">获取一元模型、二元模型和三元模型的步骤非常相似。唯一需要更新的参数是<a class="ae le" href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html" rel="noopener ugc nofollow" target="_blank"><em class="mb">TfidfVectorizer</em></a>中的ngram_range，其中对于单个字符，它是(1，1)(默认值)，对于双字符，它是(2，2)，对于三字符，它是(3，3)。</p><pre class="kp kq kr ks gt mj mk ml mm aw mn bi"><span id="b79a" class="mo mp iq mk b gy mq mr l ms mt">from sklearn.feature_extraction.text import TfidfVectorizer</span><span id="ab5a" class="mo mp iq mk b gy oq mr l ms mt"><em class="mb"># bigrams<br/></em>vectorizor = TfidfVectorizer(stop_words='english', <br/>                              ngram_range =(2, 2))</span><span id="8c53" class="mo mp iq mk b gy oq mr l ms mt">vecs = vectorizor.fit_transform(tweets.text_processed)<br/>feature_names = vectorizor.get_feature_names()<br/>dense = vecs.todense()<br/>l = dense.tolist()<br/>df = pd.DataFrame(l, columns=feature_names)</span><span id="d4d6" class="mo mp iq mk b gy oq mr l ms mt">n_grams = df.T.sum(axis=1).sort_values(ascending=False)</span><span id="a120" class="mo mp iq mk b gy oq mr l ms mt">n_grams.to_csv("bigrams.csv")</span></pre><p id="b269" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">我用一个条形图来显示前100个二元模型和三元模型。<a class="ae le" href="https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.html" rel="noopener ugc nofollow" target="_blank"><em class="mb">matplotlib . py plot</em></a>和<a class="ae le" href="https://seaborn.pydata.org/" rel="noopener ugc nofollow" target="_blank"> <em class="mb"> seaborn </em> </a>库对于这种可视化是必不可少的。以下是可视化数据的代码示例:</p><pre class="kp kq kr ks gt mj mk ml mm aw mn bi"><span id="fbea" class="mo mp iq mk b gy mq mr l ms mt"><em class="mb"># read bigram dataset<br/></em>bigrams = pd.read_csv("bigrams.csv")</span><span id="4fab" class="mo mp iq mk b gy oq mr l ms mt"><em class="mb"># Data Visualization<br/></em>import matplotlib.pyplot as plt<br/>import seaborn as sns</span><span id="08d1" class="mo mp iq mk b gy oq mr l ms mt"><em class="mb"># plot bar graph<br/></em>plt.figure(figsize=(10,20))<br/>sns.barplot(x = bigrams[:101].value , y=bigrams[:101].bigram)<br/>sns.despine(top=True, right=True, left=False, bottom=False)<br/>plt.xlabel("Value", <br/>             fontdict={'fontsize':'12', 'fontweight':'bold'})<br/>plt.ylabel("Bigrams", <br/>             fontdict={'fontsize':'12', 'fontweight':'bold'})</span><span id="e65c" class="mo mp iq mk b gy oq mr l ms mt">plt.title("Top 100 Bigrams", <br/>            fontdict={'fontsize':'16', 'fontweight':'bold'})</span><span id="e7a4" class="mo mp iq mk b gy oq mr l ms mt">plt.savefig("Top 100 Bigrams.png", bbox_inches='tight')</span></pre><p id="0c1a" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated"><em class="mb">结果:</em></p><div class="kp kq kr ks gt ab cb"><figure class="ox kt oy oz pa pb pc paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><img src="../Images/31eb117f833bb9b8400f2069ba606546.png" data-original-src="https://miro.medium.com/v2/resize:fit:978/format:webp/1*klfil4TUW4kv9lEsHTCDBw.png"/></div></figure><figure class="ox kt pd oz pa pb pc paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><img src="../Images/ca971225d89d39312ee230d452a14767.png" data-original-src="https://miro.medium.com/v2/resize:fit:1024/format:webp/1*5SUnEcLjHaDsk2NtTTwRhw.png"/></div></figure></div><p id="105f" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">就像世界云的情况一样，在第一排有诸如<em class="mb">、</em>、<em class="mb">、【covid pass】、<em class="mb">、【健康通行证】、</em>这样的术语是很容易理解的。然而，如果我们进一步观察，我们可以发现其他可能对分析有用的二元和三元模型。例如，一些有趣的二元模型可能是<em class="mb">【群体免疫】</em><em class="mb">【通过抗议者】</em><em class="mb">【假疫苗】</em><em class="mb">【杀死人群】</em><em class="mb">【传播病毒】</em>和<em class="mb">【完全接种】</em>。然而，感兴趣的三元模型可能是<em class="mb">‘真正的威胁人物’</em>和<em class="mb">‘反疫苗护照’。也就是说，需要更深入的分析才能得出准确的结论。此外，我建议不要局限于前100个二元和三元语法，而要对n元语法进行情感分析。</em></em></p></div><div class="ab cl mc md hu me" role="separator"><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh"/></div><div class="ij ik il im in"><p id="0b63" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">在这篇文章中，我解释了对谈论健康护照的推文进行数据分析的步骤。为了实现这个项目，使用了几种自然语言处理技术，例如命名实体识别、文本预处理和通过单字、双字和三字的文本分析。此外，结果以饼图、地理空间图、条形图和世界云的形式通过数据可视化显示。</p></div></div>    
</body>
</html>