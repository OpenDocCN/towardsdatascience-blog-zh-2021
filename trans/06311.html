<html>
<head>
<title>Galerkin Transformer: A One-Shot Experiment at NeurIPS 2021</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">伽辽金变压器:NeurIPS 2021上的一次性实验</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/galerkin-transformer-a-one-shot-experiment-at-neurips-2021-96efcbaefd3e?source=collection_archive---------28-----------------------#2021-06-06">https://towardsdatascience.com/galerkin-transformer-a-one-shot-experiment-at-neurips-2021-96efcbaefd3e?source=collection_archive---------28-----------------------#2021-06-06</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="13da" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsdatascience.com/tagged/thoughts-and-theory" rel="noopener" target="_blank">思想和理论</a></h2><div class=""/><div class=""><h2 id="9046" class="pw-subtitle-paragraph jw iz iq bd b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn dk translated">一个业余计算数学家关于注意力机制的数学理论和应用的旅程。</h2></div><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ko"><img src="../Images/e500abf4817fc5e17c446fb3747e2831.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RLG-NuKENQumUnsex2T1hQ.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">一个基于注意力的算子学习器直接从参数中推断出解。图片作者。</p></figure><h1 id="967f" class="le lf iq bd lg lh li lj lk ll lm ln lo kf lp kg lq ki lr kj ls kl lt km lu lv bi translated"><span class="l lw lx ly bm lz ma mb mc md di">警</span>罗罗格</h1><p id="355e" class="pw-post-body-paragraph me mf iq mg b mh mi ka mj mk ml kd mm mn mo mp mq mr ms mt mu mv mw mx my mz ij bi translated">最近，我写了我的第一篇关于机器学习的论文，这是一个有趣但具有挑战性的兼职项目，同时还有一个包含代码的开源库:【https://github.com/scaomath/fourier-transformer<a class="ae na" href="https://github.com/scaomath/fourier-transformer" rel="noopener ugc nofollow" target="_blank"/>。</p><p id="ae47" class="pw-post-body-paragraph me mf iq mg b mh nb ka mj mk nc kd mm mn nd mp mq mr ne mt mu mv nf mx my mz ij bi translated">作为一个完全的新手和这个领域的独立研究者，我疯狂而愚蠢地向NeurIPS 2021提交了这篇论文……感谢<a class="ae na" href="https://github.com/lyc102/ifem" rel="noopener ugc nofollow" target="_blank">我的导师</a>的鼓励。</p><p id="c788" class="pw-post-body-paragraph me mf iq mg b mh nb ka mj mk nc kd mm mn nd mp mq mr ne mt mu mv nf mx my mz ij bi translated">我认为这个标题很吸引人，因为搜索“伽辽金变形金刚”会在顶部找到这篇论文的arXiv链接。作为一名业余数学家，这个标题表明这更像是一篇“享受乐趣，而不是为了发表”类型的论文。然后，我看到了一些其他“明显的”NeurIPS使用LaTeX模板提交的内容，标题类似于<a class="ae na" href="https://arxiv.org/abs/2106.00106" rel="noopener ugc nofollow" target="_blank">“反Koopmanism”</a>，<a class="ae na" href="https://arxiv.org/abs/2105.15069" rel="noopener ugc nofollow" target="_blank">“Max-Margin已死，Max-Margin万岁！”</a>、<a class="ae na" href="https://arxiv.org/abs/2106.01908" rel="noopener ugc nofollow" target="_blank">“你永远不会独自群集”</a>。毫无疑问，这些CS人对此是认真的。</p><p id="1096" class="pw-post-body-paragraph me mf iq mg b mh nb ka mj mk nc kd mm mn nd mp mq mr ne mt mu mv nf mx my mz ij bi translated">论文本身是半理论半实验的。我觉得即使每个人都在使用变形金刚，因为<strong class="mg ja"> <em class="ng">注意力是你所需要的全部</em> </strong>，即使对于CV任务，说“注意力机制背后的数学不是很好理解”甚至有点轻描淡写。几乎没有任何严格的理论基础来解释为什么注意力会以如此神奇的方式在数学上起作用。</p><p id="70d8" class="pw-post-body-paragraph me mf iq mg b mh nb ka mj mk nc kd mm mn nd mp mq mr ne mt mu mv nf mx my mz ij bi translated">因此，在这篇博文的剩余部分，我将以一种随意的方式，带着一些半严肃的数学知识，介绍我们用一些我熟悉的严肃数学知识重新思考变形金刚的小小尝试:</p><blockquote class="nh ni nj"><p id="9a21" class="me mf ng mg b mh nb ka mj mk nc kd mm nk nd mp mq nl ne mt mu nm nf mx my mz ij bi translated"><strong class="mg ja">首次尝试从希尔伯特空间的逼近理论出发，用伽辽金方法解释注意机制的逼近能力。</strong></p></blockquote></div><div class="ab cl nn no hu np" role="separator"><span class="nq bw bk nr ns nt"/><span class="nq bw bk nr ns nt"/><span class="nq bw bk nr ns"/></div><div class="ij ik il im in"><p id="0d02" class="pw-post-body-paragraph me mf iq mg b mh nb ka mj mk nc kd mm mn nd mp mq mr ne mt mu mv nf mx my mz ij bi translated">[1]:曹，S. (2021)。选择一个变换器:傅立叶或伽辽金。在<a class="ae na" href="https://arxiv.org/abs/2105.14995" rel="noopener ugc nofollow" target="_blank"> <em class="ng"> arXiv预印本arXiv:2105.14995 </em> </a>。</p><p id="1b27" class="pw-post-body-paragraph me mf iq mg b mh nb ka mj mk nc kd mm mn nd mp mq mr ne mt mu mv nf mx my mz ij bi translated">[2]:瓦斯瓦尼，a .，沙泽尔，n .，帕尔马，n .，乌兹科雷特，j .，琼斯，l .，戈麦斯，A.N .，凯泽，l .，波洛苏欣，I .，2017。你需要的只是关注。<a class="ae na" href="https://arxiv.org/abs/1706.03762" rel="noopener ugc nofollow" target="_blank"> <em class="ng"> arXiv预印本arXiv:1706.03762 </em> </a>。</p></div><div class="ab cl nn no hu np" role="separator"><span class="nq bw bk nr ns nt"/><span class="nq bw bk nr ns nt"/><span class="nq bw bk nr ns"/></div><div class="ij ik il im in"><h1 id="a34a" class="le lf iq bd lg lh nu lj lk ll nv ln lo kf nw kg lq ki nx kj ls kl ny km lu lv bi translated">背景</h1><p id="4e59" class="pw-post-body-paragraph me mf iq mg b mh mi ka mj mk ml kd mm mn mo mp mq mr ms mt mu mv mw mx my mz ij bi translated">自我关注算子的数学定义如下:𝐲 ∈ ℝⁿ × ℝᵈ是输入潜在表示，查询<em class="ng"> Q </em>，键<em class="ng"> K </em>，值<em class="ng"> V </em>，以及投影矩阵<em class="ng"> W^Q、W^K、W^V </em></p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi nz"><img src="../Images/39a18bd7bf5964ca7f5afd3619d94a7c.png" data-original-src="https://miro.medium.com/v2/resize:fit:974/format:webp/1*NFIb-nK4ricdlXFPn3TJDA.png"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">使用<a class="ae na" href="http://www.codecogs.com/" rel="noopener ugc nofollow" target="_blank">编码</a>创建的方程式。</p></figure><p id="97c8" class="pw-post-body-paragraph me mf iq mg b mh nb ka mj mk nc kd mm mn nd mp mq mr ne mt mu mv nf mx my mz ij bi translated">缩放后的点积关注度为:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi oa"><img src="../Images/f0e999bf9777a553893d1723127c1171.png" data-original-src="https://miro.medium.com/v2/resize:fit:910/format:webp/1*IHn9tLjlaXWYNlHcJtUqJg.png"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">使用<a class="ae na" href="http://www.codecogs.com/" rel="noopener ugc nofollow" target="_blank">编码</a>创建的方程式。</p></figure><p id="78be" class="pw-post-body-paragraph me mf iq mg b mh nb ka mj mk nc kd mm mn nd mp mq mr ne mt mu mv nf mx my mz ij bi translated">全部注意力是:对于g(⋅)一个逐点通用逼近器(在这种情况下是前馈神经网络)</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi ob"><img src="../Images/7447f71a1de9acf6fb1b8d6c385f4dbe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1388/format:webp/1*HgDxrMakI6755o1b1mNa8A.png"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated"><strong class="bd oc"> (A) </strong>:标准的注意机制。使用<a class="ae na" href="http://www.codecogs.com/" rel="noopener ugc nofollow" target="_blank">编码</a>创建的方程式。</p></figure><p id="b0c5" class="pw-post-body-paragraph me mf iq mg b mh nb ka mj mk nc kd mm mn nd mp mq mr ne mt mu mv nf mx my mz ij bi translated">在许多关于<em class="ng">中自我注意机制的解释的论文中，注意是你所需要的全部</em>，包括最近的一些尝试，用“内核解释”进行类比，或者说将Softmax <em class="ng"> (QKᵀ)V </em>链接为可学习的内核图是一种常见的做法。Softmax( <em class="ng"> QKᵀ </em>)被描述为每个位置的特征向量(标记的学习嵌入)与每个其他位置的特征向量的相似性度量。softmax归一化变压器的一些值得注意的线性化利用了这种解释。</p><p id="e862" class="pw-post-body-paragraph me mf iq mg b mh nb ka mj mk nc kd mm mn nd mp mq mr ne mt mu mv nf mx my mz ij bi translated">我想，如果没有softmax，生活会变得简单得多，我们将解释从行方式的<strong class="mg ja">改为列方式的</strong>。所以让我们做吧！</p><h1 id="3d10" class="le lf iq bd lg lh li lj lk ll lm ln lo kf lp kg lq ki lr kj ls kl lt km lu lv bi translated">改进基于数学直觉的注意机制的尝试</h1><p id="fe69" class="pw-post-body-paragraph me mf iq mg b mh mi ka mj mk ml kd mm mn mo mp mq mr ms mt mu mv mw mx my mz ij bi translated">现在假设<em class="ng"> Q，k，V </em>的第<em class="ng"> j </em>列为在物理位置xᵢ ∈ ℝᵐ采样的(单独的)函数qⱼ(⋅)，i=1，…，n ，连同zⱼ(⋅)表示缩放的点积的输出。然后，<em class="ng"> (QKᵀ)V </em>加上一些关于跳过连接的警告，没有softmax的注意力变成了第二类弗雷德霍姆方程<a class="ae na" href="https://mathworld.wolfram.com/FredholmIntegralEquationoftheSecondKind.html" rel="noopener ugc nofollow" target="_blank">:</a></p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi od"><img src="../Images/61c046b1d4a12f9a5fcfe6ef0383f4f7.png" data-original-src="https://miro.medium.com/v2/resize:fit:954/format:webp/1*p8ilx7R_qBSm5MJBcjAMmQ.png"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">使用<a class="ae na" href="http://www.codecogs.com/" rel="noopener ugc nofollow" target="_blank">代码生成的方程式</a>。</p></figure><p id="0765" class="pw-post-body-paragraph me mf iq mg b mh nb ka mj mk nc kd mm mn nd mp mq mr ne mt mu mv nf mx my mz ij bi translated">与κ(x,ξ):=ζ_q(x)⋅ϕₖ(ξ)对于特征地图:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi oe"><img src="../Images/46e672444649f7e00703908a2b37c6d2.png" data-original-src="https://miro.medium.com/v2/resize:fit:596/format:webp/1*4BK5UJXLMaSu6Tm1M1y4Pg.png"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">使用<a class="ae na" href="http://www.codecogs.com/" rel="noopener ugc nofollow" target="_blank">代码生成的方程式</a>。</p></figure><p id="0d9b" class="pw-post-body-paragraph me mf iq mg b mh nb ka mj mk nc kd mm mn nd mp mq mr ne mt mu mv nf mx my mz ij bi translated">将注意力机制解释为积分并不新鲜，例如在LieTransformer中已经开发了对<a class="ae na" href="https://en.wikipedia.org/wiki/Group_(mathematics)" rel="noopener ugc nofollow" target="_blank">组</a>的积分。</p><p id="fac2" class="pw-post-body-paragraph me mf iq mg b mh nb ka mj mk nc kd mm mn nd mp mq mr ne mt mu mv nf mx my mz ij bi translated">尽管如此，将<strong class="mg ja">积分</strong>引入到图片中会为没有softmax的线性变量带来更有趣的解释:<em class="ng"> Q(KᵀV) </em>可以被视为彼得罗夫-伽辽金投影(如果我们将<em class="ng"> Q，k，V </em>的列视为独立函数):</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi of"><img src="../Images/dcdd5e2bb203c658b6088c581e7bcc58.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YO8vghM7Jdu1eLWPuex54Q.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated"><strong class="bd oc"> (P-G) </strong>:重写没有softmax的线性注意。使用<a class="ae na" href="http://www.codecogs.com/" rel="noopener ugc nofollow" target="_blank">编码</a>创建的方程式。</p></figure><p id="39d7" class="pw-post-body-paragraph me mf iq mg b mh nb ka mj mk nc kd mm mn nd mp mq mr ne mt mu mv nf mx my mz ij bi translated">其中𝔟(⋅,⋅)是在<strong class="mg ja"> <em class="ng">无限维希尔伯特空间</em> </strong>上定义的双线性形式，但在行动中被评估为离散近似。为了帮助我们理解，我们可以参考下面的两个数字，看看没有softmax，生活是如何变得容易得多的。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi og"><img src="../Images/7ad9cd7d7290ab1c3dc46f8c7d00f80c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ksC-r_vxAShrklKlAEEQDg.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">没有softmax的线性注意力的图示。图片作者。</p></figure><p id="e815" class="pw-post-body-paragraph me mf iq mg b mh nb ka mj mk nc kd mm mn nd mp mq mr ne mt mu mv nf mx my mz ij bi translated">现在，为了将上面关于没有softmax的线性注意力的表达式与我们熟悉的东西联系起来，我们可以进一步考虑<em class="ng"> QR </em>因式分解，或者甚至是在一组<em class="ng"> d </em>正交基函数{ <em class="ng"> qⱼ(⋅) </em> }ⱼ₌₁ᵈ:中的投影</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/160abebb653dc1c7c047135431c9dded.png" data-original-src="https://miro.medium.com/v2/resize:fit:652/format:webp/1*pyZ08E3nD8krvYIKK7w7lA.png"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">使用<a class="ae na" href="http://www.codecogs.com/" rel="noopener ugc nofollow" target="_blank">编码</a>创建的方程式。</p></figure><p id="b372" class="pw-post-body-paragraph me mf iq mg b mh nb ka mj mk nc kd mm mn nd mp mq mr ne mt mu mv nf mx my mz ij bi translated">毫不奇怪，解决方案是一个类似于线性回归解的显式表达式的投影:如果我们假设{ <em class="ng"> qⱼ(⋅) </em> }ⱼ₌₁ᵈ通过其内积诱导范数进一步规范化(想想具有进一步<em class="ng"> 1/√n </em>权重的实例规范化)。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi oi"><img src="../Images/5d7b450d006cd0bd580233d285f9e576.png" data-original-src="https://miro.medium.com/v2/resize:fit:880/format:webp/1*hRpMy1qDFpnDcT2aqL0qhQ.png"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated"><strong class="bd oc"> (F) </strong>:正交基的潜在扩展。使用<a class="ae na" href="http://www.codecogs.com/" rel="noopener ugc nofollow" target="_blank">代码生成的方程式</a>。</p></figure><p id="8668" class="pw-post-body-paragraph me mf iq mg b mh nb ka mj mk nc kd mm mn nd mp mq mr ne mt mu mv nf mx my mz ij bi translated">来看一个具体的例子:设<em class="ng">ω</em>=【<em class="ng">0，2π </em>】和<em class="ng"> qⱼ(x) </em> = sin( <em class="ng"> jx </em>)和cos( <em class="ng"> jx </em>)，那么上面的等式只是<a class="ae na" href="https://encyclopediaofmath.org/wiki/Partial_Fourier_sum" rel="noopener ugc nofollow" target="_blank"> <strong class="mg ja">傅里叶级数部分和逼近</strong><em class="ng">【x】</em>和<strong class="mg ja">中的一个特例(p-p)</strong></a></p><p id="bf28" class="pw-post-body-paragraph me mf iq mg b mh nb ka mj mk nc kd mm mn nd mp mq mr ne mt mu mv nf mx my mz ij bi translated">受这种解释的启发，提出了下面的伽辽金型简单注意算子:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/481993c56a4eec91f8a2ecbd14d1a840.png" data-original-src="https://miro.medium.com/v2/resize:fit:1344/format:webp/1*BSL5Ens7nax2Noo6yJEnZw.png"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated"><strong class="bd oc"> (G) </strong>:伽辽金注意力，一种尺度保持的注意力算子。使用<a class="ae na" href="http://www.codecogs.com/" rel="noopener ugc nofollow" target="_blank">代码生成的方程式</a>。</p></figure><p id="6ef9" class="pw-post-body-paragraph me mf iq mg b mh nb ka mj mk nc kd mm mn nd mp mq mr ne mt mu mv nf mx my mz ij bi translated">一点也没有。在进行点积之前，对潜在表示进行层归一化，就像傅立叶基可以被视为具有预内积归一化一样，</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ok"><img src="../Images/d7295ce80b40ea4a4a6f80073c7af2b2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ekraWM5ECxXfhlHNH061Ag.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">使用<a class="ae na" href="http://www.codecogs.com/" rel="noopener ugc nofollow" target="_blank">代码编码</a>创建的方程式。</p></figure></div><div class="ab cl nn no hu np" role="separator"><span class="nq bw bk nr ns nt"/><span class="nq bw bk nr ns nt"/><span class="nq bw bk nr ns"/></div><div class="ij ik il im in"><p id="8fd8" class="pw-post-body-paragraph me mf iq mg b mh nb ka mj mk nc kd mm mn nd mp mq mr ne mt mu mv nf mx my mz ij bi translated">[3]:哈钦森，m .，兰，C. L .，扎伊迪，s .，杜邦，e .，德，Y. W .，，金，H. (2020)。LieTransformer:李群的等变自注意。<em class="ng"> arXiv预印本arXiv:2012.10885 </em>。</p></div><div class="ab cl nn no hu np" role="separator"><span class="nq bw bk nr ns nt"/><span class="nq bw bk nr ns nt"/><span class="nq bw bk nr ns"/></div><div class="ij ik il im in"><h1 id="96f7" class="le lf iq bd lg lh nu lj lk ll nv ln lo kf nw kg lq ki nx kj ls kl ny km lu lv bi translated">操作员学习</h1><p id="f7f1" class="pw-post-body-paragraph me mf iq mg b mh mi ka mj mk ml kd mm mn mo mp mq mr ms mt mu mv mw mx my mz ij bi translated">现在一种新的注意力被构想出来了，问题是:</p><blockquote class="ol"><p id="5aa6" class="om on iq bd oo op oq or os ot ou mz dk translated">我们在哪里为这位新的注意力操作员搭建舞台？</p></blockquote><p id="fd6d" class="pw-post-body-paragraph me mf iq mg b mh ow ka mj mk ox kd mm mn oy mp mq mr oz mt mu mv pa mx my mz ij bi translated">即使一些初始原型表明使用<code class="fe pb pc pd pe b">fairseq</code>(例如，更快的训练)在IWSLT14 (En-De)上的BLEU评估基准上有希望的结果，对我自己来说，与<a class="ae na" href="https://en.wikipedia.org/wiki/Partial_differential_equation" rel="noopener ugc nofollow" target="_blank">偏微分方程</a>相关的操作员学习问题将是自然的选择。如果选择一个大而艰巨的自然语言处理(NLP)问题，作为一个独立的研究人员，原型-调试-改进的周期对我来说太长了。</p><p id="6222" class="pw-post-body-paragraph me mf iq mg b mh nb ka mj mk nc kd mm mn nd mp mq mr ne mt mu mv nf mx my mz ij bi translated">去年，我的导师给我发了一篇<a class="ae na" href="https://www.jiqizhixin.com/articles/2020-10-23-7" rel="noopener ugc nofollow" target="_blank">博客文章</a>关于加州理工学院ML小组使用所谓的傅立叶神经算子(FNO)⁴学习偏微分方程解算子)实现了一个最先进的性能。在仔细阅读、剖析代码并自己重新实现之后，我完全<a class="ae na" href="https://en.wikipedia.org/wiki/Pran%C4%81ma" rel="noopener ugc nofollow" target="_blank">屈服于这种令人敬畏的方法的脚下</a>,因为它在数量级上击败了以前的方法。此外，这个实现让我确信，注意力中的一般<code class="fe pb pc pd pe b">Q/K/V</code>方法变成了<code class="fe pb pc pd pe b">FFT-&gt;conv-&gt;iFFT</code>。这种变化是注意力机制的一种特殊而有效的线性变化，投影矩阵是不可训练的。因为快速傅立叶变换(FFT)或其逆变换都可以被视为通过乘以范德蒙矩阵的不可训练的基变换。在实际的注意机制中，基底的(非线性)变化的权重都是可训练的。</p><p id="1927" class="pw-post-body-paragraph me mf iq mg b mh nb ka mj mk nc kd mm mn nd mp mq mr ne mt mu mv nf mx my mz ij bi translated">最重要的是，FNO的实践表明了一件事</p><blockquote class="nh ni nj"><p id="3ca8" class="me mf ng mg b mh nb ka mj mk nc kd mm nk nd mp mq nl ne mt mu nm nf mx my mz ij bi translated">对待潜在表示的<strong class="mg ja">列</strong>而不是<strong class="mg ja">行</strong>(位置的特征向量)施加操作，是一种有前途的方法。</p></blockquote><p id="597b" class="pw-post-body-paragraph me mf iq mg b mh nb ka mj mk nc kd mm mn nd mp mq mr ne mt mu mv nf mx my mz ij bi translated">直到最近(截至2021年5月)，ML社区开始重视这一方向，例如在[5]和[6]。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi pf"><img src="../Images/ea1632aecf783defdc51a13e336a41e3.png" data-original-src="https://miro.medium.com/v2/resize:fit:840/format:webp/1*KJl-DLp2UtavK8TAXzAOYw.png"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">随机界面的扩散系数。图片作者。</p></figure><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi pg"><img src="../Images/99f5e4faebe79925ab779694a3106bc4.png" data-original-src="https://miro.medium.com/v2/resize:fit:800/format:webp/1*VUPP9fTsEQQuSXCBIZi0cg.png"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">含10%噪声的偏微分方程解。图片作者。</p></figure><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi pf"><img src="../Images/ac14cbd8bf292f4adb57112d1ad02570.png" data-original-src="https://miro.medium.com/v2/resize:fit:840/format:webp/1*R0HBc92T1XCUprBV49_8eA.png"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">来自溶液的噪声测量的重构系数。图片作者。</p></figure><p id="95d4" class="pw-post-body-paragraph me mf iq mg b mh nb ka mj mk nc kd mm mn nd mp mq mr ne mt mu mv nf mx my mz ij bi translated">那么，结果是什么呢？注意力和FNO的结合非常好，基于伽辽金注意力的转换器也可以解决困难的反系数识别问题(见上图)。在相同的参数配额下，在伯格斯方程基准中，4个伽辽金注意层+2个FNO层的模型在相同的实验室条件下(相同学习速率的<code class="fe pb pc pd pe b"><a class="ae na" href="https://arxiv.org/abs/1708.07120" rel="noopener ugc nofollow" target="_blank">1cycle</a></code> <a class="ae na" href="https://arxiv.org/abs/1708.07120" rel="noopener ugc nofollow" target="_blank">调度器</a>，100个历元)优于4个FNO层的模型4倍，优于原始的具有<code class="fe pb pc pd pe b">Batchnorm</code>的模型10倍。在<em class="ng"> L </em> -norm中平均评价相对误差的常用范围为<code class="fe pb pc pd pe b">1e-3</code>，最大值为<code class="fe pb pc pd pe b">1.7e-3</code>。达西流中的基准，连同其逆版本，在评估准确性方面也已经废黜了国王(FNO)。伽辽金变换器的训练比FNO慢，尽管理论上具有相同的复杂度。</p><p id="b06d" class="pw-post-body-paragraph me mf iq mg b mh nb ka mj mk nc kd mm mn nd mp mq mr ne mt mu mv nf mx my mz ij bi translated">除了加州理工学院集团提出的神经算子结构化网络，下一个接近的竞争对手<a class="ae na" href="https://arxiv.org/pdf/2103.10974.pdf" rel="noopener ugc nofollow" target="_blank"> DeepONets </a>要差得多……(参见<a class="ae na" href="https://arxiv.org/pdf/2103.10974.pdf" rel="noopener ugc nofollow" target="_blank">第14页</a>，图10中有<code class="fe pb pc pd pe b">3e-2</code>相对误差的单个实例)。由于DeepONets中的网络类似于<a class="ae na" href="https://arxiv.org/abs/1410.5401" rel="noopener ugc nofollow" target="_blank">神经图灵机(NMT) </a>中的附加注意力，可以预见的是，尽管DeepOnets具有启发式地符合数学的结构，但由于多个“困难的”非线性(如双曲正切和Sigmoids)的组合，deep onets更难训练。</p></div><div class="ab cl nn no hu np" role="separator"><span class="nq bw bk nr ns nt"/><span class="nq bw bk nr ns nt"/><span class="nq bw bk nr ns"/></div><div class="ij ik il im in"><p id="6b17" class="pw-post-body-paragraph me mf iq mg b mh nb ka mj mk nc kd mm mn nd mp mq mr ne mt mu mv nf mx my mz ij bi translated">[4]:李，z .，科瓦奇基，n .，阿齐扎德内谢利，k .，刘，b .，巴塔查里亚，k .，斯图尔特，a .，&amp;阿南德库马尔，A. (2020)。参数偏微分方程的傅立叶神经算子。<em class="ng"> arXiv预印本arXiv:2010.08895 </em>。</p><p id="5906" class="pw-post-body-paragraph me mf iq mg b mh nb ka mj mk nc kd mm mn nd mp mq mr ne mt mu mv nf mx my mz ij bi translated">[5]:托尔斯泰欣、霍尔斯比、科列斯尼科夫、拜尔、李、翟、安特辛纳、杨、凯泽斯、乌兹科雷特、卢契奇和多索维茨基，2021年。Mlp混合器:用于视觉的全mlp架构。<em class="ng"> arXiv预印本arXiv:2105.01601 </em>。</p><p id="28ba" class="pw-post-body-paragraph me mf iq mg b mh nb ka mj mk nc kd mm mn nd mp mq mr ne mt mu mv nf mx my mz ij bi translated">[6]:李-索普，j .，安斯利，j .，埃克斯坦，I .，&amp;翁塔农，S. (2021)。FNet:用傅立叶变换混合记号。<em class="ng"> arXiv预印本arXiv:2105.03824 </em>。</p></div><div class="ab cl nn no hu np" role="separator"><span class="nq bw bk nr ns nt"/><span class="nq bw bk nr ns nt"/><span class="nq bw bk nr ns"/></div><div class="ij ik il im in"><h2 id="77dc" class="ph lf iq bd lg pi pj dn lk pk pl dp lo mn pm pn lq mr po pp ls mv pq pr lu iw bi translated">注意:下面的最后一部分有一些半严肃的数学。</h2><h1 id="dc47" class="le lf iq bd lg lh li lj lk ll lm ln lo kf lp kg lq ki lr kj ls kl lt km lu lv bi translated">我们能证明什么？彼得罗夫-伽辽金投影</h1><p id="01ca" class="pw-post-body-paragraph me mf iq mg b mh mi ka mj mk ml kd mm mn mo mp mq mr ms mt mu mv mw mx my mz ij bi translated">注意算子或其线性变体，就其输入和可训练参数而言，是一个<strong class="mg ja"> <em class="ng">非线性</em> </strong>算子。我们如何把它连接到像伽辽金或者彼得罗夫-伽辽金投影这样的线性投影上呢？答案很简单:</p><blockquote class="nh ni nj"><p id="287f" class="me mf ng mg b mh nb ka mj mk nc kd mm nk nd mp mq nl ne mt mu nm nf mx my mz ij bi translated">我们尝试在希尔伯特框架下，在以下意义下证明<strong class="mg ja"> <em class="iq">近似容量</em> </strong>。</p></blockquote><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi ps"><img src="../Images/3072ffb4a4b0e4a520be023e41e5af13.png" data-original-src="https://miro.medium.com/v2/resize:fit:786/format:webp/1*Tw9aolaPh-KxdCR8DrK4Sw.png"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated"><strong class="bd oc"> (M) </strong>:线性投影度量的非线性映射的逼近能力。使用<a class="ae na" href="http://www.codecogs.com/" rel="noopener ugc nofollow" target="_blank">编码</a>创建的方程式。</p></figure><p id="cadb" class="pw-post-body-paragraph me mf iq mg b mh nb ka mj mk nc kd mm mn nd mp mq mr ne mt mu mv nf mx my mz ij bi translated">这转化为:g_θ，建立在伽辽金注意力上的近似器，其近似能力等同于到近似子空间<em class="ng"> 𝕍ₕ </em> ⊂ ℋ的伽辽金投影，其中ℋ是希尔伯特空间，并且我们想要在它的子集上模拟操作者的行为。</p><p id="e555" class="pw-post-body-paragraph me mf iq mg b mh nb ka mj mk nc kd mm mn nd mp mq mr ne mt mu mv nf mx my mz ij bi translated">这个子空间<em class="ng"> 𝕍ₕ </em>是基于潜在表示的当前子空间，并且在优化期间，是动态变化的。然而，在固定的潜在表征的静态视图中，注意机制有能力在当前的近似空间中提供最佳近似器，即伽辽金型投影。</p><h2 id="72a6" class="ph lf iq bd lg pi pj dn lk pk pl dp lo mn pm pn lq mr po pp ls mv pq pr lu iw bi translated">难点:用线性投影桥接非线性映射</h2><p id="4795" class="pw-post-body-paragraph me mf iq mg b mh mi ka mj mk ml kd mm mn mo mp mq mr ms mt mu mv mw mx my mz ij bi translated">有人可能会问:这不是小事吗？嗯，有几分像<strong class="mg ja"> (F) </strong>正是<em class="ng">𝕍ₕ</em>:= span {<em class="ng">vⱼ(⋅)</em>}的情况。然而，仔细观察后，用数学方法把这个结果清晰地表达出来可能并不那么容易。困难如下:</p><ul class=""><li id="5870" class="pt pu iq mg b mh nb mk nc mn pv mr pw mv px mz py pz qa qb bi translated">与伽辽金投影中的空间设置不同，<code class="fe pb pc pd pe b">Q/K/V</code>所代表的近似子空间是不同的。</li><li id="401d" class="pt pu iq mg b mh qc mk qd mn qe mr qf mv qg mz py pz qa qb bi translated">如何将<em class="ng"> Q </em>的列线性组合得到<strong class="mg ja"> (P-G) </strong>中输出<strong class="mg ja"> z </strong>的每一列取决于乘积<em class="ng"> KᵀV </em>的每一列。然而，并不完全清楚的是，<em class="ng"> KᵀV </em>的某一列是否足以给出系数，以产生任意函数<em class="ng">f</em>∈ℋ.<strong class="mg ja">的伽辽金或彼得罗夫-伽辽金投影原因是因为既不能保证<code class="fe pb pc pd pe b">Q/K/V</code>满秩，也不能保证非线性内积的满射性。</strong></li></ul><h2 id="a0fc" class="ph lf iq bd lg pi pj dn lk pk pl dp lo mn pm pn lq mr po pp ls mv pq pr lu iw bi translated">一个简单的例子来说明困难</h2><p id="6be1" class="pw-post-body-paragraph me mf iq mg b mh mi ka mj mk ml kd mm mn mo mp mq mr ms mt mu mv mw mx my mz ij bi translated">在注意力的任何线性变化的上下文中:</p><blockquote class="nh ni nj"><p id="ad11" class="me mf ng mg b mh nb ka mj mk nc kd mm nk nd mp mq nl ne mt mu nm nf mx my mz ij bi translated">q代表值，K代表查询，V代表键。</p></blockquote><p id="a0e2" class="pw-post-body-paragraph me mf iq mg b mh nb ka mj mk nc kd mm mn nd mp mq mr ne mt mu mv nf mx my mz ij bi translated">让我们考虑一个简单的例子，其中<em class="ng">ω</em>=(<em class="ng">-1，1 </em>)，由<em class="ng">−1=x₁&lt;x₂&lt;⋯&lt;xₙ=1</em>离散化。<em class="ng">q</em><strong class="mg ja"><em class="ng">列的近似空间</em> </strong>由前两个<a class="ae na" href="https://en.wikipedia.org/wiki/Chebyshev_polynomials" rel="noopener ugc nofollow" target="_blank">切比雪夫多项式</a> { <em class="ng"> 1，x </em> }构成，<em class="ng"> K </em>和<em class="ng"> V </em>的列的近似空间分别为{ <em class="ng"> a，bx </em>和{ <em class="ng"> c，dx </em> }对于<em class="ng"> a，b，b实际的列是由这些函数在<em class="ng"> xᵢ </em>的评估产生的，对于一个样本，网格点数<em class="ng"> n </em>是固定的。我们注意到序列长度<em class="ng"> n </em>可以在操作者学习管道中改变。</em></p><p id="dd1e" class="pw-post-body-paragraph me mf iq mg b mh nb ka mj mk nc kd mm mn nd mp mq mr ne mt mu mv nf mx my mz ij bi translated">现在，对于一个<em class="ng"> f </em> ∈ ℋ，<em class="ng">l</em>-<em class="ng">f</em>到<em class="ng"> ℚₕ </em> := span{ <em class="ng"> 1，x </em> }的投影为:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi qh"><img src="../Images/ecd6b969282e83a874d51f9de13dc4fb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OhOKHqSDgD_FjDjSLnry1Q.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated"><strong class="bd oc"> (L) </strong>:线性多项式空间中对f的最佳逼近。使用<a class="ae na" href="http://www.codecogs.com/" rel="noopener ugc nofollow" target="_blank">代码生成的方程式</a>。</p></figure><p id="87ce" class="pw-post-body-paragraph me mf iq mg b mh nb ka mj mk nc kd mm mn nd mp mq mr ne mt mu mv nf mx my mz ij bi translated">当将<em class="ng"> K/V </em>的列解释为网格采样的函数时，<em class="ng"> KᵀV/n </em> <strong class="mg ja">的点积成为积分</strong>的近似值，并且易于验证:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi qi"><img src="../Images/331af8a7df539fc53287bfae747ea2b2.png" data-original-src="https://miro.medium.com/v2/resize:fit:886/format:webp/1*zUvm7iS1lybuGKnPJ9NCZg.png"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">使用<a class="ae na" href="http://www.codecogs.com/" rel="noopener ugc nofollow" target="_blank">代码生成的方程式</a>。</p></figure><p id="21eb" class="pw-post-body-paragraph me mf iq mg b mh nb ka mj mk nc kd mm mn nd mp mq mr ne mt mu mv nf mx my mz ij bi translated">在这种情况下，通过进一步的简单检查，我们可以看到<em class="ng"> KᵀV/n </em>能够复制<strong class="mg ja"> (L) </strong>中投影的系数，只需乘以矢量<em class="ng"> (f₁，f₂)ᵀ </em>)。</p><p id="6874" class="pw-post-body-paragraph me mf iq mg b mh nb ka mj mk nc kd mm mn nd mp mq mr ne mt mu mv nf mx my mz ij bi translated">但是，如果把<em class="ng"> K </em>和<em class="ng"> V </em>的列改成网格点处的{ <em class="ng"> a，bx </em> }和{ <em class="ng"> cx，dx </em> }的求值呢？</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi od"><img src="../Images/67083575358943e3e0642843cc9eccc3.png" data-original-src="https://miro.medium.com/v2/resize:fit:954/format:webp/1*JkQmTXCg6nzWEx9yj4g8jw.png"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">使用<a class="ae na" href="http://www.codecogs.com/" rel="noopener ugc nofollow" target="_blank">代码生成的方程式</a>。</p></figure><p id="2d04" class="pw-post-body-paragraph me mf iq mg b mh nb ka mj mk nc kd mm mn nd mp mq mr ne mt mu mv nf mx my mz ij bi translated">突然，<strong class="mg ja">复制(L)中系数的能力没有了！</strong>因为这个矩阵乘以<em class="ng"> Q </em>后:<em class="ng"> Q(KᵀV) </em>的列所代表的子空间在里面没有常数函数！欢迎感兴趣的读者验证一下。</p><p id="1b0c" class="pw-post-body-paragraph me mf iq mg b mh nb ka mj mk nc kd mm mn nd mp mq mr ne mt mu mv nf mx my mz ij bi translated">当然，这是一种过度简化，因为:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi qj"><img src="../Images/e5721cd182aeac3eae3e4d2895c0577d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1386/format:webp/1*hedYfsroShOCEvfuiciL4g.png"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">使用<a class="ae na" href="http://www.codecogs.com/" rel="noopener ugc nofollow" target="_blank">代码生成的方程式</a>。</p></figure><p id="9dfc" class="pw-post-body-paragraph me mf iq mg b mh nb ka mj mk nc kd mm mn nd mp mq mr ne mt mu mv nf mx my mz ij bi translated">但是你明白了。</p><h2 id="0f36" class="ph lf iq bd lg pi pj dn lk pk pl dp lo mn pm pn lq mr po pp ls mv pq pr lu iw bi translated">证明:希尔伯特空间中的一个鞍点问题</h2><p id="6bb1" class="pw-post-body-paragraph me mf iq mg b mh mi ka mj mk ml kd mm mn mo mp mq mr ms mt mu mv mw mx my mz ij bi translated">通过我自己的专业技术(混合有限元)，我们可以证明<em class="ng"> Q(KᵀV) </em>具有<strong class="mg ja">能力</strong>来实现彼得罗夫-伽辽金投影所能近似的结果，前提是以下条件成立:</p><blockquote class="nh ni nj"><p id="60c1" class="me mf ng mg b mh nb ka mj mk nc kd mm nk nd mp mq nl ne mt mu nm nf mx my mz ij bi translated"><strong class="mg ja">存在从关键空间(线性注意中的V)到值空间(线性注意中的Q)的满射映射。</strong></p></blockquote><p id="4d51" class="pw-post-body-paragraph me mf iq mg b mh nb ka mj mk nc kd mm mn nd mp mq mr ne mt mu mv nf mx my mz ij bi translated">在泛函分析中，这也可以被视为在逼近算子方程的情况下的<a class="ae na" href="https://en.wikipedia.org/wiki/Closed_range_theorem" rel="noopener ugc nofollow" target="_blank">近距离定理</a>或<a class="ae na" href="https://en.wikipedia.org/wiki/Ladyzhenskaya%E2%80%93Babu%C5%A1ka%E2%80%93Brezzi_condition" rel="noopener ugc nofollow" target="_blank">Ladyzhenskaya–Babu ka–Brezzi条件</a>的条件。如果我们的一些读者对细节感兴趣，请参考论文中的<a class="ae na" href="https://arxiv.org/abs/2105.14995" rel="noopener ugc nofollow" target="_blank">附录D。</a></p><p id="f9f5" class="pw-post-body-paragraph me mf iq mg b mh nb ka mj mk nc kd mm mn nd mp mq mr ne mt mu mv nf mx my mz ij bi translated">用通俗易懂的语言来翻译这个线性注意证明的数学意义，它是:</p><blockquote class="ol"><p id="8ea3" class="om on iq bd oo op oq or os ot ou mz dk translated"><strong class="ak">对于值空间中的最佳逼近器(Petrov-Galerkin投影),至少存在一个键来匹配传入的查询以传递该最佳逼近器。</strong></p></blockquote><p id="4280" class="pw-post-body-paragraph me mf iq mg b mh ow ka mj mk ox kd mm mn oy mp mq mr oz mt mu mv pa mx my mz ij bi translated">写成一个不等式的形式:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi qk"><img src="../Images/04c1a7f43167b32b65ff32e98e6089a3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*p6wpu3VQdxrT0Nchx8gZsA.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">变压器的序列长度独立逼近。使用<a class="ae na" href="http://www.codecogs.com/" rel="noopener ugc nofollow" target="_blank">编码</a>创建的方程式。</p></figure><p id="aabf" class="pw-post-body-paragraph me mf iq mg b mh nb ka mj mk nc kd mm mn nd mp mq mr ne mt mu mv nf mx my mz ij bi translated">其中<em class="ng"> c </em>是离散近似空间上Ladyzhenskaya-Babu ka-br ezzi条件中的常数。如果<em class="ng"> c </em>可以被证明是序列长度无关的，那么伽辽金变换器的近似功率，或者变换器的任何线性变量的近似功率，是<strong class="mg ja"> <em class="ng">独立于序列长度</em> </strong>，数学证明。</p><p id="0952" class="pw-post-body-paragraph me mf iq mg b mh nb ka mj mk nc kd mm mn nd mp mq mr ne mt mu mv nf mx my mz ij bi translated">相反，近似能力取决于<code class="fe pb pc pd pe b">d_model</code>，即我们愿意支付多少基函数来近似一个子集上的操作者的响应。算子理论和(线性)注意力机制之间的完美桥梁，没有softmax。</p><p id="2a9f" class="pw-post-body-paragraph me mf iq mg b mh nb ka mj mk nc kd mm mn nd mp mq mr ne mt mu mv nf mx my mz ij bi translated">由于我自己已经完成了这个小项目，并在不久的将来返回到我自己的领域(可能会在多级转换器上再做一个工作，利用注意力机制中残差的子空间校正性质)，如果ML社区发现这个证明的重要性，利用<strong class="mg ja"> <em class="ng">键到值映射</em> </strong>的满射性，这并不奇怪，在1或2年内，将会出现像“Sobolev注意力”、“微分注意力”、“Chebyshev注意力”、使用其他积分变换的注意力之类的东西。</p><h2 id="edc4" class="ph lf iq bd lg pi pj dn lk pk pl dp lo mn pm pn lq mr po pp ls mv pq pr lu iw bi translated">连续中的连续，离散中的稳定</h2><p id="df47" class="pw-post-body-paragraph me mf iq mg b mh mi ka mj mk ml kd mm mn mo mp mq mr ms mt mu mv mw mx my mz ij bi translated">对于不熟悉希尔伯特背景下算子方程近似的人来说，按照本文的思路，最大的困惑可能是:</p><blockquote class="nh ni nj"><p id="f148" class="me mf ng mg b mh nb ka mj mk nc kd mm nk nd mp mq nl ne mt mu nm nf mx my mz ij bi translated">使用双线性形式定义键到值的映射。这种双线性形式定义在两个<strong class="mg ja">无限维希尔伯特空间</strong>上，但为什么论文只给出了它在<strong class="mg ja">有限维</strong> <strong class="mg ja">离散近似空间</strong>上的下界？</p></blockquote><p id="fa60" class="pw-post-body-paragraph me mf iq mg b mh nb ka mj mk nc kd mm mn nd mp mq mr ne mt mu mv nf mx my mz ij bi translated">答案很长。在写这篇论文之前，我读了关于第一篇论文<a class="ae na" href="https://arxiv.org/abs/2106.01506" rel="noopener ugc nofollow" target="_blank">的评论</a> ⁷试图使用<a class="ae na" href="https://en.wikipedia.org/wiki/Mercer%27s_theorem" rel="noopener ugc nofollow" target="_blank"> Mercer核</a>以数学严谨的方式解释近似容量。他们的证明主要是将早期开创性工作中的一个移植到巴拿赫·spaces⁸.的背景中从他们提交arXiv的时间来看，可以有把握地假设他们也向NeurIPS 2021提交了他们的工作(尽管没有使用LaTeX模板)。</p><p id="9573" class="pw-post-body-paragraph me mf iq mg b mh nb ka mj mk nc kd mm mn nd mp mq mr ne mt mu mv nf mx my mz ij bi translated">在他们提交的ICLR 2021年申请的公开审查页面中，审查者1提出了这个关键的有见地的问题:</p><blockquote class="nh ni nj"><p id="854e" class="me mf ng mg b mh nb ka mj mk nc kd mm nk nd mp mq nl ne mt mu nm nf mx my mz ij bi translated">论点中的另一个弱点是，核随着随机梯度下降的每一步而变化。也就是说，参数W^Q和W^K被更新，这改变了核函数。结果，变压器的训练不在单个再生核Banach空间中操作。</p></blockquote><p id="164f" class="pw-post-body-paragraph me mf iq mg b mh nb ka mj mk nc kd mm mn nd mp mq mr ne mt mu mv nf mx my mz ij bi translated">这是Mercer kernel论文的作者没有用明确的论据直接回答的主要问题之一。事实上，在操作员学习的背景下，答案很简单:</p><blockquote class="nh ni nj"><p id="adc1" class="me mf ng mg b mh nb ka mj mk nc kd mm nk nd mp mq nl ne mt mu nm nf mx my mz ij bi translated">即使Transformer提供了序列长度不变的性能，例如，在n=512上训练的模型可以在n=512或n=2048上提供相同的评估误差；对于单个样本，提供的近似是通过具有n个网格点的离散空间。因此，该理论可以用<strong class="mg ja">动态变化的有限维近似空间</strong> <strong class="mg ja">来表述，但只有一个无限维的潜在希尔伯特(或巴拿赫)空间</strong>。</p></blockquote><p id="4ef2" class="pw-post-body-paragraph me mf iq mg b mh nb ka mj mk nc kd mm mn nd mp mq mr ne mt mu mv nf mx my mz ij bi translated">例如，在ω的<em class="ng"> n </em>个网格点采样的连续分段线性函数的空间，无论那个<em class="ng"> n </em>有多大，都是<em class="ng">L(ω)</em>的子空间。每个单个样本的近似是在离散水平上完成的。近似空间通过优化动态更新，但底层无限维希尔伯特空间(<em class="ng"> L </em>、<em class="ng"> H </em>等，甚至像<em class="ng"> Lᵖ </em>这样的巴拿赫空间)没有变化！</p><p id="5a03" class="pw-post-body-paragraph me mf iq mg b mh nb ka mj mk nc kd mm mn nd mp mq mr ne mt mu mv nf mx my mz ij bi translated">我觉得对于Mercer kernel论文来说，这是一个错失的机会，因为近似理论是建立在无限维的背景上的。Transformer架构的近似能力仍然受限于有限维子空间，受限于<code class="fe pb pc pd pe b">d_model</code>,这是在我们的“面向列”解释中支付的基函数的数量。</p></div><div class="ab cl nn no hu np" role="separator"><span class="nq bw bk nr ns nt"/><span class="nq bw bk nr ns nt"/><span class="nq bw bk nr ns"/></div><div class="ij ik il im in"><p id="e6f0" class="pw-post-body-paragraph me mf iq mg b mh nb ka mj mk nc kd mm mn nd mp mq mr ne mt mu mv nf mx my mz ij bi translated">[7]:赖特，硕士，冈萨雷斯，J. E. (2021)。变形金刚是深度无限维的非Mercer二进制内核机器。arXiv预印本arXiv:2106.01506 。</p><p id="421b" class="pw-post-body-paragraph me mf iq mg b mh nb ka mj mk nc kd mm mn nd mp mq mr ne mt mu mv nf mx my mz ij bi translated">[8]:奥野，a .，哈达，t .，&amp;下代拉，H. (2018，7月)。基于神经网络的多视角特征学习概率框架。在<em class="ng">机器学习国际会议</em>(第3888–3897页)。PMLR。</p></div><div class="ab cl nn no hu np" role="separator"><span class="nq bw bk nr ns nt"/><span class="nq bw bk nr ns nt"/><span class="nq bw bk nr ns"/></div><div class="ij ik il im in"><h1 id="ed22" class="le lf iq bd lg lh nu lj lk ll nv ln lo kf nw kg lq ki nx kj ls kl ny km lu lv bi translated">收场白</h1><p id="15fb" class="pw-post-body-paragraph me mf iq mg b mh mi ka mj mk ml kd mm mn mo mp mq mr ms mt mu mv mw mx my mz ij bi translated">NeurIPS 2021委员会决定将<a class="ae na" href="https://neuripsconf.medium.com/introducing-the-neurips-2021-paper-checklist-3220d6df500b" rel="noopener">清单</a>放在提交模板的末尾:</p><blockquote class="nh ni nj"><p id="a153" class="me mf ng mg b mh nb ka mj mk nc kd mm nk nd mp mq nl ne mt mu nm nf mx my mz ij bi translated">NeurIPS论文清单旨在鼓励负责任的机器学习研究的最佳实践，解决可重复性、透明度、研究道德和社会影响等问题。</p></blockquote><p id="cd26" class="pw-post-body-paragraph me mf iq mg b mh nb ka mj mk nc kd mm mn nd mp mq mr ne mt mu mv nf mx my mz ij bi translated">我个人非常喜欢这个清单。它指导我，一个在这个领域完全是新手的人，通过准备论文，并教会我在ML研究中的以下良好实践:</p><ul class=""><li id="cdee" class="pt pu iq mg b mh nb mk nc mn pv mr pw mv px mz py pz qa qb bi translated">承认别人的资产(代码、数据集)；</li><li id="b70a" class="pt pu iq mg b mh qc mk qd mn qe mr qf mv qg mz py pz qa qb bi translated">用可再生/可复制的代码和指令展示ML研究；</li><li id="bd46" class="pt pu iq mg b mh qc mk qd mn qe mr qf mv qg mz py pz qa qb bi translated">报告关于不同种子的误差条/带。</li></ul><p id="92cb" class="pw-post-body-paragraph me mf iq mg b mh nb ka mj mk nc kd mm mn nd mp mq mr ne mt mu mv nf mx my mz ij bi translated">受到这份清单以及LieTransformer论文中致谢的启发，我<a class="ae na" href="https://github.com/scaomath/fourier-transformer#acknowledgement" rel="noopener ugc nofollow" target="_blank">感谢在这个小项目过程中启发我的每个人</a>，即使这只是在一次偶然的随意交谈中的一句俏皮话。</p></div><div class="ab cl nn no hu np" role="separator"><span class="nq bw bk nr ns nt"/><span class="nq bw bk nr ns nt"/><span class="nq bw bk nr ns"/></div><div class="ij ik il im in"><p id="e226" class="pw-post-body-paragraph me mf iq mg b mh nb ka mj mk nc kd mm mn nd mp mq mr ne mt mu mv nf mx my mz ij bi translated"><em class="ng">原载于2021年6月6日</em><a class="ae na" href="https://scaomath.github.io/blog/galerkin-transformer-neurips" rel="noopener ugc nofollow" target="_blank"><em class="ng">https://scao math . github . io</em></a><em class="ng">。</em></p></div></div>    
</body>
</html>