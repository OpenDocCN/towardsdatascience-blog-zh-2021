<html>
<head>
<title>PCA: Beyond Dimensionality Reduction</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">主成分分析:超越降维</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/pca-beyond-the-dimensionality-reduction-e352eb0bdf52?source=collection_archive---------21-----------------------#2021-10-22">https://towardsdatascience.com/pca-beyond-the-dimensionality-reduction-e352eb0bdf52?source=collection_archive---------21-----------------------#2021-10-22</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="595a" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">了解如何使用PCA算法来寻找一起变化的变量</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/df3b506253c58ba46746b54a56d92ff6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Kxr7IWtuNfnSyQxT_9AALQ.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">照片由<a class="ae ky" href="https://unsplash.com/s/photos/variance?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的<a class="ae ky" href="https://unsplash.com/@pritesh557?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Pritesh Sudra </a>拍摄</p></figure></div><div class="ab cl kz la hx lb" role="separator"><span class="lc bw bk ld le lf"/><span class="lc bw bk ld le lf"/><span class="lc bw bk ld le"/></div><div class="im in io ip iq"><h2 id="7f65" class="lg lh it bd li lj lk dn ll lm ln dp lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated">主成分分析</h2><p id="f125" class="pw-post-body-paragraph mc md it me b mf mg ju mh mi mj jx mk lp ml mm mn lt mo mp mq lx mr ms mt mu im bi translated"><em class="mv">主成分分析</em>简称PCA，是一种基于协方差计算的数学变换。</p><p id="7412" class="pw-post-body-paragraph mc md it me b mf mw ju mh mi mx jx mk lp my mm mn lt mz mp mq lx na ms mt mu im bi translated">许多初学数据的科学家第一次接触到算法，知道它有利于降维，这意味着当我们有一个包含许多变量的宽数据集时，我们可以使用PCA将我们的数据转换成我们想要的尽可能多的<em class="mv">分量</em>，从而在预测之前减少它。</p><p id="a13e" class="pw-post-body-paragraph mc md it me b mf mw ju mh mi mx jx mk lp my mm mn lt mz mp mq lx na ms mt mu im bi translated">这是真的，实际上是一个很好的技巧。但是在这篇文章中，我想告诉你PCA的另一个好的用法:<em class="mv">验证特征是如何一起变化的。</em></p><blockquote class="nb"><p id="847d" class="nc nd it bd ne nf ng nh ni nj nk mu dk translated">协方差用于计算两个变量的移动。它表示变量之间线性关系的方向。</p></blockquote><p id="20f3" class="pw-post-body-paragraph mc md it me b mf nl ju mh mi nm jx mk lp nn mm mn lt no mp mq lx np ms mt mu im bi translated">知道了什么是协方差以及它的作用，我们就可以知道变量是一起运动，相反还是彼此独立。</p><h2 id="b102" class="lg lh it bd li lj lk dn ll lm ln dp lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated">PCA做什么</h2><p id="190b" class="pw-post-body-paragraph mc md it me b mf mg ju mh mi mj jx mk lp ml mm mn lt mo mp mq lx mr ms mt mu im bi translated">明白事情比这个解释复杂多了，不过还是简单点说吧。</p><p id="9729" class="pw-post-body-paragraph mc md it me b mf mw ju mh mi mx jx mk lp my mm mn lt mz mp mq lx na ms mt mu im bi translated">假设我们有一个三维数据集。嗯，PCA将获得您的数据集，并查看这三个维度中的哪一个可以绘制最长的线，这意味着逐点查看，我可以获得每个维度的最大差异是什么。一旦计算完成，它将画出这条线，称之为主成分1。</p><blockquote class="nb"><p id="43a3" class="nc nd it bd ne nf ng nh ni nj nk mu dk translated">第一个主成分捕获了数据的最大变化。第二台PC捕捉第二多的变化。</p></blockquote><p id="fc7b" class="pw-post-body-paragraph mc md it me b mf nl ju mh mi nm jx mk lp nn mm mn lt no mp mq lx np ms mt mu im bi translated">之后，它会转到下一个维度，画另一条线，这条线必须垂直于PC1，并保持可能的最大方差。最后，将在第三维度上完成，始终遵循垂直于先前PCs的规则，并尽可能保持最大的方差。这样，如果我们有<em class="mv"> n </em>个维度，那么这个过程将被执行<em class="mv"> n </em>次。</p><p id="a2b1" class="pw-post-body-paragraph mc md it me b mf mw ju mh mi mx jx mk lp my mm mn lt mz mp mq lx na ms mt mu im bi translated">我发现解释这一点的一个好方法是想象一个瓶子。想象它充满了来自你的数据集中的点。当然，最大的变化将是从顶部到底部。这就是PC1。然后，PC2需要垂直，所以它留给我们从一边到另一边的箭头。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nq"><img src="../Images/35ad600d00d563dc721cc5995488810d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gATkRGFg6-FFKig9yrg65w.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图1:“瓶子”数据集。计算机将如何“看到”您的数据。图片由作者提供。</p></figure><p id="048a" class="pw-post-body-paragraph mc md it me b mf mw ju mh mi mx jx mk lp my mm mn lt mz mp mq lx na ms mt mu im bi translated">在那之后，我们可以继续画很多其他的线来显示我们的数据是如何分布到所有可能的边上的。这些向量以数学方式“绘制”数据集，因此计算机可以理解它。</p><h2 id="7d0f" class="lg lh it bd li lj lk dn ll lm ln dp lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated">运行PCA</h2><p id="4b30" class="pw-post-body-paragraph mc md it me b mf mg ju mh mi mj jx mk lp ml mm mn lt mo mp mq lx mr ms mt mu im bi translated">让我们编写一点代码并运行PCA。</p><pre class="kj kk kl km gt nr ns nt nu aw nv bi"><span id="890a" class="lg lh it ns b gy nw nx l ny nz">import pandas as pd<br/>import random as rd<br/>import numpy as np<br/>from sklearn.decomposition import PCA<br/>from sklearn import preprocessing<br/>import matplotlib.pyplot as plt</span><span id="24c8" class="lg lh it ns b gy oa nx l ny nz"># Create a dataset<br/>observations = ['obs' + str(i) for i in range(1,101)]<br/>sA = ['sampleA' + str(i) for i in range(1,6)]<br/>sB = ['sampleB' + str(i) for i in range(1,6)]</span><span id="fe2c" class="lg lh it ns b gy oa nx l ny nz">data = pd.DataFrame(columns=[*sA, *sB], index=observations)</span><span id="25f5" class="lg lh it ns b gy oa nx l ny nz">for observation in data.index:<br/>  data.loc[observation, 'sampleA1':'sampleA5'] = np.random.poisson(lam=rd.randrange(10,1000), size=5)<br/>  data.loc[observation, 'sampleB1':'sampleb5'] = np.random.poisson(lam=rd.randrange(10,1000), size=5)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ob"><img src="../Images/824603e7b0efe1c366f0ee9026321297.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vEPWbCMA-D9X_yQvH9yv7g.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">表1:样本数据。图片由作者提供。</p></figure><p id="c9bb" class="pw-post-body-paragraph mc md it me b mf mw ju mh mi mx jx mk lp my mm mn lt mz mp mq lx na ms mt mu im bi translated">现在我们应该缩放数据。如果一些变量的方差很大，一些很小，PCA会为PC1画一条最长的线，这样会扭曲你的数字，使其他的PCs变得很小。是的，PCA会受到异常值的影响。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oc"><img src="../Images/8f3a690d82d680fdc8b567af493f5e46.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UBYYctkqMaRFCB2kS_L49w.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">PC1方差中的主要异常值。图片由作者提供。</p></figure><p id="b593" class="pw-post-body-paragraph mc md it me b mf mw ju mh mi mx jx mk lp my mm mn lt mz mp mq lx na ms mt mu im bi translated">因此，将变量标准化将使这种影响最小化。另一方面，如果你的变量的具体范围很重要(因为你希望你的PCA在那个范围内)，也许你不想标准化，但要意识到这个问题。</p><p id="423b" class="pw-post-body-paragraph mc md it me b mf mw ju mh mi mx jx mk lp my mm mn lt mz mp mq lx na ms mt mu im bi translated">接下来，当您运行PCA时，您将看到矩阵的行是显示组件编号的行。在我们的例子中，<strong class="me iu">我们希望看到样本是如何一起变化的</strong>，所以我将转置样本，使其成行，缩放并运行PCA。</p><pre class="kj kk kl km gt nr ns nt nu aw nv bi"><span id="037a" class="lg lh it ns b gy nw nx l ny nz"># Transpose and Scale<br/>scaled_data = preprocessing.scale(data.T)</span><span id="2487" class="lg lh it ns b gy oa nx l ny nz"># PCA instance<br/>pca = PCA()</span><span id="71bb" class="lg lh it ns b gy oa nx l ny nz">#fit (learn the parameters of the data)<br/>pca.fit(scaled_data)</span><span id="e3d1" class="lg lh it ns b gy oa nx l ny nz"># transform (apply PCA)<br/>pca_data = pca.transform(scaled_data)</span><span id="9165" class="lg lh it ns b gy oa nx l ny nz"># Creating the Scree Plot to check PCs variance explanation<br/>per_var = np.round(pca.explained_variance_ratio_*100, 1)<br/>labels = ['PC' + str(x) for x in range(1, len(per_var)+1)]</span><span id="8abc" class="lg lh it ns b gy oa nx l ny nz">plt.figure(figsize=(12,6))<br/>plt.bar(x=labels, height=per_var)<br/>plt.ylabel('Variance explained')<br/>plt.show()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi od"><img src="../Images/3daac000060539d299bf1bc9d15dff73.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MmLUxI3NrtLFcdar6erYFg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">Scree Plot:每台电脑解释了多少差异。图片由作者提供。</p></figure><h2 id="649f" class="lg lh it bd li lj lk dn ll lm ln dp lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated">特征向量，特征值</h2><p id="4bf8" class="pw-post-body-paragraph mc md it me b mf mg ju mh mi mj jx mk lp ml mm mn lt mo mp mq lx mr ms mt mu im bi translated">运行PCA后，您将收到一串数字，如表2所示。这些数字是每个主成分的<em class="mv">特征向量</em>——换句话说，这些数字“创造”了计算机用来理解你的数据的箭头。</p><p id="b611" class="pw-post-body-paragraph mc md it me b mf mw ju mh mi mx jx mk lp my mm mn lt mz mp mq lx na ms mt mu im bi translated"><em class="mv">特征值</em>代表由变量<code class="fe oe of og ns b">pca.explained_variance_</code>解释的方差的量</p><pre class="kj kk kl km gt nr ns nt nu aw nv bi"><span id="2e5f" class="lg lh it ns b gy nw nx l ny nz"># Loadings Table<br/>pca_df = pd.DataFrame(pca_data, index=[*sA, *sB], columns=labels)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oh"><img src="../Images/e8339bfbaac2e69105b11a88cdd2f173.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*itBUZuZ3iPDC453rP-oXRQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">表2:特征向量。图片由作者提供。</p></figure><p id="8ebb" class="pw-post-body-paragraph mc md it me b mf mw ju mh mi mx jx mk lp my mm mn lt mz mp mq lx na ms mt mu im bi translated">如果你想降低数据集的维数，这很简单。使用参数<code class="fe oe of og ns b">n_components</code>即可。</p><pre class="kj kk kl km gt nr ns nt nu aw nv bi"><span id="ccd4" class="lg lh it ns b gy nw nx l ny nz"># PCA instance with 3 dimensions<br/>pca = PCA(n_components=3)</span><span id="9cdc" class="lg lh it ns b gy oa nx l ny nz">#fit (learn the parameters of the data)<br/>pca.fit(scaled_data)</span><span id="4638" class="lg lh it ns b gy oa nx l ny nz"># transform (apply PCA)<br/>pca_data = pca.transform(scaled_data)</span></pre><h2 id="d509" class="lg lh it bd li lj lk dn ll lm ln dp lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated">变量关系</h2><p id="829a" class="pw-post-body-paragraph mc md it me b mf mg ju mh mi mj jx mk lp ml mm mn lt mo mp mq lx mr ms mt mu im bi translated">现在我们到了这篇文章的最后一部分。让我们超越降维。让我们学习如何理解变量是否一起浮动。</p><ul class=""><li id="30db" class="oi oj it me b mf mw mi mx lp ok lt ol lx om mu on oo op oq bi translated">看每个主成分列，注意有正负符号。</li><li id="3b6c" class="oi oj it me b mf or mi os lp ot lt ou lx ov mu on oo op oq bi translated">正样本与PC的方向相同，而负号表示样本的变化方向相反。</li><li id="4286" class="oi oj it me b mf or mi os lp ot lt ou lx ov mu on oo op oq bi translated">数字意味着力量。越高，PC中样本的方差越大。</li><li id="94df" class="oi oj it me b mf or mi os lp ot lt ou lx ov mu on oo op oq bi translated">查看每台PC解释差异的百分比— PC1 = 92%。因此，我们可以只查看PC1来了解样本之间的关系，因为PC1中解释了几乎所有的方差。</li><li id="fa86" class="oi oj it me b mf or mi os lp ot lt ou lx ov mu on oo op oq bi translated">我们可以看到，A样本朝一个方向(+)前进，而B样本朝另一个方向(-)前进。</li></ul><p id="bf52" class="pw-post-body-paragraph mc md it me b mf mw ju mh mi mx jx mk lp my mm mn lt mz mp mq lx na ms mt mu im bi translated">让我们画一张图来更好地说明这个想法。</p><pre class="kj kk kl km gt nr ns nt nu aw nv bi"><span id="e9c4" class="lg lh it ns b gy nw nx l ny nz"># PC1 x PC2 scatter plot</span><span id="2349" class="lg lh it ns b gy oa nx l ny nz">plt.figure(figsize=(12,6))<br/>plt.scatter(pca_df.PC1, pca_df.PC2)<br/>plt.title('PCA graph')<br/>plt.xlabel(f'PC1 - {per_var[0]}%')<br/>plt.ylabel(f'PC2 - {per_var[1]}%')</span><span id="c815" class="lg lh it ns b gy oa nx l ny nz">for sample in pca_df.index:<br/>  plt.annotate(sample, (pca_df.PC1.loc[sample], pca_df.PC2.loc[sample]) )</span><span id="b8e1" class="lg lh it ns b gy oa nx l ny nz">plt.show()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ow"><img src="../Images/ca862d990df40bb9397ed7575cdd6303.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vxDwq6V-iRtccPTI-9ke-w.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">令人惊讶的是样品A和B是如何排列在一起的。图片由作者提供。</p></figure><h2 id="08f2" class="lg lh it bd li lj lk dn ll lm ln dp lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated">在你走之前</h2><p id="cefb" class="pw-post-body-paragraph mc md it me b mf mg ju mh mi mj jx mk lp ml mm mn lt mo mp mq lx mr ms mt mu im bi translated">在这篇文章中，我们学习了<a class="ae ky" href="https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html" rel="noopener ugc nofollow" target="_blank"> PCA算法</a>的另一个很好的用途，那就是了解哪些变量是相关的，哪些变量“浮动”在一起，哪些变量可以成为特征选择的有趣工具或者可以与聚类相结合。</p><p id="7662" class="pw-post-body-paragraph mc md it me b mf mw ju mh mi mx jx mk lp my mm mn lt mz mp mq lx na ms mt mu im bi translated">观看StatQuest 的这个视频，你可以学到更多东西，这是我找到的关于PCA的最好的视频，也是我经常回来咨询的视频。</p><p id="939d" class="pw-post-body-paragraph mc md it me b mf mw ju mh mi mx jx mk lp my mm mn lt mz mp mq lx na ms mt mu im bi translated">另一个考虑因素:</p><ul class=""><li id="fe9a" class="oi oj it me b mf mw mi mx lp ok lt ol lx om mu on oo op oq bi translated">PCA基于协方差分析，受离群值影响。</li><li id="dd78" class="oi oj it me b mf or mi os lp ot lt ou lx ov mu on oo op oq bi translated">它可以用来降低维数。</li><li id="e91e" class="oi oj it me b mf or mi os lp ot lt ou lx ov mu on oo op oq bi translated">可用于检查数据集中的要素是如何相关的。</li><li id="9a3f" class="oi oj it me b mf or mi os lp ot lt ou lx ov mu on oo op oq bi translated">如果Scree图(显示百分比的图)显示PC1的方差较低，如20%，您可能需要更多的PCs来合计至少80%的解释方差，以执行您的分析。比如下面的例子。</li></ul><pre class="kj kk kl km gt nr ns nt nu aw nv bi"><span id="f028" class="lg lh it ns b gy nw nx l ny nz"># Multiply (weigh) by the explained proportion<br/>pca_df2 = pca_df.mul(pca.explained_variance_ratio_)</span><span id="a5e2" class="lg lh it ns b gy oa nx l ny nz"># Sum of the components<br/>pca_df2 = pd.DataFrame({'fabricante':pca_df2.index, 'tt_component': pca_df2.sum(axis=1)}).sort_values(by='tt_component')</span><span id="8992" class="lg lh it ns b gy oa nx l ny nz">pca_df2.plot.scatter(x='tt_component', y='fabricante', figsize=(8,8))</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ox"><img src="../Images/4088c19402c16624fdf9eda85e62cdb2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1206/format:webp/1*fcb_MIxym-UFlbEj1UoQdw.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">用于差异分析的总件数。图片由作者提供。</p></figure><p id="d952" class="pw-post-body-paragraph mc md it me b mf mw ju mh mi mx jx mk lp my mm mn lt mz mp mq lx na ms mt mu im bi translated">代码在GitHub，<a class="ae ky" href="https://github.com/gurezende/Studying/blob/master/Python/PCA/PCA_Example_Python.ipynb" rel="noopener ugc nofollow" target="_blank">在这里</a>。</p><p id="311c" class="pw-post-body-paragraph mc md it me b mf mw ju mh mi mx jx mk lp my mm mn lt mz mp mq lx na ms mt mu im bi translated">如果你对这些内容感兴趣，请关注我的博客。</p><div class="oy oz gp gr pa pb"><a href="https://medium.com/gustavorsantos" rel="noopener follow" target="_blank"><div class="pc ab fo"><div class="pd ab pe cl cj pf"><h2 class="bd iu gy z fp pg fr fs ph fu fw is bi translated">古斯塔夫·桑托斯</h2><div class="pi l"><h3 class="bd b gy z fp pg fr fs ph fu fw dk translated">让我们做出更好的决定。数据驱动的决策。我用Python，R，Excel，SQL创建数据科学项目。</h3></div><div class="pj l"><p class="bd b dl z fp pg fr fs ph fu fw dk translated">medium.com</p></div></div><div class="pk l"><div class="pl l pm pn po pk pp ks pb"/></div></div></a></div></div></div>    
</body>
</html>