<html>
<head>
<title>3 Techniques to Avoid Overfitting of Decision Trees</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">避免决策树过度拟合的3个技巧</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/3-techniques-to-avoid-overfitting-of-decision-trees-1e7d3d985a09?source=collection_archive---------0-----------------------#2021-05-31">https://towardsdatascience.com/3-techniques-to-avoid-overfitting-of-decision-trees-1e7d3d985a09?source=collection_archive---------0-----------------------#2021-05-31</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="c5e7" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">决策树的预剪枝、后剪枝和集成的实际实现</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/b69fa95beac6e084b043567516fd218a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HBwyH019Y5wrSwcPtoCjZw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图片由来自<a class="ae ky" href="https://pixabay.com/?utm_source=link-attribution&amp;amp;utm_medium=referral&amp;amp;utm_campaign=image&amp;amp;utm_content=3839456" rel="noopener ugc nofollow" target="_blank">皮克斯拜</a>的<a class="ae ky" href="https://pixabay.com/users/thedigitalartist-202249/?utm_source=link-attribution&amp;amp;utm_medium=referral&amp;amp;utm_campaign=image&amp;amp;utm_content=3839456" rel="noopener ugc nofollow" target="_blank">皮特·林福思</a>拍摄</p></figure><p id="f1c5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">决策树是一种用于分类和回归任务的非参数监督机器学习方法。过度拟合是一个常见的问题，数据科学家在训练决策树模型时需要处理这个问题。与其他机器学习算法相比，决策树很容易过拟合。</p><h2 id="8fe8" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated">你的决策树是否过拟合？</h2><p id="4fc9" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">过度拟合是指模型完全符合训练数据，但无法概括测试中未发现的数据的情况。当模型记忆了训练数据的噪声并且未能捕获重要的模式时，过拟合情况出现。完美匹配的决策树对训练数据表现良好，但对看不见的测试数据表现不佳。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mt"><img src="../Images/199805e0bf59ff620ae5ea6aac57f01d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*h4XETbo3hFx9PtKKJH90Dg.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">(<a class="ae ky" href="https://www.google.com/url?sa=i&amp;url=https%3A%2F%2Ftowardsdatascience.com%2Fbias-and-variance-but-what-are-they-really-ac539817e171&amp;psig=AOvVaw3D08-zRrjLyyleDVGes2WR&amp;ust=1622541865558000&amp;source=images&amp;cd=vfe&amp;ved=0CAIQjRxqFwoTCOC8rI3V8_ACFQAAAAAdAAAAABAP" rel="noopener ugc nofollow" target="_blank">来源</a>)，偏差-方差权衡</p></figure><p id="be01" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果允许决策树训练到最大强度，模型将会过度拟合训练数据。有多种技术可以防止决策树模型过度拟合。在本文中，我们将讨论3种这样的技术。</p><pre class="kj kk kl km gt mu mv mw mx aw my bi"><span id="c248" class="lv lw it mv b gy mz na l nb nc"><strong class="mv iu"><em class="nd">Technique to discuss in this article:<br/></em>Pruning<br/><em class="nd">* Pre-pruning<br/>* Post-pruning</em><br/>Ensemble<br/><em class="nd">* Random Forest</em></strong></span></pre><h1 id="c355" class="ne lw it bd lx nf ng nh ma ni nj nk md jz nl ka mg kc nm kd mj kf nn kg mm no bi translated">什么是修剪？</h1><p id="b869" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">默认情况下，决策树模型可以增长到其最大深度。修剪指的是一种删除决策树的部分以防止增长到其全部深度的技术。通过调整决策树模型的超参数，可以修剪树并防止它们过度拟合。</p><p id="8c72" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">有两种类型的修剪:修剪前和修剪后。现在让我们来讨论对每种修剪技术的深入理解和实际操作。</p><h1 id="20fb" class="ne lw it bd lx nf ng nh ma ni nj nk md jz nl ka mg kc nm kd mj kf nn kg mm no bi translated">预修剪:</h1><p id="754d" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">预修剪技术指的是提前停止决策树的生长。预修剪技术包括在训练流水线之前调整决策树模型的超参数。决策树的超参数包括<code class="fe np nq nr mv b"><strong class="lb iu">max_depth</strong></code>、<code class="fe np nq nr mv b"><strong class="lb iu">min_samples_leaf</strong></code>、<code class="fe np nq nr mv b"><strong class="lb iu">min_samples_split </strong></code>可以被调整，以提前停止树的生长，防止模型过拟合。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/7628fc6f5884af7e8c12e4cbb6f8b303.png" data-original-src="https://miro.medium.com/v2/resize:fit:892/format:webp/1*aHzr6MT5yNm3G55xGt7sZg.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">(图片由作者提供)，AUC-ROC分数与最大深度</p></figure><p id="4bfc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">从上面的图中观察到，随着max_depth训练的增加，AUC-ROC分数连续增加，但是在max depth的值之后，测试AUC分数保持恒定。最佳决策树的最大深度值为5。进一步增加最大深度值会导致过度拟合问题。</p><p id="94c1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><code class="fe np nq nr mv b"><strong class="lb iu">max_depth</strong></code>、<code class="fe np nq nr mv b"><strong class="lb iu">min_samples_leaf</strong></code>、<code class="fe np nq nr mv b"><strong class="lb iu">min_samples_split</strong></code>是决策树算法的其他超参数，它们可以被调整以获得健壮的模型。可以使用GridSearchCV技术的sklearn实现来为决策树模型找到最佳的超参数集。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nt nu l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">(作者代码)</p></figure><h1 id="2771" class="ne lw it bd lx nf ng nh ma ni nj nk md jz nl ka mg kc nm kd mj kf nn kg mm no bi translated">后期修剪:</h1><p id="ee40" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">后剪枝技术允许决策树模型增长到其最大深度，然后移除树分支以防止模型过度拟合。<strong class="lb iu"> <em class="nd">成本复杂度剪枝(ccp) </em> </strong>是后剪枝技术的一种。在成本复杂性修剪的情况下，可以调整<code class="fe np nq nr mv b"><strong class="lb iu">ccp_alpha</strong></code> <strong class="lb iu"> </strong>以获得最佳拟合模型。</p><p id="5086" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">Scikit-learn包附带了使用函数<code class="fe np nq nr mv b"><strong class="lb iu">cost_complexity_pruning_path()</strong></code>计算决策树的<code class="fe np nq nr mv b"><strong class="lb iu">ccp_alpha </strong></code>值的实现。随着<code class="fe np nq nr mv b"><strong class="lb iu">ccp_apha </strong></code>值的增加，树的更多节点被修剪。</p><p id="bc00" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">成本复杂性削减(后期削减)的步骤如下:</p><ul class=""><li id="8c32" class="nv nw it lb b lc ld lf lg li nx lm ny lq nz lu oa ob oc od bi translated">将决策树分类器训练到其最大深度(默认超参数)。</li><li id="55bc" class="nv nw it lb b lc oe lf of li og lm oh lq oi lu oa ob oc od bi translated">使用函数<code class="fe np nq nr mv b"><strong class="lb iu">cost_complexity_pruning_path().</strong></code>计算ccp阿尔法值</li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/0d4e79d3d129df4bc676814c74baee61.png" data-original-src="https://miro.medium.com/v2/resize:fit:1060/format:webp/1*byQoVL-YaHVVBT72KmweYg.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">(图片由作者提供)，ccp阿尔法值</p></figure><ul class=""><li id="99f1" class="nv nw it lb b lc ld lf lg li nx lm ny lq nz lu oa ob oc od bi translated">用不同的<code class="fe np nq nr mv b"><strong class="lb iu">ccp_alphas</strong></code> <strong class="lb iu"> </strong>值训练决策树分类器，并计算训练和测试性能得分。</li><li id="3914" class="nv nw it lb b lc oe lf of li og lm oh lq oi lu oa ob oc od bi translated">绘制训练和测试分数的每个值为<code class="fe np nq nr mv b"><strong class="lb iu">ccp_alphas</strong></code> <strong class="lb iu"> </strong>值。</li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/38b5d1b900436c371ec10a5dfe4c04b6.png" data-original-src="https://miro.medium.com/v2/resize:fit:892/format:webp/1*4a0ht0gD3Kpht2B73dKqug.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">(图片由作者提供)，AUC-ROC评分vs CCP _阿尔法斯</p></figure><blockquote class="ok ol om"><p id="6f41" class="kz la nd lb b lc ld ju le lf lg jx lh on lj lk ll oo ln lo lp op lr ls lt lu im bi translated">从上面的图中，CCPα= 0.000179可以被认为是最佳参数，因为训练和测试的AUC-ROC分数分别为0.94和0.92。</p></blockquote><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nt nu l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">(作者代码)，后期修剪</p></figure><div class="kj kk kl km gt ab cb"><figure class="oq kn or os ot ou ov paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><img src="../Images/4e00a7eeba3fdd87e8af5b39ee8ca1a9.png" data-original-src="https://miro.medium.com/v2/resize:fit:978/format:webp/1*KbRVJC5B0EgO8YAyeCrLkA.png"/></div></figure><figure class="oq kn ow os ot ou ov paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><img src="../Images/c746df2d045b11d0d1ed9316d03b111a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1024/format:webp/1*_kWlVHWMwkUKu433ykVvhA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk ox di oy oz translated">(图片由作者提供)，<strong class="bd pa">左:</strong>未修剪的决策树，<strong class="bd pa">右:</strong>修剪后的决策树</p></figure></div><h1 id="2d8c" class="ne lw it bd lx nf ng nh ma ni nj nk md jz nl ka mg kc nm kd mj kf nn kg mm no bi translated">集合—随机森林:</h1><p id="dd06" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">随机森林是通过引导多个决策树进行分类和回归的集成技术。随机森林遵循引导抽样和聚集技术，以防止过度拟合。</p><blockquote class="ok ol om"><p id="1f6c" class="kz la nd lb b lc ld ju le lf lg jx lh on lj lk ll oo ln lo lp op lr ls lt lu im bi translated">阅读下面提到的文章中的<a class="ae ky" rel="noopener" target="_blank" href="/improving-the-performance-of-machine-learning-model-using-bagging-534cf4a076a7">，深入了解随机森林和其他合奏技术。</a></p></blockquote><div class="pb pc gp gr pd pe"><a rel="noopener follow" target="_blank" href="/improving-the-performance-of-machine-learning-model-using-bagging-534cf4a076a7"><div class="pf ab fo"><div class="pg ab ph cl cj pi"><h2 class="bd iu gy z fp pj fr fs pk fu fw is bi translated">使用Bagging提高机器学习模型的性能</h2><div class="pl l"><h3 class="bd b gy z fp pj fr fs pk fu fw dk translated">了解Bootstrap Aggregation (Bagging)集成学习的工作原理，并实现随机森林Bagging模型…</h3></div><div class="pm l"><p class="bd b dl z fp pj fr fs pk fu fw dk translated">towardsdatascience.com</p></div></div><div class="pn l"><div class="po l pp pq pr pn ps ks pe"/></div></div></a></div><p id="c0e9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">可以使用Scikit-Learn库实现随机森林。您可以进一步调整随机森林算法的超参数，以提高模型的性能。<code class="fe np nq nr mv b"><strong class="lb iu">n_estimator</strong></code>可以调整参数以减少模型的过拟合。</p><h1 id="0a89" class="ne lw it bd lx nf ng nh ma ni nj nk md jz nl ka mg kc nm kd mj kf nn kg mm no bi translated">结论:</h1><p id="0b10" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">在本文中，我们讨论了防止决策树模型过度拟合的技术。预修剪和后修剪技术可以用来处理过拟合的问题。仅通过训练，具有默认超参数的随机森林模型不能完全减轻过度拟合的问题，因此需要进一步调整它。</p><h1 id="8c7a" class="ne lw it bd lx nf ng nh ma ni nj nk md jz nl ka mg kc nm kd mj kf nn kg mm no bi translated">参考资料:</h1><p id="4b08" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">[1] Scikit-Learn文档:<a class="ae ky" href="https://scikit-learn.org/stable/auto_examples/tree/plot_cost_complexity_pruning.html" rel="noopener ugc nofollow" target="_blank">https://sci kit-Learn . org/stable/auto _ examples/tree/plot _ cost _ complexity _ pruning . html</a></p></div><div class="ab cl pt pu hx pv" role="separator"><span class="pw bw bk px py pz"/><span class="pw bw bk px py pz"/><span class="pw bw bk px py"/></div><div class="im in io ip iq"><p id="9c5d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="nd">喜欢这篇文章吗？成为</em> <a class="ae ky" href="https://satyam-kumar.medium.com/membership" rel="noopener"> <em class="nd">中等会员</em> </a> <em class="nd">继续无限制学习。如果你使用下面的链接，我会收到你的一小部分会员费，不需要你额外付费。</em></p><div class="pb pc gp gr pd pe"><a href="https://satyam-kumar.medium.com/membership" rel="noopener follow" target="_blank"><div class="pf ab fo"><div class="pg ab ph cl cj pi"><h2 class="bd iu gy z fp pj fr fs pk fu fw is bi translated">加入我的推荐链接-萨蒂扬库马尔媒体</h2><div class="pl l"><h3 class="bd b gy z fp pj fr fs pk fu fw dk translated">作为一个媒体会员，你的会员费的一部分会给你阅读的作家，你可以完全接触到每一个故事…</h3></div><div class="pm l"><p class="bd b dl z fp pj fr fs pk fu fw dk translated">satyam-kumar.medium.com</p></div></div><div class="pn l"><div class="qa l pp pq pr pn ps ks pe"/></div></div></a></div><blockquote class="qb"><p id="b5a7" class="qc qd it bd qe qf qg qh qi qj qk lu dk translated">感谢您的阅读</p></blockquote></div></div>    
</body>
</html>