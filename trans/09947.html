<html>
<head>
<title>5 Techniques to work with Imbalanced Data in Machine Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">机器学习中处理不平衡数据的5种技巧</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/5-techniques-to-work-with-imbalanced-data-in-machine-learning-80836d45d30c?source=collection_archive---------10-----------------------#2021-09-19">https://towardsdatascience.com/5-techniques-to-work-with-imbalanced-data-in-machine-learning-80836d45d30c?source=collection_archive---------10-----------------------#2021-09-19</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="a7cb" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">处理不平衡数据集的基本指南</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/bcc5793478487bc31db00efdf781962b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1myd9o6qNlJVnJx2FYSO_w.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图片来自<a class="ae ky" href="https://pixabay.com/?utm_source=link-attribution&amp;amp;utm_medium=referral&amp;amp;utm_campaign=image&amp;amp;utm_content=2178566" rel="noopener ugc nofollow" target="_blank"> Pixabay </a>的<a class="ae ky" href="https://pixabay.com/users/pexels-2286921/?utm_source=link-attribution&amp;amp;utm_medium=referral&amp;amp;utm_campaign=image&amp;amp;utm_content=2178566" rel="noopener ugc nofollow" target="_blank">像素</a></p></figure><p id="c16b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对于分类任务，可能会遇到目标类标签在各个类之间分布不均匀的情况。这种情况被称为不平衡的目标类。对不平衡数据集进行建模是数据科学家面临的一个主要挑战，因为由于数据中存在不平衡，模型会偏向多数类预测。</p><p id="0a6d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">因此，在模型训练之前，处理数据集中的不平衡是至关重要的。在处理不平衡数据时，需要记住各种事情。在本文中，我们将讨论各种处理类不平衡的技术，以训练一个健壮的和非常适合的机器学习模型。</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="ef28" class="ma mb it lw b gy mc md l me mf"><strong class="lw iu"><em class="mg">Checklist:</em></strong><br/><strong class="lw iu">1) Upsampling Minority Class<br/>2) Downsampling Majority Class<br/>3) Generate Synthetic Data<br/>4) Combine Upsampling &amp; Downsampling Techniques <br/>5) Balanced Class Weight</strong></span></pre><p id="329d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在讨论上述5种技术之前，让我们集中精力为不平衡的数据集任务选择正确的指标。选择一个不正确的度量标准，如准确性，看起来表现不错，但实际上偏向于多数类标签。性能指标的替代选择可以是:</p><ul class=""><li id="aefe" class="mh mi it lb b lc ld lf lg li mj lm mk lq ml lu mm mn mo mp bi translated">AUC-ROC评分</li><li id="6301" class="mh mi it lb b lc mq lf mr li ms lm mt lq mu lu mm mn mo mp bi translated">精确度、召回率和F1分数</li><li id="7334" class="mh mi it lb b lc mq lf mr li ms lm mt lq mu lu mm mn mo mp bi translated">TP、FP、FN、TN可视化的混淆矩阵</li></ul><p id="d76f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在为您的案例研究选择了正确的指标之后，您可以使用各种技术来处理数据集中的不平衡。</p><h1 id="4ba8" class="mv mb it bd mw mx my mz na nb nc nd ne jz nf ka ng kc nh kd ni kf nj kg nk nl bi translated">1.)上采样少数类:</h1><p id="bf4c" class="pw-post-body-paragraph kz la it lb b lc nm ju le lf nn jx lh li no lk ll lm np lo lp lq nq ls lt lu im bi translated">上采样或过采样是指创建人工或复制数据点或少数类样本以平衡类标签的技术。有多种过采样技术可用于创建人工数据点。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/0a4027f2c19389ae56da86f0c84cbc7d.png" data-original-src="https://miro.medium.com/v2/resize:fit:658/format:webp/1*Hgy61fKGaf98qz-rTBDtxQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">(<a class="ae ky" href="https://www.kaggle.com/rafjaa/resampling-strategies-for-imbalanced-datasets" rel="noopener ugc nofollow" target="_blank">来源</a>)，过采样插图</p></figure><blockquote class="ns nt nu"><p id="981f" class="kz la mg lb b lc ld ju le lf lg jx lh nv lj lk ll nw ln lo lp nx lr ls lt lu im bi translated">阅读下面提到的文章，更好地理解7种过采样技术:</p></blockquote><div class="ny nz gp gr oa ob"><a rel="noopener follow" target="_blank" href="/7-over-sampling-techniques-to-handle-imbalanced-data-ec51c8db349f"><div class="oc ab fo"><div class="od ab oe cl cj of"><h2 class="bd iu gy z fp og fr fs oh fu fw is bi translated">7处理不平衡数据的过采样技术</h2><div class="oi l"><h3 class="bd b gy z fp og fr fs oh fu fw dk translated">各种过采样技术的深度分析</h3></div><div class="oj l"><p class="bd b dl z fp og fr fs oh fu fw dk translated">towardsdatascience.com</p></div></div><div class="ok l"><div class="ol l om on oo ok op ks ob"/></div></div></a></div><h1 id="ac8f" class="mv mb it bd mw mx my mz na nb nc nd ne jz nf ka ng kc nh kd ni kf nj kg nk nl bi translated">2.)下采样多数类:</h1><p id="d4e8" class="pw-post-body-paragraph kz la it lb b lc nm ju le lf nn jx lh li no lk ll lm np lo lp lq nq ls lt lu im bi translated">下采样或欠采样指的是移除或减少大部分类别样本以平衡类别标签。imblearn包中实现了各种欠采样技术，包括:</p><ul class=""><li id="ae39" class="mh mi it lb b lc ld lf lg li mj lm mk lq ml lu mm mn mo mp bi translated">随机欠采样</li><li id="4040" class="mh mi it lb b lc mq lf mr li ms lm mt lq mu lu mm mn mo mp bi translated">Tomek链接</li><li id="b3b1" class="mh mi it lb b lc mq lf mr li ms lm mt lq mu lu mm mn mo mp bi translated">近似采样</li><li id="7fe6" class="mh mi it lb b lc mq lf mr li ms lm mt lq mu lu mm mn mo mp bi translated">ENN(编辑最近的邻居)</li></ul><p id="1845" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">还有很多。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oq"><img src="../Images/6455409b2338c584ac7359bc7bf4b34e.png" data-original-src="https://miro.medium.com/v2/resize:fit:662/format:webp/1*RfRHzqUI0753EjN35oI8Zg.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">(<a class="ae ky" href="https://www.kaggle.com/rafjaa/resampling-strategies-for-imbalanced-datasets" rel="noopener ugc nofollow" target="_blank">来源</a>)，欠采样插图</p></figure><blockquote class="ns nt nu"><p id="5a36" class="kz la mg lb b lc ld ju le lf lg jx lh nv lj lk ll nw ln lo lp nx lr ls lt lu im bi translated">按照imblearn文档获取上述每种技术的实现:</p></blockquote><div class="ny nz gp gr oa ob"><a href="https://imbalanced-learn.org/stable/references/under_sampling.html" rel="noopener  ugc nofollow" target="_blank"><div class="oc ab fo"><div class="od ab oe cl cj of"><h2 class="bd iu gy z fp og fr fs oh fu fw is bi translated">欠采样方法-版本0.8.0</h2><div class="oi l"><h3 class="bd b gy z fp og fr fs oh fu fw dk translated">imblearn.under_sampling提供了对数据集进行欠采样的方法。的…</h3></div><div class="oj l"><p class="bd b dl z fp og fr fs oh fu fw dk translated">imbalanced-learn.org</p></div></div><div class="ok l"><div class="or l om on oo ok op ks ob"/></div></div></a></div><h1 id="5332" class="mv mb it bd mw mx my mz na nb nc nd ne jz nf ka ng kc nh kd ni kf nj kg nk nl bi translated">3.)<strong class="ak">生成合成数据:</strong></h1><p id="0be2" class="pw-post-body-paragraph kz la it lb b lc nm ju le lf nn jx lh li no lk ll lm np lo lp lq nq ls lt lu im bi translated">不推荐使用欠采样技术，因为它会移除大多数类数据点。生成少数样本的合成数据点是一种过采样技术。其思想是在少数类样本的邻近区域或邻域中生成少数类样本的合成数据点。</p><p id="63b6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">SMOTE(合成少数过采样技术)是一种流行的合成数据生成过采样技术，它利用<strong class="lb iu">k-最近邻</strong>算法来创建合成数据。</p><p id="620c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">SMOTE有多种变体，包括:</p><ul class=""><li id="eedd" class="mh mi it lb b lc ld lf lg li mj lm mk lq ml lu mm mn mo mp bi translated">SMOTENC:连续和分类特征的SMOTE变量。</li><li id="f8c5" class="mh mi it lb b lc mq lf mr li ms lm mt lq mu lu mm mn mo mp bi translated">SMOTE:只有分类特征的数据的SMOTE变量。</li><li id="af62" class="mh mi it lb b lc mq lf mr li ms lm mt lq mu lu mm mn mo mp bi translated">边界SMOTE:将使用边界样本生成新的合成样本。</li><li id="66dd" class="mh mi it lb b lc mq lf mr li ms lm mt lq mu lu mm mn mo mp bi translated">SVMSMOTE:使用SVM算法来检测样本，以用于生成新的合成样本。</li><li id="e1e5" class="mh mi it lb b lc mq lf mr li ms lm mt lq mu lu mm mn mo mp bi translated">KMeansSMOTE:在使用SMOTE进行过采样之前，使用k均值聚类进行过采样。</li><li id="eb2a" class="mh mi it lb b lc mq lf mr li ms lm mt lq mu lu mm mn mo mp bi translated">自适应合成(ADASYN):类似于SMOTE，但它根据要过采样的类的局部分布的估计生成不同数量的样本。</li></ul><blockquote class="ns nt nu"><p id="3ee5" class="kz la mg lb b lc ld ju le lf lg jx lh nv lj lk ll nw ln lo lp nx lr ls lt lu im bi translated">按照<a class="ae ky" href="https://imbalanced-learn.org/stable/references/generated/imblearn.over_sampling.SMOTE.html" rel="noopener ugc nofollow" target="_blank"> Imblearn文档</a>执行上述SMOTE技术:</p></blockquote><h1 id="8dd5" class="mv mb it bd mw mx my mz na nb nc nd ne jz nf ka ng kc nh kd ni kf nj kg nk nl bi translated">4.)结合过采样和欠采样技术:</h1><p id="65d2" class="pw-post-body-paragraph kz la it lb b lc nm ju le lf nn jx lh li no lk ll lm np lo lp lq nq ls lt lu im bi translated">不推荐使用欠采样技术，因为它会移除大多数类数据点。过采样技术通常被认为比欠采样技术更好。其思想是将欠采样和过采样技术结合起来，以创建适合模型训练的稳健平衡数据集。</p><p id="7fae" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">其思想是首先使用过采样技术来创建重复和人工数据点，并使用欠采样技术来去除噪声或不必要的生成数据点。</p><p id="daf8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">Imblearn库实现了组合过采样和欠采样技术，例如:</p><ul class=""><li id="3636" class="mh mi it lb b lc ld lf lg li mj lm mk lq ml lu mm mn mo mp bi translated"><strong class="lb iu"> Smote-Tomek </strong> : Smote(过采样器)结合TomekLinks(欠采样器)。</li><li id="dbea" class="mh mi it lb b lc mq lf mr li ms lm mt lq mu lu mm mn mo mp bi translated"><strong class="lb iu"> Smote-ENN </strong> : Smote(过采样器)结合ENN(欠采样器)。</li></ul><blockquote class="ns nt nu"><p id="d983" class="kz la mg lb b lc ld ju le lf lg jx lh nv lj lk ll nw ln lo lp nx lr ls lt lu im bi translated">按照<a class="ae ky" href="https://imbalanced-learn.org/stable/combine.html" rel="noopener ugc nofollow" target="_blank"> Imblearn文档</a>执行Smote-Tomek和Smote-ENN技术。</p></blockquote><h1 id="9b1a" class="mv mb it bd mw mx my mz na nb nc nd ne jz nf ka ng kc nh kd ni kf nj kg nk nl bi translated">5.)平衡级重量:</h1><p id="7382" class="pw-post-body-paragraph kz la it lb b lc nm ju le lf nn jx lh li no lk ll lm np lo lp lq nq ls lt lu im bi translated">欠采样技术移除了导致数据丢失的多数类数据点，而上采样创建了少数类的人工数据点。在机器学习的训练过程中，可以使用<code class="fe os ot ou lw b"><strong class="lb iu">class_weight </strong></code>参数来处理数据集中的不平衡。</p><p id="0792" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">Scikit-learn为所有机器学习算法提供了<code class="fe os ot ou lw b"><strong class="lb iu">class_weight</strong></code>参数。</p><ul class=""><li id="523f" class="mh mi it lb b lc ld lf lg li mj lm mk lq ml lu mm mn mo mp bi translated"><strong class="lb iu">类别权重—平衡</strong>:类别权重与输入数据中的类别频率成反比。</li></ul><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="4cac" class="ma mb it lw b gy mc md l me mf"><em class="mg">Computation Formula:</em><br/><strong class="lw iu">n_samples / (n_classes * np.bincount(y))</strong></span></pre><ul class=""><li id="7152" class="mh mi it lb b lc ld lf lg li mj lm mk lq ml lu mm mn mo mp bi translated"><strong class="lb iu"> {class_label: weight} </strong>:假设，目标类标签为0和1。将输入作为<strong class="lb iu"> class_weight={0:2，1:1} </strong>传递意味着类0的权重为2，类1的权重为1。</li></ul><h1 id="4e27" class="mv mb it bd mw mx my mz na nb nc nd ne jz nf ka ng kc nh kd ni kf nj kg nk nl bi translated">结论:</h1><p id="4ca9" class="pw-post-body-paragraph kz la it lb b lc nm ju le lf nn jx lh li no lk ll lm np lo lp lq nq ls lt lu im bi translated">在训练稳健模型时，数据中类别不平衡的存在可能是一个主要挑战。上述技术可用于在训练机器学习模型之前处理类别不平衡。</p><p id="0fde" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">还可以采用成本敏感学习或惩罚增加多数类分类成本的算法。此外，决策树和随机森林应该优先于其他机器学习算法，因为它们往往在不平衡的数据上表现良好。</p><h1 id="3172" class="mv mb it bd mw mx my mz na nb nc nd ne jz nf ka ng kc nh kd ni kf nj kg nk nl bi translated">参考资料:</h1><p id="c4bb" class="pw-post-body-paragraph kz la it lb b lc nm ju le lf nn jx lh li no lk ll lm np lo lp lq nq ls lt lu im bi translated">[1.] Imblearn文档:<a class="ae ky" href="https://imbalanced-learn.org/stable/index.html" rel="noopener ugc nofollow" target="_blank">https://imbalanced-learn.org/stable/index.html</a></p></div><div class="ab cl ov ow hx ox" role="separator"><span class="oy bw bk oz pa pb"/><span class="oy bw bk oz pa pb"/><span class="oy bw bk oz pa"/></div><div class="im in io ip iq"><p id="d711" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="mg">喜欢这篇文章吗？成为</em> <a class="ae ky" href="https://satyam-kumar.medium.com/membership" rel="noopener"> <em class="mg">中等会员</em> </a> <em class="mg">继续无限制学习。如果你使用下面的链接，我会收到你的一小部分会员费，不需要你额外付费。</em></p><div class="ny nz gp gr oa ob"><a href="https://satyam-kumar.medium.com/membership" rel="noopener follow" target="_blank"><div class="oc ab fo"><div class="od ab oe cl cj of"><h2 class="bd iu gy z fp og fr fs oh fu fw is bi translated">加入我的推荐链接-萨蒂扬库马尔媒体</h2><div class="oi l"><h3 class="bd b gy z fp og fr fs oh fu fw dk translated">作为一个媒体会员，你的会员费的一部分会给你阅读的作家，你可以完全接触到每一个故事…</h3></div><div class="oj l"><p class="bd b dl z fp og fr fs oh fu fw dk translated">satyam-kumar.medium.com</p></div></div><div class="ok l"><div class="pc l om on oo ok op ks ob"/></div></div></a></div><blockquote class="pd"><p id="28fa" class="pe pf it bd pg ph pi pj pk pl pm lu dk translated">感谢您的阅读</p></blockquote></div></div>    
</body>
</html>