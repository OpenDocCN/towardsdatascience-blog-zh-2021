<html>
<head>
<title>LSTM and Bidirectional LSTM for Regression</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用于回归的LSTM和双向LSTM</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/lstm-and-bidirectional-lstm-for-regression-4fddf910c655?source=collection_archive---------0-----------------------#2021-06-26">https://towardsdatascience.com/lstm-and-bidirectional-lstm-for-regression-4fddf910c655?source=collection_archive---------0-----------------------#2021-06-26</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="fcfe" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">学习如何使用长短期记忆网络解决回归问题</h2></div><p id="5bf6" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi lb translated">STM代表长短期记忆，是1997年提出的一个模型[1]。LSTM是一个门控递归神经网络，双向LSTM只是该模型的一个扩展。关键特征是这些网络可以存储信息，这些信息可用于未来的细胞处理。我们可以把LSTM想象成一个RNN，有两个关键向量的内存池:</p><ul class=""><li id="1b9c" class="lk ll iq kh b ki kj kl km ko lm ks ln kw lo la lp lq lr ls bi translated">(1)短期状态:保持当前时间步的输出。</li><li id="b996" class="lk ll iq kh b ki lt kl lu ko lv ks lw kw lx la lp lq lr ls bi translated">(2)长期状态:在通过网络时存储、读取和拒绝对长期有意义的项目。</li></ul><p id="c799" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">读取、存储和写入的决定基于图1中的一些激活函数。这些激活函数的输出是(0，1)之间的值。</p><figure class="lz ma mb mc gt md gh gi paragraph-image"><div role="button" tabindex="0" class="me mf di mg bf mh"><div class="gh gi ly"><img src="../Images/b2552914ea94f4cf66ef08fae6986579.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*V630gG25SFelbMQhsrGQDw.png"/></div></div><p class="mk ml gj gh gi mm mn bd b be z dk translated">图1:LSTM单元的架构。所有图片由作者提供。</p></figure><p id="edfb" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">遗忘门和输出门决定是保留还是丢弃新信息。LSTM模块的记忆和输出门的条件产生模型决策。然后，输出作为输入再次传递给网络，形成循环序列。</p><h1 id="d92c" class="mo mp iq bd mq mr ms mt mu mv mw mx my jw mz jx na jz nb ka nc kc nd kd ne nf bi translated">双向LSTM</h1><p id="94ec" class="pw-post-body-paragraph kf kg iq kh b ki ng jr kk kl nh ju kn ko ni kq kr ks nj ku kv kw nk ky kz la ij bi translated">在双向LSTM中，我们引入了两个模型，而不是训练一个模型。第一个模型学习所提供的输入序列，第二个模型学习该序列的相反序列。</p><p id="621f" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">因为我们已经训练了两个模型，我们需要建立一个机制来结合两者。它通常被称为合并步骤。合并可以是以下功能之一:</p><ul class=""><li id="908c" class="lk ll iq kh b ki kj kl km ko lm ks ln kw lo la lp lq lr ls bi translated">总和</li><li id="ee63" class="lk ll iq kh b ki lt kl lu ko lv ks lw kw lx la lp lq lr ls bi translated">增加</li><li id="1734" class="lk ll iq kh b ki lt kl lu ko lv ks lw kw lx la lp lq lr ls bi translated">求平均值</li><li id="deed" class="lk ll iq kh b ki lt kl lu ko lv ks lw kw lx la lp lq lr ls bi translated">串联(默认)</li></ul><h1 id="fae7" class="mo mp iq bd mq mr ms mt mu mv mw mx my jw mz jx na jz nb ka nc kc nd kd ne nf bi translated">使用Keras的回归预测:</h1><p id="7626" class="pw-post-body-paragraph kf kg iq kh b ki ng jr kk kl nh ju kn ko ni kq kr ks nj ku kv kw nk ky kz la ij bi translated">LSTM可以在许多问题上提供帮助，这些问题涉及多个领域。LSTM模型可用于检测网络违规或意外的系统行为，或者信用卡交易中的欺诈行为。在这种背景下，LSTM有一个目标:预测不符合预期模式的事件。为了展示LSTM和双向LSTM在真实示例中的应用，我们将解决一个回归问题，预测纽约市使用出租车的乘客数量。我们可以预测下周或下个月的乘客数量，并相应地管理出租车的可用性。</p><p id="af7b" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">机器学习中用于回归的LSTM是一个典型的<strong class="kh ir">时间序列</strong>问题。与其他机器学习问题相比，时间序列的关键区别在于数据样本是按顺序出现的。序列显式或隐式地表示一个时间维度。隐式部分是输入序列的时间步长。</p><h2 id="83d6" class="nl mp iq bd mq nm nn dn mu no np dp my ko nq nr na ks ns nt nc kw nu nv ne nw bi translated">例如:纽约出租车乘客</h2><p id="5c66" class="pw-post-body-paragraph kf kg iq kh b ki ng jr kk kl nh ju kn ko ni kq kr ks nj ku kv kw nk ky kz la ij bi translated">此示例将使用LSTM和双向LSTM来预测未来事件，并预测可能脱颖而出的事件。本例中使用的数据集可以在<a class="ae nx" href="https://www.kaggle.com/gauravduttakiit/new-york-taxi" rel="noopener ugc nofollow" target="_blank"> Kaggle </a>上找到。</p><p id="14d7" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir"> (1)探索数据集</strong></p><figure class="lz ma mb mc gt md gh gi paragraph-image"><div role="button" tabindex="0" class="me mf di mg bf mh"><div class="gh gi ny"><img src="../Images/6546a755eed47f5f4f5cc6dc581a0fba.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xmQ-WPK_kcqAPqYGwz7ZQg.png"/></div></div><p class="mk ml gj gh gi mm mn bd b be z dk translated">图2:来自纽约出租车乘客的样本</p></figure><p id="c1bb" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们使用Pandas加载数据集，得到如图2所示的数据帧。该数据集有10320个条目，代表2014年7月至2015年1月的乘客需求。图3显示了2014年后六个月的需求值。</p><figure class="lz ma mb mc gt md gh gi paragraph-image"><div role="button" tabindex="0" class="me mf di mg bf mh"><div class="gh gi nz"><img src="../Images/92b377dd962ac43cdc84f3c0956408de.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oTON83BDbD0B4B1I3ybZYQ.png"/></div></div><p class="mk ml gj gh gi mm mn bd b be z dk translated">图3:数据集中出租车需求的实际模式</p></figure><p id="69cd" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如图3所示，数据集有两个异常值，它们与常规模式不同。那些高于正常水平的峰值或需求的减少，暗示我们要深入审视时代的背景。例如，图4中节假日(12月24日、25日)的需求快照包含了与其他日期不可比的独特数据点。</p><figure class="lz ma mb mc gt md gh gi paragraph-image"><div role="button" tabindex="0" class="me mf di mg bf mh"><div class="gh gi oa"><img src="../Images/a80b05f99e1ca7de02a886b20a2f732d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yjFEz7yTLv4sM2Y7ofp7sw.png"/></div></div><p class="mk ml gj gh gi mm mn bd b be z dk translated">图4:一些选定假日的需求数据点</p></figure><p id="3fbf" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir"> (2)数据序列和特征工程</strong></p><p id="193b" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">查看数据集，我们可以很快注意到一些明显的模式。例如，有每日模式(工作日与周末)，每周模式(一周的开始与结束)，以及一些其他因素，如公共假日与工作日。我们考虑构建以下有助于我们制作模型的附加功能:</p><ul class=""><li id="2b77" class="lk ll iq kh b ki kj kl km ko lm ks ln kw lo la lp lq lr ls bi translated">除了一个月中的某一天之外，还添加一周中的某一天</li><li id="94bb" class="lk ll iq kh b ki lt kl lu ko lv ks lw kw lx la lp lq lr ls bi translated">一周中同一天每小时的平均乘坐次数。</li><li id="9fa5" class="lk ll iq kh b ki lt kl lu ko lv ks lw kw lx la lp lq lr ls bi translated">白天和晚上的乘坐次数。</li></ul><pre class="lz ma mb mc gt ob oc od oe aw of bi"><span id="ecef" class="nl mp iq oc b gy og oh l oi oj"># (1) Adding day of the week.<br/>df['Weekday'] = df.timestamp.dt.weekday</span><span id="c1d5" class="nl mp iq oc b gy ok oh l oi oj"># (2) Adding the average of rides grouped by the weekday and hour<br/># 7 Days, 24 Hours = 168 values<br/>df.avg_hour_day = df.avg_hour_day.replace(df[:7344].groupby(df.Weekday.astype(str) + ' ' + df.Hour.astype(str))['value'].mean().to_dict())</span><span id="4abf" class="nl mp iq oc b gy ok oh l oi oj"># (3) Featuring the number of rides during the day and during the night.<br/># We define the day time to be any hours between 6 AM and 10 PM while Night time where usually there is less <br/># demand is any time between 10:00 PM and 6:00 AM<br/>df['day_time'] = ((df['Hour'] &gt;= 6) &amp; (df['Hour'] &lt;= 22)).astype(int)</span></pre><p id="12b5" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">添加这些特性后，数据集的另一个外观如图5所示。</p><figure class="lz ma mb mc gt md gh gi paragraph-image"><div role="button" tabindex="0" class="me mf di mg bf mh"><div class="gh gi ol"><img src="../Images/a53b510eed89fef6b2579330a6a197c2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*islLpeZmaI2r-IHxm1i9rQ.png"/></div></div><p class="mk ml gj gh gi mm mn bd b be z dk translated">图5:来自纽约出租车乘客的样本，带有额外的构造特征。</p></figure><p id="647c" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在，我们可以看到白天和晚上的需求模式。</p><figure class="lz ma mb mc gt md gh gi paragraph-image"><div role="button" tabindex="0" class="me mf di mg bf mh"><div class="gh gi om"><img src="../Images/c03ddb2c38ff6c440ac097204cb2f601.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6VWARxfki0dLXPpwmw4AqA.png"/></div></div><p class="mk ml gj gh gi mm mn bd b be z dk translated">图6:与夜间相比，乘客在白天出行的频率。</p></figure><p id="8733" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">像大多数ML模型一样，LSTM对输入规模非常敏感。我们需要重新调整数据集。我们将使用来自Sklearn的标准定标器。</p><p id="f085" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir"> (3)了解未来</strong></p><p id="b9cd" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们将要建立的模型将需要接收一些关于过去的观察来预测未来。过去的观察将不会明确地指示时间戳，但是将接收到我们所称的数据点窗口。该窗口有48个数据点:每天24小时每小时两个记录，如图7所示。</p><pre class="lz ma mb mc gt ob oc od oe aw of bi"><span id="e598" class="nl mp iq oc b gy og oh l oi oj">X_train = create_sequence(X_train, 48)<br/>X_test  = create_sequence(X_test, 48)<br/>y_train = y_train[-X_train.shape[0]:]<br/>y_test  = y_test[-X_test.shape[0]:]</span></pre><figure class="lz ma mb mc gt md gh gi paragraph-image"><div role="button" tabindex="0" class="me mf di mg bf mh"><div class="gh gi on"><img src="../Images/76824b94d82d62f2dae7e3c3e6c853c8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PVXk6bbnd4uDcjKhfFakYw.png"/></div></div><p class="mk ml gj gh gi mm mn bd b be z dk translated">图7:训练测试时间序列数据集分割。</p></figure><p id="4569" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">每个学习示例由过去的观察窗口组成，可以具有一个或多个特征。目标变量可以是单个目标或一系列目标。在本例中，模型学习预测单步值，如图8所示。输入结构必须采用以下格式[训练示例、时间步长、特征]。我们的设计具有三个特征，具有48个时间步长的窗口，使得输入结构为[9240，48，3]。</p><figure class="lz ma mb mc gt md gh gi paragraph-image"><div role="button" tabindex="0" class="me mf di mg bf mh"><div class="gh gi oo"><img src="../Images/b3e330b45514b06faaef3b13c4263a79.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jFWlp8ctYhlvPGW95XBw_Q.png"/></div></div><p class="mk ml gj gh gi mm mn bd b be z dk translated">图8:如何构建数据序列的演示。</p></figure><p id="7ad0" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">使用Keras构建双向LSTM非常简单。Keras提供了一个双向层来包装一个递归层。在我们的代码中，我们使用两个双向层包装作为参数提供的两个LSTM层。我们将展示如何构建一个LSTM和一个双向LSTM:</p><pre class="lz ma mb mc gt ob oc od oe aw of bi"><span id="4039" class="nl mp iq oc b gy og oh l oi oj">model = Sequential()<br/>model.add(LSTM(64,return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[-1])))<br/>model.add(Dropout(0.5))<br/>model.add(LSTM(20,return_sequences=False))<br/>model.add(Dropout(0.5))<br/>model.add(Dense(1))<br/>model.compile(loss='mse', optimizer='rmsprop')</span></pre><p id="666d" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">return sequences参数设置为True以获取所有隐藏状态。对于双向LSTM，输出由前向和后向层生成。第一个双向层的输入大小为(48，3)，这意味着每个样本有48个时间步长，每个时间步长有三个特征。对应的代码如下:</p><pre class="lz ma mb mc gt ob oc od oe aw of bi"><span id="1ca6" class="nl mp iq oc b gy og oh l oi oj"># Building the model<br/>model = Sequential()<br/># Adding a Bidirectional LSTM layer<br/>model.add(Bidirectional(LSTM(64,return_sequences=True, dropout=0.5, input_shape=(X_train.shape[1], X_train.shape[-1]))))<br/>model.add(Bidirectional(LSTM(20, dropout=0.5)))<br/>model.add(Dense(1))<br/>model.compile(loss='mse', optimizer='rmsprop')</span></pre><p id="ea26" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir"> (4)结果</strong></p><p id="d83b" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">一旦我们运行了fit函数，我们就可以在测试数据集上比较模型的性能。图9展示了获得的结果。这个模型实现了一个伟大的未来预测。尽管我们构建的模型被简化为专注于构建对LSTM和双向LSTM的理解，但它可以准确预测未来趋势。</p><figure class="lz ma mb mc gt md gh gi paragraph-image"><div role="button" tabindex="0" class="me mf di mg bf mh"><div class="gh gi op"><img src="../Images/1735308678457c34002a344d7f86fab0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kLaibUxMUkSu9Do8vF8ecA.png"/></div></div><p class="mk ml gj gh gi mm mn bd b be z dk translated">图9:比较预测目标值和实际目标值的线图。</p></figure><h1 id="c4c2" class="mo mp iq bd mq mr ms mt mu mv mw mx my jw mz jx na jz nb ka nc kc nd kd ne nf bi translated">摘要</h1><p id="4066" class="pw-post-body-paragraph kf kg iq kh b ki ng jr kk kl nh ju kn ko ni kq kr ks nj ku kv kw nk ky kz la ij bi translated">在本文中，我们了解了什么是LSTM网络以及如何构建双向网络。请注意，我们提到LSTM是RNN的延伸，但请记住，这不是唯一的延伸。例如，注意力模型，序列到序列RNN是其他扩展的例子。</p><p id="9b4f" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">LSTM有助于模式识别，尤其是在输入顺序是主要因素的情况下。我们已经在提供的示例中看到了如何使用Keras [2]构建LSTM来解决回归问题。本文只演示了部分代码。你可以在<a class="ae nx" href="https://github.com/malhamid/LSTM" rel="noopener ugc nofollow" target="_blank"> my Github </a>上找到完整的预处理步骤的完整代码示例。谢谢大家！</p><h1 id="eb73" class="mo mp iq bd mq mr ms mt mu mv mw mx my jw mz jx na jz nb ka nc kc nd kd ne nf bi translated">参考</h1><p id="d111" class="pw-post-body-paragraph kf kg iq kh b ki ng jr kk kl nh ju kn ko ni kq kr ks nj ku kv kw nk ky kz la ij bi translated">[1]塞普·霍赫雷特，于尔根·施密德胡贝尔；长短期记忆。<em class="oq">神经计算</em> 1997年；9 (8): 1735–1780.https://doi.org/10.1162/neco.1997.9.8.1735</p><p id="cc6b" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">[2]https://keras.io/api/layers/recurrent_layers/lstm/<a class="ae nx" href="https://keras.io/api/layers/recurrent_layers/lstm/" rel="noopener ugc nofollow" target="_blank">上提供的Keras LSTM图层</a></p></div></div>    
</body>
</html>