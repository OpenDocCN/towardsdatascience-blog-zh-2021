<html>
<head>
<title>A Handbook for Logistic Regression</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">逻辑回归手册</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/a-handbook-for-logistic-regression-bb2d0dc6d8a8?source=collection_archive---------17-----------------------#2021-10-28">https://towardsdatascience.com/a-handbook-for-logistic-regression-bb2d0dc6d8a8?source=collection_archive---------17-----------------------#2021-10-28</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="e539" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">关于逻辑回归的所有细节的备忘单</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ki"><img src="../Images/0bda1dd7bac51f55add8bf7d7c2c0502.png" data-original-src="https://miro.medium.com/v2/resize:fit:1252/format:webp/0*-YB_lKFLG5Sp3_kN.jpg"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">由<a class="ae ku" href="https://www.freepik.com/stories" rel="noopener ugc nofollow" target="_blank">故事</a>在<a class="ae ku" href="https://www.freepik.com/free-vector/instruction-manual-concept-illustration_10840255.htm" rel="noopener ugc nofollow" target="_blank"> Freepik </a>上的图片</p></figure><p id="2375" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated"><a class="ae ku" href="https://en.wikipedia.org/wiki/Logistic_regression" rel="noopener ugc nofollow" target="_blank">逻辑回归</a>是一种线性分类算法。<a class="ae ku" href="https://en.wikipedia.org/wiki/Statistical_classification" rel="noopener ugc nofollow" target="_blank">分类</a>是一个问题，其中的任务是将一个类别/类分配给一个新的实例，从现有的标记数据(称为训练集)中学习每个类的属性。分类问题的例子可以是将电子邮件分类为垃圾邮件和非垃圾邮件，查看身高、体重和其他属性来将一个人分类为健康或不健康，等等。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi lr"><img src="../Images/2f53803698d09dd6fa99c80be1f3e246.png" data-original-src="https://miro.medium.com/v2/resize:fit:900/format:webp/1*pbedATu7BWQUnAXJCi9RLw.png"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">使用邮件主题和内容将电子邮件分类为垃圾邮件/非垃圾邮件。图片来源:自创</p></figure><p id="5e81" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">线性模型只能捕捉形式为<strong class="kx iu"> <em class="ls"> Y = mx + b </em> </strong>的直线关系，其中 m 是斜率，b 是截距。</p><p id="34d2" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">逻辑回归方程看起来像…</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="lu lv di lw bf lx"><div class="gh gi lt"><img src="../Images/c8c59563905fc16ea42d3b8215c379b6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EKnMZC-NF9GrozrbtUo6Pg.png"/></div></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">逻辑回归方程</p></figure><p id="2ab4" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">…其中β是需要学习的系数，x 是特征/解释变量。y 是响应变量，我们将更详细地研究它。</p><p id="7fb9" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">你也可以在<a class="ae ku" href="https://www.youtube.com/channel/UCg0PxC9ThQrbD9nM_FU1vWA" rel="noopener ugc nofollow" target="_blank"> DataTrek 频道</a>浏览这个话题的视频内容。DataTrek Youtube 频道上的逻辑回归系列的完整播放列表可在<a class="ae ku" href="https://www.youtube.com/watch?v=0zlDF9A4UiY&amp;list=PL89V0TQq5GLrK1_bXqci8lkaikCliD4I6" rel="noopener ugc nofollow" target="_blank">此处</a>获得。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ly lz l"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated"><a class="ae ku" href="https://www.youtube.com/channel/UCg0PxC9ThQrbD9nM_FU1vWA" rel="noopener ugc nofollow" target="_blank"> DataTrek </a>:逻辑回归简介</p></figure><h1 id="e5bd" class="ma mb it bd mc md me mf mg mh mi mj mk jz ml ka mm kc mn kd mo kf mp kg mq mr bi translated">为什么要学习逻辑回归？</h1><p id="bfa0" class="pw-post-body-paragraph kv kw it kx b ky ms ju la lb mt jx ld le mu lg lh li mv lk ll lm mw lo lp lq im bi translated">我想到的一个问题是，当强大的非线性算法如<a class="ae ku" href="https://en.wikipedia.org/wiki/Support-vector_machine" rel="noopener ugc nofollow" target="_blank">支持向量机</a>、<a class="ae ku" href="https://en.wikipedia.org/wiki/Deep_learning" rel="noopener ugc nofollow" target="_blank">深度学习</a>、<a class="ae ku" href="https://en.wikipedia.org/wiki/Decision_tree" rel="noopener ugc nofollow" target="_blank">基于树的</a>等可用时，为什么还要学习逻辑回归？这样做的理由如下。</p><ol class=""><li id="d2ac" class="mx my it kx b ky kz lb lc le mz li na lm nb lq nc nd ne nf bi translated">这是数据科学从业者想到的第一个监督学习算法，用于创建一个强大的基线模型来检查提升。</li><li id="9e12" class="mx my it kx b ky ng lb nh le ni li nj lm nk lq nc nd ne nf bi translated">这是一个基本的，强大的，易于实现的算法。这也是一个非常直观和可解释的模型，因为最终输出是描述响应变量和特征之间关系的系数。</li><li id="8253" class="mx my it kx b ky ng lb nh le ni li nj lm nk lq nc nd ne nf bi translated">更重要的是，它引入了理论概念，如<a class="ae ku" href="https://en.wikipedia.org/wiki/Maximum_likelihood_estimation" rel="noopener ugc nofollow" target="_blank">最大似然估计</a>、<a class="ae ku" href="https://en.wikipedia.org/wiki/Cross_entropy" rel="noopener ugc nofollow" target="_blank">交叉熵</a>、使用<a class="ae ku" href="https://en.wikipedia.org/wiki/Sigmoid_function" rel="noopener ugc nofollow" target="_blank"> sigmoid </a>和<a class="ae ku" href="https://en.wikipedia.org/wiki/Softmax_function" rel="noopener ugc nofollow" target="_blank"> softMax 函数</a>，这些都是理解深度学习或其他复杂算法所不可或缺的。</li></ol><h1 id="1d72" class="ma mb it bd mc md me mf mg mh mi mj mk jz ml ka mm kc mn kd mo kf mp kg mq mr bi translated">逻辑回归的正确响应变量“Y”应该是什么？</h1><p id="51bb" class="pw-post-body-paragraph kv kw it kx b ky ms ju la lb mt jx ld le mu lg lh li mv lk ll lm mw lo lp lq im bi translated">线性模型的一个需求就是要有‘Y’，要连续无界的变量。因为，我们正在解决一个分类问题，我们希望为一个实例分配类，这个实例是一个离散值，既不是连续的，也不是无界的，这是一个问题。</p><p id="afae" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">备选方案可以是改为预测属于正类的实例的概率‘p’。实例属于负类的概率将自动变为(1-p)。概率是一个连续值，但仍在[0，1]之间有界，因此对无界变量的需求仍未得到满足。</p><h2 id="e54d" class="nl mb it bd mc nm nn dn mg no np dp mk le nq nr mm li ns nt mo lm nu nv mq nw bi translated">营救几率日志</h2><p id="adcc" class="pw-post-body-paragraph kv kw it kx b ky ms ju la lb mt jx ld le mu lg lh li mv lk ll lm mw lo lp lq im bi translated">概率被定义为事件发生的概率与事件不发生的概率之比。赔率的对数取赔率的对数，从而得到在-无穷大到+无穷大范围内的连续且无界的值。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/55dc470f116d87b30f868649b8e5e769.png" data-original-src="https://miro.medium.com/v2/resize:fit:404/format:webp/1*RCl6BatCH00yn-_kdJalsA.png"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">比值对数公式</p></figure><h2 id="dce8" class="nl mb it bd mc nm nn dn mg no np dp mk le nq nr mm li ns nt mo lm nu nv mq nw bi translated">找回概率</h2><p id="1390" class="pw-post-body-paragraph kv kw it kx b ky ms ju la lb mt jx ld le mu lg lh li mv lk ll lm mw lo lp lq im bi translated">在分类任务中，最终目标是将类标签分配给实例。我们需要一种方法来从响应变量中获得概率，在逻辑回归的情况下，响应变量是比值对数。我们通过一个<a class="ae ku" href="https://en.wikipedia.org/wiki/Sigmoid_function" rel="noopener ugc nofollow" target="_blank"> sigmoid </a> /logistic 函数来传递概率对数，从而得到概率。逻辑回归因使用了逻辑/Sigmoid 函数而得名逻辑。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="lu lv di lw bf lx"><div class="gh gi ny"><img src="../Images/c39043155c1e6d5e482917c273c61013.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fvIn12fk1TOl4JSVtl5TUQ.png"/></div></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">Sigmoid 函数来恢复概率。图像来源:自创</p></figure><h2 id="3f38" class="nl mb it bd mc nm nn dn mg no np dp mk le nq nr mm li ns nt mo lm nu nv mq nw bi translated">Sigmoid 是非线性函数，为什么 Logistic 回归还是线性模型？</h2><p id="c60f" class="pw-post-body-paragraph kv kw it kx b ky ms ju la lb mt jx ld le mu lg lh li mv lk ll lm mw lo lp lq im bi translated">是的，sigmoid 是一个非线性函数，但逻辑回归仍然是一个线性模型，因为估计概率响应的 logit(比值比对数)是预测值的线性函数。Sigmoid 仅用于取回概率，但实际建模作为线性模型发生，以估计响应变量(概率对数)和特征之间的关系。</p><h1 id="ab37" class="ma mb it bd mc md me mf mg mh mi mj mk jz ml ka mm kc mn kd mo kf mp kg mq mr bi translated">逻辑回归的损失函数</h1><p id="0054" class="pw-post-body-paragraph kv kw it kx b ky ms ju la lb mt jx ld le mu lg lh li mv lk ll lm mw lo lp lq im bi translated">为了学习逻辑回归模型的最佳参数/系数/β集，我们需要一个可以最小化的损失函数。</p><h2 id="e7e2" class="nl mb it bd mc nm nn dn mg no np dp mk le nq nr mm li ns nt mo lm nu nv mq nw bi translated">可以用均方差吗？</h2><p id="3547" class="pw-post-body-paragraph kv kw it kx b ky ms ju la lb mt jx ld le mu lg lh li mv lk ll lm mw lo lp lq im bi translated">损失函数的一个候选可以是预测概率(来自 sigmoid 函数)和实际类别标签(已知为训练集)之间的均方误差(MSE ),但结果表明，逻辑回归的 MSE 是非凸的，因为预测概率来自应用于 logit 的非线性，损失函数结果是非凸的。因为 MSE 是非凸的，并且全局最小值不存在，我们忽略它。</p><h2 id="9d33" class="nl mb it bd mc nm nn dn mg no np dp mk le nq nr mm li ns nt mo lm nu nv mq nw bi translated">对数损失或交叉熵损失进行救援</h2><p id="89fc" class="pw-post-body-paragraph kv kw it kx b ky ms ju la lb mt jx ld le mu lg lh li mv lk ll lm mw lo lp lq im bi translated">我们发现，逻辑回归的对数损失或交叉熵损失是一个凸函数，可以用来寻找最佳的参数/系数/β集。</p><p id="f9cc" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">设ŷ为预测概率，y 为真实标签。给定 n 个数据点，对数损失/交叉熵如下所示。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="lu lv di lw bf lx"><div class="gh gi nz"><img src="../Images/6fe1e01b074a02766f54287e350705e2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tAqmIcmmxb4--XXDeZOFlg.png"/></div></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">逻辑回归的对数损失或交叉熵损失</p></figure><p id="f022" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">最小化逻辑回归的交叉熵损失类似于最大化<a class="ae ku" href="https://en.wikipedia.org/wiki/Likelihood_function" rel="noopener ugc nofollow" target="_blank">对数似然</a>。</p><h2 id="c96a" class="nl mb it bd mc nm nn dn mg no np dp mk le nq nr mm li ns nt mo lm nu nv mq nw bi translated">最小化损失函数，学习贝塔系数</h2><p id="cd5e" class="pw-post-body-paragraph kv kw it kx b ky ms ju la lb mt jx ld le mu lg lh li mv lk ll lm mw lo lp lq im bi translated">与线性回归不同，逻辑回归不存在封闭形式/解析解。但是，由于损失函数(对数损失/交叉熵损失)是可微分的，因此可以利用像<a class="ae ku" href="https://en.wikipedia.org/wiki/Gradient_descent" rel="noopener ugc nofollow" target="_blank">梯度下降</a>这样的数值解来最小化损失并学习正确的参数。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="lu lv di lw bf lx"><div class="gh gi oa"><img src="../Images/45398c03405d48dfc86ad873801df04c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6CZcw4D9cTzYQqbosRbXyQ.png"/></div></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">我们所学内容的总结。图片来源:自创</p></figure><h1 id="4c82" class="ma mb it bd mc md me mf mg mh mi mj mk jz ml ka mm kc mn kd mo kf mp kg mq mr bi translated">逻辑回归在多类分类中的扩展</h1><p id="e0d6" class="pw-post-body-paragraph kv kw it kx b ky ms ju la lb mt jx ld le mu lg lh li mv lk ll lm mw lo lp lq im bi translated">在多类分类任务中，存在多个类/类别。例如，通过观察萼片长度、萼片宽度、花瓣长度和花瓣宽度来预测花的不同品种。给定一个实例，我们需要将它分配到一个类/类别中。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="lu lv di lw bf lx"><div class="gh gi ob"><img src="../Images/88249046a829325c65f92587efd90c6b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WEixImvej9HoF7qcJ0t_1w.png"/></div></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">图片来源:维基百科</p></figure><h2 id="2899" class="nl mb it bd mc nm nn dn mg no np dp mk le nq nr mm li ns nt mo lm nu nv mq nw bi translated">处理多类分类的逻辑回归调整</h2><ul class=""><li id="4bf1" class="mx my it kx b ky ms lb mt le oc li od lm oe lq of nd ne nf bi translated">softMax 函数用于从响应变量 y 中获取概率。给定<em class="ls"> k </em>类，类<em class="ls"> i </em>的概率如下所示。这就是为什么多类分类的逻辑回归也称为 Softmax 回归，或者也称为多项式逻辑回归。</li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi og"><img src="../Images/b31f44ce28d277ec1486ce603d3a52dc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1284/format:webp/1*C5k9jhoCe7Ev3Z3jq2JajA.png"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">图片来源:维基百科</p></figure><ul class=""><li id="5e06" class="mx my it kx b ky kz lb lc le mz li na lm nb lq of nd ne nf bi translated">在多类分类的逻辑回归中，每个类都有自己的超平面、自己的一组参数/系数/β，softmax 充当归一化层。因此，在二元分类中，每个特征学习一个β，而对于多类分类，每个类的每个特征学习一个β。</li></ul><h2 id="6033" class="nl mb it bd mc nm nn dn mg no np dp mk le nq nr mm li ns nt mo lm nu nv mq nw bi translated">解决多类分类任务的另一种方法</h2><p id="c74d" class="pw-post-body-paragraph kv kw it kx b ky ms ju la lb mt jx ld le mu lg lh li mv lk ll lm mw lo lp lq im bi translated">解决多类分类任务的另一种方法是将问题转化为二元分类任务。这是通过两种方式实现的。</p><ol class=""><li id="59b4" class="mx my it kx b ky kz lb lc le mz li na lm nb lq nc nd ne nf bi translated">一对一分类</li><li id="6288" class="mx my it kx b ky ng lb nh le ni li nj lm nk lq nc nd ne nf bi translated">一对其余分类。</li></ol><p id="a098" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">你可以在这里阅读更多相关信息<a class="ae ku" href="https://en.wikipedia.org/wiki/Multiclass_classification#Transformation_to_binary" rel="noopener ugc nofollow" target="_blank">。</a></p><p id="2a37" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">作为最后一步，为了测试我们对逻辑回归的学习，我们可以从执行探索性数据分析、拟合模型和分析学习到的系数开始，解决一个多类分类问题。在<a class="ae ku" href="https://www.kaggle.com/abhishekmungoli/logistic-regression-iris-dataset/notebook" rel="noopener ugc nofollow" target="_blank">这本笔记本中，</a>我总结了<a class="ae ku" href="https://en.wikipedia.org/wiki/Iris_flower_data_set" rel="noopener ugc nofollow" target="_blank">鸢尾花数据集</a>上逻辑回归的所有重要方面，以及额外的 EDA 和可解释的 AI 部分。请随意叉笔记本，延伸分析&amp;学习。</p><div class="oh oi gp gr oj ok"><a href="https://www.kaggle.com/abhishekmungoli/logistic-regression-iris-dataset/notebook" rel="noopener  ugc nofollow" target="_blank"><div class="ol ab fo"><div class="om ab on cl cj oo"><h2 class="bd iu gy z fp op fr fs oq fu fw is bi translated">逻辑回归 Iris 数据集</h2><div class="or l"><h3 class="bd b gy z fp op fr fs oq fu fw dk translated">使用 Kaggle 笔记本探索和运行机器学习代码|使用来自[私有数据源]的数据</h3></div><div class="os l"><p class="bd b dl z fp op fr fs oq fu fw dk translated">www.kaggle.com</p></div></div><div class="ot l"><div class="ou l ov ow ox ot oy ko ok"/></div></div></a></div><h1 id="ad79" class="ma mb it bd mc md me mf mg mh mi mj mk jz ml ka mm kc mn kd mo kf mp kg mq mr bi translated">结论</h1><p id="4521" class="pw-post-body-paragraph kv kw it kx b ky ms ju la lb mt jx ld le mu lg lh li mv lk ll lm mw lo lp lq im bi translated">通过这篇博文，我们观察了逻辑回归的不同方面，如响应变量、恢复概率、损失函数以及求解。希望你喜欢它，如果你有进一步的疑问，请随时联系我们。</p><p id="888b" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">如果你有任何疑问，请联系我。我将有兴趣知道你是否有一些有趣的问题要解决，需要一些指导。</p><p id="fd43" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated"><strong class="kx iu"> <em class="ls">我的 Youtube 频道获取更多内容:</em> </strong></p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ly lz l"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated"><a class="ae ku" href="https://www.youtube.com/channel/UCg0PxC9ThQrbD9nM_FU1vWA" rel="noopener ugc nofollow" target="_blank"> DataTrek Youtube 频道</a></p></figure><blockquote class="oz pa pb"><p id="5c15" class="kv kw ls kx b ky kz ju la lb lc jx ld pc lf lg lh pd lj lk ll pe ln lo lp lq im bi translated"><strong class="kx iu"> <em class="it">关于作者-: </em> </strong></p><p id="46eb" class="kv kw ls kx b ky kz ju la lb lc jx ld pc lf lg lh pd lj lk ll pe ln lo lp lq im bi translated">Abhishek Mungoli 是一位经验丰富的数据科学家，拥有 ML 领域的经验和计算机科学背景，跨越多个领域并具有解决问题的思维方式。擅长各种机器学习和零售业特有的优化问题。热衷于大规模实现机器学习模型，并通过博客、讲座、聚会和论文等方式分享知识。</p><p id="81da" class="kv kw ls kx b ky kz ju la lb lc jx ld pc lf lg lh pd lj lk ll pe ln lo lp lq im bi translated">我的动机总是把最困难的事情简化成最简单的版本。我喜欢解决问题、数据科学、产品开发和扩展解决方案。我喜欢在闲暇时间探索新的地方和健身。关注我的<a class="ae ku" href="https://medium.com/@mungoliabhishek81" rel="noopener"> <strong class="kx iu">中</strong> </a>、<strong class="kx iu"/><a class="ae ku" href="https://www.linkedin.com/in/abhishek-mungoli-39048355/" rel="noopener ugc nofollow" target="_blank"><strong class="kx iu">Linkedin</strong></a><strong class="kx iu"/>或<strong class="kx iu"/><a class="ae ku" href="https://www.instagram.com/simplyspartanx/" rel="noopener ugc nofollow" target="_blank"><strong class="kx iu">insta gram</strong></a><strong class="kx iu"/>并查看我的<a class="ae ku" href="https://medium.com/@mungoliabhishek81" rel="noopener">往期帖子</a>。我欢迎反馈和建设性的批评。我的一些博客-</p></blockquote><p id="3c9d" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">另外，查看我关于线性回归的博文<a class="ae ku" href="https://medium.com/geekculture/a-complete-guide-to-linear-regression-cfa984055671" rel="noopener">或视频版本</a><a class="ae ku" href="https://www.youtube.com/watch?v=-OVHiTZofN0&amp;list=PL89V0TQq5GLpnZlZMeUa8EAmM-v9QqXqQ" rel="noopener ugc nofollow" target="_blank">这里</a>。</p><div class="oh oi gp gr oj ok"><a href="https://medium.com/geekculture/a-complete-guide-to-linear-regression-cfa984055671" rel="noopener follow" target="_blank"><div class="ol ab fo"><div class="om ab on cl cj oo"><h2 class="bd iu gy z fp op fr fs oq fu fw is bi translated">线性回归完全指南</h2><div class="or l"><h3 class="bd b gy z fp op fr fs oq fu fw dk translated">涵盖线性回归的所有基础知识</h3></div><div class="os l"><p class="bd b dl z fp op fr fs oq fu fw dk translated">medium.com</p></div></div><div class="ot l"><div class="pf l ov ow ox ot oy ko ok"/></div></div></a></div><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ly lz l"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated"><a class="ae ku" href="https://www.youtube.com/channel/UCg0PxC9ThQrbD9nM_FU1vWA" rel="noopener ugc nofollow" target="_blank"> DataTrek </a>:线性回归系列</p></figure><ul class=""><li id="5fa3" class="mx my it kx b ky kz lb lc le mz li na lm nb lq of nd ne nf bi translated"><a class="ae ku" rel="noopener" target="_blank" href="/identify-your-datas-distribution-d76062fc0802">确定您的数据分布</a></li><li id="0d4a" class="mx my it kx b ky ng lb nh le ni li nj lm nk lq of nd ne nf bi translated"><a class="ae ku" rel="noopener" target="_blank" href="/dimensionality-reduction-pca-versus-autoencoders-338fcaf3297d">降维:PCA 与自动编码器</a></li><li id="c599" class="mx my it kx b ky ng lb nh le ni li nj lm nk lq of nd ne nf bi translated"><a class="ae ku" rel="noopener" target="_blank" href="/experience-the-power-of-the-genetic-algorithm-4030adf0383f">体验遗传算法的威力</a></li><li id="64f5" class="mx my it kx b ky ng lb nh le ni li nj lm nk lq of nd ne nf bi translated"><a class="ae ku" rel="noopener" target="_blank" href="/5-mistakes-every-data-scientist-should-avoid-bcc8142d7693">每个数据科学家都应该避免的 5 个错误</a></li><li id="8a36" class="mx my it kx b ky ng lb nh le ni li nj lm nk lq of nd ne nf bi translated"><a class="ae ku" rel="noopener" target="_blank" href="/decomposing-a-time-series-in-a-simple-and-intuitive-way-19d3213c420b?source=---------7------------------">以简单&amp;直观的方式分解时间序列</a></li><li id="bb46" class="mx my it kx b ky ng lb nh le ni li nj lm nk lq of nd ne nf bi translated"><a class="ae ku" href="https://medium.com/walmartlabs/how-gpu-computing-literally-saved-me-at-work-fc1dc70f48b6" rel="noopener">GPU 计算如何在工作中拯救了我？</a></li><li id="59a8" class="mx my it kx b ky ng lb nh le ni li nj lm nk lq of nd ne nf bi translated">信息论&amp; KL 分歧<a class="ae ku" rel="noopener" target="_blank" href="/part-i-a-new-tool-to-your-toolkit-kl-divergence-5b887b5b420e">第一部分</a>和<a class="ae ku" rel="noopener" target="_blank" href="/part-2-a-new-tool-to-your-toolkit-kl-divergence-736c134baa3d">第二部分</a></li><li id="bf85" class="mx my it kx b ky ng lb nh le ni li nj lm nk lq of nd ne nf bi translated"><a class="ae ku" rel="noopener" target="_blank" href="/process-wikipedia-using-apache-spark-to-create-spicy-hot-datasets-1a59720e6e25">使用 Apache Spark 处理维基百科，创建热点数据集</a></li><li id="61cc" class="mx my it kx b ky ng lb nh le ni li nj lm nk lq of nd ne nf bi translated"><a class="ae ku" rel="noopener" target="_blank" href="/a-semi-supervised-embedding-based-fuzzy-clustering-b2023c0fde7c">一种基于半监督嵌入的模糊聚类</a></li><li id="ed76" class="mx my it kx b ky ng lb nh le ni li nj lm nk lq of nd ne nf bi translated"><a class="ae ku" rel="noopener" target="_blank" href="/compare-which-machine-learning-model-performs-better-4912b2ed597d">比较哪个机器学习模型表现更好</a></li><li id="5a1b" class="mx my it kx b ky ng lb nh le ni li nj lm nk lq of nd ne nf bi translated"><a class="ae ku" rel="noopener" target="_blank" href="/analyzing-fitbit-data-to-demystify-bodily-pattern-changes-amid-pandemic-lockdown-5b0188fec0f0">分析 Fitbit 数据，揭开疫情封锁期间身体模式变化的神秘面纱</a></li><li id="4e30" class="mx my it kx b ky ng lb nh le ni li nj lm nk lq of nd ne nf bi translated"><a class="ae ku" rel="noopener" target="_blank" href="/myths-and-reality-around-correlation-9b359456d8e1">神话与现实围绕关联</a></li><li id="622f" class="mx my it kx b ky ng lb nh le ni li nj lm nk lq of nd ne nf bi translated"><a class="ae ku" rel="noopener" target="_blank" href="/a-guide-to-becoming-business-oriented-data-scientist-51da5c829ffa">成为面向业务的数据科学家指南</a></li></ul></div></div>    
</body>
</html>