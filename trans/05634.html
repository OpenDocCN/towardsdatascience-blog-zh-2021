<html>
<head>
<title>Hyperparameter tuning: Grid search and random search</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">超参数调谐:网格搜索和随机搜索</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/hyperparameter-tuning-grid-search-and-random-search-caadb703c046?source=collection_archive---------26-----------------------#2021-05-19">https://towardsdatascience.com/hyperparameter-tuning-grid-search-and-random-search-caadb703c046?source=collection_archive---------26-----------------------#2021-05-19</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="ec81" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">两种最常见的超参数调谐技术</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ki"><img src="../Images/883efd3424263e1ab38a75c18e6346ff.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/0*IVUNQmRlTZxQO_eh.png"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">作者图片</p></figure><p id="aa51" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">超参数调整是机器学习管道中最重要的部分之一。超参数值的错误选择可能导致错误的结果和性能差的模型。</p><p id="8b16" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">有几种方法可以执行超参数调整。其中两种是网格搜索和随机搜索。</p><p id="0eef" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">让我们看看它们是如何工作的。</p></div><div class="ab cl lq lr hx ls" role="separator"><span class="lt bw bk lu lv lw"/><span class="lt bw bk lu lv lw"/><span class="lt bw bk lu lv"/></div><div class="im in io ip iq"><h1 id="8728" class="lx ly it bd lz ma mb mc md me mf mg mh jz mi ka mj kc mk kd ml kf mm kg mn mo bi translated">超参数调谐的需要</h1><p id="8ea1" class="pw-post-body-paragraph ku kv it kw b kx mp ju kz la mq jx lc ld mr lf lg lh ms lj lk ll mt ln lo lp im bi translated">超参数是模型参数，其值在训练之前设置。比如一个前馈神经网络的神经元个数就是一个超参数，因为我们在训练之前就设定好了。超参数的另一个例子是随机森林中的树木数量或<a class="ae mu" href="https://www.yourdatateacher.com/2021/03/29/an-introduction-to-linear-models/" rel="noopener ugc nofollow" target="_blank">套索回归</a>的惩罚强度。它们都是在训练阶段之前设置的数字，它们的值会影响模型的行为。</p><p id="2cc1" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">我们为什么要调整模型的超参数？因为我们事先并不知道它们的最优值。具有不同超参数的模型实际上是不同的模型，因此它可能具有较低的性能。</p><p id="6319" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">在神经网络的情况下，神经元数量少会导致欠拟合，数量多会导致过拟合。在这两种情况下，模型都不好，所以我们需要找到导致最佳性能的中间神经元数量。</p><p id="7e80" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">如果模型有几个超参数，我们需要在多维空间中寻找超参数值的最佳组合。这就是为什么超参数调整，即找到超参数的正确值的过程，是一项非常复杂和耗时的任务。</p><p id="1ce5" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">我们来看两个最重要的超参数调优算法，网格搜索和随机搜索。</p><h1 id="8b36" class="lx ly it bd lz ma mv mc md me mw mg mh jz mx ka mj kc my kd ml kf mz kg mn mo bi translated">网格搜索</h1><p id="cfc2" class="pw-post-body-paragraph ku kv it kw b kx mp ju kz la mq jx lc ld mr lf lg lh ms lj lk ll mt ln lo lp im bi translated">网格搜索是最简单的超参数调整算法。基本上，我们将超参数的域划分成一个离散的网格。然后，我们尝试这个网格值的每个组合，使用交叉验证计算一些性能指标。交叉验证中最大化平均值的网格点是超参数值的最佳组合。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ki"><img src="../Images/dda95bca164371a6c59260cdb51e4711.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/0*3T9-ATiuO6_3ulKi.png"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">作者图片</p></figure><p id="18f0" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">网格搜索是一种覆盖所有组合的穷举算法，因此它实际上可以找到域中的最佳点。最大的缺点是它非常慢。检查空间的每一个组合需要大量的时间，有时这是不可行的。别忘了网格中的每个点都需要k重交叉验证，这需要<em class="na"> k </em>个训练步骤。因此，以这种方式调整模型的超参数可能相当复杂和昂贵。然而，如果我们寻找超参数值的最佳组合，网格搜索是一个非常好的想法。</p><h1 id="253e" class="lx ly it bd lz ma mv mc md me mw mg mh jz mx ka mj kc my kd ml kf mz kg mn mo bi translated">随机搜索</h1><p id="ff6c" class="pw-post-body-paragraph ku kv it kw b kx mp ju kz la mq jx lc ld mr lf lg lh ms lj lk ll mt ln lo lp im bi translated">随机搜索类似于网格搜索，但它不是使用网格中的所有点，而是只测试这些点的随机选择的子集。这个子集越小，优化速度越快，但精度越低。这个数据集越大，优化就越精确，但越接近网格搜索。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ki"><img src="../Images/9fdad836118b80d1402242fe0c7250d5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/0*N73ebpFBRj1wb5UT.png"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">作者图片</p></figure><p id="608b" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">当您有几个带有精细网格值的超参数时，随机搜索是一个非常有用的选项。使用由5-100个随机选择的点组成的子集，我们能够得到一组相当好的超参数值。它不太可能是最好的一点，但它仍然可以是一组很好的值，为我们提供一个很好的模型。</p><h1 id="164c" class="lx ly it bd lz ma mv mc md me mw mg mh jz mx ka mj kc my kd ml kf mz kg mn mo bi translated">Python中的一个例子</h1><p id="6976" class="pw-post-body-paragraph ku kv it kw b kx mp ju kz la mq jx lc ld mr lf lg lh ms lj lk ll mt ln lo lp im bi translated">让我们看看如何使用scikit-learn在Python中实现这些算法。在本例中，我们将对仅使用n_estimators和max_features超参数的<em class="na">糖尿病</em>数据集优化随机森林回归器。你可以在我的GitHub <a class="ae mu" href="https://github.com/gianlucamalato/machinelearning/blob/master/Grid_search_and_random_search.ipynb" rel="noopener ugc nofollow" target="_blank">这里</a>找到全部代码。</p><p id="7d55" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">首先，让我们导入一些有用的库:</p><pre class="kj kk kl km gt nb nc nd ne aw nf bi"><span id="a7ac" class="ng ly it nc b gy nh ni l nj nk">from sklearn.datasets import load_diabetes <br/>from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, train_test_split <br/>from sklearn.ensemble import RandomForestRegressor <br/>import numpy as np</span></pre><p id="6a54" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">然后，让我们导入数据集，并将其分成训练集和测试集。所有的计算都将在训练集上完成。</p><pre class="kj kk kl km gt nb nc nd ne aw nf bi"><span id="c6bb" class="ng ly it nc b gy nh ni l nj nk">X,y = load_diabetes(return_X_y=True) <br/>X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)</span></pre><p id="95b6" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">现在，让我们开始网格搜索。网格搜索是使用scikit-learn的<em class="na"> GridSearchCV </em>对象完成的。当我们寻找超参数值的最佳组合时，它将我们要优化的估计量、交叉验证中的折叠数以及要考虑的评分标准作为输入。最后一个参数是我们要研究的每个超参数的值列表。GridSearchCV 将为我们创建所有的组合。</p><p id="89b8" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">假设我们希望将<em class="na"> n_estimators </em>超参数从5扩展到100，步长为5，将<em class="na"> max_features </em>超参数从0.1扩展到1.0，步长为0.05。我们正在寻找这些范围的组合，使5倍交叉验证的平均值最大化。下面是我们需要编写的代码:</p><pre class="kj kk kl km gt nb nc nd ne aw nf bi"><span id="24e6" class="ng ly it nc b gy nh ni l nj nk">grid_search = GridSearchCV(RandomForestRegressor(random_state=0), <br/>{ 'n_estimators':np.arange(5,100,5), 'max_features':np.arange(0.1,1.0,0.05), }<br/>,cv=5, scoring="r2",verbose=1,n_jobs=-1 ) </span><span id="3e3c" class="ng ly it nc b gy nl ni l nj nk">grid_search.fit(X_train,y_train)</span></pre><p id="aa0c" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">一旦我们适合网格搜索，Python将跨越我们提供的列表中的所有值的组合，并挑选得分最高的一个。这个过程可能需要一段时间，因为我们的网格很大。</p><p id="4393" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">两分钟后，我们得到:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="nn no di np bf nq"><div class="gh gi nm"><img src="../Images/8c81e335556959ab1e6d3a0e700ca193.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*jjXZA5mdvIGR0J5o.png"/></div></div></figure><p id="124f" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">正如我们从第一行看到的，网格搜索适合1710次。</p><p id="745e" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">最后，我们可以看看找到的最佳组合:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/fd3ce7addac6118754ff32ee080341f3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1214/format:webp/0*oL5dZS2ySJPIiyPp.png"/></div></figure><p id="bc65" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">最好的成绩是:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/df789d8e7e8237af85146f802e1fc0a6.png" data-original-src="https://miro.medium.com/v2/resize:fit:530/format:webp/0*uc4OUiP0aKi2yBPf.png"/></div></figure><p id="265a" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">现在，让我们看看随机搜索会发生什么。代码完全相同，但是现在我们必须定义要使用的迭代次数。我们将使用50次迭代。最后，我们将添加一个随机状态，以使结果可重复。</p><pre class="kj kk kl km gt nb nc nd ne aw nf bi"><span id="cdca" class="ng ly it nc b gy nh ni l nj nk">random_search = RandomizedSearchCV(RandomForestRegressor(random_state=0), <br/>{ 'n_estimators':np.arange(5,100,5), 'max_features':np.arange(0.1,1.0,0.05), }<br/>,cv=5, scoring="r2",verbose=1,n_jobs=-1, n_iter=50, random_state = 0 ) </span><span id="bc68" class="ng ly it nc b gy nl ni l nj nk">random_search.fit(X_train,y_train)</span></pre><p id="7c11" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">现在我们只进行250次拟合(50次迭代，每次5次拟合)。这个过程只需要20秒钟就能得出结果。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="nn no di np bf nq"><div class="gh gi nt"><img src="../Images/9c5484f3255c5c97c1da36f9d393cdfc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*ux5iE5GtaUvyZQD5.png"/></div></div></figure><p id="2468" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">以下是最佳组合:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/3349041f24892de5ff14679591875aee.png" data-original-src="https://miro.medium.com/v2/resize:fit:1214/format:webp/0*Ow7FRgNVQzLDAT5R.png"/></div></figure><p id="9b67" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">它的最好成绩是:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/1fb0de31215799f1c62f0259140b5aaa.png" data-original-src="https://miro.medium.com/v2/resize:fit:568/format:webp/0*oomX9I-rhd5gp3BA.png"/></div></figure><p id="2cf9" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">正如我们所看到的，结果与网格搜索非常相似，但随机搜索使我们节省了83%的计算时间。</p><h1 id="492e" class="lx ly it bd lz ma mv mc md me mw mg mh jz mx ka mj kc my kd ml kf mz kg mn mo bi translated">结论</h1><p id="d32d" class="pw-post-body-paragraph ku kv it kw b kx mp ju kz la mq jx lc ld mr lf lg lh ms lj lk ll mt ln lo lp im bi translated">我认为，在这两种算法中，随机搜索是非常有用的，因为它更快，而且由于它不会到达网格中的最佳点，所以它避免了过度拟合，并且更能够一般化。然而，对于小网格(即少于200点)，如果训练阶段不太慢，我建议使用网格搜索。对于一般情况，随机搜索可以提高训练速度，并为我们的模型找到一个相当好的解决方案。</p><p id="f514" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">如果你对超参数调优感兴趣，加入我关于Python 中<a class="ae mu" href="https://yourdatateacher.teachable.com/p/supervised-machine-learning-with-python" rel="noopener ugc nofollow" target="_blank">监督机器学习的在线课程。</a></p></div><div class="ab cl lq lr hx ls" role="separator"><span class="lt bw bk lu lv lw"/><span class="lt bw bk lu lv lw"/><span class="lt bw bk lu lv"/></div><div class="im in io ip iq"><p id="307b" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><em class="na">原载于2021年5月19日</em><a class="ae mu" href="https://www.yourdatateacher.com/2021/05/19/hyperparameter-tuning-grid-search-and-random-search/" rel="noopener ugc nofollow" target="_blank"><em class="na"/></a><em class="na">。</em></p></div></div>    
</body>
</html>