<html>
<head>
<title>Neural Networks for Real-Time Audio: Stateless LSTM</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">实时音频的神经网络:无状态的LSTM</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/neural-networks-for-real-time-audio-stateless-lstm-97ecd1e590b8?source=collection_archive---------12-----------------------#2021-05-05">https://towardsdatascience.com/neural-networks-for-real-time-audio-stateless-lstm-97ecd1e590b8?source=collection_archive---------12-----------------------#2021-05-05</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><figure class="ip iq gp gr ir is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi io"><img src="../Images/a78046352777622ae90285a7d7f38e3e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Tl9gWH0fxYzGtqhWBgVL-w.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">作者图片</p></figure><div class=""/><p id="f3e9" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">这是关于使用神经网络进行实时音频的五部分系列的第三部分。上一篇关于WaveNet的文章，点击 <a class="ae lb" href="https://keyth72.medium.com/neural-networks-for-real-time-audio-wavenet-2b5cdf791c4f" rel="noopener"> <em class="la">这里</em> </a> <em class="la">。</em></p><p id="4219" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">在本文中，我们将使用一个无状态的LSTM神经网络来实时模拟一个吉他放大器。</p><p id="bd10" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">代表“长短期记忆”的LSTM模型是在20世纪90年代中期开发的，是一种递归神经网络(RNN)的形式。从那以后，最初的模型被修改并应用于许多不同类型的问题，包括语音识别和文本到语音转换。</p><p id="2f90" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">与WaveNet这样的“前馈”神经网络不同，LSTM有一个循环状态，每次数据流经网络时都会更新。这样，过去的信息可以用来预测现在。换句话说，网络是有记忆的。这是有状态LSTMs的情况，但是在本文中我们将尝试一些不同的东西。</p><h1 id="c3d4" class="lc ld jf bd le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz bi translated">概观</h1><p id="c8e8" class="pw-post-body-paragraph kc kd jf ke b kf ma kh ki kj mb kl km kn mc kp kq kr md kt ku kv me kx ky kz ij bi translated">对于这个例子，我们将使用一个<em class="la">无状态</em> LSTM，这意味着网络的内存对于每一个批处理都会被重置。在这个具体的例子中，我们将使用单批音频数据。因为我们使用的是单批次，所以LSTM实质上变成了一个前馈网络，因为没有递归状态。</p><p id="36f1" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">为什么我们要对单批数据使用无状态LSTM呢？无状态LSTM是一种有效的音频方法，通过将内部状态设置为零，我们降低了网络的复杂性，提高了网络速度。然而，对于我们的吉他放大器示例，网络仍然需要知道关于过去信号的一些信息，以做出准确的预测。</p><p id="c9f9" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">我们可以通过在无状态LSTM层之前添加1-D卷积层来做到这一点。在本例中，我们使用两个1-D卷积层，接着是LSTM，再接着是密集(全连接)层。网络的输入是当前样本和指定数量的先前样本。1-D卷积层用于从音频中提取特征，并减少进入LSTM层的数据量，从而显著加快处理速度。</p><figure class="mg mh mi mj gt is gh gi paragraph-image"><div class="gh gi mf"><img src="../Images/15ae459bb4c5a0409938a5ec0278d485.png" data-original-src="https://miro.medium.com/v2/resize:fit:1076/format:webp/1*-xzLPC8_mWQj417AOMYhmA.png"/></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">图1:通过网络的音频数据流(图片由作者提供)</p></figure><p id="36c6" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><em class="la">注意:“密集”层与上一篇文章的WaveNet实现中的“线性”层执行相同的功能。这只是Keras和PyTorch之间命名约定的不同。</em></p><h1 id="4717" class="lc ld jf bd le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz bi translated">Keras培训</h1><p id="c34e" class="pw-post-body-paragraph kc kd jf ke b kf ma kh ki kj mb kl km kn mc kp kq kr md kt ku kv me kx ky kz ij bi translated">我们将使用在25%增益下从Blackstar HT40放大器记录的相同的4分钟样本，如<a class="ae lb" href="https://keyth72.medium.com/neural-networks-for-real-time-audio-wavenet-2b5cdf791c4f" rel="noopener">上一篇文章</a>中所述。</p><p id="e463" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">选择Keras/Tensorflow来实现无状态LSTM模型。Keras是谷歌开发的人工智能框架Tensorflow的高级接口。Keras培训的示例代码来自Github上的<a class="ae lb" href="https://github.com/GuitarML/SmartAmpPro" rel="noopener ugc nofollow" target="_blank"> SmartAmpPro </a>项目，包含在<a class="ae lb" href="https://github.com/GuitarML/SmartAmpPro/blob/main/resources/train.py" rel="noopener ugc nofollow" target="_blank"> train.py </a>文件中。使用顺序Keras模型相当简单，如下所示。</p><figure class="mg mh mi mj gt is"><div class="bz fp l di"><div class="mk ml l"/></div></figure><p id="dc10" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><em class="la">注意:SmartAmpPro项目是训练和实时代码的结合。对于使用相同模型的纯训练代码，请参见Github上的</em><a class="ae lb" href="https://github.com/GuitarML/GuitarLSTM" rel="noopener ugc nofollow" target="_blank"><em class="la">guitar lstm</em></a><em class="la">。</em></p><p id="7a46" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">使用<code class="fe mm mn mo mp b">model = Sequential()</code>创建一个基础模型，使用<code class="fe mm mn mo mp b">.add(layer_type(layer_params,...))</code>将每个层依次添加到模型中。在上面的代码中，每一层的参数都是由先前定义的变量设置的。在Keras中，LSTM层默认是无状态的，所以唯一需要的参数是<code class="fe mm mn mo mp b">hidden_units</code>的数量。此参数决定了LSTM的大小。<code class="fe mm mn mo mp b">input_size</code>定义了将使用多少先前的音频样本来预测当前样本。稍后在实时代码中使用的默认设置是120，这是基于对准确性和处理速度的测试而选择的。这意味着当前的音频样本和先前的119个样本被用来预测下一个样本的值应该是什么。下图显示了对于给定的信号，音频数据的范围是如何馈入网络的。</p><figure class="mg mh mi mj gt is gh gi paragraph-image"><div class="gh gi mq"><img src="../Images/29a3da790913fcabd532773fae826bd1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1060/format:webp/1*UgzYBkBom0G8ud7v94R-Vw.png"/></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">图2:音频范围如何被馈送到网络的示例(此处显示的样本之间的间隔被夸大)(图片由作者提供)</p></figure><p id="3711" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><em class="la">注意:Conv1D层使用“步幅”参数，该参数用于在卷积中跳过数据点。对于步长2，网络层将跳过每个卷积运算的每隔一个数据点。这加快了计算速度，同时保留了足够的信息来进行准确的预测。</em></p><p id="e857" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">在初始化顺序模型之后，必须处理输入音频数据。对于<code class="fe mm mn mo mp b">input_size = 120</code>的情况，音频数据被切片(使用<code class="fe mm mn mo mp b">tf.gather</code>)以获得每个现有音频样本的120个样本。每批120个音频样本是网络的新输入。输入批次的顺序被随机化以提高训练性能。这种切片操作只对输入进行。wav文件，而不是输出。wav包含44100个样本(或1秒钟的音频)，带有一个<code class="fe mm mn mo mp b">input_size=120</code>，那么切片后的结果将是一个形状数组:</p><p id="cb25" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">(44100 - input_size + 1，120)或(43981，120)</p><p id="cd74" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">样本量的减少是必要的，因为对于音频的前119个样本，我们不能查看过去的120个样本来进行预测。但是现在，我们有43981个由120个样本组成的阵列，彼此重叠，而不是43981个单一的音频样本。数据加载和处理如下所示。</p><figure class="mg mh mi mj gt is"><div class="bz fp l di"><div class="mk ml l"/></div></figure><p id="2d8d" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><em class="la">注意:自定义数据加载器可用于对每个数据输入进行120个样本切片，而不是在训练前处理整个wav文件。这在训练时节省了RAM的使用。这是在SmartAmpPro项目中的</em> <a class="ae lb" href="https://github.com/GuitarML/SmartAmpPro/blob/main/train_colab_mse.ipynb" rel="noopener ugc nofollow" target="_blank"> <em class="la"> Colab脚本</em> </a> <em class="la">中实现的。</em></p><p id="6d48" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">训练由<code class="fe mm mn mo mp b">model.fit()</code>功能启动。随机化的音频数据(<code class="fe mm mn mo mp b">X_random</code>和<code class="fe mm mn mo mp b">y_random</code>)是<em class="la">拟合</em>函数的输入，同时还有时期数、批量大小以及如何分割数据以进行验证和确认。</p><h1 id="e53a" class="lc ld jf bd le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz bi translated">培训结果</h1><p id="a02d" class="pw-post-body-paragraph kc kd jf ke b kf ma kh ki kj mb kl km kn mc kp kq kr md kt ku kv me kx ky kz ij bi translated">针对这一特定LSTM实现的培训速度非常快。1-D卷积层(使用默认的SmartAmpPro设置)将每个120个样本输入减少到4个样本，然后输入到LSTM层。以前的WaveNet在一个特定的CPU上要花24个多小时，而这个模型在同一个CPU上只需要3分钟。然而，训练不如WaveNet模型精确。可以选择更高的模型参数来提高精度，但是这在实时运行时会产生问题。</p><p id="f853" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">以下是Blackstar HT40在30个时期内的训练结果。获得了0.11的损失值。这里使用的损失函数是amp仿真论文中定义的MSE(均方误差)的变体。这里显示了8毫秒数据的对比图:</p><figure class="mg mh mi mj gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi mr"><img src="../Images/f3f9c315ffb1af6fc26afd891485af2b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TdkV8-j0-wXlRECtKqB8nw.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">图3:使用无状态LSTM模型的HT40放大器的预测信号与实际信号</p></figure><p id="41c6" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">信号的幅度不像WaveNet模型那样紧密匹配，但是仍然可以学习主要特征。预测音频与实际音频的差异比较如下:</p><p id="8c0f" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">实际Blackstar HT40 amp(过驱通道，25%增益):</p><figure class="mg mh mi mj gt is"><div class="bz fp l di"><div class="ms ml l"/></div></figure><p id="e840" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">由Keras模型预测:</p><figure class="mg mh mi mj gt is"><div class="bz fp l di"><div class="ms ml l"/></div></figure><h1 id="8dc8" class="lc ld jf bd le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz bi translated">模型转换</h1><p id="d9bc" class="pw-post-body-paragraph kc kd jf ke b kf ma kh ki kj mb kl km kn mc kp kq kr md kt ku kv me kx ky kz ij bi translated">在实时使用经过训练的模型之前，必须将模型转换成合适的格式，以便加载到插件中。这里选择的格式是“json ”,这是为了可读性和计算世界的普遍接受。Keras对模型状态数据使用“. h5”格式，这是<a class="ae lb" href="https://www.hdfgroup.org/solutions/hdf5/" rel="noopener ugc nofollow" target="_blank"> HDF5 </a>压缩数据格式。SmartAmpPro " <a class="ae lb" href="https://github.com/GuitarML/SmartAmpPro/blob/main/resources/train.py" rel="noopener ugc nofollow" target="_blank"> train.py </a>"脚本中的Python代码用于执行这种转换。在转换之前，附加的“输入大小”和“步幅”参数被添加到. h5模型文件中。实时代码中需要这些参数。</p><h1 id="328a" class="lc ld jf bd le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz bi translated">实时实现</h1><p id="1499" class="pw-post-body-paragraph kc kd jf ke b kf ma kh ki kj mb kl km kn mc kp kq kr md kt ku kv me kx ky kz ij bi translated">实时实现的示例代码也来自Github上的<a class="ae lb" href="https://github.com/GuitarML/SmartAmpPro" rel="noopener ugc nofollow" target="_blank"> <strong class="ke jg"> SmartAmpPro </strong> </a>。代码使用<a class="ae lb" href="https://github.com/dpilger26/NumCpp" rel="noopener ugc nofollow" target="_blank">N<strong class="ke jg">um CPP</strong></a><strong class="ke jg"/>进行矩阵计算，使用<a class="ae lb" href="https://github.com/nlohmann/json" rel="noopener ugc nofollow" target="_blank"> <strong class="ke jg"> json </strong> </a>加载转换后的json模型。NumCpp是Numpy Python库的一个只有头文件的c++实现。</p><p id="6550" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">实时音频插件使用了<a class="ae lb" href="https://juce.com/" rel="noopener ugc nofollow" target="_blank"> <strong class="ke jg"> JUCE </strong> </a>框架，这是一个用于创建音频应用的跨平台c++框架。这里的基本目标是用高性能c++代码重新创建通过Keras顺序模型的正向传递。为了将代码转换成c++，我写了一个中间的<a class="ae lb" href="https://github.com/GuitarML/SmartAmpPro/blob/main/test/keras_np_compare_final.py" rel="noopener ugc nofollow" target="_blank"> Python脚本</a>来确保我理解底层的计算。Pytorch和Tensorflow处理图层的方法略有不同，因此实时应用程序以与训练代码完全相同的方式处理图层至关重要。</p><p id="17b9" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">在“<a class="ae lb" href="https://github.com/GuitarML/SmartAmpPro/blob/main/Source/ModelLoader.h" rel="noopener ugc nofollow" target="_blank"> ModelLoader </a>”类中加载并设置来自转换后的json模型的模型数据(状态参数)。一个经过训练的json模型的例子可以在<a class="ae lb" href="https://github.com/GuitarML/SmartAmpPro/blob/main/models/TubeClean.json" rel="noopener ugc nofollow" target="_blank"> Github </a>上查看。然后，来自ModelLoader的数据用于实例化“lstm”类，该类也包含1-D卷积层和密集层。</p><p id="c344" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">下面是<em class="la"> lstm </em>类的主要处理方法。如果您熟悉JUCE，这就是您在PluginProcessor的processBlock()方法中调用的内容:</p><figure class="mg mh mi mj gt is"><div class="bz fp l di"><div class="mk ml l"/></div></figure><p id="ff37" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">我们必须从训练代码中执行相同的音频切片，这需要对音频缓冲区进行一些仔细的处理，因为需要来自先前缓冲区(或块)的信息来预测当前缓冲区中的样本。用<code class="fe mm mn mo mp b">check_buffer(numSamples);</code>检查缓冲区大小(可能有更好的方法来处理这个问题，但是如果用户改变了缓冲区大小，<em class="la"> lstm </em>类需要知道)。然后调用<code class="fe mm mn mo mp b">set_data</code>方法来安排音频输入到LSTM推理代码。让我们看看这是怎么回事:</p><figure class="mg mh mi mj gt is"><div class="bz fp l di"><div class="mk ml l"/></div></figure><p id="8062" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">在上面的代码中，前一个缓冲区的结束被设置在临时缓冲区的开始，<code class="fe mm mn mo mp b">new_buffer</code>。然后当前缓冲区的数据被分配到<code class="fe mm mn mo mp b">new_buffer</code>的末尾。执行切片操作，获取每个音频样本并将先前的样本<code class="fe mm mn mo mp b">input_size</code>附加到二维数组<code class="fe mm mn mo mp b">data</code>。<code class="fe mm mn mo mp b">old_buffer</code>被设置为等于将用于下一个音频块的当前缓冲器。现在，来自<code class="fe mm mn mo mp b">data</code>的每个输入数组都可以输入到第一个一维卷积层。</p><p id="4c02" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">一维卷积是最复杂的层。可能有比这里写的更简单和更有效的方法来进行计算。NumCpp库用于所有矩阵计算，这里使用的主要数据类型是<code class="fe mm mn mo mp b">nc::NdArray&lt;float&gt;</code>。</p><p id="a017" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">这是零填充函数，它向输入数据添加零，以使控件成为输出形状:</p><figure class="mg mh mi mj gt is"><div class="bz fp l di"><div class="mk ml l"/></div></figure><p id="76a6" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">这是展开功能，它以与之前音频处理相同的方式分割数据。这些切片数组用于执行卷积计算。</p><figure class="mg mh mi mj gt is"><div class="bz fp l di"><div class="mk ml l"/></div></figure><p id="8ab3" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">前两个方法是从main <code class="fe mm mn mo mp b">conv1d_layer</code>方法调用的，该方法获取展开的矩阵，并对矩阵执行tensordot(或einsum)运算以完成卷积。</p><figure class="mg mh mi mj gt is"><div class="bz fp l di"><div class="mk ml l"/></div></figure><p id="8210" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><em class="la">注意:第二个Conv1d层与第一个基本相同，但它处理输入的方式不同。为了简洁起见，这里省略了它，但是可以在Github上查看完整的内容。</em></p><p id="fa68" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">处理完前两个Conv1d层后，再处理LSTM层。这是吉他仿真论文中算法的简化版本。它被简化了，因为对于一个<em class="la">无状态的</em> LSTM来说，初始单元状态和隐藏状态可以被设置为零。</p><p id="ae02" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">首先，在Conv1d层的输出和的训练LSTM权重之间执行点积。json文件。这里也加入了偏差。</p><pre class="mg mh mi mj gt mt mp mu mv aw mw bi"><span id="160c" class="mx ld jf mp b gy my mz l na nb">gates = nc::dot(conv1d_1_out, W) + bias;</span></pre><p id="4663" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">当前隐藏状态是在<em class="la">内为</em>循环计算的，其中循环次数由LSTM的hidden_size <code class="fe mm mn mo mp b">HS</code>决定。在<a class="ae lb" href="https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html" rel="noopener ugc nofollow" target="_blank"> PyTorch文档</a>中可以看到<em class="la">有状态</em> LSTM的完整方程组。对于<em class="la">无状态</em>实现，先前的隐藏状态<em class="la"> h(t-1) </em>可以被设置为零，并且不需要计算单元状态，因为它不用于下一个LSTM状态。这里显示了无状态的LSTM实现:</p><figure class="mg mh mi mj gt is"><div class="bz fp l di"><div class="mk ml l"/></div></figure><p id="d04f" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">最后，LSTM层的输出被馈送到密集层，它只是输入和来自的训练好的权重的点积。json文件加上偏差向量。该层的输出是预测音频的单个样本。下一秒声音重复44100次。咻！</p><figure class="mg mh mi mj gt is"><div class="bz fp l di"><div class="mk ml l"/></div></figure><h1 id="55f2" class="lc ld jf bd le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz bi translated">实时性能</h1><p id="b94d" class="pw-post-body-paragraph kc kd jf ke b kf ma kh ki kj mb kl km kn mc kp kq kr md kt ku kv me kx ky kz ij bi translated">一般来说，上述算法比以前的WaveNet实现运行得更快。但是，如果您将层参数增加到高于SmartAmpPro默认值，它会很快变得太慢而无法实时运行，尤其是对于Conv1d层。大于120的input_size也会降低处理速度。</p><p id="edd7" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">因为该模型只考虑了之前的119个样本(大约27毫秒)，所以预测的信号有时会在错误的方向上剧烈摆动，但通常会很快自我校正。当与WaveNet输出图比较时，信号可能更加<em class="la">不稳定</em>，这会对声音产生嗡嗡声/噪音影响。在我看来，WaveNet的声音更流畅。声音的平滑度可以通过更高数量的时期来提高。需要注意的是，声音可能会非常干燥，所以应该使用混响等附加效果来获得更自然的声音。</p><p id="24c8" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">训练速度是无状态<em class="la"> </em> LSTM的最大优势，对于TS-9电子管震音踏板这样“容易”训练的声音，在CPU上不到3分钟就能获得很高的精度。这里是一个使用SmartAmpPro实时插件(加上额外的混响)的双声道录音。该演示展示了如何将相同的基础模型用于两种不同的吉他声音(干净的和过驱动的)。</p><figure class="mg mh mi mj gt is"><div class="bz fp l di"><div class="nc ml l"/></div></figure><p id="c408" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">在下一篇文章中，我们将使用一个<em class="la">有状态</em> LSTM进行研究，看看我们能否提高训练的准确性和实时性能。</p><div class="ip iq gp gr ir nd"><a rel="noopener follow" target="_blank" href="/neural-networks-for-real-time-audio-stateful-lstm-b534babeae5d"><div class="ne ab fo"><div class="nf ab ng cl cj nh"><h2 class="bd jg gy z fp ni fr fs nj fu fw je bi translated">用于实时音频的神经网络:状态LSTM</h2><div class="nk l"><p class="bd b dl z fp ni fr fs nj fu fw dk translated">towardsdatascience.com</p></div></div><div class="nl l"><div class="nm l nn no np nl nq ix nd"/></div></div></a></div><p id="ee7c" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">感谢您的阅读！</p><ol class=""><li id="73ca" class="nr ns jf ke b kf kg kj kk kn nt kr nu kv nv kz nw nx ny nz bi translated">Alec Wright等人，“深度学习的实时吉他放大器仿真”<em class="la">应用科学</em> 10期，第3期(2020): 766。</li></ol></div></div>    
</body>
</html>