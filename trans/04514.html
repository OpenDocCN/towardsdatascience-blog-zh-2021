<html>
<head>
<title>Multiclass logistic regression from scratch</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">从零开始的多类逻辑回归</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/multiclass-logistic-regression-from-scratch-9cc0007da372?source=collection_archive---------6-----------------------#2021-04-18">https://towardsdatascience.com/multiclass-logistic-regression-from-scratch-9cc0007da372?source=collection_archive---------6-----------------------#2021-04-18</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="2fe9" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated"><em class="ki">数学和梯度</em>下降<em class="ki">用Python实现</em></h2></div><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi kj"><img src="../Images/34d493fb9aa13e92378147164bb321db.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*geF_kuekdlrdR5K13QH3GA.jpeg"/></div></div><p class="kv kw gj gh gi kx ky bd b be z dk translated">照片由<a class="ae kz" href="https://unsplash.com/@amyshamblen?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Amy Shamblen </a>在<a class="ae kz" href="https://unsplash.com/?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><figure class="kk kl km kn gt ko"><div class="bz fp l di"><div class="la lb l"/></div></figure><p id="efdd" class="pw-post-body-paragraph lc ld it le b lf lg ju lh li lj jx lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated">多类逻辑回归也称为多项逻辑回归和softmax回归。当我们想要预测两个以上的类时，就使用它。很多人一直在使用多类逻辑回归，但并不真正知道它是如何工作的。所以，我将向你们展示数学是如何工作的，并使用Python中的梯度下降从头开始实现它。</p><p id="aa9e" class="pw-post-body-paragraph lc ld it le b lf lg ju lh li lj jx lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated">免责声明:关于这个主题有各种各样的符号。我使用了我认为容易理解和形象化的符号。你可以在其他地方找到其他的符号，比如矩阵和向量被转置。</p><h1 id="4d3b" class="ly lz it bd ma mb mc md me mf mg mh mi jz mj ka mk kc ml kd mm kf mn kg mo mp bi translated">问题陈述</h1><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi mq"><img src="../Images/2a0673e193ac36dba348a3e8fcbeb4ef.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*k9g5MIgEQiqTOOy-cZh1SQ.png"/></div></div><p class="kv kw gj gh gi kx ky bd b be z dk translated">作者图片</p></figure><p id="1f49" class="pw-post-body-paragraph lc ld it le b lf lg ju lh li lj jx lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated">假设我们有N个人/观测值，每个人有M个特征，他们属于C类。我们被给予:</p><ul class=""><li id="d463" class="mr ms it le b lf lg li lj ll mt lp mu lt mv lx mw mx my mz bi translated">矩阵𝑋是ℝ𝑁×𝑀.𝑋𝑖𝑗代表具有特征j的人I</li><li id="0ad0" class="mr ms it le b lf na li nb ll nc lp nd lt ne lx mw mx my mz bi translated">𝑌是ℝ𝑁.的一个载体属于k类的𝑌𝑖represents人I</li></ul><p id="2225" class="pw-post-body-paragraph lc ld it le b lf lg ju lh li lj jx lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated">我们不知道:</p><ul class=""><li id="735d" class="mr ms it le b lf lg li lj ll mt lp mu lt mv lx mw mx my mz bi translated">权重矩阵𝑊是ℝ𝑀×𝐶.𝑊𝑗𝑘表示特征j和类别k的权重</li></ul><p id="63b1" class="pw-post-body-paragraph lc ld it le b lf lg ju lh li lj jx lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated">我们想找出𝑊，并使用𝑊来预测任何给定的观察值x的类成员</p><h1 id="222d" class="ly lz it bd ma mb mc md me mf mg mh mi jz mj ka mk kc ml kd mm kf mn kg mo mp bi translated">多类逻辑回归工作流</h1><p id="3170" class="pw-post-body-paragraph lc ld it le b lf nf ju lh li ng jx lk ll nh ln lo lp ni lr ls lt nj lv lw lx im bi translated">如果我们知道𝑋和𝑊(比如说我们给𝑊的初始值都是0)，图1显示了多类逻辑回归正向路径的工作流程。</p><ul class=""><li id="6ee0" class="mr ms it le b lf lg li lj ll mt lp mu lt mv lx mw mx my mz bi translated">首先，我们计算𝑋和𝑊的乘积，这里我们让𝑍=−𝑋𝑊.</li></ul><p id="3ad5" class="pw-post-body-paragraph lc ld it le b lf lg ju lh li lj jx lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated">有时人们在这里不包括负号。这里有没有负号并不重要。</p><p id="272b" class="pw-post-body-paragraph lc ld it le b lf lg ju lh li lj jx lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated">有时我们还会添加一个偏差项。为简单起见，我们只看本文中的权重。</p><ul class=""><li id="26b6" class="mr ms it le b lf lg li lj ll mt lp mu lt mv lx mw mx my mz bi translated">第二，我们取每行𝑍𝑖: 𝑃𝑖=softmax(𝑍𝑖)=𝑒𝑥𝑝(𝑍𝑖)/∑𝑒𝑥𝑝(𝑍𝑖𝑘).的softmax𝑍𝑖的每一行应该是𝑋(i.e.、𝑋𝑖的每一行和𝑊.的整个矩阵的乘积现在𝑃的每一行加起来应该是1。</li><li id="8eab" class="mr ms it le b lf na li nb ll nc lp nd lt ne lx mw mx my mz bi translated">第三，我们获取每行的argmax，并找到概率最高的类。</li></ul><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi nk"><img src="../Images/3a833788636506a674c3b64f5ff1aff4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AzAoL6szhZmdhAzKtt0djw.png"/></div></div><p class="kv kw gj gh gi kx ky bd b be z dk translated"><em class="ki">图一。多类逻辑回归正向路径(</em>图片由作者提供)</p></figure><p id="81bd" class="pw-post-body-paragraph lc ld it le b lf lg ju lh li lj jx lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated">当我们一次只看一个观察值时，图2显示了多类逻辑回归前向路径的另一个视图:</p><ul class=""><li id="aca6" class="mr ms it le b lf lg li lj ll mt lp mu lt mv lx mw mx my mz bi translated">首先，我们计算𝑋𝑖和w的乘积，这里我们让𝑍𝑖=−𝑋𝑖𝑊.</li><li id="f46f" class="mr ms it le b lf na li nb ll nc lp nd lt ne lx mw mx my mz bi translated">其次，我们对这一行𝑍𝑖: 𝑃𝑖=softmax(𝑍𝑖)=𝑒𝑥𝑝(𝑍𝑖)/∑𝑒𝑥𝑝(𝑍𝑖𝑘).取软最大值</li><li id="8e39" class="mr ms it le b lf na li nb ll nc lp nd lt ne lx mw mx my mz bi translated">第三，我们取这一行𝑃𝑖的argmax，并找到概率最高的索引作为𝑌𝑖.</li></ul><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi nl"><img src="../Images/b03b23db02b31598227dc187353deca1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*S8xKstB62_jVJQ6NPv8OCg.png"/></div></div><p class="kv kw gj gh gi kx ky bd b be z dk translated"><em class="ki">图二。一行上的操作。(</em>图片作者)</p></figure><h1 id="c462" class="ly lz it bd ma mb mc md me mf mg mh mi jz mj ka mk kc ml kd mm kf mn kg mo mp bi translated">可能性</h1><p id="bcb8" class="pw-post-body-paragraph lc ld it le b lf nf ju lh li ng jx lk ll nh ln lo lp ni lr ls lt nj lv lw lx im bi translated">回想一下，在问题陈述中，我们说我们得到了𝑌.所以对于一个给定的观察，我们知道这个观察的类别，这就是𝑌𝑖.𝑌𝑖给定𝑋𝑖和𝑊的似然函数是观测值I和𝑘=𝑌𝑖类的概率，是𝑍𝑖,𝑘=𝑌𝑖.的softmax而𝑌给出的𝑋和𝑊的似然函数是所有观测的产物。图3帮助我们理解这个从𝑌𝑖追溯到𝑊𝑘=𝑌𝑖.的过程</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi nm"><img src="../Images/e98f46ff4e009ff5d988f6e5c62a1b27.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3DY6V0dpZplM7vwiKitkMg.png"/></div></div><p class="kv kw gj gh gi kx ky bd b be z dk translated"><em class="ki">图3。计算可能性。(</em>图片由作者提供)</p></figure><h1 id="78cf" class="ly lz it bd ma mb mc md me mf mg mh mi jz mj ka mk kc ml kd mm kf mn kg mo mp bi translated">损失函数</h1><p id="2425" class="pw-post-body-paragraph lc ld it le b lf nf ju lh li ng jx lk ll nh ln lo lp ni lr ls lt nj lv lw lx im bi translated">接下来，我们计算损失函数。我们使用负对数似然函数，并根据样本大小将其归一化。这里需要注意的一点是</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi nn"><img src="../Images/fe6d5ed12702d0792a1b3faa1d54addd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FyDxtT1B4_krE39UAwrabQ.png"/></div></div></figure><p id="661a" class="pw-post-body-paragraph lc ld it le b lf lg ju lh li lj jx lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated">𝑇r意味着主对角线上元素的总和。图3显示了这种计算。</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi no"><img src="../Images/177fc98ee3ba48d7833998c0746f3d11.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*paVC1r9PPYsnzlVdk1M_5A.png"/></div></div><p class="kv kw gj gh gi kx ky bd b be z dk translated"><em class="ki">图四。矩阵计算。(</em>图片由作者提供)</p></figure><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi np"><img src="../Images/46de95d189f42958f44303b5c7080a03.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fY3DcOiAtbR6IhHCX2zLsA.png"/></div></div></figure><p id="2a0c" class="pw-post-body-paragraph lc ld it le b lf lg ju lh li lj jx lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated">我们经常在损失函数中加入一个𝑙2正则项，并试图最小化组合函数。事实上，scikit-learn的缺省设置使用了𝑙2惩罚。𝑙1正则化也是非常常用的。这里我们使用𝑙2正则化。</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi nq"><img src="../Images/78dbe42275bc571aef1da0cd5aee56ac.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*O5vatVQOi551sXaWnh4inQ.png"/></div></div></figure><h1 id="a717" class="ly lz it bd ma mb mc md me mf mg mh mi jz mj ka mk kc ml kd mm kf mn kg mo mp bi translated">梯度下降实现</h1><p id="4528" class="pw-post-body-paragraph lc ld it le b lf nf ju lh li ng jx lk ll nh ln lo lp ni lr ls lt nj lv lw lx im bi translated">现在我们已经计算了损失函数和梯度函数。我们可以用Python实现loss和gradient函数，实现一个非常基本的梯度下降算法。</p><figure class="kk kl km kn gt ko"><div class="bz fp l di"><div class="nr lb l"/></div></figure><p id="412c" class="pw-post-body-paragraph lc ld it le b lf lg ju lh li lj jx lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated">我们在iris数据集上尝试了我们的模型。我们拟合模型，然后绘制损失与步骤的关系，我们看到损失函数随着时间的推移而下降。当我们查看我们数据的预测时，我们看到该算法正确地预测了大多数类。</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi ns"><img src="../Images/73aaa78ff03bd1ab913e5bfc696103e2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MO18mY7vZyIRchgb5muylg.png"/></div></div></figure><p id="10ac" class="pw-post-body-paragraph lc ld it le b lf lg ju lh li lj jx lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated">这是多类逻辑回归的数学和梯度下降的基本概述。希望这篇文章对你有所帮助。</p><p id="7c35" class="pw-post-body-paragraph lc ld it le b lf lg ju lh li lj jx lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated">作者索菲亚·杨2021年4月18日</p></div></div>    
</body>
</html>