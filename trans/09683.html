<html>
<head>
<title>Supercharge Image Classification with Transfer Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">基于迁移学习的增压图像分类</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/supercharge-image-classification-with-transfer-learning-4b8c52e69c0c?source=collection_archive---------17-----------------------#2021-09-09">https://towardsdatascience.com/supercharge-image-classification-with-transfer-learning-4b8c52e69c0c?source=collection_archive---------17-----------------------#2021-09-09</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="e7c1" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">使用位模型的图像分类</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/1fb157c2083e4a3e968d36256824a260.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*sNSEkOLX0uADb-9n"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">内森·杜姆劳在<a class="ae kv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="484f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">计算机视觉领域本身可能从20世纪60年代就已经存在，最初的主要目标是建立一个模拟人类视觉能力的人工系统。像CNN这样的深度学习模型在过去十年中引起了复兴，这是由于一些因素，包括更好的算法，更快的计算(GPU)，易于使用的软件和工具(TensorFlow，PyTorch)和更多的数据可用性。</p><p id="bf62" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">CNN逐渐变得庞大而复杂，但也足以成为大多数与计算机视觉(尤其是分类)相关的任务的goto解决方案。然而，并不是每个人(或每个项目)都有资源(或可行性)利用大型GPU集群来利用这些<em class="ls">最先进的</em>巨型模型。进入<strong class="ky ir"> <em class="ls">转移学习</em> </strong> <em class="ls">！</em></p><p id="7209" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在不偏离太多的情况下，迁移学习是一种利用这种先进模型(在大型数据集上预先训练)的方法，用于特定的用例，而无需担心准备大型数据集或访问地球上最新的GPU设置的麻烦。要深入了解迁移学习，请查看[ <a class="ae kv" href="#a3c1" rel="noopener ugc nofollow"> 1 </a>和[ <a class="ae kv" href="#a3c1" rel="noopener ugc nofollow"> 2 </a> ]</p><p id="ad6f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">让我们深入到一个展示图像分类或归类背景下的迁移学习的实际例子中。这里的目标是拍摄一些动物的样本图像，看看一些预先训练好的固定模型如何对这些图像进行分类。我们将挑选几个基于复杂性的预训练的最先进的模型，比较和对比它们如何解释输入图像的真实类别。</p><h1 id="bfa4" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated">方法学</h1><p id="979a" class="pw-post-body-paragraph kw kx iq ky b kz ml jr lb lc mm ju le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated">这里的关键目标是采用现成的预训练模型，并直接使用它来预测输入图像的类别。我们在这里着重于推理，以使事情简单，而不是深入研究如何训练或微调这些模型。我们解决图像分类关键目标的方法集中于获取输入图像，从TensorFlow Hub加载Python中的预训练模型，并对输入图像的前5个可能类别进行分类。这个工作流程如图1所示。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mq"><img src="../Images/12f316149d853748bc65e28254a12efd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*12JFrOMpk6O_RBozTjXQrg.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图1用预训练的CNN进行图像分类。该图描述了使用预先训练的CNN模型的给定输入图像的前5类概率。图片来源:作者</p></figure><h1 id="5bf4" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated">预先训练的模型架构</h1><p id="8eb5" class="pw-post-body-paragraph kw kx iq ky b kz ml jr lb lc mm ju le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated">在我们的实验中，我们将利用两个最先进的预训练卷积神经网络(CNN)模型，即:</p><ul class=""><li id="1726" class="mr ms iq ky b kz la lc ld lf mt lj mu ln mv lr mw mx my mz bi translated"><strong class="ky ir"> ResNet-50: </strong>这是一个残差深度卷积神经网络(CNN)，共有50层，集中在标准卷积层和池层，这是一个典型的CNN，带有用于正则化的批量归一化层。这些模型的新颖性包括剩余或跳过连接。该模型在总共具有1000个不同类别的标准ImageNet-1K数据集上进行训练。</li><li id="7baf" class="mr ms iq ky b kz na lc nb lf nc lj nd ln ne lr mw mx my mz bi translated"><strong class="ky ir">BiT multi class ResNet-152 4x:</strong>这是谷歌在计算机视觉领域的最新(SOTA)发明，名为Big Transfer，发表于2020年5月。在这里，他们建立了他们的旗舰模型架构，一个预训练的ResNet-152模型(152层)，但比原始模型宽4倍。该模型使用分组归一化层而不是批量归一化来进行正则化。该模型在总共具有21843个类的ImageNet-21K数据集[ <a class="ae kv" href="#a3c1" rel="noopener ugc nofollow"> 3 </a>上训练。</li></ul><p id="0779" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这两种模型背后的基础架构是卷积神经网络(CNN)，其工作原理是利用具有非线性激活函数的若干卷积和池层的多层分层架构。</p><h1 id="46ec" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated">卷积神经网络</h1><p id="651f" class="pw-post-body-paragraph kw kx iq ky b kz ml jr lb lc mm ju le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated">让我们看看CNN模型的基本组件。通常，卷积神经网络(更普遍地称为CNN模型)由几个层的分层结构组成，除了输入和输出层之外，还包括卷积、汇集和密集层。典型的架构如图2所示。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nf"><img src="../Images/a5e2fb93478c7969a1974c3e62d7a750.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*384k4cAw8K68Pl52g1WY1w.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图2典型卷积神经网络的架构。这通常包括卷积和池层的堆叠层次。图片来源:作者</p></figure><p id="4495" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">CNN模型利用卷积和池层来自动提取不同层次的特征，从边缘和拐角等非常普通的特征到如图3中输入图像所示的老虎的面部结构、胡须和耳朵等非常具体的特征。通常使用展平或全局池操作符来展平特征图，以获得一维特征向量。然后，该向量作为输入通过几个完全连接的密集层发送，最后使用<em class="ls"> softmax </em>输出层预测输出类。</p><p id="a7bb" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这种多阶段层次结构的关键目标是学习模式的空间层次，这些模式也是平移不变的。这可以通过CNN架构中的两个主要层来实现:卷积层和池层。</p><p id="7c3b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">卷积层:</strong>CNN的秘制酱就是它的卷积层！这些层是通过将多个滤波器或内核与输入图像的面片进行卷积而创建的，这有助于从输入图像中自动提取特定特征。使用堆叠卷积图层的分层架构有助于学习具有特定等级的空间要素，如图3所示。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mq"><img src="../Images/bb07f625d91728727c49d58d917c8196.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9nMcwqKRNKrdWB98fLxWZg.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图3从卷积层提取的层次特征图。每一层提取输入图像的相关特征。较浅的层提取更多的一般特征，而较深的层提取属于给定输入图像的特定特征。图片来源:作者</p></figure><p id="167f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">虽然图3提供了CNN的简化视图，但核心方法在某种意义上是真实的，即在初始卷积层中提取边缘和拐角等粗糙和一般特征(以给出特征图)。这些特征在更深的卷积层中的组合有助于CNN学习更复杂的视觉特征，如鬃毛、眼睛、脸颊和鼻子。最后，使用这些特征的组合来构建老虎的整体视觉表现和概念。</p><p id="ac81" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">汇集层:</strong>我们通常使用像max、min或mean这样的聚合操作，从汇集层中的卷积层向下采样特征图。通常最大池是首选，这意味着我们接受图像像素的补丁(例如2x2补丁)并将其减少到最大值(给一个像素最大值)。最大池是优选的，因为它的计算时间更短，并且能够编码特征图的增强方面(通过取图像块的最大像素值而不是平均值)。池也有助于减少过度拟合，减少计算时间，并使CNN能够学习平移不变的特征。</p><h1 id="3148" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated">ResNet架构</h1><p id="5489" class="pw-post-body-paragraph kw kx iq ky b kz ml jr lb lc mm ju le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated">我们前面提到的两个预训练模型都是ResNet CNN架构的不同变体。ResNet代表残差网络，它引入了一个新概念，即使用残差或跳过连接来建立更深的神经网络模型，而不会面临消失梯度和模型泛化能力的问题。ResNet-50的典型架构已经过简化，如图4所示。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mq"><img src="../Images/c8bf85b659b49a5980c51d852327a65b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8GpE-Kq3KDhvuKECsBIO8w.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图4 ResNet-50 CNN架构及其组件。关键组件包括具有剩余(跳过)连接的卷积和单位块。图片来源:作者</p></figure><p id="9b34" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">很明显，ResNet-50架构由几个堆叠的卷积和池层组成，后面是最终的全局平均池和一个具有1000个单元的全连接层，以进行最终的类预测。该模型还引入了批量标准化层的概念，这些层散布在各层之间，以帮助标准化。堆叠的<em class="ls"> conv </em>和标识块是在ResNet架构中引入的新概念，其利用了如图4中的详细框图所示的剩余或跳过连接。</p><p id="ba0f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">跳过连接(也称为剩余或快捷连接)的整体思想是不仅堆叠层，而且直接将原始输入连接到几个堆叠层的输出，如图5所示，其中原始输入被添加到来自<em class="ls"> conv </em>或标识块的输出。使用跳过连接的目的是通过允许梯度的替代路径流过网络，能够构建更深的网络，而不会面临梯度消失和性能饱和等问题。我们在图5中看到ResNet架构的不同变体。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mq"><img src="../Images/a4b40b7bd8176aa1190d6ee936be6ab0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IDotMNWZIgw--7qW73b0Ew.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图5各种ResNet架构。该图显示了基于模型中存在的总层数的各种ResNet模型。图片来源:作者</p></figure><p id="4248" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">对于我们的第一个预训练模型，我们将使用ResNet-50模型，该模型已经在ImageNet-1k数据集上通过多类分类任务进行了训练。我们的第二个预训练模型使用Google的预训练大转移模型进行多标签分类(BitM ),该模型具有基于ResNet 50、101和152的变体。我们使用的模型基于ResNet-152架构的变体，其宽度是ResNet-152架构的4倍。</p><h1 id="0799" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated">大转移(BiT)预训练模型</h1><p id="7079" class="pw-post-body-paragraph kw kx iq ky b kz ml jr lb lc mm ju le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated">谷歌于2020年5月训练并发布了大转移模型，作为其开创性研究论文[ <a class="ae kv" href="#a3c1" rel="noopener ugc nofollow"> 4 </a> ]的一部分。这些预先训练好的模型建立在我们在上一节中讨论的基本ResNet架构之上，其中包含一些技巧和增强功能。大转移模型的重点包括以下内容:</p><ul class=""><li id="39b2" class="mr ms iq ky b kz la lc ld lf mt lj mu ln mv lr mw mx my mz bi translated"><strong class="ky ir">上游训练:</strong>在这里，我们在大数据集(例如ImageNet-21k)上训练大模型架构(例如ResNet ),使用长预训练时间，并使用像带权重标准化的组标准化这样的概念，而不是批量标准化。一般的观察结果是，与BatchNorm相比，具有重量标准化的GroupNorm可以很好地适应更大的批量。</li><li id="fb77" class="mr ms iq ky b kz na lc nb lf nc lj nd ln ne lr mw mx my mz bi translated"><strong class="ky ir">下游微调:</strong>一旦模型经过预训练，它就可以进行微调，并“适应”任何样本数量相对较少的新数据集。Google使用一种称为BiT-HyperRule的超参数启发式方法，其中使用随机梯度下降(SGD ),初始学习率为0.003，衰减因子为10，在30%、60%和90%的训练步骤中。</li></ul><p id="df7a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在我们接下来的实验中，我们将使用BiTM-R152x4模型，这是一个预训练的大传输模型，基于Google的旗舰CNN模型架构ResNet-152，其宽度是ResNet-152的四倍，经过训练可以在ImageNet-21k数据集上执行多标签分类。</p><h1 id="2dc5" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated">履行</h1><p id="7eee" class="pw-post-body-paragraph kw kx iq ky b kz ml jr lb lc mm ju le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated">现在，让我们使用这些预训练的模型来解决我们预测输入图像的前5类的目标。</p><blockquote class="ng nh ni"><p id="2f47" class="kw kx ls ky b kz la jr lb lc ld ju le nj lg lh li nk lk ll lm nl lo lp lq lr ij bi translated"><strong class="ky ir">提示:</strong>在位于<a class="ae kv" href="https://github.com/dipanjanS/transfer-learning-in-action" rel="noopener ugc nofollow" target="_blank">https://github.com/dipanjanS/transfer-learning-in-action</a>的GitHub资源库中可以获得支持代码笔记本</p></blockquote><p id="fb42" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们首先加载图像处理、建模和推理的特定依赖项。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nm nn l"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">代码清单:导入tensorflow和tensorflowhub</p></figure><p id="5da1" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">请注意，我们在这里使用TensorFlow 2.x，这是撰写本文时的最新版本。由于我们将直接使用预先训练的模型进行推理，因此我们需要知道ResNet-50和BiTM-R152x4模型的原始ImageNet-1K和ImageNet-21K数据集的类标签，如清单1所示。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nm nn l"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">代码清单1:查看ImageNet 1k和ImageNet21k数据集的样本类标签</p></figure><p id="5aa6" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">下一步是从TensorFlow Hub加载我们之前讨论过的两个预训练模型。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nm nn l"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">代码清单:加载预训练的resnet和bit模型</p></figure><p id="2a85" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">一旦我们准备好了预先训练好的模型，下一步将集中于构建一些特定的实用函数，您可以从本文的笔记本中访问这些函数。只是为了获得一些观点，</p><ul class=""><li id="f09f" class="mr ms iq ky b kz la lc ld lf mt lj mu ln mv lr mw mx my mz bi translated"><code class="fe no np nq nr b">preprocess_image(…)</code>功能帮助我们在0-1的范围内对输入图像像素值进行预处理、整形和缩放。</li><li id="af05" class="mr ms iq ky b kz na lc nb lf nc lj nd ln ne lr mw mx my mz bi translated"><code class="fe no np nq nr b">visualize_predictions(...)</code>函数将预训练模型、类别标签映射、模型类型和输入图像作为输入，以条形图的形式显示前5个预测。</li></ul><p id="d62b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">ResNet-50模型直接给出类别概率作为输入，但是BiTM-R152x4模型给出类别逻辑作为输出，其需要被转换成类别概率。我们可以看看清单2，它展示了<code class="fe no np nq nr b">visualize_predictions(...)</code>函数的一部分，可以帮助我们实现这一点。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nm nn l"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">代码清单2: visualize_prediction函数，它获取前5个预测的模型概率</p></figure><p id="2ceb" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">请记住，logit基本上是对数优势或非标准化类别概率，因此您需要计算这些logit的softmax，以获得总和为1的标准化类别概率，如图6所示，该图显示了一个示例神经网络架构，其中包含一个假设的3类分类问题的logit和类别概率。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mq"><img src="../Images/3085b3f3d331052ccd162179b47a2b8b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jPfzcNmITGpJ8teriQDVDw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图6神经网络中的Logits和Softmax值。图片来源:作者</p></figure><p id="6d4f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">softmax函数基本上使用图6中描述的变换来压缩逻辑，从而为我们提供标准化的类概率。现在让我们将代码付诸行动吧！使用清单3中描述的步骤序列，您可以在任何下载的图像上利用这些函数来可视化我们的两个预训练模型的前5个预测。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nm nn l"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">代码清单3:分析输入图像并可视化模型预测</p></figure><p id="cfeb" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">瞧啊。我们有来自两个预训练模型的前5个预测，如图7所示。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mq"><img src="../Images/ede5bb2ff26374e8aa20f78513fa6350.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8fOML2u87kRQ6Zqamdwtyw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图7雪豹图像的预测结果。图片来源:作者</p></figure><p id="da48" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">看起来我们的两个模型都表现得很好，正如预期的那样，BitM模型非常具体，并且更加准确，因为它已经在超过21K个具有非常具体的动物物种和品种的类上进行了训练。</p><p id="fee1" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">如图8所示，与BiTM模型相比，ResNet-50模型在预测相似属但略有不同物种的动物(如老虎和狮子)时有更多的不一致性。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mq"><img src="../Images/c845a69695144b987d8deb5adef4bf7b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FTUaa_-0oBeL1J7o8kwCGQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图BitM和ResNet-50模型的正确与错误预测。图片来源:作者</p></figure><h1 id="8e97" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated">局限性和可能的改进</h1><p id="dd2c" class="pw-post-body-paragraph kw kx iq ky b kz ml jr lb lc mm ju le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated">需要记住的另一个方面是，这些模型并不详尽。它们并没有覆盖这个星球上的每一个实体。考虑到这项任务本身的数据收集需要几个世纪，如果不是永远的话，这是不可能做到的！图9展示了一个例子，我们的模型试图从给定的图像中预测一个非常特殊的狗品种，阿富汗猎犬。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mq"><img src="../Images/907589b18a8040547afa50ba243399a2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vOYIZbAaCO_wC4XwTnJykg.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图9我们的两个模型都很难预测阿富汗猎犬。图片来源:作者</p></figure><p id="42f0" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">根据图9中的前5个预测，您可以看到，虽然我们的BiTM模型实际上获得了正确的预测，但预测概率非常低，表明我们的模型不太自信(鉴于它可能在预训练阶段的训练数据中没有看到太多该犬种的例子)。在这里，我们可以微调和调整我们的模型，使它们更适合我们的特定数据集和输出标签和结果。这形成了许多应用的基础，在这些应用中，我们可以利用预先训练的模型，并使这些模型适应非常不同和新颖的问题。文章“<a class="ae kv" rel="noopener" target="_blank" href="/supercharge-your-image-search-with-transfer-learning-75dfb5d29ceb"> <em class="ls">用迁移学习增强你的图像搜索”</em> </a>中介绍了这样一个有趣的用例</p><h1 id="c275" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated">摘要</h1><ul class=""><li id="fd89" class="mr ms iq ky b kz ml lc mm lf ns lj nt ln nu lr mw mx my mz bi translated">图像分类是深度学习和迁移学习最流行和研究最多的应用之一</li><li id="6e2f" class="mr ms iq ky b kz na lc nb lf nc lj nd ln ne lr mw mx my mz bi translated">卷积神经网络或CNN在从图像输入中提取特征时非常强大。</li><li id="efb8" class="mr ms iq ky b kz na lc nb lf nc lj nd ln ne lr mw mx my mz bi translated">ResNets或残差神经网络及其变体已经在大型多类数据集(如ImageNet)上证明了性能。</li><li id="7791" class="mr ms iq ky b kz na lc nb lf nc lj nd ln ne lr mw mx my mz bi translated">Google的最新ResNet变体BiT model非常强大，为图像分类任务提供了最先进的性能</li><li id="c165" class="mr ms iq ky b kz na lc nb lf nc lj nd ln ne lr mw mx my mz bi translated">我们很容易利用Tensorflow-Hub的预训练结果来了解这些模型在新数据集上进行迁移学习/微调的能力</li></ul></div><div class="ab cl nv nw hu nx" role="separator"><span class="ny bw bk nz oa ob"/><span class="ny bw bk nz oa ob"/><span class="ny bw bk nz oa"/></div><div class="ij ik il im in"><p id="965d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">如果你想更多地了解这些有趣的话题，可以看看由<a class="oc od ep" href="https://medium.com/u/6278d12b0682?source=post_page-----4b8c52e69c0c--------------------------------" rel="noopener" target="_blank">迪潘然(DJ)萨卡尔</a>和<a class="ae kv" href="https://raghavbali.github.io/" rel="noopener ugc nofollow" target="_blank">拉格哈夫巴里</a>与<a class="oc od ep" href="https://medium.com/u/bd9671589b55?source=post_page-----4b8c52e69c0c--------------------------------" rel="noopener" target="_blank">曼宁出版社</a>合作的《<strong class="ky ir">迁移学习在行动中</strong>》。你可以在曼宁的liveBook平台<a class="ae kv" href="https://livebook.manning.com/book/transfer-learning-in-action?origin=product-look-inside&amp;utm_source=blog&amp;utm_medium=organic&amp;utm_campaign=book_sarkar_transfer_06_18_21&amp;utm_content=tdd" rel="noopener ugc nofollow" target="_blank">这里</a>查看一下。</p><p id="c767" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在<a class="ae kv" href="https://www.manning.com/?utm_source=blog&amp;utm_medium=organic&amp;utm_campaign=book_sarkar_transfer_06_18_21&amp;utm_content=tdd" rel="noopener ugc nofollow" target="_blank">manning.com</a>结账时，将<strong class="ky ir"> fccsarkar </strong>输入折扣代码框，即可享受40%的折扣<a class="ae kv" href="https://www.manning.com/books/transfer-learning-in-action?utm_source=blog&amp;utm_medium=organic&amp;utm_campaign=book_sarkar_transfer_06_18_21&amp;utm_content=tdd" rel="noopener ugc nofollow" target="_blank"> <em class="ls">转移学习行动</em> </a>。</p></div><div class="ab cl nv nw hu nx" role="separator"><span class="ny bw bk nz oa ob"/><span class="ny bw bk nz oa ob"/><span class="ny bw bk nz oa"/></div><div class="ij ik il im in"><h1 id="a3c1" class="lt lu iq bd lv lw oe ly lz ma of mc md jw og jx mf jz oh ka mh kc oi kd mj mk bi translated">参考</h1><ul class=""><li id="3b83" class="mr ms iq ky b kz ml lc mm lf ns lj nt ln nu lr mw mx my mz bi translated">[1] <a class="ae kv" href="https://livebook.manning.com/book/transfer-learning-in-action/chapter-1/v-1/" rel="noopener ugc nofollow" target="_blank">迁移学习的基础，迁移学习在行动</a></li><li id="cd8e" class="mr ms iq ky b kz na lc nb lf nc lj nd ln ne lr mw mx my mz bi translated">[2] <a class="ae kv" href="https://ieeexplore.ieee.org/document/5288526" rel="noopener ugc nofollow" target="_blank">一项关于迁移学习的调查</a></li><li id="72ee" class="mr ms iq ky b kz na lc nb lf nc lj nd ln ne lr mw mx my mz bi translated">[3]关于ImageNet:<a class="ae kv" href="http://www.image-net.org/about-stats" rel="noopener ugc nofollow" target="_blank">http://www.image-net.org/about-stats</a></li><li id="5f99" class="mr ms iq ky b kz na lc nb lf nc lj nd ln ne lr mw mx my mz bi translated">[4]大迁移(BiT):一般视觉表征学习，科列斯尼科夫等。铝</li></ul></div></div>    
</body>
</html>