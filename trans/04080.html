<html>
<head>
<title>Pandas to PySpark in 6 Examples</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">熊猫到PySpark的6个例子</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/pandas-to-pyspark-in-6-examples-bd8ab825d389?source=collection_archive---------20-----------------------#2021-04-06">https://towardsdatascience.com/pandas-to-pyspark-in-6-examples-bd8ab825d389?source=collection_archive---------20-----------------------#2021-04-06</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="282f" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">当你去大规模的时候会发生什么</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/3f8753c3a294429f29b95b673685fa2e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FRDF9beiFzZ70Sy1XxhOaw.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">杰克·吉文斯在<a class="ae ky" href="https://unsplash.com/s/photos/fast-and-slow?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="1014" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">Pandas是操纵和分析结构化数据的主要工具之一。它提供了许多函数和方法来处理表格数据。</p><p id="8c97" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然而，随着数据变大，熊猫可能不是你最好的朋友。当处理大规模数据时，有必要同时分发数据和计算，这是Pandas无法实现的。</p><p id="5b32" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这类任务的一个非常受欢迎的选项是Spark，这是一个用于大规模数据处理的分析引擎。它让您可以将数据和计算分散到集群上，从而实现显著的性能提升。</p><p id="5370" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">收集和存储数据已经变得非常容易，所以当我们处理现实生活中的问题时，可能会有大量的数据。因此，像Spark这样的分布式引擎正在成为数据科学生态系统中的主要工具。</p><p id="5eaf" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">PySpark是Spark的Python API。它结合了Python的简单性和Spark的高性能。在本文中，我们将通过6个例子来演示PySpark版Pandas的典型数据分析和操作任务。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h2 id="7fcb" class="mc md it bd me mf mg dn mh mi mj dp mk li ml mm mn lm mo mp mq lq mr ms mt mu bi translated">示例1</h2><p id="12f2" class="pw-post-body-paragraph kz la it lb b lc mv ju le lf mw jx lh li mx lk ll lm my lo lp lq mz ls lt lu im bi translated">我们需要一个例子的数据集。因此，第一个示例是通过读取csv文件来创建数据帧。我将使用Kaggle上的墨尔本房产<a class="ae ky" href="https://www.kaggle.com/anthonypino/melbourne-housing-market" rel="noopener ugc nofollow" target="_blank">数据集</a>。</p><pre class="kj kk kl km gt na nb nc nd aw ne bi"><span id="0d10" class="mc md it nb b gy nf ng l nh ni"># Pandas<br/>import pandas as pd<br/>df = pd.read_csv("melb_housing.csv")</span></pre><p id="4927" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对于PySpark，我们首先需要创建一个SparkSession，作为Spark SQL的入口点。</p><pre class="kj kk kl km gt na nb nc nd aw ne bi"><span id="990e" class="mc md it nb b gy nf ng l nh ni">from pyspark.sql import SparkSession<br/>sc = SparkSession.builder.getOrCreate()</span><span id="6ff8" class="mc md it nb b gy nj ng l nh ni">sc.sparkContext.setLogLevel("WARN")print(sc)<br/>&lt;pyspark.sql.session.SparkSession object at 0x7fecd819e630&gt;</span></pre><p id="aa25" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们现在可以读取csv文件了。</p><pre class="kj kk kl km gt na nb nc nd aw ne bi"><span id="01d1" class="mc md it nb b gy nf ng l nh ni"># PySpark<br/>df = sc.read.option("header", "true").csv(<br/>    "/home/sparkuser/Desktop/melb_housing.csv"<br/>)</span></pre></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h2 id="ec45" class="mc md it bd me mf mg dn mh mi mj dp mk li ml mm mn lm mo mp mq lq mr ms mt mu bi translated">示例2</h2><p id="04fa" class="pw-post-body-paragraph kz la it lb b lc mv ju le lf mw jx lh li mx lk ll lm my lo lp lq mz ls lt lu im bi translated">开始数据分析的一个好方法是获得数据的概述。例如，我们可能想要检查列名、行数，并查看前几行。</p><pre class="kj kk kl km gt na nb nc nd aw ne bi"><span id="a8cd" class="mc md it nb b gy nf ng l nh ni"># Pandas</span><span id="7005" class="mc md it nb b gy nj ng l nh ni">len(df)<br/>63023</span><span id="9cdc" class="mc md it nb b gy nj ng l nh ni">df.columns<br/>Index(['Suburb', 'Address', 'Rooms', 'Type', 'Price', 'Method', 'SellerG','Date', 'Postcode', 'Regionname', 'Propertycount', 'Distance','CouncilArea'])</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nk"><img src="../Images/9a8286f8682dcd41e94b926117f1c8e7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qMgGkQEk6fJwt90H2iQoOg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">(图片由作者提供)</p></figure><p id="72c5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这些函数和方法与PySpark非常相似。</p><pre class="kj kk kl km gt na nb nc nd aw ne bi"><span id="11ac" class="mc md it nb b gy nf ng l nh ni"># PySpark</span><span id="8962" class="mc md it nb b gy nj ng l nh ni">df.count()<br/>63023</span><span id="21d8" class="mc md it nb b gy nj ng l nh ni">df.columns<br/>['Suburb', 'Address', 'Rooms', 'Type', 'Price', 'Method', 'SellerG','Date', 'Postcode', 'Regionname', 'Propertycount', 'Distance','CouncilArea']</span><span id="37fb" class="mc md it nb b gy nj ng l nh ni">df.show(5)</span></pre></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h2 id="2e55" class="mc md it bd me mf mg dn mh mi mj dp mk li ml mm mn lm mo mp mq lq mr ms mt mu bi translated">示例3</h2><p id="a270" class="pw-post-body-paragraph kz la it lb b lc mv ju le lf mw jx lh li mx lk ll lm my lo lp lq mz ls lt lu im bi translated">在某些情况下，我们需要根据列值过滤数据框。例如，我们可能对北部大都市地区价值超过100万英镑的房子感兴趣。</p><p id="badc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对于Pandas，wgionne指定条件和列名如下:</p><pre class="kj kk kl km gt na nb nc nd aw ne bi"><span id="a252" class="mc md it nb b gy nf ng l nh ni"># Pandas<br/>df_sub = df[<br/>      (df.Price &gt; 1000000) &amp; <br/>      (df.Regionname == 'Northern Metropolitan')<br/>]</span><span id="21d6" class="mc md it nb b gy nj ng l nh ni">len(df_sub)<br/>3022</span></pre><p id="2f8c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们对PySpark采用了类似的方法。</p><pre class="kj kk kl km gt na nb nc nd aw ne bi"><span id="26dd" class="mc md it nb b gy nf ng l nh ni"># PySpark<br/>from pyspark.sql import functions as F</span><span id="d37f" class="mc md it nb b gy nj ng l nh ni">df_sub = df.filter(<br/>    (F.col('Price') &gt; 1000000) &amp;<br/>    (F.col('Regionname') == 'Northern Metropolitan')<br/>)</span><span id="d124" class="mc md it nb b gy nj ng l nh ni">df_sub.count()<br/>3022</span></pre><p id="ba23" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">PySpark的SQL模块中的函数需要单独导入。在以下示例中，我们将使用本模块中的几个函数。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h2 id="18c2" class="mc md it bd me mf mg dn mh mi mj dp mk li ml mm mn lm mo mp mq lq mr ms mt mu bi translated">实例4</h2><p id="a109" class="pw-post-body-paragraph kz la it lb b lc mv ju le lf mw jx lh li mx lk ll lm my lo lp lq mz ls lt lu im bi translated">典型的任务可以是根据列选择数据集的子集。让我们以价格、地区名称和地址列为例，按价格降序排列。我们将只显示前5个观察值(即行)。</p><pre class="kj kk kl km gt na nb nc nd aw ne bi"><span id="b47b" class="mc md it nb b gy nf ng l nh ni"># Pandas<br/>df[['Price', 'Regionname', 'Address']].sort_values(by='Price', ascending=False).head()</span><span id="bec8" class="mc md it nb b gy nj ng l nh ni"># PySpark<br/>df.select("Price", "Regionname", "Address").orderBy('Price', ascending=False).show(5)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nl"><img src="../Images/31ecb7da43c251864b0e559213893ae1.png" data-original-src="https://miro.medium.com/v2/resize:fit:640/format:webp/1*4oyzaq5ZIQoZ1MlO_q5BdA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">(图片由作者提供)</p></figure><p id="68d5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">语法也非常相似。PySpark的功能更像SQL语句。此外，正如您所注意到的，PySpark像显示SQL表一样显示数据框。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h2 id="b807" class="mc md it bd me mf mg dn mh mi mj dp mk li ml mm mn lm mo mp mq lq mr ms mt mu bi translated">实例5</h2><p id="3052" class="pw-post-body-paragraph kz la it lb b lc mv ju le lf mw jx lh li mx lk ll lm my lo lp lq mz ls lt lu im bi translated">我们并不总是数字数据。文本数据是数据科学的基础部分。因此，数据分析库提供了许多操作字符串的函数。</p><p id="ead5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">例如，我们可能希望从地址列的值中提取地址的类型。</p><pre class="kj kk kl km gt na nb nc nd aw ne bi"><span id="917c" class="mc md it nb b gy nf ng l nh ni"># Pandas<br/>df.Address[:3]<br/>0    49 Lithgow St <br/>1    59A Turner St <br/>2    119B Yarra St</span></pre><p id="e6d3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">地址中的最后一个字给出了st和rd等类型。我们可以通过在空格处拆分地址并获得最后一项来获得该信息。</p><p id="d160" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">Pandas使用str访问器下的split函数来完成这个操作。</p><pre class="kj kk kl km gt na nb nc nd aw ne bi"><span id="f29d" class="mc md it nb b gy nf ng l nh ni"># Pandas<br/>df.Address.str.split(' ', expand=True)[2][:5]<br/>0    St <br/>1    St <br/>2    St <br/>3    St <br/>4    Rd</span></pre><p id="e719" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">末尾的“[:5]”表达式仅用于显示前5行。</p><p id="3269" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">Pyspark还有一个拆分功能。下面是如何使用该函数完成该操作的:</p><pre class="kj kk kl km gt na nb nc nd aw ne bi"><span id="d0aa" class="mc md it nb b gy nf ng l nh ni"># PySpark<br/>df.select(<br/>   F.split("Address", ' ').getItem(2).alias('Address_type')<br/>).show(5)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nm"><img src="../Images/0ed3171b645e9b273e92a0604cf65f51.png" data-original-src="https://miro.medium.com/v2/resize:fit:338/format:webp/1*QxYiE1dbq5m3wT5EGS0l9w.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">(图片由作者提供)</p></figure></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h2 id="d21a" class="mc md it bd me mf mg dn mh mi mj dp mk li ml mm mn lm mo mp mq lq mr ms mt mu bi translated">实例6</h2><p id="7407" class="pw-post-body-paragraph kz la it lb b lc mv ju le lf mw jx lh li mx lk ll lm my lo lp lq mz ls lt lu im bi translated">数据分析中最常用的函数之一是groupby函数。它允许根据列中的类别或不同值对行进行分组。然后，我们可以对每个组的数字列执行聚合。</p><p id="f401" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">例如，我们可以计算每个地区的平均房价。熊猫和PySpark都有这类任务的分组功能。</p><pre class="kj kk kl km gt na nb nc nd aw ne bi"><span id="0a9d" class="mc md it nb b gy nf ng l nh ni"># Pandas<br/>df.groupby('Regionname').agg(Avg_price=('Price', 'mean')).round(2)</span><span id="72e3" class="mc md it nb b gy nj ng l nh ni"># Pyspark<br/>df.groupby('Regionname').agg(F.round(F.mean('Price'), 2).alias('Avg_price'))</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nn"><img src="../Images/9c0ef71e5a1b541850d4479be1904b38.png" data-original-src="https://miro.medium.com/v2/resize:fit:910/format:webp/1*yXJfG0yDS_oOQHxdAZRJtw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">(图片由作者提供)</p></figure></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h2 id="ae55" class="mc md it bd me mf mg dn mh mi mj dp mk li ml mm mn lm mo mp mq lq mr ms mt mu bi translated">结论</h2><p id="1c60" class="pw-post-body-paragraph kz la it lb b lc mv ju le lf mw jx lh li mx lk ll lm my lo lp lq mz ls lt lu im bi translated">我们做了6个例子来比较Pandas和PySpark的语法。正如我们在例子中看到的，它们非常相似。</p><p id="01ed" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">需要注意的是，Spark针对大规模数据进行了优化。因此，在处理小规模数据时，您可能看不到任何性能提升。事实上，在处理小数据集时，Pandas可能会比PySpark表现得更好。</p><p id="8c70" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">感谢您的阅读。如果您有任何反馈，请告诉我。</p></div></div>    
</body>
</html>