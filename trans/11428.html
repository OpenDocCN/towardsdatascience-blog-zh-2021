<html>
<head>
<title>Radial Basis Function Neural Network Simplified</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">简化的径向基函数神经网络</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/radial-basis-function-neural-network-simplified-6f26e3d5e04d?source=collection_archive---------1-----------------------#2021-11-10">https://towardsdatascience.com/radial-basis-function-neural-network-simplified-6f26e3d5e04d?source=collection_archive---------1-----------------------#2021-11-10</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="9ab8" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph">深度学习</h2><div class=""/><div class=""><h2 id="58e9" class="pw-subtitle-paragraph jw iz iq bd b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn dk translated">径向基函数神经网络简介</h2></div><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ko"><img src="../Images/0efb758710bfbe771d3f32bce9b4f5f0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*J1VAdZPusS1Enl6c"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated"><a class="ae le" href="https://unsplash.com/@zoltantasi?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Zoltan·塔斯</a>在<a class="ae le" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="807d" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi mb translated"><span class="l mc md me bm mf mg mh mi mj di"> R </span> <strong class="lh ja">径向基函数(RBF)网络</strong>的架构与大多数神经网络架构完全不同。大多数神经网络结构由许多层组成，并通过重复应用非线性激活函数来引入非线性。另一方面，RBF网络仅由一个输入层、一个隐藏层和一个输出层组成。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi mk"><img src="../Images/153641bd15777f5f1f72cef211f713d5.png" data-original-src="https://miro.medium.com/v2/resize:fit:824/format:webp/1*JGp_iNl34W4PPZEshoWBFw.png"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">作者图片</p></figure><p id="bffe" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">输入层不是计算层，它只是接收输入数据并将其输入到RBF网络的特殊隐藏层。隐藏层内部发生的计算与大多数神经网络非常不同，这就是RBF网络的强大之处。输出层执行预测任务，如分类或回归。</p></div><div class="ab cl ml mm hu mn" role="separator"><span class="mo bw bk mp mq mr"/><span class="mo bw bk mp mq mr"/><span class="mo bw bk mp mq"/></div><div class="ij ik il im in"><h2 id="775c" class="ms mt iq bd mu mv mw dn mx my mz dp na lo nb nc nd ls ne nf ng lw nh ni nj iw bi translated">输入层</h2><p id="5602" class="pw-post-body-paragraph lf lg iq lh b li nk ka lk ll nl kd ln lo nm lq lr ls nn lu lv lw no ly lz ma ij bi translated">输入层只是将数据提供给隐藏层。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi np"><img src="../Images/cb35431af3c1dc387682a12b31217240.png" data-original-src="https://miro.medium.com/v2/resize:fit:716/format:webp/1*j2KXzQPhYQmi6ozmH7nWpg.png"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">作者图片</p></figure><p id="b2b4" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">因此，输入层中神经元的数量应该等于数据的维数。在输入层中，不执行任何计算，这与标准人工神经网络的情况不同。输入神经元完全连接到隐藏神经元，并向前馈送它们的输入。</p></div><div class="ab cl ml mm hu mn" role="separator"><span class="mo bw bk mp mq mr"/><span class="mo bw bk mp mq mr"/><span class="mo bw bk mp mq"/></div><div class="ij ik il im in"><h2 id="b436" class="ms mt iq bd mu mv mw dn mx my mz dp na lo nb nc nd ls ne nf ng lw nh ni nj iw bi translated">隐蔽层</h2><p id="0301" class="pw-post-body-paragraph lf lg iq lh b li nk ka lk ll nl kd ln lo nm lq lr ls nn lu lv lw no ly lz ma ij bi translated">隐藏层接受模式可能不可线性分离的输入，并将其转换到更可线性分离的新空间。隐藏层比输入层具有更高的维数，因为不可线性分离的模式通常需要被变换到更高维的空间中，以便更可线性分离。这是基于<strong class="lh ja"> Cover关于模式可分性的定理</strong>，该定理指出，通过非线性变换转换到更高维空间的模式更可能是线性可分的，因此隐藏层中的神经元数量应该大于输入神经元的数量。也就是说，隐藏层中神经元的数量应该小于或等于训练集中样本的数量。当隐层神经元的数量等于训练集中样本的数量时，可以认为模型大致等价于核回归、核支持向量机等<strong class="lh ja">核学习器</strong>。</p><p id="40ca" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">隐藏层中的计算基于与原型向量的比较，原型向量是来自训练集的向量。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/91cb4f6b5105b24d9f6225b24c0fa1cf.png" data-original-src="https://miro.medium.com/v2/resize:fit:866/format:webp/1*UirjHfswjARfIYesZng4pw.png"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">作者图片</p></figure><p id="ff6a" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">隐藏层中的每个神经元都有一个原型向量和一个带宽，分别用μ和σ表示。每个神经元计算输入向量与其原型向量之间的相似性。隐藏层中的计算可以数学地写成如下:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/992646685bd07ea560403e836187b10e.png" data-original-src="https://miro.medium.com/v2/resize:fit:410/format:webp/1*YLmTePeQdA5-kQ1t5nc1jw.png"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">作者图片</p></figure><p id="c04e" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">使用:</p><ul class=""><li id="c313" class="ns nt iq lh b li lj ll lm lo nu ls nv lw nw ma nx ny nz oa bi translated">x条作为输入向量</li><li id="1a37" class="ns nt iq lh b li ob ll oc lo od ls oe lw of ma nx ny nz oa bi translated">μ bar作为iᵗʰ神经元的原型向量</li><li id="8ad5" class="ns nt iq lh b li ob ll oc lo od ls oe lw of ma nx ny nz oa bi translated">σ为iᵗʰ神经元的带宽</li><li id="765f" class="ns nt iq lh b li ob ll oc lo od ls oe lw of ma nx ny nz oa bi translated">φ为iᵗʰ神经元的输出</li></ul><p id="a16c" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">参数μ bar和σ以无监督的方式学习，例如使用某种聚类算法。</p></div><div class="ab cl ml mm hu mn" role="separator"><span class="mo bw bk mp mq mr"/><span class="mo bw bk mp mq mr"/><span class="mo bw bk mp mq"/></div><div class="ij ik il im in"><h2 id="de41" class="ms mt iq bd mu mv mw dn mx my mz dp na lo nb nc nd ls ne nf ng lw nh ni nj iw bi translated">输出层</h2><p id="8bc2" class="pw-post-body-paragraph lf lg iq lh b li nk ka lk ll nl kd ln lo nm lq lr ls nn lu lv lw no ly lz ma ij bi translated">输出层对分类或回归任务使用线性激活函数。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi og"><img src="../Images/92cd3881c132e3c0291a525c7a30ba4a.png" data-original-src="https://miro.medium.com/v2/resize:fit:602/format:webp/1*4FBb8qP4RxrITGLCQkWwyQ.png"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">作者图片</p></figure><p id="4288" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">输出层中的计算就像标准的人工神经网络一样执行，它是输入向量和权重向量之间的线性组合。输出层中的计算可以数学地写成如下:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/6f56b514e67ec5fa9e17c1f0a9e82709.png" data-original-src="https://miro.medium.com/v2/resize:fit:350/format:webp/1*NxlmqVijGGY0ebFvJvLhRg.png"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">作者图片</p></figure><p id="1270" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">使用:</p><ul class=""><li id="f745" class="ns nt iq lh b li lj ll lm lo nu ls nv lw nw ma nx ny nz oa bi translated">wᵢ作为重量连接</li><li id="2610" class="ns nt iq lh b li ob ll oc lo od ls oe lw of ma nx ny nz oa bi translated">φ作为iᵗʰ神经元从隐藏层的输出</li><li id="a36e" class="ns nt iq lh b li ob ll oc lo od ls oe lw of ma nx ny nz oa bi translated">y作为预测结果</li></ul><p id="3bcf" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">得到的预测可用于分类或回归任务，这取决于目标和损失函数。参数w以监督的方式学习，例如梯度下降。</p><p id="9459" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">虽然RBF的输出层可以作为最终输出，但是也可以将RBF网络与其他网络进行堆叠，比如我们可以用多层感知代替RBF网络的输出层，对网络进行端到端的训练。</p></div><div class="ab cl ml mm hu mn" role="separator"><span class="mo bw bk mp mq mr"/><span class="mo bw bk mp mq mr"/><span class="mo bw bk mp mq"/></div><div class="ij ik il im in"><h2 id="4244" class="ms mt iq bd mu mv mw dn mx my mz dp na lo nb nc nd ls ne nf ng lw nh ni nj iw bi translated">结论</h2><p id="2c94" class="pw-post-body-paragraph lf lg iq lh b li nk ka lk ll nl kd ln lo nm lq lr ls nn lu lv lw no ly lz ma ij bi translated">RBF网络只包含一个单独的隐藏层，它有自己计算输出的方法。RBF网络是基于覆盖定理，它利用其隐含层将数据投射到一个更高维的空间，因此隐含层的神经元数目应该大于输入层的神经元数目。输出层使用线性激活函数，或者可以认为没有任何激活函数。</p></div><div class="ab cl ml mm hu mn" role="separator"><span class="mo bw bk mp mq mr"/><span class="mo bw bk mp mq mr"/><span class="mo bw bk mp mq"/></div><div class="ij ik il im in"><h2 id="7b6c" class="ms mt iq bd mu mv mw dn mx my mz dp na lo nb nc nd ls ne nf ng lw nh ni nj iw bi translated">参考</h2><div class="oi oj gp gr ok ol"><a href="https://www.amazon.com/Neural-Networks-Deep-Learning-Textbook/dp/3319944622" rel="noopener  ugc nofollow" target="_blank"><div class="om ab fo"><div class="on ab oo cl cj op"><h2 class="bd ja gy z fp oq fr fs or fu fw iz bi translated">神经网络和深度学习:教科书</h2><div class="os l"><h3 class="bd b gy z fp oq fr fs or fu fw dk translated">神经网络和深度学习:一本关于Amazon.com的教科书。*符合条件的优惠可享受免费*运输…</h3></div><div class="ot l"><p class="bd b dl z fp oq fr fs or fu fw dk translated">www.amazon.com</p></div></div><div class="ou l"><div class="ov l ow ox oy ou oz ky ol"/></div></div></a></div><div class="oi oj gp gr ok ol"><a href="http://vlabs.iitb.ac.in/vlabs-dev/labs_local/machine_learning/labs/exp3/theory.php" rel="noopener  ugc nofollow" target="_blank"><div class="om ab fo"><div class="on ab oo cl cj op"><h2 class="bd ja gy z fp oq fr fs or fu fw iz bi translated">虚拟实验室</h2><div class="os l"><h3 class="bd b gy z fp oq fr fs or fu fw dk translated">径向基函数网络(RBFN)是一种特殊类型的神经网络。RBFN方法比…更直观</h3></div><div class="ot l"><p class="bd b dl z fp oq fr fs or fu fw dk translated">vlabs.iitb.ac.in</p></div></div><div class="ou l"><div class="pa l ow ox oy ou oz ky ol"/></div></div></a></div><div class="oi oj gp gr ok ol"><a href="https://www.sciencedirect.com/topics/engineering/radial-basis-function-network" rel="noopener  ugc nofollow" target="_blank"><div class="om ab fo"><div class="on ab oo cl cj op"><h2 class="bd ja gy z fp oq fr fs or fu fw iz bi translated">径向基函数网络</h2><div class="os l"><h3 class="bd b gy z fp oq fr fs or fu fw dk translated">径向基函数(RBF)网络是一种常用的人工神经网络类型，用于函数逼近</h3></div><div class="ot l"><p class="bd b dl z fp oq fr fs or fu fw dk translated">www.sciencedirect.com</p></div></div><div class="ou l"><div class="pb l ow ox oy ou oz ky ol"/></div></div></a></div><div class="oi oj gp gr ok ol"><a href="https://en.wikipedia.org/wiki/Radial_basis_function_network" rel="noopener  ugc nofollow" target="_blank"><div class="om ab fo"><div class="on ab oo cl cj op"><h2 class="bd ja gy z fp oq fr fs or fu fw iz bi translated">径向基函数网络-维基百科</h2><div class="os l"><h3 class="bd b gy z fp oq fr fs or fu fw dk translated">在数学建模领域，径向基函数网络是一种使用径向基函数的人工神经网络</h3></div><div class="ot l"><p class="bd b dl z fp oq fr fs or fu fw dk translated">en.wikipedia.org</p></div></div><div class="ou l"><div class="pc l ow ox oy ou oz ky ol"/></div></div></a></div></div></div>    
</body>
</html>