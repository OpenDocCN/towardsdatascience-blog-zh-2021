<html>
<head>
<title>Deploying Kubeflow to a Bare-Metal GPU Cluster from Scratch</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">从头开始将Kubeflow部署到裸机GPU集群</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/deploying-kubeflow-to-a-bare-metal-gpu-cluster-from-scratch-6865ebcde032?source=collection_archive---------21-----------------------#2021-03-18">https://towardsdatascience.com/deploying-kubeflow-to-a-bare-metal-gpu-cluster-from-scratch-6865ebcde032?source=collection_archive---------21-----------------------#2021-03-18</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="841e" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">我在具有多个GPU的物理服务器上部署Google的Kubernetes ML工具包的经验</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/4355ed65682bb33d905be82813e9fc54.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CYLqg9KP6oMs-5rR7ajEeQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">攻击库伯弗洛。图片来自Anastasia Markovtseva，CC-BY-SA 4.0。</p></figure><h2 id="393f" class="ky kz it bd la lb lc dn ld le lf dp lg lh li lj lk ll lm ln lo lp lq lr ls lt bi translated">五金器具</h2><p id="596f" class="pw-post-body-paragraph lu lv it lw b lx ly ju lz ma mb jx mc lh md me mf ll mg mh mi lp mj mk ml mm im bi translated">我有3个标准的超微塔式服务器，每个都有256GB内存、一个固态硬盘、5个硬盘和4个GPU。以太网将它们连接到可以访问互联网的“控制器”戴尔服务器，并应该控制SSH到群集的连接。我用团队成员的家乡城市来命名这些塔；我发现这个方案比分配随机的形容词(“食蚁兽”、“无畏者”)、前缀索引(“数据科学1”、“数据科学2”)或希腊字母(“阿尔法”、“贝塔”)更有趣，我在以前工作的地方已经见过太多次了。当有人问你在哪个服务器上训练网络时，你可以回答，“我在马德里”或“我在莫斯科”。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mn"><img src="../Images/0861a064004ded3652e853a03048fbde.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*w8MzvRhkFeJyA0Btr4K5-w.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">集群网络方案。图片作者。</p></figure><h2 id="4ad1" class="ky kz it bd la lb lc dn ld le lf dp lg lh li lj lk ll lm ln lo lp lq lr ls lt bi translated">安装Linux</h2><p id="147b" class="pw-post-body-paragraph lu lv it lw b lx ly ju lz ma mb jx mc lh md me mf ll mg mh mi lp mj mk ml mm im bi translated">我在每台机器上都安装了Ubuntu 20.04。这一步已经存在一定的困难:没有配置PXE(通过网络引导)，安装操作系统的唯一文明方式是从闪存盘引导，而<a class="ae mo" href="https://www.dell.com/community/Storage-Drives-Media/Can-t-Install-OS-from-Bootable-USB-Drive-on-Dell-Poweredge/td-p/6196414" rel="noopener ugc nofollow" target="_blank">戴尔服务器不支持它</a>。我的意思是，有USB端口，当然，但他们的UEFI没有看到一个可引导的设备。我不得不在2020年刻录一张DVD，我仍然无法相信。在超微型电脑上安装Ubuntu也不是一件容易的事。<a class="ae mo" href="https://unix.stackexchange.com/questions/541489/grub-fails-to-install-during-debian-10-installer-re-uefi-supermicro-motherboa" rel="noopener ugc nofollow" target="_blank">由于UEFI </a>出了问题，GRUB无法安装，所以我不得不在安装过程中进入内核外壳并即时修复。</p><blockquote class="mp mq mr"><p id="c5a0" class="lu lv ms lw b lx mt ju lz ma mu jx mc mv mw me mf mx my mh mi mz na mk ml mm im bi translated">PXE代表<a class="ae mo" href="https://en.wikipedia.org/wiki/Preboot_Execution_Environment" rel="noopener ugc nofollow" target="_blank">预启动执行环境</a>。PXE指定机器从由<a class="ae mo" href="https://en.wikipedia.org/wiki/Trivial_File_Transfer_Protocol" rel="noopener ugc nofollow" target="_blank"> TFTP </a>下载的镜像启动，而不是像往常一样从磁盘读取。</p></blockquote><p id="96b7" class="pw-post-body-paragraph lu lv it lw b lx mt ju lz ma mu jx mc lh mw me mf ll my mh mi lp na mk ml mm im bi translated">如果我知道计算实例运行在有白名单用户的可信网络中，我总是禁用计算实例上的内核安全补丁。附带的性能损失比强加的风险要昂贵得多。所以我在GPU塔上编辑了<code class="fe nb nc nd ne b">/etc/default/grub</code>如下:</p><pre class="kj kk kl km gt nf ne ng nh aw ni bi"><span id="864a" class="ky kz it ne b gy nj nk l nl nm">GRUB_CMDLINE_LINUX_DEFAULT="pti=off spectre_v2=off l1tf=off nospec_store_bypass_disable no_stf_barrier"</span></pre><blockquote class="mp mq mr"><p id="d2f0" class="lu lv ms lw b lx mt ju lz ma mu jx mc mv mw me mf mx my mh mi mz na mk ml mm im bi translated"><a class="ae mo" href="https://en.wikipedia.org/wiki/GNU_GRUB" rel="noopener ugc nofollow" target="_blank"> GRUB </a>是一个开源的bootloader，可以将Linux内核作为常规的<a class="ae mo" href="https://en.wikipedia.org/wiki/Vmlinux" rel="noopener ugc nofollow" target="_blank"> ELF应用</a>来执行。<code class="fe nb nc nd ne b">/etc/default/grub</code>中的命令行标志是真实的<code class="fe nb nc nd ne b">argv</code>。</p></blockquote><p id="c922" class="pw-post-body-paragraph lu lv it lw b lx mt ju lz ma mu jx mc lh mw me mf ll my mh mi lp na mk ml mm im bi translated">正如我在以前的一篇博客文章中提到的，如果您计划点对点GPU通信，例如Tensorflow或PyTorch中的多GPU模型训练，禁用IOMMU至关重要。这不是什么秘密，经常出现在他们的GitHub问题中。</p><pre class="kj kk kl km gt nf ne ng nh aw ni bi"><span id="50b2" class="ky kz it ne b gy nj nk l nl nm">GRUB_CMDLINE_LINUX_DEFAULT="intel_iommu=off rcutree.rcu_idle_gp_delay=1"</span></pre><blockquote class="mp mq mr"><p id="b5a4" class="lu lv ms lw b lx mt ju lz ma mu jx mc mv mw me mf mx my mh mi mz na mk ml mm im bi translated">IOMMU代表<a class="ae mo" href="https://en.wikipedia.org/wiki/Input%E2%80%93output_memory_management_unit" rel="noopener ugc nofollow" target="_blank">输入输出内存管理单元</a>。在我们的上下文中，IOMMU与虚拟化<a class="ae mo" href="https://en.wikipedia.org/wiki/Direct_memory_access" rel="noopener ugc nofollow" target="_blank">直接内存访问</a> (DMA)相关。英特尔CPU以名为英特尔VT-d的“面向定向I/O的虚拟化技术”来实现它。</p></blockquote><p id="b366" class="pw-post-body-paragraph lu lv it lw b lx mt ju lz ma mu jx mc lh mw me mf ll my mh mi lp na mk ml mm im bi translated">我使用<a class="ae mo" href="https://git.busybox.net/busybox/tree/networking/udhcp" rel="noopener ugc nofollow" target="_blank"> udhcpd </a>在控制器中设置了内部网的静态DHCP租约，使用<code class="fe nb nc nd ne b">apt</code>很容易安装。这是我的<code class="fe nb nc nd ne b">/etc/udhcpd.conf</code>:</p><pre class="kj kk kl km gt nf ne ng nh aw ni bi"><span id="5531" class="ky kz it ne b gy nj nk l nl nm">start  192.168.2.2<br/>end  192.168.2.16<br/>interface eno2  # eno1 interface connects to the internet<br/>max_leases 32<br/>static_lease 0C:C4:7A:80:09:1F 192.168.2.2<br/>static_lease 0C:C4:7A:8A:18:87 192.168.2.12<br/>static_lease AC:1F:6B:20:F6:F9 192.168.2.3<br/>static_lease 0C:C4:7A:ED:F1:76 192.168.2.13<br/>static_lease AC:1F:6B:24:1E:FF 192.168.2.4<br/>static_lease AC:1F:6B:2F:98:52 192.168.2.14</span></pre><p id="77f3" class="pw-post-body-paragraph lu lv it lw b lx mt ju lz ma mu jx mc lh mw me mf ll my mh mi lp na mk ml mm im bi translated">您可能会注意到3台机器有6条记录。以<code class="fe nb nc nd ne b">.1x</code>结尾的是指<a class="ae mo" href="https://en.wikipedia.org/wiki/Intelligent_Platform_Management_Interface" rel="noopener ugc nofollow" target="_blank">IPMI</a>——每个塔中的独立计算单元，用于远程管理状态，例如，打开电源或查看屏幕。IPMI不需要特殊的设置:插入以太网电缆，访问HTTPS的网络接口。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nn"><img src="../Images/2869c314b49f649c9d1cd0847e94393b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VxGIapBg6OKNCvrCGKwd_Q.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">超微的IPMI截图。图片作者。</p></figure><p id="4563" class="pw-post-body-paragraph lu lv it lw b lx mt ju lz ma mu jx mc lh mw me mf ll my mh mi lp na mk ml mm im bi translated">我选择了一个基于<a class="ae mo" href="https://www.ansible.com/" rel="noopener ugc nofollow" target="_blank"> Ansible </a>的传统配置管理解决方案来管理机器。这没有Terraform的定制PXE那么花哨，但嘿，你不需要私人直升机去市区度假。Ansible的核心就像在预定义的主机上自动执行SSH命令一样简单。我用GPU塔名称填充了<code class="fe nb nc nd ne b">/etc/ansible/hosts</code>:</p><pre class="kj kk kl km gt nf ne ng nh aw ni bi"><span id="6214" class="ky kz it ne b gy nj nk l nl nm">[cluster]<br/>moscow<br/>madrid<br/>campos</span></pre><p id="9600" class="pw-post-body-paragraph lu lv it lw b lx mt ju lz ma mu jx mc lh mw me mf ll my mh mi lp na mk ml mm im bi translated">我用<code class="fe nb nc nd ne b">cluster.yml</code>来描述配置，并用<code class="fe nb nc nd ne b">ansible-playbook -K cluster.yml</code>来应用它。下面是一个示例，用于确保我的用户存在，并且可以在GitHub上使用我的带有指纹的私钥进行SSH:</p><pre class="kj kk kl km gt nf ne ng nh aw ni bi"><span id="6bef" class="ky kz it ne b gy nj nk l nl nm">---<br/>- hosts: cluster<br/>  become: yes<br/>  become_user: root<br/>  tasks:<br/>    - name: Group "docker"<br/>      group:<br/>        name: docker<br/>    - name: User "vadim"<br/>      user:<br/>        name: vadim<br/>        shell: /bin/bash<br/>        groups: adm,sudo,cdrom,docker,dip,plugdev,lxd<br/>        append: yes<br/>        uid: 1000<br/>        create_home: yes<br/>    - name: vadim's SSH keys<br/>      authorized_key:<br/>        user: vadim<br/>        state: present<br/>        key: <a class="ae mo" href="https://github.com/vmarkovtsev.keys" rel="noopener ugc nofollow" target="_blank">https://github.com/vmarkovtsev.keys</a></span></pre><p id="0120" class="pw-post-body-paragraph lu lv it lw b lx mt ju lz ma mu jx mc lh mw me mf ll my mh mi lp na mk ml mm im bi translated">当然，我需要在机器上运行一个SSH服务器。Ubuntu安装程序方便地允许在第一次启动前设置一个SSH服务器。</p><p id="e003" class="pw-post-body-paragraph lu lv it lw b lx mt ju lz ma mu jx mc lh mw me mf ll my mh mi lp na mk ml mm im bi translated">Ansible也是一个很好的增量解决方案。您不必在第0天强制配置所有内容。反而可以在有时间的时候和配置债打一场。</p><h2 id="0f15" class="ky kz it bd la lb lc dn ld le lf dp lg lh li lj lk ll lm ln lo lp lq lr ls lt bi translated">安装Kubernetes</h2><p id="6153" class="pw-post-body-paragraph lu lv it lw b lx ly ju lz ma mb jx mc lh md me mf ll mg mh mi lp mj mk ml mm im bi translated">我决定试试<a class="ae mo" href="https://k0sproject.io/" rel="noopener ugc nofollow" target="_blank"> k0s </a>。有几个优点吸引了我:</p><ul class=""><li id="1ae6" class="no np it lw b lx mt ma mu lh nq ll nr lp ns mm nt nu nv nw bi translated">轻松自举。下载一个没有外部依赖的大二进制，复制到每个节点，运行<code class="fe nb nc nd ne b">k0s install</code>，就大功告成了。</li><li id="dc04" class="no np it lw b lx nx ma ny lh nz ll oa lp ob mm nt nu nv nw bi translated">使用相同的<code class="fe nb nc nd ne b">k0s</code>命令轻松进行初始配置。</li><li id="99f5" class="no np it lw b lx nx ma ny lh nz ll oa lp ob mm nt nu nv nw bi translated">一组合理的内置电池，例如，<a class="ae mo" href="https://www.projectcalico.org/" rel="noopener ugc nofollow" target="_blank"> Calico </a>网络和<a class="ae mo" href="https://etcd.io/" rel="noopener ugc nofollow" target="_blank"> etcd </a>集群状态数据库。</li><li id="f6da" class="no np it lw b lx nx ma ny lh nz ll oa lp ob mm nt nu nv nw bi translated">香草Kubernetes，这样我就不用学习DevOps技术的另一个雪球。</li></ul><blockquote class="mp mq mr"><p id="9f09" class="lu lv ms lw b lx mt ju lz ma mu jx mc mv mw me mf mx my mh mi mz na mk ml mm im bi translated">Kubernetes是运行在一台或多台物理机器上的服务联盟。有两种操作模式:控制器和工人。管制员管理工人。两者都可以水平扩展，即增加实例的数量。如果用户不关心高可用性和故障转移，只产生一个控制器就足够了。</p></blockquote><p id="5218" class="pw-post-body-paragraph lu lv it lw b lx mt ju lz ma mu jx mc lh mw me mf ll my mh mi lp na mk ml mm im bi translated">官方文档目前缺乏细节，所以让我一步步地走完部署k0s的过程。在控制器上运行以下命令:</p><pre class="kj kk kl km gt nf ne ng nh aw ni bi"><span id="ca99" class="ky kz it ne b gy nj nk l nl nm">sudo k0s install controller</span></pre><p id="137b" class="pw-post-body-paragraph lu lv it lw b lx mt ju lz ma mu jx mc lh mw me mf ll my mh mi lp na mk ml mm im bi translated">该命令将创建<code class="fe nb nc nd ne b">/etc/systemd/system/k0scontroller.service</code>，它将k0s控制器实例打包成一个<a class="ae mo" href="https://www.digitalocean.com/community/tutorials/how-to-use-systemctl-to-manage-systemd-services-and-units" rel="noopener ugc nofollow" target="_blank"> systemd服务</a>，您可以方便地<code class="fe nb nc nd ne b">sudo systemctl start|stop|restart</code>。因此我们开始它:</p><pre class="kj kk kl km gt nf ne ng nh aw ni bi"><span id="c095" class="ky kz it ne b gy nj nk l nl nm">sudo systemctl start k0scontroller.service</span></pre><p id="ea29" class="pw-post-body-paragraph lu lv it lw b lx mt ju lz ma mu jx mc lh mw me mf ll my mh mi lp na mk ml mm im bi translated">下一个缺失的元素是<code class="fe nb nc nd ne b">kubectl</code>——人人都使用的Kubernetes命令行命令。我更喜欢用<code class="fe nb nc nd ne b">snap</code>安装:</p><pre class="kj kk kl km gt nf ne ng nh aw ni bi"><span id="a957" class="ky kz it ne b gy nj nk l nl nm">sudo snap install kubectl --classic</span></pre><p id="6908" class="pw-post-body-paragraph lu lv it lw b lx mt ju lz ma mu jx mc lh mw me mf ll my mh mi lp na mk ml mm im bi translated">k0s为kubectl提供了一个管理配置，又名<code class="fe nb nc nd ne b">KUBECONFIG</code>，我们将使用它来创建普通用户:</p><pre class="kj kk kl km gt nf ne ng nh aw ni bi"><span id="5a93" class="ky kz it ne b gy nj nk l nl nm">sudo cp /var/lib/k0s/pki/admin.conf .<br/>sudo chown $(whoami) admin.conf<br/>export KUBECONFIG=$(pwd)/admin.conf<br/>export clusterUser=$(whoami)<br/>kubectl create clusterrolebinding $clusterUser-admin-binding --clusterrole=admin --user=$clusterUser<br/>export KUBECONFIG=<br/>mkdir -p ~/.kube<br/>sudo k0s kubeconfig create --groups "system:masters" $clusterUser &gt; ~/.kube/config</span></pre><p id="3d8a" class="pw-post-body-paragraph lu lv it lw b lx mt ju lz ma mu jx mc lh mw me mf ll my mh mi lp na mk ml mm im bi translated">最后，我们必须将工作者加入集群的控制器。<a class="ae mo" href="https://docs.k0sproject.io/v0.11.0/k0s-multi-node/#create-a-join-token" rel="noopener ugc nofollow" target="_blank">文档钉住了那个程序</a>，我就不重复了。</p><p id="a0f3" class="pw-post-body-paragraph lu lv it lw b lx mt ju lz ma mu jx mc lh mw me mf ll my mh mi lp na mk ml mm im bi translated">在成功完成上述设置后，我发现了两个恼人的问题。</p><ol class=""><li id="b916" class="no np it lw b lx mt ma mu lh nq ll nr lp ns mm oc nu nv nw bi translated"><code class="fe nb nc nd ne b">kubectl logs</code>和<code class="fe nb nc nd ne b">kubectl exec</code>超时有80%几率。错误信息总是一样的:<code class="fe nb nc nd ne b">error dialing backend: dial tcp …: connection timed out</code>。</li><li id="eddd" class="no np it lw b lx nx ma ny lh nz ll oa lp ob mm oc nu nv nw bi translated"><code class="fe nb nc nd ne b">kube-system</code>名称空间中的系统盒无法引导，状态为“ImagePullBackOff”。<code class="fe nb nc nd ne b">kubectl describe pod</code>表示DNS解析超时，例如registry-1 . Docker . io—Docker容器注册表。然而，当我用<code class="fe nb nc nd ne b">kubectl get pod -n kube-system &lt;whatever&gt; -o yaml | kubectl replace --force -f -</code>手动重启pods时，有50%的时间图像都是成功的。</li></ol><blockquote class="mp mq mr"><p id="7b78" class="lu lv ms lw b lx mt ju lz ma mu jx mc mv mw me mf mx my mh mi mz na mk ml mm im bi translated"><code class="fe nb nc nd ne b">kubectl logs</code>直接连接到Kubernetes worker并请求指定pod的日志。如果pod包含多个集装箱，您必须指定要寻址的集装箱。该命令的工作方式类似于<code class="fe nb nc nd ne b">docker logs</code>。</p><p id="87c0" class="lu lv ms lw b lx mt ju lz ma mu jx mc mv mw me mf mx my mh mi mz na mk ml mm im bi translated"><code class="fe nb nc nd ne b">kubectl exec</code>直接连接到Kubernetes工作器，并在指定的pod中执行任意命令。同样，您必须指定pod运行多个容器。该命令的工作方式类似于<code class="fe nb nc nd ne b">docker exec</code>。</p></blockquote><p id="f074" class="pw-post-body-paragraph lu lv it lw b lx mt ju lz ma mu jx mc lh mw me mf ll my mh mi lp na mk ml mm im bi translated">那些问题在普通的工作机器上没有重现。Kubernetes官方文档中的<a class="ae mo" href="https://kubernetes.io/docs/tasks/administer-cluster/dns-debugging-resolution/" rel="noopener ugc nofollow" target="_blank">“调试DNS解析”中的建议没有帮助。花了几个小时调试之后，我发现:</a></p><ol class=""><li id="5639" class="no np it lw b lx mt ma mu lh nq ll nr lp ns mm oc nu nv nw bi translated"><code class="fe nb nc nd ne b">kubectl logs</code>和<code class="fe nb nc nd ne b">kubectl exec</code>超时，因为kubelet-s监听了错误的网络接口。每个GPU塔有两个物理以太网插座，网络在<code class="fe nb nc nd ne b">eno2</code>连接，而<code class="fe nb nc nd ne b">eno1</code>保持未配置。Kubernetes无法找出主要接口和Calico的路由中断。我仍然不完全理解为什么这些命令有时会起作用。我通过执行以下命令解决了这个问题:</li></ol><pre class="kj kk kl km gt nf ne ng nh aw ni bi"><span id="0261" class="ky kz it ne b gy nj nk l nl nm">kubectl set env daemonset/calico-node -n kube-system IP_AUTODETECTION_METHOD=interface=eno2</span></pre><p id="cfa3" class="pw-post-body-paragraph lu lv it lw b lx mt ju lz ma mu jx mc lh mw me mf ll my mh mi lp na mk ml mm im bi translated">2.在控制器上运行(黑客的)DNS服务器是个坏主意。虽然我没有提到，但我最初使用以下服务配置将DNS转发到<a class="ae mo" href="https://wiki.archlinux.org/index.php/Systemd-resolved" rel="noopener ugc nofollow" target="_blank"> systemd-resolved </a>:</p><pre class="kj kk kl km gt nf ne ng nh aw ni bi"><span id="4067" class="ky kz it ne b gy nj nk l nl nm">[Unit]<br/>Description = Forward DNS lookups from 192.168.2.0/24 to 127.0.0.53<br/>After = network.target</span><span id="a122" class="ky kz it ne b gy od nk l nl nm">[Service]<br/>ExecStart = /usr/bin/socat UDP-LISTEN:53,fork,reuseaddr,bind=192.168.2.1 UDP:127.0.0.53:53</span><span id="4d90" class="ky kz it ne b gy od nk l nl nm">[Install]<br/>WantedBy = multi-user.target</span></pre><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oe of l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">实用开发者合理化你糟糕的黑客工作。</p></figure><p id="9cc3" class="pw-post-body-paragraph lu lv it lw b lx mt ju lz ma mu jx mc lh mw me mf ll my mh mi lp na mk ml mm im bi translated">systemd-resolved是Ubuntu内置的DNS中间件，所以我偷工减料，搬起石头砸自己的脚。我用<a class="ae mo" href="http://www.dest-unreach.org/socat/" rel="noopener ugc nofollow" target="_blank"> socat </a>做的黑客工作出奇的好，除了<a class="ae mo" href="https://containerd.io/" rel="noopener ugc nofollow" target="_blank">容器</a>。在<code class="fe nb nc nd ne b">/etc/hosts</code>中，我打算避免配置全功能DNS服务器和硬编码IP。</p><blockquote class="mp mq mr"><p id="4a98" class="lu lv ms lw b lx mt ju lz ma mu jx mc mv mw me mf mx my mh mi mz na mk ml mm im bi translated">容器运行时是提取和执行容器映像的引擎。Kubernetes过去默认使用Docker运行时。<a class="ae mo" href="https://kubernetes.io/blog/2020/12/02/dont-panic-kubernetes-and-docker/" rel="noopener ugc nofollow" target="_blank">最近已经切换到了CRI和Docker运行时本身所基于的较低级别的containerd运行时</a>。互联网上的大多数GPU集成手册都假定了Docker运行时，因此不再适用于现代的Kubernetes。</p></blockquote><p id="6afc" class="pw-post-body-paragraph lu lv it lw b lx mt ju lz ma mu jx mc lh mw me mf ll my mh mi lp na mk ml mm im bi translated">还有最后一件事需要配置:NVIDIA GPU调度。它实际上的意思是当吊舱规格像这样</p><pre class="kj kk kl km gt nf ne ng nh aw ni bi"><span id="1aba" class="ky kz it ne b gy nj nk l nl nm">metadata:<br/>  spec:<br/>    ...<br/>    containers:<br/>    - ...<br/>      limits:<br/>        nvidia.com/gpu: 2</span></pre><p id="99a3" class="pw-post-body-paragraph lu lv it lw b lx mt ju lz ma mu jx mc lh mw me mf ll my mh mi lp na mk ml mm im bi translated">然后Kubernetes确保<code class="fe nb nc nd ne b">/dev/nvidia0</code>和<code class="fe nb nc nd ne b">/dev/nvidia1</code>存在于容器内部。</p><p id="31b8" class="pw-post-body-paragraph lu lv it lw b lx mt ju lz ma mu jx mc lh mw me mf ll my mh mi lp na mk ml mm im bi translated"><a class="ae mo" href="https://docs.k0sproject.io/v0.11.0/containerd_config/#using-custom-nvidia-container-runtime" rel="noopener ugc nofollow" target="_blank"> k0s在文档</a>中简单提到了如何启用GPU，但实际过程要复杂一点。有一个<a class="ae mo" href="https://dev.to/mweibel/add-nvidia-gpu-support-to-k3s-with-containerd-4j17" rel="noopener ugc nofollow" target="_blank">迈克尔·韦贝尔</a>关于k3s 中解决任务的很棒的帖子，k0s应该也有类似的调整。</p><p id="3962" class="pw-post-body-paragraph lu lv it lw b lx mt ju lz ma mu jx mc lh mw me mf ll my mh mi lp na mk ml mm im bi translated">首先，在每个GPU塔上安装NVIDIA驱动程序。NVIDIA提供了一个可行的角色来实现自动化。在<code class="fe nb nc nd ne b">rc.local</code>中添加<code class="fe nb nc nd ne b">nvidia-smi</code>，在引导时创建设备。按照NVIDIA文档通过<a class="ae mo" href="https://nvidia.github.io/nvidia-container-runtime/" rel="noopener ugc nofollow" target="_blank">安装<code class="fe nb nc nd ne b">nvidia-container-runtime</code>。按照</a><a class="ae mo" href="https://josephb.org/blog/containerd-nvidia/" rel="noopener ugc nofollow" target="_blank">约瑟夫·borġ's博客文章</a>中的描述，在worker节点上修补containerd配置，除了您应该用<code class="fe nb nc nd ne b">/etc/k0s/containerd.toml</code>替换<code class="fe nb nc nd ne b">/etc/containerd/config.toml</code>并如下更改头:</p><pre class="kj kk kl km gt nf ne ng nh aw ni bi"><span id="1c32" class="ky kz it ne b gy nj nk l nl nm">version = 2<br/>root = "/var/lib/k0s/containerd"<br/>state = "/run/k0s/containerd"<br/>...</span><span id="c49d" class="ky kz it ne b gy od nk l nl nm">[grpc]<br/>  address = "/run/k0s/containerd.sock"</span></pre><p id="d4f0" class="pw-post-body-paragraph lu lv it lw b lx mt ju lz ma mu jx mc lh mw me mf ll my mh mi lp na mk ml mm im bi translated">然后，从<code class="fe nb nc nd ne b">kubectl version</code>开始计算Kubernetes服务器版本，比如说1.20，并应用Google的daemonset，以便集群学习新的资源类型<code class="fe nb nc nd ne b">nvidia.com/gpu</code>:</p><pre class="kj kk kl km gt nf ne ng nh aw ni bi"><span id="fb7a" class="ky kz it ne b gy nj nk l nl nm">kubectl apply -f <a class="ae mo" href="https://raw.githubusercontent.com/kubernetes/kubernetes/release-1.20/cluster/addons/device-plugins/nvidia-gpu/daemonset.yaml" rel="noopener ugc nofollow" target="_blank">https://raw.githubusercontent.com/kubernetes/kubernetes/release-1.20/cluster/addons/device-plugins/nvidia-gpu/daemonset.yaml</a></span></pre><p id="3e34" class="pw-post-body-paragraph lu lv it lw b lx mt ju lz ma mu jx mc lh mw me mf ll my mh mi lp na mk ml mm im bi translated">然后，您必须将工作节点标记为NVIDIA友好的:</p><pre class="kj kk kl km gt nf ne ng nh aw ni bi"><span id="a386" class="ky kz it ne b gy nj nk l nl nm">kubectl label nodes --all cloud.google.com/gke-accelerator=true</span></pre><p id="6a8f" class="pw-post-body-paragraph lu lv it lw b lx mt ju lz ma mu jx mc lh mw me mf ll my mh mi lp na mk ml mm im bi translated">瞧啊。用<code class="fe nb nc nd ne b">kubectl get daemonset -n kube-system nvidia-gpu-device-plugin</code>检查状态。它应该输出<em class="ms"> n </em> / <em class="ms"> n </em>，其中<em class="ms"> n </em>是GPU塔的数量:</p><pre class="kj kk kl km gt nf ne ng nh aw ni bi"><span id="fd79" class="ky kz it ne b gy nj nk l nl nm">NAME                       DESIRED   CURRENT   READY   UP-TO-DATE<br/>nvidia-gpu-device-plugin   3         3         3       3</span></pre><blockquote class="mp mq mr"><p id="7cef" class="lu lv ms lw b lx mt ju lz ma mu jx mc mv mw me mf mx my mh mi mz na mk ml mm im bi translated">您可以通过SSH访问一个worker、<code class="fe nb nc nd ne b">sudo apt install containerd</code>、<code class="fe nb nc nd ne b">sudo systemctl disable containerd.service</code> (Kubernetes运行自己的)，并使用<code class="fe nb nc nd ne b">sudo ctr --address /run/k0s/containerd.sock -n k8s.io</code>发出命令来访问k0s的gory containerd内部。例如，下面的命令将列出所有拉出的图像:<code class="fe nb nc nd ne b">sudo ctr --address /run/k0s/containerd.sock -n k8s.io image list</code>。界面类似于<code class="fe nb nc nd ne b">docker</code>。</p></blockquote><h2 id="61d2" class="ky kz it bd la lb lc dn ld le lf dp lg lh li lj lk ll lm ln lo lp lq lr ls lt bi translated">分布式文件系统</h2><p id="bbfe" class="pw-post-body-paragraph lu lv it lw b lx ly ju lz ma mb jx mc lh md me mf ll mg mh mi lp mj mk ml mm im bi translated"><a class="ae mo" href="https://www.kubeflow.org/docs/started/k8s/kfctl-istio-dex/" rel="noopener ugc nofollow" target="_blank"> Kubeflow的文档详细介绍了现有Kubernetes集群的安装。他们多次强调缺省设置的必要性，原因是:没有共享文件系统=没有ML。定义了在Kubernetes中的pods </a>中存储和挂载持久卷的后端。该类在集群中统一工作，以便一个pod中的文件更改可以被另一个pod看到。我们必须返回到OS和Kubernetes配置。</p><p id="f990" class="pw-post-body-paragraph lu lv it lw b lx mt ju lz ma mu jx mc lh mw me mf ll my mh mi lp na mk ml mm im bi translated">我决定部署<a class="ae mo" href="https://github.com/rancher/local-path-provisioner" rel="noopener ugc nofollow" target="_blank">牧场主的本地路径供应器</a>和一个很穷但很骄傲的人的基于NFS十字坐骑和<a class="ae mo" href="https://github.com/trapexit/mergerfs" rel="noopener ugc nofollow" target="_blank"> mergerFS </a>的分布式FS。我将从DFS开始。</p><blockquote class="mp mq mr"><p id="962d" class="lu lv ms lw b lx mt ju lz ma mu jx mc mv mw me mf mx my mh mi mz na mk ml mm im bi translated">NFS代表<a class="ae mo" href="https://en.wikipedia.org/wiki/Network_File_System" rel="noopener ugc nofollow" target="_blank">网络文件系统</a>，由Linux、macOS和Windows 10原生支持。这是一个比博客作者还要古老的协议。不要让它的年龄欺骗了你:NFS通常是快速和可靠的。</p></blockquote><p id="66df" class="pw-post-body-paragraph lu lv it lw b lx mt ju lz ma mu jx mc lh mw me mf ll my mh mi lp na mk ml mm im bi translated">我以前有过患<a class="ae mo" href="https://www.gluster.org/" rel="noopener ugc nofollow" target="_blank"> GlusterFS </a>的经验。虽然它以前工作稳定，但我对它的性能并不满意。在类似的集群配置中，写入、读取和删除大量小文件(例如ImageNet)所需的时间是不可接受的。当时，GlusterFS是由出色的DevOps工程师<a class="og oh ep" href="https://medium.com/u/1254fe362752?source=post_page-----6865ebcde032--------------------------------" rel="noopener" target="_blank"> Maartje Eyskens </a>和<a class="og oh ep" href="https://medium.com/u/f78752bca73e?source=post_page-----6865ebcde032--------------------------------" rel="noopener" target="_blank"> Rafael Porres Molina </a>部署的，因此糟糕的性能应该不是由糟糕的配置造成的。</p><p id="5eda" class="pw-post-body-paragraph lu lv it lw b lx mt ju lz ma mu jx mc lh mw me mf ll my mh mi lp na mk ml mm im bi translated">所以我设计了一个机器间NFS安装的全连接图。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oi"><img src="../Images/3f8e12f0c45cff26ed342cf57448e87c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iE2qlr84IXx1Fvqh5wdhgg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">NFS全对全连接方案。图片作者。</p></figure><p id="38e0" class="pw-post-body-paragraph lu lv it lw b lx mt ju lz ma mu jx mc lh mw me mf ll my mh mi lp na mk ml mm im bi translated">节点X通过NFS导出<code class="fe nb nc nd ne b">/data/X</code>,并通过NFS挂载剩余的<code class="fe nb nc nd ne b">/data</code>子目录。这还不是一个共享的文件系统:我们需要将所有的子目录联合在一起。Linux中存在几种合并目录或映像的文件系统，比如OverlayFS——Docker用它来堆叠容器层。我们的目标与Docker的不同，因为我们没有层级，所有4个子<code class="fe nb nc nd ne b">/data</code>都是等价的。mergerFS 是一个很好的FUSE(不需要内核模块)工具来达到这个目标。以下是项目自述文件中的一个示例:</p><pre class="kj kk kl km gt nf ne ng nh aw ni bi"><span id="305b" class="ky kz it ne b gy nj nk l nl nm">A         +      B        =       C<br/>/disk1           /disk2           /merged<br/>|                |                |<br/>+-- /dir1        +-- /dir1        +-- /dir1<br/>|   |            |   |            |   |<br/>|   +-- file1    |   +-- file2    |   +-- file1<br/>|                |   +-- file3    |   +-- file2<br/>+-- /dir2        |                |   +-- file3<br/>|   |            +-- /dir3        |<br/>|   +-- file4        |            +-- /dir2<br/>|                     +-- file5   |   |<br/>+-- file6                         |   +-- file4<br/>                                  |<br/>                                  +-- /dir3<br/>                                  |   |<br/>                                  |   +-- file5<br/>                                  |<br/>                                  +-- file6</span></pre><p id="7bba" class="pw-post-body-paragraph lu lv it lw b lx mt ju lz ma mu jx mc lh mw me mf ll my mh mi lp na mk ml mm im bi translated">mergerFS支持如何执行文件系统操作的各种策略，它称之为策略。当用户创建一个目录时，它可以出现在所有合并的<code class="fe nb nc nd ne b">/data</code> -s中，也可以只出现在其中一个中；当用户创建一个文件时，它可以去本地的<code class="fe nb nc nd ne b">/data</code>或者有最多空闲空间的地方。例如，这就是我如何在<code class="fe nb nc nd ne b">moscow</code>上配置<code class="fe nb nc nd ne b">/etc/fstab</code>以在<code class="fe nb nc nd ne b">/dfs</code>挂载合并的目录(为了清楚起见，制表符被替换为新行):</p><pre class="kj kk kl km gt nf ne ng nh aw ni bi"><span id="3a22" class="ky kz it ne b gy nj nk l nl nm">/data/moscow:/data/campos:/data/madrid:/data/controller<br/>/dfs<br/>fuse.mergerfs allow_other,use_ino,cache.files=partial,dropcacheonclose=true,category.create=all</span></pre><blockquote class="mp mq mr"><p id="09f0" class="lu lv ms lw b lx mt ju lz ma mu jx mc mv mw me mf mx my mh mi mz na mk ml mm im bi translated"><code class="fe nb nc nd ne b">/etc/fstab</code>定义主机上的本地文件系统。操作系统在启动时挂载列出的项目。</p></blockquote><p id="cd5d" class="pw-post-body-paragraph lu lv it lw b lx mt ju lz ma mu jx mc lh mw me mf ll my mh mi lp na mk ml mm im bi translated">当我们在<code class="fe nb nc nd ne b">moscow</code>上写一个新文件时，我们在本地<code class="fe nb nc nd ne b">/data/moscow</code>上操作，其他机器通过NFS访问它。另一方面，目录结构到处复制。</p><p id="8b52" class="pw-post-body-paragraph lu lv it lw b lx mt ju lz ma mu jx mc lh mw me mf ll my mh mi lp na mk ml mm im bi translated">让我们考虑一下这样的DFS的利弊。</p><p id="d60e" class="pw-post-body-paragraph lu lv it lw b lx mt ju lz ma mu jx mc lh mw me mf ll my mh mi lp na mk ml mm im bi translated">优点:</p><ul class=""><li id="5434" class="no np it lw b lx mt ma mu lh nq ll nr lp ns mm nt nu nv nw bi translated">不可能腐败。文件作为一个整体存储在节点上，而不是分散的块中。</li><li id="388d" class="no np it lw b lx nx ma ny lh nz ll oa lp ob mm nt nu nv nw bi translated">如果我们使用本地文件，我们的性能与本机磁盘IOPS相当。</li><li id="1f5c" class="no np it lw b lx nx ma ny lh nz ll oa lp ob mm nt nu nv nw bi translated">外部文件的对等NFS读取也是有性能的。</li><li id="81a1" class="no np it lw b lx nx ma ny lh nz ll oa lp ob mm nt nu nv nw bi translated">快速文件删除。</li></ul><p id="0df7" class="pw-post-body-paragraph lu lv it lw b lx mt ju lz ma mu jx mc lh mw me mf ll my mh mi lp na mk ml mm im bi translated">缺点:</p><ul class=""><li id="7a23" class="no np it lw b lx mt ma mu lh nq ll nr lp ns mm nt nu nv nw bi translated">如果某台机器坏了，我们就无法访问它的文件。但是，当我们有4台机器时，这不是问题。</li><li id="29db" class="no np it lw b lx nx ma ny lh nz ll oa lp ob mm nt nu nv nw bi translated">读写非本地存储的大文件比在真正的DFS-s上运行要慢，DFS-s将文件块分散在各个节点上，因此当您读写它们时，您可以并行聚合数据。我们可以通过本地复制大文件来缓解这个问题，老实说，如果磁盘有足够的空闲空间，这是典型ML/DL任务的最佳方法。</li></ul><blockquote class="mp mq mr"><p id="ae97" class="lu lv ms lw b lx mt ju lz ma mu jx mc mv mw me mf mx my mh mi mz na mk ml mm im bi translated">NFS挂载是用<code class="fe nb nc nd ne b">nofail,soft,retry=1,timeo=10</code>配置的，这样内核就不会无限期地等待一个断开的共享重新出现，从而阻塞用户空间进程并引发混乱和破坏。</p></blockquote><p id="a0b2" class="pw-post-body-paragraph lu lv it lw b lx mt ju lz ma mu jx mc lh mw me mf ll my mh mi lp na mk ml mm im bi translated">剩下的工作是在Kubernetes上部署本地路径<code class="fe nb nc nd ne b">StorageClass</code>，这样工作人员就可以在<code class="fe nb nc nd ne b">/dfs</code>上持久化数据:</p><pre class="kj kk kl km gt nf ne ng nh aw ni bi"><span id="7466" class="ky kz it ne b gy nj nk l nl nm">wget <a class="ae mo" href="https://raw.githubusercontent.com/rancher/local-path-provisioner/master/deploy/local-path-storage.yaml" rel="noopener ugc nofollow" target="_blank">https://raw.githubusercontent.com/rancher/local-path-provisioner/master/deploy/local-path-storage.yaml</a><br/>sed -i 's/\/opt\/local-path-provisioner/\/dfs/g' local-path-storage.yaml<br/>kubectl apply -f local-path-storage.yaml<br/>kubectl patch storageclass local-path -p '{"metadata": {"annotations":{"storageclass.kubernetes.io/is-default-class":"true"}}}'</span></pre><p id="4822" class="pw-post-body-paragraph lu lv it lw b lx mt ju lz ma mu jx mc lh mw me mf ll my mh mi lp na mk ml mm im bi translated">完成这些操作后，您应该会看到类似的内容:</p><pre class="kj kk kl km gt nf ne ng nh aw ni bi"><span id="fe0d" class="ky kz it ne b gy nj nk l nl nm">$ kubectl get storageclass<br/>NAME                   PROVISIONER             RECLAIMPOLICY   VOLUMEBINDINGMODE      ALLOWVOLUMEEXPANSION   AGE<br/>local-path (default)   rancher.io/local-path   Delete          WaitForFirstConsumer   false                  13d</span></pre><h2 id="4034" class="ky kz it bd la lb lc dn ld le lf dp lg lh li lj lk ll lm ln lo lp lq lr ls lt bi translated">安装Kubeflow</h2><p id="8507" class="pw-post-body-paragraph lu lv it lw b lx ly ju lz ma mb jx mc lh md me mf ll mg mh mi lp mj mk ml mm im bi translated">理论上，如果所有先决条件都满足，部署Kubeflow就很容易:</p><pre class="kj kk kl km gt nf ne ng nh aw ni bi"><span id="a894" class="ky kz it ne b gy nj nk l nl nm">sudo mkdir /opt/my-kubeflow &amp;&amp; cd /opt/my-kubeflow<br/>chown $(whoami) .<br/>export BASE_DIR=/opt<br/>export KF_NAME=my-kubeflow<br/>export KF_DIR=${BASE_DIR}/${KF_NAME}</span><span id="91bb" class="ky kz it ne b gy od nk l nl nm"># Download kfctl from <a class="ae mo" href="https://github.com/kubeflow/kfctl/releases" rel="noopener ugc nofollow" target="_blank">https://github.com/kubeflow/kfctl/releases</a></span><span id="17c4" class="ky kz it ne b gy od nk l nl nm"># Suppose that the version is 1.2<br/>wget <a class="ae mo" href="https://raw.githubusercontent.com/kubeflow/manifests/v1.2-branch/kfdef/kfctl_istio_dex.v1.2.0.yaml" rel="noopener ugc nofollow" target="_blank">https://raw.githubusercontent.com/kubeflow/manifests/v1.2-branch/kfdef/kfctl_istio_dex.v1.2.0.yaml</a> -o kfctl_istio_dex.yaml<br/>kfctl apply -V -f kfctl_istio_dex.yaml</span></pre><p id="83f5" class="pw-post-body-paragraph lu lv it lw b lx mt ju lz ma mu jx mc lh mw me mf ll my mh mi lp na mk ml mm im bi translated">不幸的是，练习并不顺利。<code class="fe nb nc nd ne b">kfctl</code>打印出一切都部署好了，只是没有。我和katib 遇到了一个<a class="ae mo" href="https://github.com/kubeflow/katib/issues/1415" rel="noopener ugc nofollow" target="_blank">棘手的问题。Katib是Google针对Kubernetes/Kubeflow的AutoML解决方案。例如，它可以进行超优化。</a></p><pre class="kj kk kl km gt nf ne ng nh aw ni bi"><span id="2e5f" class="ky kz it ne b gy nj nk l nl nm">$ kubectl get pods -n kubeflow</span><span id="1e21" class="ky kz it ne b gy od nk l nl nm">NAME                     READY   STATUS             RESTARTS<br/>katib-db-manager-...     0/1     CrashLoopBackOff   235<br/>katib-mysql-...          0/1     <!-- -->CrashLoopBackOff<!-- -->   24</span></pre><p id="e974" class="pw-post-body-paragraph lu lv it lw b lx mt ju lz ma mu jx mc lh mw me mf ll my mh mi lp na mk ml mm im bi translated">日志消息信息不多，所以我花了几个小时才明白失败的原因。我应该简要回顾一下Kubernetes中的就绪性和活性探测。</p><blockquote class="mp mq mr"><p id="03f4" class="lu lv ms lw b lx mt ju lz ma mu jx mc mv mw me mf mx my mh mi mz na mk ml mm im bi translated">Kubernetes通过执行就绪探测来了解发射的吊舱是否准备好工作。它定期运行一个shell脚本并检查退出代码。如果退出代码为0，Kubernetes会将pod标记为就绪。</p><p id="673d" class="lu lv ms lw b lx mt ju lz ma mu jx mc mv mw me mf mx my mh mi mz na mk ml mm im bi translated">Kubernetes通过执行活性探测来了解启动的pod是否起作用(“活性”)。它定期运行一个shell脚本并检查退出代码。如果退出代码不是0，Kubernetes将重新启动pod。</p></blockquote><p id="22c1" class="pw-post-body-paragraph lu lv it lw b lx mt ju lz ma mu jx mc lh mw me mf ll my mh mi lp na mk ml mm im bi translated">katib-mysql在第一次启动时初始化一个mysql数据库。该配置将活动探测延迟30秒。偏偏30秒太少，吊舱无法按时完成初始化。Kubernetes杀了它。不幸的是，DB初始化是不可抢占的，重启的pod再次失败。</p><p id="0a9c" class="pw-post-body-paragraph lu lv it lw b lx mt ju lz ma mu jx mc lh mw me mf ll my mh mi lp na mk ml mm im bi translated">该问题的解决方案是增加活性探针的<code class="fe nb nc nd ne b">initialDelaySeconds</code>。</p><pre class="kj kk kl km gt nf ne ng nh aw ni bi"><span id="6e20" class="ky kz it ne b gy nj nk l nl nm">KUBE_EDITOR=nano kubectl edit deployment katib-mysql -n kubeflow<br/># locate livenessProbe and set initialDelaySeconds to 300<br/># save and exit</span><span id="cc60" class="ky kz it ne b gy od nk l nl nm"># Important! Delete the old screwed database, e.g.<br/># pvc-25dc2b81-9873-430f-8bc4-365fe5ff0357_kubeflow_katib-mysql</span><span id="489d" class="ky kz it ne b gy od nk l nl nm">kubectl delete pod -n kubeflow katib-mysql-...</span></pre><p id="5852" class="pw-post-body-paragraph lu lv it lw b lx mt ju lz ma mu jx mc lh mw me mf ll my mh mi lp na mk ml mm im bi translated">最后，我们必须设置草稿端口转发来访问web界面。我写了一个systemd服务:</p><pre class="kj kk kl km gt nf ne ng nh aw ni bi"><span id="6fc6" class="ky kz it ne b gy nj nk l nl nm">[Unit]<br/>Description=kubeflow port forwarding<br/>ConditionFileIsExecutable=/snap/bin/kubectl</span><span id="2c9c" class="ky kz it ne b gy od nk l nl nm">After=k0sserver.service<br/>Wants=k0sserver.service</span><span id="333b" class="ky kz it ne b gy od nk l nl nm">[Service]<br/>StartLimitInterval=5<br/>StartLimitBurst=10<br/>Environment="KUBECONFIG=/var/lib/k0s/pki/admin.conf"<br/>ExecStart=/snap/bin/kubectl port-forward svc/istio-ingressgateway -n istio-system 8080:80</span><span id="41fc" class="ky kz it ne b gy od nk l nl nm">...</span></pre><blockquote class="mp mq mr"><p id="989c" class="lu lv ms lw b lx mt ju lz ma mu jx mc mv mw me mf mx my mh mi mz na mk ml mm im bi translated">Istio是Kubeflow的一部分，负责组织服务网络和管理流量。例如，Kuberflow使用Istio以受控和统一的方式将内部服务公开给外部，这样您就不必使用<code class="fe nb nc nd ne b">kubectl port-forward</code>单独的pod。</p></blockquote><p id="54e1" class="pw-post-body-paragraph lu lv it lw b lx mt ju lz ma mu jx mc lh mw me mf ll my mh mi lp na mk ml mm im bi translated">你好，库伯弗洛！</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oj"><img src="../Images/826724cceae9b863fdd5b3b39f688fda.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QBI59Nz8U8kMFfKh1CZxNg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">Kuberflow起始页。图片作者。</p></figure><h2 id="ea53" class="ky kz it bd la lb lc dn ld le lf dp lg lh li lj lk ll lm ln lo lp lq lr ls lt bi translated">包裹</h2><p id="4ce9" class="pw-post-body-paragraph lu lv it lw b lx ly ju lz ma mb jx mc lh md me mf ll mg mh mi lp mj mk ml mm im bi translated">我试图从零开始勾勒出建立一个裸机集群来做ML的过程。这绝对是一种痛苦。我必须解决大量的问题，我不能在帖子中描述所有的问题，因为否则它会爆炸。然而，对于一个经验丰富的DevOps工程师来说，这并不复杂。2021年的MLOps肯定比几年前容易。然而，如果你没有做好心理准备来打开现代DevOps技术的潘多拉魔盒——因为你不会在不了解自己在做什么的情况下解决不可避免的问题——我建议忘记裸机，继续在云中飞行。</p><p id="deec" class="pw-post-body-paragraph lu lv it lw b lx mt ju lz ma mu jx mc lh mw me mf ll my mh mi lp na mk ml mm im bi translated">正如一些人意识到的那样，我在走了很多捷径。一个专门的DevOps团队将重建集群，以充分利用Kubernetes和底层硬件。我估计我的努力是2周的全职工作。另一方面，配置越先进，对维护人员的依赖性就越强，暴露组件不当行为和损坏的面就越广。</p><p id="9640" class="pw-post-body-paragraph lu lv it lw b lx mt ju lz ma mu jx mc lh mw me mf ll my mh mi lp na mk ml mm im bi translated">请在Twitter上订阅<a class="ae mo" href="https://twitter.com/vadimlearning" rel="noopener ugc nofollow" target="_blank"> @vadimlearning </a>，这样你就不会错过我的下一篇博文。非常感谢<a class="ae mo" href="https://twitter.com/Miau_DB" rel="noopener ugc nofollow" target="_blank"> @Miau_DB </a>的资助、支持和宝贵意见；<a class="ae mo" href="https://twitter.com/aina_fiol" rel="noopener ugc nofollow" target="_blank">@艾纳_菲奥</a>进行校对。我们正在进行咨询，如果感兴趣，请发邮件到fragile.tech的guillem。</p></div></div>    
</body>
</html>