<html>
<head>
<title>The Four Policy Classes of Reinforcement Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">强化学习的四个策略类别</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/the-four-policy-classes-of-reinforcement-learning-38185daa6c8a?source=collection_archive---------12-----------------------#2021-05-11">https://towardsdatascience.com/the-four-policy-classes-of-reinforcement-learning-38185daa6c8a?source=collection_archive---------12-----------------------#2021-05-11</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="86ec" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsdatascience.com/tagged/thoughts-and-theory" rel="noopener" target="_blank">思想和理论</a></h2><div class=""/><div class=""><h2 id="9b67" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">强化学习解决策略的综合分类</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/0bd0609fe2e763ca15a6cbade33cb567.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OtZdV6HfCuHygPvMchJOzA.jpeg"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">图片由<a class="ae lh" href="https://pixabay.com/nl/users/hans-2/?utm_source=link-attribution&amp;amp;utm_medium=referral&amp;amp;utm_campaign=image&amp;amp;utm_content=9241" rel="noopener ugc nofollow" target="_blank">汉斯·布拉克斯梅尔</a>通过<a class="ae lh" href="https://pixabay.com/nl/?utm_source=link-attribution&amp;amp;utm_medium=referral&amp;amp;utm_campaign=image&amp;amp;utm_content=9241" rel="noopener ugc nofollow" target="_blank">皮克斯贝</a>拍摄</p></figure><p id="e3af" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">强化学习(RL)的政策笼罩在某种神秘之中。简单地说，策略<em class="me"> π: s →a </em>是返回问题可行行动的任何函数。不多也不少。例如，你可以简单地采取第一个想到的行动，随机选择一个行动，或者运行启发式。然而，RL的特别之处在于，我们主动预测决策的下游影响，并从我们的观察中学习；因此，我们希望我们的政策中有一些智慧。在他的顺序决策框架[1]中，Warren Powell认为RL有四个政策类别。这四类技术被广泛应用于各个领域，但还没有得到普遍认可。本文将对这种解决方案策略的分类做一个简短的介绍，这无疑是不完整的。</p></div><div class="ab cl mf mg hx mh" role="separator"><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk"/></div><div class="im in io ip iq"><h1 id="d953" class="mm mn it bd mo mp mq mr ms mt mu mv mw ki mx kj my kl mz km na ko nb kp nc nd bi translated">求解MDP模型</h1><p id="f53d" class="pw-post-body-paragraph li lj it lk b ll ne kd ln lo nf kg lq lr ng lt lu lv nh lx ly lz ni mb mc md im bi translated">在继续强化学习之前，让我们先稍微回忆一下解析解。通常，我们的目标是将RL问题公式化为马尔可夫决策过程(MDP)模型。如果我们靠近MDP，强化学习的目标将是求解相应的贝尔曼方程组，从而找到最优策略<em class="me"> π* </em>:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi nj"><img src="../Images/20bd4c3af4d667bfba582967f597791d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6CkeBvwplvlu73_Ex-D3pw.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">MDP的贝尔曼方程。找到最优策略<em class="nk"> π* </em>产生最优值函数<em class="nk"> V(s) </em>，反之亦然。</p></figure><p id="a62c" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">但是我们实际上并不需要贝尔曼方程。最终，我们只是试图最大化一个累积的回报；最优策略正是这样做的。它还消除了考虑价值函数<em class="me">【v^π*(s'】</em>的需要，这是我们只在四个策略类之一中做的事情。因此，我们可以将我们的目标陈述如下:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi nl"><img src="../Images/c73890a5dd7a2083eca1bc4fbed29be4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1272/format:webp/1*ga6LVib8njKPCmphl6272w.png"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">时间范围内的报酬函数。最优策略<em class="nk"> π*使累积报酬最大化。</em></p></figure><p id="7903" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">要解决MDP模型的最优性，基本上有两种方法:(一)政策迭代和(二)价值迭代。<em class="me">策略迭代</em>修复策略，计算相应的策略值，然后使用新值更新策略。该算法在这些步骤之间迭代，直到策略保持稳定。<em class="me">价值迭代</em>实际上依赖于非常相似的步骤(见下图)，但是旨在直接最大化价值函数，并且仅在之后更新策略。注意，找到最优值函数等同于找到最优策略；任一个都足以求解贝尔曼方程组。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi nm"><img src="../Images/04f6a33a9ee4580ad940bec4a1672eae.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VqOXOqYxpwRTXGDJGjOgLg.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">策略迭代(左)和价值迭代(右)的比较。请注意策略迭代的迭代特性和值迭代中的最大值运算符。改编自萨顿&amp;巴尔托[2]</p></figure><p id="752d" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">大多数(如果不是全部的话)RL算法要么基于策略迭代，要么基于值迭代(或者两者的组合)。由于部署的模拟方法通常不能保证找到最优策略，在RL中我们分别谈到策略<em class="me">近似值</em>和值<em class="me">近似值</em>。鲍威尔指出，这两种策略可以细分为两类，总共产生四类，即将讨论。只需要一些基本的符号，我们就可以开始了:</p><p id="1c62" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><em class="me"> s </em>:状态(做出决策所需的信息)</p><p id="66ca" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><em class="me"> a </em>:动作(可行操作on状态)</p><p id="3610" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><em class="me"> π </em>:策略(将状态映射到动作)</p><p id="2b13" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><em class="me"> ϕ </em>:基函数(从状态中导出特征)</p><p id="637d" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><em class="me"> θ </em>:特征权重(策略的参数化)</p><p id="7c44" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><em class="me"> t </em>:时间点(离散时间点)</p><p id="ddb7" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><em class="me"> R </em>:奖励功能(在状态下采取行动的直接奖励)</p><p id="dc99" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><em class="me"> V </em>:价值函数(某种状态的下游奖励)</p></div><div class="ab cl mf mg hx mh" role="separator"><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk"/></div><div class="im in io ip iq"><h1 id="c8ad" class="mm mn it bd mo mp mq mr ms mt mu mv mw ki mx kj my kl mz km na ko nb kp nc nd bi translated">策略近似</h1><p id="474a" class="pw-post-body-paragraph li lj it lk b ll ne kd ln lo nf kg lq lr ng lt lu lv nh lx ly lz ni mb mc md im bi translated">在策略近似解决方案中，我们直接修改策略本身。当策略具有清晰的结构时，这样的解决策略往往效果最佳。我们可以区分两类:PFA和CFA。</p><h2 id="1699" class="nn mn it bd mo no np dn ms nq nr dp mw lr ns nt my lv nu nv na lz nw nx nc iz bi translated">政策功能近似(PFA)</h2><p id="94d2" class="pw-post-body-paragraph li lj it lk b ll ne kd ln lo nf kg lq lr ng lt lu lv nh lx ly lz ni mb mc md im bi translated">策略函数近似(PFA)本质上是策略的参数化函数。直接插入状态返回一个可行的动作。线性PFA可能看起来像:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/c07c7dcd959cd8945fa2b48dc1a2a7db.png" data-original-src="https://miro.medium.com/v2/resize:fit:1352/format:webp/1*iW2_rzpZcwAGUBQMiytNZg.png"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">政策功能近似(PFA)示例</p></figure><p id="2d6f" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">这里的关键挑战是找到合适的基础函数<em class="me">ϕ(s】</em>抓住决策过程的本质。要做到这一点，需要对问题的结构有很好的洞察力。通过选择更通用的功能表示，例如神经网络(演员网络)，使用状态作为输入并输出动作，可以减轻设计工作。这种PFA表示的缺点是参数调整变得更加困难，并且可解释性受到影响。无论如何，对问题结构的牢固理解仍然是必要的。</p><h2 id="6293" class="nn mn it bd mo no np dn ms nq nr dp mw lr ns nt my lv nu nv na lz nw nx nc iz bi translated">成本函数近似法</h2><p id="8c11" class="pw-post-body-paragraph li lj it lk b ll ne kd ln lo nf kg lq lr ng lt lu lv nh lx ly lz ni mb mc md im bi translated">像PFA一样，成本函数近似(CFA)也直接搜索策略空间。然而，CFA不直接返回动作，而是在受约束的动作空间中解决优化问题。一个示例配方是:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi nz"><img src="../Images/1373c1790dc06327d2830e5a5092b1d3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1096/format:webp/1*3Vrt8zuXVQPa-MpOZRuPnw.png"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">成本函数近似值(CFA)示例</p></figure><p id="123d" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">PFA直接返回一个动作，而CFA要求我们首先求解一个参数化的最大化函数。请注意，真正的奖励和价值函数已经被一个近似的奖励函数所取代。此外，动作空间A^π受到策略<em class="me"> π </em>及其参数化<em class="me"> θ </em>的约束，通常产生比原始空间更小的动作空间。注意，CFA的最简单形式仅仅是贪婪启发式，然而修改的奖励函数可以嵌入探索元素。每次迭代的计算工作量可能高于PFA(由于最大化步骤)，但所需的设计工作量较少。</p><h1 id="87cb" class="mm mn it bd mo mp oa mr ms mt ob mv mw ki oc kj my kl od km na ko oe kp nc nd bi translated">价值近似值</h1><p id="edb6" class="pw-post-body-paragraph li lj it lk b ll ne kd ln lo nf kg lq lr ng lt lu lv nh lx ly lz ni mb mc md im bi translated">价值近似值明确考虑了当前决策的下游影响，考虑了整个决策范围。提醒最优价值函数等同于最优策略；它们都给出了贝尔曼方程的相同解。当政策结构不突出或我们不能恰当地监督当前决策的下游效应时，价值近似值可能是合适的。</p><h2 id="f603" class="nn mn it bd mo no np dn ms nq nr dp mw lr ns nt my lv nu nv na lz nw nx nc iz bi translated">价值函数近似(VFA)</h2><p id="71b2" class="pw-post-body-paragraph li lj it lk b ll ne kd ln lo nf kg lq lr ng lt lu lv nh lx ly lz ni mb mc md im bi translated">价值函数近似值(VFA)将下游价值表示为一个函数。贝尔曼方程的一个问题是，在采取一个行动后，随机事件可能会把我们带到许多新的状态<em class="me"> s'∈ S' </em>。因此，对于每个动作，我们应该考虑所有可到达状态<em class="me">s’</em>的值以及在那里结束的概率。vfa通过用确定性近似函数<em class="me"> V_t(s_t，a_t) </em>代替随机期望项来规避这个问题。在规范形式中，VFA看起来像:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi of"><img src="../Images/5484e224cc6182b4c46acdf7bf9ac4b2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1306/format:webp/1*eQZKonIVY3xNDQRo7GiBGQ.png"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">价值函数近似值示例(VFA)</p></figure><p id="bbb2" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">最简单的VFA是一个查找表，其中存储了每个状态-动作对的平均观察值。足够的迭代允许我们学习每一对的精确值，然而这在计算上很难处理。因此，我们通常求助于捕捉状态本质的特征，我们可以通过例如使用自动编码器来手动设计或提取这些特征。因此，我们在紧凑函数(例如，critic网络)中捕获状态-动作值，并通过观察来调整特征权重。</p><h2 id="6d40" class="nn mn it bd mo no np dn ms nq nr dp mw lr ns nt my lv nu nv na lz nw nx nc iz bi translated">直接前瞻近似(DLA)</h2><p id="0241" class="pw-post-body-paragraph li lj it lk b ll ne kd ln lo nf kg lq lr ng lt lu lv nh lx ly lz ni mb mc md im bi translated">设计VFA通常需要很好地理解问题结构，尽管神经网络在一定程度上缓解了这个问题(以额外的调整为代价)。直接前瞻逼近(DLA)只是对下游值进行采样，而不是导出显式函数。DLA可由以下代表:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi og"><img src="../Images/5a5ce9604c6940e81078629d152d136b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-zp4nKh1Cer2bmQVVg__Vw.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">直接前瞻逼近的例子(DLA)</p></figure><p id="5e80" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">诚然，这个等式看起来很麻烦，但实际上可能是最简单的。预期项意味着我们对未来进行采样，并应用一些(次优)策略来估计下游值。尽管我们在当前时间<em class="me"> t </em>最大化所有可行的动作，但是对于未来时间周期<em class="me">t’</em>，我们通常使用计算量更小的策略(例如，启发式)和/或简化的问题表示(例如，假设完全预见)。DLA战略带来了自己的挑战(情景取样、汇总技术等)。)，但与其他三种策略不同，它不需要估计函数(因此，用“前瞻”替换“函数”不仅仅是语义上的)。因此，对于其他三种策略都失败的复杂问题，它通常是最后的手段。</p><h1 id="2f15" class="mm mn it bd mo mp oa mr ms mt ob mv mw ki oc kj my kl od km na ko oe kp nc nd bi translated">混合班</h1><p id="9f1e" class="pw-post-body-paragraph li lj it lk b ll ne kd ln lo nf kg lq lr ng lt lu lv nh lx ly lz ni mb mc md im bi translated">如果不提到结合多个职业的策略的选项，那将是我们的失职。例如，经典的演员-评论家框架包含了PFA(演员)和VFA(评论家)的元素。然而，还有更多的组合，例如将VFA作为下游策略嵌入到直接前瞻算法中。与单一类别的解决方案相比，组合策略可能会消除彼此的弱点，通常会产生更好的结果。</p></div><div class="ab cl mf mg hx mh" role="separator"><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk"/></div><div class="im in io ip iq"><h1 id="5d90" class="mm mn it bd mo mp mq mr ms mt mu mv mw ki mx kj my kl mz km na ko nb kp nc nd bi translated">外卖食品</h1><p id="df84" class="pw-post-body-paragraph li lj it lk b ll ne kd ln lo nf kg lq lr ng lt lu lv nh lx ly lz ni mb mc md im bi translated">根据Powell的说法，实际上任何解决方案策略都可以归为四个策略类别中的一个(或多个)。除此之外，从分类中还可以得出一些有趣的结论:</p><ul class=""><li id="3932" class="oh oi it lk b ll lm lo lp lr oj lv ok lz ol md om on oo op bi translated"><strong class="lk jd">没有一刀切。</strong>虽然有一些经验法则，但多种策略可能会产生好的解决方案。在Powell&amp;Meisel[3]中可以找到这种说法的一个具体例子，展示了对同一问题的所有四种解决策略的实现。</li><li id="b93d" class="oh oi it lk b ll oq lo or lr os lv ot lz ou md om on oo op bi translated">学者喜欢优雅。PFA和VFA似乎在学术界最受欢迎。毕竟，在一个函数中捕捉一个复杂的决策政策有一定的数学美。</li><li id="0fc9" class="oh oi it lk b ll oq lo or lr os lv ot lz ou md om on oo op bi translated"><strong class="lk jd">行业喜欢结果。当问题变得太大或太复杂时，CFA和DLA可能会产生意想不到的好结果。尽管更多地依赖于蛮力和枚举，但设计工作量却大大减少了。</strong></li><li id="5d92" class="oh oi it lk b ll oq lo or lr os lv ot lz ou md om on oo op bi translated"><strong class="lk jd">万物皆有价。</strong>在便利性、设计努力、计算复杂性、可解释性等方面总是存在权衡。问题的本质决定了这些权衡的轻重。</li><li id="6984" class="oh oi it lk b ll oq lo or lr os lv ot lz ou md om on oo op bi translated"><strong class="lk jd">分类是关键。有许多RL社区，许多技术，许多符号风格，许多算法。为了简化领域并促进进步，需要一个清晰的总体框架。</strong></li></ul></div><div class="ab cl mf mg hx mh" role="separator"><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk"/></div><div class="im in io ip iq"><h1 id="8355" class="mm mn it bd mo mp mq mr ms mt mu mv mw ki mx kj my kl mz km na ko nb kp nc nd bi translated">参考</h1><p id="a985" class="pw-post-body-paragraph li lj it lk b ll ne kd ln lo nf kg lq lr ng lt lu lv nh lx ly lz ni mb mc md im bi translated">[1]鲍威尔，沃伦b .“随机优化的统一框架。”<em class="me">欧洲运筹学杂志</em>275.3(2019):795–821。</p><p id="d012" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">[2]萨顿、理查德s .和安德鲁g .巴尔托。<em class="me">强化学习:简介</em>。麻省理工学院出版社，2018。</p><p id="3cf9" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">[3]鲍威尔、沃伦·b .和斯蒂芬·梅塞尔。"能源中的随机优化教程——第二部分:能量存储图解."<em class="me"> IEEE电力系统汇刊</em>31.2(2015):1468–1475。</p></div></div>    
</body>
</html>