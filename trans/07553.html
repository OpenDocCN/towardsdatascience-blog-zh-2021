<html>
<head>
<title>K-Nearest Neighbours explained</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">k-最近邻解释道</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/k-nearest-neighbours-explained-52c910c035c5?source=collection_archive---------18-----------------------#2021-07-10">https://towardsdatascience.com/k-nearest-neighbours-explained-52c910c035c5?source=collection_archive---------18-----------------------#2021-07-10</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="1b46" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph">数据科学基础</h2><div class=""/><div class=""><h2 id="60cb" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">了解这种受监督的机器学习算法是如何工作的</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/f58ca57c7f3e5147b011af0180e5d423.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*D3VzLlJUq-qrB601"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">布鲁克·拉克在<a class="ae lh" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>拍摄的照片</p></figure><p id="533f" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">k-最近邻(KNN从这里开始)是一种直观且易于理解的机器学习算法。这篇文章简要介绍了KNN。我们将首先通过一个简单的例子学习算法在概念上如何工作，然后用Python从头开始实现算法，以巩固概念知识。</p></div><div class="ab cl me mf hx mg" role="separator"><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj"/></div><div class="im in io ip iq"><h1 id="9cf5" class="ml mm it bd mn mo mp mq mr ms mt mu mv ki mw kj mx kl my km mz ko na kp nb nc bi translated">📗 1.算法如何工作的摘要</h1><h2 id="d295" class="nd mm it bd mn ne nf dn mr ng nh dp mv lr ni nj mx lv nk nl mz lz nm nn nb iz bi translated">📍1.1培训</h2><p id="3c55" class="pw-post-body-paragraph li lj it lk b ll no kd ln lo np kg lq lr nq lt lu lv nr lx ly lz ns mb mc md im bi translated">训练过程非常简单快捷。在训练期间，模型存储哪些特征组合对应于训练数据中的哪个目标。换句话说，它只是记忆训练数据。</p><h2 id="db6b" class="nd mm it bd mn ne nf dn mr ng nh dp mv lr ni nj mx lv nk nl mz lz nm nn nb iz bi translated">📍 1.2.预言；预测；预告</h2><p id="67eb" class="pw-post-body-paragraph li lj it lk b ll no kd ln lo np kg lq lr nq lt lu lv nr lx ly lz ns mb mc md im bi translated">所有的艰苦工作都发生在预测过程中。为了预测一个示例的目标，该算法经历以下步骤:</p><p id="8e29" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">1️⃣找到k-最近的训练例子(即邻居)。<br/> 2️⃣在k个最近的邻居中分配典型的目标值。这意味着分类的最常见目标类和回归的平均目标值。</p><p id="f949" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">正如你现在看到的，这个算法的名字泄露了它所做的事情的要点。这里有一种方式来思考KNN是如何工作的。训练示例就像一个查找表。当我们有一个新的示例要预测时，我们会找到与这个新示例最相似的k个训练示例，并基于这些相似训练示例的典型值进行预测。很直观很好理解吧？</p><p id="f169" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">现在，让我们将刚刚学到的知识应用到一个简单的例子中。</p><h2 id="59e3" class="nd mm it bd mn ne nf dn mr ng nh dp mv lr ni nj mx lv nk nl mz lz nm nn nb iz bi translated">📍 1.3.例子</h2><p id="9d08" class="pw-post-body-paragraph li lj it lk b ll no kd ln lo np kg lq lr nq lt lu lv nr lx ly lz ns mb mc md im bi translated">假设我们有以下训练数据集，它有三个特征:<em class="nt"> x1 </em>到<em class="nt"> x3 </em>和一个二进制标签:<em class="nt"> y. </em></p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/ed007bd9a9d4068aa0e8cf73eed82198.png" data-original-src="https://miro.medium.com/v2/resize:fit:852/format:webp/1*TUx9JShyX-djva4v7FdS8A.png"/></div></figure><p id="0dda" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我们有一个如下的测试数据集:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi nv"><img src="../Images/a42eb4ac298bfaef139822fc7181280c.png" data-original-src="https://miro.medium.com/v2/resize:fit:858/format:webp/1*NTyO159FLslCY5B-reb85Q.png"/></div></figure><p id="f17a" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我们将使用欧几里德距离(一种流行的距离度量)来寻找最近的邻居，并设置k=2。现在，让我们预测测试示例<em class="nt"> h </em>的类。</p><p id="8264" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><strong class="lk jd"> 1️⃣找到它到所有训练样本的欧几里德距离。<br/> </strong>首先，让我们找出它到<em class="nt">的距离一</em>，第一个训练例子:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi nw"><img src="../Images/b113cf9149606bae80be515d0b647091.png" data-original-src="https://miro.medium.com/v2/resize:fit:1002/format:webp/1*enm5lo7CEKgEQl6RJ7B-hA.png"/></div></figure><p id="0b13" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">由于两者具有相同的特征值，其到<em class="nt"> a </em>的距离为0。现在，让我们计算它到下一个例子<em class="nt"> b </em>的距离:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/594ccf5ac5278e97d321aef1f27c4a7f.png" data-original-src="https://miro.medium.com/v2/resize:fit:984/format:webp/1*herXqN1jb7HSXGoW0F2S-g.png"/></div></figure><p id="1060" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">其到<em class="nt"> b </em>的距离为2.24。如果我们对其余的训练示例重复相同的计算，我们将得到以下距离:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/d106f49515da2123d8725e59a344cc77.png" data-original-src="https://miro.medium.com/v2/resize:fit:996/format:webp/1*HmCzM_PfE5_lnugaMimjJw.png"/></div></figure><p id="25ed" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><strong class="lk jd"> 2️⃣找到它的k个最近邻居(即具有最短距离的训练例子)。<br/> </strong>我们可以看到最近的两个邻居是<em class="nt"> a </em>和<em class="nt"> d </em>。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi nz"><img src="../Images/1c9ac27f55ed4eda69b66309de08199b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*rv7F8RCO2706ZHdjlmgFJw.png"/></div></figure><p id="f739" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><strong class="lk jd"> 3️⃣在其k个最近的邻居中找到最常见的标签。<br/> </strong>如果我们看训练数据，会发现<em class="nt"> a </em>和<em class="nt"> d </em>都属于<em class="nt">负</em>类。因此预测<em class="nt"> h </em>为<em class="nt">负</em>，因为2个最近邻中最常见的类是<em class="nt">负</em>。</p><p id="5072" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">现在，让我们为其他两个测试示例找到k个最近邻:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi oa"><img src="../Images/9fa1154665b3f3bdfcdf119870854345.png" data-original-src="https://miro.medium.com/v2/resize:fit:998/format:webp/1*U4jFe5IreCHu9TwjaGjmTg.png"/></div></figure><p id="f2fe" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><em class="nt"> i </em>和<em class="nt"> j </em>都离<em class="nt"> e </em>和<em class="nt"> f </em>最近。由于<em class="nt"> e </em>和<em class="nt"> f </em>之间的公共类为<em class="nt">正，t </em> he模型预测测试实例<em class="nt"> i </em>和<em class="nt"> j </em>为<em class="nt">正</em>。</p><p id="a099" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">直观地理解特征尺度和k值的含义是有用的。<br/> ◼️ <strong class="lk jd">特征比例:</strong>虽然例子中的特征一开始没有相等的比例，但它们非常接近。想象特征<em class="nt"> x3 </em>大10倍:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi ob"><img src="../Images/e8916fda483cd6e1731847dbb787996f.png" data-original-src="https://miro.medium.com/v2/resize:fit:844/format:webp/1*_GEhXIc5XMEaGwxxeWz6Hg.png"/></div></figure><p id="c3f9" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">对于每个示例，距离和k-最近邻都会发生变化:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi oc"><img src="../Images/a29a678069c09b055effb590d959591f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1282/format:webp/1*mHbi111PYnZLvrSOdRNC9g.png"/></div></div></figure><p id="91e7" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">比例越高的要素在距离计算中的权重越大。这几乎就像特征比例暗示了距离计算中每个特征的权重。因此，如果你想让你的特征做出同等的贡献，在建模之前将它们放在相同的比例上是很重要的。</p><p id="aa0b" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">◼️:在我们的例子中，我们将k设置为2。但是如果我们把k改成7会怎么样呢？</p><p id="f1c3" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我们在训练数据集中有7个例子，最常见的类是<em class="nt">正</em>。k=7时，模型会将任何记录预测为<em class="nt">阳性，</em>为训练数据中的多数类。下图右侧的光谱代表了这种情况:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi od"><img src="../Images/a9eca7ebb36fa9bfad47f3e1219fa7ee.png" data-original-src="https://miro.medium.com/v2/resize:fit:1272/format:webp/1*0dbLSbJrBsQYwQoKQNKCRg.png"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">作者图片</p></figure><p id="b31f" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">当k太小时，模型可以检测到数据中非常细微的模式，并且易于过拟合，而k太大会导致模型过于一般化，易于欠拟合。找到k的最佳值需要试错学习方法。找到最佳k的一种方法是从随机搜索开始，在大范围内随机测试k的一组值，然后在更窄的有希望的值范围内进行网格搜索。</p><p id="a2b8" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">如果k = 1，训练数据中没有重复的特征组合，你认为训练精度会是多少？答案是100%准确的，因为每个都是自己最近的邻居。</p><p id="f27f" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我们现在将把我们在本节中学到的内容翻译成代码。</p></div><div class="ab cl me mf hx mg" role="separator"><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj"/></div><div class="im in io ip iq"><h1 id="a956" class="ml mm it bd mn mo mp mq mr ms mt mu mv ki mw kj mx kl my km mz ko na kp nb nc bi translated">💻 2.Python中的简单实现</h1><p id="c1d4" class="pw-post-body-paragraph li lj it lk b ll no kd ln lo np kg lq lr nq lt lu lv nr lx ly lz ns mb mc md im bi translated">让我们导入库并创建相同的玩具数据集:</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="oe of l"/></div></figure><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi og"><img src="../Images/763c694afc34e7d7b2cc38bedf1d5233.png" data-original-src="https://miro.medium.com/v2/resize:fit:890/format:webp/1*H3RVn8VFrToqxipNW6uI9Q.png"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">训练在左边，测试在右边</p></figure><p id="ff55" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我们创建一个<code class="fe oh oi oj ok b">KNNClassifier</code>，拟合它并做出预测:</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="oe of l"/></div></figure><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi ol"><img src="../Images/4222a1dc8da41825507c743f509b8e11.png" data-original-src="https://miro.medium.com/v2/resize:fit:1100/format:webp/1*RGBeeGxCHH5tcnOK1JNg_A.png"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">列车在右侧，试验在左侧</p></figure><p id="5477" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">耶，我们刚刚用Python从头开始构建了一个非常简单的KNN分类器。虽然我们在这篇文章中关注的是分类，但是创建KNN回归器是很简单的。要创建一个回归变量，在上面的代码中，将第1行中的<code class="fe oh oi oj ok b">KNNClassifier</code>重命名为<code class="fe oh oi oj ok b">KNNRegressor </code>，并将第29行改为:<code class="fe oh oi oj ok b">y.append(np.mean(targets))</code>。</p><p id="27df" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">为了保持代码简单易读，上面的代码包含潜在的冗余步骤(例如求平方根)和非优化代码(例如存储所有邻居)。为了更好地理解算法，我们自己实现了算法。实际上，最好使用更优化的算法，比如来自<em class="nt"> sklearn </em>的算法。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi om"><img src="../Images/bdb78b01fda4dc32054e8e00c7d387b4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*rdoIJS7eqqEloNxL"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">希瑟·巴恩斯在<a class="ae lh" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="a744" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><em class="nt">您想访问更多这样的内容吗？媒体会员可以无限制地访问媒体上的任何文章。如果您使用</em> <a class="ae lh" href="https://zluvsand.medium.com/membership" rel="noopener"> <em class="nt">我的推荐链接</em></a><em class="nt">成为会员，您的一部分会费将直接用于支持我。</em></p></div><div class="ab cl me mf hx mg" role="separator"><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj"/></div><div class="im in io ip iq"><p id="d8cd" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">在这篇文章中，我们已经涵盖了KNN的基础知识。感谢您阅读我的文章。如果你感兴趣，这里有我其他一些帖子的链接:<br/> ◼️️ <a class="ae lh" rel="noopener" target="_blank" href="/comparing-random-forest-and-gradient-boosting-d7236b429c15">比较随机森林和梯度推进</a> <br/> ◼️️ <a class="ae lh" rel="noopener" target="_blank" href="/how-are-decision-trees-built-a8e5af57ce8?source=your_stories_page-------------------------------------">决策树是如何构建的？</a> <br/> ◼️️ <a class="ae lh" rel="noopener" target="_blank" href="/pipeline-columntransformer-and-featureunion-explained-f5491f815f?source=your_stories_page-------------------------------------">管道，ColumnTransformer和FeatureUnion说明</a>t26】◼️️<a class="ae lh" rel="noopener" target="_blank" href="/featureunion-columntransformer-pipeline-for-preprocessing-text-data-9dcb233dbcb6">feature union，ColumnTransformer &amp;管道用于预处理文本数据</a></p><p id="651d" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">再见🏃 💨</p></div></div>    
</body>
</html>