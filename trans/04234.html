<html>
<head>
<title>7 Must-Know PySpark Functions</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">7个必知的PySpark函数</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/7-must-know-pyspark-functions-d514ca9376b9?source=collection_archive---------9-----------------------#2021-04-10">https://towardsdatascience.com/7-must-know-pyspark-functions-d514ca9376b9?source=collection_archive---------9-----------------------#2021-04-10</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="3836" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">PySpark学习综合实践指南</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/7ad3b1341209ac7746e297c9db55fe45.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*A_kyqU0Si8Phv2ZaRu1vEA.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">Alexander Schimmeck 在<a class="ae ky" href="https://unsplash.com/s/photos/practice?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="ac4d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">Spark是一个用于大规模数据处理的分析引擎。它让您可以将数据和计算分散到集群上，从而实现显著的性能提升。</p><p id="541b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">随着收集、存储和传输数据的成本降低，我们在处理现实生活中的问题时可能会有大量的数据。因此，像Spark这样的分布式引擎正在成为数据科学生态系统中的主要工具。</p><p id="7248" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">PySpark是Spark的Python API。它结合了Python的简单性和Spark的高效性，这种合作得到了数据科学家和工程师的高度赞赏。</p><p id="bd1b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在本文中，我们将介绍PySpark的10个函数，这些函数对于使用结构化数据执行高效的数据分析至关重要。我们将使用用于结构化数据处理的pyspark.sql模块。</p><p id="8bef" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们首先需要创建一个SparkSession，作为Spark SQL的入口点。</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="118a" class="ma mb it lw b gy mc md l me mf">from pyspark.sql import SparkSession</span><span id="02c4" class="ma mb it lw b gy mg md l me mf">sc = SparkSession.builder.getOrCreate()<br/>sc.sparkContext.setLogLevel("WARN")</span><span id="e370" class="ma mb it lw b gy mg md l me mf">print(sc)<br/>&lt;pyspark.sql.session.SparkSession object at 0x7fecd819e630&gt;</span></pre><p id="d9b7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们将使用这个SparkSession对象与Spark SQL的函数和方法进行交互。让我们通过读取csv文件来创建一个spark数据帧。我们将使用Kaggle上的墨尔本房产<a class="ae ky" href="https://www.kaggle.com/anthonypino/melbourne-housing-market" rel="noopener ugc nofollow" target="_blank">数据集</a>。</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="9728" class="ma mb it lw b gy mc md l me mf">df = sc.read.option("header", "true").csv(<br/>    "/home/sparkuser/Desktop/fozzy/melb_housing.csv"<br/>)</span><span id="a947" class="ma mb it lw b gy mg md l me mf">df.columns<br/>['Suburb', 'Address', 'Rooms', 'Type', 'Price', 'Method', 'SellerG','Date', 'Postcode', 'Regionname', 'Propertycount', 'Distance','CouncilArea']</span></pre><p id="5719" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">该数据集包含墨尔本房屋的13个特征，包括房价。</p></div><div class="ab cl mh mi hx mj" role="separator"><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm"/></div><div class="im in io ip iq"><h2 id="7d0b" class="ma mb it bd mo mp mq dn mr ms mt dp mu li mv mw mx lm my mz na lq nb nc nd ne bi translated">1.挑选</h2><p id="b6fb" class="pw-post-body-paragraph kz la it lb b lc nf ju le lf ng jx lh li nh lk ll lm ni lo lp lq nj ls lt lu im bi translated">select函数帮助我们按列创建数据框的子集。我们只需要传递所需的列名。</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="cb5d" class="ma mb it lw b gy mc md l me mf">df.select("Date", "Regionname", "Price").show(5)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/494e4f4cce44e08b08b4f6dbda96a460.png" data-original-src="https://miro.medium.com/v2/resize:fit:564/format:webp/1*4KvrVyFGS4NDf6imuQpl-w.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">(图片由作者提供)</p></figure></div><div class="ab cl mh mi hx mj" role="separator"><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm"/></div><div class="im in io ip iq"><h2 id="b6df" class="ma mb it bd mo mp mq dn mr ms mt dp mu li mv mw mx lm my mz na lq nb nc nd ne bi translated">2.过滤器</h2><p id="ff09" class="pw-post-body-paragraph kz la it lb b lc nf ju le lf ng jx lh li nh lk ll lm ni lo lp lq nj ls lt lu im bi translated">filter函数可用于根据列值过滤数据点(即行)。例如，我们可以过滤位于北部大都市地区且价格超过100万英镑的房屋。</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="93ef" class="ma mb it lw b gy mc md l me mf">from pyspark.sql import functions as F</span><span id="273d" class="ma mb it lw b gy mg md l me mf">df.filter(<br/>    (F.col("Regionname") == "Northern Metropolitan") &amp;<br/>    (F.col("Price") &gt; 1000000)<br/>).count()<br/>3022</span></pre><p id="5d0a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们首先导入函数模块。我们应该使用col函数对列值进行比较。使用“&amp;”运算符组合多个条件。</p><p id="93cf" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">最后，count函数返回符合指定条件的行数。北部大都会区有3022栋房子的价格超过了100万英镑。</p></div><div class="ab cl mh mi hx mj" role="separator"><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm"/></div><div class="im in io ip iq"><h2 id="1d5d" class="ma mb it bd mo mp mq dn mr ms mt dp mu li mv mw mx lm my mz na lq nb nc nd ne bi translated">3.带列</h2><p id="5b4a" class="pw-post-body-paragraph kz la it lb b lc nf ju le lf ng jx lh li nh lk ll lm ni lo lp lq nj ls lt lu im bi translated">withColumn函数可用于操作列或创建新列。让我们更新价格列，使其以百万为单位显示价格。</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="2451" class="ma mb it lw b gy mc md l me mf">df = df.withColumn("Price_million", F.col("Price") / 1000000)</span><span id="71f5" class="ma mb it lw b gy mg md l me mf">df.select("Price_million").show(5)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nl"><img src="../Images/e7e3a40df5d9778b093e469fdc019861.png" data-original-src="https://miro.medium.com/v2/resize:fit:324/format:webp/1*PFZDl2o5ZI2dyoFT0UHxzA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">(图片由作者提供)</p></figure><p id="99c9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">第一个参数是列的新名称。不过，我们不必更改现有的名称。第二个参数定义对现有值的操作。</p></div><div class="ab cl mh mi hx mj" role="separator"><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm"/></div><div class="im in io ip iq"><h2 id="587c" class="ma mb it bd mo mp mq dn mr ms mt dp mu li mv mw mx lm my mz na lq nb nc nd ne bi translated">4.分组依据</h2><p id="ff90" class="pw-post-body-paragraph kz la it lb b lc nf ju le lf ng jx lh li nh lk ll lm ni lo lp lq nj ls lt lu im bi translated">数据分析中一个非常常见的操作是根据列中的不同值对数据点(即行)进行分组，并执行一些聚合。在我们的数据集中，我们可以找到不同地区的平均房价。</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="8ea5" class="ma mb it lw b gy mc md l me mf">df.groupby("Regionname").agg(<br/>     F.mean("Price_million").alias("Avg_house_price")<br/>).show()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nm"><img src="../Images/003aaef8b9061b288d52d4d8b8d0b285.png" data-original-src="https://miro.medium.com/v2/resize:fit:584/format:webp/1*nVQObVqsAtd4fC41H6Czvg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">(图片由作者提供)</p></figure><p id="8edd" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在我们根据指定的列(在我们的例子中是区域名称)对行进行分组之后，我们应用mean函数。别名方法用于为聚合列分配新名称。</p></div><div class="ab cl mh mi hx mj" role="separator"><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm"/></div><div class="im in io ip iq"><h2 id="3728" class="ma mb it bd mo mp mq dn mr ms mt dp mu li mv mw mx lm my mz na lq nb nc nd ne bi translated">5.orderby</h2><p id="adde" class="pw-post-body-paragraph kz la it lb b lc nf ju le lf ng jx lh li nh lk ll lm ni lo lp lq nj ls lt lu im bi translated">orderBy函数用于对值进行排序。我们可以将它应用于整个数据框，根据列中的值对行进行排序。另一个常见的操作是对聚合结果进行排序。</p><p id="e917" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">例如，上一步计算的平均房价可以按降序排列如下:</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="cda0" class="ma mb it lw b gy mc md l me mf">df.groupby("Regionname").agg(<br/>    F.round(F.mean("Price_million"), 2).alias("Avg_house_price")<br/>).orderBy(<br/>    F.col("Avg_house_price"), descending=False<br/>).show()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/4b80b376ac8639904bfd245355101fed.png" data-original-src="https://miro.medium.com/v2/resize:fit:536/format:webp/1*xsAlRMV7Re96vqErzUHx7w.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">(图片由作者提供)</p></figure></div><div class="ab cl mh mi hx mj" role="separator"><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm"/></div><div class="im in io ip iq"><h2 id="2d9b" class="ma mb it bd mo mp mq dn mr ms mt dp mu li mv mw mx lm my mz na lq nb nc nd ne bi translated">6.照亮</h2><p id="ae3c" class="pw-post-body-paragraph kz la it lb b lc nf ju le lf ng jx lh li nh lk ll lm ni lo lp lq nj ls lt lu im bi translated">我们可以使用lit函数通过指定一个文字值或常量值来创建一个列。</p><p id="42b0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">考虑这样一种情况，我们需要一个包含单个值的列。Pandas允许使用所需的值进行这样的操作。但是，在使用PySpark时，我们应该使用lit函数传递值。</p><p id="e3f6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们看看它的实际效果。以下代码筛选类型为“h”的数据点，然后选择三列。withColumn函数创建一个名为“is_house”的新列，并用1填充。</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="6250" class="ma mb it lw b gy mc md l me mf">df2 = df.filter(F.col("Type") == "h").select(<br/>    "Address", "Regionname", "Price"<br/>).withColumn("is_house", F.lit(1))</span><span id="456d" class="ma mb it lw b gy mg md l me mf">df2.show(5)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi no"><img src="../Images/4df88e1973c69599f406b417e55f716f.png" data-original-src="https://miro.medium.com/v2/resize:fit:772/format:webp/1*0I6UGUNc0ixtmibqrD_2Rw.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">(图片由作者提供)</p></figure></div><div class="ab cl mh mi hx mj" role="separator"><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm"/></div><div class="im in io ip iq"><h2 id="8b18" class="ma mb it bd mo mp mq dn mr ms mt dp mu li mv mw mx lm my mz na lq nb nc nd ne bi translated">7.当...的时候</h2><p id="f349" class="pw-post-body-paragraph kz la it lb b lc nf ju le lf ng jx lh li nh lk ll lm ni lo lp lq nj ls lt lu im bi translated">when函数计算给定的条件并相应地返回值。考虑到前面的例子，我们可以使用when函数实现一个更健壮的解决方案。</p><p id="1142" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">下面的代码创建一个名为“is_house”的新列，该列根据列的类型取值1或0。</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="c97b" class="ma mb it lw b gy mc md l me mf">df.select(when(df["Type"] == "h", 1)\<br/>          .otherwise(0).alias("is_house")).show(5)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi np"><img src="../Images/80c20e313dc8080a5e43df57a5a49911.png" data-original-src="https://miro.medium.com/v2/resize:fit:336/format:webp/1*1HTeOUyCRh_p-SC3FYA3Ng.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">(图片由作者提供)</p></figure></div><div class="ab cl mh mi hx mj" role="separator"><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm"/></div><div class="im in io ip iq"><h2 id="bc65" class="ma mb it bd mo mp mq dn mr ms mt dp mu li mv mw mx lm my mz na lq nb nc nd ne bi translated">结论</h2><p id="968d" class="pw-post-body-paragraph kz la it lb b lc nf ju le lf ng jx lh li nh lk ll lm ni lo lp lq nj ls lt lu im bi translated">我们已经介绍了7个PySpark函数，它们将帮助您执行高效的数据操作和分析。PySpark语法看起来像是Python和SQL的混合体。因此，如果您熟悉这些工具，那么使用PySpark就相对容易了。</p><p id="8516" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">需要注意的是，Spark针对大规模数据进行了优化。因此，在处理小规模数据时，您可能看不到任何性能提升。事实上，在处理小数据集时，Pandas可能会比PySpark表现得更好。</p><p id="bf08" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">感谢您的阅读。如果您有任何反馈，请告诉我。</p></div></div>    
</body>
</html>