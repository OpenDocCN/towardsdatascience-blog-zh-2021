<html>
<head>
<title>Convolutional Layers vs Fully Connected Layers</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">卷积层与全连接层</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/convolutional-layers-vs-fully-connected-layers-364f05ab460b?source=collection_archive---------0-----------------------#2021-11-13">https://towardsdatascience.com/convolutional-layers-vs-fully-connected-layers-364f05ab460b?source=collection_archive---------0-----------------------#2021-11-13</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="8232" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph">深度学习基础</h2><div class=""/><div class=""><h2 id="7297" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">当您使用卷积层与全连接层时，实际情况是怎样的？</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/8ebb84be031f37e4477e825ef8f13c3c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OmQi3Us6Uhtom8OgE__9Og.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">作者图片</p></figure><p id="1715" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">一开始，神经网络的设计是一件很难理解的事情。设计神经网络包括选择许多设计特征，如每层的输入和输出大小、何时何地应用批量标准化层、丢弃层、使用什么激活函数等。在本文中，我想讨论全连接层和卷积背后真正发生的事情，以及如何计算卷积层的输出大小。</p><h1 id="ffda" class="md me it bd mf mg mh mi mj mk ml mm mn ki mo kj mp kl mq km mr ko ms kp mt mu bi translated">介绍</h1><p id="a97f" class="pw-post-body-paragraph lh li it lj b lk mv kd lm ln mw kg lp lq mx ls lt lu my lw lx ly mz ma mb mc im bi translated">随着计算能力的提高和模型架构的进步，深度学习是一个在过去几年中飞速发展的研究领域。当阅读深度学习时，你会经常听到两种网络，即全连接神经网络(FCNN)和卷积神经网络(CNN)。这两个是深度学习架构的基础，几乎所有其他深度学习神经网络都源于此。在本文中，我将首先解释全连接层如何工作，然后是卷积层，最后我将通过一个 CNN 的例子。</p><h1 id="adda" class="md me it bd mf mg mh mi mj mk ml mm mn ki mo kj mp kl mq km mr ko ms kp mt mu bi translated">完全连接的层(FC 层)</h1><p id="c2a1" class="pw-post-body-paragraph lh li it lj b lk mv kd lm ln mw kg lp lq mx ls lt lu my lw lx ly mz ma mb mc im bi translated">神经网络是一组相关的非线性函数。每个单独的功能由一个神经元(或感知器)组成。在完全连接的层中，神经元通过权重矩阵对输入向量进行线性变换。然后通过非线性激活函数<em class="na"> f </em>对产品进行非线性变换。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi nb"><img src="../Images/33d3609a6988365c6b1be80f15c34d94.png" data-original-src="https://miro.medium.com/v2/resize:fit:1076/format:webp/1*d46zuQZnzgv3a8IPsBcIjg.png"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">作者图片</p></figure><p id="561b" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">这里我们取权重矩阵 W 和输入向量 x 之间的点积，偏差项(W0)可以添加到非线性函数中。我将在本文的其余部分忽略它，因为它不影响输出大小或决策，只是另一个权重。</p><p id="1d8b" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">如果我们以输入大小为 9、输出大小为 4 的 FC 神经网络中的一个层为例，则操作可以如下所示:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi nc"><img src="../Images/7e06ffdf053c4f1de47eb00ad86b61e7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-2yU6a8RfUWKogxYrkRF5A.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">作者图片</p></figure><p id="585f" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">激活函数 f 在该层的输入和该层的权重矩阵之间包装点积。注意，权重矩阵中的列将具有不同的数字，并且将随着模型的训练而被优化。</p><p id="2d4f" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">输入是 1×9 向量，权重矩阵是 9×4 矩阵。通过取点积并利用激活函数应用非线性变换，我们得到输出向量(1×4)。</p><p id="67af" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">人们也可以通过以下方式来可视化该层:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi nd"><img src="../Images/989d7b996456b0bb5b7d2e946de9a5d0.png" data-original-src="https://miro.medium.com/v2/resize:fit:826/format:webp/1*Dpj_w8J1Crm-uXuiA-fr9w.png"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">作者图片</p></figure><p id="f6fa" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">上图显示了为什么我们称这种类型的层为“完全连接”或有时称为“密集连接”。所有可能的层到层的连接都存在，这意味着输入向量的每个输入都会影响输出向量的每个输出。然而，并非所有权重都会影响所有输出。看上面每个节点之间的连线。橙色线代表该层的第一个神经元(或感知器)。这个神经元的权重只影响输出 A，对输出 B、C 或 d 没有影响。</p><h1 id="c9ca" class="md me it bd mf mg mh mi mj mk ml mm mn ki mo kj mp kl mq km mr ko ms kp mt mu bi translated">卷积层(Conv 层)</h1><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ne"><img src="../Images/33e6d2f04ea2fcec21cb6f6fc422eadb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*G056wVfKcNb8kRg72wfeJw.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">作者图片</p></figure><p id="9992" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">卷积实际上是一种滑动点积，其中内核沿着输入矩阵移动，我们将两者之间的点积视为矢量。下面是上面显示的卷积的矢量形式。您可以看到为什么橙色字段之间的点积会输出一个标量(1x44x 1 = 1x1)。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi nf"><img src="../Images/d366ed2f4b6e360fd1fd79aa1802fffc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1380/format:webp/1*MtFW0YQsEOAZ4gJuiKpZlQ.png"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">作者图片</p></figure><p id="a63e" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">同样，我们可以将这个卷积层想象为:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi ng"><img src="../Images/f03ea05b90d96346ab2ba8d5bcaa3574.png" data-original-src="https://miro.medium.com/v2/resize:fit:830/format:webp/1*GX2qCkp6o6YvmaHJ2v7rug.png"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">作者图片</p></figure><p id="dcef" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">卷积不是密集连接的，不是所有的输入节点都影响所有的输出节点。这使得卷积层在学习中具有更大的灵活性。此外，每层的权重数量要少得多，这对于图像数据等高维输入非常有帮助。这些优势赋予了 CNN 在数据中学习特征的众所周知的特性，例如图像数据中的形状和纹理。</p><h1 id="fbfd" class="md me it bd mf mg mh mi mj mk ml mm mn ki mo kj mp kl mq km mr ko ms kp mt mu bi translated">与 CNN 合作</h1><p id="911d" class="pw-post-body-paragraph lh li it lj b lk mv kd lm ln mw kg lp lq mx ls lt lu my lw lx ly mz ma mb mc im bi translated">在 FC 层中，通过选择权重矩阵中的列数，可以非常简单地指定层的输出大小。对于 Conv 层就不一样了。卷积有许多可以改变的参数，以适应运算的输出大小。</p><p id="b546" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">我强烈推荐你查看<a class="ae nh" href="https://github.com/vdumoulin/conv_arithmetic" rel="noopener ugc nofollow" target="_blank">这个链接</a>到弗朗切斯科对卷积的解释。在这本书里，他解释了卷积的所有变体，比如有和没有填充的卷积、步长、转置卷积等等。这是迄今为止我所见过的最好的，最直观的解释，我仍然经常回头参考它。</p><h2 id="3372" class="ni me it bd mf nj nk dn mj nl nm dp mn lq nn no mp lu np nq mr ly nr ns mt iz bi translated">Conv 输出大小</h2><p id="cd91" class="pw-post-body-paragraph lh li it lj b lk mv kd lm ln mw kg lp lq mx ls lt lu my lw lx ly mz ma mb mc im bi translated">要确定卷积的输出大小，可以应用以下公式:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/8f9195ccc2f7c02b8e4e147abb99a39b.png" data-original-src="https://miro.medium.com/v2/resize:fit:918/format:webp/1*WMsvsi5ni_9yMgx-aIfr8w.png"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">作者图片</p></figure><p id="1acc" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">输出大小等于输入大小加上两倍的填充减去步幅上的内核大小加上 1。大多数时候我们处理的是方阵，所以这个数字对于行和列是一样的。如果分数不是整数，我们就向上取整。我建议试着理解这个等式。除以步幅是有意义的，因为当我们跳过操作时，我们是将输出大小除以该数字。两次填充来自于填充被添加到矩阵的两侧，因此被添加两次。</p><h2 id="1001" class="ni me it bd mf nj nk dn mj nl nm dp mn lq nn no mp lu np nq mr ly nr ns mt iz bi translated">转置 Conv 尺寸</h2><p id="98a7" class="pw-post-body-paragraph lh li it lj b lk mv kd lm ln mw kg lp lq mx ls lt lu my lw lx ly mz ma mb mc im bi translated">根据上面的等式，输出将总是等于或小于输出，除非我们添加大量填充。然而，添加太多填充来增加维度将导致学习中的巨大困难，因为每层的输入将非常稀疏。为了解决这个问题，转置卷积被用来增加输入的大小。示例应用例如在卷积 VAEs 或 GANs 中。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi nu"><img src="../Images/62f5ad437068e11584d8bf1dfafec7dc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DLuZhhVQF0z2zzHJP5z41Q.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">作者图片</p></figure><p id="118a" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">上述等式可用于计算转置卷积层的输出大小。</p><p id="2a3d" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">有了这两个方程，你就可以设计一个卷积神经网络了。让我们看看 GAN 的设计，并使用上面的等式来理解它。</p><h1 id="57b6" class="md me it bd mf mg mh mi mj mk ml mm mn ki mo kj mp kl mq km mr ko ms kp mt mu bi translated">GAN 示例</h1><p id="1cf4" class="pw-post-body-paragraph lh li it lj b lk mv kd lm ln mw kg lp lq mx ls lt lu my lw lx ly mz ma mb mc im bi translated">在这里，我将介绍一个使用卷积层和转置卷积层的生成式对抗网络的架构。你会明白为什么上面的等式如此重要，为什么没有它们你就不能设计 CNN。</p><p id="5db4" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">我们先来看看鉴别器:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi nv"><img src="../Images/34cc6597241921cf79bceec19c5f9f3b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1126/format:webp/1*vgIAeugZhKq-H44NqU-HDg.png"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">作者图片</p></figure><p id="d06a" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">鉴别器的输入大小是 3x64x64 图像，输出大小是二进制 1x1 标量。我们大幅降低了维度，因此标准卷积层是这种应用的理想选择。</p><p id="6b73" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">注意，在每个卷积层之间(PyTorch 中表示为 Conv2d ),指定了激活函数(在本例中为 LeakyReLU ),并应用了批量归一化。</p><h2 id="6d01" class="ni me it bd mf nj nk dn mj nl nm dp mn lq nn no mp lu np nq mr ly nr ns mt iz bi translated">鉴别器中的 Conv 层</h2><blockquote class="nw nx ny"><p id="224b" class="lh li na lj b lk ll kd lm ln lo kg lp nz lr ls lt oa lv lw lx ob lz ma mb mc im bi translated">nn。Conv2d(nc，ndf，k = 4，s = 2，p = 1，bias=False)</p></blockquote><p id="cb93" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">第一卷积层将“ndf”卷积应用于输入的 3 层中的每一层。图像数据通常有 3 层，分别用于红绿蓝(RGB 图像)。我们可以对每一层应用一些卷积来增加维度。</p><p id="3806" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">应用的第一个卷积的内核大小为 4，跨距为 2，填充为 1。将此代入等式得出:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi oc"><img src="../Images/ceaf781782b2df7721784bcb99f7bdfd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GPG75p4b7yFWvve4lo3DDA.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">作者图片</p></figure><p id="d568" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">因此输出是一个 32x32 的图像，正如代码中提到的。您可以看到，我们已经将输入的大小减半。接下来的 3 层是相同的，这意味着每层的输出大小是 16x16，然后是 8x8，然后是 4x4。最终层使用的内核大小为 4，步幅为 1，填充为 0。代入公式，我们得到 1×1 的输出大小。</p><h2 id="a46d" class="ni me it bd mf nj nk dn mj nl nm dp mn lq nn no mp lu np nq mr ly nr ns mt iz bi translated">发生器中的转置 Conv 层</h2><blockquote class="nw nx ny"><p id="b60d" class="lh li na lj b lk ll kd lm ln lo kg lp nz lr ls lt oa lv lw lx ob lz ma mb mc im bi translated">nn。ConvTranspose2d( nz，ngf * 8，4，1，0，bias=False)</p></blockquote><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi od"><img src="../Images/eeb367e1d3a275cbcc2289f74febbfa1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1306/format:webp/1*yTsDamwhJwhHwTqRSXr5NQ.png"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">作者图片</p></figure><p id="b9eb" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">让我们看看发生器中的第一层。发生器有一个 1x1x100 矢量(1xnz)的输入，想要的输出是 3x64x64。我们正在增加维度，所以我们想使用转置卷积。</p><p id="2b83" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">第一个卷积使用的内核大小为 4，步长为 1，填充为 0。让我们把它代入转置卷积方程:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi oe"><img src="../Images/9c02494a507fb9e9f0c1df5b50132368.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pWULr7b5JnWswpMEli8i3A.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">作者图片</p></figure><p id="fe47" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">如代码所示，转置卷积的输出大小为 4x4。接下来的 4 个卷积层是相同的，核大小为 4，步长为 2，填充为 1。这使每个输入的大小加倍。于是 4x4 转 8x8，然后 16x16，32x32，最后 64x64。</p><h1 id="96e5" class="md me it bd mf mg mh mi mj mk ml mm mn ki mo kj mp kl mq km mr ko ms kp mt mu bi translated">结论</h1><p id="382c" class="pw-post-body-paragraph lh li it lj b lk mv kd lm ln mw kg lp lq mx ls lt lu my lw lx ly mz ma mb mc im bi translated">在本文中，我解释了全连接层和卷积层是如何计算的。我还解释了如何计算卷积和转置卷积层的输出大小。不了解这些，就无法设计自己的 CNN。</p><h2 id="a3f4" class="ni me it bd mf nj nk dn mj nl nm dp mn lq nn no mp lu np nq mr ly nr ns mt iz bi translated">支持我👏</h2><p id="3ed5" class="pw-post-body-paragraph lh li it lj b lk mv kd lm ln mw kg lp lq mx ls lt lu my lw lx ly mz ma mb mc im bi translated">希望这对你有所帮助，如果你喜欢，你可以<a class="ae nh" href="https://medium.com/@diegounzuetaruedas" rel="noopener"> <strong class="lj jd">关注我！</strong>T3】</a></p><p id="dd6b" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">你也可以成为<a class="ae nh" href="https://diegounzuetaruedas.medium.com/membership" rel="noopener"> <strong class="lj jd">中级会员</strong> </a> <strong class="lj jd"> </strong>使用我的推荐链接，获得我所有的文章和更多:<a class="ae nh" href="https://diegounzuetaruedas.medium.com/membership" rel="noopener">https://diegounzuetaruedas.medium.com/membership</a></p><h1 id="a55b" class="md me it bd mf mg mh mi mj mk ml mm mn ki mo kj mp kl mq km mr ko ms kp mt mu bi translated">你可能喜欢的其他文章</h1><p id="1add" class="pw-post-body-paragraph lh li it lj b lk mv kd lm ln mw kg lp lq mx ls lt lu my lw lx ly mz ma mb mc im bi translated"><a class="ae nh" rel="noopener" target="_blank" href="/differentiable-generator-networks-an-introduction-5a9650a24823">可微发电机网络:简介</a></p><p id="5fb7" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><a class="ae nh" rel="noopener" target="_blank" href="/fourier-transforms-an-intuitive-visualisation-ba186c7380ee">傅立叶变换:直观的可视化</a></p></div></div>    
</body>
</html>