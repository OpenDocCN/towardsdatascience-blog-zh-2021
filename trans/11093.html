<html>
<head>
<title>Label Smoothing as Another Regularization Trick</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">标签平滑是另一个正则化技巧</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/label-smoothing-as-another-regularization-trick-7b34c50dc0b9?source=collection_archive---------17-----------------------#2021-10-29">https://towardsdatascience.com/label-smoothing-as-another-regularization-trick-7b34c50dc0b9?source=collection_archive---------17-----------------------#2021-10-29</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="7d2d" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">什么是标签平滑，如何在 PyTorch 中实现</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/7f9505f43dcf4a40687cf66c247456ed.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9m3OQd_ZTahJFrIIoLPQRQ.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">由<a class="ae ky" href="https://unsplash.com/@johnwestrock?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">戴夫</a>在<a class="ae ky" href="https://unsplash.com/s/photos/smooth?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure></div><div class="ab cl kz la hx lb" role="separator"><span class="lc bw bk ld le lf"/><span class="lc bw bk ld le lf"/><span class="lc bw bk ld le"/></div><div class="im in io ip iq"><p id="3c7d" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated"><strong class="li iu">过拟合和概率校准是训练深度学习模型时出现的两个问题。</strong>深度学习中有很多正则化技术来解决过拟合；体重下降、提前停止和辍学是其中最受欢迎的一些。另一方面，Platt 的标度和保序回归用于模型校准。</p><p id="e2d3" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">但是有没有一种方法可以同时对抗过度适应和过度自信呢？</p><p id="f942" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated"><strong class="li iu">标签平滑是一种正则化技术，它扰动目标变量，使模型的预测更不确定。</strong>它被视为一种正则化技术，因为它抑制了输入到 softmax 函数中的最大对数比其他对数大得多。此外，作为一个副作用，最终的模型得到了更好的校准。</p><p id="8311" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">在这个故事中，我们定义了标签平滑，实现了使用这种技术的交叉熵损失函数，并评估了它的性能。如果您想了解更多关于模型校准的信息，请参考下面的故事。</p><div class="mc md gp gr me mf"><a rel="noopener follow" target="_blank" href="/classifier-calibration-7d0be1e05452"><div class="mg ab fo"><div class="mh ab mi cl cj mj"><h2 class="bd iu gy z fp mk fr fs ml fu fw is bi translated">分类器校准</h2><div class="mm l"><h3 class="bd b gy z fp mk fr fs ml fu fw dk translated">分类任务模型校准的原因、时间和方式</h3></div><div class="mn l"><p class="bd b dl z fp mk fr fs ml fu fw dk translated">towardsdatascience.com</p></div></div><div class="mo l"><div class="mp l mq mr ms mo mt ks mf"/></div></div></a></div><blockquote class="mu mv mw"><p id="e4f8" class="lg lh mx li b lj lk ju ll lm ln jx lo my lq lr ls mz lu lv lw na ly lz ma mb im bi translated"><a class="ae ky" href="https://www.dimpo.me/newsletter?utm_source=medium&amp;utm_medium=article&amp;utm_campaign=label_smooth" rel="noopener ugc nofollow" target="_blank">学习率</a>是为那些对 AI 和 MLOps 的世界感到好奇的人准备的时事通讯。你会在每周五收到我关于最新人工智能新闻和文章的更新和想法。在这里订阅<a class="ae ky" href="https://www.dimpo.me/newsletter?utm_source=medium&amp;utm_medium=article&amp;utm_campaign=label_smooth" rel="noopener ugc nofollow" target="_blank"/>！</p></blockquote><h1 id="ab3c" class="nb nc it bd nd ne nf ng nh ni nj nk nl jz nm ka nn kc no kd np kf nq kg nr ns bi translated">标签平滑</h1><p id="5948" class="pw-post-body-paragraph lg lh it li b lj nt ju ll lm nu jx lo lp nv lr ls lt nw lv lw lx nx lz ma mb im bi translated">假设我们有一个多类分类问题。在这样的问题中，目标变量通常是一个热点向量，其中我们在正确的类的位置有<code class="fe ny nz oa ob b">1</code>，在其他地方有<code class="fe ny nz oa ob b">0</code>。这是一项不同于二元分类或多标签分类的任务，二元分类中只有两个可能的类，多标签分类中一个数据点中可以有多个正确的类。因此，多标记分类问题的一个例子是，如果您必须检测图像中存在的每个对象。</p><blockquote class="oc"><p id="9885" class="od oe it bd of og oh oi oj ok ol mb dk translated">标签平滑少量改变目标向量<code class="fe ny nz oa ob b">ε</code>。<strong class="ak">因此，我们不是让我们的模型预测正确的类别的</strong> <code class="fe ny nz oa ob b"><strong class="ak">1</strong></code> <strong class="ak">，而是让它预测正确的类别的</strong> <code class="fe ny nz oa ob b"><strong class="ak">1-ε</strong></code> <strong class="ak">和所有其他类别的</strong> <code class="fe ny nz oa ob b"><strong class="ak">ε</strong></code> <strong class="ak">。</strong></p></blockquote><p id="8732" class="pw-post-body-paragraph lg lh it li b lj om ju ll lm on jx lo lp oo lr ls lt op lv lw lx oq lz ma mb im bi translated">标签平滑少量改变目标向量<code class="fe ny nz oa ob b">ε</code>。因此，我们不是要求我们的模型预测正确的类的 <code class="fe ny nz oa ob b"><strong class="li iu">1</strong></code> <strong class="li iu">，而是要求它预测正确的类的</strong> <code class="fe ny nz oa ob b"><strong class="li iu">1-ε</strong></code> <strong class="li iu">和所有其他类的</strong> <code class="fe ny nz oa ob b"><strong class="li iu">ε</strong></code> <strong class="li iu">。</strong>于是，带标签平滑的交叉熵损失函数被转化为下面的公式。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi or"><img src="../Images/78b0d6e2f476aac953ea992c7c7517b7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wnaItlASKNzBk4m4M_CHng.png"/></div></div></figure><p id="52b0" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">在这个公式中，<code class="fe ny nz oa ob b">ce(x)</code>表示<code class="fe ny nz oa ob b">x</code>(如<code class="fe ny nz oa ob b">-log(p(x))</code>)的标准交叉熵损失，<code class="fe ny nz oa ob b">ε</code>为小正数，<code class="fe ny nz oa ob b">i</code>为正确类别，<code class="fe ny nz oa ob b">N</code>为类别数。</p><p id="4958" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated"><strong class="li iu">直观上，标签平滑将正确类别的 logit 值限制为更接近其他类别的 logit 值</strong>。以这种方式，它被用作正则化技术和对抗模型过度自信的方法。</p><h1 id="250d" class="nb nc it bd nd ne nf ng nh ni nj nk nl jz nm ka nn kc no kd np kf nq kg nr ns bi translated">PyTorch 实现</h1><p id="f053" class="pw-post-body-paragraph lg lh it li b lj nt ju ll lm nu jx lo lp nv lr ls lt nw lv lw lx nx lz ma mb im bi translated">PyTorch 中标签平滑交叉熵损失函数的实现非常简单。对于这个例子，我们使用作为<a class="ae ky" href="https://youtu.be/vnOpEwmtFJ8?t=1258" rel="noopener ugc nofollow" target="_blank"> fast.ai 课程</a>的一部分开发的代码。</p><p id="f571" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">首先，让我们使用一个辅助函数来计算两个值之间的线性组合:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="os ot l"/></div></figure><p id="e4da" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">接下来，我们实现一个新的损失函数作为 PyTorch <code class="fe ny nz oa ob b">nn.Module</code>。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="os ot l"/></div></figure><p id="62bd" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">我们现在可以在代码中删除这个类。对于这个例子，我们使用标准的 fast.ai <a class="ae ky" href="https://github.com/fastai/course-v3/blob/master/nbs/dl1/lesson1-pets.ipynb" rel="noopener ugc nofollow" target="_blank"> pets 示例</a>。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="os ot l"/></div></figure><p id="bb60" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">我们将数据转换为模型可以使用的格式，选择 ResNet 架构，并以优化标签平滑交叉熵损失为目标。四个时期后，结果总结如下。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ou"><img src="../Images/54108d52af96733ca74b1748b4f9a7d4.png" data-original-src="https://miro.medium.com/v2/resize:fit:828/format:webp/1*gRMpxjMlvCO8Cyi0A7Sg1w.png"/></div></figure><p id="b617" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">我们得到 7.5%的错误率，这对于十行左右的代码来说是可以接受的，在大多数情况下，我们使用默认设置。</p><p id="f0cb" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">有许多事情我们可以调整，使我们的模型表现得更好。不同的优化器、超参数、模型架构等。例如，您可以在下面的故事中了解如何进一步发展 ResNet 架构。</p><div class="mc md gp gr me mf"><a rel="noopener follow" target="_blank" href="/xresnet-from-scratch-in-pytorch-e64e309af722"><div class="mg ab fo"><div class="mh ab mi cl cj mj"><h2 class="bd iu gy z fp mk fr fs ml fu fw is bi translated">Pytorch 中从头开始的 xResNet</h2><div class="mm l"><h3 class="bd b gy z fp mk fr fs ml fu fw dk translated">从你的 ResNet 架构中挤出一点额外的东西。</h3></div><div class="mn l"><p class="bd b dl z fp mk fr fs ml fu fw dk translated">towardsdatascience.com</p></div></div><div class="mo l"><div class="ov l mq mr ms mo mt ks mf"/></div></div></a></div><h1 id="fb6e" class="nb nc it bd nd ne nf ng nh ni nj nk nl jz nm ka nn kc no kd np kf nq kg nr ns bi translated">结论</h1><p id="a3f6" class="pw-post-body-paragraph lg lh it li b lj nt ju ll lm nu jx lo lp nv lr ls lt nw lv lw lx nx lz ma mb im bi translated">过拟合和概率校准是在训练深度学习模型时出现的两个问题。深度学习中有很多正则化技术来解决过拟合；体重下降、提前停止和辍学是其中最受欢迎的一些。另一方面，Platt 的标度和保序回归用于模型校准。</p><p id="f980" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">在这个故事中，我们研究了标签平滑，这是一种试图对抗过度拟合和过度自信的技术。我们看到了何时使用它以及如何在 PyTorch 中实现它。然后，我们训练了一个最先进的计算机视觉模型，用十行代码识别不同品种的猫和狗。</p><p id="32df" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">模型正则化和校准是两个重要的概念。更好地理解对抗方差和过度自信的工具，会让你成为更好的深度学习实践者。</p><h1 id="484e" class="nb nc it bd nd ne nf ng nh ni nj nk nl jz nm ka nn kc no kd np kf nq kg nr ns bi translated">关于作者</h1><p id="a3b5" class="pw-post-body-paragraph lg lh it li b lj nt ju ll lm nu jx lo lp nv lr ls lt nw lv lw lx nx lz ma mb im bi translated">我叫<a class="ae ky" href="https://www.dimpo.me/?utm_source=medium&amp;utm_medium=article&amp;utm_campaign=label_smooth" rel="noopener ugc nofollow" target="_blank">迪米特里斯·波罗普洛斯</a>，我是一名为<a class="ae ky" href="https://www.arrikto.com/" rel="noopener ugc nofollow" target="_blank">阿里克托</a>工作的机器学习工程师。我曾为欧洲委员会、欧盟统计局、国际货币基金组织、欧洲央行、经合组织和宜家等主要客户设计和实施过人工智能和软件解决方案。</p><p id="4fb7" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">如果你有兴趣阅读更多关于机器学习、深度学习、数据科学和数据运算的帖子，请关注我的<a class="ae ky" href="https://towardsdatascience.com/medium.com/@dpoulopoulos/follow" rel="noopener" target="_blank"> Medium </a>、<a class="ae ky" href="https://www.linkedin.com/in/dpoulopoulos/" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>或 Twitter 上的<a class="ae ky" href="https://twitter.com/james2pl" rel="noopener ugc nofollow" target="_blank"> @james2pl </a>。</p><p id="b7fe" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">所表达的观点仅代表我个人，并不代表我的雇主的观点或意见。</p></div></div>    
</body>
</html>