<html>
<head>
<title>Best of arXiv — Readings for April 2021: GPT strikes back, Video Transformers and more</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">arXiv精选——2021年4月阅读:GPT反击战，视频变形金刚等等</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/best-of-arxiv-readings-for-april-2021-gpt-strikes-back-video-transformers-and-more-d620e1cec82?source=collection_archive---------15-----------------------#2021-04-01">https://towardsdatascience.com/best-of-arxiv-readings-for-april-2021-gpt-strikes-back-video-transformers-and-more-d620e1cec82?source=collection_archive---------15-----------------------#2021-04-01</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="6072" class="pw-subtitle-paragraph jo ip iq bd b jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf dk translated">ML论文每月精选</h2></div><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi kg"><img src="../Images/86c20428c444e6f028de61d34ac5e206.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8fOgvy73TIVa6WfsR7WFFw.png"/></div></div><p class="ks kt gj gh gi ku kv bd b be z dk translated">图片作者。</p></figure><p id="85e1" class="pw-post-body-paragraph kw kx iq ky b kz la js lb lc ld jv le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在你的阅读清单上保持领先是很难的，而找到哪些论文应该在清单上就更难了。在<a class="ae ls" href="https://search.zeta-alpha.com/" rel="noopener ugc nofollow" target="_blank"> Zeta Alpha </a>我们总是密切关注最新的人工智能研究，所以我们每月都会分享一些最近的论文，以展示我们认为会有影响力的出版物，主要基于每部作品的贡献和作者的影响力。不要把这个列表看得太全面:像其他人一样，我们也有自己的偏见，但是你能从4000多篇论文中选择的就这么多了。尽情享受吧！</p><h2 id="90e7" class="lt lu iq bd lv lw lx dn ly lz ma dp mb lf mc md me lj mf mg mh ln mi mj mk ml bi translated"><a class="ae ls" href="https://arxiv.org/abs/2103.10360" rel="noopener ugc nofollow" target="_blank"> 1。所有NLP任务都是生成任务:一个通用的预训练框架</a> |👾<a class="ae ls" href="https://github.com/THUDM/GLM" rel="noopener ugc nofollow" target="_blank">代号</a></h2><p id="8c44" class="pw-post-body-paragraph kw kx iq ky b kz mm js lb lc mn jv le lf mo lh li lj mp ll lm ln mq lp lq lr ij bi translated"><em class="mr">作者:杜，钱玉洁，小刘，，，邱，，杨，。</em></p><p id="53a8" class="pw-post-body-paragraph kw kx iq ky b kz la js lb lc ld jv le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir"> 🎖Why → </strong>本文是现有的三种主要语言预训练方法的地图:自回归(例如，擅长文本生成的GPTs)，掩蔽语言建模(又称填空，如BERT，擅长NLU分类任务)和seq2seq(针对T5等编码器-解码器模型，擅长翻译或摘要等条件文本生成)。这三种技术各有优缺点，所以如果我们能得到最好的结果，那不是很好吗？这里有一个尝试。</p><p id="30a6" class="pw-post-body-paragraph kw kx iq ky b kz la js lb lc ld jv le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">💡关键见解→ </strong>下表总结了三种主要语言预处理方法的主要用途。提醒一下，NLU是基准测试中的分类任务，比如<a class="ae ls" href="https://super.gluebenchmark.com" rel="noopener ugc nofollow" target="_blank"> SuperGLUE </a>(情感分析，自然语言推理等。)，条件生成是文本生成任务，其中输入和输出序列之间存在特定的关系(如翻译或总结文本)，而无条件生成是自由生成文本的任务。</p><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi ms"><img src="../Images/383786ab32d4b95074b076eab2b26e4c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*el7f9X618iNGuilvV800aQ.png"/></div></div><p class="ks kt gj gh gi ku kv bd b be z dk translated">其中\003; =擅长；—=可以适应；✕=cannot被直接应用到。资料来源:https://arxiv.org/pdf/2103.10360.pdf</p></figure><p id="73e1" class="pw-post-body-paragraph kw kx iq ky b kz la js lb lc ld jv le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">作者提出了一种<em class="mr">统一的</em>预训练技术，他们称之为通用语言模型(GLM)，并在图描述中进行了精确总结。</p><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi mt"><img src="../Images/4a13f6aee1d0bbfd7462496179d87f9f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Q86esuI5FTyhGJN-fI_0cg.png"/></div></div><p class="ks kt gj gh gi ku kv bd b be z dk translated">来源:<a class="ae ls" href="https://arxiv.org/pdf/2103.10360.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/2103.10360.pdf</a></p></figure><p id="8390" class="pw-post-body-paragraph kw kx iq ky b kz la js lb lc ld jv le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">部分A和B的这种分离背后的动机是迫使同一模型学习双向编码器(A)和单向解码器(B)。以前基于跨度的模型(如spanBERT)之间的一个区别是，跨度的长度现在对模型来说是未知的。这项技术需要一些技巧和细节来解决，比如位置编码，这在本文中有详细说明。</p><p id="8d73" class="pw-post-body-paragraph kw kx iq ky b kz la js lb lc ld jv le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">说到结果，与RoBERTA的比较可能是最有趣的比较之一，其中采用这种新的预训练方法的相同模型优于原始实现。在某些情况下，把最初的MLM训练目标和GLM混合起来更好，这表明GLM并不是普遍优越的。对于seq2seq评估，他们执行抽象概括，与类似大小的模型相比，它表现良好。</p><h2 id="979e" class="lt lu iq bd lv lw lx dn ly lz ma dp mb lf mc md me lj mf mg mh ln mi mj mk ml bi translated"><a class="ae ls" href="https://arxiv.org/abs/2103.10385" rel="noopener ugc nofollow" target="_blank"> 2。GPT也明白了👾</a><a class="ae ls" href="https://github.com/THUDM/P-tuning" rel="noopener ugc nofollow" target="_blank">代号</a></h2><p id="1131" class="pw-post-body-paragraph kw kx iq ky b kz mm js lb lc mn jv le lf mo lh li lj mp ll lm ln mq lp lq lr ij bi translated">【作者:】刘潇、延安郑、杜、、钱玉洁、杨、。</p><p id="fb59" class="pw-post-body-paragraph kw kx iq ky b kz la js lb lc ld jv le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">🎖Why → 我可以自信地把这篇文章放在我过去一个月的首要任务中。这个想法既聪明又简单，结果看起来非常惊人，论文非常清晰，充满了真知灼见。它挑战了来自同一个研究小组的前一篇论文提出的一个既定事实:自回归预训练对NLU没有好处。好吧，拿着你的纸！继续阅读……他们提出的技术<em class="mr"> p-tuning </em>，有可能成为少量学习和微调大型LMs的标准技术，对于这些技术，传统的微调效果不是很好，或者成本太高。</p><p id="df31" class="pw-post-body-paragraph kw kx iq ky b kz la js lb lc ld jv le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">💡关键见解→</strong>2020年5月，GPT-3让最持怀疑态度的人感到惊讶，它展示了一个简单的生成性预训练如何扩展到数千亿个参数，只需通过用描述任务的自然语言和/或给它一些例子来“提示”模型，就可以显示出令人印象深刻的零击球和少击球性能。这激发了一些深入探究“提示”艺术的作品，如PET⁴.甚至有人提出了自动为模型寻找好的提示来解决任务的技术，而不需要像AutoPrompt那样更新任何模型参数。</p><p id="b1d2" class="pw-post-body-paragraph kw kx iq ky b kz la js lb lc ld jv le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在这项工作中，作者有一个绝妙的想法，不再将提示限制为固定词汇表中的实际单词。相反，他们学习固定数量的<strong class="ky ir">连续嵌入，可以通过梯度下降</strong>进行优化，他们称之为<em class="mr"> p-tuning </em>。这意味着所有的原始模型参数可以保持冻结，只有提示嵌入被更新。把这想象成某种<em class="mr">差异化编程2.0 </em>很有趣，在这里你<strong class="ky ir">学会解释一个冻结的预训练</strong>模型做什么。</p><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi mu"><img src="../Images/09546cce129de21030b69591ca336069.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*W6JCr4eSBEaLk8Cde4TnXw.png"/></div></div><p class="ks kt gj gh gi ku kv bd b be z dk translated">来源:<a class="ae ls" href="https://arxiv.org/pdf/2103.10385.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/2103.10385.pdf</a></p></figure><p id="f2e5" class="pw-post-body-paragraph kw kx iq ky b kz la js lb lc ld jv le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在微调、p-tuning和手动提示之间的比较中，结果是最有趣的。特别是对于知识探测(从一个冻结的预训练模型中提取仿真陈述)，p-tuning比其他方法表现得好得多。在SuperGLUE基准测试中，虽然p-tuning不能与其他SOTA相提并论(但考虑到那里巨大的模型大小差异，这不是一个公平的比较)，但与标准微调或手动提示相比，p-tuning显示出非常强的性能。</p><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi mv"><img src="../Images/fe9638e33ed5ebccb94a3dbfcfe35b5a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*H8ovOWpOw7ijXIsFYxYroA.png"/></div></div></figure><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi mw"><img src="../Images/18d306d91cc4097d3fe90b594bd26005.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*z-rN2SBarIDMVgRIL0vVWg.png"/></div></div><p class="ks kt gj gh gi ku kv bd b be z dk translated">来源:<a class="ae ls" href="https://arxiv.org/pdf/2103.10385.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/2103.10385.pdf</a></p></figure><h2 id="ad41" class="lt lu iq bd lv lw lx dn ly lz ma dp mb lf mc md me lj mf mg mh ln mi mj mk ml bi translated"><a class="ae ls" href="https://arxiv.org/abs/2103.05247" rel="noopener ugc nofollow" target="_blank"> 3。作为通用计算引擎的预训练变压器</a> |👾<a class="ae ls" href="https://github.com/kzl/universal-computation" rel="noopener ugc nofollow" target="_blank">代号</a> | ✍️ <a class="ae ls" href="https://bair.berkeley.edu/blog/2021/03/23/universal-computation/" rel="noopener ugc nofollow" target="_blank">博客</a></h2><p id="17ad" class="pw-post-body-paragraph kw kx iq ky b kz mm js lb lc mn jv le lf mo lh li lj mp ll lm ln mq lp lq lr ij bi translated">凯文·卢、阿迪蒂亚·格罗弗、彼得·阿贝耳和伊戈尔·莫达奇。</p><p id="176e" class="pw-post-body-paragraph kw kx iq ky b kz la js lb lc ld jv le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">🎖Why → 尽管在我看来，这篇论文的核心主张仍未确定(细节决定成败),但将预先训练好的变形金刚理解为“计算引擎”的想法很有意思——给出适当的指令，它可以计算出任何东西。在我看来，这与“GPT也理解”的论文有很好的联系，在这篇论文中，学习了任务的模型输入，而不是模型参数。</p><p id="31dc" class="pw-post-body-paragraph kw kx iq ky b kz la js lb lc ld jv le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">💡关键见解→ </strong>作者探索了变形金刚如何执行各种不同寻常的任务，特别是计算任务:位存储(重复损坏的位串)、位异或(计算两个位串的元素异或，这是NNs历史上很难执行的事情)、ListOps(给定一系列预测结果位的操作)、MNIST(手写数字数据集)和CIFAR-10(图像分类基准)。</p><p id="25c2" class="pw-post-body-paragraph kw kx iq ky b kz la js lb lc ld jv le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">更有趣的是，他们声称有一些关于语言的预训练，使它学习所有这些其他任务的普遍性(这些任务事先与语言无关)。为了研究这一假设，他们获得了一个预训练的语言转换器，<strong class="ky ir">冻结了除层归一化和输入和位置嵌入之外的所有权重，</strong>称之为冻结预训练转换器(FPT)。</p><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi mx"><img src="../Images/d6cf7ede8ea9856a8696c100e6c266e9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hWz6KJud9mSpgBmOO6q1Wg.png"/></div></div><p class="ks kt gj gh gi ku kv bd b be z dk translated">来源:https://arxiv.org/pdf/2103.05247.pdf<a class="ae ls" href="https://arxiv.org/pdf/2103.05247.pdf" rel="noopener ugc nofollow" target="_blank"/></p></figure><p id="1318" class="pw-post-body-paragraph kw kx iq ky b kz la js lb lc ld jv le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">问题出在细节上，因为允许规范化层中的微调仍然会影响自我注意力在未来层中的行为，尽管自我注意力被冻结，但这种微调还是会隐式地优化它们(参见表11中它对性能的影响)。此外，通过微调嵌入、输出和层范数参数，随机初始化的转换器已经在许多任务上表现得非常好。</p><p id="ce83" class="pw-post-body-paragraph kw kx iq ky b kz la js lb lc ld jv le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">无论如何，尽管论文关于变压器是通用计算引擎的核心主张仍有争议，但论文充满了消融研究——如下表所示——这些研究很新颖，并提供了深刻的结果，以了解它们擅长什么，不擅长什么。</p><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi my"><img src="../Images/d280ff43092534576341680998ac4424.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MO7RXkxKM-vTxGgCGqIFug.png"/></div></div><p class="ks kt gj gh gi ku kv bd b be z dk translated">来源:https://arxiv.org/pdf/2103.05247.pdf<a class="ae ls" href="https://arxiv.org/pdf/2103.05247.pdf" rel="noopener ugc nofollow" target="_blank"/></p></figure><h2 id="a1f3" class="lt lu iq bd lv lw lx dn ly lz ma dp mb lf mc md me lj mf mg mh ln mi mj mk ml bi translated"><a class="ae ls" href="https://arxiv.org/abs/2103.03841" rel="noopener ugc nofollow" target="_blank"> 4。生成具有稀疏表示的图像</a></h2><p id="19bf" class="pw-post-body-paragraph kw kx iq ky b kz mm js lb lc mn jv le lf mo lh li lj mp ll lm ln mq lp lq lr ij bi translated"><em class="mr">作者:查理·纳什、雅各布·梅尼克、桑德·迪勒曼和彼得·w·巴塔格利亚</em></p><p id="1184" class="pw-post-body-paragraph kw kx iq ky b kz la js lb lc ld jv le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir"> 🎖Why → </strong>提醒您，用于压缩的离散余弦变换(DCT)图像处理等经典众所周知的技术可以增强图像生成等ML任务。</p><p id="b789" class="pw-post-body-paragraph kw kx iq ky b kz la js lb lc ld jv le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">💡关键见解→ </strong>部分受最近基于可能性的图像生成模型(如OpenAI或VQ-VAE⁶的达尔·e⁵)的成功启发，本文探索了稀疏表示在任务中的应用。与GANs相比，基于似然性的生成模型的优点之一是，它们训练起来更稳定，并且也没有陷入没有覆盖图像分布的整个空间的模式的风险。使用稀疏表示的动机是它们易于压缩(有很多0！)，研究神经网络在这种表示空间中的表现非常有趣，与常见的图像网格结构形成对比。</p><p id="d4a1" class="pw-post-body-paragraph kw kx iq ky b kz la js lb lc ld jv le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我个人不知道JPEG压缩中使用的DCT变换，它真的很酷。以手动方式，您可以将图像分割成几个像素(即8×8)的块，然后将所有像素值拟合到2D中基于余弦的函数中，8×8 = 64个自由度。这将图像块表示为由64个系数加权的64个“频率”函数的叠加。这些系数中的大部分可以在不影响感知图像质量的情况下被移除(我们人类不会看到很多小的高频信息)，这导致了易于压缩的稀疏表示(这就是为什么它被用于JPEG文件压缩)。看完<a class="ae ls" href="https://www.youtube.com/watch?v=Q2aEzeMDHMA" rel="noopener ugc nofollow" target="_blank">这段精彩的DCT介绍视频</a>后，这篇论文会更有意义。</p><p id="3b32" class="pw-post-body-paragraph kw kx iq ky b kz la js lb lc ld jv le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">本文构建的图像表示由一个来自这个DCT变换的所有非零稀疏系数的<strong class="ky ir">列表(经过一些特殊的量化技巧，但您可以得到它的要点)以及它们的通道和位置信息组成。该模型被训练为自回归预测这些元组，考虑它们的值分类，以自我监督的方式最大化可能性。</strong></p><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi mz"><img src="../Images/fa440eaf3eb8c190fd7389f2948076bc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eTxoocDBDmq_DQO-A84mvw.png"/></div></div><p class="ks kt gj gh gi ku kv bd b be z dk translated">来源:https://arxiv.org/pdf/2103.03841.pdf<a class="ae ls" href="https://arxiv.org/pdf/2103.03841.pdf" rel="noopener ugc nofollow" target="_blank"/></p></figure><p id="702b" class="pw-post-body-paragraph kw kx iq ky b kz la js lb lc ld jv le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">就结果而言，它们总体来说非常好；即使没有超过SOTA，也可以与之相提并论(除了在比根仍然统治的阶级条件下)。然而，让我们不要忘记，这些指标只是人类判断质量的代理，所以用你自己的眼睛检查结果！</p><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi na"><img src="../Images/d57594a48372ff1e4b4e2089dc49b36b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TQPClil-J9MrQrnQo8oM4w.png"/></div></div><p class="ks kt gj gh gi ku kv bd b be z dk translated">来源:<a class="ae ls" href="https://arxiv.org/pdf/2103.03841.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/2103.03841.pdf</a></p></figure><h2 id="709b" class="lt lu iq bd lv lw lx dn ly lz ma dp mb lf mc md me lj mf mg mh ln mi mj mk ml bi translated"><a class="ae ls" href="https://arxiv.org/abs/2103.15691" rel="noopener ugc nofollow" target="_blank"> 5。ViViT:视频视觉转换器</a></h2><p id="3ae1" class="pw-post-body-paragraph kw kx iq ky b kz mm js lb lc mn jv le lf mo lh li lj mp ll lm ln mq lp lq lr ij bi translated"><em class="mr">作者:阿努拉格·阿纳、穆斯塔法·德赫加尼、格奥尔格·海戈尔德、孙辰、马里奥·卢奇奇和科迪莉亚·施密德。</em></p><p id="bb07" class="pw-post-body-paragraph kw kx iq ky b kz la js lb lc ld jv le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir"> 🎖Why → </strong>变形金刚<em class="mr">征服</em>的又一个任务(这可能是一个独立的部分)。给定足够的参数和数据(以及适当的扩充)，似乎没有变压器不能破解的任务。</p><p id="8fb9" class="pw-post-body-paragraph kw kx iq ky b kz la js lb lc ld jv le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">💡关键见解→ </strong>本文基于现有的视觉转换器(ViT ),例如“一幅图像相当于16x16 words"⁷”,并尝试使用不同的策略来同时表示空间和时间维度。这项工作最有趣的一个方面是概述不同的策略来标记视频并对它们应用变形层。首先，在对视频进行标记时，他们解释了统一帧采样与小管嵌入，见下图。</p><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi nb"><img src="../Images/951a0f89e89e2207d707ad838a3d17e6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*65sL3Smb6sBf80NtyrKYqQ.png"/></div></div><p class="ks kt gj gh gi ku kv bd b be z dk translated">来源:https://arxiv.org/pdf/2103.15691.pdf</p></figure><p id="6c37" class="pw-post-body-paragraph kw kx iq ky b kz la js lb lc ld jv le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">其次，在计算跨空间和时间的注意力时，他们提出了4种替代方案:时空注意力(一切关注一切)、因式分解的编码器(首先仅空间变换器，然后时间)、因式分解的自我注意力(每个变换器块具有空间然后时间的自我注意力块)、因式分解的点积注意力(一个具有空间头部和时间头部的自我注意力随后被连接)。</p><p id="99e7" class="pw-post-body-paragraph kw kx iq ky b kz la js lb lc ld jv le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">他们的消融显示，如果训练良好，4种不同的模型并没有<em class="mr">那么</em>不同，并且在图像数据集上利用预先训练的变压器很有帮助。事实上，他们并没有真正详细地披露他们是如何预先训练他们的模型的，只是说他们是在ImageNet或JFT数据集上进行的。他们在多个视频分类基准测试中取得了一流的性能，包括Kinetics 400和600、Epic Kitchens、Something-Something v2和Moments in Time。标签平滑、混合和随机深度等增强和技巧仍然是实现这一性能的关键，正如它们的消融所示。</p><h2 id="f7b0" class="lt lu iq bd lv lw lx dn ly lz ma dp mb lf mc md me lj mf mg mh ln mi mj mk ml bi translated"><a class="ae ls" href="https://arxiv.org/abs/2103.03206" rel="noopener ugc nofollow" target="_blank"> 6。感知者:具有重复注意的一般感知</a></h2><p id="c0ea" class="pw-post-body-paragraph kw kx iq ky b kz mm js lb lc mn jv le lf mo lh li lj mp ll lm ln mq lp lq lr ij bi translated">安德鲁·耶格尔、菲利克斯·吉梅诺、安德鲁·布洛克、安德鲁·塞斯曼、奥里奥尔·维尼亚尔斯和若昂·卡雷拉。</p><p id="a7a4" class="pw-post-body-paragraph kw kx iq ky b kz la js lb lc ld jv le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir"> 🎖Why → </strong>对数据进行尽可能少的假设是有趣的，因为它有潜力很好地转移到许多领域。在这种情况下，<em class="mr">感知者</em>是一个关注可伸缩性(消除自我关注的讨厌的N次方),并对数据结构做最小假设的架构。</p><p id="2be7" class="pw-post-body-paragraph kw kx iq ky b kz la js lb lc ld jv le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">💡关键见解→ </strong>感知者架构包括重复以下架构模块，包括:</p><ul class=""><li id="4fee" class="nc nd iq ky b kz la lc ld lf ne lj nf ln ng lr nh ni nj nk bi translated">潜在表示(大小为NxD，长度为嵌入大小)和数据的原始表示(大小为MxC，长度为通道)之间的交叉关注步骤。这使得交叉注意力具有NxM而不是MxM的复杂度，这在N&lt;<m./></li><li id="92a5" class="nc nd iq ky b kz nl lc nm lf nn lj no ln np lr nh ni nj nk bi translated">A transformer layer that maps a latent representation to another latent representation of the same shape (see figure below).</li></ul><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi nq"><img src="../Images/2c104a149aedb8120272d779c23232ff.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LrXIg1EkNYlhok0W0JA-sw.png"/></div></div><p class="ks kt gj gh gi ku kv bd b be z dk translated">Source: <a class="ae ls" href="https://arxiv.org/pdf/2103.03206.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/2103.03206.pdf</a>时是相当大的</p></figure><p id="45b6" class="pw-post-body-paragraph kw kx iq ky b kz la js lb lc ld jv le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这可以被认为是将原始表示重复缩小为潜在表示。假设在该实现中，块共享它们的权重，则可以认为是展开的RNN。实际上，在附录中我们可以看到重量共享和非重量共享之间的比较，其中前者达到更好的性能，因为它不像非重量共享那样过度拟合；这种重量分担导致44M参数模型。</p><p id="475e" class="pw-post-body-paragraph kw kx iq ky b kz la js lb lc ld jv le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">作者对各种形式进行了实验:图像、原始音频、视频、原始音频+视频和点云。虽然结果部分不是很全面，但性能与现有模型相当或更好，特别是与现有的多模态模型相比(例如，ImageNet top-1上的85.7%)。结果令人印象深刻，但我们不能忘记细节:虽然架构对所有模态保持相同，但需要一些<strong class="ky ir">模态特定的扩展和位置嵌入</strong>来实现它(裁剪、特殊位置编码等)。)</p><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi nr"><img src="../Images/0fde0c393107de6e127e82ba098a143f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lclyvtphx5mxubpan_Ec3g.png"/></div></div><p class="ks kt gj gh gi ku kv bd b be z dk translated">顶级ImageNet性能。红色的方法利用了特定领域的“图像<em class="jn">网格结构”,而蓝色的结果则没有。来源:</em>【https://arxiv.org/pdf/2103.03206.pdf】T4</p></figure><h2 id="7fcf" class="lt lu iq bd lv lw lx dn ly lz ma dp mb lf mc md me lj mf mg mh ln mi mj mk ml bi translated"><a class="ae ls" href="https://arxiv.org/abs/2103.03404" rel="noopener ugc nofollow" target="_blank"> 7。注意力并不是你所需要的全部:纯粹的注意力会随着深度成倍地下降👾</a><a class="ae ls" href="https://github.com/twistedcubic/attention-rank-collapse" rel="noopener ugc nofollow" target="_blank">代码</a></h2><p id="2c41" class="pw-post-body-paragraph kw kx iq ky b kz mm js lb lc mn jv le lf mo lh li lj mp ll lm ln mq lp lq lr ij bi translated"><em class="mr">作者董一禾、让·巴普蒂斯特·科多尼尔和安德烈亚斯·卢卡斯。</em></p><p id="3ad6" class="pw-post-body-paragraph kw kx iq ky b kz la js lb lc ld jv le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir"> 🎖Why → </strong>偶尔一篇理论论文不会要了我们的命，有时它们会提供有价值的见解，而不仅仅是到处都是可怕的核心数学。这就是一个这样的例子:为什么这些跳跃连接如此重要？</p><p id="f9e4" class="pw-post-body-paragraph kw kx iq ky b kz la js lb lc ld jv le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">💡关键见解→ </strong>感谢上帝，我们有这些跳过连接。注意<strong class="ky ir">和跳过连接</strong>是你需要的全部*。</p></div><div class="ab cl ns nt hu nu" role="separator"><span class="nv bw bk nw nx ny"/><span class="nv bw bk nw nx ny"/><span class="nv bw bk nw nx"/></div><div class="ij ik il im in"><p id="5382" class="pw-post-body-paragraph kw kx iq ky b kz la js lb lc ld jv le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">好吧，让我们扩展一下，你可能听说过跳过连接(或残差)有助于通过更深的网络传播梯度稳定训练。这篇论文提供了一个理论基础，解释了为什么这在变形金刚中如此重要:没有变形金刚，自我注意力输出可以证明会非常快速地退化——通过SGD<em class="mr">成双指数级——</em>，这意味着它会变成一个秩1矩阵，其中<em class="mr">会杀死流经它的</em>信息(即，想象一个嵌入序列，其中所有嵌入都是彼此的倍数)。</p><p id="12ae" class="pw-post-body-paragraph kw kx iq ky b kz la js lb lc ld jv le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这篇论文的主要观点与其说是惊人的见解，不如说是对现有怀疑的确认。一些工作已经根据经验展示了如何将注意力矩阵分解成对性能影响最小的低秩矩阵，例如“Linformer"⁸.”</p><h2 id="a9b5" class="lt lu iq bd lv lw lx dn ly lz ma dp mb lf mc md me lj mf mg mh ln mi mj mk ml bi translated"><a class="ae ls" href="https://arxiv.org/abs/2103.06089" rel="noopener ugc nofollow" target="_blank"> 8。可变速率离散表示学习</a> |⏯<a class="ae ls" href="https://vdrl.github.io" rel="noopener ugc nofollow" target="_blank">演示</a></h2><p id="d35a" class="pw-post-body-paragraph kw kx iq ky b kz mm js lb lc mn jv le lf mo lh li lj mp ll lm ln mq lp lq lr ij bi translated"><em class="mr">作者:桑德·迪勒曼、查理·纳什、杰西·恩格尔和卡伦·西蒙扬。</em></p><p id="0fb9" class="pw-post-body-paragraph kw kx iq ky b kz la js lb lc ld jv le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">🎖Why → 可变利率表示法的想法令我着迷。直觉上，听和理解口语，信息并不是均匀分布的，我们的表征又何必如此呢？这提出了许多挑战，但很高兴看到研究解决了这个问题。</p><p id="c499" class="pw-post-body-paragraph kw kx iq ky b kz la js lb lc ld jv le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">💡关键见解→ </strong>这项工作构建了基于<em class="mr">事件的</em>表示，涉及随时间量化的编码器解码器架构，经过训练后，解码器输出的对数似然性最大化，以适应量化的潜在表示。“慢度惩罚”激励潜在表示保持与前一时间步相同的值；这种惩罚是由明确强加容量瓶颈的想法所激发的。他们使用的另一个技巧是<em class="mr">施密特触发量化</em>:由于噪声，量化值可能会跳跃太多，因此STQ会施加一个记忆量化，只有当变量变化超过一定量时才会跳跃。</p><p id="eea4" class="pw-post-body-paragraph kw kx iq ky b kz la js lb lc ld jv le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">给定这种设置，直觉是量子化的潜在表示应该只在有<em class="mr">事件时改变。</em>例如，如果有2秒钟的沉默，表征可能会保持不变，但如果有人在说话，平均事件发生率(AER，潜在表征的变化)应该会更高。将编码器和解码器参数化的神经网络是一个“鼓形变压器”,要完成这项工作，还需要本文中详述的更多技巧。</p><p id="6d1f" class="pw-post-body-paragraph kw kx iq ky b kz la js lb lc ld jv le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">关于结果，最有趣的部分是关于所有超参数的烧蚀，如慢度、AER、量子化水平等。与现有工作的比较不像人们所希望的那样广泛，但这主要是因为口语建模的自动评估不是非常可靠。</p><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi nz"><img src="../Images/c430283c7b4bbbeee5b3fe1eb4ce54f7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KgClP8HG-R6rkZRrZeI_aA.png"/></div></div><p class="ks kt gj gh gi ku kv bd b be z dk translated">来源:<a class="ae ls" href="https://arxiv.org/pdf/2103.06089.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/2103.06089.pdf</a></p></figure><h2 id="7f2b" class="lt lu iq bd lv lw lx dn ly lz ma dp mb lf mc md me lj mf mg mh ln mi mj mk ml bi translated"><a class="ae ls" href="https://arxiv.org/abs/2103.03230" rel="noopener ugc nofollow" target="_blank"> 9。巴洛双胞胎:通过减少冗余进行自我监督学习</a> |👾<a class="ae ls" href="https://github.com/facebookresearch/barlowtwins" rel="noopener ugc nofollow" target="_blank">代号</a></h2><p id="407c" class="pw-post-body-paragraph kw kx iq ky b kz mm js lb lc mn jv le lf mo lh li lj mp ll lm ln mq lp lq lr ij bi translated">作者:Jure Zbontar、李静、Ishan Misra、Yann LeCun和Stéphane Deny。</p><p id="44bd" class="pw-post-body-paragraph kw kx iq ky b kz la js lb lc ld jv le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir"> 🎖Why → </strong>这是一个新的自我监督的损失！它非常简单，可与其他SOTA表示学习技术(SimCLR，BYOL)相媲美，并呈现了一些有趣的特性，值得进一步研究…</p><p id="691d" class="pw-post-body-paragraph kw kx iq ky b kz la js lb lc ld jv le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">💡关键见解→ </strong>我喜欢将它概念化的方式是在“每个特征”的基础上进行某种对比学习(但是不要做太多的类比，因为这是不正确的)。您有一个图像的两个视图(假设两个作物)，您最大化每个特征的相关性，同时最小化其余特征之间的相关性。您也可以将此视为计算两个表示的外积(估计两个表示的互相关)，对整个批次求和并归一化，并使其尽可能接近一个单位矩阵。</p><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi oa"><img src="../Images/f9b0a65e73e3352de03f003a7d2cd987.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rdv6IptOvEm5tdWSdMZwpg.png"/></div></div></figure><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi ob"><img src="../Images/b65c9a1a3335cc1e0382a1f16c8f5759.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0upQlhddljF3l_hupFrUvA.png"/></div></div><p class="ks kt gj gh gi ku kv bd b be z dk translated">巴洛双胞胎损失函数。</p></figure><p id="67e7" class="pw-post-body-paragraph kw kx iq ky b kz la js lb lc ld jv le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这一目标的理论依据可以追溯到1961⁹的神经科学家H. Barlow，他假设处理感官信息的目标是将其重新编码为<em class="mr">阶乘代码</em>，这意味着具有令人满意的独立成分的表示。巴洛孪晶损失函数受这一想法的启发，因为它鼓励表示只与每个分量<strong class="ky ir">相关，而不是全局相关。</strong></p><p id="6cb9" class="pw-post-body-paragraph kw kx iq ky b kz la js lb lc ld jv le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">其结果可与现有的表示学习技术相媲美，如BYOL和SimCLR，但它有几个有趣的特性。首先，与BYOL不同，它对较小的批量(小= 256，512)似乎很稳健；对于大批量(2048，4096)，它实际上会降级！我们就此询问了作者，他们告诉我们他们也很困惑。第二，表示维度似乎没有饱和，与比较的方法不同，它不断改进下游性能。</p><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi oc"><img src="../Images/8cac5344444019ecf47278b37af972b1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Y15TS64JgGz-osot3MkuSg.png"/></div></div><p class="ks kt gj gh gi ku kv bd b be z dk translated">来源:<a class="ae ls" href="https://arxiv.org/pdf/2103.03230.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/2103.03230.pdf</a></p></figure><h2 id="5452" class="lt lu iq bd lv lw lx dn ly lz ma dp mb lf mc md me lj mf mg mh ln mi mj mk ml bi translated"><a class="ae ls" href="https://arxiv.org/abs/2102.12627" rel="noopener ugc nofollow" target="_blank"> 10。如何在神经网络中表示部分-整体层次</a></h2><p id="a16b" class="pw-post-body-paragraph kw kx iq ky b kz mm js lb lc mn jv le lf mo lh li lj mp ll lm ln mq lp lq lr ij bi translated">杰弗里·辛顿。</p><p id="43bb" class="pw-post-body-paragraph kw kx iq ky b kz la js lb lc ld jv le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir"> 🎖Why → </strong>深度学习的创始人之一押注于计算机视觉的关键挑战是什么以及如何解决这些挑战。但是没有提供一个工作系统。(还没？).</p><p id="9fa3" class="pw-post-body-paragraph kw kx iq ky b kz la js lb lc ld jv le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">💡关键见解→ </strong>论文提出的第一点是，人类将视觉场景解析为<em class="mr">部分-整体层次</em>，并在元素之间建立视点不变的空间关系。换句话说，我们将图像的一部分表示为一个层次结构，即什么东西属于什么东西(或者对象的子部分等等)，并且这些是视点不变的(我们将铅笔和纸建模为当我们四处移动时仍然是相同的<em class="mr"/>)。这项工作似乎是他的胶囊网络⁰想法的自然延伸，也试图捕捉不同层次的表现。</p><p id="46b7" class="pw-post-body-paragraph kw kx iq ky b kz la js lb lc ld jv le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">根据Hinton的说法，这里的主要问题是，当前的端到端神经网络不允许我们动态构建这些解析树，并动态分配神经元组来表示其中的节点。他设想的解决方案——GLOM——最好理解为处理一系列图像(或视频)。它由代表不同层次视觉结构的向量列迭代表示图像的块组成(即每个块约5个向量)。在每个时间步，这些列以不同的贡献进行更新:自下而上的预测(L-1到L)，自上而下的预测(L+1到L)，相同级别的预测，以及补丁邻域中嵌入的注意力加权平均。理想情况下，对其进行训练将在不同级别产生相同向量的<em class="mr">岛，对应于表示其部分-整体层次的图像的解析树。</em></p><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi od"><img src="../Images/f12c24dd9e1d0e152d59281f8ac993c6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*B1D8S4Ah5Np2bAr6AlRR2g.png"/></div></div><p class="ks kt gj gh gi ku kv bd b be z dk translated">来源:<a class="ae ls" href="https://arxiv.org/pdf/2102.12627.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/2102.12627.pdf</a></p></figure><p id="cd0b" class="pw-post-body-paragraph kw kx iq ky b kz la js lb lc ld jv le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">论文继续用生物学、数学和神经网络的观点来推动这一点；以及描述关于该系统如何以及为什么工作的许多考虑，这些考虑太长了，无法在此进行总结。</p><p id="e818" class="pw-post-body-paragraph kw kx iq ky b kz la js lb lc ld jv le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">最后，尽管这篇文章没有描述一个工作系统，但是有些人已经<a class="ae ls" href="https://github.com/lucidrains/glom-pytorch" rel="noopener ugc nofollow" target="_blank">实现了它</a>，所以来看看吧！</p></div><div class="ab cl ns nt hu nu" role="separator"><span class="nv bw bk nw nx ny"/><span class="nv bw bk nw nx ny"/><span class="nv bw bk nw nx"/></div><div class="ij ik il im in"><p id="d2c6" class="pw-post-body-paragraph kw kx iq ky b kz la js lb lc ld jv le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们的每月精选到此结束；如果你想了解最新的研究，请在Twitter上关注我们。我已经在期待着分享五月的下一个选择；回头见！</p></div><div class="ab cl ns nt hu nu" role="separator"><span class="nv bw bk nw nx ny"/><span class="nv bw bk nw nx ny"/><span class="nv bw bk nw nx"/></div><div class="ij ik il im in"><p id="8129" class="pw-post-body-paragraph kw kx iq ky b kz la js lb lc ld jv le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><em class="mr">参考文献</em></p><p id="1ac3" class="pw-post-body-paragraph kw kx iq ky b kz la js lb lc ld jv le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">[1] " <a class="ae ls" href="https://arxiv.org/abs/1905.00537" rel="noopener ugc nofollow" target="_blank"> SuperGLUE:通用语言理解系统的更具粘性的基准</a>"作者A. Wang，Y. Pruksachatkun，N. Nangia，A. Singh等人2019。</p><p id="87f6" class="pw-post-body-paragraph kw kx iq ky b kz la js lb lc ld jv le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">[2] " <a class="ae ls" href="https://arxiv.org/abs/1907.10529" rel="noopener ugc nofollow" target="_blank"> SpanBERT:通过表示和预测跨度来改善预训练</a>" Mandar Joshi，Chen等人2019。</p><p id="a0c6" class="pw-post-body-paragraph kw kx iq ky b kz la js lb lc ld jv le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">[3]“<a class="ae ls" href="https://arxiv.org/abs/1907.11692" rel="noopener ugc nofollow" target="_blank">RoBERTa:稳健优化的BERT预训练方法</a>”，作者刘，Myle Ott，Naman Goyal，杜等，2019。</p><p id="5791" class="pw-post-body-paragraph kw kx iq ky b kz la js lb lc ld jv le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">[4]“<a class="ae ls" href="https://arxiv.org/abs/2009.07118" rel="noopener ugc nofollow" target="_blank">重要的不仅仅是规模:小型语言模型也是很少尝试的学习者</a>”Timo Schick著，Hinrich Schütze，2020年。</p><p id="b776" class="pw-post-body-paragraph kw kx iq ky b kz la js lb lc ld jv le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">[5]“<a class="ae ls" href="https://arxiv.org/abs/2102.12092" rel="noopener ugc nofollow" target="_blank">零镜头文本到图像生成</a>”作者Aditya Ramesh、Mikhail Pavlov、Gabriel Goh、Scott Gray、Chelsea Voss、Alec拉德福德、陈唐山和Ilya Sutskever等人，2021。</p><p id="5312" class="pw-post-body-paragraph kw kx iq ky b kz la js lb lc ld jv le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">[6] " <a class="ae ls" href="https://arxiv.org/abs/1711.00937" rel="noopener ugc nofollow" target="_blank">神经离散表征学习</a>" Aaron van den Oord，Oriol Vinyals，Koray Kavukcuoglu等人2017。</p><p id="62eb" class="pw-post-body-paragraph kw kx iq ky b kz la js lb lc ld jv le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">[7]<a class="ae ls" href="https://arxiv.org/abs/2010.11929" rel="noopener ugc nofollow" target="_blank">《一幅图像抵得上16x16个字:大规模图像识别的变形金刚</a>》阿列克谢·多索维茨基、卢卡斯·拜尔、亚历山大·科列斯尼科夫、德克·韦森博恩、翟晓华等人2020。</p><p id="d9b7" class="pw-post-body-paragraph kw kx iq ky b kz la js lb lc ld jv le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">[8]“<a class="ae ls" href="https://arxiv.org/abs/2006.04768" rel="noopener ugc nofollow" target="_blank">林前者:线性复杂性的自我注意</a>”，作者:王思农、李贝琳达、马迪安·卡巴萨、和，2020。</p><p id="c166" class="pw-post-body-paragraph kw kx iq ky b kz la js lb lc ld jv le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">[9] " <a class="ae ls" href="https://mitpress.universitypressscholarship.com/view/10.7551/mitpress/9780262518420.001.0001/upso-9780262518420-chapter-13" rel="noopener ugc nofollow" target="_blank">感官信息转换的潜在原则</a>" Horace Barlow，1961。</p><p id="d1eb" class="pw-post-body-paragraph kw kx iq ky b kz la js lb lc ld jv le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">[10] " <a class="ae ls" href="https://arxiv.org/abs/1710.09829" rel="noopener ugc nofollow" target="_blank">胶囊间的动态路由</a>" Sara Sabour，Nicholas Frosst，和Geoffrey E. Hinton，2017。</p><p id="cb3d" class="pw-post-body-paragraph kw kx iq ky b kz la js lb lc ld jv le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">[11]“自动提示:通过自动生成的提示从语言模型中获取知识”T19，作者:Taylor Shin，Yasaman Razeghi，Robert L. Logan IV，Eric Wallace和Sameer Singh，2020年。</p></div></div>    
</body>
</html>