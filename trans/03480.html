<html>
<head>
<title>An AI agent learns to play tic-tac-toe (part 3): training a Q-learning RL agent</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">一个AI智能体学习玩井字游戏(第3部分):训练一个Q学习RL智能体</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/an-ai-agent-learns-to-play-tic-tac-toe-part-3-training-a-q-learning-rl-agent-2871cef2faf0?source=collection_archive---------10-----------------------#2021-03-20">https://towardsdatascience.com/an-ai-agent-learns-to-play-tic-tac-toe-part-3-training-a-q-learning-rl-agent-2871cef2faf0?source=collection_archive---------10-----------------------#2021-03-20</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="e3cc" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated"><em class="kf">我们用Q学习训练一个强化学习代理玩井字游戏</em></h2></div><p id="9cb7" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated"><em class="lc">本文是让计算机使用强化学习玩井字游戏系列的一部分。你可以在这里找到</em> <a class="ae ld" href="https://towardsdatascience.com/tagged/rl-series-paul" rel="noopener" target="_blank"> <em class="lc">所有文章</em> </a> <em class="lc">。我们的目标是提供一个完整的实现，您可以真正从中挑选并学习强化学习。按顺序阅读文章可能是最好的。文章包括所有代码</em> <a class="ae ld" href="https://github.com/PaulHiemstra/qlearning_paper/blob/master/tictoe_qlearning_paper.ipynb" rel="noopener ugc nofollow" target="_blank"> <em class="lc">都可以在Github </em> </a> <em class="lc">上找到。</em></p><h1 id="58e2" class="le lf iq bd lg lh li lj lk ll lm ln lo jw lp jx lq jz lr ka ls kc lt kd lu lv bi translated">强化学习回顾</h1><p id="51c5" class="pw-post-body-paragraph kg kh iq ki b kj lw jr kl km lx ju ko kp ly kr ks kt lz kv kw kx ma kz la lb ij bi translated">在本系列的第1部分和第2部分中，我们为我们的强化学习(RL)代理构建了对手，所以现在我们可以开始实际的RL代理了。在我们进入实际实现之前，我想花一些时间了解一下Q学习是如何工作的。</p><p id="d98b" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">正如我们在第1部分中了解到的，RL基于以下关键概念:</p><figure class="mc md me mf gt mg gh gi paragraph-image"><div role="button" tabindex="0" class="mh mi di mj bf mk"><div class="gh gi mb"><img src="../Images/e0894736d96f13db7f11b1c9740c2806.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FYZlgTQFjJRlwc5m_nYZNA.png"/></div></div></figure><p id="96f7" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">代理的目标是找到一个有效的<strong class="ki ir">策略</strong>，即在给定的情况下什么行动是最优的。在井字游戏中，这意味着给定棋盘上的状态，哪一步棋是最优的。请注意，该策略关注的是长期<strong class="ki ir">价值</strong> Q，而不仅仅是短期<strong class="ki ir">回报</strong> R。更数学地说，该策略是具有状态𝑠和行动𝑎的函数𝑄(𝑆,𝑎，强化学习是学习该函数的方式。</p><h1 id="14ba" class="le lf iq bd lg lh li lj lk ll lm ln lo jw lp jx lq jz lr ka ls kc lt kd lu lv bi translated">RL的列表方法</h1><p id="cc85" class="pw-post-body-paragraph kg kh iq ki b kj lw jr kl km lx ju ko kp ly kr ks kt lz kv kw kx ma kz la lb ij bi translated">Q-learning是一种常见的强化学习方法。这种方法的核心是Q表，它存储状态和动作的所有组合，本质上表示Q函数的离散版本。在本文中，我们使用字典存储qtable。给定某个boardstate，我们可以确定哪个动作具有最高的Q值:</p><figure class="mc md me mf gt mg"><div class="bz fp l di"><div class="mn mo l"/></div></figure><pre class="mc md me mf gt mp mq mr ms aw mt bi"><span id="70d7" class="mu lf iq mq b gy mv mw l mx my">'e'</span></pre><p id="a032" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">给定<code class="fe mz na nb mq b">board_state_1</code>，我们能做的最好的移动是<code class="fe mz na nb mq b">e</code>(中间的方块)，因为它有最高的Q值。</p><p id="1c96" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">定义了Q表后，我们的主要挑战是学习适当的Q值。以下伪代码描述了学习过程:</p><pre class="mc md me mf gt mp mq mr ms aw mt bi"><span id="93b0" class="mu lf iq mq b gy mv mw l mx my">for all episodes:<br/>    if not boardstate in qtable:<br/>        qtable[boardstate] = 0 for all actions<br/>    if (pick random action with chance epsilon)<br/>        pick random action<br/>    else<br/>        pick action with highest q value for boardstate<br/>    Q[boardstate, action] = update_qtable(Q[boardstate, action], <br/>                                          reward for action, <br/>                                          Q[next_boardstates])<br/>    tree search makes move</span></pre><p id="798d" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">于是我们就对着树搜索玩了很多井字游戏(剧集)，慢慢更新Q值。一些重要的观察结果:</p><ul class=""><li id="4f54" class="nc nd iq ki b kj kk km kn kp ne kt nf kx ng lb nh ni nj nk bi translated">我们用<code class="fe mz na nb mq b">0</code>初始化我们的qtable，开始时没有清楚的指示什么是好的移动。</li><li id="2c4f" class="nc nd iq ki b kj nl km nm kp nn kt no kx np lb nh ni nj nk bi translated">如果存在多个具有最大Q值的移动，我们从这些移动中选择一个随机动作。这是为了刺激对boardstates的探索。在我们训练过程的开始，有多个最大值是很常见的。</li><li id="fe9a" class="nc nd iq ki b kj nl km nm kp nn kt no kx np lb nh ni nj nk bi translated">我们并不总是从q表中选择最优的移动(剥削)，而是随机选择𝜖.这是为了通过探索可能会有好的回报的随机新板状态来刺激学习过程。这确保我们不会很快陷入局部最小值，而是继续寻找更好的移动。这种开发与探索的动态是有效学习Q表的重要部分。</li><li id="680b" class="nc nd iq ki b kj nl km nm kp nn kt no kx np lb nh ni nj nk bi translated">如果我们赢了，奖励是+10分，如果我们输了，奖励是-10分，如果我们平了，奖励是+5分。</li></ul><h1 id="7124" class="le lf iq bd lg lh li lj lk ll lm ln lo jw lp jx lq jz lr ka ls kc lt kd lu lv bi translated">Q学习的魔力</h1><p id="bfd8" class="pw-post-body-paragraph kg kh iq ki b kj lw jr kl km lx ju ko kp ly kr ks kt lz kv kw kx ma kz la lb ij bi translated">Q学习中最重要的步骤是更新表中的Q值。下面的伪等式描述了它的工作原理:</p><pre class="mc md me mf gt mp mq mr ms aw mt bi"><span id="9fce" class="mu lf iq mq b gy mv mw l mx my">new Q value = old Q value + instantaneous reward + maximum Q for next states after the action we took</span></pre><p id="ea9f" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">因此，新的Q值考虑了以前的Q值，我们在状态<code class="fe mz na nb mq b">S</code>采取行动<code class="fe mz na nb mq b">a</code>得到的奖励，以及在状态<code class="fe mz na nb mq b">S</code>采取<code class="fe mz na nb mq b">a</code>行动后可能的Q值<em class="lc">。请注意，要了解长期Q值，我们只需预测下一个可能的最大Q值。这与树搜索方法形成了鲜明的对比，树搜索方法一直向前看，直到游戏结束。对我来说，通过只看一步棋就能获得长期回报，这让Q-learning变得相当神奇。当然，不利的一面是我们需要玩很多游戏来慢慢地让所有的Q值在Q表中传播。</em></p><p id="0d64" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">以下等式更正式地表达了更新规则<a class="ae ld" href="https://en.wikipedia.org/wiki/Bellman_equation" rel="noopener ugc nofollow" target="_blank"/>:</p><figure class="mc md me mf gt mg gh gi paragraph-image"><div role="button" tabindex="0" class="mh mi di mj bf mk"><div class="gh gi nq"><img src="../Images/015250faed58ccc55181f45c489060b4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PHJEdzUXlfQIKDdKVEDv3w.png"/></div></div></figure><p id="0744" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">其中(1−𝛼)𝑄(𝑠_𝑡,𝑎_𝑡)等于在𝑠_𝑡州𝑎_𝑡采取行动的旧q值，𝛼𝑟_𝑡等于瞬时回报，𝛼𝛾 max(𝑠_𝑡+1,𝑎)在采取行动<code class="fe mz na nb mq b">a</code>并在𝑠_𝑡+1.州结束后，我们下一步行动可以得到的最大q值该等式包括多个系数:</p><ul class=""><li id="4891" class="nc nd iq ki b kj kk km kn kp ne kt nf kx ng lb nh ni nj nk bi translated">𝛼学习率，它决定了给定新信息时q值更新的速度。如果学习率为零，新值就是旧值。这实质上意味着算法没有学习任何新东西。alpha值为1只是丢弃了这个旧值，并完全依赖于由其他项计算的新Q值。介于0和1之间的值允许您控制学习进度。在足够快地学习和不要太快地专注于一个特定的Q值之间取得平衡是Q学习挑战的一部分。</li><li id="2b0f" class="nc nd iq ki b kj nl km nm kp nn kt no kx np lb nh ni nj nk bi translated">𝛾贴现因子。这决定了我们是主要关注短期回报(小𝛾)还是长期价值(大𝛾).</li></ul><h1 id="13b3" class="le lf iq bd lg lh li lj lk ll lm ln lo jw lp jx lq jz lr ka ls kc lt kd lu lv bi translated">训练我们的特工</h1><p id="2ade" class="pw-post-body-paragraph kg kh iq ki b kj lw jr kl km lx ju ko kp ly kr ks kt lz kv kw kx ma kz la lb ij bi translated">有了理论基础之后，我们就可以开始实际训练我们的RL代理了。Github上的Python脚本<code class="fe mz na nb mq b"><a class="ae ld" href="https://github.com/PaulHiemstra/qlearning_paper/blob/master/train_qlearning.py" rel="noopener ugc nofollow" target="_blank">train_qlearning.py</a></code>按照我们之前定义的伪代码训练方法执行我们的整个训练循环。前十几行初始化我们<a class="ae ld" rel="noopener" target="_blank" href="/an-ai-agent-plays-tic-tac-toe-part-1-building-the-opponent-to-play-against-bb5af74fded1">构建的</a>和<a class="ae ld" rel="noopener" target="_blank" href="/an-ai-agent-plays-tic-tac-toe-part-2-speeding-up-recursive-functions-using-memoization-97253529aea8">优化的</a>的树，这将作为我们RL代理的对手。接下来，我们初始化两个对象:</p><figure class="mc md me mf gt mg"><div class="bz fp l di"><div class="mn mo l"/></div></figure><p id="00d1" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">第一个对象跟踪井字游戏棋盘的状态。它允许一个人采取行动，检查游戏是否结束，并在游戏结束时分发奖励。GitHub上的文件<code class="fe mz na nb mq b"><a class="ae ld" href="https://github.com/PaulHiemstra/qlearning_paper/blob/master/support_functions.py" rel="noopener ugc nofollow" target="_blank">support_functions.py</a></code>包含了所有的细节。第二个对象<code class="fe mz na nb mq b">player_tree</code>代表我们的RL代理，它玩游戏并慢慢更新它的Q表。注意，我们手动调整了<code class="fe mz na nb mq b">alpha</code>、<code class="fe mz na nb mq b">gamma</code>和<code class="fe mz na nb mq b">epsilon</code>的值，但是我们的学习似乎对这些值不太敏感。大概井字游戏对于Q学习来说真的不是一个有挑战性的问题。</p><p id="0a52" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">训练循环用这些Python对象玩井字游戏，慢慢学习Q表:</p><figure class="mc md me mf gt mg"><div class="bz fp l di"><div class="mn mo l"/></div></figure><p id="a5fc" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">这里我们玩10000个井字游戏。</p><h1 id="c867" class="le lf iq bd lg lh li lj lk ll lm ln lo jw lp jx lq jz lr ka ls kc lt kd lu lv bi translated">我们的代理表现如何？</h1><p id="5aea" class="pw-post-body-paragraph kg kh iq ki b kj lw jr kl km lx ju ko kp ly kr ks kt lz kv kw kx ma kz la lb ij bi translated">作为我们训练过的RL代理的基准，我们首先让一个未训练过的代理来对抗树搜索。我们简单地从零开始创建一个播放器，并让它在没有Q学习的情况下与树搜索进行游戏:</p><figure class="mc md me mf gt mg"><div class="bz fp l di"><div class="mn mo l"/></div></figure><pre class="mc md me mf gt mp mq mr ms aw mt bi"><span id="4bd0" class="mu lf iq mq b gy mv mw l mx my">-10.0    768<br/> 5.0     232<br/>dtype: int64</span></pre><p id="00e8" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">未经训练的代理通常会输掉游戏(-10)或和棋(5)。鉴于我们被允许开始游戏，这是糟糕的表现。相比之下，使用<code class="fe mz na nb mq b"><a class="ae ld" href="https://github.com/PaulHiemstra/qlearning_paper/blob/master/train_qlearning.py" rel="noopener ugc nofollow" target="_blank">train_qlearning.py</a></code>对接受过100.000次游戏培训的代理进行相同的分析:</p><figure class="mc md me mf gt mg"><div class="bz fp l di"><div class="mn mo l"/></div></figure><pre class="mc md me mf gt mp mq mr ms aw mt bi"><span id="c651" class="mu lf iq mq b gy mv mw l mx my">5.0    1000<br/>dtype: int64</span></pre><p id="02e7" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">显示RL代理已经变得非常擅长对付树，总是打成平局。</p><h1 id="ca6e" class="le lf iq bd lg lh li lj lk ll lm ln lo jw lp jx lq jz lr ka ls kc lt kd lu lv bi translated">Q表</h1><p id="81ce" class="pw-post-body-paragraph kg kh iq ki b kj lw jr kl km lx ju ko kp ly kr ks kt lz kv kw kx ma kz la lb ij bi translated">为了更深入地了解Q learning如何学习如何玩井字游戏，我们将重点放在Q表的内容上。下图显示了100.000训练游戏结束时的最终Q表。请记住，我们是<code class="fe mz na nb mq b">X</code>玩家，蓝色表示好的Q值，红色表示负的Q值:</p><figure class="mc md me mf gt mg"><div class="bz fp l di"><div class="mn mo l"/></div></figure><figure class="mc md me mf gt mg gh gi paragraph-image"><div role="button" tabindex="0" class="mh mi di mj bf mk"><div class="gh gi nr"><img src="../Images/0240cfb591a059298ff6cc692da3e6a6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AwOyYifkRkLA07HTHrYjjw.png"/></div></div></figure><p id="98fe" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">Q表包含RL代理具有的关于井字游戏策略的信息。好的例子有:</p><figure class="mc md me mf gt mg gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/49a776ae0342bdf4e7a3d351e2ce4bb7.png" data-original-src="https://miro.medium.com/v2/resize:fit:586/format:webp/1*V_MSNP0wtOPxI8BvRRVJQQ.png"/></div></figure><p id="29ec" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">在这里，我们了解到弯角是开始时很好的动作，这从深蓝色中可以明显看出。此外，我们(<code class="fe mz na nb mq b">X</code>)学会了通过树搜索(<code class="fe mz na nb mq b">O</code>)来阻止移动。这里的阻挡移动是蓝色的，其他移动是深红色的。</p><p id="e2d4" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">除了最终学习的Q表，下面的youtube电影很好地说明了Q表是如何在100，000个游戏中慢慢发展的。</p><figure class="mc md me mf gt mg"><div class="bz fp l di"><div class="nt mo l"/></div></figure><p id="8ab8" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">随着新状态的引入，表会不断增长。此外，白色慢慢地被正值(蓝色)和负值(红色)所取代，表明代理知道什么是好的和坏的移动。</p><p id="7bdd" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">在我们的下一部分，我们将关注我是如何可视化Q表的。</p><h1 id="140c" class="le lf iq bd lg lh li lj lk ll lm ln lo jw lp jx lq jz lr ka ls kc lt kd lu lv bi translated">我是谁？</h1><p id="a49f" class="pw-post-body-paragraph kg kh iq ki b kj lw jr kl km lx ju ko kp ly kr ks kt lz kv kw kx ma kz la lb ij bi translated">我叫Paul Hiemstra，是荷兰的一名教师和数据科学家。我是科学家和软件工程师的混合体，对与数据科学相关的一切都有广泛的兴趣。你可以在medium上关注我，或者在LinkedIn 上关注<a class="ae ld" href="https://www.linkedin.com/in/paul-hiemstra-77030b20/" rel="noopener ugc nofollow" target="_blank">。</a></p><p id="e7a1" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">如果你喜欢这篇文章，你可能也会喜欢我的其他一些文章:</p><ul class=""><li id="8b14" class="nc nd iq ki b kj kk km kn kp ne kt nf kx ng lb nh ni nj nk bi translated"><a class="ae ld" rel="noopener" target="_blank" href="/there-is-no-data-science-like-applied-data-science-99b6c5308b5a">没有像应用数据科学这样的数据科学</a></li><li id="6873" class="nc nd iq ki b kj nl km nm kp nn kt no kx np lb nh ni nj nk bi translated"><a class="ae ld" rel="noopener" target="_blank" href="/altair-plot-deconstruction-visualizing-the-correlation-structure-of-weather-data-38fb5668c5b1">牛郎星图解构:可视化气象数据的关联结构</a></li><li id="7917" class="nc nd iq ki b kj nl km nm kp nn kt no kx np lb nh ni nj nk bi translated"><a class="ae ld" rel="noopener" target="_blank" href="/advanced-functional-programming-for-data-science-building-code-architectures-with-function-dd989cc3b0da">面向数据科学的高级函数式编程:使用函数运算符构建代码架构</a></li></ul></div></div>    
</body>
</html>