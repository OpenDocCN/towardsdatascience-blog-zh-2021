<html>
<head>
<title>What if we didn’t have to compromise between interpretability and performance?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如果我们不必在可解释性和性能之间妥协会怎么样？</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/what-if-we-didnt-have-to-compromise-between-interpretability-and-performance-da00d4e30a44?source=collection_archive---------26-----------------------#2021-07-16">https://towardsdatascience.com/what-if-we-didnt-have-to-compromise-between-interpretability-and-performance-da00d4e30a44?source=collection_archive---------26-----------------------#2021-07-16</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="39a3" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated"><strong class="ak">小数据中的机器学习算法基准测试</strong></h2></div><p id="44eb" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在过去的几年里，我读了很多关于可解释性和性能之间的权衡(顺便说一下，这让我想起了很多偏差-方差权衡)。我见过许多图表，显示了标准机器学习算法在这两个指标方面的地位，如下所示:</p><ul class=""><li id="5f27" class="lb lc iq kh b ki kj kl km ko ld ks le kw lf la lg lh li lj bi translated">一方面，<strong class="kh ir">线性模型</strong>天生简单且易于理解，只有当数据符合线性的硬约束时才能产生良好的性能</li><li id="bed5" class="lb lc iq kh b ki lk kl ll ko lm ks ln kw lo la lg lh li lj bi translated">另一方面，我们发现了<strong class="kh ir">集成算法</strong>，如随机森林，甚至更进一步的<strong class="kh ir">深度神经网络</strong>，它们对大多数数据都有良好的性能，但代价是在没有额外策略的情况下难以解释(例如，使用<a class="ae lp" href="https://ema.drwhy.ai/LIME.html" rel="noopener ugc nofollow" target="_blank">时间</a>或<a class="ae lp" href="https://ema.drwhy.ai/shapley.html" rel="noopener ugc nofollow" target="_blank"> SHAP </a>)。</li></ul><figure class="lr ls lt lu gt lv gh gi paragraph-image"><div role="button" tabindex="0" class="lw lx di ly bf lz"><div class="gh gi lq"><img src="../Images/6723714ed637ec3ba9f7c98d02220288.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AkPf_GZQTR8nK2Rp3tL2aw.png"/></div></div><p class="mc md gj gh gi me mf bd b be z dk translated">性能与可解释性领域的主要机器学习算法</p></figure><p id="bd4d" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">有一类算法总是从那些图表以及机器学习的参考书和材料中缺失，即<strong class="kh ir">符号回归算法</strong>。<a class="mg mh ep" href="https://medium.com/u/4101e7425d00?source=post_page-----da00d4e30a44--------------------------------" rel="noopener" target="_blank">拉斐尔·鲁杰罗</a>最近写了一篇关于它的文章，名为<a class="ae lp" rel="noopener" target="_blank" href="/symbolic-regression-the-forgotten-machine-learning-method-ac50365a7d95">被遗忘的机器学习算法</a>，这说明了一切！</p></div><div class="ab cl mi mj hu mk" role="separator"><span class="ml bw bk mm mn mo"/><span class="ml bw bk mm mn mo"/><span class="ml bw bk mm mn"/></div><div class="ij ik il im in"><h1 id="42d3" class="mp mq iq bd mr ms mt mu mv mw mx my mz jw na jx nb jz nc ka nd kc ne kd nf ng bi translated">符号回归</h1><p id="0539" class="pw-post-body-paragraph kf kg iq kh b ki nh jr kk kl ni ju kn ko nj kq kr ks nk ku kv kw nl ky kz la ij bi translated">那么，这个进化算法界最了解的算法是什么呢？它包括将模型构建为数学表达式<strong class="kh ir"/>，也可以表示为表达式树<strong class="kh ir"/>。以下示例显示了模型的数学公式，其中涉及输入特征X的<strong class="kh ir">选择</strong>，使用一元运算符(正弦、平方根和对数)将其修改为<strong class="kh ir">新特征</strong>，以及使用二元运算符(加法和乘法)将其<strong class="kh ir">交互</strong>。</p><figure class="lr ls lt lu gt lv gh gi paragraph-image"><div class="gh gi nm"><img src="../Images/dadd48380beb9656d00c0ff58615d2c5.png" data-original-src="https://miro.medium.com/v2/resize:fit:622/format:webp/1*yVYsPAuFWqKUK39_C-gbAw.png"/></div><p class="mc md gj gh gi me mf bd b be z dk translated">符号回归模型的示例</p></figure><p id="db7a" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">由于搜索相关的新特征以及如何组合它们是一个非常大的搜索空间和困难的优化问题，传统的机器学习并不真正适合处理它，符号回归的大多数算法依赖于<strong class="kh ir">遗传编程</strong>。在符号回归不为ML社区所熟知的可能原因中，我们可以列举以下几个:较慢的计算时间，缺乏与ML相关的基准，性能不太好，超参数数量大…</p><p id="dd31" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">然而，在过去的几年里，事情开始发生变化:</p><ul class=""><li id="594c" class="lb lc iq kh b ki kj kl km ko ld ks le kw lf la lg lh li lj bi translated">回归中有几个比较不同符号回归算法与更传统算法(线性模型、决策树、随机森林、多层感知器)的基准，其中一个在大量ML数据集上[1]</li><li id="885c" class="lb lc iq kh b ki lk kl ll ko lm ks ln kw lo la lg lh li lj bi translated">最近发布的几个符号回归算法表明，它们可以实现与随机森林相当的T2性能，并接近T4梯度增强。这些算法分别是<strong class="kh ir"> Zoetrope遗传编程</strong>(ZGP)【2】和<strong class="kh ir">特征工程自动化工具</strong>(FEAT)【3】。</li></ul><h1 id="c52b" class="mp mq iq bd mr ms nn mu mv mw no my mz jw np jx nb jz nq ka nd kc nr kd nf ng bi translated">符号回归算法的性能</h1><p id="0fcb" class="pw-post-body-paragraph kf kg iq kh b ki nh jr kk kl ni ju kn ko nj kq kr ks nk ku kv kw nl ky kz la ij bi translated">在再次谈论可解释性之前，让我们看看符号回归算法相对于更传统的机器学习算法的表现如何。</p><p id="74db" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">下图是从<a class="ae lp" href="https://github.com/EpistasisLab/pmlb" rel="noopener ugc nofollow" target="_blank">Penn Machine Learning Benchmark</a>数据库中对近100个数据集执行的基准测试的结果生成的，这些数据集具有多达3000个观察值。它们显示了每个算法对所有数据集运行20次后，归一化均方根误差(NRMSE)和R2的重新分配。这些算法是按平均值排序的(在NRMSE是从低到高，因为我们希望它尽可能接近0，在R2是从高到低，给出的分数是1，表示完全符合)。</p><p id="3623" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">从这些图中，特别是关于R2分数的图，我们看到4种算法明显优于其他算法，即:梯度推进、Zoetrope遗传编程(ZGP)、随机森林和特征工程自动化工具(FEAT)。因此，这里要传达的信息是，良好的性能也可以通过符号回归来实现。</p><figure class="lr ls lt lu gt lv gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/f6ce0b6ddbc31104eabdeb835e6fb918.png" data-original-src="https://miro.medium.com/v2/resize:fit:888/format:webp/1*xfnvk-acC_D7OiZz1DDuwg.png"/></div><p class="mc md gj gh gi me mf bd b be z dk translated">符号回归和机器学习算法在归一化均方误差(NRMSE，顶部)和R2分数(底部)中的性能，按所有数据集的平均值排序。最好的算法在左边。</p></figure><p id="07b8" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">请注意，这里的基准测试是使用大多数方法的默认参数运行的，并且为了尽可能公平起见，对线性方法进行了交叉验证。</p><h1 id="3d7d" class="mp mq iq bd mr ms nn mu mv mw no my mz jw np jx nb jz nq ka nd kc nr kd nf ng bi translated">可解释性与性能</h1><p id="7eb5" class="pw-post-body-paragraph kf kg iq kh b ki nh jr kk kl ni ju kn ko nj kq kr ks nk ku kv kw nl ky kz la ij bi translated">我们现在回到我们最初的问题:这些算法在可解释性和性能测量方面表现如何？更准确地说，我们是否总是要在这两者之间做出妥协？答案是否定的，这要归功于符号回归算法，我们接下来要研究它。</p><p id="162f" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">下图是根据与之前相同的基准生成的。它显示了中值R2的性能与一些符号回归算法和标准机器学习算法的可解释性。这里的可解释性被测量为树中的节点数(对于符号回归或决策树)或系数数(对于线性模型或多层感知器)。</p><blockquote class="nt nu nv"><p id="bc0b" class="kf kg nw kh b ki kj jr kk kl km ju kn nx kp kq kr ny kt ku kv nz kx ky kz la ij bi translated">它表明，3个顶级符号回归算法能够实现与随机森林一样好的性能，可解释性接近线性模型，产生人类可以理解的模型。</p></blockquote><p id="bc1f" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">请注意可解释性的对数标度:这意味着符号回归算法可以实现“简单”1000倍(就节点数量而言)的模型，而性能没有损失。此外，它们可以非常清楚地显示变量之间的相互作用，这在一些应用程序中非常有用。在上面给出的表达式树的例子中，已经发现变量X9和X3的平方根之间的相互作用。</p><figure class="lr ls lt lu gt lv gh gi paragraph-image"><div role="button" tabindex="0" class="lw lx di ly bf lz"><div class="gh gi oa"><img src="../Images/5c9292ea59a038c325cca0b9fc448828.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UDxPQOv-vKM6MfxghuW6Ow.png"/></div></div><p class="mc md gj gh gi me mf bd b be z dk translated">符号回归和机器学习算法的实际性能与可解释性</p></figure><p id="1869" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">基准测试的完整细节在[2]中给出，这些结果的基准测试代码可从以下Gitlab存储库中获得:<a class="ae lp" href="https://gitlab.devenv.mydatamodels.com/publications/bench-zgp-symbolic-regression" rel="noopener ugc nofollow" target="_blank">https://git lab . devenv . mydata models . com/publications/bench-zgp-symbolic-regression</a>。</p><h1 id="c022" class="mp mq iq bd mr ms nn mu mv mw no my mz jw np jx nb jz nq ka nd kc nr kd nf ng bi translated">结论</h1><p id="0967" class="pw-post-body-paragraph kf kg iq kh b ki nh jr kk kl ni ju kn ko nj kq kr ks nk ku kv kw nl ky kz la ij bi translated">所以最后，使用符号回归，有可能获得既有好的性能又有好的可解释性的模型。取决于问题(及其大小！)，为了获得可以直接解释的模型，有时牺牲一点性能(例如与深度神经网络相比)是值得的，而不需要求助于额外的方法来理解如何从模型中获得预测。</p></div><div class="ab cl mi mj hu mk" role="separator"><span class="ml bw bk mm mn mo"/><span class="ml bw bk mm mn mo"/><span class="ml bw bk mm mn"/></div><div class="ij ik il im in"><h1 id="ccb4" class="mp mq iq bd mr ms mt mu mv mw mx my mz jw na jx nb jz nc ka nd kc ne kd nf ng bi translated">参考</h1><p id="5efe" class="pw-post-body-paragraph kf kg iq kh b ki nh jr kk kl ni ju kn ko nj kq kr ks nk ku kv kw nl ky kz la ij bi translated">[1] Orzechowski，p .，La Cava，w .，&amp; Moore，J. H. (2018年)。我们现在在哪里？近期符号回归方法的大型基准研究。在<em class="nw">遗传和进化计算会议上</em>(第1183-1190页)。</p><p id="710f" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">[2] Boisbunon，a .，Fanara，c .，Grenet，I .，Daeden，j .，Vighi，a .，和Schoenauer，M. (2021年)。回归的遗传程序设计。在2021年7月10日至14日的<em class="nw">遗传和进化计算大会</em>(GECCO’21)上。<a class="ae lp" href="https://doi.org/10.1145/3449639.3459349" rel="noopener ugc nofollow" target="_blank">https://doi.org/10.1145/3449639.3459349</a></p><p id="1503" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">[3]拉卡瓦、辛格、塔加特、苏里和摩尔(2019年)。通过进化树网络学习回归的简洁表示。在<em class="nw">国际学术代表会议</em> (ICLR)。https://openreview.net/pdf?id=Hke-JhA9Y7<a class="ae lp" href="https://openreview.net/pdf?id=Hke-JhA9Y7" rel="noopener ugc nofollow" target="_blank"/></p></div></div>    
</body>
</html>