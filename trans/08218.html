<html>
<head>
<title>Try this simple trick to improve your clustering</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">尝试这个简单的技巧来改善你的聚类</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/try-this-simple-trick-to-improve-your-clustering-b2d5d502039b?source=collection_archive---------15-----------------------#2021-07-28">https://towardsdatascience.com/try-this-simple-trick-to-improve-your-clustering-b2d5d502039b?source=collection_archive---------15-----------------------#2021-07-28</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="eea2" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsdatascience.com/tagged/tips-and-tricks" rel="noopener" target="_blank">提示和技巧</a></h2><div class=""/><div class=""><h2 id="5211" class="pw-subtitle-paragraph jw iz iq bd b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn dk translated">开始初始化的k-means++算法</h2></div><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ko"><img src="../Images/0671709292124e42c8e82323c9650026.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*AUAyXBE417VV9rfP"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">照片由<a class="ae le" href="https://unsplash.com/@bradencollum?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">布拉登·科拉姆</a>在<a class="ae le" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><p id="1053" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">k-means是一种非常简单且普遍存在的聚类算法。但是通常它对你的问题不起作用，例如因为初始化不好。我最近遇到了一个类似的问题，我对我的数据集中的少量文件应用了k-means，一切都很好，但当我对更多的样本运行它时，它只是没有可靠地获得良好的结果。</p><p id="fff6" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">幸运的是，有一种改进的初始化方法，k-means++可以帮助缓解这个问题。在本文中，我们将涵盖以下内容:</p><ul class=""><li id="48ee" class="mb mc iq lh b li lj ll lm lo md ls me lw mf ma mg mh mi mj bi translated"><a class="ae le" href="#6fb7" rel="noopener ugc nofollow">为什么初始化对k-means如此重要</a></li><li id="6de4" class="mb mc iq lh b li mk ll ml lo mm ls mn lw mo ma mg mh mi mj bi translated"><a class="ae le" href="#5d37" rel="noopener ugc nofollow">对k-means++算法的直观描述</a></li><li id="f83a" class="mb mc iq lh b li mk ll ml lo mm ls mn lw mo ma mg mh mi mj bi translated"><a class="ae le" href="#2200" rel="noopener ugc nofollow">R中k-means++的实现</a></li><li id="cf84" class="mb mc iq lh b li mk ll ml lo mm ls mn lw mo ma mg mh mi mj bi translated"><a class="ae le" href="#2350" rel="noopener ugc nofollow">k-means ++的一个常见但错误的变体</a></li></ul><p id="e231" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">如果你还没有读过我关于k-means的文章，可以去看看，因为我会经常引用那里介绍的主题。像往常一样，你也可以在我的<a class="ae le" href="https://github.com/MSHelm/algorithms-from-scratch" rel="noopener ugc nofollow" target="_blank"> GitHub </a>上找到这篇文章的所有代码。</p><div class="mp mq gp gr mr ms"><a rel="noopener follow" target="_blank" href="/a-deep-dive-into-k-means-f9a1ef2490f8"><div class="mt ab fo"><div class="mu ab mv cl cj mw"><h2 class="bd ja gy z fp mx fr fs my fu fw iz bi translated">对k-means的深入探究</h2><div class="mz l"><p class="bd b dl z fp mx fr fs my fu fw dk translated">towardsdatascience.com</p></div></div><div class="na l"><div class="nb l nc nd ne na nf ky ms"/></div></div></a></div></div><div class="ab cl ng nh hu ni" role="separator"><span class="nj bw bk nk nl nm"/><span class="nj bw bk nk nl nm"/><span class="nj bw bk nk nl"/></div><div class="ij ik il im in"><h1 id="6fb7" class="nn no iq bd np nq nr ns nt nu nv nw nx kf ny kg nz ki oa kj ob kl oc km od oe bi translated">问题是</h1><p id="af4d" class="pw-post-body-paragraph lf lg iq lh b li of ka lk ll og kd ln lo oh lq lr ls oi lu lv lw oj ly lz ma ij bi translated">正如我们在我之前的帖子中看到的，普通k-means受到随机初始化的影响。根据选择哪些点作为起始中心，<br/>解决方案可能是一个非常糟糕的局部最小值。并且由于k-means具有严格的凹损失函数，它在训练期间没有办法逃离这个局部最小值。</p><p id="aabb" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">考虑我上一个例子中的例子，我们有两个集群，但是每个集群中的点数非常不同(也就是说，集群基数不同)。由于起始中心是随机选择的，很有可能两个中心都是从大集群中选择的，这反过来导致<br/>最终完全错误的集群。</p><p id="1c70" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">但是对于大小均匀的集群，我们可能会得到错误的初始化，从而导致错误的结果:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ok"><img src="../Images/08bf65e0aaa46c5f5dfd90e31b76a1e3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Kar2WLNZUO4jXHVVpFQYkw.png"/></div></div></figure></div><div class="ab cl ng nh hu ni" role="separator"><span class="nj bw bk nk nl nm"/><span class="nj bw bk nk nl nm"/><span class="nj bw bk nk nl"/></div><div class="ij ik il im in"><h1 id="5d37" class="nn no iq bd np nq nr ns nt nu nv nw nx kf ny kg nz ki oa kj ob kl oc km od oe bi translated">k-means++算法</h1><p id="77ce" class="pw-post-body-paragraph lf lg iq lh b li of ka lk ll og kd ln lo oh lq lr ls oi lu lv lw oj ly lz ma ij bi translated">这个问题的解决方案是k-means++算法，它使用不同的初始化。这个想法非常简单:<br/>我们只随机选择第一个中心，而不是随机初始化。所有的<br/>随后的中心仍然被采样，但是其概率与它们离所有当前中心的距离的平方成比例。远离当前中心的点在下一次初始化迭代中成为中心的概率更高。</p><p id="18ea" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">这试图更均匀地填充观察空间，同时仍然保留一些随机性。即使使用k-means++，对相同数据的多次运行的结果也可能不同。虽然它在算法开始时确实需要更多的计算，但它会导致更快的收敛，使它在运行时方面比普通k-means更具竞争力。因此，许多常见的库使用k-means++初始化作为它们的缺省值，例如<em class="ol"> sk-learn </em>或<em class="ol"> MatLab </em>实现。</p><p id="22fa" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">请注意，此处仅隐式考虑了基础数据的分布，因为概率仅依赖于到其他中心的距离，而不依赖于到所有其他数据点的距离(与其他初始化算法相反，例如围绕medoids的分区)。<br/>尽管如此，点数多的区域更有可能创建自己的<br/>中心，因为更多的点数可以被选作下一个中心。</p></div><div class="ab cl ng nh hu ni" role="separator"><span class="nj bw bk nk nl nm"/><span class="nj bw bk nk nl nm"/><span class="nj bw bk nk nl"/></div><div class="ij ik il im in"><h1 id="2200" class="nn no iq bd np nq nr ns nt nu nv nw nx kf ny kg nz ki oa kj ob kl oc km od oe bi translated">R中的实现</h1><p id="b36b" class="pw-post-body-paragraph lf lg iq lh b li of ka lk ll og kd ln lo oh lq lr ls oi lu lv lw oj ly lz ma ij bi translated">base R附带的<code class="fe om on oo op b">kmeans</code>函数实际上没有k-means++初始化选项。所以让我们实现它，重用上一篇文章中k-means算法的主干。我们将添加一个可选参数，让用户选择初始化，默认为我们刚刚讨论过的k-means++初始化。</p><p id="65cf" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">第一个中心是从数据中随机选择的。然后，我们创建一个中心数据框，从第一个中心开始，稍后我们将填充其余的中心。同时，我们需要跟踪哪些点不是中心，以防止我们再次选择同一个点作为中心。</p><pre class="kp kq kr ks gt oq op or os aw ot bi"><span id="f1e4" class="ou no iq op b gy ov ow l ox oy">next_center_id &lt;- sample(seq_len(nrow(df)), 1)<br/>centers &lt;- df[next_center_id, ]<br/>non_centers &lt;- df[-next_center_id, ]</span></pre><p id="20d3" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">接下来，我们根据它们与<br/>当前中心的距离迭代地选择新中心。我们可以使用上次的帮助函数<code class="fe om on oo op b">calculate_centers</code>(完整代码见下文)，我们只需要考虑到最近中心的距离，所以我们在每一行调用<code class="fe om on oo op b">min</code>。</p><pre class="kp kq kr ks gt oq op or os aw ot bi"><span id="ef3b" class="ou no iq op b gy ov ow l ox oy">distances &lt;- calculate_distances(non_centers, centers)<br/>distances &lt;- apply(distances, 1, min)</span></pre><p id="5388" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">在接下来的步骤中，我们根据距离选择下一个中心。<br/><code class="fe om on oo op b">sample</code>函数方便地让我们通过<code class="fe om on oo op b">prob</code>参数指定每个点的概率，它们甚至不需要在0和1之间！<br/>因此，我们可以计算距离的平方，并简单地将它们传递到那里，但为了更紧密地遵循原始文件，我们还进行了归一化:</p><pre class="kp kq kr ks gt oq op or os aw ot bi"><span id="a5ef" class="ou no iq op b gy ov ow l ox oy">probabilites &lt;- distances² / sum(distances²)<br/>next_center_id &lt;- sample(seq_len(nrow(non_centers)), 1, prob = probabilities)</span></pre><p id="a1c8" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">最后，我们选择下一个中心，将其添加到我们的中心数据框中，然后<br/>将其从非中心数据框中移除</p><pre class="kp kq kr ks gt oq op or os aw ot bi"><span id="b69b" class="ou no iq op b gy ov ow l ox oy">next_center &lt;- non_centers[next_center_id, ]<br/>centers &lt;- rbind(centers, next_center)<br/>non_centers &lt;- non_centers[-next_center_id, ]</span></pre><p id="3e2b" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">我们现在只需要在一个<code class="fe om on oo op b">while</code>循环中这样做，直到我们选择了预期数量的聚类<em class="ol"> k </em>的中心。这产生了我们的最后一个函数:</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="oz pa l"/></div></figure><p id="b76e" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">如果我们使用新的kmeans++初始化，我们现在可以从数据中恢复正确的集群:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi pb"><img src="../Images/5720d2942672a3176927e87327f0444d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1332/format:webp/1*P-GDw4sOLHyvflPkwUQXbQ.png"/></div></figure></div><div class="ab cl ng nh hu ni" role="separator"><span class="nj bw bk nk nl nm"/><span class="nj bw bk nk nl nm"/><span class="nj bw bk nk nl"/></div><div class="ij ik il im in"><h1 id="2350" class="nn no iq bd np nq nr ns nt nu nv nw nx kf ny kg nz ki oa kj ob kl oc km od oe bi translated">一个常见但错误的k-means++变体</h1><p id="5fbc" class="pw-post-body-paragraph lf lg iq lh b li of ka lk ll og kd ln lo oh lq lr ls oi lu lv lw oj ly lz ma ij bi translated">与许多数据科学工具一样，k-means++技术也有一些变体。通常，它选择与所有其他中心距离最大的点<br/>作为下一个中心，而不是以与距离成比例的概率进行采样的<br/>。如果你看一下<br/>原始论文(文末链接)，这并不是真正的k-means++ <br/>算法，它还有一个重大缺点:</p><p id="e746" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">如果总是选择具有最大距离的中心，则可以容易地选择一个异常值作为中心。由于人们通常选择k与数据集的大小相比非常小，少量的离群值就足以只选择离群值作为初始中心！<br/>这不会对集群有太大的改善，因为你的数据主体没有很好的分布中心。但是，如果您想这样做，那么<code class="fe om on oo op b">while</code>循环中的代码应该如下所示:</p><pre class="kp kq kr ks gt oq op or os aw ot bi"><span id="6aeb" class="ou no iq op b gy ov ow l ox oy">distances &lt;- calculate_distances(non_centers, centers)<br/>distances &lt;- apply(distances, 1, min)<br/># Choose point with maximum distance as next center<br/>next_center_id &lt;- which.max(distances)<br/># True k-means++ does the following:<br/># probabilities &lt;- distances/max(distances)<br/># next_center_id &lt;- sample(seq_len(nrow(non_centers)), 1, prob = probabilities)<br/>next_center &lt;- non_centers[next_center_id, ]<br/>centers &lt;- rbind(centers, next_center)<br/>non_centers &lt;- non_centers[-next_center_id, ]</span></pre><h1 id="7b74" class="nn no iq bd np nq pc ns nt nu pd nw nx kf pe kg nz ki pf kj ob kl pg km od oe bi translated">摘要</h1><p id="0a6a" class="pw-post-body-paragraph lf lg iq lh b li of ka lk ll og kd ln lo oh lq lr ls oi lu lv lw oj ly lz ma ij bi translated">正如我们所见，初始化是k-means性能的关键。<br/> k-means++算法是一种简单且广泛应用的技术，可以缓解普通k-means算法存在的问题。还存在一些其他方法来进一步帮助解决这个问题，例如多次初始化中心，并选择具有最低惯性的初始化。例如，<em class="ol"> sk-learn </em>默认进行10轮初始化。</p><p id="8ad5" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">对于我的用例，不幸的是k-means++还不够，我需要更好的方法。最复杂的初始化可能包含在medoids分区(PAM)中，我将在下一篇文章中讨论。敬请关注！</p><h1 id="4841" class="nn no iq bd np nq pc ns nt nu pd nw nx kf pe kg nz ki pf kj ob kl pg km od oe bi translated">来源</h1><ul class=""><li id="19e3" class="mb mc iq lh b li of ll og lo ph ls pi lw pj ma mg mh mi mj bi translated">描述k-means++算法的原始论文:<a class="ae le" href="https://dl.acm.org/doi/10.5555/1283383.1283494" rel="noopener ugc nofollow" target="_blank">Arthur&amp;Cassilvitskii(2007):k-means ++精心播种的优势</a></li><li id="921d" class="mb mc iq lh b li mk ll ml lo mm ls mn lw mo ma mg mh mi mj bi translated">如果你没有论文，数学也在<a class="ae le" href="https://de.mathworks.com/help/stats/kmeans.html#bues5gz" rel="noopener ugc nofollow" target="_blank"> MATLAB文档</a>中描述</li><li id="47d4" class="mb mc iq lh b li mk ll ml lo mm ls mn lw mo ma mg mh mi mj bi translated"><a class="ae le" href="https://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_stability_low_dim_dense.html#sphx-glr-auto-examples-cluster-plot-kmeans-stability-low-dim-dense-py" rel="noopener ugc nofollow" target="_blank">对k-means++的随机初始化的经验评估</a></li></ul></div></div>    
</body>
</html>