<html>
<head>
<title>Music Genre Detection With Deep Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">基于深度学习的音乐流派检测</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/music-genre-detection-with-deep-learning-cf89e4cb2ecc?source=collection_archive---------6-----------------------#2021-07-20">https://towardsdatascience.com/music-genre-detection-with-deep-learning-cf89e4cb2ecc?source=collection_archive---------6-----------------------#2021-07-20</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="0da3" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">TensorFlow模型如何用几行代码对音频文件进行分类。</h2></div><p id="efc6" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi lb translated">无论你的音乐知识水平如何，都很难描述一种音乐类型是什么。为什么爵士乐听起来像爵士乐？你如何区分乡村音乐和迪斯科音乐？由于体裁没有一个系统的定义，不可能用<em class="lk"/><em class="lk">【if/else】</em>这样的语句对体裁进行程序化的分类。这就是深度学习发挥作用的时候了。我们将会看到每一种音乐类型都有自己特定类型的声学特征，我们可以从中学习。让我们开始吧！</p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><div class="gh gi ll"><img src="../Images/01150fefd764196835c03a7eb312f676.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eZBrIrzXBnEOvFabUO4YKw.jpeg"/></div></div><p class="lx ly gj gh gi lz ma bd b be z dk translated">照片由<a class="ae mb" href="https://unsplash.com/@mohammadmetri" rel="noopener ugc nofollow" target="_blank"> Mohammad Metri </a>在<a class="ae mb" href="https://unsplash.com/" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure></div><div class="ab cl mc md hu me" role="separator"><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh"/></div><div class="ij ik il im in"><h1 id="ab70" class="mj mk iq bd ml mm mn mo mp mq mr ms mt jw mu jx mv jz mw ka mx kc my kd mz na bi translated">介绍</h1><p id="ac3d" class="pw-post-body-paragraph kf kg iq kh b ki nb jr kk kl nc ju kn ko nd kq kr ks ne ku kv kw nf ky kz la ij bi translated">这个项目的目标是对音频文件进行分类(在“<em class="lk">”中)。wav" </em>格式)分成10种音乐流派:布鲁斯、古典、乡村、迪斯科、hiphop、爵士、金属、流行、雷鬼、摇滚。为此，我们将使用<a class="ae mb" href="https://www.tensorflow.org/api_docs/python/tf" rel="noopener ugc nofollow" target="_blank"> TensorFlow2/Keras </a>作为我们的深度学习框架，使用<a class="ae mb" href="https://librosa.org/doc/latest/index.html#" rel="noopener ugc nofollow" target="_blank"> Librosa </a>作为我们的音频预处理库。Librosa是一个用于音乐和音频分析的Python包，它提供了创建音乐信息检索(M. I. R .)系统所必需的构件。</p><p id="7fb4" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">为了训练我们的模型，我们将使用GTZAN数据集，该数据集由10个流派的集合组成，每个流派有100个音频文件，长度都为30秒。数据集在这里<a class="ae mb" href="https://www.kaggle.com/andradaolteanu/gtzan-dataset-music-genre-classification" rel="noopener ugc nofollow" target="_blank">可用。</a></p><p id="1f97" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">预处理、训练和推理步骤都是在Jupyter笔记本电脑上进行的，该笔记本电脑具有Conda-Python3环境，具有模块化块和功能，以实现更好的清晰度和可移植性。请在这篇文章的结论部分找到我的GitHub的链接。</p><h1 id="a328" class="mj mk iq bd ml mm ng mo mp mq nh ms mt jw ni jx mv jz nj ka mx kc nk kd mz na bi translated">声音和音频信号处理</h1><p id="2a6f" class="pw-post-body-paragraph kf kg iq kh b ki nb jr kk kl nc ju kn ko nd kq kr ks ne ku kv kw nf ky kz la ij bi translated">一旦采样，声波就以一系列<em class="lk">浮点</em>值的形式驻留在你的硬盘上。其大小取决于采样率(通常为44100Hz或22050Hz)和录制持续时间。这些时间序列可以很容易地加载到<em class="lk"> Numpy数组</em>中进行处理。让我们使用Librosa可视化来自GTZAN数据集的一个音频文件:</p><pre class="lm ln lo lp gt nl nm nn no aw np bi"><span id="44c4" class="nq mk iq nm b gy nr ns l nt nu">song, sr = librosa.load('GTZAN_Dataset/genres_original/rock/rock.00000.wav')</span><span id="8419" class="nq mk iq nm b gy nv ns l nt nu">librosa.display.waveshow(song)</span></pre><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><div class="gh gi nw"><img src="../Images/cde02af0152e773384811a8318643831.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2dJkX6b7NulTGoN8j230uQ.png"/></div></div><p class="lx ly gj gh gi lz ma bd b be z dk translated">摇滚歌曲的声音振幅随时间(30秒)变化的表示</p></figure><p id="6a24" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">通常使用<em class="lk">短时傅立叶变换</em> (STFT)来更好地理解音频信号的定性行为。这是一个数学工具，允许我们<strong class="kh ir">分析频谱随时间的变化</strong>。以下是方法。</p><p id="0176" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">首先，我们定义一个<em class="lk"> frame_size </em>(通常是一批2048个样本)。在每一帧上，通过应用<em class="lk">离散傅立叶变换</em> (DFT)来计算(n个频率仓的)频率向量。频率向量是我们的音频信号在频域中的即时表示。你可以把它看作是在一个给定的时间(例如一个给定的帧)所有频率区间上的能量分布的声音描述。</p><p id="80a3" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">第二，随着音乐播放，帧改变(下一帧与前一帧相距一个<em class="lk"> hop_length </em>)并且能量分布也随之改变。因此，我们通过对所有帧连续应用DFT来获得STFT(或频谱图)的视觉表示，并在热图上表示它们，如下所示:</p><pre class="lm ln lo lp gt nl nm nn no aw np bi"><span id="e940" class="nq mk iq nm b gy nr ns l nt nu">def plot_spectrogram(Y, sr, hop_length, y_axis="linear"):<br/>    plt.figure(figsize=(10, 5))<br/>    librosa.display.specshow(Y, sr=sr, hop_length=1024, x_axis="time", y_axis=y_axis)<br/>    plt.colorbar(format="%+2.f")</span><span id="76b8" class="nq mk iq nm b gy nv ns l nt nu">Y_log = librosa.power_to_db(np.abs(librosa.stft(song, hop_length=1024))**2)<br/>plot_spectrogram(Y_log, sr, 1024)</span></pre><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><div class="gh gi nx"><img src="../Images/3f1697f5952f60e61e56eae51d03ceae.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SFH3RupLqQqewAjybm2TpQ.png"/></div></div><p class="lx ly gj gh gi lz ma bd b be z dk translated">所有频率仓随时间(30秒)的对数振幅变化</p></figure><p id="f075" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">上图是我们的摇滚歌曲。这张热图显示了每个频段是如何随时间演变的:红色越多，能量越大，声音越大。请注意较高频率(约10kHz频段)如何在0至30秒内保持蓝色，因为这些频段的信号幅度较小。此外，均匀间隔的垂直红线出现在整个文件，这显然是所有的乐器跟着节拍。</p><blockquote class="ny"><p id="b918" class="nz oa iq bd ob oc od oe of og oh la dk translated">想过摇滚到底是什么吗？</p><p id="b94b" class="nz oa iq bd ob oc od oe of og oh la dk translated">现在至少你知道它看起来像什么了！</p></blockquote><p id="62e7" class="pw-post-body-paragraph kf kg iq kh b ki oi jr kk kl oj ju kn ko ok kq kr ks ol ku kv kw om ky kz la ij bi translated">但是我们的光谱图看起来仍然有些模糊，我们距离使它适合深度学习模型的摄取还有几步调整。你可能知道，我们人类听到的声音不是线性的:A1 (55Hz)和A2(110Hz)之间的感知音高差异与A4 (440Hz)和A5 (880Hz)相同。这两个音程都是八度音阶，但是第一个八度音阶相差55Hz，而另一个八度音阶相差440Hz。<em class="lk">梅尔标度</em>定义了相对于感知频率均匀分布的频带。<strong class="kh ir"> Mel滤波器组经过计算，对较低频率的区分度更高，对较高频率的区分度较低</strong>，就像人耳一样。通过执行如下加权鉴别来平滑谐波结构:</p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><div class="gh gi on"><img src="../Images/6b90d151663f1bac81de7bae2defd6f7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tzBuTcjqAa8poqmGjwoftg.png"/></div></div><p class="lx ly gj gh gi lz ma bd b be z dk translated">三角形Mel滤波器组</p></figure><p id="eb8e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">Mel缩放后，最后一步是应用<em class="lk">离散余弦变换</em>，即<strong class="kh ir">仅生成实值系数</strong>。作为最佳实践，我们通常保留前13个系数(<em class="lk"> n_mfcc= </em> 13)，称为<em class="lk">梅尔频率倒谱系数(mfcc)。</em>它们描述了光谱形状的最简单方面，而高阶系数对训练来说不太重要，因为它们往往描述更多类似噪声的信息。</p><p id="e723" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir"> MFCC是更大光谱结构的有用描述符</strong>，易于阅读并可由深度学习模型“解释”。让我们摘录我们摇滚歌曲中的MFCC:</p><pre class="lm ln lo lp gt nl nm nn no aw np bi"><span id="69e2" class="nq mk iq nm b gy nr ns l nt nu">mfcc_song = librosa.feature.mfcc(song, n_mfcc=13, sr=sr)</span><span id="4827" class="nq mk iq nm b gy nv ns l nt nu">plt.figure(figsize=(15, 8))<br/>librosa.display.specshow(mfcc_song, x_axis="time", sr=sr)<br/>plt.colorbar(format="%+2.f")<br/>plt.show()</span></pre><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><div class="gh gi oo"><img src="../Images/354fdc734eca01b9fc0cafe8478d16a5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CEB58jS5ENhtXynYhzSJ1w.png"/></div></div><p class="lx ly gj gh gi lz ma bd b be z dk translated">13乐队MFCC为一首30秒的摇滚歌曲</p></figure><p id="cdea" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在看起来很整洁！那么，我们这里到底有什么？<em class="lk"> mfcc_song </em>的形状返回(13，1293)，例如对应于每个系数的13行和1293帧(每个帧2048个样本)持续30秒。这是一首摇滚歌曲的声音特征。为了学习摇滚流派，GTZAN数据集还有99首歌曲要训练。让我们看看如何做到这一点。</p><h1 id="3ed7" class="mj mk iq bd ml mm ng mo mp mq nh ms mt jw ni jx mv jz nj ka mx kc nk kd mz na bi translated">准备训练、验证和测试数据集</h1><p id="241f" class="pw-post-body-paragraph kf kg iq kh b ki nb jr kk kl nc ju kn ko nd kq kr ks ne ku kv kw nf ky kz la ij bi translated">浏览完所有歌曲后，我注意到它们的长度可能会在30秒左右略有不同。为了确保我们处理的文件具有相同的持续时间，我们将在29秒内裁剪所有文件。我们还设置采样率<em class="lk"> sr </em> = 22050Hz，并计算每个切片的样本数。</p><pre class="lm ln lo lp gt nl nm nn no aw np bi"><span id="30b4" class="nq mk iq nm b gy nr ns l nt nu"><em class="lk"># Sampling rate.<br/></em>sr = 22050</span><span id="47dd" class="nq mk iq nm b gy nv ns l nt nu"><em class="lk"># Let’s make sure all files have the same amount of samples, pick a duration right under 30 seconds.<br/></em>TOTAL_SAMPLES = 29 * sr</span><span id="5a63" class="nq mk iq nm b gy nv ns l nt nu"><em class="lk"># The dataset contains 999 files (1000–1 defective). Lets make it bigger.<br/># X amount of slices =&gt; X times more training examples.<br/></em>NUM_SLICES = 10<br/>SAMPLES_PER_SLICE = int(TOTAL_SAMPLES / NUM_SLICES)</span></pre><p id="7bb5" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">此外，我们需要<strong class="kh ir">生成更多的训练样本</strong>，因为1000首歌曲并不多。将每首歌曲分成10个子部分似乎是一个合理的大小，以便从它们的声学特征中保留足够的信息。</p><p id="a8b8" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们的数据集由10个文件夹及其流派名称组成，每个文件夹包含100首歌曲。所以<strong class="kh ir">标注过程</strong>很简单:文件夹名→标签。我们现在可以将这种<strong class="kh ir">分类公式化为监督问题</strong>，因为每首歌曲都有一个分配给它的流派标签。</p><p id="fa34" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">所有MFCCs和相关标签将存储在单独的<em class="lk"> json </em>文件中，并在<em class="lk"> for循环</em>中处理，如下所示:</p><pre class="lm ln lo lp gt nl nm nn no aw np bi"><span id="fdf3" class="nq mk iq nm b gy nr ns l nt nu"><em class="lk"># Let's browse each file, slice it and generate the 13 band mfcc for each slice.</em><br/><strong class="nm ir">for</strong> i, (dirpath, dirnames, filenames) <strong class="nm ir">in </strong>enumerate(os.walk(source_path)):<br/><br/>    <strong class="nm ir">for</strong> file <strong class="nm ir">in</strong> filenames:<br/>      song, sr = librosa.load(os.path.join(dirpath, file), duration=29)<br/><br/>      <strong class="nm ir">for</strong> s <strong class="nm ir">in</strong> range(NUM_SLICES):<br/>          start_sample = SAMPLES_PER_SLICE * s<br/>          end_sample = start_sample + SAMPLES_PER_SLICE<br/>          mfcc = librosa.feature.mfcc(y=song[start_sample:end_sample], sr=sr, n_mfcc=13)<br/>          mfcc = mfcc.T<br/>          mydict["labels"].append(i-1)<br/>          mydict["mfcc"].append(mfcc.tolist())<br/><br/><em class="lk"># Let's write the dictionary in a json file.    </em><br/><strong class="nm ir">with</strong> open(json_path, 'w') <strong class="nm ir">as</strong> f:<br/>    json.dump(mydict, f)<br/>    f.close()</span></pre><p id="f492" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">最后，通过<em class="lk">Scikit-Learn train _ test _ split()</em>以20%的比率生成验证和测试数据集。</p><h1 id="ab31" class="mj mk iq bd ml mm ng mo mp mq nh ms mt jw ni jx mv jz nj ka mx kc nk kd mz na bi translated">模型架构、训练和预测</h1><p id="d8da" class="pw-post-body-paragraph kf kg iq kh b ki nb jr kk kl nc ju kn ko nd kq kr ks ne ku kv kw nf ky kz la ij bi translated">我们将使用一个<em class="lk"> TensorFlow2 </em>框架来设计我们的<em class="lk">卷积神经网络</em> (CNN)，具有3层卷积和一个最终全连接层，具有<em class="lk"> softmax激活</em>，具有10个输出(用于10个流派)。整体架构如下:</p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><div class="gh gi op"><img src="../Images/bd148c816e4f9b8f3126636240359c96.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gaOKo8mSAVosbM6twygvcQ.png"/></div></div></figure><p id="f138" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">使用<em class="lk"> batch_size </em> =32训练超过30个<em class="lk">时期</em>需要在<em class="lk">英特尔双核i5 </em> CPU上花费几分钟，并在验证数据集上收敛到77%的准确率<em class="lk"> </em>。让我们用<em class="lk"> Matplotlib </em>绘制度量曲线:</p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><div class="gh gi oq"><img src="../Images/f67ae38fdfb3a47b9f9d8fe04c9d4eac.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hioK5DMBtM0j-_3xoc8NqQ.png"/></div></div></figure><h1 id="0c76" class="mj mk iq bd ml mm ng mo mp mq nh ms mt jw ni jx mv jz nj ka mx kc nk kd mz na bi translated">结论</h1><p id="7bba" class="pw-post-body-paragraph kf kg iq kh b ki nb jr kk kl nc ju kn ko nd kq kr ks ne ku kv kw nf ky kz la ij bi translated">我在相同的数据上测试了其他模型架构，结果如下:</p><ul class=""><li id="946e" class="or os iq kh b ki kj kl km ko ot ks ou kw ov la ow ox oy oz bi translated">多层感知器→ 76%训练acc和59%阀acc</li><li id="351c" class="or os iq kh b ki pa kl pb ko pc ks pd kw pe la ow ox oy oz bi translated">递归神经网络(LSTM型)→ 90%训练acc和76%阀acc</li></ul><p id="c035" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">本文中介绍的CNN架构最终获得了前所未有的最佳数据性能(77%)。为了减少方差，可以应用各种技术来微调模型，例如正则化、数据扩充、更多的漏失层等。让我知道你是否能拿出一个更好的分数，以及你实现它的方法。</p><p id="9e30" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">请随意查看我的<a class="ae mb" href="https://github.com/msaintfelix/TensorFlow_MusicGenre_Classifier" rel="noopener ugc nofollow" target="_blank"> GitHub库</a>上的模型基准。我的<a class="ae mb" href="https://www.kaggle.com/marchenrysaintfelix/music-genre-cnn-classifier-with-75-val-acc" rel="noopener ugc nofollow" target="_blank"> Kaggle </a>上也提交了CNN的模型。感谢您的阅读！</p></div></div>    
</body>
</html>