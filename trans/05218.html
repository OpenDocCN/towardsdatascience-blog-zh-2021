<html>
<head>
<title>How to Fine-Tune GPT-2 for Text Generation</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如何为文本生成微调GPT-2</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/how-to-fine-tune-gpt-2-for-text-generation-ae2ea53bc272?source=collection_archive---------0-----------------------#2021-05-08">https://towardsdatascience.com/how-to-fine-tune-gpt-2-for-text-generation-ae2ea53bc272?source=collection_archive---------0-----------------------#2021-05-08</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="7299" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">使用GPT-2生成高质量的歌词</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/5193bf231988ad64226a5b86334245d8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*kui1cTcHGV1sh4Cx"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">来源:https://unsplash.com/photos/gUK3lA3K7Yo<a class="ae ky" href="https://unsplash.com/photos/gUK3lA3K7Yo" rel="noopener ugc nofollow" target="_blank"/></p></figure><p id="d918" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">自然语言生成(NLG)在最近几年取得了令人难以置信的进步。2019年初，OpenAI发布了GPT-2，这是一个巨大的预训练模型(1.5B参数)，能够生成类似人类质量的文本。</p><p id="a769" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">顾名思义，创成式预训练变形金刚2 (GPT-2)基于变形金刚。因此，它使用注意力机制，这意味着它学会关注与上下文最相关的前几个单词，以便预测下一个单词(更多信息，请访问<a class="ae ky" rel="noopener" target="_blank" href="/transformers-a-simple-explanation-5b17aeb587e6">这里</a>)。</p><p id="59b6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">本文的目标是向您展示如何根据您提供给它的数据，对GPT-2进行微调，以生成上下文相关的文本。</p><p id="e722" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">作为一个例子，我将生成歌词。这个想法是使用已经训练好的模型，根据我们的具体数据进行微调，然后根据模型观察到的情况，生成任何给定歌曲中应该遵循的内容。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h2 id="6d06" class="mc md it bd me mf mg dn mh mi mj dp mk li ml mm mn lm mo mp mq lq mr ms mt mu bi translated">准备数据</h2><p id="c736" class="pw-post-body-paragraph kz la it lb b lc mv ju le lf mw jx lh li mx lk ll lm my lo lp lq mz ls lt lu im bi translated">GPT-2本身可以生成质量不错的文本。但是，如果您希望它在特定的上下文中做得更好，您需要根据您的特定数据对它进行微调。在我的例子中，因为我想生成歌词，所以我将使用下面的Kaggle <a class="ae ky" href="https://www.kaggle.com/neisse/scrapped-lyrics-from-6-genres" rel="noopener ugc nofollow" target="_blank">数据集</a>，它包含总共12，500首流行摇滚歌曲的歌词，都是英文的。</p><p id="6a2f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们从导入必要的库和准备数据开始。我推荐在这个项目中使用Google Colab，因为使用GPU会让事情变得更快。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="na nb l"/></div></figure><p id="fc06" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">从第26行和第34–35行可以看出，我创建了一个小型测试集，删除了每首歌的最后20个单词。这将允许我将生成的文本与实际文本进行比较，以查看模型的执行情况。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h2 id="2330" class="mc md it bd me mf mg dn mh mi mj dp mk li ml mm mn lm mo mp mq lq mr ms mt mu bi translated">创建数据集</h2><p id="b510" class="pw-post-body-paragraph kz la it lb b lc mv ju le lf mw jx lh li mx lk ll lm my lo lp lq mz ls lt lu im bi translated">为了在我们的数据上使用GPT-2，我们还需要做一些事情。我们需要对数据进行记号化，这是将一系列字符转换成记号的过程，即把一个句子分割成单词。</p><p id="9042" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们还需要确保每首歌曲最多支持1024个令牌。</p><p id="38f7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在训练过程中,<code class="fe nc nd ne nf b">SongLyrics</code>类将为我们做这些，为我们原始数据帧中的每一首歌。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="na nb l"/></div></figure></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h2 id="20d3" class="mc md it bd me mf mg dn mh mi mj dp mk li ml mm mn lm mo mp mq lq mr ms mt mu bi translated">模型的训练</h2><p id="c39a" class="pw-post-body-paragraph kz la it lb b lc mv ju le lf mw jx lh li mx lk ll lm my lo lp lq mz ls lt lu im bi translated">我们现在可以导入预训练的GPT-2模型，以及令牌化器。另外，就像我之前提到的，GPT 2号是巨大的。如果你试图在你的电脑上使用它，你很可能会得到一堆<code class="fe nc nd ne nf b">CUDA Out of Memory</code>错误。</p><p id="aca6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">可以使用的另一种方法是<strong class="lb iu">累加梯度</strong>。</p><p id="45a8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这个想法很简单，在调用优化来执行梯度下降步骤之前，它将对几个操作的梯度求和。然后，将该总数除以累计步数，以获得训练样本的平均损失。这意味着更少的计算。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="na nb l"/></div></figure><p id="f92a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在，最后，我们可以创建训练函数，它将使用我们所有的歌词来微调GPT-2，以便它可以预测未来的优质诗句。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="na nb l"/></div></figure><p id="e355" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">随意使用各种超参数(批量大小、学习率、时期、优化器)。</p><p id="34ac" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然后，最后，我们可以训练模型。</p><pre class="kj kk kl km gt ng nf nh ni aw nj bi"><span id="f5ee" class="mc md it nf b gy nk nl l nm nn">model = train(dataset, model, tokenizer)</span></pre><p id="cb46" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">使用<code class="fe nc nd ne nf b">torch.save</code>和<code class="fe nc nd ne nf b">torch.load</code>，你也可以保存你训练好的模型以备将来使用。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h2 id="8f16" class="mc md it bd me mf mg dn mh mi mj dp mk li ml mm mn lm mo mp mq lq mr ms mt mu bi translated">歌词生成</h2><p id="4e12" class="pw-post-body-paragraph kz la it lb b lc mv ju le lf mw jx lh li mx lk ll lm my lo lp lq mz ls lt lu im bi translated">是时候使用我们全新的微调模型来生成歌词了。通过使用以下两个函数，我们可以为测试数据集中的所有歌曲生成歌词。请记住，我已经删除了每首歌的最后20个词。对于给定的歌曲，我们的模型现在将查看他的歌词，并得出歌曲的结尾。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="na nb l"/></div></figure><p id="0fbd" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对于整个测试数据帧，<code class="fe nc nd ne nf b">generate </code>函数准备生成，而<code class="fe nc nd ne nf b">text_generation </code>实际上完成生成。</p><p id="19d9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在第6行，我们指定了一代的最大长度。我将其保留为30，但这是因为标点符号很重要，稍后我将删除最后几个单词，以确保生成在句子的末尾结束。</p><p id="ad56" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">另外两个超参数值得一提:</p><ul class=""><li id="326e" class="no np it lb b lc ld lf lg li nq lm nr lq ns lu nt nu nv nw bi translated"><strong class="lb iu">温度</strong>(第8行)。它用于衡量给定单词生成的概率。因此，高温迫使模型做出更多的原创预测，而低温则防止模型偏离主题。</li><li id="e61e" class="no np it lb b lc nx lf ny li nz lm oa lq ob lu nt nu nv nw bi translated"><strong class="lb iu">顶p过滤</strong>(第7行)。该模型将按降序对单词概率进行排序。然后，它会将这些概率相加，直到p，同时丢弃其他单词。这意味着该模型只保留最相关的单词概率，而不是只保留最好的一个，因为对于给定的序列，不止一个单词是合适的。</li></ul><p id="ff8a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在下面的代码中，我简单地清理了生成的文本，确保它在句子的末尾(而不是中间)结束，并将其存储在测试数据集中的一个新列中。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="na nb l"/></div></figure></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h2 id="580b" class="mc md it bd me mf mg dn mh mi mj dp mk li ml mm mn lm mo mp mq lq mr ms mt mu bi translated">性能赋值</h2><p id="053d" class="pw-post-body-paragraph kz la it lb b lc mv ju le lf mw jx lh li mx lk ll lm my lo lp lq mz ls lt lu im bi translated">有许多方法可以评估生成文本的质量。最受欢迎的指标叫做BLEU。该算法根据生成的文本与现实的相似程度，输出一个介于0和1之间的分数。分数为1表示生成的每个单词都存在于真实文本中。</p><p id="ae29" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">下面是为生成的歌词评估BLEU分数的代码。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="na nb l"/></div></figure><p id="98e6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们获得了0.685的平均BLEU分数，相当不错。相比之下，没有任何微调的GPT-2模型的BLEU得分为0.288。</p><p id="b6b4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然而，BLEU也有其局限性。它最初是为机器翻译而创建的，只查看用于确定生成文本质量的词汇。这对我们来说是个问题。事实上，有可能产生高质量的诗句，使用与现实完全不同的词语。</p><p id="bd3f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这就是为什么我会对模型的性能做一个主观的评价。为此，我创建了一个小的web界面(使用Dash)。代码可以在我的Github库中找到。</p><p id="179e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">界面的工作方式是你给app提供一些输入单词。然后，该模型将使用它来预测下一对诗句应该是什么。以下是一些结果示例。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oc"><img src="../Images/a8ee58b5335cac7ba8d07833f37e5ae1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LqIcisQ4aYXTVN7pjPV5Gw.png"/></div></div></figure><p id="7b1d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">红色的是GPT-2模型预测的，给定黑色的输入序列。你看到它已经成功地产生了有意义的诗句，并且尊重了之前发生的事情的背景！此外，它会生成长度相似的句子，这对于保持歌曲的节奏非常重要。在这方面，当生成歌词时，输入文本中的标点符号是绝对必要的。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h2 id="a89f" class="mc md it bd me mf mg dn mh mi mj dp mk li ml mm mn lm mo mp mq lq mr ms mt mu bi translated">结论</h2><p id="d9dc" class="pw-post-body-paragraph kz la it lb b lc mv ju le lf mw jx lh li mx lk ll lm my lo lp lq mz ls lt lu im bi translated">正如本文所示，通过对特定数据进行GPT-2微调，可以相当容易地生成上下文相关的文本。</p><p id="65f6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对于歌词生成，该模型可以生成尊重上下文和句子的期望长度的歌词。当然，可以对模型进行改进。例如，我们可以强迫它生成押韵的诗句，这是写歌词时经常需要的。</p><p id="480e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">非常感谢你的阅读，我希望我能有所帮助！</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><p id="402c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">包含所有代码和模型的库可以在这里找到:<a class="ae ky" href="https://github.com/francoisstamant/lyrics-generation-with-GPT2" rel="noopener ugc nofollow" target="_blank">https://github . com/francoistamant/lyrics-generation-with-gp T2</a></p></div></div>    
</body>
</html>