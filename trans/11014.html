<html>
<head>
<title>An Exhaustive Guide to Decision Tree Classification in Python 3.x</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Python 3.x中决策树分类的详尽指南</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/an-exhaustive-guide-to-classification-using-decision-trees-8d472e77223f?source=collection_archive---------2-----------------------#2021-10-27">https://towardsdatascience.com/an-exhaustive-guide-to-classification-using-decision-trees-8d472e77223f?source=collection_archive---------2-----------------------#2021-10-27</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><figure class="ip iq gp gr ir is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi io"><img src="../Images/ec74b9349c35d0aa41c9930261522b8f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bfdBuf65baCSrwmDLt4tUA.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">图片来自<a class="ae jd" href="https://www.google.com/url?sa=i&amp;url=https%3A%2F%2Fwww.pinterest.com%2Fpardenpd%2Fillustrator-malika-favre%2F&amp;psig=AOvVaw33xxMMkiBnFIS9tylwcoOF&amp;ust=1639315821413000&amp;source=images&amp;cd=vfe&amp;ved=0CAwQjhxqFwoTCLiPzb_t2_QCFQAAAAAdAAAAABAi" rel="noopener ugc nofollow" target="_blank"> Pinterest </a></p></figure><div class=""/><div class=""><h2 id="d0d3" class="pw-subtitle-paragraph kd jf jg bd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku dk translated">使用决策树进行分类的端到端教程</h2></div><p id="4733" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">有各种机器学习算法可以用于处理分类问题。一种这样的算法是决策树算法，除了分类之外，它还可以用于解决回归问题。虽然这是一种最简单的分类算法，但是如果对其参数进行适当的调整，可以产生非常精确的结果。</p><p id="cdbe" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在这篇文章中，我将首先尝试给你一个什么是算法的直觉，以及它如何做出预测。然后，我将尝试分解与该算法密切相关的一些重要术语，最后，在本文结束时，我将使用决策树设计一个非常简单的分类器。</p><p id="52a9" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在开始这篇文章之前，我建议你看一下我以前的文章，我在这些文章中讨论了各种学习算法。另外，如果你喜欢这篇文章，请考虑为这篇文章鼓掌，并确保关注我以获得更多#MachineLearningRecipes。整篇文章的PDF副本可以从<a class="ae jd" href="https://drive.google.com/file/d/1dk9eJruySiRbFkFOChmAZrLZGP3RvVo_/view?usp=sharing" rel="noopener ugc nofollow" target="_blank">这里</a>下载，继续你的离线学习。</p><h1 id="5b76" class="lr ls jg bd lt lu lv lw lx ly lz ma mb km mc kn md kp me kq mf ks mg kt mh mi bi translated"><strong class="ak">决策树是如何做出决策的？</strong></h1><p id="6c32" class="pw-post-body-paragraph kv kw jg kx b ky mj kh la lb mk kk ld le ml lg lh li mm lk ll lm mn lo lp lq ij bi translated">正如我在以前的文章中所讨论的，决策树算法是一种简单而有效的监督学习算法，其中数据点根据某些参数和/或算法试图解决的问题被连续分割。<strong class="kx jh">决策树</strong>通常也被称为<strong class="kx jh"> CART </strong>(代表分类和回归树)。</p><p id="beda" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">如果您有兴趣了解更多关于使用决策树解决回归问题的知识，请考虑查看我以前的文章<a class="ae jd" rel="noopener" target="_blank" href="/sowing-the-seeds-of-decision-tree-regression-2bb238dfd768?source=user_profile---------5----------------------------">这里</a>。</p><figure class="mp mq mr ms gt is gh gi paragraph-image"><div class="gh gi mo"><img src="../Images/b2782e7e2247256fcf83dacafef13d32.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/0*lGdmveHcvsKYGWrU.gif"/></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">图片来自<a class="ae jd" href="https://www.hackerearth.com/practice/machine-learning/machine-learning-algorithms/ml-decision-tree/tutorial/" rel="noopener ugc nofollow" target="_blank">黑客地球</a></p></figure><p id="5422" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">每个决策树包括一个<strong class="kx jh">根节点</strong>，一些<strong class="kx jh">分支</strong>，和<strong class="kx jh">叶节点</strong>。树中的内部节点描述了不同的测试用例。决策树可以用来解决分类和回归问题。该算法可以被认为是一个图形树状结构，使用各种调整参数来预测结果。决策树将<strong class="kx jh">自顶向下的方法</strong>应用于训练期间输入的数据集。</p><p id="cf7b" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">为了理解算法实际上是如何工作的，假设需要开发一个预测模型，该模型可以预测学生申请进入特定课程是否被接受。考虑以下提供给任何决策树模型的数据集。</p><figure class="mp mq mr ms gt is gh gi paragraph-image"><div class="gh gi mt"><img src="../Images/78eec04caf7d228ce9bd70c29d7fb37a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1260/format:webp/1*JAJieODM9vUYIL1ZgmvrHg.png"/></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">图片作者<a class="ae jd" href="https://rajashwin.medium.com/" rel="noopener">作者</a></p></figure><p id="3d2a" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">某个学生的申请只有在满足下述条件的情况下才会被接受:</p><ul class=""><li id="51a6" class="mu mv jg kx b ky kz lb lc le mw li mx lm my lq mz na nb nc bi translated">入门考试成绩应等于或大于60分。</li><li id="71a1" class="mu mv jg kx b ky nd lb ne le nf li ng lm nh lq mz na nb nc bi translated">毕业、10级和12级的成绩应在60分以上。</li></ul><p id="c77c" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在这种情况下，尽管上述条件可能有某些例外。在这种情况下，申请将被列入等候名单。</p><ul class=""><li id="f3fa" class="mu mv jg kx b ky kz lb lc le mw li mx lm my lq mz na nb nc bi translated">如果申请人的GATE/毕业分数低于门槛分数，但有工作经验，则该申请将被列入名单。</li><li id="132f" class="mu mv jg kx b ky nd lb ne le nf li ng lm nh lq mz na nb nc bi translated">如果申请人在第12类中的分数超过75分，但低于第10类中的最低要求分数，则他们的申请将被列入名单。</li></ul><figure class="mp mq mr ms gt is gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/b56e31dafe1d8bd0b2cc0309df7ebceb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1272/1*zgeToKkVqDTnqRo0N5oKJg.gif"/></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">图片来自<a class="ae jd" href="https://algobeans.com/2016/07/27/decision-trees-tutorial/" rel="noopener ugc nofollow" target="_blank">豆类</a></p></figure><p id="2eb2" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">上例中考虑的问题可以用图形的形式来考虑，如决策树或流程图。一棵树将满足问题中提供的所有可能的情况。决策树算法的工作方式类似于一组嵌套的if-else语句，其中检查连续的条件，除非模型得出结论。</p><p id="791b" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">树的决策节点或简单节点是树在经过每个节点(从根节点开始)后呈现的问题。分支或子树是整个树的一个子部分。树的每个边对应于问题的结果，并且该结果由代表类别分布的叶节点或终端节点来表示。</p><h1 id="4a49" class="lr ls jg bd lt lu lv lw lx ly lz ma mb km mc kn md kp me kq mf ks mg kt mh mi bi translated"><strong class="ak">决策树在分类中是如何使用的？</strong></h1><p id="362d" class="pw-post-body-paragraph kv kw jg kx b ky mj kh la lb mk kk ld le ml lg lh li mm lk ll lm mn lo lp lq ij bi translated">决策树算法使用一种称为树的数据结构来预测特定问题的结果。由于决策树遵循有监督的方法，该算法被提供了预处理数据的集合。这些数据用于训练算法。点击了解更多关于此<a class="ae jd" rel="noopener" target="_blank" href="/sowing-the-seeds-of-decision-tree-regression-2bb238dfd768?source=user_profile---------5----------------------------">的信息。</a></p><figure class="mp mq mr ms gt is gh gi paragraph-image"><div class="ab gu cl nj"><img src="../Images/e417ff29cfd5f20421f79cf803c2e20f.png" data-original-src="https://miro.medium.com/v2/1*9kACduxnce_JdTrftM_bsA.gif"/></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">图片来自<a class="ae jd" href="https://www.google.com/url?sa=i&amp;url=https%3A%2F%2Fwww.kdnuggets.com%2F2018%2F08%2Fselecting-best-machine-learning-algorithm-regression-problem.html&amp;psig=AOvVaw00IZQGqsdaumLDR02rpBgm&amp;ust=1635603177980000&amp;source=images&amp;cd=vfe&amp;ved=0CAwQjhxqFwoTCPD0l-vm7_MCFQAAAAAdAAAAABAX" rel="noopener ugc nofollow" target="_blank"> KDNuggets </a></p></figure><p id="192e" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">决策树遵循自上而下的方法，这意味着树的根节点总是在结构的顶部，而结果由树叶表示。决策树是使用一种称为递归划分(通常称为分治法)的启发式方法构建的。根节点之后的每个节点都被分成几个节点。</p><p id="c0fe" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">其核心思想是使用决策树将数据空间划分为密集区域和稀疏区域。二叉树的分裂可以是二进制的，也可以是多进制的。该算法继续分裂树，直到数据足够同质。在训练结束时，将返回一个决策树，该决策树可用于进行最佳分类预测。</p><p id="687a" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">这个算法发展中的一个重要术语是<strong class="kx jh">熵</strong>。它可以被认为是给定数据集的不确定性的度量，其值描述了特定节点的随机程度。当结果的差异幅度非常低，模型因此对预测的准确性没有信心时，就会出现这种情况。</p><figure class="mp mq mr ms gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nk"><img src="../Images/0a65b9f73990e71f68051e33ab5a98dc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XBm0MFFNEWcnaOtfmXmY0A.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">图片来自<a class="ae jd" href="https://en.wikipedia.org/wiki/Decision_tree_learning" rel="noopener ugc nofollow" target="_blank">维基百科</a></p></figure><p id="30e9" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">熵越高，数据集中的随机性就越高。构建决策树时，应优先选择较低的熵。计算决策树熵的表达式如下:</p><figure class="mp mq mr ms gt is gh gi paragraph-image"><div class="gh gi nl"><img src="../Images/8518d73d79ee84744cf2a889d83f272f.png" data-original-src="https://miro.medium.com/v2/resize:fit:818/format:webp/1*pF1RuHzE3-W-A7Ybu84Rlg.png"/></div></figure><p id="95d2" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">用于类似目的的另一个指标是基尼指数。它使用基尼系数法来创建分割点。<strong class="kx jh">信息增益</strong>是通常用于测量数据集中不确定性减少的度量。决策树中的信息增益通常由以下公式描述:</p><figure class="mp mq mr ms gt is gh gi paragraph-image"><div class="gh gi nm"><img src="../Images/92e19256192d232791b6bf17b86facee.png" data-original-src="https://miro.medium.com/v2/resize:fit:1036/format:webp/1*_UTJ3j-lqWhAtc4loqqngA.png"/></div></figure><p id="e02b" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">该度量可以进一步用于确定决策树的根节点和要进行的分裂的数量。决策树的根节点通常被称为<strong class="kx jh">决策节点</strong>或<strong class="kx jh">主节点</strong>。</p><figure class="mp mq mr ms gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nn"><img src="../Images/382ff0dca5d2a4bd49b3b58afee70dbf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*jWITpOe4dkGZEjkc"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">图片来自<a class="ae jd" href="https://www.google.com/url?sa=i&amp;url=https%3A%2F%2Fwww.kaggle.com%2Fakashchola%2Fdecision-tree-for-classification-regression&amp;psig=AOvVaw00IZQGqsdaumLDR02rpBgm&amp;ust=1635603177980000&amp;source=images&amp;cd=vfe&amp;ved=0CAwQjhxqFwoTCPD0l-vm7_MCFQAAAAAdAAAAABBo" rel="noopener ugc nofollow" target="_blank"> Kaggle </a></p></figure><p id="c51c" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">每棵树都有一个根节点。根节点通常被认为是与所有其他特性相协调的最重要的特性。通常，所有要素中精度最高的要素被选为根节点。</p><h1 id="97f0" class="lr ls jg bd lt lu lv lw lx ly lz ma mb km mc kn md kp me kq mf ks mg kt mh mi bi translated"><strong class="ak">决策树中的分裂</strong></h1><p id="7316" class="pw-post-body-paragraph kv kw jg kx b ky mj kh la lb mk kk ld le ml lg lh li mm lk ll lm mn lo lp lq ij bi translated">随着决策树中分裂数量的增加，构建决策树所需的时间也会增加。然而，具有大量分裂的树容易过度拟合，导致精度差。然而，这可以通过决定<strong class="kx jh">最大深度</strong>参数的最佳值来管理。随着该参数值的增加，分割数也会增加。</p><p id="50c7" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">可用于控制决策树拆分的其他参数包括<strong class="kx jh"> min_samples_split </strong>、<strong class="kx jh"> min_samples_leaf </strong>和<strong class="kx jh"> max_features </strong>。</p><figure class="mp mq mr ms gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi no"><img src="../Images/1bd3d0182a4cf2acb3a7ecc9f148f3cc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1INNZpvDE2qKay-5dhYVWA.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">图片来自<a class="ae jd" href="https://en.wikipedia.org/wiki/Decision_tree_pruning" rel="noopener ugc nofollow" target="_blank">维基百科</a></p></figure><p id="88aa" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">另一种可以在很大程度上避免过度拟合的方法是通过去除在决策过程中具有很小或没有意义的分支。这被称为<strong class="kx jh">修剪</strong>。有两种不同类型的修剪— <strong class="kx jh">预修剪</strong>和<strong class="kx jh">后修剪</strong>。预修剪在树生长时完成，而后修剪在树构建到一定深度后修剪节点。</p><blockquote class="np nq nr"><p id="966c" class="kv kw ns kx b ky kz kh la lb lc kk ld nt lf lg lh nu lj lk ll nv ln lo lp lq ij bi translated">如果你已经读到这里，我相信你一定喜欢这篇文章，并且发现它内容丰富，所以请考虑为这篇文章鼓掌，并关注我后面的文章。点击这里查看我的其他#机器学习食谱<a class="ae jd" href="https://rajashwin.medium.com/" rel="noopener">。</a></p></blockquote><p id="e27d" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">现在，我已经对算法及其工作的理论概念有了更好的理解，我们将尝试应用我们的知识来构建我们自己的分类器。用于构建分类器的<a class="ae jd" href="https://github.com/ashwinraj-in/MachineLearningRecipes/blob/master/DecisionTreeClassification.ipynb" rel="noopener ugc nofollow" target="_blank">代码和其他资源</a>可以在我的<a class="ae jd" href="https://github.com/ashwinraj-in/MachineLearningRecipes" rel="noopener ugc nofollow" target="_blank"> GitHub </a>句柄中获得。</p><h2 id="8faa" class="nw ls jg bd lt nx ny dn lx nz oa dp mb le ob oc md li od oe mf lm of og mh oh bi translated"><strong class="ak">步骤1:导入所需的库和数据集</strong></h2><p id="3443" class="pw-post-body-paragraph kv kw jg kx b ky mj kh la lb mk kk ld le ml lg lh li mm lk ll lm mn lo lp lq ij bi translated">库是一组有用的功能，消除了从头编写代码的需要，在开发机器学习模型和其他应用程序中起着至关重要的作用。Python提供了大量的库，可以用来开发高度复杂的学习模型。</p><pre class="mp mq mr ms gt oi oj ok ol aw om bi"><span id="0a0a" class="nw ls jg oj b gy on oo l op oq">import numpy as np<br/>import pandas as pd<br/>import seaborn as sns<br/><br/>import matplotlib.pyplot as plt<br/>%matplotlib inline</span></pre><p id="893a" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">首先，我们将引入一些流行的库，如Pandas和NumPy。Pandas是一个快速、强大、灵活且易于使用的开源数据分析和操作工具，构建于Python编程语言之上。<strong class="kx jh">另一方面，NumPy </strong>由一组多维数组对象和处理这些NumPy数组的例程组成。</p><figure class="mp mq mr ms gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi or"><img src="../Images/87e0573e65de192d1ebb9d4e260ce253.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*qcn-cJ2V5r7nu2MJl9tCGA.gif"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">图片由<a class="ae jd" href="https://rajashwin.medium.com/" rel="noopener">作者</a></p></figure><p id="b799" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">与C/C++不同，Python为我们在程序中前进时导入库提供了灵活性。Matplotlib 是一个基于NumPy数组的多平台数据可视化库，旨在与SciPy一起工作。</p><p id="f648" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">另一个满足类似需求的库是Seaborn库，它构建在matplotlib之上，并与pandas数据结构紧密集成。可视化是Seaborn的核心部分，有助于探索和理解数据以及探索性数据分析和洞察力研究。</p><pre class="mp mq mr ms gt oi oj ok ol aw om bi"><span id="21d0" class="nw ls jg oj b gy on oo l op oq">import warnings<br/>warnings.filterwarnings('ignore')</span><span id="9366" class="nw ls jg oj b gy os oo l op oq">data = 'car_evaluation.csv'<br/>df = pd.read_csv(data, header=None)</span></pre><p id="77af" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">一旦导入了所有这些库，下一步就是获取训练和测试预测模型所需的数据集。<strong class="kx jh"> read_csv() </strong>方法用于将数据集加载到python文件/笔记本中。用于构建该决策树分类器模型的数据集可以从<a class="ae jd" href="https://www.kaggle.com/elikplim/car-evaluation-data-set/version/1" rel="noopener ugc nofollow" target="_blank">这里</a>下载。</p><h2 id="03a1" class="nw ls jg bd lt nx ny dn lx nz oa dp mb le ob oc md li od oe mf lm of og mh oh bi translated"><strong class="ak">第二步:探索性数据分析和特征工程</strong></h2><p id="5278" class="pw-post-body-paragraph kv kw jg kx b ky mj kh la lb mk kk ld le ml lg lh li mm lk ll lm mn lo lp lq ij bi translated">将数据加载到pandas数据框后，开发模型的下一步是探索性数据分析。探索性数据分析是一种分析数据的方法，以便深入了解数据中隐藏的事实和模式，这些事实和模式通常是我们看不到的。</p><figure class="mp mq mr ms gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi ot"><img src="../Images/5bbe9c7a70edc99755eadf81c2907b2f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*dmH_ot9KgNBQ0uds.gif"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">图片来自<a class="ae jd" rel="noopener" target="_blank" href="/decision-trees-60707f06e836">媒体</a>艾伦·杰弗里斯</p></figure><p id="a434" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">这一步还包括清理和预处理数据。在这一步中，我们深入了解了将要处理的数据类型。在这一步中，我们还对数据进行了一些更改，例如删除缺失值、删除某些列、查看字段、研究某些变量以及定义各种不同变量之间的关系。</p><pre class="mp mq mr ms gt oi oj ok ol aw om bi"><span id="2adb" class="nw ls jg oj b gy on oo l op oq">from sklearn.model_selection import train_test_split<br/>X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.33, random_state = 42)</span><span id="5778" class="nw ls jg oj b gy os oo l op oq">encoder = ce.OrdinalEncoder(cols=['buying', 'maint', 'doors', 'persons', 'lug_boot', 'safety'])<br/>X_train = encoder.fit_transform(X_train)<br/><br/>X_test = encoder.transform(X_test)</span></pre><p id="d867" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">数据经过适当的预处理后，下一步就是将数据分成训练数据和测试数据。通常70–80%的数据作为训练数据，而剩余的数据作为测试数据。有时，测试数据可以进一步分类到称为验证数据的附加部分，用于评估模型。</p><pre class="mp mq mr ms gt oi oj ok ol aw om bi"><span id="778b" class="nw ls jg oj b gy on oo l op oq">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.33, random_state = 42)</span></pre><figure class="mp mq mr ms gt is gh gi paragraph-image"><div class="gh gi ou"><img src="../Images/641049d7b1e2d521ddf1a8e70d3c11cf.png" data-original-src="https://miro.medium.com/v2/resize:fit:982/format:webp/1*iwg42XuJZXbvsO16x28Klg.png"/></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">图片来自<a class="ae jd" href="https://en.wikipedia.org/wiki/Decision_tree" rel="noopener ugc nofollow" target="_blank">维基百科</a></p></figure><p id="2520" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我们在训练模型之前的最后一步是特征工程。这是将原始数据转化为有用特征的过程，这些特征揭示了关于模型的有用见解，从而提高了模型的预测能力。在此步骤中，对分类值进行编码，并对数据进行其他适当的更改。到这一步结束时，预测模型就准备好了。</p><h2 id="6794" class="nw ls jg bd lt nx ny dn lx nz oa dp mb le ob oc md li od oe mf lm of og mh oh bi translated"><strong class="ak">第三步:拟合模型，评估结果，可视化树木</strong></h2><p id="8447" class="pw-post-body-paragraph kv kw jg kx b ky mj kh la lb mk kk ld le ml lg lh li mm lk ll lm mn lo lp lq ij bi translated">现在，数据已经完全准备好了，分类器被实例化，模型适合数据。为这个分类器选择的标准是熵，尽管也可以使用基尼指数。一旦我们的模型符合数据，我们尝试使用分类器模型预测值。这样做通常是为了执行公正的评估，并获得模型的准确性分数。</p><pre class="mp mq mr ms gt oi oj ok ol aw om bi"><span id="3032" class="nw ls jg oj b gy on oo l op oq">from sklearn.tree import DecisionTreeClassifier<br/>clf_en = DecisionTreeClassifier(criterion='entropy', max_depth=3, random_state=0)<br/>clf_en.fit(X_train, y_train)</span><span id="5337" class="nw ls jg oj b gy os oo l op oq">y_pred_en = clf_en.predict(X_test)</span></pre><p id="0a85" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">应确保模型既不过度拟合也不欠拟合数据。这可以通过计算训练和测试数据的准确度分数来完成。如果值是可比较的，那么模型没有过度拟合。</p><figure class="mp mq mr ms gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi ov"><img src="../Images/df9ec1716ed6b459aeddea8e60e8635c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*HW67XfucKYmczm18avgTQw.gif"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">图片来自<a class="ae jd" href="https://www.google.com/url?sa=i&amp;url=https%3A%2F%2Fcommons.wikimedia.org%2Fwiki%2FFile%3A3D_PCA_of_SWE_and_FIN.gif&amp;psig=AOvVaw045HpxtIhIthcEVpA_G21_&amp;ust=1635379057108000&amp;source=images&amp;cd=vfe&amp;ved=0CAwQjhxqFwoTCPj6gfuj6fMCFQAAAAAdAAAAABAO" rel="noopener ugc nofollow" target="_blank">维基共享资源</a></p></figure><p id="c852" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">一旦模型适合数据并做出预测，我们的最后一步将是评估分类器。这种方法最流行的工具之一是计算分类器的混淆矩阵。混淆矩阵是总结模型性能的工具。</p><pre class="mp mq mr ms gt oi oj ok ol aw om bi"><span id="5766" class="nw ls jg oj b gy on oo l op oq">from sklearn.metrics import accuracy_score<br/>print('Model accuracy score with criterion entropy: {0:0.4f}'. format(accuracy_score(y_test, y_pred_en)))</span><span id="0ac8" class="nw ls jg oj b gy os oo l op oq">y_pred_train_en = clf_en.predict(X_train)<br/>y_pred_train_en</span><span id="2b66" class="nw ls jg oj b gy os oo l op oq">plt.figure(figsize=(12,8))<br/>from sklearn import tree<br/>tree.plot_tree(clf_en.fit(X_train, y_train))</span></pre><p id="7892" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">混淆矩阵为我们提供了按每个类别细分的正确和错误预测的完整摘要，揭示了分类模型的性能和模型产生的错误。</p><pre class="mp mq mr ms gt oi oj ok ol aw om bi"><span id="777c" class="nw ls jg oj b gy on oo l op oq">from sklearn.metrics import confusion_matrix<br/>cm = confusion_matrix(y_test, y_pred_en</span><span id="3d20" class="nw ls jg oj b gy os oo l op oq">print('Confusion matrix\n\n', cm)</span></pre><figure class="mp mq mr ms gt is gh gi paragraph-image"><div class="gh gi ow"><img src="../Images/b051694a97ba281417c86e2dc42c717b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1100/format:webp/1*mXA2nLRkzzvhJkTwRDt8qA.png"/></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">图片来自<a class="ae jd" href="http://mlwiki.org/index.php/Decision_Tree_(Data_Mining)" rel="noopener ugc nofollow" target="_blank"> ML Wiki </a></p></figure><p id="a6a1" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">混淆矩阵有四种可能的结果，即真阳性、真阴性、假阳性和假阴性。混淆矩阵通常以表格形式输出。分类器的其他评估技术包括<strong class="kx jh">精度分数</strong>、<strong class="kx jh"> f1分数</strong>、<strong class="kx jh">召回率</strong>和<strong class="kx jh">支持度分数</strong>。</p><h1 id="dcc1" class="lr ls jg bd lt lu lv lw lx ly lz ma mb km mc kn md kp me kq mf ks mg kt mh mi bi translated"><strong class="ak">决策树算法的优势</strong></h1><p id="3d43" class="pw-post-body-paragraph kv kw jg kx b ky mj kh la lb mk kk ld le ml lg lh li mm lk ll lm mn lo lp lq ij bi translated">决策树分类器是最广泛使用的分类预测算法之一。使它如此受欢迎的一些特征是:</p><ul class=""><li id="8002" class="mu mv jg kx b ky kz lb lc le mw li mx lm my lq mz na nb nc bi translated">未知记录的极快分类。</li><li id="1ec9" class="mu mv jg kx b ky nd lb ne le nf li ng lm nh lq mz na nb nc bi translated">忽略预测中不太重要或不重要的特征。</li><li id="323b" class="mu mv jg kx b ky nd lb ne le nf li ng lm nh lq mz na nb nc bi translated">如果参数调整到最佳状态，效率极高。</li><li id="739d" class="mu mv jg kx b ky nd lb ne le nf li ng lm nh lq mz na nb nc bi translated">构建成本低廉，逻辑易于解释。</li></ul><h1 id="630e" class="lr ls jg bd lt lu lv lw lx ly lz ma mb km mc kn md kp me kq mf ks mg kt mh mi bi translated"><strong class="ak">决策树算法的局限性</strong></h1><p id="5b58" class="pw-post-body-paragraph kv kw jg kx b ky mj kh la lb mk kk ld le ml lg lh li mm lk ll lm mn lo lp lq ij bi translated">尽管决策树分类器是最复杂的分类算法之一，但它可能有一定的局限性，尤其是在现实世界中。其威慑因素如下所述:</p><ul class=""><li id="a20d" class="mu mv jg kx b ky kz lb lc le mw li mx lm my lq mz na nb nc bi translated">决策树分类器通常倾向于过度拟合训练数据。</li><li id="efa3" class="mu mv jg kx b ky nd lb ne le nf li ng lm nh lq mz na nb nc bi translated">数据的变化可能会导致结果发生不必要的变化。</li><li id="01d6" class="mu mv jg kx b ky nd lb ne le nf li ng lm nh lq mz na nb nc bi translated">大树有时很难解释。</li><li id="0038" class="mu mv jg kx b ky nd lb ne le nf li ng lm nh lq mz na nb nc bi translated">这些偏向于具有多个级别的特征的分割。</li></ul><figure class="mp mq mr ms gt is gh gi paragraph-image"><div class="gh gi ox"><img src="../Images/53a600c8d732826dc1c5107a17a0dfa1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1024/format:webp/1*xEhZpYoUSKJAkVMFILq0jQ.png"/></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">图片来自<a class="ae jd" href="https://en.wikipedia.org/wiki/File:Kernel_Machine.svg" rel="noopener ugc nofollow" target="_blank">维基百科</a></p></figure><p id="b187" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">由于决策树分类器在大多数情况下容易过拟合，因此对于具有大量特征的数据集，用<strong class="kx jh">主成分分析</strong>替换决策树分类器是有利的。</p><h1 id="2966" class="lr ls jg bd lt lu lv lw lx ly lz ma mb km mc kn md kp me kq mf ks mg kt mh mi bi translated"><strong class="ak">决策树分类器的应用</strong></h1><p id="5568" class="pw-post-body-paragraph kv kw jg kx b ky mj kh la lb mk kk ld le ml lg lh li mm lk ll lm mn lo lp lq ij bi translated">在讨论了决策树算法的优点和局限性之后，是时候阐明决策树分类器的应用了。决策树算法最受欢迎的用途之一是在生物医学工程中，其中它用于识别可植入设备中使用的功能和探索潜在的药物。</p><p id="b651" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">决策树分类器也在数据挖掘、金融分析和经济产品开发中得到应用，其中它们被用于理解客户满意度、商业金融和相关行为。</p><p id="5990" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在其他应用中，决策树分类器也用于系统设计和物理学，尤其是粒子检测。它还在制造和生产中得到应用，其中它可以用于质量控制、异常检测和半导体制造。</p><figure class="mp mq mr ms gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi oy"><img src="../Images/91c6c86630cdea04b44e717a6af8715d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*CugyiWadNntxwOpEjIY4xQ.gif"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">图片来自<a class="ae jd" href="https://www.google.com/url?sa=i&amp;url=https%3A%2F%2Fwww.finereport.com%2Fen%2Fdata-visualization%2Fdata-visualization-how-to-choose-the-right-chart.html&amp;psig=AOvVaw3i9hvSkiSo2-mBnIMm4AD7&amp;ust=1635381400568000&amp;source=images&amp;cd=vfe&amp;ved=0CAsQjRxqFwoTCPCays6s6fMCFQAAAAAdAAAAABB0" rel="noopener ugc nofollow" target="_blank"> Finereport </a></p></figure><p id="6ab8" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">至此，我们已经到了这篇文章的结尾。我希望这篇文章能帮助你对决策树分类器的工作原理有所了解。如果你有任何问题，或者如果你认为我犯了任何错误，请随时与我联系。通过<a class="ae jd" href="http://rajashwin733@gmail.com" rel="noopener ugc nofollow" target="_blank">邮箱</a>或<a class="ae jd" href="https://www.linkedin.com/in/rajashwin/" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>与我联系。快乐学习！</p></div></div>    
</body>
</html>