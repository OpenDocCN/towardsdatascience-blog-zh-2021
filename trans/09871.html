<html>
<head>
<title>Fast Feature Engineering in Python: Image Data</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Python中的快速要素工程:影像数据</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/fast-feature-engineering-in-python-image-data-5d3a8a7bf616?source=collection_archive---------11-----------------------#2021-09-16">https://towardsdatascience.com/fast-feature-engineering-in-python-image-data-5d3a8a7bf616?source=collection_archive---------11-----------------------#2021-09-16</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="4db9" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">使您的图像更适合输入ML系统</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/e20f73ab2866da146b93ea9267ca6028.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*mlVqTeZ-UZv0Fb5v"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">乔纳森·博尔巴在<a class="ae kv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><blockquote class="kw"><p id="a11e" class="kx ky iq bd kz la lb lc ld le lf lg dk translated">“在任何一种数据丰富的环境中，寻找模式都很容易；平庸的赌徒就是这样。关键在于确定这些图案代表的是噪音还是信号。”<br/> ― <strong class="ak">内特·西尔弗</strong></p></blockquote><p id="a402" class="pw-post-body-paragraph lh li iq lj b lk ll jr lm ln lo ju lp lq lr ls lt lu lv lw lx ly lz ma mb lg ij bi translated">本文是我的“快速特征工程”系列的第2部分。如果您还没有读过我的第一篇关于表格数据的文章，那么我请求您在这里查看一下:</p><div class="mc md gp gr me mf"><a rel="noopener follow" target="_blank" href="/fast-feature-engineering-in-python-tabular-data-d050b68bb178"><div class="mg ab fo"><div class="mh ab mi cl cj mj"><h2 class="bd ir gy z fp mk fr fs ml fu fw ip bi translated">Python中的快速要素工程:表格数据</h2><div class="mm l"><h3 class="bd b gy z fp mk fr fs ml fu fw dk translated">使您的表格数据更适合输入ML系统</h3></div><div class="mn l"><p class="bd b dl z fp mk fr fs ml fu fw dk translated">towardsdatascience.com</p></div></div><div class="mo l"><div class="mp l mq mr ms mo mt kp mf"/></div></div></a></div><p id="4912" class="pw-post-body-paragraph lh li iq lj b lk mu jr lm ln mv ju lp lq mw ls lt lu mx lw lx ly my ma mb lg ij bi translated">本文将研究在执行图像处理作为我们机器学习工作流程的一部分时要遵循的一些最佳实践。</p></div><div class="ab cl mz na hu nb" role="separator"><span class="nc bw bk nd ne nf"/><span class="nc bw bk nd ne nf"/><span class="nc bw bk nd ne"/></div><div class="ij ik il im in"><h1 id="a416" class="ng nh iq bd ni nj nk nl nm nn no np nq jw nr jx ns jz nt ka nu kc nv kd nw nx bi translated">图书馆</h1><pre class="kg kh ki kj gt ny nz oa ob aw oc bi"><span id="e580" class="od nh iq nz b gy oe of l og oh">import random<br/>from PIL import Image<br/>import cv2<br/>import numpy as np<br/>from matplotlib import pyplot as plt<br/>import json<br/>import albumentations as A<br/>import torch<br/>import torchvision.models as models<br/>import torchvision.transforms as transforms<br/>import torch.nn as nn<br/>from tqdm import tqdm_notebook<br/>from torch.utils.data import DataLoader<br/>from torchvision.datasets import CIFAR10</span></pre><h1 id="534c" class="ng nh iq bd ni nj oi nl nm nn oj np nq jw ok jx ns jz ol ka nu kc om kd nw nx bi translated">调整图像大小/缩放图像</h1><p id="dac3" class="pw-post-body-paragraph lh li iq lj b lk on jr lm ln oo ju lp lq op ls lt lu oq lw lx ly or ma mb lg ij bi translated">调整大小是该领域深度学习实践者所做的最基本的转换。这样做的主要原因是为了确保我们的深度学习系统接收到的输入是<strong class="lj ir">一致的</strong>。</p><p id="0fad" class="pw-post-body-paragraph lh li iq lj b lk mu jr lm ln mv ju lp lq mw ls lt lu mx lw lx ly my ma mb lg ij bi translated">调整大小的另一个原因是<strong class="lj ir">减少模型中参数</strong>的数量。较小的维度意味着较小的神经网络，因此节省了我们训练模型所需的时间和计算能力。</p><h2 id="5ebc" class="od nh iq bd ni os ot dn nm ou ov dp nq lq ow ox ns lu oy oz nu ly pa pb nw pc bi translated"><strong class="ak"> <em class="pd">信息丢失怎么办？</em> </strong></h2><p id="3669" class="pw-post-body-paragraph lh li iq lj b lk on jr lm ln oo ju lp lq op ls lt lu oq lw lx ly or ma mb lg ij bi translated">当你从大图中缩小尺寸时，一些信息确实会丢失。然而，根据您的任务，您可以选择愿意为训练时间和计算资源牺牲多少信息。</p><p id="818b" class="pw-post-body-paragraph lh li iq lj b lk mu jr lm ln mv ju lp lq mw ls lt lu mx lw lx ly my ma mb lg ij bi translated">例如，<a class="ae kv" href="https://en.wikipedia.org/wiki/Object_detection" rel="noopener ugc nofollow" target="_blank"> <strong class="lj ir">对象检测任务</strong> </a>将要求您保持图像的纵横比，因为目标是检测对象的准确位置。</p><p id="23b1" class="pw-post-body-paragraph lh li iq lj b lk mu jr lm ln mv ju lp lq mw ls lt lu mx lw lx ly my ma mb lg ij bi translated">相比之下，图像分类任务可能需要您将所有图像调整到指定的大小(224 x 224是一个很好的经验法则)。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi pe"><img src="../Images/b0dff3b1a1085e8975fb582f34bd4596.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*C4y0MoAtSp3qiwclP8dcrw.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">(<a class="ae kv" href="https://pixabay.com/photos/goldendoodle-puppy-two-months-1234760/" rel="noopener ugc nofollow" target="_blank">来源</a>)</p></figure><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="pf pg l"/></div></figure><p id="2af5" class="pw-post-body-paragraph lh li iq lj b lk mu jr lm ln mv ju lp lq mw ls lt lu mx lw lx ly my ma mb lg ij bi translated">调整大小后，我们的图像看起来像这样:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ph"><img src="../Images/2bcb5c24d2a0408e751922f9a6fcd7ca.png" data-original-src="https://miro.medium.com/v2/resize:fit:448/format:webp/1*pHLlODLdRGetCuBpbJ_v8w.jpeg"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">(<a class="ae kv" href="https://pixabay.com/photos/goldendoodle-puppy-two-months-1234760/" rel="noopener ugc nofollow" target="_blank">来源</a>)</p></figure><h2 id="a3c4" class="od nh iq bd ni os ot dn nm ou ov dp nq lq ow ox ns lu oy oz nu ly pa pb nw pc bi translated"><em class="pd">为什么要进行图像缩放？</em></h2><p id="ed99" class="pw-post-body-paragraph lh li iq lj b lk on jr lm ln oo ju lp lq op ls lt lu oq lw lx ly or ma mb lg ij bi translated">与表格数据类似，为分类任务缩放图像可以帮助我们的深度学习模型的学习速率更好地收敛到最小值。</p><p id="94db" class="pw-post-body-paragraph lh li iq lj b lk mu jr lm ln mv ju lp lq mw ls lt lu mx lw lx ly my ma mb lg ij bi translated">缩放可确保某个特定维度不会支配其他维度。关于这一点，我在StackExchange上找到了一个很棒的答案。你可以在这里 读到<a class="ae kv" href="https://stats.stackexchange.com/questions/185853/why-do-we-need-to-normalize-the-images-before-we-put-them-into-cnn" rel="noopener ugc nofollow" target="_blank"> <strong class="lj ir">。</strong></a></p><p id="42bc" class="pw-post-body-paragraph lh li iq lj b lk mu jr lm ln mv ju lp lq mw ls lt lu mx lw lx ly my ma mb lg ij bi translated">一种特征缩放是<strong class="lj ir">标准化</strong>我们的像素值的过程。我们通过从每个通道的像素值中减去每个通道的平均值，然后通过标准偏差对其进行划分。</p><p id="d807" class="pw-post-body-paragraph lh li iq lj b lk mu jr lm ln mv ju lp lq mw ls lt lu mx lw lx ly my ma mb lg ij bi translated">当训练用于分类任务的模型时，这是特征工程的流行选择。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="pf pg l"/></div></figure><p id="483a" class="pw-post-body-paragraph lh li iq lj b lk mu jr lm ln mv ju lp lq mw ls lt lu mx lw lx ly my ma mb lg ij bi translated"><strong class="lj ir"> <em class="pi">注意:与调整大小一样，在执行对象检测和图像生成任务时，可能不希望进行图像缩放。</em>T9】</strong></p><p id="0e6e" class="pw-post-body-paragraph lh li iq lj b lk mu jr lm ln mv ju lp lq mw ls lt lu mx lw lx ly my ma mb lg ij bi translated">上面的示例代码演示了通过标准化来缩放图像的过程。还有其他形式的缩放，如<strong class="lj ir">居中</strong>和<strong class="lj ir">正常化</strong>。</p></div><div class="ab cl mz na hu nb" role="separator"><span class="nc bw bk nd ne nf"/><span class="nc bw bk nd ne nf"/><span class="nc bw bk nd ne"/></div><div class="ij ik il im in"><h1 id="bc40" class="ng nh iq bd ni nj nk nl nm nn no np nq jw nr jx ns jz nt ka nu kc nv kd nw nx bi translated">扩充(分类)</h1><p id="382e" class="pw-post-body-paragraph lh li iq lj b lk on jr lm ln oo ju lp lq op ls lt lu oq lw lx ly or ma mb lg ij bi translated">增强图像背后的主要动机是由于计算机视觉任务需要可观的数据。通常，由于多种原因，获得足够的图像用于训练可能被证明是具有挑战性的。</p><p id="8bac" class="pw-post-body-paragraph lh li iq lj b lk mu jr lm ln mv ju lp lq mw ls lt lu mx lw lx ly my ma mb lg ij bi translated">图像增强使我们能够通过稍微修改原始样本来创建新的训练样本。</p><p id="fc81" class="pw-post-body-paragraph lh li iq lj b lk mu jr lm ln mv ju lp lq mw ls lt lu mx lw lx ly my ma mb lg ij bi translated">在这个例子中，我们将看看如何为分类任务应用普通增强。我们可以使用<strong class="lj ir">albuminations</strong>库的现成实现来做到这一点:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="pf pg l"/></div></figure><div class="kg kh ki kj gt ab cb"><figure class="pj kk pk pl pm pn po paragraph-image"><img src="../Images/692bdaae3b63a848cc7a7eea6383ae07.png" data-original-src="https://miro.medium.com/v2/resize:fit:448/format:webp/1*N9ZadWBIzRmNMnhSNHnP-A.jpeg"/></figure><figure class="pj kk pk pl pm pn po paragraph-image"><img src="../Images/181f0f020094fe39e7aec104c2dfc6f6.png" data-original-src="https://miro.medium.com/v2/resize:fit:450/format:webp/1*4RI70zpJcTHnHbI5qerXWw.jpeg"/></figure><figure class="pj kk pk pl pm pn po paragraph-image"><img src="../Images/7af9c5a612758b756ab7d67047d6323a.png" data-original-src="https://miro.medium.com/v2/resize:fit:448/format:webp/1*0Ry55qxqCT67qLwZUAhFPg.jpeg"/><p class="kr ks gj gh gi kt ku bd b be z dk pp di pq pr translated">高斯模糊、随机裁剪、翻转(<a class="ae kv" href="https://pixabay.com/photos/goldendoodle-puppy-two-months-1234760/" rel="noopener ugc nofollow" target="_blank">来源</a>)</p></figure></div><p id="8857" class="pw-post-body-paragraph lh li iq lj b lk mu jr lm ln mv ju lp lq mw ls lt lu mx lw lx ly my ma mb lg ij bi translated">通过应用图像增强，我们的深度学习模型可以更好地概括任务(避免过度拟合)，从而提高其对未知数据的预测能力。</p></div><div class="ab cl mz na hu nb" role="separator"><span class="nc bw bk nd ne nf"/><span class="nc bw bk nd ne nf"/><span class="nc bw bk nd ne"/></div><div class="ij ik il im in"><h1 id="6ef0" class="ng nh iq bd ni nj nk nl nm nn no np nq jw nr jx ns jz nt ka nu kc nv kd nw nx bi translated">增强(物体检测)</h1><p id="418c" class="pw-post-body-paragraph lh li iq lj b lk on jr lm ln oo ju lp lq op ls lt lu oq lw lx ly or ma mb lg ij bi translated">Albumentations库也可用于为其他任务(如对象检测)创建增强功能。对象检测要求我们在感兴趣的对象周围创建边界框。</p><p id="2f51" class="pw-post-body-paragraph lh li iq lj b lk mu jr lm ln mv ju lp lq mw ls lt lu mx lw lx ly my ma mb lg ij bi translated">当试图用边界框的坐标来注释图像时，处理原始数据可能被证明是具有挑战性的。</p><p id="1df0" class="pw-post-body-paragraph lh li iq lj b lk mu jr lm ln mv ju lp lq mw ls lt lu mx lw lx ly my ma mb lg ij bi translated">幸运的是，有许多公开和免费可用的数据集，我们可以使用它们来创建对象检测的增强管道。一个这样的数据集是<a class="ae kv" href="https://public.roboflow.com/object-detection/chess-full" rel="noopener ugc nofollow" target="_blank"> <strong class="lj ir">象棋数据集</strong> </a>。</p><p id="33c5" class="pw-post-body-paragraph lh li iq lj b lk mu jr lm ln mv ju lp lq mw ls lt lu mx lw lx ly my ma mb lg ij bi translated">该数据集包含棋盘上606个棋子的图像。</p><p id="47e9" class="pw-post-body-paragraph lh li iq lj b lk mu jr lm ln mv ju lp lq mw ls lt lu mx lw lx ly my ma mb lg ij bi translated">除了图像之外，还提供了一个JSON文件，其中包含与单个图像中每个棋子的边界框相关的所有信息。</p><p id="4e2e" class="pw-post-body-paragraph lh li iq lj b lk mu jr lm ln mv ju lp lq mw ls lt lu mx lw lx ly my ma mb lg ij bi translated">通过编写一个简单的函数，我们可以在应用增强后可视化数据:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="pf pg l"/></div></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ps"><img src="../Images/4df09b202ddc5210803637c6572fa8b7.png" data-original-src="https://miro.medium.com/v2/resize:fit:832/format:webp/1*MFakz3EYf73afrl_aT3S2A.jpeg"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="a143" class="pw-post-body-paragraph lh li iq lj b lk mu jr lm ln mv ju lp lq mw ls lt lu mx lw lx ly my ma mb lg ij bi translated">现在，让我们尝试使用白蛋白创建一个增强管道。</p><p id="24d3" class="pw-post-body-paragraph lh li iq lj b lk mu jr lm ln mv ju lp lq mw ls lt lu mx lw lx ly my ma mb lg ij bi translated">包含注释信息的JSON文件有以下键:</p><pre class="kg kh ki kj gt ny nz oa ob aw oc bi"><span id="28d0" class="od nh iq nz b gy oe of l og oh">dict_keys([‘info’, ‘licenses’, ‘categories’, ‘images’, ‘annotations’])</span></pre><p id="7c4a" class="pw-post-body-paragraph lh li iq lj b lk mu jr lm ln mv ju lp lq mw ls lt lu mx lw lx ly my ma mb lg ij bi translated"><code class="fe pt pu pv nz b">images</code>包含关于图像文件的信息，而<code class="fe pt pu pv nz b">annotations</code>包含关于图像中每个对象的边界框的信息。</p><p id="1335" class="pw-post-body-paragraph lh li iq lj b lk mu jr lm ln mv ju lp lq mw ls lt lu mx lw lx ly my ma mb lg ij bi translated">最后，<code class="fe pt pu pv nz b">categories</code>包含映射到图像中棋子类型的键。</p><pre class="kg kh ki kj gt ny nz oa ob aw oc bi"><span id="d9a9" class="od nh iq nz b gy oe of l og oh">image_list = json_file.get('images')<br/>anno_list = json_file.get('annotations')<br/>cat_list = json_file.get('categories')</span></pre><p id="309e" class="pw-post-body-paragraph lh li iq lj b lk mu jr lm ln mv ju lp lq mw ls lt lu mx lw lx ly my ma mb lg ij bi translated"><code class="fe pt pu pv nz b">image_list</code>:</p><pre class="kg kh ki kj gt ny nz oa ob aw oc bi"><span id="9634" class="od nh iq nz b gy oe of l og oh">[{'id': 0,<br/>  'license': 1,<br/>  'file_name': 'IMG_0317_JPG.rf.00207d2fe8c0a0f20715333d49d22b4f.jpg',<br/>  'height': 416,<br/>  'width': 416,<br/>  'date_captured': '2021-02-23T17:32:58+00:00'},<br/> {'id': 1,<br/>  'license': 1,<br/>  'file_name': '5a8433ec79c881f84ef19a07dc73665d_jpg.rf.00544a8110f323e0d7721b3acf2a9e1e.jpg',<br/>  'height': 416,<br/>  'width': 416,<br/>  'date_captured': '2021-02-23T17:32:58+00:00'},<br/> {'id': 2,<br/>  'license': 1,<br/>  'file_name': '675619f2c8078824cfd182cec2eeba95_jpg.rf.0130e3c26b1bf275bf240894ba73ed7c.jpg',<br/>  'height': 416,<br/>  'width': 416,<br/>  'date_captured': '2021-02-23T17:32:58+00:00'},<br/>.<br/>.<br/>.<br/>.</span></pre><p id="a158" class="pw-post-body-paragraph lh li iq lj b lk mu jr lm ln mv ju lp lq mw ls lt lu mx lw lx ly my ma mb lg ij bi translated"><code class="fe pt pu pv nz b">anno_list</code>:</p><pre class="kg kh ki kj gt ny nz oa ob aw oc bi"><span id="c77b" class="od nh iq nz b gy oe of l og oh">[{'id': 0,<br/>  'image_id': 0,<br/>  'category_id': 7,<br/>  'bbox': [220, 14, 18, 46.023746508293286],<br/>  'area': 828.4274371492792,<br/>  'segmentation': [],<br/>  'iscrowd': 0},<br/> {'id': 1,<br/>  'image_id': 1,<br/>  'category_id': 8,<br/>  'bbox': [187, 103, 22.686527154676014, 59.127992255841036],<br/>  'area': 1341.4088019136107,<br/>  'segmentation': [],<br/>  'iscrowd': 0},<br/> {'id': 2,<br/>  'image_id': 2,<br/>  'category_id': 10,<br/>  'bbox': [203, 24, 24.26037020843023, 60.5],<br/>  'area': 1467.752397610029,<br/>  'segmentation': [],<br/>  'iscrowd': 0},<br/>.<br/>.<br/>.<br/>.</span></pre><p id="63de" class="pw-post-body-paragraph lh li iq lj b lk mu jr lm ln mv ju lp lq mw ls lt lu mx lw lx ly my ma mb lg ij bi translated"><code class="fe pt pu pv nz b">cat_list</code>:</p><pre class="kg kh ki kj gt ny nz oa ob aw oc bi"><span id="254b" class="od nh iq nz b gy oe of l og oh">[{'id': 0, 'name': 'pieces', 'supercategory': 'none'},<br/> {'id': 1, 'name': 'bishop', 'supercategory': 'pieces'},<br/> {'id': 2, 'name': 'black-bishop', 'supercategory': 'pieces'},<br/> {'id': 3, 'name': 'black-king', 'supercategory': 'pieces'},<br/> {'id': 4, 'name': 'black-knight', 'supercategory': 'pieces'},<br/> {'id': 5, 'name': 'black-pawn', 'supercategory': 'pieces'},<br/> {'id': 6, 'name': 'black-queen', 'supercategory': 'pieces'},<br/> {'id': 7, 'name': 'black-rook', 'supercategory': 'pieces'},<br/> {'id': 8, 'name': 'white-bishop', 'supercategory': 'pieces'},<br/> {'id': 9, 'name': 'white-king', 'supercategory': 'pieces'},<br/> {'id': 10, 'name': 'white-knight', 'supercategory': 'pieces'},<br/> {'id': 11, 'name': 'white-pawn', 'supercategory': 'pieces'},<br/> {'id': 12, 'name': 'white-queen', 'supercategory': 'pieces'},<br/> {'id': 13, 'name': 'white-rook', 'supercategory': 'pieces'}]</span></pre><p id="dd85" class="pw-post-body-paragraph lh li iq lj b lk mu jr lm ln mv ju lp lq mw ls lt lu mx lw lx ly my ma mb lg ij bi translated">我们必须改变这些列表的结构，以创建一个高效的管道:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="pf pg l"/></div></figure><p id="a5ea" class="pw-post-body-paragraph lh li iq lj b lk mu jr lm ln mv ju lp lq mw ls lt lu mx lw lx ly my ma mb lg ij bi translated">现在，让我们创建一个简单的增强管道，水平翻转我们的图像，并为边界框添加一个参数:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="pf pg l"/></div></figure><p id="8b21" class="pw-post-body-paragraph lh li iq lj b lk mu jr lm ln mv ju lp lq mw ls lt lu mx lw lx ly my ma mb lg ij bi translated">最后，我们将创建一个类似于Pytorch提供的<a class="ae kv" href="https://pytorch.org/docs/stable/_modules/torch/utils/data/dataset.html#Dataset" rel="noopener ugc nofollow" target="_blank"> <strong class="lj ir">数据集类</strong> </a>的数据集。为此，我们需要定义一个实现方法<code class="fe pt pu pv nz b">__len__</code>和<code class="fe pt pu pv nz b">__getitem__</code>的类。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="pf pg l"/></div></figure><p id="a450" class="pw-post-body-paragraph lh li iq lj b lk mu jr lm ln mv ju lp lq mw ls lt lu mx lw lx ly my ma mb lg ij bi translated">以下是对自定义数据集进行迭代时的一些结果:</p><div class="kg kh ki kj gt ab cb"><figure class="pj kk pk pl pm pn po paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><img src="../Images/591063f6abe7339db810c28a80c855ba.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*YtEKA7Vk452stTAMVj2Nqg.jpeg"/></div></figure><figure class="pj kk pk pl pm pn po paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><img src="../Images/478f3834a4eab5ab92827ccd793e3704.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*FbRBwgH9AEv7Vz0j97hhIw.jpeg"/></div></figure><figure class="pj kk pk pl pm pn po paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><img src="../Images/543c040eb2268f6e5859bed254c39868.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*l524grVZ7FMPDoC2I63Inw.jpeg"/></div><p class="kr ks gj gh gi kt ku bd b be z dk pp di pq pr translated">作者图片</p></figure></div><div class="ab cb"><figure class="pj kk pw pl pm pn po paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><img src="../Images/d2c3c4616c50ee3ac8991763b72db45e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*unTgTu040HyIcnSsiMW_ow.jpeg"/></div></figure><figure class="pj kk pw pl pm pn po paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><img src="../Images/50b907ec50ffeb20094aae48f8704174.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*rWBKH_M8DusN-3m7Kwc3ug.jpeg"/></div><p class="kr ks gj gh gi kt ku bd b be z dk px di py pr translated">作者图片</p></figure></div><p id="8f1e" class="pw-post-body-paragraph lh li iq lj b lk mu jr lm ln mv ju lp lq mw ls lt lu mx lw lx ly my ma mb lg ij bi translated">因此，我们现在可以轻松地将这个自定义数据集传递给数据加载器来训练我们的模型。</p></div><div class="ab cl mz na hu nb" role="separator"><span class="nc bw bk nd ne nf"/><span class="nc bw bk nd ne nf"/><span class="nc bw bk nd ne"/></div><div class="ij ik il im in"><h1 id="c4ae" class="ng nh iq bd ni nj nk nl nm nn no np nq jw nr jx ns jz nt ka nu kc nv kd nw nx bi translated">特征抽出</h1><p id="92a9" class="pw-post-body-paragraph lh li iq lj b lk on jr lm ln oo ju lp lq op ls lt lu oq lw lx ly or ma mb lg ij bi translated">您可能听说过预训练模型用于训练图像分类器和其他监督学习任务。</p><p id="d454" class="pw-post-body-paragraph lh li iq lj b lk mu jr lm ln mv ju lp lq mw ls lt lu mx lw lx ly my ma mb lg ij bi translated">但是，您知道吗，您还可以使用预先训练的模型来提取图像的特征。</p><p id="34c7" class="pw-post-body-paragraph lh li iq lj b lk mu jr lm ln mv ju lp lq mw ls lt lu mx lw lx ly my ma mb lg ij bi translated">简而言之，特征提取是一种降维形式，其中大量像素被简化为更有效的表示。</p><p id="40e9" class="pw-post-body-paragraph lh li iq lj b lk mu jr lm ln mv ju lp lq mw ls lt lu mx lw lx ly my ma mb lg ij bi translated">这主要用于无监督的机器学习任务，如反向图像搜索。</p><p id="bf58" class="pw-post-body-paragraph lh li iq lj b lk mu jr lm ln mv ju lp lq mw ls lt lu mx lw lx ly my ma mb lg ij bi translated">让我们尝试使用Pytorch的预训练模型从图像中提取特征。为此，我们必须首先定义我们的特征提取器类:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="pf pg l"/></div></figure><p id="19a4" class="pw-post-body-paragraph lh li iq lj b lk mu jr lm ln mv ju lp lq mw ls lt lu mx lw lx ly my ma mb lg ij bi translated">请注意，在第4行中，创建了一个新模型，原始模型的所有层都保存为最后一层。您会记得，神经网络中的最后一层是用于预测输出的密集层。</p><p id="cbca" class="pw-post-body-paragraph lh li iq lj b lk mu jr lm ln mv ju lp lq mw ls lt lu mx lw lx ly my ma mb lg ij bi translated">然而，由于我们只对提取特征感兴趣，所以我们不需要这最后一层。因此，它被排除在外。</p><p id="c84e" class="pw-post-body-paragraph lh li iq lj b lk mu jr lm ln mv ju lp lq mw ls lt lu mx lw lx ly my ma mb lg ij bi translated">然后，我们通过将torchvision的预训练<code class="fe pt pu pv nz b">resnet34</code>模型传递给<code class="fe pt pu pv nz b">ResnetFeatureExtractor</code>构造器来利用它。</p><p id="e11f" class="pw-post-body-paragraph lh li iq lj b lk mu jr lm ln mv ju lp lq mw ls lt lu mx lw lx ly my ma mb lg ij bi translated">让我们使用著名的<a class="ae kv" href="https://paperswithcode.com/dataset/cifar-10" rel="noopener ugc nofollow" target="_blank"> <strong class="lj ir"> CIFAR10数据集</strong> </a> (50000张图像)，并对其进行循环以提取特征。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi pz"><img src="../Images/13a6ac8c4b43c75546045acc935bc4bd.png" data-original-src="https://miro.medium.com/v2/resize:fit:726/format:webp/1*wfXJEmLK947yfauzGLDAwQ.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">CIFAR10数据集(<a class="ae kv" href="https://www.kaggle.com/c/cifar-10" rel="noopener ugc nofollow" target="_blank">来源</a>)</p></figure><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="pf pg l"/></div></figure><p id="e4c8" class="pw-post-body-paragraph lh li iq lj b lk mu jr lm ln mv ju lp lq mw ls lt lu mx lw lx ly my ma mb lg ij bi translated">我们现在有50000个图像特征向量的列表，每个特征向量的大小为512(原始resnet模型的倒数第二层的输出大小)。</p><pre class="kg kh ki kj gt ny nz oa ob aw oc bi"><span id="5701" class="od nh iq nz b gy oe of l og oh">print(f"Number of feature vectors: {len(feature_list)}") #50000<br/>print(f"Number of feature vectors: {len(feature_list[0])}") #512</span></pre><p id="0660" class="pw-post-body-paragraph lh li iq lj b lk mu jr lm ln mv ju lp lq mw ls lt lu mx lw lx ly my ma mb lg ij bi translated">因此，这个特征向量列表现在可以被诸如KNN的统计学习模型用来搜索相似的图像。</p></div><div class="ab cl mz na hu nb" role="separator"><span class="nc bw bk nd ne nf"/><span class="nc bw bk nd ne nf"/><span class="nc bw bk nd ne"/></div><div class="ij ik il im in"><p id="511a" class="pw-post-body-paragraph lh li iq lj b lk mu jr lm ln mv ju lp lq mw ls lt lu mx lw lx ly my ma mb lg ij bi translated">如果你已经走到这一步，那么非常感谢你阅读这篇文章！我希望你有美好的一天！😄</p><p id="c02e" class="pw-post-body-paragraph lh li iq lj b lk mu jr lm ln mv ju lp lq mw ls lt lu mx lw lx ly my ma mb lg ij bi translated"><strong class="lj ir">👉</strong> <a class="ae kv" href="https://github.com/Sayar1106/TowardsDataSciencecodefiles/tree/master/fast_feature_engineering" rel="noopener ugc nofollow" target="_blank"> <strong class="lj ir">文章中使用的代码</strong> </a></p><p id="0a52" class="pw-post-body-paragraph lh li iq lj b lk mu jr lm ln mv ju lp lq mw ls lt lu mx lw lx ly my ma mb lg ij bi translated">下次见！✋</p><h1 id="8e3d" class="ng nh iq bd ni nj oi nl nm nn oj np nq jw ok jx ns jz ol ka nu kc om kd nw nx bi translated">参考资料:</h1><ul class=""><li id="1923" class="qa qb iq lj b lk on ln oo lq qc lu qd ly qe lg qf qg qh qi bi translated"><a class="ae kv" href="https://www.cs.toronto.edu/~kriz/cifar.html" rel="noopener ugc nofollow" target="_blank">https://www.cs.toronto.edu/~kriz/cifar.html</a></li><li id="bb52" class="qa qb iq lj b lk qj ln qk lq ql lu qm ly qn lg qf qg qh qi bi translated"><a class="ae kv" href="https://www.practicaldeeplearning.ai/" rel="noopener ugc nofollow" target="_blank">https://www.practicaldeeplearning.ai/</a></li></ul></div></div>    
</body>
</html>