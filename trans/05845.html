<html>
<head>
<title>Understanding Loss Functions the Smart Way</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">聪明地理解损失函数</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/understanding-loss-functions-the-smart-way-904266e9393?source=collection_archive---------16-----------------------#2021-05-25">https://towardsdatascience.com/understanding-loss-functions-the-smart-way-904266e9393?source=collection_archive---------16-----------------------#2021-05-25</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="e7da" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">了解损失函数如何用于理解模型的性能及其在Python中回归和分类问题中的实现。</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/019c2f352571dcde778b0e10c3456c56.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*f667V2zC9oCRx6bs"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">弗朗西斯科·卡里法诺在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="b1ec" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi lv translated"><span class="l lw lx ly bm lz ma mb mc md di">在</span>这篇文章中，我们将深入探讨用于提高机器学习算法性能的不同类型的损失函数。任何经典机器学习问题的主要动机都只是这两件事:</p><ol class=""><li id="fd8e" class="me mf it lb b lc ld lf lg li mg lm mh lq mi lu mj mk ml mm bi translated"><strong class="lb iu">提高模型的精确度</strong>以及</li><li id="9bfc" class="me mf it lb b lc mn lf mo li mp lm mq lq mr lu mj mk ml mm bi translated"><strong class="lb iu">减少与此相关的损失</strong>。</li></ol><blockquote class="ms mt mu"><p id="6f57" class="kz la mv lb b lc ld ju le lf lg jx lh mw lj lk ll mx ln lo lp my lr ls lt lu im bi translated">我们可以说<strong class="lb iu">损失</strong>是你为你的模型<strong class="lb iu">坏预测</strong>付出的<strong class="lb iu">代价</strong>。</p></blockquote><p id="86d3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在这一系列的三篇文章中，我们将讨论什么是损失函数，以及如何使用各种类型的损失函数来优化我们模型的性能。在第2部分，我们将研究回归损失函数，在第3部分，我们将探索分类函数。</p><p id="c9c4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">本系列充满了Python3中的实际操作代码块，给你一个很好的实践经验，告诉你如何用Python实现它们。</p><h2 id="d0d9" class="mz na it bd nb nc nd dn ne nf ng dp nh li ni nj nk lm nl nm nn lq no np nq nr bi translated">要跳到任何部分:</h2><ol class=""><li id="9ded" class="me mf it lb b lc ns lf nt li nu lm nv lq nw lu mj mk ml mm bi translated">第1部分:<strong class="lb iu">了解损失函数</strong>。</li><li id="3d56" class="me mf it lb b lc mn lf mo li mp lm mq lq mr lu mj mk ml mm bi translated">第二部分:<strong class="lb iu">回归损失函数</strong>。</li><li id="383a" class="me mf it lb b lc mn lf mo li mp lm mq lq mr lu mj mk ml mm bi translated">第三部分:<strong class="lb iu">分类损失函数</strong>。</li></ol></div><div class="ab cl nx ny hx nz" role="separator"><span class="oa bw bk ob oc od"/><span class="oa bw bk ob oc od"/><span class="oa bw bk ob oc"/></div><div class="im in io ip iq"><h2 id="654c" class="mz na it bd nb nc nd dn ne nf ng dp nh li ni nj nk lm nl nm nn lq no np nq nr bi translated">什么是损失函数？</h2><p id="19f5" class="pw-post-body-paragraph kz la it lb b lc ns ju le lf nt jx lh li oe lk ll lm of lo lp lq og ls lt lu im bi translated">准确地说，如果你的模型预测完美，那么你的损失会很少，否则损失会很大。不管问题陈述如何，通过将<strong class="lb iu"> <em class="mv">预测值</em> </strong>与<strong class="lb iu"> <em class="mv">实际值</em> </strong>进行比较，总会有<strong class="lb iu">性能评估</strong>。这就是损失函数发挥作用的地方。</p><p id="8835" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">损失函数的真正应用不仅仅是它的<strong class="lb iu">内在</strong> <strong class="lb iu">值</strong>，而是它如何被用来改善模型的性能，因此使用了成本函数。损失函数将在每个训练示例中计算，而<strong class="lb iu">成本函数</strong>将是损失函数的平均值。因此，在单个训练数据集中，损失函数将被计算多次，而成本函数将仅被计算一次。</p></div><div class="ab cl nx ny hx nz" role="separator"><span class="oa bw bk ob oc od"/><span class="oa bw bk ob oc od"/><span class="oa bw bk ob oc"/></div><div class="im in io ip iq"><h2 id="8eca" class="mz na it bd nb nc nd dn ne nf ng dp nh li ni nj nk lm nl nm nn lq no np nq nr bi translated">我们如何利用损失函数的力量？</h2><p id="6f3e" class="pw-post-body-paragraph kz la it lb b lc ns ju le lf nt jx lh li oe lk ll lm of lo lp lq og ls lt lu im bi translated">所以我们的最终目标是最小化成本函数。让我们用一个简单的实时例子来理解它。想象一下，在一个黑暗的夜晚，你站在一座山脉的顶峰。当你完成了一整天的徒步旅行，现在想下去，但外面很黑，你会怎么做？你肯定会试图用最少的能量下山，而你想到的第一件事就是环顾四周，试图用你的脚找到最短的路径。你不可能上山，因为你的目标是下山，所以唯一的选择是选择<strong class="lb iu">下山</strong>的路。所以这就是你估计的计算，如果你走下坡路，将会花费<strong class="lb iu">少得多的能量</strong>和<strong class="lb iu">努力</strong>，这正是损失函数所做的。因此，我们的目标是“<strong class="lb iu">找到模型的权重和偏差，使损失最小</strong>。”</p><blockquote class="ms mt mu"><p id="58a8" class="kz la mv lb b lc ld ju le lf lg jx lh mw lj lk ll mx ln lo lp my lr ls lt lu im bi translated">在自然界中，我们看到许多遵循这种方法的例子。</p></blockquote><blockquote class="oh"><p id="241e" class="oi oj it bd ok ol om on oo op oq lu dk translated">不管路上遇到什么，一条河总是走最短的下坡路。</p></blockquote></div><div class="ab cl nx ny hx nz" role="separator"><span class="oa bw bk ob oc od"/><span class="oa bw bk ob oc od"/><span class="oa bw bk ob oc"/></div><div class="im in io ip iq"><h2 id="a06e" class="mz na it bd nb nc nd dn ne nf ng dp nh li ni nj nk lm nl nm nn lq no np nq nr bi translated">分类和回归的损失函数:</h2><p id="2eab" class="pw-post-body-paragraph kz la it lb b lc ns ju le lf nt jx lh li oe lk ll lm of lo lp lq og ls lt lu im bi translated">现在，既然我们知道有两种类型的<strong class="lb iu">监督的</strong>机器学习问题:<strong class="lb iu">回归(连续目标值)</strong>和<strong class="lb iu">分类(离散目标值)</strong>，那么让我们来探究它们中的每一个，看看所有类型的损失函数用于它们中的每一个，并基于它们对于提高机器学习<a class="ae ky" rel="noopener" target="_blank" href="/how-to-evaluate-machine-learning-model-performance-in-python-135b4ae27f7e">模型的性能</a>的效率来比较它们。要了解更多关于您可以使用一些技术来测量性能的信息，请在完成这篇文章后访问这篇文章以了解更多信息。</p><div class="or os gp gr ot ou"><a rel="noopener follow" target="_blank" href="/how-to-evaluate-machine-learning-model-performance-in-python-135b4ae27f7e"><div class="ov ab fo"><div class="ow ab ox cl cj oy"><h2 class="bd iu gy z fp oz fr fs pa fu fw is bi translated">如何评价Python中机器学习模型性能？</h2><div class="pb l"><h3 class="bd b gy z fp oz fr fs pa fu fw dk translated">一个实用的方法来计算模型的性能和在Python中的实现，涵盖了所有数学…</h3></div><div class="pc l"><p class="bd b dl z fp oz fr fs pa fu fw dk translated">towardsdatascience.com</p></div></div><div class="pd l"><div class="pe l pf pg ph pd pi ks ou"/></div></div></a></div></div><div class="ab cl nx ny hx nz" role="separator"><span class="oa bw bk ob oc od"/><span class="oa bw bk ob oc od"/><span class="oa bw bk ob oc"/></div><div class="im in io ip iq"><h1 id="84ac" class="pj na it bd nb pk pl pm ne pn po pp nh jz pq ka nk kc pr kd nn kf ps kg nq pt bi translated">回归中使用的损失函数:</h1><p id="1dea" class="pw-post-body-paragraph kz la it lb b lc ns ju le lf nt jx lh li oe lk ll lm of lo lp lq og ls lt lu im bi translated">回归或回归分析是一种<strong class="lb iu">监督</strong> <strong class="lb iu">学习</strong> <strong class="lb iu">技术</strong>，可以定义为统计技术，用于对因变量实数变量<strong class="lb iu"> y </strong>和自变量<strong class="lb iu">【Xi】T29】之间的关系进行建模。<strong class="lb iu"> </strong>根据不同的场景及其重要性，我们使用不同类型的<em class="mv">回归方法。</em>让我们来了解各种可能的损失函数，这些函数可用于改进回归分析中的性能评估。</strong></p><h2 id="7136" class="mz na it bd nb nc nd dn ne nf ng dp nh li ni nj nk lm nl nm nn lq no np nq nr bi translated">1.均方差/L2损耗:</h2><p id="9ae6" class="pw-post-body-paragraph kz la it lb b lc ns ju le lf nt jx lh li oe lk ll lm of lo lp lq og ls lt lu im bi translated">这是最常用的损失函数，因为它非常容易理解和实现。它适用于几乎所有的回归问题。顾名思义，<strong class="lb iu"/><strong class="lb iu">均方差就是平方误差的平均值</strong>。这里的误差是模型预测值“<strong class="lb iu"> y_hat </strong>与实际值“<strong class="lb iu"> y_real </strong>之差。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pu"><img src="../Images/e500595f10b0a977da7fa2821e736ff6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1134/0*sDmAdS2uFhE4Vqb7"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><a class="ae ky" href="https://lh4.googleusercontent.com/YpZaHy2Nyz4pP01Sb51iYKSHuDLuvxh1BDgWho4jk3WlBuWNDUd5AdmA3rPD_8jVZ430cjuwGZbhBAE2hAoNITazepO2hxudbyAHbMrKkf0jIGEuVUvJfkjtkElV4SNLMLkLcrIJsKfnVm7Lxw" rel="noopener ugc nofollow" target="_blank">了解实际值和预测值之间的误差</a></p></figure><p id="0498" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在上图中，小圆圈是不同的<strong class="lb iu"> y </strong>的<strong class="lb iu"> x </strong>的实际值。这条线代表我们通过模型<strong class="lb iu">预测</strong>的<strong class="lb iu">最佳拟合</strong>。因此，这里的虚线是误差，<strong class="lb iu"> MSE </strong>是所有误差的平方平均值。</p><blockquote class="ms mt mu"><p id="45f8" class="kz la mv lb b lc ld ju le lf lg jx lh mw lj lk ll mx ln lo lp my lr ls lt lu im bi translated">平方是为了放大大的误差，也是为了观察我们损失的方差。</p></blockquote><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pv"><img src="../Images/2317cf5c2439db2297f8dc1c6541c312.png" data-original-src="https://miro.medium.com/v2/resize:fit:1026/0*3xP5-O9ASW73U9Bo"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">均方误差/L2损耗</p></figure><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="pw px l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">Python3中MSE丢失的代码实现</p></figure></div><div class="ab cl nx ny hx nz" role="separator"><span class="oa bw bk ob oc od"/><span class="oa bw bk ob oc od"/><span class="oa bw bk ob oc"/></div><div class="im in io ip iq"><h2 id="6198" class="mz na it bd nb nc nd dn ne nf ng dp nh li ni nj nk lm nl nm nn lq no np nq nr bi translated">2.平均绝对误差/ L1损耗:</h2><p id="1747" class="pw-post-body-paragraph kz la it lb b lc ns ju le lf nt jx lh li oe lk ll lm of lo lp lq og ls lt lu im bi translated">平均绝对误差是<strong class="lb iu">模型预测值和实际值之间的绝对差值的平均值</strong>。<strong class="lb iu"> <em class="mv">平均绝对误差在异常值的情况下更稳健</em> </strong>因为不适合求差的平方，因为差太大了。</p><p id="4cd4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">不管误差的方向是正还是负，因为它们可能会相互抵消，从而不会产生误差，在这种情况下，我们只考虑其<strong class="lb iu">大小</strong>。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi py"><img src="../Images/5c000d1133735a2d9ead5a1056514e8b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1066/0*5rnPkkmsotFCXWn1"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">平均绝对误差/ L1损耗</p></figure><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="pw px l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">Python3中MAE丢失的代码实现</p></figure></div><div class="ab cl nx ny hx nz" role="separator"><span class="oa bw bk ob oc od"/><span class="oa bw bk ob oc od"/><span class="oa bw bk ob oc"/></div><div class="im in io ip iq"><h2 id="4af8" class="mz na it bd nb nc nd dn ne nf ng dp nh li ni nj nk lm nl nm nn lq no np nq nr bi translated">3.均方根误差:</h2><p id="d457" class="pw-post-body-paragraph kz la it lb b lc ns ju le lf nt jx lh li oe lk ll lm of lo lp lq og ls lt lu im bi translated">均方根误差是均方差的<strong class="lb iu">平方根。当我们将平方根应用于MSE时，RMSE归结为与目标变量相同的单位。RMSE的损失函数与MSE完全相同，我们只是将阶数从2降低到1，这样就可以轻松地进行相关运算。MSE中的相关性变得困难，因为它<strong class="lb iu">严重惩罚大误差。</strong></strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="ab gu cl pz"><img src="../Images/c706fb991eeaea7f61b853c68fe6d301.png" data-original-src="https://miro.medium.com/v2/0*eymu5MlrHpQ1IhEe"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">均方根误差</p></figure><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="pw px l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">Python3中RMS损耗的代码实现</p></figure></div><div class="ab cl nx ny hx nz" role="separator"><span class="oa bw bk ob oc od"/><span class="oa bw bk ob oc od"/><span class="oa bw bk ob oc"/></div><div class="im in io ip iq"><h2 id="cdb9" class="mz na it bd nb nc nd dn ne nf ng dp nh li ni nj nk lm nl nm nn lq no np nq nr bi translated">4.平均偏差误差:</h2><p id="058d" class="pw-post-body-paragraph kz la it lb b lc ns ju le lf nt jx lh li oe lk ll lm of lo lp lq og ls lt lu im bi translated">平均偏差误差是预测值和实际值 <strong class="lb iu">之间的精确<strong class="lb iu">差，没有应用任何数学函数，如绝对值或平方根</strong>。<em class="mv">MBE的主要局限是正负误差有机会</em> <strong class="lb iu"> <em class="mv">抵消</em></strong><em class="mv"/>。这就是为什么这是很少使用和不太流行的损失函数。</strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi qa"><img src="../Images/bc92c1bfd9d8a118bf3a7560f09f4a52.png" data-original-src="https://miro.medium.com/v2/resize:fit:992/0*RKzeGF-qVvQT2cmo"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">平均偏差误差</p></figure><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="pw px l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">Python3中均值偏差损失的代码实现</p></figure></div><div class="ab cl nx ny hx nz" role="separator"><span class="oa bw bk ob oc od"/><span class="oa bw bk ob oc od"/><span class="oa bw bk ob oc"/></div><div class="im in io ip iq"><h2 id="47e2" class="mz na it bd nb nc nd dn ne nf ng dp nh li ni nj nk lm nl nm nn lq no np nq nr bi translated">5.胡伯损失:</h2><p id="a847" class="pw-post-body-paragraph kz la it lb b lc ns ju le lf nt jx lh li oe lk ll lm of lo lp lq og ls lt lu im bi translated">正如我们已经讨论过的， <strong class="lb iu"> <em class="mv"> MSE </em> </strong> <em class="mv">和</em> <strong class="lb iu"> <em class="mv"> MAE </em> </strong>都有缺点，所以这就是胡贝尔的损失所在。<strong class="lb iu">它只从MSE和MAE中提取有益的东西，并将它们纳入一个损失函数。</strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi qb"><img src="../Images/484ddbc008b6bed8ec66b6de4403ba2b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Wrb-T20uOaOn68Y5_C9Zog.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">使用MSE和MAE的Huber损失</p></figure><p id="23ff" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">所以简而言之，<strong class="lb iu"> <em class="mv"> Huber损失对于小误差接近MSE，对于异常值接近MAE</em></strong>。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi qc"><img src="../Images/f144fc1057b7f30c4180a329dc320cd6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1102/0*c7R9YQ-e14hm5obV"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">胡伯损失</p></figure><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="pw px l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">Python3中Huber丢失的代码实现</p></figure></div><div class="ab cl nx ny hx nz" role="separator"><span class="oa bw bk ob oc od"/><span class="oa bw bk ob oc od"/><span class="oa bw bk ob oc"/></div><div class="im in io ip iq"><h1 id="3943" class="pj na it bd nb pk pl pm ne pn po pp nh jz pq ka nk kc pr kd nn kf ps kg nq pt bi translated">二元分类损失函数；</h1><h2 id="073b" class="mz na it bd nb nc nd dn ne nf ng dp nh li ni nj nk lm nl nm nn lq no np nq nr bi translated">1.二元交叉熵损失；</h2><p id="c3ff" class="pw-post-body-paragraph kz la it lb b lc ns ju le lf nt jx lh li oe lk ll lm of lo lp lq og ls lt lu im bi translated">顾名思义，二元分类是指<strong class="lb iu">将一个物体分为两类</strong>中的任何一类。例如，将电子邮件分类为垃圾邮件或非垃圾邮件。<strong class="lb iu"> <em class="mv">熵基本上是不确定性的度量。</em> </strong>交叉熵是<strong class="lb iu"> <em class="mv">两个随机变量</em> </strong>的不确定性之差。</p><p id="e654" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">所以，<strong class="lb iu"> <em class="mv">如果实际类与预测概率的差异越多，那么交叉熵损失就越多。</em> </strong>例如，我们假设一个数据样本的<strong class="lb iu">实际类别为1，</strong>，<strong class="lb iu">预测概率为0.2 </strong>。这导致<strong class="lb iu">高损耗值</strong>。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi qd"><img src="../Images/65e8d29d5fbd2a851b8ff5af113d241f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*tAgnuL9LSsAurmoZ"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">二元交叉熵损失</p></figure><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="pw px l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">Python3中二值交叉熵损失的代码实现</p></figure></div><div class="ab cl nx ny hx nz" role="separator"><span class="oa bw bk ob oc od"/><span class="oa bw bk ob oc od"/><span class="oa bw bk ob oc"/></div><div class="im in io ip iq"><h2 id="27d3" class="mz na it bd nb nc nd dn ne nf ng dp nh li ni nj nk lm nl nm nn lq no np nq nr bi translated">2.铰链损耗:</h2><p id="37e9" class="pw-post-body-paragraph kz la it lb b lc ns ju le lf nt jx lh li oe lk ll lm of lo lp lq og ls lt lu im bi translated">铰链损耗主要用于<strong class="lb iu">支持向量机</strong>。任何分类问题中的最佳可能行都会尽可能少地犯分类错误。为了通过计算来表达这一思想，<strong class="lb iu">铰链损失惩罚每一个不正确的分类。</strong></p><p id="a36f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">例如，假设<strong class="lb iu">的实际值是1 </strong>，而<strong class="lb iu">的预测值是-1 </strong>，那么“<strong class="lb iu"> l(y) </strong>就变成了<strong class="lb iu"> 2 </strong>，这是一个更高的损失。而如果预测值和实际值都是1并且匹配，那么“<strong class="lb iu"> l(y) </strong>就变成了<strong class="lb iu"> 0 </strong>也就是没有损失。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi qe"><img src="../Images/3f4e6b949fe6c106c88514c66ace9418.png" data-original-src="https://miro.medium.com/v2/resize:fit:1382/0*iBWGaKmAwAxSnysW"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">铰链损耗</p></figure><blockquote class="ms mt mu"><p id="0790" class="kz la mv lb b lc ld ju le lf lg jx lh mw lj lk ll mx ln lo lp my lr ls lt lu im bi translated"><em class="it">铰链损失在[-1，1]值范围内工作良好，有时甚至比二元熵损失更好。</em></p></blockquote><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="pw px l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">Python3中铰链损耗的代码实现</p></figure></div><div class="ab cl nx ny hx nz" role="separator"><span class="oa bw bk ob oc od"/><span class="oa bw bk ob oc od"/><span class="oa bw bk ob oc"/></div><div class="im in io ip iq"><h2 id="9f2b" class="mz na it bd nb nc nd dn ne nf ng dp nh li ni nj nk lm nl nm nn lq no np nq nr bi translated">3.平方铰链损耗:</h2><p id="7dbd" class="pw-post-body-paragraph kz la it lb b lc ns ju le lf nt jx lh li oe lk ll lm of lo lp lq og ls lt lu im bi translated">铰链损耗的平方就是铰链损耗的<strong class="lb iu">平方。I <strong class="lb iu"> <em class="mv"> f你想惩罚这个大错误，平方铰链损失进入画面</em> </strong>。除了<strong class="lb iu">输出将被平方</strong>之外，最大值功能将是相同的。铰链损耗的平方<strong class="lb iu">与误差的平方</strong>成正比。</strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi qf"><img src="../Images/0c2c718321751e18c79be72bf902d18b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*m8QKBRYA1ENbcRcE"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">平方铰链损耗</p></figure><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="pw px l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">Python3中平方铰链损耗的代码实现</p></figure></div><div class="ab cl nx ny hx nz" role="separator"><span class="oa bw bk ob oc od"/><span class="oa bw bk ob oc od"/><span class="oa bw bk ob oc"/></div><div class="im in io ip iq"><h1 id="1b54" class="pj na it bd nb pk pl pm ne pn po pp nh jz pq ka nk kc pr kd nn kf ps kg nq pt bi translated">多项式分类损失函数；</h1><h2 id="73cb" class="mz na it bd nb nc nd dn ne nf ng dp nh li ni nj nk lm nl nm nn lq no np nq nr bi translated">1.分类交叉熵损失:</h2><p id="8fc1" class="pw-post-body-paragraph kz la it lb b lc ns ju le lf nt jx lh li oe lk ll lm of lo lp lq og ls lt lu im bi translated">这是<strong class="lb iu"> <em class="mv">最常用的损失函数</em> </strong>，因为它非常容易理解和实现。它几乎适用于所有的分类问题。<strong class="lb iu">分类交叉熵损失可以认为是二元交叉熵损失的推广。</strong> <strong class="lb iu"> <em class="mv">当班级数量超过2个</em> </strong>时使用。为了计算分类交叉熵损失，最佳实践是对<strong class="lb iu">分类进行一次性编码</strong>。需要为每个观察的每个类别标签计算损失，并对结果 求和<strong class="lb iu"> <em class="mv">。</em></strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi qg"><img src="../Images/f5f6cec9e2a0dd88bd30b2b429efecbf.png" data-original-src="https://miro.medium.com/v2/resize:fit:640/0*g3oP2CL-hhOP5UEP"/></div></figure><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="pw px l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">Python3中分类交叉熵损失的代码实现</p></figure></div><div class="ab cl nx ny hx nz" role="separator"><span class="oa bw bk ob oc od"/><span class="oa bw bk ob oc od"/><span class="oa bw bk ob oc"/></div><div class="im in io ip iq"><h2 id="abd7" class="mz na it bd nb nc nd dn ne nf ng dp nh li ni nj nk lm nl nm nn lq no np nq nr bi translated">2.KL (Kullback-Leibler)散度:</h2><p id="93d2" class="pw-post-body-paragraph kz la it lb b lc ns ju le lf nt jx lh li oe lk ll lm of lo lp lq og ls lt lu im bi translated">KL散度衡量任意两个分布之间的差异。它帮助我们理解，“<strong class="lb iu"> <em class="mv">当我们追求一个近似值</em> </strong>”时，我们丢失了多少信息。</p><p id="23d0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">是对<strong class="lb iu">熵</strong>的轻微修改。除了我们的概率分布，我们还添加了一个近似分布，并计算两个分布之间的差异。</p><p id="5737" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">通过这种方式，我们可以追踪我们因近似而丢失的信息。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi qh"><img src="../Images/3531353ce72fcc1479b63838e4916775.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*tVLy42cfN2asje51"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">KL(库尔巴克-莱布勒)散度</p></figure><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="pw px l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">Python3中KL发散损失的代码实现</p></figure></div><div class="ab cl nx ny hx nz" role="separator"><span class="oa bw bk ob oc od"/><span class="oa bw bk ob oc od"/><span class="oa bw bk ob oc"/></div><div class="im in io ip iq"><p id="9e12" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">希望你喜欢阅读！</p><p id="e667" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">你可以在这里阅读更多我的文章:</p><div class="or os gp gr ot ou"><a rel="noopener follow" target="_blank" href="/q-q-plots-explained-5aa8495426c0"><div class="ov ab fo"><div class="ow ab ox cl cj oy"><h2 class="bd iu gy z fp oz fr fs pa fu fw is bi translated">Q-Q图解释</h2><div class="pb l"><h3 class="bd b gy z fp oz fr fs pa fu fw dk translated">探索Q-Q图的力量。</h3></div><div class="pc l"><p class="bd b dl z fp oz fr fs pa fu fw dk translated">towardsdatascience.com</p></div></div><div class="pd l"><div class="qi l pf pg ph pd pi ks ou"/></div></div></a></div><div class="or os gp gr ot ou"><a rel="noopener follow" target="_blank" href="/how-to-learn-data-science-from-beginners-to-masters-in-just-1-year-my-personal-experience-6152bedd8157"><div class="ov ab fo"><div class="ow ab ox cl cj oy"><h2 class="bd iu gy z fp oz fr fs pa fu fw is bi translated">如何在短短1年内把数据科学从初学者学到大师(我的亲身经历)</h2><div class="pb l"><h3 class="bd b gy z fp oz fr fs pa fu fw dk translated">随着时间的推移，我的从初学者到大师学习数据科学的清单的完整汇编只需要一年时间…</h3></div><div class="pc l"><p class="bd b dl z fp oz fr fs pa fu fw dk translated">towardsdatascience.com</p></div></div><div class="pd l"><div class="qj l pf pg ph pd pi ks ou"/></div></div></a></div><div class="or os gp gr ot ou"><a rel="noopener follow" target="_blank" href="/how-i-became-kaggle-3x-expert-in-just-1-month-b63b37b53865"><div class="ov ab fo"><div class="ow ab ox cl cj oy"><h2 class="bd iu gy z fp oz fr fs pa fu fw is bi translated">我成为Kaggle大师的旅程</h2><div class="pb l"><h3 class="bd b gy z fp oz fr fs pa fu fw dk translated">先说说我成为三大类Kaggle专家，后来成为笔记本高手的故事。还有…</h3></div><div class="pc l"><p class="bd b dl z fp oz fr fs pa fu fw dk translated">towardsdatascience.com</p></div></div><div class="pd l"><div class="qk l pf pg ph pd pi ks ou"/></div></div></a></div><p id="aea1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">谢谢</strong>！</p></div></div>    
</body>
</html>