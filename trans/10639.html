<html>
<head>
<title>How To Add a New Column To a PySpark DataFrame</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如何向PySpark数据框架添加新列</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/add-new-column-pyspark-dataframe-e1ebee323fdb?source=collection_archive---------10-----------------------#2021-10-12">https://towardsdatascience.com/add-new-column-pyspark-dataframe-e1ebee323fdb?source=collection_archive---------10-----------------------#2021-10-12</a></blockquote><div><div class="fc ii ij ik il im"/><div class="in io ip iq ir"><div class=""/><div class=""><h2 id="cad9" class="pw-subtitle-paragraph jr it iu bd b js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki dk translated">探索向现有Spark数据框架添加新列的多种方法</h2></div><figure class="kk kl km kn gu ko gi gj paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gi gj kj"><img src="../Images/8026ff7ee60d19b843c27ecbbf534c36.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DVnj_j3Df-zi_FBSUA1aNQ.jpeg"/></div></div><p class="kv kw gk gi gj kx ky bd b be z dk translated"><a class="ae kz" href="https://unsplash.com/@adrian_trinkaus?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Adrian Trinkaus </a>在<a class="ae kz" href="https://unsplash.com/s/photos/column?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><h2 id="3daa" class="la lb iu bd lc ld le dn lf lg lh dp li lj lk ll lm ln lo lp lq lr ls lt lu lv bi translated">介绍</h2><p id="395e" class="pw-post-body-paragraph lw lx iu ly b lz ma jv mb mc md jy me lj mf mg mh ln mi mj mk lr ml mm mn mo in bi translated">向PySpark数据帧添加新列可能是您在日常工作中需要执行的最常见操作之一。</p><p id="f184" class="pw-post-body-paragraph lw lx iu ly b lz mp jv mb mc mq jy me lj mr mg mh ln ms mj mk lr mt mm mn mo in bi translated">在今天的简短指南中，我们将讨论如何以多种不同的方式做到这一点。具体来说，我们将探索如何添加新列并填充它们</p><ul class=""><li id="0e62" class="mu mv iu ly b lz mp mc mq lj mw ln mx lr my mo mz na nb nc bi translated">带文字</li><li id="bf20" class="mu mv iu ly b lz nd mc ne lj nf ln ng lr nh mo mz na nb nc bi translated">通过转换现有列</li><li id="21ec" class="mu mv iu ly b lz nd mc ne lj nf ln ng lr nh mo mz na nb nc bi translated">使用联接</li><li id="1c36" class="mu mv iu ly b lz nd mc ne lj nf ln ng lr nh mo mz na nb nc bi translated">使用函数或UDF</li></ul></div><div class="ab cl ni nj hy nk" role="separator"><span class="nl bw bk nm nn no"/><span class="nl bw bk nm nn no"/><span class="nl bw bk nm nn"/></div><div class="in io ip iq ir"><p id="5840" class="pw-post-body-paragraph lw lx iu ly b lz mp jv mb mc mq jy me lj mr mg mh ln ms mj mk lr mt mm mn mo in bi translated">首先，让我们创建一个示例数据框架，我们将在本文中引用它来演示我们感兴趣的概念</p><pre class="kk kl km kn gu np nq nr ns aw nt bi"><span id="915d" class="la lb iu nq b gz nu nv l nw nx">from pyspark.sql import SparkSession</span><span id="b0bb" class="la lb iu nq b gz ny nv l nw nx"># Create an instance of spark session<br/>spark_session = SparkSession.builder \<br/>    .master('local[1]') \<br/>    .appName('Example') \<br/>    .getOrCreate()</span><span id="7cc2" class="la lb iu nq b gz ny nv l nw nx"># Create an example DataFrame<br/>df = spark_session.createDataFrame(<br/>    [<br/>        (1, True, 'a', 1.0),<br/>        (2, True, 'b', 2.0),<br/>        (3, False, None, 3.0),<br/>        (4, False, 'd', None),<br/>    ],<br/>    ['colA', 'colB', 'colC', 'colD']<br/>)</span><span id="522a" class="la lb iu nq b gz ny nv l nw nx">df.show()<br/><em class="nz">+----+-----+----+----+<br/>|colA| colB|colC|colD|<br/>+----+-----+----+----+<br/>|   1| true|   a| 1.0|<br/>|   2| true|   b| 2.0|<br/>|   3|false|null| 3.0|<br/>|   4|false|   d|null|<br/>+----+-----+----+----+</em></span></pre></div><div class="ab cl ni nj hy nk" role="separator"><span class="nl bw bk nm nn no"/><span class="nl bw bk nm nn no"/><span class="nl bw bk nm nn"/></div><div class="in io ip iq ir"><h2 id="0644" class="la lb iu bd lc ld le dn lf lg lh dp li lj lk ll lm ln lo lp lq lr ls lt lu lv bi translated">使用文本添加新列</h2><p id="6259" class="pw-post-body-paragraph lw lx iu ly b lz ma jv mb mc md jy me lj mf mg mh ln mi mj mk lr ml mm mn mo in bi translated">假设您想要添加一个包含文字的新列，您可以利用用于创建文字列的<code class="fe oa ob oc nq b"><a class="ae kz" href="https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.sql.functions.lit.html" rel="noopener ugc nofollow" target="_blank">pyspark.sql.functions.lit</a></code>函数。</p><p id="0eb9" class="pw-post-body-paragraph lw lx iu ly b lz mp jv mb mc mq jy me lj mr mg mh ln ms mj mk lr mt mm mn mo in bi translated">例如，下面的命令将在每行中添加一个名为<code class="fe oa ob oc nq b">colE</code>的新列，其中包含<code class="fe oa ob oc nq b">100</code>的值。</p><pre class="kk kl km kn gu np nq nr ns aw nt bi"><span id="f8a8" class="la lb iu nq b gz nu nv l nw nx"><strong class="nq iv">df.withColumn('colE', lit(100))</strong></span><span id="5d13" class="la lb iu nq b gz ny nv l nw nx">df.show()<br/><em class="nz">+----+-----+----+----+----+<br/>|colA| colB|colC|colD|colE|<br/>+----+-----+----+----+----+<br/>|   1| true|   a| 1.0| 100|<br/>|   2| true|   b| 2.0| 100|<br/>|   3|false|null| 3.0| 100|<br/>|   4|false|   d|null| 100|<br/>+----+-----+----+----+----+</em></span></pre><p id="f01e" class="pw-post-body-paragraph lw lx iu ly b lz mp jv mb mc mq jy me lj mr mg mh ln ms mj mk lr mt mm mn mo in bi translated">注意，您必须使用<code class="fe oa ob oc nq b">lit</code>函数，因为<code class="fe oa ob oc nq b">withColumn</code>的第二个参数必须是类型<code class="fe oa ob oc nq b">Column</code>。</p><p id="c53b" class="pw-post-body-paragraph lw lx iu ly b lz mp jv mb mc mq jy me lj mr mg mh ln ms mj mk lr mt mm mn mo in bi translated">现在，如果您想添加一个包含更复杂数据结构(如数组)的列，可以如下所示进行操作:</p><pre class="kk kl km kn gu np nq nr ns aw nt bi"><span id="e7c8" class="la lb iu nq b gz nu nv l nw nx">from pyspark.sql.functions import lit, array</span><span id="2cf9" class="la lb iu nq b gz ny nv l nw nx"><strong class="nq iv">df = df.withColumn('colE', array(lit(100), lit(200), lit(300)))</strong></span><span id="bb39" class="la lb iu nq b gz ny nv l nw nx">df.show()<br/><em class="nz">+----+-----+----+----+---------------+<br/>|colA| colB|colC|colD|           colE|<br/>+----+-----+----+----+---------------+<br/>|   1| true|   a| 1.0|[100, 200, 300]|<br/>|   2| true|   b| 2.0|[100, 200, 300]|<br/>|   3|false|null| 3.0|[100, 200, 300]|<br/>|   4|false|   d|null|[100, 200, 300]|<br/>+----+-----+----+----+---------------+</em></span></pre></div><div class="ab cl ni nj hy nk" role="separator"><span class="nl bw bk nm nn no"/><span class="nl bw bk nm nn no"/><span class="nl bw bk nm nn"/></div><div class="in io ip iq ir"><h2 id="15d3" class="la lb iu bd lc ld le dn lf lg lh dp li lj lk ll lm ln lo lp lq lr ls lt lu lv bi translated">通过转换现有列来添加列</h2><p id="7d96" class="pw-post-body-paragraph lw lx iu ly b lz ma jv mb mc md jy me lj mf mg mh ln mi mj mk lr ml mm mn mo in bi translated">如果你想基于一个现有的列创建一个新的列，那么你应该在<code class="fe oa ob oc nq b">withColumn</code>方法中指定想要的操作。</p><p id="98f8" class="pw-post-body-paragraph lw lx iu ly b lz mp jv mb mc mq jy me lj mr mg mh ln ms mj mk lr mt mm mn mo in bi translated">例如，如果您想通过将现有列的值(比如说<code class="fe oa ob oc nq b">colD</code>)乘以一个常数(比如说<code class="fe oa ob oc nq b">2</code>)来创建一个新列，那么下面的方法就可以做到:</p><pre class="kk kl km kn gu np nq nr ns aw nt bi"><span id="7709" class="la lb iu nq b gz nu nv l nw nx">from pyspark.sql.functions import col</span><span id="e9f3" class="la lb iu nq b gz ny nv l nw nx"><strong class="nq iv">df = df.withColumn('colE', col('colD') * 2)</strong></span><span id="4326" class="la lb iu nq b gz ny nv l nw nx">df.show()<br/><em class="nz">+----+-----+----+----+----+<br/>|colA| colB|colC|colD|colE|<br/>+----+-----+----+----+----+<br/>|   1| true|   a| 1.0| 2.0|<br/>|   2| true|   b| 2.0| 4.0|<br/>|   3|false|null| 3.0| 6.0|<br/>|   4|false|   d|null|null|<br/>+----+-----+----+----+----+</em></span></pre></div><div class="ab cl ni nj hy nk" role="separator"><span class="nl bw bk nm nn no"/><span class="nl bw bk nm nn no"/><span class="nl bw bk nm nn"/></div><div class="in io ip iq ir"><h2 id="5965" class="la lb iu bd lc ld le dn lf lg lh dp li lj lk ll lm ln lo lp lq lr ls lt lu lv bi translated">使用联接添加新列</h2><p id="9450" class="pw-post-body-paragraph lw lx iu ly b lz ma jv mb mc md jy me lj mf mg mh ln mi mj mk lr ml mm mn mo in bi translated">或者，我们仍然可以创建一个新的数据帧，并将其连接回原来的数据帧。首先，需要创建一个新的数据帧，其中包含要添加的新列以及要在两个数据帧上连接的键</p><pre class="kk kl km kn gu np nq nr ns aw nt bi"><span id="bcb5" class="la lb iu nq b gz nu nv l nw nx">new_col = spark_session.createDataFrame(<br/>    [(1, 'hello'), (2, 'hi'), (3, 'hey'), (4, 'howdy')],<br/>    ('key', 'colE')<br/>)</span><span id="d86e" class="la lb iu nq b gz ny nv l nw nx">new_col.show()<br/><em class="nz">+---+-----+<br/>|key| colE|<br/>+---+-----+<br/>|  1|hello|<br/>|  2|   hi|<br/>|  3|  hey|<br/>|  4|howdy|<br/>+---+-----+</em></span></pre><p id="762c" class="pw-post-body-paragraph lw lx iu ly b lz mp jv mb mc mq jy me lj mr mg mh ln ms mj mk lr mt mm mn mo in bi translated">最后进行连接:</p><pre class="kk kl km kn gu np nq nr ns aw nt bi"><span id="a520" class="la lb iu nq b gz nu nv l nw nx">from pyspark.sql.functions import col</span><span id="3c8a" class="la lb iu nq b gz ny nv l nw nx"><strong class="nq iv">df = df \<br/>    .join(new_col, col('colA') == col('key'), 'leftouter') \<br/>    .drop('key')</strong></span><span id="913e" class="la lb iu nq b gz ny nv l nw nx">df.show()<br/><em class="nz">+----+-----+----+----+-----+<br/>|colA| colB|colC|colD| colE|<br/>+----+-----+----+----+-----+<br/>|   1| true|   a| 1.0|hello|<br/>|   2| true|   b| 2.0|   hi|<br/>|   3|false|null| 3.0|  hey|<br/>|   4|false|   d|null|howdy|<br/>+----+-----+----+----+-----+</em></span></pre></div><div class="ab cl ni nj hy nk" role="separator"><span class="nl bw bk nm nn no"/><span class="nl bw bk nm nn no"/><span class="nl bw bk nm nn"/></div><div class="in io ip iq ir"><h2 id="28a8" class="la lb iu bd lc ld le dn lf lg lh dp li lj lk ll lm ln lo lp lq lr ls lt lu lv bi translated">使用函数或UDF添加列</h2><p id="ce2e" class="pw-post-body-paragraph lw lx iu ly b lz ma jv mb mc md jy me lj mf mg mh ln mi mj mk lr ml mm mn mo in bi translated">另一种可能性是使用返回<code class="fe oa ob oc nq b">Column</code>的函数，并将该函数传递给<code class="fe oa ob oc nq b">withColumn</code>。例如，您可以使用内置的<code class="fe oa ob oc nq b"><a class="ae kz" href="https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.functions.rand.html" rel="noopener ugc nofollow" target="_blank">pyspark.sql.functions.rand</a></code>函数创建一个包含随机数的列，如下所示:</p><pre class="kk kl km kn gu np nq nr ns aw nt bi"><span id="5d5b" class="la lb iu nq b gz nu nv l nw nx">from pyspark.sql.functions import rand</span><span id="55e1" class="la lb iu nq b gz ny nv l nw nx"><strong class="nq iv">df = df.withColumn('colE', rand())</strong></span><span id="7db7" class="la lb iu nq b gz ny nv l nw nx">df.show()<br/><em class="nz">+----+-----+----+----+--------------------+<br/>|colA| colB|colC|colD|                colE|<br/>+----+-----+----+----+--------------------+<br/>|   1| true|   a| 1.0|0.026110187082684866|<br/>|   2| true|   b| 2.0|0.046264104329627576|<br/>|   3|false|null| 3.0|  0.7892572670252188|<br/>|   4|false|   d|null|  0.7963792998818318|<br/>+----+-----+----+----+--------------------+</em></span></pre></div><div class="ab cl ni nj hy nk" role="separator"><span class="nl bw bk nm nn no"/><span class="nl bw bk nm nn no"/><span class="nl bw bk nm nn"/></div><div class="in io ip iq ir"><h2 id="78f3" class="la lb iu bd lc ld le dn lf lg lh dp li lj lk ll lm ln lo lp lq lr ls lt lu lv bi translated">最后的想法</h2><p id="f311" class="pw-post-body-paragraph lw lx iu ly b lz ma jv mb mc md jy me lj mf mg mh ln mi mj mk lr ml mm mn mo in bi translated">在今天的简短指南中，我们讨论了如何向现有的PySpark数据帧中插入额外的列。</p></div><div class="ab cl ni nj hy nk" role="separator"><span class="nl bw bk nm nn no"/><span class="nl bw bk nm nn no"/><span class="nl bw bk nm nn"/></div><div class="in io ip iq ir"><p id="1b77" class="pw-post-body-paragraph lw lx iu ly b lz mp jv mb mc mq jy me lj mr mg mh ln ms mj mk lr mt mm mn mo in bi translated"><a class="ae kz" href="https://gmyrianthous.medium.com/membership" rel="noopener"> <strong class="ly iv">成为会员</strong> </a> <strong class="ly iv">阅读媒体上的每一个故事。你的会员费直接支持我和你看的其他作家。你也可以在媒体上看到所有的故事。</strong></p></div><div class="ab cl ni nj hy nk" role="separator"><span class="nl bw bk nm nn no"/><span class="nl bw bk nm nn no"/><span class="nl bw bk nm nn"/></div><div class="in io ip iq ir"><p id="676e" class="pw-post-body-paragraph lw lx iu ly b lz mp jv mb mc mq jy me lj mr mg mh ln ms mj mk lr mt mm mn mo in bi translated"><strong class="ly iv">你可能也会喜欢</strong></p><div class="od oe gq gs of og"><a rel="noopener follow" target="_blank" href="/how-to-efficiently-convert-a-pyspark-dataframe-to-pandas-8bda2c3875c3"><div class="oh ab fp"><div class="oi ab oj cl cj ok"><h2 class="bd iv gz z fq ol fs ft om fv fx it bi translated">加快PySpark和Pandas数据帧之间的转换</h2><div class="on l"><h3 class="bd b gz z fq ol fs ft om fv fx dk translated">将大火花数据帧转换为熊猫时节省时间</h3></div><div class="oo l"><p class="bd b dl z fq ol fs ft om fv fx dk translated">towardsdatascience.com</p></div></div><div class="op l"><div class="oq l or os ot op ou kt og"/></div></div></a></div></div><div class="ab cl ni nj hy nk" role="separator"><span class="nl bw bk nm nn no"/><span class="nl bw bk nm nn no"/><span class="nl bw bk nm nn"/></div><div class="in io ip iq ir"><div class="kk kl km kn gu og"><a rel="noopener follow" target="_blank" href="/dynamic-typing-in-python-307f7c22b24e"><div class="oh ab fp"><div class="oi ab oj cl cj ok"><h2 class="bd iv gz z fq ol fs ft om fv fx it bi translated">Python中的动态类型</h2><div class="on l"><h3 class="bd b gz z fq ol fs ft om fv fx dk translated">探索Python中对象引用的工作方式</h3></div><div class="oo l"><p class="bd b dl z fq ol fs ft om fv fx dk translated">towardsdatascience.com</p></div></div><div class="op l"><div class="ov l or os ot op ou kt og"/></div></div></a></div></div><div class="ab cl ni nj hy nk" role="separator"><span class="nl bw bk nm nn no"/><span class="nl bw bk nm nn no"/><span class="nl bw bk nm nn"/></div><div class="in io ip iq ir"><div class="kk kl km kn gu og"><a rel="noopener follow" target="_blank" href="/mastering-indexing-and-slicing-in-python-443e23457125"><div class="oh ab fp"><div class="oi ab oj cl cj ok"><h2 class="bd iv gz z fq ol fs ft om fv fx it bi translated">掌握Python中的索引和切片</h2><div class="on l"><h3 class="bd b gz z fq ol fs ft om fv fx dk translated">深入研究有序集合的索引和切片</h3></div><div class="oo l"><p class="bd b dl z fq ol fs ft om fv fx dk translated">towardsdatascience.com</p></div></div><div class="op l"><div class="ow l or os ot op ou kt og"/></div></div></a></div></div></div>    
</body>
</html>