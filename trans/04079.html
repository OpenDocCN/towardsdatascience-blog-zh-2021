<html>
<head>
<title>Putting GPT-Neo (and Others) into Production using ONNX</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用ONNX将GPT-尼奥(及其他)投入生产</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/putting-gpt-neo-and-others-into-production-using-onnx-1204541e8ff2?source=collection_archive---------19-----------------------#2021-04-06">https://towardsdatascience.com/putting-gpt-neo-and-others-into-production-using-onnx-1204541e8ff2?source=collection_archive---------19-----------------------#2021-04-06</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="820e" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">了解如何使用ONNX将torch和tensorflow模型投入生产。将推理速度提高2.5倍。</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/05b323e59d328436f608b0a11b493453.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*bXjn2CeWhw4_OwbR"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">Marc-Olivier Jodoin 在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="760b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">注<em class="lv">:全笔记本可用</em> </strong> <a class="ae ky" href="https://github.com/oborchers/Medium_Repo/blob/master/Putting%20GPT-Neo%20into%20Production%20using%C2%A0ONNX/ONNX-Export.ipynb" rel="noopener ugc nofollow" target="_blank"> <strong class="lb iu"> <em class="lv">此处</em> </strong> </a> <strong class="lb iu"> <em class="lv">。</em> </strong></p><p id="31de" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">更新<em class="lv">:如果你在很远的将来(截至2021年4月)读到这篇文章，那么这篇文章的底层代码可能会因为底层库的变化而被弃用。</em> </strong></p><h2 id="8092" class="lw lx it bd ly lz ma dn mb mc md dp me li mf mg mh lm mi mj mk lq ml mm mn mo bi translated">介绍</h2><p id="9f25" class="pw-post-body-paragraph kz la it lb b lc mp ju le lf mq jx lh li mr lk ll lm ms lo lp lq mt ls lt lu im bi translated">使用变压器已经成为最新NLP应用的新标准。想想BERT或GPT3，我们可以有把握地得出结论，几乎所有的NLP应用都从类似变压器的模型中受益匪浅。然而，这些模型通常部署起来非常昂贵，并且需要特殊的硬件来运行。在本文中，您将了解什么是ONNX，以及如何将torch和tensorflow transformers模型移植到ONNX。您还将学习如何定制torch实现，以及如何在以后导出它。具体来说，我们将了解:</p><ol class=""><li id="ebaa" class="mu mv it lb b lc ld lf lg li mw lm mx lq my lu mz na nb nc bi translated">简单导出<a class="ae ky" href="https://huggingface.co/sentence-transformers/bert-base-nli-stsb-mean-tokens" rel="noopener ugc nofollow" target="_blank">Bert-base-nli-stsb-mean-tokens</a></li><li id="d4f3" class="mu mv it lb b lc nd lf ne li nf lm ng lq nh lu mz na nb nc bi translated">自定义导出<a class="ae ky" href="https://huggingface.co/sentence-transformers/bert-base-nli-stsb-mean-tokens" rel="noopener ugc nofollow" target="_blank">Bert-base-nli-stsb-mean-tokens</a></li><li id="b7c9" class="mu mv it lb b lc nd lf ne li nf lm ng lq nh lu mz na nb nc bi translated">用ORT-CustomOps导出<a class="ae ky" href="https://tfhub.dev/google/universal-sentence-encoder-large/5" rel="noopener ugc nofollow" target="_blank">通用语句编码器</a></li><li id="1fb2" class="mu mv it lb b lc nd lf ne li nf lm ng lq nh lu mz na nb nc bi translated">试图用1.3B参数导出<a class="ae ky" href="https://huggingface.co/EleutherAI/gpt-neo-1.3B" rel="noopener ugc nofollow" target="_blank">GPT-尼奥</a></li></ol><h2 id="65ae" class="lw lx it bd ly lz ma dn mb mc md dp me li mf mg mh lm mi mj mk lq ml mm mn mo bi translated">ONNX是什么？</h2><p id="c737" class="pw-post-body-paragraph kz la it lb b lc mp ju le lf mq jx lh li mr lk ll lm ms lo lp lq mt ls lt lu im bi translated">当我很久以前开始使用变形金刚时，我的第一次体验是使用<a class="ae ky" href="https://github.com/hanxiao/bert-as-service" rel="noopener ugc nofollow" target="_blank">伯特即服务</a>。虽然BaaS仍然是一个不错的库，但现在在GPU上部署自己的模型并为其提供一个小的REST API已经相当简单了。通常，这将通过一个或多个框架来完成，如torch或tensorflow。但是这在实践中有严重的局限性。</p><p id="7c69" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这就是ONNX发挥作用的地方。笔<strong class="lb iu">笔</strong>笔【欧洲】笔<strong class="lb iu">笔</strong>网络<strong class="lb iu">笔</strong>交换的目标是提供不同参与者之间的互操作性。互操作性意味着:</p><ol class=""><li id="05b3" class="mu mv it lb b lc ld lf lg li mw lm mx lq my lu mz na nb nc bi translated">跨框架共享模型(例如，torch到tensorflow)</li><li id="7091" class="mu mv it lb b lc nd lf ne li nf lm ng lq nh lu mz na nb nc bi translated">跨各种硬件(例如，CPU、GPU、FPGA等)共享模型</li></ol><p id="3f14" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这对社区有好处。还有你的血压。尝试在同一个GPU上使用两个不同的框架部署模型。这是一种痛苦。<br/>(你好黑暗我的老朋友……)</p><p id="64f2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在幕后，ONNX提供了一种定制的文件格式，这是一种由节点组成的计算图，节点本身由基本操作符组成。ONNX持有大量深度学习和机器学习相关的核心op，还提供使用定制op的能力。引用他们的<a class="ae ky" href="https://onnx.ai/about.html" rel="noopener ugc nofollow" target="_blank">主页</a>:</p><blockquote class="ni nj nk"><p id="790e" class="kz la lv lb b lc ld ju le lf lg jx lh nl lj lk ll nm ln lo lp nn lr ls lt lu im bi translated">ONNX提供了可扩展计算图模型的定义，以及内置操作符和标准数据类型的定义。<br/>每个计算数据流图被构造为形成非循环图的节点列表。节点有一个或多个输入和一个或多个输出。每个节点都是对一个操作员的调用。该图还有元数据来帮助记录其用途、作者等。</p></blockquote><p id="4040" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果你想了解ONNX的更多信息，微软和NVIDIA有一个非常好的演示，你可以在这里找到<a class="ae ky" href="https://developer.nvidia.com/gtc/2019/video/s9979" rel="noopener ugc nofollow" target="_blank"/>。请记住，这个演示文稿来自2019年，两年内会有很多变化。</p><p id="36c8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在开始使用ONNX之前，有三个主要组件与我们的目的相关:</p><ul class=""><li id="a937" class="mu mv it lb b lc ld lf lg li mw lm mx lq my lu no na nb nc bi translated"><a class="ae ky" href="https://github.com/onnx/onnx" rel="noopener ugc nofollow" target="_blank"> ONNX </a>:提供图形格式和操作定义</li><li id="8264" class="mu mv it lb b lc nd lf ne li nf lm ng lq nh lu no na nb nc bi translated"><a class="ae ky" href="https://github.com/microsoft/onnxruntime" rel="noopener ugc nofollow" target="_blank"> ONNX运行时</a>:提供可用于在您的硬件上部署模型进行推理的运行时。它包含了<em class="lv"> ExecutionProviders </em>，使我们能够使用各种方法加速操作，例如CPU、Cuda或TensorRT。</li><li id="1e83" class="mu mv it lb b lc nd lf ne li nf lm ng lq nh lu no na nb nc bi translated">ONNX运行时工具:提供对已经转换的ONNX变压器模型进行额外优化的功能。我们不会在这里使用它，但是请记住它是存在的！</li></ul><h2 id="153c" class="lw lx it bd ly lz ma dn mb mc md dp me li mf mg mh lm mi mj mk lq ml mm mn mo bi translated">预赛</h2><p id="af54" class="pw-post-body-paragraph kz la it lb b lc mp ju le lf mq jx lh li mr lk ll lm ms lo lp lq mt ls lt lu im bi translated">要继续下去，您将需要相当多的库。我建议在开始之前构建自己的Docker映像，它支持最新的NVIDIA驱动程序，甚至可能支持TensorRT。</p><p id="d25a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">从NVIDIAs <code class="fe np nq nr ns b"><a class="ae ky" href="https://docs.nvidia.com/deeplearning/tensorrt/container-release-notes/rel_20-12.html#rel_20-12" rel="noopener ugc nofollow" target="_blank">nvcr.io/nvidia/tensorrt:20.12-py3</a></code> image开始是个好主意。根据您想走多远，您甚至可能想从头开始构建ONNXRuntime(推荐)。这是我的<a class="ae ky" href="https://github.com/oborchers/Medium_Repo/blob/master/onnxruntime-issues/Dockerfile" rel="noopener ugc nofollow" target="_blank"> Dockerfile </a>你可以用它工作。</p><p id="a7e3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这个脚本需要适应您的配置，可能不适合您。它已经在装有V100的容器上进行了测试。这个构建让您可以从ONNX运行时访问CPU、CUDA、TensorRT执行提供程序。我们也在使用最新开发版本的<code class="fe np nq nr ns b">transformers</code>库，即<code class="fe np nq nr ns b">4.5.0.dev0</code>来访问get尼奥。</p><h2 id="b572" class="lw lx it bd ly lz ma dn mb mc md dp me li mf mg mh lm mi mj mk lq ml mm mn mo bi translated">1.简单出口</h2><p id="6f49" class="pw-post-body-paragraph kz la it lb b lc mp ju le lf mq jx lh li mr lk ll lm ms lo lp lq mt ls lt lu im bi translated"><em class="lv">注:完整笔记本可用</em> <a class="ae ky" href="https://github.com/oborchers/Medium_Repo/blob/master/Putting%20GPT-Neo%20into%20Production%20using%C2%A0ONNX/ONNX-Export.ipynb" rel="noopener ugc nofollow" target="_blank"> <strong class="lb iu"> <em class="lv">此处</em> </strong> </a> <em class="lv">。<br/> </em>我们要看的第一个模型是来自句子变形金刚<a class="ae ky" href="https://github.com/UKPLab/sentence-transformers" rel="noopener ugc nofollow" target="_blank">库</a>的<code class="fe np nq nr ns b">bert-base-nli-stsb-mean-tokens</code>模型。该型号也可在<a class="ae ky" href="https://huggingface.co/sentence-transformers/bert-base-nli-stsb-mean-tokens" rel="noopener ugc nofollow" target="_blank">轮毂</a>上获得。它本质上是一个BERT模型，被训练来产生良好的句子嵌入，在相似性搜索中也表现良好。为了转换模型，让我们使用<code class="fe np nq nr ns b">convert_graph_to_onnx</code>中的变形金刚库中已经可用的方法(参见<a class="ae ky" href="https://github.com/huggingface/transformers/blob/master/src/transformers/convert_graph_to_onnx.py" rel="noopener ugc nofollow" target="_blank">这里的</a>)。用于导出的代码如下所示:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nt nu l"/></div></figure><p id="f40d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">接下来，我们只需要加载模型，创建一个推理会话。此外，我们传递一些会话选项、首选的执行提供程序，并加载导出的模型:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nt nu l"/></div></figure><p id="5909" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">看起来不错！模型正在加载，一切都很好。如果比较速度的话，基本的<code class="fe np nq nr ns b">nlp</code>流水线形式<code class="fe np nq nr ns b">transformers</code>运行在编码<code class="fe np nq nr ns b">span="Hello my friends!"</code>的<em class="lv"> 10ms </em>左右。这模拟了在线推理，这可能是最常见的用例。另一边，ONNX型号运行在<em class="lv"> 2.8ms </em>。这比V100增加了<strong class="lb iu">2.5倍</strong>，只需几行代码，无需进一步优化。请记住，对于批量编码，这些值可能会有很大的不同。</p><p id="d310" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">理论上，您现在可以将模型从<a class="ae ky" href="https://github.com/microsoft/onnxruntime/tree/master/onnxruntime/python/tools/transformers" rel="noopener ugc nofollow" target="_blank"> ONNX运行时工具</a>放到前面提到的优化器中。但是<strong class="lb iu">注意</strong>:如果您使用<code class="fe np nq nr ns b">use_gpu=True</code>运行优化器，那么请确保您安装了ONNX运行时<strong class="lb iu">而没有tensort，</strong>因为如果启用了tensort执行提供程序，导出<a class="ae ky" href="https://github.com/microsoft/onnxruntime/issues/6080" rel="noopener ugc nofollow" target="_blank">将不会工作</a>。</p><p id="67fe" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果仔细观察，您会发现打印语句中生成的形状是错误的。返回的是两个形状为(1，6，768)和(1，768)的数组的列表。理论上，我们期望返回的形状为(1，768 ),因为我们使用的是句子编码器。</p><p id="8fe9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这种行为是由于句子转换器库需要在令牌嵌入之上的管道中添加一个额外的平均池层。说实话，如果我们想要一个统一的部署框架，并且不要在事后摆弄numpy或torch，那么事后添加层并不是一个优雅的解决方案，这违背了我们的目的。</p><p id="1c8e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在我们检查自定义导出之前，让我们先来看看基准:</p><ul class=""><li id="6afa" class="mu mv it lb b lc ld lf lg li mw lm mx lq my lu no na nb nc bi translated">SentenceTransformer CUDA:每个循环12.3毫秒1.4毫秒(平均池化)</li><li id="3f37" class="mu mv it lb b lc nd lf ne li nf lm ng lq nh lu no na nb nc bi translated">ONNX CUDA (V100):每循环2.21毫秒77秒</li><li id="d2dc" class="mu mv it lb b lc nd lf ne li nf lm ng lq nh lu no na nb nc bi translated">ONNX tensort(V100，<a class="ae ky" href="https://github.com/microsoft/onnxruntime-openenclave/blob/openenclave-public/docs/execution_providers/TensorRT-ExecutionProvider.md" rel="noopener ugc nofollow" target="_blank"> ExecutionProvider </a>):每个循环3.86 ms 181 s</li></ul><p id="f65c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">坦率地说，我们在这里看到的结果很奇怪。我已经在这里开了一个问题<a class="ae ky" href="https://github.com/microsoft/onnxruntime/issues/7230" rel="noopener ugc nofollow" target="_blank">，因为我无法从TensorRT得到任何加速。</a></p><h2 id="3b7a" class="lw lx it bd ly lz ma dn mb mc md dp me li mf mg mh lm mi mj mk lq ml mm mn mo bi translated">2.定制出口</h2><p id="ceac" class="pw-post-body-paragraph kz la it lb b lc mp ju le lf mq jx lh li mr lk ll lm ms lo lp lq mt ls lt lu im bi translated">添加自定义层需要我们理解所使用的<code class="fe np nq nr ns b">convert</code>函数内部发生了什么。剧透:这很简单(对pytorch来说)。<a class="ae ky" href="https://github.com/huggingface/transformers/blob/335c0ca35c159f88d73198bdac928e61a4d480c7/src/transformers/convert_graph_to_onnx.py#L277" rel="noopener ugc nofollow" target="_blank"> convert </a>函数调用两个方便的函数，即<code class="fe np nq nr ns b">transformers.pipeline</code>对象上的<code class="fe np nq nr ns b">infer_shapes</code>和<code class="fe np nq nr ns b">ensure_valid_input</code>。之后，所有推断出的形状加上产生的<code class="fe np nq nr ns b">torch.nn.Module</code>对象被传递给<code class="fe np nq nr ns b">torch.onnx.export</code>函数。文档提供了一个关于如何正确使用导出功能的很好的例子。对于理解导出函数来说，最重要的是以下参数:</p><ol class=""><li id="57ef" class="mu mv it lb b lc ld lf lg li mw lm mx lq my lu mz na nb nc bi translated">input_names:底层torch模型的<code class="fe np nq nr ns b">forward</code>函数的参数。必须按正确的顺序。</li><li id="b2d6" class="mu mv it lb b lc nd lf ne li nf lm ng lq nh lu mz na nb nc bi translated">输出名称:输出图层的名称。</li><li id="aad7" class="mu mv it lb b lc nd lf ne li nf lm ng lq nh lu mz na nb nc bi translated">dynamic_axes:定义哪些是动态的，以及它们以何种方式是动态的(将来会更有意义)。</li><li id="eef1" class="mu mv it lb b lc nd lf ne li nf lm ng lq nh lu mz na nb nc bi translated">参数:将通过模型的一组示例输入。</li></ol><p id="a08b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这些参数是从上面提到的便利函数中导出的。所以让我们快速地把它们包起来，这样我们就可以随心所欲地摆弄它们了:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nt nu l"/></div></figure><p id="b58f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">将<code class="fe np nq nr ns b">print_transformers_shape_inference</code>应用于感兴趣的BERT模型，我们得到以下形状:</p><pre class="kj kk kl km gt nv ns nw nx aw ny bi"><span id="d441" class="lw lx it ns b gy nz oa l ob oc">Inferred shapes for sentence-transformers/bert-base-nli-stsb-mean-tokens<br/>Input names: ['input_ids', 'token_type_ids', 'attention_mask']<br/>Output names: ['output_0', 'output_1']<br/>Dynamic Axes:<br/>{<br/>    "attention_mask": {<br/>        "0": "batch",<br/>        "1": "sequence"<br/>    },<br/>    "input_ids": {<br/>        "0": "batch",<br/>        "1": "sequence"<br/>    },<br/>    "output_0": {<br/>        "0": "batch",<br/>        "1": "sequence"<br/>    },<br/>    "output_1": {<br/>        "0": "batch"<br/>    },<br/>    "token_type_ids": {<br/>        "0": "batch",<br/>        "1": "sequence"<br/>    }<br/>}<br/>Tokens:{<br/>    'input_ids': tensor([[ 101, 2023, 2003, 1037, 7099, 6434,  102]]), <br/>    'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0]]),  <br/>    'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1]])<br/>}<br/>Ordered input names: ['input_ids', 'attention_mask', 'token_type_ids']<br/>Arguments: (<br/>    tensor([[ 101, 2023, 2003, 1037, 7099, 6434,  102]]), <br/>    tensor([[1, 1, 1, 1, 1, 1, 1]]), <br/>    tensor([[0, 0, 0, 0, 0, 0, 0]])<br/>)</span></pre><p id="553d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">通过一些解释，这完全说得通。<code class="fe np nq nr ns b">output_0</code>引用<code class="fe np nq nr ns b">pooler_output</code>，<code class="fe np nq nr ns b">output_1</code>引用返回的<code class="fe np nq nr ns b">BaseModelOutputWithPoolingAndCrossAttention</code>的<code class="fe np nq nr ns b">last_hidden_state</code>。<code class="fe np nq nr ns b">input_ids</code>、<code class="fe np nq nr ns b">token_type_ids</code>和<code class="fe np nq nr ns b">attention_mask</code>都是动态的，并且是记号赋予器函数的输出。</p><p id="79a4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们继续建立一个简单的torch模型，它继承了BERT模型。我们唯一增加的是令牌嵌入的加权求和以及防止零除误差的箝位。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nt nu l"/></div></figure><p id="2291" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">最后检查一下，模型产生的输出与原始模型大致相同，我们可以继续了。</p><p id="f04f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在导出我们的新模型之前，唯一要做的事情是修改我们之前导出的<code class="fe np nq nr ns b">dynamic_axes</code>和<code class="fe np nq nr ns b">output_names</code>。这是因为我们现在有了一个不同的输出层，它也是动态的(根据批处理大小)。我们可以使用身份层的名称来更好地识别输出层。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nt nu l"/></div></figure><p id="1149" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">太好了！现在，我们已经准备好了新的ONNX模型，可以用它进行推理了。输出形状现在是(1，768)预期的形状，几乎等于原始模型，绝对公差为1e-6。此外，新模型运行在<em class="lv">2.4毫秒</em>，所以我们没有失去任何速度，并获得了一个适当的端到端模型。</p><p id="ddee" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">很明显，这个过程可以根据你的喜好进行定制。也可以在此基础上训练自己的分类器，并以同样的方式将其添加到编码器中。</p><p id="47cc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们已经创建了前两个ONNX模型。干得好！让我们做点不同的事情。</p><h2 id="f27f" class="lw lx it bd ly lz ma dn mb mc md dp me li mf mg mh lm mi mj mk lq ml mm mn mo bi translated">3.使用ort自定义操作进行导出</h2><p id="ae2a" class="pw-post-body-paragraph kz la it lb b lc mp ju le lf mq jx lh li mr lk ll lm ms lo lp lq mt ls lt lu im bi translated">这一部分特别关注<a class="ae ky" href="https://tfhub.dev/google/universal-sentence-encoder-large/5" rel="noopener ugc nofollow" target="_blank">通用句子编码器5 </a>，这是一个我长期以来一直在使用的模型，我非常喜欢它。它速度快，性能好，而且相对较小。谢天谢地，有了<code class="fe np nq nr ns b">tf2onnx</code>T10库。<code class="fe np nq nr ns b">tf2onnx</code>是一个从tensorflow模型生成ONNX文件的导出工具。由于使用tensorflow总是令人愉快的，我们不能直接导出模型，因为模型定义中包含了标记器。不幸的是，核心ONNX平台还不支持这些字符串操作。</p><p id="7296" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">幸运的是，ONNXRuntime CustomOps <a class="ae ky" href="https://github.com/microsoft/ort-customops" rel="noopener ugc nofollow" target="_blank">库</a>来帮忙了。该库也由ONNX团队维护，并为扩展ONNX基本功能的额外自定义操作提供支持。您将需要安装CMake &gt; 3.17.0，以便使用<code class="fe np nq nr ns b">pip install git+https://github.com/microsoft/ort-customops.git</code>编译和安装该文件。</p><p id="2533" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">安装完CustomOps库后，我们<a class="ae ky" href="https://tfhub.dev/google/universal-sentence-encoder-large/5" rel="noopener ugc nofollow" target="_blank">将</a>USE下载到某个文件夹，并为<code class="fe np nq nr ns b">tf2onnx</code>库提供一个输出路径。除此之外，我们可以直接导出模型:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nt nu l"/></div></figure><p id="d963" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><code class="fe np nq nr ns b">tf2onnx</code>库提供了一些其他的强大功能。例如，<code class="fe np nq nr ns b">--signature_def</code>参数允许您部分导出具有多个签名的模型，例如<a class="ae ky" href="https://tfhub.dev/google/universal-sentence-encoder-qa/3" rel="noopener ugc nofollow" target="_blank">使用v3 </a>进行QA。点击查看参数<a class="ae ky" href="https://github.com/onnx/tensorflow-onnx/blob/master/tf2onnx/convert.py" rel="noopener ugc nofollow" target="_blank">。</a></p><p id="7490" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">由于底层图形和额外Ops的不同，现在运行使用的推理略有不同。我们必须将自定义Ops库路径传递给ONNX SessionOptions对象。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nt nu l"/></div></figure><p id="c85d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们剧目中的另一个模特:-)</p><h2 id="6425" class="lw lx it bd ly lz ma dn mb mc md dp me li mf mg mh lm mi mj mk lq ml mm mn mo bi translated">4.试图导出<a class="ae ky" href="https://huggingface.co/EleutherAI/gpt-neo-1.3B" rel="noopener ugc nofollow" target="_blank">GPT-尼奥</a></h2><p id="ac88" class="pw-post-body-paragraph kz la it lb b lc mp ju le lf mq jx lh li mr lk ll lm ms lo lp lq mt ls lt lu im bi translated">现在我们开始真正有趣的东西。GPT-尼奥刚刚在变形金刚图书馆发布。它本质上是OpenAI的GPT3架构的开源变体。<a class="ae ky" href="https://github.com/EleutherAI/gpt-neo" rel="noopener ugc nofollow" target="_blank">型号</a>有两种架构:1.3B和2.7B，代表内部参数的数量。这些型号可通过<a class="ae ky" href="https://huggingface.co/EleutherAI" rel="noopener ugc nofollow" target="_blank">型号中枢</a>获得。注意:从今天起你需要<code class="fe np nq nr ns b">transformers-4.5.0.dev0</code>，因为GPT-尼奥不包括在当前的Pypi包中。</p><p id="1a2f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们从复制本教程第2步中的简单导出开始。这可能有点奇怪，可能不适合您的环境，因为这一步丢弃了所有输出，除了<code class="fe np nq nr ns b">logits</code>。但是我们可以看到一些真实世界的数字，在真实硬件上的推理速度。截至发布日期(2021年4月5日),变形金刚库提供的完整形状推断似乎没有达到预期效果，因此我们需要稍微调整一下。我们只在它周围包装一个自定义层，返回逻辑。加载模型需要3分钟，因为我们必须使用外部数据格式来补偿大的模型大小。再次运行之前的相同推理:</p><ul class=""><li id="a542" class="mu mv it lb b lc ld lf lg li mw lm mx lq my lu no na nb nc bi translated">变形金刚CUDA:每循环114毫秒20毫秒</li><li id="8ec6" class="mu mv it lb b lc nd lf ne li nf lm ng lq nh lu no na nb nc bi translated">ONNX CUDA (V100):每循环314毫秒4.15毫秒</li><li id="4719" class="mu mv it lb b lc nd lf ne li nf lm ng lq nh lu no na nb nc bi translated">ONNX TensorRT (V100，<a class="ae ky" href="https://github.com/microsoft/onnxruntime-openenclave/blob/openenclave-public/docs/execution_providers/TensorRT-ExecutionProvider.md" rel="noopener ugc nofollow" target="_blank"> ExecutionProvider </a>):初始化时出现异常:/workspace/ONNX runtime/ONNX runtime/core/providers/tensor rt/tensor rt _ execution _ provider . cc:777 subgraph collection _ t ONNX runtime::TensorrtExecutionProvider::getsupported list(subgraph collection _ t，int，int，const ONNX runtime::graph viewer&amp;，bool *)const[ONNXRuntimeError]:1:FAIL:tensor rt input:67</li></ul><p id="8838" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">好吧，所以，这是相当不满意的。我不想让您失望，但是我想在我们能够正确地导出模型之前，模型中还有更多的优化工作要做。对我来说，不清楚是什么导致了这个问题。但是，如果我们查看日志，我们可以看到正在发生的事情:</p><blockquote class="ni nj nk"><p id="9089" class="kz la lv lb b lc ld ju le lf lg jx lh nl lj lk ll nm ln lo lp nn lr ls lt lu im bi translated">在Op类型的注册表中找不到CUDA内核:Pad节点名:Pad_4368 <br/>在Op类型的注册表中找不到CUDA内核:Pad节点名:Pad_3801 <br/>在Op类型的注册表中找不到CUDA内核:LessOrEqual节点名:LessOrEqual_7094 <br/>强制回退到节点的CPU执行:Gather_5 <br/>强制回退到节点的CPU执行:Unsqueeze_17 <br/>强制回退到节点的CPU执行:Slice_37 <br/>强制</p></blockquote><p id="6115" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这些信息成千上万。我们总共有10609条这样的消息:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi od"><img src="../Images/cc2ddcef79993aac49ba773c5f3b28d8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*l6Vko1qCYues-xtHmltuoA.png"/></div></div></figure><p id="cbc8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这里的关键是:导出到ONNX是一件好事。如果你的模型不使用那么多目前不支持的操作，那么它们中的许多运行在一个CPU上。虽然总的来说这不容易避免，但是优化一个模型从它的第一行代码开始。从一开始就记住你想如何优化它。与失踪的Ops生命相关的问题<a class="ae ky" href="https://github.com/microsoft/onnxruntime/issues/7238" rel="noopener ugc nofollow" target="_blank">在这里</a>。</p><h2 id="b0ce" class="lw lx it bd ly lz ma dn mb mc md dp me li mf mg mh lm mi mj mk lq ml mm mn mo bi translated">结论</h2><p id="1108" class="pw-post-body-paragraph kz la it lb b lc mp ju le lf mq jx lh li mr lk ll lm ms lo lp lq mt ls lt lu im bi translated">在本文中，我们深入探讨了ONNX以及如何从pytorch和tensorflow中导出模型。您现在可以直接从pytorch定制和导出模型。您还可以将tensorflow模型从带有自定义ops的检查点导出到ONNX。此外，您还学会了沿途寻找特殊案例。</p><h2 id="96c4" class="lw lx it bd ly lz ma dn mb mc md dp me li mf mg mh lm mi mj mk lq ml mm mn mo bi translated">附加说明</h2><p id="778b" class="pw-post-body-paragraph kz la it lb b lc mp ju le lf mq jx lh li mr lk ll lm ms lo lp lq mt ls lt lu im bi translated">本帖笔记本<a class="ae ky" href="https://github.com/oborchers/Medium_Repo/blob/master/Putting%20GPT-Neo%20into%20Production%20using%C2%A0ONNX/ONNX-Export.ipynb" rel="noopener ugc nofollow" target="_blank"> <strong class="lb iu"> <em class="lv">此处</em> </strong> </a>。</p><h2 id="464a" class="lw lx it bd ly lz ma dn mb mc md dp me li mf mg mh lm mi mj mk lq ml mm mn mo bi translated">文学</h2><ol class=""><li id="5ca4" class="mu mv it lb b lc mp lf mq li oe lm of lq og lu mz na nb nc bi translated">ONNX主页:<a class="ae ky" href="https://onnx.ai/about.html" rel="noopener ugc nofollow" target="_blank">https://onnx.ai/about.html</a></li></ol></div></div>    
</body>
</html>