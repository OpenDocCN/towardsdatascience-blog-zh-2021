<html>
<head>
<title>Faster Spark Queries with the Best of Both Worlds: Python and Scala</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">更快的Spark查询，两者兼得:Python和Scala</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/faster-spark-queries-with-the-best-of-both-worlds-python-and-scala-7cd0d49b7561?source=collection_archive---------34-----------------------#2021-06-09">https://towardsdatascience.com/faster-spark-queries-with-the-best-of-both-worlds-python-and-scala-7cd0d49b7561?source=collection_archive---------34-----------------------#2021-06-09</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="0db1" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">当Spark SQL和Python UDFs太慢时，如何使用Scala和PySpark实现高级Spark用例</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/4889faac02c185d3a4e5b1232e08904f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OCWv36BVOteWijiCbKCN4Q.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">由<a class="ae ky" href="https://unsplash.com/photos/aQYgUYwnCsM" rel="noopener ugc nofollow" target="_blank">Unsplash</a>【CC0】拍摄的图像</p></figure><p id="3933" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi lv translated"><span class="l lw lx ly bm lz ma mb mc md di"> S </span> park是大数据时代的黄金标准和主力。几乎所有数据科学家和数据工程师都依赖Spark来处理日常数据工作负载。但是这两个用户群之间有一个典型的分界线:</p><ul class=""><li id="114a" class="me mf it lb b lc ld lf lg li mg lm mh lq mi lu mj mk ml mm bi translated">许多数据工程师使用Scala和类型化数据集API编写Spark作业，而</li><li id="0f06" class="me mf it lb b lc mn lf mo li mp lm mq lq mr lu mj mk ml mm bi translated">大多数数据科学家使用PySpark和无类型Spark数据帧。</li></ul><p id="3b7e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">Python是迄今为止数据社区中最受欢迎的语言，Spark自早期以来取得了长足的进步，统一数据集API、Arrow、Spark SQL和矢量化Pandas UDFs使PySpark用户在使用Spark时成为一等公民。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ms"><img src="../Images/8d0b5d1d808e6ec9e89212e5d508e40a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bxIDGOFSMZFK6fBlmMouPQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">[OC]</p></figure><p id="898e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">您可能正在阅读这篇博文，因为您有一些高级用例，其中实现是Scala，类型化数据集API将是有益的。</p><ul class=""><li id="6ead" class="me mf it lb b lc ld lf lg li mg lm mh lq mi lu mj mk ml mm bi translated">共享代码库，例如，数据科学家想要重用来自数据工程管道的一些复杂的业务逻辑</li><li id="a139" class="me mf it lb b lc mn lf mo li mp lm mq lq mr lu mj mk ml mm bi translated">您需要实现复杂的业务逻辑，并希望使用Scala强大的集合和方法</li><li id="4a72" class="me mf it lb b lc mn lf mo li mp lm mq lq mr lu mj mk ml mm bi translated">您希望使用复杂的自定义类型，因为您的数据不太适合表格数据模型</li><li id="b4a3" class="me mf it lb b lc mn lf mo li mp lm mq lq mr lu mj mk ml mm bi translated">你必须处理组合问题、嵌套循环和大数组，你的Spark工人会因为OOM错误而死</li></ul><p id="99f6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这只是我多年来在使用Scala和类型化数据集API时遇到的几个例子，至少是在PySpark管道的一部分中。好消息是，您不必用Scala培训所有的数据科学家，也不必重写他们所有的PySpark工作流。PySpark只是一个API，所有的PySpark工作流最终都在JVM上的Spark中运行。这使得我们可以在同一个Spark管道中轻松混合和匹配Python和Scala。</p><p id="f496" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">下面的例子演示了如何使用一些业务逻辑为每个输入行创建大量的输出行。当然，您可以在PySpark中解决这个问题:</p><pre class="kj kk kl km gt mt mu mv mw aw mx bi"><span id="f708" class="my mz it mu b gy na nb l nc nd">def getBigList() -&gt; List[int]:<br/>  return <!-- -->[random.randint(1, 999999) for x in range(10000)]</span><span id="6f76" class="my mz it mu b gy ne nb l nc nd">bigListUdf = udf(<br/>  getBigList, <br/>  ArrayType(IntegerType())<br/>).asNondeterministic()</span><span id="b49e" class="my mz it mu b gy ne nb l nc nd">spark.createDataFrame([<br/>  Row(cid='cid1'),<br/>  Row(cid='cid2'),<br/>  Row(cid='cid3')<br/>]).select(<br/>  'cid', explode(bigListUdf()).alias('sku')<br/>).join(<br/>  productDf, ['sku']<br/>)</span></pre><p id="d22e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">但是传递一个大的列表并将其分解成大量的新行需要您的节点为这一阶段的这一步留有足够的内存余量。这是一个简单的例子，但是您甚至可能再次过滤掉许多新行，因为它们与您的整个管道不相关。在您的真实世界用例中，您甚至可能有列表列表等。我们如何通过使用类型化数据集来改进这一点呢？</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nf"><img src="../Images/d3b951990b5b300c2d1bff5f4ef7d312.png" data-original-src="https://miro.medium.com/v2/resize:fit:1346/format:webp/1*Vz0o2HNb28TUxojOJROgoQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">[合理使用]</p></figure><p id="6123" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><code class="fe ng nh ni mu b">flatMap</code>类型化数据集转换相当于Dataframes的<code class="fe ng nh ni mu b">explode()</code>方法，重要的是可以使用<strong class="lb iu">迭代器。</strong>这消除了在内存中实现列表的需要。你可能还记得Python 2的【https://www.geeksforgeeks.org/range-vs-xrange-python/】，你可以阅读这篇关于生成器优势的文章:<a class="ae ky" href="https://www.geeksforgeeks.org/range-vs-xrange-python/" rel="noopener ugc nofollow" target="_blank"/></p><p id="220f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">数据工程师可能会将您的PySpark代码翻译成某种Scala版本，生成带有<code class="fe ng nh ni mu b">Iterator</code>和<code class="fe ng nh ni mu b">flatMap</code>的新行</p><pre class="kj kk kl km gt mt mu mv mw aw mx bi"><span id="e2db" class="my mz it mu b gy na nb l nc nd">package business_logic</span><span id="6125" class="my mz it mu b gy ne nb l nc nd">case class InputData(cid: String)<br/>case class OutputData(cid: String, sku: String)</span><span id="b2cd" class="my mz it mu b gy ne nb l nc nd">object BusinessLogic {<br/>  <br/>  private def generateSkus(cid: String): Iterator[OutputData] = ???</span><span id="1372" class="my mz it mu b gy ne nb l nc nd">  def getCidSku(df: DataFrame): DataFrame = {<br/>    import spark.implicits._<br/>    val ds = df.as[InputData]<br/>    ds<br/>    .flatMap( x =&gt; generateSkus(x.cid) )<br/>    .toDF<br/>  }</span><span id="802f" class="my mz it mu b gy ne nb l nc nd">}</span></pre><p id="09b4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">scala代码被数据工程师编译打包成一个<code class="fe ng nh ni mu b">jar</code>文件。</p><p id="dbe4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">数据科学家只需将罐子添加到他们的Spark会话中</p><pre class="kj kk kl km gt mt mu mv mw aw mx bi"><span id="ac1a" class="my mz it mu b gy na nb l nc nd">spark = (<br/>  SparkSession.builder<br/>  .config("spark.jars.packages", "business-logic_2.12-1.0.0.jar")<br/>  .getOrCreate()<br/>)</span></pre><p id="a036" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们希望在使用非类型化数据帧的PySpark管道中使用类型化数据集来使用这部分业务逻辑。</p><pre class="kj kk kl km gt mt mu mv mw aw mx bi"><span id="5cd2" class="my mz it mu b gy na nb l nc nd">from pyspark.sql import SparkSession, DataFrame</span><span id="a291" class="my mz it mu b gy ne nb l nc nd">spark = SparkSession.builder.getOrCreate()<br/>sc = spark._sc<br/>sqlContext = spark._wrapped</span><span id="f9bb" class="my mz it mu b gy ne nb l nc nd">cid_df = spark.createDataFrame([<br/>  Row(cid='cid1'),<br/>  Row(cid='cid2'),<br/>  Row(cid='cid3')<br/>])</span><span id="b21e" class="my mz it mu b gy ne nb l nc nd">cid_sku_df = DataFrame(<br/>  sc._jvm.business_logic.BusinessLogic.getCidSku(cid_df._jdf),<br/>  sqlContext<br/>)</span><span id="7c57" class="my mz it mu b gy ne nb l nc nd">cid_sku_df.join(<br/>  productDf, ['sku']<br/>)</span></pre><p id="1e86" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这不是最令人难忘的语法，但这是您如何从PySpark调用任何Java/Scala代码来完成Spark工作负载，并打破使用Spark和PySpark的团队之间的语言障碍。</p><p id="22f7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">Spark数据集的Scala API文档:<a class="ae ky" href="https://spark.apache.org/docs/latest/api/scala/org/apache/spark/sql/Dataset.html" rel="noopener ugc nofollow" target="_blank">https://Spark . Apache . org/Docs/latest/API/Scala/org/Apache/Spark/SQL/Dataset . html</a></p><p id="34a0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为什么不阅读下一篇文章，了解如何使用PEX打包PySpark应用程序用于生产—相当于Python的Spark和assembly JARs:</p><div class="nj nk gp gr nl nm"><a rel="noopener follow" target="_blank" href="/pex-the-secret-sauce-for-the-perfect-pyspark-deployment-of-aws-emr-workloads-9aef0d8fa3a5"><div class="nn ab fo"><div class="no ab np cl cj nq"><h2 class="bd iu gy z fp nr fr fs ns fu fw is bi translated">PEX——AWS EMR工作负载完美PySpark部署的秘方</h2><div class="nt l"><h3 class="bd b gy z fp nr fr fs ns fu fw dk translated">如何使用PEX加速PySpark应用程序在临时AWS EMR集群上的部署</h3></div><div class="nu l"><p class="bd b dl z fp nr fr fs ns fu fw dk translated">towardsdatascience.com</p></div></div><div class="nv l"><div class="nw l nx ny nz nv oa ks nm"/></div></div></a></div></div><div class="ab cl ob oc hx od" role="separator"><span class="oe bw bk of og oh"/><span class="oe bw bk of og oh"/><span class="oe bw bk of og"/></div><div class="im in io ip iq"><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oi"><img src="../Images/e86194b05c5f56cf6e8cb4a955da5b68.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*4smcy0YtzP3uQ8Dk.png"/></div></div></figure><p id="6c5c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">Jan是公司数据转型方面的成功思想领袖和顾问，拥有将数据科学大规模应用于商业生产的记录。他最近被dataIQ评为英国100位最具影响力的数据和分析从业者之一。</p><p id="7189" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">在LinkedIn上连接:</strong><a class="ae ky" href="https://www.linkedin.com/in/janteichmann/" rel="noopener ugc nofollow" target="_blank"><strong class="lb iu">https://www.linkedin.com/in/janteichmann/</strong></a></p><p id="cb10" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">阅读其他文章:</strong><a class="ae ky" href="https://medium.com/@jan.teichmann" rel="noopener">【https://medium.com/@jan.teichmann】T21</a></p></div></div>    
</body>
</html>