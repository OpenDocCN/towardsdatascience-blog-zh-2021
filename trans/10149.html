<html>
<head>
<title>Real-time Artwork Generation using Deep Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用深度学习的实时艺术品生成</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/real-time-artwork-generation-using-deep-learning-a33a2084ae98?source=collection_archive---------20-----------------------#2021-09-25">https://towardsdatascience.com/real-time-artwork-generation-using-deep-learning-a33a2084ae98?source=collection_archive---------20-----------------------#2021-09-25</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="af5e" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">用于任意内容样式图像对之间的样式转换的自适应实例标准化(AdaIN)。</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/ffa584a036f3468459e1757c3db9155c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Dn3YllcfXH-oSDdz90Ozww.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">AI生成的艺术品。图片来源:[6]</p></figure><p id="d6cb" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在这篇文章中，我们将会看到黄等人的论文《实时任意风格转换与自适应实例规范化》(AdaIN)。艾尔。我们之所以关注这篇论文，是因为它比当时或发布时的其他最先进的方法有一些关键优势。</p><p id="756d" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">最重要的是，这种方法一旦经过训练，就可以用于在任意内容样式的图像对之间转换样式，甚至是在训练期间没有看到的图像对。而Gatys等人提出的方法。艾尔。也可以在任何内容样式图像对之间转换样式，与此方法相比，它非常慢，因为它在推断过程中对样式化图像进行迭代优化。AdaIN方法也是灵活的，它允许控制风格化图像中转换的风格的强度，还允许扩展，如风格插值和空间控制。</p><p id="739e" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">接下来，我们将首先熟悉实例规范化、自适应实例规范化，然后深入研究AdaIN论文的工作。最后，我们将看到一些输出，并查看实现自适应实例规范化和训练风格转移网络的代码。</p><h1 id="2c2c" class="lr ls iq bd lt lu lv lw lx ly lz ma mb jw mc jx md jz me ka mf kc mg kd mh mi bi translated">实例规范化</h1><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mj"><img src="../Images/891c40a2fb19eb187ff86dd782b673e3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*N0uMSJp3_0X-De3qjvfdEg.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图1:不同的物化技术。图片来源:[3]。</p></figure><p id="59c0" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">批次归一化使用整批的平均值和方差对整批固定大小的特征图进行归一化。另一方面，实例标准化(也称为对比标准化)对每个样本的每个通道进行标准化。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mk"><img src="../Images/fba86d308518150c3b6b27f4f3bf0fd8.png" data-original-src="https://miro.medium.com/v2/resize:fit:676/format:webp/1*6fgg6-Sg-QoBj6-tWorLdQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">情商。1:实例规范化</p></figure><p id="487d" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">来自Eq。1，我们可以清楚地看到，每个样本的每个通道都是单独归一化的。与批处理规范的另一个区别是，与批处理规范不同，实例规范也应用于推理过程中。</p><p id="9be1" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">实例规范对输入图像的对比度进行标准化，使风格化图像独立于输入图像对比度，从而提高输出质量[2]。[1]的作者还指出，实例标准化通过匹配特征统计(均值和方差)来充当样式标准化的一种形式。这里，图像标准化的风格由可学习的仿射参数γ和β[1]定义。</p><p id="add8" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">要了解更多关于实例规范和其他规范化技术，请点击查看T2的帖子<a class="ml mm ep" href="https://medium.com/u/e1cd4f0da28f?source=post_page-----a33a2084ae98--------------------------------" rel="noopener" target="_blank">。</a></p><h2 id="8de4" class="mo ls iq bd lt mp mq dn lx mr ms dp mb le mt mu md li mv mw mf lm mx my mh mz bi translated">条件实例规范化</h2><p id="de7e" class="pw-post-body-paragraph kv kw iq kx b ky na jr la lb nb ju ld le nc lg lh li nd lk ll lm ne lo lp lq ij bi translated">条件实例标准化在[4]中介绍，其中作者建议为每种风格的图像学习不同的γs和βs。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nf"><img src="../Images/1f609ef1131d94e9eae475b42a12bf46.png" data-original-src="https://miro.medium.com/v2/resize:fit:690/format:webp/1*UWcV3ASGUyPQlV1SR9Dfeg.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">情商。2:条件实例规范化。</p></figure><p id="c433" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">虽然这种方法工作良好，但它需要2xCxS个额外参数(C: # channels，S: # styles)，从而增加了网络的规模。因为实例norm执行一种形式的样式转换[1]，为每种样式设置一组参数允许我们将图像标准化为每种样式。</p><h1 id="90ae" class="lr ls iq bd lt lu lv lw lx ly lz ma mb jw mc jx md jz me ka mf kc mg kd mh mi bi translated">自适应实例标准化</h1><p id="b9cd" class="pw-post-body-paragraph kv kw iq kx b ky na jr la lb nb ju ld le nc lg lh li nd lk ll lm ne lo lp lq ij bi translated">作者扩展了仿射参数γ和β定义实例范数的规范化风格的思想，提出了自适应实例规范化。在建议中，实例标准化被修改以适应不同的内容风格的图像对。</p><p id="ff33" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">AdaIN将内容特征和风格特征<strong class="kx ir"> x </strong>和<strong class="kx ir"> y </strong>作为输入，并简单地将内容特征的统计量(均值和方差)与风格特征<strong class="kx ir"> y </strong>的统计量进行匹配。这里没有可学习的参数，转换如下获得</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ng"><img src="../Images/baa828cd45b6b0d23e9d1cdadad43439.png" data-original-src="https://miro.medium.com/v2/resize:fit:880/format:webp/1*f467KqXa9zl4kHIyY9xxng.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">情商。3:自适应实例规范化。</p></figure><p id="25fe" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">其中，σ (⋅是标准偏差，σ(⋅)是方差，μ(⋅是平均值，所有这些都是按照实例规范中的空间维度计算的。</p><h1 id="f103" class="lr ls iq bd lt lu lv lw lx ly lz ma mb jw mc jx md jz me ka mf kc mg kd mh mi bi translated">亚当式传输网络</h1><h2 id="ef31" class="mo ls iq bd lt mp mq dn lx mr ms dp mb le mt mu md li mv mw mf lm mx my mh mz bi translated">体系结构</h2><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nh"><img src="../Images/e9dd85c51bfbbbbc7908ceb449b191fa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*u0dYdsjzoFAMuj3QKDeitQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图2:提议的风格转移网络架构/图片来源:[1]</p></figure><p id="8171" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">AdaIn StyleNet遵循编码器-解码器架构(图2)。编码器<strong class="kx ir"> f(⋅) </strong>是VGG19网络的前几个预训练层(直到relu4_1)。编码器是固定的，没有经过训练。</p><p id="eb34" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">对编码器的输出执行自适应实例标准化，如下所示</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/95e18e97578201e335e6358d8ccc5d82.png" data-original-src="https://miro.medium.com/v2/resize:fit:794/format:webp/1*2usNftwNEfYQTKa9dK4eCQ.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">情商。4:阿丹混合。</p></figure><p id="d606" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">其中f(c)和f(s)分别是由编码器为内容和风格图像产生的特征图。混合系数α∈[0，1]控制风格化图像中风格的强度。α在训练期间被设置为1。</p><p id="9198" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">解码器<strong class="kx ir"> g(⋅) </strong>与编码器相反，池被2x最近邻向上扩展所取代。解码器用随机权重初始化，并学习其权重。通过将变换后的特征图<strong class="kx ir"> t </strong>传递到生成器中来获得风格化图像。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nj"><img src="../Images/2c69b6e243878d2578b32188e7de852a.png" data-original-src="https://miro.medium.com/v2/resize:fit:358/format:webp/1*q0d_mHqOBnSqpOquk8MNLQ.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">情商。5:风格化图像生成</p></figure><h2 id="0140" class="mo ls iq bd lt mp mq dn lx mr ms dp mb le mt mu md li mv mw mf lm mx my mh mz bi translated">损失函数</h2><p id="8f38" class="pw-post-body-paragraph kv kw iq kx b ky na jr la lb nb ju ld le nc lg lh li nd lk ll lm ne lo lp lq ij bi translated">损失目标是风格和内容损失的组合。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/1a60f8ada283873ed23a606c984317f5.png" data-original-src="https://miro.medium.com/v2/resize:fit:360/format:webp/1*ZJqClzUknwLkqLeiDYiUIQ.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">情商。6:全部损失</p></figure><p id="bd37" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">内容损失是从VGG编码器获得的风格化图像的特征图和AdaIN输出<strong class="kx ir"> <em class="nl"> t </em> </strong>之间的欧几里德距离:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nm"><img src="../Images/ee58c923d1ac96cde8f0eaeb2794a7ac.png" data-original-src="https://miro.medium.com/v2/resize:fit:442/format:webp/1*QjAcCDVVP2GLHbKJ9EBnqQ.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">情商。7:内容丢失</p></figure><p id="cf50" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">AdaIN输出<strong class="kx ir"> <em class="nl"> t </em> </strong>用作内容目标，因为它有助于模型更快地收敛。风格损失的计算公式如下:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nn"><img src="../Images/398e8b39573d546c4d8461e9fb5b3dc8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*x1rcpRG3P1_QpQwVtMtB4w.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">情商。8:风格丧失</p></figure><p id="3e11" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">每个<strong class="kx ir"> ϕ </strong> i都是VGG 19网络的一层。这里使用relu1_1、relu2_1、relu3_1和relu4_1。</p><h1 id="2f94" class="lr ls iq bd lt lu lv lw lx ly lz ma mb jw mc jx md jz me ka mf kc mg kd mh mi bi translated">输出样本</h1><p id="72e9" class="pw-post-body-paragraph kv kw iq kx b ky na jr la lb nb ju ld le nc lg lh li nd lk ll lm ne lo lp lq ij bi translated">这里有一些由模型生成的艺术作品。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi no"><img src="../Images/a40c4ca3181bc87afa6c193f03785d49.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pazQE1LoR9Jb7Cf0AgOdxw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">风格转移在行动。图像来源:(a)[7]，(b)[8]，(c )[6]</p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi np"><img src="../Images/31446b881b39a2270c6d192c559e9b59.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kYkgaa4mrXjYkoBzhgxicg.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">风格转移在行动。图片来源:(a)[9]，(b)[10]，(c )[6]</p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nq"><img src="../Images/d1230e61cf6b960d5a5ce7b3ccd44bf1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UzjWhUzJjiT9oIPP54O69A.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">使用α在推断期间控制风格化图像上的风格强度。图像来源:[1]</p></figure><h1 id="d5b3" class="lr ls iq bd lt lu lv lw lx ly lz ma mb jw mc jx md jz me ka mf kc mg kd mh mi bi translated">代码和预训练模型</h1><p id="f2e8" class="pw-post-body-paragraph kv kw iq kx b ky na jr la lb nb ju ld le nc lg lh li nd lk ll lm ne lo lp lq ij bi translated">实现自适应实例规范化的代码片段:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nr ns l"/></div></figure><p id="3d00" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">训练AdaIN型传输网络的代码可在此处找到:</p><div class="nt nu gp gr nv nw"><a href="https://github.com/aadhithya/AdaIN-pytorch" rel="noopener  ugc nofollow" target="_blank"><div class="nx ab fo"><div class="ny ab nz cl cj oa"><h2 class="bd ir gy z fp ob fr fs oc fu fw ip bi translated">GitHub-aadhithya/AdaIN-py torch:py torch实现“实时任意风格传输…</h2><div class="od l"><h3 class="bd b gy z fp ob fr fs oc fu fw dk translated">PyTorch实现的“实时任意风格转换与自适应实例规范化”由黄浚…</h3></div><div class="oe l"><p class="bd b dl z fp ob fr fs oc fu fw dk translated">github.com</p></div></div><div class="of l"><div class="og l oh oi oj of ok kp nw"/></div></div></a></div><p id="10d9" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">预训练模型(pytorch和onnx格式)的权重可在此处找到:</p><div class="nt nu gp gr nv nw"><a href="https://github.com/aadhithya/AdaIN-pytorch/releases/tag/1.0" rel="noopener  ugc nofollow" target="_blank"><div class="nx ab fo"><div class="ny ab nz cl cj oa"><h2 class="bd ir gy z fp ob fr fs oc fu fw ip bi translated">释放AdaIN StyleNet重量aadhithya/AdaIN-pytorch</h2><div class="od l"><h3 class="bd b gy z fp ob fr fs oc fu fw dk translated">训练30000次迭代后的模型权重。样式权重(训练):10创建模型和加载权重:模型=…</h3></div><div class="oe l"><p class="bd b dl z fp ob fr fs oc fu fw dk translated">github.com</p></div></div><div class="of l"><div class="ol l oh oi oj of ok kp nw"/></div></div></a></div><h1 id="36c9" class="lr ls iq bd lt lu lv lw lx ly lz ma mb jw mc jx md jz me ka mf kc mg kd mh mi bi translated">结论</h1><p id="a14b" class="pw-post-body-paragraph kv kw iq kx b ky na jr la lb nb ju ld le nc lg lh li nd lk ll lm ne lo lp lq ij bi translated">这篇文章介绍了实例规范化以及自适应实例规范化如何用于任意内容样式图像对之间的样式转换。通过训练生成器网络和使用自适应实例标准化，我们能够在任何内容样式图像对之间转移样式。我们还能够在运行时控制生成图像的样式强度。该方法提供的另一个优势是推理速度超过当时的其他模型。最后，展示了一些输出，代码和预先训练的推理权重可供免费、非商业使用。</p><p id="034b" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">如果您发现帖子中有任何错误，请留下评论，我会修复它们！</p><p id="58e7" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">如果你喜欢这篇文章，请考虑关注作者，<a class="ml mm ep" href="https://medium.com/u/82053676fe58?source=post_page-----a33a2084ae98--------------------------------" rel="noopener" target="_blank"> Aadhithya Sankar </a>。</p></div><div class="ab cl om on hu oo" role="separator"><span class="op bw bk oq or os"/><span class="op bw bk oq or os"/><span class="op bw bk oq or"/></div><div class="ij ik il im in"><h1 id="864c" class="lr ls iq bd lt lu ot lw lx ly ou ma mb jw ov jx md jz ow ka mf kc ox kd mh mi bi translated">参考</h1><p id="eb6e" class="pw-post-body-paragraph kv kw iq kx b ky na jr la lb nb ju ld le nc lg lh li nd lk ll lm ne lo lp lq ij bi translated">[1]黄、荀、贝隆吉。“通过自适应实例规范化实时传输任意样式。”<em class="nl">IEEE计算机视觉国际会议论文集</em>。2017.</p><p id="5932" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">[2] D.Ulyanov，A.Vedaldi和V.Lempitsky .改进的纹理网络:在前馈风格化和纹理合成中最大化质量和多样性。2017年<em class="nl"> CVPR </em>。</p><p id="f2fd" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">[3]吴、庾信、何。“分组规范化。”欧洲计算机视觉会议(ECCV)会议录。2018.</p><p id="2447" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">[4] V. Dumoulin、J. Shlens和M. Kudlur。艺术风格的学术表现。在<em class="nl"> ICLR </em>，2017。</p><p id="8857" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">[5]李彦宏，王，刘军，侯。揭开神经类型转移的神秘面纱。<em class="nl"> arXiv预印本arXiv:1701.01036 </em>，2017。</p><p id="a2a3" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">[6]<a class="ae mn" href="https://github.com/aadhithya/AdaIN-pytorch" rel="noopener ugc nofollow" target="_blank">https://github.com/aadhithya/AdaIN-pytorch</a></p><p id="3111" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">[7]<a class="ae mn" href="https://en.wikipedia.org/wiki/File:Side_profile,_Brihadeeswara.jpg" rel="noopener ugc nofollow" target="_blank">https://en . Wikipedia . org/wiki/File:Side _ profile，_Brihadeeswara.jpg </a></p><p id="2a48" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">[8]<a class="ae mn" href="https://en.wikipedia.org/wiki/Olive_Trees_(Van_Gogh_series)#/media/File:Van_Gogh_The_Olive_Trees..jpg" rel="noopener ugc nofollow" target="_blank">https://en . Wikipedia . org/wiki/Olive _ Trees _(梵高系列)#/media/File:梵高_The_Olive_Trees..jpg </a></p><p id="072a" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">【9】<a class="ae mn" href="https://en.wikipedia.org/wiki/Taj_Mahal#/media/File:Taj_Mahal_in_India_-_Kristian_Bertel.jpg" rel="noopener ugc nofollow" target="_blank">https://en . Wikipedia . org/wiki/Taj _ Mahal #/media/File:Taj _ Mahal _ in _ India _-_ Kristian _ bertel . jpg</a></p><p id="4cad" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">【10】<a class="ae mn" href="https://en.wikipedia.org/wiki/The_Starry_Night#/media/File:Van_Gogh_-_Starry_Night_-_Google_Art_Project.jpg" rel="noopener ugc nofollow" target="_blank">https://en . Wikipedia . org/wiki/The _ Starry _ Night #/media/File:Van _ Gogh _-_ Starry _ Night _-_ Google _ Art _ project . jpg</a></p></div><div class="ab cl om on hu oo" role="separator"><span class="op bw bk oq or os"/><span class="op bw bk oq or os"/><span class="op bw bk oq or"/></div><div class="ij ik il im in"><h1 id="8528" class="lr ls iq bd lt lu ot lw lx ly ou ma mb jw ov jx md jz ow ka mf kc ox kd mh mi bi translated">作者的其他作品</h1><p id="15d3" class="pw-post-body-paragraph kv kw iq kx b ky na jr la lb nb ju ld le nc lg lh li nd lk ll lm ne lo lp lq ij bi translated">如果你喜欢这篇文章，这里有一些你可能会喜欢的文章:</p><div class="nt nu gp gr nv nw"><a rel="noopener follow" target="_blank" href="/learning-disentangled-representations-with-invertible-flow-based-interpretation-networks-9954554a28d2"><div class="nx ab fo"><div class="ny ab nz cl cj oa"><h2 class="bd ir gy z fp ob fr fs oc fu fw ip bi translated">用可逆的(基于流的)解释网络学习解纠缠的表示</h2><div class="od l"><h3 class="bd b gy z fp ob fr fs oc fu fw dk translated">什么是解开的表征？</h3></div><div class="oe l"><p class="bd b dl z fp ob fr fs oc fu fw dk translated">towardsdatascience.com</p></div></div><div class="of l"><div class="oy l oh oi oj of ok kp nw"/></div></div></a></div><div class="nt nu gp gr nv nw"><a rel="noopener follow" target="_blank" href="/demystified-wasserstein-gans-wgan-f835324899f4"><div class="nx ab fo"><div class="ny ab nz cl cj oa"><h2 class="bd ir gy z fp ob fr fs oc fu fw ip bi translated">揭秘:瓦瑟斯坦·甘斯(WGAN)</h2><div class="od l"><h3 class="bd b gy z fp ob fr fs oc fu fw dk translated">什么是瓦瑟斯坦距离？用Wasserstein距离训练GANs背后的直觉是什么？怎么样…</h3></div><div class="oe l"><p class="bd b dl z fp ob fr fs oc fu fw dk translated">towardsdatascience.com</p></div></div><div class="of l"><div class="oz l oh oi oj of ok kp nw"/></div></div></a></div><div class="nt nu gp gr nv nw"><a rel="noopener follow" target="_blank" href="/a-primer-on-atrous-convolutions-and-depth-wise-separable-convolutions-443b106919f5"><div class="nx ab fo"><div class="ny ab nz cl cj oa"><h2 class="bd ir gy z fp ob fr fs oc fu fw ip bi translated">阿特鲁卷积和深度可分卷积的初步研究</h2><div class="od l"><h3 class="bd b gy z fp ob fr fs oc fu fw dk translated">什么是萎缩/扩张和深度方向可分卷积？与标准卷积有何不同？什么…</h3></div><div class="oe l"><p class="bd b dl z fp ob fr fs oc fu fw dk translated">towardsdatascience.com</p></div></div><div class="of l"><div class="pa l oh oi oj of ok kp nw"/></div></div></a></div></div></div>    
</body>
</html>