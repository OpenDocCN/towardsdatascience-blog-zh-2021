<html>
<head>
<title>Deep Learning and Transfer Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">深度学习和迁移学习</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/deep-learning-and-transfer-learning-31c6394b10c5?source=collection_archive---------19-----------------------#2021-06-08">https://towardsdatascience.com/deep-learning-and-transfer-learning-31c6394b10c5?source=collection_archive---------19-----------------------#2021-06-08</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="805b" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">使用Keras的迁移学习的各种实现</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ki"><img src="../Images/15c3ba60fb8c0146386a827271df1558.png" data-original-src="https://miro.medium.com/v2/resize:fit:996/format:webp/1*vZiXNiixfTDZdVYWgPgcUg.png"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">作者图片</p></figure><p id="d562" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">对复杂的神经网络模型进行几天、几周甚至几个月的训练并不罕见。我们大多数人都无法获得成功训练这些模型所需的大量数据和计算资源。也就是说，有OpenAI、脸书、谷歌、脸书等实体。他们拥有开发非常复杂、精确和通用模型的资源。更重要的是，他们愿意为我们这些凡人开源这些模型，用于我们自己的研究。将预先训练好的模型融入到自己的模型中的过程被命名为“迁移学习”。</p><h1 id="6e60" class="lq lr it bd ls lt lu lv lw lx ly lz ma jz mb ka mc kc md kd me kf mf kg mg mh bi translated">涵盖的主题</h1><p id="2de5" class="pw-post-body-paragraph ku kv it kw b kx mi ju kz la mj jx lc ld mk lf lg lh ml lj lk ll mm ln lo lp im bi translated">在本教程中，我们将回顾使用Keras库的迁移学习的四种独特实现。我们将讨论如何实现一个预先训练好的模型。我们将讨论如何只选择预训练模型的特定部分用于我们自己的模型。我们将回顾如何将新的定制砝码加载到预训练模型中。最后，我们将展示如何对图像的形状进行上采样，以符合预训练模型的形状要求。</p><ol class=""><li id="e549" class="mn mo it kw b kx ky la lb ld mp lh mq ll mr lp ms mt mu mv bi translated">使用整个(顶层除外)预训练模型进行迁移学习</li><li id="2d22" class="mn mo it kw b kx mw la mx ld my lh mz ll na lp ms mt mu mv bi translated">使用预训练模型的特定部分进行迁移学习</li><li id="b209" class="mn mo it kw b kx mw la mx ld my lh mz ll na lp ms mt mu mv bi translated">通过将新权重加载到预训练模型中来转移学习</li><li id="d029" class="mn mo it kw b kx mw la mx ld my lh mz ll na lp ms mt mu mv bi translated">通过对图像形状进行上采样来适应预训练模型的输入要求的迁移学习</li></ol><h1 id="60f7" class="lq lr it bd ls lt lu lv lw lx ly lz ma jz mb ka mc kc md kd me kf mf kg mg mh bi translated">数据</h1><p id="068e" class="pw-post-body-paragraph ku kv it kw b kx mi ju kz la mj jx lc ld mk lf lg lh ml lj lk ll mm ln lo lp im bi translated">我们将利用两种图像分类模型，即谷歌的Inception_V3 ( <a class="ae nb" href="https://keras.io/api/applications/inceptionv3/" rel="noopener ugc nofollow" target="_blank">链接</a>)和微软的ResNet-50 ( <a class="ae nb" href="https://github.com/KaimingHe/deep-residual-networks" rel="noopener ugc nofollow" target="_blank">链接</a>)。两者都是高度复杂的卷积神经网络，通过1000个<a class="ae nb" href="https://gist.github.com/yrevar/942d3a0ac09ec9e5eb3a" rel="noopener ugc nofollow" target="_blank">类</a>对超过一百万张图像进行训练。我们将在使用CIFAR-10数据集的图像分类问题中使用这些模型。CIFAR-10数据集由10个类别(飞机、汽车、鸟、猫、鹿、狗、青蛙、马、船和卡车)的60，000幅32x32彩色图像组成。有50，000幅图像用于训练，10，000幅图像用于测试。总之，我们将使用迁移学习将Inception_V3和ResNet-50模型合并到我们自己的模型中，该模型将使用CIFAR-10数据集进行训练，以正确预测上面列出的10类图像。</p><h2 id="d67f" class="nc lr it bd ls nd ne dn lw nf ng dp ma ld nh ni mc lh nj nk me ll nl nm mg nn bi translated">概念上</h2><p id="3228" class="pw-post-body-paragraph ku kv it kw b kx mi ju kz la mj jx lc ld mk lf lg lh ml lj lk ll mm ln lo lp im bi translated">迁移学习背后的基本概念可以用下图来解释。假设您试图创建一个多类图像分类模型(10类:飞机、汽车、鸟、猫、鹿、狗、青蛙、马、船和卡车)。换句话说，你正在建立一个模型，它将尝试并正确地将一幅图像归类为上面列出的10个类别之一。开源的Inception_V3和ResNet-50模型是完整的图像分类模型，由40多个层组成，包括一系列卷积层、池化层、正则化层以及在顶部利用softmax激活函数的预测密集层。换句话说，您可以使用Inception_V3和ResNet-50模型，并且只能使用这些模型来对您的图像进行分类。这是因为它们在顶部都有一个完全连接的密集层，这将产生来自softmax激活函数的图像分类概率。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="np nq di nr bf ns"><div class="gh gi no"><img src="../Images/5071e433d58856bcda61fde48215f4f8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FXKX1vg8I1cNa_FPmtIJcg.png"/></div></div><p class="kq kr gj gh gi ks kt bd b be z dk translated"><a class="ae nb" href="https://www.researchgate.net/figure/Schematic-diagram-of-InceptionV3-model-compressed-view_fig6_326421398" rel="noopener ugc nofollow" target="_blank">https://www . researchgate . net/figure/Schematic-diagram-of-InceptionV3-model-compressed-view _ fig 6 _ 326421398</a></p></figure><p id="3d1f" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">也就是说，一般输入(即Inception和ResNet模型被训练的图像)和你的模型将被训练的图像可能是相似的(即动物的图像)与你试图解决的问题可能会有所不同。例如，不是试图分类1000个非常不同的图像(即飞机、汽车、鸟、猫等等)由于Inception_V3和ResNet-50被训练成这样，您可能会对分类各种狗品种感兴趣。在这种情况下，使用独立的Inception或ResNet模型对犬种进行分类可能会产生不良结果。我们要做的是利用这些复杂模型所学到的知识(即。不同类型图像之间的独特特征)并将该知识转移到我们试图分类的图像(即狗的品种)。下面是另一个传递知识的例子。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/a60fbde8fab5e4db5ba9533f91426f68.png" data-original-src="https://miro.medium.com/v2/resize:fit:526/format:webp/1*JxK2sLdxA1hszfbFxTVkMQ.png"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">孙，b .，冯，j .，，萨恩科，K. (2016)。令人沮丧的简单领域适应的回归。《第三十届AAAI人工智能会议论文集》(AAAI-16)。从http://arxiv.org/abs/1511.05547<a class="ae nb" href="https://arxiv.org/abs/1511.05547" rel="noopener ugc nofollow" target="_blank">取回</a></p></figure><p id="d1fe" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">迁移学习的过程包括从Inception或ResNet模型中移除原始的完全连接密集层。你基本上消除了模型的预测能力。接下来，您开始添加自己的自定义图层集，其中包括顶部完全连接的密集图层。通过遵循这一过程，您已经构建了一个模型，该模型包含了从Inception或ResNet模型中获取的大量一般图像分类知识，并且通过添加您自己的微调层来准确地对您自己的任务所需的图像进行分类。</p><p id="83fe" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">此外，由我们来决定我们希望在模型中使用多少预训练模型。一般来说，在初始模型的早期部分中的卷积层将会学习非常一般化的特性(即垂直/水平线、全局形状等。)与识别非常具体的特征的后面的层相比较。后面的图层对模型没有见过的图像的概化程度较低。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="np nq di nr bf ns"><div class="gh gi nu"><img src="../Images/7f47de61a52c9f3af3c8e4218b1f8dbd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RMeSQ1JWTu-Vbu6PBiOhAA.png"/></div></div><p class="kq kr gj gh gi ks kt bd b be z dk translated"><a class="ae nb" href="https://www.researchgate.net/figure/Schematic-of-the-Inception-3-model-created-by-Google-Research-Shlens2016-and-modified_fig6_321307161" rel="noopener ugc nofollow" target="_blank">https://www . Research gate . net/figure/Schematic-of-the-Inception-3-model-created-by-Google-Research-shlens 2016-and-modified _ fig 6 _ 321307161</a></p></figure><h1 id="791a" class="lq lr it bd ls lt lu lv lw lx ly lz ma jz mb ka mc kc md kd me kf mf kg mg mh bi translated">说够了，我们来编码吧！</h1><p id="d74c" class="pw-post-body-paragraph ku kv it kw b kx mi ju kz la mj jx lc ld mk lf lg lh ml lj lk ll mm ln lo lp im bi translated">本教程的所有开发都是使用Google Colab的数据处理功能完成的。你当然可以在本地机器上运行这段代码，但是除非你有一个专用的GPU，否则训练时间可能会相当长。</p><h2 id="e8ac" class="nc lr it bd ls nd ne dn lw nf ng dp ma ld nh ni mc lh nj nk me ll nl nm mg nn bi translated">图像处理</h2><p id="40ff" class="pw-post-body-paragraph ku kv it kw b kx mi ju kz la mj jx lc ld mk lf lg lh ml lj lk ll mm ln lo lp im bi translated">让我们将所需的库和数据一起导入。如上所述，我们将使用CIFAR-10数据集，它由10个类别(飞机、汽车、鸟、猫、鹿、狗、青蛙、马、船和卡车)的60，000张32x32彩色图像组成。有50，000幅图像用于训练，10，000幅图像用于测试。</p><pre class="kj kk kl km gt nv nw nx ny aw nz bi"><span id="f8bc" class="nc lr it nw b gy oa ob l oc od">import tensorflow as tf<br/>import numpy as np<br/>import pandas as pd<br/>import matplotlib.pyplot as plt<br/>from keras.utils.vis_utils import plot_model<br/>from tensorflow.keras.utils import to_categorical<br/>from keras.models import Model<br/>from keras import backend as K<br/>from tensorflow.keras.preprocessing.image import ImageDataGenerator<br/>from keras.optimizers import Adam<br/>from tensorflow.keras.applications.inception_v3 import InceptionV3<br/>from tensorflow.keras.applications.resnet50 import ResNet50</span><span id="ffab" class="nc lr it nw b gy oe ob l oc od">%matplotlib inline</span><span id="d70a" class="nc lr it nw b gy oe ob l oc od">data = tf.keras.datasets.cifar10<br/>(train_images, train_labels), (test_images, test_labels) = data.load_data()</span><span id="5217" class="nc lr it nw b gy oe ob l oc od">print(train_images.shape, train_labels.shape, test_images.shape, test_labels.shape)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi of"><img src="../Images/b1e48100a062741f39a6316107ff6e20.png" data-original-src="https://miro.medium.com/v2/resize:fit:906/format:webp/1*LayY1A4qFIoh22AVIlZiGw.png"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">50，000个训练图像，每个32x32乘3个通道(彩色)/ 50，000个训练标签0–9个类别</p></figure><p id="7555" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">正如你所想象的，32x32的图像将会非常像素化，但我们仍然可以分辨出图像的一般类别。</p><pre class="kj kk kl km gt nv nw nx ny aw nz bi"><span id="f5da" class="nc lr it nw b gy oa ob l oc od">fig, ax = plt.subplots(1, 5)<br/>fig.set_size_inches(16, 6)</span><span id="ceab" class="nc lr it nw b gy oe ob l oc od">for i in range(5):<br/>  img = train_images[i]<br/>  ax[i].imshow(img)<br/>  plt.show()</span><span id="d9e6" class="nc lr it nw b gy oe ob l oc od">classes = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']</span><span id="b7c5" class="nc lr it nw b gy oe ob l oc od">print('Image Classes:' + str([x[0] for x in train_labels[0:5]]))</span><span id="2596" class="nc lr it nw b gy oe ob l oc od">print('Image Labels: ' + str([classes[x[0]] for x in train_labels[0:5]]))</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="np nq di nr bf ns"><div class="gh gi og"><img src="../Images/96b9c72d322c38001da15ec368ae8599.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*drrAcOaHZbAGyMDIdCvZDg.png"/></div></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">作者图片</p></figure><h1 id="a23f" class="lq lr it bd ls lt lu lv lw lx ly lz ma jz mb ka mc kc md kd me kf mf kg mg mh bi translated">快速图像处理</h1><h2 id="9f33" class="nc lr it bd ls nd ne dn lw nf ng dp ma ld nh ni mc lh nj nk me ll nl nm mg nn bi translated">缩放像素值和一次性编码标签</h2><p id="1c64" class="pw-post-body-paragraph ku kv it kw b kx mi ju kz la mj jx lc ld mk lf lg lh ml lj lk ll mm ln lo lp im bi translated">因为我们的图像是彩色的，所以三个通道(红、蓝、绿)的训练像素值在0-255之间。通常的做法是将值缩放1.0/255倍，以在0和1之间转换像素值。此外，我们的类标签已经进行了标签编码(0–9 ),这引入了类之间的自然顺序(即1大于0，2大于1…).由于我们有一个分类问题，我们需要消除这些潜在的混淆关系。我们将应用一键编码，将类标签转换成二进制数组。</p><pre class="kj kk kl km gt nv nw nx ny aw nz bi"><span id="9b3d" class="nc lr it nw b gy oa ob l oc od">train_images = train_images / 255.0</span><span id="76de" class="nc lr it nw b gy oe ob l oc od">test_images = test_images / 255.0</span><span id="7cec" class="nc lr it nw b gy oe ob l oc od">train_labels = to_categorical(train_labels)</span><span id="5953" class="nc lr it nw b gy oe ob l oc od">test_labels = to_categorical(test_labels)</span></pre><h1 id="603a" class="lq lr it bd ls lt lu lv lw lx ly lz ma jz mb ka mc kc md kd me kf mf kg mg mh bi translated">1.使用整个(顶层除外)预训练模型进行迁移学习</h1><p id="1665" class="pw-post-body-paragraph ku kv it kw b kx mi ju kz la mj jx lc ld mk lf lg lh ml lj lk ll mm ln lo lp im bi translated">可以想象，根据您的特定需求，迁移学习有多种实现方式。第一种技术包括加载一个预先训练的模型，使用其权重作为模型中的第一个连续层，并添加您自己的自定义层(即。通常称为“微调”的过程)来对CIFAR-10图像进行分类。</p><p id="796b" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">首先，我们需要实例化预训练的ResNet-50模型。将“include_top”参数设置为“False”实质上将删除最后一个(即top)全连通预测(softmax)密集层。由于这将是我们的模型中的第一层，我们将input_shape设置为(32，32，3)，这是我们的CIFAR-10图像的形状(即。32x32像素和3个彩色通道(RBG)。</p><pre class="kj kk kl km gt nv nw nx ny aw nz bi"><span id="bd9f" class="nc lr it nw b gy oa ob l oc od">K.clear_session()<br/>base_model = ResNet50(include_top=False, weights='imagenet', input_shape=(32, 32, 3), classes=train_labels.shape[1])</span></pre><p id="ae09" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">由于迁移学习的全部目的是使用预先训练好的模型的知识(即。预训练权重)在我们的模型中，我们将“可训练”参数设置为假。这样，我们就不会根据自己的数据重新训练ResNet-50。我们还可以仔细检查每一层是否冻结。</p><pre class="kj kk kl km gt nv nw nx ny aw nz bi"><span id="f704" class="nc lr it nw b gy oa ob l oc od">base_model.trainable = False<br/>for layer in base_model.layers:<br/>   print(layer, layer.trainable)</span></pre><p id="129e" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">最后，我们将构建模型。“预训练模型”变量将包含来自ResNet-50的冻结重量。这些权重将被输入到Flatten层，这恰好是我们在预训练ResNet层之后添加的第一个自定义或微调层。然后，我们继续添加额外的层，密集1和密集2，接着是我们的全连接层，这将使用softmax激活功能进行10级分类。</p><pre class="kj kk kl km gt nv nw nx ny aw nz bi"><span id="cd67" class="nc lr it nw b gy oa ob l oc od">pre_trained_model = base_model.output </span><span id="c0aa" class="nc lr it nw b gy oe ob l oc od">flat = tf.keras.layers.Flatten()(pre_trained_model)</span><span id="dc12" class="nc lr it nw b gy oe ob l oc od">dense1 = tf.keras.layers.Dense(1024, activation='relu')(flat)</span><span id="c49b" class="nc lr it nw b gy oe ob l oc od">dense2 = tf.keras.layers.Dense(512, activation='relu')(dense1)</span><span id="360d" class="nc lr it nw b gy oe ob l oc od">predictions = tf.keras.layers.Dense(10, activation='softmax')(dense2)</span></pre><p id="8f62" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">最后，我们通过传递base_model输入(即预训练权重)转换为输入参数，并将我们的预测转换为输出参数。最后，我们编译模型，为训练做好准备。请注意“总参数”和“可训练参数”之间的区别。我们模型中的大多数参数属于ResNet层，如果您还记得的话，该层是冻结的。</p><pre class="kj kk kl km gt nv nw nx ny aw nz bi"><span id="60b9" class="nc lr it nw b gy oa ob l oc od">model = Model(inputs = base_model.input, outputs = predictions, name="cifar10_model")</span><span id="0e43" class="nc lr it nw b gy oe ob l oc od">model.compile(optimizer=Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy']) </span><span id="8460" class="nc lr it nw b gy oe ob l oc od">model.summary()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/a415ee8b12dac837e004ec9928c026a9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*ezMoDFe14__QrL1SSxRojw.png"/></div></figure><p id="3a44" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">这是建立这个模型的另一种方法。</p><pre class="kj kk kl km gt nv nw nx ny aw nz bi"><span id="ef06" class="nc lr it nw b gy oa ob l oc od">K.clear_session()</span><span id="9357" class="nc lr it nw b gy oe ob l oc od">base_model = ResNet50(include_top=False, weights='imagenet', input_shape=(32, 32, 3), classes=train_labels.shape[1])</span><span id="970a" class="nc lr it nw b gy oe ob l oc od">base_model.trainable = False</span><span id="1c7b" class="nc lr it nw b gy oe ob l oc od">model = tf.keras.Sequential([<br/>base_model, tf.keras.layers.Flatten(), <br/>tf.keras.layers.Dense(1024, activation='relu'), tf.keras.layers.Dense(512, activation='relu'), tf.keras.layers.Dense(10, activation='softmax')])</span></pre><h1 id="d2a8" class="lq lr it bd ls lt lu lv lw lx ly lz ma jz mb ka mc kc md kd me kf mf kg mg mh bi translated">2.使用预训练模型的特定部分进行迁移学习</h1><p id="d529" class="pw-post-body-paragraph ku kv it kw b kx mi ju kz la mj jx lc ld mk lf lg lh ml lj lk ll mm ln lo lp im bi translated">如前所述，当构建卷积网络时，早期的卷积层学习非常一般化的特征(即垂直/水平线、全局形状等。)而后面的卷积层学习非常具体的特征。后面的图层对模型没有见过的图像的概化程度较低。因此，有时最好只使用预训练模型的前几个卷积层，然后添加您自己的微调层，以了解特定于您的训练图像的特征。</p><p id="332c" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">首先，我们将再次实例化ResNet-50模型，然后我们将打印它的摘要，以确定我们到底要在哪里进行切割。你决定削减预训练模型将需要一些试验和错误。请记住，模型摘要很可能会非常大。</p><pre class="kj kk kl km gt nv nw nx ny aw nz bi"><span id="5a11" class="nc lr it nw b gy oa ob l oc od">K.clear_session()</span><span id="8471" class="nc lr it nw b gy oe ob l oc od">base_model = ResNet50(include_top=False, weights='imagenet', input_shape=(32, 32, 3), classes=train_labels.shape[1])</span><span id="4d21" class="nc lr it nw b gy oe ob l oc od">base_model.summary()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="np nq di nr bf ns"><div class="gh gi oi"><img src="../Images/42b4065db4af979acfb18b566df00b7b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_IncaB6kTf7Sfx6Tyyd2ow.png"/></div></div></figure><p id="ba39" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">让我们在名为“conv3_block1_3_conv”的图层上进行剪切。注意，我们使用get_layer()函数获取特定的层，并将其输出设置为last_output变量。Last_output成为我们的微调模型中的第一层，它的输出被传递到展平层。</p><pre class="kj kk kl km gt nv nw nx ny aw nz bi"><span id="a32b" class="nc lr it nw b gy oa ob l oc od">K.clear_session()</span><span id="1bb2" class="nc lr it nw b gy oe ob l oc od">base_model = ResNet50(include_top=False, weights='imagenet', input_shape=(32, 32, 3), classes=train_labels.shape[1])</span><span id="0707" class="nc lr it nw b gy oe ob l oc od">base_model.trainable = False</span><span id="0edd" class="nc lr it nw b gy oe ob l oc od">last_layer = base_model.get_layer('conv3_block1_3_conv')</span><span id="1ed4" class="nc lr it nw b gy oe ob l oc od">last_output = last_layer.output</span><span id="15a5" class="nc lr it nw b gy oe ob l oc od">flat = tf.keras.layers.Flatten()(last_output)</span><span id="21b1" class="nc lr it nw b gy oe ob l oc od">dense1 = tf.keras.layers.Dense(1024, activation='relu')(flat)</span><span id="cd88" class="nc lr it nw b gy oe ob l oc od">dense2 = tf.keras.layers.Dense(512, activation='relu')(dense1)</span><span id="3662" class="nc lr it nw b gy oe ob l oc od">predictions = tf.keras.layers.Dense(10, activation='softmax')(dense2)</span><span id="7485" class="nc lr it nw b gy oe ob l oc od">model = Model(inputs = base_model.input, outputs = predictions, name="cifar10_model")</span><span id="1cae" class="nc lr it nw b gy oe ob l oc od">model.compile(optimizer=Adam(learning_rate=0.01), loss='categorical_crossentropy', metrics=['accuracy']) </span></pre><h1 id="f64f" class="lq lr it bd ls lt lu lv lw lx ly lz ma jz mb ka mc kc md kd me kf mf kg mg mh bi translated">3.通过将新权重加载到预训练模型中来转移学习</h1><p id="d835" class="pw-post-body-paragraph ku kv it kw b kx mi ju kz la mj jx lc ld mk lf lg lh ml lj lk ll mm ln lo lp im bi translated">有时，您会希望解冻预训练传输模型中的某些层(即ResNet或Inception)来使用您的训练图像重新训练那些“未冻结”的层。如果您曾经想要将这个新版本的微调模型与一些重新训练的ResNet或Inception层一起重用，那么您需要在将来将这些权重上传到模型中。在本节中，我们将讨论如何将存储在“h5”文件中的自定义权重加载到预训练模型的架构中。</p><p id="2400" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">首先，您需要获得特定于模型架构的权重。这些可以作为模型的更新权重或从您自己的自定义再训练中保存的权重来获得。在本例中，我们将从github帐户下载权重。请记住，这些重量不拥有顶部(即。预测)层。注意文件标签中的“notop.h5”。</p><p id="2e87" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">您需要再次实例化预训练模型，但这次将“权重”参数设置为<strong class="kw iu"> None </strong>。接下来，我们使用load_weights()函数加载保存的权重。确保将“by_name”参数设置为<strong class="kw iu"> True </strong>，因为这将根据每个层的名称加载权重。请记住，此程序仅加载预训练模型的权重(即《盗梦空间》、《雷斯网》、《VGG》等。)它不会加载整个微调模型的权重。</p><pre class="kj kk kl km gt nv nw nx ny aw nz bi"><span id="767a" class="nc lr it nw b gy oa ob l oc od">K.clear_session()<br/>!wget --no-check-certificate \</span><span id="1701" class="nc lr it nw b gy oe ob l oc od">https://github.com/fchollet/deep-learning-models/releases/download/v0.2/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5 \</span><span id="179c" class="nc lr it nw b gy oe ob l oc od">-O /tmp/resnet_50_weights_tf_dim_ordering_tf_kernels_notop.h5</span><span id="742e" class="nc lr it nw b gy oe ob l oc od">local_weights_file = '/tmp/resnet_50_weights_tf_dim_ordering_tf_kernels_notop.h5'</span><span id="cbd4" class="nc lr it nw b gy oe ob l oc od">base_model = ResNet50(weights=None, input_shape=(32, 32, 3), include_top=False, classes=train_labels.shape[1])</span><span id="361f" class="nc lr it nw b gy oe ob l oc od">base_model.load_weights(local_weights_file, by_name=True)</span></pre><p id="f966" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">最后，我们照常构建我们的模型。我们再次将“可训练”参数设置为False，因为我们不想重新训练刚刚加载的新重量。</p><pre class="kj kk kl km gt nv nw nx ny aw nz bi"><span id="bbde" class="nc lr it nw b gy oa ob l oc od">base_model.trainable = False</span><span id="8a9d" class="nc lr it nw b gy oe ob l oc od">pre_trained_model = base_model.output </span><span id="3c45" class="nc lr it nw b gy oe ob l oc od">flat = tf.keras.layers.Flatten()(pre_trained_model)</span><span id="588b" class="nc lr it nw b gy oe ob l oc od">dense1 = tf.keras.layers.Dense(1024, activation='relu')(flat)</span><span id="5433" class="nc lr it nw b gy oe ob l oc od">dense2 = tf.keras.layers.Dense(512, activation='relu')(dense1)</span><span id="6aba" class="nc lr it nw b gy oe ob l oc od">predictions = tf.keras.layers.Dense(10, activation='softmax')(dense2)</span><span id="2177" class="nc lr it nw b gy oe ob l oc od">model = Model(inputs = base_model.input, outputs = predictions, name="cifar10_model")</span><span id="9fed" class="nc lr it nw b gy oe ob l oc od">model.compile(optimizer=Adam(learning_rate=0.01), loss='categorical_crossentropy', metrics=['accuracy']) </span></pre><h1 id="5257" class="lq lr it bd ls lt lu lv lw lx ly lz ma jz mb ka mc kc md kd me kf mf kg mg mh bi translated">4.通过对图像形状进行上采样以适应预训练模型的输入要求的迁移学习</h1><p id="83dc" class="pw-post-body-paragraph ku kv it kw b kx mi ju kz la mj jx lc ld mk lf lg lh ml lj lk ll mm ln lo lp im bi translated">有些模型要求您在传递数据之前调整数据的输入大小。例如，如果您使用我们上面讨论的第一种迁移学习方法，但是这次使用Inception_V3模型，您将会看到下面的错误。这是因为CIFAR-10图像的形状为32x32x3，而Inception_V3模型要求图像至少为75x75，而不考虑通道的数量。另一方面，ResNet有一个最佳形状，但它不要求您的输入形状是一个特定的大小。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/a014127aace8d9abf97b0fce574b8f62.png" data-original-src="https://miro.medium.com/v2/resize:fit:1180/format:webp/1*PxubF1r0AH-YjQOB2ftChA.png"/></div></figure><p id="1cf6" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">为了遵守这些形状要求，我们需要将图像“上采样”到适当的形状。有各种各样的技术用于执行这个操作，但是Keras有一个“Upsamplign2D”类，它只是将像素复制到指定的形状。在下面的例子中，原始图像中的每个像素被复制3次。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ok"><img src="../Images/abe4914cf16832681d97bf548a0d24f1.png" data-original-src="https://miro.medium.com/v2/resize:fit:994/format:webp/1*v_SzelbG3n-lH6BoLpHnrw.png"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">2016马修·吉亚萨<matthew/></p></figure><p id="55be" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">为了超过像素形状的要求，我们需要将像素放大两倍。我们再次实例化我们的base_model，这次使用Inception_V3。请注意，input_shape为(128，128，3)。我们也将冻结所有的层。现在，我们开始构建我们的顺序模型，第一层是“输入层”，它只是作为数据的入口点。我们将input_shape指定为(32，32，3)，这与我们的CIFAR-10图像的形状相匹配。接下来，我们需要对图像进行两次上采样(即32x2=64，64x2=128)。如果我们仅向上采样一次，我们的图像将具有(64，64，3)的形状，这仍然不满足(75x75)的要求。通过两次上采样，我们的图像现在是(128，128，3)，注意这是我们如何获得基本模型的输入形状的。</p><pre class="kj kk kl km gt nv nw nx ny aw nz bi"><span id="9c4c" class="nc lr it nw b gy oa ob l oc od">K.clear_session()</span><span id="80fa" class="nc lr it nw b gy oe ob l oc od">base_model = InceptionV3(include_top=False, weights='imagenet',  input_shape=(128, 128, 3), classes=train_labels.shape[1])</span><span id="5c4e" class="nc lr it nw b gy oe ob l oc od">base_model.trainable = False</span><span id="3664" class="nc lr it nw b gy oe ob l oc od">inputs = tf.keras.layers.Input(shape=(32, 32, 3)) </span><span id="b52d" class="nc lr it nw b gy oe ob l oc od">upsamp1 = tf.keras.layers.UpSampling2D((2,2))(inputs)</span><span id="69bd" class="nc lr it nw b gy oe ob l oc od">upsamp2 = tf.keras.layers.UpSampling2D((2,2))(upsamp1)</span><span id="6c2a" class="nc lr it nw b gy oe ob l oc od">pre_trained_model = base_model(upsamp2) </span><span id="5fc5" class="nc lr it nw b gy oe ob l oc od">flat = tf.keras.layers.Flatten()(pre_trained_model)</span><span id="5a1f" class="nc lr it nw b gy oe ob l oc od">dense1 = tf.keras.layers.Dense(1024, activation='relu')(flat)</span><span id="e3c3" class="nc lr it nw b gy oe ob l oc od">dense2 = tf.keras.layers.Dense(512, activation='relu')(dense1)</span><span id="a0f6" class="nc lr it nw b gy oe ob l oc od">drop1 = tf.keras.layers.Dropout(0.3)(dense2)</span><span id="e4dc" class="nc lr it nw b gy oe ob l oc od">dense3 = tf.keras.layers.Dense(128, activation='relu')(drop1)</span><span id="f80c" class="nc lr it nw b gy oe ob l oc od">predictions = tf.keras.layers.Dense(10, activation='softmax')(dense3)</span><span id="63bd" class="nc lr it nw b gy oe ob l oc od">model = Model(inputs = inputs, outputs = predictions)</span><span id="4db3" class="nc lr it nw b gy oe ob l oc od">model.compile(optimizer=Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])</span></pre><p id="ef00" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">另一种写代码的方式。</p><pre class="kj kk kl km gt nv nw nx ny aw nz bi"><span id="72ad" class="nc lr it nw b gy oa ob l oc od">K.clear_session()</span><span id="fef2" class="nc lr it nw b gy oe ob l oc od">base_model = tf.keras.applications.InceptionV3(include_top=False, weights='imagenet', input_shape=(128, 128, 3), classes=train_labels.shape[1])</span><span id="809c" class="nc lr it nw b gy oe ob l oc od">base_model.trainable = False</span><span id="ea48" class="nc lr it nw b gy oe ob l oc od">model = tf.keras.Sequential([<br/>tf.keras.layers.InputLayer(input_shape=(32, 32, 3)),<br/>tf.keras.layers.UpSampling2D(size = (2,2)),<br/>tf.keras.layers.UpSampling2D(size = (2,2)),<br/>base_model,<br/>tf.keras.layers.Flatten(),<br/>tf.keras.layers.Dense(1024, activation='relu'),<br/>tf.keras.layers.Dense(512, activation='relu'),<br/>tf.keras.layers.Dropout(0.3),<br/>tf.keras.layers.Dense(128, activation='relu'),<br/>tf.keras.layers.Dense(10, activation='softmax')])</span><span id="976f" class="nc lr it nw b gy oe ob l oc od">model.compile(optimizer=Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ol"><img src="../Images/2cb3a224f0bdb86301b1ffed22c300f2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1022/format:webp/1*OwIbSM9l4rRzFflpxvBkEg.png"/></div></figure><h1 id="8182" class="lq lr it bd ls lt lu lv lw lx ly lz ma jz mb ka mc kc md kd me kf mf kg mg mh bi translated">结论</h1><p id="4aae" class="pw-post-body-paragraph ku kv it kw b kx mi ju kz la mj jx lc ld mk lf lg lh ml lj lk ll mm ln lo lp im bi translated">迁移学习的出现已经成为人工智能在我们今天看到的许多行业中成功实施的关键驱动因素。随着我们的深度学习模型以指数级增长，变得更加复杂，渴望不断扩展的数据量，这种模型所需的计算资源也在增加。迁移学习提供了人工智能社区来利用(即stack)这些复杂的模型，而不需要拥有大量金融资源的实体所积累的计算资源。</p><p id="49b9" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">感谢您的时间，我欢迎任何反馈。</p><p id="3e4a" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">这里有一个github，它是迁移学习环境中经常使用的各种开源模型(<a class="ae nb" href="https://github.com/BVLC/caffe/wiki/Model-Zoo" rel="noopener ugc nofollow" target="_blank">链接</a>)。</p></div></div>    
</body>
</html>