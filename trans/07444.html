<html>
<head>
<title>Introducing Packed BERT for 2x Training Speed-up in Natural Language Processing</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">引入压缩BERT，将自然语言处理的训练速度提高2倍</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/introducing-packed-bert-for-2x-faster-training-in-natural-language-processing-eadb749962b1?source=collection_archive---------11-----------------------#2021-07-07">https://towardsdatascience.com/introducing-packed-bert-for-2x-faster-training-in-natural-language-processing-eadb749962b1?source=collection_archive---------11-----------------------#2021-07-07</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="c616" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsdatascience.com/tagged/thoughts-and-theory" rel="noopener" target="_blank">思想和理论</a></h2><div class=""/><div class=""><h2 id="5a9b" class="pw-subtitle-paragraph jw iz iq bd b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn dk translated"><em class="ko">用于更高效训练的新BERT打包算法</em></h2></div><p id="447f" class="pw-post-body-paragraph kp kq iq kr b ks kt ka ku kv kw kd kx ky kz la lb lc ld le lf lg lh li lj lk ij bi translated">作者:<a class="ae ll" href="https://medium.com/@dr.mario/about" rel="noopener">马里奥·迈克尔·克雷尔博士</a>和<a class="ae ll" href="https://medium.com/@matej.kosec/about" rel="noopener">马特伊·科塞克</a></p><figure class="ln lo lp lq gt lr gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi lm"><img src="../Images/193e0208146d820bd70c52de8238030b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Mj8FHQ5tVXFEPnv5ab__vg.png"/></div></div><p class="ly lz gj gh gi ma mb bd b be z dk translated">图片作者。</p></figure><p id="6231" class="pw-post-body-paragraph kp kq iq kr b ks kt ka ku kv kw kd kx ky kz la lb lc ld le lf lg lh li lj lk ij bi translated">通过使用一种新的打包算法，我们在训练BERT-Large时将自然语言处理的速度提高了2倍以上。我们的新打包技术消除了填充，从而显著提高了计算效率。</p><p id="5e64" class="pw-post-body-paragraph kp kq iq kr b ks kt ka ku kv kw kd kx ky kz la lb lc ld le lf lg lh li lj lk ij bi translated">我们怀疑这也可以应用于基因组学和蛋白质折叠模型以及其他具有偏斜长度分布的模型，从而在不同的行业和应用中产生更广泛的影响。</p><p id="5964" class="pw-post-body-paragraph kp kq iq kr b ks kt ka ku kv kw kd kx ky kz la lb lc ld le lf lg lh li lj lk ij bi translated">我们在一篇新论文[1]中介绍了Graphcore的高效非负最小二乘直方图打包算法(或NNLSHP)以及我们应用于打包序列的BERT算法。</p><h1 id="6786" class="mc md iq bd me mf mg mh mi mj mk ml mm kf mn kg mo ki mp kj mq kl mr km ms mt bi translated">由于序列填充造成的NLP中的计算浪费</h1><p id="2e39" class="pw-post-body-paragraph kp kq iq kr b ks mu ka ku kv mv kd kx ky mw la lb lc mx le lf lg my li lj lk ij bi translated">在我们最近向ml perf提交的<a class="ae ll" href="https://www.graphcore.ai/posts/raising-the-bar-graphcores-first-mlperf-results" rel="noopener ugc nofollow" target="_blank">基准测试中，我们开始研究优化BERT训练的新方法。我们的目标是开发实用的优化方法，以便在实际应用中轻松采用。BERT作为这些优化的重点模型之一是一个自然的选择，因为它广泛应用于工业和我们的许多客户。</a></p><p id="efa5" class="pw-post-body-paragraph kp kq iq kr b ks kt ka ku kv kw kd kx ky kz la lb lc ld le lf lg lh li lj lk ij bi translated">我们惊讶地发现，在我们自己的使用维基百科数据集的BERT-Large训练应用程序中，数据集中50%的标记是填充的——这导致了大量的计算浪费。</p><p id="a9d1" class="pw-post-body-paragraph kp kq iq kr b ks kt ka ku kv kw kd kx ky kz la lb lc ld le lf lg lh li lj lk ij bi translated">填充序列以使它们长度相等是GPU常用的方法，但我们认为值得尝试一种不同的方法。</p><p id="bcc8" class="pw-post-body-paragraph kp kq iq kr b ks kt ka ku kv kw kd kx ky kz la lb lc ld le lf lg lh li lj lk ij bi translated">序列的长度变化很大，原因有两个:</p><ol class=""><li id="77ec" class="mz na iq kr b ks kt kv kw ky nb lc nc lg nd lk ne nf ng nh bi translated">维基百科的基础数据显示了文档长度的巨大差异</li><li id="5003" class="mz na iq kr b ks ni kv nj ky nk lc nl lg nm lk ne nf ng nh bi translated">BERT预处理本身随机减小被组合以生成训练序列的提取文档的大小</li></ol><p id="9111" class="pw-post-body-paragraph kp kq iq kr b ks kt ka ku kv kw kd kx ky kz la lb lc ld le lf lg lh li lj lk ij bi translated">将长度填充到最大长度512会导致所有标记的50%是填充标记。用实际数据替换50%的填充数据可以在相同的计算量下多处理50%的数据，从而在最佳条件下实现2倍的速度提升。</p><figure class="ln lo lp lq gt lr gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi nn"><img src="../Images/7098f5c783af2e39bb1842e8bca58097.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9FtehUu_Chxn3o5s0O6l4w.png"/></div></div><p class="ly lz gj gh gi ma mb bd b be z dk translated">图1:维基百科数据集分布。图片作者。</p></figure><p id="646c" class="pw-post-body-paragraph kp kq iq kr b ks kt ka ku kv kw kd kx ky kz la lb lc ld le lf lg lh li lj lk ij bi translated">这是维基百科特有的吗？号码</p><p id="82b0" class="pw-post-body-paragraph kp kq iq kr b ks kt ka ku kv kw kd kx ky kz la lb lc ld le lf lg lh li lj lk ij bi translated">好吧，那它是特定于语言的吗？号码</p><p id="6914" class="pw-post-body-paragraph kp kq iq kr b ks kt ka ku kv kw kd kx ky kz la lb lc ld le lf lg lh li lj lk ij bi translated">事实上，偏斜的长度分布无处不在:在语言、基因组学和蛋白质折叠中。图2和图3显示了SQuAD 1.1数据集和GLUE数据集的分布。</p><figure class="ln lo lp lq gt lr gh gi paragraph-image"><div class="gh gi no"><img src="../Images/59b9390c191062c0a010879d03226285.png" data-original-src="https://miro.medium.com/v2/resize:fit:1130/format:webp/1*M3LFL3s9hpdFDRZ7q4qRPw.png"/></div><p class="ly lz gj gh gi ma mb bd b be z dk translated">图2:最大序列长度为384的SQuAD 1.1 BERT预训练数据集序列长度直方图。图片作者。</p></figure><figure class="ln lo lp lq gt lr gh gi paragraph-image"><div class="gh gi np"><img src="../Images/95072dddd4b859a2ecb9b08271a00bd8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1088/format:webp/1*8YCigtZg9nyGve5hwmW-uw.png"/></div><p class="ly lz gj gh gi ma mb bd b be z dk translated">图3:最大序列长度为128的粘合数据集序列长度直方图。图片作者。</p></figure><p id="1299" class="pw-post-body-paragraph kp kq iq kr b ks kt ka ku kv kw kd kx ky kz la lb lc ld le lf lg lh li lj lk ij bi translated">我们如何在避免计算浪费的同时处理不同的长度？</p><p id="576f" class="pw-post-body-paragraph kp kq iq kr b ks kt ka ku kv kw kd kx ky kz la lb lc ld le lf lg lh li lj lk ij bi translated">当前的方法需要针对不同长度的不同计算内核，或者工程师需要通过编程移除填充，然后针对每个注意块和损失计算重复地添加回填充。通过放大代码并使其更复杂来节省计算并不吸引人，所以我们寻找更好的东西。难道我们不能把多个序列放在一个最大长度的包里一起处理吗？事实证明，我们可以！</p><p id="0662" class="pw-post-body-paragraph kp kq iq kr b ks kt ka ku kv kw kd kx ky kz la lb lc ld le lf lg lh li lj lk ij bi translated">这种方法需要三个关键因素:</p><ol class=""><li id="0769" class="mz na iq kr b ks kt kv kw ky nb lc nc lg nd lk ne nf ng nh bi translated">一种有效的算法来决定将哪些样本放在一起以具有尽可能少的剩余填充</li><li id="70bb" class="mz na iq kr b ks ni kv nj ky nk lc nl lg nm lk ne nf ng nh bi translated">调整BERT模型以处理包装而不是序列</li><li id="c0cc" class="mz na iq kr b ks ni kv nj ky nk lc nl lg nm lk ne nf ng nh bi translated">以及调整超参数</li></ol><h1 id="79da" class="mc md iq bd me mf mg mh mi mj mk ml mm kf mn kg mo ki mp kj mq kl mr km ms mt bi translated">包装</h1><p id="bf84" class="pw-post-body-paragraph kp kq iq kr b ks mu ka ku kv mv kd kx ky mw la lb lc mx le lf lg my li lj lk ij bi translated">起初，你似乎不太可能非常有效地打包像维基百科这样的大型数据集。这个问题通常被称为装箱问题。即使打包限制为三个或更少的序列，产生的问题仍然是强NP完全的，缺乏有效的算法解决方案。现有的启发式打包算法没有前途，因为它们的复杂性至少为<em class="nq"> O </em> ( <em class="nq"> n log </em> ( <em class="nq"> n </em>))，其中<em class="nq"> n </em>是序列的数量(维基百科的大约16M)。我们感兴趣的是可以扩展到数百万个序列的方法。</p><p id="e3ae" class="pw-post-body-paragraph kp kq iq kr b ks kt ka ku kv kw kd kx ky kz la lb lc ld le lf lg lh li lj lk ij bi translated">两个技巧帮助我们大幅降低了复杂性:</p><ol class=""><li id="68a8" class="mz na iq kr b ks kt kv kw ky nb lc nc lg nd lk ne nf ng nh bi translated">将一个包中的序列数量限制为三个(对于我们的第一个解决方案)</li><li id="f3f0" class="mz na iq kr b ks ni kv nj ky nk lc nl lg nm lk ne nf ng nh bi translated">仅在序列长度的直方图上操作，每个出现的长度有一个仓</li></ol><p id="992d" class="pw-post-body-paragraph kp kq iq kr b ks kt ka ku kv kw kd kx ky kz la lb lc ld le lf lg lh li lj lk ij bi translated">我们的最大序列长度是512。因此，移动到直方图将维度和复杂性从1600万个序列减少到512个长度计数。允许一个包中最多有三个序列，将允许的长度组合数量减少到22K。这已经包括了要求序列在包中按长度排序的技巧。那么为什么不试试4个序列呢？这将组合的数量从22K增加到940K，这对我们的第一个建模方法来说太多了。此外，深度3已经实现了非常高的包装效率。</p><p id="803e" class="pw-post-body-paragraph kp kq iq kr b ks kt ka ku kv kw kd kx ky kz la lb lc ld le lf lg lh li lj lk ij bi translated">最初，我们认为在一个包中使用三个以上的序列会增加计算开销并影响训练期间的收敛行为。然而，为了支持推理等需要更快、实时打包的应用，我们开发了高效的非负最小二乘直方图打包(NNLSHP)算法。</p><h2 id="87d4" class="nr md iq bd me ns nt dn mi nu nv dp mm ky nw nx mo lc ny nz mq lg oa ob ms iw bi translated">非负最小二乘直方图打包</h2><p id="9058" class="pw-post-body-paragraph kp kq iq kr b ks mu ka ku kv mv kd kx ky mw la lb lc mx le lf lg my li lj lk ij bi translated">装箱经常被公式化为一个数学优化问题。然而，对于1600万(或更多)的序列，这是不实际的。光是问题变量就超过了大多数机器的内存。基于直方图的方法的数学程序非常简洁。为了简单起见，我们决定使用直方图矢量<em class="nq"> b </em>的最小二乘法(<em class="nq"> Ax=b </em>)。我们扩展了它，要求策略向量<em class="nq"> x </em>为非负，并增加权重以允许少量填充。</p><p id="112b" class="pw-post-body-paragraph kp kq iq kr b ks kt ka ku kv kw kd kx ky kz la lb lc ld le lf lg lh li lj lk ij bi translated">棘手的部分是战略矩阵。每一列的最大和为3，并编码哪些序列被打包在一起以精确匹配所需的总长度；我们的情况是512。这些行对每个可能的组合进行编码，以达到总长度。策略向量<em class="nq"> x </em>就是我们要找的，它描述了我们选择20k个组合的频率。有趣的是，最后只选出了600个左右的组合。为了得到一个精确的解，在<em class="nq"> x </em>中的策略计数必须是正整数，但是我们意识到一个近似舍入的解，只要有非负的<em class="nq"> x </em>就足够了。对于近似解，可以使用一个简单的开箱即用的求解器在30秒内得到结果。</p><figure class="ln lo lp lq gt lr gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi oc"><img src="../Images/04895d9ca51ae4ebdc279e6d54a3a793.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RJceuYZZJYwz1HNc5MUrug.png"/></div></div><p class="ly lz gj gh gi ma mb bd b be z dk translated">图4:序列长度8和包装深度3的策略矩阵示例。行代表打包在一起的长度为1-8的序列，列代表一个包中所有可能的长度组合，没有特定的顺序。图片作者。</p></figure><p id="f6a8" class="pw-post-body-paragraph kp kq iq kr b ks kt ka ku kv kw kd kx ky kz la lb lc ld le lf lg lh li lj lk ij bi translated">最后，我们必须修复一些没有被分配策略的样本，但这些样本是最小的。我们还开发了一个变量求解器，它强制每个序列都被打包，可能带有填充，并且具有依赖于填充的权重。花费的时间长得多，解决方案也好不到哪里去。</p><h2 id="95f9" class="nr md iq bd me ns nt dn mi nu nv dp mm ky nw nx mo lc ny nz mq lg oa ob ms iw bi translated">最短包装优先直方图包装</h2><p id="26d1" class="pw-post-body-paragraph kp kq iq kr b ks mu ka ku kv mv kd kx ky mw la lb lc mx le lf lg my li lj lk ij bi translated">NNLSHP为我们提供了充分的包装方法。然而，我们想知道我们是否可以从理论上得到一个更快的在线方法，并消除只将3个序列放在一起的限制。</p><p id="37df" class="pw-post-body-paragraph kp kq iq kr b ks kt ka ku kv kw kd kx ky kz la lb lc ld le lf lg lh li lj lk ij bi translated">因此，我们从现有的打包算法中获得了一些灵感，但仍然专注于直方图。</p><p id="196a" class="pw-post-body-paragraph kp kq iq kr b ks kt ka ku kv kw kd kx ky kz la lb lc ld le lf lg lh li lj lk ij bi translated">我们的第一个算法，最短打包优先直方图打包(SPFHP)有四个组成部分:</p><ol class=""><li id="bf32" class="mz na iq kr b ks kt kv kw ky nb lc nc lg nd lk ne nf ng nh bi translated">从最长序列到最短序列对直方图的计数进行操作</li><li id="d648" class="mz na iq kr b ks ni kv nj ky nk lc nl lg nm lk ne nf ng nh bi translated">如果当前序列长度不适合任何包，则开始一组新的包</li><li id="580b" class="mz na iq kr b ks ni kv nj ky nk lc nl lg nm lk ne nf ng nh bi translated">如果有多个匹配，取序列长度之和最短的包装，分别修改计数</li><li id="a17a" class="mz na iq kr b ks ni kv nj ky nk lc nl lg nm lk ne nf ng nh bi translated">再次检查剩余计数是否合适</li></ol><p id="d108" class="pw-post-body-paragraph kp kq iq kr b ks kt ka ku kv kw kd kx ky kz la lb lc ld le lf lg lh li lj lk ij bi translated">这种方法最容易实现，只需要0.02秒。</p><p id="8d98" class="pw-post-body-paragraph kp kq iq kr b ks kt ka ku kv kw kd kx ky kz la lb lc ld le lf lg lh li lj lk ij bi translated">一种变型是取序列长度的最大和，而不是最短和分裂计数，以获得更完美的拟合。总的来说，这并没有改变效率，但是增加了代码的复杂性。</p><figure class="ln lo lp lq gt lr"><div class="bz fp l di"><div class="od oe l"/></div><p class="ly lz gj gh gi ma mb bd b be z dk translated">最短包装优先直方图包装的工作原理。作者制作的动画。</p></figure><h2 id="6024" class="nr md iq bd me ns nt dn mi nu nv dp mm ky nw nx mo lc ny nz mq lg oa ob ms iw bi translated">维基百科，1.1小队，胶水包装结果</h2><p id="2808" class="pw-post-body-paragraph kp kq iq kr b ks mu ka ku kv mv kd kx ky mw la lb lc mx le lf lg my li lj lk ij bi translated">表1、2和3显示了我们提出的两种算法的装箱结果。<strong class="kr ja">打包深度</strong>描述了打包序列的最大数量。封装深度1是基线BERT实现。当没有设定限制时，最大出现填料深度用一个附加的“max”表示。<strong class="kr ja">包数</strong>描述了新打包数据集的长度。<strong class="kr ja">效率</strong>是打包数据集中真实令牌的百分比。<strong class="kr ja">填充系数</strong>描述了相对于填充深度1的潜在加速。</p><p id="3dcd" class="pw-post-body-paragraph kp kq iq kr b ks kt ka ku kv kw kd kx ky kz la lb lc ld le lf lg lh li lj lk ij bi translated">我们有四个主要观察结果:</p><ol class=""><li id="b660" class="mz na iq kr b ks kt kv kw ky nb lc nc lg nd lk ne nf ng nh bi translated">分布越偏，包装的好处就越大。</li><li id="9e84" class="mz na iq kr b ks ni kv nj ky nk lc nl lg nm lk ne nf ng nh bi translated">所有数据集都受益于打包。有些甚至超过2倍。</li><li id="9ba7" class="mz na iq kr b ks ni kv nj ky nk lc nl lg nm lk ne nf ng nh bi translated">当填充深度不受限制时，SPFHP效率更高。</li><li id="4943" class="mz na iq kr b ks ni kv nj ky nk lc nl lg nm lk ne nf ng nh bi translated">对于最多3个打包序列，NNLSHP越复杂，效率越高(99.75对89.44)。</li></ol><figure class="ln lo lp lq gt lr gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi of"><img src="../Images/2a08b30e60027c510b7c3b596f1ea764.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9PyECUvxhs9uY4JSxX3Xog.png"/></div></div><p class="ly lz gj gh gi ma mb bd b be z dk translated">表1:维基百科上推荐的打包算法(SPFHP和NNLSHP)的关键性能结果。图片作者。</p></figure><figure class="ln lo lp lq gt lr gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi og"><img src="../Images/85787473a04dbd9f288b254c971624b9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8Q4qLbFUTV3BIxdODZjjwQ.png"/></div></div><p class="ly lz gj gh gi ma mb bd b be z dk translated">表2:为SQUaD 1.1 BERT预训练提出的打包算法的性能结果。图片作者。</p></figure><figure class="ln lo lp lq gt lr gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi oh"><img src="../Images/c6d8bd1ca791df069993a47700f9218c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ccxGKUWzV_tvDrtkNNnxag.png"/></div></div><p class="ly lz gj gh gi ma mb bd b be z dk translated">表3:为GLUE数据集提出的打包算法的性能结果。仅显示基线和SPFHP包装结果，不限制包装深度。图片作者。</p></figure><h1 id="3cbd" class="mc md iq bd me mf mg mh mi mj mk ml mm kf mn kg mo ki mp kj mq kl mr km ms mt bi translated">伯特处理调整</h1><p id="bda2" class="pw-post-body-paragraph kp kq iq kr b ks mu ka ku kv mv kd kx ky mw la lb lc mx le lf lg my li lj lk ij bi translated">关于BERT体系结构的有趣之处在于，大多数处理都发生在令牌级，这意味着它不会干扰我们的打包。只有四个部分需要调整:注意力屏蔽、MLM损失、NSP损失和准确度。</p><p id="318b" class="pw-post-body-paragraph kp kq iq kr b ks kt ka ku kv kw kd kx ky kz la lb lc ld le lf lg lh li lj lk ij bi translated">这四种处理不同数量序列的方法的关键是向量化和使用可以连接的最大数量的序列。为了引起注意，我们已经有了一个遮罩来处理填充。从下面的TensorFlow伪代码中可以看出，将此扩展到多个序列非常简单。这个概念是，我们确保注意力被限制在单独的序列上，不能超出这个范围。</p><figure class="ln lo lp lq gt lr"><div class="bz fp l di"><div class="oi oe l"/></div><p class="ly lz gj gh gi ma mb bd b be z dk translated">注意掩码代码示例。</p></figure><figure class="ln lo lp lq gt lr gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/e56c9b7aa1dd4295c1c33e5326bb1481.png" data-original-src="https://miro.medium.com/v2/resize:fit:600/format:webp/1*ekCYadoMaGGP6-DoDv_1TQ.png"/></div><p class="ly lz gj gh gi ma mb bd b be z dk translated">图5:示例0-1掩码</p></figure><p id="239d" class="pw-post-body-paragraph kp kq iq kr b ks kt ka ku kv kw kd kx ky kz la lb lc ld le lf lg lh li lj lk ij bi translated">对于损耗计算，原则上我们拆开序列并计算单独的损耗，最终获得序列(而不是包)的平均损耗。</p><p id="fe32" class="pw-post-body-paragraph kp kq iq kr b ks kt ka ku kv kw kd kx ky kz la lb lc ld le lf lg lh li lj lk ij bi translated">对于MLM损失，代码如下所示:</p><figure class="ln lo lp lq gt lr"><div class="bz fp l di"><div class="oi oe l"/></div><p class="ly lz gj gh gi ma mb bd b be z dk translated">损失计算代码示例。</p></figure><p id="9753" class="pw-post-body-paragraph kp kq iq kr b ks kt ka ku kv kw kd kx ky kz la lb lc ld le lf lg lh li lj lk ij bi translated">对于NSP损耗和精确度，原理是相同的。在我们的公开示例中，您可以找到我们内部的<a class="ae ll" href="https://docs.graphcore.ai/projects/popart-user-guide/en/latest/intro.html" rel="noopener ugc nofollow" target="_blank"> PopART框架</a>的相应代码。</p><h2 id="4933" class="nr md iq bd me ns nt dn mi nu nv dp mm ky nw nx mo lc ny nz mq lg oa ob ms iw bi translated">维基百科开销和加速估计</h2><p id="5b47" class="pw-post-body-paragraph kp kq iq kr b ks mu ka ku kv mv kd kx ky mw la lb lc mx le lf lg my li lj lk ij bi translated">对于我们对BERT的修改，我们有两个问题:</p><ol class=""><li id="de18" class="mz na iq kr b ks kt kv kw ky nb lc nc lg nd lk ne nf ng nh bi translated">它会带来多少开销？</li><li id="9818" class="mz na iq kr b ks ni kv nj ky nk lc nl lg nm lk ne nf ng nh bi translated">多少开销取决于放在一个包中的最大序列数？</li></ol><p id="4dae" class="pw-post-body-paragraph kp kq iq kr b ks kt ka ku kv kw kd kx ky kz la lb lc ld le lf lg lh li lj lk ij bi translated">由于BERT中的数据准备可能很麻烦，我们使用了一种快捷方式，针对多种不同的填充深度编译了代码，并比较了各自的(测量)周期。结果显示在表4中。使用<strong class="kr ja">开销</strong>，我们表示由于模型的改变而导致的吞吐量减少的百分比，以支持打包(例如用于引起注意的掩蔽方案和改变的损失计算)。<strong class="kr ja">实现的加速</strong>是由于打包导致的加速<strong class="kr ja">打包因子</strong>和由于<strong class="kr ja">开销</strong>导致的吞吐量下降的组合。</p><figure class="ln lo lp lq gt lr gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi ok"><img src="../Images/fab49da33150d1719068519c36af7b46.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LHtJrk3AZylZwMjVwWw4iA.png"/></div></div><p class="ly lz gj gh gi ma mb bd b be z dk translated">表4:维基百科上建议的打包算法(SPFHP和NNLSHP)的估计加速比较。图片作者。</p></figure><p id="fb93" class="pw-post-body-paragraph kp kq iq kr b ks kt ka ku kv kw kd kx ky kz la lb lc ld le lf lg lh li lj lk ij bi translated">多亏了矢量化技术，开销出奇地小，而且将许多序列打包在一起也没有什么坏处。</p><h1 id="3952" class="mc md iq bd me mf mg mh mi mj mk ml mm kf mn kg mo ki mp kj mq kl mr km ms mt bi translated">超参数调整</h1><p id="293d" class="pw-post-body-paragraph kp kq iq kr b ks mu ka ku kv mv kd kx ky mw la lb lc mx le lf lg my li lj lk ij bi translated">有了包装，我们的有效批量(平均)翻了一番。这意味着我们需要调整训练超参数。一个简单的技巧是将梯度累积计数减半，以保持与训练前相同的有效平均批量。通过使用带有预训练检查点的基准设置，我们可以看到精度曲线完全匹配。</p><figure class="ln lo lp lq gt lr gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi ol"><img src="../Images/c3f8b5366562fa99ee10c2297b2ca0c3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*o_a_FcKc4VLW8UXwDx-Nnw.png"/></div></div><p class="ly lz gj gh gi ma mb bd b be z dk translated">图6:包装和非包装处理的学习曲线与<strong class="bd om">包装方法的减少批量</strong>的比较。作者图片。</p></figure><p id="ecd6" class="pw-post-body-paragraph kp kq iq kr b ks kt ka ku kv kw kd kx ky kz la lb lc ld le lf lg lh li lj lk ij bi translated">准确性匹配:MLM训练损失一开始可能略有不同，但很快赶上。这种最初的差异可能来自于注意力层的轻微调整，这种调整可能在先前的训练中偏向于短序列。</p><p id="3e92" class="pw-post-body-paragraph kp kq iq kr b ks kt ka ku kv kw kd kx ky kz la lb lc ld le lf lg lh li lj lk ij bi translated">为了避免减速，有时保持原来的批量大小不变，并将超参数调整到增加的有效批量大小(加倍)会有所帮助。要考虑的主要超参数是β参数和学习率。一种常见的方法是将批处理大小加倍，这在我们的例子中会降低性能。查看LAMB优化器的统计数据，我们可以证明将β参数提升到填充因子的幂对应于连续训练多个批次以保持动量和速度的可比性。</p><figure class="ln lo lp lq gt lr gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi on"><img src="../Images/195e9bcb8fd581e022700bccf2e42d09.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ANQRnItajIsY9MUZ8NBUAw.png"/></div></div><p class="ly lz gj gh gi ma mb bd b be z dk translated">图7:应用了<strong class="bd om">试探法</strong>的打包和解包处理的学习曲线比较。作者图片。</p></figure><p id="8e3c" class="pw-post-body-paragraph kp kq iq kr b ks kt ka ku kv kw kd kx ky kz la lb lc ld le lf lg lh li lj lk ij bi translated">我们的实验表明，将β取2的幂是一个很好的启发式方法。在这种情况下，不期望曲线匹配，因为增加批量大小通常会降低样本/时期意义上的收敛速度，直到达到目标准确度。</p><p id="c69c" class="pw-post-body-paragraph kp kq iq kr b ks kt ka ku kv kw kd kx ky kz la lb lc ld le lf lg lh li lj lk ij bi translated">现在的问题是，如果在实际场景中，我们真的得到预期的加速吗？</p><figure class="ln lo lp lq gt lr gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi oo"><img src="../Images/86221dd827f058153e09de267c33bfc7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HkLXFK0gRu9KPTnXYC82_A.png"/></div></div><p class="ly lz gj gh gi ma mb bd b be z dk translated">图8:在<strong class="bd om">优化设置</strong>中包装和非包装加工的学习曲线对比。作者图片。</p></figure><p id="875f" class="pw-post-body-paragraph kp kq iq kr b ks kt ka ku kv kw kd kx ky kz la lb lc ld le lf lg lh li lj lk ij bi translated">是的，我们有！我们获得了额外的加速，因为我们压缩了数据传输。</p><h1 id="4045" class="mc md iq bd me mf mg mh mi mj mk ml mm kf mn kg mo ki mp kj mq kl mr km ms mt bi translated">结论</h1><p id="e229" class="pw-post-body-paragraph kp kq iq kr b ks mu ka ku kv mv kd kx ky mw la lb lc mx le lf lg my li lj lk ij bi translated">将句子打包在一起可以节省计算工作量并保护环境。这项技术可以在任何框架中实现，包括PyTorch和TensorFlow。我们获得了明显的2倍加速，同时，我们扩展了打包算法的技术水平。</p><p id="c97d" class="pw-post-body-paragraph kp kq iq kr b ks kt ka ku kv kw kd kx ky kz la lb lc ld le lf lg lh li lj lk ij bi translated">我们感兴趣的其他应用是基因组学和蛋白质折叠，其中可以观察到类似的数据分布。视觉变形器也是一个有趣的领域，可以应用不同大小的打包图像。您认为哪些应用程序会运行良好？我们希望收到您的来信！</p><p id="0d3a" class="pw-post-body-paragraph kp kq iq kr b ks kt ka ku kv kw kd kx ky kz la lb lc ld le lf lg lh li lj lk ij bi translated"><a class="ae ll" href="https://arxiv.org/abs/2107.02027" rel="noopener ugc nofollow" target="_blank">看报纸</a></p><p id="644c" class="pw-post-body-paragraph kp kq iq kr b ks kt ka ku kv kw kd kx ky kz la lb lc ld le lf lg lh li lj lk ij bi translated"><a class="ae ll" href="https://github.com/graphcore/tutorials/tree/sdk-release-2.1/blogs_code/packedBERT" rel="noopener ugc nofollow" target="_blank">访问GitHub上的代码</a></p><h1 id="7126" class="mc md iq bd me mf mg mh mi mj mk ml mm kf mn kg mo ki mp kj mq kl mr km ms mt bi translated">谢谢你</h1><p id="dd58" class="pw-post-body-paragraph kp kq iq kr b ks mu ka ku kv mv kd kx ky mw la lb lc mx le lf lg my li lj lk ij bi translated">感谢我们在Graphcore应用工程团队的同事傅生和Mrinal Iyer对这项工作的贡献，感谢Graphcore研究团队的Douglas Orr提供的宝贵反馈。</p><h1 id="9c4f" class="mc md iq bd me mf mg mh mi mj mk ml mm kf mn kg mo ki mp kj mq kl mr km ms mt bi translated">参考</h1><p id="d77a" class="pw-post-body-paragraph kp kq iq kr b ks mu ka ku kv mv kd kx ky mw la lb lc mx le lf lg my li lj lk ij bi translated">[1] M. Kosec，S. Fu，M. M. Krell，<a class="ae ll" href="https://arxiv.org/pdf/2107.02027.pdf" rel="noopener ugc nofollow" target="_blank">打包:朝向2倍NLP BERT加速</a> (2021)，arXiv</p></div></div>    
</body>
</html>