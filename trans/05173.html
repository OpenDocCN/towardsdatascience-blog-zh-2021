<html>
<head>
<title>How do Decision Trees and Random Forests Work? 1a</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">决策树和随机森林是如何工作的？1a</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/how-do-decision-trees-and-random-forests-work-66a1094e6c5d?source=collection_archive---------29-----------------------#2021-05-06">https://towardsdatascience.com/how-do-decision-trees-and-random-forests-work-66a1094e6c5d?source=collection_archive---------29-----------------------#2021-05-06</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="18a6" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">决策树</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/b1fa0f4ebf516e5b7c7abd59326f7622.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qo2C5v5ZzipIibG8-x1k6Q.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">在<a class="ae kv" href="https://unsplash.com/s/photos/tree?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上<a class="ae kv" href="https://unsplash.com/@veeterzy?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">veeterzy</a>(<a class="kw kx ep" href="https://medium.com/u/569b8382d655?source=post_page-----66a1094e6c5d--------------------------------" rel="noopener" target="_blank">Vanja ter zic</a>)拍摄的照片</p></figure><p id="6afd" class="pw-post-body-paragraph ky kz iq la b lb lc jr ld le lf ju lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">决策树和随机森林是预测建模中两种常用的算法。在这篇文章中，我将讨论决策树背后的过程。我计划在第二部分继续讨论随机森林，然后比较两者。</p></div><div class="ab cl lu lv hu lw" role="separator"><span class="lx bw bk ly lz ma"/><span class="lx bw bk ly lz ma"/><span class="lx bw bk ly lz"/></div><div class="ij ik il im in"><p id="b9e7" class="pw-post-body-paragraph ky kz iq la b lb lc jr ld le lf ju lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">首先:决策树。决策树是根据出来的图形的形状来命名的。下图显示了一个决策树，用于确定哪些因素影响了泰坦尼克号灾难中的幸存者。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mb"><img src="../Images/223358eb3b5cd279a9a54c9b57a3ee11.png" data-original-src="https://miro.medium.com/v2/resize:fit:914/format:webp/1*3BkKVdBt-tnxKMa5NFajEg.jpeg"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者吉尔金——自己的作品，CC BY-SA 4.0，<a class="ae kv" href="https://commons.wikimedia.org/w/index.php?curid=90405437" rel="noopener ugc nofollow" target="_blank">https://commons.wikimedia.org/w/index.php?curid=90405437</a></p></figure><p id="0600" class="pw-post-body-paragraph ky kz iq la b lb lc jr ld le lf ju lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">在我们继续之前，我应该介绍一些术语。树中的每个分支点称为一个节点，代表一个包含起始数据集的部分或全部记录的数据集。按照“树”的主题，顶部(或起始)节点也称为根节点。这包含数据集的所有记录(行、个人，无论您想如何称呼它们)(或者至少是您想要包含的所有记录)。树从这个根开始生长，给我们更多的节点，直到它生成终端节点(即，那些没有被分割的节点)。这些终端节点被称为叶子。上面的树有四片叶子。每个都标有该节点的最终预测。</p><p id="710d" class="pw-post-body-paragraph ky kz iq la b lb lc jr ld le lf ju lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">有两种类型的决策树:分类和回归。分类树预测因变量的类别——是/否、苹果/橘子、死亡/幸存等。回归树预测数值变量的值，类似于线性回归。回归树需要注意的一点是，它们不能像线性回归那样在训练数据集范围之外进行外推。然而，与线性回归不同，回归树可以直接使用分类输入变量。</p><p id="253a" class="pw-post-body-paragraph ky kz iq la b lb lc jr ld le lf ju lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">虽然Titanic决策树显示了二叉分裂(每个非叶节点产生两个子节点)，但这不是一般的要求。根据决策树的不同，节点可能有三个甚至更多的子节点。在本文的其余部分，我将集中讨论分类决策树，但是回归树和分类树的基本思想是一样的。</p><p id="a9c5" class="pw-post-body-paragraph ky kz iq la b lb lc jr ld le lf ju lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">最后，我将提到这个讨论假设在r中使用rpart()函数。我听说Python不能直接处理类别变量，但是我对Python不太熟悉，尤其是对于数据分析。我相信基本理论是一样的，只是实现方式不同。</p></div><div class="ab cl lu lv hu lw" role="separator"><span class="lx bw bk ly lz ma"/><span class="lx bw bk ly lz ma"/><span class="lx bw bk ly lz"/></div><div class="ij ik il im in"><p id="c7df" class="pw-post-body-paragraph ky kz iq la b lb lc jr ld le lf ju lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">决策树是以迭代的方式创建的。首先，扫描变量以确定哪一个给出了最好的分割(稍后将详细介绍)，然后基于该确定将数据集分割成更小的子集。然后再次分析每个子集，创建新的子集，直到算法决定停止。这个决定部分由您为算法设置的参数控制。</p><p id="8dd4" class="pw-post-body-paragraph ky kz iq la b lb lc jr ld le lf ju lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">这种划分是基于对所讨论的数据集(或子集)的预测有多好。上面的巨大决策树最初基于性别创建了两个子集，这两个子集具有更好的预测价值。如果我们看看泰坦尼克号的例子，icyousee.org报告说总存活率是32%。看上面的决策树，我们看到73%的雌性存活了下来。总结整个树，我们可以生成以下规则:</p><ul class=""><li id="622a" class="mc md iq la b lb lc le lf lh me ll mf lp mg lt mh mi mj mk bi translated">如果乘客是女性，她很可能幸存(73%的几率)</li><li id="bd22" class="mc md iq la b lb ml le mm lh mn ll mo lp mp lt mh mi mj mk bi translated">如果乘客是男性，那么存活率取决于年龄和机上兄弟姐妹的数量。</li><li id="1262" class="mc md iq la b lb ml le mm lh mn ll mo lp mp lt mh mi mj mk bi translated">没有兄弟姐妹的年轻男孩很有可能存活下来(89%的几率)</li><li id="85a4" class="mc md iq la b lb ml le mm lh mn ll mo lp mp lt mh mi mj mk bi translated">大多数雄性都不走运</li></ul></div><div class="ab cl lu lv hu lw" role="separator"><span class="lx bw bk ly lz ma"/><span class="lx bw bk ly lz ma"/><span class="lx bw bk ly lz"/></div><div class="ij ik il im in"><p id="1c4f" class="pw-post-body-paragraph ky kz iq la b lb lc jr ld le lf ju lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">从现在开始，我将使用我创建的人工数据集，记录10名成年男性和10名成年女性的身高和体重。(身高是根据CDC研究的实际平均值和百分位数随机生成的——见下文。基于身高值加上身高和体重标准偏差生成体重。见最后的R代码。)</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mq"><img src="../Images/2ff784a6bc580feda20bc18d0caacf9d.png" data-original-src="https://miro.medium.com/v2/resize:fit:950/format:webp/1*kV5i87iiJAL6JLfboHasoA.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者创建的表格和数据</p></figure><p id="bdc3" class="pw-post-body-paragraph ky kz iq la b lb lc jr ld le lf ju lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">让我们看看我们是否可以使用这个数据集来预测一个个体是男性还是女性。如果我们随机选择一个人，我们有50%的机会得到一个男性或女性。有了一个像样的预测模型，我们应该可以做得更好。男性往往更高，更重，所以也许我们可以使用其中一个或两个变量。</p></div><div class="ab cl lu lv hu lw" role="separator"><span class="lx bw bk ly lz ma"/><span class="lx bw bk ly lz ma"/><span class="lx bw bk ly lz"/></div><div class="ij ik il im in"><p id="ff1b" class="pw-post-body-paragraph ky kz iq la b lb lc jr ld le lf ju lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">决策树使用一种叫做熵的东西来告诉我们我们的预测有多确定。在物理术语中，熵指的是系统中无序的数量。这里也是一样。如果熵为零，就没有无序。只有当我们完全确定从数据集中挑选某人时，我们知道会得到什么，这种情况才会发生。不要期望这在现实生活中发生——永远不要。然而，我们确实希望熵尽可能的低。存在两个类的数据集的熵计算如下:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mr"><img src="../Images/2c0e23c8ea6b80a6c34357f0465cea81.png" data-original-src="https://miro.medium.com/v2/resize:fit:826/format:webp/1*n04gFH2sW8LgVOsH7_iDEg.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者创建的所有方程式图像</p></figure><p id="a910" class="pw-post-body-paragraph ky kz iq la b lb lc jr ld le lf ju lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">其中:</p><ul class=""><li id="e82e" class="mc md iq la b lb lc le lf lh me ll mf lp mg lt mh mi mj mk bi translated">S =数据集的熵</li><li id="e7f1" class="mc md iq la b lb ml le mm lh mn ll mo lp mp lt mh mi mj mk bi translated">p₁ =个人属于1类的概率</li><li id="0484" class="mc md iq la b lb ml le mm lh mn ll mo lp mp lt mh mi mj mk bi translated">p₂ =个人属于类别2的概率</li></ul><p id="9958" class="pw-post-body-paragraph ky kz iq la b lb lc jr ld le lf ju lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">对于原始数据集，S = 1，这是该方程可实现的最大无序度。你自己试试吧。确保您使用的是以2为基数的对数。使用基数2并不重要，但这是获得最大值1的唯一方法。(注意:如果特定数据集只包含一个类，则不存在第二个术语。如果一个因变量有两个以上的可能值，就会增加额外的项。)</p><p id="5ef5" class="pw-post-body-paragraph ky kz iq la b lb lc jr ld le lf ju lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">在决策树中拆分数据集时，总熵是用子集熵的加权平均值计算的，如下所示:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ms"><img src="../Images/1a8e3afb8dc1235c9bda8464c3512ce1.png" data-original-src="https://miro.medium.com/v2/resize:fit:802/format:webp/1*hJJKI1IdLmpM_6NfGf_rug.png"/></div></figure><p id="5e0e" class="pw-post-body-paragraph ky kz iq la b lb lc jr ld le lf ju lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">其中:</p><ul class=""><li id="67e1" class="mc md iq la b lb lc le lf lh me ll mf lp mg lt mh mi mj mk bi translated">Sₜ =分裂后的总熵</li><li id="20d3" class="mc md iq la b lb ml le mm lh mn ll mo lp mp lt mh mi mj mk bi translated">Sₓ =子集x的熵(即，S₁ =子集1的熵)</li><li id="59d0" class="mc md iq la b lb ml le mm lh mn ll mo lp mp lt mh mi mj mk bi translated">fₓ =进入子集x的个体比例</li></ul><p id="7ee3" class="pw-post-body-paragraph ky kz iq la b lb lc jr ld le lf ju lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">任何点的总熵都是从所有当前叶节点计算的(即使它们以后可能会分裂)。例如，在早期的泰坦尼克树中，总熵在第一次(性别)分裂后有两个(<em class="mt"> f S </em>)项，在第二次(年龄)分裂后有三个项，在第三次(兄弟姐妹)分裂后有四个项。</p><p id="fbd1" class="pw-post-body-paragraph ky kz iq la b lb lc jr ld le lf ju lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">决策树算法将查看不同变量的不同值，以根据原始值的熵减少来确定哪个给出最佳分割。</p></div><div class="ab cl lu lv hu lw" role="separator"><span class="lx bw bk ly lz ma"/><span class="lx bw bk ly lz ma"/><span class="lx bw bk ly lz"/></div><div class="ij ik il im in"><p id="f04a" class="pw-post-body-paragraph ky kz iq la b lb lc jr ld le lf ju lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">让我们用身高/体重数据集来分析一下。先以体重为分界点。女性的平均体重是177磅，而男性的平均体重是201磅。让我们在两者的中点分开(189磅)。这将给我们两个子集:</p><ul class=""><li id="c63b" class="mc md iq la b lb lc le lf lh me ll mf lp mg lt mh mi mj mk bi translated">子集1 (&lt; 189磅，预测女性)有6名女性和3名男性(总共9名)</li><li id="0fdc" class="mc md iq la b lb ml le mm lh mn ll mo lp mp lt mh mi mj mk bi translated">子集2 (≥ 189磅，预测男性)有4名女性和7名男性(共11名)</li></ul><p id="4baa" class="pw-post-body-paragraph ky kz iq la b lb lc jr ld le lf ju lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">根据这些信息，我们可以计算两个子集的熵:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mu"><img src="../Images/06169678d2e587984a1988894c768ef2.png" data-original-src="https://miro.medium.com/v2/resize:fit:694/format:webp/1*AertUUXzm4X3HEMRdmSYYA.png"/></div></figure><p id="06c7" class="pw-post-body-paragraph ky kz iq la b lb lc jr ld le lf ju lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">和</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mv"><img src="../Images/c7e4b7bfc453df59b11876959adb24b8.png" data-original-src="https://miro.medium.com/v2/resize:fit:198/format:webp/1*IMCI4sXvFC1Ps9TlG6EZkQ.png"/></div></figure><p id="54d3" class="pw-post-body-paragraph ky kz iq la b lb lc jr ld le lf ju lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">亲爱的读者，我将把证实第二熵的任务留给你。为了计算分裂的总熵，我们使用等式2。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mw"><img src="../Images/01ce880ed3753e5e4bdeba1f03aaaeec.png" data-original-src="https://miro.medium.com/v2/resize:fit:646/format:webp/1*RJM98vGpzNBqpQadDD9i2Q.png"/></div></div></figure><p id="c6d6" class="pw-post-body-paragraph ky kz iq la b lb lc jr ld le lf ju lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">所以通过分解189磅的重量，我们得到了熵的轻微减少。决策树算法会检查许多值，看它们是否会给出更好的结果。结果是，如果我们把分裂值降到186磅，总熵会降到0.88。</p></div><div class="ab cl lu lv hu lw" role="separator"><span class="lx bw bk ly lz ma"/><span class="lx bw bk ly lz ma"/><span class="lx bw bk ly lz"/></div><div class="ij ik il im in"><p id="1ed8" class="pw-post-body-paragraph ky kz iq la b lb lc jr ld le lf ju lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">当然，我们也可以尝试基于身高的拆分。让我们从均值的中点开始。这将给我们一个66.2英寸的分割值。通过这种分割，我们得到以下两个子集:</p><ul class=""><li id="5c79" class="mc md iq la b lb lc le lf lh me ll mf lp mg lt mh mi mj mk bi translated">子集1 (&lt; 66.2英寸，预测为雌性)有9只雌性和2只雄性(总共11只)</li><li id="0ab9" class="mc md iq la b lb ml le mm lh mn ll mo lp mp lt mh mi mj mk bi translated">子集2 (≥ 66.2英寸，预测男性)有1名女性和8名男性(共9名)</li></ul><p id="b726" class="pw-post-body-paragraph ky kz iq la b lb lc jr ld le lf ju lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">有了这些信息，我们可以计算这种分裂的熵:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mx"><img src="../Images/f9c455c0e499841c470562f04b1a1f9f.png" data-original-src="https://miro.medium.com/v2/resize:fit:534/format:webp/1*7Jl8aReYX-_316PnaqjzXg.png"/></div></figure><p id="cdae" class="pw-post-body-paragraph ky kz iq la b lb lc jr ld le lf ju lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">这比使用权重进行分割要好得多。但是如果我们将分裂点提高到68.2英寸，我们可以做得更好，总熵为0.51。现在我们的决策树有了第一次分裂。接下来，该算法将检查这些子集中的每一个，以查看它们是否可以再次被分割，这次是基于权重。这样做可能会让我们得到更好的熵，但我们分裂得越多，我们就越有可能过度拟合数据。请记住，我在这里只采样了20个人，其中一些看起来像异常值(例如，最高的女性体重最轻)。对于包含许多变量的大型数据集，您可能会得到一个真正混乱的决策树。有几个参数可以用来限制这一点，但这超出了我想在这里讨论的范围。</p><p id="9e54" class="pw-post-body-paragraph ky kz iq la b lb lc jr ld le lf ju lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">在以前的帖子中，我谈到了使用rattle来帮助学习r。事实证明，Rattle包对决策树有很好的绘图功能。这是我通过Rattle的决策树算法运行数据集时得到的图像，使用标准参数(注意，它只在一次分割后就停止了):</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi my"><img src="../Images/a3aaa2a82c028b95fbb0801f561d35ab.png" data-original-src="https://miro.medium.com/v2/resize:fit:966/format:webp/1*V-BcXcVDFbAlRZZcJf0_MQ.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="3cb7" class="pw-post-body-paragraph ky kz iq la b lb lc jr ld le lf ju lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">这个情节有很多特点，我现在就来介绍一下。</p><ul class=""><li id="2feb" class="mc md iq la b lb lc le lf lh me ll mf lp mg lt mh mi mj mk bi translated">颜色对应预测的性别(绿色=女性，蓝色=男性)。</li><li id="1d21" class="mc md iq la b lb ml le mm lh mn ll mo lp mp lt mh mi mj mk bi translated">节点1(也称为“根节点”)具有50/50的性别比例，并且占观察值的100%。它的绿色意味着如果它必须从这组动物中预测性别，它会选择雌性。</li><li id="ee06" class="mc md iq la b lb ml le mm lh mn ll mo lp mp lt mh mi mj mk bi translated">在这个节点下面，我们看到第一次拆分。如果高度&lt; 68 inches, then we predict female. Otherwise, we predict male.</li><li id="2bca" class="mc md iq la b lb ml le mm lh mn ll mo lp mp lt mh mi mj mk bi translated">Node 2 accounts for 65% of the dataset and has a 77/23 split of females to males. Its green color means that everybody in this subset is predicted to be female</li><li id="ef70" class="mc md iq la b lb ml le mm lh mn ll mo lp mp lt mh mi mj mk bi translated">Node 3 accounts for 35% of the dataset and is 100% male, which is why it’s blue.</li></ul><p id="a447" class="pw-post-body-paragraph ky kz iq la b lb lc jr ld le lf ju lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">The biggest benefit of decision trees is the ease of understanding. They’re easy to read and it’s easy to see how the model made its prediction.</p><p id="83a4" class="pw-post-body-paragraph ky kz iq la b lb lc jr ld le lf ju lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">In part 2, we will investigate random forests, and see how they compare to decision trees.</p></div><div class="ab cl lu lv hu lw" role="separator"><span class="lx bw bk ly lz ma"/><span class="lx bw bk ly lz ma"/><span class="lx bw bk ly lz"/></div><div class="ij ik il im in"><h2 id="898a" class="mz na iq bd nb nc nd dn ne nf ng dp nh lh ni nj nk ll nl nm nn lp no np nq nr bi translated">Further Reading</h2><div class="ns nt gp gr nu nv"><a href="https://www.cdc.gov/nchs/fastats/body-measurements.htm" rel="noopener  ugc nofollow" target="_blank"><div class="nw ab fo"><div class="nx ab ny cl cj nz"><h2 class="bd ir gy z fp oa fr fs ob fu fw ip bi translated">FastStats</h2><div class="oc l"><h3 class="bd b gy z fp oa fr fs ob fu fw dk translated">Men: Height in inches: 69.0 Weight in pounds: 199.8 Waist circumference in inches: 40.5 Height in inches: 63.5 Weight…</h3></div><div class="od l"><p class="bd b dl z fp oa fr fs ob fu fw dk translated">www.cdc.gov</p></div></div><div class="oe l"><div class="of l og oh oi oe oj kp nv"/></div></div></a></div><div class="ns nt gp gr nu nv"><a href="http://www.icyousee.org/titanic.html" rel="noopener  ugc nofollow" target="_blank"><div class="nw ab fo"><div class="nx ab ny cl cj nz"><h2 class="bd ir gy z fp oa fr fs ob fu fw ip bi translated">Titanic: Demographics of the Passengers</h2><div class="oc l"><h3 class="bd b gy z fp oa fr fs ob fu fw dk translated">Demographics of the TITANIC Passengers: Deaths, Survivals, Nationality, and Lifeboat Occupancy I hesitated before…</h3></div><div class="od l"><p class="bd b dl z fp oa fr fs ob fu fw dk translated">www.icyousee.org</p></div></div><div class="oe l"><div class="ok l og oh oi oe oj kp nv"/></div></div></a></div><div class="ns nt gp gr nu nv"><a href="https://www.amazon.com/gp/product/1441998896/ref=as_li_tl?ie=UTF8&amp;camp=1789&amp;creative=9325&amp;creativeASIN=1441998896&amp;linkCode=as2&amp;tag=medium0074-20&amp;linkId=42b704775edff7eb2f7eef0d5185e66f" rel="noopener  ugc nofollow" target="_blank"><div class="nw ab fo"><div class="nx ab ny cl cj nz"><h2 class="bd ir gy z fp oa fr fs ob fu fw ip bi translated">Data Mining with Rattle and R: The Art of Excavating Data for Knowledge Discovery (Use R!)</h2><div class="oc l"><h3 class="bd b gy z fp oa fr fs ob fu fw dk translated">Amazon.com: Data Mining with Rattle and R: The Art of Excavating Data for Knowledge Discovery (Use R!) (8582569999992)…</h3></div><div class="od l"><p class="bd b dl z fp oa fr fs ob fu fw dk translated">www.amazon.com</p></div></div><div class="oe l"><div class="ol l og oh oi oe oj kp nv"/></div></div></a></div><p id="051f" class="pw-post-body-paragraph ky kz iq la b lb lc jr ld le lf ju lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">(Note that the Amazon link above is an affiliate link.)</p><div class="ns nt gp gr nu nv"><a rel="noopener follow" target="_blank" href="/use-rattle-to-help-you-learn-r-d495c0cc517f"><div class="nw ab fo"><div class="nx ab ny cl cj nz"><h2 class="bd ir gy z fp oa fr fs ob fu fw ip bi translated">Use Rattle to Help You Learn R</h2><div class="oc l"><h3 class="bd b gy z fp oa fr fs ob fu fw dk translated">A beginner’s guide</h3></div><div class="od l"><p class="bd b dl z fp oa fr fs ob fu fw dk translated">towardsdatascience.com</p></div></div><div class="oe l"><div class="om l og oh oi oe oj kp nv"/></div></div></a></div></div><div class="ab cl lu lv hu lw" role="separator"><span class="lx bw bk ly lz ma"/><span class="lx bw bk ly lz ma"/><span class="lx bw bk ly lz"/></div><div class="ij ik il im in"><h2 id="8ffd" class="mz na iq bd nb nc nd dn ne nf ng dp nh lh ni nj nk ll nl nm nn lp no np nq nr bi translated">R Code</h2><p id="00a9" class="pw-post-body-paragraph ky kz iq la b lb on jr ld le oo ju lg lh op lj lk ll oq ln lo lp or lr ls lt ij bi translated"><strong class="la ir">创建数据集</strong></p><pre class="kg kh ki kj gt os ot ou ov aw ow bi"><span id="7690" class="mz na iq ot b gy ox oy l oz pa"># Data from <a class="ae kv" href="https://www.cdc.gov/nchs/data/series/sr_03/sr03-046-508.pdf" rel="noopener ugc nofollow" target="_blank">https://www.cdc.gov/nchs/data/series/sr_03/sr03-046-508.pdf</a><br/># means are directly retrieved from report<br/># SDs are estimated from 15th and 85th percentiles<br/>library(dplyr)<br/>set.seed(1)</span><span id="0d8e" class="mz na iq ot b gy pb oy l oz pa"># Weight of Females over 20 - Table 4 - excludes pregnant females<br/>FWnum &lt;- 5386 # number of females in sample<br/>FWmean &lt;- 170.8 # mean weight of females, in pounds<br/>#15% = 126.9<br/>#85% = 216.4<br/>#diff / 2 = 44.75<br/>FWSD &lt;- 44 # estimated std dev, in pounds</span><span id="a6bc" class="mz na iq ot b gy pb oy l oz pa"># Weight of Males over 20 - Table 6<br/>MWnum &lt;- 5085 # number of males in sample<br/>MWmean &lt;- 199.8 # mean weight of males, in pounds<br/>#15% = 154.2<br/>#85% = 243.8<br/>#diff / 2 = 44.8<br/>MWSD &lt;- 44 # estimated std dev, in pounds</span><span id="0843" class="mz na iq ot b gy pb oy l oz pa"># Height of Females over 20 - Table 10<br/>FHnum &lt;- 5510 # number of females in sample<br/>FHmean &lt;- 63.5 # mean height of females over 20, in inches<br/>#15% = 60.6<br/>#85% = 66.3<br/>#diff / 2 = 2.85<br/>FHSD &lt;- 2.8 # estimated std dev, in pounds</span><span id="c90a" class="mz na iq ot b gy pb oy l oz pa"># Height of Males over 20 - Table 12<br/>MHnum &lt;- 5092 # number of females in sample<br/>MHmean &lt;- 69.0 # mean height of females over 20, in inches<br/>#15% = 66.0<br/>#85% = 72.0<br/>#diff / 2 = 3.0<br/>MHSD &lt;- 3 # estimated std dev, in pounds</span><span id="68cf" class="mz na iq ot b gy pb oy l oz pa"># create 10 normally distributed female heights<br/>FemaleHeight &lt;- round(rnorm(10, mean = FHmean, sd = FHSD), 1)</span><span id="ea64" class="mz na iq ot b gy pb oy l oz pa"># Calculate weight based on comparison of height to mean height<br/>FemWCorrel &lt;- FemaleHeight/FHmean * FWmean <br/># throw in some random deviation based on weight SD<br/>FemWAdj &lt;- rnorm(10, sd = FWSD/2)<br/>FemaleWeight &lt;- round(FemWCorrel + FemWAdj, 0)<br/>F &lt;- data.frame(Height = FemaleHeight, <br/>                Weight = FemaleWeight, <br/>                Gender = "F")</span><span id="6693" class="mz na iq ot b gy pb oy l oz pa"># create 10 normally distributed male heights<br/>MaleHeight &lt;- round(rnorm(10, mean = MHmean, sd = MHSD), 1)<br/># Calculate weight based on comparison of height to mean height<br/>MaleWCorrel &lt;- MaleHeight/MHmean * MWmean <br/># throw in some random deviation based on weight SD<br/>MaleWAdj &lt;- rnorm(10, sd = MWSD/2)<br/>MaleWeight &lt;- round((MaleWCorrel + MaleWAdj), 0)<br/>M &lt;- data.frame(Height = MaleHeight, <br/>                Weight = MaleWeight, <br/>                Gender = "M")</span><span id="f4b8" class="mz na iq ot b gy pb oy l oz pa">df &lt;- rbind(F, M)</span></pre></div></div>    
</body>
</html>