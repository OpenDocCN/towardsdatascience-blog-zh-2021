<html>
<head>
<title>Visualizing Neural Networks’ Decision-Making Process Part 2</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">可视化神经网络的决策过程第二部分</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/visualizing-neural-networks-decision-making-process-part-2-layer-wise-relevance-propagation-50cd913cc1c7?source=collection_archive---------35-----------------------#2021-10-19">https://towardsdatascience.com/visualizing-neural-networks-decision-making-process-part-2-layer-wise-relevance-propagation-50cd913cc1c7?source=collection_archive---------35-----------------------#2021-10-19</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/></div><div class="ab cl jn jo hu jp" role="separator"><span class="jq bw bk jr js jt"/><span class="jq bw bk jr js jt"/><span class="jq bw bk jr js"/></div><div class="ij ik il im in"><h2 id="4a50" class="ju jv iq bd jw jx jy dn jz ka kb dp kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated">逐层相关性传播</h2><p id="1a78" class="pw-post-body-paragraph kq kr iq ks b kt ku kv kw kx ky kz la kd lb lc ld kh le lf lg kl lh li lj lk ij bi translated">解释神经网络(NNs)预测是一个正在进行的研究领域。由于他们的黑箱性质，我们通常对他们如何决策知之甚少。然而，理解神经网络非常重要，特别是在现实世界的应用中，如自动驾驶汽车，其中确保模型的可靠性和鲁棒性至关重要。</p><p id="9c58" class="pw-post-body-paragraph kq kr iq ks b kt ll kv kw kx lm kz la kd ln lc ld kh lo lf lg kl lp li lj lk ij bi translated">在我的<a class="ae lq" rel="noopener" target="_blank" href="/visualizing-neural-networks-decision-making-process-part-1-class-activation-maps-cams-bd1a218fa977">上一篇文章</a>中，我描述了一种叫做类激活映射(CAM)的方法，这种方法用于突出显示输入图像中影响该图像分配到某个类的像素。在今天的帖子中，我将介绍另一种方法来找出哪些像素与预测相关，即逐层相关性传播(LRP) [1]。</p><p id="f11a" class="pw-post-body-paragraph kq kr iq ks b kt ll kv kw kx lm kz la kd ln lc ld kh lo lf lg kl lp li lj lk ij bi translated"><strong class="ks ir"> LRP算法</strong></p><p id="f422" class="pw-post-body-paragraph kq kr iq ks b kt ll kv kw kx lm kz la kd ln lc ld kh lo lf lg kl lp li lj lk ij bi translated">LRP算法背后的主要思想在于追溯输入节点对最终预测的贡献。</p><p id="501d" class="pw-post-body-paragraph kq kr iq ks b kt ll kv kw kx lm kz la kd ln lc ld kh lo lf lg kl lp li lj lk ij bi translated">首先，将最后一层中指定节点的相关性分数设置为其输出。接下来，使用重新分布规则将相关性值传播回输入层。基本的再分配规则(所谓的LPR-Z)如下所示:</p><figure class="ls lt lu lv gt lw gh gi paragraph-image"><div role="button" tabindex="0" class="lx ly di lz bf ma"><div class="gh gi lr"><img src="../Images/2e98ae99c2991fb9b4852e6010a2774e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*XMrQcBzQG_nzXPoY"/></div></div></figure><p id="d107" class="pw-post-body-paragraph kq kr iq ks b kt ll kv kw kx lm kz la kd ln lc ld kh lo lf lg kl lp li lj lk ij bi translated">其中<em class="md"> j </em>和<em class="md"> k </em>表示连续层中的神经元，zjk=ajwjk是神经元<em class="md"> j </em>的激活度乘以神经元<em class="md"> j </em>和神经元<em class="md"> k </em>之间的权重。</p><p id="4493" class="pw-post-body-paragraph kq kr iq ks b kt ll kv kw kx lm kz la kd ln lc ld kh lo lf lg kl lp li lj lk ij bi translated">传播回相关性值的过程示意性地呈现如下:</p><figure class="ls lt lu lv gt lw gh gi paragraph-image"><div class="gh gi me"><img src="../Images/f7d1b25968335f26e2f259e8cfd215b0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1326/0*dZQoYB4ixvQVR8Sv"/></div><p class="mf mg gj gh gi mh mi bd b be z dk translated">LRP程序的说明。改编自参考文献。[2]</p></figure><p id="9caa" class="pw-post-body-paragraph kq kr iq ks b kt ll kv kw kx lm kz la kd ln lc ld kh lo lf lg kl lp li lj lk ij bi translated">作为LRP算法的结果，预测被分解成指示节点对最终决策贡献多少的像素相关。</p><p id="2af5" class="pw-post-body-paragraph kq kr iq ks b kt ll kv kw kx lm kz la kd ln lc ld kh lo lf lg kl lp li lj lk ij bi translated">除此之外，还提出了许多其他更稳健的再分配规则，例如LRP-ε规则，它在分母中增加了小的正项ε，或者LRP-γ规则，它有利于正贡献超过负贡献(参见参考文献1)。[2]了解更多信息)。虽然存在多个版本的重新分布规则，但它们都共享守恒原理，即输出的激活强度在每层都是守恒的，或者换句话说，神经元相关性分数的总和在所有层中都是相同的。</p><p id="f835" class="pw-post-body-paragraph kq kr iq ks b kt ll kv kw kx lm kz la kd ln lc ld kh lo lf lg kl lp li lj lk ij bi translated"><strong class="ks ir">结果</strong></p><p id="24ba" class="pw-post-body-paragraph kq kr iq ks b kt ll kv kw kx lm kz la kd ln lc ld kh lo lf lg kl lp li lj lk ij bi translated">下面你可以看到使用[3]中的5种不同的重新分配规则为识别为西伯利亚哈士奇的图像生成的关联图，以及在CAM技术的帮助下生成的热图，我在我的<a class="ae lq" rel="noopener" target="_blank" href="/visualizing-neural-networks-decision-making-process-part-1-class-activation-maps-cams-bd1a218fa977">上一篇文章</a>中更详细地谈到了这一点。</p><figure class="ls lt lu lv gt lw gh gi paragraph-image"><div role="button" tabindex="0" class="lx ly di lz bf ma"><div class="gh gi mj"><img src="../Images/a8769c6b21d8c1f856d80f8f6b68deb4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*6jB2qP5FmqGwuUf0"/></div></div><p class="mf mg gj gh gi mh mi bd b be z dk translated"><em class="mk">使用LRP算法产生的热图，使用5种不同的关联规则(图片由</em> <a class="ae lq" href="https://unsplash.com/@muzaluiza?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> <em class="mk">路易莎·萨伊芙莉娜</em> </a> <em class="mk">在</em><a class="ae lq" href="https://unsplash.com/s/photos/husky?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"><em class="mk">Unspla</em></a><em class="mk">sh上拍摄)。</em></p></figure><figure class="ls lt lu lv gt lw gh gi paragraph-image"><div class="gh gi ml"><img src="../Images/dc4811ab2b9a0fc16a1c8fea1d6e32b2.png" data-original-src="https://miro.medium.com/v2/resize:fit:448/0*S_Pcx97bHJYEwu-a"/></div><p class="mf mg gj gh gi mh mi bd b be z dk translated"><em class="mk">使用类激活映射算法生成的热图。</em></p></figure><p id="c661" class="pw-post-body-paragraph kq kr iq ks b kt ll kv kw kx lm kz la kd ln lc ld kh lo lf lg kl lp li lj lk ij bi translated">正如您在上面的图片中所看到的，热图互不相同，这取决于计算相关性分数时使用的原则和应用的方法(LRP或CAM)。然而，在所有的图片中，狗的头部被清楚地突出显示，这表明图像的这一部分与分类结果相关。</p><p id="0059" class="pw-post-body-paragraph kq kr iq ks b kt ll kv kw kx lm kz la kd ln lc ld kh lo lf lg kl lp li lj lk ij bi translated">LRP方法成功地应用于解释面部表情识别中的神经网络决策[4]和寻找与文档分类相关的单词[5]。</p><p id="13ac" class="pw-post-body-paragraph kq kr iq ks b kt ll kv kw kx lm kz la kd ln lc ld kh lo lf lg kl lp li lj lk ij bi translated">想要更多类似的文章，请看一下NeuroSYS的博客。</p><p id="41cd" class="pw-post-body-paragraph kq kr iq ks b kt ll kv kw kx lm kz la kd ln lc ld kh lo lf lg kl lp li lj lk ij bi translated"><strong class="ks ir">文学</strong></p><p id="a92f" class="pw-post-body-paragraph kq kr iq ks b kt ll kv kw kx lm kz la kd ln lc ld kh lo lf lg kl lp li lj lk ij bi translated">[1] Bach，Sebastian等人，“通过逐层相关性传播对非线性分类器决策的逐像素解释”《公共科学图书馆综合杂志》 10.7 (2015)</p><p id="194a" class="pw-post-body-paragraph kq kr iq ks b kt ll kv kw kx lm kz la kd ln lc ld kh lo lf lg kl lp li lj lk ij bi translated">[2] Montavon，Grégoire等人，“分层相关性传播:概述”<em class="md">可解释的AI:解释、说明和可视化深度学习</em>。施普林格，查姆，2019。193–209</p><p id="aa45" class="pw-post-body-paragraph kq kr iq ks b kt ll kv kw kx lm kz la kd ln lc ld kh lo lf lg kl lp li lj lk ij bi translated">[3] Alber，Maximilian，等,“研究神经网络”<em class="md">机器学习研究杂志</em>20.93(2019):1–8。</p><p id="1620" class="pw-post-body-paragraph kq kr iq ks b kt ll kv kw kx lm kz la kd ln lc ld kh lo lf lg kl lp li lj lk ij bi translated">[4]阿尔巴布扎达、法尔哈德等人，“通过解构神经网络识别个人面部表情”<em class="md">德国模式识别会议</em>。施普林格，查姆，2016</p><p id="fc1f" class="pw-post-body-paragraph kq kr iq ks b kt ll kv kw kx lm kz la kd ln lc ld kh lo lf lg kl lp li lj lk ij bi translated">[5] Arras，Leila，等.“在文本文档中什么是相关的？":一种可解释的机器学习方法."<em class="md"> PloS one </em> 12.8 (2017)。</p></div></div>    
</body>
</html>