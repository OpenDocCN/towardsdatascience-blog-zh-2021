<html>
<head>
<title>Interpretable Neural Networks With PyTorch</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">PyTorch 可解释神经网络</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/interpretable-neural-networks-with-pytorch-76f1c31260fe?source=collection_archive---------6-----------------------#2021-12-17">https://towardsdatascience.com/interpretable-neural-networks-with-pytorch-76f1c31260fe?source=collection_archive---------6-----------------------#2021-12-17</a></blockquote><div><div class="fc ii ij ik il im"/><div class="in io ip iq ir"><h2 id="e640" class="is it iu bd b dl iv iw ix iy iz ja dk jb translated" aria-label="kicker paragraph"><a class="ae ep" href="https://medium.com/tag/explainable-ai" rel="noopener">可解释的人工智能</a></h2><div class=""/><div class=""><h2 id="9f3c" class="pw-subtitle-paragraph ka jd iu bd b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr dk translated">了解如何使用 PyTorch 构建可通过设计解释的前馈神经网络</h2></div><figure class="kt ku kv kw gu kx gi gj paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="gi gj ks"><img src="../Images/f72384ff38a07f046811826e2ed16037.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*zdCUsn8cL-9QL7pO"/></div></div><p class="le lf gk gi gj lg lh bd b be z dk translated">由<a class="ae li" href="https://unsplash.com/@wombat?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Jan Schulz 拍摄#网页设计师 Stuttgart </a>在<a class="ae li" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a></p></figure><p id="3a86" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">有几种方法来评估机器学习模型，其中两种是<strong class="ll je">准确性</strong>和<strong class="ll je">可解释性</strong>。一个高精度的模型就是我们通常所说的<em class="mf">好的</em>模型，它很好地学习了输入<em class="mf"> X </em>和输出<em class="mf"> y </em>之间的关系。</p><p id="bf79" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">如果一个模型具有高度的可解释性或可解释性，我们就能理解该模型如何做出预测，以及我们如何通过改变输入特征来影响该预测。虽然很难说当我们增加或减少输入的某个特征时，深度神经网络的输出会如何表现，但对于线性模型来说，这是非常容易的:如果你增加一个特征，输出会增加该特征的系数。简单。</p><p id="8b77" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">现在，你可能经常听到这样的话:</p><blockquote class="mg"><p id="5ced" class="mh mi iu bd mj mk ml mm mn mo mp me dk translated">“有可解释的模型，也有表现良好的模型。”—一个更不了解它的人</p></blockquote><p id="9104" class="pw-post-body-paragraph lj lk iu ll b lm mq ke lo lp mr kh lr ls ms lu lv lw mt ly lz ma mu mc md me in bi translated">但是，如果你看过我关于<a class="ae li" href="https://github.com/interpretml/interpret" rel="noopener ugc nofollow" target="_blank">可解释的助推机</a> (EBM)的文章，那么你已经知道这不是真的。EBM 是一个模型的例子，它具有很好的性能，同时又是可解释的。</p><div class="mv mw gq gs mx my"><a rel="noopener follow" target="_blank" href="/the-explainable-boosting-machine-f24152509ebb"><div class="mz ab fp"><div class="na ab nb cl cj nc"><h2 class="bd je gz z fq nd fs ft ne fv fx jd bi translated">可解释的助推器</h2><div class="nf l"><h3 class="bd b gz z fq nd fs ft ne fv fx dk translated">像梯度推进一样精确，像线性回归一样可解释。</h3></div><div class="ng l"><p class="bd b dl z fq nd fs ft ne fv fx dk translated">towardsdatascience.com</p></div></div><div class="nh l"><div class="ni l nj nk nl nh nm lc my"/></div></div></a></div><p id="ab25" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">对于我以前的文章，我创建了下图，展示了我们如何在<em class="mf">可解释性-准确性空间</em>中放置一些模型。</p><figure class="kt ku kv kw gu kx gi gj paragraph-image"><div class="gi gj nn"><img src="../Images/b161bb709a84ec477242c550dcdd14b6.png" data-original-src="https://miro.medium.com/v2/resize:fit:978/format:webp/1*x23MFtoABy_qd8Zmk6IOXA.png"/></div><p class="le lf gk gi gj lg lh bd b be z dk translated">图片由作者提供。</p></figure><p id="4a8b" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">特别是，我将深层神经网络(省略了深层神经网络)更多地放置在非常准确但难以解释的区域。当然，您可以通过使用像<a class="ae li" href="https://github.com/slundberg/shap" rel="noopener ugc nofollow" target="_blank"> shap </a>或<a class="ae li" href="https://github.com/marcotcr/lime" rel="noopener ugc nofollow" target="_blank"> lime </a>这样的库在一定程度上缓解可解释性问题，但是这些方法都有自己的一套假设和问题。因此，让我们走另一条路，创建一个可以通过本文中的设计来解释的神经网络架构。</p><blockquote class="no np nq"><p id="1fc3" class="lj lk mf ll b lm ln ke lo lp lq kh lr nr lt lu lv ns lx ly lz nt mb mc md me in bi translated"><strong class="ll je"> <em class="iu">免责声明:</em> </strong> <em class="iu">我即将呈现的架构刚刚浮现在脑海中。我不知道是否已经有关于它的文献，至少我找不到任何东西。不过，</em> <a class="ae li" href="https://medium.com/@kristpapas?source=post_info_responses---------4-----------------------" rel="noopener"> Krist Papas </a> <em class="iu">指出，这个想法可以在论文</em> <a class="ae li" href="https://arxiv.org/abs/2004.13912" rel="noopener ugc nofollow" target="_blank">中找到:用神经网络进行可解释的机器学习</a><em class="iu">【1】</em><em class="iu">作者</em> <em class="iu"> Rishabh Agarwal 等人感谢！</em></p></blockquote><h1 id="20ee" class="nu nv iu bd nw nx ny nz oa ob oc od oe kj of kk og km oh kn oi kp oj kq ok ol bi translated">可解释的建筑理念</h1><p id="27ab" class="pw-post-body-paragraph lj lk iu ll b lm om ke lo lp on kh lr ls oo lu lv lw op ly lz ma oq mc md me in bi translated">请注意，我希望你知道前馈神经网络是如何工作的。我不会在这里给出一个完整的介绍，因为已经有很多关于它的资源了。</p><p id="3d94" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">考虑以下玩具神经网络，具有三个输入节点<em class="mf"> x </em> ₁、<em class="mf"> x </em> ₂、<em class="mf"> x </em> ₃、单个输出节点<em class="mf"> ŷ </em>，以及三个各有六个节点的隐藏层。我在这里省略了偏差项。</p><figure class="kt ku kv kw gu kx gi gj paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="gi gj or"><img src="../Images/5b49d80f9557aad5bf636476d2a9bdd8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZWc9P5DlaFlSaAZvyf9A2A.png"/></div></div><p class="le lf gk gi gj lg lh bd b be z dk translated">图片由作者提供。</p></figure><p id="a11a" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">这种体系结构在可解释性方面的问题是，由于完全连接的层，输入完全混合在一起。每个单个输入节点都会影响所有隐藏层节点，随着我们深入网络，这种影响会变得更加复杂。</p><h2 id="40a8" class="os nv iu bd nw ot ou dn oa ov ow dp oe ls ox oy og lw oz pa oi ma pb pc ok ja bi translated">受树木的启发</h2><p id="b705" class="pw-post-body-paragraph lj lk iu ll b lm om ke lo lp on kh lr ls oo lu lv lw op ly lz ma oq mc md me in bi translated">基于树的模型通常也是如此，因为如果我们不加以限制，决策树可能会使用每个特征来创建分割。例如，标准的梯度增强及其派生，如<a class="ae li" href="https://xgboost.readthedocs.io/en/stable/" rel="noopener ugc nofollow" target="_blank"> XGBoost </a>、<a class="ae li" href="https://lightgbm.readthedocs.io/en/latest/" rel="noopener ugc nofollow" target="_blank"> LightGBM </a>和<a class="ae li" href="https://catboost.ai/" rel="noopener ugc nofollow" target="_blank"> CatBoost </a>本身并不能真正解释。</p><p id="2da7" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">然而，你可以通过使用<strong class="ll je">只依赖于单个特征的决策树</strong>来使梯度增强变得可解释，就像 EBM 所做的那样(阅读我的相关文章！😎).</p><p id="9883" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">在许多情况下，像这样限制树不会对性能造成太大影响，但使我们能够像这样直观地看到功能影响:</p><figure class="kt ku kv kw gu kx gi gj paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="gi gj pd"><img src="../Images/ed471ecf842751fb5639c2971718d8b8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nfrKuUAD3gWPIEiNuNFd-A.png"/></div></div><p class="le lf gk gi gj lg lh bd b be z dk translated"><strong class="bd pe">的输出解释了</strong>的显示功能。图片由作者提供。</p></figure><p id="65f4" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">看一下有蓝线的图形的顶部。它显示了在某些回归问题中 feature_4 对输出的影响。在<em class="mf"> x </em>轴上，可以看到 feature_4 的范围。<em class="mf"> y </em>轴显示<strong class="ll je">分数</strong>，它是输出改变多少的值。下面的直方图显示了 feature_4 的分布。</p><p id="e0ec" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">从图中我们可以看到以下内容:</p><ul class=""><li id="17c3" class="pf pg iu ll b lm ln lp lq ls ph lw pi ma pj me pk pl pm pn bi translated">如果 feature_4 约为 0.62，则与 feature_4 为 0.6 或 0.65 相比，输出增加约 10 倍。</li><li id="07e7" class="pf pg iu ll b lm po lp pp ls pq lw pr ma ps me pk pl pm pn bi translated">如果 feature_4 大于 0.66，对输出的影响是负面的。</li><li id="caf4" class="pf pg iu ll b lm po lp pp ls pq lw pr ma ps me pk pl pm pn bi translated">将 feature_4 在 0.4 至 0.56 范围内改变一位确实会使输出发生很大变化。</li></ul><p id="c4ba" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">然后，模型的最终预测只是不同特征分数的总和。此行为类似于 Shapley 值，但不需要计算它们。很好，对吧？现在，让我向你展示我们如何对神经网络做同样的事情。</p><h2 id="b070" class="os nv iu bd nw ot ou dn oa ov ow dp oe ls ox oy og lw oz pa oi ma pb pc ok ja bi translated">移除边缘</h2><p id="fdee" class="pw-post-body-paragraph lj lk iu ll b lm om ke lo lp on kh lr ls oo lu lv lw op ly lz ma oq mc md me in bi translated">因此，如果问题是神经网络的输入因为太多的边而分散在隐藏层周围，让我们只移除一些。特别是，我们必须删除允许信息从一个特征流向另一个特征的边。仅删除这些<em class="mf">溢出边</em>，上面的玩具神经网络变成:</p><figure class="kt ku kv kw gu kx gi gj paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="gi gj pt"><img src="../Images/179edaf697cc211c0cc21a9610865939.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*d4edVAlal7OmIGOhz3P0vQ.png"/></div></div><p class="le lf gk gi gj lg lh bd b be z dk translated">图片由作者提供。</p></figure><p id="5bf6" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">我们为三个输入变量创建了三个独立的<strong class="ll je">模块</strong>，每个模块都是一个完全连接的网络，有一个单独的部分输出<em class="mf"> ŷᵢ.</em>最后一步，将这些<em class="mf"> ŷᵢ </em>相加，加上一个偏置(图中省略)产生最终输出<em class="mf"> ŷ </em>。</p><p id="4a3b" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">我们引入了部分输出，以便能够创建与 EBM 允许的相同类型的图。上图中的一个方块代表一个情节:<em class="mf"> xᵢ </em>进去<em class="mf">，ŷᵢ </em>出来。我们将在后面看到如何做到这一点。</p></div><div class="ab cl pu pv hy pw" role="separator"><span class="px bw bk py pz qa"/><span class="px bw bk py pz qa"/><span class="px bw bk py pz"/></div><div class="in io ip iq ir"><p id="73d8" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">这里我们已经有了完整的架构！我认为理论上理解它是相当容易的，但是让我们也实施它。这样，你很高兴，因为你可以使用神经网络，企业也很高兴，因为神经网络是可解释的。</p><h1 id="b7f9" class="nu nv iu bd nw nx ny nz oa ob oc od oe kj of kk og km oh kn oi kp oj kq ok ol bi translated">PyTorch 中的实现</h1><p id="80cd" class="pw-post-body-paragraph lj lk iu ll b lm om ke lo lp on kh lr ls oo lu lv lw op ly lz ma oq mc md me in bi translated">我不指望你完全熟悉<a class="ae li" href="https://pytorch.org/" rel="noopener ugc nofollow" target="_blank"> PyTorch </a>，所以我会解释一些基础知识，帮助你理解我们的定制实现。如果你知道 PyTorch 的基础知识，你可以跳过<strong class="ll je">完全连接层</strong>部分。如果您还没有安装 PyTorch，<a class="ae li" href="https://pytorch.org/" rel="noopener ugc nofollow" target="_blank">在这里选择您的版本</a>。</p><h2 id="0b08" class="os nv iu bd nw ot ou dn oa ov ow dp oe ls ox oy og lw oz pa oi ma pb pc ok ja bi translated">完全连接的层</h2><p id="d6b6" class="pw-post-body-paragraph lj lk iu ll b lm om ke lo lp on kh lr ls oo lu lv lw op ly lz ma oq mc md me in bi translated">这些层在 PyTorch 中也被称为<strong class="ll je">线性</strong>或<strong class="ll je">密集</strong>在<a class="ae li" href="https://keras.io/" rel="noopener ugc nofollow" target="_blank"> Keras </a>中。它们使用具有乘法权重的<em class="mf"> nm </em>边沿将<em class="mf"> n </em>输入节点连接到<em class="mf"> m </em>输出节点。这基本上是一个矩阵乘法加上一个偏差项，如下面两个代码片段所示:</p><pre class="kt ku kv kw gu qb qc qd bn qe qf bi"><span id="4163" class="qg nv iu qc b be qh qi l qj qk">import torch<br/><br/>torch.manual_seed(0) # keep things reproducible<br/><br/>x = torch.tensor([1., 2.]) # create an input array<br/>linear_layer = torch.nn.Linear(2, 3) # define a linear layer<br/><br/>print(linear_layer(x)) # putting the input array into the layer<br/><br/># Output:<br/># tensor([ 0.7393, -1.0621,  0.0441], grad_fn=&lt;AddBackward0&gt;)</span></pre><p id="ba67" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">这就是如何创建完全连接的层，并将其应用于 PyTorch 张量。您可以通过<code class="fe ql qm qn qc b">linear_layer.weight</code>获得用于乘法的矩阵，通过<code class="fe ql qm qn qc b">linear_layer.bias</code>获得用于偏置的矩阵。那你可以做</p><pre class="kt ku kv kw gu qb qc qd bn qe qf bi"><span id="8123" class="qg nv iu qc b be qh qi l qj qk">print(linear_layer.weight @ x + linear_layer.bias) # @ = matrix mult<br/><br/># Output:<br/># tensor([ 0.7393, -1.0621,  0.0441], grad_fn=&lt;AddBackward0&gt;)</span></pre><p id="aa62" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">不错，是一样的！现在，PyTorch、Keras 和公司的伟大之处在于，你可以将许多层堆叠在一起，创建一个神经网络。在 PyTorch 中，你可以通过<code class="fe ql qm qn qc b">torch.nn.Sequential</code>实现这种堆叠。要从上面重建密集网络，您可以做一个简单的</p><pre class="kt ku kv kw gu qb qc qd bn qe qf bi"><span id="3d69" class="qg nv iu qc b be qh qi l qj qk">model = torch.nn.Sequential(<br/>    torch.nn.Linear(3, 6),<br/>    torch.nn.ReLU(),<br/>    torch.nn.Linear(6, 6),<br/>    torch.nn.ReLU(),<br/>    torch.nn.Linear(6, 6),<br/>    torch.nn.ReLU(),<br/>    torch.nn.Linear(6, 1),<br/>)<br/><br/>print(model(torch.randn(4, 3))) # feed it 4 random 3-dim. vectors</span></pre><blockquote class="no np nq"><p id="34c8" class="lj lk mf ll b lm ln ke lo lp lq kh lr nr lt lu lv ns lx ly lz nt mb mc md me in bi translated"><strong class="ll je"> <em class="iu">注:</em> </strong> <em class="iu">我到目前为止还没有给你演示过如何训练这个网络，只是架构的定义，包括参数的初始化。但是你可以向网络提供三维输入，接收一维输出。</em></p></blockquote></div><div class="ab cl pu pv hy pw" role="separator"><span class="px bw bk py pz qa"/><span class="px bw bk py pz qa"/><span class="px bw bk py pz"/></div><div class="in io ip iq ir"><p id="30aa" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">既然我们想创建自己的层，让我们先从简单的东西开始练习:重新创建 PyTorch 的<code class="fe ql qm qn qc b">Linear</code>层。你可以这样做:</p><pre class="kt ku kv kw gu qb qc qd bn qe qf bi"><span id="cacd" class="qg nv iu qc b be qh qi l qj qk">import torch<br/>import math<br/><br/>class MyLinearLayer(torch.nn.Module):<br/>    def __init__(self, in_features, out_features):<br/>        super().__init__()<br/>        self.in_features = in_features<br/>        self.out_features = out_features<br/>        <br/>        # multiplicative weights<br/>        weights = torch.Tensor(out_features, in_features)<br/>        self.weights = torch.nn.Parameter(weights)<br/>        torch.nn.init.kaiming_uniform_(self.weights) <br/>        <br/>        # bias<br/>        bias = torch.Tensor(out_features)<br/>        self.bias = torch.nn.Parameter(bias)<br/>        bound = 1 / math.sqrt(in_features)<br/>        torch.nn.init.uniform_(self.bias, -bound, bound)<br/><br/>    def forward(self, x):<br/>        return x @ self.weights.t() + self.bias</span></pre><p id="e439" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">这段代码值得解释一下。我们通过以下方式引入线性层的权重</p><ol class=""><li id="efcb" class="pf pg iu ll b lm ln lp lq ls ph lw pi ma pj me qo pl pm pn bi translated">创建 PyTorch 张量(包含所有零，但这无关紧要)</li><li id="7744" class="pf pg iu ll b lm po lp pp ls pq lw pr ma ps me qo pl pm pn bi translated">将它注册为层的可学习参数，这意味着梯度下降可以在训练期间更新它，然后</li><li id="dba5" class="pf pg iu ll b lm po lp pp ls pq lw pr ma ps me qo pl pm pn bi translated">初始化参数。</li></ol><p id="f233" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">初始化神经网络的参数本身就是一个完整的主题，因此我们不会进入兔子洞。如果它太困扰你，你也可以用不同的方式初始化它，例如通过使用一个标准的正态分布<code class="fe ql qm qn qc b">torch.randn(out_features, in_features)</code>，但是那时训练可能会慢一些。无论如何，我们对偏差做同样的处理。</p><p id="1627" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">然后，该层需要知道它应该在<code class="fe ql qm qn qc b">forward</code>方法中执行的数学运算。这只是线性运算，即矩阵乘法和偏差加法。</p><p id="e1e6" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">好了，现在我们已经准备好实现我们的可解释神经网络层了！</p><h2 id="311e" class="os nv iu bd nw ot ou dn oa ov ow dp oe ls ox oy og lw oz pa oi ma pb pc ok ja bi translated">块线性图层</h2><p id="7ae0" class="pw-post-body-paragraph lj lk iu ll b lm om ke lo lp on kh lr ls oo lu lv lw op ly lz ma oq mc md me in bi translated">我们现在设计一个<code class="fe ql qm qn qc b">BlockLinear</code>层，我们将以如下方式使用:首先，我们从<em class="mf"> n </em>特征开始。然后<code class="fe ql qm qn qc b">BlockLinear</code>层应该创建由<em class="mf"> h </em>个隐藏神经元组成的<em class="mf"> n </em>个块。为了简化，h 在每个块中是相同的，但是你当然可以推广这个。总的来说，第一个隐藏层将由<em class="mf"> nh </em>神经元组成，但也只有<em class="mf"> nh </em>边连接到它们(而不是<em class="mf"> n </em> <em class="mf"> h </em>用于完全连接的层)<em class="mf">。</em>为了更好的理解，再看一遍上面的图片。这里，<em class="mf"> n </em> = 3，<em class="mf"> h </em> = 2。</p><figure class="kt ku kv kw gu kx gi gj paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="gi gj pt"><img src="../Images/179edaf697cc211c0cc21a9610865939.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*d4edVAlal7OmIGOhz3P0vQ.png"/></div></div><p class="le lf gk gi gj lg lh bd b be z dk translated">图片由作者提供。</p></figure><p id="1efb" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">然后——在使用了 ReLU 这样的非线性之后——我们将在这个层之后放置另一个<code class="fe ql qm qn qc b">BlockLinear</code>层，因为不同的块不应该再次合并。我们重复这个过程很多次，直到我们在最后用一个<code class="fe ql qm qn qc b">Linear</code>层把所有的东西绑起来。</p><h2 id="f519" class="os nv iu bd nw ot ou dn oa ov ow dp oe ls ox oy og lw oz pa oi ma pb pc ok ja bi translated">块线性层的实现</h2><p id="e625" class="pw-post-body-paragraph lj lk iu ll b lm om ke lo lp on kh lr ls oo lu lv lw op ly lz ma oq mc md me in bi translated">让我们来看看代码。它与我们定制的线性层非常相似，所以代码不应该太吓人。</p><pre class="kt ku kv kw gu qb qc qd bn qe qf bi"><span id="5b0d" class="qg nv iu qc b be qh qi l qj qk">class BlockLinear(torch.nn.Module):<br/>    def __init__(self, n_blocks, in_features, out_features):<br/>        super().__init__()<br/>        self.n_blocks = n_blocks<br/>        self.in_features = in_features<br/>        self.out_features = out_features<br/>        self.block_weights = []<br/>        self.block_biases = []<br/>        for i in range(n_blocks):<br/>            block_weight = torch.Tensor(out_features, in_features)<br/>            block_weight = torch.nn.Parameter(block_weight)<br/>            torch.nn.init.kaiming_uniform_(block_weight)<br/>            self.register_parameter(<br/>                f'block_weight_{i}',<br/>                block_weight<br/>            )<br/>            self.block_weights.append(block_weight)<br/>            block_bias = torch.Tensor(out_features)<br/>            block_bias = torch.nn.Parameter(block_bias)<br/>            bound = 1 / math.sqrt(in_features)<br/>            torch.nn.init.uniform_(block_bias, -bound, bound)<br/>            self.register_parameter(<br/>                f'block_bias_{i}',<br/>                block_bias<br/>            )<br/>            self.block_biases.append(block_bias)<br/><br/>    def forward(self, x):<br/>        block_size = x.size(1) // self.n_blocks<br/>        x_blocks = torch.split(<br/>            x,<br/>            split_size_or_sections=block_size,<br/>            dim=1<br/>        )<br/>        block_outputs = []<br/>        for block_id in range(self.n_blocks):<br/>            block_outputs.append(<br/>                x_blocks[block_id] @ self.block_weights[block_id].t() + self.block_biases[block_id]<br/>            )<br/>        return torch.cat(block_outputs, dim=1)</span></pre><p id="de93" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">第一行类似于我们在自制的线性图层中看到的，只是重复了<code class="fe ql qm qn qc b">n_blocks</code>次。这为每个块创建了一个独立的线性层。</p><p id="4c54" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">在正向方法中，我们得到一个作为单个张量的<code class="fe ql qm qn qc b">x</code>，我们必须首先使用<code class="fe ql qm qn qc b">torch.split</code>将它再次分割成块。举例来说，块大小为 2 会执行以下操作:<code class="fe ql qm qn qc b">[1, 2, 3, 4, 5, 6] -&gt; [1, 2], [3, 4], [5, 6]</code>。然后，我们将独立的线性变换应用于不同的块，并用<code class="fe ql qm qn qc b">torch.cat</code>将结果粘合在一起。搞定了。</p><h2 id="935c" class="os nv iu bd nw ot ou dn oa ov ow dp oe ls ox oy og lw oz pa oi ma pb pc ok ja bi translated">训练可解释的神经网络</h2><p id="0d72" class="pw-post-body-paragraph lj lk iu ll b lm om ke lo lp on kh lr ls oo lu lv lw op ly lz ma oq mc md me in bi translated">现在，我们有了定义我们的可解释神经网络的所有要素。我们只需要首先创建一个数据集:</p><pre class="kt ku kv kw gu qb qc qd bn qe qf bi"><span id="e29b" class="qg nv iu qc b be qh qi l qj qk">X = torch.randn(1000, 3)<br/>y = 3*X[:, 0] + 2*X[:, 1]**2 + X[:, 2]**3 + torch.randn(1000)<br/>y = y.reshape(-1, 1)</span></pre><p id="7d88" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">我们可以看到，我们处理的是一个由一千个样本组成的三维数据集。如果将要素 1 和要素 2 平方，真实的关系是线性的-这就是我们想要用模型恢复的！因此，让我们定义一个能够捕捉这种关系的小模型。</p><pre class="kt ku kv kw gu qb qc qd bn qe qf bi"><span id="9174" class="qg nv iu qc b be qh qi l qj qk">class Model(torch.nn.Module):<br/>    def __init__(self):<br/>        super().__init__()<br/>        <br/>        self.features = torch.nn.Sequential(<br/>            BlockLinear(3, 1, 20),<br/>            torch.nn.ReLU(),<br/>            BlockLinear(3, 20, 20),<br/>            torch.nn.ReLU(),<br/>            BlockLinear(3, 20, 20),<br/>            torch.nn.ReLU(),<br/>            BlockLinear(3, 20, 1),<br/>        )<br/>        <br/>        self.lr = torch.nn.Linear(3, 1)<br/>        <br/>    def forward(self, x):<br/>        x_pre = self.features(x)<br/>        return self.lr(x_pre)<br/>    <br/>model = Model()</span></pre><p id="7bc9" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">我将模型分为两步:</p><ol class=""><li id="afc8" class="pf pg iu ll b lm ln lp lq ls ph lw pi ma pj me qo pl pm pn bi translated">用<code class="fe ql qm qn qc b">self.features</code>计算部分输出<em class="mf"> ŷᵢ </em>，然后</li><li id="3a46" class="pf pg iu ll b lm po lp pp ls pq lw pr ma ps me qo pl pm pn bi translated">将最终预测<em class="mf"> ŷ </em>计算为<em class="mf"> ŷᵢ </em>与<code class="fe ql qm qn qc b">self.lr</code>的加权和。</li></ol><p id="c397" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">这使得提取特征解释更加容易。在<code class="fe ql qm qn qc b">self.features</code>的定义中，您可以看到我们创建了一个具有三个模块的神经网络，因为我们在数据集中有三个特征。对于每个块，我们创建许多隐藏层，每个块有 20 个神经元。</p><p id="3aa2" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">现在，我们可以创建一个简单的训练循环:</p><pre class="kt ku kv kw gu qb qc qd bn qe qf bi"><span id="7af5" class="qg nv iu qc b be qh qi l qj qk">optimizer = torch.optim.Adam(model.parameters())<br/>criterion = torch.nn.MSELoss()<br/><br/>for i in range(2000):<br/>    optimizer.zero_grad()<br/>    y_pred = model(X)<br/>    loss = criterion(y, y_pred)<br/>    loss.backward()<br/>    optimizer.step()<br/>    if i % 100 == 0:<br/>        print(loss)</span></pre><p id="3fa2" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">基本上，我们选择 Adam 作为优化器，MSE 作为损失，然后进行标准梯度下降，即使用<code class="fe ql qm qn qc b">optimzer.zero_grad()</code>删除旧梯度，计算预测，计算损失，通过<code class="fe ql qm qn qc b">loss.backward()</code>区分损失，并通过<code class="fe ql qm qn qc b">optimizer.step()</code>更新模型参数。你可以看到培训损失随着时间的推移而下降。这里我们不关心验证或测试集。<strong class="ll je">训练</strong> <em class="mf"> r </em>结束时应大于 0.95。</p></div><div class="ab cl pu pv hy pw" role="separator"><span class="px bw bk py pz qa"/><span class="px bw bk py pz qa"/><span class="px bw bk py pz"/></div><div class="in io ip iq ir"><p id="18dc" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">我们现在可以通过以下方式打印模型说明</p><pre class="kt ku kv kw gu qb qc qd bn qe qf bi"><span id="d228" class="qg nv iu qc b be qh qi l qj qk">import matplotlib.pyplot as plt<br/><br/>x = torch.linspace(-5, 5, 100).reshape(-1, 1)<br/>x = torch.hstack(3*[x])<br/><br/>for i in range(3):<br/>    plt.plot(<br/>        x[:, 0].detach().numpy(),<br/>        model.get_submodule('lr').weight[0][i].item() * model.get_submodule('features')(x)[:, i].detach().numpy())<br/>    plt.title(f'Feature {i+1}')<br/>    plt.show()</span></pre><p id="eef2" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">然后得到</p><div class="kt ku kv kw gu ab cb"><figure class="qp kx qq qr qs qt qu paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><img src="../Images/8071dbde6a5a7a18461a28d2fb5d8e43.png" data-original-src="https://miro.medium.com/v2/resize:fit:674/format:webp/1*hs8cW_plBWB52DWe5V8bBg.png"/></div></figure><figure class="qp kx qv qr qs qt qu paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><img src="../Images/52a397e98616bf06082ed8276a15df66.png" data-original-src="https://miro.medium.com/v2/resize:fit:656/format:webp/1*GcWuUiG4bzVrJio3YRwsuw.png"/></div></figure><figure class="qp kx qq qr qs qt qu paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><img src="../Images/6998ba64d352de6eb9bec4f5039ef872.png" data-original-src="https://miro.medium.com/v2/resize:fit:674/format:webp/1*BKFtqgAHXXppxwA3xBo-_g.png"/></div><p class="le lf gk gi gj lg lh bd b be z dk qw di qx qy translated">作者图片。</p></figure></div><p id="ad3e" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">这看起来很整洁！该模型计算出特征 1 的影响是线性的，特征 2 的影响是二次的，特征 3 的影响是三次的。不仅如此，<strong class="ll je">模型能够向我们展示</strong>，这是整个建筑的伟大之处！</p><blockquote class="mg"><p id="d099" class="mh mi iu bd mj mk ml mm mn mo mp me dk translated"><strong class="ak">你甚至可以抛开网络，仅凭这些图表就做出预测！</strong></p></blockquote><p id="f614" class="pw-post-body-paragraph lj lk iu ll b lm mq ke lo lp mr kh lr ls ms lu lv lw mt ly lz ma mu mc md me in bi translated">举个例子，让我们估算一下<em class="mf"> x </em> = (2，-2，0)的网络输出。</p><ul class=""><li id="e6ac" class="pf pg iu ll b lm ln lp lq ls ph lw pi ma pj me pk pl pm pn bi translated">基于第一个数字，x  ₁ = 2 转化为预测的<strong class="ll je"> +5 </strong>。</li><li id="64e3" class="pf pg iu ll b lm po lp pp ls pq lw pr ma ps me pk pl pm pn bi translated"><em class="mf"> x </em> ₂ = -2 转化为预测的<strong class="ll je"> +9 </strong>，基于第二个数字。</li><li id="5753" class="pf pg iu ll b lm po lp pp ls pq lw pr ma ps me pk pl pm pn bi translated">根据第三个数字，<em class="mf"> x </em> ₃ = 0 转换为预测的<strong class="ll je"> +0 </strong>。</li><li id="bfa8" class="pf pg iu ll b lm po lp pp ls pq lw pr ma ps me pk pl pm pn bi translated">仍然有一个<strong class="ll je">偏差</strong>来自你可以通过<code class="fe ql qm qn qc b">model.get_submodule('lr').bias</code>访问的最后一个线性层，这个也必须加上，但是应该很小。</li></ul><p id="7617" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">总的来说，你的预测应该在<em class="mf">ŷ</em>t23】≈5+9+0+偏差<strong class="ll je"> ≈ </strong> 14 左右，还算准确。</p><p id="3dfd" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">您还可以看到如何最小化输出:为功能 1 选择较小的值，为功能 2 选择接近零的值，为功能 3 选择较小的值。这是你通常不能仅仅通过看神经网络看到的，但是通过分数函数，我们可以。这是可解释性的一个巨大好处。</p></div><div class="ab cl pu pv hy pw" role="separator"><span class="px bw bk py pz qa"/><span class="px bw bk py pz qa"/><span class="px bw bk py pz"/></div><div class="in io ip iq ir"><p id="4f8f" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">注意，从上面学习的得分函数只能对我们<strong class="ll je">实际上有训练数据</strong>的区域有把握。在我们的数据集中，我们实际上只观察到每个特征的值在-3 和 3 之间。因此，我们可以看到，我们并没有得到边上完美的<em class="mf"> x </em>和<em class="mf"> x </em>多项式。但是我认为图表的方向是正确的，这仍然令人印象深刻。为了充分理解这一点，将其与循证医学的结果进行比较:</p><div class="kt ku kv kw gu ab cb"><figure class="qp kx qz qr qs qt qu paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><img src="../Images/8e8ecd07c9048d40765623f33c4af60f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*oAs9mZDbFvvn6CZQqgPezA.png"/></div></figure><figure class="qp kx qz qr qs qt qu paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><img src="../Images/cf0de155106e4ec23fa8694c345dfdff.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*r6fnINfQVmLZKNEPUObpKg.png"/></div><p class="le lf gk gi gj lg lh bd b be z dk ra di rb qy translated">作者图片。</p></figure></div><p id="e2ab" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated"><strong class="ll je">曲线是块状的，向两边外推只是一条直线，这是基于树的方法的主要缺点之一。</strong></p><h1 id="3d13" class="nu nv iu bd nw nx ny nz oa ob oc od oe kj of kk og km oh kn oi kp oj kq ok ol bi translated">结论</h1><p id="8466" class="pw-post-body-paragraph lj lk iu ll b lm om ke lo lp on kh lr ls oo lu lv lw op ly lz ma oq mc md me in bi translated">在本文中，我们讨论了模型的可解释性，以及神经网络和梯度推进如何无法实现这一点。虽然<em class="mf">解释</em>包的作者创建了 EBM，一种可解释的梯度推进算法，但我向您展示了一种创建可解释的神经网络的方法。</p><p id="d896" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">然后我们用 PyTorch 实现了它，代码有点多，但并不太疯狂。至于 EBM，我们可以提取每个特征的学习得分函数，我们甚至可以用它来进行预测。</p><p id="ebd8" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">实际训练的模型甚至不再需要，这使得在弱硬件上部署和使用它成为可能。这是因为我们只需为每个特征存储一个查找表，占用的内存很少。每个查找表使用<em class="mf"> g </em>的网格大小导致<strong class="ll je">只存储<em class="mf">O</em>(<em class="mf">n</em>_ features *<em class="mf">g</em>)元素</strong>，而不是潜在地存储数百万甚至数十亿的模型参数。做预测也很便宜:只需从查找表中添加一些数字。由于这具有仅<strong class="ll je"><em class="mf">O</em>(<em class="mf">n</em>_ features)</strong>查找和加法的<strong class="ll je">时间复杂度，所以它比通过网络的正向传递快得多。</strong></p><h1 id="e8ff" class="nu nv iu bd nw nx ny nz oa ob oc od oe kj of kk og km oh kn oi kp oj kq ok ol bi translated">参考</h1><p id="13a4" class="pw-post-body-paragraph lj lk iu ll b lm om ke lo lp on kh lr ls oo lu lv lw op ly lz ma oq mc md me in bi translated">[1] R. Agarwal，L. Melnick，N. Frosst，X. Zhang，B. Lengerich，R. Caruana 和 G. Hinton，<a class="ae li" href="https://arxiv.org/abs/2004.13912" rel="noopener ugc nofollow" target="_blank">神经加法模型:用神经网络进行可解释的机器学习</a> (2020)</p></div><div class="ab cl pu pv hy pw" role="separator"><span class="px bw bk py pz qa"/><span class="px bw bk py pz qa"/><span class="px bw bk py pz"/></div><div class="in io ip iq ir"><p id="410a" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">我希望你今天学到了新的、有趣的、有用的东西。感谢阅读！</p><p id="d4f8" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated"><strong class="ll je">作为最后一点，如果你</strong></p><ol class=""><li id="64bf" class="pf pg iu ll b lm ln lp lq ls ph lw pi ma pj me qo pl pm pn bi translated"><strong class="ll je">想支持我多写点机器学习和</strong></li><li id="330c" class="pf pg iu ll b lm po lp pp ls pq lw pr ma ps me qo pl pm pn bi translated"><strong class="ll je">无论如何，计划获得一个中等订阅量，</strong></li></ol><p id="d3de" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated"><strong class="ll je">为什么不做</strong> <a class="ae li" href="https://dr-robert-kuebler.medium.com/membership" rel="noopener"> <strong class="ll je">通过这个环节</strong> </a> <strong class="ll je">？这将对我帮助很大！😊</strong></p><p id="8ad3" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">透明地说，给你的价格不变，但大约一半的订阅费直接归我。</p><p id="3fb2" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">非常感谢，如果你考虑支持我的话！</p><blockquote class="mg"><p id="d260" class="mh mi iu bd mj mk ml mm mn mo mp me dk translated">如果有任何问题，请在<a class="ae li" href="https://www.linkedin.com/in/dr-robert-k%C3%BCbler-983859150/" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>上给我写信！</p></blockquote></div></div>    
</body>
</html>