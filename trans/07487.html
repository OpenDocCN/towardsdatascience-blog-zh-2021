<html>
<head>
<title>Understanding Perception and Motion Planning for Autonomous Driving (2021)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">理解自动驾驶的感知和运动规划(2021)</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/understanding-perception-and-motion-planning-for-autonomous-driving-2021-e9241e067db7?source=collection_archive---------21-----------------------#2021-07-08">https://towardsdatascience.com/understanding-perception-and-motion-planning-for-autonomous-driving-2021-e9241e067db7?source=collection_archive---------21-----------------------#2021-07-08</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="8a3e" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">深入研究当前的SOTA方法，共同预测汽车周围的环境及其行为</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/06de9400f0c2a82234d4cc2311b12dd2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*etLbN0cqPMiROAaH"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">自动驾驶是移动的未来。图片由<a class="ae ky" href="https://unsplash.com/@ed259" rel="noopener ugc nofollow" target="_blank"> @ed259 </a>提供。</p></figure><h1 id="67b9" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">解释广告最新趋势的三篇论文</h1><p id="24e2" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">尊敬的CVPR 2021会议自动驾驶研讨会(WAD)，<strong class="lt iu">我想与您分享三种最先进的自动驾驶感知和运动规划方法</strong>。我选择了最近在当前基准中取得突出成绩的论文，其作者被选为CVPR 2021的主题演讲人。</p><p id="3d1e" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">为了更广泛地探索这个主题，这三篇论文涵盖了不同的方法:<strong class="lt iu"> Wayve </strong>(英国初创公司)论文使用相机图像作为监督学习的输入，<strong class="lt iu">丰田高级发展研究所(TRI-AD) </strong>使用非监督学习，<strong class="lt iu"> Waabi </strong>(多伦多初创公司)使用激光雷达和高清地图作为输入的监督方法。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ms"><img src="../Images/1629a505099f8e1f84de26799e17f9a9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YdfgCPmZ9eZafyDwJeKCrg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">本文中介绍的论文的比较。图片由作者提供。</p></figure><ul class=""><li id="90b2" class="mt mu it lt b lu mn lx mo ma mv me mw mi mx mm my mz na nb bi translated">"<strong class="lt iu"><em class="nc"/></strong><em class="nc">:来自周围单目摄像机的鸟瞰图中的未来实例预测。</em>"，安东尼·胡、扎克·穆雷兹、尼基尔·莫汉、索菲娅·杜达斯、杰弗里·霍克、‪vijay·巴德里纳拉亚南、罗伯托·奇波拉和亚历克斯·肯德尔，2021。[1]</li><li id="1722" class="mt mu it lt b lu nd lx ne ma nf me ng mi nh mm my mz na nb bi translated"><strong class="lt iu"> <em class="nc">打包</em> </strong> <em class="nc">:用于自监督单目深度估计的3D打包。</em>(CVPR 2020口述)、熊伟·吉齐利尼、拉雷斯·安布鲁斯、苏迪普·皮莱、艾伦·拉文托斯和阿德里安·盖登。[2]</li><li id="e911" class="mt mu it lt b lu nd lx ne ma nf me ng mi nh mm my mz na nb bi translated"><strong class="lt iu"> <em class="nc">感知、预测、规划</em> </strong> <em class="nc">:通过可解释的语义表示进行安全运动规划</em>。ECCV (2020年)。萨达特、卡萨斯、、任、吴、普拉纳布·达万和乌尔塔松。[3]</li></ul><h2 id="cbb9" class="ni la it bd lb nj nk dn lf nl nm dp lj ma nn no ll me np nq ln mi nr ns lp nt bi translated"><strong class="ak">为什么感知和动作规划在一起:</strong></h2><p id="bc3e" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">自主车辆(AVs)感知的目标是从多个传感器中提取语义表示，并将结果表示融合到ego-car的单个“鸟瞰图”(BEV)坐标框架中，用于下一个下游任务:运动规划。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/bdecb37831e55790f0abe60a01849e62.png" data-original-src="https://miro.medium.com/v2/resize:fit:1182/format:webp/1*7Bl7gK1rwadME_0resTXqQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">从六个摄像机(左图)转换到一个ego-car鸟瞰图(右图)的示例。我们可以看到汽车用蓝色标记，它们的运动由模型预测，用线表示。图片来自NVIDIA paper Lift，Splat，Shoot [4]。</p></figure><p id="b902" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">在最近的论文中，语义预测和鸟瞰图(BEV)是联合计算的(感知)。然后，时间状态能够联合预测周围代理的行为和本车运动(运动规划)。</p><h2 id="e699" class="ni la it bd lb nj nk dn lf nl nm dp lj ma nn no ll me np nq ln mi nr ns lp nt bi translated"><em class="nv">端到端才是王道</em></h2><p id="51b3" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">近年来，端到端多任务网络的表现优于顺序训练网络。目前，输入只是原始传感器测量值，输出是方向盘命令。为了避免“黑箱”效应，为了优化目的(每个块可能有其中间损失函数)和结果的可解释性(特别是当结果不好时)，有必要创建中间块。因此，我们可以在中间层块之间访问可解释的中间表示，如语义图、深度图、周围代理的概率行为(见下图)。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/c930acd421d9396c3ede54a77c4b6b2a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*NL4avJ1xOLq8w7Iv"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">学习使用端到端DL和CV驾驶。图片来自亚历克斯·肯德尔(Wayve) CVPR主题演讲[5]</p></figure><p id="9de8" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">这篇文章深入探讨了AD当前分裂的两个主要部分:</p><ul class=""><li id="4717" class="mt mu it lt b lu mn lx mo ma mv me mw mi mx mm my mz na nb bi translated"><strong class="lt iu">第一部分:仅使用摄像机作为输入的模型:</strong> ▹监督// ▹无监督</li><li id="1fe1" class="mt mu it lt b lu nd lx ne ma nf me ng mi nh mm my mz na nb bi translated"><strong class="lt iu">第二部分:使用激光雷达(laser)和高清地图作为输入的模型</strong> ▹监督<strong class="lt iu">。</strong></li></ul><h1 id="ebdd" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">第1)节仅使用摄像机</h1><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/fd668fd2552500db73ae4463b6b5dd61.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*gbPJUPjGQ-2ZZulE"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">仅限摄像头。图片来自<a class="ae ky" href="https://unsplash.com/@jstepphoto" rel="noopener ugc nofollow" target="_blank"> @jstepphoto </a>。</p></figure><h2 id="0dec" class="ni la it bd lb nj nk dn lf nl nm dp lj ma nn no ll me np nq ln mi nr ns lp nt bi translated">监督</h2><p id="f39f" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">我们将乘坐一辆最新型号(CVPR 2021)" T8 " FIERY"[1]，由一家名为<strong class="lt iu"> Wayve </strong> (Alex Kendall CEO)的初创公司的研发人员制造。他们的目标是从单目相机中学习3D几何和时间推理的表示。</p><p id="ca79" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">对于AV上只有单目摄像机的团队来说，第一个挑战是学习深度。这是创建鸟瞰图参考框架的重要步骤，在该参考框架中可以识别实例并规划运动。由于每个摄像机最初在其自己的参考坐标中输出其自己的推断，因此该任务更加困难。</p><p id="21a5" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">该过程的一个重要步骤是从2D图像生成3D图像，因此我将首先解释将2D图像从相机装备“提升”到所有相机共享的世界的3D表示的最新方法。NVIDIA CVPR 2020年发表的论文“<em class="nc"> Lift，Splat，Shoot:通过隐式取消投影到3D来编码来自任意相机装备的图像</em>”[4]中的这种方法也用于FIERY。</p><ol class=""><li id="2e8c" class="mt mu it lt b lu mn lx mo ma mv me mw mi mx mm nw mz na nb bi translated"><strong class="lt iu">使用“提升-拍打-拍摄”将2D编码为3D</strong></li></ol><p id="65c7" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">此处提供纸张和代码<a class="ae ky" href="https://nv-tlabs.github.io/lift-splat-shoot/" rel="noopener ugc nofollow" target="_blank">。</a></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/de149f2ce47961a04502faac1b10d52f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1358/format:webp/1*WlRuzaHi7yCNvhPrhfOP6w.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">在2D图像上创建第三个维度。这张图片展示了“抬起、拍打、拍摄”纸张的“抬起”步骤[4]。</p></figure><p id="0c9f" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><strong class="lt iu"> 1。提升</strong>:将本地2D坐标系转换为所有摄像机共享的3D坐标系。它们为每个像素生成所有可能(离散化)深度的表示。深度概率充当自我注意权重。</p><p id="1eda" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">该模型通过将来自2D图像的每个像素与离散深度d的列表相关联来人工创建大的点云。对于具有(r，g，b，a)值的每个像素p，网络预测上下文向量<em class="nc">c</em>T3】和<strong class="lt iu">深度分布<em class="nc">a</em>T7】。上下文向量c然后乘以来自分布d的每个权重a。作为矩阵的结果是a和c的外积。该操作使得能够对特定深度给予更多关注。如果深度分布<em class="nc"> a </em>全为0，但一个元素为1，则该网络充当伪激光雷达。每个图像的点云张量馈入在图像网上预先训练的有效网骨干网。EfficientNet模型将输出上述由要提升的特征<em class="nc"> c </em>和一组离散深度概率<em class="nc"> a </em>构成的外积。</strong></p><p id="cfdc" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><strong class="lt iu"> 2。Splat </strong>:外部和内部相机参数用于将3D表示法Splat到鸟瞰视图平面上。</p><p id="0231" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">他们使用最初用于激光雷达点云中物体检测的点柱技术。Pointpillar将点云转换为伪图像，以便能够应用2D卷积架构。</p><p id="0050" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">点云被离散化为x-y平面中的网格，这创建了一组柱子p。云中的每个点被转换为D维(D=9)向量，其中我们添加(<strong class="lt iu"> Xc，Yc，Zc </strong>)到柱子中所有点的算术平均值的距离和(Xp，Yp)从x-y坐标系中的柱子中心到原始点(x，y，z，反射率)的距离。从这个稀疏的支柱网格，他们创建了一个大小为(D，P，N)的密集张量，其中P是非空支柱的数量(可能用0填充)，N是每个支柱的点数(如果一个支柱中有太多的点，则可能进行采样)。这个稠密张量馈入一个点网网络以生成一个(C，P，N)张量，随后是一个max运算以创建一个(C，P)张量。最后，我们使用P将特征散射回原始的柱子位置，以创建大小为(C，H，W)的伪图像。</p><p id="4d8b" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">在“提升，拍打，射击”中，作者在D轴上使用总和池而不是最大池来创建C x H x W张量。他们将这个张量输入由ResNet块组成的第二个主干网络，将点云转换为鸟瞰图像。</p><p id="328c" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">3.<strong class="lt iu">拍摄</strong>:他们为BEV中的每个实例“拍摄”不同的轨迹，计算它们的代价和代价最小的轨迹。</p><p id="091f" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">对于轨迹规划，让我们看看“<em class="nc">火热的</em>”而不是NVIDIA论文的拍摄部分。</p></div><div class="ab cl ny nz hx oa" role="separator"><span class="ob bw bk oc od oe"/><span class="ob bw bk oc od oe"/><span class="ob bw bk oc od"/></div><div class="im in io ip iq"><p id="1eee" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><strong class="lt iu"> 2。用于运动规划的FIERY:</strong></p><p id="576e" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">代码可用<a class="ae ky" href="https://github.com/wayveai/fiery" rel="noopener ugc nofollow" target="_blank">此处</a>。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi of"><img src="../Images/6658e734bc2acb8198f4dd4f6d2ef8e4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Fs_7AgoHoJHjEW9hiMcZRw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">FIERY的整体架构分六步走。图片来自原纸[1]。</p></figure><p id="9158" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><strong class="lt iu">学习时间表征:</strong></p><p id="e730" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">让我们考虑我们已经获得了连续帧中的BEV特征<em class="nc"> X=(x_1，..，x_t) </em>从上面<em class="nc">呈现的<em class="nc"/><em class="nc">Lift-Splat-Shoot</em>的提升步骤。</em>作者用空间变换模块s将x中所有这些过去的特征<em class="nc"> x_i </em>扭曲到当前参考系t，例如<em class="nc"> x_i^t =S(x_i，a_{t-1} a_{t-2}..a_i)，</em>使用<em class="nc"> </em> a_i时的平移/旋转矩阵I</p><p id="2cdd" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">然后，这些特征被串联起来<em class="nc"> (x_1^t，…，x_t^t) </em>并馈入3D卷积网络，以创建时空状态<em class="nc"> s_t </em>。</p><p id="5ac3" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">他们用这个状态s_t来参数化两个概率分布:<strong class="lt iu">现在的P和未来的分布F </strong>。当前分布以当前状态<em class="nc"> s_t，</em>为条件，未来分布以当前状态<em class="nc"> s_t </em>以及观察到的未来标记<em class="nc"> (y_{t+1}，…，y_{t+H}) </em>为条件，其中H为未来预测范围。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi og"><img src="../Images/33af5d01676f72dc922f264c9c08385c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NEVJMvEL6HorAaPEPwiKgw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">FIERY型号的输入(a)和标签(b-f)。图片来自论文[1]。</p></figure><p id="f9b7" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">我们可以想象上图中不同的标签。标签包含实例的未来中心度(=在该位置找到实例中心的概率)(b)、偏移量(=指向用于创建分段图的实例中心的向量(c)) (d)以及该实例的流量(=位移矢量场)(e)。</p><p id="3409" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">最终的聚合实例分段图如(f)所示。</p><p id="eccd" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">这两种概率分布是对角高斯分布。因为来自当前分布的任何样本都应该编码可能的未来状态，所以当前分布被推动以覆盖具有KL散度损失的观察到的未来。</p><p id="8744" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">未来分布F是卷积门控递归单元网络，其将当前状态<em class="nc"> s_t </em>和来自F的样本(在训练期间)或来自P的样本(在推断期间)作为输入，并且递归地生成未来状态。</p><p id="9aba" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">通过top-k交叉熵来评估语义分割(仅top-k是因为大多数像素属于背景而没有任何相关信息)。</p><p id="8eec" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><strong class="lt iu">指标:</strong></p><p id="7faa" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">他们用未来视频全景质量评估他们的模型，用于评估<strong class="lt iu">分割实例度量的一致性和准确性</strong>，以及用于评估<strong class="lt iu">模型预测多模态未来的能力</strong>的广义能量距离。</p><p id="293e" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">它们在语义分割任务中优于其他最先进的方法(包括Lift-Splat ),并且在未来实例预测中也优于基线模型。</p><p id="1377" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><strong class="lt iu">结论</strong>:</p><p id="1fc1" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">这一切都是为了摄像机系统的一种最先进的监控方法。要记住的关键是感知和计划是一起完成的，主要依赖于时间状态和多模态。</p></div><div class="ab cl ny nz hx oa" role="separator"><span class="ob bw bk oc od oe"/><span class="ob bw bk oc od oe"/><span class="ob bw bk oc od"/></div><div class="im in io ip iq"><h2 id="4c6d" class="ni la it bd lb nj nk dn lf nl nm dp lj ma nn no ll me np nq ln mi nr ns lp nt bi translated">无人监督的</h2><p id="b59b" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">代码可用<a class="ae ky" href="https://github.com/TRI-ML/packnet-sfm" rel="noopener ugc nofollow" target="_blank">此处</a>。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/387eda80d4d4dc0f3e4191ae7d57e54c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*YmOa_glzrZGgA4bq"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">单幅RGB图像的深度估计。图片来自TRI-AD博客[7]</p></figure><p id="18f8" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">来自TRI-AD的Adrien Gaidon认为<strong class="lt iu">监督学习不会扩展、推广和持续</strong>。这就是为什么他在寻找一种高效的规模化监管方式…而不用贴标签！他们找到了方法:他们使用<strong class="lt iu">自我监督</strong>。</p><p id="8425" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">在没有任何监督标签的情况下，他的TRI-AD人工智能团队可以从单目图像中重建3D点云。</p><p id="14f5" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">怎么可能呢？他们使用<strong class="lt iu">射影几何的先验知识</strong>通过他们的新模型<strong class="lt iu"> PackNet </strong>产生期望的输出。他们取得了非常好的结果，他们的自我监督模型优于监督模型。</p><p id="bfb5" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">自我监督训练不需要任何深度数据。而是训练它合成深度作为中间体。</p><p id="f291" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">他们的方法解决了由于输入图像在通过传统的conv网(由于汇集)后分辨率损失而存在的瓶颈。因此，他们将卷积网络架构应用于深度估计任务。由于张量操作和3D卷积，他们的压缩模型具有保持目标图像分辨率的优势。</p><p id="1df3" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">给定一张图片作为测试时间，他们的目标是学习:</p><ul class=""><li id="8caf" class="mt mu it lt b lu mn lx mo ma mv me mw mi mx mm my mz na nb bi translated">将图像映射到每像素深度的函数<em class="nc"> f_d </em>，</li><li id="2773" class="mt mu it lt b lu nd lx ne ma nf me ng mi nh mm my mz na nb bi translated">单目自我运动估计器<em class="nc"> f_x </em>包含源图像到目标图像的旋转/平移。</li></ul><p id="e74f" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">我们将关注第一个学习目标:深度预测。</p><p id="3fb7" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">深度估计问题是一个训练过程中的图像重建问题，类似于传统的计算机视觉中的运动重建问题。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/64fc548397294dfe401bb64bd4d28467.png" data-original-src="https://miro.medium.com/v2/resize:fit:494/format:webp/1*-LJJzEFS3h3RDlnDXe5Rvg.jpeg"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">传统计算机视觉中的运动结构。图片来自<a class="ae ky" href="https://gsp.humboldt.edu/OLM/Courses/GSP_216_Online/lesson8-2/SfM.html" rel="noopener ugc nofollow" target="_blank">洪堡州立大学</a>。</p></figure><p id="f4d7" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">PackNet模型正在使用带有两个主要模块的单个摄像机学习这种SfM。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oi"><img src="../Images/28729a1e1cfd6601dedb98d0d4a1ee6f.png" data-original-src="https://miro.medium.com/v2/resize:fit:822/format:webp/1*pj70-SKaI_ZZtfUcX7OL0A.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图片来自原纸[2]。</p></figure><ol class=""><li id="a3a3" class="mt mu it lt b lu mn lx mo ma mv me mw mi mx mm nw mz na nb bi translated"><strong class="lt iu">打包块</strong>:用可逆的Space2Depth技术<a class="ae ky" href="https://arxiv.org/pdf/1609.05158.pdf" rel="noopener ugc nofollow" target="_blank">【6】</a>对输入的RGB图像张量进行缩减，然后网络学习压缩，学习用3D conv层进行扩展。结果通过整形变平，并馈送到2D卷积层。</li><li id="6eec" class="mt mu it lt b lu nd lx ne ma nf me ng mi nh mm nw mz na nb bi translated"><strong class="lt iu">解包块</strong>学习解压缩和展开打包的卷积特征回到更高的分辨率。它再次使用2D，然后三维层，然后通过深度2空间技术[6]重塑和扩展。</li></ol><p id="c7c5" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">在训练期间，网络通过从源图像中采样像素来学习生成图像<em class="nc">t</em>。</p><p id="861f" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><strong class="lt iu">损失</strong></p><p id="2b34" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">它们的深度映射损失分为两个部分:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/41387ccff0bc23fc681688a84b270196.png" data-original-src="https://miro.medium.com/v2/resize:fit:952/format:webp/1*dkS8spPhHDUm9ibVFaJ_QA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">深度映射的损失。图片由作者提供。</p></figure><p id="f5e0" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><strong class="lt iu">外观匹配损失L_p </strong>:使用结构相似度项和L1损失项评估目标图像I_t和合成图像t之间的像素相似度</p><p id="1729" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><strong class="lt iu">深度正则化损失L_s </strong>:他们鼓励估计的深度图在梯度上用L1罚函数进行局部平滑。因为在对象边缘上存在深度不连续，为了避免丢失无纹理的低梯度区域，当图像梯度高时，该平滑被加权为较低。</p><p id="7270" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">我们在这里不涉及这一点，但它们的损失利用相机速度来解决单目视觉固有的比例模糊。</p><p id="b49a" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><strong class="lt iu">结果:</strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ok"><img src="../Images/4946df4d4eedcdd6f852c294b144bb1c.png" data-original-src="https://miro.medium.com/v2/resize:fit:838/format:webp/1*vCrbBCnWsW6-BaOcP4uSuw.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图像重建与传统流水线(b)和提出的方法(c)的比较。所提出的方法保留了细节。图片来自原纸[2]。</p></figure><p id="0730" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">在著名的KITTI基准测试中，他们的模型优于self、semi和fully supervised方法。</p><p id="7be9" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><strong class="lt iu"> 2021年更新:</strong></p><p id="b046" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">他们最近通过他们的新2021型号扩展到了360度摄像机配置:<em class="nc">来自多个摄像机的全环绕单深度，</em>熊伟·圭齐利尼等人。</p><p id="4abb" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">为了做到这一点，他们非常狡猾地利用时空信息。他们使用的六个摄像头重叠太少，无法在另一个摄像头(摄像头B)的帧中重建一个摄像头(摄像头A)的图像。为了解决这个问题，他们实际上使用来自摄像机A的过去帧的图像来投影到摄像机b的当前帧。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ol"><img src="../Images/0c14122382bbdf21059655eac38368d2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OojFlasjr9cTkt0W9jkKVQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">FSM模型从6个相机装备配置(如左图所示)预测点云(右图)。图片来自原始纸张[2]</p></figure><p id="aaed" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><strong class="lt iu">半监督</strong> =自我监督+稀疏数据。</p><p id="f4ca" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">今年，TRI-AD还提出了一个半监督推理网络:<em class="nc">“用于统一单目深度预测和完成的稀疏辅助网络(SANs)</em>，熊伟·圭齐利尼等人，这些SANs可以根据推理时只有RGB图像还是稀疏点云来执行深度预测和完成。</p><h1 id="9a17" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">第2)节使用激光雷达和高清地图</h1><p id="4672" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">自动驾驶汽车最初使用激光雷达、激光传感器和高清地图来预测和规划它们的运动。近年来，多任务深度学习的使用创造了利用激光雷达技术导航的端到端模型。</p><p id="1493" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><strong class="lt iu">为什么使用高清地图？</strong>高清地图包含关于语义场景的信息(车道、位置停车标志等)。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi om"><img src="../Images/05105fbd638144e12edf0d8a70aab9d7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Obf7eEW0sWVNG6SQKWjdag.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">《感知、预测、规划》一文中提出的整体架构。图片来自论文[3]。</p></figure><p id="58e4" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">我来呈上优步·ATG在《ECCV 2020》上发表的一篇论文:“<strong class="lt iu"> <em class="nc">【感知、预测、规划】</em></strong>【3】。这篇论文是由作者之一拉奎尔·乌尔萨顿提交的，她今年资助了自己的广告初创公司，名为<strong class="lt iu"> Waadi </strong>。</p><p id="dd2c" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">她的论文提出了一个端到端的模型，它联合感知、预测和规划汽车的运动。他们创建了一个新的中间表示来学习他们的目标函数:<strong class="lt iu">一个语义占用网格</strong>来评估运动规划过程中每个轨迹的成本。这个语义层也被用作中间的和可解释的结果。这种网格使得广告比传统方法更安全，因为它不依赖于阈值来检测对象，并且可以检测任何形状</p><p id="d566" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">他们的模型被分成三块。</p><ol class=""><li id="552b" class="mt mu it lt b lu mn lx mo ma mv me mw mi mx mm nw mz na nb bi translated"><strong class="lt iu">感知块</strong></li></ol><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi on"><img src="../Images/e8aacb076ed39e2f3a12efc56572e5bf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8gA4IzqlHd3n9uJco6EAjw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">端到端感知和循环占用模型。||是串联、+元素式求和以及用于缩减的增量双线性插值。图片来自论文[3]。</p></figure><p id="2ff5" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">感知模型首先从激光雷达测量和高清地图中独立提取特征。为此，他们将激光雷达的10次连续扫描体素化为T=10帧，并将其转换为BEV(鸟瞰图)中的当前汽车帧。这将创建一个3D离散格网，每个像元都有一个二进制值:被占用或为空。</p><p id="010b" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">它们在Z轴上连接时间轴以获得(HxWxZT)张量。第三轴上的级联允许以后使用2D卷积骨干网。</p><p id="170f" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">地图的信息存储在M通道张量中。每个通道都包含不同的地图元素(道路、车道、停车标志等)。那么最后的输入张量就是<strong class="lt iu"> HxWx(ZT+M) </strong>。实际上，ZT+M=17个二进制通道。</p><p id="b5c7" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">然后，输入张量被送入由两个流组成的主干网络。一个流分别用于激光雷达和地图要素。这些流仅在使用的要素数量上有所不同(激光雷达流的要素越多)。输出被连接并馈入卷积层的最后一个块，以输出256维特征。</p><p id="9d58" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><strong class="lt iu"> 2。预测块</strong></p><p id="39e8" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">用于预测的语义类被组织成分层的组。每组都有不同的计划成本(停放的车辆没有移动的车辆重要)。这些组中的每一组都被表示为空间和时间上的分类随机变量的集合(对于x，y网格为0.4m/像素，时间为0.5s(因此10次扫描产生5s窗口)。简而言之，预测的目标是回答这个问题:谁(哪个类的哪个实例)将移动到哪里？</p><p id="0c4a" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">他们再次使用2D卷积块，主要是两个具有不同膨胀率的平行流。一个具有细粒度特征的流针对<strong class="lt iu">最近未来</strong>的预测。另一个流使用具有膨胀卷积的更粗糙的特征用于<strong class="lt iu">长期</strong>预测。使用先前的输出和连接的要素以循环方式更新输出。</p><p id="1da9" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><strong class="lt iu"> 3。运动规划模块</strong></p><p id="3393" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">他们从自我车的各种轨迹中取样，并选择一个最小化学习成本函数的轨迹。该成本函数是主要考虑语义占用预测的成本函数:<em class="nc"> fo </em>和与舒适安全和交通规则相关的<em class="nc"> fr </em>的总和。</p><p id="3a1f" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">fo由两项组成:第一项惩罚与高概率区域相交的轨迹，第二项惩罚在不确定占有区域中的高速运动。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oo"><img src="../Images/63296d163ed46e90395fe6553bc38949.png" data-original-src="https://miro.medium.com/v2/resize:fit:524/format:webp/1*Mv2AxMZgxFz-ZrQHs1Dbzg.png"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ok"><img src="../Images/242f6c14e95c80d45a8d031e6ee8daf5.png" data-original-src="https://miro.medium.com/v2/resize:fit:838/format:webp/1*OcEBmihihrS3ELcnv2-O7w.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">f是成本函数，τ是轨迹，x是输入数据，o是占用预测，w是可学习的参数。[3]</p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi op"><img src="../Images/6c2db9d50cf3c6ff8a388d4cf7543a7b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1006/format:webp/1*RuBuj-7tJcIFPrFeqIMqyQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">t是时间步长，c是占据网格的单元，τ是轨迹，λ是添加到周围物体以避免碰撞的余量参数，v是速度。[3]</p></figure><p id="e0dd" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">这些成本函数用于最终的多任务目标函数:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oq"><img src="../Images/d0c30fa4afb4abc58eceaa5fe7a9ebe9.png" data-original-src="https://miro.medium.com/v2/resize:fit:404/format:webp/1*0RyYDwjec4zmP8-e1GPu2g.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">语义占用损失和规划损失之和。[3]</p></figure><p id="809a" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><strong class="lt iu">语义占据损失</strong> <em class="nc"> L_s </em>是语义占据随机变量的地面分布<em class="nc"> p </em>和预测分布<em class="nc"> q </em>之间的交叉熵损失。</p><p id="44de" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><strong class="lt iu">规划损失</strong> <em class="nc"> L_M </em>是一种最大边际损失，鼓励人类驾驶轨迹(地面实况)比其他轨迹具有更小的成本。</p><p id="881a" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><strong class="lt iu">结果</strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi or"><img src="../Images/168c7ec489617ffdbf5c06dafb390455.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*v273FaJqfDgDwcjU_FKcqg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">导致5s帧。图片来自报纸[3]</p></figure><p id="d36e" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">因此，在碰撞数量方面，将占用网格表示添加到模型优于最先进的方法。对整个管道进行端到端(而不是一个接一个)的训练提高了安全性(10%)和人类模仿性(5%)。</p></div><div class="ab cl ny nz hx oa" role="separator"><span class="ob bw bk oc od oe"/><span class="ob bw bk oc od oe"/><span class="ob bw bk oc od"/></div><div class="im in io ip iq"><h1 id="4923" class="kz la it bd lb lc os le lf lg ot li lj jz ou ka ll kc ov kd ln kf ow kg lp lq bi translated">结论</h1><p id="39be" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">我简要地探讨了广告的最新趋势，概述了最近发表的三篇最先进的论文。</p><p id="65f5" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><strong class="lt iu">2021年感知和运动规划的趋势是:</strong></p><ul class=""><li id="9fda" class="mt mu it lt b lu mn lx mo ma mv me mw mi mx mm my mz na nb bi translated">端到端模型优于顺序模型，</li><li id="a0c1" class="mt mu it lt b lu nd lx ne ma nf me ng mi nh mm my mz na nb bi translated">相机VS激光雷达，</li><li id="db91" class="mt mu it lt b lu nd lx ne ma nf me ng mi nh mm my mz na nb bi translated">学习深度如果你只依靠相机，</li><li id="6208" class="mt mu it lt b lu nd lx ne ma nf me ng mi nh mm my mz na nb bi translated">多智能体与自我车的纠缠互动。</li></ul><p id="00d4" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">许多生产级自动驾驶公司发布了他们最近进展的详细研究论文。此外，我们应该利用他们有时将代码作为开源库发布的优势。</p><p id="8349" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><strong class="lt iu">感谢您的阅读！</strong></p><p id="e5b8" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><strong class="lt iu">中上关注我</strong> <a class="ae ky" href="https://twitter.com/ciel_delem" rel="noopener ugc nofollow" target="_blank"> <strong class="lt iu">推特</strong> </a> <strong class="lt iu">！😃</strong></p><h1 id="23ec" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">参考资料:</h1><p id="04e2" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">[1] <em class="nc">火热:从环绕单目摄像机鸟瞰视角预测未来实例，</em> Anthony Hu等<a class="ae ky" href="https://arxiv.org/abs/2104.10490" rel="noopener ugc nofollow" target="_blank"/></p><p id="2867" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">[2]“打包:3 <em class="nc">用于自监督单目深度估计的3d打包。</em>(CVPR 2020)，熊伟·吉齐利尼，拉雷斯·安布鲁斯，苏迪普·皮莱，艾伦·拉文托斯，阿德里安·盖登。<a class="ae ky" href="https://arxiv.org/abs/1905.02693" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1905.02693</a></p><p id="5693" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">[3] " <em class="nc">感知、预测和规划:通过可解释的语义表示进行安全运动规划</em>"ECCV 2020。萨达特、卡萨斯、任、吴、普拉纳布·达万和r .乌尔塔松、<a class="ae ky" href="https://arxiv.org/abs/2008.05930" rel="noopener ugc nofollow" target="_blank">和</a></p><p id="b94b" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">[4] " <em class="nc">举起、拍打、拍摄:通过隐式反投影将来自任意相机装备的图像编码为3D，</em> ECCV 2020，<em class="nc">T23】乔纳·菲利翁，桑亚·菲德勒，<em class="nc"/><a class="ae ky" href="https://arxiv.org/abs/2008.05711" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/2008.05711</a></em></p><p id="def2" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">[5]https://youtu.be/eOL_rCK59ZI<a class="ae ky" href="https://youtu.be/eOL_rCK59ZI" rel="noopener ugc nofollow" target="_blank">CVPR 2021年自动驾驶研讨会</a></p><p id="0d2b" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">[6] " <em class="nc">利用高效的亚像素卷积神经网络实现单幅图像和视频的实时超分辨率"</em>，2016，施，何塞·卡瓦列罗，费伦茨·胡斯萨尔，约翰内斯·托茨，安德鲁·p·艾特肯，罗布·毕晓普，丹尼尔·吕克尔特，，<a class="ae ky" href="https://arxiv.org/abs/1609.05158" rel="noopener ugc nofollow" target="_blank">，</a>。</p><p id="d740" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">[7]<a class="ae ky" href="https://medium.com/toyotaresearch/self-supervised-learning-in-depth-part-1-of-2-74825baaaa04" rel="noopener">https://medium . com/Toyota research/self-supervised-learning-in-depth-part-1-of-2-74825 baaaa 04</a></p></div></div>    
</body>
</html>