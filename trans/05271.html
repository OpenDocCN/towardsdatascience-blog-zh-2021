<html>
<head>
<title>Transformers — You just need Attention</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">变形金刚——你只需要关注</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/transformers-you-just-need-attention-f3775734c0da?source=collection_archive---------20-----------------------#2021-05-09">https://towardsdatascience.com/transformers-you-just-need-attention-f3775734c0da?source=collection_archive---------20-----------------------#2021-05-09</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi jn"><img src="../Images/094a2150d555e67f540b5a3eeb738d3d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xEOLl2-qU5fQCzkzEybNag.jpeg"/></div></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">保罗·斯科鲁普斯卡斯在<a class="ae kc" href="https://unsplash.com/s/photos/camera-focus?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="cab7" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">自然语言处理或NLP是处理文本分析的机器学习的子集。它与人类语言和计算机的相互作用有关。有不同的NLP技术，最新的是最先进的变压器。</p><p id="951a" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这将是第一个关于变形金刚架构的博客系列。事实证明，变形金刚在自然语言处理(NLP)领域是革命性的。自发明以来，它已经取代了所有其他自然语言处理(NLP)架构，如递归神经网络(RNN)、卷积神经网络(CNN)和长短期记忆(LSTM)。变压器有一个编码器-解码器的结构和一定的注意机制，使国家的艺术成果在许多任务。</p><p id="6233" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在这篇文章中我们会学到什么？</p><ul class=""><li id="fcf2" class="lb lc iq kf b kg kh kk kl ko ld ks le kw lf la lg lh li lj bi translated">这篇博文主要关注编码器部分，它的所有组件都将被详细讨论。在本帖中，将从理论和实践两个方面讨论编码器的内部细节(包括代码笔记本)。</li><li id="8dce" class="lb lc iq kf b kg lk kk ll ko lm ks ln kw lo la lg lh li lj bi translated">编码器结构</li><li id="3553" class="lb lc iq kf b kg lk kk ll ko lm ks ln kw lo la lg lh li lj bi translated"><strong class="kf ir">编码器的关键部件</strong>或者说编码器中的数据流</li><li id="74a6" class="lb lc iq kf b kg lk kk ll ko lm ks ln kw lo la lg lh li lj bi translated">输入-转换为嵌入。</li><li id="c2a5" class="lb lc iq kf b kg lk kk ll ko lm ks ln kw lo la lg lh li lj bi translated">位置编码</li><li id="e034" class="lb lc iq kf b kg lk kk ll ko lm ks ln kw lo la lg lh li lj bi translated"><strong class="kf ir">多头关注层(自我关注机制)</strong></li><li id="89cf" class="lb lc iq kf b kg lk kk ll ko lm ks ln kw lo la lg lh li lj bi translated">剩余网络</li><li id="f454" class="lb lc iq kf b kg lk kk ll ko lm ks ln kw lo la lg lh li lj bi translated"><strong class="kf ir">添加和定额层</strong></li><li id="5c3a" class="lb lc iq kf b kg lk kk ll ko lm ks ln kw lo la lg lh li lj bi translated"><strong class="kf ir">前馈层。</strong></li></ul><h1 id="06ae" class="lp lq iq bd lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm bi translated">架构概述</h1><figure class="mo mp mq mr gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi mn"><img src="../Images/4a2c1e6683f50ec9c21bcc9982e01d84.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*Wj2Hl3h5PwY_6CO0.png"/></div></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">根据流程，变压器的不同部分</p></figure><p id="71a5" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这篇博文将集中在上面这张图片的左边部分。</p><ol class=""><li id="c50d" class="lb lc iq kf b kg kh kk kl ko ld ks le kw lf la ms lh li lj bi translated">输入预处理</li><li id="6325" class="lb lc iq kf b kg lk kk ll ko lm ks ln kw lo la ms lh li lj bi translated">编码器</li></ol><h1 id="1769" class="lp lq iq bd lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm bi translated">1.输入预处理</h1><p id="9d3a" class="pw-post-body-paragraph kd ke iq kf b kg mt ki kj kk mu km kn ko mv kq kr ks mw ku kv kw mx ky kz la ij bi translated">让我们放大输入预处理图像，如下所示。</p><figure class="mo mp mq mr gt jr gh gi paragraph-image"><div class="gh gi my"><img src="../Images/246b0028eb8a02cc4025fc75fa05d8f9.png" data-original-src="https://miro.medium.com/v2/resize:fit:496/format:webp/0*rU8SZE1de9X2XOe3.png"/></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">位置编码器</p></figure><p id="abe7" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">举个例子:- <strong class="kf ir">《我回家了》</strong>。如果这个句子作为输入被传递到编码器，让我们看看流程。</p><h2 id="f44f" class="mz lq iq bd lr na nb dn lv nc nd dp lz ko ne nf md ks ng nh mh kw ni nj ml nk bi translated">步骤1:-单词嵌入</h2><p id="ddd7" class="pw-post-body-paragraph kd ke iq kf b kg mt ki kj kk mu km kn ko mv kq kr ks mw ku kv kw mx ky kz la ij bi translated">电脑看不懂英文字母，需要转换成数字版本。因此，句子中的每个单词都被转换成一个相应的恒定大小(维数)的随机数向量。这些向量被称为<strong class="kf ir">嵌入向量。</strong>新矩阵将是<strong class="kf ir">【句子长度，尺寸】</strong>形状。</p><figure class="mo mp mq mr gt jr gh gi paragraph-image"><div class="gh gi nl"><img src="../Images/1910bf3f04620bfac38003e61d244ff4.png" data-original-src="https://miro.medium.com/v2/resize:fit:780/format:webp/0*PmOlFZQ6pvMy4aEG.png"/></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">词的嵌入表征</p></figure><p id="d843" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在上面的例子中，我们有3个单词，每个单词都有一个长度为4的嵌入向量。于是，Shape=(3，4) <strong class="kf ir">【句子_长度，维度】</strong>。在自然语言处理中，单词嵌入中的每个数字都具有与单词的语言特征相关的信息。如果我们把两个单词的单词嵌入在一起，比如说<strong class="kf ir">【灰色】</strong><strong class="kf ir">【白色】</strong>既然这些都是颜色，那么它们在n维空间中就会彼此靠近。这些单词嵌入最初是随机数，并且在训练过程中它们得到更新并且看起来彼此更接近或相似，因为它们属于<strong class="kf ir">颜色</strong>的类别。</p><h2 id="ad29" class="mz lq iq bd lr na nb dn lv nc nd dp lz ko ne nf md ks ng nh mh kw ni nj ml nk bi translated">步骤2:-位置编码</h2><p id="4734" class="pw-post-body-paragraph kd ke iq kf b kg mt ki kj kk mu km kn ko mv kq kr ks mw ku kv kw mx ky kz la ij bi translated">在LSTM RNN，单词是按顺序输入的，因此它能理解单词的顺序。但是在transformer中，所有的单词都是并行传递的。随着刑期的增加，在LSTM复发将需要大量手术。为了记住单词的顺序，引入了<strong class="kf ir">位置编码</strong>的概念。这是一种表示单词位置的编码。简而言之，<strong class="kf ir">将</strong>位置编码<strong class="kf ir">添加到我们的<strong class="kf ir">现有字嵌入</strong>中，这就给出了最终的<strong class="kf ir">预处理嵌入</strong>以用于编码器部分。位置编码的概念很重要，我会写一篇关于它的单独的博客文章并链接到这里。</strong> </p><p id="1885" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">注意:<strong class="kf ir">位置嵌入</strong>的大小应该和我们的<strong class="kf ir">现有的字嵌入</strong>一样，就是这么加起来的。</p><h1 id="9c65" class="lp lq iq bd lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm bi translated">2.编码器堆栈:-</h1><figure class="mo mp mq mr gt jr gh gi paragraph-image"><div class="gh gi nm"><img src="../Images/3afdd186cb8deb582e2d1756f29a8618.png" data-original-src="https://miro.medium.com/v2/resize:fit:440/format:webp/0*bxxmnxL6hlplMoNW.png"/></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">编码器堆栈</p></figure><p id="8433" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">可以有N个编码器，但它们的内部结构是相同的。我们之前讨论的嵌入将通过4个单元:-</p><ol class=""><li id="d2d6" class="lb lc iq kf b kg kh kk kl ko ld ks le kw lf la ms lh li lj bi translated">多头注意力</li><li id="ebdd" class="lb lc iq kf b kg lk kk ll ko lm ks ln kw lo la ms lh li lj bi translated">添加和规范层</li><li id="a65e" class="lb lc iq kf b kg lk kk ll ko lm ks ln kw lo la ms lh li lj bi translated">前馈层</li><li id="42c9" class="lb lc iq kf b kg lk kk ll ko lm ks ln kw lo la ms lh li lj bi translated">添加和规范</li></ol><h2 id="493d" class="mz lq iq bd lr na nb dn lv nc nd dp lz ko ne nf md ks ng nh mh kw ni nj ml nk bi translated">1.多头注意力</h2><p id="8a07" class="pw-post-body-paragraph kd ke iq kf b kg mt ki kj kk mu km kn ko mv kq kr ks mw ku kv kw mx ky kz la ij bi translated">注意机制是一种为每个单词创建表示的方式，其中句子中的每个单词理解与同一句子中所有其他单词(包括其自身)的语义相似性。这是通过点积运算实现的。文中8个注意力头并行运行，所以多头注意力。它实际上有助于获得句子中所有单词之间的语义相似关系。输出嵌入将由上下文信息组成，并将显示每个单词如何与句子中的其他单词相关。它可以处理歧义，如下所示。这里的“<strong class="kf ir"> it </strong>”与“<strong class="kf ir">大象”</strong>的关联度大于“<strong class="kf ir">香蕉”</strong>。</p><figure class="mo mp mq mr gt jr gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/fa744e50f8f40c2160e7cf11e48547df.png" data-original-src="https://miro.medium.com/v2/resize:fit:1010/format:webp/0*369IQynlzaq8psQm.png"/></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">自我注意机制中的词汇表征</p></figure><p id="19e4" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">多头注意力机制将在更详细的<a class="ae kc" href="https://machinelearningmarvel.in/intuitive-maths-and-code-behind-self-attention-mechanism-of-transformers-for-dummies/" rel="noopener ugc nofollow" target="_blank"><strong class="kf ir"/></a><strong class="kf ir">中讨论。我会写一篇关于它的独立博文，并把它链接到这里。</strong>T9】</p><h2 id="c6e8" class="mz lq iq bd lr na nb dn lv nc nd dp lz ko ne nf md ks ng nh mh kw ni nj ml nk bi translated">2.添加和规范层</h2><p id="7ad6" class="pw-post-body-paragraph kd ke iq kf b kg mt ki kj kk mu km kn ko mv kq kr ks mw ku kv kw mx ky kz la ij bi translated">编码器的输出嵌入通过残差网络被添加到原始输入嵌入。残差的目的是确保原始重要信息在通过编码器的过程中不会丢失。然后这被归一化。</p><h2 id="6b78" class="mz lq iq bd lr na nb dn lv nc nd dp lz ko ne nf md ks ng nh mh kw ni nj ml nk bi translated">3.前馈层</h2><p id="805a" class="pw-post-body-paragraph kd ke iq kf b kg mt ki kj kk mu km kn ko mv kq kr ks mw ku kv kw mx ky kz la ij bi translated">然后，归一化的嵌入通过完全连接的层。</p><h2 id="b785" class="mz lq iq bd lr na nb dn lv nc nd dp lz ko ne nf md ks ng nh mh kw ni nj ml nk bi translated">4.添加和规范层</h2><p id="9836" class="pw-post-body-paragraph kd ke iq kf b kg mt ki kj kk mu km kn ko mv kq kr ks mw ku kv kw mx ky kz la ij bi translated">前馈层的输出通过残差网络被添加到编码器的输出嵌入中。残差网络的目的是确保原始重要信息在通过编码器的过程中不会丢失。然后这被归一化。</p><p id="748f" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">以上四个步骤是编码器的一部分，并且重复多次(<strong class="kf ir"> Nx </strong>如图所示)，这给了我们纯粹的上下文嵌入。</p><h2 id="dc9d" class="mz lq iq bd lr na nb dn lv nc nd dp lz ko ne nf md ks ng nh mh kw ni nj ml nk bi translated">赞成的意见</h2><p id="5300" class="pw-post-body-paragraph kd ke iq kf b kg mt ki kj kk mu km kn ko mv kq kr ks mw ku kv kw mx ky kz la ij bi translated">它在大多数NLP任务中提供了最先进的结果。即使是基本的预训练模型也能给出很好的结果。</p><h2 id="8cc6" class="mz lq iq bd lr na nb dn lv nc nd dp lz ko ne nf md ks ng nh mh kw ni nj ml nk bi translated">骗局</h2><p id="3187" class="pw-post-body-paragraph kd ke iq kf b kg mt ki kj kk mu km kn ko mv kq kr ks mw ku kv kw mx ky kz la ij bi translated">建筑太大了。它有几百万个参数需要训练，并且需要大量的计算能力来训练。下一篇文章将展示解码器部分。直到那时</p><figure class="mo mp mq mr gt jr"><div class="bz fp l di"><div class="no np l"/></div></figure></div></div>    
</body>
</html>