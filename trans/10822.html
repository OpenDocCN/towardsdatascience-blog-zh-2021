<html>
<head>
<title>Graph Neural Networks: a learning journey since 2008 — Python &amp; Deep Walk</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">图形神经网络:2008年以来的学习之旅——Python和Deep Walk</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/graph-neural-networks-a-learning-journey-since-2008-python-deep-walk-29c3e31432f?source=collection_archive---------31-----------------------#2021-10-19">https://towardsdatascience.com/graph-neural-networks-a-learning-journey-since-2008-python-deep-walk-29c3e31432f?source=collection_archive---------31-----------------------#2021-10-19</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="42f4" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsdatascience.com/tagged/hands-on-tutorials" rel="noopener" target="_blank">实践教程</a></h2><div class=""/><div class=""><h2 id="4cce" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">本系列的第四部分。今天，深度行走的实际实现🐍看看脸书的大型页面数据集👍</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/dc383d35ad231798bd17220407960582.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7o35iXZDALxab2Wb0OEYIQ.jpeg"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">图片由<a class="ae lh" href="https://unsplash.com/@franhotchin" rel="noopener ugc nofollow" target="_blank">弗朗西丝卡·霍钦</a>在<a class="ae lh" href="https://unsplash.com/photos/FN-cedy6NHA" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><div class="li lj gp gr lk ll"><a href="https://medium.com/@stefanobosisio1/membership" rel="noopener follow" target="_blank"><div class="lm ab fo"><div class="ln ab lo cl cj lp"><h2 class="bd jd gy z fp lq fr fs lr fu fw jc bi translated">通过我的推荐链接加入Medium-Stefano Bosisio</h2><div class="ls l"><h3 class="bd b gy z fp lq fr fs lr fu fw dk translated">作为一个媒体会员，你的会员费的一部分会给你阅读的作家，你可以完全接触到每一个故事…</h3></div><div class="lt l"><p class="bd b dl z fp lq fr fs lr fu fw dk translated">medium.com</p></div></div><div class="lu l"><div class="lv l lw lx ly lu lz lb ll"/></div></div></a></div><p id="ff84" class="pw-post-body-paragraph ma mb it mc b md me kd mf mg mh kg mi mj mk ml mm mn mo mp mq mr ms mt mu mv im bi translated">欢迎回到我们在图形机器学习世界的旅程的第四部分。在之前的帖子中，我们看到了<a class="ae lh" rel="noopener" target="_blank" href="/graph-neural-networks-a-learning-journey-since-2008-deep-walk-e424e716070a?source=user_profile---------0----------------------------">deep walk算法如何工作</a>及其数学背景。今天，我们将深入研究该算法的Python实现，从一个简单的例子(空手道俱乐部)开始，在<code class="fe mw mx my mz b">Numpy</code>中有一个简单的DeepWalk版本，然后我们将跳转到一个更有趣的场景，使用脸书大型页面数据集。</p><p id="6504" class="pw-post-body-paragraph ma mb it mc b md me kd mf mg mh kg mi mj mk ml mm mn mo mp mq mr ms mt mu mv im bi translated">以下是前几集的列表:</p><ul class=""><li id="44c1" class="na nb it mc b md me mg mh mj nc mn nd mr ne mv nf ng nh ni bi translated"><a class="ae lh" rel="noopener" target="_blank" href="/graph-neural-networks-a-learning-journey-since-2008-part-1-7df897834df9">斯卡塞利·GNN的理论背景</a></li><li id="eef3" class="na nb it mc b md nj mg nk mj nl mn nm mr nn mv nf ng nh ni bi translated"><a class="ae lh" rel="noopener" target="_blank" href="/graph-neural-networks-a-learning-journey-since-2008-part-2-22dbf7a3b0d">斯卡塞利GNN实际实施</a></li><li id="7458" class="na nb it mc b md nj mg nk mj nl mn nm mr nn mv nf ng nh ni bi translated"><a class="ae lh" rel="noopener" target="_blank" href="/graph-neural-networks-a-learning-journey-since-2008-deep-walk-e424e716070a?source=user_profile---------0----------------------------"> DeepWalk理论背景</a></li></ul><h1 id="1def" class="no np it bd nq nr ns nt nu nv nw nx ny ki nz kj oa kl ob km oc ko od kp oe of bi translated">串联所有深走数学步骤</h1><h2 id="372b" class="og np it bd nq oh oi dn nu oj ok dp ny mj ol om oa mn on oo oc mr op oq oe iz bi translated">生成“序列”的语料库</h2><p id="2039" class="pw-post-body-paragraph ma mb it mc b md or kd mf mg os kg mi mj ot ml mm mn ou mp mq mr ov mt mu mv im bi translated">现在我们知道了DeepWalk在数学上是如何工作的，让我们一起一步一步地看看空手道俱乐部将会发生什么，在跳到更大的应用之前修正我们的想法。当前的应用程序基于一个简单的<code class="fe mw mx my mz b">numpy</code>嵌入式神经网络应用程序。代码和输入文件可在以下位置找到:</p><div class="li lj gp gr lk ll"><a href="https://github.com/Steboss/learn_graph_ml/tree/master/Perozzi_DeepWalk" rel="noopener  ugc nofollow" target="_blank"><div class="lm ab fo"><div class="ln ab lo cl cj lp"><h2 class="bd jd gy z fp lq fr fs lr fu fw jc bi translated">learn _ graph _ ml/Perozzi _ deep walk at master ste boss/learn _ graph _ ml</h2><div class="ls l"><h3 class="bd b gy z fp lq fr fs lr fu fw dk translated">从2009年到现在的漫长旅程，带着图表——learn _ graph _ ml/Perozzi _ deep walk at master ste boss/learn _ graph _ ml</h3></div><div class="lt l"><p class="bd b dl z fp lq fr fs lr fu fw dk translated">github.com</p></div></div><div class="lu l"><div class="ow l lw lx ly lu lz lb ll"/></div></div></a></div><p id="fd60" class="pw-post-body-paragraph ma mb it mc b md me kd mf mg mh kg mi mj mk ml mm mn mo mp mq mr ms mt mu mv im bi translated">输入数据集是<code class="fe mw mx my mz b">data/karate.adjlist</code>，空手道俱乐部的邻接表，<code class="fe mw mx my mz b">graph.py</code>是读取输入文件的实用程序，<code class="fe mw mx my mz b">karate.py</code>是我们的主要DeepWalk算法文件。这些计算远非正确，但它们是具体理解正在发生的事情的一个很好的方法。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ox"><img src="../Images/4fb873c744e39754d9dd6123aef3066f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cAMivP0vnBrgUaq-G8PUZg.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">图1空手道俱乐部图表。该俱乐部由34名成员组成，我们可以定义78种相互之间的交互。</p></figure><p id="31f1" class="pw-post-body-paragraph ma mb it mc b md me kd mf mg mh kg mi mj mk ml mm mn mo mp mq mr ms mt mu mv im bi translated">在这个例子中，我们有34个节点和78条边。对于每个节点，随机行走的次数是<code class="fe mw mx my mz b">number_walks=5</code>，因此总共170次随机行走，即从一个起点开始探索一个<code class="fe mw mx my mz b">walk_length=5</code>顶点。在第378行，我们正在创建初始语料库:</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="oy oz l"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">图2:长度为5的随机游走的“句子”语料库的生成</p></figure><p id="107b" class="pw-post-body-paragraph ma mb it mc b md me kd mf mg mh kg mi mj mk ml mm mn mo mp mq mr ms mt mu mv im bi translated">从这里我们会有一个随机漫步的列表:</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="oy oz l"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">图3:从顶点22、17、13、18、11和15开始的长度为5的随机行走的例子。</p></figure><h2 id="7ccd" class="og np it bd nq oh oi dn nu oj ok dp ny mj ol om oa mn on oo oc mr op oq oe iz bi translated">Skipgram模型</h2><p id="32d8" class="pw-post-body-paragraph ma mb it mc b md or kd mf mg os kg mi mj ot ml mm mn ou mp mq mr ov mt mu mv im bi translated">一旦生成了语料库，我们就可以继续使用SkipGram模型。在这个例子中，我没有实现层次化的Softmax，但我只是使用了一个简单的softmax层，因为问题很小。这些是我们需要的步骤:</p><ul class=""><li id="7268" class="na nb it mc b md me mg mh mj nc mn nd mr ne mv nf ng nh ni bi translated"><em class="pa">生成训练数据</em>:对于每个输入序列，我们需要为训练阶段生成一对<em class="pa"> (X，y) </em>。<em class="pa"> X </em>是当前顶点，而<em class="pa"> y </em>是来自<em class="pa"> X </em>的一个<code class="fe mw mx my mz b">window_size</code>内的字</li><li id="2771" class="na nb it mc b md nj mg nk mj nl mn nm mr nn mv nf ng nh ni bi translated"><em class="pa">参数初始化:</em>随机初始化一个单词嵌入矩阵，并执行2-隐层神经网络的前向步骤</li><li id="bc96" class="na nb it mc b md nj mg nk mj nl mn nm mr nn mv nf ng nh ni bi translated"><em class="pa">神经网络:</em>更新权重和返回训练成本的函数，以向正确的嵌入收敛。</li></ul><h2 id="4e85" class="og np it bd nq oh oi dn nu oj ok dp ny mj ol om oa mn on oo oc mr op oq oe iz bi translated">生成训练数据</h2><p id="7aaf" class="pw-post-body-paragraph ma mb it mc b md or kd mf mg os kg mi mj ot ml mm mn ou mp mq mr ov mt mu mv im bi translated">训练数据强调了SkipGram模型的目标:<code class="fe mw mx my mz b">X</code>是输入单词，并且该模型预测在最大<code class="fe mw mx my mz b">window_size</code>距离处的单词成为<code class="fe mw mx my mz b">X</code>的邻居的可能性。最终的训练数据集将是一组对<code class="fe mw mx my mz b">(X, y)</code>。例如，给定输入序列<code class="fe mw mx my mz b">22, 1, 13, 4, 3</code>和<code class="fe mw mx my mz b">wind_size=5</code>，对<code class="fe mw mx my mz b">(X,y)</code>将是(例如):<code class="fe mw mx my mz b">(22, 1), (22, 13), (22, 4), (22, 3), (1, 22), (1, 13), (1, 4), (1, 3), (13, 22), (13, 1), (13, 4)</code>等。<a class="ae lh" href="https://github.com/Steboss/learn_graph_ml/blob/e0c7f6e319919c7a9a75edd8204f8c4163ef2d9c/Perozzi_DeepWalk/karate.py#L105" rel="noopener ugc nofollow" target="_blank">函数</a> <code class="fe mw mx my mz b"><a class="ae lh" href="https://github.com/Steboss/learn_graph_ml/blob/e0c7f6e319919c7a9a75edd8204f8c4163ef2d9c/Perozzi_DeepWalk/karate.py#L105" rel="noopener ugc nofollow" target="_blank">generate_training_data</a></code> <a class="ae lh" href="https://github.com/Steboss/learn_graph_ml/blob/e0c7f6e319919c7a9a75edd8204f8c4163ef2d9c/Perozzi_DeepWalk/karate.py#L105" rel="noopener ugc nofollow" target="_blank">完成了这个目标</a>，循环遍历所有输入<code class="fe mw mx my mz b">random_walks</code>及其元素，选择左右索引<code class="fe mw mx my mz b">idxs_left</code>和<code class="fe mw mx my mz b">idxs_right</code></p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="oy oz l"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">图4:生成训练数据功能。Skipgram模型的对(X，y)是通过迭代所有随机行走及其元素，每次选择相对于起始顶点I的左和右索引</p></figure><p id="bef5" class="pw-post-body-paragraph ma mb it mc b md me kd mf mg mh kg mi mj mk ml mm mn mo mp mq mr ms mt mu mv im bi translated">最终向量<code class="fe mw mx my mz b">X</code>和<code class="fe mw mx my mz b">y</code>的大小为<code class="fe mw mx my mz b">(1, 3400)</code>，其中<code class="fe mw mx my mz b">3400</code>来自随机漫步的总数(<code class="fe mw mx my mz b">170</code>)乘以每次随机漫步的配对数，即<code class="fe mw mx my mz b">20</code>。</p><h2 id="c840" class="og np it bd nq oh oi dn nu oj ok dp ny mj ol om oa mn on oo oc mr op oq oe iz bi translated">参数初始化</h2><p id="181c" class="pw-post-body-paragraph ma mb it mc b md or kd mf mg os kg mi mj ot ml mm mn ou mp mq mr ov mt mu mv im bi translated">第二步是初始化以下参数:</p><ul class=""><li id="5316" class="na nb it mc b md me mg mh mj nc mn nd mr ne mv nf ng nh ni bi translated"><code class="fe mw mx my mz b">representation_size</code>这是潜在特征尺寸，即SkipGram应该返回多少潜在“坐标”。在本例中，该值固定为2</li><li id="5be9" class="na nb it mc b md nj mg nk mj nl mn nm mr nn mv nf ng nh ni bi translated"><code class="fe mw mx my mz b">vocab_size</code>、词汇大小，即图中顶点的总数(<code class="fe mw mx my mz b">vocab_size=34+1=35</code>)</li><li id="cd05" class="na nb it mc b md nj mg nk mj nl mn nm mr nn mv nf ng nh ni bi translated">目标<code class="fe mw mx my mz b">y</code>的一键编码矩阵。热编码矩阵的大小为<code class="fe mw mx my mz b">35, 3400</code>，每次目标词出现时，其值为1:</li></ul><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="oy oz l"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">图5:将目标向量转换为热编码矩阵，其大小来自词汇大小(+1)和目标大小。</p></figure><ul class=""><li id="b683" class="na nb it mc b md me mg mh mj nc mn nd mr ne mv nf ng nh ni bi translated"><code class="fe mw mx my mz b">epochs</code>为SkipGram模式。记住SkipGram是一个神经网络，所以我们必须指定我们想要循环通过整个数据集的次数(在本例中是<code class="fe mw mx my mz b">epochs=2</code>)</li><li id="fe05" class="na nb it mc b md nj mg nk mj nl mn nm mr nn mv nf ng nh ni bi translated"><code class="fe mw mx my mz b">batch_size</code>，神经网络训练的每次迭代应该使用多少数据块(这里<code class="fe mw mx my mz b">batch_size=4096</code>，因为我想立即处理整个数据集)</li><li id="855b" class="na nb it mc b md nj mg nk mj nl mn nm mr nn mv nf ng nh ni bi translated"><code class="fe mw mx my mz b">learning_rate</code>，设置为<code class="fe mw mx my mz b">0.001</code></li><li id="9521" class="na nb it mc b md nj mg nk mj nl mn nm mr nn mv nf ng nh ni bi translated"><a class="ae lh" href="https://github.com/Steboss/learn_graph_ml/blob/e0c7f6e319919c7a9a75edd8204f8c4163ef2d9c/Perozzi_DeepWalk/karate.py#L177" rel="noopener ugc nofollow" target="_blank">神经网络</a>的嵌入和权重矩阵，通过函数<code class="fe mw mx my mz b">param_init</code>初始化。在该函数中，我们为单词嵌入创建一个随机矩阵，其大小为<code class="fe mw mx my mz b">vocab_size</code>和<code class="fe mw mx my mz b">representation_size</code>——即对于每个节点，我们将有<code class="fe mw mx my mz b">representation_size</code>坐标。其次，神经网络权重被随机初始化为:<code class="fe mw mx my mz b">np.random.randn(out_size, inp_size)</code>其中<code class="fe mw mx my mz b">inp_size</code>是图顶点的数量，<code class="fe mw mx my mz b">out_size</code>是<code class="fe mw mx my mz b">representation_size</code>。</li></ul><h2 id="5b8e" class="og np it bd nq oh oi dn nu oj ok dp ny mj ol om oa mn on oo oc mr op oq oe iz bi translated">神经网络</h2><p id="4187" class="pw-post-body-paragraph ma mb it mc b md or kd mf mg os kg mi mj ot ml mm mn ou mp mq mr ov mt mu mv im bi translated">至此，我们可以通过以下步骤旋转嵌入神经网络:</p><ul class=""><li id="8669" class="na nb it mc b md me mg mh mj nc mn nd mr ne mv nf ng nh ni bi translated">从输入数据中定义一块<code class="fe mw mx my mz b">batch_size</code></li><li id="5551" class="na nb it mc b md nj mg nk mj nl mn nm mr nn mv nf ng nh ni bi translated">运行神经网络的正向传播部分(<code class="fe mw mx my mz b">forward_propagation</code>)</li><li id="eea7" class="na nb it mc b md nj mg nk mj nl mn nm mr nn mv nf ng nh ni bi translated">通过反向传播计算梯度(<code class="fe mw mx my mz b">backward_propagation</code>)</li><li id="a9b1" class="na nb it mc b md nj mg nk mj nl mn nm mr nn mv nf ng nh ni bi translated">更新参数、嵌入矩阵和权重矩阵(<code class="fe mw mx my mz b">update_parameters</code>)</li><li id="843a" class="na nb it mc b md nj mg nk mj nl mn nm mr nn mv nf ng nh ni bi translated">通过交叉熵计算成本(<code class="fe mw mx my mz b">cross_entropy</code>)</li></ul><p id="66de" class="pw-post-body-paragraph ma mb it mc b md me kd mf mg mh kg mi mj mk ml mm mn mo mp mq mr ms mt mu mv im bi translated">SkipGram的这个<code class="fe mw mx my mz b">numpy</code>实现可以帮助我们理解主DeepWalk中隐藏了什么。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="oy oz l"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">图5:对于每个时期，我们从训练数据集中导出组块，我们执行正向传播、反向传播，并且我们更新所有参数。</p></figure><p id="5c77" class="pw-post-body-paragraph ma mb it mc b md me kd mf mg mh kg mi mj mk ml mm mn mo mp mq mr ms mt mu mv im bi translated">首先，<code class="fe mw mx my mz b">forward_propagation</code>检索每个单词的嵌入，<code class="fe mw mx my mz b">node_to_embedding</code>返回训练数据集的所有3400个<code class="fe mw mx my mz b">X</code>输入的初始随机嵌入表示。然后，调用<code class="fe mw mx my mz b">linear_dense</code>函数，更新神经网络权重:</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="oy oz l"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">图6:神经网络计算的核心。在每次正向迭代中，单词嵌入与网络权重进行点乘</p></figure><p id="348d" class="pw-post-body-paragraph ma mb it mc b md me kd mf mg mh kg mi mj mk ml mm mn mo mp mq mr ms mt mu mv im bi translated">核心神经网络仅将乘以神经网络权重的单词嵌入作为输入:<code class="fe mw mx my mz b">np.dot(weights, node_vector)</code>最后一步是通过<code class="fe mw mx my mz b">softmax</code>函数计算前一产品的softmax层。图7示出了输入单词嵌入如何通过网络计算而改变。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pb"><img src="../Images/5a1b8db80fa26dcf58b6d34ec2d2272d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AXqM8VnwgZUdhz_S_XB5Kw.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">图7:正向传播。让我们花点时间来看看输入数据的演变。随机初始化的单词嵌入通过神经网络。在输入嵌入和网络权重之间计算点积。最后一层是对网络输出的softmax计算。</p></figure><p id="60fa" class="pw-post-body-paragraph ma mb it mc b md me kd mf mg mh kg mi mj mk ml mm mn mo mp mq mr ms mt mu mv im bi translated">计算出的嵌入和权重需要通过反向传播来更新。<a class="ae lh" href="https://github.com/Steboss/learn_graph_ml/blob/e0c7f6e319919c7a9a75edd8204f8c4163ef2d9c/Perozzi_DeepWalk/karate.py#L308" rel="noopener ugc nofollow" target="_blank">函数</a> <code class="fe mw mx my mz b"><a class="ae lh" href="https://github.com/Steboss/learn_graph_ml/blob/e0c7f6e319919c7a9a75edd8204f8c4163ef2d9c/Perozzi_DeepWalk/karate.py#L308" rel="noopener ugc nofollow" target="_blank">backward_propagation</a></code> <a class="ae lh" href="https://github.com/Steboss/learn_graph_ml/blob/e0c7f6e319919c7a9a75edd8204f8c4163ef2d9c/Perozzi_DeepWalk/karate.py#L308" rel="noopener ugc nofollow" target="_blank">计算softmax层梯度、神经网络权重梯度和嵌入梯度</a>，因为我们想要训练神经网络和单词嵌入表示。</p><p id="a408" class="pw-post-body-paragraph ma mb it mc b md me kd mf mg mh kg mi mj mk ml mm mn mo mp mq mr ms mt mu mv im bi translated"><a class="ae lh" href="https://github.com/Steboss/learn_graph_ml/blob/e0c7f6e319919c7a9a75edd8204f8c4163ef2d9c/Perozzi_DeepWalk/karate.py#L328" rel="noopener ugc nofollow" target="_blank">最后，反向传播输出用于更新输入参数，即单词嵌入和网络权重</a></p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="oy oz l"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">图8:反向传播的更新过程。基于在嵌入层计算的学习速率和梯度来更新单词嵌入和网络权重。</p></figure><p id="5feb" class="pw-post-body-paragraph ma mb it mc b md me kd mf mg mh kg mi mj mk ml mm mn mo mp mq mr ms mt mu mv im bi translated">干得好！这些是deep walk skip program部分包含的所有步骤。</p><h1 id="7eed" class="no np it bd nq nr ns nt nu nv nw nx ny ki nz kj oa kl ob km oc ko od kp oe of bi translated">一个大图数据集分类:脸书大页面-页面网络</h1><p id="1c9a" class="pw-post-body-paragraph ma mb it mc b md or kd mf mg os kg mi mj ot ml mm mn ou mp mq mr ov mt mu mv im bi translated">为了证明DeepWalk获得社会潜在表征的能力，在这个例子中，我们将使用一个大规模数据集，即脸书大型页面-页面网络，可以在这里免费下载<a class="ae lh" href="https://snap.stanford.edu/data/facebook-large-page-page-network.html" rel="noopener ugc nofollow" target="_blank"/>[1]。这是包含22470个脸书页面的数据集，根据它们的内容被标记为<code class="fe mw mx my mz b">tvshow</code>、<code class="fe mw mx my mz b">government</code>、<code class="fe mw mx my mz b">company</code>或<code class="fe mw mx my mz b">politician</code>以及171002个边。边缘描述了社会互动，例如一个页面喜欢另一个页面的内容。处理这个图的代码可以在这里找到:<a class="ae lh" href="https://github.com/Steboss/learn_graph_ml/blob/master/Perozzi_DeepWalk/facebook.py" rel="noopener ugc nofollow" target="_blank">https://github . com/ste boss/learn _ graph _ ml/blob/master/Perozzi _ deep walk/Facebook . py</a></p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="oy oz l"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">图9:脸书数据集一瞥。id定义节点id，facebook_id是唯一的脸书标识符，page_name是页面的名称，page_type用作定义节点所属的类的目标。</p></figure><p id="9617" class="pw-post-body-paragraph ma mb it mc b md me kd mf mg mh kg mi mj mk ml mm mn mo mp mq mr ms mt mu mv im bi translated"><a class="ae lh" href="https://github.com/Steboss/learn_graph_ml/blob/58454b34333cb7c470ae47425bab1947f2801280/Perozzi_DeepWalk/facebook.py#L49" rel="noopener ugc nofollow" target="_blank">代码的第一部分打开[1]中提供的所有输入文件</a>，并将边信息转换为<code class="fe mw mx my mz b">networkx</code>图:</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="oy oz l"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">图10:用networkx打开输入数据并转换成图形</p></figure><p id="0785" class="pw-post-body-paragraph ma mb it mc b md me kd mf mg mh kg mi mj mk ml mm mn mo mp mq mr ms mt mu mv im bi translated"><a class="ae lh" href="https://github.com/Steboss/learn_graph_ml/blob/58454b34333cb7c470ae47425bab1947f2801280/Perozzi_DeepWalk/facebook.py#L9" rel="noopener ugc nofollow" target="_blank">其次，我们将从输入图</a>中创建80个长度为10的随机行走。输出是224'700(节点*行走长度)次随机行走，这将是SkipGram的输入序列:</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="oy oz l"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">图11:在给定输入图、要创建的步数及其长度的情况下，创建随机步的函数。</p></figure><p id="6a12" class="pw-post-body-paragraph ma mb it mc b md me kd mf mg mh kg mi mj mk ml mm mn mo mp mq mr ms mt mu mv im bi translated"><a class="ae lh" href="https://github.com/Steboss/learn_graph_ml/blob/58454b34333cb7c470ae47425bab1947f2801280/Perozzi_DeepWalk/facebook.py#L66" rel="noopener ugc nofollow" target="_blank">一旦建立了语料库，我们就可以开始制作</a><code class="fe mw mx my mz b"><a class="ae lh" href="https://github.com/Steboss/learn_graph_ml/blob/58454b34333cb7c470ae47425bab1947f2801280/Perozzi_DeepWalk/facebook.py#L66" rel="noopener ugc nofollow" target="_blank">gensim</a></code><a class="ae lh" href="https://github.com/Steboss/learn_graph_ml/blob/58454b34333cb7c470ae47425bab1947f2801280/Perozzi_DeepWalk/facebook.py#L66" rel="noopener ugc nofollow" target="_blank">skip program模型</a>——已经有一个模型多好？—如图12所示</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="oy oz l"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">图12:深度行走的核心，一旦随机行走的语料库被创建，我们可以从Gensim运行Word2Vec算法</p></figure><p id="24ec" class="pw-post-body-paragraph ma mb it mc b md me kd mf mg mh kg mi mj mk ml mm mn mo mp mq mr ms mt mu mv im bi translated"><code class="fe mw mx my mz b">Word2Vec</code>接收输入:</p><ul class=""><li id="8a01" class="na nb it mc b md me mg mh mj nc mn nd mr ne mv nf ng nh ni bi translated">句子语料库<code class="fe mw mx my mz b">walks</code></li><li id="82bc" class="na nb it mc b md nj mg nk mj nl mn nm mr nn mv nf ng nh ni bi translated">潜在维度的大小<code class="fe mw mx my mz b">representation_size=100</code></li><li id="ee72" class="na nb it mc b md nj mg nk mj nl mn nm mr nn mv nf ng nh ni bi translated">为给定单词寻找邻居的窗口大小<code class="fe mw mx my mz b">window_size=5</code></li><li id="ade4" class="na nb it mc b md nj mg nk mj nl mn nm mr nn mv nf ng nh ni bi translated">在训练过程中考虑一个单词的最少次数。在这种情况下，0意味着<code class="fe mw mx my mz b">Word2Vec</code>将把所有的单词作为输入</li><li id="82b3" class="na nb it mc b md nj mg nk mj nl mn nm mr nn mv nf ng nh ni bi translated">触发skip program模式的选项<code class="fe mw mx my mz b">sg=1</code></li><li id="4d2e" class="na nb it mc b md nj mg nk mj nl mn nm mr nn mv nf ng nh ni bi translated">触发分级softmax <code class="fe mw mx my mz b">hs=1</code>的选项</li><li id="0a0f" class="na nb it mc b md nj mg nk mj nl mn nm mr nn mv nf ng nh ni bi translated">我们想要运行的迭代次数，可选。在这个例子中是1</li><li id="e636" class="na nb it mc b md nj mg nk mj nl mn nm mr nn mv nf ng nh ni bi translated">CPU的数量</li><li id="e035" class="na nb it mc b md nj mg nk mj nl mn nm mr nn mv nf ng nh ni bi translated">再现性的随机种子<code class="fe mw mx my mz b">seed=42</code></li></ul><p id="3988" class="pw-post-body-paragraph ma mb it mc b md me kd mf mg mh kg mi mj mk ml mm mn mo mp mq mr ms mt mu mv im bi translated">这将需要一点时间来运行，但最终我们将得到给定输入图的最终嵌入，可以很容易地保存在文件中，如下所示:</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="oy oz l"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">图13:word 2 vec的嵌入输出sved的例子。22470表示节点数，100表示潜在维度。第2–4行的第一列是节点索引，而后面的数字是嵌入表示</p></figure><p id="5a0c" class="pw-post-body-paragraph ma mb it mc b md me kd mf mg mh kg mi mj mk ml mm mn mo mp mq mr ms mt mu mv im bi translated">从这里，我们可以直接询问模型，寻找与给定页面相似的页面。例如，让我们使用索引<code class="fe mw mx my mz b">14</code>，即页面<code class="fe mw mx my mz b">Nasa's Marshall Space Flight Center</code>，并使用<code class="fe mw mx my mz b">model.wv.most_similar()</code>查询模型。<code class="fe mw mx my mz b">model.wv_most_similar(14)</code>返回与页面<code class="fe mw mx my mz b">14</code>相似页面的所有索引及其概率得分</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="oy oz l"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">图14:找到所有与第14页相似的页面，美国宇航局马歇尔太空飞行中心</p></figure><p id="01b8" class="pw-post-body-paragraph ma mb it mc b md me kd mf mg mh kg mi mj mk ml mm mn mo mp mq mr ms mt mu mv im bi translated">查询输出将是:</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="oy oz l"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">图15:给定页面的Word2Vec SkipGram模型的输出</p></figure><p id="3790" class="pw-post-body-paragraph ma mb it mc b md me kd mf mg mh kg mi mj mk ml mm mn mo mp mq mr ms mt mu mv im bi translated">值得注意的是，该模型返回与给定页面高度相似的页面，而没有关于页面正在处理什么的信息。令人印象深刻的是，DeepWalk可以利用单词模型的简单相似性来询问大输入图！</p><p id="e7ad" class="pw-post-body-paragraph ma mb it mc b md me kd mf mg mh kg mi mj mk ml mm mn mo mp mq mr ms mt mu mv im bi translated">今天就到这里吧！请继续关注关于图形和ML的新帖子！！！！</p></div><div class="ab cl pc pd hx pe" role="separator"><span class="pf bw bk pg ph pi"/><span class="pf bw bk pg ph pi"/><span class="pf bw bk pg ph"/></div><div class="im in io ip iq"><p id="c98c" class="pw-post-body-paragraph ma mb it mc b md me kd mf mg mh kg mi mj mk ml mm mn mo mp mq mr ms mt mu mv im bi translated"><em class="pa">如有疑问或意见，请随时发送电子邮件至:stefanobosisio1@gmail.com或直接发送至Medium。</em></p><h1 id="5ad6" class="no np it bd nq nr ns nt nu nv nw nx ny ki nz kj oa kl ob km oc ko od kp oe of bi translated">文献学</h1><ol class=""><li id="7f9c" class="na nb it mc b md or mg os mj pj mn pk mr pl mv pm ng nh ni bi translated">Rozemberczki，Benedek，Carl Allen和Rik Sarkar。"多尺度属性节点嵌入."<em class="pa">复杂网络杂志</em> 9.2 (2021): cnab014。</li></ol></div></div>    
</body>
</html>