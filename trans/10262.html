<html>
<head>
<title>Dimensionality Reduction cheat sheet</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">降维备忘单</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/dimensionality-reduction-cheatsheet-15060fee3aa?source=collection_archive---------16-----------------------#2021-09-29">https://towardsdatascience.com/dimensionality-reduction-cheatsheet-15060fee3aa?source=collection_archive---------16-----------------------#2021-09-29</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="9dd9" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">5分钟内你应该知道的降维知识</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/236e44bc50b34dc914bd78a888b6c273.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kK4aMPHQ89ssFEus6RT4Yw.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">降维。作者图片</p></figure><p id="cac4" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在这篇文章中，你会发现一个完整的降维备忘单。在五分钟内，你将能够知道它是什么，并刷新主要算法的记忆。</p></div><div class="ab cl lu lv hx lw" role="separator"><span class="lx bw bk ly lz ma"/><span class="lx bw bk ly lz ma"/><span class="lx bw bk ly lz"/></div><div class="im in io ip iq"><p id="803a" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">降维算法代表了一种技术，即<em class="mb">减少数据集中特征</em>(非样本)的数量。</p><p id="8388" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在下面的示例中，任务是减少输入要素的数量(将swissroll从3D展开到2D ),同时保存最大比例的信息。这就是降维任务和这些算法的本质。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mc"><img src="../Images/3cd5a0ef32e224f5ddb4a1f77e838c43.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*67XVSEAGzpobEWjTDgZtFA.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">降维例子。<a class="ae md" href="https://commons.wikimedia.org/wiki/File:Lle_hlle_swissroll.png" rel="noopener ugc nofollow" target="_blank">公共领域</a></p></figure><p id="e8ed" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">降维算法的两个主要应用是:</p><ul class=""><li id="3b7c" class="me mf it la b lb lc le lf lh mg ll mh lp mi lt mj mk ml mm bi translated"><strong class="la iu">数据可视化</strong> &amp; <strong class="la iu">数据分析</strong> —将输入要素的数量减少到三个或两个，并使用数据可视化技术深入了解数据</li><li id="6bc8" class="me mf it la b lb mn le mo lh mp ll mq lp mr lt mj mk ml mm bi translated"><strong class="la iu">其他机器学习算法的预备工具</strong>。更多的输入特征通常会使预测任务更加难以建模，这就是所谓的<strong class="la iu">维数灾难</strong>。由于许多算法(来自监督和非监督学习(例如回归/分类、聚类))不能很好地处理稀疏或高维数据，因此降维算法可以大大提高质量。通常，这也提供了更快和更简单的计算。</li></ul><p id="58a1" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">方法通常分为:</p><ul class=""><li id="c7fc" class="me mf it la b lb lc le lf lh mg ll mh lp mi lt mj mk ml mm bi translated"><strong class="la iu">特征选择</strong> —找到输入特征的子集</li><li id="b1ee" class="me mf it la b lb mn le mo lh mp ll mq lp mr lt mj mk ml mm bi translated"><strong class="la iu">特征投影</strong>(或<em class="mb">特征提取</em> ) —寻找原始数据到某个低维空间的最佳投影</li></ul><p id="bcb9" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">接下来，我们将讨论第二组方法。查看<em class="mb">特征工程</em>以获得更多<em class="mb">特征选择</em>工具，例如套索回归、相关性分析等。事实上，以下算法也可用作<em class="mb">特征选择</em>工具，区别在于这些不再是原始特征，而是它们的一些修改(例如在<em class="mb"> PCA </em>情况下的线性组合)。</p><h1 id="9b27" class="ms mt it bd mu mv mw mx my mz na nb nc jz nd ka ne kc nf kd ng kf nh kg ni nj bi translated">主成分分析</h1><p id="a29e" class="pw-post-body-paragraph ky kz it la b lb nk ju ld le nl jx lg lh nm lj lk ll nn ln lo lp no lr ls lt im bi translated">为了降低维数，主成分分析(<em class="mb"> PCA </em>)使用原始数据到<em class="mb">主成分</em>的投影。主分量是描述<em class="mb">残差变化</em>的最大量的正交向量(使用<em class="mb">奇异值分解</em>找到它们)。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi np"><img src="../Images/fa84853d0c2adffcb4db0acbbe03ea92.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kgpS4BFBwUY2XywJW2fnHQ.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">具有两个主成分的高斯分布的主成分分析。<a class="ae md" href="https://en.wikipedia.org/wiki/Principal_component_analysis#/media/File:GaussianScatterPCA.svg" rel="noopener ugc nofollow" target="_blank">公共领域</a></p></figure><p id="a76d" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">因此，通过选择第一个<code class="fe nq nr ns nt b">N</code>主成分(其中<code class="fe nq nr ns nt b">N &lt; M, M is the number of features</code>，我们从M维空间移动到N维空间，其中新特征是现有特征的线性组合。</p><p id="7fda" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">为了选择组件的数量，使用了所谓的<em class="mb">弯头方法</em>。绘制解释方差的累积和图，然后选择解释所需信息比率的分量数(通常为80%或95%)。</p><p id="6ba1" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">PCA需要数据缩放和居中(<code class="fe nq nr ns nt b">sklearn.decomposition.PCA</code>类自动完成)。</p><p id="da18" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这些算法有很多流行的修改，但最流行的是:</p><ul class=""><li id="e34d" class="me mf it la b lb lc le lf lh mg ll mh lp mi lt mj mk ml mm bi translated"><em class="mb">增量PCA </em> —用于<em class="mb">在线学习</em>或数据不适合存储时</li><li id="6190" class="me mf it la b lb mn le mo lh mp ll mq lp mr lt mj mk ml mm bi translated"><em class="mb">随机化的PCA </em> —一种允许快速估计前N个分量的随机算法</li><li id="23c5" class="me mf it la b lb mn le mo lh mp ll mq lp mr lt mj mk ml mm bi translated"><em class="mb">内核PCA </em> — <em class="mb">内核技巧</em>允许执行复杂的非线性投影</li></ul><h1 id="e291" class="ms mt it bd mu mv mw mx my mz na nb nc jz nd ka ne kc nf kd ng kf nh kg ni nj bi translated">流形学习</h1><p id="35dd" class="pw-post-body-paragraph ky kz it la b lb nk ju ld le nl jx lg lh nm lj lk ll nn ln lo lp no lr ls lt im bi translated">流形学习算法基于某种距离度量守恒。这些算法在减少维度的同时节省了物体之间的距离。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nu"><img src="../Images/f70093b496598be877301cdbba81c0d2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XibbfTRrBm046AGZGDghcg.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">Scikit Learn的流形学习方法比较。<a class="ae md" href="https://scikit-learn.org/stable/auto_examples/manifold/plot_compare_methods.html" rel="noopener ugc nofollow" target="_blank">图像来源</a></p></figure><ul class=""><li id="fa08" class="me mf it la b lb lc le lf lh mg ll mh lp mi lt mj mk ml mm bi translated"><strong class="la iu"> LLE </strong></li></ul><p id="a9a9" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">LLE ( <em class="mb">局部线性嵌入</em>)研究原始空间中数据点之间的线性连接，然后试图移动到更小的维度空间，同时保持在局部邻域内。这个算法有很多修改，像<em class="mb">修改的局部线性嵌入(MLLE) </em>、<em class="mb">基于Hessian的LLE (HLLE) </em>等等。</p><ul class=""><li id="162c" class="me mf it la b lb lc le lf lh mg ll mh lp mi lt mj mk ml mm bi translated"><strong class="la iu"> Isomap </strong></li></ul><p id="5b31" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">Isomap(是<em class="mb">等距映射</em>的缩写)с通过将每个实例连接到其最近的邻居来创建一个图，然后在试图保留实例之间的<em class="mb">测地线距离</em>(图中两个顶点之间的距离)的同时减少维度。</p><ul class=""><li id="5e6c" class="me mf it la b lb lc le lf lh mg ll mh lp mi lt mj mk ml mm bi translated"><strong class="la iu"> t-SNE </strong></li></ul><p id="ce8f" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">t-SNE代表<em class="mb"> t分布随机邻居嵌入</em>。通过保存空间中各点之间的相对距离来减少维数，这样可以使相似的实例彼此靠近，而不相似的实例彼此分开。最常用于数据可视化。</p><h1 id="acff" class="ms mt it bd mu mv mw mx my mz na nb nc jz nd ka ne kc nf kd ng kf nh kg ni nj bi translated">自动编码器</h1><p id="f73e" class="pw-post-body-paragraph ky kz it la b lb nk ju ld le nl jx lg lh nm lj lk ll nn ln lo lp no lr ls lt im bi translated">我们也可以使用神经网络进行降维。Autoencoder是一个网络，当网络结构暗示<em class="mb">一个瓶颈</em>——一个神经元数量比输入层少得多的层时，它会尝试输出与输入尽可能相似的值。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nv"><img src="../Images/6fe235927e95a40da82c7aedbec62e90.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XOdXrELoQHFYD-X_9Ge41g.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">自动编码器结构。<a class="ae md" href="https://en.wikipedia.org/wiki/Autoencoder#/media/File:Autoencoder_schema.png" rel="noopener ugc nofollow" target="_blank">公共领域</a></p></figure><p id="0178" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">如果我们使用线性激活函数，我们将得到线性降维规则，像<em class="mb"> PCA </em>。但是如果我们使用非线性激活函数，我们可以得到更复杂的潜在表示。不幸的是，我们必须有大量的数据。幸运的是，这些数据是未标记的，所以通常很容易收集。</p><p id="7dd3" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">至于其他算法，有很多不同的变体，比如:</p><ul class=""><li id="7710" class="me mf it la b lb lc le lf lh mg ll mh lp mi lt mj mk ml mm bi translated"><em class="mb">降噪自动编码器</em>可以帮助清理图像或声音</li><li id="e4aa" class="me mf it la b lb mn le mo lh mp ll mq lp mr lt mj mk ml mm bi translated"><em class="mb">处理分布而不是特定值的变型自动编码器</em></li><li id="f3ae" class="me mf it la b lb mn le mo lh mp ll mq lp mr lt mj mk ml mm bi translated"><em class="mb">图像卷积自动编码器</em></li><li id="6ce7" class="me mf it la b lb mn le mo lh mp ll mq lp mr lt mj mk ml mm bi translated"><em class="mb">用于时间序列或文本的循环自动编码器</em></li></ul><h1 id="37a6" class="ms mt it bd mu mv mw mx my mz na nb nc jz nd ka ne kc nf kd ng kf nh kg ni nj bi translated">如何选择一种降维算法？</h1><p id="f5cb" class="pw-post-body-paragraph ky kz it la b lb nk ju ld le nl jx lg lh nm lj lk ll nn ln lo lp no lr ls lt im bi translated">首先，确保你对数据进行了缩放。几乎所有的降维算法都要求这样。</p><p id="dab9" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">如果你为<em class="mb">数据可视化</em>减少维度，你应该首先尝试<strong class="la iu"> t-SNE </strong>。</p><p id="eee1" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">如果你有很多数据，<strong class="la iu">自动编码器</strong>可以帮助你找到非常复杂的潜在表示。</p><p id="746b" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">如果没有大量数据，可以尝试<strong class="la iu"> PCA进行线性</strong>降维，<strong class="la iu">流形学习算法</strong> ( <em class="mb"> LLE </em>，<em class="mb"> Isomap，</em>等)<strong class="la iu">进行非线性降维</strong>。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nw"><img src="../Images/b2d3c126416852c8112b9ec198158d2f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CjNcED01h4IdN5h91quKcg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">降维算法选择。作者图片</p></figure><p id="4647" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">请注意，几乎每个算法都有许多变体，还有许多其他不太流行的算法，如:</p><ul class=""><li id="a246" class="me mf it la b lb lc le lf lh mg ll mh lp mi lt mj mk ml mm bi translated"><em class="mb">非负矩阵分解(NMF) </em></li><li id="aed4" class="me mf it la b lb mn le mo lh mp ll mq lp mr lt mj mk ml mm bi translated"><em class="mb">随机预测</em></li><li id="dc16" class="me mf it la b lb mn le mo lh mp ll mq lp mr lt mj mk ml mm bi translated"><em class="mb">线性判别分析</em></li><li id="6945" class="me mf it la b lb mn le mo lh mp ll mq lp mr lt mj mk ml mm bi translated"><em class="mb">多维标度(MDS) </em></li><li id="a729" class="me mf it la b lb mn le mo lh mp ll mq lp mr lt mj mk ml mm bi translated">以及其他等等</li></ul></div><div class="ab cl lu lv hx lw" role="separator"><span class="lx bw bk ly lz ma"/><span class="lx bw bk ly lz ma"/><span class="lx bw bk ly lz"/></div><div class="im in io ip iq"><p id="e9dc" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">本文是以下内容的一部分:</p><div class="nx ny gp gr nz oa"><a rel="noopener follow" target="_blank" href="/unsupervised-learning-algorithms-cheat-sheet-d391a39de44a"><div class="ob ab fo"><div class="oc ab od cl cj oe"><h2 class="bd iu gy z fp of fr fs og fu fw is bi translated">无监督学习算法备忘单</h2><div class="oh l"><h3 class="bd b gy z fp of fr fs og fu fw dk translated">你应该知道的所有无监督机器学习算法的完整备忘单</h3></div><div class="oi l"><p class="bd b dl z fp of fr fs og fu fw dk translated">towardsdatascience.com</p></div></div><div class="oj l"><div class="ok l ol om on oj oo ks oa"/></div></div></a></div><p id="cd16" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">您可能还对以下内容感兴趣:</p><div class="nx ny gp gr nz oa"><a rel="noopener follow" target="_blank" href="/supervised-learning-algorithms-cheat-sheet-40009e7f29f5"><div class="ob ab fo"><div class="oc ab od cl cj oe"><h2 class="bd iu gy z fp of fr fs og fu fw is bi translated">监督学习算法备忘单</h2><div class="oh l"><h3 class="bd b gy z fp of fr fs og fu fw dk translated">你应该知道的所有监督机器学习算法的完整备忘单，包括优点、缺点和…</h3></div><div class="oi l"><p class="bd b dl z fp of fr fs og fu fw dk translated">towardsdatascience.com</p></div></div><div class="oj l"><div class="op l ol om on oj oo ks oa"/></div></div></a></div></div><div class="ab cl lu lv hx lw" role="separator"><span class="lx bw bk ly lz ma"/><span class="lx bw bk ly lz ma"/><span class="lx bw bk ly lz"/></div><div class="im in io ip iq"><h1 id="11ad" class="ms mt it bd mu mv oq mx my mz or nb nc jz os ka ne kc ot kd ng kf ou kg ni nj bi translated">感谢您的阅读！</h1><ul class=""><li id="43f7" class="me mf it la b lb nk le nl lh ov ll ow lp ox lt mj mk ml mm bi translated">我希望这些材料对你有用。在Medium上关注我可以获得更多类似的文章。</li><li id="49a4" class="me mf it la b lb mn le mo lh mp ll mq lp mr lt mj mk ml mm bi translated">如果您有任何问题或意见，我将很高兴得到任何反馈。在评论中问我，或者通过<a class="ae md" href="https://www.linkedin.com/in/andimid/" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>或<a class="ae md" href="https://twitter.com/dimid_ml" rel="noopener ugc nofollow" target="_blank"> Twitter </a>联系我。</li><li id="8af5" class="me mf it la b lb mn le mo lh mp ll mq lp mr lt mj mk ml mm bi translated">为了支持我作为一名作家，并获得数以千计的其他媒体文章，使用<a class="ae md" href="https://medium.com/@andimid/membership" rel="noopener">我的推荐链接</a>获得媒体会员资格(不收取额外费用)。</li></ul></div></div>    
</body>
</html>