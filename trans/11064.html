<html>
<head>
<title>Deep Reinforcement Learning hands-on for Optimized Ad Placement</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">优化广告投放的深度强化学习实践</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/deep-reinforcement-learning-hands-on-for-optimized-ad-placement-b402ffa47245?source=collection_archive---------18-----------------------#2021-10-28">https://towardsdatascience.com/deep-reinforcement-learning-hands-on-for-optimized-ad-placement-b402ffa47245?source=collection_archive---------18-----------------------#2021-10-28</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><blockquote class="jn jo jp"><p id="8ce8" class="jq jr js jt b ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko ij bi translated">本文使用深度强化技术来优化网站上的广告投放，以最大化用户点击概率并增加数字营销收入。提供了一个详细的案例研究和代码，以帮助用户在任何现实世界的例子中实现该解决方案。</p></blockquote><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="gh gi kp"><img src="../Images/0e177c95f6c304ad68083869a3217c81.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*dLXAqsZ47-b8zx1m"/></div></div></figure></div><div class="ab cl lb lc hu ld" role="separator"><span class="le bw bk lf lg lh"/><span class="le bw bk lf lg lh"/><span class="le bw bk lf lg"/></div><div class="ij ik il im in"><p id="82c4" class="pw-post-body-paragraph jq jr iq jt b ju jv jw jx jy jz ka kb li kd ke kf lj kh ki kj lk kl km kn ko ij bi translated">联盟营销和点击付费是数字营销的两个重要方面。这些技术的优化实施可以极大地增加公司的产品/服务销售额，也可以为营销人员带来巨大的收入。随着深度强化学习的进展，数字营销是受益最大的领域之一。</p><p id="ae20" class="pw-post-body-paragraph jq jr iq jt b ju jv jw jx jy jz ka kb li kd ke kf lj kh ki kj lk kl km kn ko ij bi translated">传统的微调数字营销活动的方法需要大量的历史数据。这既耗费时间又耗费资源。通过强化学习，可以节省时间和资源，因为它们不需要任何历史数据或活动的先验信息。在这篇文章中，我们可以看到一个简单的深度RL技术如何优化一个相当复杂的数字营销活动，并取得几乎完美的结果。</p><p id="294b" class="pw-post-body-paragraph jq jr iq jt b ju jv jw jx jy jz ka kb li kd ke kf lj kh ki kj lk kl km kn ko ij bi translated">在本文中，通过一个接近真实的案例研究，让我们看看强化学习如何帮助我们管理广告投放，以获得最大的利益。</p><p id="c823" class="pw-post-body-paragraph jq jr iq jt b ju jv jw jx jy jz ka kb li kd ke kf lj kh ki kj lk kl km kn ko ij bi translated"><strong class="jt ir">问题陈述</strong></p><p id="8676" class="pw-post-body-paragraph jq jr iq jt b ju jv jw jx jy jz ka kb li kd ke kf lj kh ki kj lk kl km kn ko ij bi translated">我们管理着10个电子商务网站，每个网站都专注于销售不同类别的商品，如电脑、珠宝、巧克力等。我们的目标是通过将在我们的一个网站购物的顾客推荐到他们可能感兴趣的另一个网站来增加产品的销售。当客户查看我们的一个网站时，我们会显示另一个网站的广告，希望他们也会购买其他产品。我们的问题是，我们不知道客户应该被推荐到哪个网站，或者我们没有客户偏好的任何信息。</p><h2 id="7d8f" class="ll lm iq bd ln lo lp dn lq lr ls dp lt li lu lv lw lj lx ly lz lk ma mb mc md bi translated"><em class="me">让我们用强化学习来解决问题吧！！</em></h2><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div class="gh gi mf"><img src="../Images/5227bc6e9f6a2fd710fa2ce25599e372.png" data-original-src="https://miro.medium.com/v2/resize:fit:626/format:webp/1*TcdYm-QpbZkP6YlJTqPuEA.png"/></div><p class="mg mh gj gh gi mi mj bd b be z dk translated">图1:RL背后的基本概念示意图</p></figure><p id="d0a3" class="pw-post-body-paragraph jq jr iq jt b ju jv jw jx jy jz ka kb li kd ke kf lj kh ki kj lk kl km kn ko ij bi translated">一般来说，强化学习是一种技术，在这种技术中，我们训练一个代理在一个环境中操作。代理人在状态“s”采取行动“a ”,并从环境收到行动的奖励“r”。所以(s，a，r)成为一个状态-动作-回报元组。我们培训的目标是使代理获得的总回报最大化。因此，我们找到了(s，a，r)元组，它对于给定的状态和动作具有最大的回报。为了找到优化的元组，我们运行了许多集，每次都重新计算奖励。</p><p id="3a6d" class="pw-post-body-paragraph jq jr iq jt b ju jv jw jx jy jz ka kb li kd ke kf lj kh ki kj lk kl km kn ko ij bi translated">在这个广告投放问题中，我们需要测试不同的行为，并自动学习给定情况、状态或背景下最有益的结果。所以我们称之为<em class="js">语境强盗</em>框架，其中状态成为语境信息，代理为当前语境找到最佳行动。</p><p id="2b7b" class="pw-post-body-paragraph jq jr iq jt b ju jv jw jx jy jz ka kb li kd ke kf lj kh ki kj lk kl km kn ko ij bi translated">比方说，我们有10个网站要管理，它们构成了10个不同的州，客户在其中一个网站上。因为我们有10种不同的产品类别，所以我们可以向客户展示这10种产品中的任何一种。所以每个州有10个不同的动作。这将导致100个不同的状态-动作-回报元组。我们需要存储100个不同的数据点，并在每次有新的奖励时重新计算。在这个例子中，这似乎是合理的。但是，如果我们有1000个网站要管理，这将产生1000000个数据点。存储和重新计算这将花费大量的时间和资源。</p><p id="5e25" class="pw-post-body-paragraph jq jr iq jt b ju jv jw jx jy jz ka kb li kd ke kf lj kh ki kj lk kl km kn ko ij bi translated">这意味着当状态和动作空间很大时(状态和动作的总数很大)，强化学习会失败？？？</p><p id="a8ff" class="pw-post-body-paragraph jq jr iq jt b ju jv jw jx jy jz ka kb li kd ke kf lj kh ki kj lk kl km kn ko ij bi translated">这就是深度强化学习的由来。我们使用神经网络来提取每个状态和动作的奖励值，而不是存储每个状态、动作和奖励元组。神经网络非常擅长学习抽象概念。他们学习数据中的模式和规律，并可以将大量信息作为权重压缩到他们的内存中。因此，神经网络可以学习状态——行动和奖励之间的复杂关系。</p><p id="1b18" class="pw-post-body-paragraph jq jr iq jt b ju jv jw jx jy jz ka kb li kd ke kf lj kh ki kj lk kl km kn ko ij bi translated">神经网络充当从环境中学习以最大化回报的代理。在本文中，我们将使用PyTorch构建一个神经网络，并训练它来优化广告投放，以获得最大回报。</p><h2 id="45c5" class="ll lm iq bd ln lo lp dn lq lr ls dp lt li lu lv lw lj lx ly lz lk ma mb mc md bi translated"><em class="me">让我们从编码开始吧！！</em></h2><p id="cf2b" class="pw-post-body-paragraph jq jr iq jt b ju mk jw jx jy ml ka kb li mm ke kf lj mn ki kj lk mo km kn ko ij bi translated">让我们首先为上下文强盗创建一个模拟环境。这个环境应该包括代表10个网站(0到9)的10个状态和产生广告点击奖励的方法，以及选择一个动作(显示10个广告中的哪一个)的方法</p><pre class="kq kr ks kt gt mp mq mr ms aw mt bi"><span id="2efc" class="ll lm iq mq b gy mu mv l mw mx">class ContextBandit:<br/>    def __init__(self, arms=10):<br/>        self.arms = arms<br/>        self.init_distribution(arms)<br/>        self.update_state()<br/>        <br/>    def init_distribution(self, arms):                  #<strong class="mq ir">1</strong><br/>        self.bandit_matrix = np.random.rand(arms,arms)<br/>        <br/>    def reward(self, prob):                             <br/>        reward = 0<br/>        for i in range(self.arms):<br/>            if random.random() &lt; prob:<br/>                reward += 1<br/>        return reward<br/>        <br/>    def get_state(self):                                <br/>        return self.state<br/><br/>    def update_state(self):                             <br/>        self.state = np.random.randint(0,self.arms)<br/>        <br/>    def get_reward(self,arm):<br/>        return self.reward(self.bandit_matrix[self.get_state()][arm])<br/>        <br/>    def choose_arm(self, arm):                          #<strong class="mq ir">2</strong><br/>        reward = self.get_reward(arm)<br/>        self.update_state()<br/>        return reward</span></pre><p id="db2f" class="pw-post-body-paragraph jq jr iq jt b ju jv jw jx jy jz ka kb li kd ke kf lj kh ki kj lk kl km kn ko ij bi translated">这是一个代表每个州的矩阵。行代表状态，列代表臂(动作)</p><p id="fa25" class="pw-post-body-paragraph jq jr iq jt b ju jv jw jx jy jz ka kb li kd ke kf lj kh ki kj lk kl km kn ko ij bi translated">选择一只手臂(动作)会返回奖励并更新状态</p><p id="2042" class="pw-post-body-paragraph jq jr iq jt b ju jv jw jx jy jz ka kb li kd ke kf lj kh ki kj lk kl km kn ko ij bi translated">下面的代码显示了如何使用环境</p><pre class="kq kr ks kt gt mp mq mr ms aw mt bi"><span id="50f9" class="ll lm iq mq b gy mu mv l mw mx">env = ContextBandit(arms=10)<br/>state = env.get_state()<br/>reward = env.choose_arm(1)<br/>print(state)<br/>&gt;&gt;&gt; 1<br/>print(reward)<br/>&gt;&gt;&gt; 7</span></pre><p id="08b1" class="pw-post-body-paragraph jq jr iq jt b ju jv jw jx jy jz ka kb li kd ke kf lj kh ki kj lk kl km kn ko ij bi translated">该环境由一个名为<em class="js"> ContextBandit </em>的类组成，该类可以通过arm(动作)的数量进行初始化。在这个例子中，我们采取的状态数等于动作数。但这在现实生活中可能会有所不同。该类有一个函数get_state()，调用它时会从均匀分布中返回一个随机状态。在现实生活的例子中，状态可以来自更复杂的或与业务相关的分布。用任何动作(arm)作为输入调用choose_arm()将模拟投放广告。该方法返回对该动作的奖励，并使用新状态更新当前状态。我们需要一直调用get_state()然后选择_arm()不断获取新数据。</p><p id="3a25" class="pw-post-body-paragraph jq jr iq jt b ju jv jw jx jy jz ka kb li kd ke kf lj kh ki kj lk kl km kn ko ij bi translated">ContextualBandit也有一些辅助功能，如<em class="js">一键编码器</em>和<em class="js"> softmax。</em>一键编码器<em class="js"> </em>函数返回一个1和全0的向量，其中1代表当前状态。Softmax函数用于设置每个状态下各种动作的奖励分配。对于n个状态中的每一个，我们将有n个不同的softmax奖励分配。因此，我们需要了解状态和它们的动作分布之间的关系，并为给定的状态选择概率最高的动作。下面提到了这两个函数的代码</p><pre class="kq kr ks kt gt mp mq mr ms aw mt bi"><span id="4406" class="ll lm iq mq b gy mu mv l mw mx">def one_hot(N, pos, val=1):  #N- number of actions , pos-state<br/>    one_hot_vec = np.zeros(N)<br/>    one_hot_vec[pos] = val<br/>    return one_hot_vec</span><span id="80db" class="ll lm iq mq b gy my mv l mw mx">def softmax(av, tau=1.12):<br/>    softm = np.exp(av / tau) / np.sum( np.exp(av / tau) ) <br/>    return softm</span></pre><p id="215f" class="pw-post-body-paragraph jq jr iq jt b ju jv jw jx jy jz ka kb li kd ke kf lj kh ki kj lk kl km kn ko ij bi translated">现在让我们创建一个具有ReLU激活的两层前馈神经网络作为代理。第一层将接受10个元素的一次性编码向量(状态向量),最后一层将输出10个元素向量，表示每个动作的奖励。</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="gh gi mz"><img src="../Images/ddb10b173e8b1c8dea1a971ddc70b74a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EpQRjHx3fGQB0cAOLbG9PA.png"/></div></div><p class="mg mh gj gh gi mi mj bd b be z dk translated">图2:计算图表</p></figure><p id="3bff" class="pw-post-body-paragraph jq jr iq jt b ju jv jw jx jy jz ka kb li kd ke kf lj kh ki kj lk kl km kn ko ij bi translated">从图2中，我们可以看到get_state()函数返回一个随机状态值，该值使用一位热码编码器转换为10个元素的向量。该向量作为输入被馈送到神经网络。神经网络的输出是10个元素的向量，表示给定输入状态下每个动作的预测回报。输出是一个密集向量，使用softmax函数进一步转换为概率。基于概率，选择样本动作。一旦选择了动作，choose_arm()就会获得奖励，并使用环境中的新状态进行更新。</p><p id="d98a" class="pw-post-body-paragraph jq jr iq jt b ju jv jw jx jy jz ka kb li kd ke kf lj kh ki kj lk kl km kn ko ij bi translated">最初，对于状态0，神经网络将产生类似于[1.4，50，4.3，0.31，0.43，11，121，90，8.9，1.1]的输出向量。在运行softmax并对动作进行采样后，最有可能的动作6将被选中(最高预测奖励)。选择动作6运行choose_arm()后会产生奖励比如说8。我们训练神经网络用[1.4，50，4.3，0.31，0.43，11，8，90，8.9，1.1]更新向量，因为8是实际的奖励。现在，下一次神经网络将预测，每当看到状态0时，行动6的奖励接近8。当我们在许多状态和动作上不断训练我们模型时，神经网络将学习为各种状态-动作对预测更准确的回报</p><p id="3dbd" class="pw-post-body-paragraph jq jr iq jt b ju jv jw jx jy jz ka kb li kd ke kf lj kh ki kj lk kl km kn ko ij bi translated">下面是创建神经网络和初始化环境的代码</p><pre class="kq kr ks kt gt mp mq mr ms aw mt bi"><span id="0466" class="ll lm iq mq b gy mu mv l mw mx">arms = 10<br/>N, D_in, H, D_out = 1, arms, 100, arms</span><span id="155d" class="ll lm iq mq b gy my mv l mw mx">model = torch.nn.Sequential(<br/>    torch.nn.Linear(D_in, H),<br/>    torch.nn.ReLU(),<br/>    torch.nn.Linear(H, D_out),<br/>    torch.nn.ReLU(),<br/>)<br/>loss_fn = torch.nn.MSELoss()<br/>env = ContextBandit(arms)</span></pre><p id="f89a" class="pw-post-body-paragraph jq jr iq jt b ju jv jw jx jy jz ka kb li kd ke kf lj kh ki kj lk kl km kn ko ij bi translated">现在，让我们看看如何训练代理，并按照图2中说明的所有步骤进行操作</p><pre class="kq kr ks kt gt mp mq mr ms aw mt bi"><span id="a097" class="ll lm iq mq b gy mu mv l mw mx">def train(env, epochs=5000, learning_rate=1e-2):<br/>    cur_state = torch.Tensor(one_hot(arms,env.get_state()))      #1<br/>    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)<br/>    rewards = []<br/>    for i in range(epochs):<br/>        y_pred = model(cur_state)                                #2<br/>        av_softmax = softmax(y_pred.data.numpy(), tau=2.0)       #3<br/>        av_softmax /= av_softmax.sum()                           #4<br/>        choice = np.random.choice(arms, p=av_softmax)            #5<br/>        cur_reward = env.choose_arm(choice)                      #6<br/>        one_hot_reward = y_pred.data.numpy().copy()              #7<br/>        one_hot_reward[choice] = cur_reward                      #8<br/>        reward = torch.Tensor(one_hot_reward)<br/>        rewards.append(cur_reward)<br/>        loss = loss_fn(y_pred, reward)<br/>        optimizer.zero_grad()<br/>        loss.backward()<br/>        optimizer.step()<br/>        cur_state = torch.Tensor(one_hot(arms,env.get_state()))  #9<br/>    return np.array(rewards)</span></pre><ul class=""><li id="a492" class="na nb iq jt b ju jv jy jz li nc lj nd lk ne ko nf ng nh ni bi translated"><strong class="jt ir"> <em class="js"> 1 </em> </strong>获取环境的当前状态；转换为PyTorch变量</li><li id="da92" class="na nb iq jt b ju nj jy nk li nl lj nm lk nn ko nf ng nh ni bi translated"><strong class="jt ir"> <em class="js"> 2 </em> </strong>向前运行神经网络以获得奖励预测</li><li id="3915" class="na nb iq jt b ju nj jy nk li nl lj nm lk nn ko nf ng nh ni bi translated"><strong class="jt ir"> <em class="js"> 3 </em> </strong>用softmax将奖励预测转换成概率分布</li><li id="772e" class="na nb iq jt b ju nj jy nk li nl lj nm lk nn ko nf ng nh ni bi translated"><strong class="jt ir"> <em class="js"> 4 </em> </strong>对分布进行规范化，以确保其总和为1</li><li id="d6e5" class="na nb iq jt b ju nj jy nk li nl lj nm lk nn ko nf ng nh ni bi translated"><strong class="jt ir"> <em class="js"> 5 </em> </strong>概率性地选择新动作</li><li id="d524" class="na nb iq jt b ju nj jy nk li nl lj nm lk nn ko nf ng nh ni bi translated"><strong class="jt ir"> <em class="js"> 6 </em> </strong>采取行动，获得奖励</li><li id="3fe9" class="na nb iq jt b ju nj jy nk li nl lj nm lk nn ko nf ng nh ni bi translated"><strong class="jt ir"> <em class="js"> 7 </em> </strong>将PyTorch张量数据转换为Numpy数组</li><li id="d637" class="na nb iq jt b ju nj jy nk li nl lj nm lk nn ko nf ng nh ni bi translated"><strong class="jt ir"> <em class="js"> 8 </em> </strong>更新one_hot_reward数组作为标注的训练数据</li><li id="9326" class="na nb iq jt b ju nj jy nk li nl lj nm lk nn ko nf ng nh ni bi translated"><strong class="jt ir">T9】9T11】更新当前环境状态</strong></li></ul><p id="044a" class="pw-post-body-paragraph jq jr iq jt b ju jv jw jx jy jz ka kb li kd ke kf lj kh ki kj lk kl km kn ko ij bi translated">在对网络进行了大约5000个纪元的训练后，我们可以看到平均回报有所提高，如下所示</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div class="gh gi no"><img src="../Images/0c8c7244f1c29125a7da06d10c8025ab.png" data-original-src="https://miro.medium.com/v2/resize:fit:1176/format:webp/1*uOWsvI7J2XZZR2tJExVoww.png"/></div><p class="mg mh gj gh gi mi mj bd b be z dk translated">图3:培训后的平均回报</p></figure><p id="7a27" class="pw-post-body-paragraph jq jr iq jt b ju jv jw jx jy jz ka kb li kd ke kf lj kh ki kj lk kl km kn ko ij bi translated">我们可以看到平均奖励达到8或以上。</p><p id="c1e3" class="pw-post-body-paragraph jq jr iq jt b ju jv jw jx jy jz ka kb li kd ke kf lj kh ki kj lk kl km kn ko ij bi translated">整个项目可以在<a class="ae np" href="https://github.com/NandaKishoreJoshi/Reinforcement_Lerning/blob/main/RL_course/1_Ad_placement.ipynb" rel="noopener ugc nofollow" target="_blank">这个</a> GIT链接中找到。</p><p id="9b21" class="pw-post-body-paragraph jq jr iq jt b ju jv jw jx jy jz ka kb li kd ke kf lj kh ki kj lk kl km kn ko ij bi translated">本文基于布兰登·布朗和亚历山大·扎伊的《深度强化学习行动》一书。这本书的链接是<a class="ae np" href="https://learning.oreilly.com/library/view/deep-reinforcement-learning/9781617295430/kindle_split_011.html" rel="noopener ugc nofollow" target="_blank">这里</a></p><p id="d6db" class="pw-post-body-paragraph jq jr iq jt b ju jv jw jx jy jz ka kb li kd ke kf lj kh ki kj lk kl km kn ko ij bi translated">请在下面找到我关于各种数据科学主题的其他文章的链接</p><div class="nq nr gp gr ns nt"><a href="https://nandakishorej8.medium.com/multivariate-timeseries-forecast-with-lead-and-lag-timesteps-using-lstm-1a34915f08a" rel="noopener follow" target="_blank"><div class="nu ab fo"><div class="nv ab nw cl cj nx"><h2 class="bd ir gy z fp ny fr fs nz fu fw ip bi translated">基于LSTM的超前滞后时间步长多元时间序列预测</h2><div class="oa l"><h3 class="bd b gy z fp ny fr fs nz fu fw dk translated">为什么是多变量？它如何帮助做出更好的预测？</h3></div><div class="ob l"><p class="bd b dl z fp ny fr fs nz fu fw dk translated">nandakishorej8.medium.com</p></div></div><div class="oc l"><div class="od l oe of og oc oh kz nt"/></div></div></a></div><div class="nq nr gp gr ns nt"><a href="https://nandakishorej8.medium.com/realtime-2d-yoga-pose-estimation-with-code-walk-through-cfd69262d356" rel="noopener follow" target="_blank"><div class="nu ab fo"><div class="nv ab nw cl cj nx"><h2 class="bd ir gy z fp ny fr fs nz fu fw ip bi translated">实时2D瑜伽姿势估计与代码走查</h2><div class="oa l"><h3 class="bd b gy z fp ny fr fs nz fu fw dk translated">在线瑜伽课程是最新的趋势，也是疫情当前形势下的选择。教师不在…</h3></div><div class="ob l"><p class="bd b dl z fp ny fr fs nz fu fw dk translated">nandakishorej8.medium.com</p></div></div><div class="oc l"><div class="oi l oe of og oc oh kz nt"/></div></div></a></div><div class="nq nr gp gr ns nt"><a href="https://nandakishorej8.medium.com/auto-tuning-multiple-timeseries-sarimax-model-with-a-case-study-and-detailed-code-explanation-c136293b8457" rel="noopener follow" target="_blank"><div class="nu ab fo"><div class="nv ab nw cl cj nx"><h2 class="bd ir gy z fp ny fr fs nz fu fw ip bi translated">自动调优多时间序列SARIMAX模型——案例研究和详细的代码解释</h2><div class="oa l"><h3 class="bd b gy z fp ny fr fs nz fu fw dk translated">作为一名数据科学家，我了解到没有一个ML模型可以完成一个项目。特别是当试图…</h3></div><div class="ob l"><p class="bd b dl z fp ny fr fs nz fu fw dk translated">nandakishorej8.medium.com</p></div></div><div class="oc l"><div class="oj l oe of og oc oh kz nt"/></div></div></a></div><div class="nq nr gp gr ns nt"><a href="https://nandakishorej8.medium.com/introduction-to-pytorch-lightning-framework-for-enlightning-research-fe953bbea03b" rel="noopener follow" target="_blank"><div class="nu ab fo"><div class="nv ab nw cl cj nx"><h2 class="bd ir gy z fp ny fr fs nz fu fw ip bi translated">PYTORCH LIGHTNING简介——启发性研究框架</h2><div class="oa l"><h3 class="bd b gy z fp ny fr fs nz fu fw dk translated">我们知道，民主是一个民有、民治、民享的政府，其核心是…</h3></div><div class="ob l"><p class="bd b dl z fp ny fr fs nz fu fw dk translated">nandakishorej8.medium.com</p></div></div><div class="oc l"><div class="ok l oe of og oc oh kz nt"/></div></div></a></div><p id="ceeb" class="pw-post-body-paragraph jq jr iq jt b ju jv jw jx jy jz ka kb li kd ke kf lj kh ki kj lk kl km kn ko ij bi translated">非常欢迎反馈。你可以在<a class="ae np" href="https://www.linkedin.com/in/nanda-kishore-joshi/" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>上联系我</p></div></div>    
</body>
</html>