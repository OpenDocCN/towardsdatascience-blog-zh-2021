<html>
<head>
<title>DouZero: Mastering DouDizhu with Reinforcement Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">DouZero:通过强化学习掌握斗地主</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/douzero-mastering-doudizhu-with-reinforcement-learning-864363549c6a?source=collection_archive---------43-----------------------#2021-06-28">https://towardsdatascience.com/douzero-mastering-doudizhu-with-reinforcement-learning-864363549c6a?source=collection_archive---------43-----------------------#2021-06-28</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="faeb" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">解释最受欢迎的中国扑克斗地主的强大人工智能系统</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/5a9b4659f1968603de9dbbfa87fc18a0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*yNNzrs7YuLq47bsy.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><a class="ae kv" href="https://douzero.org/" rel="noopener ugc nofollow" target="_blank">https://douzero.org/</a>。图片作者。</p></figure><p id="aa58" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">从<a class="ae kv" href="https://www.nature.com/articles/nature16961" rel="noopener ugc nofollow" target="_blank"> AlphaGo </a>和<a class="ae kv" href="https://www.nature.com/articles/nature24270?sf123103138=1" rel="noopener ugc nofollow" target="_blank"> AlphaZero </a>开始，人工智能(AI)近年来取得了令人鼓舞的进步，在围棋、象棋、德州扑克、DOTA、星际争霸等方面击败了人类顶级选手。那么，下一个挑战是什么？</p></div><div class="ab cl ls lt hu lu" role="separator"><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx"/></div><div class="ij ik il im in"><p id="4d2e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi">In this article, I would like to introduce an <a class="ae kv" href="https://arxiv.org/abs/2106.06135" rel="noopener ugc nofollow" target="_blank">ICML paper</a> that presents a new AI System for a Chinese Poker game <a class="ae kv" href="https://en.wikipedia.org/wiki/Dou_dizhu" rel="noopener ugc nofollow" target="_blank">DouDizhu</a> (斗地主 in Chinese). DouDizhu is the most popular poker game in China with hundreds of millions of daily active players, with many tournaments held every year. It is easy to learn but difficult to master. Interestingly, DouDizhu remains to be a grand challenge for AI because of competition, collaboration, and a large number of possible moves from turn to turn.</p><h2 id="16d8" class="lz ma iq bd mb mc md dn me mf mg dp mh lf mi mj mk lj ml mm mn ln mo mp mq mr bi translated">斗地主是什么，为什么有挑战性？</h2><blockquote class="ms mt mu"><p id="5571" class="kw kx mv ky b kz la jr lb lc ld ju le mw lg lh li mx lk ll lm my lo lp lq lr ij bi translated">这是一种脱牌游戏，玩家的目标是在其他玩家之前清空自己手中的所有牌。两个农民玩家组队对抗另一个地主玩家。如果任何一个农民玩家第一个没有牌剩下，农民就赢了。每场游戏都有一个叫牌阶段，玩家根据手中牌的强弱为地主叫牌，还有一个出牌阶段，玩家依次出牌。</p></blockquote><p id="c85c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">简而言之，以下特性使得斗地主对AI来说非常具有挑战性:</p><ol class=""><li id="ecef" class="mz na iq ky b kz la lc ld lf nb lj nc ln nd lr ne nf ng nh bi translated"><strong class="ky ir">缺陷信息:</strong>与围棋或象棋不同，斗地主的棋手无法观察其他棋手的手牌。这导致了可能性的爆炸。</li><li id="0001" class="mz na iq ky b kz ni lc nj lf nk lj nl ln nm lr ne nf ng nh bi translated"><strong class="ky ir">既竞争又合作</strong>:两个农民玩家需要合作对抗地主玩家。大多数以前的扑克人工智能是为竞争环境(如德州扑克)或合作环境(如Hanabi)设计的。既竞争又合作是一个公开的挑战。</li><li id="d687" class="mz na iq ky b kz ni lc nj lf nk lj nl ln nm lr ne nf ng nh bi translated"><strong class="ky ir">大而复杂的动作空间:</strong>由于牌的组合，有27，472种可能的移动。不同回合的法律行动大相径庭。现有的人工智能往往只支持非常简单和小的行动空间。</li></ol><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nn"><img src="../Images/87c0d43030364a16fcee8bb01750d238.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*KcXj3NJ1nZtH9gsh.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">一只手及其合法组合的例子。图片由作者提供，来源:<a class="ae kv" href="https://arxiv.org/abs/2106.06135" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/2106.06135</a></p></figure><h2 id="d1d8" class="lz ma iq bd mb mc md dn me mf mg dp mh lf mi mj mk lj ml mm mn ln mo mp mq mr bi translated">用DouZero从零开始掌握斗地主</h2><p id="1f0f" class="pw-post-body-paragraph kw kx iq ky b kz no jr lb lc np ju le lf nq lh li lj nr ll lm ln ns lp lq lr ij bi translated">尽管面临这些挑战，我还是很兴奋地向大家介绍最近的<a class="ae kv" href="https://arxiv.org/abs/2106.06135" rel="noopener ugc nofollow" target="_blank"> ICML论文</a>和<a class="ae kv" href="https://github.com/kwai/DouZero" rel="noopener ugc nofollow" target="_blank"> DouZero </a>项目。DouZero首次实现了一个强大的斗地主AI，可以通过深度强化学习从零开始训练，达到人类水平的性能。有趣的是，DouZero背后的魔法出奇的简单却非常有效。作者还基于我们的<a class="ae kv" href="https://github.com/datamllab/rlcard" rel="noopener ugc nofollow" target="_blank"> RLCard项目</a>开发了一个在线演示。你可以试试这里的演示<a class="ae kv" href="https://douzero.org/" rel="noopener ugc nofollow" target="_blank"/>。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nt nu l"/></div></figure><p id="1c22" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">让我们深入探讨DouZero背后的技术。使用的核心算法称为深度蒙特卡罗(DMC)，这是经典<a class="ae kv" href="https://web.stanford.edu/class/psych209/Readings/SuttonBartoIPRLBook2ndEd.pdf" rel="noopener ugc nofollow" target="_blank">蒙特卡罗(MC)方法</a>的深度版本，用于强化学习。MC方法的目标是优化Q值<em class="mv"> Q(s，a) </em>，其中<em class="mv"> s </em>代表状态，<em class="mv"> a </em>代表动作。MC方法的关键思想是通过迭代执行以下过程来优化策略:</p><ol class=""><li id="ccec" class="mz na iq ky b kz la lc ld lf nb lj nc ln nd lr ne nf ng nh bi translated">使用当前策略生成剧集。</li><li id="587a" class="mz na iq ky b kz ni lc nj lf nk lj nl ln nm lr ne nf ng nh bi translated">对于每一个出现在剧集中的<em class="mv"> s，a </em>，计算并更新<em class="mv"> Q(s，a) </em>，返回关于<em class="mv"> s </em>、<em class="mv"> a </em>的所有样本的平均值。</li><li id="5256" class="mz na iq ky b kz ni lc nj lf nk lj nl ln nm lr ne nf ng nh bi translated">对于剧集中的每个s，用导致最大<em class="mv"> Q(s，a) </em>的动作更新策略。</li></ol><p id="2748" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">第二步的平均回报通常是通过折现的累计回报得到的。在步骤1中，我们可以使用ε-greedy来平衡勘探和开发。MC方法是最基本的强化学习算法。它们直接逼近目标值，而无需自举(即，像在DQN那样，使用下一个状态的值来估计当前状态的值)。</p><p id="e675" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">让我们来看看DMC，它结合了MC和神经网络。在MC中，<em class="mv"> Q(s，a)是用表格表示的</em>。现在，我们用Q网络代替Q表，它可以是多层神经网络。然后，在步骤2中，我们使用均方误差(MSE)损失来更新Q网络。这就产生了所谓的DMC，也就是深度版的MC。</p><p id="4a68" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">虽然MC方法被批评为不能处理不完整的情节，并且由于高方差而被认为是低效的，但是作者发现DMC非常适合斗地主。一方面，斗地主是一个情节任务，因此我们不需要处理不完整的情节。另一方面，DMC可以很容易地并行化，以有效地每秒生成许多样本，从而缓解高方差问题。DMC的一个好处是，由于实施和运行非常简单，因此可以通过多个流程轻松高效地加速。DouZero通过多进程和动作编码进一步增强了DMC(我将在后面详述)，并令人惊讶地实现了非常强大的性能。</p><h2 id="8ba0" class="lz ma iq bd mb mc md dn me mf mg dp mh lf mi mj mk lj ml mm mn ln mo mp mq mr bi translated">与DQN和政策梯度的比较</h2><p id="159d" class="pw-post-body-paragraph kw kx iq ky b kz no jr lb lc np ju le lf nq lh li lj nr ll lm ln ns lp lq lr ij bi translated">深度Q学习(DQN)和策略梯度(PG)是最流行的强化学习算法。为什么选择DMC而不是DQN或PG？</p><p id="e3f1" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在PG中，我们经常使用一个类似分类器的函数逼近器，它的输出与动作的数量成线性关系。然而，在斗地主中，有27，472个可能的动作，这使得输出非常大并且难以训练。在实际操作中，通常每回合的合法动作并不多，斗地主中的动作可以自然地编码成卡片矩阵。因此，更有希望的方法是将动作作为输入，这样我们就可以避免将27，472个动作作为输出。基于值的方法可以自然地处理这种情况，因为Q值自然地将状态和动作作为输入。</p><p id="bb05" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">同样，DQN也接近Q值。然而，在斗地主实施DQN教是很棘手的。斗地主有一大组可能的行动，其中不同的行动子集在不同的回合中是合法的。DQN中的自举步骤依赖于最大值运算，这在计算上是昂贵的，并且在可变动作空间上实现起来是棘手的。此外，当行动空间较大时，DQN可能会遭受高估。</p><p id="6041" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">相比之下，DMC非常容易实现，并且易于通过多个流程加速。虽然DMC可能遭受高方差问题，但作者表明它在具有平行参与者的斗地主中工作良好。</p><h2 id="8fff" class="lz ma iq bd mb mc md dn me mf mg dp mh lf mi mj mk lj ml mm mn ln mo mp mq mr bi translated">在斗地主实施DMC</h2><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nv"><img src="../Images/0b8d4efefe7d2c59f67b2693ad65ca6f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*7_VFN1MVeB6NtzYM.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">动作编码。图片作者。来源:https://arxiv.org/abs/2106.06135</p></figure><p id="40ad" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">斗地主中的状态包括手牌，历史招式，其他玩家手牌的数量，动作是一套牌。DouZero将每个卡组合编码成一个15 x 4的矩阵。这种表示非常通用，可以用来表示任何卡片。DouZero将状态和动作编码到这样的卡片矩阵中。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nw"><img src="../Images/10869909b579356f3c2ed9df8a184a10.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*tB8PMQ9ia0ZbsXne.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">神经架构。图片作者。来源:https://arxiv.org/abs/2106.06135<a class="ae kv" href="https://arxiv.org/abs/2106.06135" rel="noopener ugc nofollow" target="_blank"/></p></figure><p id="d844" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">对于神经网络，DouZero使用了一个神经网络，该网络通过LSTM网络对历史走势进行编码。然后，LSTM的输出与其他特征连接在一起，一起馈送到六层MLP以生成Q值。在每一步，我们只需要输入国家和所有的法律行动。网络将预测所有法律行为的Q值。我们可以选择最大的一个。</p><p id="6c11" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">DouZero为斗地主中的三个位置使用了三种不同的网络，在功能上略有不同。然后，作者在一个4-GPU服务器上并行运行45个角色进行模拟。经过几天的培训，DMC可以令人惊讶地学习非常强的政策。官方实现是<a class="ae kv" href="https://github.com/kwai/DouZero" rel="noopener ugc nofollow" target="_blank">这里</a>。我们还在<a class="ae kv" href="https://github.com/datamllab/rlcard" rel="noopener ugc nofollow" target="_blank"> RLCard </a>项目中支持DMC。RLCard是卡牌游戏中强化学习的工具包，目前支持斗地主等8种环境，DouZero等4种算法。</p><h1 id="edc4" class="nx ma iq bd mb ny nz oa me ob oc od mh jw oe jx mk jz of ka mn kc og kd mq oh bi translated">摘要</h1><p id="fecb" class="pw-post-body-paragraph kw kx iq ky b kz no jr lb lc np ju le lf nq lh li lj nr ll lm ln ns lp lq lr ij bi translated">DouZero的成功表明，经典的蒙特卡罗方法经过一些改进，可以在一个困难的领域——dou dizhu中产生非常好的结果。Monte-Carlo方法很容易实现，使用的超参数很少，但是它们非常强大。我希望你喜欢这本书，这种见解可以帮助你开发你的人工智能代理和解决其他问题。我将在后面的帖子中详细介绍DMC和DouZero的代码库。</p></div></div>    
</body>
</html>