<html>
<head>
<title>Multi-Class classification using Focal Loss and LightGBM</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用焦点损失和光照GBM的多类分类</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/multi-class-classification-using-focal-loss-and-lightgbm-a6a6dec28872?source=collection_archive---------5-----------------------#2021-04-06">https://towardsdatascience.com/multi-class-classification-using-focal-loss-and-lightgbm-a6a6dec28872?source=collection_archive---------5-----------------------#2021-04-06</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="fc99" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">在多类分类器中引入聚焦损失有几种方法。这是其中之一。</h2></div><h1 id="6f62" class="kf kg iq bd kh ki kj kk kl km kn ko kp jw kq jx kr jz ks ka kt kc ku kd kv kw bi translated">动机</h1><p id="4180" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">许多现实世界的分类问题具有不平衡的类别分布。当数据严重不平衡时，分类算法倾向于预测多数类。有几种方法可以缓解阶级不平衡。</p><p id="865f" class="pw-post-body-paragraph kx ky iq kz b la lt jr lc ld lu ju lf lg lv li lj lk lw lm ln lo lx lq lr ls ij bi translated">一种方法是分配与类别频率成反比的样本权重。另一种方法是使用过采样/欠采样技术。为少数类生成人工样本的常用技术是合成少数过采样技术(SMOTE)和自适应合成(ADASYN)，这两种技术都包含在imblearn Python库中。</p><p id="7d11" class="pw-post-body-paragraph kx ky iq kz b la lt jr lc ld lu ju lf lg lv li lj lk lw lm ln lo lx lq lr ls ij bi translated">最近，提出了使用聚焦损失目标函数。该技术被宗-林逸等人用于二元分类。</p><p id="1553" class="pw-post-body-paragraph kx ky iq kz b la lt jr lc ld lu ju lf lg lv li lj lk lw lm ln lo lx lq lr ls ij bi translated">在这篇文章中，我将演示如何将焦点损失合并到LightGBM分类器中进行多类分类。该代码可在<a class="ae ly" href="https://github.com/lucacarniato/MultiClassLightgbmWithFocalLoss" rel="noopener ugc nofollow" target="_blank"> GitHub </a>上获得。</p><h1 id="486f" class="kf kg iq bd kh ki kj kk kl km kn ko kp jw kq jx kr jz ks ka kt kc ku kd kv kw bi translated">二元分类</h1><p id="24e2" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">对于二元分类问题(标签0/1 ),焦点损失函数定义如下:</p><figure class="ma mb mc md gt me gh gi paragraph-image"><div class="gh gi lz"><img src="../Images/8c13e573106b50c65dca2624962d2fb3.png" data-original-src="https://miro.medium.com/v2/resize:fit:458/1*_1JS2Wn7enaO-AgLX_woMw.gif"/></div><p class="mh mi gj gh gi mj mk bd b be z dk translated">等式1聚焦损失函数</p></figure><p id="592d" class="pw-post-body-paragraph kx ky iq kz b la lt jr lc ld lu ju lf lg lv li lj lk lw lm ln lo lx lq lr ls ij bi translated">其中<em class="ml"> pₜ </em>是真实标签的函数。对于二元分类，该函数定义为:</p><figure class="ma mb mc md gt me gh gi paragraph-image"><div class="gh gi mm"><img src="../Images/6be23d7a6c14cb60e1fb31dae5acdc25.png" data-original-src="https://miro.medium.com/v2/resize:fit:372/1*tToZn-xq3o9EP7sTEsH38g.gif"/></div><p class="mh mi gj gh gi mj mk bd b be z dk translated">等式2类别概率</p></figure><p id="c9ed" class="pw-post-body-paragraph kx ky iq kz b la lt jr lc ld lu ju lf lg lv li lj lk lw lm ln lo lx lq lr ls ij bi translated">其中pₜ是通过将sigmoid函数应用于原始边距而获得的<em class="ml"> z: </em></p><figure class="ma mb mc md gt me gh gi paragraph-image"><div class="gh gi mn"><img src="../Images/0168b93b2805ba684c854f31f1a0d843.png" data-original-src="https://miro.medium.com/v2/resize:fit:186/1*2fitNiwkPDpfISTxRXjClw.gif"/></div><p class="mh mi gj gh gi mj mk bd b be z dk translated">等式3用于将原始边距z转换成类别概率p的Sigmoid函数</p></figure><p id="f6b2" class="pw-post-body-paragraph kx ky iq kz b la lt jr lc ld lu ju lf lg lv li lj lk lw lm ln lo lx lq lr ls ij bi translated">聚焦损失可以被解释为二元交叉熵函数乘以调制因子(1- <em class="ml"> pₜ </em> )^ <em class="ml"> γ </em>，这减少了易于分类的样本的贡献。加权因子<em class="ml"> aₜ </em>平衡调制因子。引用作者的话:<em class="ml">“当γ = 2时，与CE相比，pt = 0.9的示例的损耗低100倍，pt ≈ 0.968的示例的损耗低1000倍”。减少容易分类的例子的损失允许训练更多地集中在难以分类的例子上”。</em></p><p id="aefb" class="pw-post-body-paragraph kx ky iq kz b la lt jr lc ld lu ju lf lg lv li lj lk lw lm ln lo lx lq lr ls ij bi translated">在Max Halford的博客[ <a class="ae ly" href="https://maxhalford.github.io/blog/lightgbm-focal-loss/" rel="noopener ugc nofollow" target="_blank"> 2 </a> ]中可以找到一篇关于在二进制LigthGBM分类器中合并焦点损失的精彩文章。</p><h1 id="0991" class="kf kg iq bd kh ki kj kk kl km kn ko kp jw kq jx kr jz ks ka kt kc ku kd kv kw bi translated">多类分类</h1><p id="6236" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">在多类分类器中引入聚焦损失有几种方法。形式上，调制和加权因子应该应用于分类交叉熵。这种方法要求提供多级损失的一阶和二阶导数作为原始利润<em class="ml"> z </em>。</p><p id="497f" class="pw-post-body-paragraph kx ky iq kz b la lt jr lc ld lu ju lf lg lv li lj lk lw lm ln lo lx lq lr ls ij bi translated">另一种方法是使用一个对其余的(OvR)，其中为每个类<em class="ml"> C </em>训练一个二元分类器。来自类别<em class="ml"> C </em>的数据被视为正值，所有其他数据被视为负值。在这篇文章中，使用了OvR方法，采用了Halford开发的二元分类器。</p><p id="0c15" class="pw-post-body-paragraph kx ky iq kz b la lt jr lc ld lu ju lf lg lv li lj lk lw lm ln lo lx lq lr ls ij bi translated">下面显示的OneVsRestLightGBMWithCustomizedLoss类封装了这种方法:</p><figure class="ma mb mc md gt me"><div class="bz fp l di"><div class="mo mp l"/></div></figure><p id="7e09" class="pw-post-body-paragraph kx ky iq kz b la lt jr lc ld lu ju lf lg lv li lj lk lw lm ln lo lx lq lr ls ij bi translated">此类重新实现sklearn.multiclass命名空间的OneVsRestClassifier类。重新实现最初的OneVsRestClassifier类的动机是能够向fit方法转发附加参数。这有助于传递评估集(eval_set)以提前停止，从而减少计算时间并避免过拟合。</p><p id="b436" class="pw-post-body-paragraph kx ky iq kz b la lt jr lc ld lu ju lf lg lv li lj lk lw lm ln lo lx lq lr ls ij bi translated">此外，该类使用通用的LightGBM训练API，在处理原始利润<em class="ml"> z </em>和定制损失函数时，需要使用该API来获得有意义的结果(有关更多详细信息，请参见[ <a class="ae ly" href="https://maxhalford.github.io/blog/lightgbm-focal-loss/#first-order-derivative" rel="noopener ugc nofollow" target="_blank"> 2 </a>)。如果没有这些约束，就有可能实现更一般的类，不仅接受任何损失函数，还接受任何实现Scikit Learn模型接口的模型。</p><p id="7bdc" class="pw-post-body-paragraph kx ky iq kz b la lt jr lc ld lu ju lf lg lv li lj lk lw lm ln lo lx lq lr ls ij bi translated">该类的其他方法是Scikit学习模型接口的一部分:fit、predict和predict_proba。在predict和predict_proba方法中，基本估计量返回原始毛利<em class="ml"> z </em>。请注意，当使用定制的损失函数时，LightGBM会返回原始边距<em class="ml"> z </em>。使用sigmoid函数从边距计算类别概率，如等式所示。3.</p><h1 id="9efe" class="kf kg iq bd kh ki kj kk kl km kn ko kp jw kq jx kr jz ks ka kt kc ku kd kv kw bi translated">一个例子</h1><p id="3b3b" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">让我们首先创建一个包含3个类的人工不平衡数据集，其中1%的样本属于第一类，1%属于第二类，98%属于第三类。通常，数据集分为训练集和测试集:</p><figure class="ma mb mc md gt me"><div class="bz fp l di"><div class="mo mp l"/></div></figure><p id="efc2" class="pw-post-body-paragraph kx ky iq kz b la lt jr lc ld lu ju lf lg lv li lj lk lw lm ln lo lx lq lr ls ij bi translated">为了使实验简单，放弃了早期停止。训练后产生的混淆矩阵如下所示:</p><figure class="ma mb mc md gt me"><div class="bz fp l di"><div class="mo mp l"/></div></figure><figure class="ma mb mc md gt me gh gi paragraph-image"><div class="gh gi mq"><img src="../Images/8c7039b72d6e5a330c72ca159571ad91.png" data-original-src="https://miro.medium.com/v2/resize:fit:672/format:webp/1*4b9lr2qI3hpnBypFLpdY4Q.png"/></div><p class="mh mi gj gh gi mj mk bd b be z dk translated">图1使用标准LightGBM分类器的测试集上的混淆矩阵</p></figure><p id="ccda" class="pw-post-body-paragraph kx ky iq kz b la lt jr lc ld lu ju lf lg lv li lj lk lw lm ln lo lx lq lr ls ij bi translated">对于第一个实验，在测试集上获得了0.990的准确度和0.676的召回值。使用具有焦点损失的OneVsRestLightGBMWithCustomizedLoss分类器重复相同的实验。</p><figure class="ma mb mc md gt me"><div class="bz fp l di"><div class="mo mp l"/></div></figure><p id="a3b0" class="pw-post-body-paragraph kx ky iq kz b la lt jr lc ld lu ju lf lg lv li lj lk lw lm ln lo lx lq lr ls ij bi translated">从上面的代码可以看出，损失函数可以在分类器之外配置，并且可以注入到类构造函数中。通过向fit方法提供一个包含eval_set的字典，可以启用提前停止，如上面的注释行所示。对于第二个实验，产生的混淆矩阵如下所示:</p><figure class="ma mb mc md gt me gh gi paragraph-image"><div class="gh gi mq"><img src="../Images/2ac2ef8be3dc0974a2626ae1ed3a02bf.png" data-original-src="https://miro.medium.com/v2/resize:fit:672/format:webp/1*ie4LKZxvfUeY4eiFXZXCBA.png"/></div><p class="mh mi gj gh gi mj mk bd b be z dk translated">图2使用LightGBM和定制的多类焦损失类(OneVsRestLightGBMWithCustomizedLoss)的测试集上的混淆矩阵</p></figure><p id="acdd" class="pw-post-body-paragraph kx ky iq kz b la lt jr lc ld lu ju lf lg lv li lj lk lw lm ln lo lx lq lr ls ij bi translated">在这种情况下，获得了0.995的准确度和0.838的召回值，比使用默认对数损失的第一个实验有所改进。这一结果在混淆矩阵中也很明显，其中类别0的假阳性和类别1的假阴性显著减少。</p><h1 id="b266" class="kf kg iq bd kh ki kj kk kl km kn ko kp jw kq jx kr jz ks ka kt kc ku kd kv kw bi translated">结论</h1><p id="805f" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">在这篇文章中，我演示了一种在多类分类器中合并焦点损失的方法，使用的是一个对其余部分(OvR)的方法。</p><p id="d62d" class="pw-post-body-paragraph kx ky iq kz b la lt jr lc ld lu ju lf lg lv li lj lk lw lm ln lo lx lq lr ls ij bi translated">不需要使用聚焦损失目标函数、样品重量平衡或人工添加新样品来减少不平衡。在人工生成的多类不平衡数据集上，焦点损失的使用增加了召回值，并消除了少数类中的一些假阳性和假阴性。</p><p id="b813" class="pw-post-body-paragraph kx ky iq kz b la lt jr lc ld lu ju lf lg lv li lj lk lw lm ln lo lx lq lr ls ij bi translated">该方法的有效性必须通过探索真实世界的数据集来确认，其中噪声和非信息特征预期会影响分类结果。</p></div><div class="ab cl mr ms hu mt" role="separator"><span class="mu bw bk mv mw mx"/><span class="mu bw bk mv mw mx"/><span class="mu bw bk mv mw"/></div><div class="ij ik il im in"><p id="afc9" class="pw-post-body-paragraph kx ky iq kz b la lt jr lc ld lu ju lf lg lv li lj lk lw lm ln lo lx lq lr ls ij bi translated">[1]林，T. Y .，戈亚尔，p .，吉尔希克，r .，何，k .，&amp;多拉尔，P. (2017)。密集物体探测的聚焦损失。IEEE计算机视觉国际会议论文集(第2980–2988页)。</p><p id="08f3" class="pw-post-body-paragraph kx ky iq kz b la lt jr lc ld lu ju lf lg lv li lj lk lw lm ln lo lx lq lr ls ij bi translated">[2]马克斯·哈尔福德(2020)。LightGBM的聚焦损耗实现。<a class="ae ly" href="https://maxhalford.github.io/blog/lightgbm-focal-loss/#first-order-derivative" rel="noopener ugc nofollow" target="_blank">https://maxhalford . github . io/blog/light GBM-focal-loss/#一阶导数</a></p></div></div>    
</body>
</html>