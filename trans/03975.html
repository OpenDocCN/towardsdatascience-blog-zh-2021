<html>
<head>
<title>Neural network from TENET exploiting time inversion</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">利用时间反演原理的神经网络</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/neural-network-from-tenet-exploiting-time-inversion-fdc512f5fec3?source=collection_archive---------8-----------------------#2021-04-03">https://towardsdatascience.com/neural-network-from-tenet-exploiting-time-inversion-fdc512f5fec3?source=collection_archive---------8-----------------------#2021-04-03</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="cbbe" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">用递归神经网络建模动态系统</h2></div><p id="a746" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">前一段时间，我使用了一种基于时间反演的时间序列预测方法，这种方法完美地融入了Hans-Georg Zimmermann博士发明的一种特殊类型的递归神经网络，他是一位世界级的深度学习实践者，多年来在西门子领导了一项应用于工业和金融问题的神经网络研究[1]。他利用的潜在哲学思想超出了克里斯托弗·诺兰的宗旨范围，但这种方法仍然是超级合法的。</p></div><div class="ab cl lb lc hu ld" role="separator"><span class="le bw bk lf lg lh"/><span class="le bw bk lf lg lh"/><span class="le bw bk lf lg"/></div><div class="ij ik il im in"><h1 id="dde9" class="li lj iq bd lk ll lm ln lo lp lq lr ls jw lt jx lu jz lv ka lw kc lx kd ly lz bi translated">宗旨的理念</h1><p id="710e" class="pw-post-body-paragraph kf kg iq kh b ki ma jr kk kl mb ju kn ko mc kq kr ks md ku kv kw me ky kz la ij bi translated"><strong class="kh ir">动力系统</strong>是一组状态随时间变化的相关事物。<strong class="kh ir">开放系统</strong>可以对环境做出反应，而<strong class="kh ir">封闭系统</strong>不受环境影响，完全自主运行。<strong class="kh ir"> </strong>它们仿照如下方式:</p><figure class="mg mh mi mj gt mk gh gi paragraph-image"><div role="button" tabindex="0" class="ml mm di mn bf mo"><div class="gh gi mf"><img src="../Images/5ca49513f34ffe2b1e5a7e210184cae6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eF6PBN5JWgDWFZtqhRWfWQ.png"/></div></div><p class="mr ms gj gh gi mt mu bd b be z dk translated">(1)开放和封闭的动力系统模型</p></figure><p id="2d44" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">上述模型假设动态系统具有从过去到现在在时间上前进的<strong class="kh ir">信息</strong>(因果)。</p><p id="97de" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">但是在一些动态系统中,<strong class="kh ir">信息流从未来到现在沿着相反的时间方向前进。这些是动态系统，其随时间的变化受到涉及未来可观测性预测的规划的影响。例如，大多数市场价格不仅由供给和需求决定，还由市场参与者的计划方面决定。在这种情况下，我们可能会受益于使用假设因果和追溯因果影响混合的模型。</strong></p><figure class="mg mh mi mj gt mk gh gi paragraph-image"><div role="button" tabindex="0" class="ml mm di mn bf mo"><div class="gh gi mv"><img src="../Images/a81712d834aaef789396e158519afff9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qmGoL9jxviJNavShEB9q4w.png"/></div></div><p class="mr ms gj gh gi mt mu bd b be z dk translated">(2)具有因果和倒因果影响的动力系统模型</p></figure></div><div class="ab cl lb lc hu ld" role="separator"><span class="le bw bk lf lg lh"/><span class="le bw bk lf lg lh"/><span class="le bw bk lf lg"/></div><div class="ij ik il im in"><h1 id="2227" class="li lj iq bd lk ll lm ln lo lp lq lr ls jw lt jx lu jz lv ka lw kc lx kd ly lz bi translated"><strong class="ak">幕后模特</strong></h1><h2 id="4111" class="mw lj iq bd lk mx my dn lo mz na dp ls ko nb nc lu ks nd ne lw kw nf ng ly nh bi translated">1.封闭动力系统的动机</h2><p id="7ed5" class="pw-post-body-paragraph kf kg iq kh b ki ma jr kk kl mb ju kn ko mc kq kr ks md ku kv kw me ky kz la ij bi translated">让我们首先考虑因果开放动力系统模型(1)。如果对于转换函数，我们采用一个应用于线性状态转换的激活函数，并添加一个输出方程，您可能会识别出一个标准的<strong class="kh ir">递归神经网络</strong> (RNN)，它非常适合于通过构造进行动态系统建模，并广泛用于处理时序数据，例如时间序列和自然语言。</p><figure class="mg mh mi mj gt mk gh gi paragraph-image"><div role="button" tabindex="0" class="ml mm di mn bf mo"><div class="gh gi ni"><img src="../Images/681969a9d8ff2d87cfea798db77016c5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*W3JrFEvM52bbWRmMwjAz8Q.png"/></div></div><p class="mr ms gj gh gi mt mu bd b be z dk translated">(3)开放动力系统的简单递归神经网络</p></figure><p id="77d1" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这是一个相当复杂的模型，需要考虑内部自治系统状态和外部输入。因此，让我们<strong class="kh ir">通过将问题重新表述为更复杂的问题</strong>来简化这个复杂的模型——考虑一下<strong class="kh ir">封闭动力系统</strong>。为此，我们必须通过添加包含外部变量动态的外部子系统来扩展我们的内部系统空间。模型复杂性和问题复杂性之间的这种<strong class="kh ir">权衡</strong>并不便宜，我们以后在教模型理解扩展的内部空间的新变量关系时会付出代价。</p><figure class="mg mh mi mj gt mk gh gi paragraph-image"><div role="button" tabindex="0" class="ml mm di mn bf mo"><div class="gh gi nj"><img src="../Images/988cb837264937bd5d829f60964ceb29.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nbCJrdY8qETRPojNMnpY9g.png"/></div></div><p class="mr ms gj gh gi mt mu bd b be z dk translated">(4)将开放动力系统转化为封闭动力系统</p></figure><p id="01cc" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">但是我们获得了一个简单的模型(5 ),其长期一致性仅取决于内部系统变量的相互关系的强度，而不取决于外部输入，这在实践中也很难获得。请注意，在新模型中，我们还摆脱了复杂的输出提取—我们的输出<strong class="kh ir"> <em class="nk"> y </em> </strong>只是系统状态<strong class="kh ir"> <em class="nk"> s </em> </strong>的子集，没有任何变化。</p><figure class="mg mh mi mj gt mk gh gi paragraph-image"><div role="button" tabindex="0" class="ml mm di mn bf mo"><div class="gh gi nl"><img src="../Images/3c3d749f660e836e361d23e81d22fa22.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Zs_Vi1AG6O9evTpLmvLlQQ.png"/></div></div><p class="mr ms gj gh gi mt mu bd b be z dk translated">(5) RNN模拟一个封闭的动力系统</p></figure><h2 id="e4e2" class="mw lj iq bd lk mx my dn lo mz na dp ls ko nb nc lu ks nd ne lw kw nf ng ly nh bi translated">2.教授因果/时间向前模型</h2><p id="b973" class="pw-post-body-paragraph kf kg iq kh b ki ma jr kk kl mb ju kn ko mc kq kr ks md ku kv kw me ky kz la ij bi translated">在模型(5)中，所有拟合参数位于<strong class="kh ir">权重矩阵a中。</strong>为了拟合它们，我们使用标准误差<strong class="kh ir">时间反向传播</strong> (BPTT)算法，该算法具有在每个时间戳纠正误差的特殊技巧，称为<strong class="kh ir">教师强制</strong>。</p><p id="1c41" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">目标/损失函数</strong>，我们希望通过调整权重矩阵A使其最小化，初始状态偏差是每个时间戳的误差平方和。</p><figure class="mg mh mi mj gt mk gh gi paragraph-image"><div role="button" tabindex="0" class="ml mm di mn bf mo"><div class="gh gi nm"><img src="../Images/f84144c90bafc962b9e4fcde31622279.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*d6rae9C3DwBSXk8eHCb4ZQ.png"/></div></div><p class="mr ms gj gh gi mt mu bd b be z dk translated">(6)目标/损失函数——系统的识别</p></figure><p id="7fe3" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">拟合RNN </strong>，即时间反向传播涉及<br/> 1)正向传递，其中我们计算每个时间戳的输出<br/> 2)每个时间戳的误差计算，然后总误差计算<br/> 3)通过时间反向传播误差来计算总误差相对于模型参数的梯度<br/> 4)根据梯度更新权重<br/>重复步骤1-4，直到模型误差足够低。</p><p id="bdc7" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">由于只有一条历史路径，即观察序列<strong class="kh ir"> <em class="nk"> y </em> </strong>，我们只有一个数据样本来训练模型。</p><p id="492f" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在拟合过程的<strong class="kh ir">正向传递</strong>中，我们使用<strong class="kh ir">教师强制</strong> —在每个时间戳，目标被替换在对应于可观察输出<strong class="kh ir"><em class="nk"/></strong><em class="nk">的内部系统状态<strong class="kh ir">的部分中。</strong></em>然而，系统内部状态的其余部分<strong class="kh ir"> <em class="nk"> s </em> </strong>对应的<strong class="kh ir"> <em class="nk"> </em> </strong>这个不可观测的系统变量保持不变。这样，我们不会传播错误，因此在训练模型时不会累积错误。</p><figure class="mg mh mi mj gt mk gh gi paragraph-image"><div role="button" tabindex="0" class="ml mm di mn bf mo"><div class="gh gi nn"><img src="../Images/0a715969d5a6675fed971455360b2830.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dqUK8DCtNnxHnAjwFOX87w.png"/></div></div><p class="mr ms gj gh gi mt mu bd b be z dk translated">(7)教师强制纠错</p></figure><p id="4416" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在向前传递之后，我们可以使用模型输出和目标容易地计算总误差(6)。然后，我们必须计算误差相对于模型参数的梯度。</p><figure class="mg mh mi mj gt mk gh gi paragraph-image"><div role="button" tabindex="0" class="ml mm di mn bf mo"><div class="gh gi no"><img src="../Images/cbf2c089301c87b0220ac65fc68701e6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Mdy_MtXXnYyAGhZCuLWG3A.png"/></div></div><p class="mr ms gj gh gi mt mu bd b be z dk translated">(8)关于权重矩阵的误差梯度</p></figure><p id="43f8" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在算法的最后一步，使用计算的梯度，我们更新模型参数/状态转移相关性，即权重矩阵A:</p><figure class="mg mh mi mj gt mk gh gi paragraph-image"><div class="gh gi np"><img src="../Images/59231666566856a19340dd902f1ef804.png" data-original-src="https://miro.medium.com/v2/resize:fit:822/format:webp/1*4FNZTCh-3Ko83wus3--OQg.png"/></div><p class="mr ms gj gh gi mt mu bd b be z dk translated">(9)BPTT权重矩阵更新</p></figure><p id="1631" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">上面描述的带教师强迫的递归神经网络称为<strong class="kh ir">误差修正神经网络(ECNN) </strong>或历史一致性神经网络<strong class="kh ir"/>(HCNN)【2】。</p><p id="bd9d" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">请注意，由于自治系统扩展(4)，权重矩阵A可能非常大，其中标量权重的数量等于系统变量总数的平方。当我们增加系统空间时，它会快速增长，这是建模方法的一个瓶颈。加快训练过程的一个很好的实用技巧是使用稀疏权重矩阵A——假设系统变量之间的一些联系比假设一切都是相互关联的更接近现实。</p><p id="9517" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">您还可以查看我对这一部分的python实现:</p><ul class=""><li id="7f2d" class="nq nr iq kh b ki kj kl km ko ns ks nt kw nu la nv nw nx ny bi translated">in plain NumPy:<a class="ae nz" href="https://github.com/uselessskills/hcnn/blob/master/tutorials/hcnn_numpy.ipynb" rel="noopener ugc nofollow" target="_blank">https://github . com/uselesskills/hcnn/blob/master/tutorials/hcnn _ NumPy . ipynb</a></li><li id="4252" class="nq nr iq kh b ki oa kl ob ko oc ks od kw oe la nv nw nx ny bi translated">带PyTorch后端:<a class="ae nz" href="https://github.com/uselessskills/hcnn/blob/master/tutorials/hcnn_pytorch.ipynb" rel="noopener ugc nofollow" target="_blank">https://github . com/uselesskills/hcnn/blob/master/tutorials/hcnn _ py torch . ipynb</a></li></ul><h2 id="b8a6" class="mw lj iq bd lk mx my dn lo mz na dp ls ko nb nc lu ks nd ne lw kw nf ng ly nh bi translated">3.时间反演:从因果模型到逆因果模型</h2><p id="cf55" class="pw-post-body-paragraph kf kg iq kh b ki ma jr kk kl mb ju kn ko mc kq kr ks md ku kv kw me ky kz la ij bi translated">我在Zimmermann博士的演讲中发现的逆向工程原理的表述阐明了追溯因果模型背后的内在动机:</p><blockquote class="of og oh"><p id="c4d7" class="kf kg nk kh b ki kj jr kk kl km ju kn oi kp kq kr oj kt ku kv ok kx ky kz la ij bi translated">当我们能够确定代理人的目标时，我们就可以试着从这些目标向后解释动力学</p></blockquote><p id="e4d7" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">事实上，我们并不局限于只学习因果依赖，在这里它被利用了。让我们假设<strong class="kh ir">我们生活在反转的时间世界</strong>中，简单地使用相同的模型架构来学习从未来到过去的系统转换。</p><figure class="mg mh mi mj gt mk gh gi paragraph-image"><div role="button" tabindex="0" class="ml mm di mn bf mo"><div class="gh gi ol"><img src="../Images/56fcaa4bfb145762a25e4b98bb1050d9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*l1jRXikyennX7Ugj-47WQQ.png"/></div></div><p class="mr ms gj gh gi mt mu bd b be z dk translated">(10)时间倒推的追溯因果RNN</p></figure><p id="09ca" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">同样的方法，我们应用BPTT与教师强迫，以适应倒置的时间系统转移权重矩阵和初始状态偏差。</p><p id="ce38" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">对于这部分的实现，我们基本上只需要在<strong class="kh ir">倒序</strong>上训练来自第二部分的标准<strong class="kh ir">因果RNN </strong>。</p><h2 id="9c15" class="mw lj iq bd lk mx my dn lo mz na dp ls ko nb nc lu ks nd ne lw kw nf ng ly nh bi translated">4.【宗旨】因果和逆因果神经网络的叠加</h2><p id="7cea" class="pw-post-body-paragraph kf kg iq kh b ki ma jr kk kl mb ju kn ko mc kq kr ks md ku kv kw me ky kz la ij bi translated">最后，我们将正常时间网络和反转时间网络对称地组合成一个网络[3]。</p><figure class="mg mh mi mj gt mk gh gi paragraph-image"><div role="button" tabindex="0" class="ml mm di mn bf mo"><div class="gh gi om"><img src="../Images/bb7eec124c8aac7e17b66714e499c140.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mBSJ2hBWoU7ykMbdQfiFVg.png"/></div></div><p class="mr ms gj gh gi mt mu bd b be z dk translated">(11)组合的因果和逆因果神经网络</p></figure><p id="4ad6" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">再次训练这个网络需要BPTT和老师的强迫。</p><figure class="mg mh mi mj gt mk gh gi paragraph-image"><div role="button" tabindex="0" class="ml mm di mn bf mo"><div class="gh gi on"><img src="../Images/12495d5e0fa9b78dd30907ea8ad9776c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PxUdlSp3jsqAGth6leVvUg.png"/></div></div><p class="mr ms gj gh gi mt mu bd b be z dk translated">(12)因果和追溯因果网络结合的教师强迫</p></figure><p id="a111" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这部分我在PyTorch上的实现:<a class="ae nz" href="https://github.com/uselessskills/hcnn/blob/master/tutorials/crc_hcnn.ipynb" rel="noopener ugc nofollow" target="_blank">https://github . com/uselesskills/hcnn/blob/master/tutorials/CRC _ hcnn . ipynb</a></p><h1 id="599f" class="li lj iq bd lk ll oo ln lo lp op lr ls jw oq jx lu jz or ka lw kc os kd ly lz bi translated">结论</h1><p id="b3b0" class="pw-post-body-paragraph kf kg iq kh b ki ma jr kk kl mb ju kn ko mc kq kr ks md ku kv kw me ky kz la ij bi translated">我们从TENET提出的时间反演思想开始，然后将其映射到一个由递归神经网络驱动的模型。还是真的反过来了？</p><p id="3ce7" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">特别感谢阿列克谢·米宁博士，他向我介绍了这种非凡的方法，并在我试图理解它时耐心地给我多次建议，直到我感觉到它。</p><h1 id="7130" class="li lj iq bd lk ll oo ln lo lp op lr ls jw oq jx lu jz or ka lw kc os kd ly lz bi translated">参考</h1><p id="5b0a" class="pw-post-body-paragraph kf kg iq kh b ki ma jr kk kl mb ju kn ko mc kq kr ks md ku kv kw me ky kz la ij bi translated">[1]<a class="ae nz" href="https://www.researchgate.net/profile/Hans-Zimmermann-4" rel="noopener ugc nofollow" target="_blank">https://www.researchgate.net/profile/Hans-Zimmermann-4</a></p><p id="0e51" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">[2] <a class="ae nz" href="https://link.springer.com/chapter/10.1007/978-3-642-28696-4_10" rel="noopener ugc nofollow" target="_blank">齐默尔曼汞。、Tietz C .、Grothmann R .,《历史一致性神经网络:市场建模、预测和风险分析的新视角》,施普林格，2013年</a></p><p id="13ae" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">[3] <a class="ae nz" href="https://link.springer.com/chapter/10.1007/978-3-642-29210-1_92" rel="noopener ugc nofollow" target="_blank">齐默尔曼汞。Grothmann R .，Tietz C,“用因果-回溯-因果神经网络预测市场价格”, Springer，2012年</a></p><p id="3fd0" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">[4] Python包与我实现的网络描述(HCNN，Causal &amp; Retro-Causal NN):<a class="ae nz" href="https://github.com/uselessskills/hcnn" rel="noopener ugc nofollow" target="_blank">https://github.com/uselessskills/hcnn</a></p></div></div>    
</body>
</html>