<html>
<head>
<title>Twitter Sentiment Analysis with Keras and NLTK</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">基于Keras和NLTK的Twitter情感分析</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/twitter-sentiment-analysis-with-keras-and-nltk-48189967f190?source=collection_archive---------6-----------------------#2021-12-09">https://towardsdatascience.com/twitter-sentiment-analysis-with-keras-and-nltk-48189967f190?source=collection_archive---------6-----------------------#2021-12-09</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="7361" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">我们如何学会发现与各种在线实体相关的推文的情感？</h2></div><p id="3326" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">社交媒体代表了人们在任何时候都可以获得的关于人们的无穷无尽的信息。困难在于如何利用这些丰富的信息。</p><figure class="ld le lf lg gt lh gh gi paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="gh gi lc"><img src="../Images/0bd84db36f647de6ed298fb2b597cc55.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*QktTXQJIZSZZ7QLm"/></div></div><p class="lo lp gj gh gi lq lr bd b be z dk translated">在<a class="ae ls" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上<a class="ae ls" href="https://unsplash.com/@visuals?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">视觉</a>拍摄的照片</p></figure><p id="ba48" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">文本包含了很多信息，我们作为人类很容易提取。然而，对于计算机来说，这要困难得多，因为理解自然语言需要庞大的知识库和上下文。理解这种自然语言会带来巨大的好处，尤其是在情感分析方面。例如，一家公司可能希望利用社交媒体来了解公众对其产品的意见，以了解是否需要改进或公关变化(记住相关偏差，如<a class="ae ls" href="https://en.wikipedia.org/wiki/Self-selection_bias" rel="noopener ugc nofollow" target="_blank">自我选择偏差</a>)。</p><h1 id="013a" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated">数据集</h1><p id="8419" class="pw-post-body-paragraph kf kg iq kh b ki ml jr kk kl mm ju kn ko mn kq kr ks mo ku kv kw mp ky kz la ij bi translated">Kaggle [1]上的<a class="ae ls" href="https://www.kaggle.com/jp797498e/twitter-entity-sentiment-analysis" rel="noopener ugc nofollow" target="_blank">“Twitter情绪分析”</a>数据集是大约74，000条推文的集合，它们涉及的实体或公司，以及指定的情绪。有了这个数据集，我们可以尝试训练一个分类模型，根据对给定实体或公司的情感对进一步的推文进行分类。</p><p id="6572" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">下面给出了该数据集的一个示例。可以看到，有一些错误的数据点，例如标有“积极”的推文，推文的文本只写着“曾经”。这些推文可能会导致分类错误，但由于推文整体长度较短，它们会留在数据集中。</p><figure class="ld le lf lg gt lh"><div class="bz fp l di"><div class="mq mr l"/></div><p class="lo lp gj gh gi lq lr bd b be z dk translated">此项目的数据集的前几行示例。</p></figure><p id="ad6a" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这个数据集涵盖了几个不同的“实体”。一个容易(但不准确，不可靠！)对推文进行分类的方法是查看每个实体最常见的情绪，并预测该情绪！然而，这确实挫败了机器学习和试图理解推文中语言的意义。在接下来的部分，我们将探索一些策略来更好地理解推文中的文字，看看我们是否能比这种猜测做得更好。</p><h1 id="00e7" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated">基础数据丰富</h1><h2 id="cb5d" class="ms lu iq bd lv mt mu dn lz mv mw dp md ko mx my mf ks mz na mh kw nb nc mj nd bi translated">一键编码</h2><p id="0c7c" class="pw-post-body-paragraph kf kg iq kh b ki ml jr kk kl mm ju kn ko mn kq kr ks mo ku kv kw mp ky kz la ij bi translated">在处理Tweet文本之前，我们可以通过我们的模型轻松地将实体类编码成可用的数据。这可以通过一键编码轻松完成，因为数据集中只包含32个不同的实体，这样就增加了32个新列。这可以通过“get_dummies()”函数使用已经用于托管数据集本身的<a class="ae ls" href="https://pandas.pydata.org/" rel="noopener ugc nofollow" target="_blank"> Pandas </a>模块轻松实现:</p><figure class="ld le lf lg gt lh"><div class="bz fp l di"><div class="mq mr l"/></div><p class="lo lp gj gh gi lq lr bd b be z dk translated">熊猫一键编码。</p></figure><p id="81c6" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">通过一键编码，我们允许自己从Tweet描述的实体中获得一些洞察力。我们的模型现在将把总体情绪与实体本身联系起来，除了Tweet的文本之外还将使用它。</p><h2 id="9314" class="ms lu iq bd lv mt mu dn lz mv mw dp md ko mx my mf ks mz na mh kw nb nc mj nd bi translated">预先训练的情感分析</h2><p id="6bbc" class="pw-post-body-paragraph kf kg iq kh b ki ml jr kk kl mm ju kn ko mn kq kr ks mo ku kv kw mp ky kz la ij bi translated">在我们自己分析文本之前，我们可以采取的一个步骤是使用预先训练的模型来获得对原始文本的一些洞察。这些用于情感分析的预训练模型通常是在比我们将要使用的数据集大得多的数据集上训练的，并且可能从我们构建的模型中提取不同的见解。我们在这种情况下使用的预训练模型是TextBlob，它是基于NLTK构建的。这个模型接受原始文本输入，有两个输出，极性和主观性。我们可以使用下面的代码片段将这些数据提取到新列中:</p><figure class="ld le lf lg gt lh"><div class="bz fp l di"><div class="mq mr l"/></div><p class="lo lp gj gh gi lq lr bd b be z dk translated">下面是我们如何用TextBlob做预建的情感分析(基于NLTK！)</p></figure><h1 id="3b2d" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated">更多涉及丰富(文本处理)</h1><p id="472b" class="pw-post-body-paragraph kf kg iq kh b ki ml jr kk kl mm ju kn ko mn kq kr ks mo ku kv kw mp ky kz la ij bi translated">语言处理项目的下一步是从文本中提取特征，以便机器学习模型可以更好地理解它。这包括停用词删除、标记化和词干化等步骤。</p><h2 id="dc2f" class="ms lu iq bd lv mt mu dn lz mv mw dp md ko mx my mf ks mz na mh kw nb nc mj nd bi translated">拆分文本</h2><p id="b789" class="pw-post-body-paragraph kf kg iq kh b ki ml jr kk kl mm ju kn ko mn kq kr ks mo ku kv kw mp ky kz la ij bi translated">这是我们将文本拆分成其组成部分的步骤，组成部分通常对应于单个单词。这分两点完成，首先是为了我们可以执行其他步骤，然后是最终确定我们的数据。</p><h2 id="007e" class="ms lu iq bd lv mt mu dn lz mv mw dp md ko mx my mf ks mz na mh kw nb nc mj nd bi translated">停止单词删除</h2><p id="8a44" class="pw-post-body-paragraph kf kg iq kh b ki ml jr kk kl mm ju kn ko mn kq kr ks mo ku kv kw mp ky kz la ij bi translated">这一步包括删除所有实际上不传达任何信息的常用词。这些词的例子可能包括“the”、“and”和“it”。这些词一般存在于所有自然语言中，并不给出文本情感的任何信息。在大多数情况下，我们可以使用NLTK的停用词列表<code class="fe ne nf ng nh b">stopwords.words()</code>，但是我们可能想要根据问题的上下文向该列表添加词。</p><h2 id="11bc" class="ms lu iq bd lv mt mu dn lz mv mw dp md ko mx my mf ks mz na mh kw nb nc mj nd bi translated">堵塞物</h2><p id="138c" class="pw-post-body-paragraph kf kg iq kh b ki ml jr kk kl mm ju kn ko mn kq kr ks mo ku kv kw mp ky kz la ij bi translated">词干是一个稍微复杂一点的话题，涉及到将单词简化为词干。一个单词的词干可以说是它的基本形式。例如，在这一步中，“Drive”和“Driving”都应还原为它们的词干“Drive”。在实践中，这些单词通常被简化为词干“Driv ”,将其解释为人类很奇怪，但仍然是有效的词干，因为两个单词都被简化为同一个词干。当这个奇怪的词干也是文本中出现的一个单词时，可能会出现问题，但这种情况通常很少，不会成为我们模型的主要问题。</p><p id="abda" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们实际上并没有在这个模型中实现词干来支持引理满足。</p><h2 id="aaeb" class="ms lu iq bd lv mt mu dn lz mv mw dp md ko mx my mf ks mz na mh kw nb nc mj nd bi translated">引理满足</h2><p id="7f99" class="pw-post-body-paragraph kf kg iq kh b ki ml jr kk kl mm ju kn ko mn kq kr ks mo ku kv kw mp ky kz la ij bi translated">引理满足是一种目的类似于词干提取的方法，但在某些情况下可能更有效。这种方法试图使用单词的上下文来将它们转换成常见的形式。比如，lemmatisation要把“是”、“是”、“是”都转换成“是”。不幸的是，lemmatisation的真正实现通常不能很好地执行，尽管在这种情况下，我们更喜欢它而不是词干。</p><h2 id="70e9" class="ms lu iq bd lv mt mu dn lz mv mw dp md ko mx my mf ks mz na mh kw nb nc mj nd bi translated">将文本转换成可用的格式</h2><p id="0ea0" class="pw-post-body-paragraph kf kg iq kh b ki ml jr kk kl mm ju kn ko mn kq kr ks mo ku kv kw mp ky kz la ij bi translated">不幸的是，我们的模型不能只接受字符串作为输入，而是要求我们以某种方式对文本进行编码。这可以通过多种方式实现，本文将探讨其中的两种方法。</p><p id="c65b" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">文字袋NLP </strong></p><p id="2ee0" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们在这里考虑的第一种方法是最简单的，叫做“单词袋”NLP。此方法不考虑单词的位置，并且仅在给定字符串的任何位置包含单词时才进行编码。我们可以用Keras的tokenizer，通过<code class="fe ne nf ng nh b">texts_to_matix()</code>方法来做到这一点。这为我们提供了一个矩阵，每一列对应一个给定的单词，每一行包含一个指示器，显示该行的文本是否包含该单词。该矩阵中的列数由测试的字数决定，该字数在定义标记器时确定。</p><p id="27fd" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">符号化</strong></p><p id="bc54" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在标记化中，我们专注于保持单词的顺序。我们可以使用Keras的tokeniser的<code class="fe ne nf ng nh b">texts_to_sequences()</code>方法来实现这一点。这个方法生成一个整数序列，每个唯一的整数对应于Tweet中此时使用的单词。唯一整数的数量由标记器的语料库大小定义，该大小在定义标记器时确定。如果使用的单词之一不在语料库中，则忽略它。通常我们会选择一个相对较大的语料库，所以我们只会错过那些我们无论如何都难以理解的生僻字的信息。</p><p id="2004" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">标记化的下一步是填充序列，使它们具有相同的长度，生成大小一致的矩阵输入到我们的模型中。</p><h2 id="4276" class="ms lu iq bd lv mt mu dn lz mv mw dp md ko mx my mf ks mz na mh kw nb nc mj nd bi translated">丢弃数据</h2><p id="e5fd" class="pw-post-body-paragraph kf kg iq kh b ki ml jr kk kl mm ju kn ko mn kq kr ks mo ku kv kw mp ky kz la ij bi translated">然后，包含Tweet文本的列被删除，因为有用的信息已经被提取并转换成有用的格式</p><h1 id="5a8a" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated">分类</h1><p id="0894" class="pw-post-body-paragraph kf kg iq kh b ki ml jr kk kl mm ju kn ko mn kq kr ks mo ku kv kw mp ky kz la ij bi translated">对于本项目中的分类，由于模型输入的数量和类型都很大，因此使用了通过TensorFlow中的Keras构建的神经网络。我们使用两种不同的神经网络结构，对应于对文本数据进行编码的不同方法。这就是“单词袋”方法和“标记化LSTM”方法，这两种方法在下面都有更详细的描述。</p><h2 id="beca" class="ms lu iq bd lv mt mu dn lz mv mw dp md ko mx my mf ks mz na mh kw nb nc mj nd bi translated">单词袋NLP</h2><p id="2d9d" class="pw-post-body-paragraph kf kg iq kh b ki ml jr kk kl mm ju kn ko mn kq kr ks mo ku kv kw mp ky kz la ij bi translated">对于第一种，也是最简单的方法，我们执行单词袋NLP。通过这种方法，我们可以从密集(全连接)层构建一个简单的多层神经网络。有了这个，我们就可以使用我们所掌握的所有数据(一次性编码和预先训练的情感分析)，来尝试预测情感。</p><p id="6f23" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们定义以下模型来获取我们生成的所有数据。致密层允许相邻层中任何神经元之间的数据传输，而脱落层是为了防止权重增长过大。</p><figure class="ld le lf lg gt lh"><div class="bz fp l di"><div class="mq mr l"/></div><p class="lo lp gj gh gi lq lr bd b be z dk translated">在这里，我们构建了一个基本的Keras [2]神经网络——由一系列密集层组成。</p></figure><p id="7a72" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">将来自该数据集的所有训练数据插入到该模型中，并在以相同方式编辑的测试数据上运行，我们得到以下混淆矩阵，显示分类的准确性:</p><figure class="ld le lf lg gt lh gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/ee144a1f2ce34eea1ce832c0c95a2504.png" data-original-src="https://miro.medium.com/v2/resize:fit:702/format:webp/1*n2Cp4GRG3BS9E-U4UinMhQ.png"/></div><p class="lo lp gj gh gi lq lr bd b be z dk translated">混淆矩阵显示了正确(和不正确)分类的测试用例的比例。图片作者。</p></figure><p id="0c78" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">该矩阵显示了每个可能标签的分类准确度。我们可以看到，该模型在预测负面情绪方面表现最佳，91%的预测正确。这给我们留下了84.5%的总体准确率——还不错！</p><h2 id="cf48" class="ms lu iq bd lv mt mu dn lz mv mw dp md ko mx my mf ks mz na mh kw nb nc mj nd bi translated">象征化LSTM</h2><p id="28a4" class="pw-post-body-paragraph kf kg iq kh b ki ml jr kk kl mm ju kn ko mn kq kr ks mo ku kv kw mp ky kz la ij bi translated">这种更复杂的方法使用LSTM神经网络，其中我们使用嵌入和双向层来尝试学习上下文中单词的含义(双向)，而不是对包含这些单词的句子进行分类。这在情感方面是有用的，例如短语“不错！”传达了与“坏”这个词完全不同的意思。</p><p id="83b7" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">然而不幸的是，这种方法只能接受标记化的文本序列作为输入，而不能接受任何其他派生的数据。</p><p id="65eb" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们也以不同的方式构建这个神经网络。该网络包含3种类型的层:</p><ul class=""><li id="e495" class="nj nk iq kh b ki kj kl km ko nl ks nm kw nn la no np nq nr bi translated">嵌入层</li><li id="cace" class="nj nk iq kh b ki ns kl nt ko nu ks nv kw nw la no np nq nr bi translated">双向LSTM</li><li id="4fbc" class="nj nk iq kh b ki ns kl nt ko nu ks nv kw nw la no np nq nr bi translated">稠密的</li></ul><p id="4de6" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">嵌入</strong></p><p id="977b" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">嵌入层是我们网络的一个非常重要的部分，它允许我们根据周围其他单词的上下文来学习单词的意思。实际上，这一层将每个单词放入一个向量空间，我们将学习这个向量空间的位置以及它周围可能使用的单词。</p><p id="5e60" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><a class="ae ls" href="https://machinelearningmastery.com/use-word-embedding-layers-deep-learning-keras/" rel="noopener ugc nofollow" target="_blank">在这里了解更多关于嵌入图层的信息</a> [3]</p><p id="4a05" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">双向LSTM </strong></p><p id="2a14" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">双向层是对传统递归神经网络(RNNs)的改进，在这种网络中，数据以给定的顺序传递，因此我们可以从文本中单词的顺序进行学习。在双向层中，这发生在两个方向上，所以我们可以尝试在向前和向后的上下文中学习单词的意思。我们在这个网络中使用LSTMs(长短期记忆节点),这样我们的网络可以“记住”以前的上下文，也可以“忘记”不相关的信息。对于文本处理等应用，LSTMs比传统的rnn工作得更好，因为它们通过允许渐变不变地通过来解决渐变消失的问题。</p><p id="a1e5" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">密集</strong></p><p id="d78b" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">网络的最后一层是密集层。这仅仅意味着这一层中的所有节点都连接到前一层中的所有节点，从而可以包括来自任何LSTM单元的输入。我们使用softmax输出函数和这里的4个节点来生成每个潜在输出(每个可能的情感)的概率。</p><p id="526f" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">模型建造</strong></p><p id="f381" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">用于该分析的神经网络模型构造如下，嵌入层馈入两个双向LSTM层，输出密集层。</p><figure class="ld le lf lg gt lh"><div class="bz fp l di"><div class="mq mr l"/></div><p class="lo lp gj gh gi lq lr bd b be z dk translated">一种更复杂的双向LSTM神经网络的构造。受塞尔吉奥·维拉洪达的文章【4】的启发</p></figure><p id="a51b" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">那么，这个模型在使用同样的数据时表现如何呢？因为这是一个更先进和最新的方法，我们应该期待更好的性能，对不对？不幸的是，这里的情况并非如此:</p><figure class="ld le lf lg gt lh gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/3400c28ce5913b829c7b475cd4659474.png" data-original-src="https://miro.medium.com/v2/resize:fit:702/format:webp/1*WGoRweVZ-96A4EciNRaWBA.png"/></div><p class="lo lp gj gh gi lq lr bd b be z dk translated">同一数据集LSTM分类的混淆矩阵。图片作者。</p></figure><p id="afa4" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">发生了什么事？为什么我们的准确率这么低？</p><p id="9ec6" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这有几个潜在的原因。首先，我们实际上不知道训练数据中的推文是如何分类的。也许使用了一个单词袋类型分类器，这意味着我们只能复制它。接下来的步骤可能是检查训练和测试数据，看看是否是这种情况，并看看模型在编辑的数据上是否表现得更好。</p><p id="c7fc" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">此外，我们应该知道我们的模型的局限性。双向LSTM模型通常需要较长的文本部分来学习单词的上下文，这在推文中是不可能的。双向LSTM模型真正出彩的地方是在较长的文本部分，对于评论分类等任务，它的表现要比词袋分类器好得多。因此，我们通常不期望这种模型在Tweets上有很好的表现，因为缺乏上下文，单词袋通常可以做得很好。</p><h1 id="6b0d" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated">结论</h1><p id="9a01" class="pw-post-body-paragraph kf kg iq kh b ki ml jr kk kl mm ju kn ko mn kq kr ks mo ku kv kw mp ky kz la ij bi translated">事实证明，我们可以用“标准的”单词袋分类器对推文进行相当好的分类。我们可以推测，这在很大程度上是由于推文中缺乏上下文，这意味着更复杂的句子结构难以出现。</p><p id="e6c4" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如果我们与分类器中提到的实体之一有联系，我们可能希望设置它来自动检测人们对我们的产品或服务的意见！这可以为人们的想法提供真正有价值的见解，而不必再手动收集数据。</p><p id="06c6" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">研究更高级的方法，如预训练的BERT(来自变压器的双向编码器表示)模型，可能会产生比我们在这里得到的更好的结果！如果你想改进这个模型，BERT很可能是你开始的地方！</p><h1 id="c753" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated">代码库</h1><p id="575c" class="pw-post-body-paragraph kf kg iq kh b ki ml jr kk kl mm ju kn ko mn kq kr ks mo ku kv kw mp ky kz la ij bi translated">这个项目的代码库可以在GitHub <a class="ae ls" href="https://github.com/Cameron-Watts/Twitter_Sentiment_Analysis" rel="noopener ugc nofollow" target="_blank">这里</a>找到，或者在下面的链接:【https://github.com/Cameron-Watts/Twitter_Sentiment_Analysis】T2</p><h1 id="7240" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated">参考</h1><p id="e202" class="pw-post-body-paragraph kf kg iq kh b ki ml jr kk kl mm ju kn ko mn kq kr ks mo ku kv kw mp ky kz la ij bi translated">[1]激情-nlp，Twitter情绪分析，[第二版] (2021)，<a class="ae ls" href="https://www.kaggle.com/jp797498e/twitter-entity-sentiment-analysis" rel="noopener ugc nofollow" target="_blank">https://www . ka ggle . com/jp 797498 e/Twitter-entity-情操-分析</a>。</p><p id="a8a2" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">[2] Chollet，f .等人，Keras (2015)，<a class="ae ls" href="https://github.com/fchollet/keras" rel="noopener ugc nofollow" target="_blank">https://github.com/fchollet/keras</a>。</p><p id="2422" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">[3] J. Brownlee，<a class="ae ls" href="https://machinelearningmastery.com/use-word-embedding-layers-deep-learning-keras/" rel="noopener ugc nofollow" target="_blank">如何用Keras使用Word嵌入层进行深度学习</a> (2017)，<a class="ae ls" href="https://machinelearningmastery.com/use-word-embedding-layers-deep-learning-keras/" rel="noopener ugc nofollow" target="_blank">https://machinelingmastery . com/Use-Word-Embedding-Layers-Deep-Learning-Keras/</a>。</p><p id="5801" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">[4] S. Virahonda，<a class="ae ls" rel="noopener" target="_blank" href="/an-easy-tutorial-about-sentiment-analysis-with-deep-learning-and-keras-2bf52b9cba91">关于深度学习和Keras的情感分析的简单教程</a> (2020)，<a class="ae ls" rel="noopener" target="_blank" href="/an-easy-tutorial-about-sentiment-analysis-with-deep-learning-and-keras-2bf52b9cba91">https://towards data science . com/An-easy-tutorial-about-sensation-Analysis-with-Deep-Learning-and-Keras-2bf 52 b 9 CBA 91</a></p></div></div>    
</body>
</html>