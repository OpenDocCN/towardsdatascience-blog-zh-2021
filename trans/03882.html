<html>
<head>
<title>MultiLingual Amazon Reviews Classification</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">多语言亚马逊评论分类</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/multilingual-amazon-reviews-classification-f55f8a650c9a?source=collection_archive---------29-----------------------#2021-03-31">https://towardsdatascience.com/multilingual-amazon-reviews-classification-f55f8a650c9a?source=collection_archive---------29-----------------------#2021-03-31</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="8cfb" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">用数据做很酷的事情</h2></div><h1 id="22fe" class="ki kj it bd kk kl km kn ko kp kq kr ks jz kt ka ku kc kv kd kw kf kx kg ky kz bi translated">介绍</h1><p id="b3bd" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">NLP将成为这十年中最具变革性的技术。变压器模型推动了NLP的发展。在这篇博客中，我们来看看转换器在多语言数据上的应用。现在，许多企业的业务范围遍及全球，并以多种语言收集数据。NLP也在建立强大的处理多语言数据的能力。Transformer模型现在可以用多种语言进行训练，并无缝地接收其中的文本。这太神奇了！</p><figure class="lx ly lz ma gt mb gh gi paragraph-image"><div class="gh gi lw"><img src="../Images/8257ccae92d1ccddb144b9046289598d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1312/format:webp/1*mP92-xo6yt2IHd4KXxDMPw.jpeg"/></div><p class="me mf gj gh gi mg mh bd b be z dk translated">来源:免版税的Unsplash—<a class="ae mi" href="https://unsplash.com/photos/jhtPCeQ2mMU" rel="noopener ugc nofollow" target="_blank">https://unsplash.com/photos/jhtPCeQ2mMU</a></p></figure><p id="f8e0" class="pw-post-body-paragraph la lb it lc b ld mj ju lf lg mk jx li lj ml ll lm ln mm lp lq lr mn lt lu lv im bi translated">在这篇博客中，我们使用了一个<a class="ae mi" href="https://arxiv.org/pdf/1911.02116.pdf" rel="noopener ugc nofollow" target="_blank">预先训练的多语言XLM罗伯塔模型</a>，并在下游分类任务中对其进行了微调。XLM-罗伯塔已经接受了超过100种不同语言的训练！该模型能够在多种语言的产品评论数据集上提供最先进的性能。我认为这为企业整合来自不同国家的运营数据并产生有价值的见解提供了新的机会。</p><p id="28d9" class="pw-post-body-paragraph la lb it lc b ld mj ju lf lg mk jx li lj ml ll lm ln mm lp lq lr mn lt lu lv im bi translated">这个模型的代码可以在我的Colab笔记本<a class="ae mi" href="https://colab.research.google.com/drive/1dfn3uCAYh1L812StenVWjVUSNbWcS0jh?usp=sharing" rel="noopener ugc nofollow" target="_blank">这里</a>找到。我也上传到我的GitHub <a class="ae mi" href="https://github.com/priya-dwivedi/Deep-Learning/blob/master/MultiLingual-Amazon-Reviews-Classification/Text_classification_on_Multiilingual_Reviews.ipynb" rel="noopener ugc nofollow" target="_blank">这里</a>。</p><p id="88da" class="pw-post-body-paragraph la lb it lc b ld mj ju lf lg mk jx li lj ml ll lm ln mm lp lq lr mn lt lu lv im bi translated">这里的代码是使用HuggingFace和Datasets库运行分类的通用代码。它可以很容易地修改为许多其他数据集和涉及分类的问题。</p><h1 id="ac32" class="ki kj it bd kk kl km kn ko kp kq kr ks jz kt ka ku kc kv kd kw kf kx kg ky kz bi translated">数据集和预处理</h1><p id="b4d2" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">对于这个博客，我们使用<a class="ae mi" href="https://colab.research.google.com/drive/1dfn3uCAYh1L812StenVWjVUSNbWcS0jh?usp=sharing" rel="noopener ugc nofollow" target="_blank">亚马逊多语言评论语料库</a>。该数据集由亚马逊公开提供，包含6种不同语言(英语、日语、德语、法语、西班牙语和中文)的各种产品的客户评论。数据集库提供了一个简单的API来下载和访问数据集。</p><pre class="lx ly lz ma gt mo mp mq mr aw ms bi"><span id="21a4" class="mt kj it mp b gy mu mv l mw mx">!pip install datasets<br/>dataset  = datasets.load_dataset('amazon_reviews_multi')</span></pre><p id="526c" class="pw-post-body-paragraph la lb it lc b ld mj ju lf lg mk jx li lj ml ll lm ln mm lp lq lr mn lt lu lv im bi translated">数据集在训练集中有1.2MM的评论，在验证集中有30K的评论。每篇评论都有标题、类别、正文和星级。星级从1到5不等，平均分布在5个等级中。</p><figure class="lx ly lz ma gt mb gh gi paragraph-image"><div role="button" tabindex="0" class="mz na di nb bf nc"><div class="gh gi my"><img src="../Images/e30488a22e646c7803d5e6b417ec331e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LaHkVL4MpCIICqdHk7UFug.png"/></div></div></figure><p id="1593" class="pw-post-body-paragraph la lb it lc b ld mj ju lf lg mk jx li lj ml ll lm ln mm lp lq lr mn lt lu lv im bi translated">英语评论及其评级的示例如下:</p><pre class="lx ly lz ma gt mo mp mq mr aw ms bi"><span id="3f70" class="mt kj it mp b gy mu mv l mw mx">Review: 'I’ve had good luck with this product when I’ve bought it in stores, but this particular one I bought though Amazon was watered down! The texture was much more viscous than any of the tubes I bought in person and it lasted me about half the time. You’re better off finding it in a shop than wasting your money on watered down face wash.'<br/>Rating: 2</span></pre><p id="542b" class="pw-post-body-paragraph la lb it lc b ld mj ju lf lg mk jx li lj ml ll lm ln mm lp lq lr mn lt lu lv im bi translated">我们加载XML-Roberta模型，并用它来标记文本。文本的最大长度设置为512个标记。为了提高模型的准确性，我们将review_body、review_title和review_category连接起来。</p><p id="910c" class="pw-post-body-paragraph la lb it lc b ld mj ju lf lg mk jx li lj ml ll lm ln mm lp lq lr mn lt lu lv im bi translated">由于实际数据集相当大，我们在10%的训练样本上进行训练。数据集库中的<code class="fe nd ne nf mp b">shard</code>功能使得对一小部分数据进行采样进行实验变得很容易。完成此操作的代码如下:</p><pre class="lx ly lz ma gt mo mp mq mr aw ms bi"><span id="17e9" class="mt kj it mp b gy mu mv l mw mx">dataset = dataset.shuffle(seed=123)</span><span id="36e1" class="mt kj it mp b gy ng mv l mw mx">train_dataset = dataset["train"].shard(index=1, num_shards=10)</span><span id="295a" class="mt kj it mp b gy ng mv l mw mx">val_dataset = dataset['validation'].shard(index=1, num_shards=5)</span></pre><p id="de7d" class="pw-post-body-paragraph la lb it lc b ld mj ju lf lg mk jx li lj ml ll lm ln mm lp lq lr mn lt lu lv im bi translated">tokenizer，encode_plus选项用于标记与审查主体、审查标题和审查类别相关联的文本。实际评论的星级从1星到5星不等。我们将其转换为范围从0到4的目标标签。</p><h1 id="8e7f" class="ki kj it bd kk kl km kn ko kp kq kr ks jz kt ka ku kc kv kd kw kf kx kg ky kz bi translated">模特培训</h1><p id="14ab" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">为了简化训练过程，我们使用HuggingFace的训练器库。我们使用自动建模功能加载预训练的XLM-罗伯塔基础模型。</p><pre class="lx ly lz ma gt mo mp mq mr aw ms bi"><span id="5820" class="mt kj it mp b gy mu mv l mw mx">from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer</span><span id="7b3a" class="mt kj it mp b gy ng mv l mw mx">batch_size = 8</span><span id="95c4" class="mt kj it mp b gy ng mv l mw mx">num_labels = 5</span><span id="a9f0" class="mt kj it mp b gy ng mv l mw mx">model = AutoModelForSequenceClassification.from_pretrained('xlm-roberta-base', num_labels=num_labels)</span></pre><p id="f8a1" class="pw-post-body-paragraph la lb it lc b ld mj ju lf lg mk jx li lj ml ll lm ln mm lp lq lr mn lt lu lv im bi translated">为了实例化训练器，我们定义了两件事:</p><ol class=""><li id="98c3" class="nh ni it lc b ld mj lg mk lj nj ln nk lr nl lv nm nn no np bi translated">训练参数——这些参数包括输出目录的位置、学习速度、批量大小、次数等</li><li id="510b" class="nh ni it lc b ld nq lg nr lj ns ln nt lr nu lv nm nn no np bi translated">评估的计算指标功能—我们可以定义一个评估指标，在培训期间在val集上运行。</li></ol><p id="5b55" class="pw-post-body-paragraph la lb it lc b ld mj ju lf lg mk jx li lj ml ll lm ln mm lp lq lr mn lt lu lv im bi translated">数据集包还定义了许多自定义指标。我喜欢数据集库中的这一功能，因为它使得在数据集上计算bleu、rouge或自定义小队指标的范围变得非常容易。</p><p id="f0ed" class="pw-post-body-paragraph la lb it lc b ld mj ju lf lg mk jx li lj ml ll lm ln mm lp lq lr mn lt lu lv im bi translated">我们首先从数据集库中加载我们选择的指标</p><pre class="lx ly lz ma gt mo mp mq mr aw ms bi"><span id="e08e" class="mt kj it mp b gy mu mv l mw mx">from datasets import load_metric</span><span id="c322" class="mt kj it mp b gy ng mv l mw mx">metric = load_metric('accuracy')<br/>f1_metric = load_metric('f1')</span></pre><p id="8822" class="pw-post-body-paragraph la lb it lc b ld mj ju lf lg mk jx li lj ml ll lm ln mm lp lq lr mn lt lu lv im bi translated">metric.compute()函数可用于获取结果。</p><pre class="lx ly lz ma gt mo mp mq mr aw ms bi"><span id="621b" class="mt kj it mp b gy mu mv l mw mx">import numpy as np</span><span id="2ced" class="mt kj it mp b gy ng mv l mw mx">fake_preds = np.random.randint(1, 6, size=(64,))</span><span id="46e0" class="mt kj it mp b gy ng mv l mw mx">fake_labels = np.random.randint(1, 6, size=(64,))</span><span id="6809" class="mt kj it mp b gy ng mv l mw mx">metric.compute(predictions=fake_preds, references=fake_labels)</span><span id="13d0" class="mt kj it mp b gy ng mv l mw mx">f1_metric.compute(predictions=fake_preds, references=fake_labels, average='weighted')</span></pre><p id="1acf" class="pw-post-body-paragraph la lb it lc b ld mj ju lf lg mk jx li lj ml ll lm ln mm lp lq lr mn lt lu lv im bi translated">为了在训练期间运行它，我们从eval_pred获取预测和标签，然后运行metric.compute()。仅此而已！！</p><p id="9d0a" class="pw-post-body-paragraph la lb it lc b ld mj ju lf lg mk jx li lj ml ll lm ln mm lp lq lr mn lt lu lv im bi translated">为了开始训练，现在我们可以将所有这些参数传递给训练者。</p><pre class="lx ly lz ma gt mo mp mq mr aw ms bi"><span id="8391" class="mt kj it mp b gy mu mv l mw mx">trainer = Trainer(</span><span id="6f5a" class="mt kj it mp b gy ng mv l mw mx">model,</span><span id="0d56" class="mt kj it mp b gy ng mv l mw mx">args,</span><span id="c8cf" class="mt kj it mp b gy ng mv l mw mx">train_dataset= encoded_train_dataset,</span><span id="f426" class="mt kj it mp b gy ng mv l mw mx">eval_dataset=encoded_val_dataset,</span><span id="c00f" class="mt kj it mp b gy ng mv l mw mx">data_collator=SmartCollator(pad_token_id=tokenizer.pad_token_id),</span><span id="0a33" class="mt kj it mp b gy ng mv l mw mx">tokenizer=tokenizer,</span><span id="0b30" class="mt kj it mp b gy ng mv l mw mx">compute_metrics=compute_metrics</span><span id="0352" class="mt kj it mp b gy ng mv l mw mx">)</span></pre><p id="99e0" class="pw-post-body-paragraph la lb it lc b ld mj ju lf lg mk jx li lj ml ll lm ln mm lp lq lr mn lt lu lv im bi translated">该模型在Colab上的P100实例上训练大约需要2小时20分钟。这是针对训练数据集的10%样本的。</p><p id="fc53" class="pw-post-body-paragraph la lb it lc b ld mj ju lf lg mk jx li lj ml ll lm ln mm lp lq lr mn lt lu lv im bi translated">该模型达到了59.9%的准确率，比介绍该数据集的<a class="ae mi" href="https://arxiv.org/abs/2010.02573" rel="noopener ugc nofollow" target="_blank">论文</a>中报告的59.2%的准确率略高。对完整数据集的训练应该会进一步改善结果。</p><p id="e050" class="pw-post-body-paragraph la lb it lc b ld mj ju lf lg mk jx li lj ml ll lm ln mm lp lq lr mn lt lu lv im bi translated">我想谈谈其他一些可以用来提高训练速度/模型结果的很酷的东西</p><h2 id="a801" class="mt kj it bd kk nv nw dn ko nx ny dp ks lj nz oa ku ln ob oc kw lr od oe ky of bi translated">智能动态填充减少训练时间</h2><p id="c811" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">这里的灵感来自于的作品<a class="ae mi" href="https://github.com/ELS-RD/transformers-notebook/blob/master/Divide_Hugging_Face_Transformers_training_time_by_2_or_more.ipynb" rel="noopener ugc nofollow" target="_blank">。在训练期间，我们必须输入相同长度的样本。由于基础文本可以有不同的长度，典型的方法是将所有内容填充到最大长度。这通常在完整数据集级别完成，其中如果文本的最大长度是512个标记，则所有内容都被填充到该最大长度。然而，这是低效的。例如，一个有20个标记的句子也会被填充到512个标记。由于<em class="og">注意力在计算</em>中是二次的，这将显著减慢对这种样本的训练。</a></p><p id="e2d2" class="pw-post-body-paragraph la lb it lc b ld mj ju lf lg mk jx li lj ml ll lm ln mm lp lq lr mn lt lu lv im bi translated">这里可以做的一个优化是填充到<strong class="lc iu">批次</strong>中的最大长度，而不是数据集中的最大长度。我笔记本中的代码通过定义一个智能数据排序器来实现这一点。在数据排序器中，完成这一任务的主要模块是:</p><pre class="lx ly lz ma gt mo mp mq mr aw ms bi"><span id="9e17" class="mt kj it mp b gy mu mv l mw mx">max_size = max([len(ex['input_ids']) for ex in batch])</span><span id="03de" class="mt kj it mp b gy ng mv l mw mx">for item in batch:</span><span id="1bf9" class="mt kj it mp b gy ng mv l mw mx">batch_inputs += [pad_seq(item['input_ids'], max_size, self.pad_token_id)]</span></pre><p id="1402" class="pw-post-body-paragraph la lb it lc b ld mj ju lf lg mk jx li lj ml ll lm ln mm lp lq lr mn lt lu lv im bi translated">这种优化减少了一半的训练时间！这对一小段代码非常有用。</p><h2 id="6d48" class="mt kj it bd kk nv nw dn ko nx ny dp ks lj nz oa ku ln ob oc kw lr od oe ky of bi translated">使用Optuna的超参数优化</h2><figure class="lx ly lz ma gt mb gh gi paragraph-image"><div role="button" tabindex="0" class="mz na di nb bf nc"><div class="gh gi oh"><img src="../Images/7ffb8a8b80df89c875cc8f6f4c71e314.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7r7HSYkbn73NrmR_skvh5w.jpeg"/></div></div><p class="me mf gj gh gi mg mh bd b be z dk translated">Unsplash的免版税:<a class="ae mi" href="https://unsplash.com/photos/4Ennrbj1svk" rel="noopener ugc nofollow" target="_blank">https://unsplash.com/photos/4Ennrbj1svk</a></p></figure><p id="71a4" class="pw-post-body-paragraph la lb it lc b ld mj ju lf lg mk jx li lj ml ll lm ln mm lp lq lr mn lt lu lv im bi translated"><a class="ae mi" href="https://optuna.org/" rel="noopener ugc nofollow" target="_blank"> Optuna </a>是一个超参数搜索器，可以很容易地与教练类集成。它有效地搜索大空间的超参数，及早修剪没有希望的试验。</p><p id="3c37" class="pw-post-body-paragraph la lb it lc b ld mj ju lf lg mk jx li lj ml ll lm ln mm lp lq lr mn lt lu lv im bi translated">在我们的实验中，我们像以前一样定义训练参数和训练器，并让Optune在选定数量的试验中进行优化。</p><p id="b888" class="pw-post-body-paragraph la lb it lc b ld mj ju lf lg mk jx li lj ml ll lm ln mm lp lq lr mn lt lu lv im bi translated"><code class="fe nd ne nf mp b">trainer.hyperparameter_search(n_trials=15, direction=”maximize”)</code></p><h1 id="58b6" class="ki kj it bd kk kl km kn ko kp kq kr ks jz kt ka ku kc kv kd kw kf kx kg ky kz bi translated">结论</h1><p id="9cbe" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">这个博客展示了如何使用Transformer模型来处理多语言数据。我们使用HuggingFace库和它的许多特性来准备我们的数据，并用几行代码来训练它。经过训练的模型具有最先进的结果。代码是以非常模块化的格式编写的，可以用于任何类型的分类问题。我们还讨论了动态填充的功能，以显著加快训练速度，并研究超参数优化，以进一步改善结果。</p><p id="c43b" class="pw-post-body-paragraph la lb it lc b ld mj ju lf lg mk jx li lj ml ll lm ln mm lp lq lr mn lt lu lv im bi translated">我希望您尝试一下代码，并训练自己的模型。请在下面的评论中分享你的经历。</p><p id="7071" class="pw-post-body-paragraph la lb it lc b ld mj ju lf lg mk jx li lj ml ll lm ln mm lp lq lr mn lt lu lv im bi translated">在<a class="ae mi" href="https://deeplearninganalytics.org/" rel="noopener ugc nofollow" target="_blank">深度学习分析</a>，我们非常热衷于使用机器学习来解决现实世界的问题。我们已经帮助许多企业部署了创新的基于人工智能的解决方案。如果您看到合作的机会，请通过我们的网站<a class="ae mi" href="https://deeplearninganalytics.org/contact-us/" rel="noopener ugc nofollow" target="_blank">这里</a>联系我们。</p><h1 id="dfb9" class="ki kj it bd kk kl km kn ko kp kq kr ks jz kt ka ku kc kv kd kw kf kx kg ky kz bi translated">参考</h1><ul class=""><li id="95e6" class="nh ni it lc b ld le lg lh lj oi ln oj lr ok lv ol nn no np bi translated"><a class="ae mi" href="https://arxiv.org/pdf/1911.02116.pdf" rel="noopener ugc nofollow" target="_blank"> XML-Roberta论文</a></li><li id="2135" class="nh ni it lc b ld nq lg nr lj ns ln nt lr nu lv ol nn no np bi translated"><a class="ae mi" href="https://github.com/huggingface/notebooks/blob/master/examples/text_classification.ipynb" rel="noopener ugc nofollow" target="_blank">拥抱脸分类笔记本</a></li><li id="7943" class="nh ni it lc b ld nq lg nr lj ns ln nt lr nu lv ol nn no np bi translated"><a class="ae mi" href="https://huggingface.co/" rel="noopener ugc nofollow" target="_blank">拥抱脸</a></li><li id="fcc0" class="nh ni it lc b ld nq lg nr lj ns ln nt lr nu lv ol nn no np bi translated"><a class="ae mi" href="https://optuna.org/" rel="noopener ugc nofollow" target="_blank"> Optuna </a></li></ul></div></div>    
</body>
</html>