<html>
<head>
<title>How to Learn Long-Term Trends with LSTM</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如何学习LSTM的长期趋势</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/how-to-learn-long-term-trends-with-lstm-c992d32d73be?source=collection_archive---------10-----------------------#2021-08-11">https://towardsdatascience.com/how-to-learn-long-term-trends-with-lstm-c992d32d73be?source=collection_archive---------10-----------------------#2021-08-11</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="6c5d" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">Intel和Johns Hopkins开发的一种提高LSTM性能的方法</h2></div><p id="3bfb" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">长短期记忆(LSTM)网络广泛用于语音识别、语言翻译和时间序列预测。然而，LSTMs很难了解长期趋势。</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi lb"><img src="../Images/8d05c158603968bcb235e6483a0330e5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zeOG90tmye654fQjLslkQA.png"/></div></div><p class="ln lo gj gh gi lp lq bd b be z dk translated">图1:普通递归神经网络(RNN)的示意图。图片作者。</p></figure><p id="7770" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">2021年，英特尔和约翰霍普金斯大学的研究人员合作开发了一种LSTM细胞，提高了准确性和速度。这个方法非常新，所以不幸的是没有任何库/包。但是，它可以使用任何基本的深度学习包手动构建。</p><p id="8561" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">让我们开始吧…</p><h1 id="5859" class="lr ls iq bd lt lu lv lw lx ly lz ma mb jw mc jx md jz me ka mf kc mg kd mh mi bi translated">技术TLDR</h1><ul class=""><li id="f76a" class="mj mk iq kh b ki ml kl mm ko mn ks mo kw mp la mq mr ms mt bi translated">信息以幂律速度衰减。换句话说，随着我们沿着LSTM细胞链越走越远，我们会以幂律速率丢失先验信息。然而，LSTM忘记盖茨以指数速度删除信息，这阻止了学习长期趋势。</li><li id="4370" class="mj mk iq kh b ki mu kl mv ko mw ks mx kw my la mq mr ms mt bi translated"><strong class="kh ir">考虑到信息的幂律衰变，我们创造了一个新奇的遗忘门。</strong>该遗忘门使用sigmoid复位功能，如有必要，清除先前信息，仅清除当前时间步长的参考信息。然而，当复位未被激活时，与传统的遗忘门相比，先前的信息被保留更长时间，从而允许信息的幂律衰减。</li></ul><h1 id="fbe0" class="lr ls iq bd lt lu lv lw lx ly lz ma mb jw mc jx md jz me ka mf kc mg kd mh mi bi translated">但是，到底是怎么回事呢？</h1><p id="d9fc" class="pw-post-body-paragraph kf kg iq kh b ki ml jr kk kl mm ju kn ko mz kq kr ks na ku kv kw nb ky kz la ij bi translated">这将是一篇非常技术性的文章，但是你不需要太多的先验知识来理解核心概念。这东西很酷，所以让我们慢下来，了解到底发生了什么…</p><h2 id="8964" class="nc ls iq bd lt nd ne dn lx nf ng dp mb ko nh ni md ks nj nk mf kw nl nm mh nn bi translated">什么是rnn和LSTMs</h2><p id="4b69" class="pw-post-body-paragraph kf kg iq kh b ki ml jr kk kl mm ju kn ko mz kq kr ks na ku kv kw nb ky kz la ij bi translated">从头开始，让我们定义我们的神经网络框架。处理顺序数据(NLP、时间序列等)时。)，通常的做法是使用递归神经网络(RNN)。rnn只是一系列神经网络，其中一个的输出是下一个的输入。</p><p id="8549" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">然而，rnn并不完美。训练时，存在不稳定梯度的可能性。这就是长短期记忆(LSTM)细胞的来源。</p><p id="da59" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">lstm是RNNs中的一个专门单元，使用gates来管理输入和输出信息。</strong></p><p id="dc29" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">门是<em class="no">激活功能</em>，取值在<em class="no"> 0 </em>和<em class="no"> 1 </em>之间。如果一个门取一个小值，比如说<em class="no"> 0.001 </em>，我们将其乘以输入信息，然后大幅缩小。相反，如果门取一个大值，比如说<em class="no"> 0.998 </em>，我们将它乘以输入的信息，并让它基本保持不变。</p><p id="1e5a" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">但是这个不稳定梯度问题是什么，LSTMs是怎么解决的？</p><h2 id="4949" class="nc ls iq bd lt nd ne dn lx nf ng dp mb ko nh ni md ks nj nk mf kw nl nm mh nn bi translated">我们为什么需要LSTMs</h2><p id="245d" class="pw-post-body-paragraph kf kg iq kh b ki ml jr kk kl mm ju kn ko mz kq kr ks na ku kv kw nb ky kz la ij bi translated">嗯，和任何神经网络一样，rnn是通过按照所有先前节点导数的乘积成比例地更新节点权重来训练的。<strong class="kh ir"> </strong>所以，如果网络中其他节点的导数接近于零，我们的权重更新将会非常非常小。这种现象被称为<strong class="kh ir">消失梯度问题</strong>，并阻止我们的模型学习。</p><p id="44dc" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">同样，如果我们网络中的梯度非常大，我们的权重更新也会非常大。这被称为<strong class="kh ir">爆炸梯度问题。</strong></p><p id="baf4" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在，基本的神经网络通常不会遇到这个问题——如果你只有两三层，就没有多少小/大导数的复合。但是，如果RNN有许多神经网络依次堆叠在一起，会发生什么呢？</p><p id="b027" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">正如你可能已经猜到的，<strong class="kh ir">rnn有更多的层，因此细胞衍生物有更多的机会复合。</strong></p><p id="d76d" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这就是LSTM细胞的用武之地——它们有调节信息流的闸门，因此不稳定的梯度不太常见。</p><h2 id="fdc5" class="nc ls iq bd lt nd ne dn lx nf ng dp mb ko nh ni md ks nj nk mf kw nl nm mh nn bi translated">LSTM细胞结构</h2><p id="f0cc" class="pw-post-body-paragraph kf kg iq kh b ki ml jr kk kl mm ju kn ko mz kq kr ks na ku kv kw nb ky kz la ij bi translated">现在我们有了背景，让我们来理解LSTM细胞的解剖结构…</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi np"><img src="../Images/c38539e3edeb2d89cf7868d41553091a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vDa2BDHQWBlLZ4fGCesfmQ.png"/></div></div><p class="ln lo gj gh gi lp lq bd b be z dk translated">图2:LSTM电池的组件— <a class="ae nq" href="https://www.researchgate.net/figure/Structure-of-the-LSTM-cell-and-equations-that-describe-the-gates-of-an-LSTM-cell_fig5_329362532" rel="noopener ugc nofollow" target="_blank"> src </a>。图片作者。</p></figure><p id="afe6" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">乍一看，发生了很多事情。但是真的没那么差。</p><p id="e2f4" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在图2中，带有<em class="no"> σ </em> (sigma)的青色方框是门。它们是神经网络，决定有多少信息应该来自先前的细胞和我们的预测者。请注意，它们的编号为1-3。</p><p id="a455" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在决定保留多少信息后，gates将这些信息传递给我们的<em class="no"> tanh </em>。tanh函数负责更新节点的权重。</p><p id="8cca" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">最后，我们输出两组信息:<strong class="kh ir">单元格状态和隐藏状态。单元状态充当长期记忆，隐藏状态是短期记忆。</strong></p><p id="59a8" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">快速回顾一下这些门，让我们参考下面的数字并定义它们…</p><ol class=""><li id="cc21" class="mj mk iq kh b ki kj kl km ko nr ks ns kw nt la nu mr ms mt bi translated"><strong class="kh ir">遗忘门:</strong>确定我们应该遗忘多少长期信息。被保存的信息被输入到我们的细胞状态中。</li><li id="ea12" class="mj mk iq kh b ki mu kl mv ko mw ks mx kw my la nu mr ms mt bi translated"><strong class="kh ir">输入门:</strong>决定我们应该忘记多少短期信息。保存的信息被输入到我们的<em class="no"> tanh </em>函数中进行培训。</li><li id="9f68" class="mj mk iq kh b ki mu kl mv ko mw ks mx kw my la nu mr ms mt bi translated"><strong class="kh ir">输出门:</strong>利用长期和短期信息，确定在我们的隐藏状态下应该输出多少信息。</li></ol><p id="23c2" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">通过使用这些门来调节信息流，我们能够避免计算有问题的梯度。</p><p id="7782" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">很好，所以我们不再有消失梯度的问题了。但是，遗憾的是，LSTMs也不是一个完美的解决方案。让我们进入英特尔和约翰霍普金斯大学的研究人员讨论的概念。</p><h1 id="19b5" class="lr ls iq bd lt lu lv lw lx ly lz ma mb jw mc jx md jz me ka mf kc mg kd mh mi bi translated">问题是:信息衰减</h1><p id="2b45" class="pw-post-body-paragraph kf kg iq kh b ki ml jr kk kl mm ju kn ko mz kq kr ks na ku kv kw nb ky kz la ij bi translated">在很多情况下，我们希望从数据中了解长期趋势。例如，如果我们在看我们最喜欢的股票价格的每日时间序列，去年的价格可以代表今天的价格。但是,“遗忘之门”倾向于删除之前许多步骤中的信息——如果它在过去的100天里没有用，我们为什么要保留它呢？</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi nv"><img src="../Images/b34c850a5d7a31bd1a41f16162492b97.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lYzrlIqlLWUAr3Hm1yqbzQ.png"/></div></div><p class="ln lo gj gh gi lp lq bd b be z dk translated">图3:细胞状态的指数(红色)与幂律(蓝色)信息衰减。y轴是保留信息的比率。x轴是时间步长— <a class="ae nq" href="https://arxiv.org/pdf/2105.05944.pdf#page=9&amp;zoom=100,0,0" rel="noopener ugc nofollow" target="_blank"> src </a>。图片作者。</p></figure><p id="085a" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">事实证明，LSTM细胞中的信息以指数速度衰减，如图3中的红线所示。因此，在这个模拟中，传统的LSTM单元将会删除之前150个时间步长的几乎所有信息。这无疑给了解年度趋势带来了问题。</p><p id="e34d" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">那么，我们如何保存信息以便长期学习呢？</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div class="gh gi nw"><img src="../Images/d064a59db06f97964b4a88cc6479d06d.png" data-original-src="https://miro.medium.com/v2/resize:fit:604/format:webp/1*NcvaeGwnsqgg_SAzs8exmQ.png"/></div></figure><p id="2064" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">先前的研究表明NLP信息以幂律速率衰减，而不是指数衰减。通过判断图3中的曲线，我们可以看到幂律曲线在保留的信息方面有一个不太陡峭的下降。</p><p id="606c" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">很好，那么让我们弄清楚如何让LSTM细胞以幂律速度而不是指数速度忘记信息。</p><h1 id="45d5" class="lr ls iq bd lt lu lv lw lx ly lz ma mb jw mc jx md jz me ka mf kc mg kd mh mi bi translated">解决方案:新的遗忘之门</h1><p id="d12c" class="pw-post-body-paragraph kf kg iq kh b ki ml jr kk kl mm ju kn ko mz kq kr ks na ku kv kw nb ky kz la ij bi translated">研究人员的贡献是一种符合幂律标准的新型遗忘门。</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi nx"><img src="../Images/c8fd82ef29514666b3e74117e8ee6f57.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*T5gaTDYCu857JoaSsM5wzA.png"/></div></div><p class="ln lo gj gh gi lp lq bd b be z dk translated">图4:新的遗忘之门在LSTM细胞信息流中的位置— <a class="ae nq" href="https://www.researchgate.net/figure/Structure-of-the-LSTM-cell-and-equations-that-describe-the-gates-of-an-LSTM-cell_fig5_329362532" rel="noopener ugc nofollow" target="_blank"> src </a>。图片作者。</p></figure><p id="cd0b" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如图4所示，我们用一个新的遗忘门取代了sigmoid遗忘门，它可以更好地调节长期记忆中存储的信息。</p><p id="288c" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">免责声明，这可能会有点复杂，但我们会尽可能保持高水平。</p><h2 id="def2" class="nc ls iq bd lt nd ne dn lx nf ng dp mb ko nh ni md ks nj nk mf kw nl nm mh nn bi translated">细胞状态</h2><p id="6c45" class="pw-post-body-paragraph kf kg iq kh b ki ml jr kk kl mm ju kn ko mz kq kr ks na ku kv kw nb ky kz la ij bi translated">让我们从定义我们的细胞状态开始，也就是我们的长期记忆。</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi ny"><img src="../Images/17ae101c7bf4d6a4e096d54a44f1b2c1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MfHiio4IIgHZ4twGCZQOvA.png"/></div></div><p class="ln lo gj gh gi lp lq bd b be z dk translated">图5:时间步长t的单元状态的定义。</p></figure><p id="22ef" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在图5中，组件1(我们的遗忘门)和组件2(我们的先前细胞状态)决定了保留多少长期信息<strong class="kh ir"/>。组件3(我们的输入门)和组件4(我们的中间细胞状态)决定了保留多少短期信息。</p><p id="68ec" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如果你好奇的话，中间带点的奇怪圆圈是哈达玛积。它仅仅意味着我们将两个矩阵中每个索引的值相乘。</p><h2 id="29a3" class="nc ls iq bd lt nd ne dn lx nf ng dp mb ko nh ni md ks nj nk mf kw nl nm mh nn bi translated">新的遗忘之门</h2><p id="1bce" class="pw-post-body-paragraph kf kg iq kh b ki ml jr kk kl mm ju kn ko mz kq kr ks na ku kv kw nb ky kz la ij bi translated">现在，我们来定义一下研究者提出的小说《遗忘之门》。</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi nz"><img src="../Images/1d96f2f98230eb22c23f9d4b0113128b.png" data-original-src="https://miro.medium.com/v2/resize:fit:884/format:webp/1*uB3StWDlDq2EMd-VlFcigg.png"/></div></div><p class="ln lo gj gh gi lp lq bd b be z dk translated">图6:幂律遗忘门的定义。图片作者。</p></figure><p id="a363" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在图6中，我们有一个新的遗忘门，它控制着我们在长期记忆中回顾多远。<em class="no"> t </em>对应于当前时间步长的索引，<em class="no"> k_t </em>对应于感兴趣的第一个时间戳的索引，<em class="no"> p </em>是控制我们的信息衰减速率的可学习参数。</p><p id="2d1c" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">请记住，我们的遗忘之门的目的是调节长期信息的流入量。它通过将该信息乘以0到1之间的值来实现。所以，如果<em class="no"> f_t </em>接近1，我们保留大部分信息。反之，如果<em class="no"> f_t </em>接近于0，我们就丢失了大部分信息。</p><p id="45df" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">更具体地说，<strong class="kh ir">当回溯到很远的时候，我们的遗忘门取值接近1。这意味着我们保留了大量的长期信息。然而，如果我们着眼于短期，我们的遗忘之门会更近，我们只保留部分信息。</strong></p><h2 id="a7f4" class="nc ls iq bd lt nd ne dn lx nf ng dp mb ko nh ni md ks nj nk mf kw nl nm mh nn bi translated">重置门</h2><p id="4b26" class="pw-post-body-paragraph kf kg iq kh b ki ml jr kk kl mm ju kn ko mz kq kr ks na ku kv kw nb ky kz la ij bi translated">最后，您可以将<em class="no"> k_t </em>视为一个黑盒，它输出第一个包含有用信息的索引，然后就此打住。但是，为了完整起见，让我们看一看。</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div class="gh gi oa"><img src="../Images/2ead86aa4ab0fbae22bd20266e3b3c8f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1120/format:webp/1*CSAe6SR2aSMKJs9jAkbwrQ.png"/></div><p class="ln lo gj gh gi lp lq bd b be z dk translated">图7:幂律遗忘门复位元件的定义。图片作者。</p></figure><p id="8608" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在图7中，我们看到一个新变量:<em class="no"> r_t </em>(重置门)。复位门是一个sigmoid激活函数，意味着它取0和1之间的值。如果其值为1，则<em class="no"> k_t-1 </em>被取消并删除所有先前信息。如果其值为0，<em class="no"> t </em>取消，我们保留所有历史信息。</p><h1 id="eb1c" class="lr ls iq bd lt lu lv lw lx ly lz ma mb jw mc jx md jz me ka mf kc mg kd mh mi bi translated">摘要</h1><p id="e5fe" class="pw-post-body-paragraph kf kg iq kh b ki ml jr kk kl mm ju kn ko mz kq kr ks na ku kv kw nb ky kz la ij bi translated">哇哦。我们刚刚涵盖了大量的信息。</p><p id="db98" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如果一切都不清楚，在这些方程中输入数字和/或阅读论文(链接在评论中)可能会有帮助。话虽如此，要想深入理解，这里有三个要点:</p><ul class=""><li id="bad0" class="mj mk iq kh b ki kj kl km ko nr ks ns kw nt la mq mr ms mt bi translated">传统的LSTM细胞会很快丢弃有用的信息。</li><li id="0e56" class="mj mk iq kh b ki mu kl mv ko mw ks mx kw my la mq mr ms mt bi translated">在这篇文章中，我们概述了一个对长期信息更明智的遗忘门——它在长期学习任务中胜过传统的LSTM细胞。</li><li id="dca0" class="mj mk iq kh b ki mu kl mv ko mw ks mx kw my la mq mr ms mt bi translated">概述的遗忘门引用找到有用信息的最早索引。利用这个指数，它可以定制应该将多少长期记忆传递到我们的模型中。</li></ul></div><div class="ab cl ob oc hu od" role="separator"><span class="oe bw bk of og oh"/><span class="oe bw bk of og oh"/><span class="oe bw bk of og"/></div><div class="ij ik il im in"><p id="c7a1" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><em class="no">感谢阅读！我将再写40篇文章，将“学术”研究引入DS行业。查看我的链接/教程的意见，建立LSTM的权力法。</em></p></div></div>    
</body>
</html>