<html>
<head>
<title>Hyperparameter Optimization For Beginners</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">初学者的超参数优化</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/hyperparameter-optimization-for-beginners-32e3ab07b09c?source=collection_archive---------19-----------------------#2021-07-10">https://towardsdatascience.com/hyperparameter-optimization-for-beginners-32e3ab07b09c?source=collection_archive---------19-----------------------#2021-07-10</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="1504" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">科学家讨厌的任务数据</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/abff86449156100f9c00ad2425509a64.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*abbwQ7Vb1wk_wAtg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">照片由<a class="ae ky" href="https://unsplash.com/@drewpatrickmiller?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">德鲁·帕特里克·米勒</a>在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><p id="5096" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">许多数据科学家忽略了超参数。超参数调整是一项高度实验性的活动，这种不确定性会导致任何正常人的严重不适，这是我们自然试图避免的。</p><blockquote class="lv"><p id="ea04" class="lw lx it bd ly lz ma mb mc md me lu dk translated">“超参数调整更多地依赖于实验结果，而不是理论，因此确定最佳设置的最佳方法是尝试许多不同的组合[…]。”— Will Koehrsen，超参数调整Python中的随机森林</p></blockquote><p id="0acd" class="pw-post-body-paragraph kz la it lb b lc mf ju le lf mg jx lh li mh lk ll lm mi lo lp lq mj ls lt lu im bi translated">不幸的是，应该这样做。我们不会走进商店，从货架上挑选一双运动鞋，然后买下来。我们首先选择一双我们认为能解决我们问题的鞋，不管是衣柜故障还是其他什么原因，我们已经失去了所有的运动鞋。接下来，我们在购买之前调整超参数，如鞋子的尺寸和我们想要的颜色。</p><p id="1bcb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果我们愿意在现实世界中这样做，那么在数据科学中就不应该跳过它。</p><h2 id="047d" class="mk ml it bd mm mn mo dn mp mq mr dp ms li mt mu mv lm mw mx my lq mz na nb nc bi translated">理解超参数</h2><p id="4b31" class="pw-post-body-paragraph kz la it lb b lc nd ju le lf ne jx lh li nf lk ll lm ng lo lp lq nh ls lt lu im bi translated">超参数优化是为学习算法选择最优超参数集的问题。通过确定超参数的正确组合，模型的性能得到了最大化——这意味着当提供看不见的实例时，我们的学习算法会做出更好的决策。</p><p id="0b94" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">被选为超参数的值控制学习过程，因此，它们不同于正常参数，因为它们是在训练学习算法之前被选择的。</p><p id="f9f7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在形式上，模型超参数是在提供数据时不能由模型估计的参数，因此需要预先设置它们来估计模型的参数。相反，模型参数由学习模型根据提供的数据来估计。</p><h2 id="3358" class="mk ml it bd mm mn mo dn mp mq mr dp ms li mt mu mv lm mw mx my lq mz na nb nc bi translated">方法</h2><p id="df86" class="pw-post-body-paragraph kz la it lb b lc nd ju le lf ne jx lh li nf lk ll lm ng lo lp lq nh ls lt lu im bi translated">有许多方法可以有效地执行超参数优化——参见维基百科上的<a class="ae ky" href="https://en.wikipedia.org/wiki/Hyperparameter_optimization" rel="noopener ugc nofollow" target="_blank">超参数优化</a>获得完整的分类。Mind Foundry在Twitter上进行了一项调查，以了解平台上从业者的情绪。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ni nj l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">Mind Foundry在推特<a class="ae ky" href="https://twitter.com/MindFoundry/status/1042446256554033152?ref_src=twsrc%5Etfw%7Ctwcamp%5Etweetembed%7Ctwterm%5E1042446256554033152%7Ctwgr%5E%7Ctwcon%5Es1_&amp;ref_url=https%3A%2F%2Fcdn.embedly.com%2Fwidgets%2Fmedia.html%3Ftype%3Dtext2Fhtmlkey%3Da19fcc184b9711e1b4764040d3dc5c07schema%3Dtwitterurl%3Dhttps3A%2F%2Ftwitter.com%2Fmindfoundry%2Fstatus%2F1042446256554033152image%3Dhttps3A%2F%2Fi.embed.ly%2F1%2Fimage3Furl3Dhttps253A252F252Fpbs.twimg.com252Fprofile_images252F720909614011838464252FoYcW2zb0_400x400.jpg26key3Da19fcc184b9711e1b4764040d3dc5c07" rel="noopener ugc nofollow" target="_blank">上进行的调查</a></p></figure><p id="3a3c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们进一步了解它们，以及如何在Python中执行它们。</p><h2 id="36a0" class="mk ml it bd mm mn mo dn mp mq mr dp ms li mt mu mv lm mw mx my lq mz na nb nc bi translated">贝叶斯优化</h2><p id="c449" class="pw-post-body-paragraph kz la it lb b lc nd ju le lf ne jx lh li nf lk ll lm ng lo lp lq nh ls lt lu im bi translated">维基百科将贝叶斯优化描述为“<em class="nk">，一种针对嘈杂的黑盒函数的全局优化方法。应用于超参数优化，贝叶斯优化建立了从超参数值到在验证集上评估的目标的函数映射的概率模型。通过基于当前模型迭代地评估有希望的超参数配置，然后更新它，贝叶斯优化旨在收集尽可能多地揭示关于该函数的信息的观察值，特别是最优值的位置。它试图平衡勘探(结果最不确定的超参数)和开发(预期接近最优的超参数)。在实践中，与网格搜索和随机搜索相比，贝叶斯优化已被证明可以在更少的评估中获得更好的结果，因为它能够在实验运行之前对实验的质量进行推理。</em><strong class="lb iu">来源</strong> : <a class="ae ky" href="https://en.wikipedia.org/wiki/Bayesian_optimization" rel="noopener ugc nofollow" target="_blank">维基</a>。</p><p id="9eb8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">Scikit-Optimize (skopt)是一个优化库，它有一个<a class="ae ky" href="https://scikit-optimize.github.io/stable/auto_examples/sklearn-gridsearchcv-replacement.html" rel="noopener ugc nofollow" target="_blank">贝叶斯优化实现</a>。我建议使用这种实现，而不是尝试实现自己的解决方案——尽管如果您想更深入地了解贝叶斯优化是如何工作的，实现自己的解决方案也是有价值的。有关示例，请参见下面的代码。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nl nj l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">使用Python的贝叶斯优化示例</p></figure><h2 id="166e" class="mk ml it bd mm mn mo dn mp mq mr dp ms li mt mu mv lm mw mx my lq mz na nb nc bi translated">网格搜索</h2><p id="1d35" class="pw-post-body-paragraph kz la it lb b lc nd ju le lf ne jx lh li nf lk ll lm ng lo lp lq nh ls lt lu im bi translated">网格搜索是我学习的第一个执行超参数优化的技术。它包括在学习算法中彻底搜索超参数空间的特定值的手动子集。执行网格搜索意味着必须有一个性能指标来指导我们的算法。</p><p id="592a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">强烈建议您使用<a class="ae ky" href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html" rel="noopener ugc nofollow" target="_blank"> Sklearn实现</a>，而不是从头开始实现网格搜索。有关示例，请参见下面的代码。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nl nj l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">使用Python进行网格搜索的示例</p></figure><h2 id="19d3" class="mk ml it bd mm mn mo dn mp mq mr dp ms li mt mu mv lm mw mx my lq mz na nb nc bi translated">随机搜索</h2><p id="e9bd" class="pw-post-body-paragraph kz la it lb b lc nd ju le lf ne jx lh li nf lk ll lm ng lo lp lq nh ls lt lu im bi translated">随机搜索随机选择组合，而不是在网格搜索中穷举所有列出的组合。当少量的超参数对最终的模型性能有影响时，随机搜索可以胜过网格搜索——尽管它在上面的调查中评级很低，但随机搜索仍然是您的工具包中非常重要的技术。</p><p id="b8ff" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">像网格搜索一样，随机搜索也有一个<a class="ae ky" href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html" rel="noopener ugc nofollow" target="_blank"> Scikit Learn实现</a>，比你自己的解决方案更好用。参见下面的代码。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nl nj l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">使用Python的随机搜索示例</p></figure><h2 id="a208" class="mk ml it bd mm mn mo dn mp mq mr dp ms li mt mu mv lm mw mx my lq mz na nb nc bi translated">最后的想法</h2><p id="ee69" class="pw-post-body-paragraph kz la it lb b lc nd ju le lf ne jx lh li nf lk ll lm ng lo lp lq nh ls lt lu im bi translated">根据我的经验，在执行超参数优化时，很好地理解您正在使用的学习算法以及超参数如何影响它的行为会有所帮助。虽然这是最重要的任务之一，但我觉得超参数调优没有得到应有的重视，或者可能是我看得不够多。然而，这是你的项目中极其重要的一部分，不应该被忽视。</p><p id="62a1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">感谢阅读！</p><p id="e5e9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果你喜欢这篇文章，请通过订阅我的<strong class="lb iu">免费</strong> <a class="ae ky" href="https://mailchi.mp/ef1f7700a873/sign-up" rel="noopener ugc nofollow" target="_blank">每周简讯</a>与我联系。不要错过我写的关于人工智能、数据科学和自由职业的帖子。</p><h2 id="f99b" class="mk ml it bd mm mn mo dn mp mq mr dp ms li mt mu mv lm mw mx my lq mz na nb nc bi translated">相关文章</h2><div class="nm nn gp gr no np"><a rel="noopener follow" target="_blank" href="/cross-validation-c4fae714f1c5"><div class="nq ab fo"><div class="nr ab ns cl cj nt"><h2 class="bd iu gy z fp nu fr fs nv fu fw is bi translated">交叉验证</h2><div class="nw l"><h3 class="bd b gy z fp nu fr fs nv fu fw dk translated">验证机器学习模型的性能</h3></div><div class="nx l"><p class="bd b dl z fp nu fr fs nv fu fw dk translated">towardsdatascience.com</p></div></div><div class="ny l"><div class="nz l oa ob oc ny od ks np"/></div></div></a></div><div class="nm nn gp gr no np"><a rel="noopener follow" target="_blank" href="/oversampling-and-undersampling-5e2bbaf56dcf"><div class="nq ab fo"><div class="nr ab ns cl cj nt"><h2 class="bd iu gy z fp nu fr fs nv fu fw is bi translated">过采样和欠采样</h2><div class="nw l"><h3 class="bd b gy z fp nu fr fs nv fu fw dk translated">一种不平衡分类技术</h3></div><div class="nx l"><p class="bd b dl z fp nu fr fs nv fu fw dk translated">towardsdatascience.com</p></div></div><div class="ny l"><div class="oe l oa ob oc ny od ks np"/></div></div></a></div><div class="nm nn gp gr no np"><a rel="noopener follow" target="_blank" href="/gradient-descent-811efcc9f1d5"><div class="nq ab fo"><div class="nr ab ns cl cj nt"><h2 class="bd iu gy z fp nu fr fs nv fu fw is bi translated">梯度下降</h2><div class="nw l"><h3 class="bd b gy z fp nu fr fs nv fu fw dk translated">机器学习中梯度下降的不同变体的利弊</h3></div><div class="nx l"><p class="bd b dl z fp nu fr fs nv fu fw dk translated">towardsdatascience.com</p></div></div><div class="ny l"><div class="of l oa ob oc ny od ks np"/></div></div></a></div></div></div>    
</body>
</html>