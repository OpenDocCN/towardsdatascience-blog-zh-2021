<html>
<head>
<title>Revealing what neural networks see and learn: PytorchRevelio</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">揭示神经网络看到和学到什么:PytorchRevelio</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/reveling-what-neural-networks-see-and-learn-pytorchrevelio-a218ef5fc61f?source=collection_archive---------20-----------------------#2021-05-23">https://towardsdatascience.com/reveling-what-neural-networks-see-and-learn-pytorchrevelio-a218ef5fc61f?source=collection_archive---------20-----------------------#2021-05-23</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/></div><div class="ab cl jn jo hu jp" role="separator"><span class="jq bw bk jr js jt"/><span class="jq bw bk jr js jt"/><span class="jq bw bk jr js"/></div><div class="ij ik il im in"><figure class="jv jw jx jy gt jz gh gi paragraph-image"><div role="button" tabindex="0" class="ka kb di kc bf kd"><div class="gh gi ju"><img src="../Images/3c334ed230e8d20552a84e637bccb3f2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*uD-8Rbu3tHyCotSE.jpg"/></div></div><p class="kg kh gj gh gi ki kj bd b be z dk translated">图片来自[0]</p></figure><h1 id="baa4" class="kk kl iq bd km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh bi translated">介绍</h1><p id="c241" class="pw-post-body-paragraph li lj iq lk b ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf ij bi translated">线性和经典的学习模型，如逻辑回归[1]，SVM[2]，决策树，…很容易理解和分析。另一方面，深度神经网络很难理解，所以人们将其称为黑盒。幸运的是，已经发明了很多方法，让我们在某种程度上了解这些网络。在这篇文章中，我们将介绍其中的一些方法，同时我们也推荐<a class="ae mg" href="https://github.com/farhad-dalirani/PytorchRevelio" rel="noopener ugc nofollow" target="_blank"> PytorchRevelio </a> [0】，一个包含这些方法的工具包。</p></div><div class="ab cl jn jo hu jp" role="separator"><span class="jq bw bk jr js jt"/><span class="jq bw bk jr js jt"/><span class="jq bw bk jr js"/></div><div class="ij ik il im in"><h1 id="834c" class="kk kl iq bd km kn mh kp kq kr mi kt ku kv mj kx ky kz mk lb lc ld ml lf lg lh bi translated">为什么理解深度神经网络看到的东西很重要？</h1><p id="69b4" class="pw-post-body-paragraph li lj iq lk b ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf ij bi translated">深度神经网络，特别是卷积神经网络，在大数据集和新的训练方法的帮助下，避免了梯度消失等问题，达到了高性能。因此，它们取代了许多计算机视觉任务中的旧方法，如分类、检测、语义分割、跟踪、场景重建等。</p><p id="57ac" class="pw-post-body-paragraph li lj iq lk b ll mm ln lo lp mn lr ls lt mo lv lw lx mp lz ma mb mq md me mf ij bi translated">Grad-CAM[3]论文为理解神经网络所见的重要性提供了以下理由:</p><blockquote class="mr ms mt"><p id="30d5" class="li lj mu lk b ll mm ln lo lp mn lr ls mv mo lv lw mw mp lz ma mx mq md me mf ij bi translated">可解释性很重要。为了建立对智能<br/>系统的信任，并推动它们有意义地融入我们的日常生活，很明显，我们必须建立“透明的”<br/>模型，这些模型能够解释为什么它们预测它们所预测的<br/>。</p></blockquote><p id="7ab2" class="pw-post-body-paragraph li lj iq lk b ll mm ln lo lp mn lr ls lt mo lv lw lx mp lz ma mb mq md me mf ij bi translated">通过理解为什么模型对输入的预测是错误的，我们可以想出解决模型问题的方法。</p><p id="f21e" class="pw-post-body-paragraph li lj iq lk b ll mm ln lo lp mn lr ls lt mo lv lw lx mp lz ma mb mq md me mf ij bi translated">当一个模型在给定的任务中表现得比人更好时，我们可以通过理解它为什么做出那个决定来学习新的东西。</p><figure class="jv jw jx jy gt jz gh gi paragraph-image"><div role="button" tabindex="0" class="ka kb di kc bf kd"><div class="gh gi my"><img src="../Images/b73e831f2ac8dd5c7cf46a102194185a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jGTT6HVXiEzFcy_zDxMNcA.png"/></div></div><p class="kg kh gj gh gi ki kj bd b be z dk translated">图片来自[3]。</p></figure><p id="2342" class="pw-post-body-paragraph li lj iq lk b ll mm ln lo lp mn lr ls lt mo lv lw lx mp lz ma mb mq md me mf ij bi translated">上面的图像的第一行包含四个图像给VGG-16在ImageNet上训练[4]。这些图像被分类到错误的类别中。在第三行中，在理解神经网络的方法的帮助下，描绘了激发错误类别变得活跃的像素。例如，第四个图像是一个线圈。然而，它被归类为藤蔓蛇。原因是第三行显示的绿色曲线。</p><figure class="jv jw jx jy gt jz gh gi paragraph-image"><div role="button" tabindex="0" class="ka kb di kc bf kd"><div class="gh gi mz"><img src="../Images/5587523e174228492cb3cb42c49348d5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OpjYSVVGwaP3-QJa8FnlIA.jpeg"/></div></div><p class="kg kh gj gh gi ki kj bd b be z dk translated">图片来自[0]</p></figure><p id="863f" class="pw-post-body-paragraph li lj iq lk b ll mm ln lo lp mn lr ls lt mo lv lw lx mp lz ma mb mq md me mf ij bi translated">PytorchRevelio工具包通过使用两种不同的方法来获得上面的图像。这些图像是在ImageNet上训练的VGG11[4]的分类标签691-氧气面罩的表示。在这些图像中，不仅氧气面罩的图案可见，而且它们还包含噪声和眼睛形状。这就提出了一个问题，这个模型是否对这个阶层有偏见。它能够正确分类不在人脸上的氧气面罩图像吗？</p><figure class="jv jw jx jy gt jz gh gi paragraph-image"><div role="button" tabindex="0" class="ka kb di kc bf kd"><div class="gh gi na"><img src="../Images/df195c17320d29d38bff75d145e1225b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Lhxgb8-pNaSbzGUHNkKSyQ.jpeg"/></div></div><p class="kg kh gj gh gi ki kj bd b be z dk translated">图片来自[0]</p></figure><p id="595f" class="pw-post-body-paragraph li lj iq lk b ll mm ln lo lp mn lr ls lt mo lv lw lx mp lz ma mb mq md me mf ij bi translated">红色像素显示了在ImageNet上训练的ResNet-50 [5]集中对动物图像进行分类的部分图像，如巨嘴鸟、狐狸、鹰和孔雀。这些红色像素是通过使用<a class="ae mg" href="https://github.com/farhad-dalirani/PytorchRevelio" rel="noopener ugc nofollow" target="_blank"> PytorchRevelio </a>获得的。例如，在对巨嘴鸟进行分类时，ResNet-50主要考虑它的喙，而其他部分则不太重要。为了将图像分类为孔雀类，网络主要使用其羽毛上的眼睛图案。如果巨嘴鸟的喙被遮挡，网络能正确分类吗？</p><p id="a59a" class="pw-post-body-paragraph li lj iq lk b ll mm ln lo lp mn lr ls lt mo lv lw lx mp lz ma mb mq md me mf ij bi translated">以上图像是通过激活最大化、显著图、Grad_CAM等不同方法创建的。通过使用这些方法，回答了许多问题，并提出了新的重要问题。</p></div><div class="ab cl jn jo hu jp" role="separator"><span class="jq bw bk jr js jt"/><span class="jq bw bk jr js jt"/><span class="jq bw bk jr js"/></div><div class="ij ik il im in"><h1 id="af67" class="kk kl iq bd km kn mh kp kq kr mi kt ku kv mj kx ky kz mk lb lc ld ml lf lg lh bi translated">激活最大化</h1><p id="69dd" class="pw-post-body-paragraph li lj iq lk b ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf ij bi translated">激活最大化是一种为神经网络中的神经元/过滤器已经学习的特征寻找表示的方法。激活最大化可以在本文中找到:“<a class="ae mg" href="https://www.researchgate.net/publication/265022827_Visualizing_Higher-Layer_Features_of_a_Deep_Network" rel="noopener ugc nofollow" target="_blank">可视化深层网络的更高层特征</a>”[6]。在这种方法中，通过从诸如高斯分布的分布中随机抽取像素来创建图像。然后这个图像被传送到网络。计算神经元/滤波器的输出相对于图像的梯度。将梯度添加到图像中，以找到为神经元创建更大输出的更好的图像(梯度上升法，用于最大化)。新图像经历相同的过程，这种情况会发生几次。在每一步中获得一个新的图像，该图像比前一步更多地激活目标神经元/滤波器。</p><figure class="jv jw jx jy gt jz gh gi paragraph-image"><div role="button" tabindex="0" class="ka kb di kc bf kd"><div class="gh gi nb"><img src="../Images/5aa38ba78a608f63c5010da132c8c1e7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*eoh7NGP837VUpHz8.jpg"/></div></div><p class="kg kh gj gh gi ki kj bd b be z dk translated">图片来自[0]</p></figure><figure class="jv jw jx jy gt jz gh gi paragraph-image"><div role="button" tabindex="0" class="ka kb di kc bf kd"><div class="gh gi nb"><img src="../Images/bf0d87cca34497aeb7a5465fdfda88bf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*r0UXEgn_OzAXEsTv.jpg"/></div></div><p class="kg kh gj gh gi ki kj bd b be z dk translated">图片来自[0]</p></figure><p id="890b" class="pw-post-body-paragraph li lj iq lk b ll mm ln lo lp mn lr ls lt mo lv lw lx mp lz ma mb mq md me mf ij bi translated">上面这两幅图像代表了AlexNet[7]中名为“features.0”和“features.6”的图层中的一些学习过的过滤器。第一张图片中的滤镜属于AlexNet的第一层；他们已经学会了简单的特征，例如不同方向的边。第二个图像中的滤镜位于深度较高的图层中。如图所示，他们已经学会了更复杂的功能。然而，这种方法有一些缺点。例如，如果我们举例说明最后一层(输出层)的特征，我们将看到获得的表示非常模糊。接下来的两种方法将解决这个问题。</p></div><div class="ab cl jn jo hu jp" role="separator"><span class="jq bw bk jr js jt"/><span class="jq bw bk jr js jt"/><span class="jq bw bk jr js"/></div><div class="ij ik il im in"><h1 id="fd01" class="kk kl iq bd km kn mh kp kq kr mi kt ku kv mj kx ky kz mk lb lc ld ml lf lg lh bi translated">高斯模糊激活最大化</h1><p id="b4d0" class="pw-post-body-paragraph li lj iq lk b ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf ij bi translated">如果我们用激活最大化方法产生类的表示，我们不能容易地在学习特征的表示中找到想要的对象。会有很多高频图案降低它们的清晰度。有几种方法可以解决这些问题。最简单的方法之一是使用低通滤波器，如高斯模糊[8]。在每一步中，高斯滤波器应该应用于计算的梯度或获得的图像。其他一切都将与激活最大化方法相同。这种差异在下面的图片中是显而易见的。</p><figure class="jv jw jx jy gt jz gh gi paragraph-image"><div role="button" tabindex="0" class="ka kb di kc bf kd"><div class="gh gi nb"><img src="../Images/f4fd9e9d051c296f0b7a7b2a55a0fb11.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*Dvc7qS0ZCI_zxFTn.jpg"/></div></div><p class="kg kh gj gh gi ki kj bd b be z dk translated">图片来自[0]</p></figure><figure class="jv jw jx jy gt jz gh gi paragraph-image"><div role="button" tabindex="0" class="ka kb di kc bf kd"><div class="gh gi nb"><img src="../Images/345a0134a83e31fb872f9ad722429022.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*JNP9lp0bpIT7tBez.jpg"/></div></div><p class="kg kh gj gh gi ki kj bd b be z dk translated">图片来自[0]</p></figure><figure class="jv jw jx jy gt jz gh gi paragraph-image"><div role="button" tabindex="0" class="ka kb di kc bf kd"><div class="gh gi nb"><img src="../Images/f1c00fa36fd6f1fb800cc167dbfdb74e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*KOEYJMVwttTwIP8G.jpg"/></div></div><p class="kg kh gj gh gi ki kj bd b be z dk translated">图片来自[0]</p></figure><figure class="jv jw jx jy gt jz gh gi paragraph-image"><div role="button" tabindex="0" class="ka kb di kc bf kd"><div class="gh gi nb"><img src="../Images/a68036543f81d37d48a6c516bb6feaba.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*1x88EUGXWs4jOxZi.jpg"/></div></div><p class="kg kh gj gh gi ki kj bd b be z dk translated">图片来自[0]</p></figure><figure class="jv jw jx jy gt jz gh gi paragraph-image"><div role="button" tabindex="0" class="ka kb di kc bf kd"><div class="gh gi nb"><img src="../Images/af573f921bb04464b4e9391cf6130d47.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*OqFnLw12CmN6MSL2.jpg"/></div></div><p class="kg kh gj gh gi ki kj bd b be z dk translated">图片来自[0]</p></figure><figure class="jv jw jx jy gt jz gh gi paragraph-image"><div role="button" tabindex="0" class="ka kb di kc bf kd"><div class="gh gi nb"><img src="../Images/4bf22aeaebdf8b764a45b5fdcee57e8f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*9Qxz3WjGKubbDYBu.jpg"/></div></div><p class="kg kh gj gh gi ki kj bd b be z dk translated">图片来自[0]</p></figure><p id="14e5" class="pw-post-body-paragraph li lj iq lk b ll mm ln lo lp mn lr ls lt mo lv lw lx mp lz ma mb mq md me mf ij bi translated">这些图像是通过使用高斯模糊方法的激活最大化获得的，该方法在<a class="ae mg" href="https://github.com/farhad-dalirani/PytorchRevelio" rel="noopener ugc nofollow" target="_blank"> PytorchRevelio </a>中实现。这些代表了在ImageNet上训练的VGG11 [4]的神经元/过滤器的一些习得特征。如图片所示，第一层的要素较为简单，在后面的层中逐渐变得更加复杂和抽象。</p><p id="df96" class="pw-post-body-paragraph li lj iq lk b ll mm ln lo lp mn lr ls lt mo lv lw lx mp lz ma mb mq md me mf ij bi translated">在<strong class="lk ir">之下，通过进入更深的层，可以在所获得的特征的抽象</strong>中观察到顺序:</p><p id="4468" class="pw-post-body-paragraph li lj iq lk b ll mm ln lo lp mn lr ls lt mo lv lw lx mp lz ma mb mq md me mf ij bi translated">1-边缘/拐角</p><p id="c7e3" class="pw-post-body-paragraph li lj iq lk b ll mm ln lo lp mn lr ls lt mo lv lw lx mp lz ma mb mq md me mf ij bi translated">2-纹理</p><p id="7b3b" class="pw-post-body-paragraph li lj iq lk b ll mm ln lo lp mn lr ls lt mo lv lw lx mp lz ma mb mq md me mf ij bi translated">三模式</p><p id="bd2f" class="pw-post-body-paragraph li lj iq lk b ll mm ln lo lp mn lr ls lt mo lv lw lx mp lz ma mb mq md me mf ij bi translated">4-对象的部件</p><p id="7717" class="pw-post-body-paragraph li lj iq lk b ll mm ln lo lp mn lr ls lt mo lv lw lx mp lz ma mb mq md me mf ij bi translated">5-整个对象</p></div><div class="ab cl jn jo hu jp" role="separator"><span class="jq bw bk jr js jt"/><span class="jq bw bk jr js jt"/><span class="jq bw bk jr js"/></div><div class="ij ik il im in"><h1 id="be61" class="kk kl iq bd km kn mh kp kq kr mi kt ku kv mj kx ky kz mk lb lc ld ml lf lg lh bi translated">双边模糊激活最大化</h1><p id="c4cc" class="pw-post-body-paragraph li lj iq lk b ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf ij bi translated">该方法[9]类似于具有高斯模糊的激活最大化，但是双边滤波器被用作低通滤波器。虽然它像高斯滤波器一样模糊图像，但它保留了边缘。因此，获得了更好的表示。</p></div><div class="ab cl jn jo hu jp" role="separator"><span class="jq bw bk jr js jt"/><span class="jq bw bk jr js jt"/><span class="jq bw bk jr js"/></div><div class="ij ik il im in"><h1 id="ba20" class="kk kl iq bd km kn mh kp kq kr mi kt ku kv mj kx ky kz mk lb lc ld ml lf lg lh bi translated">显著图</h1><p id="cf09" class="pw-post-body-paragraph li lj iq lk b ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf ij bi translated">显著图[10]是示出输入图像的每个像素在计算输出类别分数中的重要性的图。为了计算显著图，需要输入图像和目标类别。为了找到每个像素的重要性，我们计算softmax之前的最后一层网络的目标神经元的输出。然后将目标类的渐变设置为1，其他输出设置为零。之后，计算输出相对于输入图像的梯度。梯度值越大，表示相应的像素越重要。计算显著图很快，因为它只需要一次向前和向后的传递。不幸的是，这种方法的输出不够好，但接下来的两种获得显著图的方法获得了令人印象深刻的结果。</p><figure class="jv jw jx jy gt jz gh gi paragraph-image"><div role="button" tabindex="0" class="ka kb di kc bf kd"><div class="gh gi nb"><img src="../Images/e2337c3cc4c9270e4967b40f72a333a2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*S2Y6DJC3LwOOxFmr.jpg"/></div></div><p class="kg kh gj gh gi ki kj bd b be z dk translated">图片来自[0]</p></figure></div><div class="ab cl jn jo hu jp" role="separator"><span class="jq bw bk jr js jt"/><span class="jq bw bk jr js jt"/><span class="jq bw bk jr js"/></div><div class="ij ik il im in"><h1 id="73dd" class="kk kl iq bd km kn mh kp kq kr mi kt ku kv mj kx ky kz mk lb lc ld ml lf lg lh bi translated">具有引导梯度的显著图</h1><p id="58f3" class="pw-post-body-paragraph li lj iq lk b ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf ij bi translated">具有引导梯度的显著图与前面的方法相同。然而，它使用引导梯度[11]而不是梯度。</p><blockquote class="mr ms mt"><p id="a6c1" class="li lj mu lk b ll mm ln lo lp mn lr ls mv mo lv lw mw mp lz ma mx mq md me mf ij bi translated">我们称这种方法为引导反向传播，因为它将来自更高层的额外引导信号添加到通常的反向传播中。这防止了负梯度的反向流动，对应于减少我们旨在可视化的更高层单元的激活的神经元。[11]</p></blockquote><p id="9a6c" class="pw-post-body-paragraph li lj iq lk b ll mm ln lo lp mn lr ls lt mo lv lw lx mp lz ma mb mq md me mf ij bi translated">使用引导梯度极大地提高了所获得的显著图的质量。下面您可以看到PytorchRevelio计算的几个输出。差异是显而易见的。</p><figure class="jv jw jx jy gt jz gh gi paragraph-image"><div role="button" tabindex="0" class="ka kb di kc bf kd"><div class="gh gi nb"><img src="../Images/19898bf5091d447b386b81c9b99dd18d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*zJfiFbwFOhPX-UsF.jpg"/></div></div><p class="kg kh gj gh gi ki kj bd b be z dk translated">图片来自[0]</p></figure><figure class="jv jw jx jy gt jz gh gi paragraph-image"><div role="button" tabindex="0" class="ka kb di kc bf kd"><div class="gh gi nb"><img src="../Images/5b4925b131e2952d378d1c9b513f7ace.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*hBQhUueOW_wHSoYD.jpg"/></div></div><p class="kg kh gj gh gi ki kj bd b be z dk translated">图片来自[0]</p></figure><figure class="jv jw jx jy gt jz gh gi paragraph-image"><div role="button" tabindex="0" class="ka kb di kc bf kd"><div class="gh gi nb"><img src="../Images/5c51a4f260c80178056e2d8cc52ec173.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*zplQ7y7lgQ8y0JwO.jpg"/></div></div><p class="kg kh gj gh gi ki kj bd b be z dk translated">图片来自[0]</p></figure><p id="376d" class="pw-post-body-paragraph li lj iq lk b ll mm ln lo lp mn lr ls lt mo lv lw lx mp lz ma mb mq md me mf ij bi translated">与前一种方法相比，这种方法产生的图像质量较高。然而，如果我们试图为给定的图像和包含几个不同对象的目标类绘制显著图，我们将观察到其他对象的部分也被标记。下图是这个问题的一个例子。虽然秃鹰是这幅图的目标职业，但是狮子的某些部位也有标记。下一个方法解决了这个问题。</p><figure class="jv jw jx jy gt jz gh gi paragraph-image"><div class="ab gu cl nc"><img src="../Images/6e3f648943b3916891817b4ba2b8d086.png" data-original-src="https://miro.medium.com/v2/format:webp/1*x7_YrHHPrthFgzSWsCFKbg.png"/></div><p class="kg kh gj gh gi ki kj bd b be z dk translated">图片来自[0]</p></figure></div><div class="ab cl jn jo hu jp" role="separator"><span class="jq bw bk jr js jt"/><span class="jq bw bk jr js jt"/><span class="jq bw bk jr js"/></div><div class="ij ik il im in"><h1 id="dadb" class="kk kl iq bd km kn mh kp kq kr mi kt ku kv mj kx ky kz mk lb lc ld ml lf lg lh bi translated">制导摄像机</h1><p id="062f" class="pw-post-body-paragraph li lj iq lk b ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf ij bi translated">Guided Grad-CAM [3]方法首先使用先前的方法获得输入图像和目标类别的显著图:“具有引导梯度的显著图”然而，它计算得到的显著图的逐元素乘积和矩阵L，以消除显著图中不想要的像素。为了获得矩阵L，它使用网络的最后一个卷积层的输出特征图。他们的理由如下:</p><blockquote class="mr ms mt"><p id="214e" class="li lj mu lk b ll mm ln lo lp mn lr ls mv mo lv lw mw mp lz ma mx mq md me mf ij bi translated">此外，卷积层自然保留了全连接层中丢失的空间信息，因此我们可以预期最后的卷积层在高级语义和详细的空间信息之间具有最佳的折衷。[3]</p></blockquote><p id="3be5" class="pw-post-body-paragraph li lj iq lk b ll mm ln lo lp mn lr ls lt mo lv lw lx mp lz ma mb mq md me mf ij bi translated">和</p><blockquote class="mr ms mt"><p id="8ea0" class="li lj mu lk b ll mm ln lo lp mn lr ls mv mo lv lw mw mp lz ma mx mq md me mf ij bi translated">Grad-CAM使用流入CNN最后一个卷积层的梯度信息，为每个神经元分配重要值，以做出感兴趣的特定决策。[3]</p></blockquote><p id="2ede" class="pw-post-body-paragraph li lj iq lk b ll mm ln lo lp mn lr ls lt mo lv lw lx mp lz ma mb mq md me mf ij bi translated">它使目标类的梯度等于21，然后相对于最后一个卷积层的输出特征图计算目标类得分的梯度。现在，为了找到每个特征图的重要性，计算该特征图的相应梯度的平均值。为了获得矩阵L，借助于计算的重要性，计算输出特征图的加权平均值，加权平均值的负元素被设置为零。如上所述，通过在矩阵L和引导梯度显著图之间执行逐元素乘积，获得最终的显著图。</p><p id="74fe" class="pw-post-body-paragraph li lj iq lk b ll mm ln lo lp mn lr ls lt mo lv lw lx mp lz ma mb mq md me mf ij bi translated">在下图中，您可以看到在ImageNet上训练的ResNet-50 [5]的该方法的一些输出。此外，为了进行比较，还提供了前面方法的输出。</p><figure class="jv jw jx jy gt jz gh gi paragraph-image"><div role="button" tabindex="0" class="ka kb di kc bf kd"><div class="gh gi nd"><img src="../Images/4a79554c5266952a085fe2cbb5888e1b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*PLqwSrpcCC8qsOnB.jpg"/></div></div><p class="kg kh gj gh gi ki kj bd b be z dk translated">图片来自[0]</p></figure><figure class="jv jw jx jy gt jz gh gi paragraph-image"><div role="button" tabindex="0" class="ka kb di kc bf kd"><div class="gh gi ne"><img src="../Images/d93d378be25878a404dd14d0ce080511.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*VZwBZnAZlj2taECM.jpg"/></div></div><p class="kg kh gj gh gi ki kj bd b be z dk translated">图片来自[0]</p></figure><figure class="jv jw jx jy gt jz gh gi paragraph-image"><div role="button" tabindex="0" class="ka kb di kc bf kd"><div class="gh gi nf"><img src="../Images/bd344b6893598a42fe88347cada2c62f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*bNAme3jtQ-wkVEMR.jpg"/></div></div><p class="kg kh gj gh gi ki kj bd b be z dk translated">图片来自[0]</p></figure><figure class="jv jw jx jy gt jz gh gi paragraph-image"><div role="button" tabindex="0" class="ka kb di kc bf kd"><div class="gh gi ng"><img src="../Images/f9b1e4ef6ebbf0c9b4f2829c7b7c564a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*o2LSBXnIf-hm30tV.jpg"/></div></div><p class="kg kh gj gh gi ki kj bd b be z dk translated">图片来自[0]</p></figure><figure class="jv jw jx jy gt jz gh gi paragraph-image"><div role="button" tabindex="0" class="ka kb di kc bf kd"><div class="gh gi nh"><img src="../Images/e3cf17c457da2429512a1a6af228caed.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*J-ANdC491Ph6g2tK.jpg"/></div></div><p class="kg kh gj gh gi ki kj bd b be z dk translated">图片来自[0]</p></figure><figure class="jv jw jx jy gt jz gh gi paragraph-image"><div role="button" tabindex="0" class="ka kb di kc bf kd"><div class="gh gi ni"><img src="../Images/c48962e6393f735986ad7a033d78bd27.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*vnKgbdbqzZSzZm8Y.jpg"/></div></div><p class="kg kh gj gh gi ki kj bd b be z dk translated">图片来自[0]</p></figure><p id="a420" class="pw-post-body-paragraph li lj iq lk b ll mm ln lo lp mn lr ls lt mo lv lw lx mp lz ma mb mq md me mf ij bi translated">在上面的图像中，图像中有不止一个对象。与引导梯度显著图不同，在引导梯度CAM方法中，只有属于输入目标类别的对象部分被突出显示。</p></div><div class="ab cl jn jo hu jp" role="separator"><span class="jq bw bk jr js jt"/><span class="jq bw bk jr js jt"/><span class="jq bw bk jr js"/></div><div class="ij ik il im in"><h1 id="193a" class="kk kl iq bd km kn mh kp kq kr mi kt ku kv mj kx ky kz mk lb lc ld ml lf lg lh bi translated">到目前为止PytorchRevelio中实现的方法</h1><figure class="jv jw jx jy gt jz gh gi paragraph-image"><div role="button" tabindex="0" class="ka kb di kc bf kd"><div class="gh gi nj"><img src="../Images/a09011df6f324d7c1cc6b596db76f000.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*D8UZPILYRAZJUONiITX8fA.png"/></div></div><p class="kg kh gj gh gi ki kj bd b be z dk translated">图片来自[0]</p></figure></div><div class="ab cl jn jo hu jp" role="separator"><span class="jq bw bk jr js jt"/><span class="jq bw bk jr js jt"/><span class="jq bw bk jr js"/></div><div class="ij ik il im in"><h1 id="9df8" class="kk kl iq bd km kn mh kp kq kr mi kt ku kv mj kx ky kz mk lb lc ld ml lf lg lh bi translated">哪种方法性能更好？</h1><ul class=""><li id="875f" class="nk nl iq lk b ll lm lp lq lt nm lx nn mb no mf np nq nr ns bi translated">列表中较高的方法在特征可视化方面表现更好:</li></ul><ol class=""><li id="6963" class="nk nl iq lk b ll mm lp mn lt nt lx nu mb nv mf nw nq nr ns bi translated">激活最大化双边模糊</li><li id="7cdc" class="nk nl iq lk b ll nx lp ny lt nz lx oa mb ob mf nw nq nr ns bi translated">激活最大化高斯模糊</li><li id="612f" class="nk nl iq lk b ll nx lp ny lt nz lx oa mb ob mf nw nq nr ns bi translated">激活_最大化</li></ol><ul class=""><li id="f65c" class="nk nl iq lk b ll mm lp mn lt nt lx nu mb nv mf np nq nr ns bi translated">列表中较高的方法对显著图执行得更好:</li></ul><ol class=""><li id="3ba3" class="nk nl iq lk b ll mm lp mn lt nt lx nu mb nv mf nw nq nr ns bi translated">grad_cam</li><li id="2841" class="nk nl iq lk b ll nx lp ny lt nz lx oa mb ob mf nw nq nr ns bi translated">显著性_地图_导向</li><li id="61f0" class="nk nl iq lk b ll nx lp ny lt nz lx oa mb ob mf nw nq nr ns bi translated">显著性_地图</li></ol></div><div class="ab cl jn jo hu jp" role="separator"><span class="jq bw bk jr js jt"/><span class="jq bw bk jr js jt"/><span class="jq bw bk jr js"/></div><div class="ij ik il im in"><p id="14f5" class="pw-post-body-paragraph li lj iq lk b ll mm ln lo lp mn lr ls lt mo lv lw lx mp lz ma mb mq md me mf ij bi translated">更多细节以及如何使用PytorchRevelio，请访问其GitHub页面:<a class="ae mg" href="https://github.com/farhad-dalirani/PytorchRevelio" rel="noopener ugc nofollow" target="_blank">https://github.com/farhad-dalirani/PytorchRevelio</a></p></div><div class="ab cl jn jo hu jp" role="separator"><span class="jq bw bk jr js jt"/><span class="jq bw bk jr js jt"/><span class="jq bw bk jr js"/></div><div class="ij ik il im in"><h1 id="7286" class="kk kl iq bd km kn mh kp kq kr mi kt ku kv mj kx ky kz mk lb lc ld ml lf lg lh bi translated">参考</h1><p id="fca3" class="pw-post-body-paragraph li lj iq lk b ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf ij bi translated">[0] PytorchRevelio，【https://github.com/farhad-dalirani/PytorchRevelio】T2</p><p id="9b7b" class="pw-post-body-paragraph li lj iq lk b ll mm ln lo lp mn lr ls lt mo lv lw lx mp lz ma mb mq md me mf ij bi translated">[1]托勒斯·J·默雷尔·WJ。逻辑回归:将患者特征与结果联系起来。<em class="mu"> JAMA。</em>2016；316(5):533–534.doi:10.1001/jama</p><p id="d618" class="pw-post-body-paragraph li lj iq lk b ll mm ln lo lp mn lr ls lt mo lv lw lx mp lz ma mb mq md me mf ij bi translated">[2]科尔特斯，科琳娜和弗拉基米尔·瓦普尼克。"支持向量网络。"机器学习20.3(1995):273–297。</p><p id="352c" class="pw-post-body-paragraph li lj iq lk b ll mm ln lo lp mn lr ls lt mo lv lw lx mp lz ma mb mq md me mf ij bi translated">[3] Selvaraju，Ramprasaath R .等人，“Grad-cam:通过基于梯度的定位从深度网络中进行可视化解释”<em class="mu">IEEE计算机视觉国际会议论文集</em>。2017.</p><p id="a877" class="pw-post-body-paragraph li lj iq lk b ll mm ln lo lp mn lr ls lt mo lv lw lx mp lz ma mb mq md me mf ij bi translated">[4]西蒙扬、卡伦和安德鲁·齐泽曼。“用于大规模图像识别的非常深的卷积网络。”<em class="mu"> arXiv预印本arXiv:1409.1556 </em> (2014)。</p><p id="de0b" class="pw-post-body-paragraph li lj iq lk b ll mm ln lo lp mn lr ls lt mo lv lw lx mp lz ma mb mq md me mf ij bi translated">[5]何，，等.“用于图像识别的深度残差学习”<em class="mu">IEEE计算机视觉和模式识别会议论文集</em>。2016.</p><p id="c454" class="pw-post-body-paragraph li lj iq lk b ll mm ln lo lp mn lr ls lt mo lv lw lx mp lz ma mb mq md me mf ij bi translated">[6] Erhan，Dumitru，等，“深层网络的高层特征可视化”蒙特利尔大学(2009年)</p><p id="84d4" class="pw-post-body-paragraph li lj iq lk b ll mm ln lo lp mn lr ls lt mo lv lw lx mp lz ma mb mq md me mf ij bi translated">[7]克里热夫斯基、亚历克斯、伊利亚·苏茨基弗和杰弗里·e·辛顿。"使用深度卷积神经网络的图像网络分类."<em class="mu">神经信息处理系统进展</em>25(2012):1097–1105。</p><p id="6534" class="pw-post-body-paragraph li lj iq lk b ll mm ln lo lp mn lr ls lt mo lv lw lx mp lz ma mb mq md me mf ij bi translated">[8]au dun m . ygard，<a class="ae mg" href="https://www.auduno.com/2015/07/29/visualizing-googlenet-classes/" rel="noopener ugc nofollow" target="_blank">https://www . au duno . com/2015/07/29/visualizing-Google net-classes/</a>，2015年。</p><p id="e5b4" class="pw-post-body-paragraph li lj iq lk b ll mm ln lo lp mn lr ls lt mo lv lw lx mp lz ma mb mq md me mf ij bi translated">[9]迈克·泰卡，<a class="ae mg" href="https://mtyka.github.io/deepdream/2016/02/05/bilateral-class-vis.html" rel="noopener ugc nofollow" target="_blank">https://mtyka . github . io/deep dream/2016/02/05/bias-class-vis . html</a>，2016。</p><p id="526a" class="pw-post-body-paragraph li lj iq lk b ll mm ln lo lp mn lr ls lt mo lv lw lx mp lz ma mb mq md me mf ij bi translated">[10] Simonyan，Karen，Andrea Vedaldi和Andrew Zisserman。"深入卷积网络内部:可视化图像分类模型和显著图."<em class="mu"> arXiv预印本arXiv:1312.6034 </em> (2013)。</p><p id="541d" class="pw-post-body-paragraph li lj iq lk b ll mm ln lo lp mn lr ls lt mo lv lw lx mp lz ma mb mq md me mf ij bi translated">[11]斯普林根伯格，乔斯特·托拜厄斯，等，“力求简单:全卷积网络。”<em class="mu"> arXiv预印本arXiv:1412.6806 </em> (2014)。</p></div></div>    
</body>
</html>