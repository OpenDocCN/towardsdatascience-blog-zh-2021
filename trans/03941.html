<html>
<head>
<title>Do Different Neural Networks Learn The Same Things?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">不同的神经网络学习的东西是一样的吗？</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/do-different-neural-networks-learn-the-same-things-ac215f2103c3?source=collection_archive---------16-----------------------#2021-04-02">https://towardsdatascience.com/do-different-neural-networks-learn-the-same-things-ac215f2103c3?source=collection_archive---------16-----------------------#2021-04-02</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="8fc1" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">在TensorFlow中实现论文并对其进行注释</h2></div><p id="3dfc" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">你有没有过一个数据集，问:这个模型学习的东西和那个模型不一样？这是阮<em class="le">等人的疑问。艾尔。</em>在他们的论文“广泛的和深度的网络学习同样的东西吗？”[1].</p><h1 id="428d" class="lf lg it bd lh li lj lk ll lm ln lo lp jz lq ka lr kc ls kd lt kf lu kg lv lw bi translated">概观</h1><p id="6949" class="pw-post-body-paragraph ki kj it kk b kl lx ju kn ko ly jx kq kr lz kt ku kv ma kx ky kz mb lb lc ld im bi translated">在这篇文章的其余部分，我引用报纸上的话</p><blockquote class="mc md me"><p id="2125" class="ki kj le kk b kl km ju kn ko kp jx kq mf ks kt ku mg kw kx ky mh la lb lc ld im bi translated">深度神经网络体系结构通常通过调整其宽度和/或深度来适应可用的计算资源。值得注意的是，这种简单的模型缩放方法可以为高资源和低资源体系带来最先进的网络(Tan &amp; Le，2019)。</p></blockquote><p id="3693" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">并在下面写下我的解释。</p><p id="698e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我将重点放在这篇24页论文的核心部分:建立网络和数据集，计算CKA，并创建图表。这也是这篇文章的大纲:我们从一个简短的介绍开始，然后建立网络。然后我解释CKA分数背后的方程式。最后，我们编写代码，从纸上创建可视化。</p><h1 id="b3f4" class="lf lg it bd lh li lj lk ll lm ln lo lp jz lq ka lr kc ls kd lt kf lu kg lv lw bi translated">介绍</h1><blockquote class="mc md me"><p id="9133" class="ki kj le kk b kl km ju kn ko kp jx kq mf ks kt ku mg kw kx ky mh la lb lc ld im bi translated">我们应用CKA (centered kernel alignment)来测量不同神经网络架构的隐藏表示的相似性，发现宽或深模型中的表示呈现出一种特征结构，我们称之为<em class="it">块结构</em>。我们研究了块结构如何在不同的训练运行中变化，并揭示了块结构和模型过度参数化之间的联系——块结构主要出现在过度参数化的模型中。</p></blockquote><p id="344b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">基于后面显示的代码，我们可以通过比较两个模型之间的激活来可视化这样的块结构:</p><figure class="mj mk ml mm gt mn gh gi paragraph-image"><div role="button" tabindex="0" class="mo mp di mq bf mr"><div class="gh gi mi"><img src="../Images/26a3a6970d226ee885785fd839d0c8a2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OUy46Lg47XzF8ez0UmtzPA.png"/></div></div><p class="mu mv gj gh gi mw mx bd b be z dk translated">两个ResNet50s的激活比较。颜色越亮表示相似度越高。由作者创作，基于[1]。</p></figure><h1 id="ad7c" class="lf lg it bd lh li lj lk ll lm ln lo lp jz lq ka lr kc ls kd lt kf lu kg lv lw bi translated">实验装置</h1><blockquote class="mc md me"><p id="20d5" class="ki kj le kk b kl km ju kn ko kp jx kq mf ks kt ku mg kw kx ky mh la lb lc ld im bi translated">[……]我们的实验装置由一个ResNets家族组成(何等，2016；Zagoruyko &amp; Komodakis，2016)在标准影像分类数据集CIFAR-10、CIFAR-100和ImageNet上进行训练。</p></blockquote><p id="46a0" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们把它写成代码。为了保持计算的可行性，我关注三个resnet[2]:50、101和152；和CIFAR-10数据集[3]。</p><h2 id="ee9e" class="my lg it bd lh mz na dn ll nb nc dp lp kr nd ne lr kv nf ng lt kz nh ni lv nj bi translated">ResNet50</h2><p id="97f0" class="pw-post-body-paragraph ki kj it kk b kl lx ju kn ko ly jx kq kr lz kt ku kv ma kx ky kz mb lb lc ld im bi translated">最小的ResNet可以很容易地实例化:我们从Keras下载模型，并加载在ImageNet上预先训练的权重[7]。我们将输入形状设置为(32，32，3)，这是CIFAR-10数据集中一幅图像的形状。我们不需要完全连接的输出层，所以我们将<em class="le"> include_top </em>设置为<em class="le"> False </em>。最后，我们想要汇集输出层，这是一个4D张量，从我们的基本模型得到一个2D输出。由于我们的数据集中有10个类，所以我们包含了自己的密集输出层，其中有10个神经元和softmax激活。</p><p id="bb64" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">最后，我们通过使用基础模型的输入作为输入，使用外部密集层作为输出来创建模型实例:</p><figure class="mj mk ml mm gt mn"><div class="bz fp l di"><div class="nk nl l"/></div></figure><h2 id="f666" class="my lg it bd lh mz na dn ll nb nc dp lp kr nd ne lr kv nf ng lt kz nh ni lv nj bi translated">ResNet101</h2><p id="c01a" class="pw-post-body-paragraph ki kj it kk b kl lx ju kn ko ly jx kq kr lz kt ku kv ma kx ky kz mb lb lc ld im bi translated">对于较大的ResNet101，过程类似:</p><figure class="mj mk ml mm gt mn"><div class="bz fp l di"><div class="nk nl l"/></div></figure><h2 id="0097" class="my lg it bd lh mz na dn ll nb nc dp lp kr nd ne lr kv nf ng lt kz nh ni lv nj bi translated">ResNet152</h2><p id="8840" class="pw-post-body-paragraph ki kj it kk b kl lx ju kn ko ly jx kq kr lz kt ku kv ma kx ky kz mb lb lc ld im bi translated">ResNet152也类似:</p><figure class="mj mk ml mm gt mn"><div class="bz fp l di"><div class="nk nl l"/></div></figure><h2 id="1a52" class="my lg it bd lh mz na dn ll nb nc dp lp kr nd ne lr kv nf ng lt kz nh ni lv nj bi translated">CIFAR-10数据集</h2><p id="e673" class="pw-post-body-paragraph ki kj it kk b kl lx ju kn ko ly jx kq kr lz kt ku kv ma kx ky kz mb lb lc ld im bi translated">获取CIFAR-10数据集非常方便，因为它很容易从TensorFlow获得。我们下载训练和测试数据集，并将像素值从<em class="le">0</em>–<em class="le">255</em>重新调整到<em class="le">0</em>–<em class="le">1.0</em>，使它们成为浮点:</p><figure class="mj mk ml mm gt mn"><div class="bz fp l di"><div class="nk nl l"/></div></figure><h1 id="843d" class="lf lg it bd lh li lj lk ll lm ln lo lp jz lq ka lr kc ls kd lt kf lu kg lv lw bi translated">测量相似性</h1><blockquote class="mc md me"><p id="31a9" class="ki kj le kk b kl km ju kn ko kp jx kq mf ks kt ku mg kw kx ky mh la lb lc ld im bi translated">我们使用线性中心核对齐(Kornblith等人，2019；Cortes et al .，2012)来度量神经网络隐藏表示之间的相似性。</p></blockquote><p id="754b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如何衡量两个神经网络之间的相似性？一种方法是计算(隐藏)层的输出之间的相似性。</p><p id="130a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">从概念上讲，这很简单:你获取一个数据批次，而不是仅仅捕获最终的输出，你还可以捕获任何隐藏层的输出。对两个网络都这样做，现在有两组激活:一组来自网络A，一组来自网络B</p><p id="bac8" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">然后，对A的激活和b的激活进行两两比较，基本上就这样了。</p><p id="29ed" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">但是，对于这样的<em class="le">相似性</em>度量有一些要求:首先，它的范围必须在0和1之间，0表示两个完全不同的激活(不相似)，1表示两个相同的激活(相似)。此外，它必须处理不同形状的激活。</p><p id="da04" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这个度量是(线性)居中的内核对齐[4][5]:</p><blockquote class="mc md me"><p id="62b0" class="ki kj le kk b kl km ju kn ko kp jx kq mf ks kt ku mg kw kx ky mh la lb lc ld im bi translated">为了减少内存消耗，我们将CKA计算为对k个迷你批次计算的平均HSIC分数的函数:</p></blockquote><figure class="mj mk ml mm gt mn gh gi paragraph-image"><div role="button" tabindex="0" class="mo mp di mq bf mr"><div class="gh gi nm"><img src="../Images/5a3781e1d2f5aea6e84abacd7b5d64b9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hUPDmco-tiFOUy_SWtHe2A.png"/></div></div></figure><blockquote class="mc md me"><p id="23a1" class="ki kj le kk b kl km ju kn ko kp jx kq mf ks kt ku mg kw kx ky mh la lb lc ld im bi translated">其中，xᵢ∈r^(n×p₁)和yᵢ∈r^(n×p₂)是包含两层激活的矩阵，一层具有p₁神经元，另一层具有p₂神经元，这两层激活是对相同的n个样本的小批量无替换采样。</p></blockquote><p id="74a0" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在我解释这个等式之前，我们需要知道HSIC是什么:</p><blockquote class="mc md me"><p id="d6ab" class="ki kj le kk b kl km ju kn ko kp jx kq mf ks kt ku mg kw kx ky mh la lb lc ld im bi translated">我们使用的无偏估计量(宋等人，2012年)，因此的值与批量无关:</p></blockquote><figure class="mj mk ml mm gt mn gh gi paragraph-image"><div role="button" tabindex="0" class="mo mp di mq bf mr"><div class="gh gi nn"><img src="../Images/2e844156364b26e99705b7e3b2c3edfd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-tNezUtVDgWeAx1SIrkccA.png"/></div></div></figure><blockquote class="mc md me"><p id="4011" class="ki kj le kk b kl km ju kn ko kp jx kq mf ks kt ku mg kw kx ky mh la lb lc ld im bi translated">其中~K和~L是通过将K和L的对角线项设置为零而获得的。</p></blockquote><p id="1a6e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">好了，让我们详细讨论这些等式，从CKA分数所基于的HSIC₁开始:</p><h2 id="91a9" class="my lg it bd lh mz na dn ll nb nc dp lp kr nd ne lr kv nf ng lt kz nh ni lv nj bi translated">HSIC</h2><p id="1c26" class="pw-post-body-paragraph ki kj it kk b kl lx ju kn ko ly jx kq kr lz kt ku kv ma kx ky kz mb lb lc ld im bi translated">HSIC是希尔伯特-施密德独立性标准[6]的缩写，它衡量两个分布之间的统计独立性。在我们的例子中，这些分布就是激活。</p><p id="64e9" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们再看一遍[6]中的等式，但这次要突出重要的部分:</p><figure class="mj mk ml mm gt mn gh gi paragraph-image"><div role="button" tabindex="0" class="mo mp di mq bf mr"><div class="gh gi no"><img src="../Images/3dcd53229d4ae08edeb2025b41df8874.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zzEDJZvBrDtdwlhXMyYhVg.png"/></div></div></figure><p id="2cdf" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">第一个框标记了tr()操作符，它是矩阵的<em class="le">轨迹</em>的缩写。矩阵的迹是主对角线上元素的和。我们计算轨迹的矩阵是修正的<strong class="kk iu"> K </strong>和<strong class="kk iu"> L </strong>之间的点积。</p><p id="440c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在，为什么矩阵用粗体书写？来表明它们是矩阵；这是一个标准惯例。这也导致了第二个标记块，即<strong class="kk iu"> 1 </strong>:这是一个长度为n的向量，填充了1(也称为单位向量)。</p><p id="7f76" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">为什么我们知道它是向量还是矩阵？看最后标记的部分，<strong class="kk iu">1</strong>^t. t表示我们调换了它左边的东西。现在，如果我们有一个正方形矩阵(n，n)，只填充1，转置操作将没有任何意义(因为转置的正方形单位矩阵等于原始矩阵)。</p><p id="1a31" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">最后，如何知道<strong class="kk iu"> 1 </strong>向量的长度？这是由矩阵<strong class="kk iu"> K </strong>和<strong class="kk iu"> L </strong>的维数决定的，它们是方阵。它们是方形的，因为我们称HSIC为</p><figure class="mj mk ml mm gt mn gh gi paragraph-image"><div class="gh gi np"><img src="../Images/f0085cadd9a1882696f940774951f05c.png" data-original-src="https://miro.medium.com/v2/resize:fit:498/format:webp/1*Vyqa_Jf-dwSrtOoh5f6xXg.png"/></div></figure><p id="e342" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在这里，我们计算原始矩阵与其转置后的第一个和第二个参数之间的点积。矩阵级的点积等于这两个矩阵的乘积。(在阅读了[4]之后，我决定坚持使用点积，他们在第3节中使用了这个命名约定)。由于我们将形状(m，k)的矩阵与其形状(k，m)的转置形式相乘，我们得到形状(m，m)的方阵。</p><blockquote class="mc md me"><p id="1538" class="ki kj le kk b kl km ju kn ko kp jx kq mf ks kt ku mg kw kx ky mh la lb lc ld im bi translated">我们使用通过在测试数据集上迭代10次获得的大小为n = 256的小批，在每个时期内采样而不替换。</p></blockquote><p id="7f4b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这表明我们的矩阵以后将具有的维度:我们首先有一个形状(256，a* b * c *……)的激活，然后它变成(256，256)——一个正方形矩阵。</p><p id="6527" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在代码级别，这会产生以下结果:</p><figure class="mj mk ml mm gt mn"><div class="bz fp l di"><div class="nk nl l"/></div></figure><p id="9731" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们现在已经涵盖了HSIC，我们可以继续与CKA:</p><h2 id="730a" class="my lg it bd lh mz na dn ll nb nc dp lp kr nd ne lr kv nf ng lt kz nh ni lv nj bi translated">CKA</h2><p id="8f15" class="pw-post-body-paragraph ki kj it kk b kl lx ju kn ko ly jx kq kr lz kt ku kv ma kx ky kz mb lb lc ld im bi translated">CKA是居中内核对齐[5]的缩写，它将HSIC归一化:</p><figure class="mj mk ml mm gt mn gh gi paragraph-image"><div role="button" tabindex="0" class="mo mp di mq bf mr"><div class="gh gi nm"><img src="../Images/5a3781e1d2f5aea6e84abacd7b5d64b9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hUPDmco-tiFOUy_SWtHe2A.png"/></div></div></figure><p id="f273" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们分数的命名者是一组激活的单个HSICs的总和。这是σ，它涵盖了所有单独的激活。这是</p><blockquote class="mc md me"><p id="bbea" class="ki kj le kk b kl km ju kn ko kp jx kq mf ks kt ku mg kw kx ky mh la lb lc ld im bi translated">我们将CKA计算为在k个迷你批次上计算的平均HSIC分数的函数</p></blockquote><p id="c7ee" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">简单地说，我们取几个批次(大小为256)，分别捕获模型A和模型B的激活。对于每个批次，我们现在都有来自模型A和模型B的激活，我们用它们来计算CKA分数。</p><p id="d9d0" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">分母是归一化因子。</p><p id="c148" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">尽管CKA中的C代表居中，但我没有发现[1]中提到的居中过程。抬头一看，我发现</p><figure class="mj mk ml mm gt mn gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/f898a465b64d1cf9930fce9e1dac6bcd.png" data-original-src="https://miro.medium.com/v2/resize:fit:430/format:webp/1*bSKabi1SbMaVYuJ16QUbYg.png"/></div></figure><p id="084d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这类似于在计算HSIC分数的最后部分中使用的居中过程。</p><p id="7312" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">当在python中实现这一点时，对来自CIFAR-10数据集的一批256幅图像的计算需要一个小时。因此，我通过仅使用一个批次来减少计算量，去掉σ:</p><figure class="mj mk ml mm gt mn"><div class="bz fp l di"><div class="nk nl l"/></div></figure><h1 id="0c1c" class="lf lg it bd lh li lj lk ll lm ln lo lp jz lq ka lr kc ls kd lt kf lu kg lv lw bi translated">结果</h1><blockquote class="mc md me"><p id="7b8f" class="ki kj le kk b kl km ju kn ko kp jx kq mf ks kt ku mg kw kx ky mh la lb lc ld im bi translated">我们通过调查模型架构的深度和宽度如何影响其内部表示结构来开始我们的研究。在不同的架构中，表示是如何通过隐藏层发展的？不同的隐藏图层表示彼此之间有多相似？为了回答这些问题，我们使用3.1节中概述的CKA表示相似性度量。</p><p id="a3fb" class="ki kj le kk b kl km ju kn ko kp jx kq mf ks kt ku mg kw kx ky mh la lb lc ld im bi translated">我们发现，随着网络变得更宽和/或更深，它们的表示显示出一种特征性的<em class="it">块结构</em>:许多(几乎)连续的隐藏层具有高度相似的表示。</p></blockquote><p id="4eb0" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们将它写入代码并验证:</p><p id="5262" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们首先在CIFAR-10数据集上训练我们的结果。为了加快计算速度，我们使用TPU和256:</p><figure class="mj mk ml mm gt mn"><div class="bz fp l di"><div class="nk nl l"/></div></figure><p id="7275" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">上面使用的<em class="le">策略</em>对象来自TensorFlow，管理TPU培训。一个历元需要大约8秒钟，由于本文/博客的重点不是高精度分数，我们将历元的数量设置为10。</p><p id="2412" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们同样训练其他人:</p><figure class="mj mk ml mm gt mn"><div class="bz fp l di"><div class="nk nl l"/></div></figure><p id="a724" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">对于较大的模型来说，这些时期大约需要15到20秒。</p><p id="d218" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在我们需要一些代码来计算两个激活之间的相似性。</p><p id="4f6f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们首先展平除第一个通道之外的所有通道，这将形状(n，a，b，c)的矩阵转换为形状(n，a*b*c)的矩阵。在此过程中不会丢失任何信息。在重塑激活之后，我们可以计算它们的CKA分数。为了节省内存，我们随后删除它们，只返回分数:</p><figure class="mj mk ml mm gt mn"><div class="bz fp l di"><div class="nk nl l"/></div></figure><p id="6ed6" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这段代码需要两次激活，但是我们如何首先激活它们呢？为此，我们可以使用Keras后端:</p><p id="6509" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们编写了一个短函数，它接受一个模型，并返回一个<em class="le">函数</em>，给定一个数据批次，该函数返回每个中间层的输出:</p><figure class="mj mk ml mm gt mn"><div class="bz fp l di"><div class="nk nl l"/></div></figure><blockquote class="mc md me"><p id="d7d9" class="ki kj le kk b kl km ju kn ko kp jx kq mf ks kt ku mg kw kx ky mh la lb lc ld im bi translated">我们可以将结果可视化为热图，x轴和y轴代表网络的图层，从输入层到输出层。</p></blockquote><p id="b2f5" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">以下函数采用两个模型和一个数据批次。我们使用前面的方法来获取数据批次的所有隐藏激活。然后，我们为热图创建一个占位符:</p><figure class="mj mk ml mm gt mn"><div class="bz fp l di"><div class="nk nl l"/></div></figure><blockquote class="mc md me"><p id="186c" class="ki kj le kk b kl km ju kn ko kp jx kq mf ks kt ku mg kw kx ky mh la lb lc ld im bi translated">热图开始时显示类似棋盘的表示相似性结构，这是因为残差连接后的表示比ResNet块内的表示更类似于其他残差后的表示。随着模型变得更宽或更深，我们看到一个独特的<em class="it">块结构</em>的出现——相当大范围的隐藏层具有非常高的表示相似性(在热图上看起来像一个黄色的正方形)。这种块结构大多出现在网络的较后层(最后两级)。</p></blockquote><p id="e009" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">基于前面的函数，我们可以通过调用</p><figure class="mj mk ml mm gt mn"><div class="bz fp l di"><div class="nk nl l"/></div></figure><p id="3e85" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">然后使用pyplot来可视化这个矩阵:</p><figure class="mj mk ml mm gt mn"><div class="bz fp l di"><div class="nk nl l"/></div></figure><p id="0462" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">将ResNet50的激活与ResNet101的激活进行比较需要大约一个小时:</p><figure class="mj mk ml mm gt mn gh gi paragraph-image"><div role="button" tabindex="0" class="mo mp di mq bf mr"><div class="gh gi nr"><img src="../Images/45b527297635c8727ade3786bceb7363.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gBg2TI_w_pNSUDZ6gfGjkw.png"/></div></div><p class="mu mv gj gh gi mw mx bd b be z dk translated">将ResNet50的激活与ResNet101的激活进行比较。</p></figure><p id="922d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">检查这个图像，我们看到开始提到的块结构，模型的前几层似乎学习非常相似的内部表示。随着层的增加，这变得更加多样化:垂直线(来自ResNet101的层的激活)与来自较小的ResNet的激活完全不同。</p><p id="fbdb" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">当将较小的ResNet与ResNet152(在Colab上耗时2小时)进行比较时，这种空白变得更加明显:</p><figure class="mj mk ml mm gt mn gh gi paragraph-image"><div role="button" tabindex="0" class="mo mp di mq bf mr"><div class="gh gi ns"><img src="../Images/db0bc277a2fe56b8d5e15fb39fdc5c83.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PjMxXjZ-8Ud9hMjzFzLKGg.png"/></div></div><p class="mu mv gj gh gi mw mx bd b be z dk translated">比较ResNet50和ResNet 152。</p></figure><p id="f1b7" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们可以再次看到早期地层中的小块状构造。值得注意的是，大ResNet152的前100至150层似乎与小得多的模型的前20层非常相似。这可能表示过度参数化:</p><p id="bdc5" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">即使我们将参数数量从最小的ResNet的25 636 712增加到ResNet152的60 419 944，但这些附加层似乎不会导致不同的表示。</p><p id="405f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">[请记住，我的论证有几个缺点:由于资源有限，我只能计算一个批次的激活量，并且只能计算几个网络的激活量。鼓励读者使用我的代码，并用更多的批处理运行它]</p><p id="474b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">视觉上明显的是下层的棋盘状图案。</p><p id="c3d5" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">就是这样。该代码可作为Colab笔记本<a class="ae nt" href="https://colab.research.google.com/drive/1O4_b74xB48o2RT8KofLjpmBtufV430Ka?usp=sharing" rel="noopener ugc nofollow" target="_blank">在这里</a>获得。</p></div><div class="ab cl nu nv hx nw" role="separator"><span class="nx bw bk ny nz oa"/><span class="nx bw bk ny nz oa"/><span class="nx bw bk ny nz"/></div><div class="im in io ip iq"><h1 id="c445" class="lf lg it bd lh li ob lk ll lm oc lo lp jz od ka lr kc oe kd lt kf of kg lv lw bi translated">摘要</h1><p id="a3a0" class="pw-post-body-paragraph ki kj it kk b kl lx ju kn ko ly jx kq kr lz kt ku kv ma kx ky kz mb lb lc ld im bi translated">我们讨论了论文的核心部分<a class="ae nt" href="https://arxiv.org/pdf/2010.15327.pdf" rel="noopener ugc nofollow" target="_blank">广泛的网络和深度的网络能学到同样的东西吗？</a>。我们从设置三个ResNets开始，并准备了CIFAR-10数据集。然后，我们研究了本文背后的等式，并用python实现了它们。在最后一部分，我们编写了几个简短的函数来比较两个网络的激活，使我们能够创建类似于论文中的图形。</p><h2 id="bc52" class="my lg it bd lh mz na dn ll nb nc dp lp kr nd ne lr kv nf ng lt kz nh ni lv nj bi translated">从这里去哪里？</h2><p id="f9a7" class="pw-post-body-paragraph ki kj it kk b kl lx ju kn ko ly jx kq kr lz kt ku kv ma kx ky kz mb lb lc ld im bi translated">有几个开放点:</p><ul class=""><li id="bc67" class="og oh it kk b kl km ko kp kr oi kv oj kz ok ld ol om on oo bi translated">实施激活的PCA(第5.1节，共[1])</li><li id="025d" class="og oh it kk b kl op ko oq kr or kv os kz ot ld ol om on oo bi translated">随着更多计算机训练更多的网络</li><li id="37ee" class="og oh it kk b kl op ko oq kr or kv os kz ot ld ol om on oo bi translated">使用更多批次来平均相似性</li></ul><h1 id="f947" class="lf lg it bd lh li lj lk ll lm ln lo lp jz lq ka lr kc ls kd lt kf lu kg lv lw bi translated">参考</h1><p id="8ac9" class="pw-post-body-paragraph ki kj it kk b kl lx ju kn ko ly jx kq kr lz kt ku kv ma kx ky kz mb lb lc ld im bi translated">[1]阮，<em class="le">等</em>，<a class="ae nt" href="https://arxiv.org/pdf/2010.15327.pdf" rel="noopener ugc nofollow" target="_blank">广网和深网学的东西一样吗？揭示神经网络表示如何随宽度和深度变化</a> (2020)，arXiv</p><p id="3dfa" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">[2] K .何，<em class="le"> et。艾尔。，</em> <a class="ae nt" href="https://openaccess.thecvf.com/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf" rel="noopener ugc nofollow" target="_blank">用于图像识别的深度残差学习</a> (2015)，IEEE计算机视觉与模式识别会议论文集</p><p id="8020" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">[3] A .克里热夫斯基，<em class="le"> et。艾尔</em>。，<a class="ae nt" href="https://www.cs.toronto.edu/~kriz/learning-features-2009-TR.pdf" rel="noopener ugc nofollow" target="_blank">从微小图像中学习多层特征</a> (2009)，<em class="le">技术报告</em></p><p id="bf5a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">[4] S. Kornblith等。艾尔。，<a class="ae nt" href="http://proceedings.mlr.press/v97/kornblith19a.html" rel="noopener ugc nofollow" target="_blank">再谈神经网络表示的相似性</a> (2019)，机器学习国际会议</p><p id="ca43" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">[5] C .科尔特斯，<em class="le">等。艾尔。</em>，<a class="ae nt" href="https://www.jmlr.org/papers/volume13/cortes12a/cortes12a.pdf" rel="noopener ugc nofollow" target="_blank">基于中心对齐的学习核算法</a> (2012)，机器学习研究杂志</p><p id="f1b5" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">[6] L .宋，<em class="le"> et。艾尔。</em>，<a class="ae nt" href="https://www.jmlr.org/papers/volume13/song12a/song12a.pdf" rel="noopener ugc nofollow" target="_blank">依赖最大化的特征选择</a> (2012)，机器学习研究杂志</p><p id="0bee" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">[7] J. Deng <em class="le">等</em>，Imagenet: <a class="ae nt" href="https://ieeexplore.ieee.org/document/5206848" rel="noopener ugc nofollow" target="_blank">一个大规模的层次化图像数据库</a> (2009)，IEEE计算机视觉与模式识别会议</p></div></div>    
</body>
</html>