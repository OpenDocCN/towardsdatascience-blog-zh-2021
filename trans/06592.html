<html>
<head>
<title>A high level explanation of popular Neural Network architectures</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">流行神经网络体系结构的高级解释</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/a-high-level-explanation-of-popular-neural-network-architectures-49dce63f02e8?source=collection_archive---------34-----------------------#2021-06-13">https://towardsdatascience.com/a-high-level-explanation-of-popular-neural-network-architectures-49dce63f02e8?source=collection_archive---------34-----------------------#2021-06-13</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="2633" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">从人工神经网络到变形金刚，包括NLP的例子</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/e187ef8b0b67ad14876d424edb97e55c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JdECMGYPClYfPXVa1cZn0w.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图片来自unsplash的Solen Feyissa(<a class="ae ky" href="https://unsplash.com/photos/AAMldegB0x8" rel="noopener ugc nofollow" target="_blank">来源</a>)</p></figure><p id="6bfe" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">神经网络无处不在，因为它们能够很好地捕捉数据中的非线性关系。本文旨在简明扼要地解释流行的神经网络结构。目的是为它们如何工作提供一种直觉，但更重要的是，为什么它们的结构可能对不同的问题有用。每个部分都有到资源的链接，用于深入解释这些主题。</p><h2 id="1599" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated">内容</h2><ol class=""><li id="7c46" class="mo mp it lb b lc mq lf mr li ms lm mt lq mu lu mv mw mx my bi translated"><a class="ae ky" href="#3eb9" rel="noopener ugc nofollow">人工神经网络</a></li><li id="4fc2" class="mo mp it lb b lc mz lf na li nb lm nc lq nd lu mv mw mx my bi translated"><a class="ae ky" href="#b0e2" rel="noopener ugc nofollow">卷积神经网络</a></li><li id="fea4" class="mo mp it lb b lc mz lf na li nb lm nc lq nd lu mv mw mx my bi translated"><a class="ae ky" href="#aad0" rel="noopener ugc nofollow">递归神经网络(RNN) </a></li><li id="4d53" class="mo mp it lb b lc mz lf na li nb lm nc lq nd lu mv mw mx my bi translated"><a class="ae ky" href="#59ad" rel="noopener ugc nofollow">变压器网络和注意机制</a></li><li id="0500" class="mo mp it lb b lc mz lf na li nb lm nc lq nd lu mv mw mx my bi translated"><a class="ae ky" href="#c49f" rel="noopener ugc nofollow">结束语</a></li></ol></div><div class="ab cl ne nf hx ng" role="separator"><span class="nh bw bk ni nj nk"/><span class="nh bw bk ni nj nk"/><span class="nh bw bk ni nj"/></div><div class="im in io ip iq"><h2 id="3eb9" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated">人工神经网络</h2><p id="e50b" class="pw-post-body-paragraph kz la it lb b lc mq ju le lf mr jx lh li nl lk ll lm nm lo lp lq nn ls lt lu im bi translated">神经网络是一个函数，它获取输入张量<strong class="lb iu"> X </strong>(具有<strong class="lb iu"> i </strong>行和<strong class="lb iu"> j </strong>列)并将其映射到输出<strong class="lb iu"> y_hat </strong>以尝试估计真实值<strong class="lb iu"> y </strong>。该函数具有影响映射的未知参数<strong class="lb iu"> θ </strong>。最佳<strong class="lb iu"> θ </strong>是使<strong class="lb iu"> y </strong>和<strong class="lb iu"> y_hat </strong>之间的误差最小的那些。</p><p id="77a4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">网络本身由输入层、隐藏层和输出层组成；下面的图1显示了一个带有一个隐藏层的简单网络。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi no"><img src="../Images/07acbbe68efc7dd99bb8ffb869edfd9e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EHu9umVWWdR_DW8nMcFMMw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图1:具有单一隐藏层和单一向量输入的神经网络</p></figure><p id="81fe" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">每一层的输出作为下一层的输入。层<strong class="lb iu"> (L) </strong>具有<strong class="lb iu"> i </strong>个节点，其值由向量<strong class="lb iu"> a_i^(L) </strong>表示。这是由非线性激活函数<strong class="lb iu"> ϕ^(L) </strong>变换的前一层<strong class="lb iu"> a_j^(L-1) </strong>的输出节点的函数，如下式所定义:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi np"><img src="../Images/3490e4cdf82567ef71bcac6e8b0453be.png" data-original-src="https://miro.medium.com/v2/resize:fit:564/format:webp/1*xrY09x8WWw9EXDCnEUB_hw.png"/></div></figure><p id="c5a4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu"> W_ij </strong>是一个乘法因子(称为权重矩阵)，它线性组合前一层的输出节点。</p><p id="4adc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在图1所示的简单网络中，预测输出由下式给出:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/d2e2495ddc963324735ab42c85a522b4.png" data-original-src="https://miro.medium.com/v2/resize:fit:548/format:webp/1*3bz9jr1vEgZBjwcPZFNfLw.png"/></div></figure><p id="0941" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">其中:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/87e67c3f97c4e87dbae951cf313b2b4f.png" data-original-src="https://miro.medium.com/v2/resize:fit:484/format:webp/1*PZ4aczpp7d7cvowGURRjsg.png"/></div></figure><p id="a59c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在这种情况下，需要估计的模型的未知参数都是权重矩阵<strong class="lb iu"> W_ij^(1) </strong>和<strong class="lb iu"> W_1j^(2) </strong>。这些通过最小化输出和真实值之间的误差来确定，<strong class="lb iu">J =误差(y_hat，y) </strong>。其梯度<strong class="lb iu"> J_i = Nabla(J) </strong>相对于每个分量<strong class="lb iu"> θ </strong>被用于将的值微移至更接近最优值。然后使用<strong class="lb iu"> θ </strong>的新值，即<strong class="lb iu"> θ* = θ-ηJ_i </strong>(其中<strong class="lb iu"> η </strong>是步长)来计算新的<strong class="lb iu"> y_hat </strong>，并且重复该过程多次迭代。这个过程被称为反向传播，也就是通常所说的<em class="ns">学习</em>。</p><p id="2bf3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">伊恩·古德菲勒的书是深入研究神经网络的数学方面的一个很好的资源。另一方面，3Blue1Brown的<a class="ae ky" href="https://www.youtube.com/watch?v=aircAruvnKk&amp;list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi" rel="noopener ugc nofollow" target="_blank">视频</a>为这些网络如何工作提供了一个很好的视觉效果。</p></div><div class="ab cl ne nf hx ng" role="separator"><span class="nh bw bk ni nj nk"/><span class="nh bw bk ni nj nk"/><span class="nh bw bk ni nj"/></div><div class="im in io ip iq"><h2 id="b0e2" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated">卷积神经网络(CNN)</h2><p id="5115" class="pw-post-body-paragraph kz la it lb b lc mq ju le lf mr jx lh li nl lk ll lm nm lo lp lq nn ls lt lu im bi translated">虽然前馈神经网络非常有用，但它们很难从高维数据中提取空间特征。卷积神经网络(CNN)正是为了做到这一点而设计的。对于一个直观的例子，考虑手写字母的图像分类。每个字母都是由像素表示的图片，如下图2所示。然而，由于这些字母是手写的，所以同样的字母会被扭曲、移位、缩小或旋转。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nt"><img src="../Images/60969cb69ff281502d2315cabefb86f2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HYT0cV6UODzvwSpYMXsfwQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图2:两个手写X像素</p></figure><p id="acc8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">作为人类，我们可以识别每幅图像中的空间特征，并认识到它们都代表字母x。然而，正常的前馈网络会将每个像素视为独立的数据点，因此会将图2中的两个字母视为完全不同。CNN通过提取空间特征纠正了这一点。图3显示了图2中代表字母x的两幅图像的特征。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nu"><img src="../Images/5e656956244267ab4e9c499aff276b1d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7BoyCK-TTG33asymzDjvDQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图3:定义字母X的空间特征</p></figure><p id="21f6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这些特征是通过滑动窗口提取的，称为<strong class="lb iu">内核</strong>，用随机权重初始化。在图像的每个窗口，像素值与核的权重进行卷积(即，例如，取点积)。然后应用称为<strong class="lb iu">池</strong>的过程，其中在每个窗口中，取最大值(有效地提取最重要的特征)。在卷积层之后，添加标准前馈层以实现分类。整个过程的图像如下图4所示。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nv"><img src="../Images/9fd86dd0871b85bcf35ddb4dcafb10ad.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*liA2gn7zG2vo3F8XB6Q1dw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图4:显示应用于图像的CNN分类器的工作原理的图表</p></figure><p id="d78d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这个图像示例也可以扩展到其他领域，例如在自然语言处理(NLP)中，您可以用单词及其嵌入维度代替像素，如图5所示。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nw"><img src="../Images/b7f691880bc8ef400974e913d6475636.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oi5vSeevuNb0bwW4Ngbk6w.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图5:应用于文本的CNN分类器</p></figure><p id="82e6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在这种情况下，内核是单向的，它从每个单词的上下文窗口中提取特征。这种思想可以扩展到更大的文本片段，例如，表示变成三维的句子。处理文本数据时的一个关键区别是句子的长度可能不同，因此通常会指定最大句子长度。</p><p id="e562" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">亚历山大·阿米尼的<a class="ae ky" href="https://www.youtube.com/watch?v=AjtX1N_VT9E" rel="noopener ugc nofollow" target="_blank">视频</a>在高层次上很好地解释了CNN。对于那些对数学方面感兴趣的人，再次推荐Ian Goodfellow的书。</p></div><div class="ab cl ne nf hx ng" role="separator"><span class="nh bw bk ni nj nk"/><span class="nh bw bk ni nj nk"/><span class="nh bw bk ni nj"/></div><div class="im in io ip iq"><h2 id="aad0" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated">递归神经网络(RNN)</h2><p id="9512" class="pw-post-body-paragraph kz la it lb b lc mq ju le lf mr jx lh li nl lk ll lm nm lo lp lq nn ls lt lu im bi translated">与图像或其他数据不同，文本包含空间和<em class="ns">序列</em>数据。因此，前馈神经网络和CNN都没有提供一种自然的<em class="ns">方式来读取文本数据。递归神经网络(RNNs)是一种具有反馈回路的结构，能够顺序读取数据。</em></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nx"><img src="../Images/63394b44ebd3681cdd6b458bfc0cc7bd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HXvFiqwrZMzFn2WM3m2JIQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图6:具有反馈回路的RNN结构(左)和展开的RNN(右)</p></figure><p id="14ee" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">因此，每个时间步的输出状态<strong class="lb iu">h _ t</strong>作为该时间步的输入<strong class="lb iu">x _ t</strong>和前一个单元的输出状态的函数给出。因此:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/882385a6d0e8fb355cd94b65399e1d05.png" data-original-src="https://miro.medium.com/v2/resize:fit:328/format:webp/1*wfg6oMCNaIuTgyo-nuQzzQ.png"/></div></figure><p id="5dc2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">此外，文本数据具有需要考虑的长期依赖性。例如，考虑预测下列句子中的下一个单词的任务:</p><blockquote class="nz oa ob"><p id="0c9f" class="kz la ns lb b lc ld ju le lf lg jx lh oc lj lk ll od ln lo lp oe lr ls lt lu im bi translated">我在法国长大，但我现在住在波士顿。我说得很流利…</p></blockquote><p id="742b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在这种情况下，RNN将不得不使用触发单词France来预测下一个单词French。然而，由于单词彼此远离，在训练期间，法国的影响消失了。因此，需要某些储存记忆的激活功能，称为门控细胞。一种常见的门控细胞被称为长短期记忆(LSTM)。示意图如图7所示。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi of"><img src="../Images/ac5325c9b4248e872411835d3aae5d94.png" data-original-src="https://miro.medium.com/v2/resize:fit:1272/format:webp/1*e2GXgj4X0no5w9lAtz_6ww.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图7:显示门控细胞的LSTM网络示意图</p></figure><p id="0858" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">需要注意的重要一点是，LSTM单元有四个作用:<strong class="lb iu">忘记</strong>先前状态的不相关信息，<strong class="lb iu">存储</strong>新信息，<strong class="lb iu">更新</strong>单元值(单独)，控制<strong class="lb iu">输出</strong>信息。这调节了流经细胞的信息，从而更好地捕捉长期依赖性。</p><p id="7dce" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">文本处理中的另一个考虑是信息是双向流动的。例如，现在考虑我们例句的未来上下文:</p><blockquote class="nz oa ob"><p id="bde7" class="kz la ns lb b lc ld ju le lf lg jx lh oc lj lk ll od ln lo lp oe lr ls lt lu im bi translated">我在法国长大，但我现在住在波士顿。我说得很流利…因为我在法兰克福住了5年。</p></blockquote><p id="a5a1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">单向RNN将仅捕获过去的信息，因此可能预测法语。然而，当考虑到未来的背景时，德语会更合适。因此，还开发了双向RNN模型，例如bi-lstm，并且显示出比单向模型执行得更好。</p><p id="515d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">值得一提的是，尽管rnn提供了更自然的文本阅读，但它们通常真的很难训练，因此仅依赖于空间特征的CNN通常在许多任务上表现更好。</p><p id="cfe4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">艾娃·索莱马尼用直观的方式解释了RNNs，特别是LSTMs。关于LSTMs的详细指南，读者可以参考Olah的<a class="ae ky" href="https://colah.github.io/posts/2015-08-Understanding-LSTMs/" rel="noopener ugc nofollow" target="_blank">博客</a>。</p></div><div class="ab cl ne nf hx ng" role="separator"><span class="nh bw bk ni nj nk"/><span class="nh bw bk ni nj nk"/><span class="nh bw bk ni nj"/></div><div class="im in io ip iq"><h2 id="59ad" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated">变压器网络和注意机制</h2><p id="d88b" class="pw-post-body-paragraph kz la it lb b lc mq ju le lf mr jx lh li nl lk ll lm nm lo lp lq nn ls lt lu im bi translated">RNN模型的一个关键限制是信息会在很长的序列中丢失。虽然LSTM能够捕获一些内存，但是结构本身是顺序的，因此输出不是每个序列元素的直接函数。注意机制通过确保序列中的每个项目直接影响输出来解决这个问题。下面的图8给出了这方面的高级示意图。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi og"><img src="../Images/2dff3c91dff4508ea04cf887c9acc8b3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JOrT7JQRynyROanFMCjtIA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图8:使用RNN(左)和基于注意力的网络(右)的机器翻译示例</p></figure><p id="fae3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这具有模仿人类注意力的经验效果，其中机器<em class="ns">关注</em>序列中信息最丰富的部分，如图9所示。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oh"><img src="../Images/771cbf368ef64dc52efe7e59c2791065.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XgOZkHKPdywc8TrXgsEftQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图9:网络关注每个单词(红色)的信息的可视化</p></figure><p id="371c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">变形金刚网络是依赖注意机制的模型，没有递归。它们的结构支持独立计算，使GPU并行化变得容易。变压器是自然语言处理任务的SOTA模型。感兴趣的读者可以参考Peter Bloem的<a class="ae ky" href="http://peterbloem.nl/blog/transformers" rel="noopener ugc nofollow" target="_blank">博客</a>和Jay Alammar的<a class="ae ky" href="https://jalammar.github.io/illustrated-transformer/" rel="noopener ugc nofollow" target="_blank">博客</a>。</p></div><div class="ab cl ne nf hx ng" role="separator"><span class="nh bw bk ni nj nk"/><span class="nh bw bk ni nj nk"/><span class="nh bw bk ni nj"/></div><div class="im in io ip iq"><h2 id="c49f" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated">结束语</h2><p id="05c2" class="pw-post-body-paragraph kz la it lb b lc mq ju le lf mr jx lh li nl lk ll lm nm lo lp lq nn ls lt lu im bi translated">随着GPU的改进，神经网络结构在数据科学中将变得越来越重要。跟上关于它们的文献几乎是不可能的，但在进一步研究之前，作为起点，了解它们在高水平上是如何工作的总是有用的。</p><p id="acbe" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我将更新这篇文章，提供更多关于其他结构的信息(如图形神经网络、层次神经网络等…)，敬请关注！</p></div><div class="ab cl ne nf hx ng" role="separator"><span class="nh bw bk ni nj nk"/><span class="nh bw bk ni nj nk"/><span class="nh bw bk ni nj"/></div><div class="im in io ip iq"><p id="e0bf" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="ns">所有图片均由作者提供，除非另有说明。</em></p></div></div>    
</body>
</html>