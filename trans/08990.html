<html>
<head>
<title>GLMs Part III: Deep Neural Networks as Recursive Generalized Linear Models</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">GLMs第三部分:作为递归广义线性模型的深度神经网络</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/glms-part-iii-deep-neural-networks-as-recursive-generalized-linear-models-ccb02817c9b5?source=collection_archive---------12-----------------------#2021-08-19">https://towardsdatascience.com/glms-part-iii-deep-neural-networks-as-recursive-generalized-linear-models-ccb02817c9b5?source=collection_archive---------12-----------------------#2021-08-19</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="430e" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsdatascience.com/tagged/hands-on-tutorials" rel="noopener" target="_blank">实践教程</a></h2><div class=""/><div class=""><h2 id="1639" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">数学推导和计算机模拟</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/67b405ddd6c514fc1329a98bf169bc10.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*MrsH9QjPp17E2nEB"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated"><a class="ae lh" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上<a class="ae lh" href="https://unsplash.com/@altumcode?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> AltumCode </a>的照片</p></figure><h1 id="f9ce" class="li lj it bd lk ll lm ln lo lp lq lr ls ki lt kj lu kl lv km lw ko lx kp ly lz bi translated">1:简介</h1><p id="038f" class="pw-post-body-paragraph ma mb it mc b md me kd mf mg mh kg mi mj mk ml mm mn mo mp mq mr ms mt mu mv im bi translated">广义线性模型(GLMs)在包括统计学、数据科学、机器学习和其他计算科学在内的领域中起着关键作用。</p><p id="f361" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">本系列的第一部分提供了一个全面的数学概述，用规范和非规范的形式证明了常见的GLMs。第二部分提供了GLMs常用迭代数值拟合程序的历史和数学背景，包括牛顿-拉夫森法、费希尔评分法、迭代加权最小二乘法和梯度下降法。</p><ul class=""><li id="c86c" class="nb nc it mc b md mw mg mx mj nd mn ne mr nf mv ng nh ni nj bi translated"><a class="ae lh" rel="noopener" target="_blank" href="/generalized-linear-models-a-rigorous-mathematical-formulation-58ac2ec7d9ea"> GLMs第一部分:严格的数学公式</a></li><li id="eaf1" class="nb nc it mc b md nk mg nl mj nm mn nn mr no mv ng nh ni nj bi translated"><a class="ae lh" rel="noopener" target="_blank" href="/glms-part-ii-newton-raphson-fisher-scoring-iteratively-reweighted-least-squares-irls-a-1a1e2911047"> GLMs第二部分:牛顿-拉夫森，费希尔评分，&amp;迭代加权最小二乘法(IRLS)——严格概述</a></li></ul><p id="9310" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">在这个三部分系列的最后一部分，我们探索神经网络及其与GLMs的联系。事实上，神经网络只不过是递归的规范glm。伴随这一块是一个完全可用的计算机模拟。</p><p id="5c8c" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">这篇文章的目录如下:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi np"><img src="../Images/ee7bc4981a9ad19916de65cbfcb1e0a5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BlESPAjXWywgJkJvu_A4gg.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">作者图片</p></figure><p id="2786" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">让我们跳进来吧！</p><h1 id="f479" class="li lj it bd lk ll lm ln lo lp lq lr ls ki lt kj lu kl lv km lw ko lx kp ly lz bi translated">2:激励神经网络</h1><p id="a603" class="pw-post-body-paragraph ma mb it mc b md me kd mf mg mh kg mi mj mk ml mm mn mo mp mq mr ms mt mu mv im bi translated">在本系列的前两篇文章中，我们重点讨论了GLMs的数学规范和恢复模型参数经验估计的拟合过程。然而，在这两篇文章中，我们都假设我们对所拟合的GLMs的参数模型规格有很强的信心。例如，假设我们有一个特性-结果关系，其中有10个特性的结果是伯努利分布的。我们可以拟合指定11个系数(1个截距项，10个特征中的每一个都有一个系数)的逻辑回归模型，而不需要特征或联合相互作用项的任何变换。只要问题的模型说明是正确的，逻辑回归模型将只返回“正确的”结果和推论。这些事实上是非常严格的参数假设。</p><p id="3d3f" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">然而，现在让我们假设我们不愿意对我们对模型规范的理解做出如此严格的参数假设。事实上，让我们假设我们根本不愿意对模型规范做任何假设。我们只是举手高呼“我们不知道模型规范是什么，我们也不愿意对它做任何假设！”。我们希望使用一种方法来大大放松对上述假设的需求。</p><p id="b735" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">有一类被称为“通用逼近器”的方法，神经网络属于其中。仅用一个隐藏层和非线性sigmoid激活函数，神经网络(理论上)就能够一致地估计任何任意模型规格。这是在某些用例中广泛采用神经网络的主要动机。</p><p id="ca8f" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">在进入神经网络背后的数学之前，让我们先回顾一下GLMs的数学知识。</p><h1 id="9bdf" class="li lj it bd lk ll lm ln lo lp lq lr ls ki lt kj lu kl lv km lw ko lx kp ly lz bi translated">3: GLM复习</h1><p id="e13e" class="pw-post-body-paragraph ma mb it mc b md me kd mf mg mh kg mi mj mk ml mm mn mo mp mq mr ms mt mu mv im bi translated">回想一下，广义线性模型(GLM)有三个主要组成部分:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi nq"><img src="../Images/40691be99204ef5137f53e4aa8a68b37.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JCM6lX2wqzyqACg_819bKg.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">作者图片</p></figure><p id="2dc5" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">现在让我们来研究一下标准和非标准形式的指数离差分布族的概率密度函数(PDF)的参数化。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi nr"><img src="../Images/c3f44fac3ae92d38b96a86e178db655b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hPdCNjgIUHCZ7s_N21a4Ig.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">作者图片</p></figure><p id="6a4b" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">回想一下，对于规范GLMs，我们有以下属性:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ns"><img src="../Images/83677fbf6bbd2493e2b4b063daec94eb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZZWD4Qs9UT5Phc5eBEq87A.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">作者图片</p></figure><p id="a799" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated"><em class="nt">(以上工程量的证明和推导见本GLM系列第一、二部分</em><em class="nt"/><a class="ae lh" rel="noopener" target="_blank" href="/generalized-linear-models-a-rigorous-mathematical-formulation-58ac2ec7d9ea"><em class="nt">)</em></a></p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi nu"><img src="../Images/180d13bb0b08c3b4de0fb09cd36fcc72.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CzKk1L6AidlRSiWdjHjdEg.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">作者图片</p></figure><h1 id="2334" class="li lj it bd lk ll lm ln lo lp lq lr ls ki lt kj lu kl lv km lw ko lx kp ly lz bi translated">4:跳到神经网络</h1><p id="2b8a" class="pw-post-body-paragraph ma mb it mc b md me kd mf mg mh kg mi mj mk ml mm mn mo mp mq mr ms mt mu mv im bi translated">假设我们有一个统计模型:</p><ul class=""><li id="4437" class="nb nc it mc b md mw mg mx mj nd mn ne mr nf mv ng nh ni nj bi translated">我们不是指定一个GLM，而是指定几个相互并行的glm。这些GLMs的平行分组我们称之为“层”。</li><li id="6eca" class="nb nc it mc b md nk mg nl mj nm mn nn mr no mv ng nh ni nj bi translated">模型不是由一个“层”组成，而是有几个层。前一层GLMs的输出成为下一层GLMs的输入特征。</li></ul><p id="da08" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">上述模型实际上是标准的前馈神经网络。我们将网络的第一层称为“输入层”，最后一层称为“输出层”，中间的所有层称为“隐藏层”。</p><p id="db22" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">让我们假设一个前馈神经网络的例子，在输入和输出层之间有两个隐藏层。下图显示了这种网络的常见图示。每个“节点”(即圆)代表一个变量。请注意，下图并未精确显示每层中存在的节点数量。同样，节点的数量将对应于所述层中“特征”的数量。每个箭头对应一个估计的“权重”或系数。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi nv"><img src="../Images/7a49c6a5b727462b9abb269cc766c8df.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wI5UOfW9rc7hEcJTIQp3hw.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">作者图片</p></figure><p id="c2d6" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">…让我们用数学方法指定每层的输入和输出，以及它们的尺寸:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi nw"><img src="../Images/a84580da5fd3e086c78683892487b762.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*v3Pd8NUNnNdNmv7lRsG24w.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">作者图片</p></figure><p id="5dd3" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">如果我们将每一层都想象成一个局部GLM，我们可能会对拟合哪种类型的GLMs感兴趣？换句话说，我们可能对利用哪些链接功能/激活功能感兴趣？下表列出了神经网络层中使用的常见GLM类型。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi nx"><img src="../Images/26de1b34c947b24609c92910a89961d0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Xx-GJVYCCM0Bq-bN9k8Znw.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">作者图片</p></figure><p id="e122" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">现在让我们来讨论如何将神经网络与数据相匹配。</p><h1 id="861c" class="li lj it bd lk ll lm ln lo lp lq lr ls ki lt kj lu kl lv km lw ko lx kp ly lz bi translated">5:拟合神经网络—带反向传播的梯度下降</h1><p id="262c" class="pw-post-body-paragraph ma mb it mc b md me kd mf mg mh kg mi mj mk ml mm mn mo mp mq mr ms mt mu mv im bi translated">在本系列的第二部分中，我们深入研究了将数据拟合到GLMs的迭代数值技术。出于计算效率的考虑，标准GLMs通常在实践中适合利用牛顿-拉夫森或费希尔评分。然而，神经网络最适合利用梯度下降。我们需要在神经网络的每一层中恢复参数估计。给定网络各层的固有依赖结构，利用梯度下降作为我们选择的迭代数值拟合过程，在数值上将问题简化为动态规划算法(正如我们很快将看到的)。回想一下，对于梯度下降，在迭代步骤“<em class="nt"> t+1 </em>”求解层“<em class="nt"> k </em>中的权重如下:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ny"><img src="../Images/d106c5ff63dee8a04f89517dafc50528.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*F9MfXthdkvr_FVTbyHLgDA.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">作者图片</p></figure><p id="7e5c" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">根据以上梯度下降算法的说明，我们需要恢复关于我们的神经网络中每个β系数的<strong class="mc jd">的成本函数的第一梯度。从技术上讲，我们可以用数学方法求解并恢复<strong class="mc jd"> <em class="nt">中任何我们喜欢的</em> </strong>顺序的梯度。然而，仔细检查我们的神经网络，如果我们以“巧妙”的顺序求解梯度，我们可以降低所需的总计算量。可以容易地估计外层中权重的梯度(类似于如何在单个GLM中估计系数)。倒数第二层中权重的梯度是最后一层中梯度的函数。倒数第三层中的梯度是倒数第二层中梯度的函数，以此类推。因此，如果我们求解最后一层中的梯度，并通过网络逐层求解梯度，这种梯度求解练习简化为一般的动态规划问题。</strong></p><p id="9d49" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">在人工智能领域和神经网络文献中，动态编程的这个特殊实例被赋予了自己的特殊名称。我们称之为<strong class="mc jd"> <em class="nt">反向传播</em> </strong>。随着梯度下降的每一次迭代，我们使用从前到后通过网络向前传播的来自步骤“<em class="nt"> t </em>的权重的当前经验值(即<strong class="mc jd"> <em class="nt">前向传播</em> </strong>)来计算每一层的变量的值，然后通过从后到前的网络工作来更新步骤“<em class="nt"> t+1 </em>的权重。</p><p id="fabd" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">让我们用数学方法推导出具有两个隐藏层的玩具问题的梯度:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi nz"><img src="../Images/4d928170ea9483d6da0a478aaeb0c497.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QphtL4fZ6L3K-AyAcL_gMg.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">作者图片</p></figure><p id="3669" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">…因此，我们对步骤“<em class="nt"> t+1 </em>”的权重更新为三层中的权重:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi oa"><img src="../Images/ac75c376f9b6b5f19939556eb0d716df.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CMy9YDe8kTtlOcxSoWg4ow.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">作者图片</p></figure><h1 id="7b91" class="li lj it bd lk ll lm ln lo lp lq lr ls ki lt kj lu kl lv km lw ko lx kp ly lz bi translated">6:计算模拟</h1><p id="5f46" class="pw-post-body-paragraph ma mb it mc b md me kd mf mg mh kg mi mj mk ml mm mn mo mp mq mr ms mt mu mv im bi translated">下面是一个用玩具数据集拟合前馈神经网络的完整工作示例(从头开始构建)。在下面的代码中，神经网络类对象允许下列可调参数:</p><ul class=""><li id="8d09" class="nb nc it mc b md mw mg mx mj nd mn ne mr nf mv ng nh ni nj bi translated">隐藏层数</li><li id="8735" class="nb nc it mc b md nk mg nl mj nm mn nn mr no mv ng nh ni nj bi translated">每个隐藏层中的节点数</li><li id="8e73" class="nb nc it mc b md nk mg nl mj nm mn nn mr no mv ng nh ni nj bi translated">每个隐藏层的激活函数</li><li id="07a8" class="nb nc it mc b md nk mg nl mj nm mn nn mr no mv ng nh ni nj bi translated">小批量</li><li id="8ca5" class="nb nc it mc b md nk mg nl mj nm mn nn mr no mv ng nh ni nj bi translated">节点丢失正则化</li></ul><p id="0003" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">让我们导入我们需要的库:</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="ob oc l"/></div></figure><p id="befe" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">…以及用于指定具有伯努利分布结果的玩具数据集的函数:</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="ob oc l"/></div></figure><p id="4cfd" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">…以及神经网络模型的函数和类对象:</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="ob oc l"/></div></figure><p id="2720" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">现在，让我们在玩具数据集上拟合神经网络模型，计算结果，并返回曲线下面积(AUC)分数</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="ob oc l"/></div></figure><p id="c3fe" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">从上面可以看出，拟合神经网络模型的AUC约为80%</p><p id="72ad" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">有关上述计算模拟的完整代码/笔记本，请参见下面的<a class="ae lh" href="https://github.com/atrothman/GLM_Neural_Network" rel="noopener ugc nofollow" target="_blank"> <strong class="mc jd"> github链接</strong> </a>。</p><h1 id="f0ad" class="li lj it bd lk ll lm ln lo lp lq lr ls ki lt kj lu kl lv km lw ko lx kp ly lz bi translated">7:总结和结论</h1><p id="4868" class="pw-post-body-paragraph ma mb it mc b md me kd mf mg mh kg mi mj mk ml mm mn mo mp mq mr ms mt mu mv im bi translated">在本系列的第一部分中，我们提供了规范和非规范形式的常见GLMs的全面的数学概述(带有证明)。在第二部分中，我们提供了对典型和非典型GLMs的三个重要的迭代数值拟合过程的严格概述:牛顿-拉夫森，费希尔评分和迭代加权最小二乘法(IRLS)。在第三篇也是最后一篇文章中，我们介绍了标准的神经网络模型，以及如何将它们表述为递归GLMs。</p><p id="0ad7" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">希望以上有见地。正如我在以前的一些文章中提到的，我认为没有足够的人花时间去做这些类型的练习。对我来说，这种基于理论的洞察力让我在实践中更容易使用方法。我个人的目标是鼓励该领域的其他人采取类似的方法。我打算在未来写一些基础作品，所以请订阅<a class="ae lh" href="https://anr248.medium.com/" rel="noopener"> <strong class="mc jd">并在媒体</strong> </a>和<a class="ae lh" href="http://www.linkedin.com/in/andrew-rothman-49739630" rel="noopener ugc nofollow" target="_blank"> <strong class="mc jd"> LinkedIn </strong> </a>上关注我的更新！</p></div></div>    
</body>
</html>