<html>
<head>
<title>Can Deep Reinforcement Learning Solve Chess?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">深度强化学习能解决象棋吗？</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/can-deep-reinforcement-learning-solve-chess-b9f52855cd1e?source=collection_archive---------11-----------------------#2021-08-23">https://towardsdatascience.com/can-deep-reinforcement-learning-solve-chess-b9f52855cd1e?source=collection_archive---------11-----------------------#2021-08-23</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="d8fd" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph">实践教程</h2><div class=""/><div class=""><h2 id="5479" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">经典和深度强化学习速成班，用Python实现</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/ff24d400ed34fcfd896f116708ebee71.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-vW5k0LO2VHoX8iKGVCJOw.jpeg"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">照片由来自<a class="ae lh" href="https://www.pexels.com/photo/robot-pointing-on-a-wall-8386440/?utm_content=attributionCopyText&amp;utm_medium=referral&amp;utm_source=pexels" rel="noopener ugc nofollow" target="_blank"> Pexels </a>的<a class="ae lh" href="https://www.pexels.com/@tara-winstead?utm_content=attributionCopyText&amp;utm_medium=referral&amp;utm_source=pexels" rel="noopener ugc nofollow" target="_blank"> Tara Winstead </a>拍摄</p></figure><h1 id="3214" class="li lj it bd lk ll lm ln lo lp lq lr ls ki lt kj lu kl lv km lw ko lx kp ly lz bi translated">目录:</h1><ul class=""><li id="cbb9" class="ma mb it mc b md me mf mg mh mi mj mk ml mm mn mo mp mq mr bi translated"><strong class="mc jd">DRL简介</strong></li><li id="0c2e" class="ma mb it mc b md ms mf mt mh mu mj mv ml mw mn mo mp mq mr bi translated"><strong class="mc jd">实现DRL </strong></li><li id="365a" class="ma mb it mc b md ms mf mt mh mu mj mv ml mw mn mo mp mq mr bi translated"><strong class="mc jd">分析结果</strong></li><li id="1c11" class="ma mb it mc b md ms mf mt mh mu mj mv ml mw mn mo mp mq mr bi translated"><strong class="mc jd">结论</strong></li></ul><p id="d5e0" class="pw-post-body-paragraph mx my it mc b md mz kd na mf nb kg nc mh nd ne nf mj ng nh ni ml nj nk nl mn im bi translated"><strong class="mc jd"> <em class="nm">注意:所有代码都是片段形式，单独执行时无法工作。完整代码可以在我的</em></strong><a class="ae lh" href="https://github.com/victorsimrbt/chess_rl" rel="noopener ugc nofollow" target="_blank"><strong class="mc jd"><em class="nm">Github repo</em></strong></a><strong class="mc jd"><em class="nm">上找到。</em>T25】</strong></p><h1 id="6fc7" class="li lj it bd lk ll lm ln lo lp lq lr ls ki lt kj lu kl lv km lw ko lx kp ly lz bi translated">深度强化学习简介:</h1><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/929a3b637a36d1edd3b2b99f9b8089ce.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VYoJpA4elFHEk2E4XSqezQ.png"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">作者提供的图片:显示环境的高级定义的流程图</p></figure><p id="41bd" class="pw-post-body-paragraph mx my it mc b md mz kd na mf nb kg nc mh nd ne nf mj ng nh ni ml nj nk nl mn im bi translated">强化学习是对一个<strong class="mc jd"> <em class="nm">智能体</em> </strong>在一个<strong class="mc jd"> <em class="nm">环境</em> </strong>中进行决策的训练。代理部署在环境中。在任何给定的帧，代理必须使用来自环境的数据来行动。这个动作会产生一个<strong class="mc jd"> <em class="nm">奖励</em> </strong>值，它代表了<strong class="mc jd"> <em class="nm">动作</em> </strong>的质量。然后，代理将相应地更新其流程，以最大化此奖励。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi no"><img src="../Images/ff81831d7f7b5ca2e293a041df812757.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GHpt1uhtJNzLvNJvhzLnqQ.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">作者图片:Q表示例</p></figure><p id="dec0" class="pw-post-body-paragraph mx my it mc b md mz kd na mf nb kg nc mh nd ne nf mj ng nh ni ml nj nk nl mn im bi translated">一个基本的强化学习算法是Q学习。在Q学习中，会创建一个Q表。Q表的行将代表环境的可能状态，而列将是环境中的行动所产生的回报。代理将与环境交互，并在Q表中记录奖励值。最终，代理会填写表格。然后，当面对环境时，代理可以选择具有最高奖励值的行动。</p><p id="9e9c" class="pw-post-body-paragraph mx my it mc b md mz kd na mf nb kg nc mh nd ne nf mj ng nh ni ml nj nk nl mn im bi translated">尽管该算法在某些情况下确保了完美的策略，但是这些算法的实际应用很少。复杂的环境通常不会对每一步都产生直接的回报，因此在填写Q表时会产生问题。</p><p id="fea7" class="pw-post-body-paragraph mx my it mc b md mz kd na mf nb kg nc mh nd ne nf mj ng nh ni ml nj nk nl mn im bi translated">环境中有天文数字般多的状态是很常见的。国际象棋估计有大约10个⁰可能的位置。由于一个位置的平均移动次数是30次，国际象棋的Q表将会太大而无法存储，更不用说完成了。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi np"><img src="../Images/c9ed7c139eef0643f7cf915a5c0a9766.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NOUb5aFMH5L97V3LghdIWA.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">作者图片</p></figure><p id="944e" class="pw-post-body-paragraph mx my it mc b md mz kd na mf nb kg nc mh nd ne nf mj ng nh ni ml nj nk nl mn im bi translated">深度强化学习是深度学习在强化学习中的应用。虽然深度学习可以以许多不同的方式应用，但它通常涉及训练深度神经网络，以逼近它尚未看到的状态的q值。</p><p id="822c" class="pw-post-body-paragraph mx my it mc b md mz kd na mf nb kg nc mh nd ne nf mj ng nh ni ml nj nk nl mn im bi translated">然而，需要做出许多假设以允许深度强化算法工作。以反向传播奖励为例。我们知道，导致获胜位置的移动一定有利于最终位置，但我们如何评估它对将死的重要性？我们只能假设越接近胜利位置的移动对胜利有越大的利害关系，所以我们对较早移动的奖励打折扣。</p><h1 id="449e" class="li lj it bd lk ll lm ln lo lp lq lr ls ki lt kj lu kl lv km lw ko lx kp ly lz bi translated">实施深度强化学习:</h1><p id="910b" class="pw-post-body-paragraph mx my it mc b md me kd na mf mg kg nc mh nq ne nf mj nr nh ni ml ns nk nl mn im bi translated">这个<a class="ae lh" href="https://github.com/victorsimrbt/chess_rl/tree/main/v2.4.0" rel="noopener ugc nofollow" target="_blank"> <strong class="mc jd">链接</strong> </a>将把你传送到包含我的实现的完整代码的仓库的子部分。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi nt"><img src="../Images/2eb7dc31a4c2aac79e4a440bbb86fcdd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pP9MxISdbX_ZkdiIRH0ygA.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">作者图片</p></figure><p id="e06d" class="pw-post-body-paragraph mx my it mc b md mz kd na mf nb kg nc mh nd ne nf mj ng nh ni ml nj nk nl mn im bi translated">这个图是程序的主要部分之间的可视化表示，主要是环境、模型和目标模型。</p><p id="091a" class="pw-post-body-paragraph mx my it mc b md mz kd na mf nb kg nc mh nd ne nf mj ng nh ni ml nj nk nl mn im bi translated">环境给模型一个状态向量，模型输出一个动作。模型向模型返回一个动作。更新的Q模型然后将更新的Q值返回给训练函数，训练函数改变模型的权重。每隔一段时间，来自目标模型的权重被设置为模型权重。</p><h1 id="cd04" class="li lj it bd lk ll lm ln lo lp lq lr ls ki lt kj lu kl lv km lw ko lx kp ly lz bi translated">环境:</h1><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="nu nv l"/></div></figure><p id="907b" class="pw-post-body-paragraph mx my it mc b md mz kd na mf nb kg nc mh nd ne nf mj ng nh ni ml nj nk nl mn im bi translated">这是描述模型与之交互的象棋环境的代码的设置。我们已经知道它必须执行的主要功能:</p><p id="1445" class="pw-post-body-paragraph mx my it mc b md mz kd na mf nb kg nc mh nd ne nf mj ng nh ni ml nj nk nl mn im bi translated">1.返回状态</p><p id="343a" class="pw-post-body-paragraph mx my it mc b md mz kd na mf nb kg nc mh nd ne nf mj ng nh ni ml nj nk nl mn im bi translated">这个函数简单地返回一个向量，该向量完全表达了代理环境可访问的信息，可以插入到神经网络中。</p><p id="549b" class="pw-post-body-paragraph mx my it mc b md mz kd na mf nb kg nc mh nd ne nf mj ng nh ni ml nj nk nl mn im bi translated">2.接受行动</p><p id="f4cc" class="pw-post-body-paragraph mx my it mc b md mz kd na mf nb kg nc mh nd ne nf mj ng nh ni ml nj nk nl mn im bi translated">这个函数接受一个动作，并通过这个动作改变环境的状态。</p><p id="2d46" class="pw-post-body-paragraph mx my it mc b md mz kd na mf nb kg nc mh nd ne nf mj ng nh ni ml nj nk nl mn im bi translated">3.回报奖励</p><p id="d865" class="pw-post-body-paragraph mx my it mc b md mz kd na mf nb kg nc mh nd ne nf mj ng nh ni ml nj nk nl mn im bi translated">应该有一个适当的系统，允许代理直接从每个行动中获取奖励。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="nu nv l"/></div></figure><h2 id="eb0a" class="nw lj it bd lk nx ny dn lo nz oa dp ls mh ob oc lu mj od oe lw ml of og ly iz bi translated">返回状态:</h2><p id="39cc" class="pw-post-body-paragraph mx my it mc b md me kd na mf mg kg nc mh nq ne nf mj nr nh ni ml ns nk nl mn im bi translated">这个要点展示了将python-chess board对象转换成矩阵的函数。它将棋盘转换为行和列，并找到棋盘上的每个方块。</p><p id="a783" class="pw-post-body-paragraph mx my it mc b md mz kd na mf nb kg nc mh nd ne nf mj ng nh ni ml nj nk nl mn im bi translated">然后，它对所有变量进行一次性编码，使其成为形状为8，8，12的数组。该函数与国际象棋环境相关，因此当调用该函数时，类中国际象棋棋盘的变化将会反映出来。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="nu nv l"/></div></figure><h2 id="2791" class="nw lj it bd lk nx ny dn lo nz oa dp ls mh ob oc lu mj od oe lw ml of og ly iz bi translated">接受行动+回报奖励:</h2><p id="9a0e" class="pw-post-body-paragraph mx my it mc b md me kd na mf mg kg nc mh nq ne nf mj nr nh ni ml ns nk nl mn im bi translated">这个要点描述了阶跃函数，它在接受一个动作时推进环境。</p><p id="a01b" class="pw-post-body-paragraph mx my it mc b md mz kd na mf nb kg nc mh nd ne nf mj ng nh ni ml nj nk nl mn im bi translated">虽然播放动作只需要一行代码，但该函数还必须记录所有关于状态的数据和动作的回报，以便对Deep-Q算法进行训练。</p><p id="de11" class="pw-post-body-paragraph mx my it mc b md mz kd na mf nb kg nc mh nd ne nf mj ng nh ni ml nj nk nl mn im bi translated">值得注意的是，我们记录奖励、状态、未来状态以及环境事件是否结束。</p><h1 id="7c1a" class="li lj it bd lk ll lm ln lo lp lq lr ls ki lt kj lu kl lv km lw ko lx kp ly lz bi translated">更新Q值:</h1><p id="1198" class="pw-post-body-paragraph mx my it mc b md me kd na mf mg kg nc mh nq ne nf mj nr nh ni ml ns nk nl mn im bi translated">事实上，我还编写了函数来更新环境中的q_values，因为从函数内部访问这些值要容易得多。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="oh nv l"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">作者图片</p></figure><p id="d621" class="pw-post-body-paragraph mx my it mc b md mz kd na mf nb kg nc mh nd ne nf mj ng nh ni ml nj nk nl mn im bi translated">这是用于评估更新后的-q值的公式。更新的q值等于在状态<em class="nm"> s </em>下执行动作<em class="nm"> a </em>获得的奖励加上未来奖励的最大值，乘以超参数γ。超参数γ可以被认为是未来回报对实施环境的重要性的度量。</p><h1 id="bb25" class="li lj it bd lk ll lm ln lo lp lq lr ls ki lt kj lu kl lv km lw ko lx kp ly lz bi translated">型号:</h1><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="nu nv l"/></div></figure><p id="545e" class="pw-post-body-paragraph mx my it mc b md mz kd na mf nb kg nc mh nd ne nf mj ng nh ni ml nj nk nl mn im bi translated">左边的要点描述了Q_model类。它包含三个功能，允许它与环境互动。</p><p id="c54d" class="pw-post-body-paragraph mx my it mc b md mz kd na mf nb kg nc mh nd ne nf mj ng nh ni ml nj nk nl mn im bi translated">为DQN创建的模型是一个简单的卷积网络，具有3个带有relu激活功能的卷积层。最后一层包含4096个神经元，代表可以在任何给定位置进行的4096种可能的移动(64个用于开始方块，64个用于结束方块，得到4096)。</p><p id="6160" class="pw-post-body-paragraph mx my it mc b md mz kd na mf nb kg nc mh nd ne nf mj ng nh ni ml nj nk nl mn im bi translated">预测函数将棋盘转换为张量，并调用模型来预测输入的输出。它输出总和为1的概率分布。我们假设每个值映射到一个移动，较大的概率表示模型的置信度。然后，它应用一个掩码，从分布中删除所有非法移动。然后，通过预定义的字典，该移动被转换为国际象棋移动。</p><p id="79d6" class="pw-post-body-paragraph mx my it mc b md mz kd na mf nb kg nc mh nd ne nf mj ng nh ni ml nj nk nl mn im bi translated">在强化学习中，模型需要探索环境以收集更多的洞察力。探索功能选择随机的合法移动来玩。这允许打破局部最小值的可能性，但是会在开始时减慢训练进度。</p><h1 id="dce4" class="li lj it bd lk ll lm ln lo lp lq lr ls ki lt kj lu kl lv km lw ko lx kp ly lz bi translated">执行剧集:</h1><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="nu nv l"/></div></figure><p id="aceb" class="pw-post-body-paragraph mx my it mc b md mz kd na mf nb kg nc mh nd ne nf mj ng nh ni ml nj nk nl mn im bi translated">这个github gist完全执行算法。它建立在我前面解释过的q-network和chess_env模块之上。</p><p id="a68d" class="pw-post-body-paragraph mx my it mc b md mz kd na mf nb kg nc mh nd ne nf mj ng nh ni ml nj nk nl mn im bi translated">它还需要一个variable_settings文件，其中包含算法的所有超参数，如ε和ε衰减率。它还包括损失函数(Huber损失)和项目的优化器设置。将文件分开会使配置变得更加容易。</p><p id="d5f5" class="pw-post-body-paragraph mx my it mc b md mz kd na mf nb kg nc mh nd ne nf mj ng nh ni ml nj nk nl mn im bi translated">ε用于在勘探和开发之间进行选择。它以变量设置文件中定义的速率衰减。</p><p id="3519" class="pw-post-body-paragraph mx my it mc b md mz kd na mf nb kg nc mh nd ne nf mj ng nh ni ml nj nk nl mn im bi translated">我没有仅仅使用model.fit()，而是使用tensorflow渐变带特性来更新模型的渐变。</p><p id="f3a3" class="pw-post-body-paragraph mx my it mc b md mz kd na mf nb kg nc mh nd ne nf mj ng nh ni ml nj nk nl mn im bi translated">经过一定数量的时期后，目标模型将配置主模型的权重。</p><h1 id="24cb" class="li lj it bd lk ll lm ln lo lp lq lr ls ki lt kj lu kl lv km lw ko lx kp ly lz bi translated">结果:</h1><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi oi"><img src="../Images/924fd72835e6b5d66780f10c4cc26071.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yKQTUZQHlmaL6VSuVOqKhQ.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">作者图片</p></figure><p id="3f46" class="pw-post-body-paragraph mx my it mc b md mz kd na mf nb kg nc mh nd ne nf mj ng nh ni ml nj nk nl mn im bi translated">从这张截图可以看出，深度强化学习无法正常进行。我最初对结果非常失望和沮丧，并尝试了许多不同的方法来改进算法。</p><p id="b1d5" class="pw-post-body-paragraph mx my it mc b md mz kd na mf nb kg nc mh nd ne nf mj ng nh ni ml nj nk nl mn im bi translated">但是，在网上看了别人的证词后，我终于找到了我问题的根源:稳定。</p><p id="0cbb" class="pw-post-body-paragraph mx my it mc b md mz kd na mf nb kg nc mh nd ne nf mj ng nh ni ml nj nk nl mn im bi translated">强化学习中的自我游戏是出了名的不稳定，因为模型的回报不是游戏质量的决定性衡量标准。</p><p id="37af" class="pw-post-body-paragraph mx my it mc b md mz kd na mf nb kg nc mh nd ne nf mj ng nh ni ml nj nk nl mn im bi translated">由于两个模型是相同的，这将导致非常弱的播放，并可能导致模型的播放质量停滞在一个非常低的水平。</p><h1 id="fca6" class="li lj it bd lk ll lm ln lo lp lq lr ls ki lt kj lu kl lv km lw ko lx kp ly lz bi translated">深度强化学习能解决象棋吗？</h1><p id="95d5" class="pw-post-body-paragraph mx my it mc b md me kd na mf mg kg nc mh nq ne nf mj nr nh ni ml ns nk nl mn im bi translated">这篇文章提出的问题的答案不是一个明确的否定，而是一个可以用来解决稳定问题的解决方案列表。</p><h2 id="4910" class="nw lj it bd lk nx ny dn lo nz oa dp ls mh ob oc lu mj od oe lw ml of og ly iz bi translated">1.稳定的对手:</h2><p id="58e4" class="pw-post-body-paragraph mx my it mc b md me kd na mf mg kg nc mh nq ne nf mj nr nh ni ml ns nk nl mn im bi translated">如果我们实现一个稳定的对手，比如stockfish，我们可能会用我们的算法得到很好的结果！</p><h2 id="0073" class="nw lj it bd lk nx ny dn lo nz oa dp ls mh ob oc lu mj od oe lw ml of og ly iz bi translated">2.稳定算法:</h2><p id="1b42" class="pw-post-body-paragraph mx my it mc b md me kd na mf mg kg nc mh nq ne nf mj nr nh ni ml ns nk nl mn im bi translated">我们可以使用不同的算法来获得更好的结果，只要我们在新算法中有一些稳定方法。因为我对这个项目的结果非常不满意，所以我从头开始实现了Google DeepMind的算法。我将很快发布一篇关于结果的文章。</p></div></div>    
</body>
</html>