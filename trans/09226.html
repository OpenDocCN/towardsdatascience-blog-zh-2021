<html>
<head>
<title>Evaluating adversarial examples with similarity metrics in Python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用Python中的相似性度量评估对立实例</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/evaluating-adversarial-examples-with-similarity-metrics-in-python-13acb9b5fa9f?source=collection_archive---------21-----------------------#2021-08-26">https://towardsdatascience.com/evaluating-adversarial-examples-with-similarity-metrics-in-python-13acb9b5fa9f?source=collection_archive---------21-----------------------#2021-08-26</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="1680" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">使用相似性度量来查看哪些攻击对图像的改变最大</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/7f63e41b4a00618da6c7b36d4c94e920.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*gq3LR0UFUj0FQtjh"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">乔恩·泰森在<a class="ae kv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="a739" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">对于分类神经网络，一个相反的例子是一个输入图像被扰乱(或有策略地修改),使得它被故意错误地分类。有各种算法利用给定分类模型的内部工作(梯度和特征图),并修改输入图像，使其要么只是误分类(无目标攻击),要么总是误分类到特定类别(有目标攻击)。</p><p id="106d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在本文中，我们将研究一些白盒攻击(在了解模型的内部工作原理后生成攻击的算法)，并使用相似性度量来指出其中一些攻击的鲁棒性增强。</p><p id="9ed9" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">攻击:</p><ul class=""><li id="b7f2" class="ls lt iq ky b kz la lc ld lf lu lj lv ln lw lr lx ly lz ma bi translated"><a class="ae kv" href="https://arxiv.org/abs/1412.6572" rel="noopener ugc nofollow" target="_blank"> <em class="mb">快速渐变标志法</em> </a> <em class="mb"> (FGSM) </em></li><li id="0f12" class="ls lt iq ky b kz mc lc md lf me lj mf ln mg lr lx ly lz ma bi translated"><a class="ae kv" href="https://arxiv.org/abs/1706.06083" rel="noopener ugc nofollow" target="_blank"><em class="mb"/></a><em class="mb">【PGD】</em></li><li id="99bf" class="ls lt iq ky b kz mc lc md lf me lj mf ln mg lr lx ly lz ma bi translated"><a class="ae kv" href="https://arxiv.org/pdf/1511.04599.pdf" rel="noopener ugc nofollow" target="_blank">T18】deep foolT20】</a></li><li id="d22e" class="ls lt iq ky b kz mc lc md lf me lj mf ln mg lr lx ly lz ma bi translated"><a class="ae kv" href="https://arxiv.org/abs/1608.04644" rel="noopener ugc nofollow" target="_blank"> <em class="mb">卡里尼&amp;瓦格纳</em> </a> <em class="mb"> (C &amp; W) </em></li></ul><p id="7ebf" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">生成攻击</strong></p><p id="78ad" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">有几个Python库已经实现了这些(和其他)攻击算法，并且还提供了现成的模块来为我们的用例生成这些算法。<a class="ae kv" href="https://github.com/cleverhans-lab/cleverhans" rel="noopener ugc nofollow" target="_blank"> Cleverhans </a>、<a class="ae kv" href="https://github.com/bethgelab/foolbox" rel="noopener ugc nofollow" target="_blank"> FoolBox </a>和<a class="ae kv" href="https://adversarial-robustness-toolbox.readthedocs.io/en/latest/" rel="noopener ugc nofollow" target="_blank"> ART </a>是三个广泛使用并定期维护的开源库，用于对抗性示例(AE)生成。目前，我们使用艺术。</p><p id="1b1e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">按如下方式安装对抗性鲁棒性工具箱:</p><p id="ad4f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><code class="fe mh mi mj mk b">pip install adversarial-robustness-toolbox</code></p><p id="b07d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">或者参考<a class="ae kv" href="https://github.com/Trusted-AI/adversarial-robustness-toolbox" rel="noopener ugc nofollow" target="_blank">官方知识库</a>获取进一步指导。</p><p id="f5fc" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">接下来，我们可以生成如下所示的攻击:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ml mm l"/></div></figure><p id="acca" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">为了生成攻击，我们使用InceptionNet V3模型作为算法生成攻击的基础。我们使用在ImageNet数据集中找到的真实世界图像，因此也使用在ImageNet上预训练的InceptionNet V3的PyTorch模型。</p><p id="5350" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><em class="mb"> attack.generate() </em>方法的输出是一个包含扰动图像的列表，格式与输入相同(采用<em class="mb">(通道，宽度，高度)</em>格式，像素值在范围[0，1]内)。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mn"><img src="../Images/24ac28247a4f132987439242953cc171.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QPDq2F90KbVnqiIwg5uKoA.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">每次攻击的样本图像。作者图片</p></figure><p id="3292" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在上面的图像中，我们可以看到FGSM攻击造成了肉眼可见的扰动。据说这是一种相对较弱的攻击。然而CW展现的是理想的情况！没有可见的扰动，并且该攻击被证明对分类器比其余的更鲁棒。</p><p id="fa11" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">接下来，我们可以测量这些被攻击的图像与原始图像的相似性。</p><p id="b247" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">对于人眼来说，很容易分辨出两个给定图像的质量有多相似。然而，如果想要量化这种差异，我们需要数学表达式。从余弦相似性到ERGAS，有几种这样的度量标准可以用来测试图像与其原始版本相比的“质量”。</p><p id="3ad5" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">通常，当从现有图像生成新图像时(在去噪、去模糊或任何此类操作之后)，量化再生的不同程度将是有益的。</p><p id="39e9" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们可以把这个应用程序也看作我们的用例。</p><p id="3979" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们从现有的文献中知道，DeepFool和CW是稳健的攻击，欺骗分类器的成功率更高。它们也很难被检测到，因为它们对目标图像的干扰(或噪声)很小。这些点已经分别使用模型分类精度的降低和图像的视觉外观进行了评估。</p><p id="ac01" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">但是让我们尝试用这个质量指数来量化后一部分。</p><p id="e5a9" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><a class="ae kv" rel="noopener" target="_blank" href="/measuring-similarity-in-two-images-using-python-b72233eb53c6">阅读更多关于在Python中实现这些图像相似性度量的信息。</a></p><h2 id="bad9" class="mo mp iq bd mq mr ms dn mt mu mv dp mw lf mx my mz lj na nb nc ln nd ne nf ng bi translated">Python中的相似性度量</h2><p id="31e4" class="pw-post-body-paragraph kw kx iq ky b kz nh jr lb lc ni ju le lf nj lh li lj nk ll lm ln nl lp lq lr ij bi translated">我们将使用Python中的<a class="ae kv" href="https://pypi.org/project/sewar/" rel="noopener ugc nofollow" target="_blank"> <em class="mb"> sewar </em> </a>库来实现一些可用的指标。</p><p id="54c9" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">从<code class="fe mh mi mj mk b">pip install sewar </code>开始，导入所需的模块，如下所示</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ml mm l"/></div></figure><p id="31e5" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">除了这些，我们将只使用<a class="ae kv" href="https://en.wikipedia.org/wiki/Peak_signal-to-noise_ratio#:~:text=Peak%20signal%2Dto%2Dnoise%20ratio%20(PSNR)%20is%20an,the%20fidelity%20of%20its%20representation." rel="noopener ugc nofollow" target="_blank"> PSNR </a>、<a class="ae kv" href="https://www.mdpi.com/2072-4292/8/10/797/pdf/1" rel="noopener ugc nofollow" target="_blank">尔加斯</a>、<a class="ae kv" href="https://en.wikipedia.org/wiki/Structural_similarity" rel="noopener ugc nofollow" target="_blank"> SSIM </a>和<a class="ae kv" href="https://www.l3harrisgeospatial.com/docs/spectralanglemapper.html" rel="noopener ugc nofollow" target="_blank">萨姆</a>。我只选择了这几个，因为在像CW和DeepFool这样的强大攻击中，在上面列出的所有攻击中，只有这几个能够以明显的方式捕捉和放大差异。</p><p id="3f86" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">进口的<em class="mb"> sewar </em>模块可以直接使用，如</p><p id="337d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><code class="fe mh mi mj mk b">ergas_score = ergas(original, adversarial)</code></p><p id="d10e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">下面你可以看到各种攻击和各种分数的结果。显然，ERGAS和SAM比其他人更能放大不同攻击之间的差异。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nm"><img src="../Images/edbc110568691cf41e7035c63c721e9c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1314/format:webp/1*fa7ZwOrDVC2EArcd3MHPyg.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">四个有效度量的原始图像和敌对图像之间的相似性分数。图片作者。</p></figure><p id="f827" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">根据我们的假设，我们看到CW攻击图像的相似性得分大于FGSM/PGD攻击。这意味着对抗图像比其他不太复杂的攻击更类似于CW/DeepFool的原始图像。</p><p id="2054" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">请自行尝试<a class="ae kv" href="https://adversarial-robustness-toolbox.readthedocs.io/en/latest/modules/attacks/evasion.html" rel="noopener ugc nofollow" target="_blank">其他类型的攻击</a>！</p><blockquote class="nn no np"><p id="af77" class="kw kx mb ky b kz la jr lb lc ld ju le nq lg lh li nr lk ll lm ns lo lp lq lr ij bi translated">感谢您从头到尾的阅读！您可以通过LinkedIn<a class="ae kv" href="https://www.linkedin.com/in/param-raval/" rel="noopener ugc nofollow" target="_blank">联系我，获取任何信息、想法或建议。</a></p></blockquote></div></div>    
</body>
</html>