<html>
<head>
<title>Understanding neural networks and backpropagation-Part I</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">理解神经网络和反向传播-第一部分</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/understanding-neural-networks-and-backpropagation-part-i-89c2c12d72cb?source=collection_archive---------41-----------------------#2021-04-26">https://towardsdatascience.com/understanding-neural-networks-and-backpropagation-part-i-89c2c12d72cb?source=collection_archive---------41-----------------------#2021-04-26</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="5a0c" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">relu非线性神经网络的训练</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi kf"><img src="../Images/d620584333ff91ac0e2afa23f940a12b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/1*G07Cc7zT4hGOM5bsQqQLSw.gif"/></div><p class="kn ko gj gh gi kp kq bd b be z dk translated">图1:(图片由作者提供)具有四个神经元的神经网络使用反向传播来训练，以匹配具有四个斜率的参考函数。</p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="gh gi kr"><img src="../Images/afc00733c49945ff34ae61a5d7921bb3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3qZy0u9wLNaD-TMVbLOFtg.png"/></div></div><p class="kn ko gj gh gi kp kq bd b be z dk translated">图2a(图片由作者提供)(左)具有初始权重和参考函数的神经网络(右)具有最终权重和参考函数的神经网络</p></figure><p id="36c5" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">上面图1中的动画显示了使用反向传播算法训练四个神经元的神经网络。参考函数是一个形状为倒V形的图，每边都有一个弯曲。这是一个范围为[0，4]的图，有四个斜率。图2a(左)显示了具有初始权重的参考函数和神经网络。图2a(右)显示了参考函数和在7200次反向传播迭代后收敛到参考函数的神经网络。</p><p id="7751" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">本文的目的是帮助读者理解神经网络和训练中使用的反向传播算法。训练神经网络在计算上非常昂贵。反向传播算法有时会给出局部而非全局最优的解决方案。因此，对于那些部署神经网络来解决他们的问题的人来说，彻底理解该算法的基础是有用的。</p><p id="6a07" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">本文分为三个部分，涵盖以下主题:(1)概述了监督学习的基本概念和神经网络正向计算和反向传播所涉及的数学方程。(b)将这些方程转化为将重量更新描述为视觉运动的陈述。这些运动是斜率的增大或减小，或者是斜率原点的移动。(c)提出了一组使用各种神经网络的例子。这些显示了反向传播算法的操作。python程序用于计算和生成图形和动画，直观地显示计算过程，最后，给出一个注释，解释结果图形如何与(b)中所述的规则相联系。我希望可视化和解释将帮助用户更好地理解神经网络和反向传播。</p></div><div class="ab cl ls lt hu lu" role="separator"><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx"/></div><div class="ij ik il im in"><p id="f9da" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><em class="lz">监督学习</em>:监督学习使用一种叫做神经网络的特殊功能来模拟测试数据集中输入和输出值之间的关系。测试数据集由一系列输入和输出值组成。神经网络具有权重参数，其值最初是猜测的。训练过程使用数据集来比较由模型预测的输出值，并修改权重，使得预测值和实际值之间的误差减小。一旦这个训练过程完成，神经网络就被用来预测新输入的输出值。</p><p id="bbe0" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><em class="lz">神经元</em>:神经元是将n个输入向量映射到单个值的函数。它是线性和非线性操作的级联，如下所示。</p><pre class="kg kh ki kj gt ma mb mc md aw me bi"><span id="fcbf" class="mf mg iq mb b gy mh mi l mj mk">N(w, x) = nl(x0*w0 + x1*w1+...+xn*wn + b)<br/>   where w = {w0, w1, ..., wn} is the weight parameter of size n,<br/>         x = {x0, x1, ..., xn} is the input vector of size n , <br/>         b is a bias       <br/>     and nl is a non-linear function,</span></pre><p id="c276" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">线性运算是输入向量与相同大小的权重向量加上标量偏差值的点积。非线性函数是定义在单个变量上的函数。使用的典型非线性函数是relu，其被定义为relu(x) = x，其中x &gt; 0，否则为0。在本文中，我们使用大小为1的输入向量，并选择relu作为非线性函数。因此，神经元的方程式是</p><p id="fa6b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">N(x) = relu(w*x+b)其中w是权重，b是偏差。</p><p id="0455" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">如果w &gt; 0，那么当x &gt; -b/w时，N(x) &gt; 0。我们将把-b/w称为神经元的起源。</p><p id="2d1e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><em class="lz">神经网络:</em>神经网络是由神经元组成的计算图。在输入层中具有n个单输入神经元并且在输出层中具有一个n输入神经元的神经网络由以下等式指定</p><p id="c8e6" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">NN(x)= w0 * relu(iw0 * x+B0)+w1 * relu(iw1 * x+B1)+..+wn*relu(iwn*x+bn)。</p><p id="96d4" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在单输入的情况下，我们可以将输入权重设置为1(以消除冗余并提高反向传播搜索效率)。因此，神经网络方程为</p><p id="c5fe" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">NN(x)= w0 * relu(x+B0)+w1 * relu(x+B1)+..+wn*relu(x+bn)</p><p id="804a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">一个关键的有用观察是(a)神经元ni (wi *relu(x+bi))仅在x &gt; -bi时对总和有贡献，即，神经元仅在x &gt; -bi时是活动的。B1点被称为神经元的起点。(b)如果神经元ni是活动的，斜率增加wi。需要注意的重要一点是，n个神经元的神经网络(具有相对非线性)表示具有n个斜率的连续图形。当x &gt; bi时，每个神经元对神经网络的斜率有贡献，其中bi是神经元的偏差。</p><p id="d64b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><em class="lz">例1 </em> ( <em class="lz">一个有四个斜率的神经网络</em>):下图3a所示为单输入单输出神经网络。输入层有4个神经元，每个神经元对大小为1的向量进行操作。每个神经元的权重和偏差分别为{1，1，1，1}和{0，-1，-2，-3}。所建模的函数具有单个输出，因此输出层具有4个输入的1个神经元。输出图层没有非线性，权重为{2，-1，-2，-1}。图3b示出了由该神经网络表示的曲线。该图表示具有四个斜率{2，1，-1，-2}的连续函数，其中斜率在位置{0，1，2，3}处变化。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ml"><img src="../Images/7825589dc1615b2d53f80b7349e6cc0c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1094/format:webp/1*ueCTZrV-2BlzrGG-E6tTwg.png"/></div><p class="kn ko gj gh gi kp kq bd b be z dk translated">图3a(图片由作者提供):宽度为4的神经网络。</p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi kf"><img src="../Images/fd0e1fc5340fef3c39374ea9ed56c79f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*hMZX-3dbStNvAD08EVj2yA.png"/></div><p class="kn ko gj gh gi kp kq bd b be z dk translated">图3b(作者提供的图片):由输入层中具有偏置{0，-1，-2，-3}和权重{1，1，1，1}的四个神经元以及输出层中具有权重{2，-1，-2，-1}和偏置0的一个四个输入神经元的神经网络建模的函数。</p></figure><p id="084a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><em class="lz">反向传播</em>:反向传播是用来减少测试数据和神经网络预测值之间误差的算法。该算法反复修改最初猜测的偏差和权重，直到测试数据和神经网络预测值之间的误差在可接受的范围内。参考函数ref_f(x)和神经网络之间的差值δ(x)和误差e(x)由下式给出</p><pre class="kg kh ki kj gt ma mb mc md aw me bi"><span id="0d89" class="mf mg iq mb b gy mh mi l mj mk">delta(x) = (ref_f(x) - Σ wi*relu(x+bi) + b)<br/>e(x) = delta(x)**2<br/>where n &gt; i ≥ 0,  n <br/>is the number of neurons in the input layer.</span></pre><p id="861e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">相对于权重和偏差的梯度被表示为相对于权重和偏差的偏导数。</p><pre class="kg kh ki kj gt ma mb mc md aw me bi"><span id="5ad1" class="mf mg iq mb b gy mh mi l mj mk">grad = Σ ∂e/∂wi*Δwi + ∂e/∂bi*Δbi + ∂e/∂bi*Δb where</span><span id="99b5" class="mf mg iq mb b gy mm mi l mj mk">∂e/∂wi = -2 * delta(x) * relu(x+bi)</span><span id="74c7" class="mf mg iq mb b gy mm mi l mj mk">∂e/∂bi = -2 * delta(x) * (x+bi &gt; 0) ? wi : 0) and</span><span id="724e" class="mf mg iq mb b gy mm mi l mj mk">∂e/∂b =  2 * delta(x)</span></pre><p id="6e2d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">权重更新的公式如下</p><pre class="kg kh ki kj gt ma mb mc md aw me bi"><span id="235b" class="mf mg iq mb b gy mh mi l mj mk">wi = wi — lr*∂e/∂wi = wi + 2*lr*delta(x) * relu(x+bi) --- equation 1</span><span id="ca2f" class="mf mg iq mb b gy mm mi l mj mk">bi = bi — lr*∂e/∂bi = bi + 2*lr* delta(x) *(x+bi &gt; 0) ? wi: 0)                              ---- equation 2</span><span id="0256" class="mf mg iq mb b gy mm mi l mj mk">b = b — lr*∂e/∂b = bi + 2*lr* delta(x) where lr is learning rate                                  ----- equation 3</span></pre><p id="7829" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在前向路径中的每次迭代期间，我们计算输出NN(x)=σwi * relu(x+bi)、误差函数e(x ),并且在反向路径中，我们更新权重和偏差。</p><p id="21af" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">权重更新规则是:</p><p id="5316" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">如果NN(x) &lt; ref_f(x) i.e, delta(x) &gt;为0(神经网络输出小于参考值)则:</p><pre class="kg kh ki kj gt ma mb mc md aw me bi"><span id="b2c8" class="mf mg iq mb b gy mh mi l mj mk">For each neuron x &gt; -bi (active neuron)<br/> weight increases (slope increases)<br/> if wi &gt; 0 bias is increased, (neuron (slope) moves left ) -- rule 1<br/> if wi &lt; 0 bias is decreased, (neuron (slope) moves right) -- rule 2<br/>Here the magnitude of weight increase is proportional to delta(x) and the distance x + bi from the origin -bi. <br/>The magnitude of change in bi is proportional to delta(x) and the value of weight wi.</span></pre><p id="9792" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">如果NN(x) &gt; ref_f(x)，即δ(x)&lt; 0, ( Neural network output is greater than the reference value) then:</p><pre class="kg kh ki kj gt ma mb mc md aw me bi"><span id="be8d" class="mf mg iq mb b gy mh mi l mj mk">For each neuron x &gt; -bi (active neuron)<br/> weight decreases (slope decreases)<br/>if wi &gt; 0 bias is decreased, (neuron (slope) moves right) -- rule 3<br/>if wi &lt; 0 bias is increased, (neuron (slope) moves left)  -- rule 4<br/>Here the magnitude of weight decrease is proportional to delta(x) and the distance x + bi from the origin -bi.<br/>The magnitude of change in bi is proportional to delta(x) and the value of weight wi.</span></pre><p id="8687" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">In the next section, I present a few examples to illustrate the operation of the backpropagation algorithm. A random number generator generates 400 numbers in the range [0, 4]. A reference function generates the corresponding output. The reference function used in the examples are neural networks with fixed biases and weights. A training neural network with<em class="lz">n</em>神经元被选择。猜测训练神经网络的初始偏差和权重。反向传播算法更新权重和偏差，使得训练神经网络和参考函数之间的误差减小。对于神经网络的训练，使用400个数字的数据集一次或多次，直到误差减少到期望的限度内或者我们达到运行时间的上限。训练所需的时间将取决于权重和偏差的初始值。</p></div><div class="ab cl ls lt hu lu" role="separator"><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx"/></div><div class="ij ik il im in"><p id="3365" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">本节中的示例附有情节和动画。每个例子具有(I)将具有初始偏差和权重的训练神经网络与参考函数进行比较的图，(ii)将具有最终偏差和权重的训练神经网络与参考函数进行比较的图，(iii)显示随着反向传播进行的每个神经元的偏差的图，(iv)显示随着反向传播进行的每个神经元的权重的图，(v)显示随着反向传播进行的神经网络和参考函数的图的动画，以及(vi)可选地，一些例子可以具有随着反向传播更新偏差和权重的单个神经元的图的动画。</p><p id="fe83" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><em class="lz">示例2 </em>:让我们使用由等式ref_fn1(x) =2 * relu(x-1.0)指定的单个神经元参考函数，其中x被定义在区间[0，4]上。对于训练，我们将使用一个神经元的神经网络，初始偏差为0，初始权重(斜率)为-0.6。选择0的偏置，使得<em class="lz">神经元在区间[0，4]上定义的整个数据集</em>上是活动的。神经网络和参考函数的方程是:</p><pre class="kg kh ki kj gt ma mb mc md aw me bi"><span id="ff50" class="mf mg iq mb b gy mh mi l mj mk">neural(x) = wt*relu(x+b) where wt = -0.6 and b = 0.<br/>ref_fn1(x) = 2* relu(x-1).</span></pre><p id="75d3" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这个例子将说明单个神经元神经网络对参考函数的收敛。这里，反向传播算法将神经元从0向右移动到1，并将其斜率从-0.6增加到2。让我们看看它是如何做到的。</p><p id="14fd" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">下面的图4a是显示神经网络在800次迭代中收敛到参考函数的动画。图4b示出了随着反向传播的进行，神经元的偏差和权重的变化。图4c示出了神经网络的初始和最终图以及参考函数。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi kf"><img src="../Images/9b8c24fc330a02ce38439826fd5cda96.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/1*pN2oqg8-fv719DC4yvkupQ.gif"/></div><p class="kn ko gj gh gi kp kq bd b be z dk translated">图4a(作者的动画) :显示在反向传播算法的800次迭代中一个神经元神经网络收敛到参考函数的动画图</p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="gh gi kr"><img src="../Images/cfb325465841353148f325b930c3b1c3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DRgG-5nFM-pSFtUxFbhLDg.png"/></div></div><p class="kn ko gj gh gi kp kq bd b be z dk translated">图4b(作者提供的图像):(左)在800次迭代的反向传播期间神经元的偏差更新和(右)神经元的权重更新</p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="gh gi kr"><img src="../Images/3c2b06188c7d8c047de9be673e427a3c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zsudbCrISE6HDOnGFSOkKQ.png"/></div></div><p class="kn ko gj gh gi kp kq bd b be z dk translated">图4c(图片由作者提供):(左)neural1(x)和ref_fn1(x)标绘初始偏差(0)和权重(-0.6)，(右)neural1(x)和ref_fn1(x)在800次反向传播权重和偏差更新后。</p></figure><p id="8282" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">上面图4a中动画所示的收敛有三个阶段。这些阶段是:</p><p id="a176" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">阶段1:最初，神经网络输出小于参考函数(如图4c(左)所示)。因此<em class="lz">斜率在4次迭代中从-0.6增加到0</em>，并且神经元向右移动(对负斜率和正δ应用规则2)。在接下来的12次迭代中，<em class="lz">斜率继续从0增加</em>到1，但是<em class="lz">神经元现在向左移动</em>，因为斜率是正的(对正斜率和正增量应用规则1)。</p><p id="1207" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">阶段2:在该阶段，当δ为正时，斜率增加，神经元向左移动(规则1)，当δ为负时，斜率减小，神经元向右移动(规则3)。然而，在300次迭代中，移动是这样的，即<em class="lz">总斜率从1.0增加到1.84(图4b(右))，并且<em class="lz">总神经元从0向右移动</em>(图4a(左))。</em></p><p id="674a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">阶段3:在该阶段中，<em class="lz">斜率继续从1.84上升</em>到2.0，并且在剩余的500次迭代中<em class="lz">神经元从0.9向右移动</em>到1.0。在此阶段<em class="lz">斜率和神经元的移动较慢</em>，因为每次迭代中的误差较小。</p><p id="6381" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">结论:这个例子显示了反向传播算法更新单神经元神经网络的偏差和权重的能力，使得它收敛于参考函数。</p><p id="ef4b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在下一个例子中，我们使用双神经元神经网络作为参考函数。</p><p id="3ed3" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><em class="lz">例3 </em>:参考函数由定义在范围[0，4]上的等式ref _ fn2(x)= 2 * relu(x)-4 * relu(x-2)指定。为了训练，我们使用一个在输入层有2个神经元的神经网络。初始偏差为{0，-0.2}，初始权重为{0。, 0.08}.选择偏置使得神经元的原点尽可能接近0。这确保了<em class="lz">两个神经元在输入范围[0，4]上定义的大部分数据集上都是活跃的</em>。每个<em class="lz">神经元的<em class="lz">初始偏置</em>值不同</em>。权重的更新取决于(x+bi)的值，其中bi是第I个神经元的偏差。通过选择不同的偏置，每个神经元的权重更新和偏置更新的轨迹将是不同的。从这个例子中可以看出这种差异的影响。具有初始偏差和权重的神经网络函数为</p><pre class="kg kh ki kj gt ma mb mc md aw me bi"><span id="2b86" class="mf mg iq mb b gy mh mi l mj mk">neural2(x) = wt0*relu(x -b0)+wt1*relu(x-b1) where b0=0, b1=-0.2, wt0 = 0, wt1 = 0.08</span></pre><p id="db4c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">图5a(左)示出了神经元的偏差和(右)权重的轨迹，它们被反向传播算法更新。neuron2的偏差仅增加到0.14，然后开始移动到-2.16，而neuron1的偏差增加到1.25，然后回到0，如图5a和下面的动画图5b所示。</p><p id="ec55" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">让我们看看轨迹不同的原因。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="gh gi kr"><img src="../Images/48a0c9a49a761b8706c0c4736dd997f2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hIAYucLlLmh58ud0h2csmA.png"/></div></div><p class="kn ko gj gh gi kp kq bd b be z dk translated">图5a(作者提供的图像):(左)图显示了随着反向传播的进行，两个神经元的偏差和(右)权重</p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi kf"><img src="../Images/02e78f2303acb83df5b44076d4ce0b02.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/1*qyTpYav9X411FT3LCw-F3Q.gif"/></div><p class="kn ko gj gh gi kp kq bd b be z dk translated">图5b(作者的动画)是两个神经元的图，它们的偏差和权重被反向传播算法更新</p></figure><p id="e49a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">两个神经元轨迹不同的原因是:</p><p id="563b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">原因1 <em class="lz">:偏差不同导致权重更新不同</em>:neuron 1初始偏差为0，neuron2初始偏差为-0.2。因此，neuron1的原点为0，neuron2的原点为0.2。权重更新(来自等式1)与neuron1的delta(x)*x和neuron2的delta(x) * (x-0.2)成比例。因此，当x &gt; 0.2时，两个神经元的权重都增加，其中神经1的权重增加的量更大。</p><p id="2ff2" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">原因2: <em class="lz">权重的差异导致偏差更新的差异</em>:偏差的更新(来自等式2)与delta(x)和神经元的权重值成比例。由于neuron1的重量较大，它比neuron2移动得更远。此外，当delta为负且权重降低时，由于neuron2的权重较小，因此其变为负(而neuron1的权重仍为正)。这导致neuron2向右移动，而neuron1仍然向左移动。这是神经元1和神经元2起源分离增加的两个原因。</p><p id="df11" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">原因3: <em class="lz">当一个神经元活动而另一个神经元不活动时，偏差和权重出现差异</em>:随着neuron2进一步向右移动，其原点越过0，则当x &lt; -b1时，它不活动(其权重不更新)。这就是为什么neuron1的斜率增加而neuron2的斜率减少的原因，如上面的图5a(右)所示。这是两个神经元的偏差和权重轨迹不同的三个原因。</p><p id="9d9b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">下面的图5c显示了神经网络收敛到参考函数时的动画图。图5d(左)示出了具有初始偏差和权重的神经网络的图，图5d(右)示出了具有最终偏差和权重的反向传播3601次迭代之后的图。动画有三个阶段。在第一阶段，两个神经元都向左移动，神经元1以更高的斜率进一步向左移动。在第二阶段，神经元2开始向右移动，而神经元1向左移动。在最后阶段，两个神经元都移动到它们的最终位置0和2，并且斜率收敛到最终值2和-4。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi kf"><img src="../Images/11438af725e26a88a351fd861c26368f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/1*YfNYT90LkXT24CphO8Mc2A.gif"/></div><p class="kn ko gj gh gi kp kq bd b be z dk translated">图5c(作者的动画)显示了神经网络在3601次迭代中收敛到参考函数的动画图</p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="gh gi kr"><img src="../Images/4a406d918be823432d64334ac7dc9889.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*O_dtm078Q8vIGP6miyvbLg.png"/></div></div><p class="kn ko gj gh gi kp kq bd b be z dk translated">图5d(作者的图像)(左)具有初始偏差和权重的参考函数和神经网络的图，以及(右)具有最终偏差和权重的参考函数和神经网络的图。</p></figure><p id="d42d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><em class="lz">结论</em>:因此我们看到反向传播算法更新了两个神经元，使得它们遵循不同的轨迹。这允许两个神经元的神经网络在3601次迭代中收敛到参考函数。</p><p id="08aa" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">接下来让我们考虑一个具有四个神经元的参考函数的例子。</p><p id="2cf4" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><em class="lz">例4 </em>:参考函数由定义在0 ≤ x ≤4范围内的方程ref _ fn(x)= 2 * relu(x)-relu(x-1)-2 * relu(x-2)-relu(x-3)指定。为了训练，我们将使用四个神经元的神经网络。初始偏差为{-0.002，-0.5，-1.0，-1.5}，权重为{0.001，0.04，0.07，0.1}。与前面的例子相似，我们选择了不同的偏差。</p><p id="8178" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">图6a(左)显示了具有初始偏差和权重的神经网络图，以及(右)具有最终偏差和权重的神经网络图。然而，在20000次迭代之后，该网络不收敛于参考函数。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="gh gi kr"><img src="../Images/e8dc3b241cfefa5b202fd9efbb1aabb4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yW17lhDRg270s7JAP1yXGg.png"/></div></div><p class="kn ko gj gh gi kp kq bd b be z dk translated">图6a(作者提供的图像)具有四个神经元的神经网络图，具有初始偏差和权重(左)以及最终偏差和权重(右)</p></figure><p id="c50f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">为了理解它不收敛的原因，让我们看看图6b中所示的四个神经元的偏差和权重的轨迹，以及图6c中显示四个神经元的运动的动画。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="gh gi kr"><img src="../Images/947874de6d52af8f08c39f9e5bf81086.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4isg_-XSmwZohzciIbH38Q.png"/></div></div><p class="kn ko gj gh gi kp kq bd b be z dk translated">图(作者提供的图像)6b(左)四个神经元的偏差图，(右)四个神经元的权重(斜率)图</p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi kf"><img src="../Images/ecbb89325ebeff74d48e77c6925ce0d3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/1*mV3FW8yZIwZtRSJXeLA7WQ.gif"/></div><p class="kn ko gj gh gi kp kq bd b be z dk translated">图6c(作者的动画)显示了随着反向传播的进行，四个神经元的位置和斜率的动画。</p></figure><p id="8386" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">全局最优要求四个神经元以斜率{2，-1，-2，-1}移动到位置{0，1，2，3}。相反，如图6b所示，四个神经元{紫色、绿色、蓝色、橙色}以斜率{1.42，0.5，-1.33，-2.1}移动到位置{0，0，1.22，2.22}。因此，neuron1和neuron2都收敛于位置0。这导致两个神经元在位置0合并成一个神经元，组合斜率为1.42+0.5 = 1.92。</p><p id="787a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">从图6b中我们可以看到，neuron1和neuron2的斜率是正的并且在上升。当delta(x) &gt; 0且x &gt; -b2时，neuron1和neuron2都向左移动。当delta(x) &lt; 0 and x &lt; -b2 (i.e., neuron2 is inactive) neuron1 will move to the right and neuron2 will not be updated as it is not active when x &lt; -b2. Overall neuron1 moves right as the movement to the right is larger, while neuron2 moves left. This is the reason for coalescing of these two neurons. Thus effectively we have a neural network of three neurons with bias {0, -1.22, -2.22} and weights {1.92, -1.33, -2.1}. This is the reason for reaching a local optimum and not a global optimum.</p><p id="91ce" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">There are two solutions to this problem.</p><p id="a71d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><em class="lz"> Solution1 </em>:一个是改变bias的初始值，使神经元更向右展开。让我们选择以下初始偏差{0，-0.75，-1.5，-2.24}和权重{0，0.02，0.05，0.08}。图7a显示这导致19200次反向传播迭代的收敛。图7b示出了偏置和权重的轨迹。我们看到三个神经元向右移动，斜率为负。由于neuron2向右移动并具有负斜率，它避免了与neuron1合并的问题。最终偏差为{0，-1.0，-1.97，-2.93}，最终权重为{1.96，-0.99，-1.82，-1.15}。图7c是显示神经网络在19200次迭代中收敛到参考函数的动画图。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="gh gi mn"><img src="../Images/d8e60805a139a52cec6048aca3c68578.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*b8WpsDTC3gmpLW9OAd2dUQ.png"/></div></div><p class="kn ko gj gh gi kp kq bd b be z dk translated">图7a(作者提供的图像)(左)具有初始位置{0，0.75，1.5，2.24}的参考函数和神经网络的图，以及(右)具有最终位置{0，1.0，1.97，2.93}的参考函数和神经网络的图</p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="gh gi mn"><img src="../Images/17e607cd90ab68d1ebebff3b82861232.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*79hbrF2bhTvry17SC9DVOQ.png"/></div></div><p class="kn ko gj gh gi kp kq bd b be z dk translated">图7b(作者提供的图像)(左)随着反向传播的进行，四个神经元的偏差图和(右)权重图。</p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi kf"><img src="../Images/58be2b7a782c6ad685881ac6f7763406.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/1*u6uXJnC2ah4ttN-d2ydA0w.gif"/></div><p class="kn ko gj gh gi kp kq bd b be z dk translated">图7c(作者提供的图像)在19200次迭代中，初始神经元位置在{0，0.75，1.5，2.24}处收敛到参考函数的神经网络的动画图</p></figure><p id="e396" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">第二种解决方案是使用具有五个神经元的神经网络。</p><p id="a90b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><em class="lz">解2 </em>:我们用一个五个神经元的神经网络，初始偏差为{0，-0.4，-0.8，-1.2，-1.6}，权重为{-0.08，-0.06，-0.02，0.02，0.06}进行训练。图8a(左)是将参考函数与具有初始偏差和权重的神经网络进行比较的图。图8a(右)显示了在20000次迭代后收敛到参考函数的具有最终偏差和权重的神经网络的图。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="gh gi kr"><img src="../Images/9a5e067ba6e2d3dbdb3fbea0848edda6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*O7AjOotzBlL5AcBHMGqPAA.png"/></div></div><p class="kn ko gj gh gi kp kq bd b be z dk translated">图8a(图片由作者提供)(左)具有初始偏差和权重的参考函数和神经网络的图，以及(右)具有最终偏差和权重的参考函数和神经网络的图。</p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="gh gi mn"><img src="../Images/5343b6fbc75a25480e26340883a12512.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nHSQKVlC_eU7zoqu6kHfsw.png"/></div></div><p class="kn ko gj gh gi kp kq bd b be z dk translated">图8b(作者提供的图像)(左)图示出了随着反向传播的进行，五个神经元的偏差和权重(右)的轨迹。</p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi kf"><img src="../Images/d7b11b94b4bdb459ec2bbf58d94289de.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/1*X7jXbvv5ZdZuQc9p6HmJgg.gif"/></div><p class="kn ko gj gh gi kp kq bd b be z dk translated">图8c(作者的动画)显示了在20000次反向传播迭代中五神经元神经网络收敛到参考函数的动画图</p></figure><p id="1f23" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">图8b示出了五个神经元的偏差和权重的轨迹。我们看到neuron1和neuron2仍然以0的偏差结合在一起。其他三个神经元的偏差收敛于{-1，-2，-2.89}。权重收敛到{1.24，0.72，-1.05，-1.67，-1.25}。因此，收敛性比先前的四个神经元的网络好得多。</p><p id="b5f8" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">下一个解决方案结合了解决方案1和解决方案2。这里，我们使用五个神经元，它们的初始偏置如解决方案1中一样向右移动。</p><p id="2bad" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><em class="lz">解3 </em>:我们用一个五个神经元的神经网络，初始偏差{0，-0.6，-1.2，-1.8，-2.4，}，初始权重{-0.06，-0.04，-0.02，0.02，0.06，}。图9a(左)示出了将具有初始偏差和权重的五个神经元的神经网络与参考函数进行比较的图，图9a(右)示出了在12，400次迭代之后具有最终偏差和权重的五个神经元的神经网络进行比较的图。图9b示出了具有五个神经元的神经网络的偏差和权重的轨迹，而图9c是示出了神经网络在12，400次迭代中收敛到参考函数的动画。最终偏差为{0.022，-0.04，-1.08，-1.99，-2.89}，最终权重为{1.43，0.54，-1.1，-1.69，-1.16}。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="gh gi kr"><img src="../Images/43432642f7eec303c41ac873a631a351.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LT7kDEveSfk_EEkehDVbFw.png"/></div></div><p class="kn ko gj gh gi kp kq bd b be z dk translated">图9a(作者的图像)(左)具有初始权重的参考函数和具有五个神经元的神经网络的图，(右)具有最终偏差和权重的参考函数和具有五个神经元的神经网络的图。</p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="gh gi mo"><img src="../Images/7314ebdaaead96da3949c71896b0fdf8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mxf0oQjS2JIMbyNKCowuOw.png"/></div></div><p class="kn ko gj gh gi kp kq bd b be z dk translated">图9b(作者提供的图像)(左)随着反向传播的进行，神经网络的偏差和(右)权重的轨迹图。</p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi kf"><img src="../Images/e259604fa5d4833ebb06285f0e4c8738.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/1*HMmjnTBDIQOwatstgE4C1Q.gif"/></div><p class="kn ko gj gh gi kp kq bd b be z dk translated">图9c(作者的动画)显示了在12，400次迭代中五神经元神经网络向参考函数的收敛的动画图。</p></figure><p id="b93a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><em class="lz">结论</em>:神经网络的监督训练非常耗时。反向传播算法使用最速下降算法移动神经元并调整权重。将初始偏差分配给输入数据范围内的值非常重要。最好给每个神经元分配不同的偏置值。这是很好的传播偏见，以避免合并神经元。</p></div></div>    
</body>
</html>