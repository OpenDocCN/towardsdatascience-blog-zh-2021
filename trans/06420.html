<html>
<head>
<title>7 NLP Techniques You Can Easily Implement with Python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用Python可以轻松实现的7种NLP技术</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/7-nlp-techniques-you-can-easily-implement-with-python-dc0ade1a53c2?source=collection_archive---------3-----------------------#2021-06-09">https://towardsdatascience.com/7-nlp-techniques-you-can-easily-implement-with-python-dc0ade1a53c2?source=collection_archive---------3-----------------------#2021-06-09</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="062b" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">您只需要几行代码就可以用Python实现NLP技术。</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/68e41082d87d052bedb82727b5d45dc9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*e0QrjnESNFlLdwxI"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">由<a class="ae ky" href="https://unsplash.com/@codestorm?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">萨法尔萨法罗夫</a>在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="2282" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">自然语言处理(NLP)致力于使计算机能够理解和处理人类语言。计算机非常擅长处理结构化数据，比如电子表格；然而，我们写或说的许多信息都是非结构化的。</p><p id="211a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">NLP的目标是让计算机理解非结构化文本，并从中检索有意义的信息。由于spaCy和NLTK等开源库，我们只需几行Python代码就可以实现许多NLP技术。</p><p id="5873" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在本文中，我们将学习7种NLP技术的核心概念，以及如何在Python中轻松实现它们。</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="8e89" class="ma mb it lw b gy mc md l me mf"><strong class="lw iu">Table of Contents<br/></strong>1. <a class="ae ky" href="#cb0b" rel="noopener ugc nofollow">Sentiment Analysis</a><br/>2. <a class="ae ky" href="#72eb" rel="noopener ugc nofollow">Named Entity Recognition (NER)</a><br/>3. <a class="ae ky" href="#0658" rel="noopener ugc nofollow">Stemming</a><br/>4. <a class="ae ky" href="#0658" rel="noopener ugc nofollow">Lemmatization</a><br/>5. <a class="ae ky" href="#3464" rel="noopener ugc nofollow">Bag of Words (BoW)</a><br/>6. <a class="ae ky" href="#fd70" rel="noopener ugc nofollow">Term Frequency–Inverse Document Frequency (TF-IDF)</a><br/>7. <a class="ae ky" href="#aed9" rel="noopener ugc nofollow">Wordcloud</a></span></pre><h1 id="cb0b" class="mg mb it bd mh mi mj mk ml mm mn mo mp jz mq ka mr kc ms kd mt kf mu kg mv mw bi translated">1.情感分析</h1><p id="6377" class="pw-post-body-paragraph kz la it lb b lc mx ju le lf my jx lh li mz lk ll lm na lo lp lq nb ls lt lu im bi translated">情感分析是最流行的NLP技术之一，它涉及获取一段文本(例如，评论、评论或文档)并确定数据是正面的、负面的还是中性的。它在医疗保健、客户服务、银行等方面有许多应用。</p><h2 id="80c9" class="ma mb it bd mh nc nd dn ml ne nf dp mp li ng nh mr lm ni nj mt lq nk nl mv nm bi translated">Python实现</h2><p id="b383" class="pw-post-body-paragraph kz la it lb b lc mx ju le lf my jx lh li mz lk ll lm na lo lp lq nb ls lt lu im bi translated">对于简单的情况，在Python中，我们可以使用NLTK包中的VADER(情感推理的价感知字典),它可以直接应用于未标记的文本数据。作为一个例子，让我们得到电视节目中角色所讲台词的所有情感分数。</p><p id="c0e2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">首先，我们在Kaggle或我的Github上争论一个名为“avatar.csv”的数据集，然后用VADER计算每一句台词的分数。所有这些都存储在<code class="fe nn no np lw b">df_character_sentiment</code>数据帧中。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nq nr l"/></div></figure><p id="86ff" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在下面的<code class="fe nn no np lw b">df_character_sentiment</code>中，我们可以看到每一个句子都得到了一个否定、中立和肯定的分数。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/a6e8657c41edfd59e0759838109d6244.png" data-original-src="https://miro.medium.com/v2/resize:fit:1164/format:webp/1*FM59x8FO7elnHlS6H4mG0A.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><p id="fdf0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们甚至可以按角色对分数进行分组，并计算平均值来获得角色的情感分数，然后通过使用matplotlib库用水平条形图来表示它(结果显示在本文<a class="ae ky" rel="noopener" target="_blank" href="/avatar-meets-data-visualization-60631f86ba7d">的</a>中)</p><p id="dd19" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="nt">注:VADER针对社交媒体文本进行了优化，因此我们应该对结果持保留态度。你可以使用更完整的算法，或者用机器学习库开发自己的算法。在下面的链接中，有一个完整的指南，介绍如何使用sklearn库用Python从头创建一个。</em></p><div class="nu nv gp gr nw nx"><a rel="noopener follow" target="_blank" href="/a-beginners-guide-to-text-classification-with-scikit-learn-632357e16f3a"><div class="ny ab fo"><div class="nz ab oa cl cj ob"><h2 class="bd iu gy z fp oc fr fs od fu fw is bi translated">Scikit-Learn简单指南—用Python构建机器学习模型</h2><div class="oe l"><h3 class="bd b gy z fp oc fr fs od fu fw dk translated">Python中的第一个ML模型。</h3></div><div class="of l"><p class="bd b dl z fp oc fr fs od fu fw dk translated">towardsdatascience.com</p></div></div><div class="og l"><div class="oh l oi oj ok og ol ks nx"/></div></div></a></div><h1 id="72eb" class="mg mb it bd mh mi mj mk ml mm mn mo mp jz mq ka mr kc ms kd mt kf mu kg mv mw bi translated">2.命名实体识别(NER)</h1><p id="1de5" class="pw-post-body-paragraph kz la it lb b lc mx ju le lf my jx lh li mz lk ll lm na lo lp lq nb ls lt lu im bi translated">命名实体识别是一种技术，用于定位文本中的命名实体并将其分类成诸如人、组织、位置、时间表达式、数量、货币值、百分比等类别。它用于优化搜索引擎算法、推荐系统、客户支持、内容分类等。</p><h2 id="e533" class="ma mb it bd mh nc nd dn ml ne nf dp mp li ng nh mr lm ni nj mt lq nk nl mv nm bi translated">Python实现</h2><p id="49d7" class="pw-post-body-paragraph kz la it lb b lc mx ju le lf my jx lh li mz lk ll lm na lo lp lq nb ls lt lu im bi translated">在Python中，我们可以使用SpaCy的命名实体识别，它支持以下实体类型。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi om"><img src="../Images/7e9dcc07d38073e074d575b8895ec4ce.png" data-original-src="https://miro.medium.com/v2/resize:fit:1022/format:webp/1*_JwDhRjdAGndy9LG3iJZxQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">来源(空间文件)</p></figure><p id="9b3d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为了看到它的运行，我们首先导入<code class="fe nn no np lw b">spacy</code>，然后创建一个<code class="fe nn no np lw b">nlp</code>变量来存储<code class="fe nn no np lw b">en_core_web_sm</code>管道。这是一个针对书面网络文本(博客、新闻、评论)的小型英语管道，包括词汇、向量、句法和实体。为了找到实体，我们将自然语言处理应用于一个句子。</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="b6de" class="ma mb it lw b gy mc md l me mf">import spacy</span><span id="8cc0" class="ma mb it lw b gy on md l me mf">nlp = spacy.load("en_core_web_sm")<br/>doc = nlp("Biden invites Ukrainian president to White House this summer")</span><span id="8315" class="ma mb it lw b gy on md l me mf">print([(X.text, X.label_) for X in doc.ents])</span></pre><p id="4bae" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这将打印以下值</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="c097" class="ma mb it lw b gy mc md l me mf">[('Biden', 'PERSON'), ('Ukrainian', 'GPE'), ('White House', 'ORG'), ('this summer', 'DATE')]</span></pre><p id="7356" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">斯帕西发现，“拜登”是一个人，“乌克兰”是GPE(国家，城市，州，“白宫”是一个组织，“今年夏天”是一个日期。</p><h1 id="0658" class="mg mb it bd mh mi mj mk ml mm mn mo mp jz mq ka mr kc ms kd mt kf mu kg mv mw bi translated">3.词干化和词汇化</h1><p id="c2fc" class="pw-post-body-paragraph kz la it lb b lc mx ju le lf my jx lh li mz lk ll lm na lo lp lq nb ls lt lu im bi translated">词干化和词汇化是自然语言处理中两种流行的技术。两者都规范了一个词，但方式不同。</p><ul class=""><li id="cdff" class="oo op it lb b lc ld lf lg li oq lm or lq os lu ot ou ov ow bi translated"><strong class="lb iu">词干化:</strong>它将一个单词截断成它的词干。例如，单词“朋友”、“友谊”、“友谊”将被简化为<strong class="lb iu">“朋友”</strong>词干分析可能不会给我们一个字典，一个特定的语法词集。</li><li id="2698" class="oo op it lb b lc ox lf oy li oz lm pa lq pb lu ot ou ov ow bi translated"><strong class="lb iu">词汇化</strong>:与词干提取技术不同，词汇化会找到字典中的单词，而不是截断原始单词。词汇化算法提取每个单词的正确词汇，因此它们通常需要一个语言词典来对每个单词进行正确分类。</li></ul><p id="6a27" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这两种技术都被广泛使用，您应该根据项目的目标明智地选择它们。与词干化相比，词汇化具有较低的处理速度，因此如果准确性不是项目的目标，而是速度，那么词干化是一种合适的方法；然而。如果准确性至关重要，那么可以考虑使用词汇化。</p><p id="91d6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">Python的NLTK库使得使用这两种技术变得很容易。让我们看看它的实际效果。</p><h2 id="4091" class="ma mb it bd mh nc nd dn ml ne nf dp mp li ng nh mr lm ni nj mt lq nk nl mv nm bi translated">Python实现(词干)</h2><p id="f309" class="pw-post-body-paragraph kz la it lb b lc mx ju le lf my jx lh li mz lk ll lm na lo lp lq nb ls lt lu im bi translated">对于英语，nltk中有两个流行的库——波特·斯特梅尔和兰卡斯特·斯泰默。</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="02f8" class="ma mb it lw b gy mc md l me mf">from nltk.stem import PorterStemmer<br/>from nltk.stem import LancasterStemmer</span><span id="61ca" class="ma mb it lw b gy on md l me mf"># PorterStemmer<br/>porter = PorterStemmer()<br/># LancasterStemmer<br/>lancaster = LancasterStemmer()</span><span id="f89a" class="ma mb it lw b gy on md l me mf">print(porter.stem("friendship"))<br/>print(lancaster.stem("friendship"))</span></pre><p id="6adc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">PorterStemmer算法不遵循语言学，而是针对不同情况的一组5个规则，这些规则分阶段应用以生成词干。<code class="fe nn no np lw b">print(porter.stem(“friendship”))</code>代码将打印单词<code class="fe nn no np lw b">friendship</code></p><p id="6016" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">LancasterStemmer很简单，但由于迭代和过度词干化，可能会产生大量词干。这导致词干不是语言性的，或者它们可能没有意义。<code class="fe nn no np lw b">print(lancaster.stem(“friendship”))</code>代码将打印单词<code class="fe nn no np lw b">friend</code>。</p><p id="513a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">你可以试试其他的词，看看这两种算法有什么不同。在其他语言的情况下，可以从<code class="fe nn no np lw b">nltk.stem</code>导入<code class="fe nn no np lw b">SnowballStemme </code></p><h2 id="a527" class="ma mb it bd mh nc nd dn ml ne nf dp mp li ng nh mr lm ni nj mt lq nk nl mv nm bi translated">Python实现(词汇化)</h2><p id="232a" class="pw-post-body-paragraph kz la it lb b lc mx ju le lf my jx lh li mz lk ll lm na lo lp lq nb ls lt lu im bi translated">我们将再次使用NLTK，但是这次我们导入了<code class="fe nn no np lw b">WordNetLemmatizer</code>,如下面的代码所示。</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="8f92" class="ma mb it lw b gy mc md l me mf">from nltk import WordNetLemmatizer</span><span id="08b0" class="ma mb it lw b gy on md l me mf">lemmatizer = WordNetLemmatizer()<br/>words = ['articles', 'friendship', 'studies', 'phones']</span><span id="2d72" class="ma mb it lw b gy on md l me mf">for word in words:<br/>    print(lemmatizer.lemmatize(word))</span></pre><p id="2ed4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">变元化为不同的词性(POS)值生成不同的输出。一些最常见的位置值是动词(v)、名词(n)、形容词(a)和副词(r)。词汇化中的缺省POS值是一个名词，所以上一个例子的打印值将是<code class="fe nn no np lw b">article</code>、<code class="fe nn no np lw b">friendship</code>、<code class="fe nn no np lw b">study</code>和<code class="fe nn no np lw b">phone</code>。</p><p id="7a89" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们将POS <em class="nt"> </em>值改为动词(v)。</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="6ebd" class="ma mb it lw b gy mc md l me mf">from nltk import WordNetLemmatizer</span><span id="cd64" class="ma mb it lw b gy on md l me mf">lemmatizer = WordNetLemmatizer()<br/>words = ['be', 'is', 'are', 'were', 'was']</span><span id="b4a3" class="ma mb it lw b gy on md l me mf">for word in words:<br/>    print(lemmatizer.lemmatize(word, pos='v'))</span></pre><p id="8008" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在这种情况下，Python将为列表中的所有值打印单词<code class="fe nn no np lw b">be</code>。</p><h1 id="3464" class="mg mb it bd mh mi mj mk ml mm mn mo mp jz mq ka mr kc ms kd mt kf mu kg mv mw bi translated">5.一袋单词</h1><p id="6a64" class="pw-post-body-paragraph kz la it lb b lc mx ju le lf my jx lh li mz lk ll lm na lo lp lq nb ls lt lu im bi translated">单词袋(BoW)模型是一种将文本转换成固定长度向量的表示方法。这有助于我们将文本表示成数字，因此我们可以将其用于机器学习模型。该模型不关心词序，但它只关心文本中的词频。它在自然语言处理、文档信息检索和文档分类中有应用。</p><p id="155a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">典型的BoW工作流包括清理原始文本、标记化、构建词汇表和生成向量。</p><h2 id="3e81" class="ma mb it bd mh nc nd dn ml ne nf dp mp li ng nh mr lm ni nj mt lq nk nl mv nm bi translated">Python实现</h2><p id="95f4" class="pw-post-body-paragraph kz la it lb b lc mx ju le lf my jx lh li mz lk ll lm na lo lp lq nb ls lt lu im bi translated">Python的库sklearn包含一个名为CountVectorizer的工具，它负责BoW工作流的大部分工作。</p><p id="6235" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们用下面的两个句子作为例子。</p><p id="6c7b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">第一句:</strong>“我喜欢用Python写代码。我喜欢Python代码”</p><p id="b055" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">第二句:“我讨厌用Java写代码。我讨厌Java代码”</p><p id="15aa" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这两个句子都将存储在一个名为<code class="fe nn no np lw b">text</code>的列表中。然后我们将创建一个数据帧<code class="fe nn no np lw b">df</code>来存储这个<code class="fe nn no np lw b">text</code>列表。在这之后，我们将初始化CountVectorizer <code class="fe nn no np lw b">(cv)</code>的一个实例，然后我们将拟合和转换文本数据以获得数字表示。这将存储在文档术语矩阵<code class="fe nn no np lw b">df_dtm</code>中。</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="2f93" class="ma mb it lw b gy mc md l me mf">import pandas as pd<br/>from sklearn.feature_extraction.text import CountVectorizer</span><span id="6479" class="ma mb it lw b gy on md l me mf">text = ["I love writing code in Python. I love Python code",<br/>        "I hate writing code in Java. I hate Java code"]</span><span id="da85" class="ma mb it lw b gy on md l me mf">df = pd.DataFrame({'review': ['review1', 'review2'], 'text':text})<br/>cv = CountVectorizer(stop_words='english')<br/>cv_matrix = cv.fit_transform(df['text'])<br/>df_dtm = pd.DataFrame(cv_matrix.toarray(),<br/>                      index=df['review'].values,<br/>                      columns=cv.get_feature_names())<br/>df_dtm</span></pre><p id="76ac" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">用存储在<code class="fe nn no np lw b">df_dtm </code>中的CountVectorizer制作的弓表示如下图所示。请记住，CountVectorizer不会考虑包含两个或更少字母的单词。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pc"><img src="../Images/409af28af16accc81f23ab2bbe36c501.png" data-original-src="https://miro.medium.com/v2/resize:fit:918/format:webp/0*MfFbD-yqMUhkZ9x1.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><p id="1de0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如你所见，矩阵中的数字代表每个单词在每次评论中被提及的次数。在这个例子中，像“爱”、“恨”和“代码”这样的词有相同的频率(2)。</p><p id="2547" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">总的来说，我们可以说CountVectorizer在标记文本、构建词汇表和生成向量方面做得很好；但是，它不会为您清理原始数据。我做了一个关于如何在Python中清理和准备数据的指南，如果你想学习最佳实践，可以去看看。</p><div class="nu nv gp gr nw nx"><a rel="noopener follow" target="_blank" href="/a-straightforward-guide-to-cleaning-and-preparing-data-in-python-8c82f209ae33"><div class="ny ab fo"><div class="nz ab oa cl cj ob"><h2 class="bd iu gy z fp oc fr fs od fu fw is bi translated">用Python清理和准备数据的简单指南</h2><div class="oe l"><h3 class="bd b gy z fp oc fr fs od fu fw dk translated">如何识别和处理脏数据？</h3></div><div class="of l"><p class="bd b dl z fp oc fr fs od fu fw dk translated">towardsdatascience.com</p></div></div><div class="og l"><div class="pd l oi oj ok og ol ks nx"/></div></div></a></div><h1 id="fd70" class="mg mb it bd mh mi mj mk ml mm mn mo mp jz mq ka mr kc ms kd mt kf mu kg mv mw bi translated">6.术语频率-逆文档频率(TF-IDF)</h1><p id="0028" class="pw-post-body-paragraph kz la it lb b lc mx ju le lf my jx lh li mz lk ll lm na lo lp lq nb ls lt lu im bi translated">与CountVectorizer不同，TF-IDF计算“权重”,表示一个单词与文档集合(也称为语料库)中的一个文档的相关程度。TF-IDF值与单词在文档中出现的次数成比例地增加，并被语料库中包含该单词的文档的数量所抵消。<strong class="lb iu">简单来说，TF-IDF得分越高，该术语就越稀有、独特或有价值，反之亦然。</strong>它在信息检索方面有应用，如搜索引擎，旨在提供与您搜索的内容最相关的结果。</p><p id="cc39" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在我们看Python实现之前，让我们看一个例子，这样你就对TF和IDF是如何计算的有了一个概念。对于下面的例子，我们将使用与CountVectorizer例子相同的句子。</p><p id="5fde" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">第一句:</strong>“我喜欢用Python写代码。我喜欢Python代码”</p><p id="6feb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">第二句:</strong>“我讨厌用Java写代码。我讨厌Java代码”</p><h2 id="f3bc" class="ma mb it bd mh nc nd dn ml ne nf dp mp li ng nh mr lm ni nj mt lq nk nl mv nm bi translated">术语频率(TF)</h2><p id="f5bf" class="pw-post-body-paragraph kz la it lb b lc mx ju le lf my jx lh li mz lk ll lm na lo lp lq nb ls lt lu im bi translated">有不同的方法来定义词频。一种建议是原始计数本身(即计数矢量器所做的事情)，但另一种建议是句子中单词的<em class="nt">频率除以句子中单词的总数。</em>在这个简单的例子中，我们将使用第一个标准，因此术语频率如下表所示。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pe"><img src="../Images/3fc765e257bf931ddc0824da740fe35a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5g64tQJazsJPCWVenjI5Fg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><p id="e518" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如您所见，这些值与之前为CountVectorizer计算的值相同。此外，两个字母或更少的单词不考虑在内。</p><h2 id="8047" class="ma mb it bd mh nc nd dn ml ne nf dp mp li ng nh mr lm ni nj mt lq nk nl mv nm bi translated">反向文档频率(IDF)</h2><p id="45f2" class="pw-post-body-paragraph kz la it lb b lc mx ju le lf my jx lh li mz lk ll lm na lo lp lq nb ls lt lu im bi translated">IDF也有不同的计算方式。尽管标准教科书符号将IDF定义为IDF(t)= log[n/(df(t)+1]，但我们稍后将在Python中使用的sklearn库默认计算公式如下。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pf"><img src="../Images/7b7708663c3de4d9fbb167c369a41d51.png" data-original-src="https://miro.medium.com/v2/resize:fit:1274/format:webp/1*n62ptZh3eH-KfVxGbAvhRg.png"/></div></figure><p id="ec2d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">还有，sklearn假设自然对数<code class="fe nn no np lw b">ln</code>而不是<code class="fe nn no np lw b">log</code>和平滑<em class="nt"> (smooth_idf=True) </em>。让我们计算每个单词的IDF值，因为sklearn会这样做。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pg"><img src="../Images/b6789152cb03e623403bb4a25f40dacf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oBF3KoCKP0aKkrccqX2tqQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><h2 id="7026" class="ma mb it bd mh nc nd dn ml ne nf dp mp li ng nh mr lm ni nj mt lq nk nl mv nm bi translated">TF-IDF</h2><p id="4aee" class="pw-post-body-paragraph kz la it lb b lc mx ju le lf my jx lh li mz lk ll lm na lo lp lq nb ls lt lu im bi translated">一旦我们有了TF和IDF值，我们就可以通过将这两个值相乘得到TF-IDF(TF-IDF = TF * IDF)。下表显示了这些值。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ph"><img src="../Images/0685137ab312d92da16b8a1216f5e779.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XGx5rjPAbhBGpZw6Y4cQwQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><h2 id="9214" class="ma mb it bd mh nc nd dn ml ne nf dp mp li ng nh mr lm ni nj mt lq nk nl mv nm bi translated">Python实现</h2><p id="b750" class="pw-post-body-paragraph kz la it lb b lc mx ju le lf my jx lh li mz lk ll lm na lo lp lq nb ls lt lu im bi translated">由于有了sklearn库，用Python计算上表所示的TF-IDF只需要几行代码。</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="4869" class="ma mb it lw b gy mc md l me mf">import pandas as pd<br/>from sklearn.feature_extraction.text import TfidfVectorizer<br/>text = ["I love writing code in Python. I love Python code",<br/>        "I hate writing code in Java. I hate Java code"]</span><span id="6b2b" class="ma mb it lw b gy on md l me mf">df = pd.DataFrame({'review': ['review1', 'review2'], 'text':text})<br/>tfidf = TfidfVectorizer(stop_words='english', norm=None)<br/>tfidf_matrix = tfidf.fit_transform(df['text'])<br/>df_dtm = pd.DataFrame(tfidf_matrix.toarray(),<br/>                      index=df['review'].values,<br/>                      columns=tfidf.get_feature_names())<br/>df_dtm</span></pre><p id="b5f3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">存储在<code class="fe nn no np lw b">df_dtm </code>中的TF-IDF表示如下图所示。同样，TF-IDF不考虑2个字母或更少的单词。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pi"><img src="../Images/b600811dfcb0b2d589abcfbe13ef054d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1098/format:webp/1*qimSqmrj0mmFY7oPd4898w.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><p id="98d1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="nt">注意:默认情况下，TfidfVectorizer()使用l2规范化，但是为了使用上面所示的公式，我们将</em> <code class="fe nn no np lw b">norm=None</code> <em class="nt">设置为参数。有关sklearn中默认使用的公式以及如何定制公式的更多详细信息，请查看其</em> <a class="ae ky" href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html" rel="noopener ugc nofollow" target="_blank"> <em class="nt">文档</em> </a> <em class="nt">。</em></p><h1 id="aed9" class="mg mb it bd mh mi mj mk ml mm mn mo mp jz mq ka mr kc ms kd mt kf mu kg mv mw bi translated">7.Wordcloud</h1><p id="dece" class="pw-post-body-paragraph kz la it lb b lc mx ju le lf my jx lh li mz lk ll lm na lo lp lq nb ls lt lu im bi translated">Wordcloud是一种帮助我们识别文本中关键词的流行技术。在单词云中，频繁出现的单词有更大更粗的字体，而不太频繁出现的单词有更小或更细的字体。在Python中，你可以用<code class="fe nn no np lw b">wordcloud</code>库制作简单的单词云，用<code class="fe nn no np lw b">stylecloud</code>库制作好看的单词云。</p><p id="716a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">下面你可以找到用Python制作单词云的代码。我用的是史蒂夫·乔布斯演讲的文本文件，你可以在我的Github上找到。</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="e5dc" class="ma mb it lw b gy mc md l me mf">import stylecloud</span><span id="5497" class="ma mb it lw b gy on md l me mf">stylecloud.gen_stylecloud(file_path='SJ-Speech.txt',<br/>                          icon_name= "fas fa-apple-alt")</span></pre><p id="9bab" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这是上面代码的结果。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pj"><img src="../Images/2f245cbb6e5079b7bbca47af1f8b6798.png" data-original-src="https://miro.medium.com/v2/resize:fit:1024/format:webp/0*UJm2BJIydZ8k_caU.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><p id="f476" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">文字云之所以如此受欢迎，是因为它们引人入胜、易于理解、易于创建。</p><p id="e0f5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">你可以通过改变颜色，删除停用词，选择你的图片，甚至添加你自己的图片作为单词云的遮罩来进一步定制。有关更多详细信息，请查看下面的指南。</p><div class="nu nv gp gr nw nx"><a rel="noopener follow" target="_blank" href="/how-to-easily-make-beautiful-wordclouds-in-python-55789102f6f5"><div class="ny ab fo"><div class="nz ab oa cl cj ob"><h2 class="bd iu gy z fp oc fr fs od fu fw is bi translated">如何用Python轻松制作漂亮的文字云</h2><div class="oe l"><h3 class="bd b gy z fp oc fr fs od fu fw dk translated">使用stylecloud和wordcloud库来定制你的wordcloud。</h3></div><div class="of l"><p class="bd b dl z fp oc fr fs od fu fw dk translated">towardsdatascience.com</p></div></div><div class="og l"><div class="pk l oi oj ok og ol ks nx"/></div></div></a></div></div><div class="ab cl pl pm hx pn" role="separator"><span class="po bw bk pp pq pr"/><span class="po bw bk pp pq pr"/><span class="po bw bk pp pq"/></div><div class="im in io ip iq"><p id="152e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="nt">就是这样！您刚刚学习了7种NLP技术的核心概念，以及如何用Python实现它们。本文所写的所有代码都可以在我的</em><a class="ae ky" href="https://github.com/ifrankandrade/data-science-projects.git" rel="noopener ugc nofollow" target="_blank"><em class="nt">Github</em></a><em class="nt">上找到。</em></p><p id="3cde" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><a class="ae ky" href="https://frankandrade.ck.page/bd063ff2d3" rel="noopener ugc nofollow" target="_blank"> <strong class="lb iu">与3k以上的人一起加入我的电子邮件列表，获取我在所有教程中使用的Python for Data Science备忘单(免费PDF) </strong> </a></p></div></div>    
</body>
</html>