<html>
<head>
<title>Error-in-Variables Models: Deming Regression</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">变量误差模型:戴明回归</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/error-in-variables-models-deming-regression-11ca93b6e138?source=collection_archive---------8-----------------------#2021-06-22">https://towardsdatascience.com/error-in-variables-models-deming-regression-11ca93b6e138?source=collection_archive---------8-----------------------#2021-06-22</a></blockquote><div><div class="fc ii ij ik il im"/><div class="in io ip iq ir"><h2 id="d443" class="is it iu bd b dl iv iw ix iy iz ja dk jb translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsdatascience.com/tagged/hands-on-tutorials" rel="noopener" target="_blank">实践教程</a></h2><div class=""/><div class=""><h2 id="d8c5" class="pw-subtitle-paragraph ka jd iu bd b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr dk translated">当到处都是错误的时候，得到正确的结论</h2></div><figure class="kt ku kv kw gu kx gi gj paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="gi gj ks"><img src="../Images/3d33baf95ce50290f5c84809588e6192.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*QTRfUsPnUP4_qrGz"/></div></div><p class="le lf gk gi gj lg lh bd b be z dk translated"><a class="ae li" href="https://unsplash.com/@syinq?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">苏珊·Q·尹</a>在<a class="ae li" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><h1 id="2de5" class="lj lk iu bd ll lm ln lo lp lq lr ls lt kj lu kk lv km lw kn lx kp ly kq lz ma bi translated">介绍</h1><p id="834a" class="pw-post-body-paragraph mb mc iu md b me mf ke mg mh mi kh mj mk ml mm mn mo mp mq mr ms mt mu mv mw in bi translated">机器学习通常都是关于以下问题:</p><blockquote class="mx"><p id="b024" class="my mz iu bd na nb nc nd ne nf ng mw dk translated">给定一个数据集(X，y)，其中X是特征矩阵，y是目标向量，求一个f，f(X) ≈ y。</p></blockquote><p id="6dca" class="pw-post-body-paragraph mb mc iu md b me nh ke mg mh ni kh mj mk nj mm mn mo nk mq mr ms nl mu mv mw in bi translated">因为目标值<em class="nm"> y </em>存在误差，我们通常做<strong class="md je">而不是</strong>执行严格的等式àla<em class="nm">f</em>(<em class="nm">X</em>)=<em class="nm">y</em>。这些错误源于这样一个事实，即我们通常无法观察到宇宙中的所有事物并将其放入我们的特征矩阵中。即使<em class="nm">如果</em>我们可以，量子力学告诉我们，系统中可能仍然存在随机性，这使得我们有可能在相同的输入下获得不同的结果。</p></div><div class="ab cl nn no hy np" role="separator"><span class="nq bw bk nr ns nt"/><span class="nq bw bk nr ns nt"/><span class="nq bw bk nr ns"/></div><div class="in io ip iq ir"><p id="02a4" class="pw-post-body-paragraph mb mc iu md b me nu ke mg mh nv kh mj mk nw mm mn mo nx mq mr ms ny mu mv mw in bi translated">长话短说，标签是嘈杂的，我们通过引入一个误差项<em class="nm"> ɛ、</em>来处理这个问题，比如线性回归情况下的<em class="nm"> y </em> = <em class="nm"> a*x + b + ɛ </em>。目前没有新消息。但请回答我以下问题:</p><blockquote class="mx"><p id="8fc8" class="my mz iu bd na nb nc nd ne nf ng mw dk translated">有没有想过特征变量X的错误？</p></blockquote><p id="1269" class="pw-post-body-paragraph mb mc iu md b me nh ke mg mh ni kh mj mk nj mm mn mo nk mq mr ms nl mu mv mw in bi translated">我当然没有。所以让我告诉你什么时候这很重要。</p><h1 id="8654" class="lj lk iu bd ll lm ln lo lp lq lr ls lt kj lu kk lv km lw kn lx kp ly kq lz ma bi translated"><strong class="ak">动机</strong></h1><p id="c6ff" class="pw-post-body-paragraph mb mc iu md b me mf ke mg mh mi kh mj mk ml mm mn mo mp mq mr ms mt mu mv mw in bi translated">我喜欢经典的、不起眼的例子。它们不会让你分心，让你专注于新事物，一步一步来。所以，我们用下面这个:<strong class="md je">预测一个人的身高，给定体重！以前从没做过这个，对吗？😉</strong></p></div><div class="ab cl nn no hy np" role="separator"><span class="nq bw bk nr ns nt"/><span class="nq bw bk nr ns nt"/><span class="nq bw bk nr ns"/></div><div class="in io ip iq ir"><blockquote class="nz oa ob"><p id="c2aa" class="mb mc nm md b me nu ke mg mh nv kh mj oc nw mm mn od nx mq mr oe ny mu mv mw in bi translated">你的目标是找到体重和身高的关系。如果一个人比另一个人重1公斤，你能说他们的身高是多少？你想使用线性模型，所以基本任务是计算直线的斜率。</p></blockquote><p id="8e9d" class="pw-post-body-paragraph mb mc iu md b me nu ke mg mh nv kh mj mk nw mm mn mo nx mq mr ms ny mu mv mw in bi translated"><strong class="md je">故事:</strong>你邀请了500人参加你的创新研究，并量了他们的体重和身高。</p><p id="bddc" class="pw-post-body-paragraph mb mc iu md b me nu ke mg mh nv kh mj mk nw mm mn mo nx mq mr ms ny mu mv mw in bi translated"><strong class="md je">转折:</strong>实验结束后，你注意到你的秤相当不准确——在反复称自己的体重后，你得到了很多满地的数字，尽管它们的均值至少在你的真实体重附近。其他500个重量级也是如此<em class="nm">。哎呀</em>。这种情况怎么处理？重新运行整个实验肯定是<strong class="md je">而不是</strong>一个选项。</p><figure class="kt ku kv kw gu kx gi gj paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="gi gj of"><img src="../Images/76cb8a2d3c57bff2b03c1771b53c146c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*UnEafdJpnA5PpW-G"/></div></div><p class="le lf gk gi gj lg lh bd b be z dk translated"><a class="ae li" href="https://unsplash.com/@themeinn?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">主题客栈</a>在<a class="ae li" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>拍摄的照片</p></figure><h2 id="d5e3" class="og lk iu bd ll oh oi dn lp oj ok dp lt mk ol om lv mo on oo lx ms op oq lz ja bi translated">数据</h2><p id="3907" class="pw-post-body-paragraph mb mc iu md b me mf ke mg mh mi kh mj mk ml mm mn mo mp mq mr ms mt mu mv mw in bi translated">我们从生成一些数据开始，这些数据可能是你在实验中获得的:首先，我们生成真实的重量<em class="nm"> x_true </em>，并从中导出身高<em class="nm"> y </em>。然后，我们给权重<em class="nm"> x_true </em>添加一些噪声，并将结果称为<em class="nm"> x </em>。</p><blockquote class="nz oa ob"><p id="3dff" class="mb mc nm md b me nu ke mg mh nv kh mj oc nw mm mn od nx mq mr oe ny mu mv mw in bi translated">注意:我用X代替X，因为我们这里只有一个单一的特性。</p></blockquote><pre class="kt ku kv kw gu or os ot bn ou ov bi"><span id="ce5a" class="ow lk iu os b be ox oy l oz pa">import numpy as np<br/><br/>np.random.seed(0)<br/><br/>n = 500<br/>x_true = 10*np.random.randn(n) + 70 # the true, unobserved weights<br/>y = x_true + 100 + 5*np.random.randn(n) # the observed heights<br/>x = x_true + 10*np.random.randn(n) # the noisy, observed weights</span></pre><p id="9b3e" class="pw-post-body-paragraph mb mc iu md b me nu ke mg mh nv kh mj mk nw mm mn mo nx mq mr ms ny mu mv mw in bi translated">在现实生活中，我们只被给予<em class="nm"> x </em>和<em class="nm"> y </em>来处理，所以这就是我们所看到的:</p><figure class="kt ku kv kw gu kx gi gj paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="gi gj pb"><img src="../Images/9f76fc84939872adfecaa8f7d9b5e9ef.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Bd80AlUMnhxKqaiV47rwyw.png"/></div></div><p class="le lf gk gi gj lg lh bd b be z dk translated">图片由作者提供。</p></figure><p id="e936" class="pw-post-body-paragraph mb mc iu md b me nu ke mg mh nv kh mj mk nw mm mn mo nx mq mr ms ny mu mv mw in bi translated">如果我们不知道坏秤，或者根本不关心坏秤，我们可以对有噪声的<em class="nm"> x </em>和<em class="nm"> y </em>应用线性回归模型，例如:</p><pre class="kt ku kv kw gu or os ot bn ou ov bi"><span id="a626" class="ow lk iu os b be ox oy l oz pa">from sklearn.linear_model import LinearRegression<br/><br/>lr = LinearRegression()<br/>lr.fit(x.reshape(-1, 1), y)<br/><br/>print(f'height = {lr.coef_[0]:.2f}*weight + {lr.intercept_:.2f}')<br/><br/># Output:<br/># height = 0.50*weight + 134.09</span></pre><p id="00a5" class="pw-post-body-paragraph mb mc iu md b me nu ke mg mh nv kh mj mk nw mm mn mo nx mq mr ms ny mu mv mw in bi translated">嗯，看起来是个合理的结果:体重越高，身高越高。健全性检查通过。但是，回头看看我们是如何生成高度值的:</p><pre class="kt ku kv kw gu or os ot bn ou ov bi"><span id="176f" class="ow lk iu os b be ox oy l oz pa">y = x_true + 100 + 5*np.random.randn(n)</span></pre><p id="78f8" class="pw-post-body-paragraph mb mc iu md b me nu ke mg mh nv kh mj mk nw mm mn mo nx mq mr ms ny mu mv mw in bi translated">真正的系数其实是1，截距是100，和我们运行线性回归得到的系数0.5，截距134相差甚远。如果这是一个问题，取决于你，虽然。</p><p id="3439" class="pw-post-body-paragraph mb mc iu md b me nu ke mg mh nv kh mj mk nw mm mn mo nx mq mr ms ny mu mv mw in bi translated">如果你继续使用你的坏秤，你只是想预测，目前的型号<strong class="md je">身高= 0.50 *体重+ 134.09 </strong>是正确的选择，因为它完全适应这种设置。你只是这样训练它。</p><p id="5403" class="pw-post-body-paragraph mb mc iu md b me nu ke mg mh nv kh mj mk nw mm mn mo nx mq mr ms ny mu mv mw in bi translated">另一方面，如果你想知道身高和真实体重之间的真实关系，你必须比那更聪明。问题是你的斜率太小了，也就是俗称的<a class="ae li" href="https://en.wikipedia.org/wiki/Regression_dilution" rel="noopener ugc nofollow" target="_blank">回归稀释或者回归衰减</a>。我们现在来看看如何解决这个问题。</p><h1 id="b3cc" class="lj lk iu bd ll lm ln lo lp lq lr ls lt kj lu kk lv km lw kn lx kp ly kq lz ma bi translated">用戴明回归法修复量表</h1><p id="46c0" class="pw-post-body-paragraph mb mc iu md b me mf ke mg mh mi kh mj mk ml mm mn mo mp mq mr ms mt mu mv mw in bi translated">即使是脏数据，也有几种方法可以检索正确的系数。最简单的一种是所谓的<a class="ae li" href="https://en.wikipedia.org/wiki/Deming_regression" rel="noopener ugc nofollow" target="_blank">戴明回归</a>，这是普通最小二乘法的一种变体，用于解释误差。</p><p id="621a" class="pw-post-body-paragraph mb mc iu md b me nu ke mg mh nv kh mj mk nw mm mn mo nx mq mr ms ny mu mv mw in bi translated">戴明回归是以威廉·爱德华·戴明博士的名字命名的，顺便说一下，他甚至没有发明这种方法，但却使它流行起来。发明者<strong class="md je"> R. J .阿德考克</strong>和<strong class="md je"> C. H .库梅尔</strong>也功不可没。</p><h2 id="3e91" class="og lk iu bd ll oh oi dn lp oj ok dp lt mk ol om lv mo on oo lx ms op oq lz ja bi translated">计算直线</h2><p id="e104" class="pw-post-body-paragraph mb mc iu md b me mf ke mg mh mi kh mj mk ml mm mn mo mp mq mr ms mt mu mv mw in bi translated">在我们开始之前，让我们快速浏览一些符号。你可能已经看到了这些东西，但是让我们在这里让每个人都在同一页上。我们假设<em class="nm"> x=(x₁，x₂，x₃，…，xₙ) </em>和<em class="nm"> y=(y₁，y₂，y₃，…，yₙ) </em>是我们的数据集。然后，我们定义如下:</p><figure class="kt ku kv kw gu kx gi gj paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="gi gj pc"><img src="../Images/6555ef6ed8a5aa9bfec77f9df116056e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iSvTYn-XKlahd9c-Rya_uQ.png"/></div></div><p class="le lf gk gi gj lg lh bd b be z dk translated">图片由作者提供。</p></figure><p id="6fa4" class="pw-post-body-paragraph mb mc iu md b me nu ke mg mh nv kh mj mk nw mm mn mo nx mq mr ms ny mu mv mw in bi translated">在我们讨论戴明回归公式之前，让我强调一下，有了这些符号，我们也可以通过</p><figure class="kt ku kv kw gu kx gi gj paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="gi gj pd"><img src="../Images/16583adf1483f0a94b36365953cef745.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OMfRM3A_oCGenFO_74c1OQ.png"/></div></div><p class="le lf gk gi gj lg lh bd b be z dk translated">图片由作者提供。</p></figure><p id="671c" class="pw-post-body-paragraph mb mc iu md b me nu ke mg mh nv kh mj mk nw mm mn mo nx mq mr ms ny mu mv mw in bi translated">在Python中翻译成</p><pre class="kt ku kv kw gu or os ot bn ou ov bi"><span id="cfcb" class="ow lk iu os b be ox oy l oz pa">s_xy = ((x - x.mean()) * (y - y.mean())).sum() <br/>s_xx = ((x - x.mean())**2).sum()<br/><br/>print(f'slope = {s_xy / s_xx:.2f}')<br/><br/># Output:<br/># slope = 0.50</span></pre><p id="de2e" class="pw-post-body-paragraph mb mc iu md b me nu ke mg mh nv kh mj mk nw mm mn mo nx mq mr ms ny mu mv mw in bi translated">你也可以由此计算截距，这里我省略了。我们从sklearn已经知道是<strong class="md je"> 134.09。</strong></p></div><div class="ab cl nn no hy np" role="separator"><span class="nq bw bk nr ns nt"/><span class="nq bw bk nr ns nt"/><span class="nq bw bk nr ns"/></div><div class="in io ip iq ir"><p id="5b64" class="pw-post-body-paragraph mb mc iu md b me nu ke mg mh nv kh mj mk nw mm mn mo nx mq mr ms ny mu mv mw in bi translated">现在，让我们为戴明、阿德考克和库梅尔做一些公正的事，并提出调整后的斜率和截距:</p><figure class="kt ku kv kw gu kx gi gj paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="gi gj pe"><img src="../Images/21a85885a9c8893a7c66e98d1676dfc8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IFhudDBilFto5wvmt3V9aw.png"/></div></div><p class="le lf gk gi gj lg lh bd b be z dk translated">图片由作者提供。</p></figure><p id="1f42" class="pw-post-body-paragraph mb mc iu md b me nu ke mg mh nv kh mj mk nw mm mn mo nx mq mr ms ny mu mv mw in bi translated">好吧，看起来稍微复杂一点，但还是可以处理的。但是公式中的这个<em class="nm"> δ </em>是什么呢？可悲的是，这是这种方法的主要缺点。这不是我们可以计算的，我们只需要<em class="nm">知道</em>或者猜测。</p><p id="c295" class="pw-post-body-paragraph mb mc iu md b me nu ke mg mh nv kh mj mk nw mm mn mo nx mq mr ms ny mu mv mw in bi translated"><em class="nm"> δ </em>是<em class="nm"> y </em>的误差方差除以<em class="nm"> x </em>的误差方差的比值。我的意思是这样的:回归公式<em class="nm"> y </em> = <em class="nm"> a*x + b + ɛ </em>，包含一个正态分布的随机变量<em class="nm"> ɛ </em>，所以我们假设。σᵧ.的情况有些变化<em class="nm"/>这就是我说的<em class="nm">“</em>y中误差的方差”。</p><p id="d7e6" class="pw-post-body-paragraph mb mc iu md b me nu ke mg mh nv kh mj mk nw mm mn mo nx mq mr ms ny mu mv mw in bi translated">我们进一步假设我们观察到的权重<em class="nm"> x </em>也是通过将高斯噪声添加到真实权重<em class="nm"> x_true，</em>而获得的，真实权重也具有一些方差，即<em class="nm">“<em class="nm">x”中误差的</em>方差。</em></p></div><div class="ab cl nn no hy np" role="separator"><span class="nq bw bk nr ns nt"/><span class="nq bw bk nr ns nt"/><span class="nq bw bk nr ns"/></div><div class="in io ip iq ir"><p id="14d8" class="pw-post-body-paragraph mb mc iu md b me nu ke mg mh nv kh mj mk nw mm mn mo nx mq mr ms ny mu mv mw in bi translated">在我们的例子中，我们被给予了一切。让我粘贴我们用来创建数据集的公式:</p><pre class="kt ku kv kw gu or os ot bn ou ov bi"><span id="bc2f" class="ow lk iu os b be ox oy l oz pa">x_true = 10*np.random.randn(n) + 70<br/><br/>y = x_true + 100 + 5*np.random.randn(n)<br/>x = x_true + 10*np.random.randn(n)</span></pre><p id="d073" class="pw-post-body-paragraph mb mc iu md b me nu ke mg mh nv kh mj mk nw mm mn mo nx mq mr ms ny mu mv mw in bi translated">这里我们可以看到<em class="nm"> y </em>中误差的方差正好是<em class="nm"> </em> 5 = 25。<em class="nm"> x </em>中误差的方差为10 = 100，因此<em class="nm">δ</em>= 25/100 =<strong class="md je">1/4</strong><em class="nm">。</em>插上所有东西给我们</p><pre class="kt ku kv kw gu or os ot bn ou ov bi"><span id="66f4" class="ow lk iu os b be ox oy l oz pa">x_mean = x.mean()<br/>y_mean = y.mean()<br/><br/>s_xy = ((x - x.mean()) * (y - y.mean())).sum() <br/>s_xx = ((x - x.mean())**2).sum()<br/>s_yy = ((y - y.mean())**2).sum()<br/><br/>delta = 1/4<br/><br/>deming_slope = (s_yy - delta*s_xx + np.sqrt((s_yy - delta*s_xx)**2 + 4*delta*s_xy**2)) / (2*s_xy)<br/>deming_intercept = y_mean - x_mean*deming_slope<br/><br/>print(f'Deming slope = {deming_slope:.2f}')<br/>print(f'Deming intercept = {deming_intercept:.2f}')<br/><br/># Output:<br/># Deming slope = 1.03<br/># Deming intercept = 97.01</span></pre><p id="2777" class="pw-post-body-paragraph mb mc iu md b me nu ke mg mh nv kh mj mk nw mm mn mo nx mq mr ms ny mu mv mw in bi translated">这已经足够接近真实值1和100了！当然，这不是100%准确，但这是最大似然估计的通常处理方式。我们可以想象结果:</p><figure class="kt ku kv kw gu kx gi gj paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="gi gj pb"><img src="../Images/d7ca8330e3eb0c3f5c7fb2c33dc35842.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*B07B5Wm-pVw52UmboyAgOw.png"/></div></div><p class="le lf gk gi gj lg lh bd b be z dk translated">图片由作者提供。</p></figure><p id="66ab" class="pw-post-body-paragraph mb mc iu md b me nu ke mg mh nv kh mj mk nw mm mn mo nx mq mr ms ny mu mv mw in bi translated">我们可以再次看到，在噪声数据上训练的回归线过于平坦——所谓的回归稀释。</p><h2 id="e99a" class="og lk iu bd ll oh oi dn lp oj ok dp lt mk ol om lv mo on oo lx ms op oq lz ja bi translated">观察</h2><p id="9701" class="pw-post-body-paragraph mb mc iu md b me mf ke mg mh mi kh mj mk ml mm mn mo mp mq mr ms mt mu mv mw in bi translated">我之前说过戴明回归是一元简单线性回归的<em class="nm">变体</em>。更准确的说，甚至是线性回归的<strong class="md je">推广</strong>。我的意思是:</p><blockquote class="mx"><p id="88c9" class="my mz iu bd na nb nc nd ne nf ng mw dk translated">如果特征<em class="pf"> x </em>中没有<strong class="ak">错误</strong>，戴明回归和简单线性回归产生相同的结果。</p></blockquote><p id="99ed" class="pw-post-body-paragraph mb mc iu md b me nh ke mg mh ni kh mj mk nj mm mn mo nk mq mr ms nl mu mv mw in bi translated">对我来说，这是相当直观的，但看公式，很难看出。我不会在这里证明，但我会努力让你相信。</p></div><div class="ab cl nn no hy np" role="separator"><span class="nq bw bk nr ns nt"/><span class="nq bw bk nr ns nt"/><span class="nq bw bk nr ns"/></div><div class="in io ip iq ir"><p id="22c5" class="pw-post-body-paragraph mb mc iu md b me nu ke mg mh nv kh mj mk nw mm mn mo nx mq mr ms ny mu mv mw in bi translated">如果<em class="nm"> x </em>没有误差，那么<em class="nm"> x </em>中误差的方差是<strong class="md je">零</strong>，这意味着实际上没有我们可以计算的<em class="nm"> δ </em>，因为我们会除以零。但是让我们天真地说<em class="nm"> δ </em>在这种情况下是<strong class="md je">无穷大</strong>因为对于<em class="nm">s</em>T38】0我们有</p><figure class="kt ku kv kw gu kx gi gj paragraph-image"><div class="gi gj pg"><img src="../Images/6acc7778e8cb06bab1224c3ba0b965d3.png" data-original-src="https://miro.medium.com/v2/resize:fit:888/format:webp/1*StBd11fw2kEFCL0A_jQRrQ.png"/></div><p class="le lf gk gi gj lg lh bd b be z dk translated">图片由作者提供。</p></figure><p id="4170" class="pw-post-body-paragraph mb mc iu md b me nu ke mg mh nv kh mj mk nw mm mn mo nx mq mr ms ny mu mv mw in bi translated">或者对非数学家来说:</p><blockquote class="mx"><p id="1560" class="my mz iu bd na nb nc nd ne nf ng mw dk translated">一个正数除以一个极小的正数就是一个大数。</p></blockquote><p id="e8bd" class="pw-post-body-paragraph mb mc iu md b me nh ke mg mh ni kh mj mk nj mm mn mo nk mq mr ms nl mu mv mw in bi translated">由于我们不能直接插入无穷大，所以我们使用了一个穷人的技巧，通过将<em class="nm"> δ </em>设置为一个巨大的数字来模拟<em class="nm">。</em></p><pre class="kt ku kv kw gu or os ot bn ou ov bi"><span id="d19f" class="ow lk iu os b be ox oy l oz pa">x_mean = x.mean()<br/>y_mean = y.mean()<br/><br/>s_xy = ((x - x.mean()) * (y - y.mean())).sum() <br/>s_xx = ((x - x.mean())**2).sum()<br/>s_yy = ((y - y.mean())**2).sum()<br/><br/>delta = 99999<br/><br/>deming_slope = (s_yy - delta*s_xx + np.sqrt((s_yy - delta*s_xx)**2 + 4*delta*s_xy**2)) / (2*s_xy)<br/>deming_intercept = y_mean - x_mean*deming_slope<br/><br/>print(f'Deming slope = {deming_slope:.2f}')<br/>print(f'Deming intercept = {deming_intercept:.2f}')<br/><br/># Output:<br/># Deming slope = 0.50<br/># Deming intercept = 134.09</span></pre><p id="980a" class="pw-post-body-paragraph mb mc iu md b me nu ke mg mh nv kh mj mk nw mm mn mo nx mq mr ms ny mu mv mw in bi translated">这正是我们简单的线性回归结果，好看！<em class="nm">小心:</em> <strong class="md je">这不是一张</strong>的校样，然而却给人一种美好的感觉。</p></div><div class="ab cl nn no hy np" role="separator"><span class="nq bw bk nr ns nt"/><span class="nq bw bk nr ns nt"/><span class="nq bw bk nr ns"/></div><div class="in io ip iq ir"><p id="4ba2" class="pw-post-body-paragraph mb mc iu md b me nu ke mg mh nv kh mj mk nw mm mn mo nx mq mr ms ny mu mv mw in bi translated"><strong class="md je">再想想另一个极端:</strong>如果特征的误差特别大，比如说<em class="nm">无穷大</em>会怎么样？嗯，然后<em class="nm"> δ </em>变为零<em class="nm"> </em>整个戴明斜率项也变为零，这很容易计算(做吧！).截距就是标签<em class="nm"> y </em>的平均值。基本上，模型只是一条平面线。</p><p id="190c" class="pw-post-body-paragraph mb mc iu md b me nu ke mg mh nv kh mj mk nw mm mn mo nx mq mr ms ny mu mv mw in bi translated">这在直觉上也是有意义的，因为这些特性在这种情况下基本上是无用的，所以最好忽略它们，就像根本没有特性一样。而没有使平方和误差最小化的特征的最佳模型只是常数均值预测。</p><h1 id="5898" class="lj lk iu bd ll lm ln lo lp lq lr ls lt kj lu kk lv km lw kn lx kp ly kq lz ma bi translated">结论</h1><p id="cf71" class="pw-post-body-paragraph mb mc iu md b me mf ke mg mh mi kh mj mk ml mm mn mo mp mq mr ms mt mu mv mw in bi translated">在本文中，我们介绍了特性<em class="nm"> X </em>中的错误问题，以及它们如何影响变量之间的真实关系。简单线性回归产生的斜率通常大大低于真实值，称为回归稀释。</p><p id="81b6" class="pw-post-body-paragraph mb mc iu md b me nu ke mg mh nv kh mj mk nw mm mn mo nx mq mr ms ny mu mv mw in bi translated">然而，聪明人发明了另一种回归来调整这些错误:戴明回归。这使得仍然可以得出正确的结论，而不需要另一次重新运行实验，这可以节省时间、金钱和精力。</p><p id="1003" class="pw-post-body-paragraph mb mc iu md b me nu ke mg mh nv kh mj mk nw mm mn mo nx mq mr ms ny mu mv mw in bi translated">戴明回归很棒，但是猜测<em class="nm"> δ </em>的正确值很棘手，这是一个主要缺点，因为回归的结果严重依赖于<em class="nm"> δ </em>。尽管如此，你甚至可以在特性出错的情况下发表声明，这很好。</p><p id="4109" class="pw-post-body-paragraph mb mc iu md b me nu ke mg mh nv kh mj mk nw mm mn mo nx mq mr ms ny mu mv mw in bi translated">现在一个自然的问题是:<strong class="md je">如果我们有一个以上的特征，会发生什么？</strong>数学家为我们介绍了<a class="ae li" href="https://en.wikipedia.org/wiki/Total_least_squares" rel="noopener ugc nofollow" target="_blank">总体最小二乘法</a>，这是对多个变量的戴明回归的直接推广。还有更多的型号，正如你在这里看到的<a class="ae li" href="https://en.wikipedia.org/wiki/Errors-in-variables_models" rel="noopener ugc nofollow" target="_blank"/>。</p><p id="08a8" class="pw-post-body-paragraph mb mc iu md b me nu ke mg mh nv kh mj mk nw mm mn mo nx mq mr ms ny mu mv mw in bi translated">现在你知道了，关注你所有测量中的<strong class="md je">误差，并采取相应措施尽可能地削弱它们的影响是有意义的。</strong></p></div><div class="ab cl nn no hy np" role="separator"><span class="nq bw bk nr ns nt"/><span class="nq bw bk nr ns nt"/><span class="nq bw bk nr ns"/></div><div class="in io ip iq ir"><p id="641b" class="pw-post-body-paragraph mb mc iu md b me nu ke mg mh nv kh mj mk nw mm mn mo nx mq mr ms ny mu mv mw in bi translated">我希望你今天学到了新的、有趣的、有用的东西。感谢阅读！</p><p id="aeb7" class="pw-post-body-paragraph mb mc iu md b me nu ke mg mh nv kh mj mk nw mm mn mo nx mq mr ms ny mu mv mw in bi translated"><strong class="md je">作为最后一点，如果你</strong></p><ol class=""><li id="9525" class="ph pi iu md b me nu mh nv mk pj mo pk ms pl mw pm pn po pp bi translated"><strong class="md je">想支持我多写点机器学习和</strong></li><li id="7201" class="ph pi iu md b me pq mh pr mk ps mo pt ms pu mw pm pn po pp bi translated"><strong class="md je">无论如何都要计划获得中等订阅量，</strong></li></ol><p id="2153" class="pw-post-body-paragraph mb mc iu md b me nu ke mg mh nv kh mj mk nw mm mn mo nx mq mr ms ny mu mv mw in bi translated"><strong class="md je">为什么不做</strong> <a class="ae li" href="https://dr-robert-kuebler.medium.com/membership" rel="noopener"> <strong class="md je">通过这个环节</strong> </a> <strong class="md je">？这将对我帮助很大！😊</strong></p><p id="724e" class="pw-post-body-paragraph mb mc iu md b me nu ke mg mh nv kh mj mk nw mm mn mo nx mq mr ms ny mu mv mw in bi translated">透明地说，给你的价格不变，但大约一半的订阅费直接归我。</p><p id="bab5" class="pw-post-body-paragraph mb mc iu md b me nu ke mg mh nv kh mj mk nw mm mn mo nx mq mr ms ny mu mv mw in bi translated">非常感谢，如果你考虑支持我的话！</p><blockquote class="mx"><p id="2e5d" class="my mz iu bd na nb nc nd ne nf ng mw dk translated"><em class="pf">有问题就在</em><a class="ae li" href="https://www.linkedin.com/in/dr-robert-k%C3%BCbler-983859150/" rel="noopener ugc nofollow" target="_blank"><em class="pf">LinkedIn</em></a><em class="pf">上写我！</em></p></blockquote></div></div>    
</body>
</html>