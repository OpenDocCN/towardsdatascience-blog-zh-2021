<html>
<head>
<title>Data Transformation Methods: Deep Neural Networks for Tabular Data</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">数据转换方法:表格数据的深度神经网络</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/data-transformation-methods-deep-neural-networks-for-tabular-data-8d9ebdeacc16?source=collection_archive---------20-----------------------#2021-10-18">https://towardsdatascience.com/data-transformation-methods-deep-neural-networks-for-tabular-data-8d9ebdeacc16?source=collection_archive---------20-----------------------#2021-10-18</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="7fef" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">深度学习模型的未征服城堡</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/96e30a078bf304ac5d62c50bad7e8255.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*pKIsSQWrEjrIcO_g"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">阿瑟尼·托古列夫在<a class="ae kv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="f894" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">深度学习最近取得了大量成功，特别是在同质数据集上。他们在图像/音频的分类和数据生成方面表现突出。然而，对于深度神经网络模型来说，表格数据集仍然是一个未征服的城堡。</p><p id="71ee" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">表格数据是异构的，可能导致密集的数字和稀疏的分类特征。此外，特征之间的相关性弱于图像或语音数据中的空间或语义关系。</p><p id="1dde" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">然而，我们看到异构数据在许多关键应用中的普遍使用，包括<a class="ae kv" href="https://arxiv.org/abs/2011.03274" rel="noopener ugc nofollow" target="_blank">医疗诊断</a>、<a class="ae kv" href="https://arxiv.org/abs/2012.15330" rel="noopener ugc nofollow" target="_blank">金融应用预测分析</a>和<a class="ae kv" href="https://arxiv.org/abs/1703.04247" rel="noopener ugc nofollow" target="_blank">点击率预测</a>。</p><h1 id="b087" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">用表格数据学习的挑战</h1><p id="00fa" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">表格数据或异构数据不同于同构数据，因为它们包含各种属性，包括连续属性和分类属性。由于深度学习模型无法实现相同水平的预测质量，因此存在不同的研究挑战或原因。</p><h2 id="33e7" class="mp lt iq bd lu mq mr dn ly ms mt dp mc lf mu mv me lj mw mx mg ln my mz mi na bi translated">不适当的训练数据</h2><p id="61ea" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">数据质量是表格数据集中的主要问题之一，正因为如此，我们在表格数据中发现了许多缺失值。它们通常包含异常值，并且相对于从数据中生成的高维特征向量而言具有较小的大小。</p><h2 id="4fbf" class="mp lt iq bd lu mq mr dn ly ms mt dp mc lf mu mv me lj mw mx mg ln my mz mi na bi translated">复杂的不规则空间依赖</h2><p id="81a2" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">在表格数据集中，我们无法找到变量之间的空间相关性。大多数情况下，依赖关系是复杂和不规则的。所以像卷积神经网络这样的方法无法对表格数据建模。</p><h2 id="296d" class="mp lt iq bd lu mq mr dn ly ms mt dp mc lf mu mv me lj mw mx mg ln my mz mi na bi translated">广泛的预处理</h2><p id="2469" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">另一个重要的挑战是表格数据中分类属性的转换。大多数情况下，一次性编码方案用于转换分类数据。然而，它产生一个稀疏矩阵，并可能引发维数灾难。此外，数据扩充对于表格数据的应用也非常具有挑战性。在这篇博客中，我们将针对这一挑战。</p><h2 id="32f5" class="mp lt iq bd lu mq mr dn ly ms mt dp mc lf mu mv me lj mw mx mg ln my mz mi na bi translated">模型灵敏度</h2><p id="5e09" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">与基于树的方法相比，深度神经网络对输入数据的微小变化极其敏感。因此，深度神经网络具有高曲率决策边界。与基于树的方法相比，深度学习模型中的超参数数量也很高，这使得调整的计算成本很高。</p><h1 id="ec5f" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">数据转换模型</h1><p id="4ca3" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">大多数深度学习方法用于转换表格数据的任务。表格数据的挑战在于分类属性的存在，因为神经网络只接受实数作为输入。因此，需要一种方法将这些类别转换成数字格式。为此使用了两种不同的形式，包括<em class="nb">确定性</em>和<em class="nb">自动</em>技术。</p><h2 id="7825" class="mp lt iq bd lu mq mr dn ly ms mt dp mc lf mu mv me lj mw mx mg ln my mz mi na bi translated">确定性技术</h2><p id="e204" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">标签或顺序编码是最常用的确定性技术，其中每个类别都映射到特定的数字。例如:{ '芒果'，'菠萝' }编码为{0，1}。然而，这产生了对神经网络无用的人工排序。</p><p id="2e34" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">另一种方法是通过一热编码进行编码。在我们的例子中，“芒果”到(1，0)，而“菠萝”到(0，1)。这种方法导致稀疏的特征向量，并可能引发维数灾难。</p><p id="71f2" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">二进制编码是用于对类别进行编码的另一种机制。扩大我们的例子，如果我们添加另一种水果:“香蕉”到我们的类别。二进制编码会是这样的:(01)，(10)，(11)。在这种情况下，只有<em class="nb">日志(类别)</em>新增栏目。</p><p id="7f75" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">最常用的方法是留一编码。该技术由Micci-Barreca (2001年)提出[1]。在这种方法中，每个类别都被该类别的目标变量的平均值所代替，并且当前行被排除在计算之外，以避免过拟合。这种方法也用在CatBoost算法中。</p><p id="66e5" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">还使用了另一种称为基于散列的编码的策略，它通过散列函数来转换固定大小的值。哈希函数也是确定性的。</p><h2 id="9489" class="mp lt iq bd lu mq mr dn ly ms mt dp mc lf mu mv me lj mw mx mg ln my mz mi na bi translated">自动编码</h2><p id="7952" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">有不同的自动编码方法用于编码分类属性。Yoon等人[2]提出的VIME方法就是其中之一。VIME最初确定哪些值是损坏的样本。使用生成二进制掩码向量和输入样本的掩码发生器来创建损坏的样本。注意，输入样本是从未标记的数据集生成的。</p><p id="9084" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">作者确保损坏的样本是表格形式的，并且类似于输入分布。然后，被破坏的特征被传递到编码器，该编码器生成特征表示。然后，特征向量估计器和掩码向量估计器分别生成恢复的特征和掩码。</p><p id="0a63" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">Sun等人[3]的SuperTML是使用卷积神经网络对表格数据进行自动编码的另一种方法。SuperTML将数据转换成可视数据格式，即2D矩阵或黑白图像。这些图像随后被输入微调后的2D CNN模型进行分类。该过程自动处理表格数据中的分类数据和缺失值。Zhu等人[4]采用了另一种类似的方法，利用卷积神经网络将表格数据转换成图像。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nc"><img src="../Images/e1ddc767a2b34de907e3c45655565c11.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cC3kG64g7LDYFn1EACWAMQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图来自孙等人的论文[3]</p></figure><p id="8847" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">数据转换是对表格数据建模的主要挑战之一。在这个领域有重大进展，包括确定性和动态方法。然而，这一领域的新想法也很受欢迎。</p><p id="2d55" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">感谢阅读我的文章。直到下一次…</p><p id="fbdc" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">快乐阅读！</p><h2 id="8164" class="mp lt iq bd lu mq mr dn ly ms mt dp mc lf mu mv me lj mw mx mg ln my mz mi na bi translated">学分:</h2><p id="74b9" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">内容灵感来自瓦迪姆等人的论文:<a class="ae kv" href="https://arxiv.org/pdf/2110.01889.pdf" rel="noopener ugc nofollow" target="_blank">深度神经网络和表格数据:调查</a>，(2021)。</p><h2 id="b51f" class="mp lt iq bd lu mq mr dn ly ms mt dp mc lf mu mv me lj mw mx mg ln my mz mi na bi translated">参考资料:</h2><p id="cbab" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">[1] Micci-Barreca和Daniele，<a class="ae kv" href="https://dl.acm.org/doi/pdf/10.1145/507533.507538" rel="noopener ugc nofollow" target="_blank">分类和预测问题中高基数分类属性的预处理方案</a> (2018)，ACM SIGKDD探索通讯3</p><p id="8fbb" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">[2]尹金松、张尧、詹姆斯·乔登和米哈埃拉·范德沙尔、维梅:<a class="ae kv" href="https://vanderschaar-lab.com/papers/NeurIPS2020_VIME.pdf" rel="noopener ugc nofollow" target="_blank">将自我和半监督学习的成功扩展到表格域</a> <em class="nb"> </em> (2020)，神经信息处理系统进展</p><p id="cfec" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">[3]孙，，，，，张，，董，杨，董，<a class="ae kv" href="https://openaccess.thecvf.com/content_CVPRW_2019/papers/Precognition/Sun_SuperTML_Two-Dimensional_Word_Embedding_for_the_Precognition_on_Structured_Tabular_CVPRW_2019_paper.pdf" rel="noopener ugc nofollow" target="_blank"> Supertml:预知结构化表格数据的二维单词嵌入</a> (2019)，IEEE/CVF计算机视觉与模式识别研讨会论文集</p><p id="aba6" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">[4]朱、、Thomas Brettin、夏芳芳、Alexander Partin、Maulik Shukla、Hyunseung Yoo、Yvonne A. Evrard、James H. Doroshow和Rick L. Stevens，<a class="ae kv" href="https://www.nature.com/articles/s41598-021-90923-y" rel="noopener ugc nofollow" target="_blank">用卷积神经网络将表格数据转换成用于深度学习的图像</a> (2021)《自然科学报告》</p></div></div>    
</body>
</html>