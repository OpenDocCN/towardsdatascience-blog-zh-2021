<html>
<head>
<title>Four Deep Learning Papers to Read in June 2021</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">2021年6月要读的四篇深度学习论文</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/four-deep-learning-papers-to-read-in-june-2021-5570cc5213bb?source=collection_archive---------6-----------------------#2021-06-01">https://towardsdatascience.com/four-deep-learning-papers-to-read-in-june-2021-5570cc5213bb?source=collection_archive---------6-----------------------#2021-06-01</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="5233" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated"><strong class="ak">从贝叶斯神经网络到自我监督学习、类别选择性和来自统计学的核心思想</strong></h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/8969eaab46d01c8f139cd72c1eca6a52.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zWc1GugeE2NFl5znBclQRg.png"/></div></div></figure><p id="28f0" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">欢迎来到六月<strong class="kw iu"> </strong>版的<a class="ae lq" href="https://twitter.com/hashtag/mlcollage" rel="noopener ugc nofollow" target="_blank">【机器学习-拼贴】系列</a>，在这里我提供了不同深度学习研究流的概述。那么什么是ML拼贴呢？简单地说，我为我最近最喜欢的一篇论文起草了一张幻灯片的视觉摘要。每一周。在月底，所有由此产生的视觉拼贴都被收集在一个摘要博客帖子中。因此，我希望给你一个视觉和直观的深入了解一些最酷的趋势。五月是一个相当不错的月份，包括虚拟ICLR 2021会议、ICML审查决定以及NeurIPS截止日期。因此，让我们深入研究我在2021年5月读过的四篇最喜欢的论文，以及为什么我认为它们对深度学习的未来很重要。</p><h2 id="96c0" class="lr ls it bd lt lu lv dn lw lx ly dp lz ld ma mb mc lh md me mf ll mg mh mi mj bi translated"><strong class="ak">“过去50年最重要的统计思想是什么”</strong></h2><p id="e751" class="pw-post-body-paragraph ku kv it kw b kx mk ju kz la ml jx lc ld mm lf lg lh mn lj lk ll mo ln lo lp im bi translated"><em class="mp">作者:盖尔曼&amp;韦赫塔里(2021) </em> |📝<a class="ae lq" href="http://arxiv.org/abs/2012.00174" rel="noopener ugc nofollow" target="_blank">论文</a></p><p id="cfae" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><strong class="kw iu">一段话总结:</strong>深度学习位于统计学和计算机科学的交汇点。许多流行的核心思想源于结合计算技巧来解决基本的统计问题。例如，权重正则化在统计学中有着悠久的传统，并允许在高维回归模型中进行推断。同时，稀疏性提高了所考虑变量的可解释性。或者变分推理——在贝叶斯深度学习和变分自动编码器中非常突出——它提供了一个优化框架来近似一个难以处理的贝叶斯推理。因此，重要的是反思统计的核心思想，并问问我们自己它们是如何塑造我们今天的思维的。Gelman和Vehtari (2021)提炼出8个核心思想:反事实因果推理、基于引导/模拟的推理、过度参数化模型&amp;正则化、层次模型、通用计算算法、自适应决策、稳健推理和探索性数据分析。多好的名单啊！很难找到不属于这些类别的子领域。试试看。那么将他们团结在一起的共同主题是什么呢？它们合并多个范例，随数据缩放，并将拟合的模型与数据分开。总的来说，这是一个很好的周末读物，它提供了一个很好的历史发展概况。就我个人而言，我喜欢这一前景，包括统计的单元测试和可解释机器学习的近期努力的继续。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mq"><img src="../Images/7e93d7c1aa707bfa4edd4c0944d9c420.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mDboyBu99CCECdPXtWbyGQ.png"/></div></div><p class="mr ms gj gh gi mt mu bd b be z dk translated">ML-Collage [17/52]:作者的数字。|📝<a class="ae lq" href="https://arxiv.org/abs/2101.04882" rel="noopener ugc nofollow" target="_blank">论文</a></p></figure><h2 id="bf55" class="lr ls it bd lt lu lv dn lw lx ly dp lz ld ma mb mc lh md me mf ll mg mh mi mj bi translated"><strong class="ak">“贝叶斯神经网络后验概率到底是什么样子的”</strong></h2><p id="a3fe" class="pw-post-body-paragraph ku kv it kw b kx mk ju kz la ml jx lc ld mm lf lg lh mn lj lk ll mo ln lo lp im bi translated"><em class="mp">作者:伊兹迈洛夫等人(2021) </em> |📝<a class="ae lq" href="https://arxiv.org/abs/2104.14421" rel="noopener ugc nofollow" target="_blank">论文</a></p><p id="ec9a" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><strong class="kw iu">一段总结:</strong>贝叶斯深度学习有望提供校准的不确定性估计，以支持有效的决策和精心的预测。但这也带来了为数百万个权重参数推断高维后验概率的挑战。为了解决这个问题，人们提出了许多近似的方法。这包括深度集成，它通过组合来自多个模型的预测或平均场变分推断来获得不确定性。但是这些方法实际上有多接近真实的后验概率仍然是一个悬而未决的问题。伊兹迈洛夫等人(2021)试图通过哈密顿蒙特卡罗(HMC)和512个TPU来回答这个问题。HMC提供了一个梯度引导的抽样程序，它规模很好，并保证从真实的后验渐近产生样本。在大规模并行化的推动下，作者研究了关于后验混合、链数以及后验近似质量的基本问题。他们发现更长的链和更平行的链有助于推断更准确的后验概率。在多项基准测试中，HMC的表现优于所有其他被考虑的基准。作者指出，许多技术提供的置信度估计不一定反映它们在分类任务上的准确性。以前有人认为,“冷”回火后验分布——由温度系数锐化——表现明显更好。伊兹迈洛夫等人(2021年)表明，这实际上是一个假象，可以通过在CNN中禁用数据增强和使用滤波器响应归一化来克服。绝对是所有贝叶斯深度学习爱好者的推荐读物。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mv"><img src="../Images/02d41c4a7ce8a81ca555e94b957cb6a4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*h4iFCv7kcXT7SzaJuRmapw.png"/></div></div><p class="mr ms gj gh gi mt mu bd b be z dk translated">ML-Collage [19/52]:数字改编自伊兹迈洛夫等人(2021) |📝<a class="ae lq" href="https://arxiv.org/abs/2104.14421" rel="noopener ugc nofollow" target="_blank">论文</a></p></figure><h2 id="8d3d" class="lr ls it bd lt lu lv dn lw lx ly dp lz ld ma mb mc lh md me mf ll mg mh mi mj bi translated">“巴洛双胞胎:通过减少冗余的自我监督学习”</h2><p id="bdf7" class="pw-post-body-paragraph ku kv it kw b kx mk ju kz la ml jx lc ld mm lf lg lh mn lj lk ll mo ln lo lp im bi translated"><em class="mp">作者:Zbontar，Ling等(2021) </em> |📝<a class="ae lq" href="http://arxiv.org/abs/2103.03230" rel="noopener ugc nofollow" target="_blank">纸张</a> |🤖<a class="ae lq" href="http://github.com/facebookresearch/barlowtwins" rel="noopener ugc nofollow" target="_blank">代码</a></p><p id="c4ac" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><strong class="kw iu">一段话总结:</strong>还记得那个<a class="ae lq" href="https://www.google.com/url?sa=i&amp;url=https://medium.com/syncedreview/yann-lecun-cake-analogy-2-0-a361da560dae&amp;psig=AOvVaw2MLuERiuuJ41FIL8kZsqsL&amp;ust=1622642446095000&amp;source=images&amp;cd=vfe&amp;ved=0CA0QjhxqFwoTCOCByKbM9vACFQAAAAAdAAAAABAD" rel="noopener ugc nofollow">臭名昭著的乐村蛋糕</a>以及相关的预测自我监督学习(SSL)是深度学习的未来吗？SSL的核心是通过引导以前收集的数据样本来解决信用分配信号稀疏的问题。例如，这可以是视频序列预测、去噪或对先前灰度图像着色的形式。SSL方法的一个流行前身是<a class="ae lq" href="https://www.cs.cmu.edu/~rsalakhu/papers/oneshot1.pdf" rel="noopener ugc nofollow" target="_blank">暹罗网络</a>。在暹罗网络中，人们扭曲样本，并训练网络生成与原始样本的扭曲版本接近的表示。因此，网络被推动以捕捉核心特征并压缩输入。在现代版本中，这种联合嵌入训练通常依赖于一系列技巧，例如在两种表示生成机制之间引入不对称性。Zbontar，Jing等人(2021)通过提出一个简单的信息瓶颈目标来克服这些诡计，该目标作用于嵌入表示的互相关矩阵。更具体地，对角线元素被推到接近1，而非对角线元素因大于0而被惩罚。因此，两件事情同时完成:表示对于失真应该变得不变，并且不必要的分量的冗余被强制减少。作者在各种半监督学习任务上测试了他们的方法，并表明与其他基线不同，提出的Barlow Twin目标即使在小批量的情况下也能很好地工作，并且它随着表示的维度而不断改进。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mw"><img src="../Images/65270687cf9a70d35afd2fd788a21b31.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QkWPeUEiByb8gNsU-izDjA.png"/></div></div><p class="mr ms gj gh gi mt mu bd b be z dk translated">ML-Collage [19/52]:数字改编自Zbontar等人(2021) |📝<a class="ae lq" href="http://arxiv.org/abs/2103.03230" rel="noopener ugc nofollow" target="_blank">论文</a></p></figure><h2 id="463d" class="lr ls it bd lt lu lv dn lw lx ly dp lz ld ma mb mc lh md me mf ll mg mh mi mj bi translated"><strong class="ak">“被认为有害的选择性:评估DNNs中类别选择性的因果影响”</strong></h2><p id="f393" class="pw-post-body-paragraph ku kv it kw b kx mk ju kz la ml jx lc ld mm lf lg lh mn lj lk ll mo ln lo lp im bi translated"><em class="mp">作者:莱维特和莫科斯(2021) </em>📝<a class="ae lq" href="https://arxiv.org/abs/2003.01262" rel="noopener ugc nofollow" target="_blank">纸张</a> |💻<a class="ae lq" href="https://ai.facebook.com/blog/easy-to-interpret-neurons-may-hinder-learning-in-deep-neural-networks" rel="noopener ugc nofollow" target="_blank">博客</a></p><p id="6565" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><strong class="kw iu">一段话总结:</strong> <a class="ae lq" href="https://openai.com/blog/microscope/" rel="noopener ugc nofollow" target="_blank"> OpenAI的显微镜</a>项目试图弄清楚人工网络中复杂的内部工作原理。他们还提供了一套美丽的可视化论证非常选择性活跃的神经元的存在和功能。这种现象让人想起在大脑中发现的著名的<a class="ae lq" href="https://www.brainlatam.com/blog/what-does-jennifer-aniston-neuron-tell-us-about-the-brain-machine-interface-467" rel="noopener ugc nofollow" target="_blank">詹妮弗·安妮斯顿神经元</a>。但是，对于神经网络正常工作来说，学习神经元类别选择性是必要的和/或充分的吗？Leavitt和Morcos (2021)通过引入正则化项来研究这个基本问题，正则化项促进/抑制神经元或特征图变得有选择性。在一组实验中，他们能够表明，如果类别选择性降低，在TinyImageNet上训练的ResNets的测试准确性可以提高。此外，如果选择性太高，甚至会损害性能。因此，尽管有很强的选择性，但神经网络似乎更有效。但是，网络能简单地学会“隐藏”正则化子的选择性吗，例如通过神经元的线性组合来分配它？作者表明这一假设没有证据。为此，他们比较了用不同正则化强度训练的神经网络的<a class="ae lq" href="https://papers.nips.cc/paper/2018/file/a7a3d70c6d17a73140918996d03c014f-Paper.pdf" rel="noopener ugc nofollow" target="_blank"> CCA转换表示</a>。神经网络不是隐藏选择性，而是通过非仿射表示变换来抑制它。那么这对深度学习研究意味着什么呢？也许我们应该对神经/拟人化更谨慎一点，并尊重我们自己在分析人工神经网络时的偏见。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mx"><img src="../Images/e2c29be7f1eac38aae54a27fdca8e3ef.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gnUglVwHz_wh3b0KmXf_BQ.png"/></div></div><p class="mr ms gj gh gi mt mu bd b be z dk translated">ML-Collage [20/52]:数字改编自Leavitt和Morcos (2021) |📝<a class="ae lq" href="https://arxiv.org/abs/2003.01262" rel="noopener ugc nofollow" target="_blank">论文</a></p></figure></div><div class="ab cl my mz hx na" role="separator"><span class="nb bw bk nc nd ne"/><span class="nb bw bk nc nd ne"/><span class="nb bw bk nc nd"/></div><div class="im in io ip iq"><p id="c978" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">这是这个月的🤗让我知道你最喜欢的论文是什么。如果你想获得一些每周ML拼贴输入，查看Twitter上的标签<a class="ae lq" href="https://twitter.com/hashtag/mlcollage" rel="noopener ugc nofollow" target="_blank"># ML collage</a>。你也可以在最后的总结中找到拼贴画📖博客帖子:</p><div class="nf ng gp gr nh ni"><a rel="noopener follow" target="_blank" href="/four-deep-learning-papers-to-read-in-may-2021-706e02071473"><div class="nj ab fo"><div class="nk ab nl cl cj nm"><h2 class="bd iu gy z fp nn fr fs no fu fw is bi translated">2021年5月要读的四篇深度学习论文</h2><div class="np l"><p class="bd b dl z fp nn fr fs no fu fw dk translated">towardsdatascience.com</p></div></div><div class="nq l"><div class="nr l ns nt nu nq nv ks ni"/></div></div></a></div></div></div>    
</body>
</html>