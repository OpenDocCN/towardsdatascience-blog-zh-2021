<html>
<head>
<title>How to avoid Memory errors with Pandas</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如何避免熊猫的记忆错误</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/how-to-avoid-memory-errors-with-pandas-22366e1371b1?source=collection_archive---------0-----------------------#2021-05-03">https://towardsdatascience.com/how-to-avoid-memory-errors-with-pandas-22366e1371b1?source=collection_archive---------0-----------------------#2021-05-03</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="fe6a" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">使用中型和大型数据集时缩放熊猫的一些策略</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/7a19ed3fb637296a8343af9f55f8aa70.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*YoJZhXIg7mW-MREj"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">斯蒂芬妮·克莱帕奇在<a class="ae kv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="0581" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">TL；如果你经常用熊猫耗尽内存或者有代码执行缓慢的问题，你可以通过测试手动方法来自娱自乐，或者你可以使用<a class="ae kv" href="https://www.terality.com/" rel="noopener ugc nofollow" target="_blank"> Terality </a>在5分钟之内解决它。我不得不艰难地发现这一点。</strong></p><h1 id="e8b0" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">上下文:探索未知数据集</h1><p id="e1e6" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">最近，我打算从一个流行的视频游戏中探索一个包含720，000行和72列的数据集:这个想法是为了发现玩家的策略中是否有任何一致的模式。但是由于内存错误，我花了更多的时间试图加载数据，而不是实际探索数据。我很清楚数据分析的80/20 <a class="ae kv" href="https://www.ibm.com/cloud/blog/ibm-data-catalog-data-scientists-productivity" rel="noopener ugc nofollow" target="_blank">法则</a>，你的大部分时间都花在探索数据上——我对此没意见。我一直认为，一个新的数据集就像探索一个新的国家，有它自己的背景和习俗，你必须破译，以便解释或发现一些模式；但是在这种情况下，我甚至不能开始工作。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mp"><img src="../Images/ab0a87b3877fe1979c54ef6f18abb27b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*gAnrPsqfGc7qeNC9"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="9e64" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">当我的数据准备好被处理时，我开始遇到一些问题，因为一些熊猫功能需要比我的机器可用的内存更多的内存来运行。例如，当我想对一列进行排序时，内存不足。鉴于这不是我第一次遇到这种情况，我应用了常用的技术来解决这个问题。首先，我将解释在我发现Terality之前我试图做什么来解决这个问题。</p><h1 id="e5c1" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">策略1:加载较少的数据(子采样)</h1><p id="d3ba" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">解决这类问题的一个策略是通过减少数据集中的行数或列数来减少数据量。然而，在我的例子中，我只加载了20%的可用数据，所以这不是一个选项，因为我会在我的数据集中排除太多重要的元素。</p><h1 id="c605" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">策略2:垂直扩展</h1><p id="b8d4" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">如果您不能或不应该使用更少的数据，并且您有缺乏资源的问题，您有两个选择:纵向扩展，这意味着向您的环境添加更多的物理资源(在这种情况下是更多的RAM )(即在一台更大的计算机上工作),或者横向扩展，这意味着将问题分配到几个更小的、协调的实例中。垂直缩放更容易，因为你只需将<a class="ae kv" href="https://www.urbandictionary.com/define.php?term=plug%20and%20pray" rel="noopener ugc nofollow" target="_blank"> <em class="mq">插入</em>新硬件<em class="mq">并以与之前相同的方式玩</em> </a>，只是更快更好。</p><p id="97fb" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我以前使用Google Colab作为我的默认选项来垂直扩展我的资源。它提供了一个类似Jupyter的环境，免费提供12GB的RAM，对时间和GPU的使用有一些限制。由于我还不需要执行任何建模任务，只需要一个简单的<em class="mq"> Pandas </em>探索和一些转换，这看起来是完美的解决方案。但是不行，熊猫在第一次操作时就耗尽了内存。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mr"><img src="../Images/e55128db77bbee1c39597d2b2e624dd5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*ObN8nTHroSyJwXKQ"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="1b59" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">策略3:修改数据类型</p><p id="0275" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">鉴于垂直扩展还不够，我决定使用一些辅助技术。第一个是通过修改用于映射一些列的数据类型来减小数据集的大小。给定某种数据类型，例如int64，python会分配足够的内存空间来存储-9223372036854775808到9223372036854775807范围内的整数。在阅读了数据的描述之后，我将数据类型更改到最小，从而将数据集的大小减少了35%以上:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ms"><img src="../Images/bb870cf98d95865430e6619a03168c1b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1222/0*4p7VjGxkkaJcRj5W"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mt"><img src="../Images/34726167f8690d5bec8ea4c8f2d47494.png" data-original-src="https://miro.medium.com/v2/resize:fit:1244/0*ws4GzwVLe5K4r91r"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="87e4" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">应用这种技术时要小心选择:有些数据类型不会增加内存大小，事实上，它甚至会使情况变得更糟。一个常见的建议是将对象类型改为分类，但是，在我的例子中，它抵消了我以前的收获:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mu"><img src="../Images/8ce2148f71efc22e217c80a5e511f337.png" data-original-src="https://miro.medium.com/v2/resize:fit:1286/0*gyy0cKvCwL6trdqn"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><h1 id="02c0" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">策略4:横向扩展</h1><p id="204d" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">水平扩展，基本上意味着添加更多的机器，将需要我将代码分发到多个服务器上。这是一个棘手的问题，通常由map-reduce或Spark解决；但是我不知道有什么解决方案可以轻松地为熊猫的代码做到这一点。一些开源解决方案可能会有所帮助，但是它们没有一些Pandas的方法，不提供相同的语法，并且它们的范围有限。</p><p id="aa72" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">当我在谷歌上搜索新的技巧时，我发现了真实度。看起来他们是<em class="mq">街区的新成员</em>为这类问题提供了一个有趣的替代方案:管理<em class="mq">熊猫</em>的水平伸缩。换句话说，似乎Terality在幕后产生了一个机器集群，将该集群与环境连接起来，并在新的集群中运行代码。这是完美的，因为从分析师的角度来看它是隐藏的，并且不需要用新的硬件升级本地环境来修改数据或代码。或许你可以在这篇中型文章中更好地了解他们的报价。</p><p id="832c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在一个简单的入职流程之后，我准备测试这种方法。令我惊讶的是，以前在我的本地和Google Colab环境中失败的加载和合并操作运行得更快，并且没有任何Terality问题。此外，鉴于他们目前处于私人测试阶段，我不需要支付任何费用。Terality团队对其产品的其他一些大胆声明包括:</p><ul class=""><li id="7b95" class="mv mw iq ky b kz la lc ld lf mx lj my ln mz lr na nb nc nd bi translated">执行<em class="mq"> Pandas </em>代码的速度提高了100倍，即使是在大型数据集上</li><li id="142c" class="mv mw iq ky b kz ne lc nf lf ng lj nh ln ni lr na nb nc nd bi translated">全面支持熊猫API(方法、集成、错误等)。)</li><li id="e55e" class="mv mw iq ky b kz ne lc nf lf ng lj nh ln ni lr na nb nc nd bi translated">节省基础设施成本</li></ul><p id="8e7c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我的用例不够大，不足以测试所有这些功能，我也不打算在同一领域用其他工具建立一个基准。但是，使用私人测试帐户，我通过Terality完美地处理了数据集。您可以在下面的屏幕截图中查看记录的加载和合并时间:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nj"><img src="../Images/ff3af657fc0e8caffd1a371c636c9922.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*4jCpyXXgAJa7hCrm"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="d2dd" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">除了惊人的速度(超过100GB的合并输出需要1分22秒)之外，唯一的区别是dataframe是一个terality.DataFrame。集成和设置花费了不到五(5)分钟，我只需要从pypypy安装他们的客户端库，然后用import terality as pd替换import pandas as pd，其余的代码根本不需要任何更改。最后，使用Terality给了我几个好处:</p><ul class=""><li id="d2bb" class="mv mw iq ky b kz la lc ld lf mx lj my ln mz lr na nb nc nd bi translated">超级简单的设置</li><li id="b01f" class="mv mw iq ky b kz ne lc nf lf ng lj nh ln ni lr na nb nc nd bi translated">没有学习曲线—不需要更改任何代码</li><li id="1df3" class="mv mw iq ky b kz ne lc nf lf ng lj nh ln ni lr na nb nc nd bi translated">即时和无限的可扩展性</li><li id="646c" class="mv mw iq ky b kz ne lc nf lf ng lj nh ln ni lr na nb nc nd bi translated">没有需要管理的基础设施</li><li id="5b9d" class="mv mw iq ky b kz ne lc nf lf ng lj nh ln ni lr na nb nc nd bi translated">更快的pandas执行速度(但是，我需要对此进行基准测试)</li></ul><p id="1372" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我很想通过处理大于几个GB的数据集来测试这个工具的局限性，比如像新冠肺炎开放研究数据集这样的大型公共数据集。我的下一步将是测试对<em class="mq"> pandas </em>代码执行的100倍改进是否是合理的。最后，在没有设置和管理负担的水平集群中运行<em class="mq">熊猫</em>的想法不仅对独立数据科学家有吸引力，对更复杂的或企业用例也有吸引力。</p><h1 id="8297" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">结论</h1><p id="c3f6" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">有许多用例有足够的数据要处理，可以打破熊猫的本地或云环境。像许多其他数据科学家一样，我尝试了几种编码技术和工具，然后尝试了一种外部解决方案，获得了更好的结果。你现在应该试试Terality，看看它是否也是解决你的<em class="mq">熊猫</em>记忆错误的合适工具。你只需要在他们的网站上联系他们的团队<a class="ae kv" href="http://terality.com" rel="noopener ugc nofollow" target="_blank">，他们会指导你完成整个过程。</a></p></div></div>    
</body>
</html>