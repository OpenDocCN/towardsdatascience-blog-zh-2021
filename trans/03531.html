<html>
<head>
<title>How to Efficiently Re-Partition Spark DataFrames</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如何有效地重新划分火花数据帧</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/how-to-efficiently-re-partition-spark-dataframes-c036e8261418?source=collection_archive---------11-----------------------#2021-03-22">https://towardsdatascience.com/how-to-efficiently-re-partition-spark-dataframes-c036e8261418?source=collection_archive---------11-----------------------#2021-03-22</a></blockquote><div><div class="fc ii ij ik il im"/><div class="in io ip iq ir"><div class=""/><div class=""><h2 id="da4a" class="pw-subtitle-paragraph jr it iu bd b js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki dk translated">如何增加或减少火花数据帧的数量</h2></div><figure class="kk kl km kn gu ko gi gj paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gi gj kj"><img src="../Images/68a17288aaa460b510c1aacc3f5e6844.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*z17xM8uDJgqo857g"/></div></div><p class="kv kw gk gi gj kx ky bd b be z dk translated">照片由<a class="ae kz" href="https://unsplash.com/@picoftasty?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">梅姆</a>在<a class="ae kz" href="/s/photos/chunks?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><p id="713c" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">Apache Spark是一个能够在合理的时间内处理大量数据的框架。这个统一引擎的效率在很大程度上取决于它对数据集合执行的工作进行分配和并行处理的能力。</p><p id="a5b2" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">在本文中，我们将介绍Spark中的分区，并解释如何对数据帧进行重新分区。此外，我们还将讨论何时值得增加或减少Spark数据帧的分区数量，以便尽可能优化执行时间。</p></div><div class="ab cl lw lx hy ly" role="separator"><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb"/></div><div class="in io ip iq ir"><h2 id="f8d1" class="md me iu bd mf mg mh dn mi mj mk dp ml lj mm mn mo ln mp mq mr lr ms mt mu mv bi translated">简单地说，火花分割</h2><p id="6499" class="pw-post-body-paragraph la lb iu lc b ld mw jv lf lg mx jy li lj my ll lm ln mz lp lq lr na lt lu lv in bi translated">为了实现高并行性，Spark将数据分割成更小的块，称为分区，分布在Spark集群的不同节点上。每个节点可以有多个执行器，每个执行器可以执行一个任务。</p><p id="c4b2" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">将工作分配给多个执行器需要将数据划分并分布在执行器之间，这样工作可以并行完成，以便优化特定作业的数据处理。</p><h2 id="a807" class="md me iu bd mf mg mh dn mi mj mk dp ml lj mm mn mo ln mp mq mr lr ms mt mu mv bi translated">如何获得当前的分区数量</h2><p id="a467" class="pw-post-body-paragraph la lb iu lc b ld mw jv lf lg mx jy li lj my ll lm ln mz lp lq lr na lt lu lv in bi translated">在开始重新分区之前，有必要描述一下获取Spark数据帧当前分区数量的方法。例如，假设我们有以下最小火花数据帧</p><figure class="kk kl km kn gu ko"><div class="bz fq l di"><div class="nb nc l"/></div></figure><p id="dc76" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">为了获得上述数据帧的分区数量，我们只需运行以下命令</p><figure class="kk kl km kn gu ko"><div class="bz fq l di"><div class="nb nc l"/></div></figure><p id="c9dc" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">请注意，输出取决于您当前的设置和配置，因此您可能会看到不同的输出。</p><h2 id="6b78" class="md me iu bd mf mg mh dn mi mj mk dp ml lj mm mn mo ln mp mq mr lr ms mt mu mv bi translated">如何增加分区数量</h2><p id="53c8" class="pw-post-body-paragraph la lb iu lc b ld mw jv lf lg mx jy li lj my ll lm ln mz lp lq lr na lt lu lv in bi translated">如果想增加数据帧的分区，只需运行<code class="fe nd ne nf ng b"><a class="ae kz" href="https://spark.apache.org/docs/2.4.0/api/python/pyspark.sql.html#pyspark.sql.DataFrame.repartition" rel="noopener ugc nofollow" target="_blank">repartition()</a></code>函数。</p><blockquote class="nh ni nj"><p id="aea5" class="la lb nk lc b ld le jv lf lg lh jy li nl lk ll lm nm lo lp lq nn ls lt lu lv in bi translated">返回由给定分区表达式分区的新的<code class="fe nd ne nf ng b"><a class="ae kz" href="https://spark.apache.org/docs/2.4.0/api/python/pyspark.sql.html#pyspark.sql.DataFrame" rel="noopener ugc nofollow" target="_blank"><strong class="lc iv">DataFrame</strong></a></code>。产生的数据帧是散列分区的。</p></blockquote><p id="8733" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">下面的代码会将分区数量增加到1000:</p><figure class="kk kl km kn gu ko"><div class="bz fq l di"><div class="nb nc l"/></div></figure><h2 id="3513" class="md me iu bd mf mg mh dn mi mj mk dp ml lj mm mn mo ln mp mq mr lr ms mt mu mv bi translated">如何减少分区的数量</h2><p id="b49c" class="pw-post-body-paragraph la lb iu lc b ld mw jv lf lg mx jy li lj my ll lm ln mz lp lq lr na lt lu lv in bi translated">现在，如果您想对Spark数据帧进行重新分区，使其具有更少的分区，您仍然可以使用<code class="fe nd ne nf ng b">repartition()</code>然而，<strong class="lc iv">有一种更有效的方法可以做到这一点。</strong></p><p id="6cca" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated"><code class="fe nd ne nf ng b"><a class="ae kz" href="https://spark.apache.org/docs/2.4.0/api/python/pyspark.sql.html#pyspark.sql.DataFrame.coalesce" rel="noopener ugc nofollow" target="_blank">coalesce()</a></code>导致一个狭窄的依赖关系，这意味着当用于减少分区数量时，将会有<strong class="lc iv"> no shuffle，</strong>这可能是Spark中代价最大的操作之一。</p><blockquote class="nh ni nj"><p id="c73a" class="la lb nk lc b ld le jv lf lg lh jy li nl lk ll lm nm lo lp lq nn ls lt lu lv in bi translated">返回一个正好有N个分区的新的<code class="fe nd ne nf ng b"><a class="ae kz" href="https://spark.apache.org/docs/2.4.0/api/python/pyspark.sql.html#pyspark.sql.DataFrame" rel="noopener ugc nofollow" target="_blank"><strong class="lc iv">DataFrame</strong></a></code>。</p></blockquote><p id="0d57" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">在下面的例子中，我们将分区限制为100个。最初有1000个分区的Spark数据帧将被重新分区为100个分区，而不进行洗牌。我们所说的不洗牌是指100个新分区中的每一个都将被分配给10个现有分区。因此，当想要减少Spark数据帧的分区数量时，调用<code class="fe nd ne nf ng b"><a class="ae kz" href="https://spark.apache.org/docs/2.4.0/api/python/pyspark.sql.html#pyspark.sql.DataFrame.coalesce" rel="noopener ugc nofollow" target="_blank">coalesce()</a></code>会更有效。</p><figure class="kk kl km kn gu ko"><div class="bz fq l di"><div class="nb nc l"/></div></figure><h2 id="4a0f" class="md me iu bd mf mg mh dn mi mj mk dp ml lj mm mn mo ln mp mq mr lr ms mt mu mv bi translated">结论</h2><p id="a64f" class="pw-post-body-paragraph la lb iu lc b ld mw jv lf lg mx jy li lj my ll lm ln mz lp lq lr na lt lu lv in bi translated">在本文中，我们讨论了如何通过分区优化数据处理，分区允许工作分布在Spark集群的执行器上。此外，我们还探讨了增加或减少数据帧中分区数量的两种可能方法。</p><p id="4615" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated"><code class="fe nd ne nf ng b">repartition()</code>可用于增加或减少火花数据帧的分区数量。然而，<code class="fe nd ne nf ng b">repartition()</code>涉及洗牌，这是一个昂贵的操作。</p><p id="57c2" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">另一方面，当我们想要减少分区数量时，可以使用<code class="fe nd ne nf ng b">coalesce()</code>,因为这种方法不会触发Spark集群节点之间的数据洗牌，因此效率更高。</p></div></div>    
</body>
</html>