<html>
<head>
<title>Fish Weight Prediction (Regression Analysis for beginners) — Part 1</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">鱼重预测(初学者回归分析)——第一部分</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/fish-weight-prediction-regression-analysis-for-beginners-part-1-8e43b0cb07e?source=collection_archive---------8-----------------------#2021-12-20">https://towardsdatascience.com/fish-weight-prediction-regression-analysis-for-beginners-part-1-8e43b0cb07e?source=collection_archive---------8-----------------------#2021-12-20</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="441c" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">如何使用顶级线性 ML 算法(线性回归、套索回归和岭回归)构建 ML 回归模型</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/6515b0ee6c9ca054281e1952c765eb3b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*O4Ws9JkOn-mvRiqx"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">雷切尔·希斯科在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><h1 id="12ac" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated"><strong class="ak">目录</strong></h1><p id="d850" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated"><a class="ae ky" href="#5676" rel="noopener ugc nofollow">简介</a> <br/> <a class="ae ky" href="#7482" rel="noopener ugc nofollow">第 1.1 部分——构建 ML 模型管道。</a> <br/> <a class="ae ky" href="#5108" rel="noopener ugc nofollow">第 1.2 部分——分析算法和方法。</a> <br/> ∘ <a class="ae ky" href="#cf37" rel="noopener ugc nofollow">什么是线性模型？</a> <br/> ∘ <a class="ae ky" href="#22ca" rel="noopener ugc nofollow">算法对比</a> <br/> ∘ <a class="ae ky" href="#a6b5" rel="noopener ugc nofollow">评价</a> <br/> <a class="ae ky" href="#6e35" rel="noopener ugc nofollow">结论</a> <br/> <a class="ae ky" href="#753b" rel="noopener ugc nofollow">参考文献</a></p><h1 id="5676" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated"><strong class="ak">简介</strong></h1><p id="441e" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">今天，我们将使用线性模型，根据鱼的物种名称、垂直长度、对角线长度、交叉长度、高度和对角线宽度来预测(估计)鱼的重量。我将介绍解决问题的顶级城镇方法，我在之前的<a class="ae ky" href="https://medium.com/@gkeretchashvili/how-to-start-your-data-science-machine-learning-journey-2af667e96d1" rel="noopener">文章</a>中解释过。首先在 1.1 部分我将建立一个模型，然后在 1.2 部分我将尝试解释每个算法和方法是如何工作的。这是一个适合初学者的回归分析问题。了解这类问题的主要构建原则和方法，有助于构建自己的 ML 回归模型如(房价预测等。)</p><h1 id="7482" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">第 1.1 部分—构建 ML 模型管道。</h1><p id="fbf4" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">一般来说，构建 ML 模型分为 8 个步骤:如下图所示。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mn"><img src="../Images/cb9e48dbf05ef879df0db38ce7876024.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6glO89H5U-YGYpg5qXJCIg.png"/></div></div></figure><p id="2ea3" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">通常，数据科学家将 80%的时间花在前三步(也称为解释性数据分析-EDA)。大多数 ML 应用程序都遵循这一流程。简单应用和高级应用的区别在于 EDA 步骤。所以我们就按照上面提到的流水线，以解决权重预测问题为例。</p><p id="09af" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated"><strong class="lt iu">第一步:收集数据</strong></p><p id="ddb9" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">数据是可以从<a class="ae ky" href="https://www.kaggle.com/aungpyaeap/fish-market" rel="noopener ugc nofollow" target="_blank"> Kaggle </a>下载的公共数据集。</p><pre class="kj kk kl km gt mt mu mv mw aw mx bi"><span id="e9e5" class="my la it mu b gy mz na l nb nc"><strong class="mu iu">import</strong> pandas <strong class="mu iu">as</strong> pd<br/><strong class="mu iu">import</strong> seaborn <strong class="mu iu">as</strong> sns<br/><strong class="mu iu">import</strong> matplotlib.pyplot <strong class="mu iu">as</strong> plt<br/><strong class="mu iu">from</strong> itertools <strong class="mu iu">import</strong> combinations<br/><strong class="mu iu">import</strong> numpy <strong class="mu iu">as</strong> np<br/>data <strong class="mu iu">=</strong> pd<strong class="mu iu">.</strong>read_csv("Fish.csv")</span></pre><p id="f31d" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated"><strong class="lt iu">第二步:可视化数据(问自己这些问题并回答)</strong></p><ul class=""><li id="b250" class="nd ne it lt b lu mo lx mp ma nf me ng mi nh mm ni nj nk nl bi translated">数据看起来怎么样？</li></ul><pre class="kj kk kl km gt mt mu mv mw aw mx bi"><span id="33da" class="my la it mu b gy mz na l nb nc">data<strong class="mu iu">.</strong>head()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nm"><img src="../Images/4d0ce8869391c3b61c70a7f1bec90cd2.png" data-original-src="https://miro.medium.com/v2/resize:fit:902/format:webp/1*9Vj8B72UED9qD_LVh3Mm4g.jpeg"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><ul class=""><li id="f8ad" class="nd ne it lt b lu mo lx mp ma nf me ng mi nh mm ni nj nk nl bi translated">数据是否有缺失值？</li></ul><pre class="kj kk kl km gt mt mu mv mw aw mx bi"><span id="57de" class="my la it mu b gy mz na l nb nc">data<strong class="mu iu">.</strong>isna()<strong class="mu iu">.</strong>sum()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/3c6dc22f4fd5db3f3a825278d4cdc7e7.png" data-original-src="https://miro.medium.com/v2/resize:fit:236/format:webp/1*ZzK7YXomISLxW2lJ2JEvyg.jpeg"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><p id="8bda" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">注意:如你所见，在这种情况下，因为问题是针对初学者的，所以我们没有丢失值，然而，情况并不总是这样。在接下来的文章中，我们将有丢失值的数据，并有可能理解如何处理它。</p><ul class=""><li id="ecf9" class="nd ne it lt b lu mo lx mp ma nf me ng mi nh mm ni nj nk nl bi translated">数字特征的分布是什么？</li></ul><pre class="kj kk kl km gt mt mu mv mw aw mx bi"><span id="0b04" class="my la it mu b gy mz na l nb nc">data_num <strong class="mu iu">=</strong> data<strong class="mu iu">.</strong>drop(columns<strong class="mu iu">=</strong>["Species"])<br/><br/>fig, axes <strong class="mu iu">=</strong> plt<strong class="mu iu">.</strong>subplots(len(data_num<strong class="mu iu">.</strong>columns)<strong class="mu iu">//</strong>3, 3, figsize<strong class="mu iu">=</strong>(15, 6))<br/>i <strong class="mu iu">=</strong> 0<br/><strong class="mu iu">for</strong> triaxis <strong class="mu iu">in</strong> axes:<br/>    <strong class="mu iu">for</strong> axis <strong class="mu iu">in</strong> triaxis:<br/>        data_num<strong class="mu iu">.</strong>hist(column <strong class="mu iu">=</strong> data_num<strong class="mu iu">.</strong>columns[i], ax<strong class="mu iu">=</strong>axis)<br/>        i <strong class="mu iu">=</strong> i<strong class="mu iu">+</strong>1</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi no"><img src="../Images/10533ab2f67cb0a42d7ebdb60313cac2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UB0lIeJkUzoq7-2vP468fg.png"/></div></div></figure><p id="6151" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">正如你所看到的，发行版是不错的。目标变量(权重)似乎有点不平衡，可以通过一些方法来平衡这些值。这是一个有点高深的话题，让我们暂时忽略它。</p><ul class=""><li id="5a11" class="nd ne it lt b lu mo lx mp ma nf me ng mi nh mm ni nj nk nl bi translated">目标变量(重量)相对于鱼种的分布情况如何？</li></ul><pre class="kj kk kl km gt mt mu mv mw aw mx bi"><span id="b5f6" class="my la it mu b gy mz na l nb nc">sns<strong class="mu iu">.</strong>displot(<br/>  data<strong class="mu iu">=</strong>data,<br/>  x<strong class="mu iu">=</strong>"Weight",<br/>  hue<strong class="mu iu">=</strong>"Species",<br/>  kind<strong class="mu iu">=</strong>"hist",<br/>  height<strong class="mu iu">=</strong>6,<br/>  aspect<strong class="mu iu">=</strong>1.4,<br/>  bins<strong class="mu iu">=</strong>15<br/>)<br/>plt<strong class="mu iu">.</strong>show()</span><span id="5564" class="my la it mu b gy np na l nb nc">sns<strong class="mu iu">.</strong>pairplot(data, kind<strong class="mu iu">=</strong>'scatter', hue<strong class="mu iu">=</strong>'Species');</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/845564b06cbc7d0ee9e9c2cbcca95579.png" data-original-src="https://miro.medium.com/v2/resize:fit:1372/format:webp/1*HsSWtod9vCX_aSE0KuBsiQ.png"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nr"><img src="../Images/64a6dd0a656435421e1f262a762ca4a3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*J43Dv-RgHp4iCVkE8QAgwg.png"/></div></div></figure><p id="9270" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">关于物种的目标变量分布表明，有些物种，如狗鱼，与其他物种相比具有巨大的重量。这种可视化为我们提供了关于“<strong class="lt iu">物种”</strong>特征如何用于预测的额外信息。</p><ul class=""><li id="1e2d" class="nd ne it lt b lu mo lx mp ma nf me ng mi nh mm ni nj nk nl bi translated">目标变量和特征之间的相关性是什么？</li></ul><pre class="kj kk kl km gt mt mu mv mw aw mx bi"><span id="36cc" class="my la it mu b gy mz na l nb nc">plt<strong class="mu iu">.</strong>figure(figsize<strong class="mu iu">=</strong>(7,6))<br/>corr <strong class="mu iu">=</strong> data_num<strong class="mu iu">.</strong>corr()<br/>sns<strong class="mu iu">.</strong>heatmap(corr, <br/>            xticklabels<strong class="mu iu">=</strong>corr<strong class="mu iu">.</strong>columns<strong class="mu iu">.</strong>values,<br/>            yticklabels<strong class="mu iu">=</strong>corr<strong class="mu iu">.</strong>columns<strong class="mu iu">.</strong>values, annot<strong class="mu iu">=True</strong>)<br/>plt<strong class="mu iu">.</strong>show()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/b4e5824841fcf303718def4bdf23ce8c.png" data-original-src="https://miro.medium.com/v2/resize:fit:816/format:webp/1*NDIFqr2-TDalf_cwL788gQ.png"/></div></figure><p id="989a" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">列的成对相关显示所有数字特征与权重正相关。这意味着长度或宽度越高，重量就越大。这似乎也符合逻辑。</p><p id="0fee" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated"><strong class="lt iu">第三步:清理数据</strong></p><pre class="kj kk kl km gt mt mu mv mw aw mx bi"><span id="1ef2" class="my la it mu b gy mz na l nb nc"><strong class="mu iu">from</strong> sklearn.model_selection <strong class="mu iu">import</strong> train_test_split<br/><strong class="mu iu">from</strong> sklearn.compose <strong class="mu iu">import</strong> make_column_transformer<br/><strong class="mu iu">from</strong> sklearn.preprocessing <strong class="mu iu">import</strong>  OneHotEncoder, StandardScaler <br/><br/><strong class="mu iu">from</strong> sklearn.linear_model <strong class="mu iu">import</strong> LinearRegression, Ridge, Lasso<br/><strong class="mu iu">from</strong> sklearn.metrics <strong class="mu iu">import</strong> mean_squared_error, mean_absolute_error, r2_score<br/><br/>ct <strong class="mu iu">=</strong> make_column_transformer(<br/>    (StandardScaler(),['Length1','Length2','Length3','Height','Width']), <em class="nt">#turn all values from 0 to 1</em><br/>    (OneHotEncoder(handle_unknown<strong class="mu iu">=</strong>"ignore"), ["Species"])<br/>)<br/><em class="nt">#create X and y values</em><br/>data_cleaned <strong class="mu iu">=</strong>   data<strong class="mu iu">.</strong>drop("Weight",axis<strong class="mu iu">=</strong>1)<br/>y <strong class="mu iu">=</strong> data['Weight']<br/><br/>x_train, x_test, y_train, y_test <strong class="mu iu">=</strong> train_test_split(data_cleaned,y, test_size<strong class="mu iu">=</strong>0.2, random_state<strong class="mu iu">=</strong>42)<br/>print(x_train<strong class="mu iu">.</strong>shape, x_test<strong class="mu iu">.</strong>shape, y_train<strong class="mu iu">.</strong>shape, y_test<strong class="mu iu">.</strong>shape)</span><span id="4d65" class="my la it mu b gy np na l nb nc">X_train_normal <strong class="mu iu">=</strong> pd<strong class="mu iu">.</strong>DataFrame(ct<strong class="mu iu">.</strong>fit_transform(x_train))<br/>X_test_normal <strong class="mu iu">=</strong> pd<strong class="mu iu">.</strong>DataFrame(ct<strong class="mu iu">.</strong>transform(x_test))</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nu"><img src="../Images/fbba2f17cb40b4a9b5020a0bd65f206a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EI5bzI7PTLOMAhqY8m2MPA.jpeg"/></div></div></figure><p id="c44c" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">使用 OneHotEncoder 将<strong class="lt iu">物种</strong>列转换为 7 个特征。数字特征被缩放是因为线性模型喜欢缩放。(我将在文章的 1.2 部分解释细节)</p><p id="0037" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated"><strong class="lt iu">第四步:训练模型</strong></p><pre class="kj kk kl km gt mt mu mv mw aw mx bi"><span id="d0b3" class="my la it mu b gy mz na l nb nc"><strong class="mu iu">def</strong> models_score(model_name, train_data, y_train, val_data,y_val):<br/>    model_list <strong class="mu iu">=</strong> ["Linear_Regression","Lasso_Regression","Ridge_Regression"]<br/>    <em class="nt">#model_1</em><br/>    <strong class="mu iu">if</strong> model_name<strong class="mu iu">==</strong>"Linear_Regression":<br/>        reg <strong class="mu iu">=</strong> LinearRegression()<br/>    <em class="nt">#model_2</em><br/>    <strong class="mu iu">elif</strong> model_name<strong class="mu iu">==</strong>"Lasso_Regression":<br/>      reg <strong class="mu iu">=</strong> Lasso(alpha<strong class="mu iu">=</strong>0.1,tol<strong class="mu iu">=</strong>0.03)<br/>        <br/>    <em class="nt">#model_3</em><br/>    <strong class="mu iu">elif</strong> model_name<strong class="mu iu">==</strong>"Ridge_Regression":<br/>        reg <strong class="mu iu">=</strong> Ridge(alpha<strong class="mu iu">=</strong>1.0)<br/>    <strong class="mu iu">else</strong>:<br/>        print("please enter correct regressor name")<br/>        <br/>    <strong class="mu iu">if</strong> model_name <strong class="mu iu">in</strong> model_list:<br/>        reg<strong class="mu iu">.</strong>fit(train_data,y_train)<br/>        pred <strong class="mu iu">=</strong> reg<strong class="mu iu">.</strong>predict(val_data)<br/>     <br/>        score_MSE <strong class="mu iu">=</strong> mean_squared_error(pred, y_val)<br/>        score_MAE <strong class="mu iu">=</strong> mean_absolute_error(pred, y_val)<br/>        score_r2score <strong class="mu iu">=</strong> r2_score(pred, y_val)<br/>        <strong class="mu iu">return</strong> round(score_MSE,2), round(score_MAE,2), round(score_r2score,2)</span><span id="cb21" class="my la it mu b gy np na l nb nc">model_list <strong class="mu iu">=</strong> ["Linear_Regression","Lasso_Regression","Ridge_Regression"]<br/>result_scores <strong class="mu iu">=</strong> []<br/><strong class="mu iu">for</strong> model <strong class="mu iu">in</strong> model_list:<br/>    score <strong class="mu iu">=</strong> models_score(model,X_train_normal,y_train, X_test_normal,y_test)<br/>    result_scores<strong class="mu iu">.</strong>append((model, score[0], score[1],score[2]))<br/>    print(model,score)</span></pre><p id="ceb1" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">在上面的代码中，我们训练了不同的模型，并测量了均方误差(MSE)、平均绝对误差(MAE)、R 得分的准确性。</p><p id="26d4" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated"><strong class="lt iu">第五步:评估</strong></p><pre class="kj kk kl km gt mt mu mv mw aw mx bi"><span id="6935" class="my la it mu b gy mz na l nb nc">df_result_scores <strong class="mu iu">=</strong> pd<strong class="mu iu">.</strong>DataFrame(result_scores,columns<strong class="mu iu">=</strong>["model","mse","mae","r2score"])<br/>df_result_scores</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nv"><img src="../Images/1324110b6226de96767be28d62e282f3.png" data-original-src="https://miro.medium.com/v2/resize:fit:772/format:webp/1*CMtaUrpSVYjtc6wy769mbg.jpeg"/></div></figure><p id="4e2c" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">最佳模型的 MSE 和 MAE 值低，R 值高。结果显示，在该数据集中，简单线性回归的表现优于 Lasso 回归和岭回归。</p><p id="719b" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated"><strong class="lt iu">步骤 6:超参数调整</strong></p><p id="b53b" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">在这种情况下，基本线性回归模型的超参数可以被认为是学习率。它被实现为一个<a class="ae ky" href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDRegressor.html" rel="noopener ugc nofollow" target="_blank"> SGDRegressor </a>，它更新学习率和权重。所以现在最好是为山脊或套索做超参数调整。让我们做岭回归，看看它是否会击败简单线性回归的分数。</p><pre class="kj kk kl km gt mt mu mv mw aw mx bi"><span id="fcb9" class="my la it mu b gy mz na l nb nc">from scipy.stats import loguniform<br/>from sklearn.model_selection import RepeatedKFold<br/>from sklearn.model_selection import RandomizedSearchCV</span><span id="78ea" class="my la it mu b gy np na l nb nc">space = dict()<br/>space['solver'] = ['svd', 'cholesky', 'lsqr', 'sag']<br/>space['alpha'] = loguniform(1e-5, 50)<br/>model = Ridge()</span><span id="c877" class="my la it mu b gy np na l nb nc">cv = RepeatedKFold(n_splits=5, n_repeats=3, random_state=1)</span><span id="c3ae" class="my la it mu b gy np na l nb nc">search = RandomizedSearchCV(model, space, n_iter=100, <br/>     scoring='neg_mean_absolute_error', n_jobs=-1, cv=cv,                 random_state=42)<br/>result = search.fit(X_train_normal, y_train)</span><span id="a8e5" class="my la it mu b gy np na l nb nc">print('Best Score: %s' % result.best_score_)<br/>print('Best Hyperparameters: %s' % result.best_params_)</span></pre><p id="4c48" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">最好成绩:-75 分。36860.78868688667</p><p id="7dbc" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">最佳超参数:{'alpha': 0.24171039031894245，' solver': 'sag'}</p><p id="753c" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">现在让我们拟合这些超参数，看看结果。</p><pre class="kj kk kl km gt mt mu mv mw aw mx bi"><span id="e7f5" class="my la it mu b gy mz na l nb nc">reg = Ridge(alpha=0.24171039031894245, solver ="sag" )<br/>reg.fit(X_train_normal,y_train)<br/>pred = reg.predict(X_test_normal)<br/>score_MSE = mean_squared_error(pred, y_test)<br/>score_MAE = mean_absolute_error(pred, y_test)<br/>score_r2score = r2_score(pred, y_test)<br/>to_append = ["Ridge_hyper_tuned",round(score_MSE,2), round(score_MAE,2), round(score_r2score,2)]<br/>df_result_scores.loc[len(df_result_scores)] = to_append<br/>df_result_scores</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nw"><img src="../Images/e7994d43b237f48a4b8bf67f98a168ec.png" data-original-src="https://miro.medium.com/v2/resize:fit:800/format:webp/1*RRuTXVix7Vj5NF-uXYzWbg.png"/></div></figure><p id="f803" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">超参数改进了以前的缺省岭结果，但是，它仍然不如简单的线性回归。</p><p id="2392" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated"><strong class="lt iu">第七步:选择最佳模型和预测</strong></p><pre class="kj kk kl km gt mt mu mv mw aw mx bi"><span id="886c" class="my la it mu b gy mz na l nb nc"><em class="nt"># winner model</em><br/>reg <strong class="mu iu">=</strong> LinearRegression()<br/>reg<strong class="mu iu">.</strong>fit(X_train_normal,y_train)<br/>pred <strong class="mu iu">=</strong> reg<strong class="mu iu">.</strong>predict(X_test_normal)<br/></span><span id="a717" class="my la it mu b gy np na l nb nc">plt<strong class="mu iu">.</strong>figure(figsize<strong class="mu iu">=</strong>(18,7))<br/>plt<strong class="mu iu">.</strong>subplot(1, 2, 1) <em class="nt"># row 1, col 2 index 1</em><br/>plt<strong class="mu iu">.</strong>scatter(range(0,len(X_test_normal)), pred,color<strong class="mu iu">=</strong>"green",label<strong class="mu iu">=</strong>"predicted")<br/>plt<strong class="mu iu">.</strong>scatter(range(0,len(X_test_normal)), y_test,color<strong class="mu iu">=</strong>"red",label<strong class="mu iu">=</strong>"True value")<br/>plt<strong class="mu iu">.</strong>legend()<br/><br/>plt<strong class="mu iu">.</strong>subplot(1, 2, 2) <em class="nt"># index 2</em><br/>plt<strong class="mu iu">.</strong>plot(range(0,len(X_test_normal)), pred,color<strong class="mu iu">=</strong>"green",label<strong class="mu iu">=</strong>"predicted")<br/>plt<strong class="mu iu">.</strong>plot(range(0,len(X_test_normal)), y_test,color<strong class="mu iu">=</strong>"red",label<strong class="mu iu">=</strong>"True value")<br/>plt<strong class="mu iu">.</strong>legend()<br/>plt<strong class="mu iu">.</strong>show()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nx"><img src="../Images/21392c7e51fd5b26cc87da0f76fb1b99.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2-cXrOhfGojlj9u_keWvmQ.png"/></div></div></figure><p id="1f74" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">为了评估 MSE、MAE 和 R2score，我们需要将预测和实际预测可视化。为什么？因为只有 MSE 或 MAE 分数，我们不能理解模型有多好。所以在上面的可视化中，预测和真值真的很接近。这意味着模型运行良好。</p></div><div class="ab cl ny nz hx oa" role="separator"><span class="ob bw bk oc od oe"/><span class="ob bw bk oc od oe"/><span class="ob bw bk oc od"/></div><div class="im in io ip iq"><h1 id="5108" class="kz la it bd lb lc of le lf lg og li lj jz oh ka ll kc oi kd ln kf oj kg lp lq bi translated"><strong class="ak">第 1.2 部分— </strong>分析算法和方法。</h1><p id="b9a6" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">在这一部分，我将解释以上算法、方法和评估背后的所有理论部分。</p><h2 id="cf37" class="my la it bd lb ok ol dn lf om on dp lj ma oo op ll me oq or ln mi os ot lp ou bi translated"><strong class="ak">什么是线性模型？</strong></h2><p id="b6f6" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">术语线性模型意味着模型被指定为特征的线性组合。线性回归、套索回归和岭回归都是线性算法。</p><ul class=""><li id="9e71" class="nd ne it lt b lu mo lx mp ma nf me ng mi nh mm ni nj nk nl bi translated"><strong class="lt iu">什么是线性回归？</strong></li></ul><p id="7bcb" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">线性回归是基于自变量 x 预测目标变量 Y 的 ML 监督学习算法，我们举一个简单的例子，计算一下如何弄清楚它的工作原理。比如 X 是工作经验，Y 是工资。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ov"><img src="../Images/56304b4814d07c935688732d9a3aa71f.png" data-original-src="https://miro.medium.com/v2/resize:fit:764/format:webp/1*EARZDJ6Hzge9lpOSOzCQgQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><p id="18cf" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">从这个形象化的例子中，我们可能很容易理解工作经验和薪水之间的关系——如果一个人有更多的经验，他们会赚更多的钱。</p><p id="e397" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">我们有一个线性回归的假设函数</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ow"><img src="../Images/a6d6c0467f311b88e577fa0a00872d16.png" data-original-src="https://miro.medium.com/v2/resize:fit:1110/format:webp/1*klRI3-CRyZjpSbJY-PQ2pg.png"/></div></figure><p id="7f78" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">其中 a 和 b 是可训练参数(也称为系数、权重)。</p><p id="d2ed" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">a-y 轴上的截距</p><p id="3375" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">b —是一个斜率</p><p id="810d" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated"><strong class="lt iu">目标</strong>:找到 a 和 b，使得我们最小化成本函数，这与普通的最小二乘法相同(在 Sklearn 中)</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ox"><img src="../Images/905700a26e630e4391ae8e982540c339.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*MD4KsWSqdDpJqJri.gif"/></div></div></figure><p id="26bc" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">在下图中，我们可以直观地了解算法是如何工作的。在第一个特征中，我们有一个假设函数 h(x) = 32，然后计算 OLS，我们更新参数并生成假设函数 h(x) = 1.6x +29，最后它达到 h(x) = 3.6 +20。对于每个参数更新，目标是最小化 OLS。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oy"><img src="../Images/1ed42793a1fb97b2e04e84912c271374.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RCIsPyht_lQEaaC1NdCjnQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><p id="7362" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">我们如何更新 a 和 b 的值来最小化 MSE？</p><p id="5d8d" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">我们使用<strong class="lt iu">梯度下降</strong>算法更新 a 和 b，直到收敛。但是首先什么是<strong class="lt iu">梯度</strong>？</p><blockquote class="oz"><p id="d1e9" class="pa pb it bd pc pd pe pf pg ph pi mm dk translated">梯度衡量的是当你稍微改变输入时，函数的输出会有多大的变化——Lex frid man(MIT)</p></blockquote><p id="0c6c" class="pw-post-body-paragraph lr ls it lt b lu pj ju lw lx pk jx lz ma pl mc md me pm mg mh mi pn mk ml mm im bi translated">简而言之，梯度测量关于成本函数的所有权重的变化。它是一个函数的斜率，斜率越高，模型学习得越快。但是如果梯度为零，权重不再更新，因此算法停止学习。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi po"><img src="../Images/a10c834811a44e3f5892ae9bc9ac1174.png" data-original-src="https://miro.medium.com/v2/resize:fit:422/format:webp/1*dCEAwoWxQUl2iOedxNATMg.png"/></div></figure><p id="d6eb" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">a 由第二部分更新，第二部分是学习率和梯度乘法。学习率对找到局部最小值有巨大的影响。当学习率太大时，它会导致在函数的局部最小值之间来回跳跃，而如果学习率太低，它将达到局部最小值，但会花费太多时间。</p><ul class=""><li id="d681" class="nd ne it lt b lu mo lx mp ma nf me ng mi nh mm ni nj nk nl bi translated"><strong class="lt iu">拉索回归</strong></li></ul><p id="e0ab" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">Laso 是线性回归的一个微小修改。它执行 L1 正则化，这意味着它添加了相当于系数幅度绝对值的惩罚。我们为什么需要它？因为它最终将大的权重降低到小的，小的权重降低到几乎为零。理论上，当我们在训练数据中有很多特征并且我们知道并非所有特征都重要时，可以使用这种方法。Lasso 回归将有助于从大量特征中识别出几个最重要的特征。</p><ul class=""><li id="46f9" class="nd ne it lt b lu mo lx mp ma nf me ng mi nh mm ni nj nk nl bi translated"><strong class="lt iu">岭回归</strong></li></ul><p id="30f2" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">岭回归也是使用 L2 正则化对线性回归的轻微修改，这意味着它从权重平方值的总和中对模型进行惩罚。因此，这导致系数的绝对值减小，并且具有均匀分布的系数。我们为什么需要这个？如果我们有一些特征，并且我们知道它们都可能影响预测，我们可以在这种情况下使用岭回归。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pp"><img src="../Images/db7fd9dac0f99034e6124ee339bba514.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*2FFmpO8ft43hhKsX.png"/></div></div></figure><p id="53bd" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">所以线性回归是没有正则化的(对参数没有惩罚)。它有时会对某些特征赋予较高的权重，从而导致小数据集中的过度拟合。这就是为什么使用套索回归(与 L1 正则化相同)或岭回归(L2 正则化)模型来调整独立变量的权重。一般来说，如果数据特征的数量远小于样本数量(#特征&lt;&lt; #rows) then it is likely that simple linear regression would work better. However, if a number of features are not much less than a number of samples it will tend to have high variance and low accuracy, in that case, Lasso and Ridge are more likely to work better.</p><h2 id="22ca" class="my la it bd lb ok ol dn lf om on dp lj ma oo op ll me oq or ln mi os ot lp ou bi translated">Comparison of the Algorithms</h2><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pq"><img src="../Images/a0746a25af3d7fe400f6ec878f2fe660.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lcW_wZqtotUJMWOMAzgj4w.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">image by author</p></figure><h2 id="a6b5" class="my la it bd lb ok ol dn lf om on dp lj ma oo op ll me oq or ln mi os ot lp ou bi translated"><strong class="ak">评估</strong></h2><p id="73c7" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">所有回归类型的模型都有相同的评估方法。最常见的是均方误差、平均绝对误差或 R 评分。我们举个例子，手工算一下每个分数。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pr"><img src="../Images/2c15d5d7c946a9cd7c194583d0c719c0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lCmPtu7iSTWL094zppJNIg.png"/></div></div></figure><p id="9707" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">y_true = [1，5，3]</p><p id="0680" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">y_pred = [2，3，4]</p><p id="5e51" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">MSE(y_true，y _ pred)= 1/3 *[(1–2)+(5–3)+(3–4)]= 2()</p><p id="69d9" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">MAE(y_true，y _ pred)= 1/3 *[| 1–2 |+| 5–3 |+| 3–4 |]= 1.33</p><p id="19a8" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">R2score(y_true，y _ pred)= 1—[(1–2)+(5–3)+(3–4)]/[[(1–3)+(5–3)+(3–3)]]= 0.25</p><p id="56fd" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">我们的目标是找到一个具有低 MSE 或 MAE 和高 R2 得分(最好是 1)的模型。</p><h1 id="6e35" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated"><strong class="ak">结论</strong></h1><p id="f5e2" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">没有现成的公式来检测每个任务和数据集选择哪种算法。这就是为什么尝试几种算法并对每一种算法进行评估仍然很重要。但是，我们需要知道每个算法背后的直觉。正如理论所说，由于我们有大约 160 个数据点，只有 12 个特征，线性回归更有可能工作得更好，事实也是如此。然而，由于在训练特征之间有很高的线性相关性，这就是为什么 Lasso 和 Ridge 也有不错的结果。在未来的文章中，我将对不同种类的数据集进行同样的尝试，让我们看看分数将如何变化。</p><p id="99d0" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">我的 GitHub 库链接的代码是<a class="ae ky" href="https://github.com/gurokeretcha/Fish-Weight-Prediction-Beginners" rel="noopener ugc nofollow" target="_blank">这里是</a>。</p><h1 id="753b" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">参考</h1><p id="44ab" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">[1]【www.geeksforgeeks.org】T2，<a class="ae ky" href="https://www.geeksforgeeks.org/ml-linear-regression/" rel="noopener ugc nofollow" target="_blank"> ML |线性回归(2018) </a></p><p id="ce76" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">[2] GAURAV SHARMA，5 <a class="ae ky" href="https://www.analyticsvidhya.com/blog/2021/05/5-regression-algorithms-you-should-know-introductory-guide/" rel="noopener ugc nofollow" target="_blank">您应该知道的回归算法—入门指南！</a> (2021)</p><p id="d607" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">【3】徐文伟，<a class="ae ky" rel="noopener" target="_blank" href="/whats-the-difference-between-linear-regression-lasso-ridge-and-elasticnet-8f997c60cf29">Sklearn 中的线性回归、套索、山脊、ElasticNet 有什么区别？</a> (2019)</p><p id="4b33" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">[4] Niklas Donges，<a class="ae ky" href="https://builtin.com/data-science/gradient-descent" rel="noopener ugc nofollow" target="_blank">梯度下降:机器学习最流行算法的介绍</a> (2021)</p><p id="61a8" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">[5] L.E. MelkumovaS.Ya. Shatskikh，<a class="ae ky" href="https://reader.elsevier.com/reader/sd/pii/S1877705817341474?token=FE0D4CB2D87828BAAAE606D88D046F7A5D8101C5B27EF6423BAD046D59731B290A304BE066796D2E95B589067B802C2C&amp;originRegion=eu-west-1&amp;originCreation=20211220161106" rel="noopener ugc nofollow" target="_blank">比较岭估计和套索估计用于数据分析</a> (2017)第三届国际会议“信息技术和纳米技术</p><p id="848c" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">[6] Jason Brownlee，<a class="ae ky" href="https://machinelearningmastery.com/hyperparameter-optimization-with-random-search-and-grid-search/" rel="noopener ugc nofollow" target="_blank">随机搜索和网格搜索的超参数优化</a> (2020)，机器学习掌握</p><p id="01de" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">[7]Aarshay Jain，<a class="ae ky" href="https://www.analyticsvidhya.com/blog/2016/01/ridge-lasso-regression-python-complete-tutorial/" rel="noopener ugc nofollow" target="_blank">Python 中脊和套索回归的完整教程</a> (2016)</p><p id="1af0" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">[8]scikit-learn.org，<a class="ae ky" href="https://scikit-learn.org/stable/modules/linear_model.html" rel="noopener ugc nofollow" target="_blank"> 1.1。线性模型</a></p><p id="7925" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">[9]scikit-learn.org，<a class="ae ky" href="https://scikit-learn.org/stable/modules/model_evaluation.html#mean-squared-error" rel="noopener ugc nofollow" target="_blank"> 3.3.4。回归指标</a></p></div></div>    
</body>
</html>