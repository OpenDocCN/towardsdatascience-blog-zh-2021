<html>
<head>
<title>Implications of Information Theory in Machine Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">信息论在机器学习中的应用</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/the-implications-of-information-theory-in-machine-learning-707132a750e7?source=collection_archive---------11-----------------------#2021-04-12">https://towardsdatascience.com/the-implications-of-information-theory-in-machine-learning-707132a750e7?source=collection_archive---------11-----------------------#2021-04-12</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="624c" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsdatascience.com/tagged/thoughts-and-theory" rel="noopener" target="_blank">思想和理论</a></h2><div class=""/><div class=""><h2 id="ed94" class="pw-subtitle-paragraph jw iz iq bd b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn dk translated">一瞥什么是信息论，机器学习如何依赖于它。</h2></div><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ko"><img src="../Images/8e867ce19e0311332ee953a9bbebbd3d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*91rDrdACaL6Ljzwl_yuf_A.jpeg"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">来自<a class="ae le" href="https://www.pexels.com/photo/light-inside-library-590493/?utm_content=attributionCopyText&amp;utm_medium=referral&amp;utm_source=pexels" rel="noopener ugc nofollow" target="_blank"> Pexels </a>的<a class="ae le" href="https://www.pexels.com/@thepoorphotographer?utm_content=attributionCopyText&amp;utm_medium=referral&amp;utm_source=pexels" rel="noopener ugc nofollow" target="_blank"> Janko Ferlic </a>摄影</p></figure><p id="6e07" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated"><strong class="lh ja">信息</strong>。这个术语在每一个可能的场景中都出现过。但是理所当然的是，世界是靠信息运行的。那是什么？解释它的最简单的方法是通过一个例子。假设你正在杂货店购物，你已经挑选了多件商品。你知道这些物品的价格；因此这是你的原始数据。稍后，当你在柜台结账时，收银员会扫描这些物品，并给你这些物品的总价。说得详细点，收银员会用每件商品的成本来处理商品的数量，给你一个固定的数字，你可以支付。在某种程度上，收银员处理原始数据(单个商品的价格)并给你信息(最终的账单金额)。所以，我可以把信息描述为有上下文意义的经过处理的数据。</p><p id="b4a2" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">再进一步，下面是两条消息:<br/> a)我没去上班。我没去上班，因为我约了医生。很明显，第二条消息比第一条包含更多的信息。但是我如何定义“更多”是什么呢？我如何量化它？这就是信息论的用武之地！下一节将探索信息论领域和其中的重要概念。</p></div><div class="ab cl mb mc hu md" role="separator"><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg"/></div><div class="ij ik il im in"><h1 id="4bba" class="mi mj iq bd mk ml mm mn mo mp mq mr ms kf mt kg mu ki mv kj mw kl mx km my mz bi translated">信息论</h1><p id="0355" class="pw-post-body-paragraph lf lg iq lh b li na ka lk ll nb kd ln lo nc lq lr ls nd lu lv lw ne ly lz ma ij bi translated">自20世纪初以来，研究人员一直在思考量化信息，1948年，克劳德·香农发表了一篇名为“<a class="ae le" href="http://people.math.harvard.edu/~ctm/home/text/others/shannon/entropy/entropy.pdf" rel="noopener ugc nofollow" target="_blank">沟通的数学理论</a>”的非凡文章这篇论文诞生了<a class="ae le" href="https://en.wikipedia.org/wiki/Information_theory#:~:text=Information%20theory%20is%20the%20scientific,Claude%20Shannon%20in%20the%201940s." rel="noopener ugc nofollow" target="_blank">信息论</a>领域。从定义上来说，信息论是对信息的量化、存储和交流的研究。但远不止如此。它对统计物理、计算机科学、经济学等领域做出了重大贡献。</p><p id="6125" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">香农论文的主要焦点是通用通信系统，因为他发表这篇文章时正在贝尔实验室工作。它建立了相当多的重要概念，如信息熵和冗余度。目前，其核心基础应用于无损数据压缩、有损数据压缩和信道编码领域。</p><p id="5443" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">信息论中使用的技术本质上是概率性的，通常处理两个特定的量，即。熵和互信息。让我们更深入地研究这两个术语。</p><h2 id="aef4" class="nf mj iq bd mk ng nh dn mo ni nj dp ms lo nk nl mu ls nm nn mw lw no np my iw bi translated">香农熵(或仅仅是熵)</h2><p id="2a31" class="pw-post-body-paragraph lf lg iq lh b li na ka lk ll nb kd ln lo nc lq lr ls nd lu lv lw ne ly lz ma ij bi translated">熵是对随机变量的不确定性或描述变量所需信息量的度量。假设<em class="nq"> x </em>是一个离散随机变量，它可以取集合中定义的任意值，<em class="nq"> χ。</em>让我们假设这个场景中的集合是有限的。<em class="nq">T5<em class="nq">x</em>的概率分布将为<em class="nq">p(x)</em>= Pr {<em class="nq">χ</em>=<em class="nq">x</em>}，<em class="nq"> x </em> ∈ <em class="nq"> χ </em>。考虑到这一点，熵可以定义为</em></p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/4a21c7944801603347e46c70d61ab93e.png" data-original-src="https://miro.medium.com/v2/resize:fit:818/format:webp/1*7jKVSu9-rfAHfqg6dZe5Fw.png"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">熵</p></figure><p id="b43a" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">熵的单位是比特。如果你观察这个公式，熵完全依赖于随机变量的<strong class="lh ja">概率，而不是依赖于<em class="nq"> x </em>本身的值。公式前面有一个负号，使它永远为正或0。如果熵为0，则没有新的信息可以获得。我将通过一个例子演示这个公式的实现。</strong></p><p id="9ed5" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">考虑抛硬币的场景。有两种可能的结果，正面或反面，概率相等。如果我们量化一下，<em class="nq">x</em>∈<em class="nq"/>{正面，反面}，<em class="nq"> p </em>(正面)= 0.5，<em class="nq"> p </em>(反面)= 0.5。如果我们将这些值代入公式:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ns"><img src="../Images/29776be083515791875d270a552eed05.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hZtiIqkH21xJobO4TmP3vA.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">在掷硬币事件中计算熵</p></figure><p id="a361" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">因此，熵是1比特，也就是说，抛硬币的结果可以完全用1比特来表示。所以，直观地表达香农熵的概念，理解为“一条消息需要多长时间才能完整地传达其价值”。我想再深入一点，讨论一下联合熵、条件熵和相对熵的概念。</p><h2 id="b4f0" class="nf mj iq bd mk ng nh dn mo ni nj dp ms lo nk nl mu ls nm nn mw lw no np my iw bi translated">联合和条件熵</h2><p id="c0db" class="pw-post-body-paragraph lf lg iq lh b li na ka lk ll nb kd ln lo nc lq lr ls nd lu lv lw ne ly lz ma ij bi translated">之前我定义熵是针对单个随机变量的，现在我把它推广到一对随机变量。这是一个简单的聚合，因为我们可以将变量对(<em class="nq"> X </em>，<em class="nq"> Y </em>)定义为单个向量值随机变量。</p><p id="a335" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">一对具有联合分布<em class="nq"> p </em> ( <em class="nq"> x </em>，<em class="nq"> y </em>)的离散随机变量(<em class="nq"> X </em>，<em class="nq"> Y </em>)的联合熵<em class="nq"> H </em> ( <em class="nq"> X </em>，<em class="nq">Y</em>定义为</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi nt"><img src="../Images/850ac6df0f68a0d5f5d5793e58eb6d01.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YI2hmWz-vZoPnfxc2c2G7A.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">相关熵</p></figure><p id="1047" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">这也可以用<a class="ae le" href="https://en.wikipedia.org/wiki/Expected_value" rel="noopener ugc nofollow" target="_blank">期望值</a>来表示。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/eb76579226842ed07ae0945efeb4c6b0.png" data-original-src="https://miro.medium.com/v2/resize:fit:826/format:webp/1*hUiBRfysj48YQ4mBWebyTw.png"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">联合熵(期望值形式)</p></figure><p id="7a2d" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">同样，对于条件熵，<em class="nq"> H </em> ( <em class="nq"> Y </em> |X)定义为:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi nv"><img src="../Images/b39aa0dd68ed864b58d50ee53a3cfb9f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1050/format:webp/1*Upy_DWyNWJ0K2FL-Ls-Urw.png"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">条件熵</p></figure><p id="0564" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">直观上，这是给定<em class="nq"> X </em>的<em class="nq"> Y </em>的熵在<em class="nq"> X </em>的所有可能值上的平均值。考虑到(<em class="nq"> X </em>，<em class="nq">Y</em>)~<em class="nq">p</em>(<em class="nq">X</em>，<em class="nq"> y </em>)，条件熵也可以用期望值来表示。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi nw"><img src="../Images/68f5b0bb905f7de06cb55b3eb5c9c038.png" data-original-src="https://miro.medium.com/v2/resize:fit:810/format:webp/1*bngw6coAPR3ZUGU0RdsTyg.png"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">条件熵(期望值形式)</p></figure><p id="f084" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">让我们尝试一个例子来更好地理解条件熵。考虑一项研究，其中受试者被问及:<br/> I)他们是否吸烟、饮酒或两者都不做。如果他们患有任何形式的癌症，我将把这些问题的回答表示为属于联合分布的两个不同的离散变量。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/27669e43863614933811e27c7f456577.png" data-original-src="https://miro.medium.com/v2/resize:fit:630/format:webp/1*75mKBWsJn4dttgafCU64Jw.png"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">数据表</p></figure><p id="5185" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">在左边，你可以看到10名受试者回答问题的数据表。变量Activity有三种不同的可能性(我称之为X)。第二列代表受试者是否患有癌症(变量Y)。这里有两种可能性，即是或否。由于我们还没有处理连续变量，我已经将这些变量保持离散。让我们创建一个概率表，使场景更加清晰。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/c586ca4670c1faabcb7ede56d0cc86a7.png" data-original-src="https://miro.medium.com/v2/resize:fit:980/format:webp/1*lzZ8juMm2WHqI_S7Hth5hg.png"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">上例的概率表</p></figure><p id="6543" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">接下来我会计算x的所有可能值的边际概率<em class="nq"> p </em> ( <em class="nq"> x </em>)的值。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi nz"><img src="../Images/91947c91d64a3111e5d70a959496b011.png" data-original-src="https://miro.medium.com/v2/resize:fit:700/format:webp/1*0srAfVU4L2mxTszgTtlNPQ.png"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">X的边际概率</p></figure><p id="55d1" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">基于概率表，我们可以在条件熵公式中插入值。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi oa"><img src="../Images/5069bb12ec43d8cdf07c8756b45f4704.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zKcq246xg9gO3MN26DPWjw.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">条件概率在上面的例子中</p></figure><h2 id="43af" class="nf mj iq bd mk ng nh dn mo ni nj dp ms lo nk nl mu ls nm nn mw lw no np my iw bi translated">相对熵</h2><p id="1c6e" class="pw-post-body-paragraph lf lg iq lh b li na ka lk ll nb kd ln lo nc lq lr ls nd lu lv lw ne ly lz ma ij bi translated">从随机变量到分布，相对熵有所不同。它是两个分布之间距离的度量。更本能的说法是:相对熵或KL散度，用<em class="nq">D</em>(<em class="nq">p</em>|<em class="nq">q</em>表示，是当真实分布是<em class="nq"> p </em>时，假设分布是<em class="nq"> q </em>的低效率的度量。它可以定义为:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi ob"><img src="../Images/e5d9796941e0fe28eb0a51b389c535b9.png" data-original-src="https://miro.medium.com/v2/resize:fit:832/format:webp/1*ILlyRucYqm7Ku15t5dRKIA.png"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">相对熵</p></figure><p id="fd1a" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">相对熵总是非负的，只有当<em class="nq"> p </em> = <em class="nq"> q </em>时才能为0。尽管这里要注意的一点是，它不是一个真实的距离，因为它在本质上是不对称的。但是它通常被认为是分布之间的“距离”。</p><p id="1af4" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">让我们举一个例子来巩固这个概念！设<em class="nq"> X = {0，1} </em>并考虑<em class="nq"> X上的两个分布<em class="nq"> p </em>和<em class="nq">q</em></em>设<em class="nq"> p </em> (0) = 1 — <em class="nq"> r </em>，<em class="nq"> p </em> (1) = <em class="nq"> r </em>设<em class="nq"> q </em> (0) = 1 — <em class="nq"> s </em>，<em class="nq"> q </em> (1</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/9e1103d5da0d886f545fa65f179b9447.png" data-original-src="https://miro.medium.com/v2/resize:fit:972/format:webp/1*_b1XuCQ2oSUwhV5aNRGzAw.png"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">p||q的相对熵</p></figure><p id="205d" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">我还要演示一下非对称性质，所以我也算一下<em class="nq">D</em>(<em class="nq">q</em>| |<em class="nq">p</em>)。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi od"><img src="../Images/595b4ffec74f637d4f2e13e491df41f2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1018/format:webp/1*7QJsWI17RivI5y3d5OkuZA.png"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">q||p的相对熵</p></figure><p id="b653" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">如果r = s，<em class="nq">D</em>(<em class="nq">p</em>| |<em class="nq">q</em>)=<em class="nq">D</em>(<em class="nq">q</em>|<em class="nq">p</em>)= 0。但是我会取一些不同的值，例如，<em class="nq"> r </em> = 1/2，s  = 1/4。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi oe"><img src="../Images/254568f846d5ae9ad30706cceac1951c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LHWC6MrE7Nw_IXrWvekt-A.png"/></div></div></figure><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi of"><img src="../Images/4a621e4881fa1a258b9e2ef89ce22c76.png" data-original-src="https://miro.medium.com/v2/resize:fit:1366/format:webp/1*zYcvrJCClyBhZUTWJ-Me-g.png"/></div></figure><p id="3edb" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">可以看到，<em class="nq">D</em>(<em class="nq">p</em>|<em class="nq">q</em>)≦<em class="nq">D</em>(<em class="nq">q</em>|<em class="nq">p</em>)。</p><p id="52b0" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">现在，既然我们已经讨论了不同类型的熵，我们可以继续讨论互信息。</p><h2 id="f6c2" class="nf mj iq bd mk ng nh dn mo ni nj dp ms lo nk nl mu ls nm nn mw lw no np my iw bi translated">交互信息</h2><p id="caaf" class="pw-post-body-paragraph lf lg iq lh b li na ka lk ll nb kd ln lo nc lq lr ls nd lu lv lw ne ly lz ma ij bi translated">互信息是一个随机变量包含的关于另一个随机变量的信息量的度量。<strong class="lh ja">或者，它可以被定义为一个变量的不确定性由于另一个变量的知识而减少。</strong>其技术定义如下:<br/>考虑两个随机变量<em class="nq"> X </em>和<em class="nq"> Y </em>，它们具有联合概率质量函数<em class="nq"> p </em> ( <em class="nq"> x </em>，<em class="nq"> y </em>)和边际概率质量函数<em class="nq"> p </em> ( <em class="nq"> x </em>和<em class="nq">p<em class="nq">相互信息<em class="nq">I</em>(<em class="nq">X</em>)；<em class="nq"> Y </em>)是联合分布与乘积分布<em class="nq">p</em>(<em class="nq">x</em>)<em class="nq">p</em>(<em class="nq">Y</em>)之间的相对熵。</em></em></p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi og"><img src="../Images/5475646d233547dbe1b5558e995d53af.png" data-original-src="https://miro.medium.com/v2/resize:fit:1398/format:webp/1*2JRcHm8UGHqA8BnxDOlikw.png"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">交互信息</p></figure><p id="720f" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">互信息也可以用熵来表示。推导过程很有趣，但我会克制自己，因为它可能会弄乱文章。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/76aaaedcb65b2cddc671d0fa7770ab3e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1214/format:webp/1*-oCjsDMJYTdWppU1F8JaHA.png"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">互信息w.r.t熵</p></figure><p id="c76c" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">从上面的等式中，我们看到互信息是由于<em class="nq"> Y </em>的知识而导致的<em class="nq"> X </em>的不确定性的减少。有一个维恩图完美地描述了这种关系。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi oi"><img src="../Images/1dbaa93cd36920831603e1be3b8352db.png" data-original-src="https://miro.medium.com/v2/resize:fit:836/format:webp/1*d9oSCSxvb4Sr3VW8E2ur8w.png"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">互信息与熵的关系</p></figure><p id="5035" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">让我们举个例子来更好地理解它。我可以用我在解释熵的时候用过的例子，把吸烟、饮酒和癌症联系起来。我们已经看到，<em class="nq">H</em>(<em class="nq">Y</em>| X)= 0.8184位。为了计算互信息，我需要多一项<em class="nq"> H </em> ( <em class="nq"> Y </em>)。<em class="nq"> H </em> ( <em class="nq"> Y </em>)，在这种情况下，将为:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/87e0063968ef820695e62015de847a19.png" data-original-src="https://miro.medium.com/v2/resize:fit:812/format:webp/1*f7meeMGc_UVyGqMqI5TeIQ.png"/></div></figure><p id="6f3c" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">因此，交互信息的定义是:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi ok"><img src="../Images/84e0ac9169f2ecc032704619c856f9c6.png" data-original-src="https://miro.medium.com/v2/resize:fit:704/format:webp/1*47s5vqs38SQxO_tGycv-Sw.png"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">互信息的计算</p></figure><p id="5f2c" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">或者，我也可以用<em class="nq"> H </em> ( <em class="nq"> X </em>)和<em class="nq"> H </em> ( <em class="nq"> X </em> | <em class="nq"> Y </em>)来计算互信息，结果也是一样的。我们可以看到，知道X对于变量y的不确定性来说意义如此之小。让我改变这个例子的段落，并引导您了解这一切在机器学习中是如何有意义的。假设X是预测变量，Y是预测变量。它们之间的互信息可以是检查该特征对于预测的有用程度的很好的前兆。让我们讨论信息论在机器学习中的含义。</p></div><div class="ab cl mb mc hu md" role="separator"><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg"/></div><div class="ij ik il im in"><h1 id="ca48" class="mi mj iq bd mk ml mm mn mo mp mq mr ms kf mt kg mu ki mv kj mw kl mx km my mz bi translated"><strong class="ak">信息论在机器学习中的应用</strong></h1><p id="a98d" class="pw-post-body-paragraph lf lg iq lh b li na ka lk ll nb kd ln lo nc lq lr ls nd lu lv lw ne ly lz ma ij bi translated">周围有相当多的应用程序，但我将坚持使用几个流行的应用程序。</p><h2 id="f5c2" class="nf mj iq bd mk ng nh dn mo ni nj dp ms lo nk nl mu ls nm nn mw lw no np my iw bi translated">决策树</h2><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ol"><img src="../Images/17e2291bbbd80fc687e052431b3b465d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7tMeTXs6Oc4XieB2RtKqUg.jpeg"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">照片由<a class="ae le" href="https://www.pexels.com/@minan1398?utm_content=attributionCopyText&amp;utm_medium=referral&amp;utm_source=pexels" rel="noopener ugc nofollow" target="_blank">安民</a>从<a class="ae le" href="https://www.pexels.com/photo/silhouette-photo-of-trees-962312/?utm_content=attributionCopyText&amp;utm_medium=referral&amp;utm_source=pexels" rel="noopener ugc nofollow" target="_blank">像素</a>拍摄</p></figure><p id="42a4" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">决策树(DTs)是一种用于分类和回归的非参数监督学习方法。目标是创建一个模型，通过学习从数据特征推断的简单决策规则来预测目标变量的值。这里使用的核心算法叫做ID3，由Ross Quinlan开发。它采用自上而下的贪婪搜索方法，并涉及将数据划分为具有同类数据的子集。<strong class="lh ja">ID3算法通过使用熵计算样本的同质性来决定划分。</strong>若样本同质，熵为0，若样本均分，则熵最大。但是熵对树的构造没有直接的影响。该算法依赖于信息增益，信息增益基于数据集在属性上拆分后熵的减少。如果你凭直觉思考，你会发现这实际上是我上面提到的相互信息。在给定一个变量的值的情况下，互信息减少了另一个变量的不确定性。在DT中，我们计算预测变量的熵。然后，基于熵分割数据集，并从先前的熵值中减去结果变量的熵。这是信息增益，很明显，是相互信息在起作用。</p><h2 id="cfeb" class="nf mj iq bd mk ng nh dn mo ni nj dp ms lo nk nl mu ls nm nn mw lw no np my iw bi translated">交叉熵</h2><p id="95a6" class="pw-post-body-paragraph lf lg iq lh b li na ka lk ll nb kd ln lo nc lq lr ls nd lu lv lw ne ly lz ma ij bi translated">交叉熵是一个非常类似于相对熵的概念。相对熵是指随机变量将真实分布<em class="nq"> p </em>与每个样本点的近似分布<em class="nq"> q </em>与<em class="nq"> p </em>的差异进行比较(散度或差异)。而交叉熵直接比较真实分布<em class="nq"> p </em>和近似分布<em class="nq"> q </em>。现在，交叉熵是深度学习领域中大量使用的术语。<strong class="lh ja">用作损失函数，衡量分类模型的性能，其输出为0到1之间的概率值。</strong>交叉熵损失随着预测概率偏离实际标签而增加。</p><h2 id="abb8" class="nf mj iq bd mk ng nh dn mo ni nj dp ms lo nk nl mu ls nm nn mw lw no np my iw bi translated">KL-散度</h2><p id="1bcf" class="pw-post-body-paragraph lf lg iq lh b li na ka lk ll nb kd ln lo nc lq lr ls nd lu lv lw ne ly lz ma ij bi translated">K-L散度或相对熵也是深度学习文献中的一个主题，特别是在VAE。变分自动编码器接受高斯分布形式的输入，而不是离散的数据点。对VAE的分布进行正则化以增加潜在空间内的重叠量是最佳的。<strong class="lh ja"> K-L散度对此进行测量，并添加到损失函数中。</strong></p><p id="9169" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">K-L散度也用于t-SNE。tSNE是一种降维技术，主要用于可视化高维数据。它将数据点之间的相似性转换为联合概率，并且<strong class="lh ja">试图最小化低维嵌入和高维数据的联合概率之间的Kullback-Leibler散度。</strong></p><h2 id="9156" class="nf mj iq bd mk ng nh dn mo ni nj dp ms lo nk nl mu ls nm nn mw lw no np my iw bi translated">计算目标类别分布的不平衡</h2><p id="096a" class="pw-post-body-paragraph lf lg iq lh b li na ka lk ll nb kd ln lo nc lq lr ls nd lu lv lw ne ly lz ma ij bi translated">熵可以用来计算目标阶级的不平衡。如果我们将预测的特征视为具有两个类别的随机变量，那么一个平衡的集合(50/50分割)应该具有最大的熵，就像我们在掷硬币的例子中看到的那样。但是，如果分裂是偏斜的，一个类有90%的流行率，那么获得的知识较少，因此熵较低。实施计算熵的链式法则，我们可以检查一个多类目标变量在一个量化值中是否平衡，尽管一个平均值掩盖了个体概率。</p></div><div class="ab cl mb mc hu md" role="separator"><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg"/></div><div class="ij ik il im in"><h1 id="8924" class="mi mj iq bd mk ml mm mn mo mp mq mr ms kf mt kg mu ki mv kj mw kl mx km my mz bi translated">摘要</h1><p id="e4a5" class="pw-post-body-paragraph lf lg iq lh b li na ka lk ll nb kd ln lo nc lq lr ls nd lu lv lw ne ly lz ma ij bi translated">信息论是一个激动人心的领域，对多个领域都有重大贡献。作为其中之一，机器学习并没有完全利用信息论所提供的一切。我觉得在机器学习的背景下，有许多信息理论概念有待发现，作为一名数据科学家，这让我非常热情。就此，我想结束我的文章。</p><p id="bbcc" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">我希望你喜欢我的文章，如果你真的真的喜欢，这里是我的:</p><ol class=""><li id="637b" class="om on iq lh b li lj ll lm lo oo ls op lw oq ma or os ot ou bi translated">我的网站:<a class="ae le" href="https://preeyonujboruah.tech/" rel="noopener ugc nofollow" target="_blank">https://preeyonujboruah.tech/</a></li><li id="941c" class="om on iq lh b li ov ll ow lo ox ls oy lw oz ma or os ot ou bi translated">Github简介:<a class="ae le" href="https://github.com/preeyonuj" rel="noopener ugc nofollow" target="_blank">https://github.com/preeyonuj</a></li><li id="ee26" class="om on iq lh b li ov ll ow lo ox ls oy lw oz ma or os ot ou bi translated">上一篇媒体文章:<a class="ae le" href="https://medium.com/mlearning-ai/detecting-heart-failure-using-machine-learning-part-3-3fb8ab5d595f" rel="noopener">https://Medium . com/analytics-vid hya/aptos-blindness-challenge-part-1-baseline-efficient net-c 7a 256 da a6e 5？sk = d0e 445 f 99 DAA 71d 79 f 0452665 f1 a59 db</a></li><li id="5910" class="om on iq lh b li ov ll ow lo ox ls oy lw oz ma or os ot ou bi translated">领英简介:【www.linkedin.com/in/pb1807 T4】</li><li id="3856" class="om on iq lh b li ov ll ow lo ox ls oy lw oz ma or os ot ou bi translated">推特简介:<a class="ae le" href="https://twitter.com/preeyonuj" rel="noopener ugc nofollow" target="_blank">https://twitter.com/preeyonuj</a></li></ol></div><div class="ab cl mb mc hu md" role="separator"><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg"/></div><div class="ij ik il im in"><h1 id="1c19" class="mi mj iq bd mk ml mm mn mo mp mq mr ms kf mt kg mu ki mv kj mw kl mx km my mz bi translated">参考</h1><ol class=""><li id="cc0c" class="om on iq lh b li na ll nb lo pa ls pb lw pc ma or os ot ou bi translated"><a class="ae le" href="https://www.khanacademy.org/computing/computer-science/informationtheory" rel="noopener ugc nofollow" target="_blank">https://www . khanacademy . org/computing/computer-science/information theory</a></li><li id="6e49" class="om on iq lh b li ov ll ow lo ox ls oy lw oz ma or os ot ou bi translated"><a class="ae le" href="https://web.stanford.edu/~montanar/RESEARCH/BOOK/partA.pdf" rel="noopener ugc nofollow" target="_blank">https://web.stanford.edu/~montanar/RESEARCH/BOOK/partA.pdf</a></li><li id="0aca" class="om on iq lh b li ov ll ow lo ox ls oy lw oz ma or os ot ou bi translated"><a class="ae le" href="https://web.mit.edu/6.933/www/Fall2001/Shannon2.pdf" rel="noopener ugc nofollow" target="_blank">https://web.mit.edu/6.933/www/Fall2001/Shannon2.pdf</a></li><li id="9400" class="om on iq lh b li ov ll ow lo ox ls oy lw oz ma or os ot ou bi translated"><a class="ae le" href="https://www.sciencedirect.com/topics/neuroscience/information-theory" rel="noopener ugc nofollow" target="_blank">https://www . science direct . com/topics/neuroscience/information-theory</a></li><li id="2981" class="om on iq lh b li ov ll ow lo ox ls oy lw oz ma or os ot ou bi translated"><a class="ae le" href="http://staff.ustc.edu.cn/~cgong821/Wiley.Interscience.Elements.of.Information.Theory.Jul.2006.eBook-DDU.pdf" rel="noopener ugc nofollow" target="_blank">http://staff.ustc.edu.cn/~cgong821/Wiley.interscience . elements . of . information . theory . 2006 . 7 . ebook-DDU . pdf</a></li></ol></div></div>    
</body>
</html>