<html>
<head>
<title>10 QuestionsTo Practice Before Your Databricks Apache Spark 3.0 Developer Exam</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Databricks Apache Spark 3.0开发人员考试前要练习的10个问题</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/10-mcqs-to-practice-before-your-databricks-apache-spark-3-0-developer-exam-bd886060b9ab?source=collection_archive---------1-----------------------#2021-06-25">https://towardsdatascience.com/10-mcqs-to-practice-before-your-databricks-apache-spark-3-0-developer-exam-bd886060b9ab?source=collection_archive---------1-----------------------#2021-06-25</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="c982" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">在这篇文章中，我分享了真正的mcq，以及我为考试做准备的3个顶级技巧。</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><a href="https://click.linksynergy.com/deeplink?id=533LxfDBSaM&amp;mid=39197&amp;murl=https%3A%2F%2Fwww.udemy.com%2Fcourse%2Fdatabricks-certified-developer-for-apache-spark-30-practice-exams%2F"><div class="gh gi ki"><img src="../Images/0bcca2774f66f10e189a864c8127c49f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*D94QGcxvlg72mnXOY4vKKQ.png"/></div></a><p class="kq kr gj gh gi ks kt bd b be z dk translated">认证徽章由<a class="ae ku" href="https://www.credential.net/group/227965" rel="noopener ugc nofollow" target="_blank">credential.net</a></p></figure><h2 id="0c6c" class="kv kw it bd kx ky kz dn la lb lc dp ld le lf lg lh li lj lk ll lm ln lo lp lq bi translated">我的读者请注意</h2><p id="ae5a" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz le ma mb mc li md me mf lm mg mh mi mj im bi translated"><em class="mk">这个帖子包括附属链接，如果你购买的话，我可以在不增加你额外费用的情况下赚取一小笔佣金。</em></p><p id="ad9d" class="pw-post-body-paragraph lr ls it lt b lu ml ju lw lx mm jx lz le mn mb mc li mo me mf lm mp mh mi mj im bi translated"><strong class="lt iu"><em class="mk">&gt;&gt;&gt;</em></strong><em class="mk">还不是中等成员？考虑与我的</em> <a class="ae ku" href="https://anbento4.medium.com/membership" rel="noopener"> <strong class="lt iu"> <em class="mk">推荐链接</em> </strong> </a> <em class="mk">签约，以获得Medium提供的一切服务，价格低至</em><strong class="lt iu"><em class="mk">【5美元一个月</em> </strong> <em class="mk">！</em></p></div><div class="ab cl mq mr hx ms" role="separator"><span class="mt bw bk mu mv mw"/><span class="mt bw bk mu mv mw"/><span class="mt bw bk mu mv"/></div><div class="im in io ip iq"><h1 id="1bc5" class="mx kw it bd kx my mz na la nb nc nd ld jz ne ka lh kc nf kd ll kf ng kg lp nh bi translated">缺乏练习的资源？</h1><p id="0f8f" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz le ma mb mc li md me mf lm mg mh mi mj im bi translated">如果你正在为Apache Spark 3.0认证  <strong class="lt iu"> </strong>而学习<a class="ae ku" href="https://academy.databricks.com/exam/databricks-certified-associate-developer" rel="noopener ugc nofollow" target="_blank"> <strong class="lt iu"> Databricks协理开发人员，你可能正面临着我几周前面临的同样问题:缺乏<strong class="lt iu">模拟测试</strong>来评估你的准备情况。</strong></a></p><p id="f805" class="pw-post-body-paragraph lr ls it lt b lu ml ju lw lx mm jx lz le mn mb mc li mo me mf lm mp mh mi mj im bi translated">现在，你应该知道考试由60道mcq组成，你将有120分钟的时间答对至少42道题(70%)。</p><p id="baa1" class="pw-post-body-paragraph lr ls it lt b lu ml ju lw lx mm jx lz le mn mb mc li mo me mf lm mp mh mi mj im bi translated">另一个我想你已经注意到的细节是<strong class="lt iu">考试将花费你240美元(包括增值税)</strong>但是你将被允许<strong class="lt iu">一次尝试</strong>，这样如果你失败了，你将不得不<strong class="lt iu">再次支付</strong>重考。有了这些前提，我猜你真的希望第一次就通过考试。</p><p id="d566" class="pw-post-body-paragraph lr ls it lt b lu ml ju lw lx mm jx lz le mn mb mc li mo me mf lm mp mh mi mj im bi translated">但你可能会疑惑:<em class="mk">“如果我找不到任何代表考试难度水平的问题示例，我又怎么能真正理解自己是否准备好了呢？”。</em></p><p id="907e" class="pw-post-body-paragraph lr ls it lt b lu ml ju lw lx mm jx lz le mn mb mc li mo me mf lm mp mh mi mj im bi translated">这也是我在参加考试前遇到的同样的困境:<strong class="lt iu">我不确定自己是否准备好超过70%的门槛，可能我没有，因为我发现真题比预期的更具挑战性。</strong></p><blockquote class="ni"><p id="1d63" class="nj nk it bd nl nm nn no np nq nr mj dk translated">我发现真题比预期的更有挑战性。</p></blockquote><p id="45da" class="pw-post-body-paragraph lr ls it lt b lu ns ju lw lx nt jx lz le nu mb mc li nv me mf lm nw mh mi mj im bi translated">尽管有一些困难(<em class="mk">也是技术性的，因为我的考试被监考人暂停了30分钟</em>)，<a class="ae ku" href="https://academy.databricks.com/award/certification/db22fd2b-a4dc-3ad3-8de6-9b7774b820ac/view" rel="noopener ugc nofollow" target="_blank">在准备了大约2个月之后，我设法以一个好分数通过了认证。</a></p><p id="966d" class="pw-post-body-paragraph lr ls it lt b lu ml ju lw lx mm jx lz le mn mb mc li mo me mf lm mp mh mi mj im bi translated">在Medium上至少还有十几篇关于这个主题的文章(<em class="mk">我把它们都读了，作为我准备工作的一部分</em>)，我发现其中有3-4篇文章非常有见地(<em class="mk">文章结尾的链接</em>)，但没有一篇包含任何模拟选择题，帮助我测试我的知识。</p><blockquote class="ni"><p id="c6a3" class="nj nk it bd nl nm nn no np nq nr mj dk translated"><strong class="ak">在本文中，我将与您分享PySpark认证版本的10个MCQs】、<strong class="ak">，您可以在真正的考试</strong>中找到它们。</strong></p></blockquote><p id="d9a7" class="pw-post-body-paragraph lr ls it lt b lu ns ju lw lx nt jx lz le nu mb mc li nv me mf lm nw mh mi mj im bi translated">为此，<strong class="lt iu">在本文中，我与大家分享PySpark版认证</strong>、<strong class="lt iu">的10个mcq，你有望在真题考试</strong>中找到。<em class="mk">请注意，我不允许透露确切的问题，所以我通过保持难度不变的方式重新表述了这些问题，因此您可以相信它们是一个有价值的学习资源。</em></p><p id="1ce7" class="pw-post-body-paragraph lr ls it lt b lu ml ju lw lx mm jx lz le mn mb mc li mo me mf lm mp mh mi mj im bi translated">在开始真正的问题之前，让我给你3个提示，你可能在其他文章中找不到，但我认为它们会对你的最终分数产生巨大的影响。</p><div class="nx ny gp gr nz oa"><a href="https://medium.com/codex/4-full-practice-tests-to-prepare-databricks-associate-certification-pyspark-2021-eab289b1ea0c" rel="noopener follow" target="_blank"><div class="ob ab fo"><div class="oc ab od cl cj oe"><h2 class="bd iu gy z fp of fr fs og fu fw is bi translated">准备Databricks助理认证(PySpark | 2021)的4次全面实践测试</h2><div class="oh l"><h3 class="bd b gy z fp of fr fs og fu fw dk translated">让我与你分享200多个现实的、高质量的问题来成为一个认证的Spark 3.0开发者</h3></div><div class="oi l"><p class="bd b dl z fp of fr fs og fu fw dk translated">medium.com</p></div></div><div class="oj l"><div class="ok l ol om on oj oo ko oa"/></div></div></a></div><div class="nx ny gp gr nz oa"><a rel="noopener follow" target="_blank" href="/3-ways-to-create-tables-with-apache-spark-32aed0f355ab"><div class="ob ab fo"><div class="oc ab od cl cj oe"><h2 class="bd iu gy z fp of fr fs og fu fw is bi translated">用Apache Spark创建表格的3种方法</h2><div class="oh l"><h3 class="bd b gy z fp of fr fs og fu fw dk translated">学习如何用PySpark构建托管和非托管表，以及如何在您的项目中有效地使用它们</h3></div><div class="oi l"><p class="bd b dl z fp of fr fs og fu fw dk translated">towardsdatascience.com</p></div></div><div class="oj l"><div class="op l ol om on oj oo ko oa"/></div></div></a></div><h1 id="942c" class="mx kw it bd kx my oq na la nb or nd ld jz os ka lh kc ot kd ll kf ou kg lp nh bi translated">我通过考试的诚实建议</h1><p id="3a83" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz le ma mb mc li md me mf lm mg mh mi mj im bi translated">不，我不会建议你阅读<a class="ae ku" href="https://www.amazon.co.uk/Spark-Definitive-Guide-Bill-Chambers/dp/1491912219" rel="noopener ugc nofollow" target="_blank"> <strong class="lt iu"> Spark -权威指南</strong> </a>或<a class="ae ku" href="https://www.oreilly.com/library/view/learning-spark-2nd/9781492050032/" rel="noopener ugc nofollow" target="_blank"> <strong class="lt iu"> 2d版学习Spark </strong> </a>因为……你已经知道它们了……对吗？我要给你的是3个技巧，它们将极大地增加你成功的机会——所以请仔细阅读！</p><h2 id="6e94" class="kv kw it bd kx ky kz dn la lb lc dp ld le lf lg lh li lj lk ll lm ln lo lp lq bi translated">#1学会用心浏览PySpark文档</h2><p id="c0ba" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz le ma mb mc li md me mf lm mg mh mi mj im bi translated">这可能是我能给excel的最好建议了。实际上，在考试期间，你可以参考右边屏幕上的<code class="fe ov ow ox oy b">pyspark.sql</code> <a class="ae ku" href="https://spark.apache.org/docs/3.0.0-preview2/api/python/pyspark.sql.html" rel="noopener ugc nofollow" target="_blank">文档</a>，但是你不能使用<strong class="lt iu"> CTRL+F </strong>来搜索关键词。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="pa pb di pc bf pd"><div class="gh gi oz"><img src="../Images/f3848c204c3ce5a6d9ddcdd566e9961d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OEwm-Y1EXu4Wsf3cFg8tbQ.png"/></div></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">考试期间你可以参考的课程。</p></figure><p id="e059" class="pw-post-body-paragraph lr ls it lt b lu ml ju lw lx mm jx lz le mn mb mc li mo me mf lm mp mh mi mj im bi translated">这意味着，除非您知道在哪里可以找到特定的方法或特定的函数，否则您可能会浪费大量的时间来来回回地滚动，相信我，这会让您感到紧张。相反，我建议你把重点放在以下三堂课的内容上:</p><ul class=""><li id="ba9d" class="pe pf it lt b lu ml lx mm le pg li ph lm pi mj pj pk pl pm bi translated"><strong class="lt iu">py spark . SQL . data frame:</strong><em class="mk">比如</em> <em class="mk">你应该能定位到</em> <code class="fe ov ow ox oy b"><em class="mk">coalesce()</em></code> <em class="mk">和</em> <code class="fe ov ow ox oy b"><em class="mk">join()</em></code> <em class="mk">函数。</em></li><li id="e45e" class="pe pf it lt b lu pn lx po le pp li pq lm pr mj pj pk pl pm bi translated"><strong class="lt iu">py spark . sq . column:</strong><em class="mk">比如你要知道</em><code class="fe ov ow ox oy b"><em class="mk">when()</em></code><em class="mk"/><code class="fe ov ow ox oy b"><em class="mk">between()</em></code><em class="mk"/><code class="fe ov ow ox oy b"><em class="mk">otherwise</em></code><em class="mk">是应用于DataFrame的列，而不是直接应用于DataFrame。</em></li><li id="e8a9" class="pe pf it lt b lu pn lx po le pp li pq lm pr mj pj pk pl pm bi translated"><strong class="lt iu"><em class="mk">py spark . SQL . functions:</em></strong><em class="mk">例如，您应该知道用于操作时间字段的函数，如</em> <code class="fe ov ow ox oy b"><em class="mk">date_add()</em></code> <em class="mk">、</em> <code class="fe ov ow ox oy b"><em class="mk">date_sun()</em></code> <em class="mk">和</em> <code class="fe ov ow ox oy b"><em class="mk">from_unixtime()</em></code> <em class="mk">(是的，我对这个函数有疑问！书上无处可寻……)在此描述。</em></li></ul><p id="f53c" class="pw-post-body-paragraph lr ls it lt b lu ml ju lw lx mm jx lz le mn mb mc li mo me mf lm mp mh mi mj im bi translated">我在测试前两天接触并研究了文档<strong class="lt iu">的结构，但我希望我能更早地这样做，因为这些知识帮助我正确回答了至少7-8个问题。</strong></p><p id="d567" class="pw-post-body-paragraph lr ls it lt b lu ml ju lw lx mm jx lz le mn mb mc li mo me mf lm mp mh mi mj im bi translated">因此，我在这里试图传达的是<strong class="lt iu">不要高估你在PySpark语法上的知识，因为你的记忆可能会在考试中背叛你。</strong>充分利用文档。</p><h2 id="84e6" class="kv kw it bd kx ky kz dn la lb lc dp ld le lf lg lh li lj lk ll lm ln lo lp lq bi translated">#2在Udemy上查看本课程:<a class="ae ku" href="https://click.linksynergy.com/deeplink?id=533LxfDBSaM&amp;mid=47900&amp;murl=https%3A%2F%2Fwww.udemy.com%2Fcourse%2Fdatabricks-certified-developer-for-apache-spark-30-practice-exams%2F%3FranMID%3D39197%26ranEAID%3D533LxfDBSaM%26ranSiteID%3D533LxfDBSaM-CA8OlQBRwWToEEwq8oC0dg%26utm_source%3Daff-campaign%26LSNPUBID%3D533LxfDBSaM%26utm_medium%3Dudemyads" rel="noopener ugc nofollow" target="_blank">data bricks Spark 3.0实践考试认证开发人员</a></h2><p id="e547" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz le ma mb mc li md me mf lm mg mh mi mj im bi translated">事实证明，实际上<strong class="lt iu">Python/Pyspark<em class="mk"/></strong>的两个完整模拟测试在<a class="ae ku" href="https://click.linksynergy.com/deeplink?id=533LxfDBSaM&amp;mid=47900&amp;murl=https%3A%2F%2Fwww.udemy.com%2Fcourse%2Fdatabricks-certified-developer-for-apache-spark-30-practice-exams%2F%3FranMID%3D39197%26ranEAID%3D533LxfDBSaM%26ranSiteID%3D533LxfDBSaM-CA8OlQBRwWToEEwq8oC0dg%26utm_source%3Daff-campaign%26LSNPUBID%3D533LxfDBSaM%26utm_medium%3Dudemyads" rel="noopener ugc nofollow" target="_blank"> Udemy </a>上可用，并且包括Apache Spark 3.0认证考试的120个模拟考试测验！</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><a href="https://click.linksynergy.com/deeplink?id=533LxfDBSaM&amp;mid=39197&amp;murl=https%3A%2F%2Fwww.udemy.com%2Fcourse%2Fdatabricks-certified-developer-for-apache-spark-30-practice-exams%2F"><div class="gh gi ps"><img src="../Images/1520cfa9778e3f9b1938b19c396c79d3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Zw_OPble1SplYV4gvF5Q5Q.png"/></div></a><p class="kq kr gj gh gi ks kt bd b be z dk translated"><a class="ae ku" href="https://click.linksynergy.com/deeplink?id=533LxfDBSaM&amp;mid=47900&amp;murl=https%3A%2F%2Fwww.udemy.com%2Fcourse%2Fdatabricks-certified-developer-for-apache-spark-30-practice-exams%2F%3FranMID%3D39197%26ranEAID%3D533LxfDBSaM%26ranSiteID%3D533LxfDBSaM-CA8OlQBRwWToEEwq8oC0dg%26utm_source%3Daff-campaign%26LSNPUBID%3D533LxfDBSaM%26utm_medium%3Dudemyads" rel="noopener ugc nofollow" target="_blank">可能是网上最新更新的测试。</a></p></figure><p id="2c17" class="pw-post-body-paragraph lr ls it lt b lu ml ju lw lx mm jx lz le mn mb mc li mo me mf lm mp mh mi mj im bi translated">我在考试前两个月购买了测试权限，因为我想研究基于真题的材料，并复习我得到错误答案的主题。</p><p id="7374" class="pw-post-body-paragraph lr ls it lt b lu ml ju lw lx mm jx lz le mn mb mc li mo me mf lm mp mh mi mj im bi translated">与真正的考试一样，您有2个小时的时间来完成测试，并且每个题目的权重也受到尊重，这意味着:</p><ul class=""><li id="260f" class="pe pf it lt b lu ml lx mm le pg li ph lm pi mj pj pk pl pm bi translated"><strong class="lt iu"> Spark DataFrame API应用(~72%) </strong></li><li id="dd30" class="pe pf it lt b lu pn lx po le pp li pq lm pr mj pj pk pl pm bi translated"><strong class="lt iu"> Spark架构:概念理解(~17%) </strong></li><li id="31b0" class="pe pf it lt b lu pn lx po le pp li pq lm pr mj pj pk pl pm bi translated"><strong class="lt iu"> Spark架构:应用理解(~11%) </strong></li></ul><p id="592c" class="pw-post-body-paragraph lr ls it lt b lu ml ju lw lx mm jx lz le mn mb mc li mo me mf lm mp mh mi mj im bi translated">就我而言，<strong class="lt iu">实际考试中至少有12-15个问题与我在这些考试中练习的问题</strong> ( <em class="mk">在措辞和解决方案方面</em>)非常相似，所以我认为这是你在学习认证时的一项出色投资。</p><h2 id="525a" class="kv kw it bd kx ky kz dn la lb lc dp ld le lf lg lh li lj lk ll lm ln lo lp lq bi translated">#3不要只运行官方指南中的代码示例。更进一步。</h2><p id="9378" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz le ma mb mc li md me mf lm mg mh mi mj im bi translated">事后看来，有一件事我在准备时可以做得更好，那就是尝试运行更多属于<strong class="lt iu">Spark</strong><strong class="lt iu">data frame API</strong>的函数和方法，并仔细检查它们的语法细节，而不是只关注书本上的代码片段。</p><p id="8827" class="pw-post-body-paragraph lr ls it lt b lu ml ju lw lx mm jx lz le mn mb mc li mo me mf lm mp mh mi mj im bi translated">想想看:你会发现至少有40-43个关于Spark DataFrame API的问题，因此有理由期待大量不同的概念将被测试(<em class="mk">甚至是你在书中找不到的概念——生活糟透了！</em>)。</p><p id="85c7" class="pw-post-body-paragraph lr ls it lt b lu ml ju lw lx mm jx lz le mn mb mc li mo me mf lm mp mh mi mj im bi translated">此外，记住<strong class="lt iu">这40-43个问题中有30%会特别棘手</strong>，至少有两个非常相似的选项，所以你需要非常确定语法。但是请记住:最坏的情况下，你可以随时查阅文档资料(<em class="mk">让我们回到第一点</em>)。</p><p id="3dbc" class="pw-post-body-paragraph lr ls it lt b lu ml ju lw lx mm jx lz le mn mb mc li mo me mf lm mp mh mi mj im bi translated">现在是进行一些测验的时候了！</p><div class="nx ny gp gr nz oa"><a rel="noopener follow" target="_blank" href="/3-nanodegrees-you-should-consider-to-advance-your-data-engineering-career-in-2021-baf597debc72"><div class="ob ab fo"><div class="oc ab od cl cj oe"><h2 class="bd iu gy z fp of fr fs og fu fw is bi translated">3门数据工程课程，在2021年推进您的职业发展</h2><div class="oh l"><h3 class="bd b gy z fp of fr fs og fu fw dk translated">加入数据行业，改变角色或通过注册数据工程简单地学习前沿技术…</h3></div><div class="oi l"><p class="bd b dl z fp of fr fs og fu fw dk translated">towardsdatascience.com</p></div></div><div class="oj l"><div class="pt l ol om on oj oo ko oa"/></div></div></a></div><h1 id="4712" class="mx kw it bd kx my oq na la nb or nd ld jz os ka lh kc ot kd ll kf ou kg lp nh bi translated">问题1</h1><pre class="kj kk kl km gt pu oy pv pw aw px bi"><span id="a7a5" class="kv kw it oy b gy py pz l qa qb"><em class="mk">Given a dataframe </em><strong class="oy iu"><em class="mk">df</em></strong><em class="mk">, select the code that returns its number of rows:</em></span><span id="bdca" class="kv kw it oy b gy qc pz l qa qb">A. df.take('all') <br/>B. df.collect()<br/>C. df.show()<br/>D. df.count() --&gt; <strong class="oy iu">CORRECT</strong><br/>E. df.numRows()</span></pre><p id="f98e" class="pw-post-body-paragraph lr ls it lt b lu ml ju lw lx mm jx lz le mn mb mc li mo me mf lm mp mh mi mj im bi translated">正确答案是<strong class="lt iu"> D </strong>，因为<code class="fe ov ow ox oy b">df.count()</code>实际上返回了一个<strong class="lt iu">数据帧</strong>中的行数，正如你在文档中看到的。这是一个热身问题，但不要忘记它，因为你可以找到类似的东西。</p><h1 id="0b69" class="mx kw it bd kx my oq na la nb or nd ld jz os ka lh kc ot kd ll kf ou kg lp nh bi translated">问题2</h1><pre class="kj kk kl km gt pu oy pv pw aw px bi"><span id="5e67" class="kv kw it oy b gy py pz l qa qb"><em class="mk">Given a DataFrame </em><strong class="oy iu"><em class="mk">df</em></strong><em class="mk"> that includes a number of columns among which a column named </em><strong class="oy iu"><em class="mk">quantity </em></strong><em class="mk">and a column named </em><strong class="oy iu"><em class="mk">price</em></strong><em class="mk">, complete the code below such that it will create a DataFrame including all the original columns and a new column </em><strong class="oy iu"><em class="mk">revenue </em></strong><em class="mk">defined as </em><strong class="oy iu"><em class="mk">quantity*price:</em></strong></span><span id="6dda" class="kv kw it oy b gy qc pz l qa qb">df._1_(_2_ , _3_)</span><span id="8c75" class="kv kw it oy b gy qc pz l qa qb">A. withColumnRenamed, "revenue", expr("quantity*price")<br/>B. withColumn, revenue, expr("quantity*price")<br/>C. withColumn, "revenue", expr("quantity*price") --&gt; <strong class="oy iu">CORRECT</strong><br/>D. withColumn, expr("quantity*price"), "revenue"<br/>E. withColumnRenamed, "revenue", col("quantity")*col("price")</span></pre><p id="71c5" class="pw-post-body-paragraph lr ls it lt b lu ml ju lw lx mm jx lz le mn mb mc li mo me mf lm mp mh mi mj im bi translated">正确答案是<strong class="lt iu"> C </strong>，因为代码应该是:</p><p id="e0f5" class="pw-post-body-paragraph lr ls it lt b lu ml ju lw lx mm jx lz le mn mb mc li mo me mf lm mp mh mi mj im bi translated"><code class="fe ov ow ox oy b">df.withColumn("revenue", expr("quantity*price"))</code></p><p id="3a53" class="pw-post-body-paragraph lr ls it lt b lu ml ju lw lx mm jx lz le mn mb mc li mo me mf lm mp mh mi mj im bi translated">你至少会被问到2-3个问题，涉及到在DF中添加一个新列或者重命名一个现有的列，所以要很好地学习<code class="fe ov ow ox oy b">withColumn()</code>和<code class="fe ov ow ox oy b">withColumnRenamed()</code>的语法。</p><h1 id="6f4d" class="mx kw it bd kx my oq na la nb or nd ld jz os ka lh kc ot kd ll kf ou kg lp nh bi translated">问题3</h1><pre class="kj kk kl km gt pu oy pv pw aw px bi"><span id="ca2c" class="kv kw it oy b gy py pz l qa qb"># <em class="mk">Given a DataFrame </em><strong class="oy iu"><em class="mk">df </em></strong><em class="mk">that</em><strong class="oy iu"><em class="mk"> </em></strong><em class="mk">has some null values in the column </em><strong class="oy iu"><em class="mk">created_date, </em></strong><em class="mk">complete the code below such that it will sort rows in ascending order based on the column </em><strong class="oy iu"><em class="mk">creted_date </em></strong><em class="mk">with null values appearing last.</em></span><span id="2541" class="kv kw it oy b gy qc pz l qa qb">df._1_(_2_)</span><span id="ebeb" class="kv kw it oy b gy qc pz l qa qb">A. orderBy, asc_nulls_last("created_date")<br/>B. sort, asc_nulls_last("created_date")<br/>C. orderBy, col("created_date").asc_nulls_last() --&gt; <strong class="oy iu">CORRECT</strong><br/>D. orderBy, col("created_date"), ascending=True)<br/>E. orderBy, col("created_date").asc()</span></pre><p id="04e8" class="pw-post-body-paragraph lr ls it lt b lu ml ju lw lx mm jx lz le mn mb mc li mo me mf lm mp mh mi mj im bi translated">正确答案是<strong class="lt iu"> C </strong>，因为代码应该是:</p><p id="e83b" class="pw-post-body-paragraph lr ls it lt b lu ml ju lw lx mm jx lz le mn mb mc li mo me mf lm mp mh mi mj im bi translated"><code class="fe ov ow ox oy b">df.orderBy(col("created_date").asc_null_last())</code></p><p id="3a7e" class="pw-post-body-paragraph lr ls it lt b lu ml ju lw lx mm jx lz le mn mb mc li mo me mf lm mp mh mi mj im bi translated">而且<code class="fe ov ow ox oy b">df.orderBy(df.created_date.asc_null_last())</code>也会起作用。</p><p id="9f39" class="pw-post-body-paragraph lr ls it lt b lu ml ju lw lx mm jx lz le mn mb mc li mo me mf lm mp mh mi mj im bi translated">实际上，与答案E中的<code class="fe ov ow ox oy b">asc()</code>一样，<code class="fe ov ow ox oy b">asc_null_last()</code>不接受任何参数，而是应用于column以返回基于列的升序的排序表达式，null值出现在非null值之后。</p><h1 id="96ef" class="mx kw it bd kx my oq na la nb or nd ld jz os ka lh kc ot kd ll kf ou kg lp nh bi translated">问题4</h1><pre class="kj kk kl km gt pu oy pv pw aw px bi"><span id="c14b" class="kv kw it oy b gy py pz l qa qb">Which one of the following commands does NOT trigger an eager evaluation?</span><span id="b740" class="kv kw it oy b gy qc pz l qa qb">A. df.collect()<br/>B. df.take()<br/>C. df.show()<br/>D. df.saveAsTable()<br/>E. df.join() --&gt; <strong class="oy iu">CORRECT</strong></span></pre><p id="b9b7" class="pw-post-body-paragraph lr ls it lt b lu ml ju lw lx mm jx lz le mn mb mc li mo me mf lm mp mh mi mj im bi translated">正确答案是<strong class="lt iu"> E </strong>因为在Apache Spark中，所有的转换都被缓慢地<em class="mk">求值，所有的动作都被急切地<em class="mk">求值</em>。在这种情况下，唯一会被延迟评估的命令是<code class="fe ov ow ox oy b">df.join()</code>。</em></p><p id="6b48" class="pw-post-body-paragraph lr ls it lt b lu ml ju lw lx mm jx lz le mn mb mc li mo me mf lm mp mh mi mj im bi translated">下面，您会发现一些经常出现在类似问题中的附加转换和操作:</p><pre class="kj kk kl km gt pu oy pv pw aw px bi"><span id="2673" class="kv kw it oy b gy py pz l qa qb"><strong class="oy iu">Transformations    Actions</strong><br/>orderBy()          show()<br/>groupBy()          take()<br/>filter()           count()<br/>select()           collect()<br/>join()             save()<br/>limit()            foreach()<br/>map(), flatMap()   first()<br/>sort()             count(), countByValue()<br/>printSchema()      reduce() <br/>cache()</span></pre><h1 id="a10e" class="mx kw it bd kx my oq na la nb or nd ld jz os ka lh kc ot kd ll kf ou kg lp nh bi translated">问题5</h1><pre class="kj kk kl km gt pu oy pv pw aw px bi"><span id="7abf" class="kv kw it oy b gy py pz l qa qb">Which of the following statements are NOT true for broadcast variables ?</span><span id="67e4" class="kv kw it oy b gy qc pz l qa qb">A. Broadcast variables are shared, immutable variables that are cached on every machine in the cluster instead of being serialized with every single task.</span><span id="00b4" class="kv kw it oy b gy qc pz l qa qb">B. A custom broadcast class can be defined by extending org.apache.spark.utilbroadcastV2 in Java or Scala or pyspark.Accumulatorparams in Python. --&gt; <strong class="oy iu">CORRECT</strong></span><span id="a465" class="kv kw it oy b gy qc pz l qa qb">C. It is a way of updating a value inside a variety of transformations and propagating that value to the driver node in an efficient and fault-tolerant way.--&gt; <strong class="oy iu">CORRECT</strong></span><span id="ff3c" class="kv kw it oy b gy qc pz l qa qb">D. It provides a mutable variable that Spark cluster can safely update on a per-row basis. --&gt; <strong class="oy iu">CORRECT</strong></span><span id="8f0a" class="kv kw it oy b gy qc pz l qa qb">E. The canonical use case is to pass around a small table that does fit in memory on executors.</span></pre><p id="fdbb" class="pw-post-body-paragraph lr ls it lt b lu ml ju lw lx mm jx lz le mn mb mc li mo me mf lm mp mh mi mj im bi translated">正确的选项是<strong class="lt iu"> B </strong>、<strong class="lt iu"> C </strong>和<strong class="lt iu"> D </strong>，因为这些是<a class="ae ku" href="https://sparkbyexamples.com/spark/spark-accumulators/" rel="noopener ugc nofollow" target="_blank"> <em class="mk">累加器</em> </a> ( <em class="mk">分布式共享变量</em>的替代类型)的特征。</p><p id="28d1" class="pw-post-body-paragraph lr ls it lt b lu ml ju lw lx mm jx lz le mn mb mc li mo me mf lm mp mh mi mj im bi translated">对于考试，请记住，广播变量是不可变的，并且在触发一个动作时会在集群中的所有节点上延迟复制。广播变量在规模上是有效的，因为它们避免了为每个任务序列化数据的成本。它们可以用在rdd或结构化API的上下文中。</p><h1 id="9de9" class="mx kw it bd kx my oq na la nb or nd ld jz os ka lh kc ot kd ll kf ou kg lp nh bi translated">问题6</h1><pre class="kj kk kl km gt pu oy pv pw aw px bi"><span id="c4d3" class="kv kw it oy b gy py pz l qa qb">The code below should return a new DataFrame with 50 percent of random records from DataFrame <strong class="oy iu">df </strong>without replacement. </span><span id="3e37" class="kv kw it oy b gy qc pz l qa qb">Choose the response that correctly fills in the numbered blanks within the code block to complete this task.</span><span id="4c4d" class="kv kw it oy b gy qc pz l qa qb"><strong class="oy iu">df._1_(_2_,_3_,_4_)</strong></span><span id="07ca" class="kv kw it oy b gy qc pz l qa qb">A. sample, False, 0.5, 5 --&gt; <strong class="oy iu">CORRECT<br/></strong>B. random, False, 0.5, 5<br/>C. sample, False, 5, 25<br/>D. sample, False, 50, 5<br/>E. sample, withoutReplacement, 0.5, 5</span></pre><p id="a7d6" class="pw-post-body-paragraph lr ls it lt b lu ml ju lw lx mm jx lz le mn mb mc li mo me mf lm mp mh mi mj im bi translated">正确答案是<strong class="lt iu"> A </strong>因为代码块应该是<code class="fe ov ow ox oy b">df.sample(False, 0.5, 5)</code>事实上<code class="fe ov ow ox oy b">sample()</code>的正确语法是:</p><p id="55d4" class="pw-post-body-paragraph lr ls it lt b lu ml ju lw lx mm jx lz le mn mb mc li mo me mf lm mp mh mi mj im bi translated"><code class="fe ov ow ox oy b">df.sample(withReplacement, fraction, seed)</code></p><p id="908e" class="pw-post-body-paragraph lr ls it lt b lu ml ju lw lx mm jx lz le mn mb mc li mo me mf lm mp mh mi mj im bi translated">在这种情况下,<code class="fe ov ow ox oy b">seed</code>是一个随机数，与答案无关。你应该记住它是顺序中的最后一个。</p><h1 id="0b1a" class="mx kw it bd kx my oq na la nb or nd ld jz os ka lh kc ot kd ll kf ou kg lp nh bi translated">问题7</h1><pre class="kj kk kl km gt pu oy pv pw aw px bi"><span id="d64a" class="kv kw it oy b gy py pz l qa qb">Which of the following DataFrame commands will NOT generate a shuffle of data from each executor across the cluster?</span><span id="d077" class="kv kw it oy b gy qc pz l qa qb">A. df.map() --&gt; <strong class="oy iu">CORRECT</strong><br/>B. df.collect()<br/>C. df.orderBy()<br/>D. df.repartition()<br/>E. df.distinct()<br/>F. df.join()</span></pre><p id="39da" class="pw-post-body-paragraph lr ls it lt b lu ml ju lw lx mm jx lz le mn mb mc li mo me mf lm mp mh mi mj im bi translated">正确答案是<strong class="lt iu"> A </strong>，因为<code class="fe ov ow ox oy b">map()</code>是列表中唯一的窄变换。</p><p id="c42c" class="pw-post-body-paragraph lr ls it lt b lu ml ju lw lx mm jx lz le mn mb mc li mo me mf lm mp mh mi mj im bi translated">特别地，转换可以被分类为具有<strong class="lt iu"> <em class="mk">窄依赖性</em> </strong>或<strong class="lt iu"> <em class="mk">宽依赖性</em> </strong>。任何可以从单个输入分区计算出单个输出分区的转换都是窄转换。例如，<code class="fe ov ow ox oy b">filter()</code>、<code class="fe ov ow ox oy b">contains()</code>和<code class="fe ov ow ox oy b">map()</code>表示窄转换，因为它们可以在单个分区上操作，并在没有任何数据交换的情况下产生结果输出分区。</p><p id="016a" class="pw-post-body-paragraph lr ls it lt b lu ml ju lw lx mm jx lz le mn mb mc li mo me mf lm mp mh mi mj im bi translated">下面你会发现一个列表，包括许多狭义和广义的变换，在考试前复习一下很有用:</p><pre class="kj kk kl km gt pu oy pv pw aw px bi"><span id="b6df" class="kv kw it oy b gy py pz l qa qb"><strong class="oy iu">WIDE TRANSORM      NARROW TRANSFORM</strong><br/>orderBy()           filter()<br/>repartition()       contains()<br/>distinct()          map()<br/>collect()           flatMap()<br/>cartesian()         MapPartition()<br/>intersection()      sample()<br/>reduceByKey()       union()<br/>groupByKey()        coalesce() --&gt; when numPartitions is reduced<br/>groupBy()           drop()<br/>join()              cache()</span></pre><h1 id="ecef" class="mx kw it bd kx my oq na la nb or nd ld jz os ka lh kc ot kd ll kf ou kg lp nh bi translated">问题8</h1><pre class="kj kk kl km gt pu oy pv pw aw px bi"><span id="4512" class="kv kw it oy b gy py pz l qa qb">When Spark runs in Cluster Mode, which of the following statements about nodes is correct ?</span><span id="f838" class="kv kw it oy b gy qc pz l qa qb">A. There is one single worker node that contains the Spark driver and all the executors.</span><span id="7fb9" class="kv kw it oy b gy qc pz l qa qb">B. The Spark Driver runs in a worker node inside the cluster. <strong class="oy iu">--&gt; CORRECT</strong></span><span id="8817" class="kv kw it oy b gy qc pz l qa qb">C. There is always more than one worker node.</span><span id="b785" class="kv kw it oy b gy qc pz l qa qb">D. There are less executors than total number of worker nodes.</span><span id="344e" class="kv kw it oy b gy qc pz l qa qb">E. Each executor is a running JVM inside of a cluster manager node.</span></pre><p id="6d68" class="pw-post-body-paragraph lr ls it lt b lu ml ju lw lx mm jx lz le mn mb mc li mo me mf lm mp mh mi mj im bi translated">正确答案是B，因为在<strong class="lt iu"> <em class="mk">集群模式</em> </strong>中，除了执行器进程之外，集群管理器还在集群内的工作节点上启动驱动程序进程。这意味着集群管理器负责维护所有Spark工作节点。因此，集群管理器将驱动程序放在一个工作节点上，将执行器放在不同的工作节点上。</p><h1 id="ffae" class="mx kw it bd kx my oq na la nb or nd ld jz os ka lh kc ot kd ll kf ou kg lp nh bi translated">问题9</h1><pre class="kj kk kl km gt pu oy pv pw aw px bi"><span id="194c" class="kv kw it oy b gy py pz l qa qb">The DataFrame <strong class="oy iu">df </strong>includes a time string column named <strong class="oy iu">timestamp_1</strong>. Which is the correct syntax that creates a new DataFrame <strong class="oy iu">df1</strong> that is just made by the time string field converted to a unix timestamp?</span><span id="58bb" class="kv kw it oy b gy qc pz l qa qb">A. df1 = df.select(unix_timestamp(col("timestamp_1"),"MM-dd-yyyy HH:mm:ss").as("timestamp_1"))</span><span id="4682" class="kv kw it oy b gy qc pz l qa qb">B. df1 = df.select(unix_timestamp(col("timestamp_1"),"MM-dd-yyyy HH:mm:ss", "America/Los Angeles").alias("timestamp_1"))</span><span id="ff00" class="kv kw it oy b gy qc pz l qa qb">C. df1 = df.select(unix_timestamp(col("timestamp_1"),"America/Los Angeles").alias("timestamp_1"))</span><span id="f8ef" class="kv kw it oy b gy qc pz l qa qb">D. df1 = df.select(unixTimestamp(col("timestamp_1"),"America/Los Angeles").alias("timestamp_1"))</span><span id="f3e4" class="kv kw it oy b gy qc pz l qa qb">E. df1 = df.select(unix_timestamp(col("timestamp_1"),"MM-dd-yyyy HH:mm:ss").alias("timestamp_1"))</span></pre><p id="f4ee" class="pw-post-body-paragraph lr ls it lt b lu ml ju lw lx mm jx lz le mn mb mc li mo me mf lm mp mh mi mj im bi translated">正确答案是<strong class="lt iu"> E </strong>，因为<code class="fe ov ow ox oy b">unix_timestamp()</code>的正确语法是:</p><p id="e0c4" class="pw-post-body-paragraph lr ls it lt b lu ml ju lw lx mm jx lz le mn mb mc li mo me mf lm mp mh mi mj im bi translated"><code class="fe ov ow ox oy b">unix_timestamp(timestamp, format)</code></p><p id="838d" class="pw-post-body-paragraph lr ls it lt b lu ml ju lw lx mm jx lz le mn mb mc li mo me mf lm mp mh mi mj im bi translated">这个函数不包括<em class="mk">时区</em>参数，因为它意味着使用默认的时区。同样，在PySpark中，在函数内部重命名列的正确方法是<code class="fe ov ow ox oy b">alias()</code>。</p><h1 id="4582" class="mx kw it bd kx my oq na la nb or nd ld jz os ka lh kc ot kd ll kf ou kg lp nh bi translated">问题10</h1><pre class="kj kk kl km gt pu oy pv pw aw px bi"><span id="1475" class="kv kw it oy b gy py pz l qa qb">If you wanted to:</span><span id="1eba" class="kv kw it oy b gy qc pz l qa qb">1. Cache a <strong class="oy iu">df</strong> as SERIALIZED Java objects in the JVM and; <br/>2. If the <strong class="oy iu">df</strong> does not fit in memory, store the partitions that don’t fit on disk, and read them from there when they’re needed; <br/>3. Replicate each partition on two cluster nodes.</span><span id="e78d" class="kv kw it oy b gy qc pz l qa qb">which command would you choose ?</span><span id="5895" class="kv kw it oy b gy qc pz l qa qb">A. <!-- -->df.persist(StorageLevel.MEMORY_ONLY)<br/>B. <!-- -->df.persist(StorageLevel.MEMORY_AND_DISK_SER)<br/>C. <!-- -->df.cache(StorageLevel.MEMORY_AND_DISK_2_SER)<br/>D. df.cache(StorageLevel.MEMORY_AND_DISK_2_SER)<br/>E. df.persist(StorageLevel.MEMORY_AND_DISK_2_SER) --&gt; <strong class="oy iu">CORRECT</strong></span></pre><p id="800e" class="pw-post-body-paragraph lr ls it lt b lu ml ju lw lx mm jx lz le mn mb mc li mo me mf lm mp mh mi mj im bi translated">正确答案是<strong class="lt iu"> E </strong>，因为正确的命令应该是:</p><p id="17eb" class="pw-post-body-paragraph lr ls it lt b lu ml ju lw lx mm jx lz le mn mb mc li mo me mf lm mp mh mi mj im bi translated"><code class="fe ov ow ox oy b">df.persist(StorageLevel.MEMORY_AND_DISK_2_SER)</code></p><p id="0a47" class="pw-post-body-paragraph lr ls it lt b lu ml ju lw lx mm jx lz le mn mb mc li mo me mf lm mp mh mi mj im bi translated">事实上，对于Spark数据帧，默认情况下，<code class="fe ov ow ox oy b">cache()</code>命令总是将数据放在内存和磁盘中(<code class="fe ov ow ox oy b">MEMORY_AND_DISK</code>)。相反，<code class="fe ov ow ox oy b">persist()</code>方法可以接受一个<code class="fe ov ow ox oy b">StorageLevel</code>对象来指定缓存数据的确切位置。</p><p id="4821" class="pw-post-body-paragraph lr ls it lt b lu ml ju lw lx mm jx lz le mn mb mc li mo me mf lm mp mh mi mj im bi translated">请记住，<strong class="lt iu"> <em class="mk">数据存储在磁盘上时总是被序列化，而您需要指定是否希望在内存</em> </strong>中序列化数据(例如<code class="fe ov ow ox oy b">MEMORY_AND_DISK_2_SER</code>)。</p><h1 id="296e" class="mx kw it bd kx my oq na la nb or nd ld jz os ka lh kc ot kd ll kf ou kg lp nh bi translated">结论</h1><p id="30ba" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz le ma mb mc li md me mf lm mg mh mi mj im bi translated">在这篇文章中，我分享了<strong class="lt iu"> 10个mcq(选择题)</strong>你应该用来准备<strong class="lt iu"> Databricks Apache Spark 3.0开发者认证</strong>。这些问题与你在真实考试中遇到的问题极其相似，因此我希望这对你来说是一个有价值的学习资源。</p><p id="93c1" class="pw-post-body-paragraph lr ls it lt b lu ml ju lw lx mm jx lz le mn mb mc li mo me mf lm mp mh mi mj im bi translated">如果你觉得这份材料有用，请随时在评论中告诉我，因为我很乐意写一份包括10个小测验的<strong class="lt iu">第2部分</strong>。</p><p id="dc71" class="pw-post-body-paragraph lr ls it lt b lu ml ju lw lx mm jx lz le mn mb mc li mo me mf lm mp mh mi mj im bi translated">现在，我将留给你一些关于媒体的其他文章，这些文章涉及与认证相关的更一般的主题。</p></div><div class="ab cl mq mr hx ms" role="separator"><span class="mt bw bk mu mv mw"/><span class="mt bw bk mu mv mw"/><span class="mt bw bk mu mv"/></div><div class="im in io ip iq"><h2 id="05eb" class="kv kw it bd kx ky kz dn la lb lc dp ld le lf lg lh li lj lk ll lm ln lo lp lq bi translated"><strong class="ak">其他有用的文章</strong></h2><ol class=""><li id="435f" class="pe pf it lt b lu lv lx ly le qd li qe lm qf mj qg pk pl pm bi translated"><a class="ae ku" href="https://shrutibhawsar94.medium.com/study-guide-for-clearing-databricks-certified-associate-developer-for-apache-spark-3-0-69377dba0107" rel="noopener">通过<a class="qh qi ep" href="https://medium.com/u/23bd446f81d8?source=post_page-----bd886060b9ab--------------------------------" rel="noopener" target="_blank"> Shruti Bhawsar </a>的【Apache Spark 3.0认证助理开发人员】考试(Python) </a>的学习指南</li><li id="5450" class="pe pf it lt b lu pn lx po le pp li pq lm pr mj qg pk pl pm bi translated"><a class="ae ku" href="https://medium.com/@anoop_mk/guide-and-tips-for-apache-spark-3-0-2-4-databricks-certification-preparation-dad93c5ed4bb" rel="noopener">Apache Spark 3.0/2.4 data bricks认证准备指南和提示</a>由<a class="qh qi ep" href="https://medium.com/u/c81db296302e?source=post_page-----bd886060b9ab--------------------------------" rel="noopener" target="_blank"> Anoop Malayamkumarath </a></li><li id="cd7e" class="pe pf it lt b lu pn lx po le pp li pq lm pr mj qg pk pl pm bi translated"><a class="ae ku" href="https://medium.com/@sriramn84_34423/crack-databricks-certified-associate-developer-for-apache-spark-3-0-cf4cb89df61d" rel="noopener">Crack data bricks Apache Spark 3.0认证助理开发人员-准备提示，信息&amp;澄清</a>由<a class="qh qi ep" href="https://medium.com/u/717abcdc6dc2?source=post_page-----bd886060b9ab--------------------------------" rel="noopener" target="_blank"> Sriram Narayanan </a></li></ol></div></div>    
</body>
</html>