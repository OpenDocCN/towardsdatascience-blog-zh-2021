<html>
<head>
<title>An interesting walk from Bayesian statistics: Differences between MAP and MLE.</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">贝叶斯统计的一次有趣的散步:MAP和MLE之间的差异。</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/an-interesting-walk-from-bayesian-statistics-differences-between-map-and-mle-2daea5fc15b?source=collection_archive---------29-----------------------#2021-07-23">https://towardsdatascience.com/an-interesting-walk-from-bayesian-statistics-differences-between-map-and-mle-2daea5fc15b?source=collection_archive---------29-----------------------#2021-07-23</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi jn"><img src="../Images/02e49fd2ec8a6809eaf9ba393bfeff7d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*_q_aty8bUoK9uFOS"/></div></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">纳赛尔·塔米米在<a class="ae kc" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="25aa" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这篇文章将让你了解任何机器学习模型的基础。用简单的文字和几个公式，你将得到贝叶斯，最大似然估计和最大后验估计充分的能力。</p><p id="9805" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">具体来说，让我们从概率和判别函数的角度开始讨论二元分类。这是一次有趣的步行，旨在了解scikit-learn和keras设施之外的机器学习算法的原理。</p><p id="e767" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">想象一下，我们必须将汽车分为两类:环保型和污染型。我们拥有的关于汽车的信息是制造年份和消耗量，我们用两个随机变量X1和X2来表示。当然，还有其他影响分类的因素，但它们是不可观测的。记住我们可以观察到的，汽车的分类由伯努利随机变量C表示(<em class="lb"> C=1 </em>表示环保，<em class="lb"> C=0 </em>污染)，条件是可观察值X =【X1，X2】。因此，如果我们知道<em class="lb"> P(C|X1，X2) </em>，当一个新的入口数据点以<em class="lb"> X1=x1 </em>和<em class="lb"> X2=x2 </em>出现时，我们会说:</p><figure class="ld le lf lg gt jr gh gi paragraph-image"><div class="gh gi lc"><img src="../Images/c59e2ed83cf275570120a089f6ae9935.png" data-original-src="https://miro.medium.com/v2/resize:fit:1032/format:webp/1*szDT9aqUvdwqoHFgHRGZaw.png"/></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">作者图片</p></figure><p id="387f" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">其中错误概率为<em class="lb"> 1-max(P(C=1|x1，x2)，P(C=0|x1，x2)) </em>。表示<em class="lb"> x=[x1，x2] </em>那么我们可以使用贝叶斯定理，它可以写成:</p><figure class="ld le lf lg gt jr gh gi paragraph-image"><div class="gh gi lh"><img src="../Images/cb00b0ed28281dbb39bd42492fcf5272.png" data-original-src="https://miro.medium.com/v2/resize:fit:500/format:webp/1*c-nw1PQYYNgq-Mr177TG5w.png"/></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">作者图片</p></figure><p id="b419" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">P(C)被称为<strong class="kf ir">先验概率</strong>，因为它是我们之前在数据集中拥有的关于C的信息，用来分析可观测的<em class="lb"> x. </em>正如你所想象的，<em class="lb"> 1=P(C=1)+P(C=0)。</em></p><p id="f047" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">P(x|C)被称为<strong class="kf ir">类可能性</strong>，因为它是属于C=c的事件具有相关观察值<em class="lb"> x. </em>的概率</p><p id="1af4" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">P(x)是<strong class="kf ir">证据</strong>，因为它是在我们的数据集中看到观察值<em class="lb"> x </em>的概率。这个术语也称为规范化术语，因为它将表达式相除，得到一个介于0和1之间的值。证据概率是所有可能情况的总和(也称为全概率定律):</p><p id="88f9" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">P(x)= P(x | C)1)P(C = 1)+P(x | C = 0)P(C = 0)</p><p id="78e3" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">最后，P(C|x)是我们想要知道的，每一类给定一个观察值的概率。这被称为<strong class="kf ir">后验概率</strong>因为我们在看到观察值x后对其进行了分类，当然<em class="lb"> P(C=0|x)+P(C=1|x)=1。</em></p><p id="7278" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在有几个类的情况下，我们可以将这个表达式扩展到<em class="lb"> K </em>类:</p><figure class="ld le lf lg gt jr gh gi paragraph-image"><div class="gh gi li"><img src="../Images/4cef6981f78f878a1b998ac205bdb442.png" data-original-src="https://miro.medium.com/v2/resize:fit:1076/format:webp/1*4r1axL1pGTHsIHFiNMzzWg.png"/></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">作者图片</p></figure><p id="6ab4" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">并且贝叶斯分类器将从<em class="lb"> K </em>个选项中选择具有最高后验概率P(Ci|x)的类。</p><p id="2d03" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><a class="ae kc" rel="noopener" target="_blank" href="/maths-behind-supervised-learning-for-dummies-the-theory-in-plain-words-part-i-8f9be4d7e33a">在之前的文章</a>中，我解释了如何根据区分函数来理解分类，区分函数限制了空间中的假设类别区域。按照同样的思路，我们可以实现那组判别函数。具体来说，我们选择C_i，如果:</p><figure class="ld le lf lg gt jr gh gi paragraph-image"><div class="gh gi lj"><img src="../Images/105a63c9cf9c4c509ef316ab4ee82dac.png" data-original-src="https://miro.medium.com/v2/resize:fit:716/format:webp/1*_UI-kF2tBw3_arVsPRVClg.png"/></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">作者图片</p></figure><p id="2077" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如果我们将判别函数视为贝叶斯分类器，则最大判别函数对应于最大后验概率，得到:</p><figure class="ld le lf lg gt jr gh gi paragraph-image"><div class="gh gi lk"><img src="../Images/fb3dfc725efec266852190b0f04bcc4f.png" data-original-src="https://miro.medium.com/v2/resize:fit:828/format:webp/1*wnmerSBAsvQ_JbLOMbm_Jg.png"/></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">作者图片</p></figure><p id="ee8c" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">请注意，我们已经删除了归一化项(证据),因为它对所有判别函数都是一样的。该集合将特征空间分成K个决策区域:</p><figure class="ld le lf lg gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi ll"><img src="../Images/8c40411526a6119d269126cd2df7262b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*k2INXO4OOEZcjMH7orpmFA.png"/></div></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">作者图片</p></figure></div><div class="ab cl lm ln hu lo" role="separator"><span class="lp bw bk lq lr ls"/><span class="lp bw bk lq lr ls"/><span class="lp bw bk lq lr"/></div><div class="ij ik il im in"><p id="3a7a" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在实践中，我们观察到的信息片段，又名<em class="lb">数据点</em>，它们通常遵循一个给定的概率分布，我们将其表示为<em class="lb"> P(X=x) </em>，正如我们之前所讨论的。当我们有像P(C=1|x)或P(X=x)这样的值时，贝叶斯分类似乎很容易计算。然而，如果我们不确切地知道<em class="lb">P(X)</em>，因为我们不能访问所有的数据点，我们试图从可用的信息中估计它，于是统计就出现了。</p><blockquote class="lt lu lv"><p id="bae1" class="kd ke lb kf b kg kh ki kj kk kl km kn lw kp kq kr lx kt ku kv ly kx ky kz la ij bi translated">统计数据是从给定样本中计算出的任何值。</p></blockquote><p id="4827" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">P(X)可以有不同的形状，但是如果给定的样本来自一个已知的分布，那么我们说它遵循一个参数分布。参数分布是非常理想的，因为我们只需要知道几个参数就可以画出一个完整的分布。我们该怎么办？基于我们所看到的(样本)，我们通过计算样本的统计量来估计分布的参数，即分布的<em class="lb">充分统计量</em>。这使我们能够获得数据的估计分布，我们将使用它来做出假设和决策。</p><h1 id="3f80" class="lz ma iq bd mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu mv mw bi translated">最大似然估计</h1><p id="3e92" class="pw-post-body-paragraph kd ke iq kf b kg mx ki kj kk my km kn ko mz kq kr ks na ku kv kw nb ky kz la ij bi translated">如果你已经到了这一步，请忘记之前贝叶斯解释的一切。</p><p id="20fc" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">让我们从头开始:我们的数据集中有N个数据点。看起来这些数据点的概率密度遵循一个清晰的分布族<strong class="kf ir"> p(x)，</strong>但是我们对真实分布的形状没有任何线索(它的真实中心，它的真实离差，等等)。如果我们面对的是参数分布，那么计算定义真实分布的参数就很简单。怎么会？让我们根据现有的给定样本来看看最有可能的选项。</p><p id="1487" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们有一个独立同分布(iid)样本:</p><figure class="ld le lf lg gt jr gh gi paragraph-image"><div class="gh gi nc"><img src="../Images/7a0f3341ee122e6797d7289707369bed.png" data-original-src="https://miro.medium.com/v2/resize:fit:476/format:webp/1*x43PAVeizj5cnndlYk3wyQ.png"/></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">作者图片</p></figure><p id="db56" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">它的概率密度由一组参数θ定义。让我们找出使给定样本的分布尽可能接近的具体θ参数。因为数据点是独立的，所以它们的似然性是各个点的似然性的乘积。具体来说，对于给定的θ，我们来看概率p(x|θ)。概率越高，匹配度越高。</p><figure class="ld le lf lg gt jr gh gi paragraph-image"><div class="gh gi nd"><img src="../Images/7ee5dbf3b32fbfa45a7d1d6bce1b005b.png" data-original-src="https://miro.medium.com/v2/resize:fit:728/format:webp/1*njeK8uJ6RDzN_9Fx_s1fgw.png"/></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">作者图片</p></figure><p id="8056" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">那么，对于给定的数据集X，<em class="lb"> l(θ|X) </em>的θ的可能性等于<em class="lb"> p(X|θ)。</em>为了使事情更简单，我们通常将对数应用于可能性，因为计算速度更快(求和而不是乘法):</p><figure class="ld le lf lg gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi ne"><img src="../Images/e31c13c1b424ed1898e34209ca2d60fe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_i99QAOZOP_30K2339Mn-w.png"/></div></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">作者图片</p></figure><p id="554e" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在，我们只需驱动对数似然，然后根据θ将其最大化(例如使用梯度下降)。</p><p id="1f1b" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir">重要提示:</strong>我们通常互换术语“可能性”和“概率”。然而，它们是不同的东西。概率是固定分布(特定情况)下的面积。对于概率，我们想知道给定分布情况下的概率。相比之下，可能性是分布中可以移动的固定点的y轴值。似然性是指为某个特征的固定值找到最佳分布。可能性包含的不确定性信息要少得多，它只是一个点而不是一个区域。</p><figure class="ld le lf lg gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nf"><img src="../Images/52a12d3e8bb611675cfdc2296c8226e4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pC65XCqekwIR_Zm5Cfl4rA.png"/></div></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">作者图片</p></figure><p id="fa82" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们搜索不同的θ值，选择使对数似然最大的θ:</p><figure class="ld le lf lg gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi ng"><img src="../Images/379fb68614e0310c607dd826923bdca1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*c98SyledFVGeEvAZSN7WVA.png"/></div></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">图片作者。蓝点代表样本的数据点。从不同的θ值得出不同的分布，选择“最匹配”的一个。</p></figure><p id="1f3e" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如果我们观察许多不同的样本，分布参数的估计量将更准确，与真实参数的偏差更小。稍后我们将看到如何评估估计量的质量，换句话说，估计量与θ的差异有多大。</p><p id="9219" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir">将最大似然法应用于伯努利分布</strong></p><p id="2bf0" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在伯努利中，有两种可能的状态:真或假，是或否。事件发生或不发生。因此，每个数据点可以取两个值:0或1。实际上，它以概率<em class="lb"> p，</em>取值1，以概率<em class="lb"> 1-p. </em>取值0，因此:</p><figure class="ld le lf lg gt jr gh gi paragraph-image"><div class="gh gi nh"><img src="../Images/01246cd1f6ccb2c749649d989d12748d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1116/format:webp/1*vuS9EdCitpO-qjaWyQ-Zdg.png"/></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">作者图片</p></figure><p id="bc45" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">该分布用一个参数建模:<em class="lb"> p. </em>因此，我们想从一个样本中计算它的估计量p̂。让我们根据<em class="lb"> p: </em>来计算它的对数似然</p><figure class="ld le lf lg gt jr gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/2278d4913e0bde1f46852194bc45b355.png" data-original-src="https://miro.medium.com/v2/resize:fit:1232/format:webp/1*9IKFDjoBN3Fx5irKe6Ow8g.png"/></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">作者图片</p></figure><p id="d334" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在，求解<em class="lb"> dL/dp=0 </em>以获得最大化表达式的p̂:</p><figure class="ld le lf lg gt jr gh gi paragraph-image"><div class="gh gi nj"><img src="../Images/0d0055a4c63c9016b6b040a3e0efbe44.png" data-original-src="https://miro.medium.com/v2/resize:fit:396/format:webp/1*r5_MJk5zHZlvshxjW4bmVA.png"/></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">作者图片</p></figure><h1 id="21be" class="lz ma iq bd mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu mv mw bi translated">最大后验概率</h1><p id="055f" class="pw-post-body-paragraph kd ke iq kf b kg mx ki kj kk my km kn ko mz kq kr ks na ku kv kw nb ky kz la ij bi translated">现在是时候回到贝叶斯理论了。有时，在查看样本之前，我们可能有一些关于参数θ可能取值范围的信息。当样本很小时，这些信息会非常有用。</p><p id="e2c3" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">因此，先验概率<em class="lb"> p(θ) </em>定义了θ在查看样本之前可能取的值。如果我们想在查看X样本后知道可能的θ值，<em class="lb"> p(θ|X)，</em>我们可以使用贝叶斯:</p><figure class="ld le lf lg gt jr gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/a5962014d0150d1775127290c5f84415.png" data-original-src="https://miro.medium.com/v2/resize:fit:932/format:webp/1*PQilOmVfbDnf2MFKo1H1JQ.png"/></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">图片由作者基于[1]</p></figure><p id="1330" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir">注:</strong> <em class="lb">我们可以忽略规格化项(或证据)，当我们在谈论优化时，它并不影响。</em></p><p id="45b3" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">因此，如果我们想要一个模型来预测给定输入x的输出，<em class="lb"> y=g(x) </em>，就像在回归中一样，我们实际上是计算<em class="lb"> y=g(x|θ)。</em>视分布情况而定。</p><p id="92ae" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">然而，由于我们不知道<em class="lb"> θ、</em>的真实值，我们必须使用<em class="lb"> θ ( </em>按概率加权)的所有可能值对预测取平均值:</p><figure class="ld le lf lg gt jr gh gi paragraph-image"><div class="gh gi nl"><img src="../Images/6c1163ead3d4ff488b58a01d0365ac0e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/1*o4XNqovbQCFq-PBn7Cs3nA.png"/></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">作者图片</p></figure><p id="f653" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">积分的计算可能非常复杂和昂贵。代替这样做，我们可以只取一个点，最可能的点，最大后验概率。</p><p id="700f" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">因此，我们想要得到<em class="lb"> θ，</em>最可能的值，或者换句话说，最大化<em class="lb"> P(θ|X)，</em>，这与最大化<em class="lb"> P(X|θ)P(θ): </em>相同</p><figure class="ld le lf lg gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nm"><img src="../Images/73722585ae99d4fef8607c691e0011a8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8yYTOwCDOirBf1zBf5X4Sg.png"/></div></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">图片由作者基于[1]</p></figure><p id="1908" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如果你花一点时间将这个公式与MLE方程进行比较，你会发现它的不同之处仅在于MAP中包含了先验<em class="lb"> P(θ) </em>。换句话说，可能性是用来自先验的信息来加权的。最大似然法是无先验信息映射的特例。</p><blockquote class="lt lu lv"><p id="9cf4" class="kd ke lb kf b kg kh ki kj kk kl km kn lw kp kq kr lx kt ku kv ly kx ky kz la ij bi translated">MLE是映射的一个特例。</p></blockquote><p id="9534" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如果这个先验信息是恒定的(均匀分布)，它对最大化没有贡献，我们将得到MLE公式。</p><p id="4e21" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">具体来说，想象θ可以取六个不同的值。因此，P(θ)在分布中处处都是1/6:</p><figure class="ld le lf lg gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nn"><img src="../Images/0528a4b6e97dda7db5f57b5c83d734a9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DjI9s9H8RSUJEoQj03wvMg.png"/></div></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">图片由作者基于[1]</p></figure><p id="532a" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">然而，如果先验不是常数(它取决于分布的区域)，概率也不是常数，必须考虑它(例如，如果先验是高斯的)。</p><p id="7d0d" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir">重要:</strong>可以选择最大后验概率而不是计算积分，前提是我们可以假设<em class="lb"> P(θ|X) </em>在其模周围有一个窄峰。为什么？因为我们取的是单个点，而不是计算所有的区域，没有不确定性的表示。当模式周围没有窄峰时，可能会发生这样的事情:</p><figure class="ld le lf lg gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi no"><img src="../Images/1d821ceeae299303f75b3e6f91712170.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pQp9mCB0E0XIs15OMFUvqQ.png"/></div></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">作者图片</p></figure><p id="5033" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">参考书目:</p><p id="a733" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">[1]奥古斯丁·克里斯蒂亚迪的博客<a class="ae kc" href="https://wiseodd.github.io/techblog/2017/01/01/mle-vs-map/" rel="noopener ugc nofollow" target="_blank">https://wiseodd.github.io/techblog/2017/01/01/mle-vs-map/</a></p><p id="dfda" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">[2]艾瑟姆·阿尔帕丁。机器学习导论，第4版。</p></div></div>    
</body>
</html>