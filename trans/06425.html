<html>
<head>
<title>Feature Selection for Machine Learning: 3 Categories and 12 Methods</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">机器学习的特征选择:3类12种方法</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/feature-selection-for-machine-learning-3-categories-and-12-methods-6a4403f86543?source=collection_archive---------8-----------------------#2021-06-09">https://towardsdatascience.com/feature-selection-for-machine-learning-3-categories-and-12-methods-6a4403f86543?source=collection_archive---------8-----------------------#2021-06-09</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><figure class="gl gn jr js jt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi jq"><img src="../Images/cf6155bf4115d4fa87c76b63f4a83fab.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*R0YmXBr-QcePNc80O_nsAw.jpeg"/></div></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">图片由<a class="ae kf" href="https://unsplash.com/@fcornish" rel="noopener ugc nofollow" target="_blank"> Faye Cornish </a>在<a class="ae kf" href="https://unsplash.com/photos/ywRNdDfvMWs" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><p id="37de" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这篇文章的大部分内容来自我最近的一篇题为:<br/>“环境数据特征选择方法的评估”的论文，任何感兴趣的人都可以在这里<a class="ae kf" href="https://www.sciencedirect.com/science/article/pii/S1574954121000157" rel="noopener ugc nofollow" target="_blank">获得</a>。</p><h1 id="5d91" class="le lf it bd lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated">降维的两种方法</h1><p id="111e" class="pw-post-body-paragraph kg kh it ki b kj mc kl km kn md kp kq kr me kt ku kv mf kx ky kz mg lb lc ld im bi translated">有两种<strong class="ki iu">方法</strong>来减少特征的数量，也就是所谓的降维。</p><p id="2db5" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="ki iu">第一种</strong>方式被称为<strong class="ki iu">特征提取</strong>，其目的是转换特征并基于原始/给定特征的组合创建全新的特征。<br/>最流行的方法是<strong class="ki iu">主成分分析</strong>、<strong class="ki iu">线性判别分析</strong>和<strong class="ki iu">多维标度</strong>。然而，新的特征空间几乎不能为我们提供关于原始特征的有用信息。新的更高层次的特征不容易被人类理解，因为我们不能将它们直接与初始特征联系起来，从而难以得出结论和解释变量。</p><p id="684c" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="ki iu">第二种</strong>降维方式是<strong class="ki iu">特征选择</strong>。<br/>它可以被认为是一个<strong class="ki iu">预处理步骤</strong>，并且不创建任何新特征，而是选择原始特征的一个子集，提供更好的<strong class="ki iu">可解释性</strong>。<br/>从一个有意义的初始数字中找到最佳特征可以帮助我们提取有价值的信息<strong class="ki iu">发现新知识</strong>。<br/>在分类问题中，特征的重要性根据其<strong class="ki iu">解决不同类别</strong>的能力进行评估。<br/>给出每个特征在区分不同类别中的便利性的估计的属性被称为<strong class="ki iu">特征相关性。</strong></p></div><div class="ab cl mh mi hx mj" role="separator"><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm"/></div><div class="im in io ip iq"><h1 id="1c69" class="le lf it bd lg lh mo lj lk ll mp ln lo lp mq lr ls lt mr lv lw lx ms lz ma mb bi translated">特征选择的目标</h1><p id="795d" class="pw-post-body-paragraph kg kh it ki b kj mc kl km kn md kp kq kr me kt ku kv mf kx ky kz mg lb lc ld im bi translated">特征选择有许多目标。<br/> 1。它<strong class="ki iu">通过保留与目标变量具有最小冗余和最大相关性的特征来消除</strong> <strong class="ki iu">不相关和有噪声的特征</strong>。<br/> 2。它<strong class="ki iu">减少了训练和测试分类器的计算时间和复杂性</strong>，因此它产生了更具成本效益的模型。<br/> 3。它<strong class="ki iu">提高学习算法的性能</strong>，避免过度拟合，并有助于创建更好的通用模型。</p></div><div class="ab cl mh mi hx mj" role="separator"><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm"/></div><div class="im in io ip iq"><h1 id="027d" class="le lf it bd lg lh mo lj lk ll mp ln lo lp mq lr ls lt mr lv lw lx ms lz ma mb bi translated">3种特征选择方法</h1><p id="d947" class="pw-post-body-paragraph kg kh it ki b kj mc kl km kn md kp kq kr me kt ku kv mf kx ky kz mg lb lc ld im bi translated">根据它们与分类器的交互方式，有三类特征选择方法，即过滤器、包装器和嵌入式方法。</p><p id="5886" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="ki iu">过滤方法</strong>是可扩展的(直到非常高维的数据),并且在分类之前执行<em class="mt">快速</em>特征选择，使得<em class="mt">学习算法的偏差不会与特征选择算法的偏差</em>相互作用。他们主要充当排序者，将特性从最好到最差排序。<br/>特征的排序取决于数据的<em class="mt">内在属性</em>，例如，方差、一致性、距离、信息、相关性等。<br/>存在许多过滤方法，新的方法也在不断发展，每种方法都使用不同的标准来衡量数据的相关性。<br/>相关性<em class="mt">的一个通用定义</em>是，如果一个特征有条件地独立于类别标签或者不影响类别标签，则该特征可以被认为是不相关的；在这些情况下，它可以被丢弃。</p><p id="c9bb" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="ki iu">包装器方法</strong>使用机器学习算法作为<em class="mt">黑盒评估器</em>来寻找特征的最佳子集，因此，它们依赖于分类器。<br/>实际上，搜索策略和建模算法的任何组合都可以用作包装器。<br/>当在具有大量特征的数据集中执行包装时，它会消耗额外的计算资源和运行时间。<br/>最后，这些方法实现起来很简单，并且可以对特征依赖进行建模。</p><p id="1539" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="ki iu">嵌入式方法</strong> <em class="mt">在过滤器和包装器之间架起了桥梁</em>。首先，他们像过滤器一样融合可测量和统计标准来选择一些特征，然后使用机器学习算法，选择具有最佳分类性能的子集。<br/>它们<em class="mt">降低了</em>包装器的计算复杂度，无需在每次迭代中对子集进行重新分类，并且可以对特征依赖性进行建模。他们<em class="mt">不执行迭代</em>。<br/>特征选择在学习阶段进行，这意味着这些方法同时实现了<em class="mt">模型拟合和特征选择</em>。一个缺点是它们依赖于分类器。</p></div><div class="ab cl mh mi hx mj" role="separator"><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm"/></div><div class="im in io ip iq"><h1 id="92fd" class="le lf it bd lg lh mo lj lk ll mp ln lo lp mq lr ls lt mr lv lw lx ms lz ma mb bi translated">过滤方法</h1><h2 id="c7c3" class="mu lf it bd lg mv mw dn lk mx my dp lo kr mz na ls kv nb nc lw kz nd ne ma nf bi translated">1.卡方检验</h2><p id="3f14" class="pw-post-body-paragraph kg kh it ki b kj mc kl km kn md kp kq kr me kt ku kv mf kx ky kz mg lb lc ld im bi translated">基于常用χ统计检验的单变量过滤器，用于测量与预期分布的偏差(如果假设要素的出现实际上与类值无关)。<br/>像任何单变量方法一样，我们计算每个特征和目标变量之间的卡方，并观察它们之间的关系。<br/>如果目标变量与特征无关，则得分较低，或者如果它们是相关的，则特征很重要。<br/>卡方值越高，表示要素与类别的相关性越高。</p><p id="0dad" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">要使用该方法，安装<em class="mt"> scikit-learn </em>。</p><blockquote class="ng nh ni"><p id="b28d" class="kg kh mt ki b kj kk kl km kn ko kp kq nj ks kt ku nk kw kx ky nl la lb lc ld im bi translated">对于所有代码示例，我们假设X是特性的熊猫数据框架，y是目标的熊猫系列。</p></blockquote><pre class="nm nn no np gt nq nr ns nt aw nu bi"><span id="6327" class="mu lf it nr b gy nv nw l nx ny">!pip install scikit-learn<br/>from sklearn.feature_selection import chi2<br/>X_norm = MinMaxScaler().fit_transform(X)<br/>chi_selector = SelectKBest(chi2, k='all')<br/>chi_selector.fit(X_norm, y)</span></pre><h2 id="70ee" class="mu lf it bd lg mv mw dn lk mx my dp lo kr mz na ls kv nb nc lw kz nd ne ma nf bi translated">2.交互信息</h2><p id="71a0" class="pw-post-body-paragraph kg kh it ki b kj mc kl km kn md kp kq kr me kt ku kv mf kx ky kz mg lb lc ld im bi translated">这个方法就是一个滤波器，也叫<em class="mt">信息增益</em>。<br/>它是最流行和最常用的单变量特征选择方法之一，因为它的计算速度和<em class="mt">简单的公式</em>。<br/>它测量每次考虑单个特征时熵的减少。<br/>如果一个特征具有高信息增益，则该特征被认为是相关的。<br/>它不能处理冗余特征，因为特征是以单变量方式选择的。<br/>因此，它属于“近视”方法，在这种方法中，独立地评估特征而不考虑它们的上下文。</p><p id="8264" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">要使用该方法，安装<em class="mt"> scikit-learn </em>。</p><pre class="nm nn no np gt nq nr ns nt aw nu bi"><span id="c30d" class="mu lf it nr b gy nv nw l nx ny">!pip install scikit-learn<br/>from sklearn.feature_selection import mutual_info_classif<br/>mi_selector = SelectKBest(mutual_info_classif, k='all')<br/>mi_selector.fit(X, y)</span></pre><h2 id="55e3" class="mu lf it bd lg mv mw dn lk mx my dp lo kr mz na ls kv nb nc lw kz nd ne ma nf bi translated">3.方差分析F值</h2><p id="903f" class="pw-post-body-paragraph kg kh it ki b kj mc kl km kn md kp kq kr me kt ku kv mf kx ky kz mg lb lc ld im bi translated">这是一种单变量过滤方法，使用<em class="mt">方差</em>来找出类之间的个体特征的可分性。<br/>适用于多类端点。</p><p id="f67c" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">要使用该方法，安装<em class="mt"> scikit-learn </em>。</p><pre class="nm nn no np gt nq nr ns nt aw nu bi"><span id="cf23" class="mu lf it nr b gy nv nw l nx ny">!pip install scikit-learn<br/>from sklearn.feature_selection import f_classif<br/>anov_selector = SelectKBest(f_classif, k='all')<br/>anov_selector.fit(X, y)</span></pre><h2 id="fcfe" class="mu lf it bd lg mv mw dn lk mx my dp lo kr mz na ls kv nb nc lw kz nd ne ma nf bi translated">4.方差阈值</h2><p id="1946" class="pw-post-body-paragraph kg kh it ki b kj mc kl km kn md kp kq kr me kt ku kv mf kx ky kz mg lb lc ld im bi translated">这种过滤方法并不总是被认为是一种要素选择方法，因为它的标准并不是在每个数据集中都能满足。<br/>移除<em class="mt">变化低于某个截止值</em>的特征。这个想法是，当一个特征在样本间变化不大时，它通常没有什么预测能力。</p><p id="d76b" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">要使用该方法，安装<em class="mt"> scikit-learn </em>。</p><pre class="nm nn no np gt nq nr ns nt aw nu bi"><span id="185b" class="mu lf it nr b gy nv nw l nx ny">!pip install scikit-learn<br/>from sklearn.feature_selection import VarianceThreshold<br/>var_selector = VarianceThreshold(<em class="mt">threshold=</em>0)<br/>var_selector.fit_transform(X)</span></pre><h2 id="7b05" class="mu lf it bd lg mv mw dn lk mx my dp lo kr mz na ls kv nb nc lw kz nd ne ma nf bi translated">5.费希尔评分</h2><p id="3eb7" class="pw-post-body-paragraph kg kh it ki b kj mc kl km kn md kp kq kr me kt ku kv mf kx ky kz mg lb lc ld im bi translated">这是一种使用均值和方差对要素进行分级的过滤方法。相同类的实例中具有相似值而不同类的实例中具有不同值的特征被认为是最佳的。与之前的单变量方法一样，它单独评估特征，并且它不能处理特征冗余。</p><p id="dde5" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">要使用该方法，请安装<em class="mt"> skfeature-chappers。</em></p><pre class="nm nn no np gt nq nr ns nt aw nu bi"><span id="29ac" class="mu lf it nr b gy nv nw l nx ny">!pip install skfeature-chappers<br/>from skfeature.function.similarity_based import fisher_score<br/>score = fisher_score.fisher_score(X.to_numpy(), y.to_numpy())</span></pre><h2 id="96d2" class="mu lf it bd lg mv mw dn lk mx my dp lo kr mz na ls kv nb nc lw kz nd ne ma nf bi translated">6.多重表面</h2><p id="9592" class="pw-post-body-paragraph kg kh it ki b kj mc kl km kn md kp kq kr me kt ku kv mf kx ky kz mg lb lc ld im bi translated">它是一种扩展的ReliefF及其多类扩展ReliefF滤波器的特征选择方法。<br/>原始relief的工作方式是从数据集中随机抽取一个<br/>实例，然后从<br/>相同和相反的类中定位其最近的邻居。<br/>将最近邻属性的值与采样实例进行比较，以更新每个属性的相关性分数。<br/>基本原理是一个有用的属性应该区分来自不同类的实例，并且对于来自相同类的实例应该有相同的值。<br/>在所有relief扩展中，MultiSURF在各种问题类型中提供了最可靠的特征选择性能。</p><p id="b92a" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">要使用该方法，请安装<em class="mt"> skrebate </em>。</p><pre class="nm nn no np gt nq nr ns nt aw nu bi"><span id="7035" class="mu lf it nr b gy nv nw l nx ny">!pip install skrebate<br/>from skrebate import MultiSURF<br/>fs = MultiSURF(n_jobs=-1, n_features_to_select=featureCutoff)<br/>fs.fit(X.values, y.values)</span></pre></div><div class="ab cl mh mi hx mj" role="separator"><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm"/></div><div class="im in io ip iq"><h1 id="0796" class="le lf it bd lg lh mo lj lk ll mp ln lo lp mq lr ls lt mr lv lw lx ms lz ma mb bi translated">包装方法</h1><h2 id="8881" class="mu lf it bd lg mv mw dn lk mx my dp lo kr mz na ls kv nb nc lw kz nd ne ma nf bi translated">1.递归特征消除</h2><p id="d540" class="pw-post-body-paragraph kg kh it ki b kj mc kl km kn md kp kq kr me kt ku kv mf kx ky kz mg lb lc ld im bi translated">这种广泛使用的包装方法使用一种算法来迭代地训练模型，并且每次都使用算法的权重作为标准来移除最不重要的特征。<br/>这是一种多变量方法，它评估共同考虑的几个特征的相关性。<br/>当用作排序器时，在每次迭代中，被移除的特性被添加到堆栈中，直到所有特性都被测试。<br/>为了提高计算效率，可以在单个步骤中移除多个特征。<br/>这种方法计算速度慢。</p><p id="e135" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">要使用该方法，安装<em class="mt"> scikit-learn </em>。</p><pre class="nm nn no np gt nq nr ns nt aw nu bi"><span id="4a8a" class="mu lf it nr b gy nv nw l nx ny">!pip install scikit-learn<br/>from sklearn.feature_selection import RFE<br/>rfe_selector = RFE(estimator=LogisticRegression(), <br/>                   n_features_to_select=1, step=1, verbose=-1)<br/>rfe_selector.fit(X_norm, y)</span></pre><h2 id="42f5" class="mu lf it bd lg mv mw dn lk mx my dp lo kr mz na ls kv nb nc lw kz nd ne ma nf bi translated">2.排列重要性</h2><p id="77d2" class="pw-post-body-paragraph kg kh it ki b kj mc kl km kn md kp kq kr me kt ku kv mf kx ky kz mg lb lc ld im bi translated">排列重要性是一种用于规范化特征重要性度量的启发式方法，可以纠正特征重要性偏差。<br/>该方法基于结果向量的重复排列，用于<br/>估算非信息环境中每个变量的测量重要性分布。<br/>观察到的重要性的p值提供了特征重要性的校正度量。</p><p id="03bb" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">要使用该方法，安装<em class="mt"> scikit-learn </em>和<em class="mt"> eli5 </em>。使用选择的算法。这里我使用了逻辑回归。</p><pre class="nm nn no np gt nq nr ns nt aw nu bi"><span id="50d7" class="mu lf it nr b gy nv nw l nx ny">!pip install eli5<br/>!pip install scikit-learn<br/>from eli5.sklearn import PermutationImportance<br/>from sklearn.feature_selection import SelectFromModel<br/>from sklearn.linear_model import LogisticRegression<br/>perm = PermutationImportance(LogisticRegression(), random_state=42, cv=10)<br/>perm.fit(X, y)<br/>perm_selector = SelectFromModel(perm,max_features=featureCutoff).fit(X, y)</span></pre><h2 id="1c85" class="mu lf it bd lg mv mw dn lk mx my dp lo kr mz na ls kv nb nc lw kz nd ne ma nf bi translated">3.SHAP</h2><p id="34fd" class="pw-post-body-paragraph kg kh it ki b kj mc kl km kn md kp kq kr me kt ku kv mf kx ky kz mg lb lc ld im bi translated">SHapley附加解释是一种解释任何机器学习模型输出的统一方法。<br/>它将博弈论与局部解释联系起来，联合了以前的几种方法，代表了唯一可能一致且局部精确的基于期望的加性特征归因方法。<br/>近年来已经成为事实上的特征选择方法。</p><p id="2ed1" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">要使用该方法，安装<em class="mt">shap</em><strong class="ki iu"><em class="mt"/></strong>并使用选择的算法。这里我用了xgboost。</p><pre class="nm nn no np gt nq nr ns nt aw nu bi"><span id="a276" class="mu lf it nr b gy nv nw l nx ny">!pip install shap<br/>import shap<br/>import xgboost<br/>model = xgboost.train({"learning_rate": 0.01}, xgboost.DMatrix(X, label=y), 100)<br/>explainer = shap.TreeExplainer(model)<br/>shap_values = explainer.shap_values(X)</span></pre><h2 id="9aaa" class="mu lf it bd lg mv mw dn lk mx my dp lo kr mz na ls kv nb nc lw kz nd ne ma nf bi translated">4.博鲁塔</h2><p id="7633" class="pw-post-body-paragraph kg kh it ki b kj mc kl km kn md kp kq kr me kt ku kv mf kx ky kz mg lb lc ld im bi translated">设计为随机森林包装器的算法。<br/>它迭代地移除被统计测试证明为不如随机探针相关的特征</p><p id="be11" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">要使用该方法，请安装<em class="mt">Boruta</em><strong class="ki iu"><em class="mt"/></strong>并使用算法。这里我使用随机森林。</p><pre class="nm nn no np gt nq nr ns nt aw nu bi"><span id="2fcd" class="mu lf it nr b gy nv nw l nx ny">!pip install Boruta<br/>from boruta import BorutaPy<br/>rf = RandomForestClassifier(n_jobs=-1, class_weight='balanced', max_depth=5)<br/>boru_selector = BorutaPy(rf, n_estimators='auto', verbose=0, random_state=1)<br/>boru_selector.fit(X.values, y.values)</span></pre></div><div class="ab cl mh mi hx mj" role="separator"><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm"/></div><div class="im in io ip iq"><h1 id="ae4a" class="le lf it bd lg lh mo lj lk ll mp ln lo lp mq lr ls lt mr lv lw lx ms lz ma mb bi translated">嵌入式方法</h1><h2 id="8568" class="mu lf it bd lg mv mw dn lk mx my dp lo kr mz na ls kv nb nc lw kz nd ne ma nf bi translated">1.嵌入式随机森林</h2><p id="6749" class="pw-post-body-paragraph kg kh it ki b kj mc kl km kn md kp kq kr me kt ku kv mf kx ky kz mg lb lc ld im bi translated">这种嵌入式特征选择使用随机森林算法。<br/>通过随机置换出袋样本中的特征，并计算与所有变量保持不变的出袋率相比，误分类率的增加百分比，来测量特征的重要性。</p><p id="f0a9" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">要使用该方法，安装<em class="mt"> scikit-learn </em>。</p><pre class="nm nn no np gt nq nr ns nt aw nu bi"><span id="8eed" class="mu lf it nr b gy nv nw l nx ny">!pip install scikit-learn<br/>from sklearn.feature_selection SelectFromModel<br/>from sklearn.ensemble import RandomForestClassifier<br/>embeded_rf_selector = SelectFromModel(RandomForestClassifier(n_estimators=100, random_state=42), max_features=featureCutoff)<br/>embeded_rf_selector.fit(X, y)</span></pre><h2 id="1605" class="mu lf it bd lg mv mw dn lk mx my dp lo kr mz na ls kv nb nc lw kz nd ne ma nf bi translated">2.嵌入式LightGBM</h2><p id="5acf" class="pw-post-body-paragraph kg kh it ki b kj mc kl km kn md kp kq kr me kt ku kv mf kx ky kz mg lb lc ld im bi translated">这种嵌入式特征选择使用流行的LGB算法。</p><p id="3569" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">要使用该方法，安装<em class="mt"> scikit-learn </em>和<em class="mt"> lightgbm </em>。</p><pre class="nm nn no np gt nq nr ns nt aw nu bi"><span id="1d8c" class="mu lf it nr b gy nv nw l nx ny">!pip install lightgbm<br/>from lightgbm import LGBMClassifier<br/>from sklearn.feature_selection SelectFromModel<br/>lgbc=LGBMClassifier(n_estimators=500, learning_rate=0.05,<br/>                    num_leaves=32, colsample_bytree=0.2,                                           <br/>                    reg_alpha=3, reg_lambda=1, min_split_gain=0.01,    <br/>                    min_child_weight=40)<br/>embeded_lgb_selector = SelectFromModel(lgbc, max_features=featureCutoff)<br/>embeded_lgb_selector.fit(X, y)</span></pre></div><div class="ab cl mh mi hx mj" role="separator"><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm"/></div><div class="im in io ip iq"><h1 id="4b4a" class="le lf it bd lg lh mo lj lk ll mp ln lo lp mq lr ls lt mr lv lw lx ms lz ma mb bi translated">不确定接下来要读什么？这里有两个选择:</h1><div class="nz oa gp gr ob oc"><a rel="noopener follow" target="_blank" href="/time-series-analysis-with-theory-plots-and-code-part-1-dd3ea417d8c4"><div class="od ab fo"><div class="oe ab of cl cj og"><h2 class="bd iu gy z fp oh fr fs oi fu fw is bi translated">时间序列分析与理论、图表和代码第1部分</h2><div class="oj l"><h3 class="bd b gy z fp oh fr fs oi fu fw dk translated">什么是时间序列，预测任务的步骤是什么？如何绘制趋势和季节性？如何…</h3></div><div class="ok l"><p class="bd b dl z fp oh fr fs oi fu fw dk translated">towardsdatascience.com</p></div></div><div class="ol l"><div class="om l on oo op ol oq jz oc"/></div></div></a></div><div class="nz oa gp gr ob oc"><a rel="noopener follow" target="_blank" href="/outlier-detection-theory-visualizations-and-code-a4fd39de540c"><div class="od ab fo"><div class="oe ab of cl cj og"><h2 class="bd iu gy z fp oh fr fs oi fu fw is bi translated">异常值检测——理论、可视化和代码</h2><div class="oj l"><h3 class="bd b gy z fp oh fr fs oi fu fw dk translated">五种算法来统治他们，五种算法来发现他们，五种算法来把他们带到黑暗中…</h3></div><div class="ok l"><p class="bd b dl z fp oh fr fs oi fu fw dk translated">towardsdatascience.com</p></div></div><div class="ol l"><div class="or l on oo op ol oq jz oc"/></div></div></a></div><h1 id="745b" class="le lf it bd lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated">保持联络</h1><p id="6d18" class="pw-post-body-paragraph kg kh it ki b kj mc kl km kn md kp kq kr me kt ku kv mf kx ky kz mg lb lc ld im bi translated">跟随我在<a class="ae kf" href="https://medium.com/@dimitris.effrosynidis" rel="noopener"> Medium </a>上获取更多类似的内容。<br/>我们在<a class="ae kf" href="https://www.linkedin.com/in/dimitrios-effrosynidis/" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>上连线吧。<br/>检查我的<a class="ae kf" href="https://github.com/Deffro" rel="noopener ugc nofollow" target="_blank"> GitHub </a>。</p></div></div>    
</body>
</html>