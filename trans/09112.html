<html>
<head>
<title>NeRF and What Happens When Graphics Becomes Differentiable</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">当图形变得可微时会发生什么</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/nerf-and-what-happens-when-graphics-becomes-differentiable-88a617561b5d?source=collection_archive---------10-----------------------#2021-08-23">https://towardsdatascience.com/nerf-and-what-happens-when-graphics-becomes-differentiable-88a617561b5d?source=collection_archive---------10-----------------------#2021-08-23</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="94e8" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsdatascience.com/tagged/thoughts-and-theory" rel="noopener" target="_blank">思想和理论</a></h2><div class=""/><div class=""><h2 id="218b" class="pw-subtitle-paragraph jw iz iq bd b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn dk translated">深度学习和计算机图形之间的浪漫是如何开始的，以及走向照片现实主义的未来之路</h2></div><p id="0b86" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated"><em class="lk">渲染</em>，任何图形系统的关键部分，<em class="lk">T5】就是让一个电脑化的三维世界在我们的二维电脑屏幕上反映出来的东西，就好像世界上的一个角色从口袋里拿出相机，拍下了他们所看到的东西。</em></p><p id="e96c" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">在过去的一年(2020年)，我们已经学会了如何使渲染过程变得可区分，并将其变成一个深度学习模块。这激发了想象力，因为深度学习的格言是:“如果它是可微分的，我们可以通过它学习”。事实上，许多深度学习突破都来自于试图软化零和一之间的差距，将离散变成连续，将<em class="lk"> argmax </em>变成<em class="lk"> softmax </em>。如果我们知道如何有区别地从3D到2D，这意味着我们也可以使用深度学习和反向传播从2D回到3D。</p></div><div class="ab cl ll lm hu ln" role="separator"><span class="lo bw bk lp lq lr"/><span class="lo bw bk lp lq lr"/><span class="lo bw bk lp lq"/></div><div class="ij ik il im in"><p id="437c" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">假设我手里拿着一张(2D的)猫坐在窗户里的照片(在现实世界中拍摄的)，并且可以访问一个<em class="lk">可微分渲染器</em>，一个将三维(计算机化)世界的<strong class="kq ja">表示</strong>转换为二维图像的系统。现在，如果我让系统渲染一个2D图像，我会得到一些看起来一点也不像猫的随机图像，因为它渲染的3d世界只是一些随机初始化的计算机化环境。</p><p id="0cf6" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">但是因为渲染器是可微分的，这意味着结果图像中的每个像素都是计算机化3d世界的可微分函数。这实质上意味着我们的深度学习框架保持了从渲染图像回到3D世界表示的链接。因此，我们可以问:“我们如何改变我们当前的3D世界表示，以便下次我们从相同的角度渲染它时，得到的图像看起来更像照片？”(数学上，我们取照片像素和图像像素之间的差的梯度)。</p><p id="94b6" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">如果我们相应地改变我们的3D世界表示，并一遍又一遍地这样做，最终我们的可区分渲染器将渲染一个看起来非常类似于原始猫照片的图像。</p></div><div class="ab cl ll lm hu ln" role="separator"><span class="lo bw bk lp lq lr"/><span class="lo bw bk lp lq lr"/><span class="lo bw bk lp lq"/></div><div class="ij ik il im in"><p id="382f" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">在那一点上，我们可以把注意力转向它所融合的计算机化的3D世界。我们能从中得到什么？它在3D窗口中有3D猫对象吗？如果我们从不同的拍摄角度(获得不同的图像)渲染同一个3D计算机化世界，它看起来还会像一只猫吗？好吧，除非我们的3D表现已经有了一些关于3D猫长什么样的预定义概念，否则很可能没有。仅仅使用一张照片，从一个角度来看，没有足够的信息来了解猫从不同的角度看起来会是什么样子。据我们所知，我们的3D世界可能会汇聚成一只印在画布上的扁平猫，而画布就放在摄像机前面。</p><figure class="lt lu lv lw gt lx gh gi paragraph-image"><div role="button" tabindex="0" class="ly lz di ma bf mb"><div class="gh gi ls"><img src="../Images/d1b90a238868d632c6d6defd5a86e486.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Kcx52nvlwTPeymy1CO3f9w.png"/></div></div><p class="me mf gj gh gi mg mh bd b be z dk translated"><strong class="bd mi">左边</strong>和<strong class="bd mi">中间</strong>:对世界的两种可能解释，右边<strong class="bd mi">的照片是在那里拍摄的</strong>。资料来源:联合国人类住区规划署</p></figure><p id="2f08" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">为了让我们对3d世界的描述汇聚成有用的东西，我们需要更多的照片，从更多的角度拍摄。</p><figure class="lt lu lv lw gt lx gh gi paragraph-image"><div role="button" tabindex="0" class="ly lz di ma bf mb"><div class="gh gi mj"><img src="../Images/0b6e5b5af9694d8e39d1108a2f739978.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*sFVWNlMP0zauDHt_"/></div></div><p class="me mf gj gh gi mg mh bd b be z dk translated">NeRF:每个场景都是从几十个不同的角度拍摄的，以找到它的三维表示。资料来源:NeRF论文[1]。</p></figure><p id="9c8d" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">这正是加州大学伯克利分校(Ren Ng的实验室)的一组研究人员在一个名为NeRF(神经辐射场的缩写)的项目中所做的。他们一次从不少于40个不同的角度拍摄了现实世界中的一个静态场景(并获得了40幅图像)，目标是获得该场景的三维计算机化表示，稍后可以从任何角度正确渲染。他们的结果令人震惊。</p><figure class="lt lu lv lw gt lx gh gi paragraph-image"><div class="gh gi mk"><img src="../Images/93753f92ad2b285d03aa12ec37ed177a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/1*Wg-kFTJ8FEQGqO5uIudEJg.gif"/></div><p class="me mf gj gh gi mg mh bd b be z dk translated">由NeRF处理的场景。所有帧都是通过从不同角度渲染相同的3D表示而完全合成的。注意地板和玻璃上的反射。来源:<a class="ae ml" href="https://www.matthewtancik.com/nerf" rel="noopener ugc nofollow" target="_blank">NeRF github页面</a>(点击查看附加示例)。</p></figure><p id="d10f" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">使用同一物体或场景的多个视图来获得其3D表示的想法并不新鲜(这被称为<em class="lk">视图合成</em>)。然而，以前的方法都无法生成逼真度如此之高的3D结构，以至于从<strong class="kq ja">新</strong>视点渲染的图像与真实照片无法区分。</p><p id="6725" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">建造NeRF的人能够搞定它。他们成功的原因是他们结合了强大的可区分渲染器和强大但不典型的3D世界表示。这个强大的3D世界表现是一个神经网络。甚至不是那些花哨的神经网络之一，而是一个多层感知器，就像过去的好时光一样，9个完全连接的层。</p></div><div class="ab cl ll lm hu ln" role="separator"><span class="lo bw bk lp lq lr"/><span class="lo bw bk lp lq lr"/><span class="lo bw bk lp lq"/></div><div class="ij ik il im in"><p id="e7ec" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">NeRF中心的神经网络是如何表现整个3D世界的？答案:它对它的3D空间进行分类！它的输入是空间中一个点的任意3个坐标(x，y，z)(以及相机视点)，它的输出是空间中该点的颜色<em class="lk"> (r，g，b) </em>和材质的不透明度(α)。</p><figure class="lt lu lv lw gt lx gh gi paragraph-image"><div role="button" tabindex="0" class="ly lz di ma bf mb"><div class="gh gi mm"><img src="../Images/9688186497ef2ceae22526d21fbaa805.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ituuFnOqVb9K8k92w3FczA.png"/></div></div><p class="me mf gj gh gi mg mh bd b be z dk translated">NeRF中心的神经网络。</p></figure><p id="9246" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">这就是了。五个号码进，四个号码出。一旦神经网络在场景中得到充分训练，我们就可以在空间的任何一点上查询它，并探测环境。这很好，但是如果我们想知道更多关于场景的实际内容，我们必须努力探测网络。这个神经网络没有明确地告诉我们场景中对象的数量、它们的类别或它们的边界。这是一种更“原子化”、更低级的看待世界的方式。</p><p id="d396" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">从计算机图形研究者的角度来看，这是激进的。它们用于在场景中以自己的数据结构(称为“网格”)来表示每个对象。它们用于处理表面、获取深度图和跟踪光源发出的光线。这里什么都没有。</p><p id="21a8" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">相反，我们有一些更类似于医学图像中用于表示体积的体素网格，如MRI或CT扫描，除了:</p><ul class=""><li id="9a84" class="mn mo iq kq b kr ks ku kv kx mp lb mq lf mr lj ms mt mu mv bi translated">MRI/CT体积具有固定的分辨率，使用离散的体素，而我们可以在任何分辨率下向NeRF网络询问任何分数(x，y，z)点。</li><li id="aa9b" class="mn mo iq kq b kr mw ku mx kx my lb mz lf na lj ms mt mu mv bi translated">与NeRF多层感知器的相对紧凑的256 x 256 x 9权重(粗略地)相比，MRI/CT体积占据了大量的空间(考虑512 x 512 x 512体素)。</li></ul><p id="4f3f" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">因此，NeRF的神经网络不是表现不同对象的最自然的方式，但它绝对是渲染它们的合适方式，也许这就是为什么结果图像如此逼真。</p></div><div class="ab cl ll lm hu ln" role="separator"><span class="lo bw bk lp lq lr"/><span class="lo bw bk lp lq lr"/><span class="lo bw bk lp lq"/></div><div class="ij ik il im in"><p id="33de" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated"><strong class="kq ja">NeRF渲染器是如何工作的？</strong></p><p id="7c7f" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">你可以想象我们正在渲染的图像就像一个屏幕放在相机前面，垂直于相机的视角。为了确定该图像中任何像素的颜色，我们从相机发出一条光线，穿过像素在屏幕上的位置并向前。如果光线击中任何表面，像素的颜色将被确定为表面在撞击点的颜色。如果表面是不透明的(𝛂=1)，我们就到此为止。但如果没有，我们必须继续跟踪光线，看看它碰到了什么其他表面，因为它们的颜色也会影响像素的颜色。</p><figure class="lt lu lv lw gt lx gh gi paragraph-image"><div role="button" tabindex="0" class="ly lz di ma bf mb"><div class="gh gi nb"><img src="../Images/ec69f6079ff11b99aab6a4c98980be76.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*HtvHtNcxRvwsoBb2"/></div></div><p class="me mf gj gh gi mg mh bd b be z dk translated"><strong class="bd mi">单像素渲染</strong>。图像被渲染为好像放置在垂直于摄像机角度的屏幕上。光线通过像素离开相机并继续。沿着射线采样几个点。像素应该在光线和苹果表面的第一个交叉点得到颜色。来源:David Novotny，pytorch3d教程[2]。</p></figure><p id="2037" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">在该渲染器的不可微分版本中，只有最接近的不透明表面决定了像素的颜色。但是如果我们想使它可微分，我们需要再次将<em class="lk"> argmax </em>变成<em class="lk"> softmax </em>。换句话说，所有被光线击中的表面都需要在计算中发挥一些作用，即使是那些被最近的表面遮挡的表面(仍然会产生最大的影响)。这是维持3D世界所有区域和最终图像像素之间必要联系的关键。</p><p id="8203" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">更具体地说，为了计算特定像素的颜色，我们沿着上述光线采样100个点，并向我们的神经网络请求颜色<em class="lk"> (r，g，b) </em>和每个点的不透明度(𝛂)。像素的最终颜色是所有<strong class="kq ja">所有</strong>采样点<strong class="kq ja">所有</strong>颜色的线性组合(加权平均)，甚至是那些在半空中的点(𝛂应该接近0)，或者那些在相机“隐藏”的表面上的点(𝛂应该接近1)。分配给光线上第<em class="lk">个</em>点(远离相机)的权重将得到权重T_i 𝛂_i，其中标量𝛂_i</p><p id="c5ac" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">由网络直接计算，并且</p><figure class="lt lu lv lw gt lx gh gi paragraph-image"><div class="gh gi nc"><img src="../Images/106989ed4bb91e563977c17fd7455347.png" data-original-src="https://miro.medium.com/v2/resize:fit:256/0*4WYdbyx_0vLMH_4Q"/></div></figure><p id="b8c0" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">取决于前面点的不透明度值。您可以看到最近的曲面如何获得最主要的权重(我们将权重归一化，因此它们的总和为1)。</p></div><div class="ab cl ll lm hu ln" role="separator"><span class="lo bw bk lp lq lr"/><span class="lo bw bk lp lq lr"/><span class="lo bw bk lp lq"/></div><div class="ij ik il im in"><p id="8f1b" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated"><strong class="kq ja">前进的道路</strong></p><p id="e674" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">我觉得这很令人兴奋。这不再是一个预测物理的神经网络。这个<strong class="kq ja">是插在PyTorch引擎内神经网络顶部的</strong>物理(或光学)。我们现在有了一个现实世界的<em class="lk">可微分模拟</em>(利用计算机图形的力量)和一个它的<em class="lk">神经表示</em>(利用深度学习的力量)。难怪结果看起来如此逼真。</p><p id="a84f" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">深度学习和图形之间的这场<strong class="kq ja"> </strong>罗曼史才刚刚开始。它将缩小计算机模拟和现实世界之间的差距。如果你有一个VR头戴设备，想象一下看电影，你不只是坐在一个静态的视角，而是可以在场景中移动，甚至可以与之互动。</p><p id="6a74" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">最新的创新将允许我们将现实世界中的物体和环境导入虚拟世界，减少对艺术家制作的资产的依赖。【在这种情况下，我推荐观看<a class="ae ml" href="https://slideslive.com/38939970/a-i-for-3d-content-creation" rel="noopener ugc nofollow" target="_blank">Nvidia的Sanja Fidler关于创建三维内容的讲座</a>，该讲座来自上一期NeurIPS关于该主题的研讨会。]</p><p id="2121" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">还有我之前提到的关于NeRF网络局限性的那些东西？这对于计算机图形学来说太低级了？这将会改变。我们将从这个3D世界表示中提取语义元素，这将使我们能够理解和控制它，就像我们对经典网格表示所做的那样，但保留了照片真实感的新标准。</p><p id="bb8f" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">在下一篇文章中，我将解释我们计划如何去做。</p><figure class="lt lu lv lw gt lx"><div class="bz fp l di"><div class="nd ne l"/></div></figure></div><div class="ab cl ll lm hu ln" role="separator"><span class="lo bw bk lp lq lr"/><span class="lo bw bk lp lq lr"/><span class="lo bw bk lp lq"/></div><div class="ij ik il im in"><p id="e00a" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated"><strong class="kq ja">延伸阅读</strong></p><p id="81bf" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">[1]米尔登霍尔等人。艾尔。<strong class="kq ja"> NeRF:将场景表示为用于视图合成的神经辐射场。</strong> ECCV 2020 [ <a class="ae ml" href="https://www.matthewtancik.com/nerf" rel="noopener ugc nofollow" target="_blank">链接</a></p><p id="a3b9" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">[2]脸书的PyTorch3D图书馆。[ <a class="ae ml" href="https://github.com/facebookresearch/pytorch3d" rel="noopener ugc nofollow" target="_blank">链接</a> ] [ <a class="ae ml" href="https://www.youtube.com/watch?v=g50RiDnfIfY" rel="noopener ugc nofollow" target="_blank">隐函数与NeRF</a>][<a class="ae ml" href="http://dedicated NeRF notebook." rel="noopener ugc nofollow" target="_blank">NeRF专用jupyter笔记本</a> ]</p><p id="64af" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">[3]桑娅·菲德勒。<strong class="kq ja">用于3D内容创作的人工智能</strong>。NeurIPS 2020 [ <a class="ae ml" href="https://slideslive.com/38939970/a-i-for-3d-content-creation" rel="noopener ugc nofollow" target="_blank">链接</a></p><p id="8259" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">[4]弗拉德伦·科尔顿:走向摄影现实主义[ <a class="ae ml" href="https://www.youtube.com/watch?v=Rd0nBO6--bM" rel="noopener ugc nofollow" target="_blank">视频</a></p></div></div>    
</body>
</html>