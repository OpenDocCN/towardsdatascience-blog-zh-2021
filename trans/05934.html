<html>
<head>
<title>Residual Networks from Scratch Applied to Computer Vision</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">从零开始的残差网络在计算机视觉中的应用</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/residual-networks-in-computer-vision-ee118d3be68f?source=collection_archive---------24-----------------------#2021-05-27">https://towardsdatascience.com/residual-networks-in-computer-vision-ee118d3be68f?source=collection_archive---------24-----------------------#2021-05-27</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="3037" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">使用Tensorflow和Keras的深度学习应用</h2></div><h1 id="3a0c" class="ki kj it bd kk kl km kn ko kp kq kr ks jz kt ka ku kc kv kd kw kf kx kg ky kz bi translated">1.介绍</h1><p id="25b9" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">深度卷积神经网络显著改变了图像分类的研究前景<a class="ae lw" href="https://papers.nips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf" rel="noopener ugc nofollow" target="_blank">【1】</a>。随着层次的增加，模型的表现力也增加了；它能够学习更复杂的表达。在某些时候，网络的深度和模型的准确性之间似乎存在正相关关系。当网络越来越深入时，消失/爆炸梯度问题变得越来越严重。规范化的初始化和中间规范化层最终解决了这个问题，深层网络开始融合。然而，随着深度的增加，模型的准确性开始饱和，然后实际上迅速下降，而不是在随后的实验之前的直觉推理。这不是由于过度拟合，而是由于当前用于优化模型的求解器的局限性<a class="ae lw" href="https://arxiv.org/pdf/1512.03385.pdf" rel="noopener ugc nofollow" target="_blank">【2】</a>。</p><p id="f06a" class="pw-post-body-paragraph la lb it lc b ld lx ju lf lg ly jx li lj lz ll lm ln ma lp lq lr mb lt lu lv im bi translated">退化问题通过引入剩余网络<a class="ae lw" href="https://arxiv.org/pdf/1512.03385.pdf" rel="noopener ugc nofollow" target="_blank">【2】</a>得到部分解决。它引入了一种系统的方法来使用<em class="mc">快捷连接</em>，即跳过一层或多层的连接。这些<em class="mc">短连接</em>只是执行身份映射，它们的输出被添加到堆叠层的输出中(这不会增加额外的参数或计算复杂度)。其背后的想法是，如果多个非线性层可以渐近地逼近复杂的函数(仍在理论上研究，但这是深度学习的基础)，那么剩余函数也可能发生同样的情况。这样做的好处是，同时简化了求解者的工作。在<a class="ae lw" href="https://arxiv.org/abs/1603.05027" rel="noopener ugc nofollow" target="_blank">【3】</a>中研究了其他类型的连接，如缩放、门控和1x1卷积的跳跃连接；然而，身份映射继续产生最低的训练损失。</p><p id="1542" class="pw-post-body-paragraph la lb it lc b ld lx ju lf lg ly jx li lj lz ll lm ln ma lp lq lr mb lt lu lv im bi translated">我们的任务是对一系列带标签的图像进行分类。我们想比较两种不同方法的准确性；第一个是经典卷积神经网络，第二个是残差网络。我们的目标是展示残余网络的力量，即使是在不太深的环境中。这是在解决退化问题的同时帮助优化过程的好方法。我们对残差网络进行了经验测试，它更容易过度拟合。为了解决这个问题，我们通过使用数据扩充策略来综合扩充我们的数据集。</p><p id="4231" class="pw-post-body-paragraph la lb it lc b ld lx ju lf lg ly jx li lj lz ll lm ln ma lp lq lr mb lt lu lv im bi translated">我们再次使用辛普森一家的角色数据集<a class="ae lw" href="https://www.kaggle.com/alexattia/the-simpsons-characters-dataset" rel="noopener ugc nofollow" target="_blank">【4】</a>。我们对数据集进行了过滤，仅包含包含100张以上图像的类(字符)。在训练、验证和测试数据集之间进行拆分后，数据集的最终大小如下:12411幅图像用于训练，3091幅图像用于验证，950幅图像用于测试。</p><p id="01c6" class="pw-post-body-paragraph la lb it lc b ld lx ju lf lg ly jx li lj lz ll lm ln ma lp lq lr mb lt lu lv im bi translated">和往常一样，代码和数据也可以在我的<a class="ae lw" href="https://github.com/luisroque/deep-learning-articles" rel="noopener ugc nofollow" target="_blank"> GitHub </a>上获得。</p><p id="ecaa" class="pw-post-body-paragraph la lb it lc b ld lx ju lf lg ly jx li lj lz ll lm ln ma lp lq lr mb lt lu lv im bi translated">本文属于使用TensorFlow进行深度学习的系列文章:</p><ul class=""><li id="bb48" class="md me it lc b ld lx lg ly lj mf ln mg lr mh lv mi mj mk ml bi translated"><a class="ae lw" rel="noopener" target="_blank" href="/transfer-learning-and-data-augmentation-applied-to-the-simpsons-image-dataset-e292716fbd43">迁移学习和数据增强应用于辛普森图像数据集</a></li><li id="171e" class="md me it lc b ld mm lg mn lj mo ln mp lr mq lv mi mj mk ml bi translated"><a class="ae lw" rel="noopener" target="_blank" href="/generating-text-with-recurrent-neural-networks-based-on-the-work-of-f-pessoa-1e804d88692d">基于F. Pessoa的工作用递归神经网络生成文本</a></li><li id="3a1a" class="md me it lc b ld mm lg mn lj mo ln mp lr mq lv mi mj mk ml bi translated"><a class="ae lw" rel="noopener" target="_blank" href="/neural-machine-translation-using-a-seq2seq-architecture-and-attention-eng-to-por-fe3cc4191175">使用Seq2Seq架构和注意力的神经机器翻译(ENG to POR) </a></li><li id="be73" class="md me it lc b ld mm lg mn lj mo ln mp lr mq lv mi mj mk ml bi translated"><a class="ae lw" rel="noopener" target="_blank" href="/residual-networks-in-computer-vision-ee118d3be68f">残差网络从无到有应用于计算机视觉</a></li></ul><h1 id="5ae7" class="ki kj it bd kk kl km kn ko kp kq kr ks jz kt ka ku kc kv kd kw kf kx kg ky kz bi translated">2.数据预处理</h1><p id="0b7f" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">我们创建生成器来将我们的数据提供给模型。我们还应用一个转换来规范化数据，在训练和验证数据集之间分割它们，并定义一个32的批量大小(参见<a class="ae lw" rel="noopener" target="_blank" href="/transfer-learning-and-data-augmentation-applied-to-the-simpsons-image-dataset-e292716fbd43">【5】</a>以更好地理解预处理和生成器)。</p><pre class="mr ms mt mu gt mv mw mx my aw mz bi"><span id="eca1" class="na kj it mw b gy nb nc l nd ne">import tensorflow as tf<br/>from tensorflow.keras.models import Model<br/>from tensorflow.keras.layers import Layer, BatchNormalization, Conv2D, Dense, Flatten, Add, Dropout, BatchNormalization<br/>import numpy as np<br/>from tensorflow.keras.datasets import fashion_mnist<br/>from tensorflow.keras.utils import to_categorical<br/>import matplotlib.pyplot as plt<br/>from sklearn.model_selection import train_test_split<br/>from tensorflow.keras.preprocessing.image import ImageDataGenerator<br/>import os<br/>from tensorflow.keras import Input, layers<br/>from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D<br/>import time</span><span id="1532" class="na kj it mw b gy nf nc l nd ne">directory_train = "./simpsons_data_split/train/"<br/>directory_test = "./simpsons_data_split/test/"</span><span id="b4d5" class="na kj it mw b gy nf nc l nd ne">def get_ImageDataGenerator(validation_split=None):<br/>    image_generator = ImageDataGenerator(rescale=(1/255.),<br/>                                         validation_split=validation_split)<br/>    return image_generator<br/><br/>image_gen_train = get_ImageDataGenerator(validation_split=0.2)<br/><br/>def get_generator(image_data_generator, directory, train_valid=None, seed=None):<br/>    train_generator = image_data_generator.flow_from_directory(directory, <br/>                                                               batch_size=32, <br/>                                                               class_mode='categorical', <br/>                                                               target_size=(128,128), <br/>                                                               subset=train_valid, <br/>                                                               seed=seed)    <br/>    return train_generator<br/><br/>train_generator = get_generator(image_gen_train, directory_train, train_valid='training', seed=1)<br/>validation_generator = get_generator(image_gen_train, directory_train, train_valid='validation')</span><span id="1de1" class="na kj it mw b gy nf nc l nd ne">Found 12411 images belonging to 19 classes.<br/>Found 3091 images belonging to 19 classes.</span></pre><p id="1e62" class="pw-post-body-paragraph la lb it lc b ld lx ju lf lg ly jx li lj lz ll lm ln ma lp lq lr mb lt lu lv im bi translated">我们还创建了一个膨胀的数据集，通过应用一组几何和光度变换来减少过度拟合的可能性。几何变换改变了图像的几何形状，使得CNN不会因位置和方向的改变而改变。光度变换通过调整图像的颜色通道，使CNN不受颜色和光照变化的影响。</p><pre class="mr ms mt mu gt mv mw mx my aw mz bi"><span id="4041" class="na kj it mw b gy nb nc l nd ne">def get_ImageDataGenerator_augmented(validation_split=None):<br/>    image_generator = ImageDataGenerator(rescale=(1/255.),<br/>                                        rotation_range=40,<br/>                                        width_shift_range=0.2,<br/>                                        height_shift_range=0.2,<br/>                                        shear_range=0.2,<br/>                                        zoom_range=0.1,<br/>                                        brightness_range=[0.8,1.2],<br/>                                        horizontal_flip=True,<br/>                                        validation_split=validation_split)<br/>    return image_generator<br/>image_gen_train_aug = get_ImageDataGenerator_augmented(validation_split=0.2)<br/>train_generator_aug = get_generator(image_gen_train_aug, directory_train, train_valid='training', seed=1)<br/>validation_generator_aug = get_generator(image_gen_train_aug, directory_train, train_valid='validation')</span><span id="20b1" class="na kj it mw b gy nf nc l nd ne">Found 12411 images belonging to 19 classes.<br/>Found 3091 images belonging to 19 classes.</span></pre><p id="66f1" class="pw-post-body-paragraph la lb it lc b ld lx ju lf lg ly jx li lj lz ll lm ln ma lp lq lr mb lt lu lv im bi translated">我们可以遍历生成器来获得一组图像，其大小等于上面定义的批处理大小。</p><pre class="mr ms mt mu gt mv mw mx my aw mz bi"><span id="751f" class="na kj it mw b gy nb nc l nd ne">target_labels = next(os.walk(directory_train))[1]<br/><br/>target_labels.sort()<br/><br/>batch = next(train_generator)<br/>batch_images = np.array(batch[0])<br/>batch_labels = np.array(batch[1])<br/><br/>target_labels = np.asarray(target_labels)<br/><br/>plt.figure(figsize=(15,10))<br/>for n, i in enumerate(np.arange(10)):<br/>    ax = plt.subplot(3,5,n+1)<br/>    plt.imshow(batch_images[i])<br/>    plt.title(target_labels[np.where(batch_labels[i]==1)[0][0]])<br/>    plt.axis('off')</span></pre><figure class="mr ms mt mu gt ng gh gi paragraph-image"><div class="ab gu cl nh"><img src="../Images/ef7382794188ffe4e7d0e86fed04a3de.png" data-original-src="https://miro.medium.com/v2/format:webp/1*QUlqj0010YP41Wen1hgyIA.png"/></div><p class="nk nl gj gh gi nm nn bd b be z dk translated">图1:由训练生成器生成的一组图像。</p></figure><h1 id="1ddc" class="ki kj it bd kk kl km kn ko kp kq kr ks jz kt ka ku kc kv kd kw kf kx kg ky kz bi translated">3.基准模型</h1><p id="fc41" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">我们定义一个简单的CNN作为基准模型。它使用2D卷积层(对图像执行空间卷积)和最大池操作。它们之后是具有128个单元和ReLU激活功能的密集层，以及速率为0.5的下降层。最后，最后一层产生我们的网络的输出，其单元数量等于目标标签的数量，并使用softmax激活函数。该模型是用Adam优化器编译的，具有默认设置和分类交叉熵损失。</p><pre class="mr ms mt mu gt mv mw mx my aw mz bi"><span id="94a0" class="na kj it mw b gy nb nc l nd ne">def get_benchmark_model(input_shape):<br/>    x = Input(shape=input_shape)<br/>    h = Conv2D(32, padding='same', kernel_size=(3,3), activation='relu')(x)<br/>    h = Conv2D(32, padding='same', kernel_size=(3,3), activation='relu')(x)<br/>    h = MaxPooling2D(pool_size=(2,2))(h)<br/>    h = Conv2D(64, padding='same', kernel_size=(3,3), activation='relu')(h)<br/>    h = Conv2D(64, padding='same', kernel_size=(3,3), activation='relu')(h)<br/>    h = MaxPooling2D(pool_size=(2,2))(h)<br/>    h = Conv2D(128, kernel_size=(3,3), activation='relu')(h)<br/>    h = Conv2D(128, kernel_size=(3,3), activation='relu')(h)<br/>    h = MaxPooling2D(pool_size=(2,2))(h)<br/>    h = Flatten()(h)<br/>    h = Dense(128, activation='relu')(h)<br/>    h = Dropout(.5)(h)<br/>    output = Dense(target_labels.shape[0], activation='softmax')(h)<br/><br/>    model = tf.keras.Model(inputs=x, outputs=output)<br/>    <br/>    model.compile(optimizer='adam',<br/>             loss='categorical_crossentropy',<br/>             metrics=['accuracy'])<br/>    return model</span><span id="ac22" class="na kj it mw b gy nf nc l nd ne">benchmark_model = get_benchmark_model((128, 128, 3))<br/>benchmark_model.summary()</span><span id="f730" class="na kj it mw b gy nf nc l nd ne">Model: "model"<br/>_________________________________________________________________<br/>Layer (type)                 Output Shape              Param #   <br/>=================================================================<br/>input_1 (InputLayer)         [(None, 128, 128, 3)]     0         <br/>_________________________________________________________________<br/>conv2d_1 (Conv2D)            (None, 128, 128, 32)      896       <br/>_________________________________________________________________<br/>max_pooling2d (MaxPooling2D) (None, 64, 64, 32)        0         <br/>_________________________________________________________________<br/>conv2d_2 (Conv2D)            (None, 64, 64, 64)        18496     <br/>_________________________________________________________________<br/>conv2d_3 (Conv2D)            (None, 64, 64, 64)        36928     <br/>_________________________________________________________________<br/>max_pooling2d_1 (MaxPooling2 (None, 32, 32, 64)        0         <br/>_________________________________________________________________<br/>conv2d_4 (Conv2D)            (None, 30, 30, 128)       73856     <br/>_________________________________________________________________<br/>conv2d_5 (Conv2D)            (None, 28, 28, 128)       147584    <br/>_________________________________________________________________<br/>max_pooling2d_2 (MaxPooling2 (None, 14, 14, 128)       0         <br/>_________________________________________________________________<br/>flatten (Flatten)            (None, 25088)             0         <br/>_________________________________________________________________<br/>dense (Dense)                (None, 128)               3211392   <br/>_________________________________________________________________<br/>dropout (Dropout)            (None, 128)               0         <br/>_________________________________________________________________<br/>dense_1 (Dense)              (None, 19)                2451      <br/>=================================================================<br/>Total params: 3,491,603<br/>Trainable params: 3,491,603<br/>Non-trainable params: 0<br/>_________________________________________________________________</span><span id="b736" class="na kj it mw b gy nf nc l nd ne">def train_model(model, train_gen, valid_gen, epochs):<br/>    train_steps_per_epoch = train_gen.n // train_gen.batch_size<br/>    val_steps = valid_gen.n // valid_gen.batch_size<br/>    <br/>    earlystopping = tf.keras.callbacks.EarlyStopping(patience=3)<br/>    history = model.fit(train_gen, <br/>                        steps_per_epoch = train_steps_per_epoch,<br/>                        epochs=epochs,<br/>                        validation_data=valid_gen, <br/>                        callbacks=[earlystopping])<br/>    <br/>    return history</span><span id="6750" class="na kj it mw b gy nf nc l nd ne">train_generator = get_generator(image_gen_train, directory_train, train_valid='training')<br/>validation_generator = get_generator(image_gen_train, directory_train, train_valid='validation')<br/>history_benchmark = train_model(benchmark_model, train_generator, validation_generator, 50)</span><span id="6b01" class="na kj it mw b gy nf nc l nd ne">Found 12411 images belonging to 19 classes.<br/>Found 3091 images belonging to 19 classes.<br/>Epoch 1/50<br/>387/387 [==============================] - 139s 357ms/step - loss: 2.7674 - accuracy: 0.1370 - val_loss: 2.1717 - val_accuracy: 0.3488<br/>Epoch 2/50<br/>387/387 [==============================] - 136s 352ms/step - loss: 2.0837 - accuracy: 0.3757 - val_loss: 1.7546 - val_accuracy: 0.4940<br/>Epoch 3/50<br/>387/387 [==============================] - 130s 335ms/step - loss: 1.5967 - accuracy: 0.5139 - val_loss: 1.3483 - val_accuracy: 0.6102<br/>Epoch 4/50<br/>387/387 [==============================] - 130s 335ms/step - loss: 1.1952 - accuracy: 0.6348 - val_loss: 1.1623 - val_accuracy: 0.6619<br/>Epoch 5/50<br/>387/387 [==============================] - 130s 337ms/step - loss: 0.9164 - accuracy: 0.7212 - val_loss: 1.0813 - val_accuracy: 0.6907<br/>Epoch 6/50<br/>387/387 [==============================] - 130s 336ms/step - loss: 0.7270 - accuracy: 0.7802 - val_loss: 1.0241 - val_accuracy: 0.7240<br/>Epoch 7/50<br/>387/387 [==============================] - 130s 336ms/step - loss: 0.5641 - accuracy: 0.8217 - val_loss: 0.9674 - val_accuracy: 0.7438<br/>Epoch 8/50<br/>387/387 [==============================] - 130s 336ms/step - loss: 0.4496 - accuracy: 0.8592 - val_loss: 1.0701 - val_accuracy: 0.7441<br/>Epoch 9/50<br/>387/387 [==============================] - 130s 336ms/step - loss: 0.3677 - accuracy: 0.8758 - val_loss: 0.9796 - val_accuracy: 0.7645<br/>Epoch 10/50<br/>387/387 [==============================] - 130s 336ms/step - loss: 0.3041 - accuracy: 0.8983 - val_loss: 1.0681 - val_accuracy: 0.7561</span></pre><h1 id="9222" class="ki kj it bd kk kl km kn ko kp kq kr ks jz kt ka ku kc kv kd kw kf kx kg ky kz bi translated">4.剩余网络</h1><p id="df43" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">深层残差网络由许多叠加的<em class="mc">残差单元</em>组成，可以定义为:</p><figure class="mr ms mt mu gt ng gh gi paragraph-image"><div class="gh gi no"><img src="../Images/f419f098dd12707b51e95d33e82a0d2e.png" data-original-src="https://miro.medium.com/v2/resize:fit:562/format:webp/1*0YR59pm2ZkLiCk5ZrChpsg.png"/></div></figure><p id="e6a4" class="pw-post-body-paragraph la lb it lc b ld lx ju lf lg ly jx li lj lz ll lm ln ma lp lq lr mb lt lu lv im bi translated">其中x_ <em class="mc"> l </em>和<em class="mc"> x_ </em> {l+1}为第l个单元的输入输出，<em class="mc"> F </em>为剩余函数，<em class="mc"> h(x_l) </em>为恒等式映射，<em class="mc"> f </em>为激活函数。<em class="mc"> W_t </em>是与第l个残差单元相关联的一组权重(和偏差)。<a class="ae lw" href="https://papers.nips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf" rel="noopener ugc nofollow" target="_blank">【2】</a>提出的层数为2或3。我们将<em class="mc"> F </em>定义为两个3×3卷积层的堆栈。在<a class="ae lw" href="https://papers.nips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf" rel="noopener ugc nofollow" target="_blank">【2】</a><em class="mc">f</em>中，一个ReLU函数在逐元素加法后被应用。我们遵循了后来提出的<a class="ae lw" href="https://arxiv.org/abs/1603.05027" rel="noopener ugc nofollow" target="_blank">【3】</a>架构，其中<em class="mc"> f </em>只是一个身份映射。在这种情况下，</p><figure class="mr ms mt mu gt ng gh gi paragraph-image"><div class="gh gi np"><img src="../Images/2e80d1df62a92d15e951f3f696d2317e.png" data-original-src="https://miro.medium.com/v2/resize:fit:198/format:webp/1*k7pSLY578QISVF9ZT_CnYA.png"/></div></figure><p id="7d92" class="pw-post-body-paragraph la lb it lc b ld lx ju lf lg ly jx li lj lz ll lm ln ma lp lq lr mb lt lu lv im bi translated">我们可以写作，</p><figure class="mr ms mt mu gt ng gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/d4bcf63f5f8e1d5edb45a8e6ef627c85.png" data-original-src="https://miro.medium.com/v2/resize:fit:524/format:webp/1*tlOoZImhzMJmARRDE6jioQ.png"/></div></figure><p id="5f99" class="pw-post-body-paragraph la lb it lc b ld lx ju lf lg ly jx li lj lz ll lm ln ma lp lq lr mb lt lu lv im bi translated">或者更一般地说，</p><figure class="mr ms mt mu gt ng gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/ffee7c776be148fd0151e8f4efde4e66.png" data-original-src="https://miro.medium.com/v2/resize:fit:636/format:webp/1*ELd9X8OREZqWH1NlePN0DA.png"/></div></figure><p id="047a" class="pw-post-body-paragraph la lb it lc b ld lx ju lf lg ly jx li lj lz ll lm ln ma lp lq lr mb lt lu lv im bi translated">对于任何较深的单元<em class="mc"> L </em>和较浅的单元<em class="mc"> l </em>。该功能</p><figure class="mr ms mt mu gt ng gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/0ce74ac3a0daed498712ce21a125a6e3.png" data-original-src="https://miro.medium.com/v2/resize:fit:582/format:webp/1*L64iKfyvlbr-jNEpzKAf3A.png"/></div></figure><p id="8216" class="pw-post-body-paragraph la lb it lc b ld lx ju lf lg ly jx li lj lz ll lm ln ma lp lq lr mb lt lu lv im bi translated">对于任何深度单元，<em class="mc"> L </em>是所有在前剩余函数的输出加上x_0的总和。</p><p id="ccee" class="pw-post-body-paragraph la lb it lc b ld lx ju lf lg ly jx li lj lz ll lm ln ma lp lq lr mb lt lu lv im bi translated">就优化过程而言，反向传播属性给出了关于为什么这种类型的连接有助于优化过程的一些直觉。我们可以把它写成:</p><figure class="mr ms mt mu gt ng gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/53e7edc1ee8d33ebbc4b6bdbe1c147b2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1172/format:webp/1*JDOPLhRt1fjuHIx8dPgKgQ.png"/></div></figure><p id="3acc" class="pw-post-body-paragraph la lb it lc b ld lx ju lf lg ly jx li lj lz ll lm ln ma lp lq lr mb lt lu lv im bi translated">其中<em class="mc"> L </em>是损失函数。注意这个梯度</p><figure class="mr ms mt mu gt ng gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/1738316aa98c63eceb989860767fae3e.png" data-original-src="https://miro.medium.com/v2/resize:fit:86/format:webp/1*SM40MoNynuWEq3QB8BQjZw.png"/></div></figure><p id="4c5b" class="pw-post-body-paragraph la lb it lc b ld lx ju lf lg ly jx li lj lz ll lm ln ma lp lq lr mb lt lu lv im bi translated">直接传播信息，而术语</p><figure class="mr ms mt mu gt ng gh gi paragraph-image"><div class="gh gi nv"><img src="../Images/4175bf0272725c4e699ee76bbb6df554.png" data-original-src="https://miro.medium.com/v2/resize:fit:554/format:webp/1*ORSAcv7oZTg9ZLKfP7eXqw.png"/></div></figure><p id="2443" class="pw-post-body-paragraph la lb it lc b ld lx ju lf lg ly jx li lj lz ll lm ln ma lp lq lr mb lt lu lv im bi translated">通过权重层传播。可以看出，利用这种形式，即使当权重任意小时，层的梯度也不会消失。</p><h2 id="5f9f" class="na kj it bd kk nw nx dn ko ny nz dp ks lj oa ob ku ln oc od kw lr oe of ky og bi translated">4.1剩余单位</h2><p id="6af5" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">我们使用层子类化来构建<em class="mc">剩余单元</em>。自定义图层类有三种方法:<code class="fe oh oi oj mw b">__init__</code>、<code class="fe oh oi oj mw b">build</code>和<code class="fe oh oi oj mw b">call</code>。<code class="fe oh oi oj mw b">__init__</code>方法用定义的关键字参数调用基本的<code class="fe oh oi oj mw b">Layer</code>类初始化器。<code class="fe oh oi oj mw b">build</code>方法创建层。在我们的例子中，我们定义了两组<code class="fe oh oi oj mw b">BatchNormalization</code>,后面是一个<code class="fe oh oi oj mw b">Conv2D</code>层，最后一个使用与层输入相同数量的过滤器。<code class="fe oh oi oj mw b">call</code>方法通过各层处理输入。在我们的例子中，我们有以下顺序:第一个<code class="fe oh oi oj mw b">BatchNormalization</code>，ReLu激活函数，第一个<code class="fe oh oi oj mw b">Conv2D</code>，第二个<code class="fe oh oi oj mw b">BatchNormalization</code>，另一个ReLu激活函数，第二个<code class="fe oh oi oj mw b">Conv2D</code>。最后，我们将输入添加到第二个<code class="fe oh oi oj mw b">Conv2D</code>层的输出，这可以看作是执行身份映射的<em class="mc">短连接</em>的实现。</p><pre class="mr ms mt mu gt mv mw mx my aw mz bi"><span id="9890" class="na kj it mw b gy nb nc l nd ne">class ResidualUnit(Layer):<br/>    def __init__(self, **kwargs):<br/>        super(ResidualUnit, self).__init__(**kwargs)<br/>        <br/>    def build(self, input_shape):<br/>        self.bn_1 = tf.keras.layers.BatchNormalization(input_shape=input_shape)<br/>        self.conv2d_1 = tf.keras.layers.Conv2D(input_shape[3], (3, 3), padding='same')<br/>        self.bn_2 = tf.keras.layers.BatchNormalization()<br/>        self.conv2d_2 = tf.keras.layers.Conv2D(input_shape[3], (3, 3), padding='same')<br/>        <br/>    def call(self, inputs, training=False):<br/>        x = self.bn_1(inputs, training)<br/>        x = tf.nn.relu(x)<br/>        x = self.conv2d_1(x)<br/>        x = self.bn_2(x, training)<br/>        x = tf.nn.relu(x)<br/>        x = self.conv2d_2(x)<br/>        x = tf.keras.layers.add([inputs, x])<br/>        return x</span><span id="6ff8" class="na kj it mw b gy nf nc l nd ne">test_model = tf.keras.Sequential([ResidualUnit(input_shape=(128, 128, 3), name="residual_unit")])<br/>test_model.summary()</span><span id="90e5" class="na kj it mw b gy nf nc l nd ne">Model: "sequential"<br/>_________________________________________________________________<br/>Layer (type)                 Output Shape              Param #   <br/>=================================================================<br/>residual_unit (ResidualUnit) (None, 128, 128, 3)       192       <br/>=================================================================<br/>Total params: 192<br/>Trainable params: 180<br/>Non-trainable params: 12<br/>_________________________________________________________________</span></pre><h2 id="f9f0" class="na kj it bd kk nw nx dn ko ny nz dp ks lj oa ob ku ln oc od kw lr oe of ky og bi translated">4.2尺寸增加剩余单元</h2><p id="306c" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">在<a class="ae lw" href="https://arxiv.org/pdf/1512.03385.pdf" rel="noopener ugc nofollow" target="_blank">【2】</a>提出的架构中，有增加维度的<em class="mc">剩余单元</em>。这通过使用直线投影<em class="mc"> W_s </em>通过快捷连接来实现，以匹配所需尺寸:</p><figure class="mr ms mt mu gt ng gh gi paragraph-image"><div class="gh gi ok"><img src="../Images/386984b016cfa93c6d533a1a7384eedc.png" data-original-src="https://miro.medium.com/v2/resize:fit:574/format:webp/1*nZqE2pW6i1eeoYEB1LPxzw.png"/></div></figure><p id="4661" class="pw-post-body-paragraph la lb it lc b ld lx ju lf lg ly jx li lj lz ll lm ln ma lp lq lr mb lt lu lv im bi translated">在这种情况下，它是由1x1卷积层完成的。</p><pre class="mr ms mt mu gt mv mw mx my aw mz bi"><span id="4b41" class="na kj it mw b gy nb nc l nd ne">class FiltersChangeResidualUnit(Layer):<br/><br/>    def __init__(self, out_filters, **kwargs):<br/>        super(FiltersChangeResidualUnit, self).__init__(**kwargs)<br/>        self.out_filters = out_filters<br/>        <br/>        <br/>    def build(self, input_shape):<br/>        number_filters = input_shape[0]<br/>        self.bn_1 = tf.keras.layers.BatchNormalization(input_shape=input_shape)<br/>        self.conv2d_1 = tf.keras.layers.Conv2D(input_shape[3], (3, 3), padding='same')<br/>        self.bn_2 = tf.keras.layers.BatchNormalization()<br/>        self.conv2d_2 = tf.keras.layers.Conv2D(self.out_filters, (3, 3), padding='same')<br/>        self.conv2d_3 = tf.keras.layers.Conv2D(self.out_filters, (1, 1))<br/>        <br/>        <br/>    def call(self, inputs, training=False):<br/>        x = self.bn_1(inputs, training)<br/>        x = tf.nn.relu(x)<br/>        x = self.conv2d_1(x)<br/>        x = self.bn_2(x, training)<br/>        x = tf.nn.relu(x)<br/>        x = self.conv2d_2(x)<br/>        x_1 = self.conv2d_3(inputs)<br/>        x = tf.keras.layers.add([x, x_1])<br/>        return x</span><span id="4514" class="na kj it mw b gy nf nc l nd ne">test_model = tf.keras.Sequential([FiltersChangeResidualUnit(16, input_shape=(32, 32, 3), name="fc_resnet_unit")])<br/>test_model.summary()</span><span id="f3e9" class="na kj it mw b gy nf nc l nd ne">Model: "sequential_1"<br/>_________________________________________________________________<br/>Layer (type)                 Output Shape              Param #   <br/>=================================================================<br/>fc_resnet_unit (FiltersChang (None, 32, 32, 16)        620       <br/>=================================================================<br/>Total params: 620<br/>Trainable params: 608<br/>Non-trainable params: 12<br/>_________________________________________________________________</span></pre><h2 id="a8a9" class="na kj it bd kk nw nx dn ko ny nz dp ks lj oa ob ku ln oc od kw lr oe of ky og bi translated">4.3模型</h2><p id="aefd" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">最后，我们可以建立完整的模型。我们首先定义一个有32个过滤器的<code class="fe oh oi oj mw b">Conv2D</code>层，一个7x7内核，步距为2。在第一层之后，我们添加我们的<em class="mc">剩余单元</em>。然后，我们添加一个新的<code class="fe oh oi oj mw b">Conv2D</code>层，有32个滤镜，一个3x3内核，步幅为2。之后，我们添加了我们的<em class="mc">剩余单元</em>，它允许以64的输出改变尺寸。为了最终确定我们的模型，我们将数据展平，并通过softmax激活函数和与类数量相同的单元数量将其馈送到一个<code class="fe oh oi oj mw b">Dense</code>层。</p><pre class="mr ms mt mu gt mv mw mx my aw mz bi"><span id="a833" class="na kj it mw b gy nb nc l nd ne">class ResNetModel(Model):<br/><br/>    def __init__(self, **kwargs):<br/>        super(ResNetModel, self).__init__()<br/>        self.conv2d_1 = tf.keras.layers.Conv2D(32, (7, 7), strides=(2,2))<br/>        self.resb = ResidualUnit()<br/>        self.conv2d_2 = tf.keras.layers.Conv2D(32, (3, 3), strides=(2,2))<br/>        self.filtersresb = FiltersChangeResidualUnit(64)<br/>        self.flatten_1 = tf.keras.layers.Flatten()<br/>        self.dense_o = tf.keras.layers.Dense(target_labels.shape[0], activation='softmax')   <br/>        <br/>    def call(self, inputs, training=False):<br/>        x = self.conv2d_1(inputs)<br/>        x = self.resb(x, training)<br/>        x = self.conv2d_2(x)<br/>        x = self.filtersresb(x, training)<br/>        x = self.flatten_1(x)<br/>        x = self.dense_o(x)<br/>        <br/>        return x</span><span id="6969" class="na kj it mw b gy nf nc l nd ne">resnet_model = ResNetModel()<br/>resnet_model(inputs= tf.random.normal((32, 128,128,3)))<br/>resnet_model.summary()</span><span id="126c" class="na kj it mw b gy nf nc l nd ne">Model: "res_net_model"<br/>_________________________________________________________________<br/>Layer (type)                 Output Shape              Param #   <br/>=================================================================<br/>conv2d_6 (Conv2D)            multiple                  4736      <br/>_________________________________________________________________<br/>residual_unit (ResidualUnit) multiple                  18752     <br/>_________________________________________________________________<br/>conv2d_7 (Conv2D)            multiple                  9248      <br/>_________________________________________________________________<br/>filters_change_residual_unit multiple                  30112     <br/>_________________________________________________________________<br/>flatten_1 (Flatten)          multiple                  0         <br/>_________________________________________________________________<br/>dense_2 (Dense)              multiple                  1094419   <br/>=================================================================<br/>Total params: 1,157,267<br/>Trainable params: 1,157,011<br/>Non-trainable params: 256<br/>_________________________________________________________________</span><span id="849b" class="na kj it mw b gy nf nc l nd ne">optimizer_obj = tf.keras.optimizers.Adam(learning_rate=0.001)<br/>loss_obj = tf.keras.losses.CategoricalCrossentropy()</span><span id="f03c" class="na kj it mw b gy nf nc l nd ne">@tf.function<br/>def grad(model, inputs, targets, loss):<br/>    with tf.GradientTape() as tape:<br/>        preds = model(inputs)<br/>        loss_value = loss(targets, preds)<br/>    return loss_value, tape.gradient(loss_value, model.trainable_variables)</span><span id="61ee" class="na kj it mw b gy nf nc l nd ne">def train_resnet(model, num_epochs, dataset, valid_dataset, optimizer, loss, grad_fn):<br/><br/>    train_steps_per_epoch = dataset.n // dataset.batch_size<br/>    train_steps_per_epoch_valid = valid_dataset.n // valid_dataset.batch_size<br/><br/>    train_loss_results = []<br/>    train_accuracy_results = []<br/>    train_loss_results_valid = []<br/>    train_accuracy_results_valid = []<br/>    for epoch in range(num_epochs):<br/>        start = time.time()<br/>        epoch_loss_avg = tf.keras.metrics.Mean()<br/>        epoch_accuracy = tf.keras.metrics.CategoricalAccuracy()<br/>        epoch_loss_avg_valid = tf.keras.metrics.Mean()<br/>        epoch_accuracy_valid = tf.keras.metrics.CategoricalAccuracy()<br/>        i=0<br/>        for x, y in dataset:<br/>            loss_value, grads = grad_fn(model, x, y, loss)<br/>            optimizer.apply_gradients(zip(grads, model.trainable_variables))<br/><br/>            epoch_loss_avg(loss_value)<br/>            epoch_accuracy(y, model(x))<br/>            if i&gt;=train_steps_per_epoch:<br/>                break<br/>            i+=1<br/>        j = 0<br/>        for x, y in valid_dataset:<br/>            model_output = model(x)<br/>            epoch_loss_avg_valid(loss_obj(y, model_output))  <br/>            epoch_accuracy_valid(y, model_output)<br/>            if j&gt;=train_steps_per_epoch_valid:<br/>                break<br/>            j+=1<br/><br/>        # End epoch<br/>        train_loss_results.append(epoch_loss_avg.result())<br/>        train_accuracy_results.append(epoch_accuracy.result())<br/>        train_loss_results_valid.append(epoch_loss_avg_valid.result())<br/>        train_accuracy_results_valid.append(epoch_accuracy_valid.result())<br/>        <br/>        print("Training -&gt; Epoch {:03d}: Loss: {:.3f}, Accuracy: {:.3%}".format(epoch,<br/>                                                                   epoch_loss_avg.result(),<br/>                                                                   epoch_accuracy.result()))<br/>        print("Validation -&gt; Epoch {:03d}: Loss: {:.3f}, Accuracy: {:.3%}".format(epoch,<br/>                                                           epoch_loss_avg_valid.result(),<br/>                                                           epoch_accuracy_valid.result()))<br/>        print(f'Time taken for 1 epoch {time.time()-start:.2f} sec\n')<br/>    <br/>    return train_loss_results, train_accuracy_results</span><span id="f64a" class="na kj it mw b gy nf nc l nd ne">train_loss_results, train_accuracy_results = train_resnet(resnet_model, <br/>                                                          40, <br/>                                                          train_generator_aug,<br/>                                                          validation_generator_aug,<br/>                                                          optimizer_obj, <br/>                                                          loss_obj, <br/>                                                          grad)</span><span id="28c7" class="na kj it mw b gy nf nc l nd ne">Training -&gt; Epoch 000: Loss: 2.654, Accuracy: 27.153%<br/>Validation -&gt; Epoch 000: Loss: 2.532, Accuracy: 23.488%<br/>Time taken for 1 epoch 137.62 sec<br/><br/>[...]<br/><br/>Training -&gt; Epoch 039: Loss: 0.749, Accuracy: 85.174%<br/>Validation -&gt; Epoch 039: Loss: 0.993, Accuracy: 75.218%<br/>Time taken for 1 epoch 137.56 sec</span></pre><h1 id="8976" class="ki kj it bd kk kl km kn ko kp kq kr ks jz kt ka ku kc kv kd kw kf kx kg ky kz bi translated">5.结果</h1><p id="b10a" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">残差网络在测试集上显示出更好的准确性，与基准模型的75.6%相比，接近81%的准确性。我们可以很容易地使剩余模型更深入，这是我们可以从这个架构中提取最大价值的场景。</p><pre class="mr ms mt mu gt mv mw mx my aw mz bi"><span id="0626" class="na kj it mw b gy nb nc l nd ne">fig, axes = plt.subplots(1, 2, sharex=True, figsize=(12, 5))<br/><br/>axes[0].set_xlabel("Epochs", fontsize=14)<br/>axes[0].set_ylabel("Loss", fontsize=14)<br/>axes[0].set_title('Loss vs epochs')<br/>axes[0].plot(train_loss_results)<br/><br/>axes[1].set_title('Accuracy vs epochs')<br/>axes[1].set_ylabel("Accuracy", fontsize=14)<br/>axes[1].set_xlabel("Epochs", fontsize=14)<br/>axes[1].plot(train_accuracy_results)<br/>plt.show()</span></pre><figure class="mr ms mt mu gt ng gh gi paragraph-image"><div class="ab gu cl nh"><img src="../Images/f3728b4debf6a83a298ae0b0b777e557.png" data-original-src="https://miro.medium.com/v2/format:webp/1*5PFdz83kqMBht2s1AaUQ2w.png"/></div><p class="nk nl gj gh gi nm nn bd b be z dk translated">图2:残差网络几个时期的训练精度和损失演变。</p></figure><pre class="mr ms mt mu gt mv mw mx my aw mz bi"><span id="45e1" class="na kj it mw b gy nb nc l nd ne">def test_model(model, test_generator):<br/><br/>    epoch_loss_avg = tf.keras.metrics.Mean()<br/>    epoch_accuracy = tf.keras.metrics.CategoricalAccuracy()<br/><br/>    train_steps_per_epoch = test_generator.n // test_generator.batch_size<br/>    i = 0<br/>    for x, y in test_generator:<br/>        model_output = model(x)<br/>        epoch_loss_avg(loss_obj(y, model_output))  <br/>        epoch_accuracy(y, model_output)<br/>        if i&gt;=train_steps_per_epoch:<br/>            break<br/>        i+=1<br/>    <br/>    print("Test loss: {:.3f}".format(epoch_loss_avg.result().numpy()))<br/>    print("Test accuracy: {:.3%}".format(epoch_accuracy.result().numpy()))</span><span id="6e3f" class="na kj it mw b gy nf nc l nd ne">print('ResNet Model')<br/>test_model(resnet_model, validation_generator)<br/>print('Benchmark Model')<br/>test_model(benchmark_model, validation_generator)</span><span id="ffb0" class="na kj it mw b gy nf nc l nd ne">ResNet Model<br/>Test loss: 0.787<br/>Test accuracy: 80.945%<br/>Benchmark Model<br/>Test loss: 1.067<br/>Test accuracy: 75.607%</span><span id="d02f" class="na kj it mw b gy nf nc l nd ne">num_test_images = validation_generator.n<br/><br/>random_test_images, random_test_labels = next(validation_generator)<br/><br/>predictions = resnet_model(random_test_images)<br/><br/>fig, axes = plt.subplots(4, 2, figsize=(25, 12))<br/>fig.subplots_adjust(hspace=0.5, wspace=-0.35)<br/><br/>j=0<br/>for i, (prediction, image, label) in enumerate(zip(predictions, random_test_images, target_labels[(tf.argmax(random_test_labels, axis=1).numpy())])):<br/>    if j &gt;3:<br/>        break<br/>    axes[i, 0].imshow(np.squeeze(image))<br/>    axes[i, 0].get_xaxis().set_visible(False)<br/>    axes[i, 0].get_yaxis().set_visible(False)<br/>    axes[i, 0].text(5., -7., f'Class {label}')<br/>    axes[i, 1].bar(np.arange(len(prediction)), prediction)<br/>    axes[i, 1].set_xticks(np.arange(len(prediction)))<br/>    axes[i, 1].set_xticklabels([l.split('_')[0] for l in target_labels], rotation=0)<br/>    pred_inx = np.argmax(prediction)<br/>    axes[i, 1].set_title(f"Categorical distribution. Model prediction: {target_labels[pred_inx]}")<br/>    j+=1<br/>plt.show()</span></pre><figure class="mr ms mt mu gt ng gh gi paragraph-image"><div class="ab gu cl nh"><img src="../Images/529d69ebc7fdaaeb661330729f31e6e2.png" data-original-src="https://miro.medium.com/v2/format:webp/1*5zx9M6xk1YQng05Rj7pqmA.png"/></div><p class="nk nl gj gh gi nm nn bd b be z dk translated">图3:随机图像(左侧)和残差网络产生的预测的相应分类分布(右侧<em class="ol"/>)。</p></figure><h1 id="5c4a" class="ki kj it bd kk kl km kn ko kp kq kr ks jz kt ka ku kc kv kd kw kf kx kg ky kz bi translated">6.结论</h1><p id="d3d8" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">残余网络引入了一种系统的方法，通过简单地执行身份映射来使用<em class="mc">快捷连接</em>。这被证明稳定了深度网络的优化过程。此外，剩余网络通过超越传统CNN显示了快捷连接的力量。</p><p id="a44c" class="pw-post-body-paragraph la lb it lc b ld lx ju lf lg ly jx li lj lz ll lm ln ma lp lq lr mb lt lu lv im bi translated">该方法可以通过增加模型的深度来扩展。这是特别合适的，因为剩余网络不会遭受退化问题。</p><p id="3951" class="pw-post-body-paragraph la lb it lc b ld lx ju lf lg ly jx li lj lz ll lm ln ma lp lq lr mb lt lu lv im bi translated">保持联系:<a class="ae lw" href="https://www.linkedin.com/in/luisbrasroque/" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a></p><h1 id="3fd7" class="ki kj it bd kk kl km kn ko kp kq kr ks jz kt ka ku kc kv kd kw kf kx kg ky kz bi translated">7.参考</h1><p id="bdc5" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated"><a class="ae lw" href="https://papers.nips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf" rel="noopener ugc nofollow" target="_blank">【1】</a>【Krizhevsky等人，2012】Krizhevsky，a .，Sutskever，I .，Hinton，G. E. (2012)。基于深度卷积神经网络的图像网分类。神经信息处理系统进展，25:1097–1105。</p><p id="c1f5" class="pw-post-body-paragraph la lb it lc b ld lx ju lf lg ly jx li lj lz ll lm ln ma lp lq lr mb lt lu lv im bi translated"><a class="ae lw" href="https://arxiv.org/pdf/1512.03385.pdf" rel="noopener ugc nofollow" target="_blank">【2】</a>【何等，2015】何，k，张，x，任，s，孙，J. (2015)。用于图像识别的深度残差学习。</p><p id="aa03" class="pw-post-body-paragraph la lb it lc b ld lx ju lf lg ly jx li lj lz ll lm ln ma lp lq lr mb lt lu lv im bi translated"><a class="ae lw" href="https://arxiv.org/abs/1603.05027" rel="noopener ugc nofollow" target="_blank">【3】</a>【何等，2016】何，k，张，x，任，s，孙，J. (2016)。深剩余网络中的身份映射。</p><p id="4acc" class="pw-post-body-paragraph la lb it lc b ld lx ju lf lg ly jx li lj lz ll lm ln ma lp lq lr mb lt lu lv im bi translated"><a class="ae lw" href="https://www.kaggle.com/alexattia/the-simpsons-characters-dataset" rel="noopener ugc nofollow" target="_blank">【4】</a><a class="ae lw" href="https://www.kaggle.com/alexattia/the-simpsons-characters-dataset" rel="noopener ugc nofollow" target="_blank">https://www . ka ggle . com/Alex attia/the-Simpsons-characters-dataset</a></p><p id="bde7" class="pw-post-body-paragraph la lb it lc b ld lx ju lf lg ly jx li lj lz ll lm ln ma lp lq lr mb lt lu lv im bi translated"><a class="ae lw" rel="noopener" target="_blank" href="/transfer-learning-and-data-augmentation-applied-to-the-simpsons-image-dataset-e292716fbd43">【5】</a><a class="ae lw" rel="noopener" target="_blank" href="/transfer-learning-and-data-augmentation-applied-to-the-simpsons-image-dataset-e292716fbd43">https://towards data science . com/transfer-learning-and-data-augmentation-applied-to-the-Simpsons-image-dataset-e 292716 FBD 43</a></p></div></div>    
</body>
</html>