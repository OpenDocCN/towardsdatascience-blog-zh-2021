<html>
<head>
<title>Hot dog or Not Hot dog</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">热狗还是不是热狗</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/hot-dog-or-not-hot-dog-ab9d67f20674?source=collection_archive---------16-----------------------#2021-09-07">https://towardsdatascience.com/hot-dog-or-not-hot-dog-ab9d67f20674?source=collection_archive---------16-----------------------#2021-09-07</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="4d42" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">用Tensorflow 2尝试著名的CNN模型来帮助杨坚</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/74169685a796a3451c6f3439fcc0eda4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*t3fAzvrsJFd7lZQwE4DDzQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">输出图像</p></figure><h1 id="def7" class="kv kw iq bd kx ky kz la lb lc ld le lf jw lg jx lh jz li ka lj kc lk kd ll lm bi translated">介绍</h1><p id="c172" class="pw-post-body-paragraph ln lo iq lp b lq lr jr ls lt lu ju lv lw lx ly lz ma mb mc md me mf mg mh mi ij bi translated">你看过HBO的《硅谷》喜剧系列吗？如果是这样，我打赌你还记得杨坚开发的<a class="ae mj" href="https://apps.apple.com/app/not-hotdog/id1212457521" rel="noopener ugc nofollow" target="_blank">非热狗应用</a>。这里有一个片段可以提醒你。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="mk ml l"/></div></figure><p id="95e1" class="pw-post-body-paragraph ln lo iq lp b lq mm jr ls lt mn ju lv lw mo ly lz ma mp mc md me mq mg mh mi ij bi translated">所以基本上这个应用程序识别某样东西是不是热狗。嗯，我们也可以用其他类型的物体来训练识别它们。</p><p id="3bde" class="pw-post-body-paragraph ln lo iq lp b lq mm jr ls lt mn ju lv lw mo ly lz ma mp mc md me mq mg mh mi ij bi translated">当我了解到CNN(卷积神经网络)的时候，我很渴望在这个问题上尝试一些流行的CNN模型，只是为了好玩。所以我选择了一些最好的CNN模型来尝试。</p><p id="2937" class="pw-post-body-paragraph ln lo iq lp b lq mm jr ls lt mn ju lv lw mo ly lz ma mp mc md me mq mg mh mi ij bi translated">对于这个问题，我使用了以下模型，</p><ol class=""><li id="4a3e" class="mr ms iq lp b lq mm lt mn lw mt ma mu me mv mi mw mx my mz bi translated">AlexNet的变体</li><li id="97a6" class="mr ms iq lp b lq na lt nb lw nc ma nd me ne mi mw mx my mz bi translated">使用VGG19进行迁移学习</li><li id="c91c" class="mr ms iq lp b lq na lt nb lw nc ma nd me ne mi mw mx my mz bi translated">使用ResNet50迁移学习</li><li id="02ac" class="mr ms iq lp b lq na lt nb lw nc ma nd me ne mi mw mx my mz bi translated">使用Inception V3进行迁移学习</li><li id="cd62" class="mr ms iq lp b lq na lt nb lw nc ma nd me ne mi mw mx my mz bi translated">借助Inception ResNet V2迁移学习</li></ol><p id="798d" class="pw-post-body-paragraph ln lo iq lp b lq mm jr ls lt mn ju lv lw mo ly lz ma mp mc md me mq mg mh mi ij bi translated">你可以在<a class="ae mj" href="https://nbviewer.jupyter.org/github/scsanjay/case-studies/blob/main/02.%20Not%20Hotdog/Not-HotDog.ipynb" rel="noopener ugc nofollow" target="_blank"> NBViewer </a>上查看笔记本，也可以在GitHub上找到，</p><div class="nf ng gp gr nh ni"><a href="https://github.com/scsanjay/case-studies/tree/main/02.%20Not%20Hotdog" rel="noopener  ugc nofollow" target="_blank"><div class="nj ab fo"><div class="nk ab nl cl cj nm"><h2 class="bd ir gy z fp nn fr fs no fu fw ip bi translated">案例研究/02。主餐/案例研究中没有热狗</h2><div class="np l"><h3 class="bd b gy z fp nn fr fs no fu fw dk translated">在GitHub上创建一个帐户，为scsanjay/案例研究的发展做出贡献。</h3></div><div class="nq l"><p class="bd b dl z fp nn fr fs no fu fw dk translated">github.com</p></div></div><div class="nr l"><div class="ns l nt nu nv nr nw kp ni"/></div></div></a></div><h1 id="8592" class="kv kw iq bd kx ky kz la lb lc ld le lf jw lg jx lh jz li ka lj kc lk kd ll lm bi translated">数据</h1><p id="0e1d" class="pw-post-body-paragraph ln lo iq lp b lq lr jr ls lt lu ju lv lw lx ly lz ma mb mc md me mf mg mh mi ij bi translated">没有高质量的数据，就没有机器学习。谢天谢地，我在ka ggle<a class="ae mj" href="https://www.kaggle.com/yashvrdnjain/hotdognothotdog/metadata" rel="noopener ugc nofollow" target="_blank">【1】</a>上找到了一个数据集。</p><p id="faf9" class="pw-post-body-paragraph ln lo iq lp b lq mm jr ls lt mn ju lv lw mo ly lz ma mp mc md me mq mg mh mi ij bi translated">总共有<strong class="lp ir"> 3000张</strong>图片可用于训练。<br/>其中<strong class="lp ir"> 1500 </strong>是热狗图像，<strong class="lp ir"> 1500 </strong>不是热狗(它们是食物、家具、人或宠物)。<br/> <strong class="lp ir">训练数据的20% </strong>将用于验证，这意味着<strong class="lp ir"> 600张</strong>图像将用于验证。<br/>测试集有来自热狗类别的<strong class="lp ir"> 322 </strong>图像和来自热狗类别的<strong class="lp ir"> 322 </strong>图像。</p><p id="48b3" class="pw-post-body-paragraph ln lo iq lp b lq mm jr ls lt mn ju lv lw mo ly lz ma mp mc md me mq mg mh mi ij bi translated">在加载的时候，我已经将所有图片的尺寸调整为256x256。另外，我已经将它们分成<strong class="lp ir"> 32 </strong>的批量。</p><p id="8adc" class="pw-post-body-paragraph ln lo iq lp b lq mm jr ls lt mn ju lv lw mo ly lz ma mp mc md me mq mg mh mi ij bi translated">让我们看一些图片，</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/20506c0a2ac458904a5a787227d0b614.png" data-original-src="https://miro.medium.com/v2/resize:fit:1136/format:webp/1*udfF1xxbbkDKQFCosSxAdA.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">数据样本</p></figure><h1 id="0a96" class="kv kw iq bd kx ky kz la lb lc ld le lf jw lg jx lh jz li ka lj kc lk kd ll lm bi translated">预处理</h1><h2 id="0234" class="ny kw iq bd kx nz oa dn lb ob oc dp lf lw od oe lh ma of og lj me oh oi ll oj bi translated">调整大小</h2><p id="c200" class="pw-post-body-paragraph ln lo iq lp b lq lr jr ls lt lu ju lv lw lx ly lz ma mb mc md me mf mg mh mi ij bi translated">将所有图像转换为相同大小的预处理步骤已经完成。</p><h2 id="34b2" class="ny kw iq bd kx nz oa dn lb ob oc dp lf lw od oe lh ma of og lj me oh oi ll oj bi translated">数据扩充</h2><p id="d64f" class="pw-post-body-paragraph ln lo iq lp b lq lr jr ls lt lu ju lv lw lx ly lz ma mb mc md me mf mg mh mi ij bi translated">数据扩充是非常有用的一步。这有助于使模型更好地泛化。此外，它从给定的图像生成新的图像，这增加了我们的数据集的大小。</p><p id="dd58" class="pw-post-body-paragraph ln lo iq lp b lq mm jr ls lt mn ju lv lw mo ly lz ma mp mc md me mq mg mh mi ij bi translated">怎么会？它可以执行各种操作，如翻转、旋转、剪切、缩放等，以创建增强的数据。请注意，TensorFlow是动态完成的，这意味着我们不必保存图像，但它们将在训练时生成。</p><p id="4a7e" class="pw-post-body-paragraph ln lo iq lp b lq mm jr ls lt mn ju lv lw mo ly lz ma mp mc md me mq mg mh mi ij bi translated">我已经执行了以下数据扩充操作，<br/> a)水平翻转<br/> b)旋转<br/> c)缩放</p><p id="e3b3" class="pw-post-body-paragraph ln lo iq lp b lq mm jr ls lt mn ju lv lw mo ly lz ma mp mc md me mq mg mh mi ij bi translated">在数据扩充之后，我们可以期待如下所示的图像，</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/b0c44229b48acca0e1c21042182daa7e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1136/format:webp/1*pZSKcwlni0VyFzc60YsiXg.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">扩充数据</p></figure><h2 id="2ee5" class="ny kw iq bd kx nz oa dn lb ob oc dp lf lw od oe lh ma of og lj me oh oi ll oj bi translated">改比例</h2><p id="278e" class="pw-post-body-paragraph ln lo iq lp b lq lr jr ls lt lu ju lv lw lx ly lz ma mb mc md me mf mg mh mi ij bi translated">我们应该将像素从[0，255]重新调整到[0，1]。我将只应用于AlexNet。</p><p id="d6b1" class="pw-post-body-paragraph ln lo iq lp b lq mm jr ls lt mn ju lv lw mo ly lz ma mp mc md me mq mg mh mi ij bi translated"><strong class="lp ir">在使用预训练模型进行迁移学习的情况下，我们将仅使用相应模型的预处理。</strong></p><h1 id="e7e1" class="kv kw iq bd kx ky kz la lb lc ld le lf jw lg jx lh jz li ka lj kc lk kd ll lm bi translated">模型结构</h1><h2 id="6105" class="ny kw iq bd kx nz oa dn lb ob oc dp lf lw od oe lh ma of og lj me oh oi ll oj bi translated">AlexNet</h2><p id="4861" class="pw-post-body-paragraph ln lo iq lp b lq lr jr ls lt lu ju lv lw lx ly lz ma mb mc md me mf mg mh mi ij bi translated">AlexNet用了三大概念，<br/> 1。数据扩充—增加数据的方差。<br/> 2。辍学——应对过度适应。<br/> 3。ReLU激活单元——处理消失梯度问题。</p><p id="89e5" class="pw-post-body-paragraph ln lo iq lp b lq mm jr ls lt mn ju lv lw mo ly lz ma mp mc md me mq mg mh mi ij bi translated">我已经创建了一个AlexNet架构的变体，并在这里和那里进行了退出和批处理规范化。总共有58，286，465个可训练参数。最后一层有1个乙状结肠活化单位。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ok ml l"/></div></figure><p id="0c20" class="pw-post-body-paragraph ln lo iq lp b lq mm jr ls lt mn ju lv lw mo ly lz ma mp mc md me mq mg mh mi ij bi translated">我将使用基于二进制交叉熵的Adam优化器进行优化。我们也会保持准确性。</p><p id="3110" class="pw-post-body-paragraph ln lo iq lp b lq mm jr ls lt mn ju lv lw mo ly lz ma mp mc md me mq mg mh mi ij bi translated">我已经运行了10个纪元。我还添加了一个提前停止的回调函数来监控val_loss。我们通过验证数据得到了<strong class="lp ir"> 69.00% </strong>的准确度，通过测试数据得到了<strong class="lp ir"> 68.47% </strong>的准确度。这并不令人鼓舞。如果我们进行调整或拥有更多数据，我们可以取得更大的成就。</p><h2 id="01c2" class="ny kw iq bd kx nz oa dn lb ob oc dp lf lw od oe lh ma of og lj me oh oi ll oj bi translated">使用预训练的VGG19进行迁移学习</h2><p id="1e14" class="pw-post-body-paragraph ln lo iq lp b lq lr jr ls lt lu ju lv lw lx ly lz ma mb mc md me mf mg mh mi ij bi translated"><strong class="lp ir">迁移学习</strong></p><p id="6778" class="pw-post-body-paragraph ln lo iq lp b lq mm jr ls lt mn ju lv lw mo ly lz ma mp mc md me mq mg mh mi ij bi translated">迁移学习是一种非常酷的技术。在迁移学习中，我们加载一个预先训练好的模型。我们移除模型的顶部(最后几个致密层)。然后我们冻结预训练模型的卷积基。现在我们可以使用这个预先训练的模型作为特征提取器。</p><p id="ef73" class="pw-post-body-paragraph ln lo iq lp b lq mm jr ls lt mn ju lv lw mo ly lz ma mp mc md me mq mg mh mi ij bi translated">在迁移学习中，有时我们也可以使用预训练模型进行权重初始化，然后训练整个模型。这通常发生在我们有大量数据和/或预训练模型没有在类似数据上训练的时候。</p><p id="3bd3" class="pw-post-body-paragraph ln lo iq lp b lq mm jr ls lt mn ju lv lw mo ly lz ma mp mc md me mq mg mh mi ij bi translated"><strong class="lp ir"> VGG19 </strong></p><p id="5c3f" class="pw-post-body-paragraph ln lo iq lp b lq mm jr ls lt mn ju lv lw mo ly lz ma mp mc md me mq mg mh mi ij bi translated">VGG19来自视觉几何组。它有19层。它有一个新的想法，当我们使用多个小内核而不是较小的大内核时，可训练参数的数量会减少。此外，它到处使用相同的3x3大小的内核和2x2最大池，这简化了架构。</p><p id="e3b4" class="pw-post-body-paragraph ln lo iq lp b lq mm jr ls lt mn ju lv lw mo ly lz ma mp mc md me mq mg mh mi ij bi translated">我已经用ImageNet权重加载了TensorFlow Keras预训练的VGG19模型，没有顶层。</p><p id="c4ce" class="pw-post-body-paragraph ln lo iq lp b lq mm jr ls lt mn ju lv lw mo ly lz ma mp mc md me mq mg mh mi ij bi translated">然后我将这个基础模型设置为不可训练。</p><p id="23a1" class="pw-post-body-paragraph ln lo iq lp b lq mm jr ls lt mn ju lv lw mo ly lz ma mp mc md me mq mg mh mi ij bi translated">在模型架构中，</p><p id="62fa" class="pw-post-body-paragraph ln lo iq lp b lq mm jr ls lt mn ju lv lw mo ly lz ma mp mc md me mq mg mh mi ij bi translated">I)我首先添加了输入层。<br/> ii)然后是数据扩充层。Tensorflow将自动管理扩增仅用于训练。<br/> iii)然后我用了VGG19模型的预处理。<br/> iv)之后，我使用了基础模型，即预训练的VGG19模型。这是不能按照迁移学习来训练的。<br/> v)然后我使用了全球平均池2D层。<br/> vi)然后是一个展平层。<br/> vii)然后是具有1000个Relu激活单元的全连接致密层。为了正规化，我也加入了退学。然后我添加了一个线性激活单元。因为我将在二进制交叉熵中使用` from_logits=True <strong class="lp ir"> ` </strong>来获得概率。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ok ml l"/></div></figure><p id="fe4c" class="pw-post-body-paragraph ln lo iq lp b lq mm jr ls lt mn ju lv lw mo ly lz ma mp mc md me mq mg mh mi ij bi translated">它有514，001个可训练参数和20，024，384个不可训练参数。</p><p id="13bc" class="pw-post-body-paragraph ln lo iq lp b lq mm jr ls lt mn ju lv lw mo ly lz ma mp mc md me mq mg mh mi ij bi translated">和前面一样，我们使用了Adam优化器和二元交叉熵。</p><p id="8af0" class="pw-post-body-paragraph ln lo iq lp b lq mm jr ls lt mn ju lv lw mo ly lz ma mp mc md me mq mg mh mi ij bi translated">这次我们用验证数据得到了<strong class="lp ir"> 93.17% </strong>的准确率，用测试数据得到了<strong class="lp ir"> 93.47% </strong>的准确率。这是一个很大的进步。</p><h2 id="9dad" class="ny kw iq bd kx nz oa dn lb ob oc dp lf lw od oe lh ma of og lj me oh oi ll oj bi translated">使用预先培训的ResNet50进行迁移学习</h2><p id="1f75" class="pw-post-body-paragraph ln lo iq lp b lq lr jr ls lt lu ju lv lw lx ly lz ma mb mc md me mf mg mh mi ij bi translated">ResNet50由<a class="ae mj" href="https://arxiv.org/pdf/1512.03385.pdf" rel="noopener ugc nofollow" target="_blank">何等人</a>创造。它有50层。它引入了剩余块的概念。它有助于建立深度大的模型。剩余的块具有跳过连接，所以如果一个块是无用的，那么它就被忽略。</p><p id="91aa" class="pw-post-body-paragraph ln lo iq lp b lq mm jr ls lt mn ju lv lw mo ly lz ma mp mc md me mq mg mh mi ij bi translated">我们按照与上面相同的步骤来创建相同的结构。唯一的区别是基础模型现在是ResNet50，预处理是ResNet50。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ok ml l"/></div></figure><p id="1710" class="pw-post-body-paragraph ln lo iq lp b lq mm jr ls lt mn ju lv lw mo ly lz ma mp mc md me mq mg mh mi ij bi translated">它有2，050，001个可训练参数和23，587，712个不可训练参数。</p><p id="7975" class="pw-post-body-paragraph ln lo iq lp b lq mm jr ls lt mn ju lv lw mo ly lz ma mp mc md me mq mg mh mi ij bi translated">编译和拟合阶段与上面类似。而这次，我们用验证数据得到了<strong class="lp ir"> 94.33% </strong>的准确率，用测试数据得到了<strong class="lp ir"> 94.56% </strong>的准确率。比以前有了一点点进步。</p><h2 id="0d90" class="ny kw iq bd kx nz oa dn lb ob oc dp lf lw od oe lh ma of og lj me oh oi ll oj bi translated">通过预先培训的Inception V3进行迁移学习</h2><p id="a408" class="pw-post-body-paragraph ln lo iq lp b lq lr jr ls lt lu ju lv lw lx ly lz ma mb mc md me mf mg mh mi ij bi translated">Inception v3是第三个版本。它是由谷歌开发的。它有48层。初始网络不是一次使用一个卷积，而是同时使用所有1x1、3x3、5x5和MaxPool。这个想法是，较小的内核将获得本地信息，而较大的内核将获得全局信息。它还有一个叫做瓶颈层的技巧，可以显著减少计算量。</p><p id="6060" class="pw-post-body-paragraph ln lo iq lp b lq mm jr ls lt mn ju lv lw mo ly lz ma mp mc md me mq mg mh mi ij bi translated">同样，我们已经将基本模型更改为Inception V3以及预处理步骤。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ok ml l"/></div></figure><p id="15f3" class="pw-post-body-paragraph ln lo iq lp b lq mm jr ls lt mn ju lv lw mo ly lz ma mp mc md me mq mg mh mi ij bi translated">它有2，050，001个可训练参数，与ResNet50相同，还有21，802，784个不可训练参数。</p><p id="a551" class="pw-post-body-paragraph ln lo iq lp b lq mm jr ls lt mn ju lv lw mo ly lz ma mp mc md me mq mg mh mi ij bi translated">编译和拟合阶段与上面类似。通过验证数据，我们得到了<strong class="lp ir"> 92.67% </strong>的准确率，通过测试数据，我们得到了<strong class="lp ir"> 94.40% </strong>的准确率。比我们用ResNet50得到的要少。</p><h2 id="f954" class="ny kw iq bd kx nz oa dn lb ob oc dp lf lw od oe lh ma of og lj me oh oi ll oj bi translated">通过预先培训的Inception ResNet V2迁移学习</h2><p id="21cc" class="pw-post-body-paragraph ln lo iq lp b lq lr jr ls lt lu ju lv lw lx ly lz ma mb mc md me mf mg mh mi ij bi translated">盗梦空间ResNet V2由谷歌开发。他们在盗梦空间中增加了ResNet的跳过连接。它允许创建一个更深的164层网络。</p><p id="ee2d" class="pw-post-body-paragraph ln lo iq lp b lq mm jr ls lt mn ju lv lw mo ly lz ma mp mc md me mq mg mh mi ij bi translated">我做了和上面类似的改动。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ok ml l"/></div></figure><p id="f068" class="pw-post-body-paragraph ln lo iq lp b lq mm jr ls lt mn ju lv lw mo ly lz ma mp mc md me mq mg mh mi ij bi translated">它有1，538，001个可训练参数和54，336，736个不可训练参数。</p><p id="3c36" class="pw-post-body-paragraph ln lo iq lp b lq mm jr ls lt mn ju lv lw mo ly lz ma mp mc md me mq mg mh mi ij bi translated">编译和拟合阶段类似于我们到目前为止所看到的。通过验证数据，我们得到了95.33% 的准确度，通过测试数据，我们得到了96.42% 的准确度。迄今为止最好的一个。</p><h1 id="8c7a" class="kv kw iq bd kx ky kz la lb lc ld le lf jw lg jx lh jz li ka lj kc lk kd ll lm bi translated">测试输出</h1><p id="2f3e" class="pw-post-body-paragraph ln lo iq lp b lq lr jr ls lt lu ju lv lw lx ly lz ma mb mc md me mf mg mh mi ij bi translated">所有四种迁移学习模式的表现都很相似，而且相当不错。</p><p id="21d2" class="pw-post-body-paragraph ln lo iq lp b lq mm jr ls lt mn ju lv lw mo ly lz ma mp mc md me mq mg mh mi ij bi translated">然而，<strong class="lp ir">盗梦空间ResNet V2 </strong>在我们的案例中具有最高的准确性。因此，我们将使用它来显示一些带有测试图像的输出。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ol"><img src="../Images/4af03f891c18ef4c23411193b80b9feb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qvJsbi8awP8pDCaWCpUmgg.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">测试输出</p></figure><h1 id="0fbf" class="kv kw iq bd kx ky kz la lb lc ld le lf jw lg jx lh jz li ka lj kc lk kd ll lm bi translated">最后的想法</h1><p id="8de0" class="pw-post-body-paragraph ln lo iq lp b lq lr jr ls lt lu ju lv lw lx ly lz ma mb mc md me mf mg mh mi ij bi translated">如果你看到我们只为模型做了几件事，数据扩充，预处理，加载基础模型，并在顶部添加几层。我们能够达到96.30%的准确率。这是因为迁移学习。我们可以在基础模型中增加更多的可训练层。这可以提高精确度。</p><p id="b85f" class="pw-post-body-paragraph ln lo iq lp b lq mm jr ls lt mn ju lv lw mo ly lz ma mp mc md me mq mg mh mi ij bi translated">参与这个项目非常有趣。也是一次很棒的学习经历，因为这是我的第一个深度学习项目。</p><p id="50e6" class="pw-post-body-paragraph ln lo iq lp b lq mm jr ls lt mn ju lv lw mo ly lz ma mp mc md me mq mg mh mi ij bi translated">深度学习发展非常快。每天都有新的研究论文发表。只有少数人制作了这些作品。深度学习就是要聪明。在所有的模型中，我们在上面看到，他们引入了一些新的聪明的技术。</p><h1 id="acb5" class="kv kw iq bd kx ky kz la lb lc ld le lf jw lg jx lh jz li ka lj kc lk kd ll lm bi translated">参考</h1><ol class=""><li id="4214" class="mr ms iq lp b lq lr lt lu lw om ma on me oo mi mw mx my mz bi translated">张量流[https://www.tensorflow.org/]</li><li id="3959" class="mr ms iq lp b lq na lt nb lw nc ma nd me ne mi mw mx my mz bi translated">数据集:Jain，Yashvardhan。2019 .热狗-不是热狗。<br/>版本1。许可证CC0:公共领域。可从以下网址获得:<a class="ae mj" href="https://www.kaggle.com/yashvrdnjain/hotdognothotdog/metadata" rel="noopener ugc nofollow" target="_blank">https://www . ka ggle . com/yashvrdnjain/hotdognotdog/metadata</a></li><li id="7cfb" class="mr ms iq lp b lq na lt nb lw nc ma nd me ne mi mw mx my mz bi translated">应用根[<a class="ae mj" href="https://www.appliedroots.com/" rel="noopener ugc nofollow" target="_blank">https://www.appliedroots.com/</a></li></ol><p id="d7ec" class="pw-post-body-paragraph ln lo iq lp b lq mm jr ls lt mn ju lv lw mo ly lz ma mp mc md me mq mg mh mi ij bi translated"><strong class="lp ir">感谢阅读博客！</strong>你可以通过我的<a class="ae mj" href="https://www.linkedin.com/in/scchouhansanjay/" rel="noopener ugc nofollow" target="_blank"> LinkedIn个人资料</a>联系到我。也👏如果你喜欢的话。</p></div></div>    
</body>
</html>