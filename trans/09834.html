<html>
<head>
<title>The fastest way to fetch BigQuery tables</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">获取BigQuery表的最快方法</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/the-fastest-way-to-fetch-bigquery-tables-352e2e26c9e1?source=collection_archive---------6-----------------------#2021-09-15">https://towardsdatascience.com/the-fastest-way-to-fetch-bigquery-tables-352e2e26c9e1?source=collection_archive---------6-----------------------#2021-09-15</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="7258" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">从BigQuery获取表的最快方法的基准。还介绍了bqfetch:一个简单易用的快速抓取工具。</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/5d54034e52451461bed80ea3913470d7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_-unLw9Bb2epUMWnHYGnbg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><p id="c6f4" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">作为一名数据工程师，我想尽可能快地从BigQuery获取表。我还需要获取这些表格作为熊猫数据帧。因此，我考虑了许多备选方案，我已经使用多个框架对许多实现进行了测试和基准测试，在本文中，我将向您展示我构建的一个工具，该工具允许我以数据帧的形式获取BQ表，并获得最佳性能。</p><blockquote class="lu lv lw"><p id="56b8" class="ky kz lx la b lb lc ju ld le lf jx lg ly li lj lk lz lm ln lo ma lq lr ls lt im bi translated">以下所有建议都是基于在Google Compute Engines (GCE)上测试的基准，根据您使用的机器和互联网带宽，可能存在一些更好的实现。</p></blockquote><h2 id="c1a8" class="mb mc it bd md me mf dn mg mh mi dp mj lh mk ml mm ll mn mo mp lp mq mr ms mt bi translated">旧方法</h2><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mu"><img src="../Images/b6ef25ff7a81830d351e8151c4450a27.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DMN8rPWFhd096I2YhcjLJA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">使用GCS的提取方法和使用dask的多重处理</p></figure><p id="82f4" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">从BigQuery获取数据的一个众所周知的方法如下:</p><ul class=""><li id="9033" class="mv mw it la b lb lc le lf lh mx ll my lp mz lt na nb nc nd bi translated">使用GZIP压缩将表提取到Google云存储中。它将创建多个csv文件，每个文件包含表中的一些行，使用GZIP格式压缩。使用BigQuery API时，此操作没有成本。</li><li id="d997" class="mv mw it la b lb ne le nf lh ng ll nh lp ni lt na nb nc nd bi translated">使用<a class="ae nj" href="https://dask.org/" rel="noopener ugc nofollow" target="_blank"> Dask </a>框架，使用<code class="fe nk nl nm nn b">read_csv(path_to_gcs_bucket)</code>方法获取桶中的所有文件，使用机器上所有可用内核的多处理。这个方法返回一个懒惰的Dask。因为懒惰而适合内存的数据帧。如果您需要将完整的df加载到内存中，那么可以使用<code class="fe nk nl nm nn b">compute()</code>方法。</li></ul><p id="76db" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这种方法可以获得合适的结果，但是有两个主要的瓶颈:</p><ul class=""><li id="adf0" class="mv mw it la b lb lc le lf lh mx ll my lp mz lt na nb nc nd bi translated">我们必须使用GCS来将表转换成多个文件，所以我们认为这不是获取数据的最合适的方法，我们应该有一个更简单和更快的方法。此外，表的压缩及其提取到GCS需要时间，所以我们肯定要删除这一部分。</li><li id="11be" class="mv mw it la b lb ne le nf lh ng ll nh lp ni lt na nb nc nd bi translated">使用这种方法，整个表被同时取出，如果不使用Dask的<code class="fe nk nl nm nn b">compute()</code>方法，我们不能正确使用数据帧，如果使用它，df必须适合内存。</li></ul><h2 id="c7ae" class="mb mc it bd md me mf dn mg mh mi dp mj lh mk ml mm ll mn mo mp lp mq mr ms mt bi translated">新方法</h2><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi no"><img src="../Images/7a20e935870cb9c36afa05462afc1323.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Pg3lC5sNXazMJLsNd-03Ow.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">通过BQ存储API使用分块表的新方法</p></figure><p id="0804" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">尽管有其局限性，<a class="ae nj" href="https://cloud.google.com/bigquery/docs/reference/storage" rel="noopener ugc nofollow" target="_blank"> BigQuery存储API </a>具有直接获取表的优势，因此我们不必担心GCS部分，这使我们在使用第一种方法时感到恼火。</p><p id="2182" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">使用示例解释该方法:</p><ul class=""><li id="81ed" class="mv mw it la b lb lc le lf lh mx ll my lp mz lt na nb nc nd bi translated">假设我们有一台16GB可用内存的机器。为了有效地使用这个内存，我们希望获取最大可用空间的数据，然后处理它，并重复这个过程，直到表被完全获取。所以第一步是把整个表分成大小为&lt;16Gb. However, a DataFrame object is greater than the raw fetched data (basically 1/3 bigger than the fetched data). We can now say that if we fetch ~10.7GB of data we’ll have a DataFrame of size 16GB that will fit into memory. So the idea is to chunk the whole table in N tables of size ~10.7GB. Ex: for a table of size 200GB we’ll create 19 tables of size ~10.7GB.</li><li id="3cbb" class="mv mw it la b lb ne le nf lh ng ll nh lp ni lt na nb nc nd bi translated">Once we have created all the chunked tables, we just have to fetch them sequentially using the BigQuery Storage API.</li></ul><blockquote class="lu lv lw"><p id="a592" class="ky kz lx la b lb lc ju ld le lf jx lg ly li lj lk lz lm ln lo ma lq lr ls lt im bi translated">But wait, why do we have do divide the main table in chunked tables? Why can’t we just use a filter with an “IN” SQL statement in order to select only some rows at each time? Here is the reason.</p></blockquote><h2 id="fba1" class="mb mc it bd md me mf dn mg mh mi dp mj lh mk ml mm ll mn mo mp lp mq mr ms mt bi translated">The filter bottleneck</h2><p id="b6af" class="pw-post-body-paragraph ky kz it la b lb np ju ld le nq jx lg lh nr lj lk ll ns ln lo lp nt lr ls lt im bi translated">The following code uses BigQuery Storage API to fetch a table by applying a row filter (only select some rows that match a certain condition in the table). Thus, only the rows that match the SQL expression given in the TableReadOptions constructor will be fetched. The table is of size 220GB and the estimated size of the rows that match a product_id = 1 or 2 is ~2GB.</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nu nv l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">Demo of BQ Storage fetching using an “IN” SQL statement</p></figure><p id="e434" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">Now when looking at the network usage of the compute engine, we figured out that when fetching this chunk of size 2GB from the table of 220GB, the bandwidth peak is only at <strong class="la iu"> 2MB/s </strong>的多个表。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nw"><img src="../Images/e1e4b124d26358aaf8644951e4972331.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Xd9nOCMMmaAfaxG3yccNrA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">获取整个表时，带宽峰值约为8MB/s</p></figure><p id="548e" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">但在取22GB的小表时，网络峰值<strong class="la iu"> 130MB/s </strong>。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nw"><img src="../Images/73eafa724cad2bb04721e94cba5bc570.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sKLQRAQLPR8kkNqL-i2e5Q.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">提取小块表时，带宽峰值约为130MB/s</p></figure><p id="2ba2" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这是由于<a class="ae nj" href="https://cloud.google.com/bigquery/quotas#storage-limits" rel="noopener ugc nofollow" target="_blank">对BigQuery存储读取API </a>的限制。更准确地说，这是因为我们使用了一个过滤器(SQL中的“IN”子句)，而API将这个过滤器的大小限制为10MB，因此对于大表来说性能很差。</p><blockquote class="lu lv lw"><p id="3adf" class="ky kz lx la b lb lc ju ld le lf jx lg ly li lj lk lz lm ln lo ma lq lr ls lt im bi translated">结论:当表很小(~≤4GB)时，使用这个API获取带有行过滤器的表要好得多。</p></blockquote><p id="37cc" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu">多个表中使用行过滤器提取75万行的时间基准:</strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nx"><img src="../Images/e45a6ee09a083dbfcb7e8bad9fba8455.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qcLYkQdxBYBrAe2nqLeEoA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">在不同大小的表中获取75万行的时间</p></figure><p id="2d68" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这就是为什么我建议绕过这一限制的解决方案是将原始表分成可以更快获取的更小的表。使用这种方法，将表分成更小的表允许您使用相同的时间(在前面的例子中是20秒)获取所选的行。</p><h2 id="28ad" class="mb mc it bd md me mf dn mg mh mi dp mj lh mk ml mm ll mn mo mp lp mq mr ms mt bi translated">bqfetch简介</h2><p id="f76d" class="pw-post-body-paragraph ky kz it la b lb np ju ld le nq jx lg lh nr lj lk ll ns ln lo lp nt lr ls lt im bi translated">希望我已经用以前的技术创建了一个工具，我正在我的新项目中使用它。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ny"><img src="../Images/9c280b923d1fd0e4f5c2aa1694d946ed.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BjipePN5gdE8Vx5oUVCUuA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><blockquote class="lu lv lw"><p id="097c" class="ky kz lx la b lb lc ju ld le lf jx lg ly li lj lk lz lm ln lo ma lq lr ls lt im bi translated">我真的鼓励你访问Github页面查看文档。</p></blockquote><p id="78ff" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">该工具背后的思想与我们之前描述的相同，但有一些特殊之处:</p><ul class=""><li id="617b" class="mv mw it la b lb lc le lf lh mx ll my lp mz lt na nb nc nd bi translated">从整个表中，选择一个具有最不同值的<code class="fe nk nl nm nn b">index column</code>(我们可以很容易地将一列分成许多大小大致相同的块)。一个好的列可以是ID列，也可以是包含每个不同值的大约。相同的行数。</li><li id="25e8" class="mv mw it la b lb ne le nf lh ng ll nh lp ni lt na nb nc nd bi translated">然后执行一个<code class="fe nk nl nm nn b">SELECT DISTINCT index_column</code>来获得一个。表中元素个数的估计。</li><li id="40ea" class="mv mw it la b lb ne le nf lh ng ll nh lp ni lt na nb nc nd bi translated">计算整个表的大小(以GB为单位),并应用一个公式来计算我们需要使用机器上的可用内存来划分表的块的数量。所有这些复杂的事情都是在后台完成的，你只需要指定一个index_column来将这个表按照这个列的块进行划分。</li><li id="5130" class="mv mw it la b lb ne le nf lh ng ll nh lp ni lt na nb nc nd bi translated">现在我们有了块的数量，算法返回一个块列表，每个块包含index_column的多个值。</li><li id="244e" class="mv mw it la b lb ne le nf lh ng ll nh lp ni lt na nb nc nd bi translated">通过循环这些块，我们只需要获取每个块。在每次读取之后，块被划分为在BigQuery中创建的最佳数量的表，这些表位于表使用的同一数据集中。然后，使用BigQuery存储API(有或没有多重处理)获取表。最后，删除以前创建的表。</li><li id="b58f" class="mv mw it la b lb ne le nf lh ng ll nh lp ni lt na nb nc nd bi translated">使用这种技术，<strong class="la iu">内存将得到最佳利用</strong>，如果您指定正确的块大小，内存将不会溢出。</li></ul><h2 id="c344" class="mb mc it bd md me mf dn mg mh mi dp mj lh mk ml mm ll mn mo mp lp mq mr ms mt bi translated">演示</h2><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nu nv l"/></div></figure><ul class=""><li id="446f" class="mv mw it la b lb lc le lf lh mx ll my lp mz lt na nb nc nd bi translated">首先，我们必须创建一个<code class="fe nk nl nm nn b">BigQueryTable</code>对象，它包含存储在GCP的BigQuery表的路径。</li><li id="cc7d" class="mv mw it la b lb ne le nf lh ng ll nh lp ni lt na nb nc nd bi translated">创建一个获取器，在参数中给定service_account.json文件的绝对路径，该文件是必需的，以便在GCP中执行操作。</li><li id="f5af" class="mv mw it la b lb ne le nf lh ng ll nh lp ni lt na nb nc nd bi translated">给定<code class="fe nk nl nm nn b">column</code>名称和<code class="fe nk nl nm nn b">chunk size</code>，将整个表分成块。在这种情况下，选择“id”列是完美的，因为该列的每个值都出现了相同的次数:1次。关于块的大小，如果<code class="fe nk nl nm nn b">by_chunk_size</code> =2，那么在机器上获取的每个块的大小都是2GB。因此，它必须适合内存。您需要节省1/3的内存，因为DataFrame对象的大小比原始获取的数据大，正如我们前面看到的。</li><li id="9a4f" class="mv mw it la b lb ne le nf lh ng ll nh lp ni lt na nb nc nd bi translated">对于每个块，获取它。<code class="fe nk nl nm nn b">nb_cores</code> =-1将使用机器上可用的内核数量。<code class="fe nk nl nm nn b">parallel_backend</code> = '台球' | 'joblib' | '多重处理'指定要使用的后端框架。如果<code class="fe nk nl nm nn b">by_chunk_size</code> =2，那么返回的数据帧大小将是2，66 (2+2*(1/3))(这在很大程度上取决于表的模式，所以请在此之前自行测试)。</li></ul><h2 id="a0ac" class="mb mc it bd md me mf dn mg mh mi dp mj lh mk ml mm ll mn mo mp lp mq mr ms mt bi translated">多重处理</h2><p id="1a05" class="pw-post-body-paragraph ky kz it la b lb np ju ld le nq jx lg lh nr lj lk ll ns ln lo lp nt lr ls lt im bi translated">是的，这个工具使用台球、joblib和多处理后端实现了多处理。您只需设置<code class="fe nk nl nm nn b">nb_cores</code>，获取将使用<code class="fe nk nl nm nn b">nb_cores</code>进程启动。每个区块将根据进程数量进行划分，每个进程将运行BQ存储读取会话，并应用包含每个区块中的值的行过滤器。</p><blockquote class="lu lv lw"><p id="f08c" class="ky kz lx la b lb lc ju ld le lf jx lg ly li lj lk lz lm ln lo ma lq lr ls lt im bi translated">然而，<strong class="la iu">我强烈建议您</strong>在开始使用之前，先阅读我之前的一篇关于使用多处理的BigQuery存储基准的文章。</p></blockquote><p id="66e0" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu">总之，这里有一个使用bqfetch的多处理实现的例子。</strong></p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nu nv l"/></div></figure><h2 id="3ead" class="mb mc it bd md me mf dn mg mh mi dp mj lh mk ml mm ll mn mo mp lp mq mr ms mt bi translated">结论</h2><p id="ab5b" class="pw-post-body-paragraph ky kz it la b lb np ju ld le nq jx lg lh nr lj lk ll ns ln lo lp nt lr ls lt im bi translated">经过多次研究和基准测试，我发现使用分块表的BigQuery存储API实现是(对于我的用例)从BigQuery获取表的最有效的方式。我用我展示给你的工具将我的主要项目的获取时间除以6，我希望它能帮助其他数据工程师的工作。我对GitHub上的改进建议或贡献请求非常开放，所以如果你想谈论这些，请不要犹豫与我联系。</p></div></div>    
</body>
</html>