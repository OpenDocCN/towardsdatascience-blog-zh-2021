<html>
<head>
<title>Probabilistic interpretation of linear regression clearly explained</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">线性回归的概率解释解释清楚</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/probabilistic-interpretation-of-linear-regression-clearly-explained-d3b9ba26823b?source=collection_archive---------6-----------------------#2021-04-21">https://towardsdatascience.com/probabilistic-interpretation-of-linear-regression-clearly-explained-d3b9ba26823b?source=collection_archive---------6-----------------------#2021-04-21</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="0b45" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">最小二乘法背后的原因</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/0108bb2e43badabe7cfc6534207eec45.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*66CXuIhqc_PvPntZ-uTfZw.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><p id="59e1" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">首先简单介绍一下线性回归:</p><p id="fa15" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu">线性回归是寻找最适合给定数据集的线性模型。</strong></p><p id="7419" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">例如，在具有一个输入变量(即一个特征)的简单线性回归中，线性模型是一条具有公式<code class="fe lu lv lw lx b">y = mx + b</code>的直线，其中<code class="fe lu lv lw lx b">m</code>是斜率，<code class="fe lu lv lw lx b">b</code>是y截距。</p><p id="2b13" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu">最佳拟合的线性模型是最小化误差平方和的模型。</strong></p><p id="078a" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">如下图所示，误差是观察值和预测值之间的差异。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ly"><img src="../Images/5a2460128444edb1b8a360ea4888b3d1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FmKPAmj28IMkGhGJYCOB6A.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><p id="511f" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">附:查看这篇介绍线性回归和梯度下降的文章。</p><div class="ma mb gp gr mc md"><a rel="noopener follow" target="_blank" href="/linear-regression-and-gradient-descent-for-absolute-beginners-eef9574eadb0"><div class="me ab fo"><div class="mf ab mg cl cj mh"><h2 class="bd iu gy z fp mi fr fs mj fu fw is bi translated">绝对初学者的线性回归和梯度下降</h2><div class="mk l"><h3 class="bd b gy z fp mi fr fs mj fu fw dk translated">梯度下降的简单解释和实现</h3></div><div class="ml l"><p class="bd b dl z fp mi fr fs mj fu fw dk translated">towardsdatascience.com</p></div></div><div class="mm l"><div class="mn l mo mp mq mm mr ks md"/></div></div></a></div></div><div class="ab cl ms mt hx mu" role="separator"><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx"/></div><div class="im in io ip iq"><p id="31e1" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">因此，我们知道最佳模型是误差平方和最小的模型。但是为什么呢？为什么平方误差？为什么不是误差的绝对值？</p><p id="1c6c" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这就是概率解释的用武之地。</p><p id="cb94" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">概率解释让我们了解为什么我们最小化误差平方和。</p></div><div class="ab cl ms mt hx mu" role="separator"><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx"/></div><div class="im in io ip iq"><p id="cf9b" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在我们继续进行概率解释之前，让我们首先在一些术语上取得一致。</p><p id="da48" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">首先，我们用<code class="fe lu lv lw lx b">θ</code>来象征线性模型的参数。让我们用h(x)来表示模型。模型是x的函数，由<code class="fe lu lv lw lx b">θ</code>参数化。</p><p id="4d43" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在只有一个特征的简单线性回归中，h(x)可以写成:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mz"><img src="../Images/bc4c827a924161387f2802c9d1de1b8d.png" data-original-src="https://miro.medium.com/v2/resize:fit:684/format:webp/1*2epfrcYTfrfq-LQvAIFMMA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">简单线性回归</p></figure><p id="9a9e" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这里，θ_ 0是y轴截距，θ_ 1是斜率。</p><p id="ce21" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">有了“n”个特性，它就变成了:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi na"><img src="../Images/d15a322f935829568eb3b40993c1a6ea.png" data-original-src="https://miro.medium.com/v2/resize:fit:1120/format:webp/1*0nloUgtKXXwa5-b0J-HLAQ.png"/></div></figure><p id="72ba" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">使用线性代数技术，我们可以将所有的θ放入一个向量中，并将输入值放入另一个向量中。因此，h(x)成为这两个向量的乘积。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nb"><img src="../Images/f6b0edc691760e0f583f971f252416c6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Hodupa3FXs82L3oI1hjXMg.png"/></div></div></figure><p id="5e73" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">根据模型，第<em class="nc">个</em>训练样本的估计为:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nd"><img src="../Images/8a1dd031bac4d1c1016e3e7c6d8a965c.png" data-original-src="https://miro.medium.com/v2/resize:fit:656/format:webp/1*EcegL3WK2hUOfu04kiMvxg.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">训练样本I的模型估计</p></figure><p id="1fef" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">现在，让我们继续线性回归的概率解释，以及为什么我们使用最小二乘法。</p></div><div class="ab cl ms mt hx mu" role="separator"><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx"/></div><div class="im in io ip iq"><h1 id="9836" class="ne nf it bd ng nh ni nj nk nl nm nn no jz np ka nq kc nr kd ns kf nt kg nu nv bi translated">线性回归的概率解释</h1><p id="a59b" class="pw-post-body-paragraph ky kz it la b lb nw ju ld le nx jx lg lh ny lj lk ll nz ln lo lp oa lr ls lt im bi translated">线性回归有两个假设。</p><ol class=""><li id="191d" class="ob oc it la b lb lc le lf lh od ll oe lp of lt og oh oi oj bi translated">对于给定的x，y的观测值是预测值加上误差项。</li></ol><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ok"><img src="../Images/06f46958c7a62aaf6eb6ee3ffec468ee.png" data-original-src="https://miro.medium.com/v2/resize:fit:776/format:webp/1*WyQPCdq7np3mwpE1n5z95w.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">实际y是预测值加上误差项</p></figure><p id="6411" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这个误差项是“残差”，或观测值减去预测值。记住，在线性回归中，我们希望最小化误差平方和。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ol"><img src="../Images/1bc1b0095850eccc518064129d3d96c2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sGSkCf-aw8m06hFa7wONJA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">线性回归的目标是最小化误差平方和</p></figure><p id="c224" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">2.该误差项独立同分布(IDD)。它具有均值为0且方差为sigma平方的正态(即高斯)分布。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi om"><img src="../Images/7a40199ab0621969aebce2c1d416eaca.png" data-original-src="https://miro.medium.com/v2/resize:fit:600/format:webp/1*lz74nLFOH8eodxlmKQQqTQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">误差项正态分布，均值为0，方差为sigma平方</p></figure><p id="e771" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">因为ε(I)具有正态分布，所以ε的概率密度函数可以写成:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi on"><img src="../Images/cc1da2bc21f0175ec989d7872ecec512.png" data-original-src="https://miro.medium.com/v2/resize:fit:1208/format:webp/1*l65TlKrXjfuLDtzZhRS8Jw.png"/></div></figure><p id="45eb" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">由于ε是x和y的函数，我们可以将等式改写为:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oo"><img src="../Images/199df3195cd14f028814047f76e73cf1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tv7qsjMScRPBSetIc7uSkQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">观察第I个训练样本的概率</p></figure><p id="e040" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">请注意，如果观察值和预测值接近，则方程的指数部分接近1。如果观察值和预测值相差很远，指数部分接近0。因此，如果观察值和预测值相差甚远，概率就会降低。</p><p id="03c1" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这进一步意味着，对于由θ参数化的给定x，y具有θ转置乘以x的平均值和σ平方的方差。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi op"><img src="../Images/e60cebee72fd75d90d35dfd21fb07a67.png" data-original-src="https://miro.medium.com/v2/resize:fit:988/format:webp/1*-fnb88rHsu_PkvGHFeLS1Q.png"/></div></figure><p id="79dd" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">下面是给定x的y的直观表示:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/0108bb2e43badabe7cfc6534207eec45.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*66CXuIhqc_PvPntZ-uTfZw.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><p id="1a0d" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们有一个训练样本的概率。整个数据集的概率呢？</p><p id="d23e" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">如果我们假设样本是独立的，根据统计:</p><p id="569a" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><code class="fe lu lv lw lx b">P(A and B) = P(A) * P(B)</code>。</p><p id="94b3" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">概括一下，我们有:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oq"><img src="../Images/6844ea69495ace2796da92839bdb8bf6.png" data-original-src="https://miro.medium.com/v2/resize:fit:968/format:webp/1*FIf0012Eu-Qf29rpDBSwIQ.png"/></div></figure><p id="9fa2" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">因此，整个数据集的概率是所有单个样本的概率的乘积。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi or"><img src="../Images/9e759b7787ae5e9d57caf47bc24e2e9a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jBbfa1F8jVvsGKIDXmWZvA.png"/></div></div></figure><p id="85a9" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这个等式被称为“参数θ的可能性”。可能性越大，观察到提供给模型的数据集的概率就越高。概率越大，模型越准确。</p><p id="fb34" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">学习算法所做的就是最大化这种可能性。这被称为最大似然估计，或MLE。</p></div><div class="ab cl ms mt hx mu" role="separator"><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx"/></div><div class="im in io ip iq"><p id="41d8" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">为了使数学更简单，让我们取可能性的对数。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi os"><img src="../Images/0394020f6e259c948e3c28c4172cfe47.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KUmERjpQZzvOBZ8EEY-i-Q.png"/></div></div></figure><p id="7f0c" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">第一项没有θ，所以在估算θ时，它是不相关的。在第二项中，sigma是一个常数，所以我们可以去掉它。注意第二项中的负号。让我们去掉负号。最大化似然性等同于最小化最大似然性的负值。</p><p id="bc7e" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">进一步简化对数(可能性)，您会得到:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ot"><img src="../Images/847d3f0f6782c2862808c431a72376df.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4xVDs6d-ndUaKsNQ-K5tJg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">成本函数是对数似然的负数</p></figure><p id="08c1" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这个方程和线性回归中的代价函数完全一样，是误差平方和的1/2倍。</p><p id="0458" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">最后，最小化误差平方和等于最大化数据集的概率。</p><p id="8212" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">因此，这就是为什么我们使用平方误差进行线性回归。平方部分来自具有高斯分布的误差项。</p></div></div>    
</body>
</html>