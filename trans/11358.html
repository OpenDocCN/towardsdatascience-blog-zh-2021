<html>
<head>
<title>Confidence Intervals vs Prediction Intervals</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">置信区间与预测区间</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/confidence-intervals-vs-prediction-intervals-7b296ae58745?source=collection_archive---------3-----------------------#2021-11-08">https://towardsdatascience.com/confidence-intervals-vs-prediction-intervals-7b296ae58745?source=collection_archive---------3-----------------------#2021-11-08</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="0efd" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">混淆这两者可能代价高昂。了解它们的不同之处以及何时使用它们！</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/abcb38ed607c70732aae0d5747a63ce7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jETWnZFqvxVxktSJosFtxg.png"/></div></div></figure><p id="2079" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">置信区间和预测区间都表示统计估计的不确定性。然而，每一个都与来自不同来源的不确定性有关。有时，人们可以用相同的数量来计算这两者，这导致了在解释统计模型时的混乱和潜在的严重错误。让我们看看它们有什么不同，它们表达了什么样的不确定性，以及何时使用它们。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="ab gu cl lq"><img src="../Images/17419e92b579007a2d140db6696ebf24.png" data-original-src="https://miro.medium.com/v2/format:webp/0*V16BsbK9LOx9iSQP.png"/></div></figure><h2 id="eb36" class="lr ls it bd lt lu lv dn lw lx ly dp lz ld ma mb mc lh md me mf ll mg mh mi mj bi translated">回归模型中的不确定区间</h2><p id="271f" class="pw-post-body-paragraph ku kv it kw b kx mk ju kz la ml jx lc ld mm lf lg lh mn lj lk ll mo ln lo lp im bi translated">让我们从实际出发，用一个简单的线性回归模型来拟合<a class="ae mp" href="https://scikit-learn.org/stable/datasets/real_world.html#california-housing-dataset" rel="noopener ugc nofollow" target="_blank">加州住房数据</a>。我们将只使用前 200 条记录，并跳过第一条作为测试用例。该模型基于一个单一的预测值，即邻居的中值收入来预测房价。我们只使用了一个预测值，以便能够很容易地看到 2D 的回归线。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mq mr l"/></div></figure><p id="ad9b" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">在模型总结中，我们看到了下表。</p><pre class="kj kk kl km gt ms mt mu mv aw mw bi"><span id="7c94" class="lr ls it mt b gy mx my l mz na">===========================================================<br/>          coef  std err       t   P&gt;|t   [0.025    0.975]<br/>-----------------------------------------------------------<br/>const   0.7548    0.078   9.633   0.000   0.600   0.909<br/>MedInc  0.3813    0.021  18.160   0.000   0.340   0.423</span></pre><p id="26fc" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">邻里收入中位数的系数 MedInc 为 0.3813，其 95%的区间为 0.340 — 0.423。这是一个<strong class="kw iu">置信区间</strong>。置信区间与从多个值估计的统计量有关，在这种情况下是回归系数。它表达了<strong class="kw iu">采样不确定性，</strong>这是因为我们的数据只是我们试图建模的人口的随机样本。它可以解释如下:如果我们收集了许多其他关于加州房屋的数据集，并为每个数据集拟合了这样一个模型，在 95%的情况下，<em class="nb">真实人口系数</em>(如果我们有加州所有<em class="nb">房屋的数据，我们就会知道这个系数)将落在置信区间内。</em></p><blockquote class="nc"><p id="c548" class="nd ne it bd nf ng nh ni nj nk nl lp dk translated">置信区间属于从多个值估计的统计量。它表示抽样不确定性。</p></blockquote><p id="0582" class="pw-post-body-paragraph ku kv it kw b kx nm ju kz la nn jx lc ld no lf lg lh np lj lk ll nq ln lo lp im bi translated">现在，让我们使用该模型对我们在培训中忽略的第一个观察结果进行预测。代替<code class="fe nr ns nt mt b">predict()</code>方法，我们将使用<code class="fe nr ns nt mt b">get_predict()</code>结合<code class="fe nr ns nt mt b">summary_frame()</code>来提取更多关于预测的信息。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mq mr l"/></div></figure><p id="02ad" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">我们得到以下数据帧:</p><pre class="kj kk kl km gt ms mt mu mv aw mw bi"><span id="b6bc" class="lr ls it mt b gy mx my l mz na">  mean mean_se mean_ci_lower mean_ci_upper obs_ci_lower obs_ci_upper<br/>3.9295  0.1174      3.697902      4.161218     2.711407     5.147713</span></pre><p id="79cf" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">这个房子的预测值是 3.9295。现在，<code class="fe nr ns nt mt b">mean_ci</code>列包含本次预测的置信区间的下限和上限，而<code class="fe nr ns nt mt b">obs_ci</code>列包含本次预测的<strong class="kw iu">预测区间</strong>的下限和上限。</p><p id="2684" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">您可以立即看到预测区间比置信区间宽得多。我们可以通过使用该模型来预测一系列不同邻里收入的房价，从而很好地可视化它，这样我们就可以看到回归线和预测值周围的区间。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mq mr l"/></div></figure><p id="d1f7" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">现在，<code class="fe nr ns nt mt b">pred</code>就像以前一样，只有 500 行，包含 0 到 15 之间 500 个不同收入值的预测和区间界限。我们现在可以用它来绘制回归线及其周围的区间。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nu"><img src="../Images/0b5adbb5debca993b8798c28bcc53271.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-Fh68q4h0RX_3spiLffBCQ.png"/></div></div><p class="nv nw gj gh gi nx ny bd b be z dk translated">周围有间隔的回归线。图片由作者提供。</p></figure><p id="406e" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">这里有两个主要的东西要看。首先，收入中值在 2 到 5 之间时，置信区间较窄，在更极端的值时，置信区间较宽。这是因为，对于数据中的大多数记录，收入在 2 到 5 之间。在这种情况下，模型有更多的数据，因此抽样不确定性较小。</p><p id="5af4" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">第二，预测区间比置信区间宽得多。这是因为表达了更多的不确定性。除了采样不确定性之外，预测区间还表示特定数据点的固有不确定性。</p><blockquote class="nc"><p id="b1fb" class="nd ne it bd nf ng nh ni nj nk nl lp dk translated">预测区间表达了采样不确定性之上的特定数据点的固有不确定性。因此它比置信区间更宽。</p></blockquote><p id="940a" class="pw-post-body-paragraph ku kv it kw b kx nm ju kz la nn jx lc ld no lf lg lh np lj lk ll nq ln lo lp im bi translated">这种数据点级别的不确定性来自于这样一个事实，即在同一个邻域中可能有多个价值不同的房屋，因此在模型中具有相同的预测值。这在这个例子中很明显，但在其他情况下也是如此。非常相似或者甚至完全相同的多个特征向量与不同的目标值相关联。</p><p id="c7f8" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">让我们回顾一下:</p><ul class=""><li id="05c8" class="nz oa it kw b kx ky la lb ld ob lh oc ll od lp oe of og oh bi translated">置信区间表示从许多数据点估计的数量的抽样不确定性。数据越多，采样不确定性越小，因此间隔越小。</li><li id="01ab" class="nz oa it kw b kx oi la oj ld ok lh ol ll om lp oe of og oh bi translated">除了采样不确定性之外，预测区间也表示单个值周围的不确定性，这使得它们比置信区间更宽。</li></ul><p id="c3fb" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">但是这些区间从何而来，它们又是如何包含这些不同的不确定性来源的呢？接下来我们就来看看吧！</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="ab gu cl lq"><img src="../Images/17419e92b579007a2d140db6696ebf24.png" data-original-src="https://miro.medium.com/v2/format:webp/0*V16BsbK9LOx9iSQP.png"/></div></figure><h2 id="dba1" class="lr ls it bd lt lu lv dn lw lx ly dp lz ld ma mb mc lh md me mf ll mg mh mi mj bi translated">音程从何而来</h2><p id="bd10" class="pw-post-body-paragraph ku kv it kw b kx mk ju kz la ml jx lc ld mm lf lg lh mn lj lk ll mo ln lo lp im bi translated">在传统的统计学中，人们会将预测 y-hat 周围的区间计算为</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi on"><img src="../Images/925d6b2c17bf3c963a58fefbc27799da.png" data-original-src="https://miro.medium.com/v2/resize:fit:1100/format:webp/1*DHH-IvGKsvjIgBDvvyUa6w.png"/></div></figure><p id="e20f" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">其中 t-crit 是 t 分布的临界值，SE 是预测的标准误差。对于置信区间和预测区间，右侧的两个数字将是不同的，并且是基于各种假设计算的。</p><p id="089d" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">然而，统计学中参数假设的时代幸运地即将结束。最近计算能力的提高允许使用<a class="ae mp" rel="noopener" target="_blank" href="/statistics-is-dead-long-live-statistics-df6c71262187">简单的、一刀切的重采样方法</a>来进行统计。因此，与其用推导和公式来烦你，不如让我向你展示如何通过重采样来构造这两种类型的区间。这种方法不仅适用于线性回归，而且基本上适用于你能想到的任何机器学习模型。此外，它将使人们瞬间清楚哪种不确定性被哪个区间所覆盖。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="ab gu cl lq"><img src="../Images/17419e92b579007a2d140db6696ebf24.png" data-original-src="https://miro.medium.com/v2/format:webp/0*V16BsbK9LOx9iSQP.png"/></div></figure><h2 id="f060" class="lr ls it bd lt lu lv dn lw lx ly dp lz ld ma mb mc lh md me mf ll mg mh mi mj bi translated">拔靴带</h2><p id="90b5" class="pw-post-body-paragraph ku kv it kw b kx mk ju kz la ml jx lc ld mm lf lg lh mn lj lk ll mo ln lo lp im bi translated">我们将使用的重采样技术是<strong class="kw iu">自举</strong>。归结起来就是从原始数据中抽取许多样本，比如说 10 000 个样本，然后替换掉。这些被称为引导样本，因为我们是用替换来绘制的，相同的观察结果可能在一个引导样本中出现多次。这样做的目的是从一个假设的总体中获得许多样本，以便我们可以观察抽样的不确定性。接下来，我们对每个 bootstrap 样本分别执行我们想要的任何分析或建模，并计算感兴趣的量，例如模型参数或单个预测。一旦我们有了这个量的 10 000 个自举值，我们就可以查看百分位数来得到区间。整个过程如下图所示。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oo"><img src="../Images/d079493e779f6fb1c7ae5c0419064139.png" data-original-src="https://miro.medium.com/v2/resize:fit:1244/format:webp/1*HZYQIyl0p8QNhgofWAYDzQ.png"/></div><p class="nv nw gj gh gi nx ny bd b be z dk translated">改编自作者在 DataCamp 教授的 R 课程中的<a class="ae mp" href="http://datacamp.com/courses/handling-missing-data-with-imputations-in-r" rel="noopener ugc nofollow" target="_blank">用插补处理缺失数据。</a></p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="ab gu cl lq"><img src="../Images/17419e92b579007a2d140db6696ebf24.png" data-original-src="https://miro.medium.com/v2/format:webp/0*V16BsbK9LOx9iSQP.png"/></div></figure><h2 id="8b3b" class="lr ls it bd lt lu lv dn lw lx ly dp lz ld ma mb mc lh md me mf ll mg mh mi mj bi translated">自举置信区间</h2><p id="a9de" class="pw-post-body-paragraph ku kv it kw b kx mk ju kz la ml jx lc ld mm lf lg lh mn lj lk ll mo ln lo lp im bi translated">让我们用 bootstrap 置信区间来预测一所房子的价值，该房子位于中值收入为 3 英镑的社区。我们采用 10 000 个 bootstrap 样本，对每个样本拟合一个回归模型，并预测 MedInc 等于 3。这样，我们得到了 10 000 个预测。我们可以打印它们的平均值，以及表示置信区间下限和上限的百分位数。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mq mr l"/></div></figure><pre class="kj kk kl km gt ms mt mu mv aw mw bi"><span id="49b8" class="lr ls it mt b gy mx my l mz na">Mean pred: 1.9019164610645232<br/>95% CI: [1.83355697 1.97350956]</span></pre><p id="b3fb" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">这个 bootstrap 样本考虑了抽样的不确定性，所以我们得到的区间是一个置信区间。现在让我们看看如何引导一个预测区间。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="ab gu cl lq"><img src="../Images/17419e92b579007a2d140db6696ebf24.png" data-original-src="https://miro.medium.com/v2/format:webp/0*V16BsbK9LOx9iSQP.png"/></div></figure><h2 id="55c9" class="lr ls it bd lt lu lv dn lw lx ly dp lz ld ma mb mc lh md me mf ll mg mh mi mj bi translated">引导预测区间</h2><p id="1e28" class="pw-post-body-paragraph ku kv it kw b kx mk ju kz la ml jx lc ld mm lf lg lh mn lj lk ll mo ln lo lp im bi translated">除了采样不确定性之外，预测间隔还应该考虑特定预测数据点的不确定性。为此，我们需要对代码做一个小小的改动。一旦我们从模型中获得预测，我们也从模型中提取随机残差，并将其添加到该预测中。通过这种方式，我们可以将个体预测的不确定性包含在 bootstrap 输出中。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mq mr l"/></div></figure><pre class="kj kk kl km gt ms mt mu mv aw mw bi"><span id="1ade" class="lr ls it mt b gy mx my l mz na">Mean pred: 1.9014631013163406<br/>95% PI: [1.07444778 2.72920388]</span></pre><p id="2344" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">正如所料，预测区间明显比置信区间宽，即使平均预测是相同的。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="ab gu cl lq"><img src="../Images/17419e92b579007a2d140db6696ebf24.png" data-original-src="https://miro.medium.com/v2/format:webp/0*V16BsbK9LOx9iSQP.png"/></div></figure><p id="b53e" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">感谢阅读！</p><p id="287c" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">如果你喜欢这篇文章，为什么不在我的新文章上<a class="ae mp" href="https://michaloleszak.medium.com/subscribe" rel="noopener"> <strong class="kw iu">订阅电子邮件更新</strong> </a>？通过<a class="ae mp" href="https://michaloleszak.medium.com/membership" rel="noopener"> <strong class="kw iu">成为媒介会员</strong> </a>，你可以支持我的写作，并无限制地访问其他作者和我自己的所有故事。</p><p id="a237" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">需要咨询？你可以问我任何事情，也可以在这里 预定我 1:1 <a class="ae mp" href="http://hiretheauthor.com/michal" rel="noopener ugc nofollow" target="_blank"> <strong class="kw iu">。</strong></a></p><p id="4aa4" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">你也可以试试我的其他文章。不能选择？从这些中选择一个:</p><div class="op oq gp gr or os"><a rel="noopener follow" target="_blank" href="/establishing-causality-part-1-49cb9230884c"><div class="ot ab fo"><div class="ou ab ov cl cj ow"><h2 class="bd iu gy z fp ox fr fs oy fu fw is bi translated">建立因果关系:第 1 部分</h2><div class="oz l"><h3 class="bd b gy z fp ox fr fs oy fu fw dk translated">随机实验的黄金标准</h3></div><div class="pa l"><p class="bd b dl z fp ox fr fs oy fu fw dk translated">towardsdatascience.com</p></div></div><div class="pb l"><div class="pc l pd pe pf pb pg ks os"/></div></div></a></div><div class="op oq gp gr or os"><a rel="noopener follow" target="_blank" href="/8-tips-for-object-oriented-programming-in-python-3e98b767ae79"><div class="ot ab fo"><div class="ou ab ov cl cj ow"><h2 class="bd iu gy z fp ox fr fs oy fu fw is bi translated">Python 中面向对象编程的 8 个技巧</h2><div class="oz l"><h3 class="bd b gy z fp ox fr fs oy fu fw dk translated">按照以下步骤让您的 Python 类防弹</h3></div><div class="pa l"><p class="bd b dl z fp ox fr fs oy fu fw dk translated">towardsdatascience.com</p></div></div><div class="pb l"><div class="ph l pd pe pf pb pg ks os"/></div></div></a></div><div class="op oq gp gr or os"><a rel="noopener follow" target="_blank" href="/6-useful-probability-distributions-with-applications-to-data-science-problems-2c0bee7cef28"><div class="ot ab fo"><div class="ou ab ov cl cj ow"><h2 class="bd iu gy z fp ox fr fs oy fu fw is bi translated">6 有用的概率分布及其在数据科学问题中的应用</h2><div class="oz l"><h3 class="bd b gy z fp ox fr fs oy fu fw dk translated">带有示例和 Python 代码的实用概述。</h3></div><div class="pa l"><p class="bd b dl z fp ox fr fs oy fu fw dk translated">towardsdatascience.com</p></div></div><div class="pb l"><div class="pi l pd pe pf pb pg ks os"/></div></div></a></div></div></div>    
</body>
</html>