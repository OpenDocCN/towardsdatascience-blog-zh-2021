<html>
<head>
<title>Generalized Linear Models (GLM)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">广义线性模型(GLM)</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/scikit-learns-generalized-linear-models-4899695445fa?source=collection_archive---------5-----------------------#2021-10-01">https://towardsdatascience.com/scikit-learns-generalized-linear-models-4899695445fa?source=collection_archive---------5-----------------------#2021-10-01</a></blockquote><div><div class="fc ii ij ik il im"/><div class="in io ip iq ir"><div class=""/><div class=""><h2 id="5dd5" class="pw-subtitle-paragraph jr it iu bd b js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki dk translated">掌握他们的理论和Scikit-Learn的实现</h2></div><p id="19de" class="pw-post-body-paragraph kj kk iu kl b km kn jv ko kp kq jy kr ks kt ku kv kw kx ky kz la lb lc ld le in bi translated">幸运的是，编写<em class="lf">“错误修复和稳定性改进”</em>的懒惰习惯还没有出现在软件库的发布说明中。如果没有检查这些笔记，我不会意识到<em class="lf">Scikit-Lean</em>0.23版实现了广义线性模型(GLM)。</p><p id="906c" class="pw-post-body-paragraph kj kk iu kl b km kn jv ko kp kq jy kr ks kt ku kv kw kx ky kz la lb lc ld le in bi translated">我格外关注Scikit-Learn。不仅因为我一直在使用它，而且在出版了我的书<a class="ae lg" href="https://amzn.to/3lIGcRC" rel="noopener ugc nofollow" target="_blank"> <em class="lf">之后，我想跟踪该库新实现的算法和特性，并把它们作为我的书的伪附录写在这里。</em></a></p><p id="6577" class="pw-post-body-paragraph kj kk iu kl b km kn jv ko kp kq jy kr ks kt ku kv kw kx ky kz la lb lc ld le in bi translated">顾名思义，广义线性模型是我们最喜欢的线性回归算法的扩展。我确信你们都非常了解线性回归背后的理论，所以我将在下一节讨论理解GLMs所需的细节。</p><h1 id="d47d" class="lh li iu bd lj lk ll lm ln lo lp lq lr ka ls kb lt kd lu ke lv kg lw kh lx ly bi translated">线性回归实际预测什么？</h1><p id="ab81" class="pw-post-body-paragraph kj kk iu kl b km lz jv ko kp ma jy kr ks mb ku kv kw mc ky kz la md lc ld le in bi translated">像所有其他工程学科一样，机器学习是建立在抽象层之上的。我们使用为我们抽象一些细节的库。甚至我们对潜在数学的理解也可以被抽象化，所以我们忽略了大部分细节，直到我们需要它们。</p><figure class="mf mg mh mi gu mj gi gj paragraph-image"><div role="button" tabindex="0" class="mk ml di mm bf mn"><div class="gi gj me"><img src="../Images/6ac1289698dd06120c7185f5e194efe4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0W3S-i64Lv8Fm2C_7Mii-g.png"/></div></div><p class="mq mr gk gi gj ms mt bd b be z dk translated">线性回归—实际方程</p></figure><p id="17de" class="pw-post-body-paragraph kj kk iu kl b km kn jv ko kp kq jy kr ks kt ku kv kw kx ky kz la lb lc ld le in bi translated">例如，你肯定记得上面的等式，但我们很少注意最后一项，即代表正态分布噪声或误差的ε。从概念上讲，我们知道预测线很少穿过任何实际目标y。这个术语是为了说明实际目标(y)和预测目标(y-hat)之间的差异。很公平！但是，如果我们模型的输出是y-hat，而不是y，无论如何，我们为什么不在这里花点时间来理解有和没有误差项的方程代表什么？</p><figure class="mf mg mh mi gu mj gi gj paragraph-image"><div role="button" tabindex="0" class="mk ml di mm bf mn"><div class="gi gj mu"><img src="../Images/7930ba09ab722d62e9430731e1e16887.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vYPP98x9yd2x49t6IGRcVQ.png"/></div></div><p class="mq mr gk gi gj ms mt bd b be z dk translated">线性回归—期望值</p></figure><p id="d7c9" class="pw-post-body-paragraph kj kk iu kl b km kn jv ko kp kq jy kr ks kt ku kv kw kx ky kz la lb lc ld le in bi translated">给定x，<em class="lf"> E(y|x) </em>，从模型中得到的值就是y的平均值或期望值。假设说，如果我们有大量的数据点，那么对于x的每个值，将有多个y值。然后，模型将预测每个x的y的期望值。并且该模型期望剩余的y是正态分布的。这就是为什么线性模型假设误差部分和实际目标是正态分布的。</p><blockquote class="mv mw mx"><p id="de74" class="kj kk lf kl b km kn jv ko kp kq jy kr my kt ku kv mz kx ky kz na lb lc ld le in bi translated"><strong class="kl iv"> <em class="iu">注:</em> </strong> <em class="iu">对于给定的x，E(y|x)为常数。因此，当我们说误差正态分布均值为零时，我们暗示实际目标y也是正态分布，其均值为E(y|x)。这很重要，因为在后面的例子中，我们可能不需要误差项，我们需要关注y的分布</em></p></blockquote><p id="2ad9" class="pw-post-body-paragraph kj kk iu kl b km kn jv ko kp kq jy kr ks kt ku kv kw kx ky kz la lb lc ld le in bi translated">我们可以使用Scikit-Learn来展示E(y|x)在实践中是如何工作的。在这里，我们为每个x值创建多个y。让我们看看模型拟合后会预测什么。</p><pre class="mf mg mh mi gu nb nc nd ne aw nf bi"><span id="84c9" class="ng li iu nc b gz nh ni l nj nk">from sklearn.linear_model import LinearRegression</span><span id="f4a0" class="ng li iu nc b gz nl ni l nj nk">x = [[1], [1], [2], [2]]<br/>y = [9, 11, 19, 21]</span><span id="2079" class="ng li iu nc b gz nl ni l nj nk">m = LinearRegression()<br/>m.fit(x, y)</span><span id="9634" class="ng li iu nc b gz nl ni l nj nk">m.predict([[1]]) # Returns 10<br/>m.predict([[2]]) # Returns 20</span></pre><p id="0c39" class="pw-post-body-paragraph kj kk iu kl b km kn jv ko kp kq jy kr ks kt ku kv kw kx ky kz la lb lc ld le in bi translated">可以看到，对于x=1，预测值是10，9和11的平均值。同样，对于x=2，我们得到19和21的平均值，即20。</p><figure class="mf mg mh mi gu mj gi gj paragraph-image"><div role="button" tabindex="0" class="mk ml di mm bf mn"><div class="gi gj nm"><img src="../Images/87c0bc05bbce7d62090978d084c2ae05.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uVxo2KeDE6C6T6sQ7gIvuQ.jpeg"/></div></div><p class="mq mr gk gi gj ms mt bd b be z dk translated">线性回归的方程式—图片归功于作者</p></figure><blockquote class="mv mw mx"><p id="928f" class="kj kk lf kl b km kn jv ko kp kq jy kr my kt ku kv mz kx ky kz na lb lc ld le in bi translated"><strong class="kl iv">注:</strong>这里使用的模型是通过最小化均方误差(MSE)来拟合的。当通过最小化平均绝对误差(MAE)来拟合模型时，产生的y帽将成为中间值而不是平均值。</p></blockquote><p id="23f5" class="pw-post-body-paragraph kj kk iu kl b km kn jv ko kp kq jy kr ks kt ku kv kw kx ky kz la lb lc ld le in bi translated">到目前为止，一切顺利。但是为什么我们需要更进一步，创建一个这种线性模型的一般化形式呢？</p><h1 id="a80b" class="lh li iu bd lj lk ll lm ln lo lp lq lr ka ls kb lt kd lu ke lv kg lw kh lx ly bi translated">为什么是广义线性模型？</h1><p id="a462" class="pw-post-body-paragraph kj kk iu kl b km lz jv ko kp ma jy kr ks mb ku kv kw mc ky kz la md lc ld le in bi translated">线性模型的预测都是直线，咄！或超平面，当在多维设置中使用时。这使得他们擅长推断。这是更复杂的算法所不擅长的，比如梯度推进和随机森林。</p><p id="4064" class="pw-post-body-paragraph kj kk iu kl b km kn jv ko kp kq jy kr ks kt ku kv kw kx ky kz la lb lc ld le in bi translated">请参见下文，了解线性回归是如何按照预期进行外推的:</p><pre class="mf mg mh mi gu nb nc nd ne aw nf bi"><span id="5c30" class="ng li iu nc b gz nh ni l nj nk">from sklearn.linear_model import LinearRegression</span><span id="1490" class="ng li iu nc b gz nl ni l nj nk">x = [[1], [2], [3]]<br/>y = [10, 20, 30]</span><span id="ec4d" class="ng li iu nc b gz nl ni l nj nk">m = LinearRegression()<br/>m.fit(x, y)</span><span id="56a9" class="ng li iu nc b gz nl ni l nj nk">m.predict([[10]]) # Returns 100, as expected</span></pre><p id="9f5a" class="pw-post-body-paragraph kj kk iu kl b km kn jv ko kp kq jy kr ks kt ku kv kw kx ky kz la lb lc ld le in bi translated">而基于树的模型无法推断:</p><pre class="mf mg mh mi gu nb nc nd ne aw nf bi"><span id="d5b1" class="ng li iu nc b gz nh ni l nj nk">from sklearn.ensemble import GradientBoostingRegressor</span><span id="a2a1" class="ng li iu nc b gz nl ni l nj nk">x = [[1], [2], [3]]<br/>y = [10, 20, 30]</span><span id="e88e" class="ng li iu nc b gz nl ni l nj nk">m = GradientBoostingRegressor()<br/>m.fit(x, y)</span><span id="8b86" class="ng li iu nc b gz nl ni l nj nk">m.predict([[10]]) # Returns 30! </span></pre><p id="ba22" class="pw-post-body-paragraph kj kk iu kl b km kn jv ko kp kq jy kr ks kt ku kv kw kx ky kz la lb lc ld le in bi translated">然而，推断有时是愚蠢的。举个例子:一件商品被卖出的几率取决于它在网站首页的位置。比方说，我们在主页上有20个位置来列出项目。我们根据一件商品所处的位置来计算它的销售频率。位于第7位的商品，有50%的销售机会。位置8的商品，40%的时间卖出，位置9的商品，30%的时间卖出。现在，我们使用这些信息来估计在位置1和20出售商品的机会。</p><figure class="mf mg mh mi gu mj gi gj paragraph-image"><div role="button" tabindex="0" class="mk ml di mm bf mn"><div class="gi gj nm"><img src="../Images/61e69ef1237efbeaa6e3f3f9efedd157.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zjAbh13wl8GXr_9eRVE5fg.jpeg"/></div></div><p class="mq mr gk gi gj ms mt bd b be z dk translated">线性回归的推断可能是愚蠢的——图片归功于作者</p></figure><pre class="mf mg mh mi gu nb nc nd ne aw nf bi"><span id="92b2" class="ng li iu nc b gz nh ni l nj nk">from sklearn.linear_model import LinearRegression</span><span id="9977" class="ng li iu nc b gz nl ni l nj nk">x = [[7],[8],[9]]<br/>y = [0.50, 0.40, 0.30]</span><span id="8094" class="ng li iu nc b gz nl ni l nj nk">m = LinearRegression()<br/>m.fit(x, y)</span><span id="5931" class="ng li iu nc b gz nl ni l nj nk">m.predict([[1], [20]]) # Returns 110% and -80%</span></pre><p id="7eaa" class="pw-post-body-paragraph kj kk iu kl b km kn jv ko kp kq jy kr ks kt ku kv kw kx ky kz la lb lc ld le in bi translated">如你所见，我们得到了不切实际的预测。一个位置导致110%的项目被出售的机会，而另一个位置有被出售的负机会。我们都知道概率应该在0和1之间。我们该怎么办？</p><p id="8ed6" class="pw-post-body-paragraph kj kk iu kl b km kn jv ko kp kq jy kr ks kt ku kv kw kx ky kz la lb lc ld le in bi translated">所有统计学家都有一把锤子:转换。如果您的模型不太符合数据，请使用某种转换来转换模型的输入或输出，比如将数据转换成对数标度，并希望您的模型能够工作。我们将在这里做一些非常类似的事情，但是不是转换模型的输入或输出，而是转换它的内部线性方程。</p><h1 id="4f51" class="lh li iu bd lj lk ll lm ln lo lp lq lr ka ls kb lt kd lu ke lv kg lw kh lx ly bi translated">相信我，你已经知道GLMs了</h1><p id="16b1" class="pw-post-body-paragraph kj kk iu kl b km lz jv ko kp ma jy kr ks mb ku kv kw mc ky kz la md lc ld le in bi translated">我们知道sigmoid函数取0到1之间的值，就像概率一样。</p><figure class="mf mg mh mi gu mj gi gj paragraph-image"><div role="button" tabindex="0" class="mk ml di mm bf mn"><div class="gi gj nn"><img src="../Images/18cb8263f7c85311c7792caa3d0bad4c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_AiNTYIaPV7c4eVsElxdIQ.jpeg"/></div></div><p class="mq mr gk gi gj ms mt bd b be z dk translated">Sigmoid Graph —图片署名归作者所有</p></figure><figure class="mf mg mh mi gu mj gi gj paragraph-image"><div class="gi gj no"><img src="../Images/4ad473ef67236ef22be4b122ec7a7fa7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1094/format:webp/1*frXqGp77DmBzBJAYee0EKA.png"/></div><p class="mq mr gk gi gj ms mt bd b be z dk translated">Sigmoid函数</p></figure><p id="c9ab" class="pw-post-body-paragraph kj kk iu kl b km kn jv ko kp kq jy kr ks kt ku kv kw kx ky kz la lb lc ld le in bi translated">厉害！现在，如果我们用线性方程代替z，我们可以肯定它的结果将总是保持在0和1之间</p><figure class="mf mg mh mi gu mj gi gj paragraph-image"><div role="button" tabindex="0" class="mk ml di mm bf mn"><div class="gi gj np"><img src="../Images/8aaf93971e0a248b05cca907fb8e5777.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AWL_bTqDvSgg106hYeWy9w.png"/></div></div><p class="mq mr gk gi gj ms mt bd b be z dk translated">将Sigmoid函数代入我们的线性方程</p></figure><p id="b813" class="pw-post-body-paragraph kj kk iu kl b km kn jv ko kp kq jy kr ks kt ku kv kw kx ky kz la lb lc ld le in bi translated">我很乐意将这个转换函数称为链接函数，但是统计学家的大脑通常是颠倒安装的，这就是为什么GLMs的创建者决定将其称为逆链接函数，因此，sigmoid的逆，<em class="lf"> logit </em>或<em class="lf"> log odds </em>，更像是链接函数！</p><figure class="mf mg mh mi gu mj gi gj paragraph-image"><div role="button" tabindex="0" class="mk ml di mm bf mn"><div class="gi gj nq"><img src="../Images/a663f6b76398d445aa304e4c019984be.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*f1YilpFpbiK-Z_YHSbQ2RQ.png"/></div></div><p class="mq mr gk gi gj ms mt bd b be z dk translated">Logit作为GLM(逻辑回归)链接函数</p></figure><p id="ccaa" class="pw-post-body-paragraph kj kk iu kl b km kn jv ko kp kq jy kr ks kt ku kv kw kx ky kz la lb lc ld le in bi translated">我肯定你知道这种型号的另一个名字。没错。是我们的老朋友，逻辑回归。</p><blockquote class="mv mw mx"><p id="5878" class="kj kk lf kl b km kn jv ko kp kq jy kr my kt ku kv mz kx ky kz na lb lc ld le in bi translated">一个<!-- -->陈词滥调的工作面试问题:“逻辑回归是分类还是回归算法？”。<strong class="kl iv">没有ML知识的人:</strong> <em class="iu">【回归】</em>。<strong class="kl iv">有点ML知识的人:</strong> <em class="iu">【分类】</em>。<strong class="kl iv">有更多ML知识的人:</strong> <em class="iu">【回归】… </em>这个问题就像一个哈希函数，根据人们的经验你可以知道他们的答案会是什么，但根据他们的答案你无法知道他们的经验水平。</p></blockquote><p id="eafd" class="pw-post-body-paragraph kj kk iu kl b km kn jv ko kp kq jy kr ks kt ku kv kw kx ky kz la lb lc ld le in bi translated">除了使用链接函数之外，误差项预计不再是正态分布的。事实上，逻辑回归并不是唯一的GLM，还有一堆其他的模型，我们将在下一节中访问其中的另一个。但是现在，除了我们从线性模型中借用的线性函数之外，这些是构成广义线性模型的主要组件:</p><ul class=""><li id="683d" class="nr ns iu kl b km kn kp kq ks nt kw nu la nv le nw nx ny nz bi translated">一个<strong class="kl iv">链接函数</strong>，它将E(y|x)链接到线性方程。</li><li id="6585" class="nr ns iu kl b km oa kp ob ks oc kw od la oe le nw nx ny nz bi translated"><strong class="kl iv">目标遵循指数分布</strong>。正态分布只是这个指数家族中的一员。</li></ul><p id="c4d9" class="pw-post-body-paragraph kj kk iu kl b km kn jv ko kp kq jy kr ks kt ku kv kw kx ky kz la lb lc ld le in bi translated">现在我们可以去一个不同的GLM了。其中之一是最近向Scikit-Learn介绍的<em class="lf">泊松回归</em>。它的朋友称之为费雪回归。</p><h1 id="3501" class="lh li iu bd lj lk ll lm ln lo lp lq lr ka ls kb lt kd lu ke lv kg lw kh lx ly bi translated">泊松回归</h1><p id="01eb" class="pw-post-body-paragraph kj kk iu kl b km lz jv ko kp ma jy kr ks mb ku kv kw mc ky kz la md lc ld le in bi translated">这个回归量非常适合预测计数。一平方英尺一年有多少雨滴？一个链接一天有多少点击量？一件物品在拍卖中有多少次出价？如您所见，所有这些都是在特定范围、时间、区域等内发生的事件的计数。当然，计数之外的一些其他用例也可能需要泊松回归。但关键是我们预测的是非负整数。</p><h2 id="bb2f" class="ng li iu bd lj of og dn ln oh oi dp lr ks oj ok lt kw ol om lv la on oo lx op bi translated">为什么对计数有好处？</h2><p id="d856" class="pw-post-body-paragraph kj kk iu kl b km lz jv ko kp ma jy kr ks mb ku kv kw mc ky kz la md lc ld le in bi translated">当然，维基百科上的随机贡献者说泊松回归适用于计数，但要理解为什么，我们必须检查模型是如何工作的。正如我们之前看到的，链接函数和目标的分布是我们理解算法的关键。</p><figure class="mf mg mh mi gu mj gi gj paragraph-image"><div role="button" tabindex="0" class="mk ml di mm bf mn"><div class="gi gj nn"><img src="../Images/eccc132213dd3d5715f0c8f9f7f8c9fb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*X7xN3_V2SMKQORbfFBVvwQ.jpeg"/></div></div><p class="mq mr gk gi gj ms mt bd b be z dk translated">指数图——图片署名归作者所有</p></figure><p id="a51c" class="pw-post-body-paragraph kj kk iu kl b km kn jv ko kp kq jy kr ks kt ku kv kw kx ky kz la lb lc ld le in bi translated">先说反向链接功能。如前所述，除了书呆子统计学家，没人关心实际的链接函数，它的倒数才是重要的。泊松回归模型中的反向链接是一个指数函数。正如你在上图中看到的，不管它的输入是什么，输出总是正的。因此，它在这里是有意义的，因为我们不希望肯定是负数。通过将指数代入我们的模型方程，它将变成如下:</p><figure class="mf mg mh mi gu mj gi gj paragraph-image"><div role="button" tabindex="0" class="mk ml di mm bf mn"><div class="gi gj oq"><img src="../Images/435076be8ce54e2c04039f30a25a806f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zXrgdlT8_1ADZs4mcRAlcw.png"/></div></div><p class="mq mr gk gi gj ms mt bd b be z dk translated">作为GLM(泊松)链接函数的指数</p></figure><p id="2243" class="pw-post-body-paragraph kj kk iu kl b km kn jv ko kp kq jy kr ks kt ku kv kw kx ky kz la lb lc ld le in bi translated">接下来是发行。让我们先了解一下与线性模型假设的正态分布相关的问题。然后我们可以讨论泊松回归中使用的分布。</p><p id="c168" class="pw-post-body-paragraph kj kk iu kl b km kn jv ko kp kq jy kr ks kt ku kv kw kx ky kz la lb lc ld le in bi translated">正态分布是围绕其平均值对称的。也就是说，如果E(y|x)是2，实际目标同样可能是<code class="fe or os ot nc b">2+5=7</code>和<code class="fe or os ot nc b">2–5=-3</code>。这在这里是不可接受的，因为我们不希望出现负数。相反，我们需要一个偏态分布。</p><p id="ac68" class="pw-post-body-paragraph kj kk iu kl b km kn jv ko kp kq jy kr ks kt ku kv kw kx ky kz la lb lc ld le in bi translated">另一个问题是线性模型的误差方差在所有x上是恒定的。这就是众所周知的同质性。想想看，如果某人的财富估计在100美元左右，我可以容忍它实际上是104美元或92美元，但我们几乎不能容忍它实际上是10万美元的错误。另一方面，如果某人的财富预计为10，000，000，我们对几十万甚至更多范围内的较大方差有更多的容忍度。这就是为什么我们需要一个方差变化的模型，双关语。</p><p id="630f" class="pw-post-body-paragraph kj kk iu kl b km kn jv ko kp kq jy kr ks kt ku kv kw kx ky kz la lb lc ld le in bi translated">泊松分布勾选了这里的所有方框。它是偏态的，它的方差和它的均值是一样的，也就是说方差随E(y|x)线性增长。这就是这里使用泊松分布的原因，也是这个模型的名字。</p><p id="d49d" class="pw-post-body-paragraph kj kk iu kl b km kn jv ko kp kq jy kr ks kt ku kv kw kx ky kz la lb lc ld le in bi translated">到目前为止，我一直在讲，没有给你看任何代码。所以，让我们用一些代码来结束这篇文章，这样skimmers就可以复制和粘贴它，然后就可以收工了。在这里，我创建了一个合成数据，其中目标y随x呈指数增长。目标具有非常数方差，并且我确保y的值都不是负值。</p><pre class="mf mg mh mi gu nb nc nd ne aw nf bi"><span id="bb18" class="ng li iu nc b gz nh ni l nj nk">exp = lambda x: np.exp(x)[0]</span><span id="2b66" class="ng li iu nc b gz nl ni l nj nk">x = np.array([[i] for i in np.random.choice(range(1,10), 100)])<br/>y = np.array([exp(i) + np.random.normal(0,0.1*exp(i),1)[0] for i in x])<br/>y[y&lt;0] = 0</span></pre><p id="6ae3" class="pw-post-body-paragraph kj kk iu kl b km kn jv ko kp kq jy kr ks kt ku kv kw kx ky kz la lb lc ld le in bi translated">然后我们拟合泊松模型如下</p><pre class="mf mg mh mi gu nb nc nd ne aw nf bi"><span id="c277" class="ng li iu nc b gz nh ni l nj nk">from sklearn.linear_model import PoissonRegressor</span><span id="670e" class="ng li iu nc b gz nl ni l nj nk">pr = PoissonRegressor(alpha=0, fit_intercept=False)<br/>y_pred_pr = pr.fit(x, y).predict(x)</span></pre><p id="00eb" class="pw-post-body-paragraph kj kk iu kl b km kn jv ko kp kq jy kr ks kt ku kv kw kx ky kz la lb lc ld le in bi translated">瞧，这个模型比老式的线性模型更符合数据。</p><figure class="mf mg mh mi gu mj gi gj paragraph-image"><div role="button" tabindex="0" class="mk ml di mm bf mn"><div class="gi gj ou"><img src="../Images/6b74b7e33c8e4786d3b076229d89ea21.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NpUC-dd1JWCl4OdMAdlyag.png"/></div></div><p class="mq mr gk gi gj ms mt bd b be z dk translated">将泊松回归与线性回归进行比较—图片来源于作者</p></figure><p id="ead4" class="pw-post-body-paragraph kj kk iu kl b km kn jv ko kp kq jy kr ks kt ku kv kw kx ky kz la lb lc ld le in bi translated">当然，我作弊了，为我的模型量身定做了数据，在这里展示了一个美好的结局，但我们都喜欢美好的结局，不是吗？</p><h1 id="9244" class="lh li iu bd lj lk ll lm ln lo lp lq lr ka ls kb lt kd lu ke lv kg lw kh lx ly bi translated">结论</h1><p id="e490" class="pw-post-body-paragraph kj kk iu kl b km lz jv ko kp ma jy kr ks mb ku kv kw mc ky kz la md lc ld le in bi translated">广义线性模型扩展了传统的普通最小二乘线性回归，增加了一个连接函数，并假设目标具有不同的分布，只要这些分布属于指数分布族。</p><p id="bb29" class="pw-post-body-paragraph kj kk iu kl b km kn jv ko kp kq jy kr ks kt ku kv kw kx ky kz la lb lc ld le in bi translated">插入链接函数允许模型将其目标限制在0和1之间(在逻辑回归的情况下)，大于0(在泊松回归的情况下)，或任何其他取决于所用链接的限制。除了这里讨论的，还有更多GLM氏症。例如，伽玛和逆高斯。</p><p id="ce48" class="pw-post-body-paragraph kj kk iu kl b km kn jv ko kp kq jy kr ks kt ku kv kw kx ky kz la lb lc ld le in bi translated">我现在没有谈论正则化，但是Scikit-Learn的GLMs实现允许在你有许多预测器的情况下，x。如果你不知道什么是正规化，我可能会建议你查看一下我的书里的解释。<em class="lf">没有压力！</em></p><p id="d320" class="pw-post-body-paragraph kj kk iu kl b km kn jv ko kp kq jy kr ks kt ku kv kw kx ky kz la lb lc ld le in bi translated">最后，线性模型擅长外推，不像基于树的模型。然而，它们没有能力捕捉特征交互或它们的非线性。我可能会写一篇后续文章，介绍一些可以用来减轻这些限制的技巧。</p><p id="7013" class="pw-post-body-paragraph kj kk iu kl b km kn jv ko kp kq jy kr ks kt ku kv kw kx ky kz la lb lc ld le in bi translated"><strong class="kl iv">同一作者的更多故事:</strong></p><div class="ov ow gq gs ox oy"><a href="https://medium.com/codex/a-gentle-introduction-to-confidence-intervals-b9ab64b8a663" rel="noopener follow" target="_blank"><div class="oz ab fp"><div class="pa ab pb cl cj pc"><h2 class="bd iv gz z fq pd fs ft pe fv fx it bi translated">置信区间的简明介绍</h2><div class="pf l"><h3 class="bd b gz z fq pd fs ft pe fv fx dk translated">在面试我的雇主的数据分析师职位的申请人时，我震惊地发现许多…</h3></div><div class="pg l"><p class="bd b dl z fq pd fs ft pe fv fx dk translated">medium.com</p></div></div><div class="ph l"><div class="pi l pj pk pl ph pm mo oy"/></div></div></a></div><div class="ov ow gq gs ox oy"><a href="https://medium.com/codex/pytests-assert-is-not-what-you-think-it-is-ea59dfcb4bfd" rel="noopener follow" target="_blank"><div class="oz ab fp"><div class="pa ab pb cl cj pc"><h2 class="bd iv gz z fq pd fs ft pe fv fx it bi translated">Pytest的断言不是你所想的那样</h2><div class="pf l"><h3 class="bd b gz z fq pd fs ft pe fv fx dk translated">AST是什么？pytest如何破解它，给你一个更好的UX？</h3></div><div class="pg l"><p class="bd b dl z fq pd fs ft pe fv fx dk translated">medium.com</p></div></div><div class="ph l"><div class="pn l pj pk pl ph pm mo oy"/></div></div></a></div><blockquote class="mv mw mx"><p id="0a7d" class="kj kk lf kl b km kn jv ko kp kq jy kr my kt ku kv mz kx ky kz na lb lc ld le in bi translated"><strong class="kl iv">最后说明:</strong>所有图表均由作者创作，所有亚马逊链接均为附属链接。而且所有的模型都是错的，但有些是有用的。</p></blockquote></div></div>    
</body>
</html>