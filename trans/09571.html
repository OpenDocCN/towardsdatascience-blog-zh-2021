<html>
<head>
<title>News classification: fine-tuning RoBERTa on TPUs with TensorFlow</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">新闻分类:用TensorFlow在TPUs上微调RoBERTa</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/news-category-classification-fine-tuning-roberta-on-tpus-with-tensorflow-f057c37b093?source=collection_archive---------6-----------------------#2021-09-06">https://towardsdatascience.com/news-category-classification-fine-tuning-roberta-on-tpus-with-tensorflow-f057c37b093?source=collection_archive---------6-----------------------#2021-09-06</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="b8a0" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">使用拥抱脸变形器的多类文本分类教程。</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/77557a0a3cc9494d83a92ca7fe4523fe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*UV7xRM6wa__WTAhv"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><a class="ae kv" href="https://unsplash.com/@freegraphictoday?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">绝对视觉</a>在<a class="ae kv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="abc3" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在下游任务上微调大型预训练模型是自然语言处理中的常见做法。在本教程中，我们将使用预训练的RoBERTa模型来完成多类分类任务。</p><p id="7df7" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><a class="ae kv" href="https://arxiv.org/abs/1907.11692" rel="noopener ugc nofollow" target="_blank"> RoBERTa:由脸书AI开发的稳健优化的BERT预训练方法</a>，通过修改关键超参数和在更大的语料库上进行预训练，对流行的BERT模型进行了改进。与普通的BERT相比，这提高了性能。</p><p id="a63c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">Hugging Face的<a class="ae kv" href="https://huggingface.co/transformers" rel="noopener ugc nofollow" target="_blank"> transformers </a>库允许使用几行代码轻松部署各种NLP任务的预训练模型。有各种各样的Auto Model类，它们包装了预先训练好的模型，自动实现了常见下游任务所需的必要架构更改。此外，这些模型可以被转换为Keras模型，从而可以通过Keras API进行简单的训练。</p><p id="280e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在本教程中，我们将对托管在Kaggle上的<a class="ae kv" href="https://www.kaggle.com/rmisra/news-category-dataset" rel="noopener ugc nofollow" target="_blank">新闻类别数据集</a>上的RoBERTa进行微调，根据它们的标题和简短描述来预测新闻的类别。该数据集包含从2012年到2018年获得的20万条新闻标题。</p><p id="c234" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">以下公开<a class="ae kv" href="https://colab.research.google.com/drive/1DxJZwZZvt1BiDZMRV2Ce_LXRfBE_sDVK?usp=sharing" rel="noopener ugc nofollow" target="_blank"> Colab笔记本</a>中有完整代码。</p></div><div class="ab cl ls lt hu lu" role="separator"><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx"/></div><div class="ij ik il im in"><h2 id="1964" class="lz ma iq bd mb mc md dn me mf mg dp mh lf mi mj mk lj ml mm mn ln mo mp mq mr bi translated">0.安装和导入依赖项</h2><p id="f32f" class="pw-post-body-paragraph kw kx iq ky b kz ms jr lb lc mt ju le lf mu lh li lj mv ll lm ln mw lp lq lr ij bi translated">我们首先要安装变形金刚库，这可以通过<code class="fe mx my mz na b">pip install transformers</code>轻松完成。</p><p id="1d59" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">接下来，我们导入本教程剩余部分所需的库。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nb nc l"/></div></figure><h2 id="da90" class="lz ma iq bd mb mc md dn me mf mg dp mh lf mi mj mk lj ml mm mn ln mo mp mq mr bi translated">1.实例化TPU</h2><p id="85d9" class="pw-post-body-paragraph kw kx iq ky b kz ms jr lb lc mt ju le lf mu lh li lj mv ll lm ln mw lp lq lr ij bi translated">该模型已经使用Colab free TPUs进行了训练。TPUs将允许我们更快地训练我们的模型，并且还允许我们使用更大的批量。要在Colab上启用TPU，请点击“编辑”-&gt;“笔记本设置”，然后在“硬件加速器”字段中选择“TPU”。为了实例化TPU以便与TensorFlow一起使用，我们需要运行以下代码</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nb nc l"/></div></figure><p id="746e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">为了充分利用TPU的潜力，我们设置了一个批量大小，它是集群中TPU数量的倍数。然后我们只需要在<code class="fe mx my mz na b">tpu_strategy.scope()</code>下实例化我们的模型。</p><h2 id="eb0f" class="lz ma iq bd mb mc md dn me mf mg dp mh lf mi mj mk lj ml mm mn ln mo mp mq mr bi translated">2.数据探索</h2><p id="3bd0" class="pw-post-body-paragraph kw kx iq ky b kz ms jr lb lc mt ju le lf mu lh li lj mv ll lm ln mw lp lq lr ij bi translated">让我们加载数据。我们将把标题和描述连接成一个单一的输入文本，稍后我们将把它发送到我们的网络。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nb nc l"/></div></figure><p id="dca0" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">新闻标题被分为41类，让我们形象化地看看它们是如何分布的。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nd"><img src="../Images/a602bc6c422181814e4418bbf0b8a460.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aYo_hTRXrmvOM3nHreyk3w.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">类别分布</p></figure><p id="7017" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们看到我们有许多条目很少的类别。此外，一些类别可能涉及密切相关或重叠的概念。因为有大量的类别需要预测，所以让我们将引用相似概念的类别聚集起来。这将使分类任务稍微容易一些。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nb nc l"/></div></figure><p id="73b7" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">因此，我们剩下28个汇总类别，分布如下</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ne"><img src="../Images/0e7bcbe3981a8925c191902fa3b3afc9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*W1kvmW-t1-ZdJ-ddmXSQxw.png"/></div></div></figure><h2 id="06bc" class="lz ma iq bd mb mc md dn me mf mg dp mh lf mi mj mk lj ml mm mn ln mo mp mq mr bi translated">3.数据预处理</h2><p id="ad3d" class="pw-post-body-paragraph kw kx iq ky b kz ms jr lb lc mt ju le lf mu lh li lj mv ll lm ln mw lp lq lr ij bi translated">我们现在必须以Tensorflow Keras模型可以使用的方式预处理我们的数据。作为第一步，我们需要将类标签转换成索引。我们不需要一次性编码，因为我们将使用张量流稀疏分类损失。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nb nc l"/></div></figure><p id="408b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">接下来，我们需要对文本进行标记，也就是说，我们需要将我们的字符串转换成可以输入到模型中的索引列表。transformers库为我们提供了AutoTokenizer类，该类允许加载用于RoBERTa的预先训练的Tokenizer。</p><p id="9b87" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">RoBERTa使用字节级BPE标记器来执行子词标记化，即未知的罕见词被分割成词汇表中存在的常见子词。我们将在示例中看到这意味着什么。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nb nc l"/></div></figure><p id="025a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这里标志<code class="fe mx my mz na b">padding=True</code>将把句子填充到批处理中传递的最大长度。另一方面，<code class="fe mx my mz na b">truncation=True</code>会将句子截断到模型可以接受的最大令牌数(RoBERTa为512，BERT为512)。</p><p id="08cb" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">让我们想象一下文本是如何被标记的。</p><pre class="kg kh ki kj gt nf na ng nh aw ni bi"><span id="27eb" class="lz ma iq na b gy nj nk l nl nm">Input: Twitter Users Just Say No To Kellyanne Conway's Drug Abuse Cure <br/>Subword tokenization: ['Twitter', 'ĠUsers', 'ĠJust', 'ĠSay', 'ĠNo', 'ĠTo', 'ĠKell', 'yan', 'ne', 'ĠConway', "'s", 'ĠDrug', 'ĠAbuse', 'ĠCure'] <br/>Indices: [0, 22838, 16034, 1801, 9867, 440, 598, 12702, 7010, 858, 13896, 18, 8006, 23827, 32641, 2, 1, 1, 1] </span><span id="649e" class="lz ma iq na b gy nn nk l nl nm">Input: Target's Wedding Dresses Are Nicer Than You Might Think (VIDEO) <br/>Subword tokenization: ['Target', "'s", 'ĠWedding', 'ĠD', 'resses', 'ĠAre', 'ĠNic', 'er', 'ĠThan', 'ĠYou', 'ĠMight', 'ĠThink', 'Ġ(', 'VIDEO', ')'] <br/>Indices: [0, 41858, 18, 21238, 211, 13937, 3945, 13608, 254, 15446, 370, 30532, 9387, 36, 36662, 43, 2, 1, 1] </span><span id="5965" class="lz ma iq na b gy nn nk l nl nm">Input: Televisa Reinstates Fired Hosts, Is Investigating Sexual Harassment Claims <br/>Subword tokenization: ['Te', 'lev', 'isa', 'ĠRe', 'inst', 'ates', 'ĠFired', 'ĠHost', 's', ',', 'ĠIs', 'ĠInvestig', 'ating', 'ĠSexual', 'ĠHar', 'assment', 'ĠClaims'] <br/>Indices: [0, 16215, 9525, 6619, 1223, 16063, 1626, 41969, 10664, 29, 6, 1534, 34850, 1295, 18600, 2482, 34145, 28128, 2]</span></pre><p id="99bb" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">出现在子词标记化中的字符<code class="fe mx my mz na b">Ġ</code>表示一个新词的开始，缺少它的标记只是一个更大的词被拆分的一部分。RoBERTa tokenizer使用0作为句子标记的开头，1作为填充标记，2作为句子标记的结尾。</p><p id="2d1a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">作为数据预处理的最后一步，我们从数据中创建一个TensorFlow数据集，并使用前10%的数据进行验证。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nb nc l"/></div></figure><h2 id="9f40" class="lz ma iq bd mb mc md dn me mf mg dp mh lf mi mj mk lj ml mm mn ln mo mp mq mr bi translated">3.加载模型和培训</h2><p id="e69c" class="pw-post-body-paragraph kw kx iq ky b kz ms jr lb lc mt ju le lf mu lh li lj mv ll lm ln mw lp lq lr ij bi translated">既然我们已经预处理了数据，我们需要实例化模型。我们将使用拥抱脸TensorFlow自动类进行序列分类。使用方法<code class="fe mx my mz na b">from_pretrained</code>，设置<code class="fe mx my mz na b">num_labels</code>等于我们数据集中的类的数量，这个类将为我们处理所有的脏工作。它将下载预先训练好的RoBERTa权重，并实例化一个顶部带有分类头的Keras模型。因此，我们可以使用所有常用的Keras方法，如<code class="fe mx my mz na b">compile</code>、<code class="fe mx my mz na b">fit</code>和<code class="fe mx my mz na b">save_weights</code>。我们用小的学习速率<code class="fe mx my mz na b">1e-5</code>和<code class="fe mx my mz na b">clipnorm=1.</code>对我们的模型进行6个时期的微调，以限制可能破坏预训练期间学习的特征的潜在大梯度。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nb nc l"/></div></figure><p id="0a89" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们看到验证损失很快饱和，而培训损失继续降低。事实上，该模型非常强大，如果训练时间更长，就会开始过度拟合。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi no"><img src="../Images/80ed2ab3e857b1d9182b3926ed138c08.png" data-original-src="https://miro.medium.com/v2/resize:fit:772/format:webp/1*g7smmW90bYil8Uhk47BsPw.png"/></div></figure><h2 id="c917" class="lz ma iq bd mb mc md dn me mf mg dp mh lf mi mj mk lj ml mm mn ln mo mp mq mr bi translated">4.估价</h2><p id="a6aa" class="pw-post-body-paragraph kw kx iq ky b kz ms jr lb lc mt ju le lf mu lh li lj mv ll lm ln mw lp lq lr ij bi translated">在总共28个类别的验证集上，该模型达到了大约77%的前1准确率和大约93%的前3准确率。</p><p id="4ca9" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">让我们在验证集上可视化混淆矩阵</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi np"><img src="../Images/bbd942fac96b8c0d5051e605b742eb26.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NDBjDejlfgb7RAUZTLD42g.png"/></div></div></figure><p id="92fd" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">让我们计算模型的(加权)精度、召回率和f1指标。对于这些指标的快速概述，你可以看看漂亮的帖子<a class="ae kv" rel="noopener" target="_blank" href="/multi-class-metrics-made-simple-part-i-precision-and-recall-9250280bddc2">多类指标变得简单，第一部分:精度和召回</a>和<a class="ae kv" rel="noopener" target="_blank" href="/multi-class-metrics-made-simple-part-ii-the-f1-score-ebe8b2c2ca1">多类指标变得简单，第二部分:F1分数</a>。</p><pre class="kg kh ki kj gt nf na ng nh aw ni bi"><span id="4e46" class="lz ma iq na b gy nj nk l nl nm">Precision:0.769<br/>Recall:0.775<br/>F1 score:0.769</span></pre><p id="4f97" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">让我们根据验证集中的一些示例的概率来可视化前3个预测。每个预测的概率用圆括号表示。</p><pre class="kg kh ki kj gt nf na ng nh aw ni bi"><span id="17fa" class="lz ma iq na b gy nj nk l nl nm">HEADLINE: Homemade Gift Ideas: Tart-Cherry and Dark Chocolate Bar Wrappers <br/>SHORT DESCRIPTION: This DIY gift only LOOKS professional. <br/>TRUE LABEL: HOME &amp; LIVING <br/>Prediction 1:HOME &amp; LIVING (77.5%); <br/>Prediction 2:FOOD, DRINK &amp; TASTE (19.8%); <br/>Prediction 3:STYLE &amp; BEAUTY (0.7%);   </span><span id="5e23" class="lz ma iq na b gy nn nk l nl nm">HEADLINE: Small Parties Claim Their Share In Upcoming Greek Elections <br/>SHORT DESCRIPTION: Some of the country's lesser-known political players believe they've spotted their chance. <br/>TRUE LABEL: WORLD NEWS <br/>Prediction 1:WORLD NEWS (99.2%); <br/>Prediction 2:POLITICS (0.4%); <br/>Prediction 3:ENVIRONMENT &amp; GREEN (0.1%);   </span><span id="9921" class="lz ma iq na b gy nn nk l nl nm">HEADLINE: 46 Tons Of Beads Found In New Orleans' Storm Drains <br/>SHORT DESCRIPTION: The Big Easy is also the big messy. <br/>TRUE LABEL: WEIRD NEWS <br/>Prediction 1:WEIRD NEWS (55.0%); <br/>Prediction 2:ENVIRONMENT &amp; GREEN (14.4%); <br/>Prediction 3:SCIENCE &amp; TECH (10.0%);</span></pre></div><div class="ab cl ls lt hu lu" role="separator"><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx"/></div><div class="ij ik il im in"><h2 id="927c" class="lz ma iq bd mb mc md dn me mf mg dp mh lf mi mj mk lj ml mm mn ln mo mp mq mr bi translated">结论</h2><p id="8e25" class="pw-post-body-paragraph kw kx iq ky b kz ms jr lb lc mt ju le lf mu lh li lj mv ll lm ln mw lp lq lr ij bi translated">我们已经看到了如何将拥抱脸变形器库与TensorFlow一起使用，以在多类分类任务中使用TPU来微调大型预训练模型。</p><p id="2e5c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">由于transformers库中有大量的Auto类，其他常见的NLP下游任务只需对提供的代码稍加修改就可以执行。</p><p id="525d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我希望这个教程是有用的，感谢阅读！</p></div></div>    
</body>
</html>