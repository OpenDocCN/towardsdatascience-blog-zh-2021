<html>
<head>
<title>Weekly review of Reinforcement Learning papers #10</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">强化学习试卷#10的每周回顾</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/weekly-review-of-reinforcement-learning-papers-10-be5947715b26?source=collection_archive---------48-----------------------#2021-05-25">https://towardsdatascience.com/weekly-review-of-reinforcement-learning-papers-10-be5947715b26?source=collection_archive---------48-----------------------#2021-05-25</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="8360" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">每周一，我都会发表我研究领域的4篇论文。大家来讨论一下吧！</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/c18e95be03310571a97bba430218e958.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*qGMBMz8puo-Xjsjp.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><p id="1695" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">[ <a class="ae lu" rel="noopener" target="_blank" href="/weekly-review-of-reinforcement-learning-papers-9-d0f7eff9eae2?sk=cdaaf56e9d7bf2a52f9fdc8e6af0dc30"> ←上一次回顾</a> ][ <a class="ae lu" href="https://qgallouedec.medium.com/weekly-review-of-reinforcement-learning-papers-11-7e1780ddf176?sk=18185b32a63640b45cc486270c267584" rel="noopener">下一次回顾→ </a></p><h1 id="2193" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">论文1:通用策略的分布条件强化学习</h1><p id="1c11" class="pw-post-body-paragraph ky kz it la b lb mn ju ld le mo jx lg lh mp lj lk ll mq ln lo lp mr lr ls lt im bi translated">Nasiriany，s .，Pong，V. H .，Nair，a .，Khazatsky，a .，Berseth，g .，和Levine，S. (2021年)。<a class="ae lu" href="https://arxiv.org/abs/2104.11707" rel="noopener ugc nofollow" target="_blank"> Disco rl:通用政策的分布条件强化学习</a>。<em class="ms"> arXiv预印本arXiv:2104.11707 </em>。</p><p id="5eda" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi mt translated">以目标为条件的强化学习包括将一个目标整合到政策论证中。例如，考虑一个机器人手臂和一个放在桌子上的立方体；任务是控制机械臂将立方体移动到期望的位置。如果目标位置不变，你知道的RL方法可以解决这个问题。但是如果我现在想把立方体移动到一个新的位置呢？我是否必须为多维数据集的每个目标位置学习一个策略？通过使用以目标为条件的策略，可以学习任何目标位置的移动任务。该策略将州和目标州作为参数。</p><p id="913e" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">以目标为条件的策略可以允许一些一般化，但是不能捕获所有可能需要的任务。举个例子，同样的例子，如果你在桌子上加了一个盒子，你想让手臂把立方体移动到盒子里，完成这个任务有很多状态:立方体可以在盒子的中心，在盒子的边缘，或者其他任何地方，只要它在盒子里，任务就完成了。</p><p id="8bee" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在本文中，作者建议用状态分布来代替目标。首先，模型必须使用对应于成功任务的状态的例子来学习奖励分配。一旦模型能够识别对应于任务成功的状态，就可以学习策略。他们在这篇论文中提出了他们的算法，适用于对奖励分布的学习，他们称之为分布条件强化学习(DisCoRL)。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nc"><img src="../Images/cc0cd0f29d52a36b903d3b73b3f66e5c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dq-4SrP3zPpn9k2XvHIR2w.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">首先，一个模型被训练来推断一个目标分布，一个策略被训练来完成期望的任务。</p></figure><p id="adb7" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这种方法的巨大优势在于，它将以目标为条件的RL方法扩展到了以前被忽略的各种各样的任务。</p><h1 id="ce02" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">论文2:拼车强化学习:综述</h1><p id="baa5" class="pw-post-body-paragraph ky kz it la b lb mn ju ld le mo jx lg lh mp lj lk ll mq ln lo lp mr lr ls lt im bi translated">秦，朱海红，叶，等(2021)。<a class="ae lu" href="https://arxiv.org/abs/2105.01099" rel="noopener ugc nofollow" target="_blank">拼车强化学习:综述</a>。<em class="ms"> arXiv预印本arXiv:2105.01099 </em>。</p><p id="f4f6" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi mt translated">本文是一篇关于拼车强化学习方法的文献综述。我们这里处理的是专业拼车，比如优步提供的服务。从某种意义上来说，司机提供了一次他本应该去的旅行(去度假或去工作场所)，这不是拼车。</p><p id="9f35" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">拼车其实集合了几个话题。(1)匹配，包括根据顾客的要求和驾驶员的位置将顾客和驾驶员联系起来；(2)重新定位，包括将空闲的驾驶员(不在驾驶中)移向一个位置，使其与未来顾客的距离最大化，或使顾客的平均等待时间最小化；(3)驱动器的动态定价。</p><p id="b8e1" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">对于这些主题中的每一个，作者探索了几个作品，讨论了每个获得的假设和结果。有趣的是，所有的工作都是首先(如果不是唯一)在模拟中完成的。因此，有必要解决所使用的模拟工具的真实性问题，以及允许从模拟到真实世界的方法。</p><p id="fa01" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">另一方面，这是他们的结论性评论，大多数作品都是最近写的。这是一个非常新的领域，还有许多方向有待探索。</p><h1 id="8541" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">论文3:通过乐观自举和逆向归纳进行原则性探索</h1><p id="0691" class="pw-post-body-paragraph ky kz it la b lb mn ju ld le mo jx lg lh mp lj lk ll mq ln lo lp mr lr ls lt im bi translated">(2021)白超群，王，李，王，郑，韩，郝建杰，刘。<a class="ae lu" href="https://arxiv.org/abs/2105.06022" rel="noopener ugc nofollow" target="_blank">乐观自举和逆向归纳的原则性探索</a>。<em class="ms"> arXiv预印本arXiv:2105.06022 </em>。</p><p id="d7c6" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi mt translated"><span class="l mu mv mw bm mx my mz na nb di"> U </span> pper置信界限(<a class="ae lu" rel="noopener" target="_blank" href="/the-upper-confidence-bound-ucb-bandit-algorithm-c05c2bf4c13f"> UCB </a>)是一种强化学习方法，其特殊性在于为很少访问的州提供探索奖励。从另一个角度来看:一个代理人对一个它从未尝试过的行动的结果持乐观态度。</p><p id="a605" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这种算法在表格情况下非常容易实现(每个状态-动作对都与一个值相关联，该值可以存储为一个表)。然而，当我们想要使用神经网络来估计该值时，该算法很难实现:不可能单独选择与状态-动作对相关联的输出。网络的输出值取决于网络的权重集，因此权重的每次更新都会对输出集产生影响。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nd"><img src="../Images/22cadd31e4dde91db3d785823b2c8d5a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1368/format:webp/1*utIIUtqfShZXymvSbUK_yA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">乐观值函数是通过对估计的Q函数和奖金B求和而获得的</p></figure><p id="10df" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在本文中，作者提出了乐观自举和逆向归纳(OB2I)，一种适应UCB探索奖金思想的深度RL算法。以前的工作允许UCB的这种适应，但仅限于线性情况。在这个版本的算法中，状态-动作对的不确定性被估计。如果这种不确定性高，奖金就高，如果不确定性低，奖金就低。然后，这个额外奖励被添加到状态-动作的值中。因此，可以将这种方法与所有基于值的算法相结合。他们通过超越其他最先进的勘探策略来验证其方法的效率。</p><h1 id="79f4" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">论文4:具有通用策略更新的快速强化学习</h1><p id="c534" class="pw-post-body-paragraph ky kz it la b lb mn ju ld le mo jx lg lh mp lj lk ll mq ln lo lp mr lr ls lt im bi translated">Barreto，a .，Hou，s .，Borsa，d .，Silver，d .，&amp; pre COPD，D. (2020年)。<a class="ae lu" href="https://www.pnas.org/content/117/48/30079" rel="noopener ugc nofollow" target="_blank">具有通用策略更新的快速强化学习</a>。<em class="ms">美国国家科学院院刊</em>，<em class="ms"> 117 </em> (48)，30079–30087。</p><p id="df9b" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi mt translated">了解一个令人满意的政策所需的交互次数通常非常多。这是强化学习与深度学习共有的限制。本文作者采用了分而治之的方法。这到底是什么意思？</p><p id="4210" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">一个复杂的任务通常由几个任务的连续或并行实现组成。通过将一项任务分成更简单的子任务，可以更有效地学习。但这就引出了一个问题:如何评价一个政策必须在几个子任务上表现良好？为了理解这一点，让我们再次从策略迭代算法开始。我们从任何政策开始，然后我们评估这个政策以获得它的状态-动作值函数(政策评估)。一旦我们有了这个价值函数，我们就创建了一个新的政策，为每个州选择产生最大回报的行动(政策改进)。然后我们重新开始。</p><p id="c376" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">政策迭代的保证之一是，在每一步，政策必然会变得更好。在本文中，作者将策略迭代扩展到几个奖励信号必须最大化的情况。它们引入了一般政策评估(GPE)和一般政策改进(GPI)。</p><p id="6586" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">让我们用几句话来解释GPI，即如何从它的价值函数出发，建立一个新的政策，比以前的政策更好。这个想法是评估几个政策。一旦我们有了每个政策的价值函数，我们就可以检查各个状态:对于每个状态，我们选择在所有政策中给出最高回报的行动。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ne"><img src="../Images/4ab7980c7a8d6cf0291e75767ec41d53.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XhKjBmvLQhvjOyNZcZmUbg.png"/></div></div></figure><p id="d59b" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">结果是好的，从某种意义上说，该算法收敛非常快，比Q学习快几个数量级。当然我简化了很多。我邀请你阅读这篇文章来理解这种概括的所有微妙之处，以及结果是如何得到的。</p></div><div class="ab cl nf ng hx nh" role="separator"><span class="ni bw bk nj nk nl"/><span class="ni bw bk nj nk nl"/><span class="ni bw bk nj nk"/></div><div class="im in io ip iq"><p id="16f7" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我很高兴向你们展示我本周的阅读材料。请随时向我发送您的反馈。</p></div></div>    
</body>
</html>