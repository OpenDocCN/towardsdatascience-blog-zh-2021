<html>
<head>
<title>How to Train a BERT Model From Scratch</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如何从头开始训练BERT模型</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/how-to-train-a-bert-model-from-scratch-72cfce554fc6?source=collection_archive---------0-----------------------#2021-07-06">https://towardsdatascience.com/how-to-train-a-bert-model-from-scratch-72cfce554fc6?source=collection_archive---------0-----------------------#2021-07-06</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="62ad" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">见见伯特的意大利表弟，菲利波托</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/0bf52229bb8fb427347d60902bff30f0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-txg2YRJbhQDd8Vrh-dH2g.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">伯特，但在意大利——作者图片</p></figure><p id="c4f2" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi lu translated">我的所有文章都聚焦于BERT——这个模型主宰了自然语言处理(NLP)的世界，标志着语言模型的新时代。</p><p id="2540" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">对于那些以前没有使用过变形金刚模型的人来说，这个过程看起来有点像这样:</p><ul class=""><li id="ed56" class="md me it la b lb lc le lf lh mf ll mg lp mh lt mi mj mk ml bi translated"><code class="fe mm mn mo mp b">pip install transformers</code></li><li id="e035" class="md me it la b lb mq le mr lh ms ll mt lp mu lt mi mj mk ml bi translated">初始化预训练的变压器模型— <code class="fe mm mn mo mp b">from_pretrained</code>。</li><li id="854c" class="md me it la b lb mq le mr lh ms ll mt lp mu lt mi mj mk ml bi translated">用一些数据测试一下。</li><li id="610e" class="md me it la b lb mq le mr lh ms ll mt lp mu lt mi mj mk ml bi translated"><em class="mv">也许</em>对模型进行微调(再多训练一些)。</li></ul><p id="7289" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">现在，这是一个伟大的方法，但如果我们只是这样做，我们缺乏对创建我们自己的变形金刚模型背后的理解。</p><p id="5640" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">此外，如果我们无法创建自己的变压器模型，我们必须依赖适合我们问题的预训练模型，但情况并非总是如此:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mw"><img src="../Images/fa91d67a77d8507af087f7134e08eb87.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FAF9GzOtEffUvNvPcrhehA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">一些关于非英语BERT模型的评论</p></figure><p id="6fb6" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">因此，在本文中，我们将探索构建我们自己的transformer模型所必须采取的步骤——特别是BERT的进一步开发版本，称为RoBERTa。</p></div><div class="ab cl mx my hx mz" role="separator"><span class="na bw bk nb nc nd"/><span class="na bw bk nb nc nd"/><span class="na bw bk nb nc"/></div><div class="im in io ip iq"><h1 id="313d" class="ne nf it bd ng nh ni nj nk nl nm nn no jz np ka nq kc nr kd ns kf nt kg nu nv bi translated">概述</h1><p id="4bb6" class="pw-post-body-paragraph ky kz it la b lb nw ju ld le nx jx lg lh ny lj lk ll nz ln lo lp oa lr ls lt im bi translated">这个过程有几个步骤，所以在我们开始之前，让我们先总结一下我们需要做什么。总的来说，有四个关键部分:</p><ul class=""><li id="992a" class="md me it la b lb lc le lf lh mf ll mg lp mh lt mi mj mk ml bi translated">获取数据</li><li id="14cf" class="md me it la b lb mq le mr lh ms ll mt lp mu lt mi mj mk ml bi translated">构建标记器</li><li id="2f5e" class="md me it la b lb mq le mr lh ms ll mt lp mu lt mi mj mk ml bi translated">创建输入管道</li><li id="818f" class="md me it la b lb mq le mr lh ms ll mt lp mu lt mi mj mk ml bi translated">训练模型</li></ul><p id="1d29" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">一旦我们完成了每一个部分，我们将使用我们构建的记号赋予器和模型——并保存它们，这样我们就可以像使用<code class="fe mm mn mo mp b">from_pretrained</code>一样使用它们。</p></div><div class="ab cl mx my hx mz" role="separator"><span class="na bw bk nb nc nd"/><span class="na bw bk nb nc nd"/><span class="na bw bk nb nc"/></div><div class="im in io ip iq"><h1 id="e686" class="ne nf it bd ng nh ni nj nk nl nm nn no jz np ka nq kc nr kd ns kf nt kg nu nv bi translated">获取数据</h1><p id="6ff3" class="pw-post-body-paragraph ky kz it la b lb nw ju ld le nx jx lg lh ny lj lk ll nz ln lo lp oa lr ls lt im bi translated">和任何机器学习项目一样，我们需要数据。在用于训练transformer模型的数据方面，我们确实有太多的选择了——我们可以使用几乎任何文本数据。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ob oc l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">使用HuggingFace的数据集库下载OSCAR数据集的视频演练</p></figure><p id="62ba" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">而且，如果说我们在互联网上有很多东西的话，那就是非结构化文本数据。</p><p id="9c97" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">从互联网上搜集的文本领域中最大的数据集之一是OSCAR数据集。</p><p id="bbc8" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">OSCAR数据集拥有大量不同的语言，其中一个最明显的从头训练用例是，我们可以将BERT应用于一些不太常用的语言，如泰卢固语或纳瓦霍语。</p><p id="ac9d" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">不幸的是，我唯一能说的语言是英语，但我的女朋友是意大利人，所以她——劳拉，将评估我们说意大利语的伯特模型的结果——菲利贝托。</p><p id="732a" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">因此，为了下载奥斯卡数据集的意大利部分，我们将使用HuggingFace的<code class="fe mm mn mo mp b">datasets</code>库——我们可以用<code class="fe mm mn mo mp b">pip install datasets</code>安装它。然后我们下载OSCAR_IT:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="od oc l"/></div></figure><p id="a129" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们来看看<code class="fe mm mn mo mp b">dataset</code>这个物体。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="od oc l"/></div></figure><p id="ffa5" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">很好，现在让我们以一种在构建我们的标记器时可以使用的格式存储我们的数据。我们需要从我们的数据集中创建一组仅包含<code class="fe mm mn mo mp b">text</code>特性的明文文件，并且我们将使用换行符<code class="fe mm mn mo mp b">\n</code>分割每个<em class="mv">样本</em>。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="od oc l"/></div></figure><p id="5677" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在我们的<code class="fe mm mn mo mp b">data/text/oscar_it</code>目录中，我们会找到:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oe"><img src="../Images/90c31446f9b654369d989f2c6d984155.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0eZ7XulYZ0iyndeXypHD5Q.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">包含我们的明文OSCAR文件的目录</p></figure></div><div class="ab cl mx my hx mz" role="separator"><span class="na bw bk nb nc nd"/><span class="na bw bk nb nc nd"/><span class="na bw bk nb nc"/></div><div class="im in io ip iq"><h1 id="6fde" class="ne nf it bd ng nh ni nj nk nl nm nn no jz np ka nq kc nr kd ns kf nt kg nu nv bi translated">构建标记器</h1><p id="f5c4" class="pw-post-body-paragraph ky kz it la b lb nw ju ld le nx jx lg lh ny lj lk ll nz ln lo lp oa lr ls lt im bi translated">下一个是记号赋予者！当使用转换器时，我们通常加载一个记号赋予器，以及它各自的转换器模型——记号赋予器是这个过程中的一个关键组件。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ob oc l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">构建自定义令牌化器的视频演练</p></figure><p id="8038" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在构建我们的标记器时，我们将向它提供我们所有的OSCAR数据，指定我们的词汇大小(标记器中的标记数量)，以及任何特殊的标记。</p><p id="8ff2" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">现在，罗伯塔特殊令牌看起来像这样:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="od oc l"/></div></figure><p id="4df0" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">因此，我们确保将它们包含在我们的标记器的<code class="fe mm mn mo mp b">train</code>方法调用的<code class="fe mm mn mo mp b">special_tokens</code>参数中。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="od oc l"/></div></figure><p id="bf42" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们的标记器现在已经准备好了，我们可以将它保存为文件供以后使用:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="od oc l"/></div></figure><p id="ca06" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">现在我们有两个文件定义了新的<em class="mv"> FiliBERTo </em>记号赋予器:</p><ul class=""><li id="691a" class="md me it la b lb lc le lf lh mf ll mg lp mh lt mi mj mk ml bi translated"><em class="mv"> merges.txt </em> —执行文本到标记的初始映射</li><li id="dd37" class="md me it la b lb mq le mr lh ms ll mt lp mu lt mi mj mk ml bi translated">vocab.json  —将令牌映射到令牌id</li></ul><p id="a255" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">有了这些，我们可以继续初始化我们的记号赋予器，这样我们就可以像使用任何其他<code class="fe mm mn mo mp b">from_pretrained</code>记号赋予器一样使用它。</p><h2 id="0d84" class="of nf it bd ng og oh dn nk oi oj dp no lh ok ol nq ll om on ns lp oo op nu oq bi translated">初始化标记器</h2><p id="375f" class="pw-post-body-paragraph ky kz it la b lb nw ju ld le nx jx lg lh ny lj lk ll nz ln lo lp oa lr ls lt im bi translated">我们首先使用之前构建的两个文件初始化记号赋予器——使用一个简单的<code class="fe mm mn mo mp b">from_pretrained</code>:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="od oc l"/></div></figure><p id="69af" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">现在我们的记号赋予器已经准备好了，我们可以试着用它来编码一些文本。当编码时，我们使用通常使用的两种方法，<code class="fe mm mn mo mp b">encode</code>和<code class="fe mm mn mo mp b">encode_batch</code>。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="od oc l"/></div></figure><p id="661e" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">从编码对象<code class="fe mm mn mo mp b">tokens</code>中，我们将提取<code class="fe mm mn mo mp b">input_ids</code>和<code class="fe mm mn mo mp b">attention_mask</code>张量用于FiliBERTo。</p></div><div class="ab cl mx my hx mz" role="separator"><span class="na bw bk nb nc nd"/><span class="na bw bk nb nc nd"/><span class="na bw bk nb nc"/></div><div class="im in io ip iq"><h1 id="07af" class="ne nf it bd ng nh ni nj nk nl nm nn no jz np ka nq kc nr kd ns kf nt kg nu nv bi translated">创建输入管道</h1><p id="b58e" class="pw-post-body-paragraph ky kz it la b lb nw ju ld le nx jx lg lh ny lj lk ll nz ln lo lp oa lr ls lt im bi translated">我们培训流程的输入管道是整个流程中更复杂的部分。它包括我们获取原始的OSCAR训练数据，对其进行转换，并将其加载到准备好进行训练的<code class="fe mm mn mo mp b">DataLoader</code>中。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ob oc l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">MLM输入管道的视频演练</p></figure><h2 id="3c6b" class="of nf it bd ng og oh dn nk oi oj dp no lh ok ol nq ll om on ns lp oo op nu oq bi translated">准备数据</h2><p id="916c" class="pw-post-body-paragraph ky kz it la b lb nw ju ld le nx jx lg lh ny lj lk ll nz ln lo lp oa lr ls lt im bi translated">我们将从一个样本开始，逐步完成准备逻辑。</p><p id="de37" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">首先，我们需要打开我们的文件——与我们保存为<em class="mv">的文件相同。txt </em>文件在先。我们基于换行符<code class="fe mm mn mo mp b">\n</code>分割每个样本，因为这表示单个样本。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="od oc l"/></div></figure><p id="128e" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">然后，我们使用<code class="fe mm mn mo mp b">tokenizer</code>对我们的数据进行编码——确保包括关键参数，如<code class="fe mm mn mo mp b">max_length</code>、<code class="fe mm mn mo mp b">padding</code>和<code class="fe mm mn mo mp b">truncation</code>。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="od oc l"/></div></figure><p id="182d" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">现在我们可以继续创建我们的张量——我们将通过掩蔽语言建模(MLM)来训练我们的模型。所以，我们需要三个张量:</p><ul class=""><li id="8b13" class="md me it la b lb lc le lf lh mf ll mg lp mh lt mi mj mk ml bi translated"><strong class="la iu"> <em class="mv"> input_ids </em> </strong> —我们的<em class="mv"> token_ids </em>有大约15%的令牌使用屏蔽令牌<code class="fe mm mn mo mp b">&lt;mask&gt;</code>屏蔽。</li><li id="95b0" class="md me it la b lb mq le mr lh ms ll mt lp mu lt mi mj mk ml bi translated"><strong class="la iu"><em class="mv">attention _ mask</em></strong>—<strong class="la iu">1</strong>s和<strong class="la iu"> 0 </strong> s的张量，标记“真实”记号/填充记号的位置——用于注意力计算。</li><li id="940d" class="md me it la b lb mq le mr lh ms ll mt lp mu lt mi mj mk ml bi translated"><strong class="la iu"> <em class="mv">标签</em> </strong> —我们的<em class="mv"> token_ids </em>带有<strong class="la iu">号</strong>屏蔽。</li></ul><p id="f548" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">如果你不熟悉MLM，我在这里已经解释过了。</p><p id="44ac" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们的<code class="fe mm mn mo mp b">attention_mask</code>和<code class="fe mm mn mo mp b">labels</code>张量就是从我们的<code class="fe mm mn mo mp b">batch</code>中简单提取出来的。然而<code class="fe mm mn mo mp b">input_ids</code>张量需要更多的关注，对于这个张量，我们屏蔽了大约15%的记号——给它们分配记号ID <code class="fe mm mn mo mp b">3</code>。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="od oc l"/></div></figure><p id="df03" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在最终输出中，我们可以看到一个编码的<code class="fe mm mn mo mp b">input_ids</code>张量的一部分。第一个令牌ID是<code class="fe mm mn mo mp b">1</code>—<code class="fe mm mn mo mp b">[CLS]</code>令牌。围绕张量我们有几个<code class="fe mm mn mo mp b">3</code>记号id——这些是我们新添加的<code class="fe mm mn mo mp b">[MASK]</code>记号。</p><h2 id="b0da" class="of nf it bd ng og oh dn nk oi oj dp no lh ok ol nq ll om on ns lp oo op nu oq bi translated">构建数据加载器</h2><p id="f29a" class="pw-post-body-paragraph ky kz it la b lb nw ju ld le nx jx lg lh ny lj lk ll nz ln lo lp oa lr ls lt im bi translated">接下来，我们定义我们的<code class="fe mm mn mo mp b">Dataset</code>类——我们用它来初始化我们的三个编码张量作为PyTorch <code class="fe mm mn mo mp b">torch.utils.data.Dataset</code>对象。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="od oc l"/></div></figure><p id="37bf" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">最后，我们的<code class="fe mm mn mo mp b">dataset</code>被加载到PyTorch <code class="fe mm mn mo mp b">DataLoader</code>对象中——在训练期间，我们用它将数据加载到我们的模型中。</p></div><div class="ab cl mx my hx mz" role="separator"><span class="na bw bk nb nc nd"/><span class="na bw bk nb nc nd"/><span class="na bw bk nb nc"/></div><div class="im in io ip iq"><h1 id="a896" class="ne nf it bd ng nh ni nj nk nl nm nn no jz np ka nq kc nr kd ns kf nt kg nu nv bi translated">训练模型</h1><p id="6dea" class="pw-post-body-paragraph ky kz it la b lb nw ju ld le nx jx lg lh ny lj lk ll nz ln lo lp oa lr ls lt im bi translated">我们训练需要两样东西，我们的<code class="fe mm mn mo mp b">DataLoader</code>和一个模型。我们有——但没有模型。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ob oc l"/></div></figure><h2 id="699f" class="of nf it bd ng og oh dn nk oi oj dp no lh ok ol nq ll om on ns lp oo op nu oq bi translated">初始化模型</h2><p id="0e6d" class="pw-post-body-paragraph ky kz it la b lb nw ju ld le nx jx lg lh ny lj lk ll nz ln lo lp oa lr ls lt im bi translated">为了训练，我们需要一个未经训练的<code class="fe mm mn mo mp b">BERTLMHeadModel</code>。为此，我们首先需要创建一个RoBERTa配置对象来描述我们希望用来初始化FiliBERTo的参数。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="od oc l"/></div></figure><p id="6b2e" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">然后，我们用语言建模(LM)头导入并初始化我们的RoBERTa模型。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="od oc l"/></div></figure><h2 id="2556" class="of nf it bd ng og oh dn nk oi oj dp no lh ok ol nq ll om on ns lp oo op nu oq bi translated">培训准备</h2><p id="58f7" class="pw-post-body-paragraph ky kz it la b lb nw ju ld le nx jx lg lh ny lj lk ll nz ln lo lp oa lr ls lt im bi translated">在进入我们的训练循环之前，我们需要设置一些东西。首先，我们设置GPU/CPU使用率。然后我们激活模型的训练模式——最后，初始化我们的优化器。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="od oc l"/></div></figure><h2 id="4a03" class="of nf it bd ng og oh dn nk oi oj dp no lh ok ol nq ll om on ns lp oo op nu oq bi translated">培养</h2><p id="d214" class="pw-post-body-paragraph ky kz it la b lb nw ju ld le nx jx lg lh ny lj lk ll nz ln lo lp oa lr ls lt im bi translated">终于——训练时间到了！我们就像平时通过PyTorch训练一样训练。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="od oc l"/></div></figure><p id="6b79" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">如果我们去Tensorboard，我们会发现随着时间的推移我们的损失——它看起来很有希望。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi os"><img src="../Images/397a50f58398b5746e3eac9042f42233.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sxDP6Yj8c1ERyFEi96NyoQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">损失/时间—在此图表中，多个培训课程串联在一起</p></figure></div><div class="ab cl mx my hx mz" role="separator"><span class="na bw bk nb nc nd"/><span class="na bw bk nb nc nd"/><span class="na bw bk nb nc"/></div><div class="im in io ip iq"><h1 id="6f62" class="ne nf it bd ng nh ni nj nk nl nm nn no jz np ka nq kc nr kd ns kf nt kg nu nv bi translated">真正的考验</h1><p id="6088" class="pw-post-body-paragraph ky kz it la b lb nw ju ld le nx jx lg lh ny lj lk ll nz ln lo lp oa lr ls lt im bi translated">现在是真正考验的时候了。我们建立了一个MLM管道，并请劳拉评估结果。可以在这里看22:44的视频回顾:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ob oc l"/></div></figure><p id="78b2" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们首先使用<code class="fe mm mn mo mp b">'fill-mask'</code>参数初始化一个<code class="fe mm mn mo mp b">pipeline</code>对象。然后像这样开始测试我们的模型:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="od oc l"/></div></figure><p id="f04e" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><em class="mv">“ciao</em><strong class="la iu"><em class="mv">来了</em> </strong> <em class="mv"> va？”</em>正确答案！这是我的意大利语达到的最高水平——所以，让我们把它交给劳拉。</p><p id="38f1" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们从<em class="mv">开始，“早上好，来弗吉尼亚吗？”</em> —或者<em class="mv">“日安，你好吗？”</em>:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="od oc l"/></div></figure><p id="c804" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">第一个回答，“buongiorno，chi va？”意思是“日安，谁在那里？”—如无意义。但是，我们的第二个答案是正确的！</p><p id="0a6a" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">接下来，是一个稍微难一点的短语，<em class="mv">“你好，我的鸽子？”</em> —或者<em class="mv">“嗨，今天下午我们在哪里见面？”</em>:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="od oc l"/></div></figure><p id="4aa7" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们返回一些更积极的结果:</p><pre class="kj kk kl km gt ot mp ou ov aw ow bi"><span id="4273" class="of nf it mp b gy ox oy l oz pa">✅ "hi, where do we see each other this afternoon?"<br/>✅ "hi, where do we meet this afternoon?"<br/>❌ "hi, where here we are this afternoon?"<br/>✅ "hi, where are we meeting this afternoon?"<br/>✅ "hi, where do we meet this afternoon?"</span></pre><p id="d8de" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">最后，还有一个更难的句子，“你成功了吗？”或者“如果我们选择了另一天，会发生什么？”：</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="od oc l"/></div></figure><p id="0ed8" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们在这里也给出了一些更好的答案:</p><pre class="kj kk kl km gt ot mp ou ov aw ow bi"><span id="7c52" class="of nf it mp b gy ox oy l oz pa">✅ "what would have happened if we had chosen another day?"<br/>✅ "what would have happened if I had chosen another day?"<br/>✅ "what would have happened if they had chosen another day?"<br/>✅ "what would have happened if you had chosen another day?"<br/>❌ "what would have happened if another day was chosen?"</span></pre><p id="fe9e" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">总的来说，看起来我们的模型通过了Laura的测试——我们现在有了一个名为FiliBERTo的合格的意大利语言模型！</p></div><div class="ab cl mx my hx mz" role="separator"><span class="na bw bk nb nc nd"/><span class="na bw bk nb nc nd"/><span class="na bw bk nb nc"/></div><div class="im in io ip iq"><p id="9d9e" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这就是从头开始训练BERT模型的演练！</p><p id="f86d" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们已经覆盖了很多领域，从获取和格式化我们的数据，一直到使用语言建模来训练我们的原始BERT模型。</p><p id="8bfc" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我希望你喜欢这篇文章！如果你有任何问题，请通过推特或者在下面的评论中告诉我。如果你想要更多这样的内容，我也会在YouTube上发布。</p><p id="387c" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">感谢阅读！</p></div><div class="ab cl mx my hx mz" role="separator"><span class="na bw bk nb nc nd"/><span class="na bw bk nb nc nd"/><span class="na bw bk nb nc"/></div><div class="im in io ip iq"><p id="8ad6" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><a class="ae or" href="https://bit.ly/nlp-transformers" rel="noopener ugc nofollow" target="_blank">🤖《变形金刚》课程NLP的70%折扣</a></p></div><div class="ab cl mx my hx mz" role="separator"><span class="na bw bk nb nc nd"/><span class="na bw bk nb nc nd"/><span class="na bw bk nb nc"/></div><div class="im in io ip iq"><p id="9cbb" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><em class="mv">*所有图片均由作者提供，除非另有说明</em></p></div></div>    
</body>
</html>