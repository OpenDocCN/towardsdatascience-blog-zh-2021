<html>
<head>
<title>Reinforcement learning Q-learning with illegal actions from scratch</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">强化学习 Q-从零开始用非法动作学习</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/reinforcement-learning-q-learning-with-illegal-actions-from-scratch-19759146c8bf?source=collection_archive---------6-----------------------#2021-12-19">https://towardsdatascience.com/reinforcement-learning-q-learning-with-illegal-actions-from-scratch-19759146c8bf?source=collection_archive---------6-----------------------#2021-12-19</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="eadd" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">这是一个井字游戏的例子，展示了当某些动作对于特定的状态是禁止的时候，如何编写 RL Q-learning 算法</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi kf"><img src="../Images/6d6fcd982162c28c905b50632c80a7c5.png" data-original-src="https://miro.medium.com/v2/resize:fit:400/1*4q_uCCiCvY8FvdSvd2jxjQ.gif"/></div><p class="kn ko gj gh gi kp kq bd b be z dk translated"><a class="ae kr" href="https://commons.wikimedia.org/wiki/File:Tic-tac-toe-animated.gif" rel="noopener ugc nofollow" target="_blank">井字游戏动画</a></p></figure><p id="8630" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">有一天，我看着我 18 个月大的儿子学习如何用勺子吃饭，我意识到，一旦他的碗空了，他会立即放下勺子，向我要食物。他根据碗中食物的位置和数量来调整拿勺子的方式，而只要没有食物了，他自然不会做任何无用的努力。这让我想起了我遇到的一些 RL 问题:在一些特定的情况下，并不是所有的行为都是允许的:例如，当电梯已经在顶层时，它永远不能上升，当已经达到最低温度时，HVAC 系统将总是停止冷却。</p><p id="5fe6" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">在本文中，我将以井字游戏为例，构建一个基于游戏规则存在非法行为的 Q 学习算法。接下来，你将会读到:</p><ol class=""><li id="feab" class="lo lp iq ku b kv kw ky kz lb lq lf lr lj ls ln lt lu lv lw bi translated">井字游戏的 RL 环境。</li><li id="6bb1" class="lo lp iq ku b kv lx ky ly lb lz lf ma lj mb ln lt lu lv lw bi translated">带有非法行为的 Q 学习算法。</li></ol><p id="7e7f" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">所有的代码都在我的<a class="ae kr" href="https://github.com/ShuyangenFrance/tic-tac-toe/tree/main/tic-tac-toe" rel="noopener ugc nofollow" target="_blank"> Github </a>上，以防你需要更多的细节。</p><h1 id="1baa" class="mc md iq bd me mf mg mh mi mj mk ml mm jw mn jx mo jz mp ka mq kc mr kd ms mt bi translated">井字游戏环境</h1><p id="d8f2" class="pw-post-body-paragraph ks kt iq ku b kv mu jr kx ky mv ju la lb mw ld le lf mx lh li lj my ll lm ln ij bi translated">井字游戏或 Xs 和 Os 是两个玩家的游戏，他们轮流用<em class="mz"> X </em>或<em class="mz"> O </em>标记 3×3 网格中的空间。成功在水平、垂直或对角线上放置三个标记的玩家获胜。</p><p id="ed1f" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">我们将使用<a class="ae kr" href="https://gym.openai.com/" rel="noopener ugc nofollow" target="_blank">健身房</a>来设置井字游戏环境。一般来说，RL 环境有四个关键功能:初始化、重置、步进和渲染。</p><h2 id="f7ad" class="na md iq bd me nb nc dn mi nd ne dp mm lb nf ng mo lf nh ni mq lj nj nk ms nl bi translated">初始化</h2><p id="fc28" class="pw-post-body-paragraph ks kt iq ku b kv mu jr kx ky mv ju la lb mw ld le lf mx lh li lj my ll lm ln ij bi translated">初始化函数主要是对奖励、done(检查游戏是否结束的值)进行初始化，特别设置了动作空间和观察(状态)空间。在井字游戏中，两个玩家(让我们称他们为玩家 O 和玩家 X)中的每一个都可以采取 9 种可能的行动:他想要标记的网格之一。一个状态由一个 3×3 的数组表示，其中每个元素可以有三个可能的值:0 表示未被标记，1 表示已被玩家 O 标记，2 表示已被玩家 X 标记)。<strong class="ku ir">注意，并非所有状态都允许所有动作:玩家不能标记已经被占据的格子。</strong></p><pre class="kg kh ki kj gt nm nn no np aw nq bi"><span id="0679" class="na md iq nn b gy nr ns l nt nu">import gym<br/>from gym import spaces<br/>import numpy as np</span><span id="b8d2" class="na md iq nn b gy nv ns l nt nu">class TicTacToe(gym.Env):<br/>    metadata = {'render.modes': ['human']}</span><span id="e511" class="na md iq nn b gy nv ns l nt nu">def __init__(self):<br/>        super(TicTacToe, self).__init__()<br/>        self.states = np.zeros((3,3))<br/>        self.counter = 0<br/>        self.done = 0<br/>        self.reward = 0<br/>        self.action_space=spaces.Discrete(9)<br/>        self.observation_space=spaces.Box(low=0,high=2, shape=(3,3))</span></pre><h2 id="d4aa" class="na md iq bd me nb nc dn mi nd ne dp mm lb nf ng mo lf nh ni mq lj nj nk ms nl bi translated">重置</h2><p id="caa3" class="pw-post-body-paragraph ks kt iq ku b kv mu jr kx ky mv ju la lb mw ld le lf mx lh li lj my ll lm ln ij bi translated">重置功能旨在将环境设置为初始状态。在我们的示例中，我们简单地将完成和奖励值设置为零，并将状态设置为游戏板上没有任何标记。</p><pre class="kg kh ki kj gt nm nn no np aw nq bi"><span id="5370" class="na md iq nn b gy nr ns l nt nu">def reset(self):<br/>        self.states=np.zeros((3,3))<br/>        self.counter = 0<br/>        self.done = 0<br/>        self.reward = 0<br/>        return np.array(self.states)</span></pre><h2 id="79f6" class="na md iq bd me nb nc dn mi nd ne dp mm lb nf ng mo lf nh ni mq lj nj nk ms nl bi translated">步骤</h2><p id="c28a" class="pw-post-body-paragraph ks kt iq ku b kv mu jr kx ky mv ju la lb mw ld le lf mx lh li lj my ll lm ln ij bi translated">阶跃函数是动作的函数，它可能是环境中最重要的部分。它代表了环境状态将如何随着一个给定的行动而转变，以及我们将获得什么样的回报。在井字游戏中，一旦执行了一个合法的动作，就会有四种不同的状态:玩家 O 赢，玩家 X 赢，游戏继续或者没人赢但游戏结束。</p><p id="6917" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">由于篇幅限制，这部分代码被省略了，但是实现非常直接:我们只需要在每个动作之后检查游戏是否继续，如果不是，谁赢了。此外，如果玩家 O 赢了，我们设置奖励为 100，玩家 X 赢了，奖励为-100。在游戏没有结束的情况下，我们对中间步骤不给予任何奖励。请在我的 Github 上查看详细信息。</p><h2 id="ba70" class="na md iq bd me nb nc dn mi nd ne dp mm lb nf ng mo lf nh ni mq lj nj nk ms nl bi translated">提供；给予</h2><p id="ca13" class="pw-post-body-paragraph ks kt iq ku b kv mu jr kx ky mv ju la lb mw ld le lf mx lh li lj my ll lm ln ij bi translated">render 函数将环境渲染到屏幕上。例如，它打印游戏的以下报告:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nw"><img src="../Images/f239b678823a65c42be39847b68c43ec.png" data-original-src="https://miro.medium.com/v2/resize:fit:542/format:webp/1*0VzEEUmgS04bb90dHURfLQ.png"/></div><p class="kn ko gj gh gi kp kq bd b be z dk translated">作者图片</p></figure><h1 id="9560" class="mc md iq bd me mf mg mh mi mj mk ml mm jw mn jx mo jz mp ka mq kc mr kd ms mt bi translated">q-用非法行为学习</h1><p id="cbbb" class="pw-post-body-paragraph ks kt iq ku b kv mu jr kx ky mv ju la lb mw ld le lf mx lh li lj my ll lm ln ij bi translated">一旦井字游戏的环境设定好了，我们就真的要训练代理人(玩家)了。在这个例子中，我将使用最基本的 RL 算法 q-learning:在一项工作中，Q-learning 寻求学习一种使总回报最大化的策略。它是一种表格方法，创建形状[状态，动作]的 q 表，并在每次训练后更新和存储 q 函数值。训练完成后，以 q 表为参考，选择奖励最大化的动作。请注意，我们游戏中的每个状态都由一个 3 乘 3 的数组表示:我们首先需要一个函数(state_to_number)来将每个状态更改为一个整数:</p><pre class="kg kh ki kj gt nm nn no np aw nq bi"><span id="2fb4" class="na md iq nn b gy nr ns l nt nu">def __init__(self,alpha = 0.1,gamma = 0.6,epsilon = 0.1,epochs=5000):<br/>        self.alpha=alpha<br/>        self.gamma=gamma<br/>        self.epsilon=epsilon<br/>        self.epochs=epochs</span><span id="e818" class="na md iq nn b gy nv ns l nt nu">def state_to_number(self, state):<br/>        state = state.reshape(9)<br/>        number = 0<br/>        for i, num in enumerate(state):<br/>            number += num * 3 ** (len(state) - i - 1)<br/>        return int(number)</span></pre><p id="6014" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">回想一下，在我们的井字游戏环境中，q_table 不能到处填充:玩家不能标记已经被他自己或他的对手标记的网格。在这种情况下，我们简单地以下面的方式屏蔽禁止的动作:我们将表中相应的元素设置为 nan，并通过忽略相应行中的 nan 值来选择 argmax。有人建议设置一个很大的负值作为无效行为的奖励，但这不是一个好主意，因为无效行为仍有可能以很小的概率被选择。</p><pre class="kg kh ki kj gt nm nn no np aw nq bi"><span id="de5d" class="na md iq nn b gy nr ns l nt nu">def learn(self,env):<br/>        if env.env_name=="tictactoe":<br/>            self.q_table = self.q_table(env)<br/>            for i in range(self.epochs):<br/>                state = env.reset()</span><span id="f30b" class="na md iq nn b gy nv ns l nt nu">epochs, reward, = 0, 0, 0<br/>                done = False</span><span id="7b3f" class="na md iq nn b gy nv ns l nt nu">while done !=True:<br/>                    if random.uniform(0, 1) &lt; self.epsilon:<br/>                        action = env.action_space.sample()  # Explore action space<br/>                        #forbiden ilegal action<br/>                        while state[int(action / 3)][action % 3] !=0:<br/>                            action=env.action_space.sample()<br/>                    else:<br/>                        action_value_list=self.q_table[self.state_to_number(state)]<br/>                        for action,action_value in enumerate(action_value_list):<br/>                            if state[int(action / 3)][action % 3]!=0:<br/>                                action_value_list[action]=np.nan<br/>                        action = np.nanargmax(action_value_list)  # Exploit learned values<br/>                    next_state, reward, done, info = env.step(action)<br/>                    old_value = self.q_table[self.state_to_number(state), action]<br/>                    next_max = np.nanmax(self.q_table[self.state_to_number(next_state)])<br/>                    new_value = (1 - self.alpha) * old_value + self.alpha * (reward + self.gamma * next_max)<br/>                    self.q_table[self.state_to_number(state), action] = new_value<br/>                    state = next_state</span><span id="843e" class="na md iq nn b gy nv ns l nt nu">epochs += 1</span></pre><p id="6ba7" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">我们显示了训练集的奖励的累积平均值，我们可以看到奖励收敛到大约 80%，也就是说，玩家 O 有 80%的机会赢得游戏。并且当然在以后的游戏中不会采取无效动作。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/756848a30e53d6baa5f2d7362986adbe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*Ij0ZhWkJShGrpxWWFJmM9A.png"/></div><p class="kn ko gj gh gi kp kq bd b be z dk translated">作者图片</p></figure><h1 id="0aff" class="mc md iq bd me mf mg mh mi mj mk ml mm jw mn jx mo jz mp ka mq kc mr kd ms mt bi translated">结论</h1><p id="ca95" class="pw-post-body-paragraph ks kt iq ku b kv mu jr kx ky mv ju la lb mw ld le lf mx lh li lj my ll lm ln ij bi translated">在这篇文章中，我用井字游戏展示了当一些动作是非法的时候如何对待 RL，以及如何在训练中用一个从零开始构建的简单 q-learning 示例来屏蔽它们。必要的屏蔽步骤防止代理在学习中无效。</p></div></div>    
</body>
</html>