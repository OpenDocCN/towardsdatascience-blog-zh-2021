<html>
<head>
<title>Journey to BERT: Part 2</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">伯特之旅:第二部分</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/journey-to-bert-part-2-6a9a1c8dbba4?source=collection_archive---------34-----------------------#2021-07-04">https://towardsdatascience.com/journey-to-bert-part-2-6a9a1c8dbba4?source=collection_archive---------34-----------------------#2021-07-04</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="e30f" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">各种NLP概念和架构的拼贴导致了现代基于变压器的NLP模型BERT。</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi kf"><img src="../Images/baf3cc7f4d66d717383652863063e637.png" data-original-src="https://miro.medium.com/v2/resize:fit:526/format:webp/1*frXxtOXX96CgQx6xFgKvYA.png"/></div><p class="kn ko gj gh gi kp kq bd b be z dk translated">现代NLP发展简史。来源:self</p></figure><p id="e0eb" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">这是我不久前写的上一篇博客<a class="ae ln" href="https://medium.com/swlh/journey-to-bert-part-1-a89413855a10" rel="noopener">伯特之旅</a>的第二部分。在这篇博客中，我将继续叙述并解释向BERT发展的概念里程碑。</p><h2 id="5b06" class="lo lp iq bd lq lr ls dn lt lu lv dp lw la lx ly lz le ma mb mc li md me mf mg bi translated">迄今为止</h2><ol class=""><li id="b167" class="mh mi iq kt b ku mj kx mk la ml le mm li mn lm mo mp mq mr bi translated">前神经单词嵌入，如Glove、TF-IDF</li><li id="0179" class="mh mi iq kt b ku ms kx mt la mu le mv li mw lm mo mp mq mr bi translated">常见的自然语言处理任务，如分类、问答和文本摘要。</li><li id="01ba" class="mh mi iq kt b ku ms kx mt la mu le mv li mw lm mo mp mq mr bi translated">Word2Vec的神经嵌入。</li><li id="89d7" class="mh mi iq kt b ku ms kx mt la mu le mv li mw lm mo mp mq mr bi translated">使用深度序列模型(RNN)进行自然语言处理</li><li id="b530" class="mh mi iq kt b ku ms kx mt la mu le mv li mw lm mo mp mq mr bi translated">发现注意力和双向性。</li></ol><p id="3a10" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">通过下面的模型和方法，我们继续关注NLP的进一步发展。从现在开始，最受欢迎的属性是迁移学习、上下文学习和规模。</p><h2 id="288b" class="lo lp iq bd lq lr ls dn lt lu lv dp lw la lx ly lz le ma mb mc li md me mf mg bi translated">TagLM</h2><p id="10f9" class="pw-post-body-paragraph kr ks iq kt b ku mj jr kw kx mk ju kz la mx lc ld le my lg lh li mz lk ll lm ij bi translated">TagLM(语言模型增强序列标记器)可能是第一个真正尝试产生上下文单词嵌入的软件。Matthew Peters等人在2017年提出了这篇<a class="ae ln" href="https://arxiv.org/pdf/1705.00108.pdf" rel="noopener ugc nofollow" target="_blank">论文</a>，该论文展示了如何在序列标记任务中通过语言模型嵌入来增强<strong class="kt ir"> un </strong>上下文单词嵌入。语言模型嵌入基于在未标记的文本数据上预先训练的双向rnn，并且与用于序列标记的多栈双向rnn的隐藏层输出连接。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="nb nc di nd bf ne"><div class="gh gi na"><img src="../Images/e5ddc7a5fb73d13e8d1da2a858093971.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZK13T_oAnTVjmXWyPDH8dQ.png"/></div></div><p class="kn ko gj gh gi kp kq bd b be z dk translated">来源:<a class="ae ln" href="https://arxiv.org/pdf/1705.00108.pdf" rel="noopener ugc nofollow" target="_blank">原创论文</a></p></figure><p id="c309" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">TagLm在CoNNL数据集上的NER识别上取得了令人印象深刻的结果，击败了所有以前的sota方法。</p><h1 id="4518" class="nf lp iq bd lq ng nh ni lt nj nk nl lw jw nm jx lz jz nn ka mc kc no kd mf np bi translated">山凹</h1><p id="ab71" class="pw-post-body-paragraph kr ks iq kt b ku mj jr kw kx mk ju kz la mx lc ld le my lg lh li mz lk ll lm ij bi translated">Context Vector( <a class="ae ln" href="https://arxiv.org/pdf/1708.00107.pdf" rel="noopener ugc nofollow" target="_blank"> CoVe </a>)处理上下文单词向量的方法与TagML不相上下。论文由<a class="ae ln" href="https://arxiv.org/pdf/1708.00107.pdf" rel="noopener ugc nofollow" target="_blank">麦肯等人</a>撰写。2018年，其主要动机是将ImageNet的迁移学习成功应用于计算机视觉的下游任务。作者认为，在NLP领域中，ImageNet的等价物可以是基于LSTM的编码器，用于使用基于LSTM的注意序列到序列模型的神经机器翻译任务。然后，预训练的编码器可以用作上下文单词嵌入的源，该上下文单词嵌入然后可以扩充传统的单词向量系统(例如Glove)。NMT在多个机器翻译数据集上进行训练，这些数据集基本上是两种语言的句子对。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="nb nc di nd bf ne"><div class="gh gi nq"><img src="../Images/98385a2acb133a8f9f6b20ebc3e249b0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*I6gcoqcMphbCpD7FE24BCw@2x.jpeg"/></div></div><p class="kn ko gj gh gi kp kq bd b be z dk translated">来源:<a class="ae ln" href="https://arxiv.org/pdf/1708.00107.pdf" rel="noopener ugc nofollow" target="_blank">原始文件</a> r</p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/5ecf9ba1d9a943c6faad8cd53a78e188.png" data-original-src="https://miro.medium.com/v2/resize:fit:552/format:webp/1*90mJMQarY7YkrC9vvS7DdA.png"/></div><p class="kn ko gj gh gi kp kq bd b be z dk translated">资料来源:<a class="ae ln" href="https://arxiv.org/pdf/1708.00107.pdf" rel="noopener ugc nofollow" target="_blank">原始文件</a> r</p></figure></div><div class="ab cl ns nt hu nu" role="separator"><span class="nv bw bk nw nx ny"/><span class="nv bw bk nw nx ny"/><span class="nv bw bk nw nx"/></div><div class="ij ik il im in"><p id="dbfc" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">该论文建议将CoVe向量(双LSTM编码器的最后一层)与Glove向量相结合，并说明了在一些常见的nlp任务中的性能增益。然而，与Tag-LM不同，CoVe需要<strong class="kt ir">标记的数据</strong>(两种语言的文本对)来训练机器翻译任务的编码器。这是这种方法的一个明显的局限性。此外，下游任务的实际性能提升更多地取决于下游任务的架构。</p><h2 id="2842" class="lo lp iq bd lq lr ls dn lt lu lv dp lw la lx ly lz le ma mb mc li md me mf mg bi translated">工程与后勤管理局</h2><p id="358a" class="pw-post-body-paragraph kr ks iq kt b ku mj jr kw kx mk ju kz la mx lc ld le my lg lh li mz lk ll lm ij bi translated">来自语言模型的嵌入(ELMO)在某种意义上是对来自同一组的Tag-LM的细化(<a class="ae ln" href="https://arxiv.org/abs/1802.05365" rel="noopener ugc nofollow" target="_blank"> Peters et al. </a>)。作者认为，在大型语料库中以无监督方式学习的(双向)语言模型携带单词的语义和句法内涵。模型的初始层捕获语法意义(NER、词性标注)，而模型的末端层捕获语义意义(情感分析、问题回答、语义相似度等)。因此，不是仅使用最后一层(如在Tag-LM或Clove中所做的，其中网络是<strong class="kt ir">预训练的</strong>和<strong class="kt ir">冻结的</strong>)，而是采用所有层的线性组合将是对单词的上下文含义的更好和丰富的估计。ELMO代表因此被认为是“深”。</p><p id="3ace" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">在ELMO，学习语言模型的通常方式是从一个句子中的前一个单词序列双向预测下一个单词。损失函数是负对数似然。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="nb nc di nd bf ne"><div class="gh gi nz"><img src="../Images/5f772bf0919283253e534b5ae3788582.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iXt3f2oq9Hw9eiqvqRjVSQ.png"/></div></div><p class="kn ko gj gh gi kp kq bd b be z dk translated">来源:<a class="ae ln" href="https://paperswithcode.com/method/elmo" rel="noopener ugc nofollow" target="_blank">论文，代码</a></p></figure><p id="90e2" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">ELMO是对Tag-LM和Clove的改进，这可以归因于表示“深入”的事实。该论文说明了ELMO在各种常见的NLP任务上实现了递增的性能增益。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="nb nc di nd bf ne"><div class="gh gi oa"><img src="../Images/0bc854a3c0c3fd64df815f5d58658381.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*G5IJZRJEzFRd75pDdXKTmw.png"/></div></div><p class="kn ko gj gh gi kp kq bd b be z dk translated">来源:<a class="ae ln" href="https://arxiv.org/pdf/1802.05365.pdf" rel="noopener ugc nofollow" target="_blank">原创论文</a></p></figure><p id="ba59" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">然而，ELMO仍然存在更多地依赖下游任务的架构来提高性能的缺点。</p><p id="4889" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">与ELMO竞争的是另一个来自<a class="ae ln" href="https://arxiv.org/abs/1801.06146" rel="noopener ugc nofollow" target="_blank">杰瑞米·霍华德等人</a> (Fast.ai)，用于文本分类的通用语言模型微调(<strong class="kt ir"> ULMFiT </strong>)的提议。除了使用RNN进行语言建模的预训练步骤之外，ULMFiT建议在目标数据集上使用<strong class="kt ir"> LM微调</strong>。基本原理是了解目标数据集的“分布”或“特定于任务的特征”。最后一步是特定任务的微调。例如分类器(使用几个线性块)。</p><h2 id="6723" class="lo lp iq bd lq lr ls dn lt lu lv dp lw la lx ly lz le ma mb mc li md me mf mg bi translated"><strong class="ak">走向变形金刚</strong></h2><p id="d127" class="pw-post-body-paragraph kr ks iq kt b ku mj jr kw kx mk ju kz la mx lc ld le my lg lh li mz lk ll lm ij bi translated">因此，《变形金刚》的架构成为了伯特家族和GPT系列等更现代作品的“最底层”构成。Vashwani等人在2018年的论文“注意力是你所需要的一切”中首次提出，它为rnn(及其风格)提供了一种处理顺序数据的替代方法。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="nb nc di nd bf ne"><div class="gh gi ob"><img src="../Images/ccac201ded68c50a364a7458b1a736ff.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jlUxSSbxLXVqvQ9bx4tKlw.png"/></div></div><p class="kn ko gj gh gi kp kq bd b be z dk translated">来源:原始文件</p></figure><p id="35cc" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">不如参考杰·阿尔玛的这篇<a class="ae ln" href="https://jalammar.github.io/illustrated-transformer/" rel="noopener ugc nofollow" target="_blank">优秀文章</a>全面了解。简单来说，该架构具有以下重要元素。</p><ol class=""><li id="7da8" class="mh mi iq kt b ku kv kx ky la oc le od li oe lm mo mp mq mr bi translated">多头自我注意:在很高的层次上，自我注意允许参考序列中的其他单词和子空间来关联单词的意思。基本上，另一种获取(长期)依赖的方式。“多头”意味着使用多个头来关注具有多个代表性空间的序列中的多个子空间。比如使用多个大脑。从数学上来说，携带自我注意力的嵌入是使用乘以值矩阵的softmax over keys.queries来计算的。</li></ol><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi of"><img src="../Images/7f60dd5121037a2385b0e855e1263cb9.png" data-original-src="https://miro.medium.com/v2/resize:fit:772/format:webp/1*__QDkz05br3QURtWiknFUA.png"/></div><p class="kn ko gj gh gi kp kq bd b be z dk translated">来源:<a class="ae ln" href="https://arxiv.org/pdf/1802.05365.pdf" rel="noopener ugc nofollow" target="_blank">原创论文</a></p></figure><p id="ec29" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">2.<strong class="kt ir">(正弦)位置编码</strong>:有趣的是，变压器本质上并不是连续的。事实上，它观察并处理整个序列。在这种情况下，位置编码封装了标记的顺序。它实际上是使用sin和cos函数创建的嵌入的向量值。这里有一个极好的参考<a class="ae ln" href="https://kazemnejad.com/blog/transformer_architecture_positional_encoding/" rel="noopener ugc nofollow" target="_blank"/>。</p><p id="7677" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">那么变压器架构给rnn(Bi-lstm)带来了什么好处呢？</p><ol class=""><li id="25f1" class="mh mi iq kt b ku kv kx ky la oc le od li oe lm mo mp mq mr bi translated">消失梯度:变压器中没有记忆门的概念，因为这种容易丢失信息的方法是通过直接访问序列的所有部分来规避的。</li><li id="682e" class="mh mi iq kt b ku ms kx mt la mu le mv li mw lm mo mp mq mr bi translated">长期依赖性:由于多头自我关注层，变形金刚更擅长捕捉长期依赖性。</li><li id="b8c5" class="mh mi iq kt b ku ms kx mt la mu le mv li mw lm mo mp mq mr bi translated">双向设计:因此，transformer编码器一次读取整个序列，并使用单词前后的所有环境。因此，它本质上是双向的。事实上，许多人认为它是<strong class="kt ir">无方向性的</strong>。</li></ol><h2 id="1ccd" class="lo lp iq bd lq lr ls dn lt lu lv dp lw la lx ly lz le ma mb mc li md me mf mg bi translated"><strong class="ak">生成式预训练变形金刚(GPT) </strong></h2><p id="a740" class="pw-post-body-paragraph kr ks iq kt b ku mj jr kw kx mk ju kz la mx lc ld le my lg lh li mz lk ll lm ij bi translated">由<a class="ae ln" href="https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf" rel="noopener ugc nofollow" target="_blank">拉德福德等人</a>于2018年首次推出。(就在伯特之前)GPT是第一批使用变形金刚架构的人之一。来自OpenAI的作者提出了这种架构，作为现有想法的有效结合<br/> a .无监督的预训练(如在ELMO看到的)和b .变形金刚。</p><p id="459a" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">此外，该框架有两个主要组成部分</p><p id="912d" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated"><strong class="kt ir"> 1。无监督的预训练(使用变压器)</strong>这基本上是在给定网络参数上的记号的上下文的情况下最大化记号的可能性。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi og"><img src="../Images/6cc7472bd631db39bcb66a59807f632b.png" data-original-src="https://miro.medium.com/v2/resize:fit:998/format:webp/1*4vuHN2ZGt8PNrLuMKng-KQ.png"/></div></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/c1c2faff67163f3337d2ab91e5a1be50.png" data-original-src="https://miro.medium.com/v2/resize:fit:490/format:webp/1*rVjjHXc9s3HCPBqQ5Qe72w.png"/></div><p class="kn ko gj gh gi kp kq bd b be z dk translated">来源:<a class="ae ln" href="https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf" rel="noopener ugc nofollow" target="_blank">原创论文</a></p></figure><p id="1380" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">为此，该论文提出使用多层(12层)变换器解码器，该解码器基本上由多头自关注层+位置前馈层组成，该位置前馈层使用softmax在目标令牌上产生分布。原始变形金刚架构的这种变化是<strong class="kt ir">单向的</strong>(从左到右)，因为自我关注仅归因于左上下文。</p><p id="29ae" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated"><strong class="kt ir"> 2。监督微调:</strong>对于分类等下游任务，标记的数据被输入到先前的模型中，用于变压器解码器的表示和微调。附加的线性图层+ softmax图层有助于最终的分类任务。作者还建议增加一个<a class="ae ln" href="https://medium.com/walmartglobaltech/the-journey-of-open-ai-gpt-models-32d95b7b7fb2#e96b" rel="noopener">额外的学习目标</a>来学习一个展示更好的泛化能力的语言模型。</p><p id="6f8c" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">除了上述生态位特征外，<strong class="kt ir">尺度</strong>是GPT一号的另一个属性。它是在一个拥有240个GPU日的大规模图书语料库上训练的。GPT 1号之后的所有后续模型都是通过强大的GPU/TPU和越来越多的参数在大量数据上进行训练的。GPT-1成功证明了基于大规模预训练+少量监督微调和附加目标学习的转换器可以满足各种NLP任务(NLI、问题回答和分类)。事实上，当时它的表现确实超过了各种sota车型。</p><p id="930b" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">然而，GPT-1模型本质上是单向的(从左到右)，因为自我关注仅基于先前的表征。伯特提出的问题。</p><h2 id="382d" class="lo lp iq bd lq lr ls dn lt lu lv dp lw la lx ly lz le ma mb mc li md me mf mg bi translated">伯特</h2><p id="a144" class="pw-post-body-paragraph kr ks iq kt b ku mj jr kw kx mk ju kz la mx lc ld le my lg lh li mz lk ll lm ij bi translated">确实是一次长途旅行！伯特(<em class="oi">来自变形金刚</em>的双向编码器表示)由作者<a class="ae ln" href="https://arxiv.org/pdf/1810.04805.pdf" rel="noopener ugc nofollow" target="_blank">德夫林等人</a>在谷歌的GPT-1之后不久发表。总的来说，这种方法看起来非常类似于GPT-1架构中提出的无监督语言模型学习，然后有监督的微调步骤。然而，BERT的架构更像Vaswani等人的原始transformer架构，并且基于多层双向Transformer编码器。而GPT-1架构只是原始架构的仅左上下文(单向)版本，通常被称为“变换器解码器”。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="nb nc di nd bf ne"><div class="gh gi oj"><img src="../Images/026d80a02a7684ab34a60d91a053fde9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5DSE-bJGs-pItQgL54FH4w.png"/></div></div><p class="kn ko gj gh gi kp kq bd b be z dk translated">来源:<a class="ae ln" href="https://jalammar.github.io/illustrated-bert/" rel="noopener ugc nofollow" target="_blank">https://jalammar.github.io/illustrated-bert/</a></p></figure><p id="86ed" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">因此，作者的主要论点是单向预训练限制了下游任务的代表性，因此<strong class="kt ir">是次优的</strong>。例如，用于微调问题回答任务的单向预训练模型是次优的，因为没有利用来自两个方向的上下文信息。由于BERT是双向的，标准语言模型任务不适合作为目标学习任务。这是因为在transformers架构中，所有单词都是一次输入到模型中的(因此是可访问的)。对于一个标准的LM任务，每个单词都可以从未来看到自己，因此学习变得微不足道。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="nb nc di nd bf ne"><div class="gh gi ok"><img src="../Images/c57878ff2feaf20e0e5b04f0bc0a7c99.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tOvpxTZs34BRCv62yZCEOQ.png"/></div></div><p class="kn ko gj gh gi kp kq bd b be z dk translated">来源:<a class="ae ln" href="https://www.cs.princeton.edu/courses/archive/fall19/cos484/lectures/lec16.pdf" rel="noopener ugc nofollow" target="_blank">普林斯顿COS 484 </a></p></figure><p id="5161" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">BERT通过使用<strong class="kt ir">“屏蔽语言建模”</strong>解决了这个问题，这实质上是屏蔽文本中的随机标记并预测它。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="nb nc di nd bf ne"><div class="gh gi ol"><img src="../Images/0ee0615b4582871b84b04e168b4d5be0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SnK33I8uSqT0PdPqjjcAZg.png"/></div></div><p class="kn ko gj gh gi kp kq bd b be z dk translated">来源:<a class="ae ln" href="https://www.cs.princeton.edu/courses/archive/fall19/cos484/lectures/lec16.pdf" rel="noopener ugc nofollow" target="_blank">普林斯顿COS 484 </a></p></figure><p id="eeb4" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">除了MLM，伯特还采用了另一个叫做<strong class="kt ir">“下一句话预测”的学习目标。</strong>在NSP中，目标是分类一个句子是否是另一个给定句子的跟随句。直觉上，这有助于学习句子之间的关系。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="nb nc di nd bf ne"><div class="gh gi om"><img src="../Images/ddeafc8032a355f8f3f2d3c4250086bd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*x98gxRPkftSx0M3hDpfdRg.png"/></div></div><p class="kn ko gj gh gi kp kq bd b be z dk translated">来源:<a class="ae ln" href="https://www.cs.princeton.edu/courses/archive/fall19/cos484/lectures/lec16.pdf" rel="noopener ugc nofollow" target="_blank">普林斯顿COS 484 </a></p></figure><p id="fdbe" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">与GPT-1一样，微调是BERT中的第二阶段。修改(输入表示和输出图层)本质上是特定于任务的。例如，对于分类任务，CLS(事件中的第一个特殊令牌)被馈送到分类器网络。学习是端到端的，这意味着所有层及其权重继续学习。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="nb nc di nd bf ne"><div class="gh gi on"><img src="../Images/c31db3a1f437fbf514f3b2c6cbc1caa3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0IQYvMhNmFlusOATQyIPOg.png"/></div></div><p class="kn ko gj gh gi kp kq bd b be z dk translated">来源:<a class="ae ln" href="https://arxiv.org/pdf/1810.04805.pdf" rel="noopener ugc nofollow" target="_blank">原创论文</a></p></figure><p id="26d9" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">伯特有两种风格，伯特基础和伯特大。它们的主要区别在于层数(变压器模块)。基础=12层，110米参数，大型= 24层，340米参数。BERT确实是自然语言处理中的一个里程碑，它成功地展示了一种sota方法，该方法实现了基于变压器(自我注意)、双向和巧妙的目标学习任务的迁移学习。并在大规模语料库(books corpus+256个TPU日的英文维基百科)上进行离线训练。</p><p id="5106" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated"><strong class="kt ir">超越BERT </strong> <br/>在最初的BERT论文之后，在各个方面已经有了很多进步。还有更复杂的变化，如<a class="ae ln" href="https://arxiv.org/abs/1907.11692" rel="noopener ugc nofollow" target="_blank"> RoBERTa </a>在更大的语料库上训练更长时间，并采用聪明的学习目标(如动态屏蔽和转储NSP)。例如，另一个名为<a class="ae ln" href="https://arxiv.org/abs/1909.11942" rel="noopener ugc nofollow" target="_blank"> ALBERT </a>的变体旨在通过使用参数缩减技术产生一个更小的模型。ELCTRA 、<a class="ae ln" href="https://arxiv.org/abs/1906.08237" rel="noopener ugc nofollow" target="_blank"> XLNet </a>是其他一些有趣的变体。</p><p id="803f" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">此外，有一些积极的研究正在进行，以使伯特模型重量轻。(BERT大~ 340 M参数)。为此已经提出了几种方法，例如权重<strong class="kt ir">修剪、量化和提取(DistillBERT) </strong>。这里有一个关于同一的优秀博客:<a class="ae ln" href="https://blog.inten.to/speeding-up-bert-5528e18bb4ea" rel="noopener ugc nofollow" target="_blank">https://blog.inten.to/speeding-up-bert-5528e18bb4ea</a></p><p id="ce19" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated"><strong class="kt ir">总结<br/> </strong>我想NLP领域已经有了快速而巨大的发展。从使用文本的统计表示到上下文感知的神经表示。从基于统计学和经典ML的方法到基于深度学习的序列模型。途中发现注意力和双向性，体会迁移学习的力量。最终走向复杂的变形金刚架构。现代NLP框架在利用这些重要的里程碑和规模方面取得了长足的进步。</p><p id="c9cd" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated"><strong class="kt ir">参考文献</strong></p><ol class=""><li id="31a9" class="mh mi iq kt b ku kv kx ky la oc le od li oe lm mo mp mq mr bi translated">http://jalammar.github.io/illustrated-bert/<a class="ae ln" href="http://jalammar.github.io/illustrated-bert/" rel="noopener ugc nofollow" target="_blank"/></li><li id="58e8" class="mh mi iq kt b ku ms kx mt la mu le mv li mw lm mo mp mq mr bi translated">【http://web.stanford.edu/class/cs224n/ T4】</li><li id="2c4e" class="mh mi iq kt b ku ms kx mt la mu le mv li mw lm mo mp mq mr bi translated"><a class="ae ln" href="https://devopedia.org/bert-language-model" rel="noopener ugc nofollow" target="_blank">https://devopedia.org/bert-language-model</a></li><li id="5187" class="mh mi iq kt b ku ms kx mt la mu le mv li mw lm mo mp mq mr bi translated"><a class="ae ln" href="https://www.cs.princeton.edu/courses/archive/fall19/cos484/lectures/lec16.pdf" rel="noopener ugc nofollow" target="_blank">https://www . cs . Princeton . edu/courses/archive/fall 19/cos 484/lectures/LEC 16 . pdf</a></li><li id="2303" class="mh mi iq kt b ku ms kx mt la mu le mv li mw lm mo mp mq mr bi translated"><a class="ae ln" href="https://openai.com/blog/better-language-models/" rel="noopener ugc nofollow" target="_blank">https://openai.com/blog/better-language-models/</a></li><li id="da9c" class="mh mi iq kt b ku ms kx mt la mu le mv li mw lm mo mp mq mr bi translated"><a class="ae ln" href="https://neptune.ai/blog/ai-limits-can-deep-learning-models-like-bert-ever-understand-language" rel="noopener ugc nofollow" target="_blank">https://Neptune . ai/blog/ai-limits-can-deep-learning-models-like-Bert-ever-understand-language</a></li></ol></div></div>    
</body>
</html>