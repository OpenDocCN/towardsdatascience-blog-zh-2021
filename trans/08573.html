<html>
<head>
<title>LDA: Linear Discriminant Analysis — How to Improve Your Models with Supervised Dimensionality Reduction</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">线性判别分析——如何用监督降维改进你的模型</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/lda-linear-discriminant-analysis-how-to-improve-your-models-with-supervised-dimensionality-52464e73930f?source=collection_archive---------3-----------------------#2021-08-08">https://towardsdatascience.com/lda-linear-discriminant-analysis-how-to-improve-your-models-with-supervised-dimensionality-52464e73930f?source=collection_archive---------3-----------------------#2021-08-08</a></blockquote><div><div class="fc ii ij ik il im"/><div class="in io ip iq ir"><div class=""/><div class=""><h2 id="b91d" class="pw-subtitle-paragraph jr it iu bd b js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki dk translated">使用Python示例对PCA和LDA进行深入比较</h2></div><figure class="kk kl km kn gu ko gi gj paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gi gj kj"><img src="../Images/711e73ceac0e7ca0ca5560527be239ce.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*N1X9a3IxbQNctgdzmv_RTQ.png"/></div></div><p class="kv kw gk gi gj kx ky bd b be z dk translated">线性判别分析(LDA)。图片由<a class="ae kz" href="https://solclover.com/" rel="noopener ugc nofollow" target="_blank">作者</a>提供。</p></figure><h1 id="bc5b" class="la lb iu bd lc ld le lf lg lh li lj lk ka ll kb lm kd ln ke lo kg lp kh lq lr bi translated">介绍</h1><p id="0965" class="pw-post-body-paragraph ls lt iu lu b lv lw jv lx ly lz jy ma mb mc md me mf mg mh mi mj mk ml mm mn in bi translated">线性判别分析(LDA)是一种常用的降维技术。然而，尽管与主成分分析(PCA)相似，但它在一个关键方面有所不同。</p><p id="fe38" class="pw-post-body-paragraph ls lt iu lu b lv mo jv lx ly mp jy ma mb mq md me mf mr mh mi mj ms ml mm mn in bi translated">它不是寻找新的轴(维度)来最大化数据中的变化，而是关注于<strong class="lu iv">最大化目标变量中已知类别(类)之间的可分性</strong>。</p><p id="9ac5" class="pw-post-body-paragraph ls lt iu lu b lv mo jv lx ly mp jy ma mb mq md me mf mr mh mi mj ms ml mm mn in bi translated">在本文中，我直观地解释了LDA的工作原理，同时强调了它与PCA的不同之处。同时，我提供了一个对真实数据执行线性判别分析的Python示例。</p><h1 id="736f" class="la lb iu bd lc ld le lf lg lh li lj lk ka ll kb lm kd ln ke lo kg lp kh lq lr bi translated">内容</h1><ul class=""><li id="647e" class="mt mu iu lu b lv lw ly lz mb mv mf mw mj mx mn my mz na nb bi translated">LDA属于机器学习技术的范畴</li><li id="05c4" class="mt mu iu lu b lv nc ly nd mb ne mf nf mj ng mn my mz na nb bi translated">对LDA工作原理的直观解释</li><li id="1d6f" class="mt mu iu lu b lv nc ly nd mb ne mf nf mj ng mn my mz na nb bi translated">对真实数据执行LDA的Python示例</li><li id="f56c" class="mt mu iu lu b lv nc ly nd mb ne mf nf mj ng mn my mz na nb bi translated">结论</li></ul><h1 id="118d" class="la lb iu bd lc ld le lf lg lh li lj lk ka ll kb lm kd ln ke lo kg lp kh lq lr bi translated">线性判别分析(LDA)属于哪一类机器学习技术？</h1><p id="f0b3" class="pw-post-body-paragraph ls lt iu lu b lv lw jv lx ly lz jy ma mb mc md me mf mg mh mi mj mk ml mm mn in bi translated">与主成分分析(PCA)不同，LDA要求你为你的目标提供特征<strong class="lu iv">和类别标签</strong>。因此，尽管它是一种类似于PCA的降维技术，但它位于机器学习的<strong class="lu iv">监督</strong>分支内。</p><p id="0f77" class="pw-post-body-paragraph ls lt iu lu b lv mo jv lx ly mp jy ma mb mq md me mf mr mh mi mj ms ml mm mn in bi translated">下图是<strong class="lu iv">交互式的，</strong>所以请点击不同的类别来<strong class="lu iv">放大并展示更多的</strong>👇。</p><figure class="kk kl km kn gu ko"><div class="bz fq l di"><div class="nh ni l"/></div><p class="kv kw gk gi gj kx ky bd b be z dk translated">机器学习算法分类。由<a class="ae kz" href="https://solclover.com/" rel="noopener ugc nofollow" target="_blank">作者</a>创建的互动图表。</p></figure><p id="b180" class="pw-post-body-paragraph ls lt iu lu b lv mo jv lx ly mp jy ma mb mq md me mf mr mh mi mj ms ml mm mn in bi translated"><strong class="lu iv"> <em class="nj">如果你喜欢数据科学和机器学习</em> </strong> <em class="nj">，请</em> <a class="ae kz" href="https://solclover.com/subscribe" rel="noopener ugc nofollow" target="_blank"> <em class="nj">订阅</em> </a> <em class="nj">每当我发布一个新故事时，你都会收到一封电子邮件。</em></p><h1 id="fbff" class="la lb iu bd lc ld le lf lg lh li lj lk ka ll kb lm kd ln ke lo kg lp kh lq lr bi translated">线性判别分析(LDA)是如何工作的？</h1><p id="270a" class="pw-post-body-paragraph ls lt iu lu b lv lw jv lx ly lz jy ma mb mc md me mf mg mh mi mj mk ml mm mn in bi translated">理解LDA概念的最简单的方法是通过一个例子。因此，我没有关注算法背后的数学，而是为我们创建了一个可视化的解释。</p><h2 id="da57" class="nk lb iu bd lc nl nm dn lg nn no dp lk mb np nq lm mf nr ns lo mj nt nu lq nv bi translated">数据</h2><p id="ce04" class="pw-post-body-paragraph ls lt iu lu b lv lw jv lx ly lz jy ma mb mc md me mf mg mh mi mj mk ml mm mn in bi translated">假设我们已经收集了一系列关于城市公寓价格的数据。我们根据某个门槛，比如100万美元，将它们分为“昂贵”和“便宜”两类。</p><pre class="kk kl km kn gu nw nx ny nz aw oa bi"><span id="4abd" class="nk lb iu nx b gz ob oc l od oe">I.e., &lt;$1M = cheap, and ≥$1M = expensive. </span></pre><p id="1bf6" class="pw-post-body-paragraph ls lt iu lu b lv mo jv lx ly mp jy ma mb mq md me mf mr mh mi mj ms ml mm mn in bi translated">我们将数据显示在一个散点图上，其中x和y轴代表纬度和经度，为我们提供公寓的位置。同时，颜色代表类别/级别(便宜与昂贵)。请注意，我们已经对这个虚构示例中的数据进行了标准化，因此它以原点为中心。</p><blockquote class="of og oh"><p id="f7ca" class="ls lt nj lu b lv mo jv lx ly mp jy ma oi mq md me oj mr mh mi ok ms ml mm mn in bi translated"><em class="iu"> *S </em>串联化是一种数据转换技术，它会重新调整数据，使每个属性的平均值为0，标准差为1。这种转换可以用下面的公式来描述:</p></blockquote><figure class="kk kl km kn gu ko gi gj paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gi gj ol"><img src="../Images/a005edf1e6278ea4bf49f358263c4041.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OGhz95TlfmY8xNNTO3PS0Q.png"/></div></div><p class="kv kw gk gi gj kx ky bd b be z dk translated">标准化。图片由<a class="ae kz" href="https://solclover.com/" rel="noopener ugc nofollow" target="_blank">作者</a>提供。</p></figure><p id="6850" class="pw-post-body-paragraph ls lt iu lu b lv mo jv lx ly mp jy ma mb mq md me mf mr mh mi mj ms ml mm mn in bi translated">让我们看一下示例数据的散点图:</p><figure class="kk kl km kn gu ko gi gj paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gi gj om"><img src="../Images/720cd818b91b3e13d9829a0ee880d7bd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Xro4Nkqexi76ZsiaFyYRFQ.png"/></div></div><p class="kv kw gk gi gj kx ky bd b be z dk translated">公寓数据散点图。图片由<a class="ae kz" href="https://solclover.com/" rel="noopener ugc nofollow" target="_blank">作者</a>提供。</p></figure><p id="570c" class="pw-post-body-paragraph ls lt iu lu b lv mo jv lx ly mp jy ma mb mq md me mf mr mh mi mj ms ml mm mn in bi translated">现在，让我们比较PCA和LDA过程，以了解这些算法是如何工作的以及它们有什么不同。</p><h2 id="ca50" class="nk lb iu bd lc nl nm dn lg nn no dp lk mb np nq lm mf nr ns lo mj nt nu lq nv bi translated">应用主成分分析(PCA)</h2><p id="0c09" class="pw-post-body-paragraph ls lt iu lu b lv lw jv lx ly lz jy ma mb mc md me mf mg mh mi mj mk ml mm mn in bi translated">由于PCA是一种无监督的机器学习技术，我们不需要提供带有类别标签的目标变量。这意味着PCA不关心公寓是属于“便宜”还是“昂贵”的类别。</p><p id="e7a6" class="pw-post-body-paragraph ls lt iu lu b lv mo jv lx ly mp jy ma mb mq md me mf mr mh mi mj ms ml mm mn in bi translated">尽管如此，我还是保留了下图中的颜色标签，以突出与LDA的主要区别。</p><figure class="kk kl km kn gu ko gi gj paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gi gj on"><img src="../Images/7d756493d673cdd03cca2f6945b0c169.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CG-Nm7VBV_ehFSH4GBuUSw.png"/></div></div><p class="kv kw gk gi gj kx ky bd b be z dk translated">寻找主成分1 (PC1)。图片由<a class="ae kz" href="https://solclover.com/" rel="noopener ugc nofollow" target="_blank">作者</a>提供。</p></figure><p id="d8fe" class="pw-post-body-paragraph ls lt iu lu b lv mo jv lx ly mp jy ma mb mq md me mf mr mh mi mj ms ml mm mn in bi translated">PCA的目标是捕捉最大的变化量，这是通过算法找到一条线来实现的，该线使数据点到该线的距离最小。有趣的是，这相当于最大化<strong class="lu iv">同一条线上数据点投影的分布，这就是为什么我们可以</strong>捕捉最大的方差。</p><p id="bea6" class="pw-post-body-paragraph ls lt iu lu b lv mo jv lx ly mp jy ma mb mq md me mf mr mh mi mj ms ml mm mn in bi translated">从上面的图表中可以看出，我们已经很好地找到了一个新的轴，它保留了大部分的方差，使我们能够从二维下降到一维。这是我们将数据映射到新的一维(PC1)后的样子:</p><figure class="kk kl km kn gu ko gi gj paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gi gj oo"><img src="../Images/f351d7be5015dcbdeb5bbd3fd27b8f47.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uh1c_--UApsOpGyaIwNHdw.png"/></div></div><p class="kv kw gk gi gj kx ky bd b be z dk translated">公寓数据映射到一个维度(PC1)。图片由<a class="ae kz" href="https://solclover.com/" rel="noopener ugc nofollow" target="_blank">作者</a>提供。</p></figure><p id="47f5" class="pw-post-body-paragraph ls lt iu lu b lv mo jv lx ly mp jy ma mb mq md me mf mr mh mi mj ms ml mm mn in bi translated">您可以清楚地看到，在这个场景中，我们能够保留大部分的差异，但是仍然丢失了一些在试图分离两个类别(类)时有用的信息。</p><p id="cfdd" class="pw-post-body-paragraph ls lt iu lu b lv mo jv lx ly mp jy ma mb mq md me mf mr mh mi mj ms ml mm mn in bi translated"><em class="nj">如果你想更深入的了解PCA，可以参考我之前的文章:</em></p><div class="op oq gq gs or os"><a rel="noopener follow" target="_blank" href="/pca-principal-component-analysis-how-to-get-superior-results-with-fewer-dimensions-7a70e8ab798c"><div class="ot ab fp"><div class="ou ab ov cl cj ow"><h2 class="bd iv gz z fq ox fs ft oy fv fx it bi translated">PCA:主成分分析——如何用更少的维度获得优越的结果？</h2><div class="oz l"><h3 class="bd b gz z fq ox fs ft oy fv fx dk translated">降维的最佳技术之一</h3></div><div class="pa l"><p class="bd b dl z fq ox fs ft oy fv fx dk translated">towardsdatascience.com</p></div></div><div class="pb l"><div class="pc l pd pe pf pb pg kt os"/></div></div></a></div><h2 id="f5c8" class="nk lb iu bd lc nl nm dn lg nn no dp lk mb np nq lm mf nr ns lo mj nt nu lq nv bi translated">应用线性判别分析(LDA)</h2><p id="e522" class="pw-post-body-paragraph ls lt iu lu b lv lw jv lx ly lz jy ma mb mc md me mf mg mh mi mj mk ml mm mn in bi translated">现在让我们回到我们的原始示例数据，并应用LDA而不是PCA。快速提醒，<strong class="lu iv">LDA的目标是最大化我们的目标变量</strong>(“便宜”、“昂贵”)中已知类别的可分性，同时降低维数。</p><p id="0bbe" class="pw-post-body-paragraph ls lt iu lu b lv mo jv lx ly mp jy ma mb mq md me mf mr mh mi mj ms ml mm mn in bi translated">下面是我们执行LDA时新轴的样子:</p><figure class="kk kl km kn gu ko gi gj paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gi gj ph"><img src="../Images/61a3f5fe481a38a03e5f4698dcff04be.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KWTVfo29yZLi9FKw8YxoFA.png"/></div></div><p class="kv kw gk gi gj kx ky bd b be z dk translated">寻找线性判别式1 (LD1)。图片由<a class="ae kz" href="https://solclover.com/" rel="noopener ugc nofollow" target="_blank">作者</a>提供。</p></figure><p id="7d6d" class="pw-post-body-paragraph ls lt iu lu b lv mo jv lx ly mp jy ma mb mq md me mf mr mh mi mj ms ml mm mn in bi translated">如您所见，在这种情况下，新轴的选择与PCA的选择非常不同。在这种情况下，通过将数据映射到LD1，我们最终会损失大量的方差。但是，我们实现了两个类别的更好分离:</p><figure class="kk kl km kn gu ko gi gj paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gi gj pi"><img src="../Images/93cdb8bde56656f972a1933feea1dabc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2CyhQgM-TWpvQvvUoSSkFQ.png"/></div></div><p class="kv kw gk gi gj kx ky bd b be z dk translated">公寓数据映射到单一维度(LD1)。图片由<a class="ae kz" href="https://solclover.com/" rel="noopener ugc nofollow" target="_blank">作者</a>提供。</p></figure><p id="08be" class="pw-post-body-paragraph ls lt iu lu b lv mo jv lx ly mp jy ma mb mq md me mf mr mh mi mj ms ml mm mn in bi translated">虽然这种分离并不完美，但它明显更好，两个类别中只有一些重叠的观察结果。</p><h2 id="e079" class="nk lb iu bd lc nl nm dn lg nn no dp lk mb np nq lm mf nr ns lo mj nt nu lq nv bi translated">LDA如何找到合适的轴？</h2><p id="b707" class="pw-post-body-paragraph ls lt iu lu b lv lw jv lx ly lz jy ma mb mc md me mf mg mh mi mj mk ml mm mn in bi translated">在为我们的新轴寻找“最佳”线时，使用了两个关键标准，这两个标准是同时考虑的。</p><ol class=""><li id="da6b" class="mt mu iu lu b lv mo ly mp mb pj mf pk mj pl mn pm mz na nb bi translated"><strong class="lu iv">最大化距离(d)。</strong> <br/> <strong class="lu iv"> <em class="nj"> 2类</em> </strong> —当你的目标变量中有两个类时，距离是指类1的均值(μ)与类2的均值之差。<br/> <strong class="lu iv"> <em class="nj">超过两个类别</em> </strong> <em class="nj"> </em> —当目标变量中有三个或更多类别时，算法首先找到所有数据的中心点，然后测量每个类别平均值(μ)到该中心点的距离。</li></ol><p id="cea6" class="pw-post-body-paragraph ls lt iu lu b lv mo jv lx ly mp jy ma mb mq md me mf mr mh mi mj ms ml mm mn in bi translated">2.<strong class="lu iv">最小化变化，也称为LDA (s)中的“分散”。</strong></p><p id="44ce" class="pw-post-body-paragraph ls lt iu lu b lv mo jv lx ly mp jy ma mb mq md me mf mr mh mi mj ms ml mm mn in bi translated">让我们用图表来说明这两个标准:</p><figure class="kk kl km kn gu ko gi gj paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gi gj pn"><img src="../Images/6bd144e8247517ef64aba22c1bcd4f1b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Je4gJJLJjEJbS2Tb9QCF7g.png"/></div></div><p class="kv kw gk gi gj kx ky bd b be z dk translated">LDA步骤。图片由<a class="ae kz" href="https://solclover.com/" rel="noopener ugc nofollow" target="_blank">作者</a>提供。</p></figure><p id="60eb" class="pw-post-body-paragraph ls lt iu lu b lv mo jv lx ly mp jy ma mb mq md me mf mr mh mi mj ms ml mm mn in bi translated">实际上，计算通常借助奇异值分解(SVD)或特征值分解来进行。这两个选项在sklearn的LDA实现中都是可用的，我们将在下一节中使用它。</p><div class="kk kl km kn gu ab cb"><figure class="po ko pp pq pr ps pt paragraph-image"><a href="https://solclover.com/membership"><img src="../Images/1b3d3abe50e10bf8f8217750c717e6f9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*SlTMz5K4dBpjkIvUHo8C_Q.png"/></a></figure><figure class="po ko pp pq pr ps pt paragraph-image"><a href="https://www.linkedin.com/in/saulius-dobilas/"><img src="../Images/60fb21d1cb2701bfb6b71f61c99403e2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*vabxOXtQ4T034N_mscHSmQ.png"/></a></figure></div><h1 id="a611" class="la lb iu bd lc ld le lf lg lh li lj lk ka ll kb lm kd ln ke lo kg lp kh lq lr bi translated">对真实数据执行LDA的Python示例</h1><p id="c4db" class="pw-post-body-paragraph ls lt iu lu b lv lw jv lx ly lz jy ma mb mc md me mf mg mh mi mj mk ml mm mn in bi translated">最后，是有趣的时候了，我们可以使用Python来应用LDA。让我们从为我们的分析获取正确的库和数据开始。</p><h2 id="63c3" class="nk lb iu bd lc nl nm dn lg nn no dp lk mb np nq lm mf nr ns lo mj nt nu lq nv bi translated">设置</h2><p id="c9ea" class="pw-post-body-paragraph ls lt iu lu b lv lw jv lx ly lz jy ma mb mc md me mf mg mh mi mj mk ml mm mn in bi translated">我们将使用以下数据和库:</p><ul class=""><li id="394c" class="mt mu iu lu b lv mo ly mp mb pj mf pk mj pl mn my mz na nb bi translated"><a class="ae kz" href="https://www.kaggle.com/quantbruce/real-estate-price-prediction?select=Real+estate.csv" rel="noopener ugc nofollow" target="_blank">来自Kaggle </a>的房价数据</li><li id="f3ed" class="mt mu iu lu b lv nc ly nd mb ne mf nf mj ng mn my mz na nb bi translated"><a class="ae kz" href="https://scikit-learn.org/stable/index.html" rel="noopener ugc nofollow" target="_blank">Scikit-learn library</a>for<br/>1)编码分类类标签(<a class="ae kz" href="https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OrdinalEncoder.html" rel="noopener ugc nofollow" target="_blank">ordinal encoder</a>)；<br/> 2)特征标准化(<a class="ae kz" href="https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html?highlight=standard%20scaler#sklearn-preprocessing-standardscaler" rel="noopener ugc nofollow" target="_blank">standard scaler</a>)；<br/> 3)执行主成分分析(<a class="ae kz" href="https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html" rel="noopener ugc nofollow" target="_blank">PCA</a>)；<br/> 4)进行线性判别分析(<a class="ae kz" href="https://scikit-learn.org/stable/modules/generated/sklearn.discriminant_analysis.LinearDiscriminantAnalysis.html#" rel="noopener ugc nofollow" target="_blank">LDA</a>)；<br/> 5)创建基于决策树的预测模型(<a class="ae kz" href="https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#" rel="noopener ugc nofollow" target="_blank">决策树分类器</a>)；<br/> 6)模型评估(<a class="ae kz" href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html" rel="noopener ugc nofollow" target="_blank">分类_报告</a>)</li><li id="4561" class="mt mu iu lu b lv nc ly nd mb ne mf nf mj ng mn my mz na nb bi translated"><a class="ae kz" href="https://plotly.com/python/" rel="noopener ugc nofollow" target="_blank"> Plotly </a>用于数据可视化</li><li id="6812" class="mt mu iu lu b lv nc ly nd mb ne mf nf mj ng mn my mz na nb bi translated"><a class="ae kz" href="https://pandas.pydata.org/docs/" rel="noopener ugc nofollow" target="_blank">熊猫</a>进行数据操作</li></ul><p id="d87b" class="pw-post-body-paragraph ls lt iu lu b lv mo jv lx ly mp jy ma mb mq md me mf mr mh mi mj ms ml mm mn in bi translated">让我们导入所有的库:</p><figure class="kk kl km kn gu ko"><div class="bz fq l di"><div class="pu ni l"/></div></figure><p id="61c4" class="pw-post-body-paragraph ls lt iu lu b lv mo jv lx ly mp jy ma mb mq md me mf mr mh mi mj ms ml mm mn in bi translated">接下来，我们从Kaggle下载一个摄取的<a class="ae kz" href="https://www.kaggle.com/quantbruce/real-estate-price-prediction?select=Real+estate.csv" rel="noopener ugc nofollow" target="_blank">房价数据</a>。</p><figure class="kk kl km kn gu ko"><div class="bz fq l di"><div class="pu ni l"/></div></figure><p id="a367" class="pw-post-body-paragraph ls lt iu lu b lv mo jv lx ly mp jy ma mb mq md me mf mr mh mi mj ms ml mm mn in bi translated">下面是数据的一个片段:</p><figure class="kk kl km kn gu ko gi gj paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gi gj pv"><img src="../Images/e6b7db511a4e15e1c1347037fcaab8c2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*F32wdvk1cga3b26RJbl9rw.png"/></div></div><p class="kv kw gk gi gj kx ky bd b be z dk translated">Kaggle的房地产数据。图片由<a class="ae kz" href="https://solclover.com/" rel="noopener ugc nofollow" target="_blank">作者</a>提供。</p></figure><h2 id="0802" class="nk lb iu bd lc nl nm dn lg nn no dp lk mb np nq lm mf nr ns lo mj nt nu lq nv bi translated">数据准备</h2><p id="0984" class="pw-post-body-paragraph ls lt iu lu b lv lw jv lx ly lz jy ma mb mc md me mf mg mh mi mj mk ml mm mn in bi translated">对于这个例子，我们想要创建一个多类目标变量。因此，我们将“Y单位面积的房价”分成三个相等的类别。我们称它们为:‘1。“负担得起”(最低的33%)，‘2。中档”(中间33%)和“3。“昂贵”(前33%)。</p><figure class="kk kl km kn gu ko"><div class="bz fq l di"><div class="pu ni l"/></div></figure><figure class="kk kl km kn gu ko gi gj paragraph-image"><div class="gi gj pw"><img src="../Images/f3cc8a027c9e75b4fe335470cd7e7aa5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1036/format:webp/1*rqNSlKJx9qR8mSY9Aa_HEw.png"/></div><p class="kv kw gk gi gj kx ky bd b be z dk translated">类别(类)标签分布。图片由<a class="ae kz" href="https://solclover.com/" rel="noopener ugc nofollow" target="_blank">作者</a>提供。</p></figure><p id="395a" class="pw-post-body-paragraph ls lt iu lu b lv mo jv lx ly mp jy ma mb mq md me mf mr mh mi mj ms ml mm mn in bi translated">因为我们已经创建了分类标签，而算法需要数字标签，所以让我们使用顺序编码器将它们转换成数字标签。<em class="nj">【注意，我们可以直接分配数字标签，但是分类标签将使我们更容易阅读即将到来的可视化】。</em></p><figure class="kk kl km kn gu ko"><div class="bz fq l di"><div class="pu ni l"/></div></figure><figure class="kk kl km kn gu ko gi gj paragraph-image"><div class="gi gj px"><img src="../Images/d9d4bda26029bd1db1dbdff06bfaefca.png" data-original-src="https://miro.medium.com/v2/resize:fit:980/format:webp/1*40W85Qo0tB04AQo_-_xMkg.png"/></div><p class="kv kw gk gi gj kx ky bd b be z dk translated">编码的目标变量类标签。图片由<a class="ae kz" href="https://solclover.com/" rel="noopener ugc nofollow" target="_blank">作者</a>提供。</p></figure><p id="2879" class="pw-post-body-paragraph ls lt iu lu b lv mo jv lx ly mp jy ma mb mq md me mf mr mh mi mj ms ml mm mn in bi translated">为了保持可视化数据的能力，我们将限制自己使用前3个特征变量:“X1交易日期”、“X2房屋年龄”和“到最近的MRT站的X3距离”。</p><p id="5faa" class="pw-post-body-paragraph ls lt iu lu b lv mo jv lx ly mp jy ma mb mq md me mf mr mh mi mj ms ml mm mn in bi translated">让我们通过在3D散点图上绘制这三个要素来看看数据是什么样的:</p><figure class="kk kl km kn gu ko"><div class="bz fq l di"><div class="pu ni l"/></div></figure><figure class="kk kl km kn gu ko"><div class="bz fq l di"><div class="nh ni l"/></div><p class="kv kw gk gi gj kx ky bd b be z dk translated">模型特征的3D散点图。图表作者<a class="ae kz" href="https://solclover.com/" rel="noopener ugc nofollow" target="_blank">作者</a>。</p></figure><p id="12b2" class="pw-post-body-paragraph ls lt iu lu b lv mo jv lx ly mp jy ma mb mq md me mf mr mh mi mj ms ml mm mn in bi translated">我们可以看到数据非常分散。此外，我们注意到，较便宜的房地产位于远离捷运站，而昂贵的往往更接近捷运，也较新。</p><p id="e228" class="pw-post-body-paragraph ls lt iu lu b lv mo jv lx ly mp jy ma mb mq md me mf mr mh mi mj ms ml mm mn in bi translated">数据准备的最后一步是特征的标准化。</p><figure class="kk kl km kn gu ko"><div class="bz fq l di"><div class="pu ni l"/></div></figure><p id="2f2c" class="pw-post-body-paragraph ls lt iu lu b lv mo jv lx ly mp jy ma mb mq md me mf mr mh mi mj ms ml mm mn in bi translated">请注意，在这个阶段，我们还应该将数据分为训练样本和测试样本，但是我们将处理整个数据集，以保持这个示例的简单性。</p><h2 id="3e61" class="nk lb iu bd lc nl nm dn lg nn no dp lk mb np nq lm mf nr ns lo mj nt nu lq nv bi translated">执行主成分分析(PCA)</h2><p id="fa54" class="pw-post-body-paragraph ls lt iu lu b lv lw jv lx ly lz jy ma mb mc md me mf mg mh mi mj mk ml mm mn in bi translated">出于兴趣，我们还将执行PCA，这样我们可以将PCA的结果与LDA的结果进行比较。</p><figure class="kk kl km kn gu ko"><div class="bz fq l di"><div class="pu ni l"/></div></figure><figure class="kk kl km kn gu ko gi gj paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gi gj py"><img src="../Images/6110a91091001b8ebbcbc0570fc888e6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oTQ7E1kxTQMlsFvGRn7ESg.png"/></div></div><p class="kv kw gk gi gj kx ky bd b be z dk translated">PCA结果。图片由<a class="ae kz" href="https://solclover.com/" rel="noopener ugc nofollow" target="_blank">作者</a>提供。</p></figure><p id="8104" class="pw-post-body-paragraph ls lt iu lu b lv mo jv lx ly mp jy ma mb mq md me mf mr mh mi mj ms ml mm mn in bi translated">有趣的是，PC1和PC2的解释方差比率仅占总方差的约69%，这意味着当从3维下降到2维时，我们损失了约31%的总方差。这很大程度上是由于这三个特征之间的相关性很小。</p><p id="f42a" class="pw-post-body-paragraph ls lt iu lu b lv mo jv lx ly mp jy ma mb mq md me mf mr mh mi mj ms ml mm mn in bi translated">以下是应用主成分分析后得到的2D散点图:</p><figure class="kk kl km kn gu ko"><div class="bz fq l di"><div class="pu ni l"/></div></figure><figure class="kk kl km kn gu ko gi gj paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gi gj pz"><img src="../Images/e039b3a597920ef99306b8aa7ac86835.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zHMwtu158DpL-7QX-aNTQA.png"/></div></div><p class="kv kw gk gi gj kx ky bd b be z dk translated">PCA图。图片由<a class="ae kz" href="https://solclover.com/" rel="noopener ugc nofollow" target="_blank">作者</a>提供。</p></figure><p id="f7a5" class="pw-post-body-paragraph ls lt iu lu b lv mo jv lx ly mp jy ma mb mq md me mf mr mh mi mj ms ml mm mn in bi translated">虽然我们损失了31%的方差，但我们仍然设法保持了两个新维度之间的大量差异。然而，由于PCA只考虑特征而不考虑目标，所以这三个类别(类)最终都混在一起了。</p><p id="4fda" class="pw-post-body-paragraph ls lt iu lu b lv mo jv lx ly mp jy ma mb mq md me mf mr mh mi mj ms ml mm mn in bi translated">接下来，我们来看看LDA与PCA相比如何。</p><h2 id="4523" class="nk lb iu bd lc nl nm dn lg nn no dp lk mb np nq lm mf nr ns lo mj nt nu lq nv bi translated">执行线性判别分析(LDA)</h2><p id="4f22" class="pw-post-body-paragraph ls lt iu lu b lv lw jv lx ly lz jy ma mb mc md me mf mg mh mi mj mk ml mm mn in bi translated">我们将使用特征分解作为我们的求解器(sklearn实现使您能够在SVD、LSQR和特征之间进行选择),并将组件参数(维数)设置为2，将所有其他参数保留为默认值。注意，这一次我们还需要向算法提供我们的目标(y)。</p><figure class="kk kl km kn gu ko"><div class="bz fq l di"><div class="pu ni l"/></div></figure><figure class="kk kl km kn gu ko gi gj paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gi gj qa"><img src="../Images/57161b31c2532680b4f5cbbf7b032f37.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EwZiq-Uik43ruL_QH9JXfQ.png"/></div></div><p class="kv kw gk gi gj kx ky bd b be z dk translated">LDA结果。图片作者<a class="ae kz" href="https://solclover.com/" rel="noopener ugc nofollow" target="_blank">作者</a>。</p></figure><p id="a707" class="pw-post-body-paragraph ls lt iu lu b lv mo jv lx ly mp jy ma mb mq md me mf mr mh mi mj ms ml mm mn in bi translated">和以前一样，我们也将结果可视化在2D散点图上。</p><figure class="kk kl km kn gu ko"><div class="bz fq l di"><div class="pu ni l"/></div></figure><figure class="kk kl km kn gu ko gi gj paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gi gj qb"><img src="../Images/b933d9e1e56629ec829132429398d570.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uE2V0K0kMAVuCUu2SX8vDQ.png"/></div></div><p class="kv kw gk gi gj kx ky bd b be z dk translated">LDA图。图片由<a class="ae kz" href="https://solclover.com/" rel="noopener ugc nofollow" target="_blank">作者</a>提供。</p></figure><p id="fd35" class="pw-post-body-paragraph ls lt iu lu b lv mo jv lx ly mp jy ma mb mq md me mf mr mh mi mj ms ml mm mn in bi translated">与PCA相比，LDA结果有显著差异。由于PCA的目标是分离目标类别，而不是最大化方差，因此它设法找到了减少蓝色(' 2。中档)和红色(‘3。昂贵的)点，使它们基本上保持在自己的空间内。与此同时，尽管在绿色(‘1。实惠)类，红蓝分离的还蛮不错的。</p><p id="2e0c" class="pw-post-body-paragraph ls lt iu lu b lv mo jv lx ly mp jy ma mb mq md me mf mr mh mi mj ms ml mm mn in bi translated">为了验证我们在这些散点图中看到的内容，让我们构建几个决策树模型，看看我们可以根据使用PCA和ld a创建的特征预测“价格带”的效果如何。</p><h2 id="d4cc" class="nk lb iu bd lc nl nm dn lg nn no dp lk mb np nq lm mf nr ns lo mj nt nu lq nv bi translated">用决策树分类器创建预测模型</h2><p id="5afb" class="pw-post-body-paragraph ls lt iu lu b lv lw jv lx ly lz jy ma mb mc md me mf mg mh mi mj mk ml mm mn in bi translated">让我们建立一个可重用的函数，我们可以快速调用它来训练模型并显示结果。</p><figure class="kk kl km kn gu ko"><div class="bz fq l di"><div class="pu ni l"/></div></figure><p id="f126" class="pw-post-body-paragraph ls lt iu lu b lv mo jv lx ly mp jy ma mb mq md me mf mr mh mi mj ms ml mm mn in bi translated">现在让我们用它来建立一个具有PCA转换特征的模型。</p><figure class="kk kl km kn gu ko"><div class="bz fq l di"><div class="pu ni l"/></div></figure><p id="63cc" class="pw-post-body-paragraph ls lt iu lu b lv mo jv lx ly mp jy ma mb mq md me mf mr mh mi mj ms ml mm mn in bi translated">以下是模型结果:</p><figure class="kk kl km kn gu ko gi gj paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gi gj qc"><img src="../Images/f4f87290244a33edcc686acbd39a7f16.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Z_3KU9emjfanL0uj--75uA.png"/></div></div><p class="kv kw gk gi gj kx ky bd b be z dk translated">使用PCA变换特征的决策树模型结果。图片由<a class="ae kz" href="https://solclover.com/" rel="noopener ugc nofollow" target="_blank">作者</a>提供。</p></figure><p id="1874" class="pw-post-body-paragraph ls lt iu lu b lv mo jv lx ly mp jy ma mb mq md me mf mr mh mi mj ms ml mm mn in bi translated">正如所料，结果不是很好，准确率只有64%。应用主成分分析后类别的混合无疑增加了预测价格标签的难度。</p><p id="a041" class="pw-post-body-paragraph ls lt iu lu b lv mo jv lx ly mp jy ma mb mq md me mf mr mh mi mj ms ml mm mn in bi translated">让我们对LDA变换的特征重复同样的操作。</p><figure class="kk kl km kn gu ko"><div class="bz fq l di"><div class="pu ni l"/></div></figure><figure class="kk kl km kn gu ko gi gj paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gi gj qd"><img src="../Images/de7f5c5805827059ac99c2fbe2c8737f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ScyuOSdmhqzWKdIKV492MA.png"/></div></div><p class="kv kw gk gi gj kx ky bd b be z dk translated">使用LDA转换特征的决策树模型结果。图片由<a class="ae kz" href="https://solclover.com/" rel="noopener ugc nofollow" target="_blank">作者</a>提供。</p></figure><p id="d071" class="pw-post-body-paragraph ls lt iu lu b lv mo jv lx ly mp jy ma mb mq md me mf mr mh mi mj ms ml mm mn in bi translated">这一次模型似乎好了很多，准确率接近79%。这些结果支持了我们早期的直觉，即与PCA相比，LDA更好地保存了与预测公寓价格相关的信息。</p><h1 id="38ba" class="la lb iu bd lc ld le lf lg lh li lj lk ka ll kb lm kd ln ke lo kg lp kh lq lr bi translated">结论</h1><p id="739a" class="pw-post-body-paragraph ls lt iu lu b lv lw jv lx ly lz jy ma mb mc md me mf mg mh mi mj mk ml mm mn in bi translated">当我们希望降低数据的维数，同时保留尽可能多的与预测目标相关的信息时，LDA是一个很好的工具。</p><p id="1708" class="pw-post-body-paragraph ls lt iu lu b lv mo jv lx ly mp jy ma mb mq md me mf mr mh mi mj ms ml mm mn in bi translated">然而，直接比较PCA和LDA可能不完全公平，因为它们是用于不同目的的两种不同技术。例如，PCA是一种<strong class="lu iv">非监督</strong>学习技术，而LDA则属于ML的<strong class="lu iv">监督</strong>分支。</p><p id="2990" class="pw-post-body-paragraph ls lt iu lu b lv mo jv lx ly mp jy ma mb mq md me mf mr mh mi mj ms ml mm mn in bi translated">因此，请不要把上面的结果作为LDA优于PCA的证据。相反，请首先评估这些算法对您的独特情况的适用性。</p><p id="5aa5" class="pw-post-body-paragraph ls lt iu lu b lv mo jv lx ly mp jy ma mb mq md me mf mr mh mi mj ms ml mm mn in bi translated">我真诚地希望我的文章能够帮助您更好地理解线性判别分析，使您能够将其融入到自己的数据科学项目中。</p><p id="6cbe" class="pw-post-body-paragraph ls lt iu lu b lv mo jv lx ly mp jy ma mb mq md me mf mr mh mi mj ms ml mm mn in bi translated">干杯！👏<br/> <strong class="lu iv">索尔·多比拉斯</strong></p></div><div class="ab cl qe qf hy qg" role="separator"><span class="qh bw bk qi qj qk"/><span class="qh bw bk qi qj qk"/><span class="qh bw bk qi qj"/></div><div class="in io ip iq ir"><p id="806a" class="pw-post-body-paragraph ls lt iu lu b lv mo jv lx ly mp jy ma mb mq md me mf mr mh mi mj ms ml mm mn in bi translated"><strong class="lu iv"> <em class="nj">如果你已经花光了这个月的学习预算，下次请记得我。</em> </strong> <em class="nj">我的个性化链接加入媒介是:</em></p><div class="op oq gq gs or os"><a href="https://solclover.com/membership" rel="noopener  ugc nofollow" target="_blank"><div class="ot ab fp"><div class="ou ab ov cl cj ow"><h2 class="bd iv gz z fq ox fs ft oy fv fx it bi translated">通过我的推荐链接加入Medium索尔·多比拉斯</h2><div class="oz l"><h3 class="bd b gz z fq ox fs ft oy fv fx dk translated">作为一个媒体会员，你的会员费的一部分会给你阅读的作家，你可以完全接触到每一个故事…</h3></div><div class="pa l"><p class="bd b dl z fq ox fs ft oy fv fx dk translated">solclover.com</p></div></div><div class="pb l"><div class="ql l pd pe pf pb pg kt os"/></div></div></a></div></div><div class="ab cl qe qf hy qg" role="separator"><span class="qh bw bk qi qj qk"/><span class="qh bw bk qi qj qk"/><span class="qh bw bk qi qj"/></div><div class="in io ip iq ir"><p id="a1bf" class="pw-post-body-paragraph ls lt iu lu b lv mo jv lx ly mp jy ma mb mq md me mf mr mh mi mj ms ml mm mn in bi translated">如果你喜欢这个故事，这里有几篇你可能会喜欢的文章:</p><div class="op oq gq gs or os"><a rel="noopener follow" target="_blank" href="/dbscan-clustering-algorithm-how-to-build-powerful-density-based-models-21d9961c4cec"><div class="ot ab fp"><div class="ou ab ov cl cj ow"><h2 class="bd iv gz z fq ox fs ft oy fv fx it bi translated">DBSCAN聚类算法——如何构建强大的基于密度的模型</h2><div class="oz l"><h3 class="bd b gz z fq ox fs ft oy fv fx dk translated">对有噪声的应用程序使用基于密度的空间聚类的详细指南</h3></div><div class="pa l"><p class="bd b dl z fq ox fs ft oy fv fx dk translated">towardsdatascience.com</p></div></div><div class="pb l"><div class="qm l pd pe pf pb pg kt os"/></div></div></a></div><div class="op oq gq gs or os"><a rel="noopener follow" target="_blank" href="/k-nearest-neighbors-knn-how-to-make-quality-predictions-with-supervised-learning-d5d2f326c3c2"><div class="ot ab fp"><div class="ou ab ov cl cj ow"><h2 class="bd iv gz z fq ox fs ft oy fv fx it bi translated">k-最近邻(kNN)-如何使用监督学习进行质量预测？</h2><div class="oz l"><h3 class="bd b gz z fq ox fs ft oy fv fx dk translated">使用kNN解决回归和分类问题的综合指南</h3></div><div class="pa l"><p class="bd b dl z fq ox fs ft oy fv fx dk translated">towardsdatascience.com</p></div></div><div class="pb l"><div class="qn l pd pe pf pb pg kt os"/></div></div></a></div></div></div>    
</body>
</html>