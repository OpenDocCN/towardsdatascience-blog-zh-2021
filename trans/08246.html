<html>
<head>
<title>Pre-Pruning or Post-Pruning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">预修剪或后修剪</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/pre-pruning-or-post-pruning-1dbc8be5cb14?source=collection_archive---------10-----------------------#2021-07-29">https://towardsdatascience.com/pre-pruning-or-post-pruning-1dbc8be5cb14?source=collection_archive---------10-----------------------#2021-07-29</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="9c13" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">了解如何以及何时在Python中预先修剪决策树</h2></div><p id="d7b8" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">爱德华·克鲁格</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi lf"><img src="../Images/e961e267879e7c7cd5235e34a2fba27e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xLdyEcCNK-TjvACqhaQnEQ.jpeg"/></div></div><p class="lr ls gj gh gi lt lu bd b be z dk translated"><a class="ae le" href="https://unsplash.com/@devin_photography?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Devin H </a>在<a class="ae le" href="https://unsplash.com/s/photos/bonsai?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="334c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在上一篇文章中，我们讨论了后期修剪决策树。在本文中，我们将重点关注预修剪决策树。</p><p id="5d6b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们简单回顾一下我们修剪决策树的动机，后修剪工作的方式和原因，以及它的优点和缺点。如果你想了解更多细节，请查看这篇文章。</p><p id="6192" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">决策树是使用CART算法生成的。CART代表“分类和回归树”不幸的是，知道缩写代表什么并不能帮助我们理解任何事情，但继续阅读，我们会涵盖它。或者，如果你想了解更多细节，可以看看我的文章:</p><div class="lv lw gp gr lx ly"><a rel="noopener follow" target="_blank" href="/learn-how-decision-trees-are-grown-22bc3d22fb51"><div class="lz ab fo"><div class="ma ab mb cl cj mc"><h2 class="bd iu gy z fp md fr fs me fu fw is bi translated">了解决策树是如何生长的</h2><div class="mf l"><p class="bd b dl z fp md fr fs me fu fw dk translated">towardsdatascience.com</p></div></div><div class="mg l"><div class="mh l mi mj mk mg ml lp ly"/></div></div></a></div><p id="5172" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">本质上，该算法沿着特征检查分区，以找到根据度量优化分数的分区。然后，该算法递归地应用于分割的每一侧。当不再有可以提高分数的分割时，算法终止。由于这个原因，树有时被称为递归划分。</p><p id="d25e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">对于回归，MSE是最常见的选择指标，也是SciKit-Learn的默认指标。对于分类问题，基尼和熵是最常见的度量。虽然它们的理论表述不同，但在实践中似乎没有什么区别。SciKit-Learn中的分类树可以使用这两个指标，缺省值为Gini。</p><p id="ae79" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">为了避免过度拟合，我们可以应用提前停止规则。本文就是关于这些规则的——预修剪只是提前停止的一个花哨术语。避免过度拟合的另一个选择是应用后修剪(有时简称为修剪)。也可以同时应用这两种方法——有时，这将是加速后期修剪过程的一个很好的方法。</p><p id="94a5" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">后期修剪考虑整个树的子树，并使用交叉验证的度量来对每个子树进行评分。为了澄清，我们使用子树来表示与原始树有相同根但没有一些分支的树。</p><p id="4f2e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">对于回归树，我们通常使用MSE进行修剪。对于分类树，我们通常使用误分类率进行修剪。误分类率与二元分类问题的准确度成比例，并且选择相同的最佳子树。</p><p id="ab17" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">实际上，有必要避免考虑每一个子树，因为二叉树的子树数量很少——在渐近上比多项式时间更差。因此，剪枝算法使用一种技巧来选择包含原始树根的所有子树集合的子序列(称为成本复杂度路径)。</p><p id="7e0d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">即使进行了这种优化，随着要素和观测值数量的增加，后期修剪也会非常缓慢。幸运的是，我们可以并行地交叉验证子树，但是在某一点上，后期修剪增加的时间令人沮丧。</p><p id="b5e7" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">另一方面，后修剪往往比预修剪/提前停止更有效。预剪枝的问题在于它是贪婪的:早期停止规则可能会使算法避开一个分区，即使后续分区可能非常有价值。早期停止比后期修剪更快，但是值得一提的是，早期停止并没有看起来那么快。使用早期停止规则生成树花费的时间更少，但是我们仍然需要执行网格搜索来确定早期停止规则的最佳阈值。</p><p id="75e9" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在SciKitLearn中，我们可以使用一些超参数来控制早期停止。在本文中，我们将重点讨论两个问题。</p><ul class=""><li id="9c60" class="mm mn it kk b kl km ko kp kr mo kv mp kz mq ld mr ms mt mu bi translated"><code class="fe mv mw mx my b">min_samples_split</code></li><li id="88f8" class="mm mn it kk b kl mz ko na kr nb kv nc kz nd ld mr ms mt mu bi translated"><code class="fe mv mw mx my b">min_samples_leaf</code></li></ul><p id="589f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们将使用来自SciKit-Learn的乳腺癌数据集进行演示。数据集包含30个连续要素。目标是肿瘤是否是恶性的。欲了解更多信息，请访问SciKit-Learn文档<a class="ae le" href="https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_breast_cancer.htm" rel="noopener ugc nofollow" target="_blank">此处</a>。</p><p id="39d3" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在我们开始预修剪之前，让我们先看看完整的树以及它是如何工作的。</p><h2 id="fd73" class="ne nf it bd ng nh ni dn nj nk nl dp nm kr nn no np kv nq nr ns kz nt nu nv nw bi translated">完整的树</h2><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi nx"><img src="../Images/aae4f08b3a2feced894123b0d18ceb2a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HddZAHFKbIYDqWl693YceQ.png"/></div></div><p class="lr ls gj gh gi lt lu bd b be z dk translated">用SciKit-Learn训练决策树</p></figure><p id="d853" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这里，我们用默认参数来拟合决策树，只是我们设置了<code class="fe mv mw mx my b">random_state</code>。我们设置了一个随机状态，因为当多个分裂一样好时，算法会随机打破平局。通过设置状态，我们将确保在预修剪时使用同一棵树的子树。</p><p id="7ab5" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们看看完整的树。</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi ny"><img src="../Images/e0ecaee5e0fb6bb575cb8d1febc6f04b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Gr9G0GbMirsTKOKaa0lOdQ.png"/></div></div><p class="lr ls gj gh gi lt lu bd b be z dk translated">用SciKit-Learn绘制决策树</p></figure><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi nz"><img src="../Images/925c32197999d0c2ffc6f99245e984c5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*r187xgl29mDrjYbJPMNXYA.png"/></div></div><p class="lr ls gj gh gi lt lu bd b be z dk translated">完整的决策树是使用上面的代码绘制的</p></figure><p id="2212" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">请注意，完整的树是相当复杂的，有18个不同的分裂！</p><p id="9a55" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们看一下整个树的指标。</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi oa"><img src="../Images/9287c40ab75444a23b17456caf7496a7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-GKijrDIUZQmMDt3f2RKQA.png"/></div></div><p class="lr ls gj gh gi lt lu bd b be z dk translated">完整树的度量</p></figure><p id="71a3" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">它达到了91%的准确率。对于恶性类，准确率和召回率分别为96%和90%。这建立了我们的基线。</p><h2 id="bcc0" class="ne nf it bd ng nh ni dn nj nk nl dp nm kr nn no np kv nq nr ns kz nt nu nv nw bi translated">分裂树的最佳最小样本</h2><p id="8ba1" class="pw-post-body-paragraph ki kj it kk b kl ob ju kn ko oc jx kq kr od kt ku kv oe kx ky kz of lb lc ld im bi translated">在这里，我们将研究如何设置一个阈值，如果一个节点上的观察值小于该阈值，那么该阈值将停止树在该节点上的生长。如果在划分之后，观察值的数量小于阈值，递归在树的这一侧停止。在SciKit-Learn中，我们可以用<code class="fe mv mw mx my b">min_sample_split</code>超参数来控制这种行为。它可以采用<code class="fe mv mw mx my b">int</code>或浮点值。当我们通过一个<code class="fe mv mw mx my b">int</code>时，如果一个节点的观测值少于指定的数目，那么该算法不会考虑该节点之后的进一步分裂。当我们通过一个<code class="fe mv mw mx my b">float</code>时，当一个节点的观测值少于数据集的指定比例时，递归停止。将超参数指定为浮点型更容易且更可重用，因为我们不必为更大或更小的数据集修改值的范围。</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi og"><img src="../Images/accf567a374f8aa4017cdf9de14df0b9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jZJWzw4nG_0SzsdH0-nA0Q.png"/></div></div><p class="lr ls gj gh gi lt lu bd b be z dk translated">用于寻找最佳最小样本分裂树的代码</p></figure><p id="aa91" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在我们的网格搜索中，我们严格地在0和1之间寻找阈值。阈值为0不会有任何影响，而阈值高于1会导致树没有分割。当生成要检查的阈值范围时，我们从略大于0的数字<code class="fe mv mw mx my b">EPS</code>开始，在1之前结束。</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi oh"><img src="../Images/e9db4b55db83f93b13b4de85eacf6438.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5tad6IrXGm6YvG4Yiq99UA.png"/></div></div><p class="lr ls gj gh gi lt lu bd b be z dk translated">寻找min_samples_split超参数的最佳值</p></figure><p id="0f43" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">网格搜索发现超参数<code class="fe mv mw mx my b">min_samples_split</code>的最佳选择约为0.075，这意味着如果节点小于数据集的7.5%，则不允许进一步分裂。</p><p id="8f89" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们画出这棵树。</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi oi"><img src="../Images/3dda0a2b10156aac77a0e9a6b148acd3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Qk86dzSu2qQLnM8CrOMufg.png"/></div></div><p class="lr ls gj gh gi lt lu bd b be z dk translated">要准备绘图和评估，请从网格搜索中提取树</p></figure><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi oj"><img src="../Images/4bba539082523ebf6e5896609f8c637e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0OlQsQt7BOJJXH8ZuPyGbg.png"/></div></div><p class="lr ls gj gh gi lt lu bd b be z dk translated">绘制最佳最小样本分裂树</p></figure><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi nz"><img src="../Images/718834968580b255c0d67f180923cdd3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vaAP4raeHRrNfU7JTtsF0A.png"/></div></div><p class="lr ls gj gh gi lt lu bd b be z dk translated">使用上面的代码绘制了最佳最小样本分裂树</p></figure><p id="04de" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">与完整的树相比，在这个数据集上，这个树更简单，只有8个分裂。</p><p id="5701" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在，让我们来看看指标。</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi ok"><img src="../Images/fd4409d69b9d59053fec461c6e9ce7c5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wTGQdx0hQkw43TOUJ_B0nw.png"/></div></div><p class="lr ls gj gh gi lt lu bd b be z dk translated">最佳最小样本分裂树的分类报告</p></figure><p id="ba8b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">不幸的是，在这种情况下，这种预先修剪的方法不如完整的树准确，准确率为89%。恶性类的召回率和准确率也略有下降。幸运的是，我们有另一种方法可以尝试。</p><h2 id="6df9" class="ne nf it bd ng nh ni dn nj nk nl dp nm kr nn no np kv nq nr ns kz nt nu nv nw bi translated">叶树上的最佳最小样本</h2><p id="0ef5" class="pw-post-body-paragraph ki kj it kk b kl ob ju kn ko oc jx kq kr od kt ku kv oe kx ky kz of lb lc ld im bi translated">让我们看看，当一片叶子上的样本数低于某个阈值时，我们试图通过停止递归来限制一棵树，会得到什么结果。SciKit-Learns用超参数<code class="fe mv mw mx my b">min_samples_leaf</code>实现了这一点。它可以接受一个<code class="fe mv mw mx my b">int</code>或一个<code class="fe mv mw mx my b">float</code>的值。当我们传递一个int时，当一个split创建的叶子少于这个数目的观察值时，算法停止。当我们通过一个<code class="fe mv mw mx my b">float</code>时，当一个分裂将导致一个叶子上的观察值下降到数据集的那个比例以下时，递归停止。一般来说，我们发现通过一个<code class="fe mv mw mx my b">float</code>更容易，因为它是一个比例；对于不同大小的数据集，我们不需要改变网格搜索的范围。</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi ny"><img src="../Images/586a6f16dde999c81493f427b6148a13.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xrSkV8VLX1jEAL4CZG_4Dg.png"/></div></div><p class="lr ls gj gh gi lt lu bd b be z dk translated">寻找最佳最小样本叶树的代码</p></figure><p id="19bf" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">为了对参数<code class="fe mv mw mx my b">min_samples_leaf</code>的不同值进行网格搜索，我们使用<code class="fe mv mw mx my b">np.arange</code>设置了一个严格介于0和. 5之间的网格值。要求每个叶子上至少有0%的数据集，不会约束算法，并且会导致SciKit-Learn的边缘情况。因此，我们从<code class="fe mv mw mx my b">EPS</code>开始网格，我们将其设置为略高于0。如果我们要求超过50%的数据集位于每片叶子上，那么进行任何分割都会违反规则:如果超过50%的数据集位于分割的两片叶子上，那么超过100%的数据集必须位于两片叶子之间。</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi ol"><img src="../Images/af7f6a4b2b8a561e0ea407a56e0057e7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FuMFAOsgIcALMzHgb1JZRA.png"/></div></div><p class="lr ls gj gh gi lt lu bd b be z dk translated">寻找min_samples_leaf超参数的最佳值</p></figure><p id="9b5d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">网格搜索发现参数<code class="fe mv mw mx my b">min_samples_leaf</code>的最佳值大约是. 025，这意味着每个叶子需要至少有2.5%的数据集。</p><p id="6658" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们画出这棵树。</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi oi"><img src="../Images/3dda0a2b10156aac77a0e9a6b148acd3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Qk86dzSu2qQLnM8CrOMufg.png"/></div></div><p class="lr ls gj gh gi lt lu bd b be z dk translated">要准备绘图和评估，请从网格搜索中提取树</p></figure><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi nx"><img src="../Images/ae745d2085bc41b83b17833fa3fb09da.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*v6f_LQune5XF04F9594fTw.png"/></div></div><p class="lr ls gj gh gi lt lu bd b be z dk translated">绘制最佳最小样本叶树</p></figure><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi nz"><img src="../Images/8d4ad5b25a33f87be55b3b89ff7b411d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RMT7eO4WFDXngCWt0ccBaQ.png"/></div></div><p class="lr ls gj gh gi lt lu bd b be z dk translated">使用上面的代码绘制了最佳最小样本叶树</p></figure><p id="d9fe" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">最佳最小叶子样本树比只有8个分裂的完整树简单得多。</p><p id="f090" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们来看看指标。</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi om"><img src="../Images/e8c0dc788459eac8098443a418d6ded7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yxrbvkyQDeTCB5YUSSSxgQ.png"/></div></div><p class="lr ls gj gh gi lt lu bd b be z dk translated">最佳最小样本叶树分类报告</p></figure><p id="fc41" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们现在有一个更好的95%的训练准确率。这棵树也在不损失精确度的情况下击败了基线的召回。对于这个数据集，该方法甚至执行后剪枝！请看这篇文章来比较这些方法:</p><div class="lv lw gp gr lx ly"><a rel="noopener follow" target="_blank" href="/build-better-decision-trees-with-pruning-8f467e73b107"><div class="lz ab fo"><div class="ma ab mb cl cj mc"><h2 class="bd iu gy z fp md fr fs me fu fw is bi translated">通过修剪构建更好的决策树</h2><div class="on l"><h3 class="bd b gy z fp md fr fs me fu fw dk translated">通过限制最大深度和修剪减少决策树的过度拟合和复杂性</h3></div><div class="mf l"><p class="bd b dl z fp md fr fs me fu fw dk translated">towardsdatascience.com</p></div></div><div class="mg l"><div class="oo l mi mj mk mg ml lp ly"/></div></div></a></div><h2 id="52ef" class="ne nf it bd ng nh ni dn nj nk nl dp nm kr nn no np kv nq nr ns kz nt nu nv nw bi translated">结论</h2><p id="e6d8" class="pw-post-body-paragraph ki kj it kk b kl ob ju kn ko oc jx kq kr od kt ku kv oe kx ky kz of lb lc ld im bi translated">在本文中，我们介绍了一些预修剪的方法，如何在SciKit-Learn中实现它们，并比较了预修剪和后修剪。概括地说，让我们来看看不同之处。</p><p id="7eff" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">预剪枝和后剪枝的优点:</strong></p><ul class=""><li id="5686" class="mm mn it kk b kl km ko kp kr mo kv mp kz mq ld mr ms mt mu bi translated">通过限制树的复杂性，修剪创建了更简单更易理解的树。</li><li id="4ec5" class="mm mn it kk b kl mz ko na kr nb kv nc kz nd ld mr ms mt mu bi translated">通过限制树的复杂性，修剪减少了过度拟合。</li><li id="ab21" class="mm mn it kk b kl mz ko na kr nb kv nc kz nd ld mr ms mt mu bi translated">由于修剪选择了最佳的交叉验证子树，所以被修剪的树倾向于很好地适合数据。</li></ul><p id="d048" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">预修剪和后修剪的缺点:</strong></p><ul class=""><li id="8c07" class="mm mn it kk b kl km ko kp kr mo kv mp kz mq ld mr ms mt mu bi translated">与原始决策树相比，没有缺点—如果修剪没有帮助，交叉验证的网格搜索可以选择原始树。</li><li id="7759" class="mm mn it kk b kl mz ko na kr nb kv nc kz nd ld mr ms mt mu bi translated">与诸如随机森林和AdaBoost之类的集成树模型相比，修剪后的树往往得不到同样的分数。</li></ul><p id="66d1" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">预修剪的优点</strong></p><ul class=""><li id="1cb6" class="mm mn it kk b kl km ko kp kr mo kv mp kz mq ld mr ms mt mu bi translated">与后期修剪相比，前期修剪速度更快。这在较大(更多要素或更多数据)的数据集上尤其重要，因为后期修剪必须评估非常大的树子集。</li><li id="77c5" class="mm mn it kk b kl mz ko na kr nb kv nc kz nd ld mr ms mt mu bi translated">预修剪有时可以获得与后修剪相似甚至更好的结果。</li></ul><p id="3459" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">后期修剪的优势</strong></p><ul class=""><li id="78d3" class="mm mn it kk b kl km ko kp kr mo kv mp kz mq ld mr ms mt mu bi translated">后修剪通常产生比预修剪更好的树，因为预修剪是贪婪的，并且可能忽略具有后续重要分裂的分裂。</li></ul><p id="9923" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">要了解更多关于决策树是如何生长的，请查看这篇文章:</p><div class="lv lw gp gr lx ly"><a rel="noopener follow" target="_blank" href="/learn-how-decision-trees-are-grown-22bc3d22fb51"><div class="lz ab fo"><div class="ma ab mb cl cj mc"><h2 class="bd iu gy z fp md fr fs me fu fw is bi translated">了解决策树是如何生长的</h2><div class="mf l"><p class="bd b dl z fp md fr fs me fu fw dk translated">towardsdatascience.com</p></div></div><div class="mg l"><div class="mh l mi mj mk mg ml lp ly"/></div></div></a></div><p id="40f6" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如果您对决策树的概述以及如何手工拟合决策树的演示感兴趣，请查看Arif R 撰写的这篇文章:</p><div class="lv lw gp gr lx ly"><a href="https://medium.datadriveninvestor.com/the-basics-of-decision-trees-e5837cc2aba7" rel="noopener  ugc nofollow" target="_blank"><div class="lz ab fo"><div class="ma ab mb cl cj mc"><h2 class="bd iu gy z fp md fr fs me fu fw is bi translated">决策树的基础</h2><div class="on l"><h3 class="bd b gy z fp md fr fs me fu fw dk translated">决策树算法-第1部分</h3></div><div class="mf l"><p class="bd b dl z fp md fr fs me fu fw dk translated">medium.datadriveninvestor.com</p></div></div><div class="mg l"><div class="or l mi mj mk mg ml lp ly"/></div></div></a></div></div></div>    
</body>
</html>