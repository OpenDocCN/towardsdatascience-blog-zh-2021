<html>
<head>
<title>Scorecard Development for Finance Industry Using PyCaret — Part 1</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用PyCaret为金融行业开发记分卡—第1部分</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/scorecard-development-for-finance-industry-using-pycaret-part-1-8277c761160a?source=collection_archive---------30-----------------------#2021-06-24">https://towardsdatascience.com/scorecard-development-for-finance-industry-using-pycaret-part-1-8277c761160a?source=collection_archive---------30-----------------------#2021-06-24</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="05e6" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">关于使用轻量级编码工作流开发分类模型的详细信息</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi kf"><img src="../Images/5a40134962947ec9d4e5bc23b3cf4ad4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*HB7MrTgoSrEoV_7kn6mLyw.jpeg"/></div><p class="kn ko gj gh gi kp kq bd b be z dk translated">照片由Ameen Fahmy (unsplash)拍摄</p></figure><p id="d42c" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">在本文中，我将尝试利用机器学习库PyCaret来描述银行业的端到端记分卡开发。我第一次接触记分卡开发发生在大约12年前，当时我开发了一个倾向记分卡，其目标是预测更有可能购买某个特定银行产品的客户。我利用SAS和SAS EMINER运行了一个逻辑回归模型，整个过程花了将近三周时间！！！</p><p id="1a22" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">随着复杂的机器学习算法的出现，我开始使用不同的R/Python包，编写冗长的代码，以获得记分卡开发的最佳模型。然而，挑战在于为各种算法做不同类型的数据准备。</p><p id="ba14" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">当涉及到开发一个具有模型可解释性的机器学习算法驱动的记分卡时——py caret就是救世主。这个<em class="ln">低代码</em>库可以用来执行复杂的机器学习任务——最近构建了一个记分卡，只花了一个小时就完成了。0</p><p id="76b5" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated"><strong class="kt ir">实际使用案例——开发记分卡，其中较低的分数意味着客户信用卡违约的可能性较高:</strong></p><p id="81f5" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">为了开发解决方案，使用的数据集来自Kaggle: <a class="ae lo" href="https://www.kaggle.com/uciml/default-of-credit-card-clients-dataset?select=UCI_Credit_Card.csv" rel="noopener ugc nofollow" target="_blank">这里是</a>(虽然数据集包含25列，但在实际用例中，考虑了2000多个特性。记住这一点，让我们看看下面的方法)</p><p id="7425" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated"><strong class="kt ir">步骤1:安装练习所需的软件包:</strong></p><pre class="kg kh ki kj gt lp lq lr ls aw lt bi"><span id="3b52" class="lu lv iq lq b gy lw lx l ly lz">pip install llvmlite -U --ignore-installed<br/>pip install -U setuptools<br/>pip install -U pip<br/>pip install pycaret==2.3.1<br/>pip install pandasql<br/>pip install matplotlib<br/>pip install shap<br/>pip install seaborn<br/>pip install sweetviz</span><span id="140d" class="lu lv iq lq b gy ma lx l ly lz"><strong class="lq ir">from</strong> <strong class="lq ir">sklearn.metrics</strong> <strong class="lq ir">import</strong> roc_auc_score,balanced_accuracy_score, f1_score, accuracy_score<br/><strong class="lq ir">from</strong> <strong class="lq ir">itertools</strong> <strong class="lq ir">import</strong> combinations, chain<br/><strong class="lq ir">from</strong> <strong class="lq ir">pandas._libs.lib</strong> <strong class="lq ir">import</strong> is_integer<br/><strong class="lq ir">from</strong> <strong class="lq ir">pycaret.classification</strong> <strong class="lq ir">import</strong> *<br/><strong class="lq ir">import</strong> <strong class="lq ir">matplotlib.patches</strong> <strong class="lq ir">as</strong> <strong class="lq ir">patches</strong><br/><strong class="lq ir">import</strong> <strong class="lq ir">matplotlib.ticker</strong> <strong class="lq ir">as</strong> <strong class="lq ir">mtick</strong><br/><strong class="lq ir">import</strong> <strong class="lq ir">matplotlib.pyplot</strong> <strong class="lq ir">as</strong> <strong class="lq ir">plt<br/>import</strong> <strong class="lq ir">seaborn</strong> <strong class="lq ir">as</strong> <strong class="lq ir">sns</strong><br/><strong class="lq ir">import</strong> <strong class="lq ir">sweetviz</strong> <strong class="lq ir">as</strong> <strong class="lq ir">sv</strong><br/><strong class="lq ir">import</strong> <strong class="lq ir">pandasql</strong> <strong class="lq ir">as</strong> <strong class="lq ir">ps</strong><br/><strong class="lq ir">import</strong> <strong class="lq ir">pandas</strong> <strong class="lq ir">as</strong> <strong class="lq ir">pd</strong><br/><strong class="lq ir">import</strong> <strong class="lq ir">numpy</strong> <strong class="lq ir">as</strong> <strong class="lq ir">np</strong><br/><em class="ln">import shap</em><br/><strong class="lq ir">import</strong> <strong class="lq ir">math</strong></span></pre><p id="0e1e" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated"><strong class="kt ir">第二步:导入数据(此处来自google bucket)和EDA </strong></p><pre class="kg kh ki kj gt lp lq lr ls aw lt bi"><span id="9182" class="lu lv iq lq b gy lw lx l ly lz">path=’gs://pt-test/UCI_Credit_Card.csv'<br/>raw = pd.read_csv(path, encoding = 'cp1252’)<br/>print(raw.shape)</span><span id="0a3c" class="lu lv iq lq b gy ma lx l ly lz">##output<br/>(30000, 25)</span><span id="6174" class="lu lv iq lq b gy ma lx l ly lz">##Let’s drop the variable gender ( Sex) as we don’t want to discriminate between male and female -</span><span id="8215" class="lu lv iq lq b gy ma lx l ly lz">dataset_raw = raw.drop(columns =['SEX'])<br/>print(dataset_raw.shape)</span><span id="0d20" class="lu lv iq lq b gy ma lx l ly lz">##output<br/>(30000, 24)</span></pre><p id="ea15" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">使用一行代码运行EDA，并使用sweetviz生成EDA报告:</p><pre class="kg kh ki kj gt lp lq lr ls aw lt bi"><span id="7e73" class="lu lv iq lq b gy lw lx l ly lz">feature_config = sv.FeatureConfig(skip=['ID']) # remove the feature that you dont want to include in the EDA<br/>my_report = sv.analyze(dataset_raw, "default.payment.next.month", feature_config)<br/>my_report.show_html()</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mb"><img src="../Images/5604f4934f35a59b3622f1b2a856aeb2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1226/format:webp/1*1HdMcG4Pi7J_s0fLhxYydw.png"/></div><p class="kn ko gj gh gi kp kq bd b be z dk translated">EDA输出:顶部的黑色突出显示目标变量</p></figure><p id="8fb2" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated"><strong class="kt ir">第三步:PyCaret的数据预处理和设置</strong></p><ol class=""><li id="d606" class="mc md iq kt b ku kv kx ky la me le mf li mg lm mh mi mj mk bi translated">识别数字和分类特征</li><li id="2ea2" class="mc md iq kt b ku ml kx mm la mn le mo li mp lm mh mi mj mk bi translated">通过平均值估算数值缺失</li><li id="7b35" class="mc md iq kt b ku ml kx mm la mn le mo li mp lm mh mi mj mk bi translated">按模式估算分类缺失</li><li id="de99" class="mc md iq kt b ku ml kx mm la mn le mo li mp lm mh mi mj mk bi translated">移除异常值—设置5%的阈值</li><li id="fe5c" class="mc md iq kt b ku ml kx mm la mn le mo li mp lm mh mi mj mk bi translated">训练数据占80%，测试数据占20%</li><li id="b767" class="mc md iq kt b ku ml kx mm la mn le mo li mp lm mh mi mj mk bi translated">移除多重共线性</li></ol><pre class="kg kh ki kj gt lp lq lr ls aw lt bi"><span id="bd87" class="lu lv iq lq b gy lw lx l ly lz">cat_feat = list(dataset_raw.select_dtypes(include=['object']).columns)<br/>int_feat = list(dataset_raw.select_dtypes(include=['int64','float64','float32']).columns)<br/>int_feat.remove('default.payment.next.month')<br/>print(cat_feat)</span><span id="e087" class="lu lv iq lq b gy ma lx l ly lz">##output<br/>[] - here we dont have categorical feature</span><span id="496d" class="lu lv iq lq b gy ma lx l ly lz">#setting up the environment:<br/>clf = setup(dataset_raw<br/>            ,target = 'default.payment.next.month'<br/>            ,ignore_features = ['ID']  #ignored from model training<br/>            ,numeric_imputation = 'mean'<br/>            ,categorical_imputation = 'mode'<br/>            ,categorical_features = cat_feat<br/>            ,numeric_features = int_feat<br/>            ,remove_outliers = True<br/>            ,train_size = 0.80            <br/>            ,session_id = 1988            <br/>           )</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mq"><img src="../Images/08c6d6ac0833d4359c71e1483fc56f13.png" data-original-src="https://miro.medium.com/v2/resize:fit:818/format:webp/1*nM0YbOVeGkbU65NdZkRmHw.png"/></div><p class="kn ko gj gh gi kp kq bd b be z dk translated">CLF输出一瞥:它有一长串不同的选项</p></figure><p id="900b" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated"><strong class="kt ir">步骤4:运行比较模型，选择前n个特征</strong></p><p id="5cd2" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">该步骤目前正在运行，主要确定最重要的功能。一般来说，我们有超过2000个特征(如客户人口统计，银行，竞争对手信息等)来开始-我们运行compre_model选项来获得最佳模型，并使用它来进行特征选择。特征选择也可以在CLF步骤中完成，但是在没有gpu的情况下需要花费大量时间来获得结果。</p><pre class="kg kh ki kj gt lp lq lr ls aw lt bi"><span id="a5aa" class="lu lv iq lq b gy lw lx l ly lz">base_model = compare_models(fold = 5,sort = 'AUC', n_select = 1)</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mr"><img src="../Images/54af7ca13b816845456134f4e6b8cf01.png" data-original-src="https://miro.medium.com/v2/resize:fit:1356/format:webp/1*iUE-RlSete8WhI_L0XH46g.png"/></div><p class="kn ko gj gh gi kp kq bd b be z dk translated">基于AUC排序的模型列表的输出:梯度增强给出最高的AUC</p></figure><p id="429f" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">现在，假设最佳模型是基于AUC的梯度提升分类器，我们将利用它来获得前n个特征，一般来说，从记分卡的角度来看，它的范围是25-30个特征(我们从2000个特征开始)。在这个虚拟数据集中，我们将选择前10个特征。</p><pre class="kg kh ki kj gt lp lq lr ls aw lt bi"><span id="ba1b" class="lu lv iq lq b gy lw lx l ly lz">n=10<br/>X_train=get_config('X_train') <br/>feature_names = pd.DataFrame(X_train.columns)<br/>feature_names['var'] = feature_names<br/>feature_imp = pd.DataFrame(base_model.feature_importances_)<br/>feature_imp['imp'] = feature_imp<br/>var_imp = pd.concat([feature_names,feature_imp],axis=1)<br/>var_imp = var_imp[['var', 'imp']]<br/>var_imp = var_imp.sort_values(['imp'],ascending=False)<br/>var_imp_fin=var_imp['var'].head(n).values.tolist()<br/>print(var_imp_fin)</span><span id="a11a" class="lu lv iq lq b gy ma lx l ly lz">##output<br/>['PAY_0', 'PAY_2', 'PAY_3', 'BILL_AMT1', 'LIMIT_BAL', 'PAY_AMT1', 'PAY_6', 'PAY_4', 'PAY_AMT3', 'PAY_AMT2']</span></pre><p id="0146" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated"><strong class="kt ir">步骤5:再次运行CLF，然后微调模型</strong></p><ol class=""><li id="8aa9" class="mc md iq kt b ku kv kx ky la me le mf li mg lm mh mi mj mk bi translated">用选定的重要变量对数据进行子集划分</li><li id="3f91" class="mc md iq kt b ku ml kx mm la mn le mo li mp lm mh mi mj mk bi translated">运行比较模型</li><li id="7b90" class="mc md iq kt b ku ml kx mm la mn le mo li mp lm mh mi mj mk bi translated">调整模型，如果需要，自定义网格搜索</li><li id="a359" class="mc md iq kt b ku ml kx mm la mn le mo li mp lm mh mi mj mk bi translated">对模型评分并评估</li></ol><pre class="kg kh ki kj gt lp lq lr ls aw lt bi"><span id="2415" class="lu lv iq lq b gy lw lx l ly lz">dataset_raw = raw[var_imp_fin + ['ID','default.payment.next.month']]</span><span id="ddfd" class="lu lv iq lq b gy ma lx l ly lz">cat_feat = list(dataset_raw.select_dtypes(include=['object']).columns)<br/>int_feat = list(dataset_raw.select_dtypes(include=['int64','float64','float32']).columns)<br/>int_feat.remove('default.payment.next.month')</span><span id="8d78" class="lu lv iq lq b gy ma lx l ly lz">clf = setup(dataset_raw<br/>            ,target = 'default.payment.next.month'<br/>            ,ignore_features = ['ID']  <br/>            ,numeric_imputation = 'mean'<br/>            ,categorical_imputation = 'mode'<br/>            ,categorical_features = cat_feat<br/>            ,numeric_features = int_feat<br/>            ,remove_outliers = True<br/>            ,train_size = 0.80            <br/>            ,session_id = 1988            <br/>           )</span><span id="d77e" class="lu lv iq lq b gy ma lx l ly lz">base_model2 = compare_models(fold = 5,sort = 'AUC', n_select = 1)</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ms"><img src="../Images/0cd443ce5ec773ff4ef21c1ca5135d09.png" data-original-src="https://miro.medium.com/v2/resize:fit:1330/format:webp/1*kocOTy-xw7bJZYVlWvSW7g.png"/></div><p class="kn ko gj gh gi kp kq bd b be z dk translated">根据AUC，gbc再次位居榜首</p></figure><p id="84d0" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">梯度增强分类器在AUC方面再次高居榜首，然而需要注意的一点是AUC已经从0.7788下降到0.7687，这是由于特征数量从25减少到10。然而，在一个更大的方面，这也可能有助于您确定您希望最终在模型中保留多少特征，因为您不想在AUC上损失太多。</p><p id="2e3d" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">当compare_model使用预定义的超参数运行模型时，自动调整模型的超参数:</p><pre class="kg kh ki kj gt lp lq lr ls aw lt bi"><span id="6cea" class="lu lv iq lq b gy lw lx l ly lz">model_tune_gbc = tune_model(base_model2, n_iter=5, optimize='AUC')</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mt"><img src="../Images/2b5f8d1b26c195afdc7dc0bbc4b732c3.png" data-original-src="https://miro.medium.com/v2/resize:fit:790/format:webp/1*jq7f58l-xDiUst6gR3bVxw.png"/></div><p class="kn ko gj gh gi kp kq bd b be z dk translated">模型调整后测试数据集的平均AUC</p></figure><p id="f929" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated"><strong class="kt ir">第六步:对训练、测试和整个数据集进行评分，并比较基尼系数</strong></p><pre class="kg kh ki kj gt lp lq lr ls aw lt bi"><span id="4cef" class="lu lv iq lq b gy lw lx l ly lz">def score(main_data,model_name):<br/>    train = get_config('X_train').index<br/>    test = get_config('X_test').index<br/>    predict = predict_model(model_name,main_data,raw_score=True)<br/>    predict['odds'] = predict['Score_1']/predict['Score_0']<br/>    predict['score'] = 200 + 28.8539*np.log(predict['odds'])<br/>    predict['score'] = predict.score.round(0).astype(int)<br/>    predict_train = predict.loc[train]<br/>    predict_test = predict.loc[test]<br/>    auc_train = roc_auc_score(predict_train['default.payment.next.month'], predict_train['Score_1'])<br/>    print('Gini_train: %.3f' % (2*auc_train-1))<br/>    auc_test = roc_auc_score(predict_test['default.payment.next.month'], predict_test['Score_1'])<br/>    print('Gini_test: %.3f' % (2*auc_test-1))<br/>    return predict,predict_train,predict_test</span><span id="2a4a" class="lu lv iq lq b gy ma lx l ly lz">#call the function<br/>scr_all1,scr_train,scr_test = score(dataset_raw,base_model2)</span><span id="d79e" class="lu lv iq lq b gy ma lx l ly lz">#output<br/>Gini_train: 0.636<br/>Gini_test: 0.565</span></pre><p id="0bdb" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">如果你看到上面的结果，训练和测试基尼系数之间的差异超过了10%,因此进行自定义网格搜索以将差距缩小到10%以下是有意义的</p><p id="cba7" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">a)首先打印最终模型超参数-</p><pre class="kg kh ki kj gt lp lq lr ls aw lt bi"><span id="0b25" class="lu lv iq lq b gy lw lx l ly lz">print(model_tune_gbc)</span><span id="ecea" class="lu lv iq lq b gy ma lx l ly lz">##output <br/>GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,learning_rate=0.1, loss='deviance', max_depth=6,<br/>max_features='log2', max_leaf_nodes=None,                           min_impurity_decrease=0.002, min_impurity_split=None,                           min_samples_leaf=4, min_samples_split=5,                          min_weight_fraction_leaf=0.0, n_estimators=70,                           n_iter_no_change=None, presort='deprecated',                           random_state=1988, subsample=0.35, tol=0.0001,                           validation_fraction=0.1, verbose=0,                        warm_start=False)</span></pre><p id="5e0f" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">在查看了超参数值之后，我将尝试一下n估计值和学习率</p><pre class="kg kh ki kj gt lp lq lr ls aw lt bi"><span id="fc29" class="lu lv iq lq b gy lw lx l ly lz">params = {'n_estimators':[30,40,50,60],<br/>          'learning_rate':[0.05,0.2]<br/>           }<br/>gbc_custom = tune_model(model_tune_gbc,custom_grid=params)</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mu"><img src="../Images/e6a4d1b62e157e40c551cbcfe02904d8.png" data-original-src="https://miro.medium.com/v2/resize:fit:804/format:webp/1*yc53YG31MeoMeLGppdq1Mw.png"/></div><p class="kn ko gj gh gi kp kq bd b be z dk translated">自定义网格搜索的输出</p></figure><p id="9e94" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">重新计算分数，检查基尼系数的差异-</p><pre class="kg kh ki kj gt lp lq lr ls aw lt bi"><span id="c918" class="lu lv iq lq b gy lw lx l ly lz">scr_all,scr_train,scr_test = score(dataset_raw,gbc_custom)</span><span id="7a97" class="lu lv iq lq b gy ma lx l ly lz">##output<br/>Gini_train: 0.593<br/>Gini_test: 0.576</span></pre><p id="0357" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">正如你所看到的，训练和测试的基尼系数相差不到3%</p><p id="56fe" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated"><strong class="kt ir">第七步:保存所有相关的数据集和模型对象</strong></p><pre class="kg kh ki kj gt lp lq lr ls aw lt bi"><span id="4515" class="lu lv iq lq b gy lw lx l ly lz">#final model<br/>scr_all1,scr_train,scr_test = score(dataset_raw,gbc_custom)<br/>scr_all1.to_csv('scr_all_gbc_custom.csv')<br/>scr_train.to_csv('scr_train_gbc_custom.csv')<br/>scr_test.to_csv('scr_test_gbc_custom.csv')<br/>save_model(gbc_custom,'gbc_custom')</span></pre><p id="530d" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">该模型将保存为管道，输出如下所示:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mv"><img src="../Images/9db78cf3b5192b88481e8ec64f538c92.png" data-original-src="https://miro.medium.com/v2/resize:fit:1100/format:webp/1*ryBFyrb3R6QhIVqW0gu0JA.png"/></div><p class="kn ko gj gh gi kp kq bd b be z dk translated">已保存模型的输出</p></figure><p id="ec8b" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">在下一部分，我将详细介绍模型评分、模型评估、作为模型文档一部分的不同指标，如稳定性、基尼系数、增益矩阵、训练和测试数据集的等级排序。</p></div></div>    
</body>
</html>