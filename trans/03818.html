<html>
<head>
<title>The Dying ReLU Problem, Clearly Explained</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">垂死的雷鲁问题，解释得很清楚</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/the-dying-relu-problem-clearly-explained-42d0c54e0d24?source=collection_archive---------4-----------------------#2021-03-30">https://towardsdatascience.com/the-dying-relu-problem-clearly-explained-42d0c54e0d24?source=collection_archive---------4-----------------------#2021-03-30</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="7fdd" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsdatascience.com/tagged/thoughts-and-theory" rel="noopener" target="_blank">思想和理论</a></h2><div class=""/><div class=""><h2 id="cad5" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">通过理解ReLU的缺点来保持你的神经网络的活力</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/9af2afcab9ff3f0ecf7f1264286dbe02.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*w80ldgn1iri7dhsM"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">照片由<a class="ae lh" href="https://unsplash.com/@solenfeyissa?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">索伦·费伊萨</a>在<a class="ae lh" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><h1 id="d177" class="li lj it bd lk ll lm ln lo lp lq lr ls ki lt kj lu kl lv km lw ko lx kp ly lz bi translated">内容</h1><blockquote class="ma mb mc"><p id="27f1" class="md me mf mg b mh mi kd mj mk ml kg mm mn mo mp mq mr ms mt mu mv mw mx my mz im bi translated"><em class="it">(</em><strong class="mg jd"><em class="it">1</em></strong><em class="it">)</em><a class="ae lh" href="#0ebe" rel="noopener ugc nofollow"><em class="it">什么是ReLU，它的优点是什么？</em></a><em class="it"><br/>(</em><strong class="mg jd"><em class="it">2</em></strong><em class="it">)</em><a class="ae lh" href="#4995" rel="noopener ugc nofollow"><em class="it">有什么垂死的热路问题？</em></a><em class="it"><br/>(</em><strong class="mg jd"><em class="it">3</em></strong><em class="it">)</em><a class="ae lh" href="#7696" rel="noopener ugc nofollow"><em class="it">什么原因导致了将死的热路问题？</em></a><em class="it"><br/>(</em><strong class="mg jd"><em class="it">4</em></strong><em class="it">)</em><a class="ae lh" href="#0863" rel="noopener ugc nofollow"><em class="it">如何解决将死的热路问题？</em>T56】</a></p></blockquote></div><div class="ab cl na nb hx nc" role="separator"><span class="nd bw bk ne nf ng"/><span class="nd bw bk ne nf ng"/><span class="nd bw bk ne nf"/></div><div class="im in io ip iq"><p id="f122" class="pw-post-body-paragraph md me it mg b mh mi kd mj mk ml kg mm nh mo mp mq ni ms mt mu nj mw mx my mz im bi translated">激活函数是定义如何将神经节点输入的加权和转换为输出的数学方程，并且它们是人工神经网络(ANN)架构的关键部分。</p><p id="e7e5" class="pw-post-body-paragraph md me it mg b mh mi kd mj mk ml kg mm nh mo mp mq ni ms mt mu nj mw mx my mz im bi translated">激活函数将<strong class="mg jd">非线性</strong>添加到神经网络中，允许网络学习数据中的复杂模式。激活函数的选择对人工神经网络的性能有着重要的影响，最受欢迎的选择之一是<strong class="mg jd">Re</strong>ctived<strong class="mg jd">L</strong>linear<strong class="mg jd">U</strong>nit(ReLU)。</p></div><div class="ab cl na nb hx nc" role="separator"><span class="nd bw bk ne nf ng"/><span class="nd bw bk ne nf ng"/><span class="nd bw bk ne nf"/></div><div class="im in io ip iq"><h1 id="0ebe" class="li lj it bd lk ll nk ln lo lp nl lr ls ki nm kj lu kl nn km lw ko no kp ly lz bi translated">什么是ReLU，它的优点是什么？</h1><p id="989f" class="pw-post-body-paragraph md me it mg b mh np kd mj mk nq kg mm nh nr mp mq ni ns mt mu nj nt mx my mz im bi translated">整流线性单元(ReLU)激活函数可描述为:</p><h2 id="1c09" class="nu lj it bd lk nv nw dn lo nx ny dp ls nh nz oa lu ni ob oc lw nj od oe ly iz bi translated"><strong class="ak"> <em class="of"> f(x) = max(0，x) </em> </strong></h2><p id="9612" class="pw-post-body-paragraph md me it mg b mh np kd mj mk nq kg mm nh nr mp mq ni ns mt mu nj nt mx my mz im bi translated">它的作用是:<br/> <strong class="mg jd"> (i) </strong>对于负输入值，输出= 0 <br/> <strong class="mg jd"> (ii) </strong>对于正输入值，输出=原始输入值</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi og"><img src="../Images/8fc41ae072e155949eee4d5c00f8bf42.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-DjRsec_Iqe6c2obsLiCxQ.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">ReLU激活功能的图示</p></figure><p id="9d9c" class="pw-post-body-paragraph md me it mg b mh mi kd mj mk ml kg mm nh mo mp mq ni ms mt mu nj mw mx my mz im bi translated">ReLU因为几个关键的优势而广受欢迎:</p><ul class=""><li id="a072" class="oh oi it mg b mh mi mk ml nh oj ni ok nj ol mz om on oo op bi translated">ReLU比其他常见的激活函数(例如，<em class="mf"> tanh </em>、<em class="mf"> sigmoid </em>)花费更少的学习时间和更少的计算开销。因为每当其输入为负时，它输出0，较少的神经元将被激活，导致<strong class="mg jd">网络稀疏</strong>，从而更高的<strong class="mg jd">计算效率</strong>。</li><li id="889f" class="oh oi it mg b mh oq mk or nh os ni ot nj ou mz om on oo op bi translated">与<em class="mf"> tanh </em>和<em class="mf"> sigmoid </em>相比，ReLU包含了<strong class="mg jd">更简单的数学运算</strong>，从而进一步提升了它的计算性能。</li><li id="f114" class="oh oi it mg b mh oq mk or nh os ni ot nj ou mz om on oo op bi translated"><em class="mf"> tanh </em>和<em class="mf"> sigmoid </em>函数容易出现<strong class="mg jd">消失梯度</strong>问题，梯度在反向传播中急剧收缩，使得网络不再能够学习。ReLU通过保留梯度来避免这一点，因为:<br/> <strong class="mg jd"> (i) </strong>它的线性部分(在正输入范围内)允许梯度在神经元的活动路径上很好地流动，并保持与节点激活成比例<br/> <strong class="mg jd"> (ii) </strong>它是一个无界函数(即，没有最大值)。</li></ul></div><div class="ab cl na nb hx nc" role="separator"><span class="nd bw bk ne nf ng"/><span class="nd bw bk ne nf ng"/><span class="nd bw bk ne nf"/></div><div class="im in io ip iq"><h1 id="4995" class="li lj it bd lk ll nk ln lo lp nl lr ls ki nm kj lu kl nn km lw ko no kp ly lz bi translated">垂死的ReLU问题是什么？</h1><p id="651f" class="pw-post-body-paragraph md me it mg b mh np kd mj mk nq kg mm nh nr mp mq ni ns mt mu nj nt mx my mz im bi translated">垂死的ReLU问题是指很多ReLU神经元只输出0值的场景。下面的红色轮廓表示当输入在负<strong class="mg jd">范围内时会发生这种情况。</strong></p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ov"><img src="../Images/9e702e9386af272696962b42cdf7008d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*r_fTwA86CGc6iqFQlVjZ-g.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">红色轮廓(在负x范围内)划分ReLU输出0的水平段</p></figure><p id="acc4" class="pw-post-body-paragraph md me it mg b mh mi kd mj mk ml kg mm nh mo mp mq ni ms mt mu nj mw mx my mz im bi translated">虽然这一特性赋予了ReLU其优势(通过网络稀疏性)，但当这些ReLU神经元的大部分输入都在负范围内时，这就成了一个问题。最糟糕的情况是整个网络都瘫痪了，这意味着它只是一个常量函数。</p><p id="34d3" class="pw-post-body-paragraph md me it mg b mh mi kd mj mk ml kg mm nh mo mp mq ni ms mt mu nj mw mx my mz im bi translated">当这些神经元中的大多数返回输出0时，梯度在反向传播期间无法流动，并且权重没有更新。最终，网络的很大一部分变得不活跃，无法进一步学习。</p><p id="9d73" class="pw-post-body-paragraph md me it mg b mh mi kd mj mk ml kg mm nh mo mp mq ni ms mt mu nj mw mx my mz im bi translated">因为ReLU在负<strong class="mg jd"> </strong>输入范围内的斜率也为零，一旦它变成死的(即卡在负范围内，给出输出0)，很可能保持不可恢复。</p><p id="6d90" class="pw-post-body-paragraph md me it mg b mh mi kd mj mk ml kg mm nh mo mp mq ni ms mt mu nj mw mx my mz im bi translated">然而，由于优化器(例如，随机梯度下降)每次都考虑多个输入值，所以垂死ReLU问题并不总是发生。<strong class="mg jd">只要不是所有的输入</strong>将ReLU推至负段(即，一些输入在正范围内)，神经元就可以保持活动，权重可以得到更新，并且网络可以继续学习。</p></div><div class="ab cl na nb hx nc" role="separator"><span class="nd bw bk ne nf ng"/><span class="nd bw bk ne nf ng"/><span class="nd bw bk ne nf"/></div><div class="im in io ip iq"><h1 id="7696" class="li lj it bd lk ll nk ln lo lp nl lr ls ki nm kj lu kl nn km lw ko no kp ly lz bi translated">是什么导致了将死的ReLU问题？</h1><p id="0c2d" class="pw-post-body-paragraph md me it mg b mh np kd mj mk nq kg mm nh nr mp mq ni ns mt mu nj nt mx my mz im bi translated">垂死的ReLU问题通常由以下两个因素驱动:</p><h2 id="41f3" class="nu lj it bd lk nv nw dn lo nx ny dp ls nh nz oa lu ni ob oc lw nj od oe ly iz bi translated">㈠高学习率</h2><p id="f6d4" class="pw-post-body-paragraph md me it mg b mh np kd mj mk nq kg mm nh nr mp mq ni ns mt mu nj nt mx my mz im bi translated">让我们首先看看反向传播中更新步骤的等式:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ow"><img src="../Images/feb40e21ff760f9959f848798aa1f4ae.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xp_bE0C7le8mNF-JE7OAhA.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">更新规则的等式(图片由作者提供)</p></figure><p id="c664" class="pw-post-body-paragraph md me it mg b mh mi kd mj mk ml kg mm nh mo mp mq ni ms mt mu nj mw mx my mz im bi translated">如果我们的学习率(<strong class="mg jd"> α </strong>)设置得太高，我们的新权重很有可能会落在高度负值的范围内，因为我们的旧权重会被减去一个大的数字。这些负权重导致ReLU的负输入，从而导致即将发生的ReLU问题。</p><p id="73ea" class="pw-post-body-paragraph md me it mg b mh mi kd mj mk ml kg mm nh mo mp mq ni ms mt mu nj mw mx my mz im bi translated"><em class="mf">注:回想激活函数的输入是(W</em><strong class="mg jd"><em class="mf">*</em></strong><em class="mf">x)+b .</em></p><h2 id="2513" class="nu lj it bd lk nv nw dn lo nx ny dp ls nh nz oa lu ni ob oc lw nj od oe ly iz bi translated">㈡大的负偏差</h2><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ox"><img src="../Images/9d747c0c8ce6f3924bd0bb1f7185bd0b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VmV4xp0m_RXnGD6Ha-3-6Q.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">简单神经网络的图解(图片由作者提供)</p></figure><p id="26b9" class="pw-post-body-paragraph md me it mg b mh mi kd mj mk ml kg mm nh mo mp mq ni ms mt mu nj mw mx my mz im bi translated">虽然到目前为止我们主要讨论了权重，但是我们不能忘记偏差项也是和权重一起传递到激活函数中的。</p><p id="a2aa" class="pw-post-body-paragraph md me it mg b mh mi kd mj mk ml kg mm nh mo mp mq ni ms mt mu nj mw mx my mz im bi translated">偏差是添加到输入和权重乘积中的常量值。考虑到这一点，较大的负偏置项会导致ReLU激活输入变为负值。如前所述，这会导致神经元持续输出0，从而导致死亡的ReLU问题。</p></div><div class="ab cl na nb hx nc" role="separator"><span class="nd bw bk ne nf ng"/><span class="nd bw bk ne nf ng"/><span class="nd bw bk ne nf"/></div><div class="im in io ip iq"><h1 id="0863" class="li lj it bd lk ll nk ln lo lp nl lr ls ki nm kj lu kl nn km lw ko no kp ly lz bi translated">如何解决垂死的热卢问题？</h1><p id="5842" class="pw-post-body-paragraph md me it mg b mh np kd mj mk nq kg mm nh nr mp mq ni ns mt mu nj nt mx my mz im bi translated">有几种方法可以解决日益严重的ReLU问题:</p><h2 id="1a12" class="nu lj it bd lk nv nw dn lo nx ny dp ls nh nz oa lu ni ob oc lw nj od oe ly iz bi translated">(I)使用<strong class="ak">较低的学习率</strong></h2><p id="d854" class="pw-post-body-paragraph md me it mg b mh np kd mj mk nq kg mm nh nr mp mq ni ns mt mu nj nt mx my mz im bi translated">由于较大的学习率会导致较高的负权重可能性(从而增加死亡的几率)，所以在训练过程中降低学习率可能是个好主意。</p><h2 id="6747" class="nu lj it bd lk nv nw dn lo nx ny dp ls nh nz oa lu ni ob oc lw nj od oe ly iz bi translated">(二)ReLU的变更</h2><p id="d452" class="pw-post-body-paragraph md me it mg b mh np kd mj mk nq kg mm nh nr mp mq ni ns mt mu nj nt mx my mz im bi translated">由于负输入范围内的平坦部分会导致即将到来的ReLU问题，自然会考虑调整该平坦部分的ReLU变化。</p><p id="0b6f" class="pw-post-body-paragraph md me it mg b mh mi kd mj mk ml kg mm nh mo mp mq ni ms mt mu nj mw mx my mz im bi translated"><strong class="mg jd">漏重流</strong>是解决垂死重流问题的常用有效方法，它通过在负范围内增加一个微小的斜率来实现。这将修改函数，以便在输入小于0时生成较小的负输出。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi og"><img src="../Images/31b3e16938b635cfd3c15bba640f7295.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CszCyqBQJPi15PRtAhlXHQ.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">ReLU和Leaky ReLU的图形比较(图片由作者提供)</p></figure><p id="a384" class="pw-post-body-paragraph md me it mg b mh mi kd mj mk ml kg mm nh mo mp mq ni ms mt mu nj mw mx my mz im bi translated">还有其他变化，如参数ReLU (PReLU)、指数线性单位(eLU)和高斯误差线性单位(GELU)。它们的细节超出了本文的范围，但是它们都有一个共同的目标，即通过避免零斜率段来防止垂死的ReLU问题。</p><h2 id="c689" class="nu lj it bd lk nv nw dn lo nx ny dp ls nh nz oa lu ni ob oc lw nj od oe ly iz bi translated">㈢修改初始化程序</h2><p id="a41c" class="pw-post-body-paragraph md me it mg b mh np kd mj mk nq kg mm nh nr mp mq ni ns mt mu nj nt mx my mz im bi translated">初始化神经网络的权重和偏差的常见方式是通过对称概率分布(例如，He初始化)。然而，这种方法容易因不良的局部极小值而导致死亡ReLU问题。</p><p id="cf54" class="pw-post-body-paragraph md me it mg b mh mi kd mj mk ml kg mm nh mo mp mq ni ms mt mu nj mw mx my mz im bi translated">已经证明，使用一个<strong class="mg jd">随机不对称初始化</strong>可以帮助防止死ReLU问题。请务必查看arXiv论文中的数学细节。</p></div><div class="ab cl na nb hx nc" role="separator"><span class="nd bw bk ne nf ng"/><span class="nd bw bk ne nf ng"/><span class="nd bw bk ne nf"/></div><div class="im in io ip iq"><h1 id="be6c" class="li lj it bd lk ll nk ln lo lp nl lr ls ki nm kj lu kl nn km lw ko no kp ly lz bi translated">结论</h1><p id="ef9c" class="pw-post-body-paragraph md me it mg b mh np kd mj mk nq kg mm nh nr mp mq ni ns mt mu nj nt mx my mz im bi translated">随着ReLU广泛应用于流行的神经网络，如多层感知器和卷积神经网络，本文提出了理论概念，实际意义，和潜在的解决方案的垂死ReLU问题。</p><h1 id="7641" class="li lj it bd lk ll lm ln lo lp lq lr ls ki lt kj lu kl lv km lw ko lx kp ly lz bi translated">在你走之前</h1><p id="7716" class="pw-post-body-paragraph md me it mg b mh np kd mj mk nq kg mm nh nr mp mq ni ns mt mu nj nt mx my mz im bi translated">欢迎您<strong class="mg jd">加入我的数据科学学习之旅！</strong>关注此<a class="ae lh" href="https://kennethleungty.medium.com/" rel="noopener">媒体</a>页面或查看我的<a class="ae lh" href="https://github.com/kennethleungty" rel="noopener ugc nofollow" target="_blank"> GitHub </a>以了解更多精彩的数据科学内容。同时，享受在你的网络中应用ReLU的乐趣！</p><div class="oy oz gp gr pa pb"><a rel="noopener follow" target="_blank" href="/pytorch-ignite-classifying-tiny-imagenet-with-efficientnet-e5b1768e5e8f"><div class="pc ab fo"><div class="pd ab pe cl cj pf"><h2 class="bd jd gy z fp pg fr fs ph fu fw jc bi translated">PyTorch Ignite教程—使用高效网络对微型图像网络进行分类</h2><div class="pi l"><h3 class="bd b gy z fp pg fr fs ph fu fw dk translated">使用PyTorch Ignite简化PyTorch深度学习实施的分步指南</h3></div><div class="pj l"><p class="bd b dl z fp pg fr fs ph fu fw dk translated">towardsdatascience.com</p></div></div><div class="pk l"><div class="pl l pm pn po pk pp lb pb"/></div></div></a></div><div class="oy oz gp gr pa pb"><a rel="noopener follow" target="_blank" href="/the-most-starred-forked-github-repos-for-python-and-data-science-f8bb3de47e96"><div class="pc ab fo"><div class="pd ab pe cl cj pf"><h2 class="bd jd gy z fp pg fr fs ph fu fw jc bi translated">面向数据科学和Python的最受欢迎的GitHub Repos</h2><div class="pi l"><h3 class="bd b gy z fp pg fr fs ph fu fw dk translated">发现许多数据专业人员和Python程序员正在关注的非常受欢迎的回购</h3></div><div class="pj l"><p class="bd b dl z fp pg fr fs ph fu fw dk translated">towardsdatascience.com</p></div></div><div class="pk l"><div class="pq l pm pn po pk pp lb pb"/></div></div></a></div></div></div>    
</body>
</html>