<html>
<head>
<title>Should all AI research be published?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">所有的AI研究都应该发表吗？</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/should-all-ai-research-be-published-5226ad5145b4?source=collection_archive---------32-----------------------#2021-05-12">https://towardsdatascience.com/should-all-ai-research-be-published-5226ad5145b4?source=collection_archive---------32-----------------------#2021-05-12</a></blockquote><div><div class="fc ig ih ii ij ik"/><div class="il im in io ip"><h2 id="1edc" class="iq ir is bd b dl it iu iv iw ix iy dk iz translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsdatascience.com/tagged/tds-podcast" rel="noopener" target="_blank">播客</a></h2><div class=""/><div class=""><h2 id="0740" class="pw-subtitle-paragraph jy jb is bd b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp dk translated">罗西·坎贝尔论人工智能中负责任的研究和出版规范</h2></div><figure class="kq kr ks kt gt ku"><div class="bz fp l di"><div class="kv kw l"/></div><p class="kx ky gj gh gi kz la bd b be z dk translated"><a class="ae lb" href="https://podcasts.apple.com/ca/podcast/towards-data-science/id1470952338?mt=2" rel="noopener ugc nofollow" target="_blank">苹果</a> | <a class="ae lb" href="https://www.google.com/podcasts?feed=aHR0cHM6Ly9hbmNob3IuZm0vcy8zNmI0ODQ0L3BvZGNhc3QvcnNz" rel="noopener ugc nofollow" target="_blank">谷歌</a> | <a class="ae lb" href="https://open.spotify.com/show/63diy2DtpHzQfeNVxAPZgU" rel="noopener ugc nofollow" target="_blank"> SPOTIFY </a> | <a class="ae lb" href="https://anchor.fm/towardsdatascience" rel="noopener ugc nofollow" target="_blank">其他</a></p></figure><p id="2223" class="pw-post-body-paragraph lc ld is le b lf lg kc lh li lj kf lk ll lm ln lo lp lq lr ls lt lu lv lw lx il bi translated"><em class="ly">编者按:这一集是我们关于数据科学和机器学习新兴问题的播客系列的一部分</em>，<em class="ly">由Jeremie Harris主持。除了主持播客，Jeremie还帮助运营一家名为</em><a class="ae lb" href="http://sharpestminds.com" rel="noopener ugc nofollow" target="_blank"><em class="ly">sharpes minds</em></a><em class="ly">的数据科学导师初创公司。</em></p><p id="babf" class="pw-post-body-paragraph lc ld is le b lf lg kc lh li lj kf lk ll lm ln lo lp lq lr ls lt lu lv lw lx il bi translated">当OpenAI在2019年初开发其GPT-2语言模型时，他们最初选择不发布该算法，因为担心它可能被恶意使用，以及人工智能行业需要试验新的、<a class="ae lb" href="https://openai.com/blog/gpt-2-6-month-follow-up/" rel="noopener ugc nofollow" target="_blank">更负责任的发布实践</a>，这些实践反映了现代人工智能系统日益增长的力量。</p><p id="ccbb" class="pw-post-body-paragraph lc ld is le b lf lg kc lh li lj kf lk ll lm ln lo lp lq lr ls lt lu lv lw lx il bi translated">这个决定是有争议的，甚至今天在某种程度上仍然如此:人工智能研究人员历来享有开放出版的文化，并默认分享他们的结果和算法。但无论你对GPT-2等算法的立场是什么，很明显，在某个时候，如果人工智能变得任意灵活和强大，那么在某些情况下，限制发布对公共安全将是重要的。</p><p id="10c9" class="pw-post-body-paragraph lc ld is le b lf lg kc lh li lj kf lk ll lm ln lo lp lq lr ls lt lu lv lw lx il bi translated">人工智能中的出版规范问题很复杂，这就是为什么它是一个值得与既有研究经验又有政策专家经验的人一起探索的话题——就像今天的《走向数据科学》播客嘉宾罗西·坎贝尔一样。罗西是AI Partnership on AI(PAI)的安全关键人工智能负责人，这是一个非营利组织，汇集了非营利组织、民间社会组织、学术机构、初创公司和谷歌、脸书、微软和亚马逊等大型科技公司，以塑造最佳实践、研究和关于人工智能对人类和社会的好处的公共对话。Rosie与PAI的同事一起，最近完成了对<a class="ae lb" href="https://www.partnershiponai.org/responsible-publication-recommendations/" rel="noopener ugc nofollow" target="_blank">白皮书的整理，该白皮书探讨了当前关于人工智能研究出版规范的激烈辩论</a>，并为参与人工智能研究的研究人员、期刊和机构提出了建议。</p><p id="958e" class="pw-post-body-paragraph lc ld is le b lf lg kc lh li lj kf lk ll lm ln lo lp lq lr ls lt lu lv lw lx il bi translated">以下是我在对话中最喜欢的一些观点:</p><ul class=""><li id="415e" class="lz ma is le b lf lg li lj ll mb lp mc lt md lx me mf mg mh bi translated">最佳实践和推荐的出版规范只有在研究人员愿意并且能够实施的情况下才有用。因此，Rosie强调了推荐出版规范的重要性，这些规范对研究过程的干扰最小，并且要求研究人员尽可能少的开销。例如，虽然她建议研究人员在发表的作品中包括一些关于他们贡献的潜在影响和使用案例的声明，但他们在这方面投入的时间应该与每次贡献的大小成比例。对于代表渐进进展的项目(这是绝大多数ML研究)，研究人员应该花更少的时间去思考他们工作的潜在危害和影响。</li><li id="9f38" class="lz ma is le b lf mi li mj ll mk lp ml lt mm lx me mf mg mh bi translated">建立负责任的出版规范的最大挑战之一是，不同意给定出版框架的单个组织或研究团队可以简单地拒绝实施它，从而破坏其他所有人的努力。事实上，这已经发生了:2019年，在OpenAI宣布他们将推迟发布完整的GPT-2模型后不久，一个独立的研究团队开始复制它，理由是限制对领先人工智能系统的访问将阻止人工智能安全研究人员进行跟上人工智能能力所需的前沿研究。这就是为什么努力在研究人员中达成共识是如此重要，这是一套每个人都愿意遵守的负责任的最低出版标准。</li><li id="a5fb" class="lz ma is le b lf mi li mj ll mk lp ml lt mm lx me mf mg mh bi translated">为了能够预测他们工作的影响或潜在危害，期望忙碌的人工智能研究人员成为未来学家和政策专家是不合理的。这就是为什么随着人工智能技术的发展，与社会科学家和伦理学家的合作将变得越来越重要。罗西提倡这些学科之间更多的混合，作为支持人工智能研究人员开发更强大的影响评估的一种手段。</li></ul><p id="9e7e" class="pw-post-body-paragraph lc ld is le b lf lg kc lh li lj kf lk ll lm ln lo lp lq lr ls lt lu lv lw lx il bi translated">你可以在这里的Twitter上关注罗西，或者在这里的Twitter上关注我<a class="ae lb" href="https://twitter.com/jeremiecharris" rel="noopener ugc nofollow" target="_blank">。</a></p></div><div class="ab cl mn mo hw mp" role="separator"><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms"/></div><div class="il im in io ip"><h2 id="c8cd" class="mu mv is bd mw mx my dn mz na nb dp nc ll nd ne nf lp ng nh ni lt nj nk nl iy bi translated">播客中引用的链接:</h2><ul class=""><li id="384d" class="lz ma is le b lf nm li nn ll no lp np lt nq lx me mf mg mh bi translated">你可以在艾的网站上找到<a class="ae lb" href="https://www.partnershiponai.org/" rel="noopener ugc nofollow" target="_blank">合作伙伴</a>。</li><li id="1bff" class="lz ma is le b lf mi li mj ll mk lp ml lt mm lx me mf mg mh bi translated">PAI关于出版规范的白皮书在这里。</li></ul><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="ns nt di nu bf nv"><div class="gh gi nr"><img src="../Images/92b8321ccc9b0bdf4c6301b58a279b4d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*J-6nzrh4kmK4vEvex-kZsw.png"/></div></div></figure><h2 id="e529" class="mu mv is bd mw mx my dn mz na nb dp nc ll nd ne nf lp ng nh ni lt nj nk nl iy bi translated"><strong class="ak">章节</strong>:</h2><ul class=""><li id="a685" class="lz ma is le b lf nm li nn ll no lp np lt nq lx me mf mg mh bi translated">0:00介绍</li><li id="c28b" class="lz ma is le b lf mi li mj ll mk lp ml lt mm lx me mf mg mh bi translated">2:05罗西的背景</li><li id="576c" class="lz ma is le b lf mi li mj ll mk lp ml lt mm lx me mf mg mh bi translated">5:40高级人工智能的风险</li><li id="21a3" class="lz ma is le b lf mi li mj ll mk lp ml lt mm lx me mf mg mh bi translated">8:15围绕出版规范的活动</li><li id="9cfa" class="lz ma is le b lf mi li mj ll mk lp ml lt mm lx me mf mg mh bi translated">12:40偏离默认模型的参数</li><li id="6128" class="lz ma is le b lf mi li mj ll mk lp ml lt mm lx me mf mg mh bi translated">15:10语言建模的有害后果</li><li id="d858" class="lz ma is le b lf mi li mj ll mk lp ml lt mm lx me mf mg mh bi translated">23:00作为一个团体进行协调</li><li id="181f" class="lz ma is le b lf mi li mj ll mk lp ml lt mm lx me mf mg mh bi translated">28:04默认发布规范</li><li id="3b96" class="lz ma is le b lf mi li mj ll mk lp ml lt mm lx me mf mg mh bi translated">研究人员的责任</li><li id="dcc0" class="lz ma is le b lf mi li mj ll mk lp ml lt mm lx me mf mg mh bi translated">34:30政府在这一领域的作用</li><li id="d276" class="lz ma is le b lf mi li mj ll mk lp ml lt mm lx me mf mg mh bi translated">40:20激励企业人工智能研究</li><li id="614e" class="lz ma is le b lf mi li mj ll mk lp ml lt mm lx me mf mg mh bi translated">44:30审计公司算法</li><li id="0d50" class="lz ma is le b lf mi li mj ll mk lp ml lt mm lx me mf mg mh bi translated">46:20 PAI和国际参与</li><li id="9959" class="lz ma is le b lf mi li mj ll mk lp ml lt mm lx me mf mg mh bi translated">50:00行动呼吁</li><li id="9e1a" class="lz ma is le b lf mi li mj ll mk lp ml lt mm lx me mf mg mh bi translated">51:55总结</li></ul></div></div>    
</body>
</html>