<html>
<head>
<title>Weight Decay and Its Peculiar Effects</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">重量衰减及其特殊效应</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/weight-decay-and-its-peculiar-effects-66e0aee3e7b8?source=collection_archive---------3-----------------------#2021-12-18">https://towardsdatascience.com/weight-decay-and-its-peculiar-effects-66e0aee3e7b8?source=collection_archive---------3-----------------------#2021-12-18</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="7130" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">它是如何工作的，为什么工作，以及一些实用的技巧。</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/fb853422ab784469962ea35c06461b92.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*UGNg5Q1Yxn1zGFmO"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">由<a class="ae kv" href="https://unsplash.com/@goodfreephoto_com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">在<a class="ae kv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的</a>好的免费照片。我们都喜欢光滑的东西，不是吗？</p></figure><h1 id="ebac" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">什么是体重衰减？</h1><p id="0646" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">权重衰减，有时被称为 L2 归一化(尽管它们并不完全相同，<a class="ae kv" rel="noopener" target="_blank" href="/weight-decay-l2-regularization-90a9e17713cd">这里是解释差异的好博客文章</a>)，是正则化神经网络的一种常见方法。它有助于神经网络学习更平滑/更简单的函数，与尖锐、嘈杂的函数相比，这些函数在大多数情况下更容易概括。</p><p id="b603" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">有许多正则化，权重衰减是其中之一，它通过在每一步用一些小因子将权重推向零来完成工作。</p><p id="d810" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">在代码中，这实现为</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="mp mq l"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">重量衰减的基本实现</p></figure><p id="714c" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">其中<code class="fe mr ms mt mu b">weight_decay</code>是一个超参数，典型值范围从 1e-5 到 1。</p><p id="7678" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">实际上，您不必亲自执行此更新。例如，PyTorch 中的优化器有一个为您处理所有更新的<code class="fe mr ms mt mu b">weight_decay</code>参数。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="mp mq l"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">在 PyTorch 中使用权重衰减</p></figure><h1 id="ad1b" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">体重衰减的直觉</h1><p id="8cc0" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">但是体重衰减实际上对模型有什么帮助呢？什么情况下应该用？</p><p id="fc3e" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">首先，我们必须理解为什么有时模型不能一般化。</p><h2 id="9cbd" class="mv kx iq bd ky mw mx dn lc my mz dp lg lx na nb li mb nc nd lk mf ne nf lm ng bi translated">古典政体</h2><p id="b14c" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">在经典的机器学习理论中，我们认为存在“欠拟合”和“过拟合”区域。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nh"><img src="../Images/28946a7114f28bf13b8d357656b046f3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dhf2-w3XPAROzm_dmI6Jxw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">传统体制中模型容量的训练和测试损失摘自<a class="ae kv" href="https://arxiv.org/abs/1812.11118" rel="noopener ugc nofollow" target="_blank">“协调现代机器学习实践和偏差-方差权衡”</a></p></figure><p id="5beb" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">在欠拟合区域，模型“太简单”而不能捕捉数据的结构，因此既有<strong class="lq ir">高训练误差</strong>又有<strong class="lq ir">高测试误差</strong>。</p><p id="b028" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">在过拟合区域，模型具有足够的容量，它可以仅记忆训练数据集而不学习数据的底层结构，导致<strong class="lq ir">低训练误差</strong>但<strong class="lq ir">高测试误差</strong>。</p><p id="eb5b" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">无论是哪种情况，这个模型都不能一概而论。传统观点认为，最好的模型位于这两个区域之间。</p><p id="dd58" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">权重衰减在这里作为一种方法<strong class="lq ir">降低模型的容量</strong>，使得过度拟合的模型不会过度拟合，并被推向最佳点。这也表明，如果模型最初在欠拟合区域中运行，重量衰减将具有负面影响。</p><h2 id="1e76" class="mv kx iq bd ky mw mx dn lc my mz dp lg lx na nb li mb nc nd lk mf ne nf lm ng bi translated">现代政体</h2><p id="2f45" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">然而，最近的研究揭示了一种称为“深度双重下降”的现象，表明实际上存在第三个区域。虽然经典理论认为进一步增加模型复杂性会导致更高的测试误差，但经验证据表明，随着我们从“过拟合”区域进入“过参数化”区域，测试误差将会下降。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nj"><img src="../Images/efe24ae31c3cbd4c05c53acb0651face.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LVh9JNMo8rt9L7kIjHtvRQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">从<a class="ae kv" href="https://arxiv.org/abs/1812.11118" rel="noopener ugc nofollow" target="_blank">“现代机器学习实践与偏差-方差权衡的调和”</a>中截取的训练和测试损失与模型能力的全貌</p></figure><p id="03ab" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">经验证据表明，过拟合和过参数化区域之间的这种边界(称为“插值阈值”)发生在模型刚好具有足够的能力来实现<strong class="lq ir">(接近)零训练损失</strong>的时候。</p><p id="d662" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">在插值阈值处，只有一组参数可以对整个训练集进行插值。最有可能出现的情况是，这种特定的插值将会非常颠簸和尖锐，因为大多数数据集都包含一定程度的噪声。当模型被迫以勉强足够的能力适应所有噪声时，它没有额外的空间来使噪声训练点之外的函数平滑，因此泛化能力很差。</p><p id="e0b1" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">随着我们进一步增加模型容量，它不仅可以完美地命中训练数据点，还可以获得额外的容量来选择在空间之间平滑插值的函数，从而更好地进行概括。</p><p id="6071" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated"><em class="ni">这就是体重下降的原因。</em></p><p id="aa82" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">在模型可以学习的所有可能的插值中，<strong class="lq ir">我们希望使我们的模型偏向更平滑、更简单的插值</strong>，这样它就能够进行推广。重量衰减正是这样做的。这就是体重下降如此强大的原因。</p><p id="61cc" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">虽然不完全准确，但我认为下面的迷因很好地总结了我们目前的理解。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nk"><img src="../Images/3643af6ee5246cbe218aea63325e0755.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TVyR_wyJlstmKLPIGUqIYg.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">完全由我原创的迷因</p></figure><h1 id="35a0" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">玩具实验</h1><p id="5515" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">为了测试上述特性，我训练了一个带有跳跃连接的简单前馈网络，以学习对二进制编码的数字执行加法。您可以在这里找到完整的源代码:</p><div class="nl nm gp gr nn no"><a href="https://github.com/andylolu2/ml-weight-decay" rel="noopener  ugc nofollow" target="_blank"><div class="np ab fo"><div class="nq ab nr cl cj ns"><h2 class="bd ir gy z fp nt fr fs nu fu fw ip bi translated">GitHub-andylolo 2/ml-重量衰减</h2><div class="nv l"><h3 class="bd b gy z fp nt fr fs nu fu fw dk translated">了解体重衰减如何帮助模型进行归纳的知识库。你可以在 Medium 上阅读相关博客…</h3></div><div class="nw l"><p class="bd b dl z fp nt fr fs nu fu fw dk translated">github.com</p></div></div><div class="nx l"><div class="ny l nz oa ob nx oc kp no"/></div></div></a></div><p id="3065" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">它在 2 位和 4 位加法上被训练，并且在 3 位加法上被测试以测量它的概括能力。以下是一个示例培训点:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="mp mq l"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">示例培训输入和标签</p></figure><p id="7c08" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">我的模型由两个剩余块组成，每个块的宽度为 256，总共有 300K 个参数，这些参数几乎肯定会在“过参数化”区域内运行。我针对重量衰减因子的不同值对其进行了训练，下面是它与训练和测试精度的关系图:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi od"><img src="../Images/17d14392c4f0a68d80fa41bdec95e254.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hwl1KqT4iXx-jBTEkcDqqQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">在我的玩具实验中，精确度与重量衰减系数λ的关系图</p></figure><p id="76d9" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">我们可以看到，对于这个模型，λ的合适值是 0.1 左右。</p><p id="3a89" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">在左手侧，当λ太低时，模型完全具有足够的能力来拟合训练数据集，但是不偏向于寻找更简单的插值，因此测试精度非常低。</p><p id="84f5" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">在右侧，当λ太高时，该模型由于被迫使用非常小的权重而受到太多限制，以至于它的表达能力不足以适应训练数据。因此，训练和测试精度都很低。</p><p id="2fe8" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">根据我们上面的直觉，我们期望在插值阈值处看到测试精度的下降——但是我们没有。为什么？</p><p id="3238" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">我的最佳猜测是因为我们的数据集是完全无噪声的，所以插值阈值处的特定权重集不会出错，最终会很好地进行概化。</p><p id="1338" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">为了进一步说明权重衰减在帮助模型概化方面的重要性，下面是λ = 0.1 的模型的训练曲线:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oe"><img src="../Images/2f43a196f816ebd174069454c0277a4e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EXUIrb5HwqFraJ1j-6TRMw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">在我的玩具实验中，λ = 0.1 的训练曲线</p></figure><p id="2904" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">在前 3665 个步骤中，该模型能够达到 100%的准确率，并且几乎没有训练损失。但是测试精度还是很低。</p><p id="464f" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">在接下来的 25K 步中，它继续最小化权重范数(模型权重的平方和),同时保持完美的训练精度。这对应于为训练数据找到更简单的插值，并且我们可以清楚地看到这与测试准确度的增加之间的相关性。</p><p id="c7f0" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">更简单的插值=利润！</p><h1 id="e104" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">选择正确的价值</h1><p id="1c36" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">好的，所以体重下降是非常有用的。现在，我如何选择正确的λ值呢？</p><p id="dadf" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">一般来说，挑选体重衰减值没有金科玉律。一些尝试和错误是不可避免的。然而，有一些规则你可以遵循。提出 AdamW 优化器的研究人员提出了以下规则:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi of"><img src="../Images/d230b48ed7f979918e6b9d54b5bd258a.png" data-original-src="https://miro.medium.com/v2/resize:fit:600/format:webp/1*GOtpymM7W2bd47ipbM8AKQ.jpeg"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><a class="ae kv" href="https://arxiv.org/abs/1711.05101" rel="noopener ugc nofollow" target="_blank">“解耦权重衰减正则化”</a>中提出的归一化权重衰减因子公式</p></figure><p id="1d31" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">其中<em class="ni"> b </em>批量，<em class="ni"> B </em>为训练点总数，<em class="ni"> T </em>为总历元数。现在要调整的新超参数是<em class="ni"> λ_norm </em>。</p><p id="c367" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">所以他们只是把一个超参数换成了另一个超参数…嗯…这个怎么有用？？</p><p id="f1f0" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">这很有帮助，因为将<em class="ni"> b </em>、<em class="ni"> B </em>和<em class="ni"> T </em>的选择从合适的权重衰减值中分离出来，因此更容易调整超参数。</p><p id="7a8a" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">但是，您仍然需要搜索<em class="ni"> λ_norm </em>的好值。作者发现 0.025 到 0.05 范围内的<em class="ni"> λ_norm </em>对于他们训练的图像分类网络来说是最佳的。也许这可以作为一个有用的起点。</p><h1 id="3c4d" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">批处理规范化的问题</h1><p id="90c7" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">如今，我们几乎总是将批量范数层放入我们的神经网络，因为它大大加快了训练速度。然而，当我运行我的实验时，当我使用具有权重衰减的批范数层时，我经常看到这种周期性波动的奇怪行为。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi og"><img src="../Images/c5e9fd19977ec04b0bdef5d950e4ce53.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QqKseoCKe4XAqaAolKPFvA.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">在我的玩具实验中，使用批量标准训练时出现了奇怪的周期波动，重量随默认值衰减</p></figure><p id="bf7f" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">这肯定不是我第一次看到这种行为。似乎当模型即将收敛时，这些波动就开始出现了。那么是什么造成的呢？</p><p id="ba9f" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">我在谷歌上寻找答案，看到了这篇文章:</p><div class="nl nm gp gr nn no"><a href="https://arxiv.org/abs/2106.15739" rel="noopener  ugc nofollow" target="_blank"><div class="np ab fo"><div class="nq ab nr cl cj ns"><h2 class="bd ir gy z fp nt fr fs nu fu fw ip bi translated">具有批量归一化和权重衰减的神经网络训练的周期行为</h2><div class="nv l"><h3 class="bd b gy z fp nt fr fs nu fu fw dk translated">近年来，使用批量归一化和权重衰减来训练神经网络已经成为一种常见的做法。在…</h3></div><div class="nw l"><p class="bd b dl z fp nt fr fs nu fu fw dk translated">arxiv.org</p></div></div><div class="nx l"><div class="oh l nz oa ob nx oc kp no"/></div></div></a></div><p id="5af9" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">总之，这与批处理规范层的工作方式有关。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi of"><img src="../Images/17b3b3b563142dc705b2820c1ed9c0ae.png" data-original-src="https://miro.medium.com/v2/resize:fit:600/format:webp/1*uFo5qi7kCRoZRREdvpBDww.jpeg"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">受<a class="ae kv" href="https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm1d.html?highlight=batch%20norm#torch.nn.BatchNorm1d" rel="noopener ugc nofollow" target="_blank"> PyTorch Doc </a>启发的批量定额层方程</p></figure><p id="4ab2" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">上面显示了 batch norm 如何计算其输出的公式。这里，<em class="ni"> x </em>是一个尺寸为<em class="ni"> (batch_size，1) </em>的特征。重要的是，它将这些值除以<em class="ni"> x </em>的方差与某个小值εϵ.之和的平方根</p><p id="7f65" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">当我们使用权重衰减时，一些权重逐渐变为零。随着权重趋近于零，无论从上一层得到什么输入，<em class="ni"> x </em>的值也将趋近于零(或恒定值)。</p><p id="898f" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">结果，批中的 Var[ <em class="ni"> x </em> ]变得很小，以至于当我们将它传递给批范数层时，它将其除以一个接近于零的值，导致最终输出放大。这导致了巨大的波动，如下图所示。</p><p id="6073" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">爆炸后，由于该步骤的巨大梯度，权重更新为某个大值，因此方差再次变高。重量会再次开始衰减，这个过程会不断重复，形成周期性的模式。</p><p id="f809" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">从实用的角度来看，为了避免这种不稳定性，我发现简单地将 epsilon 的值增加到更大的值(比如<code class="fe mr ms mt mu b">1e-2</code>而不是 PyTorch 默认的<code class="fe mr ms mt mu b">1e-5</code>)效果惊人地好。</p><p id="a8bd" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">又一个有用的东西放在你的脑后。</p><h1 id="f4a7" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">摘要</h1><p id="6fa1" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">权重衰减是一种正则化方法，通过学习更平滑的函数使模型更好地泛化。在经典(欠参数化)状态下，它有助于限制模型过拟合，而在过参数化状态下，它有助于引导模型朝向更简单的插值。实际上，有一个最佳的重量衰减值范围。</p><p id="a762" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">希望你觉得这个博客有用！</p><h1 id="b306" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">密码</h1><div class="nl nm gp gr nn no"><a href="https://github.com/andylolu2/ml-weight-decay" rel="noopener  ugc nofollow" target="_blank"><div class="np ab fo"><div class="nq ab nr cl cj ns"><h2 class="bd ir gy z fp nt fr fs nu fu fw ip bi translated">GitHub-andylolo 2/ml-重量衰减</h2><div class="nv l"><h3 class="bd b gy z fp nt fr fs nu fu fw dk translated">了解体重衰减如何帮助模型进行归纳的知识库。你可以在 Medium 上阅读相关博客…</h3></div><div class="nw l"><p class="bd b dl z fp nt fr fs nu fu fw dk translated">github.com</p></div></div><div class="nx l"><div class="ny l nz oa ob nx oc kp no"/></div></div></a></div><h1 id="a58c" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">参考</h1><ol class=""><li id="19cd" class="oi oj iq lq b lr ls lu lv lx ok mb ol mf om mj on oo op oq bi translated">普里图姆·纳基兰、加尔·卡普伦、亚米尼·班萨尔、特里斯坦·杨、波阿斯·巴拉克、伊利亚·苏茨基弗。<a class="ae kv" href="https://arxiv.org/abs/1912.02292" rel="noopener ugc nofollow" target="_blank"> <em class="ni">深度双下降:更大的模型和更多的数据伤害</em> </a>。2019.arXiv:1912.02292</li><li id="e67f" class="oi oj iq lq b lr or lu os lx ot mb ou mf ov mj on oo op oq bi translated">米哈伊尔·贝尔金，丹尼尔·许，，苏米克·曼达尔。<a class="ae kv" href="https://arxiv.org/abs/1812.11118" rel="noopener ugc nofollow" target="_blank"> <em class="ni">调和现代机器学习实践和偏差-方差权衡</em> </a>。2019.arVix:1812.11118</li><li id="f3d6" class="oi oj iq lq b lr or lu os lx ot mb ou mf ov mj on oo op oq bi translated">叶卡捷琳娜·洛巴切娃、马克西姆·科德良、娜杰日达·奇尔科娃、安德烈·马林宁、德米特里·维特罗夫。<a class="ae kv" href="https://arxiv.org/abs/2106.15739" rel="noopener ugc nofollow" target="_blank"> <em class="ni">对神经网络的周期性行为进行批量归一化和权值衰减训练</em> </a>。2021.arVix:2106.15739</li><li id="8a36" class="oi oj iq lq b lr or lu os lx ot mb ou mf ov mj on oo op oq bi translated">伊利亚·洛希洛夫，弗兰克·哈特。<a class="ae kv" href="https://arxiv.org/abs/1711.05101" rel="noopener ugc nofollow" target="_blank"> <em class="ni">解耦权重衰减正则化</em> </a>。2019.arVix:1711.05101</li></ol></div></div>    
</body>
</html>