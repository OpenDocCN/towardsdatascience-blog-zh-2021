<html>
<head>
<title>The Secret Behind Neural Networks</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">背后的秘密神经网络</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/the-secret-behind-neural-networks-8c9a77235a8a?source=collection_archive---------38-----------------------#2021-09-21">https://towardsdatascience.com/the-secret-behind-neural-networks-8c9a77235a8a?source=collection_archive---------38-----------------------#2021-09-21</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="e2be" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph">机器学习中的必备知识</h2><div class=""/><div class=""><h2 id="3b92" class="pw-subtitle-paragraph jw iz iq bd b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn dk translated">为什么神经网络实际上是后验概率的直接估计</h2></div><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ko"><img src="../Images/b603ea3911baefe043b19a7246baf7af.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*pkPuEAd876jXJRWx"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">照片由<a class="ae le" href="https://unsplash.com/@fakurian?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">法库里安设计</a>在<a class="ae le" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><p id="04bc" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">每个机器学习算法的背后都存在偏差-方差困境。机器学习模型越复杂，它的泛化能力就越好，但过度拟合的倾向就越高。理论上，高斯分类器为正态分布数据提供了最佳分类器。但正是神经网络的灵活性使得它们在最近几年变得更加重要。</p><h1 id="b2a7" class="mb mc iq bd md me mf mg mh mi mj mk ml kf mm kg mn ki mo kj mp kl mq km mr ms bi translated"><strong class="ak">普适逼近定理</strong></h1><p id="7de5" class="pw-post-body-paragraph lf lg iq lh b li mt ka lk ll mu kd ln lo mv lq lr ls mw lu lv lw mx ly lz ma ij bi translated">对于任何函数，不管有多复杂，都有一个神经网络可以完美地将输入映射到该函数的输出。</p><p id="e4cc" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">神经网络是分层排列的非线性单元的组合。非线性单元通常被称为感知器，它们的方程形式如下:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi my"><img src="../Images/28daa61d9583c7af70ee56f9af522eb5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1076/format:webp/1*PFb78zGJWslhKY1vFwyy1Q.png"/></div></figure><p id="df84" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">其中f是非线性函数。神经网络或多层感知器(MLPs)通过将一个神经元(x)的输入改变为前一个神经元的输出，将这些函数堆叠成层，如下所示:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi mz"><img src="../Images/6484d59c937cb6c16ff2ef98e047790c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*R1gT451R6lGysng-k4qfqQ.png"/></div></div></figure><p id="5689" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">通过分层堆叠这些感知器，可以实现输入和输出之间的复杂映射。这产生了一个极其通用和灵活的架构，如果足够复杂，可以有效地学习任何功能。这就是众所周知的通用近似定理。</p><h1 id="3afd" class="mb mc iq bd md me mf mg mh mi mj mk ml kf mm kg mn ki mo kj mp kl mq km mr ms bi translated"><strong class="ak">优化网络的权重</strong></h1><p id="0a28" class="pw-post-body-paragraph lf lg iq lh b li mt ka lk ll mu kd ln lo mv lq lr ls mw lu lv lw mx ly lz ma ij bi translated">为了优化神经网络的权重，模型的误差可以相对于权重来表示。然后可以计算梯度，并且可以使用类似梯度下降的优化器来更新权重。这就是所谓的反向传播算法。</p><p id="06b6" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">误差函数可以表示如下。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi na"><img src="../Images/ad12d07ffc86fb75f2ab5de194b5fa39.png" data-original-src="https://miro.medium.com/v2/resize:fit:734/format:webp/1*K7V_0TxW38-JY4hfsMBoeg.png"/></div></figure><p id="a77f" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">误差是神经网络的输出和目标值之间的平方差之和。我们知道，最后一层的输出是网络中所有权重的函数，因此通过获取梯度并应用微分链规则，我们可以获得网络中每个权重的梯度。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi nb"><img src="../Images/c0322f13557a0d6f715e9da011d5f6f4.png" data-original-src="https://miro.medium.com/v2/resize:fit:448/format:webp/1*-YLk4H_bTuar0BTmONOSUQ.png"/></div></figure><h1 id="e788" class="mb mc iq bd md me mf mg mh mi mj mk ml kf mm kg mn ki mo kj mp kl mq km mr ms bi translated"><strong class="ak">作为后验概率估计器的神经网络</strong></h1><p id="f723" class="pw-post-body-paragraph lf lg iq lh b li mt ka lk ll mu kd ln lo mv lq lr ls mw lu lv lw mx ly lz ma ij bi translated">使用贝叶斯定理，可以形成给定数据的最佳权重的表达式。正如我的一些<a class="ae le" rel="noopener" target="_blank" href="/drawing-out-the-posterior-probability-surface-of-a-gaussian-classifier-a1f6044ab5ae">其他文章</a>中提到的，这是网络权重的后验概率超平面，最大化这个返回最优权重。</p><p id="4018" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">可以看出，对于无限数据，最小化上面所示的误差函数等同于最小化下面的函数:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi nc"><img src="../Images/9cfed0916ddd1889c735bbc1a09a94be.png" data-original-src="https://miro.medium.com/v2/resize:fit:832/format:webp/1*KFSm7U9bN8K9Kd44V2gdKQ.png"/></div></figure><p id="1906" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">其中p(w|x)是权重相对于数据的后验概率。因此，<strong class="lh ja">对于无限数据，一个足够大的神经网络是后验概率的直接估计器。</strong></p><h1 id="8a3b" class="mb mc iq bd md me mf mg mh mi mj mk ml kf mm kg mn ki mo kj mp kl mq km mr ms bi translated">例子</h1><p id="59a9" class="pw-post-body-paragraph lf lg iq lh b li mt ka lk ll mu kd ln lo mv lq lr ls mw lu lv lw mx ly lz ma ij bi translated">以下示例显示了最大后验概率决策边界(黑色)、浅层神经网络估计(蓝色)和深层神经网络(红色)。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi nd"><img src="../Images/3bb7eb1ca5f4f2eb3469f8044a233c67.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tKsve04oq8nAIo3JNPDXSA.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">作者图片</p></figure><p id="ba7a" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">如您所见，最大后验概率绘制了一个与深度神经网络非常相似的决策边界。浅层神经网络虽然在分类问题上做得还不错，但离真正的后验概率还很远。</p><h1 id="6015" class="mb mc iq bd md me mf mg mh mi mj mk ml kf mm kg mn ki mo kj mp kl mq km mr ms bi translated"><strong class="ak">结论</strong></h1><p id="cba1" class="pw-post-body-paragraph lf lg iq lh b li mt ka lk ll mu kd ln lo mv lq lr ls mw lu lv lw mx ly lz ma ij bi translated">如果足够复杂，任何函数都可以通过神经网络完美地表示出来。此外，在无限数据的情况下，神经网络可以完美地重建后验概率。这两种说法是机器学习理论的关键部分。他们一起解释了为什么过去二十年中计算能力的增加导致了机器学习世界中神经网络的兴起。</p><h2 id="3404" class="ne mc iq bd md nf ng dn mh nh ni dp ml lo nj nk mn ls nl nm mp lw nn no mr iw bi translated">支持我</h2><p id="83d0" class="pw-post-body-paragraph lf lg iq lh b li mt ka lk ll mu kd ln lo mv lq lr ls mw lu lv lw mx ly lz ma ij bi translated">希望这对你有所帮助，如果你喜欢它，你可以<a class="ae le" href="https://medium.com/@diegounzuetaruedas" rel="noopener">跟随我！ </a></p><p id="1628" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">您也可以成为<a class="ae le" href="https://diegounzuetaruedas.medium.com/membership" rel="noopener"> <strong class="lh ja">中级会员</strong> </a> <strong class="lh ja"> </strong>使用我的推荐链接，访问我的所有文章以及更多:<a class="ae le" href="https://diegounzuetaruedas.medium.com/membership" rel="noopener">https://diegounzuetaruedas.medium.com/membership</a></p><h2 id="74f1" class="ne mc iq bd md nf ng dn mh nh ni dp ml lo nj nk mn ls nl nm mp lw nn no mr iw bi translated">你可能喜欢的其他文章</h2><p id="4a40" class="pw-post-body-paragraph lf lg iq lh b li mt ka lk ll mu kd ln lo mv lq lr ls mw lu lv lw mx ly lz ma ij bi translated"><a class="ae le" rel="noopener" target="_blank" href="/differentiable-generator-networks-an-introduction-5a9650a24823">可微发电机网络:简介</a></p><p id="fb10" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated"><a class="ae le" rel="noopener" target="_blank" href="/fourier-transforms-an-intuitive-visualisation-ba186c7380ee">傅立叶变换:直观的可视化</a></p></div></div>    
</body>
</html>