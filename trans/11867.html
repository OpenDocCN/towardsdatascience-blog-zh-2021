<html>
<head>
<title>Classifying Handwritten Digits Using A Multilayer Perceptron Classifier (MLP)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用多层感知器分类器(MLP)对手写数字进行分类</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/classifying-handwritten-digits-using-a-multilayer-perceptron-classifier-mlp-bc8453655880?source=collection_archive---------10-----------------------#2021-11-27">https://towardsdatascience.com/classifying-handwritten-digits-using-a-multilayer-perceptron-classifier-mlp-bc8453655880?source=collection_archive---------10-----------------------#2021-11-27</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="7d06" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">什么是多层感知器？MLP 的利与弊是什么？我们能用 MLP 分类器准确地分类手写数字吗？学来的重量是什么样的？</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/434e91efe9ee2b42c92a8fa90c887835.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2Xtu3NtR5dCfYQU3-4qTxA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图 1:一个多层感知器网络(<a class="ae ky" href="https://alexlenail.me/NN-SVG/index.html" rel="noopener ugc nofollow" target="_blank">来源</a>)。</p></figure><h1 id="9511" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">1.简短介绍</h1><h2 id="7aac" class="lr la it bd lb ls lt dn lf lu lv dp lj lw lx ly ll lz ma mb ln mc md me lp mf bi translated">1.1 什么是多层感知器(MLP)？</h2><p id="6b61" class="pw-post-body-paragraph mg mh it mi b mj mk ju ml mm mn jx mo lw mp mq mr lz ms mt mu mc mv mw mx my im bi translated">一个<strong class="mi iu"> MLP </strong>是一个<strong class="mi iu">监督</strong>机器学习(ML)算法，属于前馈人工神经网络类[1]。该算法本质上是对数据进行训练，以便学习一个函数。给定一组特征和目标变量(例如标签)，它学习用于分类或回归的非线性函数。在本文中，我们将只关注分类的情况。</p><h2 id="aa25" class="lr la it bd lb ls lt dn lf lu lv dp lj lw lx ly ll lz ma mb ln mc md me lp mf bi translated">1.2 MLP 和逻辑回归之间有什么相似之处吗？</h2><p id="fe7d" class="pw-post-body-paragraph mg mh it mi b mj mk ju ml mm mn jx mo lw mp mq mr lz ms mt mu mc mv mw mx my im bi translated">有！逻辑回归只有两层，即<strong class="mi iu">输入</strong>和<strong class="mi iu">输出</strong>，然而，在 MLP 模型的情况下，唯一的区别是我们可以有额外的中间<strong class="mi iu">非线性</strong>层。这些被称为<strong class="mi iu">隐藏层</strong>。除了输入节点(属于输入层的节点)，每个节点都是一个使用<strong class="mi iu">非线性</strong> <strong class="mi iu">激活</strong> <strong class="mi iu">函数</strong>【1】的神经元。由于这种非线性性质，MLP 可以学习复杂的非线性函数，从而区分不可线性分离的数据！参见下面的图 2 中带有一个隐藏层的 MLP 分类器的可视化表示。</p></div><div class="ab cl mz na hx nb" role="separator"><span class="nc bw bk nd ne nf"/><span class="nc bw bk nd ne nf"/><span class="nc bw bk nd ne"/></div><div class="im in io ip iq"><p id="37e2" class="pw-post-body-paragraph mg mh it mi b mj ng ju ml mm nh jx mo lw ni mq mr lz nj mt mu mc nk mw mx my im bi translated">如果你想在交互式路线图和活跃的学习社区的支持下自学数据科学，看看这个资源:<a class="ae ky" href="https://aigents.co/learn" rel="noopener ugc nofollow" target="_blank">https://aigents.co/learn</a></p></div><div class="ab cl mz na hx nb" role="separator"><span class="nc bw bk nd ne nf"/><span class="nc bw bk nd ne nf"/><span class="nc bw bk nd ne"/></div><div class="im in io ip iq"><h2 id="126d" class="lr la it bd lb ls lt dn lf lu lv dp lj lw lx ly ll lz ma mb ln mc md me lp mf bi translated">1.3 如何训练 MLP？</h2><p id="c7ea" class="pw-post-body-paragraph mg mh it mi b mj mk ju ml mm mn jx mo lw mp mq mr lz ms mt mu mc mv mw mx my im bi translated">MLP 使用<strong class="mi iu">反向传播</strong>进行训练【1】。你可以看看这个网站<a class="ae ky" href="https://en.wikipedia.org/wiki/Backpropagation" rel="noopener ugc nofollow" target="_blank">这里</a>的正式数学公式。</p><h2 id="e1b2" class="lr la it bd lb ls lt dn lf lu lv dp lj lw lx ly ll lz ma mb ln mc md me lp mf bi translated">1.4 Main 的主要优势和劣势</h2><p id="ae86" class="pw-post-body-paragraph mg mh it mi b mj mk ju ml mm mn jx mo lw mp mq mr lz ms mt mu mc mv mw mx my im bi translated"><strong class="mi iu"> <em class="nl">优点</em> </strong></p><ul class=""><li id="9c2a" class="nm nn it mi b mj ng mm nh lw no lz np mc nq my nr ns nt nu bi translated">可以学习<strong class="mi iu">非线性</strong>函数<strong class="mi iu">从而</strong>分离不可线性分离的数据【2】。</li></ul><p id="3484" class="pw-post-body-paragraph mg mh it mi b mj ng ju ml mm nh jx mo lw ni mq mr lz nj mt mu mc nk mw mx my im bi translated"><strong class="mi iu"> <em class="nl">缺点</em> </strong></p><ul class=""><li id="4da4" class="nm nn it mi b mj ng mm nh lw no lz np mc nq my nr ns nt nu bi translated">隐藏层的损失函数导致一个<strong class="mi iu">非</strong> - <strong class="mi iu">凸</strong>优化问题，因此存在局部最小值。</li><li id="8fbb" class="nm nn it mi b mj nv mm nw lw nx lz ny mc nz my nr ns nt nu bi translated">由于上述问题，不同的权重<strong class="mi iu">初始化</strong>可能导致不同的输出/权重/结果。</li><li id="8459" class="nm nn it mi b mj nv mm nw lw nx lz ny mc nz my nr ns nt nu bi translated">MLP 有一些<strong class="mi iu">超参数</strong>，例如隐藏神经元的数量，需要调整的层数(时间&amp;功耗)[2]。</li><li id="1993" class="nm nn it mi b mj nv mm nw lw nx lz ny mc nz my nr ns nt nu bi translated">MLP 可以是<strong class="mi iu">敏感</strong>的特征<strong class="mi iu">缩放</strong>【2】。</li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oa"><img src="../Images/f9baf82c62c6c62544494f443a616593.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cTEA9vG3S6mOBs5EDk0btg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图 2:带有一个隐藏层和一个标量输出的 MLP。图片改编自 scikit-learn python <a class="ae ky" href="https://scikit-learn.org/stable/modules/neural_networks_supervised.html#neural-networks-supervised" rel="noopener ugc nofollow" target="_blank">文档</a>。</p></figure><h1 id="f1d9" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">2.使用 scikit-learn 的 Python 实践示例</h1><h2 id="9e0c" class="lr la it bd lb ls lt dn lf lu lv dp lj lw lx ly ll lz ma mb ln mc md me lp mf bi translated">2.1 数据集</h2><p id="83f1" class="pw-post-body-paragraph mg mh it mi b mj mk ju ml mm mn jx mo lw mp mq mr lz ms mt mu mc mv mw mx my im bi translated">对于这个动手操作的例子，我们将使用 MNIST 数据集。MNIST 数据库是著名的手写数字数据库，用于训练几个 ML 模型[5]。有 10 个不同数字的手写图像，因此<strong class="mi iu">类别数</strong>为<strong class="mi iu"> </strong> <code class="fe ob oc od oe b">10</code>(参见<strong class="mi iu">图 3 </strong>)。</p><p id="d5fd" class="pw-post-body-paragraph mg mh it mi b mj ng ju ml mm nh jx mo lw ni mq mr lz nj mt mu mc nk mw mx my im bi translated"><strong class="mi iu">注</strong>:由于我们处理的是<strong class="mi iu">图像</strong>，这些用<strong class="mi iu">2D</strong>T28】数组表示，数据的初始维数是每幅图像的<code class="fe ob oc od oe b"><strong class="mi iu">28</strong> by <strong class="mi iu">28</strong></code>(<code class="fe ob oc od oe b">28x28 pixels</code>)。然后，2D 图像被<a class="ae ky" href="https://numpy.org/doc/stable/reference/generated/numpy.ndarray.flatten.html" rel="noopener ugc nofollow" target="_blank"> <strong class="mi iu">展平</strong> </a>，并因此在最后由矢量表示。<strong class="mi iu">每个</strong> <strong class="mi iu"> 2D </strong> <strong class="mi iu">图像</strong>被转换成一个尺寸为<code class="fe ob oc od oe b">[1, 28x28] = <strong class="mi iu">[1, 784]</strong></code>的 1D <strong class="mi iu">向量</strong>。最后，我们的数据集有<em class="nl"> </em> <code class="fe ob oc od oe b"><strong class="mi iu"><em class="nl">784</em></strong></code> <strong class="mi iu"> <em class="nl">特征/变量/列。</em>T47】</strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi of"><img src="../Images/e7620493a3de60425cfd89c7d99be1f0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1188/format:webp/1*Ft2rLuO82eItlvJn5HOi9A.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图 3:来自数据集的一些样本(<a class="ae ky" href="https://en.wikipedia.org/wiki/MNIST_database" rel="noopener ugc nofollow" target="_blank">来源</a>)。</p></figure><h2 id="b488" class="lr la it bd lb ls lt dn lf lu lv dp lj lw lx ly ll lz ma mb ln mc md me lp mf bi translated">2.2 数据导入和准备</h2><pre class="kj kk kl km gt og oe oh oi aw oj bi"><span id="5f7f" class="lr la it oe b gy ok ol l om on">import matplotlib.pyplot as plt<br/>from sklearn.datasets import fetch_openml<br/>from sklearn.neural_network import MLPClassifier<br/><br/># Load data<br/>X, y = fetch_openml("mnist_784", version=1, return_X_y=True)</span><span id="0aa4" class="lr la it oe b gy oo ol l om on"># Normalize intensity of images to make it in the range [0,1] since 255 is the max (white).<br/>X = X / 255.0</span></pre><p id="630e" class="pw-post-body-paragraph mg mh it mi b mj ng ju ml mm nh jx mo lw ni mq mr lz nj mt mu mc nk mw mx my im bi translated">记住<strong class="mi iu">每个</strong> <strong class="mi iu"> 2D </strong> <strong class="mi iu">图像</strong>现在被转换成一个<strong class="mi iu"> 1D </strong> <strong class="mi iu">向量</strong>与维度<code class="fe ob oc od oe b">[1, 28x28] = <strong class="mi iu">[1, 784]</strong></code> <strong class="mi iu">。</strong>现在我们来验证一下这个。</p><pre class="kj kk kl km gt og oe oh oi aw oj bi"><span id="47c4" class="lr la it oe b gy ok ol l om on">print(X.shape)</span></pre><p id="f444" class="pw-post-body-paragraph mg mh it mi b mj ng ju ml mm nh jx mo lw ni mq mr lz nj mt mu mc nk mw mx my im bi translated">这返回:<code class="fe ob oc od oe b"><strong class="mi iu">(70000, 784)</strong></code> <strong class="mi iu">。</strong>我们有<strong class="mi iu"> 70k 的展平图像</strong>(样本)，每个包含 784 个像素(28*28=784)(变量/特征)。这样，<strong class="mi iu">输入</strong> <strong class="mi iu">层</strong> <strong class="mi iu">权重</strong> <strong class="mi iu">矩阵</strong>将具有形状<code class="fe ob oc od oe b">784 x #neurons_in_1st_hidden_layer.</code>输出<strong class="mi iu">层权重矩阵将具有形状</strong> <code class="fe ob oc od oe b">#neurons_in_3rd_hidden_layer x #number_of_classes.</code></p><h2 id="5602" class="lr la it bd lb ls lt dn lf lu lv dp lj lw lx ly ll lz ma mb ln mc md me lp mf bi translated">2.3 模型培训</h2><p id="4149" class="pw-post-body-paragraph mg mh it mi b mj mk ju ml mm mn jx mo lw mp mq mr lz ms mt mu mc mv mw mx my im bi translated">现在让我们建立模型，训练它并执行分类。我们将使用<code class="fe ob oc od oe b">3</code>隐藏层，每个层分别带有<code class="fe ob oc od oe b">50,20 and 10</code>神经元。此外，我们将最大迭代次数设置为<code class="fe ob oc od oe b">100</code>，学习率设置为<code class="fe ob oc od oe b">0.1</code>。这些就是我在介绍中提到的<strong class="mi iu">超参数</strong>。我们不会在这里对它们进行微调。</p><pre class="kj kk kl km gt og oe oh oi aw oj bi"><span id="7449" class="lr la it oe b gy ok ol l om on"># Split the data into train/test sets<br/>X_train, X_test = X[:60000], X[60000:]<br/>y_train, y_test = y[:60000], y[60000:]<br/><br/>classifier = <a class="ae ky" href="https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html#sklearn.neural_network.MLPClassifier" rel="noopener ugc nofollow" target="_blank">MLPClassifier</a>(<br/>    hidden_layer_sizes=(50,20,10),<br/>    max_iter=100,<br/>    alpha=1e-4,<br/>    solver="sgd",<br/>    verbose=10,<br/>    random_state=1,<br/>    learning_rate_init=0.1,<br/>)</span><span id="d603" class="lr la it oe b gy oo ol l om on"># fit the model on the training data<br/>classifier.fit(X_train, y_train)</span></pre><h2 id="3a0b" class="lr la it bd lb ls lt dn lf lu lv dp lj lw lx ly ll lz ma mb ln mc md me lp mf bi translated">2.4 模型评估</h2><p id="bfc8" class="pw-post-body-paragraph mg mh it mi b mj mk ju ml mm mn jx mo lw mp mq mr lz ms mt mu mc mv mw mx my im bi translated">现在，让我们来评估这个模型。我们将估计训练和测试数据和标签的平均准确度。</p><pre class="kj kk kl km gt og oe oh oi aw oj bi"><span id="e9ff" class="lr la it oe b gy ok ol l om on">print("Training set score: %f" % classifier.score(X_train, y_train))<br/>print("Test set score: %f" % classifier.score(X_test, y_test))</span></pre><blockquote class="op oq or"><p id="a75f" class="mg mh nl mi b mj ng ju ml mm nh jx mo os ni mq mr ot nj mt mu ou nk mw mx my im bi translated">训练集得分:<code class="fe ob oc od oe b">0.998633</code> <br/>测试集得分:<code class="fe ob oc od oe b">0.970300</code></p></blockquote><p id="b7fe" class="pw-post-body-paragraph mg mh it mi b mj ng ju ml mm nh jx mo lw ni mq mr lz nj mt mu mc nk mw mx my im bi translated">伟大的成果！</p><h2 id="229f" class="lr la it bd lb ls lt dn lf lu lv dp lj lw lx ly ll lz ma mb ln mc md me lp mf bi translated">2.5 可视化成本函数演变</h2><p id="d025" class="pw-post-body-paragraph mg mh it mi b mj mk ju ml mm mn jx mo lw mp mq mr lz ms mt mu mc mv mw mx my im bi translated">训练中损失减少的速度有多快？我们来编一个好看的剧情吧！</p><pre class="kj kk kl km gt og oe oh oi aw oj bi"><span id="c113" class="lr la it oe b gy ok ol l om on">fig, axes = plt.subplots(1, 1)</span><span id="1ef2" class="lr la it oe b gy oo ol l om on">axes.plot(classifier.loss_curve_, 'o-')<br/>axes.set_xlabel("number of iteration")<br/>axes.set_ylabel("loss")<br/>plt.show()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ov"><img src="../Images/819fb45dbd18d154687ef409a8fff794.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CIXxI3p3NWX_IzbiBfBdoQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图 4:训练迭代中损失的演变。图由作者制作。</p></figure><p id="4640" class="pw-post-body-paragraph mg mh it mi b mj ng ju ml mm nh jx mo lw ni mq mr lz nj mt mu mc nk mw mx my im bi translated">在这里，我们看到损失在训练期间下降得非常快，并且在<code class="fe ob oc od oe b">40th</code>迭代之后饱和(记得我们将最大 100 次迭代定义为<strong class="mi iu">超参数</strong>)。</p><h2 id="eb42" class="lr la it bd lb ls lt dn lf lu lv dp lj lw lx ly ll lz ma mb ln mc md me lp mf bi translated">2.6 可视化学习到的重量</h2><p id="92bf" class="pw-post-body-paragraph mg mh it mi b mj mk ju ml mm mn jx mo lw mp mq mr lz ms mt mu mc mv mw mx my im bi translated">这里，我们首先需要了解权重(每层的学习模型参数)是如何存储的。</p><p id="95c7" class="pw-post-body-paragraph mg mh it mi b mj ng ju ml mm nh jx mo lw ni mq mr lz nj mt mu mc nk mw mx my im bi translated">根据<a class="ae ky" href="https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html" rel="noopener ugc nofollow" target="_blank">文档</a>，属性<code class="fe ob oc od oe b">classifier.coefs_</code>是权重数组的形状<code class="fe ob oc od oe b">(n_layers-1, )</code>列表，其中索引 I 处的权重矩阵表示层<code class="fe ob oc od oe b">i</code>和层<code class="fe ob oc od oe b">i+1</code>之间的权重。在这个例子中，我们定义了<strong class="mi iu"> 3 个隐藏层</strong>，我们还有<strong class="mi iu">输入层</strong>和<strong class="mi iu">输出层。</strong>因此，<strong class="mi iu"> </strong>我们期望层间权重有 4 个权重数组(图 5 中的<code class="fe ob oc od oe b">in-L1, L1-L2, L2-L3 </code>和<code class="fe ob oc od oe b"> L2-out</code>)。</p><p id="4269" class="pw-post-body-paragraph mg mh it mi b mj ng ju ml mm nh jx mo lw ni mq mr lz nj mt mu mc nk mw mx my im bi translated">类似地，<code class="fe ob oc od oe b">classifier.intercepts_</code>是<strong class="mi iu">偏置</strong>向量的列表，其中索引<code class="fe ob oc od oe b">i</code>处的向量表示添加到层<code class="fe ob oc od oe b">i+1</code>的偏置值。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ow"><img src="../Images/92a43f14cd96f851f426bd38fc93ee8f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ueDnk29cALtbNaah8D_axw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图 5:作者在 Notes (iOS)上的手工图。</p></figure><p id="4b74" class="pw-post-body-paragraph mg mh it mi b mj ng ju ml mm nh jx mo lw ni mq mr lz nj mt mu mc nk mw mx my im bi translated">让我们验证一下:</p><pre class="kj kk kl km gt og oe oh oi aw oj bi"><span id="d956" class="lr la it oe b gy ok ol l om on">len(classifier.intercepts_) == len(classifier.coefs_) == 4</span></pre><p id="de70" class="pw-post-body-paragraph mg mh it mi b mj ng ju ml mm nh jx mo lw ni mq mr lz nj mt mu mc nk mw mx my im bi translated">正确返回<code class="fe ob oc od oe b">True</code>。</p><p id="5315" class="pw-post-body-paragraph mg mh it mi b mj ng ju ml mm nh jx mo lw ni mq mr lz nj mt mu mc nk mw mx my im bi translated">提醒:<strong class="mi iu">输入</strong> <strong class="mi iu">层</strong> <strong class="mi iu">权重</strong> <strong class="mi iu">矩阵</strong>将具有形状<code class="fe ob oc od oe b">784 x #neurons_in_1st_hidden_layer.</code><strong class="mi iu">输出层权重矩阵将具有形状</strong> <code class="fe ob oc od oe b">#neurons_in_3rd_hidden_layer x #number_of_classes.</code></p><h2 id="fe7f" class="lr la it bd lb ls lt dn lf lu lv dp lj lw lx ly ll lz ma mb ln mc md me lp mf bi translated">可视化输入层的学习权重</h2><pre class="kj kk kl km gt og oe oh oi aw oj bi"><span id="916c" class="lr la it oe b gy ok ol l om on">target_layer = 0 #0 is input, 1 is 1st hidden etc</span><span id="3a3c" class="lr la it oe b gy oo ol l om on">fig, axes = plt.subplots(1, 1, figsize=(15,6))<br/>axes.imshow(np.transpose(classifier.coefs_[target_layer]), cmap=plt.get_cmap("gray"), aspect="auto")</span><span id="195a" class="lr la it oe b gy oo ol l om on">axes.set_xlabel(f"number of neurons in {target_layer}")<br/>axes.set_ylabel("neurons in output layer")<br/>plt.show()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ox"><img src="../Images/7420cb9f9955928a90b1871ab03229d2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xc7SuVJTO6xzebMaiVrcBg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图 6:输入和第一个隐藏层之间的神经元的学习权重的可视化。图由作者制作。</p></figure><p id="097b" class="pw-post-body-paragraph mg mh it mi b mj ng ju ml mm nh jx mo lw ni mq mr lz nj mt mu mc nk mw mx my im bi translated">或者把它们重新塑造成 2D 的形象。</p><pre class="kj kk kl km gt og oe oh oi aw oj bi"><span id="c236" class="lr la it oe b gy ok ol l om on"># choose layer to plot<br/>target_layer = 0 #0 is input, 1 is 1st hidden etc</span><span id="f27e" class="lr la it oe b gy oo ol l om on">fig, axes = <a class="ae ky" href="https://matplotlib.org/api/_as_gen/matplotlib.pyplot.subplots.html#matplotlib.pyplot.subplots" rel="noopener ugc nofollow" target="_blank">plt.subplots</a>(4, 4)<br/>vmin, vmax = classifier.coefs_[0].min(), classifier.coefs_[target_layer].max()</span><span id="8808" class="lr la it oe b gy oo ol l om on">for coef, ax in zip(classifier.coefs_[0].T, axes.ravel()):<br/>    ax.matshow(coef.reshape(28, 28), cmap=plt.cm.gray, vmin=0.5 * vmin, vmax=0.5 * vmax)<br/>    ax.set_xticks(())<br/>    ax.set_yticks(())<br/><a class="ae ky" href="https://matplotlib.org/api/_as_gen/matplotlib.pyplot.show.html#matplotlib.pyplot.show" rel="noopener ugc nofollow" target="_blank">plt.show</a>()</span></pre><h1 id="17d7" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">3.摘要</h1><p id="d70a" class="pw-post-body-paragraph mg mh it mi b mj mk ju ml mm mn jx mo lw mp mq mr lz ms mt mu mc mv mw mx my im bi translated">MLP 分类器是一个非常强大的神经网络模型，能够学习复杂数据的非线性函数。该方法使用前向传播来构建权重，然后计算损失。接下来，使用反向传播来更新权重，从而减少损失。这是以迭代的方式完成的， <strong class="mi iu">迭代</strong>的<strong class="mi iu">号</strong> <strong class="mi iu">是一个输入<strong class="mi iu">超参数</strong>，如我在介绍中所解释的。其他重要的<strong class="mi iu">超参数</strong>是每个隐层中</strong> <strong class="mi iu">神经元</strong>的<strong class="mi iu">数量</strong> <strong class="mi iu">和总共的<strong class="mi iu">隐层数量</strong>。这些都需要微调。</strong></p></div><div class="ab cl mz na hx nb" role="separator"><span class="nc bw bk nd ne nf"/><span class="nc bw bk nd ne nf"/><span class="nc bw bk nd ne"/></div><div class="im in io ip iq"><p id="7fb0" class="pw-post-body-paragraph mg mh it mi b mj ng ju ml mm nh jx mo lw ni mq mr lz nj mt mu mc nk mw mx my im bi translated">那都是乡亲们！希望你喜欢这篇文章！</p><h2 id="0aa5" class="lr la it bd lb ls lt dn lf lu lv dp lj lw lx ly ll lz ma mb ln mc md me lp mf bi translated">只需 5 秒钟就可以订阅我的邮件列表:<a class="ae ky" href="https://seralouk.medium.com/subscribe" rel="noopener">https://seralouk.medium.com/subscribe</a></h2><h1 id="65ca" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">敬请关注并支持这一努力</h1><p id="14ef" class="pw-post-body-paragraph mg mh it mi b mj mk ju ml mm mn jx mo lw mp mq mr lz ms mt mu mc mv mw mx my im bi translated">如果你喜欢并发现这篇文章有用，请关注我！</p><p id="2da2" class="pw-post-body-paragraph mg mh it mi b mj ng ju ml mm nh jx mo lw ni mq mr lz nj mt mu mc nk mw mx my im bi translated">有问题吗？把它们作为评论贴出来，我会尽快回复。</p></div><div class="ab cl mz na hx nb" role="separator"><span class="nc bw bk nd ne nf"/><span class="nc bw bk nd ne nf"/><span class="nc bw bk nd ne"/></div><div class="im in io ip iq"><h1 id="9d89" class="kz la it bd lb lc oy le lf lg oz li lj jz pa ka ll kc pb kd ln kf pc kg lp lq bi translated">最新帖子</h1><div class="pd pe gp gr pf pg"><a href="https://medium.com/mlearning-ai/how-to-use-python-sql-to-manipulate-data-in-1-min-bbf9ec17dc5d" rel="noopener follow" target="_blank"><div class="ph ab fo"><div class="pi ab pj cl cj pk"><h2 class="bd iu gy z fp pl fr fs pm fu fw is bi translated">如何使用 Python &amp; SQL 在 1 分钟内操作数据</h2><div class="pn l"><h3 class="bd b gy z fp pl fr fs pm fu fw dk translated">请继续阅读！</h3></div><div class="po l"><p class="bd b dl z fp pl fr fs pm fu fw dk translated">medium.com</p></div></div><div class="pp l"><div class="pq l pr ps pt pp pu ks pg"/></div></div></a></div><div class="pd pe gp gr pf pg"><a rel="noopener follow" target="_blank" href="/time-series-forecasting-predicting-stock-prices-using-facebooks-prophet-model-9ee1657132b5"><div class="ph ab fo"><div class="pi ab pj cl cj pk"><h2 class="bd iu gy z fp pl fr fs pm fu fw is bi translated">时间序列预测:用脸书的先知模型预测股票价格</h2><div class="pn l"><h3 class="bd b gy z fp pl fr fs pm fu fw dk translated">使用可从《先知脸书》公开获得的预测模型预测股票价格</h3></div><div class="po l"><p class="bd b dl z fp pl fr fs pm fu fw dk translated">towardsdatascience.com</p></div></div><div class="pp l"><div class="pv l pr ps pt pp pu ks pg"/></div></div></a></div><div class="pd pe gp gr pf pg"><a rel="noopener follow" target="_blank" href="/roc-curve-explained-using-a-covid-19-hypothetical-example-binary-multi-class-classification-bab188ea869c"><div class="ph ab fo"><div class="pi ab pj cl cj pk"><h2 class="bd iu gy z fp pl fr fs pm fu fw is bi translated">用新冠肺炎假设的例子解释 ROC 曲线:二分类和多分类…</h2><div class="pn l"><h3 class="bd b gy z fp pl fr fs pm fu fw dk translated">在这篇文章中，我清楚地解释了什么是 ROC 曲线以及如何阅读它。我用一个新冠肺炎的例子来说明我的观点，我…</h3></div><div class="po l"><p class="bd b dl z fp pl fr fs pm fu fw dk translated">towardsdatascience.com</p></div></div><div class="pp l"><div class="pw l pr ps pt pp pu ks pg"/></div></div></a></div><div class="pd pe gp gr pf pg"><a rel="noopener follow" target="_blank" href="/support-vector-machines-svm-clearly-explained-a-python-tutorial-for-classification-problems-29c539f3ad8"><div class="ph ab fo"><div class="pi ab pj cl cj pk"><h2 class="bd iu gy z fp pl fr fs pm fu fw is bi translated">支持向量机(SVM)解释清楚:分类问题的 python 教程…</h2><div class="pn l"><h3 class="bd b gy z fp pl fr fs pm fu fw dk translated">在这篇文章中，我解释了支持向量机的核心，为什么以及如何使用它们。此外，我还展示了如何绘制支持…</h3></div><div class="po l"><p class="bd b dl z fp pl fr fs pm fu fw dk translated">towardsdatascience.com</p></div></div><div class="pp l"><div class="px l pr ps pt pp pu ks pg"/></div></div></a></div><div class="pd pe gp gr pf pg"><a rel="noopener follow" target="_blank" href="/everything-you-need-to-know-about-min-max-normalization-in-python-b79592732b79"><div class="ph ab fo"><div class="pi ab pj cl cj pk"><h2 class="bd iu gy z fp pl fr fs pm fu fw is bi translated">关于 Python 中的最小-最大规范化，您需要知道的一切</h2><div class="pn l"><h3 class="bd b gy z fp pl fr fs pm fu fw dk translated">在这篇文章中，我将解释什么是最小-最大缩放，什么时候使用它，以及如何使用 scikit 在 Python 中实现它</h3></div><div class="po l"><p class="bd b dl z fp pl fr fs pm fu fw dk translated">towardsdatascience.com</p></div></div><div class="pp l"><div class="py l pr ps pt pp pu ks pg"/></div></div></a></div><div class="pd pe gp gr pf pg"><a rel="noopener follow" target="_blank" href="/how-and-why-to-standardize-your-data-996926c2c832"><div class="ph ab fo"><div class="pi ab pj cl cj pk"><h2 class="bd iu gy z fp pl fr fs pm fu fw is bi translated">Scikit-Learn 的标准定标器如何工作</h2><div class="pn l"><h3 class="bd b gy z fp pl fr fs pm fu fw dk translated">在这篇文章中，我将解释为什么以及如何使用 scikit-learn 应用标准化</h3></div><div class="po l"><p class="bd b dl z fp pl fr fs pm fu fw dk translated">towardsdatascience.com</p></div></div><div class="pp l"><div class="pz l pr ps pt pp pu ks pg"/></div></div></a></div><h1 id="2722" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">参考</h1><p id="8b87" class="pw-post-body-paragraph mg mh it mi b mj mk ju ml mm mn jx mo lw mp mq mr lz ms mt mu mc mv mw mx my im bi translated">[1]<a class="ae ky" href="https://en.wikipedia.org/wiki/Multilayer_perceptron" rel="noopener ugc nofollow" target="_blank">https://en.wikipedia.org/wiki/Multilayer_perceptron</a></p><p id="eb67" class="pw-post-body-paragraph mg mh it mi b mj ng ju ml mm nh jx mo lw ni mq mr lz nj mt mu mc nk mw mx my im bi translated">[2]<a class="ae ky" href="https://scikit-learn.org/stable/modules/neural_networks_supervised.html#mlp-tips" rel="noopener ugc nofollow" target="_blank">https://sci kit-learn . org/stable/modules/neural _ networks _ supervised . html # MLP-tips</a></p><p id="dd3a" class="pw-post-body-paragraph mg mh it mi b mj ng ju ml mm nh jx mo lw ni mq mr lz nj mt mu mc nk mw mx my im bi translated">[3]<a class="ae ky" href="https://scikit-learn.org/stable/modules/neural_networks_supervised.html#mathematical-formulation" rel="noopener ugc nofollow" target="_blank">https://scikit-learn . org/stable/modules/neural _ networks _ supervised . html # mathematical-formulation</a></p><p id="9c94" class="pw-post-body-paragraph mg mh it mi b mj ng ju ml mm nh jx mo lw ni mq mr lz nj mt mu mc nk mw mx my im bi translated"><a class="ae ky" href="https://en.wikipedia.org/wiki/Backpropagation" rel="noopener ugc nofollow" target="_blank">https://en.wikipedia.org/wiki/Backpropagation</a></p><p id="6ea4" class="pw-post-body-paragraph mg mh it mi b mj ng ju ml mm nh jx mo lw ni mq mr lz nj mt mu mc nk mw mx my im bi translated"><a class="ae ky" href="https://en.wikipedia.org/wiki/MNIST_database" rel="noopener ugc nofollow" target="_blank">https://en.wikipedia.org/wiki/MNIST_database</a></p><h1 id="8397" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">和我联系</h1><ul class=""><li id="8f00" class="nm nn it mi b mj mk mm mn lw qa lz qb mc qc my nr ns nt nu bi translated"><strong class="mi iu">邮件列表:</strong><a class="ae ky" href="https://seralouk.medium.com/subscribe" rel="noopener">https://seralouk.medium.com/subscribe</a></li><li id="54cc" class="nm nn it mi b mj nv mm nw lw nx lz ny mc nz my nr ns nt nu bi translated"><strong class="mi iu">领英</strong>:<a class="ae ky" href="https://www.linkedin.com/in/serafeim-loukas/" rel="noopener ugc nofollow" target="_blank">https://www.linkedin.com/in/serafeim-loukas/</a></li><li id="4c1e" class="nm nn it mi b mj nv mm nw lw nx lz ny mc nz my nr ns nt nu bi translated"><strong class="mi iu">研究之门</strong>:<a class="ae ky" href="https://www.researchgate.net/profile/Serafeim_Loukas" rel="noopener ugc nofollow" target="_blank">https://www.researchgate.net/profile/Serafeim_Loukas</a></li></ul></div></div>    
</body>
</html>