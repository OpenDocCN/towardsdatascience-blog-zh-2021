<html>
<head>
<title>Spam Email Classifier with KNN — From Scratch (Python)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">KNN垃圾邮件分类器—从零开始(Python)</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/spam-email-classifier-with-knn-from-scratch-python-6e68eeb50a9e?source=collection_archive---------4-----------------------#2021-08-08">https://towardsdatascience.com/spam-email-classifier-with-knn-from-scratch-python-6e68eeb50a9e?source=collection_archive---------4-----------------------#2021-08-08</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="4d72" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">使用Python实现KNN算法对垃圾邮件进行分类的分步指南</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/e55a08701b20b93ea239c65e7d805f95.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*7qgebPOGgaDFQUQ0"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">马特·里德利在<a class="ae kv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><h1 id="8a12" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">什么是KNN？</h1><p id="f8d6" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">KNN是一种非常简单的监督学习算法。然而，与传统的监督学习算法(如多项式朴素贝叶斯算法)不同，KNN没有独立的训练阶段，也没有基于训练模型预测测试数据标签的阶段。相反，实时地将每个测试数据项的特征与每个训练数据项的特征进行比较，然后选择K个最接近的训练数据项，并且将其中最频繁的类别给予测试数据项。</p><p id="47d9" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">在电子邮件分类(垃圾邮件或ham)的上下文中，要比较的特征是每封电子邮件中单词的频率。欧几里德距离用于确定两个电子邮件之间的相似性；距离越小，越相似。算法中使用的欧几里德距离公式如下:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mp"><img src="../Images/7a196b4b30eca559cb9b5943ab9105e6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hKodkq_IiI8G70JR9XkGzg.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="33a1" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">一旦计算了测试电子邮件和每个训练电子邮件之间的欧几里德距离，就以升序(从最近到最远)对距离进行排序，并且选择K个最近的相邻电子邮件。如果大部分是垃圾邮件，则测试电子邮件被标记为垃圾邮件，否则，它被标记为垃圾邮件。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mq"><img src="../Images/b468745feb9abd6f25b12c51cea2f867.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fBx6h6cKin_RLE-RchuGNA.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="fcb8" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">在上面的例子中，K = 5；我们正在将我们想要分类的电子邮件与最近的5个邻居进行比较。在这种情况下，5封电子邮件中有3封被分类为ham(非垃圾邮件)，2封被分类为垃圾邮件。因此，未知的电子邮件将被赋予多数人的类别:火腿。现在我们已经看到了KNN是如何工作的，让我们继续使用代码实现分类器！</p></div><div class="ab cl mr ms hu mt" role="separator"><span class="mu bw bk mv mw mx"/><span class="mu bw bk mv mw mx"/><span class="mu bw bk mv mw"/></div><div class="ij ik il im in"><h1 id="6f1c" class="kw kx iq bd ky kz my lb lc ld mz lf lg jw na jx li jz nb ka lk kc nc kd lm ln bi translated">履行</h1><p id="9315" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">为了快速了解我们将用Python编写什么，编写伪代码总是一个好的做法:</p><pre class="kg kh ki kj gt nd ne nf ng aw nh bi"><span id="a121" class="ni kx iq ne b gy nj nk l nl nm">1. Load the spam and ham emails</span><span id="fbc7" class="ni kx iq ne b gy nn nk l nl nm">2. Remove common punctuation and symbols</span><span id="7548" class="ni kx iq ne b gy nn nk l nl nm">3. Lowercase all letters</span><span id="38bb" class="ni kx iq ne b gy nn nk l nl nm">4. Remove stopwords (very common words like pronouns, articles, etc.)</span><span id="21cc" class="ni kx iq ne b gy nn nk l nl nm">5. Split emails into training email and testing emails</span><span id="fe94" class="ni kx iq ne b gy nn nk l nl nm">6. For each test email, calculate the similarity between it and all training emails<br/>    6.1. For each word that exists in either test email or training email, count its frequency in both emails<br/>    6.2. calculate the euclidean distance between both emails to determine similarity</span><span id="57fb" class="ni kx iq ne b gy nn nk l nl nm">7. Sort the emails in ascending order of euclidean distance</span><span id="f6a2" class="ni kx iq ne b gy nn nk l nl nm">8. Select the k nearest neighbors (shortest distance)</span><span id="faf5" class="ni kx iq ne b gy nn nk l nl nm">9. Assign the class which is most frequent in the selected k nearest neighbours to the new email</span></pre><h2 id="b5ec" class="ni kx iq bd ky no np dn lc nq nr dp lg lx ns nt li mb nu nv lk mf nw nx lm ny bi translated">数据集</h2><p id="8ef6" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">垃圾邮件和ham(普通电子邮件)的电子邮件数据集是从“安然-垃圾邮件数据集”中获得的。可以在Enron2下的<a class="ae kv" href="http://nlp.cs.aueb.gr/software_and_datasets/Enron-Spam/index.html" rel="noopener ugc nofollow" target="_blank">http://NLP . cs . aueb . gr/software _ and _ datasets/Enron-Spam/index . html</a>找到。我们使用的数据集包含5857封电子邮件。每封电子邮件都存储在一个文本文件中，文本文件被划分并存储在两个文件夹中，ham文件夹和spam文件夹。这意味着电子邮件已经被标记。每个文本文件都将被程序加载，每封邮件都将被读取并存储为一个字符串变量。字符串中的每个不同的单词都将被视为一个特征。</p><h2 id="cacf" class="ni kx iq bd ky no np dn lc nq nr dp lg lx ns nt li mb nu nv lk mf nw nx lm ny bi translated">使用的库</h2><pre class="kg kh ki kj gt nd ne nf ng aw nh bi"><span id="e496" class="ni kx iq ne b gy nj nk l nl nm">import os<br/>import string<br/>from nltk.corpus import stopwords<br/>from sklearn.model_selection import train_test_split<br/>from sklearn.metrics import accuracy_score<br/>import numpy as np</span></pre><p id="eb78" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated"><strong class="lq ir"> os </strong>打开并读取文件的库。<br/> <strong class="lq ir">字符串</strong>为标点符号列表<br/> <strong class="lq ir">停用词</strong>包含停用词列表。<br/> <strong class="lq ir"> train_test_split </strong>将数据拆分成训练和测试数据。<br/> <strong class="lq ir"> accuracy_score </strong>计算算法的准确度。<br/> <strong class="lq ir"> numpy </strong>允许高级数组操作。</p><h2 id="a36d" class="ni kx iq bd ky no np dn lc nq nr dp lg lx ns nt li mb nu nv lk mf nw nx lm ny bi translated">加载数据</h2><pre class="kg kh ki kj gt nd ne nf ng aw nh bi"><span id="b477" class="ni kx iq ne b gy nj nk l nl nm">def load_data():<br/>    print("Loading data...")<br/>    <br/>    ham_files_location = os.listdir("dataset/ham")<br/>    spam_files_location = os.listdir("dataset/spam")<br/>    data = []</span></pre><p id="b0fa" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">返回一个文件夹中所有文件名的列表。这用于检索每个ham和spam文件夹中文本文件的所有文件名，并将它们分别存储在ham_files_location和spam_files_location中。数据是存储每个电子邮件文本及其相应标签的列表。</p><pre class="kg kh ki kj gt nd ne nf ng aw nh bi"><span id="1218" class="ni kx iq ne b gy nj nk l nl nm">    # Load ham email<br/>    for file_path in ham_files_location:<br/>        f = open("dataset/ham/" + file_path, "r")<br/>        text = str(f.read())<br/>        data.append([text, "ham"])<br/>    <br/>    # Load spam email<br/>    for file_path in spam_files_location:<br/>        f = open("dataset/spam/" + file_path, "r")<br/>        text = str(f.read())<br/>        data.append([text, "spam"])</span></pre><p id="fe80" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">我们遍历ham文本文件名列表，使用open()打开一个文件，然后使用str(f.read())将电子邮件文本作为一个字符串读取并存储在变量文本中。由文本构成的列表和相应的标签“ham”被附加到列表数据。</p><pre class="kg kh ki kj gt nd ne nf ng aw nh bi"><span id="a0e6" class="ni kx iq ne b gy nj nk l nl nm">    data = np.array(data)<br/>    <br/>    print("flag 1: loaded data")<br/>    return data</span></pre><p id="5a2f" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">列表数据被转换成一个numpy数组，以便以后更好地操作该数组。然后返回数据。</p><h2 id="6612" class="ni kx iq bd ky no np dn lc nq nr dp lg lx ns nt li mb nu nv lk mf nw nx lm ny bi translated">数据预处理</h2><pre class="kg kh ki kj gt nd ne nf ng aw nh bi"><span id="94f8" class="ni kx iq ne b gy nj nk l nl nm"># Preprocessing data: noise removal</span><span id="9589" class="ni kx iq ne b gy nn nk l nl nm">def preprocess_data(data):<br/>    print("Preprocessing data...")<br/>    <br/>    punc = string.punctuation           # Punctuation list<br/>    sw = stopwords.words('english')     # Stopwords list</span></pre><p id="12cf" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">punc保存一个标点和符号列表<br/> sw保存一个nltk.corpus库中的停用词列表</p><pre class="kg kh ki kj gt nd ne nf ng aw nh bi"><span id="52a1" class="ni kx iq ne b gy nj nk l nl nm">    for record in data:<br/>        # Remove common punctuation and symbols<br/>        for item in punc:<br/>            record[0] = record[0].replace(item, "")</span></pre><p id="9d8c" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">对于data中的每条记录，对于punc中的每项(符号或标点)，用空字符串替换该项，以从记录[0](电子邮件文本字符串)中删除该项。</p><pre class="kg kh ki kj gt nd ne nf ng aw nh bi"><span id="92fa" class="ni kx iq ne b gy nj nk l nl nm">        # Lowercase all letters and remove stopwords <br/>        splittedWords = record[0].split()<br/>        newText = ""<br/>        for word in splittedWords:<br/>            if word not in sw:<br/>                word = word.lower()<br/>                newText = newText + " " + word  </span><span id="7ea4" class="ni kx iq ne b gy nn nk l nl nm">        record[0] = newText<br/>        <br/>    print("flag 2: preprocessed data")        <br/>    return data</span></pre><p id="6642" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">对电子邮件文本记录[0]使用split()方法返回电子邮件中所有单词的列表。遍历单词列表，如果单词不在停用词列表中，将其设置为小写，并将该单词添加到newText。新文本将包含电子邮件，但没有停用词。newText被分配回记录[0]。预处理完每个记录[0]后，返回干净的数据。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nz"><img src="../Images/f1bd574087fc9d8f378f391cbb40c006.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UWhn4jIVCNsRBGmi0e1XVw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">预处理电子邮件之前和之后</p></figure><h2 id="a64e" class="ni kx iq bd ky no np dn lc nq nr dp lg lx ns nt li mb nu nv lk mf nw nx lm ny bi translated">将数据分成训练集和测试集</h2><p id="e936" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">数据集分为训练集(73%)和测试集(27%)。</p><pre class="kg kh ki kj gt nd ne nf ng aw nh bi"><span id="cf1a" class="ni kx iq ne b gy nj nk l nl nm"># Splitting original dataset into training dataset and test dataset</span><span id="3742" class="ni kx iq ne b gy nn nk l nl nm">def split_data(data):<br/>    print("Splitting data...")<br/>    <br/>    features = data[:, 0]   # array containing all email text bodies<br/>    labels = data[:, 1]     # array containing corresponding labels<br/>    print(labels)<br/>    training_data, test_data, training_labels, test_labels =\<br/>        train_test_split(features, labels, test_size = 0.27, random_state = 42)<br/>    <br/>    print("flag 3: splitted data")<br/>    return training_data, test_data, training_labels, test_labels</span></pre><p id="cee2" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">首先，有必要将电子邮件文本放在一个自己的数组中，并将标签放在另一个自己的数组中。因此，电子邮件文本存储在特征中，标签存储在标签中。然后使用train_test_split方法将数据拆分为training_data、test_data、training_labels和test_labels。随机状态被设置为42，以确保出于测试目的获得相同的随机洗牌输出。拆分后，将返回training_data、test_data、training_labels和test_labels。</p><h2 id="458d" class="ni kx iq bd ky no np dn lc nq nr dp lg lx ns nt li mb nu nv lk mf nw nx lm ny bi translated">KNN算法</h2><p id="a557" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated"><strong class="lq ir"> get_count()函数</strong></p><pre class="kg kh ki kj gt nd ne nf ng aw nh bi"><span id="1333" class="ni kx iq ne b gy nj nk l nl nm">def get_count(text):<br/>    wordCounts = dict()<br/>    for word in text.split():<br/>        if word in wordCounts:<br/>            wordCounts[word] += 1<br/>        else:<br/>            wordCounts[word] = 1<br/>    <br/>    return wordCounts</span></pre><p id="fa48" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">该函数获取一个电子邮件文本，并使用split()将其拆分。统计电子邮件中每个单词的出现频率，并保存在wordCounts中，这是字典数据类型。然后返回字典字数。</p><p id="20e3" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated"><strong class="lq ir">欧几里得_差分()函数</strong></p><pre class="kg kh ki kj gt nd ne nf ng aw nh bi"><span id="4614" class="ni kx iq ne b gy nj nk l nl nm">def euclidean_difference(test_WordCounts, training_WordCounts):<br/>    total = 0</span></pre><p id="a29a" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">这个函数接受一个测试邮件的字数字典test_WordCounts和另一个训练邮件的字典training_wordCounts。total存储测试和训练电子邮件中单词频率的平方差之和。</p><pre class="kg kh ki kj gt nd ne nf ng aw nh bi"><span id="4143" class="ni kx iq ne b gy nj nk l nl nm">    for word in test_WordCounts:<br/>        if word in test_WordCounts and word in training_WordCounts:<br/>            total += (test_WordCounts[word] - training_WordCounts[word])**2</span></pre><p id="bc0c" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">首先，我们迭代测试电子邮件字典中的单词。对于每个单词，有三种情况。第一种情况是它同时存在于测试邮件和培训邮件中。在这种情况下，total会随着单词在测试电子邮件和培训电子邮件中出现频率的平方差而增加。</p><pre class="kg kh ki kj gt nd ne nf ng aw nh bi"><span id="a19a" class="ni kx iq ne b gy nj nk l nl nm">            del training_WordCounts[word]</span></pre><p id="95de" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">然后，从训练电子邮件字典中删除常用单词，以加速下一个for循环</p><pre class="kg kh ki kj gt nd ne nf ng aw nh bi"><span id="e16e" class="ni kx iq ne b gy nj nk l nl nm">        else:<br/>            total += test_WordCounts[word]**2</span></pre><p id="826a" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">第二种情况是这个词只出现在测试邮件中。在这种情况下，没有必要找出差异(因为它的频率在训练邮件中是0)，所以我们只需将单词的平方频率加到总数中。</p><pre class="kg kh ki kj gt nd ne nf ng aw nh bi"><span id="2466" class="ni kx iq ne b gy nj nk l nl nm">    for word in training_WordCounts:<br/>            total += training_WordCounts[word]**2</span></pre><p id="109d" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">最后一种情况是这个词只在培训邮件里。由于我们在前面的for循环中删除了所有常用单词，因此我们只需循环遍历训练电子邮件字典，并将每个单词的频率平方添加到total中。</p><pre class="kg kh ki kj gt nd ne nf ng aw nh bi"><span id="5724" class="ni kx iq ne b gy nj nk l nl nm">    return total**0.5</span></pre><p id="9faf" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">最后，total的平方根(每个单词的频率的平方差的和的平方根)作为double返回。这是欧几里德距离计算函数的结尾。</p><p id="740f" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated"><strong class="lq ir"> get_class()函数</strong></p><pre class="kg kh ki kj gt nd ne nf ng aw nh bi"><span id="b435" class="ni kx iq ne b gy nj nk l nl nm">def get_class(selected_Kvalues):<br/>    spam_count = 0<br/>    ham_count = 0</span></pre><p id="4a26" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">该函数接受所选的K个最近邻居的列表，以确定当前测试电子邮件的类别。spam_count和ham_count分别存储每个“spam”标签和“ham”标签在K个选定的最近邻居中的出现频率。</p><pre class="kg kh ki kj gt nd ne nf ng aw nh bi"><span id="5dbf" class="ni kx iq ne b gy nj nk l nl nm">    for value in selected_Kvalues:<br/>        if value[0] == "spam":<br/>            spam_count += 1<br/>        else:<br/>            ham_count += 1</span></pre><p id="336b" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">使用for循环，对于K个选定值中的每个值，如果标签值[0]等于“spam ”,则spam_count递增1。否则，ham_count递增1。</p><pre class="kg kh ki kj gt nd ne nf ng aw nh bi"><span id="82f5" class="ni kx iq ne b gy nj nk l nl nm">    if spam_count &gt; ham_count:<br/>        return "spam"<br/>    else:<br/>        return "ham"</span></pre><p id="fad7" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">在for循环之后，如果spam_count大于ham_count，则表示当前测试邮件有更大的倾向是垃圾邮件，因此返回一个字符串“spam”作为预测标签。否则，字符串“ham”将作为预测标签返回。</p><p id="a385" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated"><strong class="lq ir"> knn_classifier()函数</strong></p><pre class="kg kh ki kj gt nd ne nf ng aw nh bi"><span id="e2e0" class="ni kx iq ne b gy nj nk l nl nm">def knn_classifier(training_data, training_labels, test_data, K, tsize):<br/>    print("Running KNN Classifier...")<br/>    <br/>    result = []<br/>    counter = 1</span></pre><p id="435a" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">这是KNN分类器函数。它接收训练邮件、训练标签、测试数据、K值，以及原始27%测试邮件中要测试的测试邮件的数量。结果是包含预测标签的列表。计数器将仅用于显示目的，以指示程序运行时的进度。</p><pre class="kg kh ki kj gt nd ne nf ng aw nh bi"><span id="11a3" class="ni kx iq ne b gy nj nk l nl nm">    # word counts for training email<br/>    training_WordCounts = [] <br/>    for training_text in training_data:<br/>            training_WordCounts.append(get_count(training_text))</span></pre><p id="240f" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">由于训练集是恒定的，我们可以一劳永逸地统计每封训练邮件中的词频。因此，对于训练数据中的每个电子邮件文本，使用get_count()获得其词频字典。然后，将该词典添加到要存储的training_WordCounts列表中。</p><pre class="kg kh ki kj gt nd ne nf ng aw nh bi"><span id="d6da" class="ni kx iq ne b gy nj nk l nl nm">    for test_text in test_data:<br/>        similarity = [] # List of euclidean distances<br/>        test_WordCounts = get_count(test_text)  # word counts for test email</span></pre><p id="5147" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">现在，对于测试数据中的每封测试邮件，都执行以下操作。声明了空列表相似性。它将存储当前测试电子邮件和每个培训电子邮件之间的欧几里德距离。然后，使用get_count()获得测试邮件的词频词典。</p><pre class="kg kh ki kj gt nd ne nf ng aw nh bi"><span id="0b27" class="ni kx iq ne b gy nj nk l nl nm">        # Getting euclidean difference <br/>        for index in range(len(training_data)):<br/>            euclidean_diff =\<br/>                euclidean_difference(test_WordCounts, training_WordCounts[index])<br/>            similarity.append([training_labels[index], euclidean_diff])</span></pre><p id="6e9e" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">因为我们已经有了所有训练邮件和当前测试邮件的词频词典。我们可以继续使用迭代x次的for循环来计算当前测试电子邮件和每个训练电子邮件之间的欧几里德距离，其中x等于训练数据集的大小。在每次迭代之后，计算的欧几里德距离连同训练电子邮件的相应标签一起被附加到相似性列表。</p><pre class="kg kh ki kj gt nd ne nf ng aw nh bi"><span id="c655" class="ni kx iq ne b gy nj nk l nl nm">        # Sort list in ascending order based on euclidean difference<br/>        similarity = sorted(similarity, key = lambda i:i[1])</span></pre><p id="a811" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">在存储了所有欧几里得距离之后。我们基于第二列，即基于欧几里德距离(从最近到最远)，以升序对相似性列表进行排序。</p><pre class="kg kh ki kj gt nd ne nf ng aw nh bi"><span id="8c0d" class="ni kx iq ne b gy nj nk l nl nm">        # Select K nearest neighbours<br/>        selected_Kvalues = [] <br/>        for i in range(K):<br/>            selected_Kvalues.append(similarity[i])</span></pre><p id="e8b1" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">现在，由于相似性列表已经排序，我们可以使用简单的for循环轻松地将最近的K个邻居添加到selected_Kvalues列表中。</p><pre class="kg kh ki kj gt nd ne nf ng aw nh bi"><span id="ceb3" class="ni kx iq ne b gy nj nk l nl nm">        # Predicting the class of email<br/>        result.append(get_class(selected_Kvalues))</span></pre><p id="f4a6" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">最后，在进入下一封测试邮件之前。我们使用get_class()来确定当前测试电子邮件的类别。现在，我们已经到了一次迭代的末尾，下一次迭代可以开始分类下一封测试邮件了。</p><pre class="kg kh ki kj gt nd ne nf ng aw nh bi"><span id="358c" class="ni kx iq ne b gy nj nk l nl nm">    return result</span></pre><p id="859f" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">一旦所有的测试邮件都被分类，并且for循环已经到达它的末端，那么包含预测标签列表的结果列表将被返回。</p><p id="6745" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated"><strong class="lq ir"> main()函数</strong></p><pre class="kg kh ki kj gt nd ne nf ng aw nh bi"><span id="a58a" class="ni kx iq ne b gy nj nk l nl nm">def main(K):<br/>    data = load_data()<br/>    data = preprocess_data(data)<br/>    training_data, test_data, training_labels, test_labels = split_data(data)</span></pre><p id="35e2" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">这是程序开始运行的主要功能。这是所有东西放在一起的地方。主函数接受K值。首先，使用load_data()加载所有电子邮件，然后存储在data中。然后使用preprocess_data()对电子邮件进行预处理，并再次存储在数据中。然后使用split_data()将数据拆分为training_data、test_data、training_labels和test_labels。</p><pre class="kg kh ki kj gt nd ne nf ng aw nh bi"><span id="a044" class="ni kx iq ne b gy nj nk l nl nm">    tsize = len(test_data)</span></pre><p id="4fd5" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">tsize指定了测试电子邮件的数量(在原始的27%测试数据中)来预测它们的标签。目前，tsize被设置为等于整个测试电子邮件集。</p><pre class="kg kh ki kj gt nd ne nf ng aw nh bi"><span id="7e6e" class="ni kx iq ne b gy nj nk l nl nm">    result = knn_classifier(training_data, training_labels, test_data[:tsize], K, tsize) <br/>    accuracy = accuracy_score(test_labels[:tsize], result)</span></pre><p id="5feb" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">现在，调用knn_classifier()函数来预测测试电子邮件的标签。返回的预测标签列表存储在结果中。之后，使用sklearn库中的accuracy_score()方法计算准确度。此方法将实际标签列表test_labels与预测标签列表结果进行比较。</p><pre class="kg kh ki kj gt nd ne nf ng aw nh bi"><span id="9f91" class="ni kx iq ne b gy nj nk l nl nm">    print("training data size\t: " + str(len(training_data)))<br/>    print("test data size\t\t: " + str(len(test_data)))<br/>    print("K value\t\t\t\t: " + str(K))<br/>    print("Samples tested\t\t: " + str(tsize))<br/>    print("% accuracy\t\t\t: " + str(accuracy * 100))<br/>    print("Number correct\t\t: " + str(int(accuracy * tsize)))<br/>    print("Number wrong\t\t: " + str(int((1 - accuracy) * tsize)))</span></pre><p id="1b30" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">这些行显示运行的详细信息，如训练数据大小、测试数据大小、K值、测试的样本数量、百分比准确度、正确识别的电子邮件数量以及错误识别的电子邮件数量。</p><pre class="kg kh ki kj gt nd ne nf ng aw nh bi"><span id="c982" class="ni kx iq ne b gy nj nk l nl nm">main(11)</span></pre><p id="717b" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">最后，这一行通过调用main函数启动程序，并赋予它K值(在本例中是11)。</p></div><div class="ab cl mr ms hu mt" role="separator"><span class="mu bw bk mv mw mx"/><span class="mu bw bk mv mw mx"/><span class="mu bw bk mv mw"/></div><div class="ij ik il im in"><h1 id="2541" class="kw kx iq bd ky kz my lb lc ld mz lf lg jw na jx li jz nb ka lk kc nc kd lm ln bi translated">产出和结论</h1><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oa"><img src="../Images/6b7f99b494324047a4b99c409cec0b52.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZkCSa3FSwU2HGSts3l3zdA.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">输出</p></figure><p id="574f" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">这是上面解释的所有代码的最终输出。可以看出，使用KNN算法将电子邮件分类为垃圾邮件和垃圾邮件，K值为11，测试数据大小为1582，其准确率为76.7%。虽然不是最好的，但还是令人满意的。需要注意的一个缺点是，对1582封邮件进行分类需要很长时间。这主要是由于高时间复杂度，这是在计算测试电子邮件和训练电子邮件之间的欧几里德差时三个嵌套for循环的结果。</p><p id="32d8" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">你可以在我的Github库<a class="ae kv" href="https://github.com/abdelrahman46/Spam-Filtering-KNN/blob/0e4e373d2839be4fd48a7a8556d741579ee5737b/SpamFiltering_KNN_AlgorithmCode.py" rel="noopener ugc nofollow" target="_blank">这里</a>找到完整的源代码。</p></div></div>    
</body>
</html>