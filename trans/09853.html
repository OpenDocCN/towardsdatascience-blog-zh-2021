<html>
<head>
<title>Convert Large JSON to Parquet with Dask</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用Dask将大型JSON转换为拼花地板</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/convert-large-json-to-parquet-with-dask-58c1e6a4e4fc?source=collection_archive---------25-----------------------#2021-09-15">https://towardsdatascience.com/convert-large-json-to-parquet-with-dask-58c1e6a4e4fc?source=collection_archive---------25-----------------------#2021-09-15</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="23e5" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">使用Coiled将75GB数据集上的ETL管道扩展到云</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/b15a9062c117e6169a0e73c0c413545e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4-HeIOlC5Bdnm1TzfF_wtQ.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图片由阿德里安·匡威通过<a class="ae kv" href="http://unsplash.com/" rel="noopener ugc nofollow" target="_blank">unsplash.com</a>拍摄</p></figure><h1 id="12bf" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">TL；博士；医生</h1><p id="a574" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">这篇文章演示了一个75GB数据集的JSON到Parquet的转换，它无需将数据集下载到本地机器上就可以运行。首先在本地迭代Dask来构建和测试你的管道，然后将相同的工作流程转移到云计算服务中，如<a class="ae kv" href="http://coiled.io/" rel="noopener ugc nofollow" target="_blank"> Coiled </a>和最小的代码变更。</p><p id="073a" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated"><em class="mp">免责声明:我在Coiled工作，是一名数据科学传道者实习生。</em> <a class="ae kv" href="http://coiled.io/" rel="noopener ugc nofollow" target="_blank"> <em class="mp"> Coiled </em> </a> <em class="mp">由</em><a class="ae kv" href="https://dask.org/" rel="noopener ugc nofollow" target="_blank"><em class="mp">Dask</em></a><em class="mp">的最初作者Matthew Rocklin创立，是一个面向分布式计算的开源Python库。</em></p><h1 id="84a6" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">为什么要将JSON转换成Parquet</h1><p id="7c52" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">从web上抓取的嵌套JSON格式的数据通常需要转换成表格格式，用于探索性数据分析(EDA)和/或机器学习(ML)。Parquet文件格式是存储表格数据的最佳方法，允许像列修剪和谓词下推过滤这样的操作，这<a class="ae kv" href="https://coiled.io/blog/parquet-column-pruning-predicate-pushdown/" rel="noopener ugc nofollow" target="_blank">极大地提高了工作流的性能</a>。</p><p id="6584" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">本文展示了一个JSON到Parquet的管道，用于Github Archive项目中的75GB数据集，使用Dask和Coiled将数据转换并存储到云对象存储中。该管道不需要将数据集本地存储在您的计算机上。</p><p id="e2ca" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">完成这篇文章后，你将能够:</p><ol class=""><li id="4aeb" class="mq mr iq lq b lr mk lu ml lx ms mb mt mf mu mj mv mw mx my bi translated">首先在本地构建并测试您的ETL工作流，使用一个简单的测试文件，这个文件可以很容易地放入本地机器的内存中。</li><li id="b817" class="mq mr iq lq b lr mz lu na lx nb mb nc mf nd mj mv mw mx my bi translated">使用Coiled将相同的工作流扩展到云中，以处理整个数据集。</li></ol><p id="75dc" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated"><em class="mp">剧透——y</em>你将在两种情况下运行完全相同的代码，只是改变了计算运行的位置。</p><p id="1e02" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">你可以在本笔记本中找到完整的代码示例。要在本地运行笔记本，使用位于笔记本存储库中的environment.yml文件构建conda环境。</p><h1 id="85b1" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">在本地构建您的管道</h1><p id="198e" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">首先在本地构建管道是一个很好的实践。上面链接的笔记本一步一步地引导你完成这个过程。我们将在这里总结这些步骤。</p><p id="b28a" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">我们将在2015年使用来自<a class="ae kv" href="https://www.gharchive.org/" rel="noopener ugc nofollow" target="_blank"> Github档案项目</a>的数据。这个数据集记录了Github上的所有公共活动，在未压缩的情况下占用大约75GB。</p><h2 id="540c" class="ne kx iq bd ky nf ng dn lc nh ni dp lg lx nj nk li mb nl nm lk mf nn no lm np bi">1.</h2><p id="8c60" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">首先从Github档案中提取一个文件。这代表1小时的数据，占用大约5MB的数据。这里不需要使用任何类型的并行或云计算，所以现在可以在本地迭代。</p><pre class="kg kh ki kj gt nq nr ns nt aw nu bi"><span id="0d06" class="ne kx iq nr b gy nv nw l nx ny">!wget <a class="ae kv" href="https://data.gharchive.org/2015-01-01-15.json.gz" rel="noopener ugc nofollow" target="_blank">https://data.gharchive.org/2015-01-01-15.json.gz</a></span></pre><p id="d1ce" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated"><em class="mp">只有在必要时才扩展到云，以避免不必要的成本和代码复杂性。</em></p><h2 id="22ac" class="ne kx iq bd ky nf ng dn lc nh ni dp lg lx nj nk li mb nl nm lk mf nn no lm np bi">2.</h2><p id="2ec7" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">太好了，你已经从源头提取了数据。现在，您可以将它转换成表格数据帧格式。数据中有几个不同的模式重叠，这意味着您不能简单地将其转换为pandas或Dask数据框架。相反，您可以过滤掉一个子集，比如PushEvents，并使用它。</p><pre class="kg kh ki kj gt nq nr ns nt aw nu bi"><span id="6081" class="ne kx iq nr b gy nv nw l nx ny">records.filter(lambda record: record["type"] == "PushEvent").take(1)</span></pre><p id="5b71" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">您可以应用process函数(在笔记本中定义)将嵌套的JSON数据展平成表格格式，现在每一行代表一个Github提交。</p><pre class="kg kh ki kj gt nq nr ns nt aw nu bi"><span id="aa07" class="ne kx iq nr b gy nv nw l nx ny">records.filter(lambda record: record["type"] == "PushEvent").take(1)</span><span id="41f0" class="ne kx iq nr b gy nz nw l nx ny">flattened = records.filter(lambda record: record["type"] ==   <br/>  "PushEvent").map(process).flatten()</span></pre><p id="3957" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">然后使用<code class="fe oa ob oc nr b">.to_dataframe()</code>方法将这些数据转换成数据帧。</p><pre class="kg kh ki kj gt nq nr ns nt aw nu bi"><span id="dfa9" class="ne kx iq nr b gy nv nw l nx ny">df = flattened.to_dataframe()</span></pre><h2 id="2fce" class="ne kx iq bd ky nf ng dn lc nh ni dp lg lx nj nk li mb nl nm lk mf nn no lm np bi">3.</h2><p id="8833" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">现在，您已经准备好使用Dask DataFrame <code class="fe oa ob oc nr b">.to_parquet()</code>方法将数据帧作为. parquet文件写入本地目录。</p><pre class="kg kh ki kj gt nq nr ns nt aw nu bi"><span id="f594" class="ne kx iq nr b gy nv nw l nx ny">df.to_parquet( "test.parq", engine="pyarrow", compression="snappy" )</span></pre><h1 id="e1a0" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">利用卷绕式Dask集群进行横向扩展</h1><p id="2095" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">伟大的工作建立和测试你的工作流程在本地！现在让我们构建一个工作流，该工作流将收集一整年的数据，对其进行处理，并将其保存到云对象存储中。</p><p id="8c5e" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">我们将首先在云中启动一个Dask集群，它可以在整个数据集上运行您的管道。要运行本节中的代码，您需要登录<a class="ae kv" href="http://cloud.coiled.io" rel="noopener ugc nofollow" target="_blank"> Coiled Cloud </a>获得一个免费的Coiled帐户。你只需要提供你的Github凭证来创建一个帐户。</p><p id="a5f0" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">然后，您需要用正确的库创建一个软件环境，以便集群中的工作人员能够执行我们的计算。</p><pre class="kg kh ki kj gt nq nr ns nt aw nu bi"><span id="7220" class="ne kx iq nr b gy nv nw l nx ny">import coiled </span><span id="f89c" class="ne kx iq nr b gy nz nw l nx ny"># create Coiled software environment coiled.create_software_environment(<br/>    name="github-parquet", <br/>    conda=["dask", "pyarrow", "s3fs", "ujson", "requests", "lz4", "fastparquet"]<br/>)</span></pre><p id="df0c" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated"><em class="mp">您还可以使用Docker images、environment.yml (conda)或requirements.txt (pip)文件创建Coiled软件环境。更多信息，请查看</em> <a class="ae kv" href="https://docs.coiled.io/user_guide/software_environment_creation.html" rel="noopener ugc nofollow" target="_blank"> <em class="mp">盘绕文档</em> </a> <em class="mp">。</em></p><p id="d1a2" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">现在，让我们启动您的盘绕式集群，指定集群名称、它运行的软件环境以及Dask工作线程的数量。</p><pre class="kg kh ki kj gt nq nr ns nt aw nu bi"><span id="fea6" class="ne kx iq nr b gy nv nw l nx ny"># spin up a Coiled cluster <br/>cluster = coiled.Cluster(<br/>    name="github-parquet", <br/>    software="coiled-examples/github-parquet", <br/>    n_workers=10<br/>)</span></pre><p id="b400" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">最后，让Dask在您的盘绕式集群上运行计算。</p><pre class="kg kh ki kj gt nq nr ns nt aw nu bi"><span id="b372" class="ne kx iq nr b gy nv nw l nx ny"># connect Dask to your Coiled cluster <br/>from dask.distributed import Client <br/>client = Client(cluster) client</span></pre><p id="39da" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">我们期待已久的时刻！您的集群已经启动并运行，这意味着您已经准备好运行您在整个数据集上构建的JSON to Parquet管道。</p><p id="fa69" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">这需要对您的代码进行两处细微的更改:</p><ol class=""><li id="b115" class="mq mr iq lq b lr mk lu ml lx ms mb mt mf mu mj mv mw mx my bi translated">下载<em class="mp">Github的所有</em>存档文件，而不仅仅是一个测试文件</li><li id="d279" class="mq mr iq lq b lr mz lu na lx nb mb nc mf nd mj mv mw mx my bi translated">将df.to_parquet()指向s3存储桶，而不是本地存储桶</li></ol><p id="9738" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">请注意，下面的代码使用了一个文件名列表，其中包含2015年的所有文件和上面提到的流程函数。关于这两个物体的定义，请参考<a class="ae kv" href="https://github.com/coiled/coiled-resources/blob/main/json-to-parquet/github-parquet.ipynb" rel="noopener ugc nofollow" target="_blank">笔记本</a>。</p><pre class="kg kh ki kj gt nq nr ns nt aw nu bi"><span id="914d" class="ne kx iq nr b gy nv nw l nx ny">%%time <br/># read in json data <br/>records = db.read_text(filenames).map(ujson.loads) </span><span id="b4c8" class="ne kx iq nr b gy nz nw l nx ny"># filter out PushEvents <br/>push = records.filter(lambda record: record["type"] == "PushEvent") </span><span id="a12c" class="ne kx iq nr b gy nz nw l nx ny"># process into tabular format, each row is a single commit <br/>processed = push.map(process) </span><span id="01df" class="ne kx iq nr b gy nz nw l nx ny"># flatten and cast to dataframe <br/>df = processed.flatten().to_dataframe() </span><span id="19e4" class="ne kx iq nr b gy nz nw l nx ny"># write to parquet <br/>df.to_parquet( 's3://coiled-datasets/etl/test.parq', engine='pyarrow', compression='snappy' ) </span><span id="1fee" class="ne kx iq nr b gy nz nw l nx ny"><br/>CPU times: user 15.1 s, sys: 1.74 s, total: 16.8 s <br/>Wall time: 19min 17s</span></pre><p id="10d4" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">太好了，这很有效。但是让我们看看是否可以加快一点速度…</p><p id="3cd8" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">让我们纵向扩展我们的集群以提升性能。我们将使用<code class="fe oa ob oc nr b">cluster.scale()</code>命令将集群中的工作线程数量增加一倍。我们还将包括一个对<code class="fe oa ob oc nr b">client.wait_for_workers()</code>的调用，它将阻塞活动，直到所有的工人都在线。这样，我们就可以确信我们在计算中已经竭尽全力了。</p><pre class="kg kh ki kj gt nq nr ns nt aw nu bi"><span id="b3c2" class="ne kx iq nr b gy nv nw l nx ny"># double n_workers <br/>cluster.scale(20) </span><span id="6035" class="ne kx iq nr b gy nz nw l nx ny"># this blocks activity until the specified number of workers have joined the cluster <br/>client.wait_for_workers(20)</span></pre><p id="edcb" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">现在，让我们在扩展后的集群上重新运行相同的ETL管道。</p><pre class="kg kh ki kj gt nq nr ns nt aw nu bi"><span id="e7c0" class="ne kx iq nr b gy nv nw l nx ny">%%time <br/># re-run etl pipeline <br/>records = db.read_text(filenames).map(ujson.loads) <br/>push = records.filter(lambda record: record["type"] == "PushEvent") processed = push.map(process) <br/>df = processed.flatten().to_dataframe() <br/>df.to_parquet( 's3://coiled-datasets/etl/test.parq', engine='pyarrow', compression='snappy' ) </span><span id="3627" class="ne kx iq nr b gy nz nw l nx ny">CPU times: user 11.4 s, sys: 1.1 s, total: 12.5 s <br/>Wall time: 9min 53s</span></pre><p id="e81c" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">我们已经将运行时间减少了一半，干得好！想象一下，如果我们将n_workers增加10倍或20倍，会发生什么！</p><h1 id="fead" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">将大型JSON转换为拼花摘要</h1><p id="e437" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">在这个笔记本中，我们将原始JSON数据转换成扁平的数据帧，并以高效的Parquet文件格式存储在云对象存储中。我们首先在本地的单个测试文件上执行这个工作流。然后，我们使用Coiled上的Dask集群将相同的工作流扩展到云上运行，以处理整个75GB的数据集。</p><p id="6bbe" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">主要要点:</p><ul class=""><li id="d320" class="mq mr iq lq b lr mk lu ml lx ms mb mt mf mu mj od mw mx my bi translated">Coiled允许您将常见的ETL工作流扩展到大于内存的数据集。</li><li id="9f0f" class="mq mr iq lq b lr mz lu na lx nb mb nc mf nd mj od mw mx my bi translated">仅在需要时扩展到云。云计算带来了自己的一系列挑战和开销。因此，要战略性地决定是否以及何时导入Coiled和spin up集群。</li><li id="9737" class="mq mr iq lq b lr mz lu na lx nb mb nc mf nd mj od mw mx my bi translated">纵向扩展您的集群以提高性能。通过将我们的集群从10个工人扩展到20个工人，我们将ETL功能的运行时间减少了一半。</li></ul></div><div class="ab cl oe of hu og" role="separator"><span class="oh bw bk oi oj ok"/><span class="oh bw bk oi oj ok"/><span class="oh bw bk oi oj"/></div><div class="ij ik il im in"><p id="c51c" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">我希望这篇文章对你有帮助！<a class="ae kv" href="https://twitter.com/richardpelgrim" rel="noopener ugc nofollow" target="_blank">在Twitter上关注我</a>获取每日数据科学内容。</p><p id="0410" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">或者来我的博客打招呼:</p><div class="ol om gp gr on oo"><a href="https://crunchcrunchhuman.com/2021/12/22/kaggle-xgboost-distributed-cloud/" rel="noopener  ugc nofollow" target="_blank"><div class="op ab fo"><div class="oq ab or cl cj os"><h2 class="bd ir gy z fp ot fr fs ou fu fw ip bi translated">在20秒内对20GB数据进行XGBoost训练</h2><div class="ov l"><h3 class="bd b gy z fp ot fr fs ou fu fw dk translated">如果您正在寻找方法来更快地训练XGBoost模型，或者在大于您的机器内存的数据集上训练，这里有一个…</h3></div><div class="ow l"><p class="bd b dl z fp ot fr fs ou fu fw dk translated">crunchcrunchhuman.com</p></div></div><div class="ox l"><div class="oy l oz pa pb ox pc kp oo"/></div></div></a></div></div><div class="ab cl oe of hu og" role="separator"><span class="oh bw bk oi oj ok"/><span class="oh bw bk oi oj ok"/><span class="oh bw bk oi oj"/></div><div class="ij ik il im in"><p id="7472" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated"><em class="mp">原载于2021年9月15日</em><a class="ae kv" href="https://coiled.io/blog/convert-large-json-to-parquet-with-dask/" rel="noopener ugc nofollow" target="_blank"><em class="mp">https://coiled . io</em></a><em class="mp">。</em></p></div></div>    
</body>
</html>