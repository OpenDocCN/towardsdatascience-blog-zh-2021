<html>
<head>
<title>Gradient Editing On The Fly in Deep Neural Networks</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">深度神经网络中的动态梯度编辑</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/gradient-editing-on-the-fly-in-deep-neural-networks-4f060360e95d?source=collection_archive---------14-----------------------#2021-12-30">https://towardsdatascience.com/gradient-editing-on-the-fly-in-deep-neural-networks-4f060360e95d?source=collection_archive---------14-----------------------#2021-12-30</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="0c3c" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">介绍优化深度神经网络时调整梯度的不同方法，包括梯度裁剪、扰动、掩蔽和替换</h2></div><p id="e6ec" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">梯度是深度学习世界中的一个重要组成部分。它支持最受欢迎的算法之一——梯度下降算法，该算法通过根据模型成本相对于这些参数的当前梯度反复更改神经网络的值来优化神经网络的参数。虽然在每次更新中是一种简单的方法，但梯度下降算法家族，特别是随机梯度下降，一直是深度学习模型良好泛化性能的驱动力之一。</p><p id="6b19" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">然而，人们更多地关注算法本身，而不是梯度。部分原因是梯度大部分是自动计算的，并被插入到优化过程中。这种自动计算机制，例如使用 PyTorch 中的自动签名模块，省去了手动推导的麻烦，从而使我们更容易专注于整体算法。本系列教程旨在弥补这一差距，并演示如何动态编辑梯度，以便我们可以在利用自动微分的同时获得梯度。对于本系列的第 1 部分，我们将介绍整个模型训练过程，作为后面部分的必要背景知识。</p><p id="936e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如图 1 所示，典型的模型训练过程由四个部分组成:数据、模型、成本和优化过程。模型训练过程从一些训练数据开始。在监督学习任务中，训练数据由输入-输出对组成。每个输入条目可以包含从不同角度描述一个观察的多个特征。相应的输出包含目标标签，作为指导训练过程的正确答案。模型训练旨在生成一个映射函数，一个模型，它将给定的输入正确地映射到相应的输出。</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi lb"><img src="../Images/2e2039bf86d1d9658dc2af1f1d95832b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0wK7_Yiy0BfbKlMmtImNkg.png"/></div></div><p class="ln lo gj gh gi lp lq bd b be z dk translated">图 1 典型模型训练过程示例。工作流从可用的训练数据开始，并逐渐调整模型。调整过程需要将模型预测与目标标签进行匹配，其中通过特定的成本函数来测量差距，并将其用作下一轮调整的反馈。每次调整都会产生一个新的模型，我们希望寻找成本最低的模型。图片由作者提供。</p></figure><h1 id="de0d" class="lr ls iq bd lt lu lv lw lx ly lz ma mb jw mc jx md jz me ka mf kc mg kd mh mi bi translated">数据</h1><p id="945f" class="pw-post-body-paragraph kf kg iq kh b ki mj jr kk kl mk ju kn ko ml kq kr ks mm ku kv kw mn ky kz la ij bi translated">模型训练过程从一些训练数据开始。在监督学习任务中，训练数据由输入-输出对组成。每个输入条目可以包含从不同角度描述一个观察的多个特征。相应的输出包含目标标签，作为指导训练过程的正确答案。我们将以 MNIST 为例。以下代码下载并加载 MNIST 数据集。</p><pre class="lc ld le lf gt mo mp mq mr aw ms bi"><span id="2792" class="mt ls iq mp b gy mu mv l mw mx"><em class="my"># download MNIST dataset</em></span><span id="6450" class="mt ls iq mp b gy mz mv l mw mx"><strong class="mp ir">from</strong> torchvision <strong class="mp ir">import</strong> datasets</span><span id="f31f" class="mt ls iq mp b gy mz mv l mw mx"><strong class="mp ir">from</strong> torchvision.transforms <strong class="mp ir">import</strong> ToTensor</span><span id="5f3f" class="mt ls iq mp b gy mz mv l mw mx">train_data <strong class="mp ir">=</strong> datasets<strong class="mp ir">.</strong>MNIST(</span><span id="c912" class="mt ls iq mp b gy mz mv l mw mx">root <strong class="mp ir">=</strong> 'data',</span><span id="3664" class="mt ls iq mp b gy mz mv l mw mx">train <strong class="mp ir">=</strong> <strong class="mp ir">True</strong>,</span><span id="301b" class="mt ls iq mp b gy mz mv l mw mx">transform <strong class="mp ir">=</strong> ToTensor(),</span><span id="92c4" class="mt ls iq mp b gy mz mv l mw mx">download <strong class="mp ir">=</strong> <strong class="mp ir">True</strong>,</span><span id="cd8c" class="mt ls iq mp b gy mz mv l mw mx">)</span><span id="e6f0" class="mt ls iq mp b gy mz mv l mw mx">test_data <strong class="mp ir">=</strong> datasets<strong class="mp ir">.</strong>MNIST(</span><span id="c54e" class="mt ls iq mp b gy mz mv l mw mx">root <strong class="mp ir">=</strong> 'data',</span><span id="9939" class="mt ls iq mp b gy mz mv l mw mx">train <strong class="mp ir">=</strong> <strong class="mp ir">False</strong>,</span><span id="eafc" class="mt ls iq mp b gy mz mv l mw mx">transform <strong class="mp ir">=</strong> ToTensor()</span><span id="83e0" class="mt ls iq mp b gy mz mv l mw mx">)</span><span id="f60f" class="mt ls iq mp b gy mz mv l mw mx"><em class="my"># preparing data for training with DataLoaders</em></span><span id="59bc" class="mt ls iq mp b gy mz mv l mw mx"><strong class="mp ir">from</strong> torch.utils.data <strong class="mp ir">import</strong> DataLoader</span><span id="5eb2" class="mt ls iq mp b gy mz mv l mw mx">loaders <strong class="mp ir">=</strong> {</span><span id="aa8f" class="mt ls iq mp b gy mz mv l mw mx">'train' : torch<strong class="mp ir">.</strong>utils<strong class="mp ir">.</strong>data<strong class="mp ir">.</strong>DataLoader(train_data, <em class="my"># data source to be loaded</em></span><span id="77e3" class="mt ls iq mp b gy mz mv l mw mx">batch_size<strong class="mp ir">=</strong>100, <em class="my">#  the number of training samples used in one iteration</em></span><span id="6571" class="mt ls iq mp b gy mz mv l mw mx">shuffle<strong class="mp ir">=True</strong>), <em class="my"># samples are shuffled and loaded in batches</em></span><span id="4f34" class="mt ls iq mp b gy mz mv l mw mx">'test'  : torch<strong class="mp ir">.</strong>utils<strong class="mp ir">.</strong>data<strong class="mp ir">.</strong>DataLoader(test_data,</span><span id="3031" class="mt ls iq mp b gy mz mv l mw mx">batch_size<strong class="mp ir">=</strong>100,</span><span id="5208" class="mt ls iq mp b gy mz mv l mw mx">shuffle<strong class="mp ir">=True</strong>)}</span></pre><h1 id="1858" class="lr ls iq bd lt lu lv lw lx ly lz ma mb jw mc jx md jz me ka mf kc mg kd mh mi bi translated">模型</h1><p id="9d3a" class="pw-post-body-paragraph kf kg iq kh b ki mj jr kk kl mk ju kn ko ml kq kr ks mm ku kv kw mn ky kz la ij bi translated">经过训练的模型由两部分组成:参数和体系结构。参数是模型不可或缺的组成部分，而体系结构指定了这些组件如何与输入数据进行交互以生成最终的预测输出。为了说明的目的，我们将定义一个简单的两层全连接神经网络。</p><pre class="lc ld le lf gt mo mp mq mr aw ms bi"><span id="2c78" class="mt ls iq mp b gy mu mv l mw mx"><strong class="mp ir">import</strong> torch.nn <strong class="mp ir">as</strong> nn</span><span id="a939" class="mt ls iq mp b gy mz mv l mw mx"><strong class="mp ir">class</strong> simpleNN(nn<strong class="mp ir">.</strong>Module):</span><span id="a85d" class="mt ls iq mp b gy mz mv l mw mx"><strong class="mp ir">def</strong> __init__(self):</span><span id="722f" class="mt ls iq mp b gy mz mv l mw mx">super(simpleNN, self)<strong class="mp ir">.</strong>__init__()</span><span id="2e90" class="mt ls iq mp b gy mz mv l mw mx">self<strong class="mp ir">.</strong>fc1 <strong class="mp ir">=</strong> nn<strong class="mp ir">.</strong>Linear(28<strong class="mp ir">*</strong>28, 100)</span><span id="d69a" class="mt ls iq mp b gy mz mv l mw mx">self<strong class="mp ir">.</strong>fc2 <strong class="mp ir">=</strong> nn<strong class="mp ir">.</strong>Linear(100, 10)</span><span id="e56b" class="mt ls iq mp b gy mz mv l mw mx">self<strong class="mp ir">.</strong>relu <strong class="mp ir">=</strong> nn<strong class="mp ir">.</strong>ReLU()</span><span id="923d" class="mt ls iq mp b gy mz mv l mw mx"><strong class="mp ir">def</strong> forward(self, x):</span><span id="5772" class="mt ls iq mp b gy mz mv l mw mx">x <strong class="mp ir">=</strong> x<strong class="mp ir">.</strong>view(x<strong class="mp ir">.</strong>size(0), <strong class="mp ir">-</strong>1)</span><span id="50f1" class="mt ls iq mp b gy mz mv l mw mx">x <strong class="mp ir">=</strong> self<strong class="mp ir">.</strong>fc1(x)</span><span id="4397" class="mt ls iq mp b gy mz mv l mw mx">x <strong class="mp ir">=</strong> self<strong class="mp ir">.</strong>relu(x)</span><span id="e2a6" class="mt ls iq mp b gy mz mv l mw mx">x <strong class="mp ir">=</strong> self<strong class="mp ir">.</strong>fc2(x)</span><span id="22eb" class="mt ls iq mp b gy mz mv l mw mx">x <strong class="mp ir">=</strong> self<strong class="mp ir">.</strong>relu(x)</span><span id="4cff" class="mt ls iq mp b gy mz mv l mw mx"><strong class="mp ir">return</strong> x</span></pre><p id="768d" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们可以使用 torchsummary 包获得模型架构的概要。</p><pre class="lc ld le lf gt mo mp mq mr aw ms bi"><span id="6a9d" class="mt ls iq mp b gy mu mv l mw mx"><strong class="mp ir">from</strong> torchsummary <strong class="mp ir">import</strong> summary</span><span id="914c" class="mt ls iq mp b gy mz mv l mw mx">summary(model, input_size<strong class="mp ir">=</strong>(1, 28, 28))</span></pre><figure class="lc ld le lf gt lg gh gi paragraph-image"><div class="gh gi na"><img src="../Images/da7b6841f4175028d344b37ac2f3df72.png" data-original-src="https://miro.medium.com/v2/resize:fit:1254/0*u13Y1R4UWh3NnPzQ"/></div><p class="ln lo gj gh gi lp lq bd b be z dk translated">图 2 模型架构。图片作者。</p></figure><h1 id="d6aa" class="lr ls iq bd lt lu lv lw lx ly lz ma mb jw mc jx md jz me ka mf kc mg kd mh mi bi translated">费用</h1><p id="d943" class="pw-post-body-paragraph kf kg iq kh b ki mj jr kk kl mk ju kn ko ml kq kr ks mm ku kv kw mn ky kz la ij bi translated">然后，将该预测值与地面实况的目标标签进行比较，以共同产生标量误差度量。这里，误差表示当前成本，即预测值和实际值之间的距离。</p><p id="57e6" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在计算成本时，我们将首先使用 softmax 转换将原始预测集转换为概率，然后使用交叉熵损失(CEL)导出单值成本度量。CEL 是分类任务中常用的成本度量，因为它直观地符合好的和坏的预测之间的预期成本行为。请参见图 3 和图 4，了解转型和成本构建流程的详细信息。</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi nb"><img src="../Images/26c56feae819edebda895d8a63e54371.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*5SyG6fM0Z8W49vkw"/></div></div><p class="ln lo gj gh gi lp lq bd b be z dk translated">图 3 Softmax 变换。图片作者。</p></figure><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi nb"><img src="../Images/85154985ffcde6fdec434a1731faf33d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*BPAt-R-kMy-GSjMH"/></div></div><p class="ln lo gj gh gi lp lq bd b be z dk translated">图 4 交叉熵损失。图片作者。</p></figure><p id="f278" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们可以使用下面图 5 中的两个简单例子来验证成本构成。</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi nc"><img src="../Images/30508f3a5e39c2a200b08b28d097656d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*eTDXC3UdNH7MFoGH"/></div></div><p class="ln lo gj gh gi lp lq bd b be z dk translated">图 5 验证交叉熵损失的直观性。图片作者。</p></figure><h1 id="7b7b" class="lr ls iq bd lt lu lv lw lx ly lz ma mb jw mc jx md jz me ka mf kc mg kd mh mi bi translated">最佳化</h1><p id="d636" class="pw-post-body-paragraph kf kg iq kh b ki mj jr kk kl mk ju kn ko ml kq kr ks mm ku kv kw mn ky kz la ij bi translated">在特定的优化过程之后，训练过程调整模型参数，并且有时调整架构以降低训练成本。调整权重后，再次计算新的误差，形成反馈环。最广泛使用的优化程序是梯度下降，如下图 6 所示。</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi nd"><img src="../Images/f539c1ceb40c43e775d26684c9591923.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*ATnB8mC28FLuzxJC"/></div></div><p class="ln lo gj gh gi lp lq bd b be z dk translated">图 6 梯度下降算法。图片作者。</p></figure><p id="9b51" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们可以将优化部分定义如下。</p><pre class="lc ld le lf gt mo mp mq mr aw ms bi"><span id="54e7" class="mt ls iq mp b gy mu mv l mw mx"><em class="my"># Define the procedure</em></span><span id="79f9" class="mt ls iq mp b gy mz mv l mw mx"><strong class="mp ir">from</strong> torch <strong class="mp ir">import</strong> optim</span><span id="695c" class="mt ls iq mp b gy mz mv l mw mx">lr <strong class="mp ir">=</strong> 0.01</span><span id="bed4" class="mt ls iq mp b gy mz mv l mw mx">optimizer <strong class="mp ir">=</strong> optim<strong class="mp ir">.</strong>SGD(model<strong class="mp ir">.</strong>parameters(), lr <strong class="mp ir">=</strong> lr)</span></pre><h1 id="c0cf" class="lr ls iq bd lt lu lv lw lx ly lz ma mb jw mc jx md jz me ka mf kc mg kd mh mi bi translated">一次迭代的训练</h1><p id="5ee1" class="pw-post-body-paragraph kf kg iq kh b ki mj jr kk kl mk ju kn ko ml kq kr ks mm ku kv kw mn ky kz la ij bi translated">让我们在一次迭代中结合上述组件，观察输出层偏差的变化。</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi ne"><img src="../Images/409b405955c9db732d7cea2d0c3d8517.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*wCSBmRHc3UsGFu_U"/></div></div><p class="ln lo gj gh gi lp lq bd b be z dk translated">图 7 一次迭代的训练。图片作者。</p></figure><p id="a980" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">图 7 显示，在一次迭代后，使用相应的梯度改变了偏差。由于在优化过程中明确使用了梯度，我们可以想出不同的方法来编辑它们，包括剪辑、遮罩、扰动和替换。</p><h1 id="f273" class="lr ls iq bd lt lu lv lw lx ly lz ma mb jw mc jx md jz me ka mf kc mg kd mh mi bi translated">渐变剪辑</h1><p id="bee0" class="pw-post-body-paragraph kf kg iq kh b ki mj jr kk kl mk ju kn ko ml kq kr ks mm ku kv kw mn ky kz la ij bi translated">可能是避免梯度爆炸和递减问题的最广泛使用的技术。</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi nf"><img src="../Images/3fe08fc69d932c4758f68ac8cf03be5b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*lYqXknfq_hD2Sc6S"/></div></div><p class="ln lo gj gh gi lp lq bd b be z dk translated">图 8 渐变裁剪。图片作者。</p></figure><h1 id="b648" class="lr ls iq bd lt lu lv lw lx ly lz ma mb jw mc jx md jz me ka mf kc mg kd mh mi bi translated">梯度掩蔽</h1><p id="c8db" class="pw-post-body-paragraph kf kg iq kh b ki mj jr kk kl mk ju kn ko ml kq kr ks mm ku kv kw mn ky kz la ij bi translated">应用一系列二进制掩码来确定需要进行梯度更新的特定权重集。</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi ng"><img src="../Images/0c10b1b4d627a24fc6933a656141bf1d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*OTa__lGMWQj6r-cE"/></div></div><p class="ln lo gj gh gi lp lq bd b be z dk translated">图 9 渐变遮罩。图片作者。</p></figure><h1 id="2aaf" class="lr ls iq bd lt lu lv lw lx ly lz ma mb jw mc jx md jz me ka mf kc mg kd mh mi bi translated">梯度扰动</h1><p id="1bd0" class="pw-post-body-paragraph kf kg iq kh b ki mj jr kk kl mk ju kn ko ml kq kr ks mm ku kv kw mn ky kz la ij bi translated">噪声已被证明非常有助于提高模型的泛化性能，包括向数据、标签和优化过程本身注入噪声。在这里，我们展示了如何添加噪声和扰动梯度，这将是一个有趣的实验领域，并观察其对性能的影响。</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi nh"><img src="../Images/3844a8b7e60c17b119ceb74a8519b0ff.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*eHvYCB99bpfd7X_i"/></div></div><p class="ln lo gj gh gi lp lq bd b be z dk translated">图 10 梯度扰动。图片作者。</p></figure><h1 id="04ec" class="lr ls iq bd lt lu lv lw lx ly lz ma mb jw mc jx md jz me ka mf kc mg kd mh mi bi translated">梯度置换</h1><p id="e1cc" class="pw-post-body-paragraph kf kg iq kh b ki mj jr kk kl mk ju kn ko ml kq kr ks mm ku kv kw mn ky kz la ij bi translated">替换渐变也是另一个有趣的实验。取代依赖梯度下降机制本身，我们可以设计一些新的机制来提出(希望)更明智的梯度，从而更好地将更新导向全局最优。</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi ni"><img src="../Images/e3fa3167c95bcb35554d12c5f982dc42.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*_9sjqxABkL9q0bKF"/></div></div><p class="ln lo gj gh gi lp lq bd b be z dk translated">图 11 渐变替换。图片作者。</p></figure><p id="039b" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">完整详细的演练，请访问下面的 youTube 视频。</p><figure class="lc ld le lf gt lg"><div class="bz fp l di"><div class="nj nk l"/></div></figure><p id="5c64" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">完整的笔记本可以通过下面的链接访问。</p><div class="nl nm gp gr nn no"><a href="https://github.com/jackliu333/gradient_editing/blob/main/Gradient_editing_on_the_fly.ipynb" rel="noopener  ugc nofollow" target="_blank"><div class="np ab fo"><div class="nq ab nr cl cj ns"><h2 class="bd ir gy z fp nt fr fs nu fu fw ip bi translated">Gradient _ editing/Gradient _ editing _ on _ the _ fly . ipynb at main jack Liu 333/Gradient _ editing</h2><div class="nv l"><h3 class="bd b gy z fp nt fr fs nu fu fw dk translated">在 GitHub 上创建一个帐户，为 jackliu333/gradient_editing 开发做贡献。</h3></div><div class="nw l"><p class="bd b dl z fp nt fr fs nu fu fw dk translated">github.com</p></div></div><div class="nx l"><div class="ny l nz oa ob nx oc ll no"/></div></div></a></div></div></div>    
</body>
</html>