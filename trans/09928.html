<html>
<head>
<title>A Broad and Practical Exposition of Online Learning Techniques</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">对在线学习技术的广泛而实用的阐述</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/a-broad-and-practical-exposition-of-online-learning-techniques-a4cbc300dcd4?source=collection_archive---------21-----------------------#2021-09-18">https://towardsdatascience.com/a-broad-and-practical-exposition-of-online-learning-techniques-a4cbc300dcd4?source=collection_archive---------21-----------------------#2021-09-18</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="dd41" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsdatascience.com/tagged/thoughts-and-theory" rel="noopener" target="_blank">思想和理论</a></h2><div class=""/><div class=""><h2 id="e4f6" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">在线学习技术的概述，重点是那些对从业者最有效的技术。</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/c2008365af5a9b6311a557fff8cef86c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fWQN95-CwBunxbfiHSqZGQ.jpeg"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated"><a class="ae lh" href="https://unsplash.com/photos/wO42Rmamef8" rel="noopener ugc nofollow" target="_blank">来源</a></p></figure><p id="988e" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">在这篇博文中，我将深入探讨一下<a class="ae lh" href="https://en.wikipedia.org/wiki/Online_machine_learning" rel="noopener ugc nofollow" target="_blank">在线学习</a>这个话题——这是深度学习社区中一个非常受欢迎的研究领域。像深度学习中的许多研究课题一样，在线学习在工业环境中有着广泛的应用。也就是说，数据对学习者顺序可用的情况非常普遍；请参阅动态电子商务推荐、设备上的学习场景甚至联合学习等示例，这些示例中可能不会同时提供完整的数据集。我的目标是从从业者的角度解决在线学习的话题，回答如下问题:</p><ul class=""><li id="0480" class="me mf it lk b ll lm lo lp lr mg lv mh lz mi md mj mk ml mm bi translated">在线学习模式面临哪些问题？</li><li id="260f" class="me mf it lk b ll mn lo mo lr mp lv mq lz mr md mj mk ml mm bi translated">在线培训模型的最佳解决方案是什么？</li><li id="6daf" class="me mf it lk b ll mn lo mo lr mp lv mq lz mr md mj mk ml mm bi translated">我应该期望一个在线训练的模型有什么样的表现？</li></ul><p id="9594" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">这篇文章的目标是让从业者意识到在线学习空间中存在的选择，为模型必须从不断可用的新数据中学习的场景提供一个可行的解决方案。这种训练设置不仅消除了充满延迟的离线再训练过程(即，模型实时更新)，而且更接近地反映了智能系统如何在我们周围的世界中学习——当人类学习一项技能时，他们不需要几个小时的GPU训练来利用他们的新知识！</p><p id="b2ee" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">该员额的结构如下。我将首先介绍在线学习的主题，并从一个广泛的角度，概述这个领域中已经确定和提出的问题和解决方案。尽管存在大量关于在线学习主题的出版物，但其中只有一些出版物提供了现代大规模架构和数据集的切实好处，而这些正是当今深度学习实践者最感兴趣的。在这篇文章中，我将概述大多数现有的在线学习方法，但我将特别注意那些能给实践者带来最佳“性价比”的方法，从而提供关于最有用的方法的必要背景。</p><h1 id="94f4" class="ms mt it bd mu mv mw mx my mz na nb nc ki nd kj ne kl nf km ng ko nh kp ni nj bi translated">什么是在线学习？</h1><p id="9c5c" class="pw-post-body-paragraph li lj it lk b ll nk kd ln lo nl kg lq lr nm lt lu lv nn lx ly lz no mb mc md im bi translated">在这篇文章中，我将在线学习定义为一种训练场景，在这种场景中，模型永远不会同时获得完整的数据集。相反，该模型被顺序地暴露给数据集的部分，并被期望通过这样的部分暴露来学习完整的训练任务。通常，在暴露给数据集的某个部分之后，不允许模型稍后重新访问该数据。否则，模型可以简单地在数据集上循环，并执行正常的训练过程。</p><p id="4894" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">任何读过研究在线学习的论文的人可能都不知道这种培训应该叫什么，因为在多年的研究中已经给它取了很多名字。例如，终身学习[1，2]，持续学习[3，4]，增量学习[5，6]和流学习[7，8]——除此之外还有更多！这些名称中有许多是指非常相似的场景，只是在实验设置上略有不同。例如，终身学习通常是指按顺序学习多项任务，而增量学习则倾向于按顺序学习批量数据。有些不同的是，流学习只需通过数据集一次，永远不允许模型一次查看多个数据示例。尽管如此，我还是将“在线学习”作为一个通用术语，指的是所有这类共享部分、顺序接触数据这一共同属性的实验设置。</p><h2 id="6ed7" class="np mt it bd mu nq nr dn my ns nt dp nc lr nu nv ne lv nw nx ng lz ny nz ni iz bi translated">设置很重要吗？</h2><p id="0af4" class="pw-post-body-paragraph li lj it lk b ll nk kd ln lo nl kg lq lr nm lt lu lv nn lx ly lz no mb mc md im bi translated">鉴于在线学习技术的研究存在如此多不同的实验场景，人们可能很容易开始怀疑研究在线学习总体上是否有用。换句话说，我们能不能只研究在线学习，或者实验设置的选择对某些培训方法的有效性有重大影响？在终身学习中行之有效的方法是否也适用于增量学习？总之，<em class="ob">实验装置的选择很重要</em>。例如，以前的工作表明，为增量学习场景训练的模型很少在流设置中表现良好[7]。出于这个原因，我将试着具体说明正在讨论的确切的学习场景，尤其是在研究一篇特定的论文时。然而，幸运的是，用于所有类型的在线学习的许多培训技术都非常相似[9，10]——它们可能只需要稍微修改，以使它们在给定的环境中更有影响力。</p><h1 id="d2c5" class="ms mt it bd mu mv mw mx my mz na nb nc ki nd kj ne kl nf km ng ko nh kp ni nj bi translated">为什么在线学习很难？</h1><p id="0996" class="pw-post-body-paragraph li lj it lk b ll nk kd ln lo nl kg lq lr nm lt lu lv nn lx ly lz no mb mc md im bi translated">在深入当前的在线学习方法之前，人们可能会想为什么需要任何专门的培训方法。<em class="ob">难道我们不能在数据可用时正常训练模型吗？</em>这个问题的简短回答是否定的，但是理解为什么需要一些背景知识。</p><h2 id="d345" class="np mt it bd mu nq nr dn my ns nt dp nc lr nu nv ne lv nw nx ng lz ny nz ni iz bi translated">天真的方法…</h2><p id="8fab" class="pw-post-body-paragraph li lj it lk b ll nk kd ln lo nl kg lq lr nm lt lu lv nn lx ly lz no mb mc md im bi translated">在线学习最简单的方法之一是维护一个单一的模型，在新数据到来时对其进行微调。在这种情况下，数据对模型变得连续可用，并且不能重新访问以前的数据。因此，随着新数据的到来，模型会实时更新/微调，并随着时间的推移慢慢学习感兴趣的任务。如果传入的数据流是<a class="ae lh" href="https://en.wikipedia.org/wiki/Independent_and_identically_distributed_random_variables" rel="noopener ugc nofollow" target="_blank"> i.i.d. </a>，这种微调方法会工作得很好！即，将从训练数据的分布中均匀地采样数据流(例如，在分类任务中所有不同类别的均匀采样)，并且模型将以平衡的方式暴露给所有数据。随着时间的推移，如果数据继续变得可用，模型将开始表现得相当好。</p><p id="6b8a" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">那么，有什么问题呢？在许多实际应用中，传入的数据流是非独立同分布的，这意味着它不会以无偏的方式从底层数据集进行采样。例如，暴露给模型的所有数据示例可能来自分类任务中的单个类。更实际的是，考虑一个正在基于时尚的电子商务网站上使用的深度学习模型。在平常的一天，模型从网站上的客户活动中学习，从同一组产品中取样。然而，有一天，一个新的产品线可能会被添加到电子商务网站上，围绕一个产品组催化大量的客户活动。在这种情况下，模型暴露于与单个主题/产品相关的大量数据，导致网站上新的和现有的产品之间的不平衡暴露。很明显，这将使在线学习过程变得复杂，但是怎么做呢？</p><h2 id="82e5" class="np mt it bd mu nq nr dn my ns nt dp nc lr nu nv ne lv nw nx ng lz ny nz ni iz bi translated">灾难性遗忘</h2><p id="0c7f" class="pw-post-body-paragraph li lj it lk b ll nk kd ln lo nl kg lq lr nm lt lu lv nn lx ly lz no mb mc md im bi translated">在在线学习社区中，通过非独立身份数据流以在线方式训练模型的主要问题是灾难性遗忘[11，12]。灾难性遗忘是在线学习模型的一个属性，其中模型在暴露于新数据时会忘记如何对以前的数据进行分类。例如，考虑具有10个类的数据集(例如，CIFAR10)，并且假设在线学习模型已经在类1和类2上被训练。然后，假设模型接收的新数据只是从第三和第四类中抽取的。如果在不访问任何先前学习的数据的情况下对该新数据进行微调，则该模型将开始在类3和类4上表现良好，但很可能在类1和类2上表现恶化。换句话说，它将遭受灾难性的遗忘！</p><p id="7668" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">在线学习的目标是找出如何消除灾难性遗忘。因此，这一领域的几乎所有研究(1)都假设输入数据是非独立同分布的(即，独立同分布数据流可以通过简单的微调轻松处理),以及(2)提出了一些防止灾难性遗忘的方法。一般来说，生成非独立数据挖掘数据流(用于基于分类的学习问题)的最常见方法是将所有可能的输出类分成多个不相交的组，并将这些类组按顺序展示给在线学习者[5，12]。在这种情况下，在线学习模型必须学会如何处理新的类，同时保持普遍遇到的类的知识。对于除分类之外的任务(例如，对象检测或强化学习)，许多其他训练设置已被导出用于生成非独立同分布数据流，这些数据流会诱发灾难性遗忘[8，13]。</p><h1 id="6d9b" class="ms mt it bd mu mv mw mx my mz na nb nc ki nd kj ne kl nf km ng ko nh kp ni nj bi translated">存在哪些方法？</h1><p id="886c" class="pw-post-body-paragraph li lj it lk b ll nk kd ln lo nl kg lq lr nm lt lu lv nn lx ly lz no mb mc md im bi translated">已经提出了许多方法来减少在线学习领域中的灾难性遗忘。出于本文的目的，我将这些方法大致分为以下几类:架构修改、正则化、提炼、重放、再平衡和其他。对于其中的每一个类别，我都将提供该方法的简要概述、相关论文的摘要以及实际有效性的讨论。在这些描述中，我通常会考虑增量学习(即任务/课程增量学习)和流设置，因为它们是文献中最常见的。</p><h2 id="55e3" class="np mt it bd mu nq nr dn my ns nt dp nc lr nu nv ne lv nw nx ng lz ny nz ni iz bi translated">建筑改造</h2><p id="db80" class="pw-post-body-paragraph li lj it lk b ll nk kd ln lo nl kg lq lr nm lt lu lv nn lx ly lz no mb mc md im bi translated"><strong class="lk jd"> <em class="ob">概述。</em> </strong>架构修改背后的想法很简单:当您收到新数据时，向您的模型添加更多参数以增加其容量。这些参数可以以结构化(例如，向架构添加全新的神经元或过滤器)或非结构化(例如，在现有神经元之间添加新的连接)的方式添加。此外，在接收到新数据之后更新扩充的模型(即，这里我使用“扩充的”来指代具有添加的参数的模型)可以以两种方式来完成:(1)简单地更新模型而没有限制，或者(2)使用掩蔽/选择性可塑性策略来确保仅更新不重要的神经元(即，那些不影响先前数据的性能的神经元)。在这两种情况下，这种方法的目标是通过确保模型永远不会因能力有限而无法扩展其对潜在学习问题的知识，从而允许模型在新旧数据上都表现良好。通过总是添加更多的参数，我们确保模型可以继续从新数据中学习。</p><p id="0c39" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><strong class="lk jd"> <em class="ob">方法。</em> </strong>已经为架构修改提出了许多不同的方法，列举了上面列出的所有可能的选项。[13]研究了任务增量设置，并为每个任务实例化了一个全新的神经网络(即，每个任务都有自己的连接到输入的相同大小的隐藏/输出层)。然后，为了确保先前的知识被保持和利用，横向连接被形成到先前任务的隐藏层，允许新任务利用先前学习的特征表示来增强学习过程。类似地，[14]研究了任务增量设置，并建议每次引入新任务时将额外的神经元添加到网络中，而[15]研究了类似的(但结构化的)方法，即每次在任务增量设置中遇到新数据时将整个神经元“块”添加到网络中。最后，[16]在在线学习过程中向网络中添加新的参数，但探索各种掩蔽和可塑性策略来控制模型的参数更新，以避免之前任务对知识的破坏。</p><p id="3332" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><strong class="lk jd"> <em class="ob">讨论。虽然架构修改技术已经在小规模在线学习问题中取得了成功，但是它们有两个主要的属性限制了它们的潜力。首先，因为架构在不断地扩展或扩大，这些技术的内存需求通常很大/没有限制，这在大规模模型或数据集的情况下可能变得难以处理。理想情况下，如果在线学习算法的内存使用不依赖于接收的数据量，效果会更好。此外，大多数这样的方法依赖于“任务边界”的存在(即，输入数据流中的预定断点，例如增量学习中存在的批次)。这种任务边界在在线训练过程中提供了明显的点，在该过程中可以将参数/模块添加到网络中。一些架构修改方法完全依赖于这种任务边界的存在[13，14，15]，但是这些边界并不总是存在(例如，在流学习期间)。因此，对任务边界的依赖限制了这些技术在某些场景中的应用。</em></strong></p><h2 id="372f" class="np mt it bd mu nq nr dn my ns nt dp nc lr nu nv ne lv nw nx ng lz ny nz ni iz bi translated">正规化</h2><p id="504a" class="pw-post-body-paragraph li lj it lk b ll nk kd ln lo nl kg lq lr nm lt lu lv nn lx ly lz no mb mc md im bi translated"><strong class="lk jd"> <em class="ob">概述。</em> </strong>用于在线学习的正则化技术通常试图(I)识别“重要”的参数，以及(ii)在训练期间引入正则化项，以防止这些参数被改变太多。典型地，重要的参数被定义为当被更新/扰乱时使网络性能恶化的那些参数。已经提出了许多不同的重要性试探法，但是它们都有一个共同的目标，即表征修改该参数是否会损害网络对于不再存在的旧数据的性能。通过确保重要参数在在线训练期间不被修改，网络对旧数据的性能得以保持，因为新数据的参数更新被合并到与网络行为不相关的区域。</p><p id="5a8a" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><strong class="lk jd"> <em class="ob">战法。</em> </strong>在【17】中，参数重要性被定义为参数对先前任务的网络准确度的贡献，其使用参数的近似后验概率(使用Fisher信息矩阵计算)来估计。然后，当网络在新数据上被训练时，施加动态正则化，使得如果具有高重要性的参数从它们的原始值被更新，则它们招致大的惩罚，从而鼓励训练仅更新对于维持先前数据的性能不重要的参数。[18，19]遵循几乎相同的方法，但对参数重要性使用不同的试探法。也就是说，[18]使用整个训练中每个参数的梯度幅度来定义参数重要性，而[19]将与每个参数相关的损失变化视为学习新任务时参数重要性的代理。虽然不是精确的正则化方法，[20，21]提出了在线学习的约束优化公式，确保在线训练期间的参数更新不会损害先前任务的性能。</p><p id="bd53" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><strong class="lk jd"> <em class="ob">讨论。</em> </strong>类似于架构修改方法，基于正则化的在线学习方法在较小的规模上显示出前景。然而，当用于大规模实验时，这种方法往往不是超级有效的，并且对于大型深度学习模型来说，计算参数重要性变得极其昂贵。因此，基于正则化的在线学习方法通常不被认为对大规模在线学习应用有用[9，10]。</p><h2 id="f0f5" class="np mt it bd mu nq nr dn my ns nt dp nc lr nu nv ne lv nw nx ng lz ny nz ni iz bi translated">蒸馏</h2><p id="4651" class="pw-post-body-paragraph li lj it lk b ll nk kd ln lo nl kg lq lr nm lt lu lv nn lx ly lz no mb mc md im bi translated"><strong class="lk jd"> <em class="ob">概述。</em> </strong>在线学习的提炼方法受到深度学习中知识提炼概念的启发[22]。最初，提出了知识提取，通过训练学生在数据集上匹配教师的输出，将大型“教师”网络的知识“提取”到较小“学生”网络的参数中。稍微不同的是，在线学习方法采用提取，使得先前模型的知识(即，那些在旧数据上训练的)可以被提取到正在学习的当前网络中，以确保历史知识不会丢失。这种方法与普通的知识提炼非常相似。主要区别在于教师和学生网络通常具有相同的规模/架构，但来自在线培训阶段的不同点。这一过程类似于自蒸馏技术[39，40]。</p><p id="ea69" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><strong class="lk jd"> <em class="ob">战法。在线学习空间中已经提出了许多蒸馏方法。事实上，因为它倾向于不干扰其他方法(即，它只是对损失函数的修改！)，蒸馏通常与其他方法相结合，以增强在线学习过程[5，24]。[23]是最早提出在批量增量设置中使用蒸馏进行在线学习的著作之一。这项工作表明，如果相对于先前网络设置的蒸馏损失被添加到根据新数据计算的损失中，则可以更好地保持网络在先前任务中的性能。这种方法旨在取代以前的直接方法，以避免批量增量学习中的灾难性遗忘——增加一个微调阶段，以平衡的比例包括新旧数据(即平衡微调)[23，25]。随后的工作发现，如果新数据的分布与旧数据显著不同，用新数据执行蒸馏可能会导致性能下降。然后，通过直接缓存用于计算蒸馏损失的旧数据示例，解决了这一问题[2，5，24，26]。</em></strong></p><p id="e28e" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><strong class="lk jd"> <em class="ob">讨论。</em> </strong>蒸馏是在线学习社区中一种常用的方法，即使在大规模学习中也很有效。然而，当考虑将蒸馏用于在线学习时，后续工作表明，当之前的数据被缓存用于微调时，蒸馏的效率较低[27]。事实上，一些工作甚至认为，当维护以前数据的明确记忆以用于在线更新时，增加损失是不必要的，甚至可能是有害的[9，10，28]。因此，尽管提炼方法仍然流行，但是当允许记忆以前的数据示例时，它们的有效性是值得怀疑的。因此，存储以前的数据示例以供在线更新时使用的方法(统称为重放(或预演)技术)已经成为一种常用方法。</p><h2 id="b09a" class="np mt it bd mu nq nr dn my ns nt dp nc lr nu nv ne lv nw nx ng lz ny nz ni iz bi translated">重播</h2><p id="c2b4" class="pw-post-body-paragraph li lj it lk b ll nk kd ln lo nl kg lq lr nm lt lu lv nn lx ly lz no mb mc md im bi translated"><strong class="lk jd"> <em class="ob">概述。</em> </strong>术语“重放”泛指存储来自数据集先前部分的样本的在线学习方法。然后，当新数据到达时，这些存储的样本可以被合并到在线学习过程中，以防止灾难性的遗忘。例如，可以将这些先前的数据示例添加到蒸馏损失中，以避免网络输出偏离先前的设置太多。更常见的是，在在线学习过程中，可以简单地对先前的数据样本进行采样(即，在小批量中)以便与新数据相结合。在批量增量设置中，先前的示例将在微调期间与批量新数据混合，以确保旧知识不会丢失。类似地，流方法会将以前课程中随机抽样的样本合并到在线更新中，从而确保知识得到维护。</p><p id="b2eb" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><strong class="lk jd"> <em class="ob">方法。</em> </strong>基于重放的在线学习已经提出了很多方法。首先，许多提取方法存储以前的数据示例以供提取期间使用，这大致属于重放的范畴[2，5，6，24]。然而，纯重放通常直接用旧数据执行模型更新。[3]发现，即使保留非常少量的先前数据示例(即，每节课几个示例)直接用于在线培训，也会大大降低灾难性遗忘的影响。类似地，[29]证明了完全重放(即在缓冲区中保存所有以前的数据)完全消除了灾难性遗忘，而部分重放(即使示例数量显著减少)为在线学习提供了显著的好处——这一结果类似于[3]的发现。最近，[7]广泛探索了流设置中的重放技术，允许将压缩的要素制图表达存储在重放缓冲区中，而不是数据本身。这种方法在[8]中针对对象检测进行了修改，在大规模图像分类任务的流学习中取得了很好的结果。</p><p id="950f" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><strong class="lk jd"> <em class="ob">讨论。由于这些方法在各种应用中的规模不可知的成功，重放机制现在是大多数在线学习方法的核心组件。尽管存储以前的数据示例可能会占用大量内存，但执行重放可以极大地提高在线学习的性能。事实上，如果在缓冲器中保持足够的数据样本，重放已经被证明可以完全消除灾难性遗忘[29]。由于它的简单性和实用性，replay在在线学习社区中变得非常流行。</em></strong></p><h2 id="d918" class="np mt it bd mu nq nr dn my ns nt dp nc lr nu nv ne lv nw nx ng lz ny nz ni iz bi translated">调整资金组合</h2><p id="0588" class="pw-post-body-paragraph li lj it lk b ll nk kd ln lo nl kg lq lr nm lt lu lv nn lx ly lz no mb mc md im bi translated"><strong class="lk jd"> <em class="ob">概述。</em> </strong>最近在批量增量学习方面的几项工作已经注意到，以在线方式学习的模型往往偏向于最近观察到的数据(即，最近一批中的数据)。因此，提出了几种技术来消除这种不平衡，我称之为再平衡技术。这种方法背后的核心思想是确保预测不会偏向较新的数据(例如，在分类设置中，有偏向的模型会将几乎所有数据预测为最近观察到的一批训练数据中的一个类别)。相反，预测的大小应该在所有类别或类型的数据之间保持平衡，不知道在训练过程中何时遇到这样的数据。</p><p id="7587" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><strong class="lk jd"> <em class="ob">方法。</em> </strong>最初在【6】和【37】中探索了用于消除批量增量设置的网络分类层中的偏差的技术。在[6]中，在模型的softmax层上采用余弦归一化，从而确保每个类别的输出向量具有相等的幅度。[37]提出了一种类似的、可学习的方法，该方法利用小型验证数据集来训练线性“校正”模块，以消除分类层中的偏差。除了这些最初的工作之外，还提出了其他几种方法来固定增量学习的分类层中的偏差——通常是受这样一种想法的启发，即为一个类设定与该类第一次学习时相似数量级的分类权重。[38]当在线学习期间第一次遇到一个类时，将分类统计数据存储在一个小的存储缓冲区中，然后使用这些统计数据在以后的训练中使类分数更具有可比性。[27]遵循类似的方法，但是直接重用在线训练期间第一次学习一个类时的分类权重。</p><p id="776d" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><strong class="lk jd"> <em class="ob">讨论。</em> </strong>预测偏差是增量学习(即任务和批次/类增量)中一个已知的、可测量的问题。此外，添加再平衡被证明可以显著提高增量学习性能，即使是在大规模数据集上，如ImageNet [6，37]。因此，再平衡方法值得在这个领域使用。一般来说，最好是利用不需要任何验证集的方法来执行重新平衡(也就是说，这只是避免了创建验证集的麻烦)。例如，[6]不需要任何验证集，而[37]需要验证集。在增量学习设置之外(例如，在流学习中)，不清楚分类偏差是否遵循与增量学习中相同的模式。然而，增加再平衡不太可能损害在线学习模型的性能。</p><h2 id="3132" class="np mt it bd mu nq nr dn my ns nt dp nc lr nu nv ne lv nw nx ng lz ny nz ni iz bi translated">其他技术</h2><p id="1928" class="pw-post-body-paragraph li lj it lk b ll nk kd ln lo nl kg lq lr nm lt lu lv nn lx ly lz no mb mc md im bi translated">有些类似于replay，一些作品提出使用生成模型来“幻觉”以前课程中的例子。在[16]中，生成式对抗网络(GAN) [30]用于批量增量在线学习，其中生成器创建用于重放的数据示例，鉴别器既鉴别又解决潜在的学习问题。[31]采用自动编码器的类似方法，但在损失函数中加入了额外的蒸馏项。[32]在任务增量设置中利用GANs，其中每个任务训练鉴别器和生成器。然后，可以将生成器传送到下一个任务，以便可以通过生成器构建旧的数据表示，用于在学习新任务期间重放。[33]采用了类似的方法来解决领域扩展问题(即只有两个任务的任务增量学习)。</p><p id="2865" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">另一个比较受欢迎的研究领域是在线学习的双重记忆技术。这种方法受到大脑的启发，试图模仿记忆巩固的生物过程。在高层次上，双记忆方法——通常与replay [12]相结合——将单独的模型组件用于新形成的和长期的知识。例如，可以维护两个独立的网络，其中一个网络专门用于学习新数据，另一个网络试图解决整个学习问题(即，旧数据和新数据)[12]。[35]提出了一个类似的方法，保持两个独立的模型:短期记忆的概率模型和长期记忆的自动编码器。稍有不同的是，高度不确定的例子可以存储在一个单独的存储缓冲区中，该缓冲区随后被整合到网络中[34]。</p><p id="4a60" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">其他一些较少研究(但仍然值得注意)的在线学习方法包括稀疏编码方法[12]，基于集成的方法[36]，以及修改神经网络内激活函数以避免灾难性遗忘的方法[26]。虽然这些方法不太受欢迎，并且与常见的方法(如replay和extrification)相比已经不再受欢迎，但记住这些技术仍然是有用的，可以获得对该领域的整体理解，并(希望)为未来的创新提供思路。</p><h1 id="d43b" class="ms mt it bd mu mv mw mx my mz na nb nc ki nd kj ne kl nf km ng ko nh kp ni nj bi translated">那么…我该用什么呢？</h1><p id="5f81" class="pw-post-body-paragraph li lj it lk b ll nk kd ln lo nl kg lq lr nm lt lu lv nn lx ly lz no mb mc md im bi translated">鉴于这篇文章中对在线学习方法的广泛讨论，提供一个现有方法的总结很重要，它强调了大规模深度学习应用中最有用的方法。现有方法的效用可以简单地总结如下:</p><ul class=""><li id="557c" class="me mf it lk b ll lm lo lp lr mg lv mh lz mi md mj mk ml mm bi translated">架构修改和正则化很少使用，因为它们有某些缺点(如讨论中所提到的),并且往往在大规模时表现不佳。</li><li id="8a2d" class="me mf it lk b ll mn lo mo lr mp lv mq lz mr md mj mk ml mm bi translated">蒸馏非常受欢迎，但是当允许重放时，它的有效性是值得怀疑的。</li><li id="4767" class="me mf it lk b ll mn lo mo lr mp lv mq lz mr md mj mk ml mm bi translated">重放被广泛认为是减轻灾难性遗忘的最佳方法，并且在大规模在线学习实验中表现得非常好。</li><li id="d425" class="me mf it lk b ll mn lo mo lr mp lv mq lz mr md mj mk ml mm bi translated">再平衡在增量学习环境中很重要，因为它消除了对最近观察到的数据形成的偏见。</li></ul><p id="214c" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">因此，在线学习领域最好的“物有所值”的方法是使用基于回放的在线学习方法。例如，[7，8]提出了一种用于大规模深度学习场景的基于重放的方法，该方法表现得令人惊讶地好并且是内存高效的。类似地，像[3，29]这样的方法表明，简单地维护一个以前数据的缓冲区以供在线更新时使用是一个非常强大的工具。考虑到这一点，在几乎所有的在线学习场景中，执行重播似乎都是一个不错的选择。</p><p id="bb94" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">除了重放之外，使用蒸馏在某些情况下可以提高性能，尽管一些工作认为蒸馏在与重放结合使用时没有用。例如，[5]将提取和重放结合起来，即使在大规模的情况下也表现得非常好，从而表明提取在某些情况下可以积极地影响在线学习的性能。此外，如果使用增量学习来训练模型，利用再平衡是很重要的，因为分类层内的偏差会显著降低性能。</p></div><div class="ab cl oc od hx oe" role="separator"><span class="of bw bk og oh oi"/><span class="of bw bk og oh oi"/><span class="of bw bk og oh"/></div><div class="im in io ip iq"><p id="71b6" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">非常感谢你的阅读，我希望你喜欢这篇文章。欢迎访问我的<a class="ae lh" href="https://wolfecameron.github.io" rel="noopener ugc nofollow" target="_blank">研究页面</a>，或者联系我，询问你对这篇文章的任何问题/评论。如果你想了解我最新的出版物/博客文章，你也可以<a class="ae lh" href="https://twitter.com/cwolferesearch" rel="noopener ugc nofollow" target="_blank">在twitter </a>上关注我。如果你对这个话题感兴趣，我鼓励你访问我在莱斯大学的实验室的<a class="ae lh" href="http://akyrillidis.github.io/group/" rel="noopener ugc nofollow" target="_blank">网页，我是一名博士生，专注于深度学习的经验和理论基础。</a></p><p id="c5e8" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><em class="ob">参考书目<br/></em>【1】<a class="ae lh" href="https://arxiv.org/abs/1704.01920" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1704.01920</a></p><p id="5979" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">[2]<a class="ae lh" href="http://home.ustc.edu.cn/~saihui/papers/eccv2018_lifelong.pdf" rel="noopener ugc nofollow" target="_blank">http://home . ustc . edu . cn/~ sai hui/papers/eccv 2018 _ lifetime . pdf</a></p><p id="763b" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">[3]<a class="ae lh" href="https://arxiv.org/abs/1902.10486" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1902.10486</a></p><p id="300a" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><a class="ae lh" href="https://arxiv.org/abs/1908.04742" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1908.04742</a></p><p id="a4bb" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">[5]<a class="ae lh" href="https://arxiv.org/abs/1807.09536" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1807.09536</a></p><p id="5afd" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">[6]<a class="ae lh" href="https://openaccess.thecvf.com/content_CVPR_2019/html/Hou_Learning_a_Unified_Classifier_Incrementally_via_Rebalancing_CVPR_2019_paper.html" rel="noopener ugc nofollow" target="_blank">https://open access . the CVF . com/content _ CVPR _ 2019/html/Hou _ Learning _ a _ Unified _ Classifier _ Incrementally _ via _ Rebalancing _ CVPR _ 2019 _ paper . html</a></p><p id="39f0" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">[7]https://arxiv.org/abs/1910.02509<a class="ae lh" href="https://arxiv.org/abs/1910.02509" rel="noopener ugc nofollow" target="_blank"/></p><p id="ddac" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><a class="ae lh" href="https://arxiv.org/abs/2008.06439" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/2008.06439</a></p><p id="9d3f" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><a class="ae lh" href="https://arxiv.org/abs/2011.01844" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/2011.01844</a></p><p id="055a" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><a class="ae lh" href="https://arxiv.org/abs/1909.08383" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1909.0838</a></p><p id="afdd" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">[11]<a class="ae lh" href="https://www.sciencedirect.com/science/article/abs/pii/S0079742108605368" rel="noopener ugc nofollow" target="_blank">https://www . science direct . com/science/article/ABS/pii/s 0079742108605368</a></p><p id="ff36" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><a class="ae lh" href="https://arxiv.org/abs/1708.02072" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1708.02072</a></p><p id="d5c5" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><a class="ae lh" href="https://arxiv.org/abs/1606.04671" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1606.04671</a></p><p id="ad37" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><a class="ae lh" href="https://arxiv.org/abs/1612.03770" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1612.03770</a></p><p id="5c07" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><a class="ae lh" href="https://arxiv.org/abs/1908.08017" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1908.08017</a></p><p id="4da7" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><a class="ae lh" href="https://arxiv.org/abs/1904.03137" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1904.03137</a></p><p id="f94d" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><a class="ae lh" href="https://arxiv.org/abs/1612.00796" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1612.00796</a></p><p id="53b5" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><a class="ae lh" href="https://arxiv.org/abs/1711.09601" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1711.09601</a></p><p id="0d94" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><a class="ae lh" href="https://arxiv.org/abs/1703.04200" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1703.04200</a></p><p id="5f6f" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><a class="ae lh" href="https://arxiv.org/abs/1706.08840?source=post_page---------------------------" rel="noopener ugc nofollow" target="_blank">【https://arxiv.org/abs/1706.08840?source=post_page 20】-</a></p><p id="b4aa" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><a class="ae lh" href="https://arxiv.org/abs/1812.00420" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1812.00420</a></p><p id="d35d" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><a class="ae lh" href="https://arxiv.org/abs/1503.02531" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1503.02531</a></p><p id="8ce1" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">[23]<a class="ae lh" href="https://arxiv.org/abs/1606.09282" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1606.09282</a></p><p id="74f8" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">【https://arxiv.org/abs/1606.02355 24】<a class="ae lh" href="https://arxiv.org/abs/1606.02355" rel="noopener ugc nofollow" target="_blank"/></p><p id="ebb5" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">【https://arxiv.org/abs/1903.12648 25】<a class="ae lh" href="https://arxiv.org/abs/1903.12648" rel="noopener ugc nofollow" target="_blank"/></p><p id="2544" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><a class="ae lh" href="https://arxiv.org/abs/1607.00122" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1607.00122</a></p><p id="ff6b" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><a class="ae lh" href="https://arxiv.org/abs/2001.05755" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/2001.05755</a></p><p id="a76c" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">[28]<a class="ae lh" href="https://openaccess.thecvf.com/content_ICCV_2019/papers/Belouadah_IL2M_Class_Incremental_Learning_With_Dual_Memory_ICCV_2019_paper.pdf" rel="noopener ugc nofollow" target="_blank">https://open access . the CVF . com/content _ ICCV _ 2019/papers/Belouadah _ IL2M _ Class _ Incremental _ Learning _ With _ Dual _ Memory _ ICCV _ 2019 _ paper . pdf</a></p><p id="5d91" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">[29]<a class="ae lh" href="https://arxiv.org/abs/1809.05922" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1809.05922</a></p><p id="6e4d" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><a class="ae lh" href="https://arxiv.org/abs/1406.2661" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1406.2661</a></p><p id="2c94" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">[31]<a class="ae lh" href="https://arxiv.org/abs/1704.01920" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1704.01920</a></p><p id="a49f" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">[32]<a class="ae lh" href="https://arxiv.org/abs/1705.08690" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1705.08690</a></p><p id="7084" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">[33]<a class="ae lh" href="https://arxiv.org/abs/1705.00744" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1705.00744</a></p><p id="35f9" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">[34]<a class="ae lh" href="https://hal.archives-ouvertes.fr/hal-01418123/document" rel="noopener ugc nofollow" target="_blank">https://hal.archives-ouvertes.fr/hal-01418123/document</a></p><p id="5372" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">[35]<a class="ae lh" href="https://arxiv.org/abs/1711.10563" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1711.10563</a></p><p id="40db" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">[36]<a class="ae lh" href="https://arxiv.org/abs/1611.06194" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1611.06194</a></p><p id="d2da" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">[37]<a class="ae lh" href="https://arxiv.org/abs/1905.13260" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1905.13260</a></p><p id="c783" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">[38]<a class="ae lh" href="https://openaccess.thecvf.com/content_ICCV_2019/papers/Belouadah_IL2M_Class_Incremental_Learning_With_Dual_Memory_ICCV_2019_paper.pdf" rel="noopener ugc nofollow" target="_blank">https://open access . the CVF . com/content _ ICCV _ 2019/papers/Belouadah _ IL2M _ Class _ Incremental _ Learning _ With _ Dual _ Memory _ ICCV _ 2019 _ paper . pdf</a></p><p id="2b5a" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">[39]<a class="ae lh" href="https://openaccess.thecvf.com/content_cvpr_2017/papers/Yim_A_Gift_From_CVPR_2017_paper.pdf" rel="noopener ugc nofollow" target="_blank">https://open access . the CVF . com/content _ cvpr _ 2017/papers/Yim _ A _ Gift _ From _ CVPR _ 2017 _ paper . pdf</a></p><p id="7bef" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">[40]<a class="ae lh" href="https://arxiv.org/abs/1805.04770" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1805.04770</a></p></div></div>    
</body>
</html>