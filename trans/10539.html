<html>
<head>
<title>Illustrated Differences between MLP and Transformers for Tensor Reshaping in Deep Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">深度学习中张量整形的MLP和变压器之间的图示差异</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/illustrated-difference-between-mlp-and-transformers-for-tensor-reshaping-52569edaf89?source=collection_archive---------3-----------------------#2021-10-08">https://towardsdatascience.com/illustrated-difference-between-mlp-and-transformers-for-tensor-reshaping-52569edaf89?source=collection_archive---------3-----------------------#2021-10-08</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="f85d" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsdatascience.com/tagged/thoughts-and-theory" rel="noopener" target="_blank">思想和理论</a></h2><div class=""/><div class=""><h2 id="4cda" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">深入探究数学细节，并附有插图。</h2></div><p id="a1dd" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">在设计神经网络时，我们经常面临张量整形的需要。张量的空间形状必须随着某一层而改变，以便能够适合下游的层。就像顶部和底部表面形状不同的特殊楔形乐高积木一样，我们也需要一些神经网络中的适配块。</p><figure class="lo lp lq lr gt ls gh gi paragraph-image"><div role="button" tabindex="0" class="lt lu di lv bf lw"><div class="gh gi ln"><img src="../Images/ccb14a48bada113ba49805820a552311.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*Il2O_NRC3uMQacYK"/></div></div><p class="lz ma gj gh gi mb mc bd b be z dk translated">你为你的神经网络找到合适的形状了吗？图片来源:<a class="ae md" href="https://pixabay.com/users/aitoff-388338/?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=2129569" rel="noopener ugc nofollow" target="_blank">安德鲁·马丁</a>来自<a class="ae md" href="https://pixabay.com/" rel="noopener ugc nofollow" target="_blank"> Pixabay </a></p></figure><p id="d015" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">改变张量形状最常见的方法是通过汇集或步长卷积(非单位步长卷积)。例如，在计算机视觉中，我们可以使用池化或步长卷积将输入形状的空间维度从H x W变为H/2 x W/2，甚至变为不对称的H/4 x W/8。然而，为了涵盖简单缩放之外的更复杂的转换，例如执行单应性，我们需要更灵活的东西。<strong class="kt jd">多层感知器(MLP) </strong>或<strong class="kt jd">变形金刚(带交叉注意力)</strong>是两种现成的解决方案。</p><blockquote class="me mf mg"><p id="fff3" class="kr ks mh kt b ku kv kd kw kx ky kg kz mi lb lc ld mj lf lg lh mk lj lk ll lm im bi translated">计算机视觉中使用的神经网络张量一般具有NxHxWxC的“形状”(批次、高度、宽度、通道)。这里，我们将关注空间范围H和W中的形状变化，为了简单起见，忽略批次维度N，并保持特征通道维度C不变。我们将把HxW笼统地称为张量的“形状”或“空间维度”。</p><p id="8c7b" class="kr ks mh kt b ku kv kd kw kx ky kg kz mi lb lc ld mj lf lg lh mk lj lk ll lm im bi translated">在pytorch和许多其他深度学习库的标准术语中，“整形”不会改变张量中的元素总数。在这里，我们在更广泛的意义上使用单词reshape，张量中元素的数量可以改变。</p></blockquote><h1 id="97da" class="ml mm it bd mn mo mp mq mr ms mt mu mv ki mw kj mx kl my km mz ko na kp nb nc bi translated">如何用MLP和变形金刚重塑张量？</h1><p id="0890" class="pw-post-body-paragraph kr ks it kt b ku nd kd kw kx ne kg kz la nf lc ld le ng lg lh li nh lk ll lm im bi translated">MLP和变形金刚有相似的输入和输出接口，如果我们忽略内部处理的详细机制(通过MLP中的隐藏层和变形金刚中的交叉注意模块)，如下图所示。</p><figure class="lo lp lq lr gt ls gh gi paragraph-image"><div role="button" tabindex="0" class="lt lu di lv bf lw"><div class="gh gi ni"><img src="../Images/79db24be511ceb50c2aca455b7c4f0d5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RSJbxvlMaR_EkXzsOzKxzQ.png"/></div></div><p class="lz ma gj gh gi mb mc bd b be z dk translated">张量整形的MLP与变压器的相似界面(图表由作者制作)</p></figure><p id="d753" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">使用MLP来改变输入张量的形状是相对简单的。对于只有一个全连接层的最简单形式的MLP，从输入X到输出O的映射如下。</p><figure class="lo lp lq lr gt ls gh gi paragraph-image"><div role="button" tabindex="0" class="lt lu di lv bf lw"><div class="gh gi nj"><img src="../Images/1321adb9993bfe3950d5e9fd7012fdcf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2ewvp3nhvvTsBGtxuBaerA.png"/></div></div></figure><p id="68e4" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">如果我们在这里忽略激活函数和偏差b，那么本质是一个矩阵乘法，并且整形过程完全被权重矩阵w捕获。张量整形可以通过用w对<strong class="kt jd">左乘</strong>来实现。</p><figure class="lo lp lq lr gt ls gh gi paragraph-image"><div role="button" tabindex="0" class="lt lu di lv bf lw"><div class="gh gi nk"><img src="../Images/306c7896d16005c50f2dd708e63fb423.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zhW_88k3mbRT7wFkC9sAOw.png"/></div></div></figure><figure class="lo lp lq lr gt ls gh gi paragraph-image"><div role="button" tabindex="0" class="lt lu di lv bf lw"><div class="gh gi nl"><img src="../Images/1956836a15373f918eaa60f03e77cc57.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KEXhGkvjixioQFqxIY0etA.png"/></div></div><p class="lz ma gj gh gi mb mc bd b be z dk translated">MLP的核心运算是带学习矩阵W的matmul(作者制作的图表)</p></figure><blockquote class="me mf mg"><p id="47d0" class="kr ks mh kt b ku kv kd kw kx ky kg kz mi lb lc ld mj lf lg lh mk lj lk ll lm im bi translated">注意，我们在上面隐含地假设特征通道维数C=1，并且张量格式是HWxC，并且批次维数被忽略。这样，我们必须乘以输入左侧的W矩阵来改变空间形状。</p></blockquote><p id="7d97" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">按照最初的变压器公式，我们有以下映射。</p><figure class="lo lp lq lr gt ls gh gi paragraph-image"><div role="button" tabindex="0" class="lt lu di lv bf lw"><div class="gh gi nm"><img src="../Images/7cff3750268b3f1e46c55893805f936f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6-YxR1x8STSF-OdVqhmz0g.png"/></div></div></figure><p id="44b4" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">对于交叉注意模块，在上面的等式中，K和V是线性投影的输入X，Q是线性投影的输出查询。输出查询具有与输出o相同的空间形状。Q、K和V具有以下形状。</p><figure class="lo lp lq lr gt ls gh gi paragraph-image"><div role="button" tabindex="0" class="lt lu di lv bf lw"><div class="gh gi nn"><img src="../Images/346cf52b26a0e5f3a128ecfb1f1c33e8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*e_5eLto6IbLvvwmvc3JC9Q.png"/></div></div></figure><p id="1c10" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">与投影矩阵W相乘的目标是将输入X和输出查询提升到相同的特征维度。请注意，这里使用了<strong class="kt jd">右乘</strong>，这是一个与上述MLP中的整形操作不同的操作。如果我们忽略比例因子和Softmax激活函数，我们有以下等式。</p><figure class="lo lp lq lr gt ls gh gi paragraph-image"><div role="button" tabindex="0" class="lt lu di lv bf lw"><div class="gh gi no"><img src="../Images/80ae5397e7c48ad5a96ea00bb775cc1e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9Jn82d6OaTON5N2NTOmg5g.png"/></div></div></figure><figure class="lo lp lq lr gt ls gh gi paragraph-image"><div role="button" tabindex="0" class="lt lu di lv bf lw"><div class="gh gi np"><img src="../Images/e3d4c2f8e94f4438e225b55f5f2b6768.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zRVY7ia0ZgXBwKY5ww3c2g.png"/></div></div><p class="lz ma gj gh gi mb mc bd b be z dk translated">《变形金刚》中图解的交叉注意机制(图表由作者制作)</p></figure><blockquote class="me mf mg"><p id="d05d" class="kr ks mh kt b ku kv kd kw kx ky kg kz mi lb lc ld mj lf lg lh mk lj lk ll lm im bi translated">自我注意机制是原变形金刚论文进行特征提取的亮点。然而，自我关注保持原始的输入形状，因为输出查询也是自我关注模块中的输入X。为了对输入张量进行整形，必须使用具有不同形状(期望的输出形状)的输出查询。</p></blockquote><p id="a0ec" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">与MLP相比，我们有非常相似的公式，两者都将输入与学习的加权矩阵W左乘，以实现形状改变。然而，有两个不同之处。</p><ul class=""><li id="56c8" class="nq nr it kt b ku kv kx ky la ns le nt li nu lm nv nw nx ny bi translated">输出O经过一次额外的线性投影，将特征通道从输入1提升到输出d_k。</li><li id="c2a7" class="nq nr it kt b ku nz kx oa la ob le oc li od lm nv nw nx ny bi translated">变压器中的W矩阵取决于输入x。</li></ul><p id="0a2e" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">第一个差异相对较小，我们可以用一个额外的线性投影来匹配MLP，以改变特征通道。然而，第二个有一些重要的含义。我们将深入探讨MLP和变形金刚的两种加权矩阵之间的差异。</p><h1 id="3bc6" class="ml mm it bd mn mo mp mq mr ms mt mu mv ki mw kj mx kl my km mz ko na kp nb nc bi translated">区别1:数据依赖性</h1><p id="f8b1" class="pw-post-body-paragraph kr ks it kt b ku nd kd kw kx ne kg kz la nf lc ld le ng lg lh li nh lk ll lm im bi translated">MLP学习的w矩阵不依赖于输入数据，而变压器的w矩阵依赖于输入数据。一旦在训练期间学习，MLP的加权矩阵在推断期间是固定的。对于变压器，加权矩阵的数据依赖性可以被视为一种动态加权，使其自身适应不同的输入。类似的讨论也可以在<a class="ae md" href="https://stackoverflow.com/a/64218982" rel="noopener ugc nofollow" target="_blank"> StackOverflow </a>上找到。</p><p id="449a" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">这可能会让变形金刚更有表现力，但也会让变形金刚比MLP更难训练。具体地，对于固定的视图变换(例如<a class="ae md" rel="noopener" target="_blank" href="/monocular-birds-eye-view-semantic-segmentation-for-autonomous-driving-ee2f771afb59">逆透视映射(IPM) </a>或其他类型的单应)，MLP本质上只是学习输入和输出之间的固定映射。对于变压器，额外的输入数据可能会阻碍模型的初始收敛。在实现卓越性能之前，可能需要在GPU、数据和训练时间方面做出巨大努力，才能达到与MLP的盈亏平衡点。</p><figure class="lo lp lq lr gt ls gh gi paragraph-image"><div role="button" tabindex="0" class="lt lu di lv bf lw"><div class="gh gi oe"><img src="../Images/2f0bd976933bdc76376bd8725e96ef84.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*i7Q4mq7ai0kJggCVMHgLbw.png"/></div></div><p class="lz ma gj gh gi mb mc bd b be z dk translated">对《变形金刚》和《MLP》的投资回报率的过分简单化的看法(图表由作者绘制)</p></figure><h1 id="f3d7" class="ml mm it bd mn mo mp mq mr ms mt mu mv ki mw kj mx kl my km mz ko na kp nb nc bi translated">差异2:对输入顺序的不变性</h1><p id="0a50" class="pw-post-body-paragraph kr ks it kt b ku nd kd kw kx ne kg kz la nf lc ld le ng lg lh li nh lk ll lm im bi translated">对于MLP，输入和输出的顺序编码在矩阵w中。每行和每列对应于输入和输出形状的权重。MLP不需要位置编码来帮助索引输入和输出。</p><p id="d88a" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">另一方面，Transformers中的交叉注意模块对输入顺序是不变的。再看一下交叉注意的等式</p><figure class="lo lp lq lr gt ls gh gi paragraph-image"><div role="button" tabindex="0" class="lt lu di lv bf lw"><div class="gh gi of"><img src="../Images/4023bdae5b57ab24c4256555d22d45ad.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QX70Nv32Qu6cf8_FJpeVUQ.png"/></div></div></figure><p id="723d" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">如果x沿着空间形状维度进行某种排列，红色部分$X^T X$将保持不变，因此输出也保持不变。从另一个角度来看，K和V是一个字典的键值对，字典中的顺序并不重要，只要键值映射保持不变。交叉注意机制建立在查询和关键字之间的相似性上，而不是在位置上。</p><blockquote class="me mf mg"><p id="40bc" class="kr ks mh kt b ku kv kd kw kx ky kg kz mi lb lc ld mj lf lg lh mk lj lk ll lm im bi translated">对于自我注意，其中输出查询φ= X，那么O的顺序也经历与输入X相同的排列，数学上，自我注意是<strong class="kt jd">排列等变</strong>，而交叉注意是<strong class="kt jd">排列不变</strong>。</p></blockquote><figure class="lo lp lq lr gt ls gh gi paragraph-image"><div role="button" tabindex="0" class="lt lu di lv bf lw"><div class="gh gi og"><img src="../Images/0d7f588b6e7ceeded341af162bda7177.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*l7Pdg3g0H4Bxg7TtnWHXAw.png"/></div></div><p class="lz ma gj gh gi mb mc bd b be z dk translated">关于注意力的等变和不变的数学陈述(<a class="ae md" href="http://people.tamu.edu/~sji/classes/attn-slides.pdf" rel="noopener ugc nofollow" target="_blank">来源</a></p></figure><p id="0a22" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">注意机制不对位置信息进行编码的事实也正是位置编码(PE)对于顺序很重要的应用程序索引输入是必要的原因。具体来说，在NLP应用中，“猫追狗”和“狗追猫”会导致词对之间完全相同的注意力，这显然是有问题的。</p><p id="ffca" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">上述交叉注意机制也常用于图形神经网络(GNNs)。它允许网络在训练期间从所有输入实例中捕获共同特征，因为查询独立于输入并且由所有输入实例共享。这里有一条来自GNN先驱之一的推文，Thomas Kipf评论自我关注模块的排列等变。</p><figure class="lo lp lq lr gt ls"><div class="bz fp l di"><div class="oh oi l"/></div></figure><h1 id="4ab4" class="ml mm it bd mn mo mp mq mr ms mt mu mv ki mw kj mx kl my km mz ko na kp nb nc bi translated">用于BEV感知的张量整形</h1><p id="e535" class="pw-post-body-paragraph kr ks it kt b ku nd kd kw kx ne kg kz la nf lc ld le ng lg lh li nh lk ll lm im bi translated">我写了一篇新的博客，是关于在自动驾驶中使用交叉注意力对BEV感知进行张量整形的应用。有关更多详情，请参考下面的链接。</p><p id="81ba" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated"><a class="ae md" rel="noopener" target="_blank" href="/monocular-bev-perception-with-transformers-in-autonomous-driving-c41e4a893944">https://towards data science . com/monoclem-bev-perception-with transformers-in-autonomous-driving-c41e4a 893944</a></p></div><div class="ab cl oj ok hx ol" role="separator"><span class="om bw bk on oo op"/><span class="om bw bk on oo op"/><span class="om bw bk on oo"/></div><div class="im in io ip iq"><h1 id="ffa0" class="ml mm it bd mn mo oq mq mr ms or mu mv ki os kj mx kl ot km mz ko ou kp nb nc bi translated">外卖食品</h1><ul class=""><li id="bd96" class="nq nr it kt b ku nd kx ne la ov le ow li ox lm nv nw nx ny bi translated">MLP和变形金刚(交叉注意力)都可以用于张量整形。</li><li id="1e47" class="nq nr it kt b ku nz kx oa la ob le oc li od lm nv nw nx ny bi translated">MLP学到的整形机制不依赖于数据，而变形金刚的机制依赖于数据。这种数据依赖性使得变压器更难训练，但可能有更高的性能上限。</li><li id="8a50" class="nq nr it kt b ku nz kx oa la ob le oc li od lm nv nw nx ny bi translated">注意力不对位置信息进行编码。自我注意是置换等变的，而交叉注意是置换不变的。MLP对排列非常敏感，一个随机的排列可能会完全破坏MLP结果。</li></ul><h1 id="d4be" class="ml mm it bd mn mo mp mq mr ms mt mu mv ki mw kj mx kl my km mz ko na kp nb nc bi translated">参考</h1><ul class=""><li id="7c8f" class="nq nr it kt b ku nd kx ne la ov le ow li ox lm nv nw nx ny bi translated"><a class="ae md" href="https://arxiv.org/abs/1706.03762" rel="noopener ugc nofollow" target="_blank">注意力是你所需要的全部</a>，NeurIPS 2017</li><li id="3ec0" class="nq nr it kt b ku nz kx oa la ob le oc li od lm nv nw nx ny bi translated"><a class="ae md" href="http://people.tamu.edu/~sji/classes/attn-slides.pdf" rel="noopener ugc nofollow" target="_blank">《变形金刚中注意的数学性质》</a>，教授纪的课堂笔记，</li></ul></div></div>    
</body>
</html>