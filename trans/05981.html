<html>
<head>
<title>Training Question Answering Models from Synthetic Data (Research Paper Summary)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">从合成数据训练问答模型(研究论文摘要)</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/training-question-answering-models-from-synthetic-data-research-paper-summary-2220186703f?source=collection_archive---------37-----------------------#2021-05-28">https://towardsdatascience.com/training-question-answering-models-from-synthetic-data-research-paper-summary-2220186703f?source=collection_archive---------37-----------------------#2021-05-28</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="c04f" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">一个用合成数据训练的<strong class="ak">问答模型</strong>能打败SOTA吗？？</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/ae5e9d94cffdf159967b7a8f1545f6f2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Kewz0xLvwocuc8INzphzmA.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图片来自<a class="ae ky" href="https://unsplash.com/photos/8xAA0f9yQnE" rel="noopener ugc nofollow" target="_blank">来源</a></p></figure><p id="d17d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="lv">在这篇博客中，我尝试根据我的理解，从合成数据</em>  <em class="lv">中总结出论文</em> <a class="ae ky" href="https://arxiv.org/pdf/2002.09599.pdf" rel="noopener ugc nofollow" target="_blank"> <em class="lv">训练问答模型。请随时评论你的想法！</em></a></p><h1 id="f100" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">问题陈述</h1><p id="c515" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">这项研究的重点是在处理有限的人工标注数据时，通过生成合成问题和答案来改进问答模型。与来自训练集的人工注释问题相比，作者在仅使用合成数据的SQUAD1.1问题回答任务上实现了更好的性能。</p><h1 id="5662" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">提议的方法</h1><p id="4fcd" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">作者提出了由<strong class="lb iu">非条件答案抽取</strong>、<strong class="lb iu">问题生成</strong>和<strong class="lb iu">问题过滤</strong>组成的<em class="lv">三步流水线</em>。他们用于训练管道中所有三个组件的数据是SQUAD训练分割的一部分，并使用另一半从训练的组件中生成合成数据。一旦生成合成数据，他们就训练Q/A模型，并在SQUAD的开发集上测试其性能。下图显示了整个管道— </p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mt"><img src="../Images/74b084f71cd786b83a104e75d97d21df.png" data-original-src="https://miro.medium.com/v2/resize:fit:1010/format:webp/1*6-L6cVPbzn5x9Vv6Wgay8g.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">合成质量保证模型管道流|图片来自<a class="ae ky" href="https://arxiv.org/pdf/2002.09599.pdf" rel="noopener ugc nofollow" target="_blank">来源</a></p></figure><h2 id="f48a" class="mu lx it bd ly mv mw dn mc mx my dp mg li mz na mi lm nb nc mk lq nd ne mm nf bi translated">答案生成— a' ~ p(a|c)</h2><p id="158e" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">在这一步，他们提出了一个BERT模型，该模型学习在给定的上下文中提取答案跨度。他们只通过观察上下文而不是问题标记来训练这个模型<em class="lv">(这与典型的问答模型的训练方式有点不同)。通过这种方式，他们了解了数据集中答案的先验分布。如上所述，他们使用来自班的训练分割的(上下文和答案)对来训练该模型。</em></p><p id="c3f1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">他们通过联合学习开始和结束标记来对答案提取头进行建模，因为他们发现联合建模比单独对开始和结束标记进行建模的单个提取头执行得更好。<em class="lv">数学上可以表示为— </em></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ng"><img src="../Images/f8d58adce94271ceb1a81c23ba6120ec.png" data-original-src="https://miro.medium.com/v2/resize:fit:1106/format:webp/1*dMQFTfrXfnNcnyyLzJrSbw.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">答案生成模型</p></figure><p id="2c05" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这里，a=(s，e)和s，e，c分别是开始标记、结束标记和上下文标记。首先，我们通过BERT模型传递上下文，并在输出端获取开始和结束令牌嵌入。然后，这两种嵌入表示被连接起来，并被传递到一个多层感知器模型，随后是softmax，跨越可能的开始和结束跨度。目标是使这个软最大概率对于实际地面真实跨度更倾向于1，而对于所有其他跨度减小到0。所以，偏斜度越好，模型越好。</p><h2 id="b957" class="mu lx it bd ly mv mw dn mc mx my dp mg li mz na mi lm nb nc mk lq nd ne mm nf bi translated">问题生成— q' ~ p(q|a '，c)</h2><p id="9dc7" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">作为下一步的一部分，他们使用预训练的GPT-2语言模型来训练问题生成模型。它们将上下文标记、答案标记和问题标记连接成一个大的单个序列，由序列标记的结尾分隔。他们还在问题附近预先考虑并附加<strong class="lb iu"> <em class="lv">:问题:</em> </strong>标记作为问题标记。此外，为了确保模型知道不同的输入段，他们定义了三种类型的段嵌入，每种数据类型一种。此外，他们还将答案片段嵌入到上下文嵌入中，帮助模型在上下文中定位答案跨度。<em class="lv">如下图所示— </em></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nh"><img src="../Images/262320dd2ab967a7598b5b5861346f5d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1278/format:webp/1*LXx0ucngWH6_HbO5BqsLgg.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">问题生成培训|图片来自<a class="ae ky" href="https://arxiv.org/pdf/2002.09599.pdf" rel="noopener ugc nofollow" target="_blank">来源</a></p></figure><p id="6a5e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在推理过程中，它们提供上下文和答案标记，后跟<strong class="lb iu"> <em class="lv"> :question: </em> </strong>作为<strong class="lb iu"> </strong>触发词，并采样直到下一次出现相同的触发词。</p><h2 id="375b" class="mu lx it bd ly mv mw dn mc mx my dp mg li mz na mi lm nb nc mk lq nd ne mm nf bi translated">往返问题过滤—a’？a* ~ p(a|c，q ')</h2><p id="63b7" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">往返过滤的思想建立在过度生成然后过滤事物的动机上，其中，目标是从候选集中验证和选择最终问题。为此，我们首先在来自SQUAD的标记数据上训练一个问题回答模型p(a|c，q ),这里a、c、q分别是答案、上下文和问题。然后在推理过程中，我们传递生成的问题和上下文来生成候选答案。并且基于候选答案是否匹配生成的答案<em class="lv">(步骤-1)，选择或拒绝问题</em>。</p><h1 id="77cb" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">推理流程</h1><p id="691e" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">训练阶段完成后，我们继续进行推理阶段，其中，假设我们现在有了一个新的上下文，我们首先通过答案生成模型来生成答案。我们使用生成的答案和预定义的上下文，并将其传递给问题生成模型。一旦我们有了候选问题和答案，我们使用上下文和生成的问题来获得可能的候选答案。如果我们在此步骤中生成的答案与生成的答案<em class="lv">(步骤1的一部分)</em>匹配，我们认为生成的问题有效，而所有其他问题都被拒绝。<em class="lv">下图显示了整个推理流程— </em></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/f2c470e68a6b1b229f0c7ede6fd331c6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1186/format:webp/1*k95IOb1OxAVFtD5f6nZetg.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">推理流程|作者图片</p></figure><h1 id="d249" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">我的想法</h1><p id="7494" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated"><em class="lv">我发现这是一个非常有趣的阅读，我觉得在合成数据上训练Q/A模型的想法不仅是在数据较少的情况下采取的一个好步骤，而且也可以用作特定领域Q/A微调之后的预训练步骤的一部分。此外，为问题生成系统尝试编码器-解码器模型(如T5等)而不是仅使用解码器模型将是一个很好的实验。</em></p><blockquote class="nj"><p id="96fb" class="nk nl it bd nm nn no np nq nr ns lu dk translated">如果<em class="nt">你愿意你也可以</em> <a class="ae ky" href="https://medium.com/analytics-vidhya/summarizing-nlp-research-papers-dbd12965aa0a" rel="noopener"> <em class="nt">查看我写的其他研究论文摘要</em> </a> <em class="nt">。</em></p></blockquote><p id="bc1e" class="pw-post-body-paragraph kz la it lb b lc nu ju le lf nv jx lh li nw lk ll lm nx lo lp lq ny ls lt lu im bi translated"><strong class="lb iu">好了，这篇博客到此为止。如果你喜欢看视频而不是文字(就像我一样的:D)，一定要去看看— </strong></p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nz oa l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><a class="ae ky" href="https://www.youtube.com/channel/UCoz8NrwgL7U9535VNc0mRPA" rel="noopener ugc nofollow" target="_blank">多看看这样的视频</a></p></figure><p id="c18d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">请随意阅读整篇论文，并向作者问好，感谢他们的贡献。</p><blockquote class="ob oc od"><p id="a589" class="kz la lv lb b lc ld ju le lf lg jx lh oe lj lk ll of ln lo lp og lr ls lt lu im bi translated"><strong class="lb iu"> <em class="it">论文标题:</em> </strong> <em class="it"> </em>从合成数据中训练问答模型</p><p id="5d2e" class="kz la lv lb b lc ld ju le lf lg jx lh oe lj lk ll of ln lo lp og lr ls lt lu im bi translated"><strong class="lb iu"> <em class="it">论文链接:</em> </strong> <a class="ae ky" href="https://arxiv.org/pdf/2002.09599.pdf" rel="noopener ugc nofollow" target="_blank">访问论文</a></p><p id="315b" class="kz la lv lb b lc ld ju le lf lg jx lh oe lj lk ll of ln lo lp og lr ls lt lu im bi translated"><strong class="lb iu"> <em class="it">作者:</em> </strong>劳尔·普里、瑞安·斯普林、莫斯托法·帕特瓦雷、穆罕默德·舒伊比、布莱恩·卡坦扎罗</p></blockquote><p id="ae35" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="lv">另外，如果你喜欢读这篇文章，你可以选择</em> <strong class="lb iu"> <em class="lv">给我买一杯【柴】</em></strong>【https://www.buymeacoffee.com/TechvizCoffee】<a class="ae ky" href="https://www.buymeacoffee.com/TechvizCoffee" rel="noopener ugc nofollow" target="_blank"><em class="lv"/></a><em class="lv">——因为我实际上不喝咖啡:)非常感谢！完全是可选的，自愿的:)</em></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><a href="https://www.buymeacoffee.com/TechvizCoffee"><div class="gh gi oh"><img src="../Images/c8131c8f55990f5ddf0dadc91a525112.png" data-original-src="https://miro.medium.com/v2/resize:fit:264/format:webp/1*oBZwTW3aPRQsEERMuioV5g.png"/></div></a></figure></div></div>    
</body>
</html>