<html>
<head>
<title>From Prediction to Action — How to Learn Optimal Policies From Data (3/4)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">从预测到行动——如何从数据中学习最佳策略(3/4)</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/from-prediction-to-action-how-to-learn-optimal-policies-from-data-3-4-aa56c974a505?source=collection_archive---------24-----------------------#2021-08-06">https://towardsdatascience.com/from-prediction-to-action-how-to-learn-optimal-policies-from-data-3-4-aa56c974a505?source=collection_archive---------24-----------------------#2021-08-06</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi jn"><img src="../Images/2f71851eeb4676e621eebc00fd6f480f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*wJ2iBWTeZj5-F426"/></div></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">照片由<a class="ae kc" href="https://unsplash.com/@burst?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">上的</a>爆裂<a class="ae kc" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">未爆裂</a></p></figure><p id="3ad1" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在<a class="ae kc" rel="noopener" target="_blank" href="/from-prediction-to-action-how-to-learn-optimal-policies-from-data-part-1-1edbfdcb725d">第一部分</a>中，我们讨论了从数据中学习<em class="lb">最优</em>策略的必要性。政策优化涵盖了广泛的实际情况，我们简要地看了医疗保健、防止流失、目标营销和市政府的例子。</p><p id="711b" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在<a class="ae kc" rel="noopener" target="_blank" href="/from-making-predictions-to-optimizing-actions-an-introduction-to-policy-learning-2-4-9fc46ba8f3d0">第2部分</a>中，我们描述了如何创建一个数据集，使其适合策略优化。</p><p id="3c67" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在这篇文章中，我们描述了一个简单的(在我看来也是<em class="lb">神奇的</em>)方法来使用这样一个数据集来估计<strong class="kf ir"> <em class="lb">任何</em> </strong>政策的结果。</p><p id="dd4b" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在<a class="ae kc" rel="noopener" target="_blank" href="/from-prediction-to-action-how-to-learn-optimal-policies-from-data-4-4-14c63cc0c938">第4部分</a>中，我们将学习如何使用这样的数据集来寻找一个<strong class="kf ir">最优</strong> <em class="lb"> </em>策略。</p></div><div class="ab cl lc ld hu le" role="separator"><span class="lf bw bk lg lh li"/><span class="lf bw bk lg lh li"/><span class="lf bw bk lg lh"/></div><div class="ij ik il im in"><p id="b690" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">甚至当你有能力找到<em class="lb">最优</em>政策时，能够快速估计<em class="lb">任何</em>政策的结果也是非常有用的。</p><p id="6dfc" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">例如，在商业环境中，人们总是对简单的政策感兴趣，这些政策要求对每个顾客采取相同的行动。这些政策易于理解和执行，因此企业领导人自然更喜欢它们。为了帮助他们决定是否应该采用简单的政策，他们可能想知道使用简单的政策而不是复杂的政策会“放弃”多少结果。如果预期结果下降不多，我们也可以采用简单的政策，对吗？</p><p id="33ad" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">(BTW，简单的政策比比皆是。例如，在临床试验中，我们对两个简单策略(1)给每个人<em class="lb"> </em>药物，和(2)给每个人<em class="lb"> </em>安慰剂)之间平均结果的<em class="lb">差异</em>感兴趣</p></div><div class="ab cl lc ld hu le" role="separator"><span class="lf bw bk lg lh li"/><span class="lf bw bk lg lh li"/><span class="lf bw bk lg lh"/></div><div class="ij ik il im in"><p id="2d9e" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">让我们回到我们在这个博客系列中处理的网飞客户流失的例子。</p><p id="7958" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">假设你的老板带着一个新政策的想法来找你:</p><blockquote class="lj lk ll"><p id="2a50" class="kd ke lb kf b kg kh ki kj kk kl km kn lm kp kq kr ln kt ku kv lo kx ky kz la ij bi translated">如果一个顾客在过去的一个月里狂看了一个节目，“什么都不做”。</p><p id="3eed" class="kd ke lb kf b kg kh ki kj kk kl km kn lm kp kq kr ln kt ku kv lo kx ky kz la ij bi translated">否则，如果客户上个月的观看时间比上个月少了10–30 %,则提供“2个以上的设备”。如果客户上个月的观看时长环比下降超过30%，则提供“八折”。</p><p id="81d0" class="kd ke lb kf b kg kh ki kj kk kl km kn lm kp kq kr ln kt ku kv lo kx ky kz la ij bi translated">如果这些标准都不适用，“什么都不做”。</p></blockquote><p id="6a56" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">当然，您可以做一个实验来估计这个策略的平均结果:随机抽取一些客户，对他们运行这个策略，并测量结果。但这需要时间和努力。</p><p id="3d7b" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">有更快的方法。你可以使用我们在<a class="ae kc" rel="noopener" target="_blank" href="/from-making-predictions-to-optimizing-actions-an-introduction-to-policy-learning-2-4-9fc46ba8f3d0">第二部分</a>中准备的数据集来快速估算你老板脑波<em class="lb">的平均结果，而无需进行实验。</em></p><p id="3664" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这个问题的关键是一个神奇的东西，叫做<a class="ae kc" href="https://en.wikipedia.org/wiki/Horvitz%E2%80%93Thompson_estimator" rel="noopener ugc nofollow" target="_blank">霍维茨-汤姆森估计器</a>，下面是它的工作原理。</p><p id="4159" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">让我们从<a class="ae kc" rel="noopener" target="_blank" href="/from-making-predictions-to-optimizing-actions-an-introduction-to-policy-learning-2-4-9fc46ba8f3d0">第2部分</a>中组装的数据集开始。</p><figure class="lq lr ls lt gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi lp"><img src="../Images/dc9e6ea23f1d32c552374ffe54d9b918.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*L9RVI_NuAiQFFGETOdwH3g.png"/></div></div></figure><p id="a8c8" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">对于上面数据集中的每个客户，确定由提议的策略指定的操作(实际上，这应该是一个简单的SQL查询或类似的查询)。</p><figure class="lq lr ls lt gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi lu"><img src="../Images/44f6560ad2000ffb49740aa2a46108ef.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Xa7LHjFhekgnsmrPcr-qvg.png"/></div></div></figure><p id="23ca" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">接下来，向数据集中添加“调整后的结果”列。</p><figure class="lq lr ls lt gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi lv"><img src="../Images/976fa863f409435cadb546f5040350e2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*glTgVvOtQZ_xa7g-SQQvOw.png"/></div></div></figure><p id="5f7d" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在是巧妙的部分:<em class="lb">对于每一个被分配的实际行动与新政策规定的行动相匹配的客户，将结果除以被分配行动的概率，并将其填入调整后的结果列。</em></p><figure class="lq lr ls lt gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi lw"><img src="../Images/666d6b6b7be653d233c7a1a5034fd88f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kO_PeYU7kVvAil6FvRfsug.png"/></div></div></figure><p id="aae7" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">例如，分配给第三个客户的实际操作<em class="lb">和新策略指定的操作</em>都是“20%折扣”，因此我们将结果1.0除以分配给“20%折扣”操作的概率(92%)，得到调整后的结果1.0/0.92 = 1.1(类似地，标有红框的其他三行也是如此)。</p><p id="bab0" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">对于数据集中的所有<em class="lb">其他</em>行(即动作<em class="lb">与</em>不匹配的地方)，将0.0指定为调整后的结果。</p><figure class="lq lr ls lt gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi lx"><img src="../Images/b4763b4e5a0ca08caecbf50be739ed78.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LR7WwPapUTPzwY1TB4rxlg.png"/></div></div></figure><p id="a5f0" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">“调整后的结果”栏已完成。现在，简单地取这个列的平均值，瞧！你对你老板的政策的平均结果有一个估计！</p><figure class="lq lr ls lt gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi ly"><img src="../Images/bcf3b35491508faab55191864e12741b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TZWBDC6JlE2hfaUVeOkaZA.png"/></div></div></figure><p id="d5c8" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">几个小注意事项:</p><ul class=""><li id="91eb" class="lz ma iq kf b kg kh kk kl ko mb ks mc kw md la me mf mg mh bi translated">我们通过除以概率来计算调整后的结果。现在你明白了为什么我在第2部分中坚持认为数据收集策略中的每个概率都必须是非零的。</li><li id="268d" class="lz ma iq kf b kg mi kk mj ko mk ks ml kw mm la me mf mg mh bi translated">同样，建议确保您的数据收集策略中的概率不要太小。用非常小的概率来划分结果会夸大调整后的结果数字，并使估计变得不那么可靠。</li><li id="3367" class="lz ma iq kf b kg mi kk mj ko mk ks ml kw mm la me mf mg mh bi translated">Horvitz-Thompson估计量有一个“标准化”版本，称为Hajek估计量，具有更好的统计特性。我们不是寻找“调整结果”列的简单平均值，而是将该列的总和除以用于调整结果的概率的倒数之和(即除以1/0.92 + 1/0.45 + 1/0.22 + … + 1/0.56)。详见<a class="ae kc" href="http://arxiv.org/abs/2010.15835" rel="noopener ugc nofollow" target="_blank">杨等(2020) </a>第5.1节。</li><li id="4bb4" class="lz ma iq kf b kg mi kk mj ko mk ks ml kw mm la me mf mg mh bi translated">我想重申<a class="ae kc" rel="noopener" target="_blank" href="/from-making-predictions-to-optimizing-actions-an-introduction-to-policy-learning-2-4-9fc46ba8f3d0">第二部分</a>中的一个警告:如果训练数据集不是通过随机实验而是通过历史观察数据收集的，那么数据集中的客户将不是随机样本，这可能会严重影响估计结果。因此，对这些数字要有所保留。</li></ul></div><div class="ab cl lc ld hu le" role="separator"><span class="lf bw bk lg lh li"/><span class="lf bw bk lg lh li"/><span class="lf bw bk lg lh"/></div><div class="ij ik il im in"><p id="1524" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">鉴于这种评估任何政策的能力，您可以通过集思广益寻找新政策，评估它们有多好，并试图改进它们。</p><p id="9089" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">或者，你可以<em class="lb">直接</em>找一个<strong class="kf ir">最优</strong>策略。在第4部分中，我们将这样做。</p></div><div class="ab cl lc ld hu le" role="separator"><span class="lf bw bk lg lh li"/><span class="lf bw bk lg lh li"/><span class="lf bw bk lg lh"/></div><div class="ij ik il im in"><p id="e585" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">出于对数学的好奇，我在维基百科页面上尝试了一个更清晰的<a class="ae kc" href="https://en.wikipedia.org/wiki/Horvitz%E2%80%93Thompson_estimator#Proof_of_Horvitz-Thompson_Unbiased_Estimation_of_the_Mean" rel="noopener ugc nofollow" target="_blank">证明版本，展示了霍维茨-汤普森估计量如何无偏地估计任何政策的结果。</a></p><figure class="lq lr ls lt gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi mn"><img src="../Images/35ec8bcd4e6dcd31af9965edaefd263c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qkQYQk6au3Fu3sNrp8X2HQ.jpeg"/></div></div></figure><figure class="lq lr ls lt gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi ca"><img src="../Images/faebe803d42e8572178230e6aa7e6db2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4U6bQLj_F-7R9h2KjkBiUw.jpeg"/></div></div></figure><figure class="lq lr ls lt gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi mo"><img src="../Images/eebce9ce49da915e71e06fb3c32d15e8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*w1PYIzU-tg4zXNhrKPAjTQ.jpeg"/></div></div></figure></div></div>    
</body>
</html>