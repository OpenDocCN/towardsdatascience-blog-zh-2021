<html>
<head>
<title>Is attention what you really need in Transformers?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">《变形金刚》中你真正需要的是关注吗？</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/is-attention-what-you-really-need-in-transformers-6c161c2fca83?source=collection_archive---------20-----------------------#2021-06-02">https://towardsdatascience.com/is-attention-what-you-really-need-in-transformers-6c161c2fca83?source=collection_archive---------20-----------------------#2021-06-02</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="aeb9" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsdatascience.com/tagged/hands-on-tutorials" rel="noopener" target="_blank">实践教程</a></h2><div class=""/><div class=""><h2 id="098e" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">探索变形金刚模型中自我注意机制的有希望的替代和改进。</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/70e91478d448155b4c39cee741a916bb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RRVESZXE29hlczDaNeD1Yw.png"/></div></div></figure><p id="da5a" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">近年来，基于自我关注的方法，尤其是变形金刚，出现了爆炸式增长，首先出现在自然语言处理领域，最近也出现在计算机视觉领域。</p><p id="0622" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated"><a class="ae lz" rel="noopener" target="_blank" href="/transformers-an-exciting-revolution-from-text-to-videos-dc70a15e617b">如果你不知道什么是变形金刚，或者你想了解更多关于自我关注的机制，我建议你看看我关于这个话题的第一篇文章。</a></p><p id="fbf0" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">与以前的架构(如自然语言处理中的RNNs或计算机视觉中的卷积网络)相比，变压器的成功与它们以更好的方式解决重要问题的极端效率和能力有关。在变形金刚的基础上，有而且一直都有注意力的机制，被认为是“你所需要的一切”，是这个建筑不可或缺的和真正跳动的心脏。但并非所有闪光的都是金子，事实上，自我注意力的计算带来了巨大的计算和存储成本，例如需要非常大量的视频存储器并导致很高的训练时间。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ma"><img src="../Images/519345354f59adb25e16b91037aefaa4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*IQCDJmw3qHr4juGgdSYjyQ.gif"/></div></div></figure><p id="bd84" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">苹果和谷歌等大公司并没有忽视这一点，它们一直在努力制造不仅能够实现最先进效果，而且效率也很高的变压器。</p><h1 id="a949" class="mb mc it bd md me mf mg mh mi mj mk ml ki mm kj mn kl mo km mp ko mq kp mr ms bi translated">高效变压器</h1><p id="cc6c" class="pw-post-body-paragraph ld le it lf b lg mt kd li lj mu kg ll lm mv lo lp lq mw ls lt lu mx lw lx ly im bi translated">最近，《变形金刚》的联合创作者之一、谷歌的研究员卢卡斯·凯泽(Lukasz Kaiser)提出了一系列改进，让变形金刚在保持自我关注机制的情况下更加高效，而他关注的第一个、可能也是最重要的一个方面就是内存效率。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ma"><img src="../Images/deee5c4b033b7a4b5358eac6b8ca6259.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*LKBLWWLf70jdlRVvpCogPQ.gif"/></div></div><p class="my mz gj gh gi na nb bd b be z dk translated">基于<a class="ae lz" href="https://www.youtube.com/watch?v=xNZ5eiGLS2U" rel="noopener ugc nofollow" target="_blank">高效变压器</a></p></figure><p id="1f16" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">变形金刚使用大量内存，因为在它们执行期间会创建和维护多个中间张量，并且随着它们的累积，它们会在缺少大量资源的情况下很快使视频内存饱和。</p><p id="1627" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">Google Brain提出的解决这个问题的方法是避免将所有张量保存在内存中，而是使这个过程的每一步都是可逆的。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ma"><img src="../Images/f2a2279cc54d30a37d6efcdbd61e2008.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*jB92H0_Pe-HwJRT90YKwyA.gif"/></div></div><p class="my mz gj gh gi na nb bd b be z dk translated">基于<a class="ae lz" href="https://www.youtube.com/watch?v=xNZ5eiGLS2U" rel="noopener ugc nofollow" target="_blank">高效变压器</a></p></figure><p id="ec5b" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">为此，每一步都要维护两个张量，一个来自层的应用，另一个是前一个张量的副本。这允许过程继续，而不必维护整个中间张量链，而只维护最后一步的那些。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ma"><img src="../Images/6edd40a8f1e587d5b5bac8fb3cb44ef4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*maF44RRy0sZXgd4H-Q4qNQ.gif"/></div></div><p class="my mz gj gh gi na nb bd b be z dk translated">基于<a class="ae lz" href="https://www.youtube.com/watch?v=xNZ5eiGLS2U" rel="noopener ugc nofollow" target="_blank">高效变压器</a></p></figure><p id="720c" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">通过这种策略，我们可以显著降低内存成本，同时仍能获得与普通变压器相同的效果。这可能是目前已知的最聪明的方法之一，以保持变压器架构完全基于传统的自我关注，但成本较低。</p><h1 id="93c5" class="mb mc it bd md me mf mg mh mi mj mk ml ki mm kj mn kl mo km mp ko mq kp mr ms bi translated">AFT:无注意变压器</h1><p id="67e3" class="pw-post-body-paragraph ld le it lf b lg mt kd li lj mu kg ll lm mv lo lp lq mw ls lt lu mx lw lx ly im bi translated">但是在什么情况下，自我关注的计算变得如此复杂而难以管理呢？有没有消除二次复杂度的方法？我们现在真的需要注意力计算吗？这些是苹果公司的研究人员问自己的问题，也是注意力自由变形金刚的基础。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ma"><img src="../Images/9924d1b2fcc4f10298f58bd5243de596.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*9YoAtnbliwZbU9m89V6Heg.gif"/></div></div></figure><p id="ac9f" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">问题在于点积，点积用于组合查询、键和值，它是通过将每个输入向量都视为一个查询来完成的。意识到这一点，注意力自由变压器的目的是永远不会点产品，同时保留的好处。</p><p id="a811" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">与最初的转换器一样，AFT最初创建Q、K和V，作为输入与查询、键和值的矩阵的线性转换的结果。</p><p id="76e4" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">独特之处在于，在这一点上，不是执行点积来创建关注矩阵，而是对每个目标位置执行值的加权平均。这样做的结果通过逐元素乘法与查询相结合。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi nc"><img src="../Images/ab55a5d880938d42dff34157b91cd501.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*b6joq-qIJYn9_DO0ly9ntg.png"/></div></div><p class="my mz gj gh gi na nb bd b be z dk translated">来源:<a class="ae lz" href="https://arxiv.org/abs/2105.14103" rel="noopener ugc nofollow" target="_blank">一个注意力不集中的变形金刚</a></p></figure><p id="fba9" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">通过这种机制，可以获得依赖于输出特征数量和所考虑序列长度的线性计算和空间复杂度。从概念上讲，这只是一种不同的使信息在序列中流动的方式，但成本要低得多。</p><p id="9734" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">通过使用原始变压器在文献中先前测试的许多任务上测试注意力自由变压器，可以看到，例如在图(左)所示的视觉变压器的情况下，从AFT(右)获得的特征(在这种情况下，是AFT-Conv版本)即使被近似也似乎仍然是有意义的。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi nd"><img src="../Images/1b280517bb88099c73bc9ec68d748365.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tc-0vp4zVuW5Sikf9KdrZw.png"/></div></div><p class="my mz gj gh gi na nb bd b be z dk translated">来源:<a class="ae lz" href="https://arxiv.org/abs/2105.14103" rel="noopener ugc nofollow" target="_blank">一个注意力不集中的变压器</a></p></figure><p id="f4ba" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">通过这种机制，不仅可以降低计算注意力的成本，而且在所有考虑的任务中都获得了优异的结果，这表明该解决方案能够保持点积的所有优点，而不需要计算成本。</p><h1 id="a0ac" class="mb mc it bd md me mf mg mh mi mj mk ml ki mm kj mn kl mo km mp ko mq kp mr ms bi translated">FNet:傅立叶网络</h1><p id="eab3" class="pw-post-body-paragraph ld le it lf b lg mt kd li lj mu kg ll lm mv lo lp lq mw ls lt lu mx lw lx ly im bi translated">但也有人考虑完全放弃注意力的计算，转而寻求一种与注意力同样有效但计算成本却不那么高的机制。</p><p id="cc5a" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">看起来傅立叶变换是这项任务的一个很好的候选者。傅立叶变换只不过是在一个域(如时间域)中取一个函数，并将其带入另一个域(如频率域)中。</p><p id="ef66" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">谷歌提出的基于该机制的傅立叶网络与普通变压器完全相同，但注意力计算模块被一个处理傅立叶变换的层所取代。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ma"><img src="../Images/b0fbf32a26c71eeafc93735a209e0cb5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*TDFoR_CxjHYwet7fGGhaOA.gif"/></div></div></figure><p id="9be4" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">具有由T个记号组成的N个输入向量，傅立叶变换首先应用于所谓的“隐藏域”，然后应用于“序列域”。所有这些都是在没有任何类型的参数的情况下完成的，这带来了巨大的优势，因为其他层中存在的参数是唯一可以训练的参数，从而减少了模型参数的数量。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ma"><img src="../Images/fbd8d727d2520d1a777e9b4f48e82b24.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*h0JOYh6o6-6fo4lelNZPtw.gif"/></div></div></figure><p id="3ec8" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">因此，转换是线性的，首先按列，然后按行应用于输入。除了通过应用傅立叶变换获得的简单性之外，还有它是可逆的优点。</p><p id="289c" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">与AFT的情况完全一样，这一系列转换使得序列的各个部分能够相互影响，并且结果是包含从输入序列的各个部分导出的信息的转换表示。</p><p id="656a" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">显然，这似乎是一个非常有趣的方法，能够显著降低注意力计算的成本，并获得离散的结果。事实上，FNet似乎并不比经典变压器更好，可能有其他更好的方法以更低的成本获得类似的结果，但在缺乏大量计算资源的情况下，FNet可能是一个真正有效的选择。</p><h1 id="08c3" class="mb mc it bd md me mf mg mh mi mj mk ml ki mm kj mn kl mo km mp ko mq kp mr ms bi translated">未来等待我们的是什么？</h1><p id="35b3" class="pw-post-body-paragraph ld le it lf b lg mt kd li lj mu kg ll lm mv lo lp lq mw ls lt lu mx lw lx ly im bi translated">现在很清楚，Transformer是一个非常强大的架构，能够以我们从未见过的结果解决最多样化的问题，从翻译到分割到分类。然而，这些问题长期以来一直与它们对资源的过度消耗联系在一起，它们在计算机视觉领域的出现更加突出了这个问题，并促使许多研究人员寻求解决方案。在未来，我们可能会看到基于注意力的变形金刚，但经过优化后变得更轻，或者变形金刚失去了注意力机制，为更近似的技术腾出空间，或者全新的网络，类似于变形金刚，但采用不同的输入变换策略，如傅立叶变换。如果你想做好准备并了解更多，我建议你读一下我写的关于变形金刚T2和恐龙T4的文章。</p><p id="171c" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">有一件事是肯定的，如果目前大多数人还不可能充分利用这种架构，它很快就会为每个人所用，其巨大的潜力加上可访问性将使变形金刚比现在更加普遍和重要。</p><h1 id="78cd" class="mb mc it bd md me mf mg mh mi mj mk ml ki mm kj mn kl mo km mp ko mq kp mr ms bi translated">参考文献和见解</h1><p id="8252" class="pw-post-body-paragraph ld le it lf b lg mt kd li lj mu kg ll lm mv lo lp lq mw ls lt lu mx lw lx ly im bi translated">[1]《卢卡什·凯泽》。"<a class="ae lz" href="https://www.youtube.com/watch?v=xNZ5eiGLS2U" rel="noopener ugc nofollow" target="_blank">高效变压器</a></p><p id="36e2" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">[2]《双飞斋等人》。"<a class="ae lz" href="https://arxiv.org/pdf/2105.14103.pdf" rel="noopener ugc nofollow" target="_blank">无注意力变压器</a>"</p><p id="65fa" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">[3]《李中清-索普等人》。"<a class="ae lz" href="https://arxiv.org/abs/2105.03824" rel="noopener ugc nofollow" target="_blank"> FNet:用傅立叶变换混合令牌</a>"</p><p id="469d" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">[4]《扬尼克·基尔彻》。"<a class="ae lz" href="https://www.youtube.com/watch?v=JJR3pBl78zw" rel="noopener ugc nofollow" target="_blank"> FNet:混合令牌与傅立叶变换(机器学习研究论文讲解)</a>"</p><p id="0c88" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">[5]《大卫·柯考米尼》。"<a class="ae lz" rel="noopener" target="_blank" href="/transformers-an-exciting-revolution-from-text-to-videos-dc70a15e617b">关于变压器、定时器和注意事项</a>"</p><p id="0f56" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">[6]《大卫·柯考米尼》。<a class="ae lz" rel="noopener" target="_blank" href="/on-dino-self-distillation-with-no-labels-c29e9365e382">在迪诺上，无标签自蒸馏</a></p></div></div>    
</body>
</html>