<html>
<head>
<title>Applying SVM Based Active Learning on Multi-Class Datasets</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">基于SVM的主动学习在多类数据集上的应用</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/applying-svm-based-active-learning-on-multi-class-datasets-ba6aacdb52d1?source=collection_archive---------18-----------------------#2021-08-19">https://towardsdatascience.com/applying-svm-based-active-learning-on-multi-class-datasets-ba6aacdb52d1?source=collection_archive---------18-----------------------#2021-08-19</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="3204" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsdatascience.com/tagged/hands-on-tutorials" rel="noopener" target="_blank">实践教程</a></h2><div class=""/><div class=""><h2 id="f26c" class="pw-subtitle-paragraph jw iz iq bd b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn dk translated">基于主动学习和半监督学习的多类分类问题标记策略</h2></div><p id="72b4" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">在新时代，大量的数据被收集和处理，以提取有价值的信息。同样，机器学习模型也在不断改进，新的方法也在不断提供。显然，基于监督学习的方法对于数据驱动的问题会产生更好的准确性。然而，他们不可或缺地需要每个样品的标签。标签信息对于设计更多的实体模型和获得更好的结果至关重要。不幸的是，这些发展中唯一缺乏的是获得正确的标签信息。给数据加标签通常是一个耗时且令人信服的过程。</p><p id="af9a" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">主动学习背后的想法是，考虑到标记所有样本是很困难的，找到要标记的最有信息量的样本。因此，通过标记整个样本的一个小子集，模型的准确性可能会显著提高。此时，一个简单的问题出现了。如何从整个集合中选择最具信息性的样本？我提到了一种在多类数据集中决定最有信息量的样本的方法。此外，我将提到一个简单的基于半监督的方法来增加样本集中的标签数量。以及如何在同一数据集中同时利用主动学习和半监督学习。</p><figure class="ll lm ln lo gt lp gh gi paragraph-image"><div role="button" tabindex="0" class="lq lr di ls bf lt"><div class="gh gi lk"><img src="../Images/ddaea7a287482956ee0eda3bf248b897.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*tx_Z1gTPESO3w6f8"/></div></div><p class="lw lx gj gh gi ly lz bd b be z dk translated">由<a class="ae ma" href="https://unsplash.com/@almapapi?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">克里斯汀娜·帕普</a>在<a class="ae ma" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="633e" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">为了揭示最丰富的样本，大多数研究受益于概率方法。在这篇文章中，我使用了基于SVM的方法，而不是典型的概率模型。众所周知，支持向量机(SVM)旨在找出N维空间中使不同类别点之间的距离最大化的超平面。请看图1，这是一个只包含两个特征的数据集。SVM基本上是看着这边的样点来决定上课的。类似地，点到超平面的距离(裕度)是样本属于该类有多强的一种度量。因此，SVM评论说<em class="mb">样品3 </em>和<em class="mb"> 4 </em>分别属于蓝色和红色类的概率比<em class="mb">样品1 </em>和<em class="mb"> 2 </em>高。</p><p id="ae0d" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">那么，我们如何在主动学习中使用这些信息，换句话说，找出信息量最大的样本？</p><blockquote class="mc md me"><p id="0267" class="ko kp mb kq b kr ks ka kt ku kv kd kw mf ky kz la mg lc ld le mh lg lh li lj ij bi translated">以简单的方式，靠近超平面的样本比远离超平面的样本更能提供信息。SVM为离超平面最远的样本做出具有更高置信度的预测。另一方面，越靠近超平面的样本的类别就越不确定。出于这个原因，拥有更接近超平面的样本的基础事实比其他样本更有价值。</p></blockquote><figure class="ll lm ln lo gt lp gh gi paragraph-image"><div class="gh gi mi"><img src="../Images/cb0ee6a9c00591ec6dae129cb1228cce.png" data-original-src="https://miro.medium.com/v2/resize:fit:842/format:webp/1*e96CitDU0DH_bTwan7f3NA.png"/></div><p class="lw lx gj gh gi ly lz bd b be z dk translated">图1 —二维空间中的SVM描述(图片由作者提供)</p></figure><p id="db63" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">但是，对于多类分类问题，就没那么简单了。我使用<a class="ae ma" href="https://scikit-learn.org/stable/modules/generated/sklearn.multiclass.OneVsRestClassifier.html" rel="noopener ugc nofollow" target="_blank"><em class="mb"/></a>多类策略，这是多类分类任务最常用的方法之一。原则上，每个分类器的每个类都与OvR中的所有其他类相匹配。因此，与其替代的OneVsOneClassifier(OvO)相比，它的解释相对容易。</p><p id="40ac" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">作为OvR的结果，您期望每个类都有一个预测。为了更清楚，假设我们有4个类；a、B、C和d。在这样的例子中，您获得4个不同的二元分类器结果，它们是</p><p id="f18a" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">A vs (B，C，D)；</p><p id="6684" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">B vs (A，C，D)；</p><p id="2754" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">C vs (A，B，D)；</p><p id="c4db" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">D vs (A，B，C)；</p><p id="1c33" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">同样，这意味着4个不同的距离测量，如果你在OvR上应用SVM。距离的符号简单地说明了等级；例如，如果上述示例中第一个分类器的距离(A vs (B，C，D))为正，则分类器返回class A，如果为负，则返回not A。</p><p id="7b3c" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">正如所料，1个正距离测量值和3个负距离测量值的组合是最有把握的预测，这是图2中的<em class="mb"> Sample_4 </em>。OvR声明样本属于作为4个不同分类器的结果的具有正距离值的类别。另一方面，4个正的或4个负的距离测量是不确定预测的例子。在主动学习中，我们对这些最不自信的样本感兴趣。我没有遇到<em class="mb"> Sample_5 </em>对应的情况。最有可能的情况是，您遇到像<em class="mb"> Sample_1 </em>这样的场景作为<em class="mb"> </em>最不自信的样本。在<em class="mb">样本_1 </em>之后，<em class="mb">样本_2 </em>可以被定义为第二大信息量样本。它也代表了一种歧义情况。预测声明<em class="mb"> Sample_2 </em>可能是任何B、C或D类的成员，但不是a。</p><p id="5f0d" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">然后，如果您还有手动标记样品的空间，您可以考虑像<em class="mb"> Sample_3 </em>这样的样品。此时，首先计算两个正距离值之差的绝对值。差异越小，样本的预测就越模糊。因此，您可以选择差值较小的样本。我试图在图2中总结一个4类数据集的OvR组合。同样的想法可以扩展到所有多类数据集，只需稍加修改。</p><figure class="ll lm ln lo gt lp gh gi paragraph-image"><div role="button" tabindex="0" class="lq lr di ls bf lt"><div class="gh gi mj"><img src="../Images/976ce75de71fecc57f9638f82be8e38e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MPhlJSwbPjlfRqRLLqFYVw.png"/></div></div><p class="lw lx gj gh gi ly lz bd b be z dk translated">图2-在4类数据集上应用OVR(SVM)-距离值符号的组合(图片由作者提供)</p></figure><p id="ca34" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">你也可以看看下面的代码片段，它是上述方法的实现。</p><pre class="ll lm ln lo gt mk ml mm mn aw mo bi"><span id="5957" class="mp mq iq ml b gy mr ms l mt mu"># It calculates the difference of distance results of only positive 2 distance value<br/>def posit_diff(a,b,c,d):<br/>    lst = list([a,b,c,d])<br/>    print(lst)<br/>    index_lst = [lst.index(i) for i in lst if i&gt;0]<br/>    print(index_lst)<br/>    if len(index_lst) != 2:<br/>        print('Warning! Expecting only 2 positive distance values')<br/>        return -1<br/>    else:<br/>        return abs(lst[index_lst[0]] - lst[index_lst[1]])</span><span id="18cb" class="mp mq iq ml b gy mv ms l mt mu">MODEL = LinearSVC(penalty='l2',dual=False, multi_class='ovr', class_weight = 'balanced', random_state=1709)<br/>ACTIVE_LEARNING_BATCH_SIZE = 100 # lets say I am looking for 100 most informative samples from the data set <br/># FEATURES = [...]</span><span id="9f5f" class="mp mq iq ml b gy mv ms l mt mu">def active_learning(df_train_set, df_unlabelled_set):<br/>    """<br/>    Applying active learning to an example of 4 classes classification problem. <br/>    """<br/>    ovr = OneVsRestClassifier(MODEL)<br/>    ovr.fit(df_train_set[FEATURES], df_train_set['LABEL'])<br/>    pred = ovr.predict(df_unlabelled_set[FEATURES])</span><span id="5fcd" class="mp mq iq ml b gy mv ms l mt mu">dec_func = (ovr.decision_function(df_unlabelled_set[FEATURES]))</span><span id="f32d" class="mp mq iq ml b gy mv ms l mt mu">df_dec_func = pd.DataFrame(dec_func, columns = ['1','2','3','4'])<br/>    df_dec_func = pd.concat([df_dec_func, df_unlabelled_set],axis=1)</span><span id="8444" class="mp mq iq ml b gy mv ms l mt mu">df_dec_func['positives'] = df_dec_func[['1', '2', '3', '4']].gt(0).sum(axis=1)<br/>    df_dec_func['negatives'] = df_dec_func[['1', '2', '3', '4']].lt(0).sum(axis=1)</span><span id="f5bb" class="mp mq iq ml b gy mv ms l mt mu">df_dec_func_posit_0 = df_dec_func.loc[df_dec_func['positives']==0] # the most informative ones<br/>    df_dec_func_posit_3 = df_dec_func.loc[df_dec_func['positives']==3] # the second most informative ones<br/>    df_dec_func_posit_2 = df_dec_func.loc[df_dec_func['positives']==2] # the third most informative ones</span><span id="76d5" class="mp mq iq ml b gy mv ms l mt mu">df_dec_func_posit_2['posit_diff'] = df_dec_func_posit_2[['1','2','3','4']].apply(lambda x: posit_diff(*x), axis=1)<br/>    df_dec_func_posit_2 = df_dec_func_posit_2.sort_values(by=['posit_diff'], ascending = True)<br/>    df_dec_func_posit_2.reset_index(drop=True, inplace=True)<br/>    rest_needed = (ACTIVE_LEARNING_BATCH_SIZE) - (df_dec_func_posit_0.shape[0] + df_dec_func_posit_3.shape[0])<br/>    <br/>    if rest_needed &gt; 0:<br/>        df_dec_func_posit_2_al = df_dec_func_posit_2.iloc[0:rest_needed,:]<br/>        df_act_learn = pd.concat([df_dec_func_posit_0, df_dec_func_posit_3, df_dec_func_posit_2_al], axis=0)<br/>    else:<br/>        df_act_learn = pd.concat([df_dec_func_posit_0, df_dec_func_posit_3], axis=0)<br/>        df_act_learn = df_act_learn.sort_values(by=['positives'], ascending=True)<br/>        df_act_learn = df_act_learn.iloc[0:ACTIVE_LEARNING_BATCH_SIZE,:]<br/>    df_act_learn.reset_index(drop=True, inplace=True)<br/>    <br/>    return df_act_learn</span></pre><p id="dc66" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">更进一步，聚类可以应用于推断为最有信息的样本的样本集。通过从聚类的质心和边界选择样本(等于类的数量)，可以确保最终集合中的变化。</p></div><div class="ab cl mw mx hu my" role="separator"><span class="mz bw bk na nb nc"/><span class="mz bw bk na nb nc"/><span class="mz bw bk na nb"/></div><div class="ij ik il im in"><h2 id="46f1" class="mp mq iq bd nd ne nf dn ng nh ni dp nj kx nk nl nm lb nn no np lf nq nr ns iw bi translated">半监督学习</h2><p id="ebf6" class="pw-post-body-paragraph ko kp iq kq b kr nt ka kt ku nu kd kw kx nv kz la lb nw ld le lf nx lh li lj ij bi translated">增加数据集中标签数量的另一种方法是以自动方式标记最有把握的样本。换句话说，如果预测样本高于预定的阈值，则可以迭代地标记样本。并且阈值可以在每次迭代中增加。</p><p id="c3bd" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">假设，分类器预测一个样本属于A类的概率为87%，高于阈值85%。因此，在接下来的步骤中，它可能被评估为A类成员。训练集和未标记集在每次迭代结束时被更新。注意，事实上，它可能不是类A的成员，这是基于半监督的方法的一个非常严重的缺点。我在下面的代码片段中分享了这种方法的一个简单代码示例:</p><pre class="ll lm ln lo gt mk ml mm mn aw mo bi"><span id="c07c" class="mp mq iq ml b gy mr ms l mt mu">MODEL = RandomForestClassifier(max_depth=4, n_estimators=200, class_weight='balanced', random_state=1709)<br/>LIST_LABELLED_SAMPLES = df_train_set.shape[0]<br/>SS_LEARNING_LABELLING_THRESHOLD = 0.80<br/>SS_THRESHOLD_INCREASE = 0.01<br/>MAX_SS_ITERATION = 10<br/># FEATURES = [...]</span><span id="da36" class="mp mq iq ml b gy mv ms l mt mu">def ss_iterations(df_train_set, df_unlabelled_set):  <br/>    """<br/>    It covers all steps of semi supervised learning.<br/>    It uses a simple Ranfom Forest to fit df_train_set and predict df_unlabelled_set <br/>    to determine the most confident samples <br/>    those are predicted with a higher accuracy than SS_LEARNING_LABELLING_THRESHOLD. <br/>    """<br/>    pred_threshold = SS_LEARNING_LABELLING_THRESHOLD<br/>    <br/>    ovr = OneVsRestClassifier(MODEL)<br/>    print('Before iterations, size of labelled and unlabelled data', df_train_set.shape[0], df_unlabelled_set.shape[0])</span><span id="dece" class="mp mq iq ml b gy mv ms l mt mu">for i in range (0, MAX_SS_ITERATION):</span><span id="d7fc" class="mp mq iq ml b gy mv ms l mt mu">ovr.fit(df_train_set[FEATURES], df_train_set['LABEL'])<br/>        preds = ovr.predict_proba(df_unlabelled_set[FEATURES])</span><span id="abf3" class="mp mq iq ml b gy mv ms l mt mu">df_pred = pd.DataFrame({'1': preds[:, 0], '2': preds[:, 1], '3': preds[:, 2], '4': preds[:, 3]})<br/>        df_pred_ss= pd.concat([df_unlabelled_set, df_pred], axis=1)</span><span id="4e53" class="mp mq iq ml b gy mv ms l mt mu">df_pred_ss['MAX_PRED_RATIO'] = df_pred_ss[['1', '2', '3', '4']].max(axis=1)<br/>        df_pred_ss['LABEL'] = df_pred_ss[['1', '2', '3', '4']].idxmax(axis=1)<br/>        df_pred_ss['LABEL'] = df_pred_ss['LABEL'].astype(int)<br/>        df_pred_ss_up = df_pred_ss[df_pred_ss['MAX_PRED_RATIO'] &gt;= pred_threshold]<br/>        print('The number of samples predicted with high confidence level:', df_pred_ss_up.shape)</span><span id="2b0c" class="mp mq iq ml b gy mv ms l mt mu">if len(df_pred_ss_up) &gt; 0:</span><span id="0bdd" class="mp mq iq ml b gy mv ms l mt mu"># deleting from unlabelled set<br/>            df_unlabelled_set.drop(index=df_pred_ss_up.index.tolist(),inplace=True)<br/>            df_unlabelled_set.reset_index(drop=True,inplace=True)</span><span id="2347" class="mp mq iq ml b gy mv ms l mt mu"># adding to train set as if they are ground truths<br/>            df_train_set = pd.concat([df_train_set, df_pred_ss_up[df_train_set.columns]],axis=0)<br/>            df_train_set.reset_index(drop=True, inplace=True)</span><span id="f839" class="mp mq iq ml b gy mv ms l mt mu">print('Threshold ratio', pred_threshold)<br/>            print('Remaining unlabelled data', df_unlabelled_set.shape[0])<br/>            print('Total labelled data', df_train_set.shape[0])<br/>            print('Iteration is completed', i)</span><span id="4214" class="mp mq iq ml b gy mv ms l mt mu">pred_threshold += SS_THRESHOLD_INCREASE<br/>        else:<br/>            print('No improvement!')<br/>            break</span><span id="4786" class="mp mq iq ml b gy mv ms l mt mu">df_train_set.reset_index(drop=True,inplace=True)<br/>    df_unlabelled_set.reset_index(drop=True,inplace=True)<br/>    <br/>    return df_train_set, df_unlabelled_set</span></pre></div><div class="ab cl mw mx hu my" role="separator"><span class="mz bw bk na nb nc"/><span class="mz bw bk na nb nc"/><span class="mz bw bk na nb"/></div><div class="ij ik il im in"><h1 id="dc59" class="ny mq iq bd nd nz oa ob ng oc od oe nj kf of kg nm ki og kj np kl oh km ns oi bi translated"><strong class="ak">标签增强——主动学习vs半监督学习</strong></h1><p id="94d5" class="pw-post-body-paragraph ko kp iq kq b kr nt ka kt ku nu kd kw kx nv kz la lb nw ld le lf nx lh li lj ij bi translated">总之，标签增加背后的原因非常简单。实际上，主动学习背后的推理与半监督学习相反。</p><blockquote class="mc md me"><p id="7569" class="ko kp mb kq b kr ks ka kt ku kv kd kw mf ky kz la mg lc ld le mh lg lh li lj ij bi translated">在主动学习中，我们的目标是找到以低概率预测的信息量最大的样本。在找到它们之后，这些样本被分享给专家进行人工标记。如果您有大量未标记的数据，而标记这些数据的资源有限，那么这是一种非常有效的方法。另一方面，预测概率高(信息量最少)的样本被自动标记，就好像它们是基本事实一样，并被添加到下一次迭代的训练集中。</p></blockquote><p id="c2a4" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">主动学习和半监督学习都可以顺序地应用于相同的数据集。首先，应用多个半监督迭代，直到不再有概率高于阈值的预测。然后主动学习的单次迭代可以应用于前一步骤的结果，并且推断的信息样本可以与要标记的领域专家共享。可能会重复一定数量的迭代集。</p><h1 id="9ac9" class="ny mq iq bd nd nz oj ob ng oc ok oe nj kf ol kg nm ki om kj np kl on km ns oi bi translated"><strong class="ak">有用的链接</strong></h1><div class="oo op gp gr oq or"><a rel="noopener follow" target="_blank" href="/active-learning-in-machine-learning-525e61be16e5"><div class="os ab fo"><div class="ot ab ou cl cj ov"><h2 class="bd ja gy z fp ow fr fs ox fu fw iz bi translated">机器学习中的主动学习</h2><div class="oy l"><h3 class="bd b gy z fp ow fr fs ox fu fw dk translated">大多数有监督的机器学习模型需要大量的数据才能训练出好的结果。即使这样…</h3></div><div class="oz l"><p class="bd b dl z fp ow fr fs ox fu fw dk translated">towardsdatascience.com</p></div></div><div class="pa l"><div class="pb l pc pd pe pa pf lu or"/></div></div></a></div><div class="oo op gp gr oq or"><a rel="noopener follow" target="_blank" href="/active-learning-say-yeah-7598767806b2"><div class="os ab fo"><div class="ot ab ou cl cj ov"><h2 class="bd ja gy z fp ow fr fs ox fu fw iz bi translated">主动学习——说是！</h2><div class="oy l"><h3 class="bd b gy z fp ow fr fs ox fu fw dk translated">机器学习这个，机器学习那个！你知道该怎么做。来说一个人只是…</h3></div><div class="oz l"><p class="bd b dl z fp ow fr fs ox fu fw dk translated">towardsdatascience.com</p></div></div><div class="pa l"><div class="pg l pc pd pe pa pf lu or"/></div></div></a></div></div></div>    
</body>
</html>