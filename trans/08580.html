<html>
<head>
<title>Stream Your Cosmos DB Changes To Databricks with Spark 3</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用Spark 3将您的Cosmos DB更改传输到Databricks</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/stream-your-cosmos-db-changes-to-databricks-with-spark-3-8044b237b853?source=collection_archive---------10-----------------------#2021-08-08">https://towardsdatascience.com/stream-your-cosmos-db-changes-to-databricks-with-spark-3-8044b237b853?source=collection_archive---------10-----------------------#2021-08-08</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/1471ea9b3adba0b9d86895cb62cc845c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Yhz2ege0BGNM810M6izkHA.jpeg"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">图片来自<a class="ae jg" href="https://pixabay.com/?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=1756274" rel="noopener ugc nofollow" target="_blank">皮克斯拜</a></p></figure><div class=""/><div class=""><h2 id="f676" class="pw-subtitle-paragraph kg ji jj bd b kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx dk translated">Azure Cosmos DB Apache Spark 3 OLTP连接器使用Python在Databricks DBR 8.0+中实现</h2></div><p id="a4b6" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">Cosmos DB越来越受欢迎，因为它的低延迟响应使它更适合作为事务数据库解决方案。然而，Cosmos DB中的数据也可能需要用于数据分析和报告。在这种情况下，我们需要使用更合适的工具。Spark通常被认为是大数据分析的最佳选择。</p><p id="32b6" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">Databricks作为一个现代化的云数据平台，构建在Spark之上，提供了许多很酷的功能，以改善开发体验和实际的数据处理性能。因此，它被各行各业的许多公司所使用。</p><p id="43b0" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">微软曾经有大量关于Spark 2连接器的资源和参考资料。然而，当涉及到基于Spark 3的Databricks DBR (Databricks运行时版本)8.x时，我们必须使用相应的Spark 3连接器。不幸的是，还没有太多的文档和示例可供参考。</p><p id="5f67" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在本文中，我将提供这种场景的一个简单示例，以及我们遇到的一些常见问题及其解决方案。</p><h1 id="5df5" class="lu lv jj bd lw lx ly lz ma mb mc md me kp mf kq mg ks mh kt mi kv mj kw mk ml bi translated">建立结构化流</h1><figure class="mm mn mo mp gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/87a4e622b21cb246f0ef2637495ee29f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QTlXrpPZ3rhk0vgLFCcUQA.jpeg"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">来自<a class="ae jg" href="https://pixabay.com/?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=5765785" rel="noopener ugc nofollow" target="_blank"> Pixabay </a>的<a class="ae jg" href="https://pixabay.com/users/sonyuser-11407366/?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=5765785" rel="noopener ugc nofollow" target="_blank">约尔格·维耶里</a>的图片</p></figure><p id="1db4" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">结构化流是一个基于Spark SQL引擎的可扩展和容错的流处理引擎。您可以像表达静态数据上的批处理计算一样表达您的流计算。Spark SQL引擎将负责增量地、持续地运行它，并随着流数据不断到达而更新最终结果。</p><p id="97a2" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">Cosmos DB Change Feed的官方文档建议使用Azure函数来接收增量数据。有了Databricks，我们不必再增加一层，因为Spark Streaming是解决这一问题的完美方案。</p><p id="c17a" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">现在，跟着我一步一步地学习这些教程。这并不困难，你可以在10分钟内将你的火花传送到Cosmos DB change feed。</p><h2 id="ba86" class="mq lv jj bd lw mr ms dn ma mt mu dp me lh mv mw mg ll mx my mi lp mz na mk nb bi translated">1.准备连接详细信息</h2><p id="0ac0" class="pw-post-body-paragraph ky kz jj la b lb nc kk ld le nd kn lg lh ne lj lk ll nf ln lo lp ng lr ls lt im bi translated">我们需要拥有URI端点和我们将要读取的来自Cosmos DB的密钥。建议使用只读键，因为我们不会将任何内容写回Cosmos DB。授予最低要求的权限总是一种好的方式。</p><p id="ed16" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这个URL可以在Azure门户的Cosmos DB概述页面上找到。</p><figure class="mm mn mo mp gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nh"><img src="../Images/f655bf3959ce5426906385632ed397df.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rY-C54Fmr8WdnWUOnO8fvg.png"/></div></div></figure><p id="676a" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在同一个页面上，您还需要记下数据库的名称。</p><figure class="mm mn mo mp gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ni"><img src="../Images/a0557835c517c673f74d794287fd4e62.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*p72KgSCmFUPngvf68K4e_A.png"/></div></div></figure><p id="2cc6" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">然后，可以在“密钥”选项卡中找到只读密钥。</p><figure class="mm mn mo mp gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nj"><img src="../Images/0dc63f7ddfa4fcc0c7c993f24480b59f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Xbd_18aO0LujdIb-B7gjkA.png"/></div></div></figure><p id="60c7" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">还强烈建议将这些连接详细信息放入Azure Key Vault，这样我们就不会将它们作为明文代码放在Databricks笔记本中，这将成为一个严重的安全漏洞。</p><h2 id="9248" class="mq lv jj bd lw mr ms dn ma mt mu dp me lh mv mw mg ll mx my mi lp mz na mk nb bi translated">2.安装Cosmos DB Spark 3连接器</h2><p id="df5b" class="pw-post-body-paragraph ky kz jj la b lb nc kk ld le nd kn lg lh ne lj lk ll nf ln lo lp ng lr ls lt im bi translated">在使用连接器之前，我们需要将库安装到集群上。转到Databricks工作区中的“Compute”选项卡，选择要使用的集群。然后，导航到“库”选项卡，并单击“安装新的”。</p><figure class="mm mn mo mp gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nk"><img src="../Images/4b4f960ab838b7c070a45a23eb6848ef.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NMSJLUCG-7mUfhEG1gFKVA.png"/></div></div></figure><p id="63ed" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">之后选择Maven作为库源，输入坐标:</p><pre class="mm mn mo mp gt nl nm nn no aw np bi"><span id="2540" class="mq lv jj nm b gy nq nr l ns nt">com.azure.cosmos.spark:azure-cosmos-spark_3-1_2-12:4.0.0</span></pre><figure class="mm mn mo mp gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nu"><img src="../Images/6171c7cdfe43ff5548574a600f836bc2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1196/format:webp/1*M94gWNID4kOGP8BNgYZZcw.png"/></div></div></figure><p id="446b" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">请注意，根据您阅读我的文章的时间，版本可能会有所不同:)最好从Azure文档中查看最新版本。</p><h2 id="885e" class="mq lv jj bd lw mr ms dn ma mt mu dp me lh mv mw mg ll mx my mi lp mz na mk nb bi translated">3.在Databricks中配置Cosmos DB连接的读取</h2><p id="6553" class="pw-post-body-paragraph ky kz jj la b lb nc kk ld le nd kn lg lh ne lj lk ll nf ln lo lp ng lr ls lt im bi translated">一旦我们将Cosmos DB只读密钥放入Azure Key Vault，我们需要在Databricks工作区中创建一个秘密范围。这不是本文的重点，所以我将跳过它。如果您不知道如何创建Databricks secret作用域，请遵循以下文档。</p><div class="is it gp gr iu nv"><a href="https://docs.microsoft.com/en-us/azure/databricks/security/secrets/secret-scopes" rel="noopener  ugc nofollow" target="_blank"><div class="nw ab fo"><div class="nx ab ny cl cj nz"><h2 class="bd jk gy z fp oa fr fs ob fu fw ji bi translated">秘密范围- Azure数据块-工作区</h2><div class="oc l"><h3 class="bd b gy z fp oa fr fs ob fu fw dk translated">管理机密从创建机密范围开始。机密范围是由名称标识的机密的集合。一个…</h3></div><div class="od l"><p class="bd b dl z fp oa fr fs ob fu fw dk translated">docs.microsoft.com</p></div></div><div class="oe l"><div class="of l og oh oi oe oj ja nv"/></div></div></a></div><p id="a7e8" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在创建Python字典作为变更提要配置对象之前，我们需要从Azure Key Vault中读取连接细节。</p><pre class="mm mn mo mp gt nl nm nn no aw np bi"><span id="c85c" class="mq lv jj nm b gy nq nr l ns nt">cosmosDatabaseName = dbutils.secrets.get('YOUR-KEY-VAUL', 'DB_NAME')<br/>cosmosContainerName = dbutils.widgets.get("container_name")<br/>cosmosEndpoint = dbutils.secrets.get('YOUR-KEY-VAUL', 'URI_ENDPOINT')<br/>cosmosMasterKey = dbutils.secrets.get('YOUR-KEY-VAUL', 'READ-ONLY-KEY')</span></pre><p id="984d" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">是否使用<code class="fe ok ol om nm b">widgets</code>作为容器名称是可选的。在我的例子中，我希望参数化容器名，以便以后可以重用这个笔记本。</p><p id="98a7" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">现在，我们可以构造配置对象了。</p><pre class="mm mn mo mp gt nl nm nn no aw np bi"><span id="e121" class="mq lv jj nm b gy nq nr l ns nt"># Initiate Cosmos Connection Config Object<br/>changeFeedCfg = {<br/>  "spark.cosmos.accountEndpoint": cosmosEndpoint,<br/>  "spark.cosmos.accountKey": cosmosMasterKey,<br/>  "spark.cosmos.database": cosmosDatabaseName,<br/>  "spark.cosmos.container": cosmosContainerName,<br/>  "spark.cosmos.read.partitioning.strategy": "Default",<br/>  "spark.cosmos.read.inferSchema.enabled" : "false",<br/>  "spark.cosmos.changeFeed.startFrom" : "Now",<br/>  "spark.cosmos.changeFeed.mode" : "Incremental"<br/>  "spark.cosmos.changeFeed.maxItemCountPerTriggerHint" : "50000"<br/>}</span></pre><p id="09ab" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">一些配置对象需要一些解释:</p><ul class=""><li id="565f" class="on oo jj la b lb lc le lf lh op ll oq lp or lt os ot ou ov bi translated"><code class="fe ok ol om nm b">spark.cosmos.read.partitioning.strategy</code></li></ul><p id="9ef6" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这是为了定义所使用的分区策略(默认、定制、限制或主动)。通常，我们可以使用默认设置，以便动态计算分区数量，从而保持优化。</p><ul class=""><li id="b157" class="on oo jj la b lb lc le lf lh op ll oq lp or lt os ot ou ov bi translated"><code class="fe ok ol om nm b">spark.cosmos.read.inferSchema.enabled</code></li></ul><p id="609d" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我建议不要启用推断模式，尽管它看起来很聪明。当我们在Cosmos DB中有相对大量的文档时，推断出的模式很有可能是错误的。此外，如果Cosmos容器的模式不一致，几乎肯定会导致模式错误。</p><p id="f61b" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们可以读取原始的JSON字符串，然后使用PySpark对其进行处理。</p><ul class=""><li id="532e" class="on oo jj la b lb lc le lf lh op ll oq lp or lt os ot ou ov bi translated"><code class="fe ok ol om nm b">spark.cosmos.changeFeed.startFrom</code></li></ul><p id="3153" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">此项告诉连接器从哪里开始。通常，我们可以将它设置为“Beginning ”,这样流的第一批将从表的开头开始。</p><p id="422d" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">然而，如果我们从一个巨大的容器中阅读。更聪明的做法是从“现在”开始读取，并在单独的作业中加载历史数据，以提高性能和健壮性。</p><ul class=""><li id="df0d" class="on oo jj la b lb lc le lf lh op ll oq lp or lt os ot ou ov bi translated"><code class="fe ok ol om nm b">spark.cosmos.changeFeed.mode</code></li></ul><p id="c414" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这必须设置为<code class="fe ok ol om nm b">Incremental</code>,因为我们将从更改提要中读取。</p><ul class=""><li id="e037" class="on oo jj la b lb lc le lf lh op ll oq lp or lt os ot ou ov bi translated"><code class="fe ok ol om nm b">spark.cosmos.changeFeed.maxItemCountPerTriggerHint</code></li></ul><p id="0057" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这是可选的。但是，如果您在Cosmos DB端的更新不一致，这将非常有用。例如，有些批次很小，而有些批次很大。该选项将尝试每个触发器处理有限数量的项目，以便当有非常大的一批项目到达时，不会立即破坏您的集群。</p><h2 id="25a3" class="mq lv jj bd lw mr ms dn ma mt mu dp me lh mv mw mg ll mx my mi lp mz na mk nb bi translated">4.配置流数据帧的写入</h2><p id="1ddb" class="pw-post-body-paragraph ky kz jj la b lb nc kk ld le nd kn lg lh ne lj lk ll nf ln lo lp ng lr ls lt im bi translated">为了简化本教程，我们不必为Spark流编写复杂的配置对象。我们唯一需要的是检查站的位置。</p><pre class="mm mn mo mp gt nl nm nn no aw np bi"><span id="49fb" class="mq lv jj nm b gy nq nr l ns nt">writeCfg = {<br/>  "checkpointLocation": "/FileStore/checkpoints/"<br/>}</span></pre><p id="c584" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">检查点非常重要，因为如果我们的Spark流被计划或非计划地停止，它会记住检查点，以便我们可以从那里读取。</p><p id="2630" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">Cosmos Spark连接器将创建一个子目录，这样我们就不必为当前正在读取的特定容器指定名称。</p><h2 id="5880" class="mq lv jj bd lw mr ms dn ma mt mu dp me lh mv mw mg ll mx my mi lp mz na mk nb bi translated">5.创建一个表来接收数据</h2><p id="9b1c" class="pw-post-body-paragraph ky kz jj la b lb nc kk ld le nd kn lg lh ne lj lk ll nf ln lo lp ng lr ls lt im bi translated">我们需要创建一个表，然后才能写入从Cosmos DB Change Feed接收到的增量数据。如果您没有像我一样使用推断模式特性，那么该表应该定义如下。</p><pre class="mm mn mo mp gt nl nm nn no aw np bi"><span id="38f8" class="mq lv jj nm b gy nq nr l ns nt">CREATE TABLE IF NOT EXISTS &lt;schema_name&gt;.&lt;table_name&gt; (<br/>  _rawBody STRING,<br/>  id STRING,<br/>  _ts LONG,<br/>  _etag STRING<br/>)<br/>USING DELTA<br/>LOCATION '&lt;dbfs_location&gt;';</span></pre><p id="cf47" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">如果您使用推断模式，您将需要用相应的列定义表。</p><h2 id="e794" class="mq lv jj bd lw mr ms dn ma mt mu dp me lh mv mw mg ll mx my mi lp mz na mk nb bi translated">6.建立火花流</h2><p id="cefd" class="pw-post-body-paragraph ky kz jj la b lb nc kk ld le nd kn lg lh ne lj lk ll nf ln lo lp ng lr ls lt im bi translated">现在我们可以使用配置对象来建立流。以下代码将建立流，并将其读入Spark数据帧。</p><pre class="mm mn mo mp gt nl nm nn no aw np bi"><span id="f994" class="mq lv jj nm b gy nq nr l ns nt">df = spark.readStream.format("cosmos.oltp.changeFeed").options(**changeFeedCfg).load()</span></pre><p id="fa24" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">您可能想对数据帧进行一些转换。之后，我们可以将它写入刚刚创建的表中。</p><pre class="mm mn mo mp gt nl nm nn no aw np bi"><span id="cbd5" class="mq lv jj nm b gy nq nr l ns nt">df.writeStream.format('delta').outputMode('append').options(**writeCfg).table('&lt;schema_name&gt;.&lt;table_name&gt;')</span></pre><h1 id="5c41" class="lu lv jj bd lw lx ly lz ma mb mc md me kp mf kq mg ks mh kt mi kv mj kw mk ml bi translated">创建数据块作业</h1><figure class="mm mn mo mp gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/2a72d98356b2e4226e931b39bc7b0ca2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AtAqSJIRM_ugreboZLC00g.jpeg"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">图片由<a class="ae jg" href="https://pixabay.com/users/www_slon_pics-5203613/?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=2539844" rel="noopener ugc nofollow" target="_blank"> www_slon_pics </a>发自<a class="ae jg" href="https://pixabay.com/?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=2539844" rel="noopener ugc nofollow" target="_blank"> Pixabay </a></p></figure><p id="4e0d" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们实际上已经完成了笔记本。如果您运行它，它将建立Spark流，并开始接收来自Cosmos DB的任何更改，并将其写入表中。</p><p id="0b23" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">但是，强烈建议将所有内容都放在一个作业中，这样我们就可以使用作业集群，而不是与所有其他活动共享的交互式集群。此外，如果我们将流作为一项工作来运行，它在可靠性方面会更有信心。</p><p id="d158" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">导航到Databricks工作区中的“作业”。输入任务名称，并选择我们刚刚创建的笔记本。</p><figure class="mm mn mo mp gt iv gh gi paragraph-image"><div class="gh gi ow"><img src="../Images/dcdddec6786eaf7b2591086cd3e10545.png" data-original-src="https://miro.medium.com/v2/resize:fit:990/format:webp/1*YhlQ0E5rz_34Zxua8JOV8w.png"/></div></figure><p id="f138" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">不要忘记添加连接器作为依赖库。</p><figure class="mm mn mo mp gt iv gh gi paragraph-image"><div class="gh gi ox"><img src="../Images/22a78097ccc8c44263fc845c1aaa9c30.png" data-original-src="https://miro.medium.com/v2/resize:fit:554/format:webp/1*Hx2z0fmUgUjDZFM13NM_kA.png"/></div></figure><h1 id="6743" class="lu lv jj bd lw lx ly lz ma mb mc md me kp mf kq mg ks mh kt mi kv mj kw mk ml bi translated">解决纷争</h1><figure class="mm mn mo mp gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/5e7ad36e1d07094071d95b139c20cb38.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2mzQ3RyCJXyALrL3JCFAfg.jpeg"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">由<a class="ae jg" href="https://pixabay.com/photos/?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=1209011" rel="noopener ugc nofollow" target="_blank">免费提供的图像-来自<a class="ae jg" href="https://pixabay.com/?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=1209011" rel="noopener ugc nofollow" target="_blank">像素</a>的照片</a></p></figure><p id="6544" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在我们的开发过程中，因为连接器在文档方面相对不成熟，我们发现了以下两个问题。希望如果你也遇到他们，这能帮你节省一些时间。</p><h2 id="cf44" class="mq lv jj bd lw mr ms dn ma mt mu dp me lh mv mw mg ll mx my mi lp mz na mk nb bi translated">1.火花流总是从开始开始</h2><p id="3e6a" class="pw-post-body-paragraph ky kz jj la b lb nc kk ld le nd kn lg lh ne lj lk ll nf ln lo lp ng lr ls lt im bi translated">如果您在cosmos DB中有一个相对较大的容器，您可能希望在当前时间戳开始流传输，并在单独的活动中加载历史数据。</p><p id="d011" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">但是，如果更改源已设置为<code class="fe ok ol om nm b">Beginning</code>，并且检查点未被清除，则流可能无法从“现在”开始，因为检查点“告诉”它:“嘿，您应该从我们之前到达的地方开始！”</p><pre class="mm mn mo mp gt nl nm nn no aw np bi"><span id="eb59" class="mq lv jj nm b gy nq nr l ns nt">"spark.cosmos.changeFeed.startFrom" : "Beginning"<br/># v.s.<br/>"spark.cosmos.changeFeed.startFrom" : "Now"</span></pre><p id="f837" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">解决方案是从DBFS手动删除检查点，或将检查点更改为新路径。然后，论点“现在”将正确地发挥作用。</p><h2 id="3afc" class="mq lv jj bd lw mr ms dn ma mt mu dp me lh mv mw mg ll mx my mi lp mz na mk nb bi translated">2.异常:readtimeout异常</h2><p id="34b2" class="pw-post-body-paragraph ky kz jj la b lb nc kk ld le nd kn lg lh ne lj lk ll nf ln lo lp ng lr ls lt im bi translated">在流运行一定时间后，您可能会看到此异常。</p><pre class="mm mn mo mp gt nl nm nn no aw np bi"><span id="1a6f" class="mq lv jj nm b gy nq nr l ns nt">azure_cosmos_spark.io.netty.handler.timeout.ReadTimeoutException</span></pre><p id="9fd2" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">出现此异常的原因之一可能是群集的当前运行节点太少，因此可用性将成为瓶颈。此外，如果突然有一个大批量进入，一个相对较小的集群将无法按时完成处理。</p><p id="0664" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">因此，解决方案有两个步骤。</p><p id="eaca" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">首先，我们可以将“最低工资”增加到至少2人。如果你真的在处理高速运动，那就给它更多。</p><figure class="mm mn mo mp gt iv gh gi paragraph-image"><div class="gh gi oy"><img src="../Images/624543435a6d2a70adabbceaa0874888.png" data-original-src="https://miro.medium.com/v2/resize:fit:1290/format:webp/1*V1FNFgKN6GYZBT5yuR7KqA.png"/></div></figure><p id="e8fe" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">第二个解决方案是为作业添加重试策略。因此，当作业失败时，它将在一段时间后重新运行。</p><p id="efb0" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们可以在“高级选项”中找到重试策略</p><figure class="mm mn mo mp gt iv gh gi paragraph-image"><div class="gh gi oz"><img src="../Images/4f74e155d0ae597c7aafcf7e9f0fbb72.png" data-original-src="https://miro.medium.com/v2/resize:fit:762/format:webp/1*Lg3hmYb7og_uPrdfjY4NrQ.png"/></div></figure><p id="dd0d" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">然后，由您决定设置重试的次数以及等待重试的时间。</p><figure class="mm mn mo mp gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi pa"><img src="../Images/8337c52711315c2c03361e5997960b3a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IpXViIcvTm14GkOHHID2iQ.png"/></div></div></figure><h1 id="0d1c" class="lu lv jj bd lw lx ly lz ma mb mc md me kp mf kq mg ks mh kt mi kv mj kw mk ml bi translated">摘要</h1><figure class="mm mn mo mp gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/6769c88f255cb90051d05180dfe4d0c7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TSThInINYRg5DJ7avH9h_A.jpeg"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">图片由<a class="ae jg" href="https://pixabay.com/users/startupstockphotos-690514/?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=593333" rel="noopener ugc nofollow" target="_blank"> StartupStockPhotos </a>来自<a class="ae jg" href="https://pixabay.com/?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=593333" rel="noopener ugc nofollow" target="_blank"> Pixabay </a></p></figure><p id="8500" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在本文中，我使用了一个简单的例子来演示如何使用Azure Cosmos DB Spark 3 OLTP连接器建立Spark结构化流来读取Cosmos DB change feed。我相信这是将增量变化从Cosmos DB读入Databricks workspace进行进一步数据分析的最简单方法。</p><div class="is it gp gr iu nv"><a href="https://medium.com/@qiuyujx/membership" rel="noopener follow" target="_blank"><div class="nw ab fo"><div class="nx ab ny cl cj nz"><h2 class="bd jk gy z fp oa fr fs ob fu fw ji bi translated">通过我的推荐链接加入Medium克里斯托弗·陶</h2><div class="oc l"><h3 class="bd b gy z fp oa fr fs ob fu fw dk translated">作为一个媒体会员，你的会员费的一部分会给你阅读的作家，你可以完全接触到每一个故事…</h3></div><div class="od l"><p class="bd b dl z fp oa fr fs ob fu fw dk translated">medium.com</p></div></div><div class="oe l"><div class="pb l og oh oi oe oj ja nv"/></div></div></a></div><p id="1294" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la jk">如果你觉得我的文章有帮助，请考虑加入灵媒会员来支持我和成千上万的其他作家！(点击上面的链接)</strong></p></div></div>    
</body>
</html>