<html>
<head>
<title>Regression Tree in Python From Scratch</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Python中的回归树从头开始</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/regression-tree-in-python-from-scratch-9b7b64c815e3?source=collection_archive---------6-----------------------#2021-04-25">https://towardsdatascience.com/regression-tree-in-python-from-scratch-9b7b64c815e3?source=collection_archive---------6-----------------------#2021-04-25</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="4c55" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">用Python编写流行的回归树算法，并解释其本质</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/c20e8005fc4950d800fba53791063ccf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*v4KK_UlEsxLuIWpgp1dxpg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">回归树的图形；作者模式</p></figure><p id="b2d1" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">本文旨在向读者展示python中回归树算法背后的代码和直觉。我发现浏览一个算法的代码是一个非常好的教育工具，可以帮助我理解在引擎盖下发生了什么。我希望读者也能从中受益。</p><p id="090c" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">解释的算法是回归树算法。它用于模拟连续变量<strong class="la iu"> Y </strong>和一组特征<strong class="la iu"> X: </strong>之间的关系</p><pre class="kj kk kl km gt lu lv lw lx aw ly bi"><span id="a25a" class="lz ma it lv b gy mb mc l md me"><strong class="lv iu">Y = f(X)</strong></span></pre><p id="884d" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">函数<strong class="la iu"> f </strong>是一组特征和特征值的规则，它在给定特征x的情况下做了解释Y变量的“最好”工作</p></div><div class="ab cl mf mg hx mh" role="separator"><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk"/></div><div class="im in io ip iq"><p id="3692" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">本文是上一篇关于决策树的文章的延续:</p><div class="mm mn gp gr mo mp"><a rel="noopener follow" target="_blank" href="/decision-tree-algorithm-in-python-from-scratch-8c43f0e40173"><div class="mq ab fo"><div class="mr ab ms cl cj mt"><h2 class="bd iu gy z fp mu fr fs mv fu fw is bi translated">Python中的决策树算法从零开始</h2><div class="mw l"><h3 class="bd b gy z fp mu fr fs mv fu fw dk translated">用Python编写只使用NumPy和Pandas的流行算法，并解释其中的内容</h3></div><div class="mx l"><p class="bd b dl z fp mu fr fs mv fu fw dk translated">towardsdatascience.com</p></div></div><div class="my l"><div class="mz l na nb nc my nd ks mp"/></div></div></a></div><p id="5679" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我假设读者熟悉节点、分裂和树的层次的概念。所有这些概念在前一篇文章中都有解释。</p><p id="165c" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在上一篇文章中，<strong class="la iu"> Y </strong>变量是一个包含两个值——0和1的二进制变量。</p><p id="4a8d" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">回归树是处理连续响应变量<strong class="la iu"> Y. </strong>的估计器，例如，身高、工资、点击量等。该算法在我的GitHub存储库中编码和实现(以及附带的笔记本):</p><div class="mm mn gp gr mo mp"><a href="https://github.com/Eligijus112/decision-tree-python" rel="noopener  ugc nofollow" target="_blank"><div class="mq ab fo"><div class="mr ab ms cl cj mt"><h2 class="bd iu gy z fp mu fr fs mv fu fw is bi translated">eligijus 112/决策树-python</h2><div class="mw l"><h3 class="bd b gy z fp mu fr fs mv fu fw dk translated">从头开始实现决策树。为eligijus 112/决策树-python开发做出贡献，创建一个…</h3></div><div class="mx l"><p class="bd b dl z fp mu fr fs mv fu fw dk translated">github.com</p></div></div><div class="my l"><div class="ne l na nb nc my nd ks mp"/></div></div></a></div></div><div class="ab cl mf mg hx mh" role="separator"><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk"/></div><div class="im in io ip iq"><p id="f480" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">回归的节点类非常类似于上一篇文章中的二叉分类树类:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nf ng l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">回归节点；作者代码</p></figure><p id="b85a" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">每个节点，无论其级别如何，都具有以下主要属性:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nh"><img src="../Images/1432319afd9e7f2a48f5b20c5b66e793.png" data-original-src="https://miro.medium.com/v2/resize:fit:38/0*8avmmuamPIqZ5BuN"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">落入某个节点的Y变量的平均值</p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/3374e99ee7d4c4a4fe7fe72be287cb51.png" data-original-src="https://miro.medium.com/v2/resize:fit:230/0*I6P3Jp3lMnc6wZ6I"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">残差(真实Y值和平均值之间的差值)</p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nj"><img src="../Images/66d20b55c3443c5cb73b408fed49f185.png" data-original-src="https://miro.medium.com/v2/resize:fit:344/0*rW4gd9g-6BhtL7VG"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">每个节点的均方误差</p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/cf88bb05bdbb420c1160d306ab9fde9a.png" data-original-src="https://miro.medium.com/v2/resize:fit:150/0*wCxzqxlrx8Cp32fM"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">节点的特征和响应变量</p></figure><p id="1fd9" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">上述所有属性都将用于拆分过程。</p></div><div class="ab cl mf mg hx mh" role="separator"><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk"/></div><div class="im in io ip iq"><p id="4a53" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">分割前的特征准备与分类决策树中的相同:</p><p id="9b42" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">对于数据集中的每个数字特征，我们对特征值进行排序，并获取两个相邻值的平均值。</p><p id="c3a1" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">例如，假设我们的特征1如下:</p><pre class="kj kk kl km gt lu lv lw lx aw ly bi"><span id="d7d7" class="lz ma it lv b gy mb mc l md me">feature_1 = [7, 5, 9, 1, 2, 8]</span></pre><p id="3116" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">已排序:</p><pre class="kj kk kl km gt lu lv lw lx aw ly bi"><span id="e984" class="lz ma it lv b gy mb mc l md me">feature_1 = [1, 2, 5, 7, 8, 9]</span></pre><p id="e746" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">邻居的方式:</p><pre class="kj kk kl km gt lu lv lw lx aw ly bi"><span id="b032" class="lz ma it lv b gy mb mc l md me">feature_1 = [1.5, 3.5, 6.5, 7.5, 8.5]</span></pre><p id="99dd" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们对所有的数字特征做同样的事情。</p><p id="68f0" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">对于分类特征，比如</p><pre class="kj kk kl km gt lu lv lw lx aw ly bi"><span id="7257" class="lz ma it lv b gy mb mc l md me">feature_2 = [‘cat’, ‘dog’, ‘dog’, ‘cow’, ‘cat’, ‘cow’] </span></pre><p id="bbdf" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们发现该特性的独特价值:</p><pre class="kj kk kl km gt lu lv lw lx aw ly bi"><span id="f57f" class="lz ma it lv b gy mb mc l md me">feature_2 = [‘cat’, ‘dog’, ‘cow’]</span></pre><p id="0160" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">通过这样的预处理，对于每个特征和每个特征值，我们将节点的数据分成两个子集，左和右:</p><p id="230d" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">对于数字特征(例如，feature_p):</p><pre class="kj kk kl km gt lu lv lw lx aw ly bi"><span id="2f0a" class="lz ma it lv b gy mb mc l md me"><strong class="lv iu">left = X[X[feature_p]&lt;value]</strong></span><span id="0127" class="lz ma it lv b gy nl mc l md me"><strong class="lv iu">right = X[X[feature_p]≥value]</strong></span></pre><p id="66f4" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">对于分类特征(例如，feature_c):</p><pre class="kj kk kl km gt lu lv lw lx aw ly bi"><span id="3f0b" class="lz ma it lv b gy mb mc l md me"><strong class="lv iu">left = X[X[feature_c]==feature_c]</strong></span><span id="fe84" class="lz ma it lv b gy nl mc l md me"><strong class="lv iu">right = X[X[feature_c]!=feature_c]</strong></span></pre><p id="e9c0" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在分类树中，我们需要计算每个数据集(左侧和右侧)中的观察值数量。在回归树中，我们将两个数据集的所有残差放在一起，并计算联合的<strong class="la iu">均方误差</strong>统计量。给出最低<strong class="la iu"> mse </strong>的特征和特征值是分裂标准。</p><p id="8c33" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">例如，让我们考虑两种可能的拆分，<strong class="la iu"> X在4 </strong>和<strong class="la iu"> X在6.5 </strong>:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nm"><img src="../Images/fe163c29c0deb0c3121dd8fa866a9be5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BlVSAtOHtKjd2Hz-iUYjGA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">示例拆分1；按作者分类的图表</p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nn"><img src="../Images/b129e9420c3f2f6c206667a7cf4f556c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GNv94od9uOoHeQKbdxONBA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">示例拆分2；按作者分类的图表</p></figure><p id="8fcd" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">左侧节点的第一个示例中的残差是<strong class="la iu">【3.5，4.5】</strong></p><p id="db3f" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">右节点第一个例子中的残差是<strong class="la iu">【2，0，-2】</strong></p><p id="5e49" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">左节点第二个例子中的残差是<strong class="la iu"> [-0.67，0.33，0.33] </strong></p><p id="e81a" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">右节点的第二个例子中的残差是<strong class="la iu">【1，-1】</strong></p><p id="2391" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">因此，第一次拆分的残差为:</p><p id="c8a4" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu"> r1 = [3.5，4.5，2，0，-2] </strong></p><p id="f17e" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">第二个:</p><p id="6460" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu"> r2 = [-0.67，0.33，0.33，1，-1] </strong></p><p id="5304" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">然后，我们找到两个残差集的mse统计量，并对它们进行比较:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi no"><img src="../Images/9231e5bc973a25e299625c0867e77631.png" data-original-src="https://miro.medium.com/v2/resize:fit:258/0*t749-mEkD89u5PYN"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">第一次分割残差的MSE</p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi np"><img src="../Images/0a88cb717fd047dc475874146d1a0333.png" data-original-src="https://miro.medium.com/v2/resize:fit:284/0*N0ItxBj8cY_odT0N"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">第二次分割残差的MSE</p></figure><p id="bfaa" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">正如我们所见，第二个mse比第一个小得多。因此，在两种可能的分裂中，第二种<strong class="la iu"> (X ≥ 6) </strong>更好。</p></div><div class="ab cl mf mg hx mh" role="separator"><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk"/></div><div class="im in io ip iq"><p id="0937" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">让我们用一个例子来试试定制的NodeRegression类。</p><p id="9d07" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们想要创建一个树<strong class="la iu"> f </strong>,这样:</p><pre class="kj kk kl km gt lu lv lw lx aw ly bi"><span id="2fcd" class="lz ma it lv b gy mb mc l md me"><strong class="lv iu">mpg = f(horsepower, weight)</strong></span></pre><p id="8c4b" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这里</p><p id="e543" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">每加仑汽油跑多少英里</p><p id="0668" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu">马力</strong> —一辆车的马力数；发动机功率</p><p id="f0c3" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu">重量</strong>——一辆汽车的重量，单位为千克</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/29eef199c8b492327e653b332141bd9c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1038/format:webp/1*NRO-7vnEjb98rIB0DGPlSQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">mpg与马力和重量的关系:按作者分类的图表</p></figure><p id="79b0" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">实现与前面的树方法非常相似:</p><pre class="kj kk kl km gt lu lv lw lx aw ly bi"><span id="d76b" class="lz ma it lv b gy mb mc l md me"># Reading data<br/>d = pd.read_csv("data/regression/auto-mpg.csv")</span><span id="056b" class="lz ma it lv b gy nl mc l md me"># Subsetting<br/>d = d[d['horsepower']!='?']</span><span id="937e" class="lz ma it lv b gy nl mc l md me"># Constructing the X and Y matrices<br/>features = ['horsepower', 'weight']</span><span id="b790" class="lz ma it lv b gy nl mc l md me"># Ensuring the correct types <br/>for ft in features:<br/>    d[ft] = pd.to_numeric(d[ft])</span><span id="1c10" class="lz ma it lv b gy nl mc l md me"># Constructing the X and Y matrices<br/>X = d[features]<br/>Y = d['mpg'].values.tolist()</span></pre><p id="e52b" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">节点初始化和树的生长:</p><pre class="kj kk kl km gt lu lv lw lx aw ly bi"><span id="98ec" class="lz ma it lv b gy mb mc l md me"># Initiating the Node<br/>root = NodeRegression(Y, X, max_depth=2, min_samples_split=3)</span><span id="a080" class="lz ma it lv b gy nl mc l md me"># Growing the tree<br/>root.grow_tree()</span></pre><p id="f2e5" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">最后打印出这棵树:</p><pre class="kj kk kl km gt lu lv lw lx aw ly bi"><span id="342e" class="lz ma it lv b gy mb mc l md me"># Printing tree<br/>root.print_tree()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/c882fc398fb7f01ae371dc92927ac0ce.png" data-original-src="https://miro.medium.com/v2/resize:fit:1106/format:webp/1*g_wHUvlPvWBboca5dWGi8Q.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">长成的树；图片来自作者的笔记本</p></figure><p id="3795" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们可以将结果与回归树<a class="ae ns" href="https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html" rel="noopener ugc nofollow" target="_blank">的scikit-learn实现进行比较，https://scikit-learn . org/stable/modules/generated/sk learn . tree . decision tree regressor . html</a>具有相同的超参数:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/78a3c691e15e75aba28bff8df0b30967.png" data-original-src="https://miro.medium.com/v2/resize:fit:604/format:webp/1*3NbK2Ho3HmWRru7lpSkwew.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">Scikit学习实现；图片来自作者的笔记本</p></figure><p id="36ab" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">正如我们所看到的，分割值和特征都是相同的。</p></div><div class="ab cl mf mg hx mh" role="separator"><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk"/></div><div class="im in io ip iq"><p id="37e7" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">最后要注意的是，节点的预测是节点中Y个观测值的平均值。在分类器决策树中，预测是节点中具有最多观察值的类。</p><p id="fda3" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在上面生长的树木中，如果我们遵循规则:</p><pre class="kj kk kl km gt lu lv lw lx aw ly bi"><span id="9430" class="lz ma it lv b gy mb mc l md me"><strong class="lv iu">weight ≤2764.5</strong></span><span id="90f0" class="lz ma it lv b gy nl mc l md me"><strong class="lv iu"> →</strong></span><span id="39cb" class="lz ma it lv b gy nl mc l md me"><strong class="lv iu">horsepower ≤70.5</strong></span></pre><p id="dbd6" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们会得到这个节点有69个观察值。如果我们对这些变量的mpg变量进行平均，我们会得到这些汽车的平均mpg值为33.68。这是对所有未来汽车的预测，它们遵循上述路径。</p></div><div class="ab cl mf mg hx mh" role="separator"><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk"/></div><div class="im in io ip iq"><p id="76ce" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">总之，我强烈推荐仔细阅读<strong class="la iu"> NodeRegressor </strong>类的代码，因为我试图非常明确地编写它。通过从我的GitHub存储库中加载数据并一行一行地检查代码，您很快就会发现回归树是一种非常简单和直观的算法。</p><p id="d7c0" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在我的git存储库中随意派生、复制、克隆代码并创建pull请求，这在第一段中提到过。</p><p id="1509" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">如果任何概念仍然不清楚，请随时留下评论。</p><p id="f76c" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">感谢阅读和快乐编码！</p></div></div>    
</body>
</html>