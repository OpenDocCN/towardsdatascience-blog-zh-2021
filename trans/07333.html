<html>
<head>
<title>PySpark on Google Colab 101</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">谷歌Colab 101上的PySpark</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/pyspark-on-google-colab-101-d31830b238be?source=collection_archive---------8-----------------------#2021-07-04">https://towardsdatascience.com/pyspark-on-google-colab-101-d31830b238be?source=collection_archive---------8-----------------------#2021-07-04</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="c6d6" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">使用Google Colab的PySpark初学者实践指南</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/5c63acf00277c662fb96b2ff7fafea0e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1X0-98EiQNkwBJj2vnTTqQ.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">克里斯里德在<a class="ae kv" href="https://unsplash.com/s/photos/python-code?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="a9e6" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">Apache Spark是一个用于数据处理的闪电般快速的框架，可以在大规模数据集上执行超快速的处理任务。它还可以独立地或与其他分布式计算工具协作，将数据处理任务分布在多个设备上。</p><p id="fc35" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">PySpark是使用Python编程语言访问Spark的接口。PySpark是用python开发的API，用于Spark编程和用Python风格编写spark应用程序，尽管底层执行模型对于所有API语言都是相同的。</p><p id="018d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">Google的Colab是一个基于Jupyter Notebook的非常强大的工具。由于它运行在谷歌服务器上，我们不需要在我们的系统中本地安装任何东西，无论是Spark还是任何深度学习模型。</p><p id="c0f5" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在本文中，我们将看到如何在Google协作笔记本中运行PySpark。我们还将执行大多数数据科学问题中常见的一些基本数据探索任务。所以，让我们开始吧！</p><p id="491d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><em class="ls">注意——我假设你已经熟悉Python、Spark和Google Colab的基础知识。</em></p><h1 id="bbcc" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated">在Colab建立PySpark</h1><p id="7a0e" class="pw-post-body-paragraph kw kx iq ky b kz ml jr lb lc mm ju le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated">Spark是用Scala编程语言编写的，需要Java虚拟机(JVM)才能运行。所以，我们的首要任务是下载Java。</p><pre class="kg kh ki kj gt mq mr ms mt aw mu bi"><span id="6d68" class="mv lu iq mr b gy mw mx l my mz">!apt-get install openjdk-8-jdk-headless -qq &gt; /dev/null</span></pre><p id="635f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">接下来，我们将用<strong class="ky ir"> Hadoop 2.7 </strong>下载并解压<strong class="ky ir"> Apache Spark </strong>进行安装。</p><p id="a4da" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><em class="ls">注意——对于本文，我正在下载Spark的</em><strong class="ky ir"><em class="ls">3 . 1 . 2</em></strong><em class="ls">版本，这是目前最新的稳定版本。如果这一步失败了，那么很可能spark的新版本已经取代了它。所以，上网查查他们的最新版本，然后用那个代替。</em></p><pre class="kg kh ki kj gt mq mr ms mt aw mu bi"><span id="de01" class="mv lu iq mr b gy mw mx l my mz">!wget -q <a class="ae kv" href="https://www-us.apache.org/dist/spark/spark-3.1.1/spark-3.1.1-bin-hadoop2.7.tgz" rel="noopener ugc nofollow" target="_blank">https://www-us.apache.org/dist/spark/spark-3.1.2/spark-3.1.2-bin-hadoop2.7.tgz</a></span><span id="4d91" class="mv lu iq mr b gy na mx l my mz">!tar xf spark-3.1.2-bin-hadoop2.7.tgz</span></pre><p id="6bfd" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在，是时候设置“环境”路径了。</p><pre class="kg kh ki kj gt mq mr ms mt aw mu bi"><span id="4ca4" class="mv lu iq mr b gy mw mx l my mz">import os<br/>os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-8-openjdk-amd64"<br/>os.environ["SPARK_HOME"] = "/content/spark-3.1.2-bin-hadoop2.7"</span></pre><p id="31b2" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">然后我们需要安装并导入'<a class="ae kv" href="https://pypi.org/project/findspark/" rel="noopener ugc nofollow" target="_blank"><strong class="ky ir">find spark</strong></a><strong class="ky ir">'</strong>库，它将在系统上定位Spark并将其作为常规库导入。</p><pre class="kg kh ki kj gt mq mr ms mt aw mu bi"><span id="61eb" class="mv lu iq mr b gy mw mx l my mz">!pip install -q findspark</span><span id="b822" class="mv lu iq mr b gy na mx l my mz">import findspark<br/>findspark.init()</span></pre><p id="9f8d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在，我们可以从pyspark.sql导入sparkSession并创建一个SparkSession，这是Spark的入口点。</p><pre class="kg kh ki kj gt mq mr ms mt aw mu bi"><span id="2c75" class="mv lu iq mr b gy mw mx l my mz">from pyspark.sql import SparkSession<br/>spark = SparkSession.builder\<br/>        .master("local")\<br/>        .appName("Colab")\<br/>        .config('spark.ui.port', '4050')\<br/>        .getOrCreate()</span></pre><p id="dd22" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">就是这样！现在让我们从PySpark开始吧！</p></div><div class="ab cl nb nc hu nd" role="separator"><span class="ne bw bk nf ng nh"/><span class="ne bw bk nf ng nh"/><span class="ne bw bk nf ng"/></div><div class="ij ik il im in"><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ni"><img src="../Images/2c22689f41cbb9aa445a747bd807520b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tz7GMzuomveJwAjdjq88bQ.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">迈克·范·登博斯在<a class="ae kv" href="https://unsplash.com/s/photos/loading?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><h1 id="516a" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated">将数据加载到PySpark</h1><p id="bcdf" class="pw-post-body-paragraph kw kx iq ky b kz ml jr lb lc mm ju le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated">Spark有多种模块可以读取不同格式的数据。它还自动确定每一列的数据类型，但是它必须检查一次。</p><p id="b6a1" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">对于本文，我在Github中创建了一个样本JSON数据集。您可以使用“<strong class="ky ir">wget”</strong>命令将文件直接下载到Colab中，如下所示:</p><pre class="kg kh ki kj gt mq mr ms mt aw mu bi"><span id="315c" class="mv lu iq mr b gy mw mx l my mz">!wget --continue https://raw.githubusercontent.com/GarvitArya/pyspark-demo/main/sample_books.json -O /tmp/sample_books.json</span></pre><p id="6415" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在使用<strong class="ky ir">读取</strong>模块将该文件读入Spark数据帧。</p><pre class="kg kh ki kj gt mq mr ms mt aw mu bi"><span id="dd63" class="mv lu iq mr b gy mw mx l my mz">df = spark.read.json("/tmp/sample_books.json")</span></pre><p id="02e5" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在是时候使用PySpark dataframe函数来研究我们的数据了。</p><h1 id="11fa" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated"><strong class="ak">使用PySpark进行探索性</strong>数据分析</h1><h2 id="306d" class="mv lu iq bd lv nj nk dn lz nl nm dp md lf nn no mf lj np nq mh ln nr ns mj nt bi translated">让我们来看看它的模式:</h2><p id="8045" class="pw-post-body-paragraph kw kx iq ky b kz ml jr lb lc mm ju le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated">在对数据集进行任何切片之前，我们应该首先了解它的所有列及其数据类型。</p><pre class="kg kh ki kj gt mq mr ms mt aw mu bi"><span id="87cf" class="mv lu iq mr b gy mw mx l my mz">df.printSchema()</span><span id="9601" class="mv lu iq mr b gy na mx l my mz"><strong class="mr ir">Sample Output:<br/></strong>root  <br/>  |-- author: string (nullable = true)  <br/>  |-- edition: string (nullable = true)  <br/>  |-- price: double (nullable = true)  <br/>  |-- title: string (nullable = true)  <br/>  |-- year_written: long (nullable = true)</span></pre><h2 id="4ded" class="mv lu iq bd lv nj nk dn lz nl nm dp md lf nn no mf lj np nq mh ln nr ns mj nt bi translated">给我看一些样品:</h2><pre class="kg kh ki kj gt mq mr ms mt aw mu bi"><span id="8474" class="mv lu iq mr b gy mw mx l my mz">df.show(4,False)</span><span id="5dd0" class="mv lu iq mr b gy na mx l my mz"><strong class="mr ir">Sample Output:<br/></strong>+----------------+---------------+-----+--------------+------------+<br/>|author          |edition        |price|title         |year_written|<br/>+----------------+---------------+-----+--------------+------------+<br/>|Tolstoy, Leo    |Penguin        |12.7 |War and Peace |1865        | <br/>|Tolstoy, Leo    |Penguin        |13.5 |Anna Karenina |1875        | <br/>|Woolf, Virginia |Harcourt Brace |25.0 |Mrs. Dalloway |1925        |<br/>|Dickens, Charles|Random House   |5.75 |Bleak House   |1870        |<br/>+----------------+---------------+-----+--------------+------------+</span></pre><h2 id="ab78" class="mv lu iq bd lv nj nk dn lz nl nm dp md lf nn no mf lj np nq mh ln nr ns mj nt bi translated">数据集有多大:</h2><pre class="kg kh ki kj gt mq mr ms mt aw mu bi"><span id="9f3d" class="mv lu iq mr b gy mw mx l my mz">df.count()</span><span id="27bf" class="mv lu iq mr b gy na mx l my mz"><strong class="mr ir">Sample Output:<br/></strong>13</span></pre><h2 id="ec92" class="mv lu iq bd lv nj nk dn lz nl nm dp md lf nn no mf lj np nq mh ln nr ns mj nt bi translated">选择几个感兴趣的列:</h2><pre class="kg kh ki kj gt mq mr ms mt aw mu bi"><span id="fb27" class="mv lu iq mr b gy mw mx l my mz">df.select(“title”, “price”, “year_written”).show(5)</span><span id="0ae4" class="mv lu iq mr b gy na mx l my mz"><strong class="mr ir">Sample Output:</strong><br/>+----------------+-----+------------+ <br/>|           title|price|year_written| <br/>+----------------+-----+------------+ <br/>|Northanger Abbey| 18.2|        1814| <br/>|   War and Peace| 12.7|        1865| <br/>|   Anna Karenina| 13.5|        1875| <br/>|   Mrs. Dalloway| 25.0|        1925| <br/>|       The Hours|12.35|        1999| <br/>+----------------+-----+------------+ </span></pre><h2 id="d62f" class="mv lu iq bd lv nj nk dn lz nl nm dp md lf nn no mf lj np nq mh ln nr ns mj nt bi translated">过滤数据集:</h2><pre class="kg kh ki kj gt mq mr ms mt aw mu bi"><span id="8f52" class="mv lu iq mr b gy mw mx l my mz"><strong class="mr ir"># Get books that are written after 1950 &amp; cost greater than $10</strong></span><span id="3661" class="mv lu iq mr b gy na mx l my mz">df_filtered = df.filter("year_written &gt; 1950 AND price &gt; 10 AND title IS NOT NULL")</span><span id="3c04" class="mv lu iq mr b gy na mx l my mz">df_filtered.select("title", "price", "year_written").show(50, False)</span><span id="848d" class="mv lu iq mr b gy na mx l my mz"><strong class="mr ir">Sample Output:<br/></strong>+-----------------------------+-----+------------+ <br/>|title                        |price|year_written| <br/>+-----------------------------+-----+------------+ <br/>|The Hours                    |12.35|1999        | <br/>|Harry Potter                 |19.95|2000        | <br/>|One Hundred Years of Solitude|14.0 |1967        | <br/>+-----------------------------+-----+------------+</span><span id="cfe6" class="mv lu iq mr b gy na mx l my mz"><strong class="mr ir"># Get books that have Harry Porter in their title</strong></span><span id="4de8" class="mv lu iq mr b gy na mx l my mz">df_filtered.select("title", "year_written").filter("title LIKE '%Harry Potter%'").distinct().show(20, False)</span><span id="4595" class="mv lu iq mr b gy na mx l my mz"><strong class="mr ir">Sample Output:<br/></strong>+------------+------------+ <br/>|title       |year_written| <br/>+------------+------------+ <br/>|Harry Potter|2000        | <br/>+------------+------------+</span></pre><h2 id="5b04" class="mv lu iq bd lv nj nk dn lz nl nm dp md lf nn no mf lj np nq mh ln nr ns mj nt bi translated">使用Pyspark SQL函数:</h2><pre class="kg kh ki kj gt mq mr ms mt aw mu bi"><span id="9d9e" class="mv lu iq mr b gy mw mx l my mz">from pyspark.sql.functions import max</span><span id="72c3" class="mv lu iq mr b gy na mx l my mz"><strong class="mr ir"># Find the costliest book<br/></strong>maxValue = df_filtered.agg(max("price")).collect()[0][0]<br/>print("maxValue: ",maxValue)</span><span id="030a" class="mv lu iq mr b gy na mx l my mz">df_filtered.select("title","price").filter(df.price == maxValue).show(20, False)</span><span id="27d3" class="mv lu iq mr b gy na mx l my mz"><strong class="mr ir">Sample Output:</strong><br/>maxValue:  29.0 <br/>+-----------------------------+------+ <br/>|title                        |price | <br/>+-----------------------------+------+ <br/>|A Room of One's Own          |29.0  | <br/>+-----------------------------+------+</span></pre></div><div class="ab cl nb nc hu nd" role="separator"><span class="ne bw bk nf ng nh"/><span class="ne bw bk nf ng nh"/><span class="ne bw bk nf ng"/></div><div class="ij ik il im in"><h1 id="3651" class="lt lu iq bd lv lw nu ly lz ma nv mc md jw nw jx mf jz nx ka mh kc ny kd mj mk bi translated">结束注释</h1><p id="f8e5" class="pw-post-body-paragraph kw kx iq ky b kz ml jr lb lc mm ju le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated">我希望你和我写这篇文章时一样喜欢在Colab与PySpark一起工作！您可以在我的Github资源库-<a class="ae kv" href="https://github.com/GarvitArya/pyspark-demo" rel="noopener ugc nofollow" target="_blank">https://github.com/GarvitArya/pyspark-demo</a>找到这个完整的工作示例Colab文件。</p><p id="8253" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">▶️ <em class="ls">请在下面的评论中提出任何问题/疑问或分享任何建议。</em></p><p id="38a8" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">▶️ <em class="ls">如果你喜欢这篇文章，那么请考虑关注我&amp;把它也分享给你的朋友:)</em></p><p id="4dc8" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">▶️ <em class="ls">可以联系我在—</em><a class="ae kv" href="https://www.linkedin.com/in/garvitarya/" rel="noopener ugc nofollow" target="_blank"><em class="ls">LinkedIn</em></a><em class="ls">|</em><a class="ae kv" href="https://twitter.com/garvitishere" rel="noopener ugc nofollow" target="_blank"><em class="ls">Twitter</em></a><em class="ls">|</em><a class="ae kv" href="https://github.com/GarvitArya/" rel="noopener ugc nofollow" target="_blank"><em class="ls">github</em></a><em class="ls">|</em><a class="ae kv" href="https://www.instagram.com/garvitarya/" rel="noopener ugc nofollow" target="_blank"><em class="ls">insta gram</em></a><em class="ls">|</em><a class="ae kv" href="https://www.facebook.com/garvitishere" rel="noopener ugc nofollow" target="_blank"><em class="ls">Facebook</em></a><em class="ls"/></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nz"><img src="../Images/26eb9950ccaeafba69e8f996cfb2dedb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cnuTrXFhcpC3GgDS8RxFjw.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">在<a class="ae kv" href="https://unsplash.com/s/photos/thank-you?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上由<a class="ae kv" href="https://unsplash.com/@cmhedger?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">Courtney hedge</a>拍摄的照片</p></figure></div></div>    
</body>
</html>