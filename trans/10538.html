<html>
<head>
<title>How to Train Bert For Q&amp;A in Any Language</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如何训练Bert进行任何语言的问答</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/how-to-train-bert-for-q-a-in-any-language-63b62c780014?source=collection_archive---------2-----------------------#2021-10-08">https://towardsdatascience.com/how-to-train-bert-for-q-a-in-any-language-63b62c780014?source=collection_archive---------2-----------------------#2021-10-08</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="5067" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">从零开始的多语言问答简单指南</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/4299d3d50088960733164a492d2d56ee.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*MZZZZlMVGgzoOZ4p"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">杰里米·贝赞格在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="8658" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">问答(Q&amp;A)变压器应用广泛，是现代自然语言处理的非常酷的应用。</p><p id="b0ae" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">乍一看，我们大多数人会认为建造这样的东西是一个非常困难的壮举。幸运的是，我们大多数人都错了。</p><p id="818a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">尽管变形金刚有着令人难以置信的性能，但训练或微调它们来完成特定任务却异常简单。</p><p id="04bf" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">再加上网上可用的许多<em class="lv">大规模</em>语言数据集，我们就拥有了构建一些令人难以置信的工具所需的一切。</p><p id="a96d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们将解释从头开始训练Bert transformer模型<em class="lv">所需的关键步骤</em>，以及我们如何针对Q &amp; A微调该模型。这里的一个关键因素是数据集，我们也将在整篇文章中概述它。</p></div><div class="ab cl lw lx hx ly" role="separator"><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb"/></div><div class="im in io ip iq"><h1 id="902f" class="md me it bd mf mg mh mi mj mk ml mm mn jz mo ka mp kc mq kd mr kf ms kg mt mu bi translated">从零开始问答</h1><p id="8dd7" class="pw-post-body-paragraph kz la it lb b lc mv ju le lf mw jx lh li mx lk ll lm my lo lp lq mz ls lt lu im bi translated">第一，变形金刚训练流程是什么样子的？虽然这些模型非常强大，但训练过程却出奇的简单。</p><p id="add9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们首先需要明白的是，有一个<em class="lv">核心</em> Bert模型。这个核心模型充当了中央转换引擎——正是这个引擎构建了对语言的理解。</p><p id="3e1d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这个中央模块的输出本身并不是特别有用。它只是一个语言理解引擎，它不能告诉你一个句子是快乐还是悲伤，识别实体，或者回答我们的问题。</p><p id="0ba1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对于所有这些任务以及更多的任务，我们需要一个训练有素的变压器头。</p><p id="c4a3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">变压器的头部连接到核心引擎。核心引擎消耗一些文本，并将其处理成有意义的数字向量。这些向量是我们变压器头的丰富信息输入。</p><p id="0c32" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">每个transformer头的结构都是为特定任务而构建的，许多只不过是由几个前馈神经网络层组成，其组织方式是，一旦训练了头，我们就可以产生有意义的功能。</p><p id="ce80" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">因此，要从头构建问答Bert模型，我们必须:</p><ul class=""><li id="3472" class="na nb it lb b lc ld lf lg li nc lm nd lq ne lu nf ng nh ni bi translated">训练一个核心Bert引擎。</li><li id="5f91" class="na nb it lb b lc nj lf nk li nl lm nm lq nn lu nf ng nh ni bi translated">培养一个问答Bert问答头。</li></ul><p id="33a3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">有时可能没有使用您的语言的现有Bert模型。如果是这种情况，您还必须训练一个Bert单词片段标记器。我已经在这里<a class="ae ky" rel="noopener" target="_blank" href="/how-to-build-a-wordpiece-tokenizer-for-bert-f505d97dddbb">写了这个</a>。</p><p id="6421" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为了简洁起见，这里我们将只关注两个模型训练任务。让我们进入核心的Bert训练过程。</p></div><div class="ab cl lw lx hx ly" role="separator"><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb"/></div><div class="im in io ip iq"><h1 id="83d0" class="md me it bd mf mg mh mi mj mk ml mm mn jz mo ka mp kc mq kd mr kf ms kg mt mu bi translated">训练核心</h1><p id="e469" class="pw-post-body-paragraph kz la it lb b lc mv ju le lf mw jx lh li mx lk ll lm my lo lp lq mz ls lt lu im bi translated">伯特最初是用两个并行的过程训练的。掩蔽语言建模(MLM)和下一句预测(NSP)。两者都需要大量的训练数据，这些数据必须是非结构化文本的形式。</p><p id="9e2c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">幸运的是，如果说我们在互联网上有很多东西的话，那就是非结构化文本。</p><h2 id="208d" class="no me it bd mf np nq dn mj nr ns dp mn li nt nu mp lm nv nw mr lq nx ny mt nz bi translated">MLM和NSP的数据集</h2><p id="cb15" class="pw-post-body-paragraph kz la it lb b lc mv ju le lf mw jx lh li mx lk ll lm my lo lp lq mz ls lt lu im bi translated">对于MLM和NSP，我们只需要一个大型的非结构化文本数据集。在多语言非结构化文本方面，没有比OSCAR语料库更好的来源了。</p><p id="c77f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">OSCAR目前涵盖166种语言，从英语到纳瓦特尔语。<a class="ae ky" href="https://oscar-corpus.com" rel="noopener ugc nofollow" target="_blank">完整列表可在此处找到</a>。</p><p id="d8b2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">从OSCAR下载数据最简单的方法是通过HuggingFace的<code class="fe oa ob oc od b">datasets</code>库。我个人会100%推荐采取这种方式。我们可以非常快速地下载——例如——奥斯卡的英文子集，如下所示:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oe of l"/></div></figure><p id="8f99" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">还有大量其他可用的数据集，我们可以在<a class="ae ky" href="https://huggingface.co/datasets/viewer/" rel="noopener ugc nofollow" target="_blank"> HuggingFace的数据集查看器</a>上找到。</p><p id="32f7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">当下载较小的数据集时，我们可以毫无问题地使用上面的代码——但是较大的数据集(特别是来自OSCAR的数据集)可能会非常大——OSCAR的意大利子集有69GB的数据，而英国只有1.8TB。</p><p id="4ce0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在这种情况下，我们可以通过将<code class="fe oa ob oc od b">streaming=True</code>参数添加到<code class="fe oa ob oc od b">load_datasets</code>来<em class="lv">流式传输</em>数据。</p><p id="2aec" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然后，要么用<code class="fe oa ob oc od b">streaming=True</code>构建完整的输入管道，要么只遍历数据集的较小部分并将它们存储在内存中(通常<code class="fe oa ob oc od b">streaming=True</code>会导致代码问题，在这种情况下，后一种选择是最好的)。</p><p id="ea51" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果你需要通过HF数据集下载数据的细节方面的帮助，我会在这段视频中一一介绍:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="og of l"/></div></figure><p id="3d19" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">一旦数据下载完毕，我们就开始用MLM ( <em class="lv">和</em> NSP，如果愿意的话)训练核心模型。</p><h2 id="026b" class="no me it bd mf np nq dn mj nr ns dp mn li nt nu mp lm nv nw mr lq nx ny mt nz bi translated">屏蔽语言建模</h2><p id="75b8" class="pw-post-body-paragraph kz la it lb b lc mv ju le lf mw jx lh li mx lk ll lm my lo lp lq mz ls lt lu im bi translated">MLM包括向伯特提供一个句子，其中几个单词(或<em class="lv">记号</em>)已经使用<em class="lv">掩码</em>记号隐藏起来。这个掩码标记无非是隐藏原始单词。</p><pre class="kj kk kl km gt oh od oi oj aw ok bi"><span id="d55d" class="no me it od b gy ol om l on oo"><strong class="od iu">ORIGINAL</strong>: "flying fish flew by the space station"</span><span id="63cd" class="no me it od b gy op om l on oo"><strong class="od iu">MASKED</strong>: "<em class="lv">[MASK]</em> fish flew by the <em class="lv">[MASK]</em> station"</span></pre><p id="6308" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然后，原始句子作为核心Bert模型的目标输出传递。通过这样做，Bert被迫阅读句子中的其他单词，并试图理解上下文以正确预测被屏蔽的单词。</p><h2 id="7370" class="no me it bd mf np nq dn mj nr ns dp mn li nt nu mp lm nv nw mr lq nx ny mt nz bi translated">下一句预测</h2><p id="3e8f" class="pw-post-body-paragraph kz la it lb b lc mv ju le lf mw jx lh li mx lk ll lm my lo lp lq mz ls lt lu im bi translated">除了MLM之外，还发现一种叫做NSP的额外训练过程提高了Bert在下游任务(那些需要特定变压器头的任务)中的性能。</p><p id="e3d5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">当Bert试图猜测掩码标记背后的真实单词时，它还负责识别作为输入提供的两个句子是否属于一起。</p><p id="3fc1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这意味着，给定两个句子——<code class="fe oa ob oc od b">A</code>和<code class="fe oa ob oc od b">B</code>。句子<code class="fe oa ob oc od b">B</code>是句子<code class="fe oa ob oc od b">A</code>的逻辑延续吗？</p><pre class="kj kk kl km gt oh od oi oj aw ok bi"><span id="7110" class="no me it od b gy ol om l on oo"><strong class="od iu">Logical continuation (<em class="lv">IsNext</em>)</strong><br/>A: "Alia went to the shop to buy pasta"<br/>B: "The shop was closed"</span><span id="a7ff" class="no me it od b gy op om l on oo"><strong class="od iu">Not logical continuation (NotNext)</strong><br/>A: "Alia went to the shop to buy pasta"<br/>B: "The armadillo was not very happy with the lion"</span></pre><p id="ce93" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">MLM和NSP是否都被用于训练核心的伯特模型——或者仅仅是MLM，取决于你的偏好。只训练MLM更简单，通常能达到MLM <em class="lv">和</em> NSP可能表现的90%左右。</p><p id="659e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在下面的视频中，我们介绍了MLM的训练过程(如果你想用MLM <em class="lv">和</em> NSP，<a class="ae ky" href="https://youtu.be/IC9FaVPKlYc" rel="noopener ugc nofollow" target="_blank">在这里找到</a>)。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="og of l"/></div></figure></div><div class="ab cl lw lx hx ly" role="separator"><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb"/></div><div class="im in io ip iq"><h1 id="fcf7" class="md me it bd mf mg mh mi mj mk ml mm mn jz mo ka mp kc mq kd mr kf ms kg mt mu bi translated">问答负责人</h1><p id="d4cd" class="pw-post-body-paragraph kz la it lb b lc mv ju le lf mw jx lh li mx lk ll lm my lo lp lq mz ls lt lu im bi translated">我们有一个经过全面训练的<em class="lv">核心</em> Bert模型，我们可以采用该核心并添加几个<em class="lv">头部</em>，从而允许我们使用该模型执行不同的任务。但是，这些负责人最初是未经培训的，因此我们必须对他们进行培训！</p><p id="8048" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果您喜欢视频，我们在这里也涵盖一切:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="og of l"/></div></figure><p id="604b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对于问答，最受欢迎的数据集是斯坦福问答数据集(SQuAD)。除了最初的英文版本，现在还有其他几种语言版本。</p><pre class="kj kk kl km gt oh od oi oj aw ok bi"><span id="5c5a" class="no me it od b gy ol om l on oo"><strong class="od iu">Language options <em class="lv">(on HF datasets at the time of writing)</em></strong></span><span id="cc5a" class="no me it od b gy op om l on oo">Spanish: <strong class="od iu">squad_es<br/></strong>Portuguese: <strong class="od iu">squad_v1_pt<br/></strong>Italian: <strong class="od iu">squad_it<br/></strong>Korean: [<strong class="od iu">squad_kor_v1</strong>, <strong class="od iu">squad_kor_v2</strong>]<strong class="od iu"><br/></strong>Thai: [<strong class="od iu">iapp_wiki_qa_squad</strong>, <strong class="od iu">thaiqa_squad</strong>]<strong class="od iu"><br/></strong>English: [<strong class="od iu">squad</strong>, <strong class="od iu">squad_v2</strong>, <strong class="od iu">squadshifts</strong>, <strong class="od iu">squad_adversarial</strong>]</span></pre><p id="db5f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为了下载英国队的数据，我们使用:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oe of l"/></div></figure><pre class="kj kk kl km gt oh od oi oj aw ok bi"><span id="91e3" class="no me it od b gy ol om l on oo"><strong class="od iu">Language options <em class="lv">(at the time of writing)</em></strong></span><span id="2b5f" class="no me it od b gy op om l on oo">Spanish: <strong class="od iu">squad_es<br/></strong>Portuguese: <strong class="od iu">squad_v1_pt<br/></strong>Italian: <strong class="od iu">squad_it<br/></strong>Korean: [<strong class="od iu">squad_kor_v1</strong>, <strong class="od iu">squad_kor_v2</strong>]<strong class="od iu"><br/></strong>Thai: [<strong class="od iu">iapp_wiki_qa_squad</strong>, <strong class="od iu">thaiqa_squad</strong>]<strong class="od iu"><br/></strong>English: [<strong class="od iu">squad</strong>, <strong class="od iu">squad_v2</strong>, <strong class="od iu">squadshifts</strong>, <strong class="od iu">squad_adversarial</strong>]</span></pre><p id="35fb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对于每个样本，我们的数据可以分为三个部分:</p><ul class=""><li id="a757" class="na nb it lb b lc ld lf lg li nc lm nd lq ne lu nf ng nh ni bi translated"><strong class="lb iu">问题</strong> —包含我们将向Bert提出的问题的字符串。</li><li id="6505" class="na nb it lb b lc nj lf nk li nl lm nm lq nn lu nf ng nh ni bi translated"><strong class="lb iu">上下文</strong> —包含我们问题答案的更大的序列(段落)。</li><li id="8d35" class="na nb it lb b lc nj lf nk li nl lm nm lq nn lu nf ng nh ni bi translated"><strong class="lb iu">回答</strong> —回答我们问题的一段上下文。</li></ul><p id="bc0f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">给定一个<em class="lv">问题</em>和<em class="lv">上下文</em>，我们的Q &amp; A模型必须读取两者并返回上下文中预测答案的标记位置。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oq"><img src="../Images/b90edce8fcb4fde79fb46364f075cab0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4tZcrwaIIMBGLR1rDUmXFw.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">我们的问题、上下文和答案输入的示例，以及来自模型的(希望的)正确预测。请注意，这些跨度值并不精确，而是假设一个单词=一个标记。</p></figure><h2 id="4ab7" class="no me it bd mf np nq dn mj nr ns dp mn li nt nu mp lm nv nw mr lq nx ny mt nz bi translated">格式化答案</h2><p id="2383" class="pw-post-body-paragraph kz la it lb b lc mv ju le lf mw jx lh li mx lk ll lm my lo lp lq mz ls lt lu im bi translated">在我们开始标记化、训练等之前，我们需要将我们的<code class="fe oa ob oc od b">answers</code>特性重新格式化为正确的训练格式。目前看起来像是:</p><pre class="kj kk kl km gt oh od oi oj aw ok bi"><span id="91ed" class="no me it od b gy ol om l on oo">{'text': ['the answer is here'], 'answer_start': [71]}</span></pre><p id="5642" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><code class="fe oa ob oc od b">71</code>代表我们的<code class="fe oa ob oc od b">context</code>字符串中答案开始的字符位置。我们可以通过简单地将<code class="fe oa ob oc od b">text</code>的长度加到<code class="fe oa ob oc od b">answer_start</code>来得到答案范围，我们首先这样做:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oe of l"/></div></figure><p id="e0d2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">数据格式现在可以进行标记化了。</p><h2 id="654c" class="no me it bd mf np nq dn mj nr ns dp mn li nt nu mp lm nv nw mr lq nx ny mt nz bi translated">标记化</h2><p id="c715" class="pw-post-body-paragraph kz la it lb b lc mv ju le lf mw jx lh li mx lk ll lm my lo lp lq mz ls lt lu im bi translated">我们需要对阵容数据进行记号化，以便它可以被我们的Bert模型读取。对于<code class="fe oa ob oc od b">context</code>和<code class="fe oa ob oc od b">question</code>特征，我们可以使用标准的<code class="fe oa ob oc od b">tokenizer()</code>函数:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oe of l"/></div></figure><p id="0f7f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">它将我们的<code class="fe oa ob oc od b">context</code>和<code class="fe oa ob oc od b">question</code>字符串编码成单个令牌数组。这将作为我们Q &amp; A培训的输入，但是我们还没有目标。</p><p id="8f5a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们的目标是答案的开始和结束位置，这是我们之前使用<code class="fe oa ob oc od b">context</code>字符串中的<em class="lv">字符</em>开始和结束位置构建的。然而，我们将把<em class="lv">令牌</em>输入到Bert中，所以我们需要提供<em class="lv">令牌</em>的开始和结束位置。</p><p id="ec61" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为此，我们需要将字符的开始和结束位置转换成记号的开始和结束位置——使用我们的<code class="fe oa ob oc od b">add_token_positions</code>函数很容易做到:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oe of l"/></div></figure><p id="ccaa" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这个函数给我们的<code class="fe oa ob oc od b">Encoding</code>对象增加了两个张量(我们将它们输入到Bert中)—<code class="fe oa ob oc od b">start_positions</code>和<code class="fe oa ob oc od b">end_positions</code>。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oe of l"/></div></figure><p id="11e3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们的张量现在准备好训练Bert Q&amp;A头了。</p><h2 id="8989" class="no me it bd mf np nq dn mj nr ns dp mn li nt nu mp lm nv nw mr lq nx ny mt nz bi translated">培养</h2><p id="6cd5" class="pw-post-body-paragraph kz la it lb b lc mv ju le lf mw jx lh li mx lk ll lm my lo lp lq mz ls lt lu im bi translated">我们将使用PyTorch进行训练，这意味着我们需要将我们构建的张量转换成PyTorch <code class="fe oa ob oc od b">Dataset</code>对象。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oe of l"/></div></figure><p id="e789" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们将使用一个<code class="fe oa ob oc od b">Dataloader</code>对象把我们的<code class="fe oa ob oc od b">Dataset</code>输入到我们的Q &amp; A训练循环中，我们用:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oe of l"/></div></figure><p id="acdc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">最后，我们设置模型参数并开始训练循环。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oe of l"/></div></figure><p id="474f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">训练完我们的模型后，我们需要做的就是保存它！</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oe of l"/></div></figure><p id="3231" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们完成了，我们从零开始训练了一个问答模型。</p></div><div class="ab cl lw lx hx ly" role="separator"><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb"/></div><div class="im in io ip iq"><p id="31de" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这就是本文的全部内容，我们已经介绍了训练用于问答的Bert transformer模型的核心和头部背后的要点，并探索了一些我们可以用于这两者的最佳数据集。</p><p id="0ba9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我希望你喜欢它！如果你有任何问题，请通过<a class="ae ky" href="https://twitter.com/jamescalam" rel="noopener ugc nofollow" target="_blank"> Twitter </a>或在下面的评论中告诉我。如果你想要更多这样的内容，我也会在<a class="ae ky" href="https://www.youtube.com/c/jamesbriggs" rel="noopener ugc nofollow" target="_blank"> YouTube </a>上发布。</p><p id="3b34" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">感谢阅读！</p></div><div class="ab cl lw lx hx ly" role="separator"><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb"/></div><div class="im in io ip iq"><p id="3557" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><a class="ae ky" href="https://bit.ly/nlp-transformers" rel="noopener ugc nofollow" target="_blank">🤖《变形金刚》课程NLP的70%折扣</a></p></div><div class="ab cl lw lx hx ly" role="separator"><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb"/></div><div class="im in io ip iq"><p id="cece" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="lv">*所有图片均由作者提供，除非另有说明</em></p></div></div>    
</body>
</html>