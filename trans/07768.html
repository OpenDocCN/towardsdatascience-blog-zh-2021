<html>
<head>
<title>Frequentist vs Bayesian Statistics</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">常客vs贝叶斯统计</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/frequentist-vs-bayesian-statistics-54a197db21?source=collection_archive---------6-----------------------#2021-07-16">https://towardsdatascience.com/frequentist-vs-bayesian-statistics-54a197db21?source=collection_archive---------6-----------------------#2021-07-16</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="9fc6" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">用Python进行参数估计的实用介绍</h2></div><h2 id="3829" class="kf kg iq bd kh ki kj dn kk kl km dp kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">参数估计是统计推断、数据科学和机器学习工作流的关键组成部分。尽管这个主题可能很复杂，但我们还是用一些理论和代码介绍了比较两种方法的过程。</h2><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi lb"><img src="../Images/ce35b87ffe0ddfb033a6f48a39a07063.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lBLfHa5oku4WxyiWktSUSA.jpeg"/></div></div><p class="ln lo gj gh gi lp lq bd b be z dk translated">演职员表:<a class="ae lr" href="https://unsplash.com/@cgbriggs19" rel="noopener ugc nofollow" target="_blank">https://unsplash.com/@cgbriggs19</a></p></figure><p id="f253" class="pw-post-body-paragraph ls lt iq lu b lv lw jr lx ly lz ju ma ko mb mc md ks me mf mg kw mh mi mj mk ij bi translated">无论是尚未发生的结果，还是我们尚无法一瞥的现实，我们都沉迷于了解未知。我们花费巨大的资源，希望对未来做出更准确的预测，并欣赏那些预测一贯正确的人。在过去的一个世纪里，新兴的统计推断领域已经为这些不可知的结果和关系提供了一个更强大的工具集。统计推断的目的是使用可观察的数据来推断一个<strong class="lu ir">随机变量</strong> (RV)的性质。术语“随机”可能会引起混淆，它并不意味着一个变量完全随机地取值，而是它取不同的值，这些值是由潜在的概率分布决定的。</p><p id="3107" class="pw-post-body-paragraph ls lt iq lu b lv lw jr lx ly lz ju ma ko mb mc md ks me mf mg kw mh mi mj mk ij bi translated">在这篇博文中，我们将使用一个最简单的RVs，即抛硬币的结果，来理解推理的一个基本方面，即参数估计。虽然硬币是一个简单的RV，因为它可以取两个值-正面或反面，但您可以考虑其他RV，如骰子滚动(在6面骰子的情况下可以取6个值)或股票价格(理论上可以取任何正值)。</p><p id="bbdf" class="pw-post-body-paragraph ls lt iq lu b lv lw jr lx ly lz ju ma ko mb mc md ks me mf mg kw mh mi mj mk ij bi translated">在统计推断中，我们希望了解RVs之间的关系，以便了解和预测我们周围的世界。控制不同RVs之间关系的实体称为参数，通常用希腊字母θ (theta)表示。我们可以用以下方式用数学方法写出这种关系:</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div class="gh gi ml"><img src="../Images/a3505032bf39abf5733757419b6cba73.png" data-original-src="https://miro.medium.com/v2/resize:fit:424/format:webp/1*qT6XN0I43q3yTYZpGuHj2A.png"/></div></figure><p id="223d" class="pw-post-body-paragraph ls lt iq lu b lv lw jr lx ly lz ju ma ko mb mc md ks me mf mg kw mh mi mj mk ij bi translated">y是我们的因变量(或结果变量)，X是我们的独立变量(或预测变量)。θ是参数空间，包含所有决定Y和x之间关系的潜在值。</p><p id="8d85" class="pw-post-body-paragraph ls lt iq lu b lv lw jr lx ly lz ju ma ko mb mc md ks me mf mg kw mh mi mj mk ij bi translated">虽然<strong class="lu ir">一个参数的真实值根据定义是未知的</strong>，我们可以用它的近似值来工作。在这篇博文中，我们将探讨两种不同的方法。我们将从使用最大似然估计(MLE)的频率主义者(或经典方法)开始，然后我们将继续讨论贝叶斯框架。但在此之前，让我们简要讨论一下二项式分布，这是一种相对简单但却非常重要的分布，每个数据科学家都应该知道，以及如何通过编写Python代码来模拟它。</p><h2 id="1449" class="kf kg iq bd kh ki kj dn kk kl km dp kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">二项式分布</h2><p id="e03d" class="pw-post-body-paragraph ls lt iq lu b lv mm jr lx ly mn ju ma ko mo mc md ks mp mf mg kw mq mi mj mk ij bi translated">我们将围绕抛硬币的例子进行讨论(这是任何统计文本都必须的)。具体来说，我们感兴趣的是在一次给定的投掷中硬币正面朝上的概率。在这种情况下，我们抛硬币的结果是我们的RV，它的值可以是0(反面)或1(正面)。</p><p id="1f47" class="pw-post-body-paragraph ls lt iq lu b lv lw jr lx ly lz ju ma ko mb mc md ks me mf mg kw mh mi mj mk ij bi translated">我们可以将单次抛硬币的结果表示为伯努利过程，这是一个有趣的术语，它表示Y是一个有两个潜在值的<em class="mr">单个结果</em>。形式上，我们有:</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div class="gh gi ms"><img src="../Images/1c673501cf309db51f20abf30a8e5c7a.png" data-original-src="https://miro.medium.com/v2/resize:fit:760/format:webp/1*T6jnBOLz91WTqz9gnuKsgQ.png"/></div></figure><p id="8c6e" class="pw-post-body-paragraph ls lt iq lu b lv lw jr lx ly lz ju ma ko mb mc md ks me mf mg kw mh mi mj mk ij bi translated">在这个上下文中，<strong class="lu ir"> <em class="mr"> p </em>是硬币正面朝上的概率，这是我们感兴趣的参数</strong>。我们的任务将是尽可能精确地估计p。</p><p id="d4f0" class="pw-post-body-paragraph ls lt iq lu b lv lw jr lx ly lz ju ma ko mb mc md ks me mf mg kw mh mi mj mk ij bi translated">因此，我们有:</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div class="gh gi mt"><img src="../Images/d39b0cc4ed616575c9a550c4529fcf73.png" data-original-src="https://miro.medium.com/v2/resize:fit:716/format:webp/1*fBL_Iq24fOPlOClaQ_HBWg.png"/></div></figure><p id="237a" class="pw-post-body-paragraph ls lt iq lu b lv lw jr lx ly lz ju ma ko mb mc md ks me mf mg kw mh mi mj mk ij bi translated">那么，给定一枚硬币，我们将如何估计<em class="mr"> p </em>？我们可以只抛一次硬币，但这不会提供太多信息。想象一下，真相是<em class="mr"> p=0.5 </em>(硬币是公平的，有相等的概率翻转正面或反面)，一次翻转后我们观察到一个正面。如果我们只依赖于那一次翻转，我们可能会得出<em class="mr"> p=1，</em>所以翻转头部的概率是100%，我们会一直翻转头部，这听起来很可疑。</p><p id="c1bd" class="pw-post-body-paragraph ls lt iq lu b lv lw jr lx ly lz ju ma ko mb mc md ks me mf mg kw mh mi mj mk ij bi translated">我们要做的是观察大量的翻转。当一个伯努利过程重复多次时，它被称为<em class="mr">二项式过程。</em>二项式过程建立在所有试验的假设之上，或者在我们的例子中是掷硬币，都是独立的——无论你现在掷的是正面还是反面，都不会对后面的迭代产生影响。</p><p id="4ce4" class="pw-post-body-paragraph ls lt iq lu b lv lw jr lx ly lz ju ma ko mb mc md ks me mf mg kw mh mi mj mk ij bi translated">假设现在我们掷硬币n次。在这些<em class="mr"> n </em>次投掷中，总的人头数<em class="mr"> Y </em>是一个带有参数<em class="mr"> n </em>和<em class="mr"> p的二项式随机变量。n </em>表示试掷或掷硬币的次数，<em class="mr"> p </em>表示成功的概率，或投掷人头的概率。最后，二项式分布的概率质量函数(PMF)给出了对于每一个<em class="mr"> p </em>值，在<em class="mr"> n次</em>试验中准确观察到Y个头的概率。</p><p id="8232" class="pw-post-body-paragraph ls lt iq lu b lv lw jr lx ly lz ju ma ko mb mc md ks me mf mg kw mh mi mj mk ij bi translated">形式上，我们有:</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div class="gh gi mu"><img src="../Images/f55fca14a7a34bf07121a157b7b6fc8f.png" data-original-src="https://miro.medium.com/v2/resize:fit:832/format:webp/1*_6HzpgmIARhBR9VJrJoswg.png"/></div></figure><p id="251a" class="pw-post-body-paragraph ls lt iq lu b lv lw jr lx ly lz ju ma ko mb mc md ks me mf mg kw mh mi mj mk ij bi translated">既然我们对参数估计和将要使用的分布有了理论上的理解，我们就可以开始编码了。</p><h2 id="800a" class="kf kg iq bd kh ki kj dn kk kl km dp kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">模拟一些数据</h2><p id="5bef" class="pw-post-body-paragraph ls lt iq lu b lv mm jr lx ly mn ju ma ko mo mc md ks mp mf mg kw mq mi mj mk ij bi translated">现在让我们假设有人给了我们一个有60%概率翻转正面的有偏硬币，但是我们不知道我们想自己估计这个概率。</p><p id="18e1" class="pw-post-body-paragraph ls lt iq lu b lv lw jr lx ly lz ju ma ko mb mc md ks me mf mg kw mh mi mj mk ij bi translated">我们可以使用<code class="fe mv mw mx my b">scipy.stats</code>库从二项式分布中得出结果。模拟是非常有用的，因为我们可以硬编码“真实”的参数，然后允许我们比较不同的框架在近似它方面的比较。</p><p id="1b40" class="pw-post-body-paragraph ls lt iq lu b lv lw jr lx ly lz ju ma ko mb mc md ks me mf mg kw mh mi mj mk ij bi translated">让我们模拟10000次抛硬币，观察一些结果。</p><pre class="lc ld le lf gt mz my na nb aw nc bi"><span id="4cc1" class="kf kg iq my b gy nd ne l nf ng">import numpy as np<br/>import scipy.stats as stats<br/>from matplotlib import pyplot as plt</span><span id="893d" class="kf kg iq my b gy nh ne l nf ng">np.random.seed(42) # set seed for reproducibility</span><span id="af77" class="kf kg iq my b gy nh ne l nf ng"># Set the probability of heads = 0.6<br/>p = 0.6</span><span id="7829" class="kf kg iq my b gy nh ne l nf ng"># Flip the Coin 10000 times and observe the results<br/>n_trials = 10000</span><span id="270e" class="kf kg iq my b gy nh ne l nf ng">data = stats.bernoulli.rvs(p, size = n_trials)</span><span id="4261" class="kf kg iq my b gy nh ne l nf ng"># plot results<br/>plt.hist(data);</span></pre><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi ni"><img src="../Images/925a31b9c90fcc53168d9d8389bc1b8a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8sfFit7GVZogf9jvH3E6yw.png"/></div></div><p class="ln lo gj gh gi lp lq bd b be z dk translated">10000次抛硬币的直方图</p></figure><pre class="lc ld le lf gt mz my na nb aw nc bi"><span id="aaf9" class="kf kg iq my b gy nd ne l nf ng">sum(data)</span><span id="a703" class="kf kg iq my b gy nh ne l nf ng"># 6108 coin flips out of 10000 are heads, since heads are coded as 1s and tails are coded as 0s</span></pre><p id="f57d" class="pw-post-body-paragraph ls lt iq lu b lv lw jr lx ly lz ju ma ko mb mc md ks me mf mg kw mh mi mj mk ij bi translated">正如预期的那样，正面和反面的比例接近60/40，在10000次掷硬币中有6108次是正面，这只是因为我们告诉虚拟硬币翻转器，硬币有60%的机会是正面！</p><p id="a456" class="pw-post-body-paragraph ls lt iq lu b lv lw jr lx ly lz ju ma ko mb mc md ks me mf mg kw mh mi mj mk ij bi translated">现在我们有了数据，让我们比较频率主义者和贝叶斯方法如何获得感兴趣的参数:<em class="mr"> p </em>。</p><h2 id="ae42" class="kf kg iq bd kh ki kj dn kk kl km dp kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">频繁主义者的方法</h2><p id="21cf" class="pw-post-body-paragraph ls lt iq lu b lv mm jr lx ly mn ju ma ko mo mc md ks mp mf mg kw mq mi mj mk ij bi translated">频率统计使用最大似然估计(MLE)。虽然对极大似然估计的全面处理超出了这篇博文的范围，但它的工作就在它的名字里:它符合一个模型，这个模型最大化了观察到的观察数据的可能性。</p><p id="3dd1" class="pw-post-body-paragraph ls lt iq lu b lv lw jr lx ly lz ju ma ko mb mc md ks me mf mg kw mh mi mj mk ij bi translated">对于二项分布，MLE是成功的样本比例[1]。这仅仅意味着，在频率主义框架下，我们假设<em class="mr"> p </em>的真实值是所有掷硬币中正面的数量:如果我们在10次掷硬币中有6次正面，那么我们认为<em class="mr"> p </em>应该接近6/10，或者60%。</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div class="gh gi nj"><img src="../Images/cc1782677fd80e89a5819e7485a6626b.png" data-original-src="https://miro.medium.com/v2/resize:fit:764/format:webp/1*loVlK3g7e94o5bfadfvOyg.png"/></div><p class="ln lo gj gh gi lp lq bd b be z dk translated">最大似然估计是成功的样本比例</p></figure><p id="8ad0" class="pw-post-body-paragraph ls lt iq lu b lv lw jr lx ly lz ju ma ko mb mc md ks me mf mg kw mh mi mj mk ij bi translated">在获得p的估计值后，下一步是量化该估计值的不确定性。记住，我们永远不知道它的真实价值，所以我们也必须量化它的全部潜在价值。这个范围称为置信区间，很容易计算出二项分布。首先，我们计算参数的标准误差，即其标准偏差乘以√N(样本大小)。然后，我们可以通过将标准误差乘以95% Z-stat(等于1.96)来找到我们的95%置信区间。</p><p id="5a40" class="pw-post-body-paragraph ls lt iq lu b lv lw jr lx ly lz ju ma ko mb mc md ks me mf mg kw mh mi mj mk ij bi translated">总之，我们有:</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/428eda7ebb98441e11867fae0d3fe21d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1136/format:webp/1*JwfHwIkLpVfPKgFZBisjNg.png"/></div><p class="ln lo gj gh gi lp lq bd b be z dk translated">捕捉二项分布的不确定性</p></figure><p id="d7b5" class="pw-post-body-paragraph ls lt iq lu b lv lw jr lx ly lz ju ma ko mb mc md ks me mf mg kw mh mi mj mk ij bi translated">让我们看看当我们投掷10枚硬币时会发生什么。在下图中，绿线表示<em class="mr"> p </em>的“真实”值，我们在模拟数据时决定该值为0.6。红色虚线表示最大似然估计，蓝色虚线表示置信区间，即<em class="mr"> p </em>所在的可信值范围。</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div class="gh gi nl"><img src="../Images/2bee598b57f1dbdecf931bdb8e6061d0.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/format:webp/1*Ml4Mg0BdMN1Z3IAIf2mE4Q.png"/></div><p class="ln lo gj gh gi lp lq bd b be z dk translated">MLE估计10次抛硬币</p></figure><p id="57ff" class="pw-post-body-paragraph ls lt iq lu b lv lw jr lx ly lz ju ma ko mb mc md ks me mf mg kw mh mi mj mk ij bi translated">因为我们观察了5个头，MLE是0.5(回想一下<em class="mr"> p </em>的真值是0.6)。不确定性被置信区间捕获，置信区间表明<em class="mr"> p </em>的真实值有95%的概率在~ 0.2和~ 0.8之间。由于置信区间与样本大小成正比，我们可以预计它们会随着抛硬币次数的增加而缩小——我们拥有的数据越多，我们对自己的预测就越有信心。为了说明这一点，让我们看看当我们翻转越来越多的硬币时会发生什么。</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi nm"><img src="../Images/d22d6f3403f69b971af09155975102ab.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9ZlAH-ZUdO8uY931M5OyWQ.png"/></div></div><p class="ln lo gj gh gi lp lq bd b be z dk translated">随着n的增加，MLE和CI估计值</p></figure><p id="9b9d" class="pw-post-body-paragraph ls lt iq lu b lv lw jr lx ly lz ju ma ko mb mc md ks me mf mg kw mh mi mj mk ij bi translated">在上面的图中，我们在1次翻转、10次翻转、25次翻转等多达10，000次翻转后拍摄结果的快照。这些图让我们了解了当我们掷硬币的次数越来越多时，我们的估计值以及我们对这些估计值的信心是如何变化的。</p><p id="5976" class="pw-post-body-paragraph ls lt iq lu b lv lw jr lx ly lz ju ma ko mb mc md ks me mf mg kw mh mi mj mk ij bi translated">随着我们一遍又一遍地继续抛硬币，我们获得了越来越多的关于硬币性质的信息。因此，我们应该期待我们的估计会变得越来越精确，也越来越准确。当我们只抛硬币几次(比如1到100次)时，我们可以看到我们的置信区间相当宽。这是因为我们还没有看到足够的信息来排除头部的真实概率位于我们当前最大似然估计两侧的可能性。然而，随着我们继续抛硬币，观察到越来越多的关于我们感兴趣的参数的证据，我们看到我们的置信区间开始变窄并接近MLE。当我们把硬币抛了10，000次后，我们的置信区间只是稍微偏向我们的最大似然估计。</p><p id="56af" class="pw-post-body-paragraph ls lt iq lu b lv lw jr lx ly lz ju ma ko mb mc md ks me mf mg kw mh mi mj mk ij bi translated">让我们凭直觉思考这个问题:随着我们获得更多的证据，我们应该对我们的估计越来越有信心。此外，最重要的是，我们应该期待我们的估计越来越接近事实！<strong class="lu ir">这是大数定律</strong>:随着样本量的增加，其参数estimand越来越接近总体的真实值。我们将正面的真实概率设置为0.6，这一点得到了证实，事实上，我们在翻转1000次后的最大似然估计为0.61，并且在此之后不再波动(只是置信区间变窄)。</p><h1 id="8bbe" class="nn kg iq bd kh no np nq kk nr ns nt kn jw nu jx kr jz nv ka kv kc nw kd kz nx bi translated">贝叶斯方法</h1><p id="6693" class="pw-post-body-paragraph ls lt iq lu b lv mm jr lx ly mn ju ma ko mo mc md ks mp mf mg kw mq mi mj mk ij bi translated">本节素材——尤其是模拟代码，大量亏欠<a class="ae lr" href="https://tinyheero.github.io/2017/03/08/how-to-bayesian-infer-101.html" rel="noopener ugc nofollow" target="_blank">https://tinyheero . github . io/2017/03/08/how-to-Bayesian-infer-101 . html</a>[2]。</p><p id="0458" class="pw-post-body-paragraph ls lt iq lu b lv lw jr lx ly lz ju ma ko mb mc md ks me mf mg kw mh mi mj mk ij bi translated">回顾一下，贝叶斯定理通过将模型参数建立为基于我们观察到的数据的分布条件来估计模型参数。</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/e8d02b1844d1d37795c20dcd244300c7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1304/format:webp/1*IvfhKEENIs-Z-dXOlsWIDQ.png"/></div><p class="ln lo gj gh gi lp lq bd b be z dk translated">贝叶斯定理</p></figure><ul class=""><li id="11a7" class="nz oa iq lu b lv lw ly lz ko ob ks oc kw od mk oe of og oh bi translated">P(θ)是我们模型参数的先验分布，它代表了我们在看到任何数据之前对结果和预测变量之间关系的看法。</li><li id="a455" class="nz oa iq lu b lv oi ly oj ko ok ks ol kw om mk oe of og oh bi translated">P(X|θ)是似然项，表示先验数据与观测数据的拟合程度。</li><li id="6979" class="nz oa iq lu b lv oi ly oj ko ok ks ol kw om mk oe of og oh bi translated">P(X)是预测变量的边际分布。换句话说，它代表观察到给定<em class="mr">所有</em>θ可能值的数据的概率。当处理离散分布时，P(X)可以通过对θ的所有值求和得到；在连续情况下，它是通过对θ积分得到的。</li></ul><h2 id="bac1" class="kf kg iq bd kh ki kj dn kk kl km dp kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">院长</h2><p id="b283" class="pw-post-body-paragraph ls lt iq lu b lv mm jr lx ly mn ju ma ko mo mc md ks mp mf mg kw mq mi mj mk ij bi translated">贝叶斯工作流程的第一步是指定我们对结果变量的先验信念。对于这个例子来说，这意味着对我们关于抛硬币正面朝上的概率的信念进行编码:没错——我们把一个概率放在一个概率上。</p><figure class="lc ld le lf gt lg"><div class="bz fp l di"><div class="on oo l"/></div></figure><p id="ce16" class="pw-post-body-paragraph ls lt iq lu b lv lw jr lx ly lz ju ma ko mb mc md ks me mf mg kw mh mi mj mk ij bi translated">上面的代码块创建了先验分布。首先，<code class="fe mv mw mx my b">np.linspace</code>函数创建11个介于0和1之间的值，间隔为0.1。其次，我们硬编码了与先验分布中每个值相关的概率。随意使用不同的概率值——只要它们的总和为1！</p><figure class="lc ld le lf gt lg"><div class="bz fp l di"><div class="on oo l"/></div></figure><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi op"><img src="../Images/a5974fe0c1b632d9166f5e27b27044da.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7I2LA1Z9ioKgEfwEvfZ7rw.png"/></div></div><p class="ln lo gj gh gi lp lq bd b be z dk translated">θ的先验:翻转头部的概率</p></figure><p id="7024" class="pw-post-body-paragraph ls lt iq lu b lv lw jr lx ly lz ju ma ko mb mc md ks me mf mg kw mh mi mj mk ij bi translated">先验分布概括了我们在没有看到任何数据的情况下对击中头部的概率的想法。为简单起见，我们将允许θ从0到1只取10个0.1增量的值。我们假设我们不知道硬币是有偏差的，也没有任何迹象表明它会有偏差。</p><p id="db73" class="pw-post-body-paragraph ls lt iq lu b lv lw jr lx ly lz ju ma ko mb mc md ks me mf mg kw mh mi mj mk ij bi translated">因此，让我们建立一个峰值为0.5，值为0.2的分布:我们说有20%的机会硬币是公平的。我们也在考虑硬币可能不公平的可能性，这反映在θ从0.1到0.9的概率上。我们认为唯一不可能的情况是p等于0(没有机会永远不摇头)或1(没有机会永远不摇头)。</p><h2 id="4017" class="kf kg iq bd kh ki kj dn kk kl km dp kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">可能性</h2><p id="c224" class="pw-post-body-paragraph ls lt iq lu b lv mm jr lx ly mn ju ma ko mo mc md ks mp mf mg kw mq mi mj mk ij bi translated">既然我们已经定义并编码了我们先前的信念，贝叶斯方法的下一步就是收集数据并将其纳入我们的估计。这一步与我们在Frequentist方法中所做的没有什么不同:我们将观察完全相同的数据，并再次使用似然函数来提取关于参数空间的信息。唯一的区别是，我们现在不仅仅是在MLE之后，或者我们在Frequentist方法中得出的点估计，我们需要参数空间中每个值的似然值。</p><p id="f30c" class="pw-post-body-paragraph ls lt iq lu b lv lw jr lx ly lz ju ma ko mb mc md ks me mf mg kw mh mi mj mk ij bi translated">可能性可能是一个难以理解的复杂概念，但它所做的基本上是返回一个数字，告诉我们某个值<em class="mr"> p </em>符合数据的程度。一组<em class="mr"> p </em>的高似然值意味着它们“适合”数据，反之亦然。我们基本上是在问:对于任何给定的<em class="mr"> p，</em>我们能确信它确实产生了我们所见证的数据吗？</p><p id="b441" class="pw-post-body-paragraph ls lt iq lu b lv lw jr lx ly lz ju ma ko mb mc md ks me mf mg kw mh mi mj mk ij bi translated">二项式可能性的等式是其概率质量函数(PMF) [3]。</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi oq"><img src="../Images/ca9a7400a17268c5fe8659d56c020887.png" data-original-src="https://miro.medium.com/v2/resize:fit:908/format:webp/1*dlJLqPhULIMe3BPjKQegFA.png"/></div></div><p class="ln lo gj gh gi lp lq bd b be z dk translated">二项式似然函数</p></figure><p id="4604" class="pw-post-body-paragraph ls lt iq lu b lv lw jr lx ly lz ju ma ko mb mc md ks me mf mg kw mh mi mj mk ij bi translated">让我们解开上面的等式。我们说:给定<em class="mr"> n </em>次投掷和<em class="mr"> y </em>次投掷，与<em class="mr"> p </em>相关的可能性是多少？对于θ中的每个<em class="mr"> p </em>值，似然性评估该<em class="mr"> p </em>值的概率。举个例子，如果我们观察到1个头像，那么p= 0的可能性一定是0——因为我们观察到至少1个头像，那么永远不会翻转头像的可能性一定是0。另一个例子，虽然不太可能(看我在那里做了什么？)将见证10次翻转中有10次正面朝上。在这种情况下，可能性将测量值<em class="mr"> p </em> = 1是极有可能的，并且很可能分配它接近100%的概率。</p><p id="3283" class="pw-post-body-paragraph ls lt iq lu b lv lw jr lx ly lz ju ma ko mb mc md ks me mf mg kw mh mi mj mk ij bi translated">现在让我们计算可能性，并将其可视化。我们将保留前一个示例中的前10个数据点，因此10次翻转中有5次是正面。</p><figure class="lc ld le lf gt lg"><div class="bz fp l di"><div class="on oo l"/></div></figure><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi or"><img src="../Images/2f2b0f7aa7d04be05f6f9b094edfaadb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ozp729dDhYVsHzX4QwZ7Bg.png"/></div></div><p class="ln lo gj gh gi lp lq bd b be z dk translated">观察10次抛硬币中5次正面的可能性</p></figure><p id="62ef" class="pw-post-body-paragraph ls lt iq lu b lv lw jr lx ly lz ju ma ko mb mc md ks me mf mg kw mh mi mj mk ij bi translated">您会注意到，可能性在0.5左右达到峰值，离它越远，值就越小。这是有道理的:这种可能性用数据更新了我们之前对θ的信念，数据显示，10次翻转中有5次是正面——根据我们观察到的数据，θ最有可能的真实值是0.5。因此，0.5将获得最高的可能性。反之亦然，像0.1和0.9这样的值不太可能是真实值，我们看到图表中相应地反映了这一点。反过来，极端值没有得到数据的证实，因此给出了非常小的似然值。</p><p id="8555" class="pw-post-body-paragraph ls lt iq lu b lv lw jr lx ly lz ju ma ko mb mc md ks me mf mg kw mh mi mj mk ij bi translated">然而，需要注意的是<strong class="lu ir">可能性不是有效的概率分布</strong>。你可以自己检查一下，把θ的所有概率加起来，结果不等于1！这就是归一化常数/分母的用武之地:将每个可能性概率除以归一化常数(它是一个标量)得到一个有效的概率分布，我们称之为后验概率。</p><h2 id="2060" class="kf kg iq bd kh ki kj dn kk kl km dp kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">后验分布</h2><p id="a563" class="pw-post-body-paragraph ls lt iq lu b lv mm jr lx ly mn ju ma ko mo mc md ks mp mf mg kw mq mi mj mk ij bi translated">现在我们已经计算了似然值，我们通过将它们与先前的值相乘来获得贝叶斯定理的分子，这产生了θ和x的联合分布。反过来，分母通过对所得向量求和来获得(忽略θ)。</p><p id="50a3" class="pw-post-body-paragraph ls lt iq lu b lv lw jr lx ly lz ju ma ko mb mc md ks me mf mg kw mh mi mj mk ij bi translated">您可能已经看到，分母常常使贝叶斯推理在计算上难以处理。后面的博客文章将解释为什么会这样，但是对于这个场景，我们的优势是使用离散的参数空间(求和比积分容易),并且只取10个值，所以计算分母是完全可行的。</p><p id="b03d" class="pw-post-body-paragraph ls lt iq lu b lv lw jr lx ly lz ju ma ko mb mc md ks me mf mg kw mh mi mj mk ij bi translated">在下一篇博文中，我们将讨论当我们使用连续发行版时会发生什么。现在，请注意我们推导X的边际分布的能力是可能的，因为我们使用的是一个变量，而这个变量只有很少(11)个值。这使得对参数空间中的所有值求和变得容易，而如果我们处理的是可能有数千个(或无限个)预测值的多个预测值，情况就不一样了！如在连续的情况下)值的数量。</p><figure class="lc ld le lf gt lg"><div class="bz fp l di"><div class="on oo l"/></div></figure><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi os"><img src="../Images/5a216490fde3b93b46c0178056c8af2f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qd-I3FuFSzfPjdnlwdKtLA.png"/></div></div><p class="ln lo gj gh gi lp lq bd b be z dk translated">后验分布是可能性和先验之间的折衷</p></figure><p id="df90" class="pw-post-body-paragraph ls lt iq lu b lv lw jr lx ly lz ju ma ko mb mc md ks me mf mg kw mh mi mj mk ij bi translated">那么后验分布是怎样的呢？它反映了我们对我们感兴趣的参数的信念，包含了我们可以获得的所有信息。它是两个部分的产物:我们对θ的先验信念，以及反映我们从观测数据中获得的信息的可能性或证据。结合这两部分，<strong class="lu ir">我们得到了我们感兴趣的参数</strong>的概率分布。这是区分贝叶斯和频率主义方法的关键。在frequentist方法论中，我们的答案以点估计的形式表示(有一个置信区间)。然而，在贝叶斯方法中，我们的答案以概率分布的形式表达，允许我们对每个潜在的<em class="mr"> p </em>正确的概率进行赋值。</p><p id="3bc6" class="pw-post-body-paragraph ls lt iq lu b lv lw jr lx ly lz ju ma ko mb mc md ks me mf mg kw mh mi mj mk ij bi translated">在上面的例子中，在观察了10个翻转和5个头部之后，我们的先验分布已经被向θ= 0.5的可能性挤压，这似乎越来越成为最有可能的答案。但是，我们还没有看到足够的信息来排除θ位于0.5一侧某处的可能性，所以我们将继续观察更多的数据。类似于我们在frequentist讨论中所做的，让我们看看当我们看到越来越多的数据时，我们的贝叶斯结论是如何变化的。</p><h2 id="240d" class="kf kg iq bd kh ki kj dn kk kl km dp kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">把所有的放在一起</h2><p id="3e90" class="pw-post-body-paragraph ls lt iq lu b lv mm jr lx ly mn ju ma ko mo mc md ks mp mf mg kw mq mi mj mk ij bi translated">现在让我们看看在贝叶斯框架下，随着抛硬币次数的增加会发生什么。下图覆盖了后验分布(蓝色，左侧Y轴上的值)和可能性(红色，右侧Y轴上的值)。</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi nm"><img src="../Images/f04ec742534718119ea8b71e8061898e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sfYAXas0RZF0Z2VDk0BT3w.png"/></div></div><p class="ln lo gj gh gi lp lq bd b be z dk translated">n增加时的后验概率和可能性</p></figure><p id="c0c4" class="pw-post-body-paragraph ls lt iq lu b lv lw jr lx ly lz ju ma ko mb mc md ks me mf mg kw mh mi mj mk ij bi translated">让我们从分析我们已经抛过一次硬币并观察到正面的情况开始。在frequentist方法中，我们看到，在这种情况下，我们对p = 1的估计，显然是一个不正确的结论，然而我们没有其他方法来回答这个问题。在贝叶斯世界中，我们的答案非常不同。因为我们定义了我们的先验信念，即θ最可能的值是0.5，所以我们只受到第一次翻转的轻微影响。凭直觉想一想，如果你真的相信一枚硬币是公平的，你抛了一次，硬币正面朝上，这足以证明这枚硬币不公平吗？大概不会！</p><p id="8c02" class="pw-post-body-paragraph ls lt iq lu b lv lw jr lx ly lz ju ma ko mb mc md ks me mf mg kw mh mi mj mk ij bi translated">当要处理的数据相对较少时，贝叶斯方法比频率主义方法表现得更好。频率主义者答案的可行性依赖于大数定律，因此在缺乏大量数据的情况下，结果并不总是可靠的。</p><p id="2599" class="pw-post-body-paragraph ls lt iq lu b lv lw jr lx ly lz ju ma ko mb mc md ks me mf mg kw mh mi mj mk ij bi translated">然而，随着我们开始观察更多的硬币翻转，我们开始看到我们的贝叶斯答案变得非常清楚，最终我们将所有的鸡蛋放在p = 0.6的篮子里。仅仅翻转100次后，我们给不等于0.6的值分配了一个非常小的概率，最终我们分配给0.6的概率收敛到1，或者大约100%。这意味着我们接近100%确定，60%的时间，我们的硬币将是正面。</p><p id="1544" class="pw-post-body-paragraph ls lt iq lu b lv lw jr lx ly lz ju ma ko mb mc md ks me mf mg kw mh mi mj mk ij bi translated">在背景中，可能性(或观察到的数据)开始支配我们一开始建立的先验信念。这是因为证据是压倒性的，所以我们更依赖它。如上图所示，随着<em class="mr"> n </em>的增加，似然性和后验分布收敛(注意，X轴和Y轴上的比例不同，因为似然性不是概率分布)。当我们观察到更多的数据时，先验的影响会被冲掉，数据会自己说话。</p><h2 id="1fff" class="kf kg iq bd kh ki kj dn kk kl km dp kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">结论</h2><p id="ef7a" class="pw-post-body-paragraph ls lt iq lu b lv mm jr lx ly mn ju ma ko mo mc md ks mp mf mg kw mq mi mj mk ij bi translated">在这篇博文中，我们研究了估计未知参数的两种不同方法。在Frequentist方法中，我们让数据说话:我们通过建立一个尽可能符合观察数据的模型来估计X和Y之间的关系。这给了我们一个单点估计，最大似然估计，不确定性被模型的标准误差捕获，它与样本大小成反比。因此，典型的Frequentist努力收集尽可能多的关于感兴趣的参数的数据，以便达到更精确的估计(至少在理论上)。</p><p id="7d98" class="pw-post-body-paragraph ls lt iq lu b lv lw jr lx ly lz ju ma ko mb mc md ks me mf mg kw mh mi mj mk ij bi translated">在贝叶斯方法下，我们从变量之间的关系的概念开始分析:先验分布。然后我们观察我们的数据并更新我们对参数分布的信念:这给出了后验分布。<strong class="lu ir">贝叶斯框架下的一个重要但微妙的区别是，我们将参数视为随机变量，以捕捉其真实值的不确定性</strong>。这需要在建模过程的每个阶段都使用发行版。</p><p id="6323" class="pw-post-body-paragraph ls lt iq lu b lv lw jr lx ly lz ju ma ko mb mc md ks me mf mg kw mh mi mj mk ij bi translated">如果我们回忆一下我们展示的频率主义方法的图表，我们会发现，随着样本量的增加，贝叶斯和频率主义的答案是一致的，这是理所应当的！随着我们获得关于我们感兴趣的参数的更全面的信息(即观察更多的数据)，我们的答案应该变得更加客观。</p><p id="aa8c" class="pw-post-body-paragraph ls lt iq lu b lv lw jr lx ly lz ju ma ko mb mc md ks me mf mg kw mh mi mj mk ij bi translated">所以你可能会想，如果他们最终给出相同的答案，那么采用不同的方法又有什么意义呢？这实际上取决于用例。我们认为，在你有很多很多数据的情况下，部署一个完全成熟的贝叶斯框架可能是多余的。在数据较少的情况下，例如在社会科学中，处理后验分布的能力是非常有洞察力的。此外，在频率主义者的方法中，我们完全受制于数据的准确性。如果我们的数据不准确或有偏差，那么我们的估计也会如此。在贝叶斯方法中，我们可以通过设置更强的先验来抵消这种影响(例如，对我们先前的信念更有信心)。我们还认为，贝叶斯分析在简单性方面的不足，在整体精度方面得到了弥补，在许多情况下，向客户或同事展示后验分布比点估计更能提供信息。</p><p id="7440" class="pw-post-body-paragraph ls lt iq lu b lv lw jr lx ly lz ju ma ko mb mc md ks me mf mg kw mh mi mj mk ij bi translated">在这篇博文中，我们将先验空间视为相对简单的空间，只有11个p可以取的任意值。但是如果p可以取0到1之间任何可能的值呢？当我们开始处理参数空间的连续分布时，事情变得有点不确定，这将是下一篇博客文章的主题，我们将了解为什么使用马尔可夫链蒙特卡罗近似法——敬请关注！</p><h2 id="620f" class="kf kg iq bd kh ki kj dn kk kl km dp kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">我们意识到有很多东西需要打开，所以这里有一个TLDR给你:</h2><p id="7a3b" class="pw-post-body-paragraph ls lt iq lu b lv mm jr lx ly mn ju ma ko mo mc md ks mp mf mg kw mq mi mj mk ij bi translated">Frequentist方法使用最大似然估计(MLE)来估计未知参数:在给定我们观察到的数据的情况下，点估计是参数最可能的真实值。</p><p id="bbdb" class="pw-post-body-paragraph ls lt iq lu b lv lw jr lx ly lz ju ma ko mb mc md ks me mf mg kw mh mi mj mk ij bi translated">Frequentist答案完全由观察到的数据形成，并以单点估计的形式交付。这使我们受到数据准确性和质量的支配。</p><p id="aff4" class="pw-post-body-paragraph ls lt iq lu b lv lw jr lx ly lz ju ma ko mb mc md ks me mf mg kw mh mi mj mk ij bi translated">-贝叶斯方法将先验概率分布与观测数据(以似然分布的形式)相结合，以获得后验概率分布。</p><p id="af64" class="pw-post-body-paragraph ls lt iq lu b lv lw jr lx ly lz ju ma ko mb mc md ks me mf mg kw mh mi mj mk ij bi translated">-虽然贝叶斯方法也依赖于数据，但其估计值结合了我们关于感兴趣参数的先验<em class="mr">*知识* </em>，其答案以感兴趣参数的概率分布的形式给出。我们可以通过设定更强的先验来抵消对数据的依赖。</p><p id="9060" class="pw-post-body-paragraph ls lt iq lu b lv lw jr lx ly lz ju ma ko mb mc md ks me mf mg kw mh mi mj mk ij bi translated">-随着样本量的增加，统计推断变得客观，频率主义者和贝叶斯估计开始彼此相等。</p><h2 id="b31e" class="kf kg iq bd kh ki kj dn kk kl km dp kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">完整代码</h2><p id="de6d" class="pw-post-body-paragraph ls lt iq lu b lv mm jr lx ly mn ju ma ko mo mc md ks mp mf mg kw mq mi mj mk ij bi translated"><a class="ae lr" href="https://github.com/gabgilling/Bayesian-Blogs/blob/main/Blog%20Post%201%20final.ipynb" rel="noopener ugc nofollow" target="_blank">https://github . com/gabgilling/Bayesian-Blogs/blob/main/Blog % 20 post % 201% 20 final . ipynb</a></p><h2 id="10e4" class="kf kg iq bd kh ki kj dn kk kl km dp kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">引文</h2><p id="95c2" class="pw-post-body-paragraph ls lt iq lu b lv mm jr lx ly mn ju ma ko mo mc md ks mp mf mg kw mq mi mj mk ij bi translated">[1]https://online.stat.psu.edu/stat504/lesson/1/1.5<a class="ae lr" href="https://online.stat.psu.edu/stat504/lesson/1/1.5" rel="noopener ugc nofollow" target="_blank"/></p><p id="69bc" class="pw-post-body-paragraph ls lt iq lu b lv lw jr lx ly lz ju ma ko mb mc md ks me mf mg kw mh mi mj mk ij bi translated">[2]<a class="ae lr" href="https://tinyheero.github.io/2017/03/08/how-to-bayesian-infer-101.html" rel="noopener ugc nofollow" target="_blank">https://tinyheero . github . io/2017/03/08/how-to-Bayesian-infer-101 . html</a></p><p id="d102" class="pw-post-body-paragraph ls lt iq lu b lv lw jr lx ly lz ju ma ko mb mc md ks me mf mg kw mh mi mj mk ij bi translated">[3]<a class="ae lr" href="https://sites.warnercnr.colostate.edu/gwhite/wp-content/uploads/sites/73/2017/04/BinomialLikelihood.pdf" rel="noopener ugc nofollow" target="_blank">https://sites . warner CNR . colo state . edu/g white/WP-content/uploads/sites/73/2017/04/binomiallikelihood . pdf</a></p><h2 id="349f" class="kf kg iq bd kh ki kj dn kk kl km dp kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">承认</h2><p id="7fe6" class="pw-post-body-paragraph ls lt iq lu b lv mm jr lx ly mn ju ma ko mo mc md ks mp mf mg kw mq mi mj mk ij bi translated">感谢我们在IBM的同事:Alexander Ivanoff、Steven Hwang、Dheeraj Aremsetty和Lindsay Sample的校对和评论！</p></div></div>    
</body>
</html>