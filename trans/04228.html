<html>
<head>
<title>CenterNet, Explained</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">中心网，解释</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/centernet-explained-a7386f368962?source=collection_archive---------3-----------------------#2021-04-10">https://towardsdatascience.com/centernet-explained-a7386f368962?source=collection_archive---------3-----------------------#2021-04-10</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="6738" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">是什么让CenterNet有别于其他对象检测体系结构</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/b18333d068d509912b416376bca82b6f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MFhUtNzCTFS4Z6Ph4QJ5tg.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">湿狗热图。来源:<a class="ae kv" href="https://www.facebook.com/uri.almog.photography/" rel="noopener ugc nofollow" target="_blank">尤里阿尔莫格摄影</a></p></figure><p id="98a3" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">CenterNet是一种无锚点对象检测架构。这种结构具有重要的优点，因为它在后处理中用更优雅的算法代替了经典的NMS(非最大抑制)，这对于CNN流来说是自然的。这种机制能够实现更快的推理。参见图1。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ls"><img src="../Images/f9fbf70de367d7947ae42645a3a1016b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cWeOnI9db1_ULbYCYy53wQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图一。COCO mAP与不同模型的推理时间，由CenterNet作者测量。由Uri Almog创建的图像。</p></figure><p id="5503" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">正如我在关于<a class="ae kv" href="https://medium.com/swlh/object-detection-with-deep-learning-rcnn-anchors-non-maximum-suppression-ce5a83c7c62b?source=friends_link&amp;sk=f364d88880502c32e2f5147a6d6ed982" rel="noopener">物体检测</a>的帖子中所描述的，大多数检测器使用多个(通常是3或5个)基本框或锚来编码它们的预测。输出要素地图中的每个空间像元预测几个盒子。每个框预测被编码为相对于单元格中心的x和y偏移，以及相对于相应锚点的宽度和高度偏移。完整的解释可以在我的<a class="ae kv" rel="noopener" target="_blank" href="/yolo-v3-explained-ff5b850390f"> YOLOv3 </a>帖子中找到。</p><p id="7105" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">基于锚点的检测的问题是它会产生许多垃圾预测:例如，YOLOv3为每幅图像预测了超过7K个盒子。然后，后处理块需要对重叠的预测进行成对检查，如果重叠很高(对于COCO测试集，通常IoU &gt; 0.7)，则预测被认为是指同一对象，并且具有较低置信度的预测被丢弃。这种方法有两个主要缺陷:</p><ol class=""><li id="31b6" class="lt lu iq ky b kz la lc ld lf lv lj lw ln lx lr ly lz ma mb bi translated">其复杂性与预测数量的平方成正比——对于基于锚的高精度模型(使用高分辨率输入)来说尤其糟糕。</li><li id="a49f" class="lt lu iq ky b kz mc lc md lf me lj mf ln mg lr ly lz ma mb bi translated">它迫使网络在NMS之前解码所有预测，对大多数不相关的预测执行更多的时间和功率消耗操作。</li></ol><p id="8c2c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">论文<a class="ae kv" href="https://arxiv.org/pdf/1904.07850.pdf" rel="noopener ugc nofollow" target="_blank">由周等人于2019年发表，以物为点</a>探索了无锚点预测。在训练集准备阶段，他们在地面实况(GT)框中心绘制一个带有delta函数的地图。然后，他们使用高斯滤波器涂抹这些中心，生成一个平滑的分布，在对象中心达到峰值。然后，该模型使用两个预测头:一个训练来预测置信度热图，另一个训练，如在其他检测器中一样，来预测盒子尺寸和偏移的回归值，这是指由第一个训练头预测的盒子中心。参见图2。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mh"><img src="../Images/4550f2cd539bf28659c6fadce4b7049b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZRJC3zspKwHSJvwSnq_zlQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图二。中心网络热图预测。蓝色表示低置信度，红色表示高置信度，表示单元格包含一个方框中心。CenterNet为每个类预测了一个单独的热图。由Uri Almog创建。图片来源:<a class="ae kv" href="https://www.facebook.com/uri.almog.photography" rel="noopener ugc nofollow" target="_blank">乌里阿尔莫摄影</a></p></figure><p id="a099" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在是最酷的部分:我们可以直接使用置信度热图来删除不相关的预测，而无需解码盒子，也不会破坏深层的CNN流程。参见图3:</p><ol class=""><li id="5e0a" class="lt lu iq ky b kz la lc ld lf lv lj lw ln lx lr ly lz ma mb bi translated">在热图上运行maxpool 3x3/1。这将提高每个细胞一点点，除了在非常高峰。在非常接近峰值的地方产生一个小的平坦区域。</li><li id="4a29" class="lt lu iq ky b kz mc lc md lf me lj mf ln mg lr ly lz ma mb bi translated">在maxpool输入和输出之间运行布尔元素比较。以FP值的形式返回值(0。或者1。).这将创建一个新的热点图，其中除了值为1的局部最大值之外，所有位置都为0(因为maxpool操作提升了每个值)。</li><li id="e1e3" class="lt lu iq ky b kz mc lc md lf me lj mf ln mg lr ly lz ma mb bi translated">在阶段2的输出和最大池的输入之间运行元素级乘法。这将使每个局部最大值返回到其置信水平，将非最大值保留为0。</li><li id="67f8" class="lt lu iq ky b kz mc lc md lf me lj mf ln mg lr ly lz ma mb bi translated">像在其他检测器中一样应用置信度阈值。</li><li id="7b87" class="lt lu iq ky b kz mc lc md lf me lj mf ln mg lr ly lz ma mb bi translated">仅解码对应于幸存最大值的框。</li></ol><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mh"><img src="../Images/21f0bcb24f2588bd4e049dde0ef52a2f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5_2GRhESUpGnm_LhSCQfZQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图3。中心网NMS流量。图中显示了流程列表中的阶段1-3。第二个热图在置信度峰值附近包含一个小而平坦的红色区域。所有其他值都略高于它们的原始值。这在元素式比较输出中产生一个奇点，然后通过元素式乘法将该奇点缩放回置信值。资料来源:Uri Almog。</p></figure><p id="8085" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">作者使用DLA34和沙漏-101主链测试了遵循上述机制的经典NMS的需要，并得出结论，影响很小(第一种情况下0.7% mAP，第二种情况下0%)。</p><p id="2264" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这个检测框架已经被最初的作者和其他研究人员用于更多的检测任务，例如姿态估计和3D对象检测。带有ResNet18和ResNet50主干的训练过的检查点可以在<a class="ae kv" href="https://cv.gluon.ai/model_zoo/detection.html" rel="noopener ugc nofollow" target="_blank"> GluonCV模型动物园—检测</a>下载。</p><h1 id="57fd" class="mi mj iq bd mk ml mm mn mo mp mq mr ms jw mt jx mu jz mv ka mw kc mx kd my mz bi translated">结论</h1><p id="6653" class="pw-post-body-paragraph kw kx iq ky b kz na jr lb lc nb ju le lf nc lh li lj nd ll lm ln ne lp lq lr ij bi translated">CenterNet是一种深度检测架构，消除了对锚点和计算量大的NMS的需求。它基于这样的认识，即盒预测可以基于它们的中心位置而不是它们与对象的重叠来排序相关性。这种见解现在正被用于其他深度学习任务。</p><p id="0220" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我的其他帖子:</p><p id="f965" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><a class="ae kv" href="https://medium.com/@urialmog/what-is-a-neural-network-dac400d5307d?source=friends_link&amp;sk=764555affffd1bbb5f73e6f87d36ed58" rel="noopener"> <strong class="ky ir"> <em class="nf">什么是神经网络？</em> </strong> </a></p><p id="17a5" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><a class="ae kv" href="https://medium.com/@urialmog/practical-problems-math-and-ai-c934a95bde28?source=friends_link&amp;sk=bc78b0acc4786b7a251fd4986b01a16b" rel="noopener"> <strong class="ky ir"> <em class="nf">数学和人工智能</em> </strong> </a></p><p id="99a2" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><a class="ae kv" href="https://medium.com/@urialmog/practical-problems-math-and-ai-c934a95bde28?source=friends_link&amp;sk=bc78b0acc4786b7a251fd4986b01a16b" rel="noopener"><strong class="ky ir"><em class="nf"/></strong></a></p><p id="87bc" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><a class="ae kv" href="https://medium.com/@urialmog/practical-problems-math-and-ai-c934a95bde28?source=friends_link&amp;sk=bc78b0acc4786b7a251fd4986b01a16b" rel="noopener"> <strong class="ky ir"> <em class="nf">训练神经网络简单解释</em> </strong> </a></p><p id="84fe" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><a class="ae kv" href="https://medium.com/swlh/object-detection-with-deep-learning-rcnn-anchors-non-maximum-suppression-ce5a83c7c62b?source=friends_link&amp;sk=f364d88880502c32e2f5147a6d6ed982" rel="noopener"> <strong class="ky ir"> <em class="nf">深度学习的物体检测——RCNN、锚点、非最大抑制</em> </strong> </a></p><p id="aded" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><a class="ae kv" rel="noopener" target="_blank" href="/yolo-v3-explained-ff5b850390f"> <strong class="ky ir"> <em class="nf">约洛夫3解释</em> </strong> </a></p></div></div>    
</body>
</html>