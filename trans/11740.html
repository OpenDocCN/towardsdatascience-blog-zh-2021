<html>
<head>
<title>TF-IDF Demystified</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">TF-IDF揭秘</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/tf-idf-demystified-7dd3ef071b24?source=collection_archive---------19-----------------------#2021-11-22">https://towardsdatascience.com/tf-idf-demystified-7dd3ef071b24?source=collection_archive---------19-----------------------#2021-11-22</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="bc6a" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">了解文本矢量化中的关键人物</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/a8bce0b13ec65b9ca78eb2703bbe9fef.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mQ1kpHglKIIj7cioIJyBNA.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">照片由<a class="ae kv" href="https://www.pexels.com/photo/alphabet-close-up-communication-conceptual-278887/?utm_content=attributionCopyText&amp;utm_medium=referral&amp;utm_source=pexels" rel="noopener ugc nofollow" target="_blank">像素</a>的<a class="ae kv" href="https://www.pexels.com/@pixabay?utm_content=attributionCopyText&amp;utm_medium=referral&amp;utm_source=pexels" rel="noopener ugc nofollow" target="_blank">皮克斯拜</a>拍摄</p></figure><p id="8a80" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">有很多方法可以将文本转换为机器学习和自然语言应用的向量。在这里，我们探索一种更令人头痛的方法:TF-IDF。</p><p id="e66c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">当然，当我称之为头痛诱发时，我主要是为我自己说话。这是我刚开始学习数据科学时难以理解的一个话题。</p><p id="c99f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">TF-IDF用于许多需要分析文本或构建利用NLP的产品的项目中。当我深入研究各种NLP项目时，我经常发现自己一次又一次地碰到这个话题。</p><p id="b6e5" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">为此，我强调熟悉以色列国防军特遣部队的重要性。</p><p id="7218" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">也就是说，仅仅理解概念可能是不够的；你可能还需要能够口头解释它。</p><p id="8251" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">TF-IDF经常出现在需要NLP专业知识的职位的数据科学面试中。虽然它不是最复杂的单词矢量化方法，但它可能很难用单词来解释。</p><p id="38f2" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">因此，这已经成为许多善意的(或虐待狂的)希望淘汰候选人的面试官的热门话题。</p><p id="8e76" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我写这篇文章时投入了一些感情，希望这个五个字母的谜不要像我一样让人绕着圈子跑。</p><h1 id="68a7" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">TF-TDF</h1><p id="d7eb" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">术语频率-逆文档频率，或TF-IDF，是一种将单词表示为向量以从文本数据中提取洞察力的方法。</p><p id="7da4" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">初学者难以理解TF-IDF的原因之一是它的数字表示是抽象的。分配给每个文档中每个单词的值没有具体的含义。</p><p id="3179" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">TF-IDF的抽象性源于它不是一个实际的统计数据；它是两个独立统计数据的产物。</p><p id="e405" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">单词的TF-IDF由两个因素决定:术语频率(TF)和逆文档频率(IDF)。</p><p id="71a5" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">让我们慢慢地分解每一部分，以免你头疼，过早地放弃这篇文章。</p><h2 id="70d1" class="mp lt iq bd lu mq mr dn ly ms mt dp mc lf mu mv me lj mw mx mg ln my mz mi na bi translated">检索词频率</h2><p id="437a" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">术语频率指的是一个单词在所选文本体中出现的次数。它可以通过以下公式导出:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nb"><img src="../Images/15282a8000576284d0f85b85540ea923.png" data-original-src="https://miro.medium.com/v2/resize:fit:1194/format:webp/1*tBXLw2azRH5WzUUAYKW3FQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">由作者创建</p></figure><p id="d543" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">仅根据每个单词在文档中的出现频率来表示它，与单词袋方法非常相似。</p><p id="3f9b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">单词袋模型只是记录每个单词在每个文档中出现的次数。仅依赖于上下文的词频会导致可能在多个文档中普遍存在的词(例如停用词)的较高权重。</p><h2 id="6b94" class="mp lt iq bd lu mq mr dn ly ms mt dp mc lf mu mv me lj mw mx mg ln my mz mi na bi translated">逆文档频率</h2><p id="f0f7" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">这个问题可以通过引入逆文档频率(IDF)组件来抵消在许多文档中高频出现的单词来解决。</p><p id="7ddc" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">逆文档频率可以通过以下公式导出:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nc"><img src="../Images/569570398143a788559c5fceaf162536.png" data-original-src="https://miro.medium.com/v2/resize:fit:1020/format:webp/1*mSP9ML5PtOBARVKyuUu0Eg.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">由作者创建</p></figure><p id="f503" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">有了这个统计，如果一个词出现在许多文档中，它就不会得到过高的评价。</p><p id="04de" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">可以把IDF组件看作一种平衡，确保TF组件不会高估一个单词在文档中的相关性。</p><p id="a372" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">有了单词的TF和IDF值，您现在就能够有效地量化该单词在文档中的重要性。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nd"><img src="../Images/6eab36e68d00d01a92488563226cd2ef.png" data-original-src="https://miro.medium.com/v2/resize:fit:722/format:webp/1*uwQHW2rxw6YBg3lmILS7SA.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">由作者创建</p></figure><p id="184c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">通过组合TF组件和IDF组件，可以得到TF-IDF。</p><p id="589f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">令人震惊，我知道。</p><p id="458b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">文档中某个单词的数字表示可以让我们了解该单词在文档中的重要性。如果向量中的一个单词比另一个单词具有更高的TF-IDF值，则认为它与该文档的相关性更大。</p><h2 id="d8d3" class="mp lt iq bd lu mq mr dn ly ms mt dp mc lf mu mv me lj mw mx mg ln my mz mi na bi translated">用例</h2><p id="b56c" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">TF-IDF已被证明是一种有效的文本矢量化方法，适用于许多现实场景。</p><p id="5bae" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">由于它能够量化一个单词在文档中的重要性，所以它是关键字提取的理想选择。利用TF-IDF对文档中每个单词的评估，它可以识别具有最高值的单词，并将它们视为关键词(对于文本分类或文本摘要也是有用的)。</p><p id="fbe1" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">TF-IDF通常也用于信息检索。想想你经常使用的搜索引擎。它们旨在通过基于文档与用户查询的相关性评估每个文档并返回排名最高的结果，为您提供最相关的文档。</p><h2 id="de71" class="mp lt iq bd lu mq mr dn ly ms mt dp mc lf mu mv me lj mw mx mg ln my mz mi na bi translated">限制</h2><p id="478d" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">尽管TF-IDF能够对文档中出现的单词给出很多见解，但它也有一些缺点。</p><p id="7dcb" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">首先，TF-IDF没有注意到文本中单词的顺序。仅这一点就会导致文本矢量化后丢失一些上下文。</p><p id="397d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">其次，TF-IDF没有考虑单词的语义值。每个单词都被认为是独立的。</p><p id="aebe" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">假设您正在搜索与单词“king”相关的信息。在这种情况下，TF-IDF只会考虑“王”这个词。像“统治者”或“君主”这样的同义词将被视为完全不同的实体，即使它们具有相似的语义。</p><h1 id="0206" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated"><strong class="ak">在Python中使用TF-IDF</strong></h1><p id="876f" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">一个词的TF-IDF值不难计算；这是简单的代数。您可以很容易地开发自己的矢量器，使用公式将文本转换成TF-IDF矢量。</p><p id="69b5" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">也就是说，如果您的目标是在任何NLP项目中使用TF-IDF，那么最好依赖scikit-learn模块的<a class="ae kv" href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html" rel="noopener ugc nofollow" target="_blank"> TF-IDF矢量器</a>。</p><p id="3c3a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">scikit-learn的TF-IDF矢量器可以为您执行所有繁琐的计算，但不仅仅是这样。</p><p id="5073" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">矢量器还允许您:</p><ul class=""><li id="ae44" class="ne nf iq ky b kz la lc ld lf ng lj nh ln ni lr nj nk nl nm bi translated">归一化TF-IDF值(这减轻了来自过长或过短的语料库的偏差)</li><li id="e877" class="ne nf iq ky b kz nn lc no lf np lj nq ln nr lr nj nk nl nm bi translated">选择矢量化中使用的n-gram范围(如果你需要复习，我写过一篇文章介绍了<a class="ae kv" rel="noopener" target="_blank" href="/leveraging-n-grams-to-extract-context-from-text-bdc576b47049">n-gram</a></li><li id="5869" class="ne nf iq ky b kz nn lc no lf np lj nq ln nr lr nj nk nl nm bi translated">通过仅包括词频最高的词来降低维度</li></ul><p id="9327" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这里，我们将从NLTK包的内置语料库中加载3段文本。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ns nt l"/></div></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nu"><img src="../Images/af35aad286c1fb688241fa8ea2c960e9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qB0A6V-ysknf7cmYNvrC6w.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">代码输出(由作者创建)</p></figure><p id="fa8a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">为了便于演示，让我们构建一个矢量器，它删除了停用词，考虑了单词和双词，并且只选择了10个词频最高的词。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ns nt l"/></div></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nv"><img src="../Images/ea0c38443bb2fab643118fabfd8eb4d0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*f9ra8ews6CO_QiyNU13tuA.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">代码输出(由作者创建)</p></figure><p id="9cb1" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">从输出中，您可以看到TF-IDF值的每一行是如何表示每个文档的。</p><p id="ce8e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><em class="nw">注意:选择的参数不是最优的。向量的最佳n元语法范围和维度取决于所讨论的文本，并且只能通过实验来确定。</em></p><h1 id="b319" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">结论</h1><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nx"><img src="../Images/23ae83587790241b8d160421a89a9cb2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iEGV0LbyKJ-xY4rBoSIi-A.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">照片来自<a class="ae kv" href="https://www.pexels.com/photo/black-and-white-laptop-2740956/?utm_content=attributionCopyText&amp;utm_medium=referral&amp;utm_source=pexels" rel="noopener ugc nofollow" target="_blank"> Pexels </a></p></figure><p id="5da5" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在您已经熟悉了TF-IDF是如何计算的，它的值代表什么，以及为什么它在NLP应用中如此普遍。</p><p id="15e2" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">如果您已经对这种文本矢量化方法有了深入的了解，那么您在掌握自然语言处理的道路上已经到达了一个重要的里程碑。</p><p id="fce3" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我祝你在数据科学的努力中好运！</p></div></div>    
</body>
</html>