<html>
<head>
<title>Object Detection Explained: R-CNN</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">物体探测解释:R-CNN</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/object-detection-explained-r-cnn-a6c813937a76?source=collection_archive---------3-----------------------#2021-03-20">https://towardsdatascience.com/object-detection-explained-r-cnn-a6c813937a76?source=collection_archive---------3-----------------------#2021-03-20</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><figure class="ip iq gp gr ir is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi io"><img src="../Images/c2d565567009e63585f418ec1e75c509.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oclt-WAUsyGITJ5c2MFt7w.jpeg"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated"><a class="ae jd" href="https://unsplash.com/@mattartz" rel="noopener ugc nofollow" target="_blank">马特·阿特兹</a>途经<a class="ae jd" href="https://unsplash.com/photos/2hPCkRAkVbY" rel="noopener ugc nofollow" target="_blank"> Unsplash </a></p></figure><div class=""/><div class=""><h2 id="ec8c" class="pw-subtitle-paragraph kd jf jg bd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku dk translated">基于区域的卷积神经网络</h2></div><p id="30c9" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">目标检测包括两个独立的任务，即分类和定位。R-CNN代表基于区域的卷积神经网络。R-CNN系列背后的关键概念是地区提案。区域建议用于定位图像中的对象。在接下来的博客中，我决定写一些在物体检测中使用的不同方法和架构。因此，我很高兴从基于R-CNN的物体探测器开始这次旅程。</p><h1 id="37bf" class="lr ls jg bd lt lu lv lw lx ly lz ma mb km mc kn md kp me kq mf ks mg kt mh mi bi translated">工作细节</h1><figure class="mk ml mm mn gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi mj"><img src="../Images/97ee02d1f7902b77647483cdb53f4021.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*Esmqth8McxPM2YtB.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">RCNN:工作细节。来源:【https://arxiv.org/pdf/1311.2524.pdf<a class="ae jd" href="https://arxiv.org/pdf/1311.2524.pdf" rel="noopener ugc nofollow" target="_blank"/><a class="ae jd" href="https://arxiv.org/pdf/1311.2524.pdf" rel="noopener ugc nofollow" target="_blank">。</a></p></figure><p id="d593" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">如上图所示，在将图像通过网络之前，我们需要使用选择性搜索等算法提取区域建议或感兴趣区域。然后，我们需要调整(包装)所有提取的作物，并通过网络传递它们。</p><p id="0a11" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">最后，网络从C + 1中分配一个类别，包括给定作物的“背景”标签、类别。此外，它还预测delta Xs和Ys来塑造给定的作物。</p><h1 id="c37d" class="lr ls jg bd lt lu lv lw lx ly lz ma mb km mc kn md kp me kq mf ks mg kt mh mi bi translated">提取区域建议</h1><p id="1526" class="pw-post-body-paragraph kv kw jg kx b ky mo kh la lb mp kk ld le mq lg lh li mr lk ll lm ms lo lp lq ij bi translated">选择性搜索是一种用于对象定位的区域提议算法，它基于区域的像素强度将区域分组在一起。因此，它根据相似像素的层次分组来对像素进行分组。在原始论文中，作者摘录了大约2000条建议。</p><h1 id="f15e" class="lr ls jg bd lt lu lv lw lx ly lz ma mb km mc kn md kp me kq mf ks mg kt mh mi bi translated">正面和反面的例子</h1><p id="7a11" class="pw-post-body-paragraph kv kw jg kx b ky mo kh la lb mp kk ld le mq lg lh li mr lk ll lm ms lo lp lq ij bi translated">在我们提取我们的区域提案之后，我们还必须为它们添加标签以便进行培训。因此，作者将IOU至少为0.5的所有提案标上任何基本事实边界框及其相应的类别。但是，IOU低于0.3的所有其他区域提案都被标记为背景。因此，其余的都被忽略了。</p><h1 id="2d30" class="lr ls jg bd lt lu lv lw lx ly lz ma mb km mc kn md kp me kq mf ks mg kt mh mi bi translated">包围盒回归</h1><figure class="mk ml mm mn gt is gh gi paragraph-image"><div class="gh gi mt"><img src="../Images/a15aa18ea4dcbff03c4dc4f845367d38.png" data-original-src="https://miro.medium.com/v2/resize:fit:654/format:webp/1*3XFQzZaMiirq5990312vYQ.png"/></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">包围盒回归。来源:<a class="ae jd" href="https://arxiv.org/pdf/1311.2524.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1311.2524.pdf</a><a class="ae jd" href="https://arxiv.org/pdf/1311.2524.pdf" rel="noopener ugc nofollow" target="_blank">。</a></p></figure><p id="a412" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">上图显示了CNN预测的三角洲。所以，x，y是中心坐标。而w、h分别是宽度和高度。最后，G和P分别代表地面实况包围盒和区域提议。值得注意的是，边界框丢失仅针对正样本进行计算。</p><h1 id="cf43" class="lr ls jg bd lt lu lv lw lx ly lz ma mb km mc kn md kp me kq mf ks mg kt mh mi bi translated">失败</h1><p id="3036" class="pw-post-body-paragraph kv kw jg kx b ky mo kh la lb mp kk ld le mq lg lh li mr lk ll lm ms lo lp lq ij bi translated">总损失计算为分类和回归损失的总和。但是后一个有一个系数λ，原文中是1000。注意，对于负面例子，回归损失被忽略。</p><h1 id="f957" class="lr ls jg bd lt lu lv lw lx ly lz ma mb km mc kn md kp me kq mf ks mg kt mh mi bi translated">体系结构</h1><p id="6ef7" class="pw-post-body-paragraph kv kw jg kx b ky mo kh la lb mp kk ld le mq lg lh li mr lk ll lm ms lo lp lq ij bi translated">通常，我们通过VGG 16或ResNet 50传递调整后的裁剪，以获得特征。它们随后通过输出预测的完全连接的层。</p><figure class="mk ml mm mn gt is"><div class="bz fp l di"><div class="mu mv l"/></div></figure><p id="3645" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">如果你想看完整的代码，你可以很容易地在我的<a class="ae jd" href="https://github.com/chingisooinar/Object-Detection_from-Scratch/blob/main/RCNN/RCNN.ipynb" rel="noopener ugc nofollow" target="_blank"> GitHub上找到一个木星笔记本。</a></p><h1 id="c6ad" class="lr ls jg bd lt lu lv lw lx ly lz ma mb km mc kn md kp me kq mf ks mg kt mh mi bi translated">一些遗言</h1><figure class="mk ml mm mn gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi mw"><img src="../Images/ca345a8b746abe60d5f2f56074b3f5c5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Z-krT0ffqDJAs_giIyotXg.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated"><a class="ae jd" href="https://github.com/matterport/Mask_RCNN/releases" rel="noopener ugc nofollow" target="_blank">气球数据集</a></p></figure><p id="2a9f" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我只训练了它5个时期，所以你可以看到它能够检测到图像中的一些气球。为什么不再使用它有几个缺点。最大的缺点是用于提议提取的选择性搜索算法。考虑到算法是在cpu上执行的，推理时间变得很慢。此外，所有提案都必须调整大小并通过网络传递，这也增加了开销。因此，我将写一些其他的算法来克服这些问题。</p><h1 id="e31f" class="lr ls jg bd lt lu lv lw lx ly lz ma mb km mc kn md kp me kq mf ks mg kt mh mi bi translated">纸</h1><p id="701d" class="pw-post-body-paragraph kv kw jg kx b ky mo kh la lb mp kk ld le mq lg lh li mr lk ll lm ms lo lp lq ij bi translated"><a class="ae jd" href="https://arxiv.org/pdf/1311.2524.pdf" rel="noopener ugc nofollow" target="_blank">丰富的特征层次，用于精确的对象检测和语义分割</a></p><h1 id="7cc5" class="lr ls jg bd lt lu lv lw lx ly lz ma mb km mc kn md kp me kq mf ks mg kt mh mi bi translated">相关文章</h1><div class="ip iq gp gr ir mx"><a rel="noopener follow" target="_blank" href="/r-cnn-fast-r-cnn-faster-r-cnn-yolo-object-detection-algorithms-36d53571365e"><div class="my ab fo"><div class="mz ab na cl cj nb"><h2 class="bd jh gy z fp nc fr fs nd fu fw jf bi translated">R-CNN，快速R-CNN，更快R-CNN，YOLO —目标检测算法</h2><div class="ne l"><h3 class="bd b gy z fp nc fr fs nd fu fw dk translated">了解对象检测算法</h3></div><div class="nf l"><p class="bd b dl z fp nc fr fs nd fu fw dk translated">towardsdatascience.com</p></div></div><div class="ng l"><div class="nh l ni nj nk ng nl ix mx"/></div></div></a></div><div class="ip iq gp gr ir mx"><a href="https://medium.com/dataseries/understanding-selective-search-for-object-detection-3f38709067d7" rel="noopener follow" target="_blank"><div class="my ab fo"><div class="mz ab na cl cj nb"><h2 class="bd jh gy z fp nc fr fs nd fu fw jf bi translated">了解用于对象检测的选择性搜索</h2><div class="ne l"><h3 class="bd b gy z fp nc fr fs nd fu fw dk translated">用Python实现</h3></div><div class="nf l"><p class="bd b dl z fp nc fr fs nd fu fw dk translated">medium.com</p></div></div><div class="ng l"><div class="nm l ni nj nk ng nl ix mx"/></div></div></a></div></div></div>    
</body>
</html>