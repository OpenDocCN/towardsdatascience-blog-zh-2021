<html>
<head>
<title>What is Neural Architecture Search? And Why Should You Care?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">什么是神经架构搜索？你为什么要在乎？</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/what-is-neural-architecture-search-and-why-should-you-care-1e22393de461?source=collection_archive---------9-----------------------#2021-09-18">https://towardsdatascience.com/what-is-neural-architecture-search-and-why-should-you-care-1e22393de461?source=collection_archive---------9-----------------------#2021-09-18</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="c367" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">由算法创建的神经网络</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/1077cdf768c72c08d3643bb108fd67e0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*5tb6B2oNd68-iPUA"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><a class="ae ky" href="https://unsplash.com/@cdc?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">疾控中心</a>在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>拍摄的照片</p></figure><h1 id="6d5f" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">深度学习的民主化</h1><p id="01a2" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated"><strong class="lt iu">深度学习</strong>模型的使用每天都变得越来越<strong class="lt iu">民主</strong>，在<strong class="lt iu">很多行业</strong>变得不可或缺。然而，有效的神经网络的实现通常需要建筑工程的背景和大量的时间来在迭代过程中探索我们知识的全部解决方案。神经网络的形式和结构将根据特定的需要而变化。因此，有必要根据给定的需求设计特定的架构。以试错的方式设计这些网络是一项乏味的任务，需要<strong class="lt iu">架构工程技能和领域专业知识</strong>。专家利用他们过去的经验或技术知识来创建和设计神经网络。这意味着潜在使用和评估的体系结构集合将减少到专家已知的那些。</p><h1 id="9011" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">迫切需要效率和优化</h1><p id="2ab2" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">在一些行业中，神经网络的效率是至关重要的。为了使神经网络一般化并且不过度适应训练数据集，找到<strong class="lt iu">优化的架构</strong>是很重要的。然而，在生产力比质量更重要的时代，一些行业<strong class="lt iu">忽视了他们的模式</strong>的效率，并且满足于第一个实现他们目标的模式，而没有更进一步。寻找合适的架构是一项<strong class="lt iu">耗时</strong>且<strong class="lt iu">易出错的任务</strong>并且需要架构设计技能。由于缺乏时间或架构专业知识，这些行业没有通过“足够”的模型充分挖掘其数据的潜力。</p><p id="3094" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">本文旨在展示<strong class="lt iu">神经架构搜索(NAS) </strong>的进展，它面临的困难和提出的解决方案，以及<strong class="lt iu"> NAS </strong>现在的受欢迎程度和未来趋势。</p></div><div class="ab cl ms mt hx mu" role="separator"><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx"/></div><div class="im in io ip iq"><h1 id="fa9e" class="kz la it bd lb lc mz le lf lg na li lj jz nb ka ll kc nc kd ln kf nd kg lp lq bi translated">如何理解神经架构搜索的复杂性</h1><p id="0f68" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated"><strong class="lt iu">神经架构搜索</strong>旨在为满足特定需求的神经网络发现最佳架构。NAS本质上是一个人手动调整神经网络并学习什么工作得好的过程，并自动完成这项任务以发现更复杂的体系结构。这个领域代表了一套工具和方法，这些工具和方法将<strong class="lt iu">使用搜索策略测试和评估搜索空间中的大量架构，并通过最大化适应度函数</strong>来选择最符合给定问题 <strong class="lt iu">的目标的架构。</strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ne"><img src="../Images/a87b469819de554192fae93c62dd030b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5RGSnCFQ3H7dJ-BSFFmjoQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><a class="ae ky" href="https://medium.com/digital-catapult/neural-architecture-search-the-foundations-a6cc85f7562" rel="noopener">参考文献</a> —神经架构搜索概述</p></figure><p id="1a69" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">NAS是<strong class="lt iu"> AutoML </strong>的一个子领域，它封装了所有自动化机器学习问题的过程，因此也是深度学习问题。随着<a class="ae ky" href="https://arxiv.org/abs/1611.01578" rel="noopener ugc nofollow" target="_blank"> <em class="nf"> Zoph和Le </em> </a>或<a class="ae ky" href="https://arxiv.org/abs/1611.02167" rel="noopener ugc nofollow" target="_blank"> <em class="nf"> Baker和al </em> </a>的工作，2016年标志着NAS的开始，他们利用强化学习算法实现了最先进的图像识别和语言建模架构。这项工作极大地推动了这一领域的发展。</p><h2 id="b807" class="ng la it bd lb nh ni dn lf nj nk dp lj ma nl nm ll me nn no ln mi np nq lp nr bi translated">著名项目</h2><p id="5181" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">神经结构搜索是机器学习中发展最快的领域之一。在不同的行业和不同的问题中，大量的研究工作涉及对神经网络结构的搜索的自动化。如今，许多手动体系结构已经被NAS构建的体系结构取代:</p><ul class=""><li id="e4e9" class="ns nt it lt b lu mn lx mo ma nu me nv mi nw mm nx ny nz oa bi translated">目标检测—图像处理— <a class="ae ky" href="https://arxiv.org/pdf/1611.01578.pdf" rel="noopener ugc nofollow" target="_blank"> Zoph和al 2017 </a></li><li id="f9e2" class="ns nt it lt b lu ob lx oc ma od me oe mi of mm nx ny nz oa bi translated">图像分类—图像处理— <a class="ae ky" href="https://arxiv.org/pdf/1802.01548.pdf" rel="noopener ugc nofollow" target="_blank"> Real和al，201 </a> 9</li><li id="6c21" class="ns nt it lt b lu ob lx oc ma od me oe mi of mm nx ny nz oa bi translated">超参数优化— AutoML — <a class="ae ky" href="https://www.automl.org/wp-content/uploads/2019/05/AutoML_Book_Chapter1.pdf" rel="noopener ugc nofollow" target="_blank">福雷尔和胡特，2019 </a></li><li id="c5d1" class="ns nt it lt b lu ob lx oc ma od me oe mi of mm nx ny nz oa bi translated">元学习— AutoML — <a class="ae ky" href="https://arxiv.org/pdf/1810.03548.pdf" rel="noopener ugc nofollow" target="_blank"> Vanchoren，2018 </a></li></ul><p id="075a" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">最近在NAS方面的工作表明，该领域正在全面扩展并成为趋势。2019年和2020年标志着正在进行的研究数量的加速。虽然早期的工作可以被认为是概念的证明，但当前的研究正在解决跨<strong class="lt iu">几个行业</strong>和<strong class="lt iu">研究领域</strong>的更具体需求。这一趋势显示了NAS所能带来的潜力，不仅体现在它的效率和<strong class="lt iu">适应任何类型问题的能力</strong>，还体现在工程师在非自动化任务上节省的时间。</p><h2 id="aef3" class="ng la it bd lb nh ni dn lf nj nk dp lj ma nl nm ll me nn no ln mi np nq lp nr bi translated">神经架构搜索的好处要不要投资？局限性呢？</h2><p id="a074" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">NAS方法探索<strong class="lt iu">许多具有可变复杂性的潜在解决方案</strong>，因此<strong class="lt iu">在计算上非常昂贵</strong>。他们的搜索空间越大，需要测试、训练和评估的架构就越多。这些方法需要大量的资源和时间来找到一个足够好的模型。他们基于强化学习的NAS方法Zoph等人用了800个NVIDIA K40 GPUs，用了28天。自从第一种方法以来，新的模型已经出现，搜索时间大大缩短。例如，<a class="ae ky" href="https://arxiv.org/pdf/1712.00559.pdf" rel="noopener ugc nofollow" target="_blank">渐进式神经架构搜索</a>展示了类似的最先进的结果，搜索速度提高了5-8倍。<a class="ae ky" href="https://arxiv.org/pdf/1802.03268.pdf" rel="noopener ugc nofollow" target="_blank">高效的神经架构搜索</a>大约需要7个小时才能找到该架构，与NAS相比，GPU时数减少了50，000倍以上。</p><p id="bbfa" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">然而，这一领域受到其他几个限制。事实上，很难知道一个潜在的模型在真实数据上的表现如何。由于<strong class="lt iu">架构是用训练数据</strong>评估的，如果我们期望在真实数据上有一个执行模型，后者必须是高质量的。<br/>仍有必要定义算法将如何找到并评估这些架构。这个任务还是手工完成，需要微调。然而，领域知识的缺乏不会降低架构的效率。这些知识有助于加速搜索过程，它将指导搜索，从而算法将更快地收敛到最优解。</p><p id="8da2" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">最近的算法，如PNAS方法，试图<strong class="lt iu">近似未来性能</strong>，但这些预测器必须微调，仍然是近似值。此外，某些方法受到<strong class="lt iu">鲁棒性问题</strong>的困扰，很难训练。</p><p id="8e5b" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">一些实际的研究现在集中在使用<a class="ae ky" href="https://arxiv.org/abs/1905.07350" rel="noopener ugc nofollow" target="_blank"> <strong class="lt iu">生物启发算法</strong> </a>作为NAS方法。这些算法对于<strong class="lt iu">优化任务</strong>非常有效，因此似乎是寻找神经网络最佳架构的理想候选。</p></div><div class="ab cl ms mt hx mu" role="separator"><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx"/></div><div class="im in io ip iq"><p id="f1e4" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">带走:</p><ul class=""><li id="0b76" class="ns nt it lt b lu mn lx mo ma nu me nv mi nw mm nx ny nz oa bi translated">NAS从大量备选方案中找到理想的解决方案，并选择最符合给定问题目标的方案</li><li id="d0dc" class="ns nt it lt b lu ob lx oc ma od me oe mi of mm nx ny nz oa bi translated">优化算法</li><li id="5195" class="ns nt it lt b lu ob lx oc ma od me oe mi of mm nx ny nz oa bi translated">生物灵感</li><li id="6a28" class="ns nt it lt b lu ob lx oc ma od me oe mi of mm nx ny nz oa bi translated">计算成本非常高</li><li id="c101" class="ns nt it lt b lu ob lx oc ma od me oe mi of mm nx ny nz oa bi translated">很难估计它在真实数据中的表现</li></ul><h1 id="b0cb" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">结论</h1><p id="25fe" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">在一个<strong class="lt iu">优化</strong>和<strong class="lt iu">性能</strong>至关重要的时代，神经架构搜索是一个<strong class="lt iu">快速扩展的领域</strong>。这个非常新的领域仍然面临一些困难，以成为行业中深度学习项目设计的成熟步骤。然而，最近的工作表明，随着更快和更完整的体系结构评估方法的出现，这些困难将在未来几年消失。领域知识的贡献将不再是不可或缺的，而是提高研究方法效率的优势。因此，NAS将为行业和公司带来更多的灵活性，这些工具能够适应多种特定需求。</p><p id="5ef1" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">🚀- <a class="og oh ep" href="https://medium.com/u/52eb891e229c?source=post_page-----1e22393de461--------------------------------" rel="noopener" target="_blank">圣甲虫的消息</a></p><div class="oi oj gp gr ok ol"><a href="https://medium.com/@tgey/about-me-thomas-gey-a8a42f6622cc" rel="noopener follow" target="_blank"><div class="om ab fo"><div class="on ab oo cl cj op"><h2 class="bd iu gy z fp oq fr fs or fu fw is bi translated">关于我——圣甲虫的新闻</h2><div class="os l"><h3 class="bd b gy z fp oq fr fs or fu fw dk translated">欢迎页面</h3></div><div class="ot l"><p class="bd b dl z fp oq fr fs or fu fw dk translated">medium.com</p></div></div><div class="ou l"><div class="ov l ow ox oy ou oz ks ol"/></div></div></a></div><div class="oi oj gp gr ok ol"><a href="https://medium.com/@ScarabNews/membership" rel="noopener follow" target="_blank"><div class="om ab fo"><div class="on ab oo cl cj op"><h2 class="bd iu gy z fp oq fr fs or fu fw is bi translated">用我的推荐链接加入媒体-圣甲虫的新闻</h2><div class="os l"><h3 class="bd b gy z fp oq fr fs or fu fw dk translated">作为一个媒体会员，你的会员费的一部分会给你阅读的作家，你可以完全接触到每一个故事…</h3></div><div class="ot l"><p class="bd b dl z fp oq fr fs or fu fw dk translated">medium.com</p></div></div><div class="ou l"><div class="pa l ow ox oy ou oz ks ol"/></div></div></a></div><div class="oi oj gp gr ok ol"><a href="https://medium.com/subscribe/@ScarabNews" rel="noopener follow" target="_blank"><div class="om ab fo"><div class="on ab oo cl cj op"><h2 class="bd iu gy z fp oq fr fs or fu fw is bi translated">每当圣甲虫的新闻出版的时候得到一封电子邮件。</h2><div class="os l"><h3 class="bd b gy z fp oq fr fs or fu fw dk translated">每当圣甲虫的新闻出版的时候得到一封电子邮件。通过注册，您将创建一个中型帐户，如果您还没有…</h3></div><div class="ot l"><p class="bd b dl z fp oq fr fs or fu fw dk translated">medium.com</p></div></div><div class="ou l"><div class="pb l ow ox oy ou oz ks ol"/></div></div></a></div><p id="0cba" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><em class="nf">感谢阅读本文！如果你有任何问题，请在下面留言。</em></p><p id="cb4a" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><em class="nf">参考文献:</em></p><ul class=""><li id="252e" class="ns nt it lt b lu mn lx mo ma nu me nv mi nw mm nx ny nz oa bi translated"><a class="ae ky" href="https://arxiv.org/pdf/1611.01578.pdf" rel="noopener ugc nofollow" target="_blank"> <em class="nf">带强化学习的神经架构搜索</em> </a> <em class="nf"> — Zoph和al，2017 </em></li><li id="7959" class="ns nt it lt b lu ob lx oc ma od me oe mi of mm nx ny nz oa bi translated"><a class="ae ky" href="https://arxiv.org/pdf/1611.02167.pdf" rel="noopener ugc nofollow" target="_blank"> <em class="nf">利用强化学习设计神经网络架构</em> </a> <em class="nf"> — Baker and al，2017 </em></li><li id="6cae" class="ns nt it lt b lu ob lx oc ma od me oe mi of mm nx ny nz oa bi translated"><a class="ae ky" href="https://arxiv.org/pdf/1810.03548.pdf" rel="noopener ugc nofollow" target="_blank"> <em class="nf">元学习，一项调查</em></a><em class="nf">—Joaquin vans choren，2018 </em></li><li id="557a" class="ns nt it lt b lu ob lx oc ma od me oe mi of mm nx ny nz oa bi translated"><a class="ae ky" href="https://arxiv.org/pdf/1712.00559.pdf" rel="noopener ugc nofollow" target="_blank"> <em class="nf">渐进式神经架构搜索</em> </a> <em class="nf"> —刘等，2018 </em></li><li id="c112" class="ns nt it lt b lu ob lx oc ma od me oe mi of mm nx ny nz oa bi translated"><a class="ae ky" href="https://arxiv.org/pdf/1802.03268.pdf" rel="noopener ugc nofollow" target="_blank"> <em class="nf">通过参数共享的高效神经架构搜索</em> </a> <em class="nf"> — Pham和al，2018 </em></li><li id="342c" class="ns nt it lt b lu ob lx oc ma od me oe mi of mm nx ny nz oa bi translated"><a class="ae ky" href="https://arxiv.org/pdf/1802.01548.pdf" rel="noopener ugc nofollow" target="_blank"> <em class="nf">图像分类器架构搜索的正则化进化</em> </a> <em class="nf"> —Real and al，2019 </em></li><li id="890b" class="ns nt it lt b lu ob lx oc ma od me oe mi of mm nx ny nz oa bi translated"><a class="ae ky" href="https://www.automl.org/wp-content/uploads/2019/05/AutoML_Book_Chapter1.pdf" rel="noopener ugc nofollow" target="_blank"> <em class="nf">超参数优化</em> </a> <em class="nf"> —马蒂亚斯·福雷尔和弗兰克·赫特，2019 </em></li><li id="34eb" class="ns nt it lt b lu ob lx oc ma od me oe mi of mm nx ny nz oa bi translated"><a class="ae ky" href="https://arxiv.org/pdf/1905.07350.pdf" rel="noopener ugc nofollow" target="_blank"> <em class="nf"> DeepSwarm:利用群体智能优化卷积神经网络</em> </a> <em class="nf"> — Byla and Pang，2019 </em></li></ul></div></div>    
</body>
</html>