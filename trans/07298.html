<html>
<head>
<title>Three Tricks to Speed Up and Optimise Your Python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">加快和优化Python的三个技巧</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/three-tricks-to-speed-up-and-optimise-your-python-d9b5d49d68a6?source=collection_archive---------26-----------------------#2021-07-02">https://towardsdatascience.com/three-tricks-to-speed-up-and-optimise-your-python-d9b5d49d68a6?source=collection_archive---------26-----------------------#2021-07-02</a></blockquote><div><div class="fc ik il im in io"/><div class="ip iq ir is it"><h2 id="6474" class="iu iv iw bd b dl ix iy iz ja jb jc dk jd translated" aria-label="kicker paragraph">数据科学讨论</h2><div class=""/><div class=""><h2 id="09f2" class="pw-subtitle-paragraph kc jf iw bd b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt dk translated">关于我在六月份的阅读中发现的三个Python技巧的评论。</h2></div><figure class="kv kw kx ky gu kz gi gj paragraph-image"><div role="button" tabindex="0" class="la lb di lc bf ld"><div class="gi gj ku"><img src="../Images/f6f008b3c74ab29349108167e6915ff2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6b-xJqCpnnMVKSVyoYS6lw.jpeg"/></div></div><p class="lg lh gk gi gj li lj bd b be z dk translated">图片来自<a class="ae lk" href="https://pixabay.com/?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=3151762" rel="noopener ugc nofollow" target="_blank"> Pixabay </a>的<a class="ae lk" href="https://pixabay.com/users/pasja1000-6355831/?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=3151762" rel="noopener ugc nofollow" target="_blank"> pasja1000 </a></p></figure><p id="785b" class="pw-post-body-paragraph ll lm iw ln b lo lp kg lq lr ls kj lt lu lv lw lx ly lz ma mb mc md me mf mg ip bi translated">科学家需要保持每一项数据的最新状态:他们每天都应该读、读、再读。</p><blockquote class="mh mi mj"><p id="83bf" class="ll lm mk ln b lo lp kg lq lr ls kj lt ml lv lw lx mm lz ma mb mn md me mf mg ip bi translated">没有人生来就受过教育！</p></blockquote><p id="9678" class="pw-post-body-paragraph ll lm iw ln b lo lp kg lq lr ls kj lt lu lv lw lx ly lz ma mb mc md me mf mg ip bi translated">让自己保持最新状态的一个可能策略是注册Twitter并<strong class="ln jg">关注最有影响力的数据科学家</strong>，以及订阅<strong class="ln jg">时事通讯</strong>并加入与数据科学相关的<strong class="ln jg">团体</strong>。</p><p id="ff2e" class="pw-post-body-paragraph ll lm iw ln b lo lp kg lq lr ls kj lt lu lv lw lx ly lz ma mb mc md me mf mg ip bi translated">在我六月份的发现中，我发现了三个有趣的Python包，它们允许加速操作和减少内存使用。</p><p id="011e" class="pw-post-body-paragraph ll lm iw ln b lo lp kg lq lr ls kj lt lu lv lw lx ly lz ma mb mc md me mf mg ip bi translated">事实上，每个人迟早都会碰到<strong class="ln jg">执行操作</strong>慢的问题。事实上，我们经常面临管理<strong class="ln jg">相当大的数据集</strong>，这需要很长的计算时间。</p><p id="2b23" class="pw-post-body-paragraph ll lm iw ln b lo lp kg lq lr ls kj lt lu lv lw lx ly lz ma mb mc md me mf mg ip bi translated">发现的三个包如下:</p><ul class=""><li id="301c" class="mo mp iw ln b lo lp lr ls lu mq ly mr mc ms mg mt mu mv mw bi translated">农巴</li><li id="c775" class="mo mp iw ln b lo mx lr my lu mz ly na mc nb mg mt mu mv mw bi translated">PySpark</li><li id="3ae8" class="mo mp iw ln b lo mx lr my lu mz ly na mc nb mg mt mu mv mw bi translated">熊猫(事实上我已经知道了…)，有一些技巧。</li></ul><h1 id="ace7" class="nc nd iw bd ne nf ng nh ni nj nk nl nm kl nn km no ko np kp nq kr nr ks ns nt bi translated">农巴</h1><p id="f532" class="pw-post-body-paragraph ll lm iw ln b lo nu kg lq lr nv kj lt lu nw lw lx ly nx ma mb mc ny me mf mg ip bi translated">据<a class="ae lk" href="http://numba.pydata.org/" rel="noopener ugc nofollow" target="_blank">Numba官方网站</a>介绍，Numba是一款<em class="mk">开源JIT编译器，将Python和NumPy代码的子集翻译成快速机器码。</em>这意味着Numba允许<strong class="ln jg">加速一些科学计算的操作</strong>。</p><p id="c9cd" class="pw-post-body-paragraph ll lm iw ln b lo lp kg lq lr ls kj lt lu lv lw lx ly lz ma mb mc md me mf mg ip bi translated">Numba可以通过以下命令经由<code class="fe nz oa ob oc b">pip</code>安装:</p><pre class="kv kw kx ky gu od oc oe of aw og bi"><span id="cd30" class="oh nd iw oc b gz oi oj l ok ol">pip install numba</span></pre><p id="a0a6" class="pw-post-body-paragraph ll lm iw ln b lo lp kg lq lr ls kj lt lu lv lw lx ly lz ma mb mc md me mf mg ip bi translated">一旦安装完毕，它的用法非常简单:只需导入<code class="fe nz oa ob oc b">numba</code>库，然后在函数前加上下面的命令:<code class="fe nz oa ob oc b"><a class="ae lk" href="http://twitter.com/numba" rel="noopener ugc nofollow" target="_blank">@numba</a>.jit()</code>。</p><p id="4bee" class="pw-post-body-paragraph ll lm iw ln b lo lp kg lq lr ls kj lt lu lv lw lx ly lz ma mb mc md me mf mg ip bi translated">以下示例显示了如何在没有<code class="fe nz oa ob oc b">numba</code>的情况下计算前N个数的总和:</p><pre class="kv kw kx ky gu od oc oe of aw og bi"><span id="86c8" class="oh nd iw oc b gz oi oj l ok ol"># without numba<br/>def sum_of_n(nsamples):<br/>    total = 0<br/>    for i in range(nsamples):<br/>        total += i<br/>    return total</span></pre><p id="800a" class="pw-post-body-paragraph ll lm iw ln b lo lp kg lq lr ls kj lt lu lv lw lx ly lz ma mb mc md me mf mg ip bi translated">和<code class="fe nz oa ob oc b">numba</code>一起:</p><pre class="kv kw kx ky gu od oc oe of aw og bi"><span id="3fe8" class="oh nd iw oc b gz oi oj l ok ol">import numba</span><span id="3eb5" class="oh nd iw oc b gz om oj l ok ol"><a class="ae lk" href="http://twitter.com/numba" rel="noopener ugc nofollow" target="_blank">@numba</a>.jit()<br/>def sum_of_n(nsamples):<br/>    total = 0<br/>    for i in range(nsamples):<br/>        total += i<br/>    return total</span></pre><p id="618f" class="pw-post-body-paragraph ll lm iw ln b lo lp kg lq lr ls kj lt lu lv lw lx ly lz ma mb mc md me mf mg ip bi translated">下图显示了在有和没有<code class="fe nz oa ob oc b">numba</code>的情况下计算前N个数之和所用的时间，N从1000到10，000，000不等:</p><figure class="kv kw kx ky gu kz gi gj paragraph-image"><div role="button" tabindex="0" class="la lb di lc bf ld"><div class="gi gj on"><img src="../Images/8ea572b86aaac988f1a9a6a054758980.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JXmO-IOOOBEZQr6rowsakQ.png"/></div></div><p class="lg lh gk gi gj li lj bd b be z dk translated">作者图片</p></figure><p id="b870" class="pw-post-body-paragraph ll lm iw ln b lo lp kg lq lr ls kj lt lu lv lw lx ly lz ma mb mc md me mf mg ip bi translated">有趣的是，当N增加时，使用<code class="fe nz oa ob oc b">numba</code>的计算时间并没有增加。</p><p id="7cf3" class="pw-post-body-paragraph ll lm iw ln b lo lp kg lq lr ls kj lt lu lv lw lx ly lz ma mb mc md me mf mg ip bi translated">这个计算的完整代码可以从<a class="ae lk" href="https://github.com/alod83/data-science/blob/master/Preprocessing/SpeedUp/Numba.ipynb" rel="noopener ugc nofollow" target="_blank">我的Github库</a>下载。</p><p id="34b2" class="pw-post-body-paragraph ll lm iw ln b lo lp kg lq lr ls kj lt lu lv lw lx ly lz ma mb mc md me mf mg ip bi translated">关于<code class="fe nz oa ob oc b">numba</code>的更多细节，你可以阅读下面这篇文章，它是我的灵感来源:</p><div class="oo op gq gs oq or"><a href="https://www.infoworld.com/article/3622013/speed-up-your-python-with-numba.html" rel="noopener  ugc nofollow" target="_blank"><div class="os ab fp"><div class="ot ab ou cl cj ov"><h2 class="bd jg gz z fq ow fs ft ox fv fx jf bi translated">用Numba加速你的Python</h2><div class="oy l"><h3 class="bd b gz z fq ow fs ft ox fv fx dk translated">Python不是最快的语言，但是速度的不足并没有阻止它成为分析领域的主要力量…</h3></div><div class="oz l"><p class="bd b dl z fq ow fs ft ox fv fx dk translated">www.infoworld.com</p></div></div><div class="pa l"><div class="pb l pc pd pe pa pf le or"/></div></div></a></div><h1 id="0453" class="nc nd iw bd ne nf ng nh ni nj nk nl nm kl nn km no ko np kp nq kr nr ks ns nt bi translated">PySpark</h1><p id="dac9" class="pw-post-body-paragraph ll lm iw ln b lo nu kg lq lr nv kj lt lu nw lw lx ly nx ma mb mc ny me mf mg ip bi translated">PySpark是Apache Spark的Python接口，Apache Spark是一个开源项目，可以将代码速度提高100倍。关于PySpark安装的细节可以在我之前的<a class="ae lk" rel="noopener" target="_blank" href="/how-to-speed-up-your-python-code-through-pyspark-e3296e39da6">帖子</a>中找到。</p><p id="ae8f" class="pw-post-body-paragraph ll lm iw ln b lo lp kg lq lr ls kj lt lu lv lw lx ly lz ma mb mc md me mf mg ip bi translated">为了测试PySpark的性能，我计算了用Pandas和PySpark加载数据集所用的时间，数据集大小是可变的。这个计算的完整代码可以在我的<a class="ae lk" href="https://github.com/alod83/data-science/blob/master/Preprocessing/SpeedUp/PySpark.ipynb" rel="noopener ugc nofollow" target="_blank"> Github库</a>中找到。</p><p id="0990" class="pw-post-body-paragraph ll lm iw ln b lo lp kg lq lr ls kj lt lu lv lw lx ly lz ma mb mc md me mf mg ip bi translated">下图显示了结果:</p><figure class="kv kw kx ky gu kz gi gj paragraph-image"><div role="button" tabindex="0" class="la lb di lc bf ld"><div class="gi gj pg"><img src="../Images/5f7ac86796137e92ae12d999767381ca.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7A31eZVymJrM0lPs-hFZtg.png"/></div></div><p class="lg lh gk gi gj li lj bd b be z dk translated">作者图片</p></figure><p id="6d99" class="pw-post-body-paragraph ll lm iw ln b lo lp kg lq lr ls kj lt lu lv lw lx ly lz ma mb mc md me mf mg ip bi translated">有趣的是，Pandas总是花费相同的时间来加载不同大小的数据集，而PySpark对于小数据集非常快。</p><p id="9d9f" class="pw-post-body-paragraph ll lm iw ln b lo lp kg lq lr ls kj lt lu lv lw lx ly lz ma mb mc md me mf mg ip bi translated">如果你想深化PySpark库，可以看看下面这篇文章，从中我得到了启发:</p><div class="oo op gq gs oq or"><a rel="noopener follow" target="_blank" href="/a-hands-on-demo-of-analyzing-big-data-with-spark-68cb6600a295"><div class="os ab fp"><div class="ot ab ou cl cj ov"><h2 class="bd jg gz z fq ow fs ft ox fv fx jf bi translated">使用Spark分析大数据的实践演示</h2><div class="oy l"><h3 class="bd b gz z fq ow fs ft ox fv fx dk translated">扫描一部小说，计算圆周率，对五千万行进行回归</h3></div><div class="oz l"><p class="bd b dl z fq ow fs ft ox fv fx dk translated">towardsdatascience.com</p></div></div><div class="pa l"><div class="ph l pc pd pe pa pf le or"/></div></div></a></div><h1 id="7026" class="nc nd iw bd ne nf ng nh ni nj nk nl nm kl nn km no ko np kp nq kr nr ks ns nt bi translated">熊猫优化</h1><p id="4b30" class="pw-post-body-paragraph ll lm iw ln b lo nu kg lq lr nv kj lt lu nw lw lx ly nx ma mb mc ny me mf mg ip bi translated">Python Pandas是一个非常流行的管理数据集的库。我相信几乎所有的Python数据科学家都知道熊猫。然而并不是所有人(包括我，在看<a class="ae lk" rel="noopener" target="_blank" href="/optimize-pandas-memory-usage-while-reading-large-datasets-1b047c762c9b">这篇文章</a>之前，感叹！)知道如何优化Pandas在大型数据集<strong class="ln jg">内存使用方面的性能</strong>。</p><p id="1392" class="pw-post-body-paragraph ll lm iw ln b lo lp kg lq lr ls kj lt lu lv lw lx ly lz ma mb mc md me mf mg ip bi translated">减少大型数据集内存使用的基本思想是通过观察列值的范围来降低列数据类型的等级。例如，假设可能值的数量是255，一个u <code class="fe nz oa ob oc b">int32</code>可以降级为<code class="fe nz oa ob oc b">uint8</code>。</p><p id="a22f" class="pw-post-body-paragraph ll lm iw ln b lo lp kg lq lr ls kj lt lu lv lw lx ly lz ma mb mc md me mf mg ip bi translated">为了测试如何通过从<code class="fe nz oa ob oc b">int64</code>降级到<code class="fe nz oa ob oc b">int8</code>来减少数据集的大小，我构建了以下数据集，其中包含3个类型为<code class="fe nz oa ob oc b">int64</code>的列和可变数量的记录(从1000到100，000)。然后我把每一列的类型缩减为<code class="fe nz oa ob oc b">int8</code>。下图显示了在类型<code class="fe nz oa ob oc b">int64</code>和<code class="fe nz oa ob oc b">int8</code>的情况下，不同记录数的内存使用情况:</p><figure class="kv kw kx ky gu kz gi gj paragraph-image"><div role="button" tabindex="0" class="la lb di lc bf ld"><div class="gi gj pi"><img src="../Images/dd93bc3bcf09dbd3785f3e4bd642541b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VNl_OPVyu-c10Hw3NQkHmA.png"/></div></div><p class="lg lh gk gi gj li lj bd b be z dk translated">作者图片</p></figure><p id="f32a" class="pw-post-body-paragraph ll lm iw ln b lo lp kg lq lr ls kj lt lu lv lw lx ly lz ma mb mc md me mf mg ip bi translated">在数据类型为<code class="fe nz oa ob oc b">int8</code>的情况下，内存使用量非常小。要了解更多详情，您可以阅读全文:</p><div class="oo op gq gs oq or"><a rel="noopener follow" target="_blank" href="/optimize-pandas-memory-usage-while-reading-large-datasets-1b047c762c9b"><div class="os ab fp"><div class="ot ab ou cl cj ov"><h2 class="bd jg gz z fq ow fs ft ox fv fx jf bi translated">为大型数据集优化Pandas的内存使用</h2><div class="oy l"><h3 class="bd b gz z fq ow fs ft ox fv fx dk translated">有效利用数据类型来防止内存崩溃</h3></div><div class="oz l"><p class="bd b dl z fq ow fs ft ox fv fx dk translated">towardsdatascience.com</p></div></div><div class="pa l"><div class="pj l pc pd pe pa pf le or"/></div></div></a></div><h1 id="7d7d" class="nc nd iw bd ne nf ng nh ni nj nk nl nm kl nn km no ko np kp nq kr nr ks ns nt bi translated">摘要</h1><p id="4dfd" class="pw-post-body-paragraph ll lm iw ln b lo nu kg lq lr nv kj lt lu nw lw lx ly nx ma mb mc ny me mf mg ip bi translated">在这篇文章中，我修改了我在6月份的阅读中发现的三个技巧，并得出以下推论:</p><ul class=""><li id="d4ea" class="mo mp iw ln b lo lp lr ls lu mq ly mr mc ms mg mt mu mv mw bi translated">如果你需要运行科学计算，你可以利用<code class="fe nz oa ob oc b">numba</code>包</li><li id="4569" class="mo mp iw ln b lo mx lr my lu mz ly na mc nb mg mt mu mv mw bi translated">如果需要处理大型数据集，可以利用<code class="fe nz oa ob oc b">pyspark</code>包，或者尽可能降低columns数据类型。</li></ul><p id="54f4" class="pw-post-body-paragraph ll lm iw ln b lo lp kg lq lr ls kj lt lu lv lw lx ly lz ma mb mc md me mf mg ip bi translated">如果你想了解我的研究和其他活动的最新情况，你可以在<a class="ae lk" href="https://twitter.com/alod83" rel="noopener ugc nofollow" target="_blank"> Twitter </a>、<a class="ae lk" href="https://www.youtube.com/channel/UC4O8-FtQqGIsgDW_ytXIWOg?view_as=subscriber" rel="noopener ugc nofollow" target="_blank"> Youtube </a>和<a class="ae lk" href="https://github.com/alod83" rel="noopener ugc nofollow" target="_blank"> Github </a>上关注我。</p><h1 id="c5e7" class="nc nd iw bd ne nf ng nh ni nj nk nl nm kl nn km no ko np kp nq kr nr ks ns nt bi translated">相关文章</h1><div class="oo op gq gs oq or"><a rel="noopener follow" target="_blank" href="/how-to-speed-up-your-python-code-through-pyspark-e3296e39da6"><div class="os ab fp"><div class="ot ab ou cl cj ov"><h2 class="bd jg gz z fq ow fs ft ox fv fx jf bi translated">如何通过PySpark加速您的Python代码</h2><div class="oy l"><h3 class="bd b gz z fq ow fs ft ox fv fx dk translated">关于如何安装和运行Apache Spark和PySpark以提高代码性能的教程。</h3></div><div class="oz l"><p class="bd b dl z fq ow fs ft ox fv fx dk translated">towardsdatascience.com</p></div></div><div class="pa l"><div class="pk l pc pd pe pa pf le or"/></div></div></a></div><div class="oo op gq gs oq or"><a rel="noopener follow" target="_blank" href="/how-to-load-huge-csv-datasets-in-python-pandas-d306e75ff276"><div class="os ab fp"><div class="ot ab ou cl cj ov"><h2 class="bd jg gz z fq ow fs ft ox fv fx jf bi translated">如何在Python Pandas中加载巨大的CSV数据集</h2><div class="oy l"><h3 class="bd b gz z fq ow fs ft ox fv fx dk translated">可能会出现这样的情况，您的硬盘中有一个巨大的CSV数据集，占用了4或5gb(甚至更多),而您…</h3></div><div class="oz l"><p class="bd b dl z fq ow fs ft ox fv fx dk translated">towardsdatascience.com</p></div></div><div class="pa l"><div class="pl l pc pd pe pa pf le or"/></div></div></a></div><div class="oo op gq gs oq or"><a rel="noopener follow" target="_blank" href="/dataset-manipulation-with-open-refine-a5043b7294a7"><div class="os ab fp"><div class="ot ab ou cl cj ov"><h2 class="bd jg gz z fq ow fs ft ox fv fx jf bi translated">使用Open Refine操作数据集</h2><div class="oy l"><h3 class="bd b gz z fq ow fs ft ox fv fx dk translated">Open Refine是一个用于清理、转换和丰富数据集的web应用程序。它可以在下载…</h3></div><div class="oz l"><p class="bd b dl z fq ow fs ft ox fv fx dk translated">towardsdatascience.com</p></div></div><div class="pa l"><div class="pm l pc pd pe pa pf le or"/></div></div></a></div></div></div>    
</body>
</html>