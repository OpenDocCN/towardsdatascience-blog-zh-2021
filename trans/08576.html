<html>
<head>
<title>Compare PySpark DataFrames based on Grain</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">基于颗粒比较PySpark数据帧</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/compare-pyspark-dataframes-based-on-grain-e963e0a8aacf?source=collection_archive---------6-----------------------#2021-08-08">https://towardsdatascience.com/compare-pyspark-dataframes-based-on-grain-e963e0a8aacf?source=collection_archive---------6-----------------------#2021-08-08</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="b167" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">基于粒度比较Pyspark数据帧并使用数据样本生成报告的简单方法</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/dd817d1a8c6ea8f33cdb4261bae18195.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*phbOluCX5OLNntz5"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">米利安·耶西耶在<a class="ae kv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="fed1" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">比较两个数据集并生成准确而有意义的见解是大数据世界中一项常见而重要的任务。通过在Pyspark中运行并行作业，我们可以基于<a class="ae kv" href="http://www.datamartist.com/dimensional-tables-and-fact-tables#:~:text=The%20GRAIN%20or%20GRANULARITY%20of,one%20line%20for%20some%20orders)." rel="noopener ugc nofollow" target="_blank"> <strong class="ky ir">粒度</strong> </a>高效地比较庞大的数据集，并生成高效的报告来查明每一列级别的差异。</p></div><div class="ab cl ls lt hu lu" role="separator"><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx"/></div><div class="ij ik il im in"><p id="1312" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">要求:</strong></p><p id="dee6" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们从Salesforce获取了大量数据集。该数据集有600多列和大约2000万行。必须对开发和验证数据集进行比较，以生成一个报告，该报告可以基于<a class="ae kv" href="http://www.datamartist.com/dimensional-tables-and-fact-tables#:~:text=The%20GRAIN%20or%20GRANULARITY%20of,one%20line%20for%20some%20orders)." rel="noopener ugc nofollow" target="_blank"> <strong class="ky ir">粒度</strong> </a>查明列级别的差异。</p><p id="1aab" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">下面是与包含粒度列的数据样本一起生成的洞察，可用于进一步查询和分析。</p><ul class=""><li id="d427" class="lz ma iq ky b kz la lc ld lf mb lj mc ln md lr me mf mg mh bi translated">重复记录。</li><li id="dc56" class="lz ma iq ky b kz mi lc mj lf mk lj ml ln mm lr me mf mg mh bi translated">缺失记录。</li><li id="0b70" class="lz ma iq ky b kz mi lc mj lf mk lj ml ln mm lr me mf mg mh bi translated">捕获开发和验证数据集中的列值之间的差异。</li></ul></div><div class="ab cl ls lt hu lu" role="separator"><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx"/></div><div class="ij ik il im in"><p id="1ce3" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">方法:</strong></p><p id="cabd" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们可以使用数据框的subtract方法减去两个数据框，这将生成不匹配的记录，但是很难确定600列中的哪一列实际上发生了变化以及实际值有所不同。下面是解决这个问题的一个简单方法。</p><p id="d02d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">从这里开始，我将验证数据集称为源数据框架(src_df)，将开发数据集称为目标数据框架(tar_df)。由于相同的两个数据帧被一次又一次地用于不同的分析查询，因此我们可以将这两个数据帧保存在内存中，以便进行更快的分析。</p><p id="1832" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在下面的方法中，假设两个数据框具有相同的模式，即两个数据框具有相同的列数、相同的列名以及所有列的相同数据类型。</p><p id="8786" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">识别重复和缺失的谷物记录:</strong></p><p id="8ae5" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">为了识别重复记录，我们可以使用Pyspark函数通过查询编写一个小组。grainDuplicateCheck()函数也是如此。</p><p id="16c9" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">为了识别丢失的颗粒，我们可以从两个数据帧中选择颗粒列并直接减去它们。</p><p id="df9e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">需要注意的是，我们需要执行src_df - tar_df来获取有效的缺失颗粒，还需要执行tar_df - src_df来获取无效的额外记录颗粒。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="mn mo l"/></div></figure><p id="f227" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">比较列值:</strong></p><p id="2a4b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在，我们必须比较数据帧之间的列值，并逐列生成报告，同时找到特定粒度的预期值和无效值。</p><p id="dd7a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">由于我们已经确定了丢失的记录，现在我们将连接粒度列上的两个数据框，并比较在两个数据框中具有匹配粒度的所有记录的列值。</p><p id="a719" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">值得注意的是</p><ul class=""><li id="91b2" class="lz ma iq ky b kz la lc ld lf mb lj mc ln md lr me mf mg mh bi translated">颗粒列可以有空值，我们必须执行<strong class="ky ir">空安全连接</strong>，否则，那些记录不会被连接，记录将被跳过。为此，我使用了<a class="ae kv" href="https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.sql.Column.eqNullSafe.html" rel="noopener ugc nofollow" target="_blank"> <strong class="ky ir"> eqNullSafe() </strong> </a>内置的Pyspark函数。</li><li id="84fe" class="lz ma iq ky b kz mi lc mj lf mk lj ml ln mm lr me mf mg mh bi translated">包含双精度值(如amount)的列不会完全匹配，因为不同的计算可能会生成小数精度略有差异的值。因此，对于这些类型的列，我们必须减去这些值，并标记那些差异≥阈值的记录。</li></ul><p id="0d6b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在下面的compareDf()函数中，使用连接的数据框，我们选择要验证的粒度和特定列，并对它们进行比较。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="mn mo l"/></div></figure><p id="cd92" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">通过运行并行作业比较列值:</strong></p><p id="9370" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">上述函数的局限性在于，我们是按顺序逐列比较两个数据帧，遍历数据帧的每一列。</p><p id="225b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在让我们来看一些统计数据。有600多列和大约2000万行，比较每列需要15秒。对于所有600列，大约需要150分钟(约2.5小时)。</p><p id="2030" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">列比较任务彼此独立，因此我们可以并行化它们。如果我们平行比较8根柱子，那么所用的时间大约是19分钟。这是一个巨大的差异，因为它快了8倍。</p><p id="4e05" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在下面的函数compareDfParallel()中，我使用多处理内置python包创建了一个指定大小的线程池。使用<em class="mp">parallel _ thread _ count =</em>8，将并行比较8列。compareDFColumn()是将由每个线程为特定列执行的实际比较函数。</p><p id="da7d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">必须根据集群中节点的容量来选择并行线程数。我们可以使用spark-submit中指定的驱动核心数作为并行线程数。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="mn mo l"/></div></figure></div><div class="ab cl ls lt hu lu" role="separator"><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx"/></div><div class="ij ik il im in"><p id="7d53" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">假设pyspark_utils.py文件中存在上述所有函数，下面是示例数据的用法示例。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="mn mo l"/></div></figure><p id="cbb7" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">compareDfParallel()的输出将具有前缀为<em class="mp"> _src </em>的列，用于引用源数据帧的列值，以及前缀为<em class="mp"> _tar </em>的列，用于引用目标数据帧的列值。</p><p id="b316" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在上面的示例中，<em class="mp"> billing_amount_src </em>是1100，这是源数据帧中billing_amount的值，而<em class="mp"> billing_amount_tar </em>是1000，这是目标数据帧中billing_amount的值。</p><p id="5a04" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">脚本的输出可以通过管道传输到一个文件，以获得一个合并的报告。可以修改该函数来生成HTML报告，而不是文本文件报告。</p></div><div class="ab cl ls lt hu lu" role="separator"><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx"/></div><div class="ij ik il im in"><p id="79e3" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这是基于粒度有效比较Pyspark数据帧并使用数据样本生成报告的简单方法之一。</p><p id="0a2b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">数据仓库快乐！</p></div></div>    
</body>
</html>