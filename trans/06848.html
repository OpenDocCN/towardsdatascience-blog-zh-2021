<html>
<head>
<title>3 versions of k-Means</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">k-Means的3个版本</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/three-versions-of-k-means-cf939b65f4ea?source=collection_archive---------13-----------------------#2021-06-21">https://towardsdatascience.com/three-versions-of-k-means-cf939b65f4ea?source=collection_archive---------13-----------------------#2021-06-21</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="e922" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph">哈迪根-王，麦克奎恩，劳埃德</h2><div class=""/><div class=""><h2 id="dc26" class="pw-subtitle-paragraph jw iz iq bd b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn dk translated">k-Means的3种实现的简短视觉指南</h2></div><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ko"><img src="../Images/b0f037add8fa28c357af5b4feb60a2b6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5_Ve34hUjw92WfjxvYeJsQ.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">(图片由作者提供)</p></figure><p id="3436" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">在这篇文章中，我们将谈论劳氏、麦克奎恩和哈迪根-王的k-Means。我不仅将为您提供所有实现的伪代码，而且您还将获得实现如何在内部工作的可视化。</p><h1 id="d657" class="ma mb iq bd mc md me mf mg mh mi mj mk kf ml kg mm ki mn kj mo kl mp km mq mr bi translated">k均值</h1><p id="0632" class="pw-post-body-paragraph le lf iq lg b lh ms ka lj lk mt kd lm ln mu lp lq lr mv lt lu lv mw lx ly lz ij bi translated">在聚类中，我们有一组数据点，我们希望根据它们的相似性将它们分配给聚类。这应该以这样的方式完成，即目标函数(平方和，..)最小化。有许多聚类算法，它们都有各自的优缺点。K-Means可能是其中最著名的。这里我们有质心作为我们集群的原型。每个数据点属于离他最近的质心(对于聚类也是如此)。我们如何通过算法找到这些质心，你会在下面的文章中看到。</p><h1 id="6e99" class="ma mb iq bd mc md me mf mg mh mi mj mk kf ml kg mm ki mn kj mo kl mp km mq mr bi translated">劳氏k-均值</h1><p id="c50e" class="pw-post-body-paragraph le lf iq lg b lh ms ka lj lk mt kd lm ln mu lp lq lr mv lt lu lv mw lx ly lz ij bi translated">劳埃德是k-Means最广为人知的实现。当学生第一次接触聚类时，他们几乎肯定会学习劳氏k-Means。正如Lloyd所说，这既容易理解，也容易实现。让我们把大象从房间里拿出来，看看这个算法实际上是如何工作的。</p><p id="53ac" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">劳埃德最初随机选择k个数据点作为质心。之后，所有数据点被分配到最近的质心，然后重新计算质心，作为所有分配数据点的平均值。劳埃德循环这最后两步，直到没有点改变它的分配。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi mx"><img src="../Images/a7f4cca6a7045c6bc0f8a2d56da4d362.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NhP4udPb2vPaTFQezbv-4A.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">(图片由作者提供)</p></figure><p id="269a" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">不幸的是，Lloyd确实深受许多聚类算法所面临的问题的困扰。也就是说，团簇的形成受到初始质心选择的严重影响。或者用更数学的术语来说。劳埃德可能会陷入局部最优，这取决于它的初始化。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi my"><img src="../Images/0ca10e9de3c0e562b4d11af17d371d43.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/1*iYXJamN3tXmW7hPC_4hmww.gif"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">(图片由作者提供)</p></figure><h1 id="3673" class="ma mb iq bd mc md me mf mg mh mi mj mk kf ml kg mm ki mn kj mo kl mp km mq mr bi translated">麦克奎恩k均值</h1><p id="12ac" class="pw-post-body-paragraph le lf iq lg b lh ms ka lj lk mt kd lm ln mu lp lq lr mv lt lu lv mw lx ly lz ij bi translated">当Lloyd更新数据点的分配时，它不会更新质心。这是有问题的，因为每一次新的赋值，质心都会改变它的位置。可能发生数据点被错误地分配给质心，仅仅因为所述质心没有被更新。MacQueen试图解决这个问题，因为它用每个新的赋值来更新质心。显然，这导致更多的计算时间。</p><p id="48e3" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">MacQueen的初始化方式和Lloyd一样。两者之间的唯一区别是，我们在数据点上迭代，当重新分配它们时，我们重新计算受影响的质心。我们也在一个循环中这样做，直到没有点改变它的赋值。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi mz"><img src="../Images/908cff6166ef925250ae4af2365c3cfd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Kt8QxvGnjgPPaiPOu8Momg.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">(图片由作者提供)</p></figure><p id="28e7" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">在下面的例子中，我们可以看到，MacQueen确实在所有数据点的第一次迭代中找到了非常好的赋值。第二次迭代仅重新分配少量最初错误分配的数据点。这种行为对于麦克奎恩的k均值来说是典型的[1]。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi my"><img src="../Images/5d351008d6891627f164258e10879816.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/1*WIhhEmaNjp0tDlJtYuZjhw.gif"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">(图片由作者提供)</p></figure><h1 id="d431" class="ma mb iq bd mc md me mf mg mh mi mj mk kf ml kg mm ki mn kj mo kl mp km mq mr bi translated">哈廷根-黄k-均值</h1><p id="ef97" class="pw-post-body-paragraph le lf iq lg b lh ms ka lj lk mt kd lm ln mu lp lq lr mv lt lu lv mw lx ly lz ij bi translated">还记得劳埃德的初始化问题吗？麦克奎恩在某种程度上也遭受了这一点。毕竟，麦克奎恩和劳埃德选择初始质心没有什么不同。另一方面，Hartigan-Wong最初将所有数据点分配给随机质心。之后，后者被计算为它们被分配的数据点的平均值。这导致所有的质心最初或多或少都在同一点上。Hartigan-Wong比Lloyd更不容易收敛到局部最优[2]。</p><p id="42d1" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">在Hartigan-Wong中，我们从所有质心中选择一个作为赋值，其中所述赋值产生最小的目标函数。在下面的例子中，我们使用从每个数据点到其指定中心的距离平方和作为目标函数。结果是，一个数据点可以被指定给一个质心，即使该质心不是它最近的一个。这是可行的，如果它降低了目标函数，从而提高了我们的聚类的总体质量。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi na"><img src="../Images/773b318e844dd92ad079f5aeee854528.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cut8tJKpPSJMgZd03BPciQ.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">(图片由作者提供)</p></figure><p id="bbc3" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">在这种情况下，收敛意味着没有点改变它的分配。显然，这种方法导致所有三种提出的实现的最高计算时间。Hartigan-Wong比Lloyd或MacQueen复杂得多，因为它在重新分配点时更新质心，并且考虑将点分配给质心的更广泛影响。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi my"><img src="../Images/9fd8451bd1156ae0e65f3cfeb422efbf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/1*s7VjG4lffxKnxuilNzaEGw.gif"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">(图片由作者提供)</p></figure><h1 id="69a6" class="ma mb iq bd mc md me mf mg mh mi mj mk kf ml kg mm ki mn kj mo kl mp km mq mr bi translated">结论</h1><p id="d04f" class="pw-post-body-paragraph le lf iq lg b lh ms ka lj lk mt kd lm ln mu lp lq lr mv lt lu lv mw lx ly lz ij bi translated">现在，您已经了解了k-Means的3种不同实现。一些比另一些更复杂。如果你现在不确定下一步实施哪个，我只能说看情况。如果你的数据区分得有些清晰，就像我用的例子，你可能用MacQueen或Lloyd更快。</p><p id="7bd0" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">我希望我能帮助你，我很高兴下次见到你。</p></div><div class="ab cl nb nc hu nd" role="separator"><span class="ne bw bk nf ng nh"/><span class="ne bw bk nf ng nh"/><span class="ne bw bk nf ng"/></div><div class="ij ik il im in"><p id="4dd1" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">最后，我想提一下，有多种方法来选择初始质心，以改善劳埃德和麦克奎恩。如果想了解更多，建议看下面这篇文章。</p><div class="ni nj gp gr nk nl"><a href="https://medium.com/analytics-vidhya/comparison-of-initialization-strategies-for-k-means-d5ddd8b0350e" rel="noopener follow" target="_blank"><div class="nm ab fo"><div class="nn ab no cl cj np"><h2 class="bd ja gy z fp nq fr fs nr fu fw iz bi translated">k-Means初始化策略的比较</h2><div class="ns l"><h3 class="bd b gy z fp nq fr fs nr fu fw dk translated">k-Means是一种数据划分算法，是聚类算法中最直接的选择之一。一些…</h3></div><div class="nt l"><p class="bd b dl z fp nq fr fs nr fu fw dk translated">medium.com</p></div></div><div class="nu l"><div class="nv l nw nx ny nu nz ky nl"/></div></div></a></div></div><div class="ab cl nb nc hu nd" role="separator"><span class="ne bw bk nf ng nh"/><span class="ne bw bk nf ng nh"/><span class="ne bw bk nf ng"/></div><div class="ij ik il im in"><h1 id="7fc1" class="ma mb iq bd mc md oa mf mg mh ob mj mk kf oc kg mm ki od kj mo kl oe km mq mr bi translated">参考</h1><p id="d2f2" class="pw-post-body-paragraph le lf iq lg b lh ms ka lj lk mt kd lm ln mu lp lq lr mv lt lu lv mw lx ly lz ij bi translated">[1]莫里塞特，劳伦斯&amp;沙特尔，西尔万。(2013).k-means聚类技术:一般考虑和在Mathematica中的实现。心理学定量方法教程。9.15–24.10.20982/tqmp</p><p id="af8a" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">[2]斯洛尼姆，诺姆&amp;阿哈尼，埃胡德&amp;克拉默，科比。(2013).哈迪根的K均值与劳埃德的K均值:是时候改变了吗？。第23届国际人工智能联合会议论文集。1677–1684.</p></div></div>    
</body>
</html>