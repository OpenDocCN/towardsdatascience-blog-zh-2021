<html>
<head>
<title>Feature Selection with Boruta in Python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Python 中 Boruta 的特征选择</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/feature-selection-with-boruta-in-python-676e3877e596?source=collection_archive---------6-----------------------#2021-11-30">https://towardsdatascience.com/feature-selection-with-boruta-in-python-676e3877e596?source=collection_archive---------6-----------------------#2021-11-30</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="c0c2" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated"><em class="kf">了解 Boruta 算法如何用于特征选择。说明+模板</em></h2></div><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi kg"><img src="../Images/650d1eebd918f9279497775633680c77.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ryDRX3P8RvW2aZyPAfeHwg.jpeg"/></div></div><p class="ks kt gj gh gi ku kv bd b be z dk translated"><a class="ae kw" href="https://unsplash.com/@a_cat?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">卡洛琳</a>在<a class="ae kw" href="https://unsplash.com/s/visual/54b7f4bd-7ab2-410c-a96a-3033270ab95e?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">号宇宙飞船</a>上拍摄的照片</p></figure><p id="5e91" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">特征选择过程是任何机器学习项目的基础。在本文中，我们将介绍 Boruta 算法，该算法允许我们对我们的特性、<strong class="kz ir">进行<strong class="kz ir">排序，从最重要到对我们的模型影响最小</strong>。Boruta 使用简单，是一种强大的技术，分析师应该将它纳入到他们的管道中。</strong></p><h1 id="5e5a" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated">介绍</h1><p id="a60d" class="pw-post-body-paragraph kx ky iq kz b la ml jr lc ld mm ju lf lg mn li lj lk mo lm ln lo mp lq lr ls ij bi translated">Boruta 不是一个独立的算法:<strong class="kz ir">它位于随机森林算法</strong>之上。事实上，这个名字<em class="mq"> Boruta </em>来源于斯拉夫神话中森林之魂的名字<a class="ae kw" href="https://en.wikipedia.org/wiki/Leshy" rel="noopener ugc nofollow" target="_blank">。为了理解算法是如何工作的，我们将对随机森林做一个简单的介绍。</a></p><p id="f713" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">随机森林是基于<strong class="kz ir">装袋</strong>的概念——即从训练集中创建许多随机样本，并为每个样本训练不同的统计模型。对于分类任务，结果是来自模型的大多数投票，而对于回归任务，结果是各种模型的平均值。</p><p id="87f7" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">规范装袋与随机森林的区别在于后者总是只使用<strong class="kz ir">决策树模型</strong>。对于所考虑的每个样本，决策树考虑了一组有限的特征。这使得随机森林算法能够估计每个特征的重要性，因为它基于所考虑的特征分割将误差存储在预测中。</p><p id="cb1a" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">让我们考虑一个分类任务。RF 估计特征重要性的方法分两个阶段进行。首先，<strong class="kz ir">每个决策树创建并存储一个预测。</strong>其次，通过各种训练样本随机排列某些特征的值，并重复上一步，再次追踪预测的结果。单个决策树的特征的重要性被计算为使用原始特征的模型与使用置换特征的模型之间的性能差异除以训练集中的示例数量。<strong class="kz ir">特征的重要性是该特征在所有树上的测量值的平均值</strong>。在此过程中<strong class="kz ir">没有做的是计算每个特征的 z 分数</strong>。这就是博鲁塔发挥作用的地方。</p><h1 id="2c92" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated">Boruta 如何工作</h1><p id="cd38" class="pw-post-body-paragraph kx ky iq kz b la ml jr lc ld mm ju lf lg mn li lj lk mo lm ln lo mp lq lr ls ij bi translated">Boruta 的想法既有趣又简单:对于原始数据集中的所有特征，我们将创建它们的随机副本(称为<em class="mq">阴影特征</em>)，并基于这个扩展的数据集训练分类器。<strong class="kz ir">为了理解一个特征的重要性，我们将它与所有生成的阴影特征进行比较</strong>。只有在统计上比这些合成特征更重要的特征才会被保留，因为它们对模型性能的贡献更大。让我们更详细地看看这些步骤。</p><ol class=""><li id="9c07" class="mr ms iq kz b la lb ld le lg mt lk mu lo mv ls mw mx my mz bi translated"><strong class="kz ir">创建训练集特征</strong>的副本，并将它们与原始特征合并</li><li id="1710" class="mr ms iq kz b la na ld nb lg nc lk nd lo ne ls mw mx my mz bi translated"><strong class="kz ir">在这些合成特征</strong>上创建随机排列，以消除它们与目标变量<em class="mq"> y </em>之间的任何相关性——基本上，这些合成特征是它们所源自的原始特征的随机组合</li><li id="dd1d" class="mr ms iq kz b la na ld nb lg nc lk nd lo ne ls mw mx my mz bi translated">合成特征在每次新的迭代中被随机化</li><li id="c8a3" class="mr ms iq kz b la na ld nb lg nc lk nd lo ne ls mw mx my mz bi translated">在每次新的迭代中，<strong class="kz ir">计算所有原始和合成特征的 z 值</strong>。如果某个特征的重要性高于所有合成特征的最大重要性，则认为该特征是相关的</li><li id="e4aa" class="mr ms iq kz b la na ld nb lg nc lk nd lo ne ls mw mx my mz bi translated">对所有原始特征应用统计测试并保存其结果。无效假设是一个特征的重要性等于合成特征的最大重要性。统计测试测试原始特征和合成特征之间的相等性。<strong class="kz ir">当一个特征的重要性明显高于或低于一个综合特征的重要性时，零假设被拒绝</strong></li><li id="f496" class="mr ms iq kz b la na ld nb lg nc lk nd lo ne ls mw mx my mz bi translated"><strong class="kz ir">从原始和合成数据集中移除被认为不重要的特征</strong></li><li id="b487" class="mr ms iq kz b la na ld nb lg nc lk nd lo ne ls mw mx my mz bi translated">重复所有步骤，重复<em class="mq"> n </em>次，直到所有特征都被移除或被认为重要</li></ol><p id="d5db" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">应该注意的是，Boruta 是一种启发式算法:它的性能没有保证。因此，建议多次运行该过程，并反复评估结果。</p><h1 id="fd97" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated">Boruta 在 Python 中的实现</h1><p id="a801" class="pw-post-body-paragraph kx ky iq kz b la ml jr lc ld mm ju lf lg mn li lj lk mo lm ln lo mp lq lr ls ij bi translated">让我们看看 Boruta 如何在 Python 中使用它的<a class="ae kw" href="https://github.com/scikit-learn-contrib/boruta_py" rel="noopener ugc nofollow" target="_blank">专用库</a>工作。我们将使用 Sklearn.datasets 的<em class="mq"> load_diabetes() </em>数据集在一个回归问题上测试 Boruta。</p><p id="e341" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">特征集 X 由变量组成</p><ul class=""><li id="49c7" class="mr ms iq kz b la lb ld le lg mt lk mu lo mv ls nf mx my mz bi translated">年龄(年)</li><li id="dba8" class="mr ms iq kz b la na ld nb lg nc lk nd lo ne ls nf mx my mz bi translated">性</li><li id="46aa" class="mr ms iq kz b la na ld nb lg nc lk nd lo ne ls nf mx my mz bi translated">身体质量指数</li><li id="7d80" class="mr ms iq kz b la na ld nb lg nc lk nd lo ne ls nf mx my mz bi translated">平均血压</li><li id="08a6" class="mr ms iq kz b la na ld nb lg nc lk nd lo ne ls nf mx my mz bi translated">s1(总胆固醇)</li><li id="ad48" class="mr ms iq kz b la na ld nb lg nc lk nd lo ne ls nf mx my mz bi translated">s2(低密度脂蛋白)</li><li id="297b" class="mr ms iq kz b la na ld nb lg nc lk nd lo ne ls nf mx my mz bi translated">s3(高密度脂蛋白，高密度脂蛋白)</li><li id="a1ac" class="mr ms iq kz b la na ld nb lg nc lk nd lo ne ls nf mx my mz bi translated">s4(总胆固醇/高密度脂蛋白胆固醇)</li><li id="54c6" class="mr ms iq kz b la na ld nb lg nc lk nd lo ne ls nf mx my mz bi translated">s5 (ltg，甘油三酯水平的对数)</li><li id="0179" class="mr ms iq kz b la na ld nb lg nc lk nd lo ne ls nf mx my mz bi translated">s6 (glu，血糖水平)</li></ul><p id="d73e" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">目标<em class="mq"> y </em>是记录的糖尿病随时间的进展。</p><figure class="kh ki kj kk gt kl"><div class="bz fp l di"><div class="ng nh l"/></div></figure><p id="584f" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">通过运行这个脚本，我们将在终端中看到 Boruta 是如何构建推理的</p><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/65d883fe986494c4480f2e2ca7eac62e.png" data-original-src="https://miro.medium.com/v2/resize:fit:452/format:webp/1*dBbG6J7HjQmjozbhPRrf7g.png"/></div><p class="ks kt gj gh gi ku kv bd b be z dk translated">Boruta 在 Sklearn 糖尿病数据集上 10 次迭代的结果。图片作者。</p></figure><p id="f5b8" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">这份报告可读性也很强</p><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi nj"><img src="../Images/cbcde6839c431d10073e634ad642c0cc.png" data-original-src="https://miro.medium.com/v2/resize:fit:794/format:webp/1*maqbNOut0mNOsy94HfHbmg.png"/></div></div><p class="ks kt gj gh gi ku kv bd b be z dk translated">Boruta 结果报告——简单易懂的特征选择。图片作者。</p></figure><p id="1ec3" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">根据 Boruta 的说法，<em class="mq"> bmi、bp、s5 </em>和<em class="mq"> s6 </em>是对构建我们的预测模型贡献最大的特征。为了过滤我们的数据集并只选择对 Boruta 重要的特性，我们使用了<em class="mq">feat _ selector . transform(NP . array(X))</em>，它将返回一个 Numpy 数组。</p><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/f92cc8d7dc8b87486a3497dc4b20f911.png" data-original-src="https://miro.medium.com/v2/resize:fit:658/format:webp/1*AWoMXzmg-N4FcUrcXhzWIA.png"/></div><p class="ks kt gj gh gi ku kv bd b be z dk translated">由 Boruta 选择的特性。拟合 _ 变换。这个向量就可以用于训练了。图片作者。</p></figure><p id="96a3" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">我们现在准备好为我们的 RandomForestRegressor 模型提供一组选定的 X 特性。我们训练模型并打印均方根误差(RMSE)。</p><figure class="kh ki kj kk gt kl"><div class="bz fp l di"><div class="ng nh l"/></div></figure><p id="0959" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">这是训练结果</p><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div class="gh gi nl"><img src="../Images/dcc06d3c62ffd7535d9c38e510d8b0c6.png" data-original-src="https://miro.medium.com/v2/resize:fit:646/format:webp/1*0nQR-wisuBe8o88Pmrn_cg.png"/></div><p class="ks kt gj gh gi ku kv bd b be z dk translated">RandomForestRegressor 模型对所选值的要素集的预测和误差。图片作者。</p></figure><h1 id="387f" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated">结论</h1><p id="4b0e" class="pw-post-body-paragraph kx ky iq kz b la ml jr lc ld mm ju lf lg mn li lj lk mo lm ln lo mp lq lr ls ij bi translated">Boruta 是一个强大而简单的特征选择算法，在网上得到广泛使用和赞赏，尤其是在 Kaggle 上。它的有效性和解释的简易性为数据科学家的工具包增加了价值，因为它是从著名的决策/随机森林算法扩展而来的。</p><p id="9165" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">尝试一下，享受一个更高性能的模型，它接收的信号比噪声多一点；)</p><h1 id="418e" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated">推荐阅读</h1><p id="f110" class="pw-post-body-paragraph kx ky iq kz b la ml jr lc ld mm ju lf lg mn li lj lk mo lm ln lo mp lq lr ls ij bi translated">对于感兴趣的人来说，这里有一个我为每个与 ML 相关的主题推荐的书籍列表。在我看来，有一些必不可少的书籍对我的职业生涯产生了巨大影响。<br/> <em class="mq">免责声明:这些是亚马逊会员链接。我会收到亚马逊为你推荐这些项目的一小笔佣金。你的体验不会改变，你也不会被收取更多费用，但它将帮助我扩大业务规模，围绕人工智能制作更多内容。</em></p><ul class=""><li id="7f38" class="mr ms iq kz b la lb ld le lg mt lk mu lo mv ls nf mx my mz bi translated"><strong class="kz ir">ML 简介</strong> : <a class="ae kw" href="https://amzn.to/3WZ51cE" rel="noopener ugc nofollow" target="_blank"> <em class="mq">自信的数据技能:掌握数据工作的基本原理，为你的职业生涯增压</em> </a> <em class="mq"> </em>作者:基里尔·叶列缅科</li><li id="b298" class="mr ms iq kz b la na ld nb lg nc lk nd lo ne ls nf mx my mz bi translated"><strong class="kz ir">sk Learn/tensor flow</strong>:<a class="ae kw" href="https://amzn.to/3jseVGb" rel="noopener ugc nofollow" target="_blank"><em class="mq">使用 Scikit-Learn、Keras 和 TensorFlow </em> </a>进行动手机器学习，作者 Aurelien Géron</li><li id="da92" class="mr ms iq kz b la na ld nb lg nc lk nd lo ne ls nf mx my mz bi translated"><strong class="kz ir"> NLP </strong> : <a class="ae kw" href="https://amzn.to/3l9FO22" rel="noopener ugc nofollow" target="_blank"> <em class="mq">文本即数据:机器学习和社会科学的新框架</em> </a> <em class="mq"> </em>作者贾斯汀·格里默</li><li id="8867" class="mr ms iq kz b la na ld nb lg nc lk nd lo ne ls nf mx my mz bi translated"><strong class="kz ir">sk Learn/PyTorch</strong>:<a class="ae kw" href="https://amzn.to/3wYZf0e" rel="noopener ugc nofollow" target="_blank"><em class="mq">用 py torch 和 Scikit 进行机器学习——Learn:用 Python 开发机器学习和深度学习模型</em></a>Sebastian Raschka</li><li id="048b" class="mr ms iq kz b la na ld nb lg nc lk nd lo ne ls nf mx my mz bi translated">Cole Knaflic 著<strong class="kz ir"> Data 即</strong> : <a class="ae kw" href="https://amzn.to/3HUtGtB" rel="noopener ugc nofollow" target="_blank"> <em class="mq">用数据讲故事:商务人士数据可视化指南</em> </a></li></ul><h1 id="1128" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated">有用的链接(我写的)</h1><ul class=""><li id="0c58" class="mr ms iq kz b la ml ld mm lg nm lk nn lo no ls nf mx my mz bi translated"><strong class="kz ir">了解如何在 Python 中执行顶层探索性数据分析</strong>:<a class="ae kw" rel="noopener" target="_blank" href="/exploratory-data-analysis-in-python-a-step-by-step-process-d0dfa6bf94ee"><em class="mq">Python 中的探索性数据分析——一步一步的过程</em> </a></li><li id="e1e1" class="mr ms iq kz b la na ld nb lg nc lk nd lo ne ls nf mx my mz bi translated"><strong class="kz ir">学习 TensorFlow 的基础知识</strong>:<a class="ae kw" href="https://medium.com/towards-data-science/a-comprehensive-introduction-to-tensorflows-sequential-api-and-model-for-deep-learning-c5e31aee49fa" rel="noopener"><em class="mq">tensor flow 2.0 入门—深度学习入门</em> </a></li><li id="951c" class="mr ms iq kz b la na ld nb lg nc lk nd lo ne ls nf mx my mz bi translated"><strong class="kz ir">用 Python 中的 TF-IDF 进行文本聚类</strong> : <a class="ae kw" href="https://medium.com/mlearning-ai/text-clustering-with-tf-idf-in-python-c94cd26a31e7" rel="noopener"> <em class="mq">用 Python 中的 TF-IDF 进行文本聚类</em> </a></li></ul><p id="4a2d" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated"><strong class="kz ir">如果你想支持我的内容创作活动，欢迎点击我下面的推荐链接，加入 Medium 的会员计划</strong>。我将收到你投资的一部分，你将能够以无缝的方式访问 Medium 的大量数据科学文章。</p><div class="np nq gp gr nr ns"><a href="https://medium.com/@theDrewDag/membership" rel="noopener follow" target="_blank"><div class="nt ab fo"><div class="nu ab nv cl cj nw"><h2 class="bd ir gy z fp nx fr fs ny fu fw ip bi translated">通过我的推荐链接加入 Medium-Andrew D # data science</h2><div class="nz l"><h3 class="bd b gy z fp nx fr fs ny fu fw dk translated">阅读 Andrew D #datascience(以及媒体上成千上万的其他作者)的每一个故事。您的会员费直接…</h3></div><div class="oa l"><p class="bd b dl z fp nx fr fs ny fu fw dk translated">medium.com</p></div></div><div class="ob l"><div class="oc l od oe of ob og kq ns"/></div></div></a></div><h1 id="6eb6" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated">代码模板</h1><p id="491a" class="pw-post-body-paragraph kx ky iq kz b la ml jr lc ld mm ju lf lg mn li lj lk mo lm ln lo mp lq lr ls ij bi translated">这是本文中展示的整个代码的复制粘贴版本。</p><figure class="kh ki kj kk gt kl"><div class="bz fp l di"><div class="ng nh l"/></div></figure></div></div>    
</body>
</html>