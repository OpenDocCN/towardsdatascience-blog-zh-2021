<html>
<head>
<title>ElegantRL-Podracer: A Scalable and Elastic Library for Cloud-Native Deep Reinforcement Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">ElegantRL-Podracer:一个可扩展的弹性库，用于云原生深度强化学习</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/elegantrl-podracer-scalable-and-elastic-library-for-cloud-native-deep-reinforcement-learning-bafda6f7fbe0?source=collection_archive---------9-----------------------#2021-12-11">https://towardsdatascience.com/elegantrl-podracer-scalable-and-elastic-library-for-cloud-native-deep-reinforcement-learning-bafda6f7fbe0?source=collection_archive---------9-----------------------#2021-12-11</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="fa96" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">代理作为pod在云上进行赛车比赛。</p><blockquote class="kl km kn"><p id="d0df" class="jn jo ko jp b jq jr js jt ju jv jw jx kp jz ka kb kq kd ke kf kr kh ki kj kk ij bi translated">“当风暴结束时，你可以看到我的赛车。我在造一辆赛车！”</p><p id="5153" class="jn jo ko jp b jq jr js jt ju jv jw jx kp jz ka kb kq kd ke kf kr kh ki kj kk ij bi translated">—阿纳金·天行者</p></blockquote><p id="8ae1" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们正在云上构建一个赛车，使用<a class="ae ks" href="https://github.com/AI4Finance-Foundation/ElegantRL" rel="noopener ugc nofollow" target="_blank"><em class="ko"/></a><em class="ko">。</em></p><p id="8a39" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">如果你想看一个在几分钟内训练一个强大的深度强化学习(DRL)代理的“赛车”，这篇文章就是为你准备的。ElegantRL-Podracer是一个云解决方案，支持数百万个GPU核心在多个级别上进行大规模并行DRL训练。让我们开始吧！</p><p id="619b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">Steven Li和<a class="ae ks" href="https://twitter.com/XiaoYangLiu10" rel="noopener ugc nofollow" target="_blank">刘晓阳</a>的这篇文章描述了我们最近在<strong class="jp ir">neur IPS 2021:Deep RL Workshop</strong>上发表的论文<strong class="jp ir"> ElegantRL-Podracer:可扩展和弹性的云原生深度强化学习库</strong>。</p><div class="kt ku gp gr kv kw"><a href="https://arxiv.org/abs/2112.05923" rel="noopener  ugc nofollow" target="_blank"><div class="kx ab fo"><div class="ky ab kz cl cj la"><h2 class="bd ir gy z fp lb fr fs lc fu fw ip bi translated">ElegantRL-Podracer:用于云原生深度强化学习的可扩展弹性库</h2><div class="ld l"><h3 class="bd b gy z fp lb fr fs lc fu fw dk translated">深度强化学习(DRL)已经彻底改变了游戏和学习等应用中的学习和驱动</h3></div><div class="le l"><p class="bd b dl z fp lb fr fs lc fu fw dk translated">arxiv.org</p></div></div><div class="lf l"><div class="lg l lh li lj lf lk ll kw"/></div></div></a></div><p id="1b1c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">代码、文档和演示可在<a class="ae ks" href="https://github.com/AI4Finance-Foundation/ElegantRL" rel="noopener ugc nofollow" target="_blank"> GitHub </a>上获得。</p><div class="kt ku gp gr kv kw"><a href="https://github.com/AI4Finance-Foundation/ElegantRL" rel="noopener  ugc nofollow" target="_blank"><div class="kx ab fo"><div class="ky ab kz cl cj la"><h2 class="bd ir gy z fp lb fr fs lc fu fw ip bi translated">GitHub-ai4 finance-Foundation/ElegantRL:可扩展的弹性深度强化学习使用…</h2><div class="ld l"><h3 class="bd b gy z fp lb fr fs lc fu fw dk translated">ElegantRL是为研究人员和实践者开发的，具有以下优点:轻量级:核心代码…</h3></div><div class="le l"><p class="bd b dl z fp lb fr fs lc fu fw dk translated">github.com</p></div></div><div class="lf l"><div class="lm l lh li lj lf lk ll kw"/></div></div></a></div></div><div class="ab cl ln lo hu lp" role="separator"><span class="lq bw bk lr ls lt"/><span class="lq bw bk lr ls lt"/><span class="lq bw bk lr ls"/></div><div class="ij ik il im in"><h1 id="64ee" class="lu lv iq bd lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr bi translated">最大的挑战是什么？</h1><p id="92ee" class="pw-post-body-paragraph jn jo iq jp b jq ms js jt ju mt jw jx jy mu ka kb kc mv ke kf kg mw ki kj kk ij bi mx translated">eep强化学习平衡了(未知领域的)探索和(当前信息的)利用，在游戏和机器人控制等应用中彻底改变了学习和驱动。</p><p id="a791" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">然而，数据收集的<strong class="jp ir">成本</strong>仍然是在复杂和动态环境的现实世界问题中更广泛采用DRL的一个主要挑战。因此，一个引人注目的解决方案是在数百甚至数千个GPU上进行大规模并行训练，比如数百万个GPU核心。</p><p id="c396" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">现有的DRL框架/库要么缺乏可伸缩性，要么学习难度很大。</p><figure class="nh ni nj nk gt nl gh gi paragraph-image"><div role="button" tabindex="0" class="nm nn di no bf np"><div class="gh gi ng"><img src="../Images/79bcb71cc34155af48636a6578f65811.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0m6EDihzaayn8a41d0kMzA.png"/></div></div><p class="nr ns gj gh gi nt nu bd b be z dk translated">图1:不同框架/库的比较。【图片由作者提供。]</p></figure><h1 id="6315" class="lu lv iq bd lw lx nv lz ma mb nw md me mf nx mh mi mj ny ml mm mn nz mp mq mr bi translated">设计原则</h1><p id="b27e" class="pw-post-body-paragraph jn jo iq jp b jq ms js jt ju mt jw jx jy mu ka kb kc mv ke kf kg mw ki kj kk ij bi translated">我们开发了一个用户友好的开源库，利用云资源来训练DRL特工。该库强调以下设计原则:</p><ul class=""><li id="5e3c" class="oa ob iq jp b jq jr ju jv jy oc kc od kg oe kk of og oh oi bi translated"><strong class="jp ir">横向扩展</strong>:可伸缩性和弹性。</li><li id="6c25" class="oa ob iq jp b jq oj ju ok jy ol kc om kg on kk of og oh oi bi translated"><strong class="jp ir">效率</strong>:低通信开销，大规模并行模拟，代理的健壮性。</li><li id="a7e4" class="oa ob iq jp b jq oj ju ok jy ol kc om kg on kk of og oh oi bi translated"><strong class="jp ir">可达性</strong>:轻量化、定制化。</li></ul><h1 id="12e7" class="lu lv iq bd lw lx nv lz ma mb nw md me mf nx mh mi mj ny ml mm mn nz mp mq mr bi translated">基于锦标赛的集成方案</h1><p id="569d" class="pw-post-body-paragraph jn jo iq jp b jq ms js jt ju mt jw jx jy mu ka kb kc mv ke kf kg mw ki kj kk ij bi translated">ElegantRL-Podracer采用基于锦标赛的合奏方案，在数百甚至数千个GPU上编排训练过程，安排排行榜和具有数百个代理(pod)的训练池之间的交互。</p><figure class="nh ni nj nk gt nl gh gi paragraph-image"><div role="button" tabindex="0" class="nm nn di no bf np"><div class="gh gi oo"><img src="../Images/683f017d2665f4e798a5e52a077bc484.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*K_yCdcJrl52MlS7xGekcJw.png"/></div></div><p class="nr ns gj gh gi nt nu bd b be z dk translated">图2:基于锦标赛的合奏方案。【图片由作者提供。]</p></figure><p id="b60b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">与进化策略(es)不同，在进化策略中，一群智能体经过几代进化，我们的基于锦标赛的集成方案并行地异步更新智能体，这将群体进化和单个智能体学习解耦。基于锦标赛的合奏方案的关键是<strong class="jp ir">排行榜</strong>和<strong class="jp ir">训练池</strong>之间的互动。</p><ol class=""><li id="0722" class="oa ob iq jp b jq jr ju jv jy oc kc od kg oe kk op og oh oi bi translated">一个<strong class="jp ir">编排器</strong>实例化一个新的代理(pod)并将其放入一个训练池。</li><li id="e086" class="oa ob iq jp b jq oj ju ok jy ol kc om kg on kk op og oh oi bi translated">一个<strong class="jp ir">生成器</strong>用从排行榜中选择的网络和优化器初始化一个代理(pod)。</li><li id="126c" class="oa ob iq jp b jq oj ju ok jy ol kc om kg on kk op og oh oi bi translated">一个<strong class="jp ir">更新器</strong>在一个代理被训练一定步数或一定时间后，根据其表现确定是否将代理插入排行榜以及在何处插入。</li></ol><p id="8200" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">直觉上，ElegantRL-Podracer的灵感来自凯文·凯利的书《失控》(失控)(嵌套层次结构的优点)和《从愚蠢的事情中变得聪明》。对于低级别的训练，ElegantRL-Podracer通过采用面向硬件的优化来实现嵌套层次。对于高级调度，我们从数百个弱代理中获得一个智能代理。</p><h1 id="ba61" class="lu lv iq bd lw lx nv lz ma mb nw md me mf nx mh mi mj ny ml mm mn nz mp mq mr bi translated"><strong class="ak">云原生范式</strong></h1><p id="c469" class="pw-post-body-paragraph jn jo iq jp b jq ms js jt ju mt jw jx jy mu ka kb kc mv ke kf kg mw ki kj kk ij bi translated">ElegantRL-Podracer通过实现微服务和容器化的开发原则，遵循了云原生范式。我们将培训过程分解为五个组成部分，并将其作为微服务来实施，例如，编排者、排行榜、工作者、学习者和评估者。我们使用<a class="ae ks" href="https://kubernetes.io/" rel="noopener ugc nofollow" target="_blank"> Kubernetes </a> (K8s)作为资源管理器，并在独立的容器和pod中执行每个流程。</p><h1 id="1705" class="lu lv iq bd lw lx nv lz ma mb nw md me mf nx mh mi mj ny ml mm mn nz mp mq mr bi translated">ElegantRL-Podracer的特点</h1><p id="1638" class="pw-post-body-paragraph jn jo iq jp b jq ms js jt ju mt jw jx jy mu ka kb kc mv ke kf kg mw ki kj kk ij bi translated"><strong class="jp ir">可扩展的并行性</strong>:ElegantRL-pod racer的多级并行性带来了很高的可扩展性。</p><ul class=""><li id="8094" class="oa ob iq jp b jq jr ju jv jy oc kc od kg oe kk of og oh oi bi translated"><strong class="jp ir">代理并行</strong>:训练池中的代理是并行的，因此可以扩展到大量。并行代理的异步训练也可以减少代理对代理的通信频率。</li><li id="211b" class="oa ob iq jp b jq oj ju ok jy ol kc om kg on kk of og oh oi bi translated"><strong class="jp ir">学习者并行</strong>:一个agent采用多个学习者并行训练神经网络，然后融合网络的参数得到一个结果agent，而不是使用分布式SGD。这种通过网络参数的模型融合涉及低频通信，因为融合过程发生在一个时期的末尾。</li><li id="7c72" class="oa ob iq jp b jq oj ju ok jy ol kc om kg on kk of og oh oi bi translated"><strong class="jp ir"> Worker parallelism </strong>:一个代理利用多个部署工作器并行地对转换进行采样。</li></ul><p id="9fc5" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">弹性资源分配</strong>:弹性对于云级应用至关重要，因为它有助于用户适应云资源的变化，防止资源过度供应和供应不足。ElegantRL-Podracer可以通过使用orchestrator来监控可用的计算资源和当前的训练状态，从而灵活地分配代理(pod)的数量。</p><p id="9b11" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">面向云的优化</strong> : ElegantRL-Podracer在GPU上共同定位微服务，以加速数据收集和模型训练的并行计算。对于数据传输和存储，ElegantRL-Podracer将数据表示为张量以加快通信速度，并在GPU的连续内存上分配共享重放缓冲区以提高寻址速度。</p><h1 id="5507" class="lu lv iq bd lw lx nv lz ma mb nw md me mf nx mh mi mj ny ml mm mn nz mp mq mr bi translated">实验</h1><p id="22ef" class="pw-post-body-paragraph jn jo iq jp b jq ms js jt ju mt jw jx jy mu ka kb kc mv ke kf kg mw ki kj kk ij bi translated">金融是DRL算法的一个充满希望和挑战的现实世界应用。作为例子，我们将ElegantRL-podracer应用于股票交易任务，以展示其在量化金融中的潜力。</p><p id="7a03" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们的目标是训练一个DRL代理人来决定在股票市场上在哪里交易，在什么价格交易，在什么数量交易，因此问题的目标是最大化期望收益和最小化风险。我们将股票交易任务建模为马尔可夫决策过程(MDP)，如<a class="ae ks" href="https://github.com/AI4Finance-Foundation/FinRL" rel="noopener ugc nofollow" target="_blank"> FinRL </a>所示。我们遵循一个训练-回溯测试管道，并将数据集分为两组:从2016年1月1日到2020年5月25日的数据用于训练，从2020年5月26日到2021年5月26日的数据用于回溯测试。</p><p id="77e3" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这些实验是在一个<a class="ae ks" href="https://www.nvidia.com/en-us/data-center/dgx-superpod/" rel="noopener ugc nofollow" target="_blank"> DGX SuperPOD云</a>中使用NVIDIA DGX-2服务器执行的，这是一个云原生基础设施。</p><figure class="nh ni nj nk gt nl gh gi paragraph-image"><div role="button" tabindex="0" class="nm nn di no bf np"><div class="gh gi oq"><img src="../Images/9bad3658ee2dd8a5b7f4aadad6d9771f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AHPPzwbOBuMqWEdRmcXEiQ.png"/></div></div><p class="nr ns gj gh gi nt nu bd b be z dk translated">左图:分钟级别纳斯达克100指数成份股的累积回报率(初始资本<br/> $1，000，000，交易成本0.2%)。右图:使用ElegantRL-podracer和RLlib的模型快照，达到累积<br/>奖励1.7和1.8的训练时间(挂钟时间)。</p></figure><figure class="nh ni nj nk gt nl gh gi paragraph-image"><div role="button" tabindex="0" class="nm nn di no bf np"><div class="gh gi or"><img src="../Images/90f37b1015b4031516199270e76dd59d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3h-335IOU36NG0y0DP_J-g.png"/></div></div><p class="nr ns gj gh gi nt nu bd b be z dk translated">纳斯达克100成份股的分钟级数据的股票交易表现。</p></figure><p id="3fea" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">就累积回报而言，所有DRL代理人都可以取得比市场基准更好的表现，证明了算法的有效性。我们观察到ElegantRL-podracer累计回报104.743%，年回报103.591%，夏普比率2.20，大幅跑赢RLlib。然而，ElegantRL-podracer在回溯测试期间不像RLlib那样稳定:它实现了最大35.357%的年波动率。下降-17.187%，卡尔马尔比率为6.02。这种不稳定性有两个可能的原因:</p><ol class=""><li id="9bbe" class="oa ob iq jp b jq jr ju jv jy oc kc od kg oe kk op og oh oi bi translated">股票交易环境中的报酬设计主要与累积收益相关，从而导致代理人较少顾及风险；</li><li id="bdb8" class="oa ob iq jp b jq oj ju ok jy ol kc om kg on kk op og oh oi bi translated">ElegantRL-podracer在2021–03年前后持有大量资金，自然导致了更大的滑盘。</li></ol><p id="b897" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们比较了不同数量的GPU上的训练性能，即8、16、32和80。我们测量所需的训练时间，以获得两个分别为1.7和1.8的累积回报。无论是ElegantRL-podracer还是RLlib，随着GPU数量的增加，达到相同的累积回报所需的训练时间更少，这直接证明了云计算资源在DRL训练上的优势。对于80个GPU的ElegantRL-podracer，需要(1900，2200)才能达到1.7和1.8的累积回报。拥有32个和16个GPU的ElegantRL-podracer需要(2400、2800)和(3400、4000)才能实现相同的累积回报。它展示了ElegantRL-podracer的高可扩展性和我们面向云的优化的有效性。对于使用RLlib的实验来说，增加GPU的数量并不会带来很大的加速。</p></div><div class="ab cl ln lo hu lp" role="separator"><span class="lq bw bk lr ls lt"/><span class="lq bw bk lr ls lt"/><span class="lq bw bk lr ls"/></div><div class="ij ik il im in"><h1 id="facf" class="lu lv iq bd lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr bi translated">确认</h1><p id="9214" class="pw-post-body-paragraph jn jo iq jp b jq ms js jt ju mt jw jx jy mu ka kb kc mv ke kf kg mw ki kj kk ij bi translated">这项研究使用了由<a class="ae ks" href="https://idea.edu.cn/en" rel="noopener ugc nofollow" target="_blank"> IDEA研究院</a>提供的GPU云平台的计算资源。</p><h1 id="94fe" class="lu lv iq bd lw lx nv lz ma mb nw md me mf nx mh mi mj ny ml mm mn nz mp mq mr bi translated">参考</h1><p id="b84b" class="pw-post-body-paragraph jn jo iq jp b jq ms js jt ju mt jw jx jy mu ka kb kc mv ke kf kg mw ki kj kk ij bi translated">[1]埃里克·梁、理查德·廖、罗伯特·西原、菲利普·莫里茨、罗伊·福克斯、肯·戈德堡、约瑟夫·冈萨雷斯、迈克尔·乔丹和扬·斯托伊察。RLlib:分布式强化学习的抽象。在<em class="ko"> ICML </em>，第3053–3062页。PMLR，2018。</p><p id="e8b2" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">[2] J .舒尔曼、f .沃尔斯基、普拉富拉·德里瓦尔、亚历克·拉德福德和奥列格·克里莫夫。近似策略优化算法。<em class="ko"> ArXiv </em>，abs/1707.06347，2017。</p><p id="a542" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">[3] Erik Wijmans、Abhishek Kadian、Ari S. Morcos、Stefan Lee、Irfan Essa、Devi Parikh、M. Savva和Dhruv Batra。DD-PPO:从25亿帧中学习近乎完美的pointgoal导航仪。在<em class="ko"> ICLR </em>，2020年。</p><p id="058a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">[4]凯文·凯利。<em class="ko">失控:新生物文明的崛起</em>。艾迪森-韦斯利朗曼出版公司，1994年。</p></div></div>    
</body>
</html>