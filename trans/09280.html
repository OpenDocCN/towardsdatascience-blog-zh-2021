<html>
<head>
<title>The Complete Practical Guide to Topic Modelling</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">主题建模完全实用指南</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/topic-modelling-f51e5ebfb40a?source=collection_archive---------12-----------------------#2021-08-28">https://towardsdatascience.com/topic-modelling-f51e5ebfb40a?source=collection_archive---------12-----------------------#2021-08-28</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="3a2b" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">使用pyLDAvis执行主题建模的完整指南</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/94a075a4cbb88a60f07ff6f1fa9da81a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*dgkgCcW5tLZXG7SL"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">梅尔·普尔在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="1898" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">主题建模:<br/> </strong>这个NLP步骤的目的是理解输入数据中的主题，这些主题有助于分析文章或文档的上下文。该步骤还将进一步帮助使用在该步骤中跨每组相似文档生成的主题的数据标注需求。</p><p id="72a9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这种无监督统计建模算法的主要目的是理解输入数据中的主题。</p><p id="b2e4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这一步真的很关键，也需要一定程度的数学理解。</p><p id="9986" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">用于执行主题建模的重要库有:Pandas、Gensim、pyLDAvis。</p><p id="f882" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">用于生成主题的算法:LDA </strong></p><p id="c0e0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">LDA(潜在狄利克雷分配)是一种生成统计模型，它允许一组观察值由未观察到的组来解释，这些组解释了为什么数据的某些部分是相似的。</p><ul class=""><li id="534d" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated">它将文档视为一个单词集合或一个单词包。</li><li id="618c" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">LDA假设文档由帮助确定主题的单词组成，然后通过将文档中的每个单词分配给不同的主题，将相似的文档映射到主题列表。</li></ul><p id="1e5e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">下面，我将讨论将数据输入LDA模型以生成文档主题的不同步骤。</p><p id="c1c3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">步骤1: </strong>读取预处理后的数据集。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mj mk l"/></div></figure><p id="ddc3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">步骤2: </strong>读取N-Grams</p><ul class=""><li id="a83c" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated">在这一步中，创建的函数将读取传递的文本文件并返回列表。</li><li id="6d10" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">在同一步骤中，另一个函数将读取二元模型和三元模型，并返回N_Grams的列表</li></ul><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mj mk l"/></div></figure><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mj mk l"/></div></figure><p id="f404" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">步骤3: </strong>生成组合N_Grams</p><ul class=""><li id="0cc9" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated">这一步将读取获得的N_Grams，并使用' _ '返回组合的N_Grams列表。</li></ul><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mj mk l"/></div></figure><p id="65bb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">步骤4: </strong>将组合的N _ gram映射到单独的N _ gram</p><ul class=""><li id="a13c" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated">这一步将进行映射并返回字典。</li></ul><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mj mk l"/></div></figure><p id="b0eb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">步骤5: </strong>将N_Grams添加回数据集中每篇文章的文本中。</p><ul class=""><li id="b026" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated">这一步将用组合的N_Grams替换文本中最初出现的N_Grams。</li></ul><p id="733f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">例如:我已经为我的部落尝试了这么多种干粮。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mj mk l"/></div></figure><p id="5658" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">步骤6: </strong>从输入文本中删除停用词。</p><ul class=""><li id="3c5b" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated">这是至关重要的一步，因为这些词在定义主题时不起任何作用。</li></ul><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mj mk l"/></div></figure><p id="e37d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">第七步:</strong>去掉标点符号。</p><ul class=""><li id="87c2" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated">我们实际上考虑了一些特殊字符(。,?！)对于我们未来的工作是有效的，但是主题建模步骤根本不需要标点符号。</li><li id="a480" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">记住！主题建模背后的思想是文档由主题组成，主题由单词组成。所以，没有必要在这里保留标点符号。</li></ul><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mj mk l"/></div></figure><p id="570a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">步骤8: </strong>标记化</p><ul class=""><li id="c04a" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated">将文本分解为标记列表，为主题模型创建字典和文档术语矩阵。</li><li id="07d6" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">结果将是一个输入文本列表。</li></ul><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mj mk l"/></div></figure><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mj mk l"/></div></figure><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mj mk l"/></div></figure><p id="3aa0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">步骤9: </strong>根据POS_Tags过滤代币。</p><ul class=""><li id="0bc9" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated">这个函数将使用NLTK标记对应于语料库中每个标记的部分语音。</li></ul><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mj mk l"/></div></figure><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mj mk l"/></div></figure><p id="6e68" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">步骤10: </strong>创建字典和文档术语矩阵</p><ul class=""><li id="8214" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated">使用数据的符号化输入，并准备字典和文档术语矩阵。</li></ul><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mj mk l"/></div></figure><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mj mk l"/></div></figure><p id="5fc1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">使用LDA的主题建模:<br/> </strong>主题建模是指识别最能描述一组文档的主题的任务。LDA的目标是以某种方式将所有文档映射到主题，使得每个文档中的单词大部分被那些虚构的主题捕获。</p><p id="0e2e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">步骤-11: </strong>准备题目模型。</p><ul class=""><li id="429e" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated">在不同k值上训练LDA模型</li><li id="e3b9" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">LDA模型需要良好的内存和内核来加快训练速度。</li><li id="0a04" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">因此，明智地选择块大小参数非常重要。</li><li id="6c38" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">为了得到更好的结果，我已经运行这个模型10次了。</li><li id="8f24" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">不要忘记保存所有的结果，因为有可能10个结果中的第一个是最好的。</li></ul><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mj mk l"/></div></figure><p id="9e79" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">步骤-12: </strong>生成连贯性评分。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mj mk l"/></div></figure><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mj mk l"/></div></figure><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mj mk l"/></div></figure><ul class=""><li id="9335" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated">我已经为前25个LDA模型生成了一致性分数。</li><li id="c0f7" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">下面是5个LDA模型及其相应一致性分数的快照。</li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ml"><img src="../Images/ed2d67a1f120391e487f5db4fdc2b8b9.png" data-original-src="https://miro.medium.com/v2/resize:fit:822/format:webp/1*cCch1nUr1SumKyQHpGPGCA.png"/></div></figure><p id="f7f2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">可视化前25个LDA模型的一致性分数:</strong></p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mj mk l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mm"><img src="../Images/8df4ecb1fd89ea86ff75be90ff9c0852.png" data-original-src="https://miro.medium.com/v2/resize:fit:876/format:webp/1*KqX1BC82WJSoPpGqWi123w.png"/></div></figure><ul class=""><li id="86d5" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated">因为从上面的视觉效果可以非常明显地看出，对于等于4的主题数量，一致性得分最高。</li></ul><p id="c628" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu"> LDA模型结果:</strong></p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mj mk l"/></div></figure><ul class=""><li id="e28e" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated">由于我不能真正展示主题和视觉效果，我突出了视觉效果的一些部分。</li><li id="ac63" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">这是主题和主题5的关键字出现频率的可视化表示。</li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mn"><img src="../Images/d9b05c9c045e9c52188a08508db30bd5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1304/format:webp/1*YHHtmAGbE3NRJQKE7rMLtw.png"/></div></figure><p id="9fce" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">LDA模型的4个阶段:</strong></p><ul class=""><li id="86c4" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated">我花了一些时间对参数进行实验，得出了一些结果，我认为这些结果对将来超调LDA的参数非常有帮助，可以得到更好的结果。</li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mo"><img src="../Images/d5d0c4c264343c36a5008e2e28bab537.png" data-original-src="https://miro.medium.com/v2/resize:fit:748/format:webp/1*w9l07yHy-HEDV6DigYBylg.png"/></div></figure><p id="4d9a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">请参考<a class="ae ky" href="https://colab.research.google.com/drive/1LxteCTYhwRNzJ9aCPn7sAqdj79KAm4-p?usp=sharing" rel="noopener ugc nofollow" target="_blank"> <strong class="lb iu">完整代码</strong> </a>以便更好地理解。</p><h2 id="8d23" class="mp mq it bd mr ms mt dn mu mv mw dp mx li my mz na lm nb nc nd lq ne nf ng nh bi translated"><strong class="ak">主题建模的主要发现:</strong></h2><ul class=""><li id="ea74" class="lv lw it lb b lc ni lf nj li nk lm nl lq nm lu ma mb mc md bi translated">从上面的实验和结果，我得出结论，没有名词过滤器的模型表现最好。</li><li id="a754" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">保持alpha和eta值等于默认值，以获得更好的结果。</li><li id="9220" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">我得到了k=16和没有过滤器的数据的最佳结果。</li><li id="cb3d" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">LDA在小数据集上表现不佳。</li><li id="5b81" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">由于内存/时间问题，LDA几乎无法在大数据上运行。如果你有64x架构和16 GB或更大RAM容量的机器，它会运行得更好。</li><li id="5f7d" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">非常接近0的lambda值将显示对所选主题更具体的术语。这意味着我们将看到对特定主题“重要”但对整个语料库不一定“重要”的术语。</li><li id="a219" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">非常接近于1的λ值将显示那些在特定主题的术语的频率和来自语料库的术语的总频率之间具有最高比率的术语。</li><li id="ef27" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">对于所提供的数据集，我使用LDA找到了16个独特的主题。</li><li id="7b53" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">使用这个目标，我能够在每篇文章中找到覆盖文章大部分的最主要的主题。</li><li id="399d" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">我还看到了不同主题的文档数量，以及这些文档在同一主题中的相似程度。</li><li id="e8ad" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">结果真的令人印象深刻，因为主题是独特的和可分的。还包含了一组类似的文章，涵盖了每个独特的主题。</li></ul><p id="ff3d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">挑战:</strong></p><ul class=""><li id="9405" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated">LDA的时间复杂度和不可发现的输出。</li></ul><p id="9f88" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">得出结论，对LDA进行改进:</strong></p><ul class=""><li id="52c0" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated">经过一点研究，我发现LDA不适合小数据集。因为它是一种无监督的学习方法。我所说的小数据是指100篇文章或长度小于50个令牌的文章。因此，它可能无法很好地处理评论或评论数据。</li><li id="cd41" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">用于创建字典&amp; doc_term_matrix的矢量化技术可能是不可发现输出的问题，需要重新考虑。我们应该在word2vec()、doc2vec()和doc2bow()技术之间进行试验。</li><li id="a486" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">而不是只在LDA中过滤名词。应该着手检查标记有不同POS _ tags的令牌。在我的例子中，POS_tags (CD，MD，POS，CJ，PRP，DT)已经被删除。</li><li id="1532" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">对于POS_tags (VB，VBG，DT，PRP)，查看重要的标记，并在停用列表中提供其余不必要的标记。</li><li id="59ac" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">我已经写了执行这个过滤的确切函数，它最终确实改善了结果。要查看那个函数，请参考<a class="ae ky" href="https://colab.research.google.com/drive/1LxteCTYhwRNzJ9aCPn7sAqdj79KAm4-p?usp=sharing" rel="noopener ugc nofollow" target="_blank">这里的<strong class="lb iu">完整代码</strong> </a>。</li></ul><p id="725b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为了改善结果，应该总是通过改变其参数值来进行实验。</p><p id="4128" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">希望这篇文章能对你有所帮助，解决大部分与LDA相关的问题。</p><p id="8f10" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">感谢您的阅读。</p><p id="129c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">所以，如果我的博客帖子对你有所帮助，而你此刻觉得很慷慨，请不要犹豫，请给我买杯咖啡。☕😍</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><a href="https://www.buymeacoffee.com/techykajal"><div class="gh gi nn"><img src="../Images/d0d4f4d264b3d53ab60b92ba81600f19.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xpxVFhGlTQmm57O_jswZ-Q.png"/></div></a></figure><p id="b0c6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">是的，点击我。</p><pre class="kj kk kl km gt no np nq nr aw ns bi"><span id="f9f8" class="mp mq it np b gy nt nu l nv nw">And yes, buying me a coffee <strong class="np iu">(and lots of it if you are feeling extra generous)</strong> goes a long way in ensuring that I keep producing content every day in the years to come.</span></pre></div><div class="ab cl nx ny hx nz" role="separator"><span class="oa bw bk ob oc od"/><span class="oa bw bk ob oc od"/><span class="oa bw bk ob oc"/></div><div class="im in io ip iq"><p id="2e17" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">您可以通过以下方式联系我:</p><ol class=""><li id="fd5d" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu oe mb mc md bi translated">订阅我的<a class="ae ky" href="https://www.youtube.com/channel/UCdwAaZMWiRmvIBIT96ApVjw" rel="noopener ugc nofollow" target="_blank"> <strong class="lb iu"> YouTube频道</strong> </a>视频内容即将上线<a class="ae ky" href="https://www.youtube.com/channel/UCdwAaZMWiRmvIBIT96ApVjw" rel="noopener ugc nofollow" target="_blank"> <strong class="lb iu">这里</strong> </a></li><li id="cadd" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu oe mb mc md bi translated">跟我上<a class="ae ky" href="https://medium.com/@TechyKajal" rel="noopener"> <strong class="lb iu">中</strong> </a></li><li id="70ff" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu oe mb mc md bi translated">通过<a class="ae ky" href="http://www.linkedin.com/in/techykajal" rel="noopener ugc nofollow" target="_blank"> <strong class="lb iu"> LinkedIn </strong> </a>联系我</li><li id="5415" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu oe mb mc md bi translated">跟随我的博客之旅:-<a class="ae ky" href="https://kajalyadav.com/" rel="noopener ugc nofollow" target="_blank"><strong class="lb iu">https://kajalyadav.com/</strong></a></li><li id="aa09" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu oe mb mc md bi translated">成为会员:<a class="ae ky" href="https://techykajal.medium.com/membership" rel="noopener">https://techykajal.medium.com/membershipT21</a></li></ol></div><div class="ab cl nx ny hx nz" role="separator"><span class="oa bw bk ob oc od"/><span class="oa bw bk ob oc od"/><span class="oa bw bk ob oc"/></div><div class="im in io ip iq"><p id="2c1c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">也可以看看我的其他博客:</p><div class="of og gp gr oh oi"><a rel="noopener follow" target="_blank" href="/15-free-open-source-data-resources-for-your-next-data-science-project-6480edee9bc1"><div class="oj ab fo"><div class="ok ab ol cl cj om"><h2 class="bd iu gy z fp on fr fs oo fu fw is bi translated">为您的下一个数据科学项目提供15种免费开源数据资源</h2><div class="op l"><h3 class="bd b gy z fp on fr fs oo fu fw dk translated">为初学者和专业人士按不同类别组织的免费数据集的合并列表</h3></div><div class="oq l"><p class="bd b dl z fp on fr fs oo fu fw dk translated">towardsdatascience.com</p></div></div><div class="or l"><div class="os l ot ou ov or ow ks oi"/></div></div></a></div><div class="of og gp gr oh oi"><a rel="noopener follow" target="_blank" href="/text-clustering-using-k-means-ec19768aae48"><div class="oj ab fo"><div class="ok ab ol cl cj om"><h2 class="bd iu gy z fp on fr fs oo fu fw is bi translated">基于K-means的文本聚类</h2><div class="op l"><h3 class="bd b gy z fp on fr fs oo fu fw dk translated">K-means算法的理论和实践理解完全指南</h3></div><div class="oq l"><p class="bd b dl z fp on fr fs oo fu fw dk translated">towardsdatascience.com</p></div></div><div class="or l"><div class="ox l ot ou ov or ow ks oi"/></div></div></a></div></div></div>    
</body>
</html>