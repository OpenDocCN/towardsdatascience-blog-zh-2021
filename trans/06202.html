<html>
<head>
<title>Label Smoothing — Make your model less (over)confident</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">标注平滑-使您的模型不那么(过于)自信</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/label-smoothing-make-your-model-less-over-confident-b12ea6f81a9a?source=collection_archive---------16-----------------------#2021-06-03">https://towardsdatascience.com/label-smoothing-make-your-model-less-over-confident-b12ea6f81a9a?source=collection_archive---------16-----------------------#2021-06-03</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="cecd" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">解决机器学习中过度自信的直观解释。</h2></div><blockquote class="kf"><p id="5550" class="kg kh iq bd ki kj kk kl km kn ko kp dk translated">艺术家的某种死亡是过度自信——罗宾·特罗沃</p></blockquote><p id="17c1" class="pw-post-body-paragraph kq kr iq ks b kt ku jr kv kw kx ju ky kz la lb lc ld le lf lg lh li lj lk kp ij bi translated">还记得那次<strong class="ks ir"> <em class="ll">你关掉了谷歌地图</em> </strong>因为你非常自信知道去目的地的路，但结果却是因为施工道路被关闭了..毕竟你不得不使用谷歌地图。那是你过于自信了。如何解决对机器的过度自信？有时，很容易把一件事和另一件事混淆。因此，对某些你100%有信心的事情，最好少一点信心。</p><figure class="ln lo lp lq gt lr gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi lm"><img src="../Images/3a65de18641448c6722fbe1a5add75fa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kDbXkI7CwLPzHqfuvGAFUw.jpeg"/></div></div><p class="ly lz gj gh gi ma mb bd b be z dk translated">来源:<a class="ae mc" href="https://unsplash.com/s/photos/car" rel="noopener ugc nofollow" target="_blank"> unsplash </a></p></figure><p id="5057" class="pw-post-body-paragraph kq kr iq ks b kt md jr kv kw me ju ky kz mf lb lc ld mg lf lg lh mh lj lk kp ij bi translated">标签平滑可以防止网络变得过于自信，并已被用于许多最先进的模型，包括图像分类、语言翻译和语音识别。标签平滑是一个简单而有效的正则化工具，可以对标签进行操作。</p><p id="09d1" class="pw-post-body-paragraph kq kr iq ks b kt md jr kv kw me ju ky kz mf lb lc ld mg lf lg lh mh lj lk kp ij bi translated">通过谈论机器学习中的过度自信，我们主要是在谈论硬标签。</p><p id="4624" class="pw-post-body-paragraph kq kr iq ks b kt md jr kv kw me ju ky kz mf lb lc ld mg lf lg lh mh lj lk kp ij bi translated"><strong class="ks ir">软标签</strong>:软标签是一个带有某种概率/可能性的分数。例如:(0.1 0.2 0.8)</p><p id="7f06" class="pw-post-body-paragraph kq kr iq ks b kt md jr kv kw me ju ky kz mf lb lc ld mg lf lg lh mh lj lk kp ij bi translated"><strong class="ks ir">硬标签</strong>:硬标签通常是这两个类之一的一部分。它本质上是二进制的(0或1)</p><p id="97e1" class="pw-post-body-paragraph kq kr iq ks b kt md jr kv kw me ju ky kz mf lb lc ld mg lf lg lh mh lj lk kp ij bi translated">对于二进制交叉熵损失，我们通过在均匀分布和硬标签之间应用加权平均来将硬标签转换成软标签。标签平滑通常用于提高鲁棒性和改善分类问题。</p><figure class="ln lo lp lq gt lr gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi gj"><img src="../Images/79c5f5aea3d61b99d055f019588bbe6b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BDXCF-C9azGzr-OFzBFlEw.png"/></div></div><p class="ly lz gj gh gi ma mb bd b be z dk translated">来源:<a class="ae mc" href="https://arxiv.org/pdf/2011.12562.pdf" rel="noopener ugc nofollow" target="_blank">深入研究标签平滑</a></p></figure><p id="9991" class="pw-post-body-paragraph kq kr iq ks b kt md jr kv kw me ju ky kz mf lb lc ld mg lf lg lh mh lj lk kp ij bi translated">标签平滑是输出分布正则化的一种形式，它通过软化训练数据中的基本事实标签来防止神经网络的过度拟合，以试图惩罚过度自信的输出。</p><p id="50c1" class="pw-post-body-paragraph kq kr iq ks b kt md jr kv kw me ju ky kz mf lb lc ld mg lf lg lh mh lj lk kp ij bi translated">标签平滑背后的直觉不是让模型知道特定的输入仅导致特定的输出<strong class="ks ir"> <em class="ll">。这与过度适应有关，但与过度自信关系更大。我们可以将硬标签转换为ie，而不是像上面显示的那样将100%的概率分配给某个类别索引。100%到91%,把剩下的9%的不确定性交给其他完全没有信心的阶层。这不会损害模型性能，同时提供了更高的泛化能力。</em></strong></p><p id="a80f" class="pw-post-body-paragraph kq kr iq ks b kt md jr kv kw me ju ky kz mf lb lc ld mg lf lg lh mh lj lk kp ij bi translated">Tensorflow只需将指定为参数，就可以更容易地实现交叉熵损失的label_smoothing。</p><figure class="ln lo lp lq gt lr"><div class="bz fp l di"><div class="mi mj l"/></div></figure><p id="64d3" class="pw-post-body-paragraph kq kr iq ks b kt md jr kv kw me ju ky kz mf lb lc ld mg lf lg lh mh lj lk kp ij bi translated">您可以使用以下公式执行标注平滑:</p><p id="8ce4" class="pw-post-body-paragraph kq kr iq ks b kt md jr kv kw me ju ky kz mf lb lc ld mg lf lg lh mh lj lk kp ij bi translated"><code class="fe mk ml mm mn b">new_labels = original_labels * (1 – label_smoothing) + label_smoothing / num_classes</code></p><p id="1fb9" class="pw-post-body-paragraph kq kr iq ks b kt md jr kv kw me ju ky kz mf lb lc ld mg lf lg lh mh lj lk kp ij bi translated">示例:假设您有三个label_smoothing factor为0.3的类。</p><p id="48d0" class="pw-post-body-paragraph kq kr iq ks b kt md jr kv kw me ju ky kz mf lb lc ld mg lf lg lh mh lj lk kp ij bi translated">那么，根据上面的公式，new_labels将是:</p><p id="e335" class="pw-post-body-paragraph kq kr iq ks b kt md jr kv kw me ju ky kz mf lb lc ld mg lf lg lh mh lj lk kp ij bi translated"><strong class="ks ir"><em class="ll">=[0 1 2]*(1–0.3)+(0.3/3)</em></strong></p><p id="86e1" class="pw-post-body-paragraph kq kr iq ks b kt md jr kv kw me ju ky kz mf lb lc ld mg lf lg lh mh lj lk kp ij bi translated"><strong class="ks ir"><em class="ll">=[0 1 2]*(0.7)+0.1 =[0.1 0.8 1.5]</em></strong></p><h2 id="cc6b" class="mo mp iq bd mq mr ms dn mt mu mv dp mw kz mx my mz ld na nb nc lh nd ne nf ng bi translated">现在，新标签将是[<strong class="ak">0.1 0.8 1.5]</strong>而不是[0 1 2]</h2><p id="2255" class="pw-post-body-paragraph kq kr iq ks b kt nh jr kv kw ni ju ky kz nj lb lc ld nk lf lg lh nl lj lk kp ij bi translated">如你所见，模型变得不太自信，带有极度自信的标签。这正是我们想要避免的。现在，由于不正确的预测而给予模型的惩罚将略低于使用<strong class="ks ir">硬</strong>标签，这将导致更小的梯度。</p><h2 id="5085" class="mo mp iq bd mq mr ms dn mt mu mv dp mw kz mx my mz ld na nb nc lh nd ne nf ng bi translated">参考资料:</h2><p id="b170" class="pw-post-body-paragraph kq kr iq ks b kt nh jr kv kw ni ju ky kz nj lb lc ld nk lf lg lh nl lj lk kp ij bi translated">[1]<a class="ae mc" href="https://ai.stackexchange.com/questions/9635/about-the-definition-of-soft-label-and-hard-label" rel="noopener ugc nofollow" target="_blank">https://ai . stack exchange . com/questions/9635/about-the-definition-of-soft-label-and-hard-label</a></p><p id="4622" class="pw-post-body-paragraph kq kr iq ks b kt md jr kv kw me ju ky kz mf lb lc ld mg lf lg lh mh lj lk kp ij bi translated">[2] <a class="ae mc" href="https://arxiv.org/pdf/1906.02629.pdf" rel="noopener ugc nofollow" target="_blank">标签平滑什么时候有帮助？</a></p><p id="8f8c" class="pw-post-body-paragraph kq kr iq ks b kt md jr kv kw me ju ky kz mf lb lc ld mg lf lg lh mh lj lk kp ij bi translated">[3] <a class="ae mc" href="https://arxiv.org/pdf/2011.12562.pdf" rel="noopener ugc nofollow" target="_blank">深入研究标签平滑</a></p></div></div>    
</body>
</html>