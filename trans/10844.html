<html>
<head>
<title>Why Gradient Descent Works?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">为什么梯度下降有效？</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/why-gradient-descent-works-4e487d3c84c1?source=collection_archive---------15-----------------------#2021-10-20">https://towardsdatascience.com/why-gradient-descent-works-4e487d3c84c1?source=collection_archive---------15-----------------------#2021-10-20</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="fa8f" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">每个人都知道什么是梯度下降，以及它是如何工作的。有没有想过它为什么有效？这里有一个数学解释</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/7cd28cf9881fa869ba4615725feb7ea0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*UuWQSZLhG7BvlXjq"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">由<a class="ae kv" href="https://unsplash.com/@ychemerys?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">塞维多夫·切梅里斯</a>在<a class="ae kv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><h1 id="a370" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">什么是梯度下降？</h1><p id="d0ca" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">梯度下降是一种迭代优化算法，用于优化机器学习模型(线性回归、神经网络等)的权重。)通过最小化该模型的成本函数。</p><p id="a019" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">梯度下降背后的直觉是这样的:把代价函数(用<strong class="lq ir"><em class="mp">f(</em>θ̅<em class="mp">)</em></strong>其中<strong class="lq ir">θ̅</strong>=[θ₁、……θₙ<strong class="lq ir"><em class="mp"/></strong>)<strong class="lq ir"><em class="mp"/></strong>标绘在<strong class="lq ir"> <em class="mp"> n </em> </strong> <em class="mp"> </em>维度上)画成一个碗。想象在那个碗上随机放置一个点，用<strong class="lq ir"> <em class="mp"> n </em> </strong> <em class="mp"> </em>坐标表示(这是你的代价函数的初始值)。这个“函数”的最小值将是碗的底部。</p><p id="3469" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">目标是通过在碗上逐渐向下移动到达碗的底部(或最小化成本)。为了实现这一点，我们应该知道碗在不同方向上相对于当前点的斜率，这样我们就可以决定向哪个方向移动以获得最快的收敛。为了确定这一点，我们计算函数在该点的梯度。通过迭代，我们将最终到达某个最小值点，该点的坐标将是模型的最优权重。</p><h1 id="1614" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">梯度下降是如何工作的</h1><p id="d225" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">下面是梯度下降的更正式的定义:</p><p id="706c" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">从输入矩阵<strong class="lq ir"><em class="mp">×t35<strong class="lq ir"><em class="mp"/></strong>开始，n维输入向量和一个初始值给权重<strong class="lq ir">【θ̅】，</strong>我们知道，在线性回归方程中给出:<strong class="lq ir"><em class="mp">【y̅=</em>【θ̅ᵗ】<em class="mp">×</em>。</strong>现在，如果成本函数由<strong class="lq ir"><em class="mp">f(</em>θ̅<em class="mp">)，</em> </strong>给出，这就是我们如何制定梯度下降:</em></strong></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mq"><img src="../Images/d2a861e103317f63e2940014f75f468f.png" data-original-src="https://miro.medium.com/v2/resize:fit:784/format:webp/1*kL62kPkeS9aOCti5GfJG7Q.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">梯度下降算法公式(图片由作者提供)</p></figure><p id="aafb" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">这里，<strong class="lq ir"> <em class="mp"> t </em> </strong> <em class="mp"> </em>是迭代计数器，<strong class="lq ir"><em class="mp">【tₘₐₓ】</em></strong>是我们运行循环的最大迭代次数。</p><h1 id="26c9" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">为什么有效</h1><p id="dd7b" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">这里的第一个问题应该是:二阶导数测试以及任何函数<strong class="lq ir"><em class="mp">f(</em>【θ̅】<em class="mp">)</em></strong><em class="mp"/>等于零时的二阶导数告诉你<strong class="lq ir">θ̅</strong>是该函数的局部最大值还是局部最小值。现在，既然我们对最小化感兴趣，我们可以逆向工程这个概念，求解<strong class="lq ir">θ̅</strong>给定<strong class="lq ir">t13】f(θ̅<em class="mp">)</em></strong><em class="mp"/>的二阶导数为负。</p><p id="ce63" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">那么，为什么我们不这样做，而是迭代地解决最小化问题(如梯度下降)？答案在于，首先，也是最重要的，二阶导数不能保证永远存在。第二，求解<strong class="lq ir">θ̅ₘᵢₙ</strong>的二阶导数方程的计算复杂度非常高。事实上，有时不可能用微积分来解决，而必须用数值来解决。</p><h2 id="21fd" class="mr kx iq bd ky ms mt dn lc mu mv dp lg lx mw mx li mb my mz lk mf na nb lm nc bi translated">最小化问题</h2><p id="21e5" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">因此，在数字上指定一般的最小化问题(并因此迭代)，我们说<strong class="lq ir">θ̅ₜ₊₁=θ̅ₜ+<em class="mp">dₜ</em></strong>其中<strong class="lq ir"> <em class="mp"> dₜ </em> </strong>是在时间<strong class="lq ir"><em class="mp"/></strong>的最小化的一些一般方向(回头看碗的例子)</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nd"><img src="../Images/dc7c524251b0dfa3eeb8b4c3608f7b86.png" data-original-src="https://miro.medium.com/v2/resize:fit:1274/format:webp/1*FSjUaTITXUZoxIOV_lOc5w.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">最小化算法通用格式的规范(图片由作者提供)</p></figure><p id="9473" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">现在，来选择合适的<strong class="lq ir"><em class="mp"/></strong>dₜ<strong class="lq ir">θ̅ₜ₊₁&lt;θ̅ₜ</strong>——然后我们走向最小化。</p><h2 id="5aa0" class="mr kx iq bd ky ms mt dn lc mu mv dp lg lx mw mx li mb my mz lk mf na nb lm nc bi translated">泰勒级数</h2><p id="2520" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">为了解决上述最小化问题，我们利用泰勒级数来表示<strong class="lq ir"> <em class="mp"> n </em> </strong>维度。我们用以<strong class="lq ir"><em class="mp">f(</em>θ̅ₜ<em class="mp">)为中心的泰勒级数来计算<strong class="lq ir"><em class="mp">f</em></strong>(<strong class="lq ir">θ̅ₜ₊₁</strong>)。</em> </strong>这是它的样子:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ne"><img src="../Images/a09de5e85a185d8312e84cbcf5187ee1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1312/format:webp/1*9Un57Vw4qnMGyk826_aIJA.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">n维泰勒级数及其在1维泰勒级数中的对应项(图片由作者提供)</p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nf"><img src="../Images/3c4b4345a4eff7006bb438ce8e92c8e9.png" data-original-src="https://miro.medium.com/v2/resize:fit:956/format:webp/1*Kb9yjPogecUbByC2RmCOFg.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">海森矩阵(图片由作者提供)</p></figure><p id="a567" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">这里的矩阵<strong class="lq ir"><em class="mp"/></strong>是海森矩阵，代表函数<strong class="lq ir"><em class="mp">f(</em><em class="mp">)</em></strong>的二阶偏导数</p><p id="4ab0" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">接下来，我们从最小化的定义中代入方程:<strong class="lq ir">θ̅ₜ₊₁=θ̅ₜ+<em class="mp">dₜ</em></strong></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ng"><img src="../Images/09eca2ff822ae7358aeb5786196cdb06.png" data-original-src="https://miro.medium.com/v2/resize:fit:1212/format:webp/1*mnTU8VW9S4nP_oPx5-RDYA.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">求解泰勒级数(图片由作者提供)</p></figure><p id="6eec" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">在上图中，我们在渐近项本身中加入了三次项，因为我们只想处理前两项，我们得到了<strong class="lq ir"><em class="mp">【o(||dₜ||】</em></strong><strong class="lq ir"><em class="mp">)。</em> </strong>接下来，如前所述我们尝试选择最小化的方向<strong class="lq ir"><em class="mp"/></strong>这将给我们<strong class="lq ir"><em class="mp">f</em></strong>(<strong class="lq ir">【θ̅ₜ₊₁】&lt;<em class="mp">f</em></strong><em class="mp">(</em><strong class="lq ir">【θ̅ₜ】</strong>)并因此引导我们在<strong class="lq ir"> <em class="mp"> t </em> </strong>时间步长上朝向最小化。</p><p id="9784" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">我们选择<strong class="lq ir"> <em class="mp"> dₜ </em> </strong>作为时间<strong class="lq ir"><em class="mp"/>—</strong>的代价函数的梯度的负值，并且开始出现与梯度下降的相似性，并且我们获得最后一条线。注意，在上面最后一行的等式中，第一项和第二项是相似的，最终结果可以是正的，也可以是负的。这实际上取决于包含在渐近界限内的高阶项。为了确保我们最终在右边得到一个负值，并且随着时间的推移得到一个递减的值，我们引入了一个新术语<strong class="lq ir"> <em class="mp"> α </em> </strong>。</p><h2 id="792b" class="mr kx iq bd ky ms mt dn lc mu mv dp lg lx mw mx li mb my mz lk mf na nb lm nc bi translated">…还有维奥拉！</h2><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nh"><img src="../Images/ddf928f2a5f674d785c8cd57b72d8244.png" data-original-src="https://miro.medium.com/v2/resize:fit:1276/format:webp/1*i5Wzn0zZjbzZ7XRKgM-j4w.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">介绍最小化中的学习率(图片由作者提供)</p></figure><p id="377b" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">你猜对了！<strong class="lq ir"> <em class="mp"> α </em> </strong>就是我们平常看到的梯度下降的学习率。引入<strong class="lq ir"> <em class="mp"> α </em> </strong>并确保很少保证上述最终等式中的第二项总是小于第一项，因此，我们在右侧得到一个负的结果。这反过来确保了我们最小化函数<strong class="lq ir"><em class="mp">f(</em><em class="mp">)</em></strong>随时间<strong class="lq ir"><em class="mp"/></strong></p></div><div class="ab cl ni nj hu nk" role="separator"><span class="nl bw bk nm nn no"/><span class="nl bw bk nm nn no"/><span class="nl bw bk nm nn"/></div><div class="ij ik il im in"><p id="dddc" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">本文灵感来源于Rutgers大学Cowan博士的系列讲座。</p><p id="830e" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">你可以在这里找到方程式<a class="ae kv" href="https://github.com/kunjmehta/Medium-Article-Codes/blob/master/why_gradient_descent_works.pdf" rel="noopener ugc nofollow" target="_blank">的Latex文件</a>。我很乐意在Linkedin 上联系！</p></div></div>    
</body>
</html>