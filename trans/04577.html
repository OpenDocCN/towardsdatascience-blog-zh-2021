<html>
<head>
<title>An Introduction to Reinforcement Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">强化学习导论</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/an-introduction-to-reinforcement-learning-4ecc096ee80f?source=collection_archive---------43-----------------------#2021-04-19">https://towardsdatascience.com/an-introduction-to-reinforcement-learning-4ecc096ee80f?source=collection_archive---------43-----------------------#2021-04-19</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="dedf" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">展示支持RL的关键概念</h2></div><p id="46fb" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">最近在强化学习的帮助下取得的成功被媒体广泛报道。人们可以想到DeepMind的<a class="ae le" href="https://deepmind.com/research/case-studies/alphago-the-story-so-far" rel="noopener ugc nofollow" target="_blank"> AlphaGo </a>算法:使用强化学习(和大量昂贵的硬件来训练它)，AlphaGo学会了玩围棋这一古老的游戏，甚至开发了自己的游戏风格。</p><p id="eb84" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">OpenAI展示了另一个例子，其研究人员<a class="ae le" href="https://openai.com/blog/solving-rubiks-cube/" rel="noopener ugc nofollow" target="_blank">教会了一只机器手解魔方</a>。有趣的是，它是在虚拟环境中训练的；因为即使拿着一个立方体转动而不掉它也是相当困难的，这是令人惊奇的第一步。</p><p id="b460" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我写这篇文章的目的是首先解释使这种成功成为可能的概念。我从介绍开始，然后逐步完善RL的定义。</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi lf"><img src="../Images/35768d59b343e696240ab5fef35a9f5d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*JWQOPAFeQqGt3Qv9"/></div></div><p class="lr ls gj gh gi lt lu bd b be z dk translated">由<a class="ae le" href="https://unsplash.com/@davidleveque?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">大卫·莱维克</a>在<a class="ae le" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><h1 id="fc15" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">介绍</h1><p id="269a" class="pw-post-body-paragraph ki kj it kk b kl mn ju kn ko mo jx kq kr mp kt ku kv mq kx ky kz mr lb lc ld im bi translated">强化学习是机器学习的一个领域。您可能已经使用过监督学习(数据被标记)和非监督学习(未标记的数据，例如，用于生成技术)。这两大领域由强化学习领域补充。两个关键区别在于，数据通常不是独立且相同地分布的(即，它可能非常杂乱，没有明显的结构)，并且代理的行为决定了它遇到的未来数据。</p><p id="ff2d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">用一句非正式的话来说，强化学习学习通过互动来达到一个目标。</p><p id="57cd" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们来看看机器学习这一分支背后的基本成分:</p><p id="9100" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">首先，我们有提到的代理。代理既是学习者又是决策者。现实生活中的类比就是你和我，都在做决策，都在不停地学习。第二，代理可以与之交互的一切都是环境(对我们来说是我们的正常环境:其他人、建筑物、汽车、狗、猫等等)。第三，这种环境可以通过互动来改变，这是代理人产生影响的唯一方式。在每一种可能的情况下，代理人可以决定一个行动，并依次获得奖励。这种奖励会影响学习过程。</p><h1 id="d3e1" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">MDP</h1><p id="5a5c" class="pw-post-body-paragraph ki kj it kk b kl mn ju kn ko mo jx kq kr mp kt ku kv mq kx ky kz mr lb lc ld im bi translated">这个描述相当模糊。为了给强化学习下一个更精确的定义，我们需要一种数学方法。这就是马尔可夫决策过程的用武之地，它是一种对状态之间的转换进行建模的数学方法。我们有一个五元组(s，a，r，p，p₀) [1，2]:</p><p id="4cb6" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">S是状态空间，包含一个代理可能遇到的所有可能的状态。因此，每一个状态s ∈ S。</p><p id="74ac" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">a是动作空间，包含代理可能的动作。同样，我们有每一个动作。</p><p id="2570" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><em class="ms"> R </em>是奖励函数，它基于当前状态sₜ、选择的动作aₜ和结果状态sₜ₊₁.，传递代理动作的奖励我们包括结果状态也是为了考虑状态之间的转换。如果没有它，在不同的时间步骤在相同的状态下行动会产生相同的回报——不考虑代理人可能已经学到了什么。换句话说，动作a在<em class="ms"> t </em>和<em class="ms"> t+k </em>可以是相同的，但是可能导致不同的新状态。我们通过添加sₜ₊₁.来考虑这一点奖励是一个实数，比如+1，-2.5等等。</p><p id="56a1" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">P是将状态和动作映射到概率的函数。它决定了从sₜ出发并选择aₜ.行动时到达sₜ₊₁州的概率我们的环境可能会受到我们无法控制的事件的影响。为此，我们有P，它考虑了外部影响。如果我们没有这样的影响，我们简单地使用一个确定性的转变，也就是说，新的状态是保证达到的。</p><p id="7fed" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">最后，我们有p₀，这是初始状态的分布，也就是说，它包含了所有我们可以开始的状态。</p></div><div class="ab cl mt mu hx mv" role="separator"><span class="mw bw bk mx my mz"/><span class="mw bw bk mx my mz"/><span class="mw bw bk mx my"/></div><div class="im in io ip iq"><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi na"><img src="../Images/1b7f878432fd80b563e668f84821220c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*Z61SXGWPWE8KylY6"/></div></div><p class="lr ls gj gh gi lt lu bd b be z dk translated"><a class="ae le" href="https://unsplash.com/@gronemo?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Roméo A. </a>在<a class="ae le" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="bd4d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">你还记得手机游戏DoodleJump吗？还是任天堂设备上的马里奥游戏？让我们构建一个与这两个类似的设置:</p><p id="b97a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们的敌人从右侧进来(这是来自马里奥)，我们向上跳一级就可以躲开他们(这是来自嘟嘟跳)。我们的状态空间<em class="ms"> S </em>由两个状态组成，<em class="ms">低</em>和<em class="ms">高</em>。</p><p id="1ce3" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在这两种状态下，我们有两种可能的动作，<em class="ms">跳转</em>和<em class="ms">等待</em>，它们构成了动作空间<em class="ms"> A </em>。</p><p id="9504" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">每次我们从低处跳到高处或者从高处跳到高处，我们都会得到+1的奖励。当我们被敌人击中时，我们会下降一级，奖励是-1(注意，这里的奖励不是与一个动作相关联，例如跳跃或等待，而是与我们被外部影响击中的事实相关联)。等待的回报是0。</p><p id="a72d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在到<em class="ms"> P </em>，它决定了我们达到下一个水平的可能性有多大。这是80 %，有20 %我们保持在目前的水平。等待可能导致停留在当前水平或被敌人击中的概率相等；因此，两者的概率都是50 %。最后，我们的<em class="ms"> p </em> ₀只包含状态<em class="ms">低</em>。</p><h1 id="17ef" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">政策</h1><p id="022d" class="pw-post-body-paragraph ki kj it kk b kl mn ju kn ko mo jx kq kr mp kt ku kv mq kx ky kz mr lb lc ld im bi translated">到目前为止，我们已经讨论了动作和环境，但是没有讨论如何在给定的状态下确定适当的动作。因为我们有多个状态和多个时间步，我们需要一些进一步的算法来帮助我们。这是策略的任务π，从一个状态到一个动作的映射:π(aₜ|sₜ)是在状态sₜ选择aₜ的概率[1，2](你可以想一套规则:如果这个那么那个)。由此出发，我们可以从所有可能的动作中抽取动作，用aₜπ表示。|sₜ).</p><p id="627f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">该策略是可以由深度RL中的神经网络建模的部分[1]。</p></div><div class="ab cl mt mu hx mv" role="separator"><span class="mw bw bk mx my mz"/><span class="mw bw bk mx my mz"/><span class="mw bw bk mx my"/></div><div class="im in io ip iq"><p id="372a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">对于我们的示例，我们的策略由以下映射组成:(jump|low) = (jump|high) = 50 %。这意味着在低和高的状态中，我们以50 %的概率选择动作<em class="ms">跳转</em>。(等待|低)和(等待|高)的概率各为50 %。</p><h1 id="9702" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">奖励</h1><p id="ecef" class="pw-post-body-paragraph ki kj it kk b kl mn ju kn ko mo jx kq kr mp kt ku kv mq kx ky kz mr lb lc ld im bi translated">到目前为止，我们已经介绍了政策，但还没有介绍奖励的形式和作用。开始时，我注意到我们希望代理实现一个目标。这个比较模糊。更正式地说，我们希望代理人最大化预期报酬。</p><p id="a362" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们来看看这个:</p><p id="62e7" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">从我们最初的状态，s₀和a₀把我们带到s₁.和a₁一起我们去了s₂.和a₂一起，我们前进到s₃.这种情况可能会持续无限长的时间。这种状态和动作的序列被称为<em class="ms">轨迹</em>，表示为τ。此外，我们还会在每个动作之后获得奖励，奖励rₜ₊₁(因为奖励考虑了结果状态，所以奖励也会在下一个时间步支付，<em class="ms"> t+1 </em>)。有了轨迹和奖励，我们可以通过对所有个人奖励求和来计算我们的最终<em class="ms">回报</em>，或累计奖励【1，2】</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi nb"><img src="../Images/8a5dde8a05842fa9355cc0a84ed5ae9f.png" data-original-src="https://miro.medium.com/v2/resize:fit:644/format:webp/1*tPgOA7pUOFcRw_kRQbeYhQ.png"/></div></figure></div><div class="ab cl mt mu hx mv" role="separator"><span class="mw bw bk mx my mz"/><span class="mw bw bk mx my mz"/><span class="mw bw bk mx my"/></div><div class="im in io ip iq"><p id="75be" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">回到我们的例子，考虑这个轨迹:(低，跳，高，跳，高)。由低到高过渡的奖励是+1，由高到高的奖励也是+1。对于这个轨迹，回报是2。</p></div><div class="ab cl mt mu hx mv" role="separator"><span class="mw bw bk mx my mz"/><span class="mw bw bk mx my mz"/><span class="mw bw bk mx my"/></div><div class="im in io ip iq"><p id="b015" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在有限的动作序列的情况下，这就是我们所需要的。但是当我们处理无休止运行的环境时，这就产生了一个问题:</p><p id="fb6d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">每个奖励目前都具有相同的重要性，无论它发生在一个近时间点(因此可能更有形)还是无限的边缘(非常无形)。此外，这也是更重要的，我们的回报是没有界限的，它不会收敛到一个固定的值。这使得比较两个策略变得很困难:一个可能总是得到+1，-1，+1，-1，…，而另一个策略可能经常得到+10，+10，+10，只是后来犯了一个严重的错误，抵消了我们到目前为止所取得的一切。换句话说，当轨迹无穷无尽时，我们不知道从长远来看哪种政策会更好，因为一切都可能发生。</p><p id="2df9" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">幸运的是，这可以通过使用折扣回报来解决，这是一种更通用的方法:我们用折扣乘以每个奖励，这量化了它的重要性(将折扣设置为1导致之前的未折扣回报)[1，2]:</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi nc"><img src="../Images/3c506e05a1302146f1800f0be44372a8.png" data-original-src="https://miro.medium.com/v2/resize:fit:848/format:webp/1*peCUw1Xd3vlIUsh3B5e-KA.png"/></div></figure><p id="a18e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">因此，贴现回报迫使代理人“早赢比晚赢好”[3]。</p><p id="818f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">为了最大化预期回报，我们需要我们的政策(检查)，我们的轨迹(检查)，和回报(检查)。配备了它们，我们开始:</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi nd"><img src="../Images/03b5e325ca34fc68e679700f7a10c0a2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1178/format:webp/1*0pxx_YUTGqYJoyZGtRc1vw.png"/></div></figure><p id="0ccf" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我解释一下这个等式:给定一个策略π，我们想知道我们得到一个特定的状态和动作序列的可能性有多大(这是轨迹，τ)。为此，我们取我们的初始状态s₀的概率，然后乘以所有进一步的概率(这是∏部分)。对于我们到达的每一个后来的状态sₜ₊₁，我们检查它到达这个状态(这是P(sₜ₊₁|sₜ,aₜ)的可能性，并且将它乘以采取引导我们到达那里的行动的可能性(这是π(aₜ|sₜ)).)</p><p id="caa1" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在此基础上，我们写下预期回报是我们通过政策π获得的回报，然后根据它采取行动(这为我们提供了轨迹，即状态-行动序列的列表)。如上所述，对于每个这样的轨迹，我们都有一个回报。检查所有可能的轨迹(所有可能的状态-行动序列)，并总结它们各自的回报，我们就有了预期的回报[1，2]:</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi ne"><img src="../Images/ecbb4e12fa9b238cad1477dd8d37d975.png" data-original-src="https://miro.medium.com/v2/resize:fit:618/format:webp/1*GVEa5SDSr55NE1GXYOX-sg.png"/></div></figure><p id="0dc3" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我更详细地描述这个方程:我们通过写出τ∞p(⋅|π).来取每个可能的轨迹正如我上面所写的，每个轨迹都有发生的概率，每个轨迹都有回报。通过将概率乘以回报(用R(τ)来描述)，我们根据可能性对结果进行加权。这遵循了<a class="ae le" href="https://en.wikipedia.org/wiki/Expected_value#Finite_case" rel="noopener ugc nofollow" target="_blank">计算期望值</a>的正常方式:</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi nf"><img src="../Images/059ea6940575eb4a17bade3f7a081f20.png" data-original-src="https://miro.medium.com/v2/resize:fit:1046/format:webp/1*m1LEnEBSFU_9KfU1kGGlLA.png"/></div></figure></div><div class="ab cl mt mu hx mv" role="separator"><span class="mw bw bk mx my mz"/><span class="mw bw bk mx my mz"/><span class="mw bw bk mx my"/></div><div class="im in io ip iq"><p id="5a04" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">对于我们的例子，我们有这两个轨迹:(低，跳，高，跳，高)(奖励为2)，以及轨迹(低，跳，高，等待，低)(奖励为0: +1表示前进，-1表示等待并被敌人击中，从而下降一级)。</p><p id="2388" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在让我们计算第一个轨迹的概率:它是1×0.8×0.5×0.8×0.5 = 0.16；1是开始状态低的概率，0.8 × 0.5是前进到下一个级别的概率，乘以选择动作跳转的概率。类似地，我们继续第二个轨迹:1 × 0.8 × 0.5 × 0.5 × 0.5 = 0.1，唯一的区别是0.5 × 0.5。这里，我们取等待的概率为0.5(也就是在这种情况下下降一级)，并将其乘以成功率，0.5。</p><p id="d915" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">有了每条轨迹的回报和概率，我们现在可以按照上面的等式计算预期回报:0.16 × 2 + 0.1 × 0=0.32。因此我们的预期收益是0.32。</p></div><div class="ab cl mt mu hx mv" role="separator"><span class="mw bw bk mx my mz"/><span class="mw bw bk mx my mz"/><span class="mw bw bk mx my"/></div><div class="im in io ip iq"><p id="8ef3" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这样，我们可以进一步形式化我们的RL定义:找到最大化期望回报的策略[1，2]</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi ng"><img src="../Images/331162c342f55eaa2b5b3572b01e7ee3.png" data-original-src="https://miro.medium.com/v2/resize:fit:516/format:webp/1*BYlLGw14oZ26dGA9RO8Bjw.png"/></div></figure><p id="f53f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这是最佳策略。</p><h1 id="888d" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">价值函数</h1><p id="ddcb" class="pw-post-body-paragraph ki kj it kk b kl mn ju kn ko mo jx kq kr mp kt ku kv mq kx ky kz mr lb lc ld im bi translated">说了这么多，我们怎么知道一个动作是好的动作，或者一个状态是好的状态呢？我们可以通过价值函数和行动价值函数找到答案。简而言之，给定一个策略π，一个状态的值就是从这个状态得出的预期回报，并且此后总是按照π行事。使用数学符号，我们把它写成</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi nh"><img src="../Images/464f90d3f3f51192c7223539f89ed7f9.png" data-original-src="https://miro.medium.com/v2/resize:fit:798/format:webp/1*32KUk_dQi9lXZYRSpJtkbg.png"/></div></figure><p id="d96e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">类似地，对于行动价值函数，我们也需要策略π。与之前不同，我们现在将值分配给一对状态和动作[1，2]。同样，这个价值也是我们在s₀开始与a₀:合作时的预期回报</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/1a65eb867139db85e6083b2981e27af3.png" data-original-src="https://miro.medium.com/v2/resize:fit:956/format:webp/1*JwzfHTvjS24DOx0lNo8qOw.png"/></div></figure><p id="45e9" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">有了<em class="ms"> Q </em>，我们就可以简单地选择一个状态的下一个动作，作为最大化期望回报的动作</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi nj"><img src="../Images/ab54edc724fc771b865cbdf347bf01e1.png" data-original-src="https://miro.medium.com/v2/resize:fit:598/format:webp/1*dsp6JcXsibWRbiIHwFWG9Q.png"/></div></figure><p id="62ea" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">同样，我们可以通过查询<em class="ms"> V </em>来检查一个状态的值。</p><p id="6385" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">酷的是，有了最优政策，我们可以找到最优价值和最优行动价值函数。这个最优策略对于任何MDP都是存在的。挑战在于找到它。</p></div><div class="ab cl mt mu hx mv" role="separator"><span class="mw bw bk mx my mz"/><span class="mw bw bk mx my mz"/><span class="mw bw bk mx my"/></div><div class="im in io ip iq"><h1 id="94bb" class="lv lw it bd lx ly nk ma mb mc nl me mf jz nm ka mh kc nn kd mj kf no kg ml mm bi translated">参考</h1><p id="8442" class="pw-post-body-paragraph ki kj it kk b kl mn ju kn ko mo jx kq kr mp kt ku kv mq kx ky kz mr lb lc ld im bi translated">[1]乔希·阿奇姆，<a class="ae le" href="https://spinningup.openai.com/en/latest/spinningup/rl_intro.html" rel="noopener ugc nofollow" target="_blank">在《极深RL》</a>(2020)，OpenAI</p><p id="6e21" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">[2]萨顿，理查德S和巴尔托，安德鲁G，<a class="ae le" href="https://web.stanford.edu/class/psych209/Readings/SuttonBartoIPRLBook2ndEd.pdf" rel="noopener ugc nofollow" target="_blank">《强化学习:导论》</a> (2018)，麻省理工学院出版社</p><p id="dae4" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">[3]迈克尔·l·利特曼，<a class="ae le" href="https://www2.cs.duke.edu/courses/spring07/cps296.3/littman94markov.pdf" rel="noopener ugc nofollow" target="_blank">马尔可夫博弈作为多主体强化学习的框架</a> (1994)，机器学习论文集</p></div></div>    
</body>
</html>