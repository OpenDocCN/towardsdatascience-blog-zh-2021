<html>
<head>
<title>K-Nearest Neighbors: Theory and Practice</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">k近邻:理论与实践</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/k-nearest-neighbors-theory-and-practice-7f6f6ee48e56?source=collection_archive---------20-----------------------#2021-07-28">https://towardsdatascience.com/k-nearest-neighbors-theory-and-practice-7f6f6ee48e56?source=collection_archive---------20-----------------------#2021-07-28</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/f174ebaebcb7f97d3ea4ea8008ff0d5d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*XzhxnYfl1HL-jT5R"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">照片由艾米丽-乔·苏克利夫在<a class="ae jg" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><div class=""/><div class=""><h2 id="07d3" class="pw-subtitle-paragraph kg ji jj bd b kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx dk translated">了解如何使用KNN，这是最直观的分类和回归算法之一</h2></div><h1 id="ca46" class="ky kz jj bd la lb lc ld le lf lg lh li kp lj kq lk ks ll kt lm kv ln kw lo lp bi translated">介绍</h1><p id="6aab" class="pw-post-body-paragraph lq lr jj ls b lt lu kk lv lw lx kn ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">k-最近邻，也被称为KNN，可能是最直观的算法之一，它对<strong class="ls jk">分类</strong>和<strong class="ls jk">回归</strong>任务都有效。因为它很容易理解，所以它是比较其他算法的一个很好的基线，特别是在<strong class="ls jk">可解释性</strong>变得越来越重要的今天。</p><h1 id="1f94" class="ky kz jj bd la lb lc ld le lf lg lh li kp lj kq lk ks ll kt lm kv ln kw lo lp bi translated">直觉</h1><p id="6db8" class="pw-post-body-paragraph lq lr jj ls b lt lu kk lv lw lx kn ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">比如说你想卖掉你的房子，需要估算一下价格。你可以看看你最近的9个邻居最近买房子的价格，然后计算平均值来估计你自己的房价。您刚刚使用了一个用于<strong class="ls jk">回归</strong>的KNN算法。是的，就这么简单。</p><p id="97e5" class="pw-post-body-paragraph lq lr jj ls b lt mm kk lv lw mn kn ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">在<strong class="ls jk">分类</strong>任务中，假设您将KNN应用于著名的泰坦尼克号数据集，目标是预测泰坦尼克号乘客是否幸存。你可以找到4个与你试图预测的乘客最相似的乘客，然后看看大多数人发生了什么:如果幸存的人比死亡的人多，你就把这个乘客归类为幸存者。</p><p id="39c0" class="pw-post-body-paragraph lq lr jj ls b lt mm kk lv lw mn kn ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">以下是一个示意性示例，帮助您一步一步地想象这是如何发生的:</p><figure class="ms mt mu mv gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi mr"><img src="../Images/ba8a16d82ee880d0fa0017bdfb5cb676.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Gy4K8fvWWSh5l3O0Ura8hw.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">作者图片</p></figure><p id="5561" class="pw-post-body-paragraph lq lr jj ls b lt mm kk lv lw mn kn ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">这些例子引出了两个主要问题:</p><ol class=""><li id="a0f7" class="mw mx jj ls b lt mm lw mn lz my md mz mh na ml nb nc nd ne bi translated">在第一个例子中，很容易定义您的最近邻居，但在第二个例子中，您如何衡量两个乘客有多相似？</li><li id="91f6" class="mw mx jj ls b lt nf lw ng lz nh md ni mh nj ml nb nc nd ne bi translated">为什么第一个示例中有9个最近邻，而第二个示例中有4个？</li></ol><p id="4d95" class="pw-post-body-paragraph lq lr jj ls b lt mm kk lv lw mn kn ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">嗯，第一个问题是关于定义和计算<strong class="ls jk">距离度量</strong>，第二个问题是关于定义“K-Nearest Neighbors”中K 的最佳数量<strong class="ls jk">。现在让我们更详细地讨论这些问题。</strong></p><h1 id="719d" class="ky kz jj bd la lb lc ld le lf lg lh li kp lj kq lk ks ll kt lm kv ln kw lo lp bi translated">距离测量</h1><p id="22cd" class="pw-post-body-paragraph lq lr jj ls b lt lu kk lv lw lx kn ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">当我们谈论距离时，我们倾向于想到<strong class="ls jk">欧几里德距离，</strong>使用空间坐标:</p><figure class="ms mt mu mv gt iv gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/88b55458f216a9e86089d9b4451375f1.png" data-original-src="https://miro.medium.com/v2/resize:fit:812/format:webp/1*1pEN-9mQ3b8AiSo1OMKDUg.png"/></div><p class="jc jd gj gh gi je jf bd b be z dk translated">欧几里德距离-作者图片</p></figure><p id="379d" class="pw-post-body-paragraph lq lr jj ls b lt mm kk lv lw mn kn ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">在上图中，A和B之间的欧氏距离是d。</p><p id="1be7" class="pw-post-body-paragraph lq lr jj ls b lt mm kk lv lw mn kn ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">然而，有多种方法可以计算两个物体有多近。例如，在同一个例子中，<strong class="ls jk">曼哈顿距离</strong>将是<em class="nl"> D = d1 + d2 </em>(想想如果这些是街道，你会走多远)。</p><p id="c2c3" class="pw-post-body-paragraph lq lr jj ls b lt mm kk lv lw mn kn ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">虽然在这个例子中我们只有二维，但当我们处理数据时，我们通常有更多的维度。然而，所有这些距离度量都适用于多个维度。</p><p id="fa58" class="pw-post-body-paragraph lq lr jj ls b lt mm kk lv lw mn kn ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">然而，有一个警告:当你使用KNN时，你绝对需要在计算这些距离之前通过标准化来<strong class="ls jk">缩放你的数据</strong>。否则，你会用完全不同的尺度来测量距离。在我们的泰坦尼克号的例子中，你将使用不同的变量来测量两个乘客之间的距离，例如他们的年龄和他们支付的船票价格，这显然不是同一个尺度。这将赋予倾向于具有更大值的变量更大的权重。</p><p id="ba5e" class="pw-post-body-paragraph lq lr jj ls b lt mm kk lv lw mn kn ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">一旦您缩放了数据并选择了距离度量，您就可以计算所有观测值之间的距离(显然您不需要自己计算，KNN实现已经为您完成了)。</p><h1 id="ee41" class="ky kz jj bd la lb lc ld le lf lg lh li kp lj kq lk ks ll kt lm kv ln kw lo lp bi translated">“K-最近邻”中K的最佳个数</h1><p id="6982" class="pw-post-body-paragraph lq lr jj ls b lt lu kk lv lw lx kn ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">好了，一旦您知道如何测量两个观测值的接近程度，您如何决定您将使用多少个邻居来进行估计呢？简单:通过选择在验证集上产生最佳误差度量的值，就像对任何其他超参数一样。</p><p id="9ec2" class="pw-post-body-paragraph lq lr jj ls b lt mm kk lv lw mn kn ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">首先选择一个误差度量作为决策标准(例如，RMSE回归或AUC分类)，然后使用网格搜索或您选择的其他方法测试K的多个值，看哪个效果最好。</p><p id="0060" class="pw-post-body-paragraph lq lr jj ls b lt mm kk lv lw mn kn ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">但是，如果你想了解这个选择将如何影响你的算法，请思考一下它如何应用于我们的房价示例:K越小，你就越忽略远处的邻居，而只关注隔壁的邻居。这应该是好事吧？好吧，但在这种情况下，你也更容易受到你的模型没有考虑到的其他特征的差异的影响:如果你的隔壁邻居有一个更大的房子和一个游泳池怎么办？或者，如果他只是幸运地以一个不为人知的原因以便宜得多的价格买下了他的房子呢？嗯，比你的估计不会很好。通过考虑许多邻居，你最终会消除这种噪音(但也可能最终会考虑离你家太远的房子)。所以这里有一个权衡，可以通过使用<a class="ae jg" href="https://en.wikipedia.org/wiki/Cross-validation_(statistics)" rel="noopener ugc nofollow" target="_blank">交叉验证</a>来解决。</p><h1 id="7336" class="ky kz jj bd la lb lc ld le lf lg lh li kp lj kq lk ks ll kt lm kv ln kw lo lp bi translated">如何在实践中使用KNN</h1><p id="db1c" class="pw-post-body-paragraph lq lr jj ls b lt lu kk lv lw lx kn ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">让我们使用Python做一个简单的KNN分类示例，并尝试估计Airbnb住宿的价格是高于还是低于中位数。</p><p id="39b4" class="pw-post-body-paragraph lq lr jj ls b lt mm kk lv lw mn kn ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">这是一个<strong class="ls jk">分类</strong>任务，因为我们不试图估计价格本身，而是价格是高于还是低于中位数(一个<strong class="ls jk">二元结果</strong>)。</p><p id="5f4f" class="pw-post-body-paragraph lq lr jj ls b lt mm kk lv lw mn kn ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">作为独立变量，我们有像<strong class="ls jk">位置</strong>(地理坐标)<strong class="ls jk">邻居</strong>和<strong class="ls jk">评论数量</strong>这样的东西。</p><h2 id="1c7a" class="nm kz jj bd la nn no dn le np nq dp li lz nr ns lk md nt nu lm mh nv nw lo nx bi translated">密码</h2><pre class="ms mt mu mv gt ny nz oa ob aw oc bi"><span id="989f" class="nm kz jj nz b gy od oe l of og"><strong class="nz jk">[IN]:</strong><br/>import pandas as pd<br/>import numpy as np<br/>from sklearn.preprocessing import StandardScaler, LabelBinarizer<br/>from sklearn.model_selection import train_test_split<br/>from sklearn.neighbors import KNeighborsClassifier<br/>from sklearn.metrics import auc, roc_curve, confusion_matrix</span></pre><p id="d085" class="pw-post-body-paragraph lq lr jj ls b lt mm kk lv lw mn kn ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">这里，除了我们将用于任何分类任务的经典库，如<em class="nl"> pandas </em>、<em class="nl"> numpy </em>和<em class="nl"> sklearn.metrics </em>，我们还导入了<em class="nl"> StandardScaler </em>，这将使我们能够缩放我们的数据，以及<em class="nl"> KNeighborsClassifier </em>，这是使用KNN进行分类的sklearn模块(对于回归，使用<em class="nl"> KNeighborsRegressor </em>)。</p><pre class="ms mt mu mv gt ny nz oa ob aw oc bi"><span id="3982" class="nm kz jj nz b gy od oe l of og"><strong class="nz jk">[IN]:<br/># Reading data</strong><br/>df = pd.read_csv("C:/Users/Arthur/Downloads/AB_NYC_20192.csv")</span><span id="b0b4" class="nm kz jj nz b gy oh oe l of og"><strong class="nz jk"># Transforming categorical variables into binary</strong><br/>df = pd.concat([<br/>    df,<br/>pd.get_dummies(df[['neighbourhood','neighbourhood_group','room_type']])<br/>], axis=1)</span><span id="e6dd" class="nm kz jj nz b gy oh oe l of og">df = df.drop(['neighbourhood','neighbourhood_group','room_type'], axis=1)</span><span id="5a97" class="nm kz jj nz b gy oh oe l of og"><strong class="nz jk"># Creating our target variable</strong><br/>df['price_cat'] = df['price'] &lt; df['price'].median()</span><span id="274f" class="nm kz jj nz b gy oh oe l of og"><strong class="nz jk"># Train test split</strong><br/>X, y = df.drop('price_cat', axis=1), df.price_cat<br/>X_train, X_test, y_train, y_test = train_test_split(<br/>    X, y, test_size=0.33, random_state=42)</span></pre><p id="01d7" class="pw-post-body-paragraph lq lr jj ls b lt mm kk lv lw mn kn ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">我们首先重新计算我们的数据，然后我们得到一些分类变量，并把它们转换成二进制。然后我们创建我们的目标变量(低于中值价格/不低于中值价格),最后我们进行训练/测试分割。</p><p id="1254" class="pw-post-body-paragraph lq lr jj ls b lt mm kk lv lw mn kn ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">现在让我们尝试第一个模型，使用地理坐标(不按比例缩放)，以便仅根据地理位置找到最近的邻居。</p><pre class="ms mt mu mv gt ny nz oa ob aw oc bi"><span id="d23c" class="nm kz jj nz b gy od oe l of og"><strong class="nz jk">[IN]:</strong><br/><strong class="nz jk"># Fit &amp; predict</strong><br/>neigh = KNeighborsClassifier(n_neighbors=50)<br/>neigh.fit(X_train[['latitude','longitude']], y_train)<br/>y_pred = neigh.predict(X_test[['latitude','longitude']])</span><span id="8f9c" class="nm kz jj nz b gy oh oe l of og"><strong class="nz jk"># Evaluation</strong><br/>fpr, tpr, thresholds = roc_curve(y_test, y_pred, pos_label=True)<br/>auc_score = round(auc(fpr, tpr), 2)<br/>threshold = 0.5<br/>print("AUC: {}".format(auc_score))</span><span id="0ea1" class="nm kz jj nz b gy oh oe l of og"><strong class="nz jk">[OUT]:<br/></strong>AUC: 0.72</span></pre><p id="c222" class="pw-post-body-paragraph lq lr jj ls b lt mm kk lv lw mn kn ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">正如您所看到的，我们使用50作为邻居的数量:这是一个任意的选择，稍后我们将回到这一点。</p><p id="0322" class="pw-post-body-paragraph lq lr jj ls b lt mm kk lv lw mn kn ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">如果你不知道，AUC是分类任务的质量指标，越接近1越好。0.72的AUC是可以的，但不是很好。让我们通过向我们的模型添加更多的变量来尝试改进它，这一次，缩放它们。</p><pre class="ms mt mu mv gt ny nz oa ob aw oc bi"><span id="dd79" class="nm kz jj nz b gy od oe l of og"><strong class="nz jk">[IN]:</strong><br/>categorical_variables = [<br/>    col for col in df if col.startswith('neighbourhood') or col.startswith('room_type')<br/>]<br/>variables = [<br/>    'latitude', 'longitude',<br/>    'number_of_reviews', 'reviews_per_month'<br/>]</span><span id="c1b7" class="nm kz jj nz b gy oh oe l of og">variables.extend(categorical_variables)<br/>scaler = StandardScaler().fit(X_train[variables])<br/>X_train_norm = np.nan_to_num(scaler.transform(X_train[variables]))<br/>X_test_norm = np.nan_to_num(scaler.transform(X_test[variables]))</span><span id="7d90" class="nm kz jj nz b gy oh oe l of og"><strong class="nz jk"># Fit &amp; predict</strong><br/>neigh = KNeighborsClassifier(n_neighbors=50)<br/>neigh.fit(X_train_norm, y_train)<br/>y_pred = neigh.predict(X_test_norm)</span><span id="2341" class="nm kz jj nz b gy oh oe l of og"><strong class="nz jk"># Evaluation</strong><br/>fpr, tpr, thresholds = roc_curve(y_test, y_pred, pos_label=True)<br/>auc_score = round(auc(fpr, tpr), 2)<br/>print("AUC: {}".format(auc_score))</span><span id="8857" class="nm kz jj nz b gy oh oe l of og"><strong class="nz jk">[OUT]:</strong><br/>AUC: 0.82</span></pre><p id="7ede" class="pw-post-body-paragraph lq lr jj ls b lt mm kk lv lw mn kn ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">如你所见，我们的AUC显著提高，基本上是通过添加那些新功能，这些新功能明显添加了许多关于合并的相关信息。</p><p id="3df8" class="pw-post-body-paragraph lq lr jj ls b lt mm kk lv lw mn kn ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">然而，我们没有改变KNN分类器中的任何参数。</p><p id="e2f9" class="pw-post-body-paragraph lq lr jj ls b lt mm kk lv lw mn kn ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">让我们来看看对于不同的距离度量和不同的K值，分数会如何表现:</p><pre class="ms mt mu mv gt ny nz oa ob aw oc bi"><span id="2860" class="nm kz jj nz b gy od oe l of og"><strong class="nz jk">[IN]:</strong><br/>for dist in ['manhattan', 'euclidean']:<br/>    print("Distance metric: {}".format(dist))<br/>    for k in [2, 4, 10, 50, 100, 500]:<br/>        # Fit &amp; predict<br/>        neigh = KNeighborsClassifier(n_neighbors=k, metric=dist)<br/>        neigh.fit(X_train_norm, y_train)<br/>        y_pred = neigh.predict(X_test_norm)</span><span id="456b" class="nm kz jj nz b gy oh oe l of og">        # Evaluation<br/>        fpr, tpr, thresholds = roc_curve(y_test, y_pred,        pos_label=True)<br/>        auc_score = round(auc(fpr, tpr), 2)<br/>        print("For K = {}, AUC: {}".format(k, auc_score))</span><span id="092d" class="nm kz jj nz b gy oh oe l of og"><strong class="nz jk">[OUT]:<br/></strong>Distance metric: manhattan<br/>For K = 2, AUC: 0.77<br/>For K = 4, AUC: 0.8<br/>For K = 10, AUC: 0.83<br/>For K = 50, AUC: 0.82<br/>For K = 100, AUC: 0.82<br/>For K = 500, AUC: 0.81</span><span id="2de7" class="nm kz jj nz b gy oh oe l of og">Distance metric: euclidean<br/>For K = 2, AUC: 0.77<br/>For K = 4, AUC: 0.8<br/>For K = 10, AUC: 0.83<br/>For K = 50, AUC: 0.82<br/>For K = 100, AUC: 0.82<br/>For K = 500, AUC: 0.81</span></pre><p id="2ae9" class="pw-post-body-paragraph lq lr jj ls b lt mm kk lv lw mn kn ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">首先，我们可以看到距离度量对我们的度量没有影响。</p><p id="9bb5" class="pw-post-body-paragraph lq lr jj ls b lt mm kk lv lw mn kn ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">我们还可以看到，尽管K似乎对我们用例的AUC没有很大的影响，但它的最佳值在10左右。</p><p id="0ce8" class="pw-post-body-paragraph lq lr jj ls b lt mm kk lv lw mn kn ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">根据上面的结果，下一步可能是尝试10左右的其他值，如8或12，看看我们是否可以微调得更多一点，但这不太可能产生更好的结果。进行这种分析的另一种方法是在折线图上绘制AUC与K的关系，以观察AUC在多个K值下的表现。</p><h2 id="25fd" class="nm kz jj bd la nn no dn le np nq dp li lz nr ns lk md nt nu lm mh nv nw lo nx bi translated">额外参数</h2><p id="16d4" class="pw-post-body-paragraph lq lr jj ls b lt lu kk lv lw lx kn ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">关于scikit-learn的KNN实现，您应该知道其他一些参数。您应该将它们添加到<em class="nl"> KNeighborsClassifier </em>或<em class="nl"> KNeighborsRegressor </em>中(更多信息，请查看<a class="ae jg" href="https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html" rel="noopener ugc nofollow" target="_blank">文档</a> ) <em class="nl">。</em></p><ul class=""><li id="8699" class="mw mx jj ls b lt mm lw mn lz my md mz mh na ml oi nc nd ne bi translated"><em class="nl">度量</em>:这是选择要使用的<strong class="ls jk">距离度量</strong>的参数。我们试过“欧几里得”和“曼哈顿”。如果你专门处理<strong class="ls jk">地理坐标</strong>和整个<strong class="ls jk">大面积</strong>，“哈弗辛”可能是个不错的选择，因为它会考虑到地球的曲率。在处理一个小区域的时候，这样的城市，大概不会有太大的区别。当处理分类变量时，尝试使用“汉明”距离。</li><li id="59a7" class="mw mx jj ls b lt nf lw ng lz nh md ni mh nj ml oi nc nd ne bi translated"><em class="nl">权重</em>:该参数将决定如何对每个邻居的结果进行加权。将其设置为“距离”实际上会根据邻居到感兴趣的观察值的距离对目标变量进行加权:<strong class="ls jk">邻居越靠近</strong>，<strong class="ls jk">其值对我们的估计的影响</strong>越大。</li></ul><h1 id="e301" class="ky kz jj bd la lb lc ld le lf lg lh li kp lj kq lk ks ll kt lm kv ln kw lo lp bi translated">结论</h1><p id="4904" class="pw-post-body-paragraph lq lr jj ls b lt lu kk lv lw lx kn ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">正如你所注意到的，KNN有一些优势:</p><ul class=""><li id="095a" class="mw mx jj ls b lt mm lw mn lz my md mz mh na ml oi nc nd ne bi translated">这很容易解释，当你必须向高层管理人员“推销”你的模型时，这很好</li><li id="f7e7" class="mw mx jj ls b lt nf lw ng lz nh md ni mh nj ml oi nc nd ne bi translated">这是一个<strong class="ls jk">非参数</strong>算法，这意味着我们不必对我们的数据及其分布做任何假设。</li></ul><p id="b7d9" class="pw-post-body-paragraph lq lr jj ls b lt mm kk lv lw mn kn ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">T21的一些缺点是:</p><ul class=""><li id="7e22" class="mw mx jj ls b lt mm lw mn lz my md mz mh na ml oi nc nd ne bi translated">KNN不能处理<strong class="ls jk">缺失值</strong>的事实(强迫你输入一些估计值)</li><li id="3295" class="mw mx jj ls b lt nf lw ng lz nh md ni mh nj ml oi nc nd ne bi translated">虽然它相对较快，但对于大型数据集，它会变得很快</li></ul><p id="70da" class="pw-post-body-paragraph lq lr jj ls b lt mm kk lv lw mn kn ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">如果你想巩固你的知识，我建议尝试自己实现算法，从零开始(计算距离，寻找最近的邻居等。).这应该不难做到，而且可以成为一种强有力的学习经历。</p></div><div class="ab cl oj ok hx ol" role="separator"><span class="om bw bk on oo op"/><span class="om bw bk on oo op"/><span class="om bw bk on oo"/></div><div class="im in io ip iq"><p id="00e9" class="pw-post-body-paragraph lq lr jj ls b lt mm kk lv lw mn kn ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">如果你喜欢这篇文章，你可能也会喜欢这些:</p><div class="is it gp gr iu oq"><a rel="noopener follow" target="_blank" href="/still-using-the-same-old-hypothesis-tests-for-data-science-67a9913ce9b8"><div class="or ab fo"><div class="os ab ot cl cj ou"><h2 class="bd jk gy z fp ov fr fs ow fu fw ji bi translated">还在用同样的旧假设检验数据科学吗？</h2><div class="ox l"><p class="bd b dl z fp ov fr fs ow fu fw dk translated">towardsdatascience.com</p></div></div><div class="oy l"><div class="oz l pa pb pc oy pd ja oq"/></div></div></a></div><div class="is it gp gr iu oq"><a rel="noopener follow" target="_blank" href="/support-vector-machine-theory-and-practice-13c2cbef1980"><div class="or ab fo"><div class="os ab ot cl cj ou"><h2 class="bd jk gy z fp ov fr fs ow fu fw ji bi translated">支持向量机:理论与实践</h2><div class="pe l"><h3 class="bd b gy z fp ov fr fs ow fu fw dk translated">了解SVM，最强大的最大似然算法之一</h3></div><div class="ox l"><p class="bd b dl z fp ov fr fs ow fu fw dk translated">towardsdatascience.com</p></div></div><div class="oy l"><div class="pf l pa pb pc oy pd ja oq"/></div></div></a></div><blockquote class="pg ph pi"><p id="515b" class="lq lr nl ls b lt mm kk lv lw mn kn ly pj mo mb mc pk mp mf mg pl mq mj mk ml im bi translated">如果你想进一步讨论，请随时在LinkedIn上联系我，这将是我的荣幸(老实说)。</p></blockquote></div></div>    
</body>
</html>