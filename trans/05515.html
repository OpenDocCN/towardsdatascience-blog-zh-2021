<html>
<head>
<title>Application of Feature Selection Techniques in a Regression Problem</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">特征选择技术在回归问题中的应用</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/application-of-feature-selection-techniques-in-a-regression-problem-4278e2efd503?source=collection_archive---------18-----------------------#2021-05-16">https://towardsdatascience.com/application-of-feature-selection-techniques-in-a-regression-problem-4278e2efd503?source=collection_archive---------18-----------------------#2021-05-16</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><figure class="ip iq gp gr ir is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi io"><img src="../Images/519b0f6a47f750a0539e0ea96cbab0d8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GRB6-yRZbUuFHfsHVtbkuA.jpeg"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">维多利亚诺·伊斯基耶多在<a class="ae jd" href="https://unsplash.com/?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><div class=""/><div class=""><h2 id="52ce" class="pw-subtitle-paragraph kd jf jg bd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku dk translated">Python中的要素选择和优化示例</h2></div><p id="462b" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在与您的数据管理员多次通话，通过各种渠道和<a class="ae jd" href="https://www.tracers.com/blog/what-is-data-enrichment/" rel="noopener ugc nofollow" target="_blank">数据丰富</a>平台获取最有用的数据后，您作为一名数据科学家完成了数据收集。现在你有一个庞大的数据集可以浏览。为了优化计算时间、性能和可解释性之间的权衡，您只需要包括与您的机器学习模型最相关的列。此时，功能选择开始发挥作用。</p><h2 id="2b99" class="lr ls jg bd lt lu lv dn lw lx ly dp lz le ma mb mc li md me mf lm mg mh mi mj bi translated">为什么我们需要特征选择？</h2><p id="b4d8" class="pw-post-body-paragraph kv kw jg kx b ky mk kh la lb ml kk ld le mm lg lh li mn lk ll lm mo lo lp lq ij bi translated">主要原因是在将数据提供给模型之前减少数据集中的字段数量。为什么需要这样做？需要降低模型的复杂性，这可能会导致过度拟合。我们的数据应该符合信号，而不是不相关的预测者产生的噪音。复杂性的降低也增加了我们模型的可解释性，这在与其他利益相关者交流结果时是必不可少的。最后，它减少了内存和计算需求，这是针对生产中的模型进行优化的一个方面。</p><h2 id="9a3d" class="lr ls jg bd lt lu lv dn lw lx ly dp lz le ma mb mc li md me mf lm mg mh mi mj bi translated">最常见的特征选择方法</h2><p id="7c44" class="pw-post-body-paragraph kv kw jg kx b ky mk kh la lb ml kk ld le mm lg lh li mn lk ll lm mo lo lp lq ij bi translated"><em class="mp">过滤方法</em>仅包括与目标变量相关性分数高的模型最相关的特征。这是非常简单和计算友好的，因为使用一个相关措施，一个分数计算所有的预测。具有最高分数的特征被过滤以在模型中使用。一个警告是，在线性模型中总是存在共线性的风险。过滤方法只检查与目标变量的相关性，这就是为什么预测值之间的任何相关性都需要在预处理的后期处理。</p><p id="ed99" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><em class="mp">包装器方法</em>在每次迭代中用特性子集训练模型，并检查它们对性能的影响。然后利用不同的子集训练另一个模型。在每一步中，来自先前模型的学习被考虑来挑选产生更好结果的不同子集。迭代继续进行，直到选择出最佳特征。这种方法计算量很大，因为在每次迭代中，都用新的特征子集来训练新的模型。</p><p id="d34b" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><em class="mp">嵌入式方法</em>利用在建模过程中支持特征选择的算法。这种方法减少了预处理时间，因为训练和特征选择是一起完成的。最好的例子可以是Lasso和<a class="ae jd" rel="noopener" target="_blank" href="/the-power-of-ridge-regression-4281852a64d6">岭回归</a>或者决策树。</p><p id="5a78" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">下面我将为<em class="mp">过滤器</em>、<em class="mp">包装器</em>和<em class="mp">嵌入方法</em>应用一些例子。前三个将是皮尔逊相关、单变量特征选择和方差阈值来表示过滤方法，然后是包装方法的递归特征消除，最后是嵌入方法的Lasso。</p><h2 id="d4b5" class="lr ls jg bd lt lu lv dn lw lx ly dp lz le ma mb mc li md me mf lm mg mh mi mj bi translated"><em class="mq">方法1:皮尔逊相关法</em></h2><p id="d76f" class="pw-post-body-paragraph kv kw jg kx b ky mk kh la lb ml kk ld le mm lg lh li mn lk ll lm mo lo lp lq ij bi translated">皮尔逊相关系数是介于-1和+1之间的一种度量。如果系数越来越接近一端，那么相关性就越高。最好的可视化方法是利用<code class="fe mr ms mt mu b">sns.heatmap()</code>。在下图的情况下，单元格越亮，特征之间的相关性越高。</p><figure class="mw mx my mz gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi mv"><img src="../Images/80b1f3293369c6236a5199fb2848fdd9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bnKtonoppyXWgnyOsd9ejg.png"/></div></div></figure><p id="b2dd" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">为了找到模型中最有用的列，可以从相关矩阵中筛选出目标变量。由于我对相关性的大小比对其方向更感兴趣，所以我检查了矩阵的绝对值。</p><pre class="mw mx my mz gt na mu nb nc aw nd bi"><span id="4cf1" class="lr ls jg mu b gy ne nf l ng nh"># creating dataframe including the correlations with SalePrice<br/>df_corr = abs(df_train.corr()).sort_values(by='SalePrice', ascending=False)[['SalePrice']]</span><span id="24bf" class="lr ls jg mu b gy ni nf l ng nh"># threshold of 0.4 is selected to filter the features<br/>df_corr[df_corr['SalePrice']&gt;0.4]</span></pre><p id="9125" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">挑选与目标变量最相关的列很容易，但是我们需要解决任何线性模型中的共线性问题。为了解决这个问题，我建立了一个共线性检测器:</p><pre class="mw mx my mz gt na mu nb nc aw nd bi"><span id="8676" class="lr ls jg mu b gy ne nf l ng nh">def colinear_detector(df, target_col):<br/>  """<br/>  aim: detect any colinearity<br/>  inputs: all the columns<br/>  output: colinear pairs<br/>  """<br/>  # dropping the target columns<br/>  df_x = df.drop(columns=target_col)</span><span id="61d9" class="lr ls jg mu b gy ni nf l ng nh">  # initiating an empty dictionary <br/>  colinear_dict = {}<br/>  <br/>  # sorting the most correlated column for each column<br/>  for column in  df_x.columns.tolist():<br/>    df_corre = abs(df_x.corr()).sort_values(by=column, ascending=False)[[column]]<br/>  <br/>    # if score is higher than 0.8, add them to a new list<br/>    # every predictor's correlation with itself is 1 so drop the first<br/>    colinear_cols = df_corre[df_corre[column]&gt;0.8].index.tolist()[1:]<br/>    # add a list of correlated columns for each column<br/>    colinear_dict[column]= colinear_cols<br/>  <br/>  # return the correlation dictionary<br/>  return colinear_dict</span></pre><p id="dce4" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">输出将是一个字典，其中键是具有共线性的列，值是相应键的所有共线性列的列表。</p><h2 id="a1b3" class="lr ls jg bd lt lu lv dn lw lx ly dp lz le ma mb mc li md me mf lm mg mh mi mj bi translated">方法2:单变量特征选择</h2><p id="4c14" class="pw-post-body-paragraph kv kw jg kx b ky mk kh la lb ml kk ld le mm lg lh li mn lk ll lm mo lo lp lq ij bi translated">单变量特征选择计算特征和目标变量之间的相互作用，并对每个特征分别进行计算。对于每个特征，计算p值，表明该特征是否对目标变量有显著影响。由于我们正在处理一个回归问题，我将利用sklearn的<code class="fe mr ms mt mu b">f_regression</code>。在幕后，它分别训练每个特征的简单线性回归模型来预测目标变量。每个模型的F值解释了该特征预测目标变量变化的程度。在<code class="fe mr ms mt mu b">f_regression</code>的例子中，一个简单的线性回归模型的F值变成了它被训练的特征的F值。该分数随后用于确定该特征相对于其他特征与目标变量的相关性。</p><figure class="mw mx my mz gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nj"><img src="../Images/def392e55227b2d614b9f0cabe764c75.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Prx1ri_he2n4ikd01l9_dw.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">与目标变量高度相关的特征</p></figure><figure class="mw mx my mz gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nj"><img src="../Images/4b9d94a928b05dedfd19d83e9d72fca1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5yQ9MUwTYyWDISoQrjr4hg.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">与目标变量相关性差的特征</p></figure><p id="a16f" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">下面，我挑选了25个得分最高的列来包含在我的模型中。</p><pre class="mw mx my mz gt na mu nb nc aw nd bi"><span id="6d77" class="lr ls jg mu b gy ne nf l ng nh">from sklearn.feature_selection import f_regression<br/># assigning f and p values to 2 separate arrays<br/>f_val, p_val = f_regression(X_train,y_train)<br/><br/># creating a dictionary from the arrays<br/>feature_dict={'features':X_train.columns.tolist(),<br/>              'f_score':f_val.tolist()}<br/><br/># creating a sorted dataframe from the dictionary<br/>feature_df = pd.DataFrame(feature_dict).sort_values(by='f_scores', ascending=False).reset_index(drop=True)<br/><br/># printing 25 features with the highest scores<br/>feature_df.iloc[:25,:]['columns'].tolist()</span></pre><h2 id="d130" class="lr ls jg bd lt lu lv dn lw lx ly dp lz le ma mb mc li md me mf lm mg mh mi mj bi translated">方法3:方差阈值</h2><p id="488e" class="pw-post-body-paragraph kv kw jg kx b ky mk kh la lb ml kk ld le mm lg lh li mn lk ll lm mo lo lp lq ij bi translated">在训练数据集中，仅采用一个值的要素对模型的整体性能没有贡献。不管有没有，性能都是一样的，因为该特性的方差为零。换句话说，如果方差很低，该特征就不能提供关于目标变量的有意义的见解。这就是为什么它不会增加模型的预测能力。由于一个特征的方差很好地表明了它的预测能力，<code class="fe mr ms mt mu b">VarianceThreshold</code>函数消除了低于给定方差的所有特征。</p><p id="6a9b" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">下面是一个箱线图，显示了6个不同特征的中心趋势。与前五个特征相比，最后一个特征的方差为零，因为99%的值都是0。换句话说，它对目标变量几乎没有预测能力，因为它的值从不改变。这就是为什么将它添加到我们的模型中不会提高性能。</p><figure class="mw mx my mz gt is gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/888ab2aa007540b30f621baa04af7bb3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1198/format:webp/1*FoNBEKu12HTNi8Hm9cowiA.png"/></div></figure><p id="0ccc" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">下面我更喜欢使用阈值水平0.25，因为默认值不会删除我的训练数据中的任何列。请注意，一旦数据被标准化，这个过程是有用的，这样所有的差异都在一个相似的范围内。</p><pre class="mw mx my mz gt na mu nb nc aw nd bi"><span id="4328" class="lr ls jg mu b gy ne nf l ng nh">from sklearn.feature_selection import VarianceThreshold</span><span id="5a3e" class="lr ls jg mu b gy ni nf l ng nh">var_selector=VarianceThreshold(threshold=0.25)<br/>df_var = var_selector.fit_transform(X_train)</span></pre><h2 id="1ccf" class="lr ls jg bd lt lu lv dn lw lx ly dp lz le ma mb mc li md me mf lm mg mh mi mj bi translated">方法4:递归特征消除</h2><p id="252d" class="pw-post-body-paragraph kv kw jg kx b ky mk kh la lb ml kk ld le mm lg lh li mn lk ll lm mo lo lp lq ij bi translated">递归特征消除方法试图找到包含在模型中的最优特征子集。在这种方法中，形成特征的子集，并且在每次迭代中，丢弃表现最差的特征。因此，在达到最佳子集之前，子集在每一步中都变得越来越少。</p><p id="1471" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">为了利用递归特征消除，我从sklearn中选择了<code class="fe mr ms mt mu b">RFECV</code>函数。在RFE函数中启动并加载一个模型，其中<code class="fe mr ms mt mu b">step=5</code>意味着每一步将消除5个特征，而<code class="fe mr ms mt mu b">cv=3</code>用于3重交叉验证。</p><pre class="mw mx my mz gt na mu nb nc aw nd bi"><span id="dd38" class="lr ls jg mu b gy ne nf l ng nh">from sklearn import linear_model<br/>from sklearn.feature_selection import RFECV<br/><br/># initiating a linear model<br/>model = linear_model.LinearRegression()<br/><br/># feeding the model to RFECV<br/>rfe = RFECV(estimator=model, step=5, cv=3)<br/><br/># fitting to the trained rfe model<br/>rfe_fit = rfe.fit(X_train,y_train)<br/><br/># zipping the column names and their rankings according to RFECV<br/>rfe_dict = dict(zip(np.array(X_train.columns),rfe_fit.ranking_))<br/><br/># picking the features ranked 1st in the above dictionary<br/>selected = {}<br/>for key, value in rfe_dict.items():<br/>  if value==1:<br/>    selected[key]=value<br/>print(list(selected.keys()))</span></pre><h2 id="c853" class="lr ls jg bd lt lu lv dn lw lx ly dp lz le ma mb mc li md me mf lm mg mh mi mj bi translated">方法5:套索回归</h2><p id="7b0a" class="pw-post-body-paragraph kv kw jg kx b ky mk kh la lb ml kk ld le mm lg lh li mn lk ll lm mo lo lp lq ij bi translated">虽然Lasso是一种正则化技术，但它也可用于特征选择，因为它强制不相关特征的系数变为零。如果正则化的强度通过增加α而增加，则成本函数优先考虑最小化系数，而不是最小化误差平方和，这将迫使系数更快地达到零。</p><figure class="mw mx my mz gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nl"><img src="../Images/7b697f615996adfa53f69b1e36e3fdec.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GT-PgStdwpq3rmUcOFgzlQ.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">套索回归的目标函数</p></figure><p id="b3cd" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在应用正则化之前对数据进行归一化是非常重要的，因为否则要素及其系数将不会以类似的方式受到正则化项的影响。下面，我在建模前使用了<code class="fe mr ms mt mu b">StandardScaler</code>。</p><pre class="mw mx my mz gt na mu nb nc aw nd bi"><span id="4fcd" class="lr ls jg mu b gy ne nf l ng nh">from sklearn.linear_model import Lasso<br/>from sklearn.preprocessing import StandardScaler<br/>from sklearn.feature_selection import SelectFromModel<br/>from sklearn.pipeline import Pipeline<br/>from sklearn.model_selection import GridSearchCV</span><span id="5e52" class="lr ls jg mu b gy ni nf l ng nh"># creating a pipeline to scale and model<br/>pipeline = Pipeline([('sc',StandardScaler()),('model',Lasso())])</span><span id="c207" class="lr ls jg mu b gy ni nf l ng nh"># gridsearch over the pipeline to find the best alpha for lasso<br/># scoring is picked as mean squared error<br/>lassocv = GridSearchCV(pipeline,<br/>                      {'model__alpha':np.arange(0.1,10,0.2)},<br/>                      cv = 5, scoring="neg_root_mean_squared_error"<br/>                      )</span><span id="9a68" class="lr ls jg mu b gy ni nf l ng nh"># fitting to the model<br/>lassocv.fit(X_train,y_train)<br/><br/># dataframe of variables, coefficients and absolute coefficients <br/>coefs = pd.DataFrame({'variable':X.columns,                            'coef':lassocv.best_estimator_.named_steps['model'].coef_,                            'abs_coef':np.abs(lassocv.best_estimator_.named_steps['model'].coef_)})<br/><br/># sorting based on the absolute of the coefficients<br/>coefs.sort_values('abs_coef', inplace=True, ascending=False)</span></pre><p id="e189" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx jh">结论</strong></p><p id="0f87" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">以上，我试图总结最常见的特征选择技术；<em class="mp">过滤器</em>、<em class="mp">包装器</em>和<em class="mp">嵌入方法</em>及其在一个回归问题中的应用。重要的是要记住，上述方法中的一种并不比另一种优越，因为每个应用程序的用例及优先级都不同。关键的一点是，在大多数情况下，我们不需要引入模型过度拟合的复杂程度。我们需要选择对目标变量具有最高预测能力的特征。</p><p id="b94f" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">感谢阅读到最后，你可以在这里找到我的笔记本<a class="ae jd" href="https://github.com/demirkeseny/feature_selection_in_regression/blob/main/Feature_Selection_Techniques_in_a_Regression_Problem.ipynb" rel="noopener ugc nofollow" target="_blank">。如有任何问题或意见，请联系</a><a class="ae jd" href="https://www.linkedin.com/in/yalimdemirkesen/" rel="noopener ugc nofollow" target="_blank">。</a></p></div><div class="ab cl nm nn hu no" role="separator"><span class="np bw bk nq nr ns"/><span class="np bw bk nq nr ns"/><span class="np bw bk nq nr"/></div><div class="ij ik il im in"><p id="6dc9" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">[1] Tracers，<a class="ae jd" href="https://www.tracers.com/blog/what-is-data-enrichment/" rel="noopener ugc nofollow" target="_blank">什么是数据浓缩？</a>，追踪者博客</p><p id="6f13" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">[2] Sklearn文档，<a class="ae jd" href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFECV.html#sklearn-feature-selection-rfecv" rel="noopener ugc nofollow" target="_blank"> sklearn.feature_selection。RFECV </a></p><p id="053b" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">[3] J. Brownlee，<a class="ae jd" href="https://machinelearningmastery.com/rfe-feature-selection-in-python/" rel="noopener ugc nofollow" target="_blank">递归特征消除(RFE)用于Python中的特征选择</a>，机器学习掌握</p></div></div>    
</body>
</html>