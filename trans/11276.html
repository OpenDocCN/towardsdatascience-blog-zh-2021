<html>
<head>
<title>A DL solution for Tab-Text Data</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">制表文本数据的DL解决方案</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/a-dl-solution-for-tab-text-data-f92e2b68eb16?source=collection_archive---------27-----------------------#2021-11-04">https://towardsdatascience.com/a-dl-solution-for-tab-text-data-f92e2b68eb16?source=collection_archive---------27-----------------------#2021-11-04</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="ce73" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">它能打败XGBoost吗？</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/60a2cfaf421fb3e58cc21ee97f567b27.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*O6425escxb0ZTzda"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图像<a class="ae kv" href="https://images.unsplash.com/photo-1495592822108-9e6261896da8?ixid=MnwxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8&amp;ixlib=rb-1.2.1&amp;auto=format&amp;fit=crop&amp;w=1740&amp;q=80" rel="noopener ugc nofollow" target="_blank">去飞溅</a></p></figure><h2 id="f717" class="kw kx iq bd ky kz la dn lb lc ld dp le lf lg lh li lj lk ll lm ln lo lp lq lr bi translated">动机</h2><p id="d55a" class="pw-post-body-paragraph ls lt iq lu b lv lw jr lx ly lz ju ma lf mb mc md lj me mf mg ln mh mi mj mk ij bi translated">DL模型在视觉和自然语言处理等领域取得了巨大的成就。然而，当涉及到表格数据时，它似乎显示出较低的成功率，并且在大多数应用程序中，它的性能比<a class="ae kv" href="https://machinelearningmastery.com/gentle-introduction-xgboost-applied-machine-learning/" rel="noopener ugc nofollow" target="_blank"> XGBoost </a>差。</p><p id="e477" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated">有很多原因可以解释这种现象，例如，对于表格数据没有明显的可排序度量(当我们处理布尔变量或类别时，我们可以优先假定的唯一度量是离散的)。但是，对组合数据的处理很少:既包含表格列又包含文本的数据。在这篇文章中，我将介绍这种数据库的DL解决方案。我们的目标变量将是一个多分类变量。</p><p id="a919" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated">为了展示引擎，我将使用<a class="ae kv" href="https://scikit-learn.org/stable/" rel="noopener ugc nofollow" target="_blank"> scikit-learn </a>的数据集服务创建一个模拟数据库。</p><pre class="kg kh ki kj gt mq mr ms mt aw mu bi"><span id="516a" class="kw kx iq mr b gy mv mw l mx my"><strong class="mr ir">def </strong>get_mock_db(my_pickle_name):<br/>     covert=fetch_covtype(data_home=<strong class="mr ir">None</strong>, download_if_missing=<strong class="mr ir">True</strong>,<br/>                            random_state=<strong class="mr ir">None</strong>, shuffle=<strong class="mr ir">False</strong>)<br/>     categories=[<strong class="mr ir">'alt.atheism'</strong>, <strong class="mr ir">'soc.religion.christian'</strong>,            <br/>                       <strong class="mr ir">'comp.graphics'</strong>, <strong class="mr ir">'sci.med'</strong>]<br/>     twenty_train = fetch_20newsgroups(subset=<strong class="mr ir">'train'</strong>,remove= <br/>           (<strong class="mr ir">'headers'</strong>, <strong class="mr ir">'footers'</strong>, <strong class="mr ir">'quotes'</strong>), categories=categories,          <br/>                                shuffle=<strong class="mr ir">True</strong>, random_state=42)</span><span id="5572" class="kw kx iq mr b gy mz mw l mx my">clean_text=[(i.replace(<strong class="mr ir">'\n'</strong>,<strong class="mr ir">''</strong>).replace(<strong class="mr ir">'\t'</strong>,<strong class="mr ir">''</strong>).replace('\\',<br/>     ''),j) for i,j in <strong class="mr ir">z</strong>ip(twenty_train.data,twenty_train.target)]    <br/>     clean_text =[  i <strong class="mr ir">for </strong>i <strong class="mr ir">in </strong>clean_text <strong class="mr ir">if </strong>len(i[0])&gt;20]<br/>     len_0 =len([i <strong class="mr ir">for </strong>i <strong class="mr ir">in </strong>clean_text <strong class="mr ir">if </strong>i[1]==0])<br/>     len_1 =len([i <strong class="mr ir">for </strong>i <strong class="mr ir">in </strong>clean_text <strong class="mr ir">if </strong>i[1]==1])<br/>     len_2= len([i <strong class="mr ir">for </strong>i <strong class="mr ir">in </strong>clean_text <strong class="mr ir">if </strong>i[1]==2])<br/>     len_3 =len([i <strong class="mr ir">for </strong>i <strong class="mr ir">in </strong>clean_text <strong class="mr ir">if </strong>i[1]==3])<br/> a, b =covert.data.shape<br/> cov_0 = [covert.data[j] <strong class="mr ir">for </strong>j <strong class="mr ir">in </strong>range(a) <strong class="mr ir">if </strong>covert[<strong class="mr ir">'target'</strong>]<br/>     [j] ==4 ][:len_0]</span><span id="5189" class="kw kx iq mr b gy mz mw l mx my">cntr=0<br/><strong class="mr ir">for </strong>j <strong class="mr ir">in </strong>range(len(clean_text)):<br/>    <strong class="mr ir">if </strong>clean_text[j][1]==0:<br/>            a1,b1 =clean_text[j]<br/>            clean_text[j]= (a1, [cntr],b1)<br/>        cntr+=1<br/>    <strong class="mr ir">if </strong>cntr ==len_0:<br/>            <strong class="mr ir">break<br/></strong>cov_1 = [covert.data[j] <strong class="mr ir">for </strong>j <strong class="mr ir">in </strong>range(a) <strong class="mr ir">if </strong>covert[<strong class="mr ir">'target'</strong>]<br/>     [j]==1 ][:len_1]<br/>cntr=0<br/><strong class="mr ir">for </strong>j <strong class="mr ir">in </strong>range(len(clean_text)):<br/>    <strong class="mr ir">if </strong>(len(clean_text[j])==2) <strong class="mr ir">and </strong>clean_text[j][1]==1:<br/>        a1,b1 =clean_text[j]<br/>        clean_text[j]= (a1, cov_1[cntr],b1)<br/>        cntr+=1<br/>    <strong class="mr ir">if </strong>cntr ==len_0:<br/>        <strong class="mr ir">break</strong></span><span id="c3b0" class="kw kx iq mr b gy mz mw l mx my">cov_2=[covert.data[j] <strong class="mr ir">for </strong>j <strong class="mr ir">in </strong>range(a) <strong class="mr ir">if </strong>covert[<strong class="mr ir">'target'</strong>][j]==3 ][:len_2]<br/>cntr=0<br/><strong class="mr ir">for </strong>j <strong class="mr ir">in </strong>range(len(clean_text)):<br/>    <strong class="mr ir">if  </strong>(len(clean_text[j])==2) <strong class="mr ir">and </strong>clean_text[j][1]==2:<br/>        a1,b1 =clean_text[j]<br/>        clean_text[j]= (a1, cov_2[cntr],b1)<br/>        cntr+=1<br/>    <strong class="mr ir">if </strong>cntr ==len_0:<br/>        <strong class="mr ir">break</strong></span><span id="c5c4" class="kw kx iq mr b gy mz mw l mx my">cov_3=[covert.data[j] <strong class="mr ir">for </strong>j <strong class="mr ir">in </strong>range(a) <strong class="mr ir">if </strong>covert[<strong class="mr ir">'target'</strong>][j]==3 ][:len_3]<br/>cntr=0<br/><strong class="mr ir">for </strong>j <strong class="mr ir">in </strong>range(len(clean_text)):<br/>    <strong class="mr ir">if </strong>(len(clean_text[j])==2) <strong class="mr ir">and </strong>clean_text[j][1]==3:<br/>        a1,b1 =clean_text[j]<br/>        clean_text[j]= (a1, cov_3[cntr],b1)<br/>        cntr+=1<br/>    <strong class="mr ir">if </strong>cntr ==len_0:<br/>        <strong class="mr ir">break<br/>with   </strong>open(my_pickle_name, <strong class="mr ir">'wb'</strong>) <strong class="mr ir">as </strong>f:<br/>    pickle.dump(clean_text, f, pickle.HIGHEST_PROTOCOL)<br/>print (<strong class="mr ir">"files_genrated"</strong>)<br/><strong class="mr ir">return</strong></span></pre><p id="13f6" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated">我们可以用多种方式转换表格数据，比如按原样接受整数列，执行确定性聚合，或者将布尔变量转换为一个统一的编码表示。然而，在这篇文章中，我将尝试一种不同的方法:我将为。表格数据。为此，我们将使用<a class="ae kv" href="https://github.com/dreamquark-ai/tabnet" rel="noopener ugc nofollow" target="_blank"> Tabnet </a>。这个<a class="ae kv" href="https://arxiv.org/pdf/1908.07442.pdf" rel="noopener ugc nofollow" target="_blank">包</a>提供了几种嵌入方法。</p><p id="4302" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated">下面的代码执行这样的嵌入</p><pre class="kg kh ki kj gt mq mr ms mt aw mu bi"><span id="02f1" class="kw kx iq mr b gy mv mw l mx my"><strong class="mr ir">from </strong>pytorch_tabnet.pretraining <strong class="mr ir">import </strong>TabNetPretrainer<br/>clf = TabNetPretrainer()   <em class="na"><br/></em>clf.fit(X_train[:n_steps])</span><span id="4905" class="kw kx iq mr b gy mz mw l mx my">#Here we embed the data <br/>embed_tabular = clf.predict(X_test)[1]</span><span id="f172" class="kw kx iq mr b gy mz mw l mx my">raw_data =[(text[j], embed_tabular[j], label[j]i[0],j,i[2]) <strong class="mr ir">for </strong>i,j <strong class="mr ir">in </strong>zip(data, embed_tabular]</span></pre><p id="ec2f" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated">现在，数据已经准备好并保存到pickle文件中。我们可以使用<a class="ae kv" href="https://huggingface.co/" rel="noopener ugc nofollow" target="_blank"> Huggingsface的</a>分词器对文本进行预处理</p><h2 id="54d8" class="kw kx iq bd ky kz la dn lb lc ld dp le lf lg lh li lj lk ll lm ln lo lp lq lr bi translated">标记化</h2><p id="ae73" class="pw-post-body-paragraph ls lt iq lu b lv lw jr lx ly lz ju ma lf mb mc md lj me mf mg ln mh mi mj mk ij bi translated">在这一部分中，我们将文本转换成一种允许Huggingsface引擎执行嵌入的格式。我习惯了标记化器的类型:</p><p id="8dbb" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated"><a class="ae kv" href="https://huggingface.co/bert-base-multilingual-uncased" rel="noopener ugc nofollow" target="_blank">伯特</a>和<a class="ae kv" href="https://huggingface.co/google/fnet-base" rel="noopener ugc nofollow" target="_blank">T3】FnetT5<strong class="lu ir">。</strong>前者因</a><a class="ae kv" href="https://arxiv.org/pdf/1706.03762.pdf" rel="noopener ugc nofollow" target="_blank">变压器</a>的出现而广为人知，后者是一种全新的方法，建议用FFT代替多头层。人们可以在这里了解它。</p><pre class="kg kh ki kj gt mq mr ms mt aw mu bi"><span id="c4e3" class="kw kx iq mr b gy mv mw l mx my"><strong class="mr ir">def </strong>get_tokenizer(data_yaml, bert_tokeinizer_name):<br/>    <strong class="mr ir">if </strong>data_yaml == enum_obj.huggins_embedding.fnet.value:<br/>        <strong class="mr ir">return </strong>FNetTokenizer.from_pretrained(<strong class="mr ir">'google/fnet-base'</strong>)<br/>    tokenizer = AutoTokenizer.from_pretrained(bert_tokeinizer_name,   <br/>             use_fast=<strong class="mr ir">True</strong>)<br/>    <strong class="mr ir">return </strong>tokenizer</span><span id="4925" class="kw kx iq mr b gy mz mw l mx my">batch_encoding = tokenizer.batch_encode_plus([text  <strong class="mr ir">for text </strong> <strong class="mr ir">in </strong>data], **tokenizer_params, return_tensors=<strong class="mr ir">'pt'</strong>)</span></pre><p id="1cc1" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated">根据外部Yaml标志，我们选择我们的记号赋予器。最后一行是用伪代码写的，表示令牌化步骤本身。</p><h2 id="3113" class="kw kx iq bd ky kz la dn lb lc ld dp le lf lg lh li lj lk ll lm ln lo lp lq lr bi translated">创建张量文件夹</h2><p id="65d5" class="pw-post-body-paragraph ls lt iq lu b lv lw jr lx ly lz ju ma lf mb mc md lj me mf mg ln mh mi mj mk ij bi translated">为了训练和评估，我们需要将数据(表格和文本的组合)放入张量文件的文件夹中，这些文件将使用Pytorch的<strong class="lu ir">数据加载器</strong>上传到神经网络。我们给出了生成这个文件夹的几段代码。我们从这两个函数开始:</p><pre class="kg kh ki kj gt mq mr ms mt aw mu bi"><span id="10ea" class="kw kx iq mr b gy mv mw l mx my"><strong class="mr ir"><br/>def </strong>bert_gen_tensor(input_t, tab_tensor, lab_all_Place,     <br/>       file_name, batch_enc, index_0):<br/>    input_m = torch.squeeze(torch.index_select(<br/>         batch_enc[<strong class="mr ir">"attention_mask"</strong>], dim=0,                   <br/>                 index=torch.tensor(index_0))) <br/>    torch.save([input_t, input_m, tab_tensor, lab_all_Place],     <br/>            file_name)<br/>    <strong class="mr ir">return<br/><br/>def </strong>fnet_gen_tensor(input_t, tab_tensor, lab_all_Place,          <br/>           file_name, batch_enc=<strong class="mr ir">None</strong>, index_0=-1):  <br/>     torch.save([input_t, tab_tensor, lab_all_Place], file_name)<br/>     <strong class="mr ir">return</strong></span></pre><p id="e128" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated">人们可以看出它们几乎是相似的。它们反映了<strong class="lu ir"> Bert </strong>和<strong class="lu ir"> Fnet所需要的张量之间的差异，</strong> Bert需要一个键:<strong class="lu ir"> attention_mask" </strong>，而Fnet不存在这个键。因此，一方面，我们需要为每种嵌入方法保存不同的张量集，而另一方面，我们希望有一个唯一的代码。我们如下解决这个问题:</p><pre class="kg kh ki kj gt mq mr ms mt aw mu bi"><span id="5ff9" class="kw kx iq mr b gy mv mw l mx my"><strong class="mr ir">def </strong>generate_data_folder_w_tensor(data_lab_and_docs, data_yaml):<br/> .<br/> .</span><span id="5f1b" class="kw kx iq mr b gy mz mw l mx my"><strong class="mr ir">    if </strong>data_yaml[<strong class="mr ir">'embed_val'</strong>] ==               <br/>                       enum_obj.huggins_embedding.fnet.value:<br/>        proc_func = fnet_gen_tensor<br/>    <strong class="mr ir">else</strong>:<br/>        proc_func = bert_gen_tensor</span></pre><p id="e25e" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated">除了关键问题之外，该文件还包含输入数据、表格数据和标签。我们对这两种方法的需求处理如下:</p><p id="4db1" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated">我们现在准备迭代数据项并创建tensors文件夹</p><pre class="kg kh ki kj gt mq mr ms mt aw mu bi"><span id="b754" class="kw kx iq mr b gy mv mw l mx my"><strong class="mr ir">for </strong>i, data <strong class="mr ir">in </strong>enumerate(data_lab_and_docs):<br/>        file_name = pref_val + <strong class="mr ir">"_" </strong>+ str(i) + <strong class="mr ir">"_" </strong>+ suff_val<br/>        tab_tensor = torch.tensor(data[1], dtype=torch.float32)<br/>        input_t =  <br/>      torch.squeeze(torch.index_select(batch_encoding[<strong class="mr ir">"input_ids"</strong>],dim=0,    index=torch.tensor(i)))<br/><br/>        proc_func(input_t, tab_tensor,  data[2],file_name, <br/>                batch_enc= batch_encoding, 0index_0=i)<br/>        dic_for_pic[file_name]= data[2]<br/><br/>        <strong class="mr ir">with   </strong>open(data_yaml[<strong class="mr ir">'labales_file'</strong>], <strong class="mr ir">'wb'</strong>) <strong class="mr ir">as </strong>f:<br/>           pickle.dump(dic_for_pic, f, pickle.HIGHEST_PROTOCOL)</span></pre><p id="2a6e" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated">这个循环非常简单；我们设置文件名并使用函数来保存信息。我们创建一个字典，将文件名映射到它们的标签值，以备将来需要。</p><p id="a380" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated">当过程结束时，我们有一个tensors文件夹用于训练和评估步骤。我们差不多可以开始训练了。差不多？是啊！因为我们正在使用Pytorch，所以我们需要创建我们的<strong class="lu ir">数据加载器。</strong></p><pre class="kg kh ki kj gt mq mr ms mt aw mu bi"><span id="6cbb" class="kw kx iq mr b gy mv mw l mx my"><strong class="mr ir">import </strong>torch<br/><strong class="mr ir">import </strong>enum_collection <strong class="mr ir">as </strong>enum_obj<br/><strong class="mr ir">import </strong>random<br/><br/><strong class="mr ir">class </strong>dataset_tensor :<br/>    <strong class="mr ir">def </strong>__init__ (self, list_of_files, embed_val ):<br/><br/>        self.list_of_files =list_of_files<br/>        random.shuffle(list_of_files)<br/>        <strong class="mr ir">if </strong>embed_val==enum_obj.huggins_embedding.fnet.value:<br/>            self.ref =[0,1,2]<br/>        <strong class="mr ir">else</strong>:<br/>            self.ref=[1,2,3]<br/><br/>    <strong class="mr ir">def </strong>__getitem__(self, idx):<br/>       aa =torch.load(self.list_of_files[idx])<br/>       <strong class="mr ir">return </strong>aa[0], aa[self.ref[0]], a[self.ref[1]],aa[self.ref[2]]<br/><br/><br/>    <strong class="mr ir">def </strong>__len__(self) :<br/>        <strong class="mr ir">return </strong>len(self.list_of_files)</span></pre><p id="fc25" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated">您可能会注意到，这是Pytorch数据加载器的标准结构。有一件事需要澄清:<strong class="lu ir"> self.ref </strong>数组。由于<strong class="lu ir"> Fnet </strong>和<strong class="lu ir"> Bert </strong>在单个文件中使用不同的张量集合，并且我们希望使用相同的训练程序，我们为<strong class="lu ir"> Fnet </strong>输出一个void变量(输入项两次)。<strong class="lu ir"> self.ref </strong>决定我们输出的文件中张量的索引。</p><h1 id="487a" class="nb kx iq bd ky nc nd ne lb nf ng nh le jw ni jx li jz nj ka lm kc nk kd lq nl bi translated"><strong class="ak">型号</strong></h1><p id="fc9e" class="pw-post-body-paragraph ls lt iq lu b lv lw jr lx ly lz ju ma lf mb mc md lj me mf mg ln mh mi mj mk ij bi translated">这可能是代码中最有趣的部分。该模型执行两个步骤:</p><ul class=""><li id="b11f" class="nm nn iq lu b lv ml ly mm lf no lj np ln nq mk nr ns nt nu bi translated">嵌入按照给定的配方(<strong class="lu ir"> Fnet </strong>或<strong class="lu ir"> Bert </strong>)嵌入标记化的数据</li><li id="857b" class="nm nn iq lu b lv nv ly nw lf nx lj ny ln nz mk nr ns nt nu bi translated">处理嵌入张量，并将其映射为类别数量大小的张量</li></ul><p id="4920" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated">第一步取决于我们选择的嵌入类型。因此，我们要求他们每个人都有一个特殊的“<strong class="lu ir">前锋</strong></p><pre class="kg kh ki kj gt mq mr ms mt aw mu bi"><span id="1a69" class="kw kx iq mr b gy mv mw l mx my"><strong class="mr ir">class </strong>my_fnet(FNetForSequenceClassification):<br/><br/>    <strong class="mr ir">def </strong>__init__(self, config, dim=768):<br/>        super(my_fnet, self).__init__(config )<br/>        self.dim= dim<br/>        self.num_labels = 4<br/><br/>       <br/>        self.distilbert = FNetModel(config)<br/>        self.init_weights()<br/><br/>        self.pre_classifier = nn.Linear(self.dim, self.dim)<br/>    <strong class="mr ir">def </strong>forward(self,  input_ids=<strong class="mr ir">None</strong>):<br/>            outputs = self.distilbert( input_ids=input_ids)<br/>            pooled_output = outputs[1]<br/>            pooled_output = self.dropout(pooled_output)<br/>            <strong class="mr ir">return </strong>pooled_output</span><span id="64df" class="kw kx iq mr b gy mz mw l mx my"><strong class="mr ir">class </strong>my_bert(BertForSequenceClassification):<br/> <em class="na">   </em><strong class="mr ir">def </strong>__init__(self, config, dim=768):<br/>        super(my_bert, self).__init__(config )<br/>        self.dim= dim<br/>        self.num_labels = 4<br/>   <em class="na">     </em>self.distilbert = BertModel(config)<br/>        self.init_weights()<br/><br/>        self.pre_classifier = nn.Linear(self.dim, self.dim)<br/>    <strong class="mr ir">def </strong>forward(self,<br/>                    input_ids=<strong class="mr ir">None</strong>,<br/>                    attention_mask=<strong class="mr ir">None</strong>,<br/>                    head_mask=<strong class="mr ir">None</strong>,<br/>                    inputs_embeds=<strong class="mr ir">None</strong>,<br/>                    output_attentions=<strong class="mr ir">None</strong>,<br/>                    output_hidden_states=<strong class="mr ir">None</strong>,<br/>                    return_dict=<strong class="mr ir">None</strong>,<br/><br/>                    ):<br/>            return_dict = return_dict <br/>          <strong class="mr ir">if </strong>return_dict <strong class="mr ir">is not None else   <br/>                 </strong>self.config.use_return_dict<br/>          <em class="na"><br/>              </em>distilbert_output = self.distilbert(<br/>                input_ids=input_ids,<br/>                attention_mask=attention_mask,<br/>                head_mask=head_mask,<br/>                inputs_embeds=inputs_embeds,<br/>                output_attentions=output_attentions,<br/>                output_hidden_states=output_hidden_states,<br/>                return_dict=return_dict,<br/>            )<br/>            hidden_state = distilbert_output[0]  <em class="na"><br/>            </em>pooled_output = hidden_state[:, 0]  <em class="na"><br/>            # pooled_output = self.pre_classifier(pooled_output)  <br/>            # hidden_state = distilbert_output[0]   <br/>            </em><strong class="mr ir">return </strong>pooled_output</span></pre><p id="3ed4" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated">模型本身如下所示:</p><pre class="kg kh ki kj gt mq mr ms mt aw mu bi"><span id="9b9f" class="kw kx iq mr b gy mv mw l mx my"><strong class="mr ir">class </strong>my_model(nn.Module):<br/>   <em class="na"><br/>    </em><strong class="mr ir">def </strong>__init__(self, data_yaml, my_tab_form=1, dim=768):<br/>        super(my_model, self).__init__( )<br/>        self.forward =self.bert_forward<br/>        <strong class="mr ir">if </strong>data_yaml[<strong class="mr ir">'embed_val'</strong>] ==    <br/>               eum_obj.huggins_embedding.distil_bert.value:<br/>            self.dist_model  = my_dist.from_pretrained(<strong class="mr ir">'distilbert-           <br/>                         base-multilingual-cased'</strong>,num_labels=4 )<br/>        <strong class="mr ir">elif </strong>data_yaml[<strong class="mr ir">'embed_val'</strong>] ==   <br/>          enum_obj.huggins_embedding.base_bert.value:<br/>           <em class="na">           </em>self.dist_model =   <br/>           my_bert.from_pretrained(<strong class="mr ir">'bert-base-multilingual-cased'</strong>,  <br/>                             num_labels=4)<br/>        <strong class="mr ir">else</strong>:<br/>            self.dist_model = my_fnet.from_pretrained(<strong class="mr ir">'google/fnet-     <br/>                       base'</strong>, num_labels=4)<br/>            self.forward = self.fnet_forward<br/>        <strong class="mr ir">if </strong>my_tab_form&gt;0 :<br/>           localtab= data_yaml[<strong class="mr ir">'tab_format'</strong>]<br/>        <strong class="mr ir">else </strong>:<br/>            localtab =my_tab_form<br/>        <strong class="mr ir">if </strong>localtab ==  enum_obj.tab_label.no_tab.value:<br/>            print (<strong class="mr ir">"no_tab"</strong>)<br/>            self.embed_to_fc = self.cat_no_tab<br/>            self.tab_dim = 0<br/>        <strong class="mr ir">else </strong>:<br/>            self.embed_to_fc = self.cat_w_tab<br/>            self.tab_dim =data_yaml[<strong class="mr ir">'tab_dim'</strong>]<br/>       <br/>        self.dim=dim<br/>        self.num_labels =4<br/><br/>        self.pre_classifier = nn.Linear( self.dim, self.dim)<br/>        self.inter_m0= nn.Linear(self.dim +self.tab_dim,216)<br/><br/>        self.inter_m1 = nn.Linear(216,64)<br/>        self.inter_m1a = nn.Linear(64, 32)<br/><br/><br/>        self.inter_m3 = nn.Linear(32, self.num_labels)<br/>        self.classifier = nn.Linear(self.dim, self.num_labels)<br/>        self.dropout = nn.Dropout(0.2)<br/><br/>   <em class="na"><br/><br/>    </em><strong class="mr ir">def </strong>cat_no_tab (self, hidden, x):<br/>        <strong class="mr ir">return </strong>hidden<br/>    <strong class="mr ir">def </strong>cat_w_tab (self, hidden, x):<br/>        <strong class="mr ir">return </strong>torch.cat((hidden, x),dim=1)<br/><br/>    <strong class="mr ir">def </strong>fnet_forward(self, x,<br/>                 input_ids=<strong class="mr ir">None</strong>, attention_mask=<strong class="mr ir">None</strong>):<br/><br/>        hidden_state = self.dist_model(input_ids)<br/><br/>        pooled_output = torch.cat((hidden_state, x), dim=1)<br/>        pooled_output = self.inter_m0(pooled_output)  <em class="na"># (bs, dim)<br/>        </em>pooled_output = nn.ReLU()(pooled_output)  <em class="na"># (bs, dim)<br/>        </em>pooled_output = self.dropout(pooled_output)  <em class="na"># (bs, dim)<br/><br/>        </em>pooled_output = self.inter_m1(pooled_output)  <em class="na"># (bs, dim)<br/>        </em>pooled_output = nn.ReLU()(pooled_output)  <em class="na"># (bs, dim)<br/>        </em>pooled_output = self.dropout(pooled_output)  <em class="na"># (bs, dim)<br/><br/>        </em>pooled_output = self.inter_m1a(pooled_output)   <em class="na"><br/>        </em>pooled_output = nn.ReLU()(pooled_output)  <em class="na"><br/>        </em>pooled_output = self.dropout(pooled_output)    <em class="na"><br/>        </em>logits = self.inter_m3(pooled_output)  <em class="na"><br/>        </em><strong class="mr ir">return </strong>logits<br/>    <br/> <br/>   <strong class="mr ir">def </strong>bert_forward(self, x, input_ids=<strong class="mr ir">None</strong>,                <br/>        attention_mask=<strong class="mr ir">None</strong>) :<br/>        hidden_state =self.dist_model(input_ids, attention_mask)<br/>        pooled_output = self.embed_to_fc(hidden_state, x)<br/>        <br/>        pooled_output = self.inter_m0(pooled_output)  <em class="na"><br/>        </em>pooled_output = nn.ReLU()(pooled_output)  <em class="na"># (bs, dim)<br/>        </em>pooled_output = self.dropout(pooled_output)  <em class="na"># (bs, dim)<br/>        </em>pooled_output = self.inter_m1(pooled_output)  <em class="na"># (bs, dim)<br/>        </em>pooled_output = nn.ReLU()(pooled_output)  <em class="na"># (bs, dim)<br/>        </em>pooled_output = self.dropout(pooled_output)  <em class="na"># (bs, dim)<br/>        </em>pooled_output = self.inter_m1a (pooled_output)  <em class="na"># (bs, dim)<br/>        </em>pooled_output = nn.ReLU()(pooled_output)  <em class="na"># (bs, dim)<br/>        </em>pooled_output = self.dropout(pooled_output)  <em class="na"># (bs, dim)<br/>        </em>logits = self.inter_m3(pooled_output)  <em class="na"># (bs, num_labels)<br/>        </em><strong class="mr ir">return </strong>logits</span></pre><p id="2175" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated">除了我们已经讨论过的不同转发功能之外。我们有<strong class="lu ir"> my_tab_form </strong>标志。这个标志表示我们是单独使用文本还是组合数据(我们通常使用后者)。在代码方面，我们用一个void函数代替<strong class="lu ir"> torch.cat </strong>(它输出输入张量)</p><pre class="kg kh ki kj gt mq mr ms mt aw mu bi"><span id="e666" class="kw kx iq mr b gy mv mw l mx my"><strong class="mr ir">if </strong>my_tab_form&gt;0 :<br/>           localtab= data_yaml[<strong class="mr ir">'tab_format'</strong>]<br/><strong class="mr ir">else </strong>:<br/>            localtab =my_tab_form</span></pre><h2 id="a836" class="kw kx iq bd ky kz la dn lb lc ld dp le lf lg lh li lj lk ll lm ln lo lp lq lr bi translated">损失函数</h2><p id="0e95" class="pw-post-body-paragraph ls lt iq lu b lv lw jr lx ly lz ju ma lf mb mc md lj me mf mg ln mh mi mj mk ij bi translated">我们的损失往往是一个标准的交叉熵。尽管如此，因为对于某些应用程序，我们需要减少与其中一个类别相关联的特定故障，所以我添加了几个特定的函数来“唯一地惩罚”这些错误。人们可以在这里阅读更多<a class="ae kv" href="https://github.com/natank1/Text_Tab-DL/blob/main/loss_and_re_manager.py" rel="noopener ugc nofollow" target="_blank"/></p><h1 id="8772" class="nb kx iq bd ky nc nd ne lb nf ng nh le jw ni jx li jz nj ka lm kc nk kd lq nl bi translated">主循环</h1><p id="bc74" class="pw-post-body-paragraph ls lt iq lu b lv lw jr lx ly lz ju ma lf mb mc md lj me mf mg ln mh mi mj mk ij bi translated">最后，我们可以使用来自外部的整个块来执行训练步骤。我们从带来数据开始:</p><pre class="kg kh ki kj gt mq mr ms mt aw mu bi"><span id="53d3" class="kw kx iq mr b gy mv mw l mx my"><strong class="mr ir">with </strong>open(<strong class="mr ir">"project_yaml.yaml"</strong>, <strong class="mr ir">'r'</strong>) <strong class="mr ir">as </strong>stream:<br/>    data_yaml = yaml.safe_load(stream)<br/>list_of_files =os.listdir(data_yaml[<strong class="mr ir">'tensors_folder'</strong>])<br/>list_of_files =[data_yaml[<strong class="mr ir">'tensors_folder'</strong>]+i <strong class="mr ir">for </strong>i <strong class="mr ir">in </strong>list_of_files]</span><span id="96d8" class="kw kx iq mr b gy mz mw l mx my">X_train,X_test =train_test_split(list_of_files, test_size=0.2)</span><span id="bf6b" class="kw kx iq mr b gy mz mw l mx my"># Creating dataloader!!<br/>train_t =dataset_tensor(X_train, data_yaml[<strong class="mr ir">'embed_val'</strong>])<br/>train_loader = DataLoader(train_t, batch_size=data_yaml[<strong class="mr ir">'batch_size'</strong>], shuffle=<strong class="mr ir">True</strong>)<br/><br/>test_t = dataset_tensor(X_test, data_yaml[<strong class="mr ir">'embed_val'</strong>])<br/>test_loader = DataLoader(test_t, batch_size=data_yaml[<strong class="mr ir">'batch_size'</strong>], shuffle=<strong class="mr ir">True</strong>)</span></pre><p id="8890" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated">我们上传Yaml文件并将数据分割以进行训练和测试(例如，0.2的测试大小不需要深入的理论支持)。对于不太熟悉torch的读者来说，最后几行为训练和测试文件创建了数据加载器。</p><pre class="kg kh ki kj gt mq mr ms mt aw mu bi"><span id="a21e" class="kw kx iq mr b gy mv mw l mx my">device = <strong class="mr ir">""<br/>if </strong>torch.cuda.is_available():<br/>    device = torch.device(<strong class="mr ir">"cuda:0"</strong>)<br/><br/>model = my_model(data_yaml)</span><span id="c6e6" class="kw kx iq mr b gy mz mw l mx my"><strong class="mr ir"><br/>#pre -training usage<br/>if </strong>data_yaml[<strong class="mr ir">'improv_model'</strong>]:<br/>    print(<strong class="mr ir">"Loading mode"</strong>)<br/>    model_place = data_yaml[<strong class="mr ir">'pre_trained_folder'</strong>]<br/>    print (model_place)<br/>    model.load_state_dict(torch.load(model_place, map_location=<strong class="mr ir">'cpu'</strong>))<br/><br/><strong class="mr ir">if </strong>device:<br/>    model =model.to(device)</span></pre><p id="ca30" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated">我们为拥有GPU的幸运读者设置了设备。之后，我们定义模型结构并上传其权重，以防我们希望使用预训练的模型。</p><pre class="kg kh ki kj gt mq mr ms mt aw mu bi"><span id="f35a" class="kw kx iq mr b gy mv mw l mx my">optimizer = torch.optim.AdamW(model.parameters(),<br/>                              lr=1e-5, eps=1e-8)<em class="na"><br/>                              </em><br/>loss_man = loss_manager(data_yaml[<strong class="mr ir">'batch_size'</strong>],   <br/>        data_yaml[<strong class="mr ir">'target_val'</strong>],data_yaml[<strong class="mr ir">'reg_term'</strong>])<br/>model.train()</span></pre><p id="ac16" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated">我们定义了优化器(它应该降低大多数变压器任务的学习率)和损失函数。现在我们可以执行训练迭代</p><pre class="kg kh ki kj gt mq mr ms mt aw mu bi"><span id="c0c4" class="kw kx iq mr b gy mv mw l mx my"><strong class="mr ir">for </strong>i_epoc <strong class="mr ir">in </strong>range(data_yaml[<strong class="mr ir">'n_epochs'</strong>]):<br/>    running_loss = 0.0<br/>    counter_a=0<br/>    <strong class="mr ir">for </strong>batch_idx, data <strong class="mr ir">in </strong>enumerate(train_loader):<br/><br/>        a, b, d, c=  data<br/>        <strong class="mr ir">if </strong>device :<br/>            a=a.to(device)<br/>            b=b.to(device)<br/>            c=c.to(device)<br/>            d=d.to(device)<br/><br/>        ss=model(x=d, input_ids=a, attention_mask=b)<br/><br/><br/>        loss =loss_man.crit(ss, c)<br/>        running_loss += loss.item()<br/><br/>        loss.backward()<br/>        print(loss, batch_idx)<br/>        optimizer.step()<br/>        optimizer.zero_grad()<br/><br/><br/>    print (<strong class="mr ir">"Epoch loss= "</strong>,running_loss/(counter_a+0.))<br/>    print (<strong class="mr ir">"End of epoc"</strong>)<br/>    torch.save(model.state_dict(),  data_yaml[<strong class="mr ir">'models_folder'</strong>] + <strong class="mr ir">"model_epoch_" </strong>+ str(i_epoc) + <strong class="mr ir">".bin"</strong>)</span></pre><p id="a943" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated">我们可以在培训结束时使用<strong class="lu ir"> tqdm </strong>添加一个评估循环:</p><pre class="kg kh ki kj gt mq mr ms mt aw mu bi"><span id="7b19" class="kw kx iq mr b gy mv mw l mx my"><strong class="mr ir">from </strong>tqdm <strong class="mr ir">import  </strong>tqdm<br/>.<br/>.<br/>.<strong class="mr ir"><br/>with </strong>torch.no_grad():<br/>    <strong class="mr ir">with </strong>tqdm(total=len(test_loader), ncols=70) <strong class="mr ir">as </strong>pbar:<br/>        labels = []<br/>        predic_y = []<br/>        <strong class="mr ir">for </strong>batch_idx, data <strong class="mr ir">in </strong>enumerate(test_loader):<br/><br/>            a, b, d, c = data<br/>            <strong class="mr ir">if </strong>device:<br/>                a = a.to(device)<br/>                b = b.to(device)<br/>                c = c.to(device)<br/>                d = d.to(device)<br/>            labels.append(c)<br/>            outp = model(x=d, input_ids=a, attention_mask=b)<br/>            probs = nn.functional.softmax(outp, dim=1)<br/>            predic_y.append(probs)<br/>            pbar.update(1)<br/><br/>        y_true, y_pred = convert_eval_score_and_label_to_np(labels, predic_y)</span></pre><p id="371f" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated">结果和他们的分析留给他们自己感兴趣的读者。</p><h1 id="6506" class="nb kx iq bd ky nc nd ne lb nf ng nh le jw ni jx li jz nj ka lm kc nk kd lq nl bi translated">摘要</h1><p id="808b" class="pw-post-body-paragraph ls lt iq lu b lv lw jr lx ly lz ju ma lf mb mc md lj me mf mg ln mh mi mj mk ij bi translated">我们提出了一个组合数据模型和一个训练它的DL引擎。从架构上讲，我们提出了一个简单的神经网络。但是，对于一些经过训练和测试的任务，它显示出比类似的XGBoost模型更好的结果。这给了DL引擎能够很好地处理表格数据的希望。除了单纯的DL方面之外，我相信组合数据任务被赋予了很好的数学谜题，因为它不仅研究不同的数据类型，还研究它们所导致的度量和拓扑。可排序和不可排序变量以及完全和不完全拓扑之间的接口可以提供广泛的研究领域。在阅读这篇文章时，人们可能会问的另一个问题是，我们是否可以改进用于Fnet的FFT(例如，使用小波)。</p><h2 id="dc02" class="kw kx iq bd ky kz la dn lb lc ld dp le lf lg lh li lj lk ll lm ln lo lp lq lr bi translated">承认</h2><p id="27bf" class="pw-post-body-paragraph ls lt iq lu b lv lw jr lx ly lz ju ma lf mb mc md lj me mf mg ln mh mi mj mk ij bi translated">我希望感谢Uri Itai在撰写本文期间提供了富有成效的想法。</p><p id="ffb7" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated">代码位于<a class="ae kv" href="https://github.com/natank1/Text_Tab-DL" rel="noopener ugc nofollow" target="_blank">这里</a>。</p></div></div>    
</body>
</html>