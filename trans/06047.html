<html>
<head>
<title>Maths behind Supervised Learning for Dummies: The theory in plain words (Part II)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">傻瓜监督学习背后的数学:浅显易懂的理论(下)</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/maths-behind-supervised-learning-for-dummies-the-theory-in-plain-words-part-ii-b3681d690c6e?source=collection_archive---------31-----------------------#2021-05-30">https://towardsdatascience.com/maths-behind-supervised-learning-for-dummies-the-theory-in-plain-words-part-ii-b3681d690c6e?source=collection_archive---------31-----------------------#2021-05-30</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="10af" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">每个人监督学习背后的代数和几何的快速概述。</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/d89806aaa54deb01def555bab132e730.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*mE_8DqrC_FbNva6q"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">乔希·里默尔在<a class="ae kv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="f91f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><a class="ae kv" rel="noopener" target="_blank" href="/maths-behind-supervised-learning-for-dummies-the-theory-in-plain-words-part-i-8f9be4d7e33a">在我之前的文章</a>中，我写了这个系列的第一部分:<em class="ls">傻瓜监督学习背后的数学:简单明了的理论，</em>在这里我们看到了几何&amp;代数是监督学习的基础的概述。在本系列的第二部分中，我将介绍Vapnik-Chervonenkis维度，可能近似正确的学习，如何将我们的二元分类器扩展到多个类，以及回归。</p><h1 id="653f" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated">瓦普尼克-切尔沃嫩基斯维度也叫蛮力</h1><p id="6a4b" class="pw-post-body-paragraph kw kx iq ky b kz ml jr lb lc mm ju le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated">如果四个个体(<em class="ls"> x1，x2，x3，x4 </em>)可以分为两类(a类或b类)，那么就有2^4=16分类的可能性:(a中的<em class="ls"> x1 </em>，a中的<em class="ls"> x2 </em>，a中的<em class="ls"> x3 </em>，a中的<em class="ls"> x4 </em>或者(a中的<em class="ls"> x1 </em>，a中的<em class="ls"> x2 </em>，a中的<em class="ls"> x3 </em>因此，2^N学习问题(或可能的标签)可以由n个数据点给出。如果对于所有这些学习问题，我们可以建立一个<em class="ls"> h </em>(属于H类)将元素正确地分成它们的两个类，那么H粉碎了N个点。换句话说，给定的N个点总是被来自H类的假设<em class="ls"> h </em>正确无误地分开。</p><p id="d505" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">被H粉碎的点的最大数量被称为Vapnik-Chervonenkis维数(它足以粉碎空间中N个点的特定样本，而不是2维中的任何N个点)。例如，四个点可以被矩形打碎，尽管它们不可能对齐，但我们已经找到了一个这样的例子。对于二维中给定的4个点，我们可以找到一个矩形<em class="ls"> h </em>，它允许正确划分任何2⁴=16学习问题(或可能的标记)，因此4个点被h:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mq"><img src="../Images/66e57558b69e845994ca72ef880c6b42.png" data-original-src="https://miro.medium.com/v2/resize:fit:850/format:webp/1*yNcLuPVZDTC1JnJmWRdz2Q.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="65b6" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">然而，在二维空间中，没有办法用矩形粉碎5个点。对于二维空间中的任意5个点，总有一些2⁵=32可能标号无法求解:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mr"><img src="../Images/4b81853dee424f8c0e1eb7984fa3e0c6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KRvwnrL3c9yiOPbamxQl2A.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="4a7d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">当H是二维中轴对齐矩形的假设类时，H，<em class="ls"> VC(H)，</em>的Vapnik-Chervonenkis维数等于4。</p><p id="0011" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">为什么我说这也叫蛮力？好吧，大多数现实生活中的问题不能采取2^N的可能性。现实生活中的问题通常局限于一个分布，它们的点是从这个分布中提取的，并且空间中接近的点通常具有相同的标号。我们不必担心所有可能的学习问题，我们将使用一个具有较小VC维的假设类，而不是其他具有较高维度的假设类。这将简化我们的搜索，并且对于大多数给定的点都很有效(你还记得偏差的定义吗？).有些情况下，我们无法将所有绘制的点正确地分为两类。在这种情况下，我们将不得不说我们的假设类有多好或多准确，例如，<em class="ls">它以94%的准确性工作良好，</em>或<em class="ls">它在超过95%的情况下以小于3%的误差工作。</em></p><h1 id="96ea" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated">可能近似正确的学习又名现实生活中的近似</h1><p id="fa15" class="pw-post-body-paragraph kw kx iq ky b kz ml jr lb lc mm ju le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated">因此，假设我们有一个来自H的矩形假设<em class="ls"> h </em>来解决一个现实生活中的问题，它的数据点来自一个未知的概率分布。我们要找出假设<em class="ls"> h </em>以概率<em class="ls"> 1-d </em>最多误差<em class="ls"> E </em>的点数<em class="ls"> N </em>。换句话说，如果正确划分所有点的来自H的原始类是<em class="ls"> C，</em>那么当使用<em class="ls"> h </em>作为假设<em class="ls">时，在<em class="ls"> C </em>和<em class="ls"> h </em>之间的差异区域会产生假阴性。</em>在<em class="ls"> 1-d% </em>的情况下(置信概率)，该区域内误分配点的概率必须小于E(错误概率):</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ms"><img src="../Images/cec0e23cdc919033697b043d6572e2e3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iNANKcIlkC2A0jUawwff2Q.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="5ade" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">C和h之间的区域是另一个矩形(图像中的阴影矩形)，因此它可以被分成4个带(在角上重叠)。落在每一条中的概率是E/4，随机抽取的点错过每一条的概率是1-E/4。所有n个独立点错过该条的概率是(1-E/4)^N.另外，所有n个独立点错过四条中任何一条的概率是4(1-E/4)^N.这个概率最多一定是<em class="ls"> d </em>，因此4(1-e/4)^n&lt;d。我们不想深入细节，但是两边除以4，取日志并重新排列，我们得到N &gt; (4/E)log(4/ <em class="ls"> d </em>)。因此，我们至少需要这N个大小，以确保错误概率具有给定的置信概率。根据你的假设类别(我们用矩形)，这个公式必须被计算。</p><h1 id="4944" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated">学习多种课程</h1><p id="4c95" class="pw-post-body-paragraph kw kx iq ky b kz ml jr lb lc mm ju le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated">到目前为止，我们已经关注了二元分类器，但是如果有几个类呢？如果我们有K个不同的类，其中每个实例只属于其中的一个，我们可以将此视为K个两类问题。来自训练集X的经验误差现在是所有实例(N)上所有类别(k)的预测的总和:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mt"><img src="../Images/409a8d1875a5786e996b46524a45e4cd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IakRdOsknhcKJfAXYo7JTw.png"/></div></div></figure><h1 id="df4f" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated">回归</h1><p id="5883" class="pw-post-body-paragraph kw kx iq ky b kz ml jr lb lc mm ju le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated">在分类中，我们用二元行为计算经验误差:预测做得好或不好。对于给定的实例x，C(x)要么是0，要么是1。当我们不预测类，而是预测值时，问题就不同了，叫做回归。现在，<em class="ls"> r </em>是一个实值，从输入中获得如下<em class="ls"> r=f(x)+E. </em>我们必须将噪声E添加到输出中，它代表我们看不到的隐藏变量的影响。为了100%准确，我们应该说r=f*(x，z)，其中z表示隐藏变量。如果没有噪声，它将是一个插值而不是回归(例如f*是插值)。请注意，自然输出函数中的噪声不是由于记录中的不精确或标记中的误差，这些误差仅在预测时产生影响。</p><p id="18d4" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">所以我们想用我们的假设类G()来逼近f()，训练集X上的经验误差必须考虑输出中的数值距离，而不仅仅是分类中使用的等于/不等于。考虑该距离的一种方法是使用差值的平方:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mu"><img src="../Images/6044f65b504860281799d6444e77f3fe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zm55XzZyYaHNX0OBh3W3gA.png"/></div></div></figure><p id="c26e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们的G()可以是线性或二次或任何其他高阶函数。我们只需用其在经验误差公式上的定义替换G()，并通过对系数<em class="ls"> w ( </em>为参数的解析解)取偏导数来最小化它。请注意，首先我们选择G()类(例如，我们决定使用一个线性函数)，但是一旦我们设置了最小化经验误差的系数，那么我们就有了来自G()类的实例G()。我们应该选择哪个G()。我在上一篇文章中说过，当函数的阶数增加时，训练误差减小，但你可能会陷入过拟合。</p><p id="fe19" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><em class="ls"> Adrian Perez是数据科学主管，拥有超级计算并行算法博士学位。你可以在他的</em> <a class="ae kv" href="https://adrianpd.medium.com/" rel="noopener"> <em class="ls">中简介</em> </a> <em class="ls">中查看更多关于他的西班牙语和英语内容。</em></p><p id="accd" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">参考书目</strong></p><p id="a664" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><em class="ls">机器学习入门。</em>第四版。埃塞姆·阿尔帕丁山。麻省理工学院出版社2020年。</p></div></div>    
</body>
</html>