<html>
<head>
<title>Random Forest Algorithm in Python from Scratch</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Python中的随机森林算法</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/random-forest-algorithm-in-python-from-scratch-487b314048a8?source=collection_archive---------22-----------------------#2021-06-15">https://towardsdatascience.com/random-forest-algorithm-in-python-from-scratch-487b314048a8?source=collection_archive---------22-----------------------#2021-06-15</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="918f" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">使用(主要)数组和循环在python中编写强大的算法</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/3288c27175da9557024c8f6c416272d4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oQzsfYnRD-Q_sGlD_-bL2A.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">随机森林简化。按作者分类的图表</p></figure></div><div class="ab cl ky kz hx la" role="separator"><span class="lb bw bk lc ld le"/><span class="lb bw bk lc ld le"/><span class="lb bw bk lc ld"/></div><div class="im in io ip iq"><p id="f00c" class="pw-post-body-paragraph lf lg it lh b li lj ju lk ll lm jx ln lo lp lq lr ls lt lu lv lw lx ly lz ma im bi translated">本文旨在揭开流行的随机森林(random forest)算法的神秘面纱，并通过使用图形、代码片段和代码输出来展示其原理。</p><p id="4383" class="pw-post-body-paragraph lf lg it lh b li lj ju lk ll lm jx ln lo lp lq lr ls lt lu lv lw lx ly lz ma im bi translated">我用python写的<strong class="lh iu"> RF </strong>算法的完整实现可以通过:【https://github.com/Eligijus112/decision-tree-python】T4访问</p><p id="d986" class="pw-post-body-paragraph lf lg it lh b li lj ju lk ll lm jx ln lo lp lq lr ls lt lu lv lw lx ly lz ma im bi translated">我强烈鼓励偶然看到这篇文章的人深入研究代码，因为对代码的理解将使将来阅读关于<strong class="lh iu"> RF </strong>的任何文档更加简单明了，压力也更小。</p><p id="66a5" class="pw-post-body-paragraph lf lg it lh b li lj ju lk ll lm jx ln lo lp lq lr ls lt lu lv lw lx ly lz ma im bi translated">任何关于优化的建议都受到高度鼓励，并欢迎通过GitHub上的pull请求。</p></div><div class="ab cl ky kz hx la" role="separator"><span class="lb bw bk lc ld le"/><span class="lb bw bk lc ld le"/><span class="lb bw bk lc ld"/></div><div class="im in io ip iq"><p id="791d" class="pw-post-body-paragraph lf lg it lh b li lj ju lk ll lm jx ln lo lp lq lr ls lt lu lv lw lx ly lz ma im bi translated">RF的构建模块是简单的决策树。如果读者熟悉分类决策树的概念，这篇文章会更容易阅读。强烈建议在进一步阅读之前通读以下文章:</p><div class="mc md gp gr me mf"><a rel="noopener follow" target="_blank" href="/decision-tree-algorithm-in-python-from-scratch-8c43f0e40173"><div class="mg ab fo"><div class="mh ab mi cl cj mj"><h2 class="bd iu gy z fp mk fr fs ml fu fw is bi translated">Python中的决策树算法从零开始</h2><div class="mm l"><h3 class="bd b gy z fp mk fr fs ml fu fw dk translated">用Python编写只使用NumPy和Pandas的流行算法，并解释其中的内容</h3></div><div class="mn l"><p class="bd b dl z fp mk fr fs ml fu fw dk translated">towardsdatascience.com</p></div></div><div class="mo l"><div class="mp l mq mr ms mo mt ks mf"/></div></div></a></div></div><div class="ab cl ky kz hx la" role="separator"><span class="lb bw bk lc ld le"/><span class="lb bw bk lc ld le"/><span class="lb bw bk lc ld"/></div><div class="im in io ip iq"><p id="b45d" class="pw-post-body-paragraph lf lg it lh b li lj ju lk ll lm jx ln lo lp lq lr ls lt lu lv lw lx ly lz ma im bi translated"><strong class="lh iu"><em class="mu">sci kit-learn</em></strong>python中有一个RF算法的实现，速度很快，已经审核了几百次:</p><div class="mc md gp gr me mf"><a href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html" rel="noopener  ugc nofollow" target="_blank"><div class="mg ab fo"><div class="mh ab mi cl cj mj"><h2 class="bd iu gy z fp mk fr fs ml fu fw is bi translated">sk learn . ensemble . randomforestclassifier-sci kit-learn 0 . 24 . 2文档</h2><div class="mm l"><h3 class="bd b gy z fp mk fr fs ml fu fw dk translated">随机森林分类器。随机森林是一个元估计器，它适合于许多决策树分类器…</h3></div><div class="mn l"><p class="bd b dl z fp mk fr fs ml fu fw dk translated">scikit-learn.org</p></div></div><div class="mo l"><div class="mv l mq mr ms mo mt ks mf"/></div></div></a></div><p id="919e" class="pw-post-body-paragraph lf lg it lh b li lj ju lk ll lm jx ln lo lp lq lr ls lt lu lv lw lx ly lz ma im bi translated">在文档中，需要定义的第一个超参数是<strong class="lh iu"> <em class="mu"> n_estimators </em> </strong>参数，默认值为100。这个参数的解释非常优雅:</p><blockquote class="mw mx my"><p id="ce0b" class="lf lg mu lh b li lj ju lk ll lm jx ln mz lp lq lr na lt lu lv nb lx ly lz ma im bi translated">森林中树木的数量。</p></blockquote><p id="aee0" class="pw-post-body-paragraph lf lg it lh b li lj ju lk ll lm jx ln lo lp lq lr ls lt lu lv lw lx ly lz ma im bi translated">这个解释非常浪漫，但是一个顽固的数学家第一次阅读时可能会觉得有点困惑:我们在谈论什么样的森林和什么样的树？</p></div><div class="ab cl ky kz hx la" role="separator"><span class="lb bw bk lc ld le"/><span class="lb bw bk lc ld le"/><span class="lb bw bk lc ld"/></div><div class="im in io ip iq"><p id="e122" class="pw-post-body-paragraph lf lg it lh b li lj ju lk ll lm jx ln lo lp lq lr ls lt lu lv lw lx ly lz ma im bi translated">随机森林算法背后的直觉可以分成两大部分:<strong class="lh iu"> <em class="mu">随机</em> </strong>部分和<strong class="lh iu"> <em class="mu">森林</em> </strong>部分。让我们从后者开始。</p><p id="0eaf" class="pw-post-body-paragraph lf lg it lh b li lj ju lk ll lm jx ln lo lp lq lr ls lt lu lv lw lx ly lz ma im bi translated">现实生活中的森林是由一堆树组成的。一个随机森林分类器是由一堆决策树分类器组成的(这里和整个文本— <strong class="lh iu"> DT </strong>)。组成整个森林的<strong class="lh iu"> DT </strong> s的确切数量由前面提到的<strong class="lh iu"> <em class="mu"> n_estimators </em> </strong>变量定义。</p><p id="7013" class="pw-post-body-paragraph lf lg it lh b li lj ju lk ll lm jx ln lo lp lq lr ls lt lu lv lw lx ly lz ma im bi translated">一个<strong class="lh iu"> RF </strong>算法中的每个<strong class="lh iu"> DT </strong>完全相互独立。如果n_estimators变量设置为50，则森林由50棵决策树组成，这些决策树彼此完全独立生长，不共享任何信息。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nc"><img src="../Images/bb7dfcf50bf65e8488e09633e7632e5c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Pn4aDGmGrKtEsmX_VQsFbQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">50分钟。按作者分类的图表</p></figure><p id="8434" class="pw-post-body-paragraph lf lg it lh b li lj ju lk ll lm jx ln lo lp lq lr ls lt lu lv lw lx ly lz ma im bi translated">二进制分类决策树可以被视为一个函数，它接受输入<strong class="lh iu"> X </strong>并输出1或0:</p><p id="179c" class="pw-post-body-paragraph lf lg it lh b li lj ju lk ll lm jx ln lo lp lq lr ls lt lu lv lw lx ly lz ma im bi translated"><strong class="lh iu"> DT: X → {0，1} </strong></p><p id="2198" class="pw-post-body-paragraph lf lg it lh b li lj ju lk ll lm jx ln lo lp lq lr ls lt lu lv lw lx ly lz ma im bi translated">RF<strong class="lh iu">的最终预测</strong>是从每个单独DT做出的预测的多数投票。</p><p id="fc71" class="pw-post-body-paragraph lf lg it lh b li lj ju lk ll lm jx ln lo lp lq lr ls lt lu lv lw lx ly lz ma im bi translated">如果在50棵树中，有30棵树将新的观察标记为<strong class="lh iu"> <em class="mu"> 1 </em> </strong>，20棵树将相同的观察标记为<strong class="lh iu"> <em class="mu"> 0 </em> </strong>随机森林的最终预测将是<strong class="lh iu"> <em class="mu"> 1 </em> </strong>。</p><p id="b0ed" class="pw-post-body-paragraph lf lg it lh b li lj ju lk ll lm jx ln lo lp lq lr ls lt lu lv lw lx ly lz ma im bi translated">在关于简单分类决策树的文章中，很明显，对于相同的输入和相同的超参数，决策树将学习相同的输出和相同的规则。那么，为什么要种50(或100，或1000，或<strong class="lh iu"> k </strong>呢？这就是第二部分<strong class="lh iu"> RF </strong>直觉的用武之地:随机<strong class="lh iu">部分。</strong></p></div><div class="ab cl ky kz hx la" role="separator"><span class="lb bw bk lc ld le"/><span class="lb bw bk lc ld le"/><span class="lb bw bk lc ld"/></div><div class="im in io ip iq"><p id="c64e" class="pw-post-body-paragraph lf lg it lh b li lj ju lk ll lm jx ln lo lp lq lr ls lt lu lv lw lx ly lz ma im bi translated">RF中的随机部分可以分为两部分。</p><p id="02eb" class="pw-post-body-paragraph lf lg it lh b li lj ju lk ll lm jx ln lo lp lq lr ls lt lu lv lw lx ly lz ma im bi translated"><strong class="lh iu">第一部分</strong>是数据的自举。数据自举是其行的替换子样本。python实现中创建引导示例的部分:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nd ne l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">用于引导的代码。按作者</p></figure><p id="4561" class="pw-post-body-paragraph lf lg it lh b li lj ju lk ll lm jx ln lo lp lq lr ls lt lu lv lw lx ly lz ma im bi translated">例如，如果整个数据集<strong class="lh iu"> d </strong>有10行:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nf"><img src="../Images/7e93e759ab2627a6c9c5d48bd75bea90.png" data-original-src="https://miro.medium.com/v2/resize:fit:794/format:webp/1*pcd0ehsp26sMzvYMpEyBvQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">数据集示例。作者图片</p></figure><p id="9240" class="pw-post-body-paragraph lf lg it lh b li lj ju lk ll lm jx ln lo lp lq lr ls lt lu lv lw lx ly lz ma im bi translated">响应变量是客户流失列，其他三列是特性。</p><p id="800b" class="pw-post-body-paragraph lf lg it lh b li lj ju lk ll lm jx ln lo lp lq lr ls lt lu lv lw lx ly lz ma im bi translated"><strong class="lh iu"> d </strong>的三个独立引导数据集可能看起来像:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ng"><img src="../Images/0c0a64c3f6ede3e9c3778fce3785a045.png" data-original-src="https://miro.medium.com/v2/resize:fit:812/format:webp/1*lQPaYGyqirDxosWT6sP3qA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">3个数据引导。作者图片</p></figure><p id="abb2" class="pw-post-body-paragraph lf lg it lh b li lj ju lk ll lm jx ln lo lp lq lr ls lt lu lv lw lx ly lz ma im bi translated">从上图可以看出，第一个样本中，第5排和第8排是一样的。此外，并不是所有的原始观察都存在。</p><p id="3a42" class="pw-post-body-paragraph lf lg it lh b li lj ju lk ll lm jx ln lo lp lq lr ls lt lu lv lw lx ly lz ma im bi translated">如果我们定义<strong class="lh iu"> RF </strong>具有k个决策树(<em class="mu"> n_estimators = k </em>)，那么将会创建k个不同的引导数据集，并且每个树将会随着不同的数据集而增长。每个数据集的行数可以与原始数据集相同，也可以比原始数据集少。</p><p id="53f9" class="pw-post-body-paragraph lf lg it lh b li lj ju lk ll lm jx ln lo lp lq lr ls lt lu lv lw lx ly lz ma im bi translated">因此，如果<strong class="lh iu"> RF </strong>由50个决策树组成，那么<strong class="lh iu"> RF </strong>的高层图将如下所示:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nh"><img src="../Images/b1586d46cc2bf21b97017952e375990f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9MClAS2xuiEFGFPGIKn0-Q.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">引导程序。按作者分类的图表</p></figure><p id="3f70" class="pw-post-body-paragraph lf lg it lh b li lj ju lk ll lm jx ln lo lp lq lr ls lt lu lv lw lx ly lz ma im bi translated">五十棵决策树中的每一棵都将随着唯一的数据集而增长。</p><p id="fee4" class="pw-post-body-paragraph lf lg it lh b li lj ju lk ll lm jx ln lo lp lq lr ls lt lu lv lw lx ly lz ma im bi translated"><strong class="lh iu">随机森林中的随机第二部</strong>正在训练阶段。当在数据上拟合分类决策树时，在每个分割选择期间，特征保持不变。</p><p id="e046" class="pw-post-body-paragraph lf lg it lh b li lj ju lk ll lm jx ln lo lp lq lr ls lt lu lv lw lx ly lz ma im bi translated">在随机森林分类器创建过程中，每个决策树的增长与这里实现的算法略有不同</p><p id="1e44" class="pw-post-body-paragraph lf lg it lh b li lj ju lk ll lm jx ln lo lp lq lr ls lt lu lv lw lx ly lz ma im bi translated"><a class="ae mb" href="https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier" rel="noopener ugc nofollow" target="_blank">https://sci kit-learn . org/stable/modules/generated/sk learn . tree . decision tree classifier . html # sk learn . tree . decision tree classifier</a></p><p id="bae3" class="pw-post-body-paragraph lf lg it lh b li lj ju lk ll lm jx ln lo lp lq lr ls lt lu lv lw lx ly lz ma im bi translated">唯一的区别是，在每个分割创建时刻，抽取一个具有<strong class="lh iu">特征的随机子样本，并利用这些特征计算最大基尼系数。</strong></p><p id="e17c" class="pw-post-body-paragraph lf lg it lh b li lj ju lk ll lm jx ln lo lp lq lr ls lt lu lv lw lx ly lz ma im bi translated"><a class="ae mb" href="https://gist.github.com/Eligijus112/4c47be2f7566299bb8c4f97c107d82c6" rel="noopener ugc nofollow" target="_blank">https://gist . github . com/eligijus 112/4c 47 be 2 f 7566299 bb 8 C4 f 97 c 107d 82 c6f</a></p><p id="fa49" class="pw-post-body-paragraph lf lg it lh b li lj ju lk ll lm jx ln lo lp lq lr ls lt lu lv lw lx ly lz ma im bi translated">例如，假设我们有一个具有100个初始特征的自举数据样本<strong class="lh iu"> d </strong>和<strong class="lh iu"> X </strong>矩阵。我们可以用参数<strong class="lh iu">X _ features _ fraction</strong>(sci kit-learn实现中的<strong class="lh iu"> max_features </strong>)定义在每个分割阶段要留下多少个特征。让我们将其设置为0.05，这意味着在每次分割时，将选择5个随机特征(或5%)进行分割。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ni"><img src="../Images/1974374810176eb6aa832c24ba3b2a8c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*X70dYyXhbhtx3sl2dasaNQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">射频树示例。按作者分类的图表</p></figure><p id="0d38" class="pw-post-body-paragraph lf lg it lh b li lj ju lk ll lm jx ln lo lp lq lr ls lt lu lv lw lx ly lz ma im bi translated">在第一个节点中，有五个X特征:1、25、28、30和98。当用值x分割第25个特征时，找到最佳分割。下面两个节点具有另外5个随机特征，其中搜索最佳分割。</p></div><div class="ab cl ky kz hx la" role="separator"><span class="lb bw bk lc ld le"/><span class="lb bw bk lc ld le"/><span class="lb bw bk lc ld"/></div><div class="im in io ip iq"><p id="2165" class="pw-post-body-paragraph lf lg it lh b li lj ju lk ll lm jx ln lo lp lq lr ls lt lu lv lw lx ly lz ma im bi translated">因此，<strong class="lh iu"> RF </strong>算法如下:</p><p id="5d98" class="pw-post-body-paragraph lf lg it lh b li lj ju lk ll lm jx ln lo lp lq lr ls lt lu lv lw lx ly lz ma im bi translated">选择一组超参数。</p><p id="eaf4" class="pw-post-body-paragraph lf lg it lh b li lj ju lk ll lm jx ln lo lp lq lr ls lt lu lv lw lx ly lz ma im bi translated">对于<strong class="lh iu"> 1到</strong>k<strong class="lh iu">树做:</strong></p><ul class=""><li id="f144" class="nj nk it lh b li lj ll lm lo nl ls nm lw nn ma no np nq nr bi translated">生成一个随机引导数据样本<strong class="lh iu"> d </strong>。</li><li id="6836" class="nj nk it lh b li ns ll nt lo nu ls nv lw nw ma no np nq nr bi translated">对数据<strong class="lh iu"> d </strong>拟合决策树。在生长阶段的每次分割过程中，选择要分割的要素的随机子样本。</li></ul><p id="e9cf" class="pw-post-body-paragraph lf lg it lh b li lj ju lk ll lm jx ln lo lp lq lr ls lt lu lv lw lx ly lz ma im bi translated">预测时，获得每一个<strong class="lh iu"> k </strong>树的预测，然后使用多数投票选择标签。</p></div><div class="ab cl ky kz hx la" role="separator"><span class="lb bw bk lc ld le"/><span class="lb bw bk lc ld le"/><span class="lb bw bk lc ld"/></div><div class="im in io ip iq"><p id="5b79" class="pw-post-body-paragraph lf lg it lh b li lj ju lk ll lm jx ln lo lp lq lr ls lt lu lv lw lx ly lz ma im bi translated">让我们在一些真实数据上安装一个<strong class="lh iu"> RF </strong>分类器。完整的笔记本和数据可以通过以下方式访问:</p><p id="d42f" class="pw-post-body-paragraph lf lg it lh b li lj ju lk ll lm jx ln lo lp lq lr ls lt lu lv lw lx ly lz ma im bi translated"><a class="ae mb" href="https://github.com/Eligijus112/decision-tree-python/blob/main/RandomForestShowcase.ipynb" rel="noopener ugc nofollow" target="_blank">https://github . com/eligijus 112/decision-tree-python/blob/main/randomforestshowcase . ipynb</a></p><p id="8876" class="pw-post-body-paragraph lf lg it lh b li lj ju lk ll lm jx ln lo lp lq lr ls lt lu lv lw lx ly lz ma im bi translated">我的GitHub存储库中的数据集有3333行，我们将使用以下各列:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nx"><img src="../Images/e03ea047f95f059c304e08cfc0d41f85.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4n1up5wQ7z_6bM45nU0Ovw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">数据样本。作者图片</p></figure><p id="2cf1" class="pw-post-body-paragraph lf lg it lh b li lj ju lk ll lm jx ln lo lp lq lr ls lt lu lv lw lx ly lz ma im bi translated">目标是创建一个分类器，预测电信客户是否会流失。<strong class="lh iu"> <em class="mu">流失</em> </strong>是客户不再使用公司产品的事件。如果churn = 1，则意味着用户已经切换到不同的服务提供商，或者由于任何其他原因，没有显示任何活动。</p><p id="bb6b" class="pw-post-body-paragraph lf lg it lh b li lj ju lk ll lm jx ln lo lp lq lr ls lt lu lv lw lx ly lz ma im bi translated">让我们将数据拆分为训练和测试拆分。训练数据将有75%的行(2500)，测试集将有25%的行(833)。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nd ne l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">如何使用自定义RF类的示例代码。作者代码</p></figure><p id="d31a" class="pw-post-body-paragraph lf lg it lh b li lj ju lk ll lm jx ln lo lp lq lr ls lt lu lv lw lx ly lz ma im bi translated">生长5个决策树，每个分裂中具有25%的特征，并且每个树的深度为2，我们得到以下森林:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/ee1babec726b6b43a5d683496480a339.png" data-original-src="https://miro.medium.com/v2/resize:fit:612/format:webp/1*H79idwCykMNVOFXCVtWcXQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">森林的生长。作者图片</p></figure><pre class="kj kk kl km gt nz oa ob oc aw od bi"><span id="6717" class="oe of it oa b gy og oh l oi oj">------ <br/><br/>Tree number: 1 <br/><br/>Root<br/>   | GINI impurity of the node: 0.27<br/>   | Class distribution in the node: {0: 2099, 1: 401}<br/>   | Predicted class: 0<br/>|-------- Split rule: DayCalls &lt;= 113.5<br/>           | GINI impurity of the node: 0.24<br/>           | Class distribution in the node: {0: 1598, 1: 257}<br/>           | Predicted class: 0<br/>|---------------- Split rule: DayMins &lt;= 287.7<br/>                   | GINI impurity of the node: 0.21<br/>                   | Class distribution in the node: {0: 1591, 1: 218}<br/>                   | Predicted class: 0<br/>|---------------- Split rule: DayMins &gt; 287.7<br/>                   | GINI impurity of the node: 0.26<br/>                   | Class distribution in the node: {1: 39, 0: 7}<br/>                   | Predicted class: 1<br/>|-------- Split rule: DayCalls &gt; 113.5<br/>           | GINI impurity of the node: 0.35<br/>           | Class distribution in the node: {0: 501, 1: 144}<br/>           | Predicted class: 0<br/>|---------------- Split rule: DayMins &lt;= 225.0<br/>                   | GINI impurity of the node: 0.25<br/>                   | Class distribution in the node: {0: 431, 1: 76}<br/>                   | Predicted class: 0<br/>|---------------- Split rule: DayMins &gt; 225.0<br/>                   | GINI impurity of the node: 0.5<br/>                   | Class distribution in the node: {1: 68, 0: 70}<br/>                   | Predicted class: 0<br/>------ <br/><br/>------ <br/><br/>Tree number: 2 <br/><br/>Root<br/>   | GINI impurity of the node: 0.26<br/>   | Class distribution in the node: {0: 2124, 1: 376}<br/>   | Predicted class: 0<br/>|-------- Split rule: OverageFee &lt;= 13.235<br/>           | GINI impurity of the node: 0.24<br/>           | Class distribution in the node: {0: 1921, 1: 307}<br/>           | Predicted class: 0<br/>|---------------- Split rule: DayMins &lt;= 261.45<br/>                   | GINI impurity of the node: 0.18<br/>                   | Class distribution in the node: {0: 1853, 1: 210}<br/>                   | Predicted class: 0<br/>|---------------- Split rule: DayMins &gt; 261.45<br/>                   | GINI impurity of the node: 0.48<br/>                   | Class distribution in the node: {0: 68, 1: 97}<br/>                   | Predicted class: 1<br/>|-------- Split rule: OverageFee &gt; 13.235<br/>           | GINI impurity of the node: 0.38<br/>           | Class distribution in the node: {1: 69, 0: 203}<br/>           | Predicted class: 0<br/>|---------------- Split rule: DayMins &lt;= 220.35<br/>                   | GINI impurity of the node: 0.13<br/>                   | Class distribution in the node: {0: 186, 1: 14}<br/>                   | Predicted class: 0<br/>|---------------- Split rule: DayMins &gt; 220.35<br/>                   | GINI impurity of the node: 0.36<br/>                   | Class distribution in the node: {1: 55, 0: 17}<br/>                   | Predicted class: 1<br/>------ <br/><br/>------ <br/><br/>Tree number: 3 <br/><br/>Root<br/>   | GINI impurity of the node: 0.25<br/>   | Class distribution in the node: {1: 366, 0: 2134}<br/>   | Predicted class: 0<br/>|-------- Split rule: DataUsage &lt;= 0.315<br/>           | GINI impurity of the node: 0.29<br/>           | Class distribution in the node: {1: 286, 0: 1364}<br/>           | Predicted class: 0<br/>|---------------- Split rule: MonthlyCharge &lt;= 62.05<br/>                   | GINI impurity of the node: 0.18<br/>                   | Class distribution in the node: {1: 144, 0: 1340}<br/>                   | Predicted class: 0<br/>|---------------- Split rule: MonthlyCharge &gt; 62.05<br/>                   | GINI impurity of the node: 0.25<br/>                   | Class distribution in the node: {1: 142, 0: 24}<br/>                   | Predicted class: 1<br/>|-------- Split rule: DataUsage &gt; 0.315<br/>           | GINI impurity of the node: 0.17<br/>           | Class distribution in the node: {0: 770, 1: 80}<br/>           | Predicted class: 0<br/>|---------------- Split rule: RoamMins &lt;= 13.45<br/>                   | GINI impurity of the node: 0.12<br/>                   | Class distribution in the node: {0: 701, 1: 49}<br/>                   | Predicted class: 0<br/>|---------------- Split rule: RoamMins &gt; 13.45<br/>                   | GINI impurity of the node: 0.43<br/>                   | Class distribution in the node: {0: 69, 1: 31}<br/>                   | Predicted class: 0<br/>------ <br/><br/>------ <br/><br/>Tree number: 4 <br/><br/>Root<br/>   | GINI impurity of the node: 0.26<br/>   | Class distribution in the node: {0: 2119, 1: 381}<br/>   | Predicted class: 0<br/>|-------- Split rule: DayCalls &lt;= 49.5<br/>           | GINI impurity of the node: 0.49<br/>           | Class distribution in the node: {1: 8, 0: 6}<br/>           | Predicted class: 1<br/>|---------------- Split rule: MonthlyCharge &lt;= 31.5<br/>                   | GINI impurity of the node: 0.0<br/>                   | Class distribution in the node: {0: 4}<br/>                   | Predicted class: 0<br/>|---------------- Split rule: MonthlyCharge &gt; 31.5<br/>                   | GINI impurity of the node: 0.32<br/>                   | Class distribution in the node: {1: 8, 0: 2}<br/>                   | Predicted class: 1<br/>|-------- Split rule: DayCalls &gt; 49.5<br/>           | GINI impurity of the node: 0.26<br/>           | Class distribution in the node: {0: 2113, 1: 373}<br/>           | Predicted class: 0<br/>|---------------- Split rule: DayMins &lt;= 264.6<br/>                   | GINI impurity of the node: 0.21<br/>                   | Class distribution in the node: {0: 2053, 1: 279}<br/>                   | Predicted class: 0<br/>|---------------- Split rule: DayMins &gt; 264.6<br/>                   | GINI impurity of the node: 0.48<br/>                   | Class distribution in the node: {1: 94, 0: 60}<br/>                   | Predicted class: 1<br/>------ <br/><br/>------ <br/><br/>Tree number: 5 <br/><br/>Root<br/>   | GINI impurity of the node: 0.24<br/>   | Class distribution in the node: {0: 2155, 1: 345}<br/>   | Predicted class: 0<br/>|-------- Split rule: OverageFee &lt;= 7.945<br/>           | GINI impurity of the node: 0.15<br/>           | Class distribution in the node: {0: 475, 1: 43}<br/>           | Predicted class: 0<br/>|---------------- Split rule: AccountWeeks &lt;= 7.0<br/>                   | GINI impurity of the node: 0.28<br/>                   | Class distribution in the node: {1: 5, 0: 1}<br/>                   | Predicted class: 1<br/>|---------------- Split rule: AccountWeeks &gt; 7.0<br/>                   | GINI impurity of the node: 0.14<br/>                   | Class distribution in the node: {0: 474, 1: 38}<br/>                   | Predicted class: 0<br/>|-------- Split rule: OverageFee &gt; 7.945<br/>           | GINI impurity of the node: 0.26<br/>           | Class distribution in the node: {0: 1680, 1: 302}<br/>           | Predicted class: 0<br/>|---------------- Split rule: DayMins &lt;= 259.9<br/>                   | GINI impurity of the node: 0.2<br/>                   | Class distribution in the node: {0: 1614, 1: 203}<br/>                   | Predicted class: 0<br/>|---------------- Split rule: DayMins &gt; 259.9<br/>                   | GINI impurity of the node: 0.48<br/>                   | Class distribution in the node: {0: 66, 1: 99}<br/>                   | Predicted class: 1<br/>------</span></pre><p id="2b49" class="pw-post-body-paragraph lf lg it lh b li lj ju lk ll lm jx ln lo lp lq lr ls lt lu lv lw lx ly lz ma im bi translated">每棵树都有点不同。具有所选超参数的测试集上的精度和召回分数是:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ok"><img src="../Images/4db7f31e5371f902f01f929daca1c57d.png" data-original-src="https://miro.medium.com/v2/resize:fit:620/format:webp/1*DpL79DxuOWPdFYadt_LKpA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">准确性结果。作者图片</p></figure><p id="91b7" class="pw-post-body-paragraph lf lg it lh b li lj ju lk ll lm jx ln lo lp lq lr ls lt lu lv lw lx ly lz ma im bi translated">让我们尝试通过种植更复杂的随机森林来提高准确性指标。</p><p id="49db" class="pw-post-body-paragraph lf lg it lh b li lj ju lk ll lm jx ln lo lp lq lr ls lt lu lv lw lx ly lz ma im bi translated">当生长一个有30棵树的随机森林，每个分割点有50%的特征，max_depth为4时，测试集中的精度为:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ol"><img src="../Images/5bd14b599d7cf35c46ebdbf96de1735d.png" data-original-src="https://miro.medium.com/v2/resize:fit:602/format:webp/1*1LKB48BGTJRwzsQiKJfM4g.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">更复杂RF下的精度。作者图片</p></figure><p id="f30e" class="pw-post-body-paragraph lf lg it lh b li lj ju lk ll lm jx ln lo lp lq lr ls lt lu lv lw lx ly lz ma im bi translated">如果我们种植100棵树，具有75%的特征，并且树的最大深度是5，则结果是:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi om"><img src="../Images/d1b056d7e7b936e0338513a9d2c1d1fe.png" data-original-src="https://miro.medium.com/v2/resize:fit:914/format:webp/1*8eurECXLv-QFRSTwTelHnQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">更复杂的射频结果。作者图片</p></figure><p id="81de" class="pw-post-body-paragraph lf lg it lh b li lj ju lk ll lm jx ln lo lp lq lr ls lt lu lv lw lx ly lz ma im bi translated">scikit learn实现给出了非常相似的结果:</p><pre class="kj kk kl km gt nz oa ob oc aw od bi"><span id="c203" class="oe of it oa b gy og oh l oi oj"># Skicit learn implementation<br/>from sklearn.ensemble import RandomForestClassifier</span><span id="78a2" class="oe of it oa b gy on oh l oi oj"># Initiating<br/>rf_scikit = RandomForestClassifier(n_estimators=100, max_features=0.75, max_depth=5, min_samples_split=5)</span><span id="a017" class="oe of it oa b gy on oh l oi oj"># Fitting <br/>start = time.time()<br/>rf_scikit.fit(X=train[features], y=train[‘Churn’])<br/>print(f”Time took for scikit learn: {round(time.time() — start, 2)} seconds”)</span><span id="4740" class="oe of it oa b gy on oh l oi oj"># Forecasting <br/>yhatsc = rf_scikit.predict(test[features])<br/>test[‘yhatsc’] = yhatsc</span><span id="8bd1" class="oe of it oa b gy on oh l oi oj">print(f”Total churns in test set: {test[‘Churn’].sum()}”)<br/>print(f”Total predicted churns in test set: {test[‘yhat’].sum()}”)</span><span id="72af" class="oe of it oa b gy on oh l oi oj">print(f”Precision: {round(precision_score(test[‘Churn’], test[‘yhatsc’]), 2) * 100} %”)<br/>print(f”Recall: {round(recall_score(test[‘Churn’], test[‘yhatsc’]), 2) * 100} %”)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oo"><img src="../Images/9567fcb5cc71c1ce49632d7ab969f82a.png" data-original-src="https://miro.medium.com/v2/resize:fit:952/format:webp/1*Q22JyT-osXALxpJ3R2Ulhg.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">Scikit学习结果。作者照片</p></figure><p id="e015" class="pw-post-body-paragraph lf lg it lh b li lj ju lk ll lm jx ln lo lp lq lr ls lt lu lv lw lx ly lz ma im bi translated"><em class="mu">请注意，sklearn和定制代码结果之间的差异可能会有所不同，因为每棵树的自举数据的随机性质以及用于找到最佳分割的随机特征。即使使用相同的超参数运行相同的实现，我们每次都会得到稍微不同的结果。</em></p></div><div class="ab cl ky kz hx la" role="separator"><span class="lb bw bk lc ld le"/><span class="lb bw bk lc ld le"/><span class="lb bw bk lc ld"/></div><div class="im in io ip iq"><p id="aaa0" class="pw-post-body-paragraph lf lg it lh b li lj ju lk ll lm jx ln lo lp lq lr ls lt lu lv lw lx ly lz ma im bi translated">总之，随机森林算法是由独立的简单决策树组成的。</p><p id="9749" class="pw-post-body-paragraph lf lg it lh b li lj ju lk ll lm jx ln lo lp lq lr ls lt lu lv lw lx ly lz ma im bi translated">每个决策树都是使用自定义引导数据集创建的。在每次分裂和每个决策树中，当搜索增加GINI增益的最佳特征和特征值时，考虑特征的随机子样本。</p><p id="5315" class="pw-post-body-paragraph lf lg it lh b li lj ju lk ll lm jx ln lo lp lq lr ls lt lu lv lw lx ly lz ma im bi translated">一个<strong class="lh iu"> RF </strong>分类器的最终预测是所有独立决策树的多数投票。</p></div></div>    
</body>
</html>