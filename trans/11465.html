<html>
<head>
<title>The bigger picture: How SparkSQL relates to Spark core (RDD API)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">更大的图片:SparkSQL 如何与 Spark 核心(RDD API)相关</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/the-bigger-picture-how-sparksql-relates-to-spark-core-rdd-api-56b85791c44c?source=collection_archive---------9-----------------------#2021-11-11">https://towardsdatascience.com/the-bigger-picture-how-sparksql-relates-to-spark-core-rdd-api-56b85791c44c?source=collection_archive---------9-----------------------#2021-11-11</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="922a" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsdatascience.com/tagged/notes-from-industry" rel="noopener" target="_blank">行业笔记</a></h2><div class=""/><div class=""><h2 id="431d" class="pw-subtitle-paragraph jw iz iq bd b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn dk translated">人们常说 Spark 的高级 API SparkSQL“构建在”Spark 核心之上。然而，这并不是不言自明的。我潜水了。</h2></div><p id="2635" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">Apache Spark 的高级 API SparkSQL 提供了一个简洁且极具表现力的 API 来对分布式数据执行结构化查询。尽管它构建在 Spark core API 之上，但人们通常并不清楚这两者之间的关系。在这篇文章中，我将试着描绘出更大的图景，并说明这些事情是如何相互关联的。</p><p id="4254" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">Spark 核心所基于的最基本的概念是 MapReduce。MapReduce 是声明性和过程性方法的混合。一方面，它要求用户指定两个功能—即 map 和 reduce —在分布式数据上同时执行。这是声明部分——我们只告诉 MR 做什么，而不是如何完成的每一步。另一方面，这两项职能中的逻辑是程序性的。这意味着我们编写的代码会对每一部分数据逐步执行。逻辑本身——函数的语义——对框架来说是不透明的。它唯一知道的是这些函数的输入和输出类型。因此，自动优化仅在 map 和 reduce 函数级别上是可能的。</p><p id="e763" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">正如我在 2018 年欧洲 Spark &amp; AI 峰会上所阐述的那样，Spark core 基本上在 MapReduce 范式的基础上增加了一个新的抽象层。它与指定更高级的转换没有什么不同，这些转换被转换成 map 和 reduce 函数的优化链。然而，优化只能发生在 map/reduce 函数级别。因此，Spark core 的优化策略仅限于将<strong class="kq ja">狭窄的转换</strong>分成几个阶段(<a class="ae lk" href="https://www.youtube.com/watch?v=KfrmJTQ_AfA&amp;t=801s" rel="noopener ugc nofollow" target="_blank">观看此</a>以深入了解 Spark core)。</p><p id="a1b6" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">相反，SparkSQL 提供了一种在分布式数据上指定完全声明性查询的方法。然而，为了做到这一点，数据的结构需要通过提供显式模式(即结构化数据)而为框架所知。然后，API 允许用户指定对数据的查询，提供许多预定义的操作符(非常类似于 SQL)。因此，结构化查询是运算符的组合，也称为表达式。API 提供了两种指定用户查询的方法:使用 Dataset-API 中定义的函数，或者直接编写 SQL 查询。这两种方法都简单地提供了一种方式来告诉 Spark 如何使用预定义的操作符从输入数据集派生出结果数据集。然而，在某些情况下，如果 Spark 中不存在合适的操作符，我们会发现自己被迫编写所谓的用户定义函数。这些函数只是对每个记录执行过程代码的一种方式。</p><p id="f679" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">用户为产生所需结果而指定的查询(或表达式)构建了一个抽象语法树(AST)。该树由表达式和运算符组成，这些运算符本身接受表达式。与 Spark core 不同的是，在 Spark core 中，我们组装了一系列正在执行的不透明函数，现在我们有了一系列用于产生最终结果的逻辑操作数。因为我们可以区分这些操作符，所以我们现在可以在这些操作的级别上应用优化规则。称为 catalyst 的 Spark 优化器指定了许多优化这类树的通用规则。</p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><div class="gh gi ll"><img src="../Images/15facda60d1c64b11a3a02f464a29157.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*IOtmKz2-Uv9BMGS7.png"/></div></div><p class="lx ly gj gh gi lz ma bd b be z dk translated">作者图片</p></figure><p id="2b04" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">第一步，通过使用内部维护的目录解析列名和表名，将一个 AST 转换成另一个 AST。然后，通过递归地应用优化规则直到达到固定点，将解析的逻辑计划转换成优化的逻辑计划。许多优化规则由 catalyst 实现。然而，catalyst 被设计成可以通过自定义规则轻松扩展。注意，当我们使用框架提供的逻辑操作符以声明的方式指定我们的查询时，操作符级别的优化变得内在可行。我们还可以为 SparkSQL 提供的所有功能编写用户定义的函数。然而，如果我们使用 Spark core，我们最终会得到完全相同程度的优化:也就是说，在级水平上的优化，因为级内部对催化剂是不透明的。</p><p id="8a9e" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">现在，有趣的问题是:我们如何在代表我们的查询的 AST 和 Spark 核心之间架起一座桥梁，SparkSQL 就是建立在 Spark 核心之上的？查询执行过程中还有另一个主要步骤:物理规划。这一步通过用物理运算符替换逻辑运算符，将逻辑计划转换为物理计划。它通过应用两组优化来做到这一点:基于规则和基于成本。</p><p id="9096" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">与逻辑优化类似，基于规则的优化应用一组规则将子树转换成逻辑上等价的子树。即使结果保持不变，规则也会影响产生结果的方式(例如流水线、谓词下推)。这一步类似于 Spark 核心中将逻辑计划转换成物理计划的转换步骤，它将狭窄的依赖关系汇集成阶段。相反，基于成本的优化取代了具有各种执行策略的操作符，比如连接。每种执行策略可能对性能有不同的影响。成本模型用于假设最有效的策略。</p><p id="c7e3" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">最后，优化后的 AST 的每个表达式都被转换成 scala 代码的 AST，用于计算表达式。在运行时，这些代码被编译成 Java 字节码，这些字节码将被发送给执行器。流水线操作符并把它们编译成一个函数，减少了每次需要计算 AST 表达式时必须解析它的开销，并减少了虚函数调用。解析和编译将这些步骤重构为一个只有一次的任务。生成的函数基本上实现了一个迭代器，它将表达式应用于分区的每个条目。</p><p id="49e3" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">在为流水线表达式生成代码之后，我们建立一个 RDDs 的逻辑计划。我们最终为每个生成的代码函数提供一个 RDD，为物理规划选择的每个连接提供一个 RDD。Spark core 将这个逻辑计划转换成由阶段和任务组成的物理执行计划。物理计划可以由集群简单地通过将任务调度给执行者来执行。</p><p id="5d5c" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">我们已经看到，使用 SparkSQL 这样的高级 API 的好处源于我们使用 Spark 提供的操作符编写声明性查询。Spark 理解查询的语义，并且可以通过用更有效的模式替换查询中的模式，在操作符级别进行优化。因此，如果我们编写用户定义的函数来表达我们的逻辑，我们禁止 catalyst 从逻辑上优化我们的查询，因为函数中编写的代码对优化器来说是不透明的。优化的查询在物理规划期间(通过流水线)被转换成 map/reduce(Spark 术语中的 map/shuffle)步骤的物理规划，这与 Spark 核心执行模型直接相关。每个阶段执行的函数在运行时被编译成 Java 字节码并发送给执行器。这减少了为每个记录再次解析表达式树的开销。</p></div></div>    
</body>
</html>