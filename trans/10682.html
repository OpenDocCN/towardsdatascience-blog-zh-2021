<html>
<head>
<title>Graph Neural Networks: A learning journey since 2008 — Deep Walk</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">图形神经网络:2008年以来的学习之旅——深度行走</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/graph-neural-networks-a-learning-journey-since-2008-deep-walk-e424e716070a?source=collection_archive---------23-----------------------#2021-10-13">https://towardsdatascience.com/graph-neural-networks-a-learning-journey-since-2008-deep-walk-e424e716070a?source=collection_archive---------23-----------------------#2021-10-13</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><p id="73d7" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">图形神经网络之旅的第三部分。今天，佩罗齐2014年论文《深度行走:社会表征的在线学习》的理论背景</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ko"><img src="../Images/8536d29f2bc067504e3242fb1f9eefd3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*r9XShdjCikjvKV1p1PmuXg.jpeg"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">图片由<a class="ae le" href="https://unsplash.com/@wsokolowskij" rel="noopener ugc nofollow" target="_blank"> Wladislaw Sokolowskij </a>在<a class="ae le" href="https://unsplash.com/photos/0vw4InAC-yM" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><div class="lf lg gp gr lh li"><a href="https://medium.com/@stefanobosisio1/membership" rel="noopener follow" target="_blank"><div class="lj ab fo"><div class="lk ab ll cl cj lm"><h2 class="bd iu gy z fp ln fr fs lo fu fw is bi translated">通过我的推荐链接加入Medium-Stefano Bosisio</h2><div class="lp l"><h3 class="bd b gy z fp ln fr fs lo fu fw dk translated">作为一个媒体会员，你的会员费的一部分会给你阅读的作家，你可以完全接触到每一个故事…</h3></div><div class="lq l"><p class="bd b dl z fp ln fr fs lo fu fw dk translated">medium.com</p></div></div><div class="lr l"><div class="ls l lt lu lv lr lw ky li"/></div></div></a></div><p id="2c89" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><a class="ae le" rel="noopener" target="_blank" href="/graph-neural-networks-a-learning-journey-since-2008-part-1-7df897834df9">在本系列的第一篇文章中，我们学习了图形神经网络模型是如何工作的</a>。我们看到，GNN返回基于节点和基于图形的预测，它有坚实的数学背景支持。特别地，转移函数和输出函数满足Banach不动点定理。然而，尽管成功的GNN应用，也有一些障碍，如解释[1]。GNN模型的主要思想是建立状态转换，函数<em class="lx"> f𝓌 </em>和<em class="lx"> g𝓌 </em>，并迭代直到这些函数收敛在一个阈值内。这是一个强约束，可能会限制模型的可扩展性和表示能力。其次，GNN不能利用<em class="lx">表示学习</em>，即如何从低维特征向量中表示图形。第三，GNN基于迭代学习过程，其中标签是混合的特征。这种混合可能会导致一些级联错误，如[6]所示</p><p id="58aa" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">为了解决这些问题，DeepWalk [2]在2014年出现，作为GNN的一个可能的增强。DeepWalk是第一个实现<em class="lx">嵌入方法</em>的图，它基于表示学习和语言建模——skip gram模型[3]</p><h1 id="ff37" class="ly lz it bd ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu mv bi translated">深度行走模型</h1><h2 id="f352" class="mw lz it bd ma mx my dn me mz na dp mi kb nb nc mm kf nd ne mq kj nf ng mu nh bi translated">定义</h2><p id="d480" class="pw-post-body-paragraph jq jr it js b jt ni jv jw jx nj jz ka kb nk kd ke kf nl kh ki kj nm kl km kn im bi translated">DeepWalk通过对一系列短距离随机行走建模来学习图顶点的社会表现。<em class="lx">社会表征</em>是跨越图形顶点的隐藏连接，可以解码为潜在特征。因此，DeepWalk读取输入图形并输出图形的潜在表示。</p><p id="5093" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">如何获得图形的潜在表示？嗯，<strong class="js iu">这里我们有与斯卡塞利的方法</strong>的主要区别。斯卡塞利的GNN是一种有监督的方法，而DeepWalk是一种无监督的方法，因此它必须从图的特征中学习，以理解和捕捉图的结构和目标，独立于节点或图的标签。这种映射方法已被证明比其他社会维度分类器更有效[4，5]，将微观F1分数的分类性能提高了5-10%，甚至在训练数据少60%的情况下也优于“竞争对手”。最后，DeepWalk框架允许简单的可伸缩性，以处理网络规模的图表，如脸书，Twitter或Youtube。</p><h2 id="1f60" class="mw lz it bd ma mx my dn me mz na dp mi kb nb nc mm kf nd ne mq kj nf ng mu nh bi translated">数学洞察I:定义</h2><p id="8293" class="pw-post-body-paragraph jq jr it js b jt ni jv jw jx nj jz ka kb nk kd ke kf nl kh ki kj nm kl km kn im bi translated">DeepWalk基于节点的社会属性返回输入图的有洞察力的潜在表示。图定义为数学函数<em class="lx"> G=(V，E) </em>，是由<em class="lx"> V </em>顶点和<em class="lx"> E </em>边组成的<em class="lx"> </em>。顶点可以部分标记，<em class="lx"> Y标记，</em>并由属性/特征<em class="lx"> X描述。</em>属性/特征是实数域的一部分，其维数为<em class="lx"> S，</em>和<em class="lx"> </em>标记由实数向量定义。</p><p id="b6c4" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">给定输入真实域，DeepWalk将节点的特征<em class="lx"> X </em>转换为潜在域，其维度为<em class="lx"> d. </em>这种映射描述了具有较小特征子集的社会现象，这有助于更好地理解底层数据的关系，并在摄取到PCA或t-SNE算法中时具有即时的视觉答案。</p><p id="14ef" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">为了理解DeepWalk的理论和过程，在处理实际实现之前，<a class="ae le" rel="noopener" target="_blank" href="/graph-neural-networks-a-learning-journey-since-2008-part-2-22dbf7a3b0d?source=user_profile---------1----------------------------">我在这里提供了一个空手道俱乐部图的例子</a>。图1示出了输入图G，其中顶点是34，边是78。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi nn"><img src="../Images/4fb873c744e39754d9dd6123aef3066f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cAMivP0vnBrgUaq-G8PUZg.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">图1输入空手道俱乐部图表</p></figure><p id="c71e" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">输入特征矩阵<em class="lx"> X </em>描述了节点之间的关系，我们希望使用DeepWalk找到图形的潜在表示，以找到俱乐部中的最佳分组。例如，潜在表示维度可以是10，因此DeepWalk将产生10个新特征来将图细分成组。</p><h2 id="b393" class="mw lz it bd ma mx my dn me mz na dp mi kb nb nc mm kf nd ne mq kj nf ng mu nh bi translated">数学洞察II:随机漫步的产生</h2><p id="887e" class="pw-post-body-paragraph jq jr it js b jt ni jv jw jx nj jz ka kb nk kd ke kf nl kh ki kj nm kl km kn im bi translated">为了实现潜在表示，DeepWalk开始从输入图创建γ(例如100)个随机行走。随机游走<em class="lx"> Wvᵢ </em>从顶点<em class="lx"> vᵢ </em>开始，随机选择<em class="lx"> k </em>(例如10个)其他图的顶点<em class="lx"> vₖ </em>(例如见图2)</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi no"><img src="../Images/75a1c825c0ece8f22ad0fd66ad142846.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5HQfyMmlew5tVDXfSDgEtw.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">图2:顶点22和31的截断随机行走的例子。随机漫步允许轻松地同时探索多个图形区域</p></figure><p id="c98c" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">随机漫步的选择允许算法从网络中提取信息，一方面保证计算的容易并行化，另一方面保证探索图形的动态方式，一旦图形改变，可以封装新的信息。后一方面允许从单个图子集迭代更新模型训练，而没有整个图计算的问题。</p><h2 id="4b19" class="mw lz it bd ma mx my dn me mz na dp mi kb nb nc mm kf nd ne mq kj nf ng mu nh bi translated">数学洞察III:语言建模</h2><p id="0d27" class="pw-post-body-paragraph jq jr it js b jt ni jv jw jx nj jz ka kb nk kd ke kf nl kh ki kj nm kl km kn im bi translated">一旦收集了γ(例如100)“截断的”随机游走，就有可能利用一些语言建模来提取输入社会表征的潜在描述。特别地，随机漫步的集合可以被看作是由<em class="lx"> k </em>单词<em class="lx"> w </em> : <em class="lx"> Wvᵢ = (w₀，w₁，…wₖ).)组成的句子的编码集合(图3显示了10个随机漫步的例子)从这里，DeepWalk计算所有输入语料库G </em>的单词嵌入或节点嵌入表示。这是一个值得注意的点，因为这是在图形世界中第一次有人写节点嵌入的概念。</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="np nq l"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">图3:从顶点22、13、31、16、10、5、17、16、24和18开始的10次随机漫步。</p></figure><p id="d2fc" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">单词嵌入可以由一个函数φ来描述，该函数的值位于实数域中。φ是一个矩阵，它的维数是顶点数<em class="lx"> V </em>和潜在维数<em class="lx"> d </em>(在这个例子中是34 x 10)。目标是在给定所有先前访问过的顶点嵌入的情况下，检索观察到顶点<em class="lx"> vᵢ </em>的可能性:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/468d269c92379ff18828f76643e3325b.png" data-original-src="https://miro.medium.com/v2/resize:fit:606/format:webp/1*rSAYEGTCVzof7scfg2m4eQ.png"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">等式1:给定所有先前的顶点嵌入，顶点vi被观察到的可能性</p></figure><p id="26e6" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">然而，这种计算取决于图的维数和随机游走的长度，因为它们在增长。等式1是计算性的，效率不高。在[7] [8]中提出了一个解决方案，其中问题是相反的。如图4所示，我们将计算一个单词在给定上下文中作为邻居出现的可能性，而不是预测一个句子中缺失单词的出现</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ns"><img src="../Images/17ef4ba3405dfdcc453f31b9ce4bdaf6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cyPOFVS0J5rBDq4GLHnpgw.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">图SkipGram方法的直观示例。给定一个句子，我们希望预测一个单词与给定单词相邻的可能性(例如walk)。</p></figure><p id="0b35" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">这种方法称为skip gram[7–11]，旨在解决这一优化问题:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/4313df257ccdce4573448889c05d915c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1196/format:webp/1*mkr_KU1Xy50ZvQFgHzx4eg.png"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">等式SkipGram的优化问题。目的是在给定单词v_i的映射的情况下，通过估计上下文单词的对数似然来最小化嵌入表示</p></figure><p id="a2f6" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">从等式2可以描述d维潜在表示中的顶点，使得相似的顶点具有相似的表示。</p><h2 id="be51" class="mw lz it bd ma mx my dn me mz na dp mi kb nb nc mm kf nd ne mq kj nf ng mu nh bi translated">数学洞察IV:分层Softmax</h2><p id="0d46" class="pw-post-body-paragraph jq jr it js b jt ni jv jw jx nj jz ka kb nk kd ke kf nl kh ki kj nm kl km kn im bi translated">仔细观察SkipGram算法，我们可以获得以下“成分”:</p><ul class=""><li id="bc4c" class="nu nv it js b jt ju jx jy kb nw kf nx kj ny kn nz oa ob oc bi translated">尺寸为<em class="lx"> w </em>的窗口滑过输入序列</li><li id="cd19" class="nu nv it js b jt od jx oe kb of kf og kj oh kn nz oa ob oc bi translated">单词嵌入潜在维度<em class="lx"> d </em></li><li id="0ad5" class="nu nv it js b jt od jx oe kb of kf og kj oh kn nz oa ob oc bi translated">一个2层神经网络，接收输入语料库</li><li id="954a" class="nu nv it js b jt od jx oe kb of kf og kj oh kn nz oa ob oc bi translated">计算单词概率分布的激活层</li></ul><p id="f1fd" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">对于给定随机漫步中的每个顶点<em class="lx"> vᵢ </em>，我们有一个表示向量<em class="lx">φ(vᵢ</em>。给定这个顶点表示，我们希望在随机行走中最大化它的邻居的概率(等式2)。这样的后验分布可以通过几种算法来学习。在DeepWalk中，该模型使用分层的Softmax方法[12，13]，这减轻了计算成本并加快了训练时间，从而逼近概率分布。计算后验概率可能会导致巨大的工作量，这等于图中顶点的数量(在某些情况下高达数十亿)。</p><p id="b000" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">分级Softmax基于一个二叉树，即霍夫曼树/编码，它确保了属于任何树节点每一侧的点的分布之间的平衡。来自等式2的顶点<em class="lx"> vᵢ </em>是根节点。从那里，我们可以从随机漫步到最后一个顶点创建分支。最终的后验概率是通过从词根开始遍历树来计算我们遇到的每个单词的概率之和。</p><h2 id="95da" class="mw lz it bd ma mx my dn me mz na dp mi kb nb nc mm kf nd ne mq kj nf ng mu nh bi translated">最后备注:想象嵌入</h2><p id="0018" class="pw-post-body-paragraph jq jr it js b jt ni jv jw jx nj jz ka kb nk kd ke kf nl kh ki kj nm kl km kn im bi translated">最后会有什么？一旦DeepWalk已经运行，Skipgram <code class="fe oi oj ok ol b">Word2Vec</code>模型将返回我们的输入图的潜在空间描述，即一个图嵌入文件:</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="np nq l"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">图5:空手道俱乐部图的最终嵌入示例。数字34和10是图的节点数和潜在表示维度。对于每个顶点节点(每行的第一个数字，例如34，1，33，3，2)，我们可以找到相关的潜在表示坐标</p></figure><p id="8053" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">对于每个顶点，该文件报告模型的嵌入，即顶点在潜在表示空间中的位置。我们如何使用这种表示法？嗯，我们可以使用至少两种分解算法来获取这样一个空间的“人类”表示。第一个是<a class="ae le" href="https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding" rel="noopener ugc nofollow" target="_blank"> t-SNE方法</a>【14】，第二个是<a class="ae le" href="https://en.wikipedia.org/wiki/Principal_component_analysis" rel="noopener ugc nofollow" target="_blank">主成分分析(PCA) </a>。图6显示了空手道俱乐部输出嵌入的PCA分解。正如你所看到的，根据输入特征/节点间的关系，DepeWalk可以检索至少3个空手道俱乐部细分的主要组。这使得人们可以分析巨大的图表，例如脸书网页，了解网页之间的社会关系，而不用担心处理表示和标签。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi om"><img src="../Images/81ab097af4b9b013be2208685cd8c15b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cs9eq-eY7HHNpblCbDqvpg.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">图6:空手道俱乐部深走嵌入的PCA 2维表示</p></figure><p id="def1" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">请继续关注我们的下一篇文章:DeepWalk的两个实际应用，以全面了解Python实现:)</p></div><div class="ab cl on oo hx op" role="separator"><span class="oq bw bk or os ot"/><span class="oq bw bk or os ot"/><span class="oq bw bk or os"/></div><div class="im in io ip iq"><p id="f7b4" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">如果有任何问题或意见，请随时给我发电子邮件，地址是:stefanobosisio1@gmail.com，或者直接在Medium这里。</p><h1 id="9987" class="ly lz it bd ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu mv bi translated">文献学</h1><ol class=""><li id="2646" class="nu nv it js b jt ni jx nj kb ou kf ov kj ow kn ox oa ob oc bi translated">周，解，等，“图形神经网络:方法与应用综述”<em class="lx">艾公开赛</em>1(2020):57–81。</li><li id="c638" class="nu nv it js b jt od jx oe kb of kf og kj oh kn ox oa ob oc bi translated">佩罗齐、布莱恩、拉米·艾尔弗和史蒂文·斯基纳。"深度行走:社交表征的在线学习."<em class="lx">第20届ACM SIGKDD知识发现和数据挖掘国际会议论文集</em>。2014.</li><li id="94ea" class="nu nv it js b jt od jx oe kb of kf og kj oh kn ox oa ob oc bi translated">《向量空间中单词表征的有效估计》arXiv预印本arXiv:1301.3781  (2013)</li><li id="63c2" class="nu nv it js b jt od jx oe kb of kf og kj oh kn ox oa ob oc bi translated">唐、雷、。"通过潜在社会维度的关系学习."<em class="lx">第15届ACM SIGKDD知识发现和数据挖掘国际会议论文集</em>。2009.</li><li id="6fe1" class="nu nv it js b jt od jx oe kb of kf og kj oh kn ox oa ob oc bi translated">唐、雷、。“利用社交媒体网络进行分类。”<em class="lx">数据挖掘和知识发现</em>23.3(2011):447–478。</li><li id="13c8" class="nu nv it js b jt od jx oe kb of kf og kj oh kn ox oa ob oc bi translated">内维尔，詹妮弗和大卫·延森。"使用集体推理的模型偏差/方差分解."<em class="lx">机器学习</em>73.1(2008):87–106。</li><li id="b24f" class="nu nv it js b jt od jx oe kb of kf og kj oh kn ox oa ob oc bi translated">《向量空间中单词表征的有效估计》<em class="lx"> arXiv预印本arXiv:1301.3781 </em> (2013)。</li><li id="99ce" class="nu nv it js b jt od jx oe kb of kf og kj oh kn ox oa ob oc bi translated">单词和短语的分布式表征及其组合性。<em class="lx">神经信息处理系统的进展</em>。2013.</li><li id="ba47" class="nu nv it js b jt od jx oe kb of kf og kj oh kn ox oa ob oc bi translated">大卫·格思里等着，〈跳过语法建模的更近距离观察〉。<em class="lx"> LREC </em>。第六卷。2006.</li><li id="85e2" class="nu nv it js b jt od jx oe kb of kf og kj oh kn ox oa ob oc bi translated">Gittens，Alex，Dimitris Achlioptas和Michael W. Mahoney。" Skip-Gram Zipf+Uniform =矢量可加性."计算语言学协会第55届年会会议录(第一卷:长篇论文)。2017.</li><li id="6ecc" class="nu nv it js b jt od jx oe kb of kf og kj oh kn ox oa ob oc bi translated">明诺、大卫和劳雷·汤普森。"负抽样跳跃图的奇异几何."<em class="lx">自然语言处理中的经验方法</em>。2017.</li><li id="de7e" class="nu nv it js b jt od jx oe kb of kf og kj oh kn ox oa ob oc bi translated">Mnih，Andriy和Geoffrey E. Hinton。"一个可扩展的分层分布式语言模型."<em class="lx">神经信息处理系统进展</em>21(2008):1081–1088。</li><li id="513b" class="nu nv it js b jt od jx oe kb of kf og kj oh kn ox oa ob oc bi translated">莫林，弗雷德里克，还有约舒阿·本吉奥。"分层概率神经网络语言模型."人工智能与统计国际研讨会。PMLR，2005年。</li><li id="e66c" class="nu nv it js b jt od jx oe kb of kf og kj oh kn ox oa ob oc bi translated">辛顿、杰弗里和萨姆·t·罗伊斯。“随机邻居嵌入。”<em class="lx">钳口</em>。第15卷。2002.</li></ol></div></div>    
</body>
</html>