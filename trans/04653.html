<html>
<head>
<title>Derivative of the Softmax Function and the Categorical Cross-Entropy Loss</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Softmax函数的导数和分类交叉熵损失</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/derivative-of-the-softmax-function-and-the-categorical-cross-entropy-loss-ffceefc081d1?source=collection_archive---------0-----------------------#2021-04-22">https://towardsdatascience.com/derivative-of-the-softmax-function-and-the-categorical-cross-entropy-loss-ffceefc081d1?source=collection_archive---------0-----------------------#2021-04-22</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="f83a" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">简单快速的推导</h2></div><p id="f4e6" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在这篇短文中，我们将计算softmax函数的雅可比矩阵。通过应用一个优雅的计算技巧，我们将使推导非常短。使用获得的雅可比矩阵，我们将计算分类交叉熵损失的梯度。</p><h1 id="9b33" class="lb lc iq bd ld le lf lg lh li lj lk ll jw lm jx ln jz lo ka lp kc lq kd lr ls bi translated">Softmax函数</h1><p id="f1fa" class="pw-post-body-paragraph kf kg iq kh b ki lt jr kk kl lu ju kn ko lv kq kr ks lw ku kv kw lx ky kz la ij bi translated">softmax函数的主要用途是获取任意实数的向量，并将其转换为概率:</p><figure class="lz ma mb mc gt md gh gi paragraph-image"><div role="button" tabindex="0" class="me mf di mg bf mh"><div class="gh gi ly"><img src="../Images/efbb750cabb715fefcd1dbc9d2143dd6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gctBX5YHUUpBEK3MWD6r3Q.png"/></div></div><p class="mk ml gj gh gi mm mn bd b be z dk translated">(图片由作者提供)</p></figure><p id="fc64" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">上面公式中的指数函数确保获得的值是非负的。由于分母中的归一化项，获得的值总和为1。此外，所有值都在0和1之间。softmax函数的一个重要属性是它保留了其输入值的等级顺序:</p><figure class="lz ma mb mc gt md gh gi paragraph-image"><div class="gh gi mo"><img src="../Images/f11eecffffd3cf6062a062932b52f7ba.png" data-original-src="https://miro.medium.com/v2/resize:fit:1008/format:webp/1*WKZXuzkLucnnvCOyv35oEA.png"/></div></figure><h1 id="e1d6" class="lb lc iq bd ld le lf lg lh li lj lk ll jw lm jx ln jz lo ka lp kc lq kd lr ls bi translated">Softmax函数的雅可比矩阵</h1><p id="27e5" class="pw-post-body-paragraph kf kg iq kh b ki lt jr kk kl lu ju kn ko lv kq kr ks lw ku kv kw lx ky kz la ij bi translated">形式上，softmax函数是所谓的<em class="mp">矢量函数</em>，它将一个矢量作为输入，并产生一个矢量作为输出:</p><figure class="lz ma mb mc gt md gh gi paragraph-image"><div class="gh gi mq"><img src="../Images/0a4e02b4602ec70e0e47a5fc3e92acae.png" data-original-src="https://miro.medium.com/v2/resize:fit:334/format:webp/1*1YCKjMAJfrxmpNA3vkRJeg.png"/></div></figure><p id="8a17" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">所以，我们在讲softmax函数的导数时，其实讲的是它的雅可比矩阵，雅可比矩阵是所有一阶偏导数的矩阵:</p><figure class="lz ma mb mc gt md gh gi paragraph-image"><div class="gh gi mr"><img src="../Images/1cb90832bbc2fc8a62f91a64656dca13.png" data-original-src="https://miro.medium.com/v2/resize:fit:580/format:webp/1*4vjr6rqpDJ-x7JN2w_9iWw.png"/></div></figure><p id="b083" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在哪里</p><figure class="lz ma mb mc gt md gh gi paragraph-image"><div class="gh gi ms"><img src="../Images/e4d0170031743a5de45124f93f367802.png" data-original-src="https://miro.medium.com/v2/resize:fit:504/format:webp/1*z83XR7-ZAhquC2R-XlBk_A.png"/></div></figure><p id="071e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">请注意，softmax函数的每个输出如何依赖于所有输入值(由于分母)。由于这个原因，雅可比矩阵的非对角线元素不为零。</p><p id="19c6" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">由于softmax函数的输出是严格的正值，因此我们可以通过应用以下技巧来使下面的求导非常简短:我们不求输出的偏导数，而是求输出的对数的偏导数(也称为“对数导数”):</p><figure class="lz ma mb mc gt md gh gi paragraph-image"><div class="gh gi mt"><img src="../Images/091b441f8c14caf50a5dfe92504056df.png" data-original-src="https://miro.medium.com/v2/resize:fit:354/format:webp/1*7nIDZEK411ch0tNCVieWwA.png"/></div></figure><p id="3124" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">其中右边的表达式直接来自链式法则。接下来，我们重新排列上面的公式，得到:</p><figure class="lz ma mb mc gt md gh gi paragraph-image"><div class="gh gi mu"><img src="../Images/d6de69e25d437556ddd452aa8f17e261.png" data-original-src="https://miro.medium.com/v2/resize:fit:360/format:webp/1*3G3qUxiDPI6L1D8WXzQ2vQ.png"/></div></figure><p id="7f45" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">左手边正是我们要找的偏导数。我们很快就会看到，右边简化了导数的计算，因此我们不需要导数的商法则。我们必须首先取𝑠:的对数</p><figure class="lz ma mb mc gt md gh gi paragraph-image"><div class="gh gi mv"><img src="../Images/615d9c850c9881ba1364013799b73ce4.png" data-original-src="https://miro.medium.com/v2/resize:fit:718/format:webp/1*HaJ1TWd0ePxJy9REeBh_2w.png"/></div></figure><p id="bd8c" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">所得表达式的偏导数为:</p><figure class="lz ma mb mc gt md gh gi paragraph-image"><div class="gh gi mr"><img src="../Images/2484ac5b32f0b9df12111cda22813900.png" data-original-src="https://miro.medium.com/v2/resize:fit:580/format:webp/1*5Sa6igwzpsK569BZelpw0w.png"/></div></figure><p id="ab42" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">让我们看看右边的第一项:</p><figure class="lz ma mb mc gt md gh gi paragraph-image"><div class="gh gi mw"><img src="../Images/a620ebea2cfcd6f1d7fe0ed2f88aac96.png" data-original-src="https://miro.medium.com/v2/resize:fit:370/format:webp/1*lA6oaKNSRSD4TeL5G3uK-Q.png"/></div></figure><p id="4914" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">可以使用指示符函数1{ }简明地写出。如果indicator函数的参数为真，则该函数的值为1，否则为0。</p><p id="a8be" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">右边的第二项可以通过应用链式法则来计算:</p><figure class="lz ma mb mc gt md gh gi paragraph-image"><div class="gh gi mx"><img src="../Images/791a2edf2568ca4bea10d6320fbda7a8.png" data-original-src="https://miro.medium.com/v2/resize:fit:728/format:webp/1*SUHB2ZO6-0NXwUST-xZdGA.png"/></div></figure><p id="1abc" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在上面的步骤中，我们使用了自然对数的导数:</p><figure class="lz ma mb mc gt md gh gi paragraph-image"><div class="gh gi my"><img src="../Images/d375d9f17c763ab3b5a31e910a53f0f0.png" data-original-src="https://miro.medium.com/v2/resize:fit:256/format:webp/1*WrW9DwuC62wYDE90fsvUBw.png"/></div></figure><p id="2eaf" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">获得和的偏导数是很简单的:</p><figure class="lz ma mb mc gt md gh gi paragraph-image"><div class="gh gi mz"><img src="../Images/7311e594f6d6c7dca8c9a3f26a29ba51.png" data-original-src="https://miro.medium.com/v2/resize:fit:1078/format:webp/1*8LXoBnWXBaRA-_LQkwDRTw.png"/></div></figure><p id="c60c" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">将结果代入公式得出:</p><figure class="lz ma mb mc gt md gh gi paragraph-image"><div class="gh gi na"><img src="../Images/b147ec7faf9cb081dc55d85296bde232.png" data-original-src="https://miro.medium.com/v2/resize:fit:758/format:webp/1*Xn0tGN_h0sFQypzzgIMLqA.png"/></div></figure><p id="a746" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">最后，我们必须用𝑠乘上表达式，如本节开头所示:</p><figure class="lz ma mb mc gt md gh gi paragraph-image"><div class="gh gi nb"><img src="../Images/391e6e9809ed9813eea341fe9185f67b.png" data-original-src="https://miro.medium.com/v2/resize:fit:686/format:webp/1*PZgbRJC5CprX8unqknlrdA.png"/></div></figure><p id="4c12" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们的推导到此结束。我们已经得到了雅可比矩阵所有元素的公式(对角的和非对角的)。对于𝑛 = 4的特殊情况，我们得到:</p><figure class="lz ma mb mc gt md gh gi paragraph-image"><div class="gh gi nc"><img src="../Images/3cd81a6f3bf3200a92c7106fd39a8211.png" data-original-src="https://miro.medium.com/v2/resize:fit:1108/format:webp/1*SWfgFQLDIPXDf1C6CHmr8A.png"/></div></figure><p id="bf5c" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">看看对角线元素与非对角线元素有何不同。</p><h1 id="dd45" class="lb lc iq bd ld le lf lg lh li lj lk ll jw lm jx ln jz lo ka lp kc lq kd lr ls bi translated">分类交叉熵损失</h1><p id="8ad4" class="pw-post-body-paragraph kf kg iq kh b ki lt jr kk kl lu ju kn ko lv kq kr ks lw ku kv kw lx ky kz la ij bi translated">分类交叉熵损失与softmax函数密切相关，因为它实际上仅用于输出处有softmax图层的网络。在我们正式引入分类交叉熵损失(通常也称为softmax损失)之前，我们必须先澄清两个术语:<strong class="kh ir">多类分类</strong>和<strong class="kh ir">交叉熵</strong>。</p><p id="d286" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">分类问题可以细分为以下两类:</p><ul class=""><li id="39c7" class="nd ne iq kh b ki kj kl km ko nf ks ng kw nh la ni nj nk nl bi translated"><strong class="kh ir">多类别分类</strong>，每个样本只属于一个类别(互斥)</li><li id="6701" class="nd ne iq kh b ki nm kl nn ko no ks np kw nq la ni nj nk nl bi translated"><strong class="kh ir">多标签分类</strong>，每个样本可能属于多个类别(或不属于任何类别)</li></ul><p id="db79" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">类别交叉熵损失专门用于多类分类任务，其中每个样本恰好属于<em class="mp"> 𝙲 </em>类之一。因此，分配给每个样本的真实标签由0和<em class="mp"> 𝙲 </em> -1之间的单个整数值组成。标签可以由大小为<em class="mp"> 𝙲 </em>的独热编码向量来表示，对于正确的类，该向量的值为1，而在其他地方为0，参见下面的示例，其中<em class="mp"> 𝙲 </em> = 4:</p><figure class="lz ma mb mc gt md gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/a131425f225227f56133411951f3cddd.png" data-original-src="https://miro.medium.com/v2/resize:fit:384/format:webp/1*ZZnxZONE_IdhqaAOS5_iiw.png"/></div></figure><p id="bbf6" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">交叉熵</strong>将两个离散的概率分布(简单地，其元素位于0，..，1和sum to 1)并输出单个实值(！)表示两种概率分布相似性的数字:</p><figure class="lz ma mb mc gt md gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/0713000d70f9fc3da163f4523ad0a61b.png" data-original-src="https://miro.medium.com/v2/resize:fit:440/format:webp/1*gTCyYHl7vZwyrYaq2JPxGA.png"/></div></figure><p id="5cf9" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">其中<em class="mp"> 𝙲 </em>表示不同类别的数量，下标𝑖表示向量的𝑖-th元素。交叉熵越小，两个概率分布越相似。</p><p id="db56" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">当交叉熵被用作多类分类任务中的损失函数时，𝒚被馈送一个热码编码标签，并且由softmax层生成的概率被放入<strong class="kh ir"> 𝑠 </strong>。这样我们就不会取零的对数，因为数学上softmax永远不会真正产生零值。</p><p id="4ebe" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">通过最小化训练期间的损失，我们基本上迫使预测的概率逐渐类似于真正的独热编码向量。</p><figure class="lz ma mb mc gt md gh gi paragraph-image"><div role="button" tabindex="0" class="me mf di mg bf mh"><div class="gh gi nt"><img src="../Images/146bb5dfb2bb873abdf645799e92ec45.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8iL_I-Mvg95zWXu0gs7d_A.png"/></div></div><p class="mk ml gj gh gi mm mn bd b be z dk translated">(图片由作者提供)</p></figure><p id="dd58" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">为了启动反向传播过程，如本文<a class="ae nu" rel="noopener" target="_blank" href="/deriving-the-backpropagation-equations-from-scratch-part-1-343b300c585a">帖子</a>所述，我们必须计算损耗w.r.t对输出层的<em class="mp">加权输入</em> 𝑧的导数，见上图:</p><figure class="lz ma mb mc gt md gh gi paragraph-image"><div class="gh gi nv"><img src="../Images/ca9ad1459819a74b46406970bbb63b88.png" data-original-src="https://miro.medium.com/v2/resize:fit:1062/format:webp/1*FJDgxynEJ2aMH3GvGd448Q.png"/></div></figure><p id="3148" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">让我们代入上一节得到的导数:</p><figure class="lz ma mb mc gt md gh gi paragraph-image"><div class="gh gi nw"><img src="../Images/9194296d477c8bca78eeedaca3267a9e.png" data-original-src="https://miro.medium.com/v2/resize:fit:980/format:webp/1*-xT4RVc-yBKUe3W3DgvlQw.png"/></div></figure><p id="c492" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">并在最后一项中扩展产品:</p><figure class="lz ma mb mc gt md gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/b09afdaee21b021f8b96c04b6db86094.png" data-original-src="https://miro.medium.com/v2/resize:fit:568/format:webp/1*CW_lWgMZxntLJ3jNny3uTg.png"/></div></figure><p id="3e83" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">对于𝑖 = 𝑗，指示符函数1 { 0 }取值为1，其他地方取值为0:</p><figure class="lz ma mb mc gt md gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/1df884ceabc337d67a6037d19135d969.png" data-original-src="https://miro.medium.com/v2/resize:fit:352/format:webp/1*IJoNmaFG4qveIjLzZ5ew4Q.png"/></div></figure><p id="c753" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">接下来，我们将𝑠从总和中剔除，因为它不依赖于𝑖:指数</p><figure class="lz ma mb mc gt md gh gi paragraph-image"><div class="gh gi ms"><img src="../Images/6013b224eef1d5c188d50c562f79ec2e.png" data-original-src="https://miro.medium.com/v2/resize:fit:504/format:webp/1*HCdzsNKMgtvGO0wtQBNEcA.png"/></div></figure><p id="7d64" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在最后一步中，我们使用了这样一个事实，即独热编码向量𝒚和为1。请记住，一个独热编码向量可以被解释为一个概率分布，其概率质量以单个值为中心。在简明的矢量符号中，我们得到:</p><figure class="lz ma mb mc gt md gh gi paragraph-image"><div class="gh gi nz"><img src="../Images/97a8e230f58259b4930b938c94047bda.png" data-original-src="https://miro.medium.com/v2/resize:fit:210/format:webp/1*oywcdZSuj7S3YOD2o4_CZQ.png"/></div></figure><h1 id="d254" class="lb lc iq bd ld le lf lg lh li lj lk ll jw lm jx ln jz lo ka lp kc lq kd lr ls bi translated">更多阅读</h1><p id="be1d" class="pw-post-body-paragraph kf kg iq kh b ki lt jr kk kl lu ju kn ko lv kq kr ks lw ku kv kw lx ky kz la ij bi translated"><a class="ae nu" href="https://eli.thegreenplace.net/2016/the-softmax-function-and-its-derivative/" rel="noopener ugc nofollow" target="_blank">伊莱·本德斯基的网站</a> <br/> <a class="ae nu" href="https://gombru.github.io/2018/05/23/cross_entropy_loss/" rel="noopener ugc nofollow" target="_blank">劳尔·戈麦斯的博客</a></p></div></div>    
</body>
</html>