<html>
<head>
<title>Understand Support Vector Machines</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">理解支持向量机</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/understand-support-vector-machines-5bfe800a0e03?source=collection_archive---------17-----------------------#2021-04-01">https://towardsdatascience.com/understand-support-vector-machines-5bfe800a0e03?source=collection_archive---------17-----------------------#2021-04-01</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="b015" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">一个初学者友好的SVM方程推导与直观的解释</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ki"><img src="../Images/b8e8e434e1f3316e46b3e2d3ad55da07.png" data-original-src="https://miro.medium.com/v2/resize:fit:1180/1*83sCfQfEj9YxrxlSrvOW4w.gif"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">正交投影给出了我们从一个点到一个平面的最短距离，就像我们在日常生活中所习惯的一样(图片由作者提供)</p></figure><blockquote class="ku"><p id="befc" class="kv kw it bd kx ky kz la lb lc ld le dk translated">"真正的知识来自对一个主题及其内部运作的深刻理解."[人名]阿尔伯特·爱因斯坦(犹太裔理论物理学家)</p></blockquote></div><div class="ab cl lf lg hx lh" role="separator"><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk"/></div><div class="im in io ip iq"><h1 id="627e" class="lm ln it bd lo lp lq lr ls lt lu lv lw jz lx ka ly kc lz kd ma kf mb kg mc md bi translated">为什么我要费心学习SVM背后的数学？</h1><h2 id="59ef" class="me ln it bd lo mf mg dn ls mh mi dp lw mj mk ml ly mm mn mo ma mp mq mr mc ms bi translated">有许多可用的库可以帮助我使用SVM，而不必担心底层的概念。</h2><p id="81c1" class="pw-post-body-paragraph mt mu it mv b mw mx ju my mz na jx nb mj nc nd ne mm nf ng nh mp ni nj nk le im bi translated">当然可以！你完全正确。你不需要理解SVM就能使用它。您甚至可以应用SVM来解决分类问题，而无需理解其背后的基本概念。有大量的工具随时可用。再也没有人在现实世界的项目中从头开始实现SVM了。</p><blockquote class="ku"><p id="be31" class="kv kw it bd kx ky nl nm nn no np le dk translated">然而，如果你是一个有抱负的数据科学家，你应该花一些时间学习这些基本概念，以了解各种代数和几何参数如何最终成为一种算法。</p></blockquote><p id="e5fe" class="pw-post-body-paragraph mt mu it mv b mw nq ju my mz nr jx nb mj ns nd ne mm nt ng nh mp nu nj nk le im bi translated">学习或遵循每个算法的本质是不可能的，但有一些绝对基本的概念，任何认真的从业者都应该接触到。这样的曝光将新手和熟练者区分开来。</p><p id="2f98" class="pw-post-body-paragraph mt mu it mv b mw nv ju my mz nw jx nb mj nx nd ne mm ny ng nh mp nz nj nk le im bi translated">SVM只是一个工具。就像锤子是木匠的工具一样。然而，拥有锤子并不会让每个人都成为有用的木匠。同样，把访问SVM作为一种工具并不一定会让你成为<em class="oa">有用的</em>数据科学家。</p><p id="31be" class="pw-post-body-paragraph mt mu it mv b mw nv ju my mz nw jx nb mj nx nd ne mm ny ng nh mp nz nj nk le im bi translated">数据科学既是一门艺术，也是一门科学。作为一名数据科学家，如果你想精通你的技术，那么我建议你必须花一些时间去理解一些精选的、众所周知的算法的数学和几何论证。</p><blockquote class="ob oc od"><p id="bc0a" class="mt mu oa mv b mw nv ju my mz nw jx nb oe nx nd ne of ny ng nh og nz nj nk le im bi translated">这正是这篇文章的目的:使用高中数学和几何推导SVM方程，并使用直观和易于理解的论点！</p></blockquote><p id="aabf" class="pw-post-body-paragraph mt mu it mv b mw nv ju my mz nw jx nb mj nx nd ne mm ny ng nh mp nz nj nk le im bi translated">SVM是理解监督机器学习的几何观点和巩固向量代数在机器学习中的使用的理想算法。让我们开始吧！</p></div><div class="ab cl lf lg hx lh" role="separator"><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk"/></div><div class="im in io ip iq"><blockquote class="ku"><p id="dd01" class="kv kw it bd kx ky nl nm nn no np le dk translated">如果你能仔细阅读这篇文章，那么你应该最终对SVM背后的数学有一个清晰的理解，并且(希望)认识到SVM数学只是看起来吓人，但实际上非常简单！</p></blockquote><p id="c286" class="pw-post-body-paragraph mt mu it mv b mw nq ju my mz nr jx nb mj ns nd ne mm nt ng nh mp nu nj nk le im bi translated">在以前的文章中，我已经给出了一个非常温和的介绍，仅提供几何直观。如果你不太熟悉SVM的基本概念(或者对SVM与逻辑回归的区别有点生疏)，那么我建议你先读一读我以前关于这个主题的文章。</p><div class="oh oi gp gr oj ok"><a rel="noopener follow" target="_blank" href="/understand-support-vector-machines-6cc9e4a15e7e"><div class="ol ab fo"><div class="om ab on cl cj oo"><h2 class="bd iu gy z fp op fr fs oq fu fw is bi translated">理解支持向量机</h2><div class="or l"><h3 class="bd b gy z fp op fr fs oq fu fw dk translated">5个系列的第1部分，提供全面介绍的简短文章；不假设任何先验知识</h3></div><div class="os l"><p class="bd b dl z fp op fr fs oq fu fw dk translated">towardsdatascience.com</p></div></div><div class="ot l"><div class="ou l ov ow ox ot oy ko ok"/></div></div></a></div></div><div class="ab cl lf lg hx lh" role="separator"><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk"/></div><div class="im in io ip iq"><h1 id="e80e" class="lm ln it bd lo lp lq lr ls lt lu lv lw jz lx ka ly kc lz kd ma kf mb kg mc md bi translated">线性可分的两类</h1><p id="788b" class="pw-post-body-paragraph mt mu it mv b mw mx ju my mz na jx nb mj nc nd ne mm nf ng nh mp ni nj nk le im bi translated">帮助解释SVM概念的最基本的场景是解决一个二元分类问题，其中两个类是线性可分的。相应的解决方案也称为“硬”SVM。这是因为一旦找到了“最优”决策边界，就不会有错误。所有训练点都在决策边界的正确一侧。这与“软余量”SVM形成对比，当我们处理不是线性可分的分类任务并且允许一些错误时。</p><p id="d1d6" class="pw-post-body-paragraph mt mu it mv b mw nv ju my mz nw jx nb mj nx nd ne mm ny ng nh mp nz nj nk le im bi translated">SVM方程(从技术上讲，是一个方程和一个不等式)是:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="pa pb di pc bf pd"><div class="gh gi oz"><img src="../Images/5385f9a0f1591e2f35017bd3fd61491b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uJqKRB-K8C49QDuDQ7J5Vw.png"/></div></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">硬边界支持向量机方程(图片由作者提供)</p></figure><p id="2154" class="pw-post-body-paragraph mt mu it mv b mw nv ju my mz nw jx nb mj nx nd ne mm ny ng nh mp nz nj nk le im bi translated">文章的其余部分将推导出上述内容。先来个数学鸟瞰！</p><p id="5342" class="pw-post-body-paragraph mt mu it mv b mw nv ju my mz nw jx nb mj nx nd ne mm ny ng nh mp nz nj nk le im bi translated"><em class="oa">注:知名的机器学习书籍中有很多关于这个话题的优秀资源。根据您查看的资源，您会发现在使用的符号方面有细微的变化。为了让这篇文章易于理解，我选择了“机器学习的数学”所采用的符号，可以在</em> <a class="ae pe" href="https://github.com/mml-book/mml-book.github.io/tree/master/book" rel="noopener ugc nofollow" target="_blank"> <em class="oa"> GitHub </em> </a> <em class="oa">上免费获得。</em></p></div><div class="ab cl lf lg hx lh" role="separator"><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk"/></div><div class="im in io ip iq"><h1 id="8022" class="lm ln it bd lo lp lq lr ls lt lu lv lw jz lx ka ly kc lz kd ma kf mb kg mc md bi translated">分类的高级数学视图</h1><p id="8b32" class="pw-post-body-paragraph mt mu it mv b mw mx ju my mz na jx nb mj nc nd ne mm nf ng nh mp ni nj nk le im bi translated">您可以将分类任务视为一个映射，该映射接受一个<em class="oa"> n维</em>输入(特征向量，<strong class="mv iu"> <em class="oa"> x </em> </strong>)并输出一个<em class="oa"> 1维</em>类标签，<em class="oa"> y </em>(假设一个类为正，另一个类为负)。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pf"><img src="../Images/4bda54ab436a1a00fb825c4ca55316e3.png" data-original-src="https://miro.medium.com/v2/resize:fit:518/format:webp/1*2FEPv211AH9HG_-avlRs4Q.png"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">实数的n维向量空间(输入)被映射到1维向量空间(输出)的函数；(图片由作者提供)</p></figure><p id="bcf0" class="pw-post-body-paragraph mt mu it mv b mw nv ju my mz nw jx nb mj nx nd ne mm ny ng nh mp nz nj nk le im bi translated">为了进一步挖掘，我们需要理解超平面的概念！</p></div><div class="ab cl lf lg hx lh" role="separator"><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk"/></div><div class="im in io ip iq"><h1 id="b361" class="lm ln it bd lo lp lq lr ls lt lu lv lw jz lx ka ly kc lz kd ma kf mb kg mc md bi translated"><strong class="ak">超平面</strong></h1><p id="cb79" class="pw-post-body-paragraph mt mu it mv b mw mx ju my mz na jx nb mj nc nd ne mm nf ng nh mp ni nj nk le im bi translated">什么是超平面？在二维情况下，超平面是一条分隔线。在一个三维的T21案例中，它是一个平面。在<em class="oa"> n维</em>的情况下，它是一个<em class="oa"> (n-1) </em>维的向量空间(一个<em class="oa">超平面)。</em></p><h2 id="59c3" class="me ln it bd lo mf mg dn ls mh mi dp lw mj mk ml ly mm mn mo ma mp mq mr mc ms bi translated">内积是表示超平面的一种方便的符号</h2><p id="eac8" class="pw-post-body-paragraph mt mu it mv b mw mx ju my mz na jx nb mj nc nd ne mm nf ng nh mp ni nj nk le im bi translated">我们需要在<strong class="mv iu"> <em class="oa"> x </em> </strong>中找到一些不同特征的线性组合，一个方便的向量符号是使用内积(在这种情况下，这也被称为标量或点积)。因此，我们对于分离超平面的表达式变成:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pg"><img src="../Images/ca046a30b2b753b57deb1598ab469d9a.png" data-original-src="https://miro.medium.com/v2/resize:fit:452/format:webp/1*a_U3phCxcKiT2556NfOycw.png"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">超平面的表达式；(图片由作者提供)</p></figure><p id="02dd" class="pw-post-body-paragraph mt mu it mv b mw nv ju my mz nw jx nb mj nx nd ne mm ny ng nh mp nz nj nk le im bi translated">在上面的表达式中，<strong class="mv iu"> <em class="oa"> w </em> </strong>是权重向量，<strong class="mv iu"> <em class="oa"> x </em> </strong>是特征向量，<em class="oa"> b </em>是偏差项(进一步阅读可以理解这些术语的几何意义)。两个向量之间的内积是一种紧凑的表示，它将每个单独的对应分量相乘，然后将所有的乘积相加。例如，如果我们正在处理二维情况，我们可以将超平面表达式表示为:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ph"><img src="../Images/6663b4646b7c0e7486481103f418165c.png" data-original-src="https://miro.medium.com/v2/resize:fit:652/format:webp/1*sNeBrxZOtMA4xog0W5E6Vg.png"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">二维超平面的表达(图片由作者提供)</p></figure><p id="ae50" class="pw-post-body-paragraph mt mu it mv b mw nv ju my mz nw jx nb mj nx nd ne mm ny ng nh mp nz nj nk le im bi translated">本例中，<strong class="mv iu"><em class="oa">w</em></strong><em class="oa">=(w</em>₀<em class="oa">，w </em> ₁ <em class="oa">)，以及</em><strong class="mv iu"><em class="oa">x</em></strong><em class="oa">=(x</em>₀<em class="oa">，x </em> ₁ <em class="oa">)。</em> <strong class="mv iu"> <em class="oa"> </em> </strong>说白了，<strong class="mv iu"> <em class="oa"> w </em> </strong>和<strong class="mv iu"> <em class="oa"> x </em> </strong>都是向量，各有两个分量。</p></div><div class="ab cl lf lg hx lh" role="separator"><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk"/></div><div class="im in io ip iq"><h1 id="bea6" class="lm ln it bd lo lp lq lr ls lt lu lv lw jz lx ka ly kc lz kd ma kf mb kg mc md bi translated">超平面不等式和方程</h1><p id="9f7b" class="pw-post-body-paragraph mt mu it mv b mw mx ju my mz na jx nb mj nc nd ne mm nf ng nh mp ni nj nk le im bi translated">我们对超平面表达式本身不感兴趣，而是使用它来帮助我们将n维<em class="oa">空间划分成区域。我们可以通过指定带有不等式的条件来做到这一点。</em></p><p id="7378" class="pw-post-body-paragraph mt mu it mv b mw nv ju my mz nw jx nb mj nx nd ne mm ny ng nh mp nz nj nk le im bi translated">在我们的具有<em class="oa"> n </em>个特征的两类线性可分分类问题中，让我们假设这两个类被标记为+1或-1(我们可以假设不同的数字来表示每个类，但是这种选择使得随后的数学变得简单)。我们想从超平面中得到的是保证任何属于一类的点(比如说+1)在超平面的一边(比如说正边)，属于另一类的点(比如说当yn=-1时)在负边。</p><p id="2c8b" class="pw-post-body-paragraph mt mu it mv b mw nv ju my mz nw jx nb mj nx nd ne mm ny ng nh mp nz nj nk le im bi translated">数学上，对于正类，超平面不等式变成:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pi"><img src="../Images/fddf6d4b9fbe6acd471574770a96ce93.png" data-original-src="https://miro.medium.com/v2/resize:fit:1150/format:webp/1*iVCNcvXsninRAJoCeqmfbQ.png"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">类为+1时的超平面不等式(图片由作者提供)</p></figure><p id="77f1" class="pw-post-body-paragraph mt mu it mv b mw nv ju my mz nw jx nb mj nx nd ne mm ny ng nh mp nz nj nk le im bi translated">对于消极阶层来说，</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pi"><img src="../Images/4b4a4feda64c735b615f59390068e986.png" data-original-src="https://miro.medium.com/v2/resize:fit:1150/format:webp/1*2QPAOg0TnKhJJtFTuXr2bA.png"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">类为-1时的超平面不等式(图片由作者提供)</p></figure><p id="79cb" class="pw-post-body-paragraph mt mu it mv b mw nv ju my mz nw jx nb mj nx nd ne mm ny ng nh mp nz nj nk le im bi translated">我们可以将上述两个不等式表示为一个不等式:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pj"><img src="../Images/854cf371687c2daba76821d28795b6bf.png" data-original-src="https://miro.medium.com/v2/resize:fit:748/format:webp/1*V3N2r3JaQshH3v_aY1Tk9w.png"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">以上两个超平面不等式的紧凑表示，方程A(图片由作者提供)</p></figure><p id="012d" class="pw-post-body-paragraph mt mu it mv b mw nv ju my mz nw jx nb mj nx nd ne mm ny ng nh mp nz nj nk le im bi translated"><em class="oa">(如果你对你的高中数学有点生疏的话，紧凑表示法相当于上面的两个表达式，因为当我们将一个不等式乘以一个负数时，不等式的符号会改变)</em></p><p id="a729" class="pw-post-body-paragraph mt mu it mv b mw nv ju my mz nw jx nb mj nx nd ne mm ny ng nh mp nz nj nk le im bi translated">简单地说，上述不等式说明了属于这两类的点位于超平面的相对两侧的条件。</p><p id="3f9c" class="pw-post-body-paragraph mt mu it mv b mw nv ju my mz nw jx nb mj nx nd ne mm ny ng nh mp nz nj nk le im bi translated">满足上述约束的特定超平面将是超平面方程。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pk"><img src="../Images/761986dfcaf5b929f5c1f5dc314f0e0c.png" data-original-src="https://miro.medium.com/v2/resize:fit:686/format:webp/1*8Pn1F5hZLtaqcrcVC5fb4A.png"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">超平面方程(作者图片)</p></figure></div><div class="ab cl lf lg hx lh" role="separator"><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk"/></div><div class="im in io ip iq"><h1 id="2379" class="lm ln it bd lo lp lq lr ls lt lu lv lw jz lx ka ly kc lz kd ma kf mb kg mc md bi translated">超平面的几何视图</h1><p id="dbe7" class="pw-post-body-paragraph mt mu it mv b mw mx ju my mz na jx nb mj nc nd ne mm nf ng nh mp ni nj nk le im bi translated">任何超平面都可以用一个权重向量、<strong class="mv iu"> <em class="oa"> w、</em> </strong>和一个偏差项、<em class="oa"> b、</em>来表示，从图形上看，一个超平面在3D中看起来就像下图。权重向量<strong class="mv iu"> <em class="oa"> w </em> </strong>垂直于(成90度角)平面，偏移项<em class="oa"> b </em>是平面到原点的距离(偏移)，并且<em class="oa"> x </em> ₘ和<em class="oa"> x </em> ₙ是平面上的两个点(平面上的任意两个点！).</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="pa pb di pc bf pd"><div class="gh gi pl"><img src="../Images/334492bddeeb36322d50152aeafdaee5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*esu06PwDhPZAIjhM18obUw.png"/></div></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">3D中的分离超平面，改编自Deisenroth，Marc Peter，A. Aldo Faisal和Cheng Soon Ong。<em class="pm">机器学习的数学</em> 。”(图片由作者提供)</p></figure><p id="2570" class="pw-post-body-paragraph mt mu it mv b mw nv ju my mz nw jx nb mj nx nd ne mm ny ng nh mp nz nj nk le im bi translated">如果你把上面的三维图形投影到一个平面上，这就变得很清楚了。你可以直观地认为投影是从边缘看同一个图形，这样平面看起来就像一条线。从那边看，连接ₘ和ₙ的线将会是一条线。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pn"><img src="../Images/477b3c6c7ab1b0553b76a742064ca3a1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1112/format:webp/1*dD-iHX2agS7xXxKE0mJtmQ.png"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">从侧面看的分离超平面(图片由作者提供)</p></figure><p id="a7df" class="pw-post-body-paragraph mt mu it mv b mw nv ju my mz nw jx nb mj nx nd ne mm ny ng nh mp nz nj nk le im bi translated">此时，你可能会奇怪为什么重量向量，<strong class="mv iu"> <em class="oa"> w，</em> </strong>垂直于平面。对此的一个简单证明就是，想象超平面上的任意两点，然后算出<strong class="mv iu"> <em class="oa"> w </em> </strong>与平面上两点连接形成的向量的内积为0。换句话说，<strong class="mv iu"> <em class="oa"> w </em> </strong>和<strong class="mv iu">在超平面上的任何一个</strong>向量的点积将会是0。当两个向量的内积为零时，那么这两个向量总是正交的。下图以较慢的速度提供了相同的证明。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="pa pb di pc bf pd"><div class="gh gi po"><img src="../Images/7fb92f3b8c126e3ac00eef8e22aa56f7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8U5TBO7mL3IjjoZSagAD6Q.png"/></div></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">为什么超平面方程中的<strong class="bd pp"> <em class="pm"> w </em> </strong> <em class="pm">与平面正交的证明(图片作者提供)</em></p></figure><p id="3e2a" class="pw-post-body-paragraph mt mu it mv b mw nv ju my mz nw jx nb mj nx nd ne mm ny ng nh mp nz nj nk le im bi translated"><strong class="mv iu">要记住的底线是这样的:</strong></p><blockquote class="ku"><p id="c5dc" class="kv kw it bd kx ky nl nm nn no np le dk translated">超平面方程由一个矢量和一个偏差项表示。等式中的向量与超平面正交，偏差项表示从原点的偏移量。</p></blockquote></div><div class="ab cl lf lg hx lh" role="separator"><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk"/></div><div class="im in io ip iq"><h1 id="ae43" class="lm ln it bd lo lp lq lr ls lt lu lv lw jz lx ka ly kc lz kd ma kf mb kg mc md bi translated">点到超平面的距离</h1><p id="9c88" class="pw-post-body-paragraph mt mu it mv b mw mx ju my mz na jx nb mj nc nd ne mm nf ng nh mp ni nj nk le im bi translated">我们现在几乎完成了。记住SVM的目的:找到导致最大利润的<strong class="mv iu"><em class="oa"/></strong>超平面。边缘只不过是最近的类数据点和超平面之间的距离。这正是我们现在要做的:找出点到超平面的距离。</p><p id="b705" class="pw-post-body-paragraph mt mu it mv b mw nv ju my mz nw jx nb mj nx nd ne mm ny ng nh mp nz nj nk le im bi translated">让我们考虑一个点，<em class="oa"> xₐ </em>，属于其中一类。当我们说从一个点到一个平面的距离时，我们指的不是任何距离，而是最短的距离。</p><blockquote class="ku"><p id="ee4f" class="kv kw it bd kx ky nl nm nn no np le dk translated">想象一下把你的手机放在桌子上方。当我们说手机到桌子的距离时，我们指的是最短的距离，<strong class="ak"> <em class="pm">的直线距离，而不是</em> </strong>的任何其他距离。</p></blockquote><figure class="pr ps pt pu pv kn gh gi paragraph-image"><div role="button" tabindex="0" class="pa pb di pc bf pd"><div class="gh gi pq"><img src="../Images/3dc2295cddd0f829baf1bf07308a8ffe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*W4-yQdOG0I-gnCvpFqIUJw.png"/></div></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">正交投影给出了点到平面的最短距离(图片由作者提供)</p></figure><blockquote class="ku"><p id="fb43" class="kv kw it bd kx ky kz la lb lc ld le dk translated">同样的概念也适用于此。我们感兴趣的是这个点到超平面的最短距离(这也叫点在平面上的正交投影)。在下图中，让我们将此标记为<em class="pm"> x'ₐ </em>。</p></blockquote><figure class="pr ps pt pu pv kn gh gi paragraph-image"><div class="gh gi pw"><img src="../Images/c1bc01706ac26d2ea2a7d9b96b958486.png" data-original-src="https://miro.medium.com/v2/resize:fit:1184/format:webp/1*F7NeWyJcRVfyNwtjxeKung.png"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">点(xₐ)和平面之间距离的概念，说明了正交投影的概念，改编自"<a class="ae pe" href="https://github.com/mml-book/mml-book.github.io/tree/master/book" rel="noopener ugc nofollow" target="_blank"> Deisenroth、Marc Peter、A. Aldo Faisal和Cheng Soon Ong。<em class="pm">机器学习的数学</em> </a>(图片由作者提供)</p></figure><p id="3daf" class="pw-post-body-paragraph mt mu it mv b mw nv ju my mz nw jx nb mj nx nd ne mm ny ng nh mp nz nj nk le im bi translated"><em class="oa"> x'ₐ </em>和<em class="oa"> xₐ </em>之间的矢量可以看作是由r(边距)缩放的单位正交矢量(从<strong class="mv iu"> <em class="oa"> w </em> </strong>导出)。使用简单的向量加法和减法，我们得到:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="pa pb di pc bf pd"><div class="gh gi px"><img src="../Images/c233afc9687d107e0d456634f775459f.png" data-original-src="https://miro.medium.com/v2/resize:fit:734/format:webp/1*jRNWdR2aDo3lQJ7APgg8Jg.png"/></div></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">等式1(图片由作者提供)</p></figure><p id="6957" class="pw-post-body-paragraph mt mu it mv b mw nv ju my mz nw jx nb mj nx nd ne mm ny ng nh mp nz nj nk le im bi translated">在哪里</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi py"><img src="../Images/a97189abdf4432e8b090e0fa98cc0f11.png" data-original-src="https://miro.medium.com/v2/resize:fit:242/format:webp/1*wxxNOBmG7FTFZPjwn3nThw.png"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">作者图片</p></figure><p id="7fb3" class="pw-post-body-paragraph mt mu it mv b mw nv ju my mz nw jx nb mj nx nd ne mm ny ng nh mp nz nj nk le im bi translated">是单位缩放矢量。如果你对向量加减法有点生疏，那么可以把等式(1)看作是从原点到达x'ₐ的两种等价方式。基于等式(1)的左手边，你可以直接沿着矢量x'ₐ.走基于等式(1)的右手边，你可以沿着xₐ走，然后沿着<em class="oa">-r</em>(<strong class="mv iu"><em class="oa">w/| | w | |</em></strong>)。在这两种情况下，你都将从原点出发，最终到达同一点，x'ₐ.</p></div><div class="ab cl lf lg hx lh" role="separator"><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk"/></div><div class="im in io ip iq"><h1 id="1115" class="lm ln it bd lo lp lq lr ls lt lu lv lw jz lx ka ly kc lz kd ma kf mb kg mc md bi translated">SVM方程，最后</h1><p id="fd0e" class="pw-post-body-paragraph mt mu it mv b mw mx ju my mz na jx nb mj nc nd ne mm nf ng nh mp ni nj nk le im bi translated">我们现在几乎完成了。想想下面显示的场景:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="pa pb di pc bf pd"><div class="gh gi pz"><img src="../Images/26c96b4ee9193f436252aa24ef91c6a0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7IXpY6-Apmroi3ZJOsDSQg.png"/></div></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">帮助推导利润公式的插图，改编自"<a class="ae pe" href="https://github.com/mml-book/mml-book.github.io/tree/master/book" rel="noopener ugc nofollow" target="_blank"> Deisenroth、Marc Peter、A. Aldo Faisal和Cheng Soon Ong。<em class="pm">机器学习的数学</em> </a>(图片由作者提供)</p></figure><p id="c35d" class="pw-post-body-paragraph mt mu it mv b mw nv ju my mz nw jx nb mj nx nd ne mm ny ng nh mp nz nj nk le im bi translated">让我们假设xₐ是离超平面最近的点，离正类的距离为1。给定为x'ₐ的xₐ的投影位于超平面上，因此:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi qa"><img src="../Images/ede094af246b29f034cbf8f53aaaa4fd.png" data-original-src="https://miro.medium.com/v2/resize:fit:744/format:webp/1*ao25uTahKGLIngy43WaIyQ.png"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">等式2(图片由作者提供)</p></figure><p id="c07e" class="pw-post-body-paragraph mt mu it mv b mw nv ju my mz nw jx nb mj nx nd ne mm ny ng nh mp nz nj nk le im bi translated">将等式(1)代入等式(2):</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi qb"><img src="../Images/2bd59c7c2e301842c38464fcde98fa2e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1048/format:webp/1*ZaCwyoH9nERtyIUV_1s9UQ.png"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">等式3(图片由作者提供)</p></figure><p id="79a0" class="pw-post-body-paragraph mt mu it mv b mw nv ju my mz nw jx nb mj nx nd ne mm ny ng nh mp nz nj nk le im bi translated">展开等式(3)中的各项并重新排列，我们得到:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi qc"><img src="../Images/322f967a9dc6dbbc0121ee53e00da802.png" data-original-src="https://miro.medium.com/v2/resize:fit:1158/format:webp/1*SHmxPptjKVphIR-Ls_-LOA.png"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">等式4(图片由作者提供)</p></figure><p id="11ec" class="pw-post-body-paragraph mt mu it mv b mw nv ju my mz nw jx nb mj nx nd ne mm ny ng nh mp nz nj nk le im bi translated">使用非常简单的简化方法(如下所示)，</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi qd"><img src="../Images/9427e5f385aa310fe3f751a93d8e0c60.png" data-original-src="https://miro.medium.com/v2/resize:fit:1124/format:webp/1*hYJRef6o6xJKz3hS4pru7w.png"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">方程式4的简化(图片由作者提供)</p></figure><p id="023b" class="pw-post-body-paragraph mt mu it mv b mw nv ju my mz nw jx nb mj nx nd ne mm ny ng nh mp nz nj nk le im bi translated">我们得到:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi qe"><img src="../Images/2bfa5d0d027ea25dac813e1e54dfd94a.png" data-original-src="https://miro.medium.com/v2/resize:fit:336/format:webp/1*cMRL_jaHOSw3bLhS_SjN_A.png"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">用垂直于超平面的向量表示的边距，等式B(图片由作者提供)</p></figure><p id="34fc" class="pw-post-body-paragraph mt mu it mv b mw nv ju my mz nw jx nb mj nx nd ne mm ny ng nh mp nz nj nk le im bi translated">因此，SVM要求是最大化余量(等式B)并确保点位于正确的一侧，如前面的不等式所示(等式A)。</p><p id="f7a6" class="pw-post-body-paragraph mt mu it mv b mw nv ju my mz nw jx nb mj nx nd ne mm ny ng nh mp nz nj nk le im bi translated">为了数学上的方便，我们没有最大化等式B，而是最小化它的倒数(和平方)并添加1/2项(这不会改变优化函数，但在计算梯度时只会导致更整洁的解)。因此，目标是:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="pa pb di pc bf pd"><div class="gh gi oz"><img src="../Images/5385f9a0f1591e2f35017bd3fd61491b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uJqKRB-K8C49QDuDQ7J5Vw.png"/></div></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">硬边界SVM方程(图片由作者提供)</p></figure></div><div class="ab cl lf lg hx lh" role="separator"><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk"/></div><div class="im in io ip iq"><p id="a437" class="pw-post-body-paragraph mt mu it mv b mw nv ju my mz nw jx nb mj nx nd ne mm ny ng nh mp nz nj nk le im bi translated">好了，这些是SVM方程，你可以在几本机器学习的书中找到。它包括服从约束的最小化(即找到导致最大余量的<strong class="mv iu"> <em class="oa"> w </em> </strong>的值)。该约束表示在不等式中，该不等式确保来自正类和负类的数据点都在超平面的正确侧。这是一个约束优化问题，可以用语言乘数法来解决！</p><blockquote class="ku"><p id="b5e5" class="kv kw it bd kx ky nl nm nn no np le dk translated">“任何领域的专家都曾是初学者”海伦·海丝</p></blockquote><p id="bb25" class="pw-post-body-paragraph mt mu it mv b mw nq ju my mz nr jx nb mj ns nd ne mm nt ng nh mp nu nj nk le im bi translated"><strong class="mv iu">祝贺你，你向成为数据科学专家又迈进了一步！</strong></p><div class="oh oi gp gr oj ok"><a href="https://ahmarshah.medium.com/membership" rel="noopener follow" target="_blank"><div class="ol ab fo"><div class="om ab on cl cj oo"><h2 class="bd iu gy z fp op fr fs oq fu fw is bi translated">阅读艾哈迈尔·沙阿博士(牛津)的每一个故事(以及媒体上成千上万的其他作家)</h2><div class="or l"><h3 class="bd b gy z fp op fr fs oq fu fw dk translated">作为一个媒体会员，你的会员费的一部分会给你阅读的作家，你可以完全接触到每一个故事…</h3></div><div class="os l"><p class="bd b dl z fp op fr fs oq fu fw dk translated">ahmarshah.medium.com</p></div></div><div class="ot l"><div class="qf l ov ow ox ot oy ko ok"/></div></div></a></div></div></div>    
</body>
</html>