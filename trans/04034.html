<html>
<head>
<title>Export Datastore to BigQuery using Google Dataflow</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用Google数据流将数据存储导出到BigQuery</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/export-datastore-to-bigquery-using-google-dataflow-1801c25ae482?source=collection_archive---------14-----------------------#2021-04-05">https://towardsdatascience.com/export-datastore-to-bigquery-using-google-dataflow-1801c25ae482?source=collection_archive---------14-----------------------#2021-04-05</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><figure class="ip iq gp gr ir is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi io"><img src="../Images/0f9b4ce81bc49b3d6f49a7508455ff2e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AM6GIySotyiDsMR69fLtcQ.jpeg"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">波多黎各德拉克鲁斯(作者)</p></figure><div class=""/><div class=""><h2 id="fc47" class="pw-subtitle-paragraph kc je jf bd b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt dk translated">如何使用Google Dataflow将数据存储导出到BigQuery，并对实体进行额外过滤</h2></div><p id="b68e" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">在上一篇文章中，我展示了如何构建一个无服务器的解决方案，将各种数据从Datastore导出到BigQuery。该文章中介绍的方法完全有效，甚至适用于大型数据存储。然而，主要的缺点是每次我们将所有行从数据存储导出到BigQuery。对于大型数据存储来说，这可能会产生不必要的成本，消耗不必要的时间。</p><div class="ip iq gp gr ir lq"><a rel="noopener follow" target="_blank" href="/serverless-approach-to-export-datastore-to-bigquery-4156fadb8509"><div class="lr ab fo"><div class="ls ab lt cl cj lu"><h2 class="bd jg gy z fp lv fr fs lw fu fw je bi translated">将数据存储导出到BigQuery的无服务器方法</h2><div class="lx l"><h3 class="bd b gy z fp lv fr fs lw fu fw dk translated">一种在Google云平台上使用无服务器方法定期将数据存储导出到BigQuery的简单方法</h3></div><div class="ly l"><p class="bd b dl z fp lv fr fs lw fu fw dk translated">towardsdatascience.com</p></div></div><div class="lz l"><div class="ma l mb mc md lz me ix lq"/></div></div></a></div><p id="2a3b" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">解决这个问题的方法之一是对数据库进行一系列更新。例如，<a class="ae mf" href="https://aws.amazon.com/dynamodb/" rel="noopener ugc nofollow" target="_blank"> AWS DynamoDB </a>提供了<a class="ae mf" href="https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Streams.html" rel="noopener ugc nofollow" target="_blank">流</a>，可以很容易地与AWS Lambdas链接。在<a class="ae mf" href="https://cloud.google.com/firestore" rel="noopener ugc nofollow" target="_blank">谷歌Firestore </a>(被命名为数据存储的下一代<a class="ae mf" href="https://cloud.google.com/datastore/docs/firestore-or-datastore" rel="noopener ugc nofollow" target="_blank">)中可以找到非常相似的功能，其中对文档的更改会触发云功能— </a><a class="ae mf" href="https://cloud.google.com/functions/docs/calling/cloud-firestore" rel="noopener ugc nofollow" target="_blank">参见文档。</a></p><p id="0678" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">尽管datastore不提供任何流功能，但我们仍然可以尝试通过使用查询来解决这个问题。数据存储库<a class="ae mf" href="https://cloud.google.com/datastore/docs/export-import-entities" rel="noopener ugc nofollow" target="_blank">导入/导出</a>不支持实体的本地过滤。因此，我们必须手动操作<em class="mg">。</em>程序如下:</p><ol class=""><li id="6ac1" class="mh mi jf kw b kx ky la lb ld mj lh mk ll ml lp mm mn mo mp bi translated">过滤实体，导出到JSON，存储到云存储中</li><li id="321f" class="mh mi jf kw b kx mq la mr ld ms lh mt ll mu lp mm mn mo mp bi translated">将JSONs从云存储加载到BigQuery</li></ol><p id="f79f" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">让我用谷歌数据流来完成这个任务。</p><p id="4e8d" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">Google Dataflow是一个托管解决方案，用于执行不同的数据处理模式，如ETL、批处理和流处理。但是谷歌数据流是<a class="ae mf" href="http://www.vldb.org/pvldb/vol8/p1792-Akidau.pdf" rel="noopener ugc nofollow" target="_blank">数据流模型</a>的可能实现之一。用于描述处理的SDK是在框架<a class="ae mf" href="https://beam.apache.org/" rel="noopener ugc nofollow" target="_blank"> Apache Beam </a>下实现的。</p><h1 id="40c8" class="mv mw jf bd mx my mz na nb nc nd ne nf kl ng km nh ko ni kp nj kr nk ks nl nm bi translated">数据流管道</h1><p id="87c3" class="pw-post-body-paragraph ku kv jf kw b kx nn kg kz la no kj lc ld np lf lg lh nq lj lk ll nr ln lo lp ij bi translated">数据流模型是围绕管道组织的，管道是你从头到尾的数据处理工作流程。在管道内部，有两个对象很重要。PCollection表示分布式数据集，PTransform表示对PCollection的处理操作。</p><figure class="nt nu nv nw gt is gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/28f2f6307107829caf8cf01eabd79378.png" data-original-src="https://miro.medium.com/v2/resize:fit:1230/format:webp/1*0sWx6tm7plrmc_8j09IIeA.png"/></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">p收集/p转换概述(按作者)</p></figure><p id="1e29" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">我将使用Python作为编程语言。但是，管道也可以在Java和Golang中构建。完整的工作示例可在GitHub项目中获得(<a class="ae mf" href="https://github.com/jkrajniak/demo-datastore-export-filtering" rel="noopener ugc nofollow" target="_blank">https://GitHub . com/jkrajniak/demo-datastore-export-filtering</a>)。在这里，我将只对重要的代码块进行评论。</p><div class="ip iq gp gr ir lq"><a href="https://github.com/jkrajniak/demo-datastore-export-filtering" rel="noopener  ugc nofollow" target="_blank"><div class="lr ab fo"><div class="ls ab lt cl cj lu"><h2 class="bd jg gy z fp lv fr fs lw fu fw je bi translated">jkrajniak/demo-数据存储-导出-过滤</h2><div class="lx l"><h3 class="bd b gy z fp lv fr fs lw fu fw dk translated">在GitHub上创建一个帐号，为jkrajniak/demo-datastore-export-filtering开发做贡献。</h3></div><div class="ly l"><p class="bd b dl z fp lv fr fs lw fu fw dk translated">github.com</p></div></div><div class="lz l"><div class="nx l mb mc md lz me ix lq"/></div></div></a></div><h2 id="8267" class="ny mw jf bd mx nz oa dn nb ob oc dp nf ld od oe nh lh of og nj ll oh oi nl oj bi translated">管道</h2><p id="dca8" class="pw-post-body-paragraph ku kv jf kw b kx nn kg kz la no kj lc ld np lf lg lh nq lj lk ll nr ln lo lp ij bi translated">让我们开始构建管道:</p><pre class="nt nu nv nw gt ok ol om on aw oo bi"><span id="470b" class="ny mw jf ol b gy op oq l or os">with beam.Pipeline(options=pipeline_options) as p:<br/>    # Create a query and filter</span></pre><p id="aa7d" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">这将创建一个管道<code class="fe ot ou ov ol b">p</code>，选项存储在<code class="fe ot ou ov ol b">pipeline_options</code>中。接下来，操作符<code class="fe ot ou ov ol b">|</code>将被用于连接每个<em class="mg">p转换块</em></p><pre class="nt nu nv nw gt ok ol om on aw oo bi"><span id="d8e9" class="ny mw jf ol b gy op oq l or os">rows = p | 'get all kinds' &gt;&gt; GetAllKinds(project_id, to_ignore)</span></pre><p id="c04d" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">这是第一阶段，它将从给定项目的数据存储中读取所有类型，并从该列表中产生一个PCollection。该块在内部实现了<code class="fe ot ou ov ol b">expand</code>方法(如下)。此外，过滤是为了去除一些我们不希望被导出的种类。最后，<code class="fe ot ou ov ol b"><a class="ae mf" href="https://github.com/apache/beam/blob/master/sdks/python/apache_beam/transforms/core.py#L2906" rel="noopener ugc nofollow" target="_blank">Create</a></code> <a class="ae mf" href="https://github.com/apache/beam/blob/master/sdks/python/apache_beam/transforms/core.py#L2906" rel="noopener ugc nofollow" target="_blank">转换</a>用于从种类列表中构建一个p集合。</p><figure class="nt nu nv nw gt is"><div class="bz fp l di"><div class="ow ox l"/></div></figure><p id="a17e" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">接下来，对于每一种类型，我们必须构建一个查询——这是通过下一个PTransform块<code class="fe ot ou ov ol b">'create queries'</code>实现的</p><pre class="nt nu nv nw gt ok ol om on aw oo bi"><span id="7da5" class="ny mw jf ol b gy op oq l or os">rows = (p <br/>        | 'get all kinds' &gt;&gt; GetAllKinds(project_id, to_ignore)<br/>        | 'create queries' &gt;&gt; beam.ParDo(CreateQuery(project_id, param))<br/>       )</span></pre><p id="20e7" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">我们使用<code class="fe ot ou ov ol b"><a class="ae mf" href="https://beam.apache.org/documentation/programming-guide/#pardo" rel="noopener ugc nofollow" target="_blank">ParDo</a></code>，它是一个通用的并行处理转换模块。它接受一个从<code class="fe ot ou ov ol b">beam.DoFn</code>类派生的对象，该对象必须实现方法<code class="fe ot ou ov ol b">process(self, *args, **kwargs)</code>。下面是<code class="fe ot ou ov ol b">CreateQuery</code>类的<code class="fe ot ou ov ol b">process</code>方法的实现。</p><pre class="nt nu nv nw gt ok ol om on aw oo bi"><span id="5e7d" class="ny mw jf ol b gy op oq l or os">def process(self, kind_name, **kwargs):<br/>    <em class="mg">"""<br/>    </em><strong class="ol jg"><em class="mg">:param</em></strong><em class="mg"> **kwargs:<br/>    </em><strong class="ol jg"><em class="mg">:param</em></strong><em class="mg"> kind_name: a kind name<br/>    </em><strong class="ol jg"><em class="mg">:return</em></strong><em class="mg">: Query<br/>    """<br/><br/>    </em>logging.info(f'CreateQuery.process {kind_name} {kwargs}')<br/><br/>    q = Query(kind=kind_name, project=self.project_id)<br/>    if kind_name in self.entity_filtering:<br/>        q.filters = self.entity_filtering[kind_name].get_filter()<br/><br/>    logging.info(f'Query for kind {kind_name}: {q}')<br/><br/>    yield q</span></pre><p id="0475" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">上面的方法负责根据过滤参数生成获取元素的查询。一个简单的YAML配置文件用于定义过滤选项</p><figure class="nt nu nv nw gt is"><div class="bz fp l di"><div class="ow ox l"/></div></figure><p id="8af7" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">这个解决方案的一个重要注意事项。数据存储中的实体需要有一些可用于获取记录子集的字段。在这个例子中，我们设置字段<code class="fe ot ou ov ol b">timestamp</code>将用于获取记录的子集。如果管道每天执行一次，那么记录匹配查询<code class="fe ot ou ov ol b">(endTime-24h)&lt;= timestamp &lt; endTime</code>将被选择。您可以想象任何其他类型的查询，而不仅仅是基于时间戳。例如，您可以将上次获取的记录的id存储在某个地方，下次只获取比存储的id大的记录。</p><p id="da7c" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">接下来，我们向管道添加另外三个元素:</p><ul class=""><li id="ccb6" class="mh mi jf kw b kx ky la lb ld mj lh mk ll ml lp oy mn mo mp bi translated">应用查询和提取实体</li><li id="7d17" class="mh mi jf kw b kx mq la mr ld ms lh mt ll mu lp oy mn mo mp bi translated">将实体转换为JSON</li><li id="e9a5" class="mh mi jf kw b kx mq la mr ld ms lh mt ll mu lp oy mn mo mp bi translated">将JSONs保存到BigQuery</li></ul><p id="8044" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">管道中的最后两个阶段非常明显:</p><figure class="nt nu nv nw gt is"><div class="bz fp l di"><div class="ow ox l"/></div></figure><p id="4e93" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated"><em class="mg">从数据存储中读取</em>使用上一步中创建的查询从数据存储中获取实体。结果，创建了来自数据存储的实体的p集合。接下来，在<code class="fe ot ou ov ol b">beam.Map(entity_to_json)</code>中，每个实体都被转换成JSON表示。<code class="fe ot ou ov ol b">beam.Map</code>是<code class="fe ot ou ov ol b">beam.ParDo</code>的特例。它从PCollection中获取一个元素并生成一个元素。</p><p id="5aac" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">管道的最后一个元素是输出PTransform。没有经过过滤的实体被定向到一个空表中。另一个是从过滤中获得的，被附加到现有的表中。为了将元素导入这两个输出，我们使用了一个<a class="ae mf" href="https://beam.apache.org/documentation/programming-guide/#additional-outputs" rel="noopener ugc nofollow" target="_blank">标记特性</a>，它允许生成多个PCollections。</p><figure class="nt nu nv nw gt is"><div class="bz fp l di"><div class="ow ox l"/></div></figure><p id="76bc" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">如果种类名称在要过滤的选项中，那么我们用<code class="fe ot ou ov ol b">write_append</code>标记元素，否则，我们将<code class="fe ot ou ov ol b">write_truncate</code>标记附加到元素上。</p><p id="4cda" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">接下来，我们将这两个拆分的集合写入BigQuery:</p><figure class="nt nu nv nw gt is"><div class="bz fp l di"><div class="ow ox l"/></div></figure><p id="0f4c" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">在每种写方法中，我们使用<code class="fe ot ou ov ol b">SCHEMA_AUTODETECT</code>选项。输出表名称是从种类名称动态派生的，如果需要的话。</p><p id="9535" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">如果您在Google Dataflow中运行管道，那么整个作业将如下所示:</p><figure class="nt nu nv nw gt is gh gi paragraph-image"><div class="gh gi oz"><img src="../Images/a6d258d40b2c455079a3ac1ce72731c8.png" data-original-src="https://miro.medium.com/v2/resize:fit:976/format:webp/1*X4tcr1FtRMHd9Wicd1x-dA.png"/></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">数据管道(按作者)</p></figure></div><div class="ab cl pa pb hu pc" role="separator"><span class="pd bw bk pe pf pg"/><span class="pd bw bk pe pf pg"/><span class="pd bw bk pe pf"/></div><div class="ij ik il im in"><p id="87ba" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">所以当你调用运行管道的命令时，实际上会发生什么呢？基本上，如果你用runner <code class="fe ot ou ov ol b">direct</code>来做，工作流将会在你的本地机器上运行。</p><p id="8b3d" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">有了runner <code class="fe ot ou ov ol b">dataflow</code>，工作流将在GCP执行。首先，你的管道代码被打包成一个PyPi包(你可以在日志中看到命令<code class="fe ot ou ov ol b">python setup.py sdist</code>被执行)，然后<code class="fe ot ou ov ol b">zip</code>文件被复制到Google云存储桶。下一个工人准备好了。工人无非是<a class="ae mf" href="https://cloud.google.com/compute" rel="noopener ugc nofollow" target="_blank">谷歌云计算</a>的实例。您甚至可以在云控制台中看到它们:</p><figure class="nt nu nv nw gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi ph"><img src="../Images/d90a457e70fd8475a91060eea432c946.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vSMro0WWwPtw5codnpQGPQ.png"/></div></div></figure><p id="f58d" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">而且，如果你需要的话，你可以进入它们。请注意，启动workers、下载并在workers上安装您的管道需要时间。</p><blockquote class="pi pj pk"><p id="1c51" class="ku kv mg kw b kx ky kg kz la lb kj lc pl le lf lg pm li lj lk pn lm ln lo lp ij bi translated"><a class="ae mf" href="https://cloud.google.com/dataflow/docs/guides/using-custom-containers" rel="noopener ugc nofollow" target="_blank">事实上，Dataflow中的最新特性提供了一个定制的Docker容器映像。</a>这使您可以减少工作人员的启动时间(所有依赖项都已经打包到映像中)，您可以使用不公开的第三方库，或者您可以在后台运行一些软件——没有限制。</p></blockquote><p id="3649" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">当代码被安装在工人身上时，管道被执行。</p></div><div class="ab cl pa pb hu pc" role="separator"><span class="pd bw bk pe pf pg"/><span class="pd bw bk pe pf pg"/><span class="pd bw bk pe pf"/></div><div class="ij ik il im in"><h2 id="e70e" class="ny mw jf bd mx nz oa dn nb ob oc dp nf ld od oe nh lh of og nj ll oh oi nl oj bi translated">为什么不是纯云功能？</h2><p id="8afe" class="pw-post-body-paragraph ku kv jf kw b kx nn kg kz la no kj lc ld np lf lg lh nq lj lk ll nr ln lo lp ij bi translated">让我来讨论一下为什么谷歌数据流而不是纯云功能。云功能可能是一个完全有效的解决方案，但最终，架构和维护会非常困难。首先，与满载模式相反，这里我们必须物理地获取记录，并将它们存储在云存储中。一个函数的内存限制是4096 MB，最大运行时间是9分钟。因此，为了拥有一个健壮的、可伸缩的解决方案，我们必须对一批记录运行多个函数。您可以想象并行执行的树，对于每个页面结果，执行一个函数来获取记录的子集。然后并行的记录将被转换成JSON并加载到BigQuery</p><p id="176a" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">为了跟踪并行执行的进度，可以使用与我在之前的<a class="ae mf" href="https://medium.com/nordcloud-engineering/keep-track-on-your-cloud-computations-67dd8f172479" rel="noopener">文章中展示的类似的解决方案</a></p><div class="ip iq gp gr ir lq"><a href="https://medium.com/nordcloud-engineering/keep-track-on-your-cloud-computations-67dd8f172479" rel="noopener follow" target="_blank"><div class="lr ab fo"><div class="ls ab lt cl cj lu"><h2 class="bd jg gy z fp lv fr fs lw fu fw je bi translated">跟踪你的云计算</h2><div class="lx l"><h3 class="bd b gy z fp lv fr fs lw fu fw dk translated">如何使用无服务器组件在无限数量的函数中跟踪你的分布式计算。</h3></div><div class="ly l"><p class="bd b dl z fp lv fr fs lw fu fw dk translated">medium.com</p></div></div><div class="lz l"><div class="po l mb mc md lz me ix lq"/></div></div></a></div><p id="c5c8" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">或者<a class="ae mf" href="https://cloud.google.com/workflows" rel="noopener ugc nofollow" target="_blank">谷歌工作流</a>。虽然组织起来是可行的，但我认为这种努力不值得可能的成本降低。</p><h2 id="962e" class="ny mw jf bd mx nz oa dn nb ob oc dp nf ld od oe nh lh of og nj ll oh oi nl oj bi translated">定价</h2><p id="1f32" class="pw-post-body-paragraph ku kv jf kw b kx nn kg kz la no kj lc ld np lf lg lh nq lj lk ll nr ln lo lp ij bi translated">嗯，您需要为执行时间(按每秒的增量计费)和资源付费。管道至少有一个工作线程，它消耗vCPU、内存、存储和可选的GPU。如果您的任务不是计算和存储密集型的，那么您可以通过调整工作选项来更改默认设置。默认情况下，用于批处理的worker的磁盘大小设置为250 GB，用于流处理的worker的磁盘大小设置为400 GB。如果你的处理可以适应内存，那么这是一个相当大的数字。在上面的例子中，我使用了每个工作人员25 GB的磁盘大小——这已经足够了。</p><figure class="nt nu nv nw gt is gh gi paragraph-image"><div class="gh gi pp"><img src="../Images/b0743fda24e132a14c25f5eb61de0642.png" data-original-src="https://miro.medium.com/v2/resize:fit:906/format:webp/1*9L-qhNX_IYFzuT8xG27lLw.png"/></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">价格估算，两个工人，每个工人25GB，每月vCPU 1小时(由作者提供)</p></figure></div><div class="ab cl pa pb hu pc" role="separator"><span class="pd bw bk pe pf pg"/><span class="pd bw bk pe pf pg"/><span class="pd bw bk pe pf"/></div><div class="ij ik il im in"><p id="7000" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">Google Dataflow和Apache Beam model是一个强大的数据工程工具，允许构建复杂的数据管道。它可以用于批处理和流处理，具有不同的输入源和输出目的地。此外，工作被有效地无缝地分配给工人，没有任何调整。</p></div><div class="ab cl pa pb hu pc" role="separator"><span class="pd bw bk pe pf pg"/><span class="pd bw bk pe pf pg"/><span class="pd bw bk pe pf"/></div><div class="ij ik il im in"><p id="46b7" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">我希望你喜欢这个故事，它会对你的日常工作有所帮助。如果您有任何问题或建议，请随时通过<a class="ae mf" href="https://twitter.com/MrTheodor" rel="noopener ugc nofollow" target="_blank"> Twitter </a>或<a class="ae mf" href="https://www.linkedin.com/in/jkrajniak/" rel="noopener ugc nofollow" target="_blank"> Linkedin </a>联系我。</p></div></div>    
</body>
</html>