<html>
<head>
<title>8 Dimensionality Reduction Techniques every Data Scientists should know</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">每个数据科学家都应该知道的8种降维技术</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/8-dimensionality-reduction-techniques-every-data-scientists-should-know-1c594f0aa7f2?source=collection_archive---------13-----------------------#2021-07-22">https://towardsdatascience.com/8-dimensionality-reduction-techniques-every-data-scientists-should-know-1c594f0aa7f2?source=collection_archive---------13-----------------------#2021-07-22</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="2c60" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">Python中各种降维技术的基本指南</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/2f4f8bb46125c13454bd0b74e19c5405.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XqEKGhVeNpuYEhzIxyuYDg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图片由<a class="ae ky" href="https://pixabay.com/users/geralt-9301/?utm_source=link-attribution&amp;amp;utm_medium=referral&amp;amp;utm_campaign=image&amp;amp;utm_content=1023846" rel="noopener ugc nofollow" target="_blank">格尔德·奥特曼</a>来自<a class="ae ky" href="https://pixabay.com/?utm_source=link-attribution&amp;amp;utm_medium=referral&amp;amp;utm_campaign=image&amp;amp;utm_content=1023846" rel="noopener ugc nofollow" target="_blank">皮克斯拜</a></p></figure><p id="ed96" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">探索性数据分析是数据科学模型开发流程的重要组成部分。数据科学家将大部分时间花在数据清理、特性工程和执行其他数据争论技术上。降维是数据科学家在执行特征工程时使用的技术之一。</p><p id="6281" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">降维是将高维数据集转换到可比较的低维空间的过程。真实世界的数据集通常有许多冗余要素。降维技术可用于去除这种冗余特征或将n维数据集转换成2维或3维以便可视化。</p><p id="ae0c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在本文中，我们将讨论8种这样的降维技术，它们可用于各种用例来降低数据集的维度。</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="4fdf" class="ma mb it lw b gy mc md l me mf"><strong class="lw iu"><em class="mg">Checklist:</em></strong><br/><strong class="lw iu">1. Missing Value<br/>2. Correlation Filter<br/>3. Variance Filter<br/>4. Forward / Backward Feature Selection Techniques<br/>5. PCA (Principal Component Analysis)<br/>6. t-SNE (t-distributed Stochastic Neighbourhood Embedding)<br/>7. UMAP<br/>8. Auto-Encoders</strong></span></pre><h1 id="4abb" class="mh mb it bd mi mj mk ml mm mn mo mp mq jz mr ka ms kc mt kd mu kf mv kg mw mx bi translated">(1.)缺少值:</h1><p id="20bf" class="pw-post-body-paragraph kz la it lb b lc my ju le lf mz jx lh li na lk ll lm nb lo lp lq nc ls lt lu im bi translated">真实世界的数据集通常包含大量缺失记录，这可能是由于数据损坏或记录数据时出现故障造成的。人们可以尝试各种数据插补技术来填补缺失的记录，但这只适用于某个特征缺失有限数量的记录的情况。</p><p id="2586" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果缺失特征值的数量大于确定的阈值，则最好从训练数据中移除该特征。可以移除缺失特征记录大于阈值(比如50%)的所有特征，从而降低数据的维度。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nd"><img src="../Images/f5b277d06c4182f44c074ca3895f0f6a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*a244aUbbAmWBGk2bHONjuw.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">(图片由作者提供)，缺失值的可视化:白线表示缺失值的存在</p></figure><p id="3678" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">使用<code class="fe ne nf ng lw b"><a class="ae ky" href="https://github.com/ResidentMario/missingno" rel="noopener ugc nofollow" target="_blank"><strong class="lb iu">missingno</strong></a></code>包为<a class="ae ky" href="https://www.kaggle.com/c/titanic" rel="noopener ugc nofollow" target="_blank">钛数据</a>生成上述缺失值解释图像。特征“年龄”和“船舱”具有大量缺失记录，最终，它们可以从训练样本中移除。</p><h1 id="d77d" class="mh mb it bd mi mj mk ml mm mn mo mp mq jz mr ka ms kc mt kd mu kf mv kg mw mx bi translated">(2.)相关滤波器:</h1><p id="ed2b" class="pw-post-body-paragraph kz la it lb b lc my ju le lf mz jx lh li na lk ll lm nb lo lp lq nc ls lt lu im bi translated">一个或多个要素的相关性可能会导致多重共线性问题。要素的多重共线性会破坏独立变量的统计显著性。想法是丢弃与其他独立特征相关的特征。还可以删除与目标类别标签不相关的特征。</p><p id="d041" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">有各种技术来计算独立特征之间的相关性，包括Pearson、Spearman、Kendall、卡方检验等。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nd"><img src="../Images/3d8feba24b891c5d4413743a08f6df2d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JRWabrT81LpYUsiOt8lDEQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">(图片由作者提供)，相关矩阵的热图</p></figure><p id="65bd" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">使用<code class="fe ne nf ng lw b"><strong class="lb iu">df.corr()</strong></code> <strong class="lb iu"> </strong>函数计算上述关联矩阵的热图(针对<a class="ae ky" href="https://www.kaggle.com/c/titanic" rel="noopener ugc nofollow" target="_blank">泰坦尼克号数据集</a>)。</p><h1 id="e6f3" class="mh mb it bd mi mj mk ml mm mn mo mp mq jz mr ka ms kc mt kd mu kf mv kg mw mx bi translated">(3.)方差过滤器:</h1><p id="54b4" class="pw-post-body-paragraph kz la it lb b lc my ju le lf mz jx lh li na lk ll lm nb lo lp lq nc ls lt lu im bi translated">只有一个特征类别的分类特征或具有非常低方差的数字特征变量可以从训练样本中排除。这些特征可能对模型训练没有帮助。</p><p id="9aa7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">函数<code class="fe ne nf ng lw b"><strong class="lb iu">DataFrame.var()</strong></code>可以计算熊猫数据框所有特征的方差。<code class="fe ne nf ng lw b"><strong class="lb iu">DataFrame.value_counts()</strong></code>函数可以计算每个特征的分布。</p><h1 id="92b9" class="mh mb it bd mi mj mk ml mm mn mo mp mq jz mr ka ms kc mt kd mu kf mv kg mw mx bi translated">(4.)向前/向后特征选择:</h1><p id="625d" class="pw-post-body-paragraph kz la it lb b lc my ju le lf mz jx lh li na lk ll lm nb lo lp lq nc ls lt lu im bi translated">前向特征选择技术是选择最佳特征集的包装器技术。这是一个循序渐进的过程，基于上一步的推断选择特征。正向特征选择技术的步骤是:</p><ol class=""><li id="838a" class="nh ni it lb b lc ld lf lg li nj lm nk lq nl lu nm nn no np bi translated">分别使用每个d特征训练机器学习模型，并测量每个模型的性能。</li><li id="bed6" class="nh ni it lb b lc nq lf nr li ns lm nt lq nu lu nm nn no np bi translated">采用具有最佳性能的特征，并使用其余特征重新训练单个模型。</li><li id="c9c8" class="nh ni it lb b lc nq lf nr li ns lm nt lq nu lu nm nn no np bi translated">使用我们获得最佳性能的功能与上一步中的功能列表连接在一起。</li><li id="10c7" class="nh ni it lb b lc nq lf nr li ns lm nt lq nu lu nm nn no np bi translated">重复步骤2和3，直到获得所需数量的特征。</li></ol><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nv"><img src="../Images/5ad9683695019bb35d8cdbeb5806e930.png" data-original-src="https://miro.medium.com/v2/resize:fit:1366/format:webp/1*kSiFSXk8hZNSbUaK8149QQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">(<a class="ae ky" href="https://www.analyticsvidhya.com/wp-content/uploads/2016/11/Embedded_1.png" rel="noopener ugc nofollow" target="_blank">源</a>)，正向特征选择</p></figure><p id="0b24" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">后向特征选择技术类似于前向特征选择，但是工作方式正好相反，最初选择所有特征，并且在每一步中移除最冗余的特征。</p><h1 id="652c" class="mh mb it bd mi mj mk ml mm mn mo mp mq jz mr ka ms kc mt kd mu kf mv kg mw mx bi translated">(5.)主成分分析:</h1><p id="4d34" class="pw-post-body-paragraph kz la it lb b lc my ju le lf mz jx lh li na lk ll lm nb lo lp lq nc ls lt lu im bi translated">主成分分析(PCA)是一种非常古老的降维技术。PCA通过保持特征的方差将特征向量投影到低维空间。它寻找最大方差的方向以获得最佳特征列表。</p><p id="9111" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">PCA可以用于将非常高维的数据投影到期望的维度中。PCA算法的步骤是:</p><ol class=""><li id="d7ef" class="nh ni it lb b lc ld lf lg li nj lm nk lq nl lu nm nn no np bi translated">列标准化数据集</li><li id="6f7a" class="nh ni it lb b lc nq lf nr li ns lm nt lq nu lu nm nn no np bi translated">计算标准化数据集的协方差矩阵</li><li id="eb6c" class="nh ni it lb b lc nq lf nr li ns lm nt lq nu lu nm nn no np bi translated">从协方差矩阵计算特征值和特征向量</li><li id="ed0f" class="nh ni it lb b lc nq lf nr li ns lm nt lq nu lu nm nn no np bi translated">取特征向量与具有高特征值的特征向量的点积。</li></ol><p id="1c7c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">Scikit-learn包附带了PCA的实现，请阅读<a class="ae ky" href="https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html" rel="noopener ugc nofollow" target="_blank">文档</a>以了解有关实现的更多信息。</p><blockquote class="nw nx ny"><p id="011a" class="kz la mg lb b lc ld ju le lf lg jx lh nz lj lk ll oa ln lo lp ob lr ls lt lu im bi translated">请阅读我之前发表的文章，了解PCA如何提高分类模型的性能</p></blockquote><div class="oc od gp gr oe of"><a rel="noopener follow" target="_blank" href="/dimensionality-reduction-can-pca-improve-the-performance-of-a-classification-model-d4e34194c544"><div class="og ab fo"><div class="oh ab oi cl cj oj"><h2 class="bd iu gy z fp ok fr fs ol fu fw is bi translated">降维——主成分分析能提高分类模型的性能吗？</h2><div class="om l"><h3 class="bd b gy z fp ok fr fs ol fu fw dk translated">利用PCA——降维技术提高ML模型的性能</h3></div><div class="on l"><p class="bd b dl z fp ok fr fs ol fu fw dk translated">towardsdatascience.com</p></div></div><div class="oo l"><div class="op l oq or os oo ot ks of"/></div></div></a></div><h1 id="27a4" class="mh mb it bd mi mj mk ml mm mn mo mp mq jz mr ka ms kc mt kd mu kf mv kg mw mx bi translated">(6.)t-SNE:</h1><p id="fe6e" class="pw-post-body-paragraph kz la it lb b lc my ju le lf mz jx lh li na lk ll lm nb lo lp lq nc ls lt lu im bi translated"><strong class="lb iu">t-SNE</strong>(t-分布式随机邻域嵌入)<strong class="lb iu"> </strong>是一种降维技术，主要用于数据可视化。t-SNE将高维数据集转换成可以进一步可视化的2或3维向量。</p><p id="b555" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">t-SNE比PCA性能更好，因为它保留了数据的局部结构，并通过保留邻域局部结构将每个数据点从高维空间嵌入到低维空间。</p><p id="2be3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">Scikit-learn包附带了t-SNE的实现，阅读<a class="ae ky" href="https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html" rel="noopener ugc nofollow" target="_blank">文档</a>了解更多关于实现的信息。</p><blockquote class="nw nx ny"><p id="eed7" class="kz la mg lb b lc ld ju le lf lg jx lh nz lj lk ll oa ln lo lp ob lr ls lt lu im bi translated">阅读下面提到的来自<a class="ae ky" href="https://distill.pub/" rel="noopener ugc nofollow" target="_blank">distilt . pub</a>的文章，了解如何有效地使用t-SNE:</p></blockquote><div class="oc od gp gr oe of"><a href="https://distill.pub/2016/misread-tsne/" rel="noopener  ugc nofollow" target="_blank"><div class="og ab fo"><div class="oh ab oi cl cj oj"><h2 class="bd iu gy z fp ok fr fs ol fu fw is bi translated">如何有效地使用t-SNE</h2><div class="om l"><h3 class="bd b gy z fp ok fr fs ol fu fw dk translated">一种流行的探索高维数据的方法叫做t-SNE，是由范德马滕和辛顿提出的…</h3></div><div class="on l"><p class="bd b dl z fp ok fr fs ol fu fw dk translated">蒸馏. pub</p></div></div><div class="oo l"><div class="ou l oq or os oo ot ks of"/></div></div></a></div><h1 id="1449" class="mh mb it bd mi mj mk ml mm mn mo mp mq jz mr ka ms kc mt kd mu kf mv kg mw mx bi translated">(7.)UMAP:</h1><p id="ff89" class="pw-post-body-paragraph kz la it lb b lc my ju le lf mz jx lh li na lk ll lm nb lo lp lq nc ls lt lu im bi translated">UMAP(均匀流形逼近)是一种降维技术，其工作原理类似于t-SNE，通过将高维数据集投影到可比较的低维空间。</p><p id="02d5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">UMAP在数据的原始空间中构建了一个邻居图，并试图在更低维度中找到一个相似的图。</p><blockquote class="nw nx ny"><p id="c50b" class="kz la mg lb b lc ld ju le lf lg jx lh nz lj lk ll oa ln lo lp ob lr ls lt lu im bi translated">阅读Scikit documentaion的下述文章，了解如何有效地使用UMAP:</p></blockquote><div class="oc od gp gr oe of"><a href="https://umap-learn.readthedocs.io/en/latest/basic_usage.html" rel="noopener  ugc nofollow" target="_blank"><div class="og ab fo"><div class="oh ab oi cl cj oj"><h2 class="bd iu gy z fp ok fr fs ol fu fw is bi translated">如何使用UMAP - umap 0.5文档</h2><div class="om l"><h3 class="bd b gy z fp ok fr fs ol fu fw dk translated">UMAP是一种通用的流形学习和降维算法。它旨在与…兼容</h3></div><div class="on l"><p class="bd b dl z fp ok fr fs ol fu fw dk translated">umap-learn.readthedocs.io</p></div></div><div class="oo l"><div class="ov l oq or os oo ot ks of"/></div></div></a></div><h1 id="f0a2" class="mh mb it bd mi mj mk ml mm mn mo mp mq jz mr ka ms kc mt kd mu kf mv kg mw mx bi translated">(9.)自动编码器:</h1><p id="d242" class="pw-post-body-paragraph kz la it lb b lc my ju le lf mz jx lh li na lk ll lm nb lo lp lq nc ls lt lu im bi translated">自动编码器是一种基于单层感知器的降维方法。它有两个组成部分:压缩(编码器)和扩展(解码器)。输入和输出层中的节点数量是相同的，而中间层与输入和输出层相比具有较少的神经元。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ow"><img src="../Images/692a4ce85ce422c7c3b3636db3b7d083.png" data-original-src="https://miro.medium.com/v2/resize:fit:1056/format:webp/1*BWjSn3tZuEIbJ1ECo7AevQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">(<a class="ae ky" href="https://en.wikipedia.org/wiki/Autoencoder#/media/File:Autoencoder_schema.png" rel="noopener ugc nofollow" target="_blank">图像源</a>)，自动编码器架构</p></figure><p id="f411" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">数据集被传递到自动编码器神经网络模型，并被编码到较低维度的隐藏层。然后，它尝试从简化的编码中生成，以获得尽可能接近其原始输入的表示。中间层是降低到相当低的维度的向量。</p></div><div class="ab cl ox oy hx oz" role="separator"><span class="pa bw bk pb pc pd"/><span class="pa bw bk pb pc pd"/><span class="pa bw bk pb pc"/></div><div class="im in io ip iq"><h1 id="ec37" class="mh mb it bd mi mj pe ml mm mn pf mp mq jz pg ka ms kc ph kd mu kf pi kg mw mx bi translated">结论:</h1><p id="b47f" class="pw-post-body-paragraph kz la it lb b lc my ju le lf mz jx lh li na lk ll lm nb lo lp lq nc ls lt lu im bi translated">在本文中，我们讨论了基于特征选择的降维方法、基于组件的降维技术、基于投影的方法，最后讨论了基于神经网络的自动编码器。</p><p id="8530" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">ISOMAP是另一种基于投影的降维方法，其工作原理类似于UMAP和t-SNE。SVD和ISO是其他一些基于组件的降维技术。</p><h1 id="603b" class="mh mb it bd mi mj mk ml mm mn mo mp mq jz mr ka ms kc mt kd mu kf mv kg mw mx bi translated">参考资料:</h1><p id="8945" class="pw-post-body-paragraph kz la it lb b lc my ju le lf mz jx lh li na lk ll lm nb lo lp lq nc ls lt lu im bi translated">[1] Scikit-learn文档:<a class="ae ky" href="https://scikit-learn.org/stable/modules/unsupervised_reduction.html" rel="noopener ugc nofollow" target="_blank">https://sci kit-learn . org/stable/modules/unsupervised _ reduction . html</a></p><blockquote class="pj"><p id="5cd7" class="pk pl it bd pm pn po pp pq pr ps lu dk translated">感谢您的阅读</p></blockquote></div></div>    
</body>
</html>