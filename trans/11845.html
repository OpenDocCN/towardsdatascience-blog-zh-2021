<html>
<head>
<title>Modeling uncertainty in neural networks with TensorFlow Probability</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用张量流概率模拟神经网络中的不确定性</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/modeling-uncertainty-in-neural-networks-with-tensorflow-probability-391b29538a7a?source=collection_archive---------3-----------------------#2021-11-26">https://towardsdatascience.com/modeling-uncertainty-in-neural-networks-with-tensorflow-probability-391b29538a7a?source=collection_archive---------3-----------------------#2021-11-26</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="095e" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsdatascience.com/tagged/hands-on-tutorials" rel="noopener" target="_blank">实践教程</a></h2><div class=""/><div class=""><h2 id="a221" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">第 4 部分:完全概率化</h2></div><p id="2389" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated"><em class="ln">本系列是使用张量流概率库对不确定性建模的简要介绍。我是作为我的</em><a class="ae lo" href="https://pydata.org/global2021/schedule/presentation/13/modeling-aleatoric-and-epistemic-uncertainty-using-tensorflow-and-tensorflow-probability/" rel="noopener ugc nofollow" target="_blank"><em class="ln">PyData Global 2021 talk</em></a><em class="ln">关于神经网络中不确定性估计的补充材料写的。</em></p><p id="fbe3" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated"><strong class="kt jd">系列文章:</strong></p><ul class=""><li id="3746" class="lp lq it kt b ku kv kx ky la lr le ls li lt lm lu lv lw lx bi translated"><a class="ae lo" rel="noopener" target="_blank" href="/modeling-uncertainty-in-neural-networks-with-tensorflow-probability-part-1-an-introduction-2bb564c67d6"> <strong class="kt jd">第一部分:</strong>一个简介</a></li><li id="3dcf" class="lp lq it kt b ku ly kx lz la ma le mb li mc lm lu lv lw lx bi translated"><a class="ae lo" rel="noopener" target="_blank" href="/modeling-uncertainty-in-neural-networks-with-tensorflow-probability-a706c2274d12"> <strong class="kt jd">第二部分</strong>:任意不确定性</a></li><li id="24cb" class="lp lq it kt b ku ly kx lz la ma le mb li mc lm lu lv lw lx bi translated"><a class="ae lo" rel="noopener" target="_blank" href="/modeling-uncertainty-in-neural-networks-with-tensorflow-probability-d519a4426e9c"> <strong class="kt jd">第三部分</strong>:认知的不确定性</a></li><li id="9f79" class="lp lq it kt b ku ly kx lz la ma le mb li mc lm lu lv lw lx bi translated">第四部分:完全概率化</li></ul><figure class="me mf mg mh gt mi gh gi paragraph-image"><div class="gh gi md"><img src="../Images/b66ee525ff7f43a446d1c4a321cf8fb7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*Ul-ddrliay5N2Vk4ea1RpA.jpeg"/></div><p class="ml mm gj gh gi mn mo bd b be z dk translated">图片由<a class="ae lo" href="https://www.pexels.com/@jessef11" rel="noopener ugc nofollow" target="_blank">张本渝</a>在<a class="ae lo" href="https://www.pexels.com/photo/green-mountain-painting-732548/" rel="noopener ugc nofollow" target="_blank">像素</a>拍摄</p></figure><h2 id="7f81" class="mp mq it bd mr ms mt dn mu mv mw dp mx la my mz na le nb nc nd li ne nf ng iz bi translated">介绍</h2><p id="d323" class="pw-post-body-paragraph kr ks it kt b ku nh kd kw kx ni kg kz la nj lc ld le nk lg lh li nl lk ll lm im bi nm translated"><span class="l nn no np bm nq nr ns nt nu di">我们</span>走了这么远的路！今天，是我们最后一节课的时间了。我们将使用我们获得的所有知识，并将其应用于一个新的更具挑战性的数据集。</p><p id="33f7" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">让我们把手弄脏吧！</p><h2 id="e0c9" class="mp mq it bd mr ms mt dn mu mv mw dp mx la my mz na le nb nc nd li ne nf ng iz bi translated">数据</h2><p id="cfab" class="pw-post-body-paragraph kr ks it kt b ku nh kd kw kx ni kg kz la nj lc ld le nk lg lh li nl lk ll lm im bi translated">本周，我们将建立一个非线性异方差数据集。这个问题比我们之前看到的问题更难建模。</p><p id="75de" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">我们将使用以下流程来生成数据:</p><figure class="me mf mg mh gt mi gh gi paragraph-image"><div role="button" tabindex="0" class="nw nx di ny bf nz"><div class="gh gi nv"><img src="../Images/cbdeb276ad167b1096765932991bfa9b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*w0BhvOrQ1_-X_92l6ptTHw.png"/></div></div><p class="ml mm gj gh gi mn mo bd b be z dk translated">我们的数据生成过程。真实的你的形象。</p></figure><p id="6f3f" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">让我们导入库，构建数据集并绘制它。</p><figure class="me mf mg mh gt mi"><div class="bz fp l di"><div class="oa ob l"/></div><p class="ml mm gj gh gi mn mo bd b be z dk translated">导入库，创建非线性训练数据并绘制它。</p></figure><figure class="me mf mg mh gt mi gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/6cf987e4bb26d8453b4146e6e742e7cb.png" data-original-src="https://miro.medium.com/v2/resize:fit:898/format:webp/1*ah5tX_M3dM3fAhhwV5n8_Q.png"/></div><p class="ml mm gj gh gi mn mo bd b be z dk translated">训练数据。请注意，<em class="od"> x </em>的值越高，方差就越大。这是异方差的一个例子。数据也是非线性的。真实的你的形象。</p></figure><p id="0ba1" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">如你所见，数据确实是非线性的，并且是异方差的。我们需要在我们的模型中引入一些非线性，以便更好地拟合这些数据——这是扩展我们概率工具包的一个绝好机会！</p><h2 id="1fe1" class="mp mq it bd mr ms mt dn mu mv mw dp mx la my mz na le nb nc nd li ne nf ng iz bi translated">建模任意的<em class="od">和</em>认知不确定性</h2><p id="a054" class="pw-post-body-paragraph kr ks it kt b ku nh kd kw kx ni kg kz la nj lc ld le nk lg lh li nl lk ll lm im bi translated">我们的数据集已经就位。为了建立一个能够模拟任意和认知不确定性的模型，我们需要结合变化层和分布层。为了对认知不确定性建模，我们将使用两个<code class="fe oe of og oh b">tfpl.DenseVariational</code>层和一个非线性激活函数。</p><p id="9cd9" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">首先，让我们定义<em class="ln">前</em>和<em class="ln">后</em>函数。你可能还记得<a class="ae lo" rel="noopener" target="_blank" href="/modeling-uncertainty-in-neural-networks-with-tensorflow-probability-d519a4426e9c">系列的前一部分</a>，我们需要将它们传递给<code class="fe oe of og oh b">tfpl.DenseVariational</code>以使其工作。</p><p id="ae7c" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">先从<em class="ln">之前的</em>说起吧:</p><figure class="me mf mg mh gt mi"><div class="bz fp l di"><div class="oa ob l"/></div><p class="ml mm gj gh gi mn mo bd b be z dk translated">先验生成函数。</p></figure><p id="b323" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">先验生成函数必须以核大小、偏差大小和<code class="fe oe of og oh b">dtype</code>为参数(后验也是如此)。我们按照与在<a class="ae lo" rel="noopener" target="_blank" href="/modeling-uncertainty-in-neural-networks-with-tensorflow-probability-d519a4426e9c">第 3 部分</a>中相同的方式定义先验母函数:</p><ul class=""><li id="b1af" class="lp lq it kt b ku kv kx ky la lr le ls li lt lm lu lv lw lx bi translated">我们使用一个不可训练的<code class="fe oe of og oh b">tfd.MultivariateNormalDiag</code>分布对象作为我们的<em class="ln">先验。</em></li><li id="4cac" class="lp lq it kt b ku ly kx lz la ma le mb li mc lm lu lv lw lx bi translated">我们将它包装成<code class="fe oe of og oh b">tfpl.DistributionLambda</code>，将<code class="fe oe of og oh b">tfd.MultivariateNormalDiag</code>分布对象转换成<strong class="kt jd"> Keras 兼容层</strong>。</li><li id="5c52" class="lp lq it kt b ku ly kx lz la ma le mb li mc lm lu lv lw lx bi translated">我们将这一层传递给<code class="fe oe of og oh b">tf.keras.Sequential</code>以实现与后验生成函数的一致性。</li></ul><p id="d86b" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">类似地，我们按照<a class="ae lo" rel="noopener" target="_blank" href="/modeling-uncertainty-in-neural-networks-with-tensorflow-probability-d519a4426e9c">第 3 部分</a>的过程定义后验生成函数:</p><figure class="me mf mg mh gt mi"><div class="bz fp l di"><div class="oa ob l"/></div></figure><ul class=""><li id="f483" class="lp lq it kt b ku kv kx ky la lr le ls li lt lm lu lv lw lx bi translated">我们用<code class="fe oe of og oh b">tf.keras.Sequential</code>。</li><li id="26af" class="lp lq it kt b ku ly kx lz la ma le mb li mc lm lu lv lw lx bi translated">我们希望我们的<em class="ln">后验</em>是可训练的，所以我们使用生成可训练变量的<code class="fe oe of og oh b">tfpl.VariableLayer</code>来参数化<code class="fe oe of og oh b">tfpl.MultivariateNormalTriL</code>。</li><li id="6b41" class="lp lq it kt b ku ly kx lz la ma le mb li mc lm lu lv lw lx bi translated">我们利用<a class="ae lo" rel="noopener" target="_blank" href="/modeling-uncertainty-in-neural-networks-with-tensorflow-probability-a706c2274d12">第 2 部分</a>中介绍的一种非常方便的方法<code class="fe oe of og oh b">.params_size()</code>来获得参数化<code class="fe oe of og oh b">tfpl.MultivariateNormalTriL</code>所需的精确参数数，并将该层用作我们的可训练<em class="ln">后验层。</em></li></ul><p id="6426" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">我们有一切必要的参数化我们的变化层。</p><p id="1533" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">我们现在需要做的就是堆叠两个<code class="fe oe of og oh b">tfpl.DenseVariational</code>层，添加一个非线性激活，并在它们上面放一个<code class="fe oe of og oh b">tfpl.MultivariateNormal</code>层。</p><p id="03e5" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">正如我们在<a class="ae lo" rel="noopener" target="_blank" href="/modeling-uncertainty-in-neural-networks-with-tensorflow-probability-a706c2274d12">第 2 部分</a>中看到的，为了训练一个以分布层作为最后一层的模型，我们需要一个特殊的对数似然成本函数。</p><p id="38c5" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">让我们将所有这些结合起来，定义我们的模型生成函数:</p><figure class="me mf mg mh gt mi"><div class="bz fp l di"><div class="oa ob l"/></div><p class="ml mm gj gh gi mn mo bd b be z dk translated">模型生成函数。我们的架构由两个“tfpl.DenseVariational”层和一个“tfpl.IndependentNormal”层组成。我们使用负对数似然成本函数来训练模型。</p></figure><p id="511a" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">让我们打开它。我们使用普通的<code class="fe oe of og oh b">tf.keras.Sequential</code>包装纸。我们将两层<code class="fe oe of og oh b">tfpl.DenseVariational</code>放入其中，并将一层<code class="fe oe of og oh b">tfpl.MultivariateNormal</code>放在上面。</p><p id="e6ea" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">第一个变化层有 8 个单元(或神经元)。首先，我们将输入形状定义为<code class="fe oe of og oh b">(1,)</code>。那是因为我们的<em class="ln"> X </em>是一维的。然后，我们将我们的先验和后验生成函数分别传递给<code class="fe oe of og oh b">make_prior_fn</code>和<code class="fe oe of og oh b">make_postrior_fn</code>。接下来，我们将 Kullback-Leibler 散度项归一化，通过训练样本数量的倒数对其进行加权。我们将<code class="fe oe of og oh b">kl_use_exact</code>设置为<code class="fe oe of og oh b">False</code>(在我们的例子中，我们也可以将其设置为<code class="fe oe of og oh b">True</code>)。最后，我们指定非线性激活 function⁴.</p><p id="4d7e" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">第二层的参数化类似。尽管有一些不同之处。我们没有对单元的数量进行硬编码，而是使用<code class="fe oe of og oh b">.params_size()</code>方法来确保我们获得了正确的单元数量来参数化我们的最后一个分布层。注意，我们在这里也不使用非线性。</p><p id="3533" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">我们的负对数似然损失函数与我们在<a class="ae lo" rel="noopener" target="_blank" href="/modeling-uncertainty-in-neural-networks-with-tensorflow-probability-a706c2274d12">第 2 部分</a>中定义的函数相同。</p><p id="4c8e" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">我们现在准备好训练模型了！</p><p id="a426" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">让我们这样做，并绘制损失图:</p><figure class="me mf mg mh gt mi"><div class="bz fp l di"><div class="oa ob l"/></div></figure><figure class="me mf mg mh gt mi gh gi paragraph-image"><div class="gh gi oi"><img src="../Images/c9824e73680eb27f67a63619b5ea6566.png" data-original-src="https://miro.medium.com/v2/resize:fit:868/format:webp/1*dpqRGcURLKQl3DHiS1cHHg.png"/></div><p class="ml mm gj gh gi mn mo bd b be z dk translated">我们的任意和认知不确定性估计模型的损失与时代数。真实的你的形象。</p></figure><p id="4354" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">损失看起来不错，我们可以尝试进一步优化模型，以更快地收敛，但这不是我们在这篇文章中的目标。</p><h2 id="1caa" class="mp mq it bd mr ms mt dn mu mv mw dp mx la my mz na le nb nc nd li ne nf ng iz bi translated">结果</h2><p id="42da" class="pw-post-body-paragraph kr ks it kt b ku nh kd kw kx ni kg kz la nj lc ld le nk lg lh li nl lk ll lm im bi nm translated">你的模型被训练。那是一个非常激动人心的时刻！我们准备好进行预测了。我们的预测将包含对<strong class="kt jd"> <em class="ln">任意性</em> </strong>和<strong class="kt jd"> <em class="ln">认知性</em> </strong>不确定性的估计。我们期望看到<em class="ln">一组</em> <em class="ln">最佳拟合线</em>和<em class="ln">一组</em> <em class="ln">置信区间</em>。为了使可视化更清晰，我们将稍微修改绘图代码，将置信区间表示为线条而不是阴影区域。与我们在<a class="ae lo" rel="noopener" target="_blank" href="/modeling-uncertainty-in-neural-networks-with-tensorflow-probability-d519a4426e9c">第 3 部分</a>中所做的类似，我们将计算 15 个独立预测:</p><figure class="me mf mg mh gt mi"><div class="bz fp l di"><div class="oa ob l"/></div><p class="ml mm gj gh gi mn mo bd b be z dk translated">我们为最佳拟合线及其置信区间生成 15 个预测，并绘制结果。</p></figure><figure class="me mf mg mh gt mi gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/14a639623068ab89d0a0dd5b91e3dedd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1274/format:webp/1*tpZgW4iHlKb4MWTP_-fIaA.png"/></div><p class="ml mm gj gh gi mn mo bd b be z dk translated">训练数据和 15 条最佳拟合线及其各自的置信区间。该图表示对任意(置信区间)和认知(15 组最佳拟合线及其 CIs)不确定性的估计。真实的你的形象。</p></figure><p id="87b7" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">如您所见，我们的模型在拟合数据方面做得非常好！🚀似乎非线性和异方差都建模得很好。置信区间似乎是可靠的，覆盖了大约 95%的点。</p><h2 id="fd4d" class="mp mq it bd mr ms mt dn mu mv mw dp mx la my mz na le nb nc nd li ne nf ng iz bi translated">摘要</h2><p id="a496" class="pw-post-body-paragraph kr ks it kt b ku nh kd kw kx ni kg kz la nj lc ld le nk lg lh li nl lk ll lm im bi translated">在这一集的<em class="ln">用张量流概率</em> <strong class="kt jd"> <em class="ln"> </em> </strong>对神经网络中的不确定性建模中，我们看到了如何使用单一模型对<em class="ln">任意性</em>和<em class="ln">认知不确定性</em>进行建模。</p><p id="ae82" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">我们生成了一个非线性和异方差数据集，并使用具有非线性激活的更深层次架构对其建模。我们更新了如何定义参数化变化层所需的<em class="ln">先验</em>和<em class="ln">后验</em>分布。最后，我们已经看到了如何使用一个变化层来参数化一个最终的概率分布层。</p><p id="269f" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">这是<em class="ln">用张量流概率</em> <strong class="kt jd"> <em class="ln"> </em> </strong>系列对神经网络中的不确定性建模的最后一篇文章。祝贺你完成这个系列！🎉</p><h2 id="655d" class="mp mq it bd mr ms mt dn mu mv mw dp mx la my mz na le nb nc nd li ne nf ng iz bi translated">谢谢大家！</h2><p id="8f07" class="pw-post-body-paragraph kr ks it kt b ku nh kd kw kx ni kg kz la nj lc ld le nk lg lh li nl lk ll lm im bi translated">如果您对本系列涵盖的材料有任何疑问，或者对您希望在未来阅读的主题有任何建议，请随时告诉我:</p><ul class=""><li id="9519" class="lp lq it kt b ku kv kx ky la lr le ls li lt lm lu lv lw lx bi translated">在评论中</li><li id="99e0" class="lp lq it kt b ku ly kx lz la ma le mb li mc lm lu lv lw lx bi translated">在<a class="ae lo" href="https://www.linkedin.com/in/aleksandermolak/" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>上给我发消息</li><li id="82fb" class="lp lq it kt b ku ly kx lz la ma le mb li mc lm lu lv lw lx bi translated">通过我的<a class="ae lo" href="https://alxndr.io/" rel="noopener ugc nofollow" target="_blank">网页</a>上列出的任何其他渠道联系</li></ul><p id="51f3" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">谢谢大家！</p></div><div class="ab cl ok ol hx om" role="separator"><span class="on bw bk oo op oq"/><span class="on bw bk oo op oq"/><span class="on bw bk oo op"/></div><div class="im in io ip iq"><h2 id="af44" class="mp mq it bd mr ms mt dn mu mv mw dp mx la my mz na le nb nc nd li ne nf ng iz bi translated">脚注</h2><p id="e3c8" class="pw-post-body-paragraph kr ks it kt b ku nh kd kw kx ni kg kz la nj lc ld le nk lg lh li nl lk ll lm im bi translated">理论上，我们可以通过使用一个变分过程作为模型的最后一层来达到更高的概率。但是这超出了本系列的范围。</p><p id="cbad" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">你可能还记得，我们将这些函数作为函数对象传递，而不调用它们。</p><p id="02c1" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">请参考<a class="ae lo" rel="noopener" target="_blank" href="/modeling-uncertainty-in-neural-networks-with-tensorflow-probability-d519a4426e9c">第 3 部分</a>进行解释。</p><p id="7f7f" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">⁴:我们在例子中使用了 sigmoid 函数。如果我们使用 ReLU，模型拟合看起来会不同吗？如果有，为什么？欢迎在评论中告诉我。</p><p id="e2b1" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi">________________</p><p id="bfe0" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">❤️ <em class="ln">有兴趣获得更多这样的内容吗？使用此链接加入:</em></p><div class="or os gp gr ot ou"><a href="https://aleksander-molak.medium.com/membership" rel="noopener follow" target="_blank"><div class="ov ab fo"><div class="ow ab ox cl cj oy"><h2 class="bd jd gy z fp oz fr fs pa fu fw jc bi translated">通过我的推荐链接加入媒体-亚历山大·莫拉克</h2><div class="pb l"><h3 class="bd b gy z fp oz fr fs pa fu fw dk translated">作为一个媒体会员，你的会员费的一部分会给你阅读的作家，你可以完全接触到每一个故事…</h3></div><div class="pc l"><p class="bd b dl z fp oz fr fs pa fu fw dk translated">aleksander-molak.medium.com</p></div></div><div class="pd l"><div class="pe l pf pg ph pd pi mj ou"/></div></div></a></div><p id="4f0f" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">谢谢大家！</p><p id="a3c9" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi">_______________</p></div></div>    
</body>
</html>