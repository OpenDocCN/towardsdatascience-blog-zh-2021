<html>
<head>
<title>Implementation Differences in LSTM Layers: TensorFlow vs PyTorch</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">LSTM层的实现差异:TensorFlow与PyTorch</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/implementation-differences-in-lstm-layers-tensorflow-vs-pytorch-77a31d742f74?source=collection_archive---------5-----------------------#2021-04-25">https://towardsdatascience.com/implementation-differences-in-lstm-layers-tensorflow-vs-pytorch-77a31d742f74?source=collection_archive---------5-----------------------#2021-04-25</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="1327" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">在张量流LSTM层和PyTorch LSTM层之间画平行线。</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/4542fb461d3960a8885c11fca0f632f4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Kp4XYg7obqvn4VVjOXi7-g.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">照片由<a class="ae kv" href="https://www.pexels.com/@anna-nekrashevich" rel="noopener ugc nofollow" target="_blank">安娜·涅克拉舍维奇</a>从<a class="ae kv" href="https://www.pexels.com/photo/pen-business-eyewear-research-6801648/" rel="noopener ugc nofollow" target="_blank">派克斯</a>拍摄</p></figure><p id="582f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">Tensorflow和Pytorch是深度学习中使用最广泛的两个库。当涉及到实现神经网络时，这两个库有不同的方法。这两个库都有很大的不同。通常，TF由于其更好的优化而更倾向于开发生产就绪的模型，Pytorch由于其更“pythonic化”的语法和急切的执行而更倾向于研究工作。但是有了Torchscript和TF 2.0，两个库的差距缩小了。</p><p id="350b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这两个图书馆都有很好的社区支持和积极的贡献。因此，我们可以很容易地实现不同类型的神经网络，而在这两个库中没有任何重大问题。但是这两个库在架构上有所不同。因此，在实现神经网络时，我们需要关注某些差异。</p><p id="1a38" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">其中一个区别是层API。在这两个库中，可以使用子类方法或顺序模型API方法来设计神经网络(NN)。子类化方法是这两种方法中更受欢迎的，因为它是面向对象的和可扩展的。在这些库中实现神经网络时，我们可以使用已经设计好的层——线性(完全连接)层、卷积层、递归层等。并将它们扩展到我们的模型中。在TF中，我们使用<code class="fe ls lt lu lv b">tensorflow.keras.layers</code>，在Pytorch中，我们使用<code class="fe ls lt lu lv b">torch.nn</code>来访问这些层。正如我前面提到的，这些层的实现有细微的差别。大多数时候，它们都是次要的，直观的。但是在LSTM(长短期记忆)层，这些差异有些主要和显著。在LSTM图层中，Pytorch和TF的图层参数化方式、参数的默认值以及图层的默认输出都有很大不同。</p><p id="5232" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在这篇文章中，我将尝试解释这两个库中LSTM层的区别。如果有人提到在一个库中实现的NN(带有LSTM层)并试图在另一个库中复制它，我希望这能有所帮助。</p></div><div class="ab cl lw lx hu ly" role="separator"><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb"/></div><div class="ij ik il im in"><h1 id="c16a" class="md me iq bd mf mg mh mi mj mk ml mm mn jw mo jx mp jz mq ka mr kc ms kd mt mu bi translated">LSTM介绍</h1><blockquote class="mv mw mx"><p id="6084" class="kw kx my ky b kz la jr lb lc ld ju le mz lg lh li na lk ll lm nb lo lp lq lr ij bi translated">我添加这一部分只是为了完善和复习(如果有人需要的话)。但是如果你正在尝试理解使用LSTM层时的实现差异，那么我希望你已经有了深度学习的背景，并且知道LSTMs的基础知识。所以你可以跳过这一部分。</p></blockquote><p id="38bf" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">递归神经网络(RNN)是一种特殊类型的神经网络，用于从序列数据中学习。传统的神经网络会接受一个输入，并仅基于该输入给出预测。它不会查看以前的输入/输出并做出决定。但是当预测序列数据中的下一个值时(例如，句子，一个城市的日平均温度)，我们不能只看当前的数据点。我们必须把这个系列的行为(到目前为止)作为一个整体来看待，并获得“系列的背景”来做出有意义的预测。这就是rnn的专业用途。rnn用于诸如语言建模、机器翻译、序列预测等任务。</p><p id="d8c3" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">图1示出了RNN如何将当前值和先前的横向输出作为输入。递归机制传递前一时间步的横向输出(如<code class="fe ls lt lu lv b">aᵗ</code>)。预计(并证明)RNN层将学会捕捉横向输出中序列的“上下文”。请注意，为了更好地理解，图1 (b)中所示的RNN的展开版本将<strong class="ky ir">相同的</strong> <strong class="ky ir">层</strong>描绘为副本，并不表示多个层。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nc"><img src="../Images/6f9fc717c307a1fb4c2896288589d842.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CQrovkr1mK2njvnKgsZCPQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图1 — (a)具有环路的一般rnn(z⁻表示单位时间延迟)。(b)相同的RNN，时间序列循环展开(由作者绘制，灵感来自<a class="ae kv" href="https://colah.github.io/posts/2015-08-Understanding-LSTMs/" rel="noopener ugc nofollow" target="_blank"> Chris </a>的作品)</p></figure><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nd ne l"/></div></figure><p id="d0b4" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">RNN的设计原则期望从系列中较早的值得到的“上下文”沿着序列持续很长一段时间。但是在实际场景中观察到，RNN人学习序列“上下文”的能力随着距离的增加而减弱。也就是说，rnn不足以捕获序列中的长期依赖性。长短期记忆(LSTM)网络是RNN的一个特殊版本，它的引入是为了在序列中保存长期的“上下文”信息。LSTMs的固有设计使它们能够通过所谓的单元状态(用<code class="fe ls lt lu lv b">cᵗ</code>表示)来捕捉长期环境。下图(图2)显示了一个典型的LSTM图层。我使用的图表是基于Chris Olah在关于<a class="ae kv" href="https://colah.github.io/posts/2015-08-Understanding-LSTMs/" rel="noopener ugc nofollow" target="_blank">理解LSTMs </a>的博客文章中使用的图表。(很精彩的博文。一定要去看看。)</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nf"><img src="../Images/e664a06f2437ac1885fe9cfc2cb15dbc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ACJoB7r1T4qNUQulNqQqLQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图2——一个基本的LSTM图层(由作者绘制，灵感来自克里斯的作品)</p></figure><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nd ne l"/></div></figure><p id="cd2b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">正如Chris在他的博客中解释的那样，细胞状态可以被认为是一个传送带，它从序列的开始到结束携带着长期的“上下文”。在基于当前输入(<code class="fe ls lt lu lv b"><em class="my">x</em>ᵗ</code>)和先前输出(<code class="fe ls lt lu lv b">hᵗ⁻¹</code>)的每个时间步，LSTM层决定从“上下文”中忘记什么信息以及将什么信息添加到“上下文”中。遗忘由遗忘门处理(<code class="fe ls lt lu lv b">Γ𝒻</code>)。<code class="fe ls lt lu lv b">Γ𝒻</code>可以被认为是一个应用于<code class="fe ls lt lu lv b">cᵗ⁻¹</code>的面具。如果<code class="fe ls lt lu lv b">Γ𝒻</code>中某个位置的值为0(或更接近)，当与<code class="fe ls lt lu lv b">cᵗ⁻¹</code>相乘时，从上下文中删除该位置的信息。如果掩码的值在某个其他位置为1(或更接近)，在与<code class="fe ls lt lu lv b">cᵗ⁻¹</code>相乘后，它允许该位置的信息在上下文中保持不变。操作<code class="fe ls lt lu lv b">Γ𝒻.*cᵗ⁻¹</code> ( <em class="my">)。* —逐元素乘法</em>)确保上下文中不需要的部分被忽略。向单元状态添加新信息分两步处理。首先，基于<code class="fe ls lt lu lv b"><em class="my">x</em>ᵗ</code>和<code class="fe ls lt lu lv b">hᵗ⁻¹</code>，通过激活<code class="fe ls lt lu lv b">tanh</code>的子层创建候选向量(<code class="fe ls lt lu lv b">c̃ᵗ</code>)。然后，更新门(<code class="fe ls lt lu lv b">Γᵤ</code>)生成一个掩码(就像在遗忘门中一样)，该掩码决定将候选向量的哪一部分添加到单元状态中。然后将<code class="fe ls lt lu lv b">Γᵤ.*c̃ᵗ</code>加到<code class="fe ls lt lu lv b">cᵗ⁻¹</code>上生成<code class="fe ls lt lu lv b">cᵗ</code>。最后，为了生成输出，我们让<code class="fe ls lt lu lv b">cᵗ </code>通过一个<code class="fe ls lt lu lv b">tanh</code>非线性并使用它。但是为了决定给出哪一部分，我们使用了更新门(<code class="fe ls lt lu lv b">Γₒ</code>)。<code class="fe ls lt lu lv b">Γₒ.*tanh(cᵗ)</code>作为LSTM层在时间步长<code class="fe ls lt lu lv b">t</code>的输出给出。</p><p id="ca24" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">请注意，<code class="fe ls lt lu lv b">cᵗ⁻¹</code>和<code class="fe ls lt lu lv b">hᵗ⁻¹</code>都是作为LSTM层的横向输入给出的，相比之下，普通RNN层只给出了<code class="fe ls lt lu lv b">aᵗ⁻¹</code>。</p><p id="f8f8" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">如果你需要更多关于RNN和LSTM原理的信息，我会推荐你去吴恩达的<a class="ae kv" href="https://www.coursera.org/learn/nlp-sequence-models" rel="noopener ugc nofollow" target="_blank">序列模型</a>课程的第一周(可以免费旁听)并阅读克里斯·奥拉的这篇精彩的<a class="ae kv" href="https://colah.github.io/posts/2015-08-Understanding-LSTMs/" rel="noopener ugc nofollow" target="_blank">博客文章</a>。</p></div><div class="ab cl lw lx hu ly" role="separator"><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb"/></div><div class="ij ik il im in"><h1 id="ad3d" class="md me iq bd mf mg mh mi mj mk ml mm mn jw mo jx mp jz mq ka mr kc ms kd mt mu bi translated">张量流中的LSTM层</h1><blockquote class="mv mw mx"><p id="b504" class="kw kx my ky b kz la jr lb lc ld ju le mz lg lh li na lk ll lm nb lo lp lq lr ij bi translated">在撰写本文时，Tensorflow版本是2.4.1</p></blockquote><p id="2664" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在TF中，我们可以使用<code class="fe ls lt lu lv b">tf.keras.layers.LSTM</code>并创建一个LSTM层。初始化LSTM层时，唯一需要的参数是<code class="fe ls lt lu lv b">units</code>。参数<code class="fe ls lt lu lv b">units</code>对应于该层的输出特征数量。用我们的术语来说就是<code class="fe ls lt lu lv b">units</code> = <code class="fe ls lt lu lv b">nₕ</code>。<code class="fe ls lt lu lv b">nₓ</code>将根据前一层的输出进行推断。因此，该库可以初始化LSTM层中的所有权重和偏置项。</p><p id="b140" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">TF LSTM层期望一个三维张量作为正向传播期间的输入。这个输入应该是<code class="fe ls lt lu lv b">(batch, timesteps, input_features)</code>的形状。这显示在下面的代码片段中。假设我们正在使用这个LSTM层来训练一个语言模型。我们的输入将是句子。第一个维度对应于我们使用多少个句子作为一批来训练模型。第二个维度对应于一个这样的句子中有多少单词。在实际设置中，每个句子的字数因句而异。因此，为了批量处理这些句子，我们可以选择训练语料库中最长句子的长度作为这个维度，并用尾随零填充其他句子。最后一个维度对应于用于表示每个单词的特征的数量。为了简单起见，如果我们说，我们正在使用一个热编码，并且在我们的词汇表中有10000个单词，那么这个维度将是10000。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nd ne l"/></div></figure><p id="374f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">但是在初始化层的时候，如果我们设置了<code class="fe ls lt lu lv b">time_major = True</code>，那么输入将会在shape - <code class="fe ls lt lu lv b">(timesteps, batch, feature)</code>中被期望。</p><p id="84c5" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">从上面的代码片段可以看出，LSTM的输出(带有默认参数)是形状<code class="fe ls lt lu lv b">(32,4)</code>，它对应于<code class="fe ls lt lu lv b">(batch, output_features)</code>。因此，如果我们回到语言模型的例子，输出每个句子有一个向量，每个句子有<code class="fe ls lt lu lv b">nₕ</code>个特征(<code class="fe ls lt lu lv b">nₕ</code> = <code class="fe ls lt lu lv b">units</code> =输出特征的数量)。这一个向量(每个句子)是对应于最后时间步长<code class="fe ls lt lu lv b">T</code>(句子的最后一个单词)的LSTM层的输出。这个输出在我们的符号中是<code class="fe ls lt lu lv b">hᵀ</code>。这在图3中进行了描述。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ng"><img src="../Images/e81c6c285066a77f10983deb5aa80aa0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*l9k82VuwtSrdg_a6EF2gMA.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图3-tensor flow LSTM图层的默认输出(图表由作者提供)</p></figure><p id="faf9" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">但是，如果我们想要堆叠多个LSTM图层，下一个图层也需要一个时间序列作为输入。在这种情况下，我们可以在初始化层时设置<code class="fe ls lt lu lv b">return_sequences=True</code>。那么输出将是对应于<code class="fe ls lt lu lv b">(batch, timesteps, output_features)</code>的形状<code class="fe ls lt lu lv b">(32,10,4)</code>。如果<code class="fe ls lt lu lv b">return_sequence</code>设置为<code class="fe ls lt lu lv b">True</code>，那么<code class="fe ls lt lu lv b">hᵗ : ∀t = 1,2…T</code>将作为输出返回。这显示在下面的代码片段和图4中的第一个LSTM层。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nd ne l"/></div></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nh"><img src="../Images/d6bb35e4a860f59aacca770beb637ee9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xRe8z9Znm2CqGeeYRwfmBw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图4 —一个简单的模型，包含两个LSTM层和两个完全连接的层。注意<code class="fe ls lt lu lv b">LSTM 1</code>层输出一个序列，LSTM 2输出一个单一矢量。(作者配图)</p></figure><p id="858a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">如果我们想获得单元状态(<code class="fe ls lt lu lv b">cᵗ</code>)作为输出，我们需要在初始化层时设置<code class="fe ls lt lu lv b">return_state=True</code>。然后我们得到一个3个张量的列表作为输出。根据<a class="ae kv" href="https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTM" rel="noopener ugc nofollow" target="_blank">文档</a>，如果我们同时设置<code class="fe ls lt lu lv b">return_sequences=True</code>和<code class="fe ls lt lu lv b">return_state=True</code>，那么这三个张量将是——<code class="fe ls lt lu lv b">whole_seq_output, final_memory_state,</code>和<code class="fe ls lt lu lv b">final_carry_state</code>。这显示在下面的代码片段中。</p><p id="010e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在我们的符号中，</p><ul class=""><li id="42dc" class="ni nj iq ky b kz la lc ld lf nk lj nl ln nm lr nn no np nq bi translated"><code class="fe ls lt lu lv b">whole_seq_output</code> —所有时间步长对应的输出。<br/><code class="fe ls lt lu lv b">hᵗ : ∀t = 1,2…T</code>；形状— <code class="fe ls lt lu lv b">(batch, timesteps, output_features)</code></li><li id="ed5e" class="ni nj iq ky b kz nr lc ns lf nt lj nu ln nv lr nn no np nq bi translated"><code class="fe ls lt lu lv b">final_memory_state</code> —对应于最后一个时间步长的输出。<br/><code class="fe ls lt lu lv b">hᵀ</code>；形状— <code class="fe ls lt lu lv b">(batch, output_features)</code></li><li id="36a4" class="ni nj iq ky b kz nr lc ns lf nt lj nu ln nv lr nn no np nq bi translated"><code class="fe ls lt lu lv b">final_carry_state</code> —最后一个单元格状态。<br/>T1；形状— <code class="fe ls lt lu lv b">(batch, output_features)</code></li></ul><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nd ne l"/></div></figure><p id="acc3" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">如果我们设置<code class="fe ls lt lu lv b">return_sequences=False</code>和<code class="fe ls lt lu lv b">return_state=True</code>，那么这三个张量就是——<code class="fe ls lt lu lv b">final_memory_state, final_memory_state, </code>和<code class="fe ls lt lu lv b">final_carry_state</code>。</p><p id="7917" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">单个LSTM层有五个使用激活函数的地方。但是如果我们查看参数，我们只看到两个参数来设置激活函数— <code class="fe ls lt lu lv b">activation</code>和<code class="fe ls lt lu lv b">recurrent_activation</code>。如果我们给<code class="fe ls lt lu lv b">activation</code>参数设置一个值，它会改变应用于候选向量的激活和应用于单元状态的激活，就在与输出门进行逐元素乘法之前。将值设置为<code class="fe ls lt lu lv b">recurrent_activation</code>将改变忽略门、更新门和输出门的激活功能。</p><p id="7ad8" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">其他参数很容易理解或者很少使用。还有一点需要注意的是，我们可以设置<code class="fe ls lt lu lv b">unroll=True</code>，网络就展开了。这将加快训练过程，但会占用大量内存(因为同一层会被复制多次)。</p><p id="6a44" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">下面的代码片段使用TF实现了图4所示的模型。注意每层的输出形状和每层中可训练参数的数量。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nd ne l"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图4中模型的张量流实现。</p></figure></div><div class="ab cl lw lx hu ly" role="separator"><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb"/></div><div class="ij ik il im in"><h1 id="5bb5" class="md me iq bd mf mg mh mi mj mk ml mm mn jw mo jx mp jz mq ka mr kc ms kd mt mu bi translated">Pytorch的LSTM层</h1><blockquote class="mv mw mx"><p id="c22f" class="kw kx my ky b kz la jr lb lc ld ju le mz lg lh li na lk ll lm nb lo lp lq lr ij bi translated">在撰写本文时，Pytorch版本是1.8.1</p></blockquote><p id="1a2e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在Pytorch中，可以使用<code class="fe ls lt lu lv b">torch.nn.LSTM</code>创建一个LSTM层。初始化时需要两个参数<code class="fe ls lt lu lv b">input_size</code>和<code class="fe ls lt lu lv b">hidden_size</code>。<code class="fe ls lt lu lv b">input_size</code>和<code class="fe ls lt lu lv b">hidden_size</code>分别对应于该层的输入特征数和该层的输出特征数。在我们的术语中，<code class="fe ls lt lu lv b">hidden_size</code> = <code class="fe ls lt lu lv b">nₕ</code>和<code class="fe ls lt lu lv b">input_size</code> = <code class="fe ls lt lu lv b">nₓ</code>。</p><p id="5740" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在正向传播期间，Pytorch LSTM层期望一个三维张量作为输入(类似于TF)。但是维度的默认顺序发生了变化。输入张量的形状应该是<code class="fe ls lt lu lv b">(timesteps, batch, input_features)</code>。如果想得到和TF一样的维数阶，就应该在层初始化的时候设置<code class="fe ls lt lu lv b">batch_first=True</code>。</p><p id="b208" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">Pytorch LSTM API中可以看到的另一个主要区别是，在启动时，我们可以设置<code class="fe ls lt lu lv b">num_layers=k</code>并启动一个作为单个对象堆叠的<code class="fe ls lt lu lv b">k</code> LSTM层块。然而，我个人不喜欢这种方法，因为它使得整个实现可读性和可维护性更差。</p><p id="67f2" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">下一个大的区别是Pytorch LSTM图层的输出。Pytorch LSTM图层的输出是包含两个元素的元组。元组的第一个元素是形状为<code class="fe ls lt lu lv b">(timesteps, batch, output_features)</code>的所有时间步长(<code class="fe ls lt lu lv b">hᵗ : ∀t = 1,2…T</code>)对应的LSTM输出。元组的第二个元素是具有两个元素的另一个元组。这个二元组的第一个元素是对应于最后一个时间步长的输出(<code class="fe ls lt lu lv b">hᵀ</code>)。它的形状是<code class="fe ls lt lu lv b">(1, batch, output_features)</code>。这个第二元组的第二个元素是对应于最后一个时间步长的单元状态(<code class="fe ls lt lu lv b">cᵀ</code>)。它也有形状<code class="fe ls lt lu lv b">(1, batch, output_features)</code>。如果我们已经通过设置<code class="fe ls lt lu lv b">num_layers=k</code>将LSTM初始化为堆叠层的块，那么<code class="fe ls lt lu lv b">hᵀ</code>和<code class="fe ls lt lu lv b">cᵀ</code>将具有形状<code class="fe ls lt lu lv b">(k, batch, output_features)</code>。这里，<code class="fe ls lt lu lv b">hᵀ</code>和<code class="fe ls lt lu lv b">cᵀ</code>都具有堆栈中所有k层的最后状态。进一步在初始化时，如果我们设置了<code class="fe ls lt lu lv b">batch_first=True</code>，那么<code class="fe ls lt lu lv b">timesteps</code>和<code class="fe ls lt lu lv b">batch</code>尺寸将在输出中交换(类似于输入)。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nd ne l"/></div></figure><p id="f795" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">据我所知，在Pytorch中改变LSTM层内部的激活函数是不可能的。此外，不可能限制LSTM层只给出一个输出(如在TF中)。但是，我们可以将输出分配给变量，使用所需的输出，忽略其他输出，如下面的代码段所示。除此之外，其他参数应该是不言自明的。</p><p id="5445" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">注意，如果你设置LSTM层是双向的(我在这篇文章中没有谈到)，那么输出形状将与我上面提到的不同。请参考<a class="ae kv" href="https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html" rel="noopener ugc nofollow" target="_blank">文档</a>了解这种情况。</p><p id="591f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">下面的代码片段使用Pytorch实现了图4所示的模型。注意每层的输出形状和每层中可训练参数的数量。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nd ne l"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图4中模型的Pytorch实现。</p></figure><p id="52fa" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">如果我们观察图4中模型的两种实现中的参数数量。可以观察到，LSTM层中的参数数量存在差异。这又是一个设计选择。Pytorch在实现LSTM方程(1)、(2)、(3)和(4)时做了微小的改变。TF在每个方程中加入一个偏置向量(如我们的方程中所示)。但是Pytorch(如这里的<a class="ae kv" href="https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html" rel="noopener ugc nofollow" target="_blank">所示</a>)为每个等式添加了<strong class="ky ir">两个偏置向量</strong>。由于每个偏置向量的形状是<code class="fe ls lt lu lv b">(nₕ,1)</code>并且每层有四个这样的附加向量，因此Pytorch LSTM层中将有更多的<code class="fe ls lt lu lv b">4*nₕ</code>个参数。在<code class="fe ls lt lu lv b">LSTM1</code>层<code class="fe ls lt lu lv b">nₕ=8</code>，所以有32个附加参数。在<code class="fe ls lt lu lv b">LSTM2</code>层<code class="fe ls lt lu lv b">nₕ=4</code>，所以多了16个参数。</p></div><div class="ab cl lw lx hu ly" role="separator"><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb"/></div><div class="ij ik il im in"><h1 id="14ef" class="md me iq bd mf mg mh mi mj mk ml mm mn jw mo jx mp jz mq ka mr kc ms kd mt mu bi translated">参考</h1><div class="nw nx gp gr ny nz"><a href="https://colah.github.io/posts/2015-08-Understanding-LSTMs/" rel="noopener  ugc nofollow" target="_blank"><div class="oa ab fo"><div class="ob ab oc cl cj od"><h2 class="bd ir gy z fp oe fr fs of fu fw ip bi translated">了解LSTM网络</h2><div class="og l"><p class="bd b dl z fp oe fr fs of fu fw dk translated">colah.github.io</p></div></div></div></a></div><div class="nw nx gp gr ny nz"><a href="https://www.coursera.org/learn/nlp-sequence-models" rel="noopener  ugc nofollow" target="_blank"><div class="oa ab fo"><div class="ob ab oc cl cj od"><h2 class="bd ir gy z fp oe fr fs of fu fw ip bi translated">序列模型</h2><div class="og l"><p class="bd b dl z fp oe fr fs of fu fw dk translated">www.coursera.org</p></div></div><div class="oh l"><div class="oi l oj ok ol oh om kp nz"/></div></div></a></div><div class="nw nx gp gr ny nz"><a href="https://medium.com/analytics-vidhya/demystifying-lstm-weights-and-biases-dimensions-c47dbd39b30a" rel="noopener follow" target="_blank"><div class="oa ab fo"><div class="ob ab oc cl cj od"><h2 class="bd ir gy z fp oe fr fs of fu fw ip bi translated">揭秘LSTM权重和偏见维度。</h2><div class="og l"><p class="bd b dl z fp oe fr fs of fu fw dk translated">medium.com</p></div></div><div class="oh l"><div class="on l oj ok ol oh om kp nz"/></div></div></a></div><div class="nw nx gp gr ny nz"><a href="https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTM" rel="noopener  ugc nofollow" target="_blank"><div class="oa ab fo"><div class="ob ab oc cl cj od"><h2 class="bd ir gy z fp oe fr fs of fu fw ip bi translated">TF . keras . layers . lstm | tensor flow Core v 2 . 4 . 1</h2><div class="oo l"><h3 class="bd b gy z fp oe fr fs of fu fw dk translated">长短期记忆层- Hochreiter 1997。</h3></div><div class="og l"><p class="bd b dl z fp oe fr fs of fu fw dk translated">www.tensorflow.org</p></div></div></div></a></div><div class="nw nx gp gr ny nz"><a href="https://stackoverflow.com/questions/44947842/can-someone-explain-to-me-the-difference-between-activation-and-recurrent-activa" rel="noopener  ugc nofollow" target="_blank"><div class="oa ab fo"><div class="ob ab oc cl cj od"><h2 class="bd ir gy z fp oe fr fs of fu fw ip bi translated">有人能给我解释一下激活和循环激活的区别吗…</h2><div class="og l"><p class="bd b dl z fp oe fr fs of fu fw dk translated">stackoverflow.com</p></div></div><div class="oh l"><div class="op l oj ok ol oh om kp nz"/></div></div></a></div><div class="nw nx gp gr ny nz"><a href="https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html" rel="noopener  ugc nofollow" target="_blank"><div class="oa ab fo"><div class="ob ab oc cl cj od"><h2 class="bd ir gy z fp oe fr fs of fu fw ip bi translated">LSTM - PyTorch 1.8.1文件</h2><div class="og l"><p class="bd b dl z fp oe fr fs of fu fw dk translated">pytorch.org</p></div></div></div></a></div></div></div>    
</body>
</html>