<html>
<head>
<title>Sequence To Sequence Labeling with Recurrent Neural Networks</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用递归神经网络进行序列间标记</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/sequence-to-sequence-labeling-with-recurrent-neural-networks-5405716a2ffa?source=collection_archive---------22-----------------------#2021-08-13">https://towardsdatascience.com/sequence-to-sequence-labeling-with-recurrent-neural-networks-5405716a2ffa?source=collection_archive---------22-----------------------#2021-08-13</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="c9ca" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">编码器-解码器模型</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/43141e831160c4388f3b68fe88a186b1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CelBsRiUdLM4QHQkJzdFNg.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">莱昂纳多·大久保俊郎在<a class="ae ky" href="https://unsplash.com/s/photos/language-learning?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="e86f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">序列到序列的标记问题是通过算法将一个字母表上的序列映射到另一个字母表上的“好”序列。这两个字母可能不同。序列的长度也是如此。这个定义隐含的意思是，有一些方法可以区分好的和坏的映射。</p><p id="f327" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们来看一些值得注意的例子。</p><p id="dd8f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">机器翻译</strong>:将单词序列从一种自然语言翻译成另一种自然语言。(比如从英语到法语。每种语言的词汇构成了它的字母表。</p><p id="374f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对话机器人:人类输入一句话或一个问题，机器人会给出合适的回答。在此设置中，输入和输出字母是相同的。人类和机器人“说着同一种语言”。</p><p id="f722" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">摘要</strong>:输入一个长句、段落或更长的文本，并对其内容进行摘要。</p><p id="ac60" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">词性标注</strong>:输入一种语言(比如英语)的单词序列，输出单词(名词、动词、形容词等)的词性标签序列。</p><p id="ce0f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">语音识别</strong>:输入一系列音素(口语)，输出它们所代表的文本序列。</p><p id="ae20" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这个标记问题是一个机器学习问题，因为它涉及到预测给定输入序列的输出序列。这是监督学习的一种形式。也就是说，输入和输出都是<em class="lv">序列</em>。</p><p id="8c35" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">这个问题难在哪里？</strong></p><p id="785a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">下面我们重点介绍机器翻译。</p><p id="93c9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">词汇是巨大的:以英语和法语为例。英语有很多单词。法语也是。哪些英语单词对应哪些法语单词？也就是说，对于那些可绘制地图的人来说。</p><p id="c4d6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">词汇可能不可映射:两种语言可能如此不同，以至于它们的词汇甚至可能不可映射。也就是说，在一种语言中用一个词就能简明表达的东西，在另一种语言中可能需要用好几个词。</p><p id="be4c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">不仅仅是单词</strong>:想想短语<em class="lv">数据挖掘</em>。它有一个非常具体的含义，来自于两个词<em class="lv">数据</em>和<em class="lv">采矿</em>的接近。(在NLP中，这种现象被称为<em class="lv">搭配</em>。)显然，我们需要以某种方式捕捉这个意思，这样它就不会在翻译过程中丢失。</p><p id="0c93" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这只是冰山一角。更微妙的概念需要更复杂的词语序列来表达。为了让他们不在翻译过程中迷失，我们需要设法抓住他们的本质。</p><p id="3c79" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">作为监督机器学习</strong></p><p id="524e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">序列到序列标记问题可以被框架化为学习预测给定输入序列的输出序列。<strong class="lb iu"> </strong>虽然这种提法在原则上听起来很吸引人，但还是有一些关键的考虑因素。</p><p id="418f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">其中最重要的是，我们是否有足够丰富和多样化的标记训练集可用。这种训练集采取(<em class="lv">输入序列</em>、<em class="lv">输出序列</em>对)的形式。</p><p id="1bca" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">“足够富裕”的富裕程度取决于我们试图解决的问题的复杂程度。学习一个通用的英语到法语的翻译可能需要比学习回答一个受限领域的问题更广泛的训练集，比如一个机器人来回答关于一个特定产品的问题。</p><p id="e29d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">接下来，我们应该考虑哪些机器学习算法？嗯，这不是监督学习吗？所以就不能随便用什么学习算法吗？不。输入和输出是序列。长短不一。经常在巨大的字母表上。因此，输入和输出空间的宇宙是巨大的。</p><p id="4828" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">幸运的是，我们有递归神经网络(RNNs)，它是为这类问题设计的。(尽管是受约束的版本；见下文。)它们输入一个序列，输出另一个序列。它们从(输入序列，输出序列)对的训练集中学习。</p><p id="2fea" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">约束是什么？输入和输出序列的长度必须相同。当输入序列中的第I个输入出现时，必须产生输出序列中的第I个输出。我们可以说输入和输出序列是对齐的。</p><p id="e008" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">词性标注用例遵守这一约束。输入序列中的每个单词在输出序列中都有一个与之相关联的词性标签。一个单词的POS标签可能依赖于序列中的一些其他单词以及它们出现的顺序，这就是为什么我们将此建模为序列到序列的标注问题。</p><p id="ec21" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">机器翻译用例通常不会。根据语言和源序列中的具体内容，源序列和目标序列的长度可能会有所不同。用源语言表达某件事可能比用目标语言要用更多(或更少)的单词。</p><p id="a3cc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">文本摘要用例肯定不会。</p><p id="cd48" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">递归神经网络和神经语言模型</strong></p><p id="9abb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们从比对序列的简单例子开始。有两个原因。首先，如前所述，递归神经网络精确地模拟了这种情况。其次，这种简单情况的解决方案将作为更高级情况的构建模块，在这种情况下，输入和输出序列具有不同的长度。</p><p id="3f9d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">有一篇中篇文章非常详细地介绍了这种(更简单的)情况[1]。这里我们只总结关键概念。</p><p id="623b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">语言模型</strong>:在NLP中，这是记号字母表上序列的<em class="lv">概率分布</em>。语言建模的一个中心问题是从例子中学习语言模型。(例如来自句子训练集的英语句子模型。)学习的模型然后可以用于预测给定记号序列的下一个记号。正如[1]中所讨论的，语言模型有许多用途。</p><p id="d97f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">递归神经网络</strong>:这是一个神经网络，输入一个向量序列<strong class="lb iu"> x </strong> (1)，…，<strong class="lb iu"> x </strong> (T)，输出一个相应的输出向量序列<strong class="lb iu"> y </strong> (1)，…，<strong class="lb iu"> y </strong> (T)。为了输出<strong class="lb iu"> y </strong> ( <em class="lv"> t </em>)，在任何时候<em class="lv"> t </em>，RNN使用它能够从序列<strong class="lb iu"> x </strong> (1)、…、<strong class="lb iu"> x </strong> ( <em class="lv"> t </em>)中学习和捕捉到的任何东西。正是这种能力让它变得如此强大——也相当复杂。与前馈神经网络形成对比，前馈神经网络仅从<strong class="lb iu"> x </strong> ( <em class="lv"> t </em>)产生<strong class="lb iu"> y </strong> ( <em class="lv"> t </em>)。</p><p id="9b99" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">那么我们如何从向量<strong class="lb iu">x</strong>(1)…，<strong class="lb iu"> x </strong> ( <em class="lv"> t </em>)的<em class="lv">序列</em>中预测<strong class="lb iu"> y </strong> ( <em class="lv"> t </em>)？很自然的一件事就是把<strong class="lb iu"> x </strong> (1)、…、<strong class="lb iu"> x </strong> ( <em class="lv"> t </em>)中的信息汇总成一个定长向量，称之为<strong class="lb iu"> h </strong> ( <em class="lv"> t </em>)，然后从<strong class="lb iu"> h </strong> ( <em class="lv"> t </em>)预测<strong class="lb iu"> y </strong> ( <em class="lv"> t </em>)。</p><p id="163f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">可以，但是我们如何从<strong class="lb iu"> x </strong> (1)，…，<strong class="lb iu"> x </strong> ( <em class="lv"> t </em>)中获得<strong class="lb iu"> h </strong> ( <em class="lv"> t </em>)？这就是<em class="lv">循环</em>出现的原因。假设我们要从<strong class="lb iu">x</strong>(1)…，<strong class="lb iu"> x </strong> ( <em class="lv"> t </em> +1)预测<strong class="lb iu"> y </strong> ( <em class="lv"> t </em> +1)，就在我们从<strong class="lb iu">x</strong>(1)…，<strong class="lb iu"> x </strong> ( <em class="lv"> t </em>预测完<strong class="lb iu"> y </strong> ( <em class="lv"> t </em>)之后。在后面的预测中，我们使用了<strong class="lb iu"> h </strong> ( <em class="lv"> t </em>)。首先我们从<strong class="lb iu"> h </strong> (t)和<strong class="lb iu"> x </strong> ( <em class="lv"> t </em> +1)计算<strong class="lb iu"> h </strong> ( <em class="lv"> t </em> +1)。于是，h( <em class="lv"> t </em> +1)总结了<strong class="lb iu"> x </strong> (1)、…、<strong class="lb iu"> x </strong> ( <em class="lv"> t </em> +1)中的相关信息。接下来，我们像之前一样从<strong class="lb iu"> h </strong> ( <em class="lv"> t </em> +1)计算<strong class="lb iu"> y </strong> ( <em class="lv"> t </em> +1)。</p><p id="8b75" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">语言模型</strong>的RNN:这里<strong class="lb iu"> x </strong> ( <em class="lv"> t </em>)是语言字母表中某个符号的矢量编码。这代表语言中相应序列中位置<em class="lv"> t </em>处的符号。我们的兴趣是从<strong class="lb iu"> x </strong> (1)，…，<strong class="lb iu"> x </strong> (T)预测<strong class="lb iu"> x </strong> (T+1)。对于这个预测任务，将目标<strong class="lb iu"> y </strong> ( <em class="lv"> t </em>)定义为<strong class="lb iu"> x </strong> ( <em class="lv"> t </em> +1)是有意义的。</p><p id="d809" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在让我们详细说明“语言字母表中某个符号的矢量编码”。主要有两条路可走:一热编码和分布式编码。在一键编码中，字母表中的每个符号都有一个维度。符号由一个向量表示，该向量在符号维度中的值为1，在其余维度中的值为0。在分布式编码(也称为嵌入)中，符号被投影到一个低得多的维度空间中，使得出现在相似上下文中的符号在该空间中彼此靠近。当字母表是一种语言的词典，即它的单词集时，这一点得到了最好的说明。人们会认为同义词的嵌入是相互靠近的。</p><p id="c2c9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在语言模型的RNN中，输出通常是独热编码。这是因为我们的主要兴趣是从<strong class="lb iu"> x </strong> (1)，…，<strong class="lb iu"> x </strong> (T)预测<strong class="lb iu"> x </strong> (T+1)，这自然表示为</p><p id="16e3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">arg max _ s P(<strong class="lb iu">x</strong>(T+1)=编码(s) | <strong class="lb iu"> x </strong> (1)，…，<strong class="lb iu"> x </strong> (T))</p><p id="ddc8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">其中<em class="lv"> s </em>表示字母表中的符号。因此，我们需要一种方法，能够为字母表中的每个符号分配成为序列中下一个符号的概率。</p><p id="f389" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">相比之下，输入原则上可以是一次性编码或分布式编码。由于输入维数很高，一键编码将使RNN在大字母表上具有更高的容量。(这可能是好的或坏的，取决于问题和训练集。)分布式编码在某种程度上增加了学习问题的难度。另一方面，人们可以利用来自预训练模型的嵌入来解决某些问题(例如对英语的语言建模)，这可以产生改进的泛化。</p><p id="7404" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">学习</strong></p><p id="dc5d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们在这篇文章中涉及这个主题，因为它将为我们提供一种训练编码器-解码器模型的编码器组件的方法:作为源语言的语言模型。</p><p id="0baa" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">RNN的状态变化和状态到输出的行为都可以从一组(<em class="lv">输入序列</em>、<em class="lv">输出序列</em>)对的训练集中自动学习。</p><p id="3874" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们将在这里描述高层次的学习。如需更深入的了解，请参见[1]。那些已经知道如何在RNN学习的人可以跳过这一步。</p><p id="d691" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">考虑当RNN在时间<em class="lv"> t </em>接收输入向量<strong class="lb iu"> x </strong> ( <em class="lv"> t </em>)时会发生什么。首先，从该输入和<strong class="lb iu"> h </strong> ( <em class="lv"> t </em> -1)导出新的状态向量<strong class="lb iu"> h </strong> ( <em class="lv"> t </em>)。接下来，从<strong class="lb iu"> h </strong> ( <em class="lv"> t </em>)导出RNN的输出向量<strong class="lb iu"> yhat </strong> ( <em class="lv"> t </em>)。</p><p id="ed39" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在到了学习的部分。我们首先将输出向量与目标向量进行比较，即<strong class="lb iu"> yhat </strong> ( <em class="lv"> t </em>)到<strong class="lb iu"> y </strong> ( <em class="lv"> t </em>)。任何差异都会推动学习。首先，我们尝试修改状态到输出函数的可学习参数，以尝试使输出向量更接近目标向量。接下来，我们尝试修改状态转移函数的参数，以尝试将状态向量移动到更接近一个状态，从该状态产生的输出向量更接近目标向量。通常，对状态输出函数的参数和状态转移函数的参数的修改是独立完成的。就好像我们保持其中一个不变，而改变另一个，以使输出更接近目标。</p><p id="f31f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">事实上，还有更多。它与嵌入在状态转移函数中的递归有关。在这一点上，引入一些正式的概念和符号将有助于阐述。这将有助于我们涵盖“更多”的部分，也使前一段的讨论更加具体。</p><p id="3d20" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">设<strong class="lb iu">h</strong>(t)=<strong class="lb iu">f</strong>(<strong class="lb iu">h</strong>(<em class="lv">t</em>-1)，<strong class="lb iu"> x </strong> ( <em class="lv"> t </em>))。这里的<strong class="lb iu"> f </strong>是输入当前状态向量<strong class="lb iu"> h </strong> ( <em class="lv"> t </em> -1)和下一个输入向量<strong class="lb iu"> x </strong> ( <em class="lv"> t </em>)产生下一个状态向量<strong class="lb iu"> h </strong> ( <em class="lv"> t </em>)的状态转移函数。</p><p id="7743" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">设<strong class="lb iu">y</strong>(<em class="lv">t</em>)=<strong class="lb iu">g</strong>(<strong class="lb iu">h</strong>(<em class="lv">t</em>))。这里<strong class="lb iu"> g </strong>是将当前状态向量<strong class="lb iu"> h </strong> ( <em class="lv"> t </em>)映射到输出向量<strong class="lb iu"> y </strong> ( <em class="lv"> t </em>)的输出函数。</p><p id="0a10" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为了有个感觉，考虑在时间T产生目标向量<strong class="lb iu"> y </strong> (T)，作为输入向量序列<strong class="lb iu"> x </strong> (1)、<strong class="lb iu"> x </strong> (2)、…、<strong class="lb iu"> x </strong> (T)的函数。任何一个输入向量<strong class="lb iu"> x </strong> ( <em class="lv"> t </em>)，1 &lt; = <em class="lv"> t </em> &lt; = T，都可能影响<strong class="lb iu"> y </strong> (T)。为了能够了解这种影响，首先，让我们及时展开RNN中的计算。从接收<strong class="lb iu"> x </strong> (1)一直到产生输出向量<strong class="lb iu"> yhat </strong> ( <em class="lv"> t </em>)。</p><p id="6c72" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们得到了</p><pre class="kj kk kl km gt lw lx ly lz aw ma bi"><span id="d1d2" class="mb mc it lx b gy md me l mf mg"><strong class="lx iu">h</strong>(1) = <strong class="lx iu">f</strong>(<strong class="lx iu">h</strong>(0), <strong class="lx iu">x</strong>(1))<br/><strong class="lx iu">h</strong>(2) = <strong class="lx iu">f</strong>(<strong class="lx iu">h</strong>(1), <strong class="lx iu">x</strong>(2))<br/>…<br/><strong class="lx iu">h</strong>(T) = <strong class="lx iu">f</strong>(<strong class="lx iu">h</strong>(T-1),<strong class="lx iu">x</strong>(T))<br/><strong class="lx iu">yhat</strong>(T) = <strong class="lx iu">g</strong>(<strong class="lx iu">h</strong>(T))</span></pre><p id="a961" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">好的，那么让我们开始讨论从零开始的学习。首先，我们尝试修改<strong class="lb iu"> g </strong>以便将<strong class="lb iu"> yhat </strong> (T)移近<strong class="lb iu"> y </strong> (T)。注意改变<strong class="lb iu"> g </strong>可以改变<strong class="lb iu"> yhat </strong> (T)。接下来，我们尝试修改<strong class="lb iu"> f </strong>，使<strong class="lb iu">yhat</strong>(T)=<strong class="lb iu">g</strong>(<strong class="lb iu">f</strong>(<strong class="lb iu">h</strong>(T-1)，<strong class="lb iu"> x </strong> (T))向<strong class="lb iu"> y </strong> (T)靠拢。注意，当考虑改变<strong class="lb iu"> f </strong>时，我们保持<strong class="lb iu"> g </strong>、<strong class="lb iu"> h </strong> (T-1)和<strong class="lb iu"> x </strong> (T)不变。接下来，我们尝试修改<strong class="lb iu"> f </strong>，使<strong class="lb iu">yhat</strong>(T)=<strong class="lb iu">g</strong>(<strong class="lb iu">f</strong>(<strong class="lb iu"><em class="lv">f</em></strong>(<strong class="lb iu">h</strong>(T-2)，x(T-1))，<strong class="lb iu"> x </strong> (T))向<strong class="lb iu"> y </strong> (T)靠拢。我们用斜体显示了对<strong class="lb iu"> f </strong>的调用，我们试图修改其行为。</p><p id="290a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">诸如此类，将学习延伸到更早的时间…</p><p id="b72d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">例子:这将更具体地说明学习。(尽管这是一个玩具，基本上没什么用。)</p><p id="351b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">假设我们希望y(T)等于x(1) + x(2) + … + x(T)。也就是说，我们只是试图让RNN学会如何维持一个运行总和。输入和输出都是一维的。</p><p id="58cd" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">假设我们的RNN结构如下:</p><pre class="kj kk kl km gt lw lx ly lz aw ma bi"><span id="ec74" class="mb mc it lx b gy md me l mf mg"><em class="lv">f</em>(<em class="lv">h</em>,x) = <em class="lv">ah</em> + <em class="lv">bx</em><br/><em class="lv">g</em>(<em class="lv">h</em>)   = <em class="lv">ch</em></span></pre><p id="23cc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="lv"> f </em>是具有可学习参数<em class="lv"> a </em>和<em class="lv"> b </em>的状态转移函数。<em class="lv"> g </em>是带有可学习参数<em class="lv"> c </em>的输出函数。</p><p id="c877" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">比方说对于<em class="lv">a</em>=<em class="lv">b</em>=<em class="lv">c</em>= 2我们给出输入序列(<em class="lv"> x </em> 1=1，<em class="lv"> x </em> 2=2，<em class="lv"> x </em> 3=3)想学习<em class="lv"> y </em> 3=1+2+3=6。假设我们开始处理输入，0 = 0。</p><pre class="kj kk kl km gt lw lx ly lz aw ma bi"><span id="1001" class="mb mc it lx b gy md me l mf mg"><em class="lv">h</em>1 = 2<em class="lv">h</em>0 + 2<em class="lv">x</em>1 = 2*0 + 2*1 = 2<br/><em class="lv">h</em>2 = 2<em class="lv">h</em>1 + 2<em class="lv">x</em>2 = 2*2 + 2*2 = 8<br/><em class="lv">h</em>3 = 2<em class="lv">h</em>2 + 2<em class="lv">x</em>3 = 2*8 + 2*3 = 22<br/><em class="lv">yhat</em>3 = 2*22 = 44</span></pre><p id="616e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="lv"> y </em> 3是6，远小于<em class="lv"> yhat </em> 3。我们当然可以将<em class="lv"> c </em>减少到<em class="lv"> yhat </em>等于6。具体来说，我们可以将<em class="lv"> c </em>设置为3/22。一般来说，这不是一件好事。我们正在对一个单一的可学习参数进行彻底的改变，使它产生我们想要的结果。这种剧烈的变化倾向于追逐最新的例子，可能会抹去以前的学习。</p><p id="364f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">更好的做法是循序渐进。到底有多缓慢？这很难量化。尽管如此，引入一个参数来控制学习速度还是有意义的。其实这个参数就是所谓的<em class="lv">学习率</em>。</p><p id="abca" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">好了，回到完成我们的例子。我们只是学得慢一点。我们先把<em class="lv"> c </em>降低一点，比如说1.5。这给了我们重新想象的<em class="lv"> yhat </em> 3是33。我们向y轴3靠近。</p><p id="1b8f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">接下来我们来看看<em class="lv"> h </em> 3，等于22。如果我们能够降低<em class="lv"> h </em> 3，我们将从当前值44降低<em class="lv"> yhat </em> 3。(之所以是44，是因为我们在计算<em class="lv"> h </em> 3对<em class="lv"> yhat </em> 3的影响时，还在使用旧值<em class="lv"> c </em>。)那么我们如何降低<em class="lv"> h </em> 3呢？通过减少<em class="lv"> a </em>或减少<em class="lv"> b </em>。我们会双管齐下——将它们分别从<em class="lv">降低到</em> 1.5。好吧，我们现在有什么。</p><pre class="kj kk kl km gt lw lx ly lz aw ma bi"><span id="faa8" class="mb mc it lx b gy md me l mf mg"><em class="lv">h</em>3 = 1.5*8 + 1.5*3 = 16.5<br/><em class="lv">yhat</em>3 = 2*16.5 = 33</span></pre><p id="2c6b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">很好，这也让<em class="lv"> yhat </em> 3更靠近<em class="lv"> y </em> 3。</p><p id="b5ab" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">同样，我们可以更进一步回到过去。尝试降低<em class="lv"> h </em> 2。在这一点上，中间计算变得更加复杂，对我们的符号进行一些改进将会有所帮助。让我们将h的计算表示为</p><pre class="kj kk kl km gt lw lx ly lz aw ma bi"><span id="86db" class="mb mc it lx b gy md me l mf mg"><em class="lv">h</em>(<em class="lv">i</em>) = <em class="lv">a</em>(i)*<em class="lv">h</em>(<em class="lv">i</em>-1) + <em class="lv">b</em>(<em class="lv">i</em>)*<em class="lv">x</em>(<em class="lv">i</em>)</span></pre><p id="ded3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这将让我们跟踪当我们改变<em class="lv"> a </em> ( <em class="lv"> i </em>)和<em class="lv"> b </em> ( <em class="lv"> i </em>)时<em class="lv"> h </em> ( <em class="lv"> i </em>)发生了什么。在一次学习迭代结束时，我们必须以某种方式确保所有的<em class="lv"> a </em> ( <em class="lv"> i </em>)和所有的<em class="lv"> b </em> ( <em class="lv"> i </em>)都具有相同的最终值。稍后会详细介绍。</p><p id="5e36" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">由于其当前值<em class="lv"> h </em> 2 = 8，<em class="lv"> yhat </em> 3为44。假设我们将<em class="lv"> a </em> 2和<em class="lv"> b </em> 2也降低到各1.5。我们得到了</p><pre class="kj kk kl km gt lw lx ly lz aw ma bi"><span id="be56" class="mb mc it lx b gy md me l mf mg"><em class="lv">h</em>2    = 1.5*2+1.5*2=6<br/><em class="lv">h</em>3    = 2<em class="lv">h</em>2 + 2<em class="lv">x</em>3 = 2*6 + 2*3 = 18<br/><em class="lv">yhat</em>3 = 2*18 = 36</span></pre><p id="066e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="lv"> yhat </em> 3再次靠近<em class="lv"> y </em> 3。</p><p id="f599" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">等等，我们继续回到过去…</p><p id="cdce" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">好了，现在我们有了新的值<em class="lv">new</em>(<em class="lv">I</em>)和<em class="lv"> bnew </em> (i)用于各种<em class="lv"> i </em>的。但实际上，我们只有两个参数<em class="lv"> a </em>和<em class="lv"> b </em>。所以我们必须以某种方式有意义地将所有的<em class="lv">新</em> ( <em class="lv"> i </em>)聚合成一个单一的<em class="lv"> a </em>，并将所有的<em class="lv">b新</em> ( <em class="lv"> i </em>)聚合成一个单一的<em class="lv"> b </em>。一个自然的选择是将<em class="lv"> a </em>和<em class="lv"> b </em>分别设置为所有<em class="lv">new</em>(<em class="lv">I</em>)和所有<em class="lv"> bnew </em> ( <em class="lv"> i </em>)的平均值。</p><p id="dc3f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">推论</strong></p><p id="bb57" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">训练有素的RNN能做出什么样的推论？涉及到什么？我们来说明一下语言建模设置中的思路。我们在这里讨论这个主题，因为它将在编码器-解码器模型的解码阶段发挥作用。</p><p id="77a2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们可以通过训练有素的RNN运行一系列单词<em class="lv"> w </em> 1、<em class="lv"> w2 </em>、…、<em class="lv"> wn </em> -1，并生成下一个可能单词的概率分布。有时候我们想要的不止这些。我们想要下一个<em class="lv"> k </em>最有可能的单词。也就是说，我们想要</p><pre class="kj kk kl km gt lw lx ly lz aw ma bi"><span id="147c" class="mb mc it lx b gy md me l mf mg"><em class="lv">wn</em>, <em class="lv">wn</em>+1, …, <em class="lv">wn</em>+<em class="lv">k</em>-1` = argmax <em class="lv">wn</em>, <em class="lv">wn</em>+1, …, <em class="lv">wn</em>+<em class="lv">k</em>-1` <em class="lv">P</em>(<em class="lv">wn</em>, <em class="lv">wn</em>+1, …, <em class="lv">wn</em>+<em class="lv">k</em>-1 | <em class="lv">w</em>1, <em class="lv">w2</em>, …, <em class="lv">wn</em>-1)</span></pre><p id="6293" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这个推理问题叫做解码问题。一般来说比较棘手，所以我们寻求启发式的解决方案。</p><p id="a8a9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">贪婪解码</strong>:寻找下一个单词的最佳候选，将该单词追加到序列中，并继续下去，直到追加了<em class="lv"> k </em>个单词。不能保证找到长度<em class="lv"> k </em>的最佳扩展，因为它会在整个过程中做出局部最优决策。随后的决定取决于这些。</p><p id="84ec" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">波束搜索</strong>:在贪婪解码中，任何时候，只有一个序列被扩展。波束搜索通过跟踪到目前为止的最佳扩展对其进行了改进，其中m是一个自由参数。(在贪婪解码中<em class="lv"> m </em>为1。)</p><p id="b7be" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">更准确地说，波束搜索保留了<em class="lv"> m </em>个最佳前缀序列。在每一次迭代中，它考虑这些<em class="lv"> m </em>前缀的所有可能的1步扩展，计算它们的概率，并形成迄今为止的<em class="lv"> m </em>最佳前缀序列的新列表。它一直持续下去，直到<em class="lv"> m </em>延长部分达到所需长度。然后，它取出这些<em class="lv"> m </em>扩展中得分最高的作为最终答案。</p><p id="3482" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这里有一个例子来说明进行波束搜索的价值。考虑一个重复的组织名称列表。我们试图找到最受欢迎的组织名称。假设由于某种原因，我们不能(或选择不)保持每个组织名称在列表中出现的次数。(可能是截然不同的名字太多了。)</p><p id="d37d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">贪婪的方法可以从列表中最常出现的单词开始，并尝试扩展它。它选择<em class="lv">银行</em>作为起点似乎是合理的，因为有很多银行。现在想象一下，某个非银行组织比任何特定的银行都受欢迎。想到了谷歌。贪婪的方法永远找不到它，因为它被委托给银行。Beam search有机会找到Google，因为它为可能的扩展保留了多个候选项。</p><p id="e930" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">编解码器型号</strong></p><p id="e251" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">考虑机器翻译。我们希望将特定源语言(比如英语)中的一系列单词翻译成特定目标语言(比如法语)。输入和输出序列可以具有不同的长度。如前所述，在这种情况下，不清楚如何使用RNNs。</p><p id="cee3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">考虑一个两阶段的过程。在第一阶段，称为<em class="lv">编码器</em>，输入序列(英语句子)被转换成固定长度的数字向量。在第二阶段，称为<em class="lv">解码器</em>，向量被转换成合适的输出序列(法语翻译)。</p><p id="bda3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们可以把这个两阶段过程看作首先从英语句子中提取合适的特征，然后把得到的特征向量转换成输出序列。这不仅规避了对准约束；直觉上，它似乎更丰富。原则上，研究一个包含丰富特征的空间比研究一个单词序列要好。</p><p id="eecc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">解码(即翻译)过程直到源序列被完全编码后才开始。</p><p id="3fcf" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在有限范围的机器翻译实例中，我们可能会从使用人工设计的功能中获益。这当然不是一个通用的解决方案，而且可能无法(在翻译质量上)有效地翻译源语言中任何看似合理的句子。</p><p id="70c7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">所以我们想从训练数据中学习编码器。一旦我们描述了端到端系统，即解码器，我们将深入学习。</p><p id="457b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">解码器</strong></p><p id="43bd" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这是另一个RNN，就像编码器一样。那么翻译序列是如何从源序列的编码中生成的呢？我们将解码器的初始状态设置为编码器的最终状态，即源序列的编码状态。然后，我们使用这个RNN作为生成器，迭代地生成翻译序列中的下一个标记。下一个令牌可能是使用贪婪解码或波束解码生成的，这是我们之前讨论过的主题。</p><p id="dfa5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">可以把解码器RNN看作是以源序列编码为条件的目标语言的语言模型。源序列的良好翻译具有高概率；可怜的低。</p><p id="d951" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">注意，“好的翻译”的一个方面是合理地安排翻译的顺序。作为一个想象的例子，假设我们想把<em class="lv">猫很小</em>翻译成法语。我们可以想象编码以某种方式捕获了关键方面:对象= <em class="lv">猫</em>，属性= <em class="lv">小</em>。翻译可能会被解释为用法语表达成一个结构良好的句子。le chat est petit 是个不错的翻译。(有趣的是，作为旁注，当我们在<em class="lv">中输入<em class="lv"> est petit le chat </em>时，谷歌将</em>翻译成法语并要求其翻译成英语，我们得到的是<em class="lv">猫很小</em>。)也就是Google translate对翻译后的序列进行了合理的排序。</p><p id="d960" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">更多这种类型的例子见[2]。</p><p id="455b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">培训概述</strong></p><p id="2a95" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在本节中，我们将对培训进行概述。我们首先讨论一种将训练分解成两部分的方法:训练编码器和训练解码器。这种方法有其局限性。然而，它也有一些潜在的好处。这也有助于教学目的。这有助于我们轻松进入联合培训讨论。</p><p id="0468" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">训练编码器</strong></p><p id="5a60" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这里我们考虑将编码器作为源语言上的语言模型来训练。潜在的缺点是，整个系统没有进行端到端的训练，因此不会捕获源序列和其目标序列之间的相互作用。潜在的好处是简化了编码器培训。很容易得到一组丰富的源序列。第二个潜在的好处是，原则上可以使用同一个经过训练的编码器将源语言翻译成任何目标语言。</p><p id="cf94" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">那么我们如何在源语言的一组序列上训练编码器呢？由于编码器是一个RNN，训练它的自然方法是教它预测序列中的每个符号，来自它前面的序列前缀的符号。我们在<strong class="lb iu">学习</strong>一节中详细讨论了这一点。</p><p id="64b8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">训练解码器</strong></p><p id="bbcb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">解码器可以被类似地训练，因为它也实现语言模型。也就是说，语言模型取决于源编码。让我们来描述一下这是如何发挥作用的。</p><p id="be0c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">考虑一对(<em class="lv"> S </em>，<em class="lv"> T </em>)。<em class="lv"> S </em>是源序列。<em class="lv"> T </em>是与之配对的目标序列。我们假设编码器已经被训练过。我们通过编码器运行<em class="lv"> S </em>得到它的编码<em class="lv"> E </em> ( <em class="lv"> S </em>)。我们将解码器RNN的状态初始化为这种编码。然后，我们教解码器为<em class="lv"> S </em>中的每个符号预测其左侧子序列中的符号。</p><p id="5160" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">联合训练</strong></p><p id="5735" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这从一组(<em class="lv"> S </em>，<em class="lv"> T </em>)对中联合训练编码器和解码器。从直观的角度来看，工作原理如下。我们将<em class="lv"> S </em>呈现给编码器。编码器输出<em class="lv"> E </em> ( <em class="lv"> S </em>)。这可能是<em class="lv"> S </em>的一个好的编码，也可能不是，这取决于编码器训练的当前状态。</p><p id="e223" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">使用该编码作为输入，解码器产生预测的目标序列<em class="lv">。</em><em class="lv">即</em>的质量取决于解码器在其当前状态下的有效性，以及编码器(在其当前状态下)产生的编码<em class="lv"> E </em> ( <em class="lv"> S </em>)的质量。因此，和<em class="lv"> T </em>之间的差异驱动解码器和编码器的学习。</p><p id="10b4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">可以把这想象成误差从解码器的输出一直反向传播到编码器的输入，就像一个长长的多层网络。因此，解码器和编码器的参数都可能改变。</p><p id="329f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为了简单起见。我们已经把在一个实例上的学习(<em class="lv"> S </em>，<em class="lv"> T </em>)描述为一次性过程。也就是说，首先<em class="lv"> S </em>被完全输入，其编码被产生，该编码被完全解码，然后预测的目标序列和解码之间的差异驱动学习。</p><p id="8c63" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">原则上，我们也可以考虑这种一次性过程的顺序变体。这可以看作是对(<em class="lv"> Si </em>，<em class="lv"> Ti </em>)的训练，其中<em class="lv"> Si </em>和<em class="lv"> Ti </em>分别表示<em class="lv"> S </em>和<em class="lv"> T </em>的<em class="lv"> i </em> th前缀。</p><p id="cda2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这可能有助于将编码器-解码器组合视为单个RNN，它在<em class="lv"> S </em> + <em class="lv"> T </em>上学习语言模型，其中+表示连接。在这个RNN中，使用编码器的参数预测<em class="lv"> S </em>中的下一个符号，使用解码器的参数预测<em class="lv"> T </em>中的下一个符号。以这种方式思考，我们还可以看到，当下一个符号仍在<em class="lv"> S </em>内时，训练仅在编码器的参数上发生，其行为就好像编码器正在学习源语言的语言模型。也就是说，这种训练与调整编码器和解码器的参数的训练混合在一起。当要预测的下一个符号在<em class="lv"> T </em>中时会发生这种情况。</p><p id="ed43" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">反向编码器</strong></p><p id="bd7d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在[4]中，发现颠倒源序列(但不是目标序列)产生更准确的翻译。这背后的直觉适用于翻译问题，其中源语言和目标语言中的句子具有相同的词序，例如从左到右。反转过程使源序列中的开始符号更接近它们在目标序列中的翻译。这似乎简化了这些部分的学习，这可能也有雪球效应。</p><p id="8ef0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">参考文献</strong></p><ol class=""><li id="286a" class="mh mi it lb b lc ld lf lg li mj lm mk lq ml lu mm mn mo mp bi translated"><a class="ae ky" rel="noopener" target="_blank" href="/neural-language-models-32bec14d01dc">神经语言模型|作者Arun Jagota </a></li><li id="b818" class="mh mi it lb b lc mq lf mr li ms lm mt lq mu lu mm mn mo mp bi translated"><a class="ae ky" href="https://cs224d.stanford.edu/lecture_notes/LectureNotes4.pdf" rel="noopener ugc nofollow" target="_blank">CS 224d:NLP的深度学习</a></li><li id="60ee" class="mh mi it lb b lc mq lf mr li ms lm mt lq mu lu mm mn mo mp bi translated"><a class="ae ky" href="https://arxiv.org/pdf/1406.1078.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1406.1078.pdf</a></li><li id="3e60" class="mh mi it lb b lc mq lf mr li ms lm mt lq mu lu mm mn mo mp bi translated"><a class="ae ky" href="https://arxiv.org/abs/1409.3215" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1409.3215</a><em class="lv">用神经网络进行序列对序列学习</em> — Stutskever等。</li></ol></div></div>    
</body>
</html>