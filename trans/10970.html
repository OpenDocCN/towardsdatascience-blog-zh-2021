<html>
<head>
<title>Gaussian Mixture Models with Python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">基于Python的高斯混合模型</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/gaussian-mixture-models-with-python-36dabed6212a?source=collection_archive---------2-----------------------#2021-10-26">https://towardsdatascience.com/gaussian-mixture-models-with-python-36dabed6212a?source=collection_archive---------2-----------------------#2021-10-26</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="1f69" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph">无监督学习</h2><div class=""/><div class=""><h2 id="4fe4" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">在这篇文章中，我简要介绍了无监督学习方法的概念，高斯混合模型，以及它在Python中的实现。</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/f416164362654c8dd92513ca99fe5d0e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*YRagjklZ8Tr7YFem"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">由<a class="ae lh" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的<a class="ae lh" href="https://unsplash.com/@edge2edgemedia?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Edge2Edge媒体</a>拍摄</p></figure><p id="d005" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi me translated"><span class="l mf mg mh bm mi mj mk ml mm di"> T </span>何<strong class="lk jd">高斯混合模型</strong> ( <strong class="lk jd"> <em class="mn"> GMM </em> </strong>)作为一种<strong class="lk jd">无监督</strong>学习算法用于聚类是众所周知的。这里，“<em class="mn">高斯</em>是指高斯分布，用均值和方差来描述；<em class="mn">混合</em>指一个以上高斯分布的混合。</p><p id="06a7" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">这个想法很简单。假设我们知道一组数据点来自多个不同的高斯模型(对于<em class="mn"> 1-d </em>数据，高斯模型由均值标量和方差标量描述，对于<em class="mn"> N-d </em>数据，由均值向量和方差矩阵描述)，如果我们知道它们的密度函数，我们可以知道每个数据点属于2个高斯模型之一的概率(如下例所示)。然后，我们能够将数据点分配给高斯混合中概率最高的一个特定模型。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi mo"><img src="../Images/64d8b6f88e78e8b2a727443617b12d6e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9pvyYyJ0hVe1sHp5lgIN_A.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">2高斯分布的高斯混合密度(图片由作者提供)。</p></figure><p id="bc54" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">从上面描述的程序，相信你已经注意到了，高斯混合模型中有<strong class="lk jd">两个最重要的</strong>东西。一个是<strong class="lk jd"> <em class="mn">估计高斯混合中每个高斯分量的参数</em> </strong>(如上图右侧所列)，另一个是<strong class="lk jd"> <em class="mn">确定一个数据点属于哪个高斯分量</em> </strong>。这就是为什么聚类只是高斯混合模型最重要的应用之一，但高斯混合模型的核心是密度估计。</p><p id="6964" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">为了估计描述高斯混合模型中每个高斯分量的参数，我们必须了解一种叫做<a class="ae lh" href="https://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm" rel="noopener ugc nofollow" target="_blank"> <strong class="lk jd">期望最大化</strong> ( <strong class="lk jd"> <em class="mn"> EM </em> </strong>)算法</a>的方法。当模型依赖于一些不可观测的潜在变量时，EM算法被广泛用于参数估计。高斯混合模型中的潜在变量是描述数据点属于哪个高斯分量的变量。由于我们只观察了数据点，这个变量是一个未观察到的潜在变量。</p><p id="5a02" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">在这篇文章中，我简要描述了<strong class="lk jd">使用EM算法</strong>构建高斯混合模型的思想，以及<strong class="lk jd">如何用Python </strong>实现该模型。我在学习EM的时候，最大的问题是对方程的理解，所以在这篇帖子里我会尽量解释没有很多方程的算法。</p><h2 id="f943" class="mp mq it bd mr ms mt dn mu mv mw dp mx lr my mz na lv nb nc nd lz ne nf ng iz bi translated">EM算法在GMM中到底做了什么？</h2><p id="f4e2" class="pw-post-body-paragraph li lj it lk b ll nh kd ln lo ni kg lq lr nj lt lu lv nk lx ly lz nl mb mc md im bi translated">简而言之，<strong class="lk jd"> EM算法</strong>是针对高斯参数估计的。为了更好地理解概念，我们从头开始。</p><p id="1b54" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">显然，在理解高斯混合分布之前，我们必须先理解单个高斯分布。假设我们有一个数据点序列，每个数据点只有一个特征(<em class="mn"> 1-D </em>数据集)。我们可以沿着该特征的轴绘制这些数据点的密度。</p><p id="c4b9" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">例如，我们想通过测量苹果的直径来描述一桶<em class="mn"> Gala </em>苹果的大小。我们不想只知道苹果的平均直径，而是想知道苹果大小的整体分布。因此，我们将分布绘制如下。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi mo"><img src="../Images/beffcb6557d548325e75a922e2f660ff.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iKjrNL6SsYmbSS-qbdTWTA.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">苹果大小的高斯分布(图片由作者提供)</p></figure><p id="88ef" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">上面的密度图也可以由下面的等式描述，</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi nm"><img src="../Images/1118cfc95d254d084074835c469b9ef2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*CBSGUGTNSz7BMVq8uGnonA.png"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">高斯分布密度函数(图片来自作者)</p></figure><p id="29be" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">其中，μ是均值，σ是标准差。</p><p id="992e" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">通常，我们用<strong class="lk jd">均值</strong>和<strong class="lk jd">方差</strong>来描述一个高斯分布。很简单，对吧？</p><p id="3062" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">接下来，假设我们不小心把一桶<em class="mn">嘎拉</em>和一桶<em class="mn">富士</em>苹果混在了一起。我们这里没有水果专家，所以周围没有人能区分水果和藤茶。我们所能做的仍然是测量苹果的大小。</p><p id="9a7a" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我们还能把它们分开吗？理论上讲，是的，如果<em class="mn">星系</em>与<em class="mn">藤球</em>在大小上有真正的差异。注意，由于我们现在只能测量大小，我们应该祈祷这个唯一的特征能够足够好地分离苹果。</p><p id="0b68" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">上面描述的情况是可以应用<strong class="lk jd"> <em class="mn">高斯混合模型</em> </strong>聚类的现实问题。如果苹果的直径遵循如下所示的两种不同的高斯分布，我们会非常高兴，</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi mo"><img src="../Images/3d735d14fb199a1ac7d584c83365fc93.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*h9rM383CnJPQRNt9fshZew.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">两种截然不同的高斯分布(图片由作者提供)</p></figure><p id="1d3a" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">在这种情况下，我们可以简单地应用硬截止来分离这两种苹果。例如，较大的(直径超过2英寸)是富士牌的🍎，更小的(直径小于2英寸)是嘎拉🍏。</p><p id="8204" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">然而，在大多数情况下，我们将观察到如下所示的混合分布。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/6da20651aad0ae4b4f25dc33367b0dda.png" data-original-src="https://miro.medium.com/v2/resize:fit:992/format:webp/1*tJG8VsCKKLp6fMNvQTRxSA.png"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">加拉和富士苹果大小的混合分布(图片来自作者)</p></figure><p id="5755" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">在上面的图中，顶部的<strong class="lk jd"> <em class="mn">黑色曲线</em> </strong>是我们观察到的苹果大小的<strong class="lk jd"><em class="mn"/></strong>，它代表了我们看不到的两个基本高斯分布。我们来思考一下如何用最直观的方式解决问题。</p><p id="8489" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><strong class="lk jd"> (1) </strong>如果我们知道两个分布的密度函数参数(均值和方差)，对于每一个苹果，我们可以很容易的得到它属于<em class="mn">富士</em>的概率，也可以得到它属于<em class="mn">嘎拉</em>的概率。如果成为<em class="mn">富士</em>的概率大于成为<em class="mn">春晚</em>的概率，那么就是<em class="mn">富士</em>，反之亦然。然而，我们不知道密度函数参数。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi no"><img src="../Images/f2cdecebf1404d6473e8b49ac40e42c0.png" data-original-src="https://miro.medium.com/v2/resize:fit:722/format:webp/1*7ROUlGMuajor9YY2aYvUmg.png"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">贴着贴纸的苹果(图片由作者提供)</p></figure><p id="864e" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><strong class="lk jd"> (2) </strong>如果我们突然发现每个苹果上其实已经有贴纸了，我们可以直接估算出<em class="mn">富士</em>和<em class="mn">嘎拉</em>的密度函数参数。但是等等，如果我们已经有了贴纸，为什么我们还要费心去做所有的事情呢？然而，真实的情况是，我们没有正确的贴纸，因为水果店老板的儿子玩苹果，把贴纸弄乱了。</p><p id="016a" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我在说什么？你以为我疯了是因为上面的点<em class="mn"> (1) </em>和点<em class="mn"> (2) </em>其实是循环题？我们必须知道其中一个才能解决另一个。</p><p id="f96f" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">实际上，我只是在描述<strong class="lk jd"> EM算法</strong>的一次迭代。别急，请让我详细解释一下。</p><h2 id="bd68" class="mp mq it bd mr ms mt dn mu mv mw dp mx lr my mz na lv nb nc nd lz ne nf ng iz bi translated">EM算法的思想</h2><p id="0fdb" class="pw-post-body-paragraph li lj it lk b ll nh kd ln lo ni kg lq lr nj lt lu lv nk lx ly lz nl mb mc md im bi translated"><strong class="lk jd"> EM算法</strong>具有两个主要过程的迭代序列，即<strong class="lk jd"> E步骤</strong>和<strong class="lk jd"> M步骤</strong>。<strong class="lk jd"> <em class="mn"> E-Step </em> </strong>估算潜变量，即每个苹果成为<em class="mn">富士</em>或<em class="mn"> Gala </em>的概率。这个潜在变量影响数据，但不可观察。<strong class="lk jd"> <em class="mn"> M步</em> </strong>通过最大化给定数据的似然性来估计分布的参数。似然性描述了一组参数与给定数据的匹配程度，我们希望得到它的<strong class="lk jd">最大</strong>值(与给定数据最匹配，也称为<strong class="lk jd"> <em class="mn">最大似然估计</em> </strong>)。</p><p id="77d9" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">所以，EM算法的真正过程应该是这样的，</p><p id="7953" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><strong class="lk jd">EM EM EM EM EM EM EM EM…</strong></p><p id="f031" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">让我们回到我们正在讨论的具体问题上来。</p><p id="ad3c" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">水果店老板的儿子帮我们完成了EM算法的<strong class="lk jd"> <em class="mn">初始化步骤</em> </strong>，即<strong class="lk jd">随机</strong>给数据点(苹果)分配标签(贴纸)。这个想法是，因为我们不能直接分离数据点，我们只是给一个初始化的猜测。从现在起，我们应该让这个小男孩远离苹果。</p><p id="ab75" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">好了，现在我们来执行EM算法中的<strong class="lk jd"> <em class="mn"> E步</em> </strong>。由于我们直接来自<strong class="lk jd"> <em class="mn">初始化步骤</em> </strong>，我们已经有了每个苹果成为<em class="mn">富士</em>或者<em class="mn"> Gala </em>的概率。对于一个贴有<em class="mn">富士</em>贴纸的苹果，它成为<em class="mn">富士</em>的概率等于1，成为<em class="mn">嘎拉</em>的概率等于0。</p><p id="7186" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">但是，如果我们来自于之前的一个<strong class="lk jd"> <em class="mn"> M步</em> </strong>，我们就需要用两个高斯的密度函数(从之前的M步估计)重新计算成为<em class="mn">富士</em>或者<em class="mn">加拉</em>的概率。如果之前贴有<em class="mn">富士</em>的苹果碰巧成为<em class="mn">嘎拉</em>的概率大于成为<em class="mn">富士</em>的概率，那么我们就用新的<em class="mn">嘎拉</em>贴纸替换当前的贴纸。如果之前的标签和当前的概率没有不一致，我们就保持标签不变。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi np"><img src="../Images/514e718f9d18f97edb292af1f9efc59c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1056/format:webp/1*f3-VjfUszAIGPox72Junng.png"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">估计概率，必要时更换标签。(图片由作者提供)</p></figure><p id="0eac" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">好了，现在我们来执行EM算法中的<strong class="lk jd"> <em class="mn"> M步</em> </strong>。不管是哪个迭代，我们已经在苹果上有了初始/更新的标签(贴纸)。我们可以用给定的标签直接估计出<em class="mn">富士</em>和<em class="mn">加拉</em>分布的高斯参数。基本上，我们只根据贴有“<em class="mn">富士</em>的苹果来估计<em class="mn">富士</em>高斯参数，然后根据贴有“<em class="mn">嘎拉</em>的苹果来估计<em class="mn">嘎拉</em>高斯参数。从理论上讲，我们找到了在给定苹果贴纸的情况下使可能性最大化的参数集。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi nq"><img src="../Images/c72ae9e7d0cd636d6e60e89970dca4d5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RgvYVWmaBXYKXM_wT-nMig.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">给定当前标签，最大化可能性。(图片由作者提供)</p></figure><p id="8d9d" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">然后，我们反复重复上述两个步骤，直到苹果的分配<strong class="lk jd">不再变化</strong>(严格来说，直到<strong class="lk jd">可能性</strong>函数中的<strong class="lk jd">变化</strong>非常<strong class="lk jd">小</strong>)。</p><p id="3696" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">这就是EM算法在现实问题中的完整过程。你觉得苹果分离任务中的EM算法怎么样？很难吗？我用了很多方程式吗？不是真的，对吧？</p><h2 id="2c29" class="mp mq it bd mr ms mt dn mu mv mw dp mx lr my mz na lv nb nc nd lz ne nf ng iz bi translated">提高执行力的技巧</h2><p id="6bf0" class="pw-post-body-paragraph li lj it lk b ll nh kd ln lo ni kg lq lr nj lt lu lv nk lx ly lz nl mb mc md im bi translated">如果你真的思考苹果分离任务，你会意识到里面有很多现实问题。</p><p id="8854" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">例如，商店老板的小儿子只是随机地发起苹果贴纸<strong class="lk jd">而不记得他做了什么。那么，我的苹果分离任务会因为不同的苹果贴纸初始分配而有很大的不同吗？另外，只看大小应该很难区分<em class="mn">嘎拉</em>和<em class="mn">富士</em>吧？</strong></p><p id="94a5" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">第一个问题的答案肯定是肯定的。理论上，对于<strong class="lk jd"><em class="mn"/></strong>的<strong class="lk jd"> <em class="mn"> EM算法</em> </strong>，并不能保证每次随机初始化都会导致相同的最终结果。为了解决这个问题，从一些不那么随机的“初始化”开始可能是一个好的选择。如果这个小男孩只是玩了1/4的苹果，而不是把所有的苹果贴纸都弄乱了，现在的开始可能会导致一个稳定得多的分离。</p><p id="348d" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">对于第二个问题，当然最好有更多的功能。例如，小男孩不仅玩苹果的贴纸，还咬每个苹果(假设他能够以定量的方式记录每个苹果的味道)。然后，对于每个苹果，我们都有大小和味道，这可能会导致更好的分离。</p><p id="44f4" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">具有二维数据的高斯分布可以被可视化为特征空间中的椭圆。下面的GIF显示了具有三个高斯分量的高斯混合模型的EM算法的过程。你可以把它想象成一项任务，根据每个苹果的已知大小和味道，将<em class="mn">富士</em>、<em class="mn">旮旯</em>和<em class="mn">甜脆</em>苹果分开。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="nr ns l"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">GMM上EM算法的过程GIF(免费GIF来自Tenor<a class="ae lh" href="https://tenor.com/view/gaussian-mixture-models-em-method-math-gauss-computer-science-nerd-gif-15288262" rel="noopener ugc nofollow" target="_blank">https://Tenor . com/view/Gaussian-mixture-models-EM-method-math-gauss-computer-science-nerd-GIF-15288262</a>)</p></figure><p id="b620" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">上面GIF中的<em class="mn"> X轴</em>和<em class="mn"> Y轴</em>可以分别是标准化的大小和标准化的味道。可以看到初始化的三个组件是如何逐渐移动到特定的apple集群的。</p><h2 id="6432" class="mp mq it bd mr ms mt dn mu mv mw dp mx lr my mz na lv nb nc nd lz ne nf ng iz bi translated">如何用Python实现？</h2><p id="9622" class="pw-post-body-paragraph li lj it lk b ll nh kd ln lo ni kg lq lr nj lt lu lv nk lx ly lz nl mb mc md im bi translated">相对于理解GMM中EM算法的概念，Python中的实现非常简单(感谢强大的包scikit-learn)。</p><pre class="ks kt ku kv gt nt nu nv nw aw nx bi"><span id="6062" class="mp mq it nu b gy ny nz l oa ob"><strong class="nu jd">import</strong> <strong class="nu jd">numpy</strong> <strong class="nu jd">as</strong> <strong class="nu jd">np</strong><br/><strong class="nu jd">from</strong> <strong class="nu jd">sklearn.mixture</strong> <strong class="nu jd">import</strong> GaussianMixture</span><span id="c5c6" class="mp mq it nu b gy oc nz l oa ob"># Suppose Data X is a 2-D Numpy array (One apple has two features, size and flavor)</span><span id="41c7" class="mp mq it nu b gy oc nz l oa ob">GMM = GaussianMixture(n_components=3, random_state=0).fit(X)</span></pre><p id="bdad" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><strong class="lk jd"><em class="mn">Gaussian mixture</em></strong>是函数，<strong class="lk jd"> <em class="mn"> n_components </em> </strong>是底层高斯分布的个数，<strong class="lk jd"> <em class="mn"> random_state </em> </strong>是初始化的随机种子，<strong class="lk jd"> <em class="mn"> X </em> </strong>是我们的数据。这里的<strong class="lk jd"> <em class="mn"> X </em> </strong>是一个<strong class="lk jd"> <em class="mn">二维</em> </strong> <em class="mn"> NumPy </em>数组，其中每个数据点有两个特征。</p><p id="03c2" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">拟合数据后，我们可以检查任何具有这两个特征的数据点(苹果)的预测聚类。</p><pre class="ks kt ku kv gt nt nu nv nw aw nx bi"><span id="ba47" class="mp mq it nu b gy ny nz l oa ob">GMM.predict([[0.5, 3], [1.2, 3.5]])</span></pre><p id="2a3a" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">有时，高斯分量的数量并不明显。我们可以通过使用<a class="ae lh" href="https://en.wikipedia.org/wiki/Akaike_information_criterion" rel="noopener ugc nofollow" target="_blank"> AIC </a>或<a class="ae lh" href="https://en.wikipedia.org/wiki/Bayesian_information_criterion" rel="noopener ugc nofollow" target="_blank"> BIC </a>作为标准来调整它(可能会在未来的帖子中进一步解释)。</p><pre class="ks kt ku kv gt nt nu nv nw aw nx bi"><span id="674a" class="mp mq it nu b gy ny nz l oa ob">aic(X)<br/>bic(X)</span></pre></div><div class="ab cl od oe hx of" role="separator"><span class="og bw bk oh oi oj"/><span class="og bw bk oh oi oj"/><span class="og bw bk oh oi"/></div><div class="im in io ip iq"><p id="8110" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">就是这样！<strong class="lk jd">的概念与实现<em class="mn"> GMM </em>中的<strong class="lk jd">Python<em class="mn">中的</em></strong>。希望有帮助。</strong></p><p id="c3c9" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">如果你觉得文章过于简单笼统，又偏爱更多的方程和数学，可以参考以下优秀帖子，这里<a class="ae lh" rel="noopener" target="_blank" href="/gaussian-mixture-models-explained-6986aaf5a95"><em class="mn"/></a><a class="ae lh" href="https://www.mygreatlearning.com/blog/gaussian-mixture-model/" rel="noopener ugc nofollow" target="_blank"><em class="mn">这里</em> </a>，这里<a class="ae lh" href="https://stephens999.github.io/fiveMinuteStats/intro_to_em.html" rel="noopener ugc nofollow" target="_blank"><em class="mn"/></a>。</p><h2 id="cd87" class="mp mq it bd mr ms mt dn mu mv mw dp mx lr my mz na lv nb nc nd lz ne nf ng iz bi translated">参考资料:</h2><div class="ok ol gp gr om on"><a href="https://scikit-learn.org/stable/modules/generated/sklearn.mixture.GaussianMixture.html#sklearn.mixture.GaussianMixture" rel="noopener  ugc nofollow" target="_blank"><div class="oo ab fo"><div class="op ab oq cl cj or"><h2 class="bd jd gy z fp os fr fs ot fu fw jc bi translated">sklearn . mixture . Gaussian mixture</h2><div class="ou l"><h3 class="bd b gy z fp os fr fs ot fu fw dk translated">高斯混合。高斯混合模型概率分布的表示。这个类允许估计…</h3></div><div class="ov l"><p class="bd b dl z fp os fr fs ot fu fw dk translated">scikit-learn.org</p></div></div><div class="ow l"><div class="ox l oy oz pa ow pb lb on"/></div></div></a></div><div class="ok ol gp gr om on"><a href="https://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm" rel="noopener  ugc nofollow" target="_blank"><div class="oo ab fo"><div class="op ab oq cl cj or"><h2 class="bd jd gy z fp os fr fs ot fu fw jc bi translated">期望值最大化算法-维基百科</h2><div class="ou l"><h3 class="bd b gy z fp os fr fs ot fu fw dk translated">在统计学中，期望最大化(EM)算法是一种寻找(局部)最大似然或…</h3></div><div class="ov l"><p class="bd b dl z fp os fr fs ot fu fw dk translated">en.wikipedia.org</p></div></div><div class="ow l"><div class="pc l oy oz pa ow pb lb on"/></div></div></a></div><div class="ok ol gp gr om on"><a href="https://machinelearningmastery.com/expectation-maximization-em-algorithm/" rel="noopener  ugc nofollow" target="_blank"><div class="oo ab fo"><div class="op ab oq cl cj or"><h2 class="bd jd gy z fp os fr fs ot fu fw jc bi translated">期望值最大化(EM算法)的温和介绍-机器学习掌握</h2><div class="ou l"><h3 class="bd b gy z fp os fr fs ot fu fw dk translated">最大似然估计是一种通过搜索概率来估计数据集密度的方法</h3></div><div class="ov l"><p class="bd b dl z fp os fr fs ot fu fw dk translated">machinelearningmastery.com</p></div></div><div class="ow l"><div class="pd l oy oz pa ow pb lb on"/></div></div></a></div><div class="ok ol gp gr om on"><a rel="noopener follow" target="_blank" href="/gaussian-mixture-models-explained-6986aaf5a95"><div class="oo ab fo"><div class="op ab oq cl cj or"><h2 class="bd jd gy z fp os fr fs ot fu fw jc bi translated">高斯混合模型解释</h2><div class="ou l"><h3 class="bd b gy z fp os fr fs ot fu fw dk translated">在机器学习领域，我们可以区分两个主要领域:监督学习和非监督学习。主要的…</h3></div><div class="ov l"><p class="bd b dl z fp os fr fs ot fu fw dk translated">towardsdatascience.com</p></div></div><div class="ow l"><div class="pe l oy oz pa ow pb lb on"/></div></div></a></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pf"><img src="../Images/dfe03db5fb385468f0f718a426fd1a39.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*RrN-xh7mnz_QtVqR"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">Christine Jou 在<a class="ae lh" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure></div></div>    
</body>
</html>