<html>
<head>
<title>Recent Developments and Views on Computer Vision x Transformer</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">计算机视觉x变形金刚的最新发展和看法</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/recent-developments-and-views-on-computer-vision-x-transformer-ed32a2c72654?source=collection_archive---------13-----------------------#2021-07-08">https://towardsdatascience.com/recent-developments-and-views-on-computer-vision-x-transformer-ed32a2c72654?source=collection_archive---------13-----------------------#2021-07-08</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="05e3" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">《变形金刚》和CNN的区别，为什么《变形金刚》很重要，它的弱点是什么。</h2></div><h1 id="18f0" class="ki kj it bd kk kl km kn ko kp kq kr ks jz kt ka ku kc kv kd kw kf kx kg ky kz bi translated">关于这篇文章</h1><p id="984a" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">本文讨论了自视觉转换器问世以来，Transformer x计算机视觉研究中的一些有趣的研究和见解。这篇文章的四个主题如下</p><ol class=""><li id="a65f" class="lw lx it lc b ld ly lg lz lj ma ln mb lr mc lv md me mf mg bi translated">Transformer和CNN感受野大小和行为的差异</li><li id="9eea" class="lw lx it lc b ld mh lg mi lj mj ln mk lr ml lv md me mf mg bi translated">自我关注对《变形金刚》来说是否必不可少？</li><li id="d956" class="lw lx it lc b ld mh lg mi lj mj ln mk lr ml lv md me mf mg bi translated">视觉变压器的缺点和改进方向</li><li id="1c67" class="lw lx it lc b ld mh lg mi lj mj ln mk lr ml lv md me mf mg bi translated">变压器的快速扩张及其重要性</li></ol><p id="f48c" class="pw-post-body-paragraph la lb it lc b ld ly ju lf lg lz jx li lj mm ll lm ln mn lp lq lr mo lt lu lv im bi translated"><strong class="lc iu"> <em class="mp">作为本文的总结，我的观点如下:</em> </strong></p><ul class=""><li id="5353" class="lw lx it lc b ld ly lg lz lj ma ln mb lr mc lv mq me mf mg bi translated">自视觉变压器以来，变压器的应用范围迅速扩大。我个人认为这样做的原因是它可以应用于各种各样的数据，并且很容易在不同的模态之间建立关联。</li><li id="22ed" class="lw lx it lc b ld mh lg mi lj mj ln mk lr ml lv mq me mf mg bi translated">《变形金刚》和CNN的主要区别之一就是视野开阔。也许正因为如此，变形金刚比CNN更能适应纹理的变化，并生成不同模式的对抗性补丁。</li><li id="3376" class="lw lx it lc b ld mh lg mi lj mj ln mk lr ml lv mq me mf mg bi translated">根据最近的一项研究，自我关注在《变形金刚》中可能并不重要。在我看来，编码器块中有两个部分很重要，一个处理全局信息，另一个在本地传播。</li><li id="cab8" class="lw lx it lc b ld mh lg mi lj mj ln mk lr ml lv mq me mf mg bi translated">Vision Transformer的弱点是需要大量内存和大量数据，但它正在快速改进。</li></ul></div><div class="ab cl mr ms hx mt" role="separator"><span class="mu bw bk mv mw mx"/><span class="mu bw bk mv mw mx"/><span class="mu bw bk mv mw"/></div><div class="im in io ip iq"><h1 id="c4a1" class="ki kj it bd kk kl my kn ko kp mz kr ks jz na ka ku kc nb kd kw kf nc kg ky kz bi translated">变压器和视觉变压器</h1><h2 id="051f" class="nd kj it bd kk ne nf dn ko ng nh dp ks lj ni nj ku ln nk nl kw lr nm nn ky no bi translated">变压器</h2><p id="1adc" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">首先，我想解释一下Vision Transformer中使用的Transformer编码器，这是一个在论文“注意力是你所需要的全部”中提出的模型。这篇论文的标题对那些使用LSTM和CNN的人来说是挑衅性的。</p><p id="6438" class="pw-post-body-paragraph la lb it lc b ld ly ju lf lg lz jx li lj mm ll lm ln mn lp lq lr mo lt lu lv im bi translated">它既不是CNN，也不是LSTM，而是一种叫做点积注意力的机制，建立在它之上的模型(Transformer)已经远远超过了现有的方法。</p><figure class="nq nr ns nt gt nu gh gi paragraph-image"><div class="gh gi np"><img src="../Images/a010646641d21fd70c649c876e83c184.png" data-original-src="https://miro.medium.com/v2/resize:fit:1258/format:webp/1*lDbP5iiwjS5IHRNT-12ZLA.png"/></div><p class="nx ny gj gh gi nz oa bd b be z dk translated">摘自[2]，变压器模型概述</p></figure><p id="0512" class="pw-post-body-paragraph la lb it lc b ld ly ju lf lg lz jx li lj mm ll lm ln mn lp lq lr mo lt lu lv im bi translated">在转换器中使用的(点积)注意中有三个变量:查询、键和值。简而言之，系统计算查询和关键词的关注权重，并将每个关键词乘以与之相关联的值。</p><figure class="nq nr ns nt gt nu gh gi paragraph-image"><div role="button" tabindex="0" class="oc od di oe bf of"><div class="gh gi ob"><img src="../Images/002b1957c351b316f1ddde4c3884e39e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tLCQolCPGi_Lnf4b8aehuA.png"/></div></div><p class="nx ny gj gh gi nz oa bd b be z dk translated">点积注意力</p></figure><p id="d3ae" class="pw-post-body-paragraph la lb it lc b ld ly ju lf lg lz jx li lj mm ll lm ln mn lp lq lr mo lt lu lv im bi translated">多头注意力，使用多个注意力头(在MLP的术语中，“隐藏层数”增加)，定义如下。上图中的(单个头)注意力原样使用q，k，但在多头注意力中，每个头都有自己的投影矩阵W_i^Q、W_i^K和W_i^V，这些矩阵投影的特征用于创建注意力。</p><figure class="nq nr ns nt gt nu gh gi paragraph-image"><div role="button" tabindex="0" class="oc od di oe bf of"><div class="gh gi og"><img src="../Images/a29291d0a814c8ee7defef9bea8da796.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZDD7Z1FLjtZ5RMNB8X_qPA.png"/></div></div><p class="nx ny gj gh gi nz oa bd b be z dk translated">多头注意。左上图取自[2]</p></figure><p id="348e" class="pw-post-body-paragraph la lb it lc b ld ly ju lf lg lz jx li lj mm ll lm ln mn lp lq lr mo lt lu lv im bi translated">在这个点积注意力中使用的Q、K和V，当它们都来自相同的数据时，称为自我注意。当点积注意力中使用的Q、K和V来自相同的数据时，这被称为自我注意力。在《变形金刚》中，编码器的部分和解码器下面的部分是自关注的。解码器的上半部分不是自关注的，因为查询来自编码器，K，V来自解码器。下图显示了注意力权重的示例。在该图中，单词“making”被用作查询，并且每个关键词的注意力权重被计算和可视化。注意力集中的每个键都在学习不同的依赖性。“关键”这几个字用多种方式着色，代表每个人头的注意力权重。</p><figure class="nq nr ns nt gt nu gh gi paragraph-image"><div role="button" tabindex="0" class="oc od di oe bf of"><div class="gh gi oh"><img src="../Images/7f1a778b5ecef1e3ffe5241e22bc7c32.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*K5mReBlZWWrQXyzL-_WkuQ.png"/></div></div><p class="nx ny gj gh gi nz oa bd b be z dk translated">引自[2]，变压器的自我关注的重量。我加上字幕。</p></figure><h2 id="9bee" class="nd kj it bd kk ne nf dn ko ng nh dp ks lj ni nj ku ln nk nl kw lr nm nn ky no bi translated">视觉转换器的工作原理</h2><p id="812f" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">Vision Transformer是将Transformer应用于图像分类任务的模型，于2020年10月提出。内容和原来的Transformer几乎一样，但是有一种别出心裁的方式，像自然语言处理一样处理图像。</p><figure class="nq nr ns nt gt nu gh gi paragraph-image"><div role="button" tabindex="0" class="oc od di oe bf of"><div class="gh gi oi"><img src="../Images/1125f21947acb73a0e1152a5371eae87.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FReqmx_EKhrrJtLPV75kJw.png"/></div></div><p class="nx ny gj gh gi nz oa bd b be z dk translated">视觉变压器架构，引自[1]。</p></figure><p id="8cac" class="pw-post-body-paragraph la lb it lc b ld ly ju lf lg lz jx li lj mm ll lm ln mn lp lq lr mo lt lu lv im bi translated">视觉转换器将图像分成N个16x16大小的小块。由于补丁本身是三维数据，它们不能由处理语言处理的转换器直接处理。因此，在将它们展平后，他们做一个线性投影，将它们转换成二维数据。通过这样做，每个补丁可以被视为一个令牌(像一个单词)，可以输入到转换器中。</p><p id="acb7" class="pw-post-body-paragraph la lb it lc b ld ly ju lf lg lz jx li lj mm ll lm ln mn lp lq lr mo lt lu lv im bi translated">此外，Vision Transformer采用了预训练→微调的策略:Vision Transformer在包含3亿幅图像的数据集JFT-300M上进行预训练，在ImageNet等下游任务上进行微调。Vision Transformer是第一款在ImageNet上实现SotA性能的纯变压器型号。这是对Transformer x计算机视觉研究大量增加的开始。</p><h2 id="120e" class="nd kj it bd kk ne nf dn ko ng nh dp ks lj ni nj ku ln nk nl kw lr nm nn ky no bi translated">为什么《视觉变形金刚》这么准？</h2><p id="43da" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">关于Transformer x计算机视觉的研究由来已久，但一直无法在ImageNet上实现SotA性能。作者从模型的归纳偏差和数据数量方面解释了原因。归纳偏差是模型对数据的一种假设。比如CNN用3x3内核处理数据，这是基于数据信息是本地聚合的数据假设。在RNNs中，当前时间的数据与前一时间的数据高度相关，但前一时间的数据仅通过前一时间的数据相关。在RNN，当前时间数据与先前时间数据高度相关，但是先前x2时间数据仅通过先前时间数据相关。这个过程也是基于数据与上一次高度相关的数据假设。另一方面，由于自我关注只与每个数据相关，可以说与CNN和RNN相比，其归纳偏差相对较低。</p><figure class="nq nr ns nt gt nu gh gi paragraph-image"><div role="button" tabindex="0" class="oc od di oe bf of"><div class="gh gi oj"><img src="../Images/6d58bb8c6b1266c5d1be6a037e2209d9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*86_dMWwYgVXlBDbKXEydsw.png"/></div></div><p class="nx ny gj gh gi nz oa bd b be z dk translated">(左)CNN，它有一个强烈的归纳偏见，即信息是本地聚合的。(中间)RNN，它有很强的归纳偏差，因为它与前一次(右)自我注意强烈相关，而后者有相对较弱的归纳偏差，因为它只与所有特征相关。</p></figure><p id="d8d7" class="pw-post-body-paragraph la lb it lc b ld ly ju lf lg lz jx li lj mm ll lm ln mn lp lq lr mo lt lu lv im bi translated">作者将ViT的强<strong class="lc iu"> </strong>解释为:“在数据很少的情况下，具有强归纳偏差的模型比那些具有弱归纳偏差的模型更强，因为它们对数据有假设。另一方面，当有大量数据时，假设就成了阻碍，所以在有大量数据的情况下，具有弱归纳偏差的模型就变得更强”。下图加强了这种解释:通过预训练数据集的大小来比较Vision Transformer和CNN。在使用JFT-300M进行预训练的情况下，它优于CNN(具有强归纳偏差的模型)。</p><figure class="nq nr ns nt gt nu gh gi paragraph-image"><div role="button" tabindex="0" class="oc od di oe bf of"><div class="gh gi ok"><img src="../Images/cca78d483e5460f242f88f53ac91d7c8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*R9cT5xNlUcPDoP5FAlYN1g.png"/></div></div><p class="nx ny gj gh gi nz oa bd b be z dk translated">数据量和准确性</p></figure></div><div class="ab cl mr ms hx mt" role="separator"><span class="mu bw bk mv mw mx"/><span class="mu bw bk mv mw mx"/><span class="mu bw bk mv mw"/></div><div class="im in io ip iq"><h1 id="79a6" class="ki kj it bd kk kl my kn ko kp mz kr ks jz na ka ku kc nb kd kw kf nc kg ky kz bi translated">变形金刚x计算机视觉的发展趋势</h1><p id="7ea5" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">从现在开始，我将介绍Transformer x计算机视觉的研究动态以及最近研究中发现的视觉变压器系统的有趣行为。下面有四个主题。请注意，它们包含了我的许多个人观点。</p><ol class=""><li id="20e1" class="lw lx it lc b ld ly lg lz lj ma ln mb lr mc lv md me mf mg bi translated">扩大变压器的应用领域及其原因。</li><li id="12e0" class="lw lx it lc b ld mh lg mi lj mj ln mk lr ml lv md me mf mg bi translated">Transformer和CNN感受野范围和行为的差异</li><li id="1fdd" class="lw lx it lc b ld mh lg mi lj mj ln mk lr ml lv md me mf mg bi translated">自我关注对《变形金刚》来说是否必不可少？</li><li id="c29c" class="lw lx it lc b ld mh lg mi lj mj ln mk lr ml lv md me mf mg bi translated">视觉转换器的缺点和改进方向</li></ol><h1 id="cb10" class="ki kj it bd kk kl km kn ko kp kq kr ks jz kt ka ku kc kv kd kw kf kx kg ky kz bi translated">1.扩大变压器的应用领域及其原因</h1><p id="05f9" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">自从引入视觉转换器以来，将转换器应用于各种数据和任务的研究数量迅速增加，尤其是在计算机视觉领域。Vision Transformer用于图像分类任务，但也有其他应用，如应用于语义分割、对象检测的Swin Transformer和应用于深度估计的DPT[17]。在不同的数据格式方面，点转换器[18]适用于点云数据，感知者[19]适用于音频、视频、点云以及图像。</p><p id="9d0a" class="pw-post-body-paragraph la lb it lc b ld ly ju lf lg lz jx li lj mm ll lm ln mn lp lq lr mo lt lu lv im bi translated">在视觉和语言领域，它将最初transformer使用的领域自然语言与计算机视觉任务相结合，也有许多应用程序，如UniT，它可以同时执行多个任务，如计算机视觉。</p><p id="c0da" class="pw-post-body-paragraph la lb it lc b ld ly ju lf lg lz jx li lj mm ll lm ln mn lp lq lr mo lt lu lv im bi translated">如你所见，变压器已经广泛应用于各个领域。为什么变压器被这样使用？在我看来，有以下原因。</p><p id="3c2c" class="pw-post-body-paragraph la lb it lc b ld ly ju lf lg lz jx li lj mm ll lm ln mn lp lq lr mo lt lu lv im bi translated">1–1。人们发现它不仅可以应用于自然语言处理，也可以应用于图像处理，因此得到了很大的发展。</p><p id="cb19" class="pw-post-body-paragraph la lb it lc b ld ly ju lf lg lz jx li lj mm ll lm ln mn lp lq lr mo lt lu lv im bi translated"><em class="mp">1–2。不需要根据数据更换网络，方便。</em></p><p id="1243" class="pw-post-body-paragraph la lb it lc b ld ly ju lf lg lz jx li lj mm ll lm ln mn lp lq lr mo lt lu lv im bi translated"><em class="mp">1–3。不同数据之间很容易建立关联。</em></p><p id="ed27" class="pw-post-body-paragraph la lb it lc b ld ly ju lf lg lz jx li lj mm ll lm ln mn lp lq lr mo lt lu lv im bi translated"><strong class="lc iu"><em class="mp">1–1。人们发现它不仅可以应用于自然语言处理，也可以应用于图像处理，因此得到了很大的发展。</em>T9】</strong></p><p id="4bfb" class="pw-post-body-paragraph la lb it lc b ld ly ju lf lg lz jx li lj mm ll lm ln mn lp lq lr mo lt lu lv im bi translated">最初，Transformer用于自然语言处理领域，其有效性在自然语言处理和语音识别中得到证实，如Transformer Transducer[20]在语音方面的成功。感谢Vision Transformer，我们现在知道它在图像领域甚至比CNN更好。单单语音和自然语言处理的研究领域就很广阔。现在，随着同样具有广泛研究领域的计算机视觉的加入，以及在语音+图像、视觉&amp;语言等复杂领域中使用它的能力，应用范围迅速扩大。</p><p id="c44c" class="pw-post-body-paragraph la lb it lc b ld ly ju lf lg lz jx li lj mm ll lm ln mn lp lq lr mo lt lu lv im bi translated"><strong class="lc iu"> <em class="mp"> 1-2。不需要根据数据更换网络，方便。</em>T13】</strong></p><p id="31c1" class="pw-post-body-paragraph la lb it lc b ld ly ju lf lg lz jx li lj mm ll lm ln mn lp lq lr mo lt lu lv im bi translated">以前需要把网络分开，比如图像-&gt; CNN，自然语言-&gt; Transformer，现在所有的数据都可以通过Transformer处理，使用起来很方便，比如感知者[19]。</p><figure class="nq nr ns nt gt nu gh gi paragraph-image"><div class="gh gi ol"><img src="../Images/21224da79c7d9c0b92d4feaba4fdf360.png" data-original-src="https://miro.medium.com/v2/resize:fit:1388/format:webp/1*9XM0j4mXiWpzwXtqreE0_A.png"/></div><p class="nx ny gj gh gi nz oa bd b be z dk translated">他们提出Preciever，一个可以处理超过10万个特征的高维输入的transformer模型，可以处理视频+音频、图像、点云等多种数据格式。它通过从潜在空间中检索Q来减少计算量。不仅实现了对图像和点云的高性能，还获得了对视频+音频的SotA性能。这些图片引自[19]。</p></figure><p id="0a1e" class="pw-post-body-paragraph la lb it lc b ld ly ju lf lg lz jx li lj mm ll lm ln mn lp lq lr mo lt lu lv im bi translated"><strong class="lc iu"> <em class="mp"> 1-3。不同数据之间易于关联。</em>T3】</strong></p><p id="16ed" class="pw-post-body-paragraph la lb it lc b ld ly ju lf lg lz jx li lj mm ll lm ln mn lp lq lr mo lt lu lv im bi translated">事实上，我们不需要有一个图像分支网络和一个自然语言分支网络，因为我们可以作为一个令牌处理一切，包括图像和语言，这可能是它越来越多地使用的另一个原因。在低层次上，我们可以将不同的模态与自我关注联系起来。典型的例子有维尔特[21]，VATT[22]和VL-T5[23]。</p><figure class="nq nr ns nt gt nu gh gi paragraph-image"><div role="button" tabindex="0" class="oc od di oe bf of"><div class="gh gi om"><img src="../Images/50eccfb5234e54f4f1d831fc7726710b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eJZR2TM2gr5UMHfgB0S-dA.png"/></div></div><p class="nx ny gj gh gi nz oa bd b be z dk translated">通过将不同模态的数据视为记号，很容易获得不同数据之间的相关性。(上)VL-T5[23]，(下)VLiT[21]。</p></figure></div><div class="ab cl mr ms hx mt" role="separator"><span class="mu bw bk mv mw mx"/><span class="mu bw bk mv mw mx"/><span class="mu bw bk mv mw"/></div><div class="im in io ip iq"><h1 id="d522" class="ki kj it bd kk kl my kn ko kp mz kr ks jz na ka ku kc nb kd kw kf nc kg ky kz bi translated">2.Transformer和CNN感受野范围和行为的差异</h1><p id="9c64" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">《变形金刚》现在在计算机视觉上和CNN一样好甚至更好，但是两者有什么区别呢？</p><p id="28b4" class="pw-post-body-paragraph la lb it lc b ld ly ju lf lg lz jx li lj mm ll lm ln mn lp lq lr mo lt lu lv im bi translated">首先在视野上有区别:CNN用的是3x3或者7x7大小的内核，所以每一层只能有一个对应的视野。并且视场随着其在各层中的传播而扩大，但是这种扩大随着深度而线性增加。</p><p id="ae61" class="pw-post-body-paragraph la lb it lc b ld ly ju lf lg lz jx li lj mm ll lm ln mn lp lq lr mo lt lu lv im bi translated">另一方面，转换器使用自我关注，这允许网络从初始层看到整个图像。由于每个面片被视为一个令牌，并且所有面片都是相互关联和计算的，因此可以从头开始学习全局特征。</p><figure class="nq nr ns nt gt nu gh gi paragraph-image"><div role="button" tabindex="0" class="oc od di oe bf of"><div class="gh gi on"><img src="../Images/ba194076906b39a2bce03844c6964f7a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YWPy6SwuU5tS6GeMxwCAdw.png"/></div></div><p class="nx ny gj gh gi nz oa bd b be z dk translated">将CNN (kernel_size=3)和自我关注应用于一个大小为32x32的图像时，视场大小的差异。CNN线性增加视野，而自我关注(视觉转换者)从一开始就拥有整个视野。</p></figure><p id="e557" class="pw-post-body-paragraph la lb it lc b ld ly ju lf lg lz jx li lj mm ll lm ln mn lp lq lr mo lt lu lv im bi translated">事实上，下图显示了视觉转换器感受野的扩展。大部分视图是线性扩展的，但也有一部分是从初始层获取全局信息。</p><figure class="nq nr ns nt gt nu gh gi paragraph-image"><div role="button" tabindex="0" class="oc od di oe bf of"><div class="gh gi oo"><img src="../Images/7fe06806ffb69ab93710abbe42aab6f1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GALJbWEa7VjHOIQMTtycCA.png"/></div></div><p class="nx ny gj gh gi nz oa bd b be z dk translated">受过训练的视觉转换者的视野。图片取自[1]。</p></figure><p id="d3d2" class="pw-post-body-paragraph la lb it lc b ld ly ju lf lg lz jx li lj mm ll lm ln mn lp lq lr mo lt lu lv im bi translated">此外，也许是由于其视野的大小，研究表明，与CNN相比，Transformer可以根据形状做出物体决策。有一项研究[3]声称，CNN的决策更多的是基于纹理而不是基于形状，这与常规理论不同，并建议CNN根据纹理而不是形状对对象进行分类。例如，在下图(c)中，人类倾向于根据猫的形状来判断猫，但基于CNN的模型根据纹理来判断印度大象。</p><figure class="nq nr ns nt gt nu gh gi paragraph-image"><div role="button" tabindex="0" class="oc od di oe bf of"><div class="gh gi op"><img src="../Images/ec44f868698617da4150707ae8dc1962.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rKEkYgY9LwA2fb7fVRsQ3Q.png"/></div></div><p class="nx ny gj gh gi nz oa bd b be z dk translated"><em class="oq">CNN是根据纹理而不是形状来判断的。图片取自[3]。</em></p></figure><p id="d68b" class="pw-post-body-paragraph la lb it lc b ld ly ju lf lg lz jx li lj mm ll lm ln mn lp lq lr mo lt lu lv im bi translated">下图定量显示了这一点。该图显示了在保持形状的同时对纹理进行扰动的图像的分类精度。红点是人类，蓝点是基于CNN的模型。人类的判断对纹理扰动是鲁棒的，而CNN的准确性大大降低。换句话说，CNN的标准高度依赖于纹理，而不是形状。</p><figure class="nq nr ns nt gt nu gh gi paragraph-image"><div role="button" tabindex="0" class="oc od di oe bf of"><div class="gh gi or"><img src="../Images/fda3e695e693f2a6371f25da44278360.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*W7Ye2dT_xuSp5twSQ2efxg.png"/></div></div><p class="nx ny gj gh gi nz oa bd b be z dk translated">定量评价CNN是用纹理而不是形状来判断的。比较人类(红色)和基于CNN的模型(蓝色)对纹理有扰动但形状保持不变的图像的分类精度。图片取自[3]。</p></figure><p id="6eea" class="pw-post-body-paragraph la lb it lc b ld ly ju lf lg lz jx li lj mm ll lm ln mn lp lq lr mo lt lu lv im bi translated">与基于CNN的模型相比，Transformer模型(ViT)对纹理扰动相对更加鲁棒。与CNN模型相比，变压器模型(ViT)对纹理扰动相对稳健[4]。这可能和变形金刚视野广有关。</p><figure class="nq nr ns nt gt nu gh gi paragraph-image"><div role="button" tabindex="0" class="oc od di oe bf of"><div class="gh gi os"><img src="../Images/7aaf18bed58dd3820fe21d528657aa0a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Bru3O7NPhgHe_k9nfMIF6g.png"/></div></div><p class="nx ny gj gh gi nz oa bd b be z dk translated">变形金刚比CNN更少依赖纹理。图片取自[4]。</p></figure><p id="7b7f" class="pw-post-body-paragraph la lb it lc b ld ly ju lf lg lz jx li lj mm ll lm ln mn lp lq lr mo lt lu lv im bi translated">还有其他有趣的特性可能归因于视场的大小。有一种技术称为对抗性例子[5]，它使用故意创建的噪声来误判模型，并且已经发现基于CNN的模型和Transformer [6]之间的噪声存在差异。在导致狗图像误判的噪声中，CNN (ResNet)噪声有很多高频成分，具有局部结构。另一方面，变压器噪声(ViT)具有相对较低的频率分量和较大的结构。这也可能是由于视野的大小。(很有意思的是，你可以在16x16的补丁中清晰地看到边界。</p><figure class="nq nr ns nt gt nu gh gi paragraph-image"><div role="button" tabindex="0" class="oc od di oe bf of"><div class="gh gi ot"><img src="../Images/74a5261acfb04920e34b25e856f5930a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3_sgRGjl_YB5RHUSFWjnLg.png"/></div></div><p class="nx ny gj gh gi nz oa bd b be z dk translated">反面例子的例子。通过添加故意制造的噪音，该模型将熊猫图像误解为长臂猿。图片摘自[5]。</p></figure><figure class="nq nr ns nt gt nu gh gi paragraph-image"><div role="button" tabindex="0" class="oc od di oe bf of"><div class="gh gi op"><img src="../Images/9344f6a0960224bdce9938b3a4a97f6b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dlfaG7Xk_maq-BCF3Z9g4w.png"/></div></div><p class="nx ny gj gh gi nz oa bd b be z dk translated"><em class="oq">对抗性噪音的比较；ViT的噪声具有较大的结构，具有相对较低的频率分量，而ResNet由高频分量组成。图片摘自[6]。</em></p></figure></div><div class="ab cl mr ms hx mt" role="separator"><span class="mu bw bk mv mw mx"/><span class="mu bw bk mv mw mx"/><span class="mu bw bk mv mw"/></div><div class="im in io ip iq"><h1 id="943a" class="ki kj it bd kk kl my kn ko kp mz kr ks jz na ka ku kc nb kd kw kf nc kg ky kz bi translated">3.自我关注是变形金刚必备的吗？</h1><p id="60fe" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">ViT在计算机视觉方面取得了巨大的成功，但也有很多研究在探索是否存在比自我关注更好的结构。例如，MLP混合器[7]不使用自我关注，而是使用多层感知器(MLP)，这是最基本的深度学习方法，结果与视觉变压器相当。与Vision Transformer一样，它需要在JFT-300M等庞大的数据集上进行预训练，但它可以在不使用自关注等复杂机制的情况下实现高性能。</p><p id="63be" class="pw-post-body-paragraph la lb it lc b ld ly ju lf lg lz jx li lj mm ll lm ln mn lp lq lr mo lt lu lv im bi translated">基本结构是，混合器层是一个包含MLP1和MLP2的块，ml P1处理跨面片的信息，MLP 2处理每个面片的信息，这些块堆叠在一起，而不是Transformer编码器。这种方法类似于ViT，它将图像分成小块，并处理小块的2D数据投影，用MLP代替ViT的变换器。</p><figure class="nq nr ns nt gt nu gh gi paragraph-image"><div role="button" tabindex="0" class="oc od di oe bf of"><div class="gh gi ou"><img src="../Images/ca52725578c1ef460c16ac1d7189a2d4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gLIcjCy7Jolcumr8gqicbw.png"/></div></div><p class="nx ny gj gh gi nz oa bd b be z dk translated"><em class="oq">MLP密炼机的结构。混合器层是一个包含MLP1和MLP2的块，ml P1处理跨面片的信息，MLP 2处理每个面片的信息，并且堆叠在彼此的顶部。图片摘自[7]。</em></p></figure><figure class="nq nr ns nt gt nu gh gi paragraph-image"><div role="button" tabindex="0" class="oc od di oe bf of"><div class="gh gi ov"><img src="../Images/05fac28919533e5b6291e4d9f1d0e757.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*d6kHNQhpo1bTm1oKcuuyRQ.png"/></div></div><p class="nx ny gj gh gi nz oa bd b be z dk translated">MLP混频器、ViT和Big Transfer的比较(BiT，基于CNN)。和ViT一样，MLP混频器在低数据条件下不如CNN，但在大数据时超过CNN。图片摘自[7]。</p></figure><p id="c424" class="pw-post-body-paragraph la lb it lc b ld ly ju lf lg lz jx li lj mm ll lm ln mn lp lq lr mo lt lu lv im bi translated">受此影响，一篇论文[8]出现，声称自我关注不是一个必不可少的元素。该论文中提出的gMLP还提出了一种仅使用MLPs的结构，类似于MLP混频器，但是具有类似于挤压和激励的结构，该结构还具有用于处理面片方向上的信息的部分和用于处理空间方向上的信息的部分。即使不使用JFT-300M这样的庞大数据集，仅通过训练ImageNet就可以达到高于EfficientNet的精度。同样需要注意的是，它不仅在计算机视觉任务上和BERT一样准确，在自然语言处理上也是如此。</p><figure class="nq nr ns nt gt nu gh gi paragraph-image"><div role="button" tabindex="0" class="oc od di oe bf of"><div class="gh gi ow"><img src="../Images/288a817451227a0e2d46539d5d85a1a4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Az3ehhxoN3PxD6MAZ4-_SA.png"/></div></div><p class="nx ny gj gh gi nz oa bd b be z dk translated"><em class="oq">gMLP的结构，分为通道方向处理信息的机制和空间方向处理信息的机制。图片摘自[8]。</em></p></figure><div class="nq nr ns nt gt ab cb"><figure class="ox nu oy oz pa pb pc paragraph-image"><div role="button" tabindex="0" class="oc od di oe bf of"><img src="../Images/cb55b3ff7ad455edf87a92735589ff03.png" data-original-src="https://miro.medium.com/v2/resize:fit:1316/format:webp/1*s_KMLYCy7tnYqhFY0Xphkg.png"/></div></figure><figure class="ox nu pd oz pa pb pc paragraph-image"><div role="button" tabindex="0" class="oc od di oe bf of"><img src="../Images/bcb942bb01e79c990ff932691d2d434b.png" data-original-src="https://miro.medium.com/v2/resize:fit:686/format:webp/1*ToJZDnsEoUkXvBCxVxPgsg.png"/></div><p class="nx ny gj gh gi nz oa bd b be z dk pe di pf pg translated">gMLP的结果。在图像识别领域，没有JFT-300M，ViT无法超越CNN，但gMLP仅用ImageNet就取得了与EfficientNet相同的结果。此外，在自然语言处理领域，gMLP已经产生了可与BERT相媲美的结果。图片摘自[8]。</p></figure></div><p id="ed19" class="pw-post-body-paragraph la lb it lc b ld ly ju lf lg lz jx li lj mm ll lm ln mn lp lq lr mo lt lu lv im bi translated">还有一项研究提出了更为激进的主张，即自我关注可以被一种完全没有学习参数的机制所取代，如果它可以接受全部信息的话。在自然语言处理领域的研究FNet [9]中，自我注意被傅立叶变换所取代。傅立叶变换没有学到任何东西，因为它只是改变了基础，并且它具有通过在傅立叶变换之前和之后添加特征而在物理上难以解释的结构，但是即使具有这样的结构，它也产生合理的结果。在这篇论文中，作者认为在令牌之间混合信息就足够了，除了傅立叶变换之外，他们还使用随机矩阵(具有固定参数)代替自关注对网络进行了实验。</p><figure class="nq nr ns nt gt nu gh gi paragraph-image"><div role="button" tabindex="0" class="oc od di oe bf of"><div class="gh gi ph"><img src="../Images/60ebc939f18c94d48ff0ea77251d3d96.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1LwxuIxNA__AvSWysskPhg.png"/></div></div><p class="nx ny gj gh gi nz oa bd b be z dk translated">FNet的结构，其中变换器的自关注部分由傅立叶变换代替。图片取自[9]。</p></figure><p id="4904" class="pw-post-body-paragraph la lb it lc b ld ly ju lf lg lz jx li lj mm ll lm ln mn lp lq lr mo lt lu lv im bi translated">最近几天，有很多关于网络的研究，这些网络可以在不使用自我关注的情况下取得结果，如MLP混合器，gMLP和FNet。这三种机制处理全局信息(ViT中的自我注意，MLP混合器中的MLP1，FNet中的傅立叶变换)并在局部传播它(ViT和FNet中的前馈，MLP混合器中的MLP2)。</p><p id="79a2" class="pw-post-body-paragraph la lb it lc b ld ly ju lf lg lz jx li lj mm ll lm ln mn lp lq lr mo lt lu lv im bi translated">虽然我的个人意见是混合的，但重要的是要有一个块结构，它不仅像FNet声称的那样在令牌(图像的情况下是补丁)之间混合信息，而且在考虑全局信息后在局部传播。</p></div><div class="ab cl mr ms hx mt" role="separator"><span class="mu bw bk mv mw mx"/><span class="mu bw bk mv mw mx"/><span class="mu bw bk mv mw"/></div><div class="im in io ip iq"><h1 id="3a7a" class="ki kj it bd kk kl my kn ko kp mz kr ks jz na ka ku kc nb kd kw kf nc kg ky kz bi translated">4.视觉转换器的缺点和改进方向</h1><p id="0853" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">虽然《视觉变形金刚》在超越CNN方面成绩斐然，但它有两大弱点。然而，改进正在迅速进行，这些弱点正在被克服。</p><ul class=""><li id="0038" class="lw lx it lc b ld ly lg lz lj ma ln mb lr mc lv mq me mf mg bi translated">由于感应偏差较弱，需要比ImageNet(130万个数据集)大的JFT-300M (3亿个数据集)才能获得良好的精度</li><li id="7659" class="lw lx it lc b ld mh lg mi lj mj ln mk lr ml lv mq me mf mg bi translated">由于自我关注的性质，它需要的内存大小等于图像长度的四次方。</li></ul><p id="fede" class="pw-post-body-paragraph la lb it lc b ld ly ju lf lg lz jx li lj mm ll lm ln mn lp lq lr mo lt lu lv im bi translated">由于感应偏差较弱，需要比ImageNet(130万个数据集)大的JFT-300M (3亿个数据集)才能获得良好的精度。Vision Transformer之所以能够超越CNN，是因为它的感性偏置弱，但另一方面，如果数据量不够大，无法利用感性偏置弱的优势，准确性就会下降。具体来说，如果没有3亿个数据集，我们还无法超越基于CNN的模型。为了克服这一点，已经提出了各种改进方法。</p><h2 id="c199" class="nd kj it bd kk ne nf dn ko ng nh dp ks lj ni nj ku ln nk nl kw lr nm nn ky no bi translated">使用CNN</h2><p id="ee5a" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">人们试图通过使用CNN来减少所需的数据量:DeiT [10]使用了一个知识提取框架，其中CNN被用作教师模型，而知识被输入到转换器模型。通过这样做，即使仅使用ImageNet，结果也不仅超过ViT，而且超过EffcientNet。也有报道称，判决的倾向很大程度上通过蒸馏更接近CNN。</p><figure class="nq nr ns nt gt nu gh gi paragraph-image"><div role="button" tabindex="0" class="oc od di oe bf of"><div class="gh gi pi"><img src="../Images/f028579ce24e3a23f0dc8237c0912ed5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YxSe9GHw1yrTR7FIKyPeig.png"/></div></div><p class="nx ny gj gh gi nz oa bd b be z dk translated"><em class="oq">(左)在DeiT中执行的知识提炼的概念图(右)使用CNN作为教师模型执行知识提炼的ViT DeiT的结果。图片取自[10]。</em></p></figure><p id="b994" class="pw-post-body-paragraph la lb it lc b ld ly ju lf lg lz jx li lj mm ll lm ln mn lp lq lr mo lt lu lv im bi translated">此外，由于Vision Transformer通过线性投影16x16大小的补丁以非常简单的方式处理本地信息，因此有研究使用CNN，它对本地信息更具抵抗力。</p><figure class="nq nr ns nt gt nu gh gi paragraph-image"><div role="button" tabindex="0" class="oc od di oe bf of"><div class="gh gi pj"><img src="../Images/138eee9b925f5ca830741f456660ef2e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IHBTslx5swXWXxksAik6bw.png"/></div></div><p class="nx ny gj gh gi nz oa bd b be z dk translated">(左)ViT在Transformer中使用patchy嵌入表示，而CeiT使用带CNN卷积的抽象嵌入表示。(右)CNN放置在变压器内部，使其对局部特征获取更加鲁棒。图片取自[11]、[12]，我添加了注释。</p></figure><p id="3f53" class="pw-post-body-paragraph la lb it lc b ld ly ju lf lg lz jx li lj mm ll lm ln mn lp lq lr mo lt lu lv im bi translated">由于自我关注的性质，它需要的内存大小是图像边长的四次方。因为自注意计算所有补丁之间的相关性，所以它需要与边长的四次方成比例的内存大小。这使得难以处理高分辨率图像。然而，有研究通过采用分层结构解决了这一问题，如CNN，初始层处理高分辨率图像，随着越来越深，分辨率逐渐降低。例如，PVT [14]使用了类似CNN的高分辨率到低分辨率的层次结构，而Swin [13]不仅使用了层次结构，还缩小了自我注意的视野，以减少斑块大小，获得更详细的信息。</p><figure class="nq nr ns nt gt nu gh gi paragraph-image"><div role="button" tabindex="0" class="oc od di oe bf of"><div class="gh gi pk"><img src="../Images/ce76bc997f8dc7c2319f854f98ddce9f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nknKgEeXwTjEC4rNgMReqg.png"/></div></div><p class="nx ny gj gh gi nz oa bd b be z dk translated">(左)ViT，固定分辨率和低分辨率。(中间)PVT，它可以通过逐渐降低分辨率来处理高分辨率信息。(右)Swin Tranformer，它像CNN一样通过逐渐扩大视野来处理高分辨率信息，而不是使用整个注意力区域。图片取自[13]、[14]。</p></figure><h2 id="e1bf" class="nd kj it bd kk ne nf dn ko ng nh dp ks lj ni nj ku ln nk nl kw lr nm nn ky no bi translated">其他改进</h2><p id="7c1a" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">除了与CNN相关的改进之外，还提出了Transformer独有的各种其他改进。例如，T2T模块[15]提出将图像嵌入与周围的补片(记号)混合，允许重叠。Swin Transformer [13]使用局部注意，不像ViT使用全局注意，但是注意的组在不同的层是不同的。</p><p id="b6b0" class="pw-post-body-paragraph la lb it lc b ld ly ju lf lg lz jx li lj mm ll lm ln mn lp lq lr mo lt lu lv im bi translated">此外，ViT还有一个问题，就是随着越来越深入，会产生同样类型的注意力地图。在《走向更深的视觉转换器》中，作者重点阐述了保持头部之间注意力多样性的观点，并引入了一个参数来混合不同头部的注意力图，即使在加深后也成功地保持了注意力图的多样性。</p><figure class="nq nr ns nt gt nu gh gi paragraph-image"><div role="button" tabindex="0" class="oc od di oe bf of"><div class="gh gi pl"><img src="../Images/2f26ad0c5046fe5a80768b8d1544e10b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KZQD0QD8lTod9O7Q7ZGxsg.png"/></div></div><p class="nx ny gj gh gi nz oa bd b be z dk translated">(左)考虑到ViT中的图像标记化(嵌入)过于简单，他们提出了一个T2T模块，允许通过混合周围的标记进行复制和重新标记化。(右上)与使用全局注意力的ViT不同，Swin在红色框架内获得局部注意力，但通过改变每层的注意力组来传播它。(右下)引入一个混合不同头部注意力的学习参数，提高了注意力的多样性。</p></figure><p id="6212" class="pw-post-body-paragraph la lb it lc b ld ly ju lf lg lz jx li lj mm ll lm ln mn lp lq lr mo lt lu lv im bi translated">Vision Transformer过去存在“需要大量数据”和“需要大内存大小”等问题，但在过去几个月中已经提出了许多改进。在实际使用中，CNN仍然是主流，但可能用不了多久，它也会在实际使用中取代CNN。</p></div><div class="ab cl mr ms hx mt" role="separator"><span class="mu bw bk mv mw mx"/><span class="mu bw bk mv mw mx"/><span class="mu bw bk mv mw"/></div><div class="im in io ip iq"><h1 id="5fb2" class="ki kj it bd kk kl my kn ko kp mz kr ks jz na ka ku kc nb kd kw kf nc kg ky kz bi translated">结论</h1><p id="a30c" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">在这篇文章中，我讨论了自Vision Transformer问世以来，Transformer x计算机视觉研究中一些有趣的研究和见解。我对这篇文章的总结如下。我非常兴奋地看到未来将在变压器上做什么研究。</p><ul class=""><li id="ce2b" class="lw lx it lc b ld ly lg lz lj ma ln mb lr mc lv mq me mf mg bi translated">自视觉变压器以来，变压器的应用范围迅速扩大。我个人认为这样做的原因是它可以应用于各种各样的数据，并且很容易在不同的模态之间建立关联。</li><li id="91e6" class="lw lx it lc b ld mh lg mi lj mj ln mk lr ml lv mq me mf mg bi translated">《变形金刚》和CNN的主要区别之一就是视野开阔。也许正因为如此，变形金刚比CNN更能适应纹理的变化，并生成不同模式的对抗性补丁。</li><li id="e8ad" class="lw lx it lc b ld mh lg mi lj mj ln mk lr ml lv mq me mf mg bi translated">根据最近的一项研究，自我关注在《变形金刚》中可能并不重要。在我看来，编码器块中有两个部分很重要，一个处理全局信息，另一个在本地传播。</li><li id="c08a" class="lw lx it lc b ld mh lg mi lj mj ln mk lr ml lv mq me mf mg bi translated">Vision Transformer的弱点是需要大量内存和大量数据，但它正在快速改进。</li></ul></div><div class="ab cl mr ms hx mt" role="separator"><span class="mu bw bk mv mw mx"/><span class="mu bw bk mv mw mx"/><span class="mu bw bk mv mw"/></div><div class="im in io ip iq"><p id="1da2" class="pw-post-body-paragraph la lb it lc b ld ly ju lf lg lz jx li lj mm ll lm ln mn lp lq lr mo lt lu lv im bi">— — — — — — — — — — — — — — — — — — –</p><h1 id="ff54" class="ki kj it bd kk kl km kn ko kp kq kr ks jz kt ka ku kc kv kd kw kf kx kg ky kz bi translated">🌟我每周发布时事通讯！请订阅！🌟</h1><div class="pm pn gp gr po pp"><a href="https://www.getrevue.co/profile/akiratosei" rel="noopener  ugc nofollow" target="_blank"><div class="pq ab fo"><div class="pr ab ps cl cj pt"><h2 class="bd iu gy z fp pu fr fs pv fu fw is bi translated">阿基拉的机器学习新闻- Revue</h2><div class="pw l"><h3 class="bd b gy z fp pu fr fs pv fu fw dk translated">由Akira的机器学习新闻-由Akihiro FUJII:制造工程师/机器学习工程师/硕士…</h3></div><div class="px l"><p class="bd b dl z fp pu fr fs pv fu fw dk translated">www.getrevue.co</p></div></div><div class="py l"><div class="pz l qa qb qc py qd nv pp"/></div></div></a></div><p id="c3d3" class="pw-post-body-paragraph la lb it lc b ld ly ju lf lg lz jx li lj mm ll lm ln mn lp lq lr mo lt lu lv im bi">— — — — — — — — — — — — — — — — — — –</p><h1 id="2e08" class="ki kj it bd kk kl km kn ko kp kq kr ks jz kt ka ku kc kv kd kw kf kx kg ky kz bi translated">关于我</h1><p id="5ca6" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">制造工程师/机器学习工程师/数据科学家/物理学硕士/<a class="ae qe" href="https://t.co/hjHHbG24Ph?amp=1" rel="noopener ugc nofollow" target="_blank">http://github.com/AkiraTOSEI/</a>，</p><div class="pm pn gp gr po pp"><a href="https://www.linkedin.com/in/%E4%BA%AE%E5%AE%8F-%E8%97%A4%E4%BA%95-999868122/?locale=en_US" rel="noopener  ugc nofollow" target="_blank"><div class="pq ab fo"><div class="pr ab ps cl cj pt"><h2 class="bd iu gy z fp pu fr fs pv fu fw is bi">Akihiro FUJII - テクニカルリード - 株式会社エクサウィザーズ/ExaWizards Inc. | LinkedIn</h2><div class="pw l"><h3 class="bd b gy z fp pu fr fs pv fu fw dk translated">Twitter(我对最新的论文有一个简单的解释) :机器上的https://twitter.com/AkiraTOSEI-周报…</h3></div><div class="px l"><p class="bd b dl z fp pu fr fs pv fu fw dk translated">www.linkedin.com</p></div></div><div class="py l"><div class="qf l qa qb qc py qd nv pp"/></div></div></a></div><p id="262f" class="pw-post-body-paragraph la lb it lc b ld ly ju lf lg lz jx li lj mm ll lm ln mn lp lq lr mo lt lu lv im bi translated">推特，我贴一句纸评论。</p><figure class="nq nr ns nt gt nu"><div class="bz fp l di"><div class="qg qh l"/></div></figure><p id="6782" class="pw-post-body-paragraph la lb it lc b ld ly ju lf lg lz jx li lj mm ll lm ln mn lp lq lr mo lt lu lv im bi">— — — — — — — — — — — — — — — — — — –</p><h1 id="207f" class="ki kj it bd kk kl km kn ko kp kq kr ks jz kt ka ku kc kv kd kw kf kx kg ky kz bi translated">参考</h1><ol class=""><li id="6354" class="lw lx it lc b ld le lg lh lj qi ln qj lr qk lv md me mf mg bi translated">Dosovitskiy、Lucas Beyer、Alexander、Dirk Weissenborn、翟晓华、Thomas Unterthiner、Mostafa Dehghani、Matthias Minderer、Georg Heigold、Sylvain Gelly、Jakob Uszkoreit、Neil Houlsby。一幅图像抵得上16x16的文字:大规模图像识别的变形金刚。第十四次评估报告(2019年)</li><li id="4d78" class="lw lx it lc b ld mh lg mi lj mj ln mk lr ml lv md me mf mg bi translated">Ashish Vaswani，Noam Shazeer，Niki Parmar，Jakob Uszkoreit，Llion Jones，Aidan N. Gomez，Lukasz Kaiser，Illia Polosukhin。你需要的只是关注。第十四次评估报告(2017年)</li><li id="4021" class="lw lx it lc b ld mh lg mi lj mj ln mk lr ml lv md me mf mg bi translated">罗伯特·盖尔霍斯、帕特里夏·鲁比什、克劳迪奥·米歇尔斯、马蒂亚斯·贝赫、费利克斯·a·威其曼、维兰德·布伦德尔。ImageNet训练的CNN偏向于纹理；增加形状偏差可以提高精确度和鲁棒性。第十四次评估报告(2018年)</li><li id="143c" class="lw lx it lc b ld mh lg mi lj mj ln mk lr ml lv md me mf mg bi translated">Shikhar Tuli，Ishita Dasgupta，Erin Grant，Thomas L. Griffiths。卷积神经网络还是变形金刚更像人类的视觉？第十四次评估报告(2021年)</li><li id="d144" class="lw lx it lc b ld mh lg mi lj mj ln mk lr ml lv md me mf mg bi translated">伊恩·j·古德菲勒，黄邦贤·史伦斯，克里斯蒂安·塞格迪。解释和利用对立的例子。第十四次评估报告(2021年)</li><li id="eeee" class="lw lx it lc b ld mh lg mi lj mj ln mk lr ml lv md me mf mg bi translated">Srinadh Bhojanapalli，Ayan Chakrabarti，Daniel，Daliang Li，Thomas Unterthiner，Andreas Veit。理解用于图像分类的变压器的鲁棒性。第十四次评估报告(2021年)</li><li id="596c" class="lw lx it lc b ld mh lg mi lj mj ln mk lr ml lv md me mf mg bi translated">Ilya Tolstikhin、Neil Houlsby、Alexander、Lucas Beyer、Zhai Hua Zhai、Thomas Unterthiner、Jessica Yung、Andreas Steiner、Daniel Keysers、Jakob Uszkoreit、Mario Lucic、Alexey Dosovitskiy。MLP混合器:全MLP的视觉建筑。第十四次评估报告(2021年)</li><li id="1c91" class="lw lx it lc b ld mh lg mi lj mj ln mk lr ml lv md me mf mg bi translated">、戴子航、苏大伟、郭诉乐。关注MLPs。第十四次评估报告(2021年)</li><li id="ccf9" class="lw lx it lc b ld mh lg mi lj mj ln mk lr ml lv md me mf mg bi translated">李中清-索普，约书亚·安斯利，伊利亚·埃克斯坦，圣地亚哥·翁塔农。FNet:用傅立叶变换混合记号。第十四次评估报告(2021年)</li><li id="217f" class="lw lx it lc b ld mh lg mi lj mj ln mk lr ml lv md me mf mg bi translated">Hugo Touvron，Matthieu Cord，Matthijs Douze，Francisco Massa，Alexandre Sablayrolles，Hervé Jégou。训练数据有效的图像转换器&amp;通过注意力进行提炼。第十四次评估报告(2020年)</li><li id="380d" class="lw lx it lc b ld mh lg mi lj mj ln mk lr ml lv md me mf mg bi translated">吴，，诺埃尔·柯黛拉，，刘，戴喜阳，陆源，。CvT:将卷积引入视觉变形器。第十四次评估报告(2021年)</li><li id="ac24" class="lw lx it lc b ld mh lg mi lj mj ln mk lr ml lv md me mf mg bi translated">袁昆，，郭，，周傲军，，于。将卷积设计结合到视觉转换器中。第十四次评估报告(2021年)</li><li id="031e" class="lw lx it lc b ld mh lg mi lj mj ln mk lr ml lv md me mf mg bi translated">、林语桐、、韩虎、、魏、、林、郭柏宁。Swin Transformer:使用移位窗口的分层视觉转换器。第十四次评估报告(2021年)</li><li id="bf80" class="lw lx it lc b ld mh lg mi lj mj ln mk lr ml lv md me mf mg bi translated">、谢恩泽、、邓、、宋、、、罗平、邵凌。金字塔视觉转换器:无卷积密集预测的通用主干。第十四次评估报告(2021年)</li><li id="7de3" class="lw lx it lc b ld mh lg mi lj mj ln mk lr ml lv md me mf mg bi translated">李远，，陈，，，俞，，蒋子航，刘永泰，冯家实，严水成。Token-to-Token ViT:在ImageNet上从头开始训练视觉变形人。arXiv(2021)。</li><li id="1590" class="lw lx it lc b ld mh lg mi lj mj ln mk lr ml lv md me mf mg bi translated">周大全、康、金晓杰、杨、连、姜子航、侯启斌、冯家实。DeepViT:走向更深层的视觉转换。第十四次评估报告(2021年)</li><li id="806f" class="lw lx it lc b ld mh lg mi lj mj ln mk lr ml lv md me mf mg bi translated">勒内·兰福特，阿列克谢·博奇科夫斯基，弗拉德连·科尔敦。用于密集预测的视觉转换器。第十四次评估报告(2021年)</li><li id="c337" class="lw lx it lc b ld mh lg mi lj mj ln mk lr ml lv md me mf mg bi translated">赵恒爽，，贾亚佳，菲利普托尔，弗拉德连科尔顿。点变压器。第十四次评估报告(2020年)</li><li id="9e81" class="lw lx it lc b ld mh lg mi lj mj ln mk lr ml lv md me mf mg bi translated">安德鲁·耶格尔，菲利克斯·吉梅诺，安德鲁·布洛克，安德鲁·齐泽曼，奥里奥尔·维尼亚尔斯，若昂·卡雷拉。感知者:具有重复注意的一般感知。第十四次评估报告(2021年)</li><li id="8cd5" class="lw lx it lc b ld mh lg mi lj mj ln mk lr ml lv md me mf mg bi translated">张倩、卢汉、哈西姆·萨克、安舒曼·特里帕蒂、埃里克·麦克德莫特、斯蒂芬·库、尚卡尔·库马尔。变压器传感器:一个流语音识别模型与变压器编码器和RNN-T损失。第十四次评估报告(2021年)</li><li id="ea8a" class="lw lx it lc b ld mh lg mi lj mj ln mk lr ml lv md me mf mg bi translated">金元宰，孙宝庆，金一斗。无卷积或区域监控的视觉-语言转换器。第十四次评估报告(2021年)</li><li id="ecd8" class="lw lx it lc b ld mh lg mi lj mj ln mk lr ml lv md me mf mg bi translated">哈桑·阿克巴里、李良哲、袁瑞倩、庄伟红、张世福、尹翠、龚柏青。VATT:从原始视频、音频和文本进行多模式自我监督学习的变形金刚。第十四次评估报告(2021年)</li><li id="daa1" class="lw lx it lc b ld mh lg mi lj mj ln mk lr ml lv md me mf mg bi translated">赵在民，解磊，郝坦，莫希特.班萨尔。通过文本生成统一视觉和语言任务。第十四次评估报告(2021年)</li></ol></div></div>    
</body>
</html>