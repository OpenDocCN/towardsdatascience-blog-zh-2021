<html>
<head>
<title>3 Ways To Create Tables With Apache Spark</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用Apache Spark创建表格的3种方法</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/3-ways-to-create-tables-with-apache-spark-32aed0f355ab?source=collection_archive---------1-----------------------#2021-04-28">https://towardsdatascience.com/3-ways-to-create-tables-with-apache-spark-32aed0f355ab?source=collection_archive---------1-----------------------#2021-04-28</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="705d" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsdatascience.com/tagged/programming" rel="noopener" target="_blank">编程</a> | <a class="ae ep" href="https://towardsdatascience.com/tagged/big-data" rel="noopener" target="_blank">大数据</a> | <a class="ae ep" href="https://towardsdatascience.com/tagged/pyspark" rel="noopener" target="_blank"> PySpark </a></h2><div class=""/><div class=""><h2 id="196a" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">在本实践教程中，学习如何使用PySpark构建托管和非托管表，以及如何在项目中有效地使用它们。</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><a href="https://anbento4.medium.com/"><div class="gh gi kr"><img src="../Images/2c7d55f6c721a017ecc4cfc54e182a5d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*uUr35NdoRt44E8vh"/></div></a><p class="kz la gj gh gi lb lc bd b be z dk translated">照片由<a class="ae ld" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的<a class="ae ld" href="https://unsplash.com/@jeztimms?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Jez Timms </a>拍摄</p></figure><h2 id="87f2" class="le lf it bd lg lh li dn lj lk ll dp lm ln lo lp lq lr ls lt lu lv lw lx ly iz bi translated">点播课程|推荐</h2><p id="cf54" class="pw-post-body-paragraph lz ma it mb b mc md kd me mf mg kg mh ln mi mj mk lr ml mm mn lv mo mp mq mr im bi translated"><em class="ms">我的一些读者联系我，要求提供点播课程，以了解更多关于使用Python的</em><strong class="mb jd"><em class="ms">Apache Spark</em></strong><em class="ms">。这是我推荐的3个很好的资源:</em></p><ul class=""><li id="6eec" class="mt mu it mb b mc mv mf mw ln mx lr my lv mz mr na nb nc nd bi translated"><a class="ae ld" href="https://imp.i115008.net/zaX10r" rel="noopener ugc nofollow" target="_blank"> <strong class="mb jd">用阿帕奇卡夫卡的数据流&amp;阿帕奇火花纳米度(UDACITY) </strong> </a></li><li id="3413" class="mt mu it mb b mc ne mf nf ln ng lr nh lv ni mr na nb nc nd bi translated"><a class="ae ld" href="https://imp.i115008.net/zaX10r" rel="noopener ugc nofollow" target="_blank"> <strong class="mb jd">数据工程纳米学位(UDACITY) </strong> </a></li><li id="63d3" class="mt mu it mb b mc ne mf nf ln ng lr nh lv ni mr na nb nc nd bi translated"><a class="ae ld" href="https://click.linksynergy.com/deeplink?id=533LxfDBSaM&amp;mid=39197&amp;murl=https%3A%2F%2Fwww.udemy.com%2Fcourse%2Fspark-and-python-for-big-data-with-pyspark%2F" rel="noopener ugc nofollow" target="_blank"> <strong class="mb jd"> Spark与Python对于大数据的配合</strong> </a></li></ul><p id="d300" class="pw-post-body-paragraph lz ma it mb b mc mv kd me mf mw kg mh ln nj mj mk lr nk mm mn lv nl mp mq mr im bi translated"><em class="ms">还不是中等会员？考虑与我的</em> <a class="ae ld" href="https://anbento4.medium.com/membership" rel="noopener"> <em class="ms">推荐链接</em> </a> <em class="ms">注册，以每月5美元的价格获得Medium提供的一切！</em></p></div><div class="ab cl nm nn hx no" role="separator"><span class="np bw bk nq nr ns"/><span class="np bw bk nq nr ns"/><span class="np bw bk nq nr"/></div><div class="im in io ip iq"><h1 id="f479" class="nt lf it bd lg nu nv nw lj nx ny nz lm ki oa kj lq kl ob km lu ko oc kp ly od bi translated">介绍</h1><p id="045d" class="pw-post-body-paragraph lz ma it mb b mc md kd me mf mg kg mh ln mi mj mk lr ml mm mn lv mo mp mq mr im bi translated"><a class="ae ld" href="https://databricks.com/spark/about" rel="noopener ugc nofollow" target="_blank"> Apache Spark </a>是一个分布式数据处理引擎，允许您创建两种主要类型的表:</p><ol class=""><li id="84f4" class="mt mu it mb b mc mv mf mw ln mx lr my lv mz mr oe nb nc nd bi translated"><strong class="mb jd">托管(或内部)表:</strong>对于这些表，Spark管理数据和元数据。特别是，数据通常保存在Spark SQL warehouse目录中——这是托管表的<em class="ms">默认设置</em>——而元数据保存在关系实体的元存储中(包括<em class="ms">数据库、表、临时视图</em>),可以通过一个称为“目录”的接口访问。</li><li id="24c0" class="mt mu it mb b mc ne mf nf ln ng lr nh lv ni mr oe nb nc nd bi translated"><strong class="mb jd">非托管(或外部)表:</strong>对于这些表，Spark只管理元数据，但是要求您指定保存表的确切位置，或者指定从中提取数据以创建表的源目录。</li></ol><p id="d4ac" class="pw-post-body-paragraph lz ma it mb b mc mv kd me mf mw kg mh ln nj mj mk lr nk mm mn lv nl mp mq mr im bi translated">此外，由于它们的目的不同:</p><ul class=""><li id="92a3" class="mt mu it mb b mc mv mf mw ln mx lr my lv mz mr na nb nc nd bi translated"><strong class="mb jd">如果您删除一个托管表</strong>，Spark将删除仓库中的表数据和元存储中的元数据，这意味着您既不能直接查询该表，也不能向其中检索数据。</li><li id="f4a2" class="mt mu it mb b mc ne mf nf ln ng lr nh lv ni mr na nb nc nd bi translated"><strong class="mb jd">如果您删除一个非托管表</strong>，Spark将只删除元数据，这意味着您将无法再查询该表，因为您的查询将不会在执行分析阶段根据目录进行解析；但是您仍然可以在外部位置找到您创建的表。</li></ul><p id="6c5b" class="pw-post-body-paragraph lz ma it mb b mc mv kd me mf mw kg mh ln nj mj mk lr nk mm mn lv nl mp mq mr im bi translated">在本教程中，我将分享三种创建托管表和非托管表的方法，并解释使用其中一种有意义的情况。</p><div class="of og gp gr oh oi"><a rel="noopener follow" target="_blank" href="/3-ways-to-aggregate-data-in-pyspark-72209197c90"><div class="oj ab fo"><div class="ok ab ol cl cj om"><h2 class="bd jd gy z fp on fr fs oo fu fw jc bi translated">PySpark中聚合数据的3种方法</h2><div class="op l"><h3 class="bd b gy z fp on fr fs oo fu fw dk translated">PySpark用编码示例解释基本聚合。</h3></div><div class="oq l"><p class="bd b dl z fp on fr fs oo fu fw dk translated">towardsdatascience.com</p></div></div><div class="or l"><div class="os l ot ou ov or ow kx oi"/></div></div></a></div><h1 id="3734" class="nt lf it bd lg nu ox nw lj nx oy nz lm ki oz kj lq kl pa km lu ko pb kp ly od bi translated">初始数据集操作</h1><p id="f362" class="pw-post-body-paragraph lz ma it mb b mc md kd me mf mg kg mh ln mi mj mk lr ml mm mn lv mo mp mq mr im bi translated">如果您希望继续学习，但是对Spark相对陌生，并且没有更好的选择，我强烈建议您使用<a class="ae ld" href="https://community.cloud.databricks.com/login.html" rel="noopener ugc nofollow" target="_blank"> Databrick的社区版</a>，因为它可以让您访问具有15GB内存和2个内核的集群来执行Spark代码。</p><p id="7aab" class="pw-post-body-paragraph lz ma it mb b mc mv kd me mf mw kg mh ln nj mj mk lr nk mm mn lv nl mp mq mr im bi translated">我将要使用的<code class="fe pc pd pe pf b">sales_redords</code>数据集相当大(600MB ),因为它包括500万行和14列——你可以在这里下载它<a class="ae ld" href="http://eforexcel.com/wp/downloads-18-sample-csv-files-data-sets-for-testing-sales/" rel="noopener ugc nofollow" target="_blank">。我选择了一个相当大的数据集——至少部分地——来复制您在现实世界中必须处理的数据量。</a></p><p id="325d" class="pw-post-body-paragraph lz ma it mb b mc mv kd me mf mw kg mh ln nj mj mk lr nk mm mn lv nl mp mq mr im bi translated">因为数据集是半结构化的CSV格式，为了在SparkSession中创建DataFrame，请确保首先上传<a class="ae ld" href="https://docs.databricks.com/data/databricks-file-system.html" rel="noopener ugc nofollow" target="_blank"> DataBricks文件系统(DBFS) </a>的<code class="fe pc pd pe pf b">/FileStore/</code>目录中的原始文件，然后运行以下代码:</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="pg ph l"/></div></figure><p id="951e" class="pw-post-body-paragraph lz ma it mb b mc mv kd me mf mw kg mh ln nj mj mk lr nk mm mn lv nl mp mq mr im bi translated">为了模拟原始数据作为日常ETL管道的一部分所经历的清理过程，我们假设您希望:</p><ul class=""><li id="369c" class="mt mu it mb b mc mv mf mw ln mx lr my lv mz mr na nb nc nd bi translated">将原始列名转换为小写，并将空格“”替换为下划线“_”；</li><li id="886a" class="mt mu it mb b mc ne mf nf ln ng lr nh lv ni mr na nb nc nd bi translated">将原来的<em class="ms">【订单日期】</em>从<code class="fe pc pd pe pf b">STRING</code>转换为<code class="fe pc pd pe pf b">DATE</code>，将原来的<em class="ms">【售出单位】、</em> <em class="ms">【单价】</em>、<em class="ms">【总收入】</em>从<code class="fe pc pd pe pf b">STRING</code>转换为<code class="fe pc pd pe pf b">FLOAT</code>；</li><li id="080c" class="mt mu it mb b mc ne mf nf ln ng lr nh lv ni mr na nb nc nd bi translated">删除以下各列，因为您的利益相关方不需要这些列:[ <em class="ms">"地区"、"国家"、"订单优先级"、"发货日期"、"总利润"、"总成本"、"单位成本"</em>]；</li><li id="cd7e" class="mt mu it mb b mc ne mf nf ln ng lr nh lv ni mr na nb nc nd bi translated">删除原“<em class="ms">订单ID </em>字段中的重复项。</li></ul><p id="7057" class="pw-post-body-paragraph lz ma it mb b mc mv kd me mf mw kg mh ln nj mj mk lr nk mm mn lv nl mp mq mr im bi translated">这可以通过执行下面的代码来实现，该代码创建了一个名为<code class="fe pc pd pe pf b">df_final</code>的新数据帧，它只包含7列和2M行。(<em class="ms">前5行也显示</em>):</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="pg ph l"/></div></figure><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi pi"><img src="../Images/dfee1be102f2365cc469212211a21add.png" data-original-src="https://miro.medium.com/v2/resize:fit:1294/format:webp/1*oXM6dPYXe2Rx5mmt8UKW8g.png"/></div></figure><p id="d755" class="pw-post-body-paragraph lz ma it mb b mc mv kd me mf mw kg mh ln nj mj mk lr nk mm mn lv nl mp mq mr im bi translated">已经对<code class="fe pc pd pe pf b">df_final</code>进行了处理，以获得想要的结果，现在可以将其用作创建表格的源了。</p><p id="7466" class="pw-post-body-paragraph lz ma it mb b mc mv kd me mf mw kg mh ln nj mj mk lr nk mm mn lv nl mp mq mr im bi translated">然而，为了向您展示所有3种不同的方法，我还必须将<code class="fe pc pd pe pf b">df_final</code>作为<strong class="mb jd">临时视图</strong>和DBFS中的<strong class="mb jd">拼花文件</strong>(分成2个分区)<strong class="mb jd"> </strong>:</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="pg ph l"/></div></figure><h1 id="6209" class="nt lf it bd lg nu ox nw lj nx oy nz lm ki oz kj lq kl pa km lu ko pb kp ly od bi translated">创建托管表</h1><p id="4a8b" class="pw-post-body-paragraph lz ma it mb b mc md kd me mf mg kg mh ln mi mj mk lr ml mm mn lv mo mp mq mr im bi translated">如上所述，当您创建一个由<em class="ms">管理的</em>表时，Spark将管理表数据和元数据(<em class="ms">关于表本身的信息</em> f)。具体来说，数据被写入默认的配置单元仓库，即在<code class="fe pc pd pe pf b">/user/hive/warehouse</code>位置设置的仓库。您可以改变这种行为，在生成<code class="fe pc pd pe pf b">SparkSession</code>时使用<code class="fe pc pd pe pf b">spark.sql.warehouse.dir</code>配置。</p><div class="of og gp gr oh oi"><a rel="noopener follow" target="_blank" href="/3-nanodegrees-you-should-consider-to-advance-your-data-engineering-career-in-2021-baf597debc72"><div class="oj ab fo"><div class="ok ab ol cl cj om"><h2 class="bd jd gy z fp on fr fs oo fu fw jc bi translated">3门数据工程课程，在2022年推进您的职业发展</h2><div class="op l"><h3 class="bd b gy z fp on fr fs oo fu fw dk translated">加入数据行业，改变角色或通过注册数据工程简单地学习前沿技术…</h3></div><div class="oq l"><p class="bd b dl z fp on fr fs oo fu fw dk translated">towardsdatascience.com</p></div></div><div class="or l"><div class="pj l ot ou ov or ow kx oi"/></div></div></a></div><h2 id="81ec" class="le lf it bd lg lh li dn lj lk ll dp lm ln lo lp lq lr ls lt lu lv lw lx ly iz bi translated">方法1</h2><p id="834d" class="pw-post-body-paragraph lz ma it mb b mc md kd me mf mg kg mh ln mi mj mk lr ml mm mn lv mo mp mq mr im bi translated">创建托管表最直接的方法是通过结构化API <code class="fe pc pd pe pf b">saveAsTable()</code>方法编写<code class="fe pc pd pe pf b">df_final</code>，而不指定任何路径:</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="pg ph l"/></div></figure><p id="5871" class="pw-post-body-paragraph lz ma it mb b mc mv kd me mf mw kg mh ln nj mj mk lr nk mm mn lv nl mp mq mr im bi translated">您可以通过运行以下命令来检查该命令是否成功创建了一个名为<code class="fe pc pd pe pf b">salesTable_manag1</code>且带有<code class="fe pc pd pe pf b">tableType = 'MANAGED'</code>的永久表:</p><pre class="ks kt ku kv gt pk pf pl pm aw pn bi"><span id="fcbd" class="le lf it pf b gy po pp l pq pr">spark.catalog.listTables()</span><span id="8361" class="le lf it pf b gy ps pp l pq pr">Out[1]: [Table(name=’salestable_manag1', database=’default’, description=None, tableType=’MANAGED’, isTemporary=False)</span></pre><p id="b15c" class="pw-post-body-paragraph lz ma it mb b mc mv kd me mf mw kg mh ln nj mj mk lr nk mm mn lv nl mp mq mr im bi translated">在大多数情况下，您应该更喜欢使用这种方法，因为它的语法非常紧凑，可读性很强，并且避免了在内存中创建临时视图的额外步骤。</p><h2 id="7908" class="le lf it bd lg lh li dn lj lk ll dp lm ln lo lp lq lr ls lt lu lv lw lx ly iz bi translated">方法2</h2><p id="6224" class="pw-post-body-paragraph lz ma it mb b mc md kd me mf mg kg mh ln mi mj mk lr ml mm mn lv mo mp mq mr im bi translated">创建托管表的另一种方法是运行SQL命令，查询temp <code class="fe pc pd pe pf b">df_final_View</code>中的所有记录:</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="pg ph l"/></div></figure><p id="0f10" class="pw-post-body-paragraph lz ma it mb b mc mv kd me mf mw kg mh ln nj mj mk lr nk mm mn lv nl mp mq mr im bi translated">在这种情况下，我使用Databricks中的<code class="fe pc pd pe pf b">%sql</code>魔法直接运行一个SQL命令，而没有将它打包到<code class="fe pc pd pe pf b">spark.sql()</code>中。但是，您可以使用以下语法获得完全相同的结果:</p><pre class="ks kt ku kv gt pk pf pl pm aw pn bi"><span id="474a" class="le lf it pf b gy po pp l pq pr">spark.sql(“CREATE TABLE IF NOT EXISTS salesTable_manag2 AS SELECT * FROM df_final_View”)</span></pre><p id="7bc1" class="pw-post-body-paragraph lz ma it mb b mc mv kd me mf mw kg mh ln nj mj mk lr nk mm mn lv nl mp mq mr im bi translated">如果您有SQL背景，这种方法可能是最熟悉的，因为您不需要为“标准的”结构化API语法而烦恼，甚至可以动态地执行额外的操作。但是，在处理大数据时，您应该考虑在集群上创建临时视图所需的额外空间。</p><h2 id="d081" class="le lf it bd lg lh li dn lj lk ll dp lm ln lo lp lq lr ls lt lu lv lw lx ly iz bi translated">方法3</h2><p id="0b1a" class="pw-post-body-paragraph lz ma it mb b mc md kd me mf mg kg mh ln mi mj mk lr ml mm mn lv mo mp mq mr im bi translated">您可以使用的最后一种方法与前一种类似，但它包括两个步骤，首先创建一个表<code class="fe pc pd pe pf b">salesTable_manag3</code>，然后通过查询临时视图将数据插入其中:</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="pg ph l"/></div></figure><p id="5d38" class="pw-post-body-paragraph lz ma it mb b mc mv kd me mf mw kg mh ln nj mj mk lr nk mm mn lv nl mp mq mr im bi translated">当您希望更改列类型时，或者如果您已经创建了一个表，并且希望替换或追加数据，而不是删除它并从头开始时，您应该选择此方法。</p><h1 id="cac8" class="nt lf it bd lg nu ox nw lj nx oy nz lm ki oz kj lq kl pa km lu ko pb kp ly od bi translated">创建非托管表</h1><p id="add1" class="pw-post-body-paragraph lz ma it mb b mc md kd me mf mg kg mh ln mi mj mk lr ml mm mn lv mo mp mq mr im bi translated"><em class="ms">非托管</em>表提供了更大的灵活性，因为表数据可以存储在您选择的位置，或者表可以直接构建在外部目录中的可用数据之上。反过来，这意味着在Spark中，对于外部表来说，位置是<em class="ms">强制的</em>。元数据再次保存在元存储中，并可通过目录访问。</p><blockquote class="pt"><p id="a9d8" class="pu pv it bd pw px py pz qa qb qc mr dk translated"><em class="qd">非托管</em>表提供了更大的灵活性，因为表数据可以存储在您选择的位置，或者表可以直接构建在外部目录中的可用数据之上。</p></blockquote><p id="a2d7" class="pw-post-body-paragraph lz ma it mb b mc qe kd me mf qf kg mh ln qg mj mk lr qh mm mn lv qi mp mq mr im bi translated">在下面的例子中，我将使用<em class="ms"> </em> <a class="ae ld" href="https://docs.databricks.com/data/databricks-file-system.html" rel="noopener ugc nofollow" target="_blank"> <em class="ms"> Databricks文件系统</em> </a>来模拟一个关于默认Spark SQL仓库的外部位置，但是当然，可以保存非托管表(或者在与Spark兼容的每个文件系统之上创建它们)，包括<strong class="mb jd"> <em class="ms">云数据仓库</em> </strong>。</p><h2 id="c864" class="le lf it bd lg lh li dn lj lk ll dp lm ln lo lp lq lr ls lt lu lv lw lx ly iz bi translated">方法1</h2><p id="a8b3" class="pw-post-body-paragraph lz ma it mb b mc md kd me mf mg kg mh ln mi mj mk lr ml mm mn lv mo mp mq mr im bi translated">要创建一个非托管(外部)表，只需在<code class="fe pc pd pe pf b">saveAsTable()</code>方法之前指定一个路径:</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="pg ph l"/></div></figure><p id="3ba2" class="pw-post-body-paragraph lz ma it mb b mc mv kd me mf mw kg mh ln nj mj mk lr nk mm mn lv nl mp mq mr im bi translated">当您运行这段代码时，Spark将:</p><ul class=""><li id="5ac0" class="mt mu it mb b mc mv mf mw ln mx lr my lv mz mr na nb nc nd bi translated">打乱<code class="fe pc pd pe pf b">df_final</code>数据帧中的数据，创建两个分区，并将它们写入<code class="fe pc pd pe pf b">/FileStore/tables/salesTable_unmanag1</code>目录。</li><li id="7489" class="mt mu it mb b mc ne mf nf ln ng lr nh lv ni mr na nb nc nd bi translated">使用存储在该位置的分区创建一个名为<code class="fe pc pd pe pf b">salesTable_unmanag1</code>的外部表，并将相关信息保存在元存储中。</li></ul><h2 id="baeb" class="le lf it bd lg lh li dn lj lk ll dp lm ln lo lp lq lr ls lt lu lv lw lx ly iz bi translated">方法2</h2><p id="bc73" class="pw-post-body-paragraph lz ma it mb b mc md kd me mf mg kg mh ln mi mj mk lr ml mm mn lv mo mp mq mr im bi translated">因为通过将位置指定为SQL查询的一部分可以获得类似的结果。在这种情况下，您需要使用临时视图作为数据源:</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="pg ph l"/></div></figure><p id="7a47" class="pw-post-body-paragraph lz ma it mb b mc mv kd me mf mw kg mh ln nj mj mk lr nk mm mn lv nl mp mq mr im bi translated">还要记住使用<code class="fe pc pd pe pf b">CREATE EXTERNAL TABLE</code>语法，而不是<code class="fe pc pd pe pf b">CREATE TABLE</code>。</p><h2 id="3d69" class="le lf it bd lg lh li dn lj lk ll dp lm ln lo lp lq lr ls lt lu lv lw lx ly iz bi translated">方法3</h2><p id="df80" class="pw-post-body-paragraph lz ma it mb b mc md kd me mf mg kg mh ln mi mj mk lr ml mm mn lv mo mp mq mr im bi translated">最后，如果您打算用来创建表的数据已经在一个外部位置可用，您可以通过使用<code class="fe pc pd pe pf b">USING format OPTIONS (path 'path to location')</code>语法指向该位置，简单地在它的上面构建表:</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="pg ph l"/></div></figure><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="qk ql di qm bf qn"><div class="gh gi qj"><img src="../Images/5b9752aa1bebe6a88b36c0f7afe09431.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sQ0vZdrpG7ufC00mc1wpWg.png"/></div></div></figure><p id="b2f9" class="pw-post-body-paragraph lz ma it mb b mc mv kd me mf mw kg mh ln nj mj mk lr nk mm mn lv nl mp mq mr im bi translated">在这种情况下，该表是在操作原始数据集后保存在<code class="fe pc pd pe pf b">FileStore</code>中的<code class="fe pc pd pe pf b">df_final.parquet</code>文件的顶部创建的。请注意，如果您使用这种方法，您将需要指定每一列的类型。</p><p id="9134" class="pw-post-body-paragraph lz ma it mb b mc mv kd me mf mw kg mh ln nj mj mk lr nk mm mn lv nl mp mq mr im bi translated">如果您尝试再次列出目录中的表:</p><pre class="ks kt ku kv gt pk pf pl pm aw pn bi"><span id="ab65" class="le lf it pf b gy po pp l pq pr">spark.catalog.listTables()</span></pre><p id="74d5" class="pw-post-body-paragraph lz ma it mb b mc mv kd me mf mw kg mh ln nj mj mk lr nk mm mn lv nl mp mq mr im bi translated">您可以看到，现在输出包括6个表，其中3个是托管表，3个是非托管表，以及在开始时创建的临时视图:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="qk ql di qm bf qn"><div class="gh gi qo"><img src="../Images/6eabff916272582837949b8d5fbb9a72.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cZFMSpL4HAOm3U7zAb6MAA.png"/></div></div></figure><p id="3c20" class="pw-post-body-paragraph lz ma it mb b mc mv kd me mf mw kg mh ln nj mj mk lr nk mm mn lv nl mp mq mr im bi translated">执行以下代码将只删除类型表中的元数据和托管表中的数据，而数据将保留在您指定的外部位置:</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="pg ph l"/></div></figure><p id="6b87" class="pw-post-body-paragraph lz ma it mb b mc mv kd me mf mw kg mh ln nj mj mk lr nk mm mn lv nl mp mq mr im bi translated">但是，删除元数据后，您将无法对任何表运行查询。</p><h1 id="d162" class="nt lf it bd lg nu ox nw lj nx oy nz lm ki oz kj lq kl pa km lu ko pb kp ly od bi translated">何时使用托管表或外部表？</h1><p id="fc3d" class="pw-post-body-paragraph lz ma it mb b mc md kd me mf mg kg mh ln mi mj mk lr ml mm mn lv mo mp mq mr im bi translated">到目前为止，您应该已经很好地理解了这两种类型的表之间的区别，并且应该准备好将上面的代码应用到您的特定用例中。然而，在一个真实的项目中，当你在两者之间选择一个的时候，也许你仍然有点困惑。</p><p id="077b" class="pw-post-body-paragraph lz ma it mb b mc mv kd me mf mw kg mh ln nj mj mk lr nk mm mn lv nl mp mq mr im bi translated">一般来说，<strong class="mb jd"> </strong>你应该将数据存储在一个<strong class="mb jd">托管表</strong>中:</p><ol class=""><li id="c5a7" class="mt mu it mb b mc mv mf mw ln mx lr my lv mz mr oe nb nc nd bi translated"><strong class="mb jd">当您希望</strong> <a class="ae ld" href="https://blog.knoldus.com/apache-spark-sdsoh/" rel="noopener ugc nofollow" target="_blank"> <strong class="mb jd">使用Spark作为数据库</strong> </a> <strong class="mb jd">执行特定或交互式查询以浏览和可视化数据集</strong> <em class="ms"> </em> <strong class="mb jd"> → </strong> <em class="ms">例如，您可以在Spark中设计一个ETL管道，最终将数据存储在一个托管表中，然后使用JDBC-ODBC连接器通过Looker、Tableau、Power BI和其他BI工具来查询该表。</em></li><li id="7e13" class="mt mu it mb b mc ne mf nf ln ng lr nh lv ni mr oe nb nc nd bi translated"><strong class="mb jd">当您正在处理一个项目，并希望在将数据写入最终位置之前，暂时将数据保存在Spark中以进行额外的操作或测试时</strong> → <em class="ms">例如，在Spark中构建、训练和评估机器学习模型时，托管表可能会很方便，因为它们消除了对外部存储的需要，从而可以保存部分迭代。</em></li><li id="a459" class="mt mu it mb b mc ne mf nf ln ng lr nh lv ni mr oe nb nc nd bi translated"><strong class="mb jd">当您希望Spark处理包括删除在内的表数据的完整生命周期，或者担心外部文件系统的安全性</strong> → <em class="ms">如果数据不需要立即与其他客户端共享，或者存在安全问题，那么将数据保存在Spark warehouse中可能是一个有效的临时解决方案。</em></li><li id="db9e" class="mt mu it mb b mc ne mf nf ln ng lr nh lv ni mr oe nb nc nd bi translated"><strong class="mb jd">当您不担心数据的可再现性时</strong> → <em class="ms">如果数据可以很容易地从其他来源检索到，或者转换数据所需的计算工作量不太大，那么错误删除受管表的风险就更小了。</em></li></ol><p id="fdd0" class="pw-post-body-paragraph lz ma it mb b mc mv kd me mf mw kg mh ln nj mj mk lr nk mm mn lv nl mp mq mr im bi translated">相反，您应该将数据存储在一个<strong class="mb jd">外部</strong>表中:</p><ol class=""><li id="ebf8" class="mt mu it mb b mc mv mf mw ln mx lr my lv mz mr oe nb nc nd bi translated"><strong class="mb jd">当您无法基于SparkSession中已经可用的现有数据框架或视图创建表格时，会附带一个推断模式</strong> → <em class="ms">在这种情况下，您必须提供首选位置并指定正确的字段类型。</em></li><li id="7172" class="mt mu it mb b mc ne mf nf ln ng lr nh lv ni mr oe nb nc nd bi translated"><strong class="mb jd">当Spark主要用于并行处理大型数据集时，通过将它们分布在集群中，或者通过批处理或流实现端到端的数据管道</strong> → <em class="ms">在这种情况下，Spark通常用于繁重的计算，但数据最终会存储到外部数据湖或写入最终的云数据仓库。</em></li><li id="fb06" class="mt mu it mb b mc ne mf nf ln ng lr nh lv ni mr oe nb nc nd bi translated"><strong class="mb jd">当数据需要保留在指定的外部位置时，即使在Spark中删除了非托管表</strong> → <em class="ms">当有多个表或模式构建在相同数据之上，并且您不希望危及它们的完整性时，通常会出现这种情况。</em></li><li id="989c" class="mt mu it mb b mc ne mf nf ln ng lr nh lv ni mr oe nb nc nd bi translated"><strong class="mb jd">当Spark不应拥有数据生命周期时，例如控制设置、目录和模式</strong> → <em class="ms">，您可能已经有了其他解决方案来完成这些任务。</em></li></ol></div><div class="ab cl nm nn hx no" role="separator"><span class="np bw bk nq nr ns"/><span class="np bw bk nq nr ns"/><span class="np bw bk nq nr"/></div><div class="im in io ip iq"><h2 id="b270" class="le lf it bd lg lh li dn lj lk ll dp lm ln lo lp lq lr ls lt lu lv lw lx ly iz bi translated">给我的读者一个提示</h2><blockquote class="qp qq qr"><p id="081b" class="lz ma ms mb b mc mv kd me mf mw kg mh qs nj mj mk qt nk mm mn qu nl mp mq mr im bi translated">这个帖子包括附属链接，如果你购买的话，我可以免费给你一点佣金。</p></blockquote><h1 id="cc42" class="nt lf it bd lg nu ox nw lj nx oy nz lm ki oz kj lq kl pa km lu ko pb kp ly od bi translated">来源</h1><ul class=""><li id="ceab" class="mt mu it mb b mc md mf mg ln qv lr qw lv qx mr na nb nc nd bi translated"><a class="ae ld" href="https://www.oreilly.com/library/view/spark-the-definitive/9781491912201/" rel="noopener ugc nofollow" target="_blank">火花——权威指南——比尔·查姆&amp;马泰·扎哈里亚</a></li><li id="52bf" class="mt mu it mb b mc ne mf nf ln ng lr nh lv ni mr na nb nc nd bi translated">学习火花——帕特里克·温德尔著&amp;马泰·扎哈里亚</li><li id="5354" class="mt mu it mb b mc ne mf nf ln ng lr nh lv ni mr na nb nc nd bi translated"><a class="ae ld" href="https://medium.com/@subashsivaji/types-of-apache-spark-tables-and-views-f468e2e53af2" rel="noopener">Apache Spark表格和视图的类型| Subash西瓦吉| Medium </a></li><li id="e556" class="mt mu it mb b mc ne mf nf ln ng lr nh lv ni mr na nb nc nd bi translated"><a class="ae ld" href="https://blog.knoldus.com/apache-spark-sdsoh/" rel="noopener ugc nofollow" target="_blank">使用Spark作为数据库— Knoldus博客</a></li><li id="47cf" class="mt mu it mb b mc ne mf nf ln ng lr nh lv ni mr na nb nc nd bi translated"><a class="ae ld" href="https://www.linkedin.com/pulse/internal-external-tables-hadoop-hive-big-data-island-amandeep-modgil/" rel="noopener ugc nofollow" target="_blank">内部&amp;Hadoop-HIVE(大数据岛)中的外部表| LinkedIn </a></li></ul></div></div>    
</body>
</html>