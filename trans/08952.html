<html>
<head>
<title>Transformers, can you rate the complexity of reading passages?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">变形金刚，你能评价阅读段落的复杂程度吗？</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/transformers-can-you-rate-the-complexity-of-reading-passages-17c76da3403?source=collection_archive---------15-----------------------#2021-08-18">https://towardsdatascience.com/transformers-can-you-rate-the-complexity-of-reading-passages-17c76da3403?source=collection_archive---------15-----------------------#2021-08-18</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="1dd7" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsdatascience.com/tagged/hands-on-tutorials" rel="noopener" target="_blank">实践教程</a></h2><div class=""/><div class=""><h2 id="1a13" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">用PyTorch微调RoBERTa以预测文本摘录的阅读难易程度</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/11d761b6c8940d31152ab10b3bc544f1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*YFXpdzHIPzVNlXdq"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">照片由<a class="ae lh" href="https://unsplash.com/@jefferyho?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Jeffery Ho </a>在<a class="ae lh" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><p id="cef5" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi me translated">变压器，它们到底是什么？它们不是用于电能动力传输的装置，也不是虚构的活生生的自主机器人<em class="mn">擎天柱</em>或<em class="mn">大黄蜂</em>可以变形为车辆等其他物体。在我们的上下文中，Transformers指的是BERT、ALBERT、RoBERTa等，它们在数据科学领域中用于解决各种自然语言处理任务，如机器翻译、文本摘要、语音识别、情感分析等。它们是用于自然语言处理的最先进的语言模型，并且在过去的几年中获得了极大的欢迎。</p><p id="16ca" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">这篇文章将展示Transformer模型的微调，特别是RoBERTa在我们感兴趣的数据集上的微调。对下游任务进行微调，以预测3-12年级课堂使用的文学作品摘录的阅读难易程度。</p><p id="537c" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">这项工作是由非营利教育技术组织CommonLit发起的。它赞助了在Kaggle上举办的一场比赛(你可以在这里阅读更多信息)，旨在使用机器学习来寻求对现有可读性评级方法的改进。这将有助于读写课程的开发者以及教育者为学生选择合适的阅读文章。呈现具有适当复杂程度和阅读挑战的引人入胜的段落将极大地有利于学生发展基本的阅读技能。</p><h1 id="a31c" class="mo mp it bd mq mr ms mt mu mv mw mx my ki mz kj na kl nb km nc ko nd kp ne nf bi translated"><strong class="ak">目录</strong></h1><p id="1153" class="pw-post-body-paragraph li lj it lk b ll ng kd ln lo nh kg lq lr ni lt lu lv nj lx ly lz nk mb mc md im bi translated"><strong class="lk jd"> 1。</strong> <a class="ae lh" href="#9ec0" rel="noopener ugc nofollow"> <strong class="lk jd">关于数据集</strong> </a> <br/> <strong class="lk jd"> 2。</strong> <a class="ae lh" href="#cee1" rel="noopener ugc nofollow"> <strong class="lk jd">拆分数据</strong> </a> <br/> <strong class="lk jd"> 3。</strong> <a class="ae lh" href="#a815" rel="noopener ugc nofollow"> <strong class="lk jd">创建数据集类</strong> </a> <br/> <strong class="lk jd"> 4。</strong> <a class="ae lh" href="#fd6f" rel="noopener ugc nofollow"> <strong class="lk jd">罗伯塔-基为我们的型号</strong> </a> <br/> <strong class="lk jd"> 5。</strong> <a class="ae lh" href="#e811" rel="noopener ugc nofollow"> <strong class="lk jd">变压器有哪些典型的原始输出？</strong> </a> <br/> <strong class="lk jd"> 6。</strong> <a class="ae lh" href="#2a6b" rel="noopener ugc nofollow"> <strong class="lk jd">定义模型类</strong> </a> <br/> ∘ <a class="ae lh" href="#1e02" rel="noopener ugc nofollow">使用pooler_output </a> <br/> ∘ <a class="ae lh" href="#1439" rel="noopener ugc nofollow">构建自己的变压器自定义头</a> <br/> ∘ <a class="ae lh" href="#1fbb" rel="noopener ugc nofollow"> (A)注意头</a> <br/> ∘ <a class="ae lh" href="#9f4a" rel="noopener ugc nofollow"> (B)串接隐藏层</a> <br/> <strong class="lk jd"> 7。</strong> <a class="ae lh" href="#6eaa" rel="noopener ugc nofollow"> <strong class="lk jd">模型训练</strong> </a> <br/> ∘ <a class="ae lh" href="#fb94" rel="noopener ugc nofollow">评价度量和损失函数</a> <br/> ∘ <a class="ae lh" href="#9d11" rel="noopener ugc nofollow">训练函数</a> <br/> ∘ <a class="ae lh" href="#7df2" rel="noopener ugc nofollow">验证函数</a> <br/> ∘ <a class="ae lh" href="#77df" rel="noopener ugc nofollow">运行训练</a> <br/> <a class="ae lh" href="#7680" rel="noopener ugc nofollow"> <strong class="lk jd">总结</strong> </a> <br/> <a class="ae lh" href="#ffa8" rel="noopener ugc nofollow"> <strong class="lk jd">参考文献<a class="ae lh" href="#9ec0" rel="noopener ugc nofollow"/></strong></a></p><h1 id="9ec0" class="mo mp it bd mq mr ms mt mu mv mw mx my ki mz kj na kl nb km nc ko nd kp ne nf bi translated">1.关于数据集</h1><p id="dd9b" class="pw-post-body-paragraph li lj it lk b ll ng kd ln lo nh kg lq lr ni lt lu lv nj lx ly lz nk mb mc md im bi translated">我们将要使用的数据集可以在<a class="ae lh" href="https://www.kaggle.com/c/commonlitreadabilityprize/data?select=train.csv" rel="noopener ugc nofollow" target="_blank">这个</a> Kaggle页面上找到。这个数据集包含大约2800条记录。我们将使用的两个重要字段是<code class="fe nl nm nn no b">excerpt</code>和<code class="fe nl nm nn no b">target</code>。</p><p id="d38f" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">查看数据，<code class="fe nl nm nn no b">excerpt</code>是预测阅读难易程度的文本，<code class="fe nl nm nn no b">target</code>是可以包含正值或负值的数值字段。如该数据集中所示，它是一个连续变量，最小值为-3.676268，最大值为1.711390。因此，给定一个特定的<code class="fe nl nm nn no b">excerpt</code>，我们需要预测<code class="fe nl nm nn no b">target</code>的值。</p><p id="7712" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">给大家一点背景知识，竞赛主持人Scott Crossley曾在<a class="ae lh" href="https://www.kaggle.com/c/commonlitreadabilityprize/discussion/240423" rel="noopener ugc nofollow" target="_blank">这个</a>讨论中提到“<em class="mn">目标值是一个</em><a class="ae lh" href="https://en.wikipedia.org/wiki/Bradley%E2%80%93Terry_model" rel="noopener ugc nofollow" target="_blank"><em class="mn">Bradley-Terry</em></a><em class="mn">分析超过111，000个两两比较摘录的结果。跨越3-12年级的教师，大多数在6-10年级教学，担任这些比较的评分者</em>。</p><p id="08d1" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">较高的<code class="fe nl nm nn no b">target</code>值对应于“<em class="mn">更易阅读”</em>，较低的值对应于“<em class="mn">更难阅读”</em>。例如，假设我们有三个摘录A、B和C，它们对应的<code class="fe nl nm nn no b">target</code>值是1.599999、-1.333333和-2.888888。这将意味着A比B更容易阅读，B比c更容易阅读。</p><p id="dec8" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">为了说明，下面是两个样本摘录。</p><pre class="ks kt ku kv gt np no nq nr aw ns bi"><span id="5439" class="nt mp it no b gy nu nv l nw nx"><strong class="no jd">Excerpt with target value of 1.541672:</strong></span><span id="6aa6" class="nt mp it no b gy ny nv l nw nx">More people came to the bus stop just before 9am. Half an hour later they are all still waiting. Sam is worried. "Maybe the bus broke down," he thinks. "Maybe we won't go to town today. Maybe I won't get my new school uniform." At 9:45am some people give up and go home. Sam starts to cry. "We will wait a bit longer," says his mother. Suddenly, they hear a noise. The bus is coming! The bus arrives at the stop at 10 o'clock. "Get in! Get in!" calls the driver. "We are very late today!" People get on the bus and sit down. The bus leaves the stop at 10:10am. "What time is the return bus this afternoon?" asks Sam's mother. "The blue bus leaves town at 2:30pm," replies the driver. Sam thinks, "We will get to town at 11 o'clock." "How much time will we have in town before the return bus?" wonders Sam.</span><span id="d75a" class="nt mp it no b gy ny nv l nw nx"><strong class="no jd">Excerpt with target value of -3.642892:</strong></span><span id="f917" class="nt mp it no b gy ny nv l nw nx">The iron cylinder weighs 23 kilogrammes; but, when the current has an intensity of 43 amperes and traverses 15 sections, the stress developed may reach 70 kilogrammes; that is to say, three times the weight of the hammer. So this latter obeys with absolute docility the motions of the operator's hands, as those who were present at the lecture were enabled to see. I will incidentally add that this power hammer was placed on a circuit derived from one that served likewise to supply three Hefner-Alteneck machines (Siemens D5 model) and a Gramme machine (Breguet model P.L.). Each of these machines was making 1,500 revolutions per minute and developing 25 kilogrammeters per second, measured by means of a Carpentier brake. All these apparatuses were operating with absolute independence, and had for generators the double excitation machine that figured at the Exhibition of Electricity. In an experiment made since then, I have succeeded in developing in each of these four machines 50 kilogrammeters per second, whatever was the number of those that were running; and I found it possible to add the hammer on a derived circuit without notably affecting the operation of the receivers.</span></pre><p id="1b72" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">显然，在这两篇摘录中，前者比后者更容易阅读。</p><h1 id="cee1" class="mo mp it bd mq mr ms mt mu mv mw mx my ki mz kj na kl nb km nc ko nd kp ne nf bi translated">2.拆分数据</h1><p id="5b78" class="pw-post-body-paragraph li lj it lk b ll ng kd ln lo nh kg lq lr ni lt lu lv nj lx ly lz nk mb mc md im bi translated">由于我们的数据集相当小，我们将使用<a class="ae lh" href="https://machinelearningmastery.com/k-fold-cross-validation/" rel="noopener ugc nofollow" target="_blank">交叉验证</a>来更准确地衡量我们模型的性能。因此，我们将使用分层k-fold将数据分为训练集和验证集。使用分层k-fold，通过保留每个类别的样本百分比来进行折叠。当我们有一个倾斜的数据集时，或者当<code class="fe nl nm nn no b">target</code>的分布不平衡时，这种方法是有用的。然而，因为我们的<code class="fe nl nm nn no b">target</code>是一个连续变量而不是类，所以我们需要某种变通方法。这就是宁滨<code class="fe nl nm nn no b">target</code>前来救援的地方。bin类似于类，这对于scikit-learn的<code class="fe nl nm nn no b"><a class="ae lh" href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedKFold.html" rel="noopener ugc nofollow" target="_blank">StratifiedKFold</a></code>来说再好不过了。</p><p id="44c9" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">代码相当简单。在计算对<code class="fe nl nm nn no b">target</code>进行分类所需的分类数之前，我们随机化数据行并重置行索引。一种方法是使用<a class="ae lh" href="https://www.vosesoftware.com/riskwiki/Sturgesrule.php" rel="noopener ugc nofollow" target="_blank">斯特奇法则</a>来确定要使用的箱数。接下来，我们使用scikit-learn的<code class="fe nl nm nn no b">StratifiedKFold</code>类，根据我们拥有的bin将数据分成5份。最后，生成的折叠编号(范围从0到4)将被分配给一个名为<code class="fe nl nm nn no b">skfold</code>的新列。过程结束时，不再需要垃圾箱，如果您愿意，可以将其丢弃。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="nz oa l"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">用<code class="fe nl nm nn no b">StratifiedKFold</code>创作褶皱，改编自Abhishek Thakur的笔记本<a class="ae lh" href="https://www.kaggle.com/abhishek/step-1-create-folds" rel="noopener ugc nofollow" target="_blank">这里</a>。</p></figure><p id="4b53" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">供您参考，完整数据集的平均值<code class="fe nl nm nn no b">target</code>为-0.96(四舍五入到小数点后两位)。拆分成5个褶皱后，我们可以看到<code class="fe nl nm nn no b">target</code>在每个褶皱上的分布形状都被保留了下来。看下面的图，每个折叠的平均值<code class="fe nl nm nn no b">target</code>几乎是一致的，它们确实非常接近-0.96。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi ob"><img src="../Images/406398a85ae4e4a8a4fae80db132a611.png" data-original-src="https://miro.medium.com/v2/resize:fit:888/format:webp/1*UkFBg85DJrbh6TItbWhqUA.png"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">图1:每个折叠的目标变量的分布形状。作者图片</p></figure><h1 id="a815" class="mo mp it bd mq mr ms mt mu mv mw mx my ki mz kj na kl nb km nc ko nd kp ne nf bi translated">3.创建数据集类</h1><p id="9c0e" class="pw-post-body-paragraph li lj it lk b ll ng kd ln lo nh kg lq lr ni lt lu lv nj lx ly lz nk mb mc md im bi translated">我们现在将创建<code class="fe nl nm nn no b">MyDataset</code>的子类<code class="fe nl nm nn no b">torch.utils.data.Dataset</code>。摘录将作为<code class="fe nl nm nn no b">texts</code>传入，同时传入的还有用于标记<code class="fe nl nm nn no b">texts</code>的<code class="fe nl nm nn no b">tokenizer</code>。在这个过程中，记号赋予器产生记号的id(称为输入id)以及输入到我们的模型中所必需的注意屏蔽。图2显示了一个输入文本为“<em class="mn">你好吗？</em>”。如果你感兴趣，可以在<a class="ae lh" href="https://huggingface.co/transformers/preprocessing.html" rel="noopener ugc nofollow" target="_blank">这里</a>找到关于记号赋予器、注意力屏蔽、填充和截断的更多细节。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="nz oa l"/></div></figure><h1 id="fd6f" class="mo mp it bd mq mr ms mt mu mv mw mx my ki mz kj na kl nb km nc ko nd kp ne nf bi translated">4.<code class="fe nl nm nn no b">roberta-base</code>作为我们的模型</h1><p id="941c" class="pw-post-body-paragraph li lj it lk b ll ng kd ln lo nh kg lq lr ni lt lu lv nj lx ly lz nk mb mc md im bi translated">RoBERTa代表<a class="ae lh" href="https://arxiv.org/abs/1907.11692" rel="noopener ugc nofollow" target="_blank"><strong class="lk jd">R</strong>obustubly<strong class="lk jd">O</strong>optimized<strong class="lk jd">BERT</strong>Pre-training<strong class="lk jd">A</strong>approach</a>，由华盛顿大学和脸书大学的研究人员于2019年提出。它是在2018年发布的<a class="ae lh" href="https://arxiv.org/abs/1810.04805" rel="noopener ugc nofollow" target="_blank"> BERT:面向语言理解的深度双向变换器预训练</a>的基础上改进的预训练程序。在整个演示过程中，我们将使用RoBERTa和PyTorch，但是如果您愿意，您也可以修改和使用其他的Transformer模型。请务必查看您所使用的变压器模型的相关文档，以确认它们支持代码所使用的输入和输出。</p><p id="a077" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">上有一些RoBERTa类的变体🤗拥抱脸。其中一个是<code class="fe nl nm nn no b">RobertaModel</code>，这里的<a class="ae lh" href="https://huggingface.co/transformers/master/model_doc/roberta.html#robertamodel" rel="noopener ugc nofollow" target="_blank">引用</a>为<em class="mn">“裸露的罗伯塔模型变压器输出原始的隐藏状态，顶部没有任何特定的头。”</em>换句话说，bare <code class="fe nl nm nn no b">RobertaModel</code>的原始输出是输入序列中每个token对应的预定义隐藏大小的隐藏状态向量。使用裸露的<code class="fe nl nm nn no b">RobertaModel</code>类，我们将添加我们自己的定制回归头来预测<code class="fe nl nm nn no b">target</code>。</p><p id="6d9c" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">对于我们的变压器微调任务，我们将使用来自🤗拥抱脸作为我们的模特。正如那里所描述的，“RoBERTa是一个以自我监督的方式在一个大的英语语料库上预先训练的变形金刚模型”。 <code class="fe nl nm nn no b">roberta-base</code>隐藏尺寸为768，由一个嵌入层后跟12个隐藏层组成。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/e69b356818d987ae3ea84793e9070e88.png" data-original-src="https://miro.medium.com/v2/resize:fit:1270/format:webp/1*c5tF04CEAYmHpPjzQ8H1IQ.png"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">图2:用<code class="fe nl nm nn no b">max_length=10</code>和<code class="fe nl nm nn no b">padding=“max_length”</code>设置tokenizer参数的例子。添加特殊标记后，任何短于10的输入序列都将被填充<code class="fe nl nm nn no b">&lt;pad&gt;</code>标记。作者图片</p></figure><h1 id="e811" class="mo mp it bd mq mr ms mt mu mv mw mx my ki mz kj na kl nb km nc ko nd kp ne nf bi translated">5.变压器的典型原始输出是什么？</h1><p id="5db6" class="pw-post-body-paragraph li lj it lk b ll ng kd ln lo nh kg lq lr ni lt lu lv nj lx ly lz nk mb mc md im bi translated">在我们开始创建和定义模型类之前，我们需要理解Transformer的原始输出是什么。这是因为我们将使用原始输出来填充自定义回归头。</p><p id="5af6" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">以下是BERT、ALBERT和RoBERTa等变压器模型通常返回的常见原始输出。它们取自文档<a class="ae lh" href="https://huggingface.co/transformers/master/model_doc/bert.html#bertmodel" rel="noopener ugc nofollow" target="_blank">这里</a>，这里<a class="ae lh" href="https://huggingface.co/transformers/master/model_doc/albert.html#albertmodel" rel="noopener ugc nofollow" target="_blank">这里</a>，这里<a class="ae lh" href="https://huggingface.co/transformers/master/model_doc/roberta.html#robertamodel" rel="noopener ugc nofollow" target="_blank">这里</a>。</p><ul class=""><li id="c71e" class="od oe it lk b ll lm lo lp lr of lv og lz oh md oi oj ok ol bi translated"><strong class="lk jd"> last_hidden_state </strong>:这是模型最后一层输出的隐藏状态序列。它是一个形状张量<code class="fe nl nm nn no b">(batch_size, sequence_length, hidden_size)</code></li><li id="4d70" class="od oe it lk b ll om lo on lr oo lv op lz oq md oi oj ok ol bi translated"><strong class="lk jd"> pooler_output </strong>:这是由线性层和Tanh激活函数进一步处理的序列的第一个令牌(分类令牌)的最后一层隐藏状态。它是一个形状为<code class="fe nl nm nn no b">(batch_size, hidden_size)</code>的张量。请注意，<code class="fe nl nm nn no b">pooler_output</code>可能不适用于某些型号的变压器。</li><li id="3afa" class="od oe it lk b ll om lo on lr oo lv op lz oq md oi oj ok ol bi translated"><strong class="lk jd">隐藏状态</strong>:可选，当<code class="fe nl nm nn no b">output_hidden_states = True</code>通过时返回。它是形状<code class="fe nl nm nn no b">(batch_size, sequence_length, hidden_size)</code>的张量元组(一个用于嵌入的输出+一个用于每层的输出)。</li></ul><p id="5845" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">那么，什么是<code class="fe nl nm nn no b">batch_size</code>、<code class="fe nl nm nn no b">sequence_length</code>、<code class="fe nl nm nn no b">hidden_size</code>？</p><p id="5218" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">通常，模型按批次处理记录。因此<code class="fe nl nm nn no b">batch_size</code>是模型在一次向前/向后传递中更新其内部参数之前处理的记录数。<code class="fe nl nm nn no b">sequence_length</code>是我们为标记器的<code class="fe nl nm nn no b">max_length</code>参数设置的值，而<code class="fe nl nm nn no b">hidden_size</code>是处于隐藏状态的特征(或元素)的数量。至于<em class="mn">张量</em>，你可以把它形象化为一个n维数组，可以用于任意数值计算。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi or"><img src="../Images/5da0f1688f5e1df35528ad31479061bc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1342/format:webp/1*WhJf7-cP-4LqOOn0nQwEFA.png"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">图3: <code class="fe nl nm nn no b">last_hidden_state</code>，即最后一层的模型输出。它是一个形状张量<code class="fe nl nm nn no b">(batch_size, sequence_length, hidden_size)</code>。作者图片</p></figure><h1 id="2a6b" class="mo mp it bd mq mr ms mt mu mv mw mx my ki mz kj na kl nb km nc ko nd kp ne nf bi translated">6.定义模型类</h1><p id="6208" class="pw-post-body-paragraph li lj it lk b ll ng kd ln lo nh kg lq lr ni lt lu lv nj lx ly lz nk mb mc md im bi translated">这里我们将创建<code class="fe nl nm nn no b">MyModel</code>和子类<code class="fe nl nm nn no b"><a class="ae lh" href="https://pytorch.org/docs/stable/generated/torch.nn.Module.html" rel="noopener ugc nofollow" target="_blank">nn.Module</a></code>。</p><p id="4023" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">An <code class="fe nl nm nn no b">nn.Module</code>是所有神经网络模块的基类，它包含多个层和一个方法<code class="fe nl nm nn no b">forward</code>，该方法接受输入并返回输出。除此之外，它还包含状态和参数，并可以通过它们进行权重更新或将其梯度归零。从<code class="fe nl nm nn no b">nn.Module</code>的<code class="fe nl nm nn no b">__call__</code>函数中调用<code class="fe nl nm nn no b">forward</code>方法。因此，当我们运行<code class="fe nl nm nn no b">MyModel(inputs)</code>时，就会调用<code class="fe nl nm nn no b">forward</code>方法。</p><h2 id="1e02" class="nt mp it bd mq os ot dn mu ou ov dp my lr ow ox na lv oy oz nc lz pa pb ne iz bi translated">使用<code class="fe nl nm nn no b">pooler_output</code></h2><p id="beff" class="pw-post-body-paragraph li lj it lk b ll ng kd ln lo nh kg lq lr ni lt lu lv nj lx ly lz nk mb mc md im bi translated">对于任何回归或分类任务，最简单的实现是直接采用<code class="fe nl nm nn no b">pooler_output</code>并附加一个额外的回归器或分类器输出层。</p><p id="3d86" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">特别是在我们的例子中，我们可以在<code class="fe nl nm nn no b">__init__</code>方法中定义一个带有一个<code class="fe nl nm nn no b"><a class="ae lh" href="https://pytorch.org/docs/stable/generated/torch.nn.Linear.html" rel="noopener ugc nofollow" target="_blank">nn.Linear</a></code>层的<code class="fe nl nm nn no b">regressor</code>作为我们网络的一部分。然后在<code class="fe nl nm nn no b">forward</code>方法中，我们将把<code class="fe nl nm nn no b">pooler_output</code>输入到<code class="fe nl nm nn no b">regressor</code>中，为<code class="fe nl nm nn no b">target</code>产生预测值。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="nz oa l"/></div></figure><h2 id="1439" class="nt mp it bd mq os ot dn mu ou ov dp my lr ow ox na lv oy oz nc lz pa pb ne iz bi translated">构建您自己的变压器定制头</h2><p id="53fd" class="pw-post-body-paragraph li lj it lk b ll ng kd ln lo nh kg lq lr ni lt lu lv nj lx ly lz nk mb mc md im bi translated">除了简单地使用<code class="fe nl nm nn no b">pooler_output</code>之外，还有许多不同的方法可以定义和组成你自己的层和自定义头。我们将演示的一个例子是<em class="mn">注意力头</em>，它是从<a class="ae lh" href="https://www.kaggle.com/maunish/clrp-pytorch-roberta-finetune-fixed-minor-issues?scriptVersionId=68364625&amp;cellId=8" rel="noopener ugc nofollow" target="_blank">这里的</a>改编而来的。</p><h2 id="1fbb" class="nt mp it bd mq os ot dn mu ou ov dp my lr ow ox na lv oy oz nc lz pa pb ne iz bi translated">🅰️ <strong class="ak">注意头像</strong></h2><p id="b49b" class="pw-post-body-paragraph li lj it lk b ll ng kd ln lo nh kg lq lr ni lt lu lv nj lx ly lz nk mb mc md im bi translated">在<code class="fe nl nm nn no b">forward</code>方法中，来自<code class="fe nl nm nn no b">last_hidden_state</code>的原始输出被输入到另一个类<code class="fe nl nm nn no b">AttentionHead</code>的实例中(我们将在下一段中讨论<code class="fe nl nm nn no b">AttentionHead</code>)。来自<code class="fe nl nm nn no b">AttentionHead</code>的输出然后被传递到我们之前看到的<code class="fe nl nm nn no b">regressor</code>中。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="nz oa l"/></div></figure><p id="cd0f" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">那么，<code class="fe nl nm nn no b">AttentionHead</code>里面是什么？<code class="fe nl nm nn no b">AttentionHead</code>中有两个线性层。<code class="fe nl nm nn no b">AttentionHead</code>将<code class="fe nl nm nn no b">last_hidden_state</code>带入第一个线性图层，在进入第二个线性图层之前，经过一个<a class="ae lh" href="https://medium.com/@cmukesh8688/activation-functions-sigmoid-tanh-relu-leaky-relu-softmax-50d3778dcea5" rel="noopener">tanh</a>T11】(双曲线正切)激活函数。这就导出了注意力得分。然后将softmax <em class="mn"> </em>函数应用于这些注意力分数，重新缩放它们，以使张量的元素位于范围[0，1]内，总和为1(嗯，试着将其视为概率分布)。然后将这些权重与<code class="fe nl nm nn no b">last_hidden_state</code>相乘，并对跨越<code class="fe nl nm nn no b">sequence_length</code> <strong class="lk jd"> </strong>维度的张量求和，最终产生形状<code class="fe nl nm nn no b">(batch_size, hidden_size)</code> <strong class="lk jd">的结果。</strong></p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="nz oa l"/></div></figure><h2 id="9f4a" class="nt mp it bd mq os ot dn mu ou ov dp my lr ow ox na lv oy oz nc lz pa pb ne iz bi translated">🅱️ <strong class="ak">连接隐藏层</strong></h2><p id="949d" class="pw-post-body-paragraph li lj it lk b ll ng kd ln lo nh kg lq lr ni lt lu lv nj lx ly lz nk mb mc md im bi translated">我们想分享的另一个技术是隐藏层的连接。这个想法来自于<a class="ae lh" href="https://arxiv.org/abs/1810.04805" rel="noopener ugc nofollow" target="_blank"> <em class="mn"> BERT:用于语言理解的深度双向转换器的预训练</em> </a>，其中作者提到，使用基于特征的方法，连接最后四个隐藏层在他们的案例研究中给出了最佳性能。</p><blockquote class="pc pd pe"><p id="e141" class="li lj mn lk b ll lm kd ln lo lp kg lq pf ls lt lu pg lw lx ly ph ma mb mc md im bi translated">“最佳执行方法是连接预训练转换器的前四个隐藏层的标记表示”</p></blockquote><p id="6d83" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">您可以在下面的代码中观察到，在调用我们的模型时，我们需要如何指定<code class="fe nl nm nn no b">output_hidden_states = True</code>。这是因为我们现在想要接收和使用来自其他隐藏层的输出，而不仅仅是<code class="fe nl nm nn no b">last_hidden_state</code>。</p><p id="f40b" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">在<code class="fe nl nm nn no b">forward</code>方法中，来自<code class="fe nl nm nn no b">hidden_states</code>的原始输出被叠加，给我们一个<code class="fe nl nm nn no b">(layers, batch_size, sequence_length, hidden_size)</code>的张量形状。由于<code class="fe nl nm nn no b">roberta-base</code>总共有13层，这简单地转化为<code class="fe nl nm nn no b">(13, batch_size, sequence_length, 768)</code>的张量形状。接下来，我们在<code class="fe nl nm nn no b">hidden_size</code>维度上连接最后四层，这给我们留下了一个张量形状<code class="fe nl nm nn no b">(batch_size, sequence_length, 768*4)</code>。连接之后，我们使用序列中第一个标记的表示。我们现在有了一个张量形状<code class="fe nl nm nn no b">(batch_size, 768*4)</code>，这最终被输入到<code class="fe nl nm nn no b">regressor</code>。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="nz oa l"/></div></figure><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi pi"><img src="../Images/8c32ffc0efa077a05b96234f1d6dc5c2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iCI7TCYnrnQEoe1gzI1low.png"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">图4:4个隐藏层的连接。作者图片</p></figure><p id="3cc5" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">如果你有兴趣继续阅读更多的例子，请看一下<a class="ae lh" href="https://www.kaggle.com/rhtsingh/utilizing-transformer-representations-efficiently/notebook" rel="noopener ugc nofollow" target="_blank">这个</a>笔记本。</p><h1 id="6eaa" class="mo mp it bd mq mr ms mt mu mv mw mx my ki mz kj na kl nb km nc ko nd kp ne nf bi translated">7.模特培训</h1><p id="b380" class="pw-post-body-paragraph li lj it lk b ll ng kd ln lo nh kg lq lr ni lt lu lv nj lx ly lz nk mb mc md im bi translated">好了，让我们继续为一个基本的模型训练过程编写训练代码。</p><p id="1df0" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">因为在这篇文章中我们不会涉及训练变形金刚的高级技术，我们将只创建简单的函数。现在，我们需要创建一个损失函数，一个训练函数，一个验证函数，最后是运行训练的主函数。</p><blockquote class="pc pd pe"><p id="3dce" class="li lj mn lk b ll lm kd ln lo lp kg lq pf ls lt lu pg lw lx ly ph ma mb mc md im bi translated">由于我们使用的是预训练模型(而不是从头开始训练)，这里的模型训练也通常被称为变压器微调过程。</p></blockquote><h2 id="fb94" class="nt mp it bd mq os ot dn mu ou ov dp my lr ow ox na lv oy oz nc lz pa pb ne iz bi translated">▶️评价度量和损失函数📉</h2><p id="cc28" class="pw-post-body-paragraph li lj it lk b ll ng kd ln lo nh kg lq lr ni lt lu lv nj lx ly lz nk mb mc md im bi translated">为了衡量我们模型的性能，我们将使用<a class="ae lh" rel="noopener" target="_blank" href="/what-does-rmse-really-mean-806b65f2e48e"> RMSE </a>(均方根误差)作为评估指标。</p><p id="e540" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">等等，损失函数是什么？它是用来做什么的？损失函数是用来测量预测输出和提供的<code class="fe nl nm nn no b">target</code>值之间的误差，以优化我们的模型。事实上，这是优化器试图最小化的函数。</p><p id="3173" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">有时评估度量和损失函数可以不同，特别是对于分类任务。但是在我们的例子中，由于这是一个回归任务，我们将使用RMSE来完成这两个任务。</p><p id="c0bc" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">因此，我们将损失函数定义如下:</p><pre class="ks kt ku kv gt np no nq nr aw ns bi"><span id="6272" class="nt mp it no b gy nu nv l nw nx">def loss_fn(predictions, targets):       <br/>    return torch.sqrt(nn.MSELoss()(predictions, targets))</span></pre><h2 id="9d11" class="nt mp it bd mq os ot dn mu ou ov dp my lr ow ox na lv oy oz nc lz pa pb ne iz bi translated">▶️训练函数</h2><p id="ce33" class="pw-post-body-paragraph li lj it lk b ll ng kd ln lo nh kg lq lr ni lt lu lv nj lx ly lz nk mb mc md im bi translated">我们正在创建的<code class="fe nl nm nn no b">train_fn</code>将使用训练数据集来训练我们的模型。在我们运行训练时的主训练循环中，每个历元都会调用这个函数。</p><p id="5587" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">该功能将首先在训练模式下设置模型。本质上，它将在数据加载器中循环所有批次的训练数据，获得批次的预测，反向传播误差，基于当前梯度更新参数，并基于调度器更新学习率。</p><p id="8197" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">需要注意的一点是，在开始反向传播之前，我们需要将梯度设置为零。这是因为PyTorch会在后续反向过程中累积梯度。</p><p id="c7f8" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">最后，该函数将返回它在批次中收集的培训损失和学习率。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="nz oa l"/></div></figure><h2 id="7df2" class="nt mp it bd mq os ot dn mu ou ov dp my lr ow ox na lv oy oz nc lz pa pb ne iz bi translated">▶️验证函数</h2><p id="cd84" class="pw-post-body-paragraph li lj it lk b ll ng kd ln lo nh kg lq lr ni lt lu lv nj lx ly lz nk mb mc md im bi translated"><code class="fe nl nm nn no b">validate_fn</code>用于对我们的验证数据集进行评估。它基本上会评估我们的模型在每个时期的训练过程中表现如何。它与我们上面写的<code class="fe nl nm nn no b">train_fn</code>非常相似，除了梯度计算被禁用。因此没有误差的反向传播，也没有参数和学习率的更新。</p><p id="4a83" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">该功能首先将模型设置为评估模式。它将在数据加载器中循环所有批次的认证数据，对认证数据(即训练期间未看到的数据)的批次运行预测，并收集将在最后返回的认证损失。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="nz oa l"/></div></figure><blockquote class="pc pd pe"><p id="8615" class="li lj mn lk b ll lm kd ln lo lp kg lq pf ls lt lu pg lw lx ly ph ma mb mc md im bi translated"><strong class="lk jd">笔记摘自PyTorch文档</strong> <a class="ae lh" href="https://pytorch.org/docs/stable/notes/autograd.html" rel="noopener ugc nofollow" target="_blank"> <strong class="lk jd">此处</strong> </a> <strong class="lk jd">和</strong> <a class="ae lh" href="https://pytorch.org/docs/stable/generated/torch.no_grad.html" rel="noopener ugc nofollow" target="_blank"> <strong class="lk jd">此处</strong> </a> <strong class="lk jd"> : </strong></p><p id="b12a" class="li lj mn lk b ll lm kd ln lo lp kg lq pf ls lt lu pg lw lx ly ph ma mb mc md im bi translated">建议我们在训练时总是使用<code class="fe nl nm nn no b">model.train()</code>，在评估我们的模型(验证/测试)时总是使用<code class="fe nl nm nn no b">model.eval()</code>，因为我们正在使用的模块可能会被更新以在训练和评估模式下表现不同。</p><p id="4335" class="li lj mn lk b ll lm kd ln lo lp kg lq pf ls lt lu pg lw lx ly ph ma mb mc md im bi translated">当我们确定不会调用<code class="fe nl nm nn no b"><a class="ae lh" href="https://pytorch.org/docs/stable/generated/torch.Tensor.backward.html#torch.Tensor.backward" rel="noopener ugc nofollow" target="_blank">.backward()</a></code>进行反向传播时，禁用梯度计算对推断(或验证/测试)很有用。这将减少原本需要梯度计算的计算的内存消耗。</p></blockquote><h2 id="77df" class="nt mp it bd mq os ot dn mu ou ov dp my lr ow ox na lv oy oz nc lz pa pb ne iz bi translated">▶️跑步训练</h2><p id="ddba" class="pw-post-body-paragraph li lj it lk b ll ng kd ln lo nh kg lq lr ni lt lu lv nj lx ly lz nk mb mc md im bi translated">现在我们已经创建了<code class="fe nl nm nn no b">train_fn</code>和<code class="fe nl nm nn no b">validate_fn</code>，让我们继续创建运行培训的主函数。</p><p id="0550" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">该功能的顶部将为模型训练做必要的准备。对于每个折叠，它将初始化标记化器，获取并创建训练和验证数据集和数据加载器，加载模型并将其发送到设备，并获取优化器和学习率调度器。</p><p id="a625" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">一旦这些都完成了，它就准备好进入训练循环了。训练循环将调用<code class="fe nl nm nn no b">train_fn</code>进行训练，然后再调用<code class="fe nl nm nn no b">validate_fn</code>对每个时期进行模型评估。一般来说，训练损失和验证损失应该随着时间逐渐减少。每当验证损失有所改善时(记住，它越低越好)，模型检查点就会被保存。否则，循环将继续，直到最后一个时期，或者当达到早期停止阈值时。基本上，当经过<em class="mn"> n </em>次迭代后确认损失仍无改善时，触发提前停止，其中<em class="mn"> n </em>为预设阈值。</p><p id="f928" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">该函数还将绘制训练和验证损失，以及每个折叠结束时的学习率时间表。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="nz oa l"/></div></figure><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pj"><img src="../Images/1890b56a0facf2885b61de136b787153.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*04N7rm25QEaV8fHP0S5IEw.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">图5:每个折叠的训练和验证损失的样本图。作者图片</p></figure><h1 id="7680" class="mo mp it bd mq mr ms mt mu mv mw mx my ki mz kj na kl nb km nc ko nd kp ne nf bi translated">摘要</h1><p id="75cb" class="pw-post-body-paragraph li lj it lk b ll ng kd ln lo nh kg lq lr ni lt lu lv nj lx ly lz nk mb mc md im bi translated">最后，我们即将结束这篇冗长的文章。总结一下:</p><p id="79fc" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">☑️:我们学习了如何使用scikit-learn的<code class="fe nl nm nn no b">StratifiedKFold</code>执行分层k-fold，将数据分成训练集和验证集。特别是在我们的案例中，我们使用了垃圾箱。</p><p id="6dec" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">☑️:我们从变压器中得到典型的原始输出的要点。</p><p id="2895" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">☑️:我们创建并定义了数据集和模型类。</p><p id="efd4" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">☑️我们探索了一些自定义回归头的例子，我们可以建立我们的模型。</p><p id="e4f3" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">☑️:我们经历了模型训练过程的基础，并为它创建了必要的功能。</p><p id="c9fd" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">这还不是全部。请关注我的下一篇文章，关于如何应用高级培训技术来微调变压器模型。</p><div class="pk pl gp gr pm pn"><a rel="noopener follow" target="_blank" href="/advanced-techniques-for-fine-tuning-transformers-82e4e61e16e"><div class="po ab fo"><div class="pp ab pq cl cj pr"><h2 class="bd jd gy z fp ps fr fs pt fu fw jc bi translated">微调变压器的先进技术</h2><div class="pu l"><h3 class="bd b gy z fp ps fr fs pt fu fw dk translated">学习这些先进的技术，看看它们如何帮助改善结果</h3></div><div class="pv l"><p class="bd b dl z fp ps fr fs pt fu fw dk translated">towardsdatascience.com</p></div></div><div class="pw l"><div class="px l py pz qa pw qb lb pn"/></div></div></a></div><p id="6034" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><em class="mn">如果你喜欢我的帖子，别忘了点击</em> <a class="ae lh" href="https://peggy1502.medium.com/" rel="noopener"> <strong class="lk jd"> <em class="mn">关注</em> </strong> </a> <em class="mn">和</em> <a class="ae lh" href="https://peggy1502.medium.com/subscribe" rel="noopener"> <strong class="lk jd"> <em class="mn">订阅</em> </strong> </a> <em class="mn">以获得邮件通知。</em></p><p id="4b46" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><em class="mn">可选地，你也可以</em> <a class="ae lh" href="https://peggy1502.medium.com/membership" rel="noopener"> <em class="mn">注册</em> </a> <em class="mn">成为媒体会员，以获得媒体上每个故事的全部访问权限。</em></p><p id="3a7e" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">📑<em class="mn">访问这个</em><a class="ae lh" href="https://github.com/peggy1502/Data-Science-Articles/blob/main/README.md" rel="noopener ugc nofollow" target="_blank"><em class="mn">GitHub repo</em></a><em class="mn">获取我在帖子中分享的所有代码和笔记本。</em></p><p id="ba73" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">2021保留所有权利。</p></div><div class="ab cl qc qd hx qe" role="separator"><span class="qf bw bk qg qh qi"/><span class="qf bw bk qg qh qi"/><span class="qf bw bk qg qh"/></div><div class="im in io ip iq"><h1 id="ffa8" class="mo mp it bd mq mr qj mt mu mv qk mx my ki ql kj na kl qm km nc ko qn kp ne nf bi translated">参考</h1><p id="06d5" class="pw-post-body-paragraph li lj it lk b ll ng kd ln lo nh kg lq lr ni lt lu lv nj lx ly lz nk mb mc md im bi translated">[1] A. Thakur，<a class="ae lh" href="https://www.amazon.com/dp/8269211508" rel="noopener ugc nofollow" target="_blank">(几乎)接近任何机器学习问题</a> (2020)</p><p id="b80c" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">[2] C .孙，x .邱，y .徐，x .黄，<a class="ae lh" href="https://arxiv.org/abs/1905.05583" rel="noopener ugc nofollow" target="_blank">如何微调用于文本分类的BERT？</a> (2020)</p><p id="b44f" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">[3] Y. Liu，M. Ott，N. Goyal，J. Du，M. Joshi，D. Chen，O. Levy，M. Lewis，L. Zettlemoyer和V. Stoyanov，<a class="ae lh" href="https://arxiv.org/abs/1907.11692" rel="noopener ugc nofollow" target="_blank"> RoBERTa:稳健优化的BERT预训练方法</a> (2019)</p><p id="7abc" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">[4] J. Devlin，M. Chang，K. Lee和K. Toutanova，<a class="ae lh" href="https://arxiv.org/abs/1810.04805" rel="noopener ugc nofollow" target="_blank"> BERT:用于语言理解的深度双向转换器的预训练</a> (2018)</p><p id="ac54" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">[5] A .瓦斯瓦尼、n .沙泽尔、n .帕尔马、j .乌兹科雷特、l .琼斯、A .戈麦斯、l .凯泽和I .波洛苏欣，<a class="ae lh" href="https://arxiv.org/abs/1706.03762" rel="noopener ugc nofollow" target="_blank">注意力是你所需要的全部</a> (2017)</p></div></div>    
</body>
</html>