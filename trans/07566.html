<html>
<head>
<title>Guide to fine-tuning Text Generation models: GPT-2, GPT-Neo and T5</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">文本生成模型微调指南:GPT新协议、GPT新协议和T5</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/guide-to-fine-tuning-text-generation-models-gpt-2-gpt-neo-and-t5-dc5de6b3bc5e?source=collection_archive---------1-----------------------#2021-07-11">https://towardsdatascience.com/guide-to-fine-tuning-text-generation-models-gpt-2-gpt-neo-and-t5-dc5de6b3bc5e?source=collection_archive---------1-----------------------#2021-07-11</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="5c3a" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">通过大规模语言模型的基础，我们了解了不同的开源模型，然后通过针对情感检测任务对每个模型进行微调来比较它们。</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/61b2839e06eeabbd2a71774d3b9fcab2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*af8-2P2j-xNUDesv"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">照片由<a class="ae kv" href="https://unsplash.com/@simplicity?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Marija Zaric </a>在<a class="ae kv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><blockquote class="kw kx ky"><p id="b5d4" class="kz la lb lc b ld le jr lf lg lh ju li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated"><em class="iq">本文使用的代码可以在这里找到——</em><a class="ae kv" href="http://mohitmayank.com/a_lazy_data_science_guide/natural_language_processing/GPTs/#finetuning-gpt-2-for-sentiment-classification" rel="noopener ugc nofollow" target="_blank"><em class="iq"/></a><em class="iq">和</em> <a class="ae kv" href="http://mohitmayank.com/a_lazy_data_science_guide/natural_language_processing/T5/#t5-finetuning" rel="noopener ugc nofollow" target="_blank"> <em class="iq"> T5 </em> </a> <em class="iq">。要阅读更多关于文本生成模型的内容，请参见</em><a class="ae kv" href="http://mohitmayank.com/a_lazy_data_science_guide/natural_language_processing/text_generation/" rel="noopener ugc nofollow" target="_blank"><em class="iq"/></a><em class="iq">。更多此类文章请访问我的网站</em><a class="ae kv" href="https://mohitmayank.com/blog/" rel="noopener ugc nofollow" target="_blank"><em class="iq"/></a><em class="iq">或者看看我的</em> <a class="ae kv" href="http://mohitmayank.com/a_lazy_data_science_guide/" rel="noopener ugc nofollow" target="_blank"> <em class="iq">最新数据科学简书</em> </a> <em class="iq">。也可以在</em><a class="ae kv" href="https://www.linkedin.com/in/imohitmayank/" rel="noopener ugc nofollow" target="_blank"><em class="iq">LinkedIn</em></a><em class="iq">上和我联系。</em></p></blockquote><h1 id="354f" class="lw lx iq bd ly lz ma mb mc md me mf mg jw mh jx mi jz mj ka mk kc ml kd mm mn bi translated">介绍</h1><p id="fef1" class="pw-post-body-paragraph kz la iq lc b ld mo jr lf lg mp ju li mq mr ll lm ms mt lp lq mu mv lt lu lv ij bi translated">最近在自然语言处理方面的研究导致了多个大规模预训练文本生成模型的发布，如GPT-{1，2，3}，GPT-{Neo，J}和T5。如果观众(包括你和我)对他们的可调参数大小达到数十亿没有印象，我们会被他们可以轻松地用于一个全新的看不见的任务而着迷，而无需任何训练！虽然这对于快速实验来说是可以的，但是对于任何实际的生产部署来说，仍然建议为特定的任务进一步训练模型。这被称为微调，在本文中，我们将实际学习微调一些目前可用的最好的语言模型的方法。我们还将通过在Twitter情感检测数据集上进行微调来比较它们的性能。我们开始吧！</p><h1 id="eb31" class="lw lx iq bd ly lz ma mb mc md me mf mg jw mh jx mi jz mj ka mk kc ml kd mm mn bi translated">文本生成模型</h1><p id="f43e" class="pw-post-body-paragraph kz la iq lc b ld mo jr lf lg mp ju li mq mr ll lm ms mt lp lq mu mv lt lu lv ij bi translated">在NLP中，文本生成是一项有趣的任务，其目的是在输入提示时生成文本。通常，我们应用某种形式的序列到序列模型来完成这项任务。它们被称为语言模型，因为它们可以用来根据前面的句子预测下一个单词。最近对该领域的兴趣激增是由于两个主要原因，(1)几个高性能预训练模型的可用性，以及(2)很容易将大量基于NLP的任务转换为文本输入文本输出类型的问题。T5作者非常直观地展示了这一点，他们可以使用相同的模型进行语言翻译、文本回归、摘要等。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mw"><img src="../Images/8b4d6ff5cf496ccdf61e82636091b962.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/1*Xu_h_UrmJgzvXZc4p7HuIg.gif"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">T5文本到文本框架示例。来源:<a class="ae kv" href="https://ai.googleblog.com/2020/02/exploring-transfer-learning-with-t5.html" rel="noopener ugc nofollow" target="_blank">谷歌AI博客</a></p></figure><p id="af02" class="pw-post-body-paragraph kz la iq lc b ld le jr lf lg lh ju li mq lk ll lm ms lo lp lq mu ls lt lu lv ij bi translated">在本文中，我们将关注以下模型，</p><ul class=""><li id="6452" class="mx my iq lc b ld le lg lh mq mz ms na mu nb lv nc nd ne nf bi translated"><strong class="lc ir"> GPT-2: </strong>是<a class="ae kv" href="https://en.wikipedia.org/wiki/OpenAI" rel="noopener ugc nofollow" target="_blank"> OpenAI </a>发布的原系列语言模型的第二次迭代。事实上，这一系列的GPT模型使语言模型出名了！GPT代表“生成式预训练变压器”，目前我们有3个版本的模型(v1，v2和v3)。其中只有GPT 1号和GPT 2号是开源的，因此我们将选择最新的版本进行实验。在技术方面，GPT-2的体系结构由变压器体系结构的解码器部分组成。</li><li id="a1f8" class="mx my iq lc b ld ng lg nh mq ni ms nj mu nk lv nc nd ne nf bi translated"><strong class="lc ir">GPT-尼奥:</strong>该模型由<a class="ae kv" href="https://www.eleuther.ai/" rel="noopener ugc nofollow" target="_blank"> EleutherAI </a>发布，以对抗未开源的GPT-3模型。该架构与GPT-3非常相似，但训练是在825 GB大小的文本数据集<a class="ae kv" href="https://pile.eleuther.ai/" rel="noopener ugc nofollow" target="_blank">上进行的。</a></li><li id="e8c3" class="mx my iq lc b ld ng lg nh mq ni ms nj mu nk lv nc nd ne nf bi translated"><strong class="lc ir"> T5: </strong>代表“文本到文本转换转换器”，是谷歌对开源语言模型的回应。T5论文展示了使用完整的编码器-解码器架构(变压器的)比仅使用解码器(如GPT系列所做的)更好，因此他们保持了原始变压器架构的真实性。</li></ul><p id="6750" class="pw-post-body-paragraph kz la iq lc b ld le jr lf lg lh ju li mq lk ll lm ms lo lp lq mu ls lt lu lv ij bi translated">不同型号的简要比较如下所示。需要注意的一点是，每个模型根据可调参数大小进一步发布了几个版本。对于这篇文章，我们将选择117米大小的GPT-2，125米大小的GPT-Neo和220米大小的T5。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nl"><img src="../Images/989933956a93f731fcd85fffe902e313.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xsoeVKraNUQpZn6zA570HQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">比较不同的文本生成模型。来源:[ <a class="ae kv" href="http://mohitmayank.com/a_lazy_data_science_guide/natural_language_processing/text_generation.html#comparing-models-basic-details" rel="noopener ugc nofollow" target="_blank">一个懒惰的数据科学指南</a></p></figure><h1 id="a8ca" class="lw lx iq bd ly lz ma mb mc md me mf mg jw mh jx mi jz mj ka mk kc ml kd mm mn bi translated">情感检测任务和数据集</h1><p id="21b8" class="pw-post-body-paragraph kz la iq lc b ld mo jr lf lg mp ju li mq mr ll lm ms mt lp lq mu mv lt lu lv ij bi translated">为了测试不同语言模型的性能，我们将在一个简单的任务——情感检测上进行微调后，比较模型的准确性。这里，我们将使用Twitter情感数据集，可以从<a class="ae kv" href="https://www.kaggle.com/kazanova/sentiment140" rel="noopener ugc nofollow" target="_blank">这里</a>下载。它总共包含超过160万条推文，他们的情绪可能是积极的，也可能是消极的。为了提高计算效率，我们将对10k条tweetss进行采样，这些tweet的情感类别分布几乎相等。然后，我们将使用95%的数据训练模型，将剩余的5%用于测试目的。为了公平比较，我们将对所有三个模型使用相同的测试和训练分割。最后，我们将执行3次分割和训练每个模型的试验——这是一种复制3重验证测试的方法。我们将报告单独的和合计的(平均)f1宏观分数，该分数可用于模型的性能比较。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nm"><img src="../Images/2bcb1078ef184edd303b6ef9360e3e45.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MY8GgCfam6kPw3kQLcY6TQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">Twitter情感数据集示例。按作者。</p></figure><p id="4157" class="pw-post-body-paragraph kz la iq lc b ld le jr lf lg lh ju li mq lk ll lm ms lo lp lq mu ls lt lu lv ij bi translated">现在，下一个明显的问题应该是，我们如何将情感检测任务转换为文本生成任务？答案很简单，我们所要做的就是创建一个直观的提示(带有数据的模板),它可以反映类似的表示如何出现在web上。让我们这样理解吧，我们希望提供tweet作为输入，希望将情绪作为输出。因此，在我们的提示中，我们应该在<code class="fe nn no np nq b">Tweet: </code>前缀后传递一条tweet，并期望模型预测<code class="fe nn no np nq b">Sentiment: </code>前缀后下一行的情绪。这种创建有效提示的过程被称为提示工程，事实证明，仅仅通过改变提示，语言模型就能表现得更好！对于我们的用例，我们可以从一个非常简单的提示格式开始。我们将有两个不同的提示，一个用于培训，一个用于测试。示例如下所示。</p><p id="d4c3" class="pw-post-body-paragraph kz la iq lc b ld le jr lf lg lh ju li mq lk ll lm ms lo lp lq mu ls lt lu lv ij bi translated"><strong class="lc ir">训练提示(就像我们希望模型学习这种“模式”来解决“任务”)</strong></p><pre class="kg kh ki kj gt nr nq ns nt aw nu bi"><span id="d49e" class="nv lx iq nq b gy nw nx l ny nz">Tweet: I am not feeling well.<br/>Sentiment: Negative</span></pre><p id="4501" class="pw-post-body-paragraph kz la iq lc b ld le jr lf lg lh ju li mq lk ll lm ms lo lp lq mu ls lt lu lv ij bi translated"><strong class="lc ir">测试提示(现在我们希望模型已经学习了“任务”，因此可以完成“模式”)</strong></p><pre class="kg kh ki kj gt nr nq ns nt aw nu bi"><span id="f74d" class="nv lx iq nq b gy nw nx l ny nz">Tweet: I am feeling well.<br/>Sentiment: </span></pre><p id="aebe" class="pw-post-body-paragraph kz la iq lc b ld le jr lf lg lh ju li mq lk ll lm ms lo lp lq mu ls lt lu lv ij bi translated">因此，在测试过程中，我们将提取前缀<code class="fe nn no np nq b">Sentiment:</code>后由模型预测的单词，并将该单词视为预测的情感标签。现在让我们深入到实现中！</p><h1 id="c215" class="lw lx iq bd ly lz ma mb mc md me mf mg jw mh jx mi jz mj ka mk kc ml kd mm mn bi translated">微调GPT 2号和GPT近地天体</h1><p id="0558" class="pw-post-body-paragraph kz la iq lc b ld mo jr lf lg mp ju li mq mr ll lm ms mt lp lq mu mv lt lu lv ij bi translated">需要注意的一点是——GPT-2和GPT-近地天体共享几乎相同的架构，因此大部分微调代码保持不变。因此，为了简洁起见，我将只分享GPT-2的代码，但我会指出使其适用于GPT-近地天体模型所需的更改。好了，让我们从处理数据集开始，为此我们将从创建Pytorch <code class="fe nn no np nq b">Dataset</code>类开始，它定义了我们如何为训练准备数据。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oa"><img src="../Images/7714df84dacc3fe6e34d1cbfbb49e1ed.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-5J_c5pwhn0pZynxbLVUDw.png"/></div></div></figure><p id="9180" class="pw-post-body-paragraph kz la iq lc b ld le jr lf lg lh ju li mq lk ll lm ms lo lp lq mu ls lt lu lv ij bi translated">这包括3个模块:</p><ul class=""><li id="4816" class="mx my iq lc b ld le lg lh mq mz ms na mu nb lv nc nd ne nf bi translated"><code class="fe nn no np nq b">__init__</code>:我们基本上标记和存储数据的地方。</li><li id="a8e2" class="mx my iq lc b ld ng lg nh mq ni ms nj mu nk lv nc nd ne nf bi translated"><code class="fe nn no np nq b">__len__ </code>:这里我们返回总数据集的长度。这是每个时期内步长计算所需要的。</li><li id="a90c" class="mx my iq lc b ld ng lg nh mq ni ms nj mu nk lv nc nd ne nf bi translated"><code class="fe nn no np nq b">__getitem__ </code>:获取一个数据，然后返回。</li></ul><p id="e264" class="pw-post-body-paragraph kz la iq lc b ld le jr lf lg lh ju li mq lk ll lm ms lo lp lq mu ls lt lu lv ij bi translated">一些附加点— (1)在第8行，我们定义了用于将原始数字情感标签转换为文本标签的映射，(2)在第12行，我们将数据转换为我们决定的训练提示，以及(3)在第14行，我们执行了标记化(将tweet拆分为标记+用它们唯一的id替换它们)。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ob"><img src="../Images/75c592477379868b78ad89a21646980c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*c39MotDnSmFGNY0qcfObBw.png"/></div></div></figure><p id="8e0a" class="pw-post-body-paragraph kz la iq lc b ld le jr lf lg lh ju li mq lk ll lm ms lo lp lq mu ls lt lu lv ij bi translated">接下来，我们用<code class="fe nn no np nq b">Dataset</code>类连接数据。代码分解如下所示，</p><ul class=""><li id="907a" class="mx my iq lc b ld le lg lh mq mz ms na mu nb lv nc nd ne nf bi translated"><code class="fe nn no np nq b">Line 4-8:</code>我们从加载数据集开始。你可以从<a class="ae kv" href="https://www.kaggle.com/kazanova/sentiment140" rel="noopener ugc nofollow" target="_blank">这里</a>下载，并在第4行修改本地路径。接下来，我们只需对相关的列进行子集划分，并对它们进行重命名。在第8行，我们为这个实验采样了10k条tweets。</li><li id="eb13" class="mx my iq lc b ld ng lg nh mq ni ms nj mu nk lv nc nd ne nf bi translated"><code class="fe nn no np nq b">Line 10–13:</code>我们将数据拆分为训练和测试，分别有95%和5%的拆分。我们使用<code class="fe nn no np nq b">stratify</code>标志，使得分裂在情感类别分布中是均匀的。</li><li id="ee3a" class="mx my iq lc b ld ng lg nh mq ni ms nj mu nk lv nc nd ne nf bi translated"><code class="fe nn no np nq b">Line 16:</code>我们将列车数据传递给<code class="fe nn no np nq b">SentimentDataset</code>类。注意，我们可以对测试数据做同样的事情，但是我只是以原始形式返回测试数据。</li></ul><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oc"><img src="../Images/89747df0f7c18fe54e65ac4703a14257.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*F4Pxzovv-dLHpl_nTj0exQ.png"/></div></div></figure><p id="0264" class="pw-post-body-paragraph kz la iq lc b ld le jr lf lg lh ju li mq lk ll lm ms lo lp lq mu ls lt lu lv ij bi translated">现在我们将为模型的训练做准备。代码分解如下:</p><ul class=""><li id="8b3d" class="mx my iq lc b ld le lg lh mq mz ms na mu nb lv nc nd ne nf bi translated">我们加载标记器，添加一些特殊的标记来表示tweets的不同部分，最后加载模型。注意，<code class="fe nn no np nq b">model_name </code>是在第5行定义的。还要注意的是，我们添加了特殊的标记，以便模型知道提示的开始和结束。这将有助于稍后的测试阶段，因为我们不希望模型继续写下一个单词，但它应该知道何时停止这个过程。这可以通过设置<code class="fe nn no np nq b">eos_token </code>并训练模型在标签后预测它来完成，如这里所做的。</li><li id="b01d" class="mx my iq lc b ld ng lg nh mq ni ms nj mu nk lv nc nd ne nf bi translated"><code class="fe nn no np nq b">Line 16:</code>使用我们之前定义的函数加载并准备数据集。</li><li id="748c" class="mx my iq lc b ld ng lg nh mq ni ms nj mu nk lv nc nd ne nf bi translated"><code class="fe nn no np nq b">Line 21–24:</code>我们为培训流程设置配置。简而言之，我们用<code class="fe nn no np nq b">batch_size</code>、<code class="fe nn no np nq b">warmup_steps </code>和<code class="fe nn no np nq b">weight_decay</code>定义了在哪里和什么时候保存模型、训练多长时间和在哪里保存日志以及训练策略。</li><li id="5077" class="mx my iq lc b ld ng lg nh mq ni ms nj mu nk lv nc nd ne nf bi translated"><code class="fe nn no np nq b">Line 27–31: </code>我们通过将模型与训练数据集相连接来开始训练。我们还定义了如何在<code class="fe nn no np nq b">data_collator</code>中处理训练数据。排序器中的前两个元素是<code class="fe nn no np nq b">input_ids </code> —标记化的提示和<code class="fe nn no np nq b">attention_mask </code> —一个简单的1/0向量，表示标记化向量的哪一部分是提示，哪一部分是填充。最后一部分非常有趣，我们将输入数据作为标签传递，而不仅仅是情感标签。这是因为我们正在训练一个语言模型，因此我们希望模型学习提示的模式，而不仅仅是情感类。在某种意义上，模型学习预测输入tweet +提示中结构化的情感的单词，并在这个过程中学习情感检测任务。</li></ul><p id="1dca" class="pw-post-body-paragraph kz la iq lc b ld le jr lf lg lh ju li mq lk ll lm ms lo lp lq mu ls lt lu lv ij bi translated">这将开始训练。根据您的计算机规格，这可能需要一些时间。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi od"><img src="../Images/357ec0b22c8396bdc445ab62190724cd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZpC07m-SpDM242j3aZXXcw.png"/></div></div></figure><p id="d0e8" class="pw-post-body-paragraph kz la iq lc b ld le jr lf lg lh ju li mq lk ll lm ms lo lp lq mu ls lt lu lv ij bi translated">最后，我们定义测试块，在这里我们获取训练好的模型，并将其应用于保留的测试数据。代码细分如下:</p><ul class=""><li id="82cf" class="mx my iq lc b ld le lg lh mq mz ms na mu nb lv nc nd ne nf bi translated"><code class="fe nn no np nq b">Line 5:</code>我们打开模型上的评估模式。</li><li id="ebc2" class="mx my iq lc b ld ng lg nh mq ni ms nj mu nk lv nc nd ne nf bi translated"><code class="fe nn no np nq b">Line 8–15:</code>对于每个测试数据，我们首先准备提示，但有一个很大的不同，我们不包括情绪标签，因为这是我们希望模型预测的。此外，记住<code class="fe nn no np nq b">eos_token</code>——我们希望模型能够预测情感标签，然后通过打印<code class="fe nn no np nq b">eos_token</code>来中断操作。最后，我们对测试提示进行标记。</li><li id="1e53" class="mx my iq lc b ld ng lg nh mq ni ms nj mu nk lv nc nd ne nf bi translated"><code class="fe nn no np nq b">Line 17:</code>我们采用测试提示，预测下一组单词。这个函数中有很多参数定义了如何预测下一个单词。关于它们中每一个的详细内容，请参考<a class="ae kv" href="https://huggingface.co/transformers/main_classes/model.html" rel="noopener ugc nofollow" target="_blank">这个</a>，或者为了更好地理解下一个单词预测的不同策略，请参考<a class="ae kv" href="https://huggingface.co/blog/how-to-generate" rel="noopener ugc nofollow" target="_blank">这个</a>。</li><li id="cc0f" class="mx my iq lc b ld ng lg nh mq ni ms nj mu nk lv nc nd ne nf bi translated"><code class="fe nn no np nq b">Line 20–30:</code>我们从解码预测文本开始，即将预测标记id重新转换为文本。然后，我们提取预测的情感标签并将所有相关信息存储到列表中。</li><li id="4374" class="mx my iq lc b ld ng lg nh mq ni ms nj mu nk lv nc nd ne nf bi translated"><code class="fe nn no np nq b">Line 33–37:</code>我们首先将所有提取的信息合并到一个pandas数据帧中以获得更好的可读性，然后使用sklearn包中的<code class="fe nn no np nq b">f1_score </code>函数来计算整个模型的性能。</li></ul><p id="b788" class="pw-post-body-paragraph kz la iq lc b ld le jr lf lg lh ju li mq lk ll lm ms lo lp lq mu ls lt lu lv ij bi translated">在运行GPT-2的代码并在数据集分割代码中使用不同的<code class="fe nn no np nq b">random_state</code>执行该操作三次时，我们观察到该模型实际上能够如预期的那样完美预测。它能够预测标签，然后使用<code class="fe nn no np nq b">eos_token</code>中断标签的执行。f1宏观表现平均分<strong class="lc ir"> 81.7% </strong>！这与我们预期的专用情感检测模型的表现相当，并且这继续强调了在NLP中使用文本生成模型进行迁移学习是多么容易。</p><p id="c4e3" class="pw-post-body-paragraph kz la iq lc b ld le jr lf lg lh ju li mq lk ll lm ms lo lp lq mu ls lt lu lv ij bi translated"><strong class="lc ir">符合GPT-尼奥标准的代码</strong></p><p id="ae4c" class="pw-post-body-paragraph kz la iq lc b ld le jr lf lg lh ju li mq lk ll lm ms lo lp lq mu ls lt lu lv ij bi translated">为了使GPT-2号代码适用于GPT-近地天体，我们必须做以下修改，</p><ul class=""><li id="73e4" class="mx my iq lc b ld le lg lh mq mz ms na mu nb lv nc nd ne nf bi translated">进口<code class="fe nn no np nq b">GPTNeoForCausalLM</code></li><li id="2c6d" class="mx my iq lc b ld ng lg nh mq ni ms nj mu nk lv nc nd ne nf bi translated">将<code class="fe nn no np nq b">model_name </code>设置为<code class="fe nn no np nq b">"EleutherAI/gpt-neo-2.7B"</code> <em class="lb">(从任何可用尺寸型号中选择)</em></li><li id="456d" class="mx my iq lc b ld ng lg nh mq ni ms nj mu nk lv nc nd ne nf bi translated">加载模型时，使用<code class="fe nn no np nq b">GPTNeoForCausalLM </code>代替<code class="fe nn no np nq b">GPT2LMHeadModel</code>。</li></ul><p id="3024" class="pw-post-body-paragraph kz la iq lc b ld le jr lf lg lh ju li mq lk ll lm ms lo lp lq mu ls lt lu lv ij bi translated">就是这样！在运行GPT-尼奥的修改代码，并遵循相同的训练策略时，平均f1宏观性能得分为<strong class="lc ir"> 80.7% </strong>！</p><h1 id="8749" class="lw lx iq bd ly lz ma mb mc md me mf mg jw mh jx mi jz mj ka mk kc ml kd mm mn bi translated">微调T5</h1><p id="da71" class="pw-post-body-paragraph kz la iq lc b ld mo jr lf lg mp ju li mq mr ll lm ms mt lp lq mu mv lt lu lv ij bi translated">T5的架构不同于GPT模型，因为它保持了原始变压器的架构，而GPT模型只保留了解码器部分。对于培训T5，我们将使用一个名为<a class="ae kv" href="https://github.com/Shivanandroy/simpleT5" rel="noopener ugc nofollow" target="_blank"> SimpleT5 </a>的优秀包装器包，它从培训阶段移除了大部分样板文件。现在请记住，虽然培训的语法会发生变化，但总体流程和直觉保持不变。先说数据部分。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oe"><img src="../Images/3ceba2c068bb2c774d411f7900ea9da8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TKx3iqK-yK_YaNYjXU84jA.png"/></div></div></figure><p id="258c" class="pw-post-body-paragraph kz la iq lc b ld le jr lf lg lh ju li mq lk ll lm ms lo lp lq mu ls lt lu lv ij bi translated">这里，大部分代码与我们之前为GPT模型所做的一样。一个主要的变化是我们不需要<code class="fe nn no np nq b">Dataset</code>类，因为SimpleT5直接在pandas数据帧上工作。因此，我们加载数据，做一些初步的预处理，分割数据，并返回熊猫数据帧。<em class="lb">(无需令牌化，创建</em> <code class="fe nn no np nq b"><em class="lb">Dataset</em></code> <em class="lb">类，这不是很棒吗！？)</em></p><p id="3c7f" class="pw-post-body-paragraph kz la iq lc b ld le jr lf lg lh ju li mq lk ll lm ms lo lp lq mu ls lt lu lv ij bi translated">还有一点需要注意的是，我们不需要为这个包创建提示格式。这样，我们可以将输入的tweet和情感标签分离到不同的列中，这里分别是<code class="fe nn no np nq b">source_text </code>和<code class="fe nn no np nq b">target_text</code>。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi of"><img src="../Images/dce40e669e290f268f31bfd7564ba7d5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1198/format:webp/1*On0r6r_ahMOzLG5-0hbmZw.png"/></div></figure><p id="6894" class="pw-post-body-paragraph kz la iq lc b ld le jr lf lg lh ju li mq lk ll lm ms lo lp lq mu ls lt lu lv ij bi translated">加载和训练模型也非常简单，只需3行代码<em class="lb">(如果你可以忽略我漂亮的换行)。</em></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi og"><img src="../Images/33eefe5aafdd2025e7dd34e21b01561e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*13A9NFpwLvDX50GV1Rnp8A.png"/></div></div></figure><p id="e3a4" class="pw-post-body-paragraph kz la iq lc b ld le jr lf lg lh ju li mq lk ll lm ms lo lp lq mu ls lt lu lv ij bi translated">接下来，我们在测试数据集上测试微调后的T5模型。如你所见，推理部分也非常简单，在第11行，我们使用了<code class="fe nn no np nq b">predict </code>函数，只需传递<code class="fe nn no np nq b">source_text </code>就可以得到预测的情感标签。我们稍后将它与<code class="fe nn no np nq b">original_label </code>进行比较，以在第18行生成性能分数。</p><p id="94ad" class="pw-post-body-paragraph kz la iq lc b ld le jr lf lg lh ju li mq lk ll lm ms lo lp lq mu ls lt lu lv ij bi translated">在运行T5代码并遵循与之前相同的训练策略时，f1宏观表现的平均得分为<strong class="lc ir"> 80.7% </strong>！</p><h1 id="c90d" class="lw lx iq bd ly lz ma mb mc md me mf mg jw mh jx mi jz mj ka mk kc ml kd mm mn bi translated">结果</h1><p id="4be4" class="pw-post-body-paragraph kz la iq lc b ld mo jr lf lg mp ju li mq mr ll lm ms mt lp lq mu mv lt lu lv ij bi translated">将所有结果合并到一个表中，我们得到，</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/ebb917c29669f7183fd43ca61a8544e1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1040/format:webp/1*vtd-E6hGfYknrtPgI-URiA.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">GPT-2、GPT-尼奥和T5在情感探测任务上的比较。</p></figure><p id="fdeb" class="pw-post-body-paragraph kz la iq lc b ld le jr lf lg lh ju li mq lk ll lm ms lo lp lq mu ls lt lu lv ij bi translated">我想讨论的一点是，我根本没有玩过超参数。除此之外，prompt工程方法学，我认为仅仅通过研究这两个方面，我们就可以进一步提高所有模型的性能指标。我会把它留给读者做练习<em class="lb">(如果你得到了更好的分数，一定要让我知道！)</em></p><h1 id="dd8a" class="lw lx iq bd ly lz ma mb mc md me mf mg jw mh jx mi jz mj ka mk kc ml kd mm mn bi translated">结论</h1><p id="a485" class="pw-post-body-paragraph kz la iq lc b ld mo jr lf lg mp ju li mq mr ll lm ms mt lp lq mu mv lt lu lv ij bi translated">虽然GPT-2可能赢得了这一轮，但结果表确实显示了文本生成模型的整体实力。他们所有人在情感检测任务上都表现得非常好，所需要的只是几个时期的训练。即使这个实验是为一个单一的任务而做的，我希望这有助于展示使用TG模型来完成全新的任务是多么容易。在某种程度上，如果我们可以将NLP问题转化为文本生成问题，请放心，预先训练的模型不会失败，至少不会彻底失败:)这使得它们成为许多任务的完美基线，如果不是最先进的。</p><h1 id="5ba6" class="lw lx iq bd ly lz ma mb mc md me mf mg jw mh jx mi jz mj ka mk kc ml kd mm mn bi translated">参考</h1><ul class=""><li id="a391" class="mx my iq lc b ld mo lg mp mq oi ms oj mu ok lv nc nd ne nf bi translated">科林·拉弗尔，诺姆·沙泽尔，Adam Roberts，凯瑟琳·李和莎兰·纳朗。用统一的文本到文本转换器探索迁移学习的局限性。2020.<a class="ae kv" href="https://arxiv.org/abs/1910.10683" rel="noopener ugc nofollow" target="_blank"> arXiv:1910.10683 </a></li><li id="89ca" class="mx my iq lc b ld ng lg nh mq ni ms nj mu nk lv nc nd ne nf bi translated">亚历克·拉德福德、杰弗里·吴、雷文·柴尔德、大卫·栾、达里奥·阿莫代伊、伊利亚·苏茨基弗等。语言模型是无人监督的多任务学习者。<em class="lb"> OpenAI博客</em>，1(8):9，2019。</li><li id="5a78" class="mx my iq lc b ld ng lg nh mq ni ms nj mu nk lv nc nd ne nf bi translated"><a class="ae kv" href="https://github.com/EleutherAI/gpt-neo" rel="noopener ugc nofollow" target="_blank"> GPT近地天体</a></li><li id="3e81" class="mx my iq lc b ld ng lg nh mq ni ms nj mu nk lv nc nd ne nf bi translated">Ashish Vaswani、Noam Shazeer、Niki Parmar、Jakob Uszkoreit、Llion Jones、Aidan N. Gomez、Lukasz Kaiser和Illia Polosukhin。你需要的只是关注。2017.<a class="ae kv" href="https://arxiv.org/abs/1706.03762" rel="noopener ugc nofollow" target="_blank"> arXiv:1706.03762 </a>。</li></ul></div><div class="ab cl ol om hu on" role="separator"><span class="oo bw bk op oq or"/><span class="oo bw bk op oq or"/><span class="oo bw bk op oq"/></div><div class="ij ik il im in"><p id="37a0" class="pw-post-body-paragraph kz la iq lc b ld le jr lf lg lh ju li mq lk ll lm ms lo lp lq mu ls lt lu lv ij bi translated">干杯。</p></div></div>    
</body>
</html>