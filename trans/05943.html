<html>
<head>
<title>Essential Lingo for the Aspiring ML Practitioner</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">有志ML从业者的基本行话</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/essential-lingo-for-the-aspiring-ml-practitioner-f939176ad7f4?source=collection_archive---------33-----------------------#2021-05-27">https://towardsdatascience.com/essential-lingo-for-the-aspiring-ml-practitioner-f939176ad7f4?source=collection_archive---------33-----------------------#2021-05-27</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="cd78" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">从层类型到功能，一个快速列表可以帮助你在开始机器学习时保持方向。</h2></div><p id="1334" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">每个行业都有自己的行话，机器学习也不例外。这里有一个关键术语的简要清单，所有有抱负的ML开发人员在构建模型时都应该熟悉这些术语。</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi lb"><img src="../Images/0f8216769494fd3b450ca554cf6bfe3f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VKtI9X_KlD26vvsgVED4iQ.jpeg"/></div></div><p class="ln lo gj gh gi lp lq bd b be z dk translated"><a class="ae lr" href="https://unsplash.com/photos/P0NuBF6nA7A" rel="noopener ugc nofollow" target="_blank">图像来源</a></p></figure><p id="c9fb" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">图层和模型类型</strong></p><p id="a57c" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在ML中，深度学习(DL)是当前的艺术状态。今天所有的DL模型都是基于<a class="ae lr" href="https://blog.perceptilabs.com/four-common-types-of-neural-network-layers-and-when-to-use-them/" rel="noopener ugc nofollow" target="_blank">神经网络</a>。神经网络包括<em class="ls">神经元</em>(又名<em class="ls">张量</em> ) 的<strong class="kh ir">层，其存储某物的状态(例如，像素值)，并且连接到存储更高级实体(例如，像素组)的状态的后续层中的神经元。在这些层之间是<strong class="kh ir"> <em class="ls">权重</em> </strong>，这些权重在训练期间被调整，并最终用于进行预测。</strong></p><p id="f7c1" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">下面是<strong class="kh ir">要熟悉的神经网络层的常见类型</strong>:</p><ul class=""><li id="128c" class="lt lu iq kh b ki kj kl km ko lv ks lw kw lx la ly lz ma mb bi translated"><strong class="kh ir">输入层</strong>:代表输入数据(如图像数据)的神经网络的第一层。</li><li id="8f2c" class="lt lu iq kh b ki mc kl md ko me ks mf kw mg la ly lz ma mb bi translated"><strong class="kh ir">输出层</strong>:神经网络的最后一层，包含给定输入的模型结果(如<a class="ae lr" href="https://blog.perceptilabs.com/top-five-ways-that-machine-learning-is-being-used-for-image-processing-and-computer-vision/" rel="noopener ugc nofollow" target="_blank">图像分类</a>)。</li><li id="4aa8" class="lt lu iq kh b ki mc kl md ko me ks mf kw mg la ly lz ma mb bi translated"><strong class="kh ir">隐藏层:</strong>位于神经网络的<em class="ls">输入</em>和<em class="ls">输出</em>层<strong class="kh ir"> </strong>之间的层，对数据进行转换。</li><li id="d4c5" class="lt lu iq kh b ki mc kl md ko me ks mf kw mg la ly lz ma mb bi translated"><strong class="kh ir">密集</strong>(又名<em class="ls">全连接层</em>):其所有输出都连接到下一层所有输入的层。</li><li id="7040" class="lt lu iq kh b ki mc kl md ko me ks mf kw mg la ly lz ma mb bi translated"><strong class="kh ir">卷积</strong>:构建<a class="ae lr" href="https://machinelearningmastery.com/convolutional-layers-for-deep-learning-neural-networks/" rel="noopener ugc nofollow" target="_blank">卷积神经网络</a> (CNN)的基础。</li><li id="41c9" class="lt lu iq kh b ki mc kl md ko me ks mf kw mg la ly lz ma mb bi translated"><strong class="kh ir">递归</strong>:提供<em class="ls">循环</em>功能，其中层的输入包括要分析的数据和对该层执行的先前计算的输出。递归层构成了<a class="ae lr" rel="noopener" target="_blank" href="/recurrent-neural-networks-d4642c9bc7ce">递归神经网络</a> (RNNs)的基础。</li><li id="2d81" class="lt lu iq kh b ki mc kl md ko me ks mf kw mg la ly lz ma mb bi translated"><strong class="kh ir">残差块</strong>:残差神经网络(ResNets)将两层或更多层组合在一起作为<em class="ls">残差块</em>，以避免<a class="ae lr" href="https://en.wikipedia.org/wiki/Vanishing_gradient_problem" rel="noopener ugc nofollow" target="_blank">消失梯度问题</a>。</li></ul><p id="0afe" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">ML从业者使用<strong class="kh ir">不同类型的层</strong>以不同的方式构建神经网络。常见的神经网络示例包括VGG16、ResNet50、InceptionV3和MobileNetV2。请注意，这些模型中有许多是由<a class="ae lr" href="https://docs.perceptilabs.com/perceptilabs/references/components/deep-learning" rel="noopener ugc nofollow" target="_blank"> Keras应用</a>提供的，并且可以通过PerceptiLabs中的组件获得，允许您使用<a class="ae lr" href="https://blog.perceptilabs.com/when-to-use-transfer-learning-in-image-processing/" rel="noopener ugc nofollow" target="_blank">迁移学习</a>快速构建模型。</p><p id="4ee6" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">基础知识</strong></p><p id="6d9b" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">模型的层和互连构成了模型的<strong class="kh ir"> <em class="ls">内部</em> </strong>或<strong class="kh ir"> <em class="ls">可学习的</em>参数</strong>，这些参数在训练过程中<strong class="kh ir">调整</strong>。关键参数包括:</p><ul class=""><li id="6dc2" class="lt lu iq kh b ki kj kl km ko lv ks lw kw lx la ly lz ma mb bi translated"><strong class="kh ir">权重</strong>:一个给定的神经元影响它在下一层所连接的张量的程度。</li><li id="b3f0" class="lt lu iq kh b ki mc kl md ko me ks mf kw mg la ly lz ma mb bi translated"><strong class="kh ir">偏差</strong>:向上或向下移动结果，使其更容易或更难激活神经元。</li></ul><p id="d7e8" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在<strong class="kh ir">训练</strong>期间，底层<strong class="kh ir"> ML引擎通过调整其权重和偏差来优化模型。</strong>为了做到这一点，采用了以下算法:</p><ul class=""><li id="4141" class="lt lu iq kh b ki kj kl km ko lv ks lw kw lx la ly lz ma mb bi translated"><strong class="kh ir">优化器</strong>:更新模型，帮助它学习复杂的数据模式。常见的算法有:<em class="ls"> ADAM </em>，<em class="ls">随机梯度下降(SGD) </em>，<em class="ls"> Adagrad </em>，<em class="ls"> RMSprop </em>。</li><li id="f642" class="lt lu iq kh b ki mc kl md ko me ks mf kw mg la ly lz ma mb bi translated"><strong class="kh ir">激活函数</strong>:给神经元输出增加非线性的数学函数。例子包括:<em class="ls">乙状结肠</em>、<em class="ls"> ReLU </em>和<em class="ls"> Tanh </em>。</li><li id="5f0f" class="lt lu iq kh b ki mc kl md ko me ks mf kw mg la ly lz ma mb bi translated"><strong class="kh ir">损失函数</strong>(又名<em class="ls">误差函数</em>):衡量优化器对单个训练样本的调整是否会产生更好的性能(即，指示给定一组权重和偏差值时模型的预测效果)。常见的损失函数有<em class="ls">交叉熵</em>、<em class="ls">二次</em>和<em class="ls">骰子</em>。</li><li id="752f" class="lt lu iq kh b ki mc kl md ko me ks mf kw mg la ly lz ma mb bi translated"><strong class="kh ir">成本函数</strong>:衡量优化器对所有训练样本的调整是否会产生更好的模型性能。</li></ul><p id="a03d" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">训练通常包括将数据分成三组:</p><ul class=""><li id="e48e" class="lt lu iq kh b ki kj kl km ko lv ks lw kw lx la ly lz ma mb bi translated"><strong class="kh ir">训练数据</strong>:用于通过提供<em class="ls">地面实况</em>数据(例如，具有已知正确的相应标签的图像)来优化模型的权重和偏差。</li><li id="4d22" class="lt lu iq kh b ki mc kl md ko me ks mf kw mg la ly lz ma mb bi translated"><strong class="kh ir">验证数据</strong>:用于测试模型在训练时的准确性。</li><li id="f85f" class="lt lu iq kh b ki mc kl md ko me ks mf kw mg la ly lz ma mb bi translated"><strong class="kh ir">测试数据</strong>:用于测试最终训练好的模型在以前没有见过的数据上的性能。</li></ul><p id="9eaa" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">重要的是要避免在训练期间<strong class="kh ir"> <em class="ls">欠拟合</em> </strong>(即训练一个太简单而不能很好地从数据中学习的模型)，以及<strong class="kh ir">过拟合</strong>(即训练一个太复杂而不能根据新的、看不见的数据准确预测的模型)。ML从业者可以调整许多用户可控的设置，称为<strong class="kh ir"> <em class="ls">超参数</em> </strong>，以开发出尽可能好的模型:</p><ul class=""><li id="00aa" class="lt lu iq kh b ki kj kl km ko lv ks lw kw lx la ly lz ma mb bi translated"><strong class="kh ir">批量大小</strong>:优化器更新模型参数之前要训练的训练样本数。</li><li id="5bc8" class="lt lu iq kh b ki mc kl md ko me ks mf kw mg la ly lz ma mb bi translated"><strong class="kh ir">时期:对整个训练数据集(包括所有批次)进行的次数。</strong></li><li id="d570" class="lt lu iq kh b ki mc kl md ko me ks mf kw mg la ly lz ma mb bi translated"><strong class="kh ir">损失:要使用的损失函数算法。</strong></li><li id="0bcf" class="lt lu iq kh b ki mc kl md ko me ks mf kw mg la ly lz ma mb bi translated"><strong class="kh ir">学习率</strong>:模型参数更新后，根据计算的误差，改变模型参数的多少。</li><li id="b516" class="lt lu iq kh b ki mc kl md ko me ks mf kw mg la ly lz ma mb bi translated"><strong class="kh ir">优化器</strong>:要使用的优化器算法。</li><li id="1ec9" class="lt lu iq kh b ki mc kl md ko me ks mf kw mg la ly lz ma mb bi translated"><strong class="kh ir"> Shuffle </strong>:随机改变训练数据顺序，使模型更健壮。</li><li id="689d" class="lt lu iq kh b ki mc kl md ko me ks mf kw mg la ly lz ma mb bi translated"><strong class="kh ir"> Dropout </strong>:通过忽略随机选择的层输出来减少过拟合的正则化技巧。然后，对层的每次更新都在该层的不同<em class="ls">视图</em>上有效地执行。</li></ul><p id="3c2e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这个清单当然不是详尽无遗的——我们可以很容易地写出一整套关于基础知识的书！然而，这些关键要素是一个良好的开端。</p><p id="8355" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">对于那些想在Perceptilabs中尝试这些基础知识的人，请务必查看PerceptiLabs <a class="ae lr" href="https://docs.perceptilabs.com/perceptilabs/" rel="noopener ugc nofollow" target="_blank">文档</a>和我们的<a class="ae lr" href="https://docs.perceptilabs.com/perceptilabs/getting-started/quickstart-guide" rel="noopener ugc nofollow" target="_blank">入门</a>指南。</p></div></div>    
</body>
</html>