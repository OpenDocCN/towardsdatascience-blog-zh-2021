<html>
<head>
<title>Hindsight Experience Replay (HER) Implementation</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">事后经验回放(HER)实现</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/hindsight-experience-replay-her-implementation-92eebab6f653?source=collection_archive---------16-----------------------#2021-10-29">https://towardsdatascience.com/hindsight-experience-replay-her-implementation-92eebab6f653?source=collection_archive---------16-----------------------#2021-10-29</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="817d" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsdatascience.com/tagged/tips-and-tricks" rel="noopener" target="_blank">提示和技巧</a></h2><div class=""/><div class=""><h2 id="8472" class="pw-subtitle-paragraph jw iz iq bd b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn dk translated">算法和代码的解释</h2></div><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ko"><img src="../Images/bc1c6bdad2faae55c34a94f4d94890ea.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*1K60Plrupz8jW-0K"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">由<a class="ae le" href="https://unsplash.com/@brett_jordan?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">布雷特·乔丹</a>在<a class="ae le" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="8b90" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">我最近为我的研究强化学习库实现了HER算法:<a class="ae le" href="https://github.com/LondonNode/Pearl" rel="noopener ugc nofollow" target="_blank"> <strong class="lh ja"> Pearl </strong> </a>。在这样做的时候，我发现虽然有文章讨论算法的理论，但是没有任何文章讨论如何用代码实现它。就这样吧，这就是我来的原因！在本文中，我将讨论如何用Python实现HER缓冲区😊</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="mb mc l"/></div></figure><h1 id="0081" class="md me iq bd mf mg mh mi mj mk ml mm mn kf mo kg mp ki mq kj mr kl ms km mt mu bi translated">快速算法概述</h1><p id="b613" class="pw-post-body-paragraph lf lg iq lh b li mv ka lk ll mw kd ln lo mx lq lr ls my lu lv lw mz ly lz ma ij bi translated">提醒一下，这是原始论文中的算法:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi na"><img src="../Images/d35fde65fc10a8fe95b06c56bed90d3f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sK3e97KH3Co3tH8x9Z4Teg.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">来源:<a class="ae le" href="https://arxiv.org/abs/1707.01495" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1707.01495</a></p></figure><p id="9e6b" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">总之，在二元且稀疏的奖励环境中(在该环境中，当代理获胜时实现唯一的非负奖励)，如果代理从未接收到任何奖励变化，则它可能很难适当地探索环境并学习实现期望目标所需的步骤序列。HER buffer试图通过复制每个经历的轨迹，并用假设目标是轨迹结束时实现的步骤计算的奖励代替实际奖励来克服这一点。这个想法是，这将帮助代理更好地探索，并学习中间目标，最终达到实际的期望目标。在本文中，我将新的采样目标称为<strong class="lh ja">她的目标</strong>，而结果轨迹将是<strong class="lh ja">她的轨迹</strong>，其中包含计算的<strong class="lh ja">她的奖励</strong>。</p><h1 id="d9a1" class="md me iq bd mf mg mh mi mj mk ml mm mn kf mo kg mp ki mq kj mr kl ms km mt mu bi translated">实现和代码</h1><p id="6958" class="pw-post-body-paragraph lf lg iq lh b li mv ka lk ll mw kd ln lo mx lq lr ls my lu lv lw mz ly lz ma ij bi translated">该实现将稍微偏离本文中的理论算法。主要区别如下:</p><ol class=""><li id="6d7e" class="nb nc iq lh b li lj ll lm lo nd ls ne lw nf ma ng nh ni nj bi translated">我们不会在添加每个轨迹后立即对目标进行采样并计算她的奖励，而是等到采样时，我们可以一次性进行矢量化计算，这在Python ⚡中要快得多</li><li id="b2e0" class="nb nc iq lh b li nk ll nl lo nm ls nn lw no ma ng nh ni nj bi translated">我们还将使用一个技巧，只使用一个缓冲区来存储观测值和下一个观测值，以将观测值存储的内存需求减少近一半！这可以通过假设存储的观察值是连续的来实现；也就是说，索引i+1处的观察值是索引I处的下一个观察值💾</li></ol><h2 id="fd27" class="np me iq bd mf nq nr dn mj ns nt dp mn lo nu nv mp ls nw nx mr lw ny nz mt iw bi translated">初始化</h2><p id="0223" class="pw-post-body-paragraph lf lg iq lh b li mv ka lk ll mw kd ln lo mx lq lr ls my lu lv lw mz ly lz ma ij bi translated">反直觉地，让我们从第2点开始，因为这是在下面的类初始化中定义的，注意没有定义<code class="fe oa ob oc od b">self.next_observations</code>。</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="oe mc l"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">她的init</p></figure><p id="b79d" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">这里需要注意的一点是，这个缓冲区只接受<code class="fe oa ob oc od b">GoalEnv</code>环境！！这让我们能够轻松跟踪剧集目标。请看下面来自OpenAI的原始描述:</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="oe mc l"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">目标环境描述</p></figure><p id="0daf" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">在轨迹的末端，下一个观察将是结束状态，并且期望的目标在整个情节中应该是恒定的。因此，我们只需要跟踪观察结果<code class="fe oa ob oc od b">desired_goal</code>和下一次观察结果<code class="fe oa ob oc od b">achieved_goal</code>，就可以计算她的奖励。同样值得注意的是<code class="fe oa ob oc od b">self.episode_end_indices</code>和<code class="fe oa ob oc od b">self.index_episode_map</code>缓冲器:</p><ul class=""><li id="a58a" class="nb nc iq lh b li lj ll lm lo nd ls ne lw nf ma of nh ni nj bi translated"><code class="fe oa ob oc od b">self.episode_end_indices</code>:跟踪轨迹缓冲器中的剧集结束索引。</li><li id="bda4" class="nb nc iq lh b li nk ll nl lo nm ls nn lw no ma of nh ni nj bi translated"><code class="fe oa ob oc od b">self.index_episode_map</code>:跟踪哪些过渡属于哪些剧集。</li></ul><p id="a847" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">最后，<code class="fe oa ob oc od b">her_ratio</code>变量指示用新的HER奖励与标准重放缓冲器轨迹进行采样的轨迹的分数。</p><h2 id="e3b1" class="np me iq bd mf nq nr dn mj ns nt dp mn lo nu nv mp ls nw nx mr lw ny nz mt iw bi translated">添加轨迹</h2><p id="0fc2" class="pw-post-body-paragraph lf lg iq lh b li mv ka lk ll mw kd ln lo mx lq lr ls my lu lv lw mz ly lz ma ij bi translated">如前所述，在添加轨迹时，我们不会对她的目标和回报进行采样，因此这种方法并不复杂。</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="oe mc l"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">将轨迹添加到缓冲区</p></figure><p id="88b6" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">注意添加<code class="fe oa ob oc od b">next_observation["observation"]</code>在算法上有点复杂，因为我们像<code class="fe oa ob oc od b">observation["observation"]</code>一样将它存储在<code class="fe oa ob oc od b">self.observations</code>中。</p><h2 id="72da" class="np me iq bd mf nq nr dn mj ns nt dp mn lo nu nv mp ls nw nx mr lw ny nz mt iw bi translated">抽样</h2><p id="b397" class="pw-post-body-paragraph lf lg iq lh b li mv ka lk ll mw kd ln lo mx lq lr ls my lu lv lw mz ly lz ma ij bi translated">这是真正的处理过程，我们批量计算她的目标和相应的奖励。让我们从定义我们想要从轨迹缓冲区返回哪些索引的高级方法开始:</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="oe mc l"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">她的高级抽样方法</p></figure><p id="43d9" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">这里要注意的关键是，我们想要从完整的剧集中采样索引，这就是为什么我们需要将<code class="fe oa ob oc od b">end_idx</code>定义为轨迹缓冲区中最后一个完整剧集的索引。</p><p id="e0bd" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">接下来，让我们看看返回轨迹的方法。这会以<code class="fe oa ob oc od b">self.her_ratio</code>定义的比率获得标准回放和她的轨迹:</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="oe mc l"/></div></figure><p id="97be" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">幸运的是，<code class="fe oa ob oc od b">GoalEnv</code>有一个方法<code class="fe oa ob oc od b">compute_reward(achieved_goal, desired_goal, info)</code>,这意味着我们不必担心自己计算她的回报。在这种方法中，我们本质上分离了我们需要计算她的回报和目标的指数，然后在最后重新组合一切以获得回报。所有观察值也与算法中指定的期望目标相联系。请再次注意<code class="fe oa ob oc od b">next_observations</code>的采样稍微复杂一些，因为我们从同一个<code class="fe oa ob oc od b">self.observations</code>缓冲区获得这些数据。</p><p id="8765" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">最后，还有一个问题是让她的目标本身来计算她的奖励。这定义如下:</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="oe mc l"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">尝试她的目标</p></figure><p id="d76a" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">还记得在算法回顾时，我说过她的目标被假定为轨迹终点观察吗？这只是部分正确，实际上有许多不同的方式来取样她的目标，我已经做了其中的两个以上。最简单的方法是获得最后的观察结果，但这篇论文显示，当你在同一集但在当前过渡之后，以随机状态对她的目标进行采样时，实际上获得了最佳结果。在这个策略中，有一个失败模式。如果采样的情节与缓冲区“重叠”；也就是说，当缓冲区溢出时，剧集在缓冲区的末尾开始，在开头结束，可以在缓冲区的末尾选取该剧集中的轨迹。因此，当调用<code class="fe oa ob oc od b">np.random.randint()</code>时，低值将是缓冲区结尾的索引，而高值将是剧集开头的结尾索引，从而导致错误。因为如果缓冲区大小相对较大(通常应该是1e6的量级)，这种情况很少发生，所以作为快速解决方案，我们可以求助于更简单的HER目标策略，即在这种情况发生时选取剧集结束索引。</p></div><div class="ab cl og oh hu oi" role="separator"><span class="oj bw bk ok ol om"/><span class="oj bw bk ok ol om"/><span class="oj bw bk ok ol"/></div><div class="ij ik il im in"><p id="c631" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">唷！坚持到了最后🎉🎉</p><p id="6285" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">如果您觉得这篇文章有用，请考虑:</p><ul class=""><li id="e86a" class="nb nc iq lh b li lj ll lm lo nd ls ne lw nf ma of nh ni nj bi translated">跟踪我🙌</li><li id="0082" class="nb nc iq lh b li nk ll nl lo nm ls nn lw no ma of nh ni nj bi translated"><a class="ae le" href="https://medium.com/subscribe/@rohan.tangri" rel="noopener"> <strong class="lh ja">订阅我的电子邮件通知</strong> </a>永不错过上传📧</li><li id="75a9" class="nb nc iq lh b li nk ll nl lo nm ls nn lw no ma of nh ni nj bi translated">使用我的媒介<a class="ae le" href="https://medium.com/@rohan.tangri/membership" rel="noopener"> <strong class="lh ja">推荐链接</strong> </a> <strong class="lh ja"> </strong>直接支持我并获得无限量的优质文章🤗</li></ul><p id="5d95" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">晋升的方式，让我知道你对这个话题的想法和快乐学习！！</p></div></div>    
</body>
</html>