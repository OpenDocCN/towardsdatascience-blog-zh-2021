<html>
<head>
<title>Deep Neural Networks and Gaussian Processes: Similarities, Differences, and Trade-Offs</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">深度神经网络和高斯过程:相似性、差异和权衡</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/deep-neural-networks-vs-gaussian-processes-similarities-differences-and-trade-offs-18647376d799?source=collection_archive---------3-----------------------#2021-09-17">https://towardsdatascience.com/deep-neural-networks-vs-gaussian-processes-similarities-differences-and-trade-offs-18647376d799?source=collection_archive---------3-----------------------#2021-09-17</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="0281" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsdatascience.com/tagged/thoughts-and-theory" rel="noopener" target="_blank">思想和理论</a></h2><div class=""/><figure class="gl gn ka kb kc kd gh gi paragraph-image"><div role="button" tabindex="0" class="ke kf di kg bf kh"><div class="gh gi jz"><img src="../Images/84ab9504e16a7c696b3a32710ca29709.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*RtSY4humlxhjrCA2"/></div></div><p class="kk kl gj gh gi km kn bd b be z dk translated"><a class="ae ko" href="https://unsplash.com/@urielsc26?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">乌列尔SC </a>在<a class="ae ko" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><h1 id="0552" class="kp kq it bd kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm bi translated">动机:比较最先进的</h1><p id="493a" class="pw-post-body-paragraph ln lo it lp b lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk im bi translated"><strong class="lp jd"> <em class="ml">深度神经网络</em></strong><em class="ml"/><strong class="lp jd"><em class="ml">(DNNs)</em></strong><em class="ml"/>和<strong class="lp jd"> <em class="ml">高斯过程(GPs)</em></strong><em class="ml">*</em><strong class="lp jd"><em class="ml"/></strong>是两类极具表现力的监督学习算法。当考虑这些方法的应用时，一个自然的问题出现了:<em class="ml">“什么时候以及为什么使用一种算法比使用另一种有意义？”</em></p><figure class="mm mn mo mp gt kd gh gi paragraph-image"><div role="button" tabindex="0" class="ke kf di kg bf kh"><div class="gh gi jz"><img src="../Images/8f9cb7718eff7c4a59690bc88895a233.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*X3a1zgPFQI_6ujaO"/></div></div><p class="kk kl gj gh gi km kn bd b be z dk translated">希望这篇文章能为你提供一些指导，告诉你什么时候一个模型比另一个模型更好。照片由<a class="ae ko" href="https://unsplash.com/@frausnippe?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Kristin Snippe </a>在<a class="ae ko" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><p id="e90c" class="pw-post-body-paragraph ln lo it lp b lq mq ls lt lu mr lw lx ly ms ma mb mc mt me mf mg mu mi mj mk im bi translated">在本文中，我们将致力于制定决定使用哪种方法的指南。然而，要开始开发这些指南，我们首先需要了解这些方法之间的关系。在本文中，我们将涵盖:</p><ol class=""><li id="3bf1" class="mv mw it lp b lq mq lu mr ly mx mc my mg mz mk na nb nc nd bi translated"><strong class="lp jd"> GPs </strong>和<strong class="lp jd"> DNNs </strong>的理论异同</li><li id="150a" class="mv mw it lp b lq ne lu nf ly ng mc nh mg ni mk na nb nc nd bi translated"><strong class="lp jd"> GPs </strong>与<strong class="lp jd"> DNNs </strong>的优缺点</li><li id="ec9f" class="mv mw it lp b lq ne lu nf ly ng mc nh mg ni mk na nb nc nd bi translated">使用<strong class="lp jd"> GPs </strong>与<strong class="lp jd"> DNNs </strong>的示例案例研究</li></ol><p id="211e" class="pw-post-body-paragraph ln lo it lp b lq mq ls lt lu mr lw lx ly ms ma mb mc mt me mf mg mu mi mj mk im bi translated">我们开始吧！</p><p id="6a5d" class="pw-post-body-paragraph ln lo it lp b lq mq ls lt lu mr lw lx ly ms ma mb mc mt me mf mg mu mi mj mk im bi translated">* *有关高斯过程(GPs)的详细介绍/入门知识，请参见</p><ol class=""><li id="04ae" class="mv mw it lp b lq mq lu mr ly mx mc my mg mz mk na nb nc nd bi translated"><a class="ae ko" rel="noopener" target="_blank" href="/gaussian-process-regression-from-first-principles-833f4aa5f842"> <strong class="lp jd">本条</strong> </a> <strong class="lp jd"> </strong>(理论)</li><li id="4bed" class="mv mw it lp b lq ne lu nf ly ng mc nh mg ni mk na nb nc nd bi translated"><a class="ae ko" rel="noopener" target="_blank" href="/modern-gaussian-process-regression-9c5196ca87ab"> <strong class="lp jd">本条</strong> </a> <strong class="lp jd"> </strong>(理论&amp;实现)</li><li id="3a76" class="mv mw it lp b lq ne lu nf ly ng mc nh mg ni mk na nb nc nd bi translated"><a class="ae ko" rel="noopener" target="_blank" href="/batched-multi-dimensional-gaussian-process-regression-with-gpytorch-3a6425185109"> <strong class="lp jd">本条</strong> </a> <strong class="lp jd"> </strong>(实现)。</li></ol><h1 id="3d9b" class="kp kq it bd kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm bi translated">(几个)理论差异</h1><h2 id="dd85" class="nj kq it bd kr nk nl dn kv nm nn dp kz ly no np ld mc nq nr lh mg ns nt ll iz bi translated">A.参数与非参数</h2><figure class="mm mn mo mp gt kd gh gi paragraph-image"><div role="button" tabindex="0" class="ke kf di kg bf kh"><div class="gh gi nu"><img src="../Images/fefcb27e5ca6286ae7cfa8185a3aab68.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*rtmsHxHhxjlWfrgg"/></div></div><p class="kk kl gj gh gi km kn bd b be z dk translated">从参数的角度来看，dnn通常比GPs有更多通过学习来调整的“旋钮”。黛安·皮凯蒂诺在<a class="ae ko" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="cb86" class="pw-post-body-paragraph ln lo it lp b lq mq ls lt lu mr lw lx ly ms ma mb mc mt me mf mg mu mi mj mk im bi translated">量化这两个模型之间差异的一个方向是考虑每个框架中参数的数量和类型。一般来说，由于高斯过程被认为是<em class="ml">非参数</em>机器学习技术，<strong class="lp jd">高斯过程(GPs) </strong>学习明显更少的参数，并且预测很大程度上由定义它们的训练数据集驱动。他们的参数选择完全由以下选项表示:</p><ol class=""><li id="1b83" class="mv mw it lp b lq mq lu mr ly mx mc my mg mz mk na nb nc nd bi translated">核/协方差函数(<strong class="lp jd"> k(x，x’)</strong>)</li><li id="8098" class="mv mw it lp b lq ne lu nf ly ng mc nh mg ni mk na nb nc nd bi translated">平均函数(<strong class="lp jd"> m(x) </strong>)</li><li id="2113" class="mv mw it lp b lq ne lu nf ly ng mc nh mg ni mk na nb nc nd bi translated">似然协方差噪声(<strong class="lp jd"> σ </strong>)。</li></ol><p id="6b48" class="pw-post-body-paragraph ln lo it lp b lq mq ls lt lu mr lw lx ly ms ma mb mc mt me mf mg mu mi mj mk im bi translated">这些选择/变量通常被称为“超参数”。</p><p id="9a6d" class="pw-post-body-paragraph ln lo it lp b lq mq ls lt lu mr lw lx ly ms ma mb mc mt me mf mg mu mi mj mk im bi translated">在<strong class="lp jd">高斯过程(GPs) </strong>中发现的参数缺乏与许多现代<strong class="lp jd">深度神经网络(DNNs) </strong>形成鲜明对比，后者旨在利用尽可能多的参数，即所谓的<strong class="lp jd">权重</strong>，来解决机器学习问题。在经典的统计学习文献中，使用大量的参数一直不被认可，因为这种想法会导致显著的过度拟合和对非分布数据的较差概括。然而，这种经典的统计学习理论未能解释过度参数化神经网络的经验成功，因此“<em class="ml">过度参数化</em>或“<em class="ml">插值</em>”机制的新理论开始占据上风[1，6]。</p><p id="fd01" class="pw-post-body-paragraph ln lo it lp b lq mq ls lt lu mr lw lx ly ms ma mb mc mt me mf mg mu mi mj mk im bi">___________________________________________________________________</p><p id="8b83" class="pw-post-body-paragraph ln lo it lp b lq mq ls lt lu mr lw lx ly ms ma mb mc mt me mf mg mu mi mj mk im bi translated"><strong class="lp jd"> TL/DR #1: GPs </strong>为(近*) <em class="ml">非参数</em>，而<strong class="lp jd"> DNNs </strong>为<em class="ml">过参数。</em></p><p id="25d6" class="pw-post-body-paragraph ln lo it lp b lq mq ls lt lu mr lw lx ly ms ma mb mc mt me mf mg mu mi mj mk im bi">___________________________________________________________________</p><p id="a040" class="pw-post-body-paragraph ln lo it lp b lq mq ls lt lu mr lw lx ly ms ma mb mc mt me mf mg mu mi mj mk im bi translated">*GPs仅通过其超参数进行参数化，例如上面找到的那些参数。</p><h2 id="c4b0" class="nj kq it bd kr nk nl dn kv nm nn dp kz ly no np ld mc nq nr lh mg ns nt ll iz bi translated"><strong class="ak"> B .直接与逆方法</strong></h2><p id="b2ec" class="pw-post-body-paragraph ln lo it lp b lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk im bi translated">与参数/非参数属性相关，<strong class="lp jd"> GPs </strong>和<strong class="lp jd"> DNNs </strong>的另一个不同之处在于<strong class="lp jd"> DNNs </strong>是<strong class="lp jd"> <em class="ml">逆方法</em> </strong>，<strong class="lp jd"> GPs </strong>是<strong class="lp jd"> <em class="ml">直接方法</em> </strong>。</p><p id="afd1" class="pw-post-body-paragraph ln lo it lp b lq mq ls lt lu mr lw lx ly ms ma mb mc mt me mf mg mu mi mj mk im bi translated"><strong class="lp jd"> <em class="ml">逆方法</em> </strong>涉及从训练数据中优化参数，因此也被称为监督学习方法。通常，这些方法涉及参数化，带有一些关于参数的初始信念(称为“先验”)。这些方法通常采用<em class="ml">自上而下</em>的方法，由此数据被用来更新参数中捕获的信念【7】。</p><p id="ad4e" class="pw-post-body-paragraph ln lo it lp b lq mq ls lt lu mr lw lx ly ms ma mb mc mt me mf mg mu mi mj mk im bi translated"><strong class="lp jd"> <em class="ml">直接方法</em> </strong>依赖于直接使用训练数据来进行新的预测/获得洞察力，正如大多数内核机器所完成的那样。特别是核机器有一个简单的现象，即缺少核函数本身的细节，新的预测完全由现有的可用数据驱动。由于这些方法能够进行实质性的数据探索和洞察，而不需要对底层模型产生强烈的初始信念，因此这些方法通常被称为<em class="ml">自底向上</em>【7】。</p><p id="3c30" class="pw-post-body-paragraph ln lo it lp b lq mq ls lt lu mr lw lx ly ms ma mb mc mt me mf mg mu mi mj mk im bi">___________________________________________________________________</p><p id="3fb9" class="pw-post-body-paragraph ln lo it lp b lq mq ls lt lu mr lw lx ly ms ma mb mc mt me mf mg mu mi mj mk im bi translated"><strong class="lp jd"> TL/DR #2: GPs </strong>是(近*) <em class="ml">正方法</em>，而<strong class="lp jd"> DNNs </strong>是<em class="ml">逆方法。</em></p><p id="bef6" class="pw-post-body-paragraph ln lo it lp b lq mq ls lt lu mr lw lx ly ms ma mb mc mt me mf mg mu mi mj mk im bi">___________________________________________________________________</p><p id="3090" class="pw-post-body-paragraph ln lo it lp b lq mq ls lt lu mr lw lx ly ms ma mb mc mt me mf mg mu mi mj mk im bi translated">GPs的超参数优化是一个间接的例程，通常使用基于梯度和Hessian的学习方法来完成。</p><h2 id="2b9b" class="nj kq it bd kr nk nl dn kv nm nn dp kz ly no np ld mc nq nr lh mg ns nt ll iz bi translated">C.学习差异</h2><figure class="mm mn mo mp gt kd gh gi paragraph-image"><div role="button" tabindex="0" class="ke kf di kg bf kh"><div class="gh gi nv"><img src="../Images/bc31ac229f6a02967f89bc085db8a72e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*oZxhNvYifVcE7Q4_"/></div></div><p class="kk kl gj gh gi km kn bd b be z dk translated">GPs和dnn的部分不同之处在于它们如何遍历用于优化性能的损耗表面。丹尼尔·科波尼亚斯在<a class="ae ko" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="d1d5" class="pw-post-body-paragraph ln lo it lp b lq mq ls lt lu mr lw lx ly ms ma mb mc mt me mf mg mu mi mj mk im bi translated">在没有深度高斯过程的情况下，在<strong class="lp jd"> GPs </strong>和<strong class="lp jd"> DNNs </strong>之间学习到的内容也不同。然而，完成学习的方法并没有太大的不同:两者都使用第一(在某些情况下，第二)阶方法。这两种方法也优化了不同的函数:对于神经网络，这是一个<strong class="lp jd">损失/风险函数</strong>，对于高斯过程，这是<strong class="lp jd">边际似然函数</strong>。</p><p id="4cae" class="pw-post-body-paragraph ln lo it lp b lq mq ls lt lu mr lw lx ly ms ma mb mc mt me mf mg mu mi mj mk im bi translated">对于高斯过程，边际似然目标往往更加非凸，因此，经常使用二阶梯度下降算法，如<a class="ae ko" href="https://en.wikipedia.org/wiki/Limited-memory_BFGS" rel="noopener ugc nofollow" target="_blank">L-BFGS</a>【5】，进行优化以避免局部极小值。</p><p id="49ed" class="pw-post-body-paragraph ln lo it lp b lq mq ls lt lu mr lw lx ly ms ma mb mc mt me mf mg mu mi mj mk im bi">___________________________________________________________________</p><p id="419b" class="pw-post-body-paragraph ln lo it lp b lq mq ls lt lu mr lw lx ly ms ma mb mc mt me mf mg mu mi mj mk im bi translated"><strong class="lp jd"> TL/DR #3: GPs </strong>一般用二阶方法优化，如L-BFGS【5】，使用目标函数的<em class="ml">海森</em>，而<strong class="lp jd"> DNNs </strong>一般用一阶方法优化，如SGD【8】，使用目标函数的<em class="ml">梯度</em>。</p><p id="1cff" class="pw-post-body-paragraph ln lo it lp b lq mq ls lt lu mr lw lx ly ms ma mb mc mt me mf mg mu mi mj mk im bi">___________________________________________________________________</p><h2 id="2bf4" class="nj kq it bd kr nk nl dn kv nm nn dp kz ly no np ld mc nq nr lh mg ns nt ll iz bi translated"><strong class="ak"> D .可解释性</strong></h2><p id="3d87" class="pw-post-body-paragraph ln lo it lp b lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk im bi translated">在应用机器学习任务中，能够解释你的结果可能与结果本身一样重要。</p><p id="de69" class="pw-post-body-paragraph ln lo it lp b lq mq ls lt lu mr lw lx ly ms ma mb mc mt me mf mg mu mi mj mk im bi translated"><strong class="lp jd">人工智能:神经网络的可解释性</strong></p><p id="8142" class="pw-post-body-paragraph ln lo it lp b lq mq ls lt lu mr lw lx ly ms ma mb mc mt me mf mg mu mi mj mk im bi translated">虽然一些较新的<strong class="lp jd"> DNN </strong>结构允许对不确定性进行更彻底的近似，例如通过任意的和认知的不确定性【9】，但是这些网络中的许多只是提供了估计的预测值，并且可能提供了多类分类的逻辑值(预测概率)。然而，由于<strong class="lp jd"> DNNs </strong>普遍缺乏可解释性已经成为一个热门的研究课题[10]，我相信未来更多的网络架构将会包含一些预测不确定性的元素。</p><p id="5332" class="pw-post-body-paragraph ln lo it lp b lq mq ls lt lu mr lw lx ly ms ma mb mc mt me mf mg mu mi mj mk im bi translated">其他进展，如GradCam [12]等梯度可视化工具，也提高了dnn的可解释性，并有助于减少它们的“黑色性”。</p><p id="96f5" class="pw-post-body-paragraph ln lo it lp b lq mq ls lt lu mr lw lx ly ms ma mb mc mt me mf mg mu mi mj mk im bi translated"><strong class="lp jd">D . II:GPs的可解释性</strong></p><p id="87b7" class="pw-post-body-paragraph ln lo it lp b lq mq ls lt lu mr lw lx ly ms ma mb mc mt me mf mg mu mi mj mk im bi translated">相反，<strong class="lp jd"> GPs </strong>固有的高斯结构使其非常适合可解释的不确定性估计。对于某些需要直观风险评估的应用，这可能使该方法更具优势。</p><p id="3ff5" class="pw-post-body-paragraph ln lo it lp b lq mq ls lt lu mr lw lx ly ms ma mb mc mt me mf mg mu mi mj mk im bi translated">此外，GPs具有良好的直观特性，即所有插值均值预测都是作为训练集中现有均值点的加权<em class="ml">线性</em>组合生成的，并按测试点到给定数据点的距离(在核函数空间中测量)进行缩放[11]。GPs以线性方式重新组合他们以前看到的点，以产生新的预测。</p><p id="8780" class="pw-post-body-paragraph ln lo it lp b lq mq ls lt lu mr lw lx ly ms ma mb mc mt me mf mg mu mi mj mk im bi">___________________________________________________________________</p><p id="76ca" class="pw-post-body-paragraph ln lo it lp b lq mq ls lt lu mr lw lx ly ms ma mb mc mt me mf mg mu mi mj mk im bi translated"><strong class="lp jd">TL/DR # 4:</strong><strong class="lp jd">GPs</strong>的<em class="ml">线性和高斯</em>特性很好地帮助他们增加这些模型的可解释性。尽管DNNs长期以来被批评为“黑箱”,但今天的重大研究工作正在进行中，以帮助使这些模型更容易解释。</p><p id="8a3c" class="pw-post-body-paragraph ln lo it lp b lq mq ls lt lu mr lw lx ly ms ma mb mc mt me mf mg mu mi mj mk im bi">___________________________________________________________________</p><h1 id="ee98" class="kp kq it bd kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm bi translated">(一些)理论上的相似之处</h1><h2 id="6f05" class="nj kq it bd kr nk nl dn kv nm nn dp kz ly no np ld mc nq nr lh mg ns nt ll iz bi translated">A.“插值机制”中的核心机器</h2><p id="f5b8" class="pw-post-body-paragraph ln lo it lp b lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk im bi translated">最近的研究表明，当具有线性激活函数的神经网络在其隐藏层中接近无限宽度时，它们渐近收敛于核机器[1，2]。这是一个<strong class="lp jd">神经切线核(NTK)</strong>【2】的想法。这种现象发生在所谓的“插值区域”，也称为“双下降曲线”的后半部分[1]。</p><p id="83fb" class="pw-post-body-paragraph ln lo it lp b lq mq ls lt lu mr lw lx ly ms ma mb mc mt me mf mg mu mi mj mk im bi translated">高斯过程也是核机器，因为确定测试点的预测均值和方差的训练点的线性组合是由高斯过程的核函数确定的。</p><p id="771e" class="pw-post-body-paragraph ln lo it lp b lq mq ls lt lu mr lw lx ly ms ma mb mc mt me mf mg mu mi mj mk im bi">___________________________________________________________________</p><p id="f710" class="pw-post-body-paragraph ln lo it lp b lq mq ls lt lu mr lw lx ly ms ma mb mc mt me mf mg mu mi mj mk im bi translated"><strong class="lp jd"> TL/DR #5: </strong>在一定条件下[1，2]，<strong class="lp jd"> DNNs </strong>可以作为所谓“插值”中的核机器进行分析，配备了一个核函数，从观察点的核加权组合中形成对观察点的预测。<strong class="lp jd"> GPs </strong>本质上是内核机器【11】。</p><p id="bce6" class="pw-post-body-paragraph ln lo it lp b lq mq ls lt lu mr lw lx ly ms ma mb mc mt me mf mg mu mi mj mk im bi">___________________________________________________________________</p><p id="1c50" class="pw-post-body-paragraph ln lo it lp b lq mq ls lt lu mr lw lx ly ms ma mb mc mt me mf mg mu mi mj mk im bi translated"><strong class="lp jd"> B .目标函数的优化</strong></p><p id="458c" class="pw-post-body-paragraph ln lo it lp b lq mq ls lt lu mr lw lx ly ms ma mb mc mt me mf mg mu mi mj mk im bi translated">虽然有各种二阶方法，如BFGS和BFGS，用于优化全球定位系统，一阶方法也可以用来优化这些模型。像DNNs一样，GPs仍然努力最小化泛函(通常是具有核正则项的负对数似然)，就像神经网络努力最小化损失函数一样。</p><p id="269f" class="pw-post-body-paragraph ln lo it lp b lq mq ls lt lu mr lw lx ly ms ma mb mc mt me mf mg mu mi mj mk im bi">___________________________________________________________________</p><p id="c57a" class="pw-post-body-paragraph ln lo it lp b lq mq ls lt lu mr lw lx ly ms ma mb mc mt me mf mg mu mi mj mk im bi translated"><strong class="lp jd"> TL/DR #6: </strong>两个<strong class="lp jd"> DNNs </strong>和<strong class="lp jd"> GPs </strong>都是通过一阶和二阶优化方法来改进他们的模型。</p><p id="a76b" class="pw-post-body-paragraph ln lo it lp b lq mq ls lt lu mr lw lx ly ms ma mb mc mt me mf mg mu mi mj mk im bi">___________________________________________________________________</p><h1 id="090a" class="kp kq it bd kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm bi translated">每种方法的优势</h1><p id="4934" class="pw-post-body-paragraph ln lo it lp b lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk im bi translated">此列表绝非详尽无遗，但在决定使用神经网络还是高斯过程时，这里仅列出了一些优势(相对于其他框架):</p><p id="87ec" class="pw-post-body-paragraph ln lo it lp b lq mq ls lt lu mr lw lx ly ms ma mb mc mt me mf mg mu mi mj mk im bi translated"><strong class="lp jd"> GP </strong> <em class="ml">优点</em> / <strong class="lp jd"> DNN </strong> <em class="ml">缺点</em>:</p><ol class=""><li id="2a01" class="mv mw it lp b lq mq lu mr ly mx mc my mg mz mk na nb nc nd bi translated">通常比<strong class="lp jd"> DNNs </strong>需要更少的数据，因为它们需要调整的参数更少。然而，拥有更多数据，特别是在固定域上密度不断增加的情况下(称为固定域渐近[1])，有助于显著提高性能。</li><li id="7d7a" class="mv mw it lp b lq ne lu nf ly ng mc nh mg ni mk na nb nc nd bi translated">只需要优化少量(超)参数。</li><li id="2c26" class="mv mw it lp b lq ne lu nf ly ng mc nh mg ni mk na nb nc nd bi translated">对爆炸和消失梯度等现象具有鲁棒性(因为，除非您使用深度<strong class="lp jd"> GPs </strong>，否则在此框架内没有“层结构”)。</li></ol><p id="ea70" class="pw-post-body-paragraph ln lo it lp b lq mq ls lt lu mr lw lx ly ms ma mb mc mt me mf mg mu mi mj mk im bi translated"><strong class="lp jd">GP</strong>T28】缺点/<strong class="lp jd">DNN</strong>T32】优点:</p><ol class=""><li id="e11f" class="mv mw it lp b lq mq lu mr ly mx mc my mg mz mk na nb nc nd bi translated">运行时间与样本数量的比例很小。运行时复杂度为<strong class="lp jd"> <em class="ml"> O(n ) </em> </strong>，其中<strong class="lp jd"> <em class="ml"> n </em> </strong>为样本数。这是必须执行大协方差矩阵的矩阵求逆(或伪求逆)的结果。</li><li id="2a45" class="mv mw it lp b lq ne lu nf ly ng mc nh mg ni mk na nb nc nd bi translated">相对于神经网络，自动学习更少，并且对于核/协方差函数、均值函数和超参数先验分布的选择，需要进行更多的设计考虑。这些参数会对<strong class="lp jd"> GP </strong>能够学习的内容产生实质性的影响。</li></ol><h1 id="d196" class="kp kq it bd kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm bi translated">每种技术的示例用例</h1><figure class="mm mn mo mp gt kd gh gi paragraph-image"><div role="button" tabindex="0" class="ke kf di kg bf kh"><div class="gh gi nv"><img src="../Images/aa1c83d287aac3d524b4785a9712d941.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*KH7mX3H1YiKbuCCI"/></div></div><p class="kk kl gj gh gi km kn bd b be z dk translated"><a class="ae ko" href="https://unsplash.com/@andrewtneel?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">安德鲁·尼尔</a>在<a class="ae ko" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍照</p></figure><p id="cef7" class="pw-post-body-paragraph ln lo it lp b lq mq ls lt lu mr lw lx ly ms ma mb mc mt me mf mg mu mi mj mk im bi translated">请注意，以下建议不是绝对的，也就是说，这些建议的目的更多的是为了应用我们在上面学到的原则。</p><p id="1f4a" class="pw-post-body-paragraph ln lo it lp b lq mq ls lt lu mr lw lx ly ms ma mb mc mt me mf mg mu mi mj mk im bi translated"><strong class="lp jd">注意:</strong>虽然我对这两个模型类都有丰富的实践和理论经验，但请不要将下面的这些建议视为绝对——在某些情况下，使用另一个模型类可能会更有利。</p><ol class=""><li id="cc3f" class="mv mw it lp b lq mq lu mr ly mx mc my mg mz mk na nb nc nd bi translated"><strong class="lp jd"/></li><li id="272f" class="mv mw it lp b lq ne lu nf ly ng mc nh mg ni mk na nb nc nd bi translated"><strong class="lp jd"> <em class="ml">数据集很大</em> </strong> → <strong class="lp jd">使用DNNs: </strong>推荐使用DNNs，因为GPs运行时间与示例数据集数量的比例很小，还因为DNNs已被证明在给定足够大的数据集的情况下，可以在各种机器学习任务上实现最先进的性能。</li><li id="b45d" class="mv mw it lp b lq ne lu nf ly ng mc nh mg ni mk na nb nc nd bi translated"><strong class="lp jd"> <em class="ml">执行连续插值→ </em>使用GPs: </strong>推荐使用连续GPs，因为连续GPs使用连续核函数测量距离，如RBF核和Matern核[11]，从而允许以考虑数据集中所有点的方式对来自现有点的新点进行<em class="ml">线性</em>加权。通过要求现有点的线性组合，仍然可以观察到精致的插值细节。</li><li id="a762" class="mv mw it lp b lq ne lu nf ly ng mc nh mg ni mk na nb nc nd bi translated"><strong class="lp jd"> <em class="ml">执行离散插值→ </em>使用GPs: </strong>推荐使用，因为离散/网格GPs使用稀疏、离散核函数测量距离，如网格插值核。稀疏结构仍然允许通过考虑所有现有的点来预测新点，但是是以计算效率更高的方式进行的。</li><li id="0da0" class="mv mw it lp b lq ne lu nf ly ng mc nh mg ni mk na nb nc nd bi translated"><strong class="lp jd"> <em class="ml">在动态数据集上学习和预测→ </em>使用DNNs: </strong>由于GPs(几乎)是直接方法，它们的预测机制很大程度上由创建它们的数据集定义。因此，如果定义GPs的数据集是动态的，这将需要重新调整/添加新的数据点，这将涉及重新计算协方差矩阵的逆，这是一项成本高昂的操作。相反，DNNs可以很容易地适应新的数据点，因为它们是逆模型，并且预测仅由这些模型所基于的数据间接确定。</li><li id="a4b0" class="mv mw it lp b lq ne lu nf ly ng mc nh mg ni mk na nb nc nd bi translated"><strong class="lp jd"> <em class="ml">其他情况→您决定:</em> </strong>当然还有其他情况没有被上述建议解决。为了解决这些情况，考虑分析上面讨论的相似性/差异/权衡，以确定两个模型类中哪一个性能更好。</li></ol><h1 id="b1e3" class="kp kq it bd kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm bi translated"><strong class="ak">总结</strong></h1><figure class="mm mn mo mp gt kd gh gi paragraph-image"><div role="button" tabindex="0" class="ke kf di kg bf kh"><div class="gh gi nw"><img src="../Images/6df0fd7fc70363a5bfaaad127045c693.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*Lst-1WKVJN_zPN1K"/></div></div><p class="kk kl gj gh gi km kn bd b be z dk translated">由<a class="ae ko" href="https://unsplash.com/@kellysikkema?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">凯利·西克玛</a>在<a class="ae ko" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="cede" class="pw-post-body-paragraph ln lo it lp b lq mq ls lt lu mr lw lx ly ms ma mb mc mt me mf mg mu mi mj mk im bi translated">我们已经讨论了<strong class="lp jd">高斯过程(GPs) </strong>和<strong class="lp jd">深度神经网络(DNNs) </strong>的理论相似性/差异、优缺点和应用。我们发现了以下情况:</p><p id="abcf" class="pw-post-body-paragraph ln lo it lp b lq mq ls lt lu mr lw lx ly ms ma mb mc mt me mf mg mu mi mj mk im bi">___________________________________________________________________</p><ol class=""><li id="39ca" class="mv mw it lp b lq mq lu mr ly mx mc my mg mz mk na nb nc nd bi translated"><strong class="lp jd"> GPs </strong>为【几乎】<em class="ml">非参数化</em>，而<strong class="lp jd"> DNNs </strong>为<em class="ml">过参数化。</em></li><li id="09e3" class="mv mw it lp b lq ne lu nf ly ng mc nh mg ni mk na nb nc nd bi translated"><strong class="lp jd"> GPs </strong>为(近)<em class="ml">正方法</em>，而<strong class="lp jd"> DNNs </strong>为<em class="ml">逆方法。</em></li><li id="9b45" class="mv mw it lp b lq ne lu nf ly ng mc nh mg ni mk na nb nc nd bi translated"><strong class="lp jd"> GPs </strong>一般用二阶方法优化，而<strong class="lp jd"> DNNs </strong>一般用一阶方法优化。</li><li id="a10c" class="mv mw it lp b lq ne lu nf ly ng mc nh mg ni mk na nb nc nd bi translated">GPs的结构使得这些模型具有很强的可解释性。尽管DNNs长期以来被批评为“黑箱”,但今天的研究正在帮助这些模型变得更容易解释。</li><li id="4684" class="mv mw it lp b lq ne lu nf ly ng mc nh mg ni mk na nb nc nd bi translated">GPs本质上是内核机器。在一定条件下，<strong class="lp jd"> DNNs </strong>也可以分析为内核机。</li><li id="3008" class="mv mw it lp b lq ne lu nf ly ng mc nh mg ni mk na nb nc nd bi translated"><strong class="lp jd"> DNNs </strong>和<strong class="lp jd"> GPs </strong>都通过一阶和二阶优化方法改进了他们的模型。</li><li id="7599" class="mv mw it lp b lq ne lu nf ly ng mc nh mg ni mk na nb nc nd bi translated"><strong class="lp jd"> GPs </strong>一般比<strong class="lp jd"> DNNs </strong>需要更少的数据，只需要优化少量(超)参数，对爆炸和消失梯度等现象具有鲁棒性。</li><li id="f5f3" class="mv mw it lp b lq ne lu nf ly ng mc nh mg ni mk na nb nc nd bi translated"><strong class="lp jd"> GP </strong>运行时相对于<strong class="lp jd"> DNNs </strong>的样本数伸缩性较差，相对于神经网络的自动学习较少。</li><li id="79b3" class="mv mw it lp b lq ne lu nf ly ng mc nh mg ni mk na nb nc nd bi translated">如果:(i) <strong class="lp jd">数据集很小</strong>，或(ii) <strong class="lp jd">执行插值→ </strong>使用<strong class="lp jd"> GPs。</strong></li><li id="64e4" class="mv mw it lp b lq ne lu nf ly ng mc nh mg ni mk na nb nc nd bi translated">如果:(i) <strong class="lp jd">数据集很大</strong>，或者(ii) <strong class="lp jd">数据集是动态的→ </strong>使用<strong class="lp jd"> DNNs。</strong></li></ol></div><div class="ab cl nx ny hx nz" role="separator"><span class="oa bw bk ob oc od"/><span class="oa bw bk ob oc od"/><span class="oa bw bk ob oc"/></div><div class="im in io ip iq"><p id="5382" class="pw-post-body-paragraph ln lo it lp b lq mq ls lt lu mr lw lx ly ms ma mb mc mt me mf mg mu mi mj mk im bi translated"><strong class="lp jd">感谢您的阅读！</strong>想看更多关于计算机视觉、强化学习、机器人技术的内容，请<a class="ae ko" href="https://rmsander.medium.com/" rel="noopener"> <strong class="lp jd">关注我</strong> </a>。考虑加盟Medium？请考虑通过这里 报名<a class="ae ko" href="https://rmsander.medium.com/subscribe" rel="noopener"> <strong class="lp jd">。感谢您的阅读！</strong></a></p></div><div class="ab cl nx ny hx nz" role="separator"><span class="oa bw bk ob oc od"/><span class="oa bw bk ob oc od"/><span class="oa bw bk ob oc"/></div><div class="im in io ip iq"><h2 id="1ea3" class="nj kq it bd kr nk nl dn kv nm nn dp kz ly no np ld mc nq nr lh mg ns nt ll iz bi translated">参考</h2><p id="b911" class="pw-post-body-paragraph ln lo it lp b lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk im bi translated">[1]贝尔金，米哈伊尔。"无所畏惧地适应:通过插值棱镜进行深度学习的显著数学现象."<em class="ml"> arXiv预印本arXiv:2105.14368 </em> (2021)。</p><p id="0c5d" class="pw-post-body-paragraph ln lo it lp b lq mq ls lt lu mr lw lx ly ms ma mb mc mt me mf mg mu mi mj mk im bi translated">[2] Jacot，Arthur，Franck Gabriel和Clément Hongler。"神经正切核:神经网络中的收敛和泛化."<em class="ml"> arXiv预印本arXiv:1806.07572 </em> (2018)。</p><p id="6bb3" class="pw-post-body-paragraph ln lo it lp b lq mq ls lt lu mr lw lx ly ms ma mb mc mt me mf mg mu mi mj mk im bi translated">[3]达米亚诺、安德烈亚斯和尼尔·劳伦斯。"深度高斯过程。"<em class="ml">人工智能与统计</em>。PMLR，2013年。</p><p id="d9c4" class="pw-post-body-paragraph ln lo it lp b lq mq ls lt lu mr lw lx ly ms ma mb mc mt me mf mg mu mi mj mk im bi translated">[4] Blomqvist、Kenneth、Samuel Kaski和Markus Heinonen。"深度卷积高斯过程."<em class="ml"> arXiv预印本arXiv:1810.03052 </em> (2018)。</p><p id="22f7" class="pw-post-body-paragraph ln lo it lp b lq mq ls lt lu mr lw lx ly ms ma mb mc mt me mf mg mu mi mj mk im bi translated">[5]Liu d . c .，Nocedal，j .关于用于大规模优化的有限内存BFGS方法。<em class="ml">数学规划</em> <strong class="lp jd"> 45、</strong>503–528(1989)。<a class="ae ko" href="https://doi.org/10.1007/BF01589116" rel="noopener ugc nofollow" target="_blank">https://doi.org/10.1007/BF01589116</a></p><p id="63ee" class="pw-post-body-paragraph ln lo it lp b lq mq ls lt lu mr lw lx ly ms ma mb mc mt me mf mg mu mi mj mk im bi translated">[6]罗伯茨、丹尼尔·a、绍·亚伊达和鲍里斯·哈宁。“深度学习理论的原理。”<em class="ml"> arXiv预印本arXiv:2106.10165 </em> (2021)。</p><p id="14a3" class="pw-post-body-paragraph ln lo it lp b lq mq ls lt lu mr lw lx ly ms ma mb mc mt me mf mg mu mi mj mk im bi translated">[7]自顶向下与自底向上的数据科学方法<a class="ae ko" href="https://blog.dataiku.com/top-down-vs.-bottom-up-approaches-to-data-science" rel="noopener ugc nofollow" target="_blank">。</a></p><p id="5934" class="pw-post-body-paragraph ln lo it lp b lq mq ls lt lu mr lw lx ly ms ma mb mc mt me mf mg mu mi mj mk im bi translated">[8] <em class="ml">赫伯特·罗宾斯和萨顿·门罗</em>一种随机逼近方法<em class="ml">《数理统计年鉴》，第22卷，№3。(1951年9月)，第400–407页，DOI: 10.1214/aoms/1177729586。</em></p><p id="5d8e" class="pw-post-body-paragraph ln lo it lp b lq mq ls lt lu mr lw lx ly ms ma mb mc mt me mf mg mu mi mj mk im bi translated">[9]阿米尼、亚力山大、威尔科·施瓦廷、艾娃·索莱马尼和丹妮拉·鲁斯。"深度证据回归"<em class="ml"> arXiv预印本arXiv:1910.02600 </em> (2019)。</p><p id="ca40" class="pw-post-body-paragraph ln lo it lp b lq mq ls lt lu mr lw lx ly ms ma mb mc mt me mf mg mu mi mj mk im bi translated">[10] Park，Sangdon等人，“深度神经网络分类器的PAC置信度预测”<em class="ml"> arXiv预印本arXiv:2011.00716 </em> (2020)。</p><p id="b114" class="pw-post-body-paragraph ln lo it lp b lq mq ls lt lu mr lw lx ly ms ma mb mc mt me mf mg mu mi mj mk im bi translated">11卡尔·爱德华·拉斯姆森。"机器学习中的高斯过程."<em class="ml">关于机器学习的暑期学校</em>。施普林格，柏林，海德堡，2003。</p><p id="aae4" class="pw-post-body-paragraph ln lo it lp b lq mq ls lt lu mr lw lx ly ms ma mb mc mt me mf mg mu mi mj mk im bi translated">[12] Selvaraju，Ramprasaath R .等人，“Grad-cam:通过基于梯度的定位从深度网络进行视觉解释。”<em class="ml">IEEE计算机视觉国际会议论文集</em>。2017.</p></div></div>    
</body>
</html>