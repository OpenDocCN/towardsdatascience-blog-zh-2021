<html>
<head>
<title>Hands-on: Optimizing and benchmarking Body Pose Estimation models</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">实践:优化和基准身体姿势估计模型</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/hands-on-optimizing-nvidia-body-pose-net-model-e27da4a9f8ec?source=collection_archive---------24-----------------------#2021-09-07">https://towardsdatascience.com/hands-on-optimizing-nvidia-body-pose-net-model-e27da4a9f8ec?source=collection_archive---------24-----------------------#2021-09-07</a></blockquote><div><div class="fc ii ij ik il im"/><div class="in io ip iq ir"><div class=""/><div class=""><h2 id="f54a" class="pw-subtitle-paragraph jr it iu bd b js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki dk translated">Nvidia Body Pose Net与OpenPifPaf模型</h2></div><figure class="kk kl km kn gu ko gi gj paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gi gj kj"><img src="../Images/413edb9e3b75fe7e7677e83898224160.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*ayB2qzrFBLV7gVr_"/></div></div><p class="kv kw gk gi gj kx ky bd b be z dk translated">来自<a class="ae kz" href="https://www.pexels.com/photo/photo-of-code-projected-over-woman-3861969/?utm_content=attributionCopyText&amp;utm_medium=referral&amp;utm_source=pexels" rel="noopener ugc nofollow" target="_blank"> Pexels </a>的<a class="ae kz" href="https://www.pexels.com/@thisisengineering?utm_content=attributionCopyText&amp;utm_medium=referral&amp;utm_source=pexels" rel="noopener ugc nofollow" target="_blank"> ThisIsEngineering </a>摄影</p></figure><p id="a443" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">伊西多拉·斯坦科维奇，内韦娜·米勒托维奇，戈兰·贝纳克<strong class="lc iv">，</strong>德贝马尔亚·比斯瓦斯— <a class="ae kz" href="https://darwinedge.com/" rel="noopener ugc nofollow" target="_blank">瑞士达尔文边缘</a></p><p id="9979" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated"><strong class="lc iv">摘要</strong> <em class="lw">。Nvidia最近宣布推出</em> <a class="ae kz" href="https://developer.nvidia.com/blog/training-optimizing-2d-pose-estimation-model-with-tao-toolkit-part-1/" rel="noopener ugc nofollow" target="_blank"> <em class="lw"> 2D身体姿势估计模型</em> </a> <em class="lw">作为迁移学习工具包3.0的一部分。在本文中，我们提供了一个详细的教程来训练和优化模型。我们进一步提供了另一个广泛用于感知任务的开源模型的基准测试结果:</em><a class="ae kz" href="https://github.com/openpifpaf/openpifpaf" rel="noopener ugc nofollow" target="_blank"><em class="lw">OpenPifPaf</em></a><em class="lw">——允许您决定何时使用哪个模型。</em></p><p id="a38a" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">在之前的一篇文章[4]中，我们通过将身体姿势模型应用于现实生活中的医疗保健应用程序，展示了它的实际效用。该应用程序在病人家中监控病人，并在有人从床上摔下来、发生事故等情况时发出警报。这对于医院和养老院的居民来说是个大问题。</p><figure class="kk kl km kn gu ko gi gj paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gi gj lx"><img src="../Images/6071ba4996d727d0c1bbd7eea216331e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*5s-PbKKVScuklM88.png"/></div></div><p class="kv kw gk gi gj kx ky bd b be z dk translated">实践中的身体姿态检测(图片由作者提供)</p></figure><h1 id="e678" class="ly lz iu bd ma mb mc md me mf mg mh mi ka mj kb mk kd ml ke mm kg mn kh mo mp bi translated">训练身体姿态网络模型</h1><p id="e0a6" class="pw-post-body-paragraph la lb iu lc b ld mq jv lf lg mr jy li lj ms ll lm ln mt lp lq lr mu lt lu lv in bi translated">在本节中，我们将介绍训练和优化Nvidia身体姿势网络模型所需的所有步骤。该模型使用TAO Toolkit进行训练，TAO Toolkit使用预训练的模型和自定义数据集来构建新的AI模型[1]。</p><p id="da26" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">第一步是安装和运行TAO工具包。遵循此链接中的步骤:</p><p id="1082" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated"><a class="ae kz" href="https://docs.nvidia.com/tao/tao-toolkit/text/tao_toolkit_quick_start_guide.html" rel="noopener ugc nofollow" target="_blank">陶工具包快速入门指南—陶工具包3.0文档</a></p><p id="c075" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">登录到NGC docker注册表后，运行以下命令:</p><pre class="kk kl km kn gu mv mw mx my aw mz bi"><span id="6183" class="na lz iu mw b gz nb nc l nd ne">mkdir Programs<br/>cd Programs/<br/>wget -O ngccli_linux.zip <a class="ae kz" href="https://ngc.nvidia.com/downloads/ngccli_linux.zip" rel="noopener ugc nofollow" target="_blank">https://ngc.nvidia.com/downloads/ngccli_linux.zip</a> &amp;&amp; unzip -o ngccli_linux.zip &amp;&amp; chmod u+x ngc<br/>mkdir ngccli_linux<br/>cd ngccli_linux/<br/>chmod u+x ngc <br/>echo "export PATH=\"\$PATH:$(pwd)\"" &gt;&gt; ~/.bash_profile &amp;&amp; source ~/.bash_profile<br/>cd ..<br/>ngc config set<br/>ngc registry model list</span></pre><p id="31bd" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">我们使用conda虚拟环境(按照<a class="ae kz" href="https://docs.anaconda.com/anaconda/install/linux/" rel="noopener ugc nofollow" target="_blank">链接</a>中的步骤安装Anaconda)。要创建并激活虚拟环境，请运行以下两个命令:</p><pre class="kk kl km kn gu mv mw mx my aw mz bi"><span id="00f2" class="na lz iu mw b gz nb nc l nd ne">conda create -n 'env_name' python=3.7 <br/>conda activate 'env_name'</span></pre><p id="6e12" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">该工具包现在可以使用了，培训准备工作可以继续进行。在<a class="ae kz" href="https://developer.nvidia.com/blog/training-optimizing-2d-pose-estimation-model-with-tao-toolkit-part-1/" rel="noopener ugc nofollow" target="_blank">这一页</a>中，解释了强制步骤。第一步是环境设置，其中下载了最新的示例(用于设置培训的配置文件)，设置了env变量并创建了挂载文件。最后，安装所需的依赖项。下一步是下载预先训练好的模型。</p><blockquote class="nf ng nh"><p id="8038" class="la lb lw lc b ld le jv lf lg lh jy li ni lk ll lm nj lo lp lq nk ls lt lu lv in bi translated">用来训练这个模型的数据集是COCO数据集，它是从<a class="ae kz" href="https://cocodataset.org/#download" rel="noopener ugc nofollow" target="_blank">这个链接</a>下载的。</p></blockquote><p id="2c7a" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">应该是这样组织的:</p><pre class="kk kl km kn gu mv mw mx my aw mz bi"><span id="f387" class="na lz iu mw b gz nb nc l nd ne">&lt;user&gt;                          <br/>├──tlt-experiments<br/>   ├──bpnet<br/>     ├──data            <br/>       ├──annotations              <br/>       ├──test2017        <br/>       ├──train2017               <br/>       └──val2017</span></pre><p id="2d30" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">数据集需要为训练做准备，因此分段掩码与<em class="lw"> tfrecords </em>一起生成。配置文件和所用命令的详细解释可在<a class="ae kz" href="https://docs.nvidia.com/tao/tao-toolkit/text/bodypose_estimation/bodyposenet.html#bodyposenet" rel="noopener ugc nofollow" target="_blank">这里</a>找到。</p><p id="c65b" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">下一步是为训练配置spec文件，它包含六个组件:训练器、数据加载器、增强、标签处理器、模型和优化器。我们使用默认的规范文件，即/workspace/examples/BP net/specs/BP net _ train _ m1 _ coco . YAML。</p><p id="fcff" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">现在可以开始训练了。</p><blockquote class="nf ng nh"><p id="4579" class="la lb lw lc b ld le jv lf lg lh jy li ni lk ll lm nj lo lp lq nk ls lt lu lv in bi translated">我们在Nvidia GeForce RTX 2080 GPU上的培训过程持续了104个小时。</p></blockquote><p id="e22a" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">用于评估模型的度量是精确度和召回率。这些值按以下方式计算:</p><figure class="kk kl km kn gu ko gi gj paragraph-image"><div class="gi gj nl"><img src="../Images/eb7ffa0c743e27fb4f2c80fc2a7709f7.png" data-original-src="https://miro.medium.com/v2/resize:fit:412/format:webp/1*jeTTEut_4e28K-xPvlGcKg.png"/></div></figure><p id="9e6e" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">其中TP是真阳性的数量，FP是假阳性的数量，FN是假阴性的数量。针对交集/并集(IoU)的不同阈值计算精度和召回率。IoU是基于预测边界框和地面真实边界框之间的重叠计算的参数。</p><p id="7fb5" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">例如，如果IoU阈值为0.5，并且用于预测的IoU值为0.7，则该预测被分类为真阳性。反之，如果IoU为0.3，则归类为假阳性。假阴性是未能预测图像中的对象的预测。</p><blockquote class="nf ng nh"><p id="d3bb" class="la lb lw lc b ld le jv lf lg lh jy li ni lk ll lm nj lo lp lq nk ls lt lu lv in bi translated">在评估结果中，IoU阈值为0.5:0.95。这意味着精度和召回率是通过对0.5和0.95之间的不同阈值以特定步长平均精度和召回率来计算的。</p></blockquote><p id="0a3c" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">我们在验证集上得到的准确率和召回率是:</p><figure class="kk kl km kn gu ko gi gj paragraph-image"><div class="gi gj nm"><img src="../Images/f443f1a5c5962dc5f9916e5a62cd39cd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1320/format:webp/1*_7iiEyqcdX2a90TxlQGJmQ.png"/></div></figure><h2 id="c3c1" class="na lz iu bd ma nn no dn me np nq dp mi lj nr ns mk ln nt nu mm lr nv nw mo nx bi translated">最佳化</h2><p id="32ba" class="pw-post-body-paragraph la lb iu lc b ld mq jv lf lg mr jy li lj ms ll lm ln mt lp lq lr mu lt lu lv in bi translated">使用TAO Toolkit对模型进行的优化在此处解释<a class="ae kz" href="https://developer.nvidia.com/blog/training-optimizing-2d-pose-estimation-model-with-tao-toolkit-part-2/" rel="noopener ugc nofollow" target="_blank">。</a>优化模型的一种方法是修剪。它消除了低量级的权重，从而产生压缩模型，占用较少的内存资源[2]。</p><blockquote class="nf ng nh"><p id="279d" class="la lb lw lc b ld le jv lf lg lh jy li ni lk ll lm nj lo lp lq nk ls lt lu lv in bi translated">修剪后的准确率和召回率不明显降低，因此使用修剪模型是合理的。</p></blockquote><figure class="kk kl km kn gu ko gi gj paragraph-image"><div class="gi gj ny"><img src="../Images/6806658251981375c1a8bd7149a148cb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1288/format:webp/1*IxoVDbFI2CBNW5h3kXSlPw.png"/></div></figure><p id="63ef" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">修剪后，建议重新训练模型，因为修剪会导致精度下降。在同一个GPU和数据集上的再训练持续了99个小时。重新训练模型的评估结果是:</p><figure class="kk kl km kn gu ko gi gj paragraph-image"><div class="gi gj nz"><img src="../Images/6aa27df8b1f4a317224f7c19bcd663fb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1292/format:webp/1*B9fWBFsvGEqMvWLjIQHLmA.png"/></div></figure><h1 id="d61e" class="ly lz iu bd ma mb mc md me mf mg mh mi ka mj kb mk kd ml ke mm kg mn kh mo mp bi translated">OpenPifPaf基准测试</h1><p id="4deb" class="pw-post-body-paragraph la lb iu lc b ld mq jv lf lg mr jy li lj ms ll lm ln mt lp lq lr mu lt lu lv in bi translated">OpenPifPaf [3]是一种通用的神经网络架构，它使用复合场来实时检测和构建时空姿态。我们使用自然人体运动的几何学来计算和检测预先确定的感兴趣区域中的身体位置，以检测一个人是否站着、坐着、躺着、摔倒等。</p><p id="a8ba" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">OpenPifPaf和Body Pose Net模型都是自底向上的实现，这意味着它们检测图像中的所有关节，然后将属于不同人的部分关联起来。另一方面，自上而下的方法首先检测人，然后对每个人执行单人姿势估计以预测人体关节[5]。</p><blockquote class="nf ng nh"><p id="8f69" class="la lb lw lc b ld le jv lf lg lh jy li ni lk ll lm nj lo lp lq nk ls lt lu lv in bi translated">自下而上的方法更适合有人群的图像，以及复杂的姿势和人们互相遮挡的情况。</p></blockquote><p id="d577" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">OpenPifPaf模型也在COCO数据集上进行了训练。评估是在与身体姿态网络模型相同的验证集上进行的。以下是我们在相同输入量下得到的结果:</p><figure class="kk kl km kn gu ko gi gj paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gi gj oa"><img src="../Images/945071180cea13d813c92067276db7dc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3wy6wXg_dya6T74Dw6nh2A.png"/></div></div></figure><p id="e7a9" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">用于训练OpenPifPaf模型的参数与用于身体姿势网络的参数略有不同。学习率是0.001，而在身体姿势网络模型中，基本学习率是2e-5，最小学习率是8e-8。动量为0.95，而BP网络使用0.9。最后，批量大小是8，而不是BP网络中的10。</p><blockquote class="nf ng nh"><p id="0fd9" class="la lb lw lc b ld le jv lf lg lh jy li ni lk ll lm nj lo lp lq nk ls lt lu lv in bi translated">事实证明，OpenPifPaf模型为每个IoU阈值提供了更好的结果。IoU=0.5:0.95的平均精度是0.621，而身体姿势网络模型中的相同度量是0.487。</p></blockquote><h2 id="79ac" class="na lz iu bd ma nn no dn me np nq dp mi lj nr ns mk ln nt nu mm lr nv nw mo nx bi translated">结果</h2><blockquote class="nf ng nh"><p id="ef96" class="la lb lw lc b ld le jv lf lg lh jy li ni lk ll lm nj lo lp lq nk ls lt lu lv in bi translated">为了模型的视觉比较，我们使用了新的数据集— <a class="ae kz" href="https://github.com/Fang-Haoshu/Halpe-FullBody" rel="noopener ugc nofollow" target="_blank"> <em class="iu"> Halpe全身人体关键点</em> </a> <em class="iu"> </em>数据集。</p></blockquote><p id="d993" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">此处描述的两个模型从未见过该数据集。选择某个子集来可视化这些模型提供的预测结果。</p><figure class="kk kl km kn gu ko gi gj paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gi gj ob"><img src="../Images/7b73a723e2d4cb27ae61eba4243ed79e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*St4uqNlZZ6mBpYjnmiJjKw.png"/></div></div><p class="kv kw gk gi gj kx ky bd b be z dk translated">Nvidia Body Pose Net与OpenPifPaf的基准测试</p></figure><h1 id="751e" class="ly lz iu bd ma mb mc md me mf mg mh mi ka mj kb mk kd ml ke mm kg mn kh mo mp bi translated">参考</h1><p id="b84d" class="pw-post-body-paragraph la lb iu lc b ld mq jv lf lg mr jy li lj ms ll lm ln mt lp lq lr mu lt lu lv in bi translated">[1] <a class="ae kz" href="https://docs.nvidia.com/tao/tao-toolkit/text/overview.html#tao-conversational-ai-workflow-overview" rel="noopener ugc nofollow" target="_blank">概述— TAO工具包3.0文档</a></p><p id="ddb5" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">[2]开尔文。<em class="lw">通过修剪进行模型压缩</em>。<a class="ae kz" rel="noopener" target="_blank" href="/model-compression-via-pruning-ac9b730a7c7b">https://towards data science . com/model-compression-via-pruning-ac9b 730 a 7 c 7 b</a></p><p id="179f" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">[3]克里斯、贝尔托尼和阿拉希:OpenPif-Paf: <em class="lw">语义关键点检测和时空关联的复合字段</em>。arXiv，abs/2103.02440，2021。</p><p id="5caf" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">[4] M. Vuletic等人。艾尔。<em class="lw">面向医疗保健应用的Edge AI框架</em>。2021年8月，关于人工智能用于老龄化、康复和智能辅助生活的第四届IJCAI研讨会(<a class="ae kz" href="https://sites.google.com/view/arial2021" rel="noopener ugc nofollow" target="_blank"> ARIAL </a>)。<a class="ae kz" href="https://medium.com/darwin-edge-ai/edge-ai-framework-for-rapid-prototyping-and-deployment-cabf466dddef" rel="noopener">https://medium . com/Darwin-edge-ai/edge-ai-framework-for-rapid-prototyping-and-deployment-cabf 466 DD def</a></p><p id="3ac5" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">[5] <a class="ae kz" href="https://beyondminds.ai/blog/an-overview-of-human-pose-estimation-with-deep-learning/" rel="noopener ugc nofollow" target="_blank">深度学习的人体姿态估计概述——beyond minds</a></p></div></div>    
</body>
</html>