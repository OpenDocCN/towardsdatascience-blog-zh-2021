<html>
<head>
<title>The Poisson Hidden Markov Model for Time Series Regression</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">时间序列回归的泊松隐马尔可夫模型</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/the-poisson-hidden-markov-model-for-time-series-regression-236c269914dd?source=collection_archive---------4-----------------------#2021-11-27">https://towardsdatascience.com/the-poisson-hidden-markov-model-for-time-series-regression-236c269914dd?source=collection_archive---------4-----------------------#2021-11-27</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="11e9" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">如何混合使用两个强大的随机过程来模拟时间序列数据</h2></div><p id="1293" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">一个<strong class="kh ir">泊松隐马尔可夫模型</strong>使用两个随机过程的混合，一个<strong class="kh ir">泊松过程</strong>和一个<strong class="kh ir">离散马尔可夫过程</strong>，来表示基于<strong class="kh ir">计数的时间序列</strong>数据。</p><p id="12af" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">基于计数的时间序列数据仅包含整数数值，如 0、1、2、3 等。这种数据的例子是电子商务网站上的每日点击数、百货商店中每天购买的肥皂的数量等等。</p><p id="83b5" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">使用诸如线性模型或 ARIMA 模型之类的模型不能充分地表示这样的数据，因为在那些模型中，因变量(<strong class="kh ir"> <em class="lb"> y </em> </strong>)被假定为允许正值和负值的实值。基于计数的时间序列数据需要使用模型，这些模型假定因变量是 1)离散的，和 2)非负的，换句话说，是整数的。</p><p id="c243" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><a class="ae lc" rel="noopener" target="_blank" href="/an-illustrated-guide-to-the-poisson-regression-model-50cccba15958">泊松</a>或类泊松模型，如<a class="ae lc" rel="noopener" target="_blank" href="/generalized-poisson-regression-for-real-world-datasets-d1ff32607d79">广义泊松</a>和<a class="ae lc" rel="noopener" target="_blank" href="/negative-binomial-regression-f99031bb25b4">负二项式</a>模型通常适用于对整个编号的数据集进行建模。</p><p id="049f" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">不幸的是，泊松系列模型没有明确说明，因此无法充分捕捉数据中的自相关，而这恰好是时间序列数据集的一个标志。</p><p id="bc68" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">多年来，研究人员对基本泊松模型进行了一些修改，使其能够解释时间序列数据中的自相关性。其中值得注意的是<a class="ae lc" rel="noopener" target="_blank" href="/poisson-regression-models-for-time-series-data-sets-54114e68c46d"> <strong class="kh ir">泊松自回归模型</strong> </a>和<a class="ae lc" rel="noopener" target="_blank" href="/an-introduction-to-the-poisson-integer-arima-regression-model-b66d3ff2e6e5"> <strong class="kh ir">泊松整数 ARIMA 模型</strong> </a> <strong class="kh ir"> </strong>，其中泊松条件均值不仅表示为回归变量<strong class="kh ir"> <em class="lb"> X </em> </strong>的线性组合，还表示为因变量<strong class="kh ir"> <em class="lb"> y </em> </strong>的滞后副本。</p><p id="9318" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">泊松隐马尔可夫模型</strong>更进了一步，混合了离散的<em class="lb"> k </em>状态马尔可夫模型，该模型是“隐”的，因为在时间序列的每个时间步，人们不能确切知道马尔可夫过程处于哪个马尔可夫状态。取而代之的是，在每个时间步，人们估计每个区域可能存在的对泊松模型预测的平均值的影响。这使得泊松过程意味着一个随机变量<strong class="kh ir">，其期望值是基础马尔可夫模型处于特定状态的概率的函数。</strong></p><p id="4ca9" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在本文中，我们将精确地描述可见泊松模型和隐马尔可夫模型预测的平均值之间的关系。</p><p id="8826" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如果你是马尔可夫过程或隐马尔可夫模型的新手，请浏览下面两篇文章:</p><div class="ld le gp gr lf lg"><a rel="noopener follow" target="_blank" href="/a-beginners-guide-to-discrete-time-markov-chains-d5be17cf0e12"><div class="lh ab fo"><div class="li ab lj cl cj lk"><h2 class="bd ir gy z fp ll fr fs lm fu fw ip bi translated">离散时间马尔可夫链初学者指南</h2><div class="ln l"><h3 class="bd b gy z fp ll fr fs lm fu fw dk translated">以及如何使用 Python 模拟离散马尔可夫过程的教程</h3></div><div class="lo l"><p class="bd b dl z fp ll fr fs lm fu fw dk translated">towardsdatascience.com</p></div></div><div class="lp l"><div class="lq l lr ls lt lp lu lv lg"/></div></div></a></div><div class="ld le gp gr lf lg"><a rel="noopener follow" target="_blank" href="/a-math-lovers-guide-to-hidden-markov-models-ad718df9fde8"><div class="lh ab fo"><div class="li ab lj cl cj lk"><h2 class="bd ir gy z fp ll fr fs lm fu fw ip bi translated">数学爱好者的隐马尔可夫模型指南</h2><div class="ln l"><h3 class="bd b gy z fp ll fr fs lm fu fw dk translated">它们是如何工作的，以及它们为什么被“隐藏”起来。</h3></div><div class="lo l"><p class="bd b dl z fp ll fr fs lm fu fw dk translated">towardsdatascience.com</p></div></div><div class="lp l"><div class="lw l lr ls lt lp lu lv lg"/></div></div></a></div><p id="1263" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我还建议回顾下面这篇关于<strong class="kh ir">马尔可夫转换动态回归模型</strong>的文章，以详细了解 MSDR 模型是如何构建的。泊松 HMM 是一种 MSDR 模型，其中模型的“可见”部分服从泊松过程。</p><div class="ld le gp gr lf lg"><a rel="noopener follow" target="_blank" href="/a-worms-eye-view-of-the-markov-switching-dynamic-regression-model-2fb706ba69f3"><div class="lh ab fo"><div class="li ab lj cl cj lk"><h2 class="bd ir gy z fp ll fr fs lm fu fw ip bi translated">马尔可夫转换动态回归模型的蠕虫视角</h2><div class="ln l"><h3 class="bd b gy z fp ll fr fs lm fu fw dk translated">详细解释了 MSDR 模型，并使用真实世界数据集介绍了关于 MSDR 的 Python 教程</h3></div><div class="lo l"><p class="bd b dl z fp ll fr fs lm fu fw dk translated">towardsdatascience.com</p></div></div><div class="lp l"><div class="lx l lr ls lt lp lu lv lg"/></div></div></a></div></div><div class="ab cl ly lz hu ma" role="separator"><span class="mb bw bk mc md me"/><span class="mb bw bk mc md me"/><span class="mb bw bk mc md"/></div><div class="ij ik il im in"><h1 id="734c" class="mf mg iq bd mh mi mj mk ml mm mn mo mp jw mq jx mr jz ms ka mt kc mu kd mv mw bi translated">泊松隐马尔可夫模型的详细说明</h1><p id="993e" class="pw-post-body-paragraph kf kg iq kh b ki mx jr kk kl my ju kn ko mz kq kr ks na ku kv kw nb ky kz la ij bi translated">我们将首先阐述“可见的”泊松过程，然后展示马尔可夫过程如何“混合”到泊松过程中。</p><p id="f1b3" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">考虑以下包含附加误差成分的模型方程:</p><figure class="nd ne nf ng gt nh gh gi paragraph-image"><div class="gh gi nc"><img src="../Images/076910b886b1efb215af2ed69619563e.png" data-original-src="https://miro.medium.com/v2/resize:fit:598/format:webp/1*BjbgLvfaP3s1T0kw226cBA.png"/></div><p class="nj nk gj gh gi nl nm bd b be z dk translated">y_t 表示为平均值和误差项之和(图片由作者提供)</p></figure><p id="7de8" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在上面的模型中，观测值<em class="lb"> y_t </em>是预测值<em class="lb"> μ_cap_t a </em>和残差<em class="lb"> ε_t </em>之和。我们进一步假设<em class="lb"> ε_t </em>是一个均值为零的<a class="ae lc" rel="noopener" target="_blank" href="/heteroscedasticity-is-nothing-to-be-afraid-of-730dd3f7ca1f">同方差</a>(常方差)正态分布随机变量，表示为<em class="lb"> N(0，σ ) </em>。</p><p id="b96a" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><em class="lb"> y_t </em>是<em class="lb">【n×1】</em>观测值向量<strong class="kh ir"> <em class="lb"> y </em> </strong>的第<em class="lb"> t- </em>个元素:</p><figure class="nd ne nf ng gt nh gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/4d6c17fb702dd1de9f2bea0ac34847e1.png" data-original-src="https://miro.medium.com/v2/resize:fit:464/format:webp/1*eMp9mSY9eLLTc4SWGKym4w.png"/></div><p class="nj nk gj gh gi nl nm bd b be z dk translated">因变量<strong class="bd no"> y </strong>(图片作者提供)</p></figure><p id="6330" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们假设<strong class="kh ir"> <em class="lb"> y </em> </strong>服从泊松过程。因此，[1…n] 中的<em class="lb"> t 的<em class="lb"> y_t </em>是<em class="lb"> n 个</em>独立的泊松分布随机变量，每个变量都具有可能不同的均值<em class="lb"> μ_t </em>，如下所示:</em></p><figure class="nd ne nf ng gt nh gh gi paragraph-image"><div role="button" tabindex="0" class="nq nr di ns bf nt"><div class="gh gi np"><img src="../Images/12595c61334d93bb43e29d2897e5ef2e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nXNq2e-juL0H94U-BGQc8g.png"/></div></div><p class="nj nk gj gh gi nl nm bd b be z dk translated">服从泊松过程的时间序列<strong class="bd no"> <em class="nu"> y </em> </strong>(图片由作者提供)</p></figure><p id="b70f" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir"><em class="lb"/></strong>的<strong class="kh ir"> P </strong>概率<strong class="kh ir">M</strong>ass<strong class="kh ir">F</strong>function(PMF)，也就是观察到<em class="lb"> y_t </em>的概率的另一种说法，由下式给出:</p><figure class="nd ne nf ng gt nh gh gi paragraph-image"><div class="gh gi nv"><img src="../Images/f4ad756ce2bb9f40a027313cf5e7d71e.png" data-original-src="https://miro.medium.com/v2/resize:fit:738/format:webp/1*NgGWqQyl1boJ3G1JQb49MQ.png"/></div><p class="nj nk gj gh gi nl nm bd b be z dk translated">泊松分布的 PMF<strong class="bd no">y</strong>均值<em class="nu">μt(图片由作者提供)</em></p></figure><p id="560d" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在一个训练好的回归模型中，我们用“拟合”平均值<em class="lb"> μ_cap_t. </em>代替<em class="lb"> μ_t </em></p><p id="88cd" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">设<strong class="kh ir"> <em class="lb"> X </em> </strong>为一个<em class="lb">【n X(m+1)】</em>大小的回归变量矩阵如下图。这个矩阵的第一列是截取<strong class="kh ir"> <em class="lb">、</em> </strong>和<strong class="kh ir"><em class="lb">x</em></strong><em class="lb">_ t</em>的占位符，是这个矩阵的一行。</p><figure class="nd ne nf ng gt nh gh gi paragraph-image"><div class="gh gi nw"><img src="../Images/1d92e253cb40faa0c8abc8d773fdb240.png" data-original-src="https://miro.medium.com/v2/resize:fit:876/format:webp/1*7zNNSLqU4_L3J1PIml8SUg.png"/></div><p class="nj nk gj gh gi nl nm bd b be z dk translated">回归变量矩阵<strong class="bd no"> X </strong>(图片来自作者)</p></figure><p id="1faa" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">设<strong class="kh ir"> <em class="lb"> β_cap </em> </strong>为回归系数的<em class="lb"> [(m+1) X 1] </em>向量。<em class="lb"/><em class="lb">β</em>上的“cap”表示它是模型训练产生的系数的拟合值。</p><figure class="nd ne nf ng gt nh gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/bd1ba042b775e1b0ba35b783114b299f.png" data-original-src="https://miro.medium.com/v2/resize:fit:172/format:webp/1*WJ6tQU4KsBco9A3DKskYpw.png"/></div><p class="nj nk gj gh gi nl nm bd b be z dk translated">拟合回归系数β_cap 的向量(图片由作者提供)</p></figure><p id="9545" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们将平均值<em class="lb"> μ_cap_t </em>表示为<strong class="kh ir"> <em class="lb"> x </em> </strong> <em class="lb"> _t 和</em> <strong class="kh ir"> <em class="lb"> β_cap </em> </strong>的指数线性组合如下:</p><figure class="nd ne nf ng gt nh gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/3c9a64d2b29c9098e326065067e51444.png" data-original-src="https://miro.medium.com/v2/resize:fit:396/format:webp/1*_GSbXugI22MM_Jtb4M_baA.png"/></div><p class="nj nk gj gh gi nl nm bd b be z dk translated">泊松回归模型的指数均值(图片由作者提供)</p></figure><p id="2562" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">其中，<strong class="kh ir"><em class="lb">x</em></strong><em class="lb">_ t</em>与<strong class="kh ir"> <em class="lb"> β_cap </em> </strong>之间的点积可以展开表示如下:</p><figure class="nd ne nf ng gt nh gh gi paragraph-image"><div role="button" tabindex="0" class="nq nr di ns bf nt"><div class="gh gi nz"><img src="../Images/1f6b2c07048de954e82eaafcc7b194d3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WBqIWGvQFV4lUCbkKE-pbA.png"/></div></div><p class="nj nk gj gh gi nl nm bd b be z dk translated">回归变量和拟合回归系数的线性组合(图片由作者提供)</p></figure><p id="de20" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">点积的指数运算确保了平均值以及模型的预测永远不会为负。这是对整数计数数据集建模的关键要求。</p><p id="0bdb" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这就完成了泊松 HMM 的泊松部分的规范。现在，让我们把注意力转向马尔可夫部分。</p><h2 id="64cd" class="oa mg iq bd mh ob oc dn ml od oe dp mp ko of og mr ks oh oi mt kw oj ok mv ol bi translated">隐马尔可夫模型中的混合</h2><p id="6647" class="pw-post-body-paragraph kf kg iq kh b ki mx jr kk kl my ju kn ko mz kq kr ks na ku kv kw nb ky kz la ij bi translated">我们将看到如何将离散马尔可夫模型“混合”到泊松回归模型中。</p><p id="256e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">考虑一个<em class="lb"> k </em>状态的马尔可夫过程，假设它处于某个状态<em class="lb"> j ϵ [1，2，3，…k] </em>。我们不知道马尔可夫过程在时间<em class="lb"> t </em>处于哪种状态。我们仅假设它以下列方式影响泊松过程模型:</p><figure class="nd ne nf ng gt nh gh gi paragraph-image"><div class="gh gi om"><img src="../Images/b5192bb0e4df6dd6a747b870425a1d12.png" data-original-src="https://miro.medium.com/v2/resize:fit:1346/format:webp/1*JoAqJkPy7mIyiaeDNAbAKA.png"/></div><p class="nj nk gj gh gi nl nm bd b be z dk translated">用预测均值<em class="nu"> μ_cap_t_j 和残差</em> ε_t 之和表示的观测值 y_t(图片由作者提供)</p></figure><p id="0038" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">注意，拟合的平均值<em class="lb"> μ_cap_t_j </em>现在用马尔可夫状态<em class="lb"> j 来索引。μ_cap_t_j </em>表示如下:</p><figure class="nd ne nf ng gt nh gh gi paragraph-image"><div class="gh gi on"><img src="../Images/e22ce8d35a1c8159a4175e44f226f829.png" data-original-src="https://miro.medium.com/v2/resize:fit:444/format:webp/1*rVJIwrA8_DSLjGPj1wAkMw.png"/></div><p class="nj nk gj gh gi nl nm bd b be z dk translated">当基础马尔可夫过程处于状态 j 时泊松 HMM 的指数均值(图片由作者提供)</p></figure><p id="5579" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">注意，我们现在使用的是与第<em class="lb">个第 j 个</em>马尔可夫状态相对应的马尔可夫状态特定回归系数向量<strong class="kh ir"> <em class="lb"> β_cap </em> </strong> <em class="lb"> _j </em>。</p><p id="1b68" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如果马尔可夫模型在'<em class="lb"> k' </em>状态<em class="lb">【1，2，…，j，…，k】</em>上运算，回归系数采用大小为<em class="lb"> [(m+1) X k] </em>的<strong class="kh ir">矩阵</strong>的形式，如下所示:</p><figure class="nd ne nf ng gt nh gh gi paragraph-image"><div class="gh gi oo"><img src="../Images/c7b86e446d17d4e993a5dd91f0d53cd2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1104/format:webp/1*alPGxGFgHPSX9flHvMMu4A.png"/></div><p class="nj nk gj gh gi nl nm bd b be z dk translated">大小为[k x (m+1)]的系数矩阵(图片由作者提供)</p></figure><p id="f5d6" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这里的直觉是，根据哪个马尔可夫状态或“体制”，回归模型系数将从<strong class="kh ir"> <em class="lb"> β_cap_s </em> </strong>切换到适当的体制特定的<em class="lb">向量</em><strong class="kh ir"><em class="lb">β_ cap</em></strong><em class="lb">_ j</em>。</p><p id="d904" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><em class="lb">k</em>-状态马尔可夫过程由以下状态转移矩阵<strong class="kh ir"> <em class="lb"> P </em> </strong>控制，其中每个元素<em class="lb"> p_ij </em>是在时间<em class="lb"> t </em>转移到<em class="lb"> j </em>的概率，假定该过程在时间<em class="lb"> (t-1) </em>处于状态<em class="lb"> i </em>:</p><figure class="nd ne nf ng gt nh gh gi paragraph-image"><div class="gh gi op"><img src="../Images/23fca6917d721a5b61a785194c67dc59.png" data-original-src="https://miro.medium.com/v2/resize:fit:794/format:webp/1*krPHER_cHxUuhHLkNtP6Qw.png"/></div><p class="nj nk gj gh gi nl nm bd b be z dk translated">马尔可夫过程的状态转移矩阵<strong class="bd no"> P </strong>(图片作者提供)</p></figure><p id="651d" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">马尔可夫过程在时间步<em class="lb"> t </em>也有如下状态概率分布<strong class="kh ir"><em class="lb">π_</em></strong><em class="lb">t</em>:</p><figure class="nd ne nf ng gt nh gh gi paragraph-image"><div role="button" tabindex="0" class="nq nr di ns bf nt"><div class="gh gi oq"><img src="../Images/fcc0534043ed39f56bfb4f4b8b648374.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dFtu0N1xTH2Qpebe9dxhxA.png"/></div></div><p class="nj nk gj gh gi nl nm bd b be z dk translated">k 状态马尔可夫过程的状态概率分布向量(图片由作者提供)</p></figure><pre class="nd ne nf ng gt or os ot ou aw ov bi"><span id="1e50" class="oa mg iq os b gy ow ox l oy oz">Quick tip: Some texts call the state vector <strong class="os ir"><em class="lb">δ</em></strong><em class="lb">_t</em> instead of <strong class="os ir"><em class="lb">π_</em></strong><em class="lb">t</em>, and they call the Markov state transition probabilities <em class="lb">γ_ij</em> instead of <em class="lb">p_ij</em>.</span></pre><p id="62c2" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">已知/假设<strong class="kh ir"> <em class="lb"> π_0 </em> </strong>在<em class="lb"> t=0 </em>时的某个值，我们计算<strong class="kh ir"><em class="lb">π_</em></strong><em class="lb">t</em>如下:</p><figure class="nd ne nf ng gt nh gh gi paragraph-image"><div class="gh gi pa"><img src="../Images/27f9adb07ad9ccb1d94074b24180caf3.png" data-original-src="https://miro.medium.com/v2/resize:fit:432/format:webp/0*cAR4MXwOT1AZKTut.png"/></div><p class="nj nk gj gh gi nl nm bd b be z dk translated">给定 t=0 时的概率分布和转移矩阵<strong class="bd no"> P </strong>(图片由作者提供)，马尔可夫过程在 t 时的状态概率分布公式</p></figure><p id="0843" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">让我们回到泊松平均值的州特定公式:</p><figure class="nd ne nf ng gt nh gh gi paragraph-image"><div class="gh gi on"><img src="../Images/e22ce8d35a1c8159a4175e44f226f829.png" data-original-src="https://miro.medium.com/v2/resize:fit:444/format:webp/1*rVJIwrA8_DSLjGPj1wAkMw.png"/></div><p class="nj nk gj gh gi nl nm bd b be z dk translated">当基础马尔可夫过程处于状态 j 时，泊松 HMM 在时间 t 的指数均值(图片由作者提供)</p></figure><p id="652a" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><em class="lb"> μ_cap_t_j </em>是假设基础马尔可夫过程处于状态<em class="lb"> j </em>时泊松回归模型在时间<em class="lb"> t </em>的预测均值。由于我们实际上不知道马尔可夫过程在时间<em class="lb"> t </em>时处于哪个状态，在每个时间步，对于一个<em class="lb"> k </em>状态的马尔可夫过程，我们必须使用<em class="lb"> k </em>这样的预测方法，如下所示:</p><figure class="nd ne nf ng gt nh gh gi paragraph-image"><div class="gh gi pb"><img src="../Images/1eab047a4497d4ec481fa2deeb4ccc1e.png" data-original-src="https://miro.medium.com/v2/resize:fit:790/format:webp/1*Q3D0xqK3JdfhKRj7ZxLR6Q.png"/></div><p class="nj nk gj gh gi nl nm bd b be z dk translated">k 个预测的向量意味着来自泊松回归模型，对应于马尔可夫过程可能处于的 k 个可能状态(图片由作者提供)</p></figure><p id="4b6d" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在每个时间步为<em class="lb"> y_t </em>生成<em class="lb"> k </em>预测是荒谬的。因此，我们使用<strong class="kh ir">预期</strong>的公式将这些<em class="lb"> k </em>预测合并成一个预测<em class="lb"> μ_cap_t </em>。诀窍在于认识到<strong class="kh ir"><em class="lb">μ_ cap</em></strong><em class="lb">_ t</em>是一个随机变量，有<em class="lb"> k </em>个可能值，每个值都与一个发生概率相关联。这个概率就是<em class="lb"> π_tj </em>这是潜在马尔可夫过程在时间<em class="lb"> t </em>处于状态<em class="lb"> j </em>的无条件概率。</p><p id="871d" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">因此:</p><p id="3183" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">泊松 HMM 的预测均值是所有可能马尔可夫状态的期望值，如下所示:</p><figure class="nd ne nf ng gt nh gh gi paragraph-image"><div class="gh gi pc"><img src="../Images/d97a5973648f69565d73f6239a353ba0.png" data-original-src="https://miro.medium.com/v2/resize:fit:950/format:webp/1*pBemqQz7S0Jir8P7-xGaFw.png"/></div><p class="nj nk gj gh gi nl nm bd b be z dk translated">用随机变量<strong class="bd no"><em class="nu">μ_ cap</em></strong><em class="nu">_ t(图片由作者提供)</em>的期望值表示的预测均值</p></figure><p id="3bde" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">概率<em class="lb"> π_tj </em>是<em class="lb">向量π_t. </em>在每个时间步<em class="lb"> t </em>的分量值，我们使用以下公式计算<em class="lb"> π_t </em>:</p><figure class="nd ne nf ng gt nh gh gi paragraph-image"><div class="gh gi pa"><img src="../Images/27f9adb07ad9ccb1d94074b24180caf3.png" data-original-src="https://miro.medium.com/v2/resize:fit:432/format:webp/0*cAR4MXwOT1AZKTut.png"/></div><p class="nj nk gj gh gi nl nm bd b be z dk translated">给定 t=0 时的概率分布和转移矩阵<strong class="bd no"> P </strong>(图片由作者提供)，马尔可夫过程在 t 时的状态概率分布公式</p></figure><h1 id="86d2" class="mf mg iq bd mh mi pd mk ml mm pe mo mp jw pf jx mr jz pg ka mt kc ph kd mv mw bi translated">训练和评估</h1><p id="7406" class="pw-post-body-paragraph kf kg iq kh b ki mx jr kk kl my ju kn ko mz kq kr ks na ku kv kw nb ky kz la ij bi translated">泊松隐马尔可夫模型的训练包括估计系数矩阵<strong class="kh ir"> <em class="lb"> β_cap_s </em> </strong>和马尔可夫转移概率矩阵<strong class="kh ir"> <em class="lb"> P </em> </strong>。估计过程通常是<a class="ae lc" href="https://en.wikipedia.org/wiki/Maximum_likelihood_estimation" rel="noopener ugc nofollow" target="_blank">最大似然估计</a> (MLE)或<a class="ae lc" href="https://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm" rel="noopener ugc nofollow" target="_blank">期望最大化</a>。</p><p id="9e1e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们将描述如何使用 MLE 来找到<strong class="kh ir"> <em class="lb"> P </em> </strong>和<strong class="kh ir"> <em class="lb"> β_cap_s </em> </strong>的最优值，这将<em class="lb">最大化观察整个训练数据集</em> <strong class="kh ir"> <em class="lb"> y </em> </strong>的联合概率密度。换句话说，我们希望最大化以下产品:</p><figure class="nd ne nf ng gt nh gh gi paragraph-image"><div class="gh gi pi"><img src="../Images/3baec6e0f15da44813f98d00cf4aa614.png" data-original-src="https://miro.medium.com/v2/resize:fit:1194/format:webp/1*teZaZEtwXujsiMBRc5nX1A.png"/></div><p class="nj nk gj gh gi nl nm bd b be z dk translated">观察数据集的可能性(图片由作者提供)</p></figure><p id="afb9" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在上面的乘积中，概率<em class="lb">P(</em><strong class="kh ir"><em class="lb">y</em></strong><em class="lb">= y _ t)</em>就是泊松过程的<strong class="kh ir"> P </strong>概率<strong class="kh ir">M</strong>ass<strong class="kh ir">F</strong>function(PMF):</p><figure class="nd ne nf ng gt nh gh gi paragraph-image"><div class="gh gi pj"><img src="../Images/ed56d398bf1652e086a110f1fa7a4689.png" data-original-src="https://miro.medium.com/v2/resize:fit:746/format:webp/1*v01cDoaseGQ0LYgTG9Vbhg.png"/></div><p class="nj nk gj gh gi nl nm bd b be z dk translated">泊松条件 PMF(作者图片)</p></figure><p id="a84f" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在给定拟合平均速率<em class="lb"> μ_cap_t 的情况下，L.H.S .上的概率被解读为在时间<em class="lb"> t </em>观察到<em class="lb"> y_t </em>的条件概率。μ_cap_t </em>是使用我们之前看到的期望值公式计算的所有可能状态的预测平均值的期望值。</p><p id="09b2" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">还有第二种方法来计算泊松 PMF。</p><p id="9680" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">让我们再来看看观测<em class="lb"> y_t </em>的预测泊松概率的公式，假设底层马尔可夫过程处于状态<em class="lb"> j </em>:</p><figure class="nd ne nf ng gt nh gh gi paragraph-image"><div class="gh gi pk"><img src="../Images/94d601ae0e3907eea7c07d32d8936027.png" data-original-src="https://miro.medium.com/v2/resize:fit:980/format:webp/1*PMJKE9143IS-1BGeuvJ-wA.png"/></div><p class="nj nk gj gh gi nl nm bd b be z dk translated">预测泊松概率观察<em class="nu"> y_t </em>，假设底层马尔可夫过程处于状态<em class="nu"> j </em>(图片由作者提供)</p></figure><p id="f164" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">其中:</p><figure class="nd ne nf ng gt nh gh gi paragraph-image"><div class="gh gi on"><img src="../Images/e22ce8d35a1c8159a4175e44f226f829.png" data-original-src="https://miro.medium.com/v2/resize:fit:444/format:webp/1*rVJIwrA8_DSLjGPj1wAkMw.png"/></div><p class="nj nk gj gh gi nl nm bd b be z dk translated">当基础马尔可夫过程处于状态 j 时，泊松 HMM 在时间 t 的指数均值(图片由作者提供)</p></figure><p id="38b0" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">马尔可夫状态特定线性组合表示如下:</p><figure class="nd ne nf ng gt nh gh gi paragraph-image"><div role="button" tabindex="0" class="nq nr di ns bf nt"><div class="gh gi pl"><img src="../Images/f8a47b27cc6463f6251cbade054ee63e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nhaxm91O5M5lh1YKXztzGA.png"/></div></div><p class="nj nk gj gh gi nl nm bd b be z dk translated">回归变量 vector <strong class="bd no"> x </strong> _t 和状态特定回归系数<strong class="bd no"><em class="nu">β_ cap</em></strong><em class="nu">_</em>j 的马尔可夫状态特定线性组合(图片由作者提供)</p></figure><p id="1e10" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">使用泊松 PMF，并假设一个<em class="lb"> k- </em>状态马尔可夫过程，在每个时间步 t，我们将得到观察<em class="lb"> y_t </em>的<em class="lb"> k </em>概率，每一个都以马尔可夫过程处于状态<em class="lb"> j. </em>为条件。以下向量捕获这些<em class="lb"> k </em>概率:</p><figure class="nd ne nf ng gt nh gh gi paragraph-image"><div role="button" tabindex="0" class="nq nr di ns bf nt"><div class="gh gi pm"><img src="../Images/acccaf8942a6c28d6462c737bdbfb061.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vSiH6n3l0uW92w-Mhh3kJA.png"/></div></div><p class="nj nk gj gh gi nl nm bd b be z dk translated"><em class="nu">观察到的 k 个</em>泊松概率<em class="nu"> y_t </em>，每一个都以马尔可夫过程在时间 t 处于状态<em class="nu"> j 为条件</em></p></figure><p id="7a03" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">和以前一样，对于每个时间步，我们希望将这些<em class="lb"> k </em>概率合并成一个概率。为此，我们求助于<a class="ae lc" href="https://en.wikipedia.org/wiki/Law_of_total_probability" rel="noopener ugc nofollow" target="_blank"> <strong class="kh ir">全概率</strong> </a>定律，该定律指出，如果事件 A 可以与事件 A1、事件 A2 或事件 A3 等一起发生，那么 A 的无条件概率可以表示如下:</p><figure class="nd ne nf ng gt nh gh gi paragraph-image"><div role="button" tabindex="0" class="nq nr di ns bf nt"><div class="gh gi pn"><img src="../Images/ef7ac19589fdc469593d2c7abb36e9ed.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-46ahmkJbvvTUDouppdMgg.png"/></div></div><p class="nj nk gj gh gi nl nm bd b be z dk translated">全概率定律(图片由作者提供)</p></figure><p id="54db" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">使用该定律，我们将计算泊松 HMM 预测的在时间<em class="lb"> t </em>观察到<em class="lb"> y_t </em>的概率如下:</p><figure class="nd ne nf ng gt nh gh gi paragraph-image"><div class="gh gi po"><img src="../Images/d8f1891d2007f7b5b823b6f23af3eae0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1228/format:webp/1*_rHXRg18MlKX4rp7ZFU3Cg.png"/></div><p class="nj nk gj gh gi nl nm bd b be z dk translated">k 状态马尔可夫模型影响下的<strong class="bd no"> <em class="nu"> y </em> </strong>的概率密度(图片由作者提供)</p></figure><p id="8468" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在上面的求和中，<em class="lb"> P(s_t=1)，P(s_t=2)，…等等。</em>是马尔可夫过程在时间<em class="lb"> t. </em>的状态概率，我们从前面的讨论中知道，这些只是简单的<em class="lb">π_ TJ</em>——向量的不同分量<em class="lb"> π_t. </em></p><p id="c6f4" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">实际上，我们已经想出了两种不同的方法来计算泊松概率<em class="lb">P(</em><strong class="kh ir"><em class="lb">y</em></strong><em class="lb">= y _ t)。</em></p><p id="973c" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">让我们回到可能性等式:</p><figure class="nd ne nf ng gt nh gh gi paragraph-image"><div class="gh gi pi"><img src="../Images/3baec6e0f15da44813f98d00cf4aa614.png" data-original-src="https://miro.medium.com/v2/resize:fit:1194/format:webp/1*teZaZEtwXujsiMBRc5nX1A.png"/></div><p class="nj nk gj gh gi nl nm bd b be z dk translated">观察数据集的可能性(图片由作者提供)</p></figure><p id="b612" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">最大化乘积的<strong class="kh ir">自然对数通常是迅速的，因为它有将乘积转换成和的好处，并且后者在微积分中更容易处理(我们很快就会知道为什么)。因此，我们将最大化下面的<strong class="kh ir"> <em class="lb">对数</em></strong>——由程式化的ℓ:表示的可能性</strong></p><figure class="nd ne nf ng gt nh gh gi paragraph-image"><div class="gh gi pp"><img src="../Images/b6d8d859314835e2fdf56d48fd9dca4c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1092/format:webp/1*8Nrj2NFyGBARgZVUdYvgFg.png"/></div><p class="nj nk gj gh gi nl nm bd b be z dk translated">观察数据集的对数似然性(图片由作者提供)</p></figure><p id="37a2" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">对数似然的最大化通过使用以下程序来完成:</p><ol class=""><li id="bfd0" class="pq pr iq kh b ki kj kl km ko ps ks pt kw pu la pv pw px py bi translated">我们将取对数似然 w . r . t .<strong class="kh ir"><em class="lb"/></strong>中每个转移<em class="lb">概率 p_ij </em>和系数矩阵<strong class="kh ir"><em class="lb">β_ cap _ s</em></strong><em class="lb"/>中每个状态特定系数<em class="lb"> β_cap_q_j </em>其中<em class="lb"> q </em>位于<em class="lb">【0，1，…，m】</em></li><li id="d198" class="pq pr iq kh b ki pz kl qa ko qb ks qc kw qd la pv pw px py bi translated">我们将把每个偏导数设为零，</li><li id="6fc6" class="pq pr iq kh b ki pz kl qa ko qb ks qc kw qd la pv pw px py bi translated">我们将使用诸如<a class="ae lc" href="https://en.wikipedia.org/wiki/Newton%27s_method" rel="noopener ugc nofollow" target="_blank"> Newton-Raphson </a>、<a class="ae lc" href="https://en.wikipedia.org/wiki/Nelder%E2%80%93Mead_method" rel="noopener ugc nofollow" target="_blank"> Nelder-Mead </a>或<a class="ae lc" href="https://en.wikipedia.org/wiki/Powell%27s_method" rel="noopener ugc nofollow" target="_blank"> Powell 等优化算法来求解与<strong class="kh ir"> <em class="lb"> β_cap_s </em> </strong>中的<em class="lb">(k+(m+1)* k】</em>个方程(实际上比这个数少得多)对应的<em class="lb"> k </em>马尔可夫转移概率、<em class="lb"> (m+1)*k </em>个系数</a></li></ol><p id="abb4" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">有一个小问题我们需要解决。在整个优化过程中，马尔可夫状态转移概率<em class="lb"> p_ij </em>需要位于<em class="lb">【0.0，1.0】</em>区间内，并且任意一行<strong class="kh ir"> <em class="lb"> P </em> </strong>的概率总和需要为 1.0。在方程形式中，这两个约束表示如下:</p><figure class="nd ne nf ng gt nh gh gi paragraph-image"><div class="gh gi qe"><img src="../Images/b3ce37dbabaed9fa603ccc0ce78b3ad4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1360/format:webp/1*iKDt-BxfTsTi5a9bsIsz_Q.png"/></div><p class="nj nk gj gh gi nl nm bd b be z dk translated">所有马尔可夫状态转移概率遵守的约束(图片由作者提供)</p></figure><p id="4062" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">当注意到如果马尔可夫过程在时间<em class="lb"> (t-1) </em>处于状态<em class="lb"> i </em>，那么在下一个时间步<em class="lb"> t </em>，它必须处于可用状态<em class="lb">【1，2，3，…，k】</em>之一时，1.0 约束的求和变得明显。</p><p id="99bf" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在优化期间，我们通过定义大小为<em class="lb"> (k x k) </em>的矩阵<strong class="kh ir"> <em class="lb"> Q </em> </strong>来处理这些约束，如下所示:</p><figure class="nd ne nf ng gt nh gh gi paragraph-image"><div class="gh gi qf"><img src="../Images/61483c90860166381c1fc2e3a973d8c6.png" data-original-src="https://miro.medium.com/v2/resize:fit:792/format:webp/1*ja2L9PHPO8wc0v--mp7DCw.png"/></div><p class="nj nk gj gh gi nl nm bd b be z dk translated">代理矩阵<strong class="bd no"> Q </strong>(图片作者提供)</p></figure><p id="4b4b" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir"> <em class="lb"> Q </em> </strong>充当<strong class="kh ir">的代理<em class="lb"> P </em> </strong>。<strong class="kh ir"> <em class="lb"> </em> </strong>我们不是优化<strong class="kh ir"> <em class="lb"> P </em> </strong>，而是通过允许<em class="lb"> q_ij </em>在-∞到+∞之间自由变化来优化<strong class="kh ir"><em class="lb"/></strong>。在每次优化迭代中，<strong class="kh ir">我们通过将局部优化的</strong><em class="lb"/><strong class="kh ir">值标准化到区间</strong><em class="lb">【0.0，1.0】</em>来获得 <em class="lb"> p_ij </em> <strong class="kh ir">，如下:</strong></p><figure class="nd ne nf ng gt nh gh gi paragraph-image"><div role="button" tabindex="0" class="nq nr di ns bf nt"><div class="gh gi qg"><img src="../Images/b328ee193be594d5388a67d51a8c3b23.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dkz5G7MbLf2lfdZkxSalsg.png"/></div></div><p class="nj nk gj gh gi nl nm bd b be z dk translated">标准化<strong class="bd no"> Q </strong>矩阵以获得<strong class="bd no"> P </strong>矩阵(图片由作者提供)</p></figure></div><div class="ab cl ly lz hu ma" role="separator"><span class="mb bw bk mc md me"/><span class="mb bw bk mc md me"/><span class="mb bw bk mc md"/></div><div class="ij ik il im in"><p id="8945" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><em class="lb">在我下周的文章中，我们将</em> <strong class="kh ir"> <em class="lb">使用 Python 和 statsmodels </em> </strong> <em class="lb">构建并训练一个泊松隐马尔可夫模型。所以下周请继续关注这个话题。快乐造型！</em></p></div><div class="ab cl ly lz hu ma" role="separator"><span class="mb bw bk mc md me"/><span class="mb bw bk mc md me"/><span class="mb bw bk mc md"/></div><div class="ij ik il im in"><h1 id="c661" class="mf mg iq bd mh mi mj mk ml mm mn mo mp jw mq jx mr jz ms ka mt kc mu kd mv mw bi translated">参考文献和版权</h1><h2 id="6fe7" class="oa mg iq bd mh ob oc dn ml od oe dp mp ko of og mr ks oh oi mt kw oj ok mv ol bi translated">书</h2><p id="bc52" class="pw-post-body-paragraph kf kg iq kh b ki mx jr kk kl my ju kn ko mz kq kr ks na ku kv kw nb ky kz la ij bi translated">卡梅伦和特里维迪出版社(2013 年)。<a class="ae lc" href="http://faculty.econ.ucdavis.edu/faculty/cameron/racd2/" rel="noopener ugc nofollow" target="_blank"> <em class="lb">计数数据的回归分析</em> </a>(第 2 版。，计量经济学会专论)。剑桥:剑桥大学出版社。doi:10.1017/CBO 9781139013901567</p><p id="27e7" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">詹姆斯·d·汉密尔顿，<a class="ae lc" href="https://press.princeton.edu/books/hardcover/9780691042893/time-series-analysis" rel="noopener ugc nofollow" target="_blank"> <em class="lb">时间序列分析</em> </a>，普林斯顿大学出版社，2020 年。ISBN: 0691218633</p><h2 id="be53" class="oa mg iq bd mh ob oc dn ml od oe dp mp ko of og mr ks oh oi mt kw oj ok mv ol bi translated">形象</h2><p id="1ebd" class="pw-post-body-paragraph kf kg iq kh b ki mx jr kk kl my ju kn ko mz kq kr ks na ku kv kw nb ky kz la ij bi translated">所有图片的版权归<a class="ae lc" href="https://www.linkedin.com/in/sachindate/" rel="noopener ugc nofollow" target="_blank"> Sachin Date </a>所有，由<a class="ae lc" href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener ugc nofollow" target="_blank"> CC-BY-NC-SA </a>所有，除非图片下面提到了不同的来源和版权。</p></div><div class="ab cl ly lz hu ma" role="separator"><span class="mb bw bk mc md me"/><span class="mb bw bk mc md me"/><span class="mb bw bk mc md"/></div><div class="ij ik il im in"><h1 id="77db" class="mf mg iq bd mh mi mj mk ml mm mn mo mp jw mq jx mr jz ms ka mt kc mu kd mv mw bi translated">相关阅读</h1><div class="ld le gp gr lf lg"><a rel="noopener follow" target="_blank" href="/a-beginners-guide-to-discrete-time-markov-chains-d5be17cf0e12"><div class="lh ab fo"><div class="li ab lj cl cj lk"><h2 class="bd ir gy z fp ll fr fs lm fu fw ip bi translated">离散时间马尔可夫链初学者指南</h2><div class="ln l"><h3 class="bd b gy z fp ll fr fs lm fu fw dk translated">以及如何使用 Python 模拟离散马尔可夫过程的教程</h3></div><div class="lo l"><p class="bd b dl z fp ll fr fs lm fu fw dk translated">towardsdatascience.com</p></div></div><div class="lp l"><div class="lq l lr ls lt lp lu lv lg"/></div></div></a></div><div class="ld le gp gr lf lg"><a rel="noopener follow" target="_blank" href="/a-math-lovers-guide-to-hidden-markov-models-ad718df9fde8"><div class="lh ab fo"><div class="li ab lj cl cj lk"><h2 class="bd ir gy z fp ll fr fs lm fu fw ip bi translated">数学爱好者的隐马尔可夫模型指南</h2><div class="ln l"><h3 class="bd b gy z fp ll fr fs lm fu fw dk translated">它们是如何工作的，以及它们为什么被“隐藏”起来。</h3></div><div class="lo l"><p class="bd b dl z fp ll fr fs lm fu fw dk translated">towardsdatascience.com</p></div></div><div class="lp l"><div class="lw l lr ls lt lp lu lv lg"/></div></div></a></div></div><div class="ab cl ly lz hu ma" role="separator"><span class="mb bw bk mc md me"/><span class="mb bw bk mc md me"/><span class="mb bw bk mc md"/></div><div class="ij ik il im in"><p id="b20a" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><em class="lb">感谢阅读！如果您喜欢这篇文章，请</em> <a class="ae lc" href="https://timeseriesreasoning.medium.com" rel="noopener"> <strong class="kh ir"> <em class="lb">关注我</em> </strong> </a> <em class="lb">获取关于回归和时间序列分析的技巧、操作方法和编程建议。</em></p></div></div>    
</body>
</html>