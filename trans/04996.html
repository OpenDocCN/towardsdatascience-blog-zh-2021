<html>
<head>
<title>Get started Spark with Databricks and PySpark</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用Databricks和PySpark开始使用Spark</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/get-started-spark-with-databricks-and-pyspark-72572179bd03?source=collection_archive---------6-----------------------#2021-05-02">https://towardsdatascience.com/get-started-spark-with-databricks-and-pyspark-72572179bd03?source=collection_archive---------6-----------------------#2021-05-02</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="e8ce" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">使用纯Python开始使用Spark和Databricks</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/51f9c5cb12c7725ea1b60dd46c51bc6f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PbxW0J2wS0PpUeVC3pAnzg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图片来自<a class="ae ky" href="https://unsplash.com/s/photos/spark" rel="noopener ugc nofollow" target="_blank">https://unsplash.com/s/photos/spark</a></p></figure><p id="c7e8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">一开始，编程大师创建了关系数据库和文件系统。但是单台机器上的文件系统变得有限且缓慢。数据黑暗就在数据库的表面。缩小地图的精神正在大数据的表面上酝酿。</p><p id="3ff3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">编程大师说，要有火花，于是就有了火花。</p><h2 id="cc5b" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated">已经有Hadoop了，为什么还要麻烦Spark</h2><p id="8a10" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">如果关系数据库是一个维护良好的数据花园；Hadoop是一个杂乱的数据森林，它可以无限增长。</p><p id="dc6f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">要将数据放入花园，数据需要被仔细清理并在那里结构化地生长。在Hadoop森林中，女士们、先生们，不要担心，这里的任何数据都可以，文本、数字，甚至音频和视频都没有数据大小和类型的限制。</p><p id="3ab4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">但是Hadoop还是有一些不足，Spark来解决。这里列出了我的4个主要区别。</p><ol class=""><li id="fe05" class="mt mu it lb b lc ld lf lg li mv lm mw lq mx lu my mz na nb bi translated">在Hadoop中，每一个映射和归约动作都是用磁盘存储作为数据中间人，磁盘操作比较慢。Spark通过利用内存直接数据访问优化流程。换句话说，在内存中存储一些中间数据来提高性能。(这就是为什么你总是阅读官方Spark介绍描绘自己比Hadoop快得多，这里没什么神奇的。)</li><li id="cb61" class="mt mu it lb b lc nc lf nd li ne lm nf lq ng lu my mz na nb bi translated">Hadoop基本上是一个分布式文件系统，可以通过它的map-reducer和批处理调度器扩展到无限大小。但是你需要用Java来实现真正的应用。Spark来提供Python之类的操作语言，r .为数据流、机器学习、数据分析提供有用的工具。</li><li id="7c6b" class="mt mu it lb b lc nc lf nd li ne lm nf lq ng lu my mz na nb bi translated">Hadoop不包含作业调度程序，需要第三方调度程序，Sparks自带作业调度程序。</li><li id="497b" class="mt mu it lb b lc nc lf nd li ne lm nf lq ng lu my mz na nb bi translated">Hadoop更便宜，需要的内存也更少。Spark需要更多内存。好吧，这个是Hadoop的优点而不是缺点。</li></ol><p id="2c72" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">有了PySpark，我们可以用纯Python代码、Jupyter Notebook或Databricks Notebook与Spark进行充分的交互。这是Spark的一大优势。</p><h2 id="cd43" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated">已经有火花了，为什么还要麻烦数据布里克</h2><p id="91f9" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">Spark是开源的、免费的、功能强大的，为什么还要使用Databricks呢？为了建立一个有用的Spark集群，并利用分布式存储，我们需要构建至少2台虚拟或物理机器。接下来，设置驱动和工作节点，配置网络和安全等。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nh"><img src="../Images/2e1e9508e15879d8f76efcd9fee17c9d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dAMroFg3ee_3f8BVZqD8ow.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">来自https://spark.apache.org/docs/latest/cluster-overview.html<a class="ae ky" href="https://spark.apache.org/docs/latest/cluster-overview.html" rel="noopener ugc nofollow" target="_blank">的火花元件</a></p></figure><p id="bb79" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">仅仅运行一个Spark“hello world”就要做更多的手工工作。如果得到JAVA_HOME找不到，或者找不到Spark path之类的错误信息就别提了。</p><p id="837e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">通过解决所有这些繁琐的配置，Databricks提供了一个开箱即用的环境。</p><p id="8736" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">以<a class="ae ky" href="https://docs.microsoft.com/en-us/azure/databricks/scenarios/quickstart-create-databricks-workspace-portal?tabs=azure-portal" rel="noopener ugc nofollow" target="_blank"> Azure Databricks </a>为例，在几次鼠标点击和几分钟等待集群启动之后。我们有一个功能齐全的火花系统。他们称之为数据砖。</p><p id="efc8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">与免费的Spark不同，Databricks通常根据集群大小和使用情况收费。小心，在创建第一个实例时选择正确的大小。</p><p id="9aa3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">还有一点需要注意，请记住你选择的Databricks运行时版本。我更愿意选择LTS 7.3。稍后，当您安装<code class="fe ni nj nk nl b">databricks-connect</code>时，版本应该是相同的。</p><h2 id="6bad" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated">开始连接</h2><p id="05a5" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">我准备用Python做所有的事情，那么我应该安装<code class="fe ni nj nk nl b">pyspark</code>包吗？不，要使用Python来控制数据块，我们需要首先卸载<code class="fe ni nj nk nl b">pyspark</code>包以避免冲突。</p><pre class="kj kk kl km gt nm nl nn no aw np bi"><span id="406b" class="lv lw it nl b gy nq nr l ns nt">pip uninstall pyspark</span></pre><p id="9a1d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">接下来，安装<code class="fe ni nj nk nl b">databricks-connect</code>。其中包括所有名称不同的PySpark函数。(确保您的本地机器上已经安装了Java 8+)</p><pre class="kj kk kl km gt nm nl nn no aw np bi"><span id="db4c" class="lv lw it nl b gy nq nr l ns nt">pip install -U "databricks-connect==7.3.*"</span></pre><p id="0271" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在配置到Databricks集群的客户端连接之前，请转到Databricks UI获取以下信息并记下。详细步骤可以在<a class="ae ky" href="https://docs.microsoft.com/en-us/azure/databricks/dev-tools/databricks-connect" rel="noopener ugc nofollow" target="_blank">这里</a>找到</p><ol class=""><li id="49f8" class="mt mu it lb b lc ld lf lg li mv lm mw lq mx lu my mz na nb bi translated">访问令牌:<code class="fe ni nj nk nl b">dapib0fxxxxxxxxx6d288bac04855bccccd</code></li><li id="5bd3" class="mt mu it lb b lc nc lf nd li ne lm nf lq ng lu my mz na nb bi translated">工作区网址:<code class="fe ni nj nk nl b"><a class="ae ky" href="https://adb-8091234370581234.18.azuredatabricks.net/" rel="noopener ugc nofollow" target="_blank">https://adb-8091234370581234.18.azuredatabricks.net/</a></code></li><li id="c5ad" class="mt mu it lb b lc nc lf nd li ne lm nf lq ng lu my mz na nb bi translated">集群id: <code class="fe ni nj nk nl b">1234-12345-abcdef123</code></li><li id="69d3" class="mt mu it lb b lc nc lf nd li ne lm nf lq ng lu my mz na nb bi translated">端口号:<code class="fe ni nj nk nl b">15001</code></li><li id="0ff4" class="mt mu it lb b lc nc lf nd li ne lm nf lq ng lu my mz na nb bi translated">组织id: <code class="fe ni nj nk nl b">8091234370581234</code>，组织id也出现在工作区url中。</li></ol><p id="ec65" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">当您准备好上述所有信息后，就可以配置到Databricks集群的本地PySpark连接了。</p><pre class="kj kk kl km gt nm nl nn no aw np bi"><span id="2211" class="lv lw it nl b gy nq nr l ns nt">databricks-connect configure</span></pre><p id="f8bb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">跟着向导走，你不会迷路的。之后，使用这个Python代码来测试连接。</p><pre class="kj kk kl km gt nm nl nn no aw np bi"><span id="91b4" class="lv lw it nl b gy nq nr l ns nt"># python <br/>from pyspark.sql import SparkSession<br/>spark = SparkSession.builder.getOrCreate()<br/>print('spark session created.')</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nu"><img src="../Images/0b5c8473068c833a1468642b7715f654.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*jebFqDfCz430jBSpJiNJGQ.gif"/></div></div></figure><p id="3faa" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果你受到“火花会议已创建”的欢迎，一个活蹦乱跳的火花簇正在云中运行。我们现在可以做一些大数据分析。</p><h2 id="c48b" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated">数据仓库:BDFS</h2><p id="a46f" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">Hadoop的HDFS允许用户在本地磁盘上构建可扩展的海量存储。BDFS几乎和HDFS一样。区别在于它的后端存储是基于云的。</p><p id="1e1b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">你可以用<code class="fe ni nj nk nl b">dbutils</code>用Python远程管理BDFS，</p><p id="12cb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在本地Python上下文中获取<code class="fe ni nj nk nl b">dbutils</code>对象处理程序。官方文件假设你使用的是Databricks笔记本，省略这一步。当用户试图在普通Python代码中使用它时会感到困惑。</p><pre class="kj kk kl km gt nm nl nn no aw np bi"><span id="b2bf" class="lv lw it nl b gy nq nr l ns nt">from pyspark.dbutils import DBUtils<br/>dbutils = DBUtils(spark) # the spark object here <br/>                         # is already initialized above</span></pre><p id="ad69" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">列出<code class="fe ni nj nk nl b">/mnt/</code>文件夹中的文件和文件夹</p><pre class="kj kk kl km gt nm nl nn no aw np bi"><span id="7f63" class="lv lw it nl b gy nq nr l ns nt">dbutils.fs.ls('dbfs:/mnt/')</span></pre><p id="e652" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">你会得到这样的信息:</p><pre class="kj kk kl km gt nm nl nn no aw np bi"><span id="e4ee" class="lv lw it nl b gy nq nr l ns nt">[FileInfo(path='dbfs:/mnt/folder1/', name='folder1/', size=123),<br/> FileInfo(path='dbfs:/mnt/folder2/', name='folder2/', size=123),<br/> FileInfo(path='dbfs:/mnt/tmp/', name='tmp/', size=123)]</span></pre><p id="7eb0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><a class="ae ky" href="https://docs.databricks.com/dev-tools/databricks-utils.html" rel="noopener ugc nofollow" target="_blank"> dbutils官方文件</a>列出了所有其他操作。</p><h2 id="b3ef" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated">用Python上传一个CSV文件到DBFS</h2><p id="02f9" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">在本地磁盘上准备一个圣经CSV文件。用您的用户名替换[用户名]以运行下面的代码。</p><pre class="kj kk kl km gt nm nl nn no aw np bi"><span id="8519" class="lv lw it nl b gy nq nr l ns nt">import urllib.request<br/>bible_url = "https://raw.githubusercontent.com/scrollmapper/bible_databases/master/csv/t_kjv.csv"<br/>urllib.request.urlretrieve(bible_url,"/home/[username]/temp/bible_kjv.csv")</span></pre><p id="c26c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在，上传圣经CSV文件到BDFS。</p><pre class="kj kk kl km gt nm nl nn no aw np bi"><span id="b058" class="lv lw it nl b gy nq nr l ns nt">bible_csv_path = "file:/home/[username]/temp/bible_kjv.csv"<br/>dbutils.fs.cp(bible_csv_path,"/tmp/bible_kjv.csv")</span></pre><p id="baaa" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果您决定移动文件而不是复制，请使用<code class="fe ni nj nk nl b">mv</code>代替<code class="fe ni nj nk nl b">cp</code>。</p><h2 id="21e1" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated">使用Spark Dataframe分析数据</h2><p id="03c1" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">读取刚刚上传的圣经CSV文件，并将其封装在Spark数据帧中(与熊猫数据帧形成对比)。</p><pre class="kj kk kl km gt nm nl nn no aw np bi"><span id="92d9" class="lv lw it nl b gy nq nr l ns nt">bible_spark_df = spark.read.format('csv')\<br/>                 .options(header='true')\<br/>                 .load('/tmp/bible_kjv.csv')<br/>bible_spark_df.show()</span></pre><p id="a35c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">你会看到结果的</p><pre class="kj kk kl km gt nm nl nn no aw np bi"><span id="ae34" class="lv lw it nl b gy nq nr l ns nt">+-------+---+---+---+--------------------+<br/>|     id|  b|  c|  v|                   t|<br/>+-------+---+---+---+--------------------+<br/>|1001001|  1|  1|  1|In the beginning ...|<br/>|1001002|  1|  1|  2|And the earth was...|<br/>|1001003|  1|  1|  3|And God said, Let...|<br/>...<br/>|1001019|  1|  1| 19|And the evening a...|<br/>|1001020|  1|  1| 20|And God said, Let...|<br/>+-------+---+---+---+--------------------+<br/>only showing top 20 rows</span></pre><p id="badf" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果你是熊猫数据框的粉丝，很容易将数据转换成熊猫数据框</p><pre class="kj kk kl km gt nm nl nn no aw np bi"><span id="115a" class="lv lw it nl b gy nq nr l ns nt">bible_pandas_df = bible_spark_df.toPandas()</span></pre><p id="95d8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们用Spark Dataframe看看每本书有多少节。</p><pre class="kj kk kl km gt nm nl nn no aw np bi"><span id="9b13" class="lv lw it nl b gy nq nr l ns nt">bible_spark_df.groupBy("b")\<br/>              .count()\<br/>              .sort("count",ascending=False)\<br/>              .show()</span></pre><p id="b190" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">你会看到结果的</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nv"><img src="../Images/7ecca2f9ff8c3f3d71fb25881c1f5a47.png" data-original-src="https://miro.medium.com/v2/resize:fit:356/format:webp/1*XvHN8PUZtalZe9wZGeSXdA.png"/></div></figure><p id="de81" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">第一卷是创世纪，这本书包含1533节经文。</p><p id="7017" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在创建临时视图的帮助下，我们还可以使用Spark SQL查询数据</p><pre class="kj kk kl km gt nm nl nn no aw np bi"><span id="0476" class="lv lw it nl b gy nq nr l ns nt">bible_spark_df.createOrReplaceTempView('bible')<br/>bible_sql_result = spark.sql('''<br/>    select * from bible <br/>    where id == 1001001<br/>''')<br/>bible_sql_result.show()</span></pre><p id="9e17" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">查询结果</p><pre class="kj kk kl km gt nm nl nn no aw np bi"><span id="30db" class="lv lw it nl b gy nq nr l ns nt">+-------+---+---+---+--------------------+<br/>|     id|  b|  c|  v|                   t|<br/>+-------+---+---+---+--------------------+<br/>|1001001|  1|  1|  1|In the beginning ...|<br/>+-------+---+---+---+--------------------+</span></pre><p id="bd8f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">将Spark Dataframe作为JSON文件保存回BDFS。</p><pre class="kj kk kl km gt nm nl nn no aw np bi"><span id="b18b" class="lv lw it nl b gy nq nr l ns nt">bible_spark_df.write.format('json').save('/tmp/bible_kjv.json')</span></pre><p id="e207" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对于所有Spark数据集操作，请查看<a class="ae ky" href="https://spark.apache.org/docs/2.2.0/sql-programming-guide.html" rel="noopener ugc nofollow" target="_blank">Spark SQL、数据帧和数据集指南</a></p><h2 id="cd34" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated">Spark数据库和表格</h2><p id="3dc0" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">Spark还支持Hive数据库和表，在上面的示例中，我创建了一个临时视图来启用SQL查询。但是当会话结束时，临时视图将会消失。启用在Hive表中存储数据，并且可以使用Spark SQL进行长期查询。我们可以在配置单元表中存储数据。</p><p id="f947" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">首先，创建一个Hive数据库</p><pre class="kj kk kl km gt nm nl nn no aw np bi"><span id="335c" class="lv lw it nl b gy nq nr l ns nt">spark.sql("create database test_hive_db")</span></pre><p id="3d10" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">接下来，将圣经火花数据帧写成表格。这里的数据库名称有点像表文件夹。</p><pre class="kj kk kl km gt nm nl nn no aw np bi"><span id="35d2" class="lv lw it nl b gy nq nr l ns nt">bible_spark_df.write.saveAsTable('<strong class="nl iu">test_hive_db</strong>.bible_kjv')</span></pre><p id="5816" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">有关Spark配置单元表操作的所有信息，请查看<a class="ae ky" href="https://spark.apache.org/docs/latest/sql-data-sources-hive-tables.html" rel="noopener ugc nofollow" target="_blank">配置单元表</a></p><h2 id="0a50" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated">总结和概括</h2><p id="a6ce" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">祝贺并感谢你阅读到这里。当我开始学习Spark和Databricks时，当书籍作者试图用复杂的图表介绍Spark后端架构时，我陷入了困境。我写这篇文章是为了那些从来没有接触过Spark的人，想在不被弄糊涂的情况下弄脏自己的手。</p><p id="32eb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果您成功地运行了所有代码，那么您应该可以开始使用Spark和Databricks了。Spark和Databricks只是工具不应该那么复杂，能比Python复杂吗？(开玩笑)</p><p id="2b5b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">还有一点要说明的是，默认的Databricks入门教程使用Databricks Notebook，很好很好看。但是在真实的项目和工作中，您可能希望用普通的Python编写代码，并在git存储库中管理您的工作。我发现带有<a class="ae ky" href="https://marketplace.visualstudio.com/items?itemName=ms-python.python" rel="noopener ugc nofollow" target="_blank"> Python </a>和<a class="ae ky" href="https://marketplace.visualstudio.com/items?itemName=paiqo.databricks-vscode" rel="noopener ugc nofollow" target="_blank"> Databricks </a>扩展的Visual Studio代码是一个很棒的工具，完全支持Databricks和Spark。</p></div></div>    
</body>
</html>