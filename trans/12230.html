<html>
<head>
<title>What Does Transformer Self-Attention Actually Look At?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">变形金刚自我关注其实看的是什么？</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/what-does-transformer-self-attention-actually-look-at-5318df114ac0?source=collection_archive---------6-----------------------#2021-12-12">https://towardsdatascience.com/what-does-transformer-self-attention-actually-look-at-5318df114ac0?source=collection_archive---------6-----------------------#2021-12-12</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="939e" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">深入探究伯特的注意力</h2></div><p id="5c23" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">背景:变形金刚、自我关注、伯特</strong></p><p id="bcc8" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在过去的几年里，自然语言理解的进展已经<a class="ae lb" href="https://www.forbes.com/sites/louisaxu/2021/12/01/a-golden-age-for-natural-language/" rel="noopener ugc nofollow" target="_blank">爆炸</a>，这主要归功于被称为<a class="ae lb" href="https://jalammar.github.io/illustrated-transformer/" rel="noopener ugc nofollow" target="_blank">转换器</a>的新型神经网络架构。Transformer模型的主要架构创新是广泛使用所谓的<a class="ae lb" rel="noopener" target="_blank" href="/illustrated-self-attention-2d627e33b20a">、【自我关注】、</a>，以至于介绍该模型的论文被命名为<a class="ae lb" href="https://arxiv.org/abs/1706.03762" rel="noopener ugc nofollow" target="_blank">、</a>。自我注意机制将每个输入编码为所有其他输入的函数，帮助算法化语言中上下文的直观概念。</p><figure class="ld le lf lg gt lh gh gi paragraph-image"><div class="gh gi lc"><img src="../Images/fa5b4329c3b0b7439a6f3fbf0bfdae39.png" data-original-src="https://miro.medium.com/v2/resize:fit:1374/0*5H6xAEoDFlvVnMAA.gif"/></div><p class="lk ll gj gh gi lm ln bd b be z dk translated">使用GitHub Copilot 实例化一个复杂的对象，只需要查看它的类签名和前面的样板文件。Copilot是通过在Github的开源代码上训练一个变压器模型而创建的。(<a class="ae lb" rel="noopener" target="_blank" href="/tips-for-using-github-copilot-in-everyday-python-programming-8ef9e91a9b47">GitHub copilot能为数据科学家做些什么？</a>)</p></figure><p id="24fb" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">自2017年推出以来，变压器架构已经分为多个子家族，最著名的是因果解码器种类(<a class="ae lb" href="https://openai.com/blog/better-language-models/" rel="noopener ugc nofollow" target="_blank"> GPT-2 </a>、<a class="ae lb" href="https://arxiv.org/abs/2005.14165" rel="noopener ugc nofollow" target="_blank"> GPT-3 </a>)，它们被训练来预测序列中的下一个字，以及编码器种类(<a class="ae lb" href="https://arxiv.org/abs/1810.04805" rel="noopener ugc nofollow" target="_blank">伯特</a>、<a class="ae lb" href="https://arxiv.org/abs/1910.10683" rel="noopener ugc nofollow" target="_blank"> T5 </a>、<a class="ae lb" href="https://blog.google/products/search/introducing-mum/" rel="noopener ugc nofollow" target="_blank">μm</a>)，它们被训练来在序列中的任意位置填空。每一种都有自己的优点和缺点，但是为了本文的目的，我们将把重点放在各种编码器上，尤其是BERT。</p><p id="8572" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">伯特建筑进修</strong></p><p id="1c20" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">BERT 被设计成一个非常通用的语言编码器模型，它可以用于许多不同类型的任务，而不需要改变它的架构。它通过以单词标记*的一般形式接收输入来实现这一点，在开始处有一个特殊的[CLS]标记，在每段文本之后有一个特殊的[SEP]标记。(*技术上讲<a class="ae lb" rel="noopener" target="_blank" href="/wordpiece-subword-based-tokenization-algorithm-1fbd14394ed7">文字块</a>令牌)</p><p id="1f63" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">然后，该输入序列被转换为向量嵌入，向量嵌入通过自关注机制(稍后将详细介绍)相对于彼此被重复重新编码，随后的每次重新编码都保持相同的序列长度。</p><p id="ebdc" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">通过这种方式，任何接收多个输入的任务，比如一段文本+一个关于文本的问题，都可以自然地作为[SEP]划分的标记序列传入。类似地，任何期望进行分类的任务，例如“输入是消极情绪还是积极情绪？”可以通过最后一层的[CLS]令牌嵌入自然输出。</p><figure class="ld le lf lg gt lh gh gi paragraph-image"><div role="button" tabindex="0" class="lp lq di lr bf ls"><div class="gh gi lo"><img src="../Images/26ee00a477361502e4cf581007890b1a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fUMpqBZb1RyykmUfajNJ6g.png"/></div></div><p class="lk ll gj gh gi lm ln bd b be z dk translated">伯特如何消化输入“男孩骑着马”+“很有趣”的图示。输入令牌被转换成嵌入，然后由每一层重复重新编码。特殊[CLS]令牌的最终嵌入可以用于执行分类。(图片由作者提供)</p></figure><p id="d59a" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">对于基本BERT模型，有12层，每层包含12个注意头，总共有144个注意头。注意力操作有些复杂(详细的演练见<a class="ae lb" rel="noopener" target="_blank" href="/illustrated-self-attention-2d627e33b20a">插图:自我注意力</a>)，但重要的是要知道，对于每个注意力头:</p><ol class=""><li id="7c62" class="lt lu iq kh b ki kj kl km ko lv ks lw kw lx la ly lz ma mb bi translated">每个输入被赋予三个向量:一个键、一个查询和值</li><li id="ec54" class="lt lu iq kh b ki mc kl md ko me ks mf kw mg la ly lz ma mb bi translated">为了确定我应该“关注”输入j多少，我们取输入I的查询向量与输入j的关键向量的点积，重新调整它，并通过一个sigmoid传递它。</li><li id="204c" class="lt lu iq kh b ki mc kl md ko me ks mf kw mg la ly lz ma mb bi translated">然后，我们使用这个结果注意力分数来加权输入j的值向量。</li></ol><p id="3392" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">好了，介绍完了，我们终于可以进入有趣的部分了。</p><p id="a303" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">伯特的注意力(不要)看什么</strong></p><p id="ea10" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">通常，当你听到对伯特注意力的高层次描述时，接下来是“这使得注意力头能够自己学习经典的NLP关系！”接下来是这样一幅挑衅性的图形:</p><figure class="ld le lf lg gt lh gh gi paragraph-image"><div role="button" tabindex="0" class="lp lq di lr bf ls"><div class="gh gi mh"><img src="../Images/15eb49e3d0cf72f829944c46cca9727f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*K0Mz3v5eqt9pBa1RV5qyXw.png"/></div></div><p class="lk ll gj gh gi lm ln bd b be z dk translated">BERT的第5层注意力头之一执行<a class="ae lb" href="https://en.wikipedia.org/wiki/Coreference" rel="noopener ugc nofollow" target="_blank">共指解析</a>。“它”最受其相关名词短语“狗”的关注。(图片由作者提供，使用<a class="ae lb" href="https://github.com/jessevig/bertviz" rel="noopener ugc nofollow" target="_blank"> BertViz </a>生成)</p></figure><p id="2f5f" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">虽然<em class="mi">一些</em>注意力集中的人确实学会了表现这样美好的可解释的关系，但大多数人没有。<a class="ae lb" href="https://arxiv.org/abs/1908.08593" rel="noopener ugc nofollow" target="_blank"> Kovaleva等人(2019) </a>将BERT的注意力焦点分为5种类型:</p><ol class=""><li id="9ff2" class="lt lu iq kh b ki kj kl km ko lv ks lw kw lx la ly lz ma mb bi translated"><em class="mi">垂直</em>:所有记号都强烈关注同一个其它记号，通常是[SEP]记号。</li><li id="6249" class="lt lu iq kh b ki mc kl md ko me ks mf kw mg la ly lz ma mb bi translated"><em class="mi">对角线</em>:所有的记号要么强烈地关注它们自己，要么关注一个具有恒定偏移量的记号，例如紧随它们自己之后的记号。</li><li id="95a9" class="lt lu iq kh b ki mc kl md ko me ks mf kw mg la ly lz ma mb bi translated"><em class="mi">垂直+对角线</em>:前两种图案的组合。</li><li id="8a52" class="lt lu iq kh b ki mc kl md ko me ks mf kw mg la ly lz ma mb bi translated"><em class="mi">块</em>:令牌强烈关注其[SEP]划分的块内的其他令牌，而不关注该块之外的任何令牌。</li><li id="7c14" class="lt lu iq kh b ki mc kl md ko me ks mf kw mg la ly lz ma mb bi translated"><em class="mi">异质</em>:更复杂、不明显的图案。</li></ol><figure class="ld le lf lg gt lh gh gi paragraph-image"><div role="button" tabindex="0" class="lp lq di lr bf ls"><div class="gh gi mj"><img src="../Images/f9e519c7b1265ae24ef9bc504dd82f40.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*8-9MhCHOU5low2Zb"/></div></div><p class="lk ll gj gh gi lm ln bd b be z dk translated">每个网格代表特定输入的特定注意头的行为。网格中的位置(I，j)表示表征I对表征j的注意力强度。此处显示的是展示5种特征注意力模式的输入。(<a class="ae lb" href="https://arxiv.org/abs/1908.08593" rel="noopener ugc nofollow" target="_blank">科瓦列娃等人，2019 </a>)</p></figure><p id="91e1" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">有趣的是，当你看到这些注意力模式有多普遍时，你会发现异质+阻断模式，也就是唯一能做任何有趣事情的模式，只解释了注意力头的一半<em class="mi">T21】的典型行为。更奇怪的是，在所有情况下都盯着同一个标记的垂直模式，占据了<strong class="kh ir"> <em class="mi">注意力头部的三分之一。</em></strong></em></p><figure class="ld le lf lg gt lh gh gi paragraph-image"><div role="button" tabindex="0" class="lp lq di lr bf ls"><div class="gh gi mj"><img src="../Images/2304c9d19138e06bff2cca95e444d569.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*G8BCUjW4wbNFIj69"/></div></div><p class="lk ll gj gh gi lm ln bd b be z dk translated">产生每种注意模式的输入部分。每个条形代表一组不同的输入，所有这些输入都是来自<a class="ae lb" href="https://gluebenchmark.com/" rel="noopener ugc nofollow" target="_blank"> GLUE </a>数据集的任务。(<a class="ae lb" href="https://arxiv.org/abs/1908.08593" rel="noopener ugc nofollow" target="_blank">科瓦列娃等人，2019 </a>)</p></figure><p id="df51" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">当您深入研究垂直模式时，您会发现大多数单个标记都是[CLS]、[SEP]和标点符号。那么，究竟为什么像伯特这么聪明的模特会花费这么多宝贵的注意力资源在这些毫无意义的符号上呢？</p><p id="ec8e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">关于这里发生的事情的理论是，当一个注意力头盯着这些停止标记之一时，它的行为就像一个不操作。因此，如果一个特定的注意力头所调谐的任何语言结构在输入中不存在，这就允许它“关闭”。</p><p id="fcc5" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><a class="ae lb" href="https://arxiv.org/abs/2004.10102" rel="noopener ugc nofollow" target="_blank"> Kobayashi et al. (2020) </a>进一步挖掘这一奇怪的发现，发现虽然这些代币上的注意力得分很高，但正在与注意力得分相乘的值向量的范数很低。如此之低，以至于最终产品接近于零。</p><figure class="ld le lf lg gt lh gh gi paragraph-image"><div role="button" tabindex="0" class="lp lq di lr bf ls"><div class="gh gi mj"><img src="../Images/5438b51fbe9cc2b3fb801359027baf46.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*s_TXwVmpg0ZJdMTE"/></div></div><p class="lk ll gj gh gi lm ln bd b be z dk translated">每一层中给予不同标记类型的注意力权重。左:天真的注意力得分，右:由价值向量的范数加权的注意力得分。(<a class="ae lb" href="https://arxiv.org/abs/2004.10102" rel="noopener ugc nofollow" target="_blank">小林等人，2020 </a></p></figure><p id="8711" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">真正有趣的是，因为有太多的这些注意力头基本上什么都不做，所以实际上你可以通过移除某些注意力头来提高模型的性能！事实上，对于像<a class="ae lb" href="https://paperswithcode.com/dataset/mrpc" rel="noopener ugc nofollow" target="_blank">【MRPC】</a>(确定两个句子是否等价)和<a class="ae lb" href="https://paperswithcode.com/dataset/rte" rel="noopener ugc nofollow" target="_blank"> RTE </a>(确定一个句子是否暗示另一个句子)这样的任务，随意去掉一个头<strong class="kh ir"><em class="mi"/></strong>更有可能帮助表现而不是伤害它。</p><figure class="ld le lf lg gt lh gh gi paragraph-image"><div role="button" tabindex="0" class="lp lq di lr bf ls"><div class="gh gi mj"><img src="../Images/06fe2eb869980a6399edbb080d60738f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*WDOglm6zowP35Er9"/></div></div><p class="lk ll gj gh gi lm ln bd b be z dk translated">禁用不同注意头时的任务绩效。橙色线表示未改变的BERT模型的准确性。y轴上显示图层编号，x轴上显示头部编号。左:MRPC任务的性能，右:RTE任务的性能。(<a class="ae lb" href="https://arxiv.org/abs/1908.08593" rel="noopener ugc nofollow" target="_blank"> Kovaleva等人，2019 </a>)</p></figure><p id="6298" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">有用的注意力模式呢？</strong></p><p id="4284" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">回到第一部分的图:的确，在BERT中有一些<em class="mi">注意力头，它们似乎被调整来执行可识别的NLP子任务。那么，这些头是什么，它们能做什么？</em></p><p id="2abd" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">发现一些注意头编码与<a class="ae lb" href="https://web.stanford.edu/~jurafsky/slp3/14.pdf" rel="noopener ugc nofollow" target="_blank">依存解析</a>树中特定边相同的关系。特别地，对于给定输入(忽略[CLS]和[SEP])的最强注意力分数始终被给予具有T20特定依赖关系的成对单词。</p><figure class="ld le lf lg gt lh gh gi paragraph-image"><div role="button" tabindex="0" class="lp lq di lr bf ls"><div class="gh gi mj"><img src="../Images/141ff697a6d78840a031b429e957b510.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*H8FiyJkM5G8kWyrL"/></div></div><p class="lk ll gj gh gi lm ln bd b be z dk translated">BERT heads编码特定的依赖关系。左:<a class="ae lb" href="https://universaldependencies.org/u/dep/obj.html" rel="noopener ugc nofollow" target="_blank">直接宾语</a>关系，中:<a class="ae lb" href="https://universaldependencies.org/u/dep/det.html" rel="noopener ugc nofollow" target="_blank">限定词</a>关系，右:<a class="ae lb" href="https://universaldependencies.org/u/dep/nmod-poss.html" rel="noopener ugc nofollow" target="_blank">所有格修饰语</a>关系。(<a class="ae lb" href="https://arxiv.org/abs/1906.04341" rel="noopener ugc nofollow" target="_blank">克拉克等人，2020 </a>)</p></figure><p id="b40a" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">许多NLP系统的另一个复杂而重要的任务是<a class="ae lb" href="https://nlp.stanford.edu/projects/coref.shtml" rel="noopener ugc nofollow" target="_blank">共指消解</a>。这就是确定一个句子中的两个词何时指代同一个实体的问题。要理解为什么这是一个如此困难的问题，考虑一下句子“莎莉给莎拉一颗曼妥思，因为她的口气不好。”在这种情况下,“她”指的是“莎拉”,但做出这一决定需要知道曼妥思是用来减轻口臭的，而不是某种道歉礼物。</p><p id="2c9d" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这种对世界知识的依赖使得这成为一项非常重要的任务，然而Clark等人发现，第5层中的Head #4以65%的准确度正确地识别(即，最强烈地关注)共同参照，相比之下，选择最近的提及作为共同参照的系统只有27%的准确度。</p><p id="6365" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们可以走的另一条路是观察所有注意力头总共学到了什么。还记得前面的依赖关系解析树吗？我们可以问的一个问题是:对于一个给定的词对，如果我们考虑由网络中所有注意力头的<em class="mi">产生的注意力分数，我们能在依存分析中计算出它们是否应该由一条边连接吗？</em></p><figure class="ld le lf lg gt lh gh gi paragraph-image"><div role="button" tabindex="0" class="lp lq di lr bf ls"><div class="gh gi mj"><img src="../Images/b3171aa6040474910ebecf7118e3dec6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*rZxzAP0zJP0pXBDE"/></div></div><p class="lk ll gj gh gi lm ln bd b be z dk translated">通过聚合所有144个注意力头的注意力分数，为给定的词对创建全网络的注意力向量(<a class="ae lb" href="https://arxiv.org/abs/1906.02715" rel="noopener ugc nofollow" target="_blank"> Coenen等人，2019 </a>)</p></figure><p id="9e05" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">事实证明，答案是肯定的，以这种方式构建的分类器可以以85.8%的准确率预测依存解析中的边缘，远高于chance ( <a class="ae lb" href="https://arxiv.org/abs/1906.02715" rel="noopener ugc nofollow" target="_blank"> Coenen et al .，2019 </a>)。虽然正如Rogers et al. (2020) 所指出的，当在这样的高维表示之上构建分类器时，并不总是清楚有多少知识包含在底层表示中，而有多少知识是由分类器注入的。</p><p id="ffed" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">最后的想法</strong></p><p id="1a26" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">关于这个主题有许多研究，我还没有时间触及，例如不同的架构决策在确定BERT的能力方面有什么作用(<a class="ae lb" href="https://arxiv.org/abs/2108.04378" rel="noopener ugc nofollow" target="_blank">Ontaón等人，2021 </a>)，在BERT的不同层内正在进行什么样的推理(<a class="ae lb" href="https://arxiv.org/abs/1905.05950" rel="noopener ugc nofollow" target="_blank"> Tenney等人，2019 </a>)，以及注意力头部下游的单词嵌入编码是什么(<a class="ae lb" href="https://nlp.stanford.edu/pubs/hewitt2019structural.pdf" rel="noopener ugc nofollow" target="_blank"> Hewitt &amp; Manning，2019 </a>)。</p><p id="ac85" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">许多头条论文都是关于这些巨大的神经网络模型所拥有的突破性能力，但这个领域最让我感兴趣的是这些模型是如何在引擎盖下工作的。我认为，特别是在这种情况下，我们发现备受吹捧的注意力机制花了大量时间来寻找无用的标记，我们意识到我们对这些模型的理解是多么贫乏。我认为这种神秘的气氛让他们更酷。</p><p id="45db" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">作品引用</strong></p><p id="15f1" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">[1] J. Alammar，插图变压器(2018)，GitHub博客</p><p id="7c36" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">[2] R. Karim，插图:自我关注(2019)，走向数据科学</p><p id="ac71" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">[3] J. Vig，解构BERT，第2部分:可视化注意力的内部工作方式(2019)，走向数据科学</p><p id="0f4b" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">[4] A .科尔提，GitHub copilot能为数据科学家做些什么？(2021)，走向数据科学</p><p id="d716" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">[5] J. Devlin，M. Chang，K. Lee和K. Toutanova，BERT:用于语言理解的深度双向转换器的预训练(2019)，ArXiv.org</p><p id="7be9" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">[6] A. Rogers，O. Kovaleva和A. Rumshisky,《伯特学入门:我们知道伯特如何工作》( 2020年), ArXiv.org</p><p id="ec89" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">[7]K·克拉克、u .汉德尔瓦尔、o .利维和C. D .曼宁,《伯特看什么》?伯特的注意力分析(2019)，ArXiv.org</p><p id="892d" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">[8] O .科瓦列娃、a .罗马诺夫、a .罗杰斯和a .拉姆什基，《揭露伯特的黑暗秘密》(2019)，ArXiv.org</p><p id="05c4" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">[9] G. Kobayashi、T. Kuribayashi、S. Yokoi和K. Inui,《注意力不仅仅是重量:用向量范数分析变形金刚》( 2020年)，ArXiv.org</p><p id="7348" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">[10] A. Coenen，E. Reif，A. Yuan，B. Kim，A. Pearce，F. Viégas和M. Wattenberg，可视化和测量BERT的几何形状(2019)，ArXiv.org</p><p id="ffd5" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">[11]s . Ontaón、J. Ainslie、V. Cvicek和Z. Fisher，《让变形金刚解决作曲任务》(2021)，ArXiv.org</p><p id="5b1e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">[12] I. Tenney，D. Das，E. Pavlick，Bert重新发现经典的NLP管道(2019)，ArXiv.org</p><p id="397f" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">[13] J. Vig，变压器模型中注意力的多尺度可视化(2019)，ACL选集</p><p id="b0b9" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">[14] C. D .曼宁、K·克拉克、j .休伊特、u .汉德尔瓦尔和o .利维，《通过自我监督训练的人工神经网络中的自然语言结构》(2020年)，《美国国家科学院院刊》</p><p id="32b1" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">[15] J. Hewitt和C. D Manning，在词表示中寻找句法的结构性探索(2019)，计算语言学协会</p><p id="62d1" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">[16] M. Marneffe、B. MacCartney和C. D. Manning，从短语结构语法分析生成类型化依赖语法分析(2006)，Lrec</p><p id="0029" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">[17] D. Jurafsky和J. H. Martin，语音和语言处理，第二版(2014年)，培生教育</p><p id="63b1" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">[18]普遍依赖关系，UniversalDependencies.org</p></div></div>    
</body>
</html>