<html>
<head>
<title>Gentle introduction to 2D Hand Pose Estimation: Let’s Code It!</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">温柔介绍2D手姿势估计:我们来编码吧！</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/gentle-introduction-to-2d-hand-pose-estimation-lets-code-it-6c82046d4acf?source=collection_archive---------17-----------------------#2021-04-30">https://towardsdatascience.com/gentle-introduction-to-2d-hand-pose-estimation-lets-code-it-6c82046d4acf?source=collection_archive---------17-----------------------#2021-04-30</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="5057" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">了解如何在PyTorch中训练2D手部姿态估计器。本教程也可能是您对PyTorch的介绍。</h2></div><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi kg"><img src="../Images/dbaa8819dfe30c6fa4149ec0bcd3ebf1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*crWOgPUxhHfhuONCcSCLyg.png"/></div></div><p class="ks kt gj gh gi ku kv bd b be z dk translated">作者图片</p></figure><p id="8895" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">欢迎回来！</p><p id="8c2c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们继续我们的手姿态估计之旅。现在你正在读第二部分，是关于编码和PyTorch的。我强烈建议您在深入研究编码之前阅读第一部分:</p><div class="ls lt gp gr lu lv"><a rel="noopener follow" target="_blank" href="/gentle-introduction-to-2d-hand-pose-estimation-approach-explained-4348d6d79b11"><div class="lw ab fo"><div class="lx ab ly cl cj lz"><h2 class="bd ir gy z fp ma fr fs mb fu fw ip bi translated">温和介绍2D手姿态估计:方法解释</h2><div class="mc l"><h3 class="bd b gy z fp ma fr fs mb fu fw dk translated">关于在哪里找到数据集，如何预处理数据，使用什么模型架构和损失的详细教程，以及…</h3></div><div class="md l"><p class="bd b dl z fp ma fr fs mb fu fw dk translated">towardsdatascience.com</p></div></div><div class="me l"><div class="mf l mg mh mi me mj kq lv"/></div></div></a></div><p id="aefa" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这将对你理解数据集、预处理、建模、训练和评估有很大帮助。</p><p id="29e0" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">对于本教程，我已经创建了一个<a class="ae mk" href="https://github.com/OlgaChernytska/2D-Hand-Pose-Estimation-RGB" rel="noopener ugc nofollow" target="_blank"> Github库</a>，在那里你可以找到关于训练手部姿态估计器和进行推理的完整代码。你可以现在或以后去那里——克隆它，阅读它并运行它。它是在PyTorch中实现的，如果您以前没有使用过这个库，这是一个很好的开始机会！这里不需要PyTorch的经验；我将解释所有的主要概念，所以本教程也可以作为你对PyTorch的介绍。</p><p id="bdee" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在…打开你们的Jupyter笔记本！</p><p id="4743" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">内容</strong> <br/>数据<br/>让我们训练<br/> —数据加载器<br/> —模型<br/>—训练器<br/>让我们做推理<br/> —后期处理<br/> —评估<br/>接下来</p><h1 id="bae5" class="ml mm iq bd mn mo mp mq mr ms mt mu mv jw mw jx mx jz my ka mz kc na kd nb nc bi translated">数据</h1><p id="a6ed" class="pw-post-body-paragraph kw kx iq ky b kz nd jr lb lc ne ju le lf nf lh li lj ng ll lm ln nh lp lq lr ij bi translated">先说数据。我们将使用FreiHAND数据集，您可以<a class="ae mk" href="https://lmb.informatik.uni-freiburg.de/resources/datasets/FreihandDataset.en.html" rel="noopener ugc nofollow" target="_blank">在此</a>下载。花些时间阅读数据集描述、探索归档结构以及打开图像和文件将会非常有用。请熟悉我们将在本教程中使用的数据。</p><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi ni"><img src="../Images/3ed1936ebbbaafd26a3aa1be7caecf0f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*e5VorGz3Gy9Cme86GnjKgw.png"/></div></div><p class="ks kt gj gh gi ku kv bd b be z dk translated"><em class="kf">图片1。FreiHAND数据集文件夹结构。绿色表示本教程所需的文件。作者图片</em></p></figure><p id="8277" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在，我们只需要一个包含RGB图像和2D标签的文件夹。通过使用相机矩阵将3D标签投影到图像平面上来计算2D标签。这是我在FreiHAND数据集<a class="ae mk" href="https://github.com/lmb-freiburg/freihand" rel="noopener ugc nofollow" target="_blank"> Github repo </a>中找到的一个公式:</p><pre class="kh ki kj kk gt nj nk nl nm aw nn bi"><span id="f8f4" class="no mm iq nk b gy np nq l nr ns">def projectPoints(xyz, K):     <br/>    xyz = np.array(xyz)     <br/>    K = np.array(K)     <br/>    uv = np.matmul(K, xyz.T).T     <br/>    return uv[:, :2] / uv[:, -1:]</span></pre><p id="8a55" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们将只使用前32，560张图像，即原始图像。数据集中的其他图像与原始图像完全相同，但有背景增强，所以我们现在跳过它们。</p><p id="f761" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">数据集分割。</strong> FreiHAND数据集已经看起来像一组混洗的手图像，因此我们可以按图像id进行分割。让前80%成为训练部分，接下来的15% —验证，最后的5% —测试。训练图像主要用于训练、验证(控制验证损失并决定何时停止模型训练)和测试(进行最终模型评估)。</p><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi nt"><img src="../Images/3a13c84fa35fcf8ea65ad738ecc3167f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Pdbxd_8hZDe8g873CTNkww.png"/></div></div><p class="ks kt gj gh gi ku kv bd b be z dk translated"><em class="kf">形象二。数据集分为训练、验证和测试部分。作者图片</em></p></figure><h1 id="9281" class="ml mm iq bd mn mo mp mq mr ms mt mu mv jw mw jx mx jz my ka mz kc na kd nb nc bi translated">让我们训练吧</h1><p id="5a9e" class="pw-post-body-paragraph kw kx iq ky b kz nd jr lb lc ne ju le lf nf lh li lj ng ll lm ln nh lp lq lr ij bi translated">为了训练一个模型，我们需要:</p><ul class=""><li id="60f0" class="nu nv iq ky b kz la lc ld lf nw lj nx ln ny lr nz oa ob oc bi translated"><strong class="ky ir">数据加载器类。</strong> FreiHAND数据集(因为大多数图像数据集)太大，无法完全存储在RAM中，因此我们将使用批量训练来训练模型。因此，需要DataLoader，它遍历数据集，一次只加载一批数据。</li><li id="e000" class="nu nv iq ky b kz od lc oe lf of lj og ln oh lr nz oa ob oc bi translated"><strong class="ky ir">模特类。</strong>我们将创建自己的定制UNet类模型。</li><li id="fbe5" class="nu nv iq ky b kz od lc oe lf of lj og ln oh lr nz oa ob oc bi translated"><strong class="ky ir">培训师类。</strong>这个类完成所有的训练:请求批量数据，获得模型预测，计算损失，更新模型权重，评估模型，并在验证损失停止减少时完成训练。</li></ul><p id="9b1e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们现在将详细讨论所有这些类。并查看一个<a class="ae mk" href="https://github.com/OlgaChernytska/2D-Hand-Pose-Estimation-RGB/blob/972c2102d95e14ebb37b1cbd452018ebd6706a44/notebooks/Train%20Notebook.ipynb" rel="noopener ugc nofollow" target="_blank">笔记本</a>有完整的培训流程。</p><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi oi"><img src="../Images/61adb94382038159c74bde2a9564efd4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZyLqqGgZNFizUZ0r8ko62Q.png"/></div></div><p class="ks kt gj gh gi ku kv bd b be z dk translated"><em class="kf">图三。培训渠道和所需PyTorch课程。作者图片</em></p></figure><h2 id="1760" class="no mm iq bd mn oj ok dn mr ol om dp mv lf on oo mx lj op oq mz ln or os nb ot bi translated">数据加载器</h2><p id="cc2b" class="pw-post-body-paragraph kw kx iq ky b kz nd jr lb lc ne ju le lf nf lh li lj ng ll lm ln nh lp lq lr ij bi translated">我们将使用两个PyTorch类来加载数据。</p><p id="77e1" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><a class="ae mk" href="https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader" rel="noopener ugc nofollow" target="_blank"> <strong class="ky ir">数据加载器</strong> </a> <strong class="ky ir">。</strong>这个类是在PyTorch中实现的，所以你只要调用它，提供Dataset类的一个实例和一些其他参数(检查它们的含义<a class="ae mk" href="https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader" rel="noopener ugc nofollow" target="_blank">这里</a>):</p><pre class="kh ki kj kk gt nj nk nl nm aw nn bi"><span id="acf9" class="no mm iq nk b gy np nq l nr ns">train_dataloader = DataLoader( <br/>    dataset=train_dataset, <br/>    batch_size=48, <br/>    shuffle=True, <br/>    drop_last=True, <br/>    num_workers=2 <br/>)</span></pre><p id="9570" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在我们可以像这样循环数据加载器:</p><pre class="kh ki kj kk gt nj nk nl nm aw nn bi"><span id="dbab" class="no mm iq nk b gy np nq l nr ns">for data_batch in train_dataloader: <br/>    # do something</span></pre><p id="4e88" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><a class="ae mk" href="https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset" rel="noopener ugc nofollow" target="_blank"> <strong class="ky ir">数据集</strong> </a> <strong class="ky ir">。PyTorch有一个已实现数据集的小列表，你可以在那里找到你需要的。但是，请做好准备—在大多数情况下，您将编写自己的数据集类。今天就是这一天。</strong></p><p id="4300" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">当您遵循以下规则时，数据集类不难编写:</p><ul class=""><li id="8265" class="nu nv iq ky b kz la lc ld lf nw lj nx ln ny lr nz oa ob oc bi translated">你的数据集类继承(子类)<a class="ae mk" href="https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset" rel="noopener ugc nofollow" target="_blank"><em class="ou">torch . utils . data . Dataset</em></a></li><li id="abdb" class="nu nv iq ky b kz od lc oe lf of lj og ln oh lr nz oa ob oc bi translated">你需要重写函数<em class="ou"> __len__() </em>，它返回数据集的长度。您可以在这里输入带标签文件的长度，或者文件夹中图像的数量。</li><li id="3354" class="nu nv iq ky b kz od lc oe lf of lj og ln oh lr nz oa ob oc bi translated">并且您需要重写函数<em class="ou"> __getitem__() </em>，该函数获取样本id并返回一个包含图像及其标签的列表或字典。稍后，<em class="ou"> __getitem__() </em>的输出会被DataLoader批量堆栈。</li></ul><p id="60ef" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">因此，FreiHAND数据集的数据集类应该如下所示。完整版在<a class="ae mk" href="https://github.com/OlgaChernytska/2D-Hand-Pose-Estimation-RGB/blob/main/utils/dataset.py" rel="noopener ugc nofollow" target="_blank"> Github </a>上。</p><pre class="kh ki kj kk gt nj nk nl nm aw nn bi"><span id="5a8b" class="no mm iq nk b gy np nq l nr ns">from torch.utils.data import Dataset<br/><br/>class FreiHAND(Dataset):<br/>    def __init__(self, config, set_type="train"):<br/>        ## initialize path to image folders<br/>	## initialize paths to files with labels<br/>	## create train/test/val split<br/>	## define data augmentations<br/>         <br/>    def __len__(self):<br/>	return len(self.anno)<br/><br/>    def __getitem__(self, idx):<br/>        ## load image by id, use PIL librabry<br/>        ## load its labels<br/>        ## do augmentations if needed<br/>        ## convert everything into PyTorch Tensors<br/><br/>        return {<br/>            "image": image,<br/>            "keypoints": keypoints,<br/>            "heatmaps": heatmaps,<br/>            "image_name": image_name,<br/>            "image_raw": image_raw,<br/>        }</span></pre><p id="8823" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我个人倾向于将所有图像信息添加到<em class="ou"> __getitem__() </em>输出中——原始图像、调整大小和标准化的图像、图像名称、数字形式的关键点以及热图形式的关键点。当我调试、绘制图像或评估模型准确性时，它大大简化了我的生活。</p><p id="01b4" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">不要忘记使用<a class="ae mk" href="https://github.com/OlgaChernytska/2D-Hand-Pose-Estimation-RGB/blob/972c2102d95e14ebb37b1cbd452018ebd6706a44/utils/prep_utils.py#L37" rel="noopener ugc nofollow" target="_blank">函数</a>计算R、G、B通道平均值和标准偏差(就在数据集和数据加载器初始化之前)。然后将值添加到Dataset类中的<em class="ou"> Normalize() </em>转换中；它发生在<em class="ou"> Resize() </em>变换之前或之后——查看<a class="ae mk" href="https://github.com/OlgaChernytska/2D-Hand-Pose-Estimation-RGB/blob/972c2102d95e14ebb37b1cbd452018ebd6706a44/utils/dataset.py#L59" rel="noopener ugc nofollow" target="_blank">这里</a>如何做到这一点。只有现在—初始化数据集和数据加载器。</p><p id="1bf1" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">总之，将有3个数据集类实例(train_dataset、val_dataset和test_dataset)和3个数据加载器类实例(train_dataloader、val_dataloder和test_dataloader)。这是因为训练集、验证集和测试集是完全不同的图像集。</p><p id="f639" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">但是，您不需要编写3个不同的数据集类。一个就够了。创建数据集实例时只需提供<em class="ou"> set_type </em>参数，如下所示:</p><pre class="kh ki kj kk gt nj nk nl nm aw nn bi"><span id="4970" class="no mm iq nk b gy np nq l nr ns">train_dataset = FreiHAND(config=config, set_type="train") <br/>train_dataloader = DataLoader( <br/>    dataset=train_dataset, <br/>    batch_size=config["batch_size"], <br/>    shuffle=True, <br/>    drop_last=True, <br/>    num_workers=2 <br/>)</span></pre><p id="6bd2" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">确保在数据集类中有一个代码片段，将图像分成训练、验证和测试部分。</p><h2 id="2643" class="no mm iq bd mn oj ok dn mr ol om dp mv lf on oo mx lj op oq mz ln or os nb ot bi translated">模型</h2><p id="9887" class="pw-post-body-paragraph kw kx iq ky b kz nd jr lb lc ne ju le lf nf lh li lj ng ll lm ln nh lp lq lr ij bi translated">PyTorch中的<a class="ae mk" href="https://pytorch.org/vision/stable/models.html" rel="noopener ugc nofollow" target="_blank">实现和预训练模型</a>的列表非常庞大——有用于图像分类、视频分类、语义和实例分割、对象检测的模型，甚至还有一个用于<a class="ae mk" href="https://pytorch.org/vision/stable/models.html#keypoint-r-cnn" rel="noopener ugc nofollow" target="_blank">人体关键点检测的模型</a>。受过预先训练。是不是很酷？！</p><p id="bd23" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">但是出于研究的目的，我们将实现我们自己的类似UNet的架构。这一个:</p><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi ov"><img src="../Images/56623a50bf8ebe2058133b7d3c997b7b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ovqDOUIAhpJ4hNlMHa8gqg.png"/></div></div><p class="ks kt gj gh gi ku kv bd b be z dk translated"><em class="kf">形象4。我的自定义UNet-like模型2D手姿态估计。作者图片</em></p></figure><p id="382c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">PyTorch中的所有定制模型都应该子类化<a class="ae mk" href="https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" rel="noopener ugc nofollow" target="_blank"> <em class="ou"> torch.nn.Module </em> </a>并重写函数<em class="ou"> forward() </em>。UNet不是典型的前馈网络，它具有跳跃连接。因此，在向前传递的过程中，应该保存一些层的输出，稍后再与更深层的输出连接。这里没有问题，你可以写任何类型的向前传球，PyTorch会理解如何自己向后传球。</p><p id="cc47" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">因此，我们的定制UNet模型应该是这样的。而这里有一个<a class="ae mk" href="https://github.com/OlgaChernytska/2D-Hand-Pose-Estimation-RGB/blob/main/utils/model.py" rel="noopener ugc nofollow" target="_blank">完整版</a>。</p><pre class="kh ki kj kk gt nj nk nl nm aw nn bi"><span id="2efe" class="no mm iq nk b gy np nq l nr ns">class ShallowUNet(nn.Module):     <br/>    def __init__(self, in_channel, out_channel): <br/>        super().__init__()  <br/>        # initialize layer - custom or from PyTorch list</span><span id="a9b8" class="no mm iq nk b gy ow nq l nr ns">    def forward(self, x):  <br/>        # implement forward pass  <br/>        # you can do literally anything here  <br/>        return out</span></pre><p id="205d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">顺便说一下，如果你的模型有一些重复的块，你可以将块实现为模块，就像模型一样——通过子类化<em class="ou"> torch.nn.Module </em>并重写函数<em class="ou"> forward() </em>。看，我是怎么用UNet中的双卷积块做到的。</p><pre class="kh ki kj kk gt nj nk nl nm aw nn bi"><span id="32c6" class="no mm iq nk b gy np nq l nr ns">class ConvBlock(nn.Module):     <br/>    def __init__(self, in_depth, out_depth):<br/>        super().__init__()<br/>        self.double_conv = nn.Sequential(<br/>            nn.BatchNorm2d(in_depth), <br/>            nn.Conv2d(in_depth, out_depth, kernel_size=3, padding=1, bias=False),<br/>            nn.ReLU(inplace=True), <br/>            nn.BatchNorm2d(out_depth), <br/>            nn.Conv2d(out_depth, out_depth, kernel_size=3, padding=1, bias=False), <br/>            nn.ReLU(inplace=True), <br/>        ) </span><span id="3880" class="no mm iq nk b gy ow nq l nr ns">     def forward(self, x): <br/>        return self.double_conv(x)</span></pre><h2 id="fdc7" class="no mm iq bd mn oj ok dn mr ol om dp mv lf on oo mx lj op oq mz ln or os nb ot bi translated">运动鞋</h2><p id="47a7" class="pw-post-body-paragraph kw kx iq ky b kz nd jr lb lc ne ju le lf nf lh li lj ng ll lm ln nh lp lq lr ij bi translated">训练师类是PyTorch根本没有的，所以你需要从头开始写，使用来自<a class="ae mk" href="https://pytorch.org/tutorials/" rel="noopener ugc nofollow" target="_blank"> PyTorch教程</a>的代码片段。</p><p id="cedd" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">有时这让我感到害怕，但有时我看到了好处:它让我更好地理解训练过程中发生的事情，并让我完全控制训练。</p><p id="54ea" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这是训练师的代号。我不打算在这篇文章中展示训练器代码，因为确切的代码没有数据集和模型类那么重要。顺便说一下，你甚至不需要把培训师写成一个职业，这是我个人的偏好。你可以使用函数，或者只是把你所有的训练代码放在一个Jupyter笔记本单元中——由你决定。</p><p id="e054" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">但是，在编写培训代码时，需要记住一些事情:</p><ul class=""><li id="b594" class="nu nv iq ky b kz la lc ld lf nw lj nx ln ny lr nz oa ob oc bi translated">每个时期应该具有训练和评估阶段。在训练之前，您需要显式地将模型置于训练模式，这样做— <em class="ou"> model.train() </em>。评测也是一样——<em class="ou">model . eval()</em>。这是因为一些层在训练和评估/推断期间可能表现不同，例如Dropout和BatchNorm。</li><li id="eaaf" class="nu nv iq ky b kz od lc oe lf of lj og ln oh lr nz oa ob oc bi translated">在评估过程中，您可以额外使用<em class="ou"> torch.no_grad() </em>功能。它告诉你的模型“现在不要计算梯度”，所以向前传递会更快，使用更少的内存。</li><li id="015a" class="nu nv iq ky b kz od lc oe lf of lj og ln oh lr nz oa ob oc bi translated">您不必在训练(或验证)期间遍历所有训练(或验证)数据集。好主意是限制每个时期的训练和验证批次的数量。</li><li id="1b76" class="nu nv iq ky b kz od lc oe lf of lj og ln oh lr nz oa ob oc bi translated">对于那些对GPU培训感兴趣的人。您的数据和模型应该在完全相同的设备上。默认情况下，torch张量和模型被初始化在CPU上，因此如果GPU可用，您应该显式地将它们转移到GPU。下面是如何做到这一点的代码片段:</li></ul><pre class="kh ki kj kk gt nj nk nl nm aw nn bi"><span id="d8f2" class="no mm iq nk b gy np nq l nr ns">device = torch.device("cuda" if torch.cuda.is_available() else "cpu") <br/>model = model.to(device) <br/>inputs = data["image"].to(device) <br/>labels = data["heatmaps"].to(device)</span></pre><p id="b5d1" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">是的，标签也一样，否则你将无法计算损失，因为模型输出是在GPU上。</p><p id="40c5" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在GPU上训练模型并在CPU上运行预测是没问题的，反之亦然。仅在加载模型权重时这样做:</p><pre class="kh ki kj kk gt nj nk nl nm aw nn bi"><span id="fa3a" class="no mm iq nk b gy np nq l nr ns">device = torch.device("cuda" if torch.cuda.is_available() else "cpu") <br/>model.load_state_dict(torch.load(model_path, map_location=device)))</span></pre><p id="5faa" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">损失。</strong> PyTorch已经实现了<a class="ae mk" href="https://pytorch.org/docs/stable/nn.html#loss-functions" rel="noopener ugc nofollow" target="_blank">标准损耗</a>，但是，如果你想使用自定义损耗，那就是你需要自己编写的。自定义损耗的实现方式与模型相同——通过子类化<em class="ou"> torch.nn.Module </em>和重写函数<em class="ou"> forward() </em>。下面是一个简短的模板，完整版本你可以在这里找到<a class="ae mk" href="https://github.com/OlgaChernytska/2D-Hand-Pose-Estimation-RGB/blob/972c2102d95e14ebb37b1cbd452018ebd6706a44/utils/prep_utils.py#L88" rel="noopener ugc nofollow" target="_blank"/>。</p><pre class="kh ki kj kk gt nj nk nl nm aw nn bi"><span id="a01f" class="no mm iq nk b gy np nq l nr ns">class IoULoss(nn.Module): <br/>    def __init__(self): <br/>        super(IoULoss, self).__init__() <br/>        #initialize parameters </span><span id="8ef0" class="no mm iq nk b gy ow nq l nr ns">    def forward(self, y_pred, y_true): <br/>        #calculate loss from labels and predicitons <br/>        return loss</span></pre><p id="b0aa" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">关于训练就是这样。如果你还没做过，我真的推荐你去翻翻<a class="ae mk" href="https://github.com/OlgaChernytska/2D-Hand-Pose-Estimation-RGB/blob/main/notebooks/Train%20Notebook.ipynb" rel="noopener ugc nofollow" target="_blank"> Train Notebook.ipynb </a>。</p><p id="1ee6" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">如果您希望自己重新训练模型，这应该是本教程前一部分的一个有用的文本片段:</p><p id="13b1" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><em class="ou">“对于本教程，我用batch_size=48和batches_per_epoch=50对模型进行了训练。我从学习率=0.1开始，每次训练损失停止减少时，就减少一半。当验证集上的损失停止下降时，我完成了培训。收敛花了大约200个历元(在GPU上花了2个小时)，我最终的训练和验证损失分别是0.437和0.476。</em></p><p id="5826" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这些数字只是地标。在训练您的模型时，您可能会得出不同数量的要收敛的历元和略有不同的最终损失。此外，随意增加/减少批量大小以适应您的机器内存，并增加/减少学习率。"</p><p id="8829" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">祝你好运！</p><h1 id="8714" class="ml mm iq bd mn mo mp mq mr ms mt mu mv jw mw jx mx jz my ka mz kc na kd nb nc bi translated">让我们做推论</h1><p id="7821" class="pw-post-body-paragraph kw kx iq ky b kz nd jr lb lc ne ju le lf nf lh li lj ng ll lm ln nh lp lq lr ij bi translated">推理管道比训练管道简单得多。我们只需要:</p><ul class=""><li id="c399" class="nu nv iq ky b kz la lc ld lf nw lj nx ln ny lr nz oa ob oc bi translated">加载训练好的模型；</li><li id="42c4" class="nu nv iq ky b kz od lc oe lf of lj og ln oh lr nz oa ob oc bi translated">为测试集创建dataloader的实例；</li><li id="e47d" class="nu nv iq ky b kz od lc oe lf of lj og ln oh lr nz oa ob oc bi translated">编写将热图转换为关键点位置矢量的后处理器；</li><li id="48d4" class="nu nv iq ky b kz od lc oe lf of lj og ln oh lr nz oa ob oc bi translated">并且，理想情况下，计算测试集的预测误差，以评估模型性能。</li></ul><p id="1db8" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">完整代码请参考<a class="ae mk" href="https://github.com/OlgaChernytska/2D-Hand-Pose-Estimation-RGB/blob/main/notebooks/Inference%20Notebook.ipynb" rel="noopener ugc nofollow" target="_blank">推理笔记本. ipynb </a>。</p><p id="52a5" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在您理解培训部分之后，加载模型、创建测试数据加载器和运行预测应该是容易的任务。但是在后处理和评估方面，我们现在将更加关注。</p><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi ox"><img src="../Images/4080282cd1062c6df423e6754650f095.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eDUPKeoNucB3GQ7DXU2vNA.png"/></div></div><p class="ks kt gj gh gi ku kv bd b be z dk translated"><em class="kf">形象4。详细的推理管道。作者图片</em></p></figure><h2 id="b431" class="no mm iq bd mn oj ok dn mr ol om dp mv lf on oo mx lj op oq mz ln or os nb ot bi translated">后处理</h2><p id="bf3c" class="pw-post-body-paragraph kw kx iq ky b kz nd jr lb lc ne ju le lf nf lh li lj ng ll lm ln nh lp lq lr ij bi translated">通过在所有热图值中求平均值来计算关键点位置更好。而这里是<a class="ae mk" href="https://github.com/OlgaChernytska/2D-Hand-Pose-Estimation-RGB/blob/972c2102d95e14ebb37b1cbd452018ebd6706a44/utils/prep_utils.py#L114" rel="noopener ugc nofollow" target="_blank">代码</a>。</p><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi oy"><img src="../Images/dc11f1145e9b68436d5ab099772a26e6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1d4R7Dv-W4zyY1aWp2Vi5w.png"/></div></div><p class="ks kt gj gh gi ku kv bd b be z dk translated"><em class="kf">图片5。如何通过平均计算热图中的关键点位置？作者图片</em></p></figure><h2 id="053e" class="no mm iq bd mn oj ok dn mr ol om dp mv lf on oo mx lj op oq mz ln or os nb ot bi translated">估价</h2><p id="a542" class="pw-post-body-paragraph kw kx iq ky b kz nd jr lb lc ne ju le lf nf lh li lj ng ll lm ln nh lp lq lr ij bi translated">在测试集上计算平均预测误差是必须做的事情。是的，您可以更进一步，报告最大和最小图像误差，显示百分位数，通过手指和关节计算误差，并可视化具有最大和最小误差的图像。但是现在，让我们保持简单。</p><p id="96f9" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">以下是我的手部姿态估计器在测试集上的评估结果:</p><ul class=""><li id="d7c7" class="nu nv iq ky b kz la lc ld lf nw lj nx ln ny lr nz oa ob oc bi translated">每个关键点的平均误差:图像尺寸的4.5%</li><li id="a6a2" class="nu nv iq ky b kz od lc oe lf of lj og ln oh lr nz oa ob oc bi translated">每个关键点的平均误差:图像128×128为6个像素</li><li id="e9c2" class="nu nv iq ky b kz od lc oe lf of lj og ln oh lr nz oa ob oc bi translated">每个关键点的平均误差:图像224×224为10个像素</li></ul><p id="13d4" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">每个关键点的平均误差意味着1)图像中的所有关键点和2)数据集中的所有图像之间的平均误差。它可以报告为图像大小的百分比，或者原始图像或调整大小的图像的像素误差。这里的关键点误差是图像平面上实际和预测关键点位置之间的欧几里德距离。</p><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi oz"><img src="../Images/3552b7324387b73858eeaefb8b411837.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ynd8M1k6n721PuOIWMGvXA.png"/></div></div><p class="ks kt gj gh gi ku kv bd b be z dk translated"><em class="kf">图片6。一些关键点预测误差的可视化。作者图片</em></p></figure><h1 id="bdf0" class="ml mm iq bd mn mo mp mq mr ms mt mu mv jw mw jx mx jz my ka mz kc na kd nb nc bi translated">下一步是什么</h1><p id="f33b" class="pw-post-body-paragraph kw kx iq ky b kz nd jr lb lc ne ju le lf nf lh li lj ng ll lm ln nh lp lq lr ij bi translated">希望现在，在阅读完整教程并浏览代码后，2D手姿态估计对你来说不再是一个复杂的任务。</p><p id="a226" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">然而，我们今天训练的手部姿态估计器还远远不能用于生产。它不适用于关键点被遮挡的姿态，并且不适用于来自不同数据集的图像。有很多需要改进的地方。</p><p id="a7e4" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">出于学习目的，从简单的东西开始，慢慢地，一步一步地将更高级的技术引入算法是一个好主意。也许以后我会写关于这些技术的教程。有兴趣就告诉我: )</p></div><div class="ab cl pa pb hu pc" role="separator"><span class="pd bw bk pe pf pg"/><span class="pd bw bk pe pf pg"/><span class="pd bw bk pe pf"/></div><div class="ij ik il im in"><p id="d781" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><em class="ou">原载于2021年4月30日</em><a class="ae mk" href="https://notrocketscience.blog/" rel="noopener ugc nofollow" target="_blank"><em class="ou">https://notrocketseconomy . blog</em></a><em class="ou">。</em></p><p id="94c5" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><em class="ou">如果你想阅读更多类似的教程，请订阅我的博客“不是火箭科学”——</em><a class="ae mk" href="https://t.me/notrocketscienceblog" rel="noopener ugc nofollow" target="_blank"><em class="ou">电报</em> </a> <em class="ou">和</em> <a class="ae mk" href="https://twitter.com/nRocketScience" rel="noopener ugc nofollow" target="_blank"> <em class="ou">推特</em> </a> <em class="ou">。</em></p></div></div>    
</body>
</html>