<html>
<head>
<title>Does BERT Need Clean Data? Part 2: Classification.</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">BERT需要干净的数据吗？第二部分:分类。</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/does-bert-need-clean-data-part-2-classification-d29adf9f745a?source=collection_archive---------6-----------------------#2021-11-14">https://towardsdatascience.com/does-bert-need-clean-data-part-2-classification-d29adf9f745a?source=collection_archive---------6-----------------------#2021-11-14</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><figure class="ip iq gp gr ir is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi io"><img src="../Images/e86657e6ed96383cfafe866d71c262eb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*n7pqCbS_wZczR-4p"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">火山是非常疯狂的灾难。我想知道是否有人曾经将一首说唱歌曲描述为“火山”……照片由<a class="ae jd" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的<a class="ae jd" href="https://unsplash.com/@yoshginsu?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Yosh Ginsu </a>拍摄。</p></figure><div class=""/><div class=""><h2 id="fa22" class="pw-subtitle-paragraph kd jf jg bd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku dk translated"><strong class="ak">现在是有趣的事情。轻清洗，重清洗，还是根本没有语言模型？为BERT寻找清理数据的最佳方法。</strong></h2></div><p id="2734" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">到本文结束时，你应该能够在<a class="ae jd" href="https://www.kaggle.com/c/nlp-getting-started/overview" rel="noopener ugc nofollow" target="_blank"> NLP灾难推文Kaggle竞赛</a>中获得前50分(84%的准确率)！</p><figure class="ls lt lu lv gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi lr"><img src="../Images/8cfaa499de3d91df967825f2a3930bd9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*3EwwlMudDPmEA94x.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">我在排行榜上是63。然而，前20名提交的答案被作弊的人占据了，他们只是提交了比赛的答案。图片作者。</p></figure><p id="9309" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">请记住，我们正在尝试对一条推文是否指明了一场灾难(如飓风或森林火灾)进行分类。这项任务的难度在于某些单词的上下文含义不同(例如，将鞋子描述为“火”)。</p><p id="f481" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">正如在<a class="ae jd" href="https://medium.com/@Alexander.Bricken/part-1-data-cleaning-does-bert-need-clean-data-6a50c9c6e9fd" rel="noopener">第1部分</a>中提到的，一旦完成标准文本清理，我们需要决定我们想要使用什么机器学习模型，以及输入数据应该是什么样子。本文的目标是比较使用神经网络的元特征学习、BERT之前强度较低(较轻)的文本预处理和BERT之前强度较高(较重)的文本预处理。</p><p id="9601" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">除此之外，我还将概述BERT，它是如何工作的，以及为什么它是目前领先的语言模型之一。</p><p id="637f" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在开始之前，我们只需要阅读第1部分的泡菜！</p><pre class="ls lt lu lv gt lw lx ly lz aw ma bi"><span id="8611" class="mb mc jg lx b gy md me l mf mg"># download the pickles saved from part 1<br/>train_df = pd.read_pickle("../data/pickles/clean_train_data.pkl")<br/>test_df = pd.read_pickle("../data/pickles/clean_test_data.pkl")</span></pre><p id="e35f" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">现在，让我们从预测灾难的第一种方法开始:使用元特征和深度神经网络。</p></div><div class="ab cl mh mi hu mj" role="separator"><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm"/></div><div class="ij ik il im in"><h1 id="9d93" class="mo mc jg bd mp mq mr ms mt mu mv mw mx km my kn mz kp na kq nb ks nc kt nd ne bi translated">元特征学习</h1><p id="a65d" class="pw-post-body-paragraph kv kw jg kx b ky nf kh la lb ng kk ld le nh lg lh li ni lk ll lm nj lo lp lq ij bi translated">在这里，我们只使用元特征数据来预测灾难推文。这是一种正在测试的替代方法。我的假设是，它不会像BERT模型中的一个那样好，但是，观察它的表现仍然很有趣。此外，也许为了在未来改进BERT，可以包括这种类型的数据，所以我们应该测试我们的元功能如何帮助区分灾难和非灾难推文。</p><h2 id="4238" class="mb mc jg bd mp nk nl dn mt nm nn dp mx le no np mz li nq nr nb lm ns nt nd nu bi translated"><strong class="ak">标准化</strong></h2><p id="0e3e" class="pw-post-body-paragraph kv kw jg kx b ky nf kh la lb ng kk ld le nh lg lh li ni lk ll lm nj lo lp lq ij bi translated">为了将我们的元特征整合到我们的建模中，我们需要规范化我们的列。规范化用于将数据集中的整数列更改为使用通用比例，而不会扭曲值范围的差异或丢失信息。我们通过使用<code class="fe nv nw nx lx b">MinMaxScaler()</code>进行归一化，这允许我们将数据保持在[0，1]的范围内。</p><figure class="ls lt lu lv gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi ny"><img src="../Images/400934b40b16d045e5bf88d1705ee3dd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*9cPIsbFzpRmCB09y.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">MinMaxScaler之后我们的数据是什么样的。图片作者。</p></figure><h2 id="cd25" class="mb mc jg bd mp nk nl dn mt nm nn dp mx le no np mz li nq nr nb lm ns nt nd nu bi translated">列车测试分离</h2><p id="a746" class="pw-post-body-paragraph kv kw jg kx b ky nf kh la lb ng kk ld le nh lg lh li ni lk ll lm nj lo lp lq ij bi translated">在运行我们的深度神经网络之前，我们需要一种分离数据的方法，以便我们可以在之前没有在训练数据中看到的测试数据集上评估模型。为此，我们使用sci-kit learn的<code class="fe nv nw nx lx b">train_test_split()</code>函数。</p><pre class="ls lt lu lv gt lw lx ly lz aw ma bi"><span id="9dc1" class="mb mc jg lx b gy md me l mf mg"># train test split<br/>X_train, X_test, y_train, y_test = train_test_split(final_train_df, train_labels, test_size=0.3, random_state=42)</span></pre><p id="572d" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">现在，我们有了一个训练数据集、一个带标签的测试数据集和一个未带标签的提交测试数据集，我们可以生成我们的模型，根据训练数据拟合它，并根据我们的测试数据测试结果，然后在将提交上传到Kaggle之前，对提交数据运行最终模型。</p><h2 id="5139" class="mb mc jg bd mp nk nl dn mt nm nn dp mx le no np mz li nq nr nb lm ns nt nd nu bi translated">使用深度神经网络生成分类概率</h2><p id="9d28" class="pw-post-body-paragraph kv kw jg kx b ky nf kh la lb ng kk ld le nh lg lh li ni lk ll lm nj lo lp lq ij bi translated">我通过使用序列函数生成深度神经网络，允许我逐层构建DNN。这里的“深”意味着NN连接得很深，如同密集层一样，这些层接收来自前一层的所有<em class="nz">神经元的输入。我添加了具有不同密度的5个层，使用了3个ReLu激活函数(修正的线性激活-该激活函数在神经网络中工作良好)和最终的sigmoid函数，该函数允许我输出输入行是灾难或非灾难的相关概率。</em></p><pre class="ls lt lu lv gt lw lx ly lz aw ma bi"><span id="4d76" class="mb mc jg lx b gy md me l mf mg"># create NN<br/>model = Sequential()<br/>model.add(Dense(90, input_dim=9, activation='relu'))<br/>model.add(Dense(120, activation='relu'))<br/>model.add(Dense(100))<br/>model.add(Dense(20, activation='relu'))<br/><br/>model.add(Dense(1, activation='sigmoid'))<br/><br/># adam optimizer<br/>optimizer = tf.keras.optimizers.Adam(lr=1e-4)<br/><br/># compile and summarise<br/>model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])<br/>model.summary()<br/><br/># first fit is history1<br/>history = model.fit(X_train, y_train, epochs=100, batch_size=35, validation_data=(X_test, y_test), verbose=1)</span></pre><p id="3ee5" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在继续进行模型评估之前，可以显示其他模型的构造，模型评估在模型之间具有相同的方法。然后，可以在模型评估部分中进行跨模型的比较。</p></div><div class="ab cl mh mi hu mj" role="separator"><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm"/></div><div class="ij ik il im in"><h1 id="1705" class="mo mc jg bd mp mq mr ms mt mu mv mw mx km my kn mz kp na kq nb ks nc kt nd ne bi translated">轻型和重型数据清理</h1><p id="f6f7" class="pw-post-body-paragraph kv kw jg kx b ky nf kh la lb ng kk ld le nh lg lh li ni lk ll lm nj lo lp lq ij bi translated">为了优化BERT的性能，一些文本清理可能是必要的，但是我们的Tweets的功能的删除量是有争议的。因为BERT是使用单词的上下文的模型，该上下文提供了单词在句子中相对于所有其他单词的位置，所以通常会建议保存这样的信息。然而，在某些情况下，如果数据被修剪到更高的程度，BERT可能会优于。在此部分中，完成了轻重文本清理。然后，我们可以在模型评估部分测试这两种方法的性能。</p><h2 id="7ea3" class="mb mc jg bd mp nk nl dn mt nm nn dp mx le no np mz li nq nr nb lm ns nt nd nu bi translated">正则表达式文本清理</h2><p id="317b" class="pw-post-body-paragraph kv kw jg kx b ky nf kh la lb ng kk ld le nh lg lh li ni lk ll lm nj lo lp lq ij bi translated">下面是一些文字清理。代码分为重码和轻码两部分。在重清洗中，进行所有替换。在灯光清洗中，只制作灯光(根据需要编辑该功能)。</p><pre class="ls lt lu lv gt lw lx ly lz aw ma bi"><span id="600d" class="mb mc jg lx b gy md me l mf mg"># function taken and modified <br/># from https://towardsdatascience.com/cleaning-text-data-with-python-b69b47b97b76<br/><br/>stopwords = set(STOPWORDS)<br/>stopwords.update(["nan"])<br/><br/>def text_clean(x):<br/><br/>    ### Light<br/>    x = x.lower() # lowercase everything<br/>    x = x.encode('ascii', 'ignore').decode()  # remove unicode characters<br/>    x = re.sub(r'https*\S+', ' ', x) # remove links<br/>    x = re.sub(r'http*\S+', ' ', x)<br/>    # cleaning up text<br/>    x = re.sub(r'\'\w+', '', x) <br/>    x = re.sub(r'\w*\d+\w*', '', x)<br/>    x = re.sub(r'\s{2,}', ' ', x)<br/>    x = re.sub(r'\s[^\w\s]\s', '', x)<br/>    <br/>    ### Heavy<br/>    x = ' '.join([word for word in x.split(' ') if word not in stopwords])<br/>    x = re.sub(r'@\S', '', x)<br/>    x = re.sub(r'#\S+', ' ', x)<br/>    x = re.sub('[%s]' % re.escape(string.punctuation), ' ', x)<br/>    # remove single letters and numbers surrounded by space<br/>    x = re.sub(r'\s[a-z]\s|\s[0-9]\s', ' ', x)<br/><br/>    return x</span></pre><p id="7128" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我们以下列方式应用该函数:</p><pre class="ls lt lu lv gt lw lx ly lz aw ma bi"><span id="755b" class="mb mc jg lx b gy md me l mf mg">train_df['cleaned_text'] = train_df.text.apply(text_clean)<br/>test_df['cleaned_text'] = test_df.text.apply(text_clean)</span></pre><h2 id="4f31" class="mb mc jg bd mp nk nl dn mt nm nn dp mx le no np mz li nq nr nb lm ns nt nd nu bi translated">Lemmatisation(用于重度清洁)</h2><p id="886d" class="pw-post-body-paragraph kv kw jg kx b ky nf kh la lb ng kk ld le nh lg lh li ni lk ll lm nj lo lp lq ij bi translated">仅仅为了大量的清理，我们增加了一个步骤:lemmatisation。在这里，一个词库被用来去除单词的屈折词尾，使它们回到基本形式，这就是所谓的引理。例如，我们将只得到“运行”，而不是“运行”或“运行”。</p><p id="4bcc" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">这有时可以帮助模型更好地解释句子，所以我们在这里测试它。</p><pre class="ls lt lu lv gt lw lx ly lz aw ma bi"><span id="4d7f" class="mb mc jg lx b gy md me l mf mg">train_list = []<br/>for word in train_text:<br/>    tokens = word_tokenize(word)<br/>    lemmatizer = WordNetLemmatizer()<br/>    lemmatized = [lemmatizer.lemmatize(word) for word in tokens]<br/>    train_list.append(' '.join(lemmatized))<br/><br/>test_list = []<br/>for word in test_text:<br/>    tokens = word_tokenize(word)<br/>    lemmatizer = WordNetLemmatizer()<br/>    lemmatized = [lemmatizer.lemmatize(word) for word in tokens]<br/>    test_list.append(' '.join(lemmatized))</span></pre><h2 id="be5c" class="mb mc jg bd mp nk nl dn mt nm nn dp mx le no np mz li nq nr nb lm ns nt nd nu bi translated">标记化和嵌入</h2><p id="4b8c" class="pw-post-body-paragraph kv kw jg kx b ky nf kh la lb ng kk ld le nh lg lh li ni lk ll lm nj lo lp lq ij bi translated">对于轻清洗和重清洗，我们最终需要标记我们的文本数据，并创建适当的嵌入，作为将文本数据包括到机器学习模型中的一种方式。</p><p id="deb3" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">记号化将一段文本分成记号。这些记号是单词。嵌入标记化的句子可以将文本数据转化为机器学习模型可以阅读的数字。我们用拥抱脸变形金刚库来做这个。</p><pre class="ls lt lu lv gt lw lx ly lz aw ma bi"><span id="c63f" class="mb mc jg lx b gy md me l mf mg"># we use a pre-trained bert model to tokenize the text<br/>PRE_TRAINED_MODEL_NAME = 'bert-base-uncased'<br/>tokenizer = BertTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME)</span></pre><p id="1865" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">因为BERT使用固定长度的序列，所以我们需要选择序列的最大长度来最好地表示模型。通过存储每条推文的长度，我们可以做到这一点，并评估覆盖范围。</p><pre class="ls lt lu lv gt lw lx ly lz aw ma bi"><span id="ac4e" class="mb mc jg lx b gy md me l mf mg">token_lens = []<br/>for txt in list(train_list):<br/>    tokens = tokenizer.encode(txt, max_length=512, truncation=True)<br/>    token_lens.append(len(tokens))</span><span id="0874" class="mb mc jg lx b gy oa me l mf mg">sns.displot(token_lens)<br/>plt.xlim([0, 100])<br/>plt.xlabel('Token count')<br/>plt.show()</span></pre><figure class="ls lt lu lv gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi ob"><img src="../Images/d508417bb338745b40dd150b3038c272.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*FWcK_u2lf19XaCtu.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">输出条形图。从这个图中，我们看到密度在40以上下降。因此，我们将max_length设置为40，并重新运行标记器。图片作者。</p></figure><p id="074e" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">但是BertTokenizer到底在做什么呢？</p><p id="74ef" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">对于正常的单词嵌入，我们给每个单词分配一些数值。嵌入是分配给每个索引的d维向量。更多信息见<a class="ae jd" href="https://www.analyticsvidhya.com/blog/2021/09/an-explanatory-guide-to-bert-tokenizer/" rel="noopener ugc nofollow" target="_blank">此处</a>。</p><figure class="ls lt lu lv gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi oc"><img src="../Images/f312a8b2795c2bf6bc251a5c759488ae.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*cfiZR4mIfKdJMhQ6.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">基本单词嵌入:每个单词都有一个唯一的索引和一个嵌入向量。图片由P. Prakash拍摄(2021)。</p></figure><p id="c22b" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">作为BERT构建基础的transformer的新颖之处在于使用正弦位置编码作为单词嵌入的位置索引。通过将正弦和余弦波用于标记化句子中的偶数和奇数索引，相同的单词可以在不同长度的句子中具有相似的嵌入。这为我们的嵌入提供了没有词序的概念，并消除了重复的嵌入值。</p><figure class="ls lt lu lv gt is gh gi paragraph-image"><div class="gh gi od"><img src="../Images/f2e4462271e27407b585dde360cd1948.png" data-original-src="https://miro.medium.com/v2/resize:fit:710/format:webp/0*VldKiBEtyP8Eocw8.png"/></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">使用正弦和余弦的位置编码公式。</p></figure><p id="4d36" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">对于BERT嵌入，这个想法与仅仅一个转换器略有不同。相反，该模型在训练阶段学习位置嵌入，并利用单词片段标记器，其中一些单词被分成子单词。因此，不只是将词汇表外(OOV)的单词标记为包含所有标记，而是将未知的单词分解为模型为其生成嵌入的子单词和字符标记。这保留了原始单词的一些上下文含义。</p><figure class="ls lt lu lv gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi oe"><img src="../Images/55aeeef187d365489dc3d088c7cda248.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*hqiewdpmtT0VC6Mc.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">您可以看到[CLS]标记，它总是出现在文本的开头，特定于分类任务。还有，[SEP]是一种分句的方式。最后，' ##ing '是一个子词嵌入。图片由P. Prakash拍摄(2021)。</p></figure><pre class="ls lt lu lv gt lw lx ly lz aw ma bi"><span id="b159" class="mb mc jg lx b gy md me l mf mg">def bert_tokenizer(text):<br/>    encoding = tokenizer.encode_plus(<br/>    text,<br/>    max_length=40,<br/>    truncation=True,<br/>    add_special_tokens=True, # Add '[CLS]' and '[SEP]'<br/>    return_token_type_ids=False,<br/>    pad_to_max_length=True,<br/>    padding='max_length',<br/>    return_attention_mask=True,<br/>    return_tensors='pt',  # Return PyTorch tensors<br/>    )<br/><br/>    return encoding['input_ids'][0], encoding['attention_mask'][0]</span></pre><p id="1a01" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我们在训练和测试数据集上使用该函数:</p><pre class="ls lt lu lv gt lw lx ly lz aw ma bi"><span id="0f03" class="mb mc jg lx b gy md me l mf mg"># train data tokenization<br/>train_tokenized_list = []<br/>train_attn_mask_list = []<br/>for text in list(train_list):<br/>    tokenized_text, attn_mask = bert_tokenizer(text)<br/>    train_tokenized_list.append(tokenized_text.numpy())<br/>    train_attn_mask_list.append(attn_mask.numpy())<br/><br/># test data tokenization<br/>test_tokenized_list = []<br/>test_attn_mask_list = []<br/>for text in list(test_list):<br/>    tokenized_text, attn_mask = bert_tokenizer(text)<br/>    test_tokenized_list.append(tokenized_text.numpy())<br/>    test_attn_mask_list.append(attn_mask.numpy())</span></pre><p id="1f04" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">然后可以将它们保存为数据帧，以显示我们现在所处的位置。</p><p id="7961" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">最后，我们对清理后的数据进行另一次训练测试分割(无论是重度清理的数据还是轻度清理的数据)。</p><pre class="ls lt lu lv gt lw lx ly lz aw ma bi"><span id="651d" class="mb mc jg lx b gy md me l mf mg"># train test split<br/>X_train, X_test, y_train, y_test, train_mask, val_mask = train_test_split(train_tokenised_text_df, train_labels, train_attn_mask_list, test_size=0.3, random_state=42)</span></pre><h2 id="6be4" class="mb mc jg bd mp nk nl dn mt nm nn dp mx le no np mz li nq nr nb lm ns nt nd nu bi translated">伯特建模</h2><p id="0e8f" class="pw-post-body-paragraph kv kw jg kx b ky nf kh la lb ng kk ld le nh lg lh li ni lk ll lm nj lo lp lq ij bi translated">那么，到底什么是BERT模型，为什么我们如此大惊小怪地以不同的方式正确清理我们的数据？</p><p id="c36b" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">虽然这篇文章中有许多技术细节我可以深入探讨，但我还是要让你参考一下谷歌的文章。BERT做了首字母缩略词描述的事情:它以双向方式训练变压器。通过使用屏蔽语言模型，在简单的意义上是“留一个”任务或完形填空删除，从句子中删除一个单词，该模型尽最大努力预测该单词会是什么。通过这样做，并在左右上下文的条件下，BERT模型学习文本的特征，从而能够区分我们推文的共同特征，而不仅仅是TF-IDF。它还使用了单词的上下文和位置。</p><figure class="ls lt lu lv gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi of"><img src="../Images/4801d752209ad26e73d7d6a466d003df.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*c9LYdc-eemF0xIM6.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">“BERT聪明的语言建模任务屏蔽了输入中15%的单词，并要求模型预测缺失的单词。”图片和说明来自https://jalammar.github.io/illustrated-bert/<a class="ae jd" href="https://jalammar.github.io/illustrated-bert/" rel="noopener ugc nofollow" target="_blank"/>。</p></figure><p id="9069" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">这里我们使用一个预训练的BERT模型。这样做是一个好主意，因为这个模型的嵌入已经在大量的文本数据上进行了训练，超出了我们可以用这个小数据集完成的任务。因此，它更加高效和准确。</p><pre class="ls lt lu lv gt lw lx ly lz aw ma bi"><span id="5e4a" class="mb mc jg lx b gy md me l mf mg">num_classes = len(train_labels.unique()) # this is just 2<br/><br/>bert_model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=num_classes)<br/><br/>checkpoint_path = "../models/light_tf_bert.ckpt"<br/>checkpoint_dir = os.path.dirname(checkpoint_path)<br/><br/>model_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,<br/>                                                 save_weights_only=True,<br/>                                                 verbose=1)<br/><br/>print('\nBert Model', bert_model.summary())<br/><br/>loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)<br/>metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')<br/>optimizer = tf.keras.optimizers.Adam(learning_rate=2e-5,epsilon=1e-08)<br/><br/>bert_model.compile(loss=loss,optimizer=optimizer,metrics=[metric])</span></pre><p id="ee9a" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">然后我们拟合模型。在10个时期(这花费了很长时间)上测试该模型之后，显示出在第二个时期之后过度拟合。因此，作为早期停止的度量，历元的数量被限制为2。有多种方法可以做到这一点，但是对于这个预先训练好的模型来说，约束历元的数量是最方便的方法。</p><pre class="ls lt lu lv gt lw lx ly lz aw ma bi"><span id="5f4e" class="mb mc jg lx b gy md me l mf mg">history=bert_model.fit(X_train,<br/>                       y_train,<br/>                       batch_size=32,<br/>                       epochs=2, # in heavy cleaning we use 3 epochs<br/>                       validation_data=(X_test, y_test),<br/>                       callbacks=[model_callback])</span></pre></div><div class="ab cl mh mi hu mj" role="separator"><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm"/></div><div class="ij ik il im in"><h1 id="04e0" class="mo mc jg bd mp mq mr ms mt mu mv mw mx km my kn mz kp na kq nb ks nc kt nd ne bi translated">模型评估</h1><p id="d18e" class="pw-post-body-paragraph kv kw jg kx b ky nf kh la lb ng kk ld le nh lg lh li ni lk ll lm nj lo lp lq ij bi translated">为了对模型进行相互评估，有许多技术可以使用。与任何机器学习模型一样，最佳做法是检查准确性、损失、val准确性和val损失。我们可以通过下面的代码做到这一点:</p><pre class="ls lt lu lv gt lw lx ly lz aw ma bi"><span id="2d88" class="mb mc jg lx b gy md me l mf mg">### Accuracy<br/>plt.figure(figsize=(16, 10))<br/>plt.plot(history.history['accuracy'])<br/>plt.plot(history.history['val_accuracy'])<br/>plt.title('model accuracy')<br/>plt.ylabel('accuracy')<br/>plt.xlabel('epoch')<br/>plt.legend(['train', 'val'], loc='upper left')<br/>plt.show()<br/><br/>### Loss<br/>plt.figure(figsize=(16, 10))<br/>plt.plot(history.history['loss'])<br/>plt.plot(history.history['val_loss'])<br/>plt.title('model loss')<br/>plt.ylabel('loss')<br/>plt.xlabel('epoch')<br/>plt.legend(['train', 'val'], loc='upper left')<br/>plt.show()</span></pre><figure class="ls lt lu lv gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi og"><img src="../Images/f20443d23dd4311157932f1dd24c4b61.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*4_c5SXlwHaRbsWZw.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">精确度图表的输出示例。图片作者。</p></figure><p id="11c9" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">这些图表有助于理解模型是过拟合、欠拟合还是良好拟合。假设我们使用预训练的BERT模型，我们需要很少的历元来训练该模型(参见<a class="ae jd" href="https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1194/reports/default/15708284.pdf" rel="noopener ugc nofollow" target="_blank">朱，J </a>)。</p><figure class="ls lt lu lv gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi lr"><img src="../Images/ab2e9af99ee1f884d240aaf2f88f2978.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*4WL7wka-iCy2e7h_.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">损失图输出示例。图片作者。</p></figure><p id="d6d3" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在用重清洁和轻清洁测试了我的模型之后，很明显，较少的时期对于防止过度拟合更好。随着精度的不断提高，这一点是显而易见的，但是随着更多时期的运行，val精度下降，val损失增加。</p><p id="e0ff" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">最后，如果我们对我们的模型满意，我们可以使用我们的测试数据集生成我们的预测，并将准确性与我们的测试数据集标签进行比较。通过这样做，我们找到最强的模型，然后使用该模型对Kaggle测试数据进行预测，并提交给Kaggle。</p><h2 id="406a" class="mb mc jg bd mp nk nl dn mt nm nn dp mx le no np mz li nq nr nb lm ns nt nd nu bi translated">预言</h2><p id="8c45" class="pw-post-body-paragraph kv kw jg kx b ky nf kh la lb ng kk ld le nh lg lh li ni lk ll lm nj lo lp lq ij bi translated">我们可以通过运行以下代码来预测任何模型:</p><pre class="ls lt lu lv gt lw lx ly lz aw ma bi"><span id="43a4" class="mb mc jg lx b gy md me l mf mg"># predictions<br/>test_pred = model.predict(X_test)<br/>print(test_pred)</span></pre><p id="e6a1" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">如果预测的输出是概率，我们可以使用这段代码将输出概率转换为预测的二进制值0和1。我们把概率分成0.5。任何高于0.5的预测都被标记为灾难，低于0.5的预测为非灾难。</p><pre class="ls lt lu lv gt lw lx ly lz aw ma bi"><span id="ed66" class="mb mc jg lx b gy md me l mf mg"># this checks if the probability of <br/># disaster is above 0.5. If so, we label 1.<br/><br/>test_pred_bool = test_pred.copy().astype(int)<br/>for index in range(len(test_pred)): <br/>    if test_pred[index]&gt;0.5:<br/>        test_pred_bool[index]=1<br/>    else:<br/>        test_pred_bool[index]=0<br/><br/>final_predictions = test_pred_bool.flatten()</span></pre><p id="107e" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">然后对于我们的任何模型，我们可以使用一个函数来评估它们的准确性，通过输入我们对测试标签的预测。</p><pre class="ls lt lu lv gt lw lx ly lz aw ma bi"><span id="1c02" class="mb mc jg lx b gy md me l mf mg"># model test function<br/>def eval_model(predictions):<br/>    print(accuracy_score(y_test, predictions))<br/>    # Compute fpr, tpr, thresholds and roc auc<br/>    fpr, tpr, thresholds = roc_curve(np.array(y_test), np.array(predictions))<br/>    roc_auc = auc(fpr, tpr)<br/><br/>    # Plot ROC curve<br/>    plt.plot(fpr, tpr, label='ROC curve (area = %0.3f)' % roc_auc)<br/>    plt.plot([0, 1], [0, 1], 'k--')  # random predictions curve<br/>    plt.xlim([0.0, 1.0])<br/>    plt.ylim([0.0, 1.0])<br/>    plt.xlabel('False Positive Rate or (1 - Specifity)')<br/>    plt.ylabel('True Positive Rate or (Sensitivity)')<br/>    plt.title('Receiver Operating Characteristic')<br/>    plt.legend(loc="lower right")<br/>    plt.show()<br/>    print(classification_report(y_test, np.array(predictions), target_names=["not disaster", "disaster"]))<br/><br/>eval_model(final_predictions)</span></pre><figure class="ls lt lu lv gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi oh"><img src="../Images/5aee8a992577f31e36a4593fd33a48c0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*VmY-eWeAe7F2FDsE.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">DNN元特征结果。图片作者。</p></figure><figure class="ls lt lu lv gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi oi"><img src="../Images/4f1288e55094498c08de959788c90151.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*S8PlmCggKgXwcdtT.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">轻数据清理BERT模型结果。图片作者。</p></figure><figure class="ls lt lu lv gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi oj"><img src="../Images/670ef31a30f36b6c6db650342e6a6b25.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*u5DMBn7o6ccx2BP8.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">大量数据清理BERT模型结果。图片作者。</p></figure><p id="0960" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">顶部打印的数字是以浮点数表示的精度。然后是ROC曲线，最后是分类报告，给出了各种指标的细节，比如精度、召回率和f-1分数。</p><p id="071e" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我们可以从上面的图表和输出中看到，light BERT模型在各种指标中表现最佳。ROC曲线最向左上方，表明假阳性率低，真阳性率高。这反映了更高的精确度和召回分数，意味着该模型更经常地，在它预测为阳性的那些中，得到真正的阳性，并且在那些阳性中，它得到大部分预测基本正确(分别是精确度和召回)。</p><p id="08a1" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">f1分数是精确度和召回率的调和平均值。这是一个很好的方法来校准我们的分类水平。然而，它在公式中平衡了精确度和召回率。在我们实际检测灾害发生的情况下，我们更害怕假阴性(第二类错误),因为我们宁愿标记更多的灾害，而不是更少的灾害，尤其是当人的生命受到威胁时。</p><figure class="ls lt lu lv gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi ok"><img src="../Images/65c31154f4e67cd5f25ccbc56c477748.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*f6lmErv3GFyvoBFSlMfjDg.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">如何计算f1分数？图片来自<a class="ae jd" href="https://en.wikipedia.org/wiki/F-score" rel="noopener ugc nofollow" target="_blank">https://en.wikipedia.org/wiki/F-score</a>。</p></figure><p id="9c55" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">Light Data Cleaning BERT模型的总体测试集准确率为84%，这是一个强有力的指标，表明它在未知的Kaggle competition测试数据集上的表现如何。</p></div><div class="ab cl mh mi hu mj" role="separator"><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm"/></div><div class="ij ik il im in"><h1 id="b634" class="mo mc jg bd mp mq mr ms mt mu mv mw mx km my kn mz kp na kq nb ks nc kt nd ne bi translated">Kaggle提交</h1><p id="eeb3" class="pw-post-body-paragraph kv kw jg kx b ky nf kh la lb ng kk ld le nh lg lh li ni lk ll lm nj lo lp lq ij bi translated">为了在Kaggle Competition数据集上测试模型，我们预测了未提供标签的干净测试数据的标签。</p><pre class="ls lt lu lv gt lw lx ly lz aw ma bi"><span id="4a7a" class="mb mc jg lx b gy md me l mf mg"># actual test predictions<br/>real_pred = bert_model.predict(test_tokenised_text_df)<br/># this is output as a tensor of logits, so we use a softmax function<br/>real_tensor_predictions = tf.math.softmax(real_pred.logits, axis=1)<br/># this outputs the related probabilities of being disaster vs. non-disaster<br/><br/># so we then use an argmax function to label<br/>real_predictions = [list(bert_model.config.id2label.keys())[i] for i in tf.math.argmax(real_tensor_predictions, axis=1).numpy()]</span></pre><p id="84be" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">终于可以提交预测了！找到从Kaggle下载的提交文件，我们可以用我们自己的预测覆盖它:</p><pre class="ls lt lu lv gt lw lx ly lz aw ma bi"><span id="1419" class="mb mc jg lx b gy md me l mf mg"># use utils function to get submission file in folder<br/>utils.kaggle_submit(real_predictions, 'submission-light.csv')</span></pre><p id="6f33" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">为了提交，我们使用Kaggle API并键入以下内容:</p><p id="5b9a" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><code class="fe nv nw nx lx b">kaggle competitions submit -c nlp-getting-started -f submission-light.csv -m "YOUR OWN MESSAGE"</code></p><h1 id="fa6c" class="mo mc jg bd mp mq ol ms mt mu om mw mx km on kn mz kp oo kq nb ks op kt nd ne bi translated">结论</h1><p id="92fc" class="pw-post-body-paragraph kv kw jg kx b ky nf kh la lb ng kk ld le nh lg lh li ni lk ll lm nj lo lp lq ij bi translated">从<a class="ae jd" href="https://medium.com/@Alexander.Bricken/part-1-data-cleaning-does-bert-need-clean-data-6a50c9c6e9fd" rel="noopener"> Part 1 </a>到这一部分，我们经历了一个清洗文本数据，从中提取特征，使用典型的预处理方法，最后测试不同的机器学习方法对灾害和非灾害进行分类的过程。结果证明了预训练的BERT模型在使用包含在文本数据中的上下文信息方面的能力。具体来说，我们发现，当输入到BERT模型中时，大量清理文本数据实际上效果更差，因为这些上下文信息丢失了。最重要的是，通过遵循本文中的步骤，任何人都可以朝着理解如何在Kaggle竞争中取得竞争结果迈出第一步。</p><p id="f0b1" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">为了进一步提高准确性，应该尝试进一步调整预训练的BERT模型，也许使用大的BERT，并且可以考虑将元特征数据实现为模型中的附加层，以提供训练数据的另一个维度。</p></div><div class="ab cl mh mi hu mj" role="separator"><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm"/></div><div class="ij ik il im in"><p id="b8a0" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">如果你喜欢这篇文章，请在<a class="ae jd" href="https://twitter.com/abrickand" rel="noopener ugc nofollow" target="_blank">推特</a>上关注我！我每天都在建造。</p><p id="d7b0" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">感谢阅读！</p><h1 id="8906" class="mo mc jg bd mp mq ol ms mt mu om mw mx km on kn mz kp oo kq nb ks op kt nd ne bi translated">文献学</h1><p id="14ee" class="pw-post-body-paragraph kv kw jg kx b ky nf kh la lb ng kk ld le nh lg lh li ni lk ll lm nj lo lp lq ij bi translated">A.<a class="ae jd" href="https://www.analyticsvidhya.com/blog/2020/05/what-is-tokenization-nlp/" rel="noopener ugc nofollow" target="_blank">什么是标记化？</a>，(2020)，分析维迪亚</p><p id="b4f9" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">G.Evitan，<a class="ae jd" href="https://www.kaggle.com/gunesevitan/nlp-with-disaster-tweets-eda-cleaning-and-bert" rel="noopener ugc nofollow" target="_blank"> NLP与灾难推特:EDA，cleaning和BERT </a>，(2019)，Kaggle笔记本</p><p id="37d9" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">G.Giacaglia，<a class="ae jd" rel="noopener" target="_blank" href="/transformers-141e32e69591">变形金刚如何工作</a>，(2019)，走向数据科学</p><p id="98b6" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">I A. Khalid，<a class="ae jd" rel="noopener" target="_blank" href="/cleaning-text-data-with-python-b69b47b97b76">用Python清理文本数据</a>，(2020)，走向数据科学</p><p id="064f" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">J.Devlin和M-W. Chang，<a class="ae jd" href="https://ai.googleblog.com/2018/11/open-sourcing-bert-state-of-art-pre.html" rel="noopener ugc nofollow" target="_blank">谷歌人工智能博客:开源BERT </a>，(2018)，谷歌博客</p><p id="7fe6" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">J.朱，<a class="ae jd" href="https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1194/reports/default/15708284.pdf" rel="noopener ugc nofollow" target="_blank">班底模型对比</a>，(未注明)，斯坦福大学</p><p id="4ac1" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">Kaggle团队，<a class="ae jd" href="https://www.kaggle.com/c/nlp-getting-started/overview" rel="noopener ugc nofollow" target="_blank">自然语言处理与灾难推文</a> (2021)，Kaggle竞赛</p><p id="64e6" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">页（page的缩写）Prakash，<a class="ae jd" href="https://www.analyticsvidhya.com/blog/2021/09/an-explanatory-guide-to-bert-tokenizer/" rel="noopener ugc nofollow" target="_blank">BERT token izer的解释性指南</a>，(2021)，分析Vidhya</p><p id="9974" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">南蒂勒，<a class="ae jd" href="https://medium.com/analytics-vidhya/basics-of-using-pre-trained-glove-vectors-in-python-d38905f356db" rel="noopener">使用预训练手套向量的基础知识</a>，(2019)，分析Vidhya</p><p id="46c2" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">维基百科，<a class="ae jd" href="https://en.wikipedia.org/wiki/F-score" rel="noopener ugc nofollow" target="_blank"> F分数</a>，(未注明)，维基百科</p></div></div>    
</body>
</html>