<html>
<head>
<title>3 Examples That Show The Unlimited Flexibility of PySpark</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">展示PySpark无限灵活性的3个示例</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/3-examples-that-show-the-unlimited-flexibility-of-pyspark-319ab22d5a?source=collection_archive---------18-----------------------#2021-06-05">https://towardsdatascience.com/3-examples-that-show-the-unlimited-flexibility-of-pyspark-319ab22d5a?source=collection_archive---------18-----------------------#2021-06-05</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="f907" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">Python和SQL的结合，但比两者都简单</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/c753bbee3b813343b0aafd9508f86da1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uN3G3dR-_2Brm4OzxfF8Uw.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">Genessa panainite在<a class="ae ky" href="https://unsplash.com/s/photos/spark?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="a890" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">Spark是一个用于大规模数据处理的分析引擎。它让您可以将数据和计算分散到集群上，从而实现显著的性能提升。</p><p id="623a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">收集、传输和存储数据比以往任何时候都更容易。因此，当我们处理现实生活中的问题时，需要处理大量的数据。因此，像Spark这样的分布式引擎在这种情况下变得必不可少。</p><p id="dfcc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">PySpark是Spark的Python API。它给我们带来了Python语法的简单性，因此我们可以轻松地处理和分析大量数据。PySpark的SQL模块更进一步，为我们提供了类似SQL的操作。</p><p id="7473" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我在这里想说的是PySpark是一个非常高效的工具，具有易于使用和直观的语法。使PySpark成为如此简单的工具的一个重要因素是它提供的灵活性。</p><p id="0292" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">无论您习惯使用Pandas还是SQL，学习PySpark都不会有困难。在本文中，我们将介绍3种常见的数据操作，展示其灵活性。</p><p id="c8c3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们首先需要创建一个SparkSession，作为Spark SQL的入口点。</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="a29e" class="ma mb it lw b gy mc md l me mf">from pyspark.sql import SparkSession</span><span id="02c4" class="ma mb it lw b gy mg md l me mf">spark = SparkSession.builder.getOrCreate()</span></pre><p id="415f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们通过读取csv文件来创建一个spark数据帧。我们将使用Kaggle上的墨尔本房产<a class="ae ky" href="https://www.kaggle.com/anthonypino/melbourne-housing-market" rel="noopener ugc nofollow" target="_blank">数据集</a>。</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="eb98" class="ma mb it lw b gy mc md l me mf">file_path = "/home/sparkuser/Downloads/melb_housing.csv"<br/>df = spark.read.csv(file_path, header=True)</span></pre></div><div class="ab cl mh mi hx mj" role="separator"><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm"/></div><div class="im in io ip iq"><h2 id="fcc8" class="ma mb it bd mo mp mq dn mr ms mt dp mu li mv mw mx lm my mz na lq nb nc nd ne bi translated">1.选择列的子集</h2><p id="7cae" class="pw-post-body-paragraph kz la it lb b lc nf ju le lf ng jx lh li nh lk ll lm ni lo lp lq nj ls lt lu im bi translated">我们可能不需要数据集中的所有列。有些列可能是多余的，或者不适用于特定的任务。</p><p id="c46b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">PySpark提供了多种选择列子集的方法。第一个是选择方法。</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="4fe1" class="ma mb it lw b gy mc md l me mf">df1 = df.select("Type", "Rooms", "Regionname", "Distance", "Price")<br/>df1.show(5)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/30d6b8ebf0ceb525afc03b427f9bdf2a.png" data-original-src="https://miro.medium.com/v2/resize:fit:696/format:webp/1*U83zNLWSEVAogzPX8mVMCw.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">(图片由作者提供)</p></figure><p id="52f9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们也可以用类似熊猫的语法做同样的操作。</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="77ad" class="ma mb it lw b gy mc md l me mf">df2 = df["Type", "Rooms", "Regionname", "Distance", "Price"]<br/>df2.show(5)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/30d6b8ebf0ceb525afc03b427f9bdf2a.png" data-original-src="https://miro.medium.com/v2/resize:fit:696/format:webp/1*U83zNLWSEVAogzPX8mVMCw.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">(图片由作者提供)</p></figure><p id="cd83" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">两种方法都可以。确保使用正确类型的括号。select方法使用括号(“()”)，而另一个方法使用方括号(“[]”)。</p></div><div class="ab cl mh mi hx mj" role="separator"><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm"/></div><div class="im in io ip iq"><h2 id="0a51" class="ma mb it bd mo mp mq dn mr ms mt dp mu li mv mw mx lm my mz na lq nb nc nd ne bi translated">2.筛选行</h2><p id="1f94" class="pw-post-body-paragraph kz la it lb b lc nf ju le lf ng jx lh li nh lk ll lm ni lo lp lq nj ls lt lu im bi translated">数据分析和操作中的一个常见操作是根据条件过滤观察值(即行)。我们将讨论3种不同的过滤方法。</p><p id="9e1e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">第一个是过滤方法。</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="871d" class="ma mb it lw b gy mc md l me mf">from pyspark.sql import functions as F</span><span id="ca42" class="ma mb it lw b gy mg md l me mf">df_sub = df2.filter(F.col("Rooms") == 4)</span></pre><p id="f0a4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">col函数用于选择房间列，因此我们只选择有4个房间的房子。</p><p id="c6d8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">就像前面的例子一样，我们可以用类似熊猫的语法做同样的操作。</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="8ee7" class="ma mb it lw b gy mc md l me mf">df_sub = df2[df2.Rooms == 4]</span></pre><p id="c3c5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">最后但同样重要的是，可以使用where方法。它类似于SQL中的where子句。</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="3d19" class="ma mb it lw b gy mc md l me mf">df_sub = df2.where("Rooms = 4")</span><span id="e7e6" class="ma mb it lw b gy mg md l me mf">df_sub.show(5)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nl"><img src="../Images/8efb94230eae21d4859b18e9747815f4.png" data-original-src="https://miro.medium.com/v2/resize:fit:698/format:webp/1*F5sKeIY07URPURfAbXjh3A.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">(图片由作者提供)</p></figure><p id="25d7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我不认为我们可以声明一个方法比其他方法简单。这里的重点是强调使用PySpark进行操作的多种方式。它不会强迫你去适应一种特定的做事方式。与此相反，PySpark符合你的口味。</p></div><div class="ab cl mh mi hx mj" role="separator"><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm"/></div><div class="im in io ip iq"><h2 id="9366" class="ma mb it bd mo mp mq dn mr ms mt dp mu li mv mw mx lm my mz na lq nb nc nd ne bi translated">3.创建新列</h2><p id="1b74" class="pw-post-body-paragraph kz la it lb b lc nf ju le lf ng jx lh li nh lk ll lm ni lo lp lq nj ls lt lu im bi translated">数据预处理通常涉及一些列操作。常见的操作之一是基于现有的列创建一个新的列。</p><p id="91bb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们来看看在PySpark中创建新列的两种不同方法。我们将创建一个以百万为单位显示房价的列。因此，我们可以通过将价格列除以100万来实现它。</p><p id="080f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">第一个选项是使用withColumn方法。</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="e5a9" class="ma mb it lw b gy mc md l me mf">df2_new = df2.withColumn("Price_mio", F.col("Price") / 1000000)<br/>df2_new.show(5)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nm"><img src="../Images/8031b95a211a7237fddea3982a546fe3.png" data-original-src="https://miro.medium.com/v2/resize:fit:840/format:webp/1*mi58KcpkixsxEMDiuOFFbw.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">(图片由作者提供)</p></figure><p id="1129" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">第一个参数是新列的名称。第二部分包含计算值的操作。在第二部分中，我们不局限于简单的操作。事实上，PySpark在创建新列时提供了很大的灵活性。</p><p id="d6f6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">考虑这样一种情况，我们需要根据不同列中的值来更新价格。例如，我们可能想提高h类房屋的价格，其他房屋的价格将保持不变。</p><p id="b76d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们可以使用withColumn方法和when函数来执行此任务，如下所示。</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="79c2" class="ma mb it lw b gy mc md l me mf">df2_new = df2.withColumn(<br/>   "Price_updated", <br/>   F.when(F.col("Type") == "h", F.col("Price") * 1.2).\<br/>   otherwise(F.col("Price")))</span><span id="ca01" class="ma mb it lw b gy mg md l me mf">df2_new.show(5)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/6b7c562fa4fbdfd0b9a00d4a27f01f48.png" data-original-src="https://miro.medium.com/v2/resize:fit:892/format:webp/1*Cjo8MUIdssEfKQBoNkbF1A.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">(图片由作者提供)</p></figure><p id="c915" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">上面的代码告诉PySpark当类型为h时将价格列相乘，否则保持价格不变。</p></div><div class="ab cl mh mi hx mj" role="separator"><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm"/></div><div class="im in io ip iq"><p id="3901" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">创建列的第二个选项是使用SQL查询。我们首先需要将数据框注册为临时表。</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="ebac" class="ma mb it lw b gy mc md l me mf">df2.registerTempTable("df2_table")</span></pre><p id="d4bc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们现在可以使用的就像一个SQL表。为了创建带有新列的数据框，我们可以编写如下SQL查询。</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="3bae" class="ma mb it lw b gy mc md l me mf">df2_updated = spark.sql(<br/>     "select *, Price / 1000000 as Price_mio from df2_table"<br/>)<br/>df2_updated.show(5)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nm"><img src="../Images/f5514bbc11f4ae9a7409027586b4dcab.png" data-original-src="https://miro.medium.com/v2/resize:fit:840/format:webp/1*Ra43uCxRYlsL8a9I7JS0gA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">(图片由作者提供)</p></figure><p id="b5f5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们可以像查询SQL表一样查询临时表(df2_table)。例如，我们可以计算每个地区的平均房屋数量如下。</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="104b" class="ma mb it lw b gy mc md l me mf">spark.sql(<br/>    """<br/>    select Regionname, avg(Price) as avg_price <br/>    from df2_table <br/>    group by Regionname<br/>    """<br/>).show()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi no"><img src="../Images/9137044c2dfd1838d69b142c2b5dbb4b.png" data-original-src="https://miro.medium.com/v2/resize:fit:594/format:webp/1*12aqmpgsRolbiqV4Yz0B2w.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">(图片由作者提供)</p></figure></div><div class="ab cl mh mi hx mj" role="separator"><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm"/></div><div class="im in io ip iq"><h2 id="1c7d" class="ma mb it bd mo mp mq dn mr ms mt dp mu li mv mw mx lm my mz na lq nb nc nd ne bi translated">结论</h2><p id="ca9f" class="pw-post-body-paragraph kz la it lb b lc nf ju le lf ng jx lh li nh lk ll lm ni lo lp lq nj ls lt lu im bi translated">我们所做的例子清楚地展示了PySpark的灵活性。无论您是Python用户还是SQL用户，您都会找到使用PySpark的舒适方式。</p><p id="1a6f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">PySpark语法有点像Python和SQL的结合，但我认为它比两者都简单。</p><p id="f5b3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">需要注意的是，Spark针对大规模数据进行了优化。因此，在处理小规模数据时，您可能看不到任何性能提升。事实上，在处理小数据集时，Pandas可能会比PySpark表现得更好。</p><p id="d202" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">感谢您的阅读。如果您有任何反馈，请告诉我。</p></div></div>    
</body>
</html>