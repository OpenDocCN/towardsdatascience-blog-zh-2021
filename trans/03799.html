<html>
<head>
<title>What can flatness teach us about why Neural Networks generalise?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">关于神经网络为什么会泛化，平坦度能教会我们什么？</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/what-can-flatness-teach-us-understanding-generalisation-in-deep-neural-networks-a7d66f69cb5c?source=collection_archive---------32-----------------------#2021-03-29">https://towardsdatascience.com/what-can-flatness-teach-us-understanding-generalisation-in-deep-neural-networks-a7d66f69cb5c?source=collection_archive---------32-----------------------#2021-03-29</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="893f" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsdatascience.com/tagged/thoughts-and-theory" rel="noopener" target="_blank">思想和理论</a></h2><div class=""/><div class=""><h2 id="11e9" class="pw-subtitle-paragraph jw iz iq bd b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn dk translated">它的不足之处在哪里？</h2></div><p id="f8da" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">这是一系列总结工作的第三篇文章，旨在提供深度神经网络(DNNs)的一般化理论。简而言之，<a class="ae lk" rel="noopener" target="_blank" href="/neural-networks-are-fundamentally-bayesian-bee9a172fad8">的第一篇文章</a>总结了用随机优化器(如SGD)训练的dnn在参数空间中找到概率与其体积成比例的函数的证据，而<a class="ae lk" rel="noopener" target="_blank" href="/deep-neural-networks-are-biased-at-initialisation-towards-simple-functions-a63487edcb99">的第二篇文章</a>认为这些大体积函数是“简单的”，因此解释了为什么dnn会泛化。</p><p id="02ba" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">在下文中，我们总结了[1]中的结果，这些结果解释了为什么“损失前景的平坦性”已被证明与一般化相关，这是一个众所周知的结果(例如参见[5])。他们提供了大量的经验证据，表明这种相关性实际上是(1)局部平坦性和周围函数的体积之间的弱相关性，和(2)体积和一般化之间的强相关性的组合。这种结合在“平坦”和概括之间产生了微弱的相关性。</p><p id="f93b" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">它是和<a class="ae lk" href="https://medium.com/@sofuncheung16" rel="noopener">张硕丰</a>一起写的，他是【1】的第一作者。</p><h2 id="0718" class="ll lm iq bd ln lo lp dn lq lr ls dp lt kx lu lv lw lb lx ly lz lf ma mb mc iw bi translated"><strong class="ak">简介和背景</strong></h2><p id="6d40" class="pw-post-body-paragraph ko kp iq kq b kr md ka kt ku me kd kw kx mf kz la lb mg ld le lf mh lh li lj ij bi translated">自20世纪90年代以来，直觉上认为损失景观的局部平坦性与深度DNNs中的良好概括相关，Hinton和Schmidhuber认为，由于更平坦的最小值需要更少的信息来描述，它们应该比尖锐的最小值更好地概括(根据奥卡姆剃刀原理)。由于这种最初的兴趣，已经开发了许多不同的局部平坦度测量，并且已经发现设计来寻找平坦区域的算法可以改进一般化。</p><figure class="mj mk ml mm gt mn gh gi paragraph-image"><div class="gh gi mi"><img src="../Images/ff79d773bbd122658c4ab5b5c95ae0e1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1376/0*HUPXEuXLuqqnh_oC"/></div><p class="mq mr gj gh gi ms mt bd b be z dk translated"><strong class="bd mu">图1: </strong>来自Keskar等人【5】，其中平坦极小值和良好概括之间的联系被重新构建，以这幅漫画作为直觉。如果测试损失函数与训练损失函数略有不同，则在平坦最小值中，训练损失和测试损失之间不会有太大差异。然而，在尖锐的最小值中，在测试和训练损失之间可能有实质的差别。</p></figure><blockquote class="mv mw mx"><p id="aa5d" class="ko kp my kq b kr ks ka kt ku kv kd kw mz ky kz la na lc ld le nb lg lh li lj ij bi translated">尽管有这些工作，所有的平坦度测量都有严重的问题:它们受到α缩放问题的困扰:对于一个ReLU激活的DNN，你可以通过(某个常数)α和1/α分别缩放两个相邻层的权重，并且<strong class="kq ja">不会改变DNN </strong>在数据上表达的函数，因此不会影响泛化。不幸的是，(大多数)平坦度测量<strong class="kq ja">在α缩放后会有很大的变化</strong>。</p></blockquote><p id="7868" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">更糟糕的是，α缩放只是<em class="my">一个<em class="my">重新参数化</em>的</em>例子，不会改变函数。这个<em class="my">重新参数化</em>可以是层方式的，也可以是神经元方式的，这取决于具体的DNN架构。</p><p id="4040" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">许多工作已经为SGD训练的dnn发现了以下情况:如果SGD在损失函数中找到平坦的最小值，则平均而言，网络将比如果SGD找到尖锐的最小值概括得更好(参见<strong class="kq ja">图1 </strong>以获得该论点的漫画)。然而，α-标度论点表明，任何平坦的极小值都可以变得任意尖锐，而不改变函数，<em class="my">以及一般化。</em></p><blockquote class="mv mw mx"><p id="f96b" class="ko kp my kq b kr ks ka kt ku kv kd kw mz ky kz la na lc ld le nb lg lh li lj ij bi translated">因此，平坦性不可能是概括良好的函数的定义特征。</p></blockquote><p id="0246" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">在[2，3，4]中提出，我们应该开始考虑<strong class="kq ja">参数空间</strong>和<strong class="kq ja">功能空间</strong>之间的关系。毕竟，dnn是高级函数逼近设备，很明显参数的选择相对于参数产生的函数来说是次要的 <em class="my">。</em>我们需要问以下两个问题:</p><ol class=""><li id="8412" class="nc nd iq kq b kr ks ku kv kx ne lb nf lf ng lj nh ni nj nk bi translated">函数空间中的什么性质赋予函数良好的推广性？</li><li id="bdb3" class="nc nd iq kq b kr nl ku nm kx nn lb no lf np lj nh ni nj nk bi translated">平坦是如何融入其中的？</li></ol><h2 id="0a0a" class="ll lm iq bd ln lo lp dn lq lr ls dp lt kx lu lv lw lb lx ly lz lf ma mb mc iw bi translated">深度神经网络的函数空间观</h2><p id="b2f5" class="pw-post-body-paragraph ko kp iq kq b kr md ka kt ku me kd kw kx mf kz la lb mg ld le lf mh lh li lj ij bi translated">dnn是从输入空间<em class="my"> X </em>到输出空间<em class="my"> Y </em>的参数化函数。更具体地说，<strong class="kq ja">参数θ的选择决定了深度神经网络表达什么函数<em class="my"> f : X→Y </em>:我们将把它写成Y =<em class="my">f(X；θ) </em>。</strong></p><p id="b090" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">对于某些数据集<em class="my"> D，</em>有一个“真”函数<em class="my"> f* : X→Y </em>描述了如何将<em class="my"> D⊂ X </em>中的输入映射到<em class="my"> Y </em>中的输出(其中<em class="my"> D </em>是数据流形)。我们的神经网络的工作是在给定一些训练数据<em class="my"> S ⊂ D </em>的情况下，尝试在整个<em class="my"> D </em>上精确地逼近<em class="my"> f* </em>。需要强调的是，在训练之后，神经网络所表达的功能在<em class="my"> S </em>上总是相同的(如果你在<em class="my"> S </em>上训练到100%的准确度)，但是在<em class="my"> D </em>的其余部分上可能不同。这些差异会导致不同的概括错误。</p><p id="9bfa" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">例如，考虑MNIST: <em class="my"> X </em>是<em class="my"> 28×28 </em>灰度图像的空间，<em class="my"> Y </em>是标签<em class="my">{ 0–9 }的空间，f </em>是从图像到标签的映射，<em class="my"> D </em>是<em class="my"> X </em>中对应于手写数字的部分，<em class="my"> S </em>是这些图像的子集。</p><p id="7814" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">更多细节和示例见<a class="ae lk" rel="noopener" target="_blank" href="/neural-networks-are-fundamentally-bayesian-bee9a172fad8">第一篇文章</a>。</p><p id="b6f6" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">在我们继续之前，我们需要定义[1]的中心量(参见<strong class="kq ja">图2 </strong>的图示):</p><ul class=""><li id="f2fb" class="nc nd iq kq b kr ks ku kv kx ne lb nf lf ng lj nq ni nj nk bi translated"><em class="my"> V(f) </em>是某个神经网络的参数空间中的体积(具有高斯度量),使得<em class="my">f(x；θ)=f </em> (x)。这意味着两件事:(1)所有函数的“总体积”是1，以及(2)因此，<em class="my"> V(f) </em>是参数空间的分数，这使得神经网络表达<em class="my"> f. </em></li><li id="a339" class="nc nd iq kq b kr nl ku nm kx nn lb no lf np lj nq ni nj nk bi translated"><em class="my"> Pₒₚₜ ( f | S ) </em>是像SGD这样的随机优化器在s上训练到100%精度后找到函数<em class="my"> f </em>的概率</li><li id="7227" class="nc nd iq kq b kr nl ku nm kx nn lb no lf np lj nq ni nj nk bi translated"><em class="my"> V( f | S) = P(S | f ) V(f) / V(S)，</em>其中<em class="my"> V(S) </em>是与一个训练集<em class="my"> S，</em>和<em class="my"> P(S | f ) </em> =1如果<em class="my"> f </em>与<em class="my"> S </em> else 0一致。</li></ul><figure class="mj mk ml mm gt mn gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/f2a0bbfa410afab97d1abb9b91f38c21.png" data-original-src="https://miro.medium.com/v2/resize:fit:1214/0*r5p7MKFt9i2Epz-0"/></div><p class="mq mr gj gh gi ms mt bd b be z dk translated"><strong class="bd mu">图2: </strong>这两个图总结了前两篇帖子中的论点。(a)是参数空间的动画，显示了当你根据训练集<em class="ns"> S </em>对其进行划分时会发生什么。有些功能在<em class="ns"> S </em>上不能很好地执行，而在那些有组合体积的功能中，<em class="ns"> V(S)，</em>中，有些功能比其他功能占据了更多的体积。有人认为具有大的<em class="ns"> V(f) </em>的函数“更简单”,因此更容易推广。量<em class="ns"> V( f | S) </em>是<em class="ns"> f </em>(某白函数)的体积除以白函数的总面积。<em class="ns"> </em> (b)认为，如果<em class="ns">v(f | s)</em>有一个很大的范围，那么你期望<em class="ns"> Pₒₚₜ ( f | S ) ≈ V( f | S) </em>。</p></figure><p id="4346" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">[3]中的大量经验证据表明，DNNs中的良好概括有以下三个原因:</p><ol class=""><li id="a885" class="nc nd iq kq b kr ks ku kv kx ne lb nf lf ng lj nh ni nj nk bi translated"><em class="my"> Pₒₚₜ ( f | S ) ≈ V( f | S) </em>用于优化器训练的DNNs</li><li id="d68a" class="nc nd iq kq b kr nl ku nm kx nn lb no lf np lj nh ni nj nk bi translated">体积较大的函数<em class="my"> V(f) </em>比体积较小的函数<em class="my"> V(f) </em>概括得更好</li><li id="15ff" class="nc nd iq kq b kr nl ku nm kx nn lb no lf np lj nh ni nj nk bi translated">[3]中的论点表明，这是因为高容量功能“简单”</li></ol><blockquote class="mv mw mx"><p id="38aa" class="ko kp my kq b kr ks ka kt ku kv kd kw mz ky kz la na lc ld le nb lg lh li lj ij bi translated">直观地说，(1)是说优化者发现函数的概率与它们在参数空间中占据的体积成比例。优化器的选择将少量影响第一个近似等式，这可以解释一般化中的小变化。</p></blockquote><p id="1ce5" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">需要澄清的最后一点是:我们如何度量函数的差异？当然，我们不可能计算所有可能数据的差异。相反，我们使用一个小的测试集<em class="my"> E⊂ D </em>。我们根据功能在<em class="my"> E </em>上的表现来区分它们。例如，假设你在大小为|S|=50000张图片的训练集<em class="my"> S </em>上训练MNIST，直到100%的训练准确率。使用该停止条件训练的任何网络将在<em class="my"> S </em>上模拟相同的函数<em class="my"> f </em>(即，它将以相同的方式对所有图像进行分类)。然而，根据随机种子，神经网络将在看不见的图像的测试集<em class="my"> E </em>上找到不同的可能分类。在图2a 的漫画中，每个白色区域对应于这些看不见的图像的不同分类。这是一种对函数进行“粗粒化”的方法，因此我们可以以一种有意义的方式计算像<em class="my"> V(f |S) </em>这样的量。</p><p id="b95b" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">该功能通常根据<em class="my"> X </em>中的一些数据<em class="my"> D </em>来定义。还有几个类似的微妙之处，这里有完整的解释。<br/>注意，在第一篇文章中，<em class="my"> V(f|S) </em>被标为<em class="my"> Pᵦ(f|S) </em>以强调这个量的贝叶斯解释。</p><h2 id="1e64" class="ll lm iq bd ln lo lp dn lq lr ls dp lt kx lu lv lw lb lx ly lz lf ma mb mc iw bi translated"><strong class="ak">平坦度和体积与泛化的关系如何？</strong></h2><p id="bc38" class="pw-post-body-paragraph ko kp iq kq b kr md ka kt ku me kd kw kx mf kz la lb mg ld le lf mh lh li lj ij bi translated">所以，我们想问下面的问题:一般化是由前一节给出的论点解释的吗，或者平坦性有什么要补充的吗？</p><p id="4546" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">在<em class="my"> V(f) </em>范围内，局部<em class="my">平坦度</em>可能有所不同，但似乎可以感觉到，高音量的函数平均来说比小音量的函数更平坦(考虑<strong class="kq ja">图2b </strong>)。</p><blockquote class="mv mw mx"><p id="dd2c" class="ko kp my kq b kr ks ka kt ku kv kd kw mz ky kz la na lc ld le nb lg lh li lj ij bi translated">然后，假设在体积和一般化之间有很强的相关性，体积和平坦度之间的弱相关性可能足以给出一般化和平坦度之间的弱相关性。</p></blockquote><p id="b658" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">[1]的主要结果提供了支持这一主张的大量经验证据。</p><p id="db17" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">平坦度和一般化之间的相关性高度依赖于优化器，并且比体积和一般化之间的相关性弱得多(即使忽略平坦度-一般化相关性可以通过α缩放任意确定)。以下结果使用[5]中的最差情况锐度指标。为了迫使优化器找到泛化能力差的函数，将由错误标记的数据组成的“攻击集”<em class="my"/>添加到训练数据中。</p><figure class="mj mk ml mm gt mn gh gi paragraph-image"><div role="button" tabindex="0" class="nu nv di nw bf nx"><div class="gh gi nt"><img src="../Images/375e3af4f131fa694a2ce855b517b735.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*5ztp0bJqdMEIL1eF"/></div></div><p class="mq mr gj gh gi ms mt bd b be z dk translated"><strong class="bd mu">图3:</strong>MNIST和CIFAR-10上音量<em class="ns"> V(f) </em>锐度和泛化精度的相关性。对于MNIST |S|=500，| E | = 1000对于CIFAR-10 |S|=5000，|E|=2000。攻击集大小|A|从0到|S|不等，并生成具有不同泛化性能的函数。(a)-(c)分别描述了MNIST FCN、CIFAR-10 FCN和CIFAR-10 Resnet-50的概化和数量之间的关系。(d)-(f)显示了相同的三个数据集-架构对的泛化能力和清晰度之间的相关性。在这个实验中，所有的dnn都用普通的SGD训练。</p></figure><figure class="mj mk ml mm gt mn gh gi paragraph-image"><div role="button" tabindex="0" class="nu nv di nw bf nx"><div class="gh gi nt"><img src="../Images/f4ee0236f7b5d689579476a1ff0eab81.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*ZbFGCSzklfHMCe76"/></div></div><p class="mq mr gj gh gi ms mt bd b be z dk translated"><strong class="bd mu">图4: </strong> SGD变体可以打破平坦度-一般化相关性，但不能打破体积-一般化相关性。这些图示出了在(a)和(d)上训练的FCN的泛化与音量或锐度的关系——具有熵-SGD的MNIST；(b)和(e)——MNIST和亚当；(c)和(f)——带有自动监测系统的CIFAR-10。对于与图3中相同的S和E。请注意，与体积的相关性实际上与普通SGD相同，但与平坦度的相关性变化很大。</p></figure><p id="7c5d" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">当我们在训练时直接比较同一DNN的平坦度和体积时，我们看到了更有趣的东西。在<strong class="kq ja">图5 </strong>中，我们测量了在MNIST训练的FCN在每个时期的音量和锐度。在第140个时期达到零训练误差，我们又过度训练了1000个时期。从初始化开始，清晰度和音量都降低，直到达到零训练误差。随后，体积保持不变，但交叉熵损失继续减少，正如这类分类问题所预期的那样。这导致锐度降低，即使函数、音量和训练误差不变。这表明平坦度是一个相对的概念，例如，取决于训练的持续时间。</p><p id="71a1" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">我们还研究了α缩放:<strong class="kq ja">图5 </strong>还显示了您可以执行α缩放并影响锐度，而无需更改函数。如插图所示，由于函数f不变，体积在α缩放时最初是不变的。然而，α缩放可以将系统驱动到参数空间的不寻常部分，在损失函数中具有陡峭的梯度，这意味着SGD几乎立即脱离零训练误差流形。然而，该系统很快就松弛到基本相同的功能和体积。这一经验发现进一步表明了平坦性论点的局限性，同时加强了功能空间视图和体积作为与良好概括相关联的良好属性。</p><figure class="mj mk ml mm gt mn gh gi paragraph-image"><div role="button" tabindex="0" class="nu nv di nw bf nx"><div class="gh gi nt"><img src="../Images/988bf956f605cdc616bcc5beb4cfab4a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*lbizGUEWhkd5WXmD"/></div></div><p class="mq mr gj gh gi ms mt bd b be z dk translated"><strong class="bd mu">图5: </strong>锐度和音量如何随时代演进。在每个时期，我们计算|S|=500的MNIST FCN的锐度和音量。绿色虚线表示时期140，在该时期达到零训练误差并且后训练开始。红色虚线表示时期639，其中α = 5.9时发生α缩放。在α缩放时，清晰度显著增加，但随后又迅速降低。插图显示，在α缩放后，音量最初没有变化。然而，大梯度意味着在随后的SGD步骤中，在恢复到(几乎)相同的功能和体积之前，功能(和体积)会发生变化。</p></figure><h2 id="a279" class="ll lm iq bd ln lo lp dn lq lr ls dp lt kx lu lv lw lb lx ly lz lf ma mb mc iw bi translated"><strong class="ak">结论</strong></h2><p id="d8d2" class="pw-post-body-paragraph ko kp iq kq b kr md ka kt ku me kd kw kx mf kz la lb mg ld le lf mh lh li lj ij bi translated">我们讨论了以下结果:</p><ol class=""><li id="8542" class="nc nd iq kq b kr ks ku kv kx ne lb nf lf ng lj nh ni nj nk bi translated">随机优化者发现函数与它们在参数空间中占据的体积成比例(有点直观)。</li><li id="6ec2" class="nc nd iq kq b kr nl ku nm kx nn lb no lf np lj nh ni nj nk bi translated">高容量函数(最常见的函数)更可能是“简单”的，因此概括得更好(因此DNNs概括得更好)。</li><li id="2a73" class="nc nd iq kq b kr nl ku nm kx nn lb no lf np lj nh ni nj nk bi translated">这些高音量函数在很大程度上比低音量函数局部更平坦。</li></ol><p id="7223" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">因此，在平坦度和一般化之间观察到的弱相关性是由于(弱)平坦度-体积相关性和(强)体积-一般化相关性的组合。</p><p id="a572" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">注意，这是对[1]的删节总结。因此，我们把重点放在了我们认为最重要的部分，因此可能忽略了一些有趣的细节。</p><h2 id="e606" class="ll lm iq bd ln lo lp dn lq lr ls dp lt kx lu lv lw lb lx ly lz lf ma mb mc iw bi translated"><strong class="ak">参考文献</strong></h2><p id="e3d2" class="pw-post-body-paragraph ko kp iq kq b kr md ka kt ku me kd kw kx mf kz la lb mg ld le lf mh lh li lj ij bi translated">[1]张硕丰，艾萨克·里德，吉列尔莫·瓦莱·佩雷斯，阿德·路易.为什么平坦度与深度神经网络的泛化相关。(2021)<a class="ae lk" href="https://arxiv.org/abs/2103.06219" rel="noopener ugc nofollow" target="_blank"/><br/>【2】c .明加德，g .瓦莱-佩雷斯，j .斯卡尔斯，a .路易。SGD是贝叶斯采样器吗？嗯，差不多了。(2020)<a class="ae lk" href="https://arxiv.org/abs/2006.15191" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/2006.15191</a><br/>【3】c .明加德、j .斯卡尔斯、g .瓦莱-佩雷斯、d .马丁内斯-卢比奥、v .米库利克、a .路易斯。神经网络先验地偏向于低熵函数。(2019)<a class="ae lk" href="https://arxiv.org/abs/1909.11522" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1909.11522</a><br/>【4】g .瓦莱-佩雷斯，c .卡马戈，a .路易。深度学习泛化是因为参数-函数图偏向简单函数。(2018)<a class="ae lk" href="https://arxiv.org/abs/1805.08522" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1805.08522</a>关于深度学习的大批量训练:泛化差距和尖锐极小值(2016)。<a class="ae lk" href="https://arxiv.org/abs/1609.04836" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1609.04836</a></p></div></div>    
</body>
</html>