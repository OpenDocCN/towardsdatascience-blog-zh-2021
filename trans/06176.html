<html>
<head>
<title>Mathematical Recipe of Dimensionality Reduction</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">降维的数学方法</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/mathematical-recipe-of-dimensionality-reduction-281ff37957e4?source=collection_archive---------31-----------------------#2021-06-02">https://towardsdatascience.com/mathematical-recipe-of-dimensionality-reduction-281ff37957e4?source=collection_archive---------31-----------------------#2021-06-02</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><figure class="gl gn jr js jt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi jq"><img src="../Images/87c54b93c41978d1f8123ce3e1cb842c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Dhk7WWxUAJy9xJsenQwb6w.jpeg"/></div></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">图像由<strong class="bd kf"> </strong> <a class="ae kg" href="https://unsplash.com/@joshstyle" rel="noopener ugc nofollow" target="_blank"> <strong class="bd kf">乔舒亚</strong> </a>上<a class="ae kg" href="https://unsplash.com" rel="noopener ugc nofollow" target="_blank"> <strong class="bd kf">下</strong> </a></p></figure><p id="e611" class="pw-post-body-paragraph kh ki it kj b kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le im bi lf translated"><span class="l lg lh li bm lj lk ll lm ln di">随着数据集中</span><em class="lo">变量</em>的增加，其维度也随之增加，这可能会带来以下挑战:</p><ol class=""><li id="16a1" class="lp lq it kj b kk kl ko kp ks lr kw ls la lt le lu lv lw lx bi translated">变量越多，数据可视化就越困难。</li><li id="af73" class="lp lq it kj b kk ly ko lz ks ma kw mb la mc le lu lv lw lx bi translated">对于特定的业务问题，所有的变量可能都不重要。</li><li id="5953" class="lp lq it kj b kk ly ko lz ks ma kw mb la mc le lu lv lw lx bi translated">更复杂的模型，因为模型试图从所有变量中学习，需要更多的计算时间。</li><li id="f9a1" class="lp lq it kj b kk ly ko lz ks ma kw mb la mc le lu lv lw lx bi translated">探索性数据分析变得困难。</li></ol><p id="2574" class="pw-post-body-paragraph kh ki it kj b kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">因此，<strong class="kj iu"> <em class="lo">降维</em> </strong>就是降低数据的维度，以保证其传达最大限度的信息的过程。有两种主要的方法来降低数据集的维度:<strong class="kj iu"> 1。</strong>通过<em class="lo">从基于不同标准的数据中只选择</em>有用的特征，或<strong class="kj iu"> 2。</strong>通过<em class="lo">从给定数据中提取</em>新特征。</p><h1 id="960c" class="md me it bd mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu mv mw mx my mz na bi translated"><strong class="ak">特征选择/减少</strong></h1><h1 id="c3af" class="md me it bd mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu mv mw mx my mz na bi translated">1.缺失值比率</h1><p id="6e36" class="pw-post-body-paragraph kh ki it kj b kk nb km kn ko nc kq kr ks nd ku kv kw ne ky kz la nf lc ld le im bi translated">对于数据集中的每个特征列，使用以下公式计算<strong class="kj iu"> <em class="lo">缺失值比率</em> </strong>:</p><figure class="nh ni nj nk gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi ng"><img src="../Images/4bc3b8666048520485f2020bda0403f5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NsGQ9yN1gpAfMWUGKcnONw.png"/></div></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">图一。计算缺失值比率的公式</p></figure><p id="6350" class="pw-post-body-paragraph kh ki it kj b kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">现在，决定一个阈值保持值，超过该值时，您将删除缺失值比率超过阈值的变量。然后，对于其余的缺失值，尝试找出原因，如数据收集过程中的无响应或错误或读取数据时的错误，并对缺失值进行插补。</p><h1 id="a3d9" class="md me it bd mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu mv mw mx my mz na bi translated">2.低方差</h1><p id="0f3d" class="pw-post-body-paragraph kh ki it kj b kk nb km kn ko nc kq kr ks nd ku kv kw ne ky kz la nf lc ld le im bi translated"><strong class="kj iu"> <em class="lo">方差</em> </strong>较低的变量(价差较小，因此所有值趋于相同)对目标变量的影响较小。因此，决定一个阈值，去掉方差小的变量。使用以下公式计算差异:</p><figure class="nh ni nj nk gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi nl"><img src="../Images/742235bfb90133b9854e183de35dcae3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JY2BIkuI6OokVHiInJTgtA.png"/></div></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">图二。计算方差的公式</p></figure><p id="0394" class="pw-post-body-paragraph kh ki it kj b kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">方差与<em class="lo">范围相关</em>。因此，在计算方差之前，<em class="lo">标准化</em>变量。</p><h1 id="a117" class="md me it bd mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu mv mw mx my mz na bi translated">3.高度相关</h1><p id="8a11" class="pw-post-body-paragraph kh ki it kj b kk nb km kn ko nc kq kr ks nd ku kv kw ne ky kz la nf lc ld le im bi translated">在建立模型时，没有必要保留所有<strong class="kj iu"> <em class="lo">相关</em> </strong>变量，因为它们都具有相似的特征(<em class="lo">消息/信息</em>)。此外，高相关性导致<em class="lo">多重共线性</em>问题。这是公式。</p><figure class="nh ni nj nk gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi nm"><img src="../Images/8423828b4ee6c6e953881d918a39c3af.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RUzO7MbVAgdSoDrJmea4iw.png"/></div></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">图三。计算相关性的公式</p></figure><p id="15ba" class="pw-post-body-paragraph kh ki it kj b kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">为了减少维度，计算所有独立变量之间的<em class="lo">相关性</em>，然后如果相关性超过阈值，则删除该变量。在两个变量之间，去掉与目标变量相关性低的一个。</p><h1 id="02f3" class="md me it bd mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu mv mw mx my mz na bi translated">4.反向特征消除</h1><p id="44d8" class="pw-post-body-paragraph kh ki it kj b kk nb km kn ko nc kq kr ks nd ku kv kw ne ky kz la nf lc ld le im bi translated">假设所有先前的<em class="lo">降维</em>方法都已实施，使用所有变量训练模型并计算其性能。<strong class="kj iu"> <em class="lo">一次消除</em> </strong>一个变量，每次计算性能。找出对性能影响不大的被剔除变量，直到不再有变量被剔除。</p><h1 id="c3dc" class="md me it bd mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu mv mw mx my mz na bi translated">5.正向特征选择</h1><p id="ec35" class="pw-post-body-paragraph kh ki it kj b kk nb km kn ko nc kq kr ks nd ku kv kw ne ky kz la nf lc ld le im bi translated">分别使用每个特征训练模型并检查性能，然后<strong class="kj iu"> <em class="lo">选择给出最佳性能的</em> </strong>变量。重复该过程，一次添加一个变量，产生最高改进的变量为<em class="lo">重新训练的变量</em>。除非模型的性能没有显著提高，否则重复整个过程。</p><p id="703a" class="pw-post-body-paragraph kh ki it kj b kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated"><strong class="kj iu"> </strong></p><h1 id="1765" class="md me it bd mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu mv mw mx my mz na bi translated">特征抽出</h1><h1 id="5dd8" class="md me it bd mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu mv mw mx my mz na bi translated">1.主成分分析</h1><p id="5810" class="pw-post-body-paragraph kh ki it kj b kk nb km kn ko nc kq kr ks nd ku kv kw ne ky kz la nf lc ld le im bi translated"><strong class="kj iu"> <em class="lo"> PCA </em> </strong>就是<em class="lo">无监督</em>的降维算法。让我们考虑一个例子，我们有一个2D数据，该数据是沿着<strong class="kj iu"> X1 </strong>的。</p><figure class="nh ni nj nk gt ju gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/48e55cf221ac270507c86311026303aa.png" data-original-src="https://miro.medium.com/v2/resize:fit:706/format:webp/1*yS-C8qxv8D3v4CervYr83A.jpeg"/></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">图4。实例一</p></figure><p id="d91f" class="pw-post-body-paragraph kh ki it kj b kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">我们可以使用低方差滤波器选择<strong class="kj iu"> X1 </strong>并消除<strong class="kj iu"> X2 </strong>(因为它有<em class="lo">低方差</em>)。</p><figure class="nh ni nj nk gt ju gh gi paragraph-image"><div class="gh gi no"><img src="../Images/ab60aeef9b419f2006009e83312a73a6.png" data-original-src="https://miro.medium.com/v2/resize:fit:846/format:webp/1*3CHGUiatFeFEprRgrCgLTg.jpeg"/></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">图五。实例二</p></figure><p id="c758" class="pw-post-body-paragraph kh ki it kj b kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">考虑这个例子，没有一个特征具有低方差。因此，我们需要找到新的轴，<strong class="kj iu"> Z1 </strong>和<strong class="kj iu"> Z2 </strong>，并沿着<strong class="kj iu"> Z1 </strong>对每个数据点进行投影(使用点积)。</p><p id="2b51" class="pw-post-body-paragraph kh ki it kj b kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated"><strong class="kj iu"> Z1 </strong>称为<strong class="kj iu"> <em class="lo">主成分</em> </strong>，在数据中具有<em class="lo">最大方差</em>的方向。这里我们看到，在2D，数据在一个平面内变化。类似地，在3D中，数据在2D平面上变化。因此，我们有两个主要组成部分。</p><p id="8d16" class="pw-post-body-paragraph kh ki it kj b kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated"><strong class="kj iu"> <em class="lo"> n维情况:</em> </strong>在<strong class="kj iu"> <em class="lo"> n </em> </strong>特征中，我们需要找到<strong class="kj iu"> <em class="lo"> k </em> </strong>独立方向的变异数据(<em class="lo">n-特征</em> : f₁、…、fₙ、<em class="lo">k-方向</em> : z₁、z₂、…、zₖ).因此，我们有<em class="lo">k-主成分</em>，<strong class="kj iu"> k &lt; n </strong>。这些新特征(<strong class="kj iu"> <em class="lo"> k </em> </strong>)相对于原始数据没有可解释的意义。</p><h2 id="b74a" class="np me it bd mf nq nr dn mj ns nt dp mn ks nu nv mr kw nw nx mv la ny nz mz oa bi translated">步伐</h2><h2 id="b772" class="np me it bd mf nq nr dn mj ns nt dp mn ks nu nv mr kw nw nx mv la ny nz mz oa bi translated">1.标准化:</h2><p id="cd6d" class="pw-post-body-paragraph kh ki it kj b kk nb km kn ko nc kq kr ks nd ku kv kw ne ky kz la nf lc ld le im bi translated">不同的特征有不同的范围和尺度，因此<em class="lo">方差</em>也不同。因此，使用<strong class="kj iu"> <em class="lo">标准化</em> </strong>使得每个特征对PCA算法的贡献相等。</p><h2 id="9a94" class="np me it bd mf nq nr dn mj ns nt dp mn ks nu nv mr kw nw nx mv la ny nz mz oa bi translated">2.协方差矩阵:</h2><p id="b971" class="pw-post-body-paragraph kh ki it kj b kk nb km kn ko nc kq kr ks nd ku kv kw ne ky kz la nf lc ld le im bi translated"><strong class="kj iu"> <em class="lo">协方差</em> </strong>是两个特征之间<em class="lo">相关性</em>的<em class="lo">强度</em>的度量。</p><figure class="nh ni nj nk gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi ob"><img src="../Images/fb9820dea8d9532547a3c0e1fab86339.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ylXwWhRnMISo57foK-AtSw.png"/></div></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">图六。计算协方差的公式</p></figure><p id="b686" class="pw-post-body-paragraph kh ki it kj b kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">一个<strong class="kj iu"> <em class="lo">正</em> </strong>协方差意味着两个特征一起<em class="lo">向同一个方向</em>移动，而<strong class="kj iu"> <em class="lo">负</em> </strong>协方差意味着两个特征反向<em class="lo">移动</em>。</p><figure class="nh ni nj nk gt ju gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/2038b367ae435e8ff5fa561594a302f5.png" data-original-src="https://miro.medium.com/v2/resize:fit:562/format:webp/1*Sph8X_62rCC48wR1n1BjzA.jpeg"/></div></figure><p id="c738" class="pw-post-body-paragraph kh ki it kj b kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">假设有三个特征:<strong class="kj iu"> <em class="lo"> Y₁ </em> </strong>，<strong class="kj iu"><em class="lo">y₂</em>&amp;<strong class="kj iu"><em class="lo">y₃</em></strong>，<strong class="kj iu"> <em class="lo"> V₁ </em> </strong>，<strong class="kj iu"><em class="lo">v₂</em></strong>&amp;<strong class="kj iu"><em class="lo">v₃</em></strong>是</strong>的方差这就是协方差矩阵的样子。</p><figure class="nh ni nj nk gt ju gh gi paragraph-image"><div class="gh gi od"><img src="../Images/aefa6a395d3dfe330cd913dad7f1453f.png" data-original-src="https://miro.medium.com/v2/resize:fit:462/format:webp/1*ckIMDEz_dQQwrrFtcLKFcQ.jpeg"/></div></figure><p id="03ab" class="pw-post-body-paragraph kh ki it kj b kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">现在，假设有三个特征:<strong class="kj iu"> <em class="lo"> A </em> </strong>，<strong class="kj iu"> <em class="lo"> B </em> </strong>，<strong class="kj iu"> <em class="lo"> C </em> </strong>有两排并且它们被<em class="lo">标准化</em>使得<strong class="kj iu">的意思是(A) </strong> = <strong class="kj iu">的意思是(B) </strong> = <strong class="kj iu">的意思是(C) </strong> = <strong class="kj iu"> 0 </strong>。设这个矩阵等于<strong class="kj iu"> <em class="lo"> X </em> </strong>。然后，</p><figure class="nh ni nj nk gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi oe"><img src="../Images/9d82f005888324e35c93f48b801d61b2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6qBIVhOOVdh0l4CM1OSkng.png"/></div></div></figure><figure class="nh ni nj nk gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi of"><img src="../Images/dc4bbf3c8c0a36fd56c8bfa531157458.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rtt8UCU0GGZbl48F8HTp0g.jpeg"/></div></div></figure><p id="8f5b" class="pw-post-body-paragraph kh ki it kj b kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">因此，<strong class="kj iu">协方差</strong> = <strong class="kj iu">转置</strong> ( <strong class="kj iu"> X </strong> ) <strong class="kj iu">。</strong>T<strong class="kj iu">T</strong></p><p id="e74f" class="pw-post-body-paragraph kh ki it kj b kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">因此，协方差矩阵是<strong class="kj iu">的<strong class="kj iu"> <em class="lo">矩阵乘</em> </strong> ( <em class="lo">点积</em>)转置</strong> <strong class="kj iu">的</strong><strong class="kj iu">X</strong>与<strong class="kj iu"> X </strong>。</p><h2 id="7194" class="np me it bd mf nq nr dn mj ns nt dp mn ks nu nv mr kw nw nx mv la ny nz mz oa bi translated">3.特征向量和特征值:</h2><p id="c39b" class="pw-post-body-paragraph kh ki it kj b kk nb km kn ko nc kq kr ks nd ku kv kw ne ky kz la nf lc ld le im bi translated"><strong class="kj iu"> <em class="lo">特征向量</em> </strong> ( <strong class="kj iu"> <em class="lo"> z </em> </strong>)是非零向量，与矩阵相乘得到<em class="lo">乘以常数<strong class="kj iu"> λ </strong>，<strong class="kj iu"> cov z </strong> = <strong class="kj iu"> λz </strong>。对于一个<em class="lo"> NxN </em>矩阵，有<strong class="kj iu"> <em class="lo"> N个</em> </strong> <em class="lo">相互独立</em>的特征向量<em class="lo">相互垂直</em>。</em></p><p id="061a" class="pw-post-body-paragraph kh ki it kj b kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">在PCA中，协方差矩阵的特征向量给出了数据方差的<em class="lo">方向</em>。<em class="lo">越大</em>的<strong class="kj iu">特征值</strong>，越<em class="lo">的</em>方向的方差越大<strong class="kj iu">特征向量</strong>。因此，<em class="lo">特征值</em>代表扩散的<em class="lo">幅度</em>。</p><figure class="nh ni nj nk gt ju gh gi paragraph-image"><div class="gh gi og"><img src="../Images/ea1eb0c92d5142869bb232ea5f2060e8.png" data-original-src="https://miro.medium.com/v2/resize:fit:722/format:webp/1*BlIb-RWA75XNDGY2EI1tcQ.jpeg"/></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">图7。公式化特征对</p></figure><h2 id="da27" class="np me it bd mf nq nr dn mj ns nt dp mn ks nu nv mr kw nw nx mv la ny nz mz oa bi translated">4.选择主成分:</h2><p id="2505" class="pw-post-body-paragraph kh ki it kj b kk nb km kn ko nc kq kr ks nd ku kv kw ne ky kz la nf lc ld le im bi translated">我们需要根据特征值选择<strong class="kj iu"> <em class="lo"> k </em> </strong>方向/特征向量。考虑一个<em class="lo"> 3x3 </em>协方差矩阵<strong class="kj iu"> C </strong>及其对应的特征值(<strong class="kj iu"> λ₁ </strong>，<strong class="kj iu"> λ₂ </strong>，<strong class="kj iu"> λ₃ </strong>)和特征向量(<strong class="kj iu"> V₁ </strong>，<strong class="kj iu"> V₂ </strong>，<strong class="kj iu"> V₃ </strong>)。</p><figure class="nh ni nj nk gt ju gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/6ede719258ae783d93ede74e4065cafd.png" data-original-src="https://miro.medium.com/v2/resize:fit:442/format:webp/1*aY8oZwq1dz5AfdC2PO19rQ.jpeg"/></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">图8。矩阵<strong class="bd kf"> C </strong></p></figure><p id="1e7f" class="pw-post-body-paragraph kh ki it kj b kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">下面是<a class="ae kg" href="https://lpsa.swarthmore.edu/MtrxVibe/EigMat/MatrixEigen.html" rel="noopener ugc nofollow" target="_blank"> <em class="lo">步骤</em> </a>计算<em class="lo">特征值</em>和<em class="lo">特征向量</em>。或者，可以在python中使用线性代数的库<strong class="kj iu"> <em class="lo"> scipy.linalg </em> </strong>来完成。</p><pre class="nh ni nj nk gt oi oj ok ol aw om bi"><span id="0739" class="np me it oj b gy on oo l op oq"><strong class="oj iu">In [1]:</strong> import scipy.linalg as la<br/>        import numpy as np</span><span id="d485" class="np me it oj b gy or oo l op oq"><strong class="oj iu">In [2]:</strong> A = np.array([[5,-1,0],[-1,8,3],[0,3,1]])<br/>        print(A)</span><span id="6f4e" class="np me it oj b gy or oo l op oq"><em class="lo">        [[ 5 -1  0]<br/>         [-1  8  3]<br/>         [ 0  3  1]]</em></span><span id="0e67" class="np me it oj b gy or oo l op oq"><strong class="oj iu">In [3]:</strong> EigenValue, EigenVector = la.eig(A)<br/><strong class="oj iu">In [4]:</strong> print(EigenValue)</span><span id="a014" class="np me it oj b gy or oo l op oq"><em class="lo">        [ 9.31426594+0.j  4.81939667+0.j -0.13366261+0.j]</em></span><span id="d7e0" class="np me it oj b gy or oo l op oq"><strong class="oj iu">In [5]:</strong> print(EigenVector)</span><span id="6f98" class="np me it oj b gy or oo l op oq"><em class="lo">        [[ 0.21302554  0.97462873 -0.06869469]<br/>         [-0.91904881  0.17602119 -0.35265538]<br/>         [-0.33161634  0.13825838  0.93322839]]</em></span></pre><p id="08c8" class="pw-post-body-paragraph kh ki it kj b kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">从上面看，我们有</p><figure class="nh ni nj nk gt ju gh gi paragraph-image"><div class="gh gi os"><img src="../Images/b3f3ed618baa78659a6e8dc8ab8c7d85.png" data-original-src="https://miro.medium.com/v2/resize:fit:812/format:webp/1*U6LDSUKVCKAqR46L6IBq5A.jpeg"/></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">图九。特征值和特征向量</p></figure><p id="6b52" class="pw-post-body-paragraph kh ki it kj b kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">现在，|λ₃| &gt; |λ₁| &gt; |λ₂|，总和= |λ₃| + |λ₁| + |λ₂| = 14.26</p><figure class="nh ni nj nk gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi ot"><img src="../Images/3e019484c038052b544f107d5fbeb333.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SsVptzCe_8pRCTZ0IPJWnw.png"/></div></div></figure><p id="84fc" class="pw-post-body-paragraph kh ki it kj b kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">Var₁ + Var₂ = 0.35</p><p id="0c4e" class="pw-post-body-paragraph kh ki it kj b kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">Var₂ + Var₃ =0.66</p><p id="9a29" class="pw-post-body-paragraph kh ki it kj b kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">Var₁ + Var₃ =0.99</p><p id="e6b1" class="pw-post-body-paragraph kh ki it kj b kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">由于<strong class="kj iu"> Var₁ </strong>和<strong class="kj iu"> Var₃ </strong>一起可以解释<em class="lo">数据中99% </em>的方差，我们可以从数据中去掉<strong class="kj iu"> Var₂ </strong>，因此<strong class="kj iu"> Var₁ </strong>和<strong class="kj iu"> Var₃ </strong>是数据的<strong class="kj iu"> <em class="lo">主成分</em> </strong>。</p><h1 id="6f2b" class="md me it bd mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu mv mw mx my mz na bi translated">2.要素分析</h1><p id="1667" class="pw-post-body-paragraph kh ki it kj b kk nb km kn ko nc kq kr ks nd ku kv kw ne ky kz la nf lc ld le im bi translated">考虑一个<em class="lo">的例子</em>:</p><figure class="nh ni nj nk gt ju gh gi paragraph-image"><div class="gh gi ou"><img src="../Images/0100d632fead172adfbc7d397b5a305f.png" data-original-src="https://miro.medium.com/v2/resize:fit:582/format:webp/1*PPOtgjfKcSFHEhxSs3VMGg.png"/></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">图10。实例三</p></figure><p id="9e9c" class="pw-post-body-paragraph kh ki it kj b kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">一般来说<em class="lo">经验</em>和<em class="lo">薪资</em>有很好的(正)相关性。一个新的变量<strong class="kj iu"> <em class="lo">值</em> </strong>可以定义这两个变量。</p><p id="378f" class="pw-post-body-paragraph kh ki it kj b kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated"><strong class="kj iu">值</strong>可以认为是<em class="lo">经验</em>和<em class="lo">薪资</em>两个变量的线性组合，或者你可以说特性<em class="lo">经验</em>或<em class="lo">薪资</em>可以由<strong class="kj iu">值</strong>导出。因此，<strong class="kj iu">值</strong>被称为<strong class="kj iu"> <em class="lo">因子</em> </strong>。</p><p id="7de3" class="pw-post-body-paragraph kh ki it kj b kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated"><em class="lo">线性</em>组合相关特征的过程称为<strong class="kj iu"> <em class="lo">因子分析</em> </strong>。<strong class="kj iu">因子</strong>是<em class="lo">原</em>变量的线性组合。</p><h2 id="213e" class="np me it bd mf nq nr dn mj ns nt dp mn ks nu nv mr kw nw nx mv la ny nz mz oa bi translated">目标</h2><ol class=""><li id="44b3" class="lp lq it kj b kk nb ko nc ks ov kw ow la ox le lu lv lw lx bi translated">对<em class="lo">减少</em>变量的数量。</li><li id="45c7" class="lp lq it kj b kk ly ko lz ks ma kw mb la mc le lu lv lw lx bi translated"><em class="lo">考察</em>变量之间的关系。</li><li id="7256" class="lp lq it kj b kk ly ko lz ks ma kw mb la mc le lu lv lw lx bi translated"><em class="lo">解决</em>多重共线性的问题。</li></ol><h2 id="1a41" class="np me it bd mf nq nr dn mj ns nt dp mn ks nu nv mr kw nw nx mv la ny nz mz oa bi translated">假设</h2><ol class=""><li id="a778" class="lp lq it kj b kk nb ko nc ks ov kw ow la ox le lu lv lw lx bi translated">原始变量应<em class="lo">规格化</em>。</li><li id="5c86" class="lp lq it kj b kk ly ko lz ks ma kw mb la mc le lu lv lw lx bi translated">因素是相互独立的。</li><li id="639e" class="lp lq it kj b kk ly ko lz ks ma kw mb la mc le lu lv lw lx bi translated">存在一些潜在的<em class="lo">因素</em>可以<em class="lo">描述</em>原始变量。</li></ol><h2 id="05c0" class="np me it bd mf nq nr dn mj ns nt dp mn ks nu nv mr kw nw nx mv la ny nz mz oa bi translated">类型</h2><ol class=""><li id="d6b5" class="lp lq it kj b kk nb ko nc ks ov kw ow la ox le lu lv lw lx bi translated"><strong class="kj iu">探索性因素分析(EFA): </strong>在<strong class="kj iu"> <em class="lo"> EFA </em> </strong>中，确定变量之间的关系，并将属于<em class="lo">相似</em> <em class="lo">概念</em>的变量分组。没有关于因素之间关系数量的预先假设。</li><li id="ceae" class="lp lq it kj b kk ly ko lz ks ma kw mb la mc le lu lv lw lx bi translated"><strong class="kj iu">验证性因素分析(CFA):<strong class="kj iu"><em class="lo">CFA</em></strong>中的</strong>，对<em class="lo">因素</em>的数量进行假设，检验变量与<strong class="kj iu"> <em class="lo"> n </em> </strong>具体<em class="lo">因素</em>相关的假设。</li></ol><h2 id="9c37" class="np me it bd mf nq nr dn mj ns nt dp mn ks nu nv mr kw nw nx mv la ny nz mz oa bi translated">步伐</h2><p id="7847" class="pw-post-body-paragraph kh ki it kj b kk nb km kn ko nc kq kr ks nd ku kv kw ne ky kz la nf lc ld le im bi translated">1.<strong class="kj iu">验证数据需求:</strong></p><ul class=""><li id="1970" class="lp lq it kj b kk kl ko kp ks lr kw ls la lt le oy lv lw lx bi translated">样本量:</li></ul><figure class="nh ni nj nk gt ju gh gi paragraph-image"><div class="gh gi oz"><img src="../Images/0bdb99b681df4ac66a00a994f05105ec.png" data-original-src="https://miro.medium.com/v2/resize:fit:402/format:webp/1*byDgBMonZCwjVbHf2eo4GQ.jpeg"/></div></figure><ul class=""><li id="d22f" class="lp lq it kj b kk kl ko kp ks lr kw ls la lt le oy lv lw lx bi translated">样本与变量的比率应为15:1或更高</li><li id="3bfd" class="lp lq it kj b kk ly ko lz ks ma kw mb la mc le oy lv lw lx bi translated">变量的相关值:</li></ul><figure class="nh ni nj nk gt ju gh gi paragraph-image"><div class="gh gi oz"><img src="../Images/3f147047b178f3fd0b6ebb1f058a653e.png" data-original-src="https://miro.medium.com/v2/resize:fit:402/format:webp/1*btd1oZykXZRbH3nnIyORhQ.jpeg"/></div></figure><p id="93f9" class="pw-post-body-paragraph kh ki it kj b kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">2.<strong class="kj iu">取变量</strong></p><p id="b702" class="pw-post-body-paragraph kh ki it kj b kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">为了提取<em class="lo">因子</em>，让我们考虑原始变量(<strong class="kj iu"> Y₁ </strong>、<strong class="kj iu"> Y₂ </strong>和<strong class="kj iu"> Y₃ </strong>)和要提取的<em class="lo">因子</em>(<strong class="kj iu">f₁</strong>和<strong class="kj iu"> F₂ </strong>)。</p><figure class="nh ni nj nk gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi pa"><img src="../Images/85e4ec0a55e6c55700d6df23f2d3ac79.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3xcGJVqHauRv1uIYfDuXKA.png"/></div></div></figure><p id="ecbb" class="pw-post-body-paragraph kh ki it kj b kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated"><strong class="kj iu"> <em class="lo">因子加载</em> </strong>定义了每个<em class="lo">因子</em>在定义原始变量时<em class="lo">的贡献</em>是多少。</p><p id="e925" class="pw-post-body-paragraph kh ki it kj b kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">3.<strong class="kj iu">假设</strong></p><ul class=""><li id="6a2d" class="lp lq it kj b kk kl ko kp ks lr kw ls la lt le oy lv lw lx bi translated">错误术语<strong class="kj iu"> <em class="lo"> eᵢ </em> </strong>相互独立。【<strong class="kj iu">表示(<em class="lo">eᵢ</em>)</strong>=<strong class="kj iu">0</strong>和<strong class="kj iu">var(<em class="lo">eᵢ</em>)</strong>=<strong class="kj iu">σᵢ</strong>)</li><li id="f0e2" class="lp lq it kj b kk ly ko lz ks ma kw mb la mc le oy lv lw lx bi translated"><em class="lo">因子</em>(<strong class="kj iu">【fⱼ】</strong>)是相互独立的<em class="lo"/>，并且带有误差项。[ <strong class="kj iu"> mean(Fⱼ) </strong> = <strong class="kj iu"> 0 </strong>和<strong class="kj iu"> var(Fⱼ) </strong> = <strong class="kj iu"> 1 </strong></li></ul><figure class="nh ni nj nk gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi pb"><img src="../Images/45fe5cf9e040abdfb638e88ff4dd6905.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dBbe_rZ_7c21O0zQV1l36A.png"/></div></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">图11。<strong class="bd kf"> </strong>方程式1</p></figure><p id="2993" class="pw-post-body-paragraph kh ki it kj b kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">4.<strong class="kj iu">观测协方差矩阵</strong></p><p id="31ed" class="pw-post-body-paragraph kh ki it kj b kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">设<strong class="kj iu"> <em class="lo"> S </em> </strong>为数据的实际已知值。那么观察到的协方差矩阵变成:</p><figure class="nh ni nj nk gt ju gh gi paragraph-image"><div class="gh gi pc"><img src="../Images/f72aaa429e4f88537e30562b974fc2fd.png" data-original-src="https://miro.medium.com/v2/resize:fit:522/format:webp/1*sqel_fZgNc95JxfzcMucTQ.jpeg"/></div></figure><p id="56e9" class="pw-post-body-paragraph kh ki it kj b kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">5.<strong class="kj iu">计算协方差</strong></p><p id="0d61" class="pw-post-body-paragraph kh ki it kj b kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">现在，我们需要根据<em class="lo">因子负载</em>计算协方差。为此，我们需要计算因子权重的<strong class="kj iu"> β </strong> <strong class="kj iu"/>值。</p><p id="ac9c" class="pw-post-body-paragraph kh ki it kj b kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">从<em class="lo">方差</em>的性质，我知道</p><figure class="nh ni nj nk gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi pd"><img src="../Images/2da326c767b48bd9d6b227c26b747de8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*StD_NjjcL1bQ3nYVYkG7Og.png"/></div></div></figure><p id="f3a3" class="pw-post-body-paragraph kh ki it kj b kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">然后是<em class="lo">等式1 </em>(图11。)变成了，</p><figure class="nh ni nj nk gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi pe"><img src="../Images/a6f516faa66f7cab32181d7f168bdc81.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cpsnebtCQlPcvL-60zCMMA.png"/></div></div></figure><p id="e4ff" class="pw-post-body-paragraph kh ki it kj b kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">还有，</p><figure class="nh ni nj nk gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi pf"><img src="../Images/8baebc796853dbb2af99dff61ff78797.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*y2XzKLEGXs9wUTKFjZogWw.png"/></div></div></figure><p id="be4d" class="pw-post-body-paragraph kh ki it kj b kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">在理想情况下，<em class="lo">变量</em>的所有<em class="lo">变量</em>都可以用<strong class="kj iu"> <em class="lo">因子</em> </strong>来解释。因此，</p><figure class="nh ni nj nk gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi pg"><img src="../Images/fd69ab58a5d0bb5f5760b73dc6a827cc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dkFD8wGz6UFbKpwjQphEcA.png"/></div></div></figure><p id="16f8" class="pw-post-body-paragraph kh ki it kj b kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">6.<strong class="kj iu">计算协方差矩阵</strong></p><p id="9c6f" class="pw-post-body-paragraph kh ki it kj b kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">我们需要<strong class="kj iu"> <em class="lo">因子加载</em> </strong> ( <strong class="kj iu"> β </strong>)和<strong class="kj iu"> σ </strong>的值。实际值也是我们已知的(<strong class="kj iu"> S₁ </strong>、<strong class="kj iu"> S₂ </strong>、<strong class="kj iu"> S₃ </strong>)，解下面的方程就可以得到<strong class="kj iu">β</strong>s和<strong class="kj iu">σ</strong>s</p><figure class="nh ni nj nk gt ju gh gi paragraph-image"><div class="gh gi ph"><img src="../Images/7475eb5524b0ce22ddcc535538719a56.png" data-original-src="https://miro.medium.com/v2/resize:fit:962/format:webp/1*V2Fo-wVM9XJ4iRpX97d_5w.jpeg"/></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">图12 <strong class="bd kf">最终协方差矩阵</strong></p></figure><p id="78a7" class="pw-post-body-paragraph kh ki it kj b kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">因此，我们可以将<strong class="kj iu"> <em class="lo">因子载荷</em> </strong>和<strong class="kj iu"> <em class="lo">因子</em> </strong>计算为<em class="lo">变量</em>的<em class="lo">线性组合。因子加载表示每个变量与<strong class="kj iu">潜在因子</strong>的<em class="lo">关系</em>。</em></p><h1 id="d80a" class="md me it bd mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu mv mw mx my mz na bi translated">结论</h1><p id="6812" class="pw-post-body-paragraph kh ki it kj b kk nb km kn ko nc kq kr ks nd ku kv kw ne ky kz la nf lc ld le im bi translated">在多维数据中寻找趋势是困难的。当数据庞大且有多个自变量时，存储、处理和可视化数据变得困难。因此，<strong class="kj iu"> <em class="lo">降维</em> </strong>有助于缓解这些问题，同时保留有用的特征。</p><figure class="nh ni nj nk gt ju gh gi paragraph-image"><div class="gh gi pi"><img src="../Images/5e26840c60421a280d4fbb896e9a566d.png" data-original-src="https://miro.medium.com/v2/resize:fit:40/format:webp/1*eUIGCxFxLFXZRNthiz4RKQ.png"/></div><p class="kb kc gj gh gi kd ke bd b be z dk translated"><a class="ae kg" href="https://www.linkedin.com/in/shubhamdhingra27/" rel="noopener ugc nofollow" target="_blank">https://www.linkedin.com/in/shubhamdhingra27/</a></p></figure></div></div>    
</body>
</html>