<html>
<head>
<title>Logic Explained Networks</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">逻辑解释网络</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/logic-explained-networks-d81c8d119bde?source=collection_archive---------18-----------------------#2021-08-12">https://towardsdatascience.com/logic-explained-networks-d81c8d119bde?source=collection_archive---------18-----------------------#2021-08-12</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="9548" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsdatascience.com/tagged/thoughts-and-theory" rel="noopener" target="_blank">思想和理论</a></h2><div class=""/><div class=""><h2 id="011d" class="pw-subtitle-paragraph jw iz iq bd b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn dk translated">深度学习模型<em class="ko">可通过设计解释</em></h2></div><p id="f3f0" class="pw-post-body-paragraph kp kq iq kr b ks kt ka ku kv kw kd kx ky kz la lb lc ld le lf lg lh li lj lk ij bi translated"><strong class="kr ja">TL；博士</strong></p><ul class=""><li id="6045" class="ll lm iq kr b ks kt kv kw ky ln lc lo lg lp lk lq lr ls lt bi translated"><strong class="kr ja">问题— </strong>神经网络无法解释<em class="lu">它们如何</em>得出预测，因此不鼓励在安全关键应用中部署它们。</li><li id="1138" class="ll lm iq kr b ks lv kv lw ky lx lc ly lg lz lk lq lr ls lt bi translated"><strong class="kr ja">解决方案</strong> — <a class="ae ma" href="https://arxiv.org/abs/2108.05149" rel="noopener ugc nofollow" target="_blank">逻辑解释网络</a>是新颖的“<em class="lu">可解释的设计”</em>深度学习模型，为其预测提供逻辑解释。</li><li id="89b6" class="ll lm iq kr b ks lv kv lw ky lx lc ly lg lz lk lq lr ls lt bi translated"><strong class="kr ja">试一试！PyTorch，解释一下！”是一个python包，提供简单的API来实现逻辑解释的网络。</strong></li></ul><figure class="mc md me mf gt mg gh gi paragraph-image"><div role="button" tabindex="0" class="mh mi di mj bf mk"><div class="gh gi mb"><img src="../Images/d4741bceea007ce50347e5dbb696f963.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3sdRX6r9E_0fGwCkU5L2yA.png"/></div></div><p class="mn mo gj gh gi mp mq bd b be z dk translated">透镜是基于概念的神经分类器，为它们的决策提供一阶逻辑解释。图片由作者提供。</p></figure></div><div class="ab cl mr ms hu mt" role="separator"><span class="mu bw bk mv mw mx"/><span class="mu bw bk mv mw mx"/><span class="mu bw bk mv mw"/></div><div class="ij ik il im in"><h1 id="5bd1" class="my mz iq bd na nb nc nd ne nf ng nh ni kf nj kg nk ki nl kj nm kl nn km no np bi translated">你为什么会感兴趣？</h1><p id="c23d" class="pw-post-body-paragraph kp kq iq kr b ks nq ka ku kv nr kd kx ky ns la lb lc nt le lf lg nu li lj lk ij bi translated">我——你是一名研究人员还是一个拥有超级酷的深度学习系统的startupper，你想部署它来拯救世界吗？嗯，你可能不能……</p><p id="40bd" class="pw-post-body-paragraph kp kq iq kr b ks kt ka ku kv kw kd kx ky kz la lb lc ld le lf lg lh li lj lk ij bi translated"><strong class="kr ja">你— </strong>为什么？</p><p id="511d" class="pw-post-body-paragraph kp kq iq kr b ks kt ka ku kv kw kd kx ky kz la lb lc ld le lf lg lh li lj lk ij bi translated">因为深度神经网络是“黑箱”:它们不是被设计来解释它们如何做出预测的！</p><p id="0ec8" class="pw-post-body-paragraph kp kq iq kr b ks kt ka ku kv kw kd kx ky kz la lb lc ld le lf lg lh li lj lk ij bi translated"><strong class="kr ja">你— </strong>那又怎么样？</p><p id="9da3" class="pw-post-body-paragraph kp kq iq kr b ks kt ka ku kv kw kd kx ky kz la lb lc ld le lf lg lh li lj lk ij bi translated">嗯，现在在美国和欧洲，许多安全关键应用程序都禁止使用“黑匣子”(如果你不相信我，可以去看看https://gdpr.eu/的<a class="ae ma" href="https://gdpr.eu/" rel="noopener ugc nofollow" target="_blank"/>)。</p><p id="abeb" class="pw-post-body-paragraph kp kq iq kr b ks kt ka ku kv kw kd kx ky kz la lb lc ld le lf lg lh li lj lk ij bi translated"><strong class="kr ja">你— </strong>那么，我能做什么呢？</p><p id="fdfc" class="pw-post-body-paragraph kp kq iq kr b ks kt ka ku kv kw kd kx ky kz la lb lc ld le lf lg lh li lj lk ij bi translated"><strong class="kr ja">我— </strong>阅读这篇文章，了解如何安全和<strong class="kr ja">合法</strong>地部署您最先进的深度学习系统<strong class="kr ja">！:-)</strong></p></div><div class="ab cl mr ms hu mt" role="separator"><span class="mu bw bk mv mw mx"/><span class="mu bw bk mv mw mx"/><span class="mu bw bk mv mw"/></div><div class="ij ik il im in"><h1 id="9a42" class="my mz iq bd na nb nc nd ne nf ng nh ni kf nj kg nk ki nl kj nm kl nn km no np bi translated">可解释的人工智能中的知识缺口(XAI)</h1><h2 id="f6c0" class="nv mz iq bd na nw nx dn ne ny nz dp ni ky oa ob nk lc oc od nm lg oe of no iw bi translated">为什么我们不能使用(标准的)深度学习来解决现实世界的问题？</h2><p id="791e" class="pw-post-body-paragraph kp kq iq kr b ks nq ka ku kv nr kd kx ky ns la lb lc nt le lf lg nu li lj lk ij bi translated">深度学习(DL)在安全关键领域的应用受到立法者的强烈限制，因为DL模型通常被认为是黑盒，其决策过程不透明且过于复杂，外行人(甚至DL专家)都难以理解！).</p><p id="e3a2" class="pw-post-body-paragraph kp kq iq kr b ks kt ka ku kv kw kd kx ky kz la lb lc ld le lf lg lh li lj lk ij bi translated">出于这个原因，可解释的人工智能(XAI)研究要么专注于<em class="lu">解释</em>黑盒决策，要么专注于开发机器学习模型“<em class="lu">可通过设计</em>”(如决策树)。然而，虽然可解释的模型产生了对其预测的信任(这就是我们喜欢它们的原因…) [1]，但黑盒模型，如神经网络，通常是那些提供最先进的任务性能的模型(这就是我们喜欢它们的原因！) [2].</p><h2 id="3199" class="nv mz iq bd na nw nx dn ne ny nz dp ni ky oa ob nk lc oc od nm lg oe of no iw bi translated">XAI研究中的知识缺口</h2><p id="fc58" class="pw-post-body-paragraph kp kq iq kr b ks nq ka ku kv nr kd kx ky ns la lb lc nt le lf lg nu li lj lk ij bi translated">大多数技术<em class="lu">解释</em>黑盒专注于寻找或排列黑盒用来做预测的最相关的特征。这种“特征评分”方法非常有效并被广泛使用，但<strong class="kr ja"> <em class="lu">它们无法解释</em> </strong>神经网络如何组成这样的特征来进行预测【3】。此外，大多数<em class="lu">解释</em>方法的一个关键问题是，解释是根据输入特征(例如像素强度)给出的，这些输入特征并不对应于人类容易理解的高级类别[4]。</p><p id="60bb" class="pw-post-body-paragraph kp kq iq kr b ks kt ka ku kv kw kd kx ky kz la lb lc ld le lf lg lh li lj lk ij bi translated">为了克服这个问题，<strong class="kr ja">基于概念的</strong>方法变得越来越流行，因为它们根据人类可理解的类别(即"<strong class="kr ja"> <em class="lu">概念</em> </strong> <em class="lu"> " </em>)而不是原始特征来提供解释[4]。简单地说，基于概念的模型是一个函数<em class="lu"> f </em>将分类输入<em class="lu"> C </em>(即“概念”)映射到分类输出<em class="lu"> Y </em>(即目标类):</p><p id="b963" class="pw-post-body-paragraph kp kq iq kr b ks kt ka ku kv kw kd kx ky kz la lb lc ld le lf lg lh li lj lk ij bi translated"><em class="lu"> f: C </em> ↦ <em class="lu"> Y </em></p><p id="b51b" class="pw-post-body-paragraph kp kq iq kr b ks kt ka ku kv kw kd kx ky kz la lb lc ld le lf lg lh li lj lk ij bi translated">如果您的输入特征不是分类的(例如图像的像素亮度)，您可以首先使用函数<em class="lu"> g </em>将您的输入<em class="lu"> X </em>映射到分类的“概念空间”<em class="lu"> C </em>:</p><p id="d728" class="pw-post-body-paragraph kp kq iq kr b ks kt ka ku kv kw kd kx ky kz la lb lc ld le lf lg lh li lj lk ij bi translated"><em class="lu"> g: X </em> ↦ <em class="lu"> C </em></p><p id="f712" class="pw-post-body-paragraph kp kq iq kr b ks kt ka ku kv kw kd kx ky kz la lb lc ld le lf lg lh li lj lk ij bi translated">看看这个基于CUB数据集的例子(一个根据图像[5]进行鸟类分类的数据集):</p><ul class=""><li id="390b" class="ll lm iq kr b ks kt kv kw ky ln lc lo lg lp lk lq lr ls lt bi translated">首先，分类器<em class="lu"> g </em>学习如何将像素强度(图像的输入特征)映射到分类概念空间<em class="lu"> C </em>中，其中每个类别对应于一种鸟类特征；</li><li id="83fc" class="ll lm iq kr b ks lv kv lw ky lx lc ly lg lz lk lq lr ls lt bi translated">第二，分类器<em class="lu"> f </em>学习如何将根据概念<em class="lu"> C </em>描述的输入样本映射到对应于鸟类名称(目标类别)的分类输出。</li></ul><figure class="mc md me mf gt mg gh gi paragraph-image"><div role="button" tabindex="0" class="mh mi di mj bf mk"><div class="gh gi og"><img src="../Images/987d4eca80dfd1bc3b744593e7be7ba1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_dwQgA0ZcNpVUZdriBpppQ.png"/></div></div><p class="mn mo gj gh gi mp mq bd b be z dk translated">一个黑盒概念瓶颈模型。图片由作者提供。</p></figure><p id="1636" class="pw-post-body-paragraph kp kq iq kr b ks kt ka ku kv kw kd kx ky kz la lb lc ld le lf lg lh li lj lk ij bi translated">然而，尽管是有用的，这些基于概念的方法中的大多数集中在最相关概念的识别上，但是它们不能解释<strong class="kr ja"><em class="lu"/></strong><em class="lu"/>这样的概念如何被分类器<em class="lu"> f </em>利用，更不能提供<strong class="kr ja"> <em class="lu">简明的解释</em> </strong>，其有效性可以被定量评估<strong class="kr ja"><em class="lu"/></strong>。</p><blockquote class="oh"><p id="8a38" class="oi oj iq bd ok ol om on oo op oq lk dk translated">很少有方法能够解释<em class="ko">神经网络如何组成特征/概念来进行预测。</em></p></blockquote><h2 id="484f" class="nv mz iq bd na nw or dn ne ny os dp ni ky ot ob nk lc ou od nm lg ov of no iw bi translated">那你需要什么？</h2><p id="ff5e" class="pw-post-body-paragraph kp kq iq kr b ks nq ka ku kv nr kd kx ky ns la lb lc nt le lf lg nu li lj lk ij bi translated">总之，你需要的是一个<strong class="kr ja"> <em class="lu">新的深度学习范例</em> </strong>允许你部署模型:</p><ol class=""><li id="fea1" class="ll lm iq kr b ks kt kv kw ky ln lc lo lg lp lk ow lr ls lt bi translated"><strong class="kr ja">实现接近等效“标准”黑盒所能达到的最先进水平的分类精度。</strong></li><li id="d201" class="ll lm iq kr b ks lv kv lw ky lx lc ly lg lz lk ow lr ls lt bi translated"><strong class="kr ja">解释</strong>如何组合输入特征以进行预测。</li><li id="185c" class="ll lm iq kr b ks lv kv lw ky lx lc ly lg lz lk ow lr ls lt bi translated"><strong class="kr ja">提供</strong> <em class="lu">清晰的</em>解释，其质量可量化<em class="lu"/>。</li></ol><p id="7b8b" class="pw-post-body-paragraph kp kq iq kr b ks kt ka ku kv kw kd kx ky kz la lb lc ld le lf lg lh li lj lk ij bi translated">您还想安全合法地部署您最先进的DL供电系统吗？</p><p id="593c" class="pw-post-body-paragraph kp kq iq kr b ks kt ka ku kv kw kd kx ky kz la lb lc ld le lf lg lh li lj lk ij bi translated">继续阅读…</p></div><div class="ab cl mr ms hu mt" role="separator"><span class="mu bw bk mv mw mx"/><span class="mu bw bk mv mw mx"/><span class="mu bw bk mv mw"/></div><div class="ij ik il im in"><h1 id="1d59" class="my mz iq bd na nb nc nd ne nf ng nh ni kf nj kg nk ki nl kj nm kl nn km no np bi translated">逻辑解释网络</h1><p id="5a14" class="pw-post-body-paragraph kp kq iq kr b ks nq ka ku kv nr kd kx ky ns la lb lc nt le lf lg nu li lj lk ij bi translated">逻辑解释网络(或透镜)是一种特殊的基于概念的神经网络，为其决策提供一阶逻辑(FOL)解释。</p><p id="c910" class="pw-post-body-paragraph kp kq iq kr b ks kt ka ku kv kw kd kx ky kz la lb lc ld le lf lg lh li lj lk ij bi translated">看看CUB数据集上的同一个例子:</p><ul class=""><li id="e743" class="ll lm iq kr b ks kt kv kw ky ln lc lo lg lp lk lq lr ls lt bi translated">分类器<em class="lu"> g </em>执行相同的动作(即从图像预测概念)；</li><li id="fe0e" class="ll lm iq kr b ks lv kv lw ky lx lc ly lg lz lk lq lr ls lt bi translated">然而，分类器<em class="lu"> f </em>现在是一个<strong class="kr ja"> <em class="lu">逻辑解释的网络</em> </strong>，提供对目标类的预测+逻辑公式，解释网络<em class="lu"> f </em>如何利用输入概念来达成决策！</li></ul><figure class="mc md me mf gt mg gh gi paragraph-image"><div role="button" tabindex="0" class="mh mi di mj bf mk"><div class="gh gi mb"><img src="../Images/d4741bceea007ce50347e5dbb696f963.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3sdRX6r9E_0fGwCkU5L2yA.png"/></div></div><p class="mn mo gj gh gi mp mq bd b be z dk translated">一个逻辑解释网络在行动！图片由作者提供。</p></figure><h2 id="2255" class="nv mz iq bd na nw nx dn ne ny nz dp ni ky oa ob nk lc oc od nm lg oe of no iw bi translated">为什么是逻辑解释？</h2><p id="c696" class="pw-post-body-paragraph kp kq iq kr b ks nq ka ku kv nr kd kx ky ns la lb lc nt le lf lg nu li lj lk ij bi translated">与其他XAI技术相比，一阶逻辑解释提供了许多关键优势:</p><ol class=""><li id="f0d3" class="ll lm iq kr b ks kt kv kw ky ln lc lo lg lp lk ow lr ls lt bi translated"><strong class="kr ja">明晰度</strong>—FOL中报告的解释是一种严谨而明确的陈述。<strong class="kr ja"> </strong>这种形式上的清晰可以服务于认知行为目的，例如产生信任、帮助偏见识别或采取行动/决策。例如，为了简单起见，省略了量词和变量，公式“斯诺∧树↔狼”可以很容易地概括训练数据集合中偏差的存在。</li><li id="8a24" class="ll lm iq kr b ks lv kv lw ky lx lc ly lg lz lk ow lr ls lt bi translated"><strong class="kr ja">模块性</strong> —不同的基于逻辑的解释可以组合起来描述一组观察结果或全局现象。例如，对于只显示人脸的图像，解释可以是“鼻子∧嘴唇→人”，而对于从后面显示人的另一个图像，有效的解释可以是“脚∧头发∧耳朵→人”。两种局部解释可以组合成“(鼻子∧嘴唇)∨(脚∧头发∧耳朵)→人”。</li><li id="b136" class="ll lm iq kr b ks lv kv lw ky lx lc ly lg lz lk ow lr ls lt bi translated"><strong class="kr ja">可测量性</strong> —基于逻辑的解释的质量可以定量测量，以检查其有效性和完整性。例如，一旦为类“人”提取了解释“(鼻子∧嘴唇)∨(脚∧头发∧耳朵)”，该逻辑公式可应用于测试集，以检查其在诸如准确性、保真度和一致性等量化度量方面的通用性。</li><li id="c690" class="ll lm iq kr b ks lv kv lw ky lx lc ly lg lz lk ow lr ls lt bi translated"><strong class="kr ja">可简化性</strong> —下面的解释可以用等价的形式重写，例如<em class="lu">析取范式</em> (DNF)和<em class="lu">合取范式</em> (CNF)。此外，奎因-麦克劳斯基算法等技术可以用来简化逻辑解释。例如，公式“(人∧鼻子)∨(人∧鼻子)’”可以很容易地简化为“鼻子”。</li></ol><blockquote class="oh"><p id="849c" class="oi oj iq bd ok ol ox oy oz pa pb lk dk translated">逻辑解释网络是一种特殊的基于概念的神经网络，为其决策提供一阶逻辑解释。</p></blockquote><h2 id="688c" class="nv mz iq bd na nw or dn ne ny os dp ni ky ot ob nk lc ou od nm lg ov of no iw bi translated">镜头范式:设计的可解释性</h2><p id="d289" class="pw-post-body-paragraph kp kq iq kr b ks nq ka ku kv nr kd kx ky ns la lb lc nt le lf lg nu li lj lk ij bi translated">透镜族是一类可通过设计 解释的神经模型<strong class="kr ja"> <em class="lu">。</em></strong></p><p id="dce3" class="pw-post-body-paragraph kp kq iq kr b ks kt ka ku kv kw kd kx ky kz la lb lc ld le lf lg lh li lj lk ij bi translated"><em class="lu">“你所说的可以通过设计来解释的模型是什么意思？”</em>，你可能会问。嗯，在<strong class="kr ja"> <em class="lu">事后</em> </strong>方法和LEN范式之间有着明显的区别，LEN范式是<strong class="kr ja"> <em class="lu">通过设计</em> </strong>来解释的。事后方法通常不对分类器施加约束:模型本身不受任何与解释方法相关的约束。这就是为什么这类方法被称为<strong class="kr ja"> <em class="lu">临时</em> </strong>的原因:在通常的训练完成后，XAI方法开始生效。相反，透镜范例是可通过设计 解释的<strong class="kr ja"> <em class="lu">，因为它在架构和损失函数中都嵌入了额外的约束，使得网络是自解释的。这就是为什么我们说透镜是可以通过设计来解释:分类器本身被限制以一种使解释自动出现的方式学习。</em></strong></p><figure class="mc md me mf gt mg gh gi paragraph-image"><div role="button" tabindex="0" class="mh mi di mj bf mk"><div class="gh gi pc"><img src="../Images/bf645a6d27ef937d81e0b15d7bd5e364.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VM1QjW_L-PDJ7MwlOhBpOw.jpeg"/></div></div><p class="mn mo gj gh gi mp mq bd b be z dk translated">如果你试图在实验后才理解“黑匣子”里发生了什么，你是在使用事后方法！来自<a class="ae ma" href="https://www.maxpixel.net/Schroedinger-Physics-Schroedingers-Cat-3864563" rel="noopener ugc nofollow" target="_blank">最大像素</a>的图像。</p></figure><blockquote class="oh"><p id="94ac" class="oi oj iq bd ok ol ox oy oz pa pb lk dk translated">透镜范式可以通过设计来解释，因为它在架构和损失函数中嵌入了额外的约束，使得解释自动出现。</p></blockquote><h2 id="2847" class="nv mz iq bd na nw or dn ne ny os dp ni ky ot ob nk lc ou od nm lg ov of no iw bi translated">理论细节(只有一点，我保证！)</h2><p id="ef85" class="pw-post-body-paragraph kp kq iq kr b ks nq ka ku kv nr kd kx ky ns la lb lc nt le lf lg nu li lj lk ij bi translated">一个逻辑解释网络的设计需要三个方面的规范:架构<strong class="kr ja">、</strong>、<strong class="kr ja">损失函数</strong>和<strong class="kr ja">简约准则</strong>。这三个元素的混合允许逻辑解释网络的唯一标识。虽然架构和损失函数是任何神经网络的标准要求，<strong class="kr ja"> <em class="lu">简约标准对LENs起着特殊的作用</em> </strong>，因为它们允许分类器模仿人类学习的方式并提供解释。事实上，当人类比较一组概述相同结果的假设时，他们倾向于对最简单的假设有一种隐含的偏见。亚里士多德在他的“后验分析”中观察到了这一现象，后来的哲学家(如奥卡姆)重新发现了这一现象，最近在认知心理学、行为经济学和决策制定中进行了研究(一些最著名的作品来自乔治·米勒[6]和诺贝尔奖及图灵奖获得者希尔伯特·西蒙[7])。这种认知偏差是XAI研究迫切需要<strong class="kr ja"><em class="lu"/></strong>简单解释的主要原因。简约标准是在端到端可区分模型(如逻辑解释网络)中编码这种归纳偏差的一种方式！</p><blockquote class="oh"><p id="7c93" class="oi oj iq bd ok ol om on oo op oq lk dk translated">当人类比较一组概述相同结果的假设时，他们往往会对最简单的假设有一种隐含的偏见——简约标准在端到端可区分模型中编码了这种归纳偏见。</p></blockquote><p id="ceab" class="pw-post-body-paragraph kp kq iq kr b ks pd ka ku kv pe kd kx ky pf la lb lc pg le lf lg ph li lj lk ij bi translated">如果您有兴趣了解更多关于LENs及其简约标准的信息，您可以在我们最近的论文[8]中找到更多详细信息。</p><p id="d90d" class="pw-post-body-paragraph kp kq iq kr b ks kt ka ku kv kw kd kx ky kz la lb lc ld le lf lg lh li lj lk ij bi translated">然而，如果你只是想玩玩镜头，那么下一节就是为你准备的！</p></div><div class="ab cl mr ms hu mt" role="separator"><span class="mu bw bk mv mw mx"/><span class="mu bw bk mv mw mx"/><span class="mu bw bk mv mw"/></div><div class="ij ik il im in"><h1 id="e4c0" class="my mz iq bd na nb nc nd ne nf ng nh ni kf nj kg nk ki nl kj nm kl nn km no np bi translated">PyTorch，解释一下！</h1><p id="5692" class="pw-post-body-paragraph kp kq iq kr b ks nq ka ku kv nr kd kx ky ns la lb lc nt le lf lg nu li lj lk ij bi translated">阅读要求:Python、pip和PyTorch的基础知识。</p><h2 id="a7f8" class="nv mz iq bd na nw nx dn ne ny nz dp ni ky oa ob nk lc oc od nm lg oe of no iw bi translated">什么是“PyTorch，解释一下！”？</h2><p id="edc3" class="pw-post-body-paragraph kp kq iq kr b ks nq ka ku kv nr kd kx ky ns la lb lc nt le lf lg nu li lj lk ij bi translated">”<em class="lu"> PyTorch，解释一下！</em>"是PyTorch开发逻辑解释网络的扩展库！</p><p id="bfd6" class="pw-post-body-paragraph kp kq iq kr b ks kt ka ku kv kw kd kx ky kz la lb lc ld le lf lg lh li lj lk ij bi translated">您可以从<a class="ae ma" href="https://pypi.org/project/torch-explain/" rel="noopener ugc nofollow" target="_blank"> PyPI </a>安装<code class="fe pi pj pk pl b">torch_explain</code>及其所有依赖项:</p><pre class="mc md me mf gt pm pl pn po aw pp bi"><span id="4547" class="nv mz iq pl b gy pq pr l ps pt">pip install -r requirements.txt torch-explain</span></pre><p id="73f5" class="pw-post-body-paragraph kp kq iq kr b ks kt ka ku kv kw kd kx ky kz la lb lc ld le lf lg lh li lj lk ij bi translated">代码可以在github 上免费获得。</p><h2 id="6930" class="nv mz iq bd na nw nx dn ne ny nz dp ni ky oa ob nk lc oc od nm lg oe of no iw bi translated">玩具示例#1</h2><p id="b71b" class="pw-post-body-paragraph kp kq iq kr b ks nq ka ku kv nr kd kx ky ns la lb lc nt le lf lg nu li lj lk ij bi translated">对于这个简单的例子，让我们使用Ciravegna等人在2020年提出的第一个逻辑解释网络来解决XOR问题。作者称之为“ψ网络”[9]。该透镜特征在于:</p><ul class=""><li id="bb1c" class="ll lm iq kr b ks kt kv kw ky ln lc lo lg lp lk lq lr ls lt bi translated">具有sigmoid激活函数的全连接层序列；</li><li id="b685" class="ll lm iq kr b ks lv kv lw ky lx lc ly lg lz lk lq lr ls lt bi translated">训练时的修剪阶段，以简化架构。</li></ul><p id="fd66" class="pw-post-body-paragraph kp kq iq kr b ks kt ka ku kv kw kd kx ky kz la lb lc ld le lf lg lh li lj lk ij bi translated">您只需要导入两个库:</p><pre class="mc md me mf gt pm pl pn po aw pp bi"><span id="bda4" class="nv mz iq pl b gy pq pr l ps pt">import torch<br/>import torch_explain as te</span></pre><p id="b8c1" class="pw-post-body-paragraph kp kq iq kr b ks kt ka ku kv kw kd kx ky kz la lb lc ld le lf lg lh li lj lk ij bi translated">并按如下方式生成训练数据:</p><pre class="mc md me mf gt pm pl pn po aw pp bi"><span id="d147" class="nv mz iq pl b gy pq pr l ps pt">x_train = torch.tensor([<br/>    [0, 0],<br/>    [0, 1],<br/>    [1, 0],<br/>    [1, 1],<br/>], dtype=torch.float)<br/>y_train = torch.tensor([0, 1, 1, 0], dtype=torch.long)</span></pre><p id="7875" class="pw-post-body-paragraph kp kq iq kr b ks kt ka ku kv kw kd kx ky kz la lb lc ld le lf lg lh li lj lk ij bi translated">让我们定义一个3层ψ网络:</p><pre class="mc md me mf gt pm pl pn po aw pp bi"><span id="b793" class="nv mz iq pl b gy pq pr l ps pt">layers = [<br/>    torch.nn.Linear(x_train.shape[1], 10),<br/>    torch.nn.Sigmoid(),<br/>    torch.nn.Linear(10, 5),<br/>    torch.nn.Sigmoid(),<br/>    torch.nn.Linear(5, 1),<br/>    torch.nn.Sigmoid(),<br/>]<br/>model = torch.nn.Sequential(*layers)</span></pre><p id="9507" class="pw-post-body-paragraph kp kq iq kr b ks kt ka ku kv kw kd kx ky kz la lb lc ld le lf lg lh li lj lk ij bi translated">现在，您可以通过优化二进制交叉熵损失和<code class="fe pi pj pk pl b">l1_loss</code>损失函数来训练网络，将人类的先验知识融入到简单的解释中。我们将在1000个纪元后修剪网络:</p><pre class="mc md me mf gt pm pl pn po aw pp bi"><span id="1939" class="nv mz iq pl b gy pq pr l ps pt">optimizer = torch.optim.AdamW(model.parameters(), lr=0.01)<br/>loss_form = torch.nn.BCELoss()<br/>model.train()<br/>for epoch in range(6001):<br/>    optimizer.zero_grad()<br/>    y_pred = model(x_train)<br/>    loss = loss_form(y_pred, y_train) + 0.000001 * te.nn.functional.l1_loss(model)<br/>    loss.backward()<br/>    optimizer.step()<br/><br/>    model = prune_equal_fanin(model, epoch, prune_epoch=1000)</span></pre><p id="cbb6" class="pw-post-body-paragraph kp kq iq kr b ks kt ka ku kv kw kd kx ky kz la lb lc ld le lf lg lh li lj lk ij bi translated">训练完成后，您可以提取一阶逻辑公式来描述网络如何构成输入要素以获得预测:</p><pre class="mc md me mf gt pm pl pn po aw pp bi"><span id="1d35" class="nv mz iq pl b gy pq pr l ps pt">from torch_explain.logic.nn import psi<br/>from torch.nn.functional import one_hot<br/><br/>y1h = one_hot(y_train.squeeze().long())<br/>explanation = psi.explain_class(model, x_train)</span></pre><p id="24dd" class="pw-post-body-paragraph kp kq iq kr b ks kt ka ku kv kw kd kx ky kz la lb lc ld le lf lg lh li lj lk ij bi translated">解释将是析取范式的逻辑公式。在这种情况下，解释将是对应于<code class="fe pi pj pk pl b">y=1 IFF f1 XOR f2</code>的<code class="fe pi pj pk pl b">y=1 IFF (f1 AND ~f2) OR (f2 AND ~f1)</code>。</p><p id="e4a1" class="pw-post-body-paragraph kp kq iq kr b ks kt ka ku kv kw kd kx ky kz la lb lc ld le lf lg lh li lj lk ij bi translated">逻辑解释的质量可以在分类准确性和规则复杂性方面进行定量评估，如下所示:</p><pre class="mc md me mf gt pm pl pn po aw pp bi"><span id="b618" class="nv mz iq pl b gy pq pr l ps pt">from torch_explain.logic.metrics import test_explanation, complexity<br/><br/>accuracy, preds = test_explanation(explanation, x_train, y1h, target_class=1)<br/>explanation_complexity = complexity(explanation)</span></pre><p id="d652" class="pw-post-body-paragraph kp kq iq kr b ks kt ka ku kv kw kd kx ky kz la lb lc ld le lf lg lh li lj lk ij bi translated">在这种情况下，准确率为100%，复杂度为4。</p><h2 id="c81b" class="nv mz iq bd na nw nx dn ne ny nz dp ni ky oa ob nk lc oc od nm lg oe of no iw bi translated">玩具示例#2</h2><p id="eb4d" class="pw-post-body-paragraph kp kq iq kr b ks nq ka ku kv nr kd kx ky ns la lb lc nt le lf lg nu li lj lk ij bi translated">现在让我们把问题复杂化一点:让我们用100个虚拟特征来解决XOR问题。您只需要生成如下的训练数据:</p><pre class="mc md me mf gt pm pl pn po aw pp bi"><span id="15ed" class="nv mz iq pl b gy pq pr l ps pt">x0 = torch.zeros((4, 100))<br/>x_train = torch.tensor([<br/>    [0, 0],<br/>    [0, 1],<br/>    [1, 0],<br/>    [1, 1],<br/>], dtype=torch.float)<br/>x_train = torch.cat([x_train, x0], dim=1)<br/>y_train = torch.tensor([0, 1, 1, 0], dtype=torch.long)</span></pre><p id="f646" class="pw-post-body-paragraph kp kq iq kr b ks kt ka ku kv kw kd kx ky kz la lb lc ld le lf lg lh li lj lk ij bi translated">为了解决这个任务，让我们定义一个更强大的镜头，使用我们最近引入的一个特殊的层(即<code class="fe pi pj pk pl b">EntropyLayer</code>)来实现一种非常有效的镜头，称为“基于熵的镜头”[10]:</p><pre class="mc md me mf gt pm pl pn po aw pp bi"><span id="6928" class="nv mz iq pl b gy pq pr l ps pt">layers = [<br/>    te.nn.EntropyLinear(x_train.shape[1], 10, n_classes=2),<br/>    torch.nn.LeakyReLU(),<br/>    torch.nn.Linear(10, 4),<br/>    torch.nn.LeakyReLU(),<br/>    torch.nn.Linear(4, 1),<br/>]<br/>model = torch.nn.Sequential(*layers)</span></pre><p id="be81" class="pw-post-body-paragraph kp kq iq kr b ks kt ka ku kv kw kd kx ky kz la lb lc ld le lf lg lh li lj lk ij bi translated">现在，您可以通过优化交叉熵损失和<code class="fe pi pj pk pl b">entropy_logic_loss</code>损失函数来训练网络，该函数结合了人类对简单解释的先验知识:</p><pre class="mc md me mf gt pm pl pn po aw pp bi"><span id="d285" class="nv mz iq pl b gy pq pr l ps pt">optimizer = torch.optim.AdamW(model.parameters(), lr=0.01)<br/>loss_form = torch.nn.CrossEntropyLoss()<br/>model.train()<br/>for epoch in range(1001):<br/>    optimizer.zero_grad()<br/>    y_pred = model(x_train).squeeze(-1)<br/>    loss = loss_form(y_pred, y_train) + 0.00001 * te.nn.functional.entropy_logic_loss(model)<br/>    loss.backward()<br/>    optimizer.step()</span></pre><p id="4e1d" class="pw-post-body-paragraph kp kq iq kr b ks kt ka ku kv kw kd kx ky kz la lb lc ld le lf lg lh li lj lk ij bi translated">训练完成后，您可以提取一阶逻辑公式来描述网络如何构成输入要素以获得预测:</p><pre class="mc md me mf gt pm pl pn po aw pp bi"><span id="5b88" class="nv mz iq pl b gy pq pr l ps pt">from torch_explain.logic.nn import entropy<br/>from torch.nn.functional import one_hot</span><span id="f8f1" class="nv mz iq pl b gy pu pr l ps pt">y1h = one_hot(y_train)<br/>explanation, _ = entropy.explain_class(model, x_train, y1h, x_train, y1h, target_class=1)</span></pre><p id="c72f" class="pw-post-body-paragraph kp kq iq kr b ks kt ka ku kv kw kd kx ky kz la lb lc ld le lf lg lh li lj lk ij bi translated">解释将是析取范式的逻辑公式。在这种情况下，解释将是对应于<code class="fe pi pj pk pl b">y=1 IFF f1 XOR f2</code>的<code class="fe pi pj pk pl b">y=1 IFF (f1 AND ~f2) OR (f2 AND ~f1)</code>。</p><p id="d5c7" class="pw-post-body-paragraph kp kq iq kr b ks kt ka ku kv kw kd kx ky kz la lb lc ld le lf lg lh li lj lk ij bi translated">逻辑解释的质量可以根据分类准确性和规则复杂性进行定量评估，如下所示:</p><pre class="mc md me mf gt pm pl pn po aw pp bi"><span id="1a61" class="nv mz iq pl b gy pq pr l ps pt">from torch_explain.logic.metrics import test_explanation, complexity</span><span id="e544" class="nv mz iq pl b gy pu pr l ps pt">accuracy, preds = test_explanation(explanation, x_train, y1h, target_class=1)<br/>explanation_complexity = complexity(explanation)</span></pre><p id="cea5" class="pw-post-body-paragraph kp kq iq kr b ks kt ka ku kv kw kd kx ky kz la lb lc ld le lf lg lh li lj lk ij bi translated">在这种情况下，准确率为100%，复杂度为4。</p></div><div class="ab cl mr ms hu mt" role="separator"><span class="mu bw bk mv mw mx"/><span class="mu bw bk mv mw mx"/><span class="mu bw bk mv mw"/></div><div class="ij ik il im in"><h1 id="31eb" class="my mz iq bd na nb nc nd ne nf ng nh ni kf nj kg nk ki nl kj nm kl nn km no np bi translated">LENs有多厉害？</h1><p id="c61c" class="pw-post-body-paragraph kp kq iq kr b ks nq ka ku kv nr kd kx ky ns la lb lc nt le lf lg nu li lj lk ij bi translated">在这一点上，你可能会被说服，镜头做他们所做的。但是，你可能还有几个实际问题！现在，我将尽我所能猜测其中的一些，并提供一个简单的答案:)</p><p id="29f8" class="pw-post-body-paragraph kp kq iq kr b ks kt ka ku kv kw kd kx ky kz la lb lc ld le lf lg lh li lj lk ij bi translated"><strong class="kr ja">问题1——镜头有多强大？</strong>或者换句话说，如果您使用LENs而不是等效的黑盒神经网络，您预计在分类准确性方面会损失多少？</p><p id="4b5b" class="pw-post-body-paragraph kp kq iq kr b ks kt ka ku kv kw kd kx ky kz la lb lc ld le lf lg lh li lj lk ij bi translated"><strong class="kr ja">答案1—镜头精度与同等黑匣子不相上下。</strong>在我们的论文中，我们展示了在具有挑战性的数据集上，基于熵的透镜相对于等效黑盒来说是非常有竞争力的，并且它们通常优于决策树和贝叶斯规则列表等白盒方法:</p><figure class="mc md me mf gt mg gh gi paragraph-image"><div role="button" tabindex="0" class="mh mi di mj bf mk"><div class="gh gi pv"><img src="../Images/a06019caeb9813da0d34b2e42cab7407.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LubIgBZftF-Ysv46F7SjrA.png"/></div></div><p class="mn mo gj gh gi mp mq bd b be z dk translated">镜头相对于最先进的白盒模型的分类精度。图片由作者提供。</p></figure><p id="333f" class="pw-post-body-paragraph kp kq iq kr b ks kt ka ku kv kw kd kx ky kz la lb lc ld le lf lg lh li lj lk ij bi translated">问题2——实践中提取的FOL解释的质量如何？逻辑公式是否准确简洁？或者，换句话说，LENs提供的解释有缺陷吗？完全是垃圾吗？</p><p id="e5f4" class="pw-post-body-paragraph kp kq iq kr b ks kt ka ku kv kw kd kx ky kz la lb lc ld le lf lg lh li lj lk ij bi translated"><strong class="kr ja">答案2——解释简单而准确！</strong>多少钱？嗯，至少可以与最先进的白盒模型提供的质量相媲美。下图显示了逻辑解释的质量，包括(1)平均分类测试误差(y轴)和(2)逻辑解释的平均复杂度(文字数)(x轴)。在提供一些最精确公式的同时，基于熵的网络也在产生最简单的公式。有多简单？好吧，垂直的红色虚线标记了外行人可以处理的最大解释复杂度(~9个字面值[6])。基于熵的网络的大多数解释更简洁(3-4个字面值)，所以它们的解释很简单:)</p><figure class="mc md me mf gt mg gh gi paragraph-image"><div role="button" tabindex="0" class="mh mi di mj bf mk"><div class="gh gi pw"><img src="../Images/ad2dfa1f51edf70af787f1e2ed2660af.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EDrEJxv6jdm_bWWqnGnMCg.png"/></div></div><p class="mn mo gj gh gi mp mq bd b be z dk translated">根据分类错误(y轴)和逻辑规则长度(x轴)对解释质量进行定量评估。图片由作者提供。</p></figure></div><div class="ab cl mr ms hu mt" role="separator"><span class="mu bw bk mv mw mx"/><span class="mu bw bk mv mw mx"/><span class="mu bw bk mv mw"/></div><div class="ij ik il im in"><h1 id="2c9c" class="my mz iq bd na nb nc nd ne nf ng nh ni kf nj kg nk ki nl kj nm kl nn km no np bi translated">带回家的信息</h1><p id="935b" class="pw-post-body-paragraph kp kq iq kr b ks nq ka ku kv nr kd kx ky ns la lb lc nt le lf lg nu li lj lk ij bi translated">在这篇文章中，我试图传达4个关键信息:</p><ol class=""><li id="a5b8" class="ll lm iq kr b ks kt kv kw ky ln lc lo lg lp lk ow lr ls lt bi translated">有一种<strong class="kr ja"> <em class="lu">迫切&amp;实用</em> </strong>需求的车型是<strong class="kr ja">既<em class="lu">可解释&amp;精确</em> </strong>。</li><li id="faf1" class="ll lm iq kr b ks lv kv lw ky lx lc ly lg lz lk ow lr ls lt bi translated">一阶逻辑解释很酷，因为它们可以被定量地<strong class="kr ja"><em class="lu"/></strong><strong class="kr ja"><em class="lu">评估</em> </strong>(这对实际的现实世界应用很关键)。</li><li id="96af" class="ll lm iq kr b ks lv kv lw ky lx lc ly lg lz lk ow lr ls lt bi translated">逻辑解释网络是一个家族<strong class="kr ja"><em class="lu"/></strong>神经模型提供<strong class="kr ja"> <em class="lu">一阶逻辑解释</em> </strong> <em class="lu">。</em></li><li id="3983" class="ll lm iq kr b ks lv kv lw ky lx lc ly lg lz lk ow lr ls lt bi translated">逻辑解释网络是<strong class="kr ja">既<em class="lu">可解释&amp;准确</em> </strong> ( <a class="ae ma" href="https://github.com/pietrobarbiero/pytorch_explain" rel="noopener ugc nofollow" target="_blank">又容易实现</a>！！！).</li></ol></div><div class="ab cl mr ms hu mt" role="separator"><span class="mu bw bk mv mw mx"/><span class="mu bw bk mv mw mx"/><span class="mu bw bk mv mw"/></div><div class="ij ik il im in"><h1 id="ce76" class="my mz iq bd na nb nc nd ne nf ng nh ni kf nj kg nk ki nl kj nm kl nn km no np bi translated">参考</h1><p id="abf9" class="pw-post-body-paragraph kp kq iq kr b ks nq ka ku kv nr kd kx ky ns la lb lc nt le lf lg nu li lj lk ij bi translated">[1]鲁丁，辛西娅。"停止解释高风险决策的黑盒机器学习模型，转而使用可解释的模型."<em class="lu">自然机器智能</em>1.5(2019):206–215。</p><p id="3d76" class="pw-post-body-paragraph kp kq iq kr b ks kt ka ku kv kw kd kx ky kz la lb lc ld le lf lg lh li lj lk ij bi translated">[2] Devlin，Jacob等《Bert:用于语言理解的深度双向转换器的预训练》<em class="lu"> arXiv预印本arXiv:1810.04805 </em> (2018)。</p><p id="07c2" class="pw-post-body-paragraph kp kq iq kr b ks kt ka ku kv kw kd kx ky kz la lb lc ld le lf lg lh li lj lk ij bi translated">[3] Kindermans，Pieter-Jan等人，“显著性方法的(不)可靠性”<em class="lu">可解释的AI:解释、说明和可视化深度学习</em>。施普林格，查姆，2019。267–280.</p><p id="3c77" class="pw-post-body-paragraph kp kq iq kr b ks kt ka ku kv kw kd kx ky kz la lb lc ld le lf lg lh li lj lk ij bi translated">[4] Koh，庞伟等，“概念瓶颈模型”<em class="lu">机器学习国际会议</em>。PMLR，2020年。</p><p id="eb81" class="pw-post-body-paragraph kp kq iq kr b ks kt ka ku kv kw kd kx ky kz la lb lc ld le lf lg lh li lj lk ij bi translated">[5] Wah，Catherine等人，“加州理工学院-加州大学圣迭戈分校鸟类-200–2011数据集。”(2011).</p><p id="0e2a" class="pw-post-body-paragraph kp kq iq kr b ks kt ka ku kv kw kd kx ky kz la lb lc ld le lf lg lh li lj lk ij bi translated">乔治·米勒:“神奇的数字七，加减二:我们处理信息能力的一些极限。”<em class="lu">心理评论</em> 63.2 (1956): 81。</p><p id="bfcc" class="pw-post-body-paragraph kp kq iq kr b ks kt ka ku kv kw kd kx ky kz la lb lc ld le lf lg lh li lj lk ij bi translated">[7]西蒙，赫伯特·a .“人类的模型；社会理性。”(1957).</p><p id="1ecb" class="pw-post-body-paragraph kp kq iq kr b ks kt ka ku kv kw kd kx ky kz la lb lc ld le lf lg lh li lj lk ij bi translated">[8] Ciravegna，Gabriele等，《逻辑解释的网络》<em class="lu"> arXiv预印本arXiv:2108.05149 </em> (2021)。</p><p id="db34" class="pw-post-body-paragraph kp kq iq kr b ks kt ka ku kv kw kd kx ky kz la lb lc ld le lf lg lh li lj lk ij bi translated">[9] Ciravegna，Gabriele等人，“基于约束的学习和解释方法”<em class="lu">AAAI人工智能会议论文集</em>。第34卷。№04.2020.</p><p id="6587" class="pw-post-body-paragraph kp kq iq kr b ks kt ka ku kv kw kd kx ky kz la lb lc ld le lf lg lh li lj lk ij bi translated">[10] Barbiero，Pietro等人，“基于熵的神经网络逻辑解释”arXiv预印本arXiv:2106.06804  (2021)。</p></div></div>    
</body>
</html>