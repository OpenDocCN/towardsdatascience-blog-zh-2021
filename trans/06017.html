<html>
<head>
<title>Imbalanced Classification in Python: SMOTE-ENN Method</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Python中的不平衡分类:SMOTE-ENN方法</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/imbalanced-classification-in-python-smote-enn-method-db5db06b8d50?source=collection_archive---------1-----------------------#2021-05-30">https://towardsdatascience.com/imbalanced-classification-in-python-smote-enn-method-db5db06b8d50?source=collection_archive---------1-----------------------#2021-05-30</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><figure class="ip iq gp gr ir is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi io"><img src="../Images/155af9ac31dd8c772e606147a126a1da.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*g3zlmtE8FqPzxNUM"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">奥马尔·弗洛雷斯在<a class="ae jd" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><div class=""/><div class=""><h2 id="9c33" class="pw-subtitle-paragraph kd jf jg bd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku dk translated">使用Python将SMOTE与编辑过的最近邻(ENN)相结合以平衡数据集</h2></div><h2 id="fc11" class="kv kw jg bd kx ky kz dn la lb lc dp ld le lf lg lh li lj lk ll lm ln lo lp lq bi translated">动机</h2><p id="d7e1" class="pw-post-body-paragraph lr ls jg lt b lu lv kh lw lx ly kk lz le ma mb mc li md me mf lm mg mh mi mj ij bi translated">在分类建模中，有许多方法可以通过过采样少数类或欠采样多数类来克服不平衡数据集。为了进一步提高模型性能，许多研究人员建议结合过采样和欠采样方法来更好地平衡数据集。</p><p id="a347" class="pw-post-body-paragraph lr ls jg lt b lu mk kh lw lx ml kk lz le mm mb mc li mn me mf lm mo mh mi mj ij bi translated">在我的<a class="ae jd" rel="noopener" target="_blank" href="/imbalanced-classification-in-python-smote-tomek-links-method-6e48dfe69bbc">上一篇文章</a>中，我已经解释过采样和欠采样相结合的方法之一，称为SMOTE-Tomek链接方法。这一次，我将通过结合SMOTE和编辑最近邻(ENN)方法(简称SMOTE-ENN)及其使用Python的实现来解释另一种变体。</p><h2 id="2c9a" class="kv kw jg bd kx ky kz dn la lb lc dp ld le lf lg lh li lj lk ll lm ln lo lp lq bi translated">概念:K-最近邻(KNN)</h2><p id="63d7" class="pw-post-body-paragraph lr ls jg lt b lu lv kh lw lx ly kk lz le ma mb mc li md me mf lm mg mh mi mj ij bi translated">KNN的思想是假设基于距离的每个数据的最近邻具有相似的类。<strong class="lt jh">当数据集中的新观测值存在时，KNN将搜索其K-最近邻，以确定新观测值将属于的类别</strong>。许多距离度量可用于计算KNN的每个观察距离，但最常用的是使用<strong class="lt jh">欧几里德距离</strong>。</p><p id="57a7" class="pw-post-body-paragraph lr ls jg lt b lu mk kh lw lx ml kk lz le mm mb mc li mn me mf lm mo mh mi mj ij bi translated">例如，假设数据集由两个类组成，黑色和白色。现在，假设有一个未知类的新观察。通过使用KNN，如果大部分新观测值的K-最近邻属于黑色类，那么新观测值将属于该黑色类，反之亦然。</p><p id="912a" class="pw-post-body-paragraph lr ls jg lt b lu mk kh lw lx ml kk lz le mm mb mc li mn me mf lm mo mh mi mj ij bi translated">给定由N个观测值组成的数据集，KNN的算法可以解释如下。</p><ol class=""><li id="ce94" class="mq mr jg lt b lu mk lx ml le ms li mt lm mu mj mv mw mx my bi translated">将K确定为最近邻居的数量。</li><li id="bd7e" class="mq mr jg lt b lu mz lx na le nb li nc lm nd mj mv mw mx my bi translated">对于数据集中的每个观测值，计算每个观测值之间的距离，然后将距离和观测值添加到有序集。</li><li id="31ac" class="mq mr jg lt b lu mz lx na le nb li nc lm nd mj mv mw mx my bi translated">根据距离按升序对距离和观测值的有序集合进行排序。</li><li id="2d2e" class="mq mr jg lt b lu mz lx na le nb li nc lm nd mj mv mw mx my bi translated">从排序的有序集合中挑选前K个条目。换句话说，选择每个观察值的K个最近邻。</li><li id="a7bf" class="mq mr jg lt b lu mz lx na le nb li nc lm nd mj mv mw mx my bi translated">从选定的K个条目中返回多数类。</li></ol><h2 id="3f1f" class="kv kw jg bd kx ky kz dn la lb lc dp ld le lf lg lh li lj lk ll lm ln lo lp lq bi translated">概念:编辑过的最近邻(ENN)</h2><p id="9b50" class="pw-post-body-paragraph lr ls jg lt b lu lv kh lw lx ly kk lz le ma mb mc li md me mf lm mg mh mi mj ij bi translated">由Wilson (1972)开发的ENN方法的工作原理是首先找到每个观测值的K-最近邻，然后检查来自观测值的K-最近邻的多数类是否与观测值的类相同。如果观测值的K-最近邻的多数类和观测值的类不同，则从数据集中删除观测值及其K-最近邻。默认情况下，ENN使用的最近邻数为K=3。</p><p id="677f" class="pw-post-body-paragraph lr ls jg lt b lu mk kh lw lx ml kk lz le mm mb mc li mn me mf lm mo mh mi mj ij bi translated">ENN的算法可以解释如下。</p><ol class=""><li id="a076" class="mq mr jg lt b lu mk lx ml le ms li mt lm mu mj mv mw mx my bi translated">给定具有N个观测值的数据集，将K确定为最近邻的数量。如果不确定，那么K=3。</li><li id="f438" class="mq mr jg lt b lu mz lx na le nb li nc lm nd mj mv mw mx my bi translated">在数据集中的其他观测值中查找该观测值的K-最近邻，然后从K-最近邻返回多数类。</li><li id="9745" class="mq mr jg lt b lu mz lx na le nb li nc lm nd mj mv mw mx my bi translated">如果观测值的类和观测值的K-最近邻中的多数类不同，则从数据集中删除观测值及其K-最近邻。</li><li id="e200" class="mq mr jg lt b lu mz lx na le nb li nc lm nd mj mv mw mx my bi translated">重复第2步和第3步，直到达到每个类别的期望比例。</li></ol><p id="e82b" class="pw-post-body-paragraph lr ls jg lt b lu mk kh lw lx ml kk lz le mm mb mc li mn me mf lm mo mh mi mj ij bi translated">此方法比托梅克链接更有效，在托梅克链接中，当观测值的类与观测值的K-最近邻的多数类不同时，ENN会移除观测值及其K-最近邻，而不是仅移除具有不同类的观测值及其1-最近邻。因此，可以预期ENN会比Tomek链接提供更深入的数据清理。</p><h2 id="d16e" class="kv kw jg bd kx ky kz dn la lb lc dp ld le lf lg lh li lj lk ll lm ln lo lp lq bi translated">斯莫特-ENN方法</h2><p id="3486" class="pw-post-body-paragraph lr ls jg lt b lu lv kh lw lx ly kk lz le ma mb mc li md me mf lm mg mh mi mj ij bi translated">该方法由巴蒂斯塔<em class="mp">等人</em> (2004)开发，结合了SMOTE为少数类生成合成样本的能力和ENN从两个类中删除一些观察值的能力，这些观察值被识别为在观察值的类和其K-最近邻多数类之间具有不同的类。SMOTE-ENN的过程可以解释如下。</p><ol class=""><li id="80dc" class="mq mr jg lt b lu mk lx ml le ms li mt lm mu mj mv mw mx my bi translated">(<strong class="lt jh">开始SMOTE </strong>)从少数类中选择随机数据。</li><li id="20b6" class="mq mr jg lt b lu mz lx na le nb li nc lm nd mj mv mw mx my bi translated">计算随机数据与其k个最近邻之间的距离。</li><li id="710a" class="mq mr jg lt b lu mz lx na le nb li nc lm nd mj mv mw mx my bi translated">将差值乘以0到1之间的随机数，然后将结果添加到少数类作为合成样本。</li><li id="9c43" class="mq mr jg lt b lu mz lx na le nb li nc lm nd mj mv mw mx my bi translated">重复步骤2-3，直到达到所需的少数族裔比例。(<strong class="lt jh">击打结束</strong>)</li><li id="c77a" class="mq mr jg lt b lu mz lx na le nb li nc lm nd mj mv mw mx my bi translated">(<strong class="lt jh">ENN的开始</strong>)确定K，作为最近邻居的数量。如果不确定，那么K=3。</li><li id="3206" class="mq mr jg lt b lu mz lx na le nb li nc lm nd mj mv mw mx my bi translated">在数据集中的其他观测值中查找该观测值的K-最近邻，然后从K-最近邻返回多数类。</li><li id="e7dd" class="mq mr jg lt b lu mz lx na le nb li nc lm nd mj mv mw mx my bi translated">如果观测值的类和观测值的K-最近邻中的多数类不同，则从数据集中删除观测值及其K-最近邻。</li><li id="331c" class="mq mr jg lt b lu mz lx na le nb li nc lm nd mj mv mw mx my bi translated">重复第2步和第3步，直到达到每个类别的期望比例。(<strong class="lt jh">ENN结束</strong></li></ol><p id="6da2" class="pw-post-body-paragraph lr ls jg lt b lu mk kh lw lx ml kk lz le mm mb mc li mn me mf lm mo mh mi mj ij bi translated">为了在实践中更好地理解这种方法，这里我将使用<code class="fe ne nf ng nh b">imbalanced-learn</code>库给出SMOTE-ENN在Python中的一些实现。对于本文，我将通过使用<code class="fe ne nf ng nh b">AdaBoostClassifier</code>使用AdaBoost分类器。为了评估我们的模型，这里我将使用重复分层交叉验证方法。</p><h2 id="9e50" class="kv kw jg bd kx ky kz dn la lb lc dp ld le lf lg lh li lj lk ll lm ln lo lp lq bi translated">履行</h2><p id="35fc" class="pw-post-body-paragraph lr ls jg lt b lu lv kh lw lx ly kk lz le ma mb mc li md me mf lm mg mh mi mj ij bi translated">为了实现，这里我使用来自Kaggle的<a class="ae jd" href="https://www.kaggle.com/uciml/pima-indians-diabetes-database" rel="noopener ugc nofollow" target="_blank"> Pima Indians糖尿病数据库</a>。这个数据集中的文件名是<code class="fe ne nf ng nh b">diabetes.csv</code>。</p><figure class="nj nk nl nm gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi ni"><img src="../Images/448ef103cfd869da184d4834495ee82c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gbwqKV02GWKnGbuio6aLdA.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">皮马印第安人糖尿病数据库(图片取自<a class="ae jd" href="https://www.kaggle.com/uciml/pima-indians-diabetes-database" rel="noopener ugc nofollow" target="_blank"> Kaggle </a></p></figure><p id="e242" class="pw-post-body-paragraph lr ls jg lt b lu mk kh lw lx ml kk lz le mm mb mc li mn me mf lm mo mh mi mj ij bi translated">首先，我们需要导入我们需要的数据和库，如下所示。</p><figure class="nj nk nl nm gt is"><div class="bz fp l di"><div class="nn no l"/></div></figure><figure class="nj nk nl nm gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi np"><img src="../Images/1224806aeeef991e8c92a8f981704d10.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*z2DAW-ImJu9gROcyc_MDvw.png"/></div></div></figure><p id="934e" class="pw-post-body-paragraph lr ls jg lt b lu mk kh lw lx ml kk lz le mm mb mc li mn me mf lm mo mh mi mj ij bi translated">让我们来看看数据描述，并检查数据集中是否有任何缺失值，如下所示。</p><pre class="nj nk nl nm gt nq nh nr ns aw nt bi"><span id="515b" class="kv kw jg nh b gy nu nv l nw nx">&gt; data.info()<br/>&lt;class 'pandas.core.frame.DataFrame'&gt;<br/>RangeIndex: 768 entries, 0 to 767<br/>Data columns (total 9 columns):<br/> #   Column                    Non-Null Count  Dtype  <br/>---  ------                    --------------  -----  <br/> 0   Pregnancies               768 non-null    int64  <br/> 1   Glucose                   768 non-null    int64  <br/> 2   BloodPressure             768 non-null    int64  <br/> 3   SkinThickness             768 non-null    int64  <br/> 4   Insulin                   768 non-null    int64  <br/> 5   BMI                       768 non-null    float64<br/> 6   DiabetesPedigreeFunction  768 non-null    float64<br/> 7   Age                       768 non-null    int64  <br/> 8   Outcome                   768 non-null    int64  <br/>dtypes: float64(2), int64(7)<br/>memory usage: 54.1 KB</span><span id="4b62" class="kv kw jg nh b gy ny nv l nw nx">&gt; data.isnull().sum()<br/>Pregnancies                 0<br/>Glucose                     0<br/>BloodPressure               0<br/>SkinThickness               0<br/>Insulin                     0<br/>BMI                         0<br/>DiabetesPedigreeFunction    0<br/>Age                         0<br/>Outcome                     0<br/>dtype: int64</span></pre><p id="ada9" class="pw-post-body-paragraph lr ls jg lt b lu mk kh lw lx ml kk lz le mm mb mc li mn me mf lm mo mh mi mj ij bi translated">我们可以看到数据集中没有丢失的值，所以我们可以跳到下一步，我们需要通过编写如下代码行来计算属于<code class="fe ne nf ng nh b">Outcome</code>变量中每个类的数据的数量。</p><pre class="nj nk nl nm gt nq nh nr ns aw nt bi"><span id="e67a" class="kv kw jg nh b gy nu nv l nw nx">&gt; data['Outcome'].value_counts()<br/>0    500<br/>1    268</span></pre><p id="a6dd" class="pw-post-body-paragraph lr ls jg lt b lu mk kh lw lx ml kk lz le mm mb mc li mn me mf lm mo mh mi mj ij bi translated">数据非常不平衡，多数阶级属于“0”(我们称之为负)标签，少数阶级属于“1”(我们称之为正)标签。接下来，我们通过编写如下代码将数据分为特性和目标。</p><figure class="nj nk nl nm gt is"><div class="bz fp l di"><div class="nn no l"/></div></figure><pre class="nj nk nl nm gt nq nh nr ns aw nt bi"><span id="7557" class="kv kw jg nh b gy nu nv l nw nx">Y=data['Outcome'].values #Target<br/>X=data.drop('Outcome',axis=1) #Features</span></pre><p id="6cf7" class="pw-post-body-paragraph lr ls jg lt b lu mk kh lw lx ml kk lz le mm mb mc li mn me mf lm mo mh mi mj ij bi translated">预处理完成。现在，让我们跳到建模过程。为了给你一些性能比较，<strong class="lt jh">这里我创建了两个模型，其中第一个没有使用任何不平衡数据处理，而另一个使用SMOTE-ENN方法来平衡数据</strong>。</p><figure class="nj nk nl nm gt is"><div class="bz fp l di"><div class="nn no l"/></div></figure><p id="7e82" class="pw-post-body-paragraph lr ls jg lt b lu mk kh lw lx ml kk lz le mm mb mc li mn me mf lm mo mh mi mj ij bi translated">在不使用SMOTE-ENN平衡数据的情况下，产生的模型性能如下。</p><pre class="nj nk nl nm gt nq nh nr ns aw nt bi"><span id="efd5" class="kv kw jg nh b gy nu nv l nw nx">Mean Accuracy: 0.7535<br/>Mean Precision: 0.7346<br/>Mean Recall: 0.7122</span></pre><figure class="nj nk nl nm gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nz"><img src="../Images/9452b2fffc05271bf6dfe5133b5ae584.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xEPTjvOwivWwYp9ltheEzw.png"/></div></div></figure><p id="a370" class="pw-post-body-paragraph lr ls jg lt b lu mk kh lw lx ml kk lz le mm mb mc li mn me mf lm mo mh mi mj ij bi translated">我们可以看到，准确率分数相当高，但召回分数略低(大约0.7122)。这意味着正确预测少数类标签的模型性能不够好。</p><p id="709b" class="pw-post-body-paragraph lr ls jg lt b lu mk kh lw lx ml kk lz le mm mb mc li mn me mf lm mo mh mi mj ij bi translated">让我们使用SMOTE-ENN来平衡我们的数据集，看看有什么不同。<strong class="lt jh">注意，我在</strong> <code class="fe ne nf ng nh b"><strong class="lt jh">EditedNearestNeighbours</strong></code> <strong class="lt jh">中使用的</strong> <code class="fe ne nf ng nh b"><strong class="lt jh">sampling_strategy</strong></code> <strong class="lt jh">是</strong> <code class="fe ne nf ng nh b"><strong class="lt jh">'all'</strong></code> <strong class="lt jh">，因为ENN的目的是从两个类中删除一些观察值，这些观察值被识别为在观察值的类与其K近邻多数类之间具有不同的类。</strong></p><figure class="nj nk nl nm gt is"><div class="bz fp l di"><div class="nn no l"/></div></figure><pre class="nj nk nl nm gt nq nh nr ns aw nt bi"><span id="e48d" class="kv kw jg nh b gy nu nv l nw nx">Mean Accuracy: 0.7257<br/>Mean Precision: 0.7188<br/>Mean Recall: 0.7354</span></pre><figure class="nj nk nl nm gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nz"><img src="../Images/cb67bbe8b7f5686c54cc8412981ca75c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pHLV0pcKlbc38Y1Vc8oHbQ.png"/></div></div></figure><p id="f9df" class="pw-post-body-paragraph lr ls jg lt b lu mk kh lw lx ml kk lz le mm mb mc li mn me mf lm mo mh mi mj ij bi translated">我们可以看到，召回分数增加，虽然准确性和精确度分数略有下降。这意味着通过使用SMOTE-ENN平衡数据，正确预测少数类标签的模型性能正在变得更好。</p><h2 id="c1a9" class="kv kw jg bd kx ky kz dn la lb lc dp ld le lf lg lh li lj lk ll lm ln lo lp lq bi translated">结论</h2><p id="75c9" class="pw-post-body-paragraph lr ls jg lt b lu lv kh lw lx ly kk lz le ma mb mc li md me mf lm mg mh mi mj ij bi translated">我们到了。现在，您已经了解了如何使用SMOTE-ENN方法来平衡分类建模中使用的数据集，从而提高模型性能。像往常一样，如果您有任何问题，请随时提问和/或讨论！</p><p id="4c01" class="pw-post-body-paragraph lr ls jg lt b lu mk kh lw lx ml kk lz le mm mb mc li mn me mf lm mo mh mi mj ij bi translated">我的下一篇文章再见！一如既往，保持健康，保持安全！</p><h2 id="6ab7" class="kv kw jg bd kx ky kz dn la lb lc dp ld le lf lg lh li lj lk ll lm ln lo lp lq bi translated">作者的联系人</h2><p id="2850" class="pw-post-body-paragraph lr ls jg lt b lu lv kh lw lx ly kk lz le ma mb mc li md me mf lm mg mh mi mj ij bi translated">领英:<a class="ae jd" href="https://www.linkedin.com/in/raden-aurelius-andhika-viadinugroho-b84b19163/" rel="noopener ugc nofollow" target="_blank">拉登·奥勒留和希卡·维亚迪努格罗霍</a></p><p id="5058" class="pw-post-body-paragraph lr ls jg lt b lu mk kh lw lx ml kk lz le mm mb mc li mn me mf lm mo mh mi mj ij bi translated">中:【https://medium.com/@radenaurelius】T2</p><h2 id="8f7b" class="kv kw jg bd kx ky kz dn la lb lc dp ld le lf lg lh li lj lk ll lm ln lo lp lq bi translated">参考</h2><p id="6316" class="pw-post-body-paragraph lr ls jg lt b lu lv kh lw lx ly kk lz le ma mb mc li md me mf lm mg mh mi mj ij bi translated">[1]舒拉、鲍耶、霍尔和凯格尔迈耶(2002年)。<a class="ae jd" href="https://arxiv.org/abs/1106.1813" rel="noopener ugc nofollow" target="_blank"> SMOTE:合成少数过采样技术</a>。《人工智能研究杂志》，第16卷，第321–357页。</p><p id="1ed6" class="pw-post-body-paragraph lr ls jg lt b lu mk kh lw lx ml kk lz le mm mb mc li mn me mf lm mo mh mi mj ij bi translated"><a class="ae jd" href="https://www.kaggle.com/uciml/pima-indians-diabetes-database" rel="noopener ugc nofollow" target="_blank">https://www.kaggle.com/uciml/pima-indians-diabetes-database</a></p><p id="4b67" class="pw-post-body-paragraph lr ls jg lt b lu mk kh lw lx ml kk lz le mm mb mc li mn me mf lm mo mh mi mj ij bi translated">[3]何和马，杨(2013)。<a class="ae jd" href="https://www.wiley.com/en-us/Imbalanced+Learning%3A+Foundations%2C+Algorithms%2C+and+Applications-p-9781118074626" rel="noopener ugc nofollow" target="_blank"> <em class="mp">不平衡学习:基础、算法、应用</em> </a>。第一版。威利。</p><p id="3a3c" class="pw-post-body-paragraph lr ls jg lt b lu mk kh lw lx ml kk lz le mm mb mc li mn me mf lm mo mh mi mj ij bi translated">[4]盖，T. M .和哈特，P. E. (1967年)。<a class="ae jd" href="https://ieeexplore.ieee.org/abstract/document/1053964?casa_token=lPmnigNVCooAAAAA:FiwjJ9K8QRVTra-GkyoxWbQigU7hcZiKShUSTSSb0D1Sq7npSi3FXW7a4swOaKAJ2pWab9yMQcbMGzw" rel="noopener ugc nofollow" target="_blank">最近邻模式分类</a>。<em class="mp"> IEEE信息论汇刊</em>，第13卷，第1期，第21–27页。</p><p id="0d37" class="pw-post-body-paragraph lr ls jg lt b lu mk kh lw lx ml kk lz le mm mb mc li mn me mf lm mo mh mi mj ij bi translated">[5]韩，j .，Kamber，m .，和裴，J. (2012年)。<a class="ae jd" href="https://www.sciencedirect.com/book/9780123814791/data-mining-concepts-and-techniques" rel="noopener ugc nofollow" target="_blank"> <em class="mp">数据挖掘概念与技术</em> </a>。第三版。波士顿:爱思唯尔。</p><p id="70f1" class="pw-post-body-paragraph lr ls jg lt b lu mk kh lw lx ml kk lz le mm mb mc li mn me mf lm mo mh mi mj ij bi translated">[6]威尔逊法学博士(1972年)。<a class="ae jd" href="https://ieeexplore.ieee.org/abstract/document/4309137/?casa_token=kvUJIlb4SMkAAAAA:66UtLLyzOyyDuDtadNIHMAVSRBR69bmBE3H9anLqJbvbL_OpeJPhZ13afAmPqbgL6PNIr0Y18Ml3_gY" rel="noopener ugc nofollow" target="_blank">使用编辑数据的最近邻规则的渐近性质</a>。<em class="mp"> IEEE系统、人和控制论汇刊</em>，SMC-2卷，第3期，第408-421页。</p><p id="d408" class="pw-post-body-paragraph lr ls jg lt b lu mk kh lw lx ml kk lz le mm mb mc li mn me mf lm mo mh mi mj ij bi translated">[7]巴蒂斯塔，两性平等协会，普拉蒂共和国，和莫纳德，M. C. (2004年)。<a class="ae jd" href="https://dl.acm.org/doi/abs/10.1145/1007730.1007735" rel="noopener ugc nofollow" target="_blank">对平衡机器学习训练数据的几种方法的行为的研究</a>。<em class="mp"> ACM SIGKDD探索时事通讯</em>，第6卷，第1期，第20–29页。</p><p id="e0cd" class="pw-post-body-paragraph lr ls jg lt b lu mk kh lw lx ml kk lz le mm mb mc li mn me mf lm mo mh mi mj ij bi translated">[8]<a class="ae jd" href="https://imbalanced-learn.org/stable/references/generated/imblearn.combine.SMOTEENN.html" rel="noopener ugc nofollow" target="_blank">https://unbalanced-learn . org/stable/references/generated/imb learn . combine . smote enn . html</a></p></div></div>    
</body>
</html>