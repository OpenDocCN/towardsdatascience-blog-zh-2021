<html>
<head>
<title>How to Create an Answer From a Question With DPR</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如何从DPR的问题中创造答案</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/how-to-create-an-answer-from-a-question-with-dpr-d76e29cc5d60?source=collection_archive---------9-----------------------#2021-04-09">https://towardsdatascience.com/how-to-create-an-answer-from-a-question-with-dpr-d76e29cc5d60?source=collection_archive---------9-----------------------#2021-04-09</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="f360" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">面向下一代智能解决方案的下一代问答技术</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/e5e1904cb7eebcd14aa1935bd1812849.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*iPaLNG5aIWGE-FjP"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">艾米丽·莫特在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="aad6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi lv translated"><span class="l lw lx ly bm lz ma mb mc md di">在</span>开放领域问答中，我们通常设计一个包含数据源、检索器和阅读器/生成器的模型架构。</p><p id="51c1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这些组件中的第一个通常是文档存储。我们在这里使用的两个最受欢迎的商店是Elasticsearch和FAISS。</p><p id="13b5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">接下来是我们的寻回犬——本文的主题。检索器的工作是在我们的文档存储中过滤相关的信息块(文档),并将它们传递给阅读器/生成器模型。</p><p id="35f2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">阅读器/生成器模型是我们问答堆栈中的最终模型。我们可以有一个<em class="me">阅读器</em>，它直接从<em class="me">上下文</em>中提取答案。或者，生成器，它使用语言生成从上下文中生成答案。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mf"><img src="../Images/1a45c8289ca4b080b4638768ca19a06d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AM-U7qMOCXxiUP6UwQDTaQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">检索器-阅读器和检索器-生成器开放域(OD) QA栈都包括外部数据源(文档存储)、检索器和阅读器/生成器。</p></figure><p id="bf6a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">检索者的工作对我们的读者表现至关重要。给定一个查询，它必须找到最相关的<em class="me">上下文</em>。</p><p id="eac6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果读者得到了<em class="me">不正确的</em>上下文，它将输出<em class="me">不正确的</em>答案。</p><p id="8e2d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果给予读者<em class="me">正确的</em>上下文，它<em class="me">可能</em>输出<em class="me">正确的</em>答案。</p><p id="428a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">所以，如果我们想有机会输出好的答案，检索器必须工作得很好。</p></div><div class="ab cl mg mh hx mi" role="separator"><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml"/></div><div class="im in io ip iq"><h1 id="5626" class="mn mo it bd mp mq mr ms mt mu mv mw mx jz my ka mz kc na kd nb kf nc kg nd ne bi translated">稀疏寻回犬</h1><p id="5027" class="pw-post-body-paragraph kz la it lb b lc nf ju le lf ng jx lh li nh lk ll lm ni lo lp lq nj ls lt lu im bi translated">在过去，我们依赖稀疏向量检索器来完成从文档存储中查找相关信息的任务。为此，我们使用了TF-IDF或BM25。</p><h2 id="d9b1" class="nk mo it bd mp nl nm dn mt nn no dp mx li np nq mz lm nr ns nb lq nt nu nd nv bi translated">TF-IDF</h2><p id="215b" class="pw-post-body-paragraph kz la it lb b lc nf ju le lf ng jx lh li nh lk ll lm ni lo lp lq nj ls lt lu im bi translated">TF-IDF算法是计算两个文本相似性的一个流行选项。</p><ul class=""><li id="050f" class="nw nx it lb b lc ld lf lg li ny lm nz lq oa lu ob oc od oe bi translated"><strong class="lb iu"> TF </strong>指在上下文中找到查询中的多少个单词。</li><li id="7cbe" class="nw nx it lb b lc of lf og li oh lm oi lq oj lu ob oc od oe bi translated"><strong class="lb iu"> IDF </strong>是包含该单词的文档分数的倒数。</li></ul><p id="6d29" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然后将这两个值相乘，得到TF-IDF分数。</p><p id="5c62" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在，我们可以发现单词<em class="me">“海马”</em>在查询和上下文之间是共享的，这将增加TF-IDF分数，因为:</p><ul class=""><li id="4a5d" class="nw nx it lb b lc ld lf lg li ny lm nz lq oa lu ob oc od oe bi translated"><strong class="lb iu"> TF </strong> —在查询和上下文中都找到该单词(高分)。</li><li id="328e" class="nw nx it lb b lc of lf og li oh lm oi lq oj lu ob oc od oe bi translated"><strong class="lb iu">IDF</strong>—<em class="me">【海马】</em>这个词是<em class="me">而不是其他很多文献中发现的</em>(所以词频的<em class="me">逆</em>是一个高数字)。</li></ul><p id="c065" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">或者，如果我们使用单词<em class="me">“the”</em>，我们将返回一个较低的TF-IDF分数，因为:</p><ul class=""><li id="0738" class="nw nx it lb b lc ld lf lg li ny lm nz lq oa lu ob oc od oe bi translated"><strong class="lb iu"> TF </strong> —在查询和上下文中都找到该单词(高分)。</li><li id="bc67" class="nw nx it lb b lc of lf og li oh lm oi lq oj lu ob oc od oe bi translated"><strong class="lb iu">IDF</strong>—<em class="me"/>这个词在其他很多文献中都有(所以词频的<em class="me">逆</em>是一个<em class="me">低</em>数)。</li></ul><p id="c159" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">由于<strong class="lb iu"> IDF </strong>因<em class="me">的常见程度而较低，因此<strong class="lb iu"> TF-IDF </strong>得分也较低。</em></p><p id="229c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">因此，TF-IDF分数对于查找包含相同不常用词的序列非常有用。</p><h2 id="b082" class="nk mo it bd mp nl nm dn mt nn no dp mx li np nq mz lm nr ns nb lq nt nu nd nv bi translated">BM25</h2><p id="27b3" class="pw-post-body-paragraph kz la it lb b lc nf ju le lf ng jx lh li nh lk ll lm ni lo lp lq nj ls lt lu im bi translated">BM25是TF-IDF的变体。在这里，我们仍然计算TF和IDF，但是在返回查询和上下文之间的大量匹配之后，TF分数被降低了。</p><p id="3f16" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">此外，它还考虑了文档长度。TF-IDF分数是标准化的，因此如果短文档和长文档具有相同数量的单词匹配，则短文档比长文档的分数更高。</p><p id="eb2c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">当使用稀疏检索器时，BM25通常比TF-IDF更受青睐。</p></div><div class="ab cl mg mh hx mi" role="separator"><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml"/></div><div class="im in io ip iq"><h1 id="a9de" class="mn mo it bd mp mq mr ms mt mu mv mw mx jz my ka mz kc na kd nb kf nc kg nd ne bi translated">密集段落检索</h1><p id="0e60" class="pw-post-body-paragraph kz la it lb b lc nf ju le lf ng jx lh li nh lk ll lm ni lo lp lq nj ls lt lu im bi translated">ODQA的密集段落检索(DPR)于2020年推出，作为传统的TF-IDF和BM25段落检索技术的替代。</p><h2 id="7562" class="nk mo it bd mp nl nm dn mt nn no dp mx li np nq mz lm nr ns nb lq nt nu nd nv bi translated">赞成的意见</h2><p id="eb4e" class="pw-post-body-paragraph kz la it lb b lc nf ju le lf ng jx lh li nh lk ll lm ni lo lp lq nj ls lt lu im bi translated">介绍DPR的论文首先指出，这种新方法比当前的Lucene(文档存储)BM25检索器的段落检索准确率高9-19%。</p><p id="843f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">DPR能够胜过传统的稀疏检索方法有两个关键原因:</p><ul class=""><li id="8f86" class="nw nx it lb b lc ld lf lg li ny lm nz lq oa lu ob oc od oe bi translated">语义相似的词(<em class="me">“嘿”、“你好”、“嘿”</em>)不会被<em class="me"> TF </em>视为匹配。DPR使用密集的语义编码向量(<em class="me">所以“嘿”、“你好”和“嘿”将紧密匹配</em>)。</li><li id="50c6" class="nw nx it lb b lc of lf og li oh lm oi lq oj lu ob oc od oe bi translated">稀疏猎犬<strong class="lb iu">不是</strong>可训练的。DPR使用嵌入函数，我们可以针对特定任务对其进行训练和微调。</li></ul><h2 id="eb44" class="nk mo it bd mp nl nm dn mt nn no dp mx li np nq mz lm nr ns nb lq nt nu nd nv bi translated">骗局</h2><p id="a4f2" class="pw-post-body-paragraph kz la it lb b lc nf ju le lf ng jx lh li nh lk ll lm ni lo lp lq nj ls lt lu im bi translated">尽管有这些明显的性能优势，但也不全是好消息。是的，我们可以训练我们的DPR模型，但这也是一个缺点——而TF-IDF和BM25是现成的——DPR没有。</p><p id="02fe" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">正如在ML中通常的情况一样，DPR需要大量的训练数据——在这种情况下，这些数据是问题和上下文对的精选数据集。</p><p id="ced9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">此外，DPR在索引和检索过程中需要更多的计算。</p><h2 id="6f99" class="nk mo it bd mp nl nm dn mt nn no dp mx li np nq mz lm nr ns nb lq nt nu nd nv bi translated">两个床位和培训</h2><p id="6f2a" class="pw-post-body-paragraph kz la it lb b lc nf ju le lf ng jx lh li nh lk ll lm ni lo lp lq nj ls lt lu im bi translated">DPR使用两种独特的伯特编码器模型。其中一个模型——eᴘ——将文本段落编码成编码的段落向量(我们将上下文向量存储在我们的文档存储中)。</p><p id="5954" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">另一个模型——EQ——将一个问题映射成一个编码的<em class="me">问题</em>向量。</p><p id="aef6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在训练期间，我们将问题-上下文对输入到我们的DPR模型中，并且模型权重将被优化以最大化两个相应的Eᴘ/EQ模型输出之间的点积:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ok"><img src="../Images/533201127b0a2b6b1217929d0160e304.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QoEy0MsJy2Wdl0S3GmqRKw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">培训期间通过DPR模型的数据流的高级视图。</p></figure><p id="7745" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">两个模型输出<em class="me"> Eᴘ(p) </em>和<em class="me"> EQ(q) </em>之间的点积值测量两个向量之间的相似性。点积越高，相似度越高，因为两个向量越接近，点积就越大。</p><p id="e1c4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">通过训练两个模型输出相同的向量，我们正在训练上下文编码器和问题编码器为相关的问题-上下文对输出<em class="me">非常</em>相似的向量。</p><h2 id="07ca" class="nk mo it bd mp nl nm dn mt nn no dp mx li np nq mz lm nr ns nb lq nt nu nd nv bi translated">运行时</h2><p id="5eea" class="pw-post-body-paragraph kz la it lb b lc nf ju le lf ng jx lh li nh lk ll lm ni lo lp lq nj ls lt lu im bi translated">一旦模型(或两个模型)被训练，我们就可以开始使用它们进行问答索引和检索。</p><p id="10ee" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">当我们第一次构建我们的文档存储时，我们需要使用Eᴘ编码器对我们存储在那里的数据进行编码——因此在文档存储初始化期间(或者当添加新文档时)—我们通过Eᴘ编码器运行每一段文本，并将输出向量存储在我们的文档存储中。</p><p id="b0bf" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对于实时问答，我们只需要e Q编码器。当我们提出一个问题时，它将被发送到EQ编码器，然后输出我们的EQ向量<em class="me"> EQ(q) </em>。</p><p id="b776" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">接下来，将<em class="me"> EQ(q) </em>向量与我们的文档存储中已经索引的<em class="me"> Eᴘ(p) </em>向量进行比较，在文档存储中，我们过滤返回最高相似性得分的向量:</p><blockquote class="ol"><p id="4bc7" class="om on it bd oo op oq or os ot ou lu dk translated"><em class="ov"> sim(q，p) = EQ(q)ᵀ Eᴘ(p) </em></p></blockquote><p id="9751" class="pw-post-body-paragraph kz la it lb b lc ow ju le lf ox jx lh li oy lk ll lm oz lo lp lq pa ls lt lu im bi translated">就是这样！我们的检索器已经为我们的问题确定了最相关的上下文。</p><p id="84bb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然后，这些相关的上下文被传递到我们的读者(或生成器)模型，该模型将基于上下文创建一个答案——我们的问答过程就完成了！</p></div><div class="ab cl mg mh hx mi" role="separator"><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml"/></div><div class="im in io ip iq"><p id="0870" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">以上是关于面向开放领域问答(ODQA)的密集段落检索(DPR)的介绍。</p><p id="ed1e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我希望你喜欢这篇文章。如果您有任何问题或建议，请通过推特<a class="ae ky" href="https://twitter.com/jamescalam" rel="noopener ugc nofollow" target="_blank">或在下面的评论中告诉我。如果你对更多类似的内容感兴趣，我也会在YouTube上发布。</a></p><p id="7434" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">感谢阅读！</p></div><div class="ab cl mg mh hx mi" role="separator"><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml"/></div><div class="im in io ip iq"><h1 id="f73f" class="mn mo it bd mp mq mr ms mt mu mv mw mx jz my ka mz kc na kd nb kf nc kg nd ne bi translated">参考</h1><p id="7bd4" class="pw-post-body-paragraph kz la it lb b lc nf ju le lf ng jx lh li nh lk ll lm ni lo lp lq nj ls lt lu im bi translated">[1] V. Karpukin等人艾尔。，<a class="ae ky" href="https://arxiv.org/pdf/2004.04906.pdf" rel="noopener ugc nofollow" target="_blank">面向开放领域问答的密集段落检索</a> (2020)，EMNLP 2020</p><p id="43a4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">拥抱脸，<a class="ae ky" href="https://huggingface.co/transformers/model_doc/dpr.html" rel="noopener ugc nofollow" target="_blank"> DPR </a>，变形金刚Docs</p><p id="450b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><a class="ae ky" href="https://bit.ly/nlp-transformers" rel="noopener ugc nofollow" target="_blank">🤖带变压器的NLP课程</a></p></div><div class="ab cl mg mh hx mi" role="separator"><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml"/></div><div class="im in io ip iq"><p id="9c90" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="me">*所有图片均由作者提供，除非另有说明</em></p></div></div>    
</body>
</html>