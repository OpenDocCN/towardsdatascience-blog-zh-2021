<html>
<head>
<title>Adversarial Machine Learning: Attacks and Possible Defense Strategies</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">对抗性机器学习:攻击和可能的防御策略</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/adversarial-machine-learning-attacks-and-possible-defense-strategies-c00eac0b395a?source=collection_archive---------13-----------------------#2021-07-31">https://towardsdatascience.com/adversarial-machine-learning-attacks-and-possible-defense-strategies-c00eac0b395a?source=collection_archive---------13-----------------------#2021-07-31</a></blockquote><div><div class="fc ik il im in io"/><div class="ip iq ir is it"><h2 id="834b" class="iu iv iw bd b dl ix iy iz ja jb jc dk jd translated" aria-label="kicker paragraph">信息论</h2><div class=""/><div class=""><h2 id="9697" class="pw-subtitle-paragraph kc jf iw bd b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt dk translated">关于机器学习和人工智能的一个新兴研究领域的概述。</h2></div><figure class="kv kw kx ky gu kz gi gj paragraph-image"><div role="button" tabindex="0" class="la lb di lc bf ld"><div class="gi gj ku"><img src="../Images/679a1b1319d85954a56a58309af2a5b9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_48UL5hpUZx3AjgRE0PYuQ.jpeg"/></div></div><p class="lg lh gk gi gj li lj bd b be z dk translated">作者图片</p></figure><p id="4602" class="pw-post-body-paragraph lk ll iw lm b ln lo kg lp lq lr kj ls lt lu lv lw lx ly lz ma mb mc md me mf ip bi translated">近年来，对机器学习(ML)模型的研究不断发展，导致了非常精确的模型的定义。事实上，ML研究人员的主要目标一直是开发更精确的模型。</p><p id="2ab1" class="pw-post-body-paragraph lk ll iw lm b ln lo kg lp lq lr kj ls lt lu lv lw lx ly lz ma mb mc md me mf ip bi translated">因此，<strong class="lm jg">研究和开发没有关注这些模型的安全性，留下了许多严重的漏洞，这在理论上可能会对已实现的模型造成重大损害。</strong></p><blockquote class="mg mh mi"><p id="5eaa" class="lk ll mj lm b ln lo kg lp lq lr kj ls mk lu lv lw ml ly lz ma mm mc md me mf ip bi translated">对抗性机器学习是一种试图修改现有机器学习模型的技术，目的是在预测中引入错误。</p></blockquote><p id="d9f7" class="pw-post-body-paragraph lk ll iw lm b ln lo kg lp lq lr kj ls lt lu lv lw lx ly lz ma mb mc md me mf ip bi translated">在这篇文章中，我将概述对抗性的ML攻击和可能的防御策略。</p><h1 id="feb0" class="mn mo iw bd mp mq mr ms mt mu mv mw mx kl my km mz ko na kp nb kr nc ks nd ne bi translated">攻击</h1><p id="9257" class="pw-post-body-paragraph lk ll iw lm b ln nf kg lp lq ng kj ls lt nh lv lw lx ni lz ma mb nj md me mf ip bi translated">在计算机安全中，<strong class="lm jg">对手</strong>(或攻击者)是试图攻击系统以达到某些目的的人(人或机器)。对手可以在两个层次上对ML模型进行攻击:</p><ul class=""><li id="8045" class="nk nl iw lm b ln lo lq lr lt nm lx nn mb no mf np nq nr ns bi translated"><strong class="lm jg">训练</strong>:攻击者试图在训练时扰乱模型或数据集，例如通过注入虚假数据或修改数据集中的数据；</li><li id="d134" class="nk nl iw lm b ln nt lq nu lt nv lx nw mb nx mf np nq nr ns bi translated"><strong class="lm jg">测试</strong>(或<strong class="lm jg">推理</strong> ) <strong class="lm jg"> : </strong>这种攻击是在模型已经训练好的情况下进行的。</li></ul><p id="a625" class="pw-post-body-paragraph lk ll iw lm b ln lo kg lp lq lr kj ls lt lu lv lw lx ly lz ma mb mc md me mf ip bi translated">无论执行何种类型的攻击，对手可能知道也可能不知道该模型，因此他们可以执行以下攻击之一:</p><ul class=""><li id="72b5" class="nk nl iw lm b ln lo lq lr lt nm lx nn mb no mf np nq nr ns bi translated"><strong class="lm jg">黑盒攻击— </strong>攻击者对模型一无所知</li><li id="fac4" class="nk nl iw lm b ln nt lq nu lt nv lx nw mb nx mf np nq nr ns bi translated"><strong class="lm jg">灰箱攻击— </strong>攻击者拥有模型的部分知识，如模型架构、参数、训练方法或训练数据；</li><li id="48fe" class="nk nl iw lm b ln nt lq nu lt nv lx nw mb nx mf np nq nr ns bi translated"><strong class="lm jg">白盒攻击</strong>:攻击者完全了解模型或训练集。</li></ul><h2 id="5f70" class="ny mo iw bd mp nz oa dn mt ob oc dp mx lt od oe mz lx of og nb mb oh oi nd jc bi translated">训练时间攻击</h2><p id="5f1b" class="pw-post-body-paragraph lk ll iw lm b ln nf kg lp lq ng kj ls lt nh lv lw lx ni lz ma mb nj md me mf ip bi translated">这种攻击的目的是修改学习过程，以便:</p><ul class=""><li id="5416" class="nk nl iw lm b ln lo lq lr lt nm lx nn mb no mf np nq nr ns bi translated">训练模型以预测特定的输出，或者</li><li id="6aca" class="nk nl iw lm b ln nt lq nu lt nv lx nw mb nx mf np nq nr ns bi translated">训练模型来预测任何输出，但不预测实际输出。</li></ul><p id="b228" class="pw-post-body-paragraph lk ll iw lm b ln lo kg lp lq lr kj ls lt lu lv lw lx ly lz ma mb mc md me mf ip bi translated">存在两类训练攻击:</p><ul class=""><li id="a120" class="nk nl iw lm b ln lo lq lr lt nm lx nn mb no mf np nq nr ns bi translated"><strong class="lm jg">数据访问</strong> —攻击者拥有数据集的(部分)访问权，因此他们可以创建一个替代模型，该模型可以在测试阶段使用。</li><li id="901e" class="nk nl iw lm b ln nt lq nu lt nv lx nw mb nx mf np nq nr ns bi translated"><strong class="lm jg">中毒</strong> —攻击者修改数据集或模型本身，以创建一个经过修改的训练模型。</li></ul><p id="909c" class="pw-post-body-paragraph lk ll iw lm b ln lo kg lp lq lr kj ls lt lu lv lw lx ly lz ma mb mc md me mf ip bi translated">实施中毒的攻击者可能采用以下策略:</p><ul class=""><li id="fa63" class="nk nl iw lm b ln lo lq lr lt nm lx nn mb no mf np nq nr ns bi translated"><strong class="lm jg">操纵</strong>现有的训练集</li><li id="1ad9" class="nk nl iw lm b ln nt lq nu lt nv lx nw mb nx mf np nq nr ns bi translated"><strong class="lm jg">向训练集注入</strong>新的虚假数据</li><li id="fffd" class="nk nl iw lm b ln nt lq nu lt nv lx nw mb nx mf np nq nr ns bi translated"><strong class="lm jg">逻辑讹误，</strong>篡改学习算法。</li></ul><p id="6772" class="pw-post-body-paragraph lk ll iw lm b ln lo kg lp lq lr kj ls lt lu lv lw lx ly lz ma mb mc md me mf ip bi translated">在前两种情况下，攻击者应该有一些控制训练集的方法，而在第三种情况下，攻击者应该能够控制模型。</p><p id="fb63" class="pw-post-body-paragraph lk ll iw lm b ln lo kg lp lq lr kj ls lt lu lv lw lx ly lz ma mb mc md me mf ip bi translated">例如，让我们假设一个分类任务预测两个输出标签:<strong class="lm jg">同意</strong>和<strong class="lm jg">不同意</strong>。攻击者可以修改一些训练标签，将两个字符<strong class="lm jg"> a </strong>和<strong class="lm jg"> s </strong>分别改为<strong class="lm jg"> d </strong>和<strong class="lm jg"> i </strong>，反之亦然。这将最终产生反向训练模型。</p><h2 id="97f8" class="ny mo iw bd mp nz oa dn mt ob oc dp mx lt od oe mz lx of og nb mb oh oi nd jc bi translated">测试时的攻击</h2><p id="21a7" class="pw-post-body-paragraph lk ll iw lm b ln nf kg lp lq ng kj ls lt nh lv lw lx ni lz ma mb nj md me mf ip bi translated">测试时的攻击在运行时执行，此时模型已经被训练和部署。这些类型的攻击不会试图修改输出。相反，测试时的攻击旨在找到训练模型中的一些漏洞，以便:</p><ul class=""><li id="5a1d" class="nk nl iw lm b ln lo lq lr lt nm lx nn mb no mf np nq nr ns bi translated">通过模型(<strong class="lm jg">规避攻击</strong>)找到一些能够规避适当输出的对抗性例子，</li><li id="121d" class="nk nl iw lm b ln nt lq nu lt nv lx nw mb nx mf np nq nr ns bi translated">推断模型或训练集的一些信息(<strong class="lm jg"> Oracle攻击</strong>)。</li></ul><p id="ae9d" class="pw-post-body-paragraph lk ll iw lm b ln lo kg lp lq lr kj ls lt lu lv lw lx ly lz ma mb mc md me mf ip bi translated">Oracle攻击包括:</p><ul class=""><li id="bc65" class="nk nl iw lm b ln lo lq lr lt nm lx nn mb no mf np nq nr ns bi translated"><strong class="lm jg">提取攻击</strong>，攻击者试图从模型预测的分析中提取模型的参数；</li><li id="da85" class="nk nl iw lm b ln nt lq nu lt nv lx nw mb nx mf np nq nr ns bi translated"><strong class="lm jg">反转攻击</strong>，攻击者试图重建训练集，包括侵犯隐私的个人信息。</li><li id="455e" class="nk nl iw lm b ln nt lq nu lt nv lx nw mb nx mf np nq nr ns bi translated"><strong class="lm jg">成员推断</strong>，攻击者试图确定作为输入提供的给定数据是否在模型训练集中。</li></ul><p id="8312" class="pw-post-body-paragraph lk ll iw lm b ln lo kg lp lq lr kj ls lt lu lv lw lx ly lz ma mb mc md me mf ip bi translated">下图总结了攻击者可以执行的主要攻击类型:</p><figure class="kv kw kx ky gu kz gi gj paragraph-image"><div role="button" tabindex="0" class="la lb di lc bf ld"><div class="gi gj ku"><img src="../Images/af5ce2841b821479a6fb5490b5dd25e0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mG57dfJY1cUd-6Xq7OoWBg.jpeg"/></div></div><p class="lg lh gk gi gj li lj bd b be z dk translated">作者图片</p></figure><h1 id="2030" class="mn mo iw bd mp mq mr ms mt mu mv mw mx kl my km mz ko na kp nb kr nc ks nd ne bi translated">防卫</h1><p id="58e5" class="pw-post-body-paragraph lk ll iw lm b ln nf kg lp lq ng kj ls lt nh lv lw lx ni lz ma mb nj md me mf ip bi translated">为了保护ML系统免受恶意ML攻击，应遵循以下步骤:</p><ul class=""><li id="453a" class="nk nl iw lm b ln lo lq lr lt nm lx nn mb no mf np nq nr ns bi translated">识别管理信息系统的<strong class="lm jg">潜在漏洞</strong></li><li id="5901" class="nk nl iw lm b ln nt lq nu lt nv lx nw mb nx mf np nq nr ns bi translated"><strong class="lm jg">设计并实施</strong>相应的攻击，并<strong class="lm jg">评估</strong>它们对系统的影响</li><li id="6400" class="nk nl iw lm b ln nt lq nu lt nv lx nw mb nx mf np nq nr ns bi translated">提出一些<strong class="lm jg">对策</strong>来保护ML系统免受已识别的攻击。</li></ul><p id="3b83" class="pw-post-body-paragraph lk ll iw lm b ln lo kg lp lq lr kj ls lt lu lv lw lx ly lz ma mb mc md me mf ip bi translated">无论如何，所有的防御策略都可能涉及系统的一些性能开销，以及模型准确性的降低。</p><h2 id="9098" class="ny mo iw bd mp nz oa dn mt ob oc dp mx lt od oe mz lx of og nb mb oh oi nd jc bi translated">防御训练攻击</h2><p id="eb92" class="pw-post-body-paragraph lk ll iw lm b ln nf kg lp lq ng kj ls lt nh lv lw lx ni lz ma mb nj md me mf ip bi translated">这种类型的防御试图<strong class="lm jg">使训练集更加健壮</strong>，例如通过删除所有导致高错误率的记录。防御训练攻击包括以下策略:</p><ul class=""><li id="3682" class="nk nl iw lm b ln lo lq lr lt nm lx nn mb no mf np nq nr ns bi translated"><strong class="lm jg">数据加密</strong>——对抗数据访问攻击。</li><li id="c86a" class="nk nl iw lm b ln nt lq nu lt nv lx nw mb nx mf np nq nr ns bi translated"><strong class="lm jg">数据净化</strong> —对比中毒攻击。这种技术测试每一个样本，以发现对立的样本。</li><li id="1805" class="nk nl iw lm b ln nt lq nu lt nv lx nw mb nx mf np nq nr ns bi translated"><strong class="lm jg">稳健统计</strong> —对比中毒攻击。该技术试图通过应用约束和正则化方法来减少由中毒引起的模型的潜在失真。</li></ul><h2 id="0106" class="ny mo iw bd mp nz oa dn mt ob oc dp mx lt od oe mz lx of og nb mb oh oi nd jc bi translated">防御测试攻击</h2><p id="d788" class="pw-post-body-paragraph lk ll iw lm b ln nf kg lp lq ng kj ls lt nh lv lw lx ni lz ma mb nj md me mf ip bi translated">这种类型的防御是在训练阶段实现的，尽管它的目的是在测试阶段保护系统。对测试攻击的防御试图<strong class="lm jg">减少对手对模型造成的扰动的影响。</strong></p><p id="30b0" class="pw-post-body-paragraph lk ll iw lm b ln lo kg lp lq lr kj ls lt lu lv lw lx ly lz ma mb mc md me mf ip bi translated">存在许多技术来保护系统免受测试攻击，包括<strong class="lm jg">对抗训练</strong>，其中具有对抗扰动的新输入和正确的输出标签被注入到训练数据中，旨在最小化由对抗数据引起的错误；<strong class="lm jg">渐变掩蔽</strong>、<strong class="lm jg">防御提取</strong>、<strong class="lm jg">集成方法</strong>、<strong class="lm jg">特征挤压</strong>和<strong class="lm jg">自动编码器。</strong></p><blockquote class="mg mh mi"><p id="c7fc" class="lk ll mj lm b ln lo kg lp lq lr kj ls mk lu lv lw ml ly lz ma mm mc md me mf ip bi translated">当对手能够执行Oracle攻击时，防御测试攻击是非常困难的，这允许他们建立一个替代模型，该模型可以用作白盒。</p></blockquote><p id="100b" class="pw-post-body-paragraph lk ll iw lm b ln lo kg lp lq lr kj ls lt lu lv lw lx ly lz ma mb mc md me mf ip bi translated">下图总结了针对对抗性机器学习攻击的可能防御策略:</p><figure class="kv kw kx ky gu kz gi gj paragraph-image"><div role="button" tabindex="0" class="la lb di lc bf ld"><div class="gi gj ku"><img src="../Images/4c5c42efe773ae1adfcee4c9f8fb4d78.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6ZTX7jQSmRoN-1dIo_0UOg.jpeg"/></div></div><p class="lg lh gk gi gj li lj bd b be z dk translated">作者图片</p></figure><h1 id="0acb" class="mn mo iw bd mp mq mr ms mt mu mv mw mx kl my km mz ko na kp nb kr nc ks nd ne bi translated">用于对抗性ML的Python库</h1><p id="52cf" class="pw-post-body-paragraph lk ll iw lm b ln nf kg lp lq ng kj ls lt nh lv lw lx ni lz ma mb nj md me mf ip bi translated">有许多Python库处理对立的ML，比如下面这些:</p><ul class=""><li id="5a4f" class="nk nl iw lm b ln lo lq lr lt nm lx nn mb no mf np nq nr ns bi translated"><a class="ae oj" href="https://github.com/Trusted-AI/adversarial-robustness-toolbox/wiki/ART-Attacks" rel="noopener ugc nofollow" target="_blank">对抗性鲁棒性工具箱</a></li><li id="0f95" class="nk nl iw lm b ln nt lq nu lt nv lx nw mb nx mf np nq nr ns bi translated"><a class="ae oj" href="https://github.com/cleverhans-lab/cleverhans" rel="noopener ugc nofollow" target="_blank">挂钩</a></li><li id="c6a9" class="nk nl iw lm b ln nt lq nu lt nv lx nw mb nx mf np nq nr ns bi translated"><a class="ae oj" href="https://github.com/bethgelab/foolbox" rel="noopener ugc nofollow" target="_blank">傻瓜箱</a></li><li id="1deb" class="nk nl iw lm b ln nt lq nu lt nv lx nw mb nx mf np nq nr ns bi translated"><a class="ae oj" href="https://github.com/iArunava/scratchai" rel="noopener ugc nofollow" target="_blank">刮擦声</a></li></ul><p id="ab7b" class="pw-post-body-paragraph lk ll iw lm b ln lo kg lp lq lr kj ls lt lu lv lw lx ly lz ma mb mc md me mf ip bi translated">在我以后的帖子中，我将调查这些库…敬请关注！</p><h1 id="2da6" class="mn mo iw bd mp mq mr ms mt mu mv mw mx kl my km mz ko na kp nb kr nc ks nd ne bi translated">摘要</h1><p id="4a0d" class="pw-post-body-paragraph lk ll iw lm b ln nf kg lp lq ng kj ls lt nh lv lw lx ni lz ma mb nj md me mf ip bi translated">在本文中，我简要描述了最著名的对抗性机器学习攻击和一些可能的防御技术，以减轻它们对已部署的ML模型的影响。</p><p id="3e0d" class="pw-post-body-paragraph lk ll iw lm b ln lo kg lp lq lr kj ls lt lu lv lw lx ly lz ma mb mc md me mf ip bi translated">如果你想了解我的研究和其他活动的最新情况，你可以在<a class="ae oj" href="https://twitter.com/alod83" rel="noopener ugc nofollow" target="_blank"> Twitter </a>、<a class="ae oj" href="https://www.youtube.com/channel/UC4O8-FtQqGIsgDW_ytXIWOg?view_as=subscriber" rel="noopener ugc nofollow" target="_blank"> Youtube </a>和<a class="ae oj" href="https://github.com/alod83" rel="noopener ugc nofollow" target="_blank"> Github </a>上关注我。</p><h1 id="ab90" class="mn mo iw bd mp mq mr ms mt mu mv mw mx kl my km mz ko na kp nb kr nc ks nd ne bi translated">参考</h1><p id="8221" class="pw-post-body-paragraph lk ll iw lm b ln nf kg lp lq ng kj ls lt nh lv lw lx ni lz ma mb nj md me mf ip bi translated"><a class="ae oj" href="https://nvlpubs.nist.gov/nistpubs/ir/2019/NIST.IR.8269-draft.pdf" rel="noopener ugc nofollow" target="_blank">21种对抗性机器学习的分类和术语</a></p><p id="7ac8" class="pw-post-body-paragraph lk ll iw lm b ln lo kg lp lq lr kj ls lt lu lv lw lx ly lz ma mb mc md me mf ip bi translated"><a class="ae oj" href="https://pralab.diee.unica.it/en/WhatIsAdversarialLearning" rel="noopener ugc nofollow" target="_blank">什么是对抗性学习？</a></p><h1 id="1325" class="mn mo iw bd mp mq mr ms mt mu mv mw mx kl my km mz ko na kp nb kr nc ks nd ne bi translated">相关文章</h1><div class="ok ol gq gs om on"><a href="https://medium.com/geekculture/is-the-semantic-web-really-dead-7113cfd1f573" rel="noopener follow" target="_blank"><div class="oo ab fp"><div class="op ab oq cl cj or"><h2 class="bd jg gz z fq os fs ft ot fv fx jf bi translated">语义网真的死了吗？</h2><div class="ou l"><h3 class="bd b gz z fq os fs ft ot fv fx dk translated">语义网好像已经快没了。这是真的吗？在这篇文章中，我们试图追溯语义学的历史…</h3></div><div class="ov l"><p class="bd b dl z fq os fs ft ot fv fx dk translated">medium.com</p></div></div><div class="ow l"><div class="ox l oy oz pa ow pb le on"/></div></div></a></div><div class="ok ol gq gs om on"><a href="https://alod83.medium.com/june-articles-a-summary-of-my-june-articles-in-case-you-have-missed-them-8bc370375419" rel="noopener follow" target="_blank"><div class="oo ab fp"><div class="op ab oq cl cj or"><h2 class="bd jg gz z fq os fs ft ot fv fx jf bi translated">我六月文章的摘要…以防你错过</h2><div class="ou l"><h3 class="bd b gz z fq os fs ft ot fv fx dk translated">快速回顾一下我在六月份发表的文章:从数据分析，到数据可视化，再到环境设置。</h3></div><div class="ov l"><p class="bd b dl z fq os fs ft ot fv fx dk translated">alod83.medium.com</p></div></div><div class="ow l"><div class="pc l oy oz pa ow pb le on"/></div></div></a></div><div class="ok ol gq gs om on"><a rel="noopener follow" target="_blank" href="/expire-span-the-new-ai-algorithm-that-forgets-irrelevant-information-61da9b442237"><div class="oo ab fp"><div class="op ab oq cl cj or"><h2 class="bd jg gz z fq os fs ft ot fv fx jf bi translated">Expire-Span:忘记无关信息的新人工智能算法</h2><div class="ov l"><p class="bd b dl z fq os fs ft ot fv fx dk translated">towardsdatascience.com</p></div></div><div class="ow l"><div class="pd l oy oz pa ow pb le on"/></div></div></a></div></div></div>    
</body>
</html>