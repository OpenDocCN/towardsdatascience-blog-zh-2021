<html>
<head>
<title>Policy Gradient REINFORCE Algorithm with Baseline</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">带基线策略梯度增强算法</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/policy-gradient-reinforce-algorithm-with-baseline-e95ace11c1c4?source=collection_archive---------5-----------------------#2021-03-19">https://towardsdatascience.com/policy-gradient-reinforce-algorithm-with-baseline-e95ace11c1c4?source=collection_archive---------5-----------------------#2021-03-19</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="9729" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsdatascience.com/tagged/hands-on-tutorials" rel="noopener" target="_blank">实践教程</a></h2><div class=""/><div class=""><h2 id="b734" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">张量流中的算法与实现</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/643d5ba7825f988fe4f328369ee4a860.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ep_th8IOf19TqTcZdKwe7Q.jpeg"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">作者照片</p></figure><p id="182d" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">策略梯度方法是非常流行的强化学习算法。它们非常有用，因为它们可以直接对策略建模，并且它们在离散和连续空间中都工作。在本文中，我们将:</p><ol class=""><li id="4776" class="md me it lj b lk ll ln lo lq mf lu mg ly mh mc mi mj mk ml bi translated">简要概述政策梯度的基本数学原理；</li><li id="b138" class="md me it lj b lk mm ln mn lq mo lu mp ly mq mc mi mj mk ml bi translated">在Tensorflow中实现策略梯度增强算法玩弹球；</li><li id="006f" class="md me it lj b lk mm ln mn lq mo lu mp ly mq mc mi mj mk ml bi translated">比较政策梯度和深Q网(DQN)</li></ol><p id="da72" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">我假设读者对强化学习的基础有所了解。作为复习，你可以快速看一下我上一篇文章<a class="ae mr" rel="noopener" target="_blank" href="/an-overview-of-classic-reinforcement-learning-algorithms-part-1-f79c8b87e5af"> <strong class="lj jd">的第一节强化学习算法的结构概述</strong> </a>。</p><p id="85e2" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">我以前也在Tensorflow中实现过深度Q-net (DQN)来玩钢管舞。如果你感兴趣，请点击查看<a class="ae mr" rel="noopener" target="_blank" href="/deep-reinforcement-learning-build-a-deep-q-network-dqn-to-play-cartpole-with-tensorflow-2-and-gym-8e105744b998">。:)</a></p><h1 id="8ff4" class="ms mt it bd mu mv mw mx my mz na nb nc ki nd kj ne kl nf km ng ko nh kp ni nj bi translated">政策梯度</h1><h2 id="288e" class="nk mt it bd mu nl nm dn my nn no dp nc lq np nq ne lu nr ns ng ly nt nu ni iz bi translated">加固</h2><p id="0e1a" class="pw-post-body-paragraph lh li it lj b lk nv kd lm ln nw kg lp lq nx ls lt lu ny lw lx ly nz ma mb mc im bi translated">不像许多其他RL算法参数化的价值函数(Q学习，SARSA，DQN等。)并从最优值函数中导出策略，策略梯度方法通过将策略参数化为:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi oa"><img src="../Images/aa8a426f5922122606f7ba6132e53948.png" data-original-src="https://miro.medium.com/v2/resize:fit:412/0*FyVhASxbAyPKGl0Y"/></div></figure><p id="7a29" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">然而，当涉及到优化时，我们仍然必须使用价值函数<em class="ob"> V(θ) </em>作为目标函数。我们的目标是最大化<em class="ob"> V(θ) </em>，即遵循策略<em class="ob"> π </em>的轨迹<em class="ob"> τ </em>的预期总回报。注意，值函数<em class="ob"> V(θ) </em>是从参数化的策略<em class="ob">π(θ)</em>中计算出来的，而不是由<em class="ob"> θ </em>直接参数化的。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/79d444a751fc692dff9a1b470a12455e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1044/format:webp/1*fu9HGx2iEqvh89-LyCs3VA.png"/></div></figure><p id="cfbc" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">其中τ是状态-动作轨迹:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi od"><img src="../Images/7cd0bba43b18aaddf1ddcb4f51679a0e.png" data-original-src="https://miro.medium.com/v2/resize:fit:954/format:webp/1*1G97-c15iPXFPiWuR2nzRQ.png"/></div></figure><p id="d8c6" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">R(τ)是轨迹τ的奖励总和:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi oe"><img src="../Images/bbe90f8b7eddc6cd83e56781f6ef7a0b.png" data-original-src="https://miro.medium.com/v2/resize:fit:460/format:webp/1*-zEPKadfR2FknKN1-_TpqQ.png"/></div></figure><p id="e700" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">如前所述，目标是找到使<em class="ob"> V(θ) </em>最大化的策略的参数<em class="ob"> θ </em>。为此，我们通过提升策略的梯度来搜索<em class="ob"> V(θ) </em>中的最大值，w.r.t参数θ。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi of"><img src="../Images/a9021ed8605d1743d9651e3c97f3be6d.png" data-original-src="https://miro.medium.com/v2/resize:fit:828/format:webp/1*Vmmy0Aq-owGqYvldSE9PPw.png"/></div></figure><p id="a766" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">策略<em class="ob"> π(θ) </em>通常使用softmax、高斯或神经网络建模，以确保其可微分。在这里，我们实现了普通策略梯度的一个流行变体:REINFORCE，它利用时间差来计算梯度。更详细的数学推导，请参考<a class="ae mr" href="http://incompleteideas.net/book/RLbook2018.pdf" rel="noopener ugc nofollow" target="_blank">萨顿的第13章和巴尔托的书</a>。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi og"><img src="../Images/0b68f1a7d4b04c36be25b9502827ca15.png" data-original-src="https://miro.medium.com/v2/resize:fit:840/format:webp/1*LzsbTMrlsKhxNIKeqUsOXw.png"/></div></figure><p id="3894" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">让我们看看训练程序是什么样的。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/e07f137518c4c5357bb59905143a58b2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1208/format:webp/1*MhgPkwPnEN2ytvN9mLRxfA.png"/></div></figure><h2 id="6646" class="nk mt it bd mu nl nm dn my nn no dp nc lq np nq ne lu nr ns ng ly nt nu ni iz bi translated">带基线的政策梯度</h2><p id="edcf" class="pw-post-body-paragraph lh li it lj b lk nv kd lm ln nw kg lp lq nx ls lt lu ny lw lx ly nz ma mb mc im bi translated">政策梯度方法的一个缺点是由经验收益引起的高方差。减少方差的一个常见方法是从政策梯度的回报中减去基线<em class="ob"> b(s) </em>。基线本质上是预期实际回报的代理，它不能给政策梯度带来任何偏差。事实上，价值函数本身就是基线的一个很好的候选。减去基线后我们得到的新项是定义优势<em class="ob"> A_t. </em></p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi oi"><img src="../Images/42da863dc8d8dff0fc5d4d37529afe8c.png" data-original-src="https://miro.medium.com/v2/resize:fit:976/format:webp/1*j4yoEArIPogOiS96mvICtQ.png"/></div></figure><p id="4ffe" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">另一方面，还有另一个流行的政策梯度变体:行动者-批评家方法，该方法使用另一个参数化模型<em class="ob"> Q(s，a) </em>来逼近优势价值，而不是使用经验回报<em class="ob"> G_t </em>。这也有助于以增加偏差为代价减少方差。</p><p id="9887" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">基线是一个参数化的价值函数，可以通过减少经验预期收益和基线预测的均方误差来学习。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/627c8eaf7e0e93d58d7e6b58ffafe1e6.png" data-original-src="https://miro.medium.com/v2/resize:fit:680/format:webp/1*-deBYHAB5na7tZnYW6twLQ.png"/></div></figure><p id="55a3" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">有了基线，我们新的训练循环有2个额外的步骤:计算优势和更新基线模型。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi ok"><img src="../Images/75316af7a20351aa0c3a284f0828e4f3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1346/format:webp/1*6065jhiJnZ3EcnPt7TfvdQ.png"/></div></figure><h1 id="3e51" class="ms mt it bd mu mv mw mx my mz na nb nc ki nd kj ne kl nf km ng ko nh kp ni nj bi translated"><strong class="ak"> Python实现(Tensorflow 2) </strong></h1><p id="2343" class="pw-post-body-paragraph lh li it lj b lk nv kd lm ln nw kg lp lq nx ls lt lu ny lw lx ly nz ma mb mc im bi translated">在本节中，我将演示如何使用Tensorflow 2实现带有基线的策略梯度增强算法来玩Cartpole。关于CartPole环境的更多细节，请参考OpenAI的<a class="ae mr" href="https://gym.openai.com/envs/CartPole-v0/" rel="noopener ugc nofollow" target="_blank">文档</a>。完整的代码可以在<a class="ae mr" href="https://github.com/VXU1230/Medium-Tutorials/tree/master/policy_gradient" rel="noopener ugc nofollow" target="_blank"> <strong class="lj jd">这里</strong>找到。</a></p><p id="8252" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">让我们从创建策略神经网络开始。对于像Cartpole这样的简单环境，2个全连接层就足够了。由于Cartpole具有离散的动作空间，我们将分类分布应用于模型输出(logits)和来自它的样本动作，如在<code class="fe ol om on oo b">action_distribution()</code>和<code class="fe ol om on oo b">sample_action()</code>中实现的。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="op oq l"/></div></figure><p id="f0e6" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">接下来，让我们创建一个基线网络。如前所述，我们使用价值函数作为基线。<code class="fe ol om on oo b">forward()</code>函数以当前状态为输入，输出预测值<em class="ob"> V(s)。</em><code class="fe ol om on oo b">update()</code>的输入参数<code class="fe ol om on oo b">target</code>是我们通过玩游戏收集的观察到的平均奖励(回报)，我将稍后讨论。通过最小化返回和预测的均方误差来更新基线网络。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="op oq l"/></div></figure><p id="8177" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">有了上面的两个网络，我们现在可以创建PolicyGradient类来完成大部分繁重的工作。<strong class="lj jd"> </strong>我将把类拆分成多个代码片段。</p><p id="3ffa" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">在<code class="fe ol om on oo b">init()</code>函数中，我们初始化模型的所有必要参数，包括我们刚刚定义的策略和基线网络。<code class="fe ol om on oo b">self.env</code>是Cartpole环境的一个实例。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="op oq l"/></div></figure><p id="1def" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">随着环境和策略网络的创建，我们可以进行几次部署来收集数据。在每一步，我们从策略网络中采样一个动作，玩一步获得奖励和下一个状态。在所有剧集的结尾，我们收集所有由状态、动作和奖励组成的游戏轨迹。这些轨迹将用于稍后更新策略网络和基线网络。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="op oq l"/></div></figure><p id="1091" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">有了这些收集到的轨迹，我们就可以计算出每个状态的回报。请注意，我们可以通过对从步骤<em class="ob"> t </em>到每集结束的所有未来折扣奖励求和，从技术上计算每个回报<em class="ob"> G_t </em>。但是，这将导致复杂度为O(n*n)。为了将其减少到O(n)，这里我们使用另一种方法:滚动平均。实质上，对于每一集，我们从其返回的最后一个状态<em class="ob"> G_t = r_t </em>开始，并以相反的顺序计算返回，以利用关系:<em class="ob"> G_t = r_t + gamma * G_t+1 </em>。</p><p id="8c25" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">一旦我们有了所有情节的数据，我们就把回报拉平，把这些情节的轨迹分批。之前在基线网络的<code class="fe ol om on oo b">update()</code>函数中，我们有一个输入参数<code class="fe ol om on oo b">target</code>，它正是我们在这里计算的回报。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="op oq l"/></div></figure><p id="5e7c" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">利用基线网络的预测值<em class="ob"> V(s) </em>和经验收益，我们也可以得到优势。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="op oq l"/></div></figure><p id="ed85" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">最后，我们拥有了更新策略网络所需的所有组件。记住在政策梯度中，目标是我们通过遵循政策所获得的价值最大化，相当于负值(损失)最小化。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="op oq l"/></div></figure><p id="3889" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">让我们把所有东西放在一起，训练我们的模型。完整的训练逻辑在<code class="fe ol om on oo b">train()</code>中实现，在每一次迭代中，我们重复这个过程:调用<code class="fe ol om on oo b">play_game()</code>得到几个情节轨迹；将轨迹(列表的列表)展平成一批(列表)；使用批数据计算回报和优势；更新基线网络和策略网络。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="op oq l"/></div></figure><p id="3561" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">还有两个更方便的函数可以帮助我们评估策略梯度模型，方法是制作它在CartPole环境中的性能视频。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="op oq l"/></div></figure><p id="a9b1" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">训练完成后，让我们运行代码并呈现一个视频。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="op oq l"/></div></figure><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi or"><img src="../Images/1eeead51d51e082fa2b8bb88f875472c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/1*E3DY7qcrC_Ul4gJ0aWkaxg.gif"/></div></figure><p id="6f80" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">我们还可以绘制出所有训练步骤的回报。政策梯度模型能够收敛到CartPole(200)的最大回报，尽管由于模型的随机性偶尔会出现一些分歧。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi os"><img src="../Images/f81f31f582a315e6fb41cb5e3565a108.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*Dj4MNoWr_8yL1pYqPLFm0Q.png"/></div></figure><h1 id="552f" class="ms mt it bd mu mv mw mx my mz na nb nc ki nd kj ne kl nf km ng ko nh kp ni nj bi translated"><strong class="ak">结论</strong></h1><p id="9759" class="pw-post-body-paragraph lh li it lj b lk nv kd lm ln nw kg lp lq nx ls lt lu ny lw lx ly nz ma mb mc im bi translated">让我们把这个结果和我之前做的DQN翻筋斗的结果进行比较。我们实际上可以看到，政策梯度方法达到收敛的速度比DQN快得多。事实上，策略梯度方法通常在具有小状态空间的环境中更快，如CartPole，这要归功于它直接对策略建模的独特设计。</p><p id="ad52" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">政策梯度方法的最大警告是高方差，这通常通过利用时间结构(加强)、引入基线或增加偏差(行动者-批评家)来解决。同样值得注意是，策略梯度方法只能保证收敛到局部最大值。然而，政策梯度方法在离散和连续空间都非常有效。</p><p id="3a69" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">我希望你喜欢这篇文章。:)</p><h1 id="1e65" class="ms mt it bd mu mv mw mx my mz na nb nc ki nd kj ne kl nf km ng ko nh kp ni nj bi translated">参考</h1><p id="5ebb" class="pw-post-body-paragraph lh li it lj b lk nv kd lm ln nw kg lp lq nx ls lt lu ny lw lx ly nz ma mb mc im bi translated"><a class="ae mr" href="http://incompleteideas.net/book/RLbook2018.pdf" rel="noopener ugc nofollow" target="_blank">强化学习，萨顿和巴尔托</a></p><p id="c36d" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">斯坦福CS234课程笔记:【https://web.stanford.edu/class/cs234/slides/ T4】</p></div></div>    
</body>
</html>