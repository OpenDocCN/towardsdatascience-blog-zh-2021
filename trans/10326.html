<html>
<head>
<title>TT-SRN: Transformer-based Video Instance Segmentation Framework</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">TT-SRN:基于变压器的视频实例分割框架</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/tt-srn-transformer-based-video-instance-segmentation-framework-part-i-ae9964126ac0?source=collection_archive---------19-----------------------#2021-10-01">https://towardsdatascience.com/tt-srn-transformer-based-video-instance-segmentation-framework-part-i-ae9964126ac0?source=collection_archive---------19-----------------------#2021-10-01</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="fc8d" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsdatascience.com/tagged/thoughts-and-theory" rel="noopener" target="_blank">思想和理论</a></h2><div class=""/><div class=""><h2 id="d84b" class="pw-subtitle-paragraph jw iz iq bd b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn dk translated"><strong class="ak">视频领域中的联合目标检测、实例分割、目标跟踪和分类</strong></h2></div><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ko"><img src="../Images/0c478b8b4e281580814c969303c76cd5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pAjdgjIrkjU73JZF6SWM1w.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">任意试验视频上TT-SRN的示例预测。我们打算指出遮挡、视点变化、重叠对象上的系统伪像以及实例的规模。从左上角到右下角，视觉效果是我们模型的连续预测。(图片由作者提供)</p></figure><p id="9536" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated"><strong class="lg ja">你有5秒钟，告诉我TT-SRN是什么？</strong></p><blockquote class="ma mb mc"><p id="e405" class="le lf md lg b lh li ka lj lk ll kd lm me lo lp lq mf ls lt lu mg lw lx ly lz ij bi translated">" F <em class="iq"> ast，基于变形金刚的简单而精确的视频实例分割模块"</em></p></blockquote><h1 id="1c32" class="mh mi iq bd mj mk ml mm mn mo mp mq mr kf ms kg mt ki mu kj mv kl mw km mx my bi translated">TT-SRN和VIS到底是什么？</h1><p id="fca6" class="pw-post-body-paragraph le lf iq lg b lh mz ka lj lk na kd lm ln nb lp lq lr nc lt lu lv nd lx ly lz ij bi translated">视频实例分割(VIS)是最近引入的计算机视觉研究，旨在对视频领域中的实例进行联合检测、分割和跟踪。最近的方法提出了实际上无法使用的高度复杂的多级网络。因此，在实践中需要使用简单而有效的方法。为了填补这一空白，我们提出了一种端到端的基于变压器的正弦表示网络(SRN)视频实例分割模块，即<strong class="lg ja"> TT-SRN </strong>，以解决这一问题。TT-SRN认为视觉感知任务是一个单阶段的直接序列预测问题，使我们能够将时间信息与空间信息结合起来。一组视频帧特征由双变换器提取，然后传播到原始变换器以产生一组实例预测。这种产生的实例级信息然后通过修改的srn传递，以获得最终实例级类id和边界框，以及自参与3d卷积，以获得分段掩码。在其核心，TT-SRN是一个自然的范例，通过相似性学习处理实例分割和跟踪，使系统能够产生快速准确的预测集。TT-SRN使用基于集合的全局损失进行端到端训练，通过二分匹配强制进行唯一预测。因此，在不牺牲分段掩码质量的情况下，流水线的总体复杂度显著降低。由于双变压器是最快的方法之一，第一次在没有隐式CNN架构的情况下解决了VIS问题。</p><p id="17e2" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">我们的方法可以很容易地分成子组件，以产生单独的实例遮罩和边界框，这将使它成为许多视觉任务的统一方法。我们通过比较竞争基准在YouTube-VIS数据集上对我们的结果进行了基准测试，并显示TT-SRN以显著的优势优于基本VIS模型。</p><p id="46d5" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated"><strong class="lg ja">代码&amp;纸张可从</strong>获得</p><div class="ne nf gp gr ng nh"><a href="https://github.com/cankocagil/TT-SPN" rel="noopener  ugc nofollow" target="_blank"><div class="ni ab fo"><div class="nj ab nk cl cj nl"><h2 class="bd ja gy z fp nm fr fs nn fu fw iz bi translated">GitHub-cankocagil/TT-SPN:TT-SPN:具有正弦表示网络的双变压器，用于…</h2><div class="no l"><h3 class="bd b gy z fp nm fr fs nn fu fw dk translated">视频实例分割是最近引入的计算机视觉研究，旨在联合检测…</h3></div><div class="np l"><p class="bd b dl z fp nm fr fs nn fu fw dk translated">github.com</p></div></div><div class="nq l"><div class="nr l ns nt nu nq nv ky nh"/></div></div></a></div><p id="36fc" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated"><strong class="lg ja">图像级目标检测&amp;图像分割版TT-SRN: </strong></p><div class="ne nf gp gr ng nh"><a href="https://github.com/cankocagil/TT-SPN---Object-Detection" rel="noopener  ugc nofollow" target="_blank"><div class="ni ab fo"><div class="nj ab nk cl cj nl"><h2 class="bd ja gy z fp nm fr fs nn fu fw iz bi translated">GitHub - cankocagil/TT-SPN -对象检测:TT-SPN的对象检测版本</h2><div class="no l"><h3 class="bd b gy z fp nm fr fs nn fu fw dk translated">视频实例分割是最近引入的计算机视觉研究，旨在联合检测…</h3></div><div class="np l"><p class="bd b dl z fp nm fr fs nn fu fw dk translated">github.com</p></div></div><div class="nq l"><div class="nw l ns nt nu nq nv ky nh"/></div></div></a></div></div><div class="ab cl nx ny hu nz" role="separator"><span class="oa bw bk ob oc od"/><span class="oa bw bk ob oc od"/><span class="oa bw bk ob oc"/></div><div class="ij ik il im in"><h1 id="6fab" class="mh mi iq bd mj mk oe mm mn mo of mq mr kf og kg mt ki oh kj mv kl oi km mx my bi translated">行动（或活动、袭击）计划</h1><ol class=""><li id="8c75" class="oj ok iq lg b lh mz lk na ln ol lr om lv on lz oo op oq or bi translated"><strong class="lg ja">简介</strong></li><li id="dfa3" class="oj ok iq lg b lh os lk ot ln ou lr ov lv ow lz oo op oq or bi translated"><strong class="lg ja">相关工作</strong> <br/> 2.1 <em class="md">图像级实例分割</em> <br/> 2.2 <em class="md">视频对象检测</em> <br/> 2.3 <em class="md">视频对象跟踪</em> <br/> 2.4 <em class="md">视频实例分割</em></li><li id="373d" class="oj ok iq lg b lh os lk ot ln ou lr ov lv ow lz oo op oq or bi translated"><strong class="lg ja">建议方法:TT-SRN </strong> <br/> 3.1 <em class="md">双变形金刚</em> <br/> 3.1.1局部分组自注意(LSA) <br/> 3.1.2全局亚采样注意(GSA) <br/> 3.2 <em class="md">经典变形金刚</em> <br/> 3.2.1时空位置编码<br/> 3.2.2变形金刚编码器<br/> 3.2.3变形金刚解码器<br/>3.3【T33</li><li id="9357" class="oj ok iq lg b lh os lk ot ln ou lr ov lv ow lz oo op oq or bi translated"><strong class="lg ja">结果<br/> </strong> 4.1实现细节<br/> 4.2。评估指标<br/> 4.3。主要结果</li><li id="692b" class="oj ok iq lg b lh os lk ot ln ou lr ov lv ow lz oo op oq or bi translated"><strong class="lg ja">结论</strong></li><li id="ff9a" class="oj ok iq lg b lh os lk ot ln ou lr ov lv ow lz oo op oq or bi translated"><strong class="lg ja">参考文献</strong></li></ol></div><div class="ab cl nx ny hu nz" role="separator"><span class="oa bw bk ob oc od"/><span class="oa bw bk ob oc od"/><span class="oa bw bk ob oc"/></div><div class="ij ik il im in"><p id="7296" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated"><strong class="lg ja"> 1。视频实例分割简介</strong></p><p id="c466" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">图像和视频中基于实例的分割和对象检测是计算机视觉领域中的基本问题。与图像实例分割不同，新问题旨在同时检测、分割和跟踪视频中的对象实例[31]。它首先是在视频实例分割论文[31]中用一种称为掩模轨迹R-CNN的新算法介绍的。视频实例分割是视频领域时空理解的关键任务，应用于视频编辑、自动驾驶、行人跟踪、增强现实、机器人视觉等等。由于它需要分割和跟踪，因此与图像级实例分割相比，这是一项更具挑战性的任务。此外，它有助于我们将时空原始数据与视频一起编码成有意义的见解，因为与视觉空间数据相比，它具有更丰富的内容。通过在解码过程中增加时间维度，我们可以进一步从视频帧中获得关于运动、视点变化、光照、遮挡、变形和局部模糊的信息。因此，视频实例分割作为一个研究领域越来越受欢迎，并且最近它吸引了沿着视频理解研究路线的社区。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ox"><img src="../Images/b7b99ec405e48e9cd4acf0fdfb1373e0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qlayjxMmrybVSnkWxRaczw.jpeg"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">TT-SRN样本预测(图片由作者提供)</p></figure><p id="6c52" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">最先进的方法开发了具有多个网络的非常复杂的体系结构..非最大抑制)以产生高质量的分段掩码和边界框。通常，基于检测跟踪(自上而下方法)[31，3，5]或时空嵌入聚类[6](自下而上)的方法被提出来处理VIS任务。在自上而下的方法中，产生图像级实例分割掩模，然后通过复杂的手工制作的规则在时间维度上关联，以将空间预测推进到时空预测，这使得解码过程变得复杂并导致实际上不可用。而在自下而上的方法中，地层中的实例级像素聚集在时空嵌入空间中，具有不重叠的区域，这些区域严重基于密集预测质量[6]，并需要多个网络来产生最终VIS结果。因此，简单而有效的、单阶段的、实际可用的和端到端可训练的方法是非常需要的。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi oy"><img src="../Images/6c406089b050fe1c732c480a73f1eb01.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dDBKKcpgLmMJaD1wPU--vQ.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">TT-SRN的整体管道。给定视频帧，一组图像特征由双变换器提取，然后传递到经典变换器架构以获得实例级预测。然后，这些预测被传递到单独的SRN分支，以产生一组类别id、置信度、边界框和自参与卷积，从而获得分段掩码。整体预测遵循视频帧顺序。(图片由作者提供)</p></figure><p id="c7c5" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">在本文中，我们提出了一种新的方法，即TTSRN，以降低整体流水线的复杂性，而不影响预测的速度和质量，以产生VIS结果。整个流水线如图1所示。给定视频帧，一组图像特征由双变换器提取，然后传递到经典变换器架构以获得实例级预测。然后，这些预测被传递到单独的SRN分支，以产生一组类别id、置信度、边界框和自参与卷积模块，从而获得分段掩码。实际上，VIS的所有子任务(分类、检测、分割和跟踪)都是相关的任务。因此，一个任务的输出可以向另一个任务提供重要信息，这将相互促进所有子模块。通过认识到这一点，在一个模块中处理子任务的范例也促进了TT-SRN。同时，由于没有人为设计的规则用于VIS的所有单个任务，实例级特征质量是TT-SRN的另一个重要部分，它由twin transformer模块完成。Twin transformer是一种基于面向空间的视觉转换器的最新架构，最近在论文[9]中提出。从NLP [28]上下文中的经典transformers出版物来看，transformers实际上是用于各种NLP任务的方法(例如，机器翻译和序列间问题)。</p><p id="d574" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">论文[11]首次提出了视觉变形金刚，展示了变形金刚在计算机视觉领域的强大功能。然而，问题是计算复杂度是图像大小的平方。为了抑制，提出了各种视觉变压器，并证明了精心设计的全局和局部注意机制可能在密集预测任务中胜过经典CNN架构[11，27，9]。一种解决方法是局部分组的自我关注(或在非重叠窗口中的自我关注，如最近的Swin Transformer [20])，其中输入在空间上分组为非重叠窗口，并且仅在每个子窗口中计算标准自我关注[9]。即使Swin变换器降低了总的计算复杂度，它也不能通过注意力在非重叠区域之间建立连接。为了克服这一点，在论文[9]中提出了双变压器，其引入了空间可分离的自我注意(SSSA)来缓解这一挑战。SSSA由局部分组自我注意(LSA)和全局亚采样注意(GSA)组成[9]。我们发现由双变压器产生的实例级特征相对于它们在传统CNN中的对应物是高度优化的。</p><p id="58ee" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">这里，我们还介绍了用于分类和对象检测任务的改进的正弦表示网络。在论文[25]中提出了正弦表示网络，并证明了由神经网络参数化的隐式定义的、连续的、可微分的信号表示已经成为一种强大的范例，提供了超过传统表示的许多可能的好处[25]。他们为隐式神经表示引入了杠杆周期激活2函数，并证明了这些被称为正弦表示网络或塞壬的网络非常适合表示复杂的自然信号及其衍生物[25]。在本文中，我们调整了SIREN架构以用于密集预测任务，并对其进行了修改以重新用于包围盒和类id预测。我们改进的SRN网络由多个具有漏失的SIREN层组成。作为SRNs的最后一层，放置具有GELU非线性的全连接层[15]以产生最终结果。SRNs的拟议架构如图2所示。实例分割和跟踪是VIS任务的其他重要方面。</p><p id="dd64" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">“为了执行自然、有效且相互包容的分割和跟踪”，我们改编了来自VisTR [29]的实例序列匹配和跟踪模块。“实例序列匹配在输出实例序列和基本事实实例序列之间执行二分图匹配，并监督唯一分配预测及其注释的序列”[29]。</p><p id="3aaa" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">因此，TT-SRN可以直接保持预测的顺序[29]。“实例序列分割通过自关注在多个帧中累积每个实例的掩模特征，并通过3D卷积分割每个实例的掩模序列以获得最终结果”[29]。</p><p id="563e" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">我们的主要贡献总结如下。</p><ul class=""><li id="6f24" class="oj ok iq lg b lh li lk ll ln oz lr pa lv pb lz pc op oq or bi translated"><em class="md">我们提出了一种高效的视频实例分割模块，该模块基于视觉和具有正弦表示网络的经典变压器，将视觉任务视为端到端的集合预测问题。</em></li><li id="e6cf" class="oj ok iq lg b lh os lk ot ln ou lr ov lv ow lz pc op oq or bi translated"><em class="md">据我们所知，这是第一次在没有任何隐式CNN架构的情况下解决视频实例分割问题，而是使用视觉变压器(例如，双变压器)作为实例级特征生成器。</em></li><li id="89ca" class="oj ok iq lg b lh os lk ot ln ou lr ov lv ow lz pc op oq or bi translated"><em class="md">在没有任何编织的情况下，TT-SRN是最快的方法之一，可以以55.3 FPS的速度运行，并在YouTube-VIS上实现了39.3 %的竞争精度，远远超过了VIS基线模型。</em></li></ul><h1 id="f5ff" class="mh mi iq bd mj mk ml mm mn mo mp mq mr kf ms kg mt ki mu kj mv kl mw km mx my bi translated">2.视频实例分割的相关工作</h1><p id="cf18" class="pw-post-body-paragraph le lf iq lg b lh mz ka lj lk na kd lm ln nb lp lq lr nc lt lu lv nd lx ly lz ij bi translated">在文献中有各种方法用于解决视频实例分割的任务，因为它通常被认为是多阶段问题，即分割/检测和跟踪组件在不同阶段被处理。然而，最近的研究提出了单阶段、简单、计算有效的方法来处理该问题，尽管单阶段方法的性能并不比多阶段方法好。因此，计算机视觉研究社区通过提出掩模轨迹R-CNN的变体或视频实例分割任务的新方法，扩展了论文[31]中所做的工作。STEm-Seg是针对此任务的另一种新算法，特别是，他们将视频剪辑建模为单个3D时空体，并提出了一种在单个阶段中跨空间和时间分割和跟踪实例的新方法[1]。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi pd"><img src="../Images/d564ad6e96de65ddc6edb55d5246c584.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*uT3hKm0iditjQia3.jpg"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">VIS数据注释和预测[31]</p></figure><p id="ff2e" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">然后，Chung-Ching Lin等人提出了一种用于视频实例分割任务的基于变分自动编码器的分割跟踪算法，因为它构建了一个共享编码器和三个并行解码器，产生了用于预测未来帧、对象检测框和实例分割掩模的三个不相交分支[16]。为了促进对该问题的研究，曹等人提出了另一种称为SipMask的单阶段新算法，该算法通过将实例的掩模预测分离到检测到的边界框的不同子区域来保留实例特定的空间信息[6]。然后，VisTR被提议为基于单级变压器的VIS架构，其将VIS任务视为直接的端到端并行序列解码/预测问题[29]。我们的一些工作改编自VisTR模块。具体来说，我们集成了他们的实例序列匹配和分段模块，以监督和分段完整的实例。实例序列匹配在输出实例序列和实际实例序列之间执行二分图匹配，并监督TT-SRN，因此TT-SRN学习实例之间的相似性[29]。实例序列分割模块执行自参与3-D卷积来学习像素级相似性。因此，有各种不同的方法来解决时间域实例级分割问题，在本文中，我们提出了解决视频实例分割问题的方法，因为我们认为它是预测问题的直接集合。即使视频实例分割的概念可以被归类为新的任务，在文献中也有研究者解决的各种类似问题，例如图像级实例分割、视频对象检测、视频对象跟踪和视频对象分割。我们将类似问题简述如下。</p><p id="55a0" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated"><strong class="lg ja"> 2.1。图像级实例分割</strong></p><p id="c127" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">实例分割不仅将像素分组到不同的语义类中，还将它们分组到不同的对象实例中[12]。通常采用两阶段范式，首先使用区域提议网络(RPN)生成对象提议，然后使用聚合RoI特征预测对象边界框和遮罩[12]。在我们的例子中，我们不仅为个体生成分割模板，还将它们在视频序列中关联起来。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi pe"><img src="../Images/98efdbc97bed8a7d29846303db0b61e4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1120/format:webp/0*e5LIKr1XvYpZ_KF7.png"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">实例分割示例(<a class="ae pf" href="https://www.researchgate.net/figure/Instance-segmentation-in-a-open-set-environment-Our-method-segments-all-image-regions_fig1_325557020" rel="noopener ugc nofollow" target="_blank">https://www . researchgate . net/figure/Instance-Segmentation-in-a-open-set-environment-Our-method-segments-all-image-regions _ fig 1 _ 325557020</a>)</p></figure><p id="f11a" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated"><strong class="lg ja"> 2.2。视频对象检测</strong></p><p id="1a08" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">视频对象检测旨在检测视频中的对象，这是作为ImageNet视觉挑战的一部分首次提出的[24]。即使关联和提供身份提高了检测质量，这种挑战也仅限于用于每帧检测的空间保留的评估度量，并且不需要联合对象检测和跟踪[31]。然而，在我们的情况下，我们的目标是联合检测、分割和跟踪，而不是视频对象检测任务。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi pg"><img src="../Images/24dd25e4bae137ecccb17bb0a74fd6b3.png" data-original-src="https://miro.medium.com/v2/resize:fit:840/0*dX8pEmlgymI5MOue.gif"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">视频对象检测示例(<a class="ae pf" href="https://miro.medium.com/max/840/1*tQ9PotwEr93jwFte56U8aA.gif" rel="noopener">https://miro . medium . com/max/840/1 * TQ 9 potwer 93 jwfte 56 u8 aa . gif</a>)</p></figure><p id="4980" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated"><strong class="lg ja"> 2.3。视频对象跟踪</strong></p><p id="d4e1" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">视频目标跟踪任务通常被认为是基于检测和无检测的跟踪方法。在基于检测的跟踪算法中，联合检测和跟踪对象，以便跟踪部分提高检测质量，而在无检测方法中，我们给出一个初始边界框，并尝试跨视频帧跟踪该对象[26，31]。由于基于检测的方法类似于我们的情况，视频实例分割需要时间分割掩模。因此，与以前的基本计算机视觉任务相反，视频实例分割需要多学科和综合的方法。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi ph"><img src="../Images/d6a42cacda5ef7f41523a56c89da4d93.png" data-original-src="https://miro.medium.com/v2/resize:fit:900/0*ihj67t5i5uGKNfSq"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">视频对象跟踪示例(<a class="ae pf" href="https://i2.wp.com/github.com/yehengchen/video_demo/raw/master/video_demo/TownCentreXVID_output.gif?w=450&amp;ssl=1" rel="noopener ugc nofollow" target="_blank">https://I2 . WP . com/github . com/yehengchen/video _ demo/raw/master/video _ demo/TownCentreXVID _ output . gif？w=450 &amp; ssl=1 </a></p></figure><p id="d3b8" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated"><strong class="lg ja"> 2.4。视频实例分割</strong></p><p id="2819" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">由于视频实例分割任务是受监督的，因此它需要面向人的高质量注释，用于边界框和具有预定义类别的二进制分割遮罩。设Ci是属于数据集D的对象类别，i = 1，…，K，其中K是D中包括背景的唯一类别的数量。然后，设B ti j和S ti j是视频帧ti ∈ T中第j个C1，…，CK对象的第j个边界框和二进制掩码，其中T表示给定视频序列中的帧数。假设在推理阶段，VIS算法产生N ∈ C1，…，CK实例假设，使得H ti Nj表示VIS产生的第N个j实例和第t次的预测。因此，H ti Nj包括置信度得分s ti j ∈ [0，1]作为识别具有预定义类别、B ti j和s ti j的实例的概率。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ox"><img src="../Images/3c088369c5f19efdcfab8a0b4cc95cab.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ruJ81J_PplFFavwRyainmg.jpeg"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">视频实例分割示例(图片由作者提供)</p></figure><h1 id="3bb7" class="mh mi iq bd mj mk ml mm mn mo mp mq mr kf ms kg mt ki mu kj mv kl mw km mx my bi translated">3.提议的方法:TT-SRN</h1><p id="d963" class="pw-post-body-paragraph le lf iq lg b lh mz ka lj lk na kd lm ln nb lp lq lr nc lt lu lv nd lx ly lz ij bi translated">我们提出了一个端到端的基于变压器的视频实例分割模块与正弦表示网络(SRN)，即TT-SRN，以解决视觉感知任务。我们的方法，TT-SRN，将VIS任务视为单一状态下的直接预测问题集，使我们能够将时间信息与空间信息聚合在一起。一组视频帧特征由双变换器提取，然后传播到原始变换器以产生一系列实例预测。这些由变换器产生的实例级信息然后通过修改的正弦表示网络，以获得最终的实例级类id和边界框，以及自参与的3-D卷积，以获得分段掩码。在其内部机制中，TT-SRN是一个自然的框架，它通过相似性学习来处理跟踪和分割，使系统能够产生一组快速而准确的预测。实例序列匹配算法改编自[29]以跨视频帧跟踪实例。TT-SRN使用基于集合的全局损失进行端到端训练，通过二分匹配强制进行唯一预测。因此，在不牺牲分段掩码质量的情况下，流水线的总体复杂度显著降低。由于双变压器是最快的方法之一，第一次在没有隐式CNN架构的情况下解决了VIS问题。我们的方法可以很容易地分成子组件，以产生单独的实例遮罩和边界框，这将使它成为许多视觉任务的统一方法。在本节中，TT-SRN被划分为子模块，并对其进行了详细描述。</p><p id="94af" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated"><strong class="lg ja"> 3.1。双变压器</strong></p><p id="4472" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">最近，在论文[9]中提出了twins，并证明了空间定向视觉变换器可以优于经典的CNN[9]。这里，我们将Twins-SVT网络集成到我们的案例中，以产生实例级的特性。他们的孪生转换器基于空间可分离的自我注意(SSSA)网络，该网络由局部分组自我注意(LSA)和全局子采样注意(GSA)组成[9]。由于其空间上可分离的模块，功能的质量大大提高。在小节中，我们将详细描述SSSA模块。</p><p id="0693" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">3.1.1局部分组自我注意(LSA)</p><p id="abab" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">在LSA，2-D特征地图被分成子窗口，使得能够在每个子窗口内自我关注。特征图被分成m×n个子窗口，这导致每4个窗口由HW mn元素组成，其中H，W表示图像维度。通过将图像划分成m×n个区域，计算成本从O(H * W * d)降低到O( H *W /(m*n) *d)，其中d是自我注意维度。在这一点上，我们没有对窗口中不重叠的区域做任何进一步的关联。因此，GSA模块开始发挥作用。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi pi"><img src="../Images/b77ca71bf7bffc13653eef4ac20aed31.png" data-original-src="https://miro.medium.com/v2/resize:fit:1064/format:webp/1*t5ZLuDkD4OUcJqdnSA1EKA.png"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">正弦表示网络的体系结构。它由两个警笛层和一个额外的全连接层组成，具有GELU激活功能。(图片由作者提供)</p></figure><p id="a0fc" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated"><strong class="lg ja"> 3.1.2全局子采样注意(GSA) </strong></p><p id="0d5b" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">因为我们需要在自我注意机制中进一步定位，所以需要全局自我注意来在非重叠区域中建立连接。在GSA模块中，来自本地参与窗口的单个代表性关键信息被用于计算全局注意力。然而，随着全局注意的计算，计算成本将增加到O(H *W *d)。为了防止这种情况，通过平均池、深度方向的步长卷积和规则步长卷积对局部参与特征进行子采样。结果表明，规则步长卷积性能最佳[9]。数学上，SSSA模块执行以下计算。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi pj"><img src="../Images/440dfd8af2b3ae2af4463761dd1f8c67.png" data-original-src="https://miro.medium.com/v2/resize:fit:644/format:webp/1*2ciugHFFKNdytDgImkWsEw.png"/></div></figure><p id="f127" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">对于i = 1，…，m和j = 1，…，n其中LSA表示局部分组自我注意，GSA表示全局子采样注意，FFN表示前馈网络，LayerNorm表示层归一化层[2]。两个注意力模块都以多头方式进行。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi pk"><img src="../Images/b31792ee7fad9b95972b30be1947cd98.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ij0ohueit9jOx_QEK4HXBQ.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">双变压器的架构[9]</p></figure><p id="6b34" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated"><strong class="lg ja"> 3.2。经典变形金刚</strong></p><p id="1d3b" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">具有6个编码器层、6个解码器层以及GELU激活[15]的经典变换器架构适于执行基于实例的查询生成。传统转换器的输出是实例的建议加上额外的无对象查询。该转换器的使用非常类似于对象检测模型DETR [7]中的转换器。在训练期间，执行二分匹配，通过唯一地将预测与基础事实分配来监督模型。没有匹配的预测应该产生“无对象”类预测，因此实例查询的数量应该大于视频帧中实例的数量。转换器的核心由编码器和解码器结构组成，这将在下面的小节中讨论。</p><p id="bca9" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated"><strong class="lg ja"> 3.2.1时空位置编码</strong></p><p id="d6dc" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">由于变换器架构是置换不变的，因此时空位置编码对于精确位置信息的建模是必要的。时空位置编码基于正弦波，并且是经典位置编码的3d版本。我们的位置编码有3个不同的维度，即时间、水平和垂直。设d为最终级联的信道位置编码维数，那么我们独立地使用具有不同频率的d/3正弦函数如下</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi pl"><img src="../Images/0f89371a02c1908bd1a88c36a455a721.png" data-original-src="https://miro.medium.com/v2/resize:fit:676/format:webp/1*VGdHNOcwDjZJej7QkXsDzw.png"/></div></figure><p id="4b4d" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">其中w_k = (1/10000)^(2*k/ (d /3))，pos是该维度中的位置。与传统的位置编码一样，这些三维位置编码被添加到输入中。</p><p id="3a59" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated"><strong class="lg ja"> 3.2.2变压器编码器</strong></p><p id="d033" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">大小为6的变换器编码器层适于学习实例方面的相似性，该相似性稍后将被传播到解码层以产生最终的实例级查询。从双变换器提取的特征被传递到具有256个输出潜在大小的单个卷积层。因此，到变换器编码器的输入是R^(NxLxHxW的形状，其中n是批量大小，l是潜在大小，h和w是单个卷积层输出的高度和宽度。请注意，时间顺序是根据输入顺序保留的。每个编码器层像常规一样执行多头自关注机制。</p><p id="ac8f" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated"><strong class="lg ja"> 3.2.3变压器解码器</strong></p><p id="fbef" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">然后，编码特征序列通过变换器解码器层，以产生实例查询预测序列。在这一层，一系列可学习的实例查询也被传递到解码器层。实例查询是固定数量的输入嵌入，以表示实例预测的总数。为了安全起见，实例查询的数量总是大于映像中实例的数量。二分匹配独特地执行实例式分配，暴露的预测被称为“无对象查询”。例如，假设我们在帧t中产生n_t个实例预测，那么让q是一般实例查询的大小，使得在所有帧中q &gt; n_t。</p><p id="0e33" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated"><strong class="lg ja"> 3.3。正弦表示网络</strong></p><p id="7e51" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">正弦表示网络是最近在论文[25]中提出的表示学习方法。正弦层由完全连接的层组成，其独特的初始化在具有正弦激活层的论文[25]中介绍。整体架构如图2所示。在这项工作中，我们针对我们的情况修改了它们的架构，在具有端GELU非线性的正弦层之间添加内部压差层[15]，以产生实例级端特性。这些最终特征然后被传播到分类、包围盒检测和实例分割分支。我们意识到隐式神经表示的周期性激活，并证明这些网络，被称为正弦表示网络或塞壬，非常适合密集的预测任务。我们的消融研究表明，末端预测层的周期性激活函数可以适用于密集预测任务。</p><p id="d506" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated"><strong class="lg ja"> 3.4。实例序列匹配</strong></p><p id="9a24" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">TT-SRN的一个重要方面，即实例序列匹配，改编自文献[29]，通过二分匹配将实例预测与基础事实唯一地分配，以监督模型。此外，该模块使我们能够推断出预测实例的精确顺序，该顺序随后能够跨视频跟踪实例。匹配损失既考虑了类别预测，也考虑了预测值和实际值的相似性[7]。设y表示地面真实盒子对象集，y∞= ˜y^n_i=1 n个预测集。我们的损失产生了预测和基本事实之间的最佳二分匹配。为了计算两个集合之间的二分匹配，计算以下最小化。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi pm"><img src="../Images/98266fad96a574309ab1017e568cf35b.png" data-original-src="https://miro.medium.com/v2/resize:fit:654/format:webp/1*Zt630kMKvYDqsT4y_LQL-w.png"/></div></figure><p id="b1c7" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">其中L_match(y_i，y _σ\\( I))是基础事实和预测之间的实例匹配成本。这个分配问题是用匈牙利方法计算的，匈牙利方法是一种组合优化算法，在多项式时间内解决分配问题[7]。匹配过程考虑了类别预测以及预测和基本事实框的相似性。设注释的每个元素I由y_i = (c_i，b_i)表示，其中ci是目标类，bi是向量，表示基本事实归一化坐标。这些坐标被组织为中心、高度和宽度，并且它们与图像大小相关。然后，对于索引为σ(i)的预测，设\u p _σ(I)(ci)表示类别概率，\u b _σ(I)为预测盒。因此，我们可以定义Lmatch(y_i，y _σ\\( I))如下。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi pn"><img src="../Images/255f288ea0754423c16fe30bcb3e8bdf.png" data-original-src="https://miro.medium.com/v2/resize:fit:662/format:webp/1*rQIQWoKqm5r7SL7E5-cobQ.png"/></div></figure><p id="ec20" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">这个过程监督一个模型，并在启发式分配过程中发挥重要作用。在经典的对象检测或实例分割任务中(如Mask RCNN [13])，这些程序是匹配建议的对应物或基础事实的锚。与经典方法显著不同的是，二部匹配唯一地分配。在这一点上，我们分配了预测和它们的基本事实，所以我们需要计算损失，在我们的例子中，所有匹配对的匈牙利损失。给定一对一分配，匈牙利损失将损失计算为类预测的负对数似然、盒和实例序列的掩膜损失的线性组合，如下所示。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi po"><img src="../Images/188decb622154ab4d84e8433182f3c22.png" data-original-src="https://miro.medium.com/v2/resize:fit:686/format:webp/1*zwtuxcpFyY8CeSPX81owwA.png"/></div></figure><p id="fa08" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">其中σ\是之前计算的最佳分配。该损失用于以端到端的方式训练模型。接下来，我们需要定义L_box和L_mask。L_box的计算类似于DETR [7]中的计算，如下所示。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi pp"><img src="../Images/4cca86a64199a932683d42262d1d68a3.png" data-original-src="https://miro.medium.com/v2/resize:fit:678/format:webp/1*3WmoALQb2htJ92r_DjBdeA.png"/></div></figure><p id="94b4" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">其中，λ_IoU和λ_L1是超参数。请注意，损失随着框架内实例的数量而标准化。</p><p id="8827" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated"><strong class="lg ja"> 3.5。实例序列分割</strong></p><p id="c9b7" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">TT-SRN的另一个重要方面，即实例序列分割模块，改编自论文[29]以产生末端分割掩模。在内部，该模块累积帧的实例特征，然后对这些累积的特征执行分割。对于每个帧，由变换器的末端解码器层收集的实例预测和由变换器的末端编码器层收集的变换器编码特征通过自关注模块。这些被关注的特征然后与由双变换器收集的特征和从变换器的末端编码器生成的编码特征融合。这个过程与维斯托[29]和DETR [7]的情况非常相似。然后，具有不同大小的实例级特征被馈送到可变形卷积层[10]，该可变形卷积层利用额外的偏移来增加模块中的空间采样位置，并从目标任务学习偏移，而无需额外的监督[10]。然后，呈R^(1xCxTxtHxW形状的融合图(其中c是通道维度，t是时间维度，h和w是空间特征维度)被馈送到具有组归一化[30]和GELU非线性[15]的3d卷积层中。在末端层，单个卷积层具有1个输出信道维度，用于获得分段掩码。最后，我们需要定义Lmask来完成损失函数。L_mask通过组合dice [22]和聚焦损失[18]计算如下。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi pq"><img src="../Images/c8e331a07c0f881b1d97e9e6374e3d4a.png" data-original-src="https://miro.medium.com/v2/resize:fit:670/format:webp/1*z6h82DJU3oD9pa-a_xve9Q.png"/></div></figure><h1 id="5203" class="mh mi iq bd mj mk ml mm mn mo mp mq mr kf ms kg mt ki mu kj mv kl mw km mx my bi translated">4.TT-SRN结果</h1><p id="d382" class="pw-post-body-paragraph le lf iq lg b lh mz ka lj lk na kd lm ln nb lp lq lr nc lt lu lv nd lx ly lz ij bi translated">在本节中，我们在YouTubeVIS数据集上展示了我们的结果[31]。YouTube-VIS是一个大型可扩展数据集，包含2，883个高分辨率YouTube视频、2，238个训练视频、302个验证视频和343个测试视频。类别标签集包括40个常见对象，如人、动物和车辆，总共有4，883个独特的视频实例，产生131k高质量的面向人的注释。因为测试集的评估是封闭的，所以评估结果基于验证集。4.1.</p><p id="6e66" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated"><strong class="lg ja"> 4.1实施细节</strong></p><p id="0bb0" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">我们继承了TT-SRN第一级双变压器[9]中使用的超参数。因此，嵌入维数被选择为64，块大小为4，局部块大小为7，深度为1。同样，双变压器第二级的超参数是128，2，7，1。对于第三阶段，选择256，2，7，5作为第三阶段的超参数。在最后阶段，嵌入大小为512，块大小为2，局部块大小为7，深度为4。这里，深度是指“双变压器”部分描述的变压器模块数量。请参考图？？。单个卷积层的隐藏大小被选择为256。在经典变压器中，有6个编码层和6个解码层，多头大小为8。在所有编码器-解码器中，变压器模块的内部激活是GELU [15]。在SRN阶段，退出概率选择为0.2。所有SRN层都使用论文[25]中描述的专用初始化方案进行初始化。那么，YouTube-VIS中带注释的视频长度的最大数是36 [31]，我们选择这个值作为输入视频长度。因此，不需要后处理来关联来自一个视频的不同剪辑。通过这种方式，我们的模型可以在单个阶段中进行端到端的训练。因为我们的模型预测每个视频帧有10个对象，所以我们将查询数设置为360。TT-SRN是通过PyTorch 1.8实现的[23]。由于其简单的构建模块，TT-SRN可推广和扩展到其他框架和视觉任务。我们还在我们的项目页面上提供单独的TT-SRN实例分割和对象检测版本。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi pr"><img src="../Images/fea780708bd0082627580b5a3edc6193.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*t0BdI79-vYgZDNZnTbIM8A.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">表1:YouTube-VIS上的视频实例分割图(%)结果[31]。结果是从他们的论文中获得的。</p></figure><p id="7126" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">在训练阶段，我们使用AdamW [21]优化所有层，从1e-4的学习速率开始，每3个时期衰减0.1。TT-SRN用18个历元进行训练，批量大小选择为16。传统的变压器权重由COCO [19]中预先训练的DETR [7]初始化。所有视频帧都按照每个通道的方式用ImageNet平均值和标准偏差值进行标准化。然后，将所有视频帧的大小调整为300 x 540，以适合GPU。我们仅使用概率为0.5的随机水平翻转作为视频数据增强。TT-SRN在一个8GB RAM的Tesla K80 GPU上训练5天。</p><p id="40e3" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">在推理阶段，TT-SRN结构没有变化。因此，我们的模型的训练和推理形状是完全相同的。此外，不需要手工制作的后处理来跨视频帧关联实例。我们将阈值设置为分数高于确定阈值的保留实例，以获得最终结果。我们将这个阈值设置为0.6。在视频帧中有一些实例被识别为不同的类。那时，我们使用最频繁预测的类别。</p><p id="bd9e" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated"><strong class="lg ja"> 4.2评估指标</strong></p><p id="96d6" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">使用图像实例分割中的标准评估指标进行评估，并根据我们的新任务进行修改[31]。具体来说，指标8是各种条件下的平均精度(AP)和平均召回率(AR) [31]。AP被定义为精确度-召回曲线下的面积[31]。置信度得分用于绘制曲线。AP是多个交集合并(IoU)阈值的平均值[31]。平均回忆描述了回忆-IoU曲线下加倍的区域。作为有条件的AP和AR，我们遵循COCO评估程序，因为它需要10个IoU阈值，从50%到95%，在步骤5%。由于我们在视频领域，我们需要在评估中包括时间一致性，例如，即使模型产生成功的分割，如果它未能跟踪实例，则表明性能不佳。因此，我们的IoU计算不同于图像实例分割，因为每个实例都包含一个掩模序列[31]，因此IoU计算通过在视频帧之间累加IoU而扩展到一批视频帧。IoU计算如下。在这里，m^i_t代表基本事实，m˜^i_t代表假设。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi ps"><img src="../Images/f429ef033aaad74bc51b40a076ac0a22.png" data-original-src="https://miro.medium.com/v2/resize:fit:790/format:webp/1*42toxG7REBE6m7WA0j3eQQ.png"/></div></figure><p id="8339" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated"><strong class="lg ja"> 4.3主要结果</strong></p><p id="f3f5" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">我们在YouTube-VIS上比较了TT-SRN和VIS中其他最新方法的速度和准确性。由于我们的方法是单阶段和端到端可训练的，我们优先将我们的方法与单阶段和端到端可训练的方法进行比较。我们将TTSRN与Mask Track R-CNN [31]、MaskProp [3]、VisTR [29]和STEm-Seg [1]进行了比较。结果总结见表1。在没有任何编织的情况下，TT-SRN是最快的方法之一，在单个GPU中以55.3 FPS的速度运行，并通过大幅超越VIS基线模型，在YouTube-VIS上实现了39.3 %的竞争精度。就速度而言，TT SRN在最先进的VIS模型中名列第二。就速度而言，目前的赢家是VisTR [29]，因为它使用ResNet-101 [14]主干网运行速度为57.7，使用ResNet-50主干网运行速度为69.9。在速度方面，TT-SRN比当前的VIS基线模型Mask Track R-CNN(运行速度为20.0 FPS)快很多。这种差异源于TTSRN的简单的基于注意力的机制，该机制需要最少的步骤来产生VIS预测。另一种竞争方法STEm-seg以2.1 FPS运行，这对于实时目的来说是非常不可用的。他们的论文中没有提到MaskProp的速度[3]。请注意，数据加载和预处理步骤时间不包括在上述结果中。在准确性方面，TT-SRN远远优于掩模轨道R-CNN，因为我们的模型在YouTube-VIS的验证集上获得了39.3 %的地图分数，而掩模轨道R-CNN获得了30.3 %的地图分数。这一显著优势源于TTSRN的结构，该结构由所有组件中最先进的方法组成。此外，TT-SRN也大幅超过STEmseg，因为STEm-seg在ResNet-101主干上获得了34.6 %的mAP分数。由于TT-SRN与VisTR相似，具有ResNet-101骨架的VisTR比TT-SRN多0.8 %的图谱分数，而TT-SRN比具有ResNet-50骨架的VisTR多3.1 %的图谱分数。目前的获胜者，MaskProp获得了46.6 %的mAP评分，远远超过了TT-SRN。TT-SRN和MaskProp之间的差距源于MaskProp的多网络设计，它由时空采样网络[4]、特征金字塔网络[17]、混合任务级联网络[8]和高分辨率掩模细化后处理[3]组成。作为最简单的VIS架构之一，TT-SRN在所有竞争对手中取得了最快和最准确的结果。此外，TT-SRN可以很容易地划分为子组件，以执行单独的VIS任务，即对象检测、实例分割和分类。这使得我们的方法简单、统一和实时，而不牺牲实例掩码的质量。</p><p id="da0c" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated"><strong class="lg ja"> 5。结论</strong></p><p id="f155" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">在这篇文章中，我们提出了一个端到端的基于正弦表示网络(SRN)的视频实例分割模块，即TT-SRN，以解决视频实例分割任务。TT-SRN将VIS任务视为单一状态下的直接序列预测问题，使我们能够将时间信息与空间信息聚合在一起。为了产生从视频帧中提取的高质量特征，我们利用了双变压器。经典变换器用于产生一系列实例预测，这些预测随后通过改进的正弦表示网络得到最终结果。TT-SRN是一种自然范式，通过相似性学习处理跟踪，使系统能够产生快速准确的预测集。利用基于集合的全局损失来端到端地训练TT-SRN，该全局损失通过二分匹配来强制进行唯一的预测，从而在不牺牲分段掩码的质量的情况下降低流水线的总体复杂度。由于双变压器是最快的方法之一，第一次在没有传统CNN架构的情况下解决了VIS问题。我们的方法可以很容易地分成子组件，以产生单独的实例遮罩和边界框，这将使它成为许多视觉任务的统一方法。我们认为，视频实例分割是视频理解领域的一项关键任务，将革新计算机视觉研究社区。我们的项目页面位于<a class="ae pf" href="https://github.com/cankocagil/" rel="noopener ugc nofollow" target="_blank">https://github.com/cankocagil/</a>TT-SRN，TT-SRN的单独检测/分割版本位于<a class="ae pf" href="https://github.com/cankocagil/" rel="noopener ugc nofollow" target="_blank">https://github.com/cankocagil/</a>TT-SRN—-对象检测。</p><h1 id="77fb" class="mh mi iq bd mj mk ml mm mn mo mp mq mr kf ms kg mt ki mu kj mv kl mw km mx my bi translated">参考</h1><p id="6ac8" class="pw-post-body-paragraph le lf iq lg b lh mz ka lj lk na kd lm ln nb lp lq lr nc lt lu lv nd lx ly lz ij bi translated">[1] A. Athar、S. Mahadevan、A. Osep、L. Leal-Taix ˇ e和B. Leibe。Stem-seg:视频中实例分割的时空嵌入，2020年。</p><p id="ea9d" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">[2]巴、基罗斯和辛顿。图层归一化，2016。</p><p id="d21b" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">[3] G. Bertasius和L. Torresani。分类、分割和跟踪视频中的对象实例与掩模传播，2020。</p><p id="be95" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">[4] G. Bertasius、L. Torresani和J. Shi。时空采样网络视频中的对象检测，2018。</p><p id="931c" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">[5] A .比雷、z .格、l .奥特、f .拉莫斯和b .乌普克罗夫特。简单的在线和实时跟踪。2016年IEEE图像处理国际会议(ICIP)，2016年9月。</p><p id="87ff" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">[6]曹国伟、安维尔、乔拉卡尔、汗、庞和邵。Sipmask:快速图像和视频实例分割的空间信息保存，2020。</p><p id="cff3" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">[7] N .卡里翁、f .马萨、G. Synnaeve、N. Usunier、a .基里洛夫和S. Zagoruyko利用变压器进行端到端物体检测，2020年。</p><p id="30a9" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">[8]陈光诚、庞俊杰、王俊杰、熊毅、李、孙、冯、刘志军、史俊杰、欧阳俊杰、刘春春和林。用于实例分割的混合任务级联，2019。</p><p id="3743" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">[9]朱晓东、田志军、王永庆、张、任、魏、夏、沈春华。双胞胎:重新审视《视觉变形金刚》中空间注意力的设计，2021。</p><p id="b245" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">[10]戴军、齐、熊、李、张、胡、魏。可变形卷积网络，2017。</p><p id="2434" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">[11] A. Dosovitskiy，L. Beyer，a .，D. Weissenborn，X. Zhai，T. Unterthiner，M. Dehghani，M. Minderer，G. Heigold，S. Gelly等.一幅图像抵得上16x16个字:大规模图像识别的变形金刚.arXiv预印本arXiv:2010.11929，2020。</p><p id="1c72" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">[12] B. Hariharan，P. Arbelaez，R. Girshick和J. Malik。同时检测和分割，2014年。</p><p id="3703" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">[13] K. He，G. Gkioxari，P. Dollar和R. Girshick。面具r-cnn，2018。</p><p id="51db" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">[14]何国光、张晓松、任少宁和孙军。图像识别的深度残差学习，2015。</p><p id="7304" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">15d .亨德里克斯和k .金佩尔。高斯误差线性单位(gelus)，2020。</p><p id="3435" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">[16]林振中、洪耀桢、费里斯和何立波。基于改进的vae架构的视频实例分割跟踪。IEEE/CVF计算机视觉和模式识别会议(CVPR)的会议录，2020年6月。</p><p id="8adf" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">[17]林光宇、杜大伟、吉希克、何、哈里哈兰和贝隆吉。用于对象检测的特征金字塔网络，2017。</p><p id="368d" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">[18]t-y Lin、P. Goyal、R. Girshick、K. He和P. Dollar。密集物体探测的焦点损失，2018。</p><p id="025a" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">[19] T.-Y .林、m .梅尔、s .贝隆吉、l .布尔德夫、r .吉尔希克、j .海斯、p .佩罗娜、d .拉马南、C. L .兹尼克和p .杜尔达。微软coco:上下文中的常见对象，2015。</p><p id="a8f9" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">[20]刘振中，林永安，曹永安，胡海红，魏永安，张振中，林绍林，郭。Swin transformer:使用移位窗口的分层视觉转换器。arXiv预印本arXiv:2103.14030，2021。</p><p id="e505" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">[21] I. Loshchilov和F. Hutter。解耦权重衰减正则化，2019。</p><p id="9dd8" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">[22] F .米莱塔里、n .纳瓦布和S.-A .艾哈迈迪。V-net:用于体积医学图像分割的全卷积神经网络，2016年。</p><p id="50b5" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">[23] A. Paszke、S. Gross、F. Massa、A. Lerer、J. Bradbury、G. Chanan、T. Killeen、Z. Lin、N. Gimelshein、L. Antiga、A. Desmaison、A. Kopf、E. Yang、Z. DeVito、M. Raison、A. Tejani、S. Chilamkurthy、B. Steiner、L. Fang、J. Bai和S. Chintala。Pytorch:命令式风格，高性能深度学习库，2019。</p><p id="4a86" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">[24]鲁萨科夫斯基、邓、苏、克劳斯、萨特西、马、黄、卡帕西、科斯拉、伯恩斯坦、伯格和。2015年Imagenet大规模视觉识别挑战赛。</p><p id="7d84" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">[25] V. Sitzmann、J. N. P. Martel、A. W. Bergman、D. B. Lindell和G. Wetzstein。具有周期性激活功能的隐性神经表征，2020。</p><p id="57b5" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">[26] J. Son、M. Baek、M. Cho和B. Han。基于四元组卷积神经网络的多目标跟踪。2017年IEEE计算机视觉和模式识别大会(CVPR)，第3786–3795页，2017。</p><p id="80f4" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">[27] H. Touvron、M. Cord、M. Douze、F. Massa、A. Sablayrolles和H. Jegou。训练数据有效的图像转换器&amp;通过注意力进行提炼。arXiv预印本arXiv:2012.12877，2020。</p><p id="4830" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">[28] A. Vaswani、N. Shazeer、N. Parmar、J. Uszkoreit、L. Jones、A. N. Gomez、L. Kaiser和I. Polosukhin你只需要关注，2017。</p><p id="06cf" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">[29]王延东、徐志勇、王晓东、沈春华、程炳良、沈海涛和夏。《变形金刚》端到端视频实例分割，2021。</p><p id="d0f2" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">[30]吴彦祖和何国光。群体常态化，2018。</p><p id="18d4" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">[31]杨立群、范友林和徐。视频实例分割，2019。</p></div></div>    
</body>
</html>