<html>
<head>
<title>Gradient Boosting for probability distributions</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">概率分布的梯度增强</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/gradient-boosting-for-probability-distributions-6848a049d8a2?source=collection_archive---------29-----------------------#2021-07-26">https://towardsdatascience.com/gradient-boosting-for-probability-distributions-6848a049d8a2?source=collection_archive---------29-----------------------#2021-07-26</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="4c48" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">TLDR-关于多维输出的梯度增强以及如何将其应用于生成概率分布的一些想法。如果您需要的不仅仅是对boosting问题的点估计，本文可能会有所帮助。</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/388ae956f9dda018cf4934afe259ec5c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KdW1k5EcTVVyiakOf87v-Q.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">伊恩·基夫在<a class="ae ky" href="https://unsplash.com/s/photos/trees?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure></div><div class="ab cl kz la hx lb" role="separator"><span class="lc bw bk ld le lf"/><span class="lc bw bk ld le lf"/><span class="lc bw bk ld le"/></div><div class="im in io ip iq"><h1 id="a563" class="lg lh it bd li lj lk ll lm ln lo lp lq jz lr ka ls kc lt kd lu kf lv kg lw lx bi translated">介绍</h1><p id="1830" class="pw-post-body-paragraph ly lz it ma b mb mc ju md me mf jx mg mh mi mj mk ml mm mn mo mp mq mr ms mt im bi translated">梯度推进可以说是当今最流行的机器学习算法之一。将多个弱学习者结合起来以产生一个强学习者似乎好得不像是真的。</p><p id="fbbb" class="pw-post-body-paragraph ly lz it ma b mb mu ju md me mv jx mg mh mw mj mk ml mx mn mo mp my mr ms mt im bi translated">然而，像<a class="ae ky" href="https://xgboost.readthedocs.io/en/latest/" rel="noopener ugc nofollow" target="_blank"> xgboost </a>这样的软件包经常在许多在线比赛中大放异彩。“只工作”的能力也使助推模型成为行业应用的有利工具。</p><p id="5cb6" class="pw-post-body-paragraph ly lz it ma b mb mu ju md me mv jx mg mh mw mj mk ml mx mn mo mp my mr ms mt im bi translated">像<a class="ae ky" href="https://lightgbm.readthedocs.io/en/latest/" rel="noopener ugc nofollow" target="_blank"> lightGBM </a>或said xgboost这样的流行库为各种不同的用例提供了许多工具。然而，一个特殊的功能，即任意多输出升压，似乎还没有在这些包中可用。</p><p id="f1b4" class="pw-post-body-paragraph ly lz it ma b mb mu ju md me mv jx mg mh mw mj mk ml mx mn mo mp my mr ms mt im bi translated">快速的谷歌搜索会提供一些关于如何在这种情况下使用sklearn的MultiOutputRegressor的解释。</p><p id="104a" class="pw-post-body-paragraph ly lz it ma b mb mu ju md me mv jx mg mh mw mj mk ml mx mn mo mp my mr ms mt im bi translated">这种解决方案可以适用于合适的损失函数。尽管如此，还是有多输出处理器方法失败的情况。在这些情况下，我们需要更深入地了解渐变增强的背景，并手动实现一些元素。</p><blockquote class="mz na nb"><p id="6c8a" class="ly lz nc ma b mb mu ju md me mv jx mg nd mw mj mk ne mx mn mo nf my mr ms mt im bi translated">“有些情况下，多输出梯度提升的直观解决方案会失败。在这些情况下，我们需要更深入地研究boosting算法的理论，并手动实现一些元素。”</p></blockquote><p id="c9a9" class="pw-post-body-paragraph ly lz it ma b mb mu ju md me mv jx mg mh mw mj mk ml mx mn mo mp my mr ms mt im bi translated">因此，今天，我想给你一些想法，如何多输出梯度升压可以'手动'完成。重点是提升目标变量概率分布的多个参数。如果你对这个理论不感兴趣，可以跳过下面的部分。</p></div><div class="ab cl kz la hx lb" role="separator"><span class="lc bw bk ld le lf"/><span class="lc bw bk ld le lf"/><span class="lc bw bk ld le"/></div><div class="im in io ip iq"><h1 id="a537" class="lg lh it bd li lj lk ll lm ln lo lp lq jz lr ka ls kc lt kd lu kf lv kg lw lx bi translated">理论(根据你的喜好跳过这一部分)</h1><p id="3de3" class="pw-post-body-paragraph ly lz it ma b mb mc ju md me mf jx mg mh mi mj mk ml mm mn mo mp mq mr ms mt im bi translated">梯度增强背后的一般思想可以概括为三个步骤:</p><ol class=""><li id="a961" class="ng nh it ma b mb mu me mv mh ni ml nj mp nk mt nl nm nn no bi translated">使用一个初始的浅薄的学习者来最小化给定的损失</li><li id="e59a" class="ng nh it ma b mb np me nq mh nr ml ns mp nt mt nl nm nn no bi translated">让第二个浅层学习者学习预测相对于第一个浅层学习者预测的损失导数</li><li id="138a" class="ng nh it ma b mb np me nq mh nr ml ns mp nt mt nl nm nn no bi translated">将第二个学习者的预测乘以一个常数加到第一个学习者的预测上</li></ol><p id="183a" class="pw-post-body-paragraph ly lz it ma b mb mu ju md me mv jx mg mh mw mj mk ml mx mn mo mp my mr ms mt im bi translated">选择3)中的常数，使得组合预测最小化目标损失。之后，我们分别重复步骤2)和3 ),直到达到最大迭代次数。</p><p id="aa6c" class="pw-post-body-paragraph ly lz it ma b mb mu ju md me mv jx mg mh mw mj mk ml mx mn mo mp my mr ms mt im bi translated"><strong class="ma iu">注</strong>:梯度提升的解释大多考虑2)中的负导数。在标准梯度下降中，这是最小化所必需的。假定3)中的学习常数可以具有任一符号，然而导数的符号可以忽略。</p><p id="ab3c" class="pw-post-body-paragraph ly lz it ma b mb mu ju md me mv jx mg mh mw mj mk ml mx mn mo mp my mr ms mt im bi translated">为了简单起见，让我们研究正导数。</p><h2 id="8611" class="nu lh it bd li nv nw dn lm nx ny dp lq mh nz oa ls ml ob oc lu mp od oe lw of bi translated">对步骤1)初始化的深入解释</h2><p id="7864" class="pw-post-body-paragraph ly lz it ma b mb mc ju md me mf jx mg mh mi mj mk ml mm mn mo mp mq mr ms mt im bi translated">从技术上讲，我们从迭代的初始预测开始<em class="nc"> k=0 </em>:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi og"><img src="../Images/808b0054ac6cb14e755b7d28e61b24ab.png" data-original-src="https://miro.medium.com/v2/resize:fit:392/format:webp/0*db56wu0Xt3LymKaL.png"/></div></figure><p id="0fec" class="pw-post-body-paragraph ly lz it ma b mb mu ju md me mv jx mg mh mw mj mk ml mx mn mo mp my mr ms mt im bi translated">该初始预测可以是优化的常数</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/4656bebe97f43fb35fd5e487f8eeecd8.png" data-original-src="https://miro.medium.com/v2/resize:fit:804/format:webp/0*E_aPNpL13lPjfoZU.png"/></div></figure><p id="a2c1" class="pw-post-body-paragraph ly lz it ma b mb mu ju md me mv jx mg mh mw mj mk ml mx mn mo mp my mr ms mt im bi translated">对于一个任意的损失。为了简单起见，我们将定义一个固定常数，而不首先优化它。</p><p id="d5cb" class="pw-post-body-paragraph ly lz it ma b mb mu ju md me mv jx mg mh mw mj mk ml mx mn mo mp my mr ms mt im bi translated">对于连续目标，损失函数通常是均方误差(MSE ),尽管这不是唯一的可能性。正如你在<a class="ae ky" href="https://xgboost-clone.readthedocs.io/en/latest/parameter.html#learning-task-parameters" rel="noopener ugc nofollow" target="_blank"> xgboost文档</a>中看到的，有更多的选项可用。</p><p id="c928" class="pw-post-body-paragraph ly lz it ma b mb mu ju md me mv jx mg mh mw mj mk ml mx mn mo mp my mr ms mt im bi translated">对损失的唯一要求是，它必须相对于预测是可微的。</p><h2 id="a02a" class="nu lh it bd li nv nw dn lm nx ny dp lq mh nz oa ls ml ob oc lu mp od oe lw of bi translated">步骤2)的深入解释)——梯度估计</h2><p id="061e" class="pw-post-body-paragraph ly lz it ma b mb mc ju md me mf jx mg mh mi mj mk ml mm mn mo mp mq mr ms mt im bi translated">显然，在这一步我们需要计算损失函数的导数。最简单的方法是从一般的公式开始，然后看一个具体的例子。对于任意的、可适当微分的损失，我们有:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oi"><img src="../Images/58837aceed2884c951abc0201f83d419.png" data-original-src="https://miro.medium.com/v2/resize:fit:398/format:webp/0*ROGa8reng0XGmgjA.png"/></div></figure><p id="cd44" class="pw-post-body-paragraph ly lz it ma b mb mu ju md me mv jx mg mh mw mj mk ml mx mn mo mp my mr ms mt im bi translated">接下来，让我们使用MSE作为一个实际的例子。MSE很简单</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/ae6f6edd60cf9b06bf3c53dc0fa3076e.png" data-original-src="https://miro.medium.com/v2/resize:fit:872/format:webp/0*VsoYfQNY-cA6Q3xo.png"/></div></figure><p id="414a" class="pw-post-body-paragraph ly lz it ma b mb mu ju md me mv jx mg mh mw mj mk ml mx mn mo mp my mr ms mt im bi translated">将这个代入上面的导数公式，我们得到</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ok"><img src="../Images/611c675b93a22a2b91ca8e84afab7ee4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*C62kdNYuVto8gpgO.png"/></div></div></figure><p id="b442" class="pw-post-body-paragraph ly lz it ma b mb mu ju md me mv jx mg mh mw mj mk ml mx mn mo mp my mr ms mt im bi translated">因此，对MSE损失的梯度提升等同于使用估计残差。请注意，这不是梯度增强的一般情况。</p><p id="d189" class="pw-post-body-paragraph ly lz it ma b mb mu ju md me mv jx mg mh mw mj mk ml mx mn mo mp my mr ms mt im bi translated">下一次迭代学习者现在必须估计残差:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ol"><img src="../Images/1b1e27618933e6403d187ae2bb0d84e6.png" data-original-src="https://miro.medium.com/v2/resize:fit:386/format:webp/0*Q2gSD4-2Giy2erKE.png"/></div></figure><p id="a7ca" class="pw-post-body-paragraph ly lz it ma b mb mu ju md me mv jx mg mh mw mj mk ml mx mn mo mp my mr ms mt im bi translated">或者，对于一般情况:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi om"><img src="../Images/b9916e31477f56ca422db970b6f3d743.png" data-original-src="https://miro.medium.com/v2/resize:fit:756/format:webp/0*bXHdgbScU45Q5D-w.png"/></div></figure><p id="2fc8" class="pw-post-body-paragraph ly lz it ma b mb mu ju md me mv jx mg mh mw mj mk ml mx mn mo mp my mr ms mt im bi translated">这是针对数据集中的每个观测值进行的。为了清楚起见，让我们引入下标索引<em class="nc"> i </em>来表示单个实例:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi on"><img src="../Images/33fd713c7811086ebadbfeaa3afc1e72.png" data-original-src="https://miro.medium.com/v2/resize:fit:858/format:webp/0*BlhkhFYmTI6_imFP.png"/></div></div></figure><h2 id="08cb" class="nu lh it bd li nv nw dn lm nx ny dp lq mh nz oa ls ml ob oc lu mp od oe lw of bi translated">步骤3)的深入解释)-模型更新</h2><p id="656e" class="pw-post-body-paragraph ly lz it ma b mb mc ju md me mf jx mg mh mi mj mk ml mm mn mo mp mq mr ms mt im bi translated">最后，我们需要更新完整的模型，如下所示:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oo"><img src="../Images/a92ff84155fd63c230ec1580001dba24.png" data-original-src="https://miro.medium.com/v2/resize:fit:976/format:webp/0*r8q2EaYe66FRSSNG.png"/></div></figure><p id="6c96" class="pw-post-body-paragraph ly lz it ma b mb mu ju md me mv jx mg mh mw mj mk ml mx mn mo mp my mr ms mt im bi translated">这是通过对大小为<em class="nc"> N </em>的完整数据集的简单优化问题来完成的:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi op"><img src="../Images/d64f6b7217066a9f36ad43c7bd9ccf72.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*_6RKcpPYpuG1dNxU.png"/></div></div></figure><p id="b438" class="pw-post-body-paragraph ly lz it ma b mb mu ju md me mv jx mg mh mw mj mk ml mx mn mo mp my mr ms mt im bi translated">我们可以向优化器提供梯度信息，或者使用例如<em class="nc">scipy . optimize . minimize</em>进行黑盒(没有显式导数)优化。</p><p id="4b03" class="pw-post-body-paragraph ly lz it ma b mb mu ju md me mv jx mg mh mw mj mk ml mx mn mo mp my mr ms mt im bi translated">从这里开始，我们只需要重复上述步骤，直到我们得到一个总共有<em class="nc"> K+1 </em>个浅层学习者的模型:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oq"><img src="../Images/c92477e8990e5699a4e5728ce333097a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1142/format:webp/1*hCj0b7-Ew0yvenmPwhES-w.png"/></div></figure><p id="0336" class="pw-post-body-paragraph ly lz it ma b mb mu ju md me mv jx mg mh mw mj mk ml mx mn mo mp my mr ms mt im bi translated">就我个人而言，我发现如此简单的算法在实践中却能产生如此好的结果，这真是太神奇了。</p></div><div class="ab cl kz la hx lb" role="separator"><span class="lc bw bk ld le lf"/><span class="lc bw bk ld le lf"/><span class="lc bw bk ld le"/></div><div class="im in io ip iq"><h1 id="a2fe" class="lg lh it bd li lj lk ll lm ln lo lp lq jz lr ka ls kc lt kd lu kf lv kg lw lx bi translated">多输出梯度升压</h1><p id="fed7" class="pw-post-body-paragraph ly lz it ma b mb mc ju md me mf jx mg mh mi mj mk ml mm mn mo mp mq mr ms mt im bi translated">最后，我们可以切换到本文的实际主题。对于这种情况，我们最重要的元素是一个合适的多输出损失函数。这种损失必须能够将多个模型的输出压缩成一个量。</p><p id="cc69" class="pw-post-body-paragraph ly lz it ma b mb mu ju md me mv jx mg mh mw mj mk ml mx mn mo mp my mr ms mt im bi translated">让我们为具有<em class="nc"> M </em>个输出的多输出情况介绍一些符号:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi or"><img src="../Images/be032e2b00e29857bf9379579f04827c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1210/format:webp/0*pHzNTtyj4abldgz3.png"/></div></figure><h2 id="2b80" class="nu lh it bd li nv nw dn lm nx ny dp lq mh nz oa ls ml ob oc lu mp od oe lw of bi translated">多输出MSE</h2><p id="0699" class="pw-post-body-paragraph ly lz it ma b mb mc ju md me mf jx mg mh mi mj mk ml mm mn mo mp mq mr ms mt im bi translated">多输出连续回归最简单的扩展是单个均方误差的总和:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi os"><img src="../Images/cef48236e093ca875a4a2f3645be8ea9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1284/format:webp/0*r0VcLZ8iDNlaqORD.png"/></div></figure><p id="2e17" class="pw-post-body-paragraph ly lz it ma b mb mu ju md me mv jx mg mh mw mj mk ml mx mn mo mp my mr ms mt im bi translated">现在，我们需要计算每个输出的导数</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ot"><img src="../Images/f6cb689394db8f11ab88ccc5099acb86.png" data-original-src="https://miro.medium.com/v2/resize:fit:1146/format:webp/0*6iqYUjohElDZToYE.png"/></div></figure><p id="b1b1" class="pw-post-body-paragraph ly lz it ma b mb mu ju md me mv jx mg mh mw mj mk ml mx mn mo mp my mr ms mt im bi translated">这基本上告诉我们，我们可以为每个输出运行单独的梯度增强实例。在这种情况下，<em class="nc">多路输出处理器</em>会立即工作。</p><p id="bf8b" class="pw-post-body-paragraph ly lz it ma b mb mu ju md me mv jx mg mh mw mj mk ml mx mn mo mp my mr ms mt im bi translated">为了证明为什么这种分离不一定是可能的，考虑一个目标类为<em class="nc"> M </em>的多类问题。</p><h2 id="84f9" class="nu lh it bd li nv nw dn lm nx ny dp lq mh nz oa ls ml ob oc lu mp od oe lw of bi translated">多级梯度提升</h2><p id="00e3" class="pw-post-body-paragraph ly lz it ma b mb mc ju md me mf jx mg mh mi mj mk ml mm mn mo mp mq mr ms mt im bi translated">我们使用多类交叉熵损失和softmax变换输出来获得有效的类概率。这产生了</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ou"><img src="../Images/df129d2b0af42bf12c47489fcc90641f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*QvwpevZC3Fnb7Qkq.png"/></div></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ov"><img src="../Images/fe337390669ab4656877db237ea9d77f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1106/format:webp/0*BLaWDSY9VARAzDSh.png"/></div></figure><p id="6fe5" class="pw-post-body-paragraph ly lz it ma b mb mu ju md me mv jx mg mh mw mj mk ml mx mn mo mp my mr ms mt im bi translated">对目标使用onehot-encoding，上述和中只有一个元素非零。我们将此表示为<em class="nc"> m^star </em>，并去掉总和:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ow"><img src="../Images/aa47b8e1b10f2c98d1aaf1adcdd61dcc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1100/format:webp/0*3nnnhAgH1o2MgeII.png"/></div></figure><p id="133b" class="pw-post-body-paragraph ly lz it ma b mb mu ju md me mv jx mg mh mw mj mk ml mx mn mo mp my mr ms mt im bi translated">现在，对于导数，我们得到</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ox"><img src="../Images/80df95bff91edeee3410052336c8ce45.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*AigvVTrFkMC69viZ.png"/></div></div></figure><p id="e4b3" class="pw-post-body-paragraph ly lz it ma b mb mu ju md me mv jx mg mh mw mj mk ml mx mn mo mp my mr ms mt im bi translated">如您所见，每个输出的导数取决于所有其他渐变增强实例的输出。因此，不再可能仅仅将这个多输出问题作为单独的问题来处理。</p><p id="8817" class="pw-post-body-paragraph ly lz it ma b mb mu ju md me mv jx mg mh mw mj mk ml mx mn mo mp my mr ms mt im bi translated">这不同于上面的MSE例子。</p><p id="d9d4" class="pw-post-body-paragraph ly lz it ma b mb mu ju md me mv jx mg mh mw mj mk ml mx mn mo mp my mr ms mt im bi translated">更广泛的含义是，如果损失函数合适，我们只能使用以前的<em class="nc">多输出处理器</em>方法。一旦每个输出的导数不仅仅依赖于相应的输出，由此产生的梯度提升问题就变得不那么简单了。</p><blockquote class="mz na nb"><p id="3ded" class="ly lz nc ma b mb mu ju md me mv jx mg nd mw mj mk ne mx mn mo nf my mr ms mt im bi translated">"一旦每个输出的导数不仅仅依赖于相应的输出，由此产生的梯度提升问题就变得不可忽视."</p></blockquote><p id="1754" class="pw-post-body-paragraph ly lz it ma b mb mu ju md me mv jx mg mh mw mj mk ml mx mn mo mp my mr ms mt im bi translated">对于多类问题，我们幸运地在标准库中准备了必要的算法。然而，更好的东西可能需要手动实现。</p></div><div class="ab cl kz la hx lb" role="separator"><span class="lc bw bk ld le lf"/><span class="lc bw bk ld le lf"/><span class="lc bw bk ld le"/></div><div class="im in io ip iq"><h1 id="7b01" class="lg lh it bd li lj lk ll lm ln lo lp lq jz lr ka ls kc lt kd lu kf lv kg lw lx bi translated">概率分布的梯度增强</h1><p id="c372" class="pw-post-body-paragraph ly lz it ma b mb mc ju md me mf jx mg mh mi mj mk ml mm mn mo mp mq mr ms mt im bi translated">其中一件更有趣的事情是预测条件概率分布的参数。考虑一个一般的概率回归设置:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oy"><img src="../Images/e99b2038765821897c5cc075b54f5eef.png" data-original-src="https://miro.medium.com/v2/resize:fit:516/format:webp/0*boV1XTcG_Nwu8A10.png"/></div></figure><p id="3539" class="pw-post-body-paragraph ly lz it ma b mb mu ju md me mv jx mg mh mw mj mk ml mx mn mo mp my mr ms mt im bi translated">我们的目标是在给定输入<em class="nc"> x </em>的情况下，预测目标<em class="nc"> y </em>的概率分布。在简单线性回归中，这通常如下所示:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oz"><img src="../Images/36cc68736b1aebe90edc1f4867a19682.png" data-original-src="https://miro.medium.com/v2/resize:fit:950/format:webp/0*P2lfkIX3Mmvi6uWZ.png"/></div></figure><p id="85cf" class="pw-post-body-paragraph ly lz it ma b mb mu ju md me mv jx mg mh mw mj mk ml mx mn mo mp my mr ms mt im bi translated">这个条件概率只是一个高斯函数，其均值线性依赖于输入。方差被假定为常数。利用<a class="ae ky" href="https://en.wikipedia.org/wiki/Maximum_likelihood_estimation" rel="noopener ugc nofollow" target="_blank">最大似然</a>可以很容易地估计出各个参数。</p><p id="82f8" class="pw-post-body-paragraph ly lz it ma b mb mu ju md me mv jx mg mh mw mj mk ml mx mn mo mp my mr ms mt im bi translated">现在，让我们用梯度推进模型来代替线性均值和恒定方差项:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pa"><img src="../Images/78677ae9a6e55ccd592bdb8fd67e5dec.png" data-original-src="https://miro.medium.com/v2/resize:fit:1046/format:webp/0*71nkRKuj_BERTEZ_.png"/></div></figure><p id="c7ae" class="pw-post-body-paragraph ly lz it ma b mb mu ju md me mv jx mg mh mw mj mk ml mx mn mo mp my mr ms mt im bi translated">为了优化我们的模型，我们使用负对数似然函数作为损失函数</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ox"><img src="../Images/92e354eb39f8841bd3e5a50d6b492db0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*K773bc0-o1Zwmm3B.png"/></div></div></figure><p id="d712" class="pw-post-body-paragraph ly lz it ma b mb mu ju md me mv jx mg mh mw mj mk ml mx mn mo mp my mr ms mt im bi translated">请注意，对于两个分布参数，这确实是一个多输出问题。然而，目标变量本身仍然是一维的。</p><blockquote class="mz na nb"><p id="73b1" class="ly lz nc ma b mb mu ju md me mv jx mg nd mw mj mk ne mx mn mo nf my mr ms mt im bi translated">“条件高斯模型确实是一个两个分布参数的多输出问题。然而，目标变量本身仍然是一维的。”</p></blockquote><p id="5308" class="pw-post-body-paragraph ly lz it ma b mb mu ju md me mv jx mg mh mw mj mk ml mx mn mo mp my mr ms mt im bi translated">最后，我们可以计算必要的导数。对于平均值的梯度提升模型，我们有:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pb"><img src="../Images/d260cae3972246c2974d6bb42846464b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1218/format:webp/0*BEU9HAuM-15yWIaL.png"/></div></figure><p id="e5d0" class="pw-post-body-paragraph ly lz it ma b mb mu ju md me mv jx mg mh mw mj mk ml mx mn mo mp my mr ms mt im bi translated">对于标准差的梯度推进模型，我们有</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pc"><img src="../Images/a4d25efb0b41ec44b0140b1237701196.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SeKO8UQiBo9FpwDjjr11yw.png"/></div></div></figure><p id="e81a" class="pw-post-body-paragraph ly lz it ma b mb mu ju md me mv jx mg mh mw mj mk ml mx mn mo mp my mr ms mt im bi translated">理论部分到此结束。我们现在准备构建POC级别的实现。</p></div><div class="ab cl kz la hx lb" role="separator"><span class="lc bw bk ld le lf"/><span class="lc bw bk ld le lf"/><span class="lc bw bk ld le"/></div><div class="im in io ip iq"><h1 id="eb47" class="lg lh it bd li lj lk ll lm ln lo lp lq jz lr ka ls kc lt kd lu kf lv kg lw lx bi translated">快速演示</h1><p id="9869" class="pw-post-body-paragraph ly lz it ma b mb mc ju md me mf jx mg mh mi mj mk ml mm mn mo mp mq mr ms mt im bi translated">Numpy、sklearn和scipy提供了我们进行概念验证所需的一切。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="pd pe l"/></div></figure><h2 id="dfe6" class="nu lh it bd li nv nw dn lm nx ny dp lq mh nz oa ls ml ob oc lu mp od oe lw of bi translated">损失和导数函数</h2><p id="6a5f" class="pw-post-body-paragraph ly lz it ma b mb mc ju md me mf jx mg mh mi mj mk ml mm mn mo mp mq mr ms mt im bi translated">首先，我们定义简单正态分布的对数似然函数:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="pd pe l"/></div></figure><p id="1cb6" class="pw-post-body-paragraph ly lz it ma b mb mu ju md me mv jx mg mh mw mj mk ml mx mn mo mp my mr ms mt im bi translated">为了正确使用<em class="nc"> scipy.optimize.minimize </em>，我们创建了一个包装函数。该包装器可以作为lambda函数插入到优化器中。</p><p id="3105" class="pw-post-body-paragraph ly lz it ma b mb mu ju md me mv jx mg mh mw mj mk ml mx mn mo mp my mr ms mt im bi translated">请注意，对数似然现在有负号，因为我们想要最大化对数似然。这对于使用实际最小化算法来最大化对数似然是必要的。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="pd pe l"/></div></figure><p id="94be" class="pw-post-body-paragraph ly lz it ma b mb mu ju md me mv jx mg mh mw mj mk ml mx mn mo mp my mr ms mt im bi translated">最后，我们需要损失函数的导数:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="pd pe l"/></div></figure><h2 id="920f" class="nu lh it bd li nv nw dn lm nx ny dp lq mh nz oa ls ml ob oc lu mp od oe lw of bi translated">抽样资料</h2><p id="1c82" class="pw-post-body-paragraph ly lz it ma b mb mc ju md me mf jx mg mh mi mj mk ml mm mn mo mp mq mr ms mt im bi translated">我们的数据生成过程应该是非线性的，并且具有非恒定的方差。为了可视化，我们还保持输入数据是一维的。</p><p id="3d2d" class="pw-post-body-paragraph ly lz it ma b mb mu ju md me mv jx mg mh mw mj mk ml mx mn mo mp my mr ms mt im bi translated">满足这些要求的一个简单过程是</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pf"><img src="../Images/ae06e7d4e8dd86b5babda5e1dca5f57e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1342/format:webp/0*jfHZO1433F-fqffr.png"/></div></figure><p id="b1da" class="pw-post-body-paragraph ly lz it ma b mb mu ju md me mv jx mg mh mw mj mk ml mx mn mo mp my mr ms mt im bi translated">这个选择完全是任意的。我们只想看看我们的模型在这一点上是否可行。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="pd pe l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pg"><img src="../Images/e4096f125620b0c31ae3fd485cdbab72.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*CNdcajc3OEagnbQY.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">来自数据生成过程的样本数据(蓝点)，具有非线性平均值(绿线)和非恒定方差(绿色区域表示平均值的两个标准偏差)</p></figure><h2 id="9574" class="nu lh it bd li nv nw dn lm nx ny dp lq mh nz oa ls ml ob oc lu mp od oe lw of bi translated">运行模型</h2><p id="1669" class="pw-post-body-paragraph ly lz it ma b mb mc ju md me mf jx mg mh mi mj mk ml mm mn mo mp mq mr ms mt im bi translated">为了尽可能简单，我们不使用任何Python类。如果您想在这个基本实现的基础上构建一些东西，请随意构建包装器类。</p><p id="ebe1" class="pw-post-body-paragraph ly lz it ma b mb mu ju md me mv jx mg mh mw mj mk ml mx mn mo mp my mr ms mt im bi translated">我们的基础学习者将是简单的决策树树桩，比如100个。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="pd pe l"/></div></figure><p id="1f64" class="pw-post-body-paragraph ly lz it ma b mb mu ju md me mv jx mg mh mw mj mk ml mx mn mo mp my mr ms mt im bi translated">为了跟踪训练预测和γ，我们总共使用4个numpy数组。这意味着我们对均值和方差模型使用不同的伽玛。一方面，这引入了更多的过度拟合风险。</p><p id="b817" class="pw-post-body-paragraph ly lz it ma b mb mu ju md me mv jx mg mh mw mj mk ml mx mn mo mp my mr ms mt im bi translated">另一方面，增加灵活性可以改善结果。这是一个需要考虑的权衡，选择使用单独的gammas也是任意的。</p><p id="561e" class="pw-post-body-paragraph ly lz it ma b mb mu ju md me mv jx mg mh mw mj mk ml mx mn mo mp my mr ms mt im bi translated">我们将初始平均预测值设置为零；初始标准偏差预测被设置为1。</p><p id="7564" class="pw-post-body-paragraph ly lz it ma b mb mu ju md me mv jx mg mh mw mj mk ml mx mn mo mp my mr ms mt im bi translated">此外，我们将所有训练预测存储在一个<em class="nc">N×N _ trees</em>矩阵中，并将所有伽马存储在一个<em class="nc">1×N _ trees</em>矩阵中。这使我们能够简单地对聚合的增强输出的各个预测和伽马列进行相乘和求和。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="pd pe l"/></div></figure><p id="0bae" class="pw-post-body-paragraph ly lz it ma b mb mu ju md me mv jx mg mh mw mj mk ml mx mn mo mp my mr ms mt im bi translated">基础学习者将存储在列表中供以后使用。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="pd pe l"/></div></figure><p id="cb55" class="pw-post-body-paragraph ly lz it ma b mb mu ju md me mv jx mg mh mw mj mk ml mx mn mo mp my mr ms mt im bi translated">现在我们可以运行训练循环。如前所述，我们将在这里使用<em class="nc"> scipy.optimize.minimize </em>。为了不使这篇文章过于臃肿，我们把它作为一个黑盒优化器。这意味着我们不向函数提供任何梯度或hessian信息。</p><p id="3064" class="pw-post-body-paragraph ly lz it ma b mb mu ju md me mv jx mg mh mw mj mk ml mx mn mo mp my mr ms mt im bi translated">在生产就绪的实现中，我们可能希望这样做—结果可能会改善。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="pd pe l"/></div></figure><p id="9cc0" class="pw-post-body-paragraph ly lz it ma b mb mu ju md me mv jx mg mh mw mj mk ml mx mn mo mp my mr ms mt im bi translated">评估过程类似于培训过程:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="pd pe l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pg"><img src="../Images/9433018400f9158c72462add1f0e7c58.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*492JSY71CSoQN5au.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">预测的均值函数(红线)和相应的方差(红色区域=均值的2个标准差)</p></figure><p id="1a8f" class="pw-post-body-paragraph ly lz it ma b mb mu ju md me mv jx mg mh mw mj mk ml mx mn mo mp my mr ms mt im bi translated">均值和标准差的结果看起来都是合理的。我们可能会通过更多的微调来改善结果，以下是一些想法:</p><ul class=""><li id="526a" class="ng nh it ma b mb mu me mv mh ni ml nj mp nk mt ph nm nn no bi translated"><strong class="ma iu">使用更强大的优化程序</strong>如前所述，我们应该为优化器提供理想的梯度和hessian函数。像tensorflow和PyTorch这样的自动签名包可以自动完成这项工作。</li><li id="cbb0" class="ng nh it ma b mb np me nq mh nr ml ns mp nt mt ph nm nn no bi translated"><strong class="ma iu">微调模型超参数</strong>树的数量和它们各自的深度将是明显的起点。</li><li id="a265" class="ng nh it ma b mb np me nq mh nr ml ns mp nt mt ph nm nn no bi translated"><strong class="ma iu">使用更复杂的提升算法</strong>我们的提升算法非常简单。有许多变化和进步可以轻松超越这种实现。</li></ul><p id="016c" class="pw-post-body-paragraph ly lz it ma b mb mu ju md me mv jx mg mh mw mj mk ml mx mn mo mp my mr ms mt im bi translated">这些考虑可能有助于进一步改善我们的结果。</p></div><div class="ab cl kz la hx lb" role="separator"><span class="lc bw bk ld le lf"/><span class="lc bw bk ld le lf"/><span class="lc bw bk ld le"/></div><div class="im in io ip iq"><h1 id="9a0e" class="lg lh it bd li lj lk ll lm ln lo lp lq jz lr ka ls kc lt kd lu kf lv kg lw lx bi translated">结论——我们还能做什么？</h1><p id="8604" class="pw-post-body-paragraph ly lz it ma b mb mc ju md me mf jx mg mh mi mj mk ml mm mn mo mp mq mr ms mt im bi translated">正态分布可能是最明显的选择，但是还有许多更有趣的选择。</p><p id="caa0" class="pw-post-body-paragraph ly lz it ma b mb mu ju md me mv jx mg mh mw mj mk ml mx mn mo mp my mr ms mt im bi translated">例如，考虑高斯分布的<a class="ae ky" href="https://en.wikipedia.org/wiki/Skew_normal_distribution" rel="noopener ugc nofollow" target="_blank">偏斜版本。实际上，大多数数据远非围绕平均值对称。解释这种行为可能会相当有利。</a></p><p id="a9dc" class="pw-post-body-paragraph ly lz it ma b mb mu ju md me mv jx mg mh mw mj mk ml mx mn mo mp my mr ms mt im bi translated">然而，随着分布参数数量的增加，我们无疑会增加过度拟合的风险。我们的目标分布中的每个新参数都意味着另一个梯度推进模型。</p><p id="c651" class="pw-post-body-paragraph ly lz it ma b mb mu ju md me mv jx mg mh mw mj mk ml mx mn mo mp my mr ms mt im bi translated">为了减少这种风险，正规化是必要的。最简单的正则化形式可能是在我们的算法中减少树桩的数量(<em class="nc"> n_trees </em>)。在我们考虑更复杂的正则化方法之前，这已经足够了。</p><p id="3f03" class="pw-post-body-paragraph ly lz it ma b mb mu ju md me mv jx mg mh mw mj mk ml mx mn mo mp my mr ms mt im bi translated">总而言之，如果您需要评估目标变量中的噪声，带有概率输出的梯度提升会非常有帮助。我在这里写了另外两篇文章<a class="ae ky" href="https://numbersandcode.com/why-probability-and-uncertainty-should-be-an-integral-part-of-regression-models-part1" rel="noopener ugc nofollow" target="_blank">这里写了</a>和<a class="ae ky" href="https://numbersandcode.com/why-probability-and-uncertainty-should-be-an-integral-part-of-regression-models-ii" rel="noopener ugc nofollow" target="_blank">这里写了</a>为什么我认为这是一个好主意。</p><blockquote class="mz na nb"><p id="c3e5" class="ly lz nc ma b mb mu ju md me mv jx mg nd mw mj mk ne mx mn mo nf my mr ms mt im bi translated">“总而言之，如果您需要评估目标变量中的噪声，带有概率输出的梯度提升会非常有帮助。”</p></blockquote><p id="dfe3" class="pw-post-body-paragraph ly lz it ma b mb mu ju md me mv jx mg mh mw mj mk ml mx mn mo mp my mr ms mt im bi translated">如果你有任何问题或反馈，请在评论中告诉我。</p></div><div class="ab cl kz la hx lb" role="separator"><span class="lc bw bk ld le lf"/><span class="lc bw bk ld le lf"/><span class="lc bw bk ld le"/></div><div class="im in io ip iq"><p id="6ff9" class="pw-post-body-paragraph ly lz it ma b mb mu ju md me mv jx mg mh mw mj mk ml mx mn mo mp my mr ms mt im bi translated"><em class="nc">原载于2021年7月26日</em><a class="ae ky" href="https://sarem-seitz.com/blog/multi-output-gradient-boosting-for-probability-distributions/" rel="noopener ugc nofollow" target="_blank"><em class="nc">【https://sarem-seitz.com】</em></a><em class="nc">。</em></p></div></div>    
</body>
</html>