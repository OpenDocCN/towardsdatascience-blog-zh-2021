<html>
<head>
<title>Fine-Tuning the BART Large Model for Text Summarization</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">文本摘要BART大模型的优化</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/fine-tuning-the-bart-large-model-for-text-summarization-3c69e4c04582?source=collection_archive---------1-----------------------#2021-11-26">https://towardsdatascience.com/fine-tuning-the-bart-large-model-for-text-summarization-3c69e4c04582?source=collection_archive---------1-----------------------#2021-11-26</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="db1c" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">微调模型以总结世界新闻</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/500b281e12c0d80cb29330d8c1b38eb4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*-DVQ_qX1LX0wejUB"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">来源:<a class="ae ky" href="https://unsplash.com/photos/Mwuod2cm8g4" rel="noopener ugc nofollow" target="_blank">https://unsplash.com/photos/Mwuod2cm8g4</a></p></figure><p id="63e9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">据《大西洋月刊》T4报道，纽约时报每天发表150多篇文章，周日发表250多篇。《华尔街日报》每天刊登大约240篇报道。其他网站，如Buzzfeed，每月发布6000多篇报道。</p><p id="d430" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">随着信息量的增加，摘要成为机器学习/自然语言处理(NLP)中一项非常受欢迎的任务也就不足为奇了。</p><p id="b9c0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">总结有两种主要方法。第一种，抽取摘要，旨在识别最重要的句子，并使用那些精确的句子作为摘要。</p><p id="d5ae" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">更高级的方法是抽象概括。它包括以一种新的方式解释和总结信息。这是我们将在本文中使用的方法。</p></div><div class="ab cl lw lx hx ly" role="separator"><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb"/></div><div class="im in io ip iq"><h2 id="61df" class="md me it bd mf mg mh dn mi mj mk dp ml li mm mn mo lm mp mq mr lq ms mt mu mv bi translated">BART大型模型</h2><p id="e1ba" class="pw-post-body-paragraph kz la it lb b lc mw ju le lf mx jx lh li my lk ll lm mz lo lp lq na ls lt lu im bi translated">像任何NLP任务一样，存在可以用作起点的高级模型。这里的想法是使用预训练神经网络模型的所有权重，并将其用作初始点，以便加速训练并提高性能。</p><p id="975a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在本教程中，使用的模型被称为<code class="fe nb nc nd ne b">facebook/bart-large-cnn</code>，由脸书开发。它包含1024个隐藏层和406个参数，并使用CNN(一个新闻摘要数据集)进行了微调。</p><p id="89d2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如你所见，这个模型非常庞大，所以我建议你使用Google Colab来运行代码。它是100%免费的，并提供轻松访问GPU，这将加快训练速度。</p></div><div class="ab cl lw lx hx ly" role="separator"><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb"/></div><div class="im in io ip iq"><h1 id="6be6" class="nf me it bd mf ng nh ni mi nj nk nl ml jz nm ka mo kc nn kd mr kf no kg mu np bi translated">代码</h1><h2 id="a24d" class="md me it bd mf mg mh dn mi mj mk dp ml li mm mn mo lm mp mq mr lq ms mt mu mv bi translated">1.导入和准备数据</h2><p id="1aa7" class="pw-post-body-paragraph kz la it lb b lc mw ju le lf mx jx lh li my lk ll lm mz lo lp lq na ls lt lu im bi translated">让我们从导入数据(并稍微清理一下)以及微调模型所需的库开始。</p><p id="507b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我将使用可以在<a class="ae ky" href="https://www.kaggle.com/mrisdal/fake-news/data" rel="noopener ugc nofollow" target="_blank">这里</a>找到的数据。我将只使用数据集的一小部分来总结被归类为阴谋的新闻。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nq nr l"/></div></figure><p id="6e48" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这里没什么特别的。blurr库将huggingface transformer模型(就像我们使用的那个)与fast.ai集成在一起，fast . ai是一个旨在使深度学习比以往任何时候都更容易使用的库。</p><p id="aac2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">正如您在第22行看到的，我在本教程中只使用了数据的一个子集，主要是因为内存和时间的限制。这是数据的样子。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ns"><img src="../Images/2025d0b762ef5a1ab7a7ee0b30b06ee2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9noiEqjNiOakaw2fn42SZQ.png"/></div></div></figure><p id="a46b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这里，<code class="fe nb nc nd ne b">text </code>列将被用作我们想要总结的文本，而<code class="fe nb nc nd ne b">title</code>列将被用作我们想要获得的目标。</p><p id="3f82" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我这样做是因为我没有实际的总结，但如果你有，当然你应该把它作为目标。</p><h2 id="ed44" class="md me it bd mf mg mh dn mi mj mk dp ml li mm mn mo lm mp mq mr lq ms mt mu mv bi translated">2.导入模型</h2><p id="749f" class="pw-post-body-paragraph kz la it lb b lc mw ju le lf mx jx lh li my lk ll lm mz lo lp lq na ls lt lu im bi translated">在本节中，我们将导入预先训练的模型，并为训练准备数据。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nq nr l"/></div></figure><p id="ee14" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这里有点复杂，所以让我们一行一行地看一下每件事是怎么做的:</p><ul class=""><li id="f94d" class="nt nu it lb b lc ld lf lg li nv lm nw lq nx lu ny nz oa ob bi translated">第2–3行:这是我们导入预训练BART大型模型的地方，我们将对其进行微调。</li><li id="9521" class="nt nu it lb b lc oc lf od li oe lm of lq og lu ny nz oa ob bi translated">第7–15行:这是处理创建小批量输入和目标的地方。我们还指定了任务是什么，以及一组超参数(在text_gen_kwargs字典中)。</li><li id="5ac3" class="nt nu it lb b lc oc lf od li oe lm of lq og lu ny nz oa ob bi translated">第20行:这一行包含创建数据集所需的内容。我们在这里提供什么是文本，什么是目标摘要。</li><li id="cc63" class="nt nu it lb b lc oc lf od li oe lm of lq og lu ny nz oa ob bi translated">第21行:我们创建数据集并提供batch_size。我建议保持这个数字很小，否则你可能会遇到<code class="fe nb nc nd ne b">CUDA OUT OF MEMORY</code>错误。</li></ul><p id="68e0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">要了解更多关于不同功能和参数的信息，我邀请您查看文档<a class="ae ky" href="https://ohmeow.github.io/blurr/data-seq2seq-core" rel="noopener ugc nofollow" target="_blank">这里</a>和<a class="ae ky" href="https://docs.fast.ai/data.block.html#DataBlock.dataloaders" rel="noopener ugc nofollow" target="_blank">这里</a>。</p><h2 id="2ad9" class="md me it bd mf mg mh dn mi mj mk dp ml li mm mn mo lm mp mq mr lq ms mt mu mv bi translated">3.培养</h2><p id="12f6" class="pw-post-body-paragraph kz la it lb b lc mw ju le lf mx jx lh li my lk ll lm mz lo lp lq na ls lt lu im bi translated">我们现在可以开始实际训练了。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nq nr l"/></div></figure><ul class=""><li id="04dc" class="nt nu it lb b lc ld lf lg li nv lm nw lq nx lu ny nz oa ob bi translated">第2–9行:创建一个字典，其中包含一系列性能指标，这些指标将用于在训练期间评估模型。</li><li id="5ea9" class="nt nu it lb b lc oc lf od li oe lm of lq og lu ny nz oa ob bi translated">第17–19行:学习函数是我们指定数据、模型和损失函数的地方，这些将用于训练。</li><li id="edd3" class="nt nu it lb b lc oc lf od li oe lm of lq og lu ny nz oa ob bi translated">第26行:使用1cycle策略来拟合一个模型，这是在这篇<a class="ae ky" href="https://arxiv.org/abs/1708.07120" rel="noopener ugc nofollow" target="_blank">论文</a>中介绍的，旨在使用较大的学习率非常快速地训练神经网络。我们在这一行中指定了历元数(3)和学习率。</li></ul><h2 id="7b51" class="md me it bd mf mg mh dn mi mj mk dp ml li mm mn mo lm mp mq mr lq ms mt mu mv bi translated">4.生成预测</h2><p id="82e2" class="pw-post-body-paragraph kz la it lb b lc mw ju le lf mx jx lh li my lk ll lm mz lo lp lq na ls lt lu im bi translated">现在让我们看看如何给定一个文本生成摘要。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nq nr l"/></div></figure><p id="3804" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我只返回一个概要，但是你可以返回多个概要，并从给定的选项中选择最佳的概要。</p><p id="c9cf" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">作为一个例子，这里有一个文本和返回的摘要。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oh"><img src="../Images/8e5c30081b6c3632ae09d71ef6b03b40.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5zm4ByBsbakunSBNogY4yQ.png"/></div></div></figure><p id="cc5d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这就对了。考虑到我所做的训练是如此之少，仅仅使用了一个非常小的数据集，这是非常令人印象深刻的。当你想对一个给定的主题保持更新，而不需要阅读关于它的所有内容时，你肯定可以看到摘要的用途。</p></div><div class="ab cl lw lx hx ly" role="separator"><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb"/></div><div class="im in io ip iq"><p id="b79b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在，您已经知道如何微调文本摘要的BART large。非常感谢你的阅读，我希望我能有所帮助！</p><p id="26de" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">代码可以在<a class="ae ky" href="https://github.com/francoisstamant/Fine-tuning-for-text-summarization" rel="noopener ugc nofollow" target="_blank">这里</a>找到。</p><p id="da87" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">成为会员:<a class="ae ky" href="https://francoisstamant.medium.com/membership" rel="noopener">https://francoisstamant.medium.com/membership</a></p></div></div>    
</body>
</html>