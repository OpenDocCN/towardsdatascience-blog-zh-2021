<html>
<head>
<title>Mastering Model Explainability in Python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">掌握Python中的模型可解释性</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/mastering-model-explainability-in-python-4e578206e000?source=collection_archive---------28-----------------------#2021-08-09">https://towardsdatascience.com/mastering-model-explainability-in-python-4e578206e000?source=collection_archive---------28-----------------------#2021-08-09</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="706c" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">解释模型特征重要性</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/b2c8f749479fe858554337f1b7ad0cec.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VLu3Qluzhvt0x-EmIteZAQ.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">照片由<a class="ae ky" href="https://www.pexels.com/@goumbik" rel="noopener ugc nofollow" target="_blank">卢卡斯</a>在<a class="ae ky" href="https://www.pexels.com/photo/person-holding-blue-and-clear-ballpoint-pen-590022/" rel="noopener ugc nofollow" target="_blank">像素</a>上拍摄</p></figure><p id="df8a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对于数据科学家来说，解释机器学习模型的一个关键部分是理解哪些因素会影响预测。为了在决策过程中有效地使用机器学习，公司需要知道哪些因素是最重要的。例如，如果一家公司想预测客户流失的可能性，他们可能也想知道究竟是什么驱使客户离开公司。在本例中，该模型可能表明，购买很少打折产品的客户更有可能停止购买。有了这些知识，公司可以在未来做出更明智的定价决策。在高层次上，这些见解可以帮助公司留住客户更长时间，并保持利润。</p><p id="aa9c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">幸运的是，Python提供了许多包，可以帮助解释机器学习模型中使用的功能。部分相关图是可视化特征和模型预测之间关系的一种有用方法。我们可以将这些图解释为作为输入特征函数的平均模型预测。随机森林，也是一种机器学习算法，使用户能够获得量化各种特征在确定模型预测中的重要性的分数。因为模型可解释性以一种直接的方式内置于Python包中，所以许多公司广泛使用随机森林。对于像深度神经网络这样的更多黑盒模型，像局部可解释模型不可知解释(LIME)和沙普利加法解释(SHAP)这样的方法是有用的。这些方法通常用于预测难以解释的机器学习模型。</p><p id="885b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">当数据科学家对这些技术有了很好的理解，他们就可以从不同的角度处理模型可解释性的问题。部分相关性图是一种很好的方法，可以很容易地将特征/预测关系可视化。随机森林有助于根据不同要素在决定结果中的重要程度对其进行排序。莱姆和SHAP确定复杂模型中的特征重要性，其中直接解释模型预测是不可行的，例如具有数百或数千个特征的深度学习模型，这些特征与输出具有复杂的非线性关系。了解这些方法中的每一种都可以帮助数据科学家接近机器学习模型变量的模型可解释性，无论它们是简单还是复杂。</p><p id="bd5c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们将讨论如何应用这些方法并解释分类模型的预测。具体来说，我们将考虑逻辑回归模型、随机森林模型以及深度神经网络的模型可解释性任务。我们将使用虚构的电信客户流失数据，此处<a class="ae ky" href="https://www.kaggle.com/blastchar/telco-customer-churn" rel="noopener ugc nofollow" target="_blank">提供了这些数据</a>。</p><h2 id="69e7" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated"><strong class="ak">将电信数据读入熊猫数据帧</strong></h2><p id="9b5a" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">首先，让我们将电信客户流失数据读入Pandas数据框架。首先，让我们导入熊猫库:</p><pre class="kj kk kl km gt mt mu mv mw aw mx bi"><span id="af1d" class="lv lw it mu b gy my mz l na nb">import pandas as pd</span></pre><p id="3dd6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们使用Pandas read_csv()方法将数据读入数据帧:</p><pre class="kj kk kl km gt mt mu mv mw aw mx bi"><span id="5aaf" class="lv lw it mu b gy my mz l na nb">df = pd.read_csv(“telco_churn.csv”)</span></pre><p id="9e0c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们显示前五行数据:</p><pre class="kj kk kl km gt mt mu mv mw aw mx bi"><span id="1e54" class="lv lw it mu b gy my mz l na nb">print(df.head())</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nc"><img src="../Images/95f3d35e3e28a42d6fe2da11fad1ea2b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*oAg2K6jp4aESHHmK"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><h2 id="e2dd" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated"><strong class="ak">建模数据准备</strong></h2><p id="5ac8" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">我们将构建的每个模型都将性别、任期、月费、无纸账单、合同、支付方式、合作伙伴、家属和设备保护作为输入。我们的预测目标将是客户流失。首先，我们需要通过将分类输入转换成机器可读的分数来为训练做准备。</p><p id="5438" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们看看将性别转换成分类代码的例子。首先，我们使用Pandas astype方法创建一个名为gender_cat的新列，其类别类型为:</p><pre class="kj kk kl km gt mt mu mv mw aw mx bi"><span id="f6cf" class="lv lw it mu b gy my mz l na nb">df[‘gender_cat’] = df[‘gender’].astype(‘category’)</span></pre><p id="309d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">接下来，我们使用Pandas cat.codes属性提取分类代码:</p><pre class="kj kk kl km gt mt mu mv mw aw mx bi"><span id="efed" class="lv lw it mu b gy my mz l na nb">df[‘gender_cat’] = df[‘gender_cat’].cat.codes</span></pre><p id="e0b0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然后，我们对剩余的分类特征重复这一过程:</p><pre class="kj kk kl km gt mt mu mv mw aw mx bi"><span id="b631" class="lv lw it mu b gy my mz l na nb">df[‘PaperlessBilling_cat’] = df[‘PaperlessBilling’].astype(‘category’)</span><span id="9836" class="lv lw it mu b gy nd mz l na nb">df[‘PaperlessBilling_cat’] = df[‘PaperlessBilling_cat’].cat.codes</span><span id="fa91" class="lv lw it mu b gy nd mz l na nb">df[‘Contract_cat’] = df[‘Contract’].astype(‘category’)</span><span id="1481" class="lv lw it mu b gy nd mz l na nb">df[‘Contract_cat’] = df[‘Contract_cat’].cat.codes</span><span id="f8b1" class="lv lw it mu b gy nd mz l na nb">df[‘PaymentMethod_cat’] = df[‘PaymentMethod’].astype(‘category’)</span><span id="bfbd" class="lv lw it mu b gy nd mz l na nb">df[‘PaymentMethod_cat’] = df[‘PaymentMethod_cat’].cat.codes</span><span id="764d" class="lv lw it mu b gy nd mz l na nb">df[‘Partner_cat’] = df[‘Partner’].astype(‘category’)</span><span id="2718" class="lv lw it mu b gy nd mz l na nb">df[‘Partner_cat’] = df[‘Partner_cat’].cat.codes</span><span id="b52d" class="lv lw it mu b gy nd mz l na nb">df[‘Dependents_cat’] = df[‘Dependents’].astype(‘category’)</span><span id="ae1c" class="lv lw it mu b gy nd mz l na nb">df[‘Dependents_cat’] = df[‘Dependents_cat’].cat.codes</span><span id="dd40" class="lv lw it mu b gy nd mz l na nb">df[‘DeviceProtection_cat’] = df[‘DeviceProtection’].astype(‘category’)</span><span id="7eed" class="lv lw it mu b gy nd mz l na nb">df[‘DeviceProtection_cat’] = df[‘DeviceProtection_cat’].cat.codes</span></pre><p id="7ce2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们还创建一个新列，将churn列中的Yes/No值映射到二进制整数(零和一)。在“流失分数”列中，当流失为“是”时，流失标签为1，当流失为“否”时，流失标签为零:</p><pre class="kj kk kl km gt mt mu mv mw aw mx bi"><span id="d6c6" class="lv lw it mu b gy my mz l na nb">import numpy as np</span><span id="687a" class="lv lw it mu b gy nd mz l na nb">df[‘churn_score’] = np.where(df[‘churn’]==’Yes’, 1, 0)</span></pre><p id="b2fc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">接下来，让我们将输入存储在名为X的变量中，将输出存储在名为y的变量中:</p><pre class="kj kk kl km gt mt mu mv mw aw mx bi"><span id="f241" class="lv lw it mu b gy my mz l na nb">X = df[[ ‘tenure’, ‘MonthlyCharges’, ‘gender_cat’, ‘PaperlessBilling_cat’,</span><span id="86c9" class="lv lw it mu b gy nd mz l na nb">‘Contract_cat’,’PaymentMethod_cat’, ‘Partner_cat’, ‘Dependents_cat’, ‘DeviceProtection_cat’ ]]</span><span id="7125" class="lv lw it mu b gy nd mz l na nb">y = df[‘churn_score’]</span></pre><p id="026b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">接下来，让我们使用scikit-learn中model_selection模块中的train _ test _ spliit方法来拆分用于训练和测试的数据:</p><pre class="kj kk kl km gt mt mu mv mw aw mx bi"><span id="94ad" class="lv lw it mu b gy my mz l na nb">from sklearn.model_selection import train_test_split</span><span id="d9ac" class="lv lw it mu b gy nd mz l na nb">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)</span></pre><p id="9a60" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">接下来，让我们从scikit-learn导入LogisticRegression模型，并使该模型适合我们的训练数据:</p><pre class="kj kk kl km gt mt mu mv mw aw mx bi"><span id="eee4" class="lv lw it mu b gy my mz l na nb">lr_model = LogisticRegression()</span><span id="7a23" class="lv lw it mu b gy nd mz l na nb">lr_model.fit(X_train, y_train)</span></pre><p id="0df6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们进行预测:</p><pre class="kj kk kl km gt mt mu mv mw aw mx bi"><span id="b9cc" class="lv lw it mu b gy my mz l na nb">y_pred = lr_model.predict(X_test)</span></pre><p id="acd3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为了了解我们的模型的表现，我们将生成一个混淆矩阵:</p><pre class="kj kk kl km gt mt mu mv mw aw mx bi"><span id="7ea4" class="lv lw it mu b gy my mz l na nb">from sklearn.metrics import confusion_matrix</span><span id="a9ec" class="lv lw it mu b gy nd mz l na nb">conmat = confusion_matrix(y_test, y_pred)</span><span id="8764" class="lv lw it mu b gy nd mz l na nb">val = np.mat(conmat)</span><span id="347b" class="lv lw it mu b gy nd mz l na nb">classnames = list(set(y_train))</span><span id="4a45" class="lv lw it mu b gy nd mz l na nb">df_cm = pd.DataFrame(</span><span id="a8ea" class="lv lw it mu b gy nd mz l na nb">val, index=classnames, columns=classnames,</span><span id="2af2" class="lv lw it mu b gy nd mz l na nb">)</span><span id="e5d3" class="lv lw it mu b gy nd mz l na nb">df_cm = df_cm.astype(‘float’) / df_cm.sum(axis=1)[:, np.newaxis]</span><span id="7736" class="lv lw it mu b gy nd mz l na nb">import matplotlib.pyplot as plt</span><span id="7653" class="lv lw it mu b gy nd mz l na nb">import seaborn as sns</span><span id="cc7a" class="lv lw it mu b gy nd mz l na nb">plt.figure()</span><span id="3d2c" class="lv lw it mu b gy nd mz l na nb">heatmap = sns.heatmap(df_cm, annot=True, cmap=”Blues”)</span><span id="b02b" class="lv lw it mu b gy nd mz l na nb">heatmap.yaxis.set_ticklabels(heatmap.yaxis.get_ticklabels(), rotation=0, ha=’right’)</span><span id="01fe" class="lv lw it mu b gy nd mz l na nb">heatmap.xaxis.set_ticklabels(heatmap.xaxis.get_ticklabels(), rotation=45, ha=’right’)</span><span id="4b93" class="lv lw it mu b gy nd mz l na nb">plt.ylabel(‘True label’)</span><span id="69ed" class="lv lw it mu b gy nd mz l na nb">plt.xlabel(‘Predicted label’)</span><span id="5c45" class="lv lw it mu b gy nd mz l na nb">plt.title(‘Churn Logistic Regression Model Results’)</span><span id="9e5d" class="lv lw it mu b gy nd mz l na nb">plt.show()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ne"><img src="../Images/ef755eff08dc8e4b0a07b4ad08b2a975.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*pPfPR1JvEw1dtFkp"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><p id="635a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们可以看到，逻辑回归模型在预测将与公司合作的客户方面做得非常好，找到了90%的真正负面因素。它在预测将要离开的顾客方面也做得相当不错，发现了52%的真实信息。</p><h2 id="0627" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated"><strong class="ak">逻辑回归模型的部分相关图</strong></h2><p id="872b" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">现在，让我们用部分相关图来解释这个模型。</p><pre class="kj kk kl km gt mt mu mv mw aw mx bi"><span id="c792" class="lv lw it mu b gy my mz l na nb">from sklearn.inspection import plot_partial_dependence</span><span id="0cfb" class="lv lw it mu b gy nd mz l na nb">features = [0, 1, (1, 0)]</span><span id="ae99" class="lv lw it mu b gy nd mz l na nb">plot_partial_dependence(lr_model, X_train, features, target=1)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nf"><img src="../Images/7362ca817e80dc7f3c6b32c2751fab54.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*nzgou-d6cRwDBWOa"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><p id="0aeb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们发现，随着任期的延长，客户离开的可能性会降低。这种模式是有道理的，因为拥有更长任期的客户可能不太可能离开。</p><p id="3995" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">此外，客户离开的概率随着月费的增加而增加，这也是直观的。我们还可以看到租用期与月费的密度图。浅绿色/黄色表示密度较高。由此我们可以看出，大量每月充电器费用较高的客户也是相对较新的客户。在这个例子中，该公司可以利用这种洞察力来锁定每月交易和折扣费用较高的新客户，以努力留住他们。</p><h2 id="2ad7" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated"><strong class="ak">随机森林特征重要性</strong></h2><p id="ec42" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">接下来，我们将构建一个随机森林模型，并显示其特征重要性图。让我们从Scikit-learn中的ensemble模块导入随机森林包，根据我们的训练数据构建我们的模型，并根据对测试集做出的预测生成混淆矩阵:</p><pre class="kj kk kl km gt mt mu mv mw aw mx bi"><span id="5a99" class="lv lw it mu b gy my mz l na nb">conmat = confusion_matrix(y_test, y_pred_rf)</span><span id="f1a1" class="lv lw it mu b gy nd mz l na nb">val = np.mat(conmat)</span><span id="4f8c" class="lv lw it mu b gy nd mz l na nb">classnames = list(set(y_train))</span><span id="7ff9" class="lv lw it mu b gy nd mz l na nb">df_cm_rf = pd.DataFrame(</span><span id="da9a" class="lv lw it mu b gy nd mz l na nb">val, index=classnames, columns=classnames,</span><span id="ede4" class="lv lw it mu b gy nd mz l na nb">)</span><span id="4c95" class="lv lw it mu b gy nd mz l na nb">df_cm_rf = df_cm_rf.astype(‘float’) / df_cm.sum(axis=1)[:, np.newaxis]</span><span id="5155" class="lv lw it mu b gy nd mz l na nb">import matplotlib.pyplot as plt</span><span id="fd61" class="lv lw it mu b gy nd mz l na nb">import seaborn as sns</span><span id="5d52" class="lv lw it mu b gy nd mz l na nb">plt.figure()</span><span id="0e78" class="lv lw it mu b gy nd mz l na nb">heatmap = sns.heatmap(df_cm, annot=True, cmap=”Blues”)</span><span id="27dc" class="lv lw it mu b gy nd mz l na nb">heatmap.yaxis.set_ticklabels(heatmap.yaxis.get_ticklabels(), rotation=0, ha=’right’)</span><span id="b654" class="lv lw it mu b gy nd mz l na nb">heatmap.xaxis.set_ticklabels(heatmap.xaxis.get_ticklabels(), rotation=45, ha=’right’)</span><span id="81dd" class="lv lw it mu b gy nd mz l na nb">plt.ylabel(‘True label’)</span><span id="dbe9" class="lv lw it mu b gy nd mz l na nb">plt.xlabel(‘Predicted label’)</span><span id="0f16" class="lv lw it mu b gy nd mz l na nb">plt.title(‘Churn Random Forest Model Results’)</span><span id="15c4" class="lv lw it mu b gy nd mz l na nb">plt.show()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ng"><img src="../Images/0f5807f1a28e8bb6790f6389f29a7fd2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*8UryOtftSFI0tDLR"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><p id="5084" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然后，我们可以显示一个带有特性重要性值的条形图:</p><pre class="kj kk kl km gt mt mu mv mw aw mx bi"><span id="73f1" class="lv lw it mu b gy my mz l na nb">features = [‘tenure’, ‘MonthlyCharges’, ‘gender_cat’, ‘PaperlessBilling_cat’,</span><span id="3a94" class="lv lw it mu b gy nd mz l na nb">‘Contract_cat’,’PaymentMethod_cat’, ‘Partner_cat’, ‘Dependents_cat’, ‘DeviceProtection_cat’ ]</span><span id="94c9" class="lv lw it mu b gy nd mz l na nb">print(rf_model.feature_importances_)</span><span id="2896" class="lv lw it mu b gy nd mz l na nb">feature_df = pd.DataFrame({‘Importance’:rf_model.feature_importances_, ‘Features’: features })</span><span id="fa75" class="lv lw it mu b gy nd mz l na nb">sns.set()</span><span id="ed3d" class="lv lw it mu b gy nd mz l na nb">plt.bar(feature_df[‘Features’], feature_df[‘Importance’])</span><span id="cb40" class="lv lw it mu b gy nd mz l na nb">plt.xticks(rotation=90)</span><span id="5914" class="lv lw it mu b gy nd mz l na nb">plt.title(‘Random Forest Model Feature Importance’)</span><span id="8833" class="lv lw it mu b gy nd mz l na nb">plt.show()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nh"><img src="../Images/17cc1a56a1a34ad299170a987bde877b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1334/0*VKieIV1kvP05GD6T"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><p id="a2d8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这里我们看到，促使客户离开的最重要因素是任期、月费和合同类型。结合部分相关图使用随机森林的特征重要性是一种强大的技术。你不仅知道哪些因素是最重要的，还知道这些因素与结果的关系。我们以终身职位为例。从随机森林特征的重要性我们看到，保有权是最重要的特征。从部分相关图中，我们看到任期和客户离开的概率之间存在负线性关系。这意味着客户在公司呆的时间越长，他们离开的可能性就越小。</p><h2 id="8d90" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated"><strong class="ak">莱姆和SHAP为神经网络模型</strong></h2><p id="f143" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">当处理像深度神经网络这样更复杂的黑盒模型时，我们需要转向模型可解释性的替代方法。这是因为，与逻辑回归模型中可用的系数或随机森林等基于树的模型的内置特征重要性不同，神经网络等复杂模型不提供对特征重要性的任何直接解释。莱姆和SHAP是解释复杂模型最常用的方法。</p><p id="ffc0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们建立一个人工神经网络分类模型。从模型构建开始，让我们从Keras导入顺序和密集方法:</p><pre class="kj kk kl km gt mt mu mv mw aw mx bi"><span id="b7a5" class="lv lw it mu b gy my mz l na nb">from tensorflow.keras.models import Sequential</span><span id="ac08" class="lv lw it mu b gy nd mz l na nb">from tensorflow.keras.layers import Dense</span></pre><p id="20ca" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">接下来，让我们初始化顺序方法:</p><pre class="kj kk kl km gt mt mu mv mw aw mx bi"><span id="9156" class="lv lw it mu b gy my mz l na nb">model = Sequential()</span></pre><p id="08df" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们向模型对象中添加两个包含八个节点的层。我们需要使用输入要素的数量来指定输入形状。在第二层中，我们指定了一个激活函数，它代表了神经元放电的过程。这里，我们使用整流线性单元(ReLu)激活函数:</p><pre class="kj kk kl km gt mt mu mv mw aw mx bi"><span id="732e" class="lv lw it mu b gy my mz l na nb">model.add(Dense(8, input_shape = (len(features),)))</span><span id="efea" class="lv lw it mu b gy nd mz l na nb">model.add(Dense(8, activation=’relu’))</span></pre><p id="6b33" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然后，我们添加带有一个节点的输出层，并编译我们的模型:</p><pre class="kj kk kl km gt mt mu mv mw aw mx bi"><span id="bac0" class="lv lw it mu b gy my mz l na nb">model.add(Dense(1, activation=’sigmoid’))</span><span id="bb34" class="lv lw it mu b gy nd mz l na nb">model.compile(optimizer=’adam’, loss=’binary_crossentropy’, metrics=[‘accuracy’])</span></pre><p id="68b7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">一旦我们的模型被编译，我们就使我们的模型适合我们的训练数据:</p><pre class="kj kk kl km gt mt mu mv mw aw mx bi"><span id="9a39" class="lv lw it mu b gy my mz l na nb">model.fit(X_train, y_train, epochs = 1)</span></pre><p id="e3c3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然后，我们可以根据测试数据进行预测:</p><pre class="kj kk kl km gt mt mu mv mw aw mx bi"><span id="c37f" class="lv lw it mu b gy my mz l na nb">y_pred = [round(float(x)) for x in model.predict(X_test)]</span></pre><p id="78be" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">并生成混淆矩阵:</p><pre class="kj kk kl km gt mt mu mv mw aw mx bi"><span id="aa86" class="lv lw it mu b gy my mz l na nb">conmat = confusion_matrix(y_test, y_pred_nn)</span><span id="7240" class="lv lw it mu b gy nd mz l na nb">val = np.mat(conmat)</span><span id="c314" class="lv lw it mu b gy nd mz l na nb">classnames = list(set(y_train))</span><span id="1d9d" class="lv lw it mu b gy nd mz l na nb">df_cm_nn = pd.DataFrame(</span><span id="2d5c" class="lv lw it mu b gy nd mz l na nb">val, index=classnames, columns=classnames,</span><span id="8ce4" class="lv lw it mu b gy nd mz l na nb">)</span><span id="2424" class="lv lw it mu b gy nd mz l na nb">df_cm_nn = df_cm_nn.astype(‘float’) / df_cm_nn.sum(axis=1)[:, np.newaxis]</span><span id="8cc8" class="lv lw it mu b gy nd mz l na nb">plt.figure()</span><span id="27c7" class="lv lw it mu b gy nd mz l na nb">heatmap = sns.heatmap(df_cm_nn, annot=True, cmap=”Blues”)</span><span id="62b3" class="lv lw it mu b gy nd mz l na nb">heatmap.yaxis.set_ticklabels(heatmap.yaxis.get_ticklabels(), rotation=0, ha=’right’)</span><span id="4cb5" class="lv lw it mu b gy nd mz l na nb">heatmap.xaxis.set_ticklabels(heatmap.xaxis.get_ticklabels(), rotation=45, ha=’right’)</span><span id="1801" class="lv lw it mu b gy nd mz l na nb">plt.ylabel(‘True label’)</span><span id="3d80" class="lv lw it mu b gy nd mz l na nb">plt.xlabel(‘Predicted label’)</span><span id="9c3b" class="lv lw it mu b gy nd mz l na nb">plt.title(‘Churn Neural Network Model Results’)</span><span id="b615" class="lv lw it mu b gy nd mz l na nb">plt.show()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ni"><img src="../Images/58b83c521d7300f7195d3fa117d85ff1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*e2Mbzye_pp-Yj2V9"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><p id="5bfd" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在，让我们用SHAP来解释我们的神经网络模型:</p><pre class="kj kk kl km gt mt mu mv mw aw mx bi"><span id="cd84" class="lv lw it mu b gy my mz l na nb">import shap</span><span id="669c" class="lv lw it mu b gy nd mz l na nb">f = lambda x: model.predict(x)</span><span id="ba1a" class="lv lw it mu b gy nd mz l na nb">med = X_train.median().values.reshape((1,X_train.shape[1]))</span><span id="4f9d" class="lv lw it mu b gy nd mz l na nb">explainer = shap.Explainer(f, med)</span><span id="5b04" class="lv lw it mu b gy nd mz l na nb">shap_values = explainer(X_test.iloc[0:1000,:])</span><span id="6d92" class="lv lw it mu b gy nd mz l na nb">shap.plots.beeswarm(shap_values)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nj"><img src="../Images/0f7eab43511329e4dd3b25652fd2e586.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*IK-NdX_9UjSW6UZ-"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><p id="98cd" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">正如我们从随机森林模型使用权中看到的，月度费用和合同是解释结果的三个主要特征。</p><p id="f7de" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">LIME是对复杂模型的特征重要性进行可视化的另一个选项。LIME的计算速度通常比SHAP快，因此如果需要快速生成结果，LIME是更好的选择。在实践中，SHAP将比LIME更准确地解释特征，因为它在数学上更严格。由于这个原因，SHAP计算量更大，如果你有足够的时间和计算资源，它是一个不错的选择。</p><p id="75f0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们用石灰来解释我们的神经网络预测:</p><pre class="kj kk kl km gt mt mu mv mw aw mx bi"><span id="04ec" class="lv lw it mu b gy my mz l na nb">import lime</span><span id="a22a" class="lv lw it mu b gy nd mz l na nb">from lime import lime_tabular</span><span id="bb31" class="lv lw it mu b gy nd mz l na nb">explainer = lime_tabular.LimeTabularExplainer(training_data=np.array(X_train),feature_names=X_train.columns,class_names=[‘Yes’, ‘No’],</span><span id="9d26" class="lv lw it mu b gy nd mz l na nb">mode=’classification’)</span><span id="8f63" class="lv lw it mu b gy nd mz l na nb">exp = explainer.explain_instance(data_row=X_test.iloc[1], predict_fn=model.predict, labels=(0,))</span><span id="0c35" class="lv lw it mu b gy nd mz l na nb">exp.show_in_notebook(show_table=True)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nk"><img src="../Images/b66b76c05523688fb2c53a9b4aa486c4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*P-h_6ZXWYOo2ddpA"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><p id="1548" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">正如我们所料，我们发现月费和租期的影响最大。这篇文章的代码可以在GitHub 上找到。</p><h2 id="37ba" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated"><strong class="ak">结论</strong></h2><p id="ac05" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">根据所解决的问题，这些方法中的一种或多种可能是解释模型预测的好选择。如果您要处理相对较少的输入要素和较小的数据集，使用逻辑回归和部分相关图就足够了。如果您正在处理中等数量的输入要素和中等大小的数据集，随机森林是一个很好的选择，因为它很可能优于逻辑回归和神经网络。如果您正在处理具有数百万行和数千个输入要素的数千兆字节的数据，神经网络将是更好的选择。从那里，根据可用的计算资源，你可以与莱姆或SHAP合作来解释神经网络预测。如果时间有限，石灰是更好的选择，虽然不太准确。如果你有足够的时间和资源，SHAP是更好的选择。</p><p id="deaa" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">虽然我们看了一个简单的例子，用一个相对小而干净的数据集来留住客户，但是有多种类型的数据可以在很大程度上影响哪种方法是合适的。例如，对于一个小问题，如给定一小组产品特征作为输入，预测产品的成功，逻辑回归和部分依赖图就足够了。当处理更标准的行业问题时，如客户保留或甚至预测信用违约，特征的数量通常是适中的(少于数百个),数据的大小也是适中的，因此基于树的模型如随机森林及其特征重要性更合适。此外，医疗保健中的许多问题，如使用EHR数据预测医院再入院，涉及数百个(有时数千个)输入特征的训练模型。在这种情况下，用莱姆或SHAP解释的神经网络更合适。对于各行各业的数据科学家来说，知道何时使用给定数据类型的特定模型和可解释方法是一项无价的技能。</p><p id="e335" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果你有兴趣学习python编程的基础知识、Pandas的数据操作以及python中的机器学习，请查看<a class="ae ky" href="https://www.amazon.com/dp/B08N38XW2Q/ref=sr_1_1?dchild=1&amp;keywords=sadrach+python&amp;qid=1604966500&amp;s=books&amp;sr=1-1" rel="noopener ugc nofollow" target="_blank"><em class="nl">Python for Data Science and Machine Learning:Python编程、Pandas和sci kit-初学者学习教程</em> </a> <em class="nl">。我希望你觉得这篇文章有用/有趣。</em></p><p id="1bf8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu"> <em class="nl">本帖原载于</em> </strong> <a class="ae ky" href="https://builtin.com/machine-learning" rel="noopener ugc nofollow" target="_blank"> <strong class="lb iu"> <em class="nl">内置博客</em> </strong> </a> <strong class="lb iu"> <em class="nl">。原片可以在这里找到</em></strong><a class="ae ky" href="https://builtin.com/machine-learning/model-explainability-python" rel="noopener ugc nofollow" target="_blank"><strong class="lb iu"><em class="nl"/></strong></a><strong class="lb iu"><em class="nl">。</em>T29】</strong></p></div></div>    
</body>
</html>