<html>
<head>
<title>Japanese multiclass text classification with 97% accuracy using BERT</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用BERT实现97%准确率的日语多类文本分类</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/japanese-multiclass-text-classification-with-97-accuracy-using-bert-11b1fdc7c27e?source=collection_archive---------16-----------------------#2021-09-20">https://towardsdatascience.com/japanese-multiclass-text-classification-with-97-accuracy-using-bert-11b1fdc7c27e?source=collection_archive---------16-----------------------#2021-09-20</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="cc6f" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">将NHK(日本广播公司)节目分为多种类型</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/35c6f374b301a4f7722237380565827b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WUrYX57p0Q_cR4DFWHByuA.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">康纳·乐迪在<a class="ae kv" href="https://unsplash.com/" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="b946" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">您是否有一些日文文本，如客户反馈、用户评论或邮件内容，并希望从手头的数据中提取见解？下面介绍的模型在处理日语文本时，在多类分类方面表现出色。该模型很好地理解了日语，并且可以潜在地用于许多应用领域中的许多目的，例如流失率预测、消费者细分、消费者情绪分析。</p><blockquote class="ls lt lu"><p id="ef4b" class="kw kx lv ky b kz la jr lb lc ld ju le lw lg lh li lx lk ll lm ly lo lp lq lr ij bi translated">P.S:这个模型只需更改2行就可以轻松适应其他语言。</p></blockquote></div><div class="ab cl lz ma hu mb" role="separator"><span class="mc bw bk md me mf"/><span class="mc bw bk md me mf"/><span class="mc bw bk md me"/></div><div class="ij ik il im in"><p id="a8fb" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir"> <em class="lv">数据</em> </strong></p><p id="5be7" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们将使用NHK(日本广播公司)的节目信息作为我们的数据源。通过<a class="ae kv" href="https://api-portal.nhk.or.jp/" rel="noopener ugc nofollow" target="_blank"> NHK的节目安排API </a>，可以获得全日本电视、电台、网络电台未来7天安排的所有节目的【标题、字幕、内容、流派】。</p><p id="8118" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir"> <em class="lv">问题陈述</em> </strong></p><p id="34b3" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">利用节目的标题、副标题和内容，我们将尝试预测其类型。一个节目可以有多种类型，如下所示:</p><p id="4f14" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi">Title: あさイチ「体験者に聞く 水害から家族・暮らしを守るには？」<br/>Genre: 1) 情報/ワイドショー, 2) ドキュメンタリー/教養</p><p id="da81" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi">In the above case, we will be assuming this show’s genre is 情報/ワイドショー (Information/ Wide show)</p><blockquote class="ls lt lu"><p id="e77e" class="kw kx lv ky b kz la jr lb lc ld ju le lw lg lh li lx lk ll lm ly lo lp lq lr ij bi translated">该模型的一个改进将是可以预测每个节目的多个类型，这将使该问题成为多类多目标问题。</p></blockquote><p id="4853" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir"> <em class="lv">【探索性数据分析(EDA)】</em></strong></p><p id="6f84" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我收集了2021年8月30日至2021年9月24日之间播出(或将播出)的10，321个独特节目的信息。以下是10，321个节目中所有13种类型(标签)的分布情况:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mg"><img src="../Images/62266665c1f5c9e39345f7a73b26d179.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CxfPHcigGgObQ2_vIsBzwQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="0306" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">正如您所看到的，数据是高度不平衡的，因此我们将在将数据分为训练和测试时对其进行“分层”(即:保持测试数据中的标签分布与整个数据集相同)，并使用加权F1分数作为适合不平衡数据集的准确性度量。</p><p id="bb81" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir"> <em class="lv">数据处理</em> </strong></p><p id="ea51" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们将数据分为训练(80%)和测试(20%)数据集。我们将在训练数据集上训练模型，并在10个时期内跟踪测试数据集的准确性(时期:模型遍历整个训练数据集的1个步骤)。产生最高精度的时期将被用作最终模型，并且来自该模型的结果将被认为是模型精度。</p><p id="2146" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir"> <em class="lv">型号</em> </strong></p><p id="2eef" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们模型的输入将是每个节目的“标题”、“副标题”和“内容”的连接。有些节目没有上述所有信息，但只要至少有一个字段(标题总是可用)就没问题</p><p id="49fe" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">模型的输出将是13个可用类型的概率分布，我们将把具有最高概率的类型作为模型输出，并将其与真实值进行比较。</p><p id="123e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir"> <em class="lv">伯特</em>T3】</strong></p><p id="d279" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">为了训练这个模型，我们将使用BERT的一个预训练模型，并针对我们的问题对它进行微调。查看<a class="ae kv" rel="noopener" target="_blank" href="/bert-explained-state-of-the-art-language-model-for-nlp-f8b21a9b6270">这篇文章</a>，看看伯特是如何工作的。我们将使用🤗拥抱人脸库，它提供了一个获取预先训练的BERT模型的接口。我们将要获取的预训练模型是<a class="ae kv" href="https://huggingface.co/cl-tohoku/bert-base-japanese-v2" rel="noopener ugc nofollow" target="_blank"><strong class="ky ir"><em class="lv">Bert-base-Japanese-v2</em></strong></a><strong class="ky ir"><em class="lv"/></strong>，它是由东北大学的研究人员使用维基百科中的3000万个日语句子训练的。对于微调，我们将使用<a class="ae kv" href="https://huggingface.co/transformers/model_doc/bert.html#bertforsequenceclassification" rel="noopener ugc nofollow" target="_blank"><strong class="ky ir"><em class="lv">BertForSequenceClassification</em></strong></a>模型，因为这本质上是一个序列分类问题。</p></div><div class="ab cl lz ma hu mb" role="separator"><span class="mc bw bk md me mf"/><span class="mc bw bk md me mf"/><span class="mc bw bk md me"/></div><div class="ij ik il im in"><p id="d092" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir"> <em class="lv">结果</em> </strong></p><p id="5128" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们在Google Colab的GPU运行时环境中训练该模型，因为BERT是一个繁重的模型，它在CPU上花费的时间明显更长。训练数据以32的批量提供给模型，以加速学习并防止RAM溢出。模型经过10个时期的训练:以下是这10个时期的损失和准确性指标:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mh"><img src="../Images/de013eaeedfe44970f5fa2a6a62749e8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Td7gj-TbGLlK3un4j8Zt6w.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="420b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">10个历元似乎足够了，因为训练损失和测试精度在该点之后持平。让我们看看我们是如何预测每个标签的:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mi"><img src="../Images/6a71e1c5f36b802d5b7ab61d83cc8480.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*J8n9ie83-2lgHHZKsQSsTQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="d572" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">最后，让我们检查混淆矩阵:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mj"><img src="../Images/49b3ed5e5cb8556b3cf7272775487c58.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2i2xgYim8JtgFun0id1HWg.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="4ae7" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">看起来不错吧？</p><h1 id="b893" class="mk ml iq bd mm mn mo mp mq mr ms mt mu jw mv jx mw jz mx ka my kc mz kd na nb bi translated">结论</h1><p id="2d74" class="pw-post-body-paragraph kw kx iq ky b kz nc jr lb lc nd ju le lf ne lh li lj nf ll lm ln ng lp lq lr ij bi translated">在本文中，我们使用了一个BERT预训练模型，并对其进行了微调，用于多类文本分类，以97%的准确率将日本电视和广播节目分类为多个流派。</p><p id="535c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我观察到的一个趋势是，随着训练数据变大，准确性也在提高。2周的数据产生94%的准确性，而1个月的数据能够产生97%的准确性。我预计，随着更多数据的收集，该模型可以达到近乎完美的预测精度。该模型的下一步将是预测每个节目的多个流派，这将使该问题成为多类别多标签问题。</p><p id="ca4d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">如果你在danyelkoca@gmail.com有任何问题/反馈，请告诉我。您可以在下面找到源代码和数据集:</p><p id="8c7e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">数据集:<a class="ae kv" href="https://github.com/danyelkoca/NHK/blob/main/data.csv" rel="noopener ugc nofollow" target="_blank">https://github.com/danyelkoca/NHK/blob/main/data.csv</a></p><p id="4972" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">代码:<a class="ae kv" href="https://colab.research.google.com/drive/12ezR2Q4MZHE9m_Ppv_RfuqnKt-G9yX3f?usp=sharing" rel="noopener ugc nofollow" target="_blank">https://colab . research . Google . com/drive/12 ezr 2 q 4 mzhe 9m _ Ppv _ RfuqnKt-g 9 yx3f？usp =共享</a></p><p id="a3e3" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">本文日文版:【https://qiita.com/dannyk/items/bee0249af1f77bc416d8 T2】</p><p id="8163" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">黑客快乐！</p></div></div>    
</body>
</html>