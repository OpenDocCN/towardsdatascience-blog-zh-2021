<html>
<head>
<title>5 Common Misconceptions about Linear Regressions</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">关于线性回归的5个常见误解</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/5-common-misconceptions-about-linear-regressions-7fe5b64ecedf?source=collection_archive---------13-----------------------#2021-05-29">https://towardsdatascience.com/5-common-misconceptions-about-linear-regressions-7fe5b64ecedf?source=collection_archive---------13-----------------------#2021-05-29</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="e03c" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">Python中的示例</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/27fee0fa13b06e42f8f483eafa734325.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2C7B4Fu8LsellsCEBYKyfQ.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">艾米丽·莫特在<a class="ae ky" href="https://unsplash.com/s/photos/question?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="c586" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">只要问题允许，我非常支持使用简单的模型。这一切都是有趣的，是与花哨的神经网络的游戏，但是如果古老的<a class="ae ky" href="https://en.wikipedia.org/wiki/Linear_regression" rel="noopener ugc nofollow" target="_blank">线性回归</a>能够完成工作，为什么不使用它呢？</p><p id="c363" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然而，线性回归是一个看似简单的概念，它有两个问题:</p><ul class=""><li id="e2dd" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated">这个理论是<strong class="lb iu">经常低估</strong>——你认为你知道事情是如何运作的，因为你已经使用它们几百次了；</li><li id="904d" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">模型很容易上手，所以题目素材很多，每个人都有自己的看法，虚假信息传播超级<strong class="lb iu">。</strong></li></ul><p id="7a3c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在这篇文章中，我将谈论5个我“最喜欢的”关于线性回归的误解。希望你能找到以前没有考虑过的东西。</p><p id="a435" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">下面是Python中的一些例子，如果你想了解代码，这些是你需要的唯一的库:</p><pre class="kj kk kl km gt mj mk ml mm aw mn bi"><span id="b97d" class="mo mp it mk b gy mq mr l ms mt"><strong class="mk iu">import</strong> numpy <strong class="mk iu">as</strong> np<br/><strong class="mk iu">from</strong> sklearn.linear_model <strong class="mk iu">import</strong> LinearRegression<br/><strong class="mk iu">from</strong> sklearn.metrics <strong class="mk iu">import</strong> r2_score<br/><strong class="mk iu">from</strong> scipy <strong class="mk iu">import</strong> stats</span></pre><h1 id="6c0b" class="mu mp it bd mv mw mx my mz na nb nc nd jz ne ka nf kc ng kd nh kf ni kg nj nk bi translated">误解1:线性回归=普通最小二乘法</h1><p id="4f21" class="pw-post-body-paragraph kz la it lb b lc nl ju le lf nm jx lh li nn lk ll lm no lo lp lq np ls lt lu im bi translated">我们的第一个误解是关于术语:人们倾向于将线性回归和OLS方法作为同义词。线性回归就是问题本身，当我们假设一个因变量和一个或多个自变量之间存在线性关系，而OLS就是众多可用估计方法中的一种。估计参数的其他方法有<a class="ae ky" href="https://en.wikipedia.org/wiki/Least_absolute_deviations" rel="noopener ugc nofollow" target="_blank">最小绝对偏差</a>、<a class="ae ky" href="https://en.wikipedia.org/wiki/Lasso_(statistics)" rel="noopener ugc nofollow" target="_blank">套索</a>和<a class="ae ky" href="https://en.wikipedia.org/wiki/Ridge_regression" rel="noopener ugc nofollow" target="_blank">岭回归</a>。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/dd091930ca2c234d504ab22229c8b3f6.png" data-original-src="https://miro.medium.com/v2/resize:fit:968/format:webp/1*PuWehXBxsEpTCIAETm3drA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">OLS vs LAD估计</p></figure><p id="4d20" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">请看左边的图表。两者之间的线性关系以红色显示。蓝色和绿色分别是OLS和拉德估计的关系，你不能肯定地宣布他们中的任何一个更好。(Lasso和ridge将给出与单因素回归中的OLS相同的结果。)</p><p id="ac4e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我不得不承认，草率地使用这些术语是我对自己的愧疚:我最近发表了一篇关于套索和岭回归的文章，我一直在区分它们和“线性回归”，而事实上它们都是线性回归。(在我的辩护中，sklearn称相应的模型为<a class="ae ky" href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html" rel="noopener ugc nofollow" target="_blank">线性回归</a>、<a class="ae ky" href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html" rel="noopener ugc nofollow" target="_blank">套索</a>和<a class="ae ky" href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html" rel="noopener ugc nofollow" target="_blank">山脊</a>。)</p><p id="2dc4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">顺便说一下，在这篇文章的其余部分，我们将重点关注OLS方法，尤其是它的假设。</p><blockquote class="nr ns nt"><p id="6c29" class="kz la nu lb b lc ld ju le lf lg jx lh nv lj lk ll nw ln lo lp nx lr ls lt lu im bi translated"><strong class="lb iu">总结</strong>:线性回归可以用多种方法和假设来求解，其中之一就是普通最小二乘法。它们不是同义词。</p></blockquote><h1 id="f737" class="mu mp it bd mv mw mx my mz na nb nc nd jz ne ka nf kc ng kd nh kf ni kg nj nk bi translated">误解2:多重共线性是世界末日</h1><p id="2bfd" class="pw-post-body-paragraph kz la it lb b lc nl ju le lf nm jx lh li nn lk ll lm no lo lp lq np ls lt lu im bi translated"><a class="ae ky" href="https://en.wikipedia.org/wiki/Multicollinearity" rel="noopener ugc nofollow" target="_blank">多重共线性</a>指线性回归中独立变量之间的高度相关性。它有时被认为是一个严重的问题，是一个必须解决的问题，这样模型才能发挥作用。但是，多重共线性只会导致单个系数估计值的可靠性和可解释性降低，模型的预测力不会受到影响。</p><p id="35d0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">缺乏多重共线性不是OLS的假设。</strong>事实上，它没有对自变量之间的关系做任何假设，除了一点:变量必须是独立的。换句话说，它们之间不可能存在完美的多重共线性。</p><p id="d532" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们来看两个例子，首先高相关性会发生什么，然后我们会看到为什么变量之间不能有一个完美的线性关系。</p><h2 id="9f46" class="mo mp it bd mv ny nz dn mz oa ob dp nd li oc od nf lm oe of nh lq og oh nj oi bi translated">示例1:高相关性</h2><p id="c9d7" class="pw-post-body-paragraph kz la it lb b lc nl ju le lf nm jx lh li nn lk ll lm no lo lp lq np ls lt lu im bi translated">我们需要生成两个相关的独立变量。我不想使用正态变量，借用了<a class="ae ky" href="https://twiecki.io/blog/2018/05/03/copulas/" rel="noopener ugc nofollow" target="_blank">的代码，从两个相关的均匀分布中生成样本。我们需要一个相对较高的相关度来进行演示，让我们将它设置为0.9。</a></p><pre class="kj kk kl km gt mj mk ml mm aw mn bi"><span id="831f" class="mo mp it mk b gy mq mr l ms mt">number_of_obs = 1000<br/>np.random.seed(my_seed)</span><span id="4c81" class="mo mp it mk b gy oj mr l ms mt">mvnorm = stats.multivariate_normal(mean=[0, 0], cov=[[1., 0.9], <br/>                                                     [0.9, 1.]])<br/>X_norm = mvnorm.rvs(number_of_obs)<br/>X = stats.norm().cdf(X_norm)<br/>X1 = X[:,0]<br/>X2 = X[:,1]</span></pre><p id="6fe7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们可以检查这些是否确实相关:</p><pre class="kj kk kl km gt mj mk ml mm aw mn bi"><span id="2cb2" class="mo mp it mk b gy mq mr l ms mt">np.corrcoef(X1,X2)</span></pre><p id="a0bc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">将返回:</p><pre class="kj kk kl km gt mj mk ml mm aw mn bi"><span id="39a6" class="mo mp it mk b gy mq mr l ms mt">array([[1.        , 0.89713787],<br/>       [0.89713787, 1.        ]])</span></pre><p id="dde8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这一点得到了证实。</p><p id="bbb7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">接下来，我们需要通过添加模拟残差来生成因变量，并构造一些系数:</p><pre class="kj kk kl km gt mj mk ml mm aw mn bi"><span id="1539" class="mo mp it mk b gy mq mr l ms mt">residuals = stats.norm(loc = 0, scale = 1).rvs(number_of_obs)<br/>y = 30 * X1 - 10 * X2 + residuals</span></pre><p id="61ac" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">接下来，让我们看看sklearn的线性回归模型如何处理高相关性:</p><pre class="kj kk kl km gt mj mk ml mm aw mn bi"><span id="0f20" class="mo mp it mk b gy mq mr l ms mt">lr = LinearRegression()<br/>lr.fit(X,y)<br/>lr.coef_</span></pre><p id="58bf" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">将返回:</p><pre class="kj kk kl km gt mj mk ml mm aw mn bi"><span id="49c1" class="mo mp it mk b gy mq mr l ms mt">array([ 30.36959656, -10.55232467])</span></pre><p id="bcc1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">估计的系数非常接近实际系数。如果重新运行残差和y代，会看到结果会在<code class="fe ok ol om mk b">[30, -10]</code>附近震荡。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi on"><img src="../Images/c3daf3c774c359f54c7f4d23e10b07e2.png" data-original-src="https://miro.medium.com/v2/resize:fit:986/format:webp/1*XqKANO5TU1c_vm8tSN3COw.png"/></div></figure><p id="9566" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我觉得一部分困惑来自于<code class="fe ok ol om mk b">X2</code>的系数为负的部分。现在，我们知道那是正确的，因为那就是我们最初如何产生<code class="fe ok ol om mk b">y</code>的！</p><p id="c4f5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">但是，如果只看数据本身，可能会产生误导。在左边的图表中，你可以观察到<code class="fe ok ol om mk b">X2</code>和<code class="fe ok ol om mk b">y</code>之间非常明显的正相关关系。但那种效果只是通过<code class="fe ok ol om mk b">X1</code>，与<code class="fe ok ol om mk b">X2</code>高度相关。如果<code class="fe ok ol om mk b">X1</code>也在图片中，那么<code class="fe ok ol om mk b">X2</code>本身对<code class="fe ok ol om mk b">y</code>有负面影响，我们再一次在生成数据时看到了这一点。</p><p id="87ad" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在继续之前，让我们考虑一个现实生活中的例子。假设X1是房产的总面积，X2是卫生间的数量，y是价格。在这样的例子中，您通常会得到浴室的负系数，然后您通常会听到有人说<em class="nu">“难怪，总面积和卧室之间存在多重共线性，它必须从模型中清除，我们必须忽略浴室变量”</em>。</p><p id="b223" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">但实际上，这个结果难道没有某种意义吗？我们必须记住，系数影响是<em class="nu">其他条件不变的</em>影响，所以其他一切都假设保持稳定。如果我们假设房子的总面积不变，并不断增加越来越多的浴室，这将很快成为一个非常怪异的房子，成本会更低，对不对？因此，如果整体尺寸也存在于模型中，浴室系数为负可能并不是世界上最愚蠢的事情。</p><h2 id="46f7" class="mo mp it bd mv ny nz dn mz oa ob dp nd li oc od nf lm oe of nh lq og oh nj oi bi translated">示例2:完美多重共线性</h2><p id="fe83" class="pw-post-body-paragraph kz la it lb b lc nl ju le lf nm jx lh li nn lk ll lm no lo lp lq np ls lt lu im bi translated">剩下的第二个问题是:如果变量之间确实存在完美的多重共线性，会发生什么？</p><p id="c55d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们用两个完全相关的变量生成一个简单的例子:</p><pre class="kj kk kl km gt mj mk ml mm aw mn bi"><span id="b058" class="mo mp it mk b gy mq mr l ms mt">obs_no = 100<br/>np.random.seed(my_seed)<br/>X1 = np.random.uniform(0,100,obs_no)<br/>X2 = 2 * X1<br/>residuals = np.random.normal(0,1,obs_no)<br/>y =  X1 + X2 + residuals</span></pre><p id="a97e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">很容易看出这里的问题是什么:有无限可能的解决方案。我们将<code class="fe ok ol om mk b">X1</code>和<code class="fe ok ol om mk b">X2</code>的系数分别定义为1和1，但是例如，3和0也可以。OLS实际上有一个封闭的解决方案，这涉及到一个矩阵求逆，如果矩阵的列秩不是满的，就不能这样做。</p><p id="57ba" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">继续上面的例子，这就是在numpy中从头开始求解OLS的方法:</p><pre class="kj kk kl km gt mj mk ml mm aw mn bi"><span id="b6a5" class="mo mp it mk b gy mq mr l ms mt">intercept = np.ones(obs_no)<br/>X = np.vstack((X1, X2, intercept)).T<br/>Xt = X.T<br/>XtX = Xt.dot(X)<br/>XtX_inv = np.linalg.inv(XtX)<br/>Xty = Xt.dot(y)<br/>coeff = XtX_inv.dot(Xty)<br/>y_pred = X.dot(coeff)</span></pre><p id="2f07" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这在常规情况下是可行的，请注意，您必须手动添加截距，并且需要相应地解释系数。然而，现在，你会得到一个错误:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oo"><img src="../Images/c5590bb97dfe871edb64dec739ab1666.png" data-original-src="https://miro.medium.com/v2/resize:fit:516/format:webp/1*EZEuCEMwwPAk1Zq269rXSw.png"/></div></figure><p id="c1b0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">所以再一次，你不能有完美多重共线性的原因是你不能对自变量的矩阵求逆。</p><h2 id="9a6c" class="mo mp it bd mv ny nz dn mz oa ob dp nd li oc od nf lm oe of nh lq og oh nj oi bi translated">Sklearn与完美多重共线性</h2><p id="fc17" class="pw-post-body-paragraph kz la it lb b lc nl ju le lf nm jx lh li nn lk ll lm no lo lp lq np ls lt lu im bi translated">如果你和我一样，你可能会想:好吧，你不应该这样做，但是如果我们把这样的数据放入sklearn会发生什么？</p><p id="f1d9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">继续上面的例子，没有numpy部分:</p><pre class="kj kk kl km gt mj mk ml mm aw mn bi"><span id="14e8" class="mo mp it mk b gy mq mr l ms mt">X = np.vstack((X1, X2)).T<br/>lr = LinearRegression()<br/>lr.fit(X, y)<br/>lr.coef_</span></pre><p id="8287" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">将返回:</p><pre class="kj kk kl km gt mj mk ml mm aw mn bi"><span id="3069" class="mo mp it mk b gy mq mr l ms mt">array([0.60023761, 1.20047523])</span></pre><p id="9d85" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">看起来是随机的。但这实际上是可能的解决方案之一:</p><pre class="kj kk kl km gt mj mk ml mm aw mn bi"><span id="7e35" class="mo mp it mk b gy mq mr l ms mt">lr.coef_[1] * 2 + lr.coef_[0]</span></pre><p id="e581" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">将返回3。当您重新运行<code class="fe ok ol om mk b">lr.fit()</code>行时，您将得到不同的系数，但是它们的加权和将接近于3。</p><p id="5837" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">那么是什么原因呢？我以为矩阵是不能求逆的！</p><p id="6050" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">事实证明，矩阵求逆是一项非常昂贵的操作，sklearn中的线性回归模型使用了一种近似估计(梯度下降)。这种方法足以找到正确的解决方案。(还是不应该做，结果真的很不稳定很诡异，再加上你应该注意到你的变量之间有这样的关系。)</p><blockquote class="nr ns nt"><p id="6743" class="kz la nu lb b lc ld ju le lf lg jx lh nv lj lk ll nw ln lo lp nx lr ls lt lu im bi translated"><strong class="lb iu">总结</strong> : OLS不假设自变量之间低相关，但它们不可能有完美的多重共线性。</p></blockquote><h1 id="80a3" class="mu mp it bd mv mw mx my mz na nb nc nd jz ne ka nf kc ng kd nh kf ni kg nj nk bi translated">误解3:你的变量必须正态分布</h1><p id="95a2" class="pw-post-body-paragraph kz la it lb b lc nl ju le lf nm jx lh li nn lk ll lm no lo lp lq np ls lt lu im bi translated">我认为这里混乱的来源是你可以阅读像<em class="nu">“y被假定为正常”</em>这样的东西，在某种程度上，这是真的，但很容易对术语感到草率。</p><p id="2dc5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">简单的回答是，我们假设唯一正常的是残差，X和y的分布可以是任意的。然而，y取的单个值也被假定为正态分布的随机变量。</p><p id="fe00" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们考虑一个例子。我们想用房子的大小来解释它的价格。这恰好是“真正的”模型，所以其他任何东西对价格都没有影响。如果我们随后看到两栋同样大小的不同房子，模型会预测出同样的价格。但是，它们的观察价格可能不同。再说一次，这不是因为我们在这个简单的例子中遗漏了任何其他因素，我们确实知道“真实”的模型。价格不同的原因是残差，它被添加到线性回归的另一个因子中。因此，具有一定大小的房子的价格也将是一个正态变量，期望值将来自大小和可能的截距，方差将由残差决定。这并不意味着如果我们把观察到的价格放在一起，它们应该是正常的。</p><p id="c771" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">另一个混淆的来源可能是看到像取对数正态、平方根等东西。任何变量。这与正态假设无关，它只是一种使自变量和因变量之间的关系“更加线性”的方法。这是一个尝试和观察什么有效的问题。</p><blockquote class="nr ns nt"><p id="798c" class="kz la nu lb b lc ld ju le lf lg jx lh nv lj lk ll nw ln lo lp nx lr ls lt lu im bi translated"><strong class="lb iu">总结</strong>:残差假设为正态，因变量或自变量不为正态。</p></blockquote><p id="d59d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu"> <em class="nu">旁注</em> </strong> <em class="nu">:其实连残差正态性都不是OLS方法的固定假设，详情查看其</em> <a class="ae ky" href="https://en.wikipedia.org/wiki/Ordinary_least_squares#Assuming_normality" rel="noopener ugc nofollow" target="_blank"> <em class="nu">维基百科页面</em> </a> <em class="nu">。如果残差是正态的，OLS是最大似然估计量，但是技术上你可以不用这个假设做OLS。唯一发生的事情是，估计量会得到不同的统计性质。我认为很容易把线性回归想成一套记住的4-5条规则，但它比那要复杂得多。</em></p><h1 id="51be" class="mu mp it bd mv mw mx my mz na nb nc nd jz ne ka nf kc ng kd nh kf ni kg nj nk bi translated">误解4:你必须标准化</h1><p id="d0cd" class="pw-post-body-paragraph kz la it lb b lc nl ju le lf nm jx lh li nn lk ll lm no lo lp lq np ls lt lu im bi translated">你不必为了OLS估计而标准化/规范化你的变量。线性函数对模型的预测力没有影响。系数和截距会相应改变，但仅此而已。</p><p id="9f11" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果你的变量在很大程度上不同，你可能想要标准化，并且你想要容易地比较系数。</p><p id="e965" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然而，我发现有一点被忽略了，那就是<strong class="lb iu">对于任何正则化技术(套索、脊、弹性网)，你绝对必须标准化</strong> <strong class="lb iu">独立变量</strong>，否则正则化将毫无意义。因变量仍然不需要任何调整。</p><p id="4ae3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">应用标准化的一个快速方法是sklearn的<a class="ae ky" href="https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html" rel="noopener ugc nofollow" target="_blank">标准定标器</a>。</p><blockquote class="nr ns nt"><p id="8357" class="kz la nu lb b lc ld ju le lf lg jx lh nv lj lk ll nw ln lo lp nx lr ls lt lu im bi translated"><strong class="lb iu">总结</strong>:OLS方法没有必要标准化。然而，当使用任何正则化方法时，你必须标准化独立变量。</p></blockquote><h1 id="b73e" class="mu mp it bd mv mw mx my mz na nb nc nd jz ne ka nf kc ng kd nh kf ni kg nj nk bi translated">误解5:你的模型应该渴望达到R = 1</h1><p id="afb2" class="pw-post-body-paragraph kz la it lb b lc nl ju le lf nm jx lh li nn lk ll lm no lo lp lq np ls lt lu im bi translated">你可能有过这样的经历，当你满怀希望地开始一个线性回归问题时，却发现你的R离1非常远。但这真的是一个大问题吗？</p><p id="369e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">和大多数事情一样，答案是:视情况而定。有些人甚至说R高达0.9是非常可疑的，很可能意味着你使用了一个不应该使用的变量。要记住的主要一点是，R给你一个粗略的概念，告诉你你的预测有多精确，但是不告诉你你是否找到了正确的模型。所以，是的，低R可能意味着你应该小心处理模型的预测，但不一定意味着你必须寻找一个更好的模型。</p><p id="b1b8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们考虑一下这个值的实际变化。r定义为:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi op"><img src="../Images/0e7aca9eb44cce93f6fe19424933d88a.png" data-original-src="https://miro.medium.com/v2/resize:fit:280/format:webp/1*Ebe0iRvkGlHIPdMvmc369Q.png"/></div></figure><p id="c89c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">其中<em class="nu"> SSres </em>，残差平方和来自实际值和预测值:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oq"><img src="../Images/5c688132e6fa8ac6ed15a49a3f2c1f2b.png" data-original-src="https://miro.medium.com/v2/resize:fit:324/format:webp/1*9vvO_9QvAwOkrg7VZqDCcg.png"/></div></figure><p id="4360" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">和<em class="nu"> SStot </em>，总平方和来自实际值和实际值的平均值:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi or"><img src="../Images/1eb7feb839f93f285b2377b08f87979a.png" data-original-src="https://miro.medium.com/v2/resize:fit:328/format:webp/1*lJygr6SkUogK-CtCL25-_w.png"/></div></figure><p id="3b98" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">很容易看出，为了使R为1，你的<em class="nu"> SSres </em>必须为0，这意味着所有的残差必须为零。所以你必须找到一个完美的线性关系才能得到R = 1。不用说，这种情况几乎不会发生，线性回归也不应该这样。</p><p id="f3b0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们生成一个示例，以便能够更好地掌握分数的大小。为了简单起见，y等于1 * X加上一个剩余因子:</p><pre class="kj kk kl km gt mj mk ml mm aw mn bi"><span id="72bc" class="mo mp it mk b gy mq mr l ms mt">number_of_obs = 1000<br/>np.random.seed(my_seed)<br/>X = np.random.uniform(0, 100, number_of_obs)<br/>residuals = np.random.normal(0, 20, number_of_obs)<br/>y = X + residuals</span></pre><p id="9506" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们将跳过拟合模型的部分，假设我们找到了可能的最佳估计值，因此y被预测为x。让我们计算R:</p><pre class="kj kk kl km gt mj mk ml mm aw mn bi"><span id="940c" class="mo mp it mk b gy mq mr l ms mt">y_avg = np.mean(y)<br/>SStot = sum((y - y_avg)**2)<br/>SSres = sum((y - X)**2)<br/>print("SStot: %i" % SStot)<br/>print("SSres: %i" % SSres)<br/>print("R2: %.5f" % (1-SSres/SStot))</span></pre><p id="6298" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">结果是:</p><pre class="kj kk kl km gt mj mk ml mm aw mn bi"><span id="dce8" class="mo mp it mk b gy mq mr l ms mt">SStot: 1249503<br/>SSres: 407976<br/>R2: 0.67349</span></pre><p id="bc64" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">(我们可以用<code class="fe ok ol om mk b">r2_score(y, X)</code>复核公式，结果是一样的。)</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi os"><img src="../Images/d2ef29c654fc30246990c5adbe26e193.png" data-original-src="https://miro.medium.com/v2/resize:fit:996/format:webp/1*etCo8N0hiYzXhzSUonXfDg.png"/></div></figure><p id="ac67" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在左边的图表中，你可以看到0.67的R是什么样子的。X轴有预测的y值(或实际的X值，在我们的例子中它们是相同的)，y轴是实际的y值。</p><p id="4026" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们必须记住，这是我们能为这个例子建立的最好的模型！没有更好的线性回归了。</p><p id="0294" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这个例子也很方便地展示了不同参数对R:</p><ul class=""><li id="e4ab" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated">增加样本量(在我们的例子中是1000)将产生最小的影响，R收敛到一个稳定的值；</li><li id="0e5e" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">增加残差的方差(20)将通过增加<em class="nu"> SSres </em>来减少R；</li><li id="ee20" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">通过增加<em class="nu"> SStot </em>来增加均匀自变量的取值范围(100)将会增加R。</li></ul><p id="64ca" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">还有一点:OLS将最小化<em class="nu"> SSres </em>(即最小二乘法中的“平方”)，并且<em class="nu"> SStot </em>对于给定的一组观测值是恒定的。因此，OLS适合将最大化R的训练集。</p><blockquote class="nr ns nt"><p id="4eb2" class="kz la nu lb b lc ld ju le lf lg jx lh nv lj lk ll nw ln lo lp nx lr ls lt lu im bi translated"><strong class="lb iu">总结</strong> : R要小心处理。该指标可让您大致了解模型的预测力度，但不会告诉您离理想模型有多近。R = 1实际上不应该发生，高R应该引起怀疑。</p></blockquote><h1 id="bb8e" class="mu mp it bd mv mw mx my mz na nb nc nd jz ne ka nf kc ng kd nh kf ni kg nj nk bi translated">摘要</h1><p id="0773" class="pw-post-body-paragraph kz la it lb b lc nl ju le lf nm jx lh li nn lk ll lm no lo lp lq np ls lt lu im bi translated">现在你知道了，我看到的5个错误观念被抛向四周，我希望你能发现一些新的东西。我认为主要的收获是知道线性回归和普通最小二乘法比很多人想象的要复杂。</p><p id="6cd7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果你仍然认为你对所有事情都完全清楚的话，看看OLS的维基百科页面。</p><p id="15e7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">最后一件事:如果你认为我对自己的误解有任何误解，请告诉我。</p></div></div>    
</body>
</html>