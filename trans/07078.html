<html>
<head>
<title>A unified view of Graph Neural Networks</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">图形神经网络的统一视图</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/a-unified-view-of-graph-neural-networks-12b40e8fdac5?source=collection_archive---------14-----------------------#2021-06-27">https://towardsdatascience.com/a-unified-view-of-graph-neural-networks-12b40e8fdac5?source=collection_archive---------14-----------------------#2021-06-27</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="0d6f" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsdatascience.com/tagged/hands-on-tutorials" rel="noopener" target="_blank">实践教程</a></h2><div class=""/><div class=""><h2 id="83ad" class="pw-subtitle-paragraph jw iz iq bd b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn dk translated">图形注意、图形卷积、网络传播都是图形神经网络中消息传递的特例。</h2></div><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi ko"><img src="../Images/2117e0c4920ff98244b0407428e1f555.png" data-original-src="https://miro.medium.com/v2/resize:fit:1090/format:webp/1*wtkkUZ7qAPL8Sixcs8c6vA.png"/></div><p class="kw kx gj gh gi ky kz bd b be z dk translated">三种不同gnn的插图。图片来自[1]。</p></figure><p id="f992" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">消息传递网络(MPN)、图形注意力网络(GAT)、图形卷积网络(GCN)，甚至网络传播(NP)都是属于图形神经网络(GNN)范畴的密切相关的方法。这篇文章将提供这些方法的统一观点，主要来自于[1]中的第5.3章。</p><h1 id="8b7e" class="lw lx iq bd ly lz ma mb mc md me mf mg kf mh kg mi ki mj kj mk kl ml km mm mn bi translated">TL；速度三角形定位法(dead reckoning)</h1><ul class=""><li id="fe0a" class="mo mp iq lc b ld mq lg mr lj ms ln mt lr mu lv mv mw mx my bi translated">NP是GCN的特例，没有隐藏特征变换和非线性。</li><li id="ded5" class="mo mp iq lc b ld mz lg na lj nb ln nc lr nd lv mv mw mx my bi translated">GCN是GAT的一个特例，其“注意力”完全由图结构单独决定，没有节点特征。</li><li id="52e0" class="mo mp iq lc b ld mz lg na lj nb ln nc lr nd lv mv mw mx my bi translated">GAT是MPN的一个特例，具有作为“消息传递”规则的隐藏特征聚合。</li><li id="3e93" class="mo mp iq lc b ld mz lg na lj nb ln nc lr nd lv mv mw mx my bi translated">这些gnn之间的关系可以通过包含关系总结如下。</li></ul><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi ne"><img src="../Images/b8630218c87a524421c71809b7758d7a.png" data-original-src="https://miro.medium.com/v2/resize:fit:550/format:webp/1*2vVC43VJu0JSET0uiRRkJw.png"/></div></figure><h1 id="99f0" class="lw lx iq bd ly lz ma mb mc md me mf mg kf mh kg mi ki mj kj mk kl ml km mm mn bi translated">图形卷积-隐藏特征的局部聚合</h1><p id="7c34" class="pw-post-body-paragraph la lb iq lc b ld mq ka lf lg mr kd li lj nf ll lm ln ng lp lq lr nh lt lu lv ij bi translated">图形卷积[2]遵循层传播规则:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/84e27049b352be6fb4300420a18633b9.png" data-original-src="https://miro.medium.com/v2/resize:fit:642/format:webp/1*eHEftX-ysFQmrLho1lpD-w.png"/></div></figure><p id="e27b" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">就每一个节点而言，传播可以看作是邻居节点的变换(<em class="nj"> psi </em>)隐藏特征<em class="nj"> h </em>)的<strong class="lc ja">局部聚集</strong> ( <em class="nj">圈加</em>)，之后是某种变换(<em class="nj">φ</em>)<em class="nj">。</em>具体来说，在GCN [1]中，<em class="nj"> phi </em>是仿射变换(<em class="nj"> W </em>)后跟非线性(<em class="nj"> sigma </em>)。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/2ecf82aa069828b9181610a61ebb3b1c.png" data-original-src="https://miro.medium.com/v2/resize:fit:582/format:webp/1*PjMJ4YSBnedTJ-DigcBrzw.png"/></div></figure><p id="22a4" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">聚集量(<em class="nj"> c_{ij} </em>)完全由<strong class="lc ja">图结构</strong>决定，比如谱归一化邻接矩阵。</p><p id="69c8" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">注意:聚合(<em class="nj"> circle-plus </em>)可以是任何排列不变函数，如求和、元素乘积等。</p><p id="6fff" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">有趣的是，网络传播可以被视为没有任何特征转换的图形卷积的特例，正如我在上一篇<a class="ae nl" rel="noopener" target="_blank" href="/network-learning-from-network-propagation-to-graph-convolution-eb3c62d09de8">帖子</a>中更详细讨论的那样。简而言之，按照这里相同的符号，网络传播将遵守如下传播规则</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi nm"><img src="../Images/afbc9240dc568b6e6a01c1802e7039b6.png" data-original-src="https://miro.medium.com/v2/resize:fit:452/format:webp/1*EFYhekiSkwlyXoqTINtb-w.png"/></div></figure><h1 id="9bd6" class="lw lx iq bd ly lz ma mb mc md me mf mg kf mh kg mi ki mj kj mk kl ml km mm mn bi translated">图形注意-重新学习边权重</h1><p id="17d5" class="pw-post-body-paragraph la lb iq lc b ld mq ka lf lg mr kd li lj nf ll lm ln ng lp lq lr nh lt lu lv ij bi translated">类似于GCN，GAT [3]执行隐藏特征的局部平均。但是，它不是只使用图结构来指导传播，而是通过一个可学习的函数<em class="nj"> alpha </em>，根据隐藏的特征来学习<strong class="lc ja">重新加权传播权重</strong> ( <em class="nj"> c_{ij} </em>)。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/a00eea981f4c411170f5c851dfdca677.png" data-original-src="https://miro.medium.com/v2/resize:fit:746/format:webp/1*1k1fiALN0K6jfzaCBmGs-Q.png"/></div></figure><p id="51df" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">在[3]中，这个<strong class="lc ja">“自我注意机制”</strong> <em class="nj"> alpha </em>被计算为可学习向量<em class="nj"> a </em>和两个节点的仿射变换隐藏特征的串联之间的softmax归一化内积。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi no"><img src="../Images/ae334b21365c446b415dc81afee9c0de.png" data-original-src="https://miro.medium.com/v2/resize:fit:874/format:webp/1*1uPdeqzMXJ3B2rE2G-iJRA.png"/></div></figure><h1 id="dd5a" class="lw lx iq bd ly lz ma mb mc md me mf mg kf mh kg mi ki mj kj mk kl ml km mm mn bi translated">消息传递—更一般的聚合</h1><p id="9856" class="pw-post-body-paragraph la lb iq lc b ld mq ka lf lg mr kd li lj nf ll lm ln ng lp lq lr nh lt lu lv ij bi translated">消息传递网络[4]通过用两个节点的隐藏特征的某个<strong class="lc ja">任意函数</strong>代替隐藏特征聚集，进一步概括了对聚集的信息量进行控制的思想。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="nq nr di ns bf nt"><div class="gh gi np"><img src="../Images/52d2df226e97d8c65d67ee09f81c39b3.png" data-original-src="https://miro.medium.com/v2/resize:fit:628/format:webp/1*Q1smK_Nfe-y1KYTiXNbBZQ.png"/></div></div></figure><p id="de29" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">将所有不同的GNN方法放在一起，我们可以看到它们都遵循相同的<strong class="lc ja">局部聚集</strong>，或者通过局部平均(NP)或者置换不变函数(<em class="nj">圆加</em>)。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/1fde4fca3ed4acc64800768c37f9af41.png" data-original-src="https://miro.medium.com/v2/resize:fit:912/format:webp/1*1nkWNfZ_n0_IVJWujZHd1w.png"/></div></figure><p id="7d20" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">最后，现在更清楚不同的gnn是如何相关的。</p><ul class=""><li id="5a09" class="mo mp iq lc b ld le lg lh lj nv ln nw lr nx lv mv mw mx my bi translated">GCN是通过将注意力函数<em class="nj">α</em>设置为谱归一化邻接矩阵来实现GAT的。</li><li id="90c4" class="mo mp iq lc b ld mz lg na lj nb ln nc lr nd lv mv mw mx my bi translated">GAT是MPN的一种实现，通过以自我关注为消息传递规则的隐藏特征聚合。</li></ul></div><div class="ab cl ny nz hu oa" role="separator"><span class="ob bw bk oc od oe"/><span class="ob bw bk oc od oe"/><span class="ob bw bk oc od"/></div><div class="ij ik il im in"><h1 id="9161" class="lw lx iq bd ly lz of mb mc md og mf mg kf oh kg mi ki oi kj mk kl oj km mm mn bi translated">参考</h1><p id="c184" class="pw-post-body-paragraph la lb iq lc b ld mq ka lf lg mr kd li lj nf ll lm ln ng lp lq lr nh lt lu lv ij bi translated">[1] M. M .布朗斯坦，j .布鲁纳，t .科恩，p .维利科维奇，<a class="ae nl" href="https://arxiv.org/abs/2104.13478" rel="noopener ugc nofollow" target="_blank">几何深度学习:网格、组、图、测地线和量规</a> (2021)</p><p id="7776" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">[2] T. N. Kipf，M. Welling，<a class="ae nl" href="https://arxiv.org/abs/1609.02907" rel="noopener ugc nofollow" target="_blank">利用图卷积网络的半监督分类</a> (2016)</p><p id="aa5c" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">[3] P .韦利奇科维奇，g .库库尔，a .卡萨诺瓦，a .罗梅罗，p .莉雅，y .本吉奥，<a class="ae nl" href="https://arxiv.org/abs/1710.10903" rel="noopener ugc nofollow" target="_blank">图形注意网络</a> (2018)</p><p id="f3f4" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">[4] J. Gilmer，S. S. Schoenholz，P. F. Filey，O. Vinayls，G. E. Dahl，量子化学的神经<a class="ae nl" href="https://arxiv.org/abs/1704.01212" rel="noopener ugc nofollow" target="_blank">信息传递</a> (2017)</p><p id="5a07" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">[5] <a class="ae nl" rel="noopener" target="_blank" href="/network-learning-from-network-propagation-to-graph-convolution-eb3c62d09de8">网络学习——从网络传播到图形卷积</a></p></div></div>    
</body>
</html>