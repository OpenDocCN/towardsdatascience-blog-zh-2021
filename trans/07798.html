<html>
<head>
<title>Decision Trees and Random Forests — Explained with Python Implementation.</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">决策树和随机森林——用Python实现来解释。</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/decision-trees-and-random-forests-explained-with-python-implementation-e5ede021a000?source=collection_archive---------8-----------------------#2021-07-17">https://towardsdatascience.com/decision-trees-and-random-forests-explained-with-python-implementation-e5ede021a000?source=collection_archive---------8-----------------------#2021-07-17</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="b435" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">在本文中，我将带您了解决策树和随机森林算法的基本工作原理。我还将借助一个例子展示它们是如何在Python中实现的。</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi kf"><img src="../Images/25b73d2be700c78ad601627801f3698a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*4oWAuJvt_EevZourC1im8w.jpeg"/></div><p class="kn ko gj gh gi kp kq bd b be z dk translated">照片鸣谢——菲利普·塞尔纳克在Unsplash上</p></figure><p id="e6ef" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">决策树是一种受监督的机器学习算法，它模仿人类的思维过程。它会做出预测，就像现实生活中人类大脑会做的那样。它可以被认为是一系列if-then-else语句，随着它的增长，它会在每个点上继续做出决策或预测。</p><p id="d489" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">决策树看起来像流程图或倒置的树。它从根长到叶，但方向颠倒。由于决策树的结构，我们可以很容易地解释决策树的决策/预测过程。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ln"><img src="../Images/b2c381c19859709539093acc7b3f05cc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1188/format:webp/1*vCOvJhXDBslmghC_vgRMIw.png"/></div><p class="kn ko gj gh gi kp kq bd b be z dk translated">典型的二叉分裂决策树</p></figure><p id="a173" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">起始节点被称为<strong class="kt ir">根节点。</strong>它通过基于某种标准做出决策而进一步分裂成所谓的<strong class="kt ir">内部节点。</strong>进一步分裂的内部节点称为父节点，由父节点形成的节点称为子节点。不再分裂的节点被称为<strong class="kt ir">叶节点</strong>。</p><p id="961a" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">根据目标变量的类型，决策树可以是分类树或回归树。在分类树的情况下，类别基于叶节点中的多数预测。在回归的情况下，最终预测值基于叶节点中的平均值。</p><p id="7965" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">根据是二分分裂还是多路分裂，有各种决策树算法。二进制拆分意味着父节点拆分成两个子节点，而在多路拆分中，父节点拆分成两个以上的子节点。</p><p id="fcf2" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated"><strong class="kt ir"> CART </strong>意为分类和回归树算法处理二叉分裂树，而<strong class="kt ir"> ID3 </strong>算法处理多路分裂树。我们将详细讨论CART算法。(<em class="lo">以下决策树是指CART算法树</em>)</p><p id="a719" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">决策树将数据分成不同的子集，然后根据选择的属性进行拆分。该属性是根据称为<strong class="kt ir">基尼指数的同质性标准选择的。</strong></p><p id="4298" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">基尼指数，基本上测量分裂发生后节点的纯度(<em class="lo">或杂质，我们可以说</em>)。也就是说，它是分割后子节点与父节点的纯度的度量。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi lp"><img src="../Images/74768dbb64e96d43eab91c426d8e23ac.png" data-original-src="https://miro.medium.com/v2/resize:fit:1064/format:webp/1*vEn0rakJ92Ppmhbd9d_KUQ.png"/></div></figure><p id="946c" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">树应该基于导致子节点最大同质性的属性进行分裂。基尼指数的公式如下:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi lq"><img src="../Images/d9b86aa29a9a28a51d9b4905b95f4aad.png" data-original-src="https://miro.medium.com/v2/resize:fit:1324/format:webp/1*Wnq44FUmqfzDqX6E6lPUMw.png"/></div></figure><p id="dc20" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">其中<strong class="kt ir"> p </strong> i是找到带有标签<strong class="kt ir"> i </strong>的点的概率，<strong class="kt ir"> n </strong>是目标变量的类数/结果值。</p><p id="d001" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">因此，该树在父节点计算基尼指数。然后为每个属性计算子节点的加权基尼指数。选择一个属性，使得我们在子节点中获得最大的同质性，并且执行分割。重复这个过程，直到所有属性都用尽。最后，我们得到了我们称之为完全成长的决策树。因此，导致初始分裂的属性可以被认为是预测的最重要的属性。</p><p id="ab7e" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">一个完全成长的决策树的问题是，它过拟合。这意味着它吸收了所有的训练数据，无法进行归纳，导致测试数据的性能很差。为了解决这个问题并从决策树中获得最大的效率，我们需要<strong class="kt ir">修剪</strong>完全成长的树或者<strong class="kt ir">截断</strong>它的成长。</p><p id="f259" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">修剪是一种技术，我们可以修剪完全生长的树的不太重要的分支，而截断是一种技术，我们控制树的生长，以避免过度拟合。</p><p id="7005" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">截断更常用，它是通过调整树的超参数来实现的，这将在示例中讨论。</p><p id="7d3d" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">总的来说，决策树是高效的算法，不需要或只需要最少的数据处理。它们可以有效地处理线性和非线性数据、分类或数字数据，并根据给定的属性集进行预测。最重要的是，它们很容易解释。</p><p id="b76d" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">决策树的一些缺点是，它们过拟合。数据的微小变化可能会改变整个树结构，这使得它们成为高方差算法。基尼系数的计算使它们变得复杂，并且消耗大量的时间和内存。此外，它们可以被称为贪婪算法，因为它们只考虑直接分割的影响，而不考虑进一步分割的影响。</p><h1 id="719e" class="lr ls iq bd lt lu lv lw lx ly lz ma mb jw mc jx md jz me ka mf kc mg kd mh mi bi translated">随机森林</h1><p id="ad7a" class="pw-post-body-paragraph kr ks iq kt b ku mj jr kw kx mk ju kz la ml lc ld le mm lg lh li mn lk ll lm ij bi translated">各种模型(线性回归、逻辑回归、决策树等)的组合。)集合在一起作为单个模型以实现最终结果，称为<strong class="kt ir">系综。</strong>因此，在系综中，不同的模型被视为一个整体，而不是分开的。</p><p id="a3ea" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">随机森林是用大量决策树构建的强大集成模型。它克服了单一决策树的缺点以及其他一些优点。但这并不意味着它总是比决策树好。在某些情况下，决策树可能比随机林表现得更好。</p><p id="4848" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">集合以这样一种方式起作用，即集合中的每个模型都有助于补偿每个其他模型的缺点。我们可以说，如果一个随机的森林由10棵决策树组成，每棵树可能在处理数据时表现不佳，但是强壮的树有助于填补虚弱的树的空白。这就是为什么集成是一个强大的机器学习模型。</p><p id="461a" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">随机森林中的单棵树必须满足两个标准:</p><ol class=""><li id="3705" class="mo mp iq kt b ku kv kx ky la mq le mr li ms lm mt mu mv mw bi translated"><strong class="kt ir">多样性</strong>:意味着集合中的每一个模型都应该独立运行，并且应该与其他模型互补。</li><li id="1ed9" class="mo mp iq kt b ku mx kx my la mz le na li nb lm mt mu mv mw bi translated"><strong class="kt ir">可接受性</strong>:这意味着集合中的每个模型都应该至少比随机猜测的模型表现得更好。</li></ol><p id="0b4f" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated"><strong class="kt ir"> Bagging </strong>是一种常用的集成技术，用于实现分集准则。装袋台代表<strong class="kt ir">B</strong>o带<strong class="kt ir"> Agg </strong> regation。<strong class="kt ir">自举</strong>是一种用于创建自举样本的方法。这些样本是通过对给定数据进行均匀采样并替换而创建的。引导样本包含整个数据集中大约30%–70%的数据。创建统一长度的自举样本，并将其作为输入提供给随机森林模型中的各个决策树。然后将所有单个模型的结果<strong class="kt ir">汇总</strong>以得出最终决策。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="nd ne di nf bf ng"><div class="gh gi nc"><img src="../Images/563f425a3ba94c0d1e334e690853248b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nwF3aAHaAQDiNMQMI_YhBg.png"/></div></div><p class="kn ko gj gh gi kp kq bd b be z dk translated">典型的随机森林分类器</p></figure><p id="a1de" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">此外，并不是所有的特征都用来训练每一棵树。在每棵树的每个节点上选择固定数量的随机特征集。这确保了不同树的功能之间没有相关性，并且它们独立地起作用。</p><p id="0a0e" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">在随机森林分类器中，由单个树预测的多数类被认为是最终预测，而在随机森林回归器中，所有单个预测值的平均值被认为是最终预测。</p><p id="dd3a" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">在随机森林模型中，通常不会将数据分为训练集和测试集。整个数据用于形成自举样本，从而总是留出一组数据点来形成各个树的测试或验证集。这些样品被称为<strong class="kt ir">非袋装(OOB) </strong>样品。因此，对于随机森林集合中的每棵树，存在一组在其训练数据中不存在的数据点，并被用作评估其性能的验证集。</p><p id="d13d" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated"><strong class="kt ir"> OOB误差</strong>计算为OOB样本中不正确预测数与OOB样本预测总数的比率。</p><p id="6aec" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">随机森林还通过消除不太重要的特征/属性，帮助确定给定数据中所有特征中的重要特征。这导致更好的预测结果。借助基尼系数计算，通过计算杂质的减少或纯度的增加来确定特征的重要性。</p><p id="43fb" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">随机森林相对于决策树的主要优势在于它们是稳定的，并且是低方差模型。它们还克服了决策树中存在的过拟合问题。由于它们使用引导数据和随机的功能集，因此确保了多样性和稳健的性能。它们不受维数灾难的影响，因为它们不会同时考虑单个树的所有特征。</p><p id="36c4" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">随机森林的主要缺点是缺乏可解释性。与决策树不同，我们无法追踪算法是如何工作的。另一个缺点是它们复杂且计算量大。</p><p id="bcab" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">现在让我们在电信客户流失数据集的帮助下，看看决策树和随机森林模型的python实现。</p><h2 id="1b64" class="nh ls iq bd lt ni nj dn lx nk nl dp mb la nm nn md le no np mf li nq nr mh ns bi translated">Python实现</h2><p id="8017" class="pw-post-body-paragraph kr ks iq kt b ku mj jr kw kx mk ju kz la ml lc ld le mm lg lh li mn lk ll lm ij bi translated">对于电信运营商来说，留住高利润的客户是头号业务目标。电信行业平均每年经历15–25%的流失率。我们将分析一家领先电信公司的客户数据，建立预测模型来识别高流失风险的客户，并识别流失的主要指标。</p><p id="552d" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">该数据集包含连续四个月(六月、七月、八月和九月)的客户级别信息。月份分别编码为6、7、8和9。我们需要使用前三个月的数据(特征)来预测最后一个月(即第九个月)的客户流失。</p><p id="97d8" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">分析流程:</p><p id="f270" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">1.导入所需的库<br/> 2。阅读并理解数据<br/> 3。数据清理和准备<br/> 4。筛选高价值客户<br/> 5。流失分析<br/> 6。探索性数据分析<br/> 7。模型建筑<br/> 8。模型构建—超参数调整<br/> 9。最佳模特</p><p id="b129" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">导入所需的库。当需要的时候，我们会从不同的库加载更多的包。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/48bf07c6908fc3157155743b29ebd552.png" data-original-src="https://miro.medium.com/v2/resize:fit:1096/format:webp/1*i1rbCAY3bem9UbJrL8wkTA.png"/></div></figure><p id="6993" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">数据被载入pandas数据框架并进行分析。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="nd ne di nf bf ng"><div class="gh gi nu"><img src="../Images/f681dee4fb560eb1b7eaf8e71e8dcb52.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BL4lIttVX9WSemvNUrnSmQ.png"/></div></div></figure><p id="2d2f" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">最初的分析告诉我们，该数据有99999行和226列。有许多需要估算或删除的缺失值。</p><p id="3d1c" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">大部分空值超过30%的列被删除，对于其他列，空值被适当地估算为平均值/中值/众数。最后，我们有99999行和185列的数据集。</p><p id="ff85" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">由于大部分收入来自高价值客户，我们相应地过滤数据，得到一个30011行185列的数据集。</p><p id="55d8" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">我们为这些客户执行探索性数据分析(EDA)。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nv"><img src="../Images/90fabd721cbaa6e7d7891869ac27c59f.png" data-original-src="https://miro.medium.com/v2/resize:fit:600/format:webp/1*2O7WyEJjDzOSv_4mjgHXgg.png"/></div><p class="kn ko gj gh gi kp kq bd b be z dk translated">流失分析</p></figure><p id="17ec" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">我们可以看到91.4%的客户是非流失客户，只有8.6%的客户是流失客户。所以这是一个不平衡的数据集。以下是来自EDA的一些分析图。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nw"><img src="../Images/b8dd78dccf542343bddc4070defe6d68.png" data-original-src="https://miro.medium.com/v2/resize:fit:800/format:webp/1*FdwSYb_0BqXFTmPyElNJJg.png"/></div></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/a9ddbdd0d7b8ed029ce72c4710ecce76.png" data-original-src="https://miro.medium.com/v2/resize:fit:722/format:webp/1*cFKy25T__8pdAt8C6nuyMg.png"/></div><p class="kn ko gj gh gi kp kq bd b be z dk translated">电信公司网络内的std呼入使用分钟数</p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/7504c30cca11086014c1a695531eb548.png" data-original-src="https://miro.medium.com/v2/resize:fit:1040/format:webp/1*aH6Al-ht6wq4iAywUMZTzQ.png"/></div></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nz"><img src="../Images/c96f23363e719bcbee3a1310bb46536a.png" data-original-src="https://miro.medium.com/v2/resize:fit:734/format:webp/1*7wgTUz0te7xAs5zMVBPimA.png"/></div><p class="kn ko gj gh gi kp kq bd b be z dk translated">流失和非流失的漫游呼入分钟数</p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oa"><img src="../Images/294dc413b13b1c6b865e4a8316a6943d.png" data-original-src="https://miro.medium.com/v2/resize:fit:746/format:webp/1*0WF2eGyxSto0jZ3PcRau3g.png"/></div><p class="kn ko gj gh gi kp kq bd b be z dk translated">流失和非流失的本地传入使用分钟数</p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="nd ne di nf bf ng"><div class="gh gi ob"><img src="../Images/75634c3575a8a5bb09fb91e55b112214.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WAA7tJwAPtSmkyffc3dbcg.png"/></div></div></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="nd ne di nf bf ng"><div class="gh gi ob"><img src="../Images/ac4f74d44af3d49264ecf32e33362adc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QsMKEizhrolHKHxmW3yszg.png"/></div></div></figure><p id="bae8" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">我们将数据分为训练集和测试集，并将流失变量分离为<strong class="kt ir"> y、</strong>因变量，其余特征为自变量<strong class="kt ir"> X、</strong></p><p id="6c59" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">我们需要处理阶级不平衡，因为我们建立的每一个模型在多数阶级中都有好的表现，而在少数阶级中表现最差。我们可以通过平衡类来处理不平衡的类。意味着增加少数或减少多数阶级。</p><p id="0581" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">有以下各种类别不平衡处理技术:</p><ol class=""><li id="1140" class="mo mp iq kt b ku kv kx ky la mq le mr li ms lm mt mu mv mw bi translated">随机欠采样</li><li id="8ce2" class="mo mp iq kt b ku mx kx my la mz le na li nb lm mt mu mv mw bi translated">随机过采样</li><li id="40bd" class="mo mp iq kt b ku mx kx my la mz le na li nb lm mt mu mv mw bi translated">SMOTE——合成少数过采样技术</li><li id="40cf" class="mo mp iq kt b ku mx kx my la mz le na li nb lm mt mu mv mw bi translated">ADASYN——自适应合成采样方法</li><li id="4d11" class="mo mp iq kt b ku mx kx my la mz le na li nb lm mt mu mv mw bi translated">smetomek—过采样后欠采样</li></ol><p id="2204" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">我们用逻辑回归、决策树和随机森林模型实现了所有的技术。我们需要专注于那些能给我们带来高灵敏度/高召回率的模型。结果如下:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="nd ne di nf bf ng"><div class="gh gi oc"><img src="../Images/dd98deddeedfcb6b5f9b930f55205abd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9e5sOW1Ow_BgfJoeSlbRZg.png"/></div></div><p class="kn ko gj gh gi kp kq bd b be z dk translated">各种模型的评估指标比较</p></figure><p id="4ce7" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">正如我们在汇总表中看到的那样，对于随机欠采样的随机森林、随机欠采样的逻辑回归、随机过采样、SMOTE、ADASYN、SMOTE+TOMEK，我们的召回值很高。在逻辑回归中，ADASYN的回忆率最高。</p><p id="a3b6" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">我们将使用欠采样方法选取随机森林进行进一步分析。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi od"><img src="../Images/918c07d66c420314ae3df33ee2d919b1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1178/format:webp/1*2_JoZ6lMFXxYvvgBUwO66w.png"/></div></figure><p id="571c" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">我们知道随机森林通过消除不太重要的特征来赋予我们特征的重要性。我们运行随机森林分类器并选择重要特征:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="nd ne di nf bf ng"><div class="gh gi oe"><img src="../Images/f6c5d0ad8854e9b26f15ca84041eb80e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3hT4f4Yb2KyDvA0cxIEFIA.png"/></div></div></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="nd ne di nf bf ng"><div class="gh gi of"><img src="../Images/0dd9d20190630bb1ae12f4d47b6ced15.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MCew374cqib30BwUGYQpIw.png"/></div></div></figure><p id="9d61" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">我们使用GridSearchCV对随机森林欠采样模型进行超参数调整。对于决策树分类器，超参数的调整是类似的。对于基本模型，我们只需要用决策树分类器替换随机森林分类器。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi og"><img src="../Images/4366d5adebf7ef6a0efca6bfab7e8c0d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1328/format:webp/1*z4Yy6jhb2XbQrtdR1J1LLg.png"/></div></figure><p id="4871" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">我们得到以下最佳分数和最佳模型。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/97410356302ecc82ef0c6b7c92ba8b8f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1314/format:webp/1*sEUj790UKNtQhd9O2Sbasg.png"/></div></figure><p id="52a0" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">由<code class="fe oi oj ok ol b"><a class="ae om" href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html#sklearn.model_selection.GridSearchCV" rel="noopener ugc nofollow" target="_blank"><strong class="kt ir">GridSearchCV</strong></a></code>提供的网格搜索从由<code class="fe oi oj ok ol b">param_grid</code>参数指定的参数值网格中详尽地生成候选值。这个过程在计算上是昂贵和耗时的，尤其是在大的超参数空间上。</p><p id="a9d6" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">还有另一种方法叫做RandomizedSearchCV来寻找最佳模型。这里，从指定的概率分布中采样固定数量的超参数。这提高了流程的时间效率，降低了模型的复杂性。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi on"><img src="../Images/42867925551b32c8f6f655ef20b809a7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1342/format:webp/1*AkoyaLdGQvJhna1vPXBIoQ.png"/></div></figure><p id="995a" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">我们得到以下最佳分数和最佳模型。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oo"><img src="../Images/247408ad968b2ef0509382a507faad50.png" data-original-src="https://miro.medium.com/v2/resize:fit:1222/format:webp/1*dWZo_pdhzeXZVPkM1fokBA.png"/></div></figure><h2 id="5f3c" class="nh ls iq bd lt ni nj dn lx nk nl dp mb la nm nn md le no np mf li nq nr mh ns bi translated">通过比较上述模型得到的分数和复杂度，我们得出结论，使用RandomizedSearchCV得到的模型是最好的模型。</h2><p id="99a7" class="pw-post-body-paragraph kr ks iq kt b ku mj jr kw kx mk ju kz la ml lc ld le mm lg lh li mn lk ll lm ij bi translated">我们对测试集进行预测，得到如下结果。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi od"><img src="../Images/1e00da0eb666324a8c7da299379633ed.png" data-original-src="https://miro.medium.com/v2/resize:fit:1178/format:webp/1*ccX19kyk0gdEOyMLumrxpQ.png"/></div></figure><p id="41aa" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">我们得到了81%的召回率和85%的准确率。以下是客户流失的10大重要预测指标。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi op"><img src="../Images/3aff4533a01356d7a0c3c1270cfd318e.png" data-original-src="https://miro.medium.com/v2/resize:fit:884/format:webp/1*adBTsREptooE7gE67b2kZA.png"/></div></figure><p id="6159" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">这就完成了我们的分析。希望这篇文章内容丰富，易于理解。此外，我希望您喜欢分析分析中包含的彩色图表。</p><p id="5d89" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">请随意评论并给出您的反馈。</p><p id="4fb9" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated"><strong class="kt ir"> <em class="lo">可以在领英上联系我:</em></strong><a class="ae om" href="https://www.linkedin.com/in/pathakpuja/" rel="noopener ugc nofollow" target="_blank"><strong class="kt ir"><em class="lo">https://www.linkedin.com/in/pathakpuja/</em></strong></a></p><p id="dc00" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated"><strong class="kt ir"> <em class="lo">请访问我的GitHub个人资料获取python代码。文中提到的代码，以及图表，可以在这里找到:</em></strong><a class="ae om" href="https://github.com/pujappathak" rel="noopener ugc nofollow" target="_blank"><strong class="kt ir"><em class="lo">https://github.com/pujappathak</em></strong></a></p><p id="baad" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated"><strong class="kt ir"> <em class="lo">参考文献:</em> </strong></p><p id="6344" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated"><a class="ae om" href="https://scikit-learn.org/0.16/modules/generated/sklearn.grid_search.RandomizedSearchCV.html" rel="noopener ugc nofollow" target="_blank"><em class="lo">https://sci kit-learn . org/0.16/modules/generated/sk learn . grid _ search。RandomizedSearchCV.html</em></a></p><p id="3ed3" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated"><a class="ae om" href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html" rel="noopener ugc nofollow" target="_blank"><em class="lo">https://sci kit-learn . org/stable/modules/generated/sk learn . model _ selection。GridSearchCV.html</em></a></p></div></div>    
</body>
</html>