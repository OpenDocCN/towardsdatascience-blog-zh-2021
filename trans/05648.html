<html>
<head>
<title>How to Effectively Manage Deployed Models</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如何有效地管理部署的模型</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/how-to-effectively-manage-deployed-models-6c35ced55f92?source=collection_archive---------40-----------------------#2021-05-19">https://towardsdatascience.com/how-to-effectively-manage-deployed-models-6c35ced55f92?source=collection_archive---------40-----------------------#2021-05-19</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="34d1" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">通过Tensorflow服务管理您的模型的生命周期</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/fb15675e2642e66fdd47cc87ba1a80bc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7N_lPi-bP1xJAp65KULL1w.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">来源(<a class="ae kv" href="https://unsplash.com/" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>)</p></figure><p id="2fc9" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">大多数型号从未投入生产。我们之前看了<a class="ae kv" rel="noopener" target="_blank" href="/putting-your-models-into-production-5ae3191722b9">使用Tensorflow服务部署Tensorflow模型</a>。一旦这一过程完成，我们可能会认为我们的工作已经全部完成。事实上，我们刚刚开始了管理模型生命周期的新旅程，并确保它保持最新和有效。</p><p id="dd63" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">像软件中的大多数东西一样，需要持续的开发和改进。一旦模型被部署，管理它的任务是一个经常被忽视的任务。在这里，我们将看看如何有效地做到这一点，并使我们的模型管道更有效。</p><p id="4199" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">多型号配置</strong></p><p id="175d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">当服务模型时，最简单的方法是指定<code class="fe ls lt lu lv b">MODEL_NAME</code>环境变量，如这里的<a class="ae kv" rel="noopener" target="_blank" href="/putting-your-models-into-production-5ae3191722b9#12d0">所示</a>。但是，如果我们想为多种型号服务呢？不同的配置选项，比如使用多少个线程或轮询频率，会怎么样？在这种情况下，我们可以使用如下所示的模型服务器配置文件。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="lx ly l"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">模型服务器配置文件</p></figure><p id="11e9" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们使用模型配置列表选项，它是模型配置协议缓冲区的列表。每个ModelConfig都应该指定一个要服务的模型。上面我们看到两个模型，分别命名为<code class="fe ls lt lu lv b">first_model</code>和<code class="fe ls lt lu lv b">second_model</code>。我们还需要指定一个<code class="fe ls lt lu lv b">base_path</code>，服务器将在这里寻找模型的版本以及<code class="fe ls lt lu lv b">model_platform</code>。我们的示例展示了在TensorFlow上运行的两个模型。</p><p id="b47f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">一旦我们有了ModelServer配置文件，我们就可以使用如下所示的命令运行docker容器。</p><pre class="kg kh ki kj gt lz lv ma mb aw mc bi"><span id="c22d" class="md me iq lv b gy mf mg l mh mi">docker run --rm -p 8501:8501 -v "$(pwd)/models/:/models/" \<br/>-t tensorflow/serving <strong class="lv ir">--model_config_file=/models/models.config \<br/>--model_config_file_poll_wait_seconds=60</strong></span></pre><p id="8d1f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在之前，我们已经看过这个<code class="fe ls lt lu lv b">docker run</code>命令<a class="ae kv" rel="noopener" target="_blank" href="/putting-your-models-into-production-5ae3191722b9#7fac">。唯一的区别是我们在最后增加了<code class="fe ls lt lu lv b">--model_config_file</code>和<code class="fe ls lt lu lv b">--model_config_file_poll_wait_seconds</code>选项。<code class="fe ls lt lu lv b">--model_config_file</code>选项设置为我们的模型服务器配置文件，而<code class="fe ls lt lu lv b">--model_config_file_poll_wait_seconds</code>选项设置为60。这意味着服务器每分钟都会在指定位置检查新的配置文件，并根据需要进行更新。</a></p><p id="c5b0" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">型号版本和版本标签</strong></p><p id="a5aa" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">当Tensorflow Serving搜索一个模型的<code class="fe ls lt lu lv b">base_path</code>时，其默认行为是总是根据版本号服务于<em class="lw">最新的</em>模型。如果在我们的路径中有版本1到5，版本5将总是被服务。如果我们想提供更早的版本呢？还是为同一型号的多个版本服务？在这种情况下，我们可以在配置文件中指定model_version_policy，并将其设置为<code class="fe ls lt lu lv b">specific</code>，提供我们想要提供的版本号。</p><pre class="kg kh ki kj gt lz lv ma mb aw mc bi"><span id="6b5b" class="md me iq lv b gy mf mg l mh mi">model_version_policy {<br/>  specific {<br/>    versions: 2<br/>  }<br/>}</span></pre><p id="f0f5" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">通过将上面的代码片段添加到我们的配置文件中，我们的服务器将知道提供我们模型的版本2，而不是最新的版本。如果我们想一次提供多个版本，我们可以简单地添加更多的版本。</p><pre class="kg kh ki kj gt lz lv ma mb aw mc bi"><span id="9cdc" class="md me iq lv b gy mf mg l mh mi">model_version_policy {<br/>  specific {<br/>    versions: 2<br/>    versions: 3<br/>    versions: 4<br/>  }<br/>}</span></pre><p id="0d70" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们还可以给模型版本分配字符串标签，这样我们可以更容易地记住它们。</p><pre class="kg kh ki kj gt lz lv ma mb aw mc bi"><span id="7b91" class="md me iq lv b gy mf mg l mh mi">model_version_policy {<br/>  specific {<br/>    versions: 2<br/>    versions: 3<br/>    versions: 4<br/>  }<br/>}<br/>version_labels {<br/>  key: 'stable'<br/>  value: 2<br/>}<br/>version_labels {<br/>  key: 'canary'<br/>  value: 3<br/>}<br/>version_labels {<br/>  key: 'dev'<br/>  value: 4<br/>}</span></pre><p id="1a54" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">上面我们已经给我们的模型版本分配了标签<code class="fe ls lt lu lv b">stable</code>、<code class="fe ls lt lu lv b">canary</code>和<code class="fe ls lt lu lv b">dev</code>。如果我们想将版本3升级到稳定版本，我们可以简单地将<code class="fe ls lt lu lv b">stable</code>的值从2改为3。类似地，如果我们想让版本4成为新的canary版本，我们只需要改变相应的值。</p><p id="e0ca" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">A/B型测试</strong></p><p id="fd4d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">一个常见的业务场景是模型A/B测试。总有一天，我们会想看看开发中的模型与当前生产中的模型相比如何。有了Tensorflow的服务，A/B测试变得容易多了。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="lx ly l"/></div></figure><p id="a2ae" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在上面的代码片段中，我们创建了一个名为<code class="fe ls lt lu lv b">get_request_url</code>的助手函数，它返回一个我们可以发送post请求的URL。关于这一点，我们可以设置一些阈值(本例中为0.05)，将一定比例的流量路由到不同的模型版本。您还可以对此进行归纳，以便我们可以将流量路由到任意数量的模型版本。</p><p id="9eb4" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">如果我们想慢慢地扩大金丝雀模型版本，我们可以在我们认为合适的时候增加这个阈值，直到金丝雀版本处理大部分传入流量。可以想象，使用这种类型的设置进行回滚也非常简单。</p><p id="aa29" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">迷你批处理推论</strong></p><p id="b385" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">Tensorflow服务的一个非常有用的特性是能够批量处理请求以增加吞吐量。如果我们一次发出一个请求，我们的服务器将会被占用，在接受任何新的请求之前等待每个请求完成。类似于我们在训练模型时如何批量训练示例，我们也可以将几个推理请求批量在一起，这样模型就可以一次性处理它们。为此，我们将创建一个<code class="fe ls lt lu lv b">batch_parameters.txt</code>文件来保存我们的批处理配置。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="lx ly l"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">一个简单的批处理配置文件</p></figure><p id="c9d3" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在上面的文件中，我们将一个批处理中的最大请求数设置为256，最大入队批处理数设置为1000000，最大并发线程数设置为4。更详尽的可用选项列表可以在<a class="ae kv" href="https://github.com/tensorflow/serving/blob/master/tensorflow_serving/batching/README.md" rel="noopener ugc nofollow" target="_blank">这里</a>的文档中找到。</p><p id="2c35" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">一旦我们有了这个文件，我们就可以通过向我们的<code class="fe ls lt lu lv b">docker run command</code>添加两个选项来启用批处理。</p><pre class="kg kh ki kj gt lz lv ma mb aw mc bi"><span id="d6e2" class="md me iq lv b gy mf mg l mh mi">docker run --rm -p 8501:8501 -v "$(pwd)/models/:/models/" \<br/>-t tensorflow/serving --model_config_file=/models/models.config \<br/>--model_config_file_poll_wait_seconds=60 <strong class="lv ir">--enable_batching=true \<br/>--batching_parameters_file=/models/batching_parameters.txt</strong></span></pre><p id="6fc2" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们只需将<code class="fe ls lt lu lv b">--enable_batching</code>选项设置为真，并将我们的文件传递给<code class="fe ls lt lu lv b">--batching_parameters_file</code>选项。就这么简单！通过在我们的服务器中启用批处理，我们可以显著提高吞吐量，从而可以处理更大的流量。不需要额外的服务器！</p><p id="d1fb" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">结论</strong></p><p id="9dc5" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们研究了Tensorflow服务的不同特性，这些特性使得管理模型部署和维护变得更加容易。当我们想要一次运行多个模型(或者并行运行同一模型的多个版本)时，我们使用一个<strong class="ky ir"> ModelServer配置文件</strong>来指定每个模型的配置。我们还研究了<strong class="ky ir">为特定模型</strong>分配字符串值，以便更容易跟踪生产中的产品和非生产中的产品。我们还讨论了<strong class="ky ir"> A/B测试</strong>，并看到将流量路由到我们模型的不同版本来比较结果是多么容易。最后，我们看了一下<strong class="ky ir">批处理</strong>，这是一种并发处理大量推理请求的简单方法，因此我们可以增加吞吐量。</p><p id="d320" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">感谢您的阅读！</p></div><div class="ab cl mj mk hu ml" role="separator"><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo"/></div><div class="ij ik il im in"><p id="e4b2" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">您可以通过以下渠道与我联系:</p><ul class=""><li id="a383" class="mq mr iq ky b kz la lc ld lf ms lj mt ln mu lr mv mw mx my bi translated"><a class="ae kv" href="https://zito-relova.medium.com/" rel="noopener">中等</a></li><li id="6dce" class="mq mr iq ky b kz mz lc na lf nb lj nc ln nd lr mv mw mx my bi translated"><a class="ae kv" href="https://www.linkedin.com/in/zrelova/" rel="noopener ugc nofollow" target="_blank">领英</a>。</li><li id="2713" class="mq mr iq ky b kz mz lc na lf nb lj nc ln nd lr mv mw mx my bi translated"><a class="ae kv" href="https://github.com/zitorelova" rel="noopener ugc nofollow" target="_blank"> Github </a></li><li id="2369" class="mq mr iq ky b kz mz lc na lf nb lj nc ln nd lr mv mw mx my bi translated"><a class="ae kv" href="https://www.kaggle.com/zitorelova" rel="noopener ugc nofollow" target="_blank">卡格尔</a></li></ul></div></div>    
</body>
</html>