<html>
<head>
<title>When Do Support Vector Machines Fail?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">支持向量机什么时候会失败？</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/when-do-support-vector-machines-fail-3f23295ebef2?source=collection_archive---------5-----------------------#2021-08-29">https://towardsdatascience.com/when-do-support-vector-machines-fail-3f23295ebef2?source=collection_archive---------5-----------------------#2021-08-29</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><figure class="ip iq gp gr ir is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi io"><img src="../Images/c2d2cfe6aaee59652cd654a536cb8712.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*eT7o37pdkA9uagVI"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">托马斯·索贝克在<a class="ae jd" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><div class=""/><div class=""><h2 id="dd16" class="pw-subtitle-paragraph kd jf jg bd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku dk translated">何时使用和不使用它们，以及如何使用</h2></div><h1 id="2123" class="kv kw jg bd kx ky kz la lb lc ld le lf km lg kn lh kp li kq lj ks lk kt ll lm bi translated">介绍</h1><p id="d60e" class="pw-post-body-paragraph ln lo jg lp b lq lr kh ls lt lu kk lv lw lx ly lz ma mb mc md me mf mg mh mi ij bi translated">你可能碰巧看到了我的<a class="ae jd" rel="noopener" target="_blank" href="/support-vector-machines-in-under-5-minutes-3074762a49bf">上一篇关于介绍支持向量机(SVMs)的文章</a>，在这篇文章中，关键的基本概念在一个高层次上被介绍了。在本文中，我们将讨论在分类和回归用例中，支持向量机何时不适合使用。</p><h1 id="3d1b" class="kv kw jg bd kx ky kz la lb lc ld le lf km lg kn lh kp li kq lj ks lk kt ll lm bi translated">1)支持向量机不适合大型数据集</h1><p id="2017" class="pw-post-body-paragraph ln lo jg lp b lq lr kh ls lt lu kk lv lw lx ly lz ma mb mc md me mf mg mh mi ij bi translated">已知最初的SVM实现具有具体的理论基础，但它不适合在大型数据集中进行分类，原因很简单-算法训练的复杂性高度依赖于数据集的大小。换句话说，训练时间会随着数据集的增长而增长，直到由于计算限制而变得不可行为止。</p><p id="e2eb" class="pw-post-body-paragraph ln lo jg lp b lq mj kh ls lt mk kk lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated">从好的方面来看，自1992年美国电话电报公司贝尔实验室首次实施以来，SVM已经取得了一些进展[1]。如今，随着数据集规模的增大，训练支持向量机的可扩展性大大提高。</p><h1 id="c1ba" class="kv kw jg bd kx ky kz la lb lc ld le lf km lg kn lh kp li kq lj ks lk kt ll lm bi translated">2)支持向量机在不平衡数据集中表现不佳</h1><p id="ea87" class="pw-post-body-paragraph ln lo jg lp b lq lr kh ls lt lu kk lv lw lx ly lz ma mb mc md me mf mg mh mi ij bi translated">这有两个更好的理由。第一个是软利润优化问题的弱点。当不平衡数据用于训练时，这导致超平面偏向少数类。</p><p id="c158" class="pw-post-body-paragraph ln lo jg lp b lq mj kh ls lt mk kk lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated">第二个原因源于不平衡支持向量比率的问题，即正和负支持向量之间的比率变得不平衡，结果，超平面的决策边界处的数据点被分类为负的可能性更高。</p><p id="96b5" class="pw-post-body-paragraph ln lo jg lp b lq mj kh ls lt mk kk lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated">然而，有一些方法可以减少这种影响。一种更常用的方法是引入类别权重，因此正面支持的大小将成比例地高于负面支持向量的大小。当使用不平衡数据集进行训练时，类权重也用于其他机器学习算法中。</p><h1 id="e07a" class="kv kw jg bd kx ky kz la lb lc ld le lf km lg kn lh kp li kq lj ks lk kt ll lm bi translated">3)具有“错误”核的支持向量机</h1><p id="9609" class="pw-post-body-paragraph ln lo jg lp b lq lr kh ls lt lu kk lv lw lx ly lz ma mb mc md me mf mg mh mi ij bi translated">这有点直观。对于现在的支持向量机来说，选择正确的核函数是关键。例如，当数据不可线性分离时，使用线性核会导致算法性能不佳。然而，选择“正确的”核本身是另一个问题，在所使用的技术中，流行的技术包括在超参数搜索中改变核函数。</p><h1 id="909c" class="kv kw jg bd kx ky kz la lb lc ld le lf km lg kn lh kp li kq lj ks lk kt ll lm bi translated">4)当数据中有太多噪声时</h1><p id="5537" class="pw-post-body-paragraph ln lo jg lp b lq lr kh ls lt lu kk lv lw lx ly lz ma mb mc md me mf mg mh mi ij bi translated">在这些有噪声数据的情况下，目标类是重叠的，也就是说，要素可能具有非常相似或重叠的属性。由于优化算法的性质，这可能导致达到几个局部最优，特别是对于高维数据集。</p><p id="2e2c" class="pw-post-body-paragraph ln lo jg lp b lq mj kh ls lt mk kk lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated">然而，值得注意的是，对于具有高偏差的核，例如线性和多项式核，噪声不应该是问题。噪声问题应该更多地存在于诸如径向基函数(RBF)的低偏差核中。</p><h1 id="d8b9" class="kv kw jg bd kx ky kz la lb lc ld le lf km lg kn lh kp li kq lj ks lk kt ll lm bi translated">附加备注</h1><p id="facd" class="pw-post-body-paragraph ln lo jg lp b lq lr kh ls lt lu kk lv lw lx ly lz ma mb mc md me mf mg mh mi ij bi translated">虽然使用原始SVM算法有几个众所周知的缺点，但多年来已经进行了大量研究来改进它，使得上述现有影响不太严重，并且可以通过其他方法来减少/减轻。</p><h1 id="0a6d" class="kv kw jg bd kx ky kz la lb lc ld le lf km lg kn lh kp li kq lj ks lk kt ll lm bi translated">参考</h1><p id="7f72" class="pw-post-body-paragraph ln lo jg lp b lq lr kh ls lt lu kk lv lw lx ly lz ma mb mc md me mf mg mh mi ij bi translated">[1] Boser，b .，Guyon，I .，Vapnik，v .:最优间隔分类器的训练算法。《第五届计算学习理论年度研讨会论文集》，匹兹堡(1992)</p><p id="33b6" class="pw-post-body-paragraph ln lo jg lp b lq mj kh ls lt mk kk lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated">[2]巴图维塔和帕拉迪(2013年)。支持向量机的类别不平衡学习方法。在不平衡的学习中。<a class="ae jd" href="https://doi.org/10.1002/9781118646106.ch5" rel="noopener ugc nofollow" target="_blank">https://doi.org/10.1002/9781118646106.ch5</a></p></div></div>    
</body>
</html>