<html>
<head>
<title>Logistic Regression Explained from Scratch (Visually, Mathematically and Programmatically)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">从头开始解释逻辑回归(视觉上、数学上和程序上)</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/logistic-regression-explained-from-scratch-visually-mathematically-and-programmatically-eb83520fdf9a?source=collection_archive---------2-----------------------#2021-03-31">https://towardsdatascience.com/logistic-regression-explained-from-scratch-visually-mathematically-and-programmatically-eb83520fdf9a?source=collection_archive---------2-----------------------#2021-03-31</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="437c" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">动手香草建模第三部分</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/2ff0cff60bc6ad3a14d5d344a756257e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZPaIJzRLHRpaAfcRjjieZw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="fa72" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在小型谷歌搜索“逻辑回归”中会出现过多的结果。有时候，对于数据科学的初学者来说，要理解逻辑回归背后的主要思想是非常困难的。为什么他们不会感到困惑！！？每个不同的教程、文章或论坛对逻辑回归都有不同的叙述(不包括教科书的合法冗长，因为那会扼杀这些精通的“快速来源”的全部目的)。一些资料来源声称它是一种“分类算法”，一些更复杂的资料来源称它为“回归器”，然而，其思想和效用仍未被揭示。<strong class="kx ir">请记住，逻辑回归是人工神经网络的基本构建模块，对它没有/错误的理解可能会使理解数据科学的高级形式变得非常困难。</strong></p><p id="e6ee" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在这里，我将尝试以一种非常基本的方式阐明逻辑回归模型及其形式，以便给读者一种理解的感觉(希望不会使他们混淆)。现在，这里提供的简单性是以跳跃一些关键方面的深入细节为代价的，深入逻辑回归每个方面的本质就像潜入分形(讨论将永无止境)。然而，对于每一个这样的概念，我会提供你应该参考的著名读物/资料来源。</p><p id="b25c" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">因为在逻辑回归的研究中有两个主要分支(I)建模和(ii)建模后分析(使用逻辑回归结果)。虽然后者是对拟合系数效果的测量，但我相信逻辑回归的黑箱方面一直存在于它的<strong class="kx ir">模型</strong>中。</p><p id="6d27" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我的目标是:</p><ol class=""><li id="b49d" class="lr ls iq kx b ky kz lb lc le lt li lu lm lv lq lw lx ly lz bi translated">用最通俗的方式来阐述逻辑回归。</li><li id="02ec" class="lr ls iq kx b ky ma lb mb le mc li md lm me lq lw lx ly lz bi translated">讨论逻辑回归中使用的两种流行的优化器(梯度下降和牛顿法)的数学基础。</li><li id="7106" class="lr ls iq kx b ky ma lb mb le mc li md lm me lq lw lx ly lz bi translated">在R中为每种类型的优化器从头开始创建逻辑回归模块。</li></ol><p id="eecd" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在我们继续之前，最后一件事是，为了避免复杂性，整篇文章都是通过记住二进制分类问题来设计的。</p></div><div class="ab cl mf mg hu mh" role="separator"><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk"/></div><div class="ij ik il im in"><h1 id="a356" class="mm mn iq bd mo mp mq mr ms mt mu mv mw jw mx jx my jz mz ka na kc nb kd nc nd bi translated">1.逻辑回归不是一个分类器</h1><p id="8c62" class="pw-post-body-paragraph kv kw iq kx b ky ne jr la lb nf ju ld le ng lg lh li nh lk ll lm ni lo lp lq ij bi translated">是的，它不是。它本质上是一个回归模型。我将描述什么和为什么逻辑回归，同时保留它与线性回归模型的共鸣。假设我的读者对线性回归的基础有所了解，很容易说线性回归通过给定特征的线性组合来预测目标变量的“值”，而另一方面，逻辑回归通过插入到等式(1)给出的<a class="ae nj" href="https://en.wikipedia.org/wiki/Logistic_function" rel="noopener ugc nofollow" target="_blank"> <strong class="kx ir">逻辑函数</strong> </a> <strong class="kx ir">(又名</strong><a class="ae nj" href="https://en.wikipedia.org/wiki/Logit" rel="noopener ugc nofollow" target="_blank"><strong class="kx ir"/></a><strong class="kx ir">)</strong>中的给定特征的线性组合来预测“概率值”:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/3976eb10379da521b27e25afc9144877.png" data-original-src="https://miro.medium.com/v2/resize:fit:306/format:webp/1*Z-XP2vn7ZmUFv70TVMzYhA.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><strong class="bd nl">等式1 </strong></p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nm"><img src="../Images/f43c1ec8e32c07fe2fef2bc30a8ccb7f.png" data-original-src="https://miro.medium.com/v2/resize:fit:910/format:webp/1*xcPQJSUEaeItrNwDUPqEaQ.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">逻辑函数(图片由作者提供)</p></figure><p id="b5cb" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">因此命名为逻辑回归。这个逻辑函数是将位于(-inf，inf)范围内的线性组合<strong class="kx ir"><em class="nn">【z】，</em> </strong>映射到概率区间[0，1]的简单策略(在逻辑回归的上下文中，这个<strong class="kx ir"><em class="nn">z</em><em class="nn"/>将被称为log(奇数)或logit或log(p/1-p))(见上图)。因此，逻辑回归是一种映射范围限于[0，1]的回归，不像简单的线性回归模型，其域和范围可以取任何实值。</strong></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi no"><img src="../Images/3a7d975a019a3894a7a4333729c48cf8.png" data-original-src="https://miro.medium.com/v2/resize:fit:300/format:webp/1*Cn5pH5wir3lIk6cYaEijCQ.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">数据的一个小样本(图片由作者提供)</p></figure><p id="8b81" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">考虑具有一个变量及其对应的二进制类0或1的简单数据。这些数据的散点图看起来像这样(左图A)。我们看到数据点在两个极端的集群中。很好，现在对于我们的预测建模，这种情况下的简单回归线将给出一个无意义的拟合(图A右侧的红线),我们实际上需要拟合的是一条曲线(或图A右侧的弯曲“S”形蓝色规则)来解释(或正确分离)最大数量的数据点。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi np"><img src="../Images/d601373611e36e475f2a326aad26fb16.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*V4qdmq4BTL61VMV5-gDvKw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">(图片由作者提供)图A</p></figure><p id="943e" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">逻辑回归是一种寻找最佳蓝色曲线的方法。现在首先让我们了解这条曲线上的每个点代表什么。给定投射在这条线上的任何变量值，这条曲线告诉该投射变量值落入第1类(比如“p”)的<strong class="kx ir">概率。因此相应地，这条线告诉我们，位于这条蓝线上的所有底部点属于1类的概率为零(p=0 ),位于这条线上的顶部点属于1类的概率为1(p=1)。现在，请记住我已经提到的逻辑(又名逆逻辑)是一种将无限拉伸空间(-inf，inf)映射到概率空间[0，1]的策略，一个<a class="ae nj" href="https://en.wikipedia.org/wiki/Logit" rel="noopener ugc nofollow" target="_blank"> <strong class="kx ir">逻辑函数</strong> </a> <strong class="kx ir"> </strong>可以将概率空间[0，1]转换为拉伸到(-inf，inf)等式(2) &amp;的空间(图B)。</strong></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/016ecfb6573727e52d55350507f1f2a5.png" data-original-src="https://miro.medium.com/v2/resize:fit:372/format:webp/1*McIav1yFXIKet8EII4MrgA.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><strong class="bd nl">等式2 </strong></p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/4be7636124d9501ea4e19d1af1a2ae3d.png" data-original-src="https://miro.medium.com/v2/resize:fit:720/format:webp/1*fAJ9mybZbEgsOba2Dr_Tvw.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><em class="ns">图b . logit函数由</em><strong class="bd nl"><em class="ns">log(p/1-p)</em></strong><em class="ns">给出，它将每个概率值映射到数轴上的点{</em><strong class="bd nl"><em class="ns">ℝ</em></strong><em class="ns">}从-无限延伸到无限</em>(图片由作者提供)</p></figure><p id="b0f5" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">记住这一点，逻辑回归建模的口头禅来了:</p><blockquote class="nt nu nv"><p id="22ff" class="kv kw nn kx b ky kz jr la lb lc ju ld nw lf lg lh nx lj lk ll ny ln lo lp lq ij bi translated">逻辑回归首先从<strong class="kx ir"><em class="iq"/></strong>将<strong class="kx ir"> <em class="iq">类概率的空间[ </em> </strong> <em class="iq"> 0，1]</em><strong class="kx ir">vs</strong><em class="iq"/><strong class="kx ir"><em class="iq">变量</em> </strong> <em class="iq"> {ℝ} </em> ( <em class="iq">如图右</em>)转换到<strong class="kx ir">logit</strong><em class="iq">{</em><strong class="kx ir"><em class="iq">ℝ<em class="iq"> </em>通过调整系数和斜率来执行“类回归”拟合，以最大化<strong class="kx ir">可能性</strong>(这是一个非常奇特的东西，我将在下一节阐述这一部分)。 <strong class="kx ir"><em class="iq">ⓑ</em></strong>一旦调整和调优完成，<strong class="kx ir">logit</strong></em><strong class="kx ir"><em class="iq">ℝ</em></strong><em class="iq">}</em><strong class="kx ir">vs变量</strong><em class="iq">{</em><strong class="kx ir"><em class="iq">ℝ</em></strong><em class="iq">}</em>空格被重新映射到<strong class="kx ir"> <em class="iq">类反复执行这个循环(<strong class="kx ir"><em class="iq">【ⓐ→ⓑ→ⓐ】</em></strong>)最终会得到最佳的曲线或最有区别的规则。</em></strong></strong></p></blockquote><p id="f162" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">哇！！！</p><p id="a333" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">那么，你可能(应该)会问(我)为什么以及如何进行这种转变？？(ii)可能性到底是什么？？以及(iii)该方案将如何产生最佳曲线？！！。</p><p id="2664" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">所以对于(I)，从有限的概率空间[0，1]到无限延伸的真实空间(-inf，inf)的转换的想法是因为它将使拟合问题非常接近于求解线性回归，对于线性回归，我们有许多优化器和技术来拟合最佳线。后面的问题最终会得到解答。</p><p id="bad6" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">现在回到我们对最佳分类蓝色曲线的搜索，想法是首先用⚠️ <strong class="kx ir"> logit vs变量空间</strong> ⚠️ <strong class="kx ir"> </strong>坐标上的任意系数绘制初始线性回归线，然后调整该拟合的系数以最大化可能性(放松！！需要的时候我会解释“可能性”)。</p><p id="8061" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在我们的单变量情况下，我们可以写出等式3:</p><p id="5fa3" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx ir">logit(p)= log(p/1-p)=</strong>β₀+β₁<strong class="kx ir">*</strong>v……(等式3)</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nz"><img src="../Images/84d38ae02567af55f69926b0c94ecf33.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZFvPX0EdEFFpRxtKqbXm9w.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">(图片由作者提供)图C</p></figure><p id="9476" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在<strong class="kx ir">图C (I) </strong>中，红线是我们为数据点拟合的任意选择的回归线，映射在不同的坐标系中，β₀(截距)为-20，β₁(slope为3.75。</p><p id="6d91" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">⚠️注意到，坐标空间不是<strong class="kx ir"> <em class="nn">类</em> </strong> {0，1} <em class="nn"> vs </em> <strong class="kx ir"> <em class="nn">变量</em> </strong> {ℝ}而是其<strong class="kx ir">logit</strong>{<strong class="kx ir">ℝ</strong>}<em class="nn"/><strong class="kx ir">vs变量</strong> { <strong class="kx ir"> ℝ </strong> } <em class="nn">。</em>此外，请注意，从图A(右)到图C(I)的变换对点的位置偏好没有影响，即上述<strong class="kx ir">等式2 </strong>中的极值，logit(0)=-无穷大，logit(1)=+无穷大。</p><p id="bfa9" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在这一点上，让我重申我们的目标:我们希望在<em class="nn">logit vs . variable</em>图中拟合数据点的直线，以这样一种方式解释(正确分离)当它通过反logit(又名逻辑函数)<strong class="kx ir"> eq(1) </strong>转换为蓝色曲线时数据点的最大数量。因此，为了实现最佳回归，简单线性回归的类似策略开始发挥作用，但尽管最小化残差的平方，想法是最大化<strong class="kx ir">可能性</strong>(放松！！).<strong class="kx ir">这样，投影到logit上的每个数据点对应于一个logit值。当这些logit值被插入逻辑函数eq(1)时，我们得到它们落入图C(III)的1类的概率。</strong></p><blockquote class="nt nu nv"><p id="7f0f" class="kv kw nn kx b ky kz jr la lb lc ju ld nw lf lg lh nx lj lk ll ny ln lo lp lq ij bi translated">注意:这也可以用数学方法证明:<strong class="kx ir"> logit(p)=logistic⁻ (p) </strong></p></blockquote><p id="ef48" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">这个概率在数学上可以用等式4来表示，非常接近伯努利分布，不是吗？。</p><p id="a25e" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx ir">p(y =<em class="nn">y</em>| x =<em class="nn">x</em>)=σ(βᵀ<em class="nn">x</em>)ʸ【1σ(βᵀ<em class="nn">x</em>)]⁽⁻ʸ⁾；</strong> <em class="nn">其中y为0或1 </em>..等式(4)</p><p id="c9c2" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">该等式表示对于给定的数据实例<strong class="kx ir"> <em class="nn"> x </em> </strong> <em class="nn">，</em>当<em class="nn"> y </em> =1时，标签Y为<em class="nn"> y </em>(其中<em class="nn"> y </em>为0或1)的概率等于logit 的<em class="nn">逻辑，如果<em class="nn"> y </em> =0，则等于(1-logit的逻辑)。这些新的概率值在我们的<strong class="kx ir"> <em class="nn">类</em> </strong> {0，1} <em class="nn"> vs </em> <strong class="kx ir"> <em class="nn">变量</em> </strong> {ℝ}空间中图示为图C(III)中的蓝点。数据点的这个新概率值就是我们所说的该数据点的<strong class="kx ir">可能性</strong>。因此，简单来说，可能性是数据点的概率值，该概率值表示该点落入1类类别的可能性有多大。并且拟合的权重向量β的训练标签的可能性仅仅是这些新发现的概率值中的每一个的乘积。</em></p><p id="c6fc" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">l(β)=ⁿ∏ᵢ₌₁p(y =y⁽ⁱ⁾| x =x⁽ⁱ⁾)……………………….等式(5)</p><p id="dcde" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">将等式4代入等式5，我们得到，</p><p id="1ac8" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx ir">l(β)=ⁿ∏ᵢ₌₁σ(βᵀ<em class="nn">x</em>⁽ⁱ⁾)ʸ⁽ⁱ⁾【1】σ(βᵀ<em class="nn">x</em>⁽ⁱ⁾)]⁽⁻ʸ⁽ⁱ⁾⁾</strong>………………等式(6)</p><p id="98b0" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">想法是估计参数(<strong class="kx ir"> β) </strong>，使得它最大化L( <strong class="kx ir"> β </strong>)。但出于数学上的方便，我们将L( <strong class="kx ir"> β </strong>)的对数最大化，称其对数似然方程为7。</p><p id="835b" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx ir">ll(β)=ⁿ∑ᵢ₌₁y⁽ⁱ⁾logσ(βᵀ<em class="nn">x</em>⁽ⁱ⁾)+(1y⁽ⁱ⁾)log【1σ(βᵀ<em class="nn">x</em>⁽ⁱ⁾)】</strong>……..等式(7)</p><p id="2a14" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">因此，在这一点上，我希望我们之前陈述的目标更容易理解，即在<em class="nn"> logit vs变量</em>空间中找到最佳拟合参数<strong class="kx ir"> β </strong>，使得<em class="nn">概率vs变量</em>空间中的<strong class="kx ir"> LL(β) </strong>最大。对此，没有<a class="ae nj" href="https://en.wikipedia.org/wiki/Closed-form_expression" rel="noopener ugc nofollow" target="_blank">的封闭形式</a>，因此在下一节中，我将涉及两种优化方法(1)梯度下降法和(2)牛顿法，以找到最佳参数。</p></div><div class="ab cl mf mg hu mh" role="separator"><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk"/></div><div class="ij ik il im in"><h1 id="14f1" class="mm mn iq bd mo mp mq mr ms mt mu mv mw jw mx jx my jz mz ka na kc nb kd nc nd bi translated">2.优化者</h1><h2 id="f2d9" class="oa mn iq bd mo ob oc dn ms od oe dp mw le of og my li oh oi na lm oj ok nc ol bi translated"><strong class="ak">坡度上升</strong></h2><p id="fa0d" class="pw-post-body-paragraph kv kw iq kx b ky ne jr la lb nf ju ld le ng lg lh li nh lk ll lm ni lo lp lq ij bi translated">我们的优化首先需要对数似然函数的偏导数。所以让我无耻地分享一个非常著名的<a class="ae nj" href="https://web.stanford.edu/class/archive/cs/cs109/cs109.1178/lectureHandouts/220-logistic-regression.pdf" rel="noopener ugc nofollow" target="_blank">讲义</a>的截图，它很好地阐明了导出<strong class="kx ir"> LL(β)的偏导数的步骤。</strong> <em class="nn">(注:这里显示的计算用θ代替β来表示参数。)</em></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi om"><img src="../Images/15b6e297d3ac93b6ec1238c285b179a6.png" data-original-src="https://miro.medium.com/v2/resize:fit:814/format:webp/1*4yZymm1cAYedTLzshW_Oyw.png"/></div></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi on"><img src="../Images/d0276b90d2a06ea809ce0a44b9fcb629.png" data-original-src="https://miro.medium.com/v2/resize:fit:524/format:webp/1*Ns6WeUn-SLNh88YbtnCkbw.png"/></div></figure><p id="5d51" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">要更新参数，达到全局最大值的步骤是:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oo"><img src="../Images/ab992ceee98d59b9f57ead8db507d05d.png" data-original-src="https://miro.medium.com/v2/resize:fit:666/format:webp/1*cagkH7HT2s4rbF96BNm9xQ.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><strong class="bd nl">其中<em class="ns"> η </em>是学习率</strong></p></figure><p id="a995" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">所以算法是:</p><blockquote class="nt nu nv"><p id="ef68" class="kv kw nn kx b ky kz jr la lb lc ju ld nw lf lg lh nx lj lk ll ny ln lo lp lq ij bi translated">初始化<strong class="kx ir"> β </strong>并设置可能性=0</p><p id="a2ba" class="kv kw nn kx b ky kz jr la lb lc ju ld nw lf lg lh nx lj lk ll ny ln lo lp lq ij bi translated">While likelihood≤max(可能性){</p><p id="5eff" class="kv kw nn kx b ky kz jr la lb lc ju ld nw lf lg lh nx lj lk ll ny ln lo lp lq ij bi translated">计算logit(<strong class="kx ir"><em class="iq">p</em></strong>)=<em class="iq">x</em><strong class="kx ir">βᵀ</strong></p><p id="94ba" class="kv kw nn kx b ky kz jr la lb lc ju ld nw lf lg lh nx lj lk ll ny ln lo lp lq ij bi translated">计算<strong class="kx ir"><em class="iq">p</em></strong>= logistic(x<strong class="kx ir">βᵀ)</strong>= 1/(1+exp(-x<strong class="kx ir">βᵀ</strong>))</p><p id="eb64" class="kv kw nn kx b ky kz jr la lb lc ju ld nw lf lg lh nx lj lk ll ny ln lo lp lq ij bi translated">计算似然l(<strong class="kx ir">β)=</strong>ⁿ∏ᵢ₌₁<strong class="kx ir"><em class="iq">if else</em></strong>(<em class="iq">y</em>(<em class="iq">I</em>)= 1，<em class="iq"> p </em> ( <em class="iq"> i </em>)，(1- <em class="iq"> p </em> ( <em class="iq"> i </em>))</p><p id="4ee4" class="kv kw nn kx b ky kz jr la lb lc ju ld nw lf lg lh nx lj lk ll ny ln lo lp lq ij bi translated">计算一阶导数<em class="iq">∇</em>ll(β)= x<strong class="kx ir">ᵀ</strong>(y-p)</p><p id="51ef" class="kv kw nn kx b ky kz jr la lb lc ju ld nw lf lg lh nx lj lk ll ny ln lo lp lq ij bi translated">更新:<strong class="kx ir"> β(新)= β(旧)+<em class="iq">η</em></strong><em class="iq">∇</em>ll(β)</p><p id="f355" class="kv kw nn kx b ky kz jr la lb lc ju ld nw lf lg lh nx lj lk ll ny ln lo lp lq ij bi">}</p></blockquote><h2 id="3474" class="oa mn iq bd mo ob oc dn ms od oe dp mw le of og my li oh oi na lm oj ok nc ol bi translated"><strong class="ak">牛顿法</strong></h2><p id="80c9" class="pw-post-body-paragraph kv kw iq kx b ky ne jr la lb nf ju ld le ng lg lh li nh lk ll lm ni lo lp lq ij bi translated">在所有可用的优化器中，牛顿法是另一个强有力的候选方法。在早期的课程中，我们已经学习了牛顿法作为算法来逐步寻找凹/凸函数上的最大/最小点:</p><p id="3a50" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><a class="ae nj" href="https://en.wikipedia.org/wiki/Newton%27s_method_in_optimization" rel="noopener ugc nofollow" target="_blank"><strong class="kx ir"><em class="nn">x</em>ₙ₊₁<em class="nn">= x</em>ₙ+∇<em class="nn">f(x</em>ₙ<em class="nn">)。</em>∇∇<em class="nn">f⁻(x</em>ₙ<em class="nn">)</em></strong></a></p><p id="5a0b" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在我们的对数似然函数的上下文中，<a class="ae nj" href="https://en.wikipedia.org/wiki/Newton%27s_method_in_optimization" rel="noopener ugc nofollow" target="_blank"><strong class="kx ir">∇</strong><strong class="kx ir"><em class="nn">f(x</em>ₙ<em class="nn">)</em></strong>将被<strong class="kx ir"> LL(β) </strong>(即∇LL(β)) <strong class="kx ir"> </strong>和<strong class="kx ir">∇∇<em class="nn">f⁻(x</em>ₙ<em class="nn"/></strong>的梯度代替好吧，我就在这里蹦跶一下细节，不过你好奇的大脑应该参考一下</a><a class="ae nj" href="https://thelaziestprogrammer.com/sharrington/math-of-machine-learning/solving-logreg-newtons-method" rel="noopener ugc nofollow" target="_blank">这个</a>。因此，在这种情况下，更新参数的最终表达式由下式给出:</p><p id="e64f" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx ir"> βₙ₊₁ <em class="nn"> = </em> βₙ <em class="nn"> + H⁻。</em> ∇LL(β) </strong></p><p id="9147" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在逻辑回归的情况下，计算<strong class="kx ir"> H </strong>非常简单，因为:</p><p id="5f90" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><em class="nn">h =</em>∇∇ll(β)= ∇ⁿ∑ᵢ₌₁[<em class="nn">y</em>——σ(βᵀ<em class="nn">x</em>⁽ⁱ⁾)].<em class="nn"> x </em> ⁽ⁱ⁾</p><p id="b7ec" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">= ∇ⁿ∑ᵢ₌₁[<em class="nn">y</em>-<em class="nn">pᵢ</em>。<em class="nn"> x </em> ⁽ⁱ⁾</p><p id="cf63" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">=−ⁿ∑ᵢ₌₁<em class="nn">x</em>⁽ⁱ⁾(∇<em class="nn">pᵢ</em>)<em class="nn">=</em>−ⁿ∑ᵢ₌₁<em class="nn">x</em>⁽ⁱ⁾<em class="nn">pᵢ</em>(1<em class="nn">-pᵢ</em>)(<em class="nn">x</em>⁽ⁱ⁾)<strong class="kx ir">ˇ</strong></p><p id="8505" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">于是，<strong class="kx ir"> H= -XWXᵀ，</strong>其中w =(<em class="nn">p</em>*(1-<em class="nn">p</em>)<strong class="kx ir">ᵀ</strong>I</p><p id="e532" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">所以算法是:</p><blockquote class="nt nu nv"><p id="5234" class="kv kw nn kx b ky kz jr la lb lc ju ld nw lf lg lh nx lj lk ll ny ln lo lp lq ij bi translated">初始化<strong class="kx ir"> β </strong>并设置可能性=0</p><p id="bc76" class="kv kw nn kx b ky kz jr la lb lc ju ld nw lf lg lh nx lj lk ll ny ln lo lp lq ij bi translated">While likelihood≤max(可能性){</p><p id="fb80" class="kv kw nn kx b ky kz jr la lb lc ju ld nw lf lg lh nx lj lk ll ny ln lo lp lq ij bi translated">计算logit(<strong class="kx ir"><em class="iq">p</em></strong>)=<em class="iq">x</em><strong class="kx ir">βᵀ</strong></p><p id="b121" class="kv kw nn kx b ky kz jr la lb lc ju ld nw lf lg lh nx lj lk ll ny ln lo lp lq ij bi translated">计算<strong class="kx ir"><em class="iq">p</em></strong>= logistic(x<strong class="kx ir">βᵀ)</strong>= 1/(1+exp(-x<strong class="kx ir">βᵀ</strong>))</p><p id="d1fc" class="kv kw nn kx b ky kz jr la lb lc ju ld nw lf lg lh nx lj lk ll ny ln lo lp lq ij bi translated">计算可能性l(<strong class="kx ir">β)=</strong>ⁿ∏ᵢ₌₁<strong class="kx ir"><em class="iq">if else</em></strong>(<em class="iq">y</em>(<em class="iq">I</em>)= 1，<em class="iq"> p </em> ( <em class="iq"> i </em>)，(1- <em class="iq"> p </em> ( <em class="iq"> i </em>))</p><p id="16e2" class="kv kw nn kx b ky kz jr la lb lc ju ld nw lf lg lh nx lj lk ll ny ln lo lp lq ij bi translated">计算一阶导数<em class="iq">∇</em>ll(β)= x<strong class="kx ir">ᵀ</strong>(y-p)</p><p id="7480" class="kv kw nn kx b ky kz jr la lb lc ju ld nw lf lg lh nx lj lk ll ny ln lo lp lq ij bi translated">计算二阶导数海森H = -Xᵀ (P*(1-P))ᵀI X</p><p id="26ed" class="kv kw nn kx b ky kz jr la lb lc ju ld nw lf lg lh nx lj lk ll ny ln lo lp lq ij bi translated"><strong class="kx ir"> β(新)= β(旧)+ <em class="iq"> H⁻。∇ </em> LL(β) </strong></p><p id="153c" class="kv kw nn kx b ky kz jr la lb lc ju ld nw lf lg lh nx lj lk ll ny ln lo lp lq ij bi translated"><strong class="kx ir"> } </strong></p></blockquote></div><div class="ab cl mf mg hu mh" role="separator"><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk"/></div><div class="ij ik il im in"><h1 id="c08e" class="mm mn iq bd mo mp mq mr ms mt mu mv mw jw mx jx my jz mz ka na kc nb kd nc nd bi translated"><strong class="ak"> 3。刮擦代码</strong></h1><h2 id="68a8" class="oa mn iq bd mo ob oc dn ms od oe dp mw le of og my li oh oi na lm oj ok nc ol bi translated">数据</h2><p id="bfe1" class="pw-post-body-paragraph kv kw iq kx b ky ne jr la lb nf ju ld le ng lg lh li nh lk ll lm ni lo lp lq ij bi translated">我要用来训练和测试我的二元分类模型的数据集可以从<a class="ae nj" href="https://github.com/AbhibhavS/Modelling-the-Logistic-Regression/blob/main/fire.csv" rel="noopener ugc nofollow" target="_blank"> <strong class="kx ir">这里</strong> </a>下载。最初，该数据集是阿尔及利亚森林火灾数据集。你可以在这里查看数据集<a class="ae nj" href="https://archive.ics.uci.edu/ml/datasets/Algerian+Forest+Fires+Dataset++" rel="noopener ugc nofollow" target="_blank">的细节。</a></p><h2 id="d256" class="oa mn iq bd mo ob oc dn ms od oe dp mw le of og my li oh oi na lm oj ok nc ol bi translated">代码</h2><p id="aa8e" class="pw-post-body-paragraph kv kw iq kx b ky ne jr la lb nf ju ld le ng lg lh li nh lk ll lm ni lo lp lq ij bi translated">对于跳过整篇文章来研究代码的读者来说，我建议快速浏览一下第二部分，因为我已经给出了一个针对优化器的特定算法，我的代码将严格遵循这个顺序。</p><p id="c741" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx ir">训练功能</strong></p><pre class="kg kh ki kj gt op oq or os aw ot bi"><span id="0003" class="oa mn iq oq b gy ou ov l ow ox">setwd("C:/Users/Dell/Desktop")<br/>set.seed(111) #to generate the same results as mine</span><span id="ee99" class="oa mn iq oq b gy oy ov l ow ox">#-------------Training Function---------------------------------#</span><span id="b982" class="oa mn iq oq b gy oy ov l ow ox">logistic.train&lt;- function(train_data, method, lr, verbose){<br/>  <br/>  b0&lt;-rep(1, nrow(train_data))<br/>  x&lt;-as.matrix(cbind(b0, train_data[,1:(ncol(train_data)-1)]))<br/>  y&lt;- train_data[, ncol(train_data)]<br/>  <br/>  beta&lt;- as.matrix(rep(0.5,ncol(x))); likelihood&lt;-0; epoch&lt;-0 #initiate<br/>  <br/>  beta_all&lt;-NULL<br/>  beta_at&lt;-c(1,10,50,100,110,150,180,200,300,500,600,800,1000,<br/>             1500,2000,4000,5000,6000,10000) #checkpoints (the epochs at which I will record the betas)<br/>  <br/>  #-----------------------------Gradient Descent---------------------#<br/>  if(method=="Gradient"){<br/>    while( (likelihood &lt; 0.95) &amp; (epoch&lt;=35000)){<br/>      <br/>      logit&lt;-x%*%beta #Calculate logit(p) = xβᵀ<br/>      <br/>      p &lt;- 1/( 1+ exp(-(logit))) #Calculate P=logistic(Xβᵀ)= 1/(1+exp(-Xβᵀ))<br/>      <br/>      # Likelihood: L(x|beta) = P(Y=1|x,beta)*P(Y=0|x,beta)<br/>      likelihood&lt;-1<br/>      for(i in 1:length(p)){<br/>        likelihood &lt;- likelihood*(ifelse( y[i]==1, p[i], (1-p[i]))) #product of all the probability<br/>      }<br/>      <br/>      first_d&lt;-  t(x) %*% (y-p)#first derivative of the likelihood function<br/>      <br/>      beta &lt;- beta + lr*first_d #updating the parameters for a step toward maximization<br/>      <br/>      #to see inside the steps of learning (irrelevant to the main working algo)<br/>      if(verbose==T){<br/>        ifelse(epoch%%200==0, <br/>               print(paste0(epoch, "th Epoch", <br/>                            "---------Likelihood=", round(likelihood,4),<br/>                            "---------log-likelihood=", round(log(likelihood),4),<br/>                            collapse = "")), NA)}<br/>      <br/>      if(epoch %in% beta_at){beta_all&lt;-cbind(beta_all, beta)}<br/>      <br/>      epoch&lt;- epoch+1<br/>    }<br/>  }<br/>  <br/>  #--------------Newton second order diff method-------------#<br/>  <br/>  else if(method=="Newton"){<br/>    while((likelihood &lt; 0.95) &amp; (epoch&lt;=35000)){<br/>      <br/>      logit&lt;-x%*%beta #Calculate logit(p) = xβᵀ<br/>      p &lt;- 1/( 1+ exp(-(logit))) #Calculate P=logistic(Xβᵀ)= 1/(1+exp(-Xβᵀ))<br/>      <br/>      # Likelihood: L(x|beta) = P(Y=1|x,beta)*P(Y=0|x,beta)<br/>      likelihood&lt;-1<br/>      for(i in 1:length(p)){<br/>        likelihood &lt;- likelihood*(ifelse( y[i]==1, p[i], (1-p[i])))<br/>      }<br/>      <br/>      first_d&lt;-  t(x) %*% (y-p)#first derivative of the likelihood function<br/>      <br/>      w&lt;-matrix(0, ncol= nrow(x), nrow = nrow(x)) #initializing p(1-p) diagonal matrix<br/>      diag(w)&lt;-p*(1-p)<br/>      hessian&lt;- -t(x) %*% w %*% x #hessian matrix<br/>      <br/>      hessian&lt;- diag(ncol(x))-hessian #Levenberg-Marquardt method: Add a scaled identity matrix to avoid singularity issues<br/>      <br/>      k&lt;- solve(hessian) %*% (t(x) %*% (y-p)) #the gradient for newton method<br/>      <br/>      beta &lt;- beta + k #updating the parameters for a step toward maximization</span><span id="61ac" class="oa mn iq oq b gy oy ov l ow ox">      if(verbose==T){<br/>        ifelse(epoch%%200==0, <br/>               print(paste0(epoch, "th Epoch", <br/>                            "---------Likelihood=", round(likelihood,4),<br/>                            "---------log-likelihood=", round(log(likelihood),4),<br/>                            collapse = "")), NA)}<br/>      <br/>      if(epoch %in% beta_at){beta_all&lt;-cbind(beta_all, beta)} #just to inside the learning<br/>      epoch&lt;- epoch+1<br/>    }<br/>  }<br/>  <br/>  else(break) <br/>  <br/>  beta_all&lt;-cbind(beta_all, beta)<br/>  colnames(beta_all)&lt;-c(beta_at[1:(ncol(beta_all)-1)], epoch-1)<br/>  <br/>  mylist&lt;-list(as.matrix(beta), likelihood, beta_all)<br/>  names(mylist)&lt;- c("Beta", "likelihood", "Beta_all")<br/>  return(mylist)<br/>} # Fitting of logistic model</span></pre><p id="fd41" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx ir">预测功能</strong></p><pre class="kg kh ki kj gt op oq or os aw ot bi"><span id="2091" class="oa mn iq oq b gy ou ov l ow ox">logistic.pred&lt;-function(model, test_data){<br/>  <br/>  test_new&lt;- cbind( rep(1, nrow(test_data)), test_data[,-ncol(test_data)]) #adding 1 to fit the intercept<br/>  <br/>  beta&lt;-as.matrix(model$Beta) #extract the best suiting beta (the beta at final epoch)<br/>  beta_all&lt;-model$Beta_all #extract all the betas at different checkpoints<br/>  ll&lt;- model$likelihood #extract the highest likelihood obtained<br/>  <br/>  log_odd&lt;-cbind(as.matrix(test_new)) %*% beta #logit(p)<br/>  <br/>  probability&lt;- 1/(1+ exp(-log_odd)) # p=logistic(logit(p))<br/>  predicted_label&lt;- ifelse(probability &gt;= 0.5, 1, 0) #discrimination rule<br/>  <br/>  k&lt;-cbind(test_data[,ncol(test_data)], predicted_label) # actual label vs predicted label<br/>  colnames(k)&lt;- c("Actual", "Predicted")<br/>  k&lt;- as.data.frame(k)<br/>  <br/>  tp&lt;-length(which(k$Actual==1 &amp; k$Predicted==1)) #true positive<br/>  tn&lt;-length(which(k$Actual==0 &amp; k$Predicted==0)) #true negative<br/>  fp&lt;-length(which(k$Actual==0 &amp; k$Predicted==1)) #false positive<br/>  fn&lt;-length(which(k$Actual==1 &amp; k$Predicted==0)) #false negative<br/>  <br/>  cf&lt;-matrix(c(tp, fn, fp, tn), 2, 2, byrow = F) #confusion matrix<br/>  rownames(cf)&lt;- c("1", "0")<br/>  colnames(cf)&lt;- c("1", "0")<br/>  <br/>  p_list&lt;-list(k, cf, beta, ll, beta_all)<br/>  names(p_list)&lt;- c("predticted", "confusion matrix","beta", "liklihood", "Beta_all")<br/>  return(p_list)<br/>  <br/>} # to make prediction from the trained model</span></pre><p id="3b6c" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx ir">数据解析</strong></p><pre class="kg kh ki kj gt op oq or os aw ot bi"><span id="09cf" class="oa mn iq oq b gy ou ov l ow ox">#importing data<br/>data&lt;-read.csv("fire.csv", header = T) #import<br/>data$Classes&lt;-as.numeric(ifelse(1:nrow(data)%in%grep("not",data$Classes), 0, 1)) # one hot encoding ; numeric conversion from label to 1 or 0<br/>data&lt;-rbind(data[which(data$Classes==0),],<br/>            data[sample(size=length(which(data$Classes==0)),which(data$Classes==1)),]) #balancing the classes<br/>data&lt;-data[sample(1:nrow(data)),] #shuffling<br/>data&lt;-as.data.frame(data) #data to data frame<br/>data&lt;-lapply(data, as.numeric) <br/>data&lt;-as.data.frame(data)</span><span id="1edc" class="oa mn iq oq b gy oy ov l ow ox">#missing data handling<br/>if(!is.null(is.na(data))){<br/>  data&lt;-data[-unique(which(is.na(data), arr.ind = T)[,1]),]<br/>}</span><span id="753a" class="oa mn iq oq b gy oy ov l ow ox">#test train partition<br/>partition&lt;-sample(c(0,1), size=nrow(data), prob = c(0.8,0.2), replace = T) <br/>train&lt;-data[which(partition==0),]<br/>test&lt;-data[which(partition==1),]</span></pre><p id="63f4" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx ir">培训→测试→结果</strong></p><pre class="kg kh ki kj gt op oq or os aw ot bi"><span id="7b4b" class="oa mn iq oq b gy ou ov l ow ox">#-------------------------TRAINING---------------------------------#</span><span id="aee1" class="oa mn iq oq b gy oy ov l ow ox">mymodel_newton&lt;- logistic.train(train, "Newton", 0.01, verbose=T) # Fitting the model using Newton method<br/>mymodel_gradient&lt;- logistic.train(train, "Gradient", 0.01, verbose=T) # Fitting the model using Gradient method</span><span id="edf8" class="oa mn iq oq b gy oy ov l ow ox">#------------------------TESTING-------------------------------------#<br/>myresult1&lt;-logistic.pred( mymodel_newton, test_data = test) #prediction using Newton trained model<br/>myresult2&lt;-logistic.pred( mymodel_gradient, test_data = test) #prediction using Gradient trained model</span><span id="a202" class="oa mn iq oq b gy oy ov l ow ox">#------------------------Results----------------------------------#<br/>myresult1$`confusion matrix`<br/>myresult2$`confusion matrix`</span></pre><p id="0ce4" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx ir">结果</strong></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oz"><img src="../Images/1a3e43d65b1723c6b456b95e6634ca9a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1196/format:webp/1*CiEmRencnM9a4VVsgSWDVQ.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">(图片由作者提供)</p></figure><p id="4723" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">两种方法得到的混淆矩阵是相同的。两种方法在独立测试集上获得的准确率都是95.2%(相当不错！！).</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi pa"><img src="../Images/5b53f5bdd86d82aadbce5942894b2d40.png" data-original-src="https://miro.medium.com/v2/resize:fit:1172/format:webp/1*NTKpAVmE14Nl9_uJL6c5bg.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">(图片由作者提供)</p></figure><p id="1e7d" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">然而，两种方法得到的最佳拟合系数<strong class="kx ir"> β </strong>在数值上有很大不同。牛顿的方法需要3566个时期来获得1的可能性，而梯度下降需要3539个时期来读取0.999的最大可能性。</p><p id="71c9" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">就所花费的时间而言，与梯度法相比，牛顿法花费更多的时间来达到最优，因为在牛顿法中，求解Hessian的逆使其成为计算量有点大且耗时的算法。</p><p id="2c22" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">正如你所注意到的，我在训练期间在不同的检查站捕捉到了贝塔。这将允许我们窥视训练过程，同时最大限度地提高可能性(见<strong class="kx ir"> ⇩⇩⇩⇩) </strong></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi pb"><img src="../Images/921e339786189a5eaec57e81080e2c2a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*W3NBkq3-2bXBt2Ij0qO6eQ.gif"/></div></div></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi pb"><img src="../Images/7b64dad655c77418e8372e7d84710904.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*O8zLPSx7yDb1KR8M9oxTNA.gif"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">(图片由作者提供)</p></figure><p id="05b6" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我们还可以通过完全训练好的模型来查看测试集上的拟合情况</p><div class="kg kh ki kj gt ab cb"><figure class="pc kk pd pe pf pg ph paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><img src="../Images/83247c41b3454b3876490e21a60c68f1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*DhafxA-s5lO_Om1Hd9hqtQ.png"/></div></figure><figure class="pc kk pd pe pf pg ph paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><img src="../Images/32de69314a05aa7979f072be74012ea0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*oFCJicBQpqsHW10cdi9_Yg.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk pi di pj pk translated">(图片由作者提供)</p></figure></div><p id="3e6d" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">所以在这一点上，我想我可以向读者重申，逻辑回归的基本性质不是分类，而是回归。通过证实回归的核心功能，逻辑回归将输出作为一个给定实例的概率。当采用大于或小于0.5的规则时，将一个实例分配给一个特定的离散类。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi pl"><img src="../Images/4c40e9171d139823f4ca9da187ed3f83.png" data-original-src="https://miro.medium.com/v2/resize:fit:996/1*IzbaZgvgS9CvN2VrGyb7Ag.gif"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><a class="ae nj" href="https://www.google.com/url?sa=i&amp;url=https%3A%2F%2Fwifflegif.com%2Ftags%2F54160-no-country-for-old-men-gifs&amp;psig=AOvVaw1ZC4zx-ZW-eetLPQTVQaSj&amp;ust=1617271993947000&amp;source=images&amp;cd=vfe&amp;ved=0CAIQjRxqFwoTCID2-Oel2u8CFQAAAAAdAAAAABAJ" rel="noopener ugc nofollow" target="_blank"> gif </a></p></figure><h1 id="f3d6" class="mm mn iq bd mo mp pm mr ms mt pn mv mw jw po jx my jz pp ka na kc pq kd nc nd bi translated">结论</h1><p id="4bff" class="pw-post-body-paragraph kv kw iq kx b ky ne jr la lb nf ju ld le ng lg lh li nh lk ll lm ni lo lp lq ij bi translated">如果你在这里，那就去好好犒劳一下自己吧，你是一个真正的MVP。我希望我对逻辑回归的随意阐述能让你对逻辑回归有更好的理解。本文包括逻辑回归的概念、基础数学和编程。虽然这里的想法描述了实际的方案，但这里讨论的优化器有一些超出范围的方面，其中优化算法可能无法实现最优，更多细节可以在这里找到<a class="ae nj" href="https://web.stanford.edu/~hastie/Papers/ESLII.pdf" rel="noopener ugc nofollow" target="_blank">。</a></p><p id="2ece" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">请随意从我的<a class="ae nj" href="https://github.com/AbhibhavS/Modelling-the-Logistic-Regression" rel="noopener ugc nofollow" target="_blank"> git </a>下载整个代码(模型和图)。</p></div><div class="ab cl mf mg hu mh" role="separator"><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk"/></div><div class="ij ik il im in"><h2 id="cc2b" class="oa mn iq bd mo ob oc dn ms od oe dp mw le of og my li oh oi na lm oj ok nc ol bi translated">一些阅读材料和资料来源:</h2><blockquote class="nt nu nv"><p id="2496" class="kv kw nn kx b ky kz jr la lb lc ju ld nw lf lg lh nx lj lk ll ny ln lo lp lq ij bi translated"><a class="ae nj" href="https://web.stanford.edu/~hastie/Papers/ESLII.pdf" rel="noopener ugc nofollow" target="_blank"> <strong class="kx ir">统计学习的要素</strong> <em class="iq">数据挖掘、推断和预测</em> </a></p></blockquote><div class="pr ps gp gr pt pu"><a href="https://thelaziestprogrammer.com/sharrington/math-of-machine-learning/solving-logreg-newtons-method" rel="noopener  ugc nofollow" target="_blank"><div class="pv ab fo"><div class="pw ab px cl cj py"><h2 class="bd ir gy z fp pz fr fs qa fu fw ip bi translated">用牛顿法求解Logistic回归</h2><div class="qb l"><h3 class="bd b gy z fp pz fr fs qa fu fw dk translated">在这篇文章中，我们介绍牛顿法，以及如何用它来解决逻辑回归。逻辑回归…</h3></div><div class="qc l"><p class="bd b dl z fp pz fr fs qa fu fw dk translated">thelaziestprogrammer.com</p></div></div><div class="qd l"><div class="qe l qf qg qh qd qi kp pu"/></div></div></a></div><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="qj qk l"/></div></figure><p id="6405" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><em class="nn">还有，随意探索一下</em> <a class="ae nj" rel="noopener" target="_blank" href="/a-laymans-guide-to-building-your-first-image-classification-model-in-r-using-keras-b285deac6572?source=your_stories_page-------------------------------------"> <em class="nn">第一部分</em> </a> <em class="nn">和</em> <a class="ae nj" rel="noopener" target="_blank" href="/understanding-a-neural-network-through-scratch-coding-in-r-a-novice-guide-a81e5777274b?source=your_stories_page-------------------------------------"> <em class="nn">第二部分</em> </a> <em class="nn">的动手香草造型系列。欢呼:)</em></p></div></div>    
</body>
</html>