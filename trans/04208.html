<html>
<head>
<title>Linear Discriminant Analysis</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">线性判别分析</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/linear-discriminant-analysis-1894bbf04359?source=collection_archive---------21-----------------------#2021-04-09">https://towardsdatascience.com/linear-discriminant-analysis-1894bbf04359?source=collection_archive---------21-----------------------#2021-04-09</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="2a6c" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">线性鉴别分析(LDA)理论和Python实现简介</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/982b981b73eaff363212e49fa0280498.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Mk1yYZO4qb88diDnGpV28Q.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">线性判别分析模型中决策边界的图示。图片作者。</p></figure><h1 id="13af" class="ky kz it bd la lb lc ld le lf lg lh li jz lj ka lk kc ll kd lm kf ln kg lo lp bi translated">内容</h1><p id="615e" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">这篇文章是我将要发表的一系列文章的一部分。你可以通过点击<a class="ae mm" href="https://cookieblues.github.io/guides/2021/04/01/bsmalea-notes-3b/" rel="noopener ugc nofollow" target="_blank">这里</a>在我的个人博客上阅读这篇文章的更详细版本。下面你可以看到该系列的概述。</p><h2 id="3f4b" class="mn kz it bd la mo mp dn le mq mr dp li lz ms mt lk md mu mv lm mh mw mx lo my bi translated">1.机器学习导论</h2><ul class=""><li id="a49d" class="mz na it ls b lt lu lw lx lz nb md nc mh nd ml ne nf ng nh bi translated"><a class="ae mm" rel="noopener" target="_blank" href="/what-is-machine-learning-91040db474f9">(一)什么是机器学习？</a></li><li id="eefd" class="mz na it ls b lt ni lw nj lz nk md nl mh nm ml ne nf ng nh bi translated"><a class="ae mm" rel="noopener" target="_blank" href="/model-selection-in-machine-learning-813fe2e63ec6"> (b)机器学习中的模型选择</a></li><li id="5f81" class="mz na it ls b lt ni lw nj lz nk md nl mh nm ml ne nf ng nh bi translated"><a class="ae mm" rel="noopener" target="_blank" href="/the-curse-of-dimensionality-5673118fe6d2"> (c)维度的诅咒</a></li><li id="9e53" class="mz na it ls b lt ni lw nj lz nk md nl mh nm ml ne nf ng nh bi translated"><a class="ae mm" rel="noopener" target="_blank" href="/what-is-bayesian-inference-4eda9f9e20a6"> (d)什么是贝叶斯推理？</a></li></ul><h2 id="5ba2" class="mn kz it bd la mo mp dn le mq mr dp li lz ms mt lk md mu mv lm mh mw mx lo my bi translated">2.回归</h2><ul class=""><li id="b095" class="mz na it ls b lt lu lw lx lz nb md nc mh nd ml ne nf ng nh bi translated"><a class="ae mm" rel="noopener" target="_blank" href="/how-linear-regression-actually-works-theory-and-implementation-8d8dcae3222c"> (a)线性回归的实际工作原理</a></li><li id="cd74" class="mz na it ls b lt ni lw nj lz nk md nl mh nm ml ne nf ng nh bi translated"><a class="ae mm" rel="noopener" target="_blank" href="/how-to-improve-your-linear-regression-with-basis-functions-and-regularization-8a6fcebdc11c"> (b)如何使用基函数和正则化改进您的线性回归</a></li></ul><h2 id="2ced" class="mn kz it bd la mo mp dn le mq mr dp li lz ms mt lk md mu mv lm mh mw mx lo my bi translated">3.分类</h2><ul class=""><li id="d2b8" class="mz na it ls b lt lu lw lx lz nb md nc mh nd ml ne nf ng nh bi translated"><a class="ae mm" rel="noopener" target="_blank" href="/overview-of-classifiers-d0a0d3eecfd1"> (a)分类器概述</a></li><li id="220f" class="mz na it ls b lt ni lw nj lz nk md nl mh nm ml ne nf ng nh bi translated"><a class="ae mm" rel="noopener" target="_blank" href="/quadratic-discriminant-analysis-ae55d8a8148a"> (b)二次判别分析(QDA) </a></li><li id="65eb" class="mz na it ls b lt ni lw nj lz nk md nl mh nm ml ne nf ng nh bi translated"><strong class="ls iu"> (c)线性判别分析</strong></li><li id="8410" class="mz na it ls b lt ni lw nj lz nk md nl mh nm ml ne nf ng nh bi translated"><a class="ae mm" rel="noopener" target="_blank" href="/gaussian-naive-bayes-4d2895d139a"> (d)(高斯)朴素贝叶斯</a></li></ul><h1 id="40c5" class="ky kz it bd la lb lc ld le lf lg lh li jz lj ka lk kc ll kd lm kf ln kg lo lp bi translated">设置和目标</h1><p id="7d07" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">在<a class="ae mm" rel="noopener" target="_blank" href="/quadratic-discriminant-analysis-ae55d8a8148a">之前的帖子</a>中，我们复习了二次判别分析(QDA)。对于线性判别分析(LDA)来说，很多理论是相同的，我们将在这篇文章中讨论。</p><p id="c444" class="pw-post-body-paragraph lq lr it ls b lt nn ju lv lw no jx ly lz np mb mc md nq mf mg mh nr mj mk ml im bi translated">线性鉴别分析(LDA)和二次鉴别分析(QDA)之间的唯一区别在于<strong class="ls iu"> LDA没有特定于类别的协方差矩阵，而是在类别之间有一个共享的协方差矩阵</strong>。</p><p id="32e8" class="pw-post-body-paragraph lq lr it ls b lt nn ju lv lw no jx ly lz np mb mc md nq mf mg mh nr mj mk ml im bi translated">再次强调，需要注意的是，LDA属于一类叫做<strong class="ls iu">高斯判别分析(GDA) </strong>模型的模型。这些是<em class="ns">生成型</em>模型(不是<em class="ns">区别型</em>模型),尽管它们的名字如此。</p><p id="1900" class="pw-post-body-paragraph lq lr it ls b lt nn ju lv lw no jx ly lz np mb mc md nq mf mg mh nr mj mk ml im bi translated">给定N个输入变量<strong class="ls iu"> x </strong>以及相应目标变量<em class="ns"> t </em>的训练数据集，LDA假设<strong class="ls iu">类条件密度</strong>是正态分布的</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/a0fb3f866d0c11e95995c68604859a6e.png" data-original-src="https://miro.medium.com/v2/resize:fit:602/1*EmDxO2g6IG1_FVkkrR4Uqg.gif"/></div></figure><p id="e1df" class="pw-post-body-paragraph lq lr it ls b lt nn ju lv lw no jx ly lz np mb mc md nq mf mg mh nr mj mk ml im bi translated">其中<strong class="ls iu"> <em class="ns"> μ </em> </strong>是<strong class="ls iu">类特定均值向量</strong>，而<strong class="ls iu">σ</strong>是<strong class="ls iu">共享协方差矩阵</strong>。利用贝叶斯定理，我们现在可以计算后验概率</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/6f5d6aa396d0e62706e067b7244fb1cb.png" data-original-src="https://miro.medium.com/v2/resize:fit:936/1*6J4ntwakLVbdDOIo82uTIQ.gif"/></div></figure><p id="5631" class="pw-post-body-paragraph lq lr it ls b lt nn ju lv lw no jx ly lz np mb mc md nq mf mg mh nr mj mk ml im bi translated">然后我们将把<strong class="ls iu"> x </strong>分类</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nv"><img src="../Images/c7c868badaaf4622a583c27b6cbeb7a0.png" data-original-src="https://miro.medium.com/v2/resize:fit:564/1*Gfs8zRy8Z6ni4Y3MDmXLpA.gif"/></div></figure><h1 id="20de" class="ky kz it bd la lb lc ld le lf lg lh li jz lj ka lk kc ll kd lm kf ln kg lo lp bi translated">衍生和培训</h1><p id="c875" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">我们用与QDA相同的方法导出对数似然。可以在<a class="ae mm" rel="noopener" target="_blank" href="/quadratic-discriminant-analysis-ae55d8a8148a">上一篇</a>中找到推导。我们得到的对数可能性是</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nw"><img src="../Images/85cfd7e6327b62d0c0d9b36b49ea1039.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hHjgojppcA-BaCVUugfBfw.png"/></div></div></figure><p id="4c32" class="pw-post-body-paragraph lq lr it ls b lt nn ju lv lw no jx ly lz np mb mc md nq mf mg mh nr mj mk ml im bi translated">查看(9)，我们可以看到，QDA和LDA之间的类特定先验和均值没有差异。我们在上一篇文章中推导出了它们</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/e1709c59041784de99515e098b28553c.png" data-original-src="https://miro.medium.com/v2/resize:fit:138/1*a4Ic75CCtOGcBaMdP4RIng.gif"/></div></figure><p id="4e73" class="pw-post-body-paragraph lq lr it ls b lt nn ju lv lw no jx ly lz np mb mc md nq mf mg mh nr mj mk ml im bi translated">和</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/05862fcddac109a23d4301556aed134c.png" data-original-src="https://miro.medium.com/v2/resize:fit:314/1*-uFwxJu0tt1nOU9NmT4ixw.gif"/></div></figure><p id="18cc" class="pw-post-body-paragraph lq lr it ls b lt nn ju lv lw no jx ly lz np mb mc md nq mf mg mh nr mj mk ml im bi translated">然而，共享协方差矩阵明显不同——对共享协方差矩阵求(1)的导数，并将其设置为0，得到</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nz"><img src="../Images/5fd39905e931019a7d969e368c595c84.png" data-original-src="https://miro.medium.com/v2/resize:fit:1022/1*26161xBvZ7VSe2EKT9gwAQ.gif"/></div></figure><p id="e36c" class="pw-post-body-paragraph lq lr it ls b lt nn ju lv lw no jx ly lz np mb mc md nq mf mg mh nr mj mk ml im bi translated">使用与上一篇文章相同的矩阵微积分性质，我们可以计算导数。你可以在我的个人博客<a class="ae mm" href="https://cookieblues.github.io/guides/2021/04/01/bsmalea-notes-3b/" rel="noopener ugc nofollow" target="_blank">这里</a>找到我更详细的帖子。我们得到了</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oa"><img src="../Images/ee69fa6e96879086656137ba639ec39b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*h6AsGnSsPwZgxjfwK_x7aQ.png"/></div></div></figure><p id="639c" class="pw-post-body-paragraph lq lr it ls b lt nn ju lv lw no jx ly lz np mb mc md nq mf mg mh nr mj mk ml im bi translated">我们发现<strong class="ls iu">共享协方差矩阵就是所有输入变量的协方差</strong>。因此，我们可以使用以下内容进行分类</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ob"><img src="../Images/7c28ac4694c7e2356de9da2d0afb2959.png" data-original-src="https://miro.medium.com/v2/resize:fit:1088/1*NkzLfJOe6J3kILLV645KaQ.gif"/></div></figure><h1 id="8977" class="ky kz it bd la lb lc ld le lf lg lh li jz lj ka lk kc ll kd lm kf ln kg lo lp bi translated">Python实现</h1><p id="e6cb" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">下面的代码是我们刚刚讨论过的LDA的简单实现。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oc od l"/></div></figure><p id="0c63" class="pw-post-body-paragraph lq lr it ls b lt nn ju lv lw no jx ly lz np mb mc md nq mf mg mh nr mj mk ml im bi translated">下面是一个图表，其中包含数据点(颜色编码以匹配其各自的类别)、我们的LDA模型找到的类别分布，以及由各自的类别分布生成的决策边界。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oe"><img src="../Images/79c0d03810c97ece7cc502c037a8ea3a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CZM0mlKHRtZeHvrN2U4YsQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">数据点的图表，其各自的类别用颜色编码，由我们的LDA模型发现的类别分布，以及从类别分布得到的决策边界。图片作者。</p></figure><p id="e313" class="pw-post-body-paragraph lq lr it ls b lt nn ju lv lw no jx ly lz np mb mc md nq mf mg mh nr mj mk ml im bi translated">正如我们所看到的，LDA具有更严格的决策边界，因为它要求类分布具有相同的协方差矩阵。</p><h1 id="1d08" class="ky kz it bd la lb lc ld le lf lg lh li jz lj ka lk kc ll kd lm kf ln kg lo lp bi translated">摘要</h1><ul class=""><li id="73b6" class="mz na it ls b lt lu lw lx lz nb md nc mh nd ml ne nf ng nh bi translated">线性<strong class="ls iu">判别</strong>分析(LDA)是一个<strong class="ls iu">生成式</strong>模型。</li><li id="c658" class="mz na it ls b lt ni lw nj lz nk md nl mh nm ml ne nf ng nh bi translated">LDA假设<strong class="ls iu">每个类都遵循高斯分布</strong>。</li><li id="c2f5" class="mz na it ls b lt ni lw nj lz nk md nl mh nm ml ne nf ng nh bi translated">QDA和LDA之间的唯一区别是<strong class="ls iu"> LDA为类假设一个共享协方差矩阵，而不是特定于类的协方差矩阵</strong>。</li><li id="713c" class="mz na it ls b lt ni lw nj lz nk md nl mh nm ml ne nf ng nh bi translated"><strong class="ls iu">共享协方差矩阵</strong>就是所有输入变量的<strong class="ls iu">协方差。</strong></li></ul></div></div>    
</body>
</html>