<html>
<head>
<title>Support Vector Machines In Under 5 Minutes</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">支持向量机不到5分钟</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/support-vector-machines-in-under-5-minutes-3074762a49bf?source=collection_archive---------22-----------------------#2021-08-14">https://towardsdatascience.com/support-vector-machines-in-under-5-minutes-3074762a49bf?source=collection_archive---------22-----------------------#2021-08-14</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><figure class="ip iq gp gr ir is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi io"><img src="../Images/d762a45646aafb75e05d005789937c5f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*ZjJmfYVVUa9Q_E9S"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">坦纳·博瑞克在<a class="ae jd" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><div class=""/><div class=""><h2 id="a41a" class="pw-subtitle-paragraph kd jf jg bd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku dk translated">关键概念简介</h2></div><h1 id="1213" class="kv kw jg bd kx ky kz la lb lc ld le lf km lg kn lh kp li kq lj ks lk kt ll lm bi translated">介绍</h1><p id="0f81" class="pw-post-body-paragraph ln lo jg lp b lq lr kh ls lt lu kk lv lw lx ly lz ma mb mc md me mf mg mh mi ij bi translated">在本文中，我将通过一些直观的例子向您介绍支持向量机(SVM)的概念，尽可能将技术细节放在一边。</p><h1 id="43ea" class="kv kw jg bd kx ky kz la lb lc ld le lf km lg kn lh kp li kq lj ks lk kt ll lm bi translated">分类示例</h1><h2 id="b447" class="mj kw jg bd kx mk ml dn lb mm mn dp lf lw mo mp lh ma mq mr lj me ms mt ll mu bi translated">线性情况</h2><p id="77bf" class="pw-post-body-paragraph ln lo jg lp b lq lr kh ls lt lu kk lv lw lx ly lz ma mb mc md me mf mg mh mi ij bi translated">假设我们有以下两类数据点集—蓝色方块和橙色圆圈:</p><figure class="mw mx my mz gt is gh gi paragraph-image"><div class="gh gi mv"><img src="../Images/4e6450b4fb84c69ff132111a8703fbc7.png" data-original-src="https://miro.medium.com/v2/resize:fit:706/format:webp/1*Lc9M1sOQon0Jm6uawKVMWQ.png"/></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">图1</p></figure><p id="501f" class="pw-post-body-paragraph ln lo jg lp b lq na kh ls lt nb kk lv lw nc ly lz ma nd mc md me ne mg mh mi ij bi translated">现在我们想确定划分数据的最佳决策边界。决策边界将位于上述两类数据之间，并且可以采取以下形式:</p><figure class="mw mx my mz gt is gh gi paragraph-image"><div class="gh gi mv"><img src="../Images/949e35691acb1a009630cc2545ebb9b3.png" data-original-src="https://miro.medium.com/v2/resize:fit:706/format:webp/1*WmF084vjeP5TuPw0nXz46A.png"/></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">图2</p></figure><p id="36cf" class="pw-post-body-paragraph ln lo jg lp b lq na kh ls lt nb kk lv lw nc ly lz ma nd mc md me ne mg mh mi ij bi translated">或者:</p><figure class="mw mx my mz gt is gh gi paragraph-image"><div class="gh gi mv"><img src="../Images/d136a8ddc40678dc685f4d39f592fe6e.png" data-original-src="https://miro.medium.com/v2/resize:fit:706/format:webp/1*aPMhCkh4Nm0xauhkgCF0IA.png"/></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">图3</p></figure><p id="84d9" class="pw-post-body-paragraph ln lo jg lp b lq na kh ls lt nb kk lv lw nc ly lz ma nd mc md me ne mg mh mi ij bi translated">或者甚至:</p><figure class="mw mx my mz gt is gh gi paragraph-image"><div class="gh gi mv"><img src="../Images/51352ff51fa16537b9670a3557b4d6f7.png" data-original-src="https://miro.medium.com/v2/resize:fit:706/format:webp/1*XBMMSLQJuXbu431frDBOkQ.png"/></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">图4</p></figure><p id="b572" class="pw-post-body-paragraph ln lo jg lp b lq na kh ls lt nb kk lv lw nc ly lz ma nd mc md me ne mg mh mi ij bi translated">哪一个可能是最佳的呢？请注意，当引入新数据时，没有优化的决策边界可能会导致大量的错误分类。</p><p id="ead4" class="pw-post-body-paragraph ln lo jg lp b lq na kh ls lt nb kk lv lw nc ly lz ma nd mc md me ne mg mh mi ij bi translated">让我们看看上面的最后一个例子。假设我们在两个类别之间定义任意分隔线:</p><figure class="mw mx my mz gt is gh gi paragraph-image"><div class="gh gi nf"><img src="../Images/b529b729be940f6aa672ed98fb817b9c.png" data-original-src="https://miro.medium.com/v2/resize:fit:708/format:webp/1*_HTfeWpzJNrA0OVkca46XQ.png"/></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">图5</p></figure><p id="2482" class="pw-post-body-paragraph ln lo jg lp b lq na kh ls lt nb kk lv lw nc ly lz ma nd mc md me ne mg mh mi ij bi translated">位于绿色分隔线上的点称为“支持向量”。它们是差值向上推的数据点，或者是最接近其他类的数据点。换句话说，我们只关心算法中的支持向量，其他所有训练数据点都不是主要关心的。准确地说，我们希望SVM算法能够查看接近另一类的极端数据点，而不是边缘。最大边缘是支持向量到决策边界超平面的最近距离的总和。请注意，对于SVM的情况，红线被称为超平面，因为SVM可以处理多维数据，其中数据点被称为向量，因为它们在多维数据空间中具有坐标。</p><p id="5d31" class="pw-post-body-paragraph ln lo jg lp b lq na kh ls lt nb kk lv lw nc ly lz ma nd mc md me ne mg mh mi ij bi translated">但是，当数据点不像上面的例子那样是线性可分的(即非线性的)时，会发生什么呢？这时候非线性SVM就出现了。</p><h2 id="f9b8" class="mj kw jg bd kx mk ml dn lb mm mn dp lf lw mo mp lh ma mq mr lj me ms mt ll mu bi translated">非线性情况</h2><p id="b4fe" class="pw-post-body-paragraph ln lo jg lp b lq lr kh ls lt lu kk lv lw lx ly lz ma mb mc md me mf mg mh mi ij bi translated">考虑下面的例子，其中数据点具有某种二次(非线性)模式:</p><figure class="mw mx my mz gt is gh gi paragraph-image"><div class="gh gi ng"><img src="../Images/4394b4fc7e14f9fc504bce4bb8ecf280.png" data-original-src="https://miro.medium.com/v2/resize:fit:746/format:webp/1*m8fqDRiVsEWEGh7vgVy1FA.png"/></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">图6</p></figure><p id="9a9b" class="pw-post-body-paragraph ln lo jg lp b lq na kh ls lt nb kk lv lw nc ly lz ma nd mc md me ne mg mh mi ij bi translated">为了获得数据的线性映射，我们可以对超平面(红线)进行变换，从而得到上面的1维空间。超平面现在是一条抛物线。</p><p id="f10c" class="pw-post-body-paragraph ln lo jg lp b lq na kh ls lt nb kk lv lw nc ly lz ma nd mc md me ne mg mh mi ij bi translated">但是，此类转换的计算开销可能很大，并且可能需要很长时间才能运行，尤其是对于大型数据集。幸运的是，有一种技术叫做“<strong class="lp jh">内核技巧</strong>”。这种方法的命名源于核函数的使用，该核函数允许在高维向量空间中操作，而不需要计算空间中数据点的坐标。相反，他们计算特征空间中向量的点积。使用这个“技巧”会导致所有点通过某种变换被映射到一个更高维度的空间。该方法的要点是将非线性空间转换成线性空间，仅此而已！</p><p id="713e" class="pw-post-body-paragraph ln lo jg lp b lq na kh ls lt nb kk lv lw nc ly lz ma nd mc md me ne mg mh mi ij bi translated">一些流行的核函数包括线性核、多项式核和径向基函数(RBF)核。支持向量机内核技巧的一个缺点是选择正确的内核并不简单，而且必须进行一些超参数调整。幸运的是，Scikit-learn等流行的Python库已经实现了SVM和超参数调整模块，用于直观的集成。关于Python实现的大量文档可以在Scikit-learn的<a class="ae jd" href="https://scikit-learn.org/stable/modules/svm.html#svm" rel="noopener ugc nofollow" target="_blank">这里</a>找到，关于SVM的<a class="ae jd" href="https://scikit-learn.org/stable/modules/grid_search.html" rel="noopener ugc nofollow" target="_blank">这里</a>找到，关于超参数调优的找到。</p><h1 id="c945" class="kv kw jg bd kx ky kz la lb lc ld le lf km lg kn lh kp li kq lj ks lk kt ll lm bi translated">回归案例呢？</h1><p id="76ca" class="pw-post-body-paragraph ln lo jg lp b lq lr kh ls lt lu kk lv lw lx ly lz ma mb mc md me mf mg mh mi ij bi translated">要使用SWMs执行回归，需要做的只是颠倒目标，而不是尝试在分类中适应最大可能的裕度(图5 ),对于回归，我们尝试在裕度定义的空间上适应尽可能多的实例:</p><figure class="mw mx my mz gt is gh gi paragraph-image"><div class="gh gi nf"><img src="../Images/851fc588a081731f0ed4086b4b316e3c.png" data-original-src="https://miro.medium.com/v2/resize:fit:708/format:webp/1*OKueH7grGFJt-SUMAtqTeQ.png"/></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">图7</p></figure><h1 id="be8d" class="kv kw jg bd kx ky kz la lb lc ld le lf km lg kn lh kp li kq lj ks lk kt ll lm bi translated">下一步是什么</h1><p id="942e" class="pw-post-body-paragraph ln lo jg lp b lq lr kh ls lt lu kk lv lw lx ly lz ma mb mc md me mf mg mh mi ij bi translated">我将在以后的文章中介绍更多关于支持向量机的内容，陈述它们的优点和缺点，以及更多的实际例子。敬请关注！</p><h1 id="9772" class="kv kw jg bd kx ky kz la lb lc ld le lf km lg kn lh kp li kq lj ks lk kt ll lm bi translated">参考</h1><p id="5558" class="pw-post-body-paragraph ln lo jg lp b lq lr kh ls lt lu kk lv lw lx ly lz ma mb mc md me mf mg mh mi ij bi translated">[1]t . Hastie，r . TiB shirani和j . h . Friedman(2009年)。统计学习的要素:数据挖掘、推理和预测。第二版。纽约:斯普林格。</p><p id="6410" class="pw-post-body-paragraph ln lo jg lp b lq na kh ls lt nb kk lv lw nc ly lz ma nd mc md me ne mg mh mi ij bi translated">[2]超参数调整方法。检索自:<a class="ae jd" href="https://scikit-learn.org/stable/modules/grid_search.html" rel="noopener ugc nofollow" target="_blank">https://scikit-learn.org/stable/modules/grid_search.html</a>。</p><p id="9394" class="pw-post-body-paragraph ln lo jg lp b lq na kh ls lt nb kk lv lw nc ly lz ma nd mc md me ne mg mh mi ij bi translated">[3]sci kit-learn中的支持向量机。检索自:<a class="ae jd" href="https://scikit-learn.org/stable/modules/svm.html#svm" rel="noopener ugc nofollow" target="_blank">https://scikit-learn.org/stable/modules/svm.html#svm</a>。</p><h1 id="bb8b" class="kv kw jg bd kx ky kz la lb lc ld le lf km lg kn lh kp li kq lj ks lk kt ll lm bi translated">评论</h1><ul class=""><li id="4bf6" class="ni nj jg lp b lq lr lt lu lw nk ma nl me nm mi nn no np nq bi translated">图1到图7是我自己手绘的，就不用引用了。</li></ul></div></div>    
</body>
</html>