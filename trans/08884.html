<html>
<head>
<title>Distributed Deep Learning — Illustrated</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">分布式深度学习—图解</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/distributed-deep-learning-illustrated-6256e07a0468?source=collection_archive---------28-----------------------#2021-08-16">https://towardsdatascience.com/distributed-deep-learning-illustrated-6256e07a0468?source=collection_archive---------28-----------------------#2021-08-16</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="5357" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated"><strong class="ak">了解如何使用多个GPU执行深度学习</strong></h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/5cdb1bb5c9bc3712bdb56489bd4f4264.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4_dnrD3a9GsVXNu65-aCiA.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">照片:<a class="ae kv" href="https://pixabay.com/" rel="noopener ugc nofollow" target="_blank"> pixabay </a></p></figure><h2 id="2e6a" class="kw kx iq bd ky kz la dn lb lc ld dp le lf lg lh li lj lk ll lm ln lo lp lq lr bi translated">分布式深度学习可视化！</h2><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ls"><img src="../Images/37641ddb3d65576ec42f4d2f1d22fc84.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*K720o-MZlKyUi_KdlPkCGg.gif"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">请继续阅读！(图片由作者提供)</p></figure><p id="47dd" class="pw-post-body-paragraph lt lu iq lv b lw lx jr ly lz ma ju mb lf mc md me lj mf mg mh ln mi mj mk ml ij bi translated">在这篇文章中，我将说明分布式深度学习是如何工作的。我已经创建了动画，应该可以帮助您对分布式深度学习有一个高层次的理解。但是让我们从基础开始。</p><h1 id="3180" class="mm kx iq bd ky mn mo mp lb mq mr ms le jw mt jx li jz mu ka lm kc mv kd lq mw bi translated">GPU如何加速深度学习？</h1><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mx"><img src="../Images/694e1a528ac9585f31d69792b34abedf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*J7ESXjIcNz4VGlfQ0_BZLQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">GPU如何加速执行(图片由作者提供)</p></figure><p id="f665" class="pw-post-body-paragraph lt lu iq lv b lw lx jr ly lz ma ju mb lf mc md me lj mf mg mh ln mi mj mk ml ij bi translated">图形处理单元(GPU)是能够执行多个同步数学计算的专用内核。深度学习计算可以分解为一系列矩阵乘法，这就是GPU优于CPU的地方。当代码被执行时，这些数学运算被卸载到GPU，而代码的其余部分在CPU上运行。由于深度学习本质上是对这些数学例程的迭代，我们通过使用GPU获得了巨大的加速。</p><h1 id="640f" class="mm kx iq bd ky mn mo mp lb mq mr ms le jw mt jx li jz mu ka lm kc mv kd lq mw bi translated">分布式深度学习</h1><p id="71fd" class="pw-post-body-paragraph lt lu iq lv b lw my jr ly lz mz ju mb lf na md me lj nb mg mh ln nc mj mk ml ij bi translated">当我们想要使用多个GPU来加速我们的模型训练过程时，就会使用分布式深度学习。分布式深度学习主要有两种类型——模型并行和数据并行。<strong class="lv ir">模型并行</strong>用于当您的模型有太多的层而无法放入单个GPU时，因此不同的层在不同的GPU上训练。<strong class="lv ir">数据并行</strong>，两者中较为常见的一种，通过将数据分割到不同的分区来实现加速。该模型使用不同的数据分区在多个GPU上同时运行。</p><p id="af33" class="pw-post-body-paragraph lt lu iq lv b lw lx jr ly lz ma ju mb lf mc md me lj mf mg mh ln mi mj mk ml ij bi translated">下面的动画展示了使用多个GPU的分布式深度学习。每个阶段调用的函数显示在动画的右侧。这些功能(原语)解释如下。</p><ul class=""><li id="0950" class="nd ne iq lv b lw lx lz ma lf nf lj ng ln nh ml ni nj nk nl bi translated"><strong class="lv ir">复制</strong>:在多个GPU上复制一个模型</li><li id="952f" class="nd ne iq lv b lw nm lz nn lf no lj np ln nq ml ni nj nk nl bi translated"><strong class="lv ir">分散</strong>:将输入分配给多个GPU</li><li id="7610" class="nd ne iq lv b lw nm lz nn lf no lj np ln nq ml ni nj nk nl bi translated"><strong class="lv ir">采集</strong>:采集所有GPU的输出，并发送给主GPU</li><li id="b275" class="nd ne iq lv b lw nm lz nn lf no lj np ln nq ml ni nj nk nl bi translated"><strong class="lv ir"> parallel_apply </strong>:在已经分布的输入上并行运行模型</li></ul><h2 id="7559" class="kw kx iq bd ky kz la dn lb lc ld dp le lf lg lh li lj lk ll lm ln lo lp lq lr bi translated">输入数据和模型分布</h2><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ls"><img src="../Images/3b37090bcd42e0e102853a6369d754e9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*dsO-RZnomPtliwFlTN9FBg.gif"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">(图片由作者提供)</p></figure><ul class=""><li id="5593" class="nd ne iq lv b lw lx lz ma lf nf lj ng ln nh ml ni nj nk nl bi translated">输入数据批被分成与GPU数量相同的分区。</li><li id="0479" class="nd ne iq lv b lw nm lz nn lf no lj np ln nq ml ni nj nk nl bi translated">主GPU (GPU0)上的模型权重被复制到所有GPU</li></ul><h2 id="550a" class="kw kx iq bd ky kz la dn lb lc ld dp le lf lg lh li lj lk ll lm ln lo lp lq lr bi translated">输出和损耗计算</h2><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ls"><img src="../Images/87fc8db7e9755c21a9fef028f4aac101.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*QIiWDcvqaSHBpL2wTfgzmQ.gif"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">(图片由作者提供)</p></figure><ul class=""><li id="24b1" class="nd ne iq lv b lw lx lz ma lf nf lj ng ln nh ml ni nj nk nl bi translated">该模型同时运行在所有GPU各自的数据分区上</li><li id="06b2" class="nd ne iq lv b lw nm lz nn lf no lj np ln nq ml ni nj nk nl bi translated">输出被收集并发送到GPU0，然后GPU 0计算损耗</li><li id="b434" class="nd ne iq lv b lw nm lz nn lf no lj np ln nq ml ni nj nk nl bi translated">丢失被发送到所有GPU</li></ul><h2 id="51bd" class="kw kx iq bd ky kz la dn lb lc ld dp le lf lg lh li lj lk ll lm ln lo lp lq lr bi translated">梯度计算和模型更新</h2><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ls"><img src="../Images/37641ddb3d65576ec42f4d2f1d22fc84.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*K720o-MZlKyUi_KdlPkCGg.gif"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">(图片由作者提供)</p></figure><ul class=""><li id="0b04" class="nd ne iq lv b lw lx lz ma lf nf lj ng ln nh ml ni nj nk nl bi translated">每个GPU在其各自的数据分区上计算梯度</li><li id="ca61" class="nd ne iq lv b lw nm lz nn lf no lj np ln nq ml ni nj nk nl bi translated">梯度被收集并发送到GPU0</li><li id="4d9b" class="nd ne iq lv b lw nm lz nn lf no lj np ln nq ml ni nj nk nl bi translated">GPU0使用平均梯度来计算更新的模型</li><li id="43cd" class="nd ne iq lv b lw nm lz nn lf no lj np ln nq ml ni nj nk nl bi translated">来自GPU0的更新模型被发送到所有GPU，它们将更新模型用于下一次迭代</li><li id="3f1d" class="nd ne iq lv b lw nm lz nn lf no lj np ln nq ml ni nj nk nl bi translated">整个过程再次从头开始迭代</li></ul><p id="97e2" class="pw-post-body-paragraph lt lu iq lv b lw lx jr ly lz ma ju mb lf mc md me lj mf mg mh ln mi mj mk ml ij bi translated">我希望现在你已经对分布式深度学习如何在多个GPU上工作有了直观的理解。我使用了PyTorch深度学习框架，但其基本原则仍然是一样的。使用PyTorch框架时的示例代码如下所示。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nr"><img src="../Images/a51fb6dacfbc15f6a53ab6100393dcb7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CAo0RgoN70toUj3AUtbiHw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">在多个GPU上运行模型的PyTorch代码</p></figure><h2 id="2314" class="kw kx iq bd ky kz la dn lb lc ld dp le lf lg lh li lj lk ll lm ln lo lp lq lr bi translated">数据并行模式的缺点</h2><p id="db1b" class="pw-post-body-paragraph lt lu iq lv b lw my jr ly lz mz ju mb lf na md me lj nb mg mh ln nc mj mk ml ij bi translated">如果你分析上面解释的PyTorch中的数据并行工作流，你可以看到设计中存在一定的局限性。GPU0充当主GPU，它比其他GPU做更多的工作。这意味着当GPU0在做额外的工作时，其他GPU处于闲置状态。一个更好的设计是确保所有的GPU都做同样多的工作，以便它们得到充分利用。</p><p id="fe5f" class="pw-post-body-paragraph lt lu iq lv b lw lx jr ly lz ma ju mb lf mc md me lj mf mg mh ln mi mj mk ml ij bi translated">PyTorch DataParallel模式的另一个限制是，它使用单个进程多线程，这种模式存在著名的python全局解释器锁(GIL)问题。数据并行的优点是所需的编码最少。如果您准备编写更多的代码，您应该在PyTorch中使用DistributedDataParallel，它使用多处理。</p><h2 id="dc57" class="kw kx iq bd ky kz la dn lb lc ld dp le lf lg lh li lj lk ll lm ln lo lp lq lr bi translated">未来的工作</h2><p id="eca2" class="pw-post-body-paragraph lt lu iq lv b lw my jr ly lz mz ju mb lf na md me lj nb mg mh ln nc mj mk ml ij bi translated">我计划涵盖其他高级分布式深度学习方法，如分布式数据并行、Ring All-reduce等。在以后的文章中。理解分布式深度学习是如何工作的，对于构建健壮的大规模机器学习解决方案至关重要。我希望这一系列文章能够帮助您朝着这个方向迈出一步。</p></div></div>    
</body>
</html>