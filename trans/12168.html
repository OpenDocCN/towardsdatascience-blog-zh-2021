<html>
<head>
<title>Stay updated with Neuroscience: November 2021 Must-Reads</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">与神经科学保持同步:2021年11月必读</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/stay-updated-with-neuroscience-november-2021-must-reads-e2fba4692b21?source=collection_archive---------29-----------------------#2021-12-08">https://towardsdatascience.com/stay-updated-with-neuroscience-november-2021-must-reads-e2fba4692b21?source=collection_archive---------29-----------------------#2021-12-08</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><p id="7c9b" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我们能雇用甘斯吗💻在神经科学实验中🐁？几何流形是什么💎我们的记忆存在于？我们能进行生物启发的独立成分分析吗📈？</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ko"><img src="../Images/fb50fc40a14077e60e3717ea2150d501.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*apxwcUQExjqnx2psYoqong.jpeg"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">杰弗里·布鲁姆在<a class="ae le" href="https://unsplash.com/photos/7-gaPkhIgqs" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的图片</p></figure><div class="lf lg gp gr lh li"><a href="https://medium.com/@stefanobosisio1/membership" rel="noopener follow" target="_blank"><div class="lj ab fo"><div class="lk ab ll cl cj lm"><h2 class="bd iu gy z fp ln fr fs lo fu fw is bi translated">通过我的推荐链接加入Medium-Stefano Bosisio</h2><div class="lp l"><h3 class="bd b gy z fp ln fr fs lo fu fw dk translated">作为一个媒体会员，你的会员费的一部分会给你阅读的作家，你可以完全接触到每一个故事…</h3></div><div class="lq l"><p class="bd b dl z fp ln fr fs lo fu fw dk translated">medium.com</p></div></div><div class="lr l"><div class="ls l lt lu lv lr lw ky li"/></div></div></a></div><p id="dc57" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">你为什么应该关心神经科学？</strong></p><p id="4744" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">神经科学是当今人工智能🧠的根源🤖。阅读并意识到神经科学中的进化和新见解不仅会让你成为一个更好的“人工智能”的家伙😎而且还是一个更好的神经网络体系结构的创造者👩‍💻！</p><p id="76f5" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">本月3篇惊险论文！来自爱丁堡大学的科学家们第一次使用了生成对抗性神经网络，从而拓展了神经科学分析的边界。对小鼠进行了实验，以研究学习前和学习后阶段之间的神经元动力学变化。第二，斯坦福大学、普林斯顿大学、哥伦比亚大学和高级研究所合作，利用递归神经网络研究了记忆几何流形应该是什么样子。最后，罗格斯大学的研究人员展示了生物启发的神经网络独立成分分析的实现及其对神经形态计算的影响。</p><p id="76b5" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">使用循环一致的对抗网络进行神经元学习分析<br/> </strong> <em class="lx">布莱恩·m·李、奥克利托斯·阿姆夫罗西迪斯、娜塔莉·l·罗什福尔、阿诺·翁肯、</em> <a class="ae le" href="https://arxiv.org/abs/2111.13073" rel="noopener ugc nofollow" target="_blank"> <strong class="js iu">论文</strong> </a></p><p id="a370" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">神经科学的核心研究领域旨在了解学习过程中发生的神经元重塑动力学。从神经实验中提取可解释的信号并进行有意义的分析现在已经成为进一步理解神经元动力学的优先要求。许多出版物显示了使用标准技术(如TCA的五氯苯甲醚)得出的显著结果，但是，这些标准技术具有线性响应和映射的基本假设。在这篇论文中，爱丁堡大学的研究人员将分析扩展到生成对抗性神经网络，特别是，他们采用朱的CycleGAN来学习神经元活动的学习前和学习后之间的映射。</p><p id="7ed9" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">CycleGAN可以学习两种概率分布，在这种情况下是学习前和学习后的神经元动力学，并将一种转换为另一种。这一过程使我们能够更好地理解哪些可能是学习过程中出现的关键特征，以及从神经元获得即时反应模式。</p><p id="010d" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在这个实验中，我们用一只头部固定的老鼠研究了学习前后的动力学，这只老鼠被放在一个可以前后移动的线性跑步机上。在鼠标前面，有一个带有明确光栅图案的监视器。如果鼠标能够在跑步机上前进120-140厘米，显示器将切换为黑色模式。在这里，如果老鼠在虚拟奖励区内舔了一下，就会得到奖励(水滴)。这项活动迫使老鼠学会利用屏幕上的视觉信息和自身运动来获得最大的回报。初级视觉皮层神经元用钙指示剂标记，并监测4天，随时间测量相对荧光。</p><p id="0527" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">第一天是学习前，第四天是学习后，所以从这几天开始，我们会分别得到学习前和学习后的概率分布。GAN的生成器和鉴别器必须以数据驱动的方式识别与动物实验相关的模式。图1显示了GAN鉴别器和生成器的主要结果。前两个顶部图显示了分别计算为平均注意力掩模的学习前和学习后鉴别器注意力图。学习前鉴别器专注于虚拟动物位置上100-130厘米之间的特定神经元组，这与奖励区一致。学习后鉴别器突出了两组神经元，总是在100-130厘米左右。同样，对于发电机来说——两个底部的图。特别是，预学习生成器(左下方)专注于流程开始和结束时的活动(奖励区)。后学习生成器在奖励区之前给予更多的关注。这些结果表明，要了解从学习后到学习前反应的转变，最重要的特征是在老鼠到达奖励区时获得的。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ly"><img src="../Images/133d012e36823ede901d028d9fe3fb89.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZuRKjy26jdfQq_BxhGHxLw.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">图1:来自CycleGAN模型的注意力地图，是老鼠在跑步机上每跑一段距离的平均值。上图:学习前(左)和学习后(右)辨别者的注意力地图。下图:学习前(左)和学习后(右)生成者的注意力地图。虚线表示奖励区。</p></figure><p id="03bb" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">总之，本文首次将GAN的方法应用于活体神经元实验。作者能够通过GAN生成的潜在特征来可视化学习过程，强调奖励区周围的活动在整个动力学中具有高度影响力。未来的研究必须加强记录过程，利用垂直和水平空间信息来丰富模型的特征。</p><p id="df73" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">连续变量工作记忆的递归神经网络模型:活动流形、连接模式、动态代码<br/> </strong> <em class="lx">克里斯托夫·j·库埃瓦、阿德尔·阿达兰、米沙·佐戴克斯、钱宁、</em> <a class="ae le" href="https://arxiv.org/abs/2111.01275" rel="noopener ugc nofollow" target="_blank"> <strong class="js iu">论文</strong> </a></p><p id="21db" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我们的记忆能够以适当维度的连续结构存储信息。例如，考虑两个视觉刺激，它们在我们视觉区域的相同位置依次出现。刺激是一样的，唯一不同的是方向，比如一个刺激指向左边，另一个指向右边。我们的大脑如何将两个刺激识别为两个不同的刺激，并避免第二个刺激在记忆中覆盖第一个刺激？从数学和计算的角度来看，这是一个奇妙的问题。为了模拟大脑动力学，我们需要找到一种在模型中存储记忆的方法，利用一些几何特性。在这篇论文中，麻省理工学院、普林斯顿大学、哥伦比亚大学和IAS的研究人员通过递归神经网络(RNNs)研究了这个问题。rnn用于存储两个不同方向的连续闪光，并研究网络的连接模式。RNN之所以被选中，是因为他们能把刺激保持在记忆中。</p><p id="ae82" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">图2显示了实验装置。延时后，两个输入信号按顺序给出。除了方向之外，信号是相同的。用于网络模式研究的RNN由100个全连接单元组成。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi lz"><img src="../Images/252f5a5e7ecb800ce5c4aa11b268d62e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XaKUTdavVyGwml84ptnsWA.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">图2:实验设置。(a)训练RNN记住两条线的方向，给定两条线之间的延迟时间。(b)RNN建筑:100个循环连接的单元。输入是32个方向调谐的信号线。输出由方向角的余弦和正弦表示</p></figure><p id="726d" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">主要结果如图3所示。首先，作者将主成分分析应用于一段时间内循环单位的活动，以了解低维结构。当两种刺激都作为输入出现时，主成分图显示Clifford torus流形的出现。一个标准的环面流形——就像一个甜甜圈形状的物体——当它有两个不同方向的两个信号的两个宿主时就会变形。相反，Clifford环面可以同等地表示这两种信号。在正交性检查中给出了进一步的证明(如图3所示)。如果我们在两个环面上运行正交性和平行性测试，我们可以看到标准环面如何不对称地处理角度，而Clifford环面可以对所有角度保持对称。在早期训练阶段的RNNs显示出与标准圆环相似的结果。在后学习中，RNN单元可以识别不同的方向，给出关于Clifford环面的正交性和平行性测试输出。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ma"><img src="../Images/1a6c956d3318d28d1cb0bb5deb4234b8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*szvh7FlbzMgE0UXbApHXqA.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">图3:标准环面和Clifford环面的区别。RNN在早期训练阶段展示了标准环面几何的典型行为。学习之后，由于克利福德流形，记忆得以保留。</p></figure><p id="62a1" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">此外，作者研究了所有100个单位的连接模式。出现了在第一延迟周期期间存储关于特定线路的信息的单元在第二延迟周期期间不总是继续存储关于同一线路的信息。特别是，有一个权重的调整和平衡，这可以防止内存被覆盖。</p><p id="2e69" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">结论是:</p><ul class=""><li id="2566" class="mb mc it js b jt ju jx jy kb md kf me kj mf kn mg mh mi mj bi translated">记忆可能存在于类似Clifford环面的几何流形上。这个环面可以保持信号的连续表示，其中任何信号都可以被区分，它是一个正交基。</li><li id="495d" class="mb mc it js b jt mk jx ml kb mm kf mn kj mo kn mg mh mi mj bi translated">在学习时，存储第一方位记忆的单元随时间改变它们的调谐，防止覆盖第一个存在的方位。</li><li id="fd81" class="mb mc it js b jt mk jx ml kb mm kf mn kj mo kn mg mh mi mj bi translated">这可能是拥有更多类脑网络和记忆系统的通用解决方案</li><li id="c410" class="mb mc it js b jt mk jx ml kb mm kf mn kj mo kn mg mh mi mj bi translated">这是对RNN如何存储多个连续变量的解释</li></ul><p id="6e71" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">一种用于独立成分分析的规范且生物学上可行的算法<br/> </strong> <em class="lx"> Yanis Bahroun，Dmitri B. Chkolvskii，Anirvan M. Sengupta，</em> <strong class="js iu">论文</strong></p><p id="d533" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">大脑是一个奇妙的机器，它可以很容易地将系统信号(如听觉、视觉和嗅觉系统)分开，从系统混合物中识别潜在的来源。这项任务被称为盲源分离(BSS)。在计算机科学中，通常用独立分量分析(ICA)来解决盲分离问题。ICA假设潜在的刺激是独立源的线性组合。尽管在文献中有成千上万种ICA风味，但是很少有ICA具有生物学启发的实现。对作者来说，生物启发意味着该算法可以1)以流的形式运行，而无需将数据集保留在内存中2)如果ICA是神经网络的一部分，则应该使用局部学习规则来更新突触权重。作者从四阶盲识别(FOBI)过程中获得灵感，探索了一种可能的生物启发ICA算法。</p><p id="fa13" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">ICA算法是以这样一种方式构造的，使得它可以在生物似然神经网络(NN-ICA)中实现。首先，神经网络输入有<em class="lx"> d </em>个神经元，是要分离成独立分量的输入数据。网络呈现输入和输出的树突之间的前馈突触，以及横向突触(图4)。实际上，输入信号首先乘以神经元的权重矩阵。输出投影然后在输出层的树枝状部分被白化。最后，躯体区域通过用局部学习规则平衡权重来计算最终输出。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi mp"><img src="../Images/23959e40ce6f8bb382af6958502d7d82.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xv5ixehwBz-toCwTGFnaTQ.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">图4:生物启发ICA的神经网络实现。输入信号由d个神经网络单元混合和接收。这里前馈突触分解输入信号。输出层由两个神经元组成，其权重遵循局部学习规则。输出的树枝状部分白化数据。身体空间重建了源头。蓝色阴影意味着输出活动被调节，以模仿大脑的可塑性。</p></figure><p id="19fa" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">图5显示了来自不同输入源的结果。图5A报告了合成数据，其中周期信号和随机噪声作为NN-ICA的输入给出。图5B报告了在16kHz下记录的真实世界语音信号的结果。图5C的结果来自自然场景图像，其中算法从混合图像中恢复所有元素。很明显，对于大范围的输入数据集，所提出的规则可靠地收敛到正确的解。此外，整个NN-ICA算法易于实现，除了扩展我们关于大脑如何能够执行BSS任务的知识之外，还可以进一步促进神经形态计算。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi mq"><img src="../Images/21ec2b8351cd15bc57d81c93212126d0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1HGtRa5FpK9FFE_OWxoDxA.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">图5:神经网络独立分量分析结果。a)来自周期信号混合的结果B)真实语音数据分解C)自然场景图像。</p></figure></div><div class="ab cl mr ms hx mt" role="separator"><span class="mu bw bk mv mw mx"/><span class="mu bw bk mv mw mx"/><span class="mu bw bk mv mw"/></div><div class="im in io ip iq"><p id="d314" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我希望你喜欢2021年11月神经科学<code class="fe my mz na nb b">arxivg.org</code>论文的这篇综述。请随时给我发电子邮件询问问题或评论，地址:stefanobosisio1@gmail.com</p></div></div>    
</body>
</html>