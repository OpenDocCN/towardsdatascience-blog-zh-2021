<html>
<head>
<title>Effortless NLP using pre-trained Hugging Face pipelines</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">轻松的自然语言处理使用预先训练的拥抱脸管道</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/effortless-nlp-using-pre-trained-hugging-face-pipelines-with-just-3-lines-of-code-a4788d95754f?source=collection_archive---------20-----------------------#2021-04-08">https://towardsdatascience.com/effortless-nlp-using-pre-trained-hugging-face-pipelines-with-just-3-lines-of-code-a4788d95754f?source=collection_archive---------20-----------------------#2021-04-08</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="5b9d" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">学习如何用3行代码进行自然语言处理</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/20e5e3d61b8dccbb6a15afb038a905bc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*YmBkOQydhGsrxpGc"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><a class="ae kv" href="https://unsplash.com/@tengyart?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">腾雅特</a>在<a class="ae kv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="3d7a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">最近，BERT模型在语言处理领域越来越受欢迎，因为它成功地将最先进的性能与有限的计算能力结合起来。在这篇文章中，我将向你展示如何使用拥抱脸变形金刚库，只用3行代码就可以使用这个模型！但首先，让我们看看伯特模型是如何工作的。</p></div><div class="ab cl ls lt hu lu" role="separator"><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx"/></div><div class="ij ik il im in"><h1 id="8feb" class="lz ma iq bd mb mc md me mf mg mh mi mj jw mk jx ml jz mm ka mn kc mo kd mp mq bi translated">伯特是什么？</h1><p id="db9b" class="pw-post-body-paragraph kw kx iq ky b kz mr jr lb lc ms ju le lf mt lh li lj mu ll lm ln mv lp lq lr ij bi translated">BERT代表来自变压器的双向编码器表示。它是一种最新的语言模型，能够在广泛的自然语言处理任务中获得前沿的结果。</p><p id="d8c9" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">BERT的主要优势之一是它是双向的，这意味着该模型可以一次考虑整个单词序列。与从左到右的方法相反，这允许BERT使用所有周围的单词(在左侧和右侧)来上下文化每个单词。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mw"><img src="../Images/c0e1df81d187be47e01dfa3d9bcdd149.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*871A7nJsIeeB8KgeJqt_Qg.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">伯特模型的两个阶段。图片来自<a class="ae kv" href="https://arxiv.org/abs/1810.04805" rel="noopener ugc nofollow" target="_blank">原创研究论文</a></p></figure><p id="ae5d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">此外，您可以使用计算能力有限的BERT模型，因为它利用了<strong class="ky ir">迁移学习</strong>:首先在一些一般任务上训练一个模型(<strong class="ky ir">预训练</strong>)，然后将获得的知识‘转移’到相关的NLP任务(<strong class="ky ir">微调</strong>)。让我们更详细地看看这两个步骤。</p><h2 id="a58e" class="mx ma iq bd mb my mz dn mf na nb dp mj lf nc nd ml lj ne nf mn ln ng nh mp ni bi translated">预培训</h2><p id="0542" class="pw-post-body-paragraph kw kx iq ky b kz mr jr lb lc ms ju le lf mt lh li lj mu ll lm ln mv lp lq lr ij bi translated">首先，该模型是在像维基百科这样的大型纯文本语料库上预先训练的。预培训应该是通用的，以便以后将该模型用于广泛的目标。其次，预训练是在自我监督的情况下完成的，因此不需要对输入进行标记，这反过来意味着我们拥有几乎无限的训练数据。BERT模型的预训练在两个任务上完成:</p><ul class=""><li id="0483" class="nj nk iq ky b kz la lc ld lf nl lj nm ln nn lr no np nq nr bi translated"><strong class="ky ir">掩蔽语言建模(MLM): </strong>对语料库中15%的词进行掩蔽，目标是预测掩蔽作品。例如，一个被屏蔽的句子可能是<code class="fe ns nt nu nv b">Paris is the [MASK] of France</code>，模型将尝试预测<code class="fe ns nt nu nv b">capital</code>。</li><li id="9098" class="nj nk iq ky b kz nw lc nx lf ny lj nz ln oa lr no np nq nr bi translated"><strong class="ky ir">下一句预测(NSP): </strong>从语料库中随机抽取两个句子进行组合。目标是预测这两个句子是否在原始语料库中相邻出现。例如，这两个句子可以是<code class="fe ns nt nu nv b">the man went to the store</code>和<code class="fe ns nt nu nv b">he bought a gallon of milk</code>，它们在逻辑上可以相互跟随。然而，句子也可以是<code class="fe ns nt nu nv b">the man went to the store</code>和<code class="fe ns nt nu nv b">penguins are flightless</code>，它们不太可能连续出现。</li></ul><p id="24aa" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这些任务的结合使伯特既能理解单词之间的关系，也能理解句子之间的关系。预训练只需要进行一次(节省计算能力)，并且预训练的模型在网上广泛可用，用于一种或多种语言，以及有大小写和无大小写的文本。</p><h2 id="02c4" class="mx ma iq bd mb my mz dn mf na nb dp mj lf nc nd ml lj ne nf mn ln ng nh mp ni bi translated">微调</h2><p id="9618" class="pw-post-body-paragraph kw kx iq ky b kz mr jr lb lc ms ju le lf mt lh li lj mu ll lm ln mv lp lq lr ij bi translated">但是，预先训练好的BERT模型还是很一般的。为了能够将其用于情感分析、命名实体识别、摘要、翻译或其他用途，我们需要针对我们的特定用例对模型进行微调。很大的好处是，这种微调相对便宜:大部分繁重的工作已经在预训练阶段完成，只需要做一次。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ob"><img src="../Images/1ed03708cae850e454d122567664a9ce.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*PJJJkDTtjvTD9_c_"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">谢尔盖·斯韦奇尼科夫在<a class="ae kv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="6304" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">如果你没有一个已标记的训练集，已经微调过的模型也可以在网上广泛获得，例如在<a class="ae kv" href="https://huggingface.co/models" rel="noopener ugc nofollow" target="_blank">拥抱脸模型中心</a>。这是我将在本文中使用的方法。</p><p id="5b13" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">关于伯特的更多信息，我会推荐<a class="ae kv" href="https://github.com/google-research/bert" rel="noopener ugc nofollow" target="_blank">github.com/google-research/bert</a>，或者为更高级的读者推荐原始研究论文“<a class="ae kv" href="https://arxiv.org/abs/1810.04805" rel="noopener ugc nofollow" target="_blank">伯特:语言理解的深度双向转换器的预训练</a></p></div><div class="ab cl ls lt hu lu" role="separator"><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx"/></div><div class="ij ik il im in"><h1 id="3507" class="lz ma iq bd mb mc md me mf mg mh mi mj jw mk jx ml jz mm ka mn kc mo kd mp mq bi translated">拥抱面部变形金刚</h1><p id="506d" class="pw-post-body-paragraph kw kx iq ky b kz mr jr lb lc ms ju le lf mt lh li lj mu ll lm ln mv lp lq lr ij bi translated">使用BERT模型最简单的方法之一是使用拥抱脸变形金刚:一个基于PyTorch和TensorFlow的最先进的NLP库。通过他们的<a class="ae kv" href="https://huggingface.co/models" rel="noopener ugc nofollow" target="_blank">模型中心</a>，Hugging Face目前提供超过7500个预训练模型，用于各种NLP任务和各种语言。这样，您几乎总能找到符合您特定目标的模型。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oc"><img src="../Images/7c505ffc7129d58719e4a6d27c3336e6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mLpIxEHkmonrpcSCS0PaOQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">看看拥抱脸模型库，其中包含超过7500个预先训练的模型。(图片由作者提供)</p></figure><p id="bc5b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这些模型中的每一个都可以使用拥抱脸变形金刚库提供的简单方法在您自己的数据集上进行微调。然而，更容易的是，模型也可以开箱即用，只需最少的编程，使用拥抱面部变形器为这11个任务提供的管道之一:</p><ul class=""><li id="3f60" class="nj nk iq ky b kz la lc ld lf nl lj nm ln nn lr no np nq nr bi translated">特征抽出</li><li id="366e" class="nj nk iq ky b kz nw lc nx lf ny lj nz ln oa lr no np nq nr bi translated">情感分析</li><li id="929e" class="nj nk iq ky b kz nw lc nx lf ny lj nz ln oa lr no np nq nr bi translated">命名实体识别</li><li id="29e1" class="nj nk iq ky b kz nw lc nx lf ny lj nz ln oa lr no np nq nr bi translated">问题回答</li><li id="8378" class="nj nk iq ky b kz nw lc nx lf ny lj nz ln oa lr no np nq nr bi translated">遮罩填充</li><li id="d585" class="nj nk iq ky b kz nw lc nx lf ny lj nz ln oa lr no np nq nr bi translated">摘要</li><li id="9527" class="nj nk iq ky b kz nw lc nx lf ny lj nz ln oa lr no np nq nr bi translated">翻译</li><li id="eb92" class="nj nk iq ky b kz nw lc nx lf ny lj nz ln oa lr no np nq nr bi translated">语言生成</li><li id="c647" class="nj nk iq ky b kz nw lc nx lf ny lj nz ln oa lr no np nq nr bi translated">文本到文本生成</li><li id="895b" class="nj nk iq ky b kz nw lc nx lf ny lj nz ln oa lr no np nq nr bi translated">零射击分类</li><li id="2813" class="nj nk iq ky b kz nw lc nx lf ny lj nz ln oa lr no np nq nr bi translated">多回合对话</li></ul><p id="97e4" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">更多信息，请看github.com/huggingface/transformers的<a class="ae kv" href="https://github.com/huggingface/transformers" rel="noopener ugc nofollow" target="_blank"/></p></div><div class="ab cl ls lt hu lu" role="separator"><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx"/></div><div class="ij ik il im in"><h1 id="25d5" class="lz ma iq bd mb mc md me mf mg mh mi mj jw mk jx ml jz mm ka mn kc mo kd mp mq bi translated">使用管道(只有3行代码！)</h1><p id="d02c" class="pw-post-body-paragraph kw kx iq ky b kz mr jr lb lc ms ju le lf mt lh li lj mu ll lm ln mv lp lq lr ij bi translated">确保你首先安装拥抱脸变形库，比如在你的终端上运行<code class="fe ns nt nu nv b">pip install transformers</code>。然后，您可以开始使用只有3行代码的拥抱脸模型！例如，请看下面的情感分析代码:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="od oe l"/></div></figure><p id="d182" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">看，那很简单！您所要做的就是导入库，初始化管道，然后您就可以开始使用这个模型了！如前所述，这些功能使用来自拥抱面部模型中枢的预训练模型。默认情况下，情感分析管道使用<a class="ae kv" href="https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english" rel="noopener ugc nofollow" target="_blank">distil Bert-base-un cased-fine tuned-SST-2-English</a>模型，但是您可以使用模型中心的任何模型。</p><p id="903e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们来看两个扩展:从模型中枢挑选不同的模型，解决不同的任务。</p><h2 id="2ad2" class="mx ma iq bd mb my mz dn mf na nb dp mj lf nc nd ml lj ne nf mn ln ng nh mp ni bi translated">使用模型中心的7500+模型之一</h2><p id="5877" class="pw-post-body-paragraph kw kx iq ky b kz mr jr lb lc ms ju le lf mt lh li lj mu ll lm ln mv lp lq lr ij bi translated">创建管线时，通过设置<code class="fe ns nt nu nv b">model</code>参数，可以很容易地使用不同于缺省模型的模型。例如，假设我们正在进行一个项目，想要预测财务状况。在model hub上快速搜索，我们会看到<a class="ae kv" href="https://huggingface.co/ProsusAI/finbert" rel="noopener ugc nofollow" target="_blank"> ProsusAI/finbert </a>模型，它专门针对金融内容的情感进行训练。这个模型的实现和前面的例子一样简单，只需包含<code class="fe ns nt nu nv b">model</code>参数:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="od oe l"/></div></figure><p id="f43d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">拥抱脸变形金刚会自动为你下载选中的型号！</p><h2 id="a976" class="mx ma iq bd mb my mz dn mf na nb dp mj lf nc nd ml lj ne nf mn ln ng nh mp ni bi translated">其他NLP任务</h2><p id="9d66" class="pw-post-body-paragraph kw kx iq ky b kz mr jr lb lc ms ju le lf mt lh li lj mu ll lm ln mv lp lq lr ij bi translated">管道目前能够处理11种不同的任务，从命名实体识别到翻译。创建管线时，可通过将<code class="fe ns nt nu nv b">'sentiment-analysis'</code>更改为其他名称来选择模型。例如，让我们试着翻译“我喜欢狗！”从英语到德语。转到模型中心，过滤任务“翻译”和语言“德”，您将看到100多个模型。我将使用<a class="ae kv" href="https://huggingface.co/t5-small" rel="noopener ugc nofollow" target="_blank">t5-小型</a>型号:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="od oe l"/></div></figure><p id="4629" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">就是这样！要获得管道可以完成的所有任务的完整列表，请看一下<a class="ae kv" href="https://huggingface.co/transformers/main_classes/pipelines.html#the-pipeline-abstraction" rel="noopener ugc nofollow" target="_blank">这个维基</a>页面。</p></div><div class="ab cl ls lt hu lu" role="separator"><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx"/></div><div class="ij ik il im in"><h1 id="4f98" class="lz ma iq bd mb mc md me mf mg mh mi mj jw mk jx ml jz mm ka mn kc mo kd mp mq bi translated">结论</h1><p id="90ce" class="pw-post-body-paragraph kw kx iq ky b kz mr jr lb lc ms ju le lf mt lh li lj mu ll lm ln mv lp lq lr ij bi translated">在本文中，您已经了解了BERT模型的工作原理及其训练方式。此外，你已经看到了拥抱脸变形金刚管道是多么强大和容易使用。<strong class="ky ir">的确，NLP对每个人都是可行的！</strong></p></div><div class="ab cl ls lt hu lu" role="separator"><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx"/></div><div class="ij ik il im in"><p id="efe7" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><em class="of">本文是作为荷兰鹿特丹伊拉兹马斯大学</em> <a class="ae kv" href="https://www.tstudents.nl/" rel="noopener ugc nofollow" target="_blank"> <em class="of">图灵机&amp;深度学习</em> </a> <em class="of">课程的一部分而撰写的。更多我的内容，可以看看我的中页或者GitHub简介:</em><a class="ae kv" href="https://github.com/RvMerle" rel="noopener ugc nofollow" target="_blank"><em class="of">RvMerle</em></a><em class="of">。</em></p></div></div>    
</body>
</html>