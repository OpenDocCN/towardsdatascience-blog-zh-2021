<html>
<head>
<title>A Beginner’s Guide to Collinearity: What it is and How it affects our regression model</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">共线性入门指南:什么是共线性以及它如何影响我们的回归模型</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/a-beginners-guide-to-collinearity-what-it-is-and-how-it-affects-our-regression-model-d442b421ff95?source=collection_archive---------13-----------------------#2021-10-01">https://towardsdatascience.com/a-beginners-guide-to-collinearity-what-it-is-and-how-it-affects-our-regression-model-d442b421ff95?source=collection_archive---------13-----------------------#2021-10-01</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="e862" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated"><em class="kf">什么是共线性？它如何影响我们的模型？我们该如何处理？</em></h2></div><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi kg"><img src="../Images/20f696750573ba68df8f4581927b1a75.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*RWAUx87gkfcE4io0"/></div></div><p class="ks kt gj gh gi ku kv bd b be z dk translated">作者在<a class="ae kw" href="https://canva.com/" rel="noopener ugc nofollow" target="_blank"> Canva </a>上创建的图片</p></figure><p id="0c00" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">当我们构建回归模型时，我们显然希望对因变量和一个或多个自变量之间的关系进行建模。然而，更多的时候，我们可能会遇到这样的情况，每个自变量的拟合系数“没有意义”，我们无法解释为什么会出现这种情况。如果您遇到这种情况，您的回归模型中可能存在共线性。</p><h1 id="3b8c" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated">什么是共线性？</h1><p id="8898" class="pw-post-body-paragraph kx ky iq kz b la ml jr lc ld mm ju lf lg mn li lj lk mo lm ln lo mp lq lr ls ij bi translated">出现共线性是因为我们用来建立回归模型的独立变量彼此相关。这是有问题的，因为顾名思义，自变量应该是独立的。它不应该和其他自变量有任何相关性。</p><p id="c2f5" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">如果自变量之间存在共线性，就违背了回归分析的关键点。在回归分析中，我们希望隔离每个自变量对因变量的影响。这样，我们可以将每个自变量的拟合系数解释为自变量每变化1个单位时因变量的平均变化，同时保持其他自变量不变。</p><p id="f132" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">现在如果我们有共线性，上面的关键点就不再成立，就好像我们改变一个自变量的值，相关的其他自变量也会改变。</p><p id="280d" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">在这篇文章中，我们将了解为什么共线性会成为我们回归模型的一个问题，我们如何检测它，它如何影响我们的模型，以及我们可以做些什么来消除共线性。</p><h1 id="c887" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated">共线性问题</h1><p id="9253" class="pw-post-body-paragraph kx ky iq kz b la ml jr lc ld mm ju lf lg mn li lj lk mo lm ln lo mp lq lr ls ij bi translated">共线性会在几个方面影响我们的模型，它们是:</p><ul class=""><li id="3e25" class="mq mr iq kz b la lb ld le lg ms lk mt lo mu ls mv mw mx my bi translated">独立变量的系数估计对模型的变化非常敏感，即使是微小的变化。假设我们想要删除或添加一个自变量，那么系数估计值将会大幅波动。这让我们很难理解每个自变量的影响。</li><li id="1aa1" class="mq mr iq kz b la mz ld na lg nb lk nc lo nd ls mv mw mx my bi translated">共线性会增大系数估计的方差和标准误差。这反过来会降低我们模型的可靠性，我们不应该相信我们模型显示的p值来判断一个独立变量对我们的模型是否具有统计显著性。</li></ul><p id="44d6" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">为了更清楚地说明为什么共线性是这样一个问题，让我们看一下下面的用例。</p><p id="b719" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">假设我们想预测一辆汽车的价格。为了预测它，我们有独立的变量，如汽车的城市英里数，公路英里数，马力，发动机大小，冲程，宽度，峰值转速和压缩比。接下来，我们建立一个回归模型，下面是我们的模型的统计摘要。</p><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi kg"><img src="../Images/88b1e0a5dbd4847c424078e2b5ac81a6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AzvpjqOyzEUXWOOPHhE3AA.png"/></div></div></figure><p id="c8ef" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">我们的模型实际上做得很好，因为它有84%的R。现在问题来了，当我们试图解释这个模型的时候。如果我们看一下系数估计，高速公路MPG (76.07)和城市MPG (-211.78)的符号相反。这完全没有意义，因为如果一辆汽车的高速公路MPG提高了价格，那么城市MPG也应该这样做。但事实并非如此。</p><p id="86b9" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">此外，模型显示公路MPG和城市MPG的p值不显著(&gt; 0.05)，表明我们可以将它们从回归模型中排除。但它们真的无足轻重吗？如果我们认为模型中可能存在轻微的共线性，我们就不应该马上相信这个p值。</p><h1 id="d8aa" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated">检测共线性</h1><p id="4be0" class="pw-post-body-paragraph kx ky iq kz b la ml jr lc ld mm ju lf lg mn li lj lk mo lm ln lo mp lq lr ls ij bi translated">有两种简单的方法来检测我们的回归模型中是否存在共线性。</p><h1 id="c29f" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated">相关矩阵</h1><p id="6f7c" class="pw-post-body-paragraph kx ky iq kz b la ml jr lc ld mm ju lf lg mn li lj lk mo lm ln lo mp lq lr ls ij bi translated">第一个是通过观察我们自变量的相关矩阵。经验法则是，如果两个独立变量的皮尔逊相关系数大于0.9，那么我们可以说这两个独立变量彼此高度相关，因此它们是共线的。这是我们用例的关联矩阵。</p><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi ne"><img src="../Images/4eb10415c80c14a8f5426618f953c2c7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WC4FQ3WV379AqCLR5qBqew.png"/></div></div><p class="ks kt gj gh gi ku kv bd b be z dk translated">作者创造的形象</p></figure><p id="3265" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">从上面的图像中，我们可以清楚地看到公路MPG和城市MPG高度相关，因为它们的皮尔逊相关系数为0.97。由于它们具有正相关性，这意味着如果我们增加高速公路MPG，城市MPG也将增加几乎相同的数量。</p><h1 id="ebba" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated">方差膨胀因子</h1><p id="757f" class="pw-post-body-paragraph kx ky iq kz b la ml jr lc ld mm ju lf lg mn li lj lk mo lm ln lo mp lq lr ls ij bi translated">方差膨胀因子或VIF衡量共线性对系数估计方差的影响。VIF可以用数学方法描述如下:</p><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi nf"><img src="../Images/91cc55ae84ad2674d22dafe0cb37657d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-y6MG9lWt8R9G6VeOXLTvw.png"/></div></div></figure><p id="faa4" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">从上式我们知道，如果自变量<strong class="kz ir"><em class="ng"/></strong><em class="ng"/>的<strong class="kz ir"> <em class="ng"> Ri </em> </strong>大或者接近1，那么<strong class="kz ir"><em class="ng"/></strong>对应的VIF也会大。这意味着自变量<strong class="kz ir"><em class="ng"/></strong>可以用其他自变量来解释或者换句话说，<strong class="kz ir"><em class="ng"/></strong>与其他自变量高度相关。因此，系数估计值<strong class="kz ir"> <em class="ng"> βi </em> </strong>的方差也很高。</p><p id="93b3" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">下面是我们用例中自变量的VIF:</p><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi nh"><img src="../Images/f0d0b39c6db31683604980a3107ab718.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GbTochr-xZ-ECe94JGY8zQ.png"/></div></div></figure><p id="b9b4" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">在我们决定我们的自变量的共线性是一个值得关注的原因之前，有很多关于什么是VIF的合适阈值的讨论，但是大多数研究论文都认为VIF高于10表明自变量之间存在严重的共线性。</p><p id="8621" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">在我们的使用案例中，我们可以看到高速公路MPG和城市MPG的VIF得分远高于10，表明它们彼此高度相关。从上面的相关矩阵中我们也可以看到这种现象。</p><p id="7635" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated"><em class="ng">查看我们的</em> <a class="ae kw" href="https://www.stratascratch.com/blog/a-comprehensive-statistics-cheat-sheet-for-data-science-interviews/?utm_source=blog&amp;utm_medium=click&amp;utm_campaign=medium" rel="noopener ugc nofollow" target="_blank"> <em class="ng">综合统计小抄</em> </a> <em class="ng">了解统计和概率的重要术语和方程</em></p><h1 id="14f4" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated">消除共线性</h1><p id="80e5" class="pw-post-body-paragraph kx ky iq kz b la ml jr lc ld mm ju lf lg mn li lj lk mo lm ln lo mp lq lr ls ij bi translated">既然我们知道独立变量中存在严重的共线性，我们需要找到一种方法来解决这个问题。有两种常见的方法可以消除共线性。</p><h1 id="60c1" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated">变量选择</h1><p id="f024" class="pw-post-body-paragraph kx ky iq kz b la ml jr lc ld mm ju lf lg mn li lj lk mo lm ln lo mp lq lr ls ij bi translated">这是消除共线性的最直接的解决方案，通常，领域知识对实现最佳解决方案非常有帮助。为了消除共线性，我们可以从回归模型中排除具有高VIF值的独立变量。让我们看一下我们的用例示例，为什么领域知识在这种情况下会有帮助:</p><ul class=""><li id="4d39" class="mq mr iq kz b la lb ld le lg ms lk mt lo mu ls mv mw mx my bi translated">我们知道公路MPG和城市MPG有很高的VIF值。如果我们有领域知识，我们知道没有必要从我们的回归模型中排除这两者。相反，我们只需要选择其中之一。假设我们从模型中排除了公路MPG。</li><li id="7b6f" class="mq mr iq kz b la mz ld na lg nb lk nc lo nd ls mv mw mx my bi translated">我们还知道马力和发动机尺寸也有相当高的VIF值。虽然它们测量的东西不同，但较高的发动机尺寸或发动机排量通常对应于较高的马力。因此，我们从模型中排除了马力。</li></ul><p id="8dc2" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">最后，我们再次建立了我们的回归模型，但这次没有公路MPG和马力。</p><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi ni"><img src="../Images/3e7ae51a91ab5c3c8b75a9dc341ae3ae.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*88LitdpFMhLk4NlHN0u0SA.png"/></div></div></figure><p id="e9bd" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">现在独立变量之间不再有严重的共线性。现在我们可以进行回归分析了。</p><h1 id="ebd4" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated">主成分分析</h1><p id="3891" class="pw-post-body-paragraph kx ky iq kz b la ml jr lc ld mm ju lf lg mn li lj lk mo lm ln lo mp lq lr ls ij bi translated">消除共线性的第二种方法是使用主成分分析或PCA，这是一种常用于降维的方法。这种方法将有利于消除共线性，因为主成分分析将我们的独立变量分解成一定数量的独立因素。</p><p id="e375" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">然而，用PCA消除共线性有一个很大的缺点。由于我们从主成分分析中得到的独立因素与我们最初的自变量完全不同，我们不再有自变量的同一性。这似乎有点违背直觉，因为我们去除共线性的主要原因是为了让我们更容易理解自变量对因变量的影响。</p><h1 id="7621" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated">回归模型中消除共线性的作用</h1><p id="cf85" class="pw-post-body-paragraph kx ky iq kz b la ml jr lc ld mm ju lf lg mn li lj lk mo lm ln lo mp lq lr ls ij bi translated">现在我们移除了自变量中的共线性，让我们比较有共线性和没有共线性的回归模型。</p><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi nj"><img src="../Images/850b11f5c5610378b2cca5a4e7c99999.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tOtrZUwt6-A0PE4MD34WlQ.png"/></div></div></figure><p id="3a7f" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">左侧是具有共线性的回归模型，右侧是通过变量选择移除共线性后的回归模型。</p><p id="f7dc" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">共线性的问题是，它会夸大系数估计的方差或标准误差。现在，如果我们看一下这两个模型，在没有共线性的回归模型中，系数估计的标准误差比有共线性的模型小得多。最明显的一个是城市MPG变量。当存在共线性时，该变量系数估计的标准误差为169.14，而当消除共线性时为60.30。</p><p id="353a" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">如果我们看一下p值，具有共线性的模型得出结论，城市MPG变量在统计上不显著，这意味着我们可以从模型中排除该变量以获得更好的性能。但是当我们去除共线性后，这个变量的p值是0.003，实际上是有统计学意义的。共线性会夸大独立变量系数估计的方差，使我们很难相信从模型中得到的p值。</p><p id="d3f1" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">此外，在充分选择变量的情况下，无共线性模型的F-统计量比有共线性模型显著得多，尽管有共线性模型的自变量更多。</p><h1 id="2358" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated">我们需要消除共线性吗？</h1><p id="0663" class="pw-post-body-paragraph kx ky iq kz b la ml jr lc ld mm ju lf lg mn li lj lk mo lm ln lo mp lq lr ls ij bi translated">我们应该注意的一件重要事情是，共线性不会影响模型预测或模型的准确性。如果你看看上面有和没有共线性的模型的R比较，两者都差不多。事实上，具有共线性的模型通常具有更高的准确性，因为它通常具有更多的独立变量。</p><p id="d12e" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">共线性仅影响系数估计的方差和p值。它影响模型的可解释性，而不是模型的预测能力。</p><p id="f1db" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">所以，如果你想建立一个回归模型来进行预测，并且不需要了解每个自变量的影响，那么你就不需要去除模型中的共线性。</p><p id="58e4" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">但是，如果模型解释对您很重要，并且您需要了解每个独立变量对模型预测的影响，那么删除模型中的共线性是必要的。</p></div><div class="ab cl nk nl hu nm" role="separator"><span class="nn bw bk no np nq"/><span class="nn bw bk no np nq"/><span class="nn bw bk no np"/></div><div class="ij ik il im in"><p id="0c59" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated"><em class="ng">最初发表于</em><a class="ae kw" href="https://www.stratascratch.com/blog/a-beginner-s-guide-to-collinearity-what-it-is-and-how-it-affects-our-regression-model/?utm_source=blog&amp;utm_medium=click&amp;utm_campaign=medium" rel="noopener ugc nofollow" target="_blank"><em class="ng">【https://www.stratascratch.com】</em></a><em class="ng">。</em></p></div></div>    
</body>
</html>