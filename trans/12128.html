<html>
<head>
<title>Each convolution kernel is a classifier! Do you truly understand how Convolutional Neural Networks work?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">每个卷积核都是一个分类器！你真的了解卷积神经网络是如何工作的吗？</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/each-convolution-kernel-is-a-classifier-5c2da17ccf6e?source=collection_archive---------17-----------------------#2021-12-07">https://towardsdatascience.com/each-convolution-kernel-is-a-classifier-5c2da17ccf6e?source=collection_archive---------17-----------------------#2021-12-07</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="1f40" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsdatascience.com/tagged/tips-and-tricks" rel="noopener" target="_blank">提示和技巧</a></h2><div class=""/><div class=""><h2 id="7a9a" class="pw-subtitle-paragraph jw iz iq bd b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn dk translated">卷积和池层背后的信息流直观指南。卷积核不是“神奇的石头”，它学习使用基于梯度的优化来提取“某种”特征。卷积神经网络中数值的真正含义是什么？幕后信息如何在卷积层和池层之间流动？</h2></div></div><div class="ab cl ko kp hu kq" role="separator"><span class="kr bw bk ks kt ku"/><span class="kr bw bk ks kt ku"/><span class="kr bw bk ks kt"/></div><div class="ij ik il im in"><figure class="kw kx ky kz gt la gh gi paragraph-image"><div role="button" tabindex="0" class="lb lc di ld bf le"><div class="gh gi kv"><img src="../Images/04f102d7f1a7b9a6e45dc1d1632b6f04.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*i0gAHzbgiarg3SjlJ5HNMQ.jpeg"/></div></div><p class="lh li gj gh gi lj lk bd b be z dk translated">来源:AdobeStock</p></figure></div><div class="ab cl ko kp hu kq" role="separator"><span class="kr bw bk ks kt ku"/><span class="kr bw bk ks kt ku"/><span class="kr bw bk ks kt"/></div><div class="ij ik il im in"><p id="7195" class="pw-post-body-paragraph ll lm iq ln b lo lp ka lq lr ls kd lt lu lv lw lx ly lz ma mb mc md me mf mg ij bi translated"><strong class="ln ja">《艾是新电》</strong> —吴恩达。可以说深度卷积神经网络(CNN)和计算机视觉也是如此:卷积神经网络是计算机视觉的新电。</p><p id="2691" class="pw-post-body-paragraph ll lm iq ln b lo lp ka lq lr ls kd lt lu lv lw lx ly lz ma mb mc md me mf mg ij bi translated">CNN确实是机器学习社区的一颗钻石，尤其是在快速增长的计算机视觉领域。它们是计算机视觉中无数基本任务的支柱，从简单的图像分类(但在20多年前被认为是非常具有挑战性或棘手的)到更复杂的任务，包括图像分割、图像超分辨率和图像字幕。这就是为什么在过去的十年里，学术界和工业界对CNN越来越感兴趣。<strong class="ln ja">尽管许多工程师和学生每天都在练习和使用CNN，但大多数人缺乏对卷积和池块这两个任何CNN最基本的构建块的全面理论观点</strong>。</p><figure class="kw kx ky kz gt la gh gi paragraph-image"><div role="button" tabindex="0" class="lb lc di ld bf le"><div class="gh gi mh"><img src="../Images/1eaef0f7c376be8cf9d4e8a27baeec79.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*LFGammhkOungP4Ht.png"/></div></div><p class="lh li gj gh gi lj lk bd b be z dk translated">来源:<a class="ae mi" href="https://github.com/gwding/draw_convnet" rel="noopener ugc nofollow" target="_blank">GitHub—gwding/draw _ conv net</a></p></figure><p id="e5b4" class="pw-post-body-paragraph ll lm iq ln b lo lp ka lq lr ls kd lt lu lv lw lx ly lz ma mb mc md me mf mg ij bi translated">对细胞神经网络有一个坚实的理论观点，特别是从信息论的角度来看，对于提出新的想法来显著提高深度学习模型在特定任务上的能力，甚至以无人能及的效率解决问题来说，尤为重要。在本文的其余部分，我将简要介绍信息论的概念，然后从信息论和统计学的角度详细解释CNN各层中数值的含义。我希望本文涵盖的内容能够帮助您以新的方式了解CNN的工作方式，并提出自己的CNN模型来更好地解决您的计算机视觉问题，在Kaggle上获得奖牌，甚至最终在CVPR和ICCV这样的顶级会议上发表论文。</p></div><div class="ab cl ko kp hu kq" role="separator"><span class="kr bw bk ks kt ku"/><span class="kr bw bk ks kt ku"/><span class="kr bw bk ks kt"/></div><div class="ij ik il im in"><h1 id="dbb1" class="mj mk iq bd ml mm mn mo mp mq mr ms mt kf mu kg mv ki mw kj mx kl my km mz na bi translated">深度神经网络中的信息论</h1><p id="7cd0" class="pw-post-body-paragraph ll lm iq ln b lo nb ka lq lr nc kd lt lu nd lw lx ly ne ma mb mc nf me mf mg ij bi translated">你可能对“特征提取”这个概念很熟悉，这正是卷积核所做的一切(从最简单的意义上来说)。但是，你有没有尝试过深入了解更多关于“特征提取”的知识？具体来说，从数值的角度来看，卷积函数是如何进行特征提取的？到头来，CNN里的只是数字，数字，数字。你的好奇心是否曾促使你去寻找CNN层中那些数字的意义和它们之间的关系？</p><p id="ab5e" class="pw-post-body-paragraph ll lm iq ln b lo lp ka lq lr ls kd lt lu lv lw lx ly lz ma mb mc md me mf mg ij bi translated">虽然我在基于CNN的论文中多次遇到术语“信息流”，但我一直在努力寻找任何涵盖信息论的论文或书籍，或至少是其直观观点，在卷积神经网络背后的可满足的深度。</p><p id="0014" class="pw-post-body-paragraph ll lm iq ln b lo lp ka lq lr ls kd lt lu lv lw lx ly lz ma mb mc md me mf mg ij bi translated">首先，我们将简要介绍信息论。</p><h2 id="c33d" class="ng mk iq bd ml nh ni dn mp nj nk dp mt lu nl nm mv ly nn no mx mc np nq mz iw bi translated">1.自我信息</h2><p id="6c0a" class="pw-post-body-paragraph ll lm iq ln b lo nb ka lq lr nc kd lt lu nd lw lx ly ne ma mb mc nf me mf mg ij bi translated">假设一个事件<strong class="ln ja"> <em class="nr"> X=x </em> </strong>有概率<strong class="ln ja"> <em class="nr"> p </em> </strong>发生。那么我们将事件<strong class="ln ja"> <em class="nr"> x </em> </strong>的信息值<strong class="ln ja"> <em class="nr"> I </em> </strong>定义如下:</p><figure class="kw kx ky kz gt la gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/b376ed01bb19a87c6c2611d0ee00a17d.png" data-original-src="https://miro.medium.com/v2/resize:fit:366/format:webp/1*ZDboHBhM8ogw9zkvZC8WiQ.png"/></div></figure><p id="9363" class="pw-post-body-paragraph ll lm iq ln b lo lp ka lq lr ls kd lt lu lv lw lx ly lz ma mb mc md me mf mg ij bi translated">价值<strong class="ln ja"> <em class="nr"> I </em> </strong>的意义简单来说就是衡量事件<strong class="ln ja"> <em class="nr"> x </em> </strong>发生的知识有多少/有多少价值。发生<strong class="ln ja"> <em class="nr"> p </em> </strong>的几率越小，信息价值越高。直觉上，当一个罕见的事件发生时，人们认为它更有价值；相反，如果一个事件太普通，我们往往认为它不包含太多的信息价值。</p><p id="2d23" class="pw-post-body-paragraph ll lm iq ln b lo lp ka lq lr ls kd lt lu lv lw lx ly lz ma mb mc md me mf mg ij bi translated">比如你知道你下周一要上班(<strong class="ln ja"> <em class="nr"> p = 0.99 </em> </strong>)，那么这个你下周一要上班的知识其实是没有用的，因为它并没有改变你现在的任何决定。然而，如果你的老板突然告诉你，你明天必须去欧洲参加一个非常重要的商务活动(<strong class="ln ja"> <em class="nr"> p = 0.01 </em> </strong>)，那么这个信息的价值就非常高，它会对你当前的计划产生很大的影响。</p><h2 id="75b4" class="ng mk iq bd ml nh ni dn mp nj nk dp mt lu nl nm mv ly nn no mx mc np nq mz iw bi translated">2.熵</h2><p id="9a87" class="pw-post-body-paragraph ll lm iq ln b lo nb ka lq lr nc kd lt lu nd lw lx ly ne ma mb mc nf me mf mg ij bi translated">自我信息衡量的是单个离散事件的信息值，而熵则更进一步，捕捉一个随机变量的信息值，无论它是离散的还是连续的。</p><p id="b34e" class="pw-post-body-paragraph ll lm iq ln b lo lp ka lq lr ls kd lt lu lv lw lx ly lz ma mb mc md me mf mg ij bi translated">如果一个随机变量<strong class="ln ja"> <em class="nr"> X </em> </strong>有<strong class="ln ja"><em class="nr">【p(X)</em></strong>的p.m.f / p.d.f，那么熵定义如下:</p><figure class="kw kx ky kz gt la gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/1ef46c9dfde79a3a6f60b4020fc0d1a4.png" data-original-src="https://miro.medium.com/v2/resize:fit:532/format:webp/1*CYLURfmX5N4rqsS7HTlsTA.png"/></div></figure><figure class="kw kx ky kz gt la gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/b4aae5a5a3a9a64d0bb92d5f050c6d39.png" data-original-src="https://miro.medium.com/v2/resize:fit:470/format:webp/1*_jjbWo5uxvephpmH4iDyWg.png"/></div></figure><ul class=""><li id="67e2" class="nv nw iq ln b lo lp lr ls lu nx ly ny mc nz mg oa ob oc od bi translated">如果<strong class="ln ja"> <em class="nr"> X </em> </strong>是连续的:</li></ul><figure class="kw kx ky kz gt la gh gi paragraph-image"><div class="gh gi oe"><img src="../Images/f191b1bd1ddd8d4715b3c69d8ee5ffd7.png" data-original-src="https://miro.medium.com/v2/resize:fit:624/format:webp/1*vvz9U83kf6PuXqr5GOrUEA.png"/></div></figure><h2 id="9bb9" class="ng mk iq bd ml nh ni dn mp nj nk dp mt lu nl nm mv ly nn no mx mc np nq mz iw bi translated">3.条件熵</h2><p id="387c" class="pw-post-body-paragraph ll lm iq ln b lo nb ka lq lr nc kd lt lu nd lw lx ly ne ma mb mc nf me mf mg ij bi translated">同样，我们将随机变量<strong class="ln ja"> <em class="nr"> X </em> </strong>和<strong class="ln ja"> <em class="nr"> Y </em> </strong>的条件熵定义如下:</p><figure class="kw kx ky kz gt la gh gi paragraph-image"><div class="gh gi of"><img src="../Images/27e0a411e86b2068e76741e07289ab2a.png" data-original-src="https://miro.medium.com/v2/resize:fit:726/format:webp/1*3l5q_eJ5sPptHPRbBffNtQ.png"/></div></figure><p id="a353" class="pw-post-body-paragraph ll lm iq ln b lo lp ka lq lr ls kd lt lu lv lw lx ly lz ma mb mc md me mf mg ij bi translated">条件熵度量已知事件在<strong class="ln ja"><em class="nr">【Y】</em></strong>发生时的期望信息值，假设我们知道事件在<strong class="ln ja"><em class="nr">【X】</em></strong>已经发生。这一特性特别重要，因为它在低至像素级别的级别上与计算机视觉中的信息论直接相关。</p><h2 id="6810" class="ng mk iq bd ml nh ni dn mp nj nk dp mt lu nl nm mv ly nn no mx mc np nq mz iw bi translated">4.卷积核中的信息论</h2><p id="6ed5" class="pw-post-body-paragraph ll lm iq ln b lo nb ka lq lr nc kd lt lu nd lw lx ly ne ma mb mc nf me mf mg ij bi translated">几乎所有CNN的专业从业者都必须对卷积核的功能有深入的了解，包括步长和特征提取。如果那些概念你不熟悉，我强烈推荐这篇关于卷积神经网络介绍的很棒的文章:<a class="ae mi" rel="noopener" target="_blank" href="/convolutional-neural-network-17fb77e76c05">卷积神经网络</a>。</p><p id="a8dc" class="pw-post-body-paragraph ll lm iq ln b lo lp ka lq lr ls kd lt lu lv lw lx ly lz ma mb mc md me mf mg ij bi translated">在下一节中，我将介绍卷积核的信息论。</p></div><div class="ab cl ko kp hu kq" role="separator"><span class="kr bw bk ks kt ku"/><span class="kr bw bk ks kt ku"/><span class="kr bw bk ks kt"/></div><div class="ij ik il im in"><h1 id="d8cb" class="mj mk iq bd ml mm mn mo mp mq mr ms mt kf mu kg mv ki mw kj mx kl my km mz na bi translated">卷积核是分类器(惊喜？)</h1><h2 id="6a29" class="ng mk iq bd ml nh ni dn mp nj nk dp mt lu nl nm mv ly nn no mx mc np nq mz iw bi translated">1.卷积评论</h2><p id="6899" class="pw-post-body-paragraph ll lm iq ln b lo nb ka lq lr nc kd lt lu nd lw lx ly ne ma mb mc nf me mf mg ij bi translated">现代CNN中最常见的内核大小是<strong class="ln ja"> <em class="nr"> 3*3 </em> </strong>内核。假设我们在原始RGB图像上应用单个<strong class="ln ja"> <em class="nr"> 3*3 </em> </strong>内核，在感受野中观察到的像素总数将是<strong class="ln ja"> <em class="nr"> 27 </em> </strong> (H:3，W:3，C:3)。设卷积核为函数<strong class="ln ja"> <em class="nr"> f </em> </strong>，则<strong class="ln ja"> <em class="nr"> f: R ⁷ → R </em> </strong>。输入是27像素的图像部分，输出是标量值。</p><figure class="kw kx ky kz gt la gh gi paragraph-image"><div class="gh gi og"><img src="../Images/7f03ed3f58338309c5ff50cec85e6373.png" data-original-src="https://miro.medium.com/v2/resize:fit:798/format:webp/1*gG7AVmWs1FsJreJwSp5RQg.png"/></div><p class="lh li gj gh gi lj lk bd b be z dk translated">来源:作者图片</p></figure><p id="9c82" class="pw-post-body-paragraph ll lm iq ln b lo lp ka lq lr ls kd lt lu lv lw lx ly lz ma mb mc md me mf mg ij bi translated"><strong class="ln ja">这个输出的标量值是什么？让卷积运算在复杂的视觉任务上工作如此高效的这个像素的真正意义是什么？</strong></p><h2 id="f822" class="ng mk iq bd ml nh ni dn mp nj nk dp mt lu nl nm mv ly nn no mx mc np nq mz iw bi translated">2.每个内核都是一个分类器</h2><p id="3e3b" class="pw-post-body-paragraph ll lm iq ln b lo nb ka lq lr nc kd lt lu nd lw lx ly ne ma mb mc nf me mf mg ij bi translated">令人惊讶的是，输出的标量实际上是一个相对概率。换句话说，<strong class="ln ja">每个卷积核都是一个分类器</strong>！这是特征提取的核心思想！</p><p id="afcc" class="pw-post-body-paragraph ll lm iq ln b lo lp ka lq lr ls kd lt lu lv lw lx ly lz ma mb mc md me mf mg ij bi translated">假设内核<strong class="ln ja"> <em class="nr"> f: R ⁷ → R </em> </strong>负责提取一个特征<strong class="ln ja"><em class="nr"/></strong>(边缘、圆形物体、明亮物体等。)我们可以将内核<strong class="ln ja"> <em class="nr"> f </em> </strong>改写为:</p><figure class="kw kx ky kz gt la gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/3acd4a20a6aa17973172e39ee3822929.png" data-original-src="https://miro.medium.com/v2/resize:fit:1084/format:webp/1*cK0zKGDY7n1E0u58NnlQzw.png"/></div></figure><p id="0293" class="pw-post-body-paragraph ll lm iq ln b lo lp ka lq lr ls kd lt lu lv lw lx ly lz ma mb mc md me mf mg ij bi translated">直观的描述就是内核<strong class="ln ja"> <em class="nr"> f </em> </strong>观察其感受野的全部27个像素，<strong class="ln ja">学习分类图像中“被看到”的区域是否包含特征<em class="nr"> k </em>或</strong>。假设<strong class="ln ja"> <em class="nr"> k </em> </strong>为“边缘”，请看下图1中的实验和评论，以获得直观的解释。</p><figure class="kw kx ky kz gt la gh gi paragraph-image"><div role="button" tabindex="0" class="lb lc di ld bf le"><div class="gh gi oi"><img src="../Images/8db9a4b4e0e8f38812c76a5b5f25b9d4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dEHlFOgLjE51e9Aw9nwXvA.png"/></div></div><p class="lh li gj gh gi lj lk bd b be z dk translated">图1:自然图像到特征空间的转换(来源:作者)</p></figure><ul class=""><li id="6603" class="nv nw iq ln b lo lp lr ls lu nx ly ny mc nz mg oa ob oc od bi translated"><strong class="ln ja"> <em class="nr">注意</em> </strong> <em class="nr">:这只是相对概率，意思是输出的标量可以是任意实数，不一定在</em><strong class="ln ja"><em class="nr">【0，1】</em></strong><em class="nr">范围内。对于提取的特征图的可视化，已经应用了softmax变换来将值转换成精确的概率。</em></li></ul><p id="b46c" class="pw-post-body-paragraph ll lm iq ln b lo lp ka lq lr ls kd lt lu lv lw lx ly lz ma mb mc md me mf mg ij bi translated">综上所述，卷积核的核心思想来自信息论方面:</p><ol class=""><li id="e562" class="nv nw iq ln b lo lp lr ls lu nx ly ny mc nz mg oj ob oc od bi translated"><strong class="ln ja">每个卷积核在技术上是一个分类器</strong>。它观察感受野中的一组像素，并输出一个标量来度量感受野中的图像区域是否包含核特征的概率。</li><li id="3cad" class="nv nw iq ln b lo ok lr ol lu om ly on mc oo mg oj ob oc od bi translated">一组<strong class="ln ja">输出的概率被组织为像素</strong>也使得提取的特征变得人眼可见。</li><li id="09a8" class="nv nw iq ln b lo ok lr ol lu om ly on mc oo mg oj ob oc od bi translated">输出像素的组织<strong class="ln ja"> <em class="nr"> p(特征k|像素)</em> </strong>对应于原始图像中的像素位置。这个<strong class="ln ja">保持了原始图像</strong>中物体的整体结构和位置，这解释了<strong class="ln ja">信息如何在卷积层之间</strong>流动。</li></ol><figure class="kw kx ky kz gt la gh gi paragraph-image"><div role="button" tabindex="0" class="lb lc di ld bf le"><div class="gh gi op"><img src="../Images/12b04a69b24c87a34b09a1776ab8516d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-kGoT5NTpB-l7CK_fH9EeQ.png"/></div></div><p class="lh li gj gh gi lj lk bd b be z dk translated">图2:保留对象位置的信息流(来源:作者)</p></figure></div><div class="ab cl ko kp hu kq" role="separator"><span class="kr bw bk ks kt ku"/><span class="kr bw bk ks kt ku"/><span class="kr bw bk ks kt"/></div><div class="ij ik il im in"><h1 id="603f" class="mj mk iq bd ml mm mn mo mp mq mr ms mt kf mu kg mv ki mw kj mx kl my km mz na bi translated">信息论视角下的池层直观观</h1><p id="5e9f" class="pw-post-body-paragraph ll lm iq ln b lo nb ka lq lr nc kd lt lu nd lw lx ly ne ma mb mc nf me mf mg ij bi translated">与卷积核类似，池化也可以用信息论和信息流来解释。</p><h2 id="23f7" class="ng mk iq bd ml nh ni dn mp nj nk dp mt lu nl nm mv ly nn no mx mc np nq mz iw bi translated">从高层次理解池层的一些好处:</h2><ol class=""><li id="042f" class="nv nw iq ln b lo nb lr nc lu oq ly or mc os mg oj ob oc od bi translated">降维以实现更快的训练和更简单的学习曲线</li><li id="837e" class="nv nw iq ln b lo ok lr ol lu om ly on mc oo mg oj ob oc od bi translated">增加信息密度，消除稀疏分布</li></ol><h2 id="2ec8" class="ng mk iq bd ml nh ni dn mp nj nk dp mt lu nl nm mv ly nn no mx mc np nq mz iw bi translated">从信息论角度看池层:</h2><p id="18cf" class="pw-post-body-paragraph ll lm iq ln b lo nb ka lq lr nc kd lt lu nd lw lx ly ne ma mb mc nf me mf mg ij bi translated">如前所述，深度特征图<strong class="ln ja"><em class="nr"/></strong>中的每个像素在技术上是原始图像中相应区域包含特征<strong class="ln ja"> <em class="nr"> k </em> </strong>的相对概率。高值表示被观察区域很可能具有特征<strong class="ln ja"> <em class="nr"> k </em> </strong>，反之亦然。</p><p id="fb1d" class="pw-post-body-paragraph ll lm iq ln b lo lp ka lq lr ls kd lt lu lv lw lx ly lz ma mb mc md me mf mg ij bi translated">由于图像本质上具有稀疏的信息分布，并且汇集层负责<strong class="ln ja">以最小的信息损失进行密集分布</strong>。</p><p id="8a74" class="pw-post-body-paragraph ll lm iq ln b lo lp ka lq lr ls kd lt lu lv lw lx ly lz ma mb mc md me mf mg ij bi translated">下图有助于阐明池层之间的信息流:</p><figure class="kw kx ky kz gt la gh gi paragraph-image"><div role="button" tabindex="0" class="lb lc di ld bf le"><div class="gh gi ot"><img src="../Images/08f54877bb43754fa7c69b2dbccd2347.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3B-fp40NFECmPZWodVMeHA.png"/></div></div><p class="lh li gj gh gi lj lk bd b be z dk translated">图3:信息密集化(来源:作者)</p></figure><p id="1ea8" class="pw-post-body-paragraph ll lm iq ln b lo lp ka lq lr ls kd lt lu lv lw lx ly lz ma mb mc md me mf mg ij bi translated">对于相邻像素的移除(例如，在移除<strong class="ln ja"> a1 </strong>、<strong class="ln ja"> a2 </strong>、<strong class="ln ja"> a7 </strong>的同时保留<strong class="ln ja"> a8 </strong>)有一种自然的担忧:它是否会显著影响层间的信息流并损害CNN的性能？答案是肯定会有信息损失，但这是一个可以接受的损失，以使特征图中的每个像素更有意义。<strong class="ln ja"> a1 </strong>、<strong class="ln ja"> a2 </strong>、<strong class="ln ja"> a7 </strong>、<strong class="ln ja"> a8 </strong>是相邻像素，倾向于相关。因此，这些像素组合的熵增益并不显著，我们可能只需要1个像素来表示特征图中这4个像素的信息。汇集层过滤三个较不重要的像素，即<strong class="ln ja"> a1 </strong>、<strong class="ln ja"> a2 </strong>、<strong class="ln ja"> a7 </strong>，并保留<strong class="ln ja"> a8 </strong>作为流向更深层的唯一信息。</p></div><div class="ab cl ko kp hu kq" role="separator"><span class="kr bw bk ks kt ku"/><span class="kr bw bk ks kt ku"/><span class="kr bw bk ks kt"/></div><div class="ij ik il im in"><h1 id="8a22" class="mj mk iq bd ml mm mn mo mp mq mr ms mt kf mu kg mv ki mw kj mx kl my km mz na bi translated">讨论</h1><p id="1e6b" class="pw-post-body-paragraph ll lm iq ln b lo nb ka lq lr nc kd lt lu nd lw lx ly ne ma mb mc nf me mf mg ij bi translated">下面是一系列后续问题，可以帮助你从信息流和信息论的角度更好地理解CNN:</p><ol class=""><li id="9e1c" class="nv nw iq ln b lo lp lr ls lu nx ly ny mc nz mg oj ob oc od bi translated">剩余连接中加法的数值意义是什么？传递什么信息？究竟为什么卷积可能有信息损失？(提示:每个卷积核可以捕获1个特定的特征，可以是简单的，也可以是复杂的，有些特征集不重叠)。</li><li id="8699" class="nv nw iq ln b lo ok lr ol lu om ly on mc oo mg oj ob oc od bi translated">为什么我们使用最大池，有时使用平均池？最小公摊有什么好处？</li><li id="4cf6" class="nv nw iq ln b lo ok lr ol lu om ly on mc oo mg oj ob oc od bi translated">池层是否被认为是CNN的瓶颈，为什么是或者不是？</li></ol></div><div class="ab cl ko kp hu kq" role="separator"><span class="kr bw bk ks kt ku"/><span class="kr bw bk ks kt ku"/><span class="kr bw bk ks kt"/></div><div class="ij ik il im in"><h1 id="0c56" class="mj mk iq bd ml mm mn mo mp mq mr ms mt kf mu kg mv ki mw kj mx kl my km mz na bi translated">参考</h1><p id="c60e" class="pw-post-body-paragraph ll lm iq ln b lo nb ka lq lr nc kd lt lu nd lw lx ly ne ma mb mc nf me mf mg ij bi translated">[1]潜入深度学习一书，18.11节:《信息论》:<br/> <a class="ae mi" href="https://d2l.ai/" rel="noopener ugc nofollow" target="_blank">潜入深度学习—潜入深度学习0.17文档(d2l.ai) </a></p><p id="e4f6" class="pw-post-body-paragraph ll lm iq ln b lo lp ka lq lr ls kd lt lu lv lw lx ly lz ma mb mc md me mf mg ij bi translated">[2]用信息论理解卷积神经网络:初步探索:<a class="ae mi" href="https://arxiv.org/pdf/1804.06537.pdf" rel="noopener ugc nofollow" target="_blank">1804.06537.pdf(arxiv.org)</a></p><p id="4f9f" class="pw-post-body-paragraph ll lm iq ln b lo lp ka lq lr ls kd lt lu lv lw lx ly lz ma mb mc md me mf mg ij bi translated">[3] <a class="ae mi" href="https://machinelearningmastery.com/information-gain-and-mutual-information/" rel="noopener ugc nofollow" target="_blank">机器学习的信息增益和互信息</a></p><p id="3d22" class="pw-post-body-paragraph ll lm iq ln b lo lp ka lq lr ls kd lt lu lv lw lx ly lz ma mb mc md me mf mg ij bi translated">[4] <a class="ae mi" href="https://machinelearningmastery.com/what-is-information-entropy/" rel="noopener ugc nofollow" target="_blank">信息熵的温和介绍</a></p><p id="4d80" class="pw-post-body-paragraph ll lm iq ln b lo lp ka lq lr ls kd lt lu lv lw lx ly lz ma mb mc md me mf mg ij bi translated">[5] <a class="ae mi" rel="noopener" target="_blank" href="/convolutional-neural-network-17fb77e76c05">卷积神经网络</a></p></div></div>    
</body>
</html>