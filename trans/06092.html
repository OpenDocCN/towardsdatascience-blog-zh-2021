<html>
<head>
<title>What Are Post-Decision States and What Do They Want From Us?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">什么是后决策状态？他们想从我们这里得到什么？</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/what-are-post-decision-states-and-what-do-they-want-from-us-9e02105b7f40?source=collection_archive---------44-----------------------#2021-05-31">https://towardsdatascience.com/what-are-post-decision-states-and-what-do-they-want-from-us-9e02105b7f40?source=collection_archive---------44-----------------------#2021-05-31</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="4972" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">阐明强化学习中的转移函数和状态-动作对</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/4a86673419ebf2fa4c6aac1b28fd24d1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lAlfpcG1LJQKvBGLE2_v3g.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">一个井字游戏完美地展示了后决策状态的概念[作者自己的作品]</p></figure></div><div class="ab cl ky kz hx la" role="separator"><span class="lb bw bk lc ld le"/><span class="lb bw bk lc ld le"/><span class="lb bw bk lc ld"/></div><div class="im in io ip iq"><p id="cd9d" class="pw-post-body-paragraph lf lg it lh b li lj ju lk ll lm jx ln lo lp lq lr ls lt lu lv lw lx ly lz ma im bi translated">以一个经验丰富的强化学习老手的反高潮开始；后决策状态并不新奇或惊天动地。还没点开？很好，因为实际上有一些内容出现了。在本文中，我们将更深入地研究状态-动作对和转移函数的概念，提供从一个问题状态转移到另一个问题状态的过程的洞察力。</p><h1 id="b23b" class="mb mc it bd md me mf mg mh mi mj mk ml jz mm ka mn kc mo kd mp kf mq kg mr ms bi translated">转换函数</h1><p id="0736" class="pw-post-body-paragraph lf lg it lh b li mt ju lk ll mu jx ln lo mv lq lr ls mw lu lv lw mx ly lz ma im bi translated">在强化学习(RL)的最简单表示中，我们有一些<strong class="lh iu">概率函数</strong> <code class="fe my mz na nb b">p(s_t+1|s_t,a_t)</code>引导我们到新的状态。我们处于状态<code class="fe my mz na nb b">s_t</code>，采取行动<code class="fe my mz na nb b">a_t</code>，并神奇地以某种概率<code class="fe my mz na nb b">p</code>转移到状态<code class="fe my mz na nb b">s_t+1</code>。此外，我们可以定义一个集合<code class="fe my mz na nb b">S’</code>,它包含所有的结果状态<code class="fe my mz na nb b">s_t+1</code>,在给定我们的动作的情况下，这些结果状态可以从我们的当前状态到达。自然地，该集合中所有状态的概率总和应该是1。</p><p id="f477" class="pw-post-body-paragraph lf lg it lh b li lj ju lk ll lm jx ln lo lp lq lr ls lt lu lv lw lx ly lz ma im bi translated">听起来很简单，但是许多有趣的信息隐含在这个看似简单的概率函数中。如果我们的目标是更明确地试图描述从状态<code class="fe my mz na nb b">s_t</code>移动到<code class="fe my mz na nb b">s_t+1</code>的过程，我们可以称之为<strong class="lh iu">转移函数</strong>；一些明确的功能<code class="fe my mz na nb b">f()</code>引导我们从一种状态到另一种状态。拥有“一些”结果状态<code class="fe my mz na nb b">S’</code>并不是非常有用，因此在这一点上引入外部信息变量<code class="fe my mz na nb b"><em class="nc">ω_t</em></code> <em class="nc">是很好的。</em>这个变量可以封装任何不在我们控制范围内的信息:骰子上的眼睛数量、明天的降雨量、比特币价格、未来的石油需求……简而言之，<code class="fe my mz na nb b"><em class="nc">ω_t</em></code> <em class="nc"> </em>捕捉到了我们转变的不确定性，并可能被视为区间<code class="fe my mz na nb b">(t,t+1]</code>中透露的信息。这就是强化学习的内容；我们部分地通过自己的行为来影响回报，部分地预测世界会给我们带来什么。目的是仔细选择我们的行动，这样我们就能在有利的位置上面对未来。</p><p id="48cb" class="pw-post-body-paragraph lf lg it lh b li lj ju lk ll lm jx ln lo lp lq lr ls lt lu lv lw lx ly lz ma im bi translated">有了我们的外生信息变量<code class="fe my mz na nb b">ω_t</code>，我们现在可以尝试定义一个显式的转移函数。设<code class="fe my mz na nb b">Ω_t</code>是所有可能变量<code class="fe my mz na nb b">ω_t</code>的集合；将这些变量视为场景可能更方便。综上所述:我们从<code class="fe my mz na nb b">s_t</code>到<code class="fe my mz na nb b">s_t+1</code>的转变取决于当前(预决策)状态<code class="fe my mz na nb b">s_t</code>，被选择的动作<code class="fe my mz na nb b">a_t</code>和外生信息<code class="fe my mz na nb b">ω_t</code>的实现:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nd"><img src="../Images/b424650f0a3db479adfa916d52a360c6.png" data-original-src="https://miro.medium.com/v2/resize:fit:696/format:webp/1*oXhH79dZ86cRrAkotfPlYA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">转换函数(从一个预决策状态到下一个状态)</p></figure><h1 id="252c" class="mb mc it bd md me mf mg mh mi mj mk ml jz mm ka mn kc mo kd mp kf mq kg mr ms bi translated">决策后状态</h1><p id="adc4" class="pw-post-body-paragraph lf lg it lh b li mt ju lk ll mu jx ln lo mv lq lr ls mw lu lv lw mx ly lz ma im bi translated">到目前为止，一切顺利。现在让我们来讨论一下Q值。正如你可能知道的，基于值的RL算法学习对应于状态-动作对的值。这些Q值——用<code class="fe my mz na nb b">Q(s_t,a_t)</code>表示——捕获了我们当前行动的<em class="nc">预期</em>下游值。作为复习，看看SARSA算法的规范更新公式[2]:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ne"><img src="../Images/a6145a5220dab0e567580f2b6f80f140.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wRexRBdYQ5P9-mPQTeFllQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">用SARSA更新状态-动作对的Q值的函数</p></figure><p id="d0f9" class="pw-post-body-paragraph lf lg it lh b li lj ju lk ll lm jx ln lo lp lq lr ls lt lu lv lw lx ly lz ma im bi translated">学习与状态-动作对相关的预期下游值听起来有点抽象，这就是<strong class="lh iu">后决策状态</strong> — <strong class="lh iu"> </strong>在【2】中详细解释的地方——可能会有所启发。我们已经确定我们的转变是部分确定的(由于我们的行动<code class="fe my mz na nb b">a_t</code>)和部分随机的(由于<code class="fe my mz na nb b">ω_t</code>的随机表现)。为什么不把这两个元素分开，看看会发生什么？</p><p id="fa40" class="pw-post-body-paragraph lf lg it lh b li lj ju lk ll lm jx ln lo lp lq lr ls lt lu lv lw lx ly lz ma im bi translated">首先，我们引入一个转移函数<code class="fe my mz na nb b">f^(1)</code>来从我们当前的预决策状态<code class="fe my mz na nb b">s_t</code>转移到后决策状态<code class="fe my mz na nb b">s_t^a</code>。请注意，这种转变是完全确定的，并且不需要时间来向前移动，我们可以在选择一个操作的瞬间计算它:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nf"><img src="../Images/c0e542a858a652824f9dfdad0eaed6cc.png" data-original-src="https://miro.medium.com/v2/resize:fit:662/format:webp/1*82WP8vQyx_e-kadSBc-kUQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">从预决策状态到后决策状态的转换函数</p></figure><p id="7e92" class="pw-post-body-paragraph lf lg it lh b li lj ju lk ll lm jx ln lo lp lq lr ls lt lu lv lw lx ly lz ma im bi translated">第二，我们有一个转换函数<code class="fe my mz na nb b">f^(2)</code>从决策后状态<code class="fe my mz na nb b">s_t^a</code>移动到下一个决策前状态<code class="fe my mz na nb b">s_t+1</code>。这个函数是完全随机的；完全取决于<code class="fe my mz na nb b">ω_t</code>的实现。根据定义，我们的动作是基于状态<code class="fe my mz na nb b">s_t</code>中嵌入的信息，因此，新信息<code class="fe my mz na nb b">ω_t</code>必须在采取动作后到达<em class="nc">。因此，第二个转换函数意味着时间的流逝，在此期间新的信息被揭示:</em></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nf"><img src="../Images/eaa78d1f724dc03806207ec0386c4ab1.png" data-original-src="https://miro.medium.com/v2/resize:fit:662/format:webp/1*OoNVY3sv3lmuK3oUF-clJA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">从决策后状态到下一个决策前状态的转换函数</p></figure><p id="01fd" class="pw-post-body-paragraph lf lg it lh b li lj ju lk ll lm jx ln lo lp lq lr ls lt lu lv lw lx ly lz ma im bi translated">所以现在我们有两个转移函数，而不是一个。这看起来没什么进步，但是请耐心听我说。</p><h1 id="e99d" class="mb mc it bd md me mf mg mh mi mj mk ml jz mm ka mn kc mo kd mp kf mq kg mr ms bi translated">井字游戏</h1><p id="7afa" class="pw-post-body-paragraph lf lg it lh b li mt ju lk ll mu jx ln lo mv lq lr ls mw lu lv lw mx ly lz ma im bi translated">有什么比用一个老式的井字游戏更好的方式来说明一个概念呢？让我们考虑一下游戏开始时的状态。决策前状态<code class="fe my mz na nb b">s_0</code>显然是一个空棋盘，而我们有九个可行的动作<code class="fe my mz na nb b">a_0(s_0)</code>，因此可能达到九个决策后状态<code class="fe my mz na nb b">s_0^a</code>(注意这相当于评估九个状态-动作对)。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ng"><img src="../Images/a4269ced62f1d0a45314dff094057bf6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eVyIuOoDp-I5hkXFgWYXRQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">决策前状态(左窗格)，九个可达到的决策后状态(右窗格)[作者自己的作品]</p></figure><p id="b5f7" class="pw-post-body-paragraph lf lg it lh b li lj ju lk ll lm jx ln lo lp lq lr ls lt lu lv lw lx ly lz ma im bi translated">为了真正达到决策后状态，我们必须选择一个动作并填写<code class="fe my mz na nb b">f^(1)(s_t,a_t)</code>。为了达到随后的预决策状态，我们必须等待——从我们的角度来看是外生的——对手的移动并填入<code class="fe my mz na nb b">f^(2)(s_t^a,ω_t)</code></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/4a86673419ebf2fa4c6aac1b28fd24d1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lAlfpcG1LJQKvBGLE2_v3g.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">第一个动作后的决策后状态(左窗格)和对手动作后的下一个决策前状态(右窗格)[作者自己的作品]</p></figure><p id="2ed1" class="pw-post-body-paragraph lf lg it lh b li lj ju lk ll lm jx ln lo lp lq lr ls lt lu lv lw lx ly lz ma im bi translated">再举个例子？我们已经接近比赛的尾声了。我们有三个要考虑的行动，或者说，在上下文中，有三个可达到的决策后状态要评估。假设我们已经有了一些观察结果，我们可以知道两个后决策状态可能会产生很差的结果。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nh"><img src="../Images/d0eaf6e6b0a750e0824296d232009fcb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1110/format:webp/1*EgAcldNTZu7miQl83v-QAg.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">游戏接近尾声时可达到的决策后状态的可视化[作者自己的作品]</p></figure><p id="a87b" class="pw-post-body-paragraph lf lg it lh b li lj ju lk ll lm jx ln lo lp lq lr ls lt lu lv lw lx ly lz ma im bi translated">现在很容易看出价值函数<code class="fe my mz na nb b">Q(s,a)</code>和<code class="fe my mz na nb b">Q(s^a)</code>非常相似。后决策状态的关键是关于未来的确定性和随机性知识的综合分离。决策后状态提供了比决策前状态更多的最新信息，明确地捕获了最新的系统状态，而没有实际地及时向前移动。</p><h1 id="1e19" class="mb mc it bd md me mf mg mh mi mj mk ml jz mm ka mn kc mo kd mp kf mq kg mr ms bi translated">特征设计</h1><p id="c325" class="pw-post-body-paragraph lf lg it lh b li mt ju lk ll mu jx ln lo mv lq lr ls mw lu lv lw mx ly lz ma im bi translated">对于井字游戏来说，所有这些可能看起来有点做作，但是当我们为更现实的RL问题设计功能时，根据决策后状态进行思考的好处变得更加明显。通常，将这些特征建立在我们拥有的最新信息的基础上是有意义的。</p><p id="0d00" class="pw-post-body-paragraph lf lg it lh b li lj ju lk ll lm jx ln lo lp lq lr ls lt lu lv lw lx ly lz ma im bi translated">假设我们有一个仓库，里面有一千种不同的商品，每种商品的数量反映在一个大的向量中(即状态)。根据预期的当前库存水平，我们决定(即行动)订购一定数量的每种商品。我们的决策后状态将是当前库存水平加上订单水平。这个<em class="nc">更新的</em>水平预测了，例如，预期的存储成本、过期库存<strong class="lh iu"> </strong>和满足未来需求的能力——简而言之，预期的下游价值。因此，如果我们计算任何特征(例如所有项目的总体积、快速移动项目的数量)，我们是基于决策后向量来这样做的。实际上，这里的后决策向量是状态向量和动作向量的简单相加。从后决策到下一状态的转换减去<code class="fe my mz na nb b">ω_t</code>的随机实现:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ni"><img src="../Images/4df89ae0c31edf19979ae8aa121f93de.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-BQCBqwPe-LRBkKVjnHubg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">有10种产品类型的库存的转移函数示例。在左边的f^(1)，通过向现有库存添加新订单，从s_t过渡到<code class="fe my mz na nb b"> s_t^a</code>。右边的f^(2)，从<code class="fe my mz na nb b">s_t^a to s_t+1 by subtracting stochastic demand.</code>过渡</p></figure><p id="ae81" class="pw-post-body-paragraph lf lg it lh b li lj ju lk ll lm jx ln lo lp lq lr ls lt lu lv lw lx ly lz ma im bi translated">观察<code class="fe my mz na nb b">s_t^a</code>是我们确定满足未来需求能力的最佳预测器。因此，当基于状态计算特性时，我们可能会利用某个函数<code class="fe my mz na nb b">ϕ_f:s_t^a↦θ_f</code>(其中函数可以是神经网络，而<code class="fe my mz na nb b">s_t^a</code>是输入向量)。当然，<code class="fe my mz na nb b">ϕ_f:(s_t,a_t)↦θ_f</code>可能有完全相同的意思，但前者肯定更明确。正如Python的禅宗所说:</p><blockquote class="nj"><p id="1e75" class="nk nl it bd nm nn no np nq nr ns ma dk translated">“显性比隐性好”</p></blockquote></div><div class="ab cl ky kz hx la" role="separator"><span class="lb bw bk lc ld le"/><span class="lb bw bk lc ld le"/><span class="lb bw bk lc ld"/></div><div class="im in io ip iq"><h1 id="f8b3" class="mb mc it bd md me nt mg mh mi nu mk ml jz nv ka mn kc nw kd mp kf nx kg mr ms bi translated">外卖食品</h1><ul class=""><li id="4bc1" class="ny nz it lh b li mt ll mu lo oa ls ob lw oc ma od oe of og bi translated">决策后状态与状态-动作对密切相关，但是提供了关于信息可用性的更明确的观点。具体地说，它们体现了将状态-动作对转换成单个信息变量。</li><li id="d019" class="ny nz it lh b li oh ll oi lo oj ls ok lw ol ma od oe of og bi translated">我们将传统的转移函数分成两个部分，清晰地区分在单个时间点的<em class="nc">确定性</em>转移(基于选定的动作)和随着时间推移的<em class="nc">随机</em>转移(基于环境)。</li><li id="1ec2" class="ny nz it lh b li oh ll oi lo oj ls ok lw ol ma od oe of og bi translated">从决策后状态的角度思考通常有利于特征设计，因为我们希望特征基于我们最近的信息。</li></ul></div><div class="ab cl ky kz hx la" role="separator"><span class="lb bw bk lc ld le"/><span class="lb bw bk lc ld le"/><span class="lb bw bk lc ld"/></div><div class="im in io ip iq"><p id="534f" class="pw-post-body-paragraph lf lg it lh b li lj ju lk ll lm jx ln lo lp lq lr ls lt lu lv lw lx ly lz ma im bi translated"><em class="nc">本文关注状态-动作对和决策后状态之间的相似性。对差异感兴趣吗？请查看:</em></p><div class="om on gp gr oo op"><a rel="noopener follow" target="_blank" href="/about-post-decision-states-again-5725e5c15d90"><div class="oq ab fo"><div class="or ab os cl cj ot"><h2 class="bd iu gy z fp ou fr fs ov fu fw is bi translated">关于后决策状态</h2><div class="ow l"><h3 class="bd b gy z fp ou fr fs ov fu fw dk translated">论强化学习中状态-动作对和决策后状态的(不那么)细微差别。</h3></div><div class="ox l"><p class="bd b dl z fp ou fr fs ov fu fw dk translated">towardsdatascience.com</p></div></div><div class="oy l"><div class="oz l pa pb pc oy pd ks op"/></div></div></a></div><h1 id="b4be" class="mb mc it bd md me mf mg mh mi mj mk ml jz mm ka mn kc mo kd mp kf mq kg mr ms bi translated">参考</h1><p id="56a4" class="pw-post-body-paragraph lf lg it lh b li mt ju lk ll mu jx ln lo mv lq lr ls mw lu lv lw mx ly lz ma im bi translated">[1]萨顿和巴尔托(2018年)。<em class="nc">强化学习:简介</em>。麻省理工出版社。</p><p id="db95" class="pw-post-body-paragraph lf lg it lh b li lj ju lk ll lm jx ln lo lp lq lr ls lt lu lv lw lx ly lz ma im bi translated">[2]鲍威尔(2007年)。<em class="nc">近似动态规划:解决维数灾难</em>。约翰·威利&amp;的儿子们。</p></div></div>    
</body>
</html>