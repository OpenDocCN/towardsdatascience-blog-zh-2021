<html>
<head>
<title>A beginner’s guide to OCTIS vol. 2: Optimizing Topic Models</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">OCTIS 入门指南第 2 卷:优化主题模型</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/a-beginners-guide-to-octis-vol-2-optimizing-topic-models-1214e58be1e5?source=collection_archive---------7-----------------------#2021-12-26">https://towardsdatascience.com/a-beginners-guide-to-octis-vol-2-optimizing-topic-models-1214e58be1e5?source=collection_archive---------7-----------------------#2021-12-26</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi jn"><img src="../Images/e1f8655d596a8611d6126699473a5b64.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*nZ6NeklriPKSRhvN"/></div></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">乔尔·菲利普在<a class="ae kc" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="a55d" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在<a class="ae kc" rel="noopener" target="_blank" href="/a-beginners-guide-to-octis-optimizing-and-comparing-topic-models-is-simple-590554ec9ba6">之前的一篇帖子</a>中，我介绍了 Python 包 OCTIS(优化和比较主题模型很简单)；我演示了如何开始和它的特性。这个包允许简单的主题模型优化和比较(顾名思义)。这篇文章主要关注名字的第一个字母:<em class="lb">O</em>optimization。</p><p id="48e0" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在这篇文章的结尾，你将会:</p><ol class=""><li id="cd68" class="lc ld iq kf b kg kh kk kl ko le ks lf kw lg la lh li lj lk bi translated">对超参数优化有基本的了解。</li><li id="fa8d" class="lc ld iq kf b kg ll kk lm ko ln ks lo kw lp la lh li lj lk bi translated">知道优化主题模型时要考虑什么。</li><li id="e692" class="lc ld iq kf b kg ll kk lm ko ln ks lo kw lp la lh li lj lk bi translated">能够优化你自己的主题模型。</li></ol><h1 id="cb4c" class="lq lr iq bd ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn bi translated">超参数优化</h1><p id="355c" class="pw-post-body-paragraph kd ke iq kf b kg mo ki kj kk mp km kn ko mq kq kr ks mr ku kv kw ms ky kz la ij bi translated">几乎所有的机器学习算法都有超参数。超参数表示学习过程的算法所使用的设置；基于这些设置，算法遵循学习程序来学习其参数。然而，确定每个超参数的最佳值并不简单，通常通过先进的试错法来完成，也称为<em class="lb">超参数优化</em>。使用这些超参数调整方法中的任何一种，算法都是基于一组超参数来训练的。该算法的结果通过性能指标或误差测量来评估。然后，(稍微)调整超参数，并基于新的设置训练和评估模型。最基本的方法是网格搜索，其中算法在一组预定义设置的所有组合上进行训练。网格搜索很容易编码，可以并行运行，不需要任何形式的调整。然而，在搜索空间中导航<em class="lb">是低效的<em class="lb"> </em>，因为它不使用在早期尝试中获得的信息。由于搜索可能不包括所有可能的设置，因此无法保证找到最佳值。</em></p><p id="a5b7" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">找到超参数的更有效的方法是通过基于模型的顺序优化(SMBO)。这里，性能指标/误差测量的替代模型可以适合先前的尝试并指示最佳点。</p><p id="0901" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">一种常见的 SMBO 方法是贝叶斯优化，它可以通过高斯过程回归来解释。使用高斯过程回归，附加信息(新的超参数设置信息)被添加到已经采样的点的先验中。贝叶斯优化仅对优化少数超参数有效。然而，随着搜索空间的增加，性能会下降。此外，贝叶斯优化不能并行化，因为新学习的信息被添加到先验中。高斯过程回归的更多细节可在<a class="ae kc" rel="noopener" target="_blank" href="/quick-start-to-gaussian-process-regression-36d838810319">这里</a>找到，超参数优化方法的更详细回顾可在<a class="ae kc" href="https://medium.com/criteo-engineering/hyper-parameter-optimization-algorithms-2fe447525903" rel="noopener">这里</a>找到。</p><h1 id="ee56" class="lq lr iq bd ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn bi translated">利用 OCTIS 优化主题模型</h1><p id="55c0" class="pw-post-body-paragraph kd ke iq kf b kg mo ki kj kk mp km kn ko mq kq kr ks mr ku kv kw ms ky kz la ij bi translated">OCTIS 使用贝叶斯优化进行超参数优化。在我之前的帖子中，我在 BBC 数据集上训练了一个 NeuralLDA 模型，所以我在这里也将这样做。</p><p id="1da7" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">假设你已经安装了 OCTIS，我们直接用 Python 开始吧。同样，我们需要导入数据集和 NeuralLDA 模型:</p><pre class="mt mu mv mw gt mx my mz na aw nb bi"><span id="5726" class="nc lr iq my b gy nd ne l nf ng">from octis.dataset.dataset import Dataset<br/>from octis.models.NeuralLDA import NeuralLDA</span></pre><p id="9325" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">此外，我们安装优化空间、一致性(我们将其用作评估每个设置的性能指标)和优化器本身:</p><pre class="mt mu mv mw gt mx my mz na aw nb bi"><span id="ad55" class="nc lr iq my b gy nd ne l nf ng">from skopt.space.space import Real, Categorical, Integer<br/>from octis.evaluation_metrics.coherence_metrics import Coherence<br/>from octis.optimization.optimizer import Optimizer</span></pre><p id="5dd1" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">让我们获取数据集并初始化模型，我们差不多准备好了:</p><pre class="mt mu mv mw gt mx my mz na aw nb bi"><span id="aa5e" class="nc lr iq my b gy nd ne l nf ng">dataset = Dataset()<br/>dataset.fetch_dataset('BBC_news')</span><span id="ba33" class="nc lr iq my b gy nh ne l nf ng">model = NeuralLDA(num_topics=20)</span></pre><p id="c49c" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在前一篇文章中，我们发现 NeuralLDA 具有以下(超级)参数:</p><ul class=""><li id="b7dc" class="lc ld iq kf b kg kh kk kl ko le ks lf kw lg la ni li lj lk bi translated"><code class="fe nj nk nl my b">num_topics</code></li><li id="3c10" class="lc ld iq kf b kg ll kk lm ko ln ks lo kw lp la ni li lj lk bi translated"><code class="fe nj nk nl my b">activation</code></li><li id="1064" class="lc ld iq kf b kg ll kk lm ko ln ks lo kw lp la ni li lj lk bi translated"><code class="fe nj nk nl my b">dropout</code></li><li id="cc06" class="lc ld iq kf b kg ll kk lm ko ln ks lo kw lp la ni li lj lk bi translated"><code class="fe nj nk nl my b">learn_priors</code></li><li id="e22a" class="lc ld iq kf b kg ll kk lm ko ln ks lo kw lp la ni li lj lk bi translated"><code class="fe nj nk nl my b">batch_size</code></li><li id="5b3b" class="lc ld iq kf b kg ll kk lm ko ln ks lo kw lp la ni li lj lk bi translated"><code class="fe nj nk nl my b">lr</code></li><li id="6a6f" class="lc ld iq kf b kg ll kk lm ko ln ks lo kw lp la ni li lj lk bi translated"><code class="fe nj nk nl my b">momentum</code></li><li id="9229" class="lc ld iq kf b kg ll kk lm ko ln ks lo kw lp la ni li lj lk bi translated"><code class="fe nj nk nl my b">solver</code></li><li id="aa8b" class="lc ld iq kf b kg ll kk lm ko ln ks lo kw lp la ni li lj lk bi translated"><code class="fe nj nk nl my b">num_epochs</code></li><li id="de49" class="lc ld iq kf b kg ll kk lm ko ln ks lo kw lp la ni li lj lk bi translated"><code class="fe nj nk nl my b">reduce_on_plateau</code></li><li id="8d67" class="lc ld iq kf b kg ll kk lm ko ln ks lo kw lp la ni li lj lk bi translated"><code class="fe nj nk nl my b">prior_mean</code></li><li id="5f71" class="lc ld iq kf b kg ll kk lm ko ln ks lo kw lp la ni li lj lk bi translated"><code class="fe nj nk nl my b">prior_variance</code></li><li id="2f65" class="lc ld iq kf b kg ll kk lm ko ln ks lo kw lp la ni li lj lk bi translated"><code class="fe nj nk nl my b">num_layers</code></li><li id="c8bb" class="lc ld iq kf b kg ll kk lm ko ln ks lo kw lp la ni li lj lk bi translated"><code class="fe nj nk nl my b">num_neurons</code></li><li id="e23a" class="lc ld iq kf b kg ll kk lm ko ln ks lo kw lp la ni li lj lk bi translated"><code class="fe nj nk nl my b">num_samples</code></li><li id="ef2e" class="lc ld iq kf b kg ll kk lm ko ln ks lo kw lp la ni li lj lk bi translated"><code class="fe nj nk nl my b">use_partition</code></li></ul><h1 id="bc23" class="lq lr iq bd ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn bi translated"><strong class="ak">搜索空间</strong></h1><p id="4764" class="pw-post-body-paragraph kd ke iq kf b kg mo ki kj kk mp km kn ko mq kq kr ks mr ku kv kw ms ky kz la ij bi translated">理论上，所有这些变量都可以优化。但由于贝叶斯优化在高维搜索空间中表现不佳，我们将搜索空间维度限制为三个变量:</p><pre class="mt mu mv mw gt mx my mz na aw nb bi"><span id="7568" class="nc lr iq my b gy nd ne l nf ng">search_space = {"num_layers": Integer(1,3), <br/>                "num_neurons": Categorical({100, 200, 300}),<br/>                "dropout": Real(0.0, 0.95)}</span></pre><p id="a576" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">对于<code class="fe nj nk nl my b">Categorical</code>值，需要列出所有可能的值，而下限和上限只需要在<code class="fe nj nk nl my b">Real</code>和<code class="fe nj nk nl my b">Integer</code>空间中给出。注意，虽然<code class="fe nj nk nl my b">num_neurons</code>是整数，但我们在这里将其定义为分类值，因此只考虑这三个值，而不是 100 到 300 之间的任何整数值。</p><h1 id="9b6d" class="lq lr iq bd ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn bi translated"><strong class="ak">连贯性</strong></h1><p id="69d7" class="pw-post-body-paragraph kd ke iq kf b kg mo ki kj kk mp km kn ko mq kq kr ks mr ku kv kw ms ky kz la ij bi translated">此外，我们基于连贯分数来评估不同的超参数设置，连贯分数指示了主题中不同单词相互支持的程度。计算一致性分数有多种方法。OCTIS 的默认一致性度量是<code class="fe nj nk nl my b">c_npmi</code>。然而，我推荐使用<code class="fe nj nk nl my b">c_v</code>，因为它与人类的理解更相关[1]:</p><pre class="mt mu mv mw gt mx my mz na aw nb bi"><span id="ebac" class="nc lr iq my b gy nd ne l nf ng">coherence = Coherence(texts=dataset.get_corpus(), measure = ‘c_v’)</span></pre><h1 id="3058" class="lq lr iq bd ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn bi translated"><strong class="ak">优化</strong></h1><p id="31c0" class="pw-post-body-paragraph kd ke iq kf b kg mo ki kj kk mp km kn ko mq kq kr ks mr ku kv kw ms ky kz la ij bi translated">现在我们几乎准备好开始优化。首先，我们需要定义优化迭代的次数，这是时间和性能之间的权衡。通常，更多的迭代意味着更高的性能，但这是以更长的训练时间为代价的。此外，更多的超参数意味着更多的迭代轮次。经验法则是使用 15 倍的超参数作为迭代次数。我们有三个超参数，所以我们将使用 45 次迭代。</p><pre class="mt mu mv mw gt mx my mz na aw nb bi"><span id="0d8d" class="nc lr iq my b gy nd ne l nf ng">optimization_runs=45</span></pre><p id="12e3" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">此外，每次训练新设置时，神经网络都用随机权重初始化。为了获得每个设置的可靠结果，每个设置都应该运行几次。然后，我们将这些测量值的中间值作为每个设置的代表。再次，模型运行次数是时间和质量的权衡；总训练时间比只跑一次长<code class="fe nj nk nl my b">model_run</code>倍。我们将使用 5:</p><pre class="mt mu mv mw gt mx my mz na aw nb bi"><span id="b231" class="nc lr iq my b gy nd ne l nf ng">model_runs=5</span></pre><p id="0aa7" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在我们准备开始优化和保存结果。让我们为优化计时，以了解培训需要多长时间。这可能便于将来参考。</p><pre class="mt mu mv mw gt mx my mz na aw nb bi"><span id="ef96" class="nc lr iq my b gy nd ne l nf ng">optimizer=Optimizer()</span><span id="ce09" class="nc lr iq my b gy nh ne l nf ng">import time</span><span id="2fb0" class="nc lr iq my b gy nh ne l nf ng">start = time.time()<br/>optimization_result = optimizer.optimize(<br/>    model, dataset, coherence, search_space, number_of_call=optimization_runs, <br/>    model_runs=model_runs, save_models=True, <br/>    extra_metrics=None, # to keep track of other metrics<br/>    save_path='results/test_neuralLDA//')<br/>end = time.time()<br/>duration = end - start</span><span id="5d14" class="nc lr iq my b gy nh ne l nf ng">optimization_result.save_to_csv("results_neuralLDA.csv")</span><span id="dacc" class="nc lr iq my b gy nh ne l nf ng">print('Optimizing model took: ' + str(round(duration)) + ' seconds.')</span></pre><p id="d06f" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">当模型正在训练时，您应该会在控制台中看到类似下图的内容:</p><figure class="mt mu mv mw gt jr gh gi paragraph-image"><div class="gh gi nm"><img src="../Images/94b2712279aace9b9cd12e8b6682c6bb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1050/format:webp/1*jIS7RV3jr5yr6opuVBsXwQ.png"/></div></figure><pre class="mt mu mv mw gt mx my mz na aw nb bi"><span id="8ed9" class="nc lr iq my b gy nd ne l nf ng">Epoch: [49/100] Samples: [76293/155700] Train Loss: 922.4048726517341 Time: 0:00:00.465756<br/>Epoch: [49/100] Samples: [334/33400] Validation Loss: 920.509180857036 Time: 0:00:00.027928<br/>Epoch: [50/100] Samples: [77850/155700] Train Loss: 922.2904436115125 Time: 0:00:00.490685<br/>Epoch: [50/100] Samples: [334/33400] Validation Loss: 920.9605340101048 Time: 0:00:00.027925<br/>Epoch: [51/100] Samples: [79407/155700] Train Loss: 927.4208815028902 Time: 0:00:00.435834<br/>Epoch: [51/100] Samples: [334/33400] Validation Loss: 920.7156858158683 Time: 0:00:00.026928<br/>Early stopping<br/>Optimizing model took: 6922 seconds.</span></pre><p id="d066" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">所以，用了不到两个小时。不算太坏。</p><h1 id="71f6" class="lq lr iq bd ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn bi translated">分析</h1><p id="e5d9" class="pw-post-body-paragraph kd ke iq kf b kg mo ki kj kk mp km kn ko mq kq kr ks mr ku kv kw ms ky kz la ij bi translated">现在，我们准备分析结果。是一本信息量很大的词典。</p><pre class="mt mu mv mw gt mx my mz na aw nb bi"><span id="d4b3" class="nc lr iq my b gy nd ne l nf ng">import json<br/>results = json.load(open(“results/test_neuralLDA/result.json”,’r’))</span><span id="5651" class="nc lr iq my b gy nh ne l nf ng">results.keys()</span><span id="e0e6" class="nc lr iq my b gy nh ne l nf ng">&gt;&gt;&gt; dict_keys(['dataset_name', 'dataset_path', 'is_cached', 'kernel', 'acq_func', 'surrogate_model', 'optimization_type', 'model_runs', 'save_models', 'save_step', 'save_name', 'save_path', 'early_stop', 'early_step', 'plot_model', 'plot_best_seen', 'plot_name', 'log_scale_plot', 'search_space', 'model_name', 'model_attributes', 'use_partitioning', 'metric_name', 'extra_metric_names', 'metric_attributes', 'extra_metric_attributes', 'current_call', 'number_of_call', 'random_state', 'x0', 'y0', 'n_random_starts', 'initial_point_generator', 'topk', 'time_eval', 'dict_model_runs', 'f_val', 'x_iters'])</span></pre><p id="cad2" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">而不是像以前那样测量时间。通过使用<code class="fe nj nk nl my b">'time_eval'</code>，我们可以看到每个模型的时间(/总时间):</p><pre class="mt mu mv mw gt mx my mz na aw nb bi"><span id="c153" class="nc lr iq my b gy nd ne l nf ng">results[‘time_eval’]</span><span id="f8aa" class="nc lr iq my b gy nh ne l nf ng">&gt;&gt;&gt; [184.53835153579712,<br/> 148.67225646972656,<br/> 109.24449896812439,<br/> 256.08221530914307,<br/> 154.24360704421997,<br/> 109.99915742874146,<br/> 160.35804772377014,<br/> 151.49168038368225,<br/> 155.40837001800537,<br/> 187.06181716918945,<br/> 130.2092640399933,<br/> 147.94106578826904,<br/> 123.84417510032654,<br/> 144.77754926681519,<br/> 161.64789295196533,<br/> 155.76212072372437,<br/> 135.94082117080688,<br/> 135.95739817619324,<br/> 173.89717316627502,<br/> 157.23684573173523,<br/> 163.68361639976501,<br/> 162.50231456756592,<br/> 149.47536158561707,<br/> 128.70402908325195,<br/> 160.75109601020813,<br/> 175.52507424354553,<br/> 172.95005679130554,<br/> 153.52869629859924,<br/> 186.61110424995422,<br/> 157.76874375343323,<br/> 191.34919786453247,<br/> 140.3497931957245,<br/> 132.05838179588318,<br/> 151.3189136981964,<br/> 153.88639116287231,<br/> 150.12475419044495,<br/> 178.27711153030396,<br/> 140.40700840950012,<br/> 94.96378469467163,<br/> 174.16378569602966,<br/> 92.2694320678711,<br/> 151.19830918312073,<br/> 152.21132159233093,<br/> 176.6853895187378,<br/> 147.0237419605255]</span><span id="e816" class="nc lr iq my b gy nh ne l nf ng">sum(results[‘time_eval’])</span><span id="e4f2" class="nc lr iq my b gy nh ne l nf ng">&gt;&gt;&gt; 6922.101717710495</span></pre><p id="6d24" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><code class="fe nj nk nl my b">'f_val'</code>显示不同运行中每个训练设置的中值。</p><pre class="mt mu mv mw gt mx my mz na aw nb bi"><span id="8a38" class="nc lr iq my b gy nd ne l nf ng">results[“f_val”]</span><span id="9704" class="nc lr iq my b gy nh ne l nf ng">&gt;&gt;&gt; [0.4534176015984939,<br/> 0.4955475245370141,<br/> 0.38868674378398477,<br/> 0.4024525856722283,<br/> 0.47846034921413033,<br/> 0.3844368770533101,<br/> 0.4581705543253348,<br/> 0.4126574046864957,<br/> 0.4434880465386791,<br/> 0.49187868514620553,<br/> 0.5102920794834441,<br/> 0.48808652324541707,<br/> 0.4935514490937605,<br/> 0.4858721622773336,<br/> 0.4952236036028558,<br/> 0.465241037841993,<br/> 0.4881279220028529,<br/> 0.5023211922031112,<br/> 0.4990924448499987,<br/> 0.5024987857267208,<br/> 0.4770480035332,<br/> 0.5067195917253968,<br/> 0.4898287484351199,<br/> 0.4550063558556884,<br/> 0.4748375121047843,<br/> 0.497097248313755,<br/> 0.5034682220702588,<br/> 0.48220918244287975,<br/> 0.486563299332411,<br/> 0.4845033510847734,<br/> 0.4564350945897047,<br/> 0.4893138798649524,<br/> 0.4832975678079854,<br/> 0.4791275238679498,<br/> 0.4723902248816144,<br/> 0.5060161207074427,<br/> 0.49878190002894496,<br/> 0.4760453248044104,<br/> 0.47155389774180045,<br/> 0.5026397878110507,<br/> 0.40795520033104554,<br/> 0.5018909026526471,<br/> 0.5056155984233146,<br/> 0.4999288519956374,<br/> 0.49207181719944526]</span></pre><p id="085c" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这不是很直观，所以让我们画出来:</p><pre class="mt mu mv mw gt mx my mz na aw nb bi"><span id="23b0" class="nc lr iq my b gy nd ne l nf ng">import matplotlib.pyplot as plt</span><span id="99e2" class="nc lr iq my b gy nh ne l nf ng">plt.xlabel(‘Iteration’)<br/>plt.ylabel(‘Coherence score (c_v)’)<br/>plt.title(‘Median coherence score per iteration’)<br/>plt.plot(results[“f_val”])<br/>plt.show()</span></pre><figure class="mt mu mv mw gt jr gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/88f238ff7600c29d2e550ddb6cbe71cd.png" data-original-src="https://miro.medium.com/v2/resize:fit:786/format:webp/1*an4Ub2-SXOvFvRdnZjVu-A.png"/></div></figure><p id="96d3" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">从这个图中，我们可以看到模型并没有从高迭代次数中受益。最大中值得分出现在第 11 次迭代中，并且具有 0.51 的中值一致性得分:</p><pre class="mt mu mv mw gt mx my mz na aw nb bi"><span id="2721" class="nc lr iq my b gy nd ne l nf ng">results[ ‘f_val’].index(max(results[ ‘f_val’]))<br/>&gt;&gt;&gt; 10</span><span id="dee2" class="nc lr iq my b gy nh ne l nf ng">results["f_val"][10]<br/>&gt;&gt;&gt; 0.5102920794834441</span></pre><p id="70fe" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在，为了找到在第 11 次迭代中使用的设置，我们使用<code class="fe nj nk nl my b">'x_iters’</code>。这是一个包含已优化参数的字典:</p><pre class="mt mu mv mw gt mx my mz na aw nb bi"><span id="f2bc" class="nc lr iq my b gy nd ne l nf ng">results[‘x_iters’].keys()</span><span id="a4f2" class="nc lr iq my b gy nh ne l nf ng">&gt;&gt;&gt; dict_keys(['dropout', 'num_layers', 'num_neurons'])</span></pre><p id="acaa" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">最佳超参数设置为:</p><pre class="mt mu mv mw gt mx my mz na aw nb bi"><span id="7926" class="nc lr iq my b gy nd ne l nf ng">print([results[‘x_iters’][parameter][10] for parameter in results[‘x_iters’].keys()])</span><span id="ad1b" class="nc lr iq my b gy nh ne l nf ng">&gt;&gt;&gt; [0.5672132691312042, 1, 200]</span></pre><p id="902a" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">因此，在给定迭代次数和主题数量的情况下，我们的搜索空间中的最佳设置是:</p><ul class=""><li id="f797" class="lc ld iq kf b kg kh kk kl ko le ks lf kw lg la ni li lj lk bi translated"><code class="fe nj nk nl my b">dropout</code> : 0.5672</li><li id="05ea" class="lc ld iq kf b kg ll kk lm ko ln ks lo kw lp la ni li lj lk bi translated"><code class="fe nj nk nl my b">num_layers</code> : 1</li><li id="d723" class="lc ld iq kf b kg ll kk lm ko ln ks lo kw lp la ni li lj lk bi translated"><code class="fe nj nk nl my b">num_neurons</code> : 200</li></ul><h1 id="5a3b" class="lq lr iq bd ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn bi translated">结论</h1><p id="6f8a" class="pw-post-body-paragraph kd ke iq kf b kg mo ki kj kk mp km kn ko mq kq kr ks mr ku kv kw ms ky kz la ij bi translated">就是这样！如果您已经完成了这篇文章的结尾，那么现在您已经准备好优化您自己的主题模型了。您对选择参数/搜索空间的数量有基本的了解，并知道如何用 Python 编程。</p><p id="9c86" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我和我的研究小组开发了新的主题建模算法，这些算法已经发表在 2021 年计算智能<a class="ae kc" href="https://attend.ieee.org/ssci-2021/" rel="noopener ugc nofollow" target="_blank">系列研讨会上。初步结果表明，与 OCTIS 涵盖的主题模型相比，这些模型具有较高的可解释性。在以后的文章中，我将解释这些模型的工作原理，并将它们与 OCTIS 现有的主题模型进行比较。</a></p><p id="7685" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">祝你在 OCTIS 中优化你自己的主题模型好运，如果你有任何问题/意见，请不要犹豫联系我。</p><p id="3304" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">此外，<a class="ae kc" href="https://colab.research.google.com/github/MIND-Lab/OCTIS/blob/master/examples/OCTIS_Optimizing_CTM.ipynb#scrollTo=er0UeANzLSkF" rel="noopener ugc nofollow" target="_blank">这里的</a>是到 Google Colab 的链接，其中一个 CTM 模型是由 OCTIS 开发人员优化的。</p></div><div class="ab cl no np hu nq" role="separator"><span class="nr bw bk ns nt nu"/><span class="nr bw bk ns nt nu"/><span class="nr bw bk ns nt"/></div><div class="ij ik il im in"><p id="0c8f" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如果你喜欢这项工作，你可能会对主题建模感兴趣。在这种情况下，您可能也会对以下内容感兴趣。</p><p id="7f60" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir">我们创建了一个新的主题建模算法，叫做 FLSA-W </strong>(官方页面是<a class="ae kc" href="https://ieeexplore.ieee.org/abstract/document/9660139" rel="noopener ugc nofollow" target="_blank">这里是</a>，但是你可以看到论文<a class="ae kc" href="https://pure.tue.nl/ws/portalfiles/portal/243684581/A_Comparative_Study_of_Fuzzy_Topic_Models_and_LDA_in_terms_of_Interpretability.pdf" rel="noopener ugc nofollow" target="_blank">这里是</a>)。</p><p id="2df8" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir">在几个开放数据集上，FLSA-W 的性能优于其他一流算法(如 LDA、ProdLDA、NMF、CTM 等)。</strong> <a class="ae kc" href="https://pure.tue.nl/ws/files/222725628/Pure_ExperimentalStudyOfFlsa_wForTopicModeling.pdf" rel="noopener ugc nofollow" target="_blank"> <strong class="kf ir">该作品</strong> </a> <strong class="kf ir">已提交，但尚未通过同行评审。</strong></p><p id="aad5" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir">如果想用 FLSA-W，可以下载</strong> <a class="ae kc" href="https://pypi.org/project/FuzzyTM/" rel="noopener ugc nofollow" target="_blank"> <strong class="kf ir">的 FuzzyTM 包</strong> </a> <strong class="kf ir">或者 Gensim 中的 flsamodel。</strong>如需引用，<a class="ae kc" href="https://ieeexplore.ieee.org/abstract/document/9882661?casa_token=UsYg7SvoSioAAAAA:3ltCVZexA9-lPveuGVeRDh5VQW6rw0pVRDxmYk39tXbx13u4OuB2sTEFZzIGJCkdRiZBg0eJ" rel="noopener ugc nofollow" target="_blank">请使用本文</a>。</p><h1 id="3c97" class="lq lr iq bd ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn bi translated"><strong class="ak">参考文献</strong></h1><p id="4949" class="pw-post-body-paragraph kd ke iq kf b kg mo ki kj kk mp km kn ko mq kq kr ks mr ku kv kw ms ky kz la ij bi translated">[1]罗德，m .，两者，a .，&amp;欣内堡，A. (2015 年 2 月)。探索话题连贯性测量的空间。第八届 ACM 网络搜索和数据挖掘国际会议论文集<em class="lb">(第 399-408 页)。</em></p></div></div>    
</body>
</html>