<html>
<head>
<title>NLP With RAPIDS, Yes It’s Possible!</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">有急流的NLP，是的，这是可能的！</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/nlp-with-rapids-yes-its-possible-1830287210ed?source=collection_archive---------18-----------------------#2021-04-10">https://towardsdatascience.com/nlp-with-rapids-yes-its-possible-1830287210ed?source=collection_archive---------18-----------------------#2021-04-10</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="9e17" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">句子嵌入的主成分分析</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi kf"><img src="../Images/ad03208c9f123962a074c5cf9c5d91a4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1256/format:webp/1*2e6eRUmfkYCbAOGidPvBZA.png"/></div><p class="kn ko gj gh gi kp kq bd b be z dk translated">图片由作者提供。<a class="ae kr" href="https://www.kaggle.com/louise2001/rapids-pca-for-sentenceembeddings?scriptVersionId=59318051" rel="noopener ugc nofollow" target="_blank">源代码</a></p></figure></div><div class="ab cl ks kt hu ku" role="separator"><span class="kv bw bk kw kx ky"/><span class="kv bw bk kw kx ky"/><span class="kv bw bk kw kx"/></div><div class="ij ik il im in"><p id="e8cf" class="pw-post-body-paragraph kz la iq lb b lc ld jr le lf lg ju lh li lj lk ll lm ln lo lp lq lr ls lt lu ij bi translated"><em class="lv">在这篇博文中，我将展示如何对复杂数据进行常规的数据分析，比如变形金刚中的句子嵌入。完整的笔记本可以在</em> <a class="ae kr" href="https://www.kaggle.com/louise2001/rapids-pca-for-sentenceembeddings" rel="noopener ugc nofollow" target="_blank"> <em class="lv">这里找到</em> </a> <em class="lv">。</em></p></div><div class="ab cl ks kt hu ku" role="separator"><span class="kv bw bk kw kx ky"/><span class="kv bw bk kw kx ky"/><span class="kv bw bk kw kx"/></div><div class="ij ik il im in"><p id="b5ef" class="pw-post-body-paragraph kz la iq lb b lc ld jr le lf lg ju lh li lj lk ll lm ln lo lp lq lr ls lt lu ij bi translated">在Kaggle的<a class="ae kr" href="https://www.kaggle.com/c/actuarial-loss-estimation" rel="noopener ugc nofollow" target="_blank"> <strong class="lb ir">精算损失竞赛</strong> </a>中，我们要<strong class="lb ir">预测工伤保险索赔成本</strong>。我们被赋予一些非常经典的特征，比如年龄、薪水、需要抚养的孩子数量等等。，但也有一个简短的文字描述的性质和情况的事故，我们必须预测最终的费用索赔。</p><p id="4169" class="pw-post-body-paragraph kz la iq lb b lc ld jr le lf lg ju lh li lj lk ll lm ln lo lp lq lr ls lt lu ij bi translated">这项工作的特殊性在于，首先，我在GPU上与 <a class="ae kr" href="https://rapids.ai/" rel="noopener ugc nofollow" target="_blank"> <strong class="lb ir"> RAPIDS </strong> </a>进行了端到端的执行，它们的库cuPY、cuDF和cuML使你能够在GPU上重现每一个Numpy、Pandas或Scikit-Learn操作；第二，我试图<strong class="lb ir">仅基于文本数据(伤害描述)</strong>提取关于数字目标(最终索赔成本)的信息。</p></div><div class="ab cl ks kt hu ku" role="separator"><span class="kv bw bk kw kx ky"/><span class="kv bw bk kw kx ky"/><span class="kv bw bk kw kx"/></div><div class="ij ik il im in"><h1 id="51a6" class="lw lx iq bd ly lz ma mb mc md me mf mg jw mh jx mi jz mj ka mk kc ml kd mm mn bi translated">环境设置</h1><p id="05be" class="pw-post-body-paragraph kz la iq lb b lc mo jr le lf mp ju lh li mq lk ll lm mr lo lp lq ms ls lt lu ij bi translated">句子嵌入将通过PyTorch转换器生成，然后在RAPIDS上通过主成分分析进行处理。</p><p id="d31a" class="pw-post-body-paragraph kz la iq lb b lc ld jr le lf lg ju lh li lj lk ll lm ln lo lp lq lr ls lt lu ij bi translated">首先，你必须确保你已经安装了<a class="ae kr" href="https://rapids.ai/start.html" rel="noopener ugc nofollow" target="_blank"> RAPIDS环境</a>(我推荐0.18版本)。我使用的是ZBook Studio和NVIDIA RTX5000，后者是Z by HP <a class="ae kr" href="https://www8.hp.com/us/en/workstations/industries/data-science.html#section=software-stack" rel="noopener ugc nofollow" target="_blank"> Data Science软件堆栈</a>附带的，所以我已经安装并正确设置了所有东西(我知道创建和管理环境可能会很痛苦)。如果您运行的是Kaggle内核，请务必添加<a class="ae kr" href="https://www.kaggle.com/cdeotte/rapids" rel="noopener ugc nofollow" target="_blank"> RAPIDS存储库</a>并安装RAPIDS，如下所示:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="mt mu l"/></div></figure><p id="aa62" class="pw-post-body-paragraph kz la iq lb b lc ld jr le lf lg ju lh li lj lk ll lm ln lo lp lq lr ls lt lu ij bi translated">然后，安装句子-transformers存储库和其他需求:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="mt mu l"/></div></figure><p id="0202" class="pw-post-body-paragraph kz la iq lb b lc ld jr le lf lg ju lh li lj lk ll lm ln lo lp lq lr ls lt lu ij bi translated">你都准备好了！</p></div><div class="ab cl ks kt hu ku" role="separator"><span class="kv bw bk kw kx ky"/><span class="kv bw bk kw kx ky"/><span class="kv bw bk kw kx"/></div><div class="ij ik il im in"><h1 id="5e14" class="lw lx iq bd ly lz ma mb mc md me mf mg jw mh jx mi jz mj ka mk kc ml kd mm mn bi translated">读取和处理数据</h1><p id="8cb9" class="pw-post-body-paragraph kz la iq lb b lc mo jr le lf mp ju lh li mq lk ll lm mr lo lp lq ms ls lt lu ij bi translated">读取数据并提取索赔描述非常简单。此外，文本数据已经被清理了，我们只需要将它转换成小写(大多数Bert衍生物是为未转换的文本数据而构建的)。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="mt mu l"/></div></figure></div><div class="ab cl ks kt hu ku" role="separator"><span class="kv bw bk kw kx ky"/><span class="kv bw bk kw kx ky"/><span class="kv bw bk kw kx"/></div><div class="ij ik il im in"><h1 id="b5d5" class="lw lx iq bd ly lz ma mb mc md me mf mg jw mh jx mi jz mj ka mk kc ml kd mm mn bi translated">Torch语句转换器的原始嵌入</h1><p id="109e" class="pw-post-body-paragraph kz la iq lb b lc mo jr le lf mp ju lh li mq lk ll lm mr lo lp lq ms ls lt lu ij bi translated">对于句子嵌入，我们希望将一个<strong class="lb ir">可变长度的输入文本</strong>映射到一个<strong class="lb ir">固定大小的密集向量</strong>。这可以通过句子转换器来实现:正如你所知，<a class="ae kr" href="https://arxiv.org/pdf/1706.03762.pdf" rel="noopener ugc nofollow" target="_blank">转换器是编码器-解码器架构</a>，其中编码器将输入映射到内部表示，然后由解码器映射到输出。在通过解码器以适应我们正在训练的任务之前，嵌入将是由编码器输出的输入的内部矢量化表示。</p><p id="1966" class="pw-post-body-paragraph kz la iq lb b lc ld jr le lf lg ju lh li lj lk ll lm ln lo lp lq lr ls lt lu ij bi translated"><a class="ae kr" href="https://www.sbert.net/" rel="noopener ugc nofollow" target="_blank">句子变形金刚</a>只是参与NLP的每个人都听说过的常见变形金刚，也就是说，Bert originally和它的所有派生版本:其他语言(CamemBert等)、robust (RoBerta)和light(distill Bert，DistilRoberta)版本。唯一的区别是他们接受训练的任务，包括在句子层面处理文本，而不是原来的单词层面。它们的用途非常广泛:</p><ul class=""><li id="a886" class="mv mw iq lb b lc ld lf lg li mx lm my lq mz lu na nb nc nd bi translated"><a class="ae kr" href="https://www.sbert.net/examples/applications/computing-embeddings/README.html" rel="noopener ugc nofollow" target="_blank">计算句子嵌入</a></li><li id="cf2d" class="mv mw iq lb b lc ne lf nf li ng lm nh lq ni lu na nb nc nd bi translated"><a class="ae kr" href="https://www.sbert.net/docs/usage/semantic_textual_similarity.html" rel="noopener ugc nofollow" target="_blank">语义文本相似度</a></li><li id="690d" class="mv mw iq lb b lc ne lf nf li ng lm nh lq ni lu na nb nc nd bi translated"><a class="ae kr" href="https://www.sbert.net/examples/applications/clustering/README.html" rel="noopener ugc nofollow" target="_blank">聚类</a></li><li id="1ab4" class="mv mw iq lb b lc ne lf nf li ng lm nh lq ni lu na nb nc nd bi translated"><a class="ae kr" href="https://www.sbert.net/examples/applications/paraphrase-mining/README.html" rel="noopener ugc nofollow" target="_blank">释义挖掘</a></li><li id="b31f" class="mv mw iq lb b lc ne lf nf li ng lm nh lq ni lu na nb nc nd bi translated"><a class="ae kr" href="https://www.sbert.net/examples/applications/parallel-sentence-mining/README.html" rel="noopener ugc nofollow" target="_blank">翻译句子挖掘</a></li><li id="7646" class="mv mw iq lb b lc ne lf nf li ng lm nh lq ni lu na nb nc nd bi translated"><a class="ae kr" href="https://www.sbert.net/examples/applications/semantic-search/README.html" rel="noopener ugc nofollow" target="_blank">语义搜索</a></li><li id="6197" class="mv mw iq lb b lc ne lf nf li ng lm nh lq ni lu na nb nc nd bi translated">等等。</li></ul><p id="b5bf" class="pw-post-body-paragraph kz la iq lb b lc ld jr le lf lg ju lh li lj lk ll lm ln lo lp lq lr ls lt lu ij bi translated">我选择的计算嵌入的模型是<a class="ae kr" href="https://huggingface.co/sentence-transformers/paraphrase-distilroberta-base-v1" rel="noopener ugc nofollow" target="_blank">distilloberta，用于释义评分</a>。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="mt mu l"/></div></figure><p id="7650" class="pw-post-body-paragraph kz la iq lb b lc ld jr le lf lg ju lh li lj lk ll lm ln lo lp lq lr ls lt lu ij bi translated">我为什么选择释义模式？嗯，这是一个纯粹的个人选择，请随意使用另一种模式。我觉得这个模型能更好地体现句子的整体意义，而不是过多地选择一个词或一个同义词(释义基本上就是这个意思)，这也是我的目的，因为这个描述非常简洁实用。</p><p id="1fa9" class="pw-post-body-paragraph kz la iq lb b lc ld jr le lf lg ju lh li lj lk ll lm ln lo lp lq lr ls lt lu ij bi translated">这实际上是一个相当大的模型，尽管是原来的罗伯塔更轻的版本:它仍然有超过8200万可训练的参数…</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="mt mu l"/></div></figure><p id="8aef" class="pw-post-body-paragraph kz la iq lb b lc ld jr le lf lg ju lh li lj lk ll lm ln lo lp lq lr ls lt lu ij bi translated">获得句子嵌入的整个过程实际上非常简单。</p><p id="a452" class="pw-post-body-paragraph kz la iq lb b lc ld jr le lf lg ju lh li lj lk ll lm ln lo lp lq lr ls lt lu ij bi translated">首先，对于每个输入的句子，您必须将单词映射到它们在模型使用的词汇中的id，添加<start>和<end>代币。</end></start></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="nk nl di nm bf nn"><div class="gh gi nj"><img src="../Images/9fad470980a0c6e2f4bcaac75ae4dbd7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*V1c80A-oVoDKVWdON87Jcg.png"/></div></div></figure><p id="c784" class="pw-post-body-paragraph kz la iq lb b lc ld jr le lf lg ju lh li lj lk ll lm ln lo lp lq lr ls lt lu ij bi translated">最终，我们必须在末尾添加<pad>代币，以便我们的输入符合模型最大输入序列长度。</pad></p><p id="7c63" class="pw-post-body-paragraph kz la iq lb b lc ld jr le lf lg ju lh li lj lk ll lm ln lo lp lq lr ls lt lu ij bi translated">在我的例子中，原始模型的最大序列长度是128，但我的输入(包括<start>和<end>代币)的最大长度是21，因此为了安全概括的目的，我将这个最大序列长度设置为25。这将为我们节省大量无用的计算和计算时间。</end></start></p><p id="ab33" class="pw-post-body-paragraph kz la iq lb b lc ld jr le lf lg ju lh li lj lk ll lm ln lo lp lq lr ls lt lu ij bi translated">然后，我们的模型将输入id映射到它们相应的令牌嵌入，它们是单词嵌入、位置嵌入(我们是第一个、第二个还是第三个单词？)和令牌类型嵌入(我们是一个普通的词吗？a <start>？).最后，我们只对特征进行归一化。</start></p><p id="c2e0" class="pw-post-body-paragraph kz la iq lb b lc ld jr le lf lg ju lh li lj lk ll lm ln lo lp lq lr ls lt lu ij bi translated">现在，对于每个代币，我们有一个初始的张量表示，我们把它馈送给编码器(我在这里将不详细讨论，关于这方面的更多信息，请参考介绍编码器-解码器体系结构的论文<a class="ae kr" href="https://arxiv.org/pdf/1706.03762.pdf" rel="noopener ugc nofollow" target="_blank"> <em class="lv">【注意是你所需要的全部】</em> </a>)。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="mt mu l"/></div></figure><p id="9c69" class="pw-post-body-paragraph kz la iq lb b lc ld jr le lf lg ju lh li lj lk ll lm ln lo lp lq lr ls lt lu ij bi translated">该编码器输出任何输入序列的768维张量表示。现在，我们该怎么处理它呢？首先，我们必须确保这些嵌入在回归模型中预测最终索偿金额时有用。这就意味着，理想情况下，要有正交的轴，轴上的坐标要有很强的解释力，还要能够将事故类型或受伤严重程度进行聚类。</p><p id="6f1b" class="pw-post-body-paragraph kz la iq lb b lc ld jr le lf lg ju lh li lj lk ll lm ln lo lp lq lr ls lt lu ij bi translated">不幸的是，情况远非如此:事实上，最初的罗伯塔释义模型是在一个庞大的折衷数据集上训练出来的，而我们的输入数据有很大的语义相似性，都是围绕健康展开的。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi no"><img src="../Images/540e886c0c3333a07bbe13290ebb1079.png" data-original-src="https://miro.medium.com/v2/resize:fit:784/format:webp/1*i7TYBShqKlJKKv2-q6SZpg.png"/></div></figure><p id="b4b1" class="pw-post-body-paragraph kz la iq lb b lc ld jr le lf lg ju lh li lj lk ll lm ln lo lp lq lr ls lt lu ij bi translated">对于规范化句子嵌入的所有768个dim，我计算了90_000个文本样本之间的坐标方差。从这个柱状图中可以看出，大多数方差都非常低:作为参考，U([0，1])的方差是1/12=0.0833。</p><p id="7d52" class="pw-post-body-paragraph kz la iq lb b lc ld jr le lf lg ju lh li lj lk ll lm ln lo lp lq lr ls lt lu ij bi translated">因此，我的陈述很可能非常相似。</p></div><div class="ab cl ks kt hu ku" role="separator"><span class="kv bw bk kw kx ky"/><span class="kv bw bk kw kx ky"/><span class="kv bw bk kw kx"/></div><div class="ij ik il im in"><h1 id="69fc" class="lw lx iq bd ly lz ma mb mc md me mf mg jw mh jx mi jz mj ka mk kc ml kd mm mn bi translated">使用RAPIDS处理来自PCA的嵌入</h1><p id="4dc5" class="pw-post-body-paragraph kz la iq lb b lc mo jr le lf mp ju lh li mq lk ll lm mr lo lp lq ms ls lt lu ij bi translated">此时，我们面临两个问题:第一，嵌入太大(768个组件，当我们只有12个额外的培训功能时)，我们更希望它们相对于我们的目标(最终产生的索赔成本)带来更多的可解释性和可分离性。</p><p id="5671" class="pw-post-body-paragraph kz la iq lb b lc ld jr le lf lg ju lh li lj lk ll lm ln lo lp lq lr ls lt lu ij bi translated"><a class="ae kr" href="https://en.wikipedia.org/wiki/Principal_component_analysis" rel="noopener ugc nofollow" target="_blank"> PCA </a>用于<a class="ae kr" href="https://en.wikipedia.org/wiki/Exploratory_data_analysis" rel="noopener ugc nofollow" target="_blank">勘探数据分析</a>和制作<a class="ae kr" href="https://en.wikipedia.org/wiki/Predictive_modeling" rel="noopener ugc nofollow" target="_blank">预测模型</a>。它通过将每个数据点仅投影到前几个主成分上来执行<a class="ae kr" href="https://en.wikipedia.org/wiki/Dimensionality_reduction" rel="noopener ugc nofollow" target="_blank">降维</a>，以获得较低维的数据，同时保留尽可能多的数据变化。第一主成分可以等效地定义为使投影数据的方差最大化的方向。然后，第二分量是与使数据方差最大化的第一分量正交的方向，等等。</p><p id="1e37" class="pw-post-body-paragraph kz la iq lb b lc ld jr le lf lg ju lh li lj lk ll lm ln lo lp lq lr ls lt lu ij bi translated">该过程以数据方差的可解释能力的递减顺序迭代地给出正交向量。在第一n个分量上投影数据通过在R^n投影它来执行降维，尽管保留尽可能多的数据方差。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="mt mu l"/></div></figure><p id="d2b4" class="pw-post-body-paragraph kz la iq lb b lc ld jr le lf lg ju lh li lj lk ll lm ln lo lp lq lr ls lt lu ij bi translated">这里要注意的是，我已经对来自train和test数据集的所有声明描述嵌入执行了主成分分析，以获得更好的适合这两个数据集的拟合。第一主成分占总数据方差的7.6%，第二主成分占3.9%。它们总共解释了11.5%的原始嵌入可变性。</p><p id="5582" class="pw-post-body-paragraph kz la iq lb b lc ld jr le lf lg ju lh li lj lk ll lm ln lo lp lq lr ls lt lu ij bi translated">现在，当我试图在这个2-d空间中可视化数据点投影时，我有了一个惊人的惊喜:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi np"><img src="../Images/0dea6370dc47eb2248e42bc7f3f0065e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1346/format:webp/1*xbKnqY2RT2_2fjpjYzLIKQ.png"/></div></figure><p id="a65c" class="pw-post-body-paragraph kz la iq lb b lc ld jr le lf lg ju lh li lj lk ll lm ln lo lp lq lr ls lt lu ij bi translated">很有意思吧。现在，有几种解释可以从中得出。首先，我们非常清楚地看到正交可变性方向。其次，似乎有2个集群的数据点，合并部分较低的价值在2轴。</p></div><div class="ab cl ks kt hu ku" role="separator"><span class="kv bw bk kw kx ky"/><span class="kv bw bk kw kx ky"/><span class="kv bw bk kw kx"/></div><div class="ij ik il im in"><h1 id="9cad" class="lw lx iq bd ly lz ma mb mc md me mf mg jw mh jx mi jz mj ka mk kc ml kd mm mn bi translated">目标可解释性</h1><p id="cd82" class="pw-post-body-paragraph kz la iq lb b lc mo jr le lf mp ju lh li mq lk ll lm mr lo lp lq ms ls lt lu ij bi translated">基于这一观察，我想知道这两个集群是否能给目标公司带来预测能力，也就是最终产生的索偿成本。因此，我决定绘制所有的训练数据点投影(我们显然没有测试数据点的目标值…)，同时根据目标值对它们进行染色。</p><p id="b1b5" class="pw-post-body-paragraph kz la iq lb b lc ld jr le lf lg ju lh li lj lk ll lm ln lo lp lq lr ls lt lu ij bi translated">在这一点上，我遇到了一个可行性问题:目标值显然是下界的(索赔成本必然是正的)，但不是上界的，其中一些非常极端，以至于它使我的颜色图爆炸，大多数值被压到底部，少数值飙升到顶部颜色。因此，我决定砍掉我最上面的五分之一的数据，以便得到一个更有规律的分布。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="mt mu l"/></div></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi kf"><img src="../Images/ad03208c9f123962a074c5cf9c5d91a4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1256/format:webp/1*2e6eRUmfkYCbAOGidPvBZA.png"/></div><p class="kn ko gj gh gi kp kq bd b be z dk translated">图片由作者提供。<a class="ae kr" href="https://www.kaggle.com/louise2001/rapids-pca-for-sentenceembeddings?scriptVersionId=59318051" rel="noopener ugc nofollow" target="_blank">源代码</a></p></figure><p id="7f79" class="pw-post-body-paragraph kz la iq lb b lc ld jr le lf lg ju lh li lj lk ll lm ln lo lp lq lr ls lt lu ij bi translated">这真是一个令人印象深刻的结果，因为它说明了在没有任何附加特征的情况下，主成分分析对文本嵌入所提供的目标的聚类能力。在这里，我们非常清楚地看到，心脏的右半部分占较高的成本，而左半部分占较低的索赔。</p></div><div class="ab cl ks kt hu ku" role="separator"><span class="kv bw bk kw kx ky"/><span class="kv bw bk kw kx ky"/><span class="kv bw bk kw kx"/></div><div class="ij ik il im in"><p id="64e8" class="pw-post-body-paragraph kz la iq lb b lc ld jr le lf lg ju lh li lj lk ll lm ln lo lp lq lr ls lt lu ij bi translated"><em class="lv">感谢你阅读我！你可以在这里</em>  <em class="lv">找到完整源码Kaggle笔记本</em> <a class="ae kr" href="https://www.kaggle.com/louise2001/rapids-pca-for-sentenceembeddings" rel="noopener ugc nofollow" target="_blank"> <em class="lv">。</em></a></p></div></div>    
</body>
</html>