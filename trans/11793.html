<html>
<head>
<title>The right way to compute your Shapley Values</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">计算Shapley值的正确方法</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/the-right-way-to-compute-your-shapley-values-cfea30509254?source=collection_archive---------8-----------------------#2021-11-24">https://towardsdatascience.com/the-right-way-to-compute-your-shapley-values-cfea30509254?source=collection_archive---------8-----------------------#2021-11-24</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="1bc7" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsdatascience.com/tagged/tips-and-tricks" rel="noopener" target="_blank">提示和技巧</a></h2><div class=""/><div class=""><h2 id="8338" class="pw-subtitle-paragraph jw iz iq bd b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn dk translated"><em class="ko">比较SHAP和ACV两个图书馆对传统机器学习用例提供的基于Shapley值的解释。</em></h2></div><p id="4baa" class="pw-post-body-paragraph kp kq iq kr b ks kt ka ku kv kw kd kx ky kz la lb lc ld le lf lg lh li lj lk ij bi translated"><em class="ll">量子计量——MAIF</em></p></div><div class="ab cl lm ln hu lo" role="separator"><span class="lp bw bk lq lr ls"/><span class="lp bw bk lq lr ls"/><span class="lp bw bk lq lr"/></div><div class="ij ik il im in"><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div role="button" tabindex="0" class="lz ma di mb bf mc"><div class="gh gi lt"><img src="../Images/9c8eebd4c3e444eed1c87412786f6d09.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*wj8d50zXGpUmjDEH"/></div></div><p class="mf mg gj gh gi mh mi bd b be z dk translated">照片由<a class="ae mj" href="https://unsplash.com/@single_lens_reflex?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">丹伯顿</a>在<a class="ae mj" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure></div><div class="ab cl lm ln hu lo" role="separator"><span class="lp bw bk lq lr ls"/><span class="lp bw bk lq lr ls"/><span class="lp bw bk lq lr"/></div><div class="ij ik il im in"><h1 id="117b" class="mk ml iq bd mm mn mo mp mq mr ms mt mu kf mv kg mw ki mx kj my kl mz km na nb bi translated">介绍</h1><p id="4102" class="pw-post-body-paragraph kp kq iq kr b ks nc ka ku kv nd kd kx ky ne la lb lc nf le lf lg ng li lj lk ij bi translated">随着集成模型和深度学习的出现，人工智能进入了一个黄金时代，机器学习社区正在进入可靠人工智能的时代。在这种背景下，随着越来越多的工具被开发出来“打开黑盒”，人们对算法可解释性的兴趣越来越大[1]。在这些工具中，有一个似乎是至高无上的:SHAP。SHAP是一种博弈论方法，使用Shapley值的有效计算来解释任何机器学习模型的输出[2]。简而言之，Shapley值估计每个变量对模型预测的<strong class="kr ja">贡献</strong>。如果你曾经试图解释黑盒模型，我敢打赌你用过SHAP…</p><p id="6b1c" class="pw-post-body-paragraph kp kq iq kr b ks kt ka ku kv kw kd kx ky kz la lb lc ld le lf lg lh li lj lk ij bi translated">尽管SHAP很受欢迎，但它也有一些局限性[3]:</p><ul class=""><li id="3fca" class="nh ni iq kr b ks kt kv kw ky nj lc nk lg nl lk nm nn no np bi translated">对独热编码分类特征的不良处理导致错误的Shapley值:特征的贡献不是独热列的贡献的总和！</li><li id="16fe" class="nh ni iq kr b ks nq kv nr ky ns lc nt lg nu lk nm nn no np bi translated">要素相关时Shapley值的有偏近似(甚至错误)。</li></ul><p id="558d" class="pw-post-body-paragraph kp kq iq kr b ks kt ka ku kv kw kd kx ky kz la lb lc ld le lf lg lh li lj lk ij bi translated">当试图理解模型如何做出预测时，使用可靠的方法是很重要的。在这方面，<a class="ae mj" href="https://github.com/salimamoukou/acv00" rel="noopener ugc nofollow" target="_blank"> ACV图书馆</a> ( <em class="ll">主动变量联盟)</em>提供了一种计算Shapley值的新方法来解决这些问题，这得益于:</p><ul class=""><li id="f2ef" class="nh ni iq kr b ks kt kv kw ky nj lc nk lg nl lk nm nn no np bi translated">一种计算特征联盟贡献的严格方法，解决了一次性编码问题。</li><li id="4a8a" class="nh ni iq kr b ks nq kv nr ky ns lc nt lg nu lk nm nn no np bi translated">特征相关时计算Shapley值的稳健方法。</li></ul><p id="2cf8" class="pw-post-body-paragraph kp kq iq kr b ks kt ka ku kv kw kd kx ky kz la lb lc ld le lf lg lh li lj lk ij bi translated">为了突出这两个库之间的差异，我们对<a class="ae mj" href="https://www.kaggle.com/blastchar/telco-customer-churn" rel="noopener ugc nofollow" target="_blank">电信客户流失数据集</a>进行了比较研究，该数据集已经在可解释的人工智能文献中使用[4]。</p></div><div class="ab cl lm ln hu lo" role="separator"><span class="lp bw bk lq lr ls"/><span class="lp bw bk lq lr ls"/><span class="lp bw bk lq lr"/></div><div class="ij ik il im in"><h1 id="263b" class="mk ml iq bd mm mn mo mp mq mr ms mt mu kf mv kg mw ki mx kj my kl mz km na nb bi translated">研究大纲</h1><p id="96ee" class="pw-post-body-paragraph kp kq iq kr b ks nc ka ku kv nd kd kx ky ne la lb lc nf le lf lg ng li lj lk ij bi translated">当在具有高比例的一次性编码分类要素和相关要素的数据集上训练模型时，SHAP和ACV之间的差异将非常显著。由于18个特征中有10个是分类的，并且总体上具有很高的相关性，所以电信客户流失数据检查了这两个框。</p><p id="3ad2" class="pw-post-body-paragraph kp kq iq kr b ks kt ka ku kv kw kd kx ky kz la lb lc ld le lf lg lh li lj lk ij bi translated">现在我们已经讨论了这个问题，我们想训练一个黑盒模型并回答这个问题:“ACV比SHAP对我的模型提供了更好的解释吗？”。根据定义，这是一个棘手的问题，因为不可能知道和理解黑匣子是如何工作的(这就是为什么我们首先需要SHAP！).那么如何比较2种解释的质量呢？</p><p id="a61a" class="pw-post-body-paragraph kp kq iq kr b ks kt ka ku kv kw kd kx ky kz la lb lc ld le lf lg lh li lj lk ij bi translated">在这项研究中，我们提出了一个简单的方法来回答这两个问题:如果我们不能看到黑盒内部，我们可以首先尝试控制我们的数据的生成关系。如果我们知道数据集<strong class="kr ja"> <em class="ll"> (X，y) </em> </strong>的“真实”生成函数<em class="ll"> </em> <strong class="kr ja"> <em class="ll"> y= g(x) </em> </strong>，并且我们假设我们的黑盒模型<strong class="kr ja"><em class="ll">【yᵖʳᵉᵈ= f(x)</em></strong>合理地忠实于<strong class="kr ja"><em class="ll">【g</em></strong>，那么衡量对<strong class="kr ja"> <em class="ll"> f </em> </strong>的解释质量的一个好方法是</p><p id="50ec" class="pw-post-body-paragraph kp kq iq kr b ks kt ka ku kv kw kd kx ky kz la lb lc ld le lf lg lh li lj lk ij bi translated">为了访问我们的数据的生成功能，我们训练了一个可解释的加法模型<strong class="kr ja"><em class="ll">g(x)= g</em></strong>₁<strong class="kr ja"><em class="ll">(x</em></strong>₁<strong class="kr ja"><em class="ll">)+g</em></strong>₂<strong class="kr ja"><em class="ll">(x</em></strong>₂<strong class="kr ja">【t43)+⋯+g</strong>ₚ<strong class="kr ja"><em class="ll">(x</em>对于这一步，我们使用了在<a class="ae mj" href="https://github.com/interpretml/interpret" rel="noopener ugc nofollow" target="_blank"> InterpretML </a>库中实现的可解释增强机器(EBM)。由于它的附加性质，这个模型很容易解释，我们可以很容易地访问将预测与每个变量联系起来的函数。换句话说，由于EBM，我们可以显示每个特征对预测的贡献<strong class="kr ja"><em class="ll">【gᵢ(xᵢ】</em></strong>的图形表示。例如，变量<em class="ll">每月费用</em>的“实际”贡献如下图所示:</strong></p><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div role="button" tabindex="0" class="lz ma di mb bf mc"><div class="gh gi nv"><img src="../Images/57b0dcf2ebd44bf41b1598cd601d6ac3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OrDYxPVYjnz3pCA_BqYwHA.png"/></div></div><p class="mf mg gj gh gi mh mi bd b be z dk translated">月收费对循证医学预测的实际贡献。图片作者。</p></figure><p id="6699" class="pw-post-body-paragraph kp kq iq kr b ks kt ka ku kv kw kd kx ky kz la lb lc ld le lf lg lh li lj lk ij bi translated">概括来说，下面是我们使用的协议的简要概述:</p><ol class=""><li id="5c31" class="nh ni iq kr b ks kt kv kw ky nj lc nk lg nl lk nw nn no np bi translated">在训练集<strong class="kr ja"><em class="ll">【xᵗʳ,yᵗʳ】</em></strong>上训练EBM(可解释助推机)获得<strong class="kr ja"> <em class="ll"> g </em> </strong>。</li><li id="18ac" class="nh ni iq kr b ks nq kv nr ky ns lc nt lg nu lk nw nn no np bi translated">用预测<strong class="kr ja"> <em class="ll">替换<strong class="kr ja"><em class="ll">【yᵗʳ】</em></strong>【yᵉᵇᵐ= g(x)+ϵ</em></strong>(添加了一些噪声<strong class="kr ja"><em class="ll"/></strong>以模拟真实情况)。</li><li id="95a9" class="nh ni iq kr b ks nq kv nr ky ns lc nt lg nu lk nw nn no np bi translated">在<strong class="kr ja"><em class="ll">【xᵗʳ,yᵗʳ】</em></strong>(我们使用Scikit Learn的GradientBoostingClassifier)上训练一个传统的黑盒梯度提升，得到<strong class="kr ja"> <em class="ll"> f </em> </strong>(我们的模型在这个半合成数据集上表现异常好:ROC_AUC=0.94)。</li><li id="7c96" class="nh ni iq kr b ks nq kv nr ky ns lc nt lg nu lk nw nn no np bi translated">用SHAP和ACV解释梯度推进模型f。</li></ol></div><div class="ab cl lm ln hu lo" role="separator"><span class="lp bw bk lq lr ls"/><span class="lp bw bk lq lr ls"/><span class="lp bw bk lq lr"/></div><div class="ij ik il im in"><h1 id="beec" class="mk ml iq bd mm mn mo mp mq mr ms mt mu kf mv kg mw ki mx kj my kl mz km na nb bi translated">沙普利价值观与SHAP和ACV</h1><p id="7dc5" class="pw-post-body-paragraph kp kq iq kr b ks nc ka ku kv nd kd kx ky ne la lb lc nf le lf lg ng li lj lk ij bi translated">训练模型后，我们计算了两组不同的Shapley值:</p><ol class=""><li id="6dd4" class="nh ni iq kr b ks kt kv kw ky nj lc nk lg nl lk nw nn no np bi translated">使用来自SHAP的树解释器算法，将特征扰动设置为“树路径相关”,这被认为是处理变量之间的相关性。</li><li id="b5a4" class="nh ni iq kr b ks nq kv nr ky ns lc nt lg nu lk nw nn no np bi translated">使用ACV的ACV树算法，该算法声称在处理相关特征方面比SHAP做得更好。</li></ol><p id="8590" class="pw-post-body-paragraph kp kq iq kr b ks kt ka ku kv kw kd kx ky kz la lb lc ld le lf lg lh li lj lk ij bi translated">在详细验证这一说法之前，我们可以比较测试集(图2)上Shapley值的平均值，看看SHAP和ACV是否同意变量的全局重要性:</p><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div role="button" tabindex="0" class="lz ma di mb bf mc"><div class="gh gi nx"><img src="../Images/963c0764f15333523a6f79cc657350a6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*o8Qqj3H2EwnPw6pwvlPJpA.png"/></div></div><p class="mf mg gj gh gi mh mi bd b be z dk translated">SHAP和ACV的要素重要性(每个要素的shapley平均值)。图片作者。</p></figure><p id="deed" class="pw-post-body-paragraph kp kq iq kr b ks kt ka ku kv kw kd kx ky kz la lb lc ld le lf lg lh li lj lk ij bi translated">两个库似乎都同意哪个特性最重要。尽管如此，我们还是可以观察到一些差异，特别是<em class="ll">月度费用</em>和<em class="ll">总费用</em>的重要性，这是两个非常相关的数值变量(皮尔逊相关系数= 0.65)。为了以更精确的方式显示差异，我们可以计算每个特征的贡献分布之间的相对L1距离:</p><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div role="button" tabindex="0" class="lz ma di mb bf mc"><div class="gh gi ny"><img src="../Images/644dd6efe42016e533a310a9e704fb09.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Y6meNbEVI3l2shW5DR615g.png"/></div></div><p class="mf mg gj gh gi mh mi bd b be z dk translated">SHAP和ACV计算的Shapley值之间的平均相对距离。图片作者。</p></figure><p id="5735" class="pw-post-body-paragraph kp kq iq kr b ks kt ka ku kv kw kd kx ky kz la lb lc ld le lf lg lh li lj lk ij bi translated"><em class="ll">月度费用</em>和<em class="ll">总费用</em>的分布差异最大，其次是<em class="ll">合同</em>、<em class="ll">流媒体</em>和<em class="ll">技术支持</em>，这是3个一次性编码的分类变量。</p></div><div class="ab cl lm ln hu lo" role="separator"><span class="lp bw bk lq lr ls"/><span class="lp bw bk lq lr ls"/><span class="lp bw bk lq lr"/></div><div class="ij ik il im in"><h1 id="e424" class="mk ml iq bd mm mn mo mp mq mr ms mt mu kf mv kg mw ki mx kj my kl mz km na nb bi translated">SHAP和ACV之间的本地解释差异</h1><p id="de84" class="pw-post-body-paragraph kp kq iq kr b ks nc ka ku kv nd kd kx ky ne la lb lc nf le lf lg ng li lj lk ij bi translated">既然我们知道某些特征有不同的解释，让我们看看它对我们预测的实际解释意味着什么。为了测量解释之间的“差异”,我们可以计算每个观察值的SHAP和ACV贡献之间的肯德尔等级相关系数。为了研究“最不同”解释的预测，我们可以看看系数最低的解释。让我们来看看SHAP和ACV在其中一个预测上的区别:</p><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div role="button" tabindex="0" class="lz ma di mb bf mc"><div class="gh gi nz"><img src="../Images/c970f8a4a561bcf9b06c12824124dc4c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*If5ATiGhDk01SoDVA8zbtQ.png"/></div></div><p class="mf mg gj gh gi mh mi bd b be z dk translated">在客户端233上预测SHAP和ACV的10个最重要的贡献。图片作者。</p></figure><ul class=""><li id="21ee" class="nh ni iq kr b ks kt kv kw ky nj lc nk lg nl lk nm nn no np bi translated"><em class="ll">月度费用</em>对SHAP来说是最重要的正贡献(0.269)，但对ACV来说是负贡献(-0.186)。</li><li id="0d82" class="nh ni iq kr b ks nq kv nr ky ns lc nt lg nu lk nm nn no np bi translated">任期对ACV (-0.479)来说是最重要的贡献，但对SHAP (-0.048)来说仅排在第十位。</li><li id="78ff" class="nh ni iq kr b ks nq kv nr ky ns lc nt lg nu lk nm nn no np bi translated"><em class="ll">合同</em>对SHAP (-0.435)是第二重要的贡献，但对ACV (-0.054)仅是第十重要的贡献。</li></ul><p id="bd13" class="pw-post-body-paragraph kp kq iq kr b ks kt ka ku kv kw kd kx ky kz la lb lc ld le lf lg lh li lj lk ij bi translated">我们可以看到，对某些人来说，ACV给出的解释与SHAP给出的解释大相径庭。在模型用于可能影响个人的决策制定的情况下，这可能会产生一些现实生活中的后果。例如，如果你想改变模型的预测，知道像<em class="ll">月费</em>这样的变量是积极的还是消极的会导致相反的决定。类似地，知道像<em class="ll">任期</em>这样的变量对于模型来说是否是最重要的，可以极大地改变解释和随后的潜在决策。</p></div><div class="ab cl lm ln hu lo" role="separator"><span class="lp bw bk lq lr ls"/><span class="lp bw bk lq lr ls"/><span class="lp bw bk lq lr"/></div><div class="ij ik il im in"><h1 id="5e67" class="mk ml iq bd mm mn mo mp mq mr ms mt mu kf mv kg mw ki mx kj my kl mz km na nb bi translated">关注每月费用和总费用</h1><p id="5580" class="pw-post-body-paragraph kp kq iq kr b ks nc ka ku kv nd kd kx ky ne la lb lc nf le lf lg ng li lj lk ij bi translated">对功能<em class="ll">每月费用</em>和<em class="ll">总费用</em>的解释差异特别有趣，因为它们的相关性很高。当特征相关时，ACV声称能提供比SHAP更好更稳定的解释。让我们将<em class="ll">月度费用</em>的计算贡献(Shapley值)分布与可解释的Boosting机器提供的数据生成图进行比较:</p><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div role="button" tabindex="0" class="lz ma di mb bf mc"><div class="gh gi oa"><img src="../Images/fa996a1e57e31665c3ae14f8c6b619ed.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Jf_65r4tpijU_fvf12BIBg.png"/></div></div><p class="mf mg gj gh gi mh mi bd b be z dk translated">(上图):y与月度费用的“实际”关系<br/>(中图):SHAP月度费用特征贡献分布<br/>(下图):ACV月度费用特征贡献分布。图片作者。</p></figure><p id="d768" class="pw-post-body-paragraph kp kq iq kr b ks kt ka ku kv kw kd kx ky kz la lb lc ld le lf lg lh li lj lk ij bi translated">即使它们都大致遵循实际贡献的趋势，SHAP和ACV的图表似乎在[20，40]和[100，120]中包含的<em class="ll">月费用</em>值的贡献符号上有一些错误(如果我们假设我们的模型大致符合实际贡献)。这可能会给我们一个错误的解释，我们的模型的行为可以制定如下:</p><p id="a099" class="pw-post-body-paragraph kp kq iq kr b ks kt ka ku kv kw kd kx ky kz la lb lc ld le lf lg lh li lj lk ij bi translated"><em class="ll">“月电荷对我们预测的贡献通常随着其值的增加而增加，除了区间[20，40]当其贡献为正时，以及区间[100，120]当其贡献迅速减少时”</em></p><p id="6dda" class="pw-post-body-paragraph kp kq iq kr b ks kt ka ku kv kw kd kx ky kz la lb lc ld le lf lg lh li lj lk ij bi translated">实际的解释应该是这样的:</p><p id="5b3d" class="pw-post-body-paragraph kp kq iq kr b ks kt ka ku kv kw kd kx ky kz la lb lc ld le lf lg lh li lj lk ij bi translated"><em class="ll">“当月度费用值小于70时，其贡献为负，此时其贡献开始增加”</em></p><p id="3525" class="pw-post-body-paragraph kp kq iq kr b ks kt ka ku kv kw kd kx ky kz la lb lc ld le lf lg lh li lj lk ij bi translated">即使这两种方法都有错误，我们确实观察到ACV更稳定，更忠实于生成模式。在图的开始，很少有贡献是正的，而在结束时“减少”就不那么明显了。<strong class="kr ja">为ACV点上！</strong></p><p id="3e2e" class="pw-post-body-paragraph kp kq iq kr b ks kt ka ku kv kw kd kx ky kz la lb lc ld le lf lg lh li lj lk ij bi translated">现在让我们来看看TotalCharges:</p><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div role="button" tabindex="0" class="lz ma di mb bf mc"><div class="gh gi ob"><img src="../Images/16c9ba3724c67287d808f37492667930.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*L-aR5CKCu2FCqe-lh5u_ig.png"/></div></div><p class="mf mg gj gh gi mh mi bd b be z dk translated">图6:(上图):y和TotalCharges之间的“真实”关系<br/>(中图):SHAP的TotalCharges特征贡献分布<br/>(下图):ACV的TotalCharges特征贡献分布。图片作者。</p></figure><p id="f257" class="pw-post-body-paragraph kp kq iq kr b ks kt ka ku kv kw kd kx ky kz la lb lc ld le lf lg lh li lj lk ij bi translated">对于这个功能，SHAP和ACV之间的区别是坚持。对于超过6000的<em class="ll">总电荷</em>的值，SHAP的贡献开始急剧增加，并达到正值2，此时生成函数保持恒定且为负。由于计算的复杂性，真的很难知道为什么SHAP会这样，但我们可以说它为数据集的重要部分提供了错误的解释。另一方面，ACV贡献保持稳定，并与生成功能一致。<strong class="kr ja">为ACV点上！</strong></p><p id="a50c" class="pw-post-body-paragraph kp kq iq kr b ks kt ka ku kv kw kd kx ky kz la lb lc ld le lf lg lh li lj lk ij bi translated">为了结束这种比较，让我们看看SHAP和ACV对直接在原始数据集上训练的黑盒模型的解释。换句话说，数据科学家会为标准的机器学习管道做什么。让我们看看在这种情况下总费用和每月费用的分摊情况:</p><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div role="button" tabindex="0" class="lz ma di mb bf mc"><div class="gh gi oc"><img src="../Images/748e13a32ccb95f945ff6550a2afae9b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*D3Ja0ENdA1vm-m5qftgPhw.png"/></div></div><p class="mf mg gj gh gi mh mi bd b be z dk translated">每月费用和合计SHAP(左)和ACV(右)的费用分担。图片作者。</p></figure><p id="54e9" class="pw-post-body-paragraph kp kq iq kr b ks kt ka ku kv kw kd kx ky kz la lb lc ld le lf lg lh li lj lk ij bi translated">有意思！我们看到，对于这两个特征，ACV似乎比SHAP更忠实于EBM所学习的功能(如图5和6所示)。由于黑匣子的性质，我们无法从这个结果中得出严格的结论。然而，这似乎表明，用ACV计算的沙普利值为该模型提供了更有力的解释。我认为这仍然值得ACV再得一分！</p></div><div class="ab cl lm ln hu lo" role="separator"><span class="lp bw bk lq lr ls"/><span class="lp bw bk lq lr ls"/><span class="lp bw bk lq lr"/></div><div class="ij ik il im in"><h1 id="150a" class="mk ml iq bd mm mn mo mp mq mr ms mt mu kf mv kg mw ki mx kj my kl mz km na nb bi translated">主动联盟与主动Shapley值</h1><p id="9a67" class="pw-post-body-paragraph kp kq iq kr b ks nc ka ku kv nd kd kx ky ne la lb lc nf le lf lg ng li lj lk ij bi translated">Shapley值通常用于查找模型的最重要特征。这种选择是在观察了解释之后做出的，保留的变量数量往往是任意的。为了解决这个问题，ACV还引入了一种新的工具，称为<em class="ll">主动沙普利值</em>【5】，它通过计算贡献被认为可以忽略的变量的“零联盟”来为你做出选择。它通过将零贡献归因于“<em class="ll">无效联盟”</em>中的变量，并在<em class="ll">有效联盟</em>的变量之间公平地分配“支出”来提供更简洁的解释。</p><p id="2f88" class="pw-post-body-paragraph kp kq iq kr b ks kt ka ku kv kw kd kx ky kz la lb lc ld le lf lg lh li lj lk ij bi translated">计算有效联盟和无效联盟的方法需要相同决策概率的概念，本文不涉及这一概念。如果你想了解更多关于这些花哨的方法，请访问<a class="ae mj" href="https://github.com/salimamoukou/acv00" rel="noopener ugc nofollow" target="_blank"> ACV GitHub </a>或阅读Amoukou等人的2021【5】。</p><p id="efc5" class="pw-post-body-paragraph kp kq iq kr b ks kt ka ku kv kw kd kx ky kz la lb lc ld le lf lg lh li lj lk ij bi translated">下图显示了为客户端233计算的活动Shapley值:</p><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div role="button" tabindex="0" class="lz ma di mb bf mc"><div class="gh gi od"><img src="../Images/956a705608938edb5ef77572414c7bdf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*N9D6eKQCJ4IxezSZ9-mZAg.png"/></div></div><p class="mf mg gj gh gi mh mi bd b be z dk translated">客户端23上预测的活动Shapley值(ASV )(底部),将<br/>与用ASV计算的“完整”Shapley值(顶部)进行比较。图片作者。</p></figure><p id="2b86" class="pw-post-body-paragraph kp kq iq kr b ks kt ka ku kv kw kd kx ky kz la lb lc ld le lf lg lh li lj lk ij bi translated">最重要的变量中只有4个被保留，我们可以看到合同和支付方式的贡献显著增加。如果您的目标是找到最重要的变量，这是一种更容易理解的计算Shapley值的方法。在我们的例子中，我们直接看到付款方式、合同、月费和任期是这个预测的最重要的变量。</p></div><div class="ab cl lm ln hu lo" role="separator"><span class="lp bw bk lq lr ls"/><span class="lp bw bk lq lr ls"/><span class="lp bw bk lq lr"/></div><div class="ij ik il im in"><h1 id="349d" class="mk ml iq bd mm mn mo mp mq mr ms mt mu kf mv kg mw ki mx kj my kl mz km na nb bi translated">结论</h1><p id="627b" class="pw-post-body-paragraph kp kq iq kr b ks nc ka ku kv nd kd kx ky ne la lb lc nf le lf lg ng li lj lk ij bi translated">在本文中，我们已经表明，ACV可以提供一种更可靠的方法来计算Shapley值，从而在特征相关/分类时导致更真实的解释。对于“更简单”的场景，两个库之间的差异并不总是那么重要，但是ACV仍然是一种更健壮的方式来提供基于Shapley值的解释。需要注意的是，ACV并不是完全模型不可知的，它只适用于基于树的模型(Random Forest，XGBoost，LightGBM…)。最后，ACV提供了其他工具，如相同的决策概率或积极沙普利值[5]来严格解释模型。访问GitHub页面了解如何使用它，并发现Salim Amoukou及其合著者的论文。</p><p id="a500" class="pw-post-body-paragraph kp kq iq kr b ks kt ka ku kv kw kd kx ky kz la lb lc ld le lf lg lh li lj lk ij bi translated">本文中的所有Shapley值图表都是使用Shapash 获得的，Shapash是一个非常好的用户友好的软件包，用于可视化Shapley值(由MAIF开发，Quantmetry公司提供)。Shapash可以使用SHAP或ACV作为计算Shapley值的后端。</p><p id="7651" class="pw-post-body-paragraph kp kq iq kr b ks kt ka ku kv kw kd kx ky kz la lb lc ld le lf lg lh li lj lk ij bi translated">MAIF和Quantmetry在一个研究合作项目中对真实数据集和内部模型进行了类似的分析。类似的差异也有报道。我们都认为值得信赖的人工智能方法必须基于透明和可测试的计算技术，我们建议在使用可解释的工具作为黑盒本身时要持批评态度！这个实验可以用ACV GithHub 上的<a class="ae mj" href="https://github.com/salimamoukou/acv00/blob/main/notebooks/shap_acv_comparison/ACV_vs_SHAP_TELCO_Churn.ipynb" rel="noopener ugc nofollow" target="_blank">笔记本重现。</a></p></div><div class="ab cl lm ln hu lo" role="separator"><span class="lp bw bk lq lr ls"/><span class="lp bw bk lq lr ls"/><span class="lp bw bk lq lr"/></div><div class="ij ik il im in"><h1 id="40a2" class="mk ml iq bd mm mn mo mp mq mr ms mt mu kf mv kg mw ki mx kj my kl mz km na nb bi translated">加入</h1><p id="b4fb" class="pw-post-body-paragraph kp kq iq kr b ks nc ka ku kv nd kd kx ky ne la lb lc nf le lf lg ng li lj lk ij bi translated">我在量子力学公司工作。Quantmetry自2011年成立以来一直是先驱和独立的公司，是法国领先的纯人工智能咨询公司。在提供卓越的数据治理和最先进的人工智能解决方案的愿望的驱动下，Quantmetry的120名员工和研究顾问将他们的热情投入到为所有行业的公司提供高业务成果的服务中。</p><h2 id="22a9" class="oe ml iq bd mm of og dn mq oh oi dp mu ky oj ok mw lc ol om my lg on oo na iw bi translated">贡献者:</h2><p id="861a" class="pw-post-body-paragraph kp kq iq kr b ks nc ka ku kv nd kd kx ky ne la lb lc nf le lf lg ng li lj lk ij bi translated">西里尔·勒梅尔、蒂博·雷亚尔、萨利姆·阿穆库、尼古拉·布鲁内尔</p></div><div class="ab cl lm ln hu lo" role="separator"><span class="lp bw bk lq lr ls"/><span class="lp bw bk lq lr ls"/><span class="lp bw bk lq lr"/></div><div class="ij ik il im in"><h1 id="a334" class="mk ml iq bd mm mn mo mp mq mr ms mt mu kf mv kg mw ki mx kj my kl mz km na nb bi translated">参考</h1><p id="ca08" class="pw-post-body-paragraph kp kq iq kr b ks nc ka ku kv nd kd kx ky ne la lb lc nf le lf lg ng li lj lk ij bi translated">[1] Christoph Molnar，<a class="ae mj" href="https://christophm.github.io/interpretable-ml-book/" rel="noopener ugc nofollow" target="_blank">制作可解释黑盒模型的指南</a> (2021)</p><p id="2c77" class="pw-post-body-paragraph kp kq iq kr b ks kt ka ku kv kw kd kx ky kz la lb lc ld le lf lg lh li lj lk ij bi translated">[2] Lundberg等人，<a class="ae mj" href="https://arxiv.org/abs/1705.07874" rel="noopener ugc nofollow" target="_blank">解释模型预测的统一方法</a> (2017)，NIPS 17</p><p id="450a" class="pw-post-body-paragraph kp kq iq kr b ks kt ka ku kv kw kd kx ky kz la lb lc ld le lf lg lh li lj lk ij bi translated">[3] Amoukou等，<a class="ae mj" href="https://arxiv.org/pdf/2103.13342.pdf" rel="noopener ugc nofollow" target="_blank">变量联盟的Shapley值提供了更好的解释</a> (2021)</p><p id="12a2" class="pw-post-body-paragraph kp kq iq kr b ks kt ka ku kv kw kd kx ky kz la lb lc ld le lf lg lh li lj lk ij bi translated">[4] Nori等人，<a class="ae mj" href="https://arxiv.org/pdf/1909.09223.pdf" rel="noopener ugc nofollow" target="_blank"> InterpretML:机器学习可解释性的统一框架</a> (2019)</p><p id="41ff" class="pw-post-body-paragraph kp kq iq kr b ks kt ka ku kv kw kd kx ky kz la lb lc ld le lf lg lh li lj lk ij bi translated">[5] Amoukou等人，<a class="ae mj" href="https://arxiv.org/pdf/2106.03820.pdf" rel="noopener ugc nofollow" target="_blank">用于解释预测和关注局部重要变量的精确和稳健的Shapley值</a> (2021)</p></div></div>    
</body>
</html>