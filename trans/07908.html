<html>
<head>
<title>The Road to a Serverless ML Pipeline in Production — Part I</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">走向生产中的无服务器ML管道——第一部分</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/the-road-to-a-serverless-ml-pipeline-in-production-part-i-893f3bf0062a?source=collection_archive---------14-----------------------#2021-07-20">https://towardsdatascience.com/the-road-to-a-serverless-ml-pipeline-in-production-part-i-893f3bf0062a?source=collection_archive---------14-----------------------#2021-07-20</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="0faa" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">Nutrino如何在生产中设计无服务器MLOps堆栈</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/6ff37de48aed3c4edbc94c6c0f0f4321.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*yr4MiFNQmFmTJp7O"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">迪诺·瑞奇穆斯在<a class="ae kv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="232b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">今天将ML模型引入生产是复杂的——不同的公司对ML栈有不同的需求，并且有许多工具，每个工具试图解决ML生命周期的不同方面。这些工具仍在开发中，没有一个针对MLOps的“明确”解决方案。在本文中，我想分享我们在创建自己的MLOps堆栈时所经历的过程，包括我们团队在该过程开始之前的工作方式，我们对不同MLOps工具所做的研究，以及我们如何决定适合我们非标准模型的解决方案。</p><p id="ecac" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">有关详细的解决方案，包括更多的技术解释，请查看本文的<a class="ae kv" href="https://medium.com/@galshen/the-road-to-a-serverless-ml-pipeline-in-production-part-ii-e065cf4caa93" rel="noopener">第二部分</a>。</p><p id="03cf" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir"><em class="ls">TL；DR</em></strong>——我们设法运行不同类型的模型(我们自己的Python模型)，每种模型都有多个生产版本——所有这些都在一个无服务器的环境中！</p><h1 id="5c89" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated">我们的ML堆栈预重构</h1><p id="e3dc" class="pw-post-body-paragraph kw kx iq ky b kz ml jr lb lc mm ju le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated">这是我们开始重构之前的ML堆栈:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mq"><img src="../Images/430ec8fb345b097d01d6be09a0536abd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jFf3rn94eIhdONvbh0tQpg.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><h2 id="5612" class="mr lu iq bd lv ms mt dn lz mu mv dp md lf mw mx mf lj my mz mh ln na nb mj nc bi translated"><em class="nd">研究环境</em></h2><p id="2b53" class="pw-post-body-paragraph kw kx iq ky b kz ml jr lb lc mm ju le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated">我们使用了一个datalake环境，该环境中有一个ETL流程，可以将生产数据传输到parquet文件，再传输到S3。数据科学家们在该环境中使用运行在EMR集群上的Zeppelin笔记本电脑进行研究(因此利用了Spark的分布式能力)。</p><h2 id="a1f8" class="mr lu iq bd lv ms mt dn lz mu mv dp md lf mw mx mf lj my mz mh ln na nb mj nc bi translated"><em class="nd">特征提取</em></h2><p id="8504" class="pw-post-body-paragraph kw kx iq ky b kz ml jr lb lc mm ju le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated">使用AWS lambdas完成特征提取，每次新数据到达我们的集中式数据存储时，由Kinesis流触发，并使用无服务器框架进行部署。</p><h2 id="c9e9" class="mr lu iq bd lv ms mt dn lz mu mv dp md lf mw mx mf lj my mz mh ln na nb mj nc bi translated"><em class="nd">培训</em></h2><ol class=""><li id="5f5f" class="ne nf iq ky b kz ml lc mm lf ng lj nh ln ni lr nj nk nl nm bi translated">一旦某个模型的研究完成，数据科学家就为该模型创建一个培训笔记本(在相同的datalake环境中)。</li><li id="9cc5" class="ne nf iq ky b kz nn lc no lf np lj nq ln nr lr nj nk nl nm bi translated">我们使用Apache Airflow定期运行培训的笔记本(并利用它在运行计划作业方面的优势)。为此，我们为每个模型的培训笔记本创建了一个DAG(有向无环图)。<br/> Airflow的Dag会在每次推送到主分支时立即部署。</li><li id="c995" class="ne nf iq ky b kz nn lc no lf np lj nq ln nr lr nj nk nl nm bi translated">Dag创建了一个运行培训笔记本的EMR集群。笔记本连接到GitHub存储库，因此笔记本中的每次提交基本上都是训练代码的自动“部署”。</li></ol><h2 id="38f6" class="mr lu iq bd lv ms mt dn lz mu mv dp md lf mw mx mf lj my mz mh ln na nb mj nc bi translated"><em class="nd">推论</em></h2><p id="c2bb" class="pw-post-body-paragraph kw kx iq ky b kz ml jr lb lc mm ju le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated">一些模型被开发为具有预测端点的Flask服务。这些模型在服务的初始化部分加载了训练工件。其他模型是使用无服务器框架作为无服务器lambda处理程序开发的，它每次都直接从S3路径加载训练工件。我们的Kubernetes集群部署了烧瓶模型。无服务器模型在每次更改时都作为HTTP lambdas部署到AWS lambda服务。</p></div><div class="ab cl ns nt hu nu" role="separator"><span class="nv bw bk nw nx ny"/><span class="nv bw bk nw nx ny"/><span class="nv bw bk nw nx"/></div><div class="ij ik il im in"><p id="3aeb" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir"> <em class="ls">如果我们的MLOps流程有效，并且培训已经自动运行笔记本中编写的相同代码，为什么要改变它？</em> </strong></p><p id="2f68" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">它运行得很好，但是随着我们的成长和有更多的模型要开发，我们遇到了两个主要问题:</p><p id="d961" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">首先，我们的模型被归类为医疗软件，需要符合联邦药品管理局(FDA)的开发法规。这意味着我们需要能够在某个批准的时间点“冻结”我们的代码，并且从那时起，没有正式的批准，我们不能更改它(这个过程可能需要几个月！).<br/>这意味着我们不能在一个模型的生产中处理一个版本，而是必须在生产中支持同一个模型的许多不同版本。</p><p id="ede6" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">第二，我们的生产部署流程部分是手动的；在气流中运行训练并将工件保存到S3之后，评估是手工完成的。只有在获得批准后，我们才将文件复制到S3的另一个地方，然后必须删除Kubernetes集群中的相关pod，以便服务加载新文件(没有宕机)。<br/>此外，我们没有任何模型部署标准，每个模型的处理方式也不一样。</p><h1 id="4b67" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated">开始探索MLOps！</h1><p id="70c1" class="pw-post-body-paragraph kw kx iq ky b kz ml jr lb lc mm ju le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated">给定基线和需求，我们开始探索哪些解决方案已经存在，哪些是行业中常用的。在选择ML产品时，我们需要考虑的一个主要要求是，我们的模型是内部编写的，而不是从任何常见的ML类库(如scikit-learn等)继承而来。).这使得将我们的模型结合到许多ML产品中的过程变得更加复杂。</p><p id="d559" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们的研究集中在三种服务上；在下一节中，我将提供一个简短的总结，说明我们选择或不选择使用该服务的原因。</p><p id="a462" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">** <strong class="ky ir"> <em class="ls">注:</em> </strong>我们在2020年7月进行了这项研究，所以你需要考虑到这个领域的事情变化和发展相当快。然而，你可能会发现我的研究和决策过程仍然有用。</p><h2 id="0bd8" class="mr lu iq bd lv ms mt dn lz mu mv dp md lf mw mx mf lj my mz mh ln na nb mj nc bi translated"><strong class="ak"> <em class="nd"> Kubeflow </em> </strong></h2><p id="64cb" class="pw-post-body-paragraph kw kx iq ky b kz ml jr lb lc mm ju le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated">Kubeflow为整个ML生命周期提供了完整的解决方案——从笔记本电脑中的研究到在生产中部署模型。因此，我认为这是一个很好的整体解决方案，尤其是如果你刚刚开始；然而，对于生命周期的更具体的方面，有一些产品掌握了生命周期的每个方面，并且做得更好。</p><p id="1370" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我当时对Kubeflow的研究表明，使用Kubeflow有两大好处:</p><ul class=""><li id="c9a4" class="ne nf iq ky b kz la lc ld lf nz lj oa ln ob lr oc nk nl nm bi translated">他们的管道产品，支持使用Dag运行计划的作业</li><li id="9e44" class="ne nf iq ky b kz nn lc no lf np lj nq ln nr lr oc nk nl nm bi translated">他们在Kubernetes上使用Dockerized容器的概念，可以很容易地部署在任何云提供商上</li></ul><p id="eb52" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">然而，对我们来说，这两个好处都没有那么有用，原因如下:</p><ul class=""><li id="fd71" class="ne nf iq ky b kz la lc ld lf nz lj oa ln ob lr oc nk nl nm bi translated">我们已经在为我们的预定工作使用气流，它比Kubeflow的管道更成熟，更适合生产。</li><li id="a10d" class="ne nf iq ky b kz nn lc no lf np lj nq ln nr lr oc nk nl nm bi translated">我们的推理服务已经在Nutrino的Kubernetes集群上工作了。</li><li id="6863" class="ne nf iq ky b kz nn lc no lf np lj nq ln nr lr oc nk nl nm bi translated">我们在使用Zeppelin笔记本，而在Kubeflow上工作需要我们使用Jupiter笔记本，这意味着我们需要迁移和改变我们的笔记本平台。</li><li id="1d58" class="ne nf iq ky b kz nn lc no lf np lj nq ln nr lr oc nk nl nm bi translated">迁移笔记本和做额外的工作来改变我们开发和部署Docker容器的方式是非常耗时的工作——这只是开始——我们仍然不确定这是否会给我们的产品带来任何显著的好处。</li></ul><p id="f81a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">此外，在将我们的模型结合到许多ML产品中时，我上面提到的复杂性也适用于尝试将我们的模型与Kubeflow结合时。</p><h2 id="1b19" class="mr lu iq bd lv ms mt dn lz mu mv dp md lf mw mx mf lj my mz mh ln na nb mj nc bi translated"><strong class="ak"> <em class="nd"> MLFlow </em> </strong></h2><p id="84b5" class="pw-post-body-paragraph kw kx iq ky b kz ml jr lb lc mm ju le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated">MLFlow侧重于模型库，并不是一个完整的生命周期解决方案。使用MLFlow，我们可以使用自己的Python模型(使用<a class="ae kv" href="https://www.mlflow.org/docs/latest/python_api/mlflow.pyfunc.html" rel="noopener ugc nofollow" target="_blank"> pyfunc模块</a>，这使我们能够轻松地将自己的模型合并到MLFlow中。</p><p id="ae97" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我应该解释说“容易”是一种轻描淡写的说法:启动并运行MLFlow服务器需要几分钟时间，并且需要一个简单的库从您的脚本与该服务器进行交互。</p><p id="d602" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">有了MLFlow，我们最大的需求得到了满足——我们现在可以通过模型名称和版本(使用它的模型方案:' models:/ <model_name> /production ')来处理所有培训的工件；我将在<a class="ae kv" href="https://medium.com/@galshen/the-road-to-a-serverless-ml-pipeline-in-production-part-ii-e065cf4caa93" rel="noopener">技术解决方案部分</a>对此进行详细阐述。<br/> MLFlow还用于通过it服务模型，而不仅仅是管理存储库，但是我们不想从MLFlow服务器服务模型。这意味着，对于每个模型，我们都需要在该服务器的不同端口上创建一个服务端点，这不是一件可以轻松部署、访问和监控的事情(我们不想为不同的模型访问不同的端口，我们希望通过模型的名称来访问模型，并在生产中支持具有许多版本的许多模型，因此这是不可伸缩的)。</model_name></p><p id="5519" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">因此，我们决定在训练期间使用log_model函数记录我们的模型，并在推理服务中调用load_model。然而，MLFlow的一个大缺点是load_model函数的性能——它花费了4-30秒或更长时间(取决于模型大小和网络，如果我们在本地或从服务器测试它——但无论哪种方式，它仍然太长了)。</p><h2 id="c3c0" class="mr lu iq bd lv ms mt dn lz mu mv dp md lf mw mx mf lj my mz mh ln na nb mj nc bi translated"><strong class="ak">T5【Sagemaker】T6</strong></h2><p id="14d7" class="pw-post-body-paragraph kw kx iq ky b kz ml jr lb lc mm ju le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated">由于我们已经将AWS用于我们的许多服务，我们喜欢使用托管AWS服务及其所有内置监控的想法。</p><p id="4dda" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们创建了一个flask服务的Docker映像，带有两个必需的端点(ping和调用；参见<a class="ae kv" href="https://docs.aws.amazon.com/sagemaker/latest/dg/your-algorithms-inference-code.html" rel="noopener ugc nofollow" target="_blank"> AWS的使用您自己的推理代码</a>了解更多信息)。这非常类似于我们自己在Kubernetes上运行的推理服务。我们希望使用Sagemaker，结合MLFlow来缩短推理时间。我们在flask应用程序初始化过程中调用load_model，并将结果(模型实例)保存在内存中。</p><p id="4f2f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">通过这种方式，当Sagemaker端点启动并运行时，我们可以确定模型已经被加载，而不必等待几秒钟来进行预测。</p><p id="a1dc" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">不幸的是，从Sagemaker端点获得预测仍然需要1.5-2秒，尽管在Docker中调用模型的predict函数只需要几毫秒。<br/>sage maker的另一个主要缺点是端点总是开启的——这意味着即使没有要预测的调用，我们也要为实例付费。</p><p id="0b6e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这些问题使我们决定不追求Sagemaker，而只是在我们现有的推理服务中实现相同的逻辑(无论如何，这个pod只是一个大型Kubernetes集群使用中的一个pod——因此它更具成本效益，并且预测更快地发送回客户端)。</p><h2 id="d5b7" class="mr lu iq bd lv ms mt dn lz mu mv dp md lf mw mx mf lj my mz mh ln na nb mj nc bi translated"><strong class="ak"> <em class="nd"> AWS EFS </em> </strong></h2><p id="737c" class="pw-post-body-paragraph kw kx iq ky b kz ml jr lb lc mm ju le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated">虽然AWS EFS不是MLOps产品，但我们希望使用EFS来进一步降低成本，实现完全无服务器。正如我在第一节中提到的——我们堆栈中的一些推理服务是在lambda函数中实现的，因此它更具成本效益；然而，我们不能在每次调用时都调用load_model，因为这会花费很长时间。</p><p id="0690" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这就是为什么我们开始关注AWS的EFS服务，并将推理lambda安装到它上面，而不是从S3加载(这是load_model的工作方式)。lambda只需用本地安装的路径调用<em class="ls"> mlflow.pyfunc.load_model </em>，而不是使用mlflow的models方案。</p></div><div class="ab cl ns nt hu nu" role="separator"><span class="nv bw bk nw nx ny"/><span class="nv bw bk nw nx ny"/><span class="nv bw bk nw nx"/></div><div class="ij ik il im in"><h1 id="bdc7" class="lt lu iq bd lv lw od ly lz ma oe mc md jw of jx mf jz og ka mh kc oh kd mj mk bi translated">于是MLOps架构诞生了…</h1><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oi"><img src="../Images/50d7949ab03e332b0d0a1a71fcb6e541.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FMaGkzQFhWu_6LUtzdqhlQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">诺帝诺的MLOps架构(图片由作者提供)</p></figure><p id="be42" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">如前所述，我们选择使用MLFlow作为模型库和工件库，并使用AWS EFS来提高性能。这种结合产生了一种解决方案，它在不到一秒钟的时间内将预测传递给了位于<em class="ls">的客户。</em></p><p id="640f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">下一步是什么？在本文的<a class="ae kv" href="https://medium.com/@galshen/the-road-to-a-serverless-ml-pipeline-in-production-part-ii-e065cf4caa93" rel="noopener">下一部分</a>中，我将深入探讨这个解决方案的技术方面，包括一些代码示例。</p></div><div class="ab cl ns nt hu nu" role="separator"><span class="nv bw bk nw nx ny"/><span class="nv bw bk nw nx ny"/><span class="nv bw bk nw nx"/></div><div class="ij ik il im in"><p id="d59e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><em class="ls">感谢您花时间阅读本文！欢迎在下面评论问题或开始讨论。</em></p></div></div>    
</body>
</html>