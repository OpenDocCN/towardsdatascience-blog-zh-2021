<html>
<head>
<title>How to Clean Text Like a Boss for NLP in Python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如何在Python中像Boss一样为NLP清理文本</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/text-cleaning-for-nlp-in-python-2716be301d5d?source=collection_archive---------19-----------------------#2021-09-25">https://towardsdatascience.com/text-cleaning-for-nlp-in-python-2716be301d5d?source=collection_archive---------19-----------------------#2021-09-25</a></blockquote><div><div class="fc ii ij ik il im"/><div class="in io ip iq ir"><div class=""/><div class=""><h2 id="9144" class="pw-subtitle-paragraph jr it iu bd b js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki dk translated">自然语言处理的关键步骤变得简单！</h2></div><figure class="kk kl km kn gu ko gi gj paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gi gj kj"><img src="../Images/17bf9ff855321b5ee796e4d76fd8b5a8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*WU4TUuNk6W9aSM4J"/></div></div><p class="kv kw gk gi gj kx ky bd b be z dk translated">照片由<a class="ae kz" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的<a class="ae kz" href="https://unsplash.com/@ratushny?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Dmitry Ratushny </a>拍摄</p></figure><h1 id="eed5" class="la lb iu bd lc ld le lf lg lh li lj lk ka ll kb lm kd ln ke lo kg lp kh lq lr bi translated">清理文本</h1><p id="25b1" class="pw-post-body-paragraph ls lt iu lu b lv lw jv lx ly lz jy ma mb mc md me mf mg mh mi mj mk ml mm mn in bi translated">自然语言处理(NLP)中最常见的任务之一是清理文本数据。为了最大化你的结果，重要的是从你的文本中提取出语料库中最重要的<em class="mo">词根</em>，并清除掉<em class="mo">不需要的噪音</em>。这篇文章将展示我是如何做到这一点的。以下是文本预处理的一般步骤:</p><ul class=""><li id="dcdf" class="mp mq iu lu b lv mr ly ms mb mt mf mu mj mv mn mw mx my mz bi translated"><strong class="lu iv">标记化</strong>:标记化将文本分解成更小的单元，而不是大块的文本。我们将这些单元理解为<em class="mo">单词</em>或<em class="mo">句子</em>，但是机器只有将它们分开才能理解。分解术语时必须特别小心，以便创建逻辑单元。大多数软件包处理边缘情况(美国闯入美国，而不是美国和美国)，但确保它正确完成总是至关重要的。</li><li id="313c" class="mp mq iu lu b lv na ly nb mb nc mf nd mj ne mn mw mx my mz bi translated"><strong class="lu iv">清洗</strong>:清洗过程对于去除对分析不重要的文本和字符至关重要。诸如URL之类的文本、诸如连字符或特殊字符之类的非关键项目、网页抓取、HTML和CSS信息都将被丢弃。</li><li id="a1a0" class="mp mq iu lu b lv na ly nb mb nc mf nd mj ne mn mw mx my mz bi translated"><strong class="lu iv">删除停用词</strong>:接下来是删除停用词的过程。停用词是出现但不增加任何理解的常用词。像“a”和“the”这样的词就是例子。这些词也非常频繁地出现，在你的分析中成为主导，模糊了有意义的词。：</li><li id="425a" class="mp mq iu lu b lv na ly nb mb nc mf nd mj ne mn mw mx my mz bi translated"><strong class="lu iv">拼写</strong>:拼写错误也可以在分析过程中纠正。根据通信媒介的不同，可能会有更多或更少的错误。官方的企业或教育文档很可能包含较少的错误，而社交媒体帖子或电子邮件等更非正式的交流可能会有更多的错误。根据期望的结果，纠正或不纠正拼写错误是关键的一步。</li><li id="2cd0" class="mp mq iu lu b lv na ly nb mb nc mf nd mj ne mn mw mx my mz bi translated"><strong class="lu iv">词干化和词汇化</strong>:词干化是从一个单词的开头或结尾移除字符，以将其缩减为词干的过程。词干化的一个例子是将“runs”简化为“run”，因为基本单词去掉了“s”，而“ran”不会在同一个词干中。然而，词汇化会将“然”归入同一词汇中。</li></ul><p id="93ed" class="pw-post-body-paragraph ls lt iu lu b lv mr jv lx ly ms jy ma mb nf md me mf ng mh mi mj nh ml mm mn in bi translated">以下是我用来清理大部分文本数据的脚本。</p><h1 id="282c" class="la lb iu bd lc ld le lf lg lh li lj lk ka ll kb lm kd ln ke lo kg lp kh lq lr bi translated">进口</h1><pre class="kk kl km kn gu ni nj nk bn nl nm bi"><span id="508c" class="nn lb iu nj b be no np l nq nr">import pandas as pd<br/>import re<br/>import string<br/>from bs4 import BeautifulSoup<br/>import nltk<br/>from nltk.stem import PorterStemmer<br/>from nltk.stem.wordnet import WordNetLemmatizer<br/>import spacy</span></pre><h1 id="41c4" class="la lb iu bd lc ld le lf lg lh li lj lk ka ll kb lm kd ln ke lo kg lp kh lq lr bi translated">清理HTML</h1><p id="0713" class="pw-post-body-paragraph ls lt iu lu b lv lw jv lx ly lz jy ma mb mc md me mf mg mh mi mj mk ml mm mn in bi translated">移除HTML是可选的，这取决于您的数据源是什么。我发现美丽的汤是最好的清洗方法。</p><pre class="kk kl km kn gu ni nj nk bn nl nm bi"><span id="76a1" class="nn lb iu nj b be no np l nq nr">def clean_html(html):<br/><br/>    # parse html content<br/>    soup = BeautifulSoup(html, "html.parser")<br/><br/>    for data in soup(['style', 'script', 'code', 'a']):<br/>        # Remove tags<br/>        data.decompose()<br/><br/>    # return data by retrieving the tag content<br/>    return ' '.join(soup.stripped_strings)</span></pre><p id="cb89" class="pw-post-body-paragraph ls lt iu lu b lv mr jv lx ly ms jy ma mb nf md me mf ng mh mi mj nh ml mm mn in bi translated"><strong class="lu iv">注意:</strong>在<code class="fe ns nt nu nj b">for</code>循环中，你可以指定你想要清理的不同HTML标签。例如，上面的步骤包括<code class="fe ns nt nu nj b">style</code>、<code class="fe ns nt nu nj b">script</code>、<code class="fe ns nt nu nj b">code</code>和<code class="fe ns nt nu nj b">a</code>标签。试验并扩充这个列表，直到你得到你想要的结果。</p><h1 id="7057" class="la lb iu bd lc ld le lf lg lh li lj lk ka ll kb lm kd ln ke lo kg lp kh lq lr bi translated">清洁其余部分</h1><p id="e32a" class="pw-post-body-paragraph ls lt iu lu b lv lw jv lx ly lz jy ma mb mc md me mf mg mh mi mj mk ml mm mn in bi translated">现在是老黄牛。</p><ol class=""><li id="7468" class="mp mq iu lu b lv mr ly ms mb mt mf mu mj mv mn nv mx my mz bi translated">将文本<strong class="lu iv">变成小写</strong>。您可能知道，python是区分大小写的，其中<code class="fe ns nt nu nj b">A != a</code>。</li><li id="2b09" class="mp mq iu lu b lv na ly nb mb nc mf nd mj ne mn nv mx my mz bi translated">移除<strong class="lu iv">断线</strong>。同样，根据您的源代码，您可能已经编码了换行符。</li><li id="82ba" class="mp mq iu lu b lv na ly nb mb nc mf nd mj ne mn nv mx my mz bi translated">移除<strong class="lu iv">标点符号</strong>。这是使用字符串库。其他标点符号可以根据需要添加。</li><li id="7024" class="mp mq iu lu b lv na ly nb mb nc mf nd mj ne mn nv mx my mz bi translated">使用<code class="fe ns nt nu nj b">NLTK</code>库删除<strong class="lu iv">停止字</strong>。下一行有一个列表，可以根据需要向函数中添加额外的停用词。这些可能是嘈杂的领域词或任何使上下文清晰的东西。</li><li id="fd05" class="mp mq iu lu b lv na ly nb mb nc mf nd mj ne mn nv mx my mz bi translated">移除<strong class="lu iv">数字</strong>。根据您的数据选择。</li><li id="c682" class="mp mq iu lu b lv na ly nb mb nc mf nd mj ne mn nv mx my mz bi translated"><strong class="lu iv">词干化</strong>或<strong class="lu iv">词干化</strong>。这个过程是函数中的一个自变量。您可以使用<code class="fe ns nt nu nj b">Stem</code>或<code class="fe ns nt nu nj b">Lem</code>选择一个过孔。默认情况下使用none。</li></ol><pre class="kk kl km kn gu ni nj nk bn nl nm bi"><span id="019a" class="nn lb iu nj b be no np l nq nr"># Load spacy<br/>nlp = spacy.load('en_core_web_sm')<br/><br/>def clean_string(text, stem="None"):<br/><br/>    final_string = ""<br/><br/>    # Make lower<br/>    text = text.lower()<br/><br/>    # Remove line breaks<br/>    # Note: that this line can be augmented and used over<br/>    # to replace any characters with nothing or a space<br/>    text = re.sub(r'\n', '', text)<br/><br/>    # Remove punctuation<br/>    translator = str.maketrans('', '', string.punctuation)<br/>    text = text.translate(translator)<br/><br/>    # Remove stop words<br/>    text = text.split()<br/>    useless_words = nltk.corpus.stopwords.words("english")<br/>    useless_words = useless_words + ['hi', 'im']<br/><br/>    text_filtered = [word for word in text if not word in useless_words]<br/><br/>    # Remove numbers<br/>    text_filtered = [re.sub(r'\w*\d\w*', '', w) for w in text_filtered]<br/><br/>    # Stem or Lemmatize<br/>    if stem == 'Stem':<br/>        stemmer = PorterStemmer() <br/>        text_stemmed = [stemmer.stem(y) for y in text_filtered]<br/>    elif stem == 'Lem':<br/>        lem = WordNetLemmatizer()<br/>        text_stemmed = [lem.lemmatize(y) for y in text_filtered]<br/>    elif stem == 'Spacy':<br/>        text_filtered = nlp(' '.join(text_filtered))<br/>        text_stemmed = [y.lemma_ for y in text_filtered]<br/>    else:<br/>        text_stemmed = text_filtered<br/><br/>    final_string = ' '.join(text_stemmed)<br/><br/>    return final_string</span></pre><h1 id="e678" class="la lb iu bd lc ld le lf lg lh li lj lk ka ll kb lm kd ln ke lo kg lp kh lq lr bi translated">例子</h1><p id="5770" class="pw-post-body-paragraph ls lt iu lu b lv lw jv lx ly lz jy ma mb mc md me mf mg mh mi mj mk ml mm mn in bi translated">要将此应用于标准数据框，请使用Pandas的<code class="fe ns nt nu nj b">apply</code>函数，如下所示。让我们来看看起始文本:</p><pre class="kk kl km kn gu ni nj nk bn nl nm bi"><span id="89c1" class="nn lb iu nj b be no np l nq nr">&lt;p&gt;<br/>  &lt;a<br/>    href="https://forge.autodesk.com/en/docs/data/v2/tutorials/download-file/#step-6-download-the-item"<br/>    rel="nofollow noreferrer"<br/>    &gt;https://forge.autodesk.com/en/docs/data/v2/tutorials/download-file/#step-6-download-the-item&lt;/a<br/>  &gt;<br/>&lt;/p&gt;<br/>\n\n<br/>&lt;p&gt;<br/>  I have followed the tutorial and have successfully obtained the contents of<br/>  the file, but where is the file being downloaded. In addition, how do I<br/>  specify the location of where I want to download the file?<br/>&lt;/p&gt;<br/>\n\n<br/>&lt;p&gt;<br/>  Result on Postman\n&lt;a<br/>    href="https://i.stack.imgur.com/VrdqP.png"<br/>    rel="nofollow noreferrer"<br/>    &gt;&lt;img<br/>      src="https://i.stack.imgur.com/VrdqP.png"<br/>      alt="enter image description here"<br/>  /&gt;&lt;/a&gt;<br/>&lt;/p&gt;</span></pre><p id="4e59" class="pw-post-body-paragraph ls lt iu lu b lv mr jv lx ly ms jy ma mb nf md me mf ng mh mi mj nh ml mm mn in bi translated">让我们从清理HTML开始。</p><pre class="kk kl km kn gu ni nj nk bn nl nm bi"><span id="6f0a" class="nn lb iu nj b be no np l nq nr"># To remove HTML first and apply it directly to the source text column.<br/>df['body'] = df['body'].apply(lambda x: clean_html(x))</span></pre><p id="a9e5" class="pw-post-body-paragraph ls lt iu lu b lv mr jv lx ly ms jy ma mb nf md me mf ng mh mi mj nh ml mm mn in bi translated">将该函数应用于清理HTML后，结果如下——非常令人印象深刻:</p><pre class="kk kl km kn gu ni nj nk bn nl nm bi"><span id="08b3" class="nn lb iu nj b be no np l nq nr">I have followed the tutorial and have successfully obtained the contents <br/>of the file, but where is the file being downloaded. In addition, how <br/>do I specify the location of where I want to download the file? Result <br/>on Postman</span></pre><p id="8bcd" class="pw-post-body-paragraph ls lt iu lu b lv mr jv lx ly ms jy ma mb nf md me mf ng mh mi mj nh ml mm mn in bi translated">接下来，让我们应用<code class="fe ns nt nu nj b">clean_string</code>函数。</p><pre class="kk kl km kn gu ni nj nk bn nl nm bi"><span id="9f98" class="nn lb iu nj b be no np l nq nr"># Next apply the clean_string function to the text<br/>df['body_clean'] = df['body'].apply(lambda x: clean_string(x, stem='Stem'))</span></pre><p id="5946" class="pw-post-body-paragraph ls lt iu lu b lv mr jv lx ly ms jy ma mb nf md me mf ng mh mi mj nh ml mm mn in bi translated">最后得到的文本是:</p><pre class="kk kl km kn gu ni nj nk bn nl nm bi"><span id="2424" class="nn lb iu nj b be no np l nq nr">follow tutori success obtain content file file download addit <br/>specifi locat want download file result postman</span></pre><p id="e661" class="pw-post-body-paragraph ls lt iu lu b lv mr jv lx ly ms jy ma mb nf md me mf ng mh mi mj nh ml mm mn in bi translated">完全干净，随时可以在您的NLP项目中使用。您可能会注意到，去掉停用词后，单词的长度大大缩短了，而且单词的词干也变成了它们的词根形式。</p><p id="fd5a" class="pw-post-body-paragraph ls lt iu lu b lv mr jv lx ly ms jy ma mb nf md me mf ng mh mi mj nh ml mm mn in bi translated"><strong class="lu iv">注意:</strong>我经常创建一个新的专栏，就像上面的<code class="fe ns nt nu nj b">body_clean</code>，所以我保留了原来的，以防需要标点符号。</p><p id="69eb" class="pw-post-body-paragraph ls lt iu lu b lv mr jv lx ly ms jy ma mb nf md me mf ng mh mi mj nh ml mm mn in bi translated">大概就是这样。上述函数中的顺序很重要。您应该在其他步骤之前完成某些步骤，例如先制作小写字母。该函数包含一个删除数字的正则表达式示例；一个可靠的实用函数，您可以调整它来使用RegEx删除文本中的其他项目。</p><h1 id="f6ba" class="la lb iu bd lc ld le lf lg lh li lj lk ka ll kb lm kd ln ke lo kg lp kh lq lr bi translated">空间与NLKT符号化</h1><p id="131f" class="pw-post-body-paragraph ls lt iu lu b lv lw jv lx ly lz jy ma mb mc md me mf mg mh mi mj mk ml mm mn in bi translated">上面的函数包含了两种不同的方法来对你的文本进行词汇化。NLTK <code class="fe ns nt nu nj b">WordNetLemmatizer</code>需要一个词性(POS)参数(<code class="fe ns nt nu nj b">noun</code>，<code class="fe ns nt nu nj b">verb</code>)，因此要么需要多次传递来获取每个单词，要么只捕获一个词性。另一种方法是使用<code class="fe ns nt nu nj b">Spacy</code>，它将自动对每个单词进行词条分类，并确定它属于哪个词性。问题是Spacy的性能会比NLTK慢很多。</p><p id="53f6" class="pw-post-body-paragraph ls lt iu lu b lv mr jv lx ly ms jy ma mb nf md me mf ng mh mi mj nh ml mm mn in bi translated">如果你喜欢阅读这样的故事，并想支持我成为一名作家，可以考虑报名成为一名媒体成员。一个月5美元，让你可以无限制地访问成千上万篇文章。如果你用<a class="ae kz" href="https://medium.com/@broepke/membership" rel="noopener">我的链接</a>注册，我会赚一小笔佣金，不需要你额外付费。</p></div></div>    
</body>
</html>