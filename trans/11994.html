<html>
<head>
<title>How To Detect Partially-Occluded Objects Using Temporal Context</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如何利用时间上下文检测部分遮挡物体</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/how-you-can-detect-partially-occluded-objects-using-temporal-context-3db1194e7171?source=collection_archive---------15-----------------------#2021-12-02">https://towardsdatascience.com/how-you-can-detect-partially-occluded-objects-using-temporal-context-3db1194e7171?source=collection_archive---------15-----------------------#2021-12-02</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="a0b4" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">解决长期存在的目标检测问题</h2></div><p id="06e0" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">检测真实世界场景中的所有对象对于任何对象检测模型都是一项具有挑战性的任务。但是，如果您有一系列由固定位置的相机拍摄的图像，您可以利用通过多个图像获得的时间背景。在本文中，我们将回顾一篇介绍<strong class="kk iu"> Context R-CNN </strong>的论文，它使用这种机制在现实世界的对象检测上实现了令人难以置信的性能，即检测遮挡、光线不佳、远处和移出图像的对象。该出版物名为<a class="ae le" href="https://arxiv.org/pdf/1912.03538.pdf" rel="noopener ugc nofollow" target="_blank"><em class="lf">“Context R-CNN:每个摄像机对象检测的长期时间上下文”</em> </a>，由加州理工学院和谷歌的 Beery 等人出版。我尽量让文章简单，这样即使没有什么先验知识的读者也能理解。事不宜迟，我们开始吧！</p><figure class="lh li lj lk gt ll gh gi paragraph-image"><div role="button" tabindex="0" class="lm ln di lo bf lp"><div class="gh gi lg"><img src="../Images/bdf5ca6edcf6e938a364e8b7ec7176c9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iT7hwSsojsQPHBU2atN-Aw.png"/></div></div><p class="ls lt gj gh gi lu lv bd b be z dk translated">上下文 R-CNN 对被遮挡或移出帧的对象的性能。来源:<a class="ae le" href="https://arxiv.org/pdf/1912.03538.pdf" rel="noopener ugc nofollow" target="_blank">【1】</a></p></figure><h1 id="b39e" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">先决条件:对象检测和更快的 R-CNN</h1><p id="558d" class="pw-post-body-paragraph ki kj it kk b kl mo ju kn ko mp jx kq kr mq kt ku kv mr kx ky kz ms lb lc ld im bi translated">如果您已经熟悉对象检测和更快的 R-CNN 模型架构，请随意跳过这一部分。</p><p id="f120" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">目标检测是计算机视觉中最具挑战性的任务之一。从本质上来说，目标是训练一个模型，它既可以通过预测物体周围的边界框(上面的蓝色/绿色)来定位场景中的物体<strong class="kk iu">，也可以通过对物体</strong>进行分类来预测物体的类别(例如，狗、猫等)。).</p><p id="1d3e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">最流行的对象检测架构之一被称为更快的 R-CNN。不要进入太多的细节，快速介绍模型的主要概念是有意义的，因为它们与理解 R-CNN 文章的上下文相关。</p><figure class="lh li lj lk gt ll gh gi paragraph-image"><div role="button" tabindex="0" class="lm ln di lo bf lp"><div class="gh gi mt"><img src="../Images/ce0ae178e69e1c02ae72bd706c44f354.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*y9ZEFwENaDRuIW58Wo4eRg.png"/></div></div><p class="ls lt gj gh gi lu lv bd b be z dk translated">通过可视化其关键概念来显示更快的 R-CNN 模型架构。来源:<a class="ae le" href="https://arxiv.org/pdf/1506.01497.pdf" rel="noopener ugc nofollow" target="_blank">【2】</a></p></figure><p id="8642" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">速度更快的 R-CNN 使用卷积骨干从图像中提取特征。这可以通过任何类型的 ConvNet 实现，例如 ResNet-50 或 AlexNet。该主干输出包含图像所有特征的特征图。现在重要的部分来了:这些特征被传递到一个<strong class="kk iu">区域提议网络(RPN) </strong>，在那里模型检测它预测包含它被训练的对象的区域。区域是由包含对象的边界框标记的区域。然后，在<strong class="kk iu"> RoI(感兴趣区域)汇集</strong>步骤中，将区域提议与特征图汇集在一起，然后传递给<strong class="kk iu">分类器</strong>，最终输出<strong class="kk iu">边界框和预测类</strong>。</p><h1 id="f7e1" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">利用时间上下文检测难以看见的物体</h1><p id="0bdb" class="pw-post-body-paragraph ki kj it kk b kl mo ju kn ko mp jx kq kr mq kt ku kv mr kx ky kz ms lb lc ld im bi translated">现在，让我们继续讨论 R-CNN 的背景。通常，对象检测模型对单帧输入进行操作，即，它们仅使用来自一个图像的信息来进行预测。这就是上下文 R-CNN 的不同之处。</p><figure class="lh li lj lk gt ll gh gi paragraph-image"><div role="button" tabindex="0" class="lm ln di lo bf lp"><div class="gh gi mu"><img src="../Images/b55ae986097c78aedccbd99177b30f22.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bYCE8S1uWQkNHKBJoy1-EQ.png"/></div></div><p class="ls lt gj gh gi lu lv bd b be z dk translated">上下文 R-CNN 的高级架构，具有窗口滑块、短期和长期记忆库。来源:<a class="ae le" href="https://arxiv.org/pdf/1912.03538.pdf" rel="noopener ugc nofollow" target="_blank">【1】</a></p></figure><p id="179d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们来分解整个架构:作为输入，使用来自不同时间的同一场景的多个帧。其中一帧(蓝色)是我们想要对其执行对象检测的图像。例如，该窗口可以设置为 5，则 5 幅图像将作为输入。也许第一张是在早上 5 点拍的，最后一张是在下午 3 点。对于这些图像中的每一个，特征由更快的 R-CNN 提取，以生成每个盒子的 RPN 特征(这就是为什么我们之前重新访问了 RPN)。因此，现在我们有了每个图像的每个边界框的特征。然后，我们将所有这些特征传递到短期记忆库中，并保存在那里。长期记忆库也包含来自过去帧的特征，但是在更长的时间范围内，例如来自过去的 50 个图像。<strong class="kk iu">只有关键帧特征，即我们实际上想要对其执行对象检测的图像的特征，被直接传递到注意机制。</strong></p><figure class="lh li lj lk gt ll gh gi paragraph-image"><div class="gh gi mv"><img src="../Images/c5036eea777489cf5de40f59a8ad0a29.png" data-original-src="https://miro.medium.com/v2/resize:fit:1140/format:webp/1*DkGJpX3zdqrz_VAuerJV8g.png"/></div><p class="ls lt gj gh gi lu lv bd b be z dk translated">用于整合来自短期和长期记忆库的保存特征的注意机制。这种关注决定了哪些特性是相关的，这比硬编码一些 IoU 匹配要好得多。来源:<a class="ae le" href="https://arxiv.org/pdf/1912.03538.pdf" rel="noopener ugc nofollow" target="_blank">【1】</a></p></figure><p id="a875" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这种注意机制在两个方面发挥作用:短时记忆和长时记忆。每次，来自关键帧的输入特征与来自记忆库的上下文特征一起被传递到注意块。在<strong class="kk iu">注意块</strong>中，模型学习<strong class="kk iu">哪些特征最相关，从而做出正确的检测</strong>。这意味着，如果模型意识到对于一个提议的边界框，它已经在过去看到过那些特征，它可以<strong class="kk iu">组合来自关键帧和来自过去</strong>的特征来执行对象的<strong class="kk iu">重新识别。我个人觉得这个机制还是蛮厉害的！</strong></p><h1 id="0805" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">结果</h1><p id="58f3" class="pw-post-body-paragraph ki kj it kk b kl mo ju kn ko mp jx kq kr mq kt ku kv mr kx ky kz ms lb lc ld im bi translated">在本文的介绍中，我们已经看到了检测移出框架或被遮挡的对象的改进性能。让我们看看上下文 R-CNN 可以发挥作用的更多领域，如<strong class="kk iu">弱光、远处的物体或背景干扰物。</strong></p><figure class="lh li lj lk gt ll gh gi paragraph-image"><div role="button" tabindex="0" class="lm ln di lo bf lp"><div class="gh gi mw"><img src="../Images/199aa308219d708eb0d7cd40157f15f9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BnQT0KTsWD8pk7zWIIcAOA.png"/></div></div><p class="ls lt gj gh gi lu lv bd b be z dk translated">在具有挑战性的场景中，上下文 R-CNN 在对象检测方面的出色性能。来源:<a class="ae le" href="https://arxiv.org/pdf/1912.03538.pdf" rel="noopener ugc nofollow" target="_blank">【1】</a></p></figure><p id="1e09" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这些结果看起来相当令人印象深刻！在左侧，您可以看到没有上下文的性能，在右侧，上下文 R-CNN 使用内存库来检测额外的对象。让我们来看看一些定量结果:</p><figure class="lh li lj lk gt ll gh gi paragraph-image"><div role="button" tabindex="0" class="lm ln di lo bf lp"><div class="gh gi mx"><img src="../Images/b8e99a9943edd634b76bbe6df956dadf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IIDfggwZYEmOE-Knl9EmGA.png"/></div></div><p class="ls lt gj gh gi lu lv bd b be z dk translated">有上下文的 R-CNN 与没有上下文的更快 R-CNN 的性能比较。对于所有类别，新模型都优于单帧检测。来源:<a class="ae le" href="https://arxiv.org/pdf/1912.03538.pdf" rel="noopener ugc nofollow" target="_blank">【1】</a></p></figure><p id="5443" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">毫不奇怪，对于动物数据集中的所有类，上下文 R-CNN 在单个帧上的检测性能优于其他检测(感兴趣的人可以使用 Snapshot Serengeti)。这可以归因于语境机制。</p><p id="1df5" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">同样非常有趣的是注意力机制的可视化。该块从时间上更接近关键帧的上下文帧中了解到特征对于对象重新识别任务也是最相关的。虽然这对人类来说是直观的，但令人印象深刻的是模型能够学习这一点。</p><figure class="lh li lj lk gt ll gh gi paragraph-image"><div role="button" tabindex="0" class="lm ln di lo bf lp"><div class="gh gi my"><img src="../Images/635ad95e416db00c4bfb4e27f708c99e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VKyDFg9cxNVCIwsU2pe0GA.png"/></div></div><p class="ls lt gj gh gi lu lv bd b be z dk translated">时间上分离的帧对于关键帧中的检测的重要性的可视化。这些帧在时间上越接近，它们对于检测任务就变得越重要。来源:<a class="ae le" href="https://arxiv.org/pdf/1912.03538.pdf" rel="noopener ugc nofollow" target="_blank">【2】</a></p></figure><p id="220d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这种模型的另一个大优势是，上下文机制可以轻松地集成到任何快速、更快或屏蔽的 R-CNN 架构中。</p><h1 id="9f86" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">包装它</h1><p id="019a" class="pw-post-body-paragraph ki kj it kk b kl mo ju kn ko mp jx kq kr mq kt ku kv mr kx ky kz ms lb lc ld im bi translated">在本文中，您已经了解了 Context R-CNN，这是一篇以短期和长期特征记忆库的形式利用时间上下文进行对象检测的论文。虽然我希望这个故事能让你对这篇论文有一个很好的初步了解，但是还有很多东西需要发现。因此，我会鼓励你自己阅读这篇论文，即使你是这个领域的新手。你必须从某个地方开始；)</p><p id="60fd" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如果你对论文中介绍的方法有更多的细节感兴趣，请随时在 Twitter 上给我留言，我的账户链接在我的媒体简介上。</p><p id="8181" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我希望你喜欢这篇论文的解释。如果你对这篇文章有任何意见，或者如果你看到任何错误，请随时留下评论。</p><p id="946e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">最后但同样重要的是，如果你想在高级计算机视觉领域更深入地探索，考虑成为我的追随者</strong>。我试着每周发一篇文章，让你和其他人了解计算机视觉研究的最新进展。</p></div><div class="ab cl mz na hx nb" role="separator"><span class="nc bw bk nd ne nf"/><span class="nc bw bk nd ne nf"/><span class="nc bw bk nd ne"/></div><div class="im in io ip iq"><p id="8409" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">参考资料:</p><p id="baf3" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">[1] Beery，Sara 等人，“背景 r-cnn:每个摄像机对象检测的长期时间背景”<em class="lf">IEEE/CVF 计算机视觉和模式识别会议论文集</em>。2020.【https://arxiv.org/pdf/1912.03538.pdf T4】</p><p id="78af" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">[2]任，，等.“快速 r-cnn:面向区域提议网络的实时目标检测”<em class="lf">神经信息处理系统进展</em>28(2015):91–99。<a class="ae le" href="https://arxiv.org/pdf/1506.01497.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1506.01497.pdf</a></p></div></div>    
</body>
</html>