<html>
<head>
<title>NHS English Prescribing Data Analysis using Python (Part 1)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用Python进行NHS英语处方数据分析(第1部分)</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/nhs-english-prescribing-data-analysis-using-python-9d86ec610880?source=collection_archive---------23-----------------------#2021-06-26">https://towardsdatascience.com/nhs-english-prescribing-data-analysis-using-python-9d86ec610880?source=collection_archive---------23-----------------------#2021-06-26</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="5c7d" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">提取和转换</h2></div><h1 id="20a8" class="kf kg iq bd kh ki kj kk kl km kn ko kp jw kq jx kr jz ks ka kt kc ku kd kv kw bi translated">共享的方法:</h1><p id="37df" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">使用请求从CKAN API提取数据(存储效率)。</p><p id="abe7" class="pw-post-body-paragraph kx ky iq kz b la lt jr lc ld lu ju lf lg lv li lj lk lw lm ln lo lx lq lr ls ij bi translated">使用Pandas读取数据(内存效率)。</p><h1 id="153e" class="kf kg iq bd kh ki kj kk kl km kn ko kp jw kq jx kr jz ks ka kt kc ku kd kv kw bi translated">目的</h1><figure class="lz ma mb mc gt md gh gi paragraph-image"><div role="button" tabindex="0" class="me mf di mg bf mh"><div class="gh gi ly"><img src="../Images/c95eecc932617ffef1102f98b02b8909.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*HwgOX97CzYbZVxfV"/></div></div><p class="mk ml gj gh gi mm mn bd b be z dk translated">玛格达·波吕祖克在<a class="ae mo" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="71c8" class="pw-post-body-paragraph kx ky iq kz b la lt jr lc ld lu ju lf lg lv li lj lk lw lm ln lo lx lq lr ls ij bi translated">在一个相对凉爽的夜晚打喷嚏和咳嗽时，我决定探索季节性过敏性鼻炎(花粉热)的处方数据。更远的目标是利用花粉计数数据和处方数据建立一个模型。</p><p id="0286" class="pw-post-body-paragraph kx ky iq kz b la lt jr lc ld lu ju lf lg lv li lj lk lw lm ln lo lx lq lr ls ij bi translated">在这个持续的过程中，我面临着多重挑战。在这里，我描述了在2017款Macbook Air (8GB内存、22GB可用存储空间、i5双核处理器)的限制下提取数据的方法</p></div><div class="ab cl mp mq hu mr" role="separator"><span class="ms bw bk mt mu mv"/><span class="ms bw bk mt mu mv"/><span class="ms bw bk mt mu"/></div><div class="ij ik il im in"><h1 id="e120" class="kf kg iq bd kh ki mw kk kl km mx ko kp jw my jx kr jz mz ka kt kc na kd kv kw bi translated">从CKAN API中提取数据</h1><p id="cab6" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">第一步需要获得足够的数据，以分成开发、训练和测试样本。使用一整年的数据来开发我的未来模型似乎是合理的，因为整个季节都会被考虑在内。</p><p id="ec94" class="pw-post-body-paragraph kx ky iq kz b la lt jr lc ld lu ju lf lg lv li lj lk lw lm ln lo lx lq lr ls ij bi translated">第一个问题，存储。<a class="ae mo" href="https://www.nhsbsa.nhs.uk/prescription-data/prescribing-data/english-prescribing-data-epd" rel="noopener ugc nofollow" target="_blank"> NHSBA网站</a>(一个月)上的一个CSV文件大小至少为6GB。这占了剩余可用存储空间的近30%。</p><p id="183d" class="pw-post-body-paragraph kx ky iq kz b la lt jr lc ld lu ju lf lg lv li lj lk lw lm ln lo lx lq lr ls ij bi translated">解决办法？通过NHSBSA使用的<a class="ae mo" href="https://docs.ckan.org/en/2.8/api/" rel="noopener ugc nofollow" target="_blank"> API </a>将CSV读入内存中的Pandas数据帧。代码如下。</p><pre class="lz ma mb mc gt nb nc nd ne aw nf bi"><span id="2b8c" class="ng kg iq nc b gy nh ni l nj nk">#Import the requests module. </span><span id="6cd4" class="ng kg iq nc b gy nl ni l nj nk">import requests</span><span id="65e2" class="ng kg iq nc b gy nl ni l nj nk">#The url, described within the API documentation, is used to view the datasets available.</span><span id="f881" class="ng kg iq nc b gy nl ni l nj nk">url = “<a class="ae mo" href="http://opendata.nhsbsa.net/api/3/action/package_list" rel="noopener ugc nofollow" target="_blank">http://opendata.nhsbsa.net/api/3/action/package_list</a>"</span><span id="f098" class="ng kg iq nc b gy nl ni l nj nk">#Request to obtain data from a url.<br/>response = requests.get(url)</span><span id="271a" class="ng kg iq nc b gy nl ni l nj nk">#Status code 200 means the request was successful.</span><span id="b746" class="ng kg iq nc b gy nl ni l nj nk">response.status_code</span><span id="4fa3" class="ng kg iq nc b gy nl ni l nj nk">#Display the response as a dictionary.<br/>response.json()</span></pre><figure class="lz ma mb mc gt md gh gi paragraph-image"><div role="button" tabindex="0" class="me mf di mg bf mh"><div class="gh gi nm"><img src="../Images/264b5958122b19b46db907bf2ca91477.png" data-original-src="https://miro.medium.com/v2/resize:fit:1182/format:webp/1*nMomhgm2Opt9E4cQWIvUMg.png"/></div></div></figure><pre class="lz ma mb mc gt nb nc nd ne aw nf bi"><span id="534a" class="ng kg iq nc b gy nh ni l nj nk">#Print the values in the ‘result’ key.<br/>#The ‘result’ key contains the names of datasets which are available.<br/>print(response.json()[‘result’])</span></pre><figure class="lz ma mb mc gt md gh gi paragraph-image"><div role="button" tabindex="0" class="me mf di mg bf mh"><div class="gh gi nn"><img src="../Images/f371b0f2b55ef181c4e7e8b80bc81487.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*o5VXfFZt27rsr632AaRhdA.png"/></div></div></figure><pre class="lz ma mb mc gt nb nc nd ne aw nf bi"><span id="f06e" class="ng kg iq nc b gy nh ni l nj nk">#Using the API documentation, the url containing the medicines data #was deduced and tested.</span><span id="3198" class="ng kg iq nc b gy nl ni l nj nk">url2 = "<a class="ae mo" href="http://opendata.nhsbsa.net/api/3/action/package_show?id=english-prescribing-data-epd" rel="noopener ugc nofollow" target="_blank">http://opendata.nhsbsa.net/api/3/action/package_show?id=english-prescribing-data-epd</a>"<br/>response2 = requests.get(url2)<br/>response2_json = response2.json() <br/>response2.status_code</span><span id="3dd4" class="ng kg iq nc b gy nl ni l nj nk">#The keys within the output of the results were investigated.<br/>response2_json['result'].keys()</span></pre><figure class="lz ma mb mc gt md gh gi paragraph-image"><div role="button" tabindex="0" class="me mf di mg bf mh"><div class="gh gi no"><img src="../Images/0a61993767bbe8eaeeac13531662fab2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*diBTVKvfVsGyZ0PwVwV1sg.png"/></div></div></figure><pre class="lz ma mb mc gt nb nc nd ne aw nf bi"><span id="2e75" class="ng kg iq nc b gy nh ni l nj nk">#Each dataset for medicines in secondary care is stored within csv files downloadable through a url.</span><span id="4474" class="ng kg iq nc b gy nl ni l nj nk">response2_json[‘result’][‘resources’][0][‘url’]</span></pre><figure class="lz ma mb mc gt md gh gi paragraph-image"><div role="button" tabindex="0" class="me mf di mg bf mh"><div class="gh gi np"><img src="../Images/d44d5f9b0004d56342899a1d7cd1d769.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LK19KyNWrdzLB3V-n_ZecA.png"/></div></div></figure><pre class="lz ma mb mc gt nb nc nd ne aw nf bi"><span id="fa04" class="ng kg iq nc b gy nh ni l nj nk">#Each url is passed into a list.</span><span id="b026" class="ng kg iq nc b gy nl ni l nj nk">datasets =[]<br/>for num in range(len(response2_json['result']['resources'])):<br/>    datasets.append(response2_json['result']['resources'][num]['url'])<br/>datasets</span></pre></div><div class="ab cl mp mq hu mr" role="separator"><span class="ms bw bk mt mu mv"/><span class="ms bw bk mt mu mv"/><span class="ms bw bk mt mu"/></div><div class="ij ik il im in"><h1 id="3f12" class="kf kg iq bd kh ki mw kk kl km mx ko kp jw my jx kr jz mz ka kt kc na kd kv kw bi translated"><strong class="ak">读取数据</strong></h1><p id="349c" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">试图阅读上面列表中的每个CSV会导致浪费数小时的等待时间，然后Jupyter笔记本崩溃。首先，我调查了一个数据帧的内存使用情况。</p><pre class="lz ma mb mc gt nb nc nd ne aw nf bi"><span id="fb35" class="ng kg iq nc b gy nh ni l nj nk">#Here, an entire CSV (one month) is read to memory. </span><span id="40b7" class="ng kg iq nc b gy nl ni l nj nk">import pandas as pd<br/>test_df = pd.read_csv(datasets[60], sep =",")<br/></span><span id="2063" class="ng kg iq nc b gy nl ni l nj nk">#The info() function outputs information about the data frame. #Additional arguments such as 'memory_usage' and 'null_counts' can #be used to determine the amount of information returned at the cost #of computational power.</span><span id="ff03" class="ng kg iq nc b gy nl ni l nj nk">test_df.info(memory_usage='deep', null_counts=True, verbose=True)</span></pre><figure class="lz ma mb mc gt md gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/bfeacccec697c4167108d92a86037a2e.png" data-original-src="https://miro.medium.com/v2/resize:fit:894/format:webp/1*eZHwvKizvXzZF3OA3hwn6A.png"/></div></figure><p id="d047" class="pw-post-body-paragraph kx ky iq kz b la lt jr lc ld lu ju lf lg lv li lj lk lw lm ln lo lx lq lr ls ij bi translated">23.8GB，惊人！在进一步研究数据以确定最合适的特征、记录和数据类型之后，创建了下面的函数。它节省内存和时间；仅读取所需的列，在适当的情况下将数据类型更改为使用较少内存的类型，并根据需要过滤记录。</p><pre class="lz ma mb mc gt nb nc nd ne aw nf bi"><span id="90bc" class="ng kg iq nc b gy nh ni l nj nk">from datetime import datetime</span><span id="74c1" class="ng kg iq nc b gy nl ni l nj nk">def LargeCSVsChop(listofCSV_URLs):<br/>    <br/>    <br/>    dfs = []<br/>    for num in range(len(listofCSV_URLs)):</span><span id="8c4f" class="ng kg iq nc b gy nl ni l nj nk">feats = ['YEAR_MONTH', 'PCO_NAME', 'PRACTICE_NAME', 'CHEMICAL_SUBSTANCE_BNF_DESCR', <br/>                'BNF_DESCRIPTION', 'TOTAL_QUANTITY', 'ACTUAL_COST']<br/>        <br/>        custom_date_parser = lambda x: datetime.strptime(x, "%Y%m")<br/>        <br/>        df = pd.read_csv(listofCSV_URLs[num], sep =",", usecols = feats, date_parser=custom_date_parser,<br/>                                                                         parse_dates=['YEAR_MONTH'])<br/>        <br/>        """"<br/>        'feats' is a list of the columns/features from the CSV that I want Pandas to read to memory.<br/>        <br/>        types = {'TOTAL_QUANTITY': int, 'ACTUAL_COST':float}<br/>        <br/>        Types is a dictionary of columns and the datatypes they should be read as using dtypes = types in the read_csv function. <br/>        <br/>        The types are optimal so no changes will be made but the dictionary can be used in future.    <br/>        <br/>        <br/>        'custom_data_parser' will be used to convert the dtype of the YEAR_MONTH feature to date format.</span><span id="d61f" class="ng kg iq nc b gy nl ni l nj nk">The dataframe will read only feats and convert the dtype of the YEAR_MONTH feature.<br/>        <br/>        'df' is used as a variable again for the filtered dataframe to prompt the deallocation of the unfiltered<br/>        #dataframe object from memory by dropping the reference count to zero and prompting garbage collection algorithms.<br/>        <br/>        <br/>        Oral antihistamines will be the focus of this analysis. 'CHEMICAL_SUBSTANCE_BNF_DESCR' is used to ensure all<br/>        forms and brands of the drugs are captured. Further work is done using 'BNF_DESCRIPTION' to remove unwanted products.<br/>        <br/>        """<br/>        <br/>        df = df[ <br/>               (df['CHEMICAL_SUBSTANCE_BNF_DESCR']=='Cetirizine hydrochloride')                              | <br/>               (df['CHEMICAL_SUBSTANCE_BNF_DESCR']=='Loratadine')                                            |<br/>               (df['CHEMICAL_SUBSTANCE_BNF_DESCR']=='Desloratadine')                                         |<br/>               (df['CHEMICAL_SUBSTANCE_BNF_DESCR']=='Fexofenadine hydrochloride')                            |<br/>               (df['CHEMICAL_SUBSTANCE_BNF_DESCR']=='Acrivastine')                                           |<br/>               (df['CHEMICAL_SUBSTANCE_BNF_DESCR']=='Bilastine')                                             |<br/>               (df['CHEMICAL_SUBSTANCE_BNF_DESCR']=='Levocetirizine')                                        |<br/>               (df['CHEMICAL_SUBSTANCE_BNF_DESCR']=='Mizolastine')                                           |<br/>               (df['CHEMICAL_SUBSTANCE_BNF_DESCR']=='Chlorphenamine maleate')                                &amp; <br/>               (df['BNF_DESCRIPTION']!='Chlorphenamine 10mg/1ml solution for injection ampoules')            |<br/>               (df['CHEMICAL_SUBSTANCE_BNF_DESCR']=='Promethazine hydrochloride')                            &amp;<br/>               (df['BNF_DESCRIPTION']!='Promethazine 25mg/1ml solution for injection ampoules')              &amp;<br/>               (df['BNF_DESCRIPTION']!='Phenergan 25mg/1ml solution for injection ampoules')                   <br/>             ]<br/>      <br/>        dfs.append(df)<br/>        <br/>    df_large = pd.concat(dfs)<br/>        <br/>    return df_large</span></pre><p id="8ab0" class="pw-post-body-paragraph kx ky iq kz b la lt jr lc ld lu ju lf lg lv li lj lk lw lm ln lo lx lq lr ls ij bi translated">结果数据框的大小如下所示。一个巨大的差异。</p><pre class="lz ma mb mc gt nb nc nd ne aw nf bi"><span id="5172" class="ng kg iq nc b gy nh ni l nj nk">Jan_2019_df = LargeCSVsChop(datasets[60:61])</span><span id="6de9" class="ng kg iq nc b gy nl ni l nj nk">Jan_2019_df.info(memory_usage='deep', null_counts=True, verbose=True)</span></pre><figure class="lz ma mb mc gt md gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/9c66982215e564c9b1fad29c6402db64.png" data-original-src="https://miro.medium.com/v2/resize:fit:978/format:webp/1*E8er8aQz1HLIiuJ87Lr8KA.png"/></div></figure></div><div class="ab cl mp mq hu mr" role="separator"><span class="ms bw bk mt mu mv"/><span class="ms bw bk mt mu mv"/><span class="ms bw bk mt mu"/></div><div class="ij ik il im in"><h1 id="173c" class="kf kg iq bd kh ki mw kk kl km mx ko kp jw my jx kr jz mz ka kt kc na kd kv kw bi translated">时间</h1><p id="cced" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">不幸的是，我发现读取CSV所花费的时间仍然相似。我假设，这是由于文档是从URL读取的，因此依赖于带宽。</p><p id="67fb" class="pw-post-body-paragraph kx ky iq kz b la lt jr lc ld lu ju lf lg lv li lj lk lw lm ln lo lx lq lr ls ij bi translated">因此，在本系列的第2部分中，我将通过AWS提供给学生的虚拟机来运行这个特定的任务。这应该释放我的机器，而EDA正在进行。</p><p id="9524" class="pw-post-body-paragraph kx ky iq kz b la lt jr lc ld lu ju lf lg lv li lj lk lw lm ln lo lx lq lr ls ij bi translated">下面的函数是我在算法数据科学模块中学到的，可以用来测量使用/不使用该函数读取CSV所需的时间。</p><pre class="lz ma mb mc gt nb nc nd ne aw nf bi"><span id="a2a5" class="ng kg iq nc b gy nh ni l nj nk">import time<br/>import numpy as np</span><span id="8c67" class="ng kg iq nc b gy nl ni l nj nk">def timefunc(function, arg, repeats = 20):<br/>    <br/>    alltime = []<br/>    <br/>    while  repeats &gt; 0:<br/>        <br/>        """<br/>        Unless specified, the number of repitions will be 20.<br/>        <br/>        """<br/>        <br/>        starttime= time.time() # record the start time</span><span id="6031" class="ng kg iq nc b gy nl ni l nj nk">result = function(arg) # run the function and store in the variable 'result' in case result is needed.</span><span id="9f5f" class="ng kg iq nc b gy nl ni l nj nk">endtime = time.time() # Record end time.</span><span id="a5a9" class="ng kg iq nc b gy nl ni l nj nk">timetaken = endtime - starttime <br/>        <br/>        alltime.append(timetaken) <br/>        <br/>        repeats -=1  <br/>       <br/>    mean = np.mean(alltime) <br/>    std = np.std(alltime) <br/>    error=std/(len(alltime)**0.5)   <br/>    <br/>    return (mean)</span><span id="ee73" class="ng kg iq nc b gy nl ni l nj nk">Without_ncols = timefunc(FuncReadCSV, datasets[60:62], repeats = 1)<br/>With_ncols = timefunc(LargeCSVsChop_MINI, datasets[60:62], repeats = 1)</span><span id="c9be" class="ng kg iq nc b gy nl ni l nj nk">#LargeCSVsChop_MINI reads the CSV and parses the YEAR_MONTH column #but filtering of the dataframe is not included. This is to #facilitate like for like comparisons.</span><span id="bc10" class="ng kg iq nc b gy nl ni l nj nk">import matplotlib.pyplot as plt<br/>%matplotlib inline</span><span id="3860" class="ng kg iq nc b gy nl ni l nj nk">fig,ax1=plt.subplots(figsize=(10,6))</span><span id="37d4" class="ng kg iq nc b gy nl ni l nj nk">plt.xlabel(‘CSV Reading Methods’)<br/>plt.ylabel(‘Mean Time Taken to Read (minutes)’)<br/>plt.title(‘Comparison of Time Taken to Read Two CSVs')</span><span id="64e4" class="ng kg iq nc b gy nl ni l nj nk">plt.yticks(range(100))<br/>            <br/>plt.bar(height=Without_ncols/60,x = 'Without Function', color = 'red')<br/>plt.bar(height=With_ncols/60,x='LargeCSVsChop_MINI', color = 'blue')<br/>plt.savefig('Time taken')</span></pre><figure class="lz ma mb mc gt md gh gi paragraph-image"><div role="button" tabindex="0" class="me mf di mg bf mh"><div class="gh gi ns"><img src="../Images/9a48d215fb50b37c4d93277e37554d49.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yp67QgcVODvb9dakQ-6Fng.png"/></div></div></figure></div><div class="ab cl mp mq hu mr" role="separator"><span class="ms bw bk mt mu mv"/><span class="ms bw bk mt mu mv"/><span class="ms bw bk mt mu"/></div><div class="ij ik il im in"><p id="a0fa" class="pw-post-body-paragraph kx ky iq kz b la lt jr lc ld lu ju lf lg lv li lj lk lw lm ln lo lx lq lr ls ij bi translated">我希望你已经发现这是有用的。我将感谢对我的代码的反馈。我的GitHub库的链接是这里的<a class="ae mo" href="https://github.com/Adenrele/Hayfever-NHS-Prescribing-Data-" rel="noopener ugc nofollow" target="_blank"/>。</p></div><div class="ab cl mp mq hu mr" role="separator"><span class="ms bw bk mt mu mv"/><span class="ms bw bk mt mu mv"/><span class="ms bw bk mt mu"/></div><div class="ij ik il im in"><p id="fd29" class="pw-post-body-paragraph kx ky iq kz b la lt jr lc ld lu ju lf lg lv li lj lk lw lm ln lo lx lq lr ls ij bi translated">有助于这一过程的文章有:</p><p id="a1a5" class="pw-post-body-paragraph kx ky iq kz b la lt jr lc ld lu ju lf lg lv li lj lk lw lm ln lo lx lq lr ls ij bi translated">[1] Vincent Teyssier，针对低内存环境优化熊猫数据帧的大小(2018)，<a class="ae mo" href="https://vincentteyssier.medium.com/optimizing-the-size-of-a-pandas-dataframe-for-low-memory-environment-5f07db3d72e" rel="noopener">https://Vincent Teyssier . medium . com/Optimizing-the-size-of-a-pandas-data frame-for-low-memory-environment-5f 07 db 3d 72e</a></p><p id="a6c9" class="pw-post-body-paragraph kx ky iq kz b la lt jr lc ld lu ju lf lg lv li lj lk lw lm ln lo lx lq lr ls ij bi translated">[2] B.Chen，用Pandas read_csv() (2020，<a class="ae mo" rel="noopener" target="_blank" href="/4-tricks-you-should-know-to-parse-date-columns-with-pandas-read-csv-27355bb2ad0e">https://towards data science . com/4-tricks-you-should-know-to-parse-date-columns-with-Pandas-read-CSV-27355 bb 2 ad 0e</a></p><p id="50aa" class="pw-post-body-paragraph kx ky iq kz b la lt jr lc ld lu ju lf lg lv li lj lk lw lm ln lo lx lq lr ls ij bi translated">[3] B.Chen，为低内存环境优化熊猫数据帧的大小，(2020)，<a class="ae mo" href="https://vincentteyssier.medium.com/optimizing-the-size-of-a-pandas-dataframe-for-low-memory-environment-5f07db3d72e" rel="noopener">https://vincentteyssier . medium . com/Optimizing-the-size-of-a-pandas-data frame-for-low-memory-environment-5f 07 db 3d 72e</a></p></div></div>    
</body>
</html>