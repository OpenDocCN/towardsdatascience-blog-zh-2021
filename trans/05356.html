<html>
<head>
<title>LIME for auditing black-box models</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">审计黑盒模型的时间</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/lime-for-auditing-black-box-models-b97d6d2580b4?source=collection_archive---------35-----------------------#2021-05-11">https://towardsdatascience.com/lime-for-auditing-black-box-models-b97d6d2580b4?source=collection_archive---------35-----------------------#2021-05-11</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="c67a" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">使用石灰(<strong class="ak">本地可解释的解释</strong>)来忠实于你的模型和预测</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi kf"><img src="../Images/1f7457fdf2984e2809a0768e17bb13ff.png" data-original-src="https://miro.medium.com/v2/resize:fit:820/format:webp/1*nTm6oIr4VgmAiEJGd6NLzA.png"/></div><p class="kn ko gj gh gi kp kq bd b be z dk translated">作者创建的图像</p></figure><p id="cf93" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">我们愿意相信这是一个人工智能和人工智能的时代，但我们忘记了，这也是一个黑箱模型的时代，它不提供关于分类器及其决策的公正和解释。</p><p id="e13a" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">做出改变的一个方法是审计黑箱模型。等等…什么是黑盒模型？</p><p id="d903" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated"><strong class="kt ir"> <em class="ln">黑箱模型</em> </strong>指的是完全黑暗的东西，因此，人们只能观察到输入和输出变量，而不能看到里面发生的事情。当我们生活在这个监督学习的世界中时，了解预测背后的原因也同样重要。</p><h2 id="570f" class="lo lp iq bd lq lr ls dn lt lu lv dp lw la lx ly lz le ma mb mc li md me mf mg bi translated">为什么可解释性很重要？</h2><p id="5908" class="pw-post-body-paragraph kr ks iq kt b ku mh jr kw kx mi ju kz la mj lc ld le mk lg lh li ml lk ll lm ij bi translated">我能想到的有数百种应用，它可以派上用场。然而，可解释性之所以重要的主要原因是因为它往往会影响特定的人群，我们希望不惜任何代价避免这种情况。仅仅因为你属于某个种族或地区，你的贷款申请就不被接受，这是没有道理的。我们需要透明的系统，能够向我们解释为什么分类器预测某个类作为输出。</p><p id="0656" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">在这么多不合理的实践之后，机器学习模型出现的主要问题是:<strong class="kt ir">我为什么要相信你？</strong></p><p id="d8f5" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">虽然预测模型应该会产生错误和错误分类，但如果错误变得非常特定于某个特定群体，那么这肯定应该被称为数据中的偏差或模型中的偏差。在许多领域，这些错误分类的后果可能是灾难性的。</p><h2 id="7927" class="lo lp iq bd lq lr ls dn lt lu lv dp lw la lx ly lz le ma mb mc li md me mf mg bi translated"><strong class="ak">石灰—当地可解释的说明</strong></h2><p id="1acd" class="pw-post-body-paragraph kr ks iq kt b ku mh jr kw kx mi ju kz la mj lc ld le mk lg lh li ml lk ll lm ij bi translated">LIME帮助你发现和理解个人预测。LIME提供了对预测值和目标变量之间关系的定性理解。它可以帮助你分析一个样本，以及为什么它会被这样分类。</p><p id="f292" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">为了向用户解释预测，它必须是<strong class="kt ir">可解释的</strong>、<strong class="kt ir">忠实的</strong>和<strong class="kt ir">模型不可知的</strong>；模型不可知论意味着它应该在任何模型上工作，而忠实意味着它应该保持其完整性。LIME主要依赖于特征和可解释数据表示之间的区别。例如:在文本分类中，特征是单词嵌入；可解释的表示是指示单词存在或不存在的向量。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="mm mn l"/></div></figure><p id="2820" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">当你训练完你的模型后，石灰可以很容易地用在jupyter笔记本上。LIME的输出显示了每个特征对特定数据样本预测的贡献。这类似于要素重要性，但它只对一个特定的数据集更明显。这有助于我们处理各种亚人群，并检测我们的数据集或模型中的偏差(如果有的话！)</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mo"><img src="../Images/66f74ede0dfe72a2d1cc5bcc218f7f1b.png" data-original-src="https://miro.medium.com/v2/resize:fit:762/format:webp/1*JgAO1vO1uEDLZ4F79nD0xg.png"/></div><p class="kn ko gj gh gi kp kq bd b be z dk translated">预测为1的随机样本的可解释性(图片由作者提供)</p></figure><p id="f6ea" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">有时，一个特征可能会在这里结束，虽然与特定结果没有直接关系，但它可能与一个或多个其他选定的特征有相关性或功能依赖性，因此，可能间接依赖于一些人口统计变量并导致数据偏差。将它与整个模型的特征重要性数据进行比较是一个很好的实践。</p><p id="d6d6" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">虽然LIME有助于提高可解释性，但我认为在公共或私营部门部署模型时，询问一些关于模型的问题也很重要:</p><ul class=""><li id="0f79" class="mp mq iq kt b ku kv kx ky la mr le ms li mt lm mu mv mw mx bi translated">精确度是如何衡量的？</li><li id="c46f" class="mp mq iq kt b ku my kx mz la na le nb li nc lm mu mv mw mx bi translated">各个亚人群之间的准确率是一样的吗？</li><li id="ada0" class="mp mq iq kt b ku my kx mz la na le nb li nc lm mu mv mw mx bi translated">为什么会出现错误？</li></ul><h2 id="ef05" class="lo lp iq bd lq lr ls dn lt lu lv dp lw la lx ly lz le ma mb mc li md me mf mg bi translated">TL；博士；医生</h2><ul class=""><li id="93de" class="mp mq iq kt b ku mh kx mi la nd le ne li nf lm mu mv mw mx bi translated">让你的模型具有可解释性和可读性，让你的客户信任你。</li><li id="037b" class="mp mq iq kt b ku my kx mz la na le nb li nc lm mu mv mw mx bi translated">石灰是分类器解释其预测的方法。</li></ul></div></div>    
</body>
</html>