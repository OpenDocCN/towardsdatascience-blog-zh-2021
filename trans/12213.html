<html>
<head>
<title>An Intro to Collaborative Filtering for Movie Recommendation</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">电影推荐的协同过滤介绍</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/a-beginner-friendly-guide-to-recommender-system-3f5fa2a57c02?source=collection_archive---------3-----------------------#2021-12-11">https://towardsdatascience.com/a-beginner-friendly-guide-to-recommender-system-3f5fa2a57c02?source=collection_archive---------3-----------------------#2021-12-11</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="17e4" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">推荐系统分步指南</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/2c4c2d2a16f2e46a95ea1365e2f0d2d9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*94aUowJ-SRS1O-omX68-EA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">推荐系统备忘单(图片来自作者的<a class="ae ky" href="https://www.visual-design.net/" rel="noopener ugc nofollow" target="_blank">网站</a></p></figure><p id="5a14" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">随着我们对个性化内容推送需求的增加，推荐系统已经成为一个新兴的话题。我想我们都熟悉 YouTube 上的推荐视频，我们都——不止一次——是深夜网飞狂看的受害者。</p><p id="4734" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">推荐系统中有两种流行的方法，基于协作的过滤和基于内容的过滤。<strong class="lb iu">基于内容的</strong>过滤根据内容属性(如流派、语言、视频长度)预测观众可能喜欢的内容。而<strong class="lb iu">协同过滤</strong>基于其他相似用户的偏好进行预测。因此，协同过滤方法倾向于基于实例的学习，通常由手头有大量数据的大公司应用。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><p id="af20" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在本文中，我将关注基于协作的过滤，并简要介绍如何使用属于这一类别的两种算法进行电影推荐，<strong class="lb iu"> K 最近邻(KNN)和奇异值分解(SVD)。</strong></p><p id="527f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我使用来自 Kaggle 的<a class="ae ky" href="https://www.kaggle.com/rounakbanik/the-movies-dataset" rel="noopener ugc nofollow" target="_blank">电影数据集</a>来预测个人级别的推荐电影。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mc"><img src="../Images/7be557ad205de34da347bff7431d7ee3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eXjgdRiYKn2oFfTdbINSyA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">评级数据集</p></figure><h1 id="aed8" class="md me it bd mf mg mh mi mj mk ml mm mn jz mo ka mp kc mq kd mr kf ms kg mt mu bi translated">推荐系统的 EDA</h1><p id="e152" class="pw-post-body-paragraph kz la it lb b lc mv ju le lf mw jx lh li mx lk ll lm my lo lp lq mz ls lt lu im bi translated">每种机器学习算法都需要不同的方式来探索数据集，以获得有价值的见解。我使用了以下三种技术来研究手头的数据。要查看更全面的 EDA 指南，请查看我的博客。</p><div class="na nb gp gr nc nd"><a rel="noopener follow" target="_blank" href="/semi-automated-exploratory-data-analysis-eda-in-python-7f96042c9809"><div class="ne ab fo"><div class="nf ab ng cl cj nh"><h2 class="bd iu gy z fp ni fr fs nj fu fw is bi translated">Python 中的半自动探索性数据分析(EDA)</h2><div class="nk l"><h3 class="bd b gy z fp ni fr fs nj fu fw dk translated">一键式全面数据探索流程</h3></div><div class="nl l"><p class="bd b dl z fp ni fr fs nj fu fw dk translated">towardsdatascience.com</p></div></div><div class="nm l"><div class="nn l no np nq nm nr ks nd"/></div></div></a></div><h2 id="0880" class="ns me it bd mf nt nu dn mj nv nw dp mn li nx ny mp lm nz oa mr lq ob oc mt od bi translated">1.唯一计数和数据形状</h2><p id="6d94" class="pw-post-body-paragraph kz la it lb b lc mv ju le lf mw jx lh li mx lk ll lm my lo lp lq mz ls lt lu im bi translated">首先，概述数据集中包括多少不同的用户和电影。使用<code class="fe oe of og oh b">df.nunique(axis = 0)</code>很容易做到这一点，然后将其绘制成条形图。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oi"><img src="../Images/e44f007d823f41169f0d4f81194e64ba.png" data-original-src="https://miro.medium.com/v2/resize:fit:1308/format:webp/1*qpRdnbzznokq9eHgTCEMNQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">唯一计数代码(图片由作者提供)</p></figure><h2 id="d6d9" class="ns me it bd mf nt nu dn mj nv nw dp mn li nx ny mp lm nz oa mr lq ob oc mt od bi translated">2.单变量分析</h2><p id="33d5" class="pw-post-body-paragraph kz la it lb b lc mv ju le lf mw jx lh li mx lk ll lm my lo lp lq mz ls lt lu im bi translated">单变量分析(一次分析一个特征)有助于我们更好地理解三个问题:</p><ol class=""><li id="b31d" class="oj ok it lb b lc ld lf lg li ol lm om lq on lu oo op oq or bi translated"><em class="os">评论最多的电影有哪些？</em></li><li id="f10e" class="oj ok it lb b lc ot lf ou li ov lm ow lq ox lu oo op oq or bi translated"><em class="os">提供最多评论的用户是谁？</em></li><li id="685c" class="oj ok it lb b lc ot lf ou li ov lm ow lq ox lu oo op oq or bi translated"><em class="os">收视率的分布情况如何？</em></li></ol><pre class="kj kk kl km gt oy oh oz pa aw pb bi"><span id="bae2" class="ns me it oh b gy pc pd l pe pf"># univariate analysis<br/>plt.figure(1, figsize = (16,4))<br/>df['movieId'].value_counts()[:50].plot(kind = 'bar') #take top 50 movies<br/>plt.figure(2, figsize = (16,4))<br/>df['userId'].value_counts()[:50].plot(kind = 'bar') #take top 50 users<br/>plt.figure(3, figsize = (8,4))<br/>df['rating'].plot(kind = 'hist')</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pg"><img src="../Images/06fe8b31d085b58f839cc783f93effa3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Z5DIW0E_LGInbgMT5cvmDQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">单变量分析结果(图片由作者提供)</p></figure><p id="6b2e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们可以从单变量分析中得到一些启示:</p><p id="78ac" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">1.分级在电影中不是均匀分布的，分级最高的电影是“356 ”,其分级不超过 350；</p><p id="e215" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">2.评级在用户中分布不均，用户最多提供 2400 个左右的评级；</p><p id="cac7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">3.大多数人可能会给出 4 左右的评级</p><h2 id="6b79" class="ns me it bd mf nt nu dn mj nv nw dp mn li nx ny mp lm nz oa mr lq ob oc mt od bi translated">3.汇总分析</h2><p id="c8e5" class="pw-post-body-paragraph kz la it lb b lc mv ju le lf mw jx lh li mx lk ll lm my lo lp lq mz ls lt lu im bi translated">单变量分析给我们更多的是单个电影或用户层面的视图，而聚合分析帮助我们理解元层面的数据。</p><p id="3018" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="os"> 1。每部电影的收视率分布是怎样的？</em></p><pre class="kj kk kl km gt oy oh oz pa aw pb bi"><span id="4f20" class="ns me it oh b gy pc pd l pe pf">ratings_per_user = df.groupby('userId')['movieId'].count() ratings_per_user.hist()</span></pre><p id="038d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">直方图显示，大多数用户(671-80%中的大约 560 人)的评分低于 250。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ph"><img src="../Images/6c889db1e6fc758b1e68ac10f0ee2e6c.png" data-original-src="https://miro.medium.com/v2/resize:fit:778/format:webp/1*Th_S0LKmdcdvl8sWkY-PNw.png"/></div></figure><p id="b9af" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="os"> 2。提供评分的用户分布如何？</em></p><pre class="kj kk kl km gt oy oh oz pa aw pb bi"><span id="c5e1" class="ns me it oh b gy pc pd l pe pf">ratings_per_movie = df.groupby('movieId')['userId'].count() ratings_per_movie.hist()</span></pre><p id="19a1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">直方图显示，大多数电影(大约 9，066-90%中的 8，200 部)的评分低于 25 分。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pi"><img src="../Images/df99cc7939f988f9374aefe8716cf16b.png" data-original-src="https://miro.medium.com/v2/resize:fit:782/format:webp/1*a4EbfJQc-UFbsx8j5n__YQ.png"/></div></figure><p id="713e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在这个阶段，我们应该对手头的数据有一个相当清晰的了解。</p><h1 id="b856" class="md me it bd mf mg mh mi mj mk ml mm mn jz mo ka mp kc mq kd mr kf ms kg mt mu bi translated">基于协作的过滤算法</h1><p id="52af" class="pw-post-body-paragraph kz la it lb b lc mv ju le lf mw jx lh li mx lk ll lm my lo lp lq mz ls lt lu im bi translated">我想介绍两种基于协作的过滤算法——K 近邻和奇异值分解。惊奇库允许我们用几行代码实现这两种算法。</p><pre class="kj kk kl km gt oy oh oz pa aw pb bi"><span id="07d4" class="ns me it oh b gy pc pd l pe pf">from surprise import KNNWithMeans<br/>from surprise import SVD</span><span id="4957" class="ns me it oh b gy pj pd l pe pf"># KNN<br/>similarity = {<br/>    "name": "cosine",<br/>    "user_based": False,  # item-based similarity<br/>}<br/>algo_KNN = KNNWithMeans(sim_options = similarity)</span><span id="ffd4" class="ns me it oh b gy pj pd l pe pf"># SVD<br/>algo_SVD = SVD()</span></pre><p id="55f4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">但是，为了恰当地实现算法，最好对每种算法背后的理论有一个基本的了解。</p><h2 id="8b50" class="ns me it bd mf nt nu dn mj nv nw dp mn li nx ny mp lm nz oa mr lq ob oc mt od bi translated">1.k 最近邻(KNN)</h2><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pk"><img src="../Images/ed1b9b3389dec5ea0d1ce5d8e03b0129.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*y1_t3ZWIaQx9K6SSd_XKWw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">KNN 插图(图片来自作者的<a class="ae ky" href="https://www.visual-design.net/" rel="noopener ugc nofollow" target="_blank">网站</a></p></figure><p id="c618" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">选项<code class="fe oe of og oh b">user_based: False</code>确定该 KNN 使用基于项目的相似性，因此我们基于具有已知评级的类似项目来预测项目 m1 的未知评级。您可以将 k 近邻算法视为在由 n 个用户定义的 n 维空间中表示电影项目。基于<strong class="lb iu">余弦相似度</strong>计算点之间的距离，余弦相似度由两个向量之间的角度决定(如图中 m1 和 m2 所示)。余弦相似度优于欧几里德距离，因为当数据集维数较高时，它受到的影响较小。</p><h2 id="d85b" class="ns me it bd mf nt nu dn mj nv nw dp mn li nx ny mp lm nz oa mr lq ob oc mt od bi translated">2.矩阵分解—奇异值分解(SVM)</h2><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pl"><img src="../Images/3233ab87e1e533d6244538361fee2c06.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LpsMrCR-zviI3StKXj8iQw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">SVD 插图(图片来自作者的<a class="ae ky" href="https://www.visual-design.net/" rel="noopener ugc nofollow" target="_blank">网站</a>)</p></figure><p id="5891" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">奇异值分解是一种<strong class="lb iu">矩阵分解</strong>技术，将矩阵分解为低维矩阵的乘积，然后从最高重要性到最低重要性提取潜在特征。这是一个相当长的句子，所以让我把它分解一下…</p><p id="de24" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">它不像 KNN 那样遍历单个评级，而是将评级矩阵视为一个整体。因此，与 KNN 相比，它的计算成本更低，但可解释性也更差。</p><p id="6b4a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">SVD 提取<strong class="lb iu">潜在特征</strong>(它不是数据集中包含的实际特征，而是算法神奇地发现的有价值的隐藏特征)以形成分解矩阵 U 和 V 转置，并将它们按特征重要性降序排列——就像图中从深蓝色到浅蓝色。然后，它通过采用基于特征重要性的加权方法转置的 U 和 V 的乘积来填充空白评级。这些潜在特征参数通过最小化误差来迭代学习。</p><p id="3d00" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">说到误差，现在来说说模型评估。</p><h1 id="dfde" class="md me it bd mf mg mh mi mj mk ml mm mn jz mo ka mp kc mq kd mr kf ms kg mt mu bi translated">模型评估</h1><p id="b02c" class="pw-post-body-paragraph kz la it lb b lc mv ju le lf mw jx lh li mx lk ll lm my lo lp lq mz ls lt lu im bi translated">协同过滤技术将推荐系统表示为一个回归模型，其输出是一个数字评分值。因此，我们可以将回归评估指标应用到我们的推荐系统中。如果您想深入了解常见的回归评估指标，例如线性回归，您可能会发现“<a class="ae ky" href="https://www.visual-design.net/post/a-simple-practical-guide-to-linear-regression" rel="noopener ugc nofollow" target="_blank">简单实用的线性回归指南</a>”中的模型评估部分很有帮助。</p><div class="na nb gp gr nc nd"><a rel="noopener follow" target="_blank" href="/a-practical-guide-to-linear-regression-3b1cb9e501a6"><div class="ne ab fo"><div class="nf ab ng cl cj nh"><h2 class="bd iu gy z fp ni fr fs nj fu fw is bi translated">线性回归实用指南</h2><div class="nk l"><h3 class="bd b gy z fp ni fr fs nj fu fw dk translated">从 EDA 到特征工程再到模型评估</h3></div><div class="nl l"><p class="bd b dl z fp ni fr fs nj fu fw dk translated">towardsdatascience.com</p></div></div><div class="nm l"><div class="pm l no np nq nm nr ks nd"/></div></div></a></div><p id="ad27" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在这个练习中，我用以下两种方法来评估 KNN 和奇异值分解。</p><h2 id="8004" class="ns me it bd mf nt nu dn mj nv nw dp mn li nx ny mp lm nz oa mr lq ob oc mt od bi translated">1.交互效度分析</h2><p id="aa55" class="pw-post-body-paragraph kz la it lb b lc mv ju le lf mw jx lh li mx lk ll lm my lo lp lq mz ls lt lu im bi translated">惊喜库提供了自动执行交叉验证的<code class="fe oe of og oh b">cross_validate</code>功能。为了让惊喜库理解数据集，我们需要使用<code class="fe oe of og oh b">load_from_df</code>将数据集摄取到惊喜阅读器对象中，并将评分范围保持在 0 到 5 之间。</p><pre class="kj kk kl km gt oy oh oz pa aw pb bi"><span id="c032" class="ns me it oh b gy pc pd l pe pf">from surprise import Dataset<br/>from surprise import Reader</span><span id="1239" class="ns me it oh b gy pj pd l pe pf"># load df into Surprise Reader object<br/>reader = Reader(rating_scale = (0,5))<br/>rating_df = Dataset.load_from_df(df[['userId','movieId','rating']], reader)</span></pre><p id="13a1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然后将算法 _KNN 和算法 _ 奇异值分解传递给交叉验证函数，进行 5 次交叉验证。</p><pre class="kj kk kl km gt oy oh oz pa aw pb bi"><span id="22f3" class="ns me it oh b gy pc pd l pe pf">from surprise.model_selection import cross_validate</span><span id="0a53" class="ns me it oh b gy pj pd l pe pf">cross_validate_KNN = cross_validate(algo_KNN, rating_df, measures=['RMSE', 'MAE'], cv=5, verbose=True)</span><span id="7604" class="ns me it oh b gy pj pd l pe pf">cross_validate_SVD = cross_validate(algo_SVD, rating_df, measures=['RMSE', 'MAE'], cv=5, verbose=True)</span></pre><p id="24fb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">结果显示了 KNN 和奇异值分解的比较。如图所示，奇异值分解具有较小的 RMSE、平均熵值，因此性能优于奇异值分解，并且计算时间也少得多。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pn"><img src="../Images/5a36e6be301a6488174a6de6d34a33a4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7atOQbOFmo3X0O6HNTesdQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">交叉验证结果</p></figure><h2 id="26e9" class="ns me it bd mf nt nu dn mj nv nw dp mn li nx ny mp lm nz oa mr lq ob oc mt od bi translated">2.训练测试分割评估</h2><p id="f449" class="pw-post-body-paragraph kz la it lb b lc mv ju le lf mw jx lh li mx lk ll lm my lo lp lq mz ls lt lu im bi translated">这种方法将数据集分成 80%用于训练，20%用于测试。与交叉验证中迭代模型构建 5 次不同，它将只训练模型一次并测试它一次。我已经定义了函数<code class="fe oe of og oh b">train_test_algo</code>来打印出 RMSE，平均误差，平均误差并返回测试数据帧。</p><pre class="kj kk kl km gt oy oh oz pa aw pb bi"><span id="3df3" class="ns me it oh b gy pc pd l pe pf">from surprise.model_selection import train_test_split<br/>from surprise import accuracy</span><span id="8219" class="ns me it oh b gy pj pd l pe pf"># define train test function<br/>def train_test_algo(algo, label):<br/>    training_set, testing_set = train_test_split(rating_df, test_size = 0.2)<br/>    algo.fit(training_set)<br/>    test_output = algo.test(testing_set)<br/>    test_df = pd.DataFrame(test_output)<br/>    <br/>    print("RMSE -",label, accuracy.rmse(test_output, verbose = False))<br/>    print("MAE -", label, accuracy.mae(test_output, verbose=False))<br/>    print("MSE -", label, accuracy.mse(test_output, verbose=False))<br/>    <br/>    return test_df</span></pre><p id="b521" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们来对比一下模型精度，一窥测试输出。</p><pre class="kj kk kl km gt oy oh oz pa aw pb bi"><span id="8c02" class="ns me it oh b gy pc pd l pe pf">train_test_KNN = train_test_algo(algo_KNN, "algo_KNN")<br/>print(train_test_KNN.head())</span><span id="8874" class="ns me it oh b gy pj pd l pe pf">train_test_SVD = train_test_algo(algo_SVD, "algo_SVD")<br/>print(train_test_SVD.head())</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi po"><img src="../Images/7609196fe6577f821ea80df8372b949f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1158/format:webp/1*gZNrrQ9fP0esPsThQoJ17Q.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">训练测试分割评估结果</p></figure><p id="6747" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">结果与交叉验证非常相似，表明 SVD 误差较小。</p><h1 id="5bba" class="md me it bd mf mg mh mi mj mk ml mm mn jz mo ka mp kc mq kd mr kf ms kg mt mu bi translated">提供顶级推荐</h1><p id="3513" class="pw-post-body-paragraph kz la it lb b lc mv ju le lf mw jx lh li mx lk ll lm my lo lp lq mz ls lt lu im bi translated">仅仅建立模型是不够的。正如您在上面看到的，当前的测试输出只预测随机分配到测试集的用户或电影的评级，我们还希望看到带有电影名称的实际推荐。</p><p id="ea02" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">首先，让我们加载<em class="os">电影元数据</em>表和<em class="os">链接</em>表，这样我们就可以将 movieId 翻译成电影名称。</p><pre class="kj kk kl km gt oy oh oz pa aw pb bi"><span id="d0b6" class="ns me it oh b gy pc pd l pe pf">movie_df = pd.read_csv("../input/the-movies-dataset/movies_metadata.csv")<br/>links_df = pd.read_csv("../input/the-movies-dataset/links.csv")<br/>movie_df['imdb_id'] = movie_df['imdb_id'].apply(lambda x: str(x)[2:].lstrip("0"))<br/>links_df['imdbId'] = links_df['imdbId'].astype(str)</span></pre><p id="bcda" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这是三个数据帧如何链接在一起的示意图。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pp"><img src="../Images/2d87b8816190cb98e8f3df49d27123a7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*T2CXgiiQmqeNLD2b-vGOuA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">合并表格(作者图片来自<a class="ae ky" href="https://www.visual-design.net/" rel="noopener ugc nofollow" target="_blank">网站</a></p></figure><p id="cfb4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然后我定义了一个<code class="fe oe of og oh b">prediction(algo, users_K)</code>函数，允许你为你感兴趣的 K 个用户创建一个数据帧，并在调用预测算法的同时遍历数据集中的所有 9067 部电影。</p><pre class="kj kk kl km gt oy oh oz pa aw pb bi"><span id="0141" class="ns me it oh b gy pc pd l pe pf">def prediction(algo, users_K):<br/>    pred_list = []<br/>    for userId in range(1,users_K):<br/>        for movieId in range(1,9067):<br/>            rating = algo.predict(userId, movieId).est<br/>            pred_list.append([userId, movieId, rating])<br/>    pred_df = pd.DataFrame(pred_list, columns = ['userId', 'movieId', 'rating'])<br/>    return pred_df</span></pre><p id="d7bf" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">最后，<code class="fe oe of og oh b">top_recommendation(pred_df, top_N)</code>执行以下程序:</p><p id="9374" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">1)使用<code class="fe oe of og oh b">pd_merge()</code>将数据集合并在一起；</p><p id="29b8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">2)按用户标识对评分进行分组，并使用<code class="fe oe of og oh b">sort_values()</code>按评分值降序排序；</p><p id="dbf3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">3)使用<code class="fe oe of og oh b">head()</code>获得最高值；</p><p id="b02f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">4)返回排序的推荐和最推荐的电影</p><pre class="kj kk kl km gt oy oh oz pa aw pb bi"><span id="a25d" class="ns me it oh b gy pc pd l pe pf">def top_recommendations(pred_df, top_N):<br/>    link_movie = pd.merge(pred_df, links_df, how='inner', left_on='movieId', right_on='movieId')<br/>    recommended_movie = pd.merge(link_movie, movie_df, how='left', left_on='imdbId', right_on='imdb_id')[['userId', 'movieId', 'rating', 'movieId','imdb_id','title']]<br/>    sorted_df = recommended_movie.groupby(('userId'), as_index = False).apply(lambda x: x.sort_values(['rating'], ascending = False)).reset_index(drop=True)<br/>    top_recommended_movies = sorted_df.groupby('userId').head(top_N)<br/>    return sorted_df, top_recommended_movies</span></pre><p id="5211" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="os">顺便提一下，当在 dataframe 中应用 merge 时，我们需要更加注意连接在一起的键的数据类型，否则您将意外地得到许多空结果。</em></p><p id="1026" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">最后，比较 KNN 和奇异值分解给出的每个用户的前 3 个预测。</p><pre class="kj kk kl km gt oy oh oz pa aw pb bi"><span id="a50e" class="ns me it oh b gy pc pd l pe pf"># KNN predictions<br/>pred_KNN = prediction(algo_KNN, 10)<br/>recommended_movies_KNN, top_recommended_movies_KNN = top_recommendations(pred_KNN, 3)</span><span id="cb32" class="ns me it oh b gy pj pd l pe pf">## SVD predictions<br/>pred_SVD = prediction(algo_SVD, 10)<br/>recommended_movies_SVD, top_recommended_movies_SVD = top_recommendations(pred_SVD, 3)</span></pre><div class="kj kk kl km gt ab cb"><figure class="pq kn pr ps pt pu pv paragraph-image"><img src="../Images/659ec93daef21ccae4cdd9dc87e084f0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1014/format:webp/1*EwvSlNNMjro4wOG6Iol38A.png"/></figure><figure class="pq kn pw ps pt pu pv paragraph-image"><img src="../Images/638abb588706764832881f7ef87d993a.png" data-original-src="https://miro.medium.com/v2/resize:fit:986/format:webp/1*negN7SxS9P2utKNsnghAOg.png"/><p class="ku kv gj gh gi kw kx bd b be z dk px di py pz translated">KNN vs SVD 推荐(图片来自作者的<a class="ae ky" href="https://www.visual-design.net/" rel="noopener ugc nofollow" target="_blank">网站</a></p></figure></div><p id="786d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">正如你所看到的，两种算法给出了不同的建议，KNN 在评分方面显得更慷慨。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><p id="7bf3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">希望你喜欢这篇文章，并感谢到目前为止！如果您想访问完整的代码，请访问我网站上的<a class="ae ky" href="https://www.visual-design.net/code" rel="noopener ugc nofollow" target="_blank">代码片段</a>。如果你想阅读我更多关于媒介的文章，我会非常感谢你的支持，注册成为媒介会员。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h1 id="8772" class="md me it bd mf mg qa mi mj mk qb mm mn jz qc ka mp kc qd kd mr kf qe kg mt mu bi translated">带回家的信息</h1><p id="a1b6" class="pw-post-body-paragraph kz la it lb b lc mv ju le lf mw jx lh li mx lk ll lm my lo lp lq mz ls lt lu im bi translated">本文将带您了解构建推荐系统的过程，并比较 KNN 和奇异值分解提供的推荐。</p><p id="0741" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">第一行摘要:</p><ul class=""><li id="65fc" class="oj ok it lb b lc ld lf lg li ol lm om lq on lu qf op oq or bi translated">推荐系统的 EDA:单变量分析、聚合分析</li><li id="da29" class="oj ok it lb b lc ot lf ou li ov lm ow lq ox lu qf op oq or bi translated">两种协同过滤算法:K 近邻和奇异值分解</li><li id="5565" class="oj ok it lb b lc ot lf ou li ov lm ow lq ox lu qf op oq or bi translated">模型评估:交叉验证与训练测试分割</li><li id="8f8c" class="oj ok it lb b lc ot lf ou li ov lm ow lq ox lu qf op oq or bi translated">提供顶级建议</li></ul><h2 id="2cc8" class="ns me it bd mf nt nu dn mj nv nw dp mn li nx ny mp lm nz oa mr lq ob oc mt od bi translated">更多这样的文章</h2><div class="na nb gp gr nc nd"><a rel="noopener follow" target="_blank" href="/clustering-algorithm-for-customer-segmentation-e2d79e28cbc3"><div class="ne ab fo"><div class="nf ab ng cl cj nh"><h2 class="bd iu gy z fp ni fr fs nj fu fw is bi translated">客户细分的聚类算法</h2><div class="nk l"><h3 class="bd b gy z fp ni fr fs nj fu fw dk translated">K-均值聚类的逐步指南</h3></div><div class="nl l"><p class="bd b dl z fp ni fr fs nj fu fw dk translated">towardsdatascience.com</p></div></div><div class="nm l"><div class="qg l no np nq nm nr ks nd"/></div></div></a></div><div class="na nb gp gr nc nd"><a rel="noopener follow" target="_blank" href="/a-practical-guide-to-linear-regression-3b1cb9e501a6"><div class="ne ab fo"><div class="nf ab ng cl cj nh"><h2 class="bd iu gy z fp ni fr fs nj fu fw is bi translated">线性回归实用指南</h2><div class="nk l"><h3 class="bd b gy z fp ni fr fs nj fu fw dk translated">从 EDA 到特征工程再到模型评估</h3></div><div class="nl l"><p class="bd b dl z fp ni fr fs nj fu fw dk translated">towardsdatascience.com</p></div></div><div class="nm l"><div class="pm l no np nq nm nr ks nd"/></div></div></a></div><div class="na nb gp gr nc nd"><a rel="noopener follow" target="_blank" href="/simple-logistic-regression-using-python-scikit-learn-86bf984f61f1"><div class="ne ab fo"><div class="nf ab ng cl cj nh"><h2 class="bd iu gy z fp ni fr fs nj fu fw is bi translated">Python 中的简单逻辑回归</h2><div class="nk l"><h3 class="bd b gy z fp ni fr fs nj fu fw dk translated">从数据预处理到模型评估的逐步指南</h3></div><div class="nl l"><p class="bd b dl z fp ni fr fs nj fu fw dk translated">towardsdatascience.com</p></div></div><div class="nm l"><div class="qh l no np nq nm nr ks nd"/></div></div></a></div></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><p id="7e7d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="os">原载于 2021 年 12 月 6 日</em><a class="ae ky" href="https://www.visual-design.net/post/semi-automated-exploratory-data-analysis-process-in-python" rel="noopener ugc nofollow" target="_blank"><em class="os">【https://www.visual-design.net】</em></a><em class="os">。</em></p></div></div>    
</body>
</html>