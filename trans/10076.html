<html>
<head>
<title>Implementing Backpropagation From Scratch on Python 3+</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">在Python 3+上从头开始实现反向传播</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/implementing-backpropagation-with-style-in-python-da4c2f49adb4?source=collection_archive---------2-----------------------#2021-09-23">https://towardsdatascience.com/implementing-backpropagation-with-style-in-python-da4c2f49adb4?source=collection_archive---------2-----------------------#2021-09-23</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="bfe4" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">让我们看看理论和实践是不是一回事。</h2></div><p id="aa13" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在<a class="ae lb" rel="noopener" target="_blank" href="/backpropagation-the-natural-proof-946c5abf63b1">的最后一个故事</a>中，我们从头开始推导了所有必要的反向传播方程。我们还介绍了所用的符号，并掌握了算法的工作原理。在这个故事中，我们将着重于用python实现这个算法。</p><p id="d02b" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">让我们首先为我们的神经网络提供一些结构</p><figure class="ld le lf lg gt lh gh gi paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="gh gi lc"><img src="../Images/c0125150abfea1bfbb12f69298c52624.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lVFALLHWHy5uqwXnt10b7w.png"/></div></div></figure><p id="f491" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们将让属性<code class="fe lo lp lq lr b">structure</code>成为一个<strong class="kh ir">列表</strong>，它包含神经网络的每一层中神经元的数量。所以如果我们做<code class="fe lo lp lq lr b">model = Network([784, 30, 10])</code>，那么我们的模型有三层。具有<em class="ls"> 784 </em>个神经元的输入层，具有<em class="ls"> 30 </em>个神经元的隐藏层，以及具有<em class="ls"> 10 </em>个神经元的输出层。请注意，当涉及到偏差和权重等参数时，我们不会考虑输入层。正如我们在乐谱中讨论的。</p><p id="991d" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">下一个重要属性是<code class="fe lo lp lq lr b">Bₙ</code>，这将是一个<strong class="kh ir">列表</strong>，包含整个网络中的所有偏置向量(bᴸ)，逐层排序。让我们停下来谈谈这里的符号。我们已经抛弃了索引符号，取而代之的是逐层(矢量)符号；<code class="fe lo lp lq lr b">Bₙ</code>只有下标表示它拥有<strong class="kh ir">网络</strong>中的所有偏置向量。<br/>如你所见<code class="fe lo lp lq lr b">Bₙ</code>是通过从第一个隐藏层开始迭代所有层，并创建一个长度等于每个层神经元数量的随机NumPy数组(偏置向量)构建的。所以本质上，对于我们刚刚讨论的网络，列表<code class="fe lo lp lq lr b">Bₙ</code>将涉及分别为(30，1)和(10，1)的两个NumPy数组(偏置向量)。</p><p id="8940" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">类似于前面的属性，我们也有<code class="fe lo lp lq lr b">Wₙ</code>，正如你所猜测的，它将是一个<strong class="kh ir">列表</strong>，包含每一层的权重矩阵(Wᴸ).也就是说，对于我们的设置，有两个维数为(784，30)和(30，10)的NumPy数组。至于它是如何构造的，<code class="fe lo lp lq lr b">zip()</code>使得同时在多个集合上循环成为可能；这里使用它来循环两个版本的<code class="fe lo lp lq lr b">structure</code>列表，这样我们总是有当前层和下一层中神经元的数量，并且可以使用它们来初始化正确维度的随机权重矩阵。关于随机性的最后一点是，我们使用NumPy的<code class="fe lo lp lq lr b">randn()</code>函数，它从标准正态分布中提取值。这对于防止由于像Sigmoid这样的一些激活引起的消失梯度问题特别有用，因为我们的大多数值将在0附近，这是Sigmoid具有强梯度的地方。</p><p id="0982" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">既然我们已经定义了网络的结构，我们就可以开始处理反向传播了。让我们从提出我们可能需要的所有方程开始。</p><figure class="ld le lf lg gt lh gh gi paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="gh gi lt"><img src="../Images/d9296fbbea9b648d8094ad51d22a968f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SkmgccD11JGU9CezwbSJMA.png"/></div></div><p class="lu lv gj gh gi lw lx bd b be z dk translated">当我们在实现中使用它们时，我们将引用它们。</p></figure><p id="bf37" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在让我们直接行动吧。</p><figure class="ld le lf lg gt lh gh gi paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="gh gi ly"><img src="../Images/3f4cb45945414ccc319ec4b12c5c9583.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ExQngpO_j6oP4IOQD69NWw.png"/></div></div><p class="lu lv gj gh gi lw lx bd b be z dk translated">整个功能。</p></figure><figure class="ld le lf lg gt lh gh gi paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="gh gi lz"><img src="../Images/4fae6763e5f7391b4d2ca7b8b6df57bd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8U787Zxwls4G-sOaexUpjg.png"/></div></div><p class="lu lv gj gh gi lw lx bd b be z dk translated">初始化</p></figure><p id="652d" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在这里，我们从数据集中的一个例子开始。我们的目标是使用它来查找∂j/∂bᴸ∂j/∂wᴸ的每一层，以便最小化给定示例的成本。因此，我们首先初始化两个变量<code class="fe lo lp lq lr b">მJⳆმWₙₛ</code>和<code class="fe lo lp lq lr b">მJⳆმBₙₛ</code>，它们看起来与<code class="fe lo lp lq lr b">Wₙ</code>和<code class="fe lo lp lq lr b">Bₙ</code>相同，但都是零。因此，一旦我们完成，如果<code class="fe lo lp lq lr b">Wₙ[L]</code>具有与某层l相关联的权重矩阵，那么<code class="fe lo lp lq lr b">მJⳆმWₙₛ[L]</code>将是该层的∂J/∂Wᴸ。这同样适用于<code class="fe lo lp lq lr b">Bₙ[L]</code>和<code class="fe lo lp lq lr b">მJⳆმBₙₛ</code>。对两者来说，下标“s”只是提醒，这些是偏导数，只因为一个例子。</p><p id="fd3e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如果你还记得的话，这个算法包括两遍。首先，通过网络向前传递，其中它使用前两个方程来使用当前权重和偏差找到所有层的<em class="ls"> a </em> ᴸ和zᴸ向量，然后再向后传递，其中我们从δᴴ开始，使用之前找到的zᴸ's和<em class="ls"> a </em> ᴸ's来找到每层的δᴸ，从而找到∂J/∂Wᴸ和∂J/∂bᴸ。</p><figure class="ld le lf lg gt lh gh gi paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="gh gi ma"><img src="../Images/3eb3cce179f0c674a5032f3fcefff264.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*u-PjSUHvtCiPCpMuaKeX3Q.png"/></div></div><p class="lu lv gj gh gi lw lx bd b be z dk translated">向前传球</p></figure><p id="504a" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">正向传递在函数中的第一个for循环中传递。我们首先创建两个空列表<code class="fe lo lp lq lr b">Zₙ</code>和<code class="fe lo lp lq lr b">Aₙ</code>，它们最终将包括我们网络中的所有zᴸ's和ᴸ's。然后我们开始逐层循环，同时保持当前层的偏移向量和权重矩阵(NumPy数组)。<br/>对于每一层，我们使用权重矩阵和偏置向量以及来自前一层的激活(或<em class="ls"> x </em>如果是第一层),根据前两个等式(在上图中)找到<em class="ls"> a </em> ᴸ和zᴸ。)</p><p id="a25e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如果还不清楚，行<code class="fe lo lp lq lr b">z = W.T @ a + b if Zₙ else W.T @ x + b</code>使用NumPy的<code class="fe lo lp lq lr b">.T</code>进行转置，使用NumPy的<code class="fe lo lp lq lr b">@</code>乘以NumPy数组(矩阵乘积)，如果<code class="fe lo lp lq lr b">Zₙ</code>不为真，只使用else旁边右边的表达式；这意味着它是空的，因此它必须是第一次迭代(第一层)。</p><p id="bf91" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">下一行简单地在zᴸ上应用激活来查找<em class="ls">一个</em> ᴸ.(第二个等式。)在这个实现中，我们使用sigmoid函数作为激活；因此，我们也在类外定义了函数</p><figure class="ld le lf lg gt lh gh gi paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="gh gi ma"><img src="../Images/80d9daaabb3a5a7262591da06edf0534.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*H_sXqIzOy6Ch2Kp8b_32KA.png"/></div></div><p class="lu lv gj gh gi lw lx bd b be z dk translated">h(z) = σ(z)像我们预期的那样，按元素方式处理向量。</p></figure><figure class="ld le lf lg gt lh gh gi paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="gh gi mb"><img src="../Images/21baa99a82563d177f5209d521ca590c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Hyzh_lMenS7muw5cLnPWFw.png"/></div></div><p class="lu lv gj gh gi lw lx bd b be z dk translated">我们以后会用这个。</p></figure><p id="2558" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">最后两行只是负责将为当前层计算的zᴸ和ᴸ添加到我们之前介绍的两个列表中。<br/>所以这里的概要是，一旦这个循环完成，我们就把我们网络的所有zᴸ's和一个ᴸ's逐层存储在<code class="fe lo lp lq lr b">Zₙ</code>和<code class="fe lo lp lq lr b">Aₙ</code>中。</p><figure class="ld le lf lg gt lh gh gi paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="gh gi lz"><img src="../Images/e3e86d4571bf0efe1eb5d5adca522db8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LhJFrqi86knz0hYcTQ8WzQ.png"/></div></div><p class="lu lv gj gh gi lw lx bd b be z dk translated">向后传球</p></figure><p id="09ee" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在，就向后传递而言，我们从计算最后一层的索引开始。因为我们从第一个隐藏层0开始计数，所以这个数字将等于我们网络的总层数减去2(或者我们在前面的故事中提到的隐藏层数)。)一旦我们准备好了，我们就用它来在我们网络中循环。带有<code class="fe lo lp lq lr b">range(H, -1, -1)</code>的for循环意味着我们从H开始，每次迭代减去1(最后一个参数)，只要没有达到-1，我们就会继续下去。(因此，最后一次迭代将涉及L = 0，这是第一个隐藏层。)至于我们在for循环体中做了什么，我们只是简单地使用了最后三个等式，非常简单。</p><figure class="ld le lf lg gt lh gh gi paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="gh gi mc"><img src="../Images/3211bbd08251fe1c11ab0769b75f3ea4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pjlnGfwfX-InKlO64SF3oQ.png"/></div></div></figure><p id="3799" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">注意，在代码中，我们使用了<code class="fe lo lp lq lr b">*</code>来表示虚线圆。(for循环中的第一行。)这是因为当乘以NumPy数组时，默认情况下通过<code class="fe lo lp lq lr b">*</code>的乘法是元素方式的。除此之外，一切都应该说得通。一旦for循环完成，意味着它已经遍历了h层到0层，并更新了每个层的∂J/∂Bᴸ &amp; ∂J/∂Wᴸ，是时候简单地</p><figure class="ld le lf lg gt lh gh gi paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="gh gi lz"><img src="../Images/26af728aa8f6205d683c0f65f71ce7b2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uCKQoaCOPCVNWHpDe9ab4Q.png"/></div></div></figure><p id="7267" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">今天到此为止。</p><p id="e4d1" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在剩下的就是实现一个函数来处理我们的小批量数据集，为每个内部观察调用<code class="fe lo lp lq lr b">backprop(x,y)</code>,并使用梯度下降更新我们网络的权重。也就是说，一些简单的事情</p><figure class="ld le lf lg gt lh gh gi paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="gh gi lz"><img src="../Images/dc6e14891dd22389a5aadf9a28df3cc4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7vc3pFus7tuCSeMxp9opQw.png"/></div></div><p class="lu lv gj gh gi lw lx bd b be z dk translated">梯度下降</p></figure><p id="0a7a" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这就是了</p><figure class="ld le lf lg gt lh gh gi paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="gh gi lz"><img src="../Images/076331b696793b9f24fd8a294d3627a2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Okyx8TpPeK1jdeXMBvI6nw.png"/></div></div><p class="lu lv gj gh gi lw lx bd b be z dk translated">整个功能</p></figure><figure class="ld le lf lg gt lh gh gi paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="gh gi md"><img src="../Images/894d95d3bf7d2c5665de9f107488f7ed.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ij_xsNGcPLZL67NZ62pwVQ.png"/></div></div><p class="lu lv gj gh gi lw lx bd b be z dk translated">初始化</p></figure><p id="6223" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">类似于我们在<code class="fe lo lp lq lr b">backprop(x,y)</code>函数中所做的，我们首先初始化两个变量<code class="fe lo lp lq lr b">მJⳆმWₙ</code>和<code class="fe lo lp lq lr b">მJⳆმBₙ</code>，它们看起来与<code class="fe lo lp lq lr b">Wₙ</code>和<code class="fe lo lp lq lr b">Bₙ</code>相同，但都是零。这一次我们去掉了s下标，因为这些将累积所有的<code class="fe lo lp lq lr b">მJⳆმWₙₛ</code>和<code class="fe lo lp lq lr b">მJⳆმBₙₛ</code>由于小批量中的例子，我们将准备使用它们更新权重<code class="fe lo lp lq lr b">Wₙ</code>和<code class="fe lo lp lq lr b">Bₙ</code>。</p><figure class="ld le lf lg gt lh gh gi paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="gh gi lz"><img src="../Images/49e66a2aee1e2fda83194d0cac1727a4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sm-FchEnA8c5_L68t-0HiQ.png"/></div></div><p class="lu lv gj gh gi lw lx bd b be z dk translated">累积梯度</p></figure><p id="0546" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在for循环中，我们检查小批量中的每个示例，使用<code class="fe lo lp lq lr b">backprop(x, y)</code>函数找到因其产生的<code class="fe lo lp lq lr b">მJⳆმBₙₛ</code>和<code class="fe lo lp lq lr b">მJⳆმWₙₛ</code>，然后通过列表理解将结果逐层累积到<code class="fe lo lp lq lr b">მJⳆმBₙ</code>和<code class="fe lo lp lq lr b">მJⳆმWₙ</code>中。<br/>这两行可以分别替换为<code class="fe lo lp lq lr b">მJⳆმBₙ = np.add(მJⳆმBₙ ,მJⳆმBₙₛ)</code>和<code class="fe lo lp lq lr b">მJⳆმWₙ = np.add(მJⳆმWₙ + მJⳆმWₙₛ)</code>，但是这将导致警告，除非您在指定<code class="fe lo lp lq lr b">dtype=object</code>的同时将列表转换为NumPy数组。最后重要的是，一旦这个for循环结束，<code class="fe lo lp lq lr b">მJⳆმBₙ</code>和<code class="fe lo lp lq lr b">მJⳆმWₙ</code>已经在所有的<code class="fe lo lp lq lr b">მJⳆმBₙₛ</code>和<code class="fe lo lp lq lr b">მJⳆმWₙₛ</code>中累加，由于小批量中的所有例子，这意味着下面公式中的两个和已经准备好了。</p><figure class="ld le lf lg gt lh gh gi paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="gh gi lz"><img src="../Images/dc6e14891dd22389a5aadf9a28df3cc4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7vc3pFus7tuCSeMxp9opQw.png"/></div></div><p class="lu lv gj gh gi lw lx bd b be z dk translated">梯度下降</p></figure><figure class="ld le lf lg gt lh gh gi paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="gh gi lz"><img src="../Images/1b6cf39dc4c661dd72af956df437999e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YsamiapDwq4q_H8i8XPsnA.png"/></div></div><p class="lu lv gj gh gi lw lx bd b be z dk translated">使用这些公式</p></figure><p id="35ea" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在最后两行，我们用上面的两个公式更新了每一层的bᴸ和Wᴸ。您应该会多次调用该函数，这取决于历元数(整个数据集的迭代次数)和您的小批量大小。如果您的数据集被分成40个小批，并且您的历元计数是20，那么您将调用20x40=800次。负责的功能如下所示</p><figure class="ld le lf lg gt lh gh gi paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="gh gi lz"><img src="../Images/a19e0b47540f3420b6f54976e8c333e2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*soZk4Pg1rpiahUggytdPZQ.png"/></div></div><p class="lu lv gj gh gi lw lx bd b be z dk translated">训练网络</p></figure><p id="73d4" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">此时，您可以尝试初始化网络，并向其输入一些随机的小批量数据，看看这会如何改变权重和偏差。</p><figure class="ld le lf lg gt lh"><div class="bz fp l di"><div class="me mf l"/></div></figure><p id="bc1e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">最后，您可能想知道我使用的Python变量名是怎么回事。事实证明，只要你的角色在世界范围内的某种语言的字母表/字符集中，Python就不会有问题。例如，<code class="fe lo lp lq lr b">მJⳆმWₙ</code>中的ⳇ和მ都是随机字母表中的字母。出于某种原因，unicode下标对Python来说也不成问题。这是我在给变量名时利用的两个事实，以避免做变量名决定，并使它看起来尽可能数学化。</p><p id="8708" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如果你喜欢阅读，并希望看到更多这样的故事，那么请考虑给帖子一些掌声，并跟我来。下次见，再见。</p><p id="0df4" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">学习资源:</strong></p><p id="a1e0" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">尼尔森迈克尔。<em class="ls">神经网络和深度学习</em>。2019，CHP。1,2.</p></div></div>    
</body>
</html>