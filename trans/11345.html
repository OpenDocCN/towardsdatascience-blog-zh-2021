<html>
<head>
<title>What length of dependencies can LSTM &amp; T-CNN really remember?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">LSTM &amp; T-CNN真正能记住的依赖长度是多少？</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/how-long-dependencies-can-lstm-t-cnn-really-remember-7095509afde8?source=collection_archive---------12-----------------------#2021-11-07">https://towardsdatascience.com/how-long-dependencies-can-lstm-t-cnn-really-remember-7095509afde8?source=collection_archive---------12-----------------------#2021-11-07</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="844f" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsdatascience.com/tagged/tips-and-tricks" rel="noopener" target="_blank">提示和技巧</a></h2><div class=""/><p id="4464" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated"><em class="ku">免责声明:本文假设读者拥有LSTM和CNN神经网络的模型直觉和架构背后的初步知识。</em></p><p id="9931" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">LSTMs是广泛用于顺序建模任务的技术，例如语言建模和时间序列预测。这样的任务通常有长期模式和短期模式，因此学习这两种模式对于准确的预测和估计是很重要的。基于变压器的技术正在兴起，这种技术有助于对长期依赖关系进行建模，远远优于LSTMs。然而，由于数据密集型培训要求和部署复杂性，变压器无法用于所有应用。在这篇文章中，我将比较LSTM和T-CNN在长期信息学习方面的差异。</p><p id="a1f3" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">让我们开始…</p><h2 id="0ce9" class="kv kw iq bd kx ky kz dn la lb lc dp ld kh le lf lg kl lh li lj kp lk ll lm iw bi translated"><strong class="ak">技术TLDR </strong></h2><p id="f90c" class="pw-post-body-paragraph jw jx iq jy b jz ln kb kc kd lo kf kg kh lp kj kk kl lq kn ko kp lr kr ks kt ij bi translated"><strong class="jy ja"> LSTM </strong>是一种长短期记忆神经网络，广泛用于学习序列数据(NLP，时间序列预测等..).由于递归神经网络(<strong class="jy ja"> RNN </strong>)存在消失梯度问题，阻碍网络学习长时间尺度相关性，而LSTM通过引入遗忘门、输入门和输出门来减少这一问题。有了这些门，它就有了代表长期记忆的细胞状态，而隐藏状态则代表短期记忆。遗憾的是，LSTM仍然不是保留长期信息的完美解决方案，因为遗忘之门倾向于从之前的步骤中移除这样的模式(信息衰减)——如果模式对于50步来说不重要，我为什么要保留它？</p><figure class="lt lu lv lw gt lx gh gi paragraph-image"><div role="button" tabindex="0" class="ly lz di ma bf mb"><div class="gh gi ls"><img src="../Images/a5032578046ed88e7c286ac0d624af26.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*T2B-0QNCxxD3bcO25OgGsQ.png"/></div></div><p class="me mf gj gh gi mg mh bd b be z dk translated">香草LSTM。作者图片</p></figure><p id="5ab3" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated"><strong class="jy ja">幂律遗忘门控LSTM (pLSTM) </strong>是最近由英特尔和约翰·霍普斯金的研究人员开发的。尽管LSTM有所改进，但遗忘机制表现出信息的指数衰减，限制了他们捕捉长时间信息的能力。可以阅读<a class="ae mi" href="https://arxiv.org/pdf/2105.05944.pdf" rel="noopener ugc nofollow" target="_blank"> <strong class="jy ja"> <em class="ku">论文</em> </strong> </a>进行详细讲解。总之，由于信息衰减遵循LSTM的指数模式，<strong class="jy ja"> pLSTM </strong>给遗忘门增加了一个衰减因子<em class="ku"> p，这使得LSTM可以控制信息衰减速率，帮助它更好地学习长期依赖关系。</em></p><figure class="lt lu lv lw gt lx gh gi paragraph-image"><div role="button" tabindex="0" class="ly lz di ma bf mb"><div class="gh gi mj"><img src="../Images/a0ed15c8018c369d564013f59f8d1e07.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*37NEmRML6_pIrOnqA_jQrA.png"/></div></div><p class="me mf gj gh gi mg mh bd b be z dk translated">pLSTM。图像<a class="ae mi" href="https://arxiv.org/pdf/2105.05944.pdf" rel="noopener ugc nofollow" target="_blank"> src </a></p></figure><p id="a29c" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated"><strong class="jy ja">时间CNN (T-CNN) </strong>是简单的1D卷积网络，可以应用于时间序列数据而不是图像数据。已知这些层具有时间属性，以学习数据中的全局和局部模式。卷积层还有助于改善模型延迟，因为预测可以并行化，不需要按顺序进行。由于CNN可能是因果性的，这意味着每个预测只能依赖于它以前的预测，因此没有来自未来的泄漏。使用深度神经网络和扩张卷积的组合，TCN构成非常长的有效历史大小。T-CNN有几种变体，如基于注意力的CNN，将LSTM与CNN结合起来，融合其他类型的架构，然而，在这篇文章中，我将坚持使用普通的T-CNN，以使它对我的读者简单，TCNN的变体本身可以是一篇单独的博客文章。可以阅读<a class="ae mi" href="https://arxiv.org/pdf/1608.08242.pdf" rel="noopener ugc nofollow" target="_blank"> <strong class="jy ja"> <em class="ku">论文</em> </strong> </a>进行详细解释。</p><figure class="lt lu lv lw gt lx gh gi paragraph-image"><div role="button" tabindex="0" class="ly lz di ma bf mb"><div class="gh gi mk"><img src="../Images/493dbefe4fd6c08613f6e60a69e50ee0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ntyGJT_emRgBtsBqNv_Hww.png"/></div></div><p class="me mf gj gh gi mg mh bd b be z dk translated">因果卷积与标准卷积构成了T-CNN的基础。作者图片</p></figure><p id="8b0d" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated"><strong class="jy ja">合成数据</strong></p><p id="8dc7" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">让我们从一个简单的加法函数开始:</p><p id="9b1c" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">y = f(xⁿ)+ f(x)</p><p id="7f0c" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">人们可以设计他们自己的功能，但是现在，让我们用这个。我们的假设是，序列越长，LSTM应该越难记住X⁰值，例如，具有2的序列是第1和第2个元素的相加，而100的序列是第1(0)和第100(或-1)个元素的相加。序列越大，LSTM需要携带信息的步骤就越多。</p><figure class="lt lu lv lw gt lx"><div class="bz fp l di"><div class="ml mm l"/></div><p class="me mf gj gh gi mg mh bd b be z dk translated">按作者分类的数据生成器</p></figure><p id="ca18" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated"><strong class="jy ja"> LSTM模型建筑</strong></p><p id="2016" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">我创建了一个香草LSTM架构，并用超参数和堆叠LSTM层进行实验，以验证我们的假设</p><figure class="lt lu lv lw gt lx"><div class="bz fp l di"><div class="ml mm l"/></div><p class="me mf gj gh gi mg mh bd b be z dk translated">香草LSTM建筑和作者培训</p></figure><p id="e9bc" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated"><strong class="jy ja"> CNN模型架构</strong></p><p id="1cc3" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">我创建了一个普通的T-CNN架构，并用超参数如内核大小、过滤器和卷积数来验证我们的假设</p><figure class="lt lu lv lw gt lx"><div class="bz fp l di"><div class="ml mm l"/></div><p class="me mf gj gh gi mg mh bd b be z dk translated">作者的香草T-CNN架构和培训</p></figure><p id="232b" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated"><strong class="jy ja">模型性能结果</strong></p><p id="32ff" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">MSE是评估非偏斜数据的一个很好的方法，但是不太容易解释(除了数学家可以！).为了简化，我们可以看一下当X中的一个改变时，Y的百分比变化。由于合成函数是相加的，将X中的一个改变N个百分点，应该会有y的N/2个百分点的变化，其次我们可以看看<strong class="jy ja"> <em class="ku">致盲</em> </strong>。当我们将任一个X设为0时，我们可以测量Y是否等于非零X的MSE。理论上，MSE应该是0.0</p><figure class="lt lu lv lw gt lx"><div class="bz fp l di"><div class="ml mm l"/></div><p class="me mf gj gh gi mg mh bd b be z dk translated">按作者的度量</p></figure><p id="f026" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated"><strong class="jy ja">结果和结论</strong></p><p id="c797" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">我分别对X[0]和X[-1]应用了50%的变化，以了解Y中的变化，并计算第0个变化列和第-1个变化列。高达68个序列长度LSTM能够记住从时间序列开始的先前信息，但最终在第69个序列长度时忘记，而CNN仍然能够做出准确的预测，这仅仅是因为我增加了内核大小，以便它有机会查看序列的初始和结束值。盲法显示了相同的结果。当序列长度69的第0个值设置为0时，MSE跳到0.251%。总的来说，LSTM似乎对-1的变化比对0的变化更敏感。[ <em class="ku">这些结果会随着不同的数据大小和函数而变化，但信息是明确的“LSTMs并不真的长！”] </em></p><figure class="lt lu lv lw gt lx"><div class="bz fp l di"><div class="ml mm l"/></div><p class="me mf gj gh gi mg mh bd b be z dk translated">按作者列出的结果</p></figure><p id="c594" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">这些结果与pLSTM作者在他们的信息衰减部分描述的路径相同</p><figure class="lt lu lv lw gt lx gh gi paragraph-image"><div class="gh gi mn"><img src="../Images/aaec178176d5f053bbeba4961f6ce6cb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1316/format:webp/1*_xLWYkCG6U5wgTNGI7GflA.png"/></div><p class="me mf gj gh gi mg mh bd b be z dk translated">pLSTM的信息衰减<a class="ae mi" href="https://arxiv.org/pdf/2105.05944.pdf" rel="noopener ugc nofollow" target="_blank">作者</a></p></figure><p id="552c" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated"><em class="ku">在本文中，我没有与pLSTM进行比较，因为目前还没有可用的开源实现，不过，我将在下一篇博客中跟进我自己的pLSTM实现。</em></p><p id="145d" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">** <a class="ae mi" href="https://github.com/dwipam/medium-3" rel="noopener ugc nofollow" target="_blank"> <em class="ku">在这里</em> </a> <em class="ku">，你可以找到</em><em class="ku">链接</em> <em class="ku">的完整代码。</em></p></div></div>    
</body>
</html>