<html>
<head>
<title>Explaining Scikit-learn models with SHAP</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">解释 sci kit-与 SHAP 一起学习模型</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/explaining-scikit-learn-models-with-shap-61daff21b12a?source=collection_archive---------2-----------------------#2021-11-02">https://towardsdatascience.com/explaining-scikit-learn-models-with-shap-61daff21b12a?source=collection_archive---------2-----------------------#2021-11-02</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/1dde57815ca4574d8dc0ca69ae4ab7e9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*8bd-BYX528blFhTz"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">照片<a class="ae jg" href="https://unsplash.com/@marekpiwnicki?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">由 Marek Piwn </a> icki <a class="ae jg" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">在 Unsp </a> lash 上拍摄</p></figure><h2 id="5d00" class="jh ji jj bd b dl jk jl jm jn jo jp dk jq translated" aria-label="kicker paragraph">数据科学基础</h2><div class=""/><div class=""><h2 id="3c2e" class="pw-subtitle-paragraph kp js jj bd b kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg dk translated">走向可解释的人工智能</h2></div><p id="1bd9" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><em class="md">可解释的人工智能(XAI) </em>通过使机器学习模型更加透明，帮助建立对它们的信任和信心。<a class="ae jg" href="https://cloud.google.com/blog/products/ai-machine-learning/why-you-need-to-explain-machine-learning-models" rel="noopener ugc nofollow" target="_blank"> XAI 是一套工具和框架，可用于理解和解释机器学习模型如何做出决策。一个有用的 XAI 工具是 Python 中的 SHAP 库。此工具允许我们量化特征对单个预测以及整体预测的贡献。该库还带有美观易用的可视化功能。在这篇文章中，我们将学习 SHAP 库的基础知识，以理解来自<em class="md"> Scikit-learn </em>中内置的回归和分类模型的预测。</a></p><figure class="mf mg mh mi gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi me"><img src="../Images/e189fa5966c0e8a832302f2e716de6e4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*7SqYLrV19qcWtBOL"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">由<a class="ae jg" href="https://unsplash.com/@lobostudiohamburg?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">汉堡龙虾工作室</a>在<a class="ae jg" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>拍摄</p></figure></div><div class="ab cl mj mk hx ml" role="separator"><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo"/></div><div class="im in io ip iq"><h1 id="b99b" class="mq mr jj bd ms mt mu mv mw mx my mz na ky nb kz nc lb nd lc ne le nf lf ng nh bi translated">1.夏普价值观✨</h1><blockquote class="ni nj nk"><p id="7cee" class="lh li md lj b lk ll kt lm ln lo kw lp nl lr ls lt nm lv lw lx nn lz ma mb mc im bi translated">形状值帮助我们量化<em class="jj">特征对预测的贡献</em>。Shap 值越接近零表示该要素对预测的贡献越小，而 shap 值远离零表示该要素的贡献越大。</p></blockquote><p id="a8cd" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">让我们来学习如何为回归问题提取要素的 shap 值。我们将从加载库和样本数据开始，然后构建一个快速模型来预测糖尿病进展:</p><pre class="mf mg mh mi gt no np nq nr aw ns bi"><span id="5917" class="nt mr jj np b gy nu nv l nw nx">import numpy as np<br/>np.set_printoptions(formatter={'float':lambda x:"{:.4f}".format(x)})<br/>import pandas as pd<br/>pd.options.display.float_format = "{:.3f}".format</span><span id="3e20" class="nt mr jj np b gy ny nv l nw nx">import seaborn as sns<br/>import matplotlib.pyplot as plt<br/>sns.set(style='darkgrid', context='talk', palette='rainbow')</span><span id="93fd" class="nt mr jj np b gy ny nv l nw nx">from sklearn.datasets import load_diabetes<br/>from sklearn.model_selection import train_test_split<br/>from sklearn.ensemble import (RandomForestRegressor, <br/>                              RandomForestClassifier)</span><span id="ef53" class="nt mr jj np b gy ny nv l nw nx">import shap # v0.39.0<br/>shap.initjs()</span><span id="d04b" class="nt mr jj np b gy ny nv l nw nx"># Import sample data<br/>diabetes = load_diabetes(as_frame=True)<br/>X = diabetes['data'].iloc[:, :4] # Select first 4 columns<br/>y = diabetes['target']</span><span id="4c6b" class="nt mr jj np b gy ny nv l nw nx"># Partition data<br/>X_train, X_test, y_train, y_test = train_test_split(X, y, <br/>                                                    test_size=0.2, <br/>                                                    random_state=1)<br/>print(f"Training features shape: {X_train.shape}")<br/>print(f"Training target shape: {y_train.shape}\n")<br/>print(f"Test features shape: {X_test.shape}")<br/>print(f"Test target shape: {y_test.shape}")<br/>display(X_train.head())</span><span id="34f1" class="nt mr jj np b gy ny nv l nw nx"># Train a simple model<br/>model = RandomForestRegressor(random_state=42)<br/>model.fit(X_train, y_train)</span></pre><figure class="mf mg mh mi gt iv gh gi paragraph-image"><div class="gh gi nz"><img src="../Images/21f02ff5f6215f761a200fcc040601fd.png" data-original-src="https://miro.medium.com/v2/resize:fit:680/format:webp/1*nAybWM4ILCJsxE3ikkf1Kg.png"/></div></figure><p id="7a78" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">获取 shap 值的一个常见方法是使用<a class="ae jg" href="https://shap.readthedocs.io/en/latest/generated/shap.Explainer.html" rel="noopener ugc nofollow" target="_blank">解释器</a>对象。让我们创建一个<code class="fe oa ob oc np b">Explainer</code>对象，并为测试数据提取<code class="fe oa ob oc np b">shap_test</code>:</p><pre class="mf mg mh mi gt no np nq nr aw ns bi"><span id="8f5e" class="nt mr jj np b gy nu nv l nw nx">explainer = shap.Explainer(model)<br/>shap_test = explainer(X_test)<br/>print(f"Shap values length: {len(shap_test)}\n")<br/>print(f"Sample shap value:\n{shap_test[0]}")</span></pre><figure class="mf mg mh mi gt iv gh gi paragraph-image"><div class="gh gi od"><img src="../Images/3122394bafc64af7b7c6e53f2165e2fd.png" data-original-src="https://miro.medium.com/v2/resize:fit:862/format:webp/1*cgh4mDZ_LbvmjNIcOkY6mQ.png"/></div></figure><p id="cdba" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><code class="fe oa ob oc np b">shap_test</code>的长度是 89，因为它包含了每个测试实例的记录。从第一个测试记录中，我们可以看到有三个属性:<br/> ◼ <code class="fe oa ob oc np b">shap_test[0].base_values</code>:目标的基值<br/> ◼ <code class="fe oa ob oc np b">shap_test[0].data</code>:每个特性的值<br/> ◼ <code class="fe oa ob oc np b">shap_test[0].values</code>:每个特性的形状值</p><p id="2cb8" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">让我们了解一下这些属性中的每一个向我们展示了什么。</p><h2 id="81d5" class="nt mr jj bd ms oe of dn mw og oh dp na lq oi oj nc lu ok ol ne ly om on ng jp bi translated">📍1.1 基础值</h2><p id="accd" class="pw-post-body-paragraph lh li jj lj b lk oo kt lm ln op kw lp lq oq ls lt lu or lw lx ly os ma mb mc im bi translated"><em class="md">基值(</em> <code class="fe oa ob oc np b">shap_test.base_values</code> <em class="md"> ) </em>，也称为<em class="md">期望值(</em> <code class="fe oa ob oc np b">explainer.expected_value</code>)，是训练数据<em class="md">中的<em class="md">平均目标值</em>。</em>我们可以用下面的代码来检查这一点:</p><pre class="mf mg mh mi gt no np nq nr aw ns bi"><span id="12c7" class="nt mr jj np b gy nu nv l nw nx">print(f"Expected value: {explainer.expected_value[0]:.1f}")<br/>print(f"Average target value (training data): {y_train.mean():.1f}")<br/>print(f"Base value: {np.unique(shap_test.base_values)[0]:.1f}")</span></pre><figure class="mf mg mh mi gt iv gh gi paragraph-image"><div class="gh gi ot"><img src="../Images/b7828127dcdeffb3a7e985864e33ccaa.png" data-original-src="https://miro.medium.com/v2/resize:fit:846/format:webp/1*SH7qqV7cswuzzgE7hbdSXw.png"/></div></figure><p id="779e" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">因此，我们将互换使用词语<em class="md">期望值</em>和<em class="md">基础值</em>。</p><h2 id="b9b9" class="nt mr jj bd ms oe of dn mw og oh dp na lq oi oj nc lu ok ol ne ly om on ng jp bi translated">📦 1.2.数据</h2><p id="c782" class="pw-post-body-paragraph lh li jj lj b lk oo kt lm ln op kw lp lq oq ls lt lu or lw lx ly os ma mb mc im bi translated">接下来，<code class="fe oa ob oc np b">shap_test.data</code>包含与<code class="fe oa ob oc np b">X_test</code>相同的值:</p><pre class="mf mg mh mi gt no np nq nr aw ns bi"><span id="14e1" class="nt mr jj np b gy nu nv l nw nx">(shap_test.data == X_test).describe()</span></pre><figure class="mf mg mh mi gt iv gh gi paragraph-image"><div class="gh gi ou"><img src="../Images/0ed7940bf84f7704e3f3acd81699875f.png" data-original-src="https://miro.medium.com/v2/resize:fit:500/format:webp/1*V-zcxwymOoVhQpKESK-M0g.png"/></div></figure><p id="ba69" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">让我们把它转换成数据帧来看看:</p><pre class="mf mg mh mi gt no np nq nr aw ns bi"><span id="1713" class="nt mr jj np b gy nu nv l nw nx">pd.DataFrame(shap_test.data, columns=shap_test.feature_names, <br/>             index=X_test.index)</span></pre><figure class="mf mg mh mi gt iv gh gi paragraph-image"><div class="gh gi ov"><img src="../Images/920d87c87e544a3898aaa09c28b064a9.png" data-original-src="https://miro.medium.com/v2/resize:fit:550/format:webp/1*BoHeSJut3Z5JUYglhx6iNg.png"/></div></figure><p id="5b4e" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">这只是我们传递给它的数据集的副本。</p><h2 id="7a07" class="nt mr jj bd ms oe of dn mw og oh dp na lq oi oj nc lu ok ol ne ly om on ng jp bi translated">✨ 1.3.价值观念</h2><p id="fdd8" class="pw-post-body-paragraph lh li jj lj b lk oo kt lm ln op kw lp lq oq ls lt lu or lw lx ly os ma mb mc im bi translated">来自<code class="fe oa ob oc np b">shap_test</code>的最重要的属性是<code class="fe oa ob oc np b">values</code>属性。这是因为我们可以从中访问 shap 值。让我们将 shap 值转换成数据帧，以便于操作:</p><pre class="mf mg mh mi gt no np nq nr aw ns bi"><span id="1961" class="nt mr jj np b gy nu nv l nw nx">shap_df = pd.DataFrame(shap_test.values, <br/>                       columns=shap_test.feature_names, <br/>                       index=X_test.index)<br/>shap_df</span></pre><figure class="mf mg mh mi gt iv gh gi paragraph-image"><div class="gh gi ow"><img src="../Images/51ed3db46d236db256a8ef2c584707c4.png" data-original-src="https://miro.medium.com/v2/resize:fit:594/format:webp/1*6ndtP0C_rUeeE5TTjeVX5g.png"/></div></figure><p id="34a7" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">我们可以看到每个记录的 shap 值。如果我们把这些 shap 值加到期望值上，我们将得到预测:</p><figure class="mf mg mh mi gt iv gh gi paragraph-image"><div class="gh gi ox"><img src="../Images/4113f6b6c91b6f8a409abccada68057a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1050/format:webp/1*5BxjpcnNIIWkMCjgLzt1pg.png"/></div></figure><p id="7548" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">我们来看看是不是这样:</p><pre class="mf mg mh mi gt no np nq nr aw ns bi"><span id="ebf8" class="nt mr jj np b gy nu nv l nw nx">np.isclose(model.predict(X_test), <br/>           explainer.expected_value[0] + shap_df.sum(axis=1))</span></pre><figure class="mf mg mh mi gt iv gh gi paragraph-image"><div class="gh gi oy"><img src="../Images/2c991e19931e5b3703d64ddb998caf76.png" data-original-src="https://miro.medium.com/v2/resize:fit:1356/format:webp/1*SeCn-51fGRLp3nh5PKDfSA.png"/></div></figure><p id="ae8d" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">厉害！这里我们用<code class="fe oa ob oc np b">np.isclose()</code>忽略<a class="ae jg" href="https://stackoverflow.com/questions/2100490/floating-point-inaccuracy-examples" rel="noopener ugc nofollow" target="_blank">浮点不准确</a>。现在，我们有了 shap 值，我们可以像这样进行自定义可视化，以了解特性贡献:</p><pre class="mf mg mh mi gt no np nq nr aw ns bi"><span id="8393" class="nt mr jj np b gy nu nv l nw nx">columns = shap_df.apply(np.abs).mean()\<br/>                 .sort_values(ascending=False).index</span><span id="eb98" class="nt mr jj np b gy ny nv l nw nx">fig, ax = plt.subplots(1, 2, figsize=(11,4))<br/>sns.barplot(data=shap_df[columns].apply(np.abs), orient='h', <br/>            ax=ax[0])<br/>ax[0].set_title("Mean absolute shap value")<br/>sns.boxplot(data=shap_df[columns], orient='h', ax=ax[1])<br/>ax[1].set_title("Distribution of shap values");</span></pre><figure class="mf mg mh mi gt iv gh gi paragraph-image"><div class="gh gi oz"><img src="../Images/6fa209019bbd8e128a98b38ac95823d2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1318/format:webp/1*-0pGJ9tdnYQ0pMooXgPZ2w.png"/></div></figure><p id="8fd8" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">左侧子图显示了每个特征的平均绝对形状值，而右侧子图显示了形状值按特征的分布。从这些图表中可以看出,<code class="fe oa ob oc np b">bmi</code>在使用的 4 个特性中贡献最大。</p></div><div class="ab cl mj mk hx ml" role="separator"><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo"/></div><div class="im in io ip iq"><h1 id="e1d5" class="mq mr jj bd ms mt mu mv mw mx my mz na ky nb kz nc lb nd lc ne le nf lf ng nh bi translated">2.形状内置图📊</h1><p id="e5b4" class="pw-post-body-paragraph lh li jj lj b lk oo kt lm ln op kw lp lq oq ls lt lu or lw lx ly os ma mb mc im bi translated">虽然我们可以使用 shap 值构建我们自己的可视化，但是<code class="fe oa ob oc np b">shap </code>包带有内置的奇特的可视化。在这一节，我们将熟悉这些观想中的一些精选。我们将会看到两种主要的情节🌳<strong class="lj jt">全局:</strong>显示特征总体贡献的图形。这种图显示了一个特征对整个数据的总体贡献。<br/>🍀<strong class="lj jt">局部:</strong>显示特定实例中特征贡献的图。这有助于我们深入了解单个预测。</p><h2 id="008f" class="nt mr jj bd ms oe of dn mw og oh dp na lq oi oj nc lu ok ol ne ly om on ng jp bi translated">🌳 2.1.全局|条形图</h2><p id="13f8" class="pw-post-body-paragraph lh li jj lj b lk oo kt lm ln op kw lp lq oq ls lt lu or lw lx ly os ma mb mc im bi translated">对于前面显示的左侧子图，有一个等效的内置函数，只需几次击键:</p><pre class="mf mg mh mi gt no np nq nr aw ns bi"><span id="1e4d" class="nt mr jj np b gy nu nv l nw nx">shap.plots.bar(shap_test)</span></pre><figure class="mf mg mh mi gt iv gh gi paragraph-image"><div class="gh gi pa"><img src="../Images/dd3c87a75de991b3c268c25aecb5467e.png" data-original-src="https://miro.medium.com/v2/resize:fit:984/format:webp/1*RycQbrtJA9114qvIQh-9zw.png"/></div></figure><p id="a26f" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">这个简单但有用的图显示了特性贡献的强度。该图基于特征的平均绝对形状值:<code class="fe oa ob oc np b">shap_df.apply(np.abs).mean()</code>。要素从上到下排列，具有最高平均绝对形状值的要素显示在顶部。</p><h2 id="832b" class="nt mr jj bd ms oe of dn mw og oh dp na lq oi oj nc lu ok ol ne ly om on ng jp bi translated">🌳 2.2.全局|汇总图</h2><p id="4859" class="pw-post-body-paragraph lh li jj lj b lk oo kt lm ln op kw lp lq oq ls lt lu or lw lx ly os ma mb mc im bi translated">另一个有用的情节是摘要情节:</p><pre class="mf mg mh mi gt no np nq nr aw ns bi"><span id="e14a" class="nt mr jj np b gy nu nv l nw nx">shap.summary_plot(shap_test)</span></pre><figure class="mf mg mh mi gt iv gh gi paragraph-image"><div class="gh gi pb"><img src="../Images/05fac7effd1e15aa7a530c0e94d1d294.png" data-original-src="https://miro.medium.com/v2/resize:fit:954/format:webp/1*C6ubQH7ufzP17UdNHZ7qSg.png"/></div></figure><p id="262d" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">这里有一个替代语法:<code class="fe oa ob oc np b">shap.plots.beeswarm(shap_test)</code>用于这个具体的情节。与之前一样，要素按其平均绝对形状值排序。与之前的条形图相比，此图表更加复杂，包含更多信息。下面是解释该图的指南:<br/> ◼️ <strong class="lj jt">图的横轴</strong>显示了特征的 shap 值分布。每个点代表数据集中的一条记录。例如，我们可以看到对于<code class="fe oa ob oc np b">bmi</code>，点非常分散，在 0 附近几乎没有任何点，而对于<code class="fe oa ob oc np b">age</code>，点更靠近 0 聚集。<br/> ◼️ <strong class="lj jt">点的颜色</strong>显示特征值。这个额外的维度允许我们看到 shap 值如何随着特征值的改变而改变。换句话说，我们可以看到关系的方向。例如，我们可以看到，当<code class="fe oa ob oc np b">bmi</code>为高时，shap 值往往较高(由粉红色圆点表示)，当<code class="fe oa ob oc np b">bmi</code>为低时，shap 值往往较低(由蓝色圆点表示)。还有一些紫色的点分散在光谱中。</p><p id="f75e" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">如果我们发现默认颜色图不直观或不合适，我们可以将其更改为我们首选的<a class="ae jg" href="https://matplotlib.org/stable/tutorials/colors/colormaps.html" rel="noopener ugc nofollow" target="_blank"> matplotlib 颜色图</a>，如下所示:</p><pre class="mf mg mh mi gt no np nq nr aw ns bi"><span id="96ad" class="nt mr jj np b gy nu nv l nw nx">shap.summary_plot(shap_test, cmap=plt.get_cmap("winter_r"))</span></pre><figure class="mf mg mh mi gt iv gh gi paragraph-image"><div class="gh gi pc"><img src="../Images/8ec1fbe4158cbaf9c1259065f8008c0d.png" data-original-src="https://miro.medium.com/v2/resize:fit:972/format:webp/1*x9mgwmAYbUkULSKNIY6fnQ.png"/></div></figure><p id="cc22" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">此外，我们还可以使用不同的绘图类型。这里有一个例子:</p><pre class="mf mg mh mi gt no np nq nr aw ns bi"><span id="aeb4" class="nt mr jj np b gy nu nv l nw nx">shap.summary_plot(shap_test, plot_type='violin')</span></pre><figure class="mf mg mh mi gt iv gh gi paragraph-image"><div class="gh gi pb"><img src="../Images/263dcdb48d3cc85d4db9a692eedb87e9.png" data-original-src="https://miro.medium.com/v2/resize:fit:954/format:webp/1*sOWvleFfebLoXjqIWe1Y7w.png"/></div></figure><h2 id="f84a" class="nt mr jj bd ms oe of dn mw og oh dp na lq oi oj nc lu ok ol ne ly om on ng jp bi translated">🌳 2.3.全球|热图</h2><p id="f687" class="pw-post-body-paragraph lh li jj lj b lk oo kt lm ln op kw lp lq oq ls lt lu or lw lx ly os ma mb mc im bi translated">热图是可视化 shap 值的另一种方式。我们看到的是彩色编码的单个值，而不是将形状值聚合成一个平均值。特征标绘在 y 轴上，记录标绘在 x 轴上:</p><pre class="mf mg mh mi gt no np nq nr aw ns bi"><span id="2475" class="nt mr jj np b gy nu nv l nw nx">shap.plots.heatmap(shap_test)</span></pre><figure class="mf mg mh mi gt iv gh gi paragraph-image"><div class="gh gi pd"><img src="../Images/00f219c6a96b357b0c6e9f9cd7dc2939.png" data-original-src="https://miro.medium.com/v2/resize:fit:686/format:webp/1*c_5V2c3qQiG1ccUYVRe7ow.png"/></div></figure><p id="3d48" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">该热图由顶部每个记录的预测值(即<code class="fe oa ob oc np b">f(x)</code>)的线图补充。</p><p id="8e0c" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">我们可以使用<code class="fe oa ob oc np b">cmap</code>参数将颜色图更改为我们想要的颜色图:</p><pre class="mf mg mh mi gt no np nq nr aw ns bi"><span id="a0a8" class="nt mr jj np b gy nu nv l nw nx">shap.plots.heatmap(shap_test, cmap=plt.get_cmap("winter_r"))</span></pre><figure class="mf mg mh mi gt iv gh gi paragraph-image"><div class="gh gi pe"><img src="../Images/e85fc04693a2356e503a65f997cef643.png" data-original-src="https://miro.medium.com/v2/resize:fit:698/format:webp/1*pjZXmJvS-xUnTLkBmjWwxA.png"/></div></figure><h2 id="4e60" class="nt mr jj bd ms oe of dn mw og oh dp na lq oi oj nc lu ok ol ne ly om on ng jp bi translated">🌳 2.4.全局|力图</h2><p id="4f02" class="pw-post-body-paragraph lh li jj lj b lk oo kt lm ln op kw lp lq oq ls lt lu or lw lx ly os ma mb mc im bi translated">这个交互图让我们可以看到由记录构成的 shap 值:</p><pre class="mf mg mh mi gt no np nq nr aw ns bi"><span id="1585" class="nt mr jj np b gy nu nv l nw nx">shap.force_plot(explainer.expected_value, shap_test.values, <br/>                X_test)</span></pre><figure class="mf mg mh mi gt iv gh gi paragraph-image"><div class="gh gi pf"><img src="../Images/2229c2b5792a13712a843880d5546187.png" data-original-src="https://miro.medium.com/v2/resize:fit:1336/format:webp/1*ciMuXASd89oEYyycT8JdQw.png"/></div></figure><p id="3cd5" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">就像热图一样，x 轴显示每条记录。正的形状值显示为红色，负的形状值显示为蓝色。例如，由于第一条记录的红色成分比蓝色成分多，因此这条记录的预测值将高于预期值。</p><p id="3be6" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">交互性允许我们改变两个轴。例如，y 轴显示预测，<code class="fe oa ob oc np b">f(x)</code>，x 轴按上面快照中的输出(预测)值排序。</p><h2 id="7e68" class="nt mr jj bd ms oe of dn mw og oh dp na lq oi oj nc lu ok ol ne ly om on ng jp bi translated">🍀 2.5.局部|条形图</h2><p id="d4a1" class="pw-post-body-paragraph lh li jj lj b lk oo kt lm ln op kw lp lq oq ls lt lu or lw lx ly os ma mb mc im bi translated">现在，我们将通过图表来理解个别情况下的预测。让我们从柱状图开始:</p><pre class="mf mg mh mi gt no np nq nr aw ns bi"><span id="d9c2" class="nt mr jj np b gy nu nv l nw nx">shap.plots.bar(shap_test[0])</span></pre><figure class="mf mg mh mi gt iv gh gi paragraph-image"><div class="gh gi pg"><img src="../Images/6b205e52197edf2b153c89107986557a.png" data-original-src="https://miro.medium.com/v2/resize:fit:978/format:webp/1*jLibMUFMj1gcXMV4PK50Gw.png"/></div></figure><p id="5e91" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">语法与<em class="md">第 2.1 节中的完全相同。Global | Bar plot' </em>除了这次我们对单个记录的数据进行切片。</p><h2 id="da29" class="nt mr jj bd ms oe of dn mw og oh dp na lq oi oj nc lu ok ol ne ly om on ng jp bi translated">🍀 2.6.局部|瀑布图</h2><p id="0a77" class="pw-post-body-paragraph lh li jj lj b lk oo kt lm ln op kw lp lq oq ls lt lu or lw lx ly os ma mb mc im bi translated">这是条形图的另一种替代形式:</p><pre class="mf mg mh mi gt no np nq nr aw ns bi"><span id="4967" class="nt mr jj np b gy nu nv l nw nx">class WaterfallData():<br/>    def __init__ (self, shap_test, index):<br/>        self.values = shap_test[index].values<br/>        self.base_values = shap_test[index].base_values[0]<br/>        self.data = shap_test[index].data<br/>        self.feature_names = shap_test.feature_names</span><span id="c5e8" class="nt mr jj np b gy ny nv l nw nx">shap.plots.waterfall(WaterfallData(shap_test, 0))</span></pre><figure class="mf mg mh mi gt iv gh gi paragraph-image"><div class="gh gi ph"><img src="../Images/3113e010fbf06473bad55276f36fdea6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1244/format:webp/1*2DaTkx_B4KcXu-97U5Fusw.png"/></div></figure><p id="164e" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">瀑布图是信息密集的，有四位信息:<br/> ◼️在 y 轴上，我们看到记录的实际特征值。如果您不确定我的意思，请将<code class="fe oa ob oc np b">X_test.head(1)</code>与 y 轴上的值进行比较。<br/> ◼️在图表的右下角，我们看到<code class="fe oa ob oc np b">E[f(X)]</code>，预期值。<br/> ◼️在左上角，我们看到<code class="fe oa ob oc np b">f(x)</code>，预测值。<br/> ◼️ ️Just 像前面的柱状图一样，横条代表颜色编码的特征贡献。从底部的期望值开始，我们可以看到每个贡献是如何上下移动预测值以最终达到预测值的。</p><h2 id="8dc3" class="nt mr jj bd ms oe of dn mw og oh dp na lq oi oj nc lu ok ol ne ly om on ng jp bi translated">🍀 2.7.局部|力图</h2><p id="d8e2" class="pw-post-body-paragraph lh li jj lj b lk oo kt lm ln op kw lp lq oq ls lt lu or lw lx ly os ma mb mc im bi translated">最后一个要熟悉的图是单个记录的力图。如果我们将该图旋转 90 度并绘制多个记录，我们将看到全局力图。</p><pre class="mf mg mh mi gt no np nq nr aw ns bi"><span id="8f5b" class="nt mr jj np b gy nu nv l nw nx">shap.plots.force(shap_test[0])</span></pre><figure class="mf mg mh mi gt iv gh gi paragraph-image"><div class="gh gi pi"><img src="../Images/4dedbdd1fcbc26d646b3ba892517963f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1138/format:webp/1*uJY7cGMJWzqKaZzq5EdYCA.png"/></div></figure><p id="6af8" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">我们既可以看到基础值:153.4，也可以看到预测值:103.48。我们还可以看到特性贡献的分解。</p></div><div class="ab cl mj mk hx ml" role="separator"><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo"/></div><div class="im in io ip iq"><h1 id="e62f" class="mq mr jj bd ms mt mu mv mw mx my mz na ky nb kz nc lb nd lc ne le nf lf ng nh bi translated">3.调整分类🌓</h1><p id="d2ea" class="pw-post-body-paragraph lh li jj lj b lk oo kt lm ln op kw lp lq oq ls lt lu or lw lx ly os ma mb mc im bi translated">到目前为止，我们已经关注了一个回归示例。在这一节中，我们将学习一种方法来调整我们所学的内容以适应二元分类。让我们导入 titanic 数据的子集并训练一个简单的模型:</p><pre class="mf mg mh mi gt no np nq nr aw ns bi"><span id="f2f1" class="nt mr jj np b gy nu nv l nw nx"># Import sample data<br/>df =  sns.load_dataset('titanic')<br/>df['is_male'] = df['sex'].map({'male': 1, 'female': 0}) # Encode<br/># Keep numerical complete columns<br/>df = df.select_dtypes('number').dropna() <br/>X = df.drop(columns=['survived'])<br/>y = df['survived']</span><span id="c4cc" class="nt mr jj np b gy ny nv l nw nx"># Partition data<br/>X_train, X_test, y_train, y_test = train_test_split(X, y, <br/>                                                    test_size=0.2, <br/>                                                    random_state=1)</span><span id="ce0f" class="nt mr jj np b gy ny nv l nw nx">print(f"Training features shape: {X_train.shape}")<br/>print(f"Training target shape: {y_train.shape}\n")<br/>print(f"Test features shape: {X_test.shape}")<br/>print(f"Test target shape: {y_test.shape}")<br/>display(X_train.head())</span><span id="86f7" class="nt mr jj np b gy ny nv l nw nx"># Train a simple model<br/>model = RandomForestClassifier(random_state=42)<br/>model.fit(X_train, y_train)</span></pre><figure class="mf mg mh mi gt iv gh gi paragraph-image"><div class="gh gi pj"><img src="../Images/bc1a2c94e977a85c1c7d51e91867670c.png" data-original-src="https://miro.medium.com/v2/resize:fit:836/format:webp/1*G6dvCcDIScsDhBc-S_vmUQ.png"/></div></figure><h2 id="a611" class="nt mr jj bd ms oe of dn mw og oh dp na lq oi oj nc lu ok ol ne ly om on ng jp bi translated">✨ 3.1.形状值</h2><p id="1bca" class="pw-post-body-paragraph lh li jj lj b lk oo kt lm ln op kw lp lq oq ls lt lu or lw lx ly os ma mb mc im bi translated">我们将创建一个解释器对象，并像前面一样提取 shap 值:</p><pre class="mf mg mh mi gt no np nq nr aw ns bi"><span id="856f" class="nt mr jj np b gy nu nv l nw nx">explainer = shap.Explainer(model)<br/>shap_test = explainer(X_test)<br/>print(f"Length of shap_test: {len(shap_test)}\n")<br/>print(f"Sample shap_test:\n{shap_test[0]}")</span></pre><figure class="mf mg mh mi gt iv gh gi paragraph-image"><div class="gh gi pi"><img src="../Images/edfb2dbb734f38fcb0dde4298466c563.png" data-original-src="https://miro.medium.com/v2/resize:fit:1138/format:webp/1*99YKLa-HVvk2KFQWEO-sgg.png"/></div></figure><p id="4012" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">就像回归<code class="fe oa ob oc np b">shap_test.data</code>将包含与<code class="fe oa ob oc np b">X_test</code>相同数量的记录。但是，从这个示例记录中，我们可以看到<code class="fe oa ob oc np b">values </code>和<code class="fe oa ob oc np b">base_values</code>的尺寸不同。</p><p id="52b4" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">我们先来看看基值。基值现在告诉我们每个类的概率。我们将关注积极类(即<code class="fe oa ob oc np b">y==1</code>，生存):</p><pre class="mf mg mh mi gt no np nq nr aw ns bi"><span id="af10" class="nt mr jj np b gy nu nv l nw nx">print(f"Expected value: {explainer.expected_value[1]:.2f}")<br/>print(f"Average target value (training data): {y_train.mean():.2f}")<br/>print(f"Base value: {np.unique(shap_test.base_values)[0]:.2f}")</span></pre><figure class="mf mg mh mi gt iv gh gi paragraph-image"><div class="gh gi pk"><img src="../Images/8ee01cf1e49988a86a04db2d0dde31a0.png" data-original-src="https://miro.medium.com/v2/resize:fit:830/format:webp/1*ERyP5NaPXn9bqvLQG4XYbA.png"/></div></figure><p id="82f9" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">现在，让我们看看 shap 值。还为这两个类提供了 Shap 值。你也会注意到它们是彼此的反面。我们可以这样提取正类的 shap 值:</p><pre class="mf mg mh mi gt no np nq nr aw ns bi"><span id="dc45" class="nt mr jj np b gy nu nv l nw nx">shap_df = pd.DataFrame(shap_test.values[:,:,1], <br/>                       columns=shap_test.feature_names, <br/>                       index=X_test.index)<br/>shap_df</span></pre><figure class="mf mg mh mi gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi pl"><img src="../Images/f42c245a460c769391953a3a566356dd.png" data-original-src="https://miro.medium.com/v2/resize:fit:814/format:webp/1*VQTkkNn0S3zWm5YRgWPwIw.png"/></div></div></figure><p id="6d84" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">让我们仔细检查一下将 shap 值的总和加到预期概率上是否会得到预测概率:</p><pre class="mf mg mh mi gt no np nq nr aw ns bi"><span id="df80" class="nt mr jj np b gy nu nv l nw nx">np.isclose(model.predict_proba(X_test)[:,1], <br/>           explainer.expected_value[1] + shap_df.sum(axis=1))</span></pre><figure class="mf mg mh mi gt iv gh gi paragraph-image"><div class="gh gi pm"><img src="../Images/34779d38569374fd21a03bd233173f49.png" data-original-src="https://miro.medium.com/v2/resize:fit:1346/format:webp/1*Fi8FADBzWeKnqwgnD8QC7g.png"/></div></figure><p id="d415" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">太棒了，现在你知道如何得到概率单位的 shap 值了。</p><h2 id="c1e0" class="nt mr jj bd ms oe of dn mw og oh dp na lq oi oj nc lu ok ol ne ly om on ng jp bi translated">📊 3.2.形状内置图</h2><p id="16f7" class="pw-post-body-paragraph lh li jj lj b lk oo kt lm ln op kw lp lq oq ls lt lu or lw lx ly os ma mb mc im bi translated">是时候调整我们之前看到的情节了。为了避免重复，我将展示一个全局图的示例和一个局部图的示例，因为其他图可以使用相同的逻辑来复制。</p><p id="89ba" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jt">全局|条形图:</strong>让我们检查特征对预测正类的总体贡献:</p><pre class="mf mg mh mi gt no np nq nr aw ns bi"><span id="baca" class="nt mr jj np b gy nu nv l nw nx">shap.plots.bar(shap_test[:,:,1])</span></pre><figure class="mf mg mh mi gt iv gh gi paragraph-image"><div class="gh gi pn"><img src="../Images/bb2fc7119b1bdbe2ccf944da46880e5c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1032/format:webp/1*3QC7Vd5tjpNvF4nM8sn0lQ.png"/></div></figure><p id="cf5e" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jt">局部|瀑布图:</strong>我们来看看第一个测试记录的瀑布图:</p><pre class="mf mg mh mi gt no np nq nr aw ns bi"><span id="04ca" class="nt mr jj np b gy nu nv l nw nx">shap.plots.waterfall(shap_test[:,:,1][0])</span></pre><figure class="mf mg mh mi gt iv gh gi paragraph-image"><div class="gh gi po"><img src="../Images/b7fc8e1b758a2e5c1cadfbc820f70fbc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1108/format:webp/1*Qnesupowgj541YTXrvJmPw.png"/></div></figure><p id="df5f" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">从这个图中，我们可以看到每个特征是如何对属于正类的预测概率做出贡献的:对于这个记录是 0.98。</p><h2 id="2587" class="nt mr jj bd ms oe of dn mw og oh dp na lq oi oj nc lu ok ol ne ly om on ng jp bi translated">📍 3.3.使用案例</h2><p id="8f44" class="pw-post-body-paragraph lh li jj lj b lk oo kt lm ln op kw lp lq oq ls lt lu or lw lx ly os ma mb mc im bi translated">在我们结束之前，让我们看一个示例用例。我们将为幸存者找到最不正确的例子，并试图理解为什么模型做出不正确的预测:</p><pre class="mf mg mh mi gt no np nq nr aw ns bi"><span id="1924" class="nt mr jj np b gy nu nv l nw nx">test = pd.concat([X_test, y_test], axis=1)<br/>test['probability'] = model.predict_proba(X_test)[:,1]<br/>test['order'] = np.arange(len(test))<br/>test.query('survived==1').nsmallest(5, 'probability')</span></pre><figure class="mf mg mh mi gt iv gh gi paragraph-image"><div class="gh gi pp"><img src="../Images/e2d8370644179dc7cc22fbe27d258afb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1254/format:webp/1*xa7hohitxZ9idHcY60BHkQ.png"/></div></figure><p id="5d51" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">第一次记录的存活概率是 0.03。让我们看看特性是如何促成这一预测的:</p><pre class="mf mg mh mi gt no np nq nr aw ns bi"><span id="387c" class="nt mr jj np b gy nu nv l nw nx">ind1 = test.query('survived==1')\<br/>           .nsmallest(1, 'probability')['order'].values[0]<br/>shap.plots.waterfall(shap_test[:,:,1][ind1])</span></pre><figure class="mf mg mh mi gt iv gh gi paragraph-image"><div class="gh gi pq"><img src="../Images/36c8c4b335505eed07eb7100cdc2a36d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1208/format:webp/1*6TNBZ1cbOkK5VskLuioixQ.png"/></div></figure><p id="df43" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">年轻的三等舱男性乘客……我们可以看到，性别、乘客级别和年龄已经大大降低了预测。让我们在训练数据中找到类似的例子:</p><pre class="mf mg mh mi gt no np nq nr aw ns bi"><span id="fe65" class="nt mr jj np b gy nu nv l nw nx">pd.concat([X_train, y_train], <br/>          axis=1)[(X_train['is_male']==1) &amp; <br/>                  (X_train['pclass']==3) &amp; <br/>                  (X_train['age']==22) &amp; <br/>                  (X_train['fare'].between(7,8))]</span></pre><figure class="mf mg mh mi gt iv gh gi paragraph-image"><div class="gh gi pr"><img src="../Images/af92e97bc0dc4a9ff4decac6792b6f33.png" data-original-src="https://miro.medium.com/v2/resize:fit:950/format:webp/1*1YGjdOvfy1na2F3DKPLDfw.png"/></div></figure><p id="beea" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">所有类似的训练例子其实都没有存活下来。这就说得通了。这是一个小用例，说明 shap 如何有助于解释为什么模型会得出不正确的预测。</p></div><div class="ab cl mj mk hx ml" role="separator"><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo"/></div><div class="im in io ip iq"><figure class="mf mg mh mi gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ps"><img src="../Images/c6704224f337036623e2d8d1b66f837e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*HsBIRXgcv-lkIBpZ"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated"><a class="ae jg" href="https://unsplash.com/@albedo?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">云</a>在<a class="ae jg" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍照</p></figure><p id="bb61" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">希望您喜欢学习如何使用 SHAP 库来理解 Scikit-learn 中内置的回归和分类模型。因为 SHAP 是模型不可知的，所以你也可以把它用于其他模型。我希望你能很快使用这个可解释的人工智能工具来理解为什么模型会做出预测，以便改进模型并更好地向他人解释它。</p><p id="0f80" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><em class="md">您想访问更多这样的内容吗？媒体会员可以无限制地访问媒体上的任何文章。如果你使用</em> <a class="ae jg" href="https://zluvsand.medium.com/membership" rel="noopener"> <em class="md">我的推荐链接</em> </a>，<em class="md">成为会员，你的一部分会费会直接去支持我。</em></p></div><div class="ab cl mj mk hx ml" role="separator"><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo"/></div><div class="im in io ip iq"><p id="31aa" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">感谢您阅读这篇文章。如果你感兴趣，这里有我其他一些帖子的链接:<br/> ◼️️ <a class="ae jg" rel="noopener" target="_blank" href="/k-nearest-neighbours-explained-52c910c035c5"> K 近邻解释</a> <br/> ◼️️ <a class="ae jg" rel="noopener" target="_blank" href="/logistic-regression-explained-7695f15d1b8b">逻辑回归解释</a> <br/> ◼️️ <a class="ae jg" rel="noopener" target="_blank" href="/comparing-random-forest-and-gradient-boosting-d7236b429c15">比较随机森林和梯度推进</a> <br/> ◼️️ <a class="ae jg" rel="noopener" target="_blank" href="/how-are-decision-trees-built-a8e5af57ce8?source=your_stories_page-------------------------------------">决策树是如何构建的？</a> <br/> ◼️️ <a class="ae jg" rel="noopener" target="_blank" href="/pipeline-columntransformer-and-featureunion-explained-f5491f815f?source=your_stories_page-------------------------------------">管道、ColumnTransformer 和 FeatureUnion 说明</a> <br/> ◼️️ <a class="ae jg" rel="noopener" target="_blank" href="/featureunion-columntransformer-pipeline-for-preprocessing-text-data-9dcb233dbcb6"> FeatureUnion、ColumnTransformer &amp;管道用于预处理文本数据</a></p><p id="34eb" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">再见🏃 💨</p></div></div>    
</body>
</html>