<html>
<head>
<title>Data Manipulation Pandas-PySpark Conversion Guide</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">数据操作熊猫-PySpark转换指南</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/data-manipulation-pandas-pyspark-conversion-guide-abed50a818?source=collection_archive---------32-----------------------#2021-08-04">https://towardsdatascience.com/data-manipulation-pandas-pyspark-conversion-guide-abed50a818?source=collection_archive---------32-----------------------#2021-08-04</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="5bc9" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">使用PySpark on Databricks进行探索性数据分析(EDA)所需的一切</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/ca705709bd91a9310cedf0cd600e9c58.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gurxnYeAPF-3-rp-ZxPTaQ.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">萨米·米提特鲁多在<a class="ae ky" href="https://unsplash.com/s/photos/red-and-white-bricks?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="6c71" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在处理大数据时，Jupyter notebook会失败。这也是一些公司使用ML平台如Databricks，SageMaker，Alteryx等的一个原因。一个好的ML平台支持从数据摄取到建模和监控的整个机器学习生命周期，从而提高团队的生产力和效率。在这个简单的教程中，我将分享我关于将Pandas脚本转换为Pyspark的笔记，这样您也可以无缝地转换这两种语言！</p><h1 id="81e1" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">介绍</h1><h2 id="47a8" class="mn lw it bd lx mo mp dn mb mq mr dp mf li ms mt mh lm mu mv mj lq mw mx ml my bi translated"><strong class="ak">什么是火花？</strong></h2><p id="4a23" class="pw-post-body-paragraph kz la it lb b lc mz ju le lf na jx lh li nb lk ll lm nc lo lp lq nd ls lt lu im bi translated">Spark是一个开源的云计算框架。这是一个可扩展的、大规模并行的内存执行环境，用于运行分析应用。Spark是处理Hadoop数据的快速而强大的引擎。它通过Hadoop YARN或Spark的独立模式运行在Hadoop集群中，它可以处理HDFS、HBase、Cassandra、Hive和任何Hadoop InputFormat中的数据。它旨在执行一般的数据处理(类似于MapReduce)和新的工作负载，如流、交互式查询和机器学习。你可以在这里阅读更多关于Mapreduce的内容:<a class="ae ky" href="https://medium.com/@francescomandru/mapreduce-explained-45a858c5ac1d" rel="noopener">https://medium . com/@ francescomandru/Mapreduce-explained-45a 858 C5 ac1d</a>(P . S我最喜欢的解释之一！)</p><h2 id="e35a" class="mn lw it bd lx mo mp dn mb mq mr dp mf li ms mt mh lm mu mv mj lq mw mx ml my bi translated"><strong class="ak">什么是数据块？</strong></h2><p id="060e" class="pw-post-body-paragraph kz la it lb b lc mz ju le lf na jx lh li nb lk ll lm nc lo lp lq nd ls lt lu im bi translated">Databricks为您的所有数据提供了一个统一的开放平台。它为数据科学家、数据工程师和数据分析师提供了一个简单的协作环境。就数据服务而言，它是市场领导者之一。它由Ali Gozii于2013年创建，他是阿帕奇火花三角洲湖和MLflow的原始创造者之一。它来自一些世界上最受欢迎的开源项目的原始创建者，Apache Spark、Delta Lake、MLflow和Koalas。它建立在这些技术之上，提供了一个真正的湖屋架构，结合了最好的数据湖和数据仓库，形成了一个快速、可扩展和可靠的平台。专为云构建，您的数据存储在低成本的云对象存储中，如AWS s3和Azure data lake storage，通过缓存、优化的数据布局和其他技术实现性能访问。您可以启动包含数百台机器的集群，每台机器都混合了您的分析所需的CPU和GPU。</p><h1 id="ed43" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">主要概念</h1><p id="c6ab" class="pw-post-body-paragraph kz la it lb b lc mz ju le lf na jx lh li nb lk ll lm nc lo lp lq nd ls lt lu im bi translated"><strong class="lb iu">探索数据- </strong>下表总结了用于获得数据概览的主要函数。</p><pre class="kj kk kl km gt ne nf ng nh aw ni bi"><span id="f87b" class="mn lw it nf b gy nj nk l nl nm">              Pandas      |            PySpark               <br/> -------------------------|------------------------------------ <br/>  pd.read_csv(path)       | spark.read.csv(path)<br/>  df.shape                | print(df.count(), len(df.columns)) <br/>  df.head(10)             | df.limit(10).toPandas()            <br/>  df[col].isnull().sum()  | df.where(df.col.isNull()).count()<br/>  df                      | Display(df)</span></pre><h1 id="bcc0" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">数据预处理</h1><ul class=""><li id="58b8" class="nn no it lb b lc mz lf na li np lm nq lq nr lu ns nt nu nv bi translated"><strong class="lb iu">删除重复项- </strong>删除所选多列中的重复行</li><li id="882b" class="nn no it lb b lc nw lf nx li ny lm nz lq oa lu ns nt nu nv bi translated"><strong class="lb iu">过滤- </strong>我们可以根据一些条件过滤行</li><li id="459c" class="nn no it lb b lc nw lf nx li ny lm nz lq oa lu ns nt nu nv bi translated"><strong class="lb iu">更改列- </strong>更改列名，转换数据类型，创建新列</li><li id="6529" class="nn no it lb b lc nw lf nx li ny lm nz lq oa lu ns nt nu nv bi translated"><strong class="lb iu">条件列-</strong><strong class="lb iu"/>列可以使用下面的R命令针对一组特定的条件取不同的值</li><li id="4e3d" class="nn no it lb b lc nw lf nx li ny lm nz lq oa lu ns nt nu nv bi translated"><strong class="lb iu">排序- </strong>对数据进行排序</li><li id="6ce9" class="nn no it lb b lc nw lf nx li ny lm nz lq oa lu ns nt nu nv bi translated"><strong class="lb iu">日期时间转换- </strong>包含日期时间值的字段从字符串转换为日期时间</li><li id="c1bc" class="nn no it lb b lc nw lf nx li ny lm nz lq oa lu ns nt nu nv bi translated"><strong class="lb iu"> Groupby - </strong>数据帧可以根据给定的列进行聚合</li><li id="3888" class="nn no it lb b lc nw lf nx li ny lm nz lq oa lu ns nt nu nv bi translated"><strong class="lb iu"> Join() </strong> -将列转换为逗号分隔的列表</li><li id="fb0f" class="nn no it lb b lc nw lf nx li ny lm nz lq oa lu ns nt nu nv bi translated"><strong class="lb iu">填充nan值- </strong>用值替换列中缺失的空值</li></ul><pre class="kj kk kl km gt ne nf ng nh aw ni bi"><span id="4646" class="mn lw it nf b gy nj nk l nl nm">             Pandas       |              PySpark               <br/> -------------------------|------------------------------------         <br/>  df.drop_duplicates()    | df.dropDuplicates()<br/>  df.drop(xx,axis=1)      | df.drop(xx)<br/>  df[['col1','col2']]     | df.select('col1','col2')<br/>  df[df.isin(xxx)]        | df.filter(df.xx.isin()).show()<br/>  df.rename(columns={})   | df.withColumnRenamed('col1','col2')<br/>  df.x.astype(str)        |df.withColumn(x,col(x).cast(StringType())<br/>  df.sort_values()        | df.sort() or df.orderby()<br/>  np.datetime64('today')  | current_date()<br/>  pd.to_datetime()        | to_date(column, time_format)<br/>  df.groupby()            | df.groupBy()<br/>  ','.join()         |','.join(list(df.select[col].toPandas()[col]))<br/>  df.fillna(0)            | df.x.fill(value=0)</span></pre><h1 id="d045" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">数据帧转换</h1><ul class=""><li id="198d" class="nn no it lb b lc mz lf na li np lm nq lq nr lu ns nt nu nv bi translated"><strong class="lb iu">合并数据帧- </strong>我们可以根据给定的字段合并两个数据帧</li><li id="1eec" class="nn no it lb b lc nw lf nx li ny lm nz lq oa lu ns nt nu nv bi translated"><strong class="lb iu">连接数据帧- </strong>将两个或多个数据帧连接成一个数据帧</li></ul><pre class="kj kk kl km gt ne nf ng nh aw ni bi"><span id="3a72" class="mn lw it nf b gy nj nk l nl nm">            Pandas        |             PySpark               <br/> -------------------------|------------------------------------ <br/>  df1.join(df2, on=,how=) | df1.join(df2,df1.id=df2.id, how='')<br/>  pd.concat([df1, df2])   | df1.unionAll(df1,df2)</span></pre><h1 id="3817" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">结论</h1><p id="b454" class="pw-post-body-paragraph kz la it lb b lc mz ju le lf na jx lh li nb lk ll lm nc lo lp lq nd ls lt lu im bi translated">希望这个简单的对比备忘单可以帮助你更快的上手PySpark和Databricks。以上对比只是起点！更多细节和例子，请查看这个<a class="ae ky" href="https://sparkbyexamples.com/" rel="noopener ugc nofollow" target="_blank"> Pyspark初学者教程页面</a>。当谈到学习任何新的语言或工具时，最好的学习方法就是实践。我强烈建议你自己仔细阅读pyspark代码，甚至可以在<a class="ae ky" href="https://databricks.com/" rel="noopener ugc nofollow" target="_blank"> Databricks </a>上开始一个项目！😉</p></div><div class="ab cl ob oc hx od" role="separator"><span class="oe bw bk of og oh"/><span class="oe bw bk of og oh"/><span class="oe bw bk of og"/></div><div class="im in io ip iq"><p id="3c88" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果你觉得这很有帮助，请关注我，看看我的其他博客。敬请关注更多内容！❤</p><div class="oi oj gp gr ok ol"><a rel="noopener follow" target="_blank" href="/10-tips-to-land-your-first-data-science-job-as-a-new-grad-87ecc06c17f7"><div class="om ab fo"><div class="on ab oo cl cj op"><h2 class="bd iu gy z fp oq fr fs or fu fw is bi translated">作为新毕业生获得第一份数据科学工作的10个技巧</h2><div class="os l"><h3 class="bd b gy z fp oq fr fs or fu fw dk translated">从我的求职之旅中学到的经验</h3></div><div class="ot l"><p class="bd b dl z fp oq fr fs or fu fw dk translated">towardsdatascience.com</p></div></div><div class="ou l"><div class="ov l ow ox oy ou oz ks ol"/></div></div></a></div><div class="oi oj gp gr ok ol"><a rel="noopener follow" target="_blank" href="/how-to-prepare-for-business-case-interview-as-an-analyst-6e9d68ce2fd8"><div class="om ab fo"><div class="on ab oo cl cj op"><h2 class="bd iu gy z fp oq fr fs or fu fw is bi translated">作为分析师如何准备商业案例面试？</h2><div class="os l"><h3 class="bd b gy z fp oq fr fs or fu fw dk translated">作为数据分析师或数据科学家，我们不仅需要知道概率和统计，机器学习算法…</h3></div><div class="ot l"><p class="bd b dl z fp oq fr fs or fu fw dk translated">towardsdatascience.com</p></div></div><div class="ou l"><div class="pa l ow ox oy ou oz ks ol"/></div></div></a></div><div class="oi oj gp gr ok ol"><a rel="noopener follow" target="_blank" href="/10-questions-you-must-know-to-ace-any-sql-interviews-2faa0a424f07"><div class="om ab fo"><div class="on ab oo cl cj op"><h2 class="bd iu gy z fp oq fr fs or fu fw is bi translated">想要在SQL面试中胜出，你必须知道的10个问题</h2><div class="os l"><h3 class="bd b gy z fp oq fr fs or fu fw dk translated">2021年你应该知道的SQL面试问题</h3></div><div class="ot l"><p class="bd b dl z fp oq fr fs or fu fw dk translated">towardsdatascience.com</p></div></div><div class="ou l"><div class="pb l ow ox oy ou oz ks ol"/></div></div></a></div></div></div>    
</body>
</html>