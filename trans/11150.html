<html>
<head>
<title>Which Of Your Features Are Overfitting?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">你的哪些特征是过度拟合的？</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/which-of-your-features-are-overfitting-c46d0762e769?source=collection_archive---------7-----------------------#2021-11-01">https://towardsdatascience.com/which-of-your-features-are-overfitting-c46d0762e769?source=collection_archive---------7-----------------------#2021-11-01</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="a4e8" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsdatascience.com/tagged/tips-and-tricks" rel="noopener" target="_blank">提示和技巧</a></h2><div class=""/><div class=""><h2 id="8687" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">发现“ParShap”:一种高级方法，用于检测哪些列使您的模型在新数据上表现不佳</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/c9a7fab25cfc85cceeafd4a99e7a1784.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IzkK7hNXrIiVYwmP4fv6CA.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">[图片由作者提供]</p></figure><p id="fe4c" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi md translated"><span class="l me mf mg bm mh mi mj mk ml di"> W </span>机器学习中最重要的是对新数据做出正确的预测。</p><p id="bb36" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">当训练数据上的预测很好，但测试数据上的预测很差时，就说模型“<strong class="lj jd">过拟合</strong>”。这意味着该模型从训练数据中学习了太多的噪声模式，因此它无法很好地推广到它以前没有见过的数据。</p><p id="f523" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">过度拟合该怪谁？换句话说，</p><blockquote class="mm"><p id="82c8" class="mn mo it bd mp mq mr ms mt mu mv mc dk translated"><strong class="ak">哪些特性(数据集的列)会妨碍模型对新数据进行良好的概括</strong>？</p></blockquote><p id="8f47" class="pw-post-body-paragraph lh li it lj b lk mw kd lm ln mx kg lp lq my ls lt lu mz lw lx ly na ma mb mc im bi translated">在本文中，借助真实世界的数据集，我们将看到一种高级方法来回答这个问题。</p><h1 id="86dc" class="nb nc it bd nd ne nf ng nh ni nj nk nl ki nm kj nn kl no km np ko nq kp nr ns bi translated"><strong class="ak">功能重要性这次救不了你了！</strong></h1><p id="4cdd" class="pw-post-body-paragraph lh li it lj b lk nt kd lm ln nu kg lp lq nv ls lt lu nw lw lx ly nx ma mb mc im bi translated">如果你对上述问题的回答是“我会考虑特性的重要性”，那就再努力一点。</p><blockquote class="mm"><p id="4e29" class="mn mo it bd mp mq mr ms mt mu mv mc dk translated">要素重要性并没有说明要素在新数据上的表现。</p></blockquote><p id="3b9e" class="pw-post-body-paragraph lh li it lj b lk mw kd lm ln mx kg lp lq my ls lt lu mz lw lx ly na ma mb mc im bi translated">事实上，它只是模型在训练阶段所学内容的代理。如果模型已经学习了许多关于特征“年龄”的模式，那么该特征将在特征重要性中排名较高。这并没有说明<strong class="lj jd">这些模式是否正确</strong>(所谓“正确”，我指的是一种足够通用的模式，对新数据也适用)。</p><p id="7fe3" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">因此，我们需要一种不同的方法来解决这个问题。</p><h1 id="81d2" class="nb nc it bd nd ne nf ng nh ni nj nk nl ki nm kj nn kl no km np ko nq kp nr ns bi translated">“曾经有一家德国医院…”</h1><p id="814d" class="pw-post-body-paragraph lh li it lj b lk nt kd lm ln nu kg lp lq nv ls lt lu nw lw lx ly nx ma mb mc im bi translated">为了解释这种方法，我将使用一个包含从1984年到1988年德国健康注册记录的数据集(该数据集可以从库<a class="ae ny" href="https://github.com/iamaziz/PyDataset" rel="noopener ugc nofollow" target="_blank"> Pydataset </a>访问，使用<a class="ae ny" href="https://github.com/iamaziz/PyDataset/blob/master/LICENSE.txt" rel="noopener ugc nofollow" target="_blank"> MIT许可证</a>)。</p><p id="8295" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">下载数据非常简单:</p><pre class="ks kt ku kv gt nz oa ob oc aw od bi"><span id="056d" class="oe nc it oa b gy of og l oh oi">import pydataset</span><span id="2fbb" class="oe nc it oa b gy oj og l oh oi">X = pydataset.data('rwm5yr')<br/>y = (X['hospvis'] &gt; 1).astype(int)<br/>X = X.drop('hospvis', axis = 1)</span></pre><p id="dfcf" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">该数据集由19，609行组成，其中每行包含给定年份的某个患者的一些信息。请注意，对患者的观察跨越不同的年份，因此同一患者可能出现在数据帧的不同行中。</p><p id="4a77" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">目标变量是:</p><ul class=""><li id="2f2a" class="ok ol it lj b lk ll ln lo lq om lu on ly oo mc op oq or os bi translated">`<strong class="lj jd"> hospvis </strong>`:患者在相应年度内住院时间是否超过1天。</li></ul><p id="c3e4" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">我们处理了16列:</p><ol class=""><li id="cf4e" class="ok ol it lj b lk ll ln lo lq om lu on ly oo mc ot oq or os bi translated">`<strong class="lj jd"> id </strong>`:患者ID(1-7028)；</li><li id="3490" class="ok ol it lj b lk ou ln ov lq ow lu ox ly oy mc ot oq or os bi translated">`<strong class="lj jd"> docvis </strong>`:一年中看医生的次数(0-121)；</li><li id="ab70" class="ok ol it lj b lk ou ln ov lq ow lu ox ly oy mc ot oq or os bi translated">`<strong class="lj jd">年</strong>`:年(1984-1988)；</li><li id="8741" class="ok ol it lj b lk ou ln ov lq ow lu ox ly oy mc ot oq or os bi translated">`<strong class="lj jd"> edlevel </strong>`:教育程度(1-4)；</li><li id="1633" class="ok ol it lj b lk ou ln ov lq ow lu ox ly oy mc ot oq or os bi translated">`<strong class="lj jd">年龄</strong>`:年龄(25-64)；</li><li id="1d7b" class="ok ol it lj b lk ou ln ov lq ow lu ox ly oy mc ot oq or os bi translated">`<strong class="lj jd"> outwork </strong> `: 1如果失业，0否则；</li><li id="a24a" class="ok ol it lj b lk ou ln ov lq ow lu ox ly oy mc ot oq or os bi translated">`<strong class="lj jd">女</strong> `: 1如果女，0否则；</li><li id="6939" class="ok ol it lj b lk ou ln ov lq ow lu ox ly oy mc ot oq or os bi translated">`<strong class="lj jd">已婚</strong> `: 1如果已婚，0否则；</li><li id="d0e8" class="ok ol it lj b lk ou ln ov lq ow lu ox ly oy mc ot oq or os bi translated">`<strong class="lj jd"> kids </strong> `: 1如果有孩子，0否则；</li><li id="aa09" class="ok ol it lj b lk ou ln ov lq ow lu ox ly oy mc ot oq or os bi translated">`<strong class="lj jd"> hhninc </strong>`:家庭年收入，单位为马克(马克)；</li><li id="53b4" class="ok ol it lj b lk ou ln ov lq ow lu ox ly oy mc ot oq or os bi translated">`<strong class="lj jd"> educ </strong>`:正规教育年限(7-18年)；</li><li id="36fd" class="ok ol it lj b lk ou ln ov lq ow lu ox ly oy mc ot oq or os bi translated">`<strong class="lj jd">自营</strong> `: 1如果自营，0否则；</li><li id="7546" class="ok ol it lj b lk ou ln ov lq ow lu ox ly oy mc ot oq or os bi translated">`<strong class="lj jd"> edlevel1 </strong> `: 1如果不到高中毕业，否则为0；</li><li id="1631" class="ok ol it lj b lk ou ln ov lq ow lu ox ly oy mc ot oq or os bi translated">`<strong class="lj jd"> edlevel2 </strong> `: 1如果高中毕业，0否则；</li><li id="1738" class="ok ol it lj b lk ou ln ov lq ow lu ox ly oy mc ot oq or os bi translated">`<strong class="lj jd"> edlevel3 </strong> `: 1如果大学/学院，0否则；</li><li id="2c2b" class="ok ol it lj b lk ou ln ov lq ow lu ox ly oy mc ot oq or os bi translated">edlevel4  `: 1如果读研，0其他。</li></ol><p id="6a39" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">让我们将数据分为训练集和测试集。有更复杂的方法可以做到这一点，如交叉验证，但让我们保持简单。由于这是一个实验，我们将(天真地)把所有的列当作数字特征。</p><pre class="ks kt ku kv gt nz oa ob oc aw od bi"><span id="9236" class="oe nc it oa b gy of og l oh oi">from sklearn.model_selection import train_test_split<br/>from catboost import CatBoostClassifier</span><span id="8d40" class="oe nc it oa b gy oj og l oh oi">X_train, X_test, y_train, y_test = train_test_split(<br/>  X, y, test_size = .2, stratify = y)</span><span id="f592" class="oe nc it oa b gy oj og l oh oi">cat = CatBoostClassifier(silent = True).fit(X_train, y_train)</span></pre><p id="f3bf" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">一旦模型被训练，让我们看看特征重要性:</p><pre class="ks kt ku kv gt nz oa ob oc aw od bi"><span id="6762" class="oe nc it oa b gy of og l oh oi">import pandas as pd</span><span id="bfdc" class="oe nc it oa b gy oj og l oh oi">fimpo = pd.Series(cat.feature_importances_, index = X_train.columns)</span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi oz"><img src="../Images/95248dc2430bd2a9ab446347bc4f51b5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*28HAKR2PolEc_5e6I_nQMA.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">训练模型的特征重要性。[图片由作者提供]</p></figure><p id="535e" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">不足为奇的是,“docvis”——看医生的次数——对于预测患者是否住院超过1天非常重要。“年龄”和“收入”也有些明显。但是病人的id在重要性上排名第二的事实应该让我们怀疑，特别是因为我们已经把它当作一个数字特征。</p><p id="b1c1" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">现在，让我们在训练集和测试集上计算模型性能(ROC下的面积)。</p><pre class="ks kt ku kv gt nz oa ob oc aw od bi"><span id="d3c8" class="oe nc it oa b gy of og l oh oi">from sklearn.metrics import roc_auc_score</span><span id="4fc9" class="oe nc it oa b gy oj og l oh oi">roc_train = roc_auc_score(y_train, cat.predict_proba(X_train)[:, 1])<br/>roc_test = roc_auc_score(y_test, cat.predict_proba(X_test)[:, 1])</span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pa"><img src="../Images/d8e26e13df872ebb7e4203e23acf0250.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YbAaWeY9FJnnIbC9kYEK_Q.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">在训练集和测试集上模拟性能。[图片由作者提供]</p></figure><p id="f029" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">差别太大了！<strong class="lj jd">这是严重过度拟合的迹象</strong>。但是哪些特征对此“负责”呢？</p><h1 id="88cb" class="nb nc it bd nd ne nf ng nh ni nj nk nl ki nm kj nn kl no km np ko nq kp nr ns bi translated">听说过SHAP价值观吗？</h1><p id="99ea" class="pw-post-body-paragraph lh li it lj b lk nt kd lm ln nu kg lp lq nv ls lt lu nw lw lx ly nx ma mb mc im bi translated">我们有许多指标来衡量一个<strong class="lj jd">模型</strong>在一些数据上的表现。但是我们如何衡量一个<strong class="lj jd">特性</strong>在一些数据上的表现呢？</p><p id="1d08" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">做到这一点的最有力的工具叫做“SHAP价值观”。</p><p id="f649" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">一般来说，要高效地计算任何预测模型的SHAP值，可以使用<a class="ae ny" href="https://github.com/slundberg/shap" rel="noopener ugc nofollow" target="_blank">专用的Python库</a>。然而，在本例中，我们将利用Catboost的本机方法:</p><pre class="ks kt ku kv gt nz oa ob oc aw od bi"><span id="d02d" class="oe nc it oa b gy of og l oh oi">from catboost import Pool</span><span id="9ba5" class="oe nc it oa b gy oj og l oh oi">shap_train = pd.DataFrame(<br/>  data = cat.get_feature_importance(<br/>    data = Pool(X_train), <br/>    type = 'ShapValues')[:, :-1], <br/>  index = X_train.index, <br/>  columns = X_train.columns<br/>)</span><span id="59f7" class="oe nc it oa b gy oj og l oh oi">shap_test = pd.DataFrame(<br/>  data = cat.get_feature_importance(<br/>    data = Pool(X_test), <br/>    type = 'ShapValues')[:, :-1], <br/>  index = X_test.index, <br/>  columns = X_test.columns<br/>)</span></pre><p id="fcb9" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">如果你看一下<code class="fe pb pc pd oa b">shap_train</code>和<code class="fe pb pc pd oa b">shap_test</code>，你会注意到它们是各自数据集的相同形状。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi pe"><img src="../Images/c2699c59c04e9b0d5a79f0a8670424b1.png" data-original-src="https://miro.medium.com/v2/resize:fit:752/format:webp/1*2odRWEJGfMn5Me5P-6skVA.png"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">数据帧的形状。[图片由作者提供]</p></figure><p id="f5e2" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">如果你想更多地了解SHAP价值观及其运作方式，你可以从这里的<a class="ae ny" rel="noopener" target="_blank" href="/shap-explained-the-way-i-wish-someone-explained-it-to-me-ab81cc69ef30"/>和这里的<a class="ae ny" rel="noopener" target="_blank" href="/black-box-models-are-actually-more-explainable-than-a-logistic-regression-f263c22795d"/>开始。但是——为了我们的目的——你需要知道的是，SHAP值让你了解每个单一特征对模型在一次或多次观察中做出的最终预测的<strong class="lj jd">影响。</strong></p><p id="d123" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">让我们看几个例子:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pf"><img src="../Images/02b8c94f370806146b644e3f1365beaa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8dJuHxXhuKQ6QRki3_M8SA.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">原始数据与相应的SHAP值。[图片由作者提供]</p></figure><p id="3e85" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">第12071排的患者已0次就诊。相应的SHAP值(-0.753)告诉我们，这条信息将他住院超过1天的概率(实际上是对数优势)降低了-0.753。相反，排18650的患者已经去看过4次医生，这将她住院超过1天的对数比提高了0.918。</p><h1 id="c87f" class="nb nc it bd nd ne nf ng nh ni nj nk nl ki nm kj nn kl no km np ko nq kp nr ns bi translated">会见ParShap</h1><p id="fc51" class="pw-post-body-paragraph lh li it lj b lk nt kd lm ln nu kg lp lq nv ls lt lu nw lw lx ly nx ma mb mc im bi translated">直观地说，数据集上要素性能的一个很好的代理是要素的SHAP值和目标变量之间的<strong class="lj jd">相关性。事实上，如果模型在一个特征上发现了好的模式，那么该特征的SHAP值必须与目标变量高度正相关。</strong></p><p id="a52e" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">例如，如果我们希望计算特征“docvis”和测试集中包含的观测值上的目标变量之间的相关性:</p><pre class="ks kt ku kv gt nz oa ob oc aw od bi"><span id="b706" class="oe nc it oa b gy of og l oh oi">np.corrcoef(shap_test['docvis'], y_test)</span></pre><p id="87b5" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">但是，SHAP值是相加的，这意味着最终预测是所有要素形状的总和。因此，如果我们在计算相关性之前<strong class="lj jd">去除其他特征<em class="pg">的影响，这将更有意义。这正是“<a class="ae ny" href="https://en.wikipedia.org/wiki/Partial_correlation" rel="noopener ugc nofollow" target="_blank"> <strong class="lj jd">偏相关</strong> </a>”的定义。Python库<a class="ae ny" href="https://pingouin-stats.org/" rel="noopener ugc nofollow" target="_blank"> Pingouin </a>中包含了部分相关的一个方便实现:</em></strong></p><pre class="ks kt ku kv gt nz oa ob oc aw od bi"><span id="3835" class="oe nc it oa b gy of og l oh oi">import pingouin</span><span id="2f09" class="oe nc it oa b gy oj og l oh oi">pingouin.partial_corr(<br/>  data = pd.concat([shap_test, y_test], axis = 1).astype(float), <br/>  x = 'docvis', <br/>  y = y_test.name,<br/>  x_covar = [feature for feature in shap_test.columns if feature != 'docvis'] <br/>)</span></pre><p id="eef6" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">这段代码意味着“在去除所有其他特征的影响之后，计算特征‘doc vis’的SHAP值和测试集的观测值上的目标变量之间的相关性”。</p><p id="befd" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">既然每个人都需要一个名字，我就把这个公式叫做<strong class="lj jd">【ParShap】</strong>(来自“Shap值的偏相关”)。</p><p id="7485" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">我们可以在训练集和测试集上对每个特征重复这个过程:</p><pre class="ks kt ku kv gt nz oa ob oc aw od bi"><span id="7600" class="oe nc it oa b gy of og l oh oi">parshap_train = partial_correlation(shap_train, y_train)<br/>parshap_test = partial_correlation(shap_test, y_test)</span></pre><blockquote class="ph pi pj"><p id="d4f8" class="lh li pg lj b lk ll kd lm ln lo kg lp pk lr ls lt pl lv lw lx pm lz ma mb mc im bi translated">注意:你可以在本文末尾找到函数<code class="fe pb pc pd oa b">partial_correlation</code>的定义。</p></blockquote><p id="07c2" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">现在，让我们在x轴<em class="pg">上绘制<code class="fe pb pc pd oa b">parshap_train</code>，在y轴</em>上绘制<code class="fe pb pc pd oa b">parshap_test</code>。</p><pre class="ks kt ku kv gt nz oa ob oc aw od bi"><span id="e816" class="oe nc it oa b gy of og l oh oi">plt.scatter(parshap_train, parshap_test)</span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pn"><img src="../Images/e7d009abc4248b2c7538dac755381b15.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LncV2bm1f0J0dVwlCwkzAg.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">在训练集和测试集上，SHAP值和目标变量之间的部分相关。注意:颜色条代表特征的重要性。[图片由作者提供]</p></figure><p id="a28a" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jd">如果一个特征位于等分线上，这意味着它在训练集和测试集上的表现完全相同</strong>。这是最理想的情况，既不过度配合也不欠配合。相反，如果一个特征位于等分线以下，这意味着它在测试集上的表现比在训练集上的表现差。这是过度拟合区域。</p><p id="7b53" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">从视觉上看，哪些特性表现不佳是显而易见的:我用蓝色圈出了它们。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pn"><img src="../Images/3a703ff75ddcb54b5e617e33e4f50fa6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*34iKbUAPdyoZjba7gWyYXA.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">蓝色圆圈中的要素最容易过度拟合当前模型。[图片由作者提供]</p></figure><p id="2642" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">因此，<code class="fe pb pc pd oa b">parshap_test</code>和<code class="fe pb pc pd oa b">parshap_train</code>之间的算术差(等于每个特征和平分线之间的垂直距离)为我们的模型提供了特征过度拟合程度的度量。</p><pre class="ks kt ku kv gt nz oa ob oc aw od bi"><span id="617f" class="oe nc it oa b gy of og l oh oi">parshap_diff = parshap_test — parshap_train</span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi po"><img src="../Images/402175853f8c63f68ed5c9a1ec36a44e.png" data-original-src="https://miro.medium.com/v2/resize:fit:998/format:webp/1*_WOHwfxM1RQZY8kKVn3D9g.png"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">parshap_test和parshap_train之间的算术差异。[图片由作者提供]</p></figure><p id="5799" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">我们应该如何解读这个输出？基于我们上面所说的，<strong class="lj jd">这个分数越负，特征</strong>带来的过度拟合就越多。</p><h1 id="93ac" class="nb nc it bd nd ne nf ng nh ni nj nk nl ki nm kj nn kl no km np ko nq kp nr ns bi translated">“我不相信你”</h1><p id="a6e6" class="pw-post-body-paragraph lh li it lj b lk nt kd lm ln nu kg lp lq nv ls lt lu nw lw lx ly nx ma mb mc im bi translated">没关系:你不必！你能想出一个方法来检查这篇文章背后的直觉是否正确吗？</p><p id="cb5f" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">在逻辑上，如果我们从数据集中删除“过拟合特征”，我们应该能够减少过拟合(即<code class="fe pb pc pd oa b">roc_train</code>和<code class="fe pb pc pd oa b">roc_test</code>之间的距离)。</p><p id="0879" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">因此，让我们尝试一次删除一个特性，看看ROC下的区域如何变化。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pp"><img src="../Images/22abeac2bacbf3d17244099d1aaa772d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6TBKO7qERC1eOcfTokUAPQ.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">在训练集和测试集上的性能，一次删除一个特征，按特征重要性(左)或ParShap(右)排序。[图片由作者提供]</p></figure><p id="004a" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">在<strong class="lj jd">左侧</strong>，我们一次移除一个特征，按照<strong class="lj jd">特征重要性</strong>排序。因此，首先删除最不重要的(` edlevel4 `),然后删除两个最不重要的(` edlevel4 '和` edlevel1 `),依此类推。</p><p id="6094" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">在右侧<strong class="lj jd">的</strong>上，我们遵循相同的过程，但是移除的顺序由<strong class="lj jd"> ParShap </strong>给出。因此，首先删除最负的ParShap (`id `),然后删除两个最负的ParShap (`id '和` year `),依此类推。</p><p id="67eb" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">正如我们所希望的那样，删除具有最大负ParShap的特性导致过度拟合的大幅减少。事实上，<code class="fe pb pc pd oa b">roc_train</code>离<code class="fe pb pc pd oa b">roc_test</code>越来越近了。</p><p id="2ed7" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">请注意<strong class="lj jd">这只是一个测试，用来检查我们推理路线</strong>的正确性。一般来说，ParShap <strong class="lj jd">不应该作为特征选择</strong>的方法。的确，<strong class="lj jd">一些特征容易过度拟合的事实并不意味着这些特征根本没有携带有用的信息！</strong>(例如，本例中的收入和年龄)。</p><p id="019f" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">然而，ParShap在给我们提示如何调试我们的模型方面非常有帮助。事实上，它允许我们将注意力集中在那些需要更多特性工程或规则的特性上。</p></div><div class="ab cl pq pr hx ps" role="separator"><span class="pt bw bk pu pv pw"/><span class="pt bw bk pu pv pw"/><span class="pt bw bk pu pv"/></div><div class="im in io ip iq"><blockquote class="ph pi pj"><p id="762c" class="lh li pg lj b lk ll kd lm ln lo kg lp pk lr ls lt pl lv lw lx pm lz ma mb mc im bi translated">本文使用的完整代码(由于随机种子，您可能会获得略有不同的结果):</p></blockquote><pre class="ks kt ku kv gt nz oa ob oc aw od bi"><span id="0e14" class="oe nc it oa b gy of og l oh oi"># Import libraries<br/>import pandas as pd<br/>import pydataset<br/>from sklearn.model_selection import train_test_split<br/>from catboost import CatBoostClassifier, Pool<br/>from sklearn.metrics import roc_auc_score<br/>from pingouin import partial_corr<br/>import matplotlib.pyplot as plt</span><span id="c684" class="oe nc it oa b gy oj og l oh oi"># Print documentation and read data<br/>print('################# Print docs')<br/>pydataset.data('rwm5yr', show_doc = True)</span><span id="455c" class="oe nc it oa b gy oj og l oh oi">X = pydataset.data('rwm5yr')<br/>y = (X['hospvis'] &gt; 1).astype(int)<br/>X = X.drop('hospvis', axis = 1)</span><span id="80e7" class="oe nc it oa b gy oj og l oh oi"># Split data in train + test<br/>X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .2, stratify = y)</span><span id="6ad4" class="oe nc it oa b gy oj og l oh oi"># Fit model<br/>cat = CatBoostClassifier(silent = True).fit(X_train, y_train)</span><span id="41fa" class="oe nc it oa b gy oj og l oh oi"># Show feature importance<br/>fimpo = pd.Series(cat.feature_importances_, index = X_train.columns)<br/>fig, ax = plt.subplots()<br/>fimpo.sort_values().plot.barh(ax = ax)<br/>fig.savefig('fimpo.png', dpi = 200, bbox_inches="tight")<br/>fig.show()</span><span id="c0bb" class="oe nc it oa b gy oj og l oh oi"># Compute metrics<br/>roc_train = roc_auc_score(y_train, cat.predict_proba(X_train)[:, 1])<br/>roc_test = roc_auc_score(y_test, cat.predict_proba(X_test)[:, 1])</span><span id="9d43" class="oe nc it oa b gy oj og l oh oi">print('\n################# Print roc')<br/>print('roc_auc train: {:.2f}'.format(roc_train))<br/>print('roc_auc  test: {:.2f}'.format(roc_test))</span><span id="11b6" class="oe nc it oa b gy oj og l oh oi"># Compute SHAP values  <br/>shap_train = pd.DataFrame(<br/>  data = cat.get_feature_importance(data = Pool(X_train), type = 'ShapValues')[:, :-1],<br/>  index = X_train.index, <br/>  columns = X_train.columns<br/>)<br/>shap_test = pd.DataFrame(<br/>  data = cat.get_feature_importance(data = Pool(X_test), type = 'ShapValues')[:, :-1],<br/>  index = X_test.index, <br/>  columns = X_test.columns<br/>)</span><span id="c8e1" class="oe nc it oa b gy oj og l oh oi">print('\n################# Print df shapes')<br/>print(f'X_train.shape:    {X_train.shape}')<br/>print(f'X_test.shape:     {X_test.shape}\n')<br/>print(f'shap_train.shape: {shap_train.shape}')<br/>print(f'shap_test.shape:  {shap_test.shape}')</span><span id="49c3" class="oe nc it oa b gy oj og l oh oi">print('\n################# Print data and SHAP')<br/>print('Original data:')<br/>display(X_test.head(3))<br/>print('\nCorresponding SHAP values:')<br/>display(shap_test.head(3).round(3))</span><span id="75f1" class="oe nc it oa b gy oj og l oh oi"># Define function for partial correlation<br/>def partial_correlation(X, y):<br/>  out = pd.Series(index = X.columns, dtype = float)<br/>  for feature_name in X.columns:<br/>    out[feature_name] = partial_corr(<br/>      data = pd.concat([X, y], axis = 1).astype(float), <br/>      x = feature_name, <br/>      y = y.name,<br/>      x_covar = [f for f in X.columns if f != feature_name] <br/>    ).loc['pearson', 'r']<br/>  return out</span><span id="20ac" class="oe nc it oa b gy oj og l oh oi"># Compute ParShap<br/>parshap_train = partial_correlation(shap_train, y_train)<br/>parshap_test = partial_correlation(shap_test, y_test)<br/>parshap_diff = pd.Series(parshap_test - parshap_train, name = 'parshap_diff')</span><span id="e69d" class="oe nc it oa b gy oj og l oh oi">print('\n################# Print parshap_diff')<br/>print(parshap_diff.sort_values())<br/>                         <br/># Plot parshap<br/>plotmin, plotmax = min(parshap_train.min(), parshap_test.min()), max(parshap_train.max(), parshap_test.max())<br/>plotbuffer = .05 * (plotmax - plotmin)<br/>fig, ax = plt.subplots()<br/>if plotmin &lt; 0:<br/>    ax.vlines(0, plotmin - plotbuffer, plotmax + plotbuffer, color = 'darkgrey', zorder = 0)<br/>    ax.hlines(0, plotmin - plotbuffer, plotmax + plotbuffer, color = 'darkgrey', zorder = 0)<br/>ax.plot(<br/>    [plotmin - plotbuffer, plotmax + plotbuffer], [plotmin - plotbuffer, plotmax + plotbuffer], <br/>    color = 'darkgrey', zorder = 0<br/>)<br/>sc = ax.scatter(<br/>    parshap_train, parshap_test, <br/>    edgecolor = 'grey', c = fimpo, s = 50, cmap = plt.cm.get_cmap('Reds'), vmin = 0, vmax = fimpo.max())<br/>ax.set(title = 'Partial correlation bw SHAP and target...', xlabel = '... on Train data', ylabel = '... on Test data')<br/>cbar = fig.colorbar(sc)<br/>cbar.set_ticks([])<br/>for txt in parshap_train.index:<br/>    ax.annotate(txt, (parshap_train[txt], parshap_test[txt] + plotbuffer / 2), ha = 'center', va = 'bottom')<br/>fig.savefig('parshap.png', dpi = 300, bbox_inches="tight")<br/>fig.show()</span><span id="3869" class="oe nc it oa b gy oj og l oh oi"># Feature selection<br/>n_drop_max = 5<br/>iterations = 4</span><span id="1f6a" class="oe nc it oa b gy oj og l oh oi">features = {'parshap': parshap_diff, 'fimpo': fimpo}<br/>features_dropped = {}<br/>roc_auc_scores = {<br/>  'fimpo': {'train': pd.DataFrame(), 'test': pd.DataFrame()},<br/>  'parshap': {'train': pd.DataFrame(), 'test': pd.DataFrame()}<br/>}</span><span id="1838" class="oe nc it oa b gy oj og l oh oi">for type_ in ['parshap', 'fimpo']:<br/>  for n_drop in range(n_drop_max + 1):<br/>    features_drop = features[type_].sort_values().head(n_drop).index.to_list()<br/>    features_dropped[type_] = features_drop<br/>    X_drop = X.drop(features_drop, axis = 1)<br/>    for i in range(iterations):<br/>      X_train, X_test, y_train, y_test = train_test_split(X_drop, y, test_size = .2, stratify = y)<br/>      cat = CatBoostClassifier(silent = True).fit(X_train, y_train)<br/>      roc_auc_scores[type_]['train'].loc[n_drop, i] = roc_auc_score(y_train, cat.predict_proba(X_train)[:, 1])<br/>      roc_auc_scores[type_]['test'].loc[n_drop, i] = roc_auc_score(y_test, cat.predict_proba(X_test)[:, 1])<br/>        <br/># Plot feature selection<br/>fig, axs = plt.subplots(1, 2, sharey = True, figsize = (8, 3))<br/>plt.subplots_adjust(wspace = .1)<br/>axs[0].plot(roc_auc_scores['fimpo']['train'].index, roc_auc_scores['fimpo']['train'].mean(axis = 1), lw = 3, label = 'Train')<br/>axs[0].plot(roc_auc_scores['fimpo']['test'].index, roc_auc_scores['fimpo']['test'].mean(axis = 1), lw = 3, label = 'Test')<br/>axs[0].set_xticks(roc_auc_scores['fimpo']['train'].index)<br/>axs[0].set_xticklabels([''] + features_dropped['fimpo'], rotation = 90)<br/>axs[0].set_title('Feature Importance')<br/>axs[0].set_xlabel('Feature dropped')<br/>axs[0].grid()<br/>axs[0].legend(loc = 'center left')<br/>axs[0].set(ylabel = 'ROC-AUC score')<br/>axs[1].plot(roc_auc_scores['parshap']['train'].index, roc_auc_scores['parshap']['train'].mean(axis = 1), lw = 3, label = 'Train')<br/>axs[1].plot(roc_auc_scores['parshap']['test'].index, roc_auc_scores['parshap']['test'].mean(axis = 1), lw = 3, label = 'Test')<br/>axs[1].set_xticks(roc_auc_scores['parshap']['train'].index)<br/>axs[1].set_xticklabels([''] + features_dropped['parshap'], rotation = 90)<br/>axs[1].set_title('ParShap')<br/>axs[1].set_xlabel('Feature dropped')<br/>axs[1].grid()<br/>axs[1].legend(loc = 'center left')<br/>fig.savefig('feature_selection.png', dpi = 300, bbox_inches="tight")<br/>fig.show()</span></pre></div><div class="ab cl pq pr hx ps" role="separator"><span class="pt bw bk pu pv pw"/><span class="pt bw bk pu pv pw"/><span class="pt bw bk pu pv"/></div><div class="im in io ip iq"><blockquote class="ph pi pj"><p id="9e25" class="lh li pg lj b lk ll kd lm ln lo kg lp pk lr ls lt pl lv lw lx pm lz ma mb mc im bi translated">感谢您的阅读！我希望你喜欢这篇文章。如果你愿意，<a class="ae ny" href="https://www.linkedin.com/in/samuelemazzanti/" rel="noopener ugc nofollow" target="_blank">在Linkedin上加我</a>！</p></blockquote></div></div>    
</body>
</html>