<html>
<head>
<title>Making neural networks solve tougher problems by “thinking” for longer!</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">让神经网络通过更长时间的“思考”来解决更棘手的问题！</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/making-neural-networks-solve-tougher-problems-by-thinking-for-longer-32ecb599bcd?source=collection_archive---------29-----------------------#2021-07-31">https://towardsdatascience.com/making-neural-networks-solve-tougher-problems-by-thinking-for-longer-32ecb599bcd?source=collection_archive---------29-----------------------#2021-07-31</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/></div><div class="ab cl jn jo hu jp" role="separator"><span class="jq bw bk jr js jt"/><span class="jq bw bk jr js jt"/><span class="jq bw bk jr js"/></div><div class="ij ik il im in"><blockquote class="ju jv jw"><p id="631e" class="jx jy jz ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">这是我根据对论文的理解写的一篇博文——“<strong class="ka ir">你能学会一个算法吗？从简单到困难的循环网络问题的归纳</strong>，来自Schwarzchild等人(2021) [ <a class="ae kw" href="https://arxiv.org/abs/2106.04537" rel="noopener ugc nofollow" target="_blank">链接</a>。</p><p id="8eec" class="jx jy jz ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka ir">TL；在这里，作者在更简单的迷宫/谜题形式上训练一个递归神经网络(RNN)，并在测试时，在“更难”的问题上评估它们。与通常的训练/测试集评估不同，在这里，RNN可以“思考更长时间”进行测试。更具体地说，它将比在训练阶段有更多的循环块。作者注意到，与具有类似深度的简单前馈神经网络相比，通过思考更长时间，RNNs能够以有原则的方式解决更复杂的问题。也就是通过使用更多的循环层。这里，相似深度指的是训练时RNN和前馈神经网络的有效深度。</strong></p><p id="1393" class="jx jy jz ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka ir">长处</strong> —论文开始了一个重要的方向——“如何让神经网络处理看不见的测试情况？使用比训练时更多的计算有助于在测试时表现得更好吗？NNs能从它的记忆(或内部表征)中归纳/推断吗？在这里，作者让RNN推断它对类似的，但“更难”的问题的理解。最初的实验看起来很有希望，并可能成为我们理解什么实际上使更深(或更宽)的模型表现更好的方法。</p><p id="3fa2" class="jx jy jz ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka ir">告诫</strong> — <strong class="ka ir"> </strong>除了少数简单明了的案例，很难对问题的难度进行精选和排序。因此，在较简单的问题上进行训练并理解对较难问题的影响将是一个挑战。这在其他研究领域也是一个众所周知的问题，比如课程学习(Bengio et al .，2008)。</p><p id="cdde" class="jx jy jz ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka ir">未来方向</strong> —作者注意到，由于更好的感受野，利用扩张的卷积得到了更好的结果。一个增加的方法是使用注意层、变压器等来观察更大感受野的效果。因此，回答了一个更大的问题，什么是重要的——思考更长时间还是对整体空间有“感觉”,或者两者都有。此外，这里通过使用相同的权重(权重共享)天真地增加了循环迭代。观察随机扰动一小部分权重的效果会很有趣。最后，尽管rnn考虑的时间更长，但他们应该考虑的“时间”仍然是预定义的——使额外的循环步骤的数量由数据驱动可能是一个有趣的实验。</p></blockquote><p id="0e70" class="pw-post-body-paragraph jx jy iq ka b kb kc kd ke kf kg kh ki kx kk kl km ky ko kp kq kz ks kt ku kv ij bi translated"><strong class="ka ir">简介</strong></p><p id="48f9" class="pw-post-body-paragraph jx jy iq ka b kb kc kd ke kf kg kh ki kx kk kl km ky ko kp kq kz ks kt ku kv ij bi translated">人类表现出解决复杂问题的能力，这是基于他们对简单玩具问题的学习。例如，解决了一个3x3难题的人，可以使用类似的逻辑，花更多的时间来解决一个5x5难题，尽管以前从未见过它。更具体地说，人类可以通过“更努力/更长时间地思考”，从更小的问题中得出解决更大问题的模式。</p><p id="45f9" class="pw-post-body-paragraph jx jy iq ka b kb kc kd ke kf kg kh ki kx kk kl km ky ko kp kq kz ks kt ku kv ij bi translated">在Schwarzchild等人(2021)的文章中，作者从人类身上看到的这些现象中获得了灵感，以了解是否可以在神经网络(NNs)中观察到类似的模式。也就是说，我们能否让神经网络基于对简单问题的学习来解决<em class="jz">未知和更复杂的</em>问题。这里，对3种主要类型的数据进行分析——前缀和计算、迷宫和象棋。</p><p id="ae90" class="pw-post-body-paragraph jx jy iq ka b kb kc kd ke kf kg kh ki kx kk kl km ky ko kp kq kz ks kt ku kv ij bi translated">出于多种原因，在测试推断中思考更长时间可能是一个有趣且重要的问题，但不限于—</p><ol class=""><li id="60e3" class="la lb iq ka b kb kc kf kg kx lc ky ld kz le kv lf lg lh li bi translated">计算预算的限制——模型可以在测试时进化，而不是训练非常深的网络。</li><li id="50cf" class="la lb iq ka b kb lj kf lk kx ll ky lm kz ln kv lf lg lh li bi translated">这在问题不断发展的领域(与训练相比，输入的复杂性和输入的大小都在增加)，以及对测试分布的访问是有限的，可能是特别有趣的。</li><li id="b0ae" class="la lb iq ka b kb lj kf lk kx ll ky lm kz ln kv lf lg lh li bi translated">理解神经网络从其记忆中进行推断的能力，并为<em class="jz">推广</em>带来新的视角。</li></ol><p id="dd5f" class="pw-post-body-paragraph jx jy iq ka b kb kc kd ke kf kg kh ki kx kk kl km ky ko kp kq kz ks kt ku kv ij bi translated">已经有大量的文献从<strong class="ka ir">迁移学习</strong> <em class="jz">的角度研究类似的方面(Hinton等人，2006；刘冰等人，2007年；本吉奥等，2012) </em>，<strong class="ka ir">神经网络中的泛化</strong> <em class="jz">(张等，2016；阿罗拉等人，2019；马内尔等人，2020年；Garg等人，2021) </em>，<strong class="ka ir">元学习</strong> <em class="jz"> (Schmidhuber，1987；本吉奥等人，1992年；Thrun &amp; Pratt，1998；Andrychowicz等人，2016；拉维&amp;拉罗歇尔，2017；Finn等人，2017) </em>、<strong class="ka ir">神经架构搜索</strong> <em class="jz"> (Zoph等人，2016) </em>等等。虽然，本文中使用的任务类型(Schwarzchild等人，2021)与上述作品不同—</p><ol class=""><li id="a735" class="la lb iq ka b kb kc kf kg kx lc ky ld kz le kv lf lg lh li bi translated">在这里，重点是让神经网络像人类一样“思考更久”，以解决更复杂的问题——也就是说，在这里，模型被提供更多的计算(或更长的执行时间)，直到它对答案有信心。在传统的迁移学习框架中，模型根据<em class="jz">开放世界知识</em>进行预训练，然后根据相关的下游任务进行微调——目的是让模型从大量可用数据中学习有用的表示，并使用它来提高下游任务的性能。</li><li id="0407" class="la lb iq ka b kb lj kf lk kx ll ky lm kz ln kv lf lg lh li bi translated">在元学习设置中，重点是<strong class="ka ir"> <em class="jz">学习一个可以学习</em></strong>的算法。而在目前的工作中，作者着眼于学习一种算法来解决一个特定的问题，例如，一个国际象棋配置或拼图游戏。</li><li id="edb8" class="la lb iq ka b kb lj kf lk kx ll ky lm kz ln kv lf lg lh li bi translated">典型的<em class="jz">概括与记忆</em>工作侧重于理解神经网络从数据中学到了什么，以及它们基于相同分布、不同分布等的“概括误差”(训练/验证误差)概括得有多好。在这项工作中，重点是了解模型是否可以从较小的问题中学习模式，并在额外计算的帮助下将它们用于较大的问题，从而进行归纳。</li></ol><h2 id="a349" class="lo lp iq bd lq lr ls dn lt lu lv dp lw kx lx ly lz ky ma mb mc kz md me mf mg bi translated"><strong class="ak">数据集</strong></h2><p id="4c16" class="pw-post-body-paragraph jx jy iq ka b kb mh kd ke kf mi kh ki kx mj kl km ky mk kp kq kz ml kt ku kv ij bi translated">如前所述，作者分析了3类问题的假设——</p><ol class=""><li id="30d2" class="la lb iq ka b kb kc kf kg kx lc ky ld kz le kv lf lg lh li bi translated"><strong class="ka ir">前缀求和问题</strong> [ <a class="ae kw" href="https://en.wikipedia.org/wiki/Prefix_sum" rel="noopener ugc nofollow" target="_blank">链接</a> ]—这里输入的是32位长度的二进制字符串(测试时使用40到44位的字符串)。重点是计算二进制字符串模2的前缀和。也就是说，输出将是计算输入模2的累积和的等长字符串。</li></ol><figure class="mn mo mp mq gt mr gh gi paragraph-image"><div class="gh gi mm"><img src="../Images/9d07bf13e4c0c5207eee0aad69eb2b4a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1366/format:webp/1*acg9B_rtmtCHschqJfQr1w.png"/></div><p class="mu mv gj gh gi mw mx bd b be z dk translated">模2前缀和的一个例子。[ <a class="ae kw" href="https://arxiv.org/pdf/2106.04537.pdf" rel="noopener ugc nofollow" target="_blank">来源</a></p></figure><p id="8c3a" class="pw-post-body-paragraph jx jy iq ka b kb kc kd ke kf kg kh ki kx kk kl km ky ko kp kq kz ks kt ku kv ij bi translated">2.<strong class="ka ir">迷宫</strong> —训练时输入为9x9迷宫，测试时输入为13x13迷宫。预期输出是二进制掩码输出，它突出显示输入迷宫中绿色和红色点之间的最短路径。</p><figure class="mn mo mp mq gt mr gh gi paragraph-image"><div class="gh gi my"><img src="../Images/1e14cd1bc6a5d1bdc134dc9246a2ddbb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1154/format:webp/1*mKeM3dw4Mrv_ATWMMIaQFw.png"/></div><p class="mu mv gj gh gi mw mx bd b be z dk translated">左侧面板是迷宫的示例，绿点代表起点，红点代表终点(目的地)。白色像素是代理可以采取的步骤，黑色像素是代理无法通过的障碍物。右图显示了这些点之间的最短路径(这是网络的预期输出)。[ <a class="ae kw" href="https://arxiv.org/abs/2106.04537" rel="noopener ugc nofollow" target="_blank">来源</a></p></figure><p id="457d" class="pw-post-body-paragraph jx jy iq ka b kb kc kd ke kf kg kh ki kx kk kl km ky ko kp kq kz ks kt ku kv ij bi translated">3.<strong class="ka ir"> Chess — </strong>输入数据是一个8x8x12数组，输入棋盘中所有棋子的位置。这些配置来自在线平台Lichess。与之前的数据集不同，其中输入大小在训练和测试时间中变化，这里我们有不同的“难度评级”:模型在难度较低的样本上训练(基于评级)，在难度较高的样本上测试。</p><figure class="mn mo mp mq gt mr gh gi paragraph-image"><div class="gh gi my"><img src="../Images/48fcf5a3569bbe8dbf3a3483d6eef3b1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1154/format:webp/1*hCjy2cWouediUY30nLPHdg.png"/></div><p class="mu mv gj gh gi mw mx bd b be z dk translated">左图是输入的国际象棋布局，右图是可以引导玩家走向胜利的(单个)最佳走法。来源</p></figure><h2 id="2098" class="lo lp iq bd lq lr ls dn lt lu lv dp lw kx lx ly lz ky ma mb mc kz md me mf mg bi translated">模型架构和培训</h2><figure class="mn mo mp mq gt mr gh gi paragraph-image"><div class="gh gi mz"><img src="../Images/d32e0070ef50c734c5cefab564a751b7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1180/format:webp/1*npNs69TmK1-fWMBb_0ygUA.png"/></div><p class="mu mv gj gh gi mw mx bd b be z dk translated">基于循环块的<em class="na">展开</em>测量有效深度的样本。[ <a class="ae kw" href="https://arxiv.org/abs/2102.11011" rel="noopener ugc nofollow" target="_blank">来源</a></p></figure><p id="7d62" class="pw-post-body-paragraph jx jy iq ka b kb kc kd ke kf kg kh ki kx kk kl km ky ko kp kq kz ks kt ku kv ij bi translated">所用的前馈网络基于ResNet架构，但有一些变化——除了初始层和最后一个递归层之后，宽度没有变化，没有批处理，完全卷积——以确保递归模型尽可能接近前馈实现。用于循环网络的模型架构类似于经典的ResNet模型。也就是说，两个模型之间的主要区别在于，权重在<em class="jz">循环块</em>中共享，但在前馈网络中不是这样。</p><h2 id="4e3c" class="lo lp iq bd lq lr ls dn lt lu lv dp lw kx lx ly lz ky ma mb mc kz md me mf mg bi translated">结果</h2><figure class="mn mo mp mq gt mr gh gi paragraph-image"><div role="button" tabindex="0" class="nc nd di ne bf nf"><div class="gh gi nb"><img src="../Images/00e7995ab2ef7edf9819d220b5b28fd2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PL4WAYTdTXrSJL7sQoLy8w.png"/></div></div><p class="mu mv gj gh gi mw mx bd b be z dk translated">该图将最佳前馈模型与等效深度的递归模型进行了比较。[ <a class="ae kw" href="https://arxiv.org/abs/2106.04537" rel="noopener ugc nofollow" target="_blank">来源</a></p></figure><p id="907b" class="pw-post-body-paragraph jx jy iq ka b kb kc kd ke kf kg kh ki kx kk kl km ky ko kp kq kz ks kt ku kv ij bi translated"><strong class="ka ir">前缀总和</strong> —从上图中我们可以看出，随着迭代次数的增加(循环块的展开)，性能会提高。</p><figure class="mn mo mp mq gt mr gh gi paragraph-image"><div role="button" tabindex="0" class="nc nd di ne bf nf"><div class="gh gi ng"><img src="../Images/3bf8a1597df8233731ae789bb3ebbc86.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VGAvTZpCO4riELOG88Ndyg.png"/></div></div><p class="mu mv gj gh gi mw mx bd b be z dk translated">结果基于在32位字符串上训练后在40位字符串上的测试。[ <a class="ae kw" href="https://arxiv.org/abs/2106.04537" rel="noopener ugc nofollow" target="_blank">来源</a></p></figure><p id="dbdb" class="pw-post-body-paragraph jx jy iq ka b kb kc kd ke kf kg kh ki kx kk kl km ky ko kp kq kz ks kt ku kv ij bi translated"><strong class="ka ir">迷宫求解— </strong>模型在经历更多迭代的同时改进并学习其路径，如下图所示。递归模型表现出比传统前馈模型更好的性能</p><figure class="mn mo mp mq gt mr gh gi paragraph-image"><div role="button" tabindex="0" class="nc nd di ne bf nf"><div class="gh gi nh"><img src="../Images/01ee057257af70aa40e9304aa47f5d64.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fm9AC_b3OQ4RWulx2bOlag.png"/></div></div><p class="mu mv gj gh gi mw mx bd b be z dk translated">在这里，模型解决迷宫的顺序思考能力可以被可视化。此外，根据模型在其输出中的置信度对像素进行着色。[ <a class="ae kw" href="https://arxiv.org/abs/2106.04537" rel="noopener ugc nofollow" target="_blank">来源</a></p></figure><figure class="mn mo mp mq gt mr gh gi paragraph-image"><div role="button" tabindex="0" class="nc nd di ne bf nf"><div class="gh gi ni"><img src="../Images/5d69ce5f95f4b3a69474c70db5d570b3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lndZCH4VccizYcRQ3UtqGw.png"/></div></div><p class="mu mv gj gh gi mw mx bd b be z dk translated">8×8拼图训练后在13×13格子上的表现。递归模型始终比深度相当的前馈模型表现更好。</p></figure><figure class="mn mo mp mq gt mr gh gi paragraph-image"><div role="button" tabindex="0" class="nc nd di ne bf nf"><div class="gh gi nj"><img src="../Images/01f48200552e508510b432e6d07afdfb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*m_TuzrDZb1jFv82PV3sm_g.png"/></div></div><p class="mu mv gj gh gi mw mx bd b be z dk translated">这里将递归模型与最佳前馈模型进行比较。通过思考更长的时间，循环模型能够表现得更好。</p></figure><p id="c8b2" class="pw-post-body-paragraph jx jy iq ka b kb kc kd ke kf kg kh ki kx kk kl km ky ko kp kq kz ks kt ku kv ij bi translated"><strong class="ka ir">国际象棋— </strong>观察到的国际象棋趋势也类似。在考虑这个难题的最佳解决方案时，为了简洁起见，忽略了一些边缘情况。</p><figure class="mn mo mp mq gt mr gh gi paragraph-image"><div role="button" tabindex="0" class="nc nd di ne bf nf"><div class="gh gi nh"><img src="../Images/a778bc0d06883329543b4a27b3fd7475.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PaaVAsy2u8wyPaXHBBSm8g.png"/></div></div><p class="mu mv gj gh gi mw mx bd b be z dk translated">我们可以注意到一个相似的趋势，模型依次分析所有可能的移动，并且随着迭代的进行越来越自信。</p></figure><figure class="mn mo mp mq gt mr gh gi paragraph-image"><div role="button" tabindex="0" class="nc nd di ne bf nf"><div class="gh gi nj"><img src="../Images/2953dcbe253ccf6f0f6c35d478411808.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CvwodM3VnRO2CNWCK6pJvg.png"/></div></div><p class="mu mv gj gh gi mw mx bd b be z dk translated">与表现最好的深度为20的前馈网络相比，在测试时思考得更深入有助于递归模型更好地解决更难的象棋难题。</p></figure><h2 id="f7c1" class="lo lp iq bd lq lr ls dn lt lu lv dp lw kx lx ly lz ky ma mb mc kz md me mf mg bi translated">讨论</h2><p id="2dea" class="pw-post-body-paragraph jx jy iq ka b kb mh kd ke kf mi kh ki kx mj kl km ky mk kp kq kz ml kt ku kv ij bi translated">作者提出了一个关于循环层如何帮助的重要问题。这里仍有许多未解的问题</p><ol class=""><li id="9a68" class="la lb iq ka b kb kc kf kg kx lc ky ld kz le kv lf lg lh li bi translated">递归模型如何模拟分层滤波器，即，尽管权重共享，但是与每层中具有特定滤波器的前馈网络相比，递归模型如何能够提取相似的模式以学习特定模式？这个问题在作者的另一篇论文中得到了部分回答——复发和深度的不可思议的相似性[ <a class="ae kw" href="https://arxiv.org/pdf/2102.11011.pdf" rel="noopener ugc nofollow" target="_blank">链接</a> ](很快会有更多关于这篇论文的内容！).</li><li id="7b1b" class="la lb iq ka b kb lj kf lk kx ll ky lm kz ln kv lf lg lh li bi translated">怎样才能让模型明白什么时候停止思考？人类在找到一个谜题的最优解后，不会突然走向一个随机的方向，最后得出一个错误的答案(至少，我们<em class="jz">大部分</em>不会:)</li><li id="cc5e" class="la lb iq ka b kb lj kf lk kx ll ky lm kz ln kv lf lg lh li bi translated">理解思考时间更长、视野更大的影响。该论文表明，对于较大的感受野使用扩张的卷积是有帮助的。这是否意味着有一个方向，我们不仅可以看到模型思考了多长时间，还可以看到所有模型(在空间上)可以思考/感觉什么？</li></ol><p id="ab68" class="pw-post-body-paragraph jx jy iq ka b kb kc kd ke kf kg kh ki kx kk kl km ky ko kp kq kz ks kt ku kv ij bi translated">如果你已经读到这里，非常感谢，希望你能读到一些新的有趣的东西！请查看<a class="ae kw" href="https://arxiv.org/pdf/2106.04537.pdf" rel="noopener ugc nofollow" target="_blank">原始论文</a>以获得更深入的分析和结果！请随时让我知道你的想法。我是这方面文献的初学者，如有理解上的错误，欢迎随时指出！乐学！</p><p id="d86a" class="pw-post-body-paragraph jx jy iq ka b kb kc kd ke kf kg kh ki kx kk kl km ky ko kp kq kz ks kt ku kv ij bi translated"><a class="ae kw" href="https://twitter.com/rohanalchemist" rel="noopener ugc nofollow" target="_blank"> Twitter </a> | <a class="ae kw" href="http://linkedin.com/in/rohan-sukumaran-3271ba145" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a></p><p id="6869" class="pw-post-body-paragraph jx jy iq ka b kb kc kd ke kf kg kh ki kx kk kl km ky ko kp kq kz ks kt ku kv ij bi translated">PS——所有的数字都来自原始论文，如果没有，我也提到了来源。这些信息图表的所有荣誉归原作者所有。谢谢！</p></div></div>    
</body>
</html>