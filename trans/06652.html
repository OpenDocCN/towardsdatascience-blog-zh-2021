<html>
<head>
<title>Understand &amp; Implement Logistic Regression in Python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">理解并在Python中实现逻辑回归</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/understand-implement-logistic-regression-in-python-c1e1a329f460?source=collection_archive---------9-----------------------#2021-06-15">https://towardsdatascience.com/understand-implement-logistic-regression-in-python-c1e1a329f460?source=collection_archive---------9-----------------------#2021-06-15</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="ed11" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">Sigmoid函数、线性回归和参数估计(对数似然和交叉熵损失)</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/0e3bb89978b376dd5c29b653eed94551.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*Ag1WGm58e7O5cGIe"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">梅格·波尔登在<a class="ae kv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><h1 id="47ab" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">目标</h1><p id="f224" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">这篇文章的主要目的是理解二元逻辑回归是如何工作的。</p><ul class=""><li id="fa10" class="mk ml iq lq b lr mm lu mn lx mo mb mp mf mq mj mr ms mt mu bi translated">我将在高层次上回顾理解逻辑回归所涉及的基本数学概念和函数。</li><li id="1660" class="mk ml iq lq b lr mv lu mw lx mx mb my mf mz mj mr ms mt mu bi translated">在此过程中，我将介绍两种众所周知的梯度方法(上升/下降),以使用对数似然和交叉熵损失函数来估计𝜃参数。</li><li id="0f0d" class="mk ml iq lq b lr mv lu mw lx mx mb my mf mz mj mr ms mt mu bi translated">我将使用Kaggle的泰坦尼克号数据集从头创建一个逻辑回归模型来预测乘客存活率。train.csv和test.csv文件可以在<a class="ae kv" href="https://www.kaggle.com/c/titanic/data" rel="noopener ugc nofollow" target="_blank"> Kaggle的Titanic数据页面</a>上找到。为了加快模型构建，我将只使用三个特征，<strong class="lq ir">年龄</strong>(连续)、<strong class="lq ir">p类</strong>(第一类= 1，第二类= 2，第三类= 3)，以及<strong class="lq ir">性别</strong>(女性= 1，男性= 0)。</li></ul><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi na"><img src="../Images/01a7a42d72b81f9945f54a61f2464011.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*S5Hy2K45v4s8O3Pago1yhg.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图0:图片由<a class="ae kv" href="https://dolee.medium.com" rel="noopener">作者</a>提供；数据来源:<a class="ae kv" href="https://www.kaggle.com/c/titanic/data" rel="noopener ugc nofollow" target="_blank"> Kaggle的泰坦尼克号数据</a></p></figure><h1 id="2d13" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">目录</h1><ul class=""><li id="b8fd" class="mk ml iq lq b lr ls lu lv lx nb mb nc mf nd mj mr ms mt mu bi translated">算法概要</li><li id="e8c8" class="mk ml iq lq b lr mv lu mw lx mx mb my mf mz mj mr ms mt mu bi translated">数学概念和函数</li><li id="7c9b" class="mk ml iq lq b lr mv lu mw lx mx mb my mf mz mj mr ms mt mu bi translated">行动中的代码</li><li id="a395" class="mk ml iq lq b lr mv lu mw lx mx mb my mf mz mj mr ms mt mu bi translated">最后的想法</li></ul><h1 id="467a" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">算法概要</h1><p id="b416" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">逻辑回归是一种分类算法，它输出一组给定实例的预测概率，这些实例的特征与优化的𝜃参数和偏差项配对。这些参数也称为权重或系数。概率被转换成预测例如成功(“1”)或失败(“0”)的目标类别(例如，0或1)。虽然我将仔细研究二元逻辑回归模型，但是逻辑回归也可以用于进行多类预测。</p><p id="601b" class="pw-post-body-paragraph lo lp iq lq b lr mm jr lt lu mn ju lw lx ne lz ma mb nf md me mf ng mh mi mj ij bi translated"><strong class="lq ir">我们如何获取线性组合的输入特征和参数，并进行二元预测？</strong>答案是自然对数(以e为底的对数)。更确切地说，对数概率。对线性组合的输入要素和参数求和，以生成对数优势形式的值。然后，对数优势值被插入到sigmoid函数中，并生成一个概率。就其核心而言，像许多其他机器学习问题一样，这是一个优化问题。使用梯度上升(例如，最大化对数似然)或下降(例如，最小化交叉熵损失)来估计最佳参数，其中选择的<a class="ae kv" href="https://www.courses.psu.edu/for/for466w_mem14/Ch11/HTML/Sec1/ch11sec1_ObjFn.htm" rel="noopener ugc nofollow" target="_blank">目标(例如，成本、损失等。)功能</a>决定梯度接近。</p><h1 id="f472" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">数学概念和函数</h1><p id="f4ea" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">为了让一切变得更简单，我们必须深入数学。在逻辑回归中，sigmoid函数起着关键作用，因为它输出一个介于0和1之间的值，非常适合概率。因此，这种表示通常被称为逻辑sigmoid函数。请记住，在自然环境中还有其他具有不同有界范围的<a class="ae kv" href="https://deepai.org/machine-learning-glossary-and-terms/sigmoid-function" rel="noopener ugc nofollow" target="_blank"> sigmoid函数</a>。并且因为响应是二元的(例如，真对假，是对否，幸存对未幸存)，所以响应变量将具有伯努利分布。此外，每个响应结果由预测的成功概率决定，如图5所示。</p><blockquote class="nh ni nj"><p id="904e" class="lo lp nk lq b lr mm jr lt lu mn ju lw nl ne lz ma nm nf md me nn ng mh mi mj ij bi translated"><strong class="lq ir">关于数学符号:</strong>小写的“I<em class="iq">”</em>将表示数据集中的行位置，而小写的“j<em class="iq">”</em>将表示数据集中的特征或列位置。你还会碰到小写粗体非斜体的“<strong class="lq ir"> <em class="iq"> x </em> </strong>”。这代表一个特征向量。更具体地说，当“I”伴随有“<strong class="lq ir"><em class="iq">x</em></strong>”(<strong class="lq ir"><em class="iq">x</em></strong>I)时，如图5、6、7、9所示，这代表一个向量(一个实例/单行)具有所有特征值。</p><p id="c602" class="lo lp nk lq b lr mm jr lt lu mn ju lw nl ne lz ma nm nf md me nn ng mh mi mj ij bi translated">当您在图8和图10中看到小写斜体“x”(Xi，j)的“I”和“j”时，该值是第I个(单个特征向量)实例中第j个特征的表示。数据集中要素(列)的数量将表示为“n ”,而实例(行)的数量将表示为“m”变量。</p></blockquote></div><div class="ab cl no np hu nq" role="separator"><span class="nr bw bk ns nt nu"/><span class="nr bw bk ns nt nu"/><span class="nr bw bk ns nt"/></div><div class="ij ik il im in"><h2 id="93d5" class="nv kx iq bd ky nw nx dn lc ny nz dp lg lx oa ob li mb oc od lk mf oe of lm og bi translated">Sigmoid函数</h2><p id="ce9e" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">在图1中，第一个方程是sigmoid函数，它创建了我们经常在逻辑回归中看到的S曲线。给定<strong class="lq ir"> x </strong>，输出等于条件概率<em class="nk"> y </em> = 1，由𝜃.参数化在这种情况下，<strong class="lq ir"> x </strong>是表示为特征向量的单个实例(训练集中的观察值)。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oh"><img src="../Images/50e3a00755737a1ea5213f6ec338bef2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*29l5zQ4dgRxS822_nksnuA.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图1: Sigmoid函数[sigma(z)]和[1—sigma(z)]；图片作者<a class="ae kv" href="https://dolee.medium.com" rel="noopener">作者</a></p></figure><p id="ca99" class="pw-post-body-paragraph lo lp iq lq b lr mm jr lt lu mn ju lw lx ne lz ma mb nf md me mf ng mh mi mj ij bi translated">向量中的每个特征将具有使用优化算法估计的相应𝜃参数。稍后我会在梯度上升/下降部分详细讨论这一点。还需要注意的是，通过求解log(odds)= log(<em class="nk">p</em>/(1-<em class="nk">p</em>))中的<em class="nk"> p </em>，我们得到了具有<em class="nk"> z = </em> log(odds)的sigmoid函数。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oi"><img src="../Images/fce94c33f05e1d03bbf29eeea04364ae.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JmuotVGWJCMR54f0Xh04GQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图2:s形曲线；图片作者<a class="ae kv" href="https://dolee.medium.com" rel="noopener">作者</a></p></figure><pre class="kg kh ki kj gt oj ok ol om aw on bi"><span id="00eb" class="nv kx iq ok b gy oo op l oq or"><br/>proba1, proba0 = [], []<br/>feature_example = [i for i in range(-10, 11)]</span><span id="7d2c" class="nv kx iq ok b gy os op l oq or">for instance in feature_example:<br/>    p1 = 1/(1 + np.exp(-instance)) # sigmoid function<br/>    p0 = 1 — p1<br/>    proba1.append(p1)<br/>    proba0.append(p0)</span><span id="b355" class="nv kx iq ok b gy os op l oq or">plt.figure(figsize=(12,6))<br/>plt.plot(feature_example, proba1, marker=’.’, label=’predict proba “1”’)<br/>plt.plot(feature_example, proba0, marker=’.’, linestyle=’dashed’, label=’predict proba “0”’)<br/>plt.title(‘Sigmoid Curve’)<br/>plt.xlabel(‘feature_example (in log-odds)’)<br/>plt.ylabel(‘probability’)<br/>plt.legend(prop={‘size’: 12});</span></pre></div><div class="ab cl no np hu nq" role="separator"><span class="nr bw bk ns nt nu"/><span class="nr bw bk ns nt nu"/><span class="nr bw bk ns nt"/></div><div class="ij ik il im in"><h2 id="e5ae" class="nv kx iq bd ky nw nx dn lc ny nz dp lg lx oa ob li mb oc od lk mf oe of lm og bi translated">赔率和对数赔率</h2><p id="1334" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">使用线性回归函数估计的y值(y-hat)代表对数优势。围绕比值或<a class="ae kv" href="https://stats.idre.ucla.edu/other/mult-pkg/faq/general/faq-how-do-i-interpret-odds-ratios-in-logistic-regression/" rel="noopener ugc nofollow" target="_blank">比值比</a>包装对数的过程被称为<strong class="lq ir"> logit变换</strong>。关键是对数几率是无限的(-无穷大到+无穷大)。但是，我们需要一个介于0和1之间的值来预测概率。所以，本质上，对数概率是弥合线性和概率形式之间差距的桥梁。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ot"><img src="../Images/f0f20ae15794c657cf1688b3a4dc0192.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8DLdPzg7mv3eTXdzBq767g.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图3:线性回归函数和y估计值的对数比；图片作者<a class="ae kv" href="https://dolee.medium.com" rel="noopener">作者</a></p></figure><p id="e859" class="pw-post-body-paragraph lo lp iq lq b lr mm jr lt lu mn ju lw lx ne lz ma mb nf md me mf ng mh mi mj ij bi translated">什么是对数概率？我们首先要知道赔率的定义——成功的概率除以失败的概率，P(成功)/P(失败)。例如，对于一枚普通硬币，正面和反面的概率都是0.5。因此，几率是0.5/0.5，这意味着得到反面的几率是1。如果我们使用一个偏向反面的硬币，其中反面的概率现在是0.7，那么得到反面的几率是2.33 (0.7/0.3)。如图3所示，赔率等于p/(1-p)。下一步是将赔率转换成对数赔率。</p><p id="8400" class="pw-post-body-paragraph lo lp iq lq b lr mm jr lt lu mn ju lw lx ne lz ma mb nf md me mf ng mh mi mj ij bi translated">将概率转换为比值以及将比值转换为对数比值的一个基本要点是，这些关系是单调的。当概率增加时，几率增加，反之亦然。当赔率增加时，对数赔率也增加，反之亦然。在图4中，我使用Titanic训练集和Scikit-Learn的逻辑回归函数创建了两个图来说明这一点。我们可以清楚地看到概率、赔率和对数赔率之间的单调关系。</p><p id="f834" class="pw-post-body-paragraph lo lp iq lq b lr mm jr lt lu mn ju lw lx ne lz ma mb nf md me mf ng mh mi mj ij bi translated">为什么这很重要？log-odds值越高，概率越高。在图2中，我们可以非常清楚地看到这一点。任何等于或大于0的对数优势值都有0.5或更高的概率。这是由于我们在图4中观察到的单调关系。当我们解释估计的参数时，这也会派上用场。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ou"><img src="../Images/3cff303bff474d509c93c4899382f61e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4t7IOaHu8QJWdm6XVNDa5A.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图4:概率vs .赔率&amp;赔率vs .对数赔率；图片由<a class="ae kv" href="https://dolee.medium.com" rel="noopener">作者</a></p></figure><pre class="kg kh ki kj gt oj ok ol om aw on bi"><span id="a654" class="nv kx iq ok b gy oo op l oq or">import pandas as pd<br/>import numpy as np<br/>from sklearn.linear_model import LogisticRegression<br/>from sklearn.preprocessing import StandardScaler<br/>from sklearn.pipeline import make_pipeline<br/>import matplotlib.pyplot as plt<br/>plt.style.use(‘ggplot’)<br/>%matplotlib inline</span><span id="4016" class="nv kx iq ok b gy os op l oq or">logreg = LogisticRegression(random_state=0)<br/>model_pipe = make_pipeline(StandardScaler(), logreg)<br/>X = train[[‘age’,’pclass’,’sex’]]<br/>y = train[‘survived’]<br/>model_pipe.fit(X, y)<br/>model_pipe.predict_proba(X)</span><span id="afcd" class="nv kx iq ok b gy os op l oq or">y_pred_proba_1 = model_pipe.predict_proba(X)[:,1]<br/>y_pred_proba_0 = model_pipe.predict_proba(X)[:,0]<br/>odds = y_pred_proba_1 / y_neg_pred_proba_0<br/>log_odds = np.log(odds)</span><span id="491e" class="nv kx iq ok b gy os op l oq or">fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16,6))<br/>ax1.set_title(‘Probability vs. Odds’)<br/>ax1.set(xlabel=’Probability’, ylabel=’Odds’)<br/>ax1.scatter(y_pos_pred_proba, odds)<br/>ax2.set_title(‘Odds vs. Log Odds’)<br/>ax2.set(xlabel=’Odds’, ylabel=’Log Odds’)<br/>ax2.scatter(odds, log_odds);</span></pre></div><div class="ab cl no np hu nq" role="separator"><span class="nr bw bk ns nt nu"/><span class="nr bw bk ns nt nu"/><span class="nr bw bk ns nt"/></div><div class="ij ik il im in"><h2 id="4b9b" class="nv kx iq bd ky nw nx dn lc ny nz dp lg lx oa ob li mb oc od lk mf oe of lm og bi translated">Sigmoid组合概率、似然和对数似然</h2><p id="5406" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">我们覆盖了很多领域，现在我们处于理解高层次逻辑回归的最后一英里。图5中的概率函数P(Y=yi|X=xi)捕捉到了Y=1和Y=0的形式。基于Y (0或1)，点积中的一项变为1并下降。这种组合形式对理解可能性至关重要。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ov"><img src="../Images/e35094d237807411622e3ffe69602fc8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-LMD2Xb-HsiN00AnAA6pFw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图5:概率函数作为Y=0和Y=1项的组合形式；图片由<a class="ae kv" href="https://dolee.medium.com" rel="noopener">作者</a></p></figure><p id="6ffb" class="pw-post-body-paragraph lo lp iq lq b lr mm jr lt lu mn ju lw lx ne lz ma mb nf md me mf ng mh mi mj ij bi translated"><strong class="lq ir">让我们来看看我们是如何得到</strong> <a class="ae kv" href="https://en.wikipedia.org/wiki/Likelihood_function" rel="noopener ugc nofollow" target="_blank"> <strong class="lq ir">可能性</strong> </a> <strong class="lq ir">、L(𝜃).的</strong>对于训练集中的每个实例，我们使用<strong class="lq ir">随机估计参数(𝜃's) </strong>计算对数优势，并使用对应于特定二元目标变量(0或1)的sigmoid函数预测概率。这些概率的乘积将给出所有实例的概率和<a class="ae kv" href="https://stats.stackexchange.com/questions/2641/what-is-the-difference-between-likelihood-and-probability" rel="noopener ugc nofollow" target="_blank">可能性</a>，如图6所示。重要的是要注意，可能性被表示为𝜃的可能性，而概率被指定为y的概率</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ow"><img src="../Images/4904069457ac2f5cd0b17482724a90e4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4bSwPlxsYIMMBotSRldCIg.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图6:似然函数；来源阅读:<a class="ae kv" href="https://stanford.edu/~cpiech/bio/index.html" rel="noopener ugc nofollow" target="_blank">克里斯皮赫</a>，CS109 @斯坦福大学；图片由<a class="ae kv" href="https://dolee.medium.com" rel="noopener">作者</a></p></figure><p id="46c0" class="pw-post-body-paragraph lo lp iq lq b lr mm jr lt lu mn ju lw lx ne lz ma mb nf md me mf ng mh mi mj ij bi translated">结果，通过最大化可能性，我们收敛到最优参数。换句话说，最大化似然估计最佳参数，我们直接最大化y的概率，这就是所谓的最大似然估计(MLE)。因此，当达到最大值时，初始参数值将逐渐收敛到最优值。收敛是由优化算法——梯度上升/下降驱动的。</p><p id="087d" class="pw-post-body-paragraph lo lp iq lq b lr mm jr lt lu mn ju lw lx ne lz ma mb nf md me mf ng mh mi mj ij bi translated">对数似然法如何适应这种情况？取似然函数的对数，就变成了求和问题对乘法问题。我们知道log(XY) = log(X) + log(Y)和log(X^b) = b * log(X)。因此，我们可以轻松地将似然性(L(𝜃)转换为对数似然性(LL(𝜃)，如图7所示。因为似然性到对数似然性是一种单调变换，最大化对数似然性也将产生最佳参数，这被称为最大对数似然性。请记住，这是我们正在优化的𝜃的对数似然。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ox"><img src="../Images/05471e8b723dc7ee75b5f539845c7a39.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RhLZTyQuxP9CAv1cfI1N0g.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图7:对数似然函数；来源阅读:<a class="ae kv" href="https://stanford.edu/~cpiech/bio/index.html" rel="noopener ugc nofollow" target="_blank">克里斯皮赫</a>，CS109 @斯坦福大学；图片由<a class="ae kv" href="https://dolee.medium.com" rel="noopener">作者</a></p></figure><p id="b1a0" class="pw-post-body-paragraph lo lp iq lq b lr mm jr lt lu mn ju lw lx ne lz ma mb nf md me mf ng mh mi mj ij bi translated">如何使用对数似然达到最大值？我们对每个𝜃参数取对数似然函数的偏导数。换句话说，你为每个参数取梯度，它有大小和方向。例如，在Titanic训练集中，我们有三个特征加上一个偏差项，所有实例的x0都等于1。图8中的偏导数表示训练集中的单个实例(<em class="nk"> i) </em>和单个𝜃参数(<em class="nk"> j) </em>。<em class="nk"> x (i，j) </em>表示实例中与其对应的𝜃 ( <em class="nk"> i，j </em>)参数配对的单个特征。因此，对于单个实例，总共创建了四个偏导数——偏差项、pclass、性别和年龄。这些组成了梯度向量。</p><p id="e153" class="pw-post-body-paragraph lo lp iq lq b lr mm jr lt lu mn ju lw lx ne lz ma mb nf md me mf ng mh mi mj ij bi translated">η是学习率，它决定了梯度上升算法每次迭代的步长。我们不希望学习率太低，需要很长时间才能收敛，也不希望学习率太高，会超调，会跳来跳去。学习率也是一个可以优化的超参数，但是我将在Titanic练习中使用固定的学习率0.1。</p><p id="68db" class="pw-post-body-paragraph lo lp iq lq b lr mm jr lt lu mn ju lw lx ne lz ma mb nf md me mf ng mh mi mj ij bi translated">一旦有了梯度向量和学习率，两个实体相乘并添加到当前要更新的参数中，如图8中的第二个等式所示。本质上，我们是在梯度方向上一小步一小步地缓慢而坚定地到达顶峰。这个更新步骤重复进行，直到参数收敛到它们的最优值——这是<strong class="lq ir">梯度上升算法</strong>在起作用。因为对数似然函数是凹的，最终，小的上坡步骤将达到全局最大值。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oy"><img src="../Images/2ed164512a417d9ef2a01831b27993a2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Fz3EvHHDhoFBNracwpGU4A.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图8:每个𝜃参数的偏导数和梯度上升算法；图片作者<a class="ae kv" href="https://dolee.medium.com" rel="noopener">作者</a></p></figure><p id="4698" class="pw-post-body-paragraph lo lp iq lq b lr mm jr lt lu mn ju lw lx ne lz ma mb nf md me mf ng mh mi mj ij bi translated"><strong class="lq ir">最小化成本函数怎么样？</strong>我们经常听到我们需要最小化成本或损失函数。它也被称为目标函数，因为我们试图最大化或最小化某个数值。在成本或损失函数的上下文中，目标收敛到全局最小值。例如，通过在对数似然函数前面放置一个负号，如图9所示，它就变成了交叉熵损失函数。目标是使用<a class="ae kv" href="https://www.khanacademy.org/math/multivariable-calculus/applications-of-multivariable-derivatives/optimizing-multivariable-functions/a/what-is-gradient-descent#:~:text=Gradient%20descent%20is%20an%20algorithm,like%20we've%20seen%20before." rel="noopener ugc nofollow" target="_blank"> <strong class="lq ir">梯度下降算法</strong> </a> <strong class="lq ir"> </strong>(图10中的第二个方程)最小化这个负函数。这个过程与最大化对数似然相同，只是我们通过下降到最小值来最小化它。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oz"><img src="../Images/36ca01be5e8a70e09597cffeecf007b2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HXnTOP78wtCp_odxrEVZrA.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图9:交叉熵损失函数或负对数似然(NLL)函数；来源阅读:<a class="ae kv" href="https://www.amazon.com/Machine-Learning-Probabilistic-Perspective-Computation/dp/0262018020" rel="noopener ugc nofollow" target="_blank">机器学习:概率视角凯文·p·墨菲</a> —第8章；图片由<a class="ae kv" href="https://dolee.medium.com" rel="noopener">作者</a></p></figure><p id="5de3" class="pw-post-body-paragraph lo lp iq lq b lr mm jr lt lu mn ju lw lx ne lz ma mb nf md me mf ng mh mi mj ij bi translated">一旦每个𝜃参数的偏导数(图10)被导出，形式与图8相同。最大的区别是减法项，它用sigmoid预测概率减去实际<em class="nk"> y </em> (0或1)重新排序。该项然后乘以<em class="nk"> x (i，j) </em>特征。该过程与上面的梯度上升部分中描述的过程相同。因此，我们将得到训练集中每个实例的四个偏导数。并且使用<strong class="lq ir"> </strong>梯度下降算法，我们更新参数直到它们收敛到它们的最优值。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi pa"><img src="../Images/0a460c65b946e47216f685b5ba336ae9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RWZOtNOVqthnR0dayocOFw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图10:每个θ参数的偏导数和梯度下降算法；来源阅读:<a class="ae kv" href="https://web.stanford.edu/~jurafsky/slp3/" rel="noopener ugc nofollow" target="_blank">丹·茹拉夫斯基、詹姆斯·h·马丁著《言语与语言过程》(第三版草稿)</a> —第五章；图片由<a class="ae kv" href="https://dolee.medium.com" rel="noopener">作者</a></p></figure><p id="a59b" class="pw-post-body-paragraph lo lp iq lq b lr mm jr lt lu mn ju lw lx ne lz ma mb nf md me mf ng mh mi mj ij bi translated">幸运的是，交叉熵损失函数是凸的，自然有一个全局最小值。最终，在坡度方向(最陡的下坡方向)迈出足够小的步伐后，它将到达山脚。</p><h1 id="e7ee" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">行动中的代码</h1><p id="8c8c" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">既然我们已经回顾了相关的数学知识，那么现在只需要用代码来展示逻辑回归和梯度算法的能力。先说我们的数据。我们有来自卡格尔的泰坦尼克号挑战的训练和测试设备。如前所述，我只使用三个特征——年龄、阶级和性别——来预测乘客存活率。</p><p id="ea4a" class="pw-post-body-paragraph lo lp iq lq b lr mm jr lt lu mn ju lw lx ne lz ma mb nf md me mf ng mh mi mj ij bi translated"><strong class="lq ir">首先，我们需要扩展特性</strong>，这将有助于融合过程。我将使用标准化方法来缩放数字要素。在标准化中，我们取每个数字特征的平均值，并从每个值中减去平均值。然后将该项除以特征的标准偏差。接下来，我们将添加一个全1的列来表示x0。这是偏向项。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="pb pc l"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">特征标准化代码由<a class="ae kv" href="https://dolee.medium.com" rel="noopener">作者</a></p></figure><p id="dc5e" class="pw-post-body-paragraph lo lp iq lq b lr mm jr lt lu mn ju lw lx ne lz ma mb nf md me mf ng mh mi mj ij bi translated">使用上面的代码，我们已经准备好了训练输入数据集。我们现在知道log-odds是线性回归函数的输出，这个输出是sigmoid函数的输入。唯一缺失的是𝜃参数。因为我们将使用梯度上升和下降来估计这些参数，我们<strong class="lq ir">选择四个任意值</strong>作为我们的起点。我将用四个零作为初始值。我们还需要在代码中定义sigmoid函数，因为这将生成我们的概率。</p><pre class="kg kh ki kj gt oj ok ol om aw on bi"><span id="f06f" class="nv kx iq ok b gy oo op l oq or"># sigmoid function where z = log-odds<br/>def sigmoid(z):<br/>    predict_proba = 1 / (1 + np.exp(-z))<br/>    return predict_proba</span></pre><p id="33db" class="pw-post-body-paragraph lo lp iq lq b lr mm jr lt lu mn ju lw lx ne lz ma mb nf md me mf ng mh mi mj ij bi translated">我们已经准备好了所有的东西。<strong class="lq ir">接下来，我们将把对数似然函数、交叉熵损失函数和梯度转换成代码。</strong>我们还需要确定我们要对训练集进行多少次检查。我们需要定义历元的数量(在下面的代码中指定为n_epoch，这是一个帮助学习过程的超参数)。<strong class="lq ir">什么是纪元？</strong>在梯度上升/下降算法的背景下，历元是单次迭代，其中它确定多少训练实例将通过梯度算法来更新𝜃参数(如图8和10所示)。</p><p id="b10b" class="pw-post-body-paragraph lo lp iq lq b lr mm jr lt lu mn ju lw lx ne lz ma mb nf md me mf ng mh mi mj ij bi translated">因此，我们通常会遇到<strong class="lq ir">三种梯度上升/下降算法</strong>:批量、随机和小批量。对于泰坦尼克号练习，我将使用批处理方法。这意味着，对于每个时期，整个训练集将通过梯度算法来更新𝜃参数。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="pb pc l"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">梯度上升代码由<a class="ae kv" href="https://dolee.medium.com" rel="noopener">作者</a></p></figure><p id="2f38" class="pw-post-body-paragraph lo lp iq lq b lr mm jr lt lu mn ju lw lx ne lz ma mb nf md me mf ng mh mi mj ij bi translated">给你！通过<strong class="lq ir">利用梯度上升算法</strong>最大化对数似然，我们已经为泰坦尼克号训练集导出了预测乘客存活率的最佳𝜃参数。</p><ul class=""><li id="6540" class="mk ml iq lq b lr mm lu mn lx mo mb mp mf mq mj mr ms mt mu bi translated">𝜃0: -0.89362715(偏差项)</li><li id="7679" class="mk ml iq lq b lr mv lu mw lx mx mb my mf mz mj mr ms mt mu bi translated">𝜃1: 1.41685703(pclass)</li><li id="1405" class="mk ml iq lq b lr mv lu mw lx mx mb my mf mz mj mr ms mt mu bi translated">𝜃2: 1.24119596(性别)</li><li id="2394" class="mk ml iq lq b lr mv lu mw lx mx mb my mf mz mj mr ms mt mu bi translated">𝜃3: -0.60707722(年龄)</li></ul><p id="eaee" class="pw-post-body-paragraph lo lp iq lq b lr mm jr lt lu mn ju lw lx ne lz ma mb nf md me mf ng mh mi mj ij bi translated">让我们想象一下最大化的过程。在图11中，我们可以看到，在第一个时期后，我们达到了最大值，并继续保持在这个水平。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi pd"><img src="../Images/98a02c05628eeeed630eee0fcbfa8a48.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tTGA9E3CUyv7BZjuOdXY3A.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图11:最大化每个历元迭代的对数似然函数；图片作者<a class="ae kv" href="https://dolee.medium.com" rel="noopener">作者</a></p></figure><pre class="kg kh ki kj gt oj ok ol om aw on bi"><span id="e67c" class="nv kx iq ok b gy oo op l oq or">x_axis = [i for i in range(1, 11)]<br/>plt.figure(figsize=(14,6))<br/>plt.title(‘Maximizing Log-Likelihood’)<br/>plt.xticks(x_axis)<br/>plt.xlabel(‘epoch iteration’)<br/>plt.ylabel(‘log-likelihood’)<br/>plt.plot(x_axis, log_likelihood_vals, marker=’o’)<br/>plt.tight_layout()</span></pre><p id="e187" class="pw-post-body-paragraph lo lp iq lq b lr mm jr lt lu mn ju lw lx ne lz ma mb nf md me mf ng mh mi mj ij bi translated">我们还可以看到每次历元迭代的𝜃参数收敛。正如我们在图11中看到的，对数似然在第一个时期后达到最大值；我们应该看到同样的参数。在图12中，我们看到参数在第一个时期后收敛到它们的最佳水平，并且最佳水平随着代码在剩余时期的迭代而保持。迭代训练集一次就足以达到最佳𝜃参数。让我们检查一下在每个时期间隔期间发生了什么。</p><p id="d102" class="pw-post-body-paragraph lo lp iq lq b lr mm jr lt lu mn ju lw lx ne lz ma mb nf md me mf ng mh mi mj ij bi translated">图12右侧的曲线显示了𝜃参数值快速向其最佳值移动。当它继续迭代通过每个时期中的训练实例时，参数值上下振荡(时期间隔用黑色垂直虚线表示)。在每个时期结束时，我们以最佳参数值结束，并且保持这些值。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi pe"><img src="../Images/0fd64cda29f86753af577633ca115b94.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wXDEmunmvMaw4r_doCUj2g.png"/></div></div></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi pf"><img src="../Images/d6a5365bd182e284f90e6989707adb6e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fikG8lv11Svgp_-864VLkg.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图12:每个时期(左)和每个实例(右)的𝜃参数收敛；图片由<a class="ae kv" href="https://dolee.medium.com" rel="noopener">作者</a></p></figure><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="pb pc l"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图12 —由<a class="ae kv" href="https://dolee.medium.com" rel="noopener">作者</a>编写的代码</p></figure><p id="4a8e" class="pw-post-body-paragraph lo lp iq lq b lr mm jr lt lu mn ju lw lx ne lz ma mb nf md me mf ng mh mi mj ij bi translated">是时候使用该模型进行预测，并生成准确度分数来衡量模型性能了。有几个衡量性能的指标，但现在我们将快速了解一下准确性。下面的代码产生了79.8%的准确率。</p><pre class="kg kh ki kj gt oj ok ol om aw on bi"><span id="875d" class="nv kx iq ok b gy oo op l oq or">from sklearn.metrics import accuracy_score</span><span id="ad9d" class="nv kx iq ok b gy os op l oq or">def make_predictions(X, thetas):<br/>    X = X.copy()<br/>    predictions = []<br/>    for x_row in X:<br/>        log_odds = sum(thetas * x_row[:4])<br/>        pred_proba = sigmoid(log_odds)<br/>        <br/>        # if probability &gt;= 0.5, predicted class is 1<br/>        predictions.append(1 if pred_proba &gt;= 0.5 else 0)<br/>    return predictions</span><span id="2841" class="nv kx iq ok b gy os op l oq or">y_train = X_train[:, -1]<br/>y_predict = make_predictions(X_train[:, :4], thetas)<br/>accuracy_score(y_train, y_predict)</span></pre><p id="2556" class="pw-post-body-paragraph lo lp iq lq b lr mm jr lt lu mn ju lw lx ne lz ma mb nf md me mf ng mh mi mj ij bi translated"><strong class="lq ir">交叉熵损失函数呢？</strong>最小化交叉熵损失函数的结果将与上面相同。只有几行代码更改，然后代码就可以运行了(参见下面代码中的' # changed ')。这是代码。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="pb pc l"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">梯度下降代码作者<a class="ae kv" href="https://dolee.medium.com" rel="noopener">作者</a></p></figure><p id="03be" class="pw-post-body-paragraph lo lp iq lq b lr mm jr lt lu mn ju lw lx ne lz ma mb nf md me mf ng mh mi mj ij bi translated">让我们看看使用梯度下降最小化交叉熵损失函数。我们在第一个时期后达到最小值，正如我们观察到的最大对数似然。最大的不同是，我们正朝着最陡下降的方向前进。这就是我们经常读到和听到的——最小化成本函数来估计最佳参数。我们现在已经具备了从头构建二元逻辑回归模型的所有组件。我希望这篇文章能帮助你，就像我帮助我加深对逻辑回归和梯度算法的理解一样。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi pg"><img src="../Images/23b095fbfd02f3fb57894d39f610c4bc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mRlT-38kRXy436Q7d_67SQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图13:最小化交叉熵损失函数；图片由<a class="ae kv" href="https://dolee.medium.com" rel="noopener">作者</a></p></figure><pre class="kg kh ki kj gt oj ok ol om aw on bi"><span id="dca8" class="nv kx iq ok b gy oo op l oq or">x_axis = [i for i in range(1, 11)]<br/>plt.figure(figsize=(14,6))<br/>plt.title(‘Minimizing Cross-Entropy Loss’)<br/>plt.xticks(x_axis)<br/>plt.xlabel(‘epoch iteration’)<br/>plt.ylabel(‘cross-entropy loss’)<br/>plt.plot(x_axis, ce_loss_vals, marker=’o’)<br/>plt.tight_layout()</span></pre><h1 id="b726" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">最后的想法</h1><p id="db3b" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">在这篇文章中，我的目标是提供一个坚实的二元逻辑回归模型和估计最佳𝜃参数的两种方法的介绍性概述。正如我们在泰坦尼克号的例子中看到的，主要的障碍是估计最佳的𝜃参数来拟合模型，并使用这些估计来预测乘客的存活率。我们使用梯度上升算法检验了(最大)对数似然函数。我们还使用梯度下降算法检查了交叉熵损失函数。当然，您可以将其他成本函数应用到这个问题上，但是我们已经讨论了足够多的内容，以了解我们试图通过梯度上升/下降实现的目标。</p><p id="46b5" class="pw-post-body-paragraph lo lp iq lq b lr mm jr lt lu mn ju lw lx ne lz ma mb nf md me mf ng mh mi mj ij bi translated">我们将何去何从？在改进模型方面，我们可以探索几个领域。我们可以从学习率开始。<a class="ae kv" href="https://en.wikipedia.org/wiki/Learning_rate" rel="noopener ugc nofollow" target="_blank">学习率</a>是一个超参数，可以调整。在许多情况下，随着梯度上升/下降算法向前发展，引入学习速率表来减小步长。</p><p id="4b75" class="pw-post-body-paragraph lo lp iq lq b lr mm jr lt lu mn ju lw lx ne lz ma mb nf md me mf ng mh mi mj ij bi translated">如果数据集很大，批处理方法可能并不理想。理解<a class="ae kv" href="https://machinelearningmastery.com/gentle-introduction-mini-batch-gradient-descent-configure-batch-size/" rel="noopener ugc nofollow" target="_blank">随机和小批量梯度下降</a>算法的机制会更有帮助。然而，一旦理解了批量梯度下降，其他方法就相当简单了。除了我在本文中使用的标准化方法之外，还有不同的<a class="ae kv" href="https://scikit-learn.org/stable/auto_examples/preprocessing/plot_all_scaling.html" rel="noopener ugc nofollow" target="_blank">特性扩展</a>技术。</p><p id="6741" class="pw-post-body-paragraph lo lp iq lq b lr mm jr lt lu mn ju lw lx ne lz ma mb nf md me mf ng mh mi mj ij bi translated">如果你遇到任何问题或有反馈给我，请随时留下评论。感谢阅读！</p></div></div>    
</body>
</html>