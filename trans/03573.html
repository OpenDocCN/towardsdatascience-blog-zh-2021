<html>
<head>
<title>Normal Equation in Python: The Closed-Form Solution for Linear Regression</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Python中的正规方程:线性回归的封闭解</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/normal-equation-in-python-the-closed-form-solution-for-linear-regression-13df33f9ad71?source=collection_archive---------4-----------------------#2021-03-23">https://towardsdatascience.com/normal-equation-in-python-the-closed-form-solution-for-linear-regression-13df33f9ad71?source=collection_archive---------4-----------------------#2021-03-23</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="22d7" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">从零开始的机器学习:第3部分</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/cf744eafee49aa3de5c15d249e5d56f9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*aBW0rQ5X848cLu05.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">来源:<a class="ae ky" href="https://www.google.com/url?sa=i&amp;url=https%3A%2F%2Fen.wikipedia.org%2Fwiki%2FLinear_least_squares&amp;psig=AOvVaw0PXUKbkpH3N-ZWQL5AFMd_&amp;ust=1616438242163000&amp;source=images&amp;cd=vfe&amp;ved=0CAYQjRxqFwoTCOiz9ai1xO8CFQAAAAAdAAAAABAI" rel="noopener ugc nofollow" target="_blank">维基百科</a></p></figure><p id="ff6e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在本文中，我们将实现<strong class="lb iu">正规方程</strong>，它是线性回归算法的封闭形式的解决方案，我们可以在一个步骤中找到<code class="fe lv lw lx ly b">theta</code>的最优值，而无需使用梯度下降算法。</p><p id="279f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们将首先从<strong class="lb iu">用梯度下降算法重述</strong>，然后讨论<strong class="lb iu">使用一个叫做正规方程</strong>的公式计算 <code class="fe lv lw lx ly b"><strong class="lb iu">theta</strong></code> <strong class="lb iu">，最后，看看<strong class="lb iu">正规方程在起作用</strong>和<strong class="lb iu">图</strong>对我们随机生成的数据的预测。</strong></p><p id="9dc5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">从零开始的机器学习系列—</p><ul class=""><li id="afaf" class="lz ma it lb b lc ld lf lg li mb lm mc lq md lu me mf mg mh bi translated">第1部分:<a class="ae ky" href="https://medium.com/analytics-vidhya/linear-regression-from-scratch-in-python-b6501f91c82d?source=your_stories_page-------------------------------------" rel="noopener">Python中从头开始的线性回归</a></li><li id="0ca2" class="lz ma it lb b lc mi lf mj li mk lm ml lq mm lu me mf mg mh bi translated">第2部分:<a class="ae ky" rel="noopener" target="_blank" href="/locally-weighted-linear-regression-in-python-3d324108efbf?source=your_stories_page-------------------------------------">Python中的局部加权线性回归</a></li><li id="1778" class="lz ma it lb b lc mi lf mj li mk lm ml lq mm lu me mf mg mh bi translated">第3部分:<a class="ae ky" rel="noopener" target="_blank" href="/normal-equation-in-python-the-closed-form-solution-for-linear-regression-13df33f9ad71?source=your_stories_page-------------------------------------">使用Python的正规方程:线性回归的封闭解</a></li><li id="f61e" class="lz ma it lb b lc mi lf mj li mk lm ml lq mm lu me mf mg mh bi translated">第4部分:<a class="ae ky" rel="noopener" target="_blank" href="/polynomial-regression-in-python-b69ab7df6105">Python中从头开始的多项式回归</a></li></ul><div class="mn mo gp gr mp mq"><a href="https://medium.com/analytics-vidhya/linear-regression-from-scratch-in-python-b6501f91c82d" rel="noopener follow" target="_blank"><div class="mr ab fo"><div class="ms ab mt cl cj mu"><h2 class="bd iu gy z fp mv fr fs mw fu fw is bi translated">Python中从头开始的线性回归</h2><div class="mx l"><h3 class="bd b gy z fp mv fr fs mw fu fw dk translated">从零开始的机器学习:第1部分</h3></div><div class="my l"><p class="bd b dl z fp mv fr fs mw fu fw dk translated">medium.com</p></div></div><div class="mz l"><div class="na l nb nc nd mz ne ks mq"/></div></div></a></div><div class="mn mo gp gr mp mq"><a rel="noopener follow" target="_blank" href="/locally-weighted-linear-regression-in-python-3d324108efbf"><div class="mr ab fo"><div class="ms ab mt cl cj mu"><h2 class="bd iu gy z fp mv fr fs mw fu fw is bi translated">Python中的局部加权线性回归</h2><div class="mx l"><h3 class="bd b gy z fp mv fr fs mw fu fw dk translated">从零开始的机器学习:第2部分</h3></div><div class="my l"><p class="bd b dl z fp mv fr fs mw fu fw dk translated">towardsdatascience.com</p></div></div><div class="mz l"><div class="nf l nb nc nd mz ne ks mq"/></div></div></a></div><h1 id="c98f" class="ng nh it bd ni nj nk nl nm nn no np nq jz nr ka ns kc nt kd nu kf nv kg nw nx bi translated">梯度下降概述</h1><p id="b43b" class="pw-post-body-paragraph kz la it lb b lc ny ju le lf nz jx lh li oa lk ll lm ob lo lp lq oc ls lt lu im bi translated">我们有，</p><ul class=""><li id="2cb1" class="lz ma it lb b lc ld lf lg li mb lm mc lq md lu me mf mg mh bi translated"><code class="fe lv lw lx ly b"><strong class="lb iu">X</strong></code>→输入数据(训练数据)</li><li id="e402" class="lz ma it lb b lc mi lf mj li mk lm ml lq mm lu me mf mg mh bi translated"><code class="fe lv lw lx ly b"><strong class="lb iu">y</strong></code>→目标变量</li><li id="4900" class="lz ma it lb b lc mi lf mj li mk lm ml lq mm lu me mf mg mh bi translated"><code class="fe lv lw lx ly b"><strong class="lb iu">theta</strong></code>→参数</li><li id="fe38" class="lz ma it lb b lc mi lf mj li mk lm ml lq mm lu me mf mg mh bi translated"><code class="fe lv lw lx ly b"><strong class="lb iu">y_hat</strong></code>→预测/假设(<code class="fe lv lw lx ly b">theta</code>和<code class="fe lv lw lx ly b">X</code>的点积)。</li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi od"><img src="../Images/eb35386cd178688df03c9a460615fbf0.png" data-original-src="https://miro.medium.com/v2/resize:fit:342/format:webp/0*tSdRT0VLauZNtuih.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">等式1:矢量化预测/假设；来源:geeksforgeeks</p></figure><ul class=""><li id="34ea" class="lz ma it lb b lc ld lf lg li mb lm mc lq md lu me mf mg mh bi translated"><strong class="lb iu">损失函数</strong>→均方误差损失或均方误差损失(<code class="fe lv lw lx ly b">y_hat</code> - <code class="fe lv lw lx ly b">y</code>)</li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oe"><img src="../Images/f4d56e5fbc17287493ba1970ad0adba1.png" data-original-src="https://miro.medium.com/v2/resize:fit:644/format:webp/0*aMED8zC2rM_b91fz.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">等式MSE损失函数；来源:geeksforgeeks</p></figure><ul class=""><li id="b51b" class="lz ma it lb b lc ld lf lg li mb lm mc lq md lu me mf mg mh bi translated"><code class="fe lv lw lx ly b"><strong class="lb iu">m</strong></code>→训练例子的数量。</li><li id="17af" class="lz ma it lb b lc mi lf mj li mk lm ml lq mm lu me mf mg mh bi translated"><code class="fe lv lw lx ly b"><strong class="lb iu">n</strong></code>→功能数量</li></ul></div><div class="ab cl of og hx oh" role="separator"><span class="oi bw bk oj ok ol"/><span class="oi bw bk oj ok ol"/><span class="oi bw bk oj ok"/></div><div class="im in io ip iq"><h1 id="6f8c" class="ng nh it bd ni nj om nl nm nn on np nq jz oo ka ns kc op kd nu kf oq kg nw nx bi translated"><strong class="ak">梯度下降算法</strong> —</h1><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi or"><img src="../Images/0a07ce2a1ce8adfd5acb7bcaa009f638.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ttf76dgp65hS9gW5KV3r5A.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">梯度下降流程图；作者图片</p></figure><p id="60da" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">首先，我们随机或全零初始化参数<code class="fe lv lw lx ly b">theta</code>。然后，</p><ol class=""><li id="7039" class="lz ma it lb b lc ld lf lg li mb lm mc lq md lu os mf mg mh bi translated">使用上面的等式1计算预测/假设<code class="fe lv lw lx ly b">y_hat</code>。</li><li id="ebf0" class="lz ma it lb b lc mi lf mj li mk lm ml lq mm lu os mf mg mh bi translated">然后使用预测/假设<code class="fe lv lw lx ly b">y_hat</code>来计算MSE损失，就像这样— ( <code class="fe lv lw lx ly b">y_hat</code> - <code class="fe lv lw lx ly b">y</code>)。</li><li id="97a1" class="lz ma it lb b lc mi lf mj li mk lm ml lq mm lu os mf mg mh bi translated">然后取MSE损失相对于参数<code class="fe lv lw lx ly b">theta</code>的偏导数(梯度)。</li><li id="7e6a" class="lz ma it lb b lc mi lf mj li mk lm ml lq mm lu os mf mg mh bi translated">最后用这个偏导数(梯度)来更新参数<code class="fe lv lw lx ly b">theta</code>像这样——<code class="fe lv lw lx ly b">theta</code>:=<code class="fe lv lw lx ly b">theta</code>——<code class="fe lv lw lx ly b">lr</code>*<code class="fe lv lw lx ly b">gradient</code>，其中<code class="fe lv lw lx ly b">lr</code>是学习率。</li><li id="24d5" class="lz ma it lb b lc mi lf mj li mk lm ml lq mm lu os mf mg mh bi translated">重复步骤1至4，直到参数<code class="fe lv lw lx ly b">theta</code>达到最佳值。</li></ol></div><div class="ab cl of og hx oh" role="separator"><span class="oi bw bk oj ok ol"/><span class="oi bw bk oj ok ol"/><span class="oi bw bk oj ok"/></div><div class="im in io ip iq"><h1 id="5b3e" class="ng nh it bd ni nj om nl nm nn on np nq jz oo ka ns kc op kd nu kf oq kg nw nx bi translated">正态方程</h1><p id="53d3" class="pw-post-body-paragraph kz la it lb b lc ny ju le lf nz jx lh li oa lk ll lm ob lo lp lq oc ls lt lu im bi translated">梯度下降是一种迭代算法，这意味着您需要采取多个步骤来达到全局最优(以找到最优参数)，但事实证明，对于线性回归的特殊情况，有一种方法可以求解参数<code class="fe lv lw lx ly b">theta</code>的最优值，只需一步跳到全局最优，而不需要使用迭代算法，这种算法称为正规方程。它只适用于线性回归，不适用于任何其他算法。</p><p id="52a0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">正规方程是线性回归算法的封闭形式的解，这意味着我们可以仅通过使用包括一些矩阵乘法和求逆的公式来获得最佳参数。</p><p id="3764" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为了计算<code class="fe lv lw lx ly b">theta</code>，我们采用MSE损失函数(等式2)相对于<code class="fe lv lw lx ly b">theta</code>的偏导数，并将其设置为零。然后，做一点线性代数，得到<code class="fe lv lw lx ly b">theta</code>的值。</p><p id="1526" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这是正常的方程式—</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ot"><img src="../Images/88b6ef654e29ba87ef7ae187ef7e82f0.png" data-original-src="https://miro.medium.com/v2/resize:fit:706/format:webp/1*dWAqC_uys6He4fPft5L5bg.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">正规方程；资料来源:吴恩达</p></figure><p id="93a0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果你知道矩阵导数以及矩阵的一些性质，你应该能够自己推导出正规方程。</p><p id="9124" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">供参考— <a class="ae ky" href="https://eli.thegreenplace.net/2014/derivation-of-the-normal-equation-for-linear-regression/" rel="noopener ugc nofollow" target="_blank">线性回归正态方程的推导</a></p><p id="f0af" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">你可能会想，如果<code class="fe lv lw lx ly b">X</code>是一个不可逆矩阵，这通常会发生在你有冗余特征的情况下，即你的特征是线性相关的，可能是因为你有相同的特征重复两次。你可以做的一件事是找出哪些特性是重复的并修复它们，或者你可以使用NumPy中的<code class="fe lv lw lx ly b">np.pinv</code>函数，它也会给你正确的答案。</p><h1 id="eeee" class="ng nh it bd ni nj nk nl nm nn no np nq jz nr ka ns kc nt kd nu kf nv kg nw nx bi translated">该算法</h1><ol class=""><li id="feec" class="lz ma it lb b lc ny lf nz li ou lm ov lq ow lu os mf mg mh bi translated"><strong class="lb iu">利用法线方程计算</strong> <code class="fe lv lw lx ly b"><strong class="lb iu">theta</strong></code> <strong class="lb iu">。</strong></li><li id="4ef5" class="lz ma it lb b lc mi lf mj li mk lm ml lq mm lu os mf mg mh bi translated"><strong class="lb iu">使用</strong> <code class="fe lv lw lx ly b"><strong class="lb iu">theta</strong></code> <strong class="lb iu">进行预测。</strong></li></ol><p id="bdf0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">检查<code class="fe lv lw lx ly b">X</code>和<code class="fe lv lw lx ly b">y</code>的形状，使方程式匹配。</p></div><div class="ab cl of og hx oh" role="separator"><span class="oi bw bk oj ok ol"/><span class="oi bw bk oj ok ol"/><span class="oi bw bk oj ok"/></div><div class="im in io ip iq"><h1 id="da26" class="ng nh it bd ni nj om nl nm nn on np nq jz oo ka ns kc op kd nu kf oq kg nw nx bi translated">正常方程在起作用</h1><p id="8c4a" class="pw-post-body-paragraph kz la it lb b lc ny ju le lf nz jx lh li oa lk ll lm ob lo lp lq oc ls lt lu im bi translated">让我们以下面随机生成的数据作为一个激励性的例子来理解法线方程。</p><pre class="kj kk kl km gt ox ly oy oz aw pa bi"><span id="7ced" class="pb nh it ly b gy pc pd l pe pf"><strong class="ly iu">import numpy as np</strong></span><span id="73fd" class="pb nh it ly b gy pg pd l pe pf"><strong class="ly iu">np.random.seed(42)<br/>X = np.random.randn(500,1)<br/>y = 2*X + 1 + 1.2*np.random.randn(500,1)</strong></span><span id="3e67" class="pb nh it ly b gy pg pd l pe pf"><strong class="ly iu">X.shape, y.shape</strong><br/><strong class="ly iu">&gt;&gt;</strong>((500, 1), (500,))</span></pre><p id="a654" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这里，<code class="fe lv lw lx ly b">n</code> =1意味着矩阵<code class="fe lv lw lx ly b">X</code>只有1列，而<code class="fe lv lw lx ly b">m</code> =500意味着<code class="fe lv lw lx ly b">X</code>有500行。<code class="fe lv lw lx ly b">X</code>是一个(500×1)矩阵，而<code class="fe lv lw lx ly b">y</code>是一个长度为500的向量。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ph"><img src="../Images/50be27bedf99ef4f84eb28cfcf47a8db.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5JjRyRgwHpEJjRdgrzleHw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><h1 id="7816" class="ng nh it bd ni nj nk nl nm nn no np nq jz nr ka ns kc nt kd nu kf nv kg nw nx bi translated">寻找Theta函数</h1><p id="98bf" class="pw-post-body-paragraph kz la it lb b lc ny ju le lf nz jx lh li oa lk ll lm ob lo lp lq oc ls lt lu im bi translated">让我们用正规方程写代码来计算θ。</p><p id="a1d6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">请参见注释(#)。</p><pre class="kj kk kl km gt ox ly oy oz aw pa bi"><span id="d1b6" class="pb nh it ly b gy pc pd l pe pf"><strong class="ly iu">def find_theta(X, y):</strong><br/>    <br/>    <strong class="ly iu">m = X.shape[0]</strong> # Number of training examples. </span><span id="fcf9" class="pb nh it ly b gy pg pd l pe pf">    # Appending a cloumn of ones in X to add the bias term.<br/>    <strong class="ly iu">X = np.append(X, np.ones((m,1)), axis=1)   </strong> </span><span id="be62" class="pb nh it ly b gy pg pd l pe pf">    # reshaping y to (m,1)<br/><strong class="ly iu">    y = y.reshape(m,1)<br/></strong>    <br/>    # The Normal Equation<br/><strong class="ly iu">    theta = np.dot(np.linalg.inv(np.dot(X.T, X)), np.dot(X.T, y))<br/></strong>    <br/><strong class="ly iu">    return theta</strong></span></pre><h1 id="0451" class="ng nh it bd ni nj nk nl nm nn no np nq jz nr ka ns kc nt kd nu kf nv kg nw nx bi translated">预测功能</h1><p id="c80e" class="pw-post-body-paragraph kz la it lb b lc ny ju le lf nz jx lh li oa lk ll lm ob lo lp lq oc ls lt lu im bi translated">请参见注释(#)。</p><pre class="kj kk kl km gt ox ly oy oz aw pa bi"><span id="2814" class="pb nh it ly b gy pc pd l pe pf"><strong class="ly iu">def predict(X):</strong><br/>    <br/>    # Appending a cloumn of ones in X to add the bias term.<br/><strong class="ly iu">    X = np.append(X, np.ones((X.shape[0],1)), axis=1)<br/></strong>    <br/>    # preds is y_hat which is the dot product of X and theta.<br/><strong class="ly iu">    preds = np.dot(X, theta)<br/></strong>    <br/><strong class="ly iu">    return preds</strong></span></pre><h1 id="20f1" class="ng nh it bd ni nj nk nl nm nn no np nq jz nr ka ns kc nt kd nu kf nv kg nw nx bi translated">绘制预测</h1><p id="39c3" class="pw-post-body-paragraph kz la it lb b lc ny ju le lf nz jx lh li oa lk ll lm ob lo lp lq oc ls lt lu im bi translated">请参见注释(#)。</p><pre class="kj kk kl km gt ox ly oy oz aw pa bi"><span id="3940" class="pb nh it ly b gy pc pd l pe pf"># Getting the Value of theta using the find_theta function.<br/><strong class="ly iu">theta = find_theta(X, y)</strong></span><span id="621c" class="pb nh it ly b gy pg pd l pe pf"><strong class="ly iu">theta<br/>&gt;&gt;</strong>array([[1.90949642],<br/>        [1.0388102 ]]</span><span id="6835" class="pb nh it ly b gy pg pd l pe pf"># Getting the predictions on X using the predict function.<br/><strong class="ly iu">preds = predict(X)</strong></span><span id="eded" class="pb nh it ly b gy pg pd l pe pf"># Plotting the predictions.<br/><strong class="ly iu">fig = plt.figure(figsize=(8,6))<br/>plt.plot(X, y, 'b.')<br/>plt.plot(X, preds, 'c-')<br/>plt.xlabel('X - Input')<br/>plt.ylabel('y - target / true')</strong></span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pi"><img src="../Images/24a493088df91f7af50565bf4ddb6320.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jlG8MNDOHgBmuffFViFz9Q.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">直线(线性)拟合数据；作者图片</p></figure><p id="350f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">青色线显示所有<code class="fe lv lw lx ly b">X</code>值的预测。</p><p id="66e2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们只用一步就找到了<code class="fe lv lw lx ly b">theta</code>的最优值，我们找到的<code class="fe lv lw lx ly b">theta</code>是给定数据的MSE损失函数的全局最小值。</p></div><div class="ab cl of og hx oh" role="separator"><span class="oi bw bk oj ok ol"/><span class="oi bw bk oj ok ol"/><span class="oi bw bk oj ok"/></div><div class="im in io ip iq"><h1 id="8621" class="ng nh it bd ni nj om nl nm nn on np nq jz oo ka ns kc op kd nu kf oq kg nw nx bi translated"><strong class="ak">什么时候用正规方程？</strong></h1><p id="97ed" class="pw-post-body-paragraph kz la it lb b lc ny ju le lf nz jx lh li oa lk ll lm ob lo lp lq oc ls lt lu im bi translated">如果你想用的算法是线性回归，而且是完全线性回归，</p><ul class=""><li id="0a39" class="lz ma it lb b lc ld lf lg li mb lm mc lq md lu me mf mg mh bi translated">如果<code class="fe lv lw lx ly b">n</code>(特征数量)较小。</li><li id="e512" class="lz ma it lb b lc mi lf mj li mk lm ml lq mm lu me mf mg mh bi translated">如果<code class="fe lv lw lx ly b">m</code>(训练样本数)很小，即大约20，000。</li></ul><p id="a8d0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">法方程是一个很好的算法，可以考虑用来建立你的机器学习模型。</p></div><div class="ab cl of og hx oh" role="separator"><span class="oi bw bk oj ok ol"/><span class="oi bw bk oj ok ol"/><span class="oi bw bk oj ok"/></div><div class="im in io ip iq"><p id="9b10" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如有任何问题、评论或疑虑，请在回复部分与我联系。更多关于ML从零开始的文章即将到来。</p><p id="6a24" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">从零开始的机器学习系列—</p><ul class=""><li id="339d" class="lz ma it lb b lc ld lf lg li mb lm mc lq md lu me mf mg mh bi translated">第1部分:<a class="ae ky" href="https://medium.com/analytics-vidhya/linear-regression-from-scratch-in-python-b6501f91c82d?source=your_stories_page-------------------------------------" rel="noopener">Python中从头开始的线性回归</a></li><li id="3bbe" class="lz ma it lb b lc mi lf mj li mk lm ml lq mm lu me mf mg mh bi translated">第二部分:<a class="ae ky" rel="noopener" target="_blank" href="/locally-weighted-linear-regression-in-python-3d324108efbf?source=your_stories_page-------------------------------------">Python中的局部加权线性回归</a></li><li id="4209" class="lz ma it lb b lc mi lf mj li mk lm ml lq mm lu me mf mg mh bi translated">第3部分:<a class="ae ky" rel="noopener" target="_blank" href="/normal-equation-in-python-the-closed-form-solution-for-linear-regression-13df33f9ad71?source=your_stories_page-------------------------------------">使用Python的正规方程:线性回归的封闭解</a></li><li id="1b6b" class="lz ma it lb b lc mi lf mj li mk lm ml lq mm lu me mf mg mh bi translated">第4部分:<a class="ae ky" rel="noopener" target="_blank" href="/polynomial-regression-in-python-b69ab7df6105">Python中从头开始的多项式回归</a></li></ul></div></div>    
</body>
</html>