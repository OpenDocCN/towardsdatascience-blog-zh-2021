<html>
<head>
<title>Top 10 Projects for Beginners in Computer Vision and Medical Imaging</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">计算机视觉和医学成像初学者的十大项目</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/top-10-projects-for-beginners-in-computer-vision-and-medical-imaging-c138a646e44e?source=collection_archive---------1-----------------------#2021-04-09">https://towardsdatascience.com/top-10-projects-for-beginners-in-computer-vision-and-medical-imaging-c138a646e44e?source=collection_archive---------1-----------------------#2021-04-09</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="8408" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsdatascience.com/tagged/getting-started" rel="noopener" target="_blank">入门</a></h2><div class=""/><div class=""><h2 id="9a1e" class="pw-subtitle-paragraph jw iz iq bd b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn dk translated"><strong class="ak">计算机视觉和机器学习初学者综合实践指南</strong></h2></div><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ko"><img src="../Images/80e9b3ae03d5d142ff08e346256c6871.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*OdXpHIUlBs3Mbh4h"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">当前计算机视觉和医学成像项目的组合(图片由作者提供)</p></figure><p id="8e19" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">(人工智能)和计算机科学，使自动化系统能够看到，即以类似人类的方式处理图像和视频，以检测和识别重要的对象或区域，预测结果，甚至将图像更改为所需的格式[1]。CV领域中最受欢迎的用例包括自动驾驶的自动感知、模拟的增强和虚拟现实(ar、VR)、游戏、眼镜、房地产以及面向时尚或美容的电子商务。另一方面，医学图像(MI)处理涉及对通常为灰度级的医学图像(例如用于自动病理检测的MRI、CT或X射线图像)进行更详细的分析，这是一项需要训练有素的专家的眼睛进行检测的任务。MI领域中最受欢迎的用例包括自动病理学标记、定位、与治疗或预后的关联以及个性化医疗。</p><p id="effa" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">在深度学习方法出现之前，2D信号处理解决方案，如图像滤波、小波变换、图像配准，以及分类模型[2–3]被大量应用于解决方案框架。信号处理解决方案仍然是模型基线的首选，因为它们具有低延迟和跨数据集的高泛化能力。然而，深度学习解决方案和框架已经成为新宠，这是因为其端到端的性质完全消除了对特征工程、特征选择和输出阈值的需要。在本教程中，我们将为CV和MI领域的<em class="ma">初学者</em>回顾“<em class="ma">十大项目”</em>，并提供带有数据和起始代码的示例来帮助自定进度学习。</p><p id="9e68" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">CV和MI解决方案框架可以分为三个部分进行分析:<em class="ma">数据、过程</em>和<em class="ma">结果</em>【4】。重要的是，始终可视化此类解决方案框架所需的<em class="ma">数据</em>，使其具有格式“{X，Y}”，其中X表示图像/视频数据，Y表示数据目标或标签。虽然自然出现的未标记图像和视频序列(X)可能是大量的，但是获取准确的标记(Y)可能是一个昂贵的过程。随着[5–7]等多种数据注释平台的出现，图像和视频可以针对每个用例进行标记。</p><p id="fc90" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">由于深度学习模型通常依赖于大量带注释的数据来自动学习用于后续检测任务的特征，因此CV和MI域经常遭受“<a class="ae mb" href="https://blog.fourthbrain.ai/check-out-our-graduates-final-projects" rel="noopener ugc nofollow" target="_blank">小数据挑战</a>”，其中可用于训练机器学习模型的样本数量比模型参数的数量少几个数量级。“小数据挑战”如果得不到解决，可能会导致模型过拟合或欠拟合，从而无法推广到新的未知测试数据集。因此，为CV和MI域设计解决方案框架的<em class="ma">过程</em>必须始终包括模型复杂性约束，其中具有较少参数的模型通常是优选的，以防止模型欠拟合。最后，通过可视化解决方案对解决方案框架的结果进行定性分析，并根据众所周知的指标进行定量分析，如精确度、召回率、准确度以及F1或Dice系数[8–9]。</p><p id="d705" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">下面列出的项目在数据预处理和模型建立方面呈现了不同的难度级别(难度级别<em class="ma">简单、中等、困难</em>)。这些项目也代表了当前在研究和工程界流行的各种用例。项目是根据<em class="ma">目标、方法</em>和<em class="ma">结果</em>定义的。</p><p id="a081" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated"><strong class="lg ja">项目1: MNIST和时尚MNIST的图像分类(级别:简单)</strong></p><p id="a133" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated"><em class="ma">目标:</em>处理大小为[28x28]像素的图像(X)并将它们分类到10个输出类别(Y)中的一个。对于MNIST数据集，输入图像是0到9范围内的手写数字[10]。训练和测试数据集分别包含60，000和10，000个标记图像。受手写数字识别问题的启发，推出了另一个数据集，称为时尚MNIST数据集[11]，其目标是将图像(大小为[28x28])分类为服装类别，如图1所示。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi mc"><img src="../Images/a48a8945700f151083c4c9267cb990ce.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*wfYSrPZOQsMaAnvd"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">图1:MNIST和时尚MNIST各有10个输出类别的数据集。(图片由作者提供)</p></figure><p id="ecd0" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated"><em class="ma">方法:</em>当输入图像很小([28×28]像素)并且图像是灰度图像时，卷积神经网络(CNN)模型是合适的分类模型，其中卷积层数可以从单层变化到多层。colab文件中提供了一个使用Keras构建MNIST分类模型的示例:</p><p id="8830" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated"><a class="ae mb" href="https://colab.research.google.com/github/AviatorMoser/keras-mnist-tutorial/blob/master/MNIST%20in%20Keras.ipynb#scrollTo=uPtlBJoPhI9f" rel="noopener ugc nofollow" target="_blank"> MNIST的colab文件</a></p><p id="5e94" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">时尚MNIST数据集的另一个分类示例如下所示:</p><p id="ab0e" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated"><a class="ae mb" href="https://colab.research.google.com/github/tensorflow/tpu/blob/master/tools/colab/fashion_mnist.ipynb#scrollTo=DkYyndj8oO24" rel="noopener ugc nofollow" target="_blank">时尚MNIST Colab文件</a></p><p id="fcf1" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">在这两种情况下，要调整的关键参数包括层数、dropout、优化器(首选自适应优化器)、学习速率和内核大小，如下面的代码所示。由于这是一个多类问题，在最后一层使用“softmax”激活函数，以确保只有一个输出神经元的权重大于其他神经元。</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="md me l"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">多类问题的最终深度学习层。</p></figure><p id="eaaa" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated"><em class="ma">结果</em>:随着卷积层数从1-10增加，分类精度也增加。文献中对MNIST数据集进行了充分的研究，测试准确率在96–99%之间。对于时尚MNIST数据集，测试准确率通常在90-96%之间。下面的图2显示了使用CNN模型的MNIST分类结果的可视化示例。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi mf"><img src="../Images/d89deaa6bde02c55c0b587381950097c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*isLs9bvkahccUAwy"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">图2:MNIST数据的CNN模型的可视化结果的例子。输入显示在左上角，并显示相应的层激活。最终结果在5到8之间。见前端可视化【https://www.cs.ryerson.ca/~aharley/vis/conv/ T4】</p></figure><p id="d8fa" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated"><strong class="lg ja">项目2:医学图像的病理分类(级别:简单)</strong></p><p id="5293" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated"><em class="ma">目标:</em>将医学图像(使用光学相干断层扫描，OCT获得)分类为正常、糖尿病性黄斑水肿(DME)、玻璃疣、脉络膜新生血管(CNV)，如[12]所示。如图2所示，数据集包含大约84，000幅训练图像和大约1，000幅带有标签的测试图像，并且每个图像具有800到1，000像素的宽度。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi mg"><img src="../Images/649281da9139a06d31df8259bed3036a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*OTGPfd_7jLM0Ivwu"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">图2:来自[12]中Kaggle数据集的OCT图像示例。</p></figure><p id="9dc2" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated"><em class="ma">方法:</em>深度CNN模型如Resnet和CapsuleNet [12]已经被应用于对该数据集进行分类。数据需要调整到[512x512]或[256x256]才能输入标准分类模型。由于与非医学室外和室内图像相比，医学图像在每个图像帧的对象类别中具有较小的变化，因此发现训练大型CNN模型所需的医学图像的数量明显少于非医学图像的数量。[12]和<a class="ae mb" href="https://github.com/anoopsanka/retinal_oct/blob/main/notebooks/11-Experiments_on_Supervised_Model.ipynb" rel="noopener ugc nofollow" target="_blank"> OCT代码库</a>中的工作演示了为测试图像的迁移学习和分类重新训练ResNet层。这里要调整的参数包括优化器、学习速率、输入图像的大小和ResNet层末尾的密集层数。</p><p id="85e5" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated"><em class="ma">结果:</em>对于ResNet模型，通过改变训练图像的数量，测试精度可以在94–99%之间变化，如【12】所示。图三。定性地展示分类模型的性能。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi mh"><img src="../Images/ffd7cd2ee705d674610b15601ff84d16.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*rhTwsJtH5uOc1ZLL"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">图3:使用python中的<a class="ae mb" href="https://keras.io/examples/vision/grad_cam/" rel="noopener ugc nofollow" target="_blank"> Gradcam库</a>将每个病理的感兴趣区域(ROI)叠加在原始图像上。(图片由作者提供)</p></figure><p id="d12a" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">这些可视化是使用<a class="ae mb" href="https://keras.io/examples/vision/grad_cam/" rel="noopener ugc nofollow" target="_blank"> Gradcam库</a>产生的，该库将CNN层激活结合到原始图像上，以理解感兴趣的区域，或者自动检测的重要特征，用于分类任务。使用<em class="ma"> tf_explain </em>库的Gradcam的用法如下所示。</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="md me l"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">使用tf_explain来可视化可解释的热图。</p></figure><p id="70cc" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated"><strong class="lg ja">项目3:多标签图像分类的人工智能可解释性(级别:简单)</strong></p><p id="d157" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated"><em class="ma">目标:</em> CNN模型支持端到端交付，这意味着不需要为分类设计和排列特征，并且模型结果是期望的过程结果。然而，如专题2的后面部分所示，可视化和解释CNN模型的性能通常是很重要的。一些众所周知的可视化和可解释库是<a class="ae mb" href="https://tf-explain.readthedocs.io/en/latest/" rel="noopener ugc nofollow" target="_blank"> tf_explain </a>和<a class="ae mb" href="https://homes.cs.washington.edu/~marcotcr/blog/lime/" rel="noopener ugc nofollow" target="_blank">本地可解释的模型不可知解释(LIME) </a>。在这个项目中，目标是实现多标签分类，并解释CNN模型将什么视为以特定方式分类图像的特征。在这种情况下，我们考虑一个多标签场景，其中一个图像可以包含多个对象，例如<a class="ae mb" href="https://colab.research.google.com/github/arteagac/arteagac.github.io/blob/master/blog/lime_image.ipynb#scrollTo=8fPvSbn0woWP" rel="noopener ugc nofollow" target="_blank"> Colab for LIME </a>中的猫和狗。</p><p id="6246" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">这里，输入是包含猫和狗的图像，目标是识别哪些区域分别对应于猫或狗。</p><p id="2f62" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated"><em class="ma">方法:</em>在本项目中，每幅图像都经过超像素分割，将图像分割成几个像素颜色和纹理特征相似的子区域。划分的子区域的数量可以作为参数手动提供。接下来，调用InceptionV3模型来为每个超像素子区域分配属于InceptionV3最初被训练的1000个类别之一的概率。最后，对象概率被用作拟合回归模型的权重，该回归模型解释了对应于每个类别的ROI，如图4和下面的代码所示。</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="md me l"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">设置LIME来解释图像中的区域重要性。</p></figure><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi mi"><img src="../Images/630877e12e9b0191c0f6fdc8e894ebcc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*2ghfBk_nkZAi1EWi"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">图4:使用类回归模型解释图像超像素的可能性。(图片由作者提供)</p></figure><p id="2eea" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated"><em class="ma">结果:</em>使用所提出的方法，大多数非医学图像中的感兴趣区域应该是可以解释的。这里显示的定性评估和可解释性在极端情况下，或者在模型错误分类或错过感兴趣的对象的情况下特别有用。在这种情况下，解释CNN模型在看什么，并相应地提高ROI以校正整体分类性能，可以帮助显著减少数据引起的偏差。</p><p id="7702" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated"><strong class="lg ja">项目4:对新物体进行2D包围盒检测的迁移学习(级别:中等)</strong></p><p id="a444" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated"><em class="ma">目标:</em>图像分类后的下一步是通过在感兴趣的物体周围放置边界框来检测它们。这是自动驾驶领域中的一个重要问题，以从静态对象(如路障、街道标志、树木和建筑物)中准确识别移动对象(如汽车和行人)。本项目与以前项目的主要区别在于数据的格式。这里，标签Y通常是每个感兴趣对象的[x，Y，w，h]的形式，其中(x，Y)通常表示边界框的左上角，而<em class="ma"> w </em>和<em class="ma"> h </em>对应于输出边界框的宽度和高度。在这个项目中，目标是利用预训练的分类器的特征提取能力，然后在一小组图像上重新训练它，以创建围绕新对象的紧密边界框。</p><p id="9ea8" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated"><em class="ma">方法:</em>在代码<a class="ae mb" href="https://colab.research.google.com/github/tensorflow/models/blob/master/research/object_detection/colab_tutorials/eager_few_shot_od_training_tf2_colab.ipynb" rel="noopener ugc nofollow" target="_blank">边界框colab </a>中，我们可以扩展一个预训练的对象检测器，如具有Resnet50跳跃连接和特征金字塔网络主干的单次拍摄检测器(SSD)，它是为MS-COCO数据集[13]上的对象检测而预训练的，以检测一个完全看不见的新对象类别，在这种情况下是一个<em class="ma">橡皮鸭</em>。在这种转移学习设置中，来自对象检测器的早期层的已经学习的权重对于从图像中提取局部结构和纹理信息是有用的，并且只有最终的分类器层需要为新的对象类重新训练。这使得可以使用新对象的5-15幅图像来为新类别重新训练对象检测器，例如本用例中的橡胶鸭子。要调整的参数包括优化器、学习速率、输入图像大小和最终分类器层中的神经元数量。</p><p id="a43e" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated"><em class="ma">结果:</em>对象检测器和上面所示的先前的基于CNN的分类器模型之间的一个主要区别是称为并集上的交集(IoU) [11]的附加输出度量，其测量实际边界框和预测边界框之间的重叠程度。此外，对象检测器模型通常由分类器(预测对象类别)和边界框回归器(预测对象周围边界框的尺寸)组成。图5和下面的代码显示了Google API在新的不可见图像上进行对象检测的例子。</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="md me l"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">Google api调用对象检测。</p></figure><p id="50b7" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">在<a class="ae mb" href="https://blog.fourthbrain.ai/check-out-our-graduates-final-projects" rel="noopener ugc nofollow" target="_blank">这些项目</a>中显示了2D包围盒检测器对专门用于自动驾驶的3D包围盒的扩展。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi mj"><img src="../Images/99b4c480f4ea5d0c00e2ac282e3363cd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*YlUgY3abqP-WXbPg"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">图5:使用tensorflow <a class="ae mb" href="https://colab.research.google.com/github/tensorflow/hub/blob/master/examples/colab/object_detection.ipynb" rel="noopener ugc nofollow" target="_blank"> api进行对象检测</a>的2D包围盒检测示例</p></figure><p id="26bf" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated"><strong class="lg ja">项目5:个性化医疗和可解释性(等级:中等)</strong></p><p id="1f81" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated"><em class="ma">目标:</em>在这个项目中，目标是自动分割来自多个病理部位的ROI，以对患者贫血样苍白的程度进行分类，并随着时间的推移跟踪苍白[13]。该项目与先前项目的两个主要区别在于:1)需要跨多个图像位置(例如结膜(眼睛下方)和舌头)检测苍白，以预测单个标记，如图6所示，2)需要显示对应于苍白的ROI，并随时间进行跟踪。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi mk"><img src="../Images/4af8669557fce1a750111f1158256013.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*VOIlTHaq2up0ddkX"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">图6:使用从多个病理部位处理的图像的贫血样苍白检测的例子。(图片由作者提供)</p></figure><p id="c7d6" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated"><em class="ma">方法:</em>在这个项目中，使用Keras中的<a class="ae mb" href="https://keras.io/api/preprocessing/image/" rel="noopener ugc nofollow" target="_blank">图像数据生成器</a>对基于特征的模型和基于CNN的分类器进行大量数据扩充。为了融合来自多个病理部位的结果，可以应用早期、中期和晚期融合。[13]中的工作应用了后期融合，其中分类器之前的层被认为是图像的最佳特征表示，用于融合多个病理部位的特征。最后，如<a class="ae mb" href="https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/generative/deepdream.ipynb" rel="noopener ugc nofollow" target="_blank"> Deepdream Colab </a>所示，将Deepdream算法应用于原始眼睛和舌头图像，以可视化ROI并解释病理的程度。这个项目中要调整的参数包括项目2中的参数以及Deepdream可视化的附加梯度因子。</p><p id="adc6" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated"><em class="ma">结果:</em>此项工作的数据可用于<a class="ae mb" href="https://sites.google.com/site/sohiniroychowdhury/automated-pallor-detection-project?authuser=0" rel="noopener ugc nofollow" target="_blank">基准测试</a>。使用Deepdream算法，在图7中示出了可视化，其中，我们观察到与眼睛中的任何其他地方相比，在眼睛下方的血管中对应于苍白的特征的浓度更高。类似地，我们观察到舌头的内侧和外侧部分之间的特征差异。这些评估有助于为贫血患者创建个性化病理跟踪系统。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi ml"><img src="../Images/ea1544246207b496c51b0a52f062e84e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1120/0*t7Ku338S-3drXKLS"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">图7:深度梦实现的特征集中的例子。在结膜或眼睛下方的血管区域观察到高浓度的梯度。(图片由作者提供)</p></figure><p id="c112" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated"><strong class="lg ja">项目6:用于对象检测的点云分割。(等级:硬)</strong></p><p id="a77e" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated"><em class="ma">目标:</em>在这个项目中，输入是点云流，即提供深度分辨率的激光雷达传感器的输出。激光雷达点云与影像之间的主要区别在于点云提供3D分辨率，因此每个体素(像素的3D等效物)表示对象相对于激光雷达源的位置以及对象相对于激光雷达源的高度。点云数据模型带来的主要挑战是:I)如果使用3D卷积，模型计算复杂；ii)对象变换不变性，这意味着旋转的对象应被检测为对象本身，如[13]所示。</p><p id="a098" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated"><em class="ma">方法:</em>这个项目的数据集是ModelNet40形状分类基准，包含来自40个对象类的超过12，000个3D模型。对每个对象进行子采样，以提取固定数量的点，然后进行扩充，以适应形状的多次变换。接下来1D卷积被用于学习形状<em class="ma"> ness </em>特征，使用<a class="ae mb" href="https://colab.research.google.com/github/nikitakaraevv/pointnet/blob/master/nbs/PointNetClass.ipynb" rel="noopener ugc nofollow" target="_blank"> Pointnet colab </a>中的Pytorch库，如下所示。</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="md me l"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">Pointnet类的定义。</p></figure><p id="c8df" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">要调整的参数包括优化器、学习率和损失函数。</p><p id="18ea" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated"><em class="ma">结果:</em>模型的结果可以使用下面的图8来总结。通过该方法可以实现高达89%的对象分类训练准确度，该方法还可以扩展到3D语义分割。这项工作的扩展可以用于自动驾驶用例的3D边界框检测。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi mm"><img src="../Images/3fa3bfa010d4c76f8ebd3feac7b3a289.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*W05OKgP1TODxqGlt"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">图8:来自[15]的图像，该图像从点云中识别物体</p></figure><p id="d29a" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated"><strong class="lg ja">项目7:使用U-net进行二值和多值图像语义分割。(中等)</strong></p><p id="39a1" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated"><em class="ma">目标:</em>到目前为止，CNN模型已经被应用于自动学习特征，然后可以用于分类。这个过程被称为<em class="ma">特征编码</em>。下一步，我们应用与编码器结构相似的解码器单元来生成输出图像。编码器-解码器对的这种组合使得输入和输出能够具有相似的维度，即输入是图像，输出也是图像。</p><p id="0a27" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated"><em class="ma">方法:</em>具有剩余跳跃连接的编码器-解码器组合通常被称为U-net [15]。对于二进制和多类问题，数据必须被格式化，使得如果X(输入图像)具有维度[m×m]像素，Y具有维度[m×m×d]，其中‘d’是要预测的类的数量。要调整的参数包括优化器、学习率和U-net模型的深度，如下面的[15]和图9所示。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi mf"><img src="../Images/59a17979948250f8a79d5e6278a45c4b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*agnAfRLrZi10Zy2O"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">图9。U-net模型示例。来源<a class="ae mb" href="https://paperswithcode.com/paper/multiresunet-rethinking-the-u-net" rel="noopener ugc nofollow" target="_blank">https://papers with code . com/paper/multi runet-reflecting-the-u-net</a></p></figure><p id="e964" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated"><em class="ma">结果:</em>U-net模型可以学习从大小数据集生成二元和多类语义图[16–17]，但发现它对数据不平衡很敏感。因此，选择正确的训练数据集对于获得最佳结果非常重要。这项工作的其他扩展将包括DenseNet到模型的连接，或其他编码器-解码器网络，如MobileNet或异常网络[17]。</p><p id="c7f2" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated"><strong class="lg ja">项目8:姿态和意图分类的机器翻译(等级:硬)</strong></p><p id="50b8" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated"><em class="ma">目标:</em>姿态或手势的自动检测通常包括视频中的关键点识别(如骨骼结构的识别)，这些关键点识别可以导致行人姿态(站立、行走、移动)或意图(过马路，而不是过马路)等的识别。[18–19]，如下图10所示。对于这类问题，来自多个后续视频帧的关键帧信息被共同处理，以生成与<a class="ae mb" href="https://www.youtube.com/watch?v=a7SrsA--mtA&amp;t=1s" rel="noopener ugc nofollow" target="_blank">姿势/意图相关的预测。</a></p><div class="kp kq kr ks gt ab cb"><figure class="mn kt mo mp mq mr ms paragraph-image"><img src="../Images/dafdc32d1f5a0f8da81e54a8dd57973d.png" data-original-src="https://miro.medium.com/v2/resize:fit:200/0*W6F4QASUNIlvLBIZ"/></figure><figure class="mn kt mo mp mq mr ms paragraph-image"><img src="../Images/8e8f13e72316d27dcc150a8ebb313373.png" data-original-src="https://miro.medium.com/v2/resize:fit:200/0*SDCbejciV-lk_ix9"/><p class="la lb gj gh gi lc ld bd b be z dk mt di mu mv translated">图10:来源:【https://matthew29tang.github.io/pid-model/#/pose/ T4】</p></figure></div><p id="2dd0" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated"><em class="ma">方法:</em>对于这个项目，所应用的模型类别被称为“<em class="ma">序列对序列”</em>模型，其中来自视频的一系列图像帧被处理以预测行人是否打算过马路。该过程首先使用2D包围盒检测器来隔离行人，然后通过实时跟踪来跨帧跟踪同一包围盒。最后，来自被跟踪的边界框和骨架特征的特征被用于训练一个<a class="ae mb" href="https://arxiv.org/abs/1608.06993" rel="noopener ugc nofollow" target="_blank"> DenseNet模型</a>，该模型预测行人是否会在移动的汽车前面移动。要调整的参数包括来自项目4的边界框检测器参数、Densenet模型的层数以及每个行人姿势所适合的骨骼点的数量。骨架点的数量越多，计算复杂度越高。</p><p id="38b6" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated"><em class="ma">结果:</em>上述方法的定性结果如下图11所示，其中红色方框表示行人将穿过街道，绿色方框表示行人不会与驶来的车辆穿过道路。[18–19]中的工作和<a class="ae mb" href="https://colab.research.google.com/drive/1InGYCg_SEosqumijaGyWzC9VoLPm5ZEx#scrollTo=DGgcEDqHNyUl&amp;forceEdit=true&amp;sandboxMode=true" rel="noopener ugc nofollow" target="_blank"> Colab文件</a>可用于测试复合模型，该模型使用边界框内的特征和骨骼特征作为任何新数据集的早期融合设置，以从一系列图像帧中生成姿势相关预测。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi mw"><img src="../Images/f1335edd96f43741724cbc1f42b42960.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/0*KDJN4cPumgqTEItp"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">图11:在JAAD [18]数据集上行人意图预测的例子。来源:<a class="ae mb" href="https://matthew29tang.github.io/pid-model/#/integrated/" rel="noopener ugc nofollow" target="_blank">https://matthew29tang.github.io/pid-model/#/integrated/</a></p></figure><p id="e9f7" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated"><strong class="lg ja">项目9:用于数据增强的生成式对抗网络(GAN)(使用Pix2pix模型)(级别:硬)</strong></p><p id="d62b" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated"><em class="ma">目标:</em>CNN模型的一个流行用例是使用GANs自动生成假图像。在这样的设置中，训练两个CNN模型:1)生成器，其目标是从噪声和一些约束中创建真实的假图像，2)鉴别器，其目标是从真图像中识别假图像。基础数据集需要成对的图像和语义分割图{X，Y}用于训练，如图12和<a class="ae mb" href="https://colab.research.google.com/github/Neyri/Projet-cGAN/blob/master/BE_cGAN_Enonce.ipynb" rel="noopener ugc nofollow" target="_blank"> GAN Colab </a>中所示。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi mx"><img src="../Images/13e174837eab171f920376a5bf26ea06.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*THoGX__vTF_DbsPB"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">图12:来自<a class="ae mb" href="https://cmp.felk.cvut.cz/~tylecr1/facade/" rel="noopener ugc nofollow" target="_blank"> CMP正面数据集</a>的示例，使得语义分割图用于预测真实的RGB(红绿蓝平面)图像。</p></figure><p id="3731" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated"><em class="ma">方法:</em>对于这个项目，pix2pix设置依赖于条件GANs来训练基于CNN的<em class="ma">生成器</em>(这是一个修改的U-net模型)来生成难以从真实图像中区分/分类的假图像。GAN模型通常利用损失函数，该损失函数是<a class="ae mb" href="https://developers.google.com/machine-learning/gan/loss" rel="noopener ugc nofollow" target="_blank">最小最大损失</a>或GAN损失以及实际图像和伪图像之间的平均绝对误差的组合。要调整的参数包括优化器、学习率、特征补丁大小(通常30-70像素宽和高是<a class="ae mb" href="https://paperswithcode.com/method/patchgan" rel="noopener ugc nofollow" target="_blank"> PatchGAN </a>的首选)、输入图像大小和<em class="ma">生成器</em>模型的复杂性，如下所示。</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="md me l"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">发电机模型在GANs中的使用。</p></figure><p id="6aac" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated"><em class="ma">结果:</em><a class="ae mb" href="https://github.com/yenchenlin/pix2pix-tensorflow" rel="noopener ugc nofollow" target="_blank">pix 2 pix方法</a>可用于从灰度图像生成彩色图像，从陆地图像生成地图图像，从线描图像生成RGB图像。该模型允许低维到高维的数据生成，这对于增加训练数据集和解决“小数据挑战”是至关重要的。</p><p id="90e3" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated"><strong class="lg ja">项目10:使用CycleGAN进行图像转换(级别:硬)</strong></p><p id="f69e" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated"><em class="ma">目标:</em>虽然pix2pix支持图像转换，但它需要成对的图像和目标标签，即出于训练目的需要图像及其相应的语义分割。然而，在成对输入和标签不可用的情况下，CycleGANs [17]可能是有用的。在CycleGAN模型中，使用了两组发生器/鉴别器组合。如果目标是将域A中的图像变换到域B，则Generator_1/Discriminator_1用于将图像A变换到B，而Generator_2/Discriminator_2用于将图像B变换回图像A。因此在一个周期结束时，获得图像A的双重变换版本。总损失函数旨在减少以下各项之间的差异:1)在全周期结束时生成的图像A和实际图像A，2)从周期中间到实际图像B生成的图像B。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi my"><img src="../Images/e33de4dd42f551d11ea1903ae86ec7bc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*8VrPazWKuZF4Xigb"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">图13:没有成对训练图像的图像变换的例子。https://github.com/xhujoy/CycleGAN-tensorflow来源<a class="ae mb" href="https://github.com/xhujoy/CycleGAN-tensorflow" rel="noopener ugc nofollow" target="_blank">。</a></p></figure><p id="95ee" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">(顶行)，Generator_1/Discriminator_1以马图像为输入生成假斑马图像，(底行)Generator_2/Discriminator_2以斑马图像为输入生成假马图像。</p><p id="4fd9" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated"><em class="ma">方法:</em>使用<a class="ae mb" href="https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/generative/cyclegan.ipynb" rel="noopener ugc nofollow" target="_blank"> CycleGAN Colab </a>，我们观察到CycleGAN和条件GAN (cGAN)模型之间的主要差异是损失函数，该函数包括项目9中为每个发生器/鉴别器对定义的GAN损失以及前向和后向循环一致性损失，该损失表示A- &gt; B变换和B- &gt; A变换的伪图像和原始图像之间的平均误差。要调整的其他参数包括生成器复杂度、优化器、学习速率、输入图像大小和周期损失函数。</p><p id="38d0" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated"><em class="ma">结果:</em> CycleGAN已用于图像域转换功能，如白天到夜晚的时间转换[18]，可用于分别在白天和夜晚重复使用自动驾驶物体检测器的标签。这些模型也可以用于艺术变换、图像去噪和AR/VR变换。</p><p id="612b" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated"><strong class="lg ja">结论</strong></p><p id="461d" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">几种不同类型的基于CNN的模型框架可用于设计CV和MI问题的端到端解决方案。为了修改这种基于CNN的框架以适合各个数据集和用例，需要考虑的因素如下:1)需要调整的参数，2)损失函数的修改，3)用于模型训练的标记数据量，4)关于数据不平衡的考虑。</p><p id="2f31" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">在扩展此处显示的10个项目中的任何一个时，理想的策略是首先通过将数据集格式化为与上面示例中类似的格式来复制手头数据集的现有代码库。一旦训练过程成功，下一步将是修改/预处理数据，然后调整模型参数和结果的定性和定量报告。值得注意的是，输出指标可能因图像域和使用情形而异。例如，在自动驾驶用例中，误报物体检测可能会因过度鸣响和干扰驾驶员而破坏驾驶体验，因此在这种用例中应尽量减少误报。或者，在医学图像病理学检测用例中，遗漏病理学远比自动过度检测更糟糕，因为专家最终会查看患者图像以丢弃假阳性。因此，假阳性和假阴性的权重并不总是相同的，在报告CV和MI解决方案的结果时，应该考虑到不同用例之间的差异。</p><p id="94ae" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">本博客中展示的项目和资源应该可以帮助所有初学者和CV、MI爱好者开始他们自己的数据集自学之旅。</p><p id="a7db" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated"><strong class="lg ja">参考文献</strong></p><p id="6dba" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">[1]techno pedia[在线]:【https://www.techopedia.com/definition/32309/computer-vision T2</p><p id="db61" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">[2] Roychowdhury、Sohini、Dara D. Koozekanani和Keshab K. Parhi。"梦:使用机器学习的糖尿病视网膜病变分析."IEEE生物医学和健康信息学杂志18.5(2013):1717–1728。</p><p id="44cd" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">[3]王，李一清。" Viola-Jones人脸检测算法分析."4号线图像处理(2014):128–148。</p><p id="4b3f" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">[4]S. Roychowdhury，“使用U-net转移学习多类图像分割的机器学习工程师教程”，走向数据科学[在线]:<a class="ae mb" rel="noopener" target="_blank" href="/a-machine-learning-engineers-tutorial-to-transfer-learning-for-multi-class-image-segmentation-b34818caec6b">https://towardsdatascience . com/A-Machine-Learning-engineers-Tutorial-to-Transfer-Learning-for-Multi-class-Image-Segmentation-b 34818 caec 6b</a></p><p id="01e4" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">[5]A .杜塔等人。艾尔。https://www.robots.ox.ac.uk/~vgg/software/via/:<a class="ae mb" href="https://www.robots.ox.ac.uk/~vgg/software/via/" rel="noopener ugc nofollow" target="_blank">VGG图像注释者【在线】</a></p><p id="9e0f" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">[6]伯克利深层驱动。“Scalabel.ai”。<a class="ae mb" href="https://www.scalabel.ai/" rel="noopener ugc nofollow" target="_blank">https://www.scalabel.ai/</a></p><p id="05d6" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">[7]GNU图像处理程序[在线]<a class="ae mb" href="https://www.gimp.org/about/" rel="noopener ugc nofollow" target="_blank">https://www.gimp.org/about/</a></p><p id="695d" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">[8]E. Solutions，“准确度、精确度、召回率和F1分数:性能测量的解释”，[在线]:<a class="ae mb" href="https://blog.exsilio.com/all/accuracy-precision-recall-f1-score-interpretation-of-performance-measures/" rel="noopener ugc nofollow" target="_blank">https://blog . ex silio . com/all/Accuracy-Precision-Recall-F1-Score-Interpretation-of-Performance-Measures/</a></p><p id="7453" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">[9]E. Tiu，《度量评估你的语义分割模型》，[在线]:<a class="ae mb" rel="noopener" target="_blank" href="/metrics-to-evaluate-your-semantic-segmentation-model-6bcb99639aa2">https://towards data science . com/Metrics-to-Evaluate-your-Semantic-Segmentation-Model-6 BCB 99639 aa 2</a></p><p id="faf0" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">[10]T. Kanstren《对精度、召回和F1-Score的看法》，走向数据科学[在线]<a class="ae mb" rel="noopener" target="_blank" href="/a-look-at-precision-recall-and-f1-score-36b5fd0dd3ec">https://towardsdatascience . com/A-Look-at-Precision-Recall-and-F1-Score-36 b5 FD 0 DD 3 EC</a></p><p id="a04f" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">[11]吴，明，张及甄。"使用mnist数据集的手写数字分类."课程项目CSE802:模式分类与分析(2010)。</p><p id="e886" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">[12]Roychowdhury，Sohini等人，“SISE-PC:用于解释病理学的半监督图像子采样”arXiv预印本arXiv:2102.11560  (2021)。<a class="ae mb" href="https://github.com/anoopsanka/retinal_oct" rel="noopener ugc nofollow" target="_blank">https://github.com/anoopsanka/retinal_oct</a></p><p id="3d5a" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">13 Roychowdhury等人。艾尔。"基于Azure的贫血样苍白智能监测系统."未来互联网9.3 (2017): 39。</p><p id="2572" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">[14]N. Karaev，“点云上的深度学习:在Google Colab中实现PointNet”，走向数据科学[在线]:<a class="ae mb" rel="noopener" target="_blank" href="/deep-learning-on-point-clouds-implementing-pointnet-in-google-colab-1fd65cd3a263">https://towardsdatascience . com/Deep-Learning-on-Point-clouds-Implementing-Point net-in-Google-Colab-1fd 65 CD 3a 263</a></p><p id="494a" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">[15]齐，查尔斯·r .等，“点网:用于三维分类和分割的点集的深度学习。”IEEE计算机视觉和模式识别会议录。2017.</p><p id="65ba" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">[16]曲，，等.“增强型pix2pix去雾网络”<em class="ma">IEEE/CVF计算机视觉和模式识别会议论文集</em>。2019.</p><p id="e4e7" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">[17] Harms，Joseph等人，“用于定量锥形束计算机断层成像的成对循环GAN基图像校正。”<em class="ma">医学物理</em>46.9(2019):3998–4009。</p><p id="b98f" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">[18] Chowdhury，Sohini Roy等人，“利用强化学习和gans进行自动增强，使用前摄像机图像对交通标志进行稳健识别。”<em class="ma"> 2019第53届Asilomar信号、系统和计算机会议</em>。IEEE，2019。</p></div></div>    
</body>
</html>