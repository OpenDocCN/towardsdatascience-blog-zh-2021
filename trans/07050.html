<html>
<head>
<title>GIRAFFE: A Closer Look at the Code for CVPR 2021’s Best Paper</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">长颈鹿:仔细看看CVPR 2021最佳论文的代码</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/giraffe-a-closer-look-at-cvpr-2021s-best-paper-1ec81f593fa9?source=collection_archive---------12-----------------------#2021-06-26">https://towardsdatascience.com/giraffe-a-closer-look-at-cvpr-2021s-best-paper-1ec81f593fa9?source=collection_archive---------12-----------------------#2021-06-26</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="3f95" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">GIRAFFE是一个基于学习的、完全可区分的渲染引擎，用于将场景合成为多个“特征字段”的总和</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ki"><img src="../Images/0269129d7b6305306fd44000d107db25.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/0*MDip2WLBhOwUtcAh.gif"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">使用GIRAFFE旋转和平移GAN生成的汽车(作者使用<a class="ae ku" href="https://github.com/autonomousvision/giraffe" rel="noopener ugc nofollow" target="_blank">https://github.com/autonomousvision/giraffe</a>创建，麻省理工学院许可)。</p></figure><p id="8410" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">CVPR 2021已经结束了，多么棒的论文选择！深度学习继续主导计算机视觉领域，有SLAM、姿势估计、深度估计、新数据集、GANs的新方法，以及对去年的<a class="ae ku" href="https://www.matthewtancik.com/nerf" rel="noopener ugc nofollow" target="_blank">神经辐射场</a> [ <a class="ae ku" href="https://arxiv.org/abs/2003.08934" rel="noopener ugc nofollow" target="_blank"> 1 </a> ]，或NeRFs的许多改进，仅举几例。</p><p id="576c" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">到现在为止，你可能已经听说过一篇名为“<a class="ae ku" href="https://m-niemeyer.github.io/project-pages/giraffe/index.html" rel="noopener ugc nofollow" target="_blank">长颈鹿:将场景表示为合成生成神经特征场的论文。</a> [ <a class="ae ku" href="http://www.cvlibs.net/publications/Niemeyer2021CVPR.pdf" rel="noopener ugc nofollow" target="_blank"> 2 </a> ]“这篇论文获得了今年最佳论文奖的大奖，它将GANs、NeRFs和可微分渲染结合在一起，生成了新颖的图像。然而，更重要的是，它提供了一个模块化的框架，以完全可区分和可学习的方式从对象构建和合成3D场景，让我们更接近神经3D设计的世界。在这篇文章中，我仔细查看了长颈鹿<a class="ae ku" href="https://github.com/autonomousvision/giraffe" rel="noopener ugc nofollow" target="_blank">的源代码</a>，并生成了一些快速的可视化示例。</p><h1 id="b156" class="lr ls it bd lt lu lv lw lx ly lz ma mb jz mc ka md kc me kd mf kf mg kg mh mi bi translated">神经辐射场</h1><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mj mk l"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">NeRF (YouTube)的可视化解释和演示。</p></figure><p id="95ef" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">简要回顾一下NeRFs ，它们是一种根据3D体积中任何给定点的密度和亮度来描述和渲染3D场景的方法。它与<a class="ae ku" href="https://en.wikipedia.org/wiki/Light_field" rel="noopener ugc nofollow" target="_blank">光场</a>的概念密切相关，光场是表达光如何穿过给定空间的函数。对于空间中给定的<em class="ml"> (x，y，z) </em>视点，图片将方向为<em class="ml"> (θ，φ) </em>的光线投射到场景中。对于沿着这条线的每个点，我们收集它的密度和<em class="ml">视点相关的发射辐射</em>，并以类似于传统<a class="ae ku" href="https://en.wikipedia.org/wiki/Ray_tracing_(graphics)" rel="noopener ugc nofollow" target="_blank">光线追踪</a>的方式将这些光线合成为单个像素值。这些NeRF场景是从各种姿势拍摄的图像集合中学习来的，就像你在<a class="ae ku" href="https://en.wikipedia.org/wiki/Structure_from_motion" rel="noopener ugc nofollow" target="_blank">结构来自运动</a>应用中使用的那样。</p><h1 id="9857" class="lr ls it bd lt lu lv lw lx ly lz ma mb jz mc ka md kc me kd mf kf mg kg mh mi bi translated">长颈鹿</h1><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mj mk l"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">长颈鹿的视觉解释和演示(YouTube)。</p></figure><h2 id="edf8" class="mm ls it bd lt mn mo dn lx mp mq dp mb le mr ms md li mt mu mf lm mv mw mh mx bi translated">概观</h2><p id="73bf" class="pw-post-body-paragraph kv kw it kx b ky my ju la lb mz jx ld le na lg lh li nb lk ll lm nc lo lp lq im bi translated">本质上，GIRAFFE是一个基于学习的完全可区分的渲染引擎，它允许您将场景合成为多个“特征场”的总和，这是NeRFs中辐射场的概括。这些特征场是3D体积，其中每个体素包含一个特征向量。通过合成由接受潜在代码作为3D场景输入的GANs产生的学习表示来构建特征场。由于特征字段应用于3D体积，因此您可以应用相似性变换，如旋转、平移和缩放。您甚至可以将整个场景合成为单个特征字段的总和。该方法对NeRFs进行了以下改进:</p><ul class=""><li id="b33a" class="nd ne it kx b ky kz lb lc le nf li ng lm nh lq ni nj nk nl bi translated">可以用独立的变换表示多个对象(和一个背景)(最初的NeRF只能支持一个“场景”,不能分解单个对象)。</li><li id="d1a0" class="nd ne it kx b ky nm lb nn le no li np lm nq lq ni nj nk nl bi translated">可以对单个对象应用姿势和相似性变换，如旋转、平移和缩放。</li><li id="519b" class="nd ne it kx b ky nm lb nn le no li np lm nq lq ni nj nk nl bi translated">产生特征字段的gan可以独立学习并像组件一样重用。</li><li id="1cef" class="nd ne it kx b ky nm lb nn le no li np lm nq lq ni nj nk nl bi translated">拥有经过端到端培训的独特渲染引擎。</li><li id="82ec" class="nd ne it kx b ky nm lb nn le no li np lm nq lq ni nj nk nl bi translated">颜色值不仅仅支持RGB，还可以扩展到其他材质属性。</li><li id="ae8b" class="nd ne it kx b ky nm lb nn le no li np lm nq lq ni nj nk nl bi translated">使用位置编码(如转换器)来编码位置，这也“引入了感应偏差来学习规范方向上的3D形状表示，否则这些表示将是任意的。”</li></ul><p id="603d" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">长颈鹿项目包括<a class="ae ku" href="https://github.com/autonomousvision/giraffe" rel="noopener ugc nofollow" target="_blank">源代码</a>，你可以用它来复制它们的形象，甚至组成你自己的场景。我简要介绍了它们的源代码，并展示了我们如何使用GIRAFFE来构建一些简单的神经3D场景。</p><h2 id="ef07" class="mm ls it bd lt mn mo dn lx mp mq dp mb le mr ms md li mt mu mf lm mv mw mh mx bi translated">源代码</h2><p id="ccda" class="pw-post-body-paragraph kv kw it kx b ky my ju la lb mz jx ld le na lg lh li nb lk ll lm nc lo lp lq im bi translated"><a class="ae ku" href="https://github.com/autonomousvision/giraffe" rel="noopener ugc nofollow" target="_blank">GIRAFFE repo</a>的结构考虑了配置。<code class="fe nr ns nt nu b">configs/default.yaml</code>文件指定了应用程序的默认配置。其他配置文件如<code class="fe nr ns nt nu b">configs/256res/cars_256/pretrained.yaml</code>使用<code class="fe nr ns nt nu b">inherit_from</code>键继承这个配置文件，并通过指定其他键-值对覆盖默认值。这使我们能够用<code class="fe nr ns nt nu b">python render.py &lt;CONFIG.yaml&gt;</code>渲染图像，用<code class="fe nr ns nt nu b">python train.py &lt;CONFIG.yaml&gt;</code>通过自文档化的配置文件进行训练，而不是编写输入参数。</p><p id="31e2" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">要自己尝试一些渲染，首先运行<code class="fe nr ns nt nu b">README.md</code>文件中的<a class="ae ku" href="https://github.com/autonomousvision/giraffe#tl-dr---quick-start" rel="noopener ugc nofollow" target="_blank">快速启动指令</a>。这将下载一个预训练的模型，并将一系列输出可视化(如下图所示)写入文件夹<code class="fe nr ns nt nu b">out</code>。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ki"><img src="../Images/7e63aa328248b676e137575f1a7fbede.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/0*1XmSwfwZI6cd-WKY.gif"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">对象随Cars数据集旋转(由作者使用<a class="ae ku" href="https://github.com/autonomousvision/giraffe" rel="noopener ugc nofollow" target="_blank">https://github.com/autonomousvision/giraffe</a>，麻省理工学院许可创建)。</p></figure><p id="b088" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">配置文件只是采用默认值，并在<a class="ae ku" href="http://ai.stanford.edu/~jkrause/cars/car_dataset.html" rel="noopener ugc nofollow" target="_blank"> Cars数据集</a>上插入一个预训练模型。它提供了很多可视化的方法来操作底层渲染，比如外观插值、形状插值、背景插值、旋转和平移。这些可视化在<code class="fe nr ns nt nu b">render_program</code>键下的<code class="fe nr ns nt nu b">configs/default.yaml</code>中指定，其值是指定这些可视化的字符串列表。这些指定了长颈鹿渲染器在调用<code class="fe nr ns nt nu b">render.py</code>时将调用的“渲染程序”。在<code class="fe nr ns nt nu b">im2scene.giraffe.rendering.Renderer</code>的<code class="fe nr ns nt nu b">render_full_visualization</code>方法中，你会看到一系列的<code class="fe nr ns nt nu b">if</code>语句，寻找更多渲染程序的名字，比如‘object _ translation _ circle’、‘render _ camera _ elevation’和‘render _ add _ cars’。</p><p id="b93a" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">为什么我们不试试这些呢？创建一个名为<code class="fe nr ns nt nu b">cars_256_pretrained_more.yaml</code>的新配置文件，并添加以下内容:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nv mk l"/></div></figure><p id="393d" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">这只是我们用默认配置文件的<code class="fe nr ns nt nu b">render_program</code>键使用的先前的配置文件，它被我们想要的新渲染程序覆盖了。现在执行<code class="fe nr ns nt nu b">python render.py configs/256res/cars_256_pretrained_more.yaml</code>来产生更多的可视化效果。您应该会得到这样的结果:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ki"><img src="../Images/6ad369f7c6121e59d68667474db6e625.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/0*DZHQBaNUdFh3YhG_.gif"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">Cars数据集的摄像机仰角。请注意相机视角在背景和汽车侧面上是如何变化的，就好像相机从上向下绕着汽车旋转一样(作者使用<a class="ae ku" href="https://github.com/autonomousvision/giraffe" rel="noopener ugc nofollow" target="_blank">https://github.com/autonomousvision/giraffe</a>，麻省理工学院许可创建)。</p></figure><p id="a5ed" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">…还有这个:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ki"><img src="../Images/0815ca1dc47c89a5b4186fba492159f2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/0*F63KXZYCoLNW-whI.gif"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">使用汽车数据集添加汽车(由作者使用<a class="ae ku" href="https://github.com/autonomousvision/giraffe" rel="noopener ugc nofollow" target="_blank">https://github.com/autonomousvision/giraffe</a>，麻省理工学院许可创建)。</p></figure><p id="40e0" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">这些渲染程序实际上是如何放置、平移和旋转这些汽车的？要回答这个问题，我们需要仔细看看<code class="fe nr ns nt nu b">Renderer</code>类。对于上面的<code class="fe nr ns nt nu b">'object_rotation'</code>示例，调用了<code class="fe nr ns nt nu b">Renderer.render_object_rotation</code>方法。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nv mk l"/></div></figure><p id="772c" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">该函数为给定批次的成员生成一系列旋转矩阵<code class="fe nr ns nt nu b">r</code>。然后，它迭代地将这个范围的成员(以及一些默认的缩放和平移)传递给生成器网络的<code class="fe nr ns nt nu b">forward</code>方法，该方法由<code class="fe nr ns nt nu b">default.yaml</code>中的<code class="fe nr ns nt nu b">generator</code>键指定。如果你现在看<code class="fe nr ns nt nu b">im2scene.giraffe.models.__init__.py</code>，你会看到这个键映射到<code class="fe nr ns nt nu b">im2scene.giraffe.models.generator.Generator</code>。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nv mk l"/></div></figure><p id="90bb" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">现在，在我们看<code class="fe nr ns nt nu b">Generator.forward</code>的时候，请耐心等待。它接受各种可选的输入参数，比如<code class="fe nr ns nt nu b">transformations</code>、<code class="fe nr ns nt nu b">bg_rotation</code>和<code class="fe nr ns nt nu b">camera_matrices</code>，然后将它们传递给它的<code class="fe nr ns nt nu b">volume_render_image</code>方法。这里是合成魔法发生的地方。场景中所有物体的潜在代码，包括我们的背景，被分离成它们的形状和外观部分。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nv mk l"/></div></figure><p id="ea75" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">在本例中，这些潜在代码是使用<code class="fe nr ns nt nu b">torch.randn</code>函数随机生成的:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nv mk l"/></div></figure><p id="1bb7" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">这是解码器正向传递将3D点和摄像机观察方向映射到每个对象的<em class="ml"/>和RGB(特征)值的地方。一个不同的发生器被应用于背景(为了可读性，省略了细节)。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nv mk l"/></div></figure><p id="137f" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">然后，这些图由<em class="ml"> σ </em> max或使用<code class="fe nr ns nt nu b">composite_function</code>的平均值合成。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nv mk l"/></div></figure><p id="a473" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">最后，通过沿着射线向量用<em class="ml"> σ </em>体积对特征图进行加权来创建最终图像。最终结果是你在上面看到的那些动画之一的单个窗口中的单个帧(关于如何构造<code class="fe nr ns nt nu b">di</code>和<code class="fe nr ns nt nu b">ray_vector</code>的细节，见<code class="fe nr ns nt nu b">generator.py</code>)。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nv mk l"/></div></figure><p id="6d69" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">现在总结一下，让我们试着创建自己的渲染程序。这个将简单地结合深度平移和旋转来创建一个汽车从左向右旋转和滑动的效果。为此，我们对<code class="fe nr ns nt nu b">rendering.py</code>中的<code class="fe nr ns nt nu b">Renderer</code>类做了一些简单的添加。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nv mk l"/></div></figure><p id="7d06" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">将这些添加内容复制粘贴到<code class="fe nr ns nt nu b">rendering.py</code>中，然后创建如下配置文件<code class="fe nr ns nt nu b">configs/256res/cars_256_pretrained_wipeout.yaml</code>:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nv mk l"/></div></figure><p id="64c6" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">现在，如果您执行<code class="fe nr ns nt nu b">python render.py configs/256res/cars_256_pretrained_wipeout.yaml</code>,您应该能够产生如下所示的结果:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ki"><img src="../Images/18399ec059bb526d0578590541fa9582.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/0*QD2bccvZVmkJ04fw.gif"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">对象与汽车数据集“相消”。注意汽车从左向右移动时是如何旋转的(作者使用<a class="ae ku" href="https://github.com/autonomousvision/giraffe" rel="noopener ugc nofollow" target="_blank">https://github.com/autonomousvision/giraffe</a>，麻省理工学院许可创建)。</p></figure><h1 id="7a69" class="lr ls it bd lt lu lv lw lx ly lz ma mb jz mc ka md kc me kd mf kf mg kg mh mi bi translated">结论</h1><p id="6861" class="pw-post-body-paragraph kv kw it kx b ky my ju la lb mz jx ld le na lg lh li nb lk ll lm nc lo lp lq im bi translated">长颈鹿是最近对神经纤维和甘的大量研究中令人兴奋的新发现。辐射场表示描述了一个强大且可扩展的框架，利用该框架我们可以以可区分且可学习的方式构建3D场景。我希望对代码的深入研究对您有所帮助。如果是这样的话，我鼓励你自己去看一看源代码和作者的论文。</p><h1 id="80b7" class="lr ls it bd lt lu lv lw lx ly lz ma mb jz mc ka md kc me kd mf kf mg kg mh mi bi translated">参考</h1><p id="7c1d" class="pw-post-body-paragraph kv kw it kx b ky my ju la lb mz jx ld le na lg lh li nb lk ll lm nc lo lp lq im bi translated">[1] Ben Mildenhall，Pratul P. Srinivasan，Matthew Tancik，Jonathan T. Barron，Ravi Ramamoorthi，Ren Ng — <a class="ae ku" href="https://arxiv.org/abs/2003.08934" rel="noopener ugc nofollow" target="_blank"> NeRF:将场景表示为用于视图合成的神经辐射场(2020) </a>，ECCV 2020</p><p id="133f" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">[2] Michael Niemeyer，Andreas Geiger — <a class="ae ku" href="https://arxiv.org/abs/2011.12100" rel="noopener ugc nofollow" target="_blank">长颈鹿:将场景表示为合成生成神经特征场(2021) </a>，CVPR 2021</p></div></div>    
</body>
</html>