<html>
<head>
<title>Principal Components Analysis (PCA) In Python In Under 5 Minutes</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用Python在5分钟内完成主成分分析(PCA)</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/principal-components-analysis-pca-in-python-in-under-5-minutes-26baacb797f8?source=collection_archive---------17-----------------------#2021-08-08">https://towardsdatascience.com/principal-components-analysis-pca-in-python-in-under-5-minutes-26baacb797f8?source=collection_archive---------17-----------------------#2021-08-08</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><figure class="ip iq gp gr ir is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi io"><img src="../Images/13e9fa5462363f70d312bbec2016ec3b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*hlMPZRhHNn1qM9t4"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">弗兰基·查马基在<a class="ae jd" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><div class=""/><div class=""><h2 id="9528" class="pw-subtitle-paragraph kd jf jg bd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku dk translated">简明扼要的演练</h2></div><h1 id="edbb" class="kv kw jg bd kx ky kz la lb lc ld le lf km lg kn lh kp li kq lj ks lk kt ll lm bi translated">先决条件</h1><ul class=""><li id="6815" class="ln lo jg lp b lq lr ls lt lu lv lw lx ly lz ma mb mc md me bi translated">线性代数和矩阵分解的现有知识</li><li id="ac59" class="ln lo jg lp b lq mf ls mg lu mh lw mi ly mj ma mb mc md me bi translated">Python 3编程熟练程度</li></ul><h1 id="9301" class="kv kw jg bd kx ky kz la lb lc ld le lf km lg kn lh kp li kq lj ks lk kt ll lm bi translated">什么是主成分分析？</h1><p id="a9fa" class="pw-post-body-paragraph mk ml jg lp b lq lr kh mm ls lt kk mn lu mo mp mq lw mr ms mt ly mu mv mw ma ij bi translated">简而言之，对于具有大量特征的数据集，PCA可以说是最流行的降维算法。它用于移除高度相关的特征和冗余的特征，并修剪掉数据中的噪声。</p><p id="a78d" class="pw-post-body-paragraph mk ml jg lp b lq mx kh mm ls my kk mn lu mz mp mq lw na ms mt ly nb mv mw ma ij bi translated">最近的机器学习(ML)算法和神经网络可以很好地处理高维数据集，通常具有数十万或超过一百万个特征。请注意，由于计算技术(PCs/GPU)的最新发展，处理高维数据的效率越来越高，而无需因计算限制而降低其维数。然而，像PCA这样的降维算法仍然用于各种考虑，例如通过消除具有高相关性的特征、通过创建连续最大化方差的新的不相关变量来减少ML模型的过拟合。</p><h1 id="aa34" class="kv kw jg bd kx ky kz la lb lc ld le lf km lg kn lh kp li kq lj ks lk kt ll lm bi translated">获取主成分</h1><h2 id="b162" class="nc kw jg bd kx nd ne dn lb nf ng dp lf lu nh ni lh lw nj nk lj ly nl nm ll nn bi translated">“手动”方式</h2><p id="3efb" class="pw-post-body-paragraph mk ml jg lp b lq lr kh mm ls lt kk mn lu mo mp mq lw mr ms mt ly mu mv mw ma ij bi translated">回想一下文献，我们可以通过奇异值分解(SVD)获得主成分矩阵V，这是一种矩阵分解技术。在这里，我们有</p><figure class="np nq nr ns gt is gh gi paragraph-image"><div class="gh gi no"><img src="../Images/e17f84d5ff13d2da606c27cce9a049d8.png" data-original-src="https://miro.medium.com/v2/resize:fit:228/1*nzsrvW3B8yXQ7VR8c_paNg.gif"/></div></figure><p id="c73c" class="pw-post-body-paragraph mk ml jg lp b lq mx kh mm ls my kk mn lu mz mp mq lw na ms mt ly nb mv mw ma ij bi translated">其中<em class="nt"> V </em>中有<strong class="lp jh"> n </strong>个主成分。</p><p id="1f69" class="pw-post-body-paragraph mk ml jg lp b lq mx kh mm ls my kk mn lu mz mp mq lw na ms mt ly nb mv mw ma ij bi translated">换句话说，SVD将数据矩阵<em class="nt"> X </em>分解成三个矩阵:</p><figure class="np nq nr ns gt is gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/83391f49870bfb6e687fdfef3f4f3fea.png" data-original-src="https://miro.medium.com/v2/resize:fit:184/1*ivi-ZGU2KrfhFHOc6ufuEA.gif"/></div></figure><p id="c79c" class="pw-post-body-paragraph mk ml jg lp b lq mx kh mm ls my kk mn lu mz mp mq lw na ms mt ly nb mv mw ma ij bi translated">其中<em class="nt"> U </em>由左奇异向量组成，σ是与包含奇异值的<em class="nt"> X </em>具有相同维数的对角矩阵，<em class="nt"> V </em>包含右奇异向量/主分量。</p><p id="a01e" class="pw-post-body-paragraph mk ml jg lp b lq mx kh mm ls my kk mn lu mz mp mq lw na ms mt ly nb mv mw ma ij bi translated">在Python中，我们利用Numpy的<strong class="lp jh"> svd() </strong>函数来获得<em class="nt"> X: </em>的所有主成分</p><pre class="np nq nr ns gt nv nw nx ny aw nz bi"><span id="21bd" class="nc kw jg nw b gy oa ob l oc od">U, S, V_T = np.linalg.svd(X)</span><span id="d303" class="nc kw jg nw b gy oe ob l oc od"># transpose to get V, with number of components as V.shape[1]<br/>V = V_T.T</span><span id="16b6" class="nc kw jg nw b gy oe ob l oc od"># first component<br/>c1 = V[:, 0]</span><span id="4e01" class="nc kw jg nw b gy oe ob l oc od"># last (n-th) component<br/>cn = V[:, -1]</span></pre><p id="9933" class="pw-post-body-paragraph mk ml jg lp b lq mx kh mm ls my kk mn lu mz mp mq lw na ms mt ly nb mv mw ma ij bi translated">现在，我们准备使用找到的组件来减小<em class="nt"> X </em>的尺寸！为了将维度减少到<em class="nt"> d </em>，我们必须将数据投影到由第一个<em class="nt"> d </em>组件定义的超平面上。这样做导致超平面尽可能多地保留方差。在数学上，我们用矩阵<em class="nt"> W </em>计算<em class="nt"> X </em>的乘积，该矩阵由第一个<em class="nt"> d </em>主成分组成:</p><figure class="np nq nr ns gt is gh gi paragraph-image"><div class="gh gi of"><img src="../Images/f0d9b4531e3fb4802595a2a5a4080e67.png" data-original-src="https://miro.medium.com/v2/resize:fit:164/1*JXnYkdMCEPD1_2aDBWtwGg.gif"/></div></figure><p id="76cd" class="pw-post-body-paragraph mk ml jg lp b lq mx kh mm ls my kk mn lu mz mp mq lw na ms mt ly nb mv mw ma ij bi translated">在Python中，我们有以下内容:</p><pre class="np nq nr ns gt nv nw nx ny aw nz bi"><span id="185c" class="nc kw jg nw b gy oa ob l oc od">W = V[:, :d] # d is the number of components<br/>X_d = X.dot(W) </span></pre><h2 id="a538" class="nc kw jg bd kx nd ne dn lb nf ng dp lf lu nh ni lh lw nj nk lj ly nl nm ll nn bi translated">“Scikit-learn”方式</h2><p id="9008" class="pw-post-body-paragraph mk ml jg lp b lq lr kh mm ls lt kk mn lu mo mp mq lw mr ms mt ly mu mv mw ma ij bi translated">幸运的是，Scikit-learn Python库已经为我们设置好了这一切。假设我们之前已经定义了<em class="nt"> d </em>和<em class="nt"> X </em>。然后，我们可以使用下面的代码，通过利用第一个<em class="nt"> d </em>主分量来获得降维后的结果<em class="nt"> X </em>矩阵:</p><pre class="np nq nr ns gt nv nw nx ny aw nz bi"><span id="0343" class="nc kw jg nw b gy oa ob l oc od">from sklearn.decomposition import PCA</span><span id="cc38" class="nc kw jg nw b gy oe ob l oc od">pca = PCA(n_components = d)<br/>X_d = pca.fit_transform(X)</span></pre><p id="9996" class="pw-post-body-paragraph mk ml jg lp b lq mx kh mm ls my kk mn lu mz mp mq lw na ms mt ly nb mv mw ma ij bi translated">要访问单个组件，则略有不同。我们必须使用<strong class="lp jh">组件_ </strong>变量:</p><pre class="np nq nr ns gt nv nw nx ny aw nz bi"><span id="0f89" class="nc kw jg nw b gy oa ob l oc od"># first component<br/>c1 = pca.components_.T[:, 0]</span></pre><p id="bbb0" class="pw-post-body-paragraph mk ml jg lp b lq mx kh mm ls my kk mn lu mz mp mq lw na ms mt ly nb mv mw ma ij bi translated">这里有一个补充说明，组件是水平向量，所以你必须用一个<strong class="lp jh">来转置它们。T </strong>同上。</p><h1 id="1896" class="kv kw jg bd kx ky kz la lb lc ld le lf km lg kn lh kp li kq lj ks lk kt ll lm bi translated">选择正确的维度数量</h1><p id="8c07" class="pw-post-body-paragraph mk ml jg lp b lq lr kh mm ls lt kk mn lu mo mp mq lw mr ms mt ly mu mv mw ma ij bi translated">通常的做法是选择一定数量的维度，这些维度加起来构成方差中足够大的一部分。通常，惯例是保留95%的方差，但这取决于您的需求。例如，如果你试图减少维度来更好地可视化数据，你将不得不把它减少到2维或最多3维，因为人类不能在视觉上阅读超过3维。</p><h2 id="65d5" class="nc kw jg bd kx nd ne dn lb nf ng dp lf lu nh ni lh lw nj nk lj ly nl nm ll nn bi translated">“手动”方式</h2><p id="2572" class="pw-post-body-paragraph mk ml jg lp b lq lr kh mm ls lt kk mn lu mo mp mq lw mr ms mt ly mu mv mw ma ij bi translated">在这种方法中，我们迭代通过由组件解释的方差比例，直到达到某个阈值——解释为要保留的期望方差比例。我们使用以下函数来实现这一点:</p><pre class="np nq nr ns gt nv nw nx ny aw nz bi"><span id="3f40" class="nc kw jg nw b gy oa ob l oc od">def get_pca_components(pca, var):<br/>    cumm_var = pca.explained_variance_ratio_<br/>    total_var = 0.<br/>    N_COMPONENTS = 0<br/>    for i <strong class="nw jh">in</strong> cumm_var:<br/>        N_COMPONENTS += 1<br/>        total_var += i<br/>        if total_var &gt;= var:<br/>            break<br/>    return N_COMPONENTS</span></pre><p id="1c3a" class="pw-post-body-paragraph mk ml jg lp b lq mx kh mm ls my kk mn lu mz mp mq lw na ms mt ly nb mv mw ma ij bi translated">这里，<strong class="lp jh"> cumm_var </strong>包含每个分量解释的方差的比例，从最大到最小(第n个分量)。接下来，我们将<strong class="lp jh"> PCA() </strong>对象与数据<em class="nt"> X </em>相匹配，并提取分量数，以保留95%的方差:</p><pre class="np nq nr ns gt nv nw nx ny aw nz bi"><span id="e783" class="nc kw jg nw b gy oa ob l oc od">pca = PCA().fit(X)<br/>n_components = get_pca_components(pca, 0.95)</span></pre><p id="416b" class="pw-post-body-paragraph mk ml jg lp b lq mx kh mm ls my kk mn lu mz mp mq lw na ms mt ly nb mv mw ma ij bi translated">最后，我们可以转换数据以降低其维度:</p><pre class="np nq nr ns gt nv nw nx ny aw nz bi"><span id="789b" class="nc kw jg nw b gy oa ob l oc od">pca = PCA(n_components=n_components)<br/>X_d = pca.fit_transform(X)</span></pre><h2 id="4557" class="nc kw jg bd kx nd ne dn lb nf ng dp lf lu nh ni lh lw nj nk lj ly nl nm ll nn bi translated">“Scikit-learn”方式</h2><p id="23b9" class="pw-post-body-paragraph mk ml jg lp b lq lr kh mm ls lt kk mn lu mo mp mq lw mr ms mt ly mu mv mw ma ij bi translated">幸运的是，Scikit-learn让我们的生活变得更加简单。我们只需要两行代码:</p><pre class="np nq nr ns gt nv nw nx ny aw nz bi"><span id="6565" class="nc kw jg nw b gy oa ob l oc od">pca = PCA(n_components=0.95)<br/>X_d = pca.fit_transform(X)</span></pre><p id="ce21" class="pw-post-body-paragraph mk ml jg lp b lq mx kh mm ls my kk mn lu mz mp mq lw na ms mt ly nb mv mw ma ij bi translated">在<strong class="lp jh"> PCA() </strong>对象中，您可以将<strong class="lp jh"> n_components </strong>指定为0到1之间的浮点，以指定要保留的方差比率。如果您希望以这种方式进行降维，您还可以将组件的数量指定为一个正整数，最大为<em class="nt"> d </em>。</p><h1 id="94f4" class="kv kw jg bd kx ky kz la lb lc ld le lf km lg kn lh kp li kq lj ks lk kt ll lm bi translated">后续步骤</h1><p id="f1b3" class="pw-post-body-paragraph mk ml jg lp b lq lr kh mm ls lt kk mn lu mo mp mq lw mr ms mt ly mu mv mw ma ij bi translated">现在您已经获得了转换后的数据<strong class="lp jh"> X_d </strong>，您可以继续对其进行分析/可视化，并在其上拟合一个ML模型！</p></div></div>    
</body>
</html>