<html>
<head>
<title>NLP Preprocessing and Latent Dirichlet Allocation (LDA) Topic Modeling with Gensim</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">基于Gensim的自然语言处理预处理和潜在狄利克雷分配主题建模</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/nlp-preprocessing-and-latent-dirichlet-allocation-lda-topic-modeling-with-gensim-713d516c6c7d?source=collection_archive---------5-----------------------#2021-09-09">https://towardsdatascience.com/nlp-preprocessing-and-latent-dirichlet-allocation-lda-topic-modeling-with-gensim-713d516c6c7d?source=collection_archive---------5-----------------------#2021-09-09</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="d256" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">消化摘要:用主题建模对我的兴趣进行逆向工程(第2部分)</h2></div><h1 id="07aa" class="ki kj it bd kk kl km kn ko kp kq kr ks jz kt ka ku kc kv kd kw kf kx kg ky kz bi translated">激励主题建模</h1><p id="c9e2" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">在我的上一篇文章中，我介绍了如何从电子邮件简讯中提取元数据。具体来说，我演示了如何使用Gmail API为我的Medium Daily Digest时事通讯中的每篇建议文章收集标题、副标题、作者、出版物和会议记录(估计阅读时间中的文章长度)。有了这些数据，我的目标是调查:Medium的推荐引擎是如何如此了解我的阅读兴趣的，2)我的兴趣是如何随着时间的推移而演变的。</p><p id="2f34" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">主题建模是一种无监督的NLP技术，用于从形成文本语料库的文档集合中识别单词的重复模式。它对于发现文档集合中的模式、组织大块文本数据、从非结构化文本中检索信息等非常有用。在其最强大的形式中，它可以用于通过更多与用户行为相关的信号来提升推荐引擎，还可以用于深入了解推荐系统的内部工作方式，这正是我们将通过本文尝试实现的目标。</p><p id="1f76" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">那么它是如何工作的呢？通过检测词频和词与词之间的距离等模式，主题模型根据最常出现的词和表达方式对相似的文档进行聚类。有了这些信息，你可以很快推断出哪些文本与你感兴趣的主题相关。</p><p id="0bc2" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">主题可以被定义为“语料库中同现术语的重复模式”。一个好的主题模型可能会推断出“健康”、“医生”、“病人”、“医院”这些词都属于“医疗保健”的范畴。像“神经网络”、“反向传播”、“纪元”和“损失”这样的词可以被指定到“深度学习”主题箱中。然而，重要的是要注意，由于主题建模是一种无监督的方法，该模型可以学习某些单词彼此相关联，但是它没有在标记的数据上被训练来学习“健康”、“医生”、“病人”、“医院”作为已知类别与“医疗保健”相关联。相反，它取决于用户，要么从相似文档的未标记分组中获得洞察力，要么在包含分类主题标签的文本上训练主题分类模型，如果需要这些标签作为输出的话。</p><p id="154d" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">有许多技术方法可以用来执行主题建模。其中一些包括潜在狄利克雷分配(LDA)、文本排名、潜在语义分析(LSA)、非负矩阵分解(NMF)、弹球分配模型(PAM)等。在本文中，我们将着重于实现潜在的狄利克雷分配，这是最常见的方法。</p><h1 id="d229" class="ki kj it bd kk kl km kn ko kp kq kr ks jz kt ka ku kc kv kd kw kf kx kg ky kz bi translated">潜在狄利克雷分配综述</h1><p id="3685" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">LDA是一种使用变分例外最大化(VEM)算法开发的矩阵分解技术。LDA建立在这样一个前提之上，即每个文档可以用主题的概率分布来描述，每个主题可以用词的概率分布来描述。这可以让我们对主题之间的联系有一个更清晰的认识。</p><div class="mb mc md me gt ab cb"><figure class="mf mg mh mi mj mk ml paragraph-image"><div role="button" tabindex="0" class="mm mn di mo bf mp"><img src="../Images/446bf9a276b139048ea91b1c13be8bf8.png" data-original-src="https://miro.medium.com/v2/resize:fit:998/format:webp/1*g7I0QIB8-2u3YQOzVWhmbw.png"/></div></figure><figure class="mf mg ms mi mj mk ml paragraph-image"><div role="button" tabindex="0" class="mm mn di mo bf mp"><img src="../Images/597a0c36510745ee9c694918aa2d1fe8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1004/format:webp/1*jOOsGb9QrOcGp3uDpHjB5Q.png"/></div><p class="mt mu gj gh gi mv mw bd b be z dk mx di my mz translated">作者使用<a class="ae na" href="https://excalidraw.com" rel="noopener ugc nofollow" target="_blank"> excalidraw </a>创建的图表</p></figure></div><p id="fd8c" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">在向量空间中，任何语料库或文档集合都可以表示为由N个文档乘M个单词组成的文档-单词矩阵。该矩阵中每个单元的值表示文档<strong class="lc iu"> <em class="nb"> D_i </em> </strong>中单词<strong class="lc iu"> <em class="nb"> W_j </em> </strong>的频率。LDA算法通过将该文档-单词矩阵转换成两个低维矩阵，即分别代表文档-主题和主题-单词矩阵的M1和M2，来训练主题模型。如果你对LDA在数学层面上的工作方式有所了解——并且认真地不要强调这一点，因为除非你是一名研究人员或对计算机科学理论感兴趣，否则我们可以导入一个方便的Python包，创建一个LDA对象，然后就此结束——试着理解LDA依靠贝叶斯理论和两个关键的概率计算来更新属于主题<strong class="lc iu"><em class="nb"/></strong>的给定单词<strong class="lc iu"> <em class="nb"> w </em> </strong>的概率:</p><ul class=""><li id="8e99" class="nc nd it lc b ld lw lg lx lj ne ln nf lr ng lv nh ni nj nk bi translated"><strong class="lc iu"> <em class="nb"> p(话题t|文档d)</em></strong><em class="nb">—</em><strong class="lc iu"><em class="nb"/></strong>文档<strong class="lc iu"> <em class="nb"> d </em> </strong>中分配给话题<strong class="lc iu"><em class="nb"/></strong>的单词比例</li><li id="2e0f" class="nc nd it lc b ld nl lg nm lj nn ln no lr np lv nh ni nj nk bi translated"><strong class="lc iu"><em class="nb">p(word w | topic t)</em></strong>—捕获有多少文档因为word <strong class="lc iu"> <em class="nb"> w </em> </strong>而在topic<strong class="lc iu"><em class="nb"/></strong>中</li></ul><h1 id="6990" class="ki kj it bd kk kl km kn ko kp kq kr ks jz kt ka ku kc kv kd kw kf kx kg ky kz bi translated">数据刷新程序</h1><p id="343d" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">从本系列的第1部分，我们能够获得下面的数据集。删除重复项后，我们剩下7.1k行。查看下面嵌入的数据面板！</p><figure class="mb mc md me gt mg"><div class="bz fp l di"><div class="nq nr l"/></div></figure><p id="8fb8" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">在我们开始任何主题建模之前，让我们确保安装并导入我们将需要的所有库。</p><pre class="mb mc md me gt ns nt nu nv aw nw bi"><span id="be91" class="nx kj it nt b gy ny nz l oa ob"># Essentials<br/>import base64<br/>import re<br/>from tqdm import tqdm<br/>import numpy as np<br/>import pandas as pd<br/>import matplotlib.pyplot as plt<br/>import datapane as dp<br/>dp.login(token='INSERT_TOKEN_HERE')</span><span id="e264" class="nx kj it nt b gy oc nz l oa ob"># Gensim and LDA<br/>import gensim<br/>import gensim.corpora as corpora<br/>from gensim.utils import simple_preprocess<br/>from gensim.models import CoherenceModel<br/>from gensim.parsing.preprocessing import STOPWORDS<br/>import pyLDAvis<br/>import pyLDAvis.gensim  # don't skip this</span><span id="58df" class="nx kj it nt b gy oc nz l oa ob"># NLP stuff<br/>import contractions<br/>import demoji<br/>import string<br/>import nltk<br/>from nltk.stem import WordNetLemmatizer, SnowballStemmer<br/>from nltk.stem.porter import *<br/>from nltk.corpus import stopwords<br/>stop_words = stopwords.words('english')<br/>nltk.download('wordnet')<br/>import spacy</span><span id="7843" class="nx kj it nt b gy oc nz l oa ob"># Plotting tools<br/>from bokeh.plotting import figure, output_file, show<br/>from bokeh.models import Label<br/>from bokeh.io import output_notebook<br/>import matplotlib.colors as mcolors<br/>import matplotlib.pyplot as plt<br/>%matplotlib inline</span><span id="8721" class="nx kj it nt b gy oc nz l oa ob"># Miscellaneous<br/>from sklearn.manifold import TSNE<br/>from pprint import pprint</span></pre><h1 id="d751" class="ki kj it bd kk kl km kn ko kp kq kr ks jz kt ka ku kc kv kd kw kf kx kg ky kz bi translated">自然语言处理预处理</h1><p id="afda" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">任何NLP相关项目中最关键的步骤之一是对所有文档进行列式预处理。虽然有一些标准的清理方法，一般来说，可以适用于任何语料库，但也有相当数量的直觉驱动的决定，我们必须做出。例如，表情符号有意义吗？停用词呢？数字应该保持原样、转换成文本还是完全删除？在所有文档中出现超过80%的单词应该被忽略还是被调查？同一个词根的变体应该词干化还是词条化？</p><figure class="mb mc md me gt mg"><div class="bz fp l di"><div class="od nr l"/></div></figure><p id="2c4a" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">上面的函数将pandas series对象作为输入，并应用lambda函数，这些函数将文本转换为小写，删除表情符号，扩展缩写，删除标点符号，删除停用词，执行词汇化，并删除太短的单词(长度小于3个字符)。</p><h2 id="14ec" class="nx kj it bd kk oe of dn ko og oh dp ks lj oi oj ku ln ok ol kw lr om on ky oo bi translated">停用词删除</h2><p id="5cc9" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">当谈到停用词删除时，许多人认为这是一个实用的预处理步骤，总是利大于弊。然而，我强烈建议您实际打印出您决定使用的停用词列表(<em class="nb">注意:我使用nltk . corpus . stop words . words(' English ')set</em>)，并根据上下文进行调整以优化您的语料库。一个常用的例子是“not ”,这个词在删除前需要仔细斟酌。当文本数据源包含来自用户的基于意见的评论或其他详细描述时，删除“not”会混淆数据中的主要信号来源。在这里，我们只是在看中型文章的标题和副标题，所以无论我们决定保留“不”还是放弃它都没有太大关系，但我将在原则上保留它。</p><h2 id="ab96" class="nx kj it bd kk oe of dn ko og oh dp ks lj oi oj ku ln ok ol kw lr om on ky oo bi translated">词汇化与词干化</h2><p id="707e" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">对于那些不熟悉词干化和词干化的人，您可以将词干化视为将具有相同词根或词干但具有不同词形变化或词义派生词的单词组合在一起的过程——第三人称的单词变为第一人称，过去时态和将来时态的动词变为现在时态。词干提取是通过去除后缀或前缀将单词的词形变化减少到其词根形式的过程。与词干化不同，词汇化确保词根或词汇是属于英语的有效单词。例如，<em class="nb">跑</em>、<em class="nb">跑、</em>和<em class="nb">跑</em>都将映射到词条<em class="nb">跑</em>。为了说明词汇化和词干化的区别，单词<em class="nb"> intelligence </em>和<em class="nb"> intelligent </em>的词干是<em class="nb"> intellig </em>，实际上并不是一个有效的英语单词。</p><h2 id="0e2d" class="nx kj it bd kk oe of dn ko og oh dp ks lj oi oj ku ln ok ol kw lr om on ky oo bi translated">二元模型和三元模型</h2><p id="6a0d" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">一个<strong class="lc iu"> n-gram </strong>是来自给定文本或文档样本的n个项目的连续序列。<strong class="lc iu">二元模型</strong>是在文档中频繁出现的两个词(例如<em class="nb">数据_科学</em>、<em class="nb">机器_学习</em>、<em class="nb">情感_分析</em>)。<strong class="lc iu">三元组</strong>是3个频繁出现的词(例如<em class="nb">面向数据科学</em>、<em class="nb">自然语言处理</em>、<em class="nb">探索性数据分析</em>)</p><p id="fb17" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">Gensim的<code class="fe op oq or nt b">Phrases</code>模型可以构建和实现二元模型、三元模型、四元模型等等。<code class="fe op oq or nt b">Phrases</code>的两个重要参数是<code class="fe op oq or nt b">min_count</code>和<code class="fe op oq or nt b">threshold</code>。这些参数的值越高，单词就越难组合成二元模型和三元模型。</p></div><div class="ab cl os ot hx ou" role="separator"><span class="ov bw bk ow ox oy"/><span class="ov bw bk ow ox oy"/><span class="ov bw bk ow ox"/></div><div class="im in io ip iq"><h1 id="65b0" class="ki kj it bd kk kl oz kn ko kp pa kr ks jz pb ka ku kc pc kd kw kf pd kg ky kz bi translated">用Gensim进行主题建模</h1><p id="35c9" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated"><code class="fe op oq or nt b">gensim</code> Python库使得创建LDA主题模型简单得可笑。我们唯一要做的准备工作是创建一个字典和语料库。</p><p id="02b6" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">字典是单词id到单词的映射。为了创建我们的字典，我们可以创建一个内置的<code class="fe op oq or nt b">gensim.corpora.Dictionary</code>对象。从那里开始，<code class="fe op oq or nt b">filter_extremes()</code>方法是必不可少的，以确保我们在字典中获得期望的频率和表征。</p><pre class="mb mc md me gt ns nt nu nv aw nw bi"><span id="9425" class="nx kj it nt b gy ny nz l oa ob">id2word = corpora.Dictionary(data_preprocessed)<br/>id2word.filter_extremes(no_below=15, no_above=0.4, keep_n=80000)</span></pre><p id="ea04" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated"><code class="fe op oq or nt b">filter_extremes()</code>方法有3个参数。让我们来分解一下这些是什么意思:</p><ul class=""><li id="08c2" class="nc nd it lc b ld lw lg lx lj ne ln nf lr ng lv nh ni nj nk bi translated">过滤掉出现在少于15个文档中的令牌</li><li id="320c" class="nc nd it lc b ld nl lg nm lj nn ln no lr np lv nh ni nj nk bi translated">过滤掉出现在超过40%的文档中的标记</li><li id="755b" class="nc nd it lc b ld nl lg nm lj nn ln no lr np lv nh ni nj nk bi translated">完成上述两个步骤后，只保留前80，000个最常用的令牌</li></ul><p id="2dc6" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">语料库本质上是词id到词频的映射。对于每个文档，创建一个单词包表示:</p><pre class="mb mc md me gt ns nt nu nv aw nw bi"><span id="a1dc" class="nx kj it nt b gy ny nz l oa ob">corpus = [id2word.doc2bow(text) for text in texts]</span></pre><p id="c0b2" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">使用字典和语料库，我们可以获得一个样本文档的人类可读的词频映射:</p><pre class="mb mc md me gt ns nt nu nv aw nw bi"><span id="ce86" class="nx kj it nt b gy ny nz l oa ob">[[('better', 1),<br/>  ('creating', 1),<br/>  ('data_visualization', 1),<br/>  ('design', 1),<br/>  ('principle', 1),<br/>  ('towards', 1)]]</span></pre><p id="1ef7" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">非常漂亮…还要注意我们的一个二元模型(<em class="nb"> data_visualization </em>)刚刚出现了！</p><h2 id="23fd" class="nx kj it bd kk oe of dn ko og oh dp ks lj oi oj ku ln ok ol kw lr om on ky oo bi translated">潜在狄利克雷分配</h2><p id="3b89" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">信不信由你，构建LDA模型的代码是一行程序。</p><pre class="mb mc md me gt ns nt nu nv aw nw bi"><span id="8cab" class="nx kj it nt b gy ny nz l oa ob">lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,<br/>                                           id2word=id2word,<br/>                                           num_topics=10, <br/>                                           random_state=100,<br/>                                           update_every=1,<br/>                                           chunksize=100,<br/>                                           passes=10,<br/>                                           alpha='auto',<br/>                                           per_word_topics=True)</span></pre><p id="4ae5" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">我们已经讨论了语料库和字典参数。我们最初会将<code class="fe op oq or nt b">num_topics</code>固定为10。<code class="fe op oq or nt b">random_state</code>非常简单明了，不会对模型产生巨大影响；它只是用来获得不随笔记本的每次执行而改变的结果。<code class="fe op oq or nt b">update_every</code>决定模型参数应该多久更新一次。<code class="fe op oq or nt b">chunksize</code>是每个训练组块中要使用的文档数。<code class="fe op oq or nt b">passes</code>是培训通过的总次数。最后一个参数<code class="fe op oq or nt b">per_word_topics</code>，当设置为True时，计算主题列表，按照每个单词最可能的主题的降序排序，以及它们的phi值乘以特征长度(即字数)。</p><p id="2e28" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">现在我们已经有了“训练过的”主题模型(<em class="nb">记住:这是一种无监督的方法…把它想象成更像是聚类而不是别的)</em>，让我们打印出10个主题中的前10个关键词。</p><figure class="mb mc md me gt mg gh gi paragraph-image"><div role="button" tabindex="0" class="mm mn di mo bf mp"><div class="gh gi pe"><img src="../Images/5b46563e6edbd2d05369fae8a4e91aa3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EjZiL06DcYpRSzxuM8hweA.png"/></div></div><p class="mt mu gj gh gi mv mw bd b be z dk translated">作者图片</p></figure><p id="c4e9" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">有意思，所以python显然是这个语料库中的一个流行关键词，这是理所当然的！看起来我们有一个关于学习好设计的话题，一个关于涉及twitter数据的机器学习文章(也许？)，如何在研究中取得成功，生产力技巧，开发者故事等。这是一个良好的开端，但它还没有给我们提供一个令人满意的整体图景。</p><p id="d127" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">你可能会问的下一个问题是，我们如何知道我们的模型实际上有多好，仅仅从构建它的那一行代码中？</p><h2 id="173b" class="nx kj it bd kk oe of dn ko og oh dp ks lj oi oj ku ln ok ol kw lr om on ky oo bi translated">评估指标</h2><p id="d1d5" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">在主题建模领域，通常使用两个评估指标:<strong class="lc iu">困惑度</strong>和<strong class="lc iu">连贯性</strong>。</p><p id="6098" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated"><a class="pf pg ep" href="https://medium.com/u/cc7314ace45c?source=post_page-----713d516c6c7d--------------------------------" rel="noopener" target="_blank"> Shashank Kapadia </a>在他的文章《评估主题模型:潜在的狄利克雷分配(LDA)》中比我更好地解释了这些指标。他将它们定义如下:</p><blockquote class="ph pi pj"><p id="8f97" class="la lb nb lc b ld lw ju lf lg lx jx li pk ly ll lm pl lz lp lq pm ma lt lu lv im bi translated">困惑度表示一个模型对它以前没有见过的新数据有多惊讶，它是以一个被拒绝的测试集的归一化对数似然性来衡量的。</p><p id="fe0b" class="la lb nb lc b ld lw ju lf lg lx jx li pk ly ll lm pl lz lp lq pm ma lt lu lv im bi translated">连贯性衡量主题中高分词汇之间的语义相似程度。如果你感兴趣的话，<strong class="lc iu"> c_v </strong>测量是基于一个滑动窗口，顶部单词的一组分割和一个间接确认测量，该测量使用标准化的逐点互信息(NPMI)和余弦相似性。</p></blockquote><p id="3513" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">要计算这些值，只需执行以下代码:</p><figure class="mb mc md me gt mg"><div class="bz fp l di"><div class="od nr l"/></div></figure><p id="440d" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">我们的标准LDA模型的初始混淆度和一致性分别为-6.68和0.4。展望未来，我们希望将困惑最小化，将连贯性最大化。</p><h2 id="2da7" class="nx kj it bd kk oe of dn ko og oh dp ks lj oi oj ku ln ok ol kw lr om on ky oo bi translated">皮尔戴维斯</h2><p id="2b2d" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">现在你可能想知道，除了打印出关键词，或者，但愿不会，另一个单词云之外，我们如何可视化我们的主题。嗯，幸运的是，我们得到了皮勒戴维斯的礼物！pyLDAvis是一个用于交互式主题模型可视化的Python库。在一小段代码中，我们可以获得这些交互式子图，这些子图描述了2D平面上的主题之间的距离以及该主题中前30个最相关和最突出的术语(这两个指标都在下面可视化的右下角进行了数学定义)。</p><figure class="mb mc md me gt mg gh gi paragraph-image"><div class="gh gi pn"><img src="../Images/5904bae5cfc5e1c0d2511409f514f683.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/1*PdMhhL3RhAYNuakHEi8LbQ.gif"/></div><p class="mt mu gj gh gi mv mw bd b be z dk translated">作者拍摄的交互式可视化的屏幕记录</p></figure><p id="b90a" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">从上面的GIF图中可以明显看出，主题1和主题2彼此之间的距离最大，与其余8个主题之间的距离也最大，这8个主题基本上彼此相似。虽然这是一个超级漂亮的可视化方法，有助于了解哪些主题关系最密切，哪些单词携带最强的信号，但当前的模型似乎不是很智能。换句话说，悬停在每个主题上并不能完全帮助我们理解这个语料库中的所有主题。这表明可能是时候回到绘图板，尝试更复杂的LDA模型了。</p></div><div class="ab cl os ot hx ou" role="separator"><span class="ov bw bk ow ox oy"/><span class="ov bw bk ow ox oy"/><span class="ov bw bk ow ox"/></div><div class="im in io ip iq"><h1 id="75c1" class="ki kj it bd kk kl oz kn ko kp pa kr ks jz pb ka ku kc pc kd kw kf pd kg ky kz bi translated">LDA Mallet模型</h1><p id="8971" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">MALLET由Andrew McCallum编写，是一个基于Java的包，用于统计自然语言处理、文档分类、聚类、主题建模、信息提取和其他对文本的机器学习应用。MALLET主题建模工具包包含潜在的Dirichlet分配的高效的、基于采样的实现。主要的优化差异是Gensim(vanilla)LDA使用变分贝叶斯采样方法，该方法比Mallet的Gibbs采样更快，但精度较低。</p><p id="e954" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">在我们开始使用Gensim for LDA之前，我们必须在我们的系统上下载mallet-2.0.8.zip包并将其解压缩。这个包的下载和更详细的说明可以在<a class="ae na" href="http://mallet.cs.umass.edu/download.php" rel="noopener ugc nofollow" target="_blank">这里</a>找到。安装并解压缩后，将<code class="fe op oq or nt b">mallet_path</code>变量设置为您的下载当前驻留在本地机器上的文件路径。一旦完成，我们就准备好建立我们的LDA Mallet模型了！</p><p id="f153" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">Python为潜在的狄利克雷分配(LDA)提供了Gensim包装器。该包装器的语法是<code class="fe op oq or nt b">gensim.models.wrappers.LdaMallet</code>。该模块允许从训练语料库进行LDA模型估计，以及推断新的、看不见的文档上的主题分布。</p><pre class="mb mc md me gt ns nt nu nv aw nw bi"><span id="d70a" class="nx kj it nt b gy ny nz l oa ob">mallet_path = '~/Downloads/mallet-2.0.8/bin/mallet' # update this path<br/>ldamallet = gensim.models.wrappers.LdaMallet(mallet_path, <br/>                                             corpus=corpus, <br/>                                             num_topics=10,<br/>                                             id2word=id2word,<br/>                                             random_seed=42,<br/>                                             workers=6)</span></pre><p id="29bb" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">最初，我们可以将主题的数量设置为10，以获得Gensim的LDA与LDA Mallet的比较。从我们的LdaMallet模型中不可能得到一个困惑分数，但是可以得到一个连贯分数。下面是预览主题和计算连贯性分数的一些代码:</p><figure class="mb mc md me gt mg"><div class="bz fp l di"><div class="od nr l"/></div></figure><p id="04f7" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">接下来，让我们以编程方式确定我们将文档划分成的主题的最佳数量。为了辨别这个数字，我们将希望用越来越多的主题来测试几个模型，然后寻找产生足够高的一致性值的最少数量的主题。</p><figure class="mb mc md me gt mg"><div class="bz fp l di"><div class="od nr l"/></div></figure><figure class="mb mc md me gt mg gh gi paragraph-image"><div role="button" tabindex="0" class="mm mn di mo bf mp"><div class="gh gi po"><img src="../Images/f0b5ebb016eb7f4af647587b36c1b52d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*D3xlZmawuTiai4idn5ZRSg.png"/></div></div><p class="mt mu gj gh gi mv mw bd b be z dk translated">作者制作的Matplotlib图形</p></figure><p id="0d09" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">从上面的图表中，我们可以看到我们的一致性分数上升到大约0.38，然后到大约0.43，然后到大约0.45。由用户选择一些他们认为合适的主题。我将继续选择第二个“拐点”,即x = 24个主题，这对应于大约0.43的一致性分数。</p><p id="810d" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">我们可以像以前一样预览我们的主题，但我们也可以直接切入正题，制作一个pyLDAvis。为了实现这一点，我们实际上还需要执行一个额外的步骤:用Mallet模型的学习参数创建一个常规的LDA模型，这样我们就可以将常规的gensim LDA对象传递给pyLDAvis。这可以通过一个简单的函数来完成，如果你愿意的话，这个函数包含在完整的笔记本中。</p><figure class="mb mc md me gt mg gh gi paragraph-image"><div class="gh gi pn"><img src="../Images/050b9fb75c7676b2edfe6792bfb1d89e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/1*N3qx_aD5tNtWIJacX_Viuw.gif"/></div><p class="mt mu gj gh gi mv mw bd b be z dk translated">作者拍摄的交互式可视化屏幕记录</p></figure><p id="75a7" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">上面的主题间距离图比我们最初的模型更有趣！我们可以看到更多的话题分散在二维平面上。特别是，有2-3组主题值得分析。不考虑可视化中随机分配的主题标签，左下象限中的3个主题似乎主要由属于Python教程、指南和简介的中型文章组成。最右边的两个主题的特点是它们的关键词技术含量较低，似乎更适合商业、营销、设计趋势和技能。最右下方的主题似乎与构建可视化和以数据为中心的web应用程序有关。</p><p id="bb96" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">从这里我们可以找到每篇文章标题和副标题的最显性主题，也可以得到文档被显性主题表现的程度。这些新属性包含在下面的数据表中:</p><figure class="mb mc md me gt mg"><div class="bz fp l di"><div class="nq nr l"/></div></figure><p id="4154" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">我们还可以通过获取每个主题的文档数量、关键字和最具代表性的文档来更好地理解每个主题。</p><figure class="mb mc md me gt mg"><div class="bz fp l di"><div class="od nr l"/></div></figure><p id="d361" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">上面提供了将主题信息的数据帧与这些关键属性进行辩论的代码。请注意，不断重置索引以确保各列根据适当的主题编号对齐是很重要的。这里可以利用熊猫数据帧的<code class="fe op oq or nt b">merge</code>方法。</p><figure class="mb mc md me gt mg"><div class="bz fp l di"><div class="nq nr l"/></div></figure><p id="5634" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">从上面包含描述每个主题的特性的表中，我们可以了解到主题0包含了最多的文章。包含第二大数量文档的主题是不幸的，但也是非常直观的，这些文章谈论“疫情”、“covid”和/或“冠状病毒”“Representative_Doc”列应该有所保留，但是在给定一些主题外领域上下文的情况下，了解标题字符串如何包含相关主题关键字的子集是很有用的。</p><h2 id="30e2" class="nx kj it bd kk oe of dn ko og oh dp ks lj oi oj ku ln ok ol kw lr om on ky oo bi translated">可视化的视觉:局限性的讨论</h2><p id="71e6" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">在项目的这一点上，我们已经达到了一个足够功能的主题模型，它利用强大的信号源以一种有意义的方式聚集或分离我们的文档，我们已经通过检查关键字、主题间距离和代表性文档验证了这一点。求助于特定主题的文字云作为进一步总结每个主题的手段可能会变得非常诱人，但我想说明的是，需要更好的可视化技术。</p><figure class="mb mc md me gt mg gh gi paragraph-image"><div role="button" tabindex="0" class="mm mn di mo bf mp"><div class="gh gi pp"><img src="../Images/26a9273a64753d3fbef9f90a54666a15.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VsUudkeJNT4BehZdgJqMCQ.png"/></div></div><p class="mt mu gj gh gi mv mw bd b be z dk translated">作者制作的Matplotlib子图图形</p></figure><p id="fd3d" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">…如果出于某种原因，你仍然倾向于单词云，也许我们至少可以达成共识，随着主题数量的增加，视觉检查单词云变得越来越难以承受。所以如果你准备好从wordclouds毕业，那就拉着我的手跟我走吧。让我们找到一个更好的可视化技术。</p><h2 id="9a7e" class="nx kj it bd kk oe of dn ko og oh dp ks lj oi oj ku ln ok ol kw lr om on ky oo bi translated">降维</h2><p id="bef2" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">由Laurens van der Maaten开发的t-分布式随机邻居嵌入(t-SNE)是一种非线性降维技术，特别适合于高维数据集的可视化。它广泛应用于图像处理、自然语言处理、基因组数据和语音处理。</p><p id="4221" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">那么它是如何工作的，我们为什么要使用它呢？简而言之，根据非常有用的<a class="ae na" href="https://www.datacamp.com/community/tutorials/introduction-t-sne" rel="noopener ugc nofollow" target="_blank"> DataCamp介绍</a>，t-SNE最小化了两种分布之间的差异:一种分布测量输入对象的成对相似性，另一种分布测量嵌入中相应低维点的成对相似性。以这种方式，t-SNE将多维数据映射到较低维度的空间，并试图通过基于具有多个特征的数据点的相似性识别观察到的聚类来发现数据中的模式。</p><figure class="mb mc md me gt mg"><div class="bz fp l di"><div class="nq nr l"/></div></figure><p id="41b4" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">使用我们的24主题Mallet模型，我们可以看到主题是如何很好地聚集在二维平面上的。此外，代表单个文章的每个圆圈的大小由主题表示百分比编码。因此，较大的圆圈与它们所属的主题关联更强——这些较大圆圈的热点或集群可以容易地被识别为可能共享内聚主题的相关文章。我们还可以将鼠标悬停在每个圆圈上，以查看文章标题、副标题和主题表示百分比。</p><p id="ad41" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">这种方法的一个缺点是，在执行该过程之后，输入特征不再是可识别的。因此，它主要是一种数据探索和可视化技术。此外，如果不花大量时间在圆圈上徘徊，或者对主题编号到关键字的映射有内在的理解，很难从这个t-SNE散点图中获得对我们主题的一目了然的理解。</p><h1 id="c1d6" class="ki kj it bd kk kl km kn ko kp kq kr ks jz kt ka ku kc kv kd kw kf kx kg ky kz bi translated">结论</h1><p id="2302" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">虽然我们在理解我建议的媒体文章涵盖的一般主题方面取得了进展，但我最初的研究问题是探索我的阅读兴趣是如何随着时间的推移而演变的。我将在这篇文章的结尾声明，我对实现一个产生时态洞察力的主题模型的追求仍然是不完整的！在我的下一篇文章中，我将介绍BERTopic，并与大家分享我的阅读兴趣和阅读行为在过去一两年中如何经历了一些有趣的转变。</p><p id="81b3" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">在那之前，快乐的阅读、学习和探索！我希望通过这篇全面的教程，你对Gensim的主题建模有所了解。所有代码都可以在下面链接的GitHub资源库中找到。</p><div class="pq pr gp gr ps pt"><a href="https://github.com/sejaldua/digesting-the-digest" rel="noopener  ugc nofollow" target="_blank"><div class="pu ab fo"><div class="pv ab pw cl cj px"><h2 class="bd iu gy z fp py fr fs pz fu fw is bi translated">GitHub-sejaldua/digesting-the-digest:使用Gmail API对我推荐的媒体进行主题建模…</h2><div class="qa l"><h3 class="bd b gy z fp py fr fs pz fu fw dk translated">使用Gmail API对我推荐的媒体进行主题建模</h3></div><div class="qb l"><p class="bd b dl z fp py fr fs pz fu fw dk translated">github.com</p></div></div><div class="qc l"><div class="qd l qe qf qg qc qh mq pt"/></div></div></a></div><h1 id="d46c" class="ki kj it bd kk kl km kn ko kp kq kr ks jz kt ka ku kc kv kd kw kf kx kg ky kz bi translated">参考</h1><div class="pq pr gp gr ps pt"><a rel="noopener follow" target="_blank" href="/latent-dirichlet-allocation-lda-9d1cd064ffa2"><div class="pu ab fo"><div class="pv ab pw cl cj px"><h2 class="bd iu gy z fp py fr fs pz fu fw is bi translated">潜在狄利克雷分配</h2><div class="qa l"><h3 class="bd b gy z fp py fr fs pz fu fw dk translated">一种用于发现抽象主题的统计模型，又称主题建模。</h3></div><div class="qb l"><p class="bd b dl z fp py fr fs pz fu fw dk translated">towardsdatascience.com</p></div></div><div class="qc l"><div class="qi l qe qf qg qc qh mq pt"/></div></div></a></div><div class="pq pr gp gr ps pt"><a href="https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/" rel="noopener  ugc nofollow" target="_blank"><div class="pu ab fo"><div class="pv ab pw cl cj px"><h2 class="bd iu gy z fp py fr fs pz fu fw is bi translated">用Gensim在Python中进行主题建模</h2><div class="qa l"><h3 class="bd b gy z fp py fr fs pz fu fw dk translated">主题建模是一种从大量文本中理解和提取隐藏主题的技术。潜在的狄利克雷…</h3></div><div class="qb l"><p class="bd b dl z fp py fr fs pz fu fw dk translated">www.machinelearningplus.com</p></div></div><div class="qc l"><div class="qj l qe qf qg qc qh mq pt"/></div></div></a></div><div class="pq pr gp gr ps pt"><a href="http://mallet.cs.umass.edu" rel="noopener  ugc nofollow" target="_blank"><div class="pu ab fo"><div class="pv ab pw cl cj px"><h2 class="bd iu gy z fp py fr fs pz fu fw is bi translated">MALLET主页</h2><div class="qa l"><h3 class="bd b gy z fp py fr fs pz fu fw dk translated">MALLET是一个基于Java的软件包，用于统计自然语言处理、文档分类、聚类、主题…</h3></div><div class="qb l"><p class="bd b dl z fp py fr fs pz fu fw dk translated">mallet.cs.umass.edu</p></div></div><div class="qc l"><div class="qk l qe qf qg qc qh mq pt"/></div></div></a></div><div class="pq pr gp gr ps pt"><a rel="noopener follow" target="_blank" href="/basic-nlp-on-the-texts-of-harry-potter-topic-modeling-with-latent-dirichlet-allocation-f3c00f77b0f5"><div class="pu ab fo"><div class="pv ab pw cl cj px"><h2 class="bd iu gy z fp py fr fs pz fu fw is bi translated">《哈利·波特》文本的解读</h2><div class="qa l"><h3 class="bd b gy z fp py fr fs pz fu fw dk translated">基于潜在狄利克雷分配的主题建模</h3></div><div class="qb l"><p class="bd b dl z fp py fr fs pz fu fw dk translated">towardsdatascience.com</p></div></div><div class="qc l"><div class="ql l qe qf qg qc qh mq pt"/></div></div></a></div><div class="pq pr gp gr ps pt"><a rel="noopener follow" target="_blank" href="/mapping-the-tech-world-with-t-sne-7be8e1703137"><div class="pu ab fo"><div class="pv ab pw cl cj px"><h2 class="bd iu gy z fp py fr fs pz fu fw is bi translated">用t-SNE描绘科技世界</h2><div class="qa l"><h3 class="bd b gy z fp py fr fs pz fu fw dk translated">我们用t-SNE算法分析了20万篇科技新闻</h3></div><div class="qb l"><p class="bd b dl z fp py fr fs pz fu fw dk translated">towardsdatascience.com</p></div></div><div class="qc l"><div class="qm l qe qf qg qc qh mq pt"/></div></div></a></div></div></div>    
</body>
</html>