<html>
<head>
<title>Integrated Gradients from Scratch</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">从头开始集成渐变</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/integrated-gradients-from-scratch-b46311e4ab4?source=collection_archive---------13-----------------------#2021-10-14">https://towardsdatascience.com/integrated-gradients-from-scratch-b46311e4ab4?source=collection_archive---------13-----------------------#2021-10-14</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="ad7e" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">一个直观的算法来解释任何深度学习模型</h2></div><p id="3c25" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">当阅读一篇论文时，我认为深入相关代码以更好地理解它总是好的。虽然代码通常很长，有很多优化和实用函数，很容易丢失。在这篇文章中，我将尝试用几行代码复制论文中的算法。</p></div><div class="ab cl lb lc hu ld" role="separator"><span class="le bw bk lf lg lh"/><span class="le bw bk lf lg lh"/><span class="le bw bk lf lg"/></div><div class="ij ik il im in"><h2 id="3404" class="li lj iq bd lk ll lm dn ln lo lp dp lq ko lr ls lt ks lu lv lw kw lx ly lz ma bi translated">介绍</h2><p id="bf16" class="pw-post-body-paragraph kf kg iq kh b ki mb jr kk kl mc ju kn ko md kq kr ks me ku kv kw mf ky kz la ij bi translated">可解释的人工智能是当今的热门话题。如果一家公司想要采用人工智能解决方案，并使用它来做出直接影响客户的决定，它还需要有能力向客户和监管机构解释这些决定。有各种各样的方法来解释一个模型，特别是他们可以分为模型不可知和模型特定的方法。例如，当我们用一个简单且可解释的模型在局部近似一个复杂的模型时，LIME是模型不可知的，因为它不需要知道模型的内部结构，但是您可以简单地将<em class="mg"> predict </em>函数传递给它并得到解释。相反，像集成梯度这样的方法是特定于模型的，它们需要知道内部模型，以便计算组成模型的层的梯度。</p><p id="6610" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这是使用集成梯度解释深度神经网络模型系列的第一篇文章。在第二步中，我将覆盖<a class="ae mh" href="https://medium.com/@alexml0123/layer-conductance-from-scratch-df53005e08b8" rel="noopener"> <em class="mg">层的电导</em> </a>，再次使用Pytorch <em class="mg">亲笔签名的</em>来计算渐变。</p></div><div class="ab cl lb lc hu ld" role="separator"><span class="le bw bk lf lg lh"/><span class="le bw bk lf lg lh"/><span class="le bw bk lf lg"/></div><div class="ij ik il im in"><h2 id="6835" class="li lj iq bd lk ll lm dn ln lo lp dp lq ko lr ls lt ks lu lv lw kw lx ly lz ma bi translated">了解集成渐变</h2><p id="8fd6" class="pw-post-body-paragraph kf kg iq kh b ki mb jr kk kl mc ju kn ko md kq kr ks me ku kv kw mf ky kz la ij bi translated">为了理解积分梯度，让我们首先从如何解释一个简单的线性模型开始:</p><figure class="mj mk ml mm gt mn gh gi paragraph-image"><div class="gh gi mi"><img src="../Images/d44181d0e1944efdd7513631d67be353.png" data-original-src="https://miro.medium.com/v2/resize:fit:408/format:webp/1*MOHJHv60gWrGW4bvO32Lyw.png"/></div><p class="mq mr gj gh gi ms mt bd b be z dk translated">作者图片</p></figure><p id="cc64" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><em class="mg"> x1 </em>对<em class="mg"> y </em>的作用是梯度乘以<em class="mg"> x1 </em>的值，所以<em class="mg"> a * x1 </em>。</p><p id="35bf" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在处理深度学习模型时，像线性模型一样简单地看梯度可能会有问题。如果在这里代表深度学习模型的<em class="mg"> f(x) </em>在<em class="mg"> x </em>、<em class="mg"> df(x)/dx = 0 </em>附近是平坦的，因此<em class="mg"> x * df/dx = 0 </em>使你相信特征<em class="mg"> x </em>是不相关的。发生这种情况是因为我们将模型训练到饱和，因此预测分数在输入附近接近平坦。因此，将<em class="mg"> x </em>改变少量<em class="mg"> dx </em>，因为表面<em class="mg"> df(x)/dx </em>在那附近是平坦的，它将总是给我们零——我们不看相关的梯度。因此，为了捕捉相关梯度，我们需要了解梯度如何沿着从基线(通常为零，但取决于应用)到特征值的线性路径变化。</p><figure class="mj mk ml mm gt mn gh gi paragraph-image"><div class="gh gi mu"><img src="../Images/fbf250e38c0d12c5c9256ecaa301854c.png" data-original-src="https://miro.medium.com/v2/resize:fit:556/format:webp/1*B_u9fSgReaHBdRRJTvD8sA.png"/></div><p class="mq mr gj gh gi ms mt bd b be z dk translated"><a class="ae mh" href="https://arxiv.org/pdf/1703.01365.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1703.01365.pdf</a>公式(1)</p></figure></div><div class="ab cl lb lc hu ld" role="separator"><span class="le bw bk lf lg lh"/><span class="le bw bk lf lg lh"/><span class="le bw bk lf lg"/></div><div class="ij ik il im in"><h2 id="0a62" class="li lj iq bd lk ll lm dn ln lo lp dp lq ko lr ls lt ks lu lv lw kw lx ly lz ma bi translated">履行</h2><p id="4b0d" class="pw-post-body-paragraph kf kg iq kh b ki mb jr kk kl mc ju kn ko md kq kr ks me ku kv kw mf ky kz la ij bi translated">现在来看一个具体的例子。我们将定义一个简单的模型，并使用titanic数据集:</p><figure class="mj mk ml mm gt mn"><div class="bz fp l di"><div class="mv mw l"/></div></figure><p id="6131" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">让我们使用<em class="mg">亲笔签名的</em>来计算梯度，并绘制不同<em class="mg"> k </em>下<em class="mg">年龄</em>的梯度变化。</p><figure class="mj mk ml mm gt mn"><div class="bz fp l di"><div class="mv mw l"/></div></figure><figure class="mj mk ml mm gt mn gh gi paragraph-image"><div class="gh gi mx"><img src="../Images/976d1a8fa3c98764d6bc79e8645caa7d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1384/format:webp/1*03krjK7x_ugtUkDpq76hvA.png"/></div><p class="mq mr gj gh gi ms mt bd b be z dk translated">作者图片</p></figure><p id="dc77" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><em class="mg">注意:梯度为绝对值</em></p><p id="2668" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><em class="mg">α= 1(式(1)中k=m)时的得分</em>对应于实际输入时的预测，而<em class="mg">α= 0(式(1)中k = 0)</em>是基线时的预测。查看<em class="mg"> alpha = 1、</em>附近的梯度，我们可以观察到模型的饱和度，因为那里的梯度都接近于零，这就是为什么如果我们简单地查看<em class="mg"> df/dx </em>它将为零，并且不会给我们一个有意义的特征属性。</p><p id="a06f" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在，如果我们取这些梯度的平均值，我们得到<em class="mg">年龄的特征属性:</em></p><figure class="mj mk ml mm gt mn"><div class="bz fp l di"><div class="mv mw l"/></div></figure><p id="c55a" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">注意:如果我们设置<em class="mg">方法=‘riemann _梯形’</em>，这相当于<em class="mg"> captum </em>积分梯度法。(参见注释行21)</p></div><div class="ab cl lb lc hu ld" role="separator"><span class="le bw bk lf lg lh"/><span class="le bw bk lf lg lh"/><span class="le bw bk lf lg"/></div><div class="ij ik il im in"><h2 id="a8d1" class="li lj iq bd lk ll lm dn ln lo lp dp lq ko lr ls lt ks lu lv lw kw lx ly lz ma bi translated">结论</h2><p id="2f14" class="pw-post-body-paragraph kf kg iq kh b ki mb jr kk kl mc ju kn ko md kq kr ks me ku kv kw mf ky kz la ij bi translated">正如开始提到的，在这篇文章中，我想展示一个快速简单的集成渐变实现。如果您将它用于您的工作，您不需要从头重写它——使用伟大的<em class="mg"> captum </em>库为您完成所有工作。尽管在这篇文章中，我展示了表格数据的实现，但集成梯度可以用于任何深度学习模型，包括NLP和计算机视觉。</p><h2 id="45cd" class="li lj iq bd lk ll lm dn ln lo lp dp lq ko lr ls lt ks lu lv lw kw lx ly lz ma bi translated">参考</h2><p id="49ac" class="pw-post-body-paragraph kf kg iq kh b ki mb jr kk kl mc ju kn ko md kq kr ks me ku kv kw mf ky kz la ij bi translated">[1]https://arxiv.org/pdf/1703.01365.pd<a class="ae mh" href="https://arxiv.org/pdf/1703.01365.pdf" rel="noopener ugc nofollow" target="_blank">福</a></p><p id="44a2" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><a class="ae mh" href="https://captum.ai/tutorials/Titanic_Basic_Interpret" rel="noopener ugc nofollow" target="_blank">https://captum.ai/tutorials/Titanic_Basic_Interpret</a></p></div></div>    
</body>
</html>