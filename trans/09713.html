<html>
<head>
<title>Introduction to NLP Deep Learning Theories</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">自然语言处理深度学习理论简介</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/introduction-to-nlp-deep-learning-theories-9d6801e3aa7d?source=collection_archive---------13-----------------------#2021-09-10">https://towardsdatascience.com/introduction-to-nlp-deep-learning-theories-9d6801e3aa7d?source=collection_archive---------13-----------------------#2021-09-10</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="0089" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph">NLP深度学习</h2><div class=""/><div class=""><h2 id="65f8" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">我从自然语言处理和深度学习课程中学到的总结</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/b8dffbd6bbc15a9ffc57c0affc4ea354.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*aaUAV5z-sBGMSWlI"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">帕特里克·托马索在<a class="ae lh" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="0c65" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">在这篇文章中，我将总结我从斯坦福大学提供的<em class="me">自然语言处理与深度学习</em>中学到的东西，包括【2017年冬季视频讲座，以及【2019年冬季系列讲座。两场讲座都由斯坦福大学的克里斯托弗·曼宁教授授课。这两个讲座中涉及的深度学习NLP理论的几个主题包括:</p><ul class=""><li id="fb49" class="mf mg it lk b ll lm lo lp lr mh lv mi lz mj md mk ml mm mn bi translated">单词表示法</li><li id="fd12" class="mf mg it lk b ll mo lo mp lr mq lv mr lz ms md mk ml mm mn bi translated">NLP神经网络</li><li id="eedd" class="mf mg it lk b ll mo lo mp lr mq lv mr lz ms md mk ml mm mn bi translated">注意力和变压器</li><li id="e05f" class="mf mg it lk b ll mo lo mp lr mq lv mr lz ms md mk ml mm mn bi translated">最近的NLP深度学习模型</li></ul></div><div class="ab cl mt mu hx mv" role="separator"><span class="mw bw bk mx my mz"/><span class="mw bw bk mx my mz"/><span class="mw bw bk mx my"/></div><div class="im in io ip iq"><h1 id="dc87" class="na nb it bd nc nd ne nf ng nh ni nj nk ki nl kj nm kl nn km no ko np kp nq nr bi translated">1.单词表示法</h1><p id="1475" class="pw-post-body-paragraph li lj it lk b ll ns kd ln lo nt kg lq lr nu lt lu lv nv lx ly lz nw mb mc md im bi translated">深度学习模型依靠数字向量来“理解”输入的单词。我们可以认为数字向量是代表输入单词的高维特征。在这个高维度空间中，单词的位置要么彼此靠近，要么彼此远离。</p><p id="e96a" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">通过为给定语料库中的所有单词找到适当的数字向量表示来构建单词表示。单词表示的质量依赖于语料库。这可以很容易地理解为两个人对同一个词可以有不同的理解，取决于他喜欢花时间读现代报纸还是莎士比亚的文学。此外，单词表示的质量很大程度上依赖于找到所有单词的数字向量表示的方法。有几种通过从单词的上下文中学习来生成单词表示的方法。</p><blockquote class="nx"><p id="b719" class="ny nz it bd oa ob oc od oe of og md dk translated">从一个字所交的朋友，你就可以知道这个字。</p><p id="56b7" class="ny nz it bd oa ob oc od oe of og md dk translated">约翰·鲁珀特·弗斯</p></blockquote><p id="16a1" class="pw-post-body-paragraph li lj it lk b ll oh kd ln lo oi kg lq lr oj lt lu lv ok lx ly lz ol mb mc md im bi translated"><strong class="lk jd"> 1.1基于计数的方法:共生矩阵</strong></p><p id="7d25" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">共现描述了两个词在一定距离内的出现，该距离由固定的窗口大小定义。在语料库层面上，共现矩阵捕捉语料库中所有独特单词对的共现频率。这样，它提供了语料库的统计描述。共现矩阵中的每一行都是代表一个单词的数值向量。</p><p id="f288" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">然而，为了捕获语料库中所有独特单词对的共现频率，该共现矩阵会占用大量内存。假设你有一个拥有10k个唯一词的语料库，那么共现矩阵的规模就是10k乘10k！同时，每个词向量的维数是10k！为了降低单词向量的维数，通常使用奇异值分解(SVD)。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi om"><img src="../Images/2084dba271ee5aaae8cb22db7bf0f605.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rZHEHbefVyhn9RJskrOt6A.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">从小型语料库构建的共现矩阵。</p></figure><p id="bc19" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><strong class="lk jd"> 1.2基于预测的方法:Word2vec </strong></p><p id="f3d2" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">除了纯粹依赖于统计的基于计数的共现矩阵方法之外，还可以通过使用浅层前馈神经网络模型的基于预测的方法来生成词向量。这种基于预测的方法包括word2vec，它可以用skip-gram或连续词包(CBOW)算法来实现。skip-gram和CBOW算法都是针对固定窗口大小内的词进行预测，它们的预测方向不同。Skip-gram使用中心单词预测上下文单词，而CBOW使用上下文单词预测中心单词。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi on"><img src="../Images/680ceceeeabd9ed54a5216882dcd8b34.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VHBReqr_KQ3YuGZrCL04Mw.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">skip-gram算法使用给定的中心单词预测上下文单词。</p></figure><p id="03df" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">使用浅层神经网络来训练Skip-gram，并且输入是T个唯一单词的语料库。每个单词被随机分配为一个d维向量。这样，在这个浅层神经网络中有2*T*d个参数需要优化，中心词矩阵和上下文词矩阵各有T*d个参数。该神经网络通过迭代地将每个单词作为中心单词并在给定当前中心单词的情况下最大化上下文单词的概率来训练。</p><p id="5e83" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">Word2vec可以在<a class="ae lh" href="https://radimrehurek.com/gensim/models/word2vec.html" rel="noopener ugc nofollow" target="_blank"> Gensim </a>上获得，它可以很容易地导入到代码中，以生成基于预测的词向量。</p><p id="65ce" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><strong class="lk jd"> 1.3基于计数和基于预测相结合的方法:Glove </strong></p><p id="2c6f" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">一方面，共现矩阵在语料库水平上提供单词共现的统计描述。另一方面，word2vec通过使用浅层神经网络来捕获窗口大小内的上下文单词的预测能力。Glove是一种被提出来结合这两种方法的架构，以具有统计能力和局部上下文预测能力。与word2vec类似，Glove也是使用浅层神经网络进行训练的。成本函数是Glove与word2vec的区别。Glove的成本函数包括Xᵢⱼ，即单词j在单词I的上下文中出现的次数，并且这并入了语料库的统计描述。</p><p id="14e0" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><a class="ae lh" href="https://github.com/stanfordnlp/GloVe" rel="noopener ugc nofollow" target="_blank"> Glove </a>也可以作为Python库使用。斯坦福Glove Python库可以作为预训练的词向量生成器，因为它是在大规模网络数据集上预训练的，具有像维基百科和Twitter这样的语料库。此外，它还可以在新的语料库上进行训练。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi oo"><img src="../Images/0c700592d03de1cf31433ffa1b2ce172.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TClN0LXg8y6ANx9qhlwIqw.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">手套成本函数</p></figure><h1 id="6d02" class="na nb it bd nc nd op nf ng nh oq nj nk ki or kj nm kl os km no ko ot kp nq nr bi translated">2.NLP神经网络</h1><p id="d377" class="pw-post-body-paragraph li lj it lk b ll ns kd ln lo nt kg lq lr nu lt lu lv nv lx ly lz nw mb mc md im bi translated">到这里，我们已经学习了单词表示法，即将单词变成机器可以理解的数字向量。然后，下一步是弄清楚如何使用这些单词表示来完成NLP任务。</p><p id="c285" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">许多NLP任务可以被认为是分类问题。一个简单的例子是情感分析，它可以简单到将电影评论分为积极或消极的情感。一个不太直接的例子是下一个单词预测的NLP任务。当我想预测“我喜欢吃……”后面的下一个单词时，这个下一个单词可以是我的语料库中的任何单词。下一个单词预测的方法是计算我的语料库中所有唯一单词的概率，然后选择概率最大的单词。Softmax通常用于归一化多个输出类的概率。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ou"><img src="../Images/c30646c8efdc22b2907f62edd4ddda2f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nR0roMFpbj3NazoN9G-2yw.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">下一个单词预测任务的Softmax</p></figure><p id="bf09" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">softmax的问题在于它是输入单词向量的线性变换。因此，对于多类分类问题，它只能给出一个线性的决策边界。对于复杂的NLP任务，单词向量的线性变换是不够的。我们需要在将单词向量送入softmax的最后一层之前引入非线性。神经网络是一种解决方案，可以在softmax的最后一层之前实现，以将非线性引入模型。</p><p id="d89c" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">在本节中，我们将从基本的神经网络元素——神经元开始，然后讨论NLP任务的三种基本神经网络架构，即递归神经网络(RNN)、卷积神经网络(CNN)和树形递归神经网络(树形RNN)。</p><p id="11e2" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><strong class="lk jd"> 2.1神经元</strong></p><p id="02d0" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">神经元是神经网络中的基本计算单位。在一个神经元中，输入经过线性变换后，结果被送入逻辑回归。逻辑回归将非线性引入到神经元的计算中。因此，神经元本质上是一个二元逻辑回归单元。神经网络由多层组成，其中每层进一步由一束神经元组成。我们可以将每一层想象成同时运行的几个逻辑回归，然后将结果输入到另一个逻辑回归中。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ov"><img src="../Images/6f7950b37b9a06f641c07873eb69bd77.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*X2KbdfO9lT04vSroFgtSnw.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">神经元和神经网络</p></figure><p id="4fa0" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><strong class="lk jd"> 2.2递归神经网络(RNN) </strong></p><p id="d663" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">通常，在NLP任务中，我们会处理一系列单词，这些单词可以是不完整句子的电影评论或写得很好的研究文章。预测下一个单词需要对前面的单词序列有一个整体的理解。RNN是一种适合处理数据序列的体系结构。基本上，在每个时间步，一个单词向量被输入到RNN的一个神经元中。除了在该时间步长的输入单词向量之外，神经元还接收携带关于先前时间步长的信息的隐藏状态。这样，单词序列的信息在神经网络中传递。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ow"><img src="../Images/6594bee4e8d1d21840f2c8e5e3484fa0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3j3FFXrI-LUu0nB54RbQJw.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">递归神经网络体系结构</p></figure><p id="cc60" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">上面提到的RNN建筑也被称为香草RNN。香草RNN遭受消失梯度问题，这是由反向传播过程中的参数更新的长链引起的。反向传播旨在更新神经网络参数以最小化输出误差。然而，由于香草RNN的顺序连接性质，进一步的神经元的参数不能有效地更新。这样，神经元很容易死亡。为了克服香草RNN的消失梯度问题，提出了两个先进的RNN建筑解决方案:门控循环单元(GRU)和长短期记忆(LSTM)。GRU和LSTM解决渐变消失问题的方法是通过引入某种形式的门控机制来控制传递到当前单元格的前一个单元格的信息量。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ox"><img src="../Images/d8e1009d6f0487376be60a0233da8106.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fnYGLTfR01Xe7FFC7Mpkig.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">门控循环单元体系结构</p></figure><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi oy"><img src="../Images/c8d9244becac20243e0dab58721dc511.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-37ptKhjMLoJV6PC5ltspg.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">长短期记忆结构</p></figure><p id="b4fd" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><strong class="lk jd"> 2.3卷积神经网络(CNN) </strong></p><p id="39a9" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">RNN允许信息跨时间流动，从而能够理解全球语义。这在本质上决定了RNN计算是一个缓慢的过程，因为后面的神经元等待来自前面神经元的信息流。作为一种更快的解决方案，CNN已经被探索用于一些对全局语义理解要求较低的NLP任务。</p><p id="e225" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">CNN广泛应用于计算机视觉模型中。CNN的基本思想是对输入数据并行应用多个滤波器，每个滤波器提取某个特征。在计算机视觉任务中，这些过滤器识别空间中的局部模式。类似地，当应用于NLP任务时，这些过滤器可以被训练来识别时间上的局部模式。</p><p id="7cad" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">当窗口滑过输入时，过滤器对给定窗口大小内的输入单词向量进行处理，其中窗口大小是在CNN训练中要优化的超参数。对于每个过滤器，当找到一个特定的模式时，它将强烈激发，并可以由后续的max-pooling层选择。例如，在情感分析中，当发现“不喜欢”时，窗口大小为3的过滤器将强烈启动。这样，我们可以把一个窗口大小为n的滤波器想象成n元模式搜索器。</p><h1 id="7578" class="na nb it bd nc nd op nf ng nh oq nj nk ki or kj nm kl os km no ko ot kp nq nr bi translated">3.注意力和变压器</h1><p id="5ab5" class="pw-post-body-paragraph li lj it lk b ll ns kd ln lo nt kg lq lr nu lt lu lv nv lx ly lz nw mb mc md im bi translated">RNN和CNN是NLP深度学习模型中广泛使用的基本单元。RNN擅长顺序学习远程语义，而CNN擅长学习局部语义，可以通过并行计算实现。远程语义和并行计算很重要，有可能两者同时实现吗？在本次会议中，我将讨论一些最新的高级神经网络架构。</p><p id="42cc" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><strong class="lk jd"> 3.1注意:基于解码器查询对齐编码器信息池的相似性评分机制</strong></p><p id="2f63" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">由于伯特模型，许多人都听说过“注意力”和“变压器”这两个词。然而，与transformer架构(直到最近才在谷歌2017年名为<a class="ae lh" href="http://papers.nips.cc/paper/7181-attention-is-all-you-need" rel="noopener ugc nofollow" target="_blank"> Attention is All You Need </a> ⁴的出版物中提出)不同，注意力机制在神经机器翻译中已经使用了很长时间。根据Christopher Manning的说法，神经机器翻译是通过一个大型人工神经网络对整个机器翻译过程进行建模的方法。</p><p id="8aea" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">传统上，神经机器翻译使用带有编码器和解码器的序列到序列模型，编码器和解码器使用RNN单元构建。在没有注意机制的编码器-解码器架构中，编码器序列中的最后一个神经元是连接到解码器的唯一单元。然而，编码器序列中的最后一个神经元可能无法携带输入序列中的所有有用信息。注意机制可以解决这个问题。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi oz"><img src="../Images/29e0b91556709f2808f8934edc0bb55b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hoo7mYoe8CK_JkVgf1X2Tg.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">无注意机制的神经机器翻译</p></figure><p id="c21a" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">注意机制允许解码器神经元从编码器的信息池中提取隐藏状态。注意层采用两个输入向量，这是编码器和解码器的隐藏状态，并返回归一化的相似性得分。这样，关注层有助于根据给定的解码器查询来调整编码器信息。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pa"><img src="../Images/bc8e54338efa2309c5b2f8275f283624.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2nByX_57Cov5ay_bXJO3jw.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">具有注意机制的神经机器翻译</p></figure><p id="6376" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><strong class="lk jd"> 3.2自我关注:具有并行计算能力的序列处理</strong></p><p id="37d2" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">RNN可以携带长距离的语义信息，但是由于顺序处理的性质，计算是缓慢的。相比之下，CNN作为局部过滤器工作，可以很容易地在并行计算中实现，但同时缺乏长程语义信息。自我注意是一种以并行计算方式获得输入的全局理解的解决方案。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pb"><img src="../Images/016f32a7d5df780de1c3c6dce70124a3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*I82UrSwVtpsQ3DRtVDbk2w.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">RNN和CNN</p></figure><p id="41e0" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">在自我注意中，每个输入单词首先被编码成三个向量:q、k和v。在每个位置计算输出向量。为了获得输出向量，使用输出位置的q和所有的k来计算注意力得分。然后注意力得分和每个v向量的加权和生成输出向量。</p><p id="c977" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我认为自我关注是一种修正的CNN结构。在CNN中，过滤器仅适用于特定窗口大小内的附近输入向量。这样看来，CNN善于捕捉地方特色。由于该滤波过程是纯粹的矩阵运算，因此可以容易地以并行方式实现，多个滤波器同时工作以寻找不同的特征。在自我关注中，这个过滤过程被修改，改为在每个输入位置取一个编码向量。这种修改使人们能够关注CNN固有的矩阵运算和并行实现的优势，同时也带来了理解全局语义的新优势。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pc"><img src="../Images/388d948551313920a6cb8caef68942d9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XtJ2d7EU_prh2kfOwAGnWg.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">自我注意机制</p></figure><p id="c5f4" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><strong class="lk jd"> 3.3变压器:建立在自关注机制上的基本单元</strong></p><p id="51cc" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">transformer是2017年提出的深度学习基本单元架构，使用自关注机制构建。该变换器可以代替RNN，取序列输入，产生序列输出，同时可以实现并行计算。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pd"><img src="../Images/308fb78c8030faa55f5537354445fe68.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*B-PnquAQgCn9jNJzB8A2Ig.jpeg"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">变压器architecture⁴</p></figure><h1 id="f6c4" class="na nb it bd nc nd op nf ng nh oq nj nk ki or kj nm kl os km no ko ot kp nq nr bi translated">4.最近的NLP深度学习模型</h1><p id="094c" class="pw-post-body-paragraph li lj it lk b ll ns kd ln lo nt kg lq lr nu lt lu lv nv lx ly lz nw mb mc md im bi translated">到目前为止，我已经讨论了单词表示(共现矩阵，word2vec，Glove)和初级深度学习单元架构(RNN，CNN，transformer)。在这一节中，我们将回顾自然语言处理领域的最新进展。</p><p id="4a13" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">迁移学习已经广泛应用于计算机视觉领域，从图像分类到目标检测的计算机视觉任务大多使用在大型图像数据集上预先训练的模型。然而，NLP中的迁移学习直到2018年才成为可能。在过去的两年里，已经出现了几个预先训练好的通用语言模型。我们将经历一些突破。杰伊·阿拉玛写的一篇关于最近NLP发展的好文章可以在<a class="ae lh" href="http://jalammar.github.io/illustrated-bert/" rel="noopener ugc nofollow" target="_blank">这里</a>找到。</p><p id="56ea" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><strong class="lk jd"> 4.1 ELMo:首个上下文单词嵌入模型</strong></p><p id="cb12" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">语言嵌入模型(ELMo)是第一个上下文词嵌入模型，于20世纪初在⁸⁵.发表我们已经在这篇文章的第一部分讨论了单词表示。然而，在ELMo之前的所有单词表示模型，包括word2vec和Glove，都只能生成静态的单词嵌入。对于静态单词嵌入，我们的意思是每种单词类型只有一个单词嵌入。例如，在“河岸”和“银行账户”中，“银行”一词将具有相同的矢量表示。这种静态单词嵌入不能很好地表示单词在特定语言环境中的含义。</p><p id="3e7f" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">相反，ELMo通过使用两个双层LSTM的内部状态来生成上下文单词嵌入，这两个双层包括向前和向后单元。确定如何使用内部状态来生成加权和结果(即上下文单词嵌入)的参数是在每个特定的NLP任务中学习的。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi pe"><img src="../Images/f35e11c05c6c89bb1bbb39424ad9c681.png" data-original-src="https://miro.medium.com/v2/resize:fit:988/format:webp/1*bOn3HK7947sXEJsab6XfWQ.jpeg"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">ELMo⁶正反两层LSTM结构</p></figure><p id="c918" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><strong class="lk jd"> 4.2 ULMFit:用transformer代替lstm，探索NLP迁移学习</strong></p><p id="8602" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">继ELMo之后，通用语言模型微调(ULMFiT)也于20世纪初在⁸⁷.发布ELMo和ULMFiT的一个区别是，ULMFiT用一个变压器取代了所有的LSTM单元，这是谷歌在2018年初刚刚发布的。</p><p id="3a42" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">ULMFiT的目标是实现一个可用于迁移学习的通用语言模型。实现ULMFit分为两个阶段:在通用领域语料库上进行语言模型预训练，以及在特定的自然语言处理任务中进行微调。这样，预先训练的语言模型学习到了语言的一般特征，迁移学习有助于快速训练模型适应不同的NLP任务。</p><p id="9674" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><strong class="lk jd"> 4.3 GPT:使用正向变换器的大规模预训练NLP语言模型</strong></p><p id="c111" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">在ULMFit中，在通用领域语料库上的预训练语言模型的第一阶段是无监督的学习任务。未标记的文本语料库非常丰富，而标记良好的文本语料库却很难找到。预训练语言模型的无监督学习方式使得该模型通常可以从大量可用文本中学习。ULMFiT在一个相对较小的语料库上进行预训练，以证明迁移学习语言模型的可能性。然后在ULMFiT之后不久，生成性预训练(GPT)由OpenAI在20 ⁸⁸.出版与ULMFiT类似，GPT使用前向变换器，但GPT是在模型中有更多参数的更大语料库上训练的。</p><p id="6b9b" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">2019年初，OpenAI发布了GPT- ⁹，这是一个比GPT更大、更强大的预训练语言模型。它有1.5B个参数，并在40GB的互联网文本上进行了预训练。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi pf"><img src="../Images/817a31590d4d722cf8a416fff0700408.png" data-original-src="https://miro.medium.com/v2/resize:fit:508/format:webp/1*1KUKrjrKYTW8tPpcVKWg2Q.jpeg"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">GPT⁶的正向变压器</p></figure><p id="3e42" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><strong class="lk jd"> 4.4 BERT:使用双向转换器的大规模预训练NLP语言模型</strong></p><p id="9370" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">2018年底，谷歌发布了变形金刚(BERT)⁶.)的双向编码器表示与GPT相似，BERT也是一种语言模型，与ULMFiT相比，它在更大的语料库上进行预训练，具有更多的参数。</p><p id="769c" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">由于我在这次Kaggle TensorFlow 2.0问答比赛中使用了BERT，因此在本系列的后续帖子中将提供更多关于BERT的讨论。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi pg"><img src="../Images/76b2d580622ec94410aaea2dfd9bf436.png" data-original-src="https://miro.medium.com/v2/resize:fit:530/format:webp/1*J2nYGfmWczvJgD3JwPlXXw.jpeg"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">BERT⁶的双向变压器</p></figure><h1 id="283d" class="na nb it bd nc nd op nf ng nh oq nj nk ki or kj nm kl os km no ko ot kp nq nr bi translated">5.摘要</h1><p id="c94c" class="pw-post-body-paragraph li lj it lk b ll ns kd ln lo nt kg lq lr nu lt lu lv nv lx ly lz nw mb mc md im bi translated">在这篇文章中，我总结了我从<em class="me">自然语言处理和深度学习</em>课程中学到的东西。文章介绍了单词表示和NLP神经网络结构。希望你觉得总结有用！</p></div><div class="ab cl mt mu hx mv" role="separator"><span class="mw bw bk mx my mz"/><span class="mw bw bk mx my mz"/><span class="mw bw bk mx my"/></div><div class="im in io ip iq"><p id="974e" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">[1] Pennington，Jeffrey，Richard Socher和Christopher Manning。"手套:单词表示的全局向量."在<em class="me">2014年自然语言处理经验方法会议(EMNLP) </em>中，第1532–1543页。2014.</p><p id="65c5" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">[2]赵京贤、巴特·范·梅林波尔、卡格拉尔·古尔切雷、迪米特里·巴丹瑙、费特希·布加雷斯、奥尔赫·施文克和约舒阿·本吉奥。"使用统计机器翻译的RNN编码器-解码器学习短语表示."<em class="me"> arXiv预印本arXiv:1406.1078 </em> (2014)。</p><p id="d637" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">[3] Hochreiter、Sepp和Jürgen Schmidhuber。“长短期记忆。”<em class="me">神经计算</em> 9，8号(1997):1735–1780。</p><p id="59f8" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">[4] Vaswani、Ashish、Noam Shazeer、Niki Parmar、Jakob Uszkoreit、Llion Jones、Aidan N. Gomez、ukasz Kaiser和Illia Polosukhin。“你需要的只是关注。”在<em class="me">神经信息处理系统进展</em>中，第5998–6008页。2017.</p><p id="12d0" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">[5]彼得斯、马修·e、马克·诺依曼、莫希特·伊耶、马特·加德纳、克里斯托弗·克拉克、肯顿·李和卢克·塞特勒莫耶。"深度语境化的词语表达."<em class="me"> arXiv预印本arXiv:1802.05365 </em> (2018)。</p><p id="c053" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">[6] Devlin，Jacob，张明蔚，Kenton Lee和Kristina Toutanova。"伯特:用于语言理解的深度双向转换器的预训练."<em class="me"> arXiv预印本arXiv:1810.04805 </em> (2018)。</p><p id="fe52" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">[7]霍华德、杰里米和塞巴斯蒂安·鲁德。“用于文本分类的通用语言模型微调。”<em class="me"> arXiv预印本arXiv:1801.06146 </em> (2018)。</p><p id="821c" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">[8]拉德福德、亚历克、卡蒂克·纳拉辛汉、蒂姆·萨利曼斯和伊利亚·苏茨基弗。"通过生成性预训练提高语言理解能力."<em class="me">网址</em><a class="ae lh" href="https://s3-us-west-2." rel="noopener ugc nofollow" target="_blank"><em class="me">https://S3-us-west-2。</em> </a> <em class="me">亚马逊鹦鹉。com/open ai-assets/research covers/language unsupervised/语言理解论文。pdf </em> (2018)。</p><p id="c83e" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">[9]拉德福德、亚历克、杰弗里·吴、雷文·蔡尔德、戴维·栾、达里奥·阿莫代伊和伊利亚·苏茨基弗。"语言模型是无人监督的多任务学习者."<em class="me"> OpenAI博客</em> 1，第8期(2019)。</p></div></div>    
</body>
</html>