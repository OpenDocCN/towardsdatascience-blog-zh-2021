<html>
<head>
<title>Multiclass Classification Neural Network using Adam Optimizer</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用Adam优化器的多类分类神经网络</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/multiclass-classification-neural-network-using-adam-optimizer-fb9a4d2f73f4?source=collection_archive---------15-----------------------#2021-07-10">https://towardsdatascience.com/multiclass-classification-neural-network-using-adam-optimizer-fb9a4d2f73f4?source=collection_archive---------15-----------------------#2021-07-10</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="7d5d" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">这是我的博客系列的继续，在我的博客系列中，我使用numpy的einsum来实现完整的模型。</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/1110b6aaadfd327423213ae7ada85e4c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*rqQNZShGj-YR12Km"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">爱德华·豪厄尔在<a class="ae kv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="0f04" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我想以一种更实际的方式来看看Adam优化器和梯度下降优化器之间的区别。所以我决定改为实现它。</p><p id="9fa6" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在这里，我采用了虹膜数据集，并实现了一个多类分类2层神经网络，就像我在<a class="ae kv" rel="noopener" target="_blank" href="/multiclass-classification-neural-network-implementation-using-numpys-einsum-3675a7e1e703">以前的博客中所做的那样。</a>这次唯一不同的是我用了Adam优化器，而不是梯度下降。</p><pre class="kg kh ki kj gt ls lt lu lv aw lw bi"><span id="af92" class="lx ly iq lt b gy lz ma l mb mc">def leaky_relu(z):<br/>    return np.maximum(0.01*z,z)</span><span id="9a00" class="lx ly iq lt b gy md ma l mb mc">def leaky_relu_prime(z):<br/>    z[z&lt;=0] = 0.01<br/>    z[z&gt;0] = 1<br/>    return z</span><span id="2671" class="lx ly iq lt b gy md ma l mb mc">def softmax(z, _axis=0):<br/>    stable_z = z - np.max(z)<br/>    e_z = np.exp(stable_z)<br/>    return e_z/np.sum(e_z, axis=_axis, keepdims=True)</span><span id="b6c0" class="lx ly iq lt b gy md ma l mb mc">def binarize(z):<br/>    return (z[:,None] == np.arange(z.max()+1)).astype(int)</span><span id="74e3" class="lx ly iq lt b gy md ma l mb mc">def pasapali_batch(units_count, x, y, lr, epochs, bias=False, _seed=42):<br/>    beta1 = 0.9<br/>    beta2 = 0.999<br/>    eps = np.nextafter(0, 1)<br/>    batch_size, ni = x.shape[-2:]<br/>    units_count.insert(0,ni)<br/>    units_count_arr = np.array(units_count)<br/>    L, = units_count_arr.shape # Number of layers + 1<br/>    # RED ALERT - `as_strided` function is like a LAND-MINE ready to explode in wrong hands!<br/>    arr_view = as_strided(units_count_arr, shape=(L-1,2), strides=(4,4))<br/>#     print(arr_view)<br/>    rng = np.random.default_rng(seed=_seed)<br/>    wghts = [None]*(L-1)<br/>    intercepts = [None]*(L-1)<br/>    M_W = [None]*(L-1)<br/>    M_B = [None]*(L-1)<br/>    V_W = [None]*(L-1)<br/>    V_B = [None]*(L-1)<br/>    # WEIGHTS &amp; MOMENTS INITIALIZATION<br/>    for i in range(L-1):<br/>        w_cols, w_rows = arr_view[i,:]<br/>        wghts[i] = rng.random((w_rows, w_cols))<br/>        M_W[i] = np.zeros((epochs+1, w_rows, w_cols))<br/>        V_W[i] = np.zeros((epochs+1, w_rows, w_cols))<br/>        if bias:<br/>            intercepts[i] = rng.random((w_rows,))<br/>            M_B[i] = np.zeros((epochs+1, w_rows))<br/>            V_B[i] = np.zeros((epochs+1, w_rows))<br/>            <br/>    # COSTS INITIALIZATION<br/>    costs = np.zeros(epochs)<br/>    <br/>    # Gradient Descent<br/>    for epoch in range(epochs):<br/>        # FORWARD PROPAGATION<br/>        # hidden layer 1 implementation, relu activation   <br/>        h1a = np.einsum(’hi,Bi -&gt; Bh’, wghts[0], x)<br/>        if bias:<br/>            h1a = h1a + intercepts[0]<br/>        h1 = leaky_relu(h1a)<br/>        # hidden layer 2 implementation, softmax activation<br/>        h2a = np.einsum(’ho,Bo -&gt; Bh’, wghts[1], h1) <br/>        if bias:<br/>            h2a = h2a + intercepts[1]<br/>        hyp = softmax(h2a, _axis=1)<br/>        current_epoch_cost = -np.einsum(’Bi,Bi’, y, np.log(hyp))/batch_size<br/>#         print(current_epoch_cost)<br/>        costs[epoch] = current_epoch_cost<br/>        # BACKWARD PROPAGATION<br/>        # layer 2<br/>        dJ_dH2a = hyp - y<br/>        dJ_dW1 = np.einsum(’Bi,Bj -&gt; ij’,dJ_dH2a, h1)/batch_size<br/>        # layer 1<br/>        dJ_dH1 = np.einsum(’Bi,ij -&gt; Bj’, dJ_dH2a, wghts[1])<br/>        dJ_dH1a = dJ_dH1*leaky_relu_prime(h1a)<br/>        dJ_dW0 = np.einsum(’Bi,Bj -&gt; ij’,dJ_dH1a, x)/batch_size<br/>        # numerical optimization<br/>        beta1_denom = (1.0 - beta1**(epoch+1))<br/>        beta2_denom = (1.0 - beta2**(epoch+1))<br/>        if bias:<br/>            dJ_dB1 = np.einsum("Bi -&gt; i", dJ_dH2a)/batch_size<br/>            dJ_dB0 = np.einsum("Bi -&gt; i",dJ_dH1a)/batch_size<br/>            # MOMENTS ADJUSTMENT<br/>            M_B[0][epoch+1,:] = beta1 * M_B[0][epoch,:] + (1.0 - beta1)*dJ_dB0<br/>            M_B[1][epoch+1,:] = beta1 * M_B[1][epoch,:] + (1.0 - beta1)*dJ_dB1<br/>            <br/>            V_B[0][epoch+1,:] = beta2 * V_B[0][epoch,:] + (1.0 - beta2)*dJ_dB0**2<br/>            V_B[1][epoch+1,:] = beta2 * V_B[1][epoch,:] + (1.0 - beta2)*dJ_dB1**2<br/>            # BIAS CORRECTION<br/>            mhat_b0 = M_B[0][epoch+1,:] / beta1_denom<br/>            vhat_b0 = V_B[0][epoch+1,:] / beta2_denom<br/>            <br/>            mhat_b1 = M_B[1][epoch+1,:] / beta1_denom<br/>            vhat_b1 = V_B[1][epoch+1,:] / beta2_denom<br/>            # BIAS ADJUSTMENT with numerical stability<br/>            intercepts[1] = intercepts[1] - lr*mhat_b1/(np.sqrt(vhat_b1) + eps)<br/>            intercepts[0] = intercepts[0] - lr*mhat_b0/(np.sqrt(vhat_b0) + eps)<br/>            <br/>            <br/>        # MOMENTS ADJUSTMENT<br/>        M_W[0][epoch+1,:] = beta1 * M_W[0][epoch,:] + (1.0 - beta1)*dJ_dW0<br/>        M_W[1][epoch+1,:] = beta1 * M_W[1][epoch,:] + (1.0 - beta1)*dJ_dW1<br/>        <br/>        V_W[0][epoch+1,:] = beta2 * V_W[0][epoch,:] + (1.0 - beta2)*dJ_dW0**2<br/>        V_W[1][epoch+1,:] = beta2 * V_W[1][epoch,:] + (1.0 - beta2)*dJ_dW1**2<br/>        # BIAS CORRECTION<br/>        mhat_w0 = M_W[0][epoch+1,:] / beta1_denom<br/>        vhat_w0 = V_W[0][epoch+1,:] / beta2_denom<br/>        <br/>        mhat_w1 = M_W[1][epoch+1,:] / beta1_denom<br/>        vhat_w1 = V_W[1][epoch+1,:] / beta2_denom<br/>        # WEIGHTS ADJUSTMENT with numerical stability<br/>        wghts[1] = wghts[1] - lr*mhat_w1/(np.sqrt(vhat_w1) + eps)<br/>        wghts[0] = wghts[0] - lr*mhat_w0/(np.sqrt(vhat_w0) + eps)</span><span id="0701" class="lx ly iq lt b gy md ma l mb mc">    if bias:<br/>        return (costs, wghts, intercepts)<br/>    else:<br/>        return (costs, wghts)</span><span id="1f84" class="lx ly iq lt b gy md ma l mb mc">iris = load_iris()<br/>x = iris.data<br/>y = iris.target<br/>#NORMALIZE<br/>x_norm = normalize(x)<br/>x_train, x_test, y_train, y_test = train_test_split(x_norm, y, test_size=0.33, shuffle=True, random_state=42)<br/>#BINARIZE<br/>y_train = binarize(y_train)<br/>y_test = binarize(y_test)</span><span id="98b1" class="lx ly iq lt b gy md ma l mb mc">unit_per_layer_counts = [10,3]<br/>costs, fw, fb = pasapali_batch(unit_per_layer_counts, x_train, y_train, lr=0.01, epochs=200, bias=True)</span><span id="4529" class="lx ly iq lt b gy md ma l mb mc">plt.plot(costs)</span><span id="d42b" class="lx ly iq lt b gy md ma l mb mc">def predict(x,fw,fb):<br/>    h1a = np.einsum(’hi,Bi -&gt; Bh’, fw[0], x)+fb[0]<br/>    h1 = relu(h1a)<br/>    h2a = np.einsum(’ho,Bo-&gt; Bh’,fw[1],h1)+fb[1]<br/>    return softmax(h2a)</span></pre><p id="44c4" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在代码中，不同之处在于我已经为每一层初始化了两个矩数组，并根据<a class="ae kv" href="https://arxiv.org/abs/1412.6980" rel="noopener ugc nofollow" target="_blank"> Adam优化</a>算法更新了这些矩(<em class="me">或者我应该编写适应的</em> …)。</p><p id="235d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在正常的梯度下降优化器中，基于在同一时期中计算的梯度来调整权重。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mf"><img src="../Images/f22d85939033425d9a21b4c73fcb8658.png" data-original-src="https://miro.medium.com/v2/resize:fit:222/1*JnBDilNDt4oMiO7tVw7-7g.gif"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">梯度下降中的权重调整仅取决于当前梯度。—图片归作者所有</p></figure><p id="56e6" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在Adam optimizer中，权重根据当前和先前时段中计算的梯度移动平均值进行调整。根据Adam算法的矩调整被计算为先前和当前梯度的移动平均，然后这些矩被用于更新权重。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mg"><img src="../Images/673e0671228a41982677b2be94b5f410.png" data-original-src="https://miro.medium.com/v2/resize:fit:182/1*JsQqAVtnQ0p6hX2fw1hodA.gif"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">Adam优化器中的权重调整取决于当前和先前的梯度。—图片归作者所有</p></figure><p id="1682" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">本文中，β1 = 0.9，<strong class="ky ir"> <em class="me"> m </em> </strong>根据公式更新:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mh"><img src="../Images/b2c88908195ce9b05f210878a8295178.png" data-original-src="https://miro.medium.com/v2/resize:fit:618/format:webp/1*cv-uZgWtjlnx8A_sd8deJQ.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">自适应一阶矩公式根据作者拥有的论文图像</p></figure><p id="a472" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">让我们对每个时期逐步展开上述公式。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mi"><img src="../Images/f446f0deaff3c9a87118be6255ed1c2c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1082/format:webp/1*xmP3RJVq1GnUjQWvaVL7-g.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">三个历元中一阶矩m公式的扩展。—图片归作者所有</p></figure><p id="de6f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们看到，在每个时期，先前的梯度被包括在更新中，但是分配给远离当前时期梯度的梯度的权重变得越来越小。这有助于向最小值移动，同时抑制搜索最小值时的梯度振荡。这给了我们穿越鞍点的速度。</p><p id="2dec" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">让我们谈一谈二阶矩<strong class="ky ir"> <em class="me"> v </em> </strong>。在权重调整期间，学习率除以<strong class="ky ir">的均方根<em class="me">v</em>T7】。这有助于调整每个重量<strong class="ky ir"><em class="me"/></strong>的学习率。具有相对较大幅度的权重将具有较大的值<strong class="ky ir"> <em class="me"> v </em> </strong>，因此在该方向上具有较小的学习步长。这有助于我们放慢速度，这样我们就不会超过最小值。</strong></p><p id="b751" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">最后，我们来谈谈偏差校正部分。在原始论文中，他们给出了数学推导并给出了解释。对于门外汉来说，知道引入这种偏差校正在梯度稀疏时有所帮助就足够了，如果不进行校正会导致较大的阶跃。</p><p id="3572" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">让我们比较一下梯度下降优化器和Adam优化器方法中成本函数的收敛性。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mj"><img src="../Images/e5a8bef7706e91775b497d23f8ee779b.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/format:webp/1*R589qzGPYEebI1pfT7EmQg.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">使用梯度下降的成本函数衰减—图片归作者所有</p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mj"><img src="../Images/8a3c5c87c629958c4ec24e6891e9d38c.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/format:webp/1*xZ1gwxk0xHUV9pU3G168ww.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">使用Adam optimizer的成本函数衰减—图片归作者所有</p></figure><blockquote class="mk"><p id="00be" class="ml mm iq bd mn mo mp mq mr ms mt lr dk translated">Adam optimizer仅用了250个历元就达到了最佳成本值，而梯度下降则用了19000个历元。让我想起了超级英雄闪电侠！！</p></blockquote><figure class="mv mw mx my mz kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mu"><img src="../Images/2f398aa6c71f322a649f5e6b33ae7b8f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AEWaCpgCoDUR-ocbXRc_Eg.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">梯度下降优化器完成19000个历元所用的时间为2.98秒-图片归作者所有</p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi na"><img src="../Images/7e8298afa8376f57127a986556fe1db9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aJrWQZrWqmM987D12cC6Dw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">Adam optimizer完成250个时期所用的时间大约为87.3毫秒—图片归作者所有</p></figure><p id="16dc" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这是收敛速度的巨大进步。亚当不仅速度快。它也适用于稀疏和非常嘈杂的渐变。</p><p id="18e8" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">请让我知道你对这篇博文和代码实现的看法以及你的评论。</p></div><div class="ab cl nb nc hu nd" role="separator"><span class="ne bw bk nf ng nh"/><span class="ne bw bk nf ng nh"/><span class="ne bw bk nf ng"/></div><div class="ij ik il im in"><p id="46a8" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我是TCS的机器学习工程师，我的(数字软件和解决方案)团队正在开发令人惊叹的产品。</p></div></div>    
</body>
</html>