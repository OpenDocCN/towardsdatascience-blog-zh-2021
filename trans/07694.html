<html>
<head>
<title>Amazon SageMaker and 🤗 Transformers: Train and Deploy a Summarization Model with a Custom Dataset</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">亚马逊SageMaker和🤗变压器:使用自定义数据集训练和部署汇总模型</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/amazon-sagemaker-and-transformers-train-and-deploy-a-summarization-model-with-a-custom-dataset-5efc589fedad?source=collection_archive---------18-----------------------#2021-07-14">https://towardsdatascience.com/amazon-sagemaker-and-transformers-train-and-deploy-a-summarization-model-with-a-custom-dataset-5efc589fedad?source=collection_archive---------18-----------------------#2021-07-14</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="34bc" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">深入探讨新发布的端到端模型培训和部署功能</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/f37521622c3b2a1d873c772d0853824a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BRBqp_34so71WEiOtYY1Ug.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">Igor Saveliev 在<a class="ae kv" href="https://pixabay.com/" rel="noopener ugc nofollow" target="_blank"> Pixabay </a>上拍摄的照片</p></figure><p id="2002" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">2021年3月25日，亚马逊SageMaker和HuggingFace <a class="ae kv" href="https://huggingface.co/blog/the-partnership-amazon-sagemaker-and-hugging-face" rel="noopener ugc nofollow" target="_blank">宣布了一项合作</a>，旨在利用可访问的<a class="ae kv" href="https://huggingface.co/transformers/master/" rel="noopener ugc nofollow" target="_blank">变形金刚</a>库，让训练最先进的NLP模型变得更容易。<a class="ae kv" href="https://huggingface.co/transformers/sagemaker.html" rel="noopener ugc nofollow" target="_blank"> HuggingFace深度学习容器</a>开放了大量预先训练的模型，可直接与<a class="ae kv" href="https://sagemaker.readthedocs.io/en/stable/index.html" rel="noopener ugc nofollow" target="_blank"> SageMaker SDK </a>一起使用，从而轻松为工作提供正确的基础设施。<br/>从一个已经在一个巨大的语料库上训练过的模型开始也意味着，为了在你的特定用例上获得良好的性能，你不需要获取一个非常大的数据集。在本教程中，我将带您完成以下3个步骤:</p><ul class=""><li id="010e" class="ls lt iq ky b kz la lc ld lf lu lj lv ln lw lr lx ly lz ma bi translated">在一个定制的数据集上微调一个摘要模型:这将需要准备您的数据用于摄取，并启动一个SageMaker培训工作；</li><li id="46ac" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">将微调后的模型部署到<a class="ae kv" href="https://docs.aws.amazon.com/sagemaker/latest/dg/deploy-model.html" rel="noopener ugc nofollow" target="_blank"> SageMaker托管服务</a>:这将自动提供一个托管您的模型的持久端点，您可以从中获得实时预测。</li><li id="dc7f" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">在我们的本地机器上启动一个<a class="ae kv" href="https://streamlit.io/" rel="noopener ugc nofollow" target="_blank"> Streamlit </a>应用程序来与您的模型进行交互；这是一个开源框架，允许您从非常简单的Python脚本创建交互式web应用程序，常用于ML和数据应用程序。</li></ul></div><div class="ab cl mg mh hu mi" role="separator"><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml"/></div><div class="ij ik il im in"><p id="18eb" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们将从设置一个SageMaker Notebook实例开始，在这里我们将运行本教程的代码。您可以<a class="ae kv" href="https://docs.aws.amazon.com/sagemaker/latest/dg/nbi.html" rel="noopener ugc nofollow" target="_blank">按照这些说明</a>来设置您的笔记本电脑环境；ml.m5.xlarge Notebook实例类型应该就可以了。继续启动一个新的Jupyter笔记本，带有一个<code class="fe mn mo mp mq b">conda_pytorch_p36</code>内核。我们需要的所有软件包都预装在笔记本电脑环境中。<br/> <br/>如果你打算遵循本教程中的代码，现在是将这个回购克隆到你的实例中的好时机。有三个笔记本(数据准备、微调和部署)，其中包含接下来的代码片段。做完那件事，让我们开始吧！</p><h1 id="644a" class="mr ms iq bd mt mu mv mw mx my mz na nb jw nc jx nd jz ne ka nf kc ng kd nh ni bi translated">模型和数据集</h1><p id="d4c3" class="pw-post-body-paragraph kw kx iq ky b kz nj jr lb lc nk ju le lf nl lh li lj nm ll lm ln nn lp lq lr ij bi translated">出于本教程的目的，我们将使用<a class="ae kv" href="https://huggingface.co/google/pegasus-large" rel="noopener ugc nofollow" target="_blank"> PEGASUS </a>模型，以及在Transformers Github资源库中可用的<a class="ae kv" href="https://github.com/huggingface/transformers/blob/v4.6.1/examples/pytorch/summarization/run_summarization.py" rel="noopener ugc nofollow" target="_blank">微调脚本</a>。您可以将脚本<em class="no">原样用于PEGASUS的</em>，以及<a class="ae kv" href="https://huggingface.co/models" rel="noopener ugc nofollow" target="_blank"> HuggingFace模型中枢</a>中可用的其他几个<em class="no">序列到序列</em>模型，例如<code class="fe mn mo mp mq b"><a class="ae kv" href="https://huggingface.co/facebook/bart-large" rel="noopener ugc nofollow" target="_blank">BART</a></code>和<code class="fe mn mo mp mq b"><a class="ae kv" href="https://huggingface.co/t5-large" rel="noopener ugc nofollow" target="_blank">T5</a></code>(参见脚本的<a class="ae kv" href="https://github.com/huggingface/transformers/tree/v4.6.1/examples/pytorch/summarization/README.md" rel="noopener ugc nofollow" target="_blank">自述文件</a>的<em class="no">支持的架构</em>部分中的所有合适选项)。<br/> <br/>脚本向用户公开配置和超参数选项，同时负责适当的文本标记化、训练循环等。您可以通过两种方式之一提供您的数据集，要么是<strong class="ky ir"> 1) </strong>指定一个<code class="fe mn mo mp mq b">dataset_name</code>(将从<a class="ae kv" href="https://huggingface.co/datasets" rel="noopener ugc nofollow" target="_blank">hugging face Dataset Hub</a>)<em class="no">或</em> <strong class="ky ir"> 2) </strong>您本地数据文件的位置(<code class="fe mn mo mp mq b">test_file</code>、<code class="fe mn mo mp mq b">validation_file</code>和<code class="fe mn mo mp mq b">test_file)</code>)；我们对后者感兴趣。我们将会看到，这些文件是在培训时从亚马逊S3下载的。<br/><br/><a class="ae kv" href="https://github.com/EdinburghNLP/XSum/tree/master/XSum-Dataset" rel="noopener ugc nofollow" target="_blank">极端摘要(XSUM)数据集</a>包含约225，000篇BBC文章及其摘要，涵盖各种领域。<a class="ae kv" href="https://github.com/joaopcm1996/demo-sm-hf-summarization/blob/master/1_XSUM_dataset_prep.ipynb" rel="noopener ugc nofollow" target="_blank">在第一个笔记本</a>中，我们下载数据集，并提取每篇文章的正文和相应摘要。然后，我们将数据分为训练集和验证集，并将每个数据集的CSV文件上传到S3。这些文件中的每一行对应不同数据样本的<code class="fe mn mo mp mq b"><strong class="ky ir">text,summary</strong></code>；这种格式在训练脚本中被自动识别(保留CSV头)。上面的自述文件详细说明了如何以更通用的方式使用CSV或JSONLINES文件，以及如何在文件有多个额外列的情况下指定正确的列名。</p><h1 id="16dd" class="mr ms iq bd mt mu mv mw mx my mz na nb jw nc jx nd jz ne ka nf kc ng kd nh ni bi translated">微调</h1><p id="bed4" class="pw-post-body-paragraph kw kx iq ky b kz nj jr lb lc nk ju le lf nl lh li lj nm ll lm ln nn lp lq lr ij bi translated">现在，我们已经准备好为培训作业设置配置和超参数了！首先，我们定义一些导入，并检索我们的SageMaker执行角色、会话和默认S3存储桶:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="np nq l"/></div></figure><p id="7709" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">然后，我们将超参数传递给<code class="fe mn mo mp mq b"><a class="ae kv" href="https://sagemaker.readthedocs.io/en/stable/frameworks/huggingface/sagemaker.huggingface.html#huggingface-estimator" rel="noopener ugc nofollow" target="_blank">HuggingFace</a></code> <a class="ae kv" href="https://sagemaker.readthedocs.io/en/stable/frameworks/huggingface/sagemaker.huggingface.html#huggingface-estimator" rel="noopener ugc nofollow" target="_blank">估计器</a>，以及其他一些配置:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="np nq l"/></div></figure><p id="64ac" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">一个<a class="ae kv" href="https://sagemaker.readthedocs.io/en/stable/api/training/estimators.html" rel="noopener ugc nofollow" target="_blank">评估器</a>是一个抽象，它封装了SageMaker SDK中的训练；因为git支持是内置的，所以我们可以直接指定训练脚本名称和目录为<code class="fe mn mo mp mq b">entry_point</code>和<code class="fe mn mo mp mq b">source_dir</code>，以及<code class="fe mn mo mp mq b">git_config</code>中的repo和branch。PyTorch、Transformers和Python版本对应于撰写本文时HuggingFace容器中支持的最新版本。<br/> <br/>在不耗尽GPU内存的情况下，这个特定模型在一个<code class="fe mn mo mp mq b">ml.p3.2xlarge</code>实例上可计算的最大批量大约为2。这个容量将取决于其他因素，例如我们为输入序列定义的最大长度。由于这是一个相当大的数据集，如果你想增加批量大小，大幅加速训练，可以利用<a class="ae kv" href="https://docs.aws.amazon.com/sagemaker/latest/dg/data-parallel.html" rel="noopener ugc nofollow" target="_blank"> SageMaker分布式数据并行训练</a>；这个特性已经被集成到<code class="fe mn mo mp mq b">Transformers</code><a class="ae kv" href="https://huggingface.co/transformers/main_classes/trainer.html" rel="noopener ugc nofollow" target="_blank">训练器</a> API中，所以你可以利用它，除了你在上面代码片段中看到的最小设置之外，不需要改变你的训练脚本。<br/><br/><code class="fe mn mo mp mq b">hyperparameters</code>将作为命令行参数传递给微调脚本。我们定义了训练参数(比如时期数和学习率)，以及训练容器中数据所在的目录。在<a class="ae kv" href="https://docs.aws.amazon.com/sagemaker/latest/dg/your-algorithms-training-algo-running-container.html#your-algorithms-training-algo-running-container-trainingdata" rel="noopener ugc nofollow" target="_blank">文件模式</a>下，SageMaker从S3下载你的数据，使其在<code class="fe mn mo mp mq b">/opt/ml/input/data/&lt;channel_name&gt;</code>目录下可用。当您调用<code class="fe mn mo mp mq b">Estimator.fit()</code>——有效地开始训练作业——时，您提供&lt; channel_name &gt;，以及训练和验证集在S3的相应文件位置。<br/> <br/>培训工作结束后，一个<code class="fe mn mo mp mq b">model.tar.gz</code>文件会上传到你在S3上默认的<code class="fe mn mo mp mq b">session_bucket</code>；它将包含部署我们的微调模型和服务推理请求所需的模型和记号化器工件。</p><h1 id="cf12" class="mr ms iq bd mt mu mv mw mx my mz na nb jw nc jx nd jz ne ka nf kc ng kd nh ni bi translated">部署</h1><p id="394b" class="pw-post-body-paragraph kw kx iq ky b kz nj jr lb lc nk ju le lf nl lh li lj nm ll lm ln nn lp lq lr ij bi translated">最后，我们将微调后的模型部署到一个持久端点。SageMaker托管将公开一个RESTful API，您可以使用它从您的模型中实时获得预测。<br/> <br/>在<code class="fe mn mo mp mq b">inference_code</code>目录下，你会找到脚本<code class="fe mn mo mp mq b">inference.py</code>。这对流程很重要，它详细描述了:1)如何在端点之间转换输入和输出，2)如何将模型加载到推理容器中，以及3)如何从模型中获得预测。这个脚本必须有一个特定的结构，这样SageMaker就知道你的代码的哪一部分用于这些不同的功能。<br/> <br/>我们首先定义一个<code class="fe mn mo mp mq b"><a class="ae kv" href="https://sagemaker.readthedocs.io/en/stable/frameworks/huggingface/sagemaker.huggingface.html#sagemaker.huggingface.model.HuggingFaceModel" rel="noopener ugc nofollow" target="_blank">HuggingFaceModel</a></code>，包含关于模型和推理代码位置的细节:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="np nq l"/></div></figure><p id="5cc6" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">注意，我们可以通过使用<code class="fe mn mo mp mq b">huggingface_estimator.model_data</code>获得模型工件的S3 URL；另外，<code class="fe mn mo mp mq b">entry_point</code>和<code class="fe mn mo mp mq b">source_dir</code>指定推理脚本的名称和目录(存储在您的笔记本上)。<br/> <br/>然后，我们将模型部署到一个<code class="fe mn mo mp mq b">ml.m5.xlarge</code>实例:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="np nq l"/></div></figure><p id="a50f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">一旦部署完成，您可以直接使用<code class="fe mn mo mp mq b">predictor</code>对象从您的模型中获得预测。注意，我们可以传入任何<a class="ae kv" href="https://huggingface.co/transformers/master/main_classes/model.html#transformers.generation_utils.GenerationMixin.generate" rel="noopener ugc nofollow" target="_blank">模型的生成参数</a>；在这种情况下，我们指定<code class="fe mn mo mp mq b">length_penalty</code>——一个实数，如果&gt;为1，则激励生成更长的摘要，反之亦然<em class="no"/>。我们将在测试中使用一个新闻更新示例:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="np nq l"/></div></figure><p id="3d7a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们可以看到，在总结这个具体的新闻故事时，长度参数的变化非常有意义，因为较短的摘要忽略了较长摘要中透露的一条重要信息:计划停电的<em class="no">原因</em>。</p><h1 id="666c" class="mr ms iq bd mt mu mv mw mx my mz na nb jw nc jx nd jz ne ka nf kc ng kd nh ni bi translated">启动一个简单的Streamlit界面</h1><p id="2bbb" class="pw-post-body-paragraph kw kx iq ky b kz nj jr lb lc nk ju le lf nl lh li lj nm ll lm ln nn lp lq lr ij bi translated">为了不必一直通过手动运行代码行来与您的模型进行交互，此时您将在本地机器上部署一个Streamlit UI。正如您在脚本<code class="fe mn mo mp mq b">streamlit_app.py</code>中看到的，Streamlit允许我们用30多行代码创建一个直观且响应迅速的界面。在这段代码中，我们将使用用于Python的<a class="ae kv" href="https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/sagemaker-runtime.html" rel="noopener ugc nofollow" target="_blank"> SageMaker运行时Boto3客户端</a>，以不同于以前的方式调用我们的SageMaker端点。您需要在您的本地环境中设置适当的权限，或者通过使用AWS CLI配置您的凭证，或者直接将它们作为参数传递给客户端。<br/> <br/>首先，打开一个新的终端并运行以下命令来安装Streamlit:</p><pre class="kg kh ki kj gt nr mq ns nt aw nu bi"><span id="4441" class="nv ms iq mq b gy nw nx l ny nz">pip install streamlit</span></pre><p id="d10a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">要启动界面，请运行以下命令:</p><pre class="kg kh ki kj gt nr mq ns nt aw nu bi"><span id="27d0" class="nv ms iq mq b gy nw nx l ny nz">streamlit run \<br/>streamlit_app.py -- --endpoint_name summarization-endpoint</span></pre><p id="e311" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这将在<code class="fe mn mo mp mq b">localhost</code>上启动您的应用程序，默认监听端口8501。运行该命令后，应该会自动弹出一个浏览器标签，但如果没有，就访问<code class="fe mn mo mp mq b">http:localhost:8501</code>。<br/> <br/>这是界面的样子:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oa"><img src="../Images/1dc5aba8aada2b3cf9e942c8b28dee75.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GO_IWIiY1FGWGhYAgH1_HQ.png"/></div></div></figure><p id="3f2a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在，您甚至可以更新/切换支持应用程序的模型端点，这样您就可以在不同的数据集上进行重新训练或微调后对其进行实时测试！</p><h1 id="00f6" class="mr ms iq bd mt mu mv mw mx my mz na nb jw nc jx nd jz ne ka nf kc ng kd nh ni bi translated">结论</h1><p id="c696" class="pw-post-body-paragraph kw kx iq ky b kz nj jr lb lc nk ju le lf nl lh li lj nm ll lm ln nn lp lq lr ij bi translated">在这篇博文中，我们看到了如何利用HuggingFace SageMaker估计器的固有功能来微调最先进的摘要模型。最重要的是，我们使用了一个自定义数据集和一个现成的示例脚本，您可以复制这些东西，以便轻松地根据您个人/公司的数据训练模型。<br/> <br/>我们还看到了如何轻松地获得最终的微调模型，将其部署到完全受管的端点，并通过调用简单的Streamlit接口的<em class="no">。<br/> <br/>在现实生活场景中，您将能够轻松地优化和扩展该流程的各个部分，如设置<a class="ae kv" href="https://docs.aws.amazon.com/sagemaker/latest/dg/endpoint-auto-scaling.html" rel="noopener ugc nofollow" target="_blank">自动缩放</a>以基于负载自动启动更多端点实例，将GPU 的一部分<a class="ae kv" href="https://docs.aws.amazon.com/sagemaker/latest/dg/ei.html" rel="noopener ugc nofollow" target="_blank">附加到您的端点以加速推理，以及许多其他有用的功能。<br/> <br/>我希望你现在能够更快地利用大量可用的NLP资源。下次见！</a></em></p></div><div class="ab cl mg mh hu mi" role="separator"><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml"/></div><div class="ij ik il im in"><p id="32dd" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><em class="no">附:非常感谢</em><a class="ob oc ep" href="https://medium.com/u/993c21f1b30f?source=post_page-----5efc589fedad--------------------------------" rel="noopener" target="_blank"><em class="no">Heiko Hotz</em></a><em class="no">激励并支持这篇博文的创作。</em></p><h2 id="38f6" class="nv ms iq bd mt od oe dn mx of og dp nb lf oh oi nd lj oj ok nf ln ol om nh on bi translated"><em class="oo">参考文献</em></h2><p id="1ecc" class="pw-post-body-paragraph kw kx iq ky b kz nj jr lb lc nk ju le lf nl lh li lj nm ll lm ln nn lp lq lr ij bi translated">[1] J. Zhang，Y. Zhao，M. Saleh，P. Liu，<a class="ae kv" href="https://arxiv.org/abs/1912.08777" rel="noopener ugc nofollow" target="_blank"> PEGASUS:用于抽象概括的提取间隙句的预训练</a> (2020)第37届机器学习国际会议论文集</p><p id="7be6" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">【2】s .纳拉扬，S.B .科恩，m .拉帕塔，<a class="ae kv" href="https://arxiv.org/abs/1808.08745" rel="noopener ugc nofollow" target="_blank">不要给我讲细节，只讲概要！用于极端摘要的话题感知卷积神经网络</a>(2018)2018年自然语言处理经验方法会议录</p></div></div>    
</body>
</html>