<html>
<head>
<title>Image Captioning by Translational Visual-to-Language Models</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">基于视觉-语言转换模型的图像字幕</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/image-captioning-by-translational-visual-to-language-models-d728bced41c3?source=collection_archive---------17-----------------------#2021-10-26">https://towardsdatascience.com/image-captioning-by-translational-visual-to-language-models-d728bced41c3?source=collection_archive---------17-----------------------#2021-10-26</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="494a" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsdatascience.com/tagged/thoughts-and-theory" rel="noopener" target="_blank">思想和理论</a></h2><div class=""/><div class=""><h2 id="6466" class="pw-subtitle-paragraph jw iz iq bd b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn dk translated">生成具有视觉注意力的自主字幕</h2></div><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ko"><img src="../Images/e4bedd8ffe4903009bce09ab0195eac0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Xqo-hM6Ii3FoRXZc1jUnEg.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">示例生成的标题(图片由作者提供)</p></figure><p id="b8c2" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">这是一个以实验为目的的研究项目，有很深的学术文档，所以如果你是一个论文爱好者，那么就去查看这篇文章的项目页面</p><div class="ma mb gp gr mc md"><a href="https://github.com/cankocagil/Image-Captioning" rel="noopener  ugc nofollow" target="_blank"><div class="me ab fo"><div class="mf ab mg cl cj mh"><h2 class="bd ja gy z fp mi fr fs mj fu fw iz bi translated">GitHub-cankocagil/Image-Captioning:图像字幕的卷积语言模型</h2><div class="mk l"><h3 class="bd b gy z fp mi fr fs mj fu fw dk translated">这份报告主要是集中在图像字幕任务使用的先进技术的背景下深…</h3></div><div class="ml l"><p class="bd b dl z fp mi fr fs mj fu fw dk translated">github.com</p></div></div><div class="mm l"><div class="mn l mo mp mq mm mr ky md"/></div></div></a></div><p id="ee45" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">这篇文章将是从论文到文章的一个几乎直接的过渡，将涵盖使用简单而有效的计算机视觉和NLP应用程序的图像字幕任务，并具有深刻的解释。如果你正在寻找一个带有可解释的代码、计算和文档的图像标题的启动，你来对地方了。包括一切。本文将向您介绍任何图像字幕任务的概念和上下文设计，包括全面的视觉效果和“从头开始”的代码，以及解释。为了简单起见，代码被排除在外，但托管在项目页面上。来自其他来源的视觉资料的参考文献在标题中嵌入了链接，而其他参考文献则在最后给出。让我们潜入更深的地方。</p><p id="4458" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated"><strong class="lg ja"> <em class="ms">全景</em> </strong></p><p id="fe48" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">本文主要关注在深度学习环境下使用最新技术的图像字幕任务。图像字幕是通过使用自然语言处理和计算机视觉应用生成图像的文本描述的过程。该网络由卷积神经网络(CNN)组成，用于将图像编码为潜在空间表示，随后是递归神经网络(RNN ),用于解码特征表示和构建语言模型。具体来说，长短期记忆(LSTM)和门控循环单位(GRU)被用作RNN模型与注意机制和教师强迫算法。为了实现这一点，AlexNet、VGG网络、ResNet、DenseNet和SquezeeNet等迁移学习应用程序被用作卷积编码器，而单词表示的全局向量(GloVe)被用于单词嵌入。Microsoft上下文中的公共对象(MSCOCO)数据集用于训练和测试。实施各种数据扩充技术来提高模型性能。该模型由Adam optimizer以预定的学习率编译。掩蔽的交叉熵损失被用作模型的标准。最后，实现了beam和贪婪搜索算法，以获得最佳的图像到字幕的翻译。</p><p id="ef91" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated"><strong class="lg ja"> <em class="ms">要覆盖的关键字</em> </strong></p><ul class=""><li id="27c9" class="mt mu iq lg b lh li lk ll ln mv lr mw lv mx lz my mz na nb bi translated">图像字幕问题的表述</li><li id="765d" class="mt mu iq lg b lh nc lk nd ln ne lr nf lv ng lz my mz na nb bi translated">图像字幕的理据</li><li id="8599" class="mt mu iq lg b lh nc lk nd ln ne lr nf lv ng lz my mz na nb bi translated">视觉和语言翻译模型</li><li id="0274" class="mt mu iq lg b lh nc lk nd ln ne lr nf lv ng lz my mz na nb bi translated">迁移学习</li><li id="ec21" class="mt mu iq lg b lh nc lk nd ln ne lr nf lv ng lz my mz na nb bi translated">数据扩充</li><li id="1d27" class="mt mu iq lg b lh nc lk nd ln ne lr nf lv ng lz my mz na nb bi translated">注意模型和教师强迫算法</li><li id="f8b0" class="mt mu iq lg b lh nc lk nd ln ne lr nf lv ng lz my mz na nb bi translated">传统CNN:Alex Net、VGG网络、ResNet、DenseNet和SquezeeNet</li><li id="8676" class="mt mu iq lg b lh nc lk nd ln ne lr nf lv ng lz my mz na nb bi translated">语言模型:长短期记忆(LSTM)和门控循环单元(GRU)</li><li id="05a0" class="mt mu iq lg b lh nc lk nd ln ne lr nf lv ng lz my mz na nb bi translated">单词表示的全局向量(GloVe)</li><li id="fc4d" class="mt mu iq lg b lh nc lk nd ln ne lr nf lv ng lz my mz na nb bi translated">波束和贪婪搜索</li><li id="3904" class="mt mu iq lg b lh nc lk nd ln ne lr nf lv ng lz my mz na nb bi translated">蓝色分数和流星</li></ul><h2 id="7c6a" class="nh ni iq bd nj nk nl dn nm nn no dp np ln nq nr ns lr nt nu nv lv nw nx ny iw bi translated">1.简介和动机</h2><p id="c646" class="pw-post-body-paragraph le lf iq lg b lh nz ka lj lk oa kd lm ln ob lp lq lr oc lt lu lv od lx ly lz ij bi translated">随着与人工智能相关的新技术的出现，图像字幕已经成为最吸引研究者的领域之一。图像字幕，根据图像中观察到的内容自动生成自然语言描述，是场景理解的重要组成部分，融合了计算机视觉和自然语言处理的知识[1]。图像字幕的应用非常广泛，例如连接人机交互，它还可以帮助视障人士“看”未来的世界[1]。在图像字幕的早期阶段，统计语言模型被用来提出为图像生成字幕的解决方案。李等人提出了一种基于网络规模的n元语法方法，收集候选短语并将其合并以形成从零开始描述图像的句子[2]。Yang等人提出了一种从英语Gigaword语料库训练的语言模型，以获得图像中运动的估计以及搭配的名词、场景和介词的概率，并使用这些估计作为隐马尔可夫模型的参数[2]。根据文献综述，我们提出了深度学习背景下图像字幕技术的基本阶段。以下是图像字幕管道和广泛使用的技术的概述:</p><p id="767f" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated"><strong class="lg ja"> <em class="ms"> 1.1特征提取</em> </strong></p><p id="f004" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">图像字幕任务从从图像中提取特征开始，以将可能的3 (RGB)通道高维数据的维度减少到潜在空间表示中。在文献中，已经有预先训练的模型，这些模型通过使用由超过120万幅自然图像组成的称为“图像网”的数据集进行训练，例如AlexNet、VGG网、ResNet、GoogleNet、DenseNet、SqueezeNet等等。我们实现了所有提到的模型，除了GoogleNet，我们将在本文后面看到。</p><p id="9f81" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated"><strong class="lg ja"> <em class="ms"> 1.2语言模型</em> </strong></p><p id="d94f" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">作为图像字幕的第二阶段，字幕和潜在空间特征向量被赋予语言模型以生成字幕。为了实现这一点，有各种各样的模型在文献中广泛使用，如LSTM的，双向LSTM的，RNN的，CNN的，GRU的，和TPGN的。我们在实现中使用了LSTM和GRU的递归网络，我们将在方法部分讨论。</p><p id="4079" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated"><strong class="lg ja"> <em class="ms"> 1.3常用技巧</em> </strong></p><p id="1741" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">为了生成图像字幕模型，在文献中使用了以下图像到序列技术:</p><ul class=""><li id="0c1e" class="mt mu iq lg b lh li lk ll ln mv lr mw lv mx lz my mz na nb bi translated">编码器-解码器</li><li id="319a" class="mt mu iq lg b lh nc lk nd ln ne lr nf lv ng lz my mz na nb bi translated">注意机制</li><li id="9132" class="mt mu iq lg b lh nc lk nd ln ne lr nf lv ng lz my mz na nb bi translated">新对象范式</li><li id="34f8" class="mt mu iq lg b lh nc lk nd ln ne lr nf lv ng lz my mz na nb bi translated">语义学</li></ul><p id="9cfa" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">此外，注意有各种算法用于实现图像到序列网络，这些是广泛使用的技术。在这个项目中，我们使用了编码器-解码器模型和带有注意机制的教师强制器。</p><p id="26c1" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated"><strong class="lg ja"> <em class="ms"> 1.4数据集</em> </strong></p><p id="32d4" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">MS COCO和Flickr数据集广泛用于图像字幕任务。我们在实现中使用了MSCOCO。</p><p id="0a95" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated"><strong class="lg ja"> <em class="ms"> 1.5绩效指标</em> </strong></p><p id="088a" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">为了评估图像到序列模型的性能，使用以下评估指标:</p><ul class=""><li id="c97a" class="mt mu iq lg b lh li lk ll ln mv lr mw lv mx lz my mz na nb bi translated">BLEU-i(对于i = 1，…，4)(例如，BLUE-1)</li><li id="0d71" class="mt mu iq lg b lh nc lk nd ln ne lr nf lv ng lz my mz na nb bi translated">苹果酒</li><li id="a511" class="mt mu iq lg b lh nc lk nd ln ne lr nf lv ng lz my mz na nb bi translated">流星</li></ul><p id="f1b1" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">对于我们的实现，我们使用BLUE-1、BLUE-2、BLUE-3、BLUE-4和METEOR度量来评估语言模型在训练结束时的输出。</p><p id="8f78" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated"><strong class="lg ja"> <em class="ms"> 1.6建筑管道</em> </strong></p><p id="dca1" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">如上所述，在我们的例子中，MSCOCO数据集用于训练/验证和测试，它有超过80 000幅自然图像。这些图像的网址给我们使用。但是，由于大约10%的URL被破坏，我们有近70 000到73 000张图像用于训练和验证。整个集被分成15%的验证集和85%的训练集。此外，在该数据集中，每幅图像都配有4到5个描述该特定图像内容的相关标题。然后，实施各种数据扩充技术来提高模型的性能。对于特征提取，我们使用具有预训练模型的CNN编码器结构，ResNet152、AlexNet、VGG19-Net、DenseNet、SqueezeNet，来试验特征提取的性能。在这样的实验之后，ResNet152的表现比我们在后面几页中看到的稍好一些。(ResNet代表用于图像识别的深度残差学习。)我们的编码器CNN模型是在ImageNet数据集中预先训练的，由多个卷积层组成。对于ResNet152，残差神经网络利用跳过连接或快捷方式跳过一些层，以降低层对给定图像数据的适应性。典型的<em class="ms"> ResNet </em>模型通过双层或三层跳跃实现，其中包含非线性(<a class="ae oe" href="https://en.wikipedia.org/wiki/Rectifier_(neural_networks)" rel="noopener ugc nofollow" target="_blank"> ReLU </a>)和<a class="ae oe" href="https://en.wikipedia.org/wiki/Batch_normalization" rel="noopener ugc nofollow" target="_blank">批处理归一化</a>[3]。跳过层的一个动机是避免<a class="ae oe" href="https://en.wikipedia.org/wiki/Vanishing_gradient_problem" rel="noopener ugc nofollow" target="_blank">消失梯度</a>的问题，通过重用前一层的激活，直到相邻层学习其权重【3】。我们的剩余网络由152层组成。</p><p id="5afc" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">然后，对于语言模型，通过注意机制和教师强制技术，使用LSTM和GRU递归神经网络。作为第一步，单词嵌入层用于句子长度字幕的单词表示。为了实现这一点，我们使用了单词表示的全局向量(GloVe)，这是一种用于获得单词的向量表示的无监督学习算法[4]。在来自语料库的聚合的全局单词-单词共现统计上执行训练，并且产生的表示展示了单词向量空间的有趣的线性子结构[4]。我们使用的手套模型是用60亿个标记和400 000个语料库训练的。然后，为了最小化模型的开销，使用了掩蔽交叉熵损失，并通过Adam优化器对编码器和解码器模型进行优化。然后，在训练阶段的最后，我们希望我们的视觉和语言模型能够为自然图像生成人类级别的字幕。</p><h2 id="bf41" class="nh ni iq bd nj nk nl dn nm nn no dp np ln nq nr ns lr nt nu nv lv nw nx ny iw bi translated">2.方法</h2><p id="2453" class="pw-post-body-paragraph le lf iq lg b lh nz ka lj lk oa kd lm ln ob lp lq lr oc lt lu lv od lx ly lz ij bi translated"><strong class="lg ja"> <em class="ms"> 2.1训练/验证/测试拆分</em> </strong></p><p id="a826" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">我们将给定的训练数据集分成85%的训练和15%的验证，以便在训练时跟踪模型的历史，并且我们还应用了交叉验证技术，以便在模型开始过度拟合时停止训练。因此，在数据集分割结束时，我们有340 114个唯一的训练图像和60 021个唯一的验证图像。此外，一个单独的测试数据集给我们，以衡量一个模型的性能与各种各样的自然图像。</p><p id="0764" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated"><strong class="lg ja"> <em class="ms"> 2.2预处理</em> </strong></p><p id="489a" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">由于图像有不同的大小，我们首先转换可接受大小的图像，在我们的例子中是224x224，以满足视觉模型的要求。所有提到的CNN模型都接受(N x C x H x W)约定的输入，即PyTorch RGB图像批处理约定。在我们的例子中，所有CNN模型都接受C，H，W = (3，224，224)约定。最后，对所有图像进行归一化，以获得零均值和单位标准差分布，从而加速训练。我们通过以下方式分别对每个颜色通道应用标准化</p><ul class=""><li id="d7f3" class="mt mu iq lg b lh li lk ll ln mv lr mw lv mx lz my mz na nb bi translated">平均值= [0.485，0.456，0.406]</li><li id="2533" class="mt mu iq lg b lh nc lk nd ln ne lr nf lv ng lz my mz na nb bi translated">标准偏差= [0.229，0.224，0.225]</li></ul><p id="e480" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">使用统一公式:</p><p id="8ebe" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">输出[通道] =(输入[通道] —平均[通道]) /标准[通道]</p><p id="60ca" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">这些是ImageNet的RGB通道平均值和标准偏差，可以表示自然图像的广义平均值和标准偏差。</p><p id="abbf" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated"><strong class="lg ja"> <em class="ms"> 2.3数据扩充</em> </strong></p><p id="c9d4" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">数据分析中的数据扩充是一种用于增加数据量的技术，通过添加已有数据的稍微修改的副本或从现有数据中新创建的合成数据[5]。它充当<a class="ae oe" href="https://en.wikipedia.org/wiki/Regularization_(mathematics)" rel="noopener ugc nofollow" target="_blank">正则化子</a>，并在训练深度学习模型时帮助减少<a class="ae oe" href="https://en.wikipedia.org/wiki/Overfitting" rel="noopener ugc nofollow" target="_blank">过拟合</a>。为了实现这一点，我们应用了以下数据扩充技术:</p><ul class=""><li id="9eca" class="mt mu iq lg b lh li lk ll ln mv lr mw lv mx lz my mz na nb bi translated"><strong class="lg ja">随机水平翻转</strong></li></ul><p id="bb5b" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">我们以0.5的概率水平翻转图像数据。</p><ul class=""><li id="8c77" class="mt mu iq lg b lh li lk ll ln mv lr mw lv mx lz my mz na nb bi translated"><strong class="lg ja">随机垂直翻转</strong></li></ul><p id="8941" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">我们以0.5的概率垂直翻转图像数据。</p><ul class=""><li id="9f84" class="mt mu iq lg b lh li lk ll ln mv lr mw lv mx lz my mz na nb bi translated"><strong class="lg ja">随机裁剪</strong></li></ul><p id="49c4" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">我们随机裁剪224x224尺寸的图像。</p><ul class=""><li id="9579" class="mt mu iq lg b lh li lk ll ln mv lr mw lv mx lz my mz na nb bi translated"><strong class="lg ja">随机调整大小裁剪</strong></li></ul><p id="977c" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">我们将图像的大小调整为256x256，然后随机裁剪224x224的部分。</p><ul class=""><li id="16af" class="mt mu iq lg b lh li lk ll ln mv lr mw lv mx lz my mz na nb bi translated"><strong class="lg ja">中心裁剪</strong></li></ul><p id="d726" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">我们将图像的大小调整为256x256，然后在中心裁剪224x224的部分。</p><p id="eecf" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated"><strong class="lg ja"> <em class="ms"> 2.4传输学习:编码器CNN </em> </strong></p><p id="74cb" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">如前所述，我们分别使用了ResNet152、AlexNet、VGG19-Net、DenseNet和SqueezeNet将高维图像转换为潜在空间表示。因此，我们应用特征提取，即，我们在训练时冻结CNN模型的层的可学习/可训练参数，即，我们不更新编码器模型的网络参数。迁移学习的另一种可能的方法是微调网络，但由于计算方面的考虑，我们更喜欢只提取特征。然后，我们将我们的特征向量传递到嵌入层，得到一个嵌入的图像特征向量。这个嵌入层以一种可学习的方式将潜在空间表示转换成嵌入空间。现在，我们的图像已经准备好输入语言模型了。下图显示了所解释的体系结构。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi of"><img src="../Images/4af52e2c2f04863a139ca6050ac46ef9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*SeIrkgVkiZ5UK2qI"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated"><a class="ae oe" href="https://gitlab.com/behnam.vr/image-captioning" rel="noopener ugc nofollow" target="_blank">编码器CNN嵌入</a>由<a class="ae oe" href="https://www.behnamkvl.me/" rel="noopener ugc nofollow" target="_blank">贝南·瓦基利</a></p></figure><p id="677b" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">因此，嵌入的图像特征向量作为解码器网络的初始输入，在教师强制器算法中具有<x_start_>字幕。</x_start_></p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi og"><img src="../Images/44ae2e03c7af0a1c81ba5b5ddbef88a9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*a_QTpJQiLUaaLA_x"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated"><a class="ae oe" href="https://gitlab.com/behnam.vr/image-captioning" rel="noopener ugc nofollow" target="_blank">编码器CNN到语言模型</a>由<a class="ae oe" href="https://www.behnamkvl.me/" rel="noopener ugc nofollow" target="_blank"> Behnam Vakili </a></p></figure><p id="2e17" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated"><strong class="lg ja"> <em class="ms"> 2.5迁移学习:单词嵌入(手套)</em> </strong></p><p id="2b2f" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">如前所述，我们为单词嵌入层购买了一个预训练的手套单词向量。这加快了我们的训练阶段，因为这些向量已经用60亿个英语标记进行了训练。因此，我们成功地将字幕转换成高级单词表示，以便它们可以输入到解码器模型中。</p><p id="40b0" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">这一层将我们的句子长度序列字幕转换成嵌入的单词特征向量，以便字幕可以传递给语言模型。</p><p id="b0bf" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated"><strong class="lg ja"> <em class="ms"> 2.6解码器配教师强迫器</em> </strong></p><p id="e7cb" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">自然语言处理中的许多递归神经网络(例如，图像字幕，机器翻译)在训练过程中使用教师强制[6]。教师强制是一种快速有效地训练递归神经网络模型的方法，该模型使用来自先前时间步骤的基础事实[7]。我们举一个teacher forcer算法的例子，让任意图像的地面真相标题为“两个人在看书”。我们的模型在预测第二个单词时犯了一个错误，我们在第一个和第二个预测中分别有“两只”和“鸟”[8]</p><ul class=""><li id="c967" class="mt mu iq lg b lh li lk ll ln mv lr mw lv mx lz my mz na nb bi translated">如果没有<em class="ms">老师强迫</em>，我们会把“鸟”喂回我们的RNN去预测第三个单词。假设第三个预测是“飞行”。尽管我们的模型预测“飞行”是有意义的，因为输入是“鸟”，但这与地面事实不同。[8]</li><li id="6dfb" class="mt mu iq lg b lh nc lk nd ln ne lr nf lv ng lz my mz na nb bi translated">另一方面，如果我们使用<em class="ms">教师强制</em>，我们将在计算并记录第二次预测的损失后，为第三次预测向我们的RNN提供“人”[8]</li></ul><p id="f41c" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">然后，我们来讨论一下teacher forcer的利弊。</p><ul class=""><li id="ca2e" class="mt mu iq lg b lh li lk ll ln mv lr mw lv mx lz my mz na nb bi translated"><strong class="lg ja">优点</strong></li></ul><p id="aa71" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">用<em class="ms">老师强制</em>训练收敛更快。在训练的早期阶段，模型的预测非常糟糕。如果我们不使用<em class="ms">老师强制</em>，模型的隐藏状态会被一系列错误的预测更新，误差会累积，模型很难从中学习。[10]</p><ul class=""><li id="b813" class="mt mu iq lg b lh li lk ll ln mv lr mw lv mx lz my mz na nb bi translated"><strong class="lg ja">缺点</strong></li></ul><p id="4fa2" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">在推断过程中，由于通常没有实际情况可用，RNN模型需要将自己之前的预测反馈给自己，以便进行下一次预测。因此，训练和推理之间存在差异，这可能会导致模型性能不佳和不稳定。这在文献[11]中被称为<em class="ms">曝光偏差</em>。</p><p id="2df4" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">因此，我们在语言模型中使用了teacher forcer算法。</p><p id="dde6" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated"><strong class="lg ja"> <em class="ms"> 2.7带教师强制器和注意力机制的解码器</em> </strong></p><p id="86a0" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">作为模型的第二阶段，我们用LSTM和GRU网络实现了一个带注意机制的解码器。RNN的工作是解码特征和单词向量，并将其转化为单词序列[6]。在解码器中，作为教师强制算法的一部分，我们首先在时间t = 0将嵌入的特征向量传递给解码器。然后，我们使用实际的teacher forcer算法逐字传递字幕。因此，我们实现了一个语言模型来将潜在的空间向量映射到单词空间。[7].这里的关键思想是在时间t=0 [7]将表示图像的潜在空间向量作为输入馈送到递归单位单元。从时间t=1开始，我们可以开始将嵌入的目标句子馈送到递归单元中，作为teacher forcer算法的一部分[8]。然后，LSTM单元的输出是隐藏状态向量。因此，我们需要某种从隐藏状态空间到词汇(字典)空间的映射[9]。我们可以通过在隐藏状态空间和词汇空间之间使用完全连接的层来实现这一点[9]。以下架构描述了LSTM机制:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi of"><img src="../Images/480fd85ba438fc9c2c82a3b7fd48d9bb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*OivSuelxf8recrbp"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated"><a class="ae oe" href="https://gitlab.com/behnam.vr/image-captioning" rel="noopener ugc nofollow" target="_blank">解码器机制</a>由<a class="ae oe" href="https://www.behnamkvl.me/" rel="noopener ugc nofollow" target="_blank">贝南·瓦基利</a>制作</p></figure><p id="d029" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">然后，在注意力集中的<em class="ms">设置中，我们希望解码器能够在序列中的不同点查看图像的不同部分。我们使用所有像素的<em class="ms">加权</em>平均值，而不是简单的平均值，重要像素的权重更大【17】。该图像的加权表示可以在每一步与先前生成的单词连接，以生成下一个单词[17]。注意力机制计算这些权重来估计图像的重要部分。我们已经使用了随机软注意机制，其中像素的权重相加为1，如在“展示、出席和讲述”论文[18]中所提议的。如果在我们的编码图像中有P个像素，那么在每个时间步长t</em></p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi oh"><img src="../Images/2ecb36ac24c70ff216fca80ead4c3369.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*buB7petewsj5V5yxWT8V8A.png"/></div></div></figure><p id="f4af" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">人们可以将这整个过程解释为计算一个像素是<em class="ms">一个</em>位置的概率，以生成下一个单词[17]。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi oi"><img src="../Images/e1d5570394d0cd0d616954d2af99a181.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*F80LDyVbUa_2XcGh"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated"><a class="ae oe" href="https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Image-Captioning" rel="noopener ugc nofollow" target="_blank">注意力随时间推移的机制</a>由<a class="ae oe" href="https://github.com/sgrvinod" rel="noopener ugc nofollow" target="_blank">萨加尔·维诺达巴布</a></p></figure><p id="3037" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">数据流从卷积视觉模型开始，以创建图像的潜在空间表示，然后是递归模型，以创建LSTM的初始隐藏和单元状态，以及GRU解码器的隐藏状态。在解码的每个时间步，潜在空间表示和先前计算的递归单元的隐藏状态被用于生成图像像素的权重，作为注意机制的一部分。然后，基本事实字幕和编码的加权平均值被馈送到解码器语言模型，以生成下一个字幕，作为教师强制器和注意力算法的组合。下图代表了教师强迫者在软随机注意中的信息流。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi oj"><img src="../Images/7f2646c1fe55f2f09d920be478b49ea7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*VGYHffeMNGFTgLuy"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated"><a class="ae oe" href="https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Image-Captioning" rel="noopener ugc nofollow" target="_blank">编码器到注意力管道</a>由<a class="ae oe" href="https://github.com/sgrvinod" rel="noopener ugc nofollow" target="_blank">萨加尔·维诺达巴布</a></p></figure><p id="afbf" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated"><strong class="lg ja"> <em class="ms"> 2.8定额渐变裁剪</em> </strong></p><p id="6bce" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">梯度裁剪是一种防止在非常深的网络中(通常在递归神经网络中)爆发梯度的技术[17]。有许多方法来计算梯度裁剪，但一个常见的是重新调整梯度，使他们的范数最多是一个特定的值[17]。通过梯度裁剪，引入预定的梯度阈值，然后缩小超过该阈值的梯度范数以匹配范数[17]。这防止任何梯度具有大于阈值的范数，因此梯度被剪切[17]。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ok"><img src="../Images/34cc2109a2cc911115d5e6808692cf11.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yxVqc5Sidltqx6fToMcA9g.png"/></div></div></figure><p id="0132" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">其中g是要限幅的梯度，thres是作为超参数的阈值，而‖g‖是g的范数。</p><p id="3944" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">因此，我们实现了范数梯度裁剪，以将梯度保持在某个范围内，该范围由阈值表征，并且在这样的实验之后被确定为10</p><p id="128f" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated"><strong class="lg ja"> <em class="ms"> 2.9 GPU加速和并行分布式处理</em> </strong></p><p id="f41e" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">然后，由于我们正在使用GPU加速和分布式计算，我们需要将我们的数据类型转换为有能力在GPU中运行图像处理任务的张量。具体来说，NVIDIA Tesla K80和GeForce RTX 2080 TI被用作GPU来加速训练。然后，我们使用了分布式数据并行(DDP ),它在模块级实现了数据并行，可以跨多个GPU运行。[10]使用DDP的应用程序产生多个进程，并为每个进程创建一个DDP实例[10]。因此，我们利用了CUDA多处理。为了实现这一点，利用了深度学习框架PyTorch的DataParellel包。</p><p id="55b3" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated"><strong class="lg ja"> <em class="ms"> 2.10掩蔽交叉熵</em> </strong></p><p id="a767" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">屏蔽交叉熵实际上是一种分类交叉熵，将屏蔽应用于由序列长度确定的一些输入。这背后的原因是我们有填充序列，即数据集中的每个字幕都有不同的长度，因此要构建字幕向量，我们需要通过在字幕末尾添加<pad>来填充间隙。视觉表现是:</pad></p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi oi"><img src="../Images/ec246167900d1e5e47793cafa31d61e3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*e8xDVPG5VJGPc8kl"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated"><a class="ae oe" href="https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Image-Captioning" rel="noopener ugc nofollow" target="_blank">用<a class="ae oe" href="https://github.com/sgrvinod" rel="noopener ugc nofollow" target="_blank">萨加尔·维诺达巴布</a>填充字幕</a></p></figure><p id="7a11" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">因此，为了不计算填充区域的损失和梯度，我们实现了采用预测字幕、实际字幕和句子长度的掩蔽交叉熵，并对非填充区域应用交叉熵。这加快了训练过程。</p><p id="5d15" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated"><strong class="lg ja"> <em class="ms"> 2.11 Adam优化器</em> </strong></p><p id="e6f6" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">编码器和解码器模型都使用Adam optimizer通过以下数学表达式进行优化:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ol"><img src="../Images/b7bb497f754c3adf30a3be75deea88cd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BNIHNRQ4_uPijK_BEucUlA.png"/></div></div></figure><p id="3082" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">其中M_i和δ_V_i分别是一阶和二阶矩中梯度的累加和，θ是要更新的参数。还要注意，也使用了基本的基于随机梯度下降的学习规则、RMSprop和AdaGrad，但是Adam optimizer的性能最差，因此我们继续使用Adam optimizer。</p><p id="9441" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated"><strong class="lg ja"> <em class="ms"> 2.12自适应学习率调度器</em> </strong></p><p id="607d" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">在PyTorch优化器包的帮助下，我们应用了基于验证交叉熵和argmax搜索精度的动态学习率调度器。该算法在以下情况下降低学习率</p><ul class=""><li id="b5c6" class="mt mu iq lg b lh li lk ll ln mv lr mw lv mx lz my mz na nb bi translated"><em class="ms">交叉熵损失没有减少</em></li><li id="1c4f" class="mt mu iq lg b lh nc lk nd ln ne lr nf lv ng lz my mz na nb bi translated"><em class="ms">arg max预测的准确性没有提高</em></li></ul><p id="e851" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">因此，当指标停止改善时，我们降低了学习率。它启用了动态学习率调度器，并且可以在提前停止之前提高模型的性能。此外，我们还应用了一个基于批量改进的简单学习率调度器。因此，使用三种不同的学习延迟调度器来提高模型的性能。注意，作为进一步的实现，自适应学习速率调度器可以通过使用蓝分数来执行，蓝分数可以在网络内给出额外的自适应性。</p><p id="9067" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated"><strong class="lg ja"> <em class="ms"> 2.13交叉熵提前停止</em> </strong></p><p id="7ea1" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">基于交叉熵损失，以批次和时段方式跟踪模型的历史，以避免过拟合。如果训练和验证损失之间的差距开始增大，我们就停止训练。注意，由于我们应用了自适应学习速率调度，我们让模型适应新的学习速率，如果模型不能自我改进，我们就停止训练。幸运的是，我们的模型在训练和测试中表现相似，因此早期停止并不用于整个训练过程。</p><p id="38cb" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated"><strong class="lg ja"> <em class="ms"> 2.14光束搜索</em> </strong></p><p id="4b50" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">Show and Tell论文[11]提出了波束搜索，作为在给定输入图像的情况下生成具有最高出现可能性的句子的最终步骤。[12]该算法是一种最佳优先搜索算法，该算法迭代地将直到时间t的k个最佳句子的集合视为候选，以生成大小为t + 1的句子，并且仅保留它们中的最佳k个句子，因为这更好地近似了获得论文[12]中提到的全局最大值的概率。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi om"><img src="../Images/b31633e69ca2405b8fa8357d6becef99.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*yDLRj-mAbDwBtrkm"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated"><a class="ae oe" href="https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Image-Captioning" rel="noopener ugc nofollow" target="_blank">由<a class="ae oe" href="https://github.com/sgrvinod" rel="noopener ugc nofollow" target="_blank"> Sagar Vinodababu </a>对光束搜索</a>进行视觉解释</p></figure><p id="f4c5" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">我们使用从1到9的波束大小实现了波束搜索，以查看波束大小参数的变化。结果将在下一部分讨论。</p><p id="d010" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated"><strong class="lg ja"> <em class="ms"> 2.15 A框架加速深度编解码</em> </strong></p><p id="fe66" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">让我们回忆并收集一下我们为提高模型性能所做的工作。我们通常主要使用多重处理和多线程的概念。为了加载和转换数据，我们使用了4个线程的多线程概念。此外，我们转换加载，并将应用程序转换为GPU格式以加速。然后，我们使用了PyTorch内部实现的生成器概念，这将提高RAM效率，并允许在训练阶段进行这样的操作。最重要的是，我们在GeForce RTX 2080 TI和Tesla K80上使用了PyTorch提供的分布式并行计算应用。然后，对于神经网络模型应用，我们冻结卷积模型和手套字嵌入模型的可学习参数，即，我们不计算将明显加速训练阶段的这些层的梯度。对于语言模型，我们实现了teacher forcer算法，该算法也提高了生成字幕的时间效率。此外，考虑到模型性能和时间效率，调整了各种超参数。最后，使用提到的不同优化器来测量优化时间，以便根据批量改进和更新时间的标准来选择最佳的优化器。</p><p id="c3d8" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated"><strong class="lg ja"> <em class="ms"> 2.16翻译模型的评价指标</em> </strong></p><p id="69de" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">如上所述，我们还使用BLEU分数来评估模型，BLEU分数是双语评估替角，是用于将文本的候选翻译与一个或多个参考翻译进行比较的分数。它评估模型从一种语言翻译到另一种语言的效果。它根据生成的输出中存在的一元词、二元词或三元词为机器翻译分配一个分数，并将其与基本事实进行比较[15]。然而，它没有考虑语言的意义、句子结构和形态丰富的部分。此外，我们使用了metric METEOR(用于评估具有显式排序的翻译的度量),该度量基于unigram <a class="ae oe" href="https://en.wikipedia.org/wiki/Precision_and_recall" rel="noopener ugc nofollow" target="_blank">精度和召回</a>的<a class="ae oe" href="https://en.wikipedia.org/wiki/Harmonic_mean" rel="noopener ugc nofollow" target="_blank">调和平均值</a>,召回权重高于精度。[14]此外，在训练模型时，还计算argmax准确度以查看批量改进。注意，这对于图像到序列的转换模型来说不是一个合适的度量，实现它只是为了查看学习曲线。</p><h2 id="7469" class="nh ni iq bd nj nk nl dn nm nn no dp np ln nq nr ns lr nt nu nv lv nw nx ny iw bi translated"><strong class="ak"> 3。结果</strong></h2><p id="cccf" class="pw-post-body-paragraph le lf iq lg b lh nz ka lj lk oa kd lm ln ob lp lq lr oc lt lu lv od lx ly lz ij bi translated">我们已经为卷积编码器实现了ResNet152、AlexNet、VGG19-Net、DenseNet和SqueezeNet。然后，我们建立了一个可训练的嵌入层，嵌入大小为256(嵌入大小的值是一个超参数，它是在这样的嵌入大小实验后发现的)。之后，我们将字幕转换成嵌入大小为256的文字嵌入层。然后，嵌入的特征向量和单词嵌入通过教师强制器和注意机制被传递到语言模型中以生成字幕。提到的算法是我们在图像字幕生成方面的研究的最终发现。我们尝试了不同的批量大小，如32、64、128和256作为批量调整。我们的最终模型是用批量64训练的。当批量大小为256时，即使两个模型的性能都稍好一些，我们也会遇到与CUDA内存相关的内存问题，因此我们更倾向于将64作为理想的批量大小。即使应用了动态学习调度器，我们也从编码器和解码器模型的较高学习速率开始。对于编码器，选择4x 10–2作为初始学习速率，选择1x 10–2作为初始解码器学习速率。然后，再来说说车型性能。该模型在NVIDIA Tesla K80 GPU和GeForce RTX 2080 TI上训练，具有2-3个纪元。每个纪元需要1-1.30 GPU小时NVIDIA Tesla K80和45-60分钟GeForce RTX 2080 TI。因此，每个型号的总培训时间大约需要2-4小时，具体取决于性能。我们每个时期的训练时间非常快，因为我们已经实现了各种运行时高效算法。(参见加速深度编码器-解码器部分的框架)此外，对于所有模型，交叉熵损失从大约7开始，在训练结束时，我们已经达到了1.60–1.10(对于下面给出的模型)。我们还计算贪婪精度，即使精度不是我们的图像到序列转换模型查看学习曲线的正确度量。argmax的准确度从1%开始，在训练结束时达到大约70–80%。蓝色和流星分数是为所有型号计算的，如下所示。根据表I(见下文),到目前为止，整体视觉和语言模型中的赢家是编码器的ResNet152和解码器的GRU，其注意力机制通过BLEU评分和METEOR的标准来表示所生成字幕的语言性能，因此它们是图像到序列翻译模型的最佳评估指标。我们看到，在3个时期结束时，模型达到1.05熵损失，具有78.2 argmax准确度(即使准确度不是翻译模型的正确度量)。更重要的是，我们对BLUE-1和METEOR的评分分别达到了67.5和22.59，这对于翻译模型来说几乎是完美的。略有不同的是，2ndwinner是由ResNet152和LSTM组成的模型。因此，我们可以得出结论，ResNet152对于我们的特征提取任务表现良好。然而，当我们比较所有的视觉和语言模型时，我们发现我们使用MSCOCO数据集的实现没有很大的区别。因此，我们可以说所有的视觉和语言模型在测试案例中都表现得非常好。</p><p id="7a5c" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">然后，让我们从测试图像的标题开始。但在此之前，值得讨论的是语言的结构，以评估生成的字幕。一般来说，语言由几个部分组成，如语音学、音韵学、形态学、句法、语义学和语用学。因此，以语言结构为标准来探讨字幕生成的语言特征是值得的。</p><p id="8235" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">以下是从测试中随机选择的样本，并为其生成了标题:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi on"><img src="../Images/d6c9528dda294ebf12ce268b5f8735ed.png" data-original-src="https://miro.medium.com/v2/resize:fit:832/0*gQ2rJN9ClIFrYRvL"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">ResNet152-to-GRU为从测试数据中随机选择的图像引用和生成的标题(图片由作者提供)</p></figure><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi oo"><img src="../Images/f855561f68d3386f377ada72d992566d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QQ4f2stlQewKsViidp2WeQ.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">所有模型之间的交叉熵、Argmax精度、BLEU-1、2、3、4和流星度量比较(图片由作者提供)</p></figure><p id="66c6" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">生成的标题“一个骑马的女人”是该图像的完美标题，因此我们的模型实际上完全符合语言标准，因为它具有正确的语法、可理解的语义和正确的上下文含义。让我们继续检查例子。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi op"><img src="../Images/e9ad1ccfba081928b5671276727a1509.png" data-original-src="https://miro.medium.com/v2/resize:fit:850/0*vBaxO9o4DLEAu0ve"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">由从测试数据中随机选择的图像的ResNet152到GRU字幕引用和生成(图像由作者提供)</p></figure><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi oq"><img src="../Images/f190809153a1d7f4c87a901e6fb9432e.png" data-original-src="https://miro.medium.com/v2/resize:fit:984/0*8nhNulTSSUU3QL43"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">AlexNet-to-GRU从测试数据中随机选择的图片的参考和生成的说明(图片由作者提供)</p></figure><p id="b510" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">有趣的是，我们的模型捕捉到大象是一个婴儿，天气晴朗。我们很高兴看到一个完美的标题。此外，有趣的是，我们的模型在图中捕捉到了一支笔。因此，即使满足了语言标准，也很高兴看到模型能够捕捉到细节。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi or"><img src="../Images/baa45d3c08012639cd11fa5697eddb58.png" data-original-src="https://miro.medium.com/v2/resize:fit:1010/0*WLucb3Z2tehd4wEu"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">由从测试数据中随机选择的图像的ResNet152到LSTM字幕引用和生成(图像由作者提供)</p></figure><p id="626f" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">我们看到生成的字幕是人类级别的，即它们满足所有相应的语言结构组件。它有强大的语义和语法。让我们看看另一个由beam search创建的标题。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi os"><img src="../Images/d45fac3af492ee9b9bea6b782f758de1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DMD5A4nYxQT9I_EzYeFKmA.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">通过波束解码生成的字幕示例(图片由作者提供)</p></figure><p id="5d2a" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">我们看到由我们的视觉和语言模型生成的字幕通过额外的波束搜索算法非常强大。标题满足所有的语言标准，甚至在某些情况下，我们有比参考标题更有意义的标题。这里有更多的字幕生成的ResNet152到GRU模型与波束搜索和波束宽度是7。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi ot"><img src="../Images/4986c8d2f5959ad197a4615e4fe3b697.png" data-original-src="https://miro.medium.com/v2/resize:fit:782/0*oR2ZyvilBxTnAc9E"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">ResNet152-to-GRU为从测试数据中随机选择的图像提供参考并生成标题(图片由作者提供)</p></figure><h2 id="0123" class="nh ni iq bd nj nk nl dn nm nn no dp np ln nq nr ns lr nt nu nv lv nw nx ny iw bi translated"><strong class="ak"> 4。讨论和结论</strong></h2><p id="f0d3" class="pw-post-body-paragraph le lf iq lg b lh nz ka lj lk oa kd lm ln ob lp lq lr oc lt lu lv od lx ly lz ij bi translated">自动图像字幕远未成熟，有许多正在进行的研究项目旨在更准确的图像特征提取和语义更好的句子生成[19]。在我们的例子中，我们成功地生成了满足基本语言标准的标题，比如句法、语义、语用和形态意义。作为进一步的改进，有多种方法可以提高图像字幕环境中视觉和语言模型的性能。如上所述，出于计算考虑，我们没有对编码器模型和手套字向量的预训练参数进行微调，通过微调这些层可以获得额外的性能。此外，由于我们的CUDA内存有限，我们无法用更大的批量训练模型，这对我们的实现是一个缺点。甚至，我们使用多个GPU来训练模型，因为我们实现了7个不同的模型，它们是编码器和解码器的组合，由于计算限制，我们不能再次训练超过2-3个时期的模型。此外，可以通过BLEU分数来实现预定的学习速率，这使得学习速率对模型具有更准确的适应性。此外，你可以尝试用BLEU分数提前停止，当然是在训练时间较长的情况下。此外，波束搜索可以促进训练，这不是文献中常用的技术，但人们可以尝试看到差异。此外，可以使用更多RAM和GPU硬件组件实现更多超参数调整，可以尝试不同的学习速率、层数、神经元数量(隐藏和嵌入大小)、辍学率、批量标准化等。此外，更大的数据集可以用来达到更现实的字幕，人们可以尝试合并fli̇ckr和可可小姐数据集。在深度学习的背景下，可以尝试不同的视觉和语言模型架构作为进一步的调整。此外，还存在不止一种注意机制，如软注意(我们使用的)、硬注意、对数双线性注意、随机加倍注意等等。最后，我们在多个GPU的分布式并行计算的帮助下，使用不同的卷积编码器，然后使用不同的递归解码器，通过教师强制器和注意机制，成功实现了图像到序列的翻译模型。</p><p id="276d" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">文章到此结束。如果你有问题，不要犹豫，联系我，这里是如何。</p><h1 id="0bc9" class="ou ni iq bd nj ov ow ox nm oy oz pa np kf pb kg ns ki pc kj nv kl pd km ny pe bi translated"><strong class="ak">如何联系我</strong></h1><p id="7160" class="pw-post-body-paragraph le lf iq lg b lh nz ka lj lk oa kd lm ln ob lp lq lr oc lt lu lv od lx ly lz ij bi translated"><strong class="lg ja">电子邮件</strong>:cankocagil123@gmail.com</p><ul class=""><li id="3b3c" class="mt mu iq lg b lh li lk ll ln mv lr mw lv mx lz my mz na nb bi translated"><strong class="lg ja">推特</strong> : <a class="ae oe" href="https://twitter.com/canKocagil2" rel="noopener ugc nofollow" target="_blank">坎科卡吉尔2 </a></li><li id="2099" class="mt mu iq lg b lh nc lk nd ln ne lr nf lv ng lz my mz na nb bi translated"><strong class="lg ja">领英</strong></li></ul><div class="ma mb gp gr mc md"><a href="https://www.linkedin.com/in/can-kocagil-970506184/" rel="noopener  ugc nofollow" target="_blank"><div class="me ab fo"><div class="mf ab mg cl cj mh"><h2 class="bd ja gy z fp mi fr fs mj fu fw iz bi translated">可Kocagil -数据科学家- OREDATA | LinkedIn</h2><div class="mk l"><h3 class="bd b gy z fp mi fr fs mj fu fw dk translated">查看Can Kocagil在全球最大的职业社区LinkedIn上的个人资料。Can列出了3项工作…</h3></div><div class="ml l"><p class="bd b dl z fp mi fr fs mj fu fw dk translated">www.linkedin.com</p></div></div><div class="mm l"><div class="pf l mo mp mq mm mr ky md"/></div></div></a></div><ul class=""><li id="9f56" class="mt mu iq lg b lh li lk ll ln mv lr mw lv mx lz my mz na nb bi translated"><strong class="lg ja"> GitHub </strong></li></ul><div class="ma mb gp gr mc md"><a href="https://github.com/cankocagil" rel="noopener  ugc nofollow" target="_blank"><div class="me ab fo"><div class="mf ab mg cl cj mh"><h2 class="bd ja gy z fp mi fr fs mj fu fw iz bi translated">坎科卡吉尔-概述</h2><div class="mk l"><h3 class="bd b gy z fp mi fr fs mj fu fw dk translated">阻止或报告11月12月1月2月3月4月5月6月7月8月9月10月1日星期三Fri在1个存储库中创建了1个提交，您不能…</h3></div><div class="ml l"><p class="bd b dl z fp mi fr fs mj fu fw dk translated">github.com</p></div></div><div class="mm l"><div class="pg l mo mp mq mm mr ky md"/></div></div></a></div><h2 id="3c41" class="nh ni iq bd nj nk nl dn nm nn no dp np ln nq nr ns lr nt nu nv lv nw nx ny iw bi translated"><strong class="ak">项目页面</strong></h2><div class="ma mb gp gr mc md"><a href="https://github.com/cankocagil/Image-Captioning" rel="noopener  ugc nofollow" target="_blank"><div class="me ab fo"><div class="mf ab mg cl cj mh"><h2 class="bd ja gy z fp mi fr fs mj fu fw iz bi translated">GitHub-cankocagil/Image-Captioning:图像字幕的卷积语言模型</h2><div class="mk l"><h3 class="bd b gy z fp mi fr fs mj fu fw dk translated">这份报告主要是集中在图像字幕任务使用的先进技术的背景下深…</h3></div><div class="ml l"><p class="bd b dl z fp mi fr fs mj fu fw dk translated">github.com</p></div></div><div class="mm l"><div class="mn l mo mp mq mm mr ky md"/></div></div></a></div><p id="24c2" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated"><strong class="lg ja">参考文献</strong></p><ol class=""><li id="0a74" class="mt mu iq lg b lh li lk ll ln mv lr mw lv mx lz ph mz na nb bi translated">[1] X .荣，word2vec参数学习讲解。</li><li id="38e5" class="mt mu iq lg b lh nc lk nd ln ne lr nf lv ng lz ph mz na nb bi translated">[2]“迁移学习”，CS231n卷积神经网络用于视觉识别。【在线】。可用:【https://cs231n.github.io/transfer-learning/. T4】【访问时间:2021年1月8日】。</li><li id="b7e2" class="mt mu iq lg b lh nc lk nd ln ne lr nf lv ng lz ph mz na nb bi translated">[3]“微调Torchvision模型”，微调Torchvision模型— PyTorch教程1.2.0文档。【在线】。可用:<a class="ae oe" href="https://pytorch.org/tutorials/beginner/finetuning_torchvision_models_tutorial.html." rel="noopener ugc nofollow" target="_blank">https://py torch . org/tutorials/初学者/fine tuning _ torch vision _ models _ tutorial . html .</a>【访问时间:08-Jan-2021】。</li><li id="2ab8" class="mt mu iq lg b lh nc lk nd ln ne lr nf lv ng lz ph mz na nb bi translated">[4] Pulkit SharmaMy的研究兴趣在于机器学习和深度学习领域。拥有学习新技能和技术的热情。“迁移学习:Pytorch中的迁移学习”，分析Vidhya，2020年5月8日。【在线】。可用:<a class="ae oe" href="https://www.analyticsvidhya.com/blog/2019/10/how-to-master-transfer-learning-using-pytorch/?utm_source=blog&amp;utm_medium=building-image-" rel="noopener ugc nofollow" target="_blank">https://www . analyticsvidhya . com/blog/2019/10/how-to-master-transfer-learning-using-py torch/？UTM _ source = blog&amp;UTM _ medium = building-image-</a>分类-模型-cnn-pytorch。[访问日期:2021年1月8日]。</li><li id="9d73" class="mt mu iq lg b lh nc lk nd ln ne lr nf lv ng lz ph mz na nb bi translated">[5] P. Radhakrishnan，“深度学习中的图像字幕”，Medium，2017年10月10日。【在线】。可用:<a class="ae oe" rel="noopener" target="_blank" href="/image-captioning-in-">https://towardsdatascience.com/image-captioning-in-</a>深度学习-9cd23fb4d8d2。[访问日期:2021年1月8日]。</li><li id="f474" class="mt mu iq lg b lh nc lk nd ln ne lr nf lv ng lz ph mz na nb bi translated">[6]《基于注意力的模型教程(上)》，Karan Taneja，2018年6月2日。【在线】。可用:<a class="ae oe" href="https://krntneja.github.io/posts/2018/attention-based-models-" rel="noopener ugc nofollow" target="_blank">https://krntneja . github . io/posts/2018/Attention-based-models-</a>1 #:~:text = Attention % 2d based % 20 models % 20属于%20to，in % 20general % 2C % 20of %不同的% 20lengths。[访问日期:2021年1月8日]。</li><li id="ec31" class="mt mu iq lg b lh nc lk nd ln ne lr nf lv ng lz ph mz na nb bi translated">[7] S. Ulyanin，“用PyTorch为图像加标题”，中型，2019年2月23日。【在线】。可用:<a class="ae oe" href="https://medium.com/@stepanulyanin/captioning-images-with-" rel="noopener">https://medium.com/@stepanulyanin/captioning-images-with-</a>py torch-BC 592 e 5 FD 1 a 3。[访问日期:2021年1月8日]。</li><li id="4c20" class="mt mu iq lg b lh nc lk nd ln ne lr nf lv ng lz ph mz na nb bi translated">[8] K. Kshirsagar，“CNN和RNN的自动图像字幕”，媒体，2020年1月21日。【在线】。可用:<a class="ae oe" rel="noopener" target="_blank" href="/automatic-">https://towardsdatascience.com/automatic-</a>带cnn-rnn-aae3cd442d83的图像字幕。[访问日期:2021年1月8日]。</li><li id="9ad9" class="mt mu iq lg b lh nc lk nd ln ne lr nf lv ng lz ph mz na nb bi translated">[9]“用nn进行序列间建模。变形金刚和火炬报。【在线】。可用:<a class="ae oe" href="https://pytorch.org/tutorials/beginner/transformer_tutorial.html." rel="noopener ugc nofollow" target="_blank">https://py torch . org/tutorials/初学者/transformer _ tutorial . html</a>【访问时间:2021年1月8日】。</li><li id="0189" class="mt mu iq lg b lh nc lk nd ln ne lr nf lv ng lz ph mz na nb bi translated">[10]“nlpfromstack:translationwithesequencetosequencenetworkandattention”，nlpfromstack:translationwithesequencetosequencenetwork和Attention — PyTorch教程1.7.1文档。【在线】。可用:<a class="ae oe" href="https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html." rel="noopener ugc nofollow" target="_blank">https://py torch . org/tutorials/intermediate/seq 2 seq _ translation _ tutorial . html .</a>【访问时间:08-Jan-2021】。</li><li id="21a6" class="mt mu iq lg b lh nc lk nd ln ne lr nf lv ng lz ph mz na nb bi translated">[11]“序列模型和长短期记忆网络”，序列模型和长短期记忆网络PyTorch教程1.7.1文档。【在线】。可用:<a class="ae oe" href="https://pytorch.org/tutorials/beginner/nlp/sequence_models_tutorial.html." rel="noopener ugc nofollow" target="_blank">https://py torch . org/tutorials/初学者/NLP/sequence _ models _ tutorial . html .</a>【访问时间:2021年1月8日】。</li><li id="0347" class="mt mu iq lg b lh nc lk nd ln ne lr nf lv ng lz ph mz na nb bi translated">[12] KelvinXu，JimmyLeiBa，RyanKiros，KyunghyunCho，RuslanSalakhutdinov，RichardS。Zemel，andYoshuaBengio，Show，AttendandTell:带视觉注意力的神经图像标题生成，2016年4月。</li><li id="29d6" class="mt mu iq lg b lh nc lk nd ln ne lr nf lv ng lz ph mz na nb bi translated">[13]“单词嵌入:编码词汇语义”，单词嵌入:编码词汇语义— PyTorch教程1.7.1文档。【在线】。可用:<a class="ae oe" href="https://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html." rel="noopener ugc nofollow" target="_blank">https://py torch . org/tutorials/初学者/NLP/word _ embeddings _ tutorial . html .</a>【访问时间:2021年1月8日】。</li><li id="4af5" class="mt mu iq lg b lh nc lk nd ln ne lr nf lv ng lz ph mz na nb bi translated">[14] Sgrvinod，“Sgrvinod/a-py torch-Tutorial-to-Image-Captioning”，GitHub。【在线】。可用:<a class="ae oe" href="https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Image-" rel="noopener ugc nofollow" target="_blank">https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Image-</a>字幕。[访问日期:2021年1月8日]。</li><li id="30a8" class="mt mu iq lg b lh nc lk nd ln ne lr nf lv ng lz ph mz na nb bi translated">[15] J. Brownlee，“如何实现用于自然语言处理的波束搜索解码器”，机器学习掌握，2020年6月3日。【在线】。可用:<a class="ae oe" href="https://machinelearningmastery.com/beam-search-decoder-natural-language-processing/." rel="noopener ugc nofollow" target="_blank">https://machine learning mastery . com/beam-search-decoder-natural-language-processing/。</a>【访问时间:2021年1月8日】。</li><li id="0ad1" class="mt mu iq lg b lh nc lk nd ln ne lr nf lv ng lz ph mz na nb bi translated">[16] R. Khandelwal，“波束搜索的直观解释”，媒体，2020年2月3日。【在线】。可用:<a class="ae oe" rel="noopener" target="_blank" href="/an-intuitive-">https://towardsdatascience.com/an-intuitive-</a>解释-波束-搜索-9b1d744e7a0f。[访问日期:2021年1月8日]。</li><li id="eea5" class="mt mu iq lg b lh nc lk nd ln ne lr nf lv ng lz ph mz na nb bi translated">[17] S. Sarkar，“使用注意机制的图像字幕”，Medium，2020年3月7日。【在线】。可用:【https://medium.com/swlh/image-captioning-using-】T2注意力-机制-f3d7fc96eb0e。[访问日期:2021年1月8日]。</li><li id="f642" class="mt mu iq lg b lh nc lk nd ln ne lr nf lv ng lz ph mz na nb bi translated">[18] Aayush Bajaj，Avantari等人，机器学习工程师，“理解梯度裁剪”，2020年</li></ol></div></div>    
</body>
</html>