<html>
<head>
<title>Hyperparameter Tuning — Always Tune your Models</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">超参数调整—始终调整您的模型</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/hyperparameter-tuning-always-tune-your-models-7db7aeaf47e9?source=collection_archive---------16-----------------------#2021-07-07">https://towardsdatascience.com/hyperparameter-tuning-always-tune-your-models-7db7aeaf47e9?source=collection_archive---------16-----------------------#2021-07-07</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="b27d" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsdatascience.com/tagged/hands-on-tutorials" rel="noopener" target="_blank">实践教程</a></h2><div class=""/><div class=""><h2 id="8339" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">不要放弃免费的性能提升。</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/73b966ed3df167204a71646aa39773b5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*t4B2Gq3-x-7pQf3D"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated"><a class="ae lh" href="https://unsplash.com/@sigmund?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">西格蒙德</a>在<a class="ae lh" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍照</p></figure><h2 id="5a59" class="li lj it bd lk ll lm dn ln lo lp dp lq lr ls lt lu lv lw lx ly lz ma mb mc iz bi translated">TLDR概述</h2><ul class=""><li id="e92e" class="md me it mf b mg mh mi mj lr mk lv ml lz mm mn mo mp mq mr bi translated">网格搜索—穷举且计算量大，在超参数搜索空间受限时使用。</li><li id="6f15" class="md me it mf b mg ms mi mt lr mu lv mv lz mw mn mo mp mq mr bi translated">随机搜索—与网格搜索相比，更大的搜索空间改进了模型，网格搜索是scikit-learn内置的定位方法。</li><li id="5941" class="md me it mf b mg ms mi mt lr mu lv mv lz mw mn mo mp mq mr bi translated">贝叶斯优化—改进了随机搜索和超参数解释的性能，尽可能使用。</li></ul></div><div class="ab cl mx my hx mz" role="separator"><span class="na bw bk nb nc nd"/><span class="na bw bk nb nc nd"/><span class="na bw bk nb nc"/></div><div class="im in io ip iq"><h1 id="4e84" class="ne lj it bd lk nf ng nh ln ni nj nk lq ki nl kj lu kl nm km ly ko nn kp mc no bi translated">什么是超参数调谐？</h1><p id="9009" class="pw-post-body-paragraph np nq it mf b mg mh kd nr mi mj kg ns lr nt nu nv lv nw nx ny lz nz oa ob mn im bi translated">超参数调整是提高模型性能的一个重要方面。给定一个具有许多超参数的复杂模型，有效的超参数调整可以显著提高性能。例如，梯度增强分类器有许多不同的参数要微调，每个参数都会唯一地改变模型的性能。有些可能影响很小或没有影响，而另一些可能对模型的可行性至关重要。</p><p id="c7a5" class="pw-post-body-paragraph np nq it mf b mg oc kd nr mi od kg ns lr oe nu nv lv of nx ny lz og oa ob mn im bi translated">最常见的方法是尝试几种用户定义的超参数组合，比较结果，并选择总体最佳表现者。这种方法有一些明显的缺点，因为基于初始选项的选择是有偏差的。虽然关于目标问题的先验知识可以为理想的选择提供一些指导，但是还有潜在的更好的选择没有被检查。网格搜索遵循这一基本方法，但即使有了先验知识，这也是不理想的。</p><h2 id="9d15" class="li lj it bd lk ll lm dn ln lo lp dp lq lr ls lt lu lv lw lx ly lz ma mb mc iz bi translated">履行</h2><p id="426c" class="pw-post-body-paragraph np nq it mf b mg mh kd nr mi mj kg ns lr nt nu nv lv nw nx ny lz nz oa ob mn im bi translated">这篇文章的完整笔记可以在<a class="ae lh" href="https://github.com/zjwarnes/Data-Science-Notebooks" rel="noopener ugc nofollow" target="_blank">这里</a>找到。</p><p id="daec" class="pw-post-body-paragraph np nq it mf b mg oc kd nr mi od kg ns lr oe nu nv lv of nx ny lz og oa ob mn im bi translated">这些超参数搜索中的每一个都非常容易实现，所以您应该在模型优化过程中加入一个。对于我的例子，我使用乳腺癌数据集。首先，我加载数据、相关的库，初始化一些元参数，并拆分数据集。接下来，我创建一个小管道，运行每个搜索并打印出性能。</p><pre class="ks kt ku kv gt oh oi oj ok aw ol bi"><span id="8ec8" class="li lj it oi b gy om on l oo op"><strong class="oi jd">import</strong> <strong class="oi jd">pandas</strong> <strong class="oi jd">as</strong> <strong class="oi jd">pd</strong><br/><strong class="oi jd">from</strong> <strong class="oi jd">scipy.stats</strong> <strong class="oi jd">import</strong> uniform, geom, loguniform, randint, expon<br/><strong class="oi jd">from</strong> <strong class="oi jd">sklearn.ensemble</strong> <strong class="oi jd">import</strong> GradientBoostingClassifier<br/><strong class="oi jd">from</strong> <strong class="oi jd">sklearn.model_selection</strong> <strong class="oi jd">import</strong> cross_val_score, GridSearchCV, train_test_split, RandomizedSearchCV<br/><strong class="oi jd">from</strong> <strong class="oi jd">sklearn.metrics</strong> <strong class="oi jd">import</strong> roc_auc_score<br/><strong class="oi jd">from</strong> <strong class="oi jd">sklearn.pipeline</strong> <strong class="oi jd">import</strong> make_pipeline, Pipeline<br/><strong class="oi jd">from</strong> <strong class="oi jd">sklearn.datasets</strong> <strong class="oi jd">import</strong> load_iris, load_boston, load_diabetes, load_digits, load_linnerud, load_wine, load_breast_cancer</span><span id="5a79" class="li lj it oi b gy oq on l oo op">TEST_SIZE = 0.1 <br/>RANDOM_STATE = 10  <br/>data = load_breast_cancer() <br/>df = pd.DataFrame(data.data, columns=data.feature_names) df['target'] = data.target  </span><span id="1427" class="li lj it oi b gy oq on l oo op">X = df.drop(['target'], axis=1) <br/>y = df['target'].astype(float) <br/>X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=TEST_SIZE, random_state=RANDOM_STATE, stratify=y)</span></pre></div><div class="ab cl mx my hx mz" role="separator"><span class="na bw bk nb nc nd"/><span class="na bw bk nb nc nd"/><span class="na bw bk nb nc"/></div><div class="im in io ip iq"><h1 id="674f" class="ne lj it bd lk nf ng nh ln ni nj nk lq ki nl kj lu kl nm km ly ko nn kp mc no bi translated"><strong class="ak">网格搜索与随机搜索</strong></h1><h2 id="a0a2" class="li lj it bd lk ll lm dn ln lo lp dp lq lr ls lt lu lv lw lx ly lz ma mb mc iz bi translated">网格搜索</h2><p id="6a8e" class="pw-post-body-paragraph np nq it mf b mg mh kd nr mi mj kg ns lr nt nu nv lv nw nx ny lz nz oa ob mn im bi translated">网格搜索是最常见的超参数选择技术之一。这种方法实际上是一种强力策略，只需为每个超参数配置创建和测试一个模型，这种方法受益于穷举搜索行为。此外，可以肯定的是，每个组合都经过了比较。</p><p id="62fb" class="pw-post-body-paragraph np nq it mf b mg oc kd nr mi od kg ns lr oe nu nv lv of nx ny lz og oa ob mn im bi translated">然而，应该清楚的是，使用全面搜索超参数是昂贵的。由于这些搜索处理组合的复杂性，必须测试的模型数量急剧增加，特别是当目标模型类型有许多参数要调整时。由于这种计算负担，当一些超参数的最佳选择已知时，网格搜索是最有效的。</p><h2 id="220b" class="li lj it bd lk ll lm dn ln lo lp dp lq lr ls lt lu lv lw lx ly lz ma mb mc iz bi translated">随机搜索</h2><p id="6b35" class="pw-post-body-paragraph np nq it mf b mg mh kd nr mi mj kg ns lr nt nu nv lv nw nx ny lz nz oa ob mn im bi translated">随机搜索是网格搜索的常用替代方法。与对每个超参数配置的穷举搜索相反，随机搜索只对设定数量的样本运行。每个样本从可能的参数列表或参数分布中随机选择超参数。虽然与网格搜索相比，这种方法似乎不是最佳的，但在相同的计算量下，它表现得更好。</p><p id="d9a6" class="pw-post-body-paragraph np nq it mf b mg oc kd nr mi od kg ns lr oe nu nv lv of nx ny lz og oa ob mn im bi translated">当存在许多可忽略的超参数时，该方法的好处是显而易见的。例如，在梯度增强模型中，用于具有少量特征的模型的特征选择可以是总特征的平方根或特征数量的对数。当特征的数量很小时，两个选择之间的差异是有限的。将参数的两种变体添加到搜索中会使网格搜索中的搜索空间加倍。相比之下，随机搜索将只包括所有样本中成比例的这些参数选择。包含可忽略参数变量的样本将揭示该参数的影响，并大大节省计算时间。</p><p id="cdac" class="pw-post-body-paragraph np nq it mf b mg oc kd nr mi od kg ns lr oe nu nv lv of nx ny lz og oa ob mn im bi translated">因此，随机搜索可以在更大的超参数空间上进行优化，其计算量与网格搜索相同。因此，与通过网格搜索优化的模型相比，随机搜索进一步提高了最优模型的性能。</p><p id="af9e" class="pw-post-body-paragraph np nq it mf b mg oc kd nr mi od kg ns lr oe nu nv lv of nx ny lz og oa ob mn im bi translated">此外，随机搜索比网格搜索还有一个好处。因为网格搜索使用超参数的离散选择来创建模型，所以错过不可忽略的超参数的最佳选择的概率增加了。相反，随机搜索可以从可能的超参数的连续或完全离散分布中选择配置。这种行为已被证明在识别最佳模型设置时更有效，尤其是当超参数的搜索空间很大时。</p><h2 id="083f" class="li lj it bd lk ll lm dn ln lo lp dp lq lr ls lt lu lv lw lx ly lz ma mb mc iz bi translated">比较方法</h2><p id="d01b" class="pw-post-body-paragraph np nq it mf b mg mh kd nr mi mj kg ns lr nt nu nv lv nw nx ny lz nz oa ob mn im bi translated">各种方法之间的比较如下图所示。假设两个超参数需要调整，每个参数在每个配置下的模型真实性能以蓝色显示。应该清楚的是，网格搜索产生的模型可能会“错过”模型的最佳超参数配置。相反，随机搜索有效地忽略了第一个可忽略的超参数，并从第二个超参数中采样更多不同的值。因此，随机搜索可以找到更优的超参数选择。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi or"><img src="../Images/1c7a36250759de5e810d48851f22e72f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LIuj_iKd2c6ZJRRrnwjKfg.jpeg"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">搜索方法之间的差异(作者提供的照片)</p></figure><h2 id="99c5" class="li lj it bd lk ll lm dn ln lo lp dp lq lr ls lt lu lv lw lx ly lz ma mb mc iz bi translated">实施搜索</h2><p id="3138" class="pw-post-body-paragraph np nq it mf b mg mh kd nr mi mj kg ns lr nt nu nv lv nw nx ny lz nz oa ob mn im bi translated">网格搜索和随机搜索是标准scikit-learn包的一部分，可与scikit-learn生成的现有模型无缝协作。下面是使用网格搜索和随机搜索创建管道的大致设置。这个例子的模型是GradientBoostingClassifier，但是这个模型是可以互换的。</p><p id="cc70" class="pw-post-body-paragraph np nq it mf b mg oc kd nr mi od kg ns lr oe nu nv lv of nx ny lz og oa ob mn im bi translated">要切换到另一个模型，请更新管道定义中的模型，并更新<em class="os"> hyper_parameter </em>字典中的参数名称和选择/分布。参数名称应带有前缀“<em class="os"> model__ </em>，以匹配管道中的名称“<em class="os"> model </em>”。这些名称可以根据需要进行更改。</p><h2 id="2cec" class="li lj it bd lk ll lm dn ln lo lp dp lq lr ls lt lu lv lw lx ly lz ma mb mc iz bi translated">网格搜索</h2><pre class="ks kt ku kv gt oh oi oj ok aw ol bi"><span id="0e5b" class="li lj it oi b gy om on l oo op">N_JOBS = 1<br/>K_FOLDS = 10<br/>N_ITER_NO_CHANGE = 100<br/>SCORING_METRIC = 'roc_auc'<br/><br/><em class="os"># Gradient Boosting Parameters</em><br/>hyper_parameters = {<br/>        'model__n_estimators': [50, 150, 300],<br/>        'model__learning_rate': [0.1, 0.03, 0.001],<br/>        'model__max_depth': [3, 5, 8],<br/>        'model__max_features': ['sqrt'], <br/>        'model__min_samples_split': [2],<br/>        'model__n_iter_no_change': [N_ITER_NO_CHANGE], <em class="os"># Note single elements should still be in a list</em><br/>        'model__random_state': [RANDOM_STATE],<br/>        'model__min_samples_leaf': [2, 10]<br/>}<br/><br/><em class="os"># Make pipeline - Add steps if needed, i.e. feature selection</em><br/>pipe = Pipeline(steps=[<br/>    ('model', GradientBoostingClassifier())<br/>])<br/><br/><em class="os"># 'n_iter' no longer needed as all combinations are checked</em><br/>search_space = GridSearchCV(<br/>    pipe, hyper_parameters, <br/>    cv=K_FOLDS, scoring=SCORING_METRIC, n_jobs = N_JOBS, <br/>    return_train_score=<strong class="oi jd">True</strong>, verbose = 1<br/>)<br/>search_space.fit(X_train, y_train) <br/><br/>y_pred = search_space.best_estimator_.predict(X_train)<br/>y_pred_prob = search_space.best_estimator_.predict_proba(X_train)[:,1]<br/>y_pred_test = search_space.best_estimator_.predict_proba(X_test)[:,1]<br/><br/>print( <br/>    'Best Training Score: ', search_space.cv_results_['mean_train_score'][search_space.best_index_], <br/>    '<strong class="oi jd">\n</strong>Best Test Score: ', search_space.best_score_,<br/>    '<strong class="oi jd">\n</strong>Hold Out Test Score: ', roc_auc_score(y_test, y_pred_test)<br/>)</span></pre><p id="3fb8" class="pw-post-body-paragraph np nq it mf b mg oc kd nr mi od kg ns lr oe nu nv lv of nx ny lz og oa ob mn im bi translated">然后是随机搜索的代码，请注意从参数列表选项到统计库中定义的分布的变化。</p><h2 id="5613" class="li lj it bd lk ll lm dn ln lo lp dp lq lr ls lt lu lv lw lx ly lz ma mb mc iz bi translated">随机搜索</h2><pre class="ks kt ku kv gt oh oi oj ok aw ol bi"><span id="c479" class="li lj it oi b gy om on l oo op">N_ITER = 54<br/>N_JOBS = 1<br/>K_FOLDS = 10<br/>N_ITER_NO_CHANGE = 100<br/>SCORING_METRIC = 'roc_auc'<br/><br/>hyper_parameters = {<br/>        'model__n_estimators': randint(50,500),<br/>        'model__learning_rate': loguniform(3e-4, 3e-1),<br/>        'model__max_depth': randint(3,10),<br/>        'model__max_features': ['sqrt'], <br/>        'model__min_samples_split': randint(2,20),<br/>        'model__n_iter_no_change': [N_ITER_NO_CHANGE], <em class="os"># Note single elements should still be in a list</em><br/>        'model__random_state': [RANDOM_STATE],<br/>        'model__min_samples_leaf': randint(1,10)<br/>}<br/><br/>pipe = Pipeline(steps=[<br/>    ('model', GradientBoostingClassifier())<br/>])<br/><br/>search_space = RandomizedSearchCV(<br/>    pipe, hyper_parameters, <br/>    n_iter = N_ITER, cv=K_FOLDS, <br/>    scoring=SCORING_METRIC, n_jobs = N_JOBS, <br/>    return_train_score=<strong class="oi jd">True</strong>, verbose = 1<br/>)<br/>search_space.fit(X_train, y_train) <br/><br/>y_pred = search_space.best_estimator_.predict(X_train)<br/>y_pred_prob = search_space.best_estimator_.predict_proba(X_train)[:,1]<br/>y_pred_test = search_space.best_estimator_.predict_proba(X_test)[:,1]<br/><br/>print( <br/>    'Best Training Score: ', search_space.cv_results_['mean_train_score'][search_space.best_index_], <br/>    '<strong class="oi jd">\n</strong>Best Test Score: ', search_space.best_score_,<br/>    '<strong class="oi jd">\n</strong>Hold Out Test Score: ', roc_auc_score(y_test, y_pred_test)<br/>)</span></pre></div><div class="ab cl mx my hx mz" role="separator"><span class="na bw bk nb nc nd"/><span class="na bw bk nb nc nd"/><span class="na bw bk nb nc"/></div><div class="im in io ip iq"><h1 id="4d47" class="ne lj it bd lk nf ng nh ln ni nj nk lq ki nl kj lu kl nm km ly ko nn kp mc no bi translated"><strong class="ak">贝叶斯搜索</strong></h1><p id="c9fa" class="pw-post-body-paragraph np nq it mf b mg mh kd nr mi mj kg ns lr nt nu nv lv nw nx ny lz nz oa ob mn im bi translated">随机搜索被证明比网格搜索更有效。然而，另一种方法证明可以找到更优的超参数选择。贝叶斯搜索是一种超参数选择技术，它对超参数搜索空间进行建模和优化。该建模使用高斯过程来完成，该高斯过程对从超参数空间产生的函数进行建模。</p><p id="15b3" class="pw-post-body-paragraph np nq it mf b mg oc kd nr mi od kg ns lr oe nu nv lv of nx ny lz og oa ob mn im bi translated">搜索过程如下。样本取自超参数空间，创建多个模型，测量模型的性能，并使用argmax函数进行优化，以确定最佳超参数配置。这个过程类似于随机搜索，增加的好处是模式被外推以找到更优的模型。此外，贝叶斯超参数搜索对每个超参数进行相互比较建模，这允许对模型的有效性进行更稳健的解释。</p><h2 id="7c6f" class="li lj it bd lk ll lm dn ln lo lp dp lq lr ls lt lu lv lw lx ly lz ma mb mc iz bi translated">贝叶斯搜索与其他搜索</h2><p id="1fcb" class="pw-post-body-paragraph np nq it mf b mg mh kd nr mi mj kg ns lr nt nu nv lv nw nx ny lz nz oa ob mn im bi translated">将贝叶斯超参数与网格搜索和随机搜索进行比较，您可以看到，贝叶斯搜索不仅随机采样类似于随机搜索，而且在许多样本之后，生成一个函数来对超参数空间进行建模，然后对其进行优化。这一过程会在图像中产生虚线，最佳点由小红星表示。请注意，这仍然不是函数的真正最佳超参数选择，但却提高了模型性能。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ot"><img src="../Images/bbfc206e1483bed5951ec082b2995769.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sPTZiXuWHBct_kYiD_5aYA.jpeg"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">搜索方法之间的差异(作者提供的照片)</p></figure><h2 id="5ac5" class="li lj it bd lk ll lm dn ln lo lp dp lq lr ls lt lu lv lw lx ly lz ma mb mc iz bi translated">实现贝叶斯搜索</h2><p id="6336" class="pw-post-body-paragraph np nq it mf b mg mh kd nr mi mj kg ns lr nt nu nv lv nw nx ny lz nz oa ob mn im bi translated">不幸的是，贝叶斯优化的代码不是标准scikit-learn包的一部分。然而，软件包scikit-optimize提供了一个与scikit-learn模型集成的贝叶斯超参数优化功能。该方法的代码遵循与网格和随机搜索相似的模式，但是参数分布的定义不同。</p><pre class="ks kt ku kv gt oh oi oj ok aw ol bi"><span id="a67d" class="li lj it oi b gy om on l oo op"><strong class="oi jd">from</strong> <strong class="oi jd">skopt</strong> <strong class="oi jd">import</strong> BayesSearchCV<br/><strong class="oi jd">from</strong> <strong class="oi jd">skopt.space</strong> <strong class="oi jd">import</strong> Real, Categorical, Integer<br/><br/>N_ITER = 54<br/>N_JOBS = 1<br/>hyper_parameters = {<br/>        'n_estimators': Integer(50, 300),<br/>        'max_depth': Integer(3, 10),<br/>        'min_samples_split': Integer(2, 300),<br/>        'learning_rate': Real(3e-6, 3e-1,prior='log-uniform'),     <br/>        'max_features': Categorical(['sqrt', <strong class="oi jd">None</strong>]),<br/>}<br/>search_space = BayesSearchCV(<br/>    estimator=GradientBoostingClassifier(<br/>        n_iter_no_change=N_ITER_NO_CHANGE, <br/>        random_state=RANDOM_STATE<br/>    ),<br/>    search_spaces=hyper_parameters,<br/>    scoring=SCORING_METRIC,<br/>    cv=K_FOLDS,<br/>    n_iter=N_ITER,<br/>    random_state=RANDOM_STATE,<br/>    return_train_score=<strong class="oi jd">True</strong>,<br/>    verbose = 1,<br/>    n_jobs=N_JOBS<br/>)<br/><em class="os"># executes bayesian optimization</em><br/>_ = search_space.fit(X_train, y_train)<br/><br/>y_pred = search_space.best_estimator_.predict(X_train)<br/>y_pred_prob = search_space.best_estimator_.predict_proba(X_train)[:,1]<br/>y_pred_test = search_space.best_estimator_.predict_proba(X_test)[:,1]<br/><br/><em class="os"># model can be saved, used for predictions or scoring</em><br/>print( <br/>    'Best Training Score: ', search_space.cv_results_['mean_train_score'][search_space.best_index_], <br/>    '<strong class="oi jd">\n</strong>Best Test Score: ', search_space.best_score_,<br/>    '<strong class="oi jd">\n</strong>Hold Out Test Score: ', roc_auc_score(y_test, y_pred_test)<br/>)</span></pre><h2 id="ed90" class="li lj it bd lk ll lm dn ln lo lp dp lq lr ls lt lu lv lw lx ly lz ma mb mc iz bi translated">理解贝叶斯搜索</h2><p id="dd2a" class="pw-post-body-paragraph np nq it mf b mg mh kd nr mi mj kg ns lr nt nu nv lv nw nx ny lz nz oa ob mn im bi translated">此外，scikit-optimization提供了可视化超参数优化函数的方法。这些图可以进一步指导优化过程。此外，可以查看紧急模式，例如识别可忽略的参数和有限的搜索空间。这些图用于进一步细化搜索空间，以创建一个更加优化的模型。</p><pre class="ks kt ku kv gt oh oi oj ok aw ol bi"><span id="47ee" class="li lj it oi b gy om on l oo op"><strong class="oi jd">import</strong> <strong class="oi jd">matplotlib.pyplot</strong> <strong class="oi jd">as</strong> <strong class="oi jd">plt</strong><br/><strong class="oi jd">from</strong> <strong class="oi jd">skopt.plots</strong> <strong class="oi jd">import</strong> plot_objective, plot_histogram<br/><br/>_ = plot_objective(search_space.optimizer_results_[0],<br/>                   n_minimum_search=int(1e8))<br/>plt.show()</span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ou"><img src="../Images/12d5627bbb577eb6e8e44f26366653b1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*P0GyTAgdsN-DAkbGrgc69w.jpeg"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">每个超参数的贝叶斯优化PDP图(作者提供图片)</p></figure></div><div class="ab cl mx my hx mz" role="separator"><span class="na bw bk nb nc nd"/><span class="na bw bk nb nc nd"/><span class="na bw bk nb nc"/></div><div class="im in io ip iq"><h1 id="b08c" class="ne lj it bd lk nf ng nh ln ni nj nk lq ki nl kj lu kl nm km ly ko nn kp mc no bi translated">结论</h1><p id="655d" class="pw-post-body-paragraph np nq it mf b mg mh kd nr mi mj kg ns lr nt nu nv lv nw nx ny lz nz oa ob mn im bi translated">如果您还没有使用超参数优化，并且您的模型有超参数，那么您应该使用超参数优化。然后，根据您的可用资源和包依赖项，您可以使用所描述的三种方法中的任何一种。</p><p id="521e" class="pw-post-body-paragraph np nq it mf b mg oc kd nr mi od kg ns lr oe nu nv lv of nx ny lz og oa ob mn im bi translated">例如，贝叶斯优化非常适合快速找到好的超参数设置，而网格搜索可以在定义小搜索空间时进一步微调模型。一般来说，随机搜索是这两种方法之间的折中，您应该(至少)将这种超参数搜索整合到您现有的模型和管道中，以获得立竿见影的性能提升。</p></div><div class="ab cl mx my hx mz" role="separator"><span class="na bw bk nb nc nd"/><span class="na bw bk nb nc nd"/><span class="na bw bk nb nc"/></div><div class="im in io ip iq"><p id="6ee8" class="pw-post-body-paragraph np nq it mf b mg oc kd nr mi od kg ns lr oe nu nv lv of nx ny lz og oa ob mn im bi translated">如果你有兴趣阅读关于新颖的数据科学工具和理解机器学习算法的文章，可以考虑在Medium上关注我。</p><p id="4f22" class="pw-post-body-paragraph np nq it mf b mg oc kd nr mi od kg ns lr oe nu nv lv of nx ny lz og oa ob mn im bi translated"><em class="os">如果你对我的写作感兴趣，想直接支持我，请通过以下链接订阅。这个链接确保我会收到你的会员费的一部分。</em></p><div class="ov ow gp gr ox oy"><a href="https://zjwarnes.medium.com/membership" rel="noopener follow" target="_blank"><div class="oz ab fo"><div class="pa ab pb cl cj pc"><h2 class="bd jd gy z fp pd fr fs pe fu fw jc bi translated">通过我的推荐链接加入Medium-Zachary Warnes</h2><div class="pf l"><h3 class="bd b gy z fp pd fr fs pe fu fw dk translated">阅读扎卡里·沃恩斯(以及媒体上成千上万的其他作家)的每一个故事。您的会员费直接支持…</h3></div><div class="pg l"><p class="bd b dl z fp pd fr fs pe fu fw dk translated">zjwarnes.medium.com</p></div></div><div class="ph l"><div class="pi l pj pk pl ph pm lb oy"/></div></div></a></div></div></div>    
</body>
</html>