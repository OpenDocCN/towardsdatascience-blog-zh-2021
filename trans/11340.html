<html>
<head>
<title>Neural Network Optimizers from Scratch in Python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Python中从头开始的神经网络优化器</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/neural-network-optimizers-from-scratch-in-python-af76ee087aab?source=collection_archive---------7-----------------------#2021-11-07">https://towardsdatascience.com/neural-network-optimizers-from-scratch-in-python-af76ee087aab?source=collection_archive---------7-----------------------#2021-11-07</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="bf91" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">从数学和实践角度看非凸优化:Python中的SGD、SGDMomentum、AdaGrad、RMSprop和Adam</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/3d794ee5728935455220a406145a4ce2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jRRB_bNmfI5DZm5ZxHwnNw.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">照片由<a class="ae kv" href="https://www.pexels.com/photo/person-writing-on-notebook-669615/?utm_content=attributionCopyText&amp;utm_medium=referral&amp;utm_source=pexels" rel="noopener ugc nofollow" target="_blank"> Pexels </a>的<a class="ae kv" href="https://www.pexels.com/@goumbik?utm_content=attributionCopyText&amp;utm_medium=referral&amp;utm_source=pexels" rel="noopener ugc nofollow" target="_blank"> Lukas </a>拍摄</p></figure><p id="aad3" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">本文将从头开始提供常见非凸优化器及其Python实现的简短数学表达式。理解这些优化算法背后的数学将在训练复杂的机器学习模型时启发你的观点。这篇文章的结构如下。首先，我将谈论特定的优化算法；然后，我会给出数学公式，并提供Python代码。所有算法都是纯NumPy实现的。下面是我们将要讨论的非凸优化算法</p><ul class=""><li id="bb4e" class="ls lt iq ky b kz la lc ld lf lu lj lv ln lw lr lx ly lz ma bi translated">随机梯度下降</li><li id="4fc8" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">SGDMomentum</li><li id="66a1" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">阿达格拉德</li><li id="0cc8" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">RMSprop</li><li id="f959" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">圣经》和《古兰经》传统中）亚当（人类第一人的名字</li></ul><p id="8602" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">先说最简单的，随机梯度下降。</p><p id="805e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">随机梯度下降</strong></p><p id="25b5" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">SGD是在可微误差曲面上的迭代、非凸和一阶优化算法。它是梯度下降的随机估计，其中训练数据是随机化的。这是一种计算稳定、数学上成熟的优化算法。SGD背后的直觉是，我们对我们可以优化的参数取目标函数的偏导数，这产生了它的梯度，它显示了误差损失的增加方向。因此，我们取该梯度的负值，在损失不增加的地方向前移动。为了确保稳定和较少振荡的优化，我们引入学习率参数ŋ，然后用ŋ.乘以梯度最后，从我们可以以迭代方式优化的参数中减去获得的值。下面是SGD更新公式和Python代码。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mg"><img src="../Images/f07dbfbb66fc4047c49135efee322f22.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/format:webp/1*ZCBtXhhoUOmYsxrmMMK3kA.png"/></div></figure><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="mh mi l"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">SGD Python实现</p></figure><p id="cb1f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir"> SGDMomentum </strong></p><p id="86fb" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在SGD的上下文中，我们不是计算损失函数的精确导数，而是以迭代的方式在小批量上逼近它。因此，不能确定模型在损失最小化的方向上学习。为了提出更稳定、方向感知和快速的学习，我们引入了SGDMomentum，它将下一次更新确定为梯度和前一次更新的线性组合。因此，它也考虑了以前的更新。</p><p id="a6de" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">总的来说，动量随机梯度下降比经典下降有两个优点:</p><ul class=""><li id="0f4a" class="ls lt iq ky b kz la lc ld lf lu lj lv ln lw lr lx ly lz ma bi translated">快速收敛</li><li id="8cb0" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">少振荡训练</li></ul><p id="969f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">下面是SGDMomentum的公式和Python代码。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mj"><img src="../Images/3059f7367f97304a0922bbf5b5227975.png" data-original-src="https://miro.medium.com/v2/resize:fit:1284/format:webp/1*u46DFJvc9mjpNZljjz4z5Q.png"/></div></figure><p id="c9e8" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">其中，α是动量系数，取值范围为[0，1]。α是一个指数衰减因子，它决定了当前梯度和早期梯度对权重变化的相对贡献[1]。在α = 0的情况下，公式正好是纯SGD。在α = 1的情况下，优化过程考虑了先前更新的全部历史。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="mh mi l"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">SGDMomentum Python实现</p></figure><p id="6e5e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">从那时起，我们将在深度学习的背景下进入更复杂的优化器。有一些广泛使用的优化算法，如AdaGrad、RMSprop和Adam。它们都是自适应优化算法，即它们通过重新安排学习速率来适应学习过程，以便模型能够更有效和更快地达到期望的全局最小值。下面是公式和实现。</p><p id="bf52" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">阿达格勒</p><p id="4d0d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">AdaGrad优化算法跟踪平方梯度的总和，平方梯度衰减传播到其更新历史的参数的学习速率。其数学表达式为:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mk"><img src="../Images/109fd4ae21f4fe655a5e350fc6241fba.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JeG6K_8eBFotHfGQ2TNT3w.png"/></div></div></figure><p id="bc2a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">其中，𝛿𝑖是梯度平方的累积和，∇𝜃𝑖是𝜃𝑖的梯度，是网络的可配置参数，𝜀是防止零除的标量。当累积和逐渐增加时，更新项的分母同时增加，导致整个更新项ŋ * ∇𝜃𝑖.的减少随着更新项√𝛿𝑖+𝜀的减小，AdaGrad优化变得更慢。因此，当接近收敛时，AdaGrad被卡住，因为累积和逐渐增加，使得总的更新项显著减少。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="mh mi l"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">AdaGrad Python实现</p></figure><p id="8d52" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir"> RMSprop </strong></p><p id="fbac" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">RMSprop优化算法通过将衰减率乘以累积和来解决AdaGrad的问题，并能够在某个点之后忘记累积和的历史，这取决于有助于收敛到全局最小值的衰减项。RMSprop的数学表达式由下式给出:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ml"><img src="../Images/2f38bdfa8161aea2bc0190e1ce0bb493.png" data-original-src="https://miro.medium.com/v2/resize:fit:1080/format:webp/1*fmvKSz0fQzagAIHdX2_sZA.png"/></div></figure><p id="d849" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">𝛼是衰落的术语。因此，这个衰减项提供了更快的收敛，并且忘记了梯度历史的累积和，从而产生了更优化的解。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="mh mi l"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">RMSprop Python实现</p></figure><p id="ff47" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">亚当</strong></p><p id="2c79" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">Adam优化算法是RMSprop的发展版本，它分别采用梯度的一阶和二阶动量。因此，Adam还解决了在关闭全局最小值时收敛缓慢的问题。在这个版本的自适应算法中，数学表达式由下式给出:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mm"><img src="../Images/4d24986da0a8bc8d166de47378a43009.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*D4xzv4KHqguyV1r3lEOEGg.png"/></div></div></figure><p id="6502" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">其中，𝛿𝑀𝑖是梯度的一阶衰减累积和，𝛿𝑉𝑖是梯度的二阶衰减累积和，帽子符号𝛿𝑀和𝛿𝑉是𝛿𝑀和𝛿𝑉𝑖的偏差校正值，ŋ是学习率。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="mh mi l"/></div></figure><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="mh mi l"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">Adam Python实现</p></figure><p id="6d1f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">非凸优化在起作用</strong></p><p id="061a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在计算问题公式化的背景下，理解这些优化算法背后的直觉将启发学习曲线以及深度神经网络如何从复杂数据中学习。此外，理解这些非凸算法背后的直觉将有助于您在训练机器学习模型时选择您的优化算法。在本文中，我介绍了非凸算法及其简单Python实现的直观观点。例如，当涉及到训练大型深度神经网络时，您会有为什么许多研究人员和数据科学家使用Adam优化器而不是SGD的直觉，因为Adam优化器是自适应处理的，并且与SGD相反，具有一阶和二阶动量。文章到此结束。如果你有问题，请告诉我。以下是方法。</p><p id="91ba" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">我在这里</strong></p><p id="2c41" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><a class="ae kv" href="https://twitter.com/canKocagil2" rel="noopener ugc nofollow" target="_blank">Twitter</a>|<a class="ae kv" href="https://www.linkedin.com/in/can-kocagil-970506184/" rel="noopener ugc nofollow" target="_blank">Linkedin</a>|<a class="ae kv" href="https://cankocagil.medium.com/" rel="noopener">Medium</a>|<a class="ae kv" href="mailto:cankocagil123@gmail.com" rel="noopener ugc nofollow" target="_blank">Mail</a></p><p id="2114" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">参考文献</strong></p><p id="c680" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">[1]维基百科撰稿人，《随机梯度下降》，<em class="mn">维基百科，自由百科，</em>2021年11月6日11:32 UTC，&lt;<a class="ae kv" href="https://en.wikipedia.org/w/index.php?title=Stochastic_gradient_descent&amp;oldid=1053840730" rel="noopener ugc nofollow" target="_blank">https://en.wikipedia.org/w/index.php?title = random _ gradient _ descent&amp;oldid = 1053840730</a>&gt;【2021年11月7日访问】</p></div></div>    
</body>
</html>