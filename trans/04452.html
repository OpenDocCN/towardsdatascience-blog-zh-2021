<html>
<head>
<title>Training Your Own Message Suggestions Model Using Deep Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用深度学习训练您自己的消息建议模型</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/training-your-own-message-suggestions-model-using-deep-learning-3609c0057ba8?source=collection_archive---------4-----------------------#2021-04-16">https://towardsdatascience.com/training-your-own-message-suggestions-model-using-deep-learning-3609c0057ba8?source=collection_archive---------4-----------------------#2021-04-16</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi jn"><img src="../Images/89ded1f678880f269aef2fe6eae17083.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*c9MzJcb9g6mbOsRJ"/></div></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">照片由<a class="ae kc" href="https://unsplash.com/@hope_house_press_leather_diary_studio?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">希望之家出版社-皮革日记工作室</a>在<a class="ae kc" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a></p></figure><h1 id="e2a0" class="kd ke iq bd kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">解释业务问题</h1><p id="9917" class="pw-post-body-paragraph lb lc iq ld b le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">随着近几年智能设备的出现，电子邮件和聊天已经成为我们今天生活的一个重要部分。每天，笔记本电脑和手机用户都会回复数百条信息，不仅仅是在工作场所，在个人生活中也是如此。这些回答中的大部分倾向于特定领域或需要上下文，但有很大一部分是通用的，由常见的词语组成，如“好的”、“哇”或“早上好”等。因此，我们的设备有一些额外的功能来帮助我们在这样的日常情况下响应收到的消息是有意义的。<br/>谷歌已经有了这样一个机制，叫做智能回复，可以在Gmail中看到。智能回复通过对用户当前正在查看的消息提供3种可能的响应来工作。响应的长度通常少于10个标记，并且通常具有不同的语义意图。<br/>在本文中，我们试图训练一个类似的深度学习模型，该模型将接受用户输入，并基于这些输入，输出3种可能的相关响应。</p><h1 id="672e" class="kd ke iq bd kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">目录</h1><ul class=""><li id="04d0" class="lz ma iq ld b le lf li lj lm mb lq mc lu md ly me mf mg mh bi translated">数据来源</li><li id="563a" class="lz ma iq ld b le mi li mj lm mk lq ml lu mm ly me mf mg mh bi translated">现有工作</li><li id="3577" class="lz ma iq ld b le mi li mj lm mk lq ml lu mm ly me mf mg mh bi translated">初始方法</li><li id="c889" class="lz ma iq ld b le mi li mj lm mk lq ml lu mm ly me mf mg mh bi translated">训练LSTM模型来预测响应标签</li><li id="32e9" class="lz ma iq ld b le mi li mj lm mk lq ml lu mm ly me mf mg mh bi translated">未来的工作</li></ul><h1 id="2a2d" class="kd ke iq bd kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">数据来源</h1><p id="99a6" class="pw-post-body-paragraph lb lc iq ld b le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">互联网上充斥着可以用来训练我们模型的数据集。一个很好的选择是开放字幕数据集，可以从以下链接下载-</p><p id="1ba4" class="pw-post-body-paragraph lb lc iq ld b le mn lg lh li mo lk ll lm mp lo lp lq mq ls lt lu mr lw lx ly ij bi translated"><a class="ae kc" href="https://github.com/PolyAI-LDN/conversational-datasets/tree/master/opensubtitles" rel="noopener ugc nofollow" target="_blank"><strong class="ld ir">https://github . com/PolyAI-LDN/conversatile-datasets/tree/master/open subtitle</strong></a></p><p id="f428" class="pw-post-body-paragraph lb lc iq ld b le mn lg lh li mo lk ll lm mp lo lp lq mq ls lt lu mr lw lx ly ij bi translated">开放字幕数据集由62种语言的数千部电影的字幕组成。该数据集对于文本翻译任务的训练模型非常有帮助。</p><figure class="mt mu mv mw gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi ms"><img src="../Images/8f4fb98501dd549264f49b8f7c4a9d89.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Y7Weboejf-TM5qB2s0QmkA.png"/></div></div><p class="jy jz gj gh gi ka kb bd b be z dk translated"><strong class="bd mx">Opus主页截图</strong></p></figure><p id="2cff" class="pw-post-body-paragraph lb lc iq ld b le mn lg lh li mo lk ll lm mp lo lp lq mq ls lt lu mr lw lx ly ij bi translated">这个数据集有一个小缺点，因为这个数据集中的句子是电影对白，所以它包含了大量的俚语文本，这是不可取的，因为我们需要我们的模型输出坚持一些质量标准。<br/>Ubuntu对话语料库是另一个很好的候选，它包括从Ubuntu的技术支持聊天系统中提取的大约100万个2人对话。这个数据集可以在下面的链接中找到。</p><p id="6833" class="pw-post-body-paragraph lb lc iq ld b le mn lg lh li mo lk ll lm mp lo lp lq mq ls lt lu mr lw lx ly ij bi translated"><a class="ae kc" href="https://github.com/rkadlec/ubuntu-ranking-dataset-creator" rel="noopener ugc nofollow" target="_blank"><strong class="ld ir"/>https://github.com/rkadlec/ubuntu-ranking-dataset-creator</a></p><p id="c234" class="pw-post-body-paragraph lb lc iq ld b le mn lg lh li mo lk ll lm mp lo lp lq mq ls lt lu mr lw lx ly ij bi translated">但是由于这个数据集中的信息倾向于过多地围绕Ubuntu技术支持，它由许多命令和Ubuntu相关的术语组成，因此我们的模型最终可能会稍微倾向于使用技术术语，而不是通用的、非技术的日常词汇。<br/>最后，我们有话题聊天数据集，可以在下面的链接中找到</p><p id="3b24" class="pw-post-body-paragraph lb lc iq ld b le mn lg lh li mo lk ll lm mp lo lp lq mq ls lt lu mr lw lx ly ij bi translated"><a class="ae kc" href="https://github.com/alexa/Topical-Chat" rel="noopener ugc nofollow" target="_blank"><strong class="ld ir">https://github.com/alexa/Topical-Chat</strong></a></p><p id="f97c" class="pw-post-body-paragraph lb lc iq ld b le mn lg lh li mo lk ll lm mp lo lp lq mq ls lt lu mr lw lx ly ij bi translated">该数据集中的消息基于随机的主题，但是由于主题彼此之间有很大的不同，并且随着每次对话而变化，所以它不应该使我们的模型偏向任何单一的主题，因此该数据集对于训练我们的模型似乎是理想的。(尽管后来发现，早期的试验模型之一最终确实偏向于用与美国情景喜剧“The Simpsons”相关的文本进行响应，因为数据集中的一些对话主题与Simpsons有关。例如，当被问到“你好吗”时，模型会回答“我很好”。你喜欢看《辛普森一家》吗</p><p id="e471" class="pw-post-body-paragraph lb lc iq ld b le mn lg lh li mo lk ll lm mp lo lp lq mq ls lt lu mr lw lx ly ij bi translated">在Kaggle上，同样的数据集也有一个稍微改动过的版本，链接如下</p><p id="3010" class="pw-post-body-paragraph lb lc iq ld b le mn lg lh li mo lk ll lm mp lo lp lq mq ls lt lu mr lw lx ly ij bi translated"><a class="ae kc" href="https://www.kaggle.com/arnavsharmaas/chatbot-dataset-topical-chat" rel="noopener ugc nofollow" target="_blank"><strong class="ld ir">https://www . ka ggle . com/arnavsharmas/chatbot-dataset-topic-chat</strong></a></p><p id="c4fd" class="pw-post-body-paragraph lb lc iq ld b le mn lg lh li mo lk ll lm mp lo lp lq mq ls lt lu mr lw lx ly ij bi translated">正如页面上提到的，该数据集包含“基于人类知识的开放领域对话”，由8000多个对话和184，000多条消息组成。每条消息都分配有一个对话ID，这很有用，因为它有助于只训练与输入文本相关的响应。</p><p id="594e" class="pw-post-body-paragraph lb lc iq ld b le mn lg lh li mo lk ll lm mp lo lp lq mq ls lt lu mr lw lx ly ij bi translated">数据集中的一个对话样本(显然，话题与猫有关)如下—</p><ul class=""><li id="6342" class="lz ma iq ld b le mn li mo lm my lq mz lu na ly me mf mg mh bi translated">“猫呢，你喜欢猫吗？我自己就是狗迷。”</li><li id="5a97" class="lz ma iq ld b le mi li mj lm mk lq ml lu mm ly me mf mg mh bi translated">“猫被称为家猫和野猫。他们让我们的世界变得非常干净，没有老鼠！”</li><li id="3fd7" class="lz ma iq ld b le mi li mj lm mk lq ml lu mm ly me mf mg mh bi translated">“是啊，猫可能很酷，但它们肯定会花很多时间睡觉。”</li><li id="24a6" class="lz ma iq ld b le mi li mj lm mk lq ml lu mm ly me mf mg mh bi translated">"猫听到的声音太微弱或频率太高，人耳听不到."</li><li id="c71e" class="lz ma iq ld b le mi li mj lm mk lq ml lu mm ly me mf mg mh bi translated">“我也听说了。嗯，和你聊天很愉快。祝你愉快。”</li></ul><p id="335b" class="pw-post-body-paragraph lb lc iq ld b le mn lg lh li mo lk ll lm mp lo lp lq mq ls lt lu mr lw lx ly ij bi translated">如上例所示，数据集中的一些响应可能过于具体，因此不能用作对更广泛的用户输入的响应。因此，我们还必须确保响应是通用的和非特定的，以便每个集合都提供最大的效用。</p><p id="e448" class="pw-post-body-paragraph lb lc iq ld b le mn lg lh li mo lk ll lm mp lo lp lq mq ls lt lu mr lw lx ly ij bi translated">我们将使用来自Kaggle链接的数据集，因为大部分培训将在Kaggle上进行。Kaggle在其笔记本中提供了TPU访问，从而将培训时间从几个小时大幅减少到了几分钟。</p><p id="82e4" class="pw-post-body-paragraph lb lc iq ld b le mn lg lh li mo lk ll lm mp lo lp lq mq ls lt lu mr lw lx ly ij bi translated">这个数据集在Kaggle上被命名为“聊天机器人数据集主题聊天”,是CSV格式，有3列-</p><ol class=""><li id="4114" class="lz ma iq ld b le mn li mo lm my lq mz lu na ly nb mf mg mh bi translated">对话id</li><li id="ff88" class="lz ma iq ld b le mi li mj lm mk lq ml lu mm ly nb mf mg mh bi translated">消息</li><li id="6e93" class="lz ma iq ld b le mi li mj lm mk lq ml lu mm ly nb mf mg mh bi translated">感情</li></ol><figure class="mt mu mv mw gt jr gh gi paragraph-image"><div class="gh gi nc"><img src="../Images/c27e4226ec7773dcf0ab6592e9757c85.png" data-original-src="https://miro.medium.com/v2/resize:fit:1134/format:webp/1*D_Llsm6pHqbzeoSdF8GKvA.png"/></div><p class="jy jz gj gh gi ka kb bd b be z dk translated"><strong class="bd mx">Kaggle上数据集页面截图</strong></p></figure><p id="46c0" class="pw-post-body-paragraph lb lc iq ld b le mn lg lh li mo lk ll lm mp lo lp lq mq ls lt lu mr lw lx ly ij bi translated">列“<strong class="ld ir"> conversation_id </strong>”是一个整数，表示消息所属的会话。“<strong class="ld ir">消息</strong>”列包含实际的句子，该句子或者是对话的开始，或者是对对话中先前消息的回复。“<strong class="ld ir">情绪</strong>栏表示7种情绪之一— <strong class="ld ir"> </strong>愤怒、好奇、伪装、恐惧、快乐、悲伤和惊讶。</p><p id="2966" class="pw-post-body-paragraph lb lc iq ld b le mn lg lh li mo lk ll lm mp lo lp lq mq ls lt lu mr lw lx ly ij bi translated">为了训练我们的模型，我们将只使用数据集中的前两列，即<strong class="ld ir"> conversation_id </strong>和<strong class="ld ir"> message。</strong></p><h1 id="9bf3" class="kd ke iq bd kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">现有工作</h1><p id="e882" class="pw-post-body-paragraph lb lc iq ld b le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">正如谷歌的智能回复文件中提到的，建立一个自动建议回复电子邮件的系统并不是一个积极研究的领域。为电子邮件提出自动回复建议并不完全是一个标准的机器学习问题，因此在这个方向上几乎没有任何工作。</p><h1 id="9119" class="kd ke iq bd kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">初始方法</h1><h2 id="3087" class="nd ke iq bd kf ne nf dn kj ng nh dp kn lm ni nj kr lq nk nl kv lu nm nn kz no bi translated">预处理和数据集准备</h2><p id="a2b8" class="pw-post-body-paragraph lb lc iq ld b le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">我们首先使用Pandas将所有消息加载到一个数据帧中。</p><figure class="mt mu mv mw gt jr"><div class="bz fp l di"><div class="np nq l"/></div></figure><p id="4317" class="pw-post-body-paragraph lb lc iq ld b le mn lg lh li mo lk ll lm mp lo lp lq mq ls lt lu mr lw lx ly ij bi translated">加载数据框后，所有重复项都将被删除。</p><figure class="mt mu mv mw gt jr"><div class="bz fp l di"><div class="np nq l"/></div></figure><p id="6a9f" class="pw-post-body-paragraph lb lc iq ld b le mn lg lh li mo lk ll lm mp lo lp lq mq ls lt lu mr lw lx ly ij bi translated">进行基本的预处理以去除所有标点符号。</p><figure class="mt mu mv mw gt jr"><div class="bz fp l di"><div class="np nq l"/></div></figure><p id="ea13" class="pw-post-body-paragraph lb lc iq ld b le mn lg lh li mo lk ll lm mp lo lp lq mq ls lt lu mr lw lx ly ij bi translated">然后，数据帧中的所有消息按照会话ID进行拆分和分组。我们创建了两个名为“input_texts”和“target_texts”的列表，并将会话组中的每条消息(少于50个单词)添加到“input_texts”中，将其后的消息(少于10个单词)添加到“target_texts”中。之所以只考虑10个单词或更少的消息作为目标文本，是因为我们希望捕获广泛和一般的语句，这些语句可以作为对各种用户输入的查询的响应。简短的目标文本，如“好的”、“哇”、“很高兴与你聊天”、“你也一样”，可以作为各种查询的合适响应。另一方面，像下面这样的长而具体的句子很少被用作回应</p><p id="b5d5" class="pw-post-body-paragraph lb lc iq ld b le mn lg lh li mo lk ll lm mp lo lp lq mq ls lt lu mr lw lx ly ij bi translated">“来自这样一位才华横溢的歌手和舞者真是太棒了。我做梦也跳不出那样的舞。”</p><p id="724a" class="pw-post-body-paragraph lb lc iq ld b le mn lg lh li mo lk ll lm mp lo lp lq mq ls lt lu mr lw lx ly ij bi translated">输入文本最多允许有50个单词，因为我们假设任何更长的句子都需要更详细的响应，需要上下文，在某些情况下还需要领域知识，因此不适合从我们的模型中获取响应文本。</p><figure class="mt mu mv mw gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nr"><img src="../Images/a2ce9f446ba454327efc714a3d73524b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*K1ET0fPd5mMnB6Lt_9P9hg.png"/></div></div><p class="jy jz gj gh gi ka kb bd b be z dk translated"><strong class="bd mx">过滤输入和目标文本</strong></p></figure><p id="5153" class="pw-post-body-paragraph lb lc iq ld b le mn lg lh li mo lk ll lm mp lo lp lq mq ls lt lu mr lw lx ly ij bi translated">一旦输入和目标文本数组被填充，我们就标记和填充两个数组的每一项。标记化是自然语言处理中非常常见的任务，它将数组中的每个句子转换成更小的单元，并为该单元分配唯一的整数值。每个单元既可以是一个单独的单词，也可以是单词中的一个字符。每个单元被称为一个标记，因此这个过程被称为标记化。考虑一下“我爱咖啡”这句话。这句话可以用以下两种方式来标记—</p><p id="d3f3" class="pw-post-body-paragraph lb lc iq ld b le mn lg lh li mo lk ll lm mp lo lp lq mq ls lt lu mr lw lx ly ij bi translated">字符标记—</p><p id="af82" class="pw-post-body-paragraph lb lc iq ld b le mn lg lh li mo lk ll lm mp lo lp lq mq ls lt lu mr lw lx ly ij bi translated">i = 1，(空格)= 2，l = 3，o = 4，v = 5，e = 6，c = 7，o = 4，f = 8，e = 6</p><p id="1eb6" class="pw-post-body-paragraph lb lc iq ld b le mn lg lh li mo lk ll lm mp lo lp lq mq ls lt lu mr lw lx ly ij bi translated">工作代币—</p><p id="be43" class="pw-post-body-paragraph lb lc iq ld b le mn lg lh li mo lk ll lm mp lo lp lq mq ls lt lu mr lw lx ly ij bi translated">我= 1，爱= 2，咖啡= 3</p><p id="aaf3" class="pw-post-body-paragraph lb lc iq ld b le mn lg lh li mo lk ll lm mp lo lp lq mq ls lt lu mr lw lx ly ij bi translated">在字符级标记化中，我们最终得到8个唯一的标记，而在单词级标记化中，我们得到3个标记。</p><p id="57de" class="pw-post-body-paragraph lb lc iq ld b le mn lg lh li mo lk ll lm mp lo lp lq mq ls lt lu mr lw lx ly ij bi translated">为了训练我们的模型，我们使用了单词标记。</p><p id="8bd6" class="pw-post-body-paragraph lb lc iq ld b le mn lg lh li mo lk ll lm mp lo lp lq mq ls lt lu mr lw lx ly ij bi translated">由于每个句子中的单词数量不同，因此数据集中每个项目的输入大小也不同。这是因为，很明显，数据集中有些句子较长，有些较短。现在，如果我们在训练期间一次只向我们的模型提供一个数组项目(即批量= 1 ),这是可以的。但是这将显著增加训练时间，因此不是优选的。</p><p id="bb8f" class="pw-post-body-paragraph lb lc iq ld b le mn lg lh li mo lk ll lm mp lo lp lq mq ls lt lu mr lw lx ly ij bi translated">在填充中，单个标记化的句子以零作为前缀或后缀，以保持输入大小固定。这允许我们在训练中使用更高的批量值。</p><p id="d114" class="pw-post-body-paragraph lb lc iq ld b le mn lg lh li mo lk ll lm mp lo lp lq mq ls lt lu mr lw lx ly ij bi translated">在下面的代码片段中，我们声明了一个标记化器，然后将它“安装”在目标文本数组上。这将为目标文本数组中的每个唯一单词生成一个令牌。一旦生成了标记，就通过将目标文本数组传递给“texts_to_sequences”函数，将其转换为标记数组。完成标记化后，使用函数“generate_padded_sequences”填充新创建的“target_sequences”数组。</p><figure class="mt mu mv mw gt jr"><div class="bz fp l di"><div class="np nq l"/></div></figure><p id="b4c9" class="pw-post-body-paragraph lb lc iq ld b le mn lg lh li mo lk ll lm mp lo lp lq mq ls lt lu mr lw lx ly ij bi translated">在对输入和目标文本进行标记化和填充之后，我们训练我们的第一个模型，这是一个单词级Seq2Seq模型。</p><h2 id="8700" class="nd ke iq bd kf ne nf dn kj ng nh dp kn lm ni nj kr lq nk nl kv lu nm nn kz no bi translated">训练单词级Seq2Seq模型并使用波束搜索进行预测</h2><p id="53af" class="pw-post-body-paragraph lb lc iq ld b le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">Seq2Seq模型是一个编码器-解码器框架，它接收一个输入序列(可以是字符或单词)并输出另一个序列。例如，输入可以是英语单词序列，输出可以是完全不同的语言(如法语或印地语)的单词序列。在我们的例子中，输入是用户的查询，输出是对用户查询的响应。</p><p id="ddf1" class="pw-post-body-paragraph lb lc iq ld b le mn lg lh li mo lk ll lm mp lo lp lq mq ls lt lu mr lw lx ly ij bi translated">Seq2Seq的工作原理是首先将输入序列输入编码器，然后将其转换为状态向量。</p><figure class="mt mu mv mw gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi ns"><img src="../Images/3c4f16ff21542e4b9f5fd81e312f81d6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FPE39f17HG2C-Q5fGj_uYg.png"/></div></div><p class="jy jz gj gh gi ka kb bd b be z dk translated"><strong class="bd mx">词级Seq2Seq模型</strong></p></figure><p id="19ea" class="pw-post-body-paragraph lb lc iq ld b le mn lg lh li mo lk ll lm mp lo lp lq mq ls lt lu mr lw lx ly ij bi translated">一旦状态向量准备就绪，解码器就将状态向量与第一个目标序列一起输入，该目标序列映射到任何指示序列开始的标记，例如— <bos>或语句开始，以获得第一个时间步长的输出。解码器的输出现在与新的状态向量一起被采样。最新的输出和状态向量现在再次被馈送到解码器，以获得下一时间步的输出和状态向量。重复该过程，直到解码器输出指示序列结束的标记，例如— <eos>或语句结束或最大输出句子长度限制。</eos></bos></p><p id="1c01" class="pw-post-body-paragraph lb lc iq ld b le mn lg lh li mo lk ll lm mp lo lp lq mq ls lt lu mr lw lx ly ij bi translated">训练后，可以“贪婪地”或通过使用波束搜索来进行文本预测。在贪婪方法中，来自较早时间步骤的具有最高概率的令牌(连同状态向量)被简单地馈送给模型，以获得当前时间步骤的输出。重复这个过程，直到从模型接收到作为输出的语句结束标记。</p><figure class="mt mu mv mw gt jr"><div class="bz fp l di"><div class="np nq l"/></div><p class="jy jz gj gh gi ka kb bd b be z dk translated"><strong class="ak">贪婪文本预测</strong></p></figure><p id="3e97" class="pw-post-body-paragraph lb lc iq ld b le mn lg lh li mo lk ll lm mp lo lp lq mq ls lt lu mr lw lx ly ij bi translated">另一方面，在波束搜索中，考虑前K个可能的记号(和它们相应的状态向量),而不是仅仅一个。然后，前K个表征和状态向量在下一个时间步骤中被馈送到模型，每个表征生成下一个前K个表征，从而导致K*K个可能的输出。计算所有输出的组合概率得分(对于跨所有时间步长的令牌)。这个K*K个输出的数组根据它们的分数排序(以降序方式),除了最上面的K个，所有其他输出都被排除。重复这个过程，直到模型为K个“束”中的每一个输出语句结束标记或最大输出句子长度限制。</p><p id="3ca5" class="pw-post-body-paragraph lb lc iq ld b le mn lg lh li mo lk ll lm mp lo lp lq mq ls lt lu mr lw lx ly ij bi translated">Seq2Seq是一个基于文本生成的模型，它为输入文本生成文本输出。文本生成模型的一个问题是，有时模型可能会给出太不正式或不适合电子邮件的响应(例如，“好啊”，“非常感谢兄弟”)。在某些情况下，输出也可能是令人不快的(例如，“迷路”)。这是基于文本生成的模型的主要缺点之一。</p><p id="8764" class="pw-post-body-paragraph lb lc iq ld b le mn lg lh li mo lk ll lm mp lo lp lq mq ls lt lu mr lw lx ly ij bi translated">除此之外，我们还能如何开发LSTMs的巨大潜力，同时确保在模型响应时始终保持特定的质量标准？</p><p id="40ac" class="pw-post-body-paragraph lb lc iq ld b le mn lg lh li mo lk ll lm mp lo lp lq mq ls lt lu mr lw lx ly ij bi translated">一种方法是根据预定的响应集训练我们的模型，该响应集只包含满足所有质量响应的响应。对于使用来说被认为太不合适或太不正式的回答可以从数据集中移除，从而确保满足所有标准。一旦排除了所有不需要的响应，剩余的集合可以被分配标签，然后可以被用作训练模型的目标，输入是用户输入的文本。如果在生产环境中使用这种模型来提供某种服务(例如，为银行应用程序提供聊天机器人支持)，这种方法可以帮助我们确保所有响应都限制在所提供的服务范围内。这种方法的另一个优点是这些标签中的一些可以被映射到一些功能上，从而代替文本输出，执行一个任务。例如，继续我们前面的例子，在银行环境中，一个标签可以被映射到获取客户的银行账户余额。因此，当用户输入诸如“我的银行账户余额是多少”之类的任何文本时，(假设)客户的实时银行余额被获取并显示给他或她。</p><p id="64c6" class="pw-post-body-paragraph lb lc iq ld b le mn lg lh li mo lk ll lm mp lo lp lq mq ls lt lu mr lw lx ly ij bi translated">然而，这种方法的挑战是生成只包含适当响应的理想“响应集”。另一个挑战是将这样的响应集标签映射到训练数据集中的各个输入文本。</p><p id="e2a1" class="pw-post-body-paragraph lb lc iq ld b le mn lg lh li mo lk ll lm mp lo lp lq mq ls lt lu mr lw lx ly ij bi translated">以下几节解释了我们可以在某种程度上克服上述挑战的方法之一。</p><h1 id="49cb" class="kd ke iq bd kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">训练基于响应集的LSTM模型</h1><p id="05e8" class="pw-post-body-paragraph lb lc iq ld b le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">如上所述，响应集是语义相近的句子的集合，这些句子优选地具有10个或更少的标记(在我们的情况下),并且是通用的、非特定的句子，可以作为对各种用户输入的输入的有效响应。由于我们拥有的唯一数据是数据集中的消息，我们面临的任务是过滤出特定的响应，然后将所有语义相近的消息分组到单独的集群中。</p><p id="3c05" class="pw-post-body-paragraph lb lc iq ld b le mn lg lh li mo lk ll lm mp lo lp lq mq ls lt lu mr lw lx ly ij bi translated">实现这一点的一个方法是—</p><ol class=""><li id="69c3" class="lz ma iq ld b le mn li mo lm my lq mz lu na ly nb mf mg mh bi translated"><strong class="ld ir">将目标句子转换成向量(即句子嵌入)——</strong>应用某种算法将目标信息转换成它们的N维表示。</li><li id="59f1" class="lz ma iq ld b le mi li mj lm mk lq ml lu mm ly nb mf mg mh bi translated"><strong class="ld ir">对向量进行聚类</strong> —使用聚类算法将N维空间中的邻近点分组在一起，以创建一组聚类或响应集。一旦点(即句子的向量表示)被聚类，每个聚类可以被分配一个标签。</li><li id="94ab" class="lz ma iq ld b le mi li mj lm mk lq ml lu mm ly nb mf mg mh bi translated"><strong class="ld ir">将聚类标签映射到输入文本</strong> —将每个目标句子映射到一个聚类的标签现在可以用于将相应的输入文本映射到这些聚类。一旦映射完成，我们就可以用这个新的数据集训练一个LSTM模型，在给定一个输入文本的情况下预测一个聚类标签。</li></ol><p id="4d04" class="pw-post-body-paragraph lb lc iq ld b le mn lg lh li mo lk ll lm mp lo lp lq mq ls lt lu mr lw lx ly ij bi translated">关于这种方法的更多细节可以在下面的章节中找到。</p><h2 id="f3c6" class="nd ke iq bd kf ne nf dn kj ng nh dp kn lm ni nj kr lq nk nl kv lu nm nn kz no bi translated">将目标句子转换成向量</h2><p id="3a6f" class="pw-post-body-paragraph lb lc iq ld b le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">我们的输入和目标文本数组中的句子可以通过使用以下任何一个库转换成向量</p><ol class=""><li id="610a" class="lz ma iq ld b le mn li mo lm my lq mz lu na ly nb mf mg mh bi translated"><strong class="ld ir">sent 2 vec—</strong><a class="ae kc" href="https://github.com/pdrm83/sent2vec" rel="noopener ugc nofollow" target="_blank"><strong class="ld ir">https://github.com/pdrm83/sent2vec</strong></a></li></ol><p id="727f" class="pw-post-body-paragraph lb lc iq ld b le mn lg lh li mo lk ll lm mp lo lp lq mq ls lt lu mr lw lx ly ij bi translated">Sent2Vec可以看作是广泛使用的word2vec库的扩展。Word2vec将单词转换为固定长度的向量，这样语义相似的单词，如“美丽的”和“有吸引力的”或“愤怒的”和“狂怒的”在向量空间中更接近。我们可以使用word2vec通过使用单词袋等技术来表示句子，但是这种方法不考虑句子的顺序和整体含义。因此，需要一种更复杂的方法来将所有消息转换成向量。这就是sent2vec的用武之地。</p><figure class="mt mu mv mw gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nt"><img src="../Images/6edccddf8ad755a2cd7574c236219aed.png" data-original-src="https://miro.medium.com/v2/resize:fit:1138/format:webp/1*aId6jSnQpRPSHFqlRf1iVQ.png"/></div></div><p class="jy jz gj gh gi ka kb bd b be z dk translated"><strong class="bd mx">句去矢</strong></p></figure><p id="e625" class="pw-post-body-paragraph lb lc iq ld b le mn lg lh li mo lk ll lm mp lo lp lq mq ls lt lu mr lw lx ly ij bi translated">Sent2vec将句子编码成有意义的格式，这种格式考虑了单词的顺序和句子的整体语义。</p><p id="fde0" class="pw-post-body-paragraph lb lc iq ld b le mn lg lh li mo lk ll lm mp lo lp lq mq ls lt lu mr lw lx ly ij bi translated">因此，Sent2vec是一个不错的编码器，可用于对数据集中的消息进行编码。然而，当试图在16GB的机器上编码21，000+数据集句子时，sent2vec在编码时不断耗尽内存，最终导致笔记本崩溃。经过多次试验，发现sent2vec最多只能编码大约5000个句子。由于5000个句子不足以训练我们的模型，因此需要另一个可以编码21，000+个句子的库。</p><p id="909b" class="pw-post-body-paragraph lb lc iq ld b le mn lg lh li mo lk ll lm mp lo lp lq mq ls lt lu mr lw lx ly ij bi translated"><strong class="ld ir"> 2。惹恼—</strong><a class="ae kc" href="https://github.com/spotify/annoy/" rel="noopener ugc nofollow" target="_blank"><strong class="ld ir">https://github.com/spotify/annoy/</strong></a></p><p id="db18" class="pw-post-body-paragraph lb lc iq ld b le mn lg lh li mo lk ll lm mp lo lp lq mq ls lt lu mr lw lx ly ij bi translated">Yeah上的骚扰或近似最近邻居是Spotify开发和使用的一个库，用于提供音乐推荐。ANNOY能够在几分钟内成功地对所有21，000+点进行编码。</p><p id="9552" class="pw-post-body-paragraph lb lc iq ld b le mn lg lh li mo lk ll lm mp lo lp lq mq ls lt lu mr lw lx ly ij bi translated">这个库使用欧几里德距离、曼哈顿距离、余弦距离、汉明距离或点(内)积距离来解决N维空间中最近邻搜索的问题。给定一个查询点，库搜索接近这个点的点，并且使用非常少的内存。</p><p id="2cc1" class="pw-post-body-paragraph lb lc iq ld b le mn lg lh li mo lk ll lm mp lo lp lq mq ls lt lu mr lw lx ly ij bi translated">以下代码片段返回数据集中所有消息的N维向量—</p><figure class="mt mu mv mw gt jr"><div class="bz fp l di"><div class="np nq l"/></div></figure><p id="2719" class="pw-post-body-paragraph lb lc iq ld b le mn lg lh li mo lk ll lm mp lo lp lq mq ls lt lu mr lw lx ly ij bi translated">" inputanoyindex . build(100)"构建一个由100棵树组成的森林。一旦构建完成，inputAnnoyIndex就可以用于查找与给定点最近的k个点。</p><p id="2fcd" class="pw-post-body-paragraph lb lc iq ld b le mn lg lh li mo lk ll lm mp lo lp lq mq ls lt lu mr lw lx ly ij bi translated">下面我们随机向库输入一个查询点索引，它会返回几个相似的句子。</p><blockquote class="nu"><p id="271d" class="nv nw iq bd nx ny nz oa ob oc od ly dk translated">当然罗，你看过《功夫》吗？是的，我也是。你看过《功夫》吗？你看过功夫吗</p></blockquote><p id="737f" class="pw-post-body-paragraph lb lc iq ld b le oe lg lh li of lk ll lm og lo lp lq oh ls lt lu oi lw lx ly ij bi translated">使用同样的方法，我们为输入文本和目标文本生成一个简单的相似性矩阵。</p><figure class="mt mu mv mw gt jr"><div class="bz fp l di"><div class="np nq l"/></div></figure><p id="b26a" class="pw-post-body-paragraph lb lc iq ld b le mn lg lh li mo lk ll lm mp lo lp lq mq ls lt lu mr lw lx ly ij bi translated">在下一步中，我们使用这个相似性矩阵将数据集中所有相似的句子聚集在一起，并为每个聚类分配标签。</p><h2 id="af26" class="nd ke iq bd kf ne nf dn kj ng nh dp kn lm ni nj kr lq nk nl kv lu nm nn kz no bi translated">聚类语义相似的句子</h2><p id="c351" class="pw-post-body-paragraph lb lc iq ld b le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">聚类语义相似的句子是至关重要的一步，因为它让我们生成响应集。为了生成响应集，语义上彼此接近的句子被分组到单个簇中(或者换句话说，一个响应集)。以这种方式，数据集中的所有目标句子被分组到多个响应集合中，并被分配唯一的标签。</p><figure class="mt mu mv mw gt jr gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/47664d3b0b6d477391ed3a8b19a2a579.png" data-original-src="https://miro.medium.com/v2/resize:fit:1166/format:webp/1*m8nl5w4C5WlvbtHAzSz2eQ.png"/></div><p class="jy jz gj gh gi ka kb bd b be z dk translated"><strong class="bd mx">聚类语义相似的句子</strong></p></figure><p id="fcd8" class="pw-post-body-paragraph lb lc iq ld b le mn lg lh li mo lk ll lm mp lo lp lq mq ls lt lu mr lw lx ly ij bi translated">一旦分配了标签，我们就根据目标文本的标签在输入文本上训练模型。</p><p id="355f" class="pw-post-body-paragraph lb lc iq ld b le mn lg lh li mo lk ll lm mp lo lp lq mq ls lt lu mr lw lx ly ij bi translated">在前面的步骤中生成的句子相似性矩阵的预览如下所示</p><figure class="mt mu mv mw gt jr"><div class="bz fp l di"><div class="np nq l"/></div></figure><p id="ae68" class="pw-post-body-paragraph lb lc iq ld b le mn lg lh li mo lk ll lm mp lo lp lq mq ls lt lu mr lw lx ly ij bi translated">为了将所有相似的句子聚类在一起，我们使用DBSCAN(带噪声的基于密度的应用空间聚类)算法，这是最广泛使用的聚类算法之一。使用DBSCAN算法而不是其他聚类算法(如K-Means)的原因之一是，给定某个空间中的一组点，DBSCAN将密集邻域中的邻近点组合在一起，并自行得出聚类数。这很方便，因为我们不知道我们的目标文本数组包含多少“响应集”或相似句子集。我们将使用scikit-learn的DBSCAN算法实现来满足集群需求。更多细节可以在下面的链接中找到——</p><p id="9f10" class="pw-post-body-paragraph lb lc iq ld b le mn lg lh li mo lk ll lm mp lo lp lq mq ls lt lu mr lw lx ly ij bi translated">【T2<strong class="ld ir">https://sci kit-learn . org/stable/modules/generated/sk learn . cluster . DBS can . html</strong>T5】</p><p id="5a14" class="pw-post-body-paragraph lb lc iq ld b le mn lg lh li mo lk ll lm mp lo lp lq mq ls lt lu mr lw lx ly ij bi translated">然而，在我们开始DBSCAN之前，参数“eps”必须是固定的，它是两个点之间的最大距离，以便被认为是彼此的邻域。这很重要，因为如果eps值太小，将不会发生聚类，所有点都将被标记为噪声。另一方面，如果eps选择得太大，它会导致所有点合并成一个大簇。因此，必须将eps的最佳值传递给DBSCAN算法，使得只有语义接近的句子被分组，而将其他不相关的句子作为噪声排除在外。</p><p id="7525" class="pw-post-body-paragraph lb lc iq ld b le mn lg lh li mo lk ll lm mp lo lp lq mq ls lt lu mr lw lx ly ij bi translated">为了做到这一点，我们首先尝试通过使用熊猫的“n最小”函数来估计每个点的第10个邻居的距离分布。</p><p id="259a" class="pw-post-body-paragraph lb lc iq ld b le mn lg lh li mo lk ll lm mp lo lp lq mq ls lt lu mr lw lx ly ij bi translated">一旦我们对所有点的第10个邻居的距离的分布有了一个公平的想法，我们通过尝试多个eps值并捕获每个eps值的聚类和噪声点的数量来更深入地挖掘。</p><figure class="mt mu mv mw gt jr"><div class="bz fp l di"><div class="np nq l"/></div></figure><p id="3efc" class="pw-post-body-paragraph lb lc iq ld b le mn lg lh li mo lk ll lm mp lo lp lq mq ls lt lu mr lw lx ly ij bi translated">我们绘制了Y轴上的聚类数/噪声与X轴上的eps值的关系图。</p><figure class="mt mu mv mw gt jr gh gi paragraph-image"><div class="gh gi ok"><img src="../Images/d091638bfeff3314756e6c77a51ea9fc.png" data-original-src="https://miro.medium.com/v2/resize:fit:896/format:webp/0*3bDHXh-ia-DyXT35.png"/></div><p class="jy jz gj gh gi ka kb bd b be z dk translated"><strong class="bd mx">聚类数/噪声与EPS的关系</strong></p></figure><p id="4fa7" class="pw-post-body-paragraph lb lc iq ld b le mn lg lh li mo lk ll lm mp lo lp lq mq ls lt lu mr lw lx ly ij bi translated">基于观察，我们认为0.006是运行DBSCAN算法的eps的合适值，该算法最终给出881个语义相似句子的聚类。</p><figure class="mt mu mv mw gt jr"><div class="bz fp l di"><div class="np nq l"/></div></figure><p id="92cd" class="pw-post-body-paragraph lb lc iq ld b le mn lg lh li mo lk ll lm mp lo lp lq mq ls lt lu mr lw lx ly ij bi translated">有关881集群的更多详细信息，请点击下面的链接。</p><p id="3a17" class="pw-post-body-paragraph lb lc iq ld b le mn lg lh li mo lk ll lm mp lo lp lq mq ls lt lu mr lw lx ly ij bi translated"><a class="ae kc" href="https://www.kaggle.com/greatrxt/smart-reply-clustering-annoy-output" rel="noopener ugc nofollow" target="_blank"><strong class="ld ir">https://www . ka ggle . com/great rxt/smart-reply-clustering-asury-output</strong>T3】</a></p><h2 id="8747" class="nd ke iq bd kf ne nf dn kj ng nh dp kn lm ni nj kr lq nk nl kv lu nm nn kz no bi translated">训练模型</h2><p id="7667" class="pw-post-body-paragraph lb lc iq ld b le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">这是最关键的步骤，其中我们训练单层LSTM模型来接受输入文本并预测目标聚类标签。</p><figure class="mt mu mv mw gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi ol"><img src="../Images/3643ba2b307ef05411189511da1bd991.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WmrR5ak8GsFEi5rrpjAw6w.png"/></div></div><p class="jy jz gj gh gi ka kb bd b be z dk translated"><strong class="bd mx">模型预测工作流程</strong></p></figure><p id="fed6" class="pw-post-body-paragraph lb lc iq ld b le mn lg lh li mo lk ll lm mp lo lp lq mq ls lt lu mr lw lx ly ij bi translated">模型架构非常简单，第一层是一个嵌入层，它接受一个整数(代表数据集词汇表中的一个单词),并将其转换为该单词的512维密集矢量表示。下一层是LSTM层，后面是辍学率为20%的辍学层。最后一层是具有sigmoid激活的致密层。一旦输入文本，密集层输出每个标签的概率。</p><p id="1802" class="pw-post-body-paragraph lb lc iq ld b le mn lg lh li mo lk ll lm mp lo lp lq mq ls lt lu mr lw lx ly ij bi translated">该模型被训练25个时期。在配备16 GB RAM和第六代I7处理器的标准笔记本电脑上，每个纪元需要大约50-60分钟才能完成。因此，25个纪元的训练需要一整天。如果我们的模型需要微调，因此需要一次又一次地训练，这会使训练变得有些乏味。因此，使用TPU加速器，在Kaggle笔记本上完成了25个时期的全部训练。一旦笔记本中的加速器选项切换到“TPU”，我们使用以下代码片段来启用TensorFlow Keras中的TPU培训。</p><figure class="mt mu mv mw gt jr"><div class="bz fp l di"><div class="np nq l"/></div></figure><p id="d2a4" class="pw-post-body-paragraph lb lc iq ld b le mn lg lh li mo lk ll lm mp lo lp lq mq ls lt lu mr lw lx ly ij bi translated">我们实例化一个TPUStrategy，然后在这个TPUStrategy的范围内实例化我们的模型。</p><figure class="mt mu mv mw gt jr"><div class="bz fp l di"><div class="np nq l"/></div></figure><p id="3046" class="pw-post-body-paragraph lb lc iq ld b le mn lg lh li mo lk ll lm mp lo lp lq mq ls lt lu mr lw lx ly ij bi translated">一旦模型被训练，我们保存它以便以后使用。</p><figure class="mt mu mv mw gt jr gh gi paragraph-image"><div class="gh gi om"><img src="../Images/8237a7b71ffb4330f0a797676356b9dc.png" data-original-src="https://miro.medium.com/v2/resize:fit:860/format:webp/1*pUugtD6-VsDIGJFExkn61w.png"/></div><p class="jy jz gj gh gi ka kb bd b be z dk translated"><strong class="bd mx">模型损失与历元数的关系</strong></p></figure><p id="9dfa" class="pw-post-body-paragraph lb lc iq ld b le mn lg lh li mo lk ll lm mp lo lp lq mq ls lt lu mr lw lx ly ij bi translated">左边的图显示了模型损失与模型被训练的时期数的关系。</p><p id="96d8" class="pw-post-body-paragraph lb lc iq ld b le mn lg lh li mo lk ll lm mp lo lp lq mq ls lt lu mr lw lx ly ij bi translated">我们使用下面给出的函数“get_response”来获得文本输入的输出。该函数从用户处获取输入文本，并将其提供给模型。该模型进而预测每个分类标签的概率。然后以降序方式对概率进行排序，并且仅考虑具有前5个概率分数的标签(predictions . arg sort()[0][::-1][:n])。</p><figure class="mt mu mv mw gt jr"><div class="bz fp l di"><div class="np nq l"/></div></figure><p id="3d2f" class="pw-post-body-paragraph lb lc iq ld b le mn lg lh li mo lk ll lm mp lo lp lq mq ls lt lu mr lw lx ly ij bi translated">一旦我们有了具有前5个概率分数的聚类标签，我们就遍历每个聚类，并从每个聚类中随机选取任何句子，并将其作为可能的响应显示给用户。</p><h2 id="80b4" class="nd ke iq bd kf ne nf dn kj ng nh dp kn lm ni nj kr lq nk nl kv lu nm nn kz no bi translated">最终输出</h2><p id="ca9f" class="pw-post-body-paragraph lb lc iq ld b le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">我们模型的一些样本输出(以及每个响应的概率分数)如下所示</p><figure class="mt mu mv mw gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi on"><img src="../Images/a49a60c1ea16b91d68f97f798d817cb6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*P_FOUcU6KvlNYQwnM4GwhQ.png"/></div></div></figure><p id="ab53" class="pw-post-body-paragraph lb lc iq ld b le mn lg lh li mo lk ll lm mp lo lp lq mq ls lt lu mr lw lx ly ij bi translated">请注意，在生产设置中，得分为0的回答将被忽略。</p><figure class="mt mu mv mw gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi oo"><img src="../Images/6bfc2148401dc5b194025e22cb2cef6d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JdwsLfV37iuZNkt-n8Sk1Q.png"/></div></div></figure><figure class="mt mu mv mw gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi op"><img src="../Images/5a5a990c8c9611b0d6612bc2effefb81.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lvSsS1qT3-k5A4DcP6KKzw.png"/></div></div></figure><h1 id="14bc" class="kd ke iq bd kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated"><strong class="ak">未来工作</strong></h1><p id="4285" class="pw-post-body-paragraph lb lc iq ld b le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated"><strong class="ld ir">对输入文本和目标文本进行词干/词尾排序</strong></p><p id="11c3" class="pw-post-body-paragraph lb lc iq ld b le mn lg lh li mo lk ll lm mp lo lp lq mq ls lt lu mr lw lx ly ij bi translated">词干化是一种文本标准化技术，它通过删除常见的后缀或前缀来切断单词的结尾或开头，而词汇化则在将单词简化为其根单词之前考虑单词的形态分析。使用这样的技术，输入文本可以被标准化，然后输入到模型中，有望得到更好的预测。类似地，目标文本也可以被规范化，以便聚集相似的聚类并增加每个聚类的点数。</p><p id="29ca" class="pw-post-body-paragraph lb lc iq ld b le mn lg lh li mo lk ll lm mp lo lp lq mq ls lt lu mr lw lx ly ij bi translated"><strong class="ld ir">剔除点数较少的聚类</strong></p><p id="5fba" class="pw-post-body-paragraph lb lc iq ld b le mn lg lh li mo lk ll lm mp lo lp lq mq ls lt lu mr lw lx ly ij bi translated">一些聚类具有非常少的点(只有2、3个点)，因此不足以训练模型。通过将DBSCAN中的“min_samples”设置为更高的值(大约50)，可以删除这些点。但是这将减少响应集的数量。这个问题可以通过向我们的数据集添加更多的点来解决，如下所述。</p><p id="e467" class="pw-post-body-paragraph lb lc iq ld b le mn lg lh li mo lk ll lm mp lo lp lq mq ls lt lu mr lw lx ly ij bi translated"><strong class="ld ir">使用附加数据集进行训练</strong></p><p id="4204" class="pw-post-body-paragraph lb lc iq ld b le mn lg lh li mo lk ll lm mp lo lp lq mq ls lt lu mr lw lx ly ij bi translated">合并额外的数据集可以帮助我们在每个聚类中获得更多的点。看起来21，000+个点足以训练我们的模型，但是这些点分布在880个集群中。因此，一些聚类只有很少的训练点，这使得数据集非常不平衡。</p><p id="5b55" class="pw-post-body-paragraph lb lc iq ld b le mn lg lh li mo lk ll lm mp lo lp lq mq ls lt lu mr lw lx ly ij bi translated">通过合并更多的数据集来增加更多的点可能会解决这个问题。</p><h1 id="1444" class="kd ke iq bd kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">结论</h1><p id="1aed" class="pw-post-body-paragraph lb lc iq ld b le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">有人可能会说，使用响应集而不是传统的基于文本生成的方法来训练深度神经网络限制了它们的巨大潜力。这是真的，因为使用响应集训练的网络将只有有限数量的响应，无论模型架构有多好或训练有素。另一方面，与我们当前的模型相比，一个训练有素的具有良好架构的模型可以给出更好、更多样的建议。</p><p id="be8a" class="pw-post-body-paragraph lb lc iq ld b le mn lg lh li mo lk ll lm mp lo lp lq mq ls lt lu mr lw lx ly ij bi translated">但是，正如已经解释过的，即使是最好的模型有时也会偏离轨道，并给出不适当的响应，这在生产设置中可能是不期望的。因此，在深度神经网络变得足够好以准确处理人类给出的任何类型的输入文本之前，我们可能不得不退回到这样的替代方法，以保证在现实世界的场景中遵守所需的标准。</p><h1 id="3cce" class="kd ke iq bd kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">GitHub链接</h1><p id="0d5a" class="pw-post-body-paragraph lb lc iq ld b le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">完整的代码可以在GitHub上找到，链接如下</p><p id="cff5" class="pw-post-body-paragraph lb lc iq ld b le mn lg lh li mo lk ll lm mp lo lp lq mq ls lt lu mr lw lx ly ij bi translated">【https://github.com/greatrxt/deep-suggestions T4】</p><h1 id="6334" class="kd ke iq bd kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">作者的LinkedIn个人资料</h1><p id="127e" class="pw-post-body-paragraph lb lc iq ld b le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated"><a class="ae kc" href="https://www.linkedin.com/in/rakshitpujari/" rel="noopener ugc nofollow" target="_blank">https://www.linkedin.com/in/rakshitpujari/</a></p><h1 id="540e" class="kd ke iq bd kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated"><strong class="ak">参考文献</strong></h1><p id="64a4" class="pw-post-body-paragraph lb lc iq ld b le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated"><a class="ae kc" href="https://www.appliedaicourse.com/" rel="noopener ugc nofollow" target="_blank">https://www.appliedaicourse.com/</a></p><p id="ee02" class="pw-post-body-paragraph lb lc iq ld b le mn lg lh li mo lk ll lm mp lo lp lq mq ls lt lu mr lw lx ly ij bi translated"><a class="ae kc" rel="noopener" target="_blank" href="/lemmatization-in-natural-language-processing-nlp-and-machine-learning-a4416f69a7b6">https://towardsdatascience . com/lemma tization-in-natural-language-processing-NLP-and-machine-learning-a 4416 f 69 a7b 6</a></p><p id="7aca" class="pw-post-body-paragraph lb lc iq ld b le mn lg lh li mo lk ll lm mp lo lp lq mq ls lt lu mr lw lx ly ij bi translated"><a class="ae kc" href="https://blog.bitext.com/what-is-the-difference-between-stemming-and-lemmatization/" rel="noopener ugc nofollow" target="_blank">https://blog . bitext . com/what-is-difference-thanking-and-lemma tization/</a></p><p id="780e" class="pw-post-body-paragraph lb lc iq ld b le mn lg lh li mo lk ll lm mp lo lp lq mq ls lt lu mr lw lx ly ij bi translated"><a class="ae kc" href="https://sunjackson.github.io/2018/06/19/a3f95a167b4a350a82cb523478e89b16/" rel="noopener ugc nofollow" target="_blank">https://sun Jackson . github . io/2018/06/19/a3f 95 a 167 B4 a 350 a 82 CB 523478 e 89 b 16/</a></p><p id="e55b" class="pw-post-body-paragraph lb lc iq ld b le mn lg lh li mo lk ll lm mp lo lp lq mq ls lt lu mr lw lx ly ij bi translated"><a class="ae kc" href="https://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html" rel="noopener ugc nofollow" target="_blank">https://blog . keras . io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras . html</a></p><p id="a771" class="pw-post-body-paragraph lb lc iq ld b le mn lg lh li mo lk ll lm mp lo lp lq mq ls lt lu mr lw lx ly ij bi translated"><a class="ae kc" href="https://keras.io/examples/nlp/lstm_seq2seq/" rel="noopener ugc nofollow" target="_blank">https://keras.io/examples/nlp/lstm_seq2seq/</a></p><p id="c2d3" class="pw-post-body-paragraph lb lc iq ld b le mn lg lh li mo lk ll lm mp lo lp lq mq ls lt lu mr lw lx ly ij bi translated"><a class="ae kc" href="https://www.aclweb.org/anthology/W15-4640.pdf" rel="noopener ugc nofollow" target="_blank">https://www.aclweb.org/anthology/W15-4640.pdf</a></p><p id="854b" class="pw-post-body-paragraph lb lc iq ld b le mn lg lh li mo lk ll lm mp lo lp lq mq ls lt lu mr lw lx ly ij bi translated">谷歌的聪明回复—<a class="ae kc" href="https://www.kdd.org/kdd2016/papers/files/Paper_1069.pdf" rel="noopener ugc nofollow" target="_blank">https://www.kdd.org/kdd2016/papers/files/Paper_1069.pdf</a></p><p id="8e74" class="pw-post-body-paragraph lb lc iq ld b le mn lg lh li mo lk ll lm mp lo lp lq mq ls lt lu mr lw lx ly ij bi translated"><a class="ae kc" href="https://lena-voita.github.io/nlp_course/seq2seq_and_attention.html#main_content" rel="noopener ugc nofollow" target="_blank">https://Lena-voita . github . io/NLP _ course/seq 2 seq _ and _ attention . html # main _ content</a></p><p id="93fb" class="pw-post-body-paragraph lb lc iq ld b le mn lg lh li mo lk ll lm mp lo lp lq mq ls lt lu mr lw lx ly ij bi translated"><a class="ae kc" href="https://machinelearningmastery.com/beam-search-decoder-natural-language-processing/" rel="noopener ugc nofollow" target="_blank">https://machinelementmastery . com/beam-search-decoder-natural-language-processing/</a></p><p id="5178" class="pw-post-body-paragraph lb lc iq ld b le mn lg lh li mo lk ll lm mp lo lp lq mq ls lt lu mr lw lx ly ij bi translated"><a class="ae kc" href="https://en.wikipedia.org/wiki/Beam_search" rel="noopener ugc nofollow" target="_blank">https://en.wikipedia.org/wiki/Beam_search</a></p></div></div>    
</body>
</html>