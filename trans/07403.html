<html>
<head>
<title>Object localization using pre-trained CNN models such as MobileNet, ResNet &amp; Xception</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用预先训练的CNN模型(如MobileNet、ResNet和Xception)进行目标定位</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/object-localization-using-pre-trained-cnn-models-such-as-mobilenet-resnet-xception-f8a5f6a0228d?source=collection_archive---------4-----------------------#2021-07-06">https://towardsdatascience.com/object-localization-using-pre-trained-cnn-models-such-as-mobilenet-resnet-xception-f8a5f6a0228d?source=collection_archive---------4-----------------------#2021-07-06</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="cd5a" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated"><strong class="ak">目标定位使用不同的预先训练的CNN模型在来自</strong>牛津Pet数据集<strong class="ak">的图像中定位动物面孔</strong></h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/6093b5924995bc0c232845407a89be6f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*SXikiztKwP0TwVsD"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">照片由<a class="ae kv" href="https://unsplash.com/@kevnbhagat?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">凯文·巴加</a>在<a class="ae kv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><p id="f766" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这项工作将向您介绍使用预先训练的CNN和一些额外的有趣的改编，以根据上下文在其中找到最佳执行模型的单个对象定位。</p><p id="4c4c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">先来解释一下本地化。简而言之，在图像中定位一个物体。定位意味着在识别一个对象之后，将该对象引入一个边界框或者精确地引入一个矩形来定位该对象。有单对象定位、多对象定位和语义分割，用于做手段相似但做目的形式不同的事情。</p><p id="6cc1" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在这里，我将坚持单个对象定位，这将识别图像中所需的对象，然后使用CNN对其进行定位。另外，请注意，我将分别使用Mobile Net、ResNet和Xception作为预训练的卷积神经网络，并将为每个网络执行整个分类器和定位器。在此过程中，Union上的交集)将变得熟悉，并将为每个IOU打印出相同的结果，最后，我们将看到哪个预训练网络对我们使用的数据集表现良好。</p><p id="b9da" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">对于这个项目，牛津宠物数据集是合适的:你可以从下面的链接下载。</p><p id="d4bf" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><a class="ae kv" href="http://www.robots.ox.ac.uk/~vgg/data/pets/" rel="noopener ugc nofollow" target="_blank">http://www.robots.ox.ac.uk/~vgg/data/pets/</a></p><p id="8f8c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在让我们分析数据集。数据集包含动物的图像，并且每个图像包含单个动物。我们可以看到这些动物是不同类型的猫和狗。请注意，图像的对齐、位置和结构在每个图像中都是不同的，这可以帮助我们获得更准确的结果。通过上面的链接，我们可以下载数据集和地面实况数据。一旦我们下载了数据，我们将在两个文件中结束:图像和注释。我们可以在annotations文件夹中获得xml注释、类列表和所有内容。一旦我们掌握了所有这些，让我们进入使用不同的预先训练的CNN模型的目标定位。</p><p id="47ea" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在开始之前，我们将引入IOU作为衡量指标。<strong class="ky ir">并集上的交集(IOU) </strong>有助于理解预测的边界框与真实的边界框相差多少。这是理解我们的预测如何进行的一个很好的方法…</p><p id="5ad7" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir"> <em class="ls"> PS:对于所有用预先训练好的网络分别训练后用包围盒打印出来的图片，我们会在每张图片下面打印出IOU</em></strong>…..</p><p id="5171" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">首先，我们需要导入所有必需的库和包。</p><pre class="kg kh ki kj gt lt lu lv lw aw lx bi"><span id="ec64" class="ly lz iq lu b gy ma mb l mc md">from collections import namedtuple</span><span id="1aea" class="ly lz iq lu b gy me mb l mc md">import csv</span><span id="43ee" class="ly lz iq lu b gy me mb l mc md">import tensorflow as tf</span><span id="cf2c" class="ly lz iq lu b gy me mb l mc md">from tensorflow.keras.applications.mobilenet_v2 import MobileNetV2, preprocess_input</span><span id="9d78" class="ly lz iq lu b gy me mb l mc md">from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input</span><span id="ed4d" class="ly lz iq lu b gy me mb l mc md">from tensorflow.keras.applications.xception import Xception, preprocess_input</span><span id="09a1" class="ly lz iq lu b gy me mb l mc md">from tensorflow.keras import backend as K</span><span id="cffa" class="ly lz iq lu b gy me mb l mc md">from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout, Flatten</span><span id="5003" class="ly lz iq lu b gy me mb l mc md">from tensorflow.keras.models import Model, Sequential</span><span id="7054" class="ly lz iq lu b gy me mb l mc md">from tensorflow.keras.preprocessing.image import ImageDataGenerator</span><span id="7515" class="ly lz iq lu b gy me mb l mc md">from tensorflow.keras.utils import to_categorical</span><span id="3a69" class="ly lz iq lu b gy me mb l mc md">import matplotlib.pyplot as plt</span><span id="2fbc" class="ly lz iq lu b gy me mb l mc md">import matplotlib.patches as patches</span><span id="7635" class="ly lz iq lu b gy me mb l mc md">import numpy as np</span><span id="8423" class="ly lz iq lu b gy me mb l mc md">import os</span><span id="82ff" class="ly lz iq lu b gy me mb l mc md"># import the necessary packages</span><span id="2f4d" class="ly lz iq lu b gy me mb l mc md">from collections import namedtuple</span><span id="b065" class="ly lz iq lu b gy me mb l mc md">import numpy as np</span><span id="0956" class="ly lz iq lu b gy me mb l mc md">import cv2</span><span id="5981" class="ly lz iq lu b gy me mb l mc md"># define the `Detection` object for IOU(</span><span id="ad63" class="ly lz iq lu b gy me mb l mc md">Detection = namedtuple("Detection", ["image_path", "gt", "pred"])</span><span id="cade" class="ly lz iq lu b gy me mb l mc md">from PIL import Image, ImageOps</span><span id="82ba" class="ly lz iq lu b gy me mb l mc md"># importing XML parsing library for parsing the data</span><span id="a7cf" class="ly lz iq lu b gy me mb l mc md">import xml.etree.ElementTree as ET</span></pre><p id="d830" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在，让我们导入数据——我通常使用谷歌合作实验室。因此，我将我的google drive安装到colab上(您可以通过任何方式导入数据，只要您方便就可以保存到任何地方)。</p><p id="8e78" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">此外，这里我们可以将<strong class="ky ir">目标大小设置为(224，224)，</strong>，我们将使用Mobile Net、ResNet和Xception作为预训练网络来比较它们中的每一个。</p><pre class="kg kh ki kj gt lt lu lv lw aw lx bi"><span id="ba1c" class="ly lz iq lu b gy ma mb l mc md">from google.colab import drive</span><span id="52a2" class="ly lz iq lu b gy me mb l mc md">drive.mount('/content/drive')</span><span id="9acf" class="ly lz iq lu b gy me mb l mc md">data_images = '/content/drive/MyDrive/AI_dataset_pets/images'</span><span id="139f" class="ly lz iq lu b gy me mb l mc md">data_ClassList = '/content/drive/MyDrive/AI_dataset_pets/annotations/list.txt'</span><span id="4f47" class="ly lz iq lu b gy me mb l mc md">data_xmlAnnotations = '/content/drive/MyDrive/AI_dataset_pets/annotations/xmls'</span><span id="ebf4" class="ly lz iq lu b gy me mb l mc md">TARGET_SIZE = (224, 224)</span></pre><p id="878d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在定义边界框来定位图像中的动物。</p><pre class="kg kh ki kj gt lt lu lv lw aw lx bi"><span id="424a" class="ly lz iq lu b gy ma mb l mc md">#BoundingBox</span><span id="c5f4" class="ly lz iq lu b gy me mb l mc md">Bounding_Box = namedtuple('Bounding_Box', 'xmin ymin xmax ymax')</span><span id="9339" class="ly lz iq lu b gy me mb l mc md"># The following function will read the xml and return the values for xmin, ymin, xmax, ymax for formulating the bounding box</span><span id="7299" class="ly lz iq lu b gy me mb l mc md">def building_bounding_box(path_to_xml_annotation):</span><span id="62e2" class="ly lz iq lu b gy me mb l mc md">tree = ET.parse(path_to_xml_annotation)</span><span id="c41f" class="ly lz iq lu b gy me mb l mc md">root = tree.getroot()</span><span id="e326" class="ly lz iq lu b gy me mb l mc md">path_to_box = './object/bndbox/'</span><span id="5091" class="ly lz iq lu b gy me mb l mc md">xmin = int(root.find(path_to_box + "xmin").text)</span><span id="ab07" class="ly lz iq lu b gy me mb l mc md">ymin = int(root.find(path_to_box + "ymin").text)</span><span id="137a" class="ly lz iq lu b gy me mb l mc md">xmax = int(root.find(path_to_box + "xmax").text)</span><span id="6192" class="ly lz iq lu b gy me mb l mc md">ymax = int(root.find(path_to_box + "ymax").text)</span><span id="cb83" class="ly lz iq lu b gy me mb l mc md">return Bounding_Box(xmin, ymin, xmax, ymax)</span></pre><p id="9f16" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">因此，让我们做<strong class="ky ir">填充</strong>使图像成为一个完美的正方形，并根据填充和缩放对边界框进行必要的修改。</p><p id="90cf" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">在下面的代码中，标准化也已经完成</strong>。</p><pre class="kg kh ki kj gt lt lu lv lw aw lx bi"><span id="683c" class="ly lz iq lu b gy ma mb l mc md">def resize_image_with_bounds(path_to_image, bounding_box=None, target_size=None):</span><span id="24f2" class="ly lz iq lu b gy me mb l mc md">image = Image.open(path_to_image)</span><span id="f4da" class="ly lz iq lu b gy me mb l mc md">width, height = image.size</span><span id="e60b" class="ly lz iq lu b gy me mb l mc md">w_pad = 0</span><span id="d306" class="ly lz iq lu b gy me mb l mc md">h_pad = 0</span><span id="5d36" class="ly lz iq lu b gy me mb l mc md">bonus_h_pad = 0</span><span id="96c9" class="ly lz iq lu b gy me mb l mc md">bonus_w_pad = 0</span><span id="37a7" class="ly lz iq lu b gy me mb l mc md">#the following code helps determining where to pad or is it not necessary for the images we have.</span><span id="46ff" class="ly lz iq lu b gy me mb l mc md"># If the difference between the width and height was odd((height&lt;width)case), we add one pixel on one side</span><span id="96e3" class="ly lz iq lu b gy me mb l mc md"># If the difference between the height and width was odd((height&gt;width)case), then we add one pixel on one side.</span><span id="7611" class="ly lz iq lu b gy me mb l mc md">#if both of these are not the case, then pads=0, no padding is needed, since the image is already a square itself.</span><span id="e629" class="ly lz iq lu b gy me mb l mc md">if width &gt; height:</span><span id="d673" class="ly lz iq lu b gy me mb l mc md">pix_diff = (width - height)</span><span id="3968" class="ly lz iq lu b gy me mb l mc md">h_pad = pix_diff // 2</span><span id="fe30" class="ly lz iq lu b gy me mb l mc md">bonus_h_pad = pix_diff % 2</span><span id="0e3a" class="ly lz iq lu b gy me mb l mc md">elif height &gt; width:</span><span id="d800" class="ly lz iq lu b gy me mb l mc md">pix_diff = (height - width)</span><span id="c8de" class="ly lz iq lu b gy me mb l mc md">w_pad = pix_diff // 2</span><span id="d843" class="ly lz iq lu b gy me mb l mc md">bonus_w_pad = pix_diff % 2</span><span id="f7a2" class="ly lz iq lu b gy me mb l mc md"># When we pad the image to square, we need to adjust all the bounding box values by the amounts we added on the left or top.</span><span id="6331" class="ly lz iq lu b gy me mb l mc md">#The "bonus" pads are always done on the bottom and right so we can ignore them in terms of the box.</span><span id="f2a5" class="ly lz iq lu b gy me mb l mc md">image = ImageOps.expand(image, (w_pad, h_pad, w_pad+bonus_w_pad, h_pad+bonus_h_pad))</span><span id="00d6" class="ly lz iq lu b gy me mb l mc md">if bounding_box is not None:</span><span id="1613" class="ly lz iq lu b gy me mb l mc md">new_xmin = bounding_box.xmin + w_pad</span><span id="8cef" class="ly lz iq lu b gy me mb l mc md">new_xmax = bounding_box.xmax + w_pad</span><span id="a875" class="ly lz iq lu b gy me mb l mc md">new_ymin = bounding_box.ymin + h_pad</span><span id="9e48" class="ly lz iq lu b gy me mb l mc md">new_ymax = bounding_box.ymax + h_pad</span><span id="d0ce" class="ly lz iq lu b gy me mb l mc md"># We need to also apply the scalr to the bounding box which we used in resizing the image</span><span id="b8c4" class="ly lz iq lu b gy me mb l mc md">if target_size is not None:</span><span id="7258" class="ly lz iq lu b gy me mb l mc md"># So, width and height have changed due to the padding resize.</span><span id="a404" class="ly lz iq lu b gy me mb l mc md">width, height = image.size</span><span id="10c0" class="ly lz iq lu b gy me mb l mc md">image = image.resize(target_size)</span><span id="78fe" class="ly lz iq lu b gy me mb l mc md">width_scale = target_size[0] / width</span><span id="7c0b" class="ly lz iq lu b gy me mb l mc md">height_scale = target_size[1] / height</span><span id="f831" class="ly lz iq lu b gy me mb l mc md">if bounding_box is not None:</span><span id="864c" class="ly lz iq lu b gy me mb l mc md">new_xmin = new_xmin * width_scale</span><span id="9f81" class="ly lz iq lu b gy me mb l mc md">new_xmax = new_xmax * width_scale</span><span id="ee93" class="ly lz iq lu b gy me mb l mc md">new_ymin = new_ymin * height_scale</span><span id="bd73" class="ly lz iq lu b gy me mb l mc md">new_ymax = new_ymax * height_scale</span><span id="8518" class="ly lz iq lu b gy me mb l mc md">image_data = np.array(image.getdata()).reshape(image.size[0], image.size[1], 3)</span><span id="73e9" class="ly lz iq lu b gy me mb l mc md"># The image data is a 3D array such that 3 channels ,RGB of target_size.(RGB values are 0-255)</span><span id="83e3" class="ly lz iq lu b gy me mb l mc md">if bounding_box is None:</span><span id="ada3" class="ly lz iq lu b gy me mb l mc md">return image_data, None</span><span id="78b2" class="ly lz iq lu b gy me mb l mc md">return (image_data, Bounding_Box(new_xmin, new_ymin, new_xmax, new_ymax))</span></pre><p id="e99b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">因此，根据输入数据，我们已经重塑了图像和边界框。</p><pre class="kg kh ki kj gt lt lu lv lw aw lx bi"><span id="e035" class="ly lz iq lu b gy ma mb l mc md">def setting_sample_from_name(sample_name):</span><span id="1f46" class="ly lz iq lu b gy me mb l mc md">path_to_image = os.path.join(data_images, sample_name + '.jpg')</span><span id="294d" class="ly lz iq lu b gy me mb l mc md">path_to_xml = os.path.join(data_xmlAnnotations, sample_name + '.xml')</span><span id="52f8" class="ly lz iq lu b gy me mb l mc md">original_bounding_box = get_bounding_box(path_to_xml)</span><span id="3f9c" class="ly lz iq lu b gy me mb l mc md">image_data, bounding_box = resize_image_with_bounds(path_to_image, original_bounding_box, TARGET_SIZE)</span><span id="d18c" class="ly lz iq lu b gy me mb l mc md">return (image_data, bounding_box)</span></pre><p id="b208" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">注意<strong class="ky ir">黄色框</strong>是<strong class="ky ir">预测边界框，</strong>和<strong class="ky ir">蓝色框</strong>是<strong class="ky ir">真实边界框</strong>是<strong class="ky ir">真实边界框。</strong></p><p id="0146" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在让我们编写函数来绘制图像数据和边界框，并找到两个框在并集IOU 上的<strong class="ky ir">交集。它可以计算为IOU =重叠面积/并集面积。</strong></p><p id="90e7" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><em class="ls">代码在下面的函数‘plot _ with _ box’中。</em></p><pre class="kg kh ki kj gt lt lu lv lw aw lx bi"><span id="8520" class="ly lz iq lu b gy ma mb l mc md">def plot_with_box(image_data, bounding_box, compare_box=None):</span><span id="1f94" class="ly lz iq lu b gy me mb l mc md">fig,ax = plt.subplots(1)</span><span id="f22c" class="ly lz iq lu b gy me mb l mc md">ax.imshow(image_data)</span><span id="15a4" class="ly lz iq lu b gy me mb l mc md"># Creating a Rectangle patch for the changed one</span><span id="0459" class="ly lz iq lu b gy me mb l mc md">boxA = patches.Rectangle((bounding_box.xmin, bounding_box.ymin),</span><span id="86cb" class="ly lz iq lu b gy me mb l mc md">bounding_box.xmax - bounding_box.xmin,</span><span id="cb21" class="ly lz iq lu b gy me mb l mc md">bounding_box.ymax - bounding_box.ymin,</span><span id="ef12" class="ly lz iq lu b gy me mb l mc md">linewidth=3, edgecolor='y', facecolor='none')</span><span id="79ac" class="ly lz iq lu b gy me mb l mc md"># Add the patch to the Axes</span><span id="9353" class="ly lz iq lu b gy me mb l mc md">ax.add_patch(boxA)</span><span id="0286" class="ly lz iq lu b gy me mb l mc md">#Creating another Rectangular patch for the real one</span><span id="e50a" class="ly lz iq lu b gy me mb l mc md">if compare_box is not None:</span><span id="7144" class="ly lz iq lu b gy me mb l mc md">boxB = patches.Rectangle((compare_box.xmin, compare_box.ymin),</span><span id="723e" class="ly lz iq lu b gy me mb l mc md">compare_box.xmax - compare_box.xmin,</span><span id="aa6d" class="ly lz iq lu b gy me mb l mc md">compare_box.ymax - compare_box.ymin,</span><span id="859a" class="ly lz iq lu b gy me mb l mc md">linewidth=2, edgecolor='b', facecolor='none')</span><span id="e3dd" class="ly lz iq lu b gy me mb l mc md"># Add the patch to the Axes</span><span id="9cbf" class="ly lz iq lu b gy me mb l mc md">ax.add_patch(boxB)</span><span id="4ccd" class="ly lz iq lu b gy me mb l mc md">#FOR FINDING INTERSECTION OVER UNION</span><span id="bc76" class="ly lz iq lu b gy me mb l mc md">xA = max(bounding_box.xmin, compare_box.xmin)</span><span id="3866" class="ly lz iq lu b gy me mb l mc md">yA = max(bounding_box.ymin, compare_box.ymin)</span><span id="ee67" class="ly lz iq lu b gy me mb l mc md">xB = min(bounding_box.xmax, compare_box.xmax)</span><span id="4982" class="ly lz iq lu b gy me mb l mc md">yB = max(bounding_box.ymax, compare_box.ymax)</span><span id="194f" class="ly lz iq lu b gy me mb l mc md">interArea = max(0, xB - xA + 1) * max(0, yB - yA + 1)</span><span id="bee1" class="ly lz iq lu b gy me mb l mc md">boxAArea = (bounding_box.xmax - bounding_box.xmin + 1) * (bounding_box.ymax - bounding_box.ymin + 1)</span><span id="7b98" class="ly lz iq lu b gy me mb l mc md">boxBArea = (compare_box.xmax - compare_box.xmin + 1) * (compare_box.ymax - compare_box.ymin + 1)</span><span id="9b43" class="ly lz iq lu b gy me mb l mc md">iou =interArea/float(boxAArea+boxBArea-interArea)<br/>#By intersection of union I mean intersection over union(IOU) #itself</span><span id="4d96" class="ly lz iq lu b gy me mb l mc md">print('intersection of union =',iou)</span><span id="b785" class="ly lz iq lu b gy me mb l mc md">plt.show()</span></pre><p id="a7d1" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir"> <em class="ls">现在，让我们绘制一个随机图像，看看发生了什么，并检查预测边界框的工作情况。</em> </strong></p><pre class="kg kh ki kj gt lt lu lv lw aw lx bi"><span id="7495" class="ly lz iq lu b gy ma mb l mc md">sample_name = 'Abyssinian_10'</span><span id="1c52" class="ly lz iq lu b gy me mb l mc md">image, bounding_box = setting_sample_from_name(sample_name)</span><span id="61a6" class="ly lz iq lu b gy me mb l mc md">plot_with_box(image, bounding_box)</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mf"><img src="../Images/f0558e09002764280b496552b297238d.png" data-original-src="https://miro.medium.com/v2/resize:fit:514/format:webp/1*0vZ5put71q_zQ-C9sBH9nA.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="367f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这样我们就有了物体的边界框。</p><p id="ba59" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在，让我们处理所有的数据。此外，让我们删除所有没有注释的图像。并将其转换为Numpy数组。</p><pre class="kg kh ki kj gt lt lu lv lw aw lx bi"><span id="74b3" class="ly lz iq lu b gy ma mb l mc md">data_pros = []</span><span id="d4ab" class="ly lz iq lu b gy me mb l mc md">with open(data_ClassList) as csv_list_file:</span><span id="ffc6" class="ly lz iq lu b gy me mb l mc md">csv_reader = csv.reader(csv_list_file, delimiter=' ')</span><span id="c1b5" class="ly lz iq lu b gy me mb l mc md">for row in csv_reader:</span><span id="efdb" class="ly lz iq lu b gy me mb l mc md">if row[0].startswith('#'): continue</span><span id="1546" class="ly lz iq lu b gy me mb l mc md"># Unpack for readability</span><span id="b196" class="ly lz iq lu b gy me mb l mc md">sample_name, class_id, species, breed_id = row</span><span id="d09e" class="ly lz iq lu b gy me mb l mc md"># Not every image has a bounding box, some files are missing.So, lets ignore those by the following lines</span><span id="c76b" class="ly lz iq lu b gy me mb l mc md">try:</span><span id="95a4" class="ly lz iq lu b gy me mb l mc md">image, bounding_box = setting_sample_from_name(sample_name)</span><span id="3df0" class="ly lz iq lu b gy me mb l mc md">except FileNotFoundError:</span><span id="baec" class="ly lz iq lu b gy me mb l mc md"># This actually happens quite a lot, as you can see in the output.</span><span id="ea48" class="ly lz iq lu b gy me mb l mc md"># we end up with 7349 samples.</span><span id="3581" class="ly lz iq lu b gy me mb l mc md">print(f'cannot find annotations for {sample_name}: so skipped it')</span><span id="1209" class="ly lz iq lu b gy me mb l mc md">continue</span><span id="ba38" class="ly lz iq lu b gy me mb l mc md"># cat = 0 and dog = 1.</span><span id="4b4c" class="ly lz iq lu b gy me mb l mc md">data_tuple = (image, int(species) - 1, bounding_box)</span><span id="6aa9" class="ly lz iq lu b gy me mb l mc md">data_pros.append(data_tuple)</span><span id="eda6" class="ly lz iq lu b gy me mb l mc md">print(f'Processed {len(data_pros)} samples')</span><span id="2e68" class="ly lz iq lu b gy me mb l mc md">data_pros = np.array(data_pros)</span></pre><p id="acc6" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在，一旦我们完成了这个，让我们用6张随机的图片来测试整体</p><pre class="kg kh ki kj gt lt lu lv lw aw lx bi"><span id="b948" class="ly lz iq lu b gy ma mb l mc md">#for checking lets print 6 of them</span><span id="fa1f" class="ly lz iq lu b gy me mb l mc md">for _ in range(6):</span><span id="194d" class="ly lz iq lu b gy me mb l mc md">i = np.random.randint(len(data_pros))</span><span id="978b" class="ly lz iq lu b gy me mb l mc md">image, species, bounding_box = data_pros[i]</span><span id="f2bd" class="ly lz iq lu b gy me mb l mc md">if species == 0:</span><span id="12a2" class="ly lz iq lu b gy me mb l mc md">print(i, "it is cat")</span><span id="e419" class="ly lz iq lu b gy me mb l mc md">elif species == 1:</span><span id="c0bd" class="ly lz iq lu b gy me mb l mc md">print(i, "it is dog")</span><span id="a014" class="ly lz iq lu b gy me mb l mc md">else:</span><span id="a925" class="ly lz iq lu b gy me mb l mc md">print("ERROR FOUND: This is of invalid species type")</span><span id="86b0" class="ly lz iq lu b gy me mb l mc md">plot_with_box(image, bounding_box)</span></pre><p id="84c1" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">结果是这样的。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mg"><img src="../Images/0ecc43cb28c0ce4233bc4691c5f46d1f.png" data-original-src="https://miro.medium.com/v2/resize:fit:936/format:webp/1*aUchrcrla7HzWBAeZzWHxw.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="ed35" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir"> <em class="ls">分割给定数据进行包围盒预测。</em> </strong></p><pre class="kg kh ki kj gt lt lu lv lw aw lx bi"><span id="51c9" class="ly lz iq lu b gy ma mb l mc md">x_train = []</span><span id="d9a4" class="ly lz iq lu b gy me mb l mc md">y_class_train = []</span><span id="72ce" class="ly lz iq lu b gy me mb l mc md">y_box_train = []</span><span id="06f7" class="ly lz iq lu b gy me mb l mc md">x_validation = []</span><span id="f1dc" class="ly lz iq lu b gy me mb l mc md">y_class_validation = []</span><span id="14c2" class="ly lz iq lu b gy me mb l mc md">y_box_validation = []</span><span id="ad40" class="ly lz iq lu b gy me mb l mc md">validation_split = 0.2</span><span id="a73f" class="ly lz iq lu b gy me mb l mc md">for image, species, bounding_box in processed_data:</span><span id="ced0" class="ly lz iq lu b gy me mb l mc md">if np.random.random() &gt; validation_split:</span><span id="fe68" class="ly lz iq lu b gy me mb l mc md">x_train.append(preprocess_input(image))</span><span id="05c4" class="ly lz iq lu b gy me mb l mc md">y_class_train.append(species)</span><span id="e0b9" class="ly lz iq lu b gy me mb l mc md">y_box_train.append(bounding_box)</span><span id="ea69" class="ly lz iq lu b gy me mb l mc md">else:</span><span id="a422" class="ly lz iq lu b gy me mb l mc md">x_validation.append(preprocess_input(image))</span><span id="3c79" class="ly lz iq lu b gy me mb l mc md">y_class_validation.append(species)</span><span id="1839" class="ly lz iq lu b gy me mb l mc md">y_box_validation.append(bounding_box)</span><span id="82d1" class="ly lz iq lu b gy me mb l mc md">x_train = np.array(x_train)</span><span id="45ce" class="ly lz iq lu b gy me mb l mc md">y_class_train = np.array(y_class_train)</span><span id="2f45" class="ly lz iq lu b gy me mb l mc md">y_box_train = np.array(y_box_train)</span><span id="3b70" class="ly lz iq lu b gy me mb l mc md">x_validation = np.array(x_validation)</span><span id="3a78" class="ly lz iq lu b gy me mb l mc md">y_class_validation = np.array(y_class_validation)</span><span id="e8dc" class="ly lz iq lu b gy me mb l mc md">y_box_validation = np.array(y_box_validation)</span></pre><p id="277b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们将使用一些使用迁移学习的预训练模型。</p><p id="d68a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">首先，我使用移动网络，我将同时执行分类器和定位器。</p><pre class="kg kh ki kj gt lt lu lv lw aw lx bi"><span id="6411" class="ly lz iq lu b gy ma mb l mc md">base_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=(TARGET_SIZE[0], TARGET_SIZE[1], 3))</span><span id="77c2" class="ly lz iq lu b gy me mb l mc md">chopped_mobilenet = Model(inputs=[base_model.input], outputs=[base_model.layers[90].output])</span><span id="a624" class="ly lz iq lu b gy me mb l mc md">classification_output = GlobalAveragePooling2D()(chopped_mobilenet.output)</span><span id="09e1" class="ly lz iq lu b gy me mb l mc md">classification_output = Dense(units=1, activation='sigmoid')(classification_output)</span><span id="ef66" class="ly lz iq lu b gy me mb l mc md">localization_output = Flatten()(chopped_mobilenet.output)</span><span id="d40f" class="ly lz iq lu b gy me mb l mc md">localization_output = Dense(units=4, activation='relu')(localization_output)</span><span id="8bd6" class="ly lz iq lu b gy me mb l mc md">model = Model(inputs=[chopped_mobilenet.input], outputs=[classification_output, localization_output])</span><span id="4ff0" class="ly lz iq lu b gy me mb l mc md">model.summary()</span></pre><p id="9827" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">一旦打印出以上内容，我们将获得使用MobileNet构建模型的详细摘要。</p><p id="556d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在，绘制每个时期模型的精确度和损失。</p><pre class="kg kh ki kj gt lt lu lv lw aw lx bi"><span id="6880" class="ly lz iq lu b gy ma mb l mc md">plot_training_history(history1, model)</span></pre><p id="7d59" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这些图是:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mh"><img src="../Images/65c90e00ce81e5b502ce9e5b3a73f034.png" data-original-src="https://miro.medium.com/v2/resize:fit:826/format:webp/1*T22Qs0Q8qqfipaNH02WG3g.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><h2 id="f630" class="ly lz iq bd mi mj mk dn ml mm mn dp mo lf mp mq mr lj ms mt mu ln mv mw mx my bi translated">真实框是蓝色的，预测框是黄色的</h2><pre class="kg kh ki kj gt lt lu lv lw aw lx bi"><span id="aefd" class="ly lz iq lu b gy ma mb l mc md">for _ in range(18):</span><span id="a3ea" class="ly lz iq lu b gy me mb l mc md">i = np.random.randint(len(processed_data))</span><span id="6959" class="ly lz iq lu b gy me mb l mc md">img, species, true_bounding_box = processed_data[i]</span><span id="bd9e" class="ly lz iq lu b gy me mb l mc md">pred = model.predict(np.array([preprocess_input(img)]))</span><span id="9f41" class="ly lz iq lu b gy me mb l mc md">if pred[0][0] &lt; .5:</span><span id="0ce7" class="ly lz iq lu b gy me mb l mc md">print("it is a Cat")</span><span id="c5df" class="ly lz iq lu b gy me mb l mc md">else:</span><span id="9d92" class="ly lz iq lu b gy me mb l mc md">print("it is a dog")</span><span id="0d48" class="ly lz iq lu b gy me mb l mc md">plot_with_box(img, Bounding_Box(*pred[1][0]), true_bounding_box)</span></pre><p id="eb95" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">结果是:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mz"><img src="../Images/69e441538a921a5587ad92da66f41da9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1112/format:webp/1*3OQXLHCxDb3D1CmY8RUdnA.png"/></div></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi na"><img src="../Images/a3526b1239ba000ff1269af7085700a3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1118/format:webp/1*A_xXVqUwO8a_f6UclpRIMQ.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者提供的图片</p></figure><p id="09f4" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">请注意，这里所有的图像都使用MobileNet检测为猫。随机抽取一些样本进行检查，以了解模型的完善程度:</p><pre class="kg kh ki kj gt lt lu lv lw aw lx bi"><span id="a657" class="ly lz iq lu b gy ma mb l mc md">some_random_samples = ['Abyssinian_174','american_bulldog_59']</span><span id="cd5d" class="ly lz iq lu b gy me mb l mc md">for sample_name in some_random_samples:</span><span id="dff7" class="ly lz iq lu b gy me mb l mc md">path_to_image = os.path.join(data_images, sample_name + '.jpg')</span><span id="1a6d" class="ly lz iq lu b gy me mb l mc md">print(path_to_image)</span><span id="15ac" class="ly lz iq lu b gy me mb l mc md">img, _ = resize_image_with_bounds(path_to_image, target_size=TARGET_SIZE)</span><span id="9e97" class="ly lz iq lu b gy me mb l mc md">pred = model.predict(np.array([preprocess_input(img)]))</span><span id="4eb7" class="ly lz iq lu b gy me mb l mc md">if pred[0][0] &lt; .5:</span><span id="6f02" class="ly lz iq lu b gy me mb l mc md">print("Yes,Its a Cat")</span><span id="271a" class="ly lz iq lu b gy me mb l mc md">else:</span><span id="30f1" class="ly lz iq lu b gy me mb l mc md">print("Yes Its a dog")</span><span id="d7cb" class="ly lz iq lu b gy me mb l mc md">plot_with_box(img, Bounding_Box(*pred[1][0]),true_bounding_box)</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nb"><img src="../Images/26eea6bd2f49e11b6bc0917002f17879.png" data-original-src="https://miro.medium.com/v2/resize:fit:892/format:webp/1*6mSRJPy8ZtBidkjTry1I3A.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="a563" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">使用MobileNet时的IOU值还不错…但是在有一些小匿名的图片中，IOU值太小了…..</p><p id="f0cc" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">让我们看看它是如何使用<strong class="ky ir"> ResNet和Xception的。</strong></p><h1 id="db17" class="nc lz iq bd mi nd ne nf ml ng nh ni mo jw nj jx mr jz nk ka mu kc nl kd mx nm bi translated">现在，使用ResNet预训练网络进行同样的尝试</h1><pre class="kg kh ki kj gt lt lu lv lw aw lx bi"><span id="3cb7" class="ly lz iq lu b gy ma mb l mc md">base_model1 = ResNet50(weights='imagenet', include_top=False, input_shape=(TARGET_SIZE[0], TARGET_SIZE[1], 3))</span><span id="64e4" class="ly lz iq lu b gy me mb l mc md">chopped_resnet1 = Model(inputs=[base_model1.input], outputs=[base_model1.layers[90].output])</span><span id="50f4" class="ly lz iq lu b gy me mb l mc md">classification_output1 = GlobalAveragePooling2D()(chopped_resnet1.output)</span><span id="d86b" class="ly lz iq lu b gy me mb l mc md">classification_output1 = Dense(units=1, activation='sigmoid')(classification_output1)</span><span id="56fb" class="ly lz iq lu b gy me mb l mc md">localization_output1 = Flatten()(chopped_resnet1.output)</span><span id="2f0f" class="ly lz iq lu b gy me mb l mc md">localization_output1 = Dense(units=4, activation='relu')(localization_output1)</span><span id="e10c" class="ly lz iq lu b gy me mb l mc md">model1 = Model(inputs=[chopped_resnet1.input], outputs=[classification_output1, localization_output1])</span><span id="93e5" class="ly lz iq lu b gy me mb l mc md">model1.summary()</span></pre><p id="a778" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">请浏览一下这个摘要，一旦你把它打印出来。这将有助于你对网络有一个清晰的了解。</p><p id="acf1" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在，我们将继续编译和拟合用Resnet制作的模型。</p><pre class="kg kh ki kj gt lt lu lv lw aw lx bi"><span id="9633" class="ly lz iq lu b gy ma mb l mc md">model1.compile(optimizer='adam', metrics=['accuracy'],loss=['binary_crossentropy', 'mse'],loss_weights=[800, 1]  )</span><span id="8e59" class="ly lz iq lu b gy me mb l mc md">#lets run it through 10 epochs</span><span id="5868" class="ly lz iq lu b gy me mb l mc md">history2=model1.fit(x_train, [y_class_train, y_box_train], validation_data=(x_validation, [y_class_validation, y_box_validation]),epochs=10,verbose=True)</span><span id="4090" class="ly lz iq lu b gy me mb l mc md">history2</span></pre><p id="c2cc" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我不包括每个时期的总结和验证准确性和损失。一旦你实现了它们，你就可以看到它们，下面给出了每个时期的准确度和损失图。</p><pre class="kg kh ki kj gt lt lu lv lw aw lx bi"><span id="a33e" class="ly lz iq lu b gy ma mb l mc md">def plot_training_history(history, model):</span><span id="66fb" class="ly lz iq lu b gy me mb l mc md">plt.plot(history.history['dense_3_accuracy'])</span><span id="ce38" class="ly lz iq lu b gy me mb l mc md">plt.plot(history.history['val_dense_3_accuracy'])</span><span id="8da7" class="ly lz iq lu b gy me mb l mc md">plt.ylabel('accuracy')</span><span id="836f" class="ly lz iq lu b gy me mb l mc md">plt.xlabel('epoch')</span><span id="e851" class="ly lz iq lu b gy me mb l mc md">plt.legend(['training', 'validation'], loc='best')</span><span id="14b4" class="ly lz iq lu b gy me mb l mc md">plt.show()</span><span id="2d5b" class="ly lz iq lu b gy me mb l mc md">plt.plot(history.history['dense_3_loss'])</span><span id="6eff" class="ly lz iq lu b gy me mb l mc md">plt.plot(history.history['val_dense_3_loss'])</span><span id="8db4" class="ly lz iq lu b gy me mb l mc md">plt.title('model loss')</span><span id="a21c" class="ly lz iq lu b gy me mb l mc md">plt.ylabel('loss')</span><span id="3a2d" class="ly lz iq lu b gy me mb l mc md">plt.xlabel('epoch')</span><span id="4282" class="ly lz iq lu b gy me mb l mc md">plt.legend(['training', 'validation'], loc='best')</span><span id="c759" class="ly lz iq lu b gy me mb l mc md">plt.show()</span><span id="f424" class="ly lz iq lu b gy me mb l mc md">plot_training_history(history2, model1)</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/78100158561dcc90c1f3e22f6346c31b.png" data-original-src="https://miro.medium.com/v2/resize:fit:704/format:webp/1*iFJWmJIDoDItovLvoRqvxQ.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="319b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">让我们打印更改后的图像，并使用ResNet进行训练，并打印两个框的IOU(交集/并集),看看我们的预测有多好。</p><pre class="kg kh ki kj gt lt lu lv lw aw lx bi"><span id="9d76" class="ly lz iq lu b gy ma mb l mc md">for _ in range(3):</span><span id="e10e" class="ly lz iq lu b gy me mb l mc md">i = np.random.randint(len(processed_data))</span><span id="3cbe" class="ly lz iq lu b gy me mb l mc md">img, species, true_bounding_box = processed_data[i]</span><span id="1358" class="ly lz iq lu b gy me mb l mc md">pred = model1.predict(np.array([preprocess_input(img)]))</span><span id="9596" class="ly lz iq lu b gy me mb l mc md">if pred[0][0] &lt; .5:</span><span id="5b64" class="ly lz iq lu b gy me mb l mc md">print("it is a Cat by ResNet")</span><span id="4679" class="ly lz iq lu b gy me mb l mc md">else:</span><span id="fc04" class="ly lz iq lu b gy me mb l mc md">print("it is a dog by ResNet")</span><span id="5450" class="ly lz iq lu b gy me mb l mc md">plot_with_box(img, Bounding_Box(*pred[1][0]), true_bounding_box)</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi no"><img src="../Images/74c7b18d7bf11648d374612eccdddb3f.png" data-original-src="https://miro.medium.com/v2/resize:fit:472/format:webp/1*wNAS4a-iBipRGYdYeF3tMg.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="6b90" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这是检测一只狗作为一只猫，但IOU值相当不错！</p><p id="dbce" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在让我们试试Xception预训练网络。下面给出了实现它的代码。</p><pre class="kg kh ki kj gt lt lu lv lw aw lx bi"><span id="5f5b" class="ly lz iq lu b gy ma mb l mc md">base_model2 = Xception(weights='imagenet', include_top=False, input_shape=(TARGET_SIZE[0], TARGET_SIZE[1], 3))</span><span id="b81c" class="ly lz iq lu b gy me mb l mc md">chopped_Xception = Model(inputs=[base_model2.input], outputs=[base_model2.layers[90].output])</span><span id="d76f" class="ly lz iq lu b gy me mb l mc md">classification_output2 = GlobalAveragePooling2D()(chopped_Xception.output)</span><span id="2140" class="ly lz iq lu b gy me mb l mc md">classification_output2 = Dense(units=1, activation='sigmoid')(classification_output2)</span><span id="09b1" class="ly lz iq lu b gy me mb l mc md">localization_output2 = Flatten()(chopped_Xception.output)</span><span id="14cc" class="ly lz iq lu b gy me mb l mc md">localization_output2 = Dense(units=4, activation='relu')(localization_output2)</span><span id="6ca2" class="ly lz iq lu b gy me mb l mc md">model2 = Model(inputs=[chopped_Xception.input], outputs=[classification_output2, localization_output2])</span><span id="9fe9" class="ly lz iq lu b gy me mb l mc md">model2.summary()</span></pre><p id="2e00" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">通过异常网络编译和拟合模型:</p><pre class="kg kh ki kj gt lt lu lv lw aw lx bi"><span id="f428" class="ly lz iq lu b gy ma mb l mc md">model2.compile(optimizer='adam', metrics=['accuracy'],loss=['binary_crossentropy', 'mse'],loss_weights=[800, 1]  )</span><span id="c9c9" class="ly lz iq lu b gy me mb l mc md">#lets run it through 10 epochs</span><span id="d5ab" class="ly lz iq lu b gy me mb l mc md">history3=model2.fit(x_train, [y_class_train, y_box_train], validation_data=(x_validation, [y_class_validation, y_box_validation]),epochs=10,verbose=True)</span><span id="4c18" class="ly lz iq lu b gy me mb l mc md">history3</span></pre><p id="d00a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">画出它的精确度和损耗。</p><pre class="kg kh ki kj gt lt lu lv lw aw lx bi"><span id="bf01" class="ly lz iq lu b gy ma mb l mc md">def plot_training_history(history, model):</span><span id="5a10" class="ly lz iq lu b gy me mb l mc md">plt.plot(history.history['dense_9_accuracy'])</span><span id="4411" class="ly lz iq lu b gy me mb l mc md">plt.plot(history.history['val_dense_9_accuracy'])</span><span id="eb6a" class="ly lz iq lu b gy me mb l mc md">plt.ylabel('accuracy')</span><span id="a35d" class="ly lz iq lu b gy me mb l mc md">plt.xlabel('epoch')</span><span id="a91d" class="ly lz iq lu b gy me mb l mc md">plt.legend(['training', 'validation'], loc='best')</span><span id="f6c5" class="ly lz iq lu b gy me mb l mc md">plt.show()</span><span id="9d74" class="ly lz iq lu b gy me mb l mc md">plt.plot(history.history['dense_9_loss'])</span><span id="9429" class="ly lz iq lu b gy me mb l mc md">plt.plot(history.history['val_dense_9_loss'])</span><span id="ad01" class="ly lz iq lu b gy me mb l mc md">plt.title('model loss')</span><span id="e177" class="ly lz iq lu b gy me mb l mc md">plt.ylabel('loss')</span><span id="d0d7" class="ly lz iq lu b gy me mb l mc md">plt.xlabel('epoch')</span><span id="17ba" class="ly lz iq lu b gy me mb l mc md">plt.legend(['training', 'validation'], loc='best')</span><span id="783a" class="ly lz iq lu b gy me mb l mc md">plt.show()</span><span id="e63c" class="ly lz iq lu b gy me mb l mc md">plot_training_history(history3, model2)</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi np"><img src="../Images/e8145c37807e253b3252a60647060afc.png" data-original-src="https://miro.medium.com/v2/resize:fit:772/0*hyWJSpzLmHUBOLqd"/></div></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/f5162db037366534835c221ab1dc82de.png" data-original-src="https://miro.medium.com/v2/resize:fit:790/0*rSHdXyQ76NnMWJhm"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="a810" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">让我们使用<strong class="ky ir">异常</strong>打印更改和训练后的图像，并打印两个箱子的<strong class="ky ir">借据</strong>。</p><p id="f0cd" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">让我们看看使用Xception处理的图像:</p><pre class="kg kh ki kj gt lt lu lv lw aw lx bi"><span id="69e6" class="ly lz iq lu b gy ma mb l mc md">for _ in range(6):</span><span id="ef54" class="ly lz iq lu b gy me mb l mc md">i = np.random.randint(len(processed_data))</span><span id="f142" class="ly lz iq lu b gy me mb l mc md">img, species, true_bounding_box = processed_data[i]</span><span id="9836" class="ly lz iq lu b gy me mb l mc md">pred = model2.predict(np.array([preprocess_input(img)]))</span><span id="646a" class="ly lz iq lu b gy me mb l mc md">if pred[0][0] &lt; .5:</span><span id="3638" class="ly lz iq lu b gy me mb l mc md">print("it is a Cat by Xception")</span><span id="a666" class="ly lz iq lu b gy me mb l mc md">else:</span><span id="512b" class="ly lz iq lu b gy me mb l mc md">print("it is a dog by Xception")</span><span id="d681" class="ly lz iq lu b gy me mb l mc md">plot_with_box(img, BoundingBox(*pred[1][0]), true_bounding_box)</span></pre><p id="872d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">结果:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/60375e59362d97ebc54aa20d96b800cb.png" data-original-src="https://miro.medium.com/v2/resize:fit:934/format:webp/1*N77KjxGp7IjWmM675I7VUw.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="c206" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在，我们将使用少量随机样本来测试模型。</p><pre class="kg kh ki kj gt lt lu lv lw aw lx bi"><span id="c84e" class="ly lz iq lu b gy ma mb l mc md">#testing with rand</span><span id="be40" class="ly lz iq lu b gy me mb l mc md">Some_Random_samples = ['Abyssinian_174','american_bulldog_59']</span><span id="7183" class="ly lz iq lu b gy me mb l mc md">for sample_name in Some_Random_samples:</span><span id="f0e4" class="ly lz iq lu b gy me mb l mc md">path_to_image = os.path.join(data_images, sample_name + '.jpg')</span><span id="088e" class="ly lz iq lu b gy me mb l mc md">print(path_to_image)</span><span id="c506" class="ly lz iq lu b gy me mb l mc md">img, _ = resize_image_with_bounds(path_to_image, target_size=TARGET_SIZE)</span><span id="73c8" class="ly lz iq lu b gy me mb l mc md">pred = model2.predict(np.array([preprocess_input(img)]))</span><span id="5449" class="ly lz iq lu b gy me mb l mc md">if pred[0][0] &lt; .5:</span><span id="3e69" class="ly lz iq lu b gy me mb l mc md">print("Yes,Its a Cat by Xception")</span><span id="9113" class="ly lz iq lu b gy me mb l mc md">else:</span><span id="5665" class="ly lz iq lu b gy me mb l mc md">print("Yes Its a dog by Xception")</span><span id="7d1d" class="ly lz iq lu b gy me mb l mc md">plot_with_box(img, Bounding_Box(*pred[1][0]),true_bounding_box)</span></pre><p id="3eb3" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">结果:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/656006c8d5ffed6cc97c57ba5a10ce6b.png" data-original-src="https://miro.medium.com/v2/resize:fit:734/format:webp/1*4MUZQREyB-xKB4x05uzvKQ.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="f14e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">Xception表现很好，给出了相当准确的预测。</p><h1 id="a843" class="nc lz iq bd mi nd ne nf ml ng nh ni mo jw nj jx mr jz nk ka mu kc nl kd mx nm bi translated">在尝试使用Xception、MobileNet和Resnet时，IOU值看起来不错。</h1><p id="4868" class="pw-post-body-paragraph kw kx iq ky b kz nt jr lb lc nu ju le lf nv lh li lj nw ll lm ln nx lp lq lr ij bi translated">对于<strong class="ky ir"> MobilNet </strong> = 0.8125，获得最终层的最终验证精度</p><p id="71b6" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">对于<strong class="ky ir"> ResNet </strong> = 0.7969，获得最终层的最终验证精度</p><p id="ab65" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">对于<strong class="ky ir">例外</strong> = 0.8438，获得最终层的最终验证精度</p><p id="e4ce" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">(当您在每个模型的训练中获得每个时期的结果时，您可以获得准确的最终验证准确性)。</p><p id="33ec" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">所有预先训练好的网络，如MobileNet、ResNet和Xception，都表现得令人满意。</p><p id="7257" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">但就准确性而言，MobileNet和Xception做得很好，但就IoU而言，预测在所有这些网络中都有波动。</p><p id="69f3" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">当涉及到匿名图片时，情况就不一样了。</p><p id="1650" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">但是大部分图片的欠条都相当不错。</strong></p><p id="9e37" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">使用IOU的测量清楚地使我们了解在哪个图片中我们得到了坏的IOU，以及预测的边界框与真实的边界框不同到什么程度。</p><h1 id="810e" class="nc lz iq bd mi nd ne nf ml ng nh ni mo jw nj jx mr jz nk ka mu kc nl kd mx nm bi translated"><strong class="ak">每个模型的精度图和损失图的观察结果</strong></h1><p id="9657" class="pw-post-body-paragraph kw kx iq ky b kz nt jr lb lc nu ju le lf nv lh li lj nw ll lm ln nx lp lq lr ij bi translated">对于第一个预训练模型MobileNet，我发现在绘制每个历元中的精度和损失时，初始历元之后的训练精度高于验证精度。模型中的验证损失也很高。</p><p id="8cb2" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">对于第二个预训练模型ResNet，当从初始或起始时期本身绘图时，训练精度大大高于验证精度。验证损失太高了！</p><p id="23ea" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">对于第三个预训练模型——例外——在绘图时，发现训练精度高于第4个历元本身的验证精度。此外，验证损失很高。</p><p id="f257" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">也就是说，模型训练过度了。</p><p id="3149" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我觉得根据IOU，除了一些令人困惑的图像之外，所有这些模型都表现得相当好。</p><p id="e2a8" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">除此之外，一些图片也给出了很好的IOU值！</p><p id="ae9e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">总的来说，Xception在这个数据集上表现良好。</strong></p><p id="fd14" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我希望从这篇文章中你能了解如何在数据集中进行对象定位，并尝试各种预先训练好的网络，如Mobile Net、ResNet和Xception。</p><p id="1c1f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">结果可能会因您将采用的数据集而异，但这肯定会帮助您通过执行各种实验或测试来利用数据，并根据您的研究背景得出最佳的预训练网络。</p></div></div>    
</body>
</html>