<html>
<head>
<title>Weekly review of Reinforcement Learning papers #4</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">强化学习论文#4的每周回顾</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/weekly-review-of-reinforcement-learning-papers-4-d735531f629c?source=collection_archive---------47-----------------------#2021-04-12">https://towardsdatascience.com/weekly-review-of-reinforcement-learning-papers-4-d735531f629c?source=collection_archive---------47-----------------------#2021-04-12</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="affe" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">每周一，我都会发表我研究领域的4篇论文。大家来讨论一下吧！</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/d0ee44896cd0b738e7426378411b0b46.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*OGsLc_lsdUm00U_-.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="9b03" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">[ <a class="ae lr" href="https://medium.com/mlearning-ai/weekly-review-of-reinforcement-learning-papers-3-32f03633066e?source=friends_link&amp;sk=c53ec970bac2d20ac7c0853391b83e12" rel="noopener"> ←上一次回顾</a> ][ <a class="ae lr" href="https://qgallouedec.medium.com/weekly-review-of-reinforcement-learning-papers-5-adb88dc9dff?source=friends_link&amp;sk=c992da2710ad5e171e41c214b6de4798" rel="noopener">下一次回顾→ </a></p><p id="2134" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">纪念安德烈亚斯。</p></div><div class="ab cl ls lt hu lu" role="separator"><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx"/></div><div class="ij ik il im in"><h1 id="810d" class="lz ma iq bd mb mc md me mf mg mh mi mj jw mk jx ml jz mm ka mn kc mo kd mp mq bi translated">论文1:规划在基于模型的深度强化学习中的作用</h1><p id="2d1b" class="pw-post-body-paragraph kv kw iq kx b ky mr jr la lb ms ju ld le mt lg lh li mu lk ll lm mv lo lp lq ij bi translated">汉姆里克、J. B .、弗里森、A. L .、贝巴哈尼、f .、格斯、a .、维奥拉、f .、威瑟斯彭、s .…&amp;韦伯、T. (2020)。<a class="ae lr" href="https://arxiv.org/abs/2011.04021" rel="noopener ugc nofollow" target="_blank">论规划在基于模型的深度强化学习中的作用</a>。<em class="mw"> arXiv预印本arXiv:2011.04021 </em>。</p><p id="806c" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi mx translated"><span class="l my mz na bm nb nc nd ne nf di"> W </span>规划在强化学习中的贡献是什么？这很难知道:它是许多非常强大的算法(如MuZero)的一部分。但是这个计划阶段对好的学习结果有多大的必要呢？这是本出版物的作者试图回答的问题。为了做到这一点，他们修改了MuZero，用不同的环境，不同的消融来面对它。</p><p id="25fc" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">以下是他们回应的摘要。计划是有用的，但是<strong class="kx ir">并不总是非常有效</strong>。在某些情况下，人们会直观地定义为需要大量的推理，如推箱子，没有必要做大量的规划。在像9x9 Go这样的其他游戏中，学习表现受到计划深度的强烈影响。另一方面，规划对于良好的概括是不够的。这表明，识别良好的政策偏差可能比学习更好的模型来推动泛化更重要。</p><p id="d3e6" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">直觉上，预测未来的能力对于学习好的政策很重要。我发现通过测试基于基准模型的学习算法来质疑这种直觉是很有趣的。</p><h1 id="aa64" class="lz ma iq bd mb mc ng me mf mg nh mi mj jw ni jx ml jz nj ka mn kc nk kd mp mq bi translated">论文2:双足机器人鲁棒参数化运动控制的强化学习</h1><p id="3049" class="pw-post-body-paragraph kv kw iq kx b ky mr jr la lb ms ju ld le mt lg lh li mu lk ll lm mv lo lp lq ij bi translated">(2021年)李，郑，陈，彭，张宝兵，李宝兵，李宝兵，李宝兵。<a class="ae lr" href="https://arxiv.org/abs/2103.14295" rel="noopener ugc nofollow" target="_blank">双足机器人鲁棒参数化运动控制的强化学习</a>。<em class="mw"> arXiv预印本arXiv:2103.14295 </em>。</p><p id="a379" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi mx translated">ipedal移动是机器学习力量的一个很好的展示。大多数机器人的控制不是基于学习。我们使用几十年前就知道的线性自动方法，结果非常令人满意。但是这些方法从来没有强大到足以让双足机器人行走。正是在这个前沿领域，机器学习引起了人们极大的兴趣。</p><p id="7b80" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在本文中，作者提出了一个适用于双足机器人控制的强化学习框架。在这个框架中，第一个学习阶段是在模拟中完成的。然而，模拟系统地不同于真实世界。这被称为<em class="mw">模拟真实</em>差距。这就是他们使用域随机化的原因:模拟常数不再是常数:它们从一个模拟时期变化到下一个。这使得该策略在部署到真实机器人上时，对于它将经历的域改变更加健壮。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nl"><img src="../Images/d3aa81893208c174cdcb0eec30bea278.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LMXC0ovAE6zD8BPPmI7rjg.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图来自<a class="ae lr" href="https://arxiv.org/abs/2103.14295" rel="noopener ugc nofollow" target="_blank">文章</a>:从脚滑中恢复</p></figure><p id="0c04" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">学习到的策略允许双足机器人执行一系列有趣的行为:例如，在图形上，它们通过使机器人滑动来破坏其稳定性。我们看到它是如何对这种扰动做出正确反应的！它还执行其他任务:快速行走，转弯，支撑额外的重量…</p><p id="598e" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">deep-RL应用于机器人控制的另一个很好的演示。去看他们的<a class="ae lr" href="https://www.youtube.com/watch?v=goxCjGPQH7U" rel="noopener ugc nofollow" target="_blank">视频</a>。我发现这种认识很能说明问题，因为我们每个人都是在最初几个月学会走路的。</p><h1 id="1faa" class="lz ma iq bd mb mc ng me mf mg nh mi mj jw ni jx ml jz nj ka mn kc nk kd mp mq bi translated">论文3:使用行动者-学习者提炼的强化学习中的有效转换器</h1><p id="bc9e" class="pw-post-body-paragraph kv kw iq kx b ky mr jr la lb ms ju ld le mt lg lh li mu lk ll lm mv lo lp lq ij bi translated">Parisotto，e .，&amp; Salakhutdinov，R. (2021)。<a class="ae lr" href="https://arxiv.org/abs/2104.01655" rel="noopener ugc nofollow" target="_blank">使用行动者-学习者提取的强化学习中的高效转换器</a>。<em class="mw"> arXiv预印本arXiv:2104.01655 </em>。</p><p id="ed7d" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi mx translated"><span class="l my mz na bm nb nc nd ne nf di"> F </span>或者某些应用，特别是对机器人的实时控制，需要学习的模型响应时间低。机器人必须对环境的变化做出快速反应。因此，学习模型必须足够简单，以允许推理与实时控制的约束兼容。很多时候，学习到的强化学习模型很简单。正如安德烈·卡帕西所说</p><blockquote class="nm nn no"><p id="3f81" class="kv kw mw kx b ky kz jr la lb lc ju ld np lf lg lh nq lj lk ll nr ln lo lp lq ij bi translated">“我所知道的关于ConvNets设计的一切(resnets，bigger=better batchnorms等。)在RL是没用的。Superbasic 4层ConvNets效果最佳。”[ <a class="ae lr" href="https://twitter.com/karpathy/status/822563606344695810" rel="noopener ugc nofollow" target="_blank">来源</a></p></blockquote><p id="f977" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">但是，如何利用那些让监督学习如此成功的复杂模型呢？为了回答这个问题，这篇文章的作者提出了一个“行动者-学习者升华”(ALD)的过程。它允许将学习进度从在大型网络上学习的模型转移到较小的网络。这使得他们可以在非马尔可夫环境(即部分可观测环境)中使用非常高效但非常笨重的变压器架构。这个超级模型然后被提炼为一个更轻的LSTM模型。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ns"><img src="../Images/795934178b1f68f14b7e0aea8da5014b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*knvdqK4-JS16QhrbTLwWqw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图来自<a class="ae lr" href="https://arxiv.org/abs/2103.14295" rel="noopener ugc nofollow" target="_blank">篇</a>:ALD概况。</p></figure><p id="bac2" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">他们在相当简单的环境(I-Maze 9x9和Meta-Fetch)中对其进行了测试，学习到的模型成功地将LSTM的轻便性和变压器的效率结合起来。它可以很好地帮助协调强化学习和监督学习，有利于强化学习！</p><h1 id="3aa2" class="lz ma iq bd mb mc ng me mf mg nh mi mj jw ni jx ml jz nj ka mn kc nk kd mp mq bi translated">论文4: pH-RL:将强化学习引入健康实践的个性化架构</h1><p id="f1f0" class="pw-post-body-paragraph kv kw iq kx b ky mr jr la lb ms ju ld le mt lg lh li mu lk ll lm mv lo lp lq ij bi translated">Hassouni，A. E .、Hoogendoorn，m .、Ciharova，m .、Kleiboer，a .、Amarti，k .、Muhonen，v .、… &amp; Eiben，A. E. (2021)。<a class="ae lr" href="https://arxiv.org/abs/2103.15908" rel="noopener ugc nofollow" target="_blank"> pH-RL:将强化学习引入健康实践的个性化架构</a>。<em class="mw"> arXiv预印本arXiv:2103.15908 </em>。</p><p id="c9a0" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi mx translated">总是同一个问题:在模拟或游戏中，强化学习已经证明了自己。但是现实世界呢？在这篇论文中，作者提出了一个健康问题的通用强化学习架构:个性化，更具体地说是移动应用的个性化。他们称之为pH-RL(带RL的电子健康个性化)。这种架构允许通过学习来个性化健康应用，并且个性化的级别是可调的。</p><p id="1751" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">事实上，他们提出了在移动医疗应用程序中引入强化学习模型的指南。他们用MoodBuster应用程序(一个在线治疗心理疾病的平台)证明了他们方法的有效性。从经验上来说，他们表明，学习模型正确地选择了最大限度地每日坚持治疗模块所需的行动和信息。</p><p id="0a28" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我喜欢这种将医疗保健联系起来的文章。结果很有趣，但我忍不住评论道:当心技术解决主义，尤其是在医疗保健领域。</p><h1 id="10fe" class="lz ma iq bd mb mc ng me mf mg nh mi mj jw ni jx ml jz nj ka mn kc nk kd mp mq bi translated">奖励论文:在世纪营1.4千米冰层下的沉积物中保存了数百万年的格陵兰植被和冰川历史记录</h1><p id="27d3" class="pw-post-body-paragraph kv kw iq kx b ky mr jr la lb ms ju ld le mt lg lh li mu lk ll lm mv lo lp lq ij bi translated">Christ，A. J .，Bierman，P. R .，Schaefer，J. M .，Dahl-Jensen，d .，Steffensen，J. P .，Corbett，L. B .，… &amp; Southon，J. (2021)。世纪营地1.4千米冰层下的沉积物中保存了数百万年前的格陵兰植被和冰川历史记录。<em class="mw">美国国家科学院院刊</em>，<em class="mw">第118期</em> (13)。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nt"><img src="../Images/87e8493a49dab35f14524f203c4ef936.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HRf10k_IB5foWBbzJje7ZA.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">来自<a class="ae lr" href="https://www.pexels.com/fr-fr/photo/rhume-glacier-iceberg-fondre-5317263/?utm_content=attributionCopyText&amp;utm_medium=referral&amp;utm_source=pexels" rel="noopener ugc nofollow" target="_blank"> Pexels </a>的<a class="ae lr" href="https://www.pexels.com/fr-fr/@christian-pfeifer-3550788?utm_content=attributionCopyText&amp;utm_medium=referral&amp;utm_source=pexels" rel="noopener ugc nofollow" target="_blank"> CHRISTIAN PFEIFER </a>的图片</p></figure><p id="93e3" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi mx translated">多年后，一个被遗忘的样本揭示了格陵兰岛令人震惊的历史。由于冷战期间意外收集的岩石和土壤碎片，几十年来没有人注意到这些碎片，研究人员已经证明格陵兰冰盖在大约一百万年前已经完全融化。</p><p id="f491" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">1966年，美国研究人员被派往格陵兰的世纪营，进行1400米深的钻探。客观？官方说法:这是为了刺破北极生存的秘密。非正式地说:是为了<strong class="kx ir">藏在冰层下苏联俄罗斯</strong>能触及的600核导弹。在切斯特·朗威的指导下，1.4千米厚的冰芯和3米厚的冰下沉积物被提取、冷冻并转移到布法罗大学的一个仓库里。一份独一无二的档案，最终会被遗忘几十年。切斯特·朗威在20世纪90年代犹豫要不要销毁这些样本，以便在大学的冰柜里腾出空间。但是最终接受他们的是考纳格大学。</p><p id="dfe2" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">2017年，在一次大清理期间，这些著名的样本将被分发给世界各地的几个团队，包括佛蒙特大学的一个团队。2021年，这些来自佛蒙特大学的研究人员研究了这些样本，并发现了可追溯到一百万年前的植物化石。这意味着一百万年前，在世纪营地，可能有北方森林，而不是冰川。</p><p id="b042" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">然而，这个惊人的发现相当令人恐惧:一百万年前，平均气温只比今天高2到3度。具有讽刺意味的是，这正是对未来50年的预测。格陵兰岛的融化将导致海平面上升6到7米。那么我们还在等什么呢？我们是继续做出调整，还是真的全力以赴来防止这种情况发生？</p></div><div class="ab cl ls lt hu lu" role="separator"><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx"/></div><div class="ij ik il im in"><p id="e853" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我很高兴向你们展示我本周的阅读材料。请随时向我发送您的反馈。</p></div></div>    
</body>
</html>