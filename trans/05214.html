<html>
<head>
<title>Decision Tree — Implemented from scratch</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">决策树—从头开始实施</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/decision-tree-implemented-from-scratch-f11f7ca6aa0?source=collection_archive---------34-----------------------#2021-05-07">https://towardsdatascience.com/decision-tree-implemented-from-scratch-f11f7ca6aa0?source=collection_archive---------34-----------------------#2021-05-07</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><figure class="gl gn jr js jt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi jq"><img src="../Images/b7db6a08f4fb783c2c505e3d126b9395.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6TjkpOG_EMnbKJgWYtxOJw.jpeg"/></div></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">蝴蝶效应，与决策树无关…，波兰2020(作者供图)。</p></figure><p id="bd3f" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated"><em class="ld">我想把这个作品扩展成一个系列——教程</em> <a class="ae le" href="https://www.youtube.com/watch?v=NSVTTuOhwz0" rel="noopener ugc nofollow" target="_blank"> <em class="ld">视频</em> </a> <em class="ld">。如果你有兴趣，请</em> <strong class="kh iu"> <em class="ld">订阅</em> </strong> <em class="ld">到我的</em> <a class="ae le" href="https://landing.mailerlite.com/webforms/landing/j5y2q1" rel="noopener ugc nofollow" target="_blank"> <em class="ld">简讯</em> </a> <em class="ld">保持联系。</em></p><h1 id="0d63" class="lf lg it bd lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc bi translated">介绍</h1><p id="d68a" class="pw-post-body-paragraph kf kg it kh b ki md kk kl km me ko kp kq mf ks kt ku mg kw kx ky mh la lb lc im bi translated">当谈到制作模型时，不难有这样一种印象，即当今世界都是关于神经网络的。许多团队似乎吹嘘超级酷的架构，好像获得足够高质量的数据很简单，GPU机架24/7开放(免费)，他们的客户的耐心被设置为无穷大。</p><p id="b99f" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">在本文中，我们将介绍一种最基本的机器学习算法，称为<em class="ld">决策树</em>。决策树通常被认为是“救援”模型，因为它们不需要大量的数据就能工作，并且易于训练。尽管有过度拟合的倾向，但由于预测机制的本质，这些树在解释上有很大的优势。</p><p id="bed5" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">传统上，我们将从零开始实现和训练模型，使用基本的numpy和pandas。请注意，无论如何，我们实现它的方式不是唯一可能的方式。在这里，我们通过从第一原理设计我们的树来集中解释算法的内部工作。因此，更多地将其作为演示，而不是生产解决方案，例如在<a class="ae le" href="https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html" rel="noopener ugc nofollow" target="_blank"> scikit-learn </a>中可以找到。</p><h1 id="ce89" class="lf lg it bd lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc bi translated">理论</h1><p id="bc6a" class="pw-post-body-paragraph kf kg it kh b ki md kk kl km me ko kp kq mf ks kt ku mg kw kx ky mh la lb lc im bi translated">决策树因其类似于树的结构而得名。经过训练的树将通过一系列<em class="ld">节点</em>传递示例数据(也称为“splits”，稍后会详细介绍)。每个节点都充当下一个节点的决策点。最终节点，即“叶子”，相当于最终预测。</p><h1 id="b09f" class="lf lg it bd lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc bi translated">培训过程</h1><p id="7617" class="pw-post-body-paragraph kf kg it kh b ki md kk kl km me ko kp kq mf ks kt ku mg kw kx ky mh la lb lc im bi translated">训练决策树是识别最佳节点结构的过程。为了解释这一点，考虑一个二维输入特征向量，我们希望为其识别一个目标类。第一个节点需要“知道”要检查向量的哪个分量<em class="ld"> x[k]，k = 1，2，…，n </em>，以及要检查的阈值<em class="ld"> x[t] </em>是什么。根据决定，向量将被路由到不同的节点，依此类推。</p><p id="2dfc" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">一个训练有素的树的节点和它们各自的参数<em class="ld"> (k，x[t]) </em>以这样一种方式选择，即错误识别的目标类的数量是最低的。现在的问题是如何构建这样的节点？</p><p id="c8ed" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">这里，至少有两个数学公式可以帮助我们。</p><h2 id="3ae7" class="mi lg it bd lh mj mk dn ll ml mm dp lp kq mn mo lt ku mp mq lx ky mr ms mb mt bi translated">交叉熵公式</h2><figure class="mv mw mx my gt ju gh gi paragraph-image"><div class="gh gi mu"><img src="../Images/0bfb5659836116578d3fc958926fdf18.png" data-original-src="https://miro.medium.com/v2/resize:fit:470/format:webp/0*ZUWlbDMKtgvmYnir.png"/></div></figure><h2 id="99f0" class="mi lg it bd lh mj mk dn ll ml mm dp lp kq mn mo lt ku mp mq lx ky mr ms mb mt bi translated">基尼系数(基尼系数)</h2><figure class="mv mw mx my gt ju gh gi paragraph-image"><div class="gh gi mz"><img src="../Images/ce3baae3abc0600740cd3c47350313cf.png" data-original-src="https://miro.medium.com/v2/resize:fit:348/format:webp/0*ZUpLJcptL7Lkt7WP.png"/></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">蝴蝶效应，与决策树无关…，波兰2020(作者供图)。</p></figure><p id="bec8" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">这两个公式都表达了关于系统的某种T2信息。因为交叉熵函数看起来非常类似于基尼系数(图1。)，我们可以使用它们中的任何一个来建立我们的直觉(和模型)。然而，为了清楚起见，让我们使用后者。</p><figure class="mv mw mx my gt ju gh gi paragraph-image"><div class="gh gi na"><img src="../Images/a17b0366da5b13f73bd17246c74eebc5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1382/format:webp/0*MEdEXuwvzDGywm9i.png"/></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">图一。基尼系数与熵的关系是根据一个实例类的概率计算的。(图片由作者提供)</p></figure><h1 id="4af6" class="lf lg it bd lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc bi translated">解释</h1><p id="13d5" class="pw-post-body-paragraph kf kg it kh b ki md kk kl km me ko kp kq mf ks kt ku mg kw kx ky mh la lb lc im bi translated">如前所述，这两个函数都表达了某种信息。考虑一个二进制分类系统(在这种情况下，类的数量是<em class="ld"> C = 1 </em>，我们只能打赌一个给定的例子是否应该被赋予一个给定的标签)。从分类的角度来看，如果能够知道结果应该是什么，那么最安全的情况就是发生了。在这种情况下，与结果相关的概率要么为零，要么为一。由于对于<em class="ld"> C = 1 </em>，<em class="ld"> G = 1-p = p(1-p) </em>，这种情况下的基尼系数为零。这个系统是“纯”的，在某种意义上我们知道会发生什么。</p><p id="656a" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">现在，我们一不确定<em class="ld"> (0 &lt; p &lt; 1)，G &gt; 0 </em>，正好在我们知道最少<em class="ld"> p = 1/2 </em>的时候达到峰值。熵也是如此。它越高，系统就越“混乱”,或者说我们得到的“信息”最少。</p><p id="1b90" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">添加更多的类<em class="ld"> C &gt; 1 </em>不会改变这里的逻辑。这两个数字通过总和<em class="ld"> j = 1，2，…，C </em>从所有类别中提取贡献。</p><h1 id="94a7" class="lf lg it bd lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc bi translated">减少杂质</h1><p id="9820" class="pw-post-body-paragraph kf kg it kh b ki md kk kl km me ko kp kq mf ks kt ku mg kw kx ky mh la lb lc im bi translated">好吧，但是基尼系数如何帮助我们解决训练决策树的问题呢？如前所述，这都是关于战略性地设置节点，因此它们都有助于做出好的决策。任何决策(任何节点)都相当于在某个特定值沿给定轴对参数空间进行切片或<em class="ld">分裂</em>。如果选择<em class="ld"> (k，x[t]) </em>的方式使我们对系统的了解逐渐增加，那么更有可能做出“好”的决策。换句话说，在这两个几乎形成的子空间中，不确定性比开始时要少。</p><p id="330a" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">如果G+和G-是与分裂后的子空间相关联的基尼杂质值，我们可以将所谓的<em class="ld">基尼增益</em>定义为:</p><figure class="mv mw mx my gt ju gh gi paragraph-image"><div class="gh gi nb"><img src="../Images/1d47bf97419d5a1775248a36d1222a6c.png" data-original-src="https://miro.medium.com/v2/resize:fit:696/format:webp/0*M4C7im81w_7k_jBZ.png"/></div></figure><p id="aa3b" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">其中<em class="ld"> m </em>是示例计数，(+)和(-)是指子空间。</p><p id="1a2e" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">增益<em class="ld"> g </em>越高,“更明智”的节点将能够区分不同的情况。如果所有的节点都是这样选择的，我们就有了一个合理的模型。</p><h1 id="c79d" class="lf lg it bd lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc bi translated">履行</h1><p id="44ec" class="pw-post-body-paragraph kf kg it kh b ki md kk kl km me ko kp kq mf ks kt ku mg kw kx ky mh la lb lc im bi translated">让我们创建一个简单的人工数据集，其中每个例子都表示为一个三维向量x ∈ ℝ。</p><pre class="mv mw mx my gt nc nd ne nf aw ng bi"><span id="fe0e" class="mi lg it nd b gy nh ni l nj nk">import numpy as np<br/>import pandas as pd<br/><br/>from sklearn.datasets import make_classification<br/><br/>X, y = make_classification(<br/>        n_samples=12,<br/>        n_classes=3,<br/>        n_clusters_per_class=2,<br/>        n_features=3,<br/>        n_informative=3,<br/>        n_redundant=0,<br/>        n_repeated=0,<br/>        random_state=42)<br/><br/>df = pd.DataFrame(X, columns=[x for x in range(X.shape[1])])<br/>df["target"] = y</span></pre><p id="ec6b" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">在这里，我们使用熊猫只是为了方便“把东西放在一起”。最重要的是我们有三维数据<em class="ld"> (n = 3) </em>以及三个目标类<em class="ld"> (C = 3) </em>。下表显示了数据集。</p><pre class="mv mw mx my gt nc nd ne nf aw ng bi"><span id="e71b" class="mi lg it nd b gy nh ni l nj nk">|      0 |      1 |      2 |  target |<br/>|-------:|-------:|-------:|--------:|<br/>|  0.78  | -1.09  | -0.99  |       0 |<br/>|  1.56  |  0.87  | -0.93  |       1 |<br/>|  1.17  | -0.25  |  0.54  |       0 |<br/>| -1.84  | -0.70  | -2.71  |       2 |<br/>| -1.91  | -1.57  |  0.98  |       1 |<br/>| -0.96  | -0.77  | -3.92  |       2 |<br/>| -0.83  | -2.70  | -2.13  |       0 |<br/>|  0.02  |  2.20  | -0.08  |       2 |<br/>|  1.62  |  0.38  |  0.83  |       0 |<br/>| -0.14  | -0.70  |  1.39  |       1 |<br/>|  2.29  |  0.48  |  0.32  |       1 |<br/>|  0.53  |  2.33  | -0.12  |       2 |</span></pre><h1 id="35e0" class="lf lg it bd lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc bi translated">基尼杂质</h1><p id="6f11" class="pw-post-body-paragraph kf kg it kh b ki md kk kl km me ko kp kq mf ks kt ku mg kw kx ky mh la lb lc im bi translated">计算基尼系数是我们会反复做的事情。因此，将其包装为函数是有意义的。</p><pre class="mv mw mx my gt nc nd ne nf aw ng bi"><span id="21e1" class="mi lg it nd b gy nh ni l nj nk">def gini(df):<br/>    probs = {}<br/>    for c in df["target"].unique():<br/>        probs[c] = len(df.query("target == @c")) / len(df)<br/>    return sum([p * (1 - p) for p in probs.values()])</span></pre><p id="bdf0" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">整个函数是前面陈述的公式的实现。我们迭代目标类(<code class="fe nl nm nn nd b">line 3.</code>)，并通过计数事件获得概率。</p><h1 id="d25b" class="lf lg it bd lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc bi translated">基尼增益</h1><p id="917f" class="pw-post-body-paragraph kf kg it kh b ki md kk kl km me ko kp kq mf ks kt ku mg kw kx ky mh la lb lc im bi translated">有了基尼系数的代码，让我们创建一个基尼系数的函数。</p><pre class="mv mw mx my gt nc nd ne nf aw ng bi"><span id="5f84" class="mi lg it nd b gy nh ni l nj nk">def gini_gain(df, feature_name, feature_value):<br/>    df_a = df.query(f"{feature_name} &lt; @feature_value")<br/>    df_b = df.query(f"{feature_name} &gt;= @feature_value")<br/>    <br/>    total_gini = gini(df)<br/>    gini_a = gini(df_a)<br/>    gini_b = gini(df_b)<br/><br/>    m, m_a, m_b = len(df), len(df_a), len(df_b)<br/><br/>    return total_gini - gini_a * m_a / m - gini_b * m_b / m</span></pre><p id="bd5e" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">自然，增益值取决于对<em class="ld"> k </em> ( <code class="fe nl nm nn nd b">feature_name</code>)和<em class="ld">x【t】</em>(<code class="fe nl nm nn nd b">feature_value</code>)的具体选择。因此，该函数必须将它们作为输入。然后，我们简单地将数据集分成两部分，用<code class="fe nl nm nn nd b">_a</code>和<code class="fe nl nm nn nd b">_b</code>替换((+)和(-)，并返回增益值。</p><h1 id="1d33" class="lf lg it bd lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc bi translated">寻找最佳分割</h1><p id="b4ad" class="pw-post-body-paragraph kf kg it kh b ki md kk kl km me ko kp kq mf ks kt ku mg kw kx ky mh la lb lc im bi translated">功能<code class="fe nl nm nn nd b">gini</code>和<code class="fe nl nm nn nd b">gini_gain</code>只是工具。为了找到最佳分割<em class="ld"> (k，x[t]) </em>，我们需要比较各种选项，选择增益最大的一个。最简单的方法是迭代现有维度<em class="ld"> (k = 1，2，…，n) </em>，并沿该维度扫描阈值<em class="ld"> x[t] </em>，每次都评估增益。</p><pre class="mv mw mx my gt nc nd ne nf aw ng bi"><span id="75f7" class="mi lg it nd b gy nh ni l nj nk">def find_best_split(df, density_per_feature=10):<br/>    splits = pd.DataFrame()<br/>    <br/>    for feat in [feat for feat in df.columns if feat != "target"]:<br/>        split_candidates = np.linspace(<br/>            df[feat].min(),<br/>            df[feat].max(),<br/>            num=density_per_feature,<br/>        )<br/>        <br/>        _splits = pd.DataFrame()<br/>        _splits["candidate_values"] = split_candidates<br/>        _splits["feature"] = feat<br/>        _splits["gini_gain"] = _splits["candidate_values"].apply(<br/>            lambda value: gini_gain(df, feat, value)<br/>        )<br/><br/>        splits = pd.concat([splits, _splits])<br/><br/>    best_split = splits \<br/>        .sort_values(by="gini_gain", ascending=False) \<br/>        .iloc[0]<br/><br/>    return best_split["feature"].iloc[0], \<br/>           best_split["candidate_values"].iloc[0]</span></pre><p id="d94c" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">如您所见，<code class="fe nl nm nn nd b">density_per_feature</code>是一个附加参数，用于选择我们扫描的粒度(通常越大越好，但也越长)。<code class="fe nl nm nn nd b">line 4.</code>中的循环是循环结束。然后，我们定义<code class="fe nl nm nn nd b">split</code>和<code class="fe nl nm nn nd b">_split</code>纯粹是为了方便。有了这些辅助数据帧，我们可以使用<code class="fe nl nm nn nd b">.apply</code>方法自动获得帧中每个条目的基尼系数，并使用<code class="fe nl nm nn nd b">.sort_values</code>找到最佳选项。识别之后，我们返回节点的选择维度以及阈值。</p><h1 id="d94b" class="lf lg it bd lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc bi translated">那棵树</h1><p id="aa31" class="pw-post-body-paragraph kf kg it kh b ki md kk kl km me ko kp kq mf ks kt ku mg kw kx ky mh la lb lc im bi translated">决策树由节点组成。在训练时，我们不仅需要使节点“最优”,而且我们需要创建整个树结构，以便节点一起工作。而对于节点，我们有<code class="fe nl nm nn nd b">find_best_split</code>函数，我们需要将它们组织成一棵树。</p><blockquote class="no np nq"><p id="30db" class="kf kg ld kh b ki kj kk kl km kn ko kp nr kr ks kt ns kv kw kx nt kz la lb lc im bi translated">要理解递归，首先要理解递归。T9】</p></blockquote><p id="2e25" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">考虑以下函数:</p><pre class="mv mw mx my gt nc nd ne nf aw ng bi"><span id="74c2" class="mi lg it nd b gy nh ni l nj nk">def func(tree, max_iter=5):<br/>    max_iter -= 1<br/>    if max iter &lt;= 0:<br/>        return [max_iter]<br/>    node = [max_iter, func(tree, max_iter)]<br/>    tree.append(node)<br/>    return node</span></pre><p id="375d" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">这是一个递归函数。它随后构建了一个简单的树(或者更确切地说是一个“树枝”)，通过嵌套连续的数字，就像这样:</p><pre class="mv mw mx my gt nc nd ne nf aw ng bi"><span id="9bb4" class="mi lg it nd b gy nh ni l nj nk">tree = func([])<br/>print(tree)<br/><br/>[4, [3, [2, [1, [0]]]]]</span></pre><p id="3a1b" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">如您所见，调用<code class="fe nl nm nn nd b">func</code>的过程一直持续到它达到<code class="fe nl nm nn nd b">max_iter</code>条件。在此之前，它会一直调用自身并向结构中添加节点。</p><p id="8e6f" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">我们的决策树是一个二叉树(每个节点分支到两个子节点)。因此，我们需要稍微修改一下<code class="fe nl nm nn nd b">func</code>，以支持模式子节点，并用一对<em class="ld"> (k，x[t]) </em>替换单个数字。因此，我们可以接受一个约定，其中每个节点由四个元素的列表定义:</p><ol class=""><li id="4322" class="nu nv it kh b ki kj km kn kq nw ku nx ky ny lc nz oa ob oc bi translated">分割特征(或尺寸)<em class="ld"> k </em>，</li><li id="c2a9" class="nu nv it kh b ki od km oe kq of ku og ky oh lc nz oa ob oc bi translated">分裂值(阈值)<em class="ld">x【t】</em>，</li><li id="de9d" class="nu nv it kh b ki od km oe kq of ku og ky oh lc nz oa ob oc bi translated">如果决策条件为真，则响应的子节点，</li><li id="790a" class="nu nv it kh b ki od km oe kq of ku og ky oh lc nz oa ob oc bi translated">决策条件为假时做出响应的子节点。</li></ol><p id="c42e" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">此外，我们需要定义一个停止条件，以防止无休止的循环链。我们同意采用以下方法:</p><ol class=""><li id="2a61" class="nu nv it kh b ki kj km kn kq nw ku nx ky ny lc nz oa ob oc bi translated">一旦我们只剩下给定子节点的单个类的所有元素，这意味着已经识别了特定决策节点序列的标签。因此，我们可以存储一个类标签。</li><li id="5601" class="nu nv it kh b ki od km oe kq of ku og ky oh lc nz oa ob oc bi translated">如果我们怀疑树可能太深，我们也可以终止递归并取类的平均值(或模式)。这个模型不太精确，但是训练速度更快。</li></ol><h1 id="4d7a" class="lf lg it bd lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc bi translated">种植树木</h1><p id="2e5d" class="pw-post-body-paragraph kf kg it kh b ki md kk kl km me ko kp kq mf ks kt ku mg kw kx ky mh la lb lc im bi translated">让我们用<code class="fe nl nm nn nd b">split</code>代替<code class="fe nl nm nn nd b">func</code>的名字，并重新加工内部零件。</p><pre class="mv mw mx my gt nc nd ne nf aw ng bi"><span id="4175" class="mi lg it nd b gy nh ni l nj nk">def split(tree, df, max_depth=5):<br/>    <br/>    splitting_feature, splittng_value = find_best_split(df)<br/><br/>    df_a = df.query(f"{splitting_feature} &lt; @splitting_value")<br/>    df_b = df.query(f"{splitting_feature} &gt;= @splitting_value")<br/>    <br/>    max_depth -= 1<br/><br/>    if len(df_a) == 0 or len(df_b) == 0 or max_depth == 0:<br/>        target_a = set(df_a["target"]) if len(df_a) &gt; 0 else set([])<br/>        target_b = set(df_b["target"]) if len(df_b) &gt; 0 else set([])<br/>        target =<!-- -->Hey! Do you mind helping me out?<!-- --> target_a.union(target_b)<br/>        <br/>        node = [splitting_feature, splitting_value, target, target]<br/>        return node<br/><br/>    node = [<br/>        splitting_feature, <br/>        splitting_value, <br/>        split(tree, df_a, max_depth=max_depth),<br/>        split(tree, df_b, max_depth=max_depth),<br/>    ]<br/>    tree.append(node)<br/>    return node<br/><br/><br/>tree = split([], df, max_depth=10)</span></pre><p id="7827" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">现在，我们来分解它。如果你仔细观察，<code class="fe nl nm nn nd b">split</code>函数递归地工作，就像<code class="fe nl nm nn nd b">func</code>一样。区别在于我们如何构造每个节点。这里，在我们为一个给定的节点确定了最佳对，并将数据集分成两个子集之后，我们测试是否达到了停止条件。如果子集<code class="fe nl nm nn nd b">df_a</code>和<code class="fe nl nm nn nd b">df_b</code>中的任何一个缺少元素，这意味着我们已经到达了一片叶子，将不会再有来自该节点的分裂。同时，<code class="fe nl nm nn nd b">target_a</code>或<code class="fe nl nm nn nd b">target_b</code>是一个空集，因此集合的并集<code class="fe nl nm nn nd b">target</code>将包含剩余的标签。除非我们提前终止这个过程(通过<code class="fe nl nm nn nd b">max_iters</code>),否则<code class="fe nl nm nn nd b">set</code>操作会将标签压缩到一个单元素集合中。这样，我们不仅唯一地标识了目标类，还坚持了将节点定义为四元素列表的惯例，其中最后两个元素是可迭代的，这防止了<code class="fe nl nm nn nd b">TypeError</code>异常。</p><p id="97f5" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">如果条件不适用，该过程继续。在这种情况下，每个节点“记住”最佳参数以及源于当前参数的两个子节点。</p><p id="9d5e" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">一个经过训练的树的示例可能如下所示:</p><pre class="mv mw mx my gt nc nd ne nf aw ng bi"><span id="eaa9" class="mi lg it nd b gy nh ni l nj nk">[2, 0.208,<br/>    [1, -1.023,<br/>        [0, -0.839, {0}, {0}],<br/>        [0, 0.804,<br/>            [0, -1.845, {2}, {2}],<br/>            [0, 1.5628, {1}, {1}]<br/>        ]<br/>    ],<br/>    [1, -0.429,<br/>        [0, -1.915, {1}, {1}],<br/>        [0, 2.166,<br/>            [0, 1.172, {0}, {0}],<br/>            [0, 2.290, {1}, {1}]<br/>        ]<br/>    ]<br/>]</span></pre><h1 id="e0c2" class="lf lg it bd lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc bi translated">作为模型的树</h1><p id="5ef9" class="pw-post-body-paragraph kf kg it kh b ki md kk kl km me ko kp kq mf ks kt ku mg kw kx ky mh la lb lc im bi translated">虽然这种嵌套结构可能看起来不令人满意，但对于机器来说，理解起来非常简单。基于该树的模型只需要使用列表的第一个元素来选择要检查的特性，将其与第二个元素进行比较，并根据结果将问题转发给第三个或第四个元素。重复这个过程，直到得到一个集合，并返回它的内容作为一个预测。</p><p id="9cb4" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">让我们实现这样一个模型。</p><pre class="mv mw mx my gt nc nd ne nf aw ng bi"><span id="0fea" class="mi lg it nd b gy nh ni l nj nk">def model(tree, data):<br/>    node = tree<br/>    while not isinstance(node, set):<br/>        feature = node[0]<br/>        value = node[1]<br/>        node = node[2] if data[feature] &lt; value else node[3]<br/>    return list(node)[0]</span></pre><p id="35c3" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">现在，我们可以使用<code class="fe nl nm nn nd b">.apply</code>方法对数据集运行这个模型:</p><pre class="mv mw mx my gt nc nd ne nf aw ng bi"><span id="1bc8" class="mi lg it nd b gy nh ni l nj nk">df["predicted"] = df.get([0, 1, 2]).apply(<br/>    lambda row: model(tree, row.to_numpy()), axis=1<br/>)</span></pre><p id="a4e2" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">请注意，<code class="fe nl nm nn nd b">[0, 1, 2]</code>列表指的是特征的名称。结果如下表所示。</p><pre class="mv mw mx my gt nc nd ne nf aw ng bi"><span id="35fd" class="mi lg it nd b gy nh ni l nj nk">|          0 |         1 |          2 |   target |   predicted |<br/>|-----------:|----------:|-----------:|---------:|------------:|<br/>|  0.784972  | -1.09651  | -0.999525  |        0 |           0 |<br/>|  1.56221   |  0.876346 | -0.937159  |        1 |           1 |<br/>|  1.1727    | -0.258296 |  0.542743  |        0 |           0 |<br/>| -1.84585   | -0.703202 | -2.71293   |        2 |           2 |<br/>| -1.91564   | -1.57344  |  0.982937  |        1 |           1 |<br/>| -0.966766  | -0.774834 | -3.92814   |        2 |           2 |<br/>| -0.839739  | -2.70451  | -2.13491   |        0 |           0 |<br/>|  0.0284821 |  2.20555  | -0.0827445 |        2 |           2 |<br/>|  1.62031   |  0.382993 |  0.835343  |        0 |           0 |<br/>| -0.144515  | -0.700331 |  1.39023   |        1 |           1 |<br/>|  2.29084   |  0.486279 |  0.323968  |        1 |           1 |<br/>|  0.532064  |  2.33869  | -0.120604  |        2 |           2 |</span></pre><h1 id="c407" class="lf lg it bd lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc bi translated">结论</h1><p id="7b28" class="pw-post-body-paragraph kf kg it kh b ki md kk kl km me ko kp kq mf ks kt ku mg kw kx ky mh la lb lc im bi translated">正如您所看到的，决策树非常简单，但是实现起来可能并不简单。无论如何，在任何生产代码中，我们强烈建议您使用已经存在的实现。此外，你可能还应该研究所谓的<em class="ld">集成</em>方法，如随机森林，由于结合了几个模型的优势，它可能会给你带来额外的准确性提升。</p></div><div class="ab cl oi oj hx ok" role="separator"><span class="ol bw bk om on oo"/><span class="ol bw bk om on oo"/><span class="ol bw bk om on"/></div><div class="im in io ip iq"><p id="d099" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated"><em class="ld">原载于</em><a class="ae le" href="https://zerowithdot.com/decision-tree/" rel="noopener ugc nofollow" target="_blank"><em class="ld">https://zerowithdot.com</em></a><em class="ld">。</em></p></div></div>    
</body>
</html>