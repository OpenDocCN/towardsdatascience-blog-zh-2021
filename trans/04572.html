<html>
<head>
<title>Understanding PyTorch Activation Functions: The Maths and Algorithms (Part 2)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">理解PyTorch激活函数:数学和算法(第二部分)</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/understanding-pytorch-activation-functions-the-maths-and-algorithms-part-2-1f8bce111a7b?source=collection_archive---------38-----------------------#2021-04-19">https://towardsdatascience.com/understanding-pytorch-activation-functions-the-maths-and-algorithms-part-2-1f8bce111a7b?source=collection_archive---------38-----------------------#2021-04-19</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="bed9" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">PyTorch中激活函数的数学定义、算法和实现的分步指南</h2></div><p id="1712" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">第1部分可以在这里找到<a class="ae le" rel="noopener" target="_blank" href="/understanding-pytorch-activation-functions-the-maths-and-algorithms-part-1-7d8ade494cee">。</a></p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi lf"><img src="../Images/c017ac5f43949eff1683773c23ac387d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*Iq7OF9W1NDFUI3b-"/></div></div><p class="lr ls gj gh gi lt lu bd b be z dk translated">由<a class="ae le" href="https://unsplash.com/@billmackie?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">比尔·麦基</a>在<a class="ae le" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><h1 id="8522" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">介绍</h1><p id="4ae5" class="pw-post-body-paragraph ki kj it kk b kl mn ju kn ko mo jx kq kr mp kt ku kv mq kx ky kz mr lb lc ld im bi translated">在ML的世界中，激活函数帮助网络学习输入数据中复杂的非线性模式(或嵌入)。</p><p id="d1d9" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在这篇文章中，我们将从数学上形式化并实现PyTorch中一些更健壮但不太流行的激活函数。要了解更多关于更流行的激活功能，请在<a class="ae le" rel="noopener" target="_blank" href="/understanding-pytorch-activation-functions-the-maths-and-algorithms-part-1-7d8ade494cee">第1部分</a>中找到它们。它将根据您正在查看的输出类型进行划分，即:</p><ul class=""><li id="7e26" class="ms mt it kk b kl km ko kp kr mu kv mv kz mw ld mx my mz na bi translated"><strong class="kk iu">(轻微)阳性</strong> : ReLU6，随机泄漏</li><li id="076c" class="ms mt it kk b kl nb ko nc kr nd kv ne kz nf ld mx my mz na bi translated"><strong class="kk iu">在0和1之间</strong> : Softmin</li><li id="8b0d" class="ms mt it kk b kl nb ko nc kr nd kv ne kz nf ld mx my mz na bi translated"><strong class="kk iu">小于0 </strong> : LogSoftmax，LogSigmoid</li></ul><h1 id="ab34" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">1.(稍微)积极</h1><p id="5e48" class="pw-post-body-paragraph ki kj it kk b kl mn ju kn ko mo jx kq kr mp kt ku kv mq kx ky kz mr lb lc ld im bi translated"><em class="ng">之前的帖子强调了ReLU和LeakyReLU的用法。</em></p><p id="42fd" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如果您希望修改输入，并且只取它们的正值(在一定程度上取它们的负值)，那么ReLU6和随机化泄漏(RRELU)是合适的。</p><h2 id="7c55" class="nh lw it bd lx ni nj dn mb nk nl dp mf kr nm nn mh kv no np mj kz nq nr ml ns bi translated">a.整流线性单元6 (ReLU6)</h2><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi nt"><img src="../Images/8e589dcf4b6e9ef73376a84306e852f1.png" data-original-src="https://miro.medium.com/v2/resize:fit:822/format:webp/1*pPLqxv-axAZvZykZEw7rgQ.png"/></div></div><p class="lr ls gj gh gi lt lu bd b be z dk translated">ReLU6数学定义</p></figure><p id="3308" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">给定输入<em class="ng"> x，</em> ReLU6将取0和x之间的最大值，然后取初始输出和6之间的最小值。为什么将最大正值设置为6？根据<a class="ae le" href="https://paperswithcode.com/method/relu6" rel="noopener ugc nofollow" target="_blank">论文</a>，在低精度足够的情况下，6的大小增加了计算的鲁棒性和稳定性。从图形上看，ReLU6具有以下转变行为:</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/ba18d071ad772abec9afe094765c74d0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/0*pxHp-MSGDjky1NNU.png"/></div></figure><p id="e3e8" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在python中实现ReLU6函数的方法如下:</p><pre class="lg lh li lj gt nv nw nx ny aw nz bi"><span id="8d2c" class="nh lw it nw b gy oa ob l oc od">import numpy as np</span><span id="5ccd" class="nh lw it nw b gy oe ob l oc od">arr_before = np.array([-1, 1, 7])</span><span id="3c22" class="nh lw it nw b gy oe ob l oc od">def relu6(x):<br/>    x = np.minimum(np.maximum(0,x), 6)<br/>    return x</span><span id="bcf4" class="nh lw it nw b gy oe ob l oc od">arr_after = relu6(arr_before)</span><span id="ecb5" class="nh lw it nw b gy oe ob l oc od">arr_after<br/>#array([0, 1, 6])</span></pre><p id="7320" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">并且在PyTorch中，可以很容易地调用ReLU6激活函数。</p><pre class="lg lh li lj gt nv nw nx ny aw nz bi"><span id="19a7" class="nh lw it nw b gy oa ob l oc od">import torch.nn</span><span id="1edc" class="nh lw it nw b gy oe ob l oc od">relu6 = nn.ReLU6()<br/>input = torch.randn(2)<br/>output = relu6(input)</span></pre><h2 id="a827" class="nh lw it bd lx ni nj dn mb nk nl dp mf kr nm nn mh kv no np mj kz nq nr ml ns bi translated">b.随机化泄漏率</h2><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi of"><img src="../Images/962b6d24e46f103dccc0321110b90527.png" data-original-src="https://miro.medium.com/v2/resize:fit:802/format:webp/1*69omA80qwLDyNXKRee4BZg.png"/></div><p class="lr ls gj gh gi lt lu bd b be z dk translated">RReLU数学定义</p></figure><p id="8da8" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在RRELU中，当输入值为正值时，转换将输出给定的正值。但是，如果是负的，那么输入将乘以<em class="ng"> a </em>，其中<em class="ng"> a </em>是从均匀分布<em class="ng"> N </em>(下，上)中随机抽取的。</p><p id="5a51" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在python中实现RReLU激活函数的方法如下:</p><pre class="lg lh li lj gt nv nw nx ny aw nz bi"><span id="0bec" class="nh lw it nw b gy oa ob l oc od">import numpy as np</span><span id="8e9e" class="nh lw it nw b gy oe ob l oc od">arr_before = np.array([-1, 1, 2])</span><span id="3962" class="nh lw it nw b gy oe ob l oc od"># If x&gt;0 returns x, else returns x*sampled_a<br/>def rrelu(x, lower=0.125, upper=0.333):<br/>    sampled_a = np.random.uniform(lower, upper)<br/>    x = np.where(x&gt;0, x, x*sampled_a)<br/>    return x</span><span id="3fc4" class="nh lw it nw b gy oe ob l oc od">arr_after = rrelu(arr_before)</span><span id="9672" class="nh lw it nw b gy oe ob l oc od">arr_after<br/>#array([-0.225, 1., 2.])</span></pre><p id="b976" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在PyTorch中，您可以轻松地调用RReLU激活函数。</p><pre class="lg lh li lj gt nv nw nx ny aw nz bi"><span id="64a9" class="nh lw it nw b gy oa ob l oc od">import torch.nn</span><span id="b0a2" class="nh lw it nw b gy oe ob l oc od">neg_slope=0.01<br/>rrelu = nn.RReLU(0.125, 0.333) #Pass in the lower and upper values<br/>input = torch.randn(2)<br/>output = rrelu(input)</span></pre><h1 id="1598" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">2.介于0和1之间</h1><p id="444d" class="pw-post-body-paragraph ki kj it kk b kl mn ju kn ko mo jx kq kr mp kt ku kv mq kx ky kz mr lb lc ld im bi translated">如果您希望输出介于0和1之间的值(对于问题的概率分类类型很有用)，那么Softmin激活函数将是合适的，这取决于下面讨论的一些注意事项。</p><h2 id="2639" class="nh lw it bd lx ni nj dn mb nk nl dp mf kr nm nn mh kv no np mj kz nq nr ml ns bi translated">a.Softmin</h2><p id="dde9" class="pw-post-body-paragraph ki kj it kk b kl mn ju kn ko mo jx kq kr mp kt ku kv mq kx ky kz mr lb lc ld im bi translated">Softmin类似于softmax激活函数，因为每个元素的输出位于0和1之间的范围内(即[0,1]).区别在于softmin将输入元素与负的 1相乘，如下所示:</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi og"><img src="../Images/a0fdaff4b963ac5a64dc9aa8b0d472fa.png" data-original-src="https://miro.medium.com/v2/resize:fit:760/format:webp/1*fBfk_EIOvJ2xmpoeSkPMKQ.png"/></div><p class="lr ls gj gh gi lt lu bd b be z dk translated">软最小数学定义</p></figure><p id="49dc" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">与Softmax相比，您何时使用Softmin？这是您希望检测到更多负值的时候。例如，如果您有一个在一组图像中识别非狗的分类任务，模型在识别狗方面的置信度越低，您的分类性能就越好。</p><p id="8d07" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在python中实现Softmin函数的方法如下:</p><pre class="lg lh li lj gt nv nw nx ny aw nz bi"><span id="19f5" class="nh lw it nw b gy oa ob l oc od">import numpy as np</span><span id="c58a" class="nh lw it nw b gy oe ob l oc od">arr_before = np.array([-1, 1, 2])</span><span id="97db" class="nh lw it nw b gy oe ob l oc od">def softmin(x):<br/>    numerator = np.exp(-x)<br/>    denominator = np.sum(np.exp(-x))<br/>    x = numerator / denominator<br/>    return x</span><span id="7ba2" class="nh lw it nw b gy oe ob l oc od">arr_after = softmin(arr_before)</span><span id="93cf" class="nh lw it nw b gy oe ob l oc od">arr_after<br/>#array([0.84379473, 0.1141952 , 0.04201007]) #Sums up to 1</span></pre><p id="0e5e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在PyTorch中，您可以轻松调用Softmin激活函数。</p><pre class="lg lh li lj gt nv nw nx ny aw nz bi"><span id="670b" class="nh lw it nw b gy oa ob l oc od">import torch.nn</span><span id="097a" class="nh lw it nw b gy oe ob l oc od">softmin = nn.Softmin()<br/>input = torch.randn(2, 3)<br/>output = softmin(input)</span></pre><h1 id="711a" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">3.小于0</h1><p id="df03" class="pw-post-body-paragraph ki kj it kk b kl mn ju kn ko mo jx kq kr mp kt ku kv mq kx ky kz mr lb lc ld im bi translated">如果您希望将值转换为小于0的值，那么LogSoftmax和LogSigmoid可能是合适的激活函数。</p><h2 id="4104" class="nh lw it bd lx ni nj dn mb nk nl dp mf kr nm nn mh kv no np mj kz nq nr ml ns bi translated">a.LogSoftmax</h2><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/fd738b9d51a12a2821c5e3fa6cc53f02.png" data-original-src="https://miro.medium.com/v2/resize:fit:942/format:webp/1*Up2F7On2rFR_seocMlAUyA.png"/></div><p class="lr ls gj gh gi lt lu bd b be z dk translated">LogSoftmax数学定义</p></figure><p id="9dbb" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">与Softmax类似，LogSoftmax将日志操作应用于Softmax函数。除了数值稳定性之外，还有一个重要的优点:与Softmax相比，这个激活函数<strong class="kk iu">严重惩罚了</strong>错误的分类预测。</p><p id="7c98" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在python中实现LogSoftmax函数的方法如下:</p><pre class="lg lh li lj gt nv nw nx ny aw nz bi"><span id="11b7" class="nh lw it nw b gy oa ob l oc od">import numpy as np</span><span id="a01e" class="nh lw it nw b gy oe ob l oc od">arr_before = np.array([-1, 1, 2])</span><span id="f39d" class="nh lw it nw b gy oe ob l oc od">def logsoftmax(x):<br/>    numerator = np.exp(x)<br/>    denominator = np.sum(np.exp(x))<br/>    x = numerator / denominator</span><span id="27e0" class="nh lw it nw b gy oe ob l oc od">    log_x = np.log(x)<br/>    return log_x</span><span id="c7f1" class="nh lw it nw b gy oe ob l oc od">arr_after = logsoftmax(arr_before)</span><span id="77f5" class="nh lw it nw b gy oe ob l oc od">arr_after<br/>#array([-3.34901222, -1.34901222, -0.34901222])</span></pre><p id="ac75" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">并且在PyTorch中，可以很容易地调用LogSoftmax激活函数。</p><pre class="lg lh li lj gt nv nw nx ny aw nz bi"><span id="b3df" class="nh lw it nw b gy oa ob l oc od">import torch.nn</span><span id="5219" class="nh lw it nw b gy oe ob l oc od">logsoftmax = nn.LogSoftmax()<br/>input = torch.randn(2)<br/>output = logsoftmax(input)</span></pre><h2 id="3cc4" class="nh lw it bd lx ni nj dn mb nk nl dp mf kr nm nn mh kv no np mj kz nq nr ml ns bi translated">b.对数乙状结肠</h2><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/f0d0a5e24013c0de5804f957c105821a.png" data-original-src="https://miro.medium.com/v2/resize:fit:942/format:webp/1*9MzEJmg1ZarCtTX3pcz2jQ.png"/></div><p class="lr ls gj gh gi lt lu bd b be z dk translated">对数数学定义</p></figure><p id="c09e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">类似于LogSoftmax对Softmax，LogSigmoid将log运算符应用于Sigmoid函数。从图形上看，LogSigmoid具有以下转换行为，将输出限制为(-inf，0)。</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/919060e0e8cfb2a3902c3ea39144c6ad.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/0*ylh8EWl2DqtmEfyN.png"/></div><p class="lr ls gj gh gi lt lu bd b be z dk translated">对数图形表示</p></figure><p id="a9cf" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在python中实现LogSigmoid函数的方法如下:</p><pre class="lg lh li lj gt nv nw nx ny aw nz bi"><span id="599c" class="nh lw it nw b gy oa ob l oc od">import numpy as np</span><span id="21af" class="nh lw it nw b gy oe ob l oc od">arr_before = np.array([-1, 1, 2])</span><span id="972d" class="nh lw it nw b gy oe ob l oc od">def logsigmoid(x):<br/>    x = 1 / (1 + np.exp(-x))<br/>    return np.log(x)</span><span id="42be" class="nh lw it nw b gy oe ob l oc od">arr_after = logsigmoid(arr_before)</span><span id="15fd" class="nh lw it nw b gy oe ob l oc od">arr_after<br/>#array([-1.31326169, -0.31326169, -0.12692801])</span></pre><p id="e7be" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">并且在PyTorch中，可以很容易地调用LogSigmoid激活函数。</p><pre class="lg lh li lj gt nv nw nx ny aw nz bi"><span id="c215" class="nh lw it nw b gy oa ob l oc od">import torch.nn</span><span id="787d" class="nh lw it nw b gy oe ob l oc od">logsigmoid = nn.LogSigmoid()<br/>input = torch.randn(2)<br/>output = logsigmoid(input)</span></pre><h1 id="c421" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">结论</h1><p id="0284" class="pw-post-body-paragraph ki kj it kk b kl mn ju kn ko mo jx kq kr mp kt ku kv mq kx ky kz mr lb lc ld im bi translated">这篇文章继续讨论PyTorch中一些更健壮但不常用的激活函数的数学定义、图形表示和实际实现。如果你受益匪浅，请分享并为这篇文章鼓掌！</p></div><div class="ab cl oi oj hx ok" role="separator"><span class="ol bw bk om on oo"/><span class="ol bw bk om on oo"/><span class="ol bw bk om on"/></div><div class="im in io ip iq"><p id="5702" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu"> <em class="ng">做订阅我的邮件简讯:</em></strong><a class="ae le" href="https://tinyurl.com/2npw2fnz" rel="noopener ugc nofollow" target="_blank"><em class="ng">【https://tinyurl.com/2npw2fnz】</em></a><em class="ng"/><strong class="kk iu"><em class="ng">在这里我定期用通俗易懂的英语和漂亮的可视化总结AI研究论文。</em> </strong></p></div></div>    
</body>
</html>