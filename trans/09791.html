<html>
<head>
<title>K-means clustering: find my tribe!</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">K-means聚类:找到我的部落！</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/k-means-clustering-find-my-tribe-61460d3bc952?source=collection_archive---------30-----------------------#2021-09-13">https://towardsdatascience.com/k-means-clustering-find-my-tribe-61460d3bc952?source=collection_archive---------30-----------------------#2021-09-13</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="da7a" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">它是如何工作的？我怎样才能让它跑得更快？</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/52d0319e8d1b5556cde3f3b17b3ac157.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*v1bunirsH7SOeFpam-rXbg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">(图片由作者提供)</p></figure><p id="52d1" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">K-means聚类在需要发现大量样本之间关系的数据分析问题中有许多潜在的应用。该技术背后的概念非常简单，因此也非常灵活，它可以单独用于许多可能的配置，或者与其他机器学习技术结合使用。让我们来看看K-means聚类是如何工作的，以及一些可能的改进方法。</p></div><div class="ab cl lu lv hx lw" role="separator"><span class="lx bw bk ly lz ma"/><span class="lx bw bk ly lz ma"/><span class="lx bw bk ly lz"/></div><div class="im in io ip iq"><h2 id="02b1" class="mb mc it bd md me mf dn mg mh mi dp mj lh mk ml mm ll mn mo mp lp mq mr ms mt bi translated">什么是K-means聚类？</h2><blockquote class="mu mv mw"><p id="7466" class="ky kz mx la b lb lc ju ld le lf jx lg my li lj lk mz lm ln lo na lq lr ls lt im bi translated">查找隐藏信息</p></blockquote><p id="e804" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">K-means聚类的核心是一种试图根据某些规则对样本数据进行分类的算法。如果我们的样本数据是实数的向量，最简单的标准是数据之间的距离。</p><p id="a90f" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">假设我在二维空间中有4个样本数据点:</p><p id="7c95" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi">(0, 0), (0, 1), (0, 3), (0, 4)</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nb"><img src="../Images/60ca2af397bdafb660e936dea31f0174.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FsZ0xNkn40sN33Z0u89pZQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">数据点:(0，0)，(0，1)，(0，3)，(0，4)(图片由作者提供)</p></figure><p id="4291" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">还有假设我知道我有两个组，我怎么找到两个组的中心和每个组的成员数据点？</p><p id="be12" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">第一步是为每组选择两个起点。在这个例子中，很容易观察，比如说，选择(0，0)作为第一组的起点，选择(0，3)作为第二组的起点。</p><p id="21ce" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">但是我们不要这样做，因为那样会使这个例子太简单。</p><p id="6650" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">假设我实际上选了(0，0)和(0，1)作为起点，(0，0)代表组0，(0，1)代表组1。下一步是将四个数据点分配给两组中的每一组。我们通过计算数据点到每个组的距离，并将每个数据点分配到最近的一个组来实现这一点。</p><p id="d04c" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">从数据点(0，0)开始，它的最近点显然是(0，0)，距离为0，所以数据点(0，0)被分配到组0。移动到第二个数据点，(0，1)，它更接近组1，所以将其分配到组1。在检查完每个数据点后，我们有:</p><pre class="kj kk kl km gt nc nd ne nf aw ng bi"><span id="df26" class="mb mc it nd b gy nh ni l nj nk">group 0:<br/>    data points = (0, 0)<br/>group 1:<br/>    data points = (0, 1), (0, 3), (0, 4)</span></pre><p id="4376" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">既然我们已经将数据点分配给了每个组，那么需要更新每个组的平均值。组0的平均值保持在(0，0)不变，但是组1的平均值现在是:</p><pre class="kj kk kl km gt nc nd ne nf aw ng bi"><span id="364c" class="mb mc it nd b gy nh ni l nj nk">mean of group 1 = [(0, 1) + (0, 3) + (0, 4)] / 3 = (0, 2.333...)</span></pre><p id="4111" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们有了新的团队:</p><pre class="kj kk kl km gt nc nd ne nf aw ng bi"><span id="2031" class="mb mc it nd b gy nh ni l nj nk">group 0:<br/>    mean = (0, 0)<br/>group 1:<br/>    mean = (0, 2.333...)</span></pre><p id="8dcb" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">但是现在我们已经更新了组均值，需要重新分配每个组的数据点，以确保它们仍然最接近其组均值:</p><pre class="kj kk kl km gt nc nd ne nf aw ng bi"><span id="6307" class="mb mc it nd b gy nh ni l nj nk">(0, 0): closest to (0, 0)<br/>(0, 1): closest to (0, 0)!!<br/>(0, 3): closest to (0, 2.333...)<br/>(0, 4): closest to (0, 2.333...)</span></pre><p id="fb69" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">请注意，点(0，1)不再最接近组1的平均值，因此需要将其重新分配给组0:</p><pre class="kj kk kl km gt nc nd ne nf aw ng bi"><span id="b37d" class="mb mc it nd b gy nh ni l nj nk">group 0:<br/>    data points = (0, 0), (0, 1)<br/>group 1:<br/>    data points = (0, 3), (0, 4)</span></pre><p id="67e6" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在将数据点分配到组和更新组均值之间交替，我们最终达到稳定状态，其中不再有数据点切换组，这就是我们最终的K均值聚类:</p><pre class="kj kk kl km gt nc nd ne nf aw ng bi"><span id="2c9f" class="mb mc it nd b gy nh ni l nj nk">group 0:<br/>    mean = (0, 0.5)<br/>    data points = (0, 0), (0, 1)<br/>group 1:<br/>    mean = (0, 3.5)<br/>    data points = (0, 3), (0, 4)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nb"><img src="../Images/3665d112c30a9027c5a5548081a83d60.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hsNYcaqQo7x8DjhaGea1yw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">(图片由作者提供)</p></figure></div><div class="ab cl lu lv hx lw" role="separator"><span class="lx bw bk ly lz ma"/><span class="lx bw bk ly lz ma"/><span class="lx bw bk ly lz"/></div><div class="im in io ip iq"><h2 id="a3f1" class="mb mc it bd md me mf dn mg mh mi dp mj lh mk ml mm ll mn mo mp lp mq mr ms mt bi translated">结构</h2><blockquote class="mu mv mw"><p id="8f59" class="ky kz mx la b lb lc ju ld le lf jx lg my li lj lk mz lm ln lo na lq lr ls lt im bi translated">定义算法的结构</p></blockquote><p id="0f2c" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">从上面的例子中我们可以看出，算法可以分为两个部分，初始化和聚类。让我们来定义算法的过程:</p><pre class="kj kk kl km gt nc nd ne nf aw ng bi"><span id="3be5" class="mb mc it nd b gy nh ni l nj nk">import copy<br/>import numpy as np<br/><br/><br/>class KMean:<br/><br/>    def __init__(self, samples):<br/><br/>        self._samples = np.array(samples)<br/>        self._clusters = [-1] * len(samples)<br/>        self._means = None<br/>        self._processed = False<br/><br/>    @property<br/>    def samples(self):<br/>        return self._samples<br/><br/>    @property<br/>    def means(self):<br/>        return self._means<br/><br/>    @means.setter<br/>    def means(self, new):<br/>        self._means = new<br/><br/>    @property<br/>    def clusters(self):<br/>        return self._clusters<br/><br/>    @clusters.setter<br/>    def clusters(self, new):<br/>        self._clusters = new<br/><br/>    def init(self):<br/>        raise ValueError("init not implemented")<br/><br/>    def cluster(self):<br/>        raise ValueError("cluster not implemented")<br/><br/>    @staticmethod<br/>    def _cls_of(func):<br/>        return func.__func__.__qualname__.split('.')[0]<br/><br/>    def process(self):<br/>        if self._processed:<br/>            print("Process already completed")<br/>            return<br/><br/>        print(f"Initializing with {self._cls_of(self.init)}")<br/>        <strong class="nd iu">self.init()</strong><br/>        print(f"Initial means: {self.means}")<br/>        print(f"Clustering with {self._cls_of(self.cluster)}")<br/>        <strong class="nd iu">self.cluster()</strong><br/>        print(f"Processing {self.__class__.__name__} complete")<br/>        self._processed = True</span></pre></div><div class="ab cl lu lv hx lw" role="separator"><span class="lx bw bk ly lz ma"/><span class="lx bw bk ly lz ma"/><span class="lx bw bk ly lz"/></div><div class="im in io ip iq"><h2 id="2d40" class="mb mc it bd md me mf dn mg mh mi dp mj lh mk ml mm ll mn mo mp lp mq mr ms mt bi translated">初始化</h2><blockquote class="mu mv mw"><p id="59c4" class="ky kz mx la b lb lc ju ld le lf jx lg my li lj lk mz lm ln lo na lq lr ls lt im bi translated">从哪里开始</p></blockquote><p id="81eb" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">初始化可能非常简单，所以接下来让我们来研究一下。<br/>最简单的方法是手动输入你想要的起点:</p><pre class="kj kk kl km gt nc nd ne nf aw ng bi"><span id="428b" class="mb mc it nd b gy nh ni l nj nk">class InitInput(KMean):<br/>    def __init__(self, input_means, **kwargs):<br/>        self._input_means = input_means<br/>        super().__init__(**kwargs)<br/><br/>    def init(self):<br/>        self.means = self._input_means</span></pre><p id="ae26" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">测试它:</p><pre class="kj kk kl km gt nc nd ne nf aw ng bi"><span id="3355" class="mb mc it nd b gy nh ni l nj nk">init1 = InitInput(input_means=[[0, 0], [0, 1]],<br/>                  samples=[(0, 0), (0, 1), (0, 3), (0, 4)])<br/>init1.init()<br/>print(init1.means)<br/>"""<br/>[[0, 0], [0, 1]]<br/>"""</span></pre><p id="8a86" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">如果你知道你想从哪里开始或者当你在做研究的时候，这是非常好的。但是，当您的样本量很大，并且不知道数据点是否随机排列时，最好随机选择起始点:</p><pre class="kj kk kl km gt nc nd ne nf aw ng bi"><span id="a99f" class="mb mc it nd b gy nh ni l nj nk">class InitForgy(KMean):<br/>    def __init__(self, k, **kwargs):<br/>        self._k = k<br/>        super().__init__(**kwargs)<br/><br/>    def init(self):<br/>        indices = np.random.choice(<br/>            len(self.samples),<br/>            size=self._k,<br/>            replace=False,<br/>        )<br/>        self.means = [copy.deepcopy(self.samples[idx]) for idx in indices]</span></pre><p id="1c91" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这种方法被称为Forgy方法，测试它:</p><pre class="kj kk kl km gt nc nd ne nf aw ng bi"><span id="7f0f" class="mb mc it nd b gy nh ni l nj nk">init2 = InitForgy(k=2,<br/>                  samples=[(0, 0), (0, 1), (0, 3), (0, 4)])<br/>init2.init()<br/>print(init2.means)<br/>"""<br/>[array([0, 1]), array([0, 3])]<br/>"""</span></pre></div><div class="ab cl lu lv hx lw" role="separator"><span class="lx bw bk ly lz ma"/><span class="lx bw bk ly lz ma"/><span class="lx bw bk ly lz"/></div><div class="im in io ip iq"><h2 id="a42d" class="mb mc it bd md me mf dn mg mh mi dp mj lh mk ml mm ll mn mo mp lp mq mr ms mt bi translated">使聚集</h2><blockquote class="mu mv mw"><p id="b1f2" class="ky kz mx la b lb lc ju ld le lf jx lg my li lj lk mz lm ln lo na lq lr ls lt im bi translated">如何找到最佳聚类</p></blockquote><p id="ef26" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">对样本数据进行聚类包括在将数据点分配到组和更新组均值之间进行交替，最简单的方法是在算法的每次迭代期间，遍历每个点，计算该点到每个组均值的距离，并将数据点分配到最近的组。这就是所谓的劳埃德方法:</p><pre class="kj kk kl km gt nc nd ne nf aw ng bi"><span id="cc06" class="mb mc it nd b gy nh ni l nj nk">class ClusterLloyd(KMean):<br/>    def __init__(self, tolerance=0, **kwargs):<br/>        self._tolerance = tolerance<br/><br/>        self._history = []<br/>        super().__init__(**kwargs)<br/><br/>    @property<br/>    def history(self):<br/>        return self._history<br/><br/>    @staticmethod<br/>    def _calc_dist(sample, mean):<br/>        diff = sample - mean<br/>        dist = np.dot(diff, diff)<br/>        return dist<br/><br/>    def _calc_cluster(self, sample):<br/><br/>        <strong class="nd iu"># loop through each cluster to find the closest cluster<br/>        idx = None<br/>        dist = None<br/>        for i, mean in enumerate(self.means):<br/>            cur_dist = self._calc_dist(sample, mean)<br/>            if idx is None or cur_dist &lt; dist:<br/>                idx = i<br/>                dist = cur_dist</strong><br/><br/>        return idx<br/><br/>    def _calc_shift(self, new_clusters):<br/><br/>        shifted = 0<br/>        for i, c in enumerate(new_clusters):<br/>            if c != self.clusters[i]:<br/>                shifted = shifted + 1<br/><br/>        shift = shifted / len(new_clusters)<br/>        return shift<br/><br/>    def _calc_means(self, new_clusters):<br/><br/>        means_map = {}<br/><br/>        for i, cluster in enumerate(new_clusters):<br/>            if cluster not in means_map:<br/>                means_map[cluster] = [self.samples[i], 1]<br/>            else:<br/>                curr = means_map[cluster]<br/>                curr[0] = curr[0] + self.samples[i]<br/>                curr[1] = curr[1] + 1<br/><br/>        means = copy.deepcopy(self.means)<br/><br/>        for k, v in means_map.items():<br/>            new_mean = v[0] / v[1]<br/>            means[k] = new_mean<br/><br/>        return means<br/><br/>    def _update(self, new_clusters):<br/><br/>        new_means = self._calc_means(new_clusters)<br/>        self._history.append((new_clusters, new_means))<br/><br/>        self.clusters = new_clusters<br/>        self.means = new_means<br/><br/>    def cluster(self):<br/>        self._history.append((self.clusters, self.means))<br/>        while True:<br/>            new_clusters = []<br/>            <strong class="nd iu"># loop through each data point<br/>            for sample in self.samples:<br/>                new_clusters.append(self._calc_cluster(sample))</strong><br/><br/>            shift = self._calc_shift(new_clusters)<br/>            if shift &lt;= self._tolerance:<br/>                break<br/><br/>            self._update(new_clusters)</span></pre><p id="88ec" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">测试它:</p><pre class="kj kk kl km gt nc nd ne nf aw ng bi"><span id="ebdb" class="mb mc it nd b gy nh ni l nj nk">class InputLloyd(InitInput, ClusterLloyd):<br/>    pass</span><span id="ccca" class="mb mc it nd b gy nl ni l nj nk">km1 = InputLloyd(input_means=[[0, 0], [0, 1]],<br/>                 samples=[(0, 0), (0, 1), (0, 3), (0, 4)])<br/>km1.process()<br/>print(f"clusters: {km1.clusters}")<br/>print(f"means: {km1.means}")<br/>"""<br/>Initializing with InitInput<br/>Initial means: [[0, 0], [0, 1]]<br/>Clustering with ClusterLloyd<br/>Processing InputLloyd complete<br/>clusters: [0, 0, 1, 1]<br/>means: [array([0. , 0.5]), array([0. , 3.5])]<br/>"""</span></pre><p id="2e65" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们发现我们的集群:</p><pre class="kj kk kl km gt nc nd ne nf aw ng bi"><span id="cfdb" class="mb mc it nd b gy nh ni l nj nk">"""<br/>clusters: [0, 0, 1, 1]<br/>group 0: (0, 0), (0, 1)<br/>group 1: (0, 3), (0, 4)<br/>"""</span></pre></div><div class="ab cl lu lv hx lw" role="separator"><span class="lx bw bk ly lz ma"/><span class="lx bw bk ly lz ma"/><span class="lx bw bk ly lz"/></div><div class="im in io ip iq"><h2 id="9683" class="mb mc it bd md me mf dn mg mh mi dp mj lh mk ml mm ll mn mo mp lp mq mr ms mt bi translated">三角不等式</h2><blockquote class="mu mv mw"><p id="85d3" class="ky kz mx la b lb lc ju ld le lf jx lg my li lj lk mz lm ln lo na lq lr ls lt im bi translated">通过巧妙的观察加速算法</p></blockquote><p id="b09d" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">劳埃德的方法很容易理解，但不是最有效的。如果考虑算法的时间复杂度，在聚类过程的每次迭代中，我们需要计算每个数据点到每个聚类均值的距离。如果我们有N个数据点和K个聚类，那将是:</p><p id="4d8a" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">~O(N * K)</p><p id="44d6" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">实际上有几种方法可以加快这个过程。</p><p id="b265" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">其中一个涉及以下观察:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nb"><img src="../Images/f5fedda78282e32f004d78b151eef080.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*95DaTyWtxmN_yovJ5g-phQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">第1组最接近的数据点(0，0)平均值为2.5(图片由作者提供)</p></figure><p id="4507" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在我们的聚类算法的最后一次迭代中，组0的平均值是(0，0.5)，组1的平均值是(0，3.5)，点(0，0)到其当前分配的组0的距离是0.5。如果我们已经计算了聚类的距离平均值，在这种情况下，3，我们实际上不需要计算点(0，0)到组1的平均值(0，3.5)的距离，就可以知道点(0，0)不可能更接近组1。</p><p id="a9b1" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这是为什么呢？如上图的第二部分所示，两个组的距离为3，而(0，0)到组0的距离为0.5。所以，无论点(0，0)在离0组0.5的轨道上的什么地方，都不会比离1组2.5近！</p><p id="5050" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这本质上是三角不等式:</p><blockquote class="mu mv mw"><p id="2f7d" class="ky kz mx la b lb lc ju ld le lf jx lg my li lj lk mz lm ln lo na lq lr ls lt im bi translated">三角形的一边总是比其他两边的总和短。</p></blockquote><p id="6cbf" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">根据这一观察，我们可以更新劳埃德的方法，使其运行得更快:</p><pre class="kj kk kl km gt nc nd ne nf aw ng bi"><span id="5210" class="mb mc it nd b gy nh ni l nj nk">class ClusterElkan1(KMean):<br/>    def __init__(self, tolerance=0, **kwargs):<br/>        self._tolerance = tolerance<br/>        self._mean_maps = []<br/>        self._history = []<br/>        super().__init__(**kwargs)<br/><br/>    @property<br/>    def history(self):<br/>        return self._history<br/><br/>    @property<br/>    def mean_maps(self):<br/>        return self._mean_maps<br/><br/>    @staticmethod<br/>    def _calc_dist(sample, mean):<br/>        diff = np.array(sample) - np.array(mean)<br/>        dist = np.dot(diff, diff)<br/>        # note dist is actually distance squared<br/>        # no need to take the square root<br/>        return dist<br/><br/>    def _calc_cluster(self, sample_idx):<br/><br/>        idx = None<br/>        dist = None<br/>        for i, mean in enumerate(self.means):<br/><br/>            <strong class="nd iu"># check if need to calc dist<br/>            if dist is not None:<br/>                cluster_dist = self.mean_maps[-1][(idx, i)]<br/>                if dist * 4 &lt; cluster_dist:<br/>                    print(f"no need to recalculate dist for sample {sample_idx} with mean {i}")<br/>                    continue</strong><br/><br/>            sample = self.samples[sample_idx]<br/>            cur_dist = self._calc_dist(sample, mean)<br/>            if dist is None or cur_dist &lt; dist:<br/>                idx = i<br/>                dist = cur_dist<br/><br/>        return idx<br/><br/>    def _calc_shift(self, new_clusters):<br/><br/>        shifted = 0<br/>        for i, c in enumerate(new_clusters):<br/>            if c != self.clusters[i]:<br/>                shifted = shifted + 1<br/><br/>        shift = shifted / len(new_clusters)<br/>        return shift<br/><br/>    def _calc_means(self, new_clusters):<br/><br/>        means_map = {}<br/><br/>        for i, cluster in enumerate(new_clusters):<br/>            if cluster not in means_map:<br/>                means_map[cluster] = [self.samples[i], 1]<br/>            else:<br/>                curr = means_map[cluster]<br/>                curr[0] = curr[0] + self.samples[i]<br/>                curr[1] = curr[1] + 1<br/><br/>        means = copy.deepcopy(self.means)<br/><br/>        for k, v in means_map.items():<br/>            new_mean = v[0] / v[1]<br/>            means[k] = new_mean<br/><br/>        return means<br/><br/>    def _update(self, new_clusters):<br/><br/>        new_means = self._calc_means(new_clusters)<br/>        self.clusters = new_clusters<br/>        self.means = new_means<br/>        self._update_history()<br/><br/>    def _update_mean_map(self):<br/>        # calculate mean distances<br/>        mean_map = {}<br/>        for i, c in enumerate(self.means):<br/>            for j, c_p in enumerate(self.means[i + 1:]):<br/>                dist = self._calc_dist(c, c_p)<br/>                mean_map[(i, j + i + 1)] = dist<br/>                mean_map[(j + i + 1, i)] = dist<br/>        self._mean_maps.append(mean_map)<br/><br/>    def _update_history(self):<br/>        self._history.append((self.clusters, self.means))<br/>        self._update_mean_map()<br/><br/>    def cluster(self):<br/>        self._update_history()<br/>        while True:<br/>            new_clusters = []<br/>            for i in range(len(self.samples)):<br/>                new_clusters.append(self._calc_cluster(i))<br/><br/>            shift = self._calc_shift(new_clusters)<br/>            if shift &lt;= self._tolerance:<br/>                break<br/><br/>            self._update(new_clusters)<br/><br/><br/>class InputElkan1(InitInput, ClusterElkan1):<br/>    pass</span></pre></div><div class="ab cl lu lv hx lw" role="separator"><span class="lx bw bk ly lz ma"/><span class="lx bw bk ly lz ma"/><span class="lx bw bk ly lz"/></div><div class="im in io ip iq"><h2 id="032c" class="mb mc it bd md me mf dn mg mh mi dp mj lh mk ml mm ll mn mo mp lp mq mr ms mt bi translated">下限上限</h2><blockquote class="mu mv mw"><p id="9abc" class="ky kz mx la b lb lc ju ld le lf jx lg my li lj lk mz lm ln lo na lq lr ls lt im bi translated">又一次加速</p></blockquote><p id="9968" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这很聪明。然而，还有更聪明的方法来改进这个算法。</p><p id="f6e9" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">看看聚类算法的最后两次迭代:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nb"><img src="../Images/eba0ca2c38237a50a50290f693ce0b21.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nHPZ6V1zOJ2oCeiaSL5Tyw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">第1组均值偏移0.5，第2组均值偏移1.166…(图片由作者提供)</p></figure><p id="0eae" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">第0组的平均值移动了0.5，第1组的平均值移动了1.166…</p><p id="b778" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们如何利用这些信息来避免多余的计算？</p><p id="3664" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">嗯，我们知道点(0，0)在移动之前与组0均值的距离为0，如果组0均值移动了0.5，那么点(0，0)和组0之间的距离最多为0.5。</p><p id="0cd4" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">如果我们也知道点(0，0)与组1的平均值的距离为2.333…，那么，在移位之后，点(0，0)与组1的平均值的距离至少为1.166…(2.333 — 1.166)。</p><p id="b4bf" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">如果点(0，0)到组0的距离最多为0.5，而点(0，0)到组1的距离至少为1.166…，那么点(0，0)必须更靠近组0！不需要计算到组的距离意味着！</p><p id="c41a" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">加上这一改进，我们有了迄今为止最有效的算法:</p><pre class="kj kk kl km gt nc nd ne nf aw ng bi"><span id="f7be" class="mb mc it nd b gy nh ni l nj nk">class ClusterElkan2(KMean):<br/>    def __init__(self, tolerance=0, **kwargs):<br/>        self._tolerance = tolerance<br/>        self._mean_maps = []<br/>        self._history = []<br/><br/>        self._mean_shifts = None<br/>        self._bounds = None<br/><br/>        super().__init__(**kwargs)<br/><br/>    def _setup_cluster(self):<br/>        # the amount of shift of means from previous iteration<br/>        self._mean_shifts = [0] * len(self.means)<br/><br/>        # a list of dict, each dict is bounds (ub for assigned cluster, lb for others) of a sample<br/>        self._bounds = [{j: None for j in range(len(self.means))} for i in range(len(self.samples))]<br/><br/>    @property<br/>    def history(self):<br/>        return self._history<br/><br/>    @property<br/>    def mean_maps(self):<br/>        return self._mean_maps<br/><br/>    @property<br/>    def mean_shifts(self):<br/>        return self._mean_shifts<br/><br/>    @mean_shifts.setter<br/>    def mean_shifts(self, new_shifts):<br/>        self._mean_shifts = new_shifts<br/><br/>    @property<br/>    def bounds(self):<br/>        return self._bounds<br/><br/>    @staticmethod<br/>    def _calc_dist(sample, mean):<br/>        diff = np.array(sample) - np.array(mean)<br/>        dist = np.dot(diff, diff)<br/>        return dist<br/><br/>    def _calc_cluster(self, sample_idx):<br/><br/>        # modify to always start with the previously assigned cluster<br/>        # modify to use lb and ub use one structure<br/>        idx = self.clusters[sample_idx]<br/>        dist = self.bounds[sample_idx][idx] + self.mean_shifts[idx] \<br/>            if idx &gt;= 0 and self.bounds[sample_idx][idx] is not None \<br/>            else None<br/>        if dist is not None:<br/>            self.bounds[sample_idx][idx] = dist<br/>        for i, mean in enumerate(self.means):<br/><br/>            if i != idx:<br/>                # looping through a mean<br/><strong class="nd iu">                # update lb if needed<br/>                lb = self.bounds[sample_idx][i] - self.mean_shifts[i] \<br/>                    if self.bounds[sample_idx][i] is not None \<br/>                    else None<br/>                if dist is not None and lb is not None:<br/>                    if dist &lt; lb:<br/>                        print(f"no need to recalculate dist for sample {sample_idx} with mean {i} because of lower bound")<br/>                        self.bounds[sample_idx][i] = lb<br/>                        continue</strong><br/><br/><strong class="nd iu">                # check mean dist<br/>                if dist is not None:<br/>                    cluster_dist = self.mean_maps[-1][(idx, i)]<br/>                    if dist * 4 &lt; cluster_dist:<br/>                        print(f"no need to recalculate dist for sample {sample_idx} with mean {i} because of mean dist")<br/>                        continue</strong><br/><br/>                sample = self.samples[sample_idx]<br/>                cur_dist = self._calc_dist(sample, mean)<br/>                self.bounds[sample_idx][i] = cur_dist<br/>                if dist is None or cur_dist &lt; dist:<br/>                    idx = i<br/>                    dist = cur_dist<br/><br/>        return idx<br/><br/>    def _calc_shift(self, new_clusters):<br/><br/>        shifted = 0<br/>        for i, c in enumerate(new_clusters):<br/>            if c != self.clusters[i]:<br/>                shifted = shifted + 1<br/><br/>        shift = shifted / len(new_clusters)<br/>        return shift<br/><br/>    def _calc_means(self, new_clusters):<br/><br/>        means_map = {}<br/><br/>        for i, cluster in enumerate(new_clusters):<br/>            if cluster not in means_map:<br/>                means_map[cluster] = [self.samples[i], 1]<br/>            else:<br/>                curr = means_map[cluster]<br/>                curr[0] = curr[0] + self.samples[i]<br/>                curr[1] = curr[1] + 1<br/><br/>        means = copy.deepcopy(self.means)<br/><br/>        for k, v in means_map.items():<br/>            new_mean = v[0] / v[1]<br/>            means[k] = new_mean<br/><br/>        return means<br/><br/>    def _update(self, new_clusters):<br/><br/>        new_means = self._calc_means(new_clusters)<br/>        self.clusters = new_clusters<br/>        self.mean_shifts = [self._calc_dist(self.means[i], new_means[i]) for i in range(len(self.means))]<br/>        self.means = new_means<br/>        self._update_history()<br/><br/>    def _update_mean_map(self):<br/>        # calculate mean distances<br/>        mean_map = {}<br/>        for i, c in enumerate(self.means):<br/>            for j, c_p in enumerate(self.means[i + 1:]):<br/>                dist = self._calc_dist(c, c_p)<br/>                mean_map[(i, j + i + 1)] = dist<br/>                mean_map[(j + i + 1, i)] = dist<br/>        self._mean_maps.append(mean_map)<br/><br/>    def _update_history(self):<br/>        self._history.append((self.clusters, self.means))<br/>        self._update_mean_map()<br/><br/>    def cluster(self):<br/>        self._setup_cluster()<br/>        self._update_history()<br/>        while True:<br/>            new_clusters = []<br/>            for i in range(len(self.samples)):<br/>                new_clusters.append(self._calc_cluster(i))<br/><br/>            shift = self._calc_shift(new_clusters)<br/>            if shift &lt;= self._tolerance:<br/>                break<br/><br/>            self._update(new_clusters)<br/></span><span id="8d01" class="mb mc it nd b gy nl ni l nj nk">class InputElkan2(InitInput, ClusterElkan2):<br/>    pass</span></pre><p id="28c0" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">通过这两个改进，我们可以有效地将K-mean算法在每次聚类迭代中的时间复杂度从~O(N * K)降低到~O(N)。</p></div><div class="ab cl lu lv hx lw" role="separator"><span class="lx bw bk ly lz ma"/><span class="lx bw bk ly lz ma"/><span class="lx bw bk ly lz"/></div><div class="im in io ip iq"><h2 id="6ca4" class="mb mc it bd md me mf dn mg mh mi dp mj lh mk ml mm ll mn mo mp lp mq mr ms mt bi translated">两种解决方案</h2><blockquote class="mu mv mw"><p id="cf8f" class="ky kz mx la b lb lc ju ld le lf jx lg my li lj lk mz lm ln lo na lq lr ls lt im bi translated">不保证唯一的解决方案</p></blockquote><p id="4af2" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">让我们来看看下面这个问题:</p><pre class="kj kk kl km gt nc nd ne nf aw ng bi"><span id="b696" class="mb mc it nd b gy nh ni l nj nk">data points = (0, 0), (0, 1), (0, 2), (0, 3)<br/>number of clusters = 2</span></pre><p id="ab38" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这一次，所有四个数据点的间距相等。</p><p id="7e12" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">让我们以不同的起点将我们的算法应用于它:</p><pre class="kj kk kl km gt nc nd ne nf aw ng bi"><span id="ffeb" class="mb mc it nd b gy nh ni l nj nk"># starting points [0, 0], [0, 1]<br/>km2 = InputElkan2(input_means=[[0, 0], [0, 1]], samples=[(0, 0), (0, 1), (0, 2), (0, 3)])<br/>km2.process()<br/>print("Solution 1")<br/>print(km2.clusters)<br/>print(km2.means)<br/><br/># starting points [0, 0], [0, 3]<br/>km3 = InputElkan2(input_means=[[0, 0], [0, 3]], samples=[(0, 0), (0, 1), (0, 2), (0, 3)])<br/>km3.process()<br/>print("Solution 2")<br/>print(km3.clusters)<br/>print(km3.means)<br/>"""<br/>Solution 1<br/>[0, 1, 1, 1]<br/><strong class="nd iu">[array([0., 0.]), array([0., 2.])]</strong><br/><br/>Solution 2<br/>[0, 0, 1, 1]<br/><strong class="nd iu">[array([0. , 0.5]), array([0. , 2.5])]</strong><br/>"""</span></pre><p id="25ff" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们得到两种不同的解决方案！</p><p id="a7d1" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">原因很明显，所有4个点都是等距的，解之间没有区别:</p><pre class="kj kk kl km gt nc nd ne nf aw ng bi"><span id="9181" class="mb mc it nd b gy nh ni l nj nk">"""<br/>[0, 1, 1, 1]<br/>[0, 0, 1, 1]<br/>[0, 0, 0, 1]<br/>"""</span></pre><p id="83a4" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">那么这意味着什么呢？</p><p id="fe6e" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">对我来说，正确答案是没有一个解是正确的，聚类的个数不是两个，所以只有两个起点是没有办法正确聚类的。</p><p id="aaaf" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我认为，对于这个特殊的问题，最合理的解决方案是(0，1.5)处的一个群集或者四个群集的每个点都属于它自己的群集。</p></div><div class="ab cl lu lv hx lw" role="separator"><span class="lx bw bk ly lz ma"/><span class="lx bw bk ly lz ma"/><span class="lx bw bk ly lz"/></div><div class="im in io ip iq"><h2 id="64e6" class="mb mc it bd md me mf dn mg mh mi dp mj lh mk ml mm ll mn mo mp lp mq mr ms mt bi translated">结论</h2><blockquote class="mu mv mw"><p id="64d4" class="ky kz mx la b lb lc ju ld le lf jx lg my li lj lk mz lm ln lo na lq lr ls lt im bi translated">一切都还不完美</p></blockquote><p id="2b9c" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">k-均值聚类是一个有趣的算法，可以非常有效。但是它并不完美，正如上一个例子所表明的那样。我们如何改进我们的算法来处理更复杂的情况？那是下次的故事！</p></div></div>    
</body>
</html>