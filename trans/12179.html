<html>
<head>
<title>5 Text Decoding Techniques that every “NLP Enthusiast” Must Know</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">每个“NLP爱好者”都必须知道的5种文本解码技术</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/5-text-decoding-techniques-that-every-nlp-enthusiast-must-know-6908e72f8df9?source=collection_archive---------9-----------------------#2021-12-09">https://towardsdatascience.com/5-text-decoding-techniques-that-every-nlp-enthusiast-must-know-6908e72f8df9?source=collection_archive---------9-----------------------#2021-12-09</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="5ccb" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">可视化自然语言处理中流行的文本解码方法</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/16cdb9f91288517beda6bba82d62e941.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*a6Td9j3a7AQeWqsLzmf2DQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">来自<a class="ae ky" href="https://unsplash.com/photos/W8KTS-mhFUE" rel="noopener ugc nofollow" target="_blank">源</a>的修改图像</p></figure><p id="3748" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">自动文本生成系统的最终目标是<strong class="lb iu">生成和人类书写文本一样好的文本</strong>。在NLP中，生成任务背后的核心思想是关于生成特定于特定上下文和约束的文本，其中一些是文本摘要、语言翻译、语音到文本等。一旦模型在这些任务中的任何一个上被训练，离散单元的<strong class="lb iu">质量以及最终模型生成的序列取决于人们采用的解码策略</strong>。这些技术各有利弊。</p><p id="8755" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在这篇博客中，我们将介绍一些流行的文本采样技术，如随机采样、贪婪采样、波束搜索、top-k采样和top-p采样。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi lw"><img src="../Images/2128814b9f09c5f2c0cd39229370299a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*79Z3cUWsGCUlAIhdeaVq_w.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">随机抽样技术|作者图片</p></figure><p id="c3f5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在<strong class="lb iu">随机抽样中，</strong>想法非常简单，在生成过程中的每个时间步<em class="lv"> (t) </em>，我们根据在整个词汇表的每一步生成的条件概率分布抽样一个<strong class="lb iu">随机单词。这种技术可以被认为是我们将在这个博客中讨论的所有技术中最不稳定的一种。下图<em class="lv">显示了同一— </em>的图示</strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi lx"><img src="../Images/bd6e57de91ebb38a67b085b5b73075c4.png" data-original-src="https://miro.medium.com/v2/resize:fit:904/format:webp/1*QdogIzWVxwKgHm5kTgGMJQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">序列到序列设置中的随机解码|作者图片</p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ly"><img src="../Images/0a4c8565f2e978f29b142023e886db8c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IhkykLKEJGBDJnqpwlYUJw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">贪婪解码技术|作者图片</p></figure><p id="2b55" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在<strong class="lb iu">贪婪解码</strong>中，在生成过程中的每一个时间步<em class="lv"> (t) </em>，我们选择具有最高条件概率的<strong class="lb iu">字。因为采样不允许任何随机性，所以在模型权重冻结后，该方法默认为<strong class="lb iu">确定性的</strong>。当目的是生成一个短序列时，它工作得相当好。然而，当生成更长的序列时，这种方法会陷入循环，最终生成冗余的序列。</strong></p><p id="9b2b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="lv">下图显示了同一产品的图示视图— </em></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi lz"><img src="../Images/7629f511aa5b1a2eb922613efa477f19.png" data-original-src="https://miro.medium.com/v2/resize:fit:914/format:webp/1*KGBooPGl5JIh9c_tlxRmwA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">序列到序列设置中的贪婪解码|作者图片</p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ma"><img src="../Images/7f7e5ba27a86fcd44ba164bc5b07993c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*F-ReaYWm275axM4vHkIDEg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">波束搜索技术|作者图片</p></figure><p id="a267" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在<strong class="lb iu">波束搜索中，</strong>的想法是在生成文本的同时对一个<strong class="lb iu">高似然序列</strong>进行采样。为此，我们定义了所谓的波束大小(B ),它基于在生成阶段的上一步中选择的单元，在每个时间步长(t)对Top-B单元进行采样。这样，如果我们要生成一个5个单词的序列，那么我们将得到3⁵序列，我们从中选择并返回一个具有最大可能性的序列作为我们的最终序列。同样根据<a class="ae ky" href="https://www.youtube.com/watch?v=dCORspO2yVY" rel="noopener ugc nofollow" target="_blank">的这项研究</a>，作者发现当生成的序列很短并且不是开放式的时，波束搜索工作得很好。</p><p id="c42a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">下面的<em class="lv">图像显示了同一— </em>的图示视图</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mb"><img src="../Images/0b4f01e4c4bb71472aa71c059825b499.png" data-original-src="https://miro.medium.com/v2/resize:fit:1084/format:webp/1*BL5M2_DdzDTMlEWrZAbacQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">光束搜索技术|来自<a class="ae ky" href="https://d2l.ai/chapter_recurrent-modern/beam-search.html" rel="noopener ugc nofollow" target="_blank">源</a>的修改图像</p></figure><p id="5c6c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="lv">图示为光束搜索的工作方式。</em>假设ABD和CED是模型生成的可能性值最高的两个序列。最后，返回具有较高值的值。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ma"><img src="../Images/a1daa874d9d4ee3f74f1c1ab7d9f695d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tGvm_HKr9ulcLFLQMJDJbQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">Top-k采样技术|图片由作者提供</p></figure><p id="f364" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在<strong class="lb iu"> top-K采样</strong>中，在生成过程中的每一个时间步长<em class="lv"> (t) </em>，我们首先根据该时间步长生成的条件概率分布对<strong class="lb iu"> top-k最可能的单词</strong>进行采样。我们<strong class="lb iu">只在那些前k个单词中重新分配概率质量</strong>，并最终基于条件概率从该集合中选择一个<strong class="lb iu">随机单词。这里，随机性是基于k的选择而引入的，它有助于在每一代中试验和输出不同但相关的序列。另外，<strong class="lb iu">对于k=1，这种技术的表现类似于贪婪解码</strong>。</strong></p><p id="a292" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="lv">下图显示了同一产品的图示视图— </em></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mc"><img src="../Images/6a463107deb6a6eed1ae06d887b82155.png" data-original-src="https://miro.medium.com/v2/resize:fit:1098/format:webp/1*WpV3rj36F9zD1D674Z4emg.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">序列到序列设置中的Top-k解码|图片由作者提供</p></figure><p id="8b19" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="lv">图示为k=3时top-k的工作情况。根据概率，“男孩”、“你好”和“男人”这三个词排在我们的前三名。随后我们在那个集合中重新分配那些概率，并最终做出我们的选择。</em></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi md"><img src="../Images/17d560b35b6b0923898d2ea73b383712.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0tV_wOEdZ3v4keDhYsCRAg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">细胞核取样技术|作者图片</p></figure><p id="7865" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在<strong class="lb iu"> top-p sampling中，又称核采样，在生成过程中的每个时间步<em class="lv"> (t) </em>中的</strong>，我们生成一个累计概率超过概率质量P 的<strong class="lb iu">小词子集。然后在这个集合中重新分配概率<strong class="lb iu"/>，最后，我们基于概率</strong>从那个集合中选择一个<strong class="lb iu">随机词。top-p采样允许我们结合top-p字的动态大小窗口的概念，不像top-k为每一步固定k的大小。</strong></p><p id="00e3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">下图<em class="lv">显示了同一— </em>的图示视图</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi me"><img src="../Images/fd11ca389c5b096bb815c5200991dc18.png" data-original-src="https://miro.medium.com/v2/resize:fit:1096/format:webp/1*rMvHmunohyMOOJwn1uykNQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">序列到序列设置中的Top-p解码|图片由作者提供</p></figure><p id="8920" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="lv">图示为p=0.6时top-p的工作情况。</em>单词‘男孩’和‘男人’的概率和超过阈值概率质量0.6。我们选择这两个词，用重新分配的概率创建一个后续集，并最终做出选择。</p><h1 id="8037" class="mf mg it bd mh mi mj mk ml mm mn mo mp jz mq ka mr kc ms kd mt kf mu kg mv mw bi translated">结论</h1><p id="dce2" class="pw-post-body-paragraph kz la it lb b lc mx ju le lf my jx lh li mz lk ll lm na lo lp lq nb ls lt lu im bi translated">在上面讨论的所有解码技术中，没有一个工作得最好的硬性规则。这完全取决于您的特定用例<strong class="lb iu">，您是希望有可能的和可预测的单词序列，还是希望模型在生成时勇敢而有创造性</strong>。一般来说，对于像故事生成、诗歌生成等用例<em class="lv">(生成有趣句子的可能性是无穷无尽的)</em>你可能想要尝试Top-p或Top-k，对于像翻译这样的用例<em class="lv">(对于如何将给定的句子翻译成目标语言没有太大的变化)</em>你可能想要尝试Beam search。你也可以摆弄softmax 的<strong class="lb iu">温度值，通过将温度值设置为接近0，并在此之后应用上述技术，使分布更加偏斜，在某种意义上也更有把握。</strong></p><p id="6857" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu"> <em class="lv">对NLP感兴趣？然后你可能会想看看我的</em></strong><a class="ae ky" href="https://www.youtube.com/channel/UCoz8NrwgL7U9535VNc0mRPA/" rel="noopener ugc nofollow" target="_blank"><strong class="lb iu"><em class="lv">YouTube频道</em> </strong> </a> <strong class="lb iu"> <em class="lv">在那里我解释了NLP论文和概念:</em> </strong></p><p id="c7f5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我希望你喜欢读这篇文章。如果你愿意支持我这个作家，可以考虑报名<a class="ae ky" href="https://prakhar-mishra.medium.com/membership" rel="noopener">成为</a>中的一员。每月只需5美元，你就可以无限制地使用Medium。<em class="lv"> </em>谢谢！</p><p id="8880" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这里有一些你可能会感兴趣的帖子</p><div class="nc nd gp gr ne nf"><a rel="noopener follow" target="_blank" href="/8-types-of-sampling-techniques-b21adcdd2124"><div class="ng ab fo"><div class="nh ab ni cl cj nj"><h2 class="bd iu gy z fp nk fr fs nl fu fw is bi translated">8种取样技术</h2><div class="nm l"><h3 class="bd b gy z fp nk fr fs nl fu fw dk translated">了解采样方法(视觉和代码)</h3></div><div class="nn l"><p class="bd b dl z fp nk fr fs nl fu fw dk translated">towardsdatascience.com</p></div></div><div class="no l"><div class="np l nq nr ns no nt ks nf"/></div></div></a></div><div class="nc nd gp gr ne nf"><a href="https://medium.com/mlearning-ai/10-popular-keyword-extraction-algorithms-in-natural-language-processing-8975ada5750c" rel="noopener follow" target="_blank"><div class="ng ab fo"><div class="nh ab ni cl cj nj"><h2 class="bd iu gy z fp nk fr fs nl fu fw is bi translated">自然语言处理中10种流行的关键词提取算法</h2><div class="nm l"><h3 class="bd b gy z fp nk fr fs nl fu fw dk translated">从10篇研究论文中总结用于从文本中无监督提取关键词的方法</h3></div><div class="nn l"><p class="bd b dl z fp nk fr fs nl fu fw dk translated">medium.com</p></div></div><div class="no l"><div class="nu l nq nr ns no nt ks nf"/></div></div></a></div><div class="nc nd gp gr ne nf"><a rel="noopener follow" target="_blank" href="/5-outlier-detection-methods-that-every-data-enthusiast-must-know-f917bf439210"><div class="ng ab fo"><div class="nh ab ni cl cj nj"><h2 class="bd iu gy z fp nk fr fs nl fu fw is bi translated">每个“数据爱好者”都必须知道的5种异常检测技术</h2><div class="nm l"><h3 class="bd b gy z fp nk fr fs nl fu fw dk translated">异常检测方法(视觉和代码)</h3></div><div class="nn l"><p class="bd b dl z fp nk fr fs nl fu fw dk translated">towardsdatascience.com</p></div></div><div class="no l"><div class="nv l nq nr ns no nt ks nf"/></div></div></a></div></div></div>    
</body>
</html>