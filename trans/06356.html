<html>
<head>
<title>ByT5: Towards a token-free future with pre-trained byte-to-byte models</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">ByT5:通过预先训练的字节到字节模型走向无令牌的未来</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/byt5-towards-a-token-free-future-with-pre-trained-byte-to-byte-models-3638791a44b2?source=collection_archive---------37-----------------------#2021-06-07">https://towardsdatascience.com/byt5-towards-a-token-free-future-with-pre-trained-byte-to-byte-models-3638791a44b2?source=collection_archive---------37-----------------------#2021-06-07</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="01f9" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">NLP研究论文摘要</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/7dff780ba7f5826f32e62fc304f73c0c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*w6Bm4wiOS36Uqgeq7p_a4Q.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><p id="edfb" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><em class="lu">在这篇博客中，我试图根据我的理解，用预先训练好的字节到字节模型</em>  <em class="lu">来总结论文</em> <a class="ae lv" href="https://arxiv.org/pdf/2105.13626v1.pdf" rel="noopener ugc nofollow" target="_blank"> <em class="lu"> ByT5:迈向一个无令牌的未来。请随时评论你的想法！</em></a></p><h1 id="5e68" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">想法</h1><p id="8d10" class="pw-post-body-paragraph ky kz it la b lb mo ju ld le mp jx lg lh mq lj lk ll mr ln lo lp ms lr ls lt im bi translated">迄今为止，大多数NLP研究广泛使用了<strong class="la iu">记号赋予器</strong>的概念来将文本序列分成更小的词汇单元。如今，你会发现<a class="ae lv" href="https://www.thoughtvector.io/blog/subword-tokenization" rel="noopener ugc nofollow" target="_blank">子词标记化</a>是人们用来表示文本单元的事实上的技术，<em class="lu">(在过去的某个时候是单字，双字)</em></p><p id="b384" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><em class="lu">考虑到这些方法的局限性，其中一些是— </em></p><ul class=""><li id="c5cc" class="mt mu it la b lb lc le lf lh mv ll mw lp mx lt my mz na nb bi translated"><strong class="la iu">处理OOV时不够稳健</strong>。</li><li id="d84b" class="mt mu it la b lb nc le nd lh ne ll nf lp ng lt my mz na nb bi translated">拼写、大小写等方面的变化导致了<strong class="la iu">不同的表示</strong>。</li></ul><p id="48ca" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">作者提出的<strong class="la iu">无令牌</strong>模型直接在原始文本(字节)上操作<strong class="la iu">，给我们带来了下面提到的好处——</strong></p><ul class=""><li id="7ccd" class="mt mu it la b lb lc le lf lh mv ll mw lp mx lt my mz na nb bi translated">他们可以处理任何语言的文本。我们需要<strong class="la iu">而不是需要特定于语言的标记器</strong>。您只需要一个标记器！】</li><li id="99c4" class="mt mu it la b lb nc le nd lh ne ll nf lp ng lt my mz na nb bi translated">它们对噪声具有很强的鲁棒性，并最大限度地减少了复杂文本预处理管道的麻烦。</li><li id="849f" class="mt mu it la b lb nc le nd lh ne ll nf lp ng lt my mz na nb bi translated">我们现在<strong class="la iu">不需要庞大的词汇矩阵</strong>作为字节级模型，根据定义，只需要256个嵌入。</li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nh"><img src="../Images/2f3820f3392894ca47e19c1a6ab5647e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vCPmw_HBAeaXE6-NiVjD_A.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">mT5(薛等，2020)和ByT5(本作品)的训练前实例创建和网络架构比较|图片来自<a class="ae lv" href="https://arxiv.org/pdf/2105.13626v1.pdf" rel="noopener ugc nofollow" target="_blank">来源</a></p></figure><p id="e478" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">虽然，字节级模型的<strong class="la iu">主要缺点</strong>之一是<strong class="la iu">字节序列通常比原始文本序列长，导致处理成本较高</strong>。众所周知，变形金刚中的自我关注是一种二次计算，当试图处理越来越长的序列时，这种计算会带来巨大的挑战。话虽如此，我们确实有进步，如<a class="ae lv" href="https://github.com/allenai/longformer" rel="noopener ugc nofollow" target="_blank"> Longformer </a>等，它们利用稀疏注意力和其他巧妙的技术来处理非常大的序列。</p><h1 id="bbd9" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">mT5与ByT5 —设计</h1><ol class=""><li id="950d" class="mt mu it la b lb mo le mp lh ni ll nj lp nk lt nl mz na nb bi translated">mT5/ <a class="ae lv" rel="noopener" target="_blank" href="/understanding-t5-model-text-to-text-transfer-transformer-model-69ce4c165023"> T5 </a>使用子字标记，而ByT5使用原始字节作为模型的输入，这使得<strong class="la iu">不知道文本预处理</strong>的类型，等等。</li><li id="4e86" class="mt mu it la b lb nc le nd lh ne ll nf lp ng lt nl mz na nb bi translated">mT5/T5使用跨度掩蔽的概念作为在大量未标记数据上预先训练模型的自我监督目标。ByT5通过屏蔽字节使用类似的概念。此外，mT5平均屏蔽掉3个子字标记，这里作者发现更长的屏蔽序列有利于模型，因此他们将其平均屏蔽跨度长度设置为20字节。</li><li id="bcbe" class="mt mu it la b lb nc le nd lh ne ll nf lp ng lt nl mz na nb bi translated">mT5/T5使用所谓的“平衡架构”<em class="lu">(编码器深度= =解码器深度)，</em>然而，byT5的作者发现，当编码器深度几乎是解码器深度的3倍时，它工作得最好，从而使整个架构编码器很重。此外，即使在降低解码器的容量后，他们发现该模型在分类和生成<em class="lu">(翻译/摘要)</em>任务上表现更好。</li></ol><p id="d420" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">此外，作为质量控制协议，由于根据UTF-8，并非所有的字节序列都是合法的，因此作者通过使用python的字节解码函数—<strong class="la iu"><em class="lu">. bytes . decode(" utf-8 "，errors="ignore") </em> </strong>来删除任何无效的序列</p><h1 id="c012" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">技术性能分析</h1><ol class=""><li id="b2e5" class="mt mu it la b lb mo le mp lh ni ll nj lp nk lt nl mz na nb bi translated">通常，词汇表中每个标记的向量表示采用模型总参数空间中的大多数参数。例如，在最近的mT5-Base模型中，词汇和softmax输出矩阵占总参数计数的66%。对于字节模型，由于不会出现这种情况，如果我们要补偿大的模型参数计数，我们可以通过使我们的模型更深更宽来实现它，通过拥有更复杂的模型来给我们带来优势。</li><li id="1034" class="mt mu it la b lb nc le nd lh ne ll nf lp ng lt nl mz na nb bi translated">与使用单词或子单词标记化方案相比，给定文本片段的字节序列通常更长。因此，你会有明显较高的计算成本，因为变压器使用自我关注，具有二次时间复杂度。</li></ol><h1 id="3dbf" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">结果</h1><h2 id="edd9" class="nm lx it bd ly nn no dn mc np nq dp mg lh nr ns mi ll nt nu mk lp nv nw mm nx bi translated">摘要文本摘要<em class="ny">(英语)</em></h2><p id="2c68" class="pw-post-body-paragraph ky kz it la b lb mo ju ld le mp jx lg lh mq lj lk ll mr ln lo lp ms lr ls lt im bi translated">他们在XSum数据集上评估mT5和ByT5来进行抽象文本摘要。正如您在下表中看到的，对于所有大小的变体，ByT5都优于mT5，并且接近专门为抽象概括而训练的<a class="ae lv" href="https://medium.com/analytics-vidhya/pegasus-pre-training-with-extracted-gap-sentences-for-abstractive-summarization-acb238aa1096" rel="noopener">Pegasus</a>model<em class="lu">(17.0)</em>。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nz"><img src="../Images/c1928410309fd2df4c2f446a6844415a.png" data-original-src="https://miro.medium.com/v2/resize:fit:418/format:webp/1*ecWPCUFxfvWyQxt7f84dXw.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">GEM-XSUM |图片来自<a class="ae lv" href="https://arxiv.org/pdf/2105.13626v1.pdf" rel="noopener ugc nofollow" target="_blank">来源</a></p></figure><h2 id="daae" class="nm lx it bd ly nn no dn mc np nq dp mg lh nr ns mi ll nt nu mk lp nv nw mm nx bi translated">文本分类(英语)</h2><p id="3590" class="pw-post-body-paragraph ky kz it la b lb mo ju ld le mp jx lg lh mq lj lk ll mr ln lo lp ms lr ls lt im bi translated">他们评估了mT5和ByT5在不同模型尺寸下在<strong class="la iu">粘合</strong>和<strong class="la iu">强力胶</strong>任务上的表现。正如我们在下表中看到的，仅在小型和基本型号中，ByT5的性能优于mT5。作者解释说，这可能是由于有效的参数使用，因为大多数mT5参数只是作为词汇表矩阵被锁定。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oa"><img src="../Images/948998681d77746d2ce5863d873ca5d6.png" data-original-src="https://miro.medium.com/v2/resize:fit:816/format:webp/1*-gor_EAeFigu7wUgMsET6g.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">mT5和ByT5在胶水和强力胶上不同型号的性能|图片来自<a class="ae lv" href="https://arxiv.org/pdf/2105.13626v1.pdf" rel="noopener ugc nofollow" target="_blank">来源</a></p></figure><p id="de37" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">正如您在下表中看到的，在固定参数计数设置下，随着模型大小的增加，两个模型的<em class="lu"> dmodel </em>和<em class="lu"> dff </em>变得可比，这与模型大小较低时不同。这是上表所示行为的可能原因。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ob"><img src="../Images/05ce2427860d3591a05d515bcf4f1d99.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MJG5eDqjtr95Lux4A4TxWQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">mT5和ByT5架构的比较|图片来自<a class="ae lv" href="https://arxiv.org/pdf/2105.13626v1.pdf" rel="noopener ugc nofollow" target="_blank">来源</a></p></figure><p id="d506" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><em class="lu">好了，这篇博客到此为止。论文中提到了更多的实验。我鼓励你也阅读它们。</em></p><blockquote class="oc"><p id="29cc" class="od oe it bd of og oh oi oj ok ol lt dk translated"><em class="ny">如果你愿意，你也可以</em> <a class="ae lv" href="https://medium.com/analytics-vidhya/summarizing-nlp-research-papers-dbd12965aa0a" rel="noopener"> <em class="ny">查看我写的其他研究论文摘要</em> </a> <em class="ny">。</em></p></blockquote><p id="4cb8" class="pw-post-body-paragraph ky kz it la b lb om ju ld le on jx lg lh oo lj lk ll op ln lo lp oq lr ls lt im bi translated">请随意阅读整篇论文，并向作者问好，感谢他们的贡献。</p><blockquote class="or os ot"><p id="7263" class="ky kz lu la b lb lc ju ld le lf jx lg ou li lj lk ov lm ln lo ow lq lr ls lt im bi translated"><strong class="la iu"> <em class="it">论文标题:</em> </strong> <em class="it"> </em> ByT5:用预先训练好的字节到字节模型走向无令牌的未来</p><p id="d2e4" class="ky kz lu la b lb lc ju ld le lf jx lg ou li lj lk ov lm ln lo ow lq lr ls lt im bi translated"><strong class="la iu"> <em class="it">论文链接:</em></strong><a class="ae lv" href="https://arxiv.org/pdf/2105.13626v1.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/2105.13626v1.pdf</a></p><p id="1468" class="ky kz lu la b lb lc ju ld le lf jx lg ou li lj lk ov lm ln lo ow lq lr ls lt im bi translated"><strong class="la iu"> <em class="it">作者:</em> </strong> <a class="ae lv" href="https://arxiv.org/search/cs?searchtype=author&amp;query=Xue%2C+L" rel="noopener ugc nofollow" target="_blank">【林挺】薛</a>，<a class="ae lv" href="https://arxiv.org/search/cs?searchtype=author&amp;query=Barua%2C+A" rel="noopener ugc nofollow" target="_blank">阿迪雅巴鲁阿</a>，<a class="ae lv" href="https://arxiv.org/search/cs?searchtype=author&amp;query=Constant%2C+N" rel="noopener ugc nofollow" target="_blank">诺亚恒</a>，<a class="ae lv" href="https://arxiv.org/search/cs?searchtype=author&amp;query=Al-Rfou%2C+R" rel="noopener ugc nofollow" target="_blank">拉米阿尔-Rfou </a>，<a class="ae lv" href="https://arxiv.org/search/cs?searchtype=author&amp;query=Narang%2C+S" rel="noopener ugc nofollow" target="_blank">莎兰纳朗</a>，<a class="ae lv" href="https://arxiv.org/search/cs?searchtype=author&amp;query=Kale%2C+M" rel="noopener ugc nofollow" target="_blank">米希尔卡莱</a>，<a class="ae lv" href="https://arxiv.org/search/cs?searchtype=author&amp;query=Roberts%2C+A" rel="noopener ugc nofollow" target="_blank"> Adam Roberts </a>，<a class="ae lv" href="https://arxiv.org/search/cs?searchtype=author&amp;query=Raffel%2C+C" rel="noopener ugc nofollow" target="_blank">科林·拉斐尔</a></p></blockquote><p id="cfe6" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我希望你喜欢读这篇文章。如果你愿意支持我成为一名作家，可以考虑注册<a class="ae lv" href="https://prakhar-mishra.medium.com/membership" rel="noopener">成为</a>的媒体成员。每月只需5美元，你就可以无限制地使用Medium。谢谢你！</p></div></div>    
</body>
</html>