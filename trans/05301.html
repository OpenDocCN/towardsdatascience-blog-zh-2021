<html>
<head>
<title>Simplifying Reinforcement Learning Workflow in MATLAB</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">简化MATLAB中强化学习的工作流程</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/simplifying-reinforcement-learning-workflow-in-matlab-32b5aa5287b8?source=collection_archive---------20-----------------------#2021-05-10">https://towardsdatascience.com/simplifying-reinforcement-learning-workflow-in-matlab-32b5aa5287b8?source=collection_archive---------20-----------------------#2021-05-10</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="48b6" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">在MATLAB中求解OpenAI环境</h2></div><p id="220a" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">假设你对使用强化学习解决某个问题感兴趣。您已经在您的环境中编写了代码，并编译了一系列强化学习(RL)算法来尝试。从头开始自己实现算法是棘手和耗时的，因为它需要大量的试验，并且包含许多实现技巧。那你是做什么的？</p><p id="90cf" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">最好的答案是使用RL框架。RL框架包含RL算法的近似最优实现。算法的实现被卸载到框架中，用户只需要担心演员和评论家模型的神经架构。有很多基于TensorFlow和PyTorch的框架。然而，与MATLAB 2021a一起发布的<strong class="kh ir">强化学习设计器应用程序在这一类别中也是一个强有力的竞争者</strong>，本文就是关于这一点的。</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div class="gh gi lb"><img src="../Images/7bae35151877a7b2282987f0a8fc1685.png" data-original-src="https://miro.medium.com/v2/resize:fit:974/format:webp/1*2Yp79LxJ_V5KaUgQqRJLQA.png"/></div><p class="lj lk gj gh gi ll lm bd b be z dk translated">典型的RL环路(图片来自mathworks.com)</p></figure><p id="66e9" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">RL Designer app是强化学习工具箱的一部分。它基本上是RL工具箱功能的前端。设计器的点击功能使管理RL工作流变得极其简单，在这篇文章中，我将描述如何使用该应用程序解决一个简单的OpenAI环境。我创建了一个youtube系列，深入研究MATLAB中强化学习的细节。这篇文章的视频版本在这里:</p><figure class="lc ld le lf gt lg"><div class="bz fp l di"><div class="ln lo l"/></div></figure><p id="e18c" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">RL设计器应用程序看起来有点类似于深度网络设计器应用程序。在左侧窗格中，您可以找到代理、环境、结果和环境详细信息。您可以在应用程序内部执行整个RL工作流程。因为我们想让事情有点挑战性，我们将首先演示如何加载到外部环境中，并使用我们的自定义网络在其上训练DQN。</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lq lr di ls bf lt"><div class="gh gi lp"><img src="../Images/625a21b47815ec84f2a27fc6ed23348b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZSAAZTmsdoezCd87iYUFUg.png"/></div></div><p class="lj lk gj gh gi ll lm bd b be z dk translated">强化学习设计器</p></figure><p id="2777" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">您需要Python和OpenAI-gym包才能在环境中加载。我们开始吧，</p><blockquote class="lu lv lw"><p id="79d6" class="kf kg lx kh b ki kj jr kk kl km ju kn ly kp kq kr lz kt ku kv ma kx ky kz la ij bi translated"><strong class="kh ir"> <em class="iq">装载环境</em> </strong></p></blockquote><p id="d9e1" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">MATLAB R2021a附带了一些预构建的环境，可以通过单击环境选项卡位置中的“新建”按钮来加载它们。在本文中，我们将尝试加载我们的自定义环境，它基本上是OpenAI的mountain-Car-v 0环境的包装器。在下面的代码中，我们为体育馆环境定义了包装器。“步骤”函数在健身房环境中执行步骤，并以MATLAB友好的格式返回详细信息。</p><figure class="lc ld le lf gt lg"><div class="bz fp l di"><div class="mb lo l"/></div><p class="lj lk gj gh gi ll lm bd b be z dk translated">开放式包装</p></figure><p id="b19f" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这个类的对象需要在工作空间中创建，然后环境就可以导入RL Designer应用程序了。</p><blockquote class="lu lv lw"><p id="f49a" class="kf kg lx kh b ki kj jr kk kl km ju kn ly kp kq kr lz kt ku kv ma kx ky kz la ij bi translated"><strong class="kh ir">选择代理和型号</strong></p></blockquote><p id="1081" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">就像选择环境一样，也可以通过选择代理选项卡区域中的“新建”按钮来选择代理。该应用程序给出了可供选择的算法列表。我们将选择DQN来完成这项任务。</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div class="gh gi mc"><img src="../Images/c265e957d0cd117356cbc18535979ee0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1212/format:webp/1*r8IooCGBq6yimCK9IAxh7Q.png"/></div><p class="lj lk gj gh gi ll lm bd b be z dk translated">代理选择屏幕</p></figure><p id="3381" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">一旦我们选择了一个代理，MATLAB就会创建一个默认的演员/评论家神经网络，该网络具有完全连接的层。我们想对此进行修改，并使用我们自己的定制神经网络。这可以通过选择代理并从工作区导入我们的自定义评论家网络来完成。我们使用以下脚本创建一个简单的线性网络，并将其加载到工作空间中。</p><figure class="lc ld le lf gt lg"><div class="bz fp l di"><div class="mb lo l"/></div><p class="lj lk gj gh gi ll lm bd b be z dk translated">DQN客户网络</p></figure><p id="1b24" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们首先将网络加载到我们的MATLAB工作区，然后通过从代理选项卡中选择“导入”选项，最终将其加载到RL designer应用程序中。</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div class="gh gi md"><img src="../Images/2795d4ebc7adefe196d47556e400811a.png" data-original-src="https://miro.medium.com/v2/resize:fit:814/format:webp/1*DyrdmuEFOJ2saTQReFvTSw.png"/></div></figure><p id="964f" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们还可以使用Deep Network designer应用程序来分析和编辑网络。网络架构可以是任意的。对神经网络的唯一限制是输入和输出应该与算法和环境修复相匹配。我们还直接在主窗口中指定超参数。我们也可以直接从这个屏幕设置目标网络的行为。该应用程序会自动处理所有这些小细节。</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lq lr di ls bf lt"><div class="gh gi me"><img src="../Images/6e98efdc53480a60b224a063ee6f5b5f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mh4pNg10UhRv48tmTO3IiA.png"/></div></div><p class="lj lk gj gh gi ll lm bd b be z dk translated">设置代理的超参数</p></figure><blockquote class="lu lv lw"><p id="cd46" class="kf kg lx kh b ki kj jr kk kl km ju kn ly kp kq kr lz kt ku kv ma kx ky kz la ij bi translated"><strong class="kh ir">培训代理人</strong></p></blockquote><p id="e328" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">单击主菜单栏上的“训练”按钮，即可开始训练。在开始这个过程之前，我们需要指定剧集细节和平均细节。训练统计如下所示:</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lq lr di ls bf lt"><div class="gh gi mf"><img src="../Images/cf4f521c6b25ef1e0a3dffbe78561a13.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HNcyJRAVF7uWk4_tksz54g.png"/></div></div><p class="lj lk gj gh gi ll lm bd b be z dk translated">训练步骤</p></figure><p id="f401" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这是一个非常标准的代理培训窗口。培训完成后，您可以保存代理和网络。保存的代理可以被重新训练或用于模拟性能。</p><blockquote class="lu lv lw"><p id="36d6" class="kf kg lx kh b ki kj jr kk kl km ju kn ly kp kq kr lz kt ku kv ma kx ky kz la ij bi translated"><strong class="kh ir">验证</strong></p></blockquote><p id="07c9" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们可以在应用程序中直接模拟我们训练有素的代理的结果。可以在“模拟”选项卡中设置模拟剧集的数量。您还可以加载预先训练的代理并模拟它们，以比较不同的代理。请记住，模拟步骤只记录任何给定环境的最终分数。如果需要可视化，您需要使用脚本手动模拟环境。</p><blockquote class="lu lv lw"><p id="b06b" class="kf kg lx kh b ki kj jr kk kl km ju kn ly kp kq kr lz kt ku kv ma kx ky kz la ij bi translated"><strong class="kh ir">遗言</strong></p></blockquote><p id="f586" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们讨论了MATLAB中完整的RL工作流程。改变代理算法是非常无缝的，这是这个应用程序的主要卖点。我想强调的是，可以在RL工具箱(使用脚本)中找到额外的功能，但对于大多数用户来说，应用程序中的功能应该足够了。这个应用程序和框架的最大优势是它抽象出了所有的实现细节，以提供无缝的体验。如果你已经进入了MATLAB生态系统，试一试吧。干杯！</p><blockquote class="lu lv lw"><p id="df50" class="kf kg lx kh b ki kj jr kk kl km ju kn ly kp kq kr lz kt ku kv ma kx ky kz la ij bi translated"><strong class="kh ir">参考文献</strong></p></blockquote><ol class=""><li id="71d6" class="mg mh iq kh b ki kj kl km ko mi ks mj kw mk la ml mm mn mo bi translated">本文代码:<a class="ae mp" href="https://github.com/sol0invictus/MAT-DL/tree/main/RL-in-MATLAB" rel="noopener ugc nofollow" target="_blank"> GitHub链接</a></li><li id="8669" class="mg mh iq kh b ki mq kl mr ko ms ks mt kw mu la ml mm mn mo bi translated">RL播放列表:<a class="ae mp" href="https://www.youtube.com/watch?v=Zcjs6rcd_pY&amp;list=PLUHjJ91-nf0T4rEJk8eLrMT3XSe_wJOhn&amp;ab_channel=ChiDotPhi" rel="noopener ugc nofollow" target="_blank"> Youtube链接</a></li><li id="7c84" class="mg mh iq kh b ki mq kl mr ko ms ks mt kw mu la ml mm mn mo bi translated"><a class="ae mp" href="https://www.mathworks.com/help/reinforcement-learning/" rel="noopener ugc nofollow" target="_blank">强化学习工具箱文档</a></li></ol></div></div>    
</body>
</html>