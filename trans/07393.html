<html>
<head>
<title>Handling data bias</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">处理数据偏差</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/handling-data-bias-9775d07991d4?source=collection_archive---------29-----------------------#2021-07-05">https://towardsdatascience.com/handling-data-bias-9775d07991d4?source=collection_archive---------29-----------------------#2021-07-05</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="9277" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">走向伦理人工智能的旅程</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/357ea9e9176b4c675a695da907361f7d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CMCGgAowagH3-mzqJXkE9Q.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">由<a class="ae kv" href="https://unsplash.com/@_louisreed?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">路易斯·里德</a>在<a class="ae kv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片，由作者编辑</p></figure><p id="5084" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">如果你也有一个愿景，那就是确保你正在开发的产品遵循“人工智能永远有效”的所有书面规则，那么你肯定会遇到你的数据有偏差的情况。</p><p id="d62e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">有偏见的模型、有偏见的数据或有偏见的实现——是数据科学家生活中的典型悲哀。因此，首先我们需要理解并承认偏见的存在，并且可以采取任何形式。</p><p id="69e0" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">是的，偏差是一个宽泛的术语，它可以出现在数据收集、算法中，甚至出现在ML输出解释阶段。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ls"><img src="../Images/9d4a372c5311ca575ecca252322f944f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1062/format:webp/1*VwN3A1JWYOsNlgIIkpGGGw.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">由作者使用PowerPoint创建</p></figure><p id="cf3e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">偏见为什么会伤人？</p><blockquote class="lt lu lv"><p id="0c98" class="kw kx lw ky b kz la jr lb lc ld ju le lx lg lh li ly lk ll lm lz lo lp lq lr ij bi translated"><em class="iq">基于种族、年龄或性别等人类特征的偏见会导致不同的机会，应该予以制止</em></p></blockquote><p id="fca7" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">根据斯坦福大学的人工智能指数报告，人工智能/人工智能组织认为以下风险在行业中普遍存在，并且正在努力减轻这些风险，因为它们对他们的业务和人类普遍有害。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ma"><img src="../Images/8e1f6cf02082c59d69ae8e1feb138485.png" data-original-src="https://miro.medium.com/v2/resize:fit:780/format:webp/1*tyX1pUWnHw9PKavEL-SA8w.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><a class="ae kv" href="https://aiindex.stanford.edu/report/" rel="noopener ugc nofollow" target="_blank"> AI指数报告</a></p></figure><p id="38e6" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">数据偏差可以有多种形式:</p><ul class=""><li id="582a" class="mb mc iq ky b kz la lc ld lf md lj me ln mf lr mg mh mi mj bi translated"><strong class="ky ir">结构偏差:</strong>数据可能会有纯粹的偏差，因为它是由结构差异决定的。女性代表护士、厨师、教师，这显然是从社会结构中衍生出来的。一家电子商务巨头试图建立一个招聘工具，以获取他们现有员工的细微差别，这不用说，是有偏见的。许多属性，如运动、社交活动、成就等，都是由机器挑选的，这导致了一个偏向男性的工具。</li><li id="00c9" class="mb mc iq ky b kz mk lc ml lf mm lj mn ln mo lr mg mh mi mj bi translated"><strong class="ky ir">数据收集:</strong>数据收集偏差的可能原因可能基于一天中的时间、人群的年龄组、原籍国、阶级阶层等。输入算法的数据应该不断更新，以反映我们生活的世界的真实情况，进而反映我们想要预测的世界的未来状态。</li><li id="0725" class="mb mc iq ky b kz mk lc ml lf mm lj mn ln mo lr mg mh mi mj bi translated"><strong class="ky ir">数据操作:</strong>更容易删除没有附加标签或缺少值的实例。但重要的是要检查被剔除的观察值是否会导致性别、种族、国籍和相关属性的错误数据。</li><li id="bf57" class="mb mc iq ky b kz mk lc ml lf mm lj mn ln mo lr mg mh mi mj bi translated"><strong class="ky ir">算法偏差:</strong>算法将学习数据模式建议它学习的内容。该算法要么反映了普遍的偏见，要么放大了这些偏见，这是我们最担心的。如果判断偏向于特定人群，那么机器也会从训练数据中学习。算法中的偏差源于数据，这些数据要么不是正确的代表，要么是源于现存的偏见。如果输入数据是不平衡的，那么我们需要确保算法仍然看到足够多的少数类实例，以便对其执行良好。有多种方法可以实现数据重新平衡，主要方法包括创建合成数据，或者分配类权重，以便算法对少数类的每个错误预测施加更高的惩罚。</li><li id="ab72" class="mb mc iq ky b kz mk lc ml lf mm lj mn ln mo lr mg mh mi mj bi translated"><strong class="ky ir">实现偏差:</strong>所有的ML模型都建立在训练和测试数据集应该属于相似分布的基本假设之上。根据夏季数据训练的模型可能具有不同的特征分布，因此不适合预测冬季的消费者行为。只有当新数据与过去观察到的数据相似时，该模型才会表现良好。不仅仅是执行，解释也可能有偏差。如果我们在分析算法输出的过程中，试图叠加我们的信念并支持我们的(有偏见的)观点，会怎么样呢？</li></ul><p id="5815" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">虽然偏见是我们追求道德人工智能框架时需要改善的因素之一，但减轻它肯定不是小事。</p><p id="d101" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">构建“人工智能为善”生态系统的一些重要方面是:</p><ul class=""><li id="0a9c" class="mb mc iq ky b kz la lc ld lf md lj me ln mf lr mg mh mi mj bi translated">数据收集者、开发人员和产品经理通常是在现场工作的人，他们更接近数据。对于组织来说，重要的是提高员工的敏感性，并传播对偏见的可能原因以及如何减轻偏见的认识</li><li id="a7f0" class="mb mc iq ky b kz mk lc ml lf mm lj mn ln mo lr mg mh mi mj bi translated">拥有一位擅长识别偏见来源的专家(人工智能伦理学家)可以帮助企业将他们的愿景与道德框架相一致</li><li id="edb0" class="mb mc iq ky b kz mk lc ml lf mm lj mn ln mo lr mg mh mi mj bi translated">一个由来自不同团队的人组成的治理团队，如隐私、道德、合规、产品和工程，将有助于提供一个新的视角来识别可能被忽略的偏见。</li></ul><p id="1cef" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">没有一个规则手册可以立刻阅读和实施，它是一个不断发展的框架。</p><p id="34ec" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">此外，值得称赞的是，维护一个不偏不倚、公平和可信的人工智能框架的努力不再被视为深奥的东西，而是在全世界范围内获得了正确的关注。</p></div></div>    
</body>
</html>