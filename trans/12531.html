<html>
<head>
<title>Unsupervised Text Classification with Lbl2Vec</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">基于 Lbl2Vec 的无监督文本分类</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/unsupervised-text-classification-with-lbl2vec-6c5e040354de?source=collection_archive---------0-----------------------#2021-12-23">https://towardsdatascience.com/unsupervised-text-classification-with-lbl2vec-6c5e040354de?source=collection_archive---------0-----------------------#2021-12-23</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="dfcd" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated"><strong class="ak">对无标签文本文档的基于嵌入的分类的介绍</strong></h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/7f93be275a5426ce4c012b8e9e190cd6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*k2Q7DkLcmwYUm5mfHDCyUw.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">由<a class="ae kv" href="https://unsplash.com/@impatrickt" rel="noopener ugc nofollow" target="_blank">帕特里克·托马索</a>在<a class="ae kv" href="https://unsplash.com" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片。</p></figure><p id="1037" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><em class="ls">本帖基于我们的论文</em><a class="ae kv" href="https://www.scitepress.org/Link.aspx?doi=10.5220/0010710300003058" rel="noopener ugc nofollow" target="_blank"><em class="ls">“LBL 2 vec:一种基于嵌入的预定义主题无监督文档检索方法(2021)”</em></a><em class="ls">。你可以在那里读到更多的细节。</em></p><p id="957f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">文本分类的任务是给一个句子或文档分配一个合适的类别。类别取决于所选数据集，可以涵盖任意主题。因此，文本分类器可用于组织、构建和分类任何类型的文本。</p><p id="2350" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">常见的方法使用监督学习来分类文本。尤其是近年来基于 BERT 的语言模型取得了非常好的文本分类效果。这些传统的文本分类方法通常需要大量的标注训练数据。然而，在实践中，用于训练最先进的分类算法的带注释的文本数据集通常是不可用的。数据的标注通常涉及大量的人工工作和高昂的费用。因此，无监督的方法提供了对未标记数据集运行低成本文本分类的机会。最近，无监督文本分类也被称为零镜头文本分类。<strong class="ky ir">在本文中，你将学习如何使用</strong><a class="ae kv" href="https://github.com/sebischair/Lbl2Vec" rel="noopener ugc nofollow" target="_blank"><strong class="ky ir">LBL 2 vec</strong></a><strong class="ky ir">来执行无监督的文本分类。</strong></p><h1 id="87a2" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated">Lbl2Vec 是如何工作的？</h1><p id="33f4" class="pw-post-body-paragraph kw kx iq ky b kz ml jr lb lc mm ju le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated"><a class="ae kv" href="https://github.com/sebischair/Lbl2Vec" rel="noopener ugc nofollow" target="_blank"> <strong class="ky ir"> Lbl2Vec </strong> </a>是一种用于无监督文档分类和无监督文档检索的算法。它自动生成联合嵌入的标签、文档和词向量，并返回由手动预定义的关键字建模的类别的文档。该算法的核心思想是许多语义相似的关键词可以代表一个类别。在第一步中，该算法创建文档和单词向量的联合嵌入。一旦文档和单词被嵌入到共享向量空间中，该算法的目标是从先前手动定义的表示类别的关键词中学习标签向量。最后，该算法可以基于文档向量与标签向量的相似性来预测文档与类别的从属关系。在高层次上，该算法执行以下步骤来分类未标记的文本:</p><h2 id="c72d" class="mq lu iq bd lv mr ms dn lz mt mu dp md lf mv mw mf lj mx my mh ln mz na mj nb bi translated">1.对每个感兴趣的类别使用手动定义的关键字</h2><p id="4ac6" class="pw-post-body-paragraph kw kx iq ky b kz ml jr lb lc mm ju le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated">首先，我们必须定义关键字来描述每个感兴趣的分类类别。这个过程需要一定程度的领域知识来定义描述分类类别并且在分类类别中彼此语义相似的关键词。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nc"><img src="../Images/eff6bc3bc99f8801471cb23656c3b297.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zBrUrabZVrlP9w7M2YhItQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">不同运动分类类别的关键词示例。图片作者。</p></figure><h2 id="e8ee" class="mq lu iq bd lv mr ms dn lz mt mu dp md lf mv mw mf lj mx my mh ln mz na mj nb bi translated">2.创建联合嵌入的文档和单词向量</h2><blockquote class="nd ne nf"><p id="403e" class="kw kx ls ky b kz la jr lb lc ld ju le ng lg lh li nh lk ll lm ni lo lp lq lr ij bi translated">嵌入向量是允许我们在多维空间中表示单词或文本文档的向量。嵌入向量背后的思想是相似的单词或文本文档将具有相似的向量。<strong class="ky ir"> - </strong> <a class="ae kv" rel="noopener" target="_blank" href="/how-to-perform-topic-modeling-with-top2vec-1ae9bb4e89dc"> <strong class="ky ir">阿莫尔</strong> </a></p></blockquote><p id="f63c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">因此，在创建共同嵌入的向量之后，文档被定位成靠近其他相似的文档并且靠近最有区别的单词。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nj"><img src="../Images/c75b120c8415d00e7b54c57d161ba5cb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*C3pGuEFP3n5dhQjoiszRtQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">联合嵌入的单词和文档向量。图片作者。</p></figure><p id="29ff" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">一旦我们有了一组单词和文档向量，我们就可以进入下一步。</p><h2 id="ad6f" class="mq lu iq bd lv mr ms dn lz mt mu dp md lf mv mw mf lj mx my mh ln mz na mj nb bi translated">3.找到与每个分类类别的关键字向量相似的文档向量</h2><p id="19ad" class="pw-post-body-paragraph kw kx iq ky b kz ml jr lb lc mm ju le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated">现在我们可以计算文档和每个类别的手动定义的关键字之间的余弦相似度。与类别关键词相似的文档被分配给相应类别的一组候选文档。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nk"><img src="../Images/85a8e05cb10b54dd9c175395c4c6e456.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WfYr07WW8K06Mk3xCLcUTg.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">分类类别关键字及其各自的候选文档集。每种颜色代表不同的分类类别。图片作者。</p></figure><h2 id="df68" class="mq lu iq bd lv mr ms dn lz mt mu dp md lf mv mw mf lj mx my mh ln mz na mj nb bi translated">4.清除每个分类类别的异常文档</h2><p id="e0db" class="pw-post-body-paragraph kw kx iq ky b kz ml jr lb lc mm ju le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated">该算法使用<a class="ae kv" rel="noopener" target="_blank" href="/local-outlier-factor-lof-algorithm-for-outlier-identification-8efb887d9843"> LOF </a>从每组候选文档中清除离群文档，这些文档可能与一些描述性关键字相关，但不完全匹配预期的分类类别。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nl"><img src="../Images/bdf201b04906e90b6fd98bf686243652.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RNadvQLm1UNl_97HhOq9Yw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">红色文档是从候选文档集中移除的离群值。图片作者。</p></figure><h2 id="4ca4" class="mq lu iq bd lv mr ms dn lz mt mu dp md lf mv mw mf lj mx my mh ln mz na mj nb bi translated">5.计算异常值清除文档向量的质心，作为每个分类类别的标签向量</h2><p id="34d0" class="pw-post-body-paragraph kw kx iq ky b kz ml jr lb lc mm ju le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated">为了得到分类类别的嵌入表示，我们计算标签向量。稍后，将使用文档与标签向量的相似度来对文本文档进行分类。每个标签向量由一个类别的异常清除文档向量的<a class="ae kv" href="https://en.wikipedia.org/wiki/Centroid" rel="noopener ugc nofollow" target="_blank">质心</a>组成。该算法计算文档而不是关键词质心，因为实验表明仅基于与关键词的相似性来分类文档更加困难，即使它们共享相同的向量空间。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nm"><img src="../Images/f36cbc5cb375e8e3c5e53199f3097c39.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_yGBRIivCQ2Cne6c9F1VoQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">标签向量，计算为各个清除的候选文档向量的质心。点代表各个主题的标签向量。图片作者。</p></figure><h2 id="a68f" class="mq lu iq bd lv mr ms dn lz mt mu dp md lf mv mw mf lj mx my mh ln mz na mj nb bi translated">6.文本文档分类</h2><p id="a032" class="pw-post-body-paragraph kw kx iq ky b kz ml jr lb lc mm ju le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated">该算法为数据集中的每个标签向量和文档向量计算<em class="ls">标签向量&lt; - &gt;文档向量</em>相似度。最后，文本文档被归类为具有最高<em class="ls">标签向量&lt; - &gt;文档向量</em>相似度的类别。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nn"><img src="../Images/847d4340a6c09189572bd2b81f802aee.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ijD2IjGHU65v3vwdOr61RA.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">数据集中所有文档的分类结果。点表示分类类别的标签向量。文档颜色代表其预测的分类类别。图片作者。</p></figure><h1 id="5985" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated">Lbl2Vec 教程</h1><p id="bddf" class="pw-post-body-paragraph kw kx iq ky b kz ml jr lb lc mm ju le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated">在本教程中，我们将使用<a class="ae kv" href="https://github.com/sebischair/Lbl2Vec" rel="noopener ugc nofollow" target="_blank"> Lbl2Vec </a>对来自<a class="ae kv" href="http://qwone.com/~jason/20Newsgroups/" rel="noopener ugc nofollow" target="_blank"> 20 个新闻组数据集</a>的文本文档进行分类。它是大约 20，000 个文本文档的集合，平均分布在 20 个不同的新闻组类别中。在本教程中，我们将重点关注由类别“rec.motorcycles”、“rec.sport.baseball”、“rec.sport.hockey”和“sci.crypt”组成的 20 个新闻组数据集的子集。此外，我们将为每个分类类别使用已经预定义的关键字。预定义的关键字可以从<a class="ae kv" href="https://github.com/TimSchopf/MediumBlog/blob/main/data/20newsgroups_keywords.csv" rel="noopener ugc nofollow" target="_blank">这里</a>下载。还可以在 GitHub 上访问更多<a class="ae kv" href="https://github.com/sebischair/Lbl2Vec/tree/main/examples" rel="noopener ugc nofollow" target="_blank"> Lbl2Vec 示例</a>。</p><h2 id="4858" class="mq lu iq bd lv mr ms dn lz mt mu dp md lf mv mw mf lj mx my mh ln mz na mj nb bi translated"><strong class="ak">安装 Lbl2Vec </strong></h2><p id="d255" class="pw-post-body-paragraph kw kx iq ky b kz ml jr lb lc mm ju le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated">我们可以使用 pip 和以下命令安装 Lbl2Vec:</p><p id="fd32" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><code class="fe no np nq nr b">pip install lbl2vec</code></p><h2 id="d667" class="mq lu iq bd lv mr ms dn lz mt mu dp md lf mv mw mf lj mx my mh ln mz na mj nb bi translated">读取数据</h2><p id="4308" class="pw-post-body-paragraph kw kx iq ky b kz ml jr lb lc mm ju le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated">我们将下载的“20newsgroups_keywords.csv”文件存储在与 Python 脚本相同的目录中。然后我们读取带有<a class="ae kv" href="https://pandas.pydata.org" rel="noopener ugc nofollow" target="_blank">熊猫</a>的 CSV，并从<a class="ae kv" href="https://scikit-learn.org/0.19/datasets/twenty_newsgroups.html" rel="noopener ugc nofollow" target="_blank"> Scikit-learn </a>获取 20 个新闻组数据集。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ns nt l"/></div></figure><h2 id="ec60" class="mq lu iq bd lv mr ms dn lz mt mu dp md lf mv mw mf lj mx my mh ln mz na mj nb bi translated">预处理数据</h2><p id="968f" class="pw-post-body-paragraph kw kx iq ky b kz ml jr lb lc mm ju le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated">为了训练 Lbl2Vec 模型，我们需要对数据进行预处理。首先，我们处理用作 Lbl2Vec 输入的关键字。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ns nt l"/></div></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nu"><img src="../Images/7a9e43b3dfa1d768b2b83a11e8609244.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NYxhXpx3BiE45clA99zTJw.png"/></div></div></figure><p id="480c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们看到，关键字描述了每个分类类别，关键字的数量可能会有所不同。</p><p id="3940" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">此外，我们还需要对新闻文章进行预处理。因此，我们对每个文档进行单词标记化，并添加<a class="ae kv" href="https://radimrehurek.com/gensim/models/doc2vec.html#gensim.models.doc2vec.TaggedDocument" rel="noopener ugc nofollow" target="_blank">gensim . models . doc 2 vec . tagged document</a>标签。Lbl2Vec 需要标记化和标签化的文档作为训练输入格式。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ns nt l"/></div></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nv"><img src="../Images/e417025ee19881b3bf84ed5d910a7b68.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WInetgl04NxRaqkzgMqoWQ.png"/></div></div></figure><p id="cf7a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们可以在数据框中看到文章文本及其分类类别。“tagged_docs”列包含作为 Lbl2Vec 输入所需的预处理文档。“class_name”列中的分类类别仅用于评估，而不用于 Lbl2Vec 培训。</p><h2 id="ee83" class="mq lu iq bd lv mr ms dn lz mt mu dp md lf mv mw mf lj mx my mh ln mz na mj nb bi translated">培训 Lbl2Vec</h2><p id="5831" class="pw-post-body-paragraph kw kx iq ky b kz ml jr lb lc mm ju le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated">准备好数据后，我们现在可以在训练数据集上训练 Lbl2Vec 模型。我们用以下参数初始化模型:</p><ul class=""><li id="27a3" class="nw nx iq ky b kz la lc ld lf ny lj nz ln oa lr ob oc od oe bi translated"><strong class="ky ir"> keywords_list </strong>:列表的可重复列表，包含每个类别的描述性关键字。</li><li id="1fdd" class="nw nx iq ky b kz of lc og lf oh lj oi ln oj lr ob oc od oe bi translated"><strong class="ky ir">tagged _ documents</strong>:gensim . models . doc 2 vec . tagged document 元素的可迭代列表。每个元素由一个文档组成。</li><li id="6ccc" class="nw nx iq ky b kz of lc og lf oh lj oi ln oj lr ob oc od oe bi translated"><strong class="ky ir"> label_names </strong>:每个标签自定义名称的可重复列表。同一主题的标签名称和关键字必须具有相同的索引。</li><li id="3dca" class="nw nx iq ky b kz of lc og lf oh lj oi ln oj lr ob oc od oe bi translated"><strong class="ky ir"> similarity_threshold </strong>:只有与各个描述关键词的相似度高于该阈值的文档才用于计算标签嵌入。</li><li id="86e0" class="nw nx iq ky b kz of lc og lf oh lj oi ln oj lr ob oc od oe bi translated"><strong class="ky ir"> min_num_docs </strong>:用于计算标签嵌入的最小文档数。</li><li id="7620" class="nw nx iq ky b kz of lc og lf oh lj oi ln oj lr ob oc od oe bi translated"><strong class="ky ir">纪元</strong>:语料库的迭代次数。</li></ul><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ns nt l"/></div></figure><h2 id="1619" class="mq lu iq bd lv mr ms dn lz mt mu dp md lf mv mw mf lj mx my mh ln mz na mj nb bi translated">文本文档的分类</h2><p id="4cb4" class="pw-post-body-paragraph kw kx iq ky b kz ml jr lb lc mm ju le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated">在模型被训练之后，我们可以预测用于训练 Lbl2Vec 模型的文档的类别。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ns nt l"/></div></figure><p id="db50" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><code class="fe no np nq nr b">[Out]: F1 Score: 0.9054393305439331</code></p><p id="f3d7" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们的模型可以预测正确的文档类别，得分相当可观，为<em class="ls"> F1≈0.91 </em>。这是在培训期间甚至不需要看到文档标签的情况下实现的。</p><p id="cc78" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">此外，我们还可以预测未用于训练 Lbl2Vec 模型并因此对其完全未知的文档的分类类别。为此，我们从以前未使用的测试数据集中预测文档的类别。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ns nt l"/></div></figure><p id="246e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><code class="fe no np nq nr b">[Out]: F1 Score: 0.889937106918239</code></p><p id="fd3f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们训练的 Lbl2Vec 模型甚至可以预测新文档的分类类别，得分为<em class="ls"> F1≈0.89 </em>。如前所述，这是通过完全无监督的方法实现的，其中在训练期间不使用标签信息。</p><p id="f2d3" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">关于 Lbl2Vec 可用特性的更多细节，请查看<a class="ae kv" href="https://github.com/sebischair/Lbl2Vec" rel="noopener ugc nofollow" target="_blank"> Lbl2Vec GitHub 库</a>。我希望这篇教程对你有用。</p><h1 id="bf8c" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated">摘要</h1><p id="a368" class="pw-post-body-paragraph kw kx iq ky b kz ml jr lb lc mm ju le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated">Lbl2Vec 是最近开发的一种方法，可用于无监督的文本文档分类。与其他最先进的方法不同，它在训练期间不需要标签信息，因此提供了对无标签数据集运行低成本文本分类的机会。开源的 Lbl2Vec 库也非常容易使用，允许开发人员只需几行代码就可以训练模型。</p><h1 id="66b7" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated">来源</h1><ol class=""><li id="463b" class="nw nx iq ky b kz ml lc mm lf ok lj ol ln om lr on oc od oe bi translated">绍普夫，t。Braun d .和 Matthes f .(2021)。<a class="ae kv" href="https://www.scitepress.org/Link.aspx?doi=10.5220/0010710300003058" rel="noopener ugc nofollow" target="_blank"> Lbl2Vec:基于嵌入的预定义主题无监督文档检索方法</a>，(2021)，第 17 届网络信息系统与技术国际会议论文集</li><li id="f9b6" class="nw nx iq ky b kz of lc og lf oh lj oi ln oj lr on oc od oe bi translated"><a class="ae kv" href="https://github.com/sebischair/Lbl2Vec" rel="noopener ugc nofollow" target="_blank">https://github.com/sebischair/Lbl2Vec</a></li></ol></div></div>    
</body>
</html>