<html>
<head>
<title>No, Kernels &amp; Filters Are Not The Same</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">不，内核和过滤器是不同的</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/no-kernels-filters-are-not-the-same-b230ec192ac9?source=collection_archive---------5-----------------------#2021-10-09">https://towardsdatascience.com/no-kernels-filters-are-not-the-same-b230ec192ac9?source=collection_archive---------5-----------------------#2021-10-09</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="188d" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">解决通常的困惑。</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/abb36b4850aaa3d93b65678f17374507.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*D8Vq8eVY9kR5wh95"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">照片由<a class="ae kv" href="https://unsplash.com/@javaistan?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Afif Kusuma </a>在<a class="ae kv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><p id="94fd" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">对于我们大多数曾经是深度学习新手的人来说，尝试MNIST分类很有趣。卷积是计算机视觉中大多数算法的构建模块，除了一些较新的变体，如<a class="ae kv" href="https://arxiv.org/abs/2010.11929v2" rel="noopener ugc nofollow" target="_blank">视觉变形器</a>、<a class="ae kv" href="https://arxiv.org/abs/2105.01601" rel="noopener ugc nofollow" target="_blank">混合器</a>等。该专利声称不使用卷积就能解决图像相关的问题。DL的核心是<a class="ae kv" href="https://builtin.com/data-science/gradient-descent" rel="noopener ugc nofollow" target="_blank">梯度下降</a>(及其变体)，它帮助我们优化神经网络的参数，从而减少我们在训练模型时遭受的损失。</p><p id="d945" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><a class="ae kv" rel="noopener" target="_blank" href="/a-comprehensive-guide-to-convolutional-neural-networks-the-eli5-way-3bd2b1164a53">卷积或卷积层</a>也拥有自己的参数，俗称<strong class="ky ir"> <em class="lw">滤波器</em> </strong>。不，不是滤镜，但它们是<strong class="ky ir"> <em class="lw">内核</em> </strong>，对吗？还是很困惑🙄，嗯，这就是故事的目的！</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="lx ly l"/></div></figure><h1 id="bb85" class="lz ma iq bd mb mc md me mf mg mh mi mj jw mk jx ml jz mm ka mn kc mo kd mp mq bi translated">👉🏽卷积复习</h1><p id="b3e0" class="pw-post-body-paragraph kw kx iq ky b kz mr jr lb lc ms ju le lf mt lh li lj mu ll lm ln mv lp lq lr ij bi translated">正如大多数深度学习文献中所述，一个<strong class="ky ir"> *2D卷积层</strong>接受一个形状为<code class="fe ls lt lu lv b">( h , w , in_dims )</code>的张量，并产生一个形状为<code class="fe ls lt lu lv b">( h' , w' , out_dims )</code>的特征图。如果没有<a class="ae kv" href="https://www.geeksforgeeks.org/cnn-introduction-to-padding" rel="noopener ugc nofollow" target="_blank"><strong class="ky ir">*填充</strong> </a>，<code class="fe ls lt lu lv b">h'</code>和<code class="fe ls lt lu lv b">w'</code>分别小于<code class="fe ls lt lu lv b">h</code>和<code class="fe ls lt lu lv b">w</code>。你可能见过这个流行的动画，它描述了单通道(<code class="fe ls lt lu lv b">in_dims=1</code>)正方形(<code class="fe ls lt lu lv b">h=w</code>)图像上的典型卷积运算。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mw"><img src="../Images/97a012b2fab34768eae9c1076293a6cf.png" data-original-src="https://miro.medium.com/v2/resize:fit:780/1*V4uobVv91cccRy9LtGYkKQ.gif"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">单通道输入的卷积。来源:<a class="ae kv" href="https://commons.wikimedia.org/wiki/File:2D_Convolution_Animation.gif" rel="noopener ugc nofollow" target="_blank">维基共享资源</a></p></figure><p id="dca7" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在2D卷积的情况下，包含一些数字的矩阵，称为<strong class="ky ir"> <em class="lw">内核</em> </strong>，在图像上移动。我们通常在这里设置<code class="fe ls lt lu lv b">kernel_size</code>和<code class="fe ls lt lu lv b">strides</code>。<code class="fe ls lt lu lv b">kernel_size</code>决定内核的大小，而<code class="fe ls lt lu lv b">strides</code>是内核在一个特定方向上移动的像素(值)的数量，以执行乘法。来自输入张量的值然后以<em class="lw">元素方式</em>与内核中的相应值相乘，最后求和以产生标量。然后内核向前移动(根据<code class="fe ls lt lu lv b">strides</code>)并执行一个类似的问题。</p><p id="6b54" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">然后，这些标量按照它们获得时的方式排列在2D网格中。然后我们会在它上面应用一些激活函数，比如<a class="ae kv" href="https://machinelearningmastery.com/rectified-linear-activation-function-for-deep-learning-neural-networks" rel="noopener ugc nofollow" target="_blank"> ReLU </a>。</p><blockquote class="mx my mz"><p id="3ca7" class="kw kx lw ky b kz la jr lb lc ld ju le na lg lh li nb lk ll lm nc lo lp lq lr ij bi translated"><strong class="ky ir"> *2D卷积层:</strong>我们也有1D和3D卷积，其中内核分别在1维或3维移动。看到这个<a class="ae kv" rel="noopener" target="_blank" href="/understanding-1d-and-3d-convolution-neural-network-keras-9d8f76e29610">直观的故事</a>。由于2D卷积应用广泛，而且容易形象化，我们将只考虑研究2D卷积的情况。</p><p id="7e66" class="kw kx lw ky b kz la jr lb lc ld ju le na lg lh li nb lk ll lm nc lo lp lq lr ij bi translated"><strong class="ky ir">*填充</strong>:由于卷积运算降低了输入张量的维数，我们希望通过用零作为边界来恢复原始维数。用零执行的填充在ML域中被称为零填充。</p></blockquote><h1 id="7439" class="lz ma iq bd mb mc md me mf mg mh mi mj jw mk jx ml jz mm ka mn kc mo kd mp mq bi translated">👉🏽内核</h1><blockquote class="nd"><p id="1aab" class="ne nf iq bd ng nh ni nj nk nl nm lr dk translated">核是在输入张量的单个通道上扫过的矩阵，或者更准确地说是卷积的矩阵。</p></blockquote><p id="b3d0" class="pw-post-body-paragraph kw kx iq ky b kz nn jr lb lc no ju le lf np lh li lj nq ll lm ln nr lp lq lr ij bi translated">使用同样的图表，就像我们之前使用的一样，</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mw"><img src="../Images/97a012b2fab34768eae9c1076293a6cf.png" data-original-src="https://miro.medium.com/v2/resize:fit:780/1*V4uobVv91cccRy9LtGYkKQ.gif"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">单通道输入的卷积。来源:<a class="ae kv" href="https://commons.wikimedia.org/wiki/File:2D_Convolution_Animation.gif" rel="noopener ugc nofollow" target="_blank">维基共享资源</a></p></figure><p id="62d3" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在上面的表示中，可以清楚地观察到3 * 3核在单通道输入张量上被卷积。在大多数实现中，内核是一个列(和行)数等于<code class="fe ls lt lu lv b">kernel_size</code>的正方形矩阵，但也可以是矩形。这正是<a class="ae kv" href="https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2D" rel="noopener ugc nofollow" target="_blank"> TensorFlow文档</a>对其<code class="fe ls lt lu lv b">tf.keras.layers.Conv2D</code>层的说法，</p><blockquote class="mx my mz"><p id="9d87" class="kw kx lw ky b kz la jr lb lc ld ju le na lg lh li nb lk ll lm nc lo lp lq lr ij bi translated"><code class="fe ls lt lu lv b"><strong class="ky ir"><em class="iq">kernel</em></strong></code> <em class="iq">:一个整数或2个整数的元组/列表，指定2D卷积窗口的高度和宽度。可以是单个整数，以便为所有空间维度指定相同的值。</em></p></blockquote><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/d2e516aae277d7b69d08585b3b757cd4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*-H5Cx1XRAtb_jJKd.jpeg"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">内核的形状(其中高度=宽度)。来源:作者图片</p></figure><p id="adfc" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">执行卷积操作后，我们留下了一个单一的通道特征图，如前所述。但是，有一个问题。大多数输入张量不会只有一个通道。即使我们对RGB图像执行卷积运算，我们也需要处理3个通道(即R、G和B通道)。但是，我们不需要担心，因为我们可以简单地为3个通道使用3个内核，对吗？</p><p id="5dad" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这是针对3个通道的，但如果我们有多个输入通道，比如256个通道张量呢？使用相同的类比，我们将有256个内核卷积256个通道中的每一个，从而产生256个特征图(如前所述，具有较小的尺寸)。所有这256个特征地图被加在一起，</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/f201c6365d8255ca477344567498ec9f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*rZOYducL-HEPi-aL.jpeg"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">在多个通道上卷积内核。来源:图片由作者提供。</p></figure><p id="9031" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">如果<strong class="ky ir"> <em class="lw"> C_in </em> </strong>是输入通道数，那么，</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/5246b0f873c08e5408c4147ef9a8c4a3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*aBwfJhh2bTLhXKlX.jpeg"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">来源:作者图片</p></figure><p id="661d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这都是关于内核的！我们现在将前往<strong class="ky ir"> <em class="lw">过滤器</em> </strong>。</p><h1 id="0e1f" class="lz ma iq bd mb mc md me mf mg mh mi mj jw mk jx ml jz mm ka mn kc mo kd mp mq bi translated">👉🏽该过滤器</h1><blockquote class="nd"><p id="dafa" class="ne nf iq bd ng nh ni nj nk nl nm lr dk translated">在输入张量的通道上卷积的所有核的集合。</p></blockquote><p id="017a" class="pw-post-body-paragraph kw kx iq ky b kz nn jr lb lc no ju le lf np lh li lj nq ll lm ln nr lp lq lr ij bi translated">一个<strong class="ky ir"> <em class="lw">滤波器</em> </strong>是在输入张量的通道的卷积中使用的所有<strong class="ky ir"><em class="lw"/></strong>号内核的集合。例如，在RGB图像中，我们对R、G和b三个通道使用了三种不同的内核，这三种内核统称为滤镜。因此，单个过滤器的形状是，</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/a483f386705594054ea24fe6c8db2bf6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*YhCRkvLHiKFT0_Zj.jpeg"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="1e58" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">让我们回到<code class="fe ls lt lu lv b">tf.keras.layers.Conv2D</code>层的TensorFlow文档。它们包括该层的以下示例，</p><pre class="kg kh ki kj gt nt lv nu nv aw nw bi"><span id="4cad" class="nx ma iq lv b gy ny nz l oa ob">input_shape = (4, 28, 28, 3)<br/>conv = tf.keras.layers.Conv2D( 2, 3, activation='relu', input_shape=input_shape[1:])(x)</span></pre><p id="60be" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">对于实例化<code class="fe ls lt lu lv b">Conv2D</code>时的第一个参数，给出的描述是，</p><blockquote class="mx my mz"><p id="9b19" class="kw kx lw ky b kz la jr lb lc ld ju le na lg lh li nb lk ll lm nc lo lp lq lr ij bi translated"><code class="fe ls lt lu lv b"><strong class="ky ir"><em class="iq">filters</em></strong></code>:整数，输出空间的维数(即卷积中输出滤波器的个数)。</p></blockquote><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ns"><img src="../Images/1d4f364e8f7978154b3afe0e47474199.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*op1ewRs0yqngBEDZ.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">来源:作者图片</p></figure><p id="6c8c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">每个过滤器将产生形状的特征图<code class="fe ls lt lu lv b">H' * w'</code>。同样，<code class="fe ls lt lu lv b">filters</code>个过滤器会产生<code class="fe ls lt lu lv b">filters</code>过滤器。因此<code class="fe ls lt lu lv b">Conv2D</code>层的输出形状是<code class="fe ls lt lu lv b">( H' , W' , filters )</code>。这与<code class="fe ls lt lu lv b">tf.keras.layers.Conv2D</code>的<a class="ae kv" href="https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2D" rel="noopener ugc nofollow" target="_blank">张量流文档</a>中提到的输出形状相同，</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/6dec908e961e94a25dd0c40f76eec740.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*RUq-Id4XyqnlVE3Z.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">来源:截图来自<a class="ae kv" href="https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2D" rel="noopener ugc nofollow" target="_blank"> TensorFlow文档</a>。</p></figure><p id="dc48" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这就把我们带到了这个故事的结尾。希望你已经理解了这两个术语的区别。</p><h1 id="0413" class="lz ma iq bd mb mc md me mf mg mh mi mj jw mk jx ml jz mm ka mn kc mo kd mp mq bi translated">结束了</h1><p id="ddb4" class="pw-post-body-paragraph kw kx iq ky b kz mr jr lb lc ms ju le lf mt lh li lj mu ll lm ln mv lp lq lr ij bi translated">希望你喜欢这个小故事！欢迎在<strong class="ky ir"><em class="lw">equipintelligence@gmail.com</em></strong>或在下面的评论中发表你的想法。祝你有美好的一天！</p></div></div>    
</body>
</html>