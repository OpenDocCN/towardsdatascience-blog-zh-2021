<html>
<head>
<title>Serverless NLP Inference on Amazon SageMaker with Transformer Models from Hugging Face</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">亚马逊SageMaker上基于拥抱脸的变形金刚模型的无服务器NLP推理</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/serverless-nlp-inference-on-amazon-sagemaker-with-transformer-models-from-hugging-face-4843609a7451?source=collection_archive---------12-----------------------#2021-12-09">https://towardsdatascience.com/serverless-nlp-inference-on-amazon-sagemaker-with-transformer-models-from-hugging-face-4843609a7451?source=collection_archive---------12-----------------------#2021-12-09</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="2012" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">无服务器部署您的NLP模型。没有基础设施管理。现收现付！</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/42fcec4509faaff931e2534e29a1dd98.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*l9W-4n_Yx4tT5u9r"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">Krzysztof Kowalik 在<a class="ae kv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><h1 id="00de" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">这是怎么回事？</h1><p id="42e1" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">At re:Invent 2021 AWS推出<a class="ae kv" href="https://docs.aws.amazon.com/sagemaker/latest/dg/serverless-endpoints.html" rel="noopener ugc nofollow" target="_blank">亚马逊SageMaker无服务器推理</a>，让我们可以轻松部署机器学习模型进行推理，而无需配置或管理底层基础设施。这是我与客户打交道时最常要求的特性之一，在自然语言处理(NLP)领域尤其如此。在这篇博文中，我将展示如何使用Amazon SageMaker无服务器推理来部署NLP模型。</p><p id="f5db" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">您可以在这个<a class="ae kv" href="https://github.com/marshmellow77/nlp-serverless" rel="noopener ugc nofollow" target="_blank"> Github repo </a>中找到这篇博文的附带代码示例。</p><h1 id="e7c1" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">为什么这很重要？</h1><p id="b093" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">在NLP中，许多工作负载都有间歇性或不可预测的流量。例如，想想你选择的智能家居设备:它大部分时间都处于闲置状态，直到你要求它执行一项任务。如果我们以传统方式部署底层NLP模型，我们将不得不管理底层基础设施。更糟糕的是，我们将不得不为这一基础设施买单——即使它99.9%的时间都处于闲置状态。</p><p id="1c49" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">Amazon SageMaker无服务器推理通过根据推理请求的数量自动扩展计算能力来帮助解决这些类型的用例，而无需您预先预测流量需求或管理扩展策略。此外，您只需为运行推理代码的计算时间(以毫秒计费)和处理的数据量付费，这使它成为具有间歇性流量的工作负载的一个经济高效的选项。</p><h1 id="67ae" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">行动（或活动、袭击）计划</h1><p id="2fac" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">我们将在SageMaker上训练一个拥抱脸模型，然后使用新的SageMaker无服务器推理功能部署它。注意这个功能目前在<em class="mp">预览版</em>中，也就是说不支持某些地区和某些功能。更多信息请访问本<a class="ae kv" href="https://docs.aws.amazon.com/sagemaker/latest/dg/serverless-endpoints.html" rel="noopener ugc nofollow" target="_blank">网站</a>。然而，这并不意味着你必须注册一个测试程序或类似的程序。只要您部署在受支持的地区，就可以使用现成的新功能。</p><p id="37b3" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">我用SageMaker Studio运行这个笔记本。从实例大小来说，ml.t3.medium (2个vCPU + 4个GiB)足够运行笔记本了。确保您安装了最新的SageMaker软件包(截至2021年12月9日，该软件包为2.70.0版):</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mq"><img src="../Images/1996fe6f3036dc9186cdb4ddb3661f20.png" data-original-src="https://miro.medium.com/v2/resize:fit:558/format:webp/1*ogEaeifasGDErD856unEMQ.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><h1 id="c9eb" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">模特培训</h1><p id="e377" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">这篇博文不会深入探讨模型训练，有很多其他的博文会涉及到它。我们将使用一个取自这本<a class="ae kv" href="https://github.com/huggingface/notebooks/blob/master/sagemaker/01_getting_started_pytorch/sagemaker-notebook.ipynb" rel="noopener ugc nofollow" target="_blank">笔记本</a>的非常标准的例子:训练一个二元情感分类器。该模型将接收带有正面或负面情绪的文本，并尝试对它们进行相应的分类。</p><h1 id="08af" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">模型部署</h1><p id="789c" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">一旦模型被训练，这就是乐趣所在。预览版中这个特性的一个限制是我们还不能使用Python SDK来部署模型。这意味着这个漂亮的小方法目前还不可用:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="mr ms l"/></div></figure><p id="4aae" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">相反，我们可以使用Boto3库来建立我们自己的部署过程。这比仅仅使用deploy()方法要复杂一点，但也不会复杂太多。它要求我们自己设置端点配置。这是我们指定要使用新的无服务器配置的地方:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="mr ms l"/></div></figure><p id="1e93" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">设置好端点配置后，我们可以创建端点:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="mr ms l"/></div></figure><p id="e026" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">这将需要几分钟时间，一旦终端部署完毕，我们就可以对其进行测试:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mt"><img src="../Images/4496aef123b2d2912494b1977017f137.png" data-original-src="https://miro.medium.com/v2/resize:fit:1364/format:webp/1*BX0BiHY_UFdU6jBtpCSW8w.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><h1 id="b2ca" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">结论</h1><p id="df17" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">这又快又简单。我们已经使用SageMaker中的Huggingface集成训练了一个NLP模型。一旦模型被训练好，我们就把它部署到一个无服务器的推理端点，现在可以直接使用模型，而不必管理基础设施或在模型未被使用时付费。</p><p id="ba63" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">这篇博文向您简要介绍了如何建立无服务器NLP推理，但是当然还有更多问题需要研究:延迟是什么样子的？延迟如何依赖于模型的大小或我们提供的输入？如何优化这种设置？冷启动问题怎么办？我很高兴在接下来的几周里更多地使用这个新功能，并分享我的学习成果！</p></div></div>    
</body>
</html>