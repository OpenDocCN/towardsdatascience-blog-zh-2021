<html>
<head>
<title>Ask Wikipedia ELI5-like Questions Using Long-Form Question Answering on Haystack</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用Haystack上的长格式问题回答来问维基百科ELI5类的问题</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/ask-wikipedia-eli5-like-questions-using-long-form-question-answering-on-haystack-32cf1ca6c00e?source=collection_archive---------18-----------------------#2021-07-04">https://towardsdatascience.com/ask-wikipedia-eli5-like-questions-using-long-form-question-answering-on-haystack-32cf1ca6c00e?source=collection_archive---------18-----------------------#2021-07-04</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="b50b" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">使用你的文档和26行Python代码构建一个长形式的问题回答平台</h2></div><p id="3756" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">基于自然语言处理的问答系统的最新进展是惊人的。QA系统建立在最新的语言模型(BERT，RoBERTa等)之上。)可以相对轻松且精确地回答基于仿真陈述的问题。该任务包括找到包含答案的相关文档段落，并通过扫描正确的单词标记范围来提取答案。</p><p id="9f2f" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">更具挑战性的问答系统涉及所谓的“生成性问答”。这些系统关注于处理这样的问题，其中所提供的上下文段落不仅仅是所提取答案的源标记，而是提供更大的上下文来合成原始答案。</p><h1 id="b1ac" class="lb lc iq bd ld le lf lg lh li lj lk ll jw lm jx ln jz lo ka lp kc lq kd lr ls bi translated">长篇问答动机</h1><p id="eeb5" class="pw-post-body-paragraph kf kg iq kh b ki lt jr kk kl lu ju kn ko lv kq kr ks lw ku kv kw lx ky kz la ij bi translated">就在上周，我在复习度量学习，突然想到它和对比学习有一些相似之处。我当时没有时间去做一次深潜来满足我的好奇心，尽管我很想这样做。一个问答平台，我可以问，“度量学习和对比学习的主要区别是什么？”通过迅速提供一个可靠的、详细的答案，会使这个话题变得迅速而富有成效。</p><p id="12a7" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">还记得上一次你在谷歌上研究一个特定的主题，进行了几十次查询来寻找相关的网页结果，随后，自己煞费苦心地合成了一个长达一段的答案吗？如果QA系统能自动为你做这件事会怎么样？</p><p id="5498" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">长格式问题回答(LFQA)系统试图复制和自动化这一艰巨的活动。由于这些QA系统相对较新，研究人员在一个公开可用的数据集上为它们训练模型——Eli 5(解释一下，就像我五岁一样)。ELI5来源于subred dit<a class="ae ly" href="https://www.reddit.com/r/explainlikeimfive/" rel="noopener ugc nofollow" target="_blank">/r/explain like im five/</a>，它抓住了综合来自多个网络来源的信息并生成五岁儿童能够理解的答案的挑战。</p><p id="939c" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">那么，ELI5数据集中有哪些问题和答案呢？你可以随意看看r/explainlikeimfive/ subreddit，或者更好的是，在展会上查看ELI5专用的<a class="ae ly" href="https://facebookresearch.github.io/ELI5/" rel="noopener ugc nofollow" target="_blank">网站</a>。</p><p id="b2e3" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">尽管由Angela Fan、Yacine Jernite和Micheal Auli领导的脸书人工智能研究团队公开发布了ELI5数据集和附带的语言模型，但还没有现成的QA平台允许用户轻松定制这样的LFQA系统。直到现在。</p><h1 id="8e61" class="lb lc iq bd ld le lf lg lh li lj lk ll jw lm jx ln jz lo ka lp kc lq kd lr ls bi translated">干草堆里的LFQA</h1><p id="8d88" class="pw-post-body-paragraph kf kg iq kh b ki lt jr kk kl lu ju kn ko lv kq kr ks lw ku kv kw lx ky kz la ij bi translated"><a class="ae ly" href="https://github.com/deepset-ai/haystack" rel="noopener ugc nofollow" target="_blank"> Haystack </a>是一个端到端的开源框架，使用户能够为各种问答和语义搜索案例构建健壮的生产就绪管道。从0.9.0版本开始，Haystack支持LFQA以及之前支持的QA和语义搜索场景。</p><p id="82f7" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">使用Haystack创建自己的端到端LFQA管道非常简单。Haystack上的LFQA由三个主要模块组成:文档存储、检索器和生成器。让我们更深入地了解这三个模块，以及它们如何适应LFQA平台。</p><figure class="ma mb mc md gt me gh gi paragraph-image"><div role="button" tabindex="0" class="mf mg di mh bf mi"><div class="gh gi lz"><img src="../Images/96993da963ce4b85cadc1f6e766abd3f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*x2PvAkCyDhxnw6jb6tNzhQ.png"/></div></div><p class="ml mm gj gh gi mn mo bd b be z dk translated">Haystack中的LFQA组件和问答流程。图片由作者提供。</p></figure><h1 id="a3b3" class="lb lc iq bd ld le lf lg lh li lj lk ll jw lm jx ln jz lo ka lp kc lq kd lr ls bi translated">文档存储</h1><p id="a2d6" class="pw-post-body-paragraph kf kg iq kh b ki lt jr kk kl lu ju kn ko lv kq kr ks lw ku kv kw lx ky kz la ij bi translated">顾名思义，DocumentStore保存您的文档。Haystack有几种文档存储解决方案可用于不同的用例。对于LFQA，我们需要使用一个向量优化的文档存储，其中嵌入的文档向量表示我们的文档。因此，在我们的演示中，我们将使用FAISSDocumentStore，但我们也可以很容易地使用Haystack平台上的任何其他向量优化文档存储，如Milvus或最近添加的Weaviate。</p><h1 id="a45c" class="lb lc iq bd ld le lf lg lh li lj lk ll jw lm jx ln jz lo ka lp kc lq kd lr ls bi translated">取回的人</h1><p id="e976" class="pw-post-body-paragraph kf kg iq kh b ki lt jr kk kl lu ju kn ko lv kq kr ks lw ku kv kw lx ky kz la ij bi translated">在为给定的查询生成答案之前，我们的QA系统需要找到支持文档。检索器模块的工作是通过计算查询和文档向量之间的相似性来找到最佳候选文档。为了找到与我们的查询最匹配的文档，我们将使用Haystack的密集检索器之一EmbeddingRetriever。检索器首先通过它的语言模型传递查询，以获得查询嵌入。然后，通过比较嵌入查询和文档存储中嵌入文档向量的点积，我们可以快速找到正确的文档并检索它们。</p><p id="147f" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们将使用已经可用的名为Retribert的BERT变体，它专门针对这个查询/文档匹配任务进行了优化。Retribert语言模型在HuggingFace模型中心公开可用，其训练的细节可在<a class="ae ly" href="https://yjernite.github.io/lfqa.html#4.a---Contrastive-Training-with-ELI5-In-Batch-Negatives" rel="noopener ugc nofollow" target="_blank">这里</a>获得。</p><h1 id="dbf2" class="lb lc iq bd ld le lf lg lh li lj lk ll jw lm jx ln jz lo ka lp kc lq kd lr ls bi translated">发电机</h1><p id="559f" class="pw-post-body-paragraph kf kg iq kh b ki lt jr kk kl lu ju kn ko lv kq kr ks lw ku kv kw lx ky kz la ij bi translated">在检索器返回与我们的查询最相关的文档之后，我们就可以将选择的文档输入到基于ELI5 BART的模型中，为给定的查询生成答案。ELI5 BART语言模型也可以在HuggingFace hub上使用，它具有由Haystack的Seq2SeqGenerator实现的seq2seq(例如机器翻译)架构。</p><p id="8e28" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">为了从检索器找到的支持文档中生成一个很长的答案，我们将查询和支持文档连接起来，并将其作为输入通过ELI5 BART模型。模型的输出是我们生成的答案。关于如何训练ELI5模型的更多细节，请参考此<a class="ae ly" href="https://yjernite.github.io/lfqa.html#5.-Generating-Answers-with-a-Sequence-to-Sequence-Model" rel="noopener ugc nofollow" target="_blank">文档</a>。</p><h1 id="0fc1" class="lb lc iq bd ld le lf lg lh li lj lk ll jw lm jx ln jz lo ka lp kc lq kd lr ls bi translated">LFQA演示</h1><p id="233a" class="pw-post-body-paragraph kf kg iq kh b ki lt jr kk kl lu ju kn ko lv kq kr ks lw ku kv kw lx ky kz la ij bi translated">现在，我们对构建LFQA系统所必需的重要组件有了更好的理解，让我们使用Haystack来构建和测试它吧！</p><p id="6ee1" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在本文的其余部分，我们将向您展示如何使用上面提到的现成组件快速创建LFQA部署场景。我们将使用HuggingFace Wiki片段<a class="ae ly" href="https://huggingface.co/datasets/wiki_snippets" rel="noopener ugc nofollow" target="_blank">数据集</a>(来自100k维基百科文档的100个单词的段落)作为我们LFQA系统的源文档。然后我们将使用类似于ELI5的问题来查询系统，看看我们会得到什么样的答案。</p><p id="ad1f" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">为了遵循这一部署场景，您可以使用谷歌的合作实验室<a class="ae ly" href="https://colab.research.google.com/drive/1gPlb6DWy2f3GmMQ-asWdwr2PFhihc2NJ" rel="noopener ugc nofollow" target="_blank">笔记本</a>进行免费的GPU访问。</p><h1 id="dbfb" class="lb lc iq bd ld le lf lg lh li lj lk ll jw lm jx ln jz lo ka lp kc lq kd lr ls bi translated">搭建干草堆</h1><p id="614b" class="pw-post-body-paragraph kf kg iq kh b ki lt jr kk kl lu ju kn ko lv kq kr ks lw ku kv kw lx ky kz la ij bi translated">我们将从所需库的pip安装开始。在我们的例子中，我们需要的只是草堆和拥抱脸数据集。</p><figure class="ma mb mc md gt me"><div class="bz fp l di"><div class="mp mq l"/></div></figure><h1 id="8350" class="lb lc iq bd ld le lf lg lh li lj lk ll jw lm jx ln jz lo ka lp kc lq kd lr ls bi translated">初始化文档存储</h1><p id="3c5b" class="pw-post-body-paragraph kf kg iq kh b ki lt jr kk kl lu ju kn ko lv kq kr ks lw ku kv kw lx ky kz la ij bi translated">现在我们已经安装了所需的库及其依赖项，包括HuggingFace transformers等等，我们准备初始化我们的QA管道。我们将从FAISSDocumentStore开始存储我们的文档。</p><figure class="ma mb mc md gt me"><div class="bz fp l di"><div class="mp mq l"/></div></figure><p id="a330" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这个一行程序几乎不需要额外的解释。我们将使用FAISSDocumentStore的默认风格和“平面”索引。我们需要将vector_dim参数初始化为128，因为我们的Retribert语言模型将查询和文档编码成一个128维的向量。</p><h1 id="555b" class="lb lc iq bd ld le lf lg lh li lj lk ll jw lm jx ln jz lo ka lp kc lq kd lr ls bi translated">将维基百科文档添加到文档存储</h1><p id="ae9f" class="pw-post-body-paragraph kf kg iq kh b ki lt jr kk kl lu ju kn ko lv kq kr ks lw ku kv kw lx ky kz la ij bi translated">在FAISSDocumentStore初始化之后，我们将加载并存储我们的维基百科段落。HuggingFace数据集库提供了一种简单方便的方法来加载像Wiki片段这样的大型数据集。例如，Wiki snippets数据集有超过1700万个Wikipedia段落，但是我们将流式传输前十万个段落，并将它们存储在FAISSDocumentStore中。</p><figure class="ma mb mc md gt me"><div class="bz fp l di"><div class="mp mq l"/></div></figure><p id="baa3" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在，让我们对前100k个Wiki片段进行编写迭代，并将它们保存到我们的DocumentStore:</p><figure class="ma mb mc md gt me"><div class="bz fp l di"><div class="mp mq l"/></div></figure><p id="85f9" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在我们所有的文档都在FAISSDocumentStore中，我们需要初始化第二个Haystack组件——检索器。对于LFQA，我们将使用EmbeddingRetriever，它是用我们简单讨论过的retriebert-base-un cased语言模型初始化的。在检索器初始化之后，我们准备计算每个文档的嵌入，并将它们存储在文档存储中。</p><figure class="ma mb mc md gt me"><div class="bz fp l di"><div class="mp mq l"/></div></figure><p id="eb67" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">喝杯咖啡，因为更新FAISSDocumentStore中所有维基百科文档的嵌入大约需要15分钟。我们可以通过使用专用的GPU实例来加速文档嵌入过程，但是，为了演示的目的，即使是Colab的GPU也可以做得很好。</p><h1 id="3cd6" class="lb lc iq bd ld le lf lg lh li lj lk ll jw lm jx ln jz lo ka lp kc lq kd lr ls bi translated">测试寻回犬</h1><p id="56f1" class="pw-post-body-paragraph kf kg iq kh b ki lt jr kk kl lu ju kn ko lv kq kr ks lw ku kv kw lx ky kz la ij bi translated">在我们盲目地使用EmbeddingRetriever获取文档并将其传递给答案生成器之前，让我们首先对其进行经验测试，以确保示例查询找到相关的文档。我们将使用Haystack的预制组件进行文档搜索DocumentSearchPipeline。当你尝试类似ELI5的问题时，不要忘记你只是使用了十万个维基片段中的一小部分。在提问之前，使用下面的管道来确保您想要的主题和文档已经在数据库中。</p><figure class="ma mb mc md gt me"><div class="bz fp l di"><div class="mp mq l"/></div></figure><p id="3f5d" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">事实上，DocumentSearchPipeline确实找到了相关文档:</p><figure class="ma mb mc md gt me"><div class="bz fp l di"><div class="mp mq l"/></div></figure><h1 id="2b6b" class="lb lc iq bd ld le lf lg lh li lj lk ll jw lm jx ln jz lo ka lp kc lq kd lr ls bi translated">发电机</h1><p id="0ae2" class="pw-post-body-paragraph kf kg iq kh b ki lt jr kk kl lu ju kn ko lv kq kr ks lw ku kv kw lx ky kz la ij bi translated">LFQA堆栈中的最后一个组件是生成器。我们将使用LFQA的特定模型初始化Haystack的通用seq 2 seq generator——Bart _ Eli 5模型除了这个模型，我们将使用其他参数的默认初始化值。您可以使用其他Seq2SeqGenerator构造函数参数微调文本生成的各个方面。更多细节请参考Haystack <a class="ae ly" href="https://haystack.deepset.ai/docs/latest/apigeneratormd#Class-Seq2SeqGenerator" rel="noopener ugc nofollow" target="_blank">文档</a>。我们需要做的最后一件事是在一个预定义的Haystack管道中连接检索器和生成器GenerativeQAPipeline。</p><figure class="ma mb mc md gt me"><div class="bz fp l di"><div class="mp mq l"/></div></figure><p id="a6b9" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">正如您可能已经猜到的那样，GenerativeQAPipeline结合了检索器和生成器来为我们的查询生成答案。它代表了与我们构建的LFQA系统交互的主要API。</p><h1 id="8f26" class="lb lc iq bd ld le lf lg lh li lj lk ll jw lm jx ln jz lo ka lp kc lq kd lr ls bi translated">运行查询</h1><p id="34c8" class="pw-post-body-paragraph kf kg iq kh b ki lt jr kk kl lu ju kn ko lv kq kr ks lw ku kv kw lx ky kz la ij bi translated">我们将与GenerativeQAPipeline进行交互，以获得我们的查询的答案。除了指定查询本身，我们还将对检索器传递给生成器的匹配文档的数量进行限制。这可以是任何数字，但是对于本演示，我们选择将源限制为4。让我们先问一个类似于ELI5的查询:</p><figure class="ma mb mc md gt me"><div class="bz fp l di"><div class="mp mq l"/></div></figure><p id="95a6" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们得到以下答案:</p><figure class="ma mb mc md gt me"><div class="bz fp l di"><div class="mp mq l"/></div></figure><p id="6041" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">🚀🎇这个答案简直令人震惊。它从百慕大单桅帆船的简要说明开始，并继续阐述使其在帆船类别中广受赞誉的特征。</p><p id="17af" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">让我们试试另一个:</p><figure class="ma mb mc md gt me"><div class="bz fp l di"><div class="mp mq l"/></div></figure><p id="afad" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">答案是:</p><figure class="ma mb mc md gt me"><div class="bz fp l di"><div class="mp mq l"/></div></figure><p id="beee" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">简明扼要。答案不像上一个例子那样详细。但是，我们可以强制模型生成更长的答案。我们需要在Seq2SeqGenerator构造函数中传递可选参数<em class="mr"> min_length </em>(字数)。</p><figure class="ma mb mc md gt me"><div class="bz fp l di"><div class="mp mq l"/></div></figure><p id="c747" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">同一问题的新答案是:</p><figure class="ma mb mc md gt me"><div class="bz fp l di"><div class="mp mq l"/></div></figure><p id="b6a6" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">太好了，这是我们想要的更详细的答案。随意试验，尝试不同的问题，但不要忘记答案来自我们使用的维基百科的小样本。</p></div></div>    
</body>
</html>