<html>
<head>
<title>Select Features for Machine Learning Model with Mutual Information</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">基于互信息的机器学习模型特征选择</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/select-features-for-machine-learning-model-with-mutual-information-534fe387d5c8?source=collection_archive---------3-----------------------#2021-06-29">https://towardsdatascience.com/select-features-for-machine-learning-model-with-mutual-information-534fe387d5c8?source=collection_archive---------3-----------------------#2021-06-29</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="7e7a" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">为什么我们需要为机器学习模型挑选特征，如何使用互信息来选择特征，以及使用Scikit-Learn包中的互信息工具。</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/f2a3a99e4ac0c99fcb807cfa91ce1129.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*1t7fgC3mzsfM74pA"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图片来自<a class="ae ky" href="https://unsplash.com/photos/hJ5uMIRNg5k" rel="noopener ugc nofollow" target="_blank">unsplash.com</a></p></figure><h1 id="7958" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">特征选择问题</h1><p id="d7f8" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">当用一组适当的训练数据进行训练时，机器学习模型是惊人的。教科书中描述的ML模型和使用来自Scikit-learn的数据集，样本数据集已经被仔细策划并有效地用于ML模型。</p><p id="e933" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">问题是，在现实生活中，训练数据可能是海量的，根本不适合任何模型。例如，您希望训练一个模型，根据当前100多个客户属性(如年龄、位置、国家/地区、活跃访问次数、上次访问时间等等)来预测下个月的收入。</p><p id="7439" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">有些属性可能有用，有些可能完全没用，比如客户名称。</p><p id="8231" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">难怪许多ML专家说，为ML选择正确的特性是最重要的步骤之一，甚至比ML模型本身还重要！</p><h1 id="d869" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">相互信息是如何工作的</h1><p id="c981" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">互信息可以回答这个问题:有没有一种方法可以在一个特性和目标之间建立一个可测量的联系。</p><p id="3fdb" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">使用互信息作为特征选择器有两个好处:</p><ul class=""><li id="6672" class="ms mt it lt b lu mn lx mo ma mu me mv mi mw mm mx my mz na bi translated">MI是模型中立的，这意味着该解决方案可以应用于各种ML模型。</li><li id="be57" class="ms mt it lt b lu nb lx nc ma nd me ne mi nf mm mx my mz na bi translated">MI解的快。</li></ul><p id="38ee" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">那么，什么是互信息呢？</p><p id="aada" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">如果你熟悉决策树分类器。它与我在另一篇文章<a class="ae ky" rel="noopener" target="_blank" href="/understand-decision-tree-classifier-8a7497d4c5b3">中描述的信息增益100%相同，理解决策树分类器</a>。</p><p id="afc4" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">互信息测量目标值条件下的<strong class="lt iu">熵下降</strong>。我发现对这个概念最干净的解释是这个公式:</p><pre class="kj kk kl km gt ng nh ni nj aw nk bi"><span id="003d" class="nl la it nh b gy nm nn l no np">MI(feature;target) = Entropy(feature) - Entropy(feature|target)</span></pre><p id="1bad" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">MI分数将在从0到∞的范围内。值越高，该特征和目标之间的联系越紧密，这表明我们应该将该特征放在训练数据集中。如果MI分数为0或非常低，如0.01。低分表明该特征和目标之间的联系很弱。</p><h1 id="c109" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">使用Scikit中的交互信息——学习Python</h1><p id="36ca" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">您可以自己编写一个MI函数，或者使用Scikit-Learn中现成的函数。</p><p id="43f6" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">我将使用来自Scikit-Learn的<a class="ae ky" href="https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_breast_cancer.html" rel="noopener ugc nofollow" target="_blank">乳腺癌数据集</a>来构建一个应用了互信息的样本ML模型。使用<a class="ae ky" href="https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html" rel="noopener ugc nofollow" target="_blank">决策树分类器</a>从癌症数据中训练3个数据集，并比较结果，以查看MI分数将如何影响ML模型的有效性。</p><ol class=""><li id="425e" class="ms mt it lt b lu mn lx mo ma mu me mv mi mw mm nq my mz na bi translated">训练数据集1，使用所有特征。</li><li id="8402" class="ms mt it lt b lu nb lx nc ma nd me ne mi nf mm nq my mz na bi translated">训练数据集2，仅使用MI分数大于0.2的要素</li><li id="1881" class="ms mt it lt b lu nb lx nc ma nd me ne mi nf mm nq my mz na bi translated">训练数据集3，仅使用MI分数小于0.2的要素</li></ol><p id="2107" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><strong class="lt iu">第一步。加载乳腺癌数据</strong></p><pre class="kj kk kl km gt ng nh ni nj aw nk bi"><span id="dbc9" class="nl la it nh b gy nm nn l no np">from sklearn.datasets import load_breast_cancer as LBC<br/>cancer = LBC()<br/>X = cancer['data']<br/>y = cancer['target']</span></pre><p id="0dee" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><strong class="lt iu">第二步。计算MI分数</strong></p><pre class="kj kk kl km gt ng nh ni nj aw nk bi"><span id="0053" class="nl la it nh b gy nm nn l no np">from sklearn.feature_selection import mutual_info_classif as MIC<br/>mi_score = MIC(X,y)<br/>print(mi_score)</span></pre><p id="72e6" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">您将看到mi_score数组，如下所示:</p><pre class="kj kk kl km gt ng nh ni nj aw nk bi"><span id="bed3" class="nl la it nh b gy nm nn l no np">[0.37032947 0.09670886 0.40294198 0.36009957 0.08427789 0.21318114<br/> 0.37337734 0.43985571 0.06456878 0.00276314 0.24866738 0.00189163<br/> 0.27600984 0.33955538 0.01503326 0.07603828 0.11825812 0.12879402<br/> 0.0096701  0.03802394 0.45151801 0.12293047 0.47595645 0.46426102<br/> 0.09558928 0.22647456 0.31469449 0.43696443 0.0971793  0.06735096]</span></pre><p id="a2d8" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">30个数字代表30个特征的MI分数。</p><p id="28e9" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><strong class="lt iu">第三步。构建3个训练数据和测试数据集</strong></p><p id="4285" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">无MI评分治疗的数据集1，名称标有<code class="fe nr ns nt nh b">_1</code>。注意，所有三个数据集将使用相同的<code class="fe nr ns nt nh b">y_train</code>和<code class="fe nr ns nt nh b">y_test</code>。因此，不需要分离目标数据。</p><pre class="kj kk kl km gt ng nh ni nj aw nk bi"><span id="4a05" class="nl la it nh b gy nm nn l no np">from sklearn.model_selection import train_test_split as tts<br/>X_train_1,X_test_1,y_train,y_test = tts(<br/>    X,y<br/>    ,random_state=0<br/>    ,stratify=y<br/>)</span></pre><p id="b7e8" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">包含MI分数大于0.2的要素的数据集2</p><pre class="kj kk kl km gt ng nh ni nj aw nk bi"><span id="6a62" class="nl la it nh b gy nm nn l no np">import numpy as np<br/>mi_score_selected_index = np.where(mi_scores &gt;0.2)[0]<br/>X_2 = X[:,mi_score_selected_index]<br/>X_train_2,X_test_2,y_train,y_test = tts(<br/>    X_2,y<br/>    ,random_state=0<br/>    ,stratify=y<br/>)</span></pre><p id="b1b7" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">你会看到X_2的形状是<code class="fe nr ns nt nh b">(569,15)</code>而不是<code class="fe nr ns nt nh b">(569,30)</code>。这是因为15个特征的Mi分数大于<code class="fe nr ns nt nh b">0.2</code>。</p><p id="beb5" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">包含Mi分数小于0.2的要素的数据集3</p><pre class="kj kk kl km gt ng nh ni nj aw nk bi"><span id="0187" class="nl la it nh b gy nm nn l no np">mi_score_selected_index = np.where(mi_scores &lt; 0.2)[0]<br/>X_3 = X[:,mi_score_selected_index]<br/>X_train_3,X_test_3,y_train,y_test = tts(<br/>    X_3,y<br/>    ,random_state=0<br/>    ,stratify=y<br/>)</span></pre><p id="86e5" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">巧合的是，0.2 MI分数将30个特征分成15个和15个。</p><p id="9859" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><strong class="lt iu">第四步。用决策树分类器比较3个数据集</strong></p><pre class="kj kk kl km gt ng nh ni nj aw nk bi"><span id="c2d5" class="nl la it nh b gy nm nn l no np">from sklearn.tree import DecisionTreeClassifier as DTC<br/>model_1 = DTC().fit(X_train_1,y_train)<br/>model_2 = DTC().fit(X_train_2,y_train)<br/>model_3 = DTC().fit(X_train_3,y_train)<br/>score_1 = model_1.score(X_test_1,y_test)<br/>score_2 = model_2.score(X_test_2,y_test)<br/>score_3 = model_3.score(X_test_3,y_test)<br/>print(f"score_1:{score_1}\n score_2:{score_2}\n score_3:{score_3}")</span></pre><p id="4624" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">结果是:</p><pre class="kj kk kl km gt ng nh ni nj aw nk bi"><span id="9343" class="nl la it nh b gy nm nn l no np">score_1:0.9300699300699301<br/>score_2:0.9370629370629371<br/>score_3:0.8251748251748252</span></pre><p id="ea78" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">看，包含15个要素的数据集2的MI &gt; 0.2，精度达到0.93，与包含所有要素的数据集1一样好。而score_3仅为0.82，这是15个特征的结果，这些特征具有MI score &lt; 0.2.</p><p id="5e28" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">From this sample, it is clear that the MI score can be used as a signal for feature selection.</p><h1 id="ef50" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">Use the feature selector from Scikit-Learn</h1><p id="087b" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">In real ML projects, you may want to use the top n features, or top n percentile features instead of using a specified number 0.2 like the sample above. Scikit-Learn also provides <a class="ae ky" href="https://scikit-learn.org/stable/modules/classes.html#module-sklearn.feature_selection" rel="noopener ugc nofollow" target="_blank">许多选择器</a>作为方便的工具。以便您不必手动计算MI分数并获取所需的特征。下面是选择前50%特性的示例，其他选择器也有类似的用法。</p><pre class="kj kk kl km gt ng nh ni nj aw nk bi"><span id="3b27" class="nl la it nh b gy nm nn l no np">from sklearn.feature_selection import SelectPercentile as SP<br/>selector = SP(percentile=50) # select features with top 50% MI scores<br/>selector.fit(X,y)<br/>X_4 = selector.transform(X)<br/>X_train_4,X_test_4,y_train,y_test = tts(<br/>    X_4,y<br/>    ,random_state=0<br/>    ,stratify=y<br/>)<br/>model_4 = DTC().fit(X_train_4,y_train)<br/>score_4 = model_4.score(X_test_4,y_test)<br/>print(f"score_4:{score_4}")</span></pre><h1 id="1e12" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">参考链接</h1><ul class=""><li id="5f39" class="ms mt it lt b lu lv lx ly ma nu me nv mi nw mm mx my mz na bi translated"><a class="ae ky" href="https://machinelearningmastery.com/information-gain-and-mutual-information/" rel="noopener ugc nofollow" target="_blank">机器学习的信息增益和互信息</a></li><li id="91e0" class="ms mt it lt b lu nb lx nc ma nd me ne mi nf mm mx my mz na bi translated"><a class="ae ky" href="https://scikit-learn.org/stable/modules/feature_selection.html" rel="noopener ugc nofollow" target="_blank"> 1.13。功能选择</a></li><li id="7aa1" class="ms mt it lt b lu nb lx nc ma nd me ne mi nf mm mx my mz na bi translated"><a class="ae ky" rel="noopener" target="_blank" href="/understand-decision-tree-classifier-8a7497d4c5b3">理解决策树分类器</a></li></ul><h1 id="7920" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">附录——准则</h1><p id="772b" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">您可以在Jupyter笔记本或<a class="ae ky" href="https://code.visualstudio.com/docs/python/jupyter-support-py" rel="noopener ugc nofollow" target="_blank"> vscode python interactive </a>窗口中复制并运行以下代码。</p><pre class="kj kk kl km gt ng nh ni nj aw nk bi"><span id="2091" class="nl la it nh b gy nm nn l no np">#%% load cancer data<br/>from sklearn.datasets import load_breast_cancer as LBC<br/>cancer = LBC()<br/>X = cancer['data']<br/>y = cancer['target']<br/><br/>#%% compute MI scores<br/>from sklearn.feature_selection import mutual_info_classif as MIC<br/>mi_scores = MIC(X,y)<br/>print(mi_scores)<br/><br/>#%% prepare dataset 1<br/>from sklearn.model_selection import train_test_split as tts<br/>X_train_1,X_test_1,y_train,y_test = tts(<br/>    X,y<br/>    ,random_state=0<br/>    ,stratify=y<br/>)<br/><br/>#%% prepare dataset 2, MI &gt; 0.2<br/>import numpy as np<br/>mi_score_selected_index = np.where(mi_scores &gt;0.2)[0]<br/>X_2 = X[:,mi_score_selected_index]<br/>X_train_2,X_test_2,y_train,y_test = tts(<br/>    X_2,y<br/>    ,random_state=0<br/>    ,stratify=y<br/>)<br/><br/>#%% prepare dataset 3, MI &lt;0.2<br/>mi_score_selected_index = np.where(mi_scores &lt; 0.2)[0]<br/>X_3 = X[:,mi_score_selected_index]<br/>X_train_3,X_test_3,y_train,y_test = tts(<br/>    X_3,y<br/>    ,random_state=0<br/>    ,stratify=y<br/>)<br/><br/>#%% compare results with Decision Tree Classifier<br/>from sklearn.tree import DecisionTreeClassifier as DTC<br/>model_1 = DTC().fit(X_train_1,y_train)<br/>model_2 = DTC().fit(X_train_2,y_train)<br/>model_3 = DTC().fit(X_train_3,y_train)<br/>score_1 = model_1.score(X_test_1,y_test)<br/>score_2 = model_2.score(X_test_2,y_test)<br/>score_3 = model_3.score(X_test_3,y_test)<br/>print(f"score_1:{score_1}\n score_2:{score_2}\n score_3:{score_3}")<br/><br/>#%% use Scikit-learn feature selector<br/>from sklearn.feature_selection import SelectPercentile as SP<br/>selector = SP(percentile=50) # select features with top 50% MI scores<br/>selector.fit(X,y)<br/>X_4 = selector.transform(X)<br/>X_train_4,X_test_4,y_train,y_test = tts(<br/>    X_4,y<br/>    ,random_state=0<br/>    ,stratify=y<br/>)<br/>model_4 = DTC().fit(X_train_4,y_train)<br/>score_4 = model_4.score(X_test_4,y_test)<br/>print(f"score_4:{score_4}")</span></pre></div></div>    
</body>
</html>