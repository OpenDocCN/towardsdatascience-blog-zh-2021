<html>
<head>
<title>Intuitions behind different Activation Functions in Deep Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">深度学习中不同激活函数背后的直觉</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/intuitions-behind-different-activation-functions-in-deep-learning-a2b1c8d044a?source=collection_archive---------24-----------------------#2021-06-08">https://towardsdatascience.com/intuitions-behind-different-activation-functions-in-deep-learning-a2b1c8d044a?source=collection_archive---------24-----------------------#2021-06-08</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="4a75" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated"><strong class="ak">衍生产品、优点、缺点、Python实现和用例</strong></h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/3770a9deb884b21208ee0f187498c44f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0NOue_j8ffzOfPAp5rVhzA.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">来源:乔希·里默尔在Unsplash上的照片</p></figure><p id="d532" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">正如我们所知，在神经网络中，神经元以相应的权重、偏置和各自的激活函数工作。权重与输入相乘，然后在进入下一层之前对元素应用激活函数。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi lr"><img src="../Images/2538f065c079f18bf8aae1d6fd0579fb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1248/format:webp/1*Sw2LLw7Y8ZemggfxCpLIdQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="b163" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx ir">为什么需要激活功能？</strong></p><p id="f3e0" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">激活函数有助于将非线性引入神经元的输出，这有助于准确性、计算效率和收敛速度。为了优化的目的，激活函数相对于权重应该是单调的、可微分的和快速收敛的。</p><p id="550f" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在本文中，我们将讨论以下重要的激活功能:</p><p id="d90e" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">1)乙状结肠</p><p id="5443" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">2)双曲正切值</p><p id="4a9b" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">3)整流线性单元(ReLU)</p><p id="5cf9" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">4)泄漏的ReLU</p><p id="9353" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">5)指数线性单位(ELU)</p><p id="79c8" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">6)参数重新逻辑(预逻辑)</p><p id="14b7" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">7) Softmax</p><p id="7df4" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx ir"> 1) </strong> <strong class="kx ir">乙状结肠</strong></p><p id="de29" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">这是最常见的激活功能之一，也称为逻辑功能。该函数定义为:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ls"><img src="../Images/a80f4cd8544ea135464ea624b8ebb312.png" data-original-src="https://miro.medium.com/v2/resize:fit:888/format:webp/1*WBUZ1mskqQAiQSs8j8uPkw.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="b414" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">该函数及其导数如下所示:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi lt"><img src="../Images/95574f3d0101be8d9346b23a5cdd5b9e.png" data-original-src="https://miro.medium.com/v2/resize:fit:992/format:webp/1*pVznS3BVmd4ggeNRwtaDVQ.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="98b1" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">从上图中，我们观察到:</p><p id="c485" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">a)函数看起来像S形曲线</p><p id="06e7" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">b)该函数在0和1之间转换输入值，并以0.5 ie为中心。<strong class="kx ir">不以零为中心。</strong></p><p id="e9a0" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">c)函数是单调且可微的。注意，sigmoid函数的导数范围在0到0.25之间。</p><p id="f5ec" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx ir">乙状结肠的缺点</strong></p><p id="ea5e" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx ir"> a) </strong> <strong class="kx ir">消失梯度:</strong>在神经网络中，在反向传播阶段，权重(w)更新如下:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi lu"><img src="../Images/3844b58d616c8771796329cc6c6feb72.png" data-original-src="https://miro.medium.com/v2/resize:fit:1284/format:webp/1*ai56CFVDLCoY_3HY_Uu2Nw.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="01fe" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">从图中，我们可以理解导数的范围是0到0.25。由于微分的链式法则，导数可能很低，以至于权重可能不会显著改变或更新。这导致在反向传播阶段更新权重的问题，并且没有值得注意的信息被传递到随后的层。这个问题叫做消失梯度。</p><p id="a4d7" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx ir"> b) </strong> <strong class="kx ir">由于其指数性质，计算开销很大</strong>。</p><p id="adf0" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx ir"> c) </strong>输出<strong class="kx ir">不是以零为中心</strong>，这降低了更新权重的效率。</p><p id="3422" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx ir"> 2) </strong> <strong class="kx ir">双曲正切(Tanh) </strong></p><p id="23b7" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">另一个非常常用的激活函数是Tanh，其定义为:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi lv"><img src="../Images/0e1eb64b31602ece5c8a4759da868e49.png" data-original-src="https://miro.medium.com/v2/resize:fit:360/format:webp/1*scufeA8gL_Y9rU3EG85yEQ.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图片来自作者</p></figure><p id="fd4a" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">该函数及其导数如下所示:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi lw"><img src="../Images/4e2908b1669575ceea6dbb1aae6b38e8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1030/format:webp/1*3dSkwU0lqaRHtrTAoAqzPA.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图片来自作者</p></figure><p id="440f" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">从上图可以清楚地看出:</p><p id="6836" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">a)函数看起来像S形曲线</p><p id="c3b3" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">b)该函数转换-1和1之间的值，并以0为中心。</p><p id="2281" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">c)函数是单调且可微的。注意，双曲正切函数的导数范围在0到1之间。</p><p id="e221" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">Tanh和sigmoid都是单调递增函数，当它接近+inf和-inf时，渐近于某个有限值。</p><p id="be6a" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx ir">Tanh的缺点:</strong>与Sigmoid类似，Tanh也有消失梯度的问题，并且由于其指数运算而计算量大。</p><p id="8186" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx ir">双曲正切函数优于Sigmoid函数的优势:</strong>正如我们注意到的，双曲正切函数以零为中心，这意味着双曲正切函数关于原点对称。因此，如果训练集上每个输入变量的平均值接近于零，收敛通常会更快。</p><p id="8eee" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx ir"> 3) </strong> <strong class="kx ir">整流线性单元(ReLU) </strong></p><p id="1455" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">ReLU是更新隐藏层时最常用的激活函数。当传递负输入时，ReLU返回0，对于任何正输入，它返回值本身。该功能定义为:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi lx"><img src="../Images/ddb1672143cbae6a688c513ed1389b8c.png" data-original-src="https://miro.medium.com/v2/resize:fit:336/format:webp/1*F_M74hvCWVMDQpzEPIREXw.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图片来自作者</p></figure><p id="9a61" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">该函数及其导数如下所示:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ly"><img src="../Images/795811a9d4a8bc952cadc9be018e4e70.png" data-original-src="https://miro.medium.com/v2/resize:fit:966/format:webp/1*FmCGZ5MRo4offGkMEfzpbg.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图片来自作者</p></figure><p id="d8a9" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">从上图中，我们观察到:</p><p id="f0d2" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">a)对于任何小于零的值，该函数输出0，对于正值，该函数是单调和连续的。</p><p id="3a71" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">b)对于z &lt;0 and 1 for z&gt; 0，函数的导数为0，但是函数在点0处不可微。</p><p id="fabb" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">c)它是不可微的，对于负输入，导数是0。</p><p id="3b37" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx ir">ReLU的优点</strong></p><p id="911c" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">a) ReLU克服了消失梯度的问题，因为对于z&gt;0，导数为1。</p><p id="3e63" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">b)由于其简单的方程，与Sigmoid和Tanh激活函数相比，其计算速度更快。</p><p id="e153" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx ir">缺点(Dying ReLU): </strong>如上所述，对于负输入，导数为0，因此等式(1)导致w(new) = w(old)。这意味着，进入这种状态的神经元将停止对错误/输入的变化做出响应(因为梯度为0，所以没有变化)。这就是所谓的垂死的ReLu问题。这导致死神经元不能在反向传播中更新权重。为了克服这个问题，漏ReLU出现了。</p><p id="b051" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx ir">4)</strong>T15】漏液ReLU </p><p id="89a2" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">这是对ReLU的改进，通过调整负输入的函数，如下所示:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi lz"><img src="../Images/048c6640e38e3083bfe05afa4389db50.png" data-original-src="https://miro.medium.com/v2/resize:fit:366/format:webp/1*7MBHVxPq4nJWWjcHbH5x7A.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="aca5" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">该函数及其导数如下所示:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ly"><img src="../Images/c0320f212a1704a84c4585b688907d7d.png" data-original-src="https://miro.medium.com/v2/resize:fit:966/format:webp/1*ihdftMt_8JQmIxJh51hsUQ.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="eab3" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">基本上，泄漏ReLU允许一个小的非零恒定梯度。这确保了神经元不会因为引入非零斜率而死亡。</p><p id="217d" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx ir">Leaky ReLU的缺点:</strong>如果大部分权重是负数，那么根据导数的链式法则，它会乘以0.01的倍数。这将最终导致渐变消失，我们试图克服这一点。</p><p id="cda6" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx ir"> 5) </strong> <strong class="kx ir">指数线性单位(ELU) </strong></p><p id="deb2" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">为了解决这个问题并保持leaky relu的其他特性，elu出现了。这被定义为:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ma"><img src="../Images/e439fd7af63a20ecd21f2e44a1938eab.png" data-original-src="https://miro.medium.com/v2/resize:fit:516/format:webp/1*oL4vgqofM4XWl6gZ_MNl8w.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="eabb" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">该函数及其导数如下所示:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mb"><img src="../Images/69599f13ccc4b169bebe05b364808b56.png" data-original-src="https://miro.medium.com/v2/resize:fit:986/format:webp/1*AZtJGpAKfhZG_YGzqmw6mQ.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="5fe7" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx ir">优点:</strong></p><p id="02e0" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">a)濒死再禄问题得到解决</p><p id="aff2" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">b)输出id以零为中心</p><p id="7919" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">c)不需要在0找到导数</p><p id="e55b" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx ir">缺点:</strong>由于其指数性质，计算量很大。</p><p id="de84" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx ir"> 6) </strong> <strong class="kx ir">参数ReLU (PReLU) </strong></p><p id="2e22" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">这是所有不同ReLU变体的最一般化形式。该函数定义为:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mc"><img src="../Images/218cfdb60a822e85735565d82f63fd93.png" data-original-src="https://miro.medium.com/v2/resize:fit:398/format:webp/1*cJ3rwqmINppU29csIRFkPA.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="6d0e" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">其中，β被授权在反向传播期间学习，并且可以被视为学习参数。</p><p id="298e" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">注意，如果β = 0，类似于ReLU</p><p id="1d25" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">如果β = 0.01，类似于泄漏ReLU</p><p id="e0fe" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx ir">7)</strong>T10】soft max</p><p id="e3d6" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">Softmax计算n个不同事件的概率分布。它计算每个目标类相对于所有可能的目标类的概率。之后，计算出的概率有助于确定给定输入的目标类别。该函数定义如下:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi md"><img src="../Images/ea7860a7cdf089c303fc535ac6d8fa26.png" data-original-src="https://miro.medium.com/v2/resize:fit:560/format:webp/1*Yb7pn0v1E_o5AouVBLFiQw.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="41bd" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">该函数如下所示:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi me"><img src="../Images/896870a43680665fef8d80f2bbae8762.png" data-original-src="https://miro.medium.com/v2/resize:fit:1008/format:webp/1*Fpiwo1m8kPLo5RdKDPCcFA.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="871e" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">Softmax通常用于多类问题。如果层数超过2层，则在最后一层使用该激活功能。“最大”部分返回最大值，“软”部分确保较小的值具有较低的概率，但不会被丢弃。还要注意，所有类别的概率之和将是1。</p><h1 id="82c3" class="mf mg iq bd mh mi mj mk ml mm mn mo mp jw mq jx mr jz ms ka mt kc mu kd mv mw bi translated"><strong class="ak"> Python代码片段</strong></h1><p id="2872" class="pw-post-body-paragraph kv kw iq kx b ky mx jr la lb my ju ld le mz lg lh li na lk ll lm nb lo lp lq ij bi translated">下面的python代码用于为每个函数及其相应的导数创建上面的图形。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nc"><img src="../Images/37942d0067bb27a8331afb41e43c324c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*txi0_Hh6nNtt2EvkoOV6Lw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图片来自作者</p></figure><h1 id="db5c" class="mf mg iq bd mh mi mj mk ml mm mn mo mp jw mq jx mr jz ms ka mt kc mu kd mv mw bi translated"><strong class="ak">不同激活功能的快速汇总</strong></h1><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nd"><img src="../Images/2b0095aa33ef9f42be729ccbeb85508b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jvFJZMNBsHMetGwqFEJGmA.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><h1 id="380c" class="mf mg iq bd mh mi mj mk ml mm mn mo mp jw mq jx mr jz ms ka mt kc mu kd mv mw bi translated"><strong class="ak">更多激活功能</strong></h1><p id="77d2" class="pw-post-body-paragraph kv kw iq kx b ky mx jr la lb my ju ld le mz lg lh li na lk ll lm nb lo lp lq ij bi translated">这些是深度学习中经常使用的激活功能，但列表并没有到此为止:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi lr"><img src="../Images/a2ced17323cfe4d69a5f2a90884c183b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1248/format:webp/1*0k0gHwqT4usJcGihBVK-xw.png"/></div></figure><h1 id="a314" class="mf mg iq bd mh mi mj mk ml mm mn mo mp jw mq jx mr jz ms ka mt kc mu kd mv mw bi translated"><strong class="ak">何时使用哪些激活功能</strong></h1><p id="1c24" class="pw-post-body-paragraph kv kw iq kx b ky mx jr la lb my ju ld le mz lg lh li na lk ll lm nb lo lp lq ij bi translated">通常，如果输出范围在(0，1)或(-1，1)之间，则可以使用sigmoid或tanh。另一方面，要预测大于1的输出值，通常使用ReLU，因为根据定义，tanh或sigmoid不合适。</p><p id="db64" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在二元分类器的情况下，应该使用Sigmoid激活函数。预测多类问题的概率时，应在最后一层使用softmax激活函数。同样，tanh或sigmoid通常在隐藏层中效果不佳。应该在隐藏层中使用ReLU或Leaky ReLU。当隐藏层数很高(接近30)时，使用Swish激活功能。</p><p id="1f39" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">然而，激活函数的使用主要取决于数据、手头的问题和预期输出的范围。</p><p id="3fe2" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">希望你喜欢这篇文章！！</p></div><div class="ab cl ne nf hu ng" role="separator"><span class="nh bw bk ni nj nk"/><span class="nh bw bk ni nj nk"/><span class="nh bw bk ni nj"/></div><div class="ij ik il im in"><p id="6a5d" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">免责声明:本文所表达的观点是作者以个人身份发表的意见，而非其雇主</p><p id="377c" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx ir">参考文献:</strong></p><p id="af5f" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">https://arxiv.org/pdf/1811.03378.pdf<a class="ae nm" href="https://arxiv.org/pdf/1811.03378.pdf" rel="noopener ugc nofollow" target="_blank"/></p><p id="00e1" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><a class="ae nm" href="https://machinelearningmastery.com/choose-an-activation-function-for-deep-learning/" rel="noopener ugc nofollow" target="_blank">https://machine learning mastery . com/choose-an-activation-function-for-deep-learning/</a></p></div></div>    
</body>
</html>