<html>
<head>
<title>Introducing OpenHAC— an open source toolkit for digital biomarker analysis and machine learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">介绍open HAC——用于数字生物标记分析和机器学习的开源工具包</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/introducing-openhac-an-open-source-toolkit-for-digital-biomarker-analysis-and-machine-learning-6e107c4524ad?source=collection_archive---------25-----------------------#2021-04-23">https://towardsdatascience.com/introducing-openhac-an-open-source-toolkit-for-digital-biomarker-analysis-and-machine-learning-6e107c4524ad?source=collection_archive---------25-----------------------#2021-04-23</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="0d3e" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">用于人体活动分类研究和模型创建的新视频分析平台。</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/d3d48e6b3074b5ad0754911ab706b11d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*m5c5Hu0y3HC8IDOZtISVmQ.gif"/></div></div></figure><p id="fdf3" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">开放人类活动分类(<a class="ae ln" href="https://chags1313.github.io/OpenHAC/" rel="noopener ugc nofollow" target="_blank"> OpenHAC </a>)是一个开源用户界面，用于数字生物标记提取、分析和探索，并带有构建、评估和部署机器学习分类器的工具。它建立在现有的软件包上，用于量化行为特征和组装机器学习框架。我这个项目的目标是:1)让所有研究人员、开发人员、学者和公民科学家都可以进行数字生物标记研究，以推进健康的数字测量，并创建人类活动表型分析的新工具；2)创建一个个人社区，致力于为人类健康构建具有临床和学术意义的技术解决方案。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi lo"><img src="../Images/4467a8e6b521932d6bdee3d09fdba83e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*VCAH9EACY5UHPcURRakgZA.gif"/></div></div></figure></div><div class="ab cl lp lq hu lr" role="separator"><span class="ls bw bk lt lu lv"/><span class="ls bw bk lt lu lv"/><span class="ls bw bk lt lu"/></div><div class="ij ik il im in"><h1 id="e3a9" class="lw lx iq bd ly lz ma mb mc md me mf mg jw mh jx mi jz mj ka mk kc ml kd mm mn bi translated">数字生物标记提取</h1><p id="585f" class="pw-post-body-paragraph kr ks iq kt b ku mo jr kw kx mp ju kz la mq lc ld le mr lg lh li ms lk ll lm ij bi translated">OpenHAC中可用的数字生物标记功能目前包括面部活动、眼球运动、运动模式、身体关键点和心率。</p><p id="1410" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated"><strong class="kt ir">脸-头-凝视</strong></p><p id="64c7" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">面部、头部和凝视数据依赖于OpenFace进行测量，这些数据被前馈到OpenDBM，以获得临床和学术相关的行为特征。</p><p id="4065" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">open face:【https://github.com/TadasBaltrusaitis/OpenFace T4】</p><p id="db0a" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">open DBM:<a class="ae ln" href="https://github.com/AiCure/open_dbm" rel="noopener ugc nofollow" target="_blank">https://github.com/AiCure/open_dbm</a></p><p id="b71f" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated"><em class="mt">数据收集</em></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/605449669fef5676a83e719590ba34dd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*hyNPcGswy_dZDkr2XMYf8g.gif"/></div></div></figure><p id="b8f1" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated"><em class="mt">输出</em></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mu"><img src="../Images/8f0f72244de18f70633bb8ea5596ed97.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uzMi6GaRan6YPFtWo2xVBQ.png"/></div></div></figure><p id="05a3" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated"><strong class="kt ir">身体姿势</strong></p><p id="834f" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">身体部位包括头部、手臂和腿部的定位，总共15个部位。OpenHAC依赖于OpenCV的图像处理和OpenPose库中使用的预训练模型。</p><p id="6f06" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">https://opencv.org/</p><p id="3528" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">打开姿势:<a class="ae ln" href="https://github.com/CMU-Perceptual-Computing-Lab/openpose" rel="noopener ugc nofollow" target="_blank">https://github.com/CMU-Perceptual-Computing-Lab/openpose</a></p><p id="7587" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated"><em class="mt">数据收集:</em></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/8c2343bb12e82cf3853783676d318e3c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*4gtdFm8Ux0frG6DT1KxBLw.gif"/></div></div></figure><p id="b410" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated"><em class="mt">输出</em></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mv"><img src="../Images/1e976706a666481223b928691fec1fab.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YFtHEE2tcMyT2dZ2HXuVcA.png"/></div></div></figure><p id="f918" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated"><strong class="kt ir">心率</strong></p><p id="38d8" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">OpenHAC的心率测量基于<a class="ae ln" href="http://github.com/habom2310" rel="noopener ugc nofollow" target="_blank"> habom2310 </a>的工作成果。使用心率功能将提取每分钟的心跳数和每帧的准确度。</p><p id="facd" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">使用摄像头测量心率:<a class="ae ln" href="https://github.com/habom2310/Heart-rate-measurement-using-camera" rel="noopener ugc nofollow" target="_blank">https://github . com/habom 2310/使用摄像头测量心率</a></p><p id="0cf6" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated"><em class="mt">数据收集</em></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/f69c79906f3e870b01ae5f8a914e80bd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*0SDjqSflzGJmBiBm5XxwUQ.gif"/></div></div></figure><p id="ff53" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated"><em class="mt">输出</em></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mw"><img src="../Images/ebb6c3d57d4db98a4c526349f0a29777.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*415g0zXCA_lMq0Dwz_fKKQ.png"/></div></div></figure></div><div class="ab cl lp lq hu lr" role="separator"><span class="ls bw bk lt lu lv"/><span class="ls bw bk lt lu lv"/><span class="ls bw bk lt lu"/></div><div class="ij ik il im in"><h1 id="20fd" class="lw lx iq bd ly lz ma mb mc md me mf mg jw mh jx mi jz mj ka mk kc ml kd mm mn bi translated">人类活动分类</h1><p id="062d" class="pw-post-body-paragraph kr ks iq kt b ku mo jr kw kx mp ju kz la mq lc ld le mr lg lh li ms lk ll lm ij bi translated">通过OpenHAC，还可以使用其机器学习分类工具来创建、分析和提取新的数字生物标记特征。将行为特征与手动分类相结合，用户可以为诸如疼痛、困倦、活动水平和非典型运动等行为表现创建有效的分类器。</p><p id="36cb" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">OpenHAC使用由Scicit-Learn支持的PyCaret库来比较、创建、保存、加载和部署机器学习模型。使用OpenHAC的图形用户界面，创建人类活动分类器的过程简单明了。</p><p id="4f48" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated"><strong class="kt ir">设置数据并比较多个模型</strong></p><p id="cc2b" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">OpenHAC使用PyCaret的函数来初始化创建管道的训练环境。然后，它可以使用交叉验证来训练和评估PyCaret库中所有估计器的性能。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi lo"><img src="../Images/9a6bb0d45d91fadd3c96c37edc842632.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*y6ULvTfGPDKUyOiFwPvIOw.gif"/></div></div></figure><p id="18fe" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated"><strong class="kt ir">选择分类器并评估性能</strong></p><p id="55dd" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">OpenHAC可以训练和评估PyCaret库中的任何模型。图用于分析和解释基于维持数据的模型。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi lo"><img src="../Images/eac2a897678b958b6bca7b1ca58fc6b1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*E0aAT8jMAH0NWYTDGxcAVw.gif"/></div></div></figure><p id="2b43" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated"><strong class="kt ir">预测并保存</strong></p><p id="b606" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">OpenHAC在整个数据集上训练模型，然后分配预测标签和分数(预测类的概率)。我们的转换管道和经过训练的模型对象被保存为pickle文件供以后使用。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi lo"><img src="../Images/f50f78876a88c209e65a5e582bb0eda0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*ljzglDxBXAsS8uHHcJ9jcQ.gif"/></div></div></figure></div><div class="ab cl lp lq hu lr" role="separator"><span class="ls bw bk lt lu lv"/><span class="ls bw bk lt lu lv"/><span class="ls bw bk lt lu"/></div><div class="ij ik il im in"><p id="a480" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">对于对人类活动分类感兴趣的人来说，OpenHAC可以是一个很好的工具。它仍在发展中，因此可以从那些愿意帮助它成长的人那里获益。如果你有兴趣参与OpenHAC项目，请通过<a class="ae ln" href="http://hagencolej@gmail.com" rel="noopener ugc nofollow" target="_blank">hagencolej@gmail.com</a>联系我。谢谢大家的支持！！</p><p id="33c8" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">OpenHAC的代码可以在GitHub上找到:<a class="ae ln" href="https://github.com/chags1313/OpenHAC" rel="noopener ugc nofollow" target="_blank">https://github.com/chags1313/OpenHAC</a></p><p id="9e11" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">OpenHAC的wiki可以帮你入门:<a class="ae ln" href="https://github.com/chags1313/OpenHAC/wiki" rel="noopener ugc nofollow" target="_blank">https://github.com/chags1313/OpenHAC/wiki</a></p></div></div>    
</body>
</html>