<html>
<head>
<title>Huber and Ridge Regressions in Python: Dealing with Outliers</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Python中的Huber和Ridge回归:处理异常值</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/huber-and-ridge-regressions-in-python-dealing-with-outliers-dc4fc0ac32e4?source=collection_archive---------14-----------------------#2021-09-17">https://towardsdatascience.com/huber-and-ridge-regressions-in-python-dealing-with-outliers-dc4fc0ac32e4?source=collection_archive---------14-----------------------#2021-09-17</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="7e52" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">如何处理数据集中的异常值</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/6f9e71561867d97baae997aa5cb47cda.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YlbxljeMZcfXpJdKW-OW-Q.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">来源:照片由<a class="ae ky" href="https://pixabay.com/users/natalia_kollegova-5226803/" rel="noopener ugc nofollow" target="_blank"> Natalia_Kollegova </a>来自<a class="ae ky" href="https://pixabay.com/photos/mountains-ridge-ascent-spaciousness-2449967/" rel="noopener ugc nofollow" target="_blank"> Pixabay </a></p></figure><p id="f1a3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">当处理一组数据中的异常值时，传统的线性回归被证明有一些缺点。</p><p id="6228" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">具体来说，如果一个数据点距离该组中的其他点非常远，这可能会显著影响最小二乘回归线，即，接近该组数据点的总体方向的线将因异常值的存在而偏斜。</p><p id="b833" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为了防止这一缺点，可以使用对异常值稳健的修正回归模型。在这个特殊的例子中，我们将看看<strong class="lb iu">胡伯</strong>和<strong class="lb iu">岭</strong>回归模型。</p><h1 id="699a" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">背景</h1><p id="b16b" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">在这种情况下使用的数据集是皮马印第安人糖尿病<a class="ae ky" href="https://www.kaggle.com/uciml/pima-indians-diabetes-database" rel="noopener ugc nofollow" target="_blank">数据集</a>，其最初来自国家糖尿病、消化和肾脏疾病研究所，并根据<a class="ae ky" href="https://creativecommons.org/publicdomain/zero/1.0/" rel="noopener ugc nofollow" target="_blank"> CC0 1.0通用版(CC0 1.0) <br/>公共领域专用许可证</a>提供。</p><p id="5f7d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对于这种特殊情况，将建立一个回归模型来预测患者的体重指数(身体质量指数)水平。</p><p id="3a64" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">当查看身体质量指数的箱线图时，我们可以看到存在明显的异常值，如图表上半部分的数据点所示。</p><pre class="kj kk kl km gt ms mt mu mv aw mw bi"><span id="dce4" class="mx lw it mt b gy my mz l na nb"># Creating plot<br/>plt.boxplot(bmi)<br/>plt.ylabel("BMI")<br/>plt.title("Body Mass Index")<br/># show plot<br/>plt.show()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nc"><img src="../Images/c1b200898cf2673ad5f019fdd93145dc.png" data-original-src="https://miro.medium.com/v2/resize:fit:764/format:webp/1*lPapACGJBhrKZfhg2Kk-1g.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">来源:Jupyter笔记本输出</p></figure><p id="d832" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然后创建数据的相关矩阵:</p><pre class="kj kk kl km gt ms mt mu mv aw mw bi"><span id="d222" class="mx lw it mt b gy my mz l na nb">corr = a.corr()<br/>corr</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nd"><img src="../Images/730e91dfdff6c9fa0184ddc9bb88e984.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3doytLZFkN_Z_uIlxZzNNw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">来源:Jupyter笔记本输出</p></figure><p id="f0d3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这是一个使用seaborn的视频:</p><pre class="kj kk kl km gt ms mt mu mv aw mw bi"><span id="944c" class="mx lw it mt b gy my mz l na nb">sns.heatmap(a.corr());</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ne"><img src="../Images/c854ac2bf7b695d07b10282405a863de.png" data-original-src="https://miro.medium.com/v2/resize:fit:968/format:webp/1*AHw9V_TDgz_CduC6k4qPbA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">来源:Jupyter笔记本输出</p></figure><h1 id="8b89" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">线性回归分析</h1><p id="0a8c" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">在自变量中，<strong class="lb iu">皮肤厚度</strong>和<strong class="lb iu">葡萄糖</strong>被选为假设对身体质量指数有显著影响的两个变量。</p><p id="f8e2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对训练数据进行回归分析，得出以下结果:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nf"><img src="../Images/0ec1b1adc7a6cd8aeb2869baf7c5b8db.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vYxOKwuEZeePMFG5bCjTuQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">来源:Jupyter笔记本输出</p></figure><p id="a5de" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">根据以上结果:</p><ul class=""><li id="f8d1" class="ng nh it lb b lc ld lf lg li ni lm nj lq nk lu nl nm nn no bi translated">皮肤厚度增加1个单位导致身体质量指数增加<strong class="lb iu">0.1597</strong>——保持所有其他变量不变。</li><li id="a49d" class="ng nh it lb b lc np lf nq li nr lm ns lq nt lu nl nm nn no bi translated">葡萄糖水平增加1个单位会导致身体质量指数增加0.0418个单位——保持所有其他变量不变。</li></ul><p id="4572" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们看到这两个变量在所有水平上都非常显著(假设p值为0)。虽然17.5%的R平方很低，但这并不一定意味着这个模型不好。鉴于影响身体质量指数波动的变量范围很广，这只是表明有许多变量是这个模型没有考虑到的。然而，使用的两个独立变量显示了高度显著的关系。</p><p id="0330" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">通过验证集生成预测，并计算均方根误差(RMSE)值。在这种情况下使用RMSE，因为分数对异常值更敏感。分数越高，预测的误差越大。</p><pre class="kj kk kl km gt ms mt mu mv aw mw bi"><span id="2742" class="mx lw it mt b gy my mz l na nb">&gt;&gt;&gt; olspredictions = results.predict(X_val)<br/>&gt;&gt;&gt; print(olspredictions)</span><span id="c6c7" class="mx lw it mt b gy nu mz l na nb">[36.32534071 31.09716546 28.67493585 34.29867119 35.03070074<br/>...<br/> 36.50971909 35.97416079 36.57591618 35.10923948 34.3672371]</span></pre><p id="d24a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">均方根误差的计算方法如下:</p><pre class="kj kk kl km gt ms mt mu mv aw mw bi"><span id="2971" class="mx lw it mt b gy my mz l na nb">&gt;&gt;&gt; mean_squared_error(y_val, olspredictions)<br/>&gt;&gt;&gt; math.sqrt(mean_squared_error(y_val, olspredictions))</span><span id="0ab0" class="mx lw it mt b gy nu mz l na nb">5.830559762277098</span><span id="ebdb" class="mx lw it mt b gy nu mz l na nb">&gt;&gt;&gt; np.mean(y_val)<br/>31.809333333333342</span></pre><p id="baa0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">相对于整个验证集的平均值<strong class="lb iu"> 31.81 </strong>，均方根误差的值为<strong class="lb iu"> 5.83 </strong>。</p><p id="a8fc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在，我们将生成Huber和岭回归模型。同样，将使用这些模型对验证集进行预测，并计算均方根误差。</p><h1 id="51bf" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">胡伯对里奇</h1><p id="f886" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">Huber回归和岭回归都是为了生成一条比标准线性回归对异常值更不敏感的回归线。</p><p id="65cb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然而，这些模型的工作方式略有不同。</p><p id="a9c3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">具体来说，Huber回归依赖于所谓的<em class="nv"> M-estimat </em> e，或者对异常值的敏感度低于平均值的位置度量(如牛津统计词典(Upton和Cook，2014年)所述)。</p><p id="6433" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">岭回归使用所谓的L2 <a class="ae ky" href="https://quick-adviser.com/what-does-l2-regularization-do/#Is_weight_decay_same_as_L2_regularization" rel="noopener ugc nofollow" target="_blank">正则化</a>-使异常值的权重更小，从而对回归线的影响更小。此外，L2正则化试图估计数据的平均值以避免过度拟合，而L1正则化(如在套索回归中使用的)试图估计数据的中位数。</p><p id="f117" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在本例中，将使用Huber和岭回归进行预测，以便根据验证集计算预测的RMSE。然后，性能最佳的模型将用于对整个测试集进行预测。</p><h2 id="e4ff" class="mx lw it bd lx nw nx dn mb ny nz dp mf li oa ob mh lm oc od mj lq oe of ml og bi translated">胡伯回归</h2><p id="5802" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">这是一个Huber回归的例子:</p><pre class="kj kk kl km gt ms mt mu mv aw mw bi"><span id="6fe0" class="mx lw it mt b gy my mz l na nb">hb1 = linear_model.HuberRegressor(epsilon=1.1, max_iter=100, alpha=0.0001, warm_start=False, fit_intercept=True, tol=1e-05)</span></pre><p id="ddfd" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">具体而言，<a class="ae ky" href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.HuberRegressor.html" rel="noopener ugc nofollow" target="_blank">ε</a>的值测量应该被分类为异常值的样本的数量。该值越小，模型对异常值越稳健。</p><p id="5e94" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">从这个观点出发，计算了五个不同ε值的独立Huber回归。</p><pre class="kj kk kl km gt ms mt mu mv aw mw bi"><span id="60cc" class="mx lw it mt b gy my mz l na nb">hb1 = linear_model.HuberRegressor(epsilon=1.1, max_iter=100, alpha=0.0001, warm_start=False, fit_intercept=True, tol=1e-05)</span><span id="e9f0" class="mx lw it mt b gy nu mz l na nb">hb2 = linear_model.HuberRegressor(epsilon=1.8, max_iter=1000, alpha=0.0001, warm_start=False, fit_intercept=True, tol=1e-05)</span><span id="b168" class="mx lw it mt b gy nu mz l na nb">hb3 = linear_model.HuberRegressor(epsilon=2.5, max_iter=1000, alpha=0.0001, warm_start=False, fit_intercept=True, tol=1e-05)</span><span id="217d" class="mx lw it mt b gy nu mz l na nb">hb4 = linear_model.HuberRegressor(epsilon=3.2, max_iter=1000, alpha=0.0001, warm_start=False, fit_intercept=True, tol=1e-05)</span><span id="d50f" class="mx lw it mt b gy nu mz l na nb">hb5 = linear_model.HuberRegressor(epsilon=3.9, max_iter=1000, alpha=0.0001, warm_start=False, fit_intercept=True, tol=1e-05)</span></pre><p id="d42b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">根据验证集对五个模型进行预测。以第一个模型为例:</p><pre class="kj kk kl km gt ms mt mu mv aw mw bi"><span id="207b" class="mx lw it mt b gy my mz l na nb">&gt;&gt;&gt; hubermodel1 = hb1.fit(X_train,y_train)<br/>&gt;&gt;&gt; huberpredictions1 = hb1.predict(X_val)<br/>&gt;&gt;&gt; print(huberpredictions1)</span><span id="a067" class="mx lw it mt b gy nu mz l na nb">[35.67051275 29.43501067 27.18925225 33.91769821 34.47019359<br/>...<br/> 31.52694684 31.0940833  35.37464065 30.99181107 36.11032014]</span></pre><p id="3724" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">计算的均方误差如下:</p><ul class=""><li id="6c86" class="ng nh it lb b lc ld lf lg li ni lm nj lq nk lu nl nm nn no bi translated">hb1 = 5.803</li><li id="8f70" class="ng nh it lb b lc np lf nq li nr lm ns lq nt lu nl nm nn no bi translated">hb2 = 5.800</li><li id="de44" class="ng nh it lb b lc np lf nq li nr lm ns lq nt lu nl nm nn no bi translated">hb3 = 5.816</li><li id="de25" class="ng nh it lb b lc np lf nq li nr lm ns lq nt lu nl nm nn no bi translated">hb4 = 5.825</li><li id="600f" class="ng nh it lb b lc np lf nq li nr lm ns lq nt lu nl nm nn no bi translated">hb5 = 5.828</li></ul><p id="af52" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们看到，RMSE值都略小于使用OLS回归时计算出的5.83。hb2，或者ε值为1.8的模型，表现出最好的性能——尽管微不足道。</p><h2 id="1372" class="mx lw it bd lx nw nx dn mb ny nz dp mf li oa ob mh lm oc od mj lq oe of ml og bi translated">里脊回归</h2><p id="9c4c" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">以与前面相同的方式，使用<a class="ae ky" href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html" rel="noopener ugc nofollow" target="_blank">岭</a>回归进行预测，随后计算RMSE:</p><pre class="kj kk kl km gt ms mt mu mv aw mw bi"><span id="debb" class="mx lw it mt b gy my mz l na nb">&gt;&gt;&gt; rg = Ridge(fit_intercept=True, alpha=0.0, random_state=0, normalize=True)<br/>&gt;&gt;&gt; ridgemodel = rg.fit(X_train,y_train)<br/>&gt;&gt;&gt; ridgepredictions = rg.predict(X_val)</span><span id="9e01" class="mx lw it mt b gy nu mz l na nb">[36.32534071 31.09716546 28.67493585 34.29867119 35.03070074<br/>...<br/>31.79765405 36.18771132 31.86883756 36.98120033 35.68182273]</span></pre><p id="36a7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">均方差实际上与使用OLS回归计算的结果相同:</p><pre class="kj kk kl km gt ms mt mu mv aw mw bi"><span id="ef3d" class="mx lw it mt b gy my mz l na nb">&gt;&gt;&gt; mean_squared_error(y_val, ridgepredictions)<br/>&gt;&gt;&gt; math.sqrt(mean_squared_error(y_val, ridgepredictions))</span><span id="9afc" class="mx lw it mt b gy nu mz l na nb">5.8305</span></pre><h1 id="880e" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">针对测试集的性能</h1><p id="e6e7" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">假设ε= 1.8的Huber回归显示了对验证集的最佳性能(尽管是边缘的)，让我们看看它对测试集的性能如何。</p><p id="b86f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在这种情况下，Pima Indians糖尿病数据集的一部分与其余数据在物理上是分开的，以便检查模型在看不见的数据中的表现。</p><p id="7b56" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">以下是预测:</p><pre class="kj kk kl km gt ms mt mu mv aw mw bi"><span id="8d89" class="mx lw it mt b gy my mz l na nb">&gt;&gt;&gt; btest = t_bmi<br/>&gt;&gt;&gt; btest=btest.values<br/>&gt;&gt;&gt; bpred = hb2.predict(atest)<br/>&gt;&gt;&gt; bpred</span><span id="4915" class="mx lw it mt b gy nu mz l na nb">array([33.14957023, 30.36001456, 32.93500157, 28.91518701, <br/>...<br/>34.72666166,36.29947658, 39.13505914, 33.77051646])</span></pre><p id="0c9b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">计算测试集的RMSE和平均值:</p><pre class="kj kk kl km gt ms mt mu mv aw mw bi"><span id="2263" class="mx lw it mt b gy my mz l na nb">&gt;&gt;&gt; math.sqrt(mean_squared_error(btest, bpred))<br/>5.744712507435319</span><span id="d2b7" class="mx lw it mt b gy nu mz l na nb">&gt;&gt;&gt; np.mean(btest)<br/>32.82418300653595</span></pre><p id="5fb6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">由于RMSE值仅占平均值的17%多一点，该模型在预测整个测试集的身体质量指数值方面表现相当不错。</p><p id="906f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">正如我们所看到的，所有模型的RMSE值或多或少是相似的——休伯回归提供了略低的RMSE值。然而，根据异常值的大小，在某些情况下，休伯回归和岭回归可以明显优于OLS回归。</p><h1 id="ab8c" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">结论</h1><p id="6c1c" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">在本例中，您看到了:</p><ul class=""><li id="fcda" class="ng nh it lb b lc ld lf lg li ni lm nj lq nk lu nl nm nn no bi translated">如何直观地检测数据样本中的异常值</li><li id="bdb6" class="ng nh it lb b lc np lf nq li nr lm ns lq nt lu nl nm nn no bi translated">Huber回归和岭回归的区别</li><li id="b9ea" class="ng nh it lb b lc np lf nq li nr lm ns lq nt lu nl nm nn no bi translated">如何修改Huber回归中的异常值敏感度</li><li id="c3cf" class="ng nh it lb b lc np lf nq li nr lm ns lq nt lu nl nm nn no bi translated">使用RMSE确定模型精度</li></ul><p id="41f1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">非常感谢您的宝贵时间，非常感谢您的任何问题或反馈。你可以在michael-grogan.com的<a class="ae ky" href="https://www.michael-grogan.com/" rel="noopener ugc nofollow" target="_blank">找到更多我的内容。</a></p><p id="fcf1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="nv">免责声明:本文是在“原样”的基础上编写的，没有担保。它旨在提供数据科学概念的概述，不应被解释为专业建议。本文中的发现和解释是作者的发现和解释，不被本文中提到的任何第三方认可或隶属于任何第三方。作者与本文提及的任何第三方无任何关系。</em></p></div></div>    
</body>
</html>