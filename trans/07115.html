<html>
<head>
<title>Where do confidence intervals in linear regression come from — the case of least squares formulation</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">线性回归中的置信区间从何而来——最小二乘公式的例子</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/where-do-confidence-interval-in-linear-regression-come-from-the-case-of-least-square-formulation-78f3d3ac7117?source=collection_archive---------7-----------------------#2021-06-28">https://towardsdatascience.com/where-do-confidence-interval-in-linear-regression-come-from-the-case-of-least-square-formulation-78f3d3ac7117?source=collection_archive---------7-----------------------#2021-06-28</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="ee8d" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsdatascience.com/tagged/hands-on-tutorials" rel="noopener" target="_blank">实践教程</a></h2><div class=""/><div class=""><h2 id="0a23" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">了解线性回归中学习参数权重的标准误差，以及其他指标，如t指标和P&gt;|t| metic。</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/202aa4121f3afaef1b5acfffbbd9039c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6ieS8F4NHZ73p7R_VD7gLQ.jpeg"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">图片来自<a class="ae lh" href="https://pixabay.com/photos/stare-power-line-birds-persevere-1680951/" rel="noopener ugc nofollow" target="_blank"> Pixabay </a></p></figure><p id="2fca" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我一直认为我很了解线性回归，直到最近我的大脑对一个同事的问题一片空白——当使用Python的普通最小二乘类OLS时，参数置信区间来自哪里？我决定推导出线性回归的所有相关公式，并将这些推导汇编成一组文章。这是第一个。</p><h1 id="5e41" class="me mf it bd mg mh mi mj mk ml mm mn mo ki mp kj mq kl mr km ms ko mt kp mu mv bi translated">线性回归模型</h1><p id="eeeb" class="pw-post-body-paragraph li lj it lk b ll mw kd ln lo mx kg lq lr my lt lu lv mz lx ly lz na mb mc md im bi translated">线性回归模型<em class="nb"> f(x)=xᵀ w </em>是大多数人研究的第一个机器学习模型。在这个模型中:</p><ul class=""><li id="6b95" class="nc nd it lk b ll lm lo lp lr ne lv nf lz ng md nh ni nj nk bi translated"><em class="nb"> x </em>是特征，<em class="nb"> w </em>是模型参数。<em class="nb"> x </em>和<em class="nb"> w </em>都是长度为<em class="nb"> p </em>的向量，其中<em class="nb"> p </em>是特征的数量。<em class="nb"/>是<a class="ae lh" href="https://en.wikipedia.org/wiki/Dot_product" rel="noopener ugc nofollow" target="_blank">矢量点积</a>。</li><li id="7244" class="nc nd it lk b ll nl lo nm lr nn lv no lz np md nh ni nj nk bi translated">我们需要从训练数据集<em class="nb"> (X，Y) </em>中学习<em class="nb"> w </em>的具体值，其中<em class="nb"> X </em>是形状为<em class="nb"> n×p </em>的特征矩阵，其中<em class="nb"> n </em>是训练数据点的数量。<em class="nb"> Y </em>是形状为<em class="nb"> n×1的观测矩阵。</em></li></ul><p id="40f3" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">以下代码使用普通最小二乘类(OLS)来学习模型参数<em class="nb"> w </em>的值:</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="nq nr l"/></div></figure><p id="d563" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">第4 ~ 6行建立了一个线性回归模型，有三个特征<em class="nb">声望、教育</em>和一个常数特征(或偏差)，目标变量<em class="nb">收入</em>。</p><p id="8544" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">第10行打印了下表。在表格中:</p><ul class=""><li id="427e" class="nc nd it lk b ll lm lo lp lr ne lv nf lz ng md nh ni nj nk bi translated"><em class="nb"> coef </em>栏(用黄色背景突出显示)显示了我们的特征的学习参数值— <em class="nb">声望</em>、<em class="nb">教育</em>和添加的<em class="nb">常量</em>特征。这是我们平时关注的。</li><li id="9039" class="nc nd it lk b ll nl lo nm lr nn lv no lz np md nh ni nj nk bi translated">有趣的是，该表还显示了一些概率度量(用粉红色背景突出显示):对于学习的系数，<em class="nb"> std err </em>，<em class="nb"> t </em>，<em class="nb"> P &gt; |t| </em>和置信区间【0.025，0.975】。它们似乎是概率度量，它们意味着什么？</li></ul><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ns"><img src="../Images/928f3f77767b54f38bdd71f7c915965b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6gaMkEHZqX5F5O2QTJ8tQA.png"/></div></div></figure><p id="9a78" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">本文解释了这些概率度量的含义，以及它们是如何在线性回归模型的最小二乘公式中计算出来的。</p><p id="ef12" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">但是在我们回答这些问题之前，我们首先需要了解如何从训练数据<em class="nb"> (X，Y) </em>中学习我们的模型参数<em class="nb"> w </em>的值。我们使用的方法叫做<strong class="lk jd">最小二乘法</strong>。</p><h1 id="974f" class="me mf it bd mg mh mi mj mk ml mm mn mo ki mp kj mq kl mr km ms ko mt kp mu mv bi translated">最小二乘公式</h1><p id="c463" class="pw-post-body-paragraph li lj it lk b ll mw kd ln lo mx kg lq lr my lt lu lv mz lx ly lz na mb mc md im bi translated">在最小二乘法公式中，为了找到模型参数<em class="nb"> w、</em>的具体值，我们最小化<em class="nb"> </em>以下损失函数</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/d769f4ba611dc1cf4ae638df6330dd17.png" data-original-src="https://miro.medium.com/v2/resize:fit:1084/format:webp/1*5qmY1_lGKDFWqJ3Y5Zobiw.png"/></div></figure><p id="6c6e" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">损失函数<em class="nb"> L(w) </em>是观测值<em class="nb"> Y </em>和模型预测值<em class="nb"> X w </em>之间距离的平方。工作就是最小化这种损失——找到w的值，使得<em class="nb"> L(w)的</em>值最小，因此得名最小二乘法<em class="nb">。</em></p><h2 id="0b77" class="nu mf it bd mg nv nw dn mk nx ny dp mo lr nz oa mq lv ob oc ms lz od oe mu iz bi translated">解正规方程求w</h2><p id="eac4" class="pw-post-body-paragraph li lj it lk b ll mw kd ln lo mx kg lq lr my lt lu lv mz lx ly lz na mb mc md im bi translated">由于<em class="nb"> L(w) </em>是一个未知<em class="nb"> w </em>的二次函数，找到最小化<em class="nb"> L(w) </em>的<em class="nb"> w </em>的值的显而易见的方法是:</p><ol class=""><li id="2cb4" class="nc nd it lk b ll lm lo lp lr ne lv nf lz ng md of ni nj nk bi translated">求<em class="nb"> L(w) </em>相对于<em class="nb"> w的导数</em>导数是另一个未知的函数<em class="nb"> w </em>。</li><li id="8e8b" class="nc nd it lk b ll nl lo nm lr nn lv no lz np md of ni nj nk bi translated">将导数设置为0。这就给了你著名的<strong class="lk jd">法线方程</strong>。</li><li id="55f0" class="nc nd it lk b ll nl lo nm lr nn lv no lz np md of ni nj nk bi translated">求解法线方程以找到<em class="nb"> w </em>的具体值。</li></ol><p id="bc79" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">形式上:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi og"><img src="../Images/3230a880e5ba83622b88aa92aecb8cda.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Wmev2gGAAFCsmMU9vUFllw.png"/></div></div></figure><p id="c5a3" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">第(1)行左侧是损失函数<em class="nb"> L(w) </em>相对于<em class="nb"> w </em>的导数的符号。“<em class="nb"> ∂ </em>这个符号反映了一个事实，即<em class="nb"> w </em>是一个变量的向量，而不是单个变量，所以我们在偏导数的世界里。“偏导数”是指每个导数都与向量<em class="nb"> w </em>中的单个变量相关，从而产生<em class="nb"> p </em>偏导数，其中<em class="nb"> p </em>是<em class="nb"> w </em>向量的长度。我们将在本文后面更清楚地看到这一点。</p><p id="e6f0" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">线(2)插入<em class="nb"> L(w) </em>的定义。</p><p id="769d" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">第(3)行应用向量微分规则来计算导数。将这个导数设为0，我们就得到了正规方程。</p><p id="ffab" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">为了求解法线方程，我们做:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi oh"><img src="../Images/092be547a66282907a22aa720cacddd1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kj4ie1EaF34X5Hi8nL1Yag.png"/></div></div></figure><p id="1bfa" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">让我们用名字<em class="nb"> wₒₚₜ </em>来指代使法线方程成立的<em class="nb"> w </em>的值，换句话说:当<em class="nb"> w </em>取值<em class="nb"> wₒₚₜ=(XᵀX)⁻ XᵀY </em>时，法线方程的左侧计算结果为0。所以我写<em class="nb"> w </em>时，指的是未知模型参数的向量；而当我写<em class="nb"> wₒₚₜ </em>的时候，我指的是<em class="nb"> w </em>可以取的值，让正规方程成立:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi oi"><img src="../Images/d7edfb074b740b83e0f1f9d414dea274.png" data-original-src="https://miro.medium.com/v2/resize:fit:1072/format:webp/1*7JKEuBcHk-QS_AxcpYvspg.png"/></div></figure><p id="1de4" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">注意在<em class="nb"> wₒₚₜ、</em>的表达式中，你不能用规则<em class="nb">(xᵀx)</em>⁻=<em class="nb">x</em>⁻(<em class="nb">xᵀ)</em>⁻来简化<em class="nb"> (XᵀX)⁻ </em>，因为<em class="nb"> X </em>通常不是一个方阵。只有方阵才有逆。例如，如果有<em class="nb"> n=10个</em>数据点，并且<em class="nb"> p=2个</em>特征，<em class="nb"> X </em>的形状为10×2；它不是一个正方形矩阵。</p><h2 id="45b6" class="nu mf it bd mg nv nw dn mk nx ny dp mo lr nz oa mq lv ob oc ms lz od oe mu iz bi translated">向量微分</h2><p id="5a81" class="pw-post-body-paragraph li lj it lk b ll mw kd ln lo mx kg lq lr my lt lu lv mz lx ly lz na mb mc md im bi translated">我认为以上所有的步骤都很简单，除了我们应用向量微分法则来推导正规方程的那一步。我可以背出<em class="nb"> ∂(wᵀw) = 2w </em>这样的向量微分法则，但结果似乎还是很神秘。</p><p id="7de3" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">它看起来很神秘，因为使用了线性代数中引入的向量微分规则。这些规则提供了一定程度的抽象。一方面，它们允许你以简洁的方式写下冗长的表达；但另一方面，它们阻止你立即看到计算是如何进行的。</p><p id="a1f0" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我们应该理解所有这些向量符号和线性代数规则只是为了记账。如果展开所有向量并应用非向量版本的微分规则，向量微分的结果应该是相同的。如果你想知道这是如何解正规方程的，请查阅本文的附录。</p><p id="3cbc" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">求解正规方程是对线性回归模型进行参数学习的一种方法。存在其他方法。<a class="ae lh" rel="noopener" target="_blank" href="/can-we-use-stochastic-gradient-descent-sgd-on-a-linear-regression-model-e50327b07d33">这里</a>是关于如何通过另一种方法——随机梯度下降法找到线性回归参数值的好读物。</p><h1 id="160f" class="me mf it bd mg mh mi mj mk ml mm mn mo ki mp kj mq kl mr km ms ko mt kp mu mv bi translated">最小二乘公式中的不确定性</h1><p id="f9ec" class="pw-post-body-paragraph li lj it lk b ll mw kd ln lo mx kg lq lr my lt lu lv mz lx ly lz na mb mc md im bi translated">直到现在，我们的线性模型<em class="nb"> f(x)=x w </em>都是确定性模型，而不是概率性模型。要看这个:</p><ul class=""><li id="60ca" class="nc nd it lk b ll lm lo lp lr ne lv nf lz ng md nh ni nj nk bi translated">学习的参数值<em class="nb"> wₒₚₜ=(XᵀX)⁻ XᵀY </em>是确定性的，因为其成分<em class="nb"> X </em>和<em class="nb"> Y </em>是给定的训练数据集。<em class="nb"> wₒₚₜ </em>是一个实标量的具体向量。</li><li id="20c3" class="nc nd it lk b ll nl lo nm lr nn lv no lz np md nh ni nj nk bi translated">这个模型<em class="nb">ŷ= xwₒₚₜ</em>的预测是确定性的，因为<em class="nb"> X </em>和<em class="nb"> wₒₚₜ </em>都是确定性的<em class="nb">。</em></li></ul><p id="f444" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">但是这里再次显示的模型拟合总结包括概率指标，如标准误差、置信区间。这是怎么回事？</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ns"><img src="../Images/928f3f77767b54f38bdd71f7c915965b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6gaMkEHZqX5F5O2QTJ8tQA.png"/></div></div></figure><h2 id="3e46" class="nu mf it bd mg nv nw dn mk nx ny dp mo lr nz oa mq lv ob oc ms lz od oe mu iz bi translated">概率测度与随机变量一起存在</h2><p id="a746" class="pw-post-body-paragraph li lj it lk b ll mw kd ln lo mx kg lq lr my lt lu lv mz lx ly lz na mb mc md im bi translated">这些概率度量只与随机变量的概念一起存在。因此，为了讨论这些指标，我们需要在我们的模型中引入随机变量。具体地，由于标准推导，置信区间是用于学习的模型参数的。我们需要将学习到的模型参数值从一个标量，如0.6237的<em class="nb">声望</em>，变成一个随机变量。</p><h2 id="d2c3" class="nu mf it bd mg nv nw dn mk nx ny dp mo lr nz oa mq lv ob oc ms lz od oe mu iz bi translated">在线性模型中引入随机变量</h2><p id="3679" class="pw-post-body-paragraph li lj it lk b ll mw kd ln lo mx kg lq lr my lt lu lv mz lx ly lz na mb mc md im bi translated">我们按照以下步骤引入随机变量来表示可能的模型参数值:</p><p id="c4b6" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><strong class="lk jd">第一步</strong></p><p id="ea5f" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">引入一个随机变量向量<em class="nb"> y=X wₜᵣᵤₑ+ ε </em>，其中<em class="nb"> wₜᵣᵤₑ </em>代表我们永远不会知道的单一真实模型参数值。不要担心我们不知道<em class="nb"> wₜᵣᵤₑ </em>的值，稍后，我们会估计它。<em class="nb"> ε </em>是均值为零的多元高斯噪声，在每个数据点上是独立的，方差<em class="nb"> η。</em> <em class="nb"> y </em>为形状为<em class="nb"> n×1 </em>的随机变量向量，其中<em class="nb"> n </em>为训练数据点的个数:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi oj"><img src="../Images/493c6bfc347bc0d16765f4f9ca1c1b18.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*G8Wd8mk-BzGcakpmjyTWfA.png"/></div></div></figure><p id="7278" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">进一步理解<em class="nb"> y </em>的定义:</p><ul class=""><li id="7d01" class="nc nd it lk b ll lm lo lp lr ne lv nf lz ng md nh ni nj nk bi translated">随机变量向量<em class="nb"> y </em>中的每一项<em class="nb"> yᵢ </em>代表对应的训练数据点<em class="nb"> Yᵢ </em>可能取的值。因此，我们训练数据集中的观察值<em class="nb"> Y </em>向量可以被建模为来自随机变量向量<em class="nb"> y </em>的样本。</li><li id="eb30" class="nc nd it lk b ll nl lo nm lr nn lv no lz np md nh ni nj nk bi translated">在这个随机变量向量中，每一项<em class="nb"> yᵢ </em>被定义为两部分之和:<em class="nb"> yᵢ=Xᵢ wₜᵣᵤₑ+ εᵢ.</em><em class="nb">xᵢwₜᵣᵤₑ</em>部分是确定性的，因为<em class="nb"> Xᵢ </em>和<em class="nb"> wₜᵣᵤₑ </em>都是具体值。噪声部分<em class="nb"> εᵢ </em>是随机的——是来自高斯分布<em class="nb"> N(0，η ) </em>的一维随机变量。如此定义，<em class="nb"> yᵢ </em>也是来自一维高斯分布的随机变量。</li><li id="29c8" class="nc nd it lk b ll nl lo nm lr nn lv no lz np md nh ni nj nk bi translated">来自向量<em class="nb"> y </em>的所有单个随机变量遵循相同的定义模式。我们有<em class="nb"> n </em>个数据点，所以有<em class="nb"> n </em>个独立噪声，它们都有零均值和方差<em class="nb"> η </em>。</li><li id="5f4e" class="nc nd it lk b ll nl lo nm lr nn lv no lz np md nh ni nj nk bi translated">我们可以用一个多元高斯随机变量来更简洁地表示这些<em class="nb"> n </em>独立噪声(但数学上等价):<em class="nb"> ε~N(0，Iₙ η)。</em>所以<em class="nb"> ε </em>是长度为<em class="nb"> n </em>的随机变量向量，<em class="nb"> Iₙ </em>是<em class="nb"> n×n </em>的单位矩阵。<em class="nb"> Iₙ η </em>中的<em class="nb"> η </em>部分表示<em class="nb"> ε </em>中各单个随机变量的方差为<em class="nb">η</em>；<em class="nb"> Iₙ </em>部分表示所有这些单个随机变量都是独立的。</li><li id="9791" class="nc nd it lk b ll nl lo nm lr nn lv no lz np md nh ni nj nk bi translated">另一种写法<em class="nb"> y=X wₜᵣᵤₑ+ ε </em>是<em class="nb"> y </em> ~ <em class="nb"> N(X wₜᵣᵤₑ，Iₙ η)。</em></li></ul><p id="98d7" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><strong class="lk jd">第二步</strong></p><p id="66f8" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我们已经解出了得到<em class="nb"> wₒₚₜ=(XᵀX)⁻ XᵀY. </em>的正规方程，这是理解这个公式的另一种方式:</p><ul class=""><li id="fdac" class="nc nd it lk b ll lm lo lp lr ne lv nf lz ng md nh ni nj nk bi translated">揭示了具体值<em class="nb"> Y </em>通过变换矩阵<em class="nb"> (XᵀX)⁻ Xᵀ，</em>)线性变换为<em class="nb"> wₒₚₜ.</em>注意<em class="nb"> wₒₚₜ </em>与我们在步骤1中介绍的<em class="nb"> wₜᵣᵤₑ </em>不同。<em class="nb"> wₜᵣᵤₑ </em>是我们永远不会知道的真实系数的符号。<em class="nb"> wₒₚₜ </em>当使用最小二乘法作为估计方法时，我们对<em class="nb"> wₜᵣᵤₑ </em>的估计(或学习参数值)。</li><li id="7f27" class="nc nd it lk b ll nl lo nm lr nn lv no lz np md nh ni nj nk bi translated">既然现在我们将<em class="nb"> Y </em>建模为来自随机变量向量<em class="nb"> y </em>的样本，该向量具有分布<em class="nb"> N(X wₜᵣᵤₑ，Iₙ η ) </em>，我们可以将<em class="nb">wₒₚₜ</em>=<em class="nb">(xᵀx)⁻xᵀy</em>建模为来自新的随机变量向量<em class="nb"> ŵ </em>，定义为<em class="nb"> ŵ=(XᵀX)⁻ Xᵀ y. </em>没错，随机变量向量<em class="nb">ŵ</em></li><li id="1242" class="nc nd it lk b ll nl lo nm lr nn lv no lz np md nh ni nj nk bi translated">从结构上看，<em class="nb"> ŵ </em>是由<em class="nb"> (XᵀX)⁻ Xᵀ </em>对<em class="nb"> y </em>的线性变换，正如<em class="nb"> wₒₚₜ </em>是由<em class="nb"> Y. </em>而来，甚至<em class="nb"> ŵ </em>也是由<em class="nb"> y、</em>的线性变换，请注意<em class="nb"> w </em>是<em class="nb"> p×1 </em>的形状，而<em class="nb"/></li></ul><p id="98f4" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">作为一个随机变量向量，<em class="nb"> ŵ=(XᵀX)⁻ Xᵀ y </em>代表我们的线性回归模型的参数的所有可能值及其概率密度。我们使用<em class="nb"> ŵ </em>的概率密度函数，我们将在后面推导，来谈论我们的线性回归模型中的参数的那些概率概念，如标准差、置信区间。</p><h2 id="262f" class="nu mf it bd mg nv nw dn mk nx ny dp mo lr nz oa mq lv ob oc ms lz od oe mu iz bi translated"><em class="ok"> ŵ，“学习参数值的</em>分布”是什么意思？</h2><p id="2180" class="pw-post-body-paragraph li lj it lk b ll mw kd ln lo mx kg lq lr my lt lu lv mz lx ly lz na mb mc md im bi translated">理解我们引入<em class="nb"> ŵ </em>来表示学习参数值的分布是很重要的。但是短语“学习参数值的分布”是什么意思呢？</p><p id="3fa3" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">理解这个短语的最好方法是提醒我们自己，我们假设有一个<strong class="lk jd">单真参数值<em class="nb"> wₜᵣᵤₑ </em> </strong>。并且我们想要通过从线性模型<em class="nb">x</em>t14】wₜᵣᵤₑ的透镜来查看训练数据集<em class="nb"> (X，Y) </em>来弄清楚<em class="nb"> wₜᵣᵤₑ </em>是什么。这适用于以下情况:</p><ol class=""><li id="d83f" class="nc nd it lk b ll lm lo lp lr ne lv nf lz ng md of ni nj nk bi translated">观察值<em class="nb"> Y </em>确实是使用公式<em class="nb">x</em>t22】wₜᵣᵤₑ从<em class="nb"> X </em>生成的。</li><li id="01d7" class="nc nd it lk b ll nl lo nm lr nn lv no lz np md of ni nj nk bi translated">当我们对Y的测量确实精确时。</li></ol><p id="cb63" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">然后我们可以通过解正规方程来计算出<em class="nb"> wₜᵣᵤₑ </em>的值。</p><p id="02a4" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">作为建模者，我们可以挑战上述两个条件。</p><p id="f56e" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我们可以挑战第一个条件，说<em class="nb"> X </em>和<em class="nb"> Y </em>之间的线性关系不正确，反而应该是指数关系。这很好，它将我们引向一个模型选择问题——我们到底应该考虑线性模型吗？或者我们应该考虑指数模型，或者神经网络，或者决策树？这个题目我再写一篇。现在，让我们保持线性。</p><p id="8d02" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我们可以挑战第二个条件，说由于一些测量上的困难，测得的观测值<em class="nb"> Y </em>并不严格等于<em class="nb"> X </em> <em class="nb"> wₜᵣᵤₑ.</em>取而代之的是被具有零均值<em class="nb">的高斯噪声<em class="nb"> ε </em>破坏的<em class="nb"> X wₜᵣᵤₑ </em>。</em>一种等价的说法是<em class="nb"> Y </em>现在是随机变量<em class="nb"> y </em>的一个样本，它被定义为<em class="nb"> y~ N(X wₜᵣᵤₑ，Iₙ η)。</em>这就是我们在这篇文章中所做的。</p><p id="7c68" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">根据其定义<em class="nb"> Y </em>是来自随机变量向量<em class="nb"> y </em>的样本，我们必须将训练数据集中的具体观察值<em class="nb"> Y </em>视为随机过程的结果——我们必须承认，下次采样时，我们将获得一组不同的观察值<em class="nb"> Y </em>(我说不同的组是因为<em class="nb"> Y </em>是具有<em class="nb"> n </em>标量的具体向量)。</p><p id="6e22" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">由于学习到的参数值<em class="nb"> (XᵀX)⁻ XᵀY </em>是从<em class="nb"> Y </em>计算出来的，而<em class="nb"> Y </em>因为来自于<em class="nb"> y </em>所以现在具有随机性，所以学习到的参数值变成了一个随机变量。我们用<em class="nb"> ŵ </em>来表示这个随机变量向量，定义为<em class="nb"> ŵ=(XᵀX)⁻ Xᵀy. </em>根据定义，一个随机变量代表一个值的分布(具有不同的概率密度)，因此短语“<em class="nb"> ŵ </em>代表学习参数值的分布”。</p><h2 id="e3a0" class="nu mf it bd mg nv nw dn mk nx ny dp mo lr nz oa mq lv ob oc ms lz od oe mu iz bi translated">推导出<em class="ok"> ŵ </em>的概率密度函数</h2><p id="e013" class="pw-post-body-paragraph li lj it lk b ll mw kd ln lo mx kg lq lr my lt lu lv mz lx ly lz na mb mc md im bi translated">要谈概率性如<em class="nb"> ŵ </em>的标准差，我们需要导出它的概率密度函数。推导很简单:</p><ol class=""><li id="ae7c" class="nc nd it lk b ll lm lo lp lr ne lv nf lz ng md of ni nj nk bi translated">因为随机变量向量<em class="nb"> y </em>定义为<em class="nb"> y=X wₜᵣᵤₑ+ ε。</em>和<em class="nb"> ε </em>是多元高斯随机变量<em class="nb"> ε~N(0，Iₙ η ) </em>，<em class="nb"> y </em>也是多元高斯随机变量:<em class="nb"> y ~ N(X wₜᵣᵤₑ，Iₙ η)。我们只需要将平均值从0移动到X wₜᵣᵤₑ.</em></li><li id="bece" class="nc nd it lk b ll nl lo nm lr nn lv no lz np md of ni nj nk bi translated">由于根据多元高斯线性变换规则，<em class="nb"> ŵ </em>被定义为<em class="nb"> y </em> : <em class="nb"> ŵ=(XᵀX)⁻ Xᵀ y，</em>的线性变换，<em class="nb"> ŵ </em>也是多元高斯随机变量，规则告诉我们<em class="nb"> ŵ </em>的概率密度函数是什么。</li></ol><p id="54dd" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><strong class="lk jd">多元高斯线性变换规则</strong></p><p id="34d5" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">这个规则在机器学习里很多地方都会弹出来，比如<a class="ae lh" rel="noopener" target="_blank" href="/demystifying-tensorflow-time-series-local-linear-trend-9bec0802b24a">卡尔曼滤波</a>、<a class="ae lh" rel="noopener" target="_blank" href="/understanding-gaussian-process-the-socratic-way-ba02369d804">高斯过程</a>，所以请大家用心记住。形式上，它说:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi ol"><img src="../Images/43e382575a10214381742b6a420caf47.png" data-original-src="https://miro.medium.com/v2/resize:fit:638/format:webp/1*5jgcaDrp1mQViN7IDyKKvg.png"/></div></figure><p id="024b" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">用英文说，如果一个多元高斯随机变量<em class="nb"> a </em>有一个已知的概率密度函数，随机变量<em class="nb"> b </em>是由<em class="nb"> a </em>经变换矩阵<em class="nb"> A </em>的线性变换，那么<em class="nb"> b </em>就是与<em class="nb"> a </em>形状相同的多元高斯随机变量。并且<em class="nb"> b </em>的概率密度函数具有上述固定形式，即从<em class="nb"> a </em>的分布中提及均值、协方差矩阵，以及变换矩阵<em class="nb"> A </em>。</p><p id="ba7a" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">对于<em class="nb"> ŵ </em>，我们有<em class="nb"> : </em></p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi om"><img src="../Images/def58f5ed16c8dba98cd7f87a91ac881.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*z9GdDwvPmc-aLxWIacPSOA.png"/></div></div></figure><p id="ca08" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">对于ŵ.来说，这是一个非常长的概率密度函数幸运的是，我们可以大大简化它:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi on"><img src="../Images/5b6eed5263f898231a06987b9f93a693.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*h4S6qYpOCTyHrPEogIphKg.png"/></div></div></figure><p id="70ee" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">第(2)至(3)行简化了平均分量。第(4)～(7)行简化了协方差矩阵分量。需要注意的一点是从第(6)行到第(7)行的简化是有效的，因为<em class="nb"> (XᵀX)⁻ </em>是一个对称矩阵，它自己转置它。</p><p id="1bc7" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">从第(7)行我们注意到，<em class="nb"> ŵ </em>的平均值是真正的参数值<em class="nb"> wₜᵣᵤₑ </em>，换句话说，<em class="nb"> ŵ </em>是<strong class="lk jd">wₜᵣᵤₑ.<em class="nb">的无偏估计量</em></strong></p><h2 id="f2d6" class="nu mf it bd mg nv nw dn mk nx ny dp mo lr nz oa mq lv ob oc ms lz od oe mu iz bi translated">估计<em class="ok"> ŵ </em>的概率密度函数中的未知量</h2><p id="439a" class="pw-post-body-paragraph li lj it lk b ll mw kd ln lo mx kg lq lr my lt lu lv mz lx ly lz na mb mc md im bi translated">简化的概率密度函数，<em class="nb"> ŵ~N(wₜᵣᵤₑ，</em><em class="nb">(xᵀx)</em>⁻<em class="nb">η)</em>有两个未知数，<em class="nb"> wₜᵣᵤₑ </em>和<em class="nb"> η </em>。在使用这个概率密度函数来报告概率度量之前，我们需要给它们具体的值。</p><ul class=""><li id="0005" class="nc nd it lk b ll lm lo lp lr ne lv nf lz ng md nh ni nj nk bi translated">对于<em class="nb"> wₜᵣᵤₑ，</em>我们用<em class="nb"> wₒₚₜ=(XᵀX)⁻ XᵀY. </em></li><li id="ca5a" class="nc nd it lk b ll nl lo nm lr nn lv no lz np md nh ni nj nk bi translated">对于<em class="nb"> η，</em>我们使用以下公式，其中<em class="nb">ŷ= xwₒₚₜ</em>是来自我们的线性回归模型的样本内预测:</li></ul><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi oo"><img src="../Images/226e2a6e1a43c9ebd3a19667c97c7267.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jdNpCJ2D_nrUPBDhpKLGZA.png"/></div></div></figure><p id="d4d8" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><strong class="lk jd">估计为<em class="nb">wₜᵣᵤₑ</em>t35】</strong></p><p id="f1fc" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我们先来理解一下为什么可以用<em class="nb"> wₒₚₜ=(XᵀX)⁻ XᵀY </em>来估算<em class="nb"> wₜᵣᵤₑ.</em>从导出的<em class="nb"> ŵ~N(wₜᵣᵤₑ、</em><em class="nb">【xᵀx】</em>⁻<em class="nb">η)</em>的概率密度函数可知<em class="nb"> ŵ </em>是<em class="nb"> wₜᵣᵤₑ.的无偏估计量</em>因此，如果我们从这个分布中抽取无限数量的样本，这些样本的平均值(或期望值)将等于<em class="nb"> wₜᵣᵤₑ.</em>换句话说，我们可以用样本的期望从<em class="nb"> ŵ </em>到<strong class="lk jd">估计</strong>wₜᵣᵤₑ.<em class="nb"/></p><p id="ada3" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">实际上，我们不能从这个分布中抽取无限的样本。其实我们只有一个样本，那就是<em class="nb"> wₒₚₜ=(XᵀX)⁻ XᵀY </em>，它的平均值就是它自己。</p><p id="1ac6" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">首先，你可能会问为什么<em class="nb"> wₒₚₜ </em>是<em class="nb"> ŵ </em>分布的样本？这是</p><p id="2b35" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">因为:</p><ul class=""><li id="623b" class="nc nd it lk b ll lm lo lp lr ne lv nf lz ng md nh ni nj nk bi translated"><em class="nb"> ŵ </em>定义为线性变换，用变换矩阵<em class="nb"> (XᵀX)⁻ Xᵀ </em>，<em class="nb">，</em>从随机变量<em class="nb"> y. </em></li><li id="257f" class="nc nd it lk b ll nl lo nm lr nn lv no lz np md nh ni nj nk bi translated"><em class="nb"> Y </em>是来自<em class="nb"> y. </em>的样本</li><li id="e248" class="nc nd it lk b ll nl lo nm lr nn lv no lz np md nh ni nj nk bi translated"><em class="nb"> wₒₚₜ是通过对样本应用相同的变换而定义的</em></li></ul><p id="348c" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">那么你可能会问，我们应该用<em class="nb"> ŵ </em>分布的无限个样本的平均值来估计<em class="nb"> wₜᵣᵤₑ </em>，但是我们只有一个样本。这样可以吗？嗯，不太理想，但我们只有这些了。</p><p id="26c4" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><strong class="lk jd">估计为<em class="nb">η</em>T87】</strong></p><p id="7b86" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">现在让我们来理解对<em class="nb"> η </em>的估计:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi oo"><img src="../Images/226e2a6e1a43c9ebd3a19667c97c7267.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jdNpCJ2D_nrUPBDhpKLGZA.png"/></div></div></figure><p id="aca6" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><em class="nb"> η </em>是观测噪声随机变量的方差<em class="nb"> ε~N(0，Iₙ η)。</em>我们不直接观察噪音。相反，我们只从随机变量<em class="nb"> y=X wₜᵣᵤₑ+ ε中观察到一个样本<em class="nb"> Y </em>。</em>根据定义，<em class="nb"> y </em>的方差等于噪声<em class="nb"> ε </em>的方差，因为<em class="nb"> y </em>中唯一的随机分量来自噪声。在<em class="nb"> y的</em>定义中的<em class="nb"> X wₜᵣᵤₑ </em>分量只是移动了<em class="nb"> y </em>的平均值，而不是它的方差<em class="nb">。</em></p><p id="8df3" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><em class="nb"> y </em>是包含<em class="nb"> n </em>个随机变量的随机变量向量:</p><ul class=""><li id="33d1" class="nc nd it lk b ll lm lo lp lr ne lv nf lz ng md nh ni nj nk bi translated">这些随机变量的平均值，即<em class="nb"> X wₜᵣᵤₑ，</em>我们用<em class="nb">ŷ=x</em>t30】wₒₚₜ.来估计</li><li id="203a" class="nc nd it lk b ll nl lo nm lr nn lv no lz np md nh ni nj nk bi translated">我们知道<em class="nb"> y </em>的一个样本，那就是<em class="nb"> Y </em>。这个观察y不等于<em class="nb">ŷ.</em>换句话说，拟合的线不会通过所有的观测值。我们使用引入的噪声分量来解释这些偏差——我们的模型接受实际观测值<em class="nb"> Y </em>不等于其均值预测值<em class="nb">x</em>t42】wₒₚₜ因为在模型中，观测值<em class="nb"> Y </em>只是来自高斯分布<em class="nb"> N(X wₜᵣᵤₑ，Iₙ η ) </em>的样本，在我们插入均值估计值后，它变成了<em class="nb">n(ŷ，Iₙ η ) </em>。样本不需要等于该分布的平均值<em class="nb">。</em>样本y与均值相差多少由噪声方差<em class="nb"> Iₙ η决定。</em></li><li id="354a" class="nc nd it lk b ll nl lo nm lr nn lv no lz np md nh ni nj nk bi translated">所以<em class="nb">y-ŷ</em>可以被看作是噪声的观测值。<em class="nb">y-ŷ</em>是一个长度为<em class="nb"> n </em>的向量，代表噪声部分，每次观测一个。根据定义我们知道，所有这些噪声随机变量的均值为零，它们彼此独立，并且具有相同的方差<em class="nb"> η </em>。</li></ul><p id="3bc0" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">由于所有这些<em class="nb"> n </em>噪声共享相同的方差值<em class="nb"> η </em>，我们可以这样想:我们有一个单一的随机变量，它的均值为零，我们也有这个随机变量<em class="nb">的<em class="nb"> n </em>个样本<em class="nb">y-ŷ</em>。</em>我们要估计这个随机变量的方差，也就是<em class="nb"> η。</em>回忆一下从样本中计算随机变量方差的公式(来自<a class="ae lh" href="https://en.wikipedia.org/wiki/Bessel%27s_correction" rel="noopener ugc nofollow" target="_blank"> wiki </a>):</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi op"><img src="../Images/249e8b99859e46f2867826b25ee9243f.png" data-original-src="https://miro.medium.com/v2/resize:fit:588/format:webp/1*liy0OzvtCf1MVm40YltcMA.png"/></div></figure><p id="035c" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">在我们的例子中，我们有一些类似的东西，其中<em class="nb">y-ŷ</em>是样本，0是平均值。(<em class="nb">y-ŷ-0)ᵀ</em>(<em class="nb">y-ŷ-0)</em>为平方部分，简化为(<em class="nb">y-ŷ)ᵀ</em>(<em class="nb">y-ŷ)</em>)，所以估算<em class="nb"> η </em>的公式(再次)为:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi oo"><img src="../Images/226e2a6e1a43c9ebd3a19667c97c7267.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jdNpCJ2D_nrUPBDhpKLGZA.png"/></div></div></figure><p id="2035" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">需要规格化器<em class="nb"> 1/(n-p-1) </em>来将结果转化为噪声方差的无偏估计量——还记得在估计样本总体方差时为什么我们使用<em class="nb"> n-1 </em>而不是<em class="nb"> n </em>的讨论吗？这是同样的论点，部分<em class="nb"> -p </em>反映了我们使用<em class="nb"> p </em>特征来预测<em class="nb">ŷ.的事实</em>这导致了<em class="nb"> p </em>自由度的减少。</p><h2 id="3318" class="nu mf it bd mg nv nw dn mk nx ny dp mo lr nz oa mq lv ob oc ms lz od oe mu iz bi translated">为<em class="ok"> ŵ </em>完全指定概率密度函数</h2><p id="196d" class="pw-post-body-paragraph li lj it lk b ll mw kd ln lo mx kg lq lr my lt lu lv mz lx ly lz na mb mc md im bi translated">随着<em class="nb"> wₜᵣᵤₑ </em>和<em class="nb"> η </em>的估计，最终的、完全指定的<em class="nb"> ŵ </em>的概率密度函数为:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi oq"><img src="../Images/1725ea602c00c65ad83f853fa6d39170.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xF-JxfTgitqgxgc16RXq8Q.png"/></div></div></figure><p id="c2b3" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">在我们插入<em class="nb"> wₒₚₜ=(XᵀX)⁻ XᵀY </em>后，我们有一个学习模型参数<em class="nb"> ŵ </em>的可评估(没有未知数)概率密度函数。我们可以使用它来报告学习参数值的那些概率度量。</p><p id="f601" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">随着ŵ完全指定的概率密度函数的推导，我们终于可以理解这些概率度量是如何计算的。</p><h1 id="b5e3" class="me mf it bd mg mh mi mj mk ml mm mn mo ki mp kj mq kl mr km ms ko mt kp mu mv bi translated"><strong class="ak"><em class="ok"/></strong>的标准误差(或标准差)</h1><p id="dfb0" class="pw-post-body-paragraph li lj it lk b ll mw kd ln lo mx kg lq lr my lt lu lv mz lx ly lz na mb mc md im bi translated">汇总表中的<em class="nb">标准误差</em>栏报告了<em class="nb"> ŵ.中每个随机变量的标准偏差</em></p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi or"><img src="../Images/0b5e6296976b0cc92570f78751866df3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hXM6Xi1i4WHvqrtMWD8mqg.png"/></div></div></figure><p id="3a41" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">从<em class="nb"> ŵ </em>的分布，也就是多元高斯<em class="nb">、</em>我们得到随机变量向量<em class="nb">ŵ—</em><strong class="lk jd"><em class="nb"/></strong><em class="nb">【var(ŵ】</em>=<em class="nb">(xᵀx)</em>⁻<em class="nb">η的协方差矩阵。</em>是一个<em class="nb"> p×p </em>矩阵<em class="nb">。</em>每个随机变量的方差在这个矩阵的主对角线上。所以每个随机变量的标准差就是那些主对角线元素的平方根。</p><h1 id="0621" class="me mf it bd mg mh mi mj mk ml mm mn mo ki mp kj mq kl mr km ms ko mt kp mu mv bi translated"><strong class="ak">ŵ<em class="ok">的置信区间</em>t69】</strong></h1><p id="bc3f" class="pw-post-body-paragraph li lj it lk b ll mw kd ln lo mx kg lq lr my lt lu lv mz lx ly lz na mb mc md im bi translated">现在来说说<em class="nb"> ŵ </em>的置信区间，重点介绍如下:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi os"><img src="../Images/5b6af356a0883d0cd437662245455be3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZMmJ-d2qJZHDWqSRJ8c1pw.png"/></div></div></figure><p id="730b" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">列名[0.025，0.975]定义了系数值的95%范围—系数值的真实值有95%的可能性位于该区间内。</p><p id="2383" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">由于<em class="nb"> ŵ </em>是一个多变量高斯随机变量，每个单个随机变量的置信区间，如<em class="nb">声望</em>，在<em class="nb"> ŵ </em>只是偏离其均值的一些标准偏差(上下)。[0.025，0.975]范围对应于偏离平均值2个标准偏差，因为我们讨论的是高斯分布。</p><p id="3942" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">为了报告突出显示的置信区间，我们需要找到学习参数值的平均值和标准偏差:</p><ul class=""><li id="a14b" class="nc nd it lk b ll lm lo lp lr ne lv nf lz ng md nh ni nj nk bi translated">我们用<em class="nb"> wₒₚₜ </em>来估计<em class="nb"> ŵ </em>的均值，并在<em class="nb">系数</em>栏中报告。</li><li id="c2ee" class="nc nd it lk b ll nl lo nm lr nn lv no lz np md nh ni nj nk bi translated"><em class="nb"> ŵ </em>中每个单变量随机变量的标准偏差已经在<em class="nb">标准误差</em>列中。这是因为<a class="ae lh" rel="noopener" target="_blank" href="/understanding-gaussian-process-the-socratic-way-ba02369d804">多元高斯边缘化规则</a>。</li></ul><h1 id="21c8" class="me mf it bd mg mh mi mj mk ml mm mn mo ki mp kj mq kl mr km ms ko mt kp mu mv bi translated"><strong class="ak">特征重要性</strong></h1><p id="72de" class="pw-post-body-paragraph li lj it lk b ll mw kd ln lo mx kg lq lr my lt lu lv mz lx ly lz na mb mc md im bi translated">现在，让我们了解与功能重要性相关的列，如下所示:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ot"><img src="../Images/7513d0d85b61f2af28f3a10dc5e64183.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3GYdD04NSitVYLitvsweOg.png"/></div></div></figure><p id="d637" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">有两个相关的指标<em class="nb"> t </em>和<em class="nb"> P &gt; |t|。</em>P&gt;| t |度量对<em class="nb"> t </em>度量的假设检验。</p><p id="8a5b" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><em class="nb"> P &gt; |t| </em>度量报告模型参数值为0的概率，不管<em class="nb"> coef </em>列中报告的是什么。P &gt; |t|度量的高值表示该特征的系数可能为0，因此它对预测目标变量<em class="nb"> Y </em>没有贡献。</p><p id="a088" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">从上面的汇总表中可以看出:</p><ul class=""><li id="58da" class="nc nd it lk b ll lm lo lp lr ne lv nf lz ng md nh ni nj nk bi translated">对于特征<em class="nb">声望，</em>的<em class="nb"> P &gt; |t| </em>列报告有0%的概率<em class="nb">声望</em>的系数为0。换句话说，很有可能<em class="nb">声望</em>特征有助于预测目标变量。</li><li id="d7c1" class="nc nd it lk b ll nl lo nm lr nn lv no lz np md nh ni nj nk bi translated">对于特征<em class="nb">教育</em>，<em class="nb"/><em class="nb">P&gt;| t |</em>列报告有80%的可能性<em class="nb">教育</em>的系数为0。换句话说，<em class="nb">教育</em>功能不太可能有助于预测目标变量。</li></ul><p id="a498" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">为了计算这个0%或80%的概率，我们需要引入t统计量。</p><p id="8213" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><strong class="lk jd">t统计量</strong></p><p id="6ead" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">对于<em class="nb"> ŵ </em>中的<em class="nb">第j个</em>随机变量，我们表示为<em class="nb"> ŵⱼ，</em>我们从<em class="nb"> ŵ </em>的概率密度函数中得到这个随机变量的均值和标准差:</p><ul class=""><li id="55d3" class="nc nd it lk b ll lm lo lp lr ne lv nf lz ng md nh ni nj nk bi translated">平均值是平均值向量<em class="nb">中的第<em class="nb">个</em>条目。</em>让我们用<em class="nb"> μⱼ、</em>来表示平均值，这是一个特性的报告<em class="nb">系数</em>值——0.6237表示<em class="nb">声望</em>，0.0323表示<em class="nb">教育。</em></li><li id="3a00" class="nc nd it lk b ll nl lo nm lr nn lv no lz np md nh ni nj nk bi translated">标准差是协方差矩阵主对角线中第<em class="nb"> j个</em>项的平方根。让我们用<em class="nb"> ηⱼ </em>来表示标准偏差，这是一个特性的报告标准误差值——0.125表示<em class="nb">声望</em>，0.132表示<em class="nb">教育</em>。</li></ul><p id="00fe" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">那么第<em class="nb"> j个</em>特征<em class="nb"> tⱼ </em>的t统计量为</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi ou"><img src="../Images/bcad8fe68a2fff103f738dfb095e5e83.png" data-original-src="https://miro.medium.com/v2/resize:fit:946/format:webp/1*vXXLv7bGqlUYPTAj5jhtjw.png"/></div></figure><p id="4d89" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">该统计测量估计参数<em class="nb"> ŵⱼ </em>的平均值<em class="nb"> μⱼ </em>到平均值0 <em class="nb">之间的归一化距离。</em>该距离按分母中的标准偏差进行缩放。汇总表中的<em class="nb"> t </em>栏报告该<em class="nb"> tⱼ </em>数量。事实上，<em class="nb"> t统计量</em>是一个缩放的距离(也称为无标度度量),这一点很重要，它使得不同特征的<em class="nb"> t统计量</em>具有可比性。</p><p id="5994" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><strong class="lk jd"><em class="nb">P&gt;| t |</em>公制</strong></p><p id="4a63" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">由于<em class="nb"> tⱼ </em>统计被缩放，我们可以使用单位高斯概率密度函数来研究它。从现在开始，让我们使用<em class="nb"> t </em>而不是<em class="nb"> tⱼ </em>来与汇总表中的度量名称保持一致。</p><p id="c95f" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><em class="nb"> P &gt; |t| </em>指标报告单位高斯分布中两条阴影线的累积概率。这是[-∞，-t]和[t，+∞]中的紫色阴影区域，加在一起:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ov"><img src="../Images/13ed69a001c30a8b88390c5f4f343773.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*20JmAia5zivD8dvJjF0_NA.png"/></div></div></figure><p id="16a1" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我们要问的问题是:如果一个特征的系数确实为零，那么(从单位高斯分布来看)具有由[-∞，-t]和[t，+∞]定义的两个尾部的概率是多少。</p><p id="2369" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><em class="nb">t-统计量</em>越大(如果<em class="nb">t-统计量</em>为负，则越小)，从-t和t开始的两个尾部越小，因此计算出的<em class="nb">t-统计量</em>越不可能来自均值为零的单位高斯分布。</p><p id="40cf" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">所以，如果我们看到一个具有大的<em class="nb"> t统计量</em>的特征，我们就更确定那个特征的报告系数确实不为零。或者等价地，该特征更可能有助于预测目标变量，因此它更重要。</p><p id="1d51" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">你可能会问，为什么我们把<em class="nb"> P &gt; |t| </em>度量定义为两个尾部的累积概率，而不是只看<em class="nb"> t </em>位置的概率密度？这是因为我们想要报告一个概率，根据定义，概率是概率密度函数曲线下一系列值的面积。如果我们只看<em class="nb"> t </em>位置，概率为0，因为单位高斯是连续分布。</p><p id="2c50" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">你可能还会问，为什么我们测量两条尾巴的面积而不是一条？我们使用双尾，因为我们正在进行一个双尾假设检验，其零假设是<em class="nb"> t统计量</em>来自一个单位高斯分布。我们使用双尾假设检验，因为我们有兴趣知道计算出的<em class="nb">t-统计量</em>和零之间是否存在差异(或者是正的，由一个尾捕捉，或者是负的，由另一个尾捕捉)。</p><p id="6596" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">您可能会进一步问，我可以使用单尾假设检验来报告特性的重要性吗？是的，你可以，只是OLS班报告了一个双尾假设检验结果。</p><h1 id="36c8" class="me mf it bd mg mh mi mj mk ml mm mn mo ki mp kj mq kl mr km ms ko mt kp mu mv bi translated">结论</h1><p id="a5c4" class="pw-post-body-paragraph li lj it lk b ll mw kd ln lo mx kg lq lr my lt lu lv mz lx ly lz na mb mc md im bi translated">本文解释了如何将随机变量引入确定性最小二乘线性回归模型，以推断学习到的模型参数值的不确定性。我们使用随机变量，因为不确定性度量，如置信区间，只存在于随机变量中。</p><p id="128e" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">除了最小二乘法，线性回归还有其他公式，如完全贝叶斯公式、最大似然公式(MLE)和最大后验公式(MAP)公式。这些方法对不确定性有更自然推理方式。我将在以后的文章中介绍它们。</p><h1 id="0372" class="me mf it bd mg mh mi mj mk ml mm mn mo ki mp kj mq kl mr km ms ko mt kp mu mv bi translated">支持我</h1><p id="2497" class="pw-post-body-paragraph li lj it lk b ll mw kd ln lo mx kg lq lr my lt lu lv mz lx ly lz na mb mc md im bi translated">如果你喜欢我的故事，如果你考虑通过这个链接成为一名灵媒会员来支持我，我将不胜感激:<a class="ae lh" href="https://jasonweiyi.medium.com/membership" rel="noopener">https://jasonweiyi.medium.com/membership</a>。</p><p id="8f53" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我会继续写这些故事。</p><h1 id="11af" class="me mf it bd mg mh mi mj mk ml mm mn mo ki mp kj mq kl mr km ms ko mt kp mu mv bi translated">附录:法线方程的梯度</h1><p id="cef4" class="pw-post-body-paragraph li lj it lk b ll mw kd ln lo mx kg lq lr my lt lu lv mz lx ly lz na mb mc md im bi translated">让我们用一个例子来推导法线方程，其中我们有三个数据点，并且<em class="nb"> w </em>是一个二维向量。形式上:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ow"><img src="../Images/86e296dc2acb10b72d4bd4afc4a32b21.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zaFgocbZpjxPRnoQ_vAWVA.png"/></div></div></figure><p id="f9bf" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">在上面:</p><ul class=""><li id="e1e8" class="nc nd it lk b ll lm lo lp lr ne lv nf lz ng md nh ni nj nk bi translated"><em class="nb">【x₁=[x₁₁、x₁₂】</em><em class="nb">【x₂=[x₂₁、x₂₂】</em><em class="nb">【x₃=[x₃₁、x₃₂】</em>。它们是2×1行向量<em class="nb">。</em></li><li id="81d5" class="nc nd it lk b ll nl lo nm lr nn lv no lz np md nh ni nj nk bi translated"><em class="nb"> X₁₁、X₁₂、X₂₁、X₂₂、X₃₁、X₃₂ </em>是标量<em class="nb">。</em></li><li id="c846" class="nc nd it lk b ll nl lo nm lr nn lv no lz np md nh ni nj nk bi translated">Y <em class="nb"> ₁ </em>，Y <em class="nb"> ₂ </em>，Y <em class="nb"> ₃ </em>都是标量<em class="nb">。</em></li><li id="4025" class="nc nd it lk b ll nl lo nm lr nn lv no lz np md nh ni nj nk bi translated"><em class="nb"> w₁ </em>、<em class="nb"> w₂ </em>都是标量变量。</li></ul><p id="438c" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">下面我们进一步介绍一下向量<em class="nb"> U </em>:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ox"><img src="../Images/a7ce1f097be8876cee48fd782a7aa209.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Bp2yBIKHigEKv6ndQiFsCA.png"/></div></div></figure><p id="3747" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">其中<em class="nb"> U₁、U₂、U₃ </em>都是标量根据他们对<em class="nb"> Uᵢ=Yᵢ-Xᵢ w. </em>的定义可以清楚的看到<em class="nb"> U </em>是<em class="nb"> w </em>的函数。这为应用链式法则计算<em class="nb"> L(w) </em>相对于<em class="nb"> w </em>的导数铺平了道路。</p><p id="412d" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">现在让我们推导正常方程(再次显示):</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi oy"><img src="../Images/5a492397f3cfd8205cb7ef78385c0c86.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5LFJmBkndbz0nZ3-XrMiEg.png"/></div></div></figure><p id="c1dc" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">线(1)是损失函数<em class="nb"> L(w) </em>相对于模型参数<em class="nb"> w </em>的导数的符号。</p><p id="8da0" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">线(2)插在<em class="nb"> L(w) </em>的定义中。</p><p id="4bfa" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">第(3)行使用<em class="nb"> U </em>简化公式，并应用微分中的链式法则将整个导数分成两部分。</p><p id="245d" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">让我们分别关注这两个部分。</p><p id="ef2c" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">第一部分首先:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi oz"><img src="../Images/3d25e162fb154f1505af27543544fee2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*J28Kp2-L74d9wlHxrxHQPw.png"/></div></div></figure><p id="2245" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">第(1)行是第一项。</p><p id="3b91" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">第(2)行用基于元素的定义替换了向量<em class="nb"> U </em>。</p><p id="774c" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">第(3)行对分子应用点积运算。</p><p id="cf75" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">第(4)行明确写下了偏导数的定义——计算每个变量的导数，分别是<em class="nb"> U₁、</em>、<em class="nb">、</em>、<strong class="lk jd">按照约定</strong>将这三个导数组织成一行。</p><p id="a84c" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">第(5)行应用求导规则。</p><p id="2f9a" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">第(6)行插入u₁<em class="nb">、U₂ </em>和u₃.<em class="nb">的定义</em></p><p id="7685" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">第(7)行使用向量符号来简化公式。这给了我们一个接近正态方程中团队的项。</p><p id="655b" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">现在我们研究第二个术语:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pa"><img src="../Images/3e0c7fe229ad3268dda059e4824a553f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DIxRVhgjDKzC6XfRZVy_uw.png"/></div></div></figure><p id="43e8" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">第(1)行是第二项。</p><p id="9639" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">第(2)行扩展了<em class="nb"> U </em>和<em class="nb"> w </em>的元素式定义。</p><p id="fe8d" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">第(3)行应用向量偏导数规则，<strong class="lk jd">遵循行越过<em class="nb"> U </em>列越过<em class="nb"> w </em>的约定</strong>。</p><p id="f217" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">第(4)行扩展了<em class="nb"> U </em>的定义。</p><p id="91ed" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">第(5)行计算导数。</p><p id="5c15" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">Lien (6)使用向量符号简化了公式。</p><p id="b2ee" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">现在我们可以将这两项相乘:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pb"><img src="../Images/dc76bec8b7be746e889e438f33480ce5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Xa9tZqPtpAlBoNwMZfY98w.png"/></div></div></figure><p id="2f15" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">注意，在第(5)行，我们有一个行向量——记住，当我们做<em class="nb"> ∂w </em>时，我们遵循惯例，在一个行向量中写下关于<em class="nb"> w </em>的每个元素的偏导数。为了把<em class="nb"> ∂L(w)/∂w </em>变成一个列向量，我们转置它得到法线方程<em class="nb"> -2Xᵀ(Y-X w) </em>的左手边大小，我们也可以drop -2，因为我们有一个方程<em class="nb">。</em></p></div></div>    
</body>
</html>