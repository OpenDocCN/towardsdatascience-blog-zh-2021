<html>
<head>
<title>TFLite Micro vs GLOW AOT</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">TFLite微型vs辉光AOT</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/tflite-micro-vs-glow-aot-6524be02ba2a?source=collection_archive---------25-----------------------#2021-06-21">https://towardsdatascience.com/tflite-micro-vs-glow-aot-6524be02ba2a?source=collection_archive---------25-----------------------#2021-06-21</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="21c6" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">比较TinyML框架</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/f3e2d88f49c2bb8bce9c31d07bf68520.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KB4hgE5W7lHfpBIqt7a6aQ.png"/></div></div></figure><p id="5323" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">我用于TinyML推理的两个框架是用于微控制器的<a class="ae ln" href="https://www.tensorflow.org/lite/microcontrollers" rel="noopener ugc nofollow" target="_blank">tensor flow Lite</a>和<a class="ae ln" href="https://github.com/pytorch/glow" rel="noopener ugc nofollow" target="_blank"> GLOW </a>(更具体地说，是提前发光的<a class="ae ln" href="https://github.com/pytorch/glow/blob/master/docs/AOT.md" rel="noopener ugc nofollow" target="_blank">(AOT)</a>编译器)。由于我还没有真正看到两者之间的比较，我决定比较两个框架的实现，并执行一些基准测试。</p><h1 id="d3f2" class="lo lp iq bd lq lr ls lt lu lv lw lx ly jw lz jx ma jz mb ka mc kc md kd me mf bi translated">非常基本的概述</h1><p id="ac01" class="pw-post-body-paragraph kr ks iq kt b ku mg jr kw kx mh ju kz la mi lc ld le mj lg lh li mk lk ll lm ij bi translated">将经过训练的模型部署到微控制器(MCU)时，需要两个要素:</p><p id="f2ec" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">1)量化，针对大小和延迟优化模型。虽然从技术上讲，这不是严格要求的，但很可能只有极少数(如果有的话)模型可以在没有量化的情况下在MCU上运行</p><p id="ee33" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">2)推理机，用于对目标进行实际的推理。</p><p id="1d0f" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">虽然这两个框架都提供了量化模型的工具，但是我将把重点放在推理机上，因为这两个框架所采用的推理方法是非常不同的。</p><p id="72a7" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">TensorFlow将模型转换为包含执行推理所需的序列化步骤的<a class="ae ln" href="https://google.github.io/flatbuffers/" rel="noopener ugc nofollow" target="_blank"> FlatBuffer </a>，与运行在目标MCU上的库一起使用，该库解释FlatBuffer。FlatBuffer中包含的信息是模型中使用的权重和操作。为了有效地执行操作，TFlite微库针对不同的目标优化了内核。</p><p id="e754" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">GLOW生成编译后的代码来推断目标上的模型，因此得名“提前”编译器。将模型部署到MCU由一个包组成，该包包含一个编译的目标文件、头文件和权重文件，这些文件可以动态使用(在运行时)或静态使用(在为目标编译时)。</p><h1 id="e6e6" class="lo lp iq bd lq lr ls lt lu lv lw lx ly jw lz jx ma jz mb ka mc kc md kd me mf bi translated">比较这两种方法</h1><p id="dab8" class="pw-post-body-paragraph kr ks iq kt b ku mg jr kw kx mh ju kz la mi lc ld le mj lg lh li mk lk ll lm ij bi translated">每种框架采用的两种方法都有其优点。我将试着在下面列出优势和区别。</p><p id="a5cd" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">我不打算进入整个“在嵌入式中使用OOP”的争论。我只想说，如果在您的嵌入式项目中看到一个<code class="fe ml mm mn mo b">.cpp</code>或<code class="fe ml mm mn mo b">.cc</code>文件让您感到不舒服，那么您不必进一步阅读，因为GLOW是适合您的框架。TensorFlow使用C++，这是一个嵌入式友好的实现，但它仍然是C++。</p><h2 id="2c79" class="mp lp iq bd lq mq mr dn lu ms mt dp ly la mu mv ma le mw mx mc li my mz me na bi translated">内存使用</h2><p id="011d" class="pw-post-body-paragraph kr ks iq kt b ku mg jr kw kx mh ju kz la mi lc ld le mj lg lh li mk lk ll lm ij bi translated">你可能想知道为什么我没有在下面的基准测试中包括内存使用；这是因为我觉得这不是一个苹果与苹果的比较。</p><p id="0693" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">GLOW的包输出包含持久和可变内存所需的确切大小，因为推理代码已经编译好了，所以也很容易计算出来。这是非常确定的，它允许你很容易地把你的常数和/或你的程序放在ram中；如果您使用的是带有紧密耦合内存的ARM Cortex-M7，这可能特别有用。此外，使用GLOW生成的最终二进制文件更小。</p><p id="efef" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">另一方面，张量流不太确定。FlatBuffer大小是您得到的唯一精确值。可变内存可能是我最大的不满，因为你需要在RAM中分配一个名为<code class="fe ml mm mn mo b">tensor_arena</code>的区域，引用TensorFlow文档，“所需的大小将取决于你正在使用的模型，可能需要通过实验来确定。”。至于程序大小，在你的程序中，你指定你的模型将使用哪些操作，并且只有那些操作将被包含在你的可执行文件中。</p><p id="0e8e" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">作为一名嵌入式软件开发人员(阅读:控制狂)，我更喜欢GLOW方法。但是，如果您计划运行使用相似运算符的多个模型，TensorFlow可能会占上风，因为您将对所有模型使用相同的指令内存。</p><h2 id="6fd7" class="mp lp iq bd lq mq mr dn lu ms mt dp ly la mu mv ma le mw mx mc li my mz me na bi translated">部署</h2><p id="6e80" class="pw-post-body-paragraph kr ks iq kt b ku mg jr kw kx mh ju kz la mi lc ld le mj lg lh li mk lk ll lm ij bi translated">ML野兽的本质是不断升级更新的；因此，现场升级是必须的。这里我更喜欢TensorFlow实现。当您的模型改变时，只要您的输入和输出的形状保持不变，并且您在原始代码中包含了所有的操作，那么您需要做的就是替换您的FlatBuffer。</p><p id="6543" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">这意味着您可以将Flatbuffer存储在文件系统中，并使用您实现的任何通信协议来更新您的模型。从安全的角度来看，您可以在不改变可执行文件的情况下更新您的模型。并不是说它消除了所有的安全顾虑，但它仍然是有益的。也就是说，GLOW bundle是足够孤立的，通过一些思考，人们可以设计一个只允许模型升级的系统。</p><p id="57e0" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">虽然我更喜欢TensorFlow在现场更新模型，但这有多大好处将取决于用例。例如，如果用例不需要很多更新，或者如果更新很重要(需要改变输入或输出形状，或者引入新的操作)，好处可能不太有效。</p><h2 id="31e5" class="mp lp iq bd lq mq mr dn lu ms mt dp ly la mu mv ma le mw mx mc li my mz me na bi translated">轻便</h2><p id="eae7" class="pw-post-body-paragraph kr ks iq kt b ku mg jr kw kx mh ju kz la mi lc ld le mj lg lh li mk lk ll lm ij bi translated">可移植性有两个不同的元素:用于训练的框架和目标硬件。从技术上来说，GLOW是PyTorch的一部分，但可以很容易地与TensorFlow中训练的模型一起使用(但请注意，将PyTorch中训练的模型与TensorFlow Lite一起使用要复杂一些。)</p><p id="6b76" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">TensorFlow中的目标架构可移植性非常简单。如果您能够为您的目标编译C++代码，那么您应该能够为微控制器编译TensorFlow Lite。大概不会太优化，但是很管用。该库确实包含针对几种不同架构优化的<a class="ae ln" href="https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/micro/kernels" rel="noopener ugc nofollow" target="_blank">内核</a>(例如，<a class="ae ln" href="https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/micro/kernels/cmsis_nn" rel="noopener ugc nofollow" target="_blank"> ARM CMSIS NN </a>或<a class="ae ln" href="https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/micro/kernels/xtensa" rel="noopener ugc nofollow" target="_blank"> Cadence Xtensa </a>)。您还可以编写针对您的目标硬件优化的定制内核。例如，假设您的目标硬件有一个矩阵乘法引擎，并且您的模型使用许多深度卷积。在这种情况下，您可以创建一个定制的<a class="ae ln" href="https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/kernels/depthwise_conv.cc" rel="noopener ugc nofollow" target="_blank">深度卷积内核</a>的实现，同时使用库的其余部分。</p><p id="69c4" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">GLOW使用<a class="ae ln" href="https://llvm.org/" rel="noopener ugc nofollow" target="_blank"> LLVM </a>作为其后端编译器。如果LLVM编译器支持您的目标，它应该可以工作。我不知道它会被优化到什么程度——对于ARM来说，它似乎做得不错(见下面的基准测试)。如果您的目标架构不被支持，您可以随时添加它；毕竟LLVM是开源的。</p><p id="db28" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">我对TensorFlow内核的定制实现做了一些简单的实验，它非常简单，并且库中包含了单元测试。</p><p id="50aa" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">我主要使用TensorFlow进行训练，ARM Cortex-M进行推理，所以这两个框架对我来说都很好。</p><h1 id="af35" class="lo lp iq bd lq lr ls lt lu lv lw lx ly jw lz jx ma jz mb ka mc kc md kd me mf bi translated">基准</h1><p id="c9e8" class="pw-post-body-paragraph kr ks iq kt b ku mg jr kw kx mh ju kz la mi lc ld le mj lg lh li mk lk ll lm ij bi translated"><a class="ae ln" href="https://mlcommons.org/en/" rel="noopener ugc nofollow" target="_blank"> MLCommons </a>最近发布了<a class="ae ln" href="https://mlcommons.org/en/inference-tiny/" rel="noopener ugc nofollow" target="_blank"> MLPerf Tiny </a>，让基准测试变得真正简单。但是，应该注意的是，这里执行的基准测试仍然可以进一步优化，并不代表最高的可实现结果，而是显示了每个框架的性能，只需最少的挖掘。</p><h2 id="5e09" class="mp lp iq bd lq mq mr dn lu ms mt dp ly la mu mv ma le mw mx mc li my mz me na bi translated">硬件</h2><p id="7fb1" class="pw-post-body-paragraph kr ks iq kt b ku mg jr kw kx mh ju kz la mi lc ld le mj lg lh li mk lk ll lm ij bi translated">我使用制造商的开发工具在两个不同的MCU上运行了基准测试。纸板</p><ul class=""><li id="6c82" class="nb nc iq kt b ku kv kx ky la nd le ne li nf lm ng nh ni nj bi translated"><a class="ae ln" href="https://www.nxp.com/products/processors-and-microcontrollers/arm-microcontrollers/general-purpose-mcus/lpc5500-cortex-m33/high-efficiency-arm-cortex-m33-based-microcontroller-family:LPC55S6x" rel="noopener ugc nofollow" target="_blank"> LPC55S69 </a> —拥有双核Arm Cortex-M33，运行频率高达150 MHz。它还有一个硬件加速器，实质上是一个微型DSP。我喜欢使用的配置是用于预处理的Mini DSP(我尝试为DSP编写定制的TF内核，但我觉得用它进行预处理更有效)，其中一个内核作为推理引擎。虽然两个内核都是Cortex-M33，但只有一个内核支持<a class="ae ln" href="https://developer.arm.com/architectures/instruction-sets/dsp-extensions/dsp-for-cortex-m" rel="noopener ugc nofollow" target="_blank"> SIMD </a>，这大大加快了推理速度。</li><li id="3bf5" class="nb nc iq kt b ku nk kx nl la nm le nn li no lm ng nh ni nj bi translated">I . MX rt 1010——有一个Arm Cortex-M7，可以以高达<strong class="kt ir"> 500 MHz、</strong>的速度运行，尽管它的IOs非常有限，但大量生产时它的价格仅为<strong class="kt ir"> $0.99 </strong>。对于需要强大计算能力且不需要太多外围设备的应用程序，这是一个很有吸引力的选择(一个推理引擎浮现在脑海中)。</li></ul><h2 id="08ec" class="mp lp iq bd lq mq mr dn lu ms mt dp ly la mu mv ma le mw mx mc li my mz me na bi translated">设置</h2><p id="f520" class="pw-post-body-paragraph kr ks iq kt b ku mg jr kw kx mh ju kz la mi lc ld le mj lg lh li mk lk ll lm ij bi translated">从MLPerf，我使用了<a class="ae ln" href="https://github.com/mlcommons/tiny/tree/master/v0.5/reference_submissions/keyword_spotting" rel="noopener ugc nofollow" target="_blank">关键字定位</a>基准的<a class="ae ln" href="https://github.com/mlcommons/tiny/tree/master/v0.5" rel="noopener ugc nofollow" target="_blank"> v0.5 </a>和ee MBC<a class="ae ln" href="https://www.eembc.org/energybench/redist/?win" rel="noopener ugc nofollow" target="_blank">Windows 10</a>runner<a class="ae ln" href="https://github.com/eembc/energyrunner/tree/3.0.6" rel="noopener ugc nofollow" target="_blank">v 3 . 0 . 6</a>，我运行了中间性能和准确性测试。</p><p id="5c2c" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">对于软件，我使用了MCUXpresso IDE和恩智浦的SDK。我目前正在将不同的实现移植到一个单独的可移植项目中。一旦完成，可以在<a class="ae ln" href="https://github.com/sbranover/kws-bm" rel="noopener ugc nofollow" target="_blank">项目回购</a>中找到。两个MCU使用的<a class="ae ln" href="https://github.com/NXPmicro/mcux-sdk" rel="noopener ugc nofollow" target="_blank"> MCUXpresso SDK </a>版本I是2.9.1。</p><p id="b64b" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">对于TensorFlow基准测试，我编译了没有经过优化和经过<code class="fe ml mm mn mo b">-O3</code>优化的库。如果可能，我用存储在闪存和RAM中的值进行测试。对于i.MX RT1010，我将RAM配置为DTCM，以加快推断速度。</p><h2 id="5d52" class="mp lp iq bd lq mq mr dn lu ms mt dp ly la mu mv ma le mw mx mc li my mz me na bi translated">结果呢</h2><p id="1a18" class="pw-post-body-paragraph kr ks iq kt b ku mg jr kw kx mh ju kz la mi lc ld le mj lg lh li mk lk ll lm ij bi translated">结果可以在下表中看到</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="np nq l"/></div><p class="nr ns gj gh gi nt nu bd b be z dk translated">基准测试结果</p></figure><p id="af4d" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">GLOW似乎在优化Cortex-M7方面比Cortex-M33做得更好；这可能是因为Cortex- M7更合适(因此在优化方面投入了更多精力)，或者只是因为它是一种更成熟的架构。如果优化对您的应用程序非常重要，我建议在您的模型上尝试这两种框架。如果结果相似，进一步手动优化TensorFlow应该更容易。</p><h1 id="730a" class="lo lp iq bd lq lr ls lt lu lv lw lx ly jw lz jx ma jz mb ka mc kc md kd me mf bi translated">结论</h1><p id="5ee1" class="pw-post-body-paragraph kr ks iq kt b ku mg jr kw kx mh ju kz la mi lc ld le mj lg lh li mk lk ll lm ij bi translated">如上所述，这两种框架各有优势，因此您应该根据您的应用程序选择最适合的框架。</p><p id="6e42" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">这两个框架都很容易使用。因此，如果你是一名希望尝试机器学习的嵌入式软件工程师，或者是一名希望尝试嵌入式方面的机器学习工程师，我强烈建议尝试一下。</p><h1 id="eb3c" class="lo lp iq bd lq lr ls lt lu lv lw lx ly jw lz jx ma jz mb ka mc kc md kd me mf bi translated">官方基准</h1><p id="b2c2" class="pw-post-body-paragraph kr ks iq kt b ku mg jr kw kx mh ju kz la mi lc ld le mj lg lh li mk lk ll lm ij bi translated">在撰写本文时，第一批正式的<a class="ae ln" href="https://mlcommons.org/en/news/mlperf-tiny-v05/" rel="noopener ugc nofollow" target="_blank">基准测试已经发布</a>。值得注意的是Cortex-M4的结果，它比我在Cortex-M33 ( <a class="ae ln" href="https://community.arm.com/developer/ip-products/processors/trustzone-for-armv8-m/f/trustzone-armv8-m-forum/8338/what-is-the-top-level-difference-in-features-between-cortex-m33-and-cortex-m4" rel="noopener ugc nofollow" target="_blank">支持与Cortex-M4 </a>相同的DSP指令集)上的测试产生了更好的结果，尽管我使用的Cortex-M33运行在更高的时钟速度上。我试图通过编译LPC55S69 (Cortex-M33)的Mbed参考资料来验证结果，但没有成功。然而，我能够在<a class="ae ln" href="https://os.mbed.com/platforms/ST-Nucleo-L552ZE-Q/" rel="noopener ugc nofollow" target="_blank"> NUCLEO-L552ZE-Q </a>板上运行参考提交，这也是一个Cortex-M33，吞吐量为<code class="fe ml mm mn mo b">2.693 inf./sec.</code>，这与我得到的结果更加一致。</p><p id="147a" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">这让我想到Cortex-M33的优化可能还不够成熟，或者Cortex-M33上的DSP指令使用了更多的时钟周期。然而，正如我前面所说的，这些基准测试并不表示绝对的功能和开箱即用的体验，我相信结果可以进一步优化。</p></div><div class="ab cl nv nw hu nx" role="separator"><span class="ny bw bk nz oa ob"/><span class="ny bw bk nz oa ob"/><span class="ny bw bk nz oa"/></div><div class="ij ik il im in"><p id="dfcd" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">作者图片</p></div></div>    
</body>
</html>