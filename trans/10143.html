<html>
<head>
<title>Vanishing gradient in Deep Neural Networks</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">深度神经网络中的消失梯度</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/vanishing-gradient-in-deep-neural-network-83953217c59f?source=collection_archive---------14-----------------------#2021-09-25">https://towardsdatascience.com/vanishing-gradient-in-deep-neural-network-83953217c59f?source=collection_archive---------14-----------------------#2021-09-25</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><figure class="ip iq gp gr ir is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi io"><img src="../Images/4e2bdb11733e07ab9aedbc0082b47d3c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*LPRCy47L5K2T9DD6"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">照片由<a class="ae jd" href="https://unsplash.com/@jjying?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> JJ英</a>在<a class="ae jd" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a></p></figure><div class=""/><div class=""><h2 id="0c5a" class="pw-subtitle-paragraph kd jf jg bd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku dk translated">原因和可能的解决方案</h2></div><p id="762f" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">如今，用于图像分析的网络由许多层一层接一层堆叠而成，形成所谓的<em class="lr">深层网络</em>。训练这些架构的最大问题之一是<strong class="kx jh">消失梯度:</strong>梯度假设为零或渐近值，阻止权重被更新。</p><p id="9a64" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在本文中，我们将分析这种现象的原因，提出第一种可能的解决方案，并在后续文章中为进一步的技术留下更多细节。在开始之前，将涉及的例子将是使治疗更容易的基础。显然，同样的概念可以应用于更复杂的架构。</p><h1 id="4715" class="ls lt jg bd lu lv lw lx ly lz ma mb mc km md kn me kp mf kq mg ks mh kt mi mj bi translated">注释</h1><ul class=""><li id="4f46" class="mk ml jg kx b ky mm lb mn le mo li mp lm mq lq mr ms mt mu bi translated"><strong class="kx jh"> w: </strong> <em class="lr"> </em>重量</li><li id="4cbd" class="mk ml jg kx b ky mv lb mw le mx li my lm mz lq mr ms mt mu bi translated"><strong class="kx jh"> <em class="lr"> z: </em> </strong>神经元输入的线性和</li><li id="1831" class="mk ml jg kx b ky mv lb mw le mx li my lm mz lq mr ms mt mu bi translated"><strong class="kx jh"> <em class="lr"> g: </em> </strong>激活功能</li><li id="0701" class="mk ml jg kx b ky mv lb mw le mx li my lm mz lq mr ms mt mu bi translated"><strong class="kx jh"> <em class="lr">一个</em> </strong> <em class="lr"> : </em>激活功能的输出</li><li id="8007" class="mk ml jg kx b ky mv lb mw le mx li my lm mz lq mr ms mt mu bi translated"><strong class="kx jh"> <em class="lr">顶点</em> </strong>:层<em class="lr"> l- </em> th</li><li id="db1a" class="mk ml jg kx b ky mv lb mw le mx li my lm mz lq mr ms mt mu bi translated"><strong class="kx jh"> L: </strong>损失函数</li></ul><h1 id="58c2" class="ls lt jg bd lu lv lw lx ly lz ma mb mc km md kn me kp mf kq mg ks mh kt mi mj bi translated">1.什么是消失渐变？</h1><figure class="nb nc nd ne gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi na"><img src="../Images/e5c9ffe590df988544b92b7bd7c54883.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dTPhh99OZ-J6p5OdEeiLiA.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">图1:该图显示了FCNN VGG-16网络不同层获得的特征图(来源:图片由我提供)</p></figure><p id="a048" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在深度卷积网络中，输入附近的层从图像中提取<strong class="kx jh">空间特征</strong>:轮廓的<em class="lr">方向</em>，角度<em class="lr">的存在</em>，纹理的存在<em class="lr">，<em class="lr">颜色</em>信息。接近输出层，<strong class="kx jh">语义特征</strong>将被提取:出现<em class="lr">嘴</em>、<em class="lr">鼻子</em>等等。图1显示了一个例子，其中有一些在VGG-16的不同层提取的特征图。一般来说，通过增加网络的深度，每一层将能够学习如何提取更多的<em class="lr">一般</em>和<em class="lr">复杂特征</em>。</em></p><p id="b911" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">然而，深层网络很难训练。我们可以看到的一个问题是<strong class="kx jh">消失渐变</strong>。在反向传递过程中，靠近输入的图层的权重保持不变或更新非常缓慢，这与靠近输出的图层的情况相反。</p><figure class="nb nc nd ne gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nf"><img src="../Images/78831d0cb4638c396a0ed0edf90af4af.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*NSbQSCtG47GiMS8T.jpeg"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">图2:有三个神经元的简单网络(来源:图片由我提供)</p></figure><p id="86d1" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">让我们考虑图2中神经元的顺序。我们通过计算损失函数相对于它的导数来更新权重w^(1。为简单起见，我们假设每个神经元的偏置项<em class="lr"> b </em>为零。应用<em class="lr">链规则</em>【1】并将激活函数的导数分组，我们得到:</p><figure class="nb nc nd ne gt is gh gi paragraph-image"><div class="gh gi ng"><img src="../Images/7f06e6d29a584368de74497dff9fc8dc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1226/format:webp/0*mwtSz5iczzrxZnYf.png"/></div></figure><p id="788a" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我们来问一下，有什么可以阻止或者减缓梯度？我们分析产品∂ <em class="lr">一/∂z.</em></p><p id="a04e" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx jh">情况1: </strong>假设每次迭代只有第<em class="lr">个</em>为空(<em class="lr">例如</em> ∂ <em class="lr"> a^(2) </em> /∂z^(2)).)在这种情况下，∂L /∂w^(1)将呈现一个空值，阻止w^(1)的更新，该值将保持<strong class="kx jh">不变</strong>。事实上，应用<em class="lr">德尔塔法则</em>【2】<em class="lr"/>我们得到:</p><figure class="nb nc nd ne gt is gh gi paragraph-image"><div class="gh gi nh"><img src="../Images/f554a3fabd1d8f2893081e9f56868a38.png" data-original-src="https://miro.medium.com/v2/resize:fit:586/format:webp/1*_u8QKybKWErhknoNF259Pg.png"/></div></figure><p id="a734" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx jh">情况2: </strong>现在假设每一项∂ <em class="lr">和</em> ^(i)/∂z^(i)在每次迭代中都有渐近零值。这样，∂L /∂w^(1)也将渐近于零，w^(1)将非常缓慢地更新<strong class="kx jh"/>(<em class="lr">我们每次都将接近零的值相加</em>)。应用<em class="lr"> delta规则</em>:</p><figure class="nb nc nd ne gt is gh gi paragraph-image"><div class="gh gi nh"><img src="../Images/9604849d220fb873a4e453f63439644d.png" data-original-src="https://miro.medium.com/v2/resize:fit:586/format:webp/1*VuH6B4ycsyHDQsyflBGQ7g.png"/></div></figure><h1 id="3d9d" class="ls lt jg bd lu lv lw lx ly lz ma mb mc km md kn me kp mf kq mg ks mh kt mi mj bi translated">2.输出范围有限的激活功能</h1><figure class="nb nc nd ne gt is gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/dbda45633411e8172ffa0358493d0854.png" data-original-src="https://miro.medium.com/v2/resize:fit:1204/format:webp/1*w0RTVNG2Ep1URuQg35vQBw.png"/></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">图3:带共域[0，1]的sigmoid函数用蓝色表示。它的导数用红色表示。输入z的值表示在横坐标轴上，而相应导数的值表示在纵坐标轴上(来源:图片由me提供)</p></figure><p id="6daf" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">从上一段的例子中，我们了解了激活函数<em class="lr"> g </em>的选择是如何发挥重要作用的。消失梯度的主要原因之一是由于使用了<em class="lr">激活功能</em>和<em class="lr">有限输出范围:</em></p><ul class=""><li id="d28d" class="mk ml jg kx b ky kz lb lc le nj li nk lm nl lq mr ms mt mu bi translated">对于落在<em class="lr">有效区域</em>内的输入值<em class="lr"> z </em>，导数∂<em class="lr">a</em>/∂z<strong class="kx jh">将仅是非零的</strong></li><li id="2268" class="mk ml jg kx b ky mv lb mw le mx li my lm mz lq mr ms mt mu bi translated">在显著区域，导数呈现相对较小的值<strong class="kx jh">。考虑到大量的层和乘以∂a/∂z的值，我们可以降低渐变速度</strong></li></ul><p id="ce39" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">这两点完美地反映了案例1和案例2的内容。让我们通过将图3所示的sigmoid (σ)视为有限输出范围激活函数来分析它们。考虑到导数<em class="lr">，</em>，对于落在区间[-4.4]内的输入值<em class="lr"> z </em>，出现<em class="lr">显著区域</em>。对于该区间之外的值，导数将为零<em class="lr">(情况1)。</em></p><figure class="nb nc nd ne gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nm"><img src="../Images/0e66e4d27a04b67ac770bd7104e77534.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*bbTyQgE8iomsDC5B.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">图4:具有K个神经元的平面网络(来源:图片由我提供)</p></figure><p id="3496" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">现在假设我们有一个K层的网络，其中，对于每个神经元，<em class="lr"> z </em>在重要区域中取值。计算关于层K、K-1、K-2和1的梯度，我们得到:</p><figure class="nb nc nd ne gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nn"><img src="../Images/2a2b72e2f13c2539bd967c555950f7aa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*SQK8C8Uic58I0Mhb.jpeg"/></div></div></figure><p id="68c3" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">通过乘以红色值，我们注意到随着我们远离输出层k，乘积∂ <em class="lr"> a </em> / ∂z是如何减小的。如果<em class="lr"> K </em>等于50层，我们将得到<em class="lr">情况2 </em>:</p><figure class="nb nc nd ne gt is gh gi paragraph-image"><div class="gh gi no"><img src="../Images/9f055e7638960fd9f2969a70d9ca21e6.png" data-original-src="https://miro.medium.com/v2/resize:fit:484/format:webp/0*F06OQvuHvsa71Qyg.png"/></div></figure><h1 id="4312" class="ls lt jg bd lu lv lw lx ly lz ma mb mc km md kn me kp mf kq mg ks mh kt mi mj bi translated">3.整流线性单位(ReLU)</h1><figure class="nb nc nd ne gt is gh gi paragraph-image"><div class="gh gi np"><img src="../Images/8fdad87974fd93eb3ac8fbfcc1347316.png" data-original-src="https://miro.medium.com/v2/resize:fit:1300/format:webp/1*m5IGMCavn1qE3G0XejMdHg.png"/></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">图5:带有codomain [0，+inf]的ReLU函数用蓝色表示。它的导数用红色表示。输入z的值表示在横坐标轴上，而相应导数的值表示在纵坐标上(来源:图片由me提供)</p></figure><p id="e697" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">图5所示的ReLU激活函数可以通过<em class="lr"> max(0，z)来形式化。</em>这个<em class="lr"> </em>促成了消失渐变的分辨率(<em class="lr">至少部分)</em>。观察图6中的导数，我们注意到对于R^+中的<em class="lr"> z </em>的值，导数是单一的，因此允许梯度通过。当输入值为负时出现问题，从而得到<em class="lr">情况1 </em>。存在整流线性单元的变体，例如LeakyReLU和eLU，其被创建来减轻对于<em class="lr"> z </em> &lt; 0的零导数的问题。</p><h1 id="def1" class="ls lt jg bd lu lv lw lx ly lz ma mb mc km md kn me kp mf kq mg ks mh kt mi mj bi translated">结论</h1><p id="0245" class="pw-post-body-paragraph kv kw jg kx b ky mm kh la lb mn kk ld le nq lg lh li nr lk ll lm ns lo lp lq ij bi translated">训练具有大量层的网络并不容易。在这篇文章中，我们探讨了<em class="lr">消失梯度</em>问题<em class="lr"> </em>识别激活函数选择中的一个可能原因。建议的解决方案是使用校正的线性单位，对于正值，它不会阻碍梯度的通过。但是… ReLU并没有完全解决问题。另一个问题是<strong class="kx jh">权重的初始化。</strong>如果多层的导数∂z /∂ <em class="lr"> a </em>为零或接近零，会发生什么？可以重复刚才讨论的关于激活函数的考虑和例子来回答这个问题。</p><p id="8423" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">如果一方面我们有<em class="lr">消失</em> <em class="lr">渐变</em>，另一方面我们必须考虑完全相反的问题，即<em class="lr">爆炸渐变</em>:渐变呈现高值，从而阻止优化。</p><p id="d10b" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">对深度网络训练的一个重要贡献是引入了<strong class="kx jh">跳过连接</strong>，我们将在另一篇文章<strong class="kx jh">中讨论。</strong></p><h1 id="7269" class="ls lt jg bd lu lv lw lx ly lz ma mb mc km md kn me kp mf kq mg ks mh kt mi mj bi translated">参考</h1><p id="4c91" class="pw-post-body-paragraph kv kw jg kx b ky mm kh la lb mn kk ld le nq lg lh li nr lk ll lm ns lo lp lq ij bi translated">【1】<a class="ae jd" href="https://en.wikipedia.org/wiki/Chain_rule" rel="noopener ugc nofollow" target="_blank">微分学:链式法则</a></p><p id="afbf" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">[2] <a class="ae jd" href="https://en.wikipedia.org/wiki/Delta_rule" rel="noopener ugc nofollow" target="_blank">梯度下降学习规则:Delta规则</a></p></div></div>    
</body>
</html>