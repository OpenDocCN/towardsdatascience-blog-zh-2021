<html>
<head>
<title>Conceptual vs in-built Principal Component Analysis for Breast Cancer Diagnosis</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">乳腺癌诊断的概念主成分分析与内在主成分分析</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/conceptual-vs-inbuilt-principal-component-analysis-for-breast-cancer-diagnosis-7ec3a8455201?source=collection_archive---------34-----------------------#2021-04-26">https://towardsdatascience.com/conceptual-vs-inbuilt-principal-component-analysis-for-breast-cancer-diagnosis-7ec3a8455201?source=collection_archive---------34-----------------------#2021-04-26</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="85bf" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在PCA中拟合特征值、特征向量和python内置函数的概念，然后使用ML模型来测量乳腺癌(Wisconsin)数据集的高级精度参数。使用2个聚类和4个分类模型进行分析。</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="gh gi kl"><img src="../Images/d7dd6ee4eee9e1efa456e0350873b1da.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*oeKCeLYOa-w6isy-"/></div></div><p class="kx ky gj gh gi kz la bd b be z dk translated">在<a class="ae lb" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上由<a class="ae lb" href="https://unsplash.com/@cikstefan?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">斯特凡·斯特凡·契克</a>拍摄的照片</p></figure><p id="2b4e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">简介:</strong></p><p id="3f5e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们这些热衷于通过python探索世界的数据科学爱好者知道，python只用几行代码就使主成分分析<strong class="jp ir"> </strong>变得非常容易。但是我们不能忘记选择主成分背后的基本概念。在本文中，在将机器学习模型应用于聚类和分类之前，我已经以两种方式实现了PCA一种是计算特征值和特征向量，另一种是使用传统的scikit-learn工具。</p><p id="931e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在深入研究这些方法之前，让我先问你两个非常简单的问题:</p><p id="50f5" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">为什么要处理一个庞大的数据集，使计算成为一项艰巨的任务？</p><p id="b395" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">为什么不减少它，同时保留它所提供的几乎所有有价值的信息呢？</p><p id="f233" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这就是PCA用匕首来拯救你的地方，它砍掉了数据，减少了数据的维数，使分析更容易，同时保留了手头数据的统计数据。现在你可能会想，PCA是如何决定哪些变量要删除，哪些要保留的呢？那就是降维的妙处；它不是删除变量，而是将信息组合成新的主成分，这些主成分是原始变量的线性组合。是的，准确性确实受到了一点影响，但获得的简单性更有价值。</p><p id="c021" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">当你在思考的时候，让我为你描绘一个非常简单的场景..</p><p id="afe6" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">假设你打算很久以后去见你的学校朋友。他们在家里急切地等着你。但是，你的朋友圈挺大的，去他们各自的住处拜访，会占用相当多的时间和精力。你能想到的直接解决办法是什么？这时你决定给他们每个人打电话，约他们在附近的咖啡馆见面。这样，你可以用最少的时间和精力去结识所有的朋友，而不必排除任何人。这不就和上面的解释差不多了吗？</p><p id="2b66" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">PCA以这样的方式构造新的变量或主要成分，即大部分信息被塞入第一成分，而最大信息保留在第二成分中，以此类推。它创建了与原始变量相同数量的组件，但是每个组件中包含的信息量随着组件数量的增加而减少。一个<strong class="jp ir">简单的图表</strong>将帮助您直观地了解主成分如何解释数据中的最大总方差。当我们在数据集上实现它时，这幅图将会变得更加清晰。</p><p id="f03c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">我们将要处理的数据集- </strong></p><p id="6da3" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我选择了一个维度足够大的数据集来应用PCA，因为所有真实世界的数据，无论是结构化的还是非结构化的，都是巨大的。Kaggle的“乳腺癌(威斯康星州)”数据集包含了癌症和非癌症患者的数据。我已经分享了数据的链接-【https://www.kaggle.com/uciml/breast-cancer-wisconsin-data<em class="lc"/>。</p><p id="fe22" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">分析是在Python (Google colab)上实现的，这里是我在GitHub中的代码的链接</p><p id="7712" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><a class="ae lb" href="https://github.com/debangana97/Data-science-works/blob/main/Breast_cancer_diagnosis_using_PCA_two_ways.ipynb" rel="noopener ugc nofollow" target="_blank">https://github . com/debangana 97/Data-science-works/blob/main/Breast _ cancer _ diagnosis _ using _ PCA _ two _ ways . ipynb</a></p><p id="e57e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">从文件中可以明显看出，有569行和30列，也就是说，30个不同的变量对应于关于患者的30个不同特征的信息。这些特征有助于确定患者是否患有乳腺癌。然而，同时检查所有30个变量来预测未来患者的状况是非常乏味的，并且容易出错。对癌症的错误预测是医生最不想做的事！</p><p id="b7c4" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">目标:</strong></p><p id="4c68" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这一分析的主要目标可以清楚地表明在以下几点:-</p><ol class=""><li id="df84" class="ld le iq jp b jq jr ju jv jy lf kc lg kg lh kk li lj lk ll bi translated">使用主成分分析来降低数据的维数仅仅意味着主成分分析将把30个变量组合成更小的主成分</li><li id="9b34" class="ld le iq jp b jq lm ju ln jy lo kc lp kg lq kk li lj lk ll bi translated">根据通过主成分分析获得的新变量(主成分)对数据进行聚类</li><li id="a3a4" class="ld le iq jp b jq lm ju ln jy lo kc lp kg lq kk li lj lk ll bi translated">检查减少的数据是否可用于分类</li><li id="d4e9" class="ld le iq jp b jq lm ju ln jy lo kc lp kg lq kk li lj lk ll bi translated">基于分类模型的精确度找到主成分的最佳数量</li></ol><p id="b77b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">既然我们已经清楚了这篇文章的主要目的，那就让我们开始吧。</p><p id="090a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">准备数据..</strong></p><p id="d876" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">数据预处理或数据清理是数据分析中最重要的先决条件之一。数据的适当准备将导致可靠和可理解的结果。这里有几个步骤，我发现对我的分析是必要的-</p><ul class=""><li id="3b23" class="ld le iq jp b jq jr ju jv jy lf kc lg kg lh kk lr lj lk ll bi translated">删除对研究没有帮助的不必要的列</li></ul><pre class="km kn ko kp gt ls lt lu lv aw lw bi"><span id="546b" class="lx ly iq lt b gy lz ma l mb mc">#removing the unnecessary columns</span><span id="4e15" class="lx ly iq lt b gy md ma l mb mc">exclude = ['Unnamed: 32','id']</span><span id="6043" class="lx ly iq lt b gy md ma l mb mc">data = data.drop(exclude, axis = 1)</span></pre><ul class=""><li id="e40a" class="ld le iq jp b jq jr ju jv jy lf kc lg kg lh kk lr lj lk ll bi translated">检查缺少的值。非常幸运的是，我的数据集中没有丢失值，因此我可以使用整个原始数据，而不必操作任何点</li><li id="bd2b" class="ld le iq jp b jq lm ju ln jy lo kc lp kg lq kk lr lj lk ll bi translated">检查数据不平衡</li></ul><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="gh gi me"><img src="../Images/211029304f5204f018169a22ae06172d.png" data-original-src="https://miro.medium.com/v2/resize:fit:806/format:webp/1*LMTdnt32V5rnXY8w-quMSg.png"/></div><p class="kx ky gj gh gi kz la bd b be z dk translated">图2:显示每一个班级人数的计数图</p></figure><ul class=""><li id="593c" class="ld le iq jp b jq jr ju jv jy lf kc lg kg lh kk lr lj lk ll bi translated">将数据分为训练集和测试集，其中20%的数据被保留用于测试模型的性能，其余的用于训练。训练集的大小为455行30列</li><li id="c518" class="ld le iq jp b jq lm ju ln jy lo kc lp kg lq kk lr lj lk ll bi translated">将训练数据分成两个独立的数据帧。因变量/目标变量(y)包括癌症结果，即数据的<strong class="jp ir">‘诊断’</strong>列，特征变量(x)包括患者的所有特征</li><li id="84cf" class="ld le iq jp b jq lm ju ln jy lo kc lp kg lq kk lr lj lk ll bi translated">训练集和测试集中的特征矩阵x的标准化是为了更好的计算和无偏的结果。<em class="lc"> StandardScalar </em>包用于数据的集中和规范化</li></ul><pre class="km kn ko kp gt ls lt lu lv aw lw bi"><span id="e82d" class="lx ly iq lt b gy lz ma l mb mc">#standardization or feature scaling of the matrix of features X does both centering and normalizing of the data</span><span id="2a00" class="lx ly iq lt b gy md ma l mb mc">from sklearn.preprocessing import StandardScaler</span><span id="d3b4" class="lx ly iq lt b gy md ma l mb mc">sc = StandardScaler()<br/>x_train_std = sc.fit_transform(x_train)<br/>x_test_std = sc.fit_transform(x_test)</span></pre><ul class=""><li id="7a30" class="ld le iq jp b jq jr ju jv jy lf kc lg kg lh kk lr lj lk ll bi translated">对分类变量“y”进行编码</li></ul><p id="5091" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">主成分分析:</strong></p><p id="0a2a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">方法1 </strong></p><p id="a2df" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在进行PCA之前，有必要将数据标准化。标准化确保每个原始变量对分析的贡献相等。这种降维方法对原始变量的方差非常敏感，其中值范围较大的变量将比范围较小的变量占优势，因此结果将趋于有偏差。此步骤以这样一种方式转换变量，即整个训练数据的平均值为0，标准偏差为1。</p><pre class="km kn ko kp gt ls lt lu lv aw lw bi"><span id="d2fa" class="lx ly iq lt b gy lz ma l mb mc">print(np.mean(x_train_std))<br/>print(np.std(x_train_std))</span><span id="6a0b" class="lx ly iq lt b gy md ma l mb mc"><strong class="lt ir">Output:</strong> 4.021203394686281e-17 <br/>        1.0</span></pre><p id="2c0c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在python中，我计算了协方差矩阵。矩阵中的正负符号表明特征变量之间是直接相关还是反向相关。</p><p id="f85b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">从线性代数的知识中，从这个矩阵中，我们可以构造特征向量，这些特征向量实际上是新轴的方向，其中有最多的信息(最高方差)并且被称为主分量。它们对应的特征值是特征向量的系数，表示每个主分量传递的信息量(方差)。</p><p id="0dd7" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">那么现在应该选择哪些组件呢？</p><p id="f724" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">想..</p><p id="fd6f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">想..</p><p id="0461" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">正确！有最大特征值的那些，对吗？</p><p id="cff2" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我选择了具有最高特征值的两个分量，并基于这两个分量绘制了显示“恶性”和“良性”之间的区别的二维图。在这里，我也使用不同的组件进行了一些试验，看看哪一对组件能更好地显示这两类患者。</p><ul class=""><li id="906f" class="ld le iq jp b jq jr ju jv jy lf kc lg kg lh kk lr lj lk ll bi translated">第一次投影在第一和第二主分量的平面上。第二次投影在第二和第三主分量的平面上。</li></ul><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="gh gi mf"><img src="../Images/bd4ec44b2a66a0f088bfca059bb3e71e.png" data-original-src="https://miro.medium.com/v2/resize:fit:948/format:webp/1*4I0VHdBwRjZEp4JsXUknQg.png"/></div><p class="kx ky gj gh gi kz la bd b be z dk translated">图:基于第一和第二主成分绘制了由两类患者分组的数据</p></figure><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="gh gi mg"><img src="../Images/d5046b49a8a37877e7c6f0acfcb9b38e.png" data-original-src="https://miro.medium.com/v2/resize:fit:946/format:webp/1*4kzzqb3X6rgMx0jDOdIC8g.png"/></div><p class="kx ky gj gh gi kz la bd b be z dk translated">图:基于第二和第三主成分绘制了由两类患者分组的数据</p></figure><ul class=""><li id="33ed" class="ld le iq jp b jq jr ju jv jy lf kc lg kg lh kk lr lj lk ll bi translated">从图中可以清楚地看到，第一种投影更好地分离了类别。这是一个明显的结果，因为第一分量包含最高比例的信息。</li></ul><blockquote class="mh mi mj"><p id="6901" class="jn jo lc jp b jq jr js jt ju jv jw jx mk jz ka kb ml kd ke kf mm kh ki kj kk ij bi translated">尝试使用第一个和第三个组件绘制图表。你得到了什么？成绩提高了吗？</p></blockquote><p id="451d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">方法二</strong></p><p id="566f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">接下来，我使用scikit-learn直接执行主成分分析，这在第一种方法之后看起来非常容易。Python完成了上述所有计算，并最终向我们展示了一个图表(scree plot ),按照解释的变化百分比顺序显示了主要成分。该图显示累计约<strong class="jp ir">总变化的73%仅由</strong> <strong class="jp ir">前三个分量</strong>解释。</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="gh gi mn"><img src="../Images/5050bf20d419735c73965118eeed1c37.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FNbYrdUvDQ60RPLNirH92Q.png"/></div></div><p class="kx ky gj gh gi kz la bd b be z dk translated">图:显示30个组成部分中每一个解释的变化百分比的Scree图</p></figure><p id="b14b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">为了与方法1的结果进行比较，我绘制了显示基于前两个主成分的两个不同类的数据，这给出了与不直接使用scikit-learn的结果相似的可视化。因此，这两种方法表现得一样好，python已经和人脑不相上下了！</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="gh gi mg"><img src="../Images/233b7ad21774b8e8c334e366ae752740.png" data-original-src="https://miro.medium.com/v2/resize:fit:946/format:webp/1*lio-syiqbsxlznZ6D34BbA.png"/></div><p class="kx ky gj gh gi kz la bd b be z dk translated">图5:Python scikit-learn给出了一个与上图类似的图表</p></figure><p id="56dc" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">下一步是什么？..</p><p id="80e0" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">现在数据从30个变量减少到3个变量，定义了一个新的特征向量，它包含作为列的三个分量的特征向量。因此，新特征向量的维数是455(行)乘3(列)。</p><pre class="km kn ko kp gt ls lt lu lv aw lw bi"><span id="c6ee" class="lx ly iq lt b gy lz ma l mb mc">#transforming the data to the reduced dimension data<br/>#this is the data that we are going to work with during the analysis</span><span id="4c53" class="lx ly iq lt b gy md ma l mb mc">pca_data = pca.fit_transform(x_train_std)</span><span id="7cff" class="lx ly iq lt b gy md ma l mb mc">print('The reduced data is of the dimension: ', pca_data.shape)</span><span id="036a" class="lx ly iq lt b gy md ma l mb mc"><strong class="lt ir">Output:</strong> The reduced data is of the dimension:  (455, 3)</span></pre><p id="c2b5" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我要实现的第一个机器学习算法是K-Means聚类和层次凝聚聚类模型。粗略地说，聚类是一种无监督的ML算法，并且不需要任何训练，因为它基于特征变量创建整个数据的聚类。现在让我们看看这些模型如何在我们的数据集上工作。</p><p id="e6ba" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir"> K均值聚类与层次聚类- </strong></p><p id="9875" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在确定聚类之前，我们如何决定要将数据分成多少个聚类？集群应该以这样的方式形成，即它们之间是同质的，同时与其他集群共享异质性。</p><p id="cb26" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在K-Means聚类中，我们使用肘方法图中聚类平方和内的<strong class="jp ir">,找到最佳聚类数。你为什么要问？看一下图，你会注意到曲线形状像一个肘部，因此得名。但是我们如何决定是使用2个、3个还是5个集群呢？</strong></p><figure class="km kn ko kp gt kq"><div class="bz fp l di"><div class="mo mp l"/></div></figure><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="gh gi mq"><img src="../Images/dceda50510840d52aa03c8b77eb2879f.png" data-original-src="https://miro.medium.com/v2/resize:fit:850/format:webp/1*Pg4TSvTk1SgdBm_yOP02dQ.png"/></div><p class="kx ky gj gh gi kz la bd b be z dk translated">图:弯头方法图</p></figure><p id="23a8" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">为了消除这种困惑，我使用了一种叫做<strong class="jp ir">轮廓分数</strong>的度量标准，它测量一个聚类中的每个点与相邻聚类中的点的接近程度。侧影分数+1表明该点离相邻聚类非常远，从而表明这些聚类是不同的，而分数0表明该点与另一个聚类中的点重叠或者离它们不是非常远。因此，显然，对应于最高轮廓分数的聚类数是分析的最佳选择。</p><figure class="km kn ko kp gt kq"><div class="bz fp l di"><div class="mo mp l"/></div></figure><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="gh gi mr"><img src="../Images/a80c51fa37a6c0e27fd21a99eae6b554.png" data-original-src="https://miro.medium.com/v2/resize:fit:798/format:webp/1*g9dsoJiM_fYME8TWncw6sQ.png"/></div><p class="kx ky gj gh gi kz la bd b be z dk translated">图:每个聚类数的轮廓分数</p></figure><p id="014c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">从图中可以看出，2个集群给出了最好的结果。</p><p id="9d6f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">对于聚类的分层方法，它使用<strong class="jp ir">树状图</strong>来确定最佳的聚类数目，这显著地表明2个聚类将给出有希望的结果。</p><pre class="km kn ko kp gt ls lt lu lv aw lw bi"><span id="59f5" class="lx ly iq lt b gy lz ma l mb mc">#Using dendrogram to find the optimal number of clusters</span><span id="1239" class="lx ly iq lt b gy md ma l mb mc">import scipy.cluster.hierarchy as sch<br/>dendrogram = sch.dendrogram(sch.linkage(pca_data, method = 'ward'))</span></pre><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="gh gi ms"><img src="../Images/32bdf728f11db43a8389f21e8fa2c8b5.png" data-original-src="https://miro.medium.com/v2/resize:fit:812/format:webp/1*PSS3dtI4b9XkFddQbPg7nA.png"/></div><p class="kx ky gj gh gi kz la bd b be z dk translated">图:树状图</p></figure><p id="85c8" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在下面的代码行中，使用2、3、4和5个集群对这两种方法进行了彻底的比较。在可视化技术的帮助下，集群以不同的颜色呈现。这些图表确实引人注目，不是吗？</p><div class="km kn ko kp gt ab cb"><figure class="mt kq mu mv mw mx my paragraph-image"><img src="../Images/3e28a5512b9caa737dd62c03cc6a57f9.png" data-original-src="https://miro.medium.com/v2/resize:fit:756/format:webp/1*noGDeZokA5Kgo4cUlvd_CA.png"/></figure><figure class="mt kq mz mv mw mx my paragraph-image"><img src="../Images/9315cc1c7b22bd4f942b87ac5964e516.png" data-original-src="https://miro.medium.com/v2/resize:fit:832/format:webp/1*4qFr51QXeTyzU2IVvnK0wQ.png"/><p class="kx ky gj gh gi kz la bd b be z dk na di nb nc translated">图:使用3个集群完成的集群</p></figure></div></div><div class="ab cl nd ne hu nf" role="separator"><span class="ng bw bk nh ni nj"/><span class="ng bw bk nh ni nj"/><span class="ng bw bk nh ni"/></div><div class="ij ik il im in"><div class="ab cb"><figure class="mt kq mu mv mw mx my paragraph-image"><img src="../Images/f856eff13d708a7dc665d8efd6c0c9e7.png" data-original-src="https://miro.medium.com/v2/resize:fit:756/format:webp/1*S1Dd32GP3FhJUWOa1pIotA.png"/></figure><figure class="mt kq mz mv mw mx my paragraph-image"><img src="../Images/7149f810ab59b275057108cb9cd3ece1.png" data-original-src="https://miro.medium.com/v2/resize:fit:832/format:webp/1*nF9WSOpLByXDUscu0XtRVQ.png"/><p class="kx ky gj gh gi kz la bd b be z dk na di nb nc translated">图:使用5个集群完成的集群</p></figure></div><p id="b4e6" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">随着集群数量的增加，它们开始相互重叠。</p><p id="106a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我使用了另一个被称为Davies-Bouldin指数的验证指标来确定最佳的集群数量。它评估聚类分散和聚类分离之间的比率。</p><p id="98fc" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><em class="lc">那么，你认为这个指数的理想值是多少？</em></p><p id="3ff3" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">如果群集定义明确、紧凑且独特，则群集分散肯定小于群集间隔。因此，您认为DV指数值越低，表示聚类越好，这是正确的。下表给出了不同k值的戴维斯-波尔丁指数及其各自的质心坐标。如前所述，该索引值最小的那个是聚类的最佳选择。</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/14bdb2090ce2b70367a01a53b9ae8763.png" data-original-src="https://miro.medium.com/v2/resize:fit:876/format:webp/1*ir-5EbDB41isJVIZ4Vqpzw.png"/></div><p class="kx ky gj gh gi kz la bd b be z dk translated">图2:显示不同数量的聚类的DV指数的表格，并附有聚类坐标</p></figure><p id="53e4" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">从上面的讨论和后面的表格中可以明显看出，对于k = 2，聚类将给出最好的结果。</p><p id="48e1" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">聚类和分类可以一起应用吗？</strong></p><p id="0130" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">机器学习的基础知识将允许我们问自己，虽然聚类是一种无监督的ML算法，但分类是一种有监督的算法，那么两者如何应用于同一数据集？数据是否预先标记？我们可以为未标记的数据生成标签吗？</p><p id="fb9c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">一旦我们计算出聚类的<strong class="jp ir">纯度度量</strong>，就可以给出这些问题的答案。在得到结果之前，我将告诉你一些关于这个指标和它是如何工作的。</p><ul class=""><li id="2f28" class="ld le iq jp b jq jr ju jv jy lf kc lg kg lh kk lr lj lk ll bi translated">差的聚类将具有接近0的纯度值。纯度分数的最高值是1，其中值1意味着每个文档都有自己的聚类。这意味着每个文档都有自己的标签。正如我们所知，分类是一种受监督的机器学习算法，其中类别是预先标记的，因此如果聚类的纯度是1，这意味着它可以用于分类。</li><li id="95cb" class="ld le iq jp b jq lm ju ln jy lo kc lp kg lq kk lr lj lk ll bi translated">一般来说，当簇的数量大时，高纯度容易实现。然而，如果纯度等于1，那么形成簇就没有意义。</li><li id="1302" class="ld le iq jp b jq lm ju ln jy lo kc lp kg lq kk lr lj lk ll bi translated">在我们的例子中，我计算了K-均值聚类和层次凝聚聚类的这个度量，聚类的数量= 2、3、4和5，并且发现在所有情况下纯度值都接近1。因此，即使集群数量较少，我们也可以应用分类。</li></ul><figure class="km kn ko kp gt kq"><div class="bz fp l di"><div class="mo mp l"/></div></figure><p id="c936" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir"> K-NN分类或者Logistic回归或者支持向量机，谁分类最好？</strong></p><p id="4e5f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">因为纯度指标证实了分类模型可以有效地处理数据，所以我决定使用四种不同的模型来比较结果。目标变量(y)显示患者是否被诊断为乳腺癌。它是一个二元变量，其中“M”代表恶性,“B”代表良性。<strong class="jp ir">标签编码</strong>有助于数据的分类和可视化。对于每个模型，我使用了不同数量的组件，分析了结果，并以表格的形式展示了结果。</p><pre class="km kn ko kp gt ls lt lu lv aw lw bi"><span id="2244" class="lx ly iq lt b gy lz ma l mb mc">#Encoding the target variable</span><span id="2262" class="lx ly iq lt b gy md ma l mb mc">from sklearn.preprocessing import LabelEncoder<br/>le = LabelEncoder()</span><span id="e3e7" class="lx ly iq lt b gy md ma l mb mc">y_train = le.fit_transform(y_train)</span><span id="217c" class="lx ly iq lt b gy md ma l mb mc">y_test = le.fit_transform(y_test)</span></pre><p id="648c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">分类结果的可视化是一种享受！使用2个主要成分，我已经展示了一些迷人的图表供你欣赏..</p><div class="km kn ko kp gt ab cb"><figure class="mt kq nl mv mw mx my paragraph-image"><img src="../Images/060fc267f39697d6291f86a295816d57.png" data-original-src="https://miro.medium.com/v2/resize:fit:798/format:webp/1*o8xAtNR3dnHift5DOOw_7g.png"/></figure><figure class="mt kq nm mv mw mx my paragraph-image"><img src="../Images/cb65869c6d479037d7b2950e24e5dd1f.png" data-original-src="https://miro.medium.com/v2/resize:fit:822/format:webp/1*t3im_CVHmj1OMgTjCtXAmA.png"/><p class="kx ky gj gh gi kz la bd b be z dk nn di no nc translated">图2:训练集和测试集上的K-最近邻分类器</p></figure></div></div><div class="ab cl nd ne hu nf" role="separator"><span class="ng bw bk nh ni nj"/><span class="ng bw bk nh ni nj"/><span class="ng bw bk nh ni"/></div><div class="ij ik il im in"><div class="ab cb"><figure class="mt kq np mv mw mx my paragraph-image"><img src="../Images/73049152d7ec11b3de1008a4dbbfd89b.png" data-original-src="https://miro.medium.com/v2/resize:fit:794/format:webp/1*ohMauuBiys7SOZ_yyJnO_Q.png"/></figure><figure class="mt kq nq mv mw mx my paragraph-image"><img src="../Images/af77e4b24c7ace43bde632fa679dd5a0.png" data-original-src="https://miro.medium.com/v2/resize:fit:822/format:webp/1*uaL5MO5YgHZwtcuhwLgfSg.png"/><p class="kx ky gj gh gi kz la bd b be z dk nn di no nc translated">图:训练集和测试集的逻辑回归</p></figure></div></div><div class="ab cl nd ne hu nf" role="separator"><span class="ng bw bk nh ni nj"/><span class="ng bw bk nh ni nj"/><span class="ng bw bk nh ni"/></div><div class="ij ik il im in"><div class="ab cb"><figure class="mt kq np mv mw mx my paragraph-image"><img src="../Images/b25c9add2c914cf9f43cf004005b5f7d.png" data-original-src="https://miro.medium.com/v2/resize:fit:794/format:webp/1*GmyLk-iBEZllKqmz8KIIPg.png"/></figure><figure class="mt kq nq mv mw mx my paragraph-image"><img src="../Images/2aea7151a0fa4ed546b83bb694dbaff9.png" data-original-src="https://miro.medium.com/v2/resize:fit:822/format:webp/1*Mz-UsTjoO9p9N76h22l8Gw.png"/><p class="kx ky gj gh gi kz la bd b be z dk nn di no nc translated">图:训练集和测试集上的线性SVM</p></figure></div></div><div class="ab cl nd ne hu nf" role="separator"><span class="ng bw bk nh ni nj"/><span class="ng bw bk nh ni nj"/><span class="ng bw bk nh ni"/></div><div class="ij ik il im in"><div class="ab cb"><figure class="mt kq np mv mw mx my paragraph-image"><img src="../Images/bf1c2be116c206c8b99609be3319d1ab.png" data-original-src="https://miro.medium.com/v2/resize:fit:794/format:webp/1*sSt76ZfTnAX_ldg0xN_png.png"/></figure><figure class="mt kq nq mv mw mx my paragraph-image"><img src="../Images/88ac90808f97ddf6eee65c3c6dc8eb08.png" data-original-src="https://miro.medium.com/v2/resize:fit:822/format:webp/1*0V3CDEewqKVtP2O9MNNQZQ.png"/><p class="kx ky gj gh gi kz la bd b be z dk nn di no nc translated">图:训练集和测试集上的径向基函数(核)SVM</p></figure></div><p id="67b4" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">嗯，一口气画了很多图！但是你能指出它们之间的一些区别吗？看起来线性分类器在这个数据集上会工作得更好。</p><p id="544f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">使用不同数量的主成分，我已经应用分类模型回到回答我们的主要问题- <strong class="jp ir">我们实际上需要多少个主成分？</strong></p><p id="bab9" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">从分类分析中获得的准确度结果(百分比)可列表如下-</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/4a31efbc6bc45a260e4df30f7975221c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1216/format:webp/1*-KFmhGUCqp4QJlaKHPpeeA.png"/></div><p class="kx ky gj gh gi kz la bd b be z dk translated">图2:显示特定于主成分数量的每个分类模型的总体准确度的表格</p></figure><p id="cbc2" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">如果我们仔细研究这张表，可以得出以下几点结论</p><ul class=""><li id="70fc" class="ld le iq jp b jq jr ju jv jy lf kc lg kg lh kk lr lj lk ll bi translated">对于两个主成分，K-NN给出最高的精确度，而核SVM给出最低的精确度</li><li id="daad" class="ld le iq jp b jq lm ju ln jy lo kc lp kg lq kk lr lj lk ll bi translated">K-NN对4个分量达到最大精度，此后开始下降</li><li id="7d5b" class="ld le iq jp b jq lm ju ln jy lo kc lp kg lq kk lr lj lk ll bi translated">使用4个主成分，线性SVM导致大约96%的最大准确度</li><li id="9808" class="ld le iq jp b jq lm ju ln jy lo kc lp kg lq kk lr lj lk ll bi translated">当考虑4个主成分时，逻辑回归也能够获得大约95%的准确度，此后，它保持恒定</li><li id="6e28" class="ld le iq jp b jq lm ju ln jy lo kc lp kg lq kk lr lj lk ll bi translated">使用4个主成分，核SVM提供了大约94%的准确度</li><li id="f397" class="ld le iq jp b jq lm ju ln jy lo kc lp kg lq kk lr lj lk ll bi translated">由核SVM结果获得的总体精度低于由线性SVM获得的精度，这表明这两类可以通过直线更好地分开</li></ul><p id="14d1" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我添加的一些其他可比指标包括-</p><p id="b19f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><em class="lc">混淆矩阵:</em>这是一个2x2的列联表，显示模型做出的正确预测数和错误预测数</p><p id="4855" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><em class="lc">每个单独类别的准确度分数:</em>计算模型预测每个类别的准确度</p><p id="aa97" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><em class="lc">平均类别特定准确度:</em>上面计算的单个准确度的平均值</p><p id="01ec" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><em class="lc">预测正值得分:</em>被模型预测为患有癌症的患者实际患有癌症的概率。</p><p id="6ea5" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">去看看我提供的全部代码，你肯定会理解这些价值。</p><p id="e116" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">总结我们可以说..</strong></p><p id="b829" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">因为我们的目标是在保留最大信息量的同时降低数据的维数，所以我认为可以肯定地说主成分的最佳数量应该是4。</p><blockquote class="mh mi mj"><p id="67f3" class="jn jo lc jp b jq jr js jt ju jv jw jx mk jz ka kb ml kd ke kf mm kh ki kj kk ij bi translated">你不这么认为吗？</p></blockquote><p id="d3cb" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我希望阅读这篇文章有助于您的数据科学知识库，并帮助您获得更深入的见解。因此，请记住，从下次开始，如果你可以用较少的食物提供相同的营养，就不要过量进食你的python练习册..谢谢大家！</p></div></div>    
</body>
</html>