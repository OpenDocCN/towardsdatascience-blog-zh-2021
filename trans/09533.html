<html>
<head>
<title>How to Learn Multiple Tasks with a Single Neural Network</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如何用单个神经网络学习多项任务</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/how-to-learn-multiple-tasks-with-a-single-neural-network-50a99e01e6f9?source=collection_archive---------16-----------------------#2021-09-04">https://towardsdatascience.com/how-to-learn-multiple-tasks-with-a-single-neural-network-50a99e01e6f9?source=collection_archive---------16-----------------------#2021-09-04</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="dffb" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">灾难性遗忘曾经是一个棘手的问题。最近取得了进展，请继续阅读以了解更多信息。</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/89ccaf088d22914668b2a6bdb73c2f66.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*PPjIbWKTt95CTOZW"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">艾莉娜·格鲁布尼亚克在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="7ac1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现代神经网络非常擅长学习一件特定的事情。无论是下棋还是折叠蛋白质，只要有足够的数据和时间，神经网络就能取得惊人的结果。不幸的是，网络目前只能胜任一项任务。你可以训练一个网络擅长某件事，但一旦你试图教网络别的东西，它就会忘记在第一项任务中学到了什么。这被称为<strong class="lb iu">灾难性遗忘</strong>。由于智能的标志之一是学习和存储多项任务，因此如何在多项任务上训练神经网络(并解决灾难性遗忘)的问题极其重要。</p><p id="2dea" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">一个想法是改变训练数据，使得每个任务的训练示例相互交错。例如，假设我们有3个任务A、B和C，示例分别标记为a_i、b_i和c_i。这种思想将训练集排序为a1，B1，C1，a2，B2，C2，a3等。重点是以同等的重视程度同时训练所有三个任务，希望通过这种方式学习的网络权重将包含关于所有三个任务的信息。在实践中，这个想法是可行的——网络将同时在所有三项任务上慢慢变好，避免灾难性的遗忘。</p><p id="0e97" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">但是，在我看来这种方法是作弊。真正的智能学习(例如人类)不需要以这种方式交错任务。事实上，通常情况下，任务是在大时间块内连续学习的(我们称之为“块调度”)。典型的学校是这样安排的:首先你学1小时数学，然后1小时英语，然后再学1小时历史，以此类推。你不做一道数学题，写一句作文，然后看一句历史书。那么问题来了:有没有一种方法可以让神经网络用块调度来学习多个任务？</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><p id="5a62" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">2016年，Deepmind的研究人员出了一篇<a class="ae ky" href="https://arxiv.org/pdf/1612.00796.pdf" rel="noopener ugc nofollow" target="_blank">论文</a>解决了这个问题。我特别喜欢他们的方法，因为它没有那么复杂。他们真正做的只是对网络应用一种特殊类型的正则化。让我们仔细看看。</p><p id="9553" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">假设我们有两个任务，A和b，block schedule是先用很多例子训练A，然后我们切换到训练b，Deepmind的研究人员提出先正常训练A(即正则梯度下降/反向传播)。然后，在B块期间，我们保持从A学习的权重，并继续梯度下降。唯一的区别是我们现在包括了一个二次正则化项，它对于每个权重都是唯一的。这个想法是使用这个每权重正则化来惩罚远离从A学习的权重。被认为对A更重要的权重将被惩罚得更重。在数学上，我们在B训练块期间的成本函数是L(θ)= L _ B(θ)+σ(k _ I *(θ_Ai-θ_ I))，其中I是所有网络权重的指数，θ_ Ai是A训练块完成后的权重。L_B(θ)是B的正态成本函数，可能是平方误差或对数损失。最后，k_i是权重I对于预测a的重要性。</p><p id="cca2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">还有一种更直观的方式来思考每权重正则化。想象一个物理弹簧。当你拉弹簧时，你拉得越远，弹簧拉回来的力就越大。此外，有些弹簧比其他弹簧更坚固。这和我们的算法有什么关系？你可以想象一个弹簧连接到神经网络中的每个重量。所有弹簧的相对强度是正则化的。某些弹簧(对A来说很重要)会特别强，所以在B的训练过程中，算法会不鼓励去拉那些强弹簧，对应的权重不会有太大的变化。因此，该算法将改为拉动较弱的弹簧，并且对应于这些弹簧的权重将改变更多。</p><p id="5429" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">考虑这个算法的另一种方式是，它是对L2正则化的改进。利用L2正则化，权重不被鼓励改变太多，其惩罚对应于权重大小的平方和。然而，在L2正则化中，<strong class="lb iu">所有的</strong>权重被同等地惩罚。在我们的算法中，<strong class="lb iu">只有重要的权重</strong>被阻止改变。</p><p id="342a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">好的——我们现在直观地理解了这个算法是如何工作的。通过保持A的重要权重相对恒定，我们可以在b上成功训练的同时保持A的性能。但是，我们仍然没有解释如何确定A的“重要”权重。所以让我们问一个问题:什么使权重变得重要？一个合理的答案可能是:如果一个权重比其他权重对最终预测的影响更大，那么它就是重要的。更具体地说，如果一个权重相对于最终预测的导数具有比其他权重导数更高的幅度，我们可以说该权重是重要的。但是我们忽略了一件事——因为神经网络中的权重会影响其他权重，它们的导数是相互关联的。换句话说，我们不能只考虑给定权重的导数本身；我们需要查看所有权重导数的<strong class="lb iu">协方差矩阵。一个更正式的版本被称为费希尔信息矩阵，这是研究人员最终使用的。</strong></p><p id="a0ec" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">因此，总结一下:首先我们正常地训练A，然后我们用每权重二次正则化训练B。这些每个权重的正则化依赖于权重对A的相对重要性，这可以通过Fisher信息矩阵找到。结果是一个对A和b都很有效的神经网络。研究人员给这种方法起的名字是“<strong class="lb iu">弹性权重合并</strong>”(<strong class="lb iu">EWC</strong>)。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><p id="f8df" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">所有这些理论上的讨论都很棒，但实际上有用吗？研究人员在监督学习和强化学习环境中测试了这种新方法，以找出答案。首先，为了测试监督学习，研究人员使用了流行的MNIST任务。为了从MNIST创建多个任务，他们获取输入的MNIST图像，并用几个固定的常数对它们进行排列。这个想法是，在一个排列(MNIST图像+常数1)上训练的分类器不会在另一个排列(MNIST图像+常数2)上工作，所以实际上，我们有不同的任务。然后，研究人员比较了EWC、规则梯度下降和L2正则化的结果。在第一项任务中(MNIST图像的第一次排列)，所有三种方法都是可比较的。随着越来越多的任务被引入(其他排列)，EWC远远比其他人表现得更好。</p><p id="c9b1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">强化学习也有类似的结果。在这里，实验是学习十种不同的雅达利游戏。游戏的顺序是随机的。同样，在第一场比赛中，代理使用EWC和使用基线(<a class="ae ky" href="https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf" rel="noopener ugc nofollow" target="_blank"> DQN </a>)获得了相似的性能。随着更多游戏的引入，EWC在监督学习案例中逐渐退出。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><p id="ca3d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在这篇文章中，我们介绍了灾难性遗忘的问题，并通过EWC算法背后的直觉。我个人认为EWC是朝着真正智能的正确方向迈出的一步，我很高兴看到接下来会发生什么。请留下任何问题/评论，感谢阅读！</p></div></div>    
</body>
</html>