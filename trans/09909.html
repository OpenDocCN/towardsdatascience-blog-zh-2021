<html>
<head>
<title>Eigenvalues and eigenvectors in PCA</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">主成分分析中的特征值和特征向量</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/eigenvalues-and-eigenvectors-378e851bf372?source=collection_archive---------2-----------------------#2021-09-18">https://towardsdatascience.com/eigenvalues-and-eigenvectors-378e851bf372?source=collection_archive---------2-----------------------#2021-09-18</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="045b" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">关于我们的数据，他们告诉了我们什么？</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ki"><img src="../Images/07b9e5f7519a7fa3b2a9991508dcfe2a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/1*V4H3Cu8qGr_90WANKSO9BA.gif"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">(作者GIF)</p></figure><h1 id="fe27" class="ku kv it bd kw kx ky kz la lb lc ld le jz lf ka lg kc lh kd li kf lj kg lk ll bi translated">介绍</h1><p id="48aa" class="pw-post-body-paragraph lm ln it lo b lp lq ju lr ls lt jx lu lv lw lx ly lz ma mb mc md me mf mg mh im bi translated">我在大学的一门线性代数课上学过特征值和特征向量。这是非常枯燥和数学，所以我不明白，这是怎么回事。但是我想用一种更直观的方式向你们展示这个话题，我会用很多动画来说明。</p><p id="82dd" class="pw-post-body-paragraph lm ln it lo b lp mi ju lr ls mj jx lu lv mk lx ly lz ml mb mc md mm mf mg mh im bi translated">首先，我们将看看如何应用一个矩阵到一个矢量<strong class="lo iu">旋转</strong>和<strong class="lo iu">缩放</strong>一个矢量。这将向我们展示什么是<strong class="lo iu">特征值</strong>和<strong class="lo iu">特征向量</strong>。然后我们将学习<strong class="lo iu">主成分</strong>，它们是<strong class="lo iu">协方差矩阵</strong>的特征向量。这些知识将帮助我们理解我们最后的主题，<strong class="lo iu">主成分分析</strong>。</p><h1 id="2766" class="ku kv it bd kw kx ky kz la lb lc ld le jz lf ka lg kc lh kd li kf lj kg lk ll bi translated">矩阵乘法</h1><p id="3e0b" class="pw-post-body-paragraph lm ln it lo b lp lq ju lr ls lt jx lu lv lw lx ly lz ma mb mc md me mf mg mh im bi translated">为了理解特征值和特征向量，我们必须先看看矩阵乘法。</p><p id="71ca" class="pw-post-body-paragraph lm ln it lo b lp mi ju lr ls mj jx lu lv mk lx ly lz ml mb mc md mm mf mg mh im bi translated">让我们考虑下面的矩阵。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mn"><img src="../Images/155cab4ccb9cea540fb890a45ebbe54a.png" data-original-src="https://miro.medium.com/v2/resize:fit:264/format:webp/1*gn2xK7frRef1Wiu-zFGgMA.png"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">(图片由作者提供)</p></figure><p id="01c6" class="pw-post-body-paragraph lm ln it lo b lp mi ju lr ls mj jx lu lv mk lx ly lz ml mb mc md mm mf mg mh im bi translated">当我们取一个矩阵和一个向量的点积时，得到的向量是原始向量的<strong class="lo iu">旋转</strong>和<strong class="lo iu">缩放</strong>版本。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mo"><img src="../Images/a29f3356374efc94ea7bb920a3ec5c8d.png" data-original-src="https://miro.medium.com/v2/resize:fit:688/format:webp/1*uflnQLCyrNUowEEGuQttFg.png"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">(图片由作者提供)</p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ki"><img src="../Images/401846965ca2bae240eaa70cb18284ac.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/1*_dDhNmWycxe14qgN5mK5Zg.gif"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">(作者GIF)</p></figure><p id="83a1" class="pw-post-body-paragraph lm ln it lo b lp mi ju lr ls mj jx lu lv mk lx ly lz ml mb mc md mm mf mg mh im bi translated">在数据科学中，我们大多谈论数据点，而不是向量。但它们本质上是相同的，可以互换使用。数据点也可以像矢量一样通过矩阵乘法进行变换。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ki"><img src="../Images/5026d0b2bcb023efb6beb6712c5618f4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/1*YYc7PTRpFbgwvJzlTbSEuw.gif"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">(作者GIF)</p></figure><p id="21e2" class="pw-post-body-paragraph lm ln it lo b lp mi ju lr ls mj jx lu lv mk lx ly lz ml mb mc md mm mf mg mh im bi translated">但即使矩阵乘法旋转缩放，也是一个<strong class="lo iu">线性</strong>变换。</p><p id="f769" class="pw-post-body-paragraph lm ln it lo b lp mi ju lr ls mj jx lu lv mk lx ly lz ml mb mc md mm mf mg mh im bi translated">为什么矩阵乘法是一种<strong class="lo iu">线性</strong>变换？考虑一组数据点(用红色表示)。想象一下这些点所在的网格。当我们将矩阵应用于我们的数据点并沿着数据点移动网格时，我们看到网格的线保持笔直。如果这些线是曲线，那么转换将是非线性的。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ki"><img src="../Images/25f8c5f4d83eefe6804a8127b69dc67f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/1*j3vLqKw_vud4F8xVGu-_lQ.gif"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">(作者GIF)</p></figure><h1 id="6a77" class="ku kv it bd kw kx ky kz la lb lc ld le jz lf ka lg kc lh kd li kf lj kg lk ll bi translated">特征向量</h1><p id="360b" class="pw-post-body-paragraph lm ln it lo b lp lq ju lr ls lt jx lu lv lw lx ly lz ma mb mc md me mf mg mh im bi translated">我们考虑与上面相同的矩阵。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mn"><img src="../Images/155cab4ccb9cea540fb890a45ebbe54a.png" data-original-src="https://miro.medium.com/v2/resize:fit:264/format:webp/1*gn2xK7frRef1Wiu-zFGgMA.png"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">(图片由作者提供)</p></figure><p id="5746" class="pw-post-body-paragraph lm ln it lo b lp mi ju lr ls mj jx lu lv mk lx ly lz ml mb mc md mm mf mg mh im bi translated">当将这个矩阵应用于不同的向量时，它们的行为是不同的。他们中的一些可能会得到<strong class="lo iu">旋转和缩放</strong>。有些<strong class="lo iu">只旋转了</strong>，有些<strong class="lo iu">只缩放了</strong>，有些<strong class="lo iu">可能根本没有变化。</strong></p><p id="3bbc" class="pw-post-body-paragraph lm ln it lo b lp mi ju lr ls mj jx lu lv mk lx ly lz ml mb mc md mm mf mg mh im bi translated"><strong class="lo iu">特征向量</strong> s是向量，其中</p><ul class=""><li id="aff6" class="mp mq it lo b lp mi ls mj lv mr lz ms md mt mh mu mv mw mx bi translated"><strong class="lo iu">只进行缩放。</strong></li><li id="710d" class="mp mq it lo b lp my ls mz lv na lz nb md nc mh mu mv mw mx bi translated">或者<strong class="lo iu"> </strong>做<strong class="lo iu">完全不变。</strong></li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ki"><img src="../Images/551f9b303fa37ebc961bea6ca46b8c30.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/1*CIOA6ENQ8kqUHLoUMsgKGQ.gif"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">(作者GIF)</p></figure><p id="9add" class="pw-post-body-paragraph lm ln it lo b lp mi ju lr ls mj jx lu lv mk lx ly lz ml mb mc md mm mf mg mh im bi translated">你可以看到，特征向量保持在同一条线上，其他向量(一般向量)旋转了一定角度。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ki"><img src="../Images/8e3d17f16fc8ba14c2f0aed1499022f2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/1*k5blFj0Hr_Sjc7B5XW2GbQ.gif"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">(作者GIF)</p></figure><p id="7ccf" class="pw-post-body-paragraph lm ln it lo b lp mi ju lr ls mj jx lu lv mk lx ly lz ml mb mc md mm mf mg mh im bi translated">2x2矩阵总是有两个特征向量，但并不总是相互正交。</p><h1 id="df27" class="ku kv it bd kw kx ky kz la lb lc ld le jz lf ka lg kc lh kd li kf lj kg lk ll bi translated">本征值</h1><p id="fd28" class="pw-post-body-paragraph lm ln it lo b lp lq ju lr ls lt jx lu lv lw lx ly lz ma mb mc md me mf mg mh im bi translated">每个特征向量有一个相应的特征值。当矩阵对特征向量进行变换时，它是特征向量缩放的因子。如上所述，我们考虑相同的矩阵，因此考虑相同的两个特征向量。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mn"><img src="../Images/155cab4ccb9cea540fb890a45ebbe54a.png" data-original-src="https://miro.medium.com/v2/resize:fit:264/format:webp/1*gn2xK7frRef1Wiu-zFGgMA.png"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">(图片由作者提供)</p></figure><p id="6cad" class="pw-post-body-paragraph lm ln it lo b lp mi ju lr ls mj jx lu lv mk lx ly lz ml mb mc md mm mf mg mh im bi translated">这个矩阵的两个特征向量之一(我称之为特征向量1，但这是任意的)被缩放1.4倍。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ki"><img src="../Images/badfaeb912fc085ec3b4530698c06833.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/1*v9T_s1S7zsgP127PhzmAWw.gif"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">(作者GIF)</p></figure><p id="8d12" class="pw-post-body-paragraph lm ln it lo b lp mi ju lr ls mj jx lu lv mk lx ly lz ml mb mc md mm mf mg mh im bi translated">特征向量2的缩放系数也是1.4，但它的方向是相反的。因此，特征值2为-1.4。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ki"><img src="../Images/7a86edcbc43d8f22d5643a5e80daa6cc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/1*ICR5SemGrxKtARYm98O-TA.gif"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">(作者GIF)</p></figure><h1 id="539b" class="ku kv it bd kw kx ky kz la lb lc ld le jz lf ka lg kc lh kd li kf lj kg lk ll bi translated">特征向量和特征值在数据科学中的应用</h1><h2 id="26a2" class="nd kv it bd kw ne nf dn la ng nh dp le lv ni nj lg lz nk nl li md nm nn lk no bi translated">主成分</h2><p id="8fe5" class="pw-post-body-paragraph lm ln it lo b lp lq ju lr ls lt jx lu lv lw lx ly lz ma mb mc md me mf mg mh im bi translated">使用特征值和特征向量，我们可以找到数据的主轴。第一主轴(也称为“第一主成分”)是数据变化最大的轴。第二主轴(也称为“第二主成分”)是变化第二大的轴，以此类推。</p><p id="ae24" class="pw-post-body-paragraph lm ln it lo b lp mi ju lr ls mj jx lu lv mk lx ly lz ml mb mc md mm mf mg mh im bi translated">让我们考虑一个二维数据集。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ki"><img src="../Images/c9a81390f0899f78db5ea2f4db51840f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/1*e_kBZQz2hsa7de6TxpgJqg.gif"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">(作者GIF)</p></figure><p id="dd73" class="pw-post-body-paragraph lm ln it lo b lp mi ju lr ls mj jx lu lv mk lx ly lz ml mb mc md mm mf mg mh im bi translated">为了找到主成分，我们首先计算方差-协方差矩阵<strong class="lo iu"> C </strong>。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="nq nr di ns bf nt"><div class="gh gi np"><img src="../Images/5231dacb7f00a26f400a8dda683c8c12.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fk8ye-BKgTYgGc-U-ncsxA.png"/></div></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">(图片由作者提供)</p></figure><p id="74ae" class="pw-post-body-paragraph lm ln it lo b lp mi ju lr ls mj jx lu lv mk lx ly lz ml mb mc md mm mf mg mh im bi translated">我们可以用numpy来计算它们。注意，我们的数据(<strong class="lo iu"> X </strong>)必须像熊猫数据框一样排序。每列代表一个不同的变量/特征。</p><pre class="kj kk kl km gt nu nv nw nx aw ny bi"><span id="496c" class="nd kv it nv b gy nz oa l ob oc">import numpy as np<br/>C = np.cov(X, rowvar = False)</span></pre><p id="1a37" class="pw-post-body-paragraph lm ln it lo b lp mi ju lr ls mj jx lu lv mk lx ly lz ml mb mc md mm mf mg mh im bi translated">然后我们就可以计算出<strong class="lo iu"> C </strong>的特征向量和特征值。</p><pre class="kj kk kl km gt nu nv nw nx aw ny bi"><span id="644e" class="nd kv it nv b gy nz oa l ob oc">import numpy as np<br/>eigenvalues,eigenvectors = np.linalg.eig(C)</span></pre><p id="1ca5" class="pw-post-body-paragraph lm ln it lo b lp mi ju lr ls mj jx lu lv mk lx ly lz ml mb mc md mm mf mg mh im bi translated">特征向量向我们展示了数据主轴(主成分)的方向。特征值越大，沿该轴的变化越大。因此，具有最大特征值的特征向量对应于具有最大方差的轴。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ki"><img src="../Images/44cdebb22aaac07aa66f29f3611751ba.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/1*BpwgqgR-dVZSmIPKTaM4JQ.gif"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">(作者GIF)</p></figure><p id="1cda" class="pw-post-body-paragraph lm ln it lo b lp mi ju lr ls mj jx lu lv mk lx ly lz ml mb mc md mm mf mg mh im bi translated">我们应该记住，矩阵代表线性变换。当我们将协方差矩阵与我们的数据相乘时，我们可以看到数据的中心没有变化。并且数据在具有较大方差/特征值的特征向量的方向上被拉伸，并且沿着具有较小方差的特征向量的轴被压缩。</p><p id="6ddc" class="pw-post-body-paragraph lm ln it lo b lp mi ju lr ls mj jx lu lv mk lx ly lz ml mb mc md mm mf mg mh im bi translated">直接位于特征向量上的数据点不会旋转。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ki"><img src="../Images/d97cdf180e951ba288f95ba073b55558.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/1*0xDAg3EkUb7yBKabSThfCA.gif"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">(作者GIF)</p></figure><h2 id="5b7d" class="nd kv it bd kw ne nf dn la ng nh dp le lv ni nj lg lz nk nl li md nm nn lk no bi translated">主成分分析</h2><p id="eaa4" class="pw-post-body-paragraph lm ln it lo b lp lq ju lr ls lt jx lu lv lw lx ly lz ma mb mc md me mf mg mh im bi translated">主成分分析使用特征向量和特征值的能力来减少数据中的特征数量，同时保留大部分方差(因此也保留了大部分信息)。在PCA中，我们预先指定想要保留的成分数量。</p><p id="dc62" class="pw-post-body-paragraph lm ln it lo b lp mi ju lr ls mj jx lu lv mk lx ly lz ml mb mc md mm mf mg mh im bi translated">PCA算法由以下步骤组成。</p><ol class=""><li id="bd1f" class="mp mq it lo b lp mi ls mj lv mr lz ms md mt mh od mv mw mx bi translated">通过减去平均值并除以标准差来标准化数据</li><li id="4b8c" class="mp mq it lo b lp my ls mz lv na lz nb md nc mh od mv mw mx bi translated">计算协方差矩阵。</li><li id="0cca" class="mp mq it lo b lp my ls mz lv na lz nb md nc mh od mv mw mx bi translated">计算特征值和特征向量</li><li id="0f46" class="mp mq it lo b lp my ls mz lv na lz nb md nc mh od mv mw mx bi translated">将特征向量合并成矩阵，并将其应用于数据。这将旋转和缩放数据。主要组件现在与我们的特征轴对齐。</li><li id="8755" class="mp mq it lo b lp my ls mz lv na lz nb md nc mh od mv mw mx bi translated">保留引起最大变化的新特征，丢弃其余的。</li></ol><p id="3f62" class="pw-post-body-paragraph lm ln it lo b lp mi ju lr ls mj jx lu lv mk lx ly lz ml mb mc md mm mf mg mh im bi translated">让我们看看PCA在二维数据集上做了什么。在这个例子中，我们没有减少特征的数量。减少特征的数量对于高维数据是有意义的，因为这样可以减少特征的数量。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ki"><img src="../Images/07b9e5f7519a7fa3b2a9991508dcfe2a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/1*V4H3Cu8qGr_90WANKSO9BA.gif"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">(作者GIF)</p></figure><p id="a178" class="pw-post-body-paragraph lm ln it lo b lp mi ju lr ls mj jx lu lv mk lx ly lz ml mb mc md mm mf mg mh im bi translated">让我们加载<a class="ae oe" href="https://www.openml.org/d/61" rel="noopener ugc nofollow" target="_blank">虹膜数据集</a>。它包含了三种不同种类的鸢尾花的尺寸。这些品种是海滨鸢尾、杂色鸢尾和刚毛鸢尾。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="of og l"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">(作者代码)</p></figure><p id="be88" class="pw-post-body-paragraph lm ln it lo b lp mi ju lr ls mj jx lu lv mk lx ly lz ml mb mc md mm mf mg mh im bi translated">让我们快速浏览一下数据集。</p><pre class="kj kk kl km gt nu nv nw nx aw ny bi"><span id="f9a4" class="nd kv it nv b gy nz oa l ob oc">print(iris_df.head())</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="nq nr di ns bf nt"><div class="gh gi oh"><img src="../Images/d6c2b58f905e61c3083d0ee182bfdd14.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*10X64ar2LRUm8NhO1KXcIg.png"/></div></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">(作者GIF)</p></figure><p id="e01a" class="pw-post-body-paragraph lm ln it lo b lp mi ju lr ls mj jx lu lv mk lx ly lz ml mb mc md mm mf mg mh im bi translated">我们可以创建一个所谓的“scree plot ”,看看哪些变量对数据的可变性最大。为此，我们执行第一个主成分分析。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="of og l"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">(作者代码)</p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="nq nr di ns bf nt"><div class="gh gi oi"><img src="../Images/49ce33c3c78e0676a78e94057ef84944.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*In4EueY2x-X5gOkRJHevxQ.png"/></div></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">(图片由作者提供)</p></figure><p id="2b63" class="pw-post-body-paragraph lm ln it lo b lp mi ju lr ls mj jx lu lv mk lx ly lz ml mb mc md mm mf mg mh im bi translated">正如我们所看到的，前两个部分解释了数据中的大部分可变性。因此，我决定只保留前两个组件，放弃其余的。确定要保留的元件数量后，我们可以运行第二个PCA，减少特征的数量。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="of og l"/></div></figure><p id="ce2b" class="pw-post-body-paragraph lm ln it lo b lp mi ju lr ls mj jx lu lv mk lx ly lz ml mb mc md mm mf mg mh im bi translated">我们看一下我们的数据，它现在是一个数组。</p><pre class="kj kk kl km gt nu nv nw nx aw ny bi"><span id="7412" class="nd kv it nv b gy nz oa l ob oc">print(iris_transformed[:5,:])</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="nq nr di ns bf nt"><div class="gh gi oj"><img src="../Images/f14387bbf69e453c5412ef5c2a11810f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5S6UBFXEvQUoWEnxqsVyFA.png"/></div></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">(图片由作者提供)</p></figure><p id="2063" class="pw-post-body-paragraph lm ln it lo b lp mi ju lr ls mj jx lu lv mk lx ly lz ml mb mc md mm mf mg mh im bi translated">我们可以看到，我们只剩下两列。这些列/变量是我们原始数据的线性组合，并不对应于原始数据集的特征(如萼片宽度、萼片长度等)。</p><p id="4a3b" class="pw-post-body-paragraph lm ln it lo b lp mi ju lr ls mj jx lu lv mk lx ly lz ml mb mc md mm mf mg mh im bi translated">让我们将数据可视化。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="of og l"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">(作者代码)</p></figure><p id="f1f1" class="pw-post-body-paragraph lm ln it lo b lp mi ju lr ls mj jx lu lv mk lx ly lz ml mb mc md mm mf mg mh im bi translated">我们会在x轴和y轴上看到新的组合特征。植物种类由数据点的颜色表示。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="nq nr di ns bf nt"><div class="gh gi ok"><img src="../Images/0546584ef1296ab63157e1d158c23412.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*me4ztczH5Pdr9K9_8R3lBQ.png"/></div></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">(图片由作者提供)</p></figure><p id="ab83" class="pw-post-body-paragraph lm ln it lo b lp mi ju lr ls mj jx lu lv mk lx ly lz ml mb mc md mm mf mg mh im bi translated">我们可以看到，数据中的许多信息都被保留了下来，我们现在可以训练一个ML模型，根据三个物种对数据点进行分类。</p><h1 id="94e5" class="ku kv it bd kw kx ky kz la lb lc ld le jz lf ka lg kc lh kd li kf lj kg lk ll bi translated">结论</h1><p id="af96" class="pw-post-body-paragraph lm ln it lo b lp lq ju lr ls lt jx lu lv lw lx ly lz ma mb mc md me mf mg mh im bi translated">我现在将总结最重要的概念。</p><h2 id="9db7" class="nd kv it bd kw ne nf dn la ng nh dp le lv ni nj lg lz nk nl li md nm nn lk no bi translated">矩阵乘法</h2><p id="79af" class="pw-post-body-paragraph lm ln it lo b lp lq ju lr ls lt jx lu lv lw lx ly lz ma mb mc md me mf mg mh im bi translated">当我们用一个向量乘一个矩阵时，向量是线性变换的。这种线性变换是旋转和缩放矢量的混合。只缩放而不旋转的向量称为特征向量。它们被缩放的因子是相应的特征值。</p><h2 id="750f" class="nd kv it bd kw ne nf dn la ng nh dp le lv ni nj lg lz nk nl li md nm nn lk no bi translated">主成分</h2><p id="3ed7" class="pw-post-body-paragraph lm ln it lo b lp lq ju lr ls lt jx lu lv lw lx ly lz ma mb mc md me mf mg mh im bi translated">主成分是我们的数据显示变化最大的轴。第一主成分解释了观察到的变化的最大部分，第二主成分解释了第二大部分，以此类推。主分量是协方差矩阵的特征向量。第一主分量对应于具有最大特征值的特征向量。</p><h2 id="9ef0" class="nd kv it bd kw ne nf dn la ng nh dp le lv ni nj lg lz nk nl li md nm nn lk no bi translated">主成分分析</h2><p id="eaba" class="pw-post-body-paragraph lm ln it lo b lp lq ju lr ls lt jx lu lv lw lx ly lz ma mb mc md me mf mg mh im bi translated">主成分分析是一种减少数据集中特征数量的技术。它由以下处理步骤组成。</p><ol class=""><li id="d620" class="mp mq it lo b lp mi ls mj lv mr lz ms md mt mh od mv mw mx bi translated">通过减去平均值并除以标准差来标准化数据</li><li id="fc50" class="mp mq it lo b lp my ls mz lv na lz nb md nc mh od mv mw mx bi translated">计算协方差矩阵。</li><li id="3907" class="mp mq it lo b lp my ls mz lv na lz nb md nc mh od mv mw mx bi translated">计算特征值和特征向量</li><li id="7070" class="mp mq it lo b lp my ls mz lv na lz nb md nc mh od mv mw mx bi translated">将特征向量合并成矩阵，并将其应用于数据。这将旋转和缩放数据。主要组件现在与我们的特征轴对齐。</li><li id="bf51" class="mp mq it lo b lp my ls mz lv na lz nb md nc mh od mv mw mx bi translated">保留我们指定的尽可能多的新特性，放弃其余的。我们保留能够解释数据中最大变化的特征。</li></ol><h1 id="dfd3" class="ku kv it bd kw kx ky kz la lb lc ld le jz lf ka lg kc lh kd li kf lj kg lk ll bi translated">来源</h1><p id="9765" class="pw-post-body-paragraph lm ln it lo b lp lq ju lr ls lt jx lu lv lw lx ly lz ma mb mc md me mf mg mh im bi translated"><a class="ae oe" href="https://datascienceplus.com/understanding-the-covariance-matrix/" rel="noopener ugc nofollow" target="_blank">https://datascienceplus . com/understanding-the-协方差矩阵/ </a></p><p id="4d73" class="pw-post-body-paragraph lm ln it lo b lp mi ju lr ls mj jx lu lv mk lx ly lz ml mb mc md mm mf mg mh im bi translated">https://en.wikipedia.org/wiki/Iris_flower_data_set<a class="ae oe" href="https://en.wikipedia.org/wiki/Iris_flower_data_set" rel="noopener ugc nofollow" target="_blank"/></p><p id="bd59" class="pw-post-body-paragraph lm ln it lo b lp mi ju lr ls mj jx lu lv mk lx ly lz ml mb mc md mm mf mg mh im bi translated"><a class="ae oe" href="https://scikit-learn.org/stable/auto_examples/decomposition/plot_pca_iris.html" rel="noopener ugc nofollow" target="_blank">https://scikit-learn . org/stable/auto _ examples/decomposition/plot _ PCA _ iris . html</a></p><h1 id="9988" class="ku kv it bd kw kx ky kz la lb lc ld le jz lf ka lg kc lh kd li kf lj kg lk ll bi translated">数据集</h1><h2 id="0617" class="nd kv it bd kw ne nf dn la ng nh dp le lv ni nj lg lz nk nl li md nm nn lk no bi translated">彩虹女神</h2><p id="0d17" class="pw-post-body-paragraph lm ln it lo b lp lq ju lr ls lt jx lu lv lw lx ly lz ma mb mc md me mf mg mh im bi translated">Iris数据集和许可证可在以下位置找到:</p><p id="9e83" class="pw-post-body-paragraph lm ln it lo b lp mi ju lr ls mj jx lu lv mk lx ly lz ml mb mc md mm mf mg mh im bi translated"><a class="ae oe" href="https://www.openml.org/d/61" rel="noopener ugc nofollow" target="_blank">https://www.openml.org/d/61</a></p><p id="7183" class="pw-post-body-paragraph lm ln it lo b lp mi ju lr ls mj jx lu lv mk lx ly lz ml mb mc md mm mf mg mh im bi translated">它是在<a class="ae oe" href="https://creativecommons.org/publicdomain/mark/1.0/" rel="noopener ugc nofollow" target="_blank"> creative commons </a>下许可的，这意味着你可以复制、修改、分发和执行作品，即使是出于商业目的，都无需征得许可。</p><h1 id="c258" class="ku kv it bd kw kx ky kz la lb lc ld le jz lf ka lg kc lh kd li kf lj kg lk ll bi translated">相关文章</h1><div class="ol om gp gr on oo"><a rel="noopener follow" target="_blank" href="/einstein-index-notation-d62d48795378"><div class="op ab fo"><div class="oq ab or cl cj os"><h2 class="bd iu gy z fp ot fr fs ou fu fw is bi translated">爱因斯坦指数符号</h2><div class="ov l"><h3 class="bd b gy z fp ot fr fs ou fu fw dk translated">爱因斯坦求和、指数符号和数值</h3></div><div class="ow l"><p class="bd b dl z fp ot fr fs ou fu fw dk translated">towardsdatascience.com</p></div></div><div class="ox l"><div class="oy l oz pa pb ox pc ko oo"/></div></div></a></div><div class="ol om gp gr on oo"><a rel="noopener follow" target="_blank" href="/backpropagation-in-neural-networks-6561e1268da8"><div class="op ab fo"><div class="oq ab or cl cj os"><h2 class="bd iu gy z fp ot fr fs ou fu fw is bi translated">神经网络中的反向传播</h2><div class="ov l"><h3 class="bd b gy z fp ot fr fs ou fu fw dk translated">从零开始的神经网络，包括数学和python代码</h3></div><div class="ow l"><p class="bd b dl z fp ot fr fs ou fu fw dk translated">towardsdatascience.com</p></div></div><div class="ox l"><div class="pd l oz pa pb ox pc ko oo"/></div></div></a></div><div class="ol om gp gr on oo"><a rel="noopener follow" target="_blank" href="/matrix-calculus-for-data-scientists-6f0990b9c222"><div class="op ab fo"><div class="oq ab or cl cj os"><h2 class="bd iu gy z fp ot fr fs ou fu fw is bi translated">数据科学家的矩阵演算</h2><div class="ov l"><h3 class="bd b gy z fp ot fr fs ou fu fw dk translated">吃红色药丸，学习矩阵微积分！</h3></div><div class="ow l"><p class="bd b dl z fp ot fr fs ou fu fw dk translated">towardsdatascience.com</p></div></div><div class="ox l"><div class="pe l oz pa pb ox pc ko oo"/></div></div></a></div><h1 id="2563" class="ku kv it bd kw kx ky kz la lb lc ld le jz lf ka lg kc lh kd li kf lj kg lk ll bi translated">作者撰写的其他文章</h1><div class="ol om gp gr on oo"><a rel="noopener follow" target="_blank" href="/how-you-can-use-gpt-j-9c4299dd8526"><div class="op ab fo"><div class="oq ab or cl cj os"><h2 class="bd iu gy z fp ot fr fs ou fu fw is bi translated">如何使用GPT J</h2><div class="ov l"><h3 class="bd b gy z fp ot fr fs ou fu fw dk translated">GPT J解释了3种简单的方法，你可以如何访问它</h3></div><div class="ow l"><p class="bd b dl z fp ot fr fs ou fu fw dk translated">towardsdatascience.com</p></div></div><div class="ox l"><div class="pf l oz pa pb ox pc ko oo"/></div></div></a></div><div class="ol om gp gr on oo"><a rel="noopener follow" target="_blank" href="/deep-q-learning-is-no-rocket-science-e34912f1864"><div class="op ab fo"><div class="oq ab or cl cj os"><h2 class="bd iu gy z fp ot fr fs ou fu fw is bi translated">深度Q学习不是火箭科学</h2><div class="ov l"><h3 class="bd b gy z fp ot fr fs ou fu fw dk translated">用pytorch解释和编码的深度Q和双Q学习</h3></div><div class="ow l"><p class="bd b dl z fp ot fr fs ou fu fw dk translated">towardsdatascience.com</p></div></div><div class="ox l"><div class="pg l oz pa pb ox pc ko oo"/></div></div></a></div><div class="ol om gp gr on oo"><a rel="noopener follow" target="_blank" href="/transfer-learning-3e9bb53549f6"><div class="op ab fo"><div class="oq ab or cl cj os"><h2 class="bd iu gy z fp ot fr fs ou fu fw is bi translated">深度迁移学习</h2><div class="ov l"><h3 class="bd b gy z fp ot fr fs ou fu fw dk translated">重用他人训练的模型的艺术</h3></div><div class="ow l"><p class="bd b dl z fp ot fr fs ou fu fw dk translated">towardsdatascience.com</p></div></div><div class="ox l"><div class="ph l oz pa pb ox pc ko oo"/></div></div></a></div><p id="8a49" class="pw-post-body-paragraph lm ln it lo b lp mi ju lr ls mj jx lu lv mk lx ly lz ml mb mc md mm mf mg mh im bi translated">想要连接吗？</p><p id="a5a2" class="pw-post-body-paragraph lm ln it lo b lp mi ju lr ls mj jx lu lv mk lx ly lz ml mb mc md mm mf mg mh im bi translated">领英<br/><a class="ae oe" href="https://www.linkedin.com/in/vincent-m%C3%BCller-6b3542214/" rel="noopener ugc nofollow" target="_blank">https://www.linkedin.com/in/vincent-m%C3%BCller-6b3542214/</a><br/>脸书<br/><a class="ae oe" href="https://www.facebook.com/profile.php?id=100072095823739" rel="noopener ugc nofollow" target="_blank">https://www.facebook.com/profile.php?id=100072095823739</a><br/>推特<br/><a class="ae oe" href="https://twitter.com/Vincent02770108" rel="noopener ugc nofollow" target="_blank">https://twitter.com/Vincent02770108</a><br/>中等<br/><a class="ae oe" href="https://medium.com/@Vincent.Mueller" rel="noopener">https://medium.com/@Vincent.Mueller</a><br/>你可以成为中等会员，同时支持我<br/><a class="ae oe" href="https://medium.com/@Vincent.Mueller/membership" rel="noopener">https://medium.com/@Vincent.Mueller/membership</a></p><h1 id="cafa" class="ku kv it bd kw kx ky kz la lb lc ld le jz lf ka lg kc lh kd li kf lj kg lk ll bi translated">数学附录</h1><h2 id="7927" class="nd kv it bd kw ne nf dn la ng nh dp le lv ni nj lg lz nk nl li md nm nn lk no bi translated">计算特征值和特征向量</h2><p id="e7d5" class="pw-post-body-paragraph lm ln it lo b lp lq ju lr ls lt jx lu lv lw lx ly lz ma mb mc md me mf mg mh im bi translated">如果你喜欢数学并想深入研究，我总结了这篇博文中用到的一些数学知识。</p><p id="c071" class="pw-post-body-paragraph lm ln it lo b lp mi ju lr ls mj jx lu lv mk lx ly lz ml mb mc md mm mf mg mh im bi translated">我们可以很容易地在python中计算特征向量和特征值。</p><pre class="kj kk kl km gt nu nv nw nx aw ny bi"><span id="d1c2" class="nd kv it nv b gy nz oa l ob oc">import numpy as np<br/>eigenvalues,eigenvectors = np.linalg.eig(M)</span></pre><p id="02f3" class="pw-post-body-paragraph lm ln it lo b lp mi ju lr ls mj jx lu lv mk lx ly lz ml mb mc md mm mf mg mh im bi translated">如果我们想手工计算它们，那就有点复杂了。</p><p id="fd78" class="pw-post-body-paragraph lm ln it lo b lp mi ju lr ls mj jx lu lv mk lx ly lz ml mb mc md mm mf mg mh im bi translated">正如我们所见，当我们将矩阵<strong class="lo iu"> M </strong>乘以一个特征向量(用<strong class="lo iu"> 𝑣 </strong>表示)时，这与缩放其特征值<strong class="lo iu"> 𝜆.是一样的</strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pi"><img src="../Images/7f0add389c5715a0420e5a78dab6c59a.png" data-original-src="https://miro.medium.com/v2/resize:fit:324/format:webp/1*1__TRQNHXn44iOPs3wOFIg.png"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">(图片由作者提供)</p></figure><p id="adbf" class="pw-post-body-paragraph lm ln it lo b lp mi ju lr ls mj jx lu lv mk lx ly lz ml mb mc md mm mf mg mh im bi translated">我们现在重新排列等式。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pj"><img src="../Images/e27c734edd086f935566b4ef255b0a59.png" data-original-src="https://miro.medium.com/v2/resize:fit:484/format:webp/1*ol96dofMx5DnCfj6Te39Sw.png"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">(图片由作者提供)</p></figure><p id="4755" class="pw-post-body-paragraph lm ln it lo b lp mi ju lr ls mj jx lu lv mk lx ly lz ml mb mc md mm mf mg mh im bi translated">其中<strong class="lo iu"> I </strong>是单位矩阵，对角线上有1，其他地方有0。它的形状和<strong class="lo iu"> A </strong>一样。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pk"><img src="../Images/722365816e4c464f8c0c75644d464544.png" data-original-src="https://miro.medium.com/v2/resize:fit:636/format:webp/1*oNMtgGcpAFbFr9DkPGQL0Q.png"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">(图片由作者提供)</p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pl"><img src="../Images/1672a560c66441ee4fd25f380af2ffe8.png" data-original-src="https://miro.medium.com/v2/resize:fit:258/format:webp/1*BUmhQakxbCEfJnPclmKZVw.png"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">(图片由作者提供)</p></figure><p id="67f8" class="pw-post-body-paragraph lm ln it lo b lp mi ju lr ls mj jx lu lv mk lx ly lz ml mb mc md mm mf mg mh im bi translated">这个等式成立的唯一方法。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pj"><img src="../Images/e27c734edd086f935566b4ef255b0a59.png" data-original-src="https://miro.medium.com/v2/resize:fit:484/format:webp/1*ol96dofMx5DnCfj6Te39Sw.png"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">(图片由作者提供)</p></figure><p id="814b" class="pw-post-body-paragraph lm ln it lo b lp mi ju lr ls mj jx lu lv mk lx ly lz ml mb mc md mm mf mg mh im bi translated">这只在矩阵的行列式(<strong class="lo iu">a</strong>-𝜆⋅<strong class="lo iu">I</strong>)<strong class="lo iu"/>变为0时成立。</p><blockquote class="pm pn po"><p id="7045" class="lm ln pp lo b lp mi ju lr ls mj jx lu pq mk lx ly pr ml mb mc ps mm mf mg mh im bi translated">矩阵的行列式是一个因子，在2×2矩阵的情况下，矩阵通过该因子缩放面积，在3×3矩阵的情况下缩放体积。如果行列式为零，那么矩阵(<strong class="lo iu">a</strong>-𝜆⋅<strong class="lo iu">I</strong>)<strong class="lo iu"/>挤压<strong class="lo iu"> </strong>指向原点(原点就是零点)。这是非零向量成为零向量的唯一方法。</p></blockquote><p id="e34d" class="pw-post-body-paragraph lm ln it lo b lp mi ju lr ls mj jx lu lv mk lx ly lz ml mb mc md mm mf mg mh im bi translated">所以我们寻找所有特征值𝜆，使行列式为0。</p><p id="6dbe" class="pw-post-body-paragraph lm ln it lo b lp mi ju lr ls mj jx lu lv mk lx ly lz ml mb mc md mm mf mg mh im bi translated">在我们找到了特征值之后，我们可以求解这个方程:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pj"><img src="../Images/e27c734edd086f935566b4ef255b0a59.png" data-original-src="https://miro.medium.com/v2/resize:fit:484/format:webp/1*ol96dofMx5DnCfj6Te39Sw.png"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">(图片由作者提供)</p></figure><p id="fe7e" class="pw-post-body-paragraph lm ln it lo b lp mi ju lr ls mj jx lu lv mk lx ly lz ml mb mc md mm mf mg mh im bi translated">我们找到了特征向量。</p><h2 id="f1d8" class="nd kv it bd kw ne nf dn la ng nh dp le lv ni nj lg lz nk nl li md nm nn lk no bi translated">协方差矩阵</h2><p id="d765" class="pw-post-body-paragraph lm ln it lo b lp lq ju lr ls lt jx lu lv lw lx ly lz ma mb mc md me mf mg mh im bi translated">方差-协方差矩阵可以使用以下公式根据数据进行估计:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pt"><img src="../Images/37fea99fe67ed8fa061e281d1329adff.png" data-original-src="https://miro.medium.com/v2/resize:fit:942/format:webp/1*AC6MHft5izwZYMB-4eDFDg.png"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">(图片由作者提供)</p></figure></div></div>    
</body>
</html>