<html>
<head>
<title>Scale-Equivariant CNNs</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">尺度等变CNN</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/sesn-cec766026179?source=collection_archive---------20-----------------------#2021-04-16">https://towardsdatascience.com/sesn-cec766026179?source=collection_archive---------20-----------------------#2021-04-16</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="6dd7" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">如何设计规模均衡的网络</h2></div><p id="a40d" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">注意:因为不可能有内嵌的公式，我们用一种杂乱的方式制作了这些公式。然而，我们注意到在一些设备上渲染失败。如果“a  ⁻、<em class="lb"> a </em> ⁻、<em class="lb"> a </em> ⁰、<em class="lb"> a </em>、<em class="lb">a</em><em class="lb">的指数不是从-2到2，那么我们推荐查看我们的</em> <a class="ae lc" href="https://spetrescu.github.io/sesn-reproducibility-project/unlisted/draft.pdf" rel="noopener ugc nofollow" target="_blank"> <em class="lb">草案</em> </a> <em class="lb">，乳胶制作。</em></p><p id="3dca" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">卷积神经网络(CNN)已经被证明是计算机视觉任务中的主导力量，并且具有广泛的应用，例如在图像和视频识别、图像分类、图像分割等方面。CNN旨在通过使用多个构建块(如卷积层、汇集层和全连接层)通过反向传播自动和自适应地学习要素的空间层次。CNN的一个有趣的性质是，通过设计，CNN是翻译等变的，即输入特征的翻译导致输出的等效翻译。作为这意味着什么的直观例子，如果CNN已经被训练来检测图像中猫的存在，由于其平移等方差属性，它将能够识别猫，而不管猫出现在图像中的什么位置。这是通过卷积层中神经元感受野的权重共享来实现的。因此，在下图中，如果猫的图像被移动，CNN仍然能够检测到猫。</p><figure class="le lf lg lh gt li gh gi paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="gh gi ld"><img src="../Images/120fd5da9e0506cd9b9fb5e7890a7e20.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*F9RunEj3KJbN_RQO--JjTQ.png"/></div></div><p class="lp lq gj gh gi lr ls bd b be z dk translated">细胞神经网络的翻译等变性质的例子；识别猫的能力，不管图像如何移动。【我<em class="lt">作者法师(安东尼奥·格拉万提的猫——</em>Shutterstock.com)<em class="lt"/></p></figure><p id="71bc" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在，当同样的物体以不同的比例出现在图像中会发生什么？比如猫会以不同的尺度出现会怎么样？普通的CNN还能认出这些猫吗？不幸的是，尽管细胞神经网络具有非常强大和有趣的特性，但它们并没有被设计成对输入的旋转和尺度变化都是等变的。这就成了一个问题，对吗？因为在现实生活应用中，这种类型的输入转换一直在发生，例如，想象一下由自动车辆处理的图像，所以无论行人可能出现的比例如何，都需要检测行人。</p><figure class="le lf lg lh gt li gh gi paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="gh gi lu"><img src="../Images/376e917b2996e72e5a9ffd6c910edc4f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Dl0Rk0CTxMb2UXcPkonJPA.png"/></div></div><p class="lp lq gj gh gi lr ls bd b be z dk translated">缩放对象的示例；CNN并不是天生被设计成在不同的尺度上识别相同的物体。【我<em class="lt">作者法师(安东尼奥·格拉万提的猫——</em>Shutterstock.com)<em class="lt"/></p></figure><p id="c80b" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">那么……如何解决这个问题呢？2019年，Ivan Sosnovik、michaszmaja和Arnold Smeulders提出了一个非常有趣的解决方案。他们的团队发表了一篇名为“尺度等变可控网络”的论文，解决了这个问题。他们不仅解决了这个问题，还设法提供了一个计算成本相当高的“普通”CNN解决方案，获得了MNIST和STL-10数据集的最新结果。因此，由于我们发现这篇论文非常有趣&amp;激动人心，在接下来的章节中，我们分析了他们的贡献，提供了他们方法背后的一些直觉，最后但同样重要的是，展示了我们复制他们结果的尝试。</p><h1 id="7214" class="lv lw iq bd lx ly lz ma mb mc md me mf jw mg jx mh jz mi ka mj kc mk kd ml mm bi translated"><strong class="ak">什么是尺度等方差？</strong></h1><p id="c247" class="pw-post-body-paragraph kf kg iq kh b ki mn jr kk kl mo ju kn ko mp kq kr ks mq ku kv kw mr ky kz la ij bi translated">如前所述，CNN在计算机视觉任务中表现出色的一个最重要的原因是卷积层是平移等变的。这意味着，如果我们将输入图像移动(<em class="lb"> x </em>'，<em class="lb"> y </em>')个像素，该层的输出也会移动。</p><figure class="le lf lg lh gt li gh gi paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="gh gi ms"><img src="../Images/2263737648988a88df8a64efcbb59b20.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XI8nzpAc3jnACHc5WXqvRg.png"/></div></div><p class="lp lq gj gh gi lr ls bd b be z dk translated">图CNNs翻译等方差属性的例子；识别猫的能力，不管图像如何移动。[图片由作者提供，灵感来自<a class="ae lc" href="https://www.researchgate.net/publication/312023127_Rotation_equivariant_vector_field_networks" rel="noopener ugc nofollow" target="_blank">来源</a>，猫来自<a class="ae lc" href="https://ai.stanford.edu/~acoates/stl10/" rel="noopener ugc nofollow" target="_blank"> STL-10 </a> ]</p></figure><p id="f093" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">等方差的一个特例是不变性。不变性意味着无论我们如何变换输入，输出都保持不变。CNN中从等方差到不变性的转变发生在池层。例如，如果一个3x3池块中的最大值在中心，输入移位1不会改变该块的输出。然而，一个重要的注意事项必须作出，池只是准不变的，等方差是有限的边缘效应在细胞神经网络。现在，让我们想象一下缩放图像也是如此。如果输入图像被放大/缩小，输出也应该被放大/缩小。如前所述，我们知道，默认情况下，卷积层没有这个属性。为了解决这个问题，必须定义等尺度变化层。等尺度变化层将能够以与常规卷积层响应输入偏移相同的方式响应尺度差异。规模等方差来源于一个数学概念:组等方差。粗略地说，组等变变换意味着如果层的输入由<em class="lb"> g </em>变换，则输出也由<em class="lb"> g </em>变换。而<em class="lb"> g </em>可以是任意同态，例如:平移、<a class="ae lc" href="https://arxiv.org/pdf/1602.07576.pdf" rel="noopener ugc nofollow" target="_blank">旋转</a>、<a class="ae lc" href="https://openreview.net/pdf?id=HJgpugrKPS" rel="noopener ugc nofollow" target="_blank">缩放</a>，或者这些的组合。通过设计<em class="lb">G</em>-等变层，我们可以以一种有意义的方式进一步增加重量分担。</p></div><div class="ab cl mt mu hu mv" role="separator"><span class="mw bw bk mx my mz"/><span class="mw bw bk mx my mz"/><span class="mw bw bk mx my"/></div><div class="ij ik il im in"><h1 id="18e2" class="lv lw iq bd lx ly na ma mb mc nb me mf jw nc jx mh jz nd ka mj kc ne kd ml mm bi translated">我们应该如何处理这个问题？</h1><h2 id="d61d" class="nf lw iq bd lx ng nh dn mb ni nj dp mf ko nk nl mh ks nm nn mj kw no np ml nq bi translated"><strong class="ak">数学细节</strong></h2><p id="e07d" class="pw-post-body-paragraph kf kg iq kh b ki mn jr kk kl mo ju kn ko mp kq kr ks mq ku kv kw mr ky kz la ij bi translated">读一次论文，我们看到GitHub库是公开的，我们很高兴我们可以窃取(我的意思是，重用)代码，并有一个免费的午餐…好吧，在这样做之前，让我们了解在数学和算法层面上发生了什么。<br/>论文作者为1维信号定义了一切，随后是臭名昭著的“推广到更高维情况是直截了当的”。我们认为直观的解释不一定是一维的，但我们承诺保持低维数。</p><p id="59ba" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">首先，让我们理解什么是方向可调滤波器:方向可调滤波器是一种内核，内核的尺度可以通过一个参数很容易地改变。论文给出的数学定义是:</p><figure class="le lf lg lh gt li gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/dac08a5e242285222358ea4a8da7b88e.png" data-original-src="https://miro.medium.com/v2/resize:fit:610/0*GO4ayn48KJpxz1Ge"/></div></figure><p id="02ba" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">内部的<em class="lb"> σ </em> ⁻是缩放滤波器，而外部的<em class="lb"> σ </em> ⁻是归一化滤波器。这样，我们可以通过参数<em class="lb"> σ </em>来重新调整任意的<em class="lb"> ψ </em> ( <em class="lb"> x </em>)滤波器。下图提供了一些直观的图像:</p><figure class="le lf lg lh gt li gh gi paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="gh gi ns"><img src="../Images/125c8bc3335674a4b3158b26eaeee684.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*PL-U5aALCbs-PPI4"/></div></div><p class="lp lq gj gh gi lr ls bd b be z dk translated">同样的<em class="lt"> ψ </em> (x，y)高斯2D滤波器，左边的<em class="lt"/>= 0.5，右边的<em class="lt"/>= 1。【我<em class="lt">法师作者】</em></p></figure><p id="4905" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们要了解的第二件事是规模翻译组。该组由<em class="lb"> H </em>表示，并被定义为缩放操作后跟随平移操作。它由两个子组组成，即<em class="lb"> S </em>和<em class="lb"> T </em>。</p><p id="bc4f" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">缩放组由<em class="lb"> S </em>表示。群组运算是缩放，其表示为乘以<em class="lb"> s </em>，元素<em class="lb"> s </em>的逆是<em class="lb"> s </em> ⁻ =1/s。单位元素是s * 1/s = 1 <br/>然而，<em class="lb"> s </em>被定义为离散的缩放群组(在哈尔积分中在数学上更易于管理)，由元素[<em class="lb">a</em>⁻、<em class="lb"> a </em> ⁻、1在那种情况下，<em class="lb">一</em> ⁿ的逆就是<em class="lb">一</em> ⁻ⁿ.</p><p id="1833" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">平移组由<em class="lb"> T </em>表示。分组操作明显是翻译，表现为添加<em class="lb"> t </em>。<em class="lb"> t </em>的元素的逆是- <em class="lb"> t </em>。单位元素是<em class="lb"> t </em> +(- <em class="lb"> t </em> )=0。平移组保持连续，而不是被离散化为1个像素的倍数，因为<em class="lb"> T </em>上的连续卷积在数学上被很好地定义(并且可以随后缩放)。</p><p id="b049" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">为了执行分组操作(即缩放或加法)，我们必须通过分组操作将分组元素应用于输入函数的变量(即分别乘以或加到<em class="lb"> x </em>)。这两组的一个很好的性质是它们的半直积很容易定义。直积是笛卡儿积的群论等价物；半直积只是直积的推广。这个操作可以想象成两个向量的外积。形式上的定义是<em class="lb"> H </em> ={( <em class="lb"> s </em>，<em class="lb"> t </em> ) ∣ <em class="lb"> s </em> ∈ <em class="lb"> S </em>，<em class="lb"> t </em> ∈ <em class="lb"> T </em> }，意思是变换<em class="lb"> h </em>是缩放<em class="lb"> s </em>后跟平移<em class="lb"> t </em>。分组操作是(<em class="lb"> s </em> ₂，<em class="lb"> t </em> ₂)⋅( <em class="lb"> s </em> ₁，<em class="lb">t</em>₁)=(<em class="lb">s</em>₂<em class="lb">s</em>₁，<em class="lb">s</em>₂<em class="lb">t</em>₁+<em class="lb">t</em>。变换方程(<em class="lb"> s </em> ₂，<em class="lb"> t </em> ₂)⁻ ⋅( <em class="lb"> s </em> ₁，<em class="lb">t</em>₁)=(<em class="lb">s</em>₂⁻<em class="lb">s</em>₁，<em class="lb">s</em>₂⁻(<em class="lb">t</em>₁-<em class="lb">t</em>833))我们可以求出逆元素，即(<em class="lb">s</em>833)单位元素是(1，0)。</p><p id="6f53" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">通过定义群<em class="lb"> H </em>，我们将寻找尺度等变卷积的问题转化为寻找群等变卷积的问题。我们把这个问题从一个具体问题转化为一个一般问题，希望能找到解决的办法。幸运的是，我们按照论文的<a class="ae lc" href="https://sv.20file.org/up1/1415_0.pdf" rel="noopener ugc nofollow" target="_blank">引文</a>找到了群等变卷积的定义:</p><figure class="le lf lg lh gt li gh gi paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="gh gi ns"><img src="../Images/56db8bcb392c7eb58cc137a4d25df0c3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*G4iWYfuzkcHkkEBR"/></div></div></figure><p id="0040" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">数学细节高得惊人，因为多维微积分和群论是了解正在发生的事情的最低要求。因此，我们不建议查找它们，但是，如果你选择这样做，我们发现<a class="ae lc" href="https://en.wikipedia.org/wiki/Haar_measure" rel="noopener ugc nofollow" target="_blank">这个</a>、<a class="ae lc" href="https://math.stackexchange.com/questions/494225/what-is-haar-measure" rel="noopener ugc nofollow" target="_blank">这个</a>和<a class="ae lc" href="https://projecteuclid.org/ebook/Download?urlid=10.1214%2Fcbms%2F1462061031&amp;isFullBook=False" rel="noopener ugc nofollow" target="_blank">这个</a>是一个很好的额外来源。<br/>在组等变卷积<em class="lb">f</em>(<em class="lb">g’</em>)中，表示我们的输入信号，其对应于图像，或后面层的尺度等变输入。<br/><em class="lb">lᵍ</em>[<em class="lb">ψ]</em>(<em class="lb">g '</em>)，(或<em class="lb">ψ</em>(<em class="lb">g</em>⁻<em class="lb">g '</em>)变换后)为滤波器，<em class="lb"> μ </em> ( <em class="lb"> g </em>')表示哈尔测度。经过一系列的数学变换，我们得出:</p><figure class="le lf lg lh gt li gh gi paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="gh gi ns"><img src="../Images/95d54692231859ceaef9b0b21f004401.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*JaM3XORypzjaoGzo"/></div></div></figure><p id="d19e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们还没有谈到的一件事是渠道。常规卷积层对输入通道求和，每个输入-输出通道对有不同的滤波器。然后，让我们的公式做完全相同的事情。在下面的等式中，<em class="lb"> C </em> ᵢₙ和<em class="lb"> C </em> ₒᵤₜ分别是输入和输出通道的数量。</p><figure class="le lf lg lh gt li gh gi paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="gh gi ns"><img src="../Images/392f465d8b1628b3e4d947b199771915.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*KzJ19CPU9lBdX2Gu"/></div></div></figure><h2 id="84cb" class="nf lw iq bd lx ng nh dn mb ni nj dp mf ko nk nl mh ks nm nn mj kw no np ml nq bi translated"><strong class="ak">算法</strong></h2><p id="5da4" class="pw-post-body-paragraph kf kg iq kh b ki mn jr kk kl mo ju kn ko mp kq kr ks mq ku kv kw mr ky kz la ij bi translated">前面提到的等式不能直接用代码实现。首先，在滤波器<em class="lb"> ψ </em>中，<em class="lb">S</em>-群是无限的；这需要进一步限制。设<em class="lb"> N </em> ₛ(后来也表示为<em class="lb"> S </em>)为这个极限，那么组<em class="lb"> S </em>就变成了【a，a⁻₁，…，a^ <em class="lb"> N </em> ₛ].其次，因为输入图像是在ℤ而不是在ℝ上定义的，所以组<em class="lb"> T </em>必须被离散化。</p><p id="6bc5" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">按照作者的选择，不是将内核中的每个像素定义为一个权重，而是将每个滤波器组成为一个完整基的线性组合。当以这种方式构造函数时，基的维数通常是无限的(例如泰勒级数或傅立叶级数)。因此，我们将算法限制为具有<em class="lb"> Nb </em>维度基础。这样，我们可以将网络的权重设置为线性组合的系数。数学上来说:ψ=∑ᵢ wᵢ ψᵢ，其中wᵢ是一个重量，ψᵢ是一个基本向量。在这个技巧之后，我们可以实现神奇的等变卷积。</p><p id="c907" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><em class="lb">注1 </em>:根据该论文，具有2D高斯包络的2D埃尔米特多项式的基足够好<br/> <em class="lb">注2: </em>这样，特别是在较大的滤波器尺寸下，加强了权重共享。例如，具有4阶hermite多项式的7×7卷积核只需要10个参数，而不是常规的49个。虽然我们无法从逻辑上推理这种重量共享是否有意义，但我们的直觉告诉我们，它有意义；这种额外的重量分担可能是与等比例变化层一起增加精度的另一个来源。</p><p id="8a33" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在，讨论实现，对于每个滤波器，我们有一个长度为<em class="lb"> Nb </em>(线性组合的系数)的权重向量，以及形状为[ <em class="lb"> Nb </em>，<em class="lb"> S </em>，<em class="lb"> V </em>，<em class="lb"> V </em> ]的预缩放滤波器基，其中<em class="lb"> Nb </em>是基数，<em class="lb"> S </em>是缩放数，<em class="lb"> V </em></p><p id="ef68" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">当我们有每一个<em class="lb"> C </em> ₒᵤₜ — <em class="lb"> C </em> ᵢₙ对时，如果权重张量的形状为[ <em class="lb"> C </em> ₒᵤₜ、<em class="lb"> C </em> ᵢₙ、<em class="lb"> Nb </em> ]，乘以形状的预先计算的基[ <em class="lb"> Nb </em>、<em class="lb"> S </em>、<em class="lb"> V </em>、<em class="lb"> V </em>，这可以有效地实现。对于乘法，我们在<em class="lb"> Nb </em>维度上求和。例如，通过使用以下函数:</p><pre class="le lf lg lh gt nt nu nv nw aw nx bi"><span id="78c7" class="nf lw iq nu b gy ny nz l oa ob">torch.einsum(‘ijk, klmn -&gt; ijlmn’), weights, bases)</span></pre><p id="8040" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">对于该操作的输出，我们将具有形状为<br/><em class="lb">c</em>ₒᵤₜ、<em class="lb"> C </em> ᵢₙ、<em class="lb"> S </em>、<em class="lb"> V </em>、<em class="lb"> V </em>的滤波器，记为κ。下图显示了这一点:</p><figure class="le lf lg lh gt li gh gi paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="gh gi oc"><img src="../Images/7371d05ae4764546eff1ae02c6dd5777.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*hKBYjeW3OPO1CFfw"/></div></div><p class="lp lq gj gh gi lr ls bd b be z dk translated">单个<em class="lt"> C </em> ₒᵤₜ→ <em class="lt"> C </em> ᵢₙ通道的滤波器基础的可视化。[图片改编自<a class="ae lc" href="https://openreview.net/pdf?id=HJgpugrKPS" rel="noopener ugc nofollow" target="_blank">论文</a></p></figure><p id="6fb0" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">使用尺度平移不变卷积的方程，我们可以定义两种情况。我们试图遵循作者的注释:当层的输入具有1的尺度维度(也称为“图像”)并且过滤器具有多个尺度维度时，我们称为<em class="lb"> T </em> → <em class="lb"> H </em>，并且当层的输入和过滤器都具有多个尺度维度时，我们称为<em class="lb"> H </em> → <em class="lb"> H </em>。</p><p id="73fc" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在<em class="lb"> T </em> → <em class="lb"> H </em>的情况下，提到了“S个简并子上的求和”。我们觉得这条线有点太模糊，因此，我们提供了另一种解释:对于尺度维度中的每个<em class="lb"> s </em>，我们在κ[:，:，<em class="lb"> s </em>，:，:]和输入图像之间执行常规卷积。卷积的结果存储为图像阵列，从而产生从组<em class="lb"> T </em>到<em class="lb"> H </em>的操作。为了利用已经优化的PyTorch库，可以用以下形式实现<em class="lb"> T </em> → <em class="lb"> H </em></p><pre class="le lf lg lh gt nt nu nv nw aw nx bi"><span id="2d61" class="nf lw iq nu b gy ny nz l oa ob">convTH(f, w, ψ) = squeeze(conv2d(f, expand(w × ψ)))</span></pre><p id="6458" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在这种情况下，输入κ滤波器从[ <em class="lb"> C </em> ₒᵤₜ、<em class="lb"> C </em> ᵢₙ、<em class="lb"> S </em>、<em class="lb"> V </em>、<em class="lb"> V </em> ]$的形状扩展为[ <em class="lb"> C </em> ₒᵤₜ <em class="lb"> S </em>、<em class="lb"> C </em> ᵢₙ、<em class="lb"> V </em>、<em class="lb"> V </em>。此后，将扩展的滤波器基与输入图像(具有[ <em class="lb"> C </em> ᵢₙ，<em class="lb"> U </em>，<em class="lb"> U </em> ]的形状，其中<em class="lb"> U </em>是图像的大小)进行卷积。这个卷积的输出产生了一个形状为[ <em class="lb"> C </em> ₒᵤₜ <em class="lb"> S </em>，<em class="lb"> U </em>，<em class="lb"> U </em> ]的张量，它压缩了维度[ <em class="lb"> C </em> ₒᵤₜ，<em class="lb"> S </em>，<em class="lb"> U </em>，<em class="lb"> U </em> ]。虽然我们已经从头实现了这一层，但是我们还没有使用它；由于时间限制，我们被迫选择另一种方法，以满足项目的最后期限。</p><p id="71f9" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><em class="lb">注1 </em>:对于展开κ的形状，文中定义了尺寸<br/> [ <em class="lb"> C </em> ₒᵤₜ，<em class="lb"> C </em> ᵢₙ <em class="lb"> S </em>，<em class="lb"> V </em>，<em class="lb"> V </em>而不是<em class="lb"> C </em> ₒᵤₜ <em class="lb"> S </em>，<em class="lb"> C我们认为作者在这里犯了一个相当痛苦的错别字。<br/> <em class="lb">注2 </em>:输入图像的大小为[ <em class="lb"> U </em>，<em class="lb"> U </em> ]，因为用于基准测试的数据集使用的是正方形图像。没有任何东西限制它具有[ <em class="lb"> U </em> ₁，<em class="lb"> U </em> ₂]的形状，但是我们决定遵循作者的符号。<br/> <em class="lb">注3 </em>:应用卷积时，输出并不总是具有[ <em class="lb"> U </em>，<em class="lb"> U </em> ]的大小，它以正常方式通过填充、内核大小和步幅进行修改。</em></p><figure class="le lf lg lh gt li gh gi paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="gh gi oc"><img src="../Images/5ddd7ffd43c3c031eac196b6614ac9f4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*GNpF1Tso3ibyEMER"/></div></div><p class="lp lq gj gh gi lr ls bd b be z dk translated">卷积的可视化<em class="lt"> T </em> → <em class="lt"> H </em>。为了简单起见，隐藏了空间组件。[图片改编自<a class="ae lc" href="https://openreview.net/pdf?id=HJgpugrKPS" rel="noopener ugc nofollow" target="_blank">论文</a></p></figure><p id="99cf" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><em class="lb"> H </em> → <em class="lb"> H </em>的情况可以想象成在<em class="lb"> x </em>、<em class="lb"> y </em>、<em class="lb"> s </em>方向做卷积，其中方向<em class="lb"> s </em>指的是尺度间相互作用。然而，由于这将进一步扩展数学表述，我们愿意认为我们已经为<em class="lb"> T </em> → <em class="lb"> H </em>提供了足够的直觉，以便人们能够自己理解<em class="lb"> H </em> → <em class="lb"> H </em>。</p></div><div class="ab cl mt mu hu mv" role="separator"><span class="mw bw bk mx my mz"/><span class="mw bw bk mx my mz"/><span class="mw bw bk mx my"/></div><div class="ij ik il im in"><h1 id="93af" class="lv lw iq bd lx ly na ma mb mc nb me mf jw nc jx mh jz nd ka mj kc ne kd ml mm bi translated">数据预处理和扩充</h1><p id="ad4a" class="pw-post-body-paragraph kf kg iq kh b ki mn jr kk kl mo ju kn ko mp kq kr ks mq ku kv kw mr ky kz la ij bi translated">关于手传实验，我们必须处理和扩充两个数据集的数据，即<a class="ae lc" href="http://yann.lecun.com/exdb/mnist/" rel="noopener ugc nofollow" target="_blank"> MNIST </a>和STL-10。对于每一个，正如本文中提到的，我们都遵循了特定的步骤。</p><h2 id="e0c0" class="nf lw iq bd lx ng nh dn mb ni nj dp mf ko nk nl mh ks nm nn mj kw no np ml nq bi translated"><strong class="ak"> MNIST </strong></h2><p id="cfac" class="pw-post-body-paragraph kf kg iq kh b ki mn jr kk kl mo ju kn ko mp kq kr ks mq ku kv kw mr ky kz la ij bi translated">我们使用0.3-1之间的均匀采样因子重新缩放了<a class="ae lc" href="http://yann.lecun.com/exdb/mnist/" rel="noopener ugc nofollow" target="_blank"> MNIST </a>图像，并用零填充图像以保持初始图像的分辨率。</p><figure class="le lf lg lh gt li gh gi paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="gh gi od"><img src="../Images/af303d8361b47a547141786170a31cfc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qbY4Ouh72GlPL4d0Qp2opQ.png"/></div></div><p class="lp lq gj gh gi lr ls bd b be z dk translated">重新缩放MNIST图像的示例。【我<em class="lt">法师作者】</em></p></figure><p id="8ad2" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">此外，我们生成了该数据集的6个实现。培训10.000，测评2.000，测试48.000。</p><h2 id="f24d" class="nf lw iq bd lx ng nh dn mb ni nj dp mf ko nk nl mh ks nm nn mj kw no np ml nq bi translated"><strong class="ak"> STL-10 </strong></h2><p id="c28a" class="pw-post-body-paragraph kf kg iq kh b ki mn jr kk kl mo ju kn ko mp kq kr ks mq ku kv kw mr ky kz la ij bi translated">类似地，对于<a class="ae lc" href="https://ai.stanford.edu/~acoates/stl10/" rel="noopener ugc nofollow" target="_blank"> STL-10 </a>数据集，我们按照论文的说明为实验准备数据。我们将图像归一化，减去每个通道的平均值，除以每个通道的标准偏差，通过应用12像素零填充和随机裁剪回96x96px尺寸来增强。此外，我们使用了水平随机翻转(50%的概率)和32像素的1孔剪切。</p><figure class="le lf lg lh gt li gh gi paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="gh gi oe"><img src="../Images/09e82e906f28151fd7f4b72c4abe10bb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kw1evPtn3PeUUcPCW1eiRw.png"/></div></div><p class="lp lq gj gh gi lr ls bd b be z dk translated">STL-10图象的数据处理。【我<em class="lt">法师作者】</em></p></figure><p id="a056" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">运行<a class="ae lc" href="https://github.com/vioSpark/reproduction-project-DL2021" rel="noopener ugc nofollow" target="_blank">代码</a>，我们能够为MNIST数据集复制实验。虽然STL-10的代码是可用的，但我们无法复制结果，因为我们没有足够的计算资源来进行实验。在复制了修改后的MNIST数据集(在上一节中介绍)后，我们能够重现结果。这些有点类似，差别不大。</p><p id="348e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">对于MNIST数据集，我们在以下模型上运行了实验:<br/>mnist _ ses _ scalar _ 28<br/>mnist _ ses _ scalar _ 56<br/>mnist _ ses _ vector _ 28<br/>mnist _ ses _ vector _ 56<br/>mnist _ ses _ scalar _ 28p<br/>mnist _ ses _ scalar _ 56p<br/>mnist _ ses _ vector _ 28p<br/>mnist _ ses _ vector _ 56p</p><p id="de96" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">复制修改后的MNIST数据集后，我们能够重现结果。对于上一节底部提到的每个模型，进行了两个实验:一个实验的比例因子设置为1，另一个实验的比例因子设置为0.5。因此，对于每个模型，我们获得了12个结果(每个实现6个)。结果存储在“results.yml”中，并使用我们编写的python脚本进行进一步处理。我们复制了与作者相似的结果。结果显示在下表中。</p><figure class="le lf lg lh gt li gh gi paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="gh gi of"><img src="../Images/ec97b7cb09f0b18fb02c9404efcc28ed.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*k-puVSkDvL6EYjUruYQo9A.png"/></div></div><p class="lp lq gj gh gi lr ls bd b be z dk translated">MNIST数据集的重复结果。“+”表示缩放数据扩充。</p></figure><p id="bcae" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">作者定义的STL10模型有11M个参数。我们试图以更小的批量运行这些模型(以适应我们12GB的GPU限制)，但是为了重现有意义的结果，模型训练得太慢了。后来，这些没有包括在内。</p></div><div class="ab cl mt mu hu mv" role="separator"><span class="mw bw bk mx my mz"/><span class="mw bw bk mx my mz"/><span class="mw bw bk mx my"/></div><div class="ij ik il im in"><h1 id="c9ae" class="lv lw iq bd lx ly na ma mb mc nb me mf jw nc jx mh jz nd ka mj kc ne kd ml mm bi translated">结论</h1><p id="5335" class="pw-post-body-paragraph kf kg iq kh b ki mn jr kk kl mo ju kn ko mp kq kr ks mq ku kv kw mr ky kz la ij bi translated">通过阅读本文，我们希望你现在能更好地理解什么是等尺度方差，以及为什么设计等尺度方差CNN是有价值的。虽然乍一看，这篇论文看起来真的“数学化”，但我们可以说，在花了无数时间在维基百科的文章上试图扩展我们在群论方面的知识后，我们有点理解作者的意图了。我们认为<em class="lb">G</em>–等变卷积(如scale–等变)是细胞神经网络的一个有前途的方向，因为它们的适用性是不可否认的。总之，对于这个项目，我们重现了数据扩充步骤，重新运行了MNIST实验，并产生了准等效的结果。我们试图运行STL–10实验，但是计算资源的缺乏战胜了我们。我们试图实现这些层本身，尽管我们无法测试它，我们相信我们已经成功地实现了T→H层，但是剩下的时间不够了。</p><h2 id="1076" class="nf lw iq bd lx ng nh dn mb ni nj dp mf ko nk nl mh ks nm nn mj kw no np ml nq bi translated">感谢</h2><p id="89f4" class="pw-post-body-paragraph kf kg iq kh b ki mn jr kk kl mo ju kn ko mp kq kr ks mq ku kv kw mr ky kz la ij bi translated">这个博客是在代尔夫特大学CS4240深度学习课程的背景下创建的，作为一个小组项目。<br/>我们要感谢讷计斯·托曼和托马什·莫蒂卡的建议和有益的见解。</p><p id="f180" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">有用链接:<br/>链接到<a class="ae lc" href="https://arxiv.org/abs/1910.11093" rel="noopener ugc nofollow" target="_blank">原创论文</a> <br/>链接到这个<a class="ae lc" href="https://spetrescu.github.io/sesn-reproducibility-project/" rel="noopener ugc nofollow" target="_blank">项目的网站</a> <br/>链接到<a class="ae lc" href="https://github.com/vioSpark/reproduction-project-DL2021" rel="noopener ugc nofollow" target="_blank">我们的<em class="lb"> GitHub </em>资源库</a></p><p id="fdd4" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><em class="lb">作者:</em> <strong class="kh ir"> <br/>马克·卢卡奇</strong> <a class="ae lc" href="https://github.com/vioSpark" rel="noopener ugc nofollow" target="_blank"> <em class="lb">吉图布</em> </a> <strong class="kh ir"> <br/>斯蒂芬·彼得雷斯库</strong> <a class="ae lc" href="https://github.com/spetrescu" rel="noopener ugc nofollow" target="_blank"> <em class="lb">吉图布</em> </a></p><p id="d39e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir"> <em class="lb">如果你喜欢这篇文章，别忘了分享！</em>T3】</strong></p></div></div>    
</body>
</html>