<html>
<head>
<title>TPU Training</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">TPU培训</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/tpu-training-6eb84100d138?source=collection_archive---------16-----------------------#2021-10-12">https://towardsdatascience.com/tpu-training-6eb84100d138?source=collection_archive---------16-----------------------#2021-10-12</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="3f62" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsdatascience.com/tagged/thoughts-and-theory" rel="noopener" target="_blank">思想和理论</a></h2><div class=""/><div class=""><h2 id="c9b3" class="pw-subtitle-paragraph jw iz iq bd b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn dk translated">利用专用DNN训练芯片的力量</h2></div><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ko"><img src="../Images/57e8b551ed8d7c54437ae4798a562f61.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*vWgdoC_U2V3_O6YX"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">由<a class="ae le" href="https://unsplash.com/@fellowferdi?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">费迪南·斯托尔</a>在<a class="ae le" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="6620" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">过去十年深度学习成功背后的驱动力之一是图形处理单元(GPU)提供的巨大计算能力。虽然最初是为将图像渲染到显示设备而设计的，但它们的高度并行结构使训练速度提高了几个数量级。随着时间的推移，GPU得到了增强，以满足DNN培训不断增长的需求。今天，它们是训练大规模人工智能的主要方法。然而，在过去的几年中，潜在的挑战者已经以专门为训练DNNs设计的新芯片的形式出现。这些芯片(或ASIC——专用集成电路——更恰当的称呼)可能会以很小的成本实现加速训练。虽然市场上已经有许多专门的人工智能ASICs(例如，见这里的<a class="ae le" href="https://blog.inten.to/hardware-for-deep-learning-part-4-asic-96a542fe6a81" rel="noopener ugc nofollow" target="_blank"/>)和许多其他的正在形成中(例如，见这里的<a class="ae le" href="https://research.ibm.com/blog/ai-chip-precision-scaling" rel="noopener ugc nofollow" target="_blank"/>)，但在撰写本文时，只有少数几个通过云服务提供给公众。这种情况有望在不久的将来发生改变，例如Habana Gaudi即将在AWS上发布<a class="ae le" href="https://aws.amazon.com/ec2/instance-types/habana-gaudi/" rel="noopener ugc nofollow" target="_blank">以及备受期待的</a><a class="ae le" href="https://aws.amazon.com/machine-learning/trainium/" rel="noopener ugc nofollow" target="_blank"> AWS Trainium </a>。</p><p id="2e00" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">对于现代机器学习开发团队来说，要保持竞争力，他们需要不断掌握新进展的脉搏。这包括在评估新的人工智能芯片及其在自己项目中的潜在应用时发展高水平的熟练程度。不幸的是，让你的训练工作量适应新的芯片有时会很有挑战性。这篇博文的目的是为如何应对这一挑战提供一点指导，同时也提供一些情感上的支持。这篇文章并不是要取代在线的官方文档和教程。</p><p id="85e8" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">我要感谢<a class="ae le" href="https://www.linkedin.com/in/allen-wang-011795a4/" rel="noopener ugc nofollow" target="_blank"> Allen Wang </a>和<a class="ae le" href="https://www.linkedin.com/in/yitzhak-levi-49a217201/" rel="noopener ugc nofollow" target="_blank"> Yitzhak Levi </a>对这篇文章的贡献。</p><h1 id="919e" class="mb mc iq bd md me mf mg mh mi mj mk ml kf mm kg mn ki mo kj mp kl mq km mr ms bi translated">序幕</h1><p id="9a0f" class="pw-post-body-paragraph lf lg iq lh b li mt ka lk ll mu kd ln lo mv lq lr ls mw lu lv lw mx ly lz ma ij bi translated">在本帖中，我们建议将让你的模型适应新的人工智能芯片的挑战分解为四个步骤:</p><ol class=""><li id="5d00" class="my mz iq lh b li lj ll lm lo na ls nb lw nc ma nd ne nf ng bi translated"><strong class="lh ja">高级兼容性分析</strong>:对您的工作负载特性是否符合芯片规格进行早期评估。</li><li id="55a4" class="my mz iq lh b li nh ll ni lo nj ls nk lw nl ma nd ne nf ng bi translated"><strong class="lh ja">调整你的模型在新的芯片上运行</strong>:你可能需要对你的模型做一些调整，比如替换专用AI芯片不支持的操作。</li><li id="1194" class="my mz iq lh b li nh ll ni lo nj ls nk lw nl ma nd ne nf ng bi translated"><strong class="lh ja">优化新芯片的运行时性能</strong>:有趣的事情开始了……为了充分利用芯片，您需要分析并最大限度地提高其利用率。</li><li id="2715" class="my mz iq lh b li nh ll ni lo nj ls nk lw nl ma nd ne nf ng bi translated"><strong class="lh ja">(重新)调整模型以收敛</strong>:先前步骤中所需的更改(例如增加训练批量)可能需要调整模型超参数(例如学习率)，以确保及时收敛。</li></ol><p id="e2b1" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">显然，这种分类是一种简化。实际上，您可能会发现自己在迭代地和/或并行地执行这些步骤。例如，您可能决定将优化运行时的任务交给性能分析专家，而另一组数据科学家则致力于调整您的学习算法，以收敛于大批量训练(例如，在GPU上使用<a class="ae le" rel="noopener" target="_blank" href="/how-to-use-horovods-large-batch-simulation-to-optimize-hyperparameter-tuning-for-highly-a815c4ab1d34">大批量模拟</a>以最大限度地降低成本)。</p><p id="8748" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">在接下来的几节中，我们将通过将这四个步骤应用到<a class="ae le" href="https://cloud.google.com/tpu" rel="noopener ugc nofollow" target="_blank">谷歌云TPU </a> DNN加速器来更详细地演示它们。更具体地说，我们将讨论当您尝试使用TensorFlow版转换您的模型以在云TPU v3–8(包含8个TPU内核)上运行时可能面临的一些挑战。虽然我们是在特定的人工智能芯片和特定的训练框架上进行分区，但我们讨论的许多考虑因素也适用于其他人工智能芯片和其他训练框架。</p><p id="63ef" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">你可以在计算引擎上启动云TPU(如这里描述的<a class="ae le" href="https://cloud.google.com/tpu/docs/quickstart" rel="noopener ugc nofollow" target="_blank"/>)或者使用托管服务，如<a class="ae le" href="https://cloud.google.com/ai-platform/training/docs/using-tpus" rel="noopener ugc nofollow" target="_blank">人工智能平台</a>。对于本帖中描述的步骤，我们强烈建议在计算引擎上运行云TPU。这将使调试和分析性能更加灵活。在撰写本文时，托管API不提供对TPU系统日志和TPU系统性能的相同可见性，不支持您<a class="ae le" href="https://cloud.google.com/tpu/docs/cloud-tpu-tools" rel="noopener ugc nofollow" target="_blank">捕获性能配置文件</a>，也不支持使用<a class="ae le" href="https://www.tensorflow.org/api_docs/python/tf/data/experimental/service" rel="noopener ugc nofollow" target="_blank"> tf.data.service </a>卸载数据预处理。启动您的TPU时，请务必仔细按照说明进行操作，因为有一些细微之处是TPU设置所独有的(例如<a class="ae le" href="https://cloud.google.com/tpu/docs/storage-buckets#finegrained-access" rel="noopener ugc nofollow" target="_blank"> TPU专用服务账户</a>)。</p><p id="40d9" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">重要的是要记住，DNN的发展前景仍然极具活力。当你读到这篇文章时，我们提出的一些观点可能已经过时了。请确保关注新版本和新工具的发布，并确保根据最新可用信息做出设计决策。</p><p id="34fa" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">请原谅你可能遇到的任何不准确之处，或者更好的是，给我写信，告诉我你的更正。</p><h1 id="20cd" class="mb mc iq bd md me mf mg mh mi mj mk ml kf mm kg mn ki mo kj mp kl mq km mr ms bi translated">步骤1 —高级兼容性评估</h1><p id="8c5d" class="pw-post-body-paragraph lf lg iq lh b li mt ka lk ll mu kd ln lo mv lq lr ls mw lu lv lw mx ly lz ma ij bi translated">自然，你想做的第一件事是尝试开发一个感兴趣的人工智能芯片是否与你的用例相关的早期评估。让你的模型适应新芯片的努力可能是巨大的，你越早排除某个死胡同越好。这种初始评估通常可以根据在线资源得出，包括系统规格和性能指标评测:</p><h2 id="c1a7" class="nm mc iq bd md nn no dn mh np nq dp ml lo nr ns mn ls nt nu mp lw nv nw mr iw bi translated">ASIC描述</h2><p id="dcdf" class="pw-post-body-paragraph lf lg iq lh b li mt ka lk ll mu kd ln lo mv lq lr ls mw lu lv lw mx ly lz ma ij bi translated">一个好的起点是专用硬件的公开描述。这通常包括训练芯片的功能:支持什么模型层和架构、使用什么浮点类型、需要什么软件堆栈、芯片如何与CPU和数据存储接口、计算核心是否以及如何互连、硬件扩展到多核训练的程度等。此描述可用于识别与您的模型的潜在不兼容性。您可能会发现可用内存不能满足您的模型大小的需要，或者不支持您的训练可扩展性需要。在这种情况下，不要再往前走了。</p><p id="daab" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated"><a class="ae le" href="https://cloud.google.com/tpu/docs/tpus" rel="noopener ugc nofollow" target="_blank">云TPU文档</a>包含关于使用TPU的大量信息，包括<a class="ae le" href="https://cloud.google.com/tpu/docs/tpus#programming_model" rel="noopener ugc nofollow" target="_blank"> TPU编程模型</a>和最适合TPU的<a class="ae le" href="https://cloud.google.com/tpu/docs/tpus#when_to_use_tpus" rel="noopener ugc nofollow" target="_blank">工作负载类型</a>。从这些资源中，您可能会得出这样的结论:TPU不适合您的模型，例如，因为它们依赖于自定义ops、高精度算术或大量的元素操作。</p><h2 id="2dec" class="nm mc iq bd md nn no dn mh np nq dp ml lo nr ns mn ls nt nu mp lw nv nw mr iw bi translated">当心基准</h2><p id="75d1" class="pw-post-body-paragraph lf lg iq lh b li mt ka lk ll mu kd ln lo mv lq lr ls mw lu lv lw mx ly lz ma ij bi translated">您可能想了解的另一个资源是在线性能基准比较。在各种常见架构(包括ResNet、MaskRCNN、Bert等)上，您不难找到TPU与GPU的性能对比。不幸的是，弄清楚如何将这些结果应用到您自己的用例中是非常困难的。</p><p id="21dd" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">首先，不言而喻的是，芯片制造商提供的基准应该以合理的怀疑态度对待。但是，即使是你认为公正的分析也可能极难解读。在某种程度上，这可以说是任何学科中的基准比较，但在深度学习领域尤其如此，因为有大量因素可以对性能产生有意义的影响。首先是运行时环境CPU内核的数量及其类型、操作系统、驱动程序版本、操作系统、软件框架类型和版本——这些单独的元素中的每一个都可以单独影响性能几十个百分点。然后是模型:即使您自己的模型体系结构与基准测试中最相似的模型之间存在最微小的差异，无论是在图形体系结构、输入数据格式、预处理管道、损失函数还是优化器方面，都会对性能产生巨大的影响。质量基准比较将包括有关已进行评估的精确属性的详细信息。但是，它们不太可能涵盖影响性能的所有参数。</p><p id="95f3" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">MLPerf  : <a class="ae le" href="https://mlcommons.org/en/" rel="noopener ugc nofollow" target="_blank"> MLPerf </a>是一个经常被引用的人工智能训练和推理的基准套件，目前由<a class="ae le" href="https://mlcommons.org/en/" rel="noopener ugc nofollow" target="_blank"> MLCommons财团</a>管理。本白皮书<a class="ae le" href="https://arxiv.org/pdf/1910.01500.pdf" rel="noopener ugc nofollow" target="_blank">详细介绍了创建培训基准的理由以及参与规则。从对</a><a class="ae le" href="https://mlcommons.org/en/training-normal-10/" rel="noopener ugc nofollow" target="_blank">基准测试结果</a>的粗略回顾中可以明显看出，该基准测试影响深远，涵盖了广泛的培训环境和模型架构。然而，如上所述，鉴于每个测试用例有许多不同的参数，您可能会发现很难推断出AI芯片之间的清晰比较，更不用说将这些结果应用到您自己的特定用例中了。我发现这些结果需要进行解释(例如参见<a class="ae le" href="https://venturebeat.com/2020/07/29/google-claims-its-new-tpus-are-2-7-times-faster-than-the-previous-generation/" rel="noopener ugc nofollow" target="_blank">这篇</a>综述)，尤其是考虑到ASICs之间的潜在价格差异时(这些差异不包括在原始比较结果中)。</p><p id="3ea2" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">例如，MLPerf基准测试的所有迹象都让人相信8个NVIDIA A100 GPU核心将远远超过TPU v3–8(包含8个TPU核心)。然而，最近我们在一个模型上工作，与评论中涉及的模型没有什么不同，在这个模型上，TPU运行实际上匹配，甚至略优于我们最知名的A100 GPU运行配置。这个模型似乎完全符合TPU的规格。与此同时，对这个模型的小改动会极大地改变它的TPU兼容性，并大大增加步进时间。MLPerf报告没有给出这两种极端结果的任何指示。</p><p id="92c3" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">虽然我们认识到人工智能培训基准评估的重要性和价值，但我们认为承认它们在预测评估中包含的特定测试案例之外的性能方面的局限性也很重要。</p><h2 id="53d8" class="nm mc iq bd md nn no dn mh np nq dp ml lo nr ns mn ls nt nu mp lw nv nw mr iw bi translated">系统架构规范</h2><p id="ded6" class="pw-post-body-paragraph lf lg iq lh b li mt ka lk ll mu kd ln lo mv lq lr ls mw lu lv lw mx ly lz ma ij bi translated">如果完整的系统规格可用，您还有一个选择，就是通过对您的模型在专用硬件上的运行方式进行离线模拟，来尝试规划您的培训步骤的运行时间。这种分析需要对系统架构和DNN模型有深入的了解。我认识许多人，他们通过创建基于参数矩阵大小、触发器数量、内存大小、L2缓存延迟等精确性能预测的电子表格，过上了不错的生活。根据我的经验，这种预测往往是“命中或错过”，在复杂的机器学习工作负载的情况下，更多的时候是“错过”。</p><h2 id="a448" class="nm mc iq bd md nn no dn mh np nq dp ml lo nr ns mn ls nt nu mp lw nv nw mr iw bi translated">跳进深水区</h2><p id="a40c" class="pw-post-body-paragraph lf lg iq lh b li mt ka lk ll mu kd ln lo mv lq lr ls mw lu lv lw mx ly lz ma ij bi translated">当该说的都说了，该做的都做了，除了堕落和肮脏，真的别无选择。虽然我承认有些人可能厌倦了走在潜在的死胡同的道路上，但我相信，即使您最终没有在TPU上训练您当前的模型，随着您的项目的发展和更多ASICs的出现，您在这个过程中发展的专业知识几乎肯定会很好地服务于您和您的团队。</p><h1 id="3e4f" class="mb mc iq bd md me mf mg mh mi mj mk ml kf mm kg mn ki mo kj mp kl mq km mr ms bi translated">第二步——调整你的模型以在TPU上运行</h1><p id="5e52" class="pw-post-body-paragraph lf lg iq lh b li mt ka lk ll mu kd ln lo mv lq lr ls mw lu lv lw mx ly lz ma ij bi translated">为了在定制的ASIC上成功运行，您可能需要对您的AI应用程序进行更改。需要改变的程度将取决于许多因素，包括ASIC软件堆栈的成熟度和支持的操作范围。定制ASICs可能会对AI SW开发平台或版本施加严格限制，这可能需要进行重大调整。您应该始终努力使用最新的软件包，因为这些包可能包含最广泛和最佳的API支持。TPU享有相对健康的软件堆栈和大型开发人员社区的优势(这两者通常是相互关联的)。然而，它们可能仍然需要适应您的数据输入管道和计算图。我们将在下面的小节中演示几个例子。首先，我们将指出定制ASICs调试中潜在的复杂性。</p><h2 id="4829" class="nm mc iq bd md nn no dn mh np nq dp ml lo nr ns mn ls nt nu mp lw nv nw mr iw bi translated">为TPU调试</h2><p id="7c2f" class="pw-post-body-paragraph lf lg iq lh b li mt ka lk ll mu kd ln lo mv lq lr ls mw lu lv lw mx ly lz ma ij bi translated">GPU培训的一个显著优势是，模型设计和调试的重要部分可以在CPU上执行。如果你能在CPU上编译你的模型，那么100次中有99次你的模型在GPU上是有效的(有足够的内存)。这是现代人工智能开发人员经常想当然的事情。定制ASICs不一定是这种情况。这样做的结果是，通常需要直接在定制ASIC上进行调试，这可能会影响成本和/或持续时间。或者，ASIC制造商可能会提供一个模拟框架，用于识别和修复可能在CPU上运行的潜在问题。理想情况下，这种模拟框架还将提供如何最大化ASIC利用率的指导。不幸的是，在撰写本文时，官方的TPU模拟框架还不存在。虽然可用的<a class="ae le" href="https://cloud.google.com/tpu/docs/tensorflow-ops" rel="noopener ugc nofollow" target="_blank">文档</a>和<a class="ae le" href="https://cloud.google.com/tpu/docs/cloud-tpu-tools#tpu_compatibility_graph" rel="noopener ugc nofollow" target="_blank">工具</a>可能有助于构建一个TPU兼容的模型，但直到你在TPU上运行它，你才能确定。</p><p id="9b3f" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">您可能面临的另一个困难是理解TPU报告的错误消息。正如我们在过去的的<a class="ae le" rel="noopener" target="_blank" href="/debugging-in-tensorflow-392b193d0b8">中提到的，破译TensorFlow错误信息可能很难。TPU上的错误消息，无论是那些报告给控制台的，还是那些通过</a><a class="ae le" href="https://cloud.google.com/monitoring" rel="noopener ugc nofollow" target="_blank">云监控</a>可访问的，往往都特别隐晦(截至本文撰写之时)。我们将在下面的章节中提供一些例子。</p><h2 id="bffb" class="nm mc iq bd md nn no dn mh np nq dp ml lo nr ns mn ls nt nu mp lw nv nw mr iw bi translated">更新您的数据输入管道</h2><p id="10b9" class="pw-post-body-paragraph lf lg iq lh b li mt ka lk ll mu kd ln lo mv lq lr ls mw lu lv lw mx ly lz ma ij bi translated">尽管数据输入管道在TPU的主机CPU上运行，而不是在TPU本身上运行，TPU <a class="ae le" href="https://cloud.google.com/tpu/docs/system-architecture-tpu-vm" rel="noopener ugc nofollow" target="_blank">系统架构</a>对可以执行的操作施加了某些限制。启动TPU的<a class="ae le" href="https://cloud.google.com/tpu/docs/system-architecture-tpu-vm#tpu-node" rel="noopener ugc nofollow" target="_blank">标准方式</a>依赖于通过<a class="ae le" href="https://grpc.io/" rel="noopener ugc nofollow" target="_blank"> gRPC </a>与TPU主机通信的专用虚拟机。这种架构的后果之一是不允许任何<a class="ae le" href="https://www.tensorflow.org/guide/create_op" rel="noopener ugc nofollow" target="_blank">自定义操作符</a>或任何基于python的数据处理函数。这禁止使用<a class="ae le" href="https://www.tensorflow.org/api_docs/python/tf/py_function" rel="noopener ugc nofollow" target="_blank"> tf.py_function </a>和<a class="ae le" href="https://www.tensorflow.org/api_docs/python/tf/numpy_function" rel="noopener ugc nofollow" target="_blank"> tf.numpy_function </a>，它们通常用于绕过本机TensorFlow API施加的限制。它还禁止使用<a class="ae le" href="https://www.tensorflow.org/api_docs/python/tf/data/Dataset#from_generator" rel="noopener ugc nofollow" target="_blank">TF . data . dataset . from _ generator</a>API，该API通常用于增加输入数据集创建的灵活性。</p><p id="de5c" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">不幸的是，在撰写本文时，如果您的输入管道图无效，您很可能会得到一个模糊的gRPC错误消息，就像下面的块中的消息一样，您将不得不自己寻找罪魁祸首。</p><pre class="kp kq kr ks gt nx ny nz oa aw ob bi"><span id="0745" class="nm mc iq ny b gy oc od l oe of">W ./tensorflow/core/distributed_runtime/eager/destroy_tensor_handle_node.h:57] Ignoring an error encountered when deleting remote tensors handles: Invalid argument: Unable to find the relevant tensor remote_handle: Op ID: 22643, Output num: 7</span><span id="7026" class="nm mc iq ny b gy og od l oe of">Additional GRPC error information from remote target /job:worker/replica:0/task:0:</span><span id="1004" class="nm mc iq ny b gy og od l oe of">:{"created":"@1632916556.697142899","description":"Error received from peer ipv4:10.11.250.138:8470","file":"external/com_github_grpc_grpc/src/core/lib/surface/call.cc","file_line":1056,"grpc_message":"Unable to find the relevant tensor remote_handle: Op ID: 22643, Output num: 7","grpc_status":3}</span></pre><p id="ecf8" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">TPU常见的另一种输入管道错误与数据输入的大小有关，它会导致同样不明确的消息。正如我们将在下面看到的，有效利用TPU可能需要比您以前习惯的更大的批量。如果没有在输入数据管道上正确处理，您可能会遇到CPU内存问题。例如，如果每个输入样本是1 MB，而您的全局批处理大小是4096，那么您的批处理大小将是4 GB。解决这个问题的一个方法是修改您的数据管道以生成“本地”批处理而不是“全局”批处理；也就是说，创建每个内核批量大小的批。这可以使用<a class="ae le" href="https://www.tensorflow.org/api_docs/python/tf/distribute/Strategy#distribute_datasets_from_function" rel="noopener ugc nofollow" target="_blank">TF . distribute . strategy . distribute _ datasets _ from _ function</a>API来完成。在我们的示例中，批量大小是0.5 GB，这是一个更容易管理的大小。</p><p id="e069" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated"><strong class="lh ja">TPU-越南船民——救世在即</strong>:</p><p id="3501" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">今年6月，谷歌<a class="ae le" href="https://cloud.google.com/blog/products/compute/introducing-cloud-tpu-vms" rel="noopener ugc nofollow" target="_blank">宣布</a>一个新的云TPU架构，名为<a class="ae le" href="https://cloud.google.com/tpu/docs/users-guide-tpu-vm" rel="noopener ugc nofollow" target="_blank">云TPU虚拟机</a>。与最初的架构相反，被称为<a class="ae le" href="https://cloud.google.com/tpu/docs/system-architecture-tpu-vm#tpu-node" rel="noopener ugc nofollow" target="_blank"> TPU节点</a>、<a class="ae le" href="https://cloud.google.com/tpu/docs/system-architecture-tpu-vm#tpu-vm" rel="noopener ugc nofollow" target="_blank"> TPU虚拟机</a>，允许对TPU主机的直接SSH访问，不需要中间虚拟机。其含义是深远的。这不仅消除了对数据输入管道创建的限制，还极大地提高了我们调试和分析输入管道性能的能力。此外，删除中间虚拟机可能会减少网络开销并提高性能。</p><p id="cbf0" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">新架构已经可以在“预览”模式下使用。根据我们的经验，它还没有完全成熟。但是未来看起来是光明的。</p><h2 id="9ffa" class="nm mc iq bd md nn no dn mh np nq dp ml lo nr ns mn ls nt nu mp lw nv nw mr iw bi translated">更新您的模型图</h2><p id="cfbd" class="pw-post-body-paragraph lf lg iq lh b li mt ka lk ll mu kd ln lo mv lq lr ls mw lu lv lw mx ly lz ma ij bi translated">在TPU上训练您的模型可能还需要更改在TPU核心上运行的计算图。在这一节中，我们将演示为了符合API限制或TPU内存限制而可能实施的更改。</p><p id="46d5" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated"><strong class="lh ja"> TensorFlow Op限制</strong>:<br/>ASIC对支持的Op施加限制并不少见。这些限制可能来自硬件实施或支持软件堆栈的限制。TPU文档包括一个<a class="ae le" href="https://cloud.google.com/tpu/docs/tensorflow-ops" rel="noopener ugc nofollow" target="_blank">支持(和不支持)TensorFlow操作</a>的列表。不幸的是，(在撰写本文时)这个列表自称是不完整的。<a class="ae le" href="https://www.tensorflow.org/tensorboard" rel="noopener ugc nofollow" target="_blank"> TensorBoard </a>的图形可视化工具包括一个<a class="ae le" href="https://cloud.google.com/tpu/docs/cloud-tpu-tools#tpu_compatibility_graph" rel="noopener ugc nofollow" target="_blank"> TPU兼容性</a>选项，如下图所示，但根据我们的经验，该测试并不完全可靠。只有当您尝试运行图表时，您才可能收到图表无效的第一个指示。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi oh"><img src="../Images/b75457098abf16be35938d56b49085c6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*FvfRFZrZypX69l5p.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">TPU兼容性图(图片来自<a class="ae le" href="https://cloud.google.com/tpu/docs/cloud-tpu-tools#tpu_compatibility_graph" rel="noopener ugc nofollow" target="_blank"> GCP在线文档</a></p></figure><p id="0157" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">TPU限制包括对使用自定义运算符和可能导致张量形状不确定的运算的限制:</p><p id="fa2a" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated"><strong class="lh ja">定制操作— </strong>在GPU上进行培训的优势之一是，它支持在软件堆栈的各个级别进行定制。你可以创建自己的基于python的操作(例如使用<a class="ae le" href="https://www.tensorflow.org/api_docs/python/tf/numpy_function" rel="noopener ugc nofollow" target="_blank"> tf.numpy_function </a>)或者你可以在<a class="ae le" href="https://developer.nvidia.com/cuda-zone" rel="noopener ugc nofollow" target="_blank"> CUDA </a>中创建自己的<a class="ae le" href="https://www.tensorflow.org/guide/create_op#gpu_kernels" rel="noopener ugc nofollow" target="_blank"> GPU内核</a>。这种程度的灵活性在实现中非常有用:TensorFlow本身不支持的操作，或者专门为您的用例优化的GPU内核。这些能力在TPU是缺乏的(在撰写本文时)。如果您的图包括这些类型的定制，您将需要用本地操作来替换它们。下面的块包含一段摘录，摘自您在图形包含一个<a class="ae le" href="https://www.tensorflow.org/api_docs/python/tf/numpy_function" rel="noopener ugc nofollow" target="_blank"> tf.numpy_function </a>调用的情况下可能会遇到的错误消息类型。</p><pre class="kp kq kr ks gt nx ny nz oa aw ob bi"><span id="6df9" class="nm mc iq ny b gy oc od l oe of">(0) Invalid argument: {{function_node __inference_train_function_117771}} Detected unsupported operations when trying to compile graph cluster_train_function_10723333522471149816[] on XLA_TPU_JIT: <strong class="ny ja">PyFunc</strong> (No registered '<strong class="ny ja">PyFunc</strong>' OpKernel for XLA_TPU_JIT devices compatible with node {{node model/segLoss/PyFunc}}){{node model/loss/PyFunc}}</span></pre><p id="ff4e" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated"><strong class="lh ja">导致形状不确定的张量的操作</strong> —与GPU相反，TPU不允许某些API，因为它们使用非静态形状的张量。需要注意的是，最近的TensorFlow版本已经扩展了对此类操作的支持。然而，尽管受到支持，其中许多在TPU上表现很差，你应该尽量避免它们。</p><p id="ff3b" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">下面是一个在TPU失败的代码摘录的例子(摘自<a class="ae le" rel="noopener" target="_blank" href="/the-tensorflow-keras-summary-capture-layer-cdc436cb74ef">以前的博客文章</a>):</p><pre class="kp kq kr ks gt nx ny nz oa aw ob bi"><span id="70aa" class="nm mc iq ny b gy oc od l oe of">shape = [None,1]<br/>dtype = tf.float32<br/>record_tensor = tf.Variable(<br/>            shape=shape,<br/>            <em class="oi"># initialize with batch size 1 since batch_size <br/>            # is unknown, and set validate_shape=False</em><br/>            initial_value=tf.zeros(shape=[1]+shape[1:],<br/>                                   dtype=dtype),<br/>            validate_shape=False,<br/>            dtype=dtype,<br/>            trainable=False)</span></pre><p id="20ba" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">要为TPU修改它，我们需要修正batch_size并将<em class="oi"> validate_shape </em>改为<em class="oi"> True </em>。在这种情况下遇到的错误类似于:</p><pre class="kp kq kr ks gt nx ny nz oa aw ob bi"><span id="ddac" class="nm mc iq ny b gy oc od l oe of">tensorflow.python.framework.errors_impl.InvalidArgumentError: Dst node should be assigned to an allowed device.</span></pre><p id="76bc" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">导致未确定形状的张量的API的一个经典例子是<a class="ae le" href="https://www.tensorflow.org/api_docs/python/tf/boolean_mask" rel="noopener ugc nofollow" target="_blank"> tf.boolean_mask </a>。假设我们正在处理一个分割模型，该模型将图像作为输入，并为每个像素生成一个标签。我们可能希望从我们的损失计算中屏蔽掉图像的某些区域(由于它们的模糊性或低水平的兴趣)。在GPU <a class="ae le" href="https://www.tensorflow.org/api_docs/python/tf/boolean_mask" rel="noopener ugc nofollow" target="_blank">上，tf.boolean_mask </a>具有移除与这些区域上的损失计算相关联的所有操作的效果，并且可以显著提升性能。虽然TensorFlow最近增加了对<a class="ae le" href="https://www.tensorflow.org/api_docs/python/tf/boolean_mask" rel="noopener ugc nofollow" target="_blank"> tf.boolean_mask </a>的TPU支持，但通过计算所有像素的损失并在被遮罩的区域将结果值清零，您可能会获得更好的性能，如下面的代码块所示:</p><pre class="kp kq kr ks gt nx ny nz oa aw ob bi"><span id="3a63" class="nm mc iq ny b gy oc od l oe of"># given input logits, lables, mask and loss_fn<strong class="ny ja"><br/></strong>if tpu:<br/>    # zero out pixels according to mask<br/>    mask = tf.cast(mask, logits.dtype)<br/>    logits = logits * mask<br/>    labels = labels * mask<br/>else:<br/>    # reduce number of loss_fn operations using tf.boolean_mask<br/>    logits = tf.boolean_mask(logits, mask)<br/>    labels = tf.boolean_mask(labels, mask)<br/>sumLoss = tf.reduce_sum(loss_fn(logits, labels))</span></pre><p id="7216" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated"><strong class="lh ja"> TPU内存限制</strong> : <br/>当您将模型加载到TPU时，您可能会惊讶地发现，模型所需的内存量大大超过了GPU所需的内存量。出现这种情况的原因很可能是内存填充。我们将在下一节进一步讨论填充的主题。在下面的代码块中，我们演示了在内存需求超过可用TPUv3内存(每个内核16 GB)的情况下预期的错误类型。我们选择了一个极端的例子，填充将内存利用率提高了大约3倍，从6.4GB提高到19 GB以上。</p><pre class="kp kq kr ks gt nx ny nz oa aw ob bi"><span id="1236" class="nm mc iq ny b gy oc od l oe of">(0) Resource exhausted: {{function_node __inference_train_function_80576}} Ran out of memory in memory space hbm. <strong class="ny ja">Used 19.46G of 15.48G hbm. Exceeded hbm capacity by 3.98G</strong>.<br/>Total hbm usage &gt;= 19.98G: <br/>    reserved 530.00M <br/>    program 19.46G<br/>    arguments 0B</span><span id="f8d3" class="nm mc iq ny b gy og od l oe of">Output size 0B; shares 0B with arguments.</span><span id="6aad" class="nm mc iq ny b gy og od l oe of">Program hbm requirement 19.46G:<br/>    global 276.0K<br/>    scoped 173.0K<br/>    HLO temp 19.46G (33.1% utilization: <strong class="ny ja">Unpadded (6.40G) Padded (19.32G)</strong>, 0.7% fragmentation (147.91M))</span><span id="fcb1" class="nm mc iq ny b gy og od l oe of">  <strong class="ny ja">Largest program allocations in hbm</strong>:<br/>  <br/>  1. Size: 14.00G <br/>     Operator: op_type="OneHot" op_name="model/loss/one_hot"<br/>     Shape: s32[29360128,10]{1,0:T(8,128)}<br/>     Unpadded size: 1.09G<br/>     Extra memory due to padding: 12.91G (12.8x expansion)<br/>     XLA label: %iota.2 = s32[29360128,10]{1,0:T(8,128)} iota(), iota_dimension=1, metadata={op_type="OneHot" op_name="model/loss/one_hot"}<br/>     Allocation type: HLO temp<br/>     ==========================</span><span id="77a2" class="nm mc iq ny b gy og od l oe of">  2. Size: 2.62G<br/>     Operator: op_name="model/layer_1/Conv2D"<br/>     Shape: f32[128,256,896,18]{0,3,2,1:T(8,128)}<br/>     Unpadded size: 1.97G<br/>     Extra memory due to padding: 672.00M (1.3x expansion)<br/>     XLA label: %fusion.9.remat.1.remat = f32[128,256,896,18]{0,3,2,1:T(8,128)} fusion(f32[1,1,8,18]{3,2,1,0:T(8,128)} %get-tuple-element.4316, f32[18]{0:T(256)} %get-tuple-element.4085, f32[128,256,896,8]{0,3,2,1:T(8,128)} %get-tuple-element.3899, f32[8]{0:T(256)} %rsqrt...<br/>     Allocation type: HLO temp<br/>     ==========================</span></pre><p id="a666" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">错误消息包括最大内存分配的列表。在这种情况下，我们看到单个操作导致了12.91GB的额外填充。</p><p id="7b56" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">除了重新设计您的模型以适应内存需求之外，一个令人信服的选择是用<a class="ae le" href="https://www.tensorflow.org/guide/mixed_precision" rel="noopener ugc nofollow" target="_blank">混合精度</a>编译您的模型，并将<a class="ae le" href="https://www.tensorflow.org/api_docs/python/tf/keras/mixed_precision/Policy" rel="noopener ugc nofollow" target="_blank">混合精度策略</a>设置为<em class="oi"> mixed_bfloat16 </em>。默认情况下，所有变量都存储为tf.float32，这是一种32位浮点表示形式。Bfloat16是由Google创建的16位浮点表示。参见<a class="ae le" href="https://cloud.google.com/tpu/docs/bfloat16" rel="noopener ugc nofollow" target="_blank">此处</a>了解格式及其动态范围的详细信息。当您<a class="ae le" href="https://cloud.google.com/tpu/docs/bfloat16#changing" rel="noopener ugc nofollow" target="_blank">修改您的模型</a>以使用混合精度时，激活和梯度存储为tf.bfloat16，而权重保留在tf.float32中。这可以大大降低您的模型的内存需求，同时提高运行时性能。</p><p id="259f" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">根据Google的研究<a class="ae le" href="https://cloud.google.com/tpu/docs/bfloat16" rel="noopener ugc nofollow" target="_blank">报告</a>，大多数模型的收敛性不会受到tf.bfloat的影响。但是，如果您选择了这个选项，您应该意识到这种可能性。我们将在本文的第4步进一步讨论这个问题。</p><h1 id="78c8" class="mb mc iq bd md me mf mg mh mi mj mk ml kf mm kg mn ki mo kj mp kl mq km mr ms bi translated">步骤3-优化您的模型以在TPU上执行</h1><p id="e956" class="pw-post-body-paragraph lf lg iq lh b li mt ka lk ll mu kd ln lo mv lq lr ls mw lu lv lw mx ly lz ma ij bi translated">此时，您应该能够在TPU上成功运行一个训练周期。接下来是性能分析和优化的关键步骤。一个人工智能加速器的好坏取决于它为性能分析提供的工具。如果你不能分析性能，你就不能充分利用人工智能芯片。</p><p id="6590" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">在<a class="ae le" rel="noopener" target="_blank" href="/tensorflow-performance-analysis-314b56dceb59">之前的博客文章</a>中，我们详述了性能分析的重要性，并演示了<a class="ae le" href="https://www.tensorflow.org/guide/profiler" rel="noopener ugc nofollow" target="_blank">张量流分析器</a>和<a class="ae le" href="https://www.tensorflow.org/tensorboard" rel="noopener ugc nofollow" target="_blank">张量板</a>的使用。同样的技术也可用于分析TPU的性能。TPU文档包括一个详细的指南<a class="ae le" href="https://cloud.google.com/tpu/docs/cloud-tpu-tools" rel="noopener ugc nofollow" target="_blank">,介绍如何在TPU获取个人资料并在TensorBoard中分析结果。该文档还包括一个</a><a class="ae le" href="https://cloud.google.com/tpu/docs/performance-guide" rel="noopener ugc nofollow" target="_blank">指南</a>，介绍如何设计您的模型以优化TPU利用率。在本节中，我们将根据自己的经验重点介绍一些性能技巧。要了解更多细节，你应该回头参考这两个重要的指南。</p><h2 id="3b35" class="nm mc iq bd md nn no dn mh np nq dp ml lo nr ns mn ls nt nu mp lw nv nw mr iw bi translated">减少填充的开销</h2><p id="b4de" class="pw-post-body-paragraph lf lg iq lh b li mt ka lk ll mu kd ln lo mv lq lr ls mw lu lv lw mx ly lz ma ij bi translated">要了解TPU最重要的特性之一是张量在内存中的存储方式。未能根据TPU的<a class="ae le" href="https://cloud.google.com/tpu/docs/performance-guide#consequences_of_tiling" rel="noopener ugc nofollow" target="_blank">内存平铺方案</a>调整您的模型会导致大量的内存填充开销，这转化为未实现的潜力。在评估填充开销时，最重要的资源是TensorBoard <em class="oi"> profile </em>页面上的<em class="oi"> memory_viewer </em>选项卡。在下图中，我们展示了这个页面的一个例子。红色曲线显示未填充的内存利用率，蓝色曲线显示填充的内存利用率。在本例中，填充导致的内存占用大约是实际使用的内存大小的4倍。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi oj"><img src="../Images/ff5a9e591b36b1aa6682f06d05cc2300.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LapWs2Aww9lI8Z2HGOsg0w.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">摘自TensorBoard(作者)</p></figure><p id="6add" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">TPU文档提供了关于如何最小化填充开销的指南。简化版是这样的:</p><ol class=""><li id="4642" class="my mz iq lh b li lj ll lm lo na ls nb lw nc ma nd ne nf ng bi translated">使用128的倍数的(每个内核)批处理大小，并且</li><li id="0db0" class="my mz iq lh b li nh ll ni lo nj ls nk lw nl ma nd ne nf ng bi translated">将每个图层的输出要素的维度设置为8的倍数。</li></ol><p id="c203" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">当然，这可能并不适用于所有型号，在这种情况下，文档中会有更多的建议。不要忘记，正如我们在上面看到的，你可以选择启用<a class="ae le" href="https://www.tensorflow.org/guide/mixed_precision" rel="noopener ugc nofollow" target="_blank">混合精度</a>，以增加将一批128大小的数据放入TPU存储器的可能性。</p><h2 id="20d4" class="nm mc iq bd md nn no dn mh np nq dp ml lo nr ns mn ls nt nu mp lw nv nw mr iw bi translated">优化输入数据管道</h2><p id="2e2b" class="pw-post-body-paragraph lf lg iq lh b li mt ka lk ll mu kd ln lo mv lq lr ls mw lu lv lw mx ly lz ma ij bi translated">可以用来测量训练系统与给定模型的兼容性的参数之一是主机将训练批次馈送到加速器中的能力(例如，通过每秒批次来测量)与加速器处理输入批次的能力之间的比率。io带宽、输入管道中的数据处理操作量、CPU内核的数量和类型以及加速器的速度都会影响这一比率。如果该比率小于1，您可能会遇到输入管道瓶颈。在这种情况下，加速器在等待输入数据时将保持空闲，宝贵的计算周期将被浪费。这是一种不理想的情况，我们已经在<a class="ae le" rel="noopener" target="_blank" href="/overcoming-data-preprocessing-bottlenecks-with-tensorflow-data-service-nvidia-dali-and-other-d6321917f851">之前的帖子</a>中对此进行了阐述。</p><div class="ok ol gp gr om on"><a rel="noopener follow" target="_blank" href="/overcoming-data-preprocessing-bottlenecks-with-tensorflow-data-service-nvidia-dali-and-other-d6321917f851"><div class="oo ab fo"><div class="op ab oq cl cj or"><h2 class="bd ja gy z fp os fr fs ot fu fw iz bi translated">使用TensorFlow数据服务、NVIDIA DALI和其他解决方案克服数据预处理瓶颈</h2><div class="ou l"><h3 class="bd b gy z fp os fr fs ot fu fw dk translated">最大限度地提高培训资源利用率，加速学习，节省资金</h3></div><div class="ov l"><p class="bd b dl z fp os fr fs ot fu fw dk translated">towardsdatascience.com</p></div></div><div class="ow l"><div class="ox l oy oz pa ow pb ky on"/></div></div></a></div><p id="821e" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">在TPU上训练时，由于其消耗数据的速度很快，您在数据准备管道上遇到瓶颈的可能性可能会增加。即使您的输入管道不包括繁重的处理，被解析、混洗和批处理的大量数据也很容易阻塞CPU资源。通过使用<a class="ae le" href="https://www.tensorflow.org/api_docs/python/tf/data/experimental/service" rel="noopener ugc nofollow" target="_blank"> tf.data.service </a>卸载到辅助CPU可能会减轻一些负担。(参见<a class="ae le" href="https://github.com/tensorflow/ecosystem/tree/master/data_service" rel="noopener ugc nofollow" target="_blank">此</a>示例，了解如何使用TPU)。然而，它并不是在所有情况下都有帮助，您可能需要求助于更具创造性的解决方案，包括:调整分配给管道不同部分的进程数量，更改数据的格式和/或精度，或将计算转移到TPU上。</p><h2 id="f32e" class="nm mc iq bd md nn no dn mh np nq dp ml lo nr ns mn ls nt nu mp lw nv nw mr iw bi translated"><strong class="ak">递增steps_per_exec </strong></h2><p id="1bd4" class="pw-post-body-paragraph lf lg iq lh b li mt ka lk ll mu kd ln lo mv lq lr ls mw lu lv lw mx ly lz ma ij bi translated">最近，TensorFlow在<a class="ae le" href="https://www.tensorflow.org/api_docs/python/tf/keras/Model?hl=ru#compile" rel="noopener ugc nofollow" target="_blank"> tf.keras.Model.compile </a>输入参数中添加了<em class="oi"> steps_per_exec </em>标志。此设置控制每次调用内部训练函数时运行的训练步骤数。增加该数字可以减少TPU和主机之间的通信开销，最终提高性能。不过要记住的一点是，这将影响你进入训练回调函数的时间间隔。例如，如果您的回调类跟踪训练期间输入的批次数量，那么每次调用<em class="oi"> on_batch </em>函数时，该值应该增加<em class="oi"> steps_per_exec </em>而不是<em class="oi"> 1 </em>。更多细节参见<a class="ae le" href="https://www.tensorflow.org/api_docs/python/tf/keras/Model?hl=ru#compile" rel="noopener ugc nofollow" target="_blank">文档</a>。</p><p id="835a" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">你在一个定制的人工智能中发现的价值将在很大程度上取决于你在优化你的模型以有效使用它方面的成功。这种优化需要时间和精力，因此应该相应地进行规划。</p><h1 id="3e3a" class="mb mc iq bd md me mf mg mh mi mj mk ml kf mm kg mn ki mo kj mp kl mq km mr ms bi translated">第四步——调整你的模型，向TPU靠拢</h1><p id="3523" class="pw-post-body-paragraph lf lg iq lh b li mt ka lk ll mu kd ln lo mv lq lr ls mw lu lv lw mx ly lz ma ij bi translated">如果你已经完成了这一步，那么你已经成功地在专用的AI ASIC上以你满意的速度运行了你的模型。为了达到这一步，您可能已经对您的模型进行了一些更改。您可能更换了一些操作员，重新格式化了您的数据，增加了您的批量，或者应用了<a class="ae le" href="https://www.tensorflow.org/guide/mixed_precision" rel="noopener ugc nofollow" target="_blank">混合精度</a>。最后一步是验证你的模型成功收敛。这可能需要调整您的模型超参数，调整您的学习率，或者更换您的优化器。</p><p id="8ee3" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">即使您没有进行任何更改，这最后一步也是必需的。这是因为不同的硬件加速器以不同的方式实现，这可能会导致它们的行为存在数值差异。在一个ASIC上的收敛并不保证在另一个上的收敛。例如，TPUs的高性能归功于它使用了较低精度的浮点类型bfloat16(参见此处的<a class="ae le" href="https://cloud.google.com/blog/products/ai-machine-learning/bfloat16-the-secret-to-high-performance-on-cloud-tpus" rel="noopener ugc nofollow" target="_blank"/>)。您的模型可能对这种精度下降很敏感，在这种情况下，您需要重新调整它以收敛。</p><h2 id="8dd3" class="nm mc iq bd md nn no dn mh np nq dp ml lo nr ns mn ls nt nu mp lw nv nw mr iw bi translated">批量增加</h2><p id="96bd" class="pw-post-body-paragraph lf lg iq lh b li mt ka lk ll mu kd ln lo mv lq lr ls mw lu lv lw mx ly lz ma ij bi translated">为了充分利用TPU，您可能需要将批量增加到超出您习惯的水平。您的模型可能对训练批次大小很敏感，对其进行调整以收敛(在相同数量的数据遍历上)可能会带来巨大的挑战。查看我们之前的<a class="ae le" rel="noopener" target="_blank" href="/a-guide-to-highly-distributed-dnn-training-9e4814fb8bd3">博客文章</a>了解更多关于这个话题的细节。</p><div class="ok ol gp gr om on"><a rel="noopener follow" target="_blank" href="/a-guide-to-highly-distributed-dnn-training-9e4814fb8bd3"><div class="oo ab fo"><div class="op ab oq cl cj or"><h2 class="bd ja gy z fp os fr fs ot fu fw iz bi translated">(高度)分布式DNN训练指南</h2><div class="ou l"><h3 class="bd b gy z fp os fr fs ot fu fw dk translated">将培训扩展到多名员工时需要注意什么</h3></div><div class="ov l"><p class="bd b dl z fp os fr fs ot fu fw dk translated">towardsdatascience.com</p></div></div><div class="ow l"><div class="pc l oy oz pa ow pb ky on"/></div></div></a></div><h1 id="3fa9" class="mb mc iq bd md me mf mg mh mi mj mk ml kf mm kg mn ki mo kj mp kl mq km mr ms bi translated">摘要</h1><p id="2fa9" class="pw-post-body-paragraph lf lg iq lh b li mt ka lk ll mu kd ln lo mv lq lr ls mw lu lv lw mx ly lz ma ij bi translated">将你的模型转换为在定制的人工智能芯片上运行，可以节省大量的培训成本。然而，这可能需要相当大的努力。在这篇文章中，我们将重点放在TPU的训练上。在这一过程中，你可能会遇到各种各样的挑战。我们只讨论了几个例子。请务必参考丰富的在线文档以了解更多详细信息。</p></div></div>    
</body>
</html>