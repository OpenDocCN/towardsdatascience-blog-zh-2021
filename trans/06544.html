<html>
<head>
<title>Implementing ML Systems tutorial: Server-side or Client-side models?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">实现ML系统教程:服务器端还是客户端模型？</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/implementing-ml-systems-tutorial-server-side-or-client-side-models-3127960f9244?source=collection_archive---------17-----------------------#2021-06-12">https://towardsdatascience.com/implementing-ml-systems-tutorial-server-side-or-client-side-models-3127960f9244?source=collection_archive---------17-----------------------#2021-06-12</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="18b3" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">编写小型Flask服务器与编写WebDNN浏览器模型</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/1bc62270cbbb76d1bd90d3b7885eef42.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*AJjmhkx7gX6dZNNn"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">Krzysztof Kowalik 在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="1de6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">开发机器学习模型很有趣，也很好，但开发完之后，你可能会考虑将它们部署到应用程序中来使用它们。问题是你应该把它们放在客户端(可能是手机)还是应该把模型放在服务器上，把数据发送给服务器，然后把结果拿回来？在这篇文章中，我将讨论这两种方法的利弊，以及这两种方法需要什么。</p><h2 id="8556" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated">服务器端模型</h2><p id="da76" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">服务器端机器学习模型是最广泛使用的ML系统，因为其架构实现起来更简单。例如，您在手机(客户端)上运行了一个对图像进行分类的应用程序。用户拍了一张照片，这张照片被翻译成字节，手机用这些字节向服务器发送POST请求。服务器对这些字节运行模型的预测功能，并将带有预测的JSON响应发送回移动设备。这里没什么特别的。</p><p id="605c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在，如果您以这种方式实施您的ML系统，您将面临以下一些挑战:</p><ol class=""><li id="c4f8" class="mt mu it lb b lc ld lf lg li mv lm mw lq mx lu my mz na nb bi translated"><strong class="lb iu">延迟:</strong>如果您的应用需要密集的最大似然预测，如实时视频对象检测，您可能会在不充足的时间内获得结果，这将使用户体验非常不愉快。</li><li id="ef05" class="mt mu it lb b lc nc lf nd li ne lm nf lq ng lu my mz na nb bi translated"><strong class="lb iu">成本:</strong>典型的web应用程序不需要配备高端GPU的昂贵机器来快速运行预测。然而，如果你在服务器上托管模型，你将需要一个具有强大GPU的服务器，这是非常昂贵的。例如，我看到过这样的例子，在AWS EC2实例上托管一个图像字幕模型的成本大约是每天24美元(每年大约9000美元)。这与典型的web应用程序相比非常昂贵。</li></ol><p id="c3ef" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">因此，尽管这种方法可能更容易实现，但也有一些缺点需要考虑。当然，这取决于具体情况，因此测试您的系统并检查这些缺点是否真的会导致问题是值得的。</p><p id="3f8a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果您对以这种方式实现ML系统感兴趣，我建议尝试Python Flask服务器。它们非常容易实现和开始使用。你可以在这里查看我的教程<a class="ae ky" rel="noopener" target="_blank" href="/how-to-build-a-ml-powered-image-translation-messenger-chatbot-in-less-than-24-hours-1bf9d7b88f8b">关于构建一个使用Flask的ML messenger聊天机器人。您实际上是从运行服务器的样板代码开始，然后添加一个如下所示的预测函数:</a></p><pre class="kj kk kl km gt nh ni nj nk aw nl bi"><span id="dded" class="lv lw it ni b gy nm nn l no np">#Client sends a POST request with the image url</span><span id="3cb2" class="lv lw it ni b gy nq nn l no np"><a class="ae ky" href="http://twitter.com/app" rel="noopener ugc nofollow" target="_blank">@app</a>.route('/predict', methods=['POST'])<br/>def predict():</span><span id="2440" class="lv lw it ni b gy nq nn l no np">  if request.method == 'POST':</span><span id="d290" class="lv lw it ni b gy nq nn l no np">    data = json.loads(request.data)<br/>    url = data[0]["payload"]["url"]<br/>    image = requests.get(url)  # Get the image from the url sent by the client<br/>    img = Image.open(BytesIO(image.content))<br/>    result = predictor_api.make_prediction(url)<br/>    return jsonify({'result': result})</span></pre><h2 id="c98a" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated">客户端模型</h2><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nr"><img src="../Images/94e50d4ca2f7273f0fe921c19680db62.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*uacEPsNa3jIIfXGt"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">照片由<a class="ae ky" href="https://unsplash.com/@meymigrou?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">帕诺斯·萨卡拉基斯</a>在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><p id="8d7c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这是事情开始变得更有趣的地方。有几种方法可以做到这一点(与服务器端模型不同)，客户端模型更多的是一个正在进行的研究基础。两种最典型的客户端是浏览器或微控制器。我们将讨论浏览器，因为它们是更可能的客户端选项。</p><p id="ce46" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">客户端ML模型当然比服务器端模型便宜得多，并且理论上它们应该具有更低的延迟，因为你不会向另一个服务器发送和接收请求，你将在一个地方(客户端)做所有的事情。然而，实际上由于硬件限制，延迟实际上会更大。而且，它们可能会变得更加难以实现，因为你需要进行大量的优化，以便ML系统能够顺利运行，这就是我们将要在这里讨论的。</p><p id="0f25" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在移动设备/浏览器上部署ML模型的主要问题是由于硬件和集成的限制。本质上，大多数在浏览器上运行的web应用程序通常不需要像预测那样的计算密集型操作(除了web应用程序上的游戏)。然而，最近有大量的优化被引入，使得客户端ML模型变得更加可能。</p><p id="68dd" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">其中几个例子是:</p><ol class=""><li id="c045" class="mt mu it lb b lc ld lf lg li mv lm mw lq mx lu my mz na nb bi translated"><a class="ae ky" href="https://www.khronos.org/webgl/" rel="noopener ugc nofollow" target="_blank"> <strong class="lb iu"> WebGL </strong> </a> <strong class="lb iu">(浏览器实现</strong><a class="ae ky" href="https://www.opengl.org//" rel="noopener ugc nofollow" target="_blank"><strong class="lb iu">OpenGL</strong></a><strong class="lb iu">):</strong>web GL在2011年已经发布<strong class="lb iu">(所以其并不算太新)，但是对于ML机型来说还是挺有用的。WebGL是一个Javascript API，用于允许在浏览器上运行的任何应用程序平滑地交互和使用GPU，通常用于渲染游戏中的图形和物理交互</strong></li><li id="ad5f" class="mt mu it lb b lc nc lf nd li ne lm nf lq ng lu my mz na nb bi translated"><a class="ae ky" href="https://webassembly.org/" rel="noopener ugc nofollow" target="_blank"><strong class="lb iu">Web Assembly</strong></a><strong class="lb iu">:</strong>Web Assembly大约出现在<strong class="lb iu"> 4年前</strong>你大概能从关键词“Assembly”中感觉到它与编译和底层代码有关。WebAssembly引入了一种将web应用程序编译成更紧凑的二进制格式的新方法，这种方法从本质上减小了它们的大小，并允许它们在浏览器上更流畅地运行。它通过定义一个以二进制格式<a class="ae ky" href="https://github.com/WebAssembly/design/blob/master/BinaryEncoding.md" rel="noopener ugc nofollow" target="_blank"><strong class="lb iu"/></a>存储的<a class="ae ky" href="https://github.com/WebAssembly/design/blob/master/AstSemantics.md#abstract-syntax-tree-semantics" rel="noopener ugc nofollow" target="_blank">抽象语法树</a> (AST)来实现这一点。在“Javascript的nitrous boost”之前已经调用过了。</li><li id="2fe6" class="mt mu it lb b lc nc lf nd li ne lm nf lq ng lu my mz na nb bi translated"><a class="ae ky" href="https://www.w3.org/TR/webgpu/" rel="noopener ugc nofollow" target="_blank"> <strong class="lb iu"> WebGPU </strong> </a>:这本质上是在WebGL上的升级。WebGPU是一个正在进行的项目，用于升级WebGL以优化浏览器上的GPU操作。简而言之，WebGPU是一种新标准，它试图通过将标准分解为更多模块化的着色器组件来提高WebGL的核心性能，这些组件可以并发运行以获得更好的性能。</li><li id="7786" class="mt mu it lb b lc nc lf nd li ne lm nf lq ng lu my mz na nb bi translated"><a class="ae ky" href="https://mil-tokyo.github.io/webdnn/" rel="noopener ugc nofollow" target="_blank"><strong class="lb iu">WebDNN</strong></a><strong class="lb iu">:</strong>WebDNN是在浏览器上运行深度神经网络最好的开源库之一。WebDNN的第一个组件是Python API，它将模型转换成图形。然后，这个图被移交给第二个组件，该组件是一个JS API，它将这个图移植成兼容的格式，以便在浏览器上运行(适用于WebGL、Web Assembly等..)</li></ol><p id="8e0a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">WebDNN其实挺有意思的。我想研究一下它的使用和设置有多简单，所以下一节将是一个关于如何操作的小教程。第一个组件是python API，它按照上面的讨论转换模型。请注意，这不是一个精确的一步一步的教程，这里的主要目的是大致演示如何做到这一点，因为一个精确的教程可能需要一篇完整的文章，如果你想让我这样做，请在评论中告诉我。</p><p id="ebb4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">步骤Python API:</p><pre class="kj kk kl km gt nh ni nj nk aw nl bi"><span id="b391" class="lv lw it ni b gy nm nn l no np"># - Imports<br/>from keras.applications import resnet50<br/>from webdnn.frontend.keras import KerasConverter<br/>from webdnn.backend import generate_descriptor</span><span id="4357" class="lv lw it ni b gy nq nn l no np"># 1. load model<br/>model = resnet50.ResNet50(include_top=True, weights='imagenet')</span><span id="6374" class="lv lw it ni b gy nq nn l no np"># 2. Convert model to a graph<br/>graph = KerasConverter(batch_size=1).convert(model)<br/>exec_info = generate_descriptor("webgpu,webgl,webassembly,fallback", graph)</span><span id="337e" class="lv lw it ni b gy nq nn l no np"># 3. Save the graph  &amp; weights<br/>exec_info.save("./output")</span></pre><p id="4ef0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">来源:<a class="ae ky" href="https://github.com/mil-tokyo/webdnn" rel="noopener ugc nofollow" target="_blank"> WebDNN文档</a></p><p id="cfc9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">步骤JavaScript API:</p><pre class="kj kk kl km gt nh ni nj nk aw nl bi"><span id="46cf" class="lv lw it ni b gy nm nn l no np">let runner, image, probabilities;<br/>// Step 1: load the model</span><span id="42cb" class="lv lw it ni b gy nq nn l no np">async function init() {<br/>    // Initialize descriptor runner<br/>    runner = await WebDNN.load('./output');<br/>    image = runner.inputs[0]; <br/>    probabilities = runner.outputs[0];<br/>}</span><span id="cca4" class="lv lw it ni b gy nq nn l no np">// Step2: define a function that  runs predicitions</span><span id="bafc" class="lv lw it ni b gy nq nn l no np">async function run() {<br/>    // Set the value into input variable.<br/>    image.set(await WebDNN.Image.getImageArray('./input_image.png'));<br/>    <br/>    // Run<br/>    await runner.run();</span><span id="148b" class="lv lw it ni b gy nq nn l no np">// Show the result<br/>    console.log('Output', WebDNN.Math.argmax(probabilities));<br/>}</span></pre><p id="044b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">来源:<a class="ae ky" href="https://github.com/mil-tokyo/webdnn" rel="noopener ugc nofollow" target="_blank"> WebDNN文档</a></p><p id="fc21" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这看起来一点都不漫长也不复杂！在典型的客户端ML web应用程序中，您还需要一个额外的步骤，就是将模型集成到chrome扩展中。第一部分是在浏览器中持续运行的后台脚本，第二部分是注入每个运行页面的内容脚本。内容脚本负责将数据发送到后台脚本，以便在模型上运行预测。这实际上很容易实现。您需要一个函数来发送数据(例如，当一个按钮被点击时)，一个函数来监听/接收数据。</p><p id="d941" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这里有一个很好的例子:</p><pre class="kj kk kl km gt nh ni nj nk aw nl bi"><span id="3b97" class="lv lw it ni b gy nm nn l no np">// content script-1, sends a message<br/>$('img').each(function() {<br/> var img = $(this);<br/> chrome.extension.sendMessage({<br/>   msg: "run_model",<br/>   image: img[0].currentSrc<br/> });<br/>};</span><span id="1b55" class="lv lw it ni b gy nq nn l no np">// background script-1, listens to the message and runs the prediction<br/>chrome.extension.onMessage.addListener(<br/> function(request, sender, sendResponse){<br/>  if (request.msg == "run_model") {<br/>   chrome.tabs.query({active: true, currentWindow: true},<br/>    function(tabs) {<br/>     run(request.image, tabs[0].id)<br/>    }<br/>   )<br/>  }<br/> }<br/>)</span><span id="739b" class="lv lw it ni b gy nq nn l no np">// background script-2, send predictions back<br/>chrome.tabs.sendMessage(<br/> tab_id,<br/> {result: results}, function (response) {}<br/>);</span><span id="4d47" class="lv lw it ni b gy nq nn l no np">// content-script-2, receive model output<br/>chrome.extension.onMessage.addListenser(<br/> function(request, sender, sendResponse){<br/>  var imgid = request.imgid;<br/>  var result = request.result<br/>  // use output here<br/> }<br/>)</span></pre><p id="0241" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">来源:<a class="ae ky" href="https://github.com/milhidaka/chainer-image-caption/blob/master/webdnn/index.js" rel="noopener ugc nofollow" target="_blank"> Github示例项目</a></p><p id="e1be" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">就是这样！我希望你喜欢这篇文章，并学到了一些东西。客户端模型是ML中一个有趣的领域，你可以期待我在将来发表更多这样的文章。</p><p id="2c32" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">欢迎订阅我的时事通讯:</p><p id="5ca8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">https://artisanal-motivator-8249.ck.page/5524b8f934<a class="ae ky" href="https://artisanal-motivator-8249.ck.page/5524b8f934" rel="noopener ugc nofollow" target="_blank"/></p></div></div>    
</body>
</html>