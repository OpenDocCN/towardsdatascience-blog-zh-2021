<html>
<head>
<title>Double Machine Learning for causal inference</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">因果推理的双机器学习</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/double-machine-learning-for-causal-inference-78e0c6111f9d?source=collection_archive---------0-----------------------#2021-06-25">https://towardsdatascience.com/double-machine-learning-for-causal-inference-78e0c6111f9d?source=collection_archive---------0-----------------------#2021-06-25</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="f440" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsdatascience.com/tagged/thoughts-and-theory" rel="noopener" target="_blank">思想与理论</a>，<a class="ae ep" href="https://towardsdatascience.com/tagged/bcn-causal-algo" rel="noopener" target="_blank"> BCN因果ALGO </a></h2><div class=""/><div class=""><h2 id="8c7d" class="pw-subtitle-paragraph jw iz iq bd b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn dk translated">因果推理的双机器学习如何工作，从理论基础到应用实例。</h2></div><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ko"><img src="../Images/60c7e5a1a6275a2fb5b0793bffd2aa8d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ke3aBJyUWIhJF2xuRWTgbw.png"/></div></div></figure><p id="a3a8" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">这篇文章是与<a class="ae lw" href="https://www.linkedin.com/in/aleixrvr/" rel="noopener ugc nofollow" target="_blank">阿莱克斯·鲁伊斯·德·维拉</a>、<a class="ae lw" href="https://www.linkedin.com/in/cerquide/" rel="noopener ugc nofollow" target="_blank">耶稣·塞奎兹</a>以及整个<a class="ae lw" href="https://www.linkedin.com/groups/12501806/" rel="noopener ugc nofollow" target="_blank">因果关系ALGO·BCN</a>团队共同努力的成果。注意:我们假设读者熟悉因果推理的基本概念。</p><p id="84ca" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">如果你在因果推理的水域潜水，你可能听说过双机器学习的概念。如果你还没有听说过，我个人打赌你可能会，在不久的将来。或者更好的是，你可能甚至不知道你正在使用它。正如任何伟大的技术一样，用于因果推理的双机器学习有可能变得非常普遍。但是让我们让这位作家的热情平静下来，回到我们的任务上来。这篇文章试图简单而全面地解释什么是双机器学习以及它是如何工作的。为此，我们将从理论基础到因果推理应用的典型例子来讨论这个主题。</p><h2 id="f90e" class="lx ly iq bd lz ma mb dn mc md me dp mf lj mg mh mi ln mj mk ml lr mm mn mo iw bi translated">介绍</h2><p id="df28" class="pw-post-body-paragraph la lb iq lc b ld mp ka lf lg mq kd li lj mr ll lm ln ms lp lq lr mt lt lu lv ij bi translated">那么什么是双机器学习呢？Chernozukov等人在一系列论文(<a class="ae lw" href="https://arxiv.org/pdf/1608.00060.pdf" rel="noopener ugc nofollow" target="_blank">本</a>、<a class="ae lw" href="https://ifs.org.uk/uploads/cemmap/wps/cwp491616.pdf" rel="noopener ugc nofollow" target="_blank">本</a>和<a class="ae lw" href="https://arxiv.org/pdf/1701.08687.pdf" rel="noopener ugc nofollow" target="_blank">本</a>)中介绍并发展了这一思想，总体上旨在提供以下内容:</p><ol class=""><li id="2048" class="mu mv iq lc b ld le lg lh lj mw ln mx lr my lv mz na nb nc bi translated">使用机器学习技术估计因果效应的一般框架</li><li id="8166" class="mu mv iq lc b ld nd lg ne lj nf ln ng lr nh lv mz na nb nc bi translated">这些估计的置信区间</li><li id="b319" class="mu mv iq lc b ld nd lg ne lj nf ln ng lr nh lv mz na nb nc bi translated">“根n一致”的估计量，即在收敛性和数据效率方面具有良好特性的估计量。</li></ol><p id="bca1" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">这整个想法从何而来？一方面，显而易见，它来自于使用机器学习进行因果推理的想法。但是，如果我们看得更深入一点，就会发现两个不那么明显的关键想法:</p><ol class=""><li id="8b3d" class="mu mv iq lc b ld le lg lh lj mw ln mx lr my lv mz na nb nc bi translated">从统计学的角度来看，机器学习是非参数或半参数估计方法的集合，</li><li id="9c42" class="mu mv iq lc b ld nd lg ne lj nf ln ng lr nh lv mz na nb nc bi translated">有大量关于非参数和半参数估计方法的理论工作(关于界限、效率等)。)</li></ol><p id="992e" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">双机器学习将这两点联系起来，从第二点中获取灵感和有用的结果，用于与第一点进行因果推理。</p><h2 id="2d0d" class="lx ly iq bd lz ma mb dn mc md me dp mf lj mg mh mi ln mj mk ml lr mm mn mo iw bi translated">背景</h2><p id="7dd7" class="pw-post-body-paragraph la lb iq lc b ld mp ka lf lg mq kd li lj mr ll lm ln ms lp lq lr mt lt lu lv ij bi translated">让我们开始吧。我们首先定义数据生成流程的DAG，我们将在该流程下工作，如下图所示:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/c8dd3a64b40a36f9cf32902ad3cd779a.png" data-original-src="https://miro.medium.com/v2/resize:fit:926/format:webp/1*uIScBJMSKdqOf32C4czspw.png"/></div><p class="nj nk gj gh gi nl nm bd b be z dk translated">数据生成过程的DAG。作者配图</p></figure><p id="5cb8" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">此外，我们定义了以下控制DAG变量之间关系的部分线性模型:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi nn"><img src="../Images/ab8d80f271a068841da6ab4ceb6c3dd7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WDvI4XEVgFyRqTaOt_8Grg.png"/></div></div><p class="nj nk gj gh gi nl nm bd b be z dk translated">部分线性模型</p></figure><p id="0848" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">其中Y是结果变量，D是二元处理，Z是协变量的向量，U和V是扰动。等式1.1是主等式，θ₀是我们想要估计的目标参数，它是ate相对于d的导数。等式1.2跟踪混杂因素，即治疗对协变量的依赖性。这种依赖性是通过m₀(Z函数建模的，而结果对协变量的依赖性是通过g₀(Z).函数建模的我们将在后面看到，双机器学习也适用于完全非线性的模型，但我们开始假设这种部分线性的模型，以使该方法的解释和说明更容易。还要注意的是，我们假设了因果推理中可识别性的所有典型条件，即没有隐藏的混杂因素、阳性和一致性。</p><p id="046b" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">然后，回顾并完成引言中给出的双机器学习的定义，该方法的目标是在存在(潜在的高维)有害参数η₀=(g₀、m₀).的情况下，获得感兴趣的(低维)参数θ₀的根n一致估计量和置信区间在这种情况下，滋扰意味着我们不直接关心我们的η₀估计的正确性，只要我们得到一个好的(根n一致的)θ₀.估计</p><p id="72da" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">但是为什么我们首先对使用机器学习来完成这项任务感兴趣呢？主要有三个原因。首先也是最显而易见的是，机器学习方法在建模功能和/或预期方面的能力。第二，因为在处理高维数据时，机器学习模型比传统的统计方法(即线性回归和OLS)更擅长预测，这在我们的世界中正在成为规范。最后也是最重要的，因为与传统的统计方法相比，机器学习方法没有对m₀(Z(尤其是g₀(Z)的函数形式强加如此强的假设，而是从数据中学习那些形式。这是一个很好的防止模型设定错误的措施，在我们的例子中，即使没有不可测量的混杂因素，这也是一个会导致有偏估计的问题。</p><h2 id="c5f2" class="lx ly iq bd lz ma mb dn mc md me dp mf lj mg mh mi ln mj mk ml lr mm mn mo iw bi translated">天真的估计者</h2><p id="edd0" class="pw-post-body-paragraph la lb iq lc b ld mp ka lf lg mq kd li lj mr ll lm ln ms lp lq lr mt lt lu lv ij bi translated">那么，为什么不简单地用机器学习来直接估计θ₀呢？例如，我们可以使用交替迭代方法，用随机森林估计g₀，然后用OLS估计θ₀，重复直到收敛(注意，在这种情况下，对θ₀使用OLS并不意味着模型设定错误，因为模型在θ₀).是线性的嗯，生活没那么容易。下图显示了这种方法的θ₀-θ分布，并与平均值为0的正态分布进行了比较。从两个分布的差异可以看出，估计量是有偏的。请注意，g₀已被设置为具有很少参数的平滑函数，原则上应该由随机森林很好地近似。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi no"><img src="../Images/e1c7656e720467ea2b0bf4f114ca5127.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lhGz5G6VK0yPJDnpcv7fyw.png"/></div></div><p class="nj nk gj gh gi nl nm bd b be z dk translated">θ₀-θ分布，朴素估计量。原文<a class="ae lw" href="https://arxiv.org/pdf/1608.00060.pdf" rel="noopener ugc nofollow" target="_blank">此处</a>。</p></figure><p id="17fa" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">理解这一现象的关键观察是g₀(Z)≠𝔼[Y|Z].因此，通常不可能通过对z上的y进行“回归”来获得对g₀(Z的良好估计，这又导致不可能获得对θ₀.的良好估计然而，给定Z和d，完全有可能获得Y的非常好的预测。这就是为什么我们通常说机器学习对预测有好处，但对因果推理不利。偏差有两个来源，正则化和过拟合。双机器学习旨在纠正两者:通过正交化的方式纠正正则化偏差，以及通过交叉拟合的方式纠正过拟合偏差。接下来的部分将解释这两种偏差校正策略是如何工作的。关于偏差来源和形状的详细解释，请参见作者的原始工作或<a class="ae lw" href="https://scholar.princeton.edu/sites/default/files/bstewart/files/felton.chern_.slides.20190318.pdf" rel="noopener ugc nofollow" target="_blank">本次演讲</a>。</p><h2 id="99b5" class="lx ly iq bd lz ma mb dn mc md me dp mf lj mg mh mi ln mj mk ml lr mm mn mo iw bi translated">正交化和奈曼正交条件</h2><p id="935a" class="pw-post-body-paragraph la lb iq lc b ld mp ka lf lg mq kd li lj mr ll lm ln ms lp lq lr mt lt lu lv ij bi translated">为了说明正交化是如何工作的，我们首先提到并简要解释弗里希-沃-洛弗尔定理。该定理指出，给定线性模型Y=β₀+β₁D+β₂Z+U，估计β₁的以下两种方法产生相同的结果:</p><ol class=""><li id="26d0" class="mu mv iq lc b ld le lg lh lj mw ln mx lr my lv mz na nb nc bi translated">使用OLS对D和Z上的Y进行线性回归。</li><li id="1151" class="mu mv iq lc b ld nd lg ne lj nf ln ng lr nh lv mz na nb nc bi translated">三步程序:1)在Z上回归D；2)在Z上回归Y；3)对来自1的残差回归来自2的残差，以获得β₁(所有回归都使用OLS)。</li></ol><p id="d0e0" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">以类似的方式，回到我们的部分线性例子，我们可以如下进行:</p><ol class=""><li id="62a5" class="mu mv iq lc b ld le lg lh lj mw ln mx lr my lv mz na nb nc bi translated">使用机器学习基于Z预测D；</li><li id="3403" class="mu mv iq lc b ld nd lg ne lj nf ln ng lr nh lv mz na nb nc bi translated">使用机器学习基于Z预测Y；</li><li id="755d" class="mu mv iq lc b ld nd lg ne lj nf ln ng lr nh lv mz na nb nc bi translated">来自2的残差对来自1的残差的线性回归，用于获得θ₀.的估计</li></ol><p id="068a" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">这个过程确保了来自步骤3的模型是“正交化的”，产生了无偏的、根n一致的估计量。这种方法的θ₀-θ分布见下图。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi np"><img src="../Images/317884c65d00a6dad0321027cf1ef713.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NDG03j1TyFhEE1FJZ3FygQ.png"/></div></div><p class="nj nk gj gh gi nl nm bd b be z dk translated">θ₀-θ分布，正交估计量。原文<a class="ae lw" href="https://arxiv.org/pdf/1608.00060.pdf" rel="noopener ugc nofollow" target="_blank">此处</a>。</p></figure><p id="c79d" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">我们如何将这个过程形式化和一般化？为此，我们首先需要引入得分函数和矩条件的概念(关于广义矩方法的介绍，参见<a class="ae lw" href="https://en.wikipedia.org/wiki/Generalized_method_of_moments" rel="noopener ugc nofollow" target="_blank">本页</a>)。具体来说，在这种情况下使用的得分函数具有ψ=(d−m₀(z))×(y−g₀(z)−(d−m₀(z))θ形状，其中乘法项是我们的部分线性模型的误差项，尽管其他替代方案是可用的。现在，我们要求这个得分函数等于零，ψ=0，这就构成了我们的矩条件。这在数学上表达了我们希望回归量和误差正交，这类似于我们希望它们不相关。从操作上讲，这意味着一旦我们有了对g₀和m₀的估计，我们就能从力矩条件方程中得到θ₀。</p><p id="c15b" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">我们终于准备好将<strong class="lc ja">奈曼正交性</strong> <strong class="lc ja">条件</strong>表达和定义为</p><p id="e4e3" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">∂η𝔼[ψ(w；θ₀,η₀)][η−η₀] = 0</p><p id="a6ef" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">其中W是我们的数据。该方程解释如下:左手边是我们的得分函数相对于我们的干扰参数η在η₀.附近的gateaux方向导数我们说我们希望这个导数消失。而且，因为导数是瞬时变化率，所以我们要说的是，我们的得分函数(以及我们对θ₀的估计)对于干扰参数η的“小”扰动应该是稳健的。</p><p id="ab26" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">总之，将这个内曼正交条件应用于我们的得分函数(从而应用于我们的θ₀和η₀的估计量)使得我们的θ₀的估计量摆脱了两个偏差源之一，正则化偏差。</p><h2 id="4420" class="lx ly iq bd lz ma mb dn mc md me dp mf lj mg mh mi ln mj mk ml lr mm mn mo iw bi translated">样本分割和交叉拟合</h2><p id="3640" class="pw-post-body-paragraph la lb iq lc b ld mp ka lf lg mq kd li lj mr ll lm ln ms lp lq lr mt lt lu lv ij bi translated">现在，是时候消除偏置的另一个来源，即过拟合偏置(同样，有关这些偏置形状的详细解释，请参见本演示文稿<a class="ae lw" href="https://scholar.princeton.edu/sites/default/files/bstewart/files/felton.chern_.slides.20190318.pdf" rel="noopener ugc nofollow" target="_blank"/>)。为此，一种可能的策略是所谓的<strong class="lc ja">样本分割</strong>方法。其运作方式如下:</p><ol class=""><li id="f40e" class="mu mv iq lc b ld le lg lh lj mw ln mx lr my lv mz na nb nc bi translated">我们将数据随机分成两个子集。</li><li id="43a5" class="mu mv iq lc b ld nd lg ne lj nf ln ng lr nh lv mz na nb nc bi translated">我们在第一个子集上为D和Y拟合我们的机器学习模型。</li><li id="bf47" class="mu mv iq lc b ld nd lg ne lj nf ln ng lr nh lv mz na nb nc bi translated">我们使用步骤2中获得的模型来估计第二个子集中的θ₀。</li></ol><p id="a14a" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">这种策略的缺点是它降低了效率和统计能力。但这可以通过<strong class="lc ja">交叉拟合</strong>的方式解决，这是双机学习方法遵循的策略。内容如下:</p><ol class=""><li id="edfc" class="mu mv iq lc b ld le lg lh lj mw ln mx lr my lv mz na nb nc bi translated">我们将数据随机分成两个子集。</li><li id="7ad8" class="mu mv iq lc b ld nd lg ne lj nf ln ng lr nh lv mz na nb nc bi translated">我们在第一个子集上为D和Y拟合我们的机器学习模型。</li><li id="93c8" class="mu mv iq lc b ld nd lg ne lj nf ln ng lr nh lv mz na nb nc bi translated">我们使用步骤2中获得的模型估计第二个子集中的θ₀,₁。</li><li id="e730" class="mu mv iq lc b ld nd lg ne lj nf ln ng lr nh lv mz na nb nc bi translated">我们将我们的机器学习模型放在第二个子集中。</li><li id="71db" class="mu mv iq lc b ld nd lg ne lj nf ln ng lr nh lv mz na nb nc bi translated">我们使用步骤4中获得的模型来估计第一个子集中的θ₀,₂。</li><li id="1fc6" class="mu mv iq lc b ld nd lg ne lj nf ln ng lr nh lv mz na nb nc bi translated">我们得到了θ₀作为θ₀,₁和θ₀,₂.平均值的最终估计量</li></ol><p id="3b09" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">注意，我们可以通过对K个折叠重复该过程来获得鲁棒性，其中K大于2。</p><h2 id="572e" class="lx ly iq bd lz ma mb dn mc md me dp mf lj mg mh mi ln mj mk ml lr mm mn mo iw bi translated">该算法</h2><p id="fa7c" class="pw-post-body-paragraph la lb iq lc b ld mp ka lf lg mq kd li lj mr ll lm ln ms lp lq lr mt lt lu lv ij bi translated">我们终于到了这篇文章的结尾，是时候收集所有之前解释过的成分，并将它们放在一个算法中了。</p><p id="307e" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">让我们定义一个完全交互的模型，比部分线性的模型更一般。这个模型没有假设D(处理)是可加性分离的:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi nq"><img src="../Images/9b0358ece5d49ec7a74980992fd7030f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-tB2uvbj4y5hDYgk-8erpQ.png"/></div></div><p class="nj nk gj gh gi nl nm bd b be z dk translated">通用模型</p></figure><p id="0412" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">我们感兴趣的因果参数是ATE，𝔼[g₀(1；z)-g₀(0；Z)]，我们的得分函数是</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi nr"><img src="../Images/12bb3cc4bddcf85b653e18dc9cbdef59.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aPmYlKJ5TNN3wmgIDFxhMw.png"/></div></div></figure><p id="abef" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">它是由Robins和Rotnizsky (1995)提供的，并且是neyman正交的(并且是双重鲁棒的)，满足∂𝔼ψ(w；θ₀, η)= 0.</p><p id="4293" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">在这种设置中，双机器学习通过以下步骤提供ATE的<strong class="lc ja">无偏、根n一致估计量</strong>及其<strong class="lc ja">置信区间</strong>(置信水平为α):</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ns"><img src="../Images/de1da5e7ec187ad4abc60efdd1729146.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0LcwDzPcEk6-NJ_3ChIBjw.png"/></div></div></figure><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi nt"><img src="../Images/32c13b08801c4e60af44cfdeaa4d6a6f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*S2H0WhMXzKztXb8vfjQwjg.png"/></div></div></figure><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi nu"><img src="../Images/7e98a97dbc3b8be771ef86d6fae04069.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Qv9KzSFV81XHNxICFgI4fw.png"/></div></div></figure><p id="3a90" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">因此，我们已经完成了我们的任务。请注意，在https://github.com/DoubleML<a class="ae lw" href="https://github.com/DoubleML" rel="noopener ugc nofollow" target="_blank">的</a>中可以找到实现这个和其他相关算法的Python和R的可用包。</p></div></div>    
</body>
</html>