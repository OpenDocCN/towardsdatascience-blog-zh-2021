<html>
<head>
<title>Derivation of Least Squares Regressor and Classifier</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">最小二乘回归器和分类器的推导</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/derivation-of-least-squares-regressor-and-classifier-708be1358fe9?source=collection_archive---------12-----------------------#2021-09-17">https://towardsdatascience.com/derivation-of-least-squares-regressor-and-classifier-708be1358fe9?source=collection_archive---------12-----------------------#2021-09-17</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="da8c" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph">基本机器学习推导</h2><div class=""/><div class=""><h2 id="b601" class="pw-subtitle-paragraph jw iz iq bd b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn dk translated">一个基本但功能强大的分类器和回归器，它们的派生以及它们工作的原因</h2></div><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi ko"><img src="../Images/fffd6986d4b95f57943fb3c5e648364f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1282/format:webp/1*SlkeFp2WGDjjBdmWicXyDA.png"/></div><p class="kw kx gj gh gi ky kz bd b be z dk translated">作者图片</p></figure><p id="8970" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">在本文中，我推导了最小二乘回归和分类算法的伪逆解。</p><p id="4002" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">虽然不是很复杂，但在一些问题中，它仍然是一个非常强大的工具，今天仍然被用作其他机器学习模型的核心，如集成方法或神经网络(其中感知机提供了非常相似的算法)。如果你刚刚开始进入机器学习的世界，这是迄今为止让你头脑清醒的最重要的话题之一！</p><h1 id="9713" class="lw lx iq bd ly lz ma mb mc md me mf mg kf mh kg mi ki mj kj mk kl ml km mm mn bi translated"><strong class="ak">线性回归</strong></h1><p id="45b3" class="pw-post-body-paragraph la lb iq lc b ld mo ka lf lg mp kd li lj mq ll lm ln mr lp lq lr ms lt lu lv ij bi translated">让我们首先推导回归问题的最小二乘解。</p><p id="a777" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">我们尝试从数据矩阵x中估计目标向量y。为此，我们尝试优化权重向量w，使误差平方和最小化，如下所示:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi mt"><img src="../Images/7f0033dfd6287fd758517be4aed37b2e.png" data-original-src="https://miro.medium.com/v2/resize:fit:728/format:webp/1*yaPamd5TLX4mLWpANC_jqA.png"/></div><p class="kw kx gj gh gi ky kz bd b be z dk translated">其中E是误差平方和，y是目标向量，X是数据矩阵，w是权重向量</p></figure><p id="d517" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">最小二乘问题有一个解析解。当用w对误差求微分时，当导数等于零时，求w得到伪逆解:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi mu"><img src="../Images/089477b370ca125a554b578d1f49d51d.png" data-original-src="https://miro.medium.com/v2/resize:fit:856/format:webp/1*Q2LFwxw_nt0VhdHph9_BIQ.png"/></div></figure><h1 id="2a3d" class="lw lx iq bd ly lz ma mb mc md me mf mg kf mh kg mi ki mj kj mk kl ml km mm mn bi translated">最小平方分类器</h1><p id="9c4b" class="pw-post-body-paragraph la lb iq lc b ld mo ka lf lg mp kd li lj mq ll lm ln mr lp lq lr ms lt lu lv ij bi translated">通过尝试寻找最佳决策边界，最小二乘解也可用于解决分类问题。</p><p id="8684" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">如果试图在二维问题中进行分类，可以按以下方式调整最小二乘算法:</p><p id="9375" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">首先注意，目标张量不再是Nx1向量，而是Nxc张量，其中c是我们试图分类的类别的数量。此外，权重向量需要额外的维度来表示决策边界的截距:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="mw mx di my bf mz"><div class="gh gi mv"><img src="../Images/33b86a9de17c48b98bcba178910ac341.png" data-original-src="https://miro.medium.com/v2/resize:fit:558/format:webp/1*nM7s8rUxdK6yZ_tyhzZePA.png"/></div></div></figure><p id="6380" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">其中w0是决策边界的截距，w可以用作梯度。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi na"><img src="../Images/c2aac9b5919eb6541211a189d5b096a5.png" data-original-src="https://miro.medium.com/v2/resize:fit:458/format:webp/1*86L8wGR-H77bvCa_pXZxbQ.png"/></div></figure><p id="5288" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">数据矩阵X也必须适应要兼容的维度。将一个向量串联起来增加维度就达到了这个目的。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi nb"><img src="../Images/9da87d4f11e819184de75bfe508e0f4a.png" data-original-src="https://miro.medium.com/v2/resize:fit:644/format:webp/1*bHTFsHC145-v43fgRVl04w.png"/></div></figure><p id="63f7" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">新的数据矩阵和目标向量t可用于导出解析解。结果是最小二乘分类器及其伪逆解。</p><p id="71a1" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">这里是一个二元高斯分类器的小例子，用上面显示的方法实现，与默认的SK-learn分类器相对。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="mw mx di my bf mz"><div class="gh gi nc"><img src="../Images/cd1934029398851822b9e901695dd7a2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lorUtST3vZWBAms2tuwtcA.png"/></div></div><p class="kw kx gj gh gi ky kz bd b be z dk translated">作者图片</p></figure><p id="d332" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">决策边界的等式简单来说就是ax + by + c = 0。权重向量是[a，b，c]。孤立y我们发现分类器的梯度是-a/b，分类器的截距是-c/b。</p><p id="a76e" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">值得注意的是，这是二元高斯问题的最大后验估计量，对于无限数据，它将趋于完美的结果。</p><h1 id="979c" class="lw lx iq bd ly lz ma mb mc md me mf mg kf mh kg mi ki mj kj mk kl ml km mm mn bi translated"><strong class="ak">结论</strong></h1><p id="6ab0" class="pw-post-body-paragraph la lb iq lc b ld mo ka lf lg mp kd li lj mq ll lm ln mr lp lq lr ms lt lu lv ij bi translated">在我攻读硕士学位时，线性回归是一个反复出现的话题。它的简单性和它与最大后验解的关系是它在机器学习领域成功的原因。我希望这篇文章能帮助您理解如何将它应用于分类问题。</p><h2 id="c59d" class="nd lx iq bd ly ne nf dn mc ng nh dp mg lj ni nj mi ln nk nl mk lr nm nn mm iw bi translated">支持我👏</h2><p id="83d0" class="pw-post-body-paragraph la lb iq lc b ld mo ka lf lg mp kd li lj mq ll lm ln mr lp lq lr ms lt lu lv ij bi translated">希望这对你有所帮助，如果你喜欢，你可以<a class="ae no" href="https://medium.com/@diegounzuetaruedas" rel="noopener"> <strong class="lc ja">关注我！</strong>T3】</a></p><p id="1628" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">你也可以成为<a class="ae no" href="https://diegounzuetaruedas.medium.com/membership" rel="noopener"> <strong class="lc ja">中级会员</strong> </a> <strong class="lc ja"> </strong>使用我的推荐链接，获得我所有的文章和更多:<a class="ae no" href="https://diegounzuetaruedas.medium.com/membership" rel="noopener">https://diegounzuetaruedas.medium.com/membership</a></p><h2 id="74f1" class="nd lx iq bd ly ne nf dn mc ng nh dp mg lj ni nj mi ln nk nl mk lr nm nn mm iw bi translated">你可能喜欢的其他文章</h2><p id="4a40" class="pw-post-body-paragraph la lb iq lc b ld mo ka lf lg mp kd li lj mq ll lm ln mr lp lq lr ms lt lu lv ij bi translated"><a class="ae no" rel="noopener" target="_blank" href="/differentiable-generator-networks-an-introduction-5a9650a24823">可微发电机网络:简介</a></p><p id="89b7" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated"><a class="ae no" rel="noopener" target="_blank" href="/fourier-transforms-an-intuitive-visualisation-ba186c7380ee">傅立叶变换:直观的可视化</a></p></div></div>    
</body>
</html>