<html>
<head>
<title>The Hitchhiker’s Guide to Optimization in Machine Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">机器学习中的优化指南</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/the-hitchhikers-guide-to-optimization-in-machine-learning-edcf5a104210?source=collection_archive---------17-----------------------#2021-06-05">https://towardsdatascience.com/the-hitchhikers-guide-to-optimization-in-machine-learning-edcf5a104210?source=collection_archive---------17-----------------------#2021-06-05</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><figure class="ip iq gp gr ir is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi io"><img src="../Images/bbd147ffb05f58747841858777f7b814.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9IHbImpM6Y8iTOLyhICqhw.jpeg"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">在<a class="ae jd" href="https://unsplash.com/s/photos/hill?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上由<a class="ae jd" href="https://unsplash.com/@max_duz?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Max Duzij </a>拍照</p></figure><div class=""/><div class=""><h2 id="4ffc" class="pw-subtitle-paragraph kd jf jg bd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku dk translated">最优化和随机梯度下降的详细指南</h2></div><blockquote class="kv kw kx"><p id="9c86" class="ky kz la lb b lc ld kh le lf lg kk lh li lj lk ll lm ln lo lp lq lr ls lt lu ij bi translated">本文的目的是正确理解机器学习算法的“<em class="jg">优化</em>”的确切含义。此外，我们将看看基于梯度的类(梯度下降，随机梯度下降等)。)的优化算法。</p><p id="5b40" class="ky kz la lb b lc ld kh le lf lg kk lh li lj lk ll lm ln lo lp lq lr ls lt lu ij bi translated"><strong class="lb jh">注意:</strong>为了简单和更好的理解，我们将讨论的范围限制在<strong class="lb jh">监督机器学习</strong>算法。</p></blockquote><p id="7ffe" class="pw-post-body-paragraph ky kz jg lb b lc ld kh le lf lg kk lh lv lj lk ll lw ln lo lp lx lr ls lt lu ij bi translated">机器学习是应用数学和计算机科学的理想高潮，在这里我们训练和使用数据驱动的应用程序来对可用数据进行推理。一般来说，对于ML任务，推理的类型(即，模型做出的预测)根据问题陈述和正在为手边的任务处理的数据类型而变化。然而，与这些不同之处形成对比的是，这些算法往往也有一些相似之处，尤其是在它们如何操作的本质上。</p><p id="b6eb" class="pw-post-body-paragraph ky kz jg lb b lc ld kh le lf lg kk lh lv lj lk ll lw ln lo lp lx lr ls lt lu ij bi translated">让我们试着理解前一段。将监督ML算法视为一个超集。现在，我们可以根据这些算法共有的特征将这个超集进一步划分为更小的子组:</p><ul class=""><li id="ac40" class="ly lz jg lb b lc ld lf lg lv ma lw mb lx mc lu md me mf mg bi translated">回归与分类算法</li><li id="acdd" class="ly lz jg lb b lc mh lf mi lv mj lw mk lx ml lu md me mf mg bi translated">参数与非参数算法</li><li id="b06b" class="ly lz jg lb b lc mh lf mi lv mj lw mk lx ml lu md me mf mg bi translated">概率与非概率算法等。</li></ul><p id="5e36" class="pw-post-body-paragraph ky kz jg lb b lc ld kh le lf lg kk lh lv lj lk ll lw ln lo lp lx lr ls lt lu ij bi translated">尽管将这些差异分开，但如果我们观察监督机器学习算法的一般化表示，很明显这些算法倾向于以或多或少相同的方式工作。</p><ul class=""><li id="fbf9" class="ly lz jg lb b lc ld lf lg lv ma lw mb lx mc lu md me mf mg bi translated">首先，我们有一些带标签的数据，这些数据可以分解成特征集<strong class="lb jh"> X </strong>，以及相应的标签集<strong class="lb jh"> Y </strong>。</li></ul><figure class="mn mo mp mq gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi mm"><img src="../Images/60a023d4f37881444a758d698cbdf47e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_JQ8-W2b_hXKMwIkLDY5Gw.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">功能集和标签集(图片由作者提供)</p></figure><ul class=""><li id="7d4d" class="ly lz jg lb b lc ld lf lg lv ma lw mb lx mc lu md me mf mg bi translated">然后我们有模型函数，用<strong class="lb jh"> F </strong>表示，它是一个数学函数，将输入特征集<strong class="lb jh"> X_i </strong>映射到输出<strong class="lb jh"> ŷ_i </strong>。</li></ul><figure class="mn mo mp mq gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi mr"><img src="../Images/ea0ceea623dec72b09914c07099e0aad.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7JQC4mYrTtLeTbJDcG1AsQ.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">模型函数(图片由作者提供)</p></figure><p id="929d" class="pw-post-body-paragraph ky kz jg lb b lc ld kh le lf lg kk lh lv lj lk ll lw ln lo lp lx lr ls lt lu ij bi translated"><strong class="lb jh">通俗地说，每个有监督的最大似然算法都涉及将特征集X_i作为输入传递给模型函数f，函数f对其进行处理以生成输出ŷ_i. </strong></p><blockquote class="kv kw kx"><p id="bc4a" class="ky kz la lb b lc ld kh le lf lg kk lh li lj lk ll lm ln lo lp lq lr ls lt lu ij bi translated">然而，这只是一个模型的推断(或测试)阶段，从理论上讲，我们应该使用该模型对它从未见过的数据进行预测。</p><p id="9e86" class="ky kz la lb b lc ld kh le lf lg kk lh li lj lk ll lm ln lo lp lq lr ls lt lu ij bi translated">但是“训练”模型呢？接下来我们来看看。</p></blockquote></div><div class="ab cl ms mt hu mu" role="separator"><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx"/></div><div class="ij ik il im in"><h1 id="268f" class="mz na jg bd nb nc nd ne nf ng nh ni nj km nk kn nl kp nm kq nn ks no kt np nq bi translated">模型优化</h1><p id="59de" class="pw-post-body-paragraph ky kz jg lb b lc nr kh le lf ns kk lh lv nt lk ll lw nu lo lp lx nv ls lt lu ij bi translated">监督ML算法的训练阶段可以分为两个步骤:</p><ol class=""><li id="d22a" class="ly lz jg lb b lc ld lf lg lv ma lw mb lx mc lu nw me mf mg bi translated"><strong class="lb jh">前向传播:</strong>前向传播步骤类似于模型的推理阶段，这里我们有一个参数化的模型函数f，它对输入集<strong class="lb jh"> X_i </strong>执行转换以生成输出<strong class="lb jh"> ŷ_i </strong>。</li><li id="7d15" class="ly lz jg lb b lc mh lf mi lv mj lw mk lx ml lu nw me mf mg bi translated"><strong class="lb jh">反向传播:</strong>在我们了解反向传播的工作原理之前，了解我们为什么需要它是非常重要的。当我们使用参数化模型时，参数矩阵(模型权重和偏差)通常是随机初始化的。由于这种随机初始化以及与之相关的不确定性，初始模型的性能很有可能会令人失望。<strong class="lb jh"> <em class="la">因此，我们需要一种方法，以某种方式改变模型参数，从而提高其性能</em> </strong>。这正是反向传播的目标。在反向传播步骤中，我们首先参考实际标签<strong class="lb jh"> Y_i </strong>通过损失函数计算总信息损失(有时也称为模型成本)来评估推断的模型输出<strong class="lb jh"> ŷ_i </strong>。然后，基于所产生的损失，<strong class="lb jh">我们<em class="la">优化</em>模型</strong>，使得在下一个训练周期，模型产生的总成本减少。</li></ol><figure class="mn mo mp mq gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nx"><img src="../Images/e1d640c4595ab297fbad55fe6cbcc565.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Opu5RKufwwPbXbpMUbGLYw.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">模型训练(图片由作者提供)</p></figure><h2 id="63a3" class="ny na jg bd nb nz oa dn nf ob oc dp nj lv od oe nl lw of og nn lx oh oi np oj bi translated">那么我们到底如何定义模型优化呢？</h2><p id="3fc1" class="pw-post-body-paragraph ky kz jg lb b lc nr kh le lf ns kk lh lv nt lk ll lw nu lo lp lx nv ls lt lu ij bi translated"><strong class="lb jh">模型优化</strong>可以定义为基于一个标准(损失函数)更新模型参数(即模型权重和偏差)的过程，使得模型对训练数据进行推断时的信息损失减少。简单地说，在优化步骤中，我们迭代地更新模型参数，使得成本函数最小化。</p><p id="d5c6" class="pw-post-body-paragraph ky kz jg lb b lc ld kh le lf lg kk lh lv lj lk ll lw ln lo lp lx lr ls lt lu ij bi translated">因此，模型训练可以总结为一个迭代循环，其中我们从一些随机初始化的模型参数开始，然后通过几个反向传播步骤慢慢优化这些模型参数，目的是实现高精度和低模型成本。</p><p id="1d5b" class="pw-post-body-paragraph ky kz jg lb b lc ld kh le lf lg kk lh lv lj lk ll lw ln lo lp lx lr ls lt lu ij bi translated">现在我们知道了什么是模型优化，让我们来看看机器学习中一些最广泛使用的优化算法。</p></div><div class="ab cl ms mt hu mu" role="separator"><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx"/></div><div class="ij ik il im in"><h1 id="f061" class="mz na jg bd nb nc nd ne nf ng nh ni nj km nk kn nl kp nm kq nn ks no kt np nq bi translated">梯度下降算法</h1><p id="6e39" class="pw-post-body-paragraph ky kz jg lb b lc nr kh le lf ns kk lh lv nt lk ll lw nu lo lp lx nv ls lt lu ij bi translated">梯度下降是机器学习中最容易实现的(也可以说是最差的)优化算法之一。它是一阶(即，基于梯度的)优化算法，其中我们迭代地更新可微分成本函数的参数，直到达到其最小值。</p><p id="663d" class="pw-post-body-paragraph ky kz jg lb b lc ld kh le lf lg kk lh lv lj lk ll lw ln lo lp lx lr ls lt lu ij bi translated">在我们理解梯度下降如何工作之前，首先让我们看一下GD的广义公式:</p><figure class="mn mo mp mq gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi ok"><img src="../Images/7b06244664cbc5519fb2304eb3ecd9bc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZLpiw7lIx0x9EyIYKdjVCg.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">梯度下降(图片由作者提供)</p></figure><p id="cd91" class="pw-post-body-paragraph ky kz jg lb b lc ld kh le lf lg kk lh lv lj lk ll lw ln lo lp lx lr ls lt lu ij bi translated">这里的基本思想是在损失函数相对于参数的负梯度方向上更新模型参数。为了给你一个更好的直觉，你可以把梯度下降想成这样:</p><p id="68a8" class="pw-post-body-paragraph ky kz jg lb b lc ld kh le lf lg kk lh lv lj lk ll lw ln lo lp lx lr ls lt lu ij bi translated">让我们假设我们的成本函数是一个山区，我们的优化函数梯度下降的目的是找到这个山区的山谷(即成本函数的最小值)。继续山区的类比，让我们假设一个被蒙住眼睛的人被丢在山上的某个地方，并被要求到达山脚。</p><p id="18e2" class="pw-post-body-paragraph ky kz jg lb b lc ld kh le lf lg kk lh lv lj lk ll lw ln lo lp lx lr ls lt lu ij bi translated">现在，由于被蒙住了眼睛，这个人自然不知道去山脚的方向。因此，对于人来说，关于到基地的方向的最佳可用信息将是他们每走一步的最陡斜坡的<strong class="lb jh">方向。因此，按照梯度下降法，我们被蒙住眼睛的人会以这样一种方式移动，他们采取的每一步都将是最陡下降的方向。</strong></p><figure class="mn mo mp mq gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi ol"><img src="../Images/93695e1973c78c33f0ebbbda5a6ee97f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vRM9cQPzkD7VBCaXr9QC1g.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">下山(图片由作者提供)</p></figure><blockquote class="kv kw kx"><p id="2341" class="ky kz la lb b lc ld kh le lf lg kk lh li lj lk ll lm ln lo lp lq lr ls lt lu ij bi translated">基本上，梯度下降算法遵循一种<strong class="lb jh">贪婪搜索策略</strong>，在其他方向可能有更好的解决方案，然而，参数仅在相对于梯度(或斜率)的方向上更新。</p></blockquote><p id="4d2a" class="pw-post-body-paragraph ky kz jg lb b lc ld kh le lf lg kk lh lv lj lk ll lw ln lo lp lx lr ls lt lu ij bi translated"><strong class="lb jh">然而，这种搜索策略有一个重大缺陷！</strong>由于这是一个贪婪算法，该算法很有可能会陷入局部最小值而不是全局最小值 ，因此我们可能永远不会收敛到模型成本最小的最佳参数值。</p><h2 id="a270" class="ny na jg bd nb nz oa dn nf ob oc dp nj lv od oe nl lw of og nn lx oh oi np oj bi translated">学习率的影响(γ)</h2><p id="a696" class="pw-post-body-paragraph ky kz jg lb b lc nr kh le lf ns kk lh lv nt lk ll lw nu lo lp lx nv ls lt lu ij bi translated">当涉及到梯度下降算法中的参数更新时，表示步长(或学习率)的超参数γ的值起着关键作用。</p><ul class=""><li id="111b" class="ly lz jg lb b lc ld lf lg lv ma lw mb lx mc lu md me mf mg bi translated"><strong class="lb jh">如果</strong> γ <strong class="lb jh">太低</strong>，达到最优解的几率会增加。然而，收敛速度，即多快达到最小值，将急剧下降。这仅仅意味着当学习率太低时，模型必须被训练的时期的数量将增加。</li></ul><figure class="mn mo mp mq gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi om"><img src="../Images/208aada7647579b742f20316ef6fceb7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pG_ntjBdiHtRCM-yrvEy6w.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">γ太小(图片由作者提供)</p></figure><ul class=""><li id="07ba" class="ly lz jg lb b lc ld lf lg lv ma lw mb lx mc lu md me mf mg bi translated"><strong class="lb jh">如果</strong> γ <strong class="lb jh">太大</strong>，由于模型参数倾向于在最小值附近振荡，获得最优值的概率降低。然而，由于更大的步长，收敛速度增加。高学习率的另一个明显的缺点是一种叫做发散的现象，简单地说就是损失爆炸。</li></ul><figure class="mn mo mp mq gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi on"><img src="../Images/e19e2881d4749b2e607068e71b4bcbe9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WpDnOOKhBpDjKMwV_IA4Sg.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">γ太大(图片由作者提供)</p></figure><p id="c710" class="pw-post-body-paragraph ky kz jg lb b lc ld kh le lf lg kk lh lv lj lk ll lw ln lo lp lx lr ls lt lu ij bi translated"><strong class="lb jh">处理这种高学习率和低学习率之间的权衡的一种实用方法是具有可变的学习率</strong>——在初始时期具有较大的学习率，然后随着我们在模型训练过程中进一步进行，在以后时期具有降低的学习率。这将具有高学习率(更快的收敛)和低学习率(达到最优的更高概率)的优点。</p><figure class="mn mo mp mq gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi oo"><img src="../Images/445ef84310438ed7542b0c65fff719cc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZZAv70Y2aSUali-xF77KKA.png"/></div></div></figure><p id="8a9e" class="pw-post-body-paragraph ky kz jg lb b lc ld kh le lf lg kk lh lv lj lk ll lw ln lo lp lx lr ls lt lu ij bi translated">虽然梯度下降很容易实现，但它容易频繁陷入局部最优的缺点是不容忽视的。</p><blockquote class="kv kw kx"><p id="20f1" class="ky kz la lb b lc ld kh le lf lg kk lh li lj lk ll lm ln lo lp lq lr ls lt lu ij bi translated">如果优化算法实际上不能最小化模型成本，那么它有什么用？</p></blockquote><p id="9787" class="pw-post-body-paragraph ky kz jg lb b lc ld kh le lf lg kk lh lv lj lk ll lw ln lo lp lx lr ls lt lu ij bi translated">因此，研究人员提出了改进的、更优化的梯度下降版本——随机梯度下降。</p></div><div class="ab cl ms mt hu mu" role="separator"><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx"/></div><div class="ij ik il im in"><h1 id="8f89" class="mz na jg bd nb nc nd ne nf ng nh ni nj km nk kn nl kp nm kq nn ks no kt np nq bi translated">随机梯度下降</h1><p id="44f1" class="pw-post-body-paragraph ky kz jg lb b lc nr kh le lf ns kk lh lv nt lk ll lw nu lo lp lx nv ls lt lu ij bi translated">虽然随机梯度下降与梯度下降算法属于同一类一阶迭代优化算法，但它在计算和性能方面都比梯度下降有了显著的改进。</p><p id="7827" class="pw-post-body-paragraph ky kz jg lb b lc ld kh le lf lg kk lh lv lj lk ll lw ln lo lp lx lr ls lt lu ij bi translated">在SGD中，我们不是使用整个训练集的梯度来更新权重，而是随机地从训练集中选择一个数据实例，然后基于该单个数据实例的梯度来执行权重更新。</p><p id="ca24" class="pw-post-body-paragraph ky kz jg lb b lc ld kh le lf lg kk lh lv lj lk ll lw ln lo lp lx lr ls lt lu ij bi translated">概括的公式可以给出如下:</p><figure class="mn mo mp mq gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi op"><img src="../Images/1a30c6e118c4c41e4dbc60c87011b589.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hljMHo9fGyyA9S5Llcs65w.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">随机梯度下降公式(图片作者提供)</p></figure><p id="5b90" class="pw-post-body-paragraph ky kz jg lb b lc ld kh le lf lg kk lh lv lj lk ll lw ln lo lp lx lr ls lt lu ij bi translated">看一下SGD公式，很明显它非常类似于梯度下降算法。唯一的区别是，SGD使用从数据集中随机均匀选择的单个数据实例的模型损失梯度，而不是采用整批数据实例的模型损失梯度。</p><p id="edb4" class="pw-post-body-paragraph ky kz jg lb b lc ld kh le lf lg kk lh lv lj lk ll lw ln lo lp lx lr ls lt lu ij bi translated">然而，在实践中，与梯度下降相比，SGD工作得更好，原因是SGD中的<strong class="lb jh">噪声参数更新</strong>。由于我们是在每个实例的前向推进后更新参数，而不是在累积整个训练集的梯度后更新参数，因此参数往往会来回波动。这在某种程度上<strong class="lb jh">降低了陷入局部极小值</strong>的概率。这种现象被称为<strong class="lb jh">退火</strong>。</p><figure class="mn mo mp mq gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi oq"><img src="../Images/5ee8254cb91066201797fff8db1c9425.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*D_EA2XfG_zwKFaVrpPuw0w.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">SGD中的退火(图片由作者提供)</p></figure><p id="11b0" class="pw-post-body-paragraph ky kz jg lb b lc ld kh le lf lg kk lh lv lj lk ll lw ln lo lp lx lr ls lt lu ij bi translated">此外，还观察到<strong class="lb jh"> SGD与GD相比</strong>的计算成本更低，因此<strong class="lb jh">显著减少了训练时间</strong>。</p><p id="61f5" class="pw-post-body-paragraph ky kz jg lb b lc ld kh le lf lg kk lh lv lj lk ll lw ln lo lp lx lr ls lt lu ij bi translated">理论上，<strong class="lb jh">我们假设数据集中的所有数据实例都是相互独立的</strong>，即训练集中的所有实例都是唯一的。<strong class="lb jh">然而，在实践中，数据集中有很多冗余(相似的数据实例)。</strong>这有助于降低数据集中的噪声，从而确保更新的参数值接近(如果不是更好的话)从梯度下降中获得的参数值。</p><p id="6dd2" class="pw-post-body-paragraph ky kz jg lb b lc ld kh le lf lg kk lh lv lj lk ll lw ln lo lp lx lr ls lt lu ij bi translated"><strong class="lb jh">注意:</strong>在实践中，我们不是在单个数据实例上更新模型参数，而是在小批量数据上执行权重更新。这可以认为是GD和SGD的混合体。<strong class="lb jh"> mini-batch-SGD </strong>的优势在于它针对GPU上的并行化进行了优化(即，模型函数可以同时处理多个数据实例)，从而允许更快的训练时间和更少的噪音。</p></div><div class="ab cl ms mt hu mu" role="separator"><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx"/></div><div class="ij ik il im in"><h1 id="9022" class="mz na jg bd nb nc nd ne nf ng nh ni nj km nk kn nl kp nm kq nn ks no kt np nq bi translated">基于动量的SGD</h1><p id="0d24" class="pw-post-body-paragraph ky kz jg lb b lc nr kh le lf ns kk lh lv nt lk ll lw nu lo lp lx nv ls lt lu ij bi translated">如果我们用物理学术语来说，动量是一个物体保持其运动方向不变的属性。换句话说，动量是一个物体的物理属性，允许它保持其运动的方向和状态。</p><p id="2ce2" class="pw-post-body-paragraph ky kz jg lb b lc ld kh le lf lg kk lh lv lj lk ll lw ln lo lp lx lr ls lt lu ij bi translated">当我们在上一节讨论随机梯度下降时，我们观察到由于噪声，模型参数振荡很大。这种过多的噪声虽然在实践中对局部最小值有效，但阻止了模型损失函数达到绝对最小值。因此，成本函数可能永远不会收敛，如下图左侧所示。</p><figure class="mn mo mp mq gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi or"><img src="../Images/8ed112dbdd9a407d0baf3f543481fa62.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KWbaJM93BG7qY2vwXiXUGA.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">新币vs新币+动量(来源:<a class="ae jd" href="https://atcold.github.io/pytorch-Deep-Learning/en/week05/05-1/" rel="noopener ugc nofollow" target="_blank">https://at cold . github . io/py torch-Deep-Learning/en/week 05/05-1/</a>)</p></figure><p id="2756" class="pw-post-body-paragraph ky kz jg lb b lc ld kh le lf lg kk lh lv lj lk ll lw ln lo lp lx lr ls lt lu ij bi translated">因此，为了减少这些振荡，优化研究人员使用了牛顿物理学动量背后的动机，并引入了基于动量变化的随机梯度下降算法。</p><p id="9f2e" class="pw-post-body-paragraph ky kz jg lb b lc ld kh le lf lg kk lh lv lj lk ll lw ln lo lp lx lr ls lt lu ij bi translated">为了理解动量是如何起作用的，让我们看一下SGD带动量的通用公式。</p><figure class="mn mo mp mq gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi os"><img src="../Images/0d4e374e57e4b5149263725d3e5ba849.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*30jj5iqIq_W5AgGXNmRu5A.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">新币+动量(图片由作者提供)</p></figure><p id="1bdd" class="pw-post-body-paragraph ky kz jg lb b lc ld kh le lf lg kk lh lv lj lk ll lw ln lo lp lx lr ls lt lu ij bi translated"><strong class="lb jh">以下是SGD+momentum的工作原理:</strong></p><ul class=""><li id="c3ec" class="ly lz jg lb b lc ld lf lg lv ma lw mb lx mc lu md me mf mg bi translated">在非动量SGD中，通过梯度方向上的贪婪搜索来更新参数。</li><li id="8c04" class="ly lz jg lb b lc mh lf mi lv mj lw mk lx ml lu md me mf mg bi translated">然而，在SGD+动量中，来自前面步骤的动量添加剂影响斜坡的下降。结果，参数更新被迫沿着某个方向，而不是在负梯度的方向上振荡。结果，这抑制了噪声，并因此增加了模型成本达到绝对最小值的概率。</li></ul><p id="6c3f" class="pw-post-body-paragraph ky kz jg lb b lc ld kh le lf lg kk lh lv lj lk ll lw ln lo lp lx lr ls lt lu ij bi translated">如果你仔细注意动量公式，它只是SGD中每个参数更新步骤后梯度的累积。然而，这里需要注意的一件重要事情是，超参数β(范围在0-1之间)会在每步之后衰减累积的梯度。因此，与更早的步骤中的动量相比，由于更近的步骤中的梯度而产生的动量更多地影响参数更新。</p><p id="95cb" class="pw-post-body-paragraph ky kz jg lb b lc ld kh le lf lg kk lh lv lj lk ll lw ln lo lp lx lr ls lt lu ij bi translated">实际上，正如我们在上图中看到的，动量增加了收敛的可能性，同时计算量也不是很大。这就是为什么大多数深度学习和机器学习包使用SGD的基于动量的实现而不是常规实现的原因。</p><p id="558d" class="pw-post-body-paragraph ky kz jg lb b lc ld kh le lf lg kk lh lv lj lk ll lw ln lo lp lx lr ls lt lu ij bi translated">到此，我们来结束这篇文章。这篇文章的要点是:</p><ul class=""><li id="720f" class="ly lz jg lb b lc ld lf lg lv ma lw mb lx mc lu md me mf mg bi translated">我们看到了什么是模型优化。</li><li id="5f23" class="ly lz jg lb b lc mh lf mi lv mj lw mk lx ml lu md me mf mg bi translated">我们概述了一些最广泛使用的优化算法，如梯度下降、随机梯度下降和基于动量的SGD。</li></ul><p id="80df" class="pw-post-body-paragraph ky kz jg lb b lc ld kh le lf lg kk lh lv lj lk ll lw ln lo lp lx lr ls lt lu ij bi translated">如果你觉得这篇文章很有帮助，并希望继续看到更多这样的文章，请确保点击“关注”按钮。</p><p id="f8e5" class="pw-post-body-paragraph ky kz jg lb b lc ld kh le lf lg kk lh lv lj lk ll lw ln lo lp lx lr ls lt lu ij bi translated"><strong class="lb jh">这里是我的一些其他策划的文章:</strong></p><div class="ip iq gp gr ir ot"><a rel="noopener follow" target="_blank" href="/ml-from-scratch-k-nearest-neighbors-classifier-3fc51438346b"><div class="ou ab fo"><div class="ov ab ow cl cj ox"><h2 class="bd jh gy z fp oy fr fs oz fu fw jf bi translated">ML从零开始:K-最近邻分类器</h2><div class="pa l"><h3 class="bd b gy z fp oy fr fs oz fu fw dk translated">KNN分类算法的完整指南，在这里我们将看到如何实现一个基于KNN的机器学习…</h3></div><div class="pb l"><p class="bd b dl z fp oy fr fs oz fu fw dk translated">towardsdatascience.com</p></div></div><div class="pc l"><div class="pd l pe pf pg pc ph ix ot"/></div></div></a></div><div class="ip iq gp gr ir ot"><a rel="noopener follow" target="_blank" href="/ml-from-scratch-multinomial-logistic-regression-6dda9cbacf9d"><div class="ou ab fo"><div class="ov ab ow cl cj ox"><h2 class="bd jh gy z fp oy fr fs oz fu fw jf bi translated">ML从零开始-多项式逻辑回归</h2><div class="pa l"><h3 class="bd b gy z fp oy fr fs oz fu fw dk translated">你的多项逻辑回归完全指南，又名Softmax回归</h3></div><div class="pb l"><p class="bd b dl z fp oy fr fs oz fu fw dk translated">towardsdatascience.com</p></div></div><div class="pc l"><div class="pi l pe pf pg pc ph ix ot"/></div></div></a></div><div class="ip iq gp gr ir ot"><a rel="noopener follow" target="_blank" href="/linear-regression-model-with-numpy-7d270feaca63"><div class="ou ab fo"><div class="ov ab ow cl cj ox"><h2 class="bd jh gy z fp oy fr fs oz fu fw jf bi translated">数字线性回归模型</h2><div class="pa l"><h3 class="bd b gy z fp oy fr fs oz fu fw dk translated">在这个项目中，我们将看到如何创建一个使用多元线性回归算法的机器学习模型。</h3></div><div class="pb l"><p class="bd b dl z fp oy fr fs oz fu fw dk translated">towardsdatascience.com</p></div></div><div class="pc l"><div class="pj l pe pf pg pc ph ix ot"/></div></div></a></div><p id="5e5f" class="pw-post-body-paragraph ky kz jg lb b lc ld kh le lf lg kk lh lv lj lk ll lw ln lo lp lx lr ls lt lu ij bi translated">你可以在LinkedIn上联系我:</p><div class="ip iq gp gr ir ot"><a href="https://www.linkedin.com/in/amansharma2910/" rel="noopener  ugc nofollow" target="_blank"><div class="ou ab fo"><div class="ov ab ow cl cj ox"><h2 class="bd jh gy z fp oy fr fs oz fu fw jf bi translated">Aman Sharma -微软学生大使-微软| LinkedIn</h2><div class="pa l"><h3 class="bd b gy z fp oy fr fs oz fu fw dk translated">作为一名深度学习、数据科学爱好者和充满激情的技术博客作者，我相信数据是最强大的…</h3></div><div class="pb l"><p class="bd b dl z fp oy fr fs oz fu fw dk translated">www.linkedin.com</p></div></div><div class="pc l"><div class="pk l pe pf pg pc ph ix ot"/></div></div></a></div><p id="28f0" class="pw-post-body-paragraph ky kz jg lb b lc ld kh le lf lg kk lh lv lj lk ll lw ln lo lp lx lr ls lt lu ij bi translated">快乐学习！</p></div></div>    
</body>
</html>