<html>
<head>
<title>Why Discount Future Rewards In Reinforcement Learning?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">为什么强化学习要对未来的回报打折扣？</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/why-discount-future-rewards-in-reinforcement-learning-a833d0ae1942?source=collection_archive---------13-----------------------#2021-10-28">https://towardsdatascience.com/why-discount-future-rewards-in-reinforcement-learning-a833d0ae1942?source=collection_archive---------13-----------------------#2021-10-28</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="c77a" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">从数学，金融，生活，当然还有强化学习的角度讨论贴现率。</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/08b18304418474d75542ca835f092955.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*J69XcYDoeGRWnTT_"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">不是那种折扣。<a class="ae ky" href="https://unsplash.com/@belart84?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Artem Beliaikin </a>在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure></div><div class="ab cl kz la hx lb" role="separator"><span class="lc bw bk ld le lf"/><span class="lc bw bk ld le lf"/><span class="lc bw bk ld le"/></div><div class="im in io ip iq"><p id="4f22" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">虽然折现率是马尔可夫决策问题和强化学习(RL)的一个组成部分，但我们经常会不假思索地选择<em class="mc"> γ=0.9 </em>或<em class="mc"> γ=0.99 </em>。当然，当被问到时，我们会有一些直觉，比如“今天的回报比明天的回报更有价值”或者“补偿不确定性”。当被追问时，你能为为什么这些直觉成立辩护吗？为什么挑<em class="mc"> γ= </em> 0.8 而不是<em class="mc"> γ= </em> 0.9？不确定性不是已经纳入期望值了吗？如果你没有现成的即时答案，这篇文章可能会对这个问题有所启发。</p><h2 id="cd37" class="md me it bd mf mg mh dn mi mj mk dp ml lp mm mn mo lt mp mq mr lx ms mt mu mv bi translated">🧮数学中的折现</h2><p id="137f" class="pw-post-body-paragraph lg lh it li b lj mw ju ll lm mx jx lo lp my lr ls lt mz lv lw lx na lz ma mb im bi translated">从严格的数学角度来看，贴现率的目的是显而易见的，至少对于无限期问题是如此。从贝尔曼的递归方程中，我们学会了求解状态序列的值函数:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nb"><img src="../Images/21eaf517f52c7616e9bd133d842a2ac6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WwN-sAkWsHTEh8ECNJE6KA.png"/></div></div></figure><p id="92ff" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">如果那个数列是无穷的，那么奖励数列也是无穷的。考虑以下累积奖励序列<em class="mc"> G_t </em>:</p><p id="6a71" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated"><code class="fe nc nd ne nf b">G_t = R_t + R_t+1 + R_t+2 + … = 1 + 1 + 1 + … = ∞</code></p><p id="1581" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">众所周知，对一个无穷奖励级数求和会产生无穷奖励，使得方程组无解。幸运的是，加上贴现率<em class="mc">γ∈【0，1】</em>会产生一个收敛的几何级数。例如，如果我们设置<em class="mc"> γ=0.8 </em>，我们得到:</p><p id="c88e" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated"><code class="fe nc nd ne nf b"><em class="mc">G_t = γ⁰R_t + γ¹R_t+1 + γ²R_t+2 + … = 1 + 0.8 + 0.64 + … = 5</em></code></p><p id="26ae" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">有了这个技巧，我们可以将值附加到状态上，并求解贝尔曼方程组。当然，在强化学习中，这个解决方案是近似的。</p><p id="1a7d" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">这解释了无限的情况，但是为什么要为有限时间范围的贴现而烦恼呢？你可能会说我们补偿了不确定性，但这不是已经反映在预期价值中了吗(用未来回报乘以它们的概率)？数学的观点不能解决这个问题，我们需要深入人类的心灵。</p><h2 id="e2ee" class="md me it bd mf mg mh dn mi mj mk dp ml lp mm mn mo lt mp mq mr lx ms mt mu mv bi translated">金融中的贴现💸</h2><p id="3a45" class="pw-post-body-paragraph lg lh it li b lj mw ju ll lm mx jx lo lp my lr ls lt mz lv lw lx na lz ma mb im bi translated">有什么比金钱更能诠释人类的精神？</p><p id="f18f" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">投资的一个重要组成部分是存在一个无风险利率。这是在没有任何不确定性或违约风险的情况下可以获得的回报，作为所有其他回报的基准。美国国债经常被用作代理。将 1 美元投入 2%的美国国债，一年后你将获得 1.02 美元的保底收益。因此，我们更喜欢今天的 1 美元，而不是明年的 1 美元。不需要努力，我们的财富每年可以增长 2%，因此我们会将未来的回报折现 2%，以反映<strong class="li iu">的时间价值</strong>。</p><p id="a284" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">当考虑股票等风险工具时，这变得更有意思了。假设一只股票从现在起一年后会上涨 0%或 4%，两者的概率都是 0.5。<strong class="li iu">的预期收益</strong>是 2%。然而，空手而归的可能性很大。在这种情况下，典型的投资者会更喜欢无风险的 2%债券，尽管<em class="mc">预期的</em>收益是相等的。由此可见，股票回报的折现率高于债券回报。</p><p id="77cd" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">这种现象被称为<em class="mc">避险</em> <strong class="li iu">。人们期望不确定性得到补偿，否则他们会选择更安全的替代方案。不确定性越多，折现率越高。也许我们会选择收益率为 10%而不是 4%的股票(将预期收益提高到 5%)。投资需要在补偿时间价值的基础上提供一定的<strong class="li iu">风险溢价</strong>。</strong></p><p id="8d91" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">仍然有许多话题没有触及，例如使用<strong class="li iu">指数贴现</strong>的趋势(类似于 RL)、机会或后悔成本(你只能投资一次)、冒险行为(从理性投资者的角度来看，彩票几乎没有意义)，以及奇怪的不一致边界情况。现在，让我们确定贴现率既反映了时间价值又反映了风险溢价的基本原理。</p><h2 id="ae2e" class="md me it bd mf mg mh dn mi mj mk dp ml lp mm mn mo lt mp mq mr lx ms mt mu mv bi translated">生活中的折扣⏳</h2><p id="d847" class="pw-post-body-paragraph lg lh it li b lj mw ju ll lm mx jx lo lp my lr ls lt mz lv lw lx na lz ma mb im bi translated">贴现行为不仅限于美元。在日常生活中，我们不断地平衡短期满足和长期后果，权衡确定性和不确定性。晚睡，明天会很累。冬天尽情地吃，夏天你需要减掉脂肪来打造海滩身材。现在没日没夜地学习，希望以后有所收获。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ng"><img src="../Images/3b16131f72ba4655318f7fd0c42e228e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Jlo_l2_0yx_EoQdgbdPihA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">指数贴现和双曲线贴现的比较。前者通常用于强化学习，后者根据经验在人类中观察到。图片来自维基媒体</p></figure><p id="f3c1" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">在这个问题上已经进行了无数的心理学研究，表明人类在决策中倾向于执行类似于双曲线贴现的东西。手臂上纹有<em class="mc">及时行乐</em>纹身的人很可能对未来的奖励大打折扣，其他人可能会更看重它们。尽管个人之间存在差异，但我们所有人都在某种程度上打了折扣。</p><p id="444c" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">这种现象的一个非常自然的原因是<strong class="li iu">风险率</strong>——我们无法活到明天收获回报的概率。尽管急性死亡的风险不像我们狩猎采集的祖先那样高，但偏好奖励的生物冲动现在仍然非常完整。危险不需要像死亡一样病态。膝盖受伤可能会缩短运动生涯，而五年后的亚洲背包旅行也可能无法实现。即使在最平庸的层面上，我们也更喜欢现在吃一块饼干，而不是下周吃一块。</p><p id="8314" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">作为人类，无论明智与否，我们只是天生倾向于把现在的回报看得比(遥远的)未来更高。</p><h2 id="7442" class="md me it bd mf mg mh dn mi mj mk dp ml lp mm mn mo lt mp mq mr lx ms mt mu mv bi translated">强化学习中的折扣📖</h2><p id="2af3" class="pw-post-body-paragraph lg lh it li b lj mw ju ll lm mx jx lo lp my lr ls lt mz lv lw lx na lz ma mb im bi translated">现在我们对人类贴现的基本原理有了一些了解，但是这种推理对强化学习问题成立吗？尽管神经网络和人脑之间存在一些松散的联系，但典型的 RL 算法并不是为了模仿人类行为而设计的。危险率也是一个易受影响的基本原理，因为我们可以将危险模拟到环境中。例如，一个<a class="ae ky" rel="noopener" target="_blank" href="/walking-off-the-cliff-with-off-policy-reinforcement-learning-7fdbcdfe31ff">悬崖行走游戏</a>在代理人踏入悬崖时结束——我们不需要额外打折来反映心脏病发作或脚趾受伤。一个相反的论点是，这种危险可能发生在政策没有经过训练的环境中，利用贴现率作为一种稳健性手段。</p><p id="af8f" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">对于许多 RL 问题，不贴现未来的奖励是完全可以接受的。尽管如此，即使是有限的时间范围，也有充分的理由对 RL 进行贴现。其中之一是决策对长期绩效的实际影响。最终，由建模者决定什么样的折现率最能反映问题背景下的累积回报。</p><p id="80e2" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">假设今晚我必须在清淡和丰盛的晚餐中做出选择。这个决定可能会影响我之后的健身课，但可能不会影响我明年的晋升。在这里，我们看到了贴现未来回报的明确理由——一些后果很难与今天的行动联系起来。RL 的目的是纳入当前决策的下游影响，然而<strong class="li iu">未来的回报可能是不相关的</strong>。一般来说，环境越随机，我们的行为对绩效的持久影响就越小。</p><p id="8c78" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">StackExchange 上的一个<a class="ae ky" href="https://stats.stackexchange.com/questions/221402/understanding-the-role-of-the-discount-factor-in-reinforcement-learning" rel="noopener ugc nofollow" target="_blank">答案以一种有趣的方式形式化了这个概念，表达了一个参数<em class="mc"> τ </em>，它反映了我们感兴趣的时间间隔:</a></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nh"><img src="../Images/a2965b274c0f5d5fe6ff50ca44bf9b1d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1LuXuLAZapiULzAQn2U71Q.png"/></div></div></figure><p id="1713" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">和以前一样，假设回报总是 1。随着<em class="mc"> γ=0.8 </em>，级数收敛到 5。实际上，超过五个时间步长的奖励——注意<em class="mc">e^(-1/5)≈0.8</em>——几乎没有影响。类似的，一个级数在<em class="mc"> γ=0.9 </em>时收敛到 10，在γ=0.99 时收敛到 100。请注意:在<em class="mc"> t+τ </em>之后突然获得+100 奖励仍然会对折扣奖励产生重大影响，但是根据<strong class="li iu">的经验法则</strong>这种方法是有意义的。如果你认为五个时间步后的回报与现在所做的决定没有什么关系，<em class="mc"> γ=0.8 </em>可能是一个合适的贴现率。</p><p id="c0f0" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">希望这篇文章在贴现率的话题上提供了一些清晰度，然而它仅仅触及了表面。有人认为<a class="ae ky" href="https://arxiv.org/pdf/1910.02140.pdf" rel="noopener ugc nofollow" target="_blank">整个概念对于函数逼近(我们通常在 RL 中使用)是有缺陷的</a>，并且<em class="mc">平均</em>奖励是比<em class="mc">贴现</em>奖励更好的衡量标准。另一个有趣的方法是放弃固定贴现率的概念，转而使用依赖于州的贴现率。考虑到人类的偏好，一个全新的世界就展现出来了。</p><p id="476d" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">的确，那个小小的参数<em class="mc"> γ </em>隐藏了很大的深度。</p><h2 id="bdb2" class="md me it bd mf mg mh dn mi mj mk dp ml lp mm mn mo lt mp mq mr lx ms mt mu mv bi translated">外卖食品</h2><ul class=""><li id="7fd9" class="ni nj it li b lj mw lm mx lp nk lt nl lx nm mb nn no np nq bi translated">贴现通常是解决无限期问题所必需的。折扣率<em class="mc"> γ &lt; 1 </em>确保了回报的几何级数收敛。</li><li id="6038" class="ni nj it li b lj nr lm ns lp nt lt nu lx nv mb nn no np nq bi translated">从金融学中，我们了解到贴现反映了时间价值和风险补偿。像在强化学习中一样，指数贴现通常是假定的。</li><li id="3933" class="ni nj it li b lj nr lm ns lp nt lt nu lx nv mb nn no np nq bi translated">人类天生倾向于对未来回报进行贴现(接近双曲线贴现)，风险率是一个重要的生物学原理。</li><li id="a690" class="ni nj it li b lj nr lm ns lp nt lt nu lx nv mb nn no np nq bi translated">对于有限范围的问题，是否需要贴现很大程度上取决于问题的性质和建模者的偏好。</li><li id="86c2" class="ni nj it li b lj nr lm ns lp nt lt nu lx nv mb nn no np nq bi translated">随机环境要求贴现率不那么强调未来回报；当前奖励对遥远未来的影响很小。贴现率隐含地反映了你希望预测的时间步数。</li></ul></div><div class="ab cl kz la hx lb" role="separator"><span class="lc bw bk ld le lf"/><span class="lc bw bk ld le lf"/><span class="lc bw bk ld le"/></div><div class="im in io ip iq"><h2 id="2872" class="md me it bd mf mg mh dn mi mj mk dp ml lp mm mn mo lt mp mq mr lx ms mt mu mv bi translated">进一步阅读</h2><p id="ec9c" class="pw-post-body-paragraph lg lh it li b lj mw ju ll lm mx jx lo lp my lr ls lt mz lv lw lx na lz ma mb im bi translated">达斯古普塔和马斯金(2005 年)。不确定性和双曲线贴现。<a class="ae ky" href="https://scholar.harvard.edu/files/maskin/files/uncertainty_and_hyperbolic_discounting_aer.pdf" rel="noopener ugc nofollow" target="_blank">https://scholar . Harvard . edu/files/maskin/files/uncertainty _ and _ 双曲线 _ 贴现 _aer.pdf </a></p><p id="a1b9" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">Fedus，w .，Gelada，C .，Bengio，y .，Bellemare，m .和 Larochelle，H. (2019)。双曲线贴现和在多个时间跨度上的学习。【https://arxiv.org/pdf/1902.06865.pdf T2】号</p><p id="1332" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">Investopedia (2021 年)。折扣率。【https://www.investopedia.com/terms/d/discountrate.asp T4】</p><p id="72cc" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">Investopedia (2021 年)。风险溢价。<a class="ae ky" href="https://www.investopedia.com/terms/r/riskpremium.asp" rel="noopener ugc nofollow" target="_blank">https://www.investopedia.com/terms/r/riskpremium.asp</a></p><p id="b44d" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">卡尼曼博士(2017)。思考，忽快忽慢。</p><p id="0482" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">纳伊克，a .，沙里夫，r .，Yasui，n .，姚，h .和萨顿，R. (2019)。折扣强化学习不是一个优化问题。<a class="ae ky" href="https://arxiv.org/pdf/1910.02140.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1910.02140.pdf</a></p><p id="8bfb" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">Pitus，S. (2019 年)。再思考强化学习中的折扣因子:决策理论方法。<a class="ae ky" href="https://arxiv.org/pdf/1902.02893.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1902.02893.pdf</a></p><p id="b6c1" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">StackExchange (2016)。理解折扣因子在强化学习中的作用。<a class="ae ky" href="https://stats.stackexchange.com/questions/221402/understanding-the-role-of-the-discount-factor-in-reinforcement-learning" rel="noopener ugc nofollow" target="_blank">https://stats . stack exchange . com/questions/221402/understanding-the-role-of-discount-factor-in-reinforcement-learning</a></p><p id="564e" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">维基百科(2021)。几何级数。<a class="ae ky" href="https://en.wikipedia.org/wiki/Geometric_series" rel="noopener ugc nofollow" target="_blank">https://en.wikipedia.org/wiki/Geometric_series</a></p></div></div>    
</body>
</html>