<html>
<head>
<title>A Deep Learning Approach in Predicting the Next Word(s)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">预测下一个单词的深度学习方法</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/a-deep-learning-approach-in-predicting-the-next-word-s-7b0ee9341bfe?source=collection_archive---------8-----------------------#2021-06-04">https://towardsdatascience.com/a-deep-learning-approach-in-predicting-the-next-word-s-7b0ee9341bfe?source=collection_archive---------8-----------------------#2021-06-04</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="f343" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">教计算机预测句子中的下一组单词</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/2862380312c690e17b47d88451bd3857.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*OTviEvFaPh0xnTiL"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">Szabo Viktor 在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="d0a8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在本教程中，我们将逐步完成构建深度学习模型的过程，该模型用于预测种子短语后面的下一个单词。例如，在我们键入“候选人是”之后，我们将要求计算机预测接下来的10个单词。</p><p id="ec9e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">虽然智能手机中用于帮助发送短信的尖端模型要复杂得多，但本文应该会让您对这一预测(分类)任务中涉及的方法有一个大致的了解。</p><h2 id="eb8f" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated">本文将回顾:</h2><ol class=""><li id="a800" class="mo mp it lb b lc mq lf mr li ms lm mt lq mu lu mv mw mx my bi translated">文本处理:标记化、n_gram排序、工程特征和标签以及单词嵌入</li><li id="8e21" class="mo mp it lb b lc mz lf na li nb lm nc lq nd lu mv mw mx my bi translated">建立双向LSTM模型</li><li id="97ab" class="mo mp it lb b lc mz lf na li nb lm nc lq nd lu mv mw mx my bi translated">使用我们的模型基于种子短语预测单词</li></ol><h1 id="cb9d" class="ne lw it bd lx nf ng nh ma ni nj nk md jz nl ka mg kc nm kd mj kf nn kg mm no bi translated">我们来编码吧！</h1><p id="5892" class="pw-post-body-paragraph kz la it lb b lc mq ju le lf mr jx lh li np lk ll lm nq lo lp lq nr ls lt lu im bi translated">本教程的所有开发都是使用Google Colab的数据处理功能完成的。你当然可以在本地机器上运行这段代码，但是除非你有一个专用的GPU，否则训练时间可能会相当长。</p><p id="4c80" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在本教程中，我们将使用用户对纽约时报文章<a class="ae ky" href="https://www.kaggle.com/aashita/nyt-comments?select=ArticlesMarch2018.csv" rel="noopener ugc nofollow" target="_blank">链接</a>的评论片段。由于这只是一个教程，自然语言数据在计算上非常昂贵，这个数据集的有限范围非常适合我们的需要。</p><p id="e525" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们加载所需的库，因为训练数据驻留在Google Drive上，所以我们将gdrive挂载到Colab。最后，我们将把目录更改为存放训练数据的位置。请注意，将您的Google Drive与Colab链接会自动在您的Google Drive中创建“Colab笔记本”目录。由于“Colab”和“Notebooks”之间的空格，您的路径目录可能略有不同。例如，注意用于指定空间“/content/drive/my drive/Colab \ Notebooks”的反斜杠</p><pre class="kj kk kl km gt ns nt nu nv aw nw bi"><span id="39f6" class="lv lw it nt b gy nx ny l nz oa">import csv<br/>import nltk<br/>import string<br/>import pandas as pd<br/>import numpy as np<br/>from google.colab import drive<br/>from keras import backend as K<br/>import tensorflow as tf<br/>from tensorflow.keras.preprocessing.sequence import pad_sequences<br/>from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional<br/>from tensorflow.keras.preprocessing.text import Tokenizer<br/>from tensorflow.keras.models import Sequential<br/>from tensorflow.keras.optimizers import Adam</span><span id="4611" class="lv lw it nt b gy ob ny l nz oa">drive.mount('/content/drive/')</span><span id="6ad6" class="lv lw it nt b gy ob ny l nz oa">%cd '/content/drive/MyDrive/Colab Notebooks'</span></pre><p id="71f9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">确定哪些数据将用于训练我们的深度学习模型非常重要。在尝试构建现代文本预测模型时，您显然不希望使用莎士比亚的作品来训练您的模型。你的模型将会学习莎士比亚所写的那些不能有效地翻译成今天的现代英语的单词的上下文细微差别。使用最有可能反映你希望预测的句子类型的数据集将产生最佳结果。也就是说，利用非常大的广义文本数据集，想想维基百科，也可以产生准确的结果。</p><p id="53eb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们将使用Pandas读入CSV文件并打印出前5行。我们将使用突出显示的“snippet”列，其中包含用户评论的摘录。我们的数据有很强的政治和全球新闻内涵，但对于本教程来说，这就足够了。</p><pre class="kj kk kl km gt ns nt nu nv aw nw bi"><span id="3358" class="lv lw it nt b gy nx ny l nz oa">path = '/content/drive/MyDrive/Colab Notebooks/ArticlesMarch2018.csv'</span><span id="7146" class="lv lw it nt b gy ob ny l nz oa">df = pd.read_csv(path)</span><span id="3dd9" class="lv lw it nt b gy ob ny l nz oa">df.head()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oc"><img src="../Images/3aca487bbf267f91fdf722cf80e14a50.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fb5HeED1QCqiIjIA5ZN_qQ.png"/></div></div></figure><h1 id="3b79" class="ne lw it bd lx nf ng nh ma ni nj nk md jz nl ka mg kc nm kd mj kf nn kg mm no bi translated">文本处理</h1><p id="1962" class="pw-post-body-paragraph kz la it lb b lc mq ju le lf mr jx lh li np lk ll lm nq lo lp lq nr ls lt lu im bi translated">在构建模型之前，我们需要对数据应用一些处理步骤。首先，让我们将所有片段组合成一系列字符串。</p><pre class="kj kk kl km gt ns nt nu nv aw nw bi"><span id="382e" class="lv lw it nt b gy nx ny l nz oa">snippet = '\n'.join(df['snippet'])</span><span id="4c13" class="lv lw it nt b gy ob ny l nz oa">print(type(snippet))</span><span id="e560" class="lv lw it nt b gy ob ny l nz oa">print(snippet)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi od"><img src="../Images/0f53abf877e4c535c97846c6cec77b39.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fJgPZ8zVlK-kuBp0Efdv3g.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">一个由外部片段组成的长串联字符串</p></figure><p id="9b86" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">接下来，我们将把任何大写的单词转换成小写，以减少我们的词汇量。否则，我们会有重复的单词，唯一的区别是大写字母(即。美洲/美国)。注意lower()函数将我们的字符串转换成了一个字符串列表。</p><p id="70e5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们的语料库包含1385个独特的片段。</p><pre class="kj kk kl km gt ns nt nu nv aw nw bi"><span id="c513" class="lv lw it nt b gy nx ny l nz oa">corpus = snippet.lower().split('\n')</span><span id="cdd2" class="lv lw it nt b gy ob ny l nz oa">print(len(corpus))</span><span id="f415" class="lv lw it nt b gy ob ny l nz oa">print(type(corpus))</span><span id="3b05" class="lv lw it nt b gy ob ny l nz oa">print(corpus[:2])</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oe"><img src="../Images/5c802d58074b1320a4c033ba8665502d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1108/format:webp/1*F4v6E9Hpx-l0KHTV-q3n4g.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">包含我们的片段字符串的列表</p></figure><h2 id="1d7d" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated">标记器</h2><p id="ffc1" class="pw-post-body-paragraph kz la it lb b lc mq ju le lf mr jx lh li np lk ll lm nq lo lp lq nr ls lt lu im bi translated">由于计算机不能处理原始文本数据，我们需要对我们的语料库进行标记，以将文本转换为数值。Keras的Tokenizer类根据词频转换文本，其中最常见的词的标记值为1，其次是最常见的词的标记值为2，依此类推。结果是一个字典，其中包含唯一单词的键/值对及其基于词频确定的分配标记。</p><p id="dd3b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们的词汇表中有6862个独特的单词，外加(1)个词汇表之外的单词。注意，Keras的Tokenizer类自动从我们的语料库中删除所有标点符号。通常，在处理NLP预测任务时，我们会删除所谓的“停用词”或对句子没有太大意义的术语(即。那个、他、在、为等。).然而，由于我们试图预测类似人类语言的句子，我们将保留停用词。</p><pre class="kj kk kl km gt ns nt nu nv aw nw bi"><span id="6344" class="lv lw it nt b gy nx ny l nz oa">tokenizer = Tokenizer()<br/>tokenizer.fit_on_texts(corpus)<br/>word_index = tokenizer.word_index<br/>total_unique_words = len(tokenizer.word_index) + 1 </span><span id="4088" class="lv lw it nt b gy ob ny l nz oa">print(total_unique_words)<em class="of"><br/></em>print(word_index)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi og"><img src="../Images/c99e72f8e2f29e783cf10289f9fb8e87.png" data-original-src="https://miro.medium.com/v2/resize:fit:1298/format:webp/1*6VYxYgpF9EgspaWmgFKFQg.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">独一无二的单词及其标记化版本的单词索引词典</p></figure><p id="e942" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">虽然下面的代码对于我们的分析来说不是必需的，但是为了准确起见，比较实际的文本和它的标记化版本是一个很好的实践。</p><p id="888e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">通过应用下面的代码，我们可以看到我们整个标记化的语料库。例如，第一个片段以<em class="of">“美国有一个生产力问题”</em>开始，这个问题已经被标记为【193，14，2，2796，699】。检查上面的word_index字典，我们看到字母“a”已经用数字2进行了标记，这与标记的第一个代码片段相对应。</p><pre class="kj kk kl km gt ns nt nu nv aw nw bi"><span id="8614" class="lv lw it nt b gy nx ny l nz oa">for line in corpus:<br/>   seqs = tokenizer.texts_to_sequences([line])[0]</span><span id="bdd3" class="lv lw it nt b gy ob ny l nz oa">print(seqs)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/0f5230a2c153d37c2f25bc852b65da8b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1332/format:webp/1*n3PF0wec3Akr0DB6BRxF0A.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">标记化代码片段示例</p></figure><h2 id="fea0" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated">n克</h2><p id="6898" class="pw-post-body-paragraph kz la it lb b lc mq ju le lf mr jx lh li np lk ll lm nq lo lp lq nr ls lt lu im bi translated">这是本教程的一个棘手的部分。在典型的监督回归或分类问题中，我们的数据集将包含x值(即输入要素)和y_values(即。标签)，这允许模型学习我们的特征中与我们的标签相关的独特模式。我们的数据集似乎缺少y_values(即标签)。不要担心，n_grams帮助我们分割数据，以获得我们的标签。n_gram是长度为n的单词序列。例如，“芝加哥小熊队”是长度为3的n_gram(即。3 _克)。</p><h2 id="a7bf" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated">多对一N元序列</h2><p id="f34b" class="pw-post-body-paragraph kz la it lb b lc mq ju le lf mr jx lh li np lk ll lm nq lo lp lq nr ls lt lu im bi translated">我们将把这个概念向前推进一步，迭代每个标记化的片段，以创建n_gram+1大小的n_gram。有时这被称为“多对一”序列图。例如，我们在第一个代码片段中的前五个标记化单词(即。<em class="of">美国有生产力问题)</em>有【193，14，2，2796，699】<em class="of">。</em>通过创建n_gram+1个n_gram序列，我们得到下面的列表。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oi"><img src="../Images/811e5dba23a84376b66669839e7a47a1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1062/format:webp/1*4qJlCx_I0v_KV6S_5EEuKw.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">请记住，这种多对一的方法适用于每个片段，而不是整个语料库。因此，排序后的n_grams从每个新片段开始。</p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/c7a4e185307d23294cf8dbae1ad973cb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*ylg5OtdYe_joPzKFUVaN1A.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">假代币例句</p></figure><p id="2080" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们对所有的片段使用这种方法，最终产生了将近27，000个n _ grams。让我们更详细地检查一下代码。首先，我们创建一个空列表来保存上面描述的n_gram序列。接下来，我们遍历未分词片段的语料库，并对每个未分词片段应用“texts_to_sequences”方法。正如我们在上面看到的,“texts_to_sequences”方法只是将每个片段转换成它的标记化版本。接下来，我们迭代每个标记，从第二个标记(index=1)开始，直到所有剩余的标记。在每次迭代中，我们将n_gram的序列附加到input_sequences，并扩展n_gram+1。</p><pre class="kj kk kl km gt ns nt nu nv aw nw bi"><span id="133e" class="lv lw it nt b gy nx ny l nz oa">input_sequences = []</span><span id="9f8f" class="lv lw it nt b gy ob ny l nz oa">for line in corpus:<br/>  token_list = tokenizer.texts_to_sequences([line])[0]<br/>    for i in range(1, len(token_list)): <br/>      n_gram_seqs = token_list[:i+1]<br/>      input_sequences.append(n_gram_seqs)</span><span id="ef18" class="lv lw it nt b gy ob ny l nz oa">print(len(input_sequences))<br/>print(input_sequences)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ok"><img src="../Images/6d388c79dea404b3ab8cb39d21137495.png" data-original-src="https://miro.medium.com/v2/resize:fit:1050/format:webp/1*z5VdJ4JuUCmrP78eEWf3_A.png"/></div></figure><h2 id="2441" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated">N_Grams的填充</h2><p id="b9e5" class="pw-post-body-paragraph kz la it lb b lc mq ju le lf mr jx lh li np lk ll lm nq lo lp lq nr ls lt lu im bi translated">请注意，n_grams长度不同，我们的模型要求n_grams的每个输入序列长度相同。因此，我们需要将每个n_gram序列用零“填充”到最长n_gram的长度。Max_seq_length将标识最长n_gram序列的长度(即41).那么“pad_sequences”方法将在我们的令牌之前添加零(即padding='pre)到max_seq_lenth(即。41).</p><pre class="kj kk kl km gt ns nt nu nv aw nw bi"><span id="0527" class="lv lw it nt b gy nx ny l nz oa">max_seq_length = max([len(x) for x in input_sequences])<br/>input_seqs = np.array(pad_sequences(input_sequences, maxlen=max_seq_length, padding='pre'))</span><span id="b76a" class="lv lw it nt b gy ob ny l nz oa">print(max_seq_length)<br/>print(input_seqs[:5])</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ol"><img src="../Images/9e5de8c7b7b3ab5bf2262b71cab16dce.png" data-original-src="https://miro.medium.com/v2/resize:fit:1098/format:webp/1*0ZCzBo0iWkKPR3mPj8U-FQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">长度为41的填充n_gram序列</p></figure><h2 id="2bb1" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated">创建要素和标签</h2><p id="e690" class="pw-post-body-paragraph kz la it lb b lc mq ju le lf mr jx lh li np lk ll lm nq lo lp lq nr ls lt lu im bi translated">现在我们已经填充创建和填充了n_gram序列，我们可以提取我们的特征和标签。回想一下，每个n_gram序列的长度是41个值。前40个值将是我们的特性，第41个值将是我们的标签。理论上，这是一个多类分类问题，因为模型将学习我们单词之间的关系，然后提供关于哪个单词应该是我们序列中的下一个单词的概率。该模型只能提供它见过的单词的概率(即唯一单词总数:6863)。由于这是一个多分类问题，我们也将对我们的标签应用一次性编码。下面我们打印了前三个特征和标签(没有一次性编码)来展示我们的方法。</p><pre class="kj kk kl km gt ns nt nu nv aw nw bi"><span id="535f" class="lv lw it nt b gy nx ny l nz oa">x_values, labels = input_seqs[:, :-1], input_seqs[:, -1]</span><span id="74a1" class="lv lw it nt b gy ob ny l nz oa">y_values = tf.keras.utils.to_categorical(labels, num_classes=total_unique_words)\</span><span id="b01d" class="lv lw it nt b gy ob ny l nz oa">print(x_values[:3])<br/>print(labels[:3])</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi om"><img src="../Images/5cb179430eca3188515491de2c22afcd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1122/format:webp/1*lSZluVF9Ho0tXdJww4v3WQ.png"/></div></figure><p id="337e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们的一次性编码标签(即y_values)的形状为26，937(即测序的n _ grams的数量)乘以6，863(即语料库中唯一单词的数量)。除了第一个单词，我们语料库中的每个单词都是一个特征和标签，因为模型将学习哪些单词更有可能和更不可能跟随其他单词序列。</p><h2 id="6e8a" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated">单词嵌入</h2><p id="716c" class="pw-post-body-paragraph kz la it lb b lc mq ju le lf mr jx lh li np lk ll lm nq lo lp lq nr ls lt lu im bi translated">在这些类型的NLP问题中，游戏的名称是“上下文”。我们如何优化我们的战略(即。数据处理和模型复杂性)以使我们的模型能够更好地学习单词之间的关系。以目前的形式，我们的数据为我们的模型学习提供了前进的背景。换句话说，由于我们的n_gram序列，我们知道哪些单词更有可能或更不可能跟在其他单词后面。然而，用于表示每个单词的标记只不过是频率计数，其提供了关于单词之间的上下文信息的模型有限信息。例如，单词“good”和“great”在意思上非常相似，但它们的标记值却是(299，673)。</p><pre class="kj kk kl km gt ns nt nu nv aw nw bi"><span id="c140" class="lv lw it nt b gy nx ny l nz oa">print(tokenizer.word_index['good'])</span><span id="6ddb" class="lv lw it nt b gy ob ny l nz oa">print(tokenizer.word_index['great'])</span></pre><p id="3610" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">单词嵌入使我们能够在n维空间中表示单词，其中诸如“good”和“great”之类的单词在这个n维空间中具有计算机能够理解的相似表示。在下图中，我们可以看到单词的嵌入(7维),如狗、小狗、猫、房子、男人、女人、国王和王后。维度对我们来说是未知的(不可解释的),因为模型在迭代数据时会学习这些维度。也就是说，为了帮助读者理解单词嵌入模型，让我们假设维度是“生物、猫、人、性别、皇室、动词、复数”。我们可以看到“房子”有一个-0.8嵌入在D1(即。而所有其他单词在“生物”维度上得分相对较高。通过学习这些嵌入，计算机能够更好地理解单词之间的上下文关系，因此应该能够更好地预测种子短语之后的下一个单词。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi on"><img src="../Images/654a9b057372bdea80ec0df2fbc2a005.png" data-original-src="https://miro.medium.com/v2/resize:fit:1386/format:webp/1*9ATq3g4mqIlm3jMzXh-zgg.png"/></div></figure><p id="7b34" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">Keras有一个“嵌入”层，可以基于我们的语料库构建自己的单词嵌入，但我们将利用斯坦福大学名为“GloVe”(单词表示的全局向量)的预训练单词嵌入模型。手套嵌入有几种风格，但我们将使用100维版本。您需要从这个<a class="ae ky" href="https://nlp.stanford.edu/projects/glove/" rel="noopener ugc nofollow" target="_blank">链接</a>下载预先训练好的模型。这个模型被训练了十亿个单词。独特的话)40万字。</p><p id="d16f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">首先，让我们处理嵌入文本文件并生成一个字典，其中包含作为键的单词/字符和作为值的100维数组。</p><pre class="kj kk kl km gt ns nt nu nv aw nw bi"><span id="8dbe" class="lv lw it nt b gy nx ny l nz oa">path = '/content/drive/MyDrive/Colab Notebooks/glove.6B.100d.txt'</span><span id="f1d1" class="lv lw it nt b gy ob ny l nz oa">embeddings_index = {}</span><span id="4c45" class="lv lw it nt b gy ob ny l nz oa">with open(path) as f:<br/>  for line in f:<br/>    values = line.split()<br/>    word = values[0]<br/>    coeffs = np.array(values[1:], dtype='float32')<br/>    embeddings_index[word] = coeffs</span><span id="f25e" class="lv lw it nt b gy ob ny l nz oa">dict(list(embeddings_index.items())[0:2])</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oo"><img src="../Images/d02593d38a43389daefad3ca6cb044e0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1232/format:webp/1*eCb0CcMikn6iT66b02QQ8A.png"/></div></figure><p id="8fe5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在让我们创建一个矩阵，其中包含手套词嵌入(即。100维数组)仅用于我们词汇表中的单词。首先，我们创建一个形状为6863的零矩阵(即我们的语料库中唯一单词的总数)乘以100(即手套包含100个尺寸)。然后，我们将遍历“word_index”字典，该字典包含您的语料库中的唯一单词作为键，以及它们相应的标记作为值。在word_index字典的每次迭代中，我们将获取相应的手套嵌入(即100维数组)存储在“嵌入_索引”中，并更新嵌入_矩阵(即用100维数组替换零)。</p><pre class="kj kk kl km gt ns nt nu nv aw nw bi"><span id="d998" class="lv lw it nt b gy nx ny l nz oa">embeddings_matrix = np.zeros((total_unique_words, 100))</span><span id="fc17" class="lv lw it nt b gy ob ny l nz oa">for word, i in word_index.items():<br/>   embedding_vector = embeddings_index.get(word)<br/>   if embedding_vector is not None:<br/>     embeddings_matrix[i] = embedding_vector;</span></pre><h1 id="ec8c" class="ne lw it bd lx nf ng nh ma ni nj nk md jz nl ka mg kc nm kd mj kf nn kg mm no bi translated">构建我们的模型</h1><p id="ba09" class="pw-post-body-paragraph kz la it lb b lc mq ju le lf mr jx lh li np lk ll lm nq lo lp lq nr ls lt lu im bi translated">我们将构建一个相对较浅的模型，由一个嵌入层、三个LSTM层、三个分离层和一个完全连接的密集层组成。作为嵌入层的第一层将使我们能够利用预训练的手套单词嵌入系数/权重。嵌入层需要唯一字总数的<strong class="lb iu"> input_dim </strong>(即词汇量的大小)和一个<strong class="lb iu"> output_dim </strong>，它指定了我们想要的单词嵌入维数。因为我们使用手套100维，我们的out_dim参数将是100。我们将把embeddings_matrix传递到权重参数中，以便使用手套权重，并将可训练参数设置为“False”，否则，我们将重新训练手套权重。最后，我们将把<strong class="lb iu">输入长度</strong>参数设置为“最大序列长度-1”。</p><h2 id="f681" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated">递归神经网络</h2><p id="2459" class="pw-post-body-paragraph kz la it lb b lc mq ju le lf mr jx lh li np lk ll lm nq lo lp lq nr ls lt lu im bi translated">回想一下，帮助您的模型理解上下文或单词之间的关系最终会产生一个性能更好的模型。我们使用n_gram序列和单词嵌入来帮助我们的模型学习这些关系。我们现在可以利用一个特别适合于检查单词序列之间关系的特定模型结构。在传统的神经网络中，输入和预测/输出彼此完全独立。另一方面，我们试图预测句子中的下一个单词，这意味着网络需要“记住”来自以前单词的上下文信息。</p><p id="67c5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">一个<strong class="lb iu">递归神经网络(RNN) </strong>模型架构结合了以前的数据来做出未来的预测。换句话说，RNNs有一个内置的记忆功能，它存储来自前一个单词的信息，并在预测下一个单词时使用该信息。也就是说，简单的rnn很难“记住”序列中较早学习到的信息。换句话说，给定一个单词的句子，RNN可以合并几个相邻单词之间的信息，但是随着RNN继续迭代句子，句子中第一个和后面的单词之间的关系开始失去它们的意义。当您有一个包含几十或几百个隐藏层非常深的模型时，这尤其重要。</p><p id="87fd" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">一种<strong class="lb iu">长短期记忆(LSTM) </strong>模型通过学习单词之间的关系并允许重要的关系通过网络传播来解决这个问题。这样，在一个序列的前几个单词中学习到的信息可以影响序列/句子、段落、章节等中的进一步预测。此外，我们将利用双向LSTM，它将学习从左到右和从右到左阅读句子的单词之间的关系。</p><p id="7437" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们的LSTM层的输入总是三维的(即批量大小，最大序列长度，特征数量)。<strong class="lb iu"> batch_size </strong>将被设置为<strong class="lb iu"> None </strong>，这简单地指定该层将接受任意大小的批处理。<strong class="lb iu"> Max_seq_length是40 </strong>,因为所有的代码片段都已被标记化并填充为41减1作为我们的标签。最后，我们有<strong class="lb iu"> 100个特征</strong>，因为手套有100个尺寸。</p><p id="d342" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们将添加三个<a class="ae ky" href="https://machinelearningmastery.com/dropout-regularization-deep-learning-models-keras/" rel="noopener ugc nofollow" target="_blank"> <strong class="lb iu">删除层</strong> </a>(删除30%)来帮助模型过度拟合训练数据。</p><p id="8886" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">深度学习是一种徒劳的练习，因为你可以在剩余的时间里调整参数。选择正确的优化器只是我们可以调整的另一个参数。我们最终选择了Adam，因为它在直接模型准确性和效率方面都优于SGD和RMSProp，因为损失减少的速度几乎是SGD和RMSProp的两倍。</p><p id="df17" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">由于该模型的输出可以是6863中的1(即total_unique_words)单词，这是一个多类分类问题，这就是为什么我们使用<strong class="lb iu">分类交叉熵</strong>作为我们的损失函数。</p><pre class="kj kk kl km gt ns nt nu nv aw nw bi"><span id="7911" class="lv lw it nt b gy nx ny l nz oa">K.clear_session()</span><span id="f7c6" class="lv lw it nt b gy ob ny l nz oa">model = tf.keras.Sequential([</span><span id="c5cc" class="lv lw it nt b gy ob ny l nz oa">tf.keras.layers.Embedding(input_dim = total_unique_words, output_dim=100, weights=[embeddings_matrix], input_length=max_seq_length-1, trainable=False),</span><span id="0c66" class="lv lw it nt b gy ob ny l nz oa">tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(256, return_sequences=True)),</span><span id="4451" class="lv lw it nt b gy ob ny l nz oa">tf.keras.layers.Dropout(0.2), </span><span id="c39a" class="lv lw it nt b gy ob ny l nz oa">tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(256)),</span><span id="84f3" class="lv lw it nt b gy ob ny l nz oa">tf.keras.layers.Dropout(0.2),</span><span id="e016" class="lv lw it nt b gy ob ny l nz oa">tf.keras.layers.Dense(128, activation='relu'),</span><span id="af81" class="lv lw it nt b gy ob ny l nz oa">tf.keras.layers.Dense(total_unique_words , activation='softmax')])</span><span id="9373" class="lv lw it nt b gy ob ny l nz oa">model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])</span><span id="1e5c" class="lv lw it nt b gy ob ny l nz oa">tf.keras.utils.plot_model(model, show_shapes=True)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi op"><img src="../Images/0f03e09b24576e7187595ac1d93cc89b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1080/format:webp/1*0L8UTyegqXkLQvemT0v2HQ.png"/></div></div></figure><p id="1af2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">最后，我们将使用x值和y值来拟合/训练模型。选择batch_size是我们可以调整的另一个参数。因为我们有26，937个n _ gram来训练我们的模型，所以一次加载所有26，937个n _ gram是非常低效的。让我们针对120个时期训练我们的模型，其中1个时期需要所有26，937个n _ grams运行模型一次。因此，26937(即。27，000)除以120意味着每批大小应为225 n克。然而，为了充分利用我们的内存，我们将使用batch _ size 256。我们还将对训练和验证数据应用80/20分割。</p><pre class="kj kk kl km gt ns nt nu nv aw nw bi"><span id="2adc" class="lv lw it nt b gy nx ny l nz oa">history = model.fit(x_values, y_values, epochs=120, validation_split=0.2, verbose=1, batch_size=256)</span></pre><h1 id="64bf" class="ne lw it bd lx nf ng nh ma ni nj nk md jz nl ka mg kc nm kd mj kf nn kg mm no bi translated">模型评估</h1><p id="90b0" class="pw-post-body-paragraph kz la it lb b lc mq ju le lf mr jx lh li np lk ll lm nq lo lp lq nr ls lt lu im bi translated">我们的模型肯定是过度拟合了我们的训练数据，因为训练精度继续提高，而验证精度却停滞不前。性能良好的NLP模型非常需要数据。拥有数千万单词的数据集并不罕见，而我们的语料库只有6863个单词。我们可以添加额外的LSTM层，更多的正则化技术以及更多的数据来改进模型，但本教程旨在为读者提供如何构建和建立单词预测模型的一般概念。也就是说，让我们看看我们的模型预测实际的新片段有多好。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oq"><img src="../Images/2649d2bf0b5ad7dfb8c0945c8ff77ea2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1056/format:webp/1*mghjhKhcCq1ZOiuOGSd2Qw.png"/></div></figure><h1 id="6095" class="ne lw it bd lx nf ng nh ma ni nj nk md jz nl ka mg kc nm kd mj kf nn kg mm no bi translated">种子文本预测</h1><p id="6ba0" class="pw-post-body-paragraph kz la it lb b lc mq ju le lf mr jx lh li np lk ll lm nq lo lp lq nr ls lt lu im bi translated">让我们来看看我们的模型能在多大程度上推广到以前从未见过的新闻片段。我们将用上个月每个实际新闻片段的前几个词作为模型的种子，并比较模型的预测。</p><pre class="kj kk kl km gt ns nt nu nv aw nw bi"><span id="c96a" class="lv lw it nt b gy nx ny l nz oa">def prediction(<em class="of">seed_text</em>, <em class="of">next_words</em>): <br/>  for _ in range(next_words):<br/>  token_list = tokenizer.texts_to_sequences([seed_text])[0]<br/>  token_list = pad_sequences([token_list], maxlen=max_seq_length-1, padding='pre')</span><span id="b919" class="lv lw it nt b gy ob ny l nz oa">  predicted = np.argmax(model.predict(token_list, verbose=1), axis=-1)</span><span id="0c3a" class="lv lw it nt b gy ob ny l nz oa">  ouput_word = ""</span><span id="0af6" class="lv lw it nt b gy ob ny l nz oa">  for word, index in tokenizer.word_index.items():<br/>    if index == predicted:<br/>      output_word = word<br/>      break<br/>  seed_text += ' '+output_word<br/>  print(seed_text)</span><span id="bf15" class="lv lw it nt b gy ob ny l nz oa">seed_phrase = "I understand that they could meet"<br/>next_words = len("with us, patronize us and do nothing in the end".split())</span><span id="362f" class="lv lw it nt b gy ob ny l nz oa">prediction(seed_text, next_words)</span></pre><p id="8ed3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">模型生成的文本当然是围绕全球新闻和政治的。我们看到诸如“一代”、“政府”、“借贷”、“监管”、“政策”等词，这些词符合模型学习的数据类型。不幸的是，从人类片段中识别模型生成的文本并不困难。此外，当我们使用一个上下文非常不同的种子短语“我真的很喜欢骑我的摩托车”时，模型很难对一个类似于准确英语句子的句子进行分类。</p><h2 id="e81e" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated">实际新闻片段1</h2><p id="5084" class="pw-post-body-paragraph kz la it lb b lc mq ju le lf mr jx lh li np lk ll lm nq lo lp lq nr ls lt lu im bi translated">“我知道他们可能会见我们，资助我们，但最终什么也不做”</p><h2 id="9e32" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated">种子短语1</h2><p id="1617" class="pw-post-body-paragraph kz la it lb b lc mq ju le lf mr jx lh li np lk ll lm nq lo lp lq nr ls lt lu im bi translated">“我知道他们可以见面”</p><h2 id="0b17" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated">模型生成1</h2><p id="6463" class="pw-post-body-paragraph kz la it lb b lc mq ju le lf mr jx lh li np lk ll lm nq lo lp lq nr ls lt lu im bi translated">“我知道他们可以应对你们这一代人的挑战，也知道你们为什么要借钱”</p></div><div class="ab cl or os hx ot" role="separator"><span class="ou bw bk ov ow ox"/><span class="ou bw bk ov ow ox"/><span class="ou bw bk ov ow"/></div><div class="im in io ip iq"><h2 id="e8c8" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated">实际新闻片段2</h2><p id="8ecc" class="pw-post-body-paragraph kz la it lb b lc mq ju le lf mr jx lh li np lk ll lm nq lo lp lq nr ls lt lu im bi translated">“该机构计划在周二公布一项新规定，该规定将限制该机构在制定政策时可以使用的科学研究种类。”</p><h2 id="4b6c" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated">种子短语2</h2><p id="8003" class="pw-post-body-paragraph kz la it lb b lc mq ju le lf mr jx lh li np lk ll lm nq lo lp lq nr ls lt lu im bi translated">"该机构计划在周二发布一项新的规定，限制."</p><h2 id="9448" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated">模型生成2</h2><p id="cd1f" class="pw-post-body-paragraph kz la it lb b lc mq ju le lf mr jx lh li np lk ll lm nq lo lp lq nr ls lt lu im bi translated">“该机构计划在周二公布一项新的规定，将限制政府在9月份发生的事情，他们必须是第三个”</p></div><div class="ab cl or os hx ot" role="separator"><span class="ou bw bk ov ow ox"/><span class="ou bw bk ov ow ox"/><span class="ou bw bk ov ow"/></div><div class="im in io ip iq"><h2 id="756f" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated">实际新闻片段3</h2><p id="1b0e" class="pw-post-body-paragraph kz la it lb b lc mq ju le lf mr jx lh li np lk ll lm nq lo lp lq nr ls lt lu im bi translated">“支持增加限制的枪支拥有者可能是一个被忽视的群体。一些人变得更加直言不讳，游行并证明支持限制。</p><h2 id="e3bb" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated">种子短语3</h2><p id="83e3" class="pw-post-body-paragraph kz la it lb b lc mq ju le lf mr jx lh li np lk ll lm nq lo lp lq nr ls lt lu im bi translated">“支持增加限制的枪支拥有者”</p><h2 id="9b9d" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated">模型生成3</h2><p id="43a8" class="pw-post-body-paragraph kz la it lb b lc mq ju le lf mr jx lh li np lk ll lm nq lo lp lq nr ls lt lu im bi translated">支持增加对专家限制的枪支所有者最近计划对钢铁和铝关税征收全面关税，同时</p></div><div class="ab cl or os hx ot" role="separator"><span class="ou bw bk ov ow ox"/><span class="ou bw bk ov ow ox"/><span class="ou bw bk ov ow"/></div><div class="im in io ip iq"><h2 id="db44" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated">自生成种子短语</h2><p id="4b03" class="pw-post-body-paragraph kz la it lb b lc mq ju le lf mr jx lh li np lk ll lm nq lo lp lq nr ls lt lu im bi translated">“我真的很喜欢骑摩托车”</p><h2 id="211e" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated">模型生成短语</h2><p id="dc04" class="pw-post-body-paragraph kz la it lb b lc mq ju le lf mr jx lh li np lk ll lm nq lo lp lq nr ls lt lu im bi translated">“我真的很喜欢骑摩托车，但每个人都问我讨厌这个问题”</p><h1 id="9d25" class="ne lw it bd lx nf ng nh ma ni nj nk md jz nl ka mg kc nm kd mj kf nn kg mm no bi translated">结论</h1><p id="d147" class="pw-post-body-paragraph kz la it lb b lc mq ju le lf mr jx lh li np lk ll lm nq lo lp lq nr ls lt lu im bi translated">在自然语言处理领域，这无疑是一个激动人心的时刻。像OpenAI的“GPT-3”这样的模型正在继续推动我们的自然语言处理能力的进步。这篇教程是对文本分类的简单介绍，我希望它能激发你对这个令人兴奋的领域的兴趣。</p><p id="bd61" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对OpenAI的GPT 3能力的一些见解<a class="ae ky" href="https://prototypr.io/post/gpt-3-design-hype/" rel="noopener" target="_blank">链接</a>。</p></div></div>    
</body>
</html>