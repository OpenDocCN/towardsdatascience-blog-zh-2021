<html>
<head>
<title>A 3-minute read on how to accelerate NLP model inferences on commodity hardware.</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">关于如何在商用硬件上加速NLP模型推理的3分钟阅读。</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/a-3-minute-read-on-how-to-accelerate-nlp-model-inferences-in-commodity-hardware-575a8075d424?source=collection_archive---------36-----------------------#2021-06-21">https://towardsdatascience.com/a-3-minute-read-on-how-to-accelerate-nlp-model-inferences-in-commodity-hardware-575a8075d424?source=collection_archive---------36-----------------------#2021-06-21</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="d92b" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated"><strong class="ak">罗盘导航</strong>地景<strong class="ak">加速推理</strong></h2></div><h2 id="d747" class="kf kg iq bd kh ki kj dn kk kl km dp kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated"><strong class="ak">序幕</strong></h2><p id="d933" class="pw-post-body-paragraph lb lc iq ld b le lf jr lg lh li ju lj ko lk ll lm ks ln lo lp kw lq lr ls lt ij bi lu translated"><span class="l lv lw lx bm ly lz ma mb mc di">一种</span>在使用串行和/或并行专用硬件的数据中心中加速服务器端模型推断被证明是简单的(理解为简单而不容易)。一段时间以来，数据中心一直在使用专门的硬件，即:<em class="md">图形处理单元(GPU)、现场可编程门阵列(FPGAs)和专用集成电路(例如谷歌TPU) </em>，这要感谢云服务提供商，他们使这些硬件变得非常容易访问。现在，像FPGA这样的专用硬件甚至出现在智能手机这样的消费设备上(iPhones上的<a class="ae me" href="https://www.forbes.com/sites/aarontilley/2016/10/17/iphone-7-fpga-chip-artificial-intelligence/#:~:text=An%20FPGA%20is%20a%20type,has%20appeared%20in%20an%20iPhone." rel="noopener ugc nofollow" target="_blank">FPGA</a>)，所以，手机上的ASICs不是想象中的事情。像东京的LeapMind这样的公司正在积极开发专用硬件，用于受约束和边缘设备的训练和加速推理。但使用专门的硬件来加速推理是一个非常狭窄的世界，并留下了巨大的空间。<em class="md">但幸运的是，已经有很多关于在没有任何专门硬件的情况下，即在商品硬件和</em> <strong class="ld ir"> <em class="md">中加速推理的研究，这篇文章将作为一个指南针，帮助你浏览可用的选项。</em>T19】</strong></p><h2 id="93a0" class="kf kg iq bd kh ki kj dn kk kl km dp kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">最大的问题</h2><blockquote class="mf"><p id="762c" class="mg mh iq bd mi mj mk ml mm mn mo lt dk translated">如何在服务器端和客户端(胖客户端和受限/边缘设备)的商用硬件中加速模型推理？</p></blockquote><h2 id="9466" class="kf kg iq bd kh ki mp dn kk kl mq dp kn ko mr kq kr ks ms ku kv kw mt ky kz la bi translated">但是为什么呢？</h2><p id="ac06" class="pw-post-body-paragraph lb lc iq ld b le lf jr lg lh li ju lj ko lk ll lm ks ln lo lp kw lq lr ls lt ij bi translated">由于“<strong class="ld ir">推理预算”</strong>通常非常少，因此有几个原因可以加速商用硬件上的模型推理，尤其是在客户端。此外，由于<strong class="ld ir">延迟</strong>和<strong class="ld ir">带宽</strong>的问题，并不是所有基于ML的解决方案都能提供服务器端推理的最佳用户体验。对于一些应用来说，<strong class="ld ir">数据</strong> <strong class="ld ir">隐私</strong>是另一个挑战:消费者不希望数据像智能手机一样离开他们的设备。像<a class="ae me" href="https://en.wikipedia.org/wiki/Differential_privacy" rel="noopener ugc nofollow" target="_blank"> <strong class="ld ir">差分</strong> <strong class="ld ir">隐私</strong> </a>和<a class="ae me" href="https://en.wikipedia.org/wiki/Federated_learning" rel="noopener ugc nofollow" target="_blank"> <strong class="ld ir">联合</strong> <strong class="ld ir">学习</strong> </a>这样的创新确实有助于缓解担忧，但实现起来很复杂。因此，我们最好在更紧的推理预算内寻找一些操作选项。</p><h1 id="974b" class="mu kg iq bd kh mv mw mx kk my mz na kn jw nb jx kr jz nc ka kv kc nd kd kz ne bi translated"><strong class="ak">基于自然语言处理模型的心理模型</strong></h1><p id="b3c7" class="pw-post-body-paragraph lb lc iq ld b le lf jr lg lh li ju lj ko lk ll lm ks ln lo lp kw lq lr ls lt ij bi translated">在商品硬件中，加速推理是棘手的，因为它不是推理预算等式中的唯一变量。模型尺寸和我们可能权衡的精确度也是至关重要的。因此，这里有一些技术可以帮助<strong class="ld ir"> <em class="md">压缩模型(可选)和/或加速推理，而不会在准确性上做出巨大的妥协。</em> </strong></p><p id="6623" class="pw-post-body-paragraph lb lc iq ld b le nf jr lg lh ng ju lj ko nh ll lm ks ni lo lp kw nj lr ls lt ij bi translated">这篇文章的范围是分享关于可用选项的广泛观点，并给出一般性建议。所以在这篇文章中，你不会找到太多关于这些技术如何在内部工作的信息。我会写一篇后续。</p><h2 id="e128" class="kf kg iq bd kh ki kj dn kk kl km dp kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">可用选项</h2><p id="4104" class="pw-post-body-paragraph lb lc iq ld b le lf jr lg lh li ju lj ko lk ll lm ks ln lo lp kw lq lr ls lt ij bi translated">NLP中的任何推理请求都有两部分:一是<strong class="ld ir">输入</strong> <strong class="ld ir">标记化</strong>二是实际<strong class="ld ir">模型</strong> <strong class="ld ir">预测</strong>调用。HuggingFace通过将标记化的最新研究民主化，钉死了标记化部分。这里有一个<a class="ae me" href="https://huggingface.co/transformers/tokenizer_summary.html" rel="noopener ugc nofollow" target="_blank"> <strong class="ld ir">概要</strong> </a>。</p><figure class="nl nm nn no gt np gh gi paragraph-image"><div role="button" tabindex="0" class="nq nr di ns bf nt"><div class="gh gi nk"><img src="../Images/4a2560099d6dfa04e153dc957623d33f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VnoxWJRAip5rcgg8gEzplw.png"/></div></div><p class="nw nx gj gh gi ny nz bd b be z dk translated">商用硬件上的NLP推理—可用选项(图片由作者提供)</p></figure><h2 id="0f27" class="kf kg iq bd kh ki kj dn kk kl km dp kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">胖客户端/服务器端推断(例如Outlook自动响应)</h2><figure class="nl nm nn no gt np gh gi paragraph-image"><div role="button" tabindex="0" class="nq nr di ns bf nt"><div class="gh gi nk"><img src="../Images/b4baeebd550c5122d0ae06603b72742a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*824C7anGa1aY2-jLnIAyRQ.png"/></div></div><p class="nw nx gj gh gi ny nz bd b be z dk translated">商用硬件上胖客户端和服务器端推理的路径(图片由作者提供)</p></figure><h2 id="3ba3" class="kf kg iq bd kh ki kj dn kk kl km dp kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">瘦客户端推理(例如基于浏览器)</h2><figure class="nl nm nn no gt np gh gi paragraph-image"><div role="button" tabindex="0" class="nq nr di ns bf nt"><div class="gh gi nk"><img src="../Images/c7030b795a3a10f7a89d9307b7f7ff71.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PQM9ScQ-AB4SENuC44xQQQ.png"/></div></div><p class="nw nx gj gh gi ny nz bd b be z dk translated">瘦客户端路径，如基于浏览器的推理硬件(图片由作者提供)</p></figure><h2 id="7107" class="kf kg iq bd kh ki kj dn kk kl km dp kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">受限/边缘设备推断(例如智能手机、物联网设备)</h2><figure class="nl nm nn no gt np gh gi paragraph-image"><div role="button" tabindex="0" class="nq nr di ns bf nt"><div class="gh gi nk"><img src="../Images/69d3e36fa800fb84caf64719b9795b99.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5h9xwZo_GcyhWnwsmZjwkw.png"/></div></div><p class="nw nx gj gh gi ny nz bd b be z dk translated">边缘或受约束设备推断的路径(图片由作者提供)</p></figure><p id="4b1e" class="pw-post-body-paragraph lb lc iq ld b le nf jr lg lh ng ju lj ko nh ll lm ks ni lo lp kw nj lr ls lt ij bi translated">想法/评论？</p><p id="657e" class="pw-post-body-paragraph lb lc iq ld b le nf jr lg lh ng ju lj ko nh ll lm ks ni lo lp kw nj lr ls lt ij bi translated"><a class="ae me" href="https://huggingface.co/blog/accelerated-inference" rel="noopener ugc nofollow" target="_blank"> <strong class="ld ir">参考文献:</strong> </a></p><ol class=""><li id="e285" class="oa ob iq ld b le nf lh ng ko oc ks od kw oe lt of og oh oi bi translated">【https://huggingface.co/blog/accelerated-inference T4】</li><li id="1872" class="oa ob iq ld b le oj lh ok ko ol ks om kw on lt of og oh oi bi translated"><a class="ae me" rel="noopener" target="_blank" href="/low-precision-inference-with-tensorrt-6eb3cda0730b">https://towards data science . com/low-precision-inference-with-tensorrt-6 EB 3c da 0730 b</a></li><li id="d122" class="oa ob iq ld b le oj lh ok ko ol ks om kw on lt of og oh oi bi translated"><a class="ae me" href="https://en.wikipedia.org/wiki/Application-specific_integrated_circuit" rel="noopener ugc nofollow" target="_blank">https://en . Wikipedia . org/wiki/专用集成电路</a></li></ol></div></div>    
</body>
</html>