<html>
<head>
<title>PonderNet explained</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">庞德奈特解释道</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/pondernet-explained-5e9571e657d?source=collection_archive---------20-----------------------#2021-11-17">https://towardsdatascience.com/pondernet-explained-5e9571e657d?source=collection_archive---------20-----------------------#2021-11-17</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="4107" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">为 MNIST 数据集实现一个思考网络</h2></div></div><div class="ab cl kf kg hu kh" role="separator"><span class="ki bw bk kj kk kl"/><span class="ki bw bk kj kk kl"/><span class="ki bw bk kj kk"/></div><div class="ij ik il im in"><p id="a56c" class="pw-post-body-paragraph km kn iq ko b kp kq jr kr ks kt ju ku kv kw kx ky kz la lb lc ld le lf lg lh ij bi translated">你可以在<a class="ae li" href="https://github.com/conradkun/PonderNet_MNIST" rel="noopener ugc nofollow" target="_blank"> this GitHub repo </a>中找到这篇文章的代码。现在就克隆它！</p><p id="1993" class="pw-post-body-paragraph km kn iq ko b kp kq jr kr ks kt ju ku kv kw kx ky kz la lb lc ld le lf lg lh ij bi translated">你可以用<a class="ae li" href="https://colab.research.google.com/drive/1ZIfcrpV_Pv6WCfRMHJw9x_0PMolOn6O_?usp=sharing" rel="noopener ugc nofollow" target="_blank">这个 Colab 笔记本</a>运行你阅读的代码。现在打开它！</p></div><div class="ab cl kf kg hu kh" role="separator"><span class="ki bw bk kj kk kl"/><span class="ki bw bk kj kk kl"/><span class="ki bw bk kj kk"/></div><div class="ij ik il im in"><figure class="lk ll lm ln gt lo gh gi paragraph-image"><div role="button" tabindex="0" class="lp lq di lr bf ls"><div class="gh gi lj"><img src="../Images/38a6b28163cfd822283554434fa9a88d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*gixVHrd0ndFDwZzH"/></div></div><p class="lv lw gj gh gi lx ly bd b be z dk translated"><a class="ae li" href="https://unsplash.com/@tingeyinjurylawfirm?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">廷杰律师事务所</a>在<a class="ae li" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="2826" class="pw-post-body-paragraph km kn iq ko b kp kq jr kr ks kt ju ku kv kw kx ky kz la lb lc ld le lf lg lh ij bi translated">如果我们真的想实现机器人霸主的反乌托邦未来，我们必须认识到，当前的人工智能永远不会成功。大多数现代神经网络都缺少某些东西，这是阻止它们统治世界的一个关键因素:它们无法思考。</p><p id="9826" class="pw-post-body-paragraph km kn iq ko b kp kq jr kr ks kt ju ku kv kw kx ky kz la lb lc ld le lf lg lh ij bi translated">幸运的是，<em class="lz"> DeepMind </em>最近发布了<a class="ae li" href="https://arxiv.org/abs/2107.05407" rel="noopener ugc nofollow" target="_blank"> <strong class="ko ir"> PonderNet </strong> </a>，这是一个可能会让任何网络思考的框架。突然间，未来又变得光明了。</p><p id="716d" class="pw-post-body-paragraph km kn iq ko b kp kq jr kr ks kt ju ku kv kw kx ky kz la lb lc ld le lf lg lh ij bi translated">玩笑归玩笑，思考是一个重要的概念，可能会对我们如何设计新模型产生严重的影响。在这篇文章中，我们将讨论这个理论(你会惊讶于它有多简单！)和<strong class="ko ir">实现了一个在 MNIST 数据集上执行图像分类的 PonderNet </strong>版本。在后面的部分，我们将进行一些<strong class="ko ir">实验</strong>来确定思考能力的影响力有多大。</p><p id="c486" class="pw-post-body-paragraph km kn iq ko b kp kq jr kr ks kt ju ku kv kw kx ky kz la lb lc ld le lf lg lh ij bi translated">我们将使用<a class="ae li" href="https://www.pytorchlightning.ai/" rel="noopener ugc nofollow" target="_blank"><strong class="ko ir">py torch Lightning</strong></a>作为我们的框架(因为它实在是太棒了)；如果你对它不熟悉，现在是学习基础知识的好时机！对于日志记录，我们将使用<a class="ae li" href="http://wandb.ai/" rel="noopener ugc nofollow" target="_blank"> <strong class="ko ir">权重&amp;偏差</strong> </a>(也因为它实在是太棒了)。我强烈建议您尝试一下，但是如果您喜欢不同的日志程序，您只需要修改一行代码(因为 PyTorch Lightning 实在是太棒了)。</p><div class="ma mb gp gr mc md"><a rel="noopener follow" target="_blank" href="/from-pytorch-to-pytorch-lightning-a-gentle-introduction-b371b7caaf09"><div class="me ab fo"><div class="mf ab mg cl cj mh"><h2 class="bd ir gy z fp mi fr fs mj fu fw ip bi translated">从 PyTorch 到 py torch Lightning——一个温和的介绍</h2><div class="mk l"><h3 class="bd b gy z fp mi fr fs mj fu fw dk translated">这篇文章对使用 PyTorch 和 PyTorch Lightning 实现的 MNIST 进行了对比。</h3></div><div class="ml l"><p class="bd b dl z fp mi fr fs mj fu fw dk translated">towardsdatascience.com</p></div></div><div class="mm l"><div class="mn l mo mp mq mm mr lt md"/></div></div></a></div></div><div class="ab cl kf kg hu kh" role="separator"><span class="ki bw bk kj kk kl"/><span class="ki bw bk kj kk kl"/><span class="ki bw bk kj kk"/></div><div class="ij ik il im in"><h1 id="6920" class="ms mt iq bd mu mv mw mx my mz na nb nc jw nd jx ne jz nf ka ng kc nh kd ni nj bi translated">1.动机</h1><p id="afa7" class="pw-post-body-paragraph km kn iq ko b kp nk jr kr ks nl ju ku kv nm kx ky kz nn lb lc ld no lf lg lh ij bi translated">这一切都很好，但是思考它到底意味着什么呢？作者是这样表述的:</p><blockquote class="np nq nr"><p id="5849" class="km kn lz ko b kp kq jr kr ks kt ju ku ns kw kx ky nt la lb lc nu le lf lg lh ij bi translated"><strong class="ko ir">思考</strong>就是根据任务的复杂程度来调整计算预算。</p></blockquote><p id="418a" class="pw-post-body-paragraph km kn iq ko b kp kq jr kr ks kt ju ku kv kw kx ky kz la lb lc ld le lf lg lh ij bi translated">根据这个定义，很明显，机器学习研究人员和工程师<strong class="ko ir">一直在思考</strong>:每次他们选择特定数量的隐藏层，选择不同的 GPU 来训练他们的模型，或者做出任何影响网络架构的决定。</p><p id="251a" class="pw-post-body-paragraph km kn iq ko b kp kq jr kr ks kt ju ku kv kw kx ky kz la lb lc ld le lf lg lh ij bi translated">同样清楚的是，大多数电视网不能够考虑 T7。我们可以争辩说，数字 6 的图像确实很容易识别，而人们可能需要更多的时间来区分 1 和 7；然而，CNN 将花费相同数量的资源来预测这两种图像的标签。这主要是由于神经网络的刚性结构，以及它们如何充当黑盒映射。</p><p id="80bb" class="pw-post-body-paragraph km kn iq ko b kp kq jr kr ks kt ju ku kv kw kx ky kz la lb lc ld le lf lg lh ij bi translated">PonderNet 从之前的研究<a class="ae li" href="https://arxiv.org/abs/1603.08983" rel="noopener ugc nofollow" target="_blank">中创新出来，能够将更多的资源分配给它认为需要的投入。如果我们希望模型学习超越当前的最先进水平，这是一个关键属性。这也是在经典算法的背景下思考神经网络的一步，可以为该领域带来许多新鲜的想法(不要错过美丽的</a><a class="ae li" href="https://arxiv.org/pdf/2107.05407.pdf" rel="noopener ugc nofollow" target="_blank">附录 E </a>)。</p></div><div class="ab cl kf kg hu kh" role="separator"><span class="ki bw bk kj kk kl"/><span class="ki bw bk kj kk kl"/><span class="ki bw bk kj kk"/></div><div class="ij ik il im in"><h1 id="2fec" class="ms mt iq bd mu mv mw mx my mz na nb nc jw nd jx ne jz nf ka ng kc nh kd ni nj bi translated">2.PonderNet 框架</h1><p id="24e5" class="pw-post-body-paragraph km kn iq ko b kp nk jr kr ks nl ju ku kv nm kx ky kz nn lb lc ld no lf lg lh ij bi translated">在这一节中，我们将放下 PonderNet 背后的所有理论。首先，让我们试着理解它是如何在高水平上调整其计算预算的。</p><h2 id="5827" class="nv mt iq bd mu nw nx dn my ny nz dp nc kv oa ob ne kz oc od ng ld oe of ni og bi translated">直觉</h2><p id="6a37" class="pw-post-body-paragraph km kn iq ko b kp nk jr kr ks nl ju ku kv nm kx ky kz nn lb lc ld no lf lg lh ij bi translated">假设我们有一个想要解决的任务(例如，对 MNIST 的数字进行分类！)和解决它的模型(如 good ol' CNN)。传统的方法是简单地处理一次输入并产生一个输出。相比之下，PonderNet 框架允许输入被多次处理，并且能够找到合适的时间停止并输出结果。</p><figure class="lk ll lm ln gt lo gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/882d35cbc360628e46498ba306f70924.png" data-original-src="https://miro.medium.com/v2/resize:fit:756/format:webp/1*VHQLKwssXa28f9mpVwWaZA.png"/></div><p class="lv lw gj gh gi lx ly bd b be z dk translated">图 PonderNet 的选项。(图片由作者提供)</p></figure><p id="1788" class="pw-post-body-paragraph km kn iq ko b kp kq jr kr ks kt ju ku kv kw kx ky kz la lb lc ld le lf lg lh ij bi translated">概括地说，PonderNet 做了以下工作:</p><ul class=""><li id="312c" class="oi oj iq ko b kp kq ks kt kv ok kz ol ld om lh on oo op oq bi translated">处理原始输入。</li><li id="f405" class="oi oj iq ko b kp or ks os kv ot kz ou ld ov lh on oo op oq bi translated">产生一个预测和一个在当前步骤停止计算的概率。</li><li id="e9ad" class="oi oj iq ko b kp or ks os kv ot kz ou ld ov lh on oo op oq bi translated">抛硬币决定是“暂停”还是“继续”。</li><li id="4580" class="oi oj iq ko b kp or ks os kv ot kz ou ld ov lh on oo op oq bi translated">如果我们暂停，输出最新的预测；否则，再次处理输入以及一些上下文信息。</li></ul><p id="a986" class="pw-post-body-paragraph km kn iq ko b kp kq jr kr ks kt ju ku kv kw kx ky kz la lb lc ld le lf lg lh ij bi translated">图 1 通过将暂停选项表示为二叉树总结了这一思想。</p><h2 id="158e" class="nv mt iq bd mu nw nx dn my ny nz dp nc kv oa ob ne kz oc od ng ld oe of ni og bi translated">形式定义</h2><p id="6f62" class="pw-post-body-paragraph km kn iq ko b kp nk jr kr ks nl ju ku kv nm kx ky kz nn lb lc ld no lf lg lh ij bi translated">更正式地说，我们可以将 PonderNet 框架定义为满足以下等式的阶跃函数<em class="lz"> s </em>(通常是神经网络):</p><figure class="lk ll lm ln gt lo gh gi paragraph-image"><div class="gh gi ow"><img src="../Images/8ae6d00a62db489711734b049f3be6ad.png" data-original-src="https://miro.medium.com/v2/resize:fit:780/format:webp/1*bUxy5mTfu0LYM5vQUafe7g.png"/></div></figure><p id="e712" class="pw-post-body-paragraph km kn iq ko b kp kq jr kr ks kt ju ku kv kw kx ky kz la lb lc ld le lf lg lh ij bi translated">这里，<em class="lz"> x </em>表示原始输入，两个<em class="lz"> h </em>表示通过不同步骤传播的隐藏状态，<em class="lz"> y </em>是当前步骤的输出，λ是在当前步骤停止的概率。</p><p id="cf90" class="pw-post-body-paragraph km kn iq ko b kp kq jr kr ks kt ju ku kv kw kx ky kz la lb lc ld le lf lg lh ij bi translated">换句话说，这意味着在每一步，PonderNet 都获取原始输入和最新的隐藏状态，并为新的一步产生一个预测、停止的概率和更新的隐藏状态。它将以概率λ投掷一枚有偏向的硬币，以决定是暂停并输出<em class="lz"> y </em>还是通过进一步传播<em class="lz"> h </em>来继续。</p><figure class="lk ll lm ln gt lo gh gi paragraph-image"><div role="button" tabindex="0" class="lp lq di lr bf ls"><div class="gh gi ox"><img src="../Images/915ad1a177c637671ce38f5ecf1a84bb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3nV4vVxe2qHXB38SIfnLnA.png"/></div></div><p class="lv lw gj gh gi lx ly bd b be z dk translated">图 2:展开的 PonderNet。(图片由作者提供)</p></figure><p id="17b5" class="pw-post-body-paragraph km kn iq ko b kp kq jr kr ks kt ju ku kv kw kx ky kz la lb lc ld le lf lg lh ij bi translated">图 2 非常直观地显示了隐藏状态如何流经所有步骤，以及每一步如何产生一对输出和停止的概率。如果你熟悉 RNNs，你会在结构中发现一些<strong class="ko ir">相似之处。</strong></p><div class="ma mb gp gr mc md"><a rel="noopener follow" target="_blank" href="/recurrent-neural-networks-rnns-3f06d7653a85"><div class="me ab fo"><div class="mf ab mg cl cj mh"><h2 class="bd ir gy z fp mi fr fs mj fu fw ip bi translated">递归神经网络</h2><div class="mk l"><h3 class="bd b gy z fp mi fr fs mj fu fw dk translated">用 Python 从头开始实现 RNN。</h3></div><div class="ml l"><p class="bd b dl z fp mi fr fs mj fu fw dk translated">towardsdatascience.com</p></div></div><div class="mm l"><div class="oy l mo mp mq mm mr lt md"/></div></div></a></div><h2 id="6b12" class="nv mt iq bd mu nw nx dn my ny nz dp nc kv oa ob ne kz oc od ng ld oe of ni og bi translated">λ的含义</h2><p id="1936" class="pw-post-body-paragraph km kn iq ko b kp nk jr kr ks nl ju ku kv nm kx ky kz nn lb lc ld no lf lg lh ij bi translated">值得注意的是，从技术上讲，λ是在当前步骤<em class="lz">停止的概率，假定</em>在之前的步骤中没有发生停止。这使得我们可以将λ视为一个<strong class="ko ir">伯努利随机变量</strong>的概率，它告诉我们是否应该在当前步骤停止。如果我们希望找到在当前步骤停止的<em class="lz">无条件</em>概率(此后为<em class="lz"> p </em>，它还必须包括在先前步骤中不停止的概率:</p><figure class="lk ll lm ln gt lo gh gi paragraph-image"><div class="gh gi oz"><img src="../Images/23f83e2c7219ae5e9ebc0db38e382ed1.png" data-original-src="https://miro.medium.com/v2/resize:fit:852/format:webp/1*77afY5Mjk8kxnK_JNl8mzQ.png"/></div></figure><h2 id="302d" class="nv mt iq bd mu nw nx dn my ny nz dp nc kv oa ob ne kz oc od ng ld oe of ni og bi translated">思考步骤</h2><p id="f1c8" class="pw-post-body-paragraph km kn iq ko b kp nk jr kr ks nl ju ku kv nm kx ky kz nn lb lc ld no lf lg lh ij bi translated">我们通常只允许特定的最大数量的“思考步骤”。这意味着我们将在最后一步强制λ为 1，从而保证停止。</p><p id="0d1c" class="pw-post-body-paragraph km kn iq ko b kp kq jr kr ks kt ju ku kv kw kx ky kz la lb lc ld le lf lg lh ij bi translated">在推理过程中，我们并不明确需要这样的限制，因为我们可以让网络无限期地运行，直到其中一个硬币的投掷使它自然停止；然后，我们将输出在该步骤中获得的<em class="lz"> y </em>。尽管如此，限制思考的步骤仍然是一个好主意，因为从理论上讲<strong class="ko ir">它可以永远运行</strong>。</p><p id="9061" class="pw-post-body-paragraph km kn iq ko b kp kq jr kr ks kt ju ku kv kw kx ky kz la lb lc ld le lf lg lh ij bi translated">在训练期间，由于损失函数，思考步骤的界限是必需的，正如我们现在将看到的。</p><h2 id="2bf4" class="nv mt iq bd mu nw nx dn my ny nz dp nc kv oa ob ne kz oc od ng ld oe of ni og bi translated">训练思考网</h2><p id="33da" class="pw-post-body-paragraph km kn iq ko b kp nk jr kr ks nl ju ku kv nm kx ky kz nn lb lc ld no lf lg lh ij bi translated">像几乎所有的神经网络一样，PonderNet 试图优化一个损失函数:</p><figure class="lk ll lm ln gt lo gh gi paragraph-image"><div class="gh gi pa"><img src="../Images/ffb97512b96f721174cc4fded4815fb3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1242/format:webp/1*UcW_mHwWZ9CDSBZRMelcSw.png"/></div></figure><p id="5ae4" class="pw-post-body-paragraph km kn iq ko b kp kq jr kr ks kt ju ku kv kw kx ky kz la lb lc ld le lf lg lh ij bi translated">让我们来分析一下。<em class="lz"> L </em>可分为两种损失，即<strong class="ko ir">重构损失</strong>和<strong class="ko ir">正则化损失</strong>(类似于 VAEs)；这两者之间的权衡由超参数β调节。在这两种情况下，损失可以分解成 N 个项；这是我们思考步骤的最大数量。</p><p id="45cf" class="pw-post-body-paragraph km kn iq ko b kp kq jr kr ks kt ju ku kv kw kx ky kz la lb lc ld le lf lg lh ij bi translated">重建损失是非常直观的:对于我们思考的每一步，我们计算该步输出的损失，并以我们在其中停止的无条件概率进行加权。从这个意义上说，重建损失无非是跨越所有步骤的<strong class="ko ir">预期损失</strong>。请注意，在整个计算过程中，我们使用的是我们试图解决的任务的潜在损失函数(在多类分类的情况下，交叉熵)。</p><p id="abb2" class="pw-post-body-paragraph km kn iq ko b kp kq jr kr ks kt ju ku kv kw kx ky kz la lb lc ld le lf lg lh ij bi translated">正则化损失在停止应该如何表现方面引入了偏差。它试图最小化由有质网生成的暂停分布和具有某个超参数λ <em class="lz"> p </em>的先验几何分布之间的<strong class="ko ir">KL-散度</strong>。对于那些不熟悉 KL-divergence 的人，可以随意查看下面的链接；直观上，我们所说的是我们希望网络产生的所有λ都接近λ <em class="lz"> p. </em></p><div class="ma mb gp gr mc md"><a rel="noopener follow" target="_blank" href="/light-on-math-machine-learning-intuitive-guide-to-understanding-kl-divergence-2b382ca2b2a8"><div class="me ab fo"><div class="mf ab mg cl cj mh"><h2 class="bd ir gy z fp mi fr fs mj fu fw ip bi translated">理解 KL 分歧的直观指南</h2><div class="mk l"><h3 class="bd b gy z fp mi fr fs mj fu fw dk translated">我正在开始一个新的博客系列文章，遵循一个初学者友好的方法来理解一些…</h3></div><div class="ml l"><p class="bd b dl z fp mi fr fs mj fu fw dk translated">towardsdatascience.com</p></div></div><div class="mm l"><div class="pb l mo mp mq mm mr lt md"/></div></div></a></div><p id="e6aa" class="pw-post-body-paragraph km kn iq ko b kp kq jr kr ks kt ju ku kv kw kx ky kz la lb lc ld le lf lg lh ij bi translated">这个正则项的影响是双重的。一方面，我们<strong class="ko ir">将预期的思考步骤数</strong>偏向 1/λ <em class="lz"> p </em>(因为这是几何分布的预期值)。另一方面，它<strong class="ko ir">促进探索</strong>通过在任何步骤中给予正概率来停止，不管它有多远。</p></div><div class="ab cl kf kg hu kh" role="separator"><span class="ki bw bk kj kk kl"/><span class="ki bw bk kj kk kl"/><span class="ki bw bk kj kk"/></div><div class="ij ik il im in"><h1 id="d1e8" class="ms mt iq bd mu mv mw mx my mz na nb nc jw nd jx ne jz nf ka ng kc nh kd ni nj bi translated">3.为 MNIST 实施 PonderNet</h1><p id="653b" class="pw-post-body-paragraph km kn iq ko b kp nk jr kr ks nl ju ku kv nm kx ky kz nn lb lc ld no lf lg lh ij bi translated">没那么糟吧。我们现在准备开始动手了！请记住，在阅读时，您是按照本文的 Colab 笔记本来运行代码的。如果你想在本地运行它，所有的代码(和一些额外的东西！)可以在这个<a class="ae li" href="https://github.com/conradkun/PonderNet_MNIST" rel="noopener ugc nofollow" target="_blank"> GitHub repo </a>中找到；如果你发现任何错误，不要犹豫，打开一个问题！</p><h2 id="dec3" class="nv mt iq bd mu nw nx dn my ny nz dp nc kv oa ob ne kz oc od ng ld oe of ni og bi translated">数据模块</h2><p id="cb8e" class="pw-post-body-paragraph km kn iq ko b kp nk jr kr ks nl ju ku kv nm kx ky kz nn lb lc ld no lf lg lh ij bi translated">让我们首先解决数据模块的问题。这里没有什么新奇的东西，除了这个数据模块允许你拥有<strong class="ko ir">多个测试数据加载器</strong>(这是一个惊喜的工具，以后会对我们有帮助！).</p><figure class="lk ll lm ln gt lo"><div class="bz fp l di"><div class="pc pd l"/></div></figure><h2 id="2253" class="nv mt iq bd mu nw nx dn my ny nz dp nc kv oa ob ne kz oc od ng ld oe of ni og bi translated">损耗</h2><p id="1486" class="pw-post-body-paragraph km kn iq ko b kp nk jr kr ks nl ju ku kv nm kx ky kz nn lb lc ld no lf lg lh ij bi translated">我们将损失中的每一项分别建模，作为扩展<code class="fe pe pf pg ph b">nn.Module</code>的两个不同类别。让我们从<strong class="ko ir">重建损失</strong>说起。我们唯一需要做的是计算加权平均值，这很容易实现:</p><figure class="lk ll lm ln gt lo"><div class="bz fp l di"><div class="pc pd l"/></div></figure><p id="6a6d" class="pw-post-body-paragraph km kn iq ko b kp kq jr kr ks kt ju ku kv kw kx ky kz la lb lc ld le lf lg lh ij bi translated">至于正则化损失，还有一些多余的细节要讲。为了计算生成的暂停分布和我们的先验之间的 KL-divergence，我们将首先制造我们的先验的值，这在初始化函数中完成。</p><figure class="lk ll lm ln gt lo"><div class="bz fp l di"><div class="pc pd l"/></div></figure><p id="9cbb" class="pw-post-body-paragraph km kn iq ko b kp kq jr kr ks kt ju ku kv kw kx ky kz la lb lc ld le lf lg lh ij bi translated">最后，我们创建一个类来将这两个损失包装在一起，以便它们可以紧凑地传递给函数。</p><figure class="lk ll lm ln gt lo"><div class="bz fp l di"><div class="pc pd l"/></div></figure><h2 id="1863" class="nv mt iq bd mu nw nx dn my ny nz dp nc kv oa ob ne kz oc od ng ld oe of ni og bi translated">助手模块</h2><p id="16f4" class="pw-post-body-paragraph km kn iq ko b kp nk jr kr ks nl ju ku kv nm kx ky kz nn lb lc ld no lf lg lh ij bi translated">为了使我们的生活更容易，我们为一个简单的 CNN 创建了一个类。这将用于将图像嵌入到 PonderNet 中的矢量表示中。</p><figure class="lk ll lm ln gt lo"><div class="bz fp l di"><div class="pc pd l"/></div></figure><p id="979a" class="pw-post-body-paragraph km kn iq ko b kp kq jr kr ks kt ju ku kv kw kx ky kz la lb lc ld le lf lg lh ij bi translated">同样，我们创建了一个基本的多层感知器，将图像嵌入与 PonderNet 内部的隐藏状态结合起来。</p><figure class="lk ll lm ln gt lo"><div class="bz fp l di"><div class="pc pd l"/></div></figure><h2 id="f7c5" class="nv mt iq bd mu nw nx dn my ny nz dp nc kv oa ob ne kz oc od ng ld oe of ni og bi translated">PonderNet</h2><p id="5e84" class="pw-post-body-paragraph km kn iq ko b kp nk jr kr ks nl ju ku kv nm kx ky kz nn lb lc ld no lf lg lh ij bi translated">最后，我们得到了重要的东西。为了更详细地评论<code class="fe pe pf pg ph b">LightningModule</code>的不同部分，它内部的一些函数将在单独的代码片段中显示；只要记住他们都属于同一类！</p><p id="80d4" class="pw-post-body-paragraph km kn iq ko b kp kq jr kr ks kt ju ku kv kw kx ky kz la lb lc ld le lf lg lh ij bi translated">我们的 PonderNet 具体实现由多个子模块组成。首先，CNN 将图像嵌入到矢量表示中。该向量与对应于前一步骤的隐藏状态连接，并通过 MLP 获得当前步骤的隐藏状态。这又被推过两个不同的线性层，一方面获得预测的逻辑，另一方面获得 lambdas 的逻辑。下面的片段显示了这些子模块如何被声明为 PonderNet 的一部分，以及损耗和一些指标:</p><figure class="lk ll lm ln gt lo"><div class="bz fp l di"><div class="pc pd l"/></div></figure><p id="b3fb" class="pw-post-body-paragraph km kn iq ko b kp kq jr kr ks kt ju ku kv kw kx ky kz la lb lc ld le lf lg lh ij bi translated">我们现在到达了可以说是代码中最复杂的部分:向前传递<strong class="ko ir"/>。获得<em class="lz"> h </em>，<em class="lz"> y </em>和λ是直截了当的，记住，为了获得λ，你必须使用 sigmoid 函数，否则你就不能强迫它成为一个概率。</p><p id="243c" class="pw-post-body-paragraph km kn iq ko b kp kq jr kr ks kt ju ku kv kw kx ky kz la lb lc ld le lf lg lh ij bi translated">由于我们是成批操作的，所以当涉及到网络停止的思考步骤时，我们需要找到一种方法来单独跟踪每个元素。为此，我们维护一个向量<code class="fe pe pf pg ph b">halting_step</code>,对于每个元素，如果 PonderNet 尚未停止，则该向量为 0，否则停止的步骤为 0。当然，更新<code class="fe pe pf pg ph b">halting_step</code>包括在每一步从伯努利分布中提取，以确定网络是否应该停止。</p><p id="8cf5" class="pw-post-body-paragraph km kn iq ko b kp kq jr kr ks kt ju ku kv kw kx ky kz la lb lc ld le lf lg lh ij bi translated">最后，我们还维护了一个向量<code class="fe pe pf pg ph b">un_halted_prob</code>，它帮助我们以一种廉价而快速的方式获得每一步的<em class="lz"> p </em>的值。</p><p id="bc29" class="pw-post-body-paragraph km kn iq ko b kp kq jr kr ks kt ju ku kv kw kx ky kz la lb lc ld le lf lg lh ij bi translated">最后，我们返回所有批处理元素和所有步骤的所有预测、所有批处理元素和所有步骤的所有<em class="lz"> p </em>以及所有批处理元素的暂停步骤。</p><figure class="lk ll lm ln gt lo"><div class="bz fp l di"><div class="pc pd l"/></div></figure><p id="7651" class="pw-post-body-paragraph km kn iq ko b kp kq jr kr ks kt ju ku kv kw kx ky kz la lb lc ld le lf lg lh ij bi translated">由于需要处理前向传递的输出以获得损失(在一些情况下我们需要这样做)，我们定义了一个帮助函数来计算损失、预测和一些其他指标。</p><p id="713e" class="pw-post-body-paragraph km kn iq ko b kp kq jr kr ks kt ju ku kv kw kx ky kz la lb lc ld le lf lg lh ij bi translated"><code class="fe pe pf pg ph b">if</code>条件有助于规避我个人遇到的一个技术问题。在训练的早期阶段，对于选定的步骤，一些<em class="lz"> p </em>值可以是 0。当这通过正则化损失时，零点变成负无穷大，网络最终开始返回 nan。忽略包含值为 0 的<em class="lz"> p </em>的元素解决了这个问题，并且这些相同的元素在后面的时期被正确分类。</p><figure class="lk ll lm ln gt lo"><div class="bz fp l di"><div class="pc pd l"/></div></figure><p id="f6aa" class="pw-post-body-paragraph km kn iq ko b kp kq jr kr ks kt ju ku kv kw kx ky kz la lb lc ld le lf lg lh ij bi translated">最后，PyTorch Lightning 模块中需要的其他常用功能可以在下面找到。这些包括日志和一些回调，使培训更加用户友好。</p><figure class="lk ll lm ln gt lo"><div class="bz fp l di"><div class="pc pd l"/></div></figure><h2 id="164d" class="nv mt iq bd mu nw nx dn my ny nz dp nc kv oa ob ne kz oc od ng ld oe of ni og bi translated">运行插值实验</h2><p id="9304" class="pw-post-body-paragraph km kn iq ko b kp nk jr kr ks nl ju ku kv nm kx ky kz nn lb lc ld no lf lg lh ij bi translated">我们基本上完成了！这就是 PyTorch 闪电的美妙之处。使用下面的代码片段，我们将能够在香草 MNIST 上运行一个基本的实验；我们的记录者将在整个训练过程中记录损失(一起和单独的)、准确性和思考步骤。如果您想使用不同的记录器，您只需要从 PyTorch Lightning 导入支持的类，并在第 40 行中相应地实例化它；就这么简单！</p><figure class="lk ll lm ln gt lo"><div class="bz fp l di"><div class="pc pd l"/></div></figure><p id="1489" class="pw-post-body-paragraph km kn iq ko b kp kq jr kr ks kt ju ku kv kw kx ky kz la lb lc ld le lf lg lh ij bi translated">下面是我们在 MNIST 测试集上评估时获得的正确率和思考步骤的平均数。正如我们所见，PonderNet 做得很好。更有趣的是，思考步骤的平均数量非常接近 5，这是 1/λ <em class="lz"> p </em>的值，即预期的步骤数量；这意味着我们的正规化确实在起作用！</p><figure class="lk ll lm ln gt lo gh gi paragraph-image"><div role="button" tabindex="0" class="lp lq di lr bf ls"><div class="gh gi pi"><img src="../Images/f68854627a4b417baf079ec65213f3e5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*npGI2-U1ojc7l7r98g9Qew.png"/></div></div></figure><h1 id="bfcd" class="ms mt iq bd mu mv pj mx my mz pk nb nc jw pl jx ne jz pm ka ng kc pn kd ni nj bi translated">4.我们真的在思考吗？</h1><p id="042e" class="pw-post-body-paragraph km kn iq ko b kp nk jr kr ks nl ju ku kv nm kx ky kz nn lb lc ld no lf lg lh ij bi translated">运行上面的代码应该可以让你相信，PonderNet 至少可以在 MNIST 上获得相当高的精度…但是我们使用了它的任何属性吗？难道我们不能用一个简单的 CNN 取得类似的结果吗？我们如何知道它确实对更难的输入考虑得更久？我们将试图通过外推实验来回答其中的一些问题。</p><p id="4b0d" class="pw-post-body-paragraph km kn iq ko b kp kq jr kr ks kt ju ku kv kw kx ky kz la lb lc ld le lf lg lh ij bi translated">阅读原始论文，人们可能会对实验如何在<strong class="ko ir">玩具数据集</strong>或<strong class="ko ir">任务上进行感到有点沮丧，这些任务是如此复杂</strong>以至于不可能解释任何东西。MNIST 数据集提供了一个中间地带，在这里，任务肯定没有被设计为使用 PonderNet 产生预期的结果，并且其结果在某种程度上是可以解释的。</p><p id="aefe" class="pw-post-body-paragraph km kn iq ko b kp kq jr kr ks kt ju ku kv kw kx ky kz la lb lc ld le lf lg lh ij bi translated">我们实验的前提是<strong class="ko ir">旋转的图像更难分类</strong>。遵循与 PonderNet 论文中提出的框架相似的框架，我们将对“稍微硬”的输入进行训练，并对一系列“硬”输入进行评估。特别是，我们将对旋转了 22.5 度的图像进行训练，并对旋转了 22.5 度、45 度、67.5 度和 90 度的图像进行评估。我们预计准确率会逐渐下降(一些图像一旦旋转到这种程度，甚至可能无法分类)，但希望我们看到步骤数的<strong class="ko ir">增加，这表明网络发现任务更难了，并决定分配更多资源给它。下面是这样一个实验的代码:</strong></p><figure class="lk ll lm ln gt lo"><div class="bz fp l di"><div class="pc pd l"/></div></figure><p id="7a64" class="pw-post-body-paragraph km kn iq ko b kp kq jr kr ks kt ju ku kv kw kx ky kz la lb lc ld le lf lg lh ij bi translated">我们进行这个实验后得到的结果有点争议。一方面，在任何旋转数据集上进行测试确实需要的平均步骤数明显高于插值实验所需的步骤数，这意味着 PonderNet 认为对旋转图像进行分类是一项<strong class="ko ir">更困难的任务</strong>。</p><p id="e52e" class="pw-post-body-paragraph km kn iq ko b kp kq jr kr ks kt ju ku kv kw kx ky kz la lb lc ld le lf lg lh ij bi translated">然而，另一方面，网络似乎需要更少的步骤来实现更明显的旋转，这是违反直觉的。我们希望 PonderNet 对高度倾斜的图像不确定，从而分配更多的资源用于它们的分类和更长时间的思考，但似乎它反而变得不正确。精度也会降低，尽管在这种情况下，这是意料之中的。</p><figure class="lk ll lm ln gt lo gh gi paragraph-image"><div role="button" tabindex="0" class="lp lq di lr bf ls"><div class="gh gi po"><img src="../Images/9bf18b142e975c251f48ccc6796022d9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IVC7g8IBHbysMVdEC5BcNQ.png"/></div></div><p class="lv lw gj gh gi lx ly bd b be z dk translated">注意:此处的数据加载器分别对应 22.5 度、45 度、67.5 度和 90 度旋转。</p></figure><h1 id="35d3" class="ms mt iq bd mu mv pj mx my mz pk nb nc jw pl jx ne jz pm ka ng kc pn kd ni nj bi translated">5.结论</h1><p id="81b9" class="pw-post-body-paragraph km kn iq ko b kp nk jr kr ks nl ju ku kv nm kx ky kz nn lb lc ld no lf lg lh ij bi translated">PonderNet 是深度学习领域的一个很好的补充。它以数学为基础的设计选择有足够的理由让<strong class="ko ir">对<strong class="ko ir"> </strong>它的可能性感到</strong>兴奋，原始论文中呈现的结果令人鼓舞。然而，可悲的是，并没有多少努力去诚实地解释这个网络真正的能力。</p><p id="7559" class="pw-post-body-paragraph km kn iq ko b kp kq jr kr ks kt ju ku kv kw kx ky kz la lb lc ld le lf lg lh ij bi translated">我们试图通过实现 MNIST 的 PonderNet(该领域的通用基准)来阐明这个问题，并通过在不同难度的任务中训练网络来进行实验。</p><p id="75c7" class="pw-post-body-paragraph km kn iq ko b kp kq jr kr ks kt ju ku kv kw kx ky kz la lb lc ld le lf lg lh ij bi translated">我们的结果是不确定的，因为在某些方面它们与我们的预期一致，而在另一些方面则不一致。这可能有多种原因。一方面，很有可能旋转的角度对于这个特定任务的复杂性来说不是一个很好的启发，我的人类偏见干扰了实验设计。另一方面，我们不能放弃这样一种可能性，即 PonderNet 并不是我们所希望的全能框架；毕竟这方面还有很多研究要做。</p><p id="f973" class="pw-post-body-paragraph km kn iq ko b kp kq jr kr ks kt ju ku kv kw kx ky kz la lb lc ld le lf lg lh ij bi translated">总而言之，我希望你今天学到了一些东西；也许是我启发你在自己的项目中尝试了 PonderNet！我很乐意听到您的任何意见、问题或建议。<strong class="ko ir">感谢您的阅读！</strong></p><h1 id="2226" class="ms mt iq bd mu mv pj mx my mz pk nb nc jw pl jx ne jz pm ka ng kc pn kd ni nj bi translated">6.承认</h1><p id="f67b" class="pw-post-body-paragraph km kn iq ko b kp nk jr kr ks nl ju ku kv nm kx ky kz nn lb lc ld no lf lg lh ij bi translated">如果没有三个主要来源，这篇博文是不可能发表的。</p><ul class=""><li id="8409" class="oi oj iq ko b kp kq ks kt kv ok kz ol ld om lh on oo op oq bi translated">第一个是<a class="ae li" href="https://www.youtube.com/watch?v=nQDZmf2Yb9k" rel="noopener ugc nofollow" target="_blank">Yannic Kilcher 的 YouTube 视频</a>解释了 PonderNet 的基础知识；如果你没有订阅他的频道，我不知道你在做什么！</li><li id="9623" class="oi oj iq ko b kp or ks os kv ot kz ou ld ov lh on oo op oq bi translated">PonderNet 实现的一个重要部分是从 MildlyOverfitted 的<a class="ae li" href="https://github.com/jankrepl/mildlyoverfitted/tree/b8765bd66893f1ec7f373559bdce61c4766ceba9/github_adventures/pondernet" rel="noopener ugc nofollow" target="_blank">这个 GitHub repo </a>借来的。虽然在我偶然发现他的代码之前，我已经有了一个运行的原型，但我还是忍不住使用了他的酷把戏！</li><li id="4a11" class="oi oj iq ko b kp or ks os kv ot kz ou ld ov lh on oo op oq bi translated">最后，一些样板代码取自本教程中关于 PyTorch Lightning 的<a class="ae li" href="https://colab.research.google.com/github/wandb/examples/blob/master/colabs/pytorch-lightning/Supercharge_your_Training_with_Pytorch_Lightning_%2B_Weights_%26_Biases.ipynb" rel="noopener ugc nofollow" target="_blank">和权重&amp;偏差。</a></li></ul><p id="065f" class="pw-post-body-paragraph km kn iq ko b kp kq jr kr ks kt ju ku kv kw kx ky kz la lb lc ld le lf lg lh ij bi translated">最后，我要感谢我的导师 Fabian Laumer，他鼓励我尝试不同的东西，作为我的研讨会演示的一部分。我可能在这个小项目上损失了几十个小时，但我确实学到了很多！</p><h2 id="dcc1" class="nv mt iq bd mu nw nx dn my ny nz dp nc kv oa ob ne kz oc od ng ld oe of ni og bi translated">参考</h2><p id="15b4" class="pw-post-body-paragraph km kn iq ko b kp nk jr kr ks nl ju ku kv nm kx ky kz nn lb lc ld no lf lg lh ij bi translated">[1] A .巴尼诺，j .巴拉格尔，c .布伦德尔，<a class="ae li" href="https://arxiv.org/abs/2107.05407" rel="noopener ugc nofollow" target="_blank">庞德奈特:学会思考</a> (2021)，arXiv: 2107.05407。</p><p id="70c0" class="pw-post-body-paragraph km kn iq ko b kp kq jr kr ks kt ju ku kv kw kx ky kz la lb lc ld le lf lg lh ij bi translated">[2] A. Graves，<a class="ae li" href="https://arxiv.org/abs/1603.08983" rel="noopener ugc nofollow" target="_blank">递归神经网络的自适应计算时间</a> (2017)，arXiv: 1603.08983。</p></div></div>    
</body>
</html>