<html>
<head>
<title>Why you should store custom logs of your data pipelines and how to build a Data Catalog with them</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">为什么应该存储数据管道的自定义日志，以及如何使用它们构建数据目录</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/why-you-should-store-custom-logs-of-your-data-pipelines-and-how-to-build-a-data-catalog-with-them-ee96a99a1c96?source=collection_archive---------14-----------------------#2021-10-05">https://towardsdatascience.com/why-you-should-store-custom-logs-of-your-data-pipelines-and-how-to-build-a-data-catalog-with-them-ee96a99a1c96?source=collection_archive---------14-----------------------#2021-10-05</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="62d2" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">如何使用Azure Data Factory、Databricks和SQL Server实现这一点的分步指南。</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/19b2f64e7e51a097fbe6b5b581e7f85d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_8y1rxFaQHCmGMbVollhvQ.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><a class="ae kv" href="https://unsplash.com/photos/n7eJHQwefeI" rel="noopener ugc nofollow" target="_blank">Unsplash的Sunder Muthukumaran</a></p></figure><pre class="kg kh ki kj gt kw kx ky kz aw la bi"><span id="0f7d" class="lb lc iq kx b gy ld le l lf lg">“An ounce of prevention is worth a pound of cure.”   <br/>  ― <strong class="kx ir">Benjamin Franklin</strong></span></pre><p id="7a7e" class="pw-post-body-paragraph lh li iq lj b lk ll jr lm ln lo ju lp lq lr ls lt lu lv lw lx ly lz ma mb mc ij bi translated"><strong class="lj ir">简介</strong></p><p id="ca77" class="pw-post-body-paragraph lh li iq lj b lk ll jr lm ln lo ju lp lq lr ls lt lu lv lw lx ly lz ma mb mc ij bi translated">在Azure Data Factory 的<a class="ae kv" href="https://docs.microsoft.com/en-us/azure/data-factory/monitor-visually" rel="noopener ugc nofollow" target="_blank"> Monitor选项卡中，可以看到许多关于所有执行状态的信息。这是一种简单且图形化的方法来检查某个东西是否失败以及失败的原因。</a></p><p id="eb0a" class="pw-post-body-paragraph lh li iq lj b lk ll jr lm ln lo ju lp lq lr ls lt lu lv lw lx ly lz ma mb mc ij bi translated">显然，当管道没有问题，但数据有问题时，问题就来了(<em class="md"> EX </em> : <em class="md"> Delta数据比平时少，巨大的结构变化……</em>)。这种类型的信息不能仅通过监控您的管道来获得。这就是为什么有必要构建一个特定的监控系统来存储每个执行的特定信息。</p><h2 id="fb3a" class="lb lc iq bd me mf mg dn mh mi mj dp mk lq ml mm mn lu mo mp mq ly mr ms mt mu bi translated">为什么您的数据管道需要定制日志？</h2><ul class=""><li id="0e16" class="mv mw iq lj b lk mx ln my lq mz lu na ly nb mc nc nd ne nf bi translated"><strong class="lj ir">故障排除</strong>。您可以跟踪表的问题是何时开始的，并复制环境来研究问题。</li><li id="cef2" class="mv mw iq lj b lk ng ln nh lq ni lu nj ly nk mc nc nd ne nf bi translated"><strong class="lj ir">统计</strong>。能够用简单的SQL查询解决关于元存储的问题:每天处理多少行？你的桌子有多大？你每小时执行多少工作？</li><li id="d8de" class="mv mw iq lj b lk ng ln nh lq ni lu nj ly nk mc nc nd ne nf bi translated"><strong class="lj ir">监控</strong>。您可以很容易地看到所有执行的状态，或者您可以实现<a class="ae kv" href="https://en.wikipedia.org/wiki/Cron" rel="noopener ugc nofollow" target="_blank"> Cron jobs </a>来持续监控几个指标，以便发出警报。</li><li id="ed52" class="mv mw iq lj b lk ng ln nh lq ni lu nj ly nk mc nc nd ne nf bi translated"><strong class="lj ir">数据质量检测</strong>使用简单的指标(表时间戳、行数、结构变化……)。</li></ul><h2 id="bb0d" class="lb lc iq bd me mf mg dn mh mi mj dp mk lq ml mm mn lu mo mp mq ly mr ms mt mu bi translated">记录表的结构示例:</h2><pre class="kg kh ki kj gt kw kx ky kz aw la bi"><span id="f736" class="lb lc iq kx b gy ld le l lf lg">CREATE TABLE logs.EXAMPLE_LOG_TABLE (</span><span id="ba24" class="lb lc iq kx b gy nl le l lf lg">ID bigint IDENTITY(1,1) NOT NULL,</span><span id="1f04" class="lb lc iq kx b gy nl le l lf lg">ID_TRACK_PROCESSING bigint NOT NULL,</span><span id="f701" class="lb lc iq kx b gy nl le l lf lg">TABLE_NAME nvarchar(MAX) COLLATE SQL_Latin1_General_CP1_CI_AS NOT NULL,</span><span id="d46e" class="lb lc iq kx b gy nl le l lf lg">SCHEMA_NAME nvarchar(50) COLLATE SQL_Latin1_General_CP1_CI_AS NOT NULL,</span><span id="2ac6" class="lb lc iq kx b gy nl le l lf lg">PRIMARY_KEYS nvarchar(MAX) COLLATE SQL_Latin1_General_CP1_CI_AS NULL,</span><span id="6aaa" class="lb lc iq kx b gy nl le l lf lg">STATUS nvarchar(10) COLLATE SQL_Latin1_General_CP1_CI_AS NOT NULL,</span><span id="49b2" class="lb lc iq kx b gy nl le l lf lg">TIME_TAKEN bigint NOT NULL,</span><span id="72bd" class="lb lc iq kx b gy nl le l lf lg">RUN_DT datetime NOT NULL,</span><span id="e1a7" class="lb lc iq kx b gy nl le l lf lg">CREATED_BY_ID nvarchar(50) COLLATE SQL_Latin1_General_CP1_CI_AS NOT NULL,</span><span id="14fc" class="lb lc iq kx b gy nl le l lf lg">CREATED_TS datetime NOT NULL,</span><span id="9031" class="lb lc iq kx b gy nl le l lf lg">DATABRICKS_JOB_URL nvarchar(MAX) COLLATE SQL_Latin1_General_CP1_CI_AS NULL,</span><span id="c14d" class="lb lc iq kx b gy nl le l lf lg">DATAFACTORY_PIPELINE_URL nvarchar(MAX) COLLATE SQL_Latin1_General_CP1_CI_AS NULL,</span><span id="4ef7" class="lb lc iq kx b gy nl le l lf lg">LAST_DSTS nvarchar(20) COLLATE SQL_Latin1_General_CP1_CI_AS NULL,</span><span id="51a2" class="lb lc iq kx b gy nl le l lf lg">LIVE_ROWS nvarchar(100) COLLATE SQL_Latin1_General_CP1_CI_AS NULL,</span><span id="7a45" class="lb lc iq kx b gy nl le l lf lg">REPLICATION_ROWS nvarchar(20) COLLATE SQL_Latin1_General_CP1_CI_AS NULL,</span><span id="915b" class="lb lc iq kx b gy nl le l lf lg">DELTA_VERSION nvarchar(10) COLLATE SQL_Latin1_General_CP1_CI_AS NULL,</span><span id="d9a3" class="lb lc iq kx b gy nl le l lf lg">COLUMNS nvarchar(MAX) COLLATE SQL_Latin1_General_CP1_CI_AS NULL,</span><span id="0f5d" class="lb lc iq kx b gy nl le l lf lg"><strong class="kx ir">CONSTRAINT</strong> PK__EX_LOG__YYY432 PRIMARY KEY (ID)</span><span id="ccb5" class="lb lc iq kx b gy nl le l lf lg">)</span></pre><ul class=""><li id="346c" class="mv mw iq lj b lk ll ln lo lq nm lu nn ly no mc nc nd ne nf bi translated"><strong class="lj ir"> ID </strong>:日志的ID。这将是一个自动生成的值(参见<em class="md">约束</em>)。</li><li id="01ac" class="mv mw iq lj b lk ng ln nh lq ni lu nj ly nk mc nc nd ne nf bi translated"><strong class="lj ir"> ID_TRACK_PROCESSING </strong>:触发作业执行的摄取表的ID(在TRACK_PROCESSING表中)。</li><li id="1064" class="mv mw iq lj b lk ng ln nh lq ni lu nj ly nk mc nc nd ne nf bi translated"><strong class="lj ir">模式名&amp;表名</strong>:被插入/处理的表的模式和表名。</li><li id="ce9e" class="mv mw iq lj b lk ng ln nh lq ni lu nj ly nk mc nc nd ne nf bi translated"><strong class="lj ir"> PRIMARY_KEYS </strong>:如果表格有主键，并且这些主键正被用于执行合并。</li><li id="11d3" class="mv mw iq lj b lk ng ln nh lq ni lu nj ly nk mc nc nd ne nf bi translated"><strong class="lj ir">状态</strong>:流程状态(成功或失败)。</li><li id="254d" class="mv mw iq lj b lk ng ln nh lq ni lu nj ly nk mc nc nd ne nf bi translated"><strong class="lj ir"> RUN_DT </strong>:作业开始时的时间戳。</li><li id="49c2" class="mv mw iq lj b lk ng ln nh lq ni lu nj ly nk mc nc nd ne nf bi translated"><strong class="lj ir">耗时</strong>:工作完成所需的时间。</li><li id="ece0" class="mv mw iq lj b lk ng ln nh lq ni lu nj ly nk mc nc nd ne nf bi translated"><strong class="lj ir"> CREATED_BY_ID </strong>:标识创建日志的工具(在我们的例子中是<em class="md"> Azure数据工厂</em>)。</li><li id="5dc8" class="mv mw iq lj b lk ng ln nh lq ni lu nj ly nk mc nc nd ne nf bi translated"><strong class="lj ir"> CREATED_TS </strong>:日志创建时的时间戳。</li><li id="bef1" class="mv mw iq lj b lk ng ln nh lq ni lu nj ly nk mc nc nd ne nf bi translated"><strong class="lj ir"> DATABRICKS_JOB_URL </strong> : URL，在其中可以找到执行的每一步的代码和阶段。</li><li id="9cbb" class="mv mw iq lj b lk ng ln nh lq ni lu nj ly nk mc nc nd ne nf bi translated"><strong class="lj ir"> DATAFACTORY_JOB_URL </strong>:标识作业已完成的<em class="md"> ADF </em>管道的URL。</li><li id="ee6c" class="mv mw iq lj b lk ng ln nh lq ni lu nj ly nk mc nc nd ne nf bi translated"><strong class="lj ir"> LAST_DSTS </strong>:表格的最新时间戳。</li><li id="ce65" class="mv mw iq lj b lk ng ln nh lq ni lu nj ly nk mc nc nd ne nf bi translated"><strong class="lj ir"> LIVE_ROWS </strong>:作业执行后表格的行数。</li><li id="04ef" class="mv mw iq lj b lk ng ln nh lq ni lu nj ly nk mc nc nd ne nf bi translated"><strong class="lj ir"> REPLICATION_ROWS </strong>:最近一次执行中插入/处理的行数(如果满载，将等于LIVE_ROWS)。</li><li id="33c6" class="mv mw iq lj b lk ng ln nh lq ni lu nj ly nk mc nc nd ne nf bi translated"><strong class="lj ir"> DELTA_VERSION </strong> : <a class="ae kv" href="https://databricks.com/blog/2017/10/25/databricks-delta-a-unified-management-system-for-real-time-big-data.html" rel="noopener ugc nofollow" target="_blank">数据块摄取作业后表格的DELTA版本</a>。</li><li id="16d9" class="mv mw iq lj b lk ng ln nh lq ni lu nj ly nk mc nc nd ne nf bi translated"><strong class="lj ir">列</strong>:摄取作业后表格的结构(列名和类型)。</li></ul><p id="c6f3" class="pw-post-body-paragraph lh li iq lj b lk ll jr lm ln lo ju lp lq lr ls lt lu lv lw lx ly lz ma mb mc ij bi translated"><strong class="lj ir">您可以使用任何您认为对您的用例有用的字段。我选择的字段是我的上下文中需要的字段。</strong></p><h2 id="c2c4" class="lb lc iq bd me mf mg dn mh mi mj dp mk lq ml mm mn lu mo mp mq ly mr ms mt mu bi translated">如何生成日志</h2><p id="eb83" class="pw-post-body-paragraph lh li iq lj b lk mx jr lm ln my ju lp lq np ls lt lu nq lw lx ly nr ma mb mc ij bi translated">每个数据作业都应该生成关于它刚刚完成的过程的元数据。按照我的另一篇文章中描述的例子:</p><div class="ns nt gp gr nu nv"><a href="https://medium.com/@ivangomezarnedo/how-to-orchestrate-databricks-jobs-from-azure-data-factory-using-databricks-rest-api-4d5e8c577581" rel="noopener follow" target="_blank"><div class="nw ab fo"><div class="nx ab ny cl cj nz"><h2 class="bd ir gy z fp oa fr fs ob fu fw ip bi translated">如何使用Databricks REST API编排Azure数据工厂中的Databricks作业</h2><div class="oc l"><h3 class="bd b gy z fp oa fr fs ob fu fw dk translated">比使用Azure Data Factory官方Databricks笔记本更好(也更便宜)的控制工作的方式…</h3></div><div class="od l"><p class="bd b dl z fp oa fr fs ob fu fw dk translated">medium.com</p></div></div><div class="oe l"><div class="of l og oh oi oe oj kp nv"/></div></div></a></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ok"><img src="../Images/c7a841c5136bb0167b2dcb710f10ee24.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QM5o00QP-DttunCeVo8awA.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><strong class="bd ol">(图片作者)</strong>红色表示写日志的活动。</p></figure><p id="ae7e" class="pw-post-body-paragraph lh li iq lj b lk ll jr lm ln lo ju lp lq lr ls lt lu lv lw lx ly lz ma mb mc ij bi translated">一旦作业被标识为完成(成功或失败)，将执行一个活动，该活动将从执行中收集数据并将其写入相应的日志表(日志生成):</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi om"><img src="../Images/8cc80a1753ca0aac9257a353f453093d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1144/format:webp/1*yzt99EsHQ87tbf-UA9kUHA.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><strong class="bd ol">(图片由作者提供)</strong>确保在重试策略中使用较高的数字，以确保活动将被执行</p></figure><p id="cc47" class="pw-post-body-paragraph lh li iq lj b lk ll jr lm ln lo ju lp lq lr ls lt lu lv lw lx ly lz ma mb mc ij bi translated"><strong class="lj ir">使用的查询:</strong></p><pre class="kg kh ki kj gt kw kx ky kz aw la bi"><span id="401d" class="lb lc iq kx b gy ld le l lf lg">INSERT INTO logs.EXAMPLE_LOG_TABLE<br/>(ID_TRACK_PROCESSING, TABLE_NAME, SCHEMA_NAME, PRIMARY_KEYS, STATUS, TIME_TAKEN, RUN_DT, CREATED_BY_ID, CREATED_TS, DATABRICKS_JOB_URL, DATAFACTORY_PIPELINE_URL, LAST_DSTS, LIVE_ROWS, REPLICATION_ROWS, DELTA_VERSION, COLUMNS)<br/> VALUES(<br/><strong class="kx ir">-- Data from the table that triggers the execution.</strong><br/>  @{item().ID},<br/> '@{item().TABLE_NAME}',<br/> '@{item().SCHEMA_NAME}',<br/> '@{item().PRIMARY_KEY}',<br/> 'SUCCESS',<br/><strong class="kx ir">-- Statistics and metadata of the execution.</strong><br/>@{activity('Check job status').output.metadata.execution_duration}/1000,<br/> DATEADD(SECOND , -@{activity('Check job status').output.metadata.execution_duration}/1000,CURRENT_TIMESTAMP),<br/> 'AZURE DATA FACTORY',<br/> CURRENT_TIMESTAMP, <br/>'@{activity('Check job status').output.metadata.run_page_url}',<br/> '<a class="ae kv" href="https://adf.azure.com/monitoring/pipelineruns/@{pipeline().RunId}?factory=%2Fsubscriptions%2Ff772839c-a925-4576-80fc-b8cb60966326%2FresourceGroups%2FRG_EDL_COREDLZ_01_PROD%2Fproviders%2FMicrosoft.DataFactory%2Ffactories%2Fedlproddlzadf01'," rel="noopener ugc nofollow" target="_blank">https://adf.azure.com/monitoring/pipelineruns/@{pipeline().RunId}...',</a><br/><strong class="kx ir">-- Output from the execution.</strong><br/>'@{split(activity('Check job status').output.notebook_output.result,'|')[0]}', --Table timestamp<br/>'@{split(activity('Check job status').output.notebook_output.result,'|')[1]}', --Table nº of rows<br/>'@{split(activity('Check job status').output.notebook_output.result,'|')[2]}', --Rows ingested/updated in the job<br/>'@{split(activity('Check job status').output.notebook_output.result,'|')[3]}', --Databricks Delta version.<br/>'@{split(activity('Check job status').output.notebook_output.result,'|')[4]}' --Schema of the table.<br/>); SELECT 1+1</span></pre><p id="56e0" class="pw-post-body-paragraph lh li iq lj b lk ll jr lm ln lo ju lp lq lr ls lt lu lv lw lx ly lz ma mb mc ij bi translated">日志由以下内容生成:</p><ul class=""><li id="a103" class="mv mw iq lj b lk ll ln lo lq nm lu nn ly no mc nc nd ne nf bi translated"><strong class="lj ir">触发执行的表中的数据。</strong></li><li id="0dcf" class="mv mw iq lj b lk ng ln nh lq ni lu nj ly nk mc nc nd ne nf bi translated"><strong class="lj ir">统计和元数据的执行。</strong></li><li id="6283" class="mv mw iq lj b lk ng ln nh lq ni lu nj ly nk mc nc nd ne nf bi translated"><strong class="lj ir">从执行中输出。</strong></li></ul><p id="0200" class="pw-post-body-paragraph lh li iq lj b lk ll jr lm ln lo ju lp lq lr ls lt lu lv lw lx ly lz ma mb mc ij bi translated">为了从执行中提取输出，由于<em class="md">数据块</em>被用作核心处理工具，数据作业中执行的最新命令将是:</p><pre class="kg kh ki kj gt kw kx ky kz aw la bi"><span id="3d31" class="lb lc iq kx b gy ld le l lf lg">dbutils.notebook.exit(string)</span></pre><p id="d439" class="pw-post-body-paragraph lh li iq lj b lk ll jr lm ln lo ju lp lq lr ls lt lu lv lw lx ly lz ma mb mc ij bi translated">并非所有的进程都必须向同一个日志表中写入新条目，不同的进程可能有不同的需求，并且可能需要在其日志表中存储不同的信息。如果您想将所有日志写到同一个表中，那么一个好的选择是添加一个新字段来标识生成它们的进程。</p><h1 id="d4d7" class="on lc iq bd me oo op oq mh or os ot mk jw ou jx mn jz ov ka mq kc ow kd mt ox bi translated">使用日志创建数据目录</h1><pre class="kg kh ki kj gt kw kx ky kz aw la bi"><span id="6ef2" class="lb lc iq kx b gy ld le l lf lg">“Simplicity is a great virtue but it requires hard work to achieve it and education to appreciate it. And to make matters worse: complexity sells better.”<br/>― <strong class="kx ir">Edsger Wybe Dijkstra</strong></span></pre><p id="a1ed" class="pw-post-body-paragraph lh li iq lj b lk ll jr lm ln lo ju lp lq lr ls lt lu lv lw lx ly lz ma mb mc ij bi translated"><strong class="lj ir">这里建立的数据目录是一个简单的解决方案，并不意味着要取代一个更完整的解决方案(<em class="md"> EX </em> : </strong> <a class="ae kv" href="https://datahubproject.io/" rel="noopener ugc nofollow" target="_blank"> <strong class="lj ir"> Linkedin数据中心</strong> </a> <strong class="lj ir">)，但在大多数情况下，这已经足够了。</strong></p><p id="bcb1" class="pw-post-body-paragraph lh li iq lj b lk ll jr lm ln lo ju lp lq lr ls lt lu lv lw lx ly lz ma mb mc ij bi translated">一旦我们在日志表中有了足够多的条目，我们就可以创建一个视图来使用这些信息，并生成一个包含元存储完整快照的“数据目录”。</p><p id="8b02" class="pw-post-body-paragraph lh li iq lj b lk ll jr lm ln lo ju lp lq lr ls lt lu lv lw lx ly lz ma mb mc ij bi translated"><strong class="lj ir">好处</strong>:</p><ul class=""><li id="f532" class="mv mw iq lj b lk ll ln lo lq nm lu nn ly no mc nc nd ne nf bi translated">不要重新发明轮子:已经用于其他目的(故障排除、统计……)的东西将被重新用于创建数据目录。</li><li id="23c9" class="mv mw iq lj b lk ng ln nh lq ni lu nj ly nk mc nc nd ne nf bi translated">当一个视图被使用时，“数据目录”将总是尽可能地更新，并且不需要其他管道/过程。</li></ul><pre class="kg kh ki kj gt kw kx ky kz aw la bi"><span id="a143" class="lb lc iq kx b gy ld le l lf lg">CREATE VIEW logs.all_tables_data_catalog AS<br/>SELECT id,<br/>       schema_name,<br/>       table_name,<br/>       status ,<br/>       time_taken ,<br/>       created_ts ,<br/>       databricks_job_url ,<br/>       datafactory_pipeline_url,<br/>       'SAP_INGESTION' AS type,<br/>       last_dsts ,<br/>       columns,<br/>       <em class="md">Cast</em>(live_rows AS <em class="md">DECIMAL</em>) AS "LIVE_ROWS",<br/>       delta_version<br/>FROM   (<br/>                SELECT   *,<br/>                         Row_number() OVER (partition BY table_name, schema_name ORDER BY created_ts DESC ) AS row_no<br/>                FROM     [logs].[SAP_INGESTION_LOG_TABLE]<br/>                WHERE    status ='SUCCESS') logs_SAP<br/>WHERE  row_no=1<br/>UNION<br/>SELECT id,<br/>       schema_name,<br/>       table_name ,<br/>       status ,<br/>       time_taken_copy+time_taken_merge AS "TIME_TAKEN" ,<br/>       created_ts ,<br/>       databricks_job_url ,<br/>       datafactory_pipeline_url,<br/>       'MYSQL_INGESTION' AS type,<br/>       last_dsts,<br/>       columns,<br/>       <em class="md">Cast</em>(live_rows AS <em class="md">DECIMAL</em>) AS "LIVE_ROWS",<br/>       delta_version<br/>FROM   (<br/>                SELECT   *,<br/>                         <strong class="kx ir">Row_number</strong>() OVER (partition BY table_name, schema_name ORDER BY created_ts DESC ) AS row_no<br/>                FROM     [logs].[MYSQL_INGESTION_LOG_TABLE]<br/>                WHERE    status ='SUCCESS') logs_MYSQL<br/>WHERE  row_no=1<br/>UNION<br/>SELECT id,<br/>       schema_name,<br/>       table_name ,<br/>       status ,<br/>       time_taken ,<br/>       created_ts ,<br/>       databricks_job_url ,<br/>       datafactory_pipeline_url,<br/>       'ORACLE_INGESTION' AS type,<br/>       last_dsts,<br/>       columns,<br/>       <em class="md">Cast</em>(live_rows AS <em class="md">DECIMAL</em>) AS "LIVE_ROWS",<br/>       delta_version<br/>FROM   (<br/>                SELECT   *,<br/>                         <strong class="kx ir">Row_number</strong>() OVER (partition BY table_name, schema_name ORDER BY created_ts DESC ) AS row_no<br/>                FROM     [logs].[ORACLE_INGESTION_LOG_TABLE]<br/>                WHERE    status ='SUCCESS') logs_HANA<br/>WHERE  row_no=1<br/>where  status ='SUCCESS') x WHERE row_no=1</span></pre><p id="ddf2" class="pw-post-body-paragraph lh li iq lj b lk ll jr lm ln lo ju lp lq lr ls lt lu lv lw lx ly lz ma mb mc ij bi translated">从每个日志表中只选择公共字段(只能对相同数量的列执行union)，只搜索成功的摄取，并使用r <em class="md"> ow_number窗口函数</em>提取每个表的最新日志(由其模式和名称标识)。</p><p id="5090" class="pw-post-body-paragraph lh li iq lj b lk ll jr lm ln lo ju lp lq lr ls lt lu lv lw lx ly lz ma mb mc ij bi translated">这为用户提供了一种方法来检查元存储中的每个表，其中包括:</p><ul class=""><li id="8403" class="mv mw iq lj b lk ll ln lo lq nm lu nn ly no mc nc nd ne nf bi translated">表的结构(列名和类型)。</li><li id="d445" class="mv mw iq lj b lk ng ln nh lq ni lu nj ly nk mc nc nd ne nf bi translated">上次更新的时间。</li><li id="3faf" class="mv mw iq lj b lk ng ln nh lq ni lu nj ly nk mc nc nd ne nf bi translated">表行的最新时间戳(可能表加载成功，但数据已经过期)。</li><li id="9488" class="mv mw iq lj b lk ng ln nh lq ni lu nj ly nk mc nc nd ne nf bi translated">行数。</li></ul><p id="3fd2" class="pw-post-body-paragraph lh li iq lj b lk ll jr lm ln lo ju lp lq lr ls lt lu lv lw lx ly lz ma mb mc ij bi translated">它还可以用来轻松检测整个元存储中的数据质量问题。</p><p id="1041" class="pw-post-body-paragraph lh li iq lj b lk ll jr lm ln lo ju lp lq lr ls lt lu lv lw lx ly lz ma mb mc ij bi translated">这个视图可以在不同的仪表板中使用，这些仪表板可以向用户显示相同的信息，但是以一种更“<em class="md">美化</em>的方式。这个简单的“数据目录”可以解决大部分考虑购买第三方数据目录解决方案的团队的需求。</p><h2 id="6351" class="lb lc iq bd me mf mg dn mh mi mj dp mk lq ml mm mn lu mo mp mq ly mr ms mt mu bi translated">如果您的日志表在不同的数据库实例中会怎样？</h2><p id="b1f2" class="pw-post-body-paragraph lh li iq lj b lk mx jr lm ln my ju lp lq np ls lt lu nq lw lx ly nr ma mb mc ij bi translated">如果您想要在“数据目录”中显示的数据在不同的系统中(<em class="md">例如</em> : SQL Server、Azure SQL和HANA)，您可以使用<a class="ae kv" href="https://docs.microsoft.com/en-us/sql/relational-databases/linked-servers/create-linked-servers-sql-server-database-engine?view=sql-server-ver15" rel="noopener ugc nofollow" target="_blank"> SQL Server链接服务器</a>查询其他系统，就好像它们的表属于第一个系统一样。</p><p id="763f" class="pw-post-body-paragraph lh li iq lj b lk ll jr lm ln lo ju lp lq lr ls lt lu lv lw lx ly lz ma mb mc ij bi translated"><strong class="lj ir">好处</strong>:</p><ul class=""><li id="5ac1" class="mv mw iq lj b lk ll ln lo lq nm lu nn ly no mc nc nd ne nf bi translated">避免不必要的数据移动，因为数据是直接从源系统查询的。</li><li id="cf2f" class="mv mw iq lj b lk ng ln nh lq ni lu nj ly nk mc nc nd ne nf bi translated">数据总是尽可能地更新(因为没有使用源表的快照，而是查询活动表)。</li><li id="db76" class="mv mw iq lj b lk ng ln nh lq ni lu nj ly nk mc nc nd ne nf bi translated">在网络受限或需要查询大量外部资源的情况下，可以实现类似于<a class="ae kv" href="https://docs.microsoft.com/en-us/sql/t-sql/statements/create-materialized-view-as-select-transact-sql?view=azure-sqldw-latest" rel="noopener ugc nofollow" target="_blank">物化视图</a>的东西，但是显示的数据可能不是最新的。</li></ul><p id="74ba" class="pw-post-body-paragraph lh li iq lj b lk ll jr lm ln lo ju lp lq lr ls lt lu lv lw lx ly lz ma mb mc ij bi translated"><strong class="lj ir"> SQL Server与HANA的连接:</strong></p><div class="ns nt gp gr nu nv"><a href="https://komport.medium.com/if-you-need-access-you-data-stored-in-sap-hana-database-from-mssql-database-server-you-can-add-877e8793acf7" rel="noopener follow" target="_blank"><div class="nw ab fo"><div class="nx ab ny cl cj nz"><h2 class="bd ir gy z fp oa fr fs ob fu fw ip bi translated">将您的SAP HANA数据库添加为MSSQL Server链接服务器。</h2><div class="oc l"><h3 class="bd b gy z fp oa fr fs ob fu fw dk translated">如果您需要从MSSQL数据库服务器访问存储在SAP HANA数据库中的数据，您可以添加…</h3></div><div class="od l"><p class="bd b dl z fp oa fr fs ob fu fw dk translated">komport.medium.com</p></div></div><div class="oe l"><div class="oy l og oh oi oe oj kp nv"/></div></div></a></div><p id="27d1" class="pw-post-body-paragraph lh li iq lj b lk ll jr lm ln lo ju lp lq lr ls lt lu lv lw lx ly lz ma mb mc ij bi translated">首先<a class="ae kv" href="https://help.sap.com/viewer/647582cc310341319e6e8d6f23f1f230/2.00.2.0/en-US/63ee40091a1847b886e3bcfe71ecb91a.html" rel="noopener ugc nofollow" target="_blank">从SAP官方网站</a>下载必要的ODBC驱动，然后按照上一篇文章的步骤操作。在安装了HANA实例的机器上完成安装后，ODBC将如下所示:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oz"><img src="../Images/602784fe4ef3d3c56c478ae8423102e7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ihK7jJbCCmqRkDh_y4VUwA.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><strong class="bd ol">作者图片</strong></p></figure><p id="7871" class="pw-post-body-paragraph lh li iq lj b lk ll jr lm ln lo ju lp lq lr ls lt lu lv lw lx ly lz ma mb mc ij bi translated">链接服务器是使用SQL Server中的以下命令创建的:</p><pre class="kg kh ki kj gt kw kx ky kz aw la bi"><span id="e252" class="lb lc iq kx b gy ld le l lf lg">EXEC sp_addlinkedserver</span><span id="09d6" class="lb lc iq kx b gy nl le l lf lg">@server='HANA_PRO', -- this is just a descriptive name</span><span id="f52c" class="lb lc iq kx b gy nl le l lf lg">@srvproduct='HANA1.0',  -- this is just a descriptive name</span><span id="d336" class="lb lc iq kx b gy nl le l lf lg">@provider='MSDASQL',   --&gt;This is a fixed value as this is the standard name for this type of provider connection</span><span id="fb04" class="lb lc iq kx b gy nl le l lf lg">@datasrc='HANAPRO';  --&gt;Here needs the Data Source Name to be entered that was created before</span><span id="894d" class="lb lc iq kx b gy nl le l lf lg">EXEC sp_addlinkedsrvlogin</span><span id="1452" class="lb lc iq kx b gy nl le l lf lg">@useself = 'FALSE',</span><span id="65b0" class="lb lc iq kx b gy nl le l lf lg">@rmtsrvname = 'HANA_PRO',  -- You need to use the name that you have used in the sp_addlinkedserver as @server name</span><span id="5fc6" class="lb lc iq kx b gy nl le l lf lg">@locallogin = NULL,</span><span id="6379" class="lb lc iq kx b gy nl le l lf lg">@rmtuser = 'YOUR HANA USER',</span><span id="05f3" class="lb lc iq kx b gy nl le l lf lg">@rmtpassword = 'YOUR HANA PASSWORD';</span></pre><p id="106e" class="pw-post-body-paragraph lh li iq lj b lk ll jr lm ln lo ju lp lq lr ls lt lu lv lw lx ly lz ma mb mc ij bi translated">在完成所有上述步骤后，通过在SQL Server中运行以下查询来执行系统之间的连接检查:</p><pre class="kg kh ki kj gt kw kx ky kz aw la bi"><span id="bb33" class="lb lc iq kx b gy ld le l lf lg">SELECT * FROM</span><span id="c044" class="lb lc iq kx b gy nl le l lf lg">(SELECT *  FROM OPENQUERY([HANA_PRO],'SELECT * FROM LOGS.HANA_LOG_TABLE;')) a;</span></pre><p id="dfda" class="pw-post-body-paragraph lh li iq lj b lk ll jr lm ln lo ju lp lq lr ls lt lu lv lw lx ly lz ma mb mc ij bi translated"><strong class="lj ir"> SQL Server与Azure SQL的连接:</strong></p><p id="3d7e" class="pw-post-body-paragraph lh li iq lj b lk ll jr lm ln lo ju lp lq lr ls lt lu lv lw lx ly lz ma mb mc ij bi translated">这是一个更简单的过程，因为不需要ODBC:</p><div class="ns nt gp gr nu nv"><a href="https://www.sqlshack.com/create-linked-server-azure-sql-database/" rel="noopener  ugc nofollow" target="_blank"><div class="nw ab fo"><div class="nx ab ny cl cj nz"><h2 class="bd ir gy z fp oa fr fs ob fu fw ip bi translated">如何创建到Azure SQL数据库的链接服务器</h2><div class="oc l"><h3 class="bd b gy z fp oa fr fs ob fu fw dk translated">2017年9月27日通过链接服务器允许从另一个SQL Server或另一个数据源(如Excel)访问数据…</h3></div><div class="od l"><p class="bd b dl z fp oa fr fs ob fu fw dk translated">www.sqlshack.com</p></div></div><div class="oe l"><div class="pa l og oh oi oe oj kp nv"/></div></div></a></div><p id="0f60" class="pw-post-body-paragraph lh li iq lj b lk ll jr lm ln lo ju lp lq lr ls lt lu lv lw lx ly lz ma mb mc ij bi translated">最后，由于使用<em class="md">链接的服务器</em>就像在SQL Server中添加其他DB系统作为新的DB，视图的代码将与前面的非常相似(视图只从同一个DB实例中读取数据):</p><pre class="kg kh ki kj gt kw kx ky kz aw la bi"><span id="32dc" class="lb lc iq kx b gy ld le l lf lg">CREATE VIEW logs.all_tables_data_catalog<br/>AS<br/>  SELECT id,<br/>         schema_name,<br/>         table_name,<br/>         status,<br/>         time_taken,<br/>         created_ts,<br/>         databricks_job_url,<br/>         datafactory_pipeline_url,<br/>         'SAP_INGESTION'            AS TYPE,<br/>         last_dsts,<br/>         columns,<br/>         <em class="md">Cast</em>(live_rows AS <em class="md">DECIMAL</em>) AS "LIVE_ROWS",<br/>         delta_version<br/>  FROM   (SELECT *,<br/>                 Row_number()<br/>                   OVER (<br/>                     partition BY table_name, schema_name<br/>                     ORDER BY created_ts DESC ) AS row_no<br/>          FROM   <strong class="kx ir">[logs].[sap_ingestion_log_table] -- Query to an internal table</strong><br/>          WHERE  status = 'SUCCESS') sql_server_row_no<br/>  WHERE  row_no = 1 <br/>  UNION<br/>  SELECT id,<br/>         schema_name,<br/>         table_name,<br/>         status,<br/>         time_taken_copy + time_taken_merge AS "TIME_TAKEN",<br/>         created_ts,<br/>         databricks_job_url,<br/>         datafactory_pipeline_url,<br/>         'MYSQL_INGESTION'                  AS TYPE,<br/>         last_dsts,<br/>         columns,<br/>         <em class="md">Cast</em>(live_rows AS <em class="md">DECIMAL</em>)         AS "LIVE_ROWS",<br/>         delta_version<br/>  FROM   (SELECT *,<br/>                 Row_number()<br/>                   OVER (<br/>                     partition BY table_name, schema_name<br/>                     ORDER BY created_ts DESC ) AS row_no<br/>          FROM   [<strong class="kx ir">AZURE SQL (PROD)].[BD].[logs].[azure_sql_ingestion_log] -- Query to an external table in AZURE SQL.</strong><br/>          WHERE  status = 'SUCCESS') azure_sql_row_no<br/>  WHERE  row_no = 1<br/>  UNION<br/>  SELECT id,<br/>         schema_name,<br/>         table_name,<br/>         status,<br/>         time_taken,<br/>         created_ts,<br/>         databricks_job_url,<br/>         datafactory_pipeline_url,<br/>         type,<br/>         last_dsts,<br/>         columns,<br/>         <em class="md">Cast</em>(live_rows AS <em class="md">DECIMAL</em>) AS "LIVE_ROWS",<br/>         delta_version<br/>  FROM   (SELECT *,<br/>                 Row_number()<br/>                   OVER (<br/>                     partition BY table_name, schema_name<br/>                     ORDER BY created_ts DESC ) AS row_no<br/>          FROM   (SELECT *<br/>                  FROM   <strong class="kx ir"><em class="md">Openquery</em>([hana_pro], 'SELECT ID,SCHEMA_NAME, TABLE_NAME,STATUS , TIME_TAKEN , CREATED_TS , DATABRICKS_JOB_URL ,DATAFACTORY_PIPELINE_URL, ''HANA_INGESTION'' as TYPE, LAST_DSTS ,COLUMNS, CAST(LIVE_ROWS as decimal) as "LIVE_ROWS", DELTA_VERSION, STATUS  FROM LOGS.HANA_INGESTION_LOG <br/>WHERE  STATUS= 'SUCCESS' ;') ) hana1) hana_row_no -- Query to an external table in HANA.</strong><br/>  WHERE  row_no = 1</span><span id="db22" class="lb lc iq kx b gy nl le l lf lg"><em class="md">-- Note that the Row_function could be implemented directly in the source system being queried, but has been left as is for code clarity</em>.</span></pre></div><div class="ab cl pb pc hu pd" role="separator"><span class="pe bw bk pf pg ph"/><span class="pe bw bk pf pg ph"/><span class="pe bw bk pf pg"/></div><div class="ij ik il im in"><h2 id="cd81" class="lb lc iq bd me mf mg dn mh mi mj dp mk lq ml mm mn lu mo mp mq ly mr ms mt mu bi translated">结论</h2><p id="effb" class="pw-post-body-paragraph lh li iq lj b lk mx jr lm ln my ju lp lq np ls lt lu nq lw lx ly nr ma mb mc ij bi translated">在本文中，我们看到:</p><ul class=""><li id="1d89" class="mv mw iq lj b lk ll ln lo lq nm lu nn ly no mc nc nd ne nf bi translated">为什么自定义日志是对数据管道的一个很好的补充。</li><li id="c42e" class="mv mw iq lj b lk ng ln nh lq ni lu nj ly nk mc nc nd ne nf bi translated">如何在完成数据管道执行之前生成自定义日志。</li><li id="3c3c" class="mv mw iq lj b lk ng ln nh lq ni lu nj ly nk mc nc nd ne nf bi translated">创建数据目录的好处。</li><li id="dac0" class="mv mw iq lj b lk ng ln nh lq ni lu nj ly nk mc nc nd ne nf bi translated">如何使用自定义日志创建数据目录。</li><li id="43c1" class="mv mw iq lj b lk ng ln nh lq ni lu nj ly nk mc nc nd ne nf bi translated">如何在没有ETL的情况下组合来自不同源系统的日志？</li></ul><p id="8206" class="pw-post-body-paragraph lh li iq lj b lk ll jr lm ln lo ju lp lq lr ls lt lu lv lw lx ly lz ma mb mc ij bi translated">尽管给出了关于如何使用某些工具开始创建定制日志的具体示例，但是该概念可以应用于其他编排器和其他处理工具。</p><p id="297d" class="pw-post-body-paragraph lh li iq lj b lk ll jr lm ln lo ju lp lq lr ls lt lu lv lw lx ly lz ma mb mc ij bi translated">仅此而已。我希望这对您有用，并且您不必像我一样花费时间。</p><p id="f4aa" class="pw-post-body-paragraph lh li iq lj b lk ll jr lm ln lo ju lp lq lr ls lt lu lv lw lx ly lz ma mb mc ij bi translated">感谢阅读。<strong class="lj ir">你可以在</strong><a class="ae kv" href="https://www.linkedin.com/in/ivan-gomez-arnedo/" rel="noopener ugc nofollow" target="_blank"><strong class="lj ir">Linkedin</strong></a><strong class="lj ir">上找到我。</strong></p><pre class="kg kh ki kj gt kw kx ky kz aw la bi"><span id="8514" class="lb lc iq kx b gy ld le l lf lg">“The success formula: solve your own problems and freely share the solutions.”<br/>― <a class="ae kv" href="https://twitter.com/naval/status/1444741381579177984" rel="noopener ugc nofollow" target="_blank"><strong class="kx ir">Naval Ravikant</strong></a></span></pre></div></div>    
</body>
</html>