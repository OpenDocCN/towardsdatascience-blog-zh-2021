<html>
<head>
<title>A Simple Maths Free PyTorch Model Framework</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">一个简单的数学自由PyTorch模型框架</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/a-simple-maths-free-pytorch-model-framework-3eedfd738bd4?source=collection_archive---------18-----------------------#2021-08-29">https://towardsdatascience.com/a-simple-maths-free-pytorch-model-framework-3eedfd738bd4?source=collection_archive---------18-----------------------#2021-08-29</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="d17b" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">喜欢数据科学和深度学习，但被所有的数学和公式搞糊涂了，只想要一个简单的解释和一组例子？我也是。所以现在我们来纠正一下，好吗？</h2></div><p id="7a94" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">本文的目标是尝试并制作一个简单的可解释的构建PyTorch深度学习模型的示例，以给出回归、分类(二元和多类)和多标签分类的示例。</p><p id="bdde" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我发现每一种都有几十个甚至几百个例子，但没有一个以相似的方式给出了每一种的例子，并且经常归结为大型的数学公式，所以我在不同的风格和方法之间来回转换，因此我一次又一次地困惑自己，所以我想要一些简单的东西可以重复使用。我还想尽可能简单地完成它们，使用可爱的、随处可得的库和模块，尽可能少地定制“我的解释”。</p><p id="b42a" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">因此，我不想陷入EDA、特征工程和所有那些非常重要的东西(我会说比模型更重要)，并希望尽可能保持简单和“只是模型的框架”。<br/>考虑到这一点，我们将使用来自<strong class="kh ir"> sklearn.datasets </strong>的精彩的<strong class="kh ir"> make_regression </strong>、<strong class="kh ir"> make_classification、</strong>和<strong class="kh ir">make _ multi Label _ classification</strong>，从而模拟一旦您为第一个基线模型做好所有EDA和功能工程准备，您的数据将处于的状态，这意味着我们将不会进行任何标签编码、解决不平衡等。</p><p id="8de5" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我也想完全远离数学。我会解释为什么我们在没有符号、公式和算法的情况下做我们正在做的事情。<br/>这不仅仅是给你一些代码来剪切/粘贴，而是向你展示我在这个过程中遇到的一些错误，从而产生(我希望)一组有用的函数和信息。</p><p id="026b" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我写这篇文章是为了帮助我拥有一个入门笔记本，我可以用它来做各种各样的事情，我希望这样做能帮助到其他人，所以我们开始吧。</p><p id="82c8" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">准备笔记本</strong></p><p id="f8e1" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">首先，加载相关模块。基本上就<strong class="kh ir">学</strong>、<strong class="kh ir"> torch </strong>、<strong class="kh ir"> NumPy </strong>、<strong class="kh ir"> matplotlib </strong>。</p><pre class="lb lc ld le gt lf lg lh li aw lj bi"><span id="c6e3" class="lk ll iq lg b gy lm ln l lo lp">from sklearn import metrics</span><span id="d7c8" class="lk ll iq lg b gy lq ln l lo lp">from sklearn.model_selection import train_test_split<br/>from sklearn.datasets import make_regression, make_classification, make_multilabel_classification<br/>from sklearn.preprocessing import LabelEncoder, StandardScaler, MinMaxScaler</span><span id="ff05" class="lk ll iq lg b gy lq ln l lo lp">import torch<br/>from torch.utils.data import Dataset, DataLoader<br/>import torch.optim as torch_optim<br/>import torch.nn as nn<br/>import torch.nn.functional as F</span><span id="ca28" class="lk ll iq lg b gy lq ln l lo lp">import numpy as np<br/>import matplotlib.pyplot as plt</span></pre><p id="3eba" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">创建可重复使用的PyTorch组件</strong></p><p id="9769" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">好了，下一节是我们将在整篇文章中反复使用的函数。我将概述他们做什么，但是损失函数和精度函数的任何细节将随着我们的进展详细解释。</p><p id="f16d" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">使用PyTorch数据集</strong></p><p id="4dee" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">作为其中的一部分，我们将熟悉并使用PyTorch数据集。为什么？嗯，它们提供了一个很好的方法来在我们的数据周围放置一个迭代器，并允许所有的好处，比如干净地批处理数据等等，所以为什么不使用它们呢。</p><p id="3f08" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这是建立一个数据库的最低要求，而且都是不言自明的。</p><p id="bfb7" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我相信你知道，深度学习模型喜欢数字，所以当你准备好数据时，所有的分类特征都已经被编码了。如果它是一个整数，即所有特性的np.float32和目标的np.int64，否则它也是np.float32。</p><p id="f0db" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在这个过程中，它还把我们可爱的numpy数组变成了时髦的PyTorch张量。</p><pre class="lb lc ld le gt lf lg lh li aw lj bi"><span id="27dd" class="lk ll iq lg b gy lm ln l lo lp">class MyCustomDataset(Dataset):<br/>    def __init__(self, X, Y, scale=False):<br/>        self.X = torch.from_numpy(X.astype(np.float32))<br/>        y_dtype = np.int64 if Y.dtype == np.int64 else np.float32<br/>        if scale: self.y = torch.from_numpy(MinMaxScaler().fit_transform(Y.reshape(-1,1)).astype(y_dtype))<br/>        else:     self.y = torch.from_numpy(Y.astype(y_dtype))<br/>        <br/>    def __len__(self):<br/>        return len(self.y)<br/>    <br/>    def __getitem__(self, idx):<br/>        return self.X[idx], self.y[idx]</span></pre><p id="4e1a" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">创建一个简单的PyTorch模型</strong></p><p id="8c18" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这里我们将创建一个相当简单的模型，因为这不是一篇关于特定问题类型的最佳模型类型的文章。这给你的是构建PyTorch模型的类的结构，你可以用你认为合适的任何东西改变/扩展/替换这些模型。</p><p id="bb23" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">它以输入X形状10(这将是我们在这些例子中使用的数据的大小)开始，并且可以有一个参数传递给它以形成最终的层(y形状)，默认为1。</p><pre class="lb lc ld le gt lf lg lh li aw lj bi"><span id="2a95" class="lk ll iq lg b gy lm ln l lo lp">class BetterTabularModule(nn.Module):<br/>    def __init__(self, out_features=1):<br/>        super().__init__()<br/>        self.lin1 = nn.Linear(10, 200)<br/>        self.lin2 = nn.Linear(200, 70)<br/>        self.lin3 = nn.Linear(70, out_features)<br/>        self.bn1 = nn.BatchNorm1d(10)<br/>        self.bn2 = nn.BatchNorm1d(200)<br/>        self.bn3 = nn.BatchNorm1d(70)<br/>        self.drops = nn.Dropout(0.3)<br/>    def forward(self, x):<br/>        x = self.bn1(x)<br/>        x = F.relu(self.lin1(x))<br/>        x = self.drops(x)<br/>        x = self.bn2(x)<br/>        x = F.relu(self.lin2(x))<br/>        x = self.drops(x)<br/>        x = self.bn3(x)<br/>        x = self.lin3(x)<br/>        return x</span></pre><p id="779f" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">设置一个简单的优化器</strong></p><p id="65a1" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">对于所有这些问题，我们将坚持使用Adam优化器。看起来很合适。</p><pre class="lb lc ld le gt lf lg lh li aw lj bi"><span id="3def" class="lk ll iq lg b gy lm ln l lo lp">def get_optimizer(model, lr=0.001, wd=0.0):<br/>    parameters = filter(lambda p: p.requires_grad, model.parameters())<br/>    optim = torch_optim.Adam(parameters, lr=lr, weight_decay=wd)<br/>    return optim</span></pre><p id="e107" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">现在一个简单的训练功能，训练循环和评估功能</strong></p><p id="84ca" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这些是使用PyTorch的标准方法，所以让我们将它们放在函数中，并在整个过程中使用它们。唯一需要改变的是，是的，你已经猜到了，损失函数，它将作为一个参数传入，还有精度函数。</p><pre class="lb lc ld le gt lf lg lh li aw lj bi"><span id="98cc" class="lk ll iq lg b gy lm ln l lo lp">def train_model(model, optim, train_dl, loss_func):<br/>    # Ensure the model is in Training mode<br/>    model.train()<br/>    total = 0<br/>    sum_loss = 0</span><span id="4722" class="lk ll iq lg b gy lq ln l lo lp">    for x, y in train_dl:<br/>        batch = y.shape[0]<br/>        # Train the model for this batch worth of data<br/>        logits = model(x)<br/>        # Run the loss function. We will decide what this will be when we call our Training Loop<br/>        loss = loss_func(logits, y)<br/>        # The next 3 lines do all the PyTorch back propagation goodness<br/>        optim.zero_grad()<br/>        loss.backward()<br/>        optim.step()<br/>        # Keep a running check of our total number of samples in this epoch<br/>        total += batch<br/>        # And keep a running total of our loss<br/>        sum_loss += batch*(loss.item())<br/>    return sum_loss/total</span><span id="d79c" class="lk ll iq lg b gy lq ln l lo lp">def train_loop(model, epochs, loss_func, lr=0.1, wd=0.001):<br/>    optim = get_optimizer(model, lr=lr, wd=wd)</span><span id="d2df" class="lk ll iq lg b gy lq ln l lo lp">    train_loss_list = []<br/>    val_loss_list = []<br/>    acc_list = []</span><span id="a424" class="lk ll iq lg b gy lq ln l lo lp">    for i in range(epochs): <br/>        loss = train_model(model, optim, train_dl, loss_func)<br/>        # After training this epoch, keep a list of progress of the loss of each epoch <br/>        train_loss_list.append(loss)<br/>        val, acc = val_loss(model, valid_dl, loss_func)<br/>        # Likewise for the validation loss and accuracy (if applicable)<br/>        val_loss_list.append(val)<br/>        acc_list.append(acc)<br/>        if acc &gt; 0: print("training loss: %.5f     valid loss: %.5f     accuracy: %.5f" % (loss, val, acc))<br/>        else:       print("training loss: %.5f     valid loss: %.5f" % (loss, val))<br/>      <br/>    return train_loss_list, val_loss_list, acc_list</span><span id="239b" class="lk ll iq lg b gy lq ln l lo lp">def val_loss(model, valid_dl, loss_func):<br/>    # Put the model into evaluation mode, not training mode<br/>    model.eval()<br/>    total = 0<br/>    sum_loss = 0<br/>    correct = 0</span><span id="a3bf" class="lk ll iq lg b gy lq ln l lo lp">    for x, y in valid_dl:<br/>        current_batch_size = y.shape[0]<br/>        logits = model(x)<br/>        loss = loss_func(logits, y)<br/>        sum_loss += current_batch_size*(loss.item())<br/>        total += current_batch_size</span><span id="a06a" class="lk ll iq lg b gy lq ln l lo lp">        # All of the code above is the same, in essence, to Training, so see the comments there</span><span id="7091" class="lk ll iq lg b gy lq ln l lo lp">        # However the functions to assess Accuracy change based on the type of problem we are doing.<br/>        # Therefore the following lines will make more sense as we progress through the article.</span><span id="d4d1" class="lk ll iq lg b gy lq ln l lo lp">        # Accuracy for Binary and Multi-Class Classification<br/>        if loss_func == F.cross_entropy:<br/>          # Find out which of the returned predictions is the loudest of them all, and that's our prediction(s)<br/>          preds = logits.sigmoid().argmax(1)<br/>          # See if our predictions are right<br/>          correct += (preds == y).float().mean().item()</span><span id="139f" class="lk ll iq lg b gy lq ln l lo lp">        # Accuracy for Multi-Label Classification<br/>        if loss_func == F.binary_cross_entropy_with_logits:<br/>          # Find out, of ALL the returned predictions, which ones are higher than our test threshold (50%), and they are our predictions<br/>          preds = logits<br/>          correct += ((preds&gt;0.5) == y.bool()).float().mean().item()</span><span id="32a6" class="lk ll iq lg b gy lq ln l lo lp">    return sum_loss/total, correct/total</span></pre><p id="db1d" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在一个小函数来查看结果。</p><pre class="lb lc ld le gt lf lg lh li aw lj bi"><span id="d703" class="lk ll iq lg b gy lm ln l lo lp">def view_results(train_loss_list, val_loss_list, acc_list):<br/>  plt.figure()<br/>  epochs = np.arange(0, len(train_loss_list))<br/>  plt.plot(epochs-0.5, train_loss_list) # offset by half an epoch as Training calculated mid epoch but val calculated at end of epoch<br/>  plt.plot(epochs, val_loss_list)<br/>  plt.title('model loss')<br/>  plt.ylabel('loss')<br/>  plt.xlabel('epoch')<br/>  plt.legend(['train', 'val', 'acc'], loc = 'upper left')<br/>  plt.show()<br/>  <br/>  if acc_list[0]:<br/>    plt.figure()<br/>    plt.plot(acc_list)<br/>    plt.title('accuracy')<br/>    plt.ylabel('accuracy')<br/>    plt.xlabel('epoch')<br/>    plt.legend(['train','val', 'acc'], loc = 'upper left')<br/>    plt.show()</span></pre><h1 id="c53e" class="lr ll iq bd ls lt lu lv lw lx ly lz ma jw mb jx mc jz md ka me kc mf kd mg mh bi translated">回归</h1><p id="b44b" class="pw-post-body-paragraph kf kg iq kh b ki mi jr kk kl mj ju kn ko mk kq kr ks ml ku kv kw mm ky kz la ij bi translated">好的，让我们从回归模型开始。具有10个特征的1000个样本，其中8个是信息性的(即，在模型/预测中实际有用的数量),让我们进行80/20训练测试分割。</p><pre class="lb lc ld le gt lf lg lh li aw lj bi"><span id="1bf0" class="lk ll iq lg b gy lm ln l lo lp">X, y = make_regression(n_samples=1000, n_features=10, n_informative=8, random_state=1972)<br/>X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=1972</span></pre><p id="0998" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在将它们加载到我们的PyTorch数据集，并在PyTorch数据加载器中将它们分成256个大小的批次。</p><pre class="lb lc ld le gt lf lg lh li aw lj bi"><span id="30ae" class="lk ll iq lg b gy lm ln l lo lp">train_ds = MyCustomDataset(X_train, y_train, scale=True)<br/>valid_ds = MyCustomDataset(X_val, y_val, scale=True)</span><span id="0f90" class="lk ll iq lg b gy lq ln l lo lp">batch_size = 256<br/>train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True)<br/>valid_dl = DataLoader(valid_ds, batch_size=batch_size, shuffle=True)</span></pre><p id="71ac" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如果我们愿意的话，我们可以看看这些，看看他们玩得好不好。</p><pre class="lb lc ld le gt lf lg lh li aw lj bi"><span id="277c" class="lk ll iq lg b gy lm ln l lo lp">train_features, train_labels = next(iter(train_dl))<br/>train_features.shape, train_labels.shape<br/>(torch.Size([256, 10]), torch.Size([256, 1]))</span><span id="cc7c" class="lk ll iq lg b gy lq ln l lo lp">train_features[0], train_labels[0]<br/>(tensor([ 0.8939, -1.0572, -2.1115,  0.9993, -0.4022, -0.7168, -0.1831,  0.3448, -0.6449, -0.4287]), tensor([0.5383]))</span></pre><p id="07c7" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">现在开始训练</strong></p><p id="da94" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">回归是我们这里最简单的问题类型。</p><p id="1a4f" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">让我们建立我们的模型。对于输出目标的数量，我们可以坚持默认值1。</p><p id="d451" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们这里的损失函数是MSE损失。(MSE代表均方误差，基本上就是我们的预测与y目标相比有多“错误”)。这是回归问题的一个很好的起点，所以让我们用它来训练。</p><p id="c213" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们无法计算回归模型的准确性，因为回归模型的性能必须报告为回归预测中的错误。</p><pre class="lb lc ld le gt lf lg lh li aw lj bi"><span id="c577" class="lk ll iq lg b gy lm ln l lo lp">model = BetterTabularModule()<br/>train_loss_list, val_loss_list, acc_list = train_loop(model, epochs=10, loss_func=F.mse_loss)<br/>view_results(train_loss_list, val_loss_list, acc_list)</span></pre><figure class="lb lc ld le gt mo gh gi paragraph-image"><div class="gh gi mn"><img src="../Images/5dbd0261b018820cb2512bccfb41f8c9.png" data-original-src="https://miro.medium.com/v2/resize:fit:762/format:webp/1*_ppBtbKjF7yI1VsFrD1Wtg.png"/></div></figure><p id="d624" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">很好，是吧？</p><h1 id="fdf6" class="lr ll iq bd ls lt lu lv lw lx ly lz ma jw mb jx mc jz md ka me kc mf kd mg mh bi translated">分类—单一类别</h1><p id="e361" class="pw-post-body-paragraph kf kg iq kh b ki mi jr kk kl mj ju kn ko mk kq kr ks ml ku kv kw mm ky kz la ij bi translated">好了，现在让我们来看一个分类模型。具有10个特征的1000个样本，其中8个是信息性的(也就是说，在模型/预测中实际有用的数量)，让我们再次进行80/20训练测试分割。</p><pre class="lb lc ld le gt lf lg lh li aw lj bi"><span id="d0c8" class="lk ll iq lg b gy lm ln l lo lp">X, y = make_classification(n_samples=1000, n_classes=2, n_features=10, n_informative=8, n_redundant=0, random_state=1972)<br/>X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=1972</span></pre><p id="b145" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在将它们加载到我们的PyTorch数据集，并在PyTorch数据加载器中将它们分成256个大小的批次。</p><pre class="lb lc ld le gt lf lg lh li aw lj bi"><span id="3c5e" class="lk ll iq lg b gy lm ln l lo lp">train_ds = MyCustomDataset(X_train, y_train)<br/>valid_ds = MyCustomDataset(X_val, y_val)</span><span id="7aba" class="lk ll iq lg b gy lq ln l lo lp">batch_size = 256<br/>train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True)<br/>valid_dl = DataLoader(valid_ds, batch_size=batch_size, shuffle=True)</span></pre><p id="8f92" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">现在训练</strong></p><p id="34a2" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">二元分类(例如，是/否)比回归稍微复杂一点，但不会太复杂。</p><p id="fdb5" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">让我们建立我们的模型。我们需要为输出目标的数量传入2。首先是答案是第一个选项的概率。第二个是它是第二个选项的概率。</p><p id="dbc0" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我知道你在想什么。为什么不只有一个输出，并根据它接近0或1的程度给出我们的答案呢？是的，我们可以这样做，但我认为这种方式更好，原因将变得清楚。</p><p id="f109" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们这里的损失函数是交叉熵损失(F.cross_entropy ),它的基本功能是返回每个答案的概率。它通过在内部将log_softmax(一条漂亮的sigmoid曲线中的所有值和整行值的总和为1)和nll_loss(根据适当的目标标签选择适当的损失)组合成一个函数来实现这一点。</p><p id="8e08" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">对于分类，我们只关心哪个返回的预测是最响亮的，这就是我们的预测。我们可以使用logits.argmax(1)来基本上说明F.cross_entropy的预测行中的哪一项是最大值。</p><p id="de02" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">然后，为了查看我们的预测是否正确，我们将所有预测与实际目标(y)进行比较，并返回正确的数字:(preds == y)。浮动()。平均值()。项目()。这是分类问题的一个很好的起点，所以让我们用损失函数进行训练，用准确度函数进行验证。</p><pre class="lb lc ld le gt lf lg lh li aw lj bi"><span id="3301" class="lk ll iq lg b gy lm ln l lo lp">model = BetterTabularModule(2)</span><span id="1696" class="lk ll iq lg b gy lq ln l lo lp">train_loss_list, val_loss_list, acc_list = train_loop(model, epochs=100, loss_func=F.cross_entropy, lr=0.001, wd=0.001)</span><span id="5e84" class="lk ll iq lg b gy lq ln l lo lp">view_results(train_loss_list, val_loss_list, acc_list)</span></pre><figure class="lb lc ld le gt mo gh gi paragraph-image"><div class="gh gi mr"><img src="../Images/6aec3d7e7f07844d5f207f85fb6070d1.png" data-original-src="https://miro.medium.com/v2/resize:fit:792/format:webp/1*cJLaJtJqjr5GBw2P3Wk6Bw.png"/></div></figure><figure class="lb lc ld le gt mo gh gi paragraph-image"><div class="gh gi ms"><img src="../Images/ffc6b24cc938c87c7c1bc0115377cb6d.png" data-original-src="https://miro.medium.com/v2/resize:fit:820/format:webp/1*ng0wxZMPKeTBlnpTZz6VEQ.png"/></div></figure><p id="01f3" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">一点都不差！！</p><h1 id="d9a2" class="lr ll iq bd ls lt lu lv lw lx ly lz ma jw mb jx mc jz md ka me kc mf kd mg mh bi translated"><strong class="ak">分类—多类</strong></h1><p id="2c2d" class="pw-post-body-paragraph kf kg iq kh b ki mi jr kk kl mj ju kn ko mk kq kr ks ml ku kv kw mm ky kz la ij bi translated">好了，现在让我们来看一个多类分类模型。具有10个特征的1000个样本，其中8个是信息性的(即，在模型/预测中实际有用的数量)，同样具有80/20训练测试分割。</p><pre class="lb lc ld le gt lf lg lh li aw lj bi"><span id="8152" class="lk ll iq lg b gy lm ln l lo lp">X, y = make_classification(n_samples=1000, n_classes=3, n_features=10, n_informative=8, n_redundant=0, random_state=1972)<br/>X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=1972)</span></pre><p id="818e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在将它们加载到我们的PyTorch数据集，并在PyTorch数据加载器中将它们分成256个大小的批次。</p><pre class="lb lc ld le gt lf lg lh li aw lj bi"><span id="ffb8" class="lk ll iq lg b gy lm ln l lo lp">train_ds = MyCustomDataset(X_train, y_train)<br/>valid_ds = MyCustomDataset(X_val, y_val)</span><span id="f650" class="lk ll iq lg b gy lq ln l lo lp">batch_size = 256<br/>train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True)<br/>valid_dl = DataLoader(valid_ds, batch_size=batch_size, shuffle=True)</span></pre><p id="da40" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">现在列车</strong></p><p id="fbba" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">多类分类(比如狗/猫/马)比二元分类难多了。</p><p id="f479" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">笑话！！！一点也不，因为我们用二元分类法做的。想想看，我们建立的模型有两个输出目标。第一个是答案是第一个选项的概率。第二个是它是第二个选项的概率。</p><p id="99fb" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">那么，我们真的需要现在就用更多的输出目标来构建模型，并让我们的损失函数和准确度与我们使用二元分类时完全相同吗？</p><p id="00b2" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">没错。！</p><p id="d933" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">告诉过你这很容易。</p><pre class="lb lc ld le gt lf lg lh li aw lj bi"><span id="df5b" class="lk ll iq lg b gy lm ln l lo lp">model = BetterTabularModule(3) # Now we want 3 outputs<br/>train_loss_list, val_loss_list, acc_list = train_loop(model, epochs=100, loss_func=F.cross_entropy, lr=0.001, wd=0.001)<br/>view_results(train_loss_list, val_loss_list, acc_list)</span></pre><figure class="lb lc ld le gt mo gh gi paragraph-image"><div class="gh gi mt"><img src="../Images/d5428e089155651aefaa15639e2c9cc2.png" data-original-src="https://miro.medium.com/v2/resize:fit:796/format:webp/1*a89K3L9o6j1jU-mpiV9hBw.png"/></div></figure><figure class="lb lc ld le gt mo gh gi paragraph-image"><div class="gh gi mu"><img src="../Images/91a34aae86436f33fd18a1108b7f079b.png" data-original-src="https://miro.medium.com/v2/resize:fit:824/format:webp/1*jAH_X1lF8kZiEHdyf0CH_Q.png"/></div></figure><h1 id="ab30" class="lr ll iq bd ls lt lu lv lw lx ly lz ma jw mb jx mc jz md ka me kc mf kd mg mh bi translated">分类—多标签</h1><pre class="lb lc ld le gt lf lg lh li aw lj bi"><span id="7dd9" class="lk ll iq lg b gy lm ln l lo lp">X, y = make_multilabel_classification(n_samples=1000, n_features=10, n_classes=3, n_labels=1, allow_unlabeled=False, random_state=1972)<br/># This returned mimicked one hot encoded data, but we need those as Floats, not Integers for our Accuracy Function<br/>y = y.astype(np.float32)<br/>X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=1972)</span></pre><p id="2c3d" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在将它们加载到我们的PyTorch数据集，并在PyTorch数据加载器中将它们分成256个大小的批次。</p><pre class="lb lc ld le gt lf lg lh li aw lj bi"><span id="36d3" class="lk ll iq lg b gy lm ln l lo lp">train_ds = MyCustomDataset(X_train, y_train)<br/>valid_ds = MyCustomDataset(X_val, y_val)</span><span id="1f02" class="lk ll iq lg b gy lq ln l lo lp">batch_size = 256<br/>train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True)<br/>valid_dl = DataLoader(valid_ds, batch_size=batch_size, shuffle=True)</span></pre><p id="03ab" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">现在列车</strong></p><p id="bd55" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们这里的损失函数是f . binary _ cross _ entropy _ with _ logits loss，它只是在BCE损失之前做一个sigmoid，因此最终因为我们可以有一个以上最响亮的激活(因为它可以是多标签)，所以我们希望让它们通过sigmoid，并基于阈值来识别它们是否正确。即:正确+= ((preds&gt;0.5) == y.bool())。浮动()。平均值()。项目()</p><pre class="lb lc ld le gt lf lg lh li aw lj bi"><span id="44b7" class="lk ll iq lg b gy lm ln l lo lp">model = BetterTabularModule(3)<br/>train_loss_list, val_loss_list, acc_list = train_loop(model, epochs=50, loss_func=F.binary_cross_entropy_with_logits, lr=0.001, wd=0.001)<br/>view_results(train_loss_list, val_loss_list, acc_list)</span></pre><figure class="lb lc ld le gt mo gh gi paragraph-image"><div class="gh gi mv"><img src="../Images/9c621eaf918b3bb65b6d5a299edf1e12.png" data-original-src="https://miro.medium.com/v2/resize:fit:770/format:webp/1*o6VCPcwqQ9vT3Jo08ZRrow.png"/></div></figure><figure class="lb lc ld le gt mo gh gi paragraph-image"><div class="gh gi mw"><img src="../Images/fb2946726e4b19ef4729a5edfaee23ca.png" data-original-src="https://miro.medium.com/v2/resize:fit:848/format:webp/1*TUj9Au9AKqS_sh5iYSBWDQ.png"/></div></figure><p id="20f0" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">同样，我认为这在样本数据上训练和验证得非常好。</p><h1 id="ed63" class="lr ll iq bd ls lt lu lv lw lx ly lz ma jw mb jx mc jz md ka me kc mf kd mg mh bi translated">一路上的教训</h1><p id="cc38" class="pw-post-body-paragraph kf kg iq kh b ki mi jr kk kl mj ju kn ko mk kq kr ks ml ku kv kw mm ky kz la ij bi translated">在把这些放在一起的过程中，我遇到了各种各样的错误和问题，下面我整理了一些。其原因是，当你(希望)利用这一点创建自己的模型，改变损失函数或精度函数，甚至当然只是使用自己的数据时，你可能会遇到相同或类似的问题。</p><p id="032f" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">目标2出界。</strong></p><ul class=""><li id="6028" class="mx my iq kh b ki kj kl km ko mz ks na kw nb la nc nd ne nf bi translated">这是因为当我创建我的模型时，我没有改变一个输出目标的默认值。所以一旦a尝试了一个分类模型，我就得到错误<br/>通过将2传递给函数来建立我的模型，我正在纠正我的错误，并确保我的模型有两个可能的结果目标</li></ul><p id="aadb" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">我的训练看起来很平静，没有什么事情发生</strong></p><ul class=""><li id="80fe" class="mx my iq kh b ki kj kl km ko mz ks na kw nb la nc nd ne nf bi translated">我错误地没有重置我的模型，所以这个训练是在我已经做过的训练之上的，所以看起来根本没有什么不同</li></ul><p id="c320" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir"> RuntimeError:张量a (256)的大小必须与非单一维度1的张量b (3)的大小相匹配</strong></p><ul class=""><li id="d9fd" class="mx my iq kh b ki kj kl km ko mz ks na kw nb la nc nd ne nf bi translated">在多标签分类中，我使用argmaxing而不是sigmoid，因此，对于所有目标，我没有获得介于0和1之间的良好概率，而是像我对标准分类所做的那样，再次选择最响亮的目标</li></ul><h1 id="86ab" class="lr ll iq bd ls lt lu lv lw lx ly lz ma jw mb jx mc jz md ka me kc mf kd mg mh bi translated">结论</h1><p id="8ad6" class="pw-post-body-paragraph kf kg iq kh b ki mi jr kk kl mj ju kn ko mk kq kr ks ml ku kv kw mm ky kz la ij bi translated">现在你知道了。一个简单的PyTorch用例的不同问题类型及其方法的例子，您可以根据您的问题需求对其进行扩展。</p><p id="147e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我希望这有所帮助。</p></div></div>    
</body>
</html>