<html>
<head>
<title>Why Bagging Works</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">为什么装袋有效</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/why-bagging-works-b9961354ee73?source=collection_archive---------20-----------------------#2021-07-10">https://towardsdatascience.com/why-bagging-works-b9961354ee73?source=collection_archive---------20-----------------------#2021-07-10</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><figure class="gl gn jr js jt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi jq"><img src="../Images/8df4b4d340b0c81155c9ae1152d9ec87.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*FJZQO0TRwXViKhxQ"/></div></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">罗斯·斯奈登在<a class="ae kf" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="d02d" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在这篇文章中，我深入探讨了打包<em class="le">或</em>自举聚合。重点是建立对底层机制的直觉，以便你更好地理解为什么这项技术如此强大。Bagging通常与随机森林模型相关联，但其基本思想更为普遍，可以应用于任何模型。</p><p id="edf9" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">bagging——就像boosting一样——与学习者大家庭坐在一起。装袋包括三个关键要素:</p><ol class=""><li id="32fd" class="lf lg it ki b kj kk kn ko kr lh kv li kz lj ld lk ll lm ln bi translated">使学习者适应数据的自举样本</li><li id="169e" class="lf lg it ki b kj lo kn lp kr lq kv lr kz ls ld lk ll lm ln bi translated">对许多学习者和引导样本这样做；</li><li id="f64d" class="lf lg it ki b kj lo kn lp kr lq kv lr kz ls ld lk ll lm ln bi translated">从所有学习者中提取预测，并将平均值/模式值作为最终预测。</li></ol><p id="49c8" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">当每个学习者在输入特征的不同子集上被训练时，这种方法的有效性被进一步增强(尽管对于随机森林来说，这是稍微更细微的)。<strong class="ki iu"> <em class="le">但为什么这种做法在实践中如此有效？</em>T9】</strong></p><h2 id="44fc" class="lt lu it bd lv lw lx dn ly lz ma dp mb kr mc md me kv mf mg mh kz mi mj mk ml bi translated">什么是自举？</h2><p id="cc48" class="pw-post-body-paragraph kg kh it ki b kj mm kl km kn mn kp kq kr mo kt ku kv mp kx ky kz mq lb lc ld im bi translated">Bootstrapping是从数据集中抽取替换的随机样本并计算每个样本的统计数据(例如，平均值、中值)的过程。当使用替换数据点执行采样时，原始数据中的数据点将被复制到引导数据集中。另一种方法是<strong class="ki iu">粘贴</strong>，其中数据被采样而没有替换。基本思想是通过随机生成数据<em class="le">可能是<em class="le"> </em>的替代视图来解决采样误差/偏差。<strong class="ki iu">该过程将生成感兴趣的统计结果的分布，在此基础上您可以估计标准误差和置信区间</strong>。一般来说，引导映射过程创建的基础统计数据的估计偏差较小。</em></p><h2 id="fc04" class="lt lu it bd lv lw lx dn ly lz ma dp mb kr mc md me kv mf mg mh kz mi mj mk ml bi translated"><strong class="ak">学习拼图的随机部分</strong></h2><p id="c1d7" class="pw-post-body-paragraph kg kh it ki b kj mm kl km kn mn kp kq kr mo kt ku kv mp kx ky kz mq lb lc ld im bi translated">在预测建模的背景下，这种想法尤其强大。机器学习的一个基本挑战是过度拟合。当模型记住了它被训练的数据时，就会发生过度拟合。然而，如果一个单一的模型从未被展示完整的数据集，它的记忆能力就会受到极大的限制。从统计角度来看，自举是实现这一目标的便捷方式。当我们在机器学习中使用bootstrapping时，我们避免向任何单个模型显示完整的数据集。</p><p id="cf96" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">为每个模型(或随机森林中的每个分裂)添加随机特征选择进一步防止过度拟合。实际上，bagging方法的<strong class="ki iu">威力在于多个模型可以学习数据集</strong>不同组成部分之间的关系——样本和特征的不同组合——而不会过度拟合。</p><p id="2327" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这意味着bagging模型中的每个子模型将学习底层数据的不同组成部分。如果每个子模型学习相同的结构或相同的参数估计，则在该过程中没有附加值。<strong class="ki iu">因此，当每个子模型不相关时，Bagging是最有效的。</strong></p><h2 id="6026" class="lt lu it bd lv lw lx dn ly lz ma dp mb kr mc md me kv mf mg mh kz mi mj mk ml bi translated">提取预测区间</h2><p id="9424" class="pw-post-body-paragraph kg kh it ki b kj mm kl km kn mn kp kq kr mo kt ku kv mp kx ky kz mq lb lc ld im bi translated">bagging模型的另一个好处是你可以提取预测区间。因为每个模型都适合数据的随机子集(以及潜在的随机特征空间)，所以每个学习者都会学到输入和输出之间不同的关系集。这将为数据中的每个样本生成预测分布。</p><p id="2df5" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">下面的代码将随机森林拟合到房价数据上，从每个子模型中提取预测，计算95%的预测区间，并绘制每个样本的预测区间图:</p><figure class="mr ms mt mu gt ju"><div class="bz fp l di"><div class="mv mw l"/></div></figure><figure class="mr ms mt mu gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi mx"><img src="../Images/cd6290212538b3feac61d30d3afa987f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BZEsZoi3HdO5crpJ3hI8qg.png"/></div></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">图片作者。</p></figure><p id="3f98" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">上图显示了数据中所有样本的平均预测和95%预测区间。<strong class="ki iu">当您需要识别预测高度不确定的样本时，这非常有用。当你想在预测中加入缓冲时，它也很有用。在这种情况下，您可以对所有样本进行P95预测。</strong></p></div><div class="ab cl my mz hx na" role="separator"><span class="nb bw bk nc nd ne"/><span class="nb bw bk nc nd ne"/><span class="nb bw bk nc nd"/></div><div class="im in io ip iq"><p id="e01c" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我希望这篇短文有助于发展统计学和机器学习中bagging方法的直觉。如果你喜欢这个，你可能也会喜欢我在<strong class="ki iu">“为什么助推有效”上的这篇文章。</strong></p><div class="nf ng gp gr nh ni"><a rel="noopener follow" target="_blank" href="/gradient-boosting-is-one-of-the-most-effective-ml-techniques-out-there-af6bfd0df342"><div class="nj ab fo"><div class="nk ab nl cl cj nm"><h2 class="bd iu gy z fp nn fr fs no fu fw is bi translated">为什么助推有效</h2><div class="np l"><h3 class="bd b gy z fp nn fr fs no fu fw dk translated">梯度推进是最有效的ML技术之一。</h3></div><div class="nq l"><p class="bd b dl z fp nn fr fs no fu fw dk translated">towardsdatascience.com</p></div></div><div class="nr l"><div class="ns l nt nu nv nr nw jz ni"/></div></div></a></div><p id="7aae" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">感谢阅读！</p></div></div>    
</body>
</html>