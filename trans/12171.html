<html>
<head>
<title>Implement Value Iteration in Python — A Minimal Working Example</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用Python实现值迭代——一个最小的工作示例</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/implement-value-iteration-in-python-a-minimal-working-example-f638907f3437?source=collection_archive---------1-----------------------#2021-12-09">https://towardsdatascience.com/implement-value-iteration-in-python-a-minimal-working-example-f638907f3437?source=collection_archive---------1-----------------------#2021-12-09</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="6461" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">掌握简单和经典的动态规划算法，寻找马尔可夫决策过程模型的最优解</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/558dbeda81df08f6b4b8a9ae18806b5b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*60g4MjOyf2ANM7bT"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><a class="ae ky" href="https://unsplash.com/@sharonmccutcheon?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">莎伦·麦卡琴</a>在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure></div><div class="ab cl kz la hx lb" role="separator"><span class="lc bw bk ld le lf"/><span class="lc bw bk ld le lf"/><span class="lc bw bk ld le"/></div><div class="im in io ip iq"><p id="a0c1" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">在人工智能时代，精确算法并不完全是<em class="mc">热</em>。如果机器不能自己学习，那还有什么意义呢？为什么要费事去解决<strong class="li iu">马尔可夫决策过程</strong>模型，而这些模型的解决方案无论如何都无法扩展呢？为什么不直接研究强化学习算法呢？</p><p id="ad7a" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">学习经典的<strong class="li iu">动态编程</strong>技术仍然是值得的。首先，它们在实践中仍然被广泛使用。许多软件开发工作将动态编程作为面试过程的一部分。尽管您可以列举的状态和动作只有这么多，但您可能会惊讶于现实世界中的问题仍然可以通过优化来解决。</p><p id="7c78" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">第二，即使只对强化收入感兴趣，该领域的许多算法都牢牢植根于动态编程。<a class="ae ky" rel="noopener" target="_blank" href="/the-four-policy-classes-of-reinforcement-learning-38185daa6c8a">在强化学习中可以区分四个策略类别</a>，其中之一是<strong class="li iu">价值函数近似</strong>。在使用这些方法之前，理解经典的值迭代算法是非常重要的。幸运的是，这篇文章恰好概述了这一点。</p><div class="md me gp gr mf mg"><a rel="noopener follow" target="_blank" href="/the-four-policy-classes-of-reinforcement-learning-38185daa6c8a"><div class="mh ab fo"><div class="mi ab mj cl cj mk"><h2 class="bd iu gy z fp ml fr fs mm fu fw is bi translated">强化学习的四个策略类别</h2><div class="mn l"><p class="bd b dl z fp ml fr fs mm fu fw dk translated">towardsdatascience.com</p></div></div><div class="mo l"><div class="mp l mq mr ms mo mt ks mg"/></div></div></a></div><h1 id="10be" class="mu mv it bd mw mx my mz na nb nc nd ne jz nf ka ng kc nh kd ni kf nj kg nk nl bi translated">价值迭代</h1><p id="d823" class="pw-post-body-paragraph lg lh it li b lj nm ju ll lm nn jx lo lp no lr ls lt np lv lw lx nq lz ma mb im bi translated">价值迭代算法的优雅是值得钦佩的。它只需要几行数学表达式，而不是更多的代码行。让我们看看萨顿&amp;巴尔托的开创性外延:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nr"><img src="../Images/bd4df8252baf69466915155af946e62e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Rg3xj63Uwbw7_pBgEqEJdA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">价值迭代算法[来源:萨顿&amp;巴尔托(公开资料)，2019]</p></figure><p id="9750" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">直觉相当简单。首先，你<strong class="li iu">为每个状态初始化</strong>一个值，例如0。</p><p id="b64b" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">然后，对于每个状态，你<strong class="li iu">通过将每个动作的奖励<code class="fe ns nt nu nv b">a</code>(直接奖励<code class="fe ns nt nu nv b">r</code> +下游值<code class="fe ns nt nu nv b">V(s’)</code>)乘以转移概率<code class="fe ns nt nu nv b">p</code>来计算</strong> <strong class="li iu">值</strong> <code class="fe ns nt nu nv b">V(s)</code>。</p><p id="715d" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">假设我们确实初始化为0，并且直接奖励<code class="fe ns nt nu nv b">r</code>是非零的。差异将直接在<strong class="li iu">差异</strong>表达式<code class="fe ns nt nu nv b">|v-V(s)|</code>中可见，其中<code class="fe ns nt nu nv b">v</code>是旧的估计值，<code class="fe ns nt nu nv b">V(s)</code>是新的估计值。因此，误差<code class="fe ns nt nu nv b">Δ</code>将超过阈值<code class="fe ns nt nu nv b">θ</code>(一个小值)，新的迭代随之而来。</p><p id="c1a8" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">通过执行足够多的迭代，算法将<strong class="li iu">收敛</strong>到一点，在该点<code class="fe ns nt nu nv b">|v-V(s)|&lt;θ</code>对于每个状态。然后，您可以解析<code class="fe ns nt nu nv b">argmax</code>来找到每个状态的最佳操作；知道真正的价值函数<code class="fe ns nt nu nv b">V(s)</code>等同于拥有最优策略<code class="fe ns nt nu nv b">π</code>。</p><p id="eed1" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">请注意，如果<code class="fe ns nt nu nv b">θ</code>设置过大，则不能保证<strong class="li iu">最优</strong>。出于实际目的，合理的小误差容限就可以了，但是对于我们当中的数学爱好者来说，记住最优性条件是有好处的。</p><h1 id="8142" class="mu mv it bd mw mx my mz na nb nc nd ne jz nf ka ng kc nh kd ni kf nj kg nk nl bi translated">一个最小的工作示例</h1><p id="07a7" class="pw-post-body-paragraph lg lh it li b lj nm ju ll lm nn jx lo lp no lr ls lt np lv lw lx nq lz ma mb im bi translated">数学已经过时了，让我们继续编码的例子。把重点放在算法上，问题极其简单。</p><h2 id="4282" class="nw mv it bd mw nx ny dn na nz oa dp ne lp ob oc ng lt od oe ni lx of og nk oh bi translated">问题是</h2><p id="55c0" class="pw-post-body-paragraph lg lh it li b lj nm ju ll lm nn jx lo lp no lr ls lt np lv lw lx nq lz ma mb im bi translated">考虑一个一维世界(一排瓷砖)，只有一个终止状态。进入终结状态获得+10奖励，其他动作花费-1。代理可以向左或向右移动，但是——为了不使它变得太微不足道——代理在10%的时间里向错误的方向移动。这是一个非常简单的问题，但是它有(I)直接回报，(ii)预期下游回报，和(iii)转移概率。</p><h2 id="fdde" class="nw mv it bd mw nx ny dn na nz oa dp ne lp ob oc ng lt od oe ni lx of og nk oh bi translated">该算法</h2><p id="b590" class="pw-post-body-paragraph lg lh it li b lj nm ju ll lm nn jx lo lp no lr ls lt np lv lw lx nq lz ma mb im bi translated">Python算法非常接近萨顿&amp;巴尔托提供的数学大纲，所以不需要扩展太多。完整的代码符合一个要点:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oi oj l"/></div></figure><h2 id="5631" class="nw mv it bd mw nx ny dn na nz oa dp ne lp ob oc ng lt od oe ni lx of og nk oh bi translated">一些实验</h2><p id="fd01" class="pw-post-body-paragraph lg lh it li b lj nm ju ll lm nn jx lo lp no lr ls lt np lv lw lx nq lz ma mb im bi translated">好吧，那就做些实验。我们首先详细展示了该算法的两次迭代。</p><p id="8256" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">首先，我们设置<code class="fe ns nt nu nv b">v</code>等于<code class="fe ns nt nu nv b">V(0)</code>，等于0: <code class="fe ns nt nu nv b">v=V(0)=0</code></p><p id="1d1c" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">接下来，我们更新<code class="fe ns nt nu nv b">V(0)</code>。注意<code class="fe ns nt nu nv b">r</code>对于每个状态是固定的；我们实际上只通过<code class="fe ns nt nu nv b">s’</code>对下一组状态求和。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ok"><img src="../Images/5bc4a89ad17b447e7fae20075e25bdff.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4No0QEdh1J0v06FqKqz-pA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">值迭代步骤1，状态0[作者图片]</p></figure><p id="2a21" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">对于这样一个小问题来说，这似乎是很大的计算工作量。事实上，很容易理解为什么动态编程不能很好地扩展。在这种情况下，所有值<code class="fe ns nt nu nv b">V(s)</code>仍然为0——正如我们刚刚开始的那样——所以估计的状态值<code class="fe ns nt nu nv b">V(0)</code>就是直接奖励-1。</p><p id="89ef" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">让我们再尝试一个，更进一步。同样的计算，但是现在值<code class="fe ns nt nu nv b">V(s)</code>已经在几次迭代中更新了。我们现在有<code class="fe ns nt nu nv b">V=[5.17859, 7.52759, 10.0, 7.52759, 5.17859]</code>。同样，我们插入这些值:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ol"><img src="../Images/3e3a64acb29e8d326e3058db4ff17815.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JYX5aS6CA6sOVXlFBY3-cg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">值迭代步骤4，状态0[作者图片]</p></figure><p id="6fac" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">因此，我们将<code class="fe ns nt nu nv b">V(0)</code>从5.18更新到5.56。误差将是<code class="fe ns nt nu nv b">Δ=(5.56–5.18)=0.38</code>。反过来，这将影响其他状态的更新，并且对于所有状态，该过程持续到<code class="fe ns nt nu nv b">Δ&lt;θ</code>。对于状态0，最佳值是5.68，在10次迭代内命中。</p><p id="9f9e" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">这里要测试的最有趣的参数是误差容限<code class="fe ns nt nu nv b">θ</code>，它影响收敛前的迭代次数。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi om"><img src="../Images/f01eaf9c2537f9ff634ce2c26dbb10e6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*B-TDfuEGTNMP3S5xpm3J7A.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">各种误差容差θ所需的迭代次数。容差越低，算法收敛前需要的迭代次数就越多。</p></figure><h1 id="240b" class="mu mv it bd mw mx my mz na nb nc nd ne jz nf ka ng kc nh kd ni kf nj kg nk nl bi translated">最后的话</h1><p id="b4ee" class="pw-post-body-paragraph lg lh it li b lj nm ju ll lm nn jx lo lp no lr ls lt np lv lw lx nq lz ma mb im bi translated">值迭代是强化学习的基石之一。很容易实现和理解。在转向更高级的实现之前，请确保掌握这个基本算法。</p></div><div class="ab cl kz la hx lb" role="separator"><span class="lc bw bk ld le lf"/><span class="lc bw bk ld le lf"/><span class="lc bw bk ld le"/></div><div class="im in io ip iq"><p id="dcdf" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated"><em class="mc">您可能感兴趣的一些其他最小工作示例:</em></p><div class="md me gp gr mf mg"><a rel="noopener follow" target="_blank" href="/implement-policy-iteration-in-python-a-minimal-working-example-6bf6cc156ca9"><div class="mh ab fo"><div class="mi ab mj cl cj mk"><h2 class="bd iu gy z fp ml fr fs mm fu fw is bi translated">用Python实现策略迭代——一个最小的工作示例</h2><div class="on l"><h3 class="bd b gy z fp ml fr fs mm fu fw dk translated">了解这个经典的动态规划算法，以优化解决马尔可夫决策过程模型</h3></div><div class="mn l"><p class="bd b dl z fp ml fr fs mm fu fw dk translated">towardsdatascience.com</p></div></div><div class="mo l"><div class="oo l mq mr ms mo mt ks mg"/></div></div></a></div><div class="md me gp gr mf mg"><a rel="noopener follow" target="_blank" href="/a-minimal-working-example-for-deep-q-learning-in-tensorflow-2-0-e0ca8a944d5e"><div class="mh ab fo"><div class="mi ab mj cl cj mk"><h2 class="bd iu gy z fp ml fr fs mm fu fw is bi translated">TensorFlow 2.0中深度Q学习的最小工作示例</h2><div class="on l"><h3 class="bd b gy z fp ml fr fs mm fu fw dk translated">一个多臂土匪的例子来训练一个Q网络。使用TensorFlow，更新过程只需要几行代码</h3></div><div class="mn l"><p class="bd b dl z fp ml fr fs mm fu fw dk translated">towardsdatascience.com</p></div></div><div class="mo l"><div class="op l mq mr ms mo mt ks mg"/></div></div></a></div><div class="md me gp gr mf mg"><a rel="noopener follow" target="_blank" href="/a-minimal-working-example-for-continuous-policy-gradients-in-tensorflow-2-0-d3413ec38c6b"><div class="mh ab fo"><div class="mi ab mj cl cj mk"><h2 class="bd iu gy z fp ml fr fs mm fu fw is bi translated">TensorFlow 2.0中连续策略梯度的最小工作示例</h2><div class="on l"><h3 class="bd b gy z fp ml fr fs mm fu fw dk translated">一个简单的训练高斯演员网络的例子。定义自定义损失函数并应用梯度胶带…</h3></div><div class="mn l"><p class="bd b dl z fp ml fr fs mm fu fw dk translated">towardsdatascience.com</p></div></div><div class="mo l"><div class="oq l mq mr ms mo mt ks mg"/></div></div></a></div><div class="md me gp gr mf mg"><a rel="noopener follow" target="_blank" href="/a-minimal-working-example-for-discrete-policy-gradients-in-tensorflow-2-0-d6a0d6b1a6d7"><div class="mh ab fo"><div class="mi ab mj cl cj mk"><h2 class="bd iu gy z fp ml fr fs mm fu fw is bi translated">TensorFlow 2.0中离散策略梯度的最小工作示例</h2><div class="on l"><h3 class="bd b gy z fp ml fr fs mm fu fw dk translated">一个训练离散演员网络的多兵种土匪例子。在梯度胶带功能的帮助下…</h3></div><div class="mn l"><p class="bd b dl z fp ml fr fs mm fu fw dk translated">towardsdatascience.com</p></div></div><div class="mo l"><div class="or l mq mr ms mo mt ks mg"/></div></div></a></div><h1 id="c7cb" class="mu mv it bd mw mx my mz na nb nc nd ne jz nf ka ng kc nh kd ni kf nj kg nk nl bi translated">参考</h1><p id="276a" class="pw-post-body-paragraph lg lh it li b lj nm ju ll lm nn jx lo lp no lr ls lt np lv lw lx nq lz ma mb im bi translated">萨顿，理查德s和安德鲁g巴尔托。<em class="mc">强化学习:简介</em>。麻省理工学院出版社，2018。</p></div></div>    
</body>
</html>