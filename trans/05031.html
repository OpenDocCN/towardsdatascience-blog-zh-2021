<html>
<head>
<title>A Primer to Bayesian Additive Regression Tree with R</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">带R的贝叶斯可加回归树入门</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/a-primer-to-bayesian-additive-regression-tree-with-r-b9d0dbf704d?source=collection_archive---------8-----------------------#2021-05-03">https://towardsdatascience.com/a-primer-to-bayesian-additive-regression-tree-with-r-b9d0dbf704d?source=collection_archive---------8-----------------------#2021-05-03</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="4393" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">传统集成算法的贝叶斯方法</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/62f93dba61e16cbaba26300b80814d73.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xeFw6dY8cDgZWQRq_FTwVg.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">由<a class="ae ky" href="https://unsplash.com/@kazuend?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> kazuend </a>在<a class="ae ky" href="https://unsplash.com/s/photos/forest?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>拍摄的照片</p></figure><h1 id="fe86" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated"><strong class="ak">简介</strong></h1><p id="5b19" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">在XGBoost、Lightgbm和catboost在几次高调的kaggle竞赛和机器学习研究中取得巨大成功之后，梯度增强机器(GBM)/集成算法成为许多涉及结构化数据的机器学习问题的主力。GBM如此成功的原因之一是它可以同样好地处理数值和分类数据。然而，这些传统的基于集成的方法的问题是，它们给我们一个点估计，因此，我们没有办法估计该特定估计有多可信。通过量化经验风险，即rmse、对数损失等。，我们可以得到一种模型不确定性的感觉(认知不确定性)；然而，对于迭代主动学习或更好的风险管理(对于高风险情况)，随机不确定性或个体预测不确定性也是非常重要的。为了缓解这个问题，Chipman等人发表了这篇<a class="ae ky" href="http://www-stat.wharton.upenn.edu/~edgeorge/Research_papers/BART%20June%2008.pdf" rel="noopener ugc nofollow" target="_blank">论文</a>，这是由系综方法的相同精神所激发的；然而，它需要贝叶斯方法来识别模型参数，其中我们可以对我们关于树的形状和总体结构的先验信念进行编码。然后，基于该数据，它将使用MCMC反向拟合算法更新先验。在他们的文章中，他们声称在42个数据集(包括模拟和药物发现数据)上，它优于其他传统的集成方法。在这篇文章中，我的目的是用波士顿住房数据集上的一个说明性例子来证明这种说法。</p><h1 id="ac7e" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">树木的集合</h1><p id="9b1f" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">系综树的想法来自于这样一种想法，即通过组合比随机猜测稍好的弱学习者的集合，我们可以得到一个强学习者。这些个体学习者中每一个都是低偏差-高方差模型。当组合在一起形成一个集合时，这些树形成一个中等偏差-中等方差模型，该模型在理论和实践上都具有极好的样本外预测性能。</p><p id="2584" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">做出推论的一般问题可以写成:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ms"><img src="../Images/2bcfc871d417a54cf11c54931d8316f1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yeYYWk83eFVy296IB8a8_g.png"/></div></div></figure><p id="4cce" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">在集成算法中，未知函数被近似为树的和h(x)。树和模型是一个以多元成分为核心的加性模型，与广义加性模型相反，它可以自然地包括交互作用效应。此外，它还可以包括单棵树无法获得的附加效果。由于这些优异的性质，系综方法吸引了大量的注意力，并导致了boosting [ <a class="ae ky" href="https://www.jstor.org/stable/2699986" rel="noopener ugc nofollow" target="_blank"> Friedman，2001 </a> ]、random forest [ <a class="ae ky" href="https://link.springer.com/article/10.1023/A:1010933404324" rel="noopener ugc nofollow" target="_blank"> Breiman，2001 </a> ]和bagging [ <a class="ae ky" href="https://link.springer.com/article/10.1007/BF00058655" rel="noopener ugc nofollow" target="_blank"> Breiman，1996 </a> ]的发展。随机森林使用特征子集来构建每棵树，然后将所有这些单棵树的预测进行组合。相比之下，bagging使用数据的子集来构建每棵树。Boosting与其他两种不同之处在于，它按顺序形成模型，其中每棵树都试图适应前一棵树无法解释的数据变化。贝叶斯加性回归树(BART)类似于Boosting算法，因为它结合了顺序弱学习者的贡献来进行预测。</p><h1 id="137f" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">贝叶斯加性回归树</h1><p id="281f" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">在BART中，类似于梯度提升的反向拟合算法用于获得树的集合，其中一棵小树与数据拟合，然后该树的残差与另一棵树迭代拟合。然而，BART在两个方面不同于GBM，1 .它如何通过使用先验来削弱单个树，以及2 .它如何在固定数量的树上使用贝叶斯反向拟合来迭代拟合新树。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mt"><img src="../Images/b8488479c6d049dfcb160f09c22528e6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ASldtWH1iPAjdWTg3tLQvA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">拟合单棵树(信用:<a class="ae ky" href="https://www.youtube.com/watch?v=9d5-3_7u5a4&amp;ab_channel=PutnamDataSciences" rel="noopener ugc nofollow" target="_blank">https://www.youtube.com/watch?v=9d5-3_7u5a4&amp;ab _ channel = PutnamDataSciences</a></p></figure><p id="5e49" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">在GBM中，存在巨大的过度拟合风险，因为我们可以不断迭代，直到我们的经验风险在训练集示例上最小化(此时，所有这些树中存在的比特如此之大，以至于它本质上记住了所有的训练点，而不是概括)。在传统的GBM中，限制树深或提前停止等。来规范算法。相比之下，BART使用智能先验系统地学习树木的收缩量或深度。先验是这样选择的，1。树很小，2。与每个树终端节点相关联的参数值趋向于零(限制灵活性)，以及3。残留噪声的标准差较小。有趣的是，树的数量没有被控制在先验范围内，因为这将导致优化过程中巨大的计算成本。相反，它被设置为默认值50，这对于大多数应用程序来说已经很不错了。在获得上述三个参数的后验分布后，通过从模型空间(三个参数的后验)连续抽取样本，用反拟合算法进行推断。通过取所有这些图的平均值，我们可以得到任何试验数据的点估计。类似地，相应的分位数给出了测试数据的不确定性区间。</p><p id="ce18" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">现在是使用BART建立预测模型的时候了。对于这部分，我们使用r中可用的<a class="ae ky" href="https://cran.r-project.org/web/packages/bartMachine/bartMachine.pdf" rel="noopener ugc nofollow" target="_blank"> bartmachine </a>包，程序的核心是用java编写的。BART的原作者的本机实现(<a class="ae ky" href="https://cran.r-project.org/web/packages/BayesTree/BayesTree.pdf" rel="noopener ugc nofollow" target="_blank"> bayestree </a>)是用C++编写的，但是它有几个限制(如下所列)。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mu"><img src="../Images/a0d00ac8b4dff8c15a60b900acb4e244.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UqbrMHtGk_MUFZXldAQmAA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">信用:【https://arxiv.org/pdf/1312.2171.pdf】T4</p></figure><p id="3b9b" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">在本练习中，我们将使用波士顿住房数据，这些数据在R中不可用，但我们可以通过mlbench软件包轻松获得。在下面的代码中，我们将数据加载到R:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mv mw l"/></div></figure><p id="be22" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">接下来，我们使用caret包将数据分为训练集和测试集:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mv mw l"/></div></figure><p id="804f" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">接下来，我们绘制目标变量的直方图，它是一组特征的房价中值:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mv mw l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mx"><img src="../Images/f033eb2e3aa2b1ba214af9feab081dc4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*y_Ia1XvVN5Al3P6WLk9wwA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">中间价直方图</p></figure><p id="04e5" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">要查看数据中每个要素之间的相关性，我们可以查看相关图:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mx"><img src="../Images/d4fa701fddcc9c2d11923513f354939e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*u1igJJ-iY8YZs_m5oYKZfA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">相关图</p></figure><p id="a72c" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">看起来在一些特征之间有一些高相关性，这对于其他ml算法可能是一个问题，但是对于基于树的集成，我们可以安全地忽略它们。现在，我们可以初始化bartmachine:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mv mw l"/></div></figure><p id="f884" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">对于大约400个数据点和13个特征来说，它非常快(只需要大约2-3秒)。训练数据的均方根误差为1.4，考虑到目标值范围在4-50之间，这已经相当不错了。夏皮罗-维尔克检验的p值表明数据是异方差的。</p><p id="1487" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">使用rmse_by_num_trees函数，我们可以很容易地找到最佳的树数，而不是使用默认的50。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mv mw l"/></div></figure><p id="cc18" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">基于rmse对树数量的图，我们用65棵树重新训练该模型，并绘制收敛诊断图。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mv mw l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi my"><img src="../Images/591bc49d66adbb215cc04195b8dafda0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EsoZ-tt_8a6IzVbrqybyGQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">会聚诊断图</p></figure><p id="69de" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">我们可以看到，MCMC采样在大约200次迭代时达到收敛。到2010年，它仍在波动。跑更长的时间和更多的磨合是有益的。然而，对于这个练习，这种收敛行为可以被认为是可接受的。</p><p id="dae2" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">接下来，我们可以使用内置函数查看剩余属性:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mv mw l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi my"><img src="../Images/a2dbb2af204104545be154f22f1fc305.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*J8yYAP-1OQzaO48etamQmg.png"/></div></div></figure><p id="02ae" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">残差或多或少在零附近波动，但总体上残差看起来呈正态分布。现在，我们通过绘制预测中值价格与实际中值价格的平价图来说明该算法在训练集和测试集上的性能。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mv mw l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi my"><img src="../Images/639a3b23d550f2211b5e0e5f558a0570.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*M_raPkYcvc-tb9QStMk6Xw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">列车数据的奇偶图</p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi my"><img src="../Images/6e1bcc729e96b2bc7f00fc67ac320b23.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NKLhtOzmAv0px5iRFxn5-w.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">测试数据的奇偶图</p></figure><p id="cef4" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">这令人印象深刻，因为我们现在能够给出每个数据点的(95%预测区间)预测范围，其准确度约为90%。我们还可以在测试集上计算rmse和其他指标。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mv mw l"/></div></figure><p id="0adb" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">测试集上的rmse为3.52，r平方值为0.89，pearson r为0.94。现在，让我们检查一下作者们的说法是真是假。我们用python对波士顿住房数据运行了XGBoost算法:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mv mw l"/></div></figure><p id="d153" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">这两种实现之间只有微小的差别。通过一些超参数优化，结果可能会改变(我也对训练和测试集使用随机80–20分割)。总之，bartmachine的性能令人印象深刻，因为它也可以给我们一些内在的估计不确定性。</p><p id="59c5" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">我们还可以使用bartmachine中可用的函数来绘制特性重要性:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mv mw l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi my"><img src="../Images/be440382d3abbebc43b5117038e7982f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pVPzhf7jJD6jpcWZOm7xww.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">特征重要性图</p></figure><h1 id="b66e" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">摘要</h1><ul class=""><li id="d0c6" class="mz na it lt b lu lv lx ly ma nb me nc mi nd mm ne nf ng nh bi translated">BART是一种非常复杂的算法，能够提供与用于构建模型的参数相关的内置不确定性估计。</li><li id="9525" class="mz na it lt b lu ni lx nj ma nk me nl mi nm mm ne nf ng nh bi translated">对于各种各样的现实世界机器学习问题，BART是一个很好的选择。</li><li id="7046" class="mz na it lt b lu ni lx nj ma nk me nl mi nm mm ne nf ng nh bi translated">它可以用于设计空间的迭代探索(贝叶斯主动学习),以最小化收集昂贵数据的成本。</li></ul></div></div>    
</body>
</html>