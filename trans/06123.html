<html>
<head>
<title>A deep dive into k-means</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">对k-means的深入探究</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/a-deep-dive-into-k-means-f9a1ef2490f8?source=collection_archive---------24-----------------------#2021-06-01">https://towardsdatascience.com/a-deep-dive-into-k-means-f9a1ef2490f8?source=collection_archive---------24-----------------------#2021-06-01</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="ddc3" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">关于最流行的聚类算法，您只需要知道</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/443173385fba1798bb83c1f2245801fe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*9t9dqK8ZyEXUgEr0"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><a class="ae kv" href="https://unsplash.com/@evieshaffer?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Evie S. </a>在<a class="ae kv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><h1 id="2835" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">介绍</h1><p id="13dc" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">聚类问题在数据科学中非常常见。潜在的问题总是在你的数据中找到相似的观察组。根据您的领域，这可能是具有相似偏好的客户、在您的生物检测中表现相似的股票或细胞。最大的吸引力在于它是一种无人监管的方法。这意味着你不需要任何标签，而是算法为你找到标签。</p><p id="f368" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">最简单的算法是k-means，我相信您已经听说过它，并且可能使用过它。但我总是发现从头开始编写这些算法很有帮助，即使是简单的算法。今天的帖子结构如下，只需跳转到你最感兴趣的部分，或者阅读所有内容以获得完整图片:</p><ul class=""><li id="db6f" class="mp mq iq lq b lr mk lu ml lx mr mb ms mf mt mj mu mv mw mx bi translated">k-均值理论简介</li><li id="0883" class="mp mq iq lq b lr my lu mz lx na mb nb mf nc mj mu mv mw mx bi translated">k均值实现</li><li id="f990" class="mp mq iq lq b lr my lu mz lx na mb nb mf nc mj mu mv mw mx bi translated">如何求k的理想数</li><li id="1db5" class="mp mq iq lq b lr my lu mz lx na mb nb mf nc mj mu mv mw mx bi translated">k-means的优缺点</li><li id="8d0a" class="mp mq iq lq b lr my lu mz lx na mb nb mf nc mj mu mv mw mx bi translated">与其他聚类算法的关系，如k-mediods或模糊c-means</li><li id="1992" class="mp mq iq lq b lr my lu mz lx na mb nb mf nc mj mu mv mw mx bi translated">摘要</li></ul><p id="6e38" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">说到这里，让我们开始吧。</p></div><div class="ab cl nd ne hu nf" role="separator"><span class="ng bw bk nh ni nj"/><span class="ng bw bk nh ni nj"/><span class="ng bw bk nh ni"/></div><div class="ij ik il im in"><h1 id="e127" class="kw kx iq bd ky kz nk lb lc ld nl lf lg jw nm jx li jz nn ka lk kc no kd lm ln bi translated">k均值理论</h1><p id="7e50" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">无监督学习方法试图在你的数据中找到结构，而不需要你方太多的初始输入。这使得它们对任何类型的应用都非常有吸引力。与任何其他聚类算法一样，k-means希望将相似的观察值分组在一起，同时分离不相似的点。K-means只需要1个超参数，即<em class="np"> k </em>，期望聚类数。这使得它很容易运行，但也有一些缺点，稍后会讨论。在数学上，k-means专注于最小化类内平方和(WCSS ),也称为类内方差、类内距离或惯性:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/ba242580d400712afaa0b036e2ff7eba.png" data-original-src="https://miro.medium.com/v2/resize:fit:360/format:webp/1*VmUkGaiBEz_b0xB-FclruQ.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">组内平方和的定义。k表示集群。</p></figure><p id="518e" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">其中k是集群，||。||是欧几里得范数，在这种情况下是两点之间的欧几里得距离。</p><p id="6743" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">由于我们最小化所有的集群<em class="np"> C </em>，我们可以将优化函数写成如下:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/81cd36676019adc35d2aec5d617a1c01.png" data-original-src="https://miro.medium.com/v2/resize:fit:866/format:webp/1*YwBgvi-RY3p66fDOO5IyxA.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">k均值的优化函数</p></figure><p id="98ab" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">其中|Cᵢ|是群集的基数，即其中的观察值的数量。</p><p id="cab7" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">人们也可以将其解释为最大化聚类之间的总方差，也称为聚类间距离，如总方差定律所述:</p><p id="763b" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated"><em class="np">总方差=解释方差(WCSS) +未解释方差(聚类间距离)</em></p></div><div class="ab cl nd ne hu nf" role="separator"><span class="ng bw bk nh ni nj"/><span class="ng bw bk nh ni nj"/><span class="ng bw bk nh ni"/></div><div class="ij ik il im in"><h1 id="7bb7" class="kw kx iq bd ky kz nk lb lc ld nl lf lg jw nm jx li jz nn ka lk kc no kd lm ln bi translated">k均值实现</h1><p id="beca" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">我们将逐一介绍k-means的所有步骤。对于这个实现，我试图非常明确，使代码尽可能容易理解。当然，这不是最快的实现，但这里的目标是首先理解。你也可以在我的<a class="ae kv" href="https://github.com/MSHelm/algorithms-from-scratch" rel="noopener ugc nofollow" target="_blank"> GitHub </a>上以RMarkdown文档的形式关注这个例子。</p><p id="b5d6" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">因为我们将在这个例子中使用iris数据集，所以让我们假设我们是园丁。我们辛辛苦苦培育出三种不同的美丽花朵，可惜忘了给幼苗加上标签。它们还没有开花，但我们真的想确保只一起种同样的花。我们现在所能做的就是测量外面的叶子，也就是所谓的萼片。让我们先来看看数据:</p><pre class="kg kh ki kj gt ns nt nu nv aw nw bi"><span id="235d" class="nx kx iq nt b gy ny nz l oa ob">head(iris)</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/5d140471f35f860a8af2ce56aba1c721.png" data-original-src="https://miro.medium.com/v2/resize:fit:1216/format:webp/1*Sh93cX6SsQ_rlbWxDgteTA.png"/></div></figure><p id="b0ca" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">如你所见，数据集包含了关于萼片、花瓣的信息，以及这种植物来自三个物种中的哪一个，这就是我们想要预测的。因此，让我们首先只选择我们示例中的数据:</p><pre class="kg kh ki kj gt ns nt nu nv aw nw bi"><span id="ef8a" class="nx kx iq nt b gy ny nz l oa ob">df &lt;- iris[, c(“Sepal.Length”, “Sepal.Width”)] <br/>plot(df)</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi od"><img src="../Images/78ad3ebd4d3cff1df622f8eb8ba797bb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AwgyPkeNrEPJLh_by-mnsg.png"/></div></div></figure><p id="97d9" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">目测只显示两组，尽管我们预期有3种不同的物种。那么让我们看看k-means是否也能找到第三组。</p><h2 id="0b2e" class="nx kx iq bd ky oe of dn lc og oh dp lg lx oi oj li mb ok ol lk mf om on lm oo bi translated">距离度量</h2><p id="e8a5" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">首先，我们需要定义距离度量，我们将使用欧几里德距离来测量每个点到中心的距离:</p><pre class="kg kh ki kj gt ns nt nu nv aw nw bi"><span id="0e4c" class="nx kx iq nt b gy ny nz l oa ob">euclidean_distance &lt;- function(p1, p2) {<br/>  dist &lt;- sqrt( (p1[1] - p2[1])^2 + (p1[2] - p2[2])^2 )<br/>  as.numeric(dist)<br/>}</span></pre><p id="602d" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">这个距离函数当然只对二维情况有效，但是可以很容易地扩展到任何你想要的维度。如果输入是一个data.frame或一个命名的vector，我们将输出封装在对<code class="fe op oq or nt b">as.numeric()</code>的调用中以删除任何名称。</p><h2 id="b41b" class="nx kx iq bd ky oe of dn lc og oh dp lg lx oi oj li mb ok ol lk mf om on lm oo bi translated">初始化</h2><p id="9bbe" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">K-means需要对第一个中心进行初始猜测。这也意味着，我们需要将期望中心的数量输入到算法中，这是k-means最突出的缺点之一。我们将在它自己的章节中讨论这一点，现在让我们假设我们知道我们有三个不同的品种。</p><p id="8362" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">为了初始化算法，我们简单地从数据集中随机选择三个点。为了强调我们需要在没有替换的情况下这样做，我也写出了这个可选参数，即使默认值已经设置为FALSE。</p><pre class="kg kh ki kj gt ns nt nu nv aw nw bi"><span id="26a7" class="nx kx iq nt b gy ny nz l oa ob">k &lt;- 3 <br/>centers &lt;- df[sample(nrow(df), k, replace = FALSE), ]</span></pre><h2 id="b890" class="nx kx iq bd ky oe of dn lc og oh dp lg lx oi oj li mb ok ol lk mf om on lm oo bi translated">循环</h2><p id="88df" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">接下来，我们需要计算每个点到这些中心的距离。</p><pre class="kg kh ki kj gt ns nt nu nv aw nw bi"><span id="8f55" class="nx kx iq nt b gy ny nz l oa ob">distances &lt;- matrix(Inf, nrow = nrow(df), ncol = k)<br/>    for (i in seq_len(nrow(df))) {<br/>      for (j in seq_len(k)) {<br/>        distances[i, j] &lt;- euclidean_distance(df[i, ], centers[j, ])<br/>      }<br/>    }</span></pre><p id="c070" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">现在我们可以将每个点分配到它最近的中心。为此，我们逐行检查结果，并选择距离最小的条目。</p><pre class="kg kh ki kj gt ns nt nu nv aw nw bi"><span id="e6a4" class="nx kx iq nt b gy ny nz l oa ob">cluster_id &lt;- apply(distances, 1, which.min)</span></pre><p id="7df3" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">最后，我们需要计算每个集群的新中心。为了保持高度显式的编码风格，我们也在for循环中这样做。</p><pre class="kg kh ki kj gt ns nt nu nv aw nw bi"><span id="b4ca" class="nx kx iq nt b gy ny nz l oa ob">for (i in seq_len(k)) {<br/>      this_cluster &lt;- df[cluster_id == i,]<br/>      centers[k, ] &lt;- apply(this_cluster, 2, mean)<br/>    }</span></pre><p id="0c04" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">瞧，这就是一轮k均值的全部内容。因为我们在这一步中有效地移动了中心的坐标，所以k-means属于坐标下降算法组。</p><p id="4b77" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">但是当然，一次迭代是不够的。相反，我们需要在数据集上迭代多次，并总是调整我们的中心。在我们的实现中，我们选择在终止之前运行k-means定义的迭代次数。要做到这一点，我们可以将所有内容包装在while循环中，并放入一个函数中。可选地，如果算法已经收敛(即，中心在两次迭代之间不改变)，或者总WCSS的变化低于定义的阈值，则可以包括提前终止的检查。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="os ot l"/></div></figure><h2 id="c548" class="nx kx iq bd ky oe of dn lc og oh dp lg lx oi oj li mb ok ol lk mf om on lm oo bi translated">分析我们的测试数据</h2><p id="4ee4" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">让我们进行分析，并与真实物种进行比较</p><pre class="kg kh ki kj gt ns nt nu nv aw nw bi"><span id="a742" class="nx kx iq nt b gy ny nz l oa ob">set.seed(42)<br/>cluster_id &lt;- my_kmeans(data = df, k = 3, n_iterations = 10)</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi od"><img src="../Images/354b0b07ad9c35a9d50fd59853dd6a26.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*U0Gcbp5bV5x4NjJR-k5WyA.png"/></div></div></figure><p id="45a8" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">正如你所看到的，K-means确实能够将植物分成三组，这与潜在的地面真相相似。</p></div><div class="ab cl nd ne hu nf" role="separator"><span class="ng bw bk nh ni nj"/><span class="ng bw bk nh ni nj"/><span class="ng bw bk nh ni"/></div><div class="ij ik il im in"><h1 id="cf66" class="kw kx iq bd ky kz nk lb lc ld nl lf lg jw nm jx li jz nn ka lk kc no kd lm ln bi translated">如何求k的理想数</h1><p id="c1d7" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">正如我们所见，k-means要求我们输入预期的聚类数。这是一个严重的局限性，因为我们可能不一定事先知道这一点。此外，k-means将<strong class="lq ir">总是</strong>输出具有指定数量的<em class="np"> k </em>个聚类的聚类，不管这是否有意义。那么我们该如何应对呢？有几种可能的方法。</p><h2 id="e74a" class="nx kx iq bd ky oe of dn lc og oh dp lg lx oi oj li mb ok ol lk mf om on lm oo bi translated">肘法</h2><p id="228c" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">一种非常常见的解决方案是所谓的肘法。我们用几个候选值<em class="np"> k </em>执行k-means，然后选择最好的一个。为了进行这种选择，我们使用总WCSS，即所有集群的WCSS的总和。</p><p id="2b67" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">为了更直观地解释这一点，首先想象一个k = 1的例子。所有事件将属于同一个聚类，这意味着总WCSS将等于数据集中的总平方和。如果我们将<em class="np">k</em>增加到2，至少一些点将属于第二簇。这将减少第一簇内的SS，并且总WCSS也将更小，因为总WCSS具有<em class="np"> k = 1 </em>。所以k<em class="np">变得越高，WCSS就越小。在<em class="np"> k =观测数量</em>的极端情况下，WCSS将为0。</em></p><p id="98a9" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">为了选择最佳的<em class="np"> k </em>，我们需要在WCSS的减少和我们的模型对数据的过度拟合之间找到最佳的平衡。肘方法通过选择WCSS行为中“弯曲”最强的点来实现。为了说明这一点，让我们在数据中绘制从1到10的k值的行为。从现在开始，我们将使用已经实现的k-means方法，因为它更快，并且已经实现了WCSS，这是结果列表的<em class="np"> tot.withinss </em>元素。</p><pre class="kg kh ki kj gt ns nt nu nv aw nw bi"><span id="7c18" class="nx kx iq nt b gy ny nz l oa ob">within_cluster_ss &lt;- c()<br/>for (k in 1:10) {<br/>  within_cluster_ss[k] &lt;- kmeans(df, k)$tot.withinss<br/>}</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi od"><img src="../Images/d11554f02e5b3d3aad941ceb7fb71958.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LWeuz26TOTYEvV-cxtsE3w.png"/></div></div></figure><p id="c2db" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">我们可以推测弯曲最强的点在<em class="np"> k = 3 </em>处，但不妨在<em class="np"> k = 4 </em>处。</p><p id="cf29" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">此外，如果能自动获得这个选项，而不需要我们手动选择，那就太好了。我们可以用一个小的助手函数来自动化这个过程。首先，我们在最大值点和最小值点之间拟合一条线。然后我们计算每个点到这条线的距离。距离最高的点是肘点。或者，可以用最大绝对二阶导数来计算点。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="os ot l"/></div></figure><p id="ca0c" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">事实上，它返回<em class="np"> k = 3 </em>，但这只是一个试探。例如，重新运行相同的示例，但是这次在1和20之间改变k。您将看到，在这种情况下，它将返回<em class="np"> k = 4 </em>作为最佳聚类数。</p><h2 id="3efb" class="nx kx iq bd ky oe of dn lc og oh dp lg lx oi oj li mb ok ol lk mf om on lm oo bi translated">剪影分数</h2><p id="daba" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">另一种评估哪个<em class="np"> k </em>最符合数据的方法是所谓的轮廓分数。它测量一个聚类中的点的相似性，并将其与其他聚类中的点的相似性进行比较。其范围在-1和1之间，其中-1表示与其指定的分类非常不相似的点，而1表示与其分类高度相似的点。值为0表示该点位于两个相邻簇之间的边缘。让我们一步一步来看:</p><p id="75c3" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">在运行k-means之后，对于每个聚类，我们计算每个点到同一聚类中所有其他点的平均距离。也可以将其解释为每个点与其被分配到的聚类有多相似，低值表示高相似性。设<em class="np"> i </em>是聚类<em class="np"> Cᵢ </em>中的数据点，并且<em class="np"> d </em>是距离度量(在我们的例子中是欧几里德距离)。那么这个点I到它的集群的距离被定义为:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ou"><img src="../Images/d0e47168d0ae1b5f57e26991d14e998f.png" data-original-src="https://miro.medium.com/v2/resize:fit:624/format:webp/1*6mzP3iqESbmN9tuV1suyzA.png"/></div></figure><p id="ee78" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">其中<em class="np"> |Cᵢ| </em>再次是集群的基数，即其中元素的数量。较小的值表示一个与其聚类非常相似的点。</p><p id="2ba1" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">或者，我们可以排除一个点到自身的距离，得到:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ov"><img src="../Images/a8ebcbbcb9ab88569e242ff79e66f308.png" data-original-src="https://miro.medium.com/v2/resize:fit:742/format:webp/1*NBqZw895ZVkviQ-WtJ-rdg.png"/></div></figure><p id="ce0d" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">在第二步中，我们计算该点与其他聚类的相似性。因此，对于每个其他聚类，我们计算点到属于聚类<em class="np"> k </em>的所有点<em class="np"> j </em>的平均距离。然后，相异度被定义为到最近聚类的平均距离，最近聚类也被称为点<em class="np"> i </em>的“相邻聚类”。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ow"><img src="../Images/bd53ff0e2a42e861661feb1fe2c94fad.png" data-original-src="https://miro.medium.com/v2/resize:fit:844/format:webp/1*6rT7S1Jrz7ToYJwmHe1X5A.png"/></div></figure><p id="9828" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">最后，我们需要相互权衡，得出轮廓分数:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ox"><img src="../Images/0b5f5f2658ba4e9c1959f302822b98e0.png" data-original-src="https://miro.medium.com/v2/resize:fit:966/format:webp/1*DJiJaOaZOsi6OXrZam2B5g.png"/></div></figure><p id="6df4" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">让我们假设我们有一个点在它所分配的簇的正中心。在这种情况下，到其分配的聚类的距离为0，这产生1:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oy"><img src="../Images/adb2d211a1b9412e67b492310b70b7a7.png" data-original-src="https://miro.medium.com/v2/resize:fit:516/format:webp/1*AS19foAblKN5_pf74sd7-A.png"/></div></figure><p id="62c4" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">相反，两个聚类之间的边界处的点到其所分配的聚类的中心和最近的相邻聚类的中心的距离相等。接下来是:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oz"><img src="../Images/3d6651eb89afbc98d59cdb5c12c5dd0a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1046/format:webp/1*9Th1gF56Arpn3CTS0DiBDA.png"/></div></figure><p id="19f7" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">理论上，剪影分数也可以变为负值(直到-1)，这表示一个点应该被指定给不同的聚类。对于k-意味着不应该发生这种情况，因为我们根据距离来分配点。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="os ot l"/></div></figure><p id="da6f" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">对于一个集群只包含一个成员的情况，我们需要定义一个固定的分数。我们选择0，因为它是介于-1和1之间的中性选择。所以在<em class="np"> s(i) = 0的情况下，如果|Cᵢ| = 1 </em></p><p id="a574" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">对于聚类是否良好的程序性评估，通常计算每个<em class="np"> k </em>在所有点上的<em class="np"> s </em>的平均值。然后，我们选择具有最高平均值的<em class="np"> k </em>。</p><pre class="kg kh ki kj gt ns nt nu nv aw nw bi"><span id="bb70" class="nx kx iq nt b gy ny nz l oa ob">silhouette_scores &lt;- c()</span><span id="179f" class="nx kx iq nt b gy pa nz l oa ob">for (k in 2:10) {<br/>  cluster_id &lt;- kmeans(df, k)$cluster<br/>  silhouettes &lt;- silhouette(df, cluster_id)<br/>  silhouette_scores &lt;- cbind(silhouette_scores, silhouettes)<br/>}</span><span id="5948" class="nx kx iq nt b gy pa nz l oa ob">colnames(silhouette_scores) &lt;- 2:10<br/>silhouette_scores &lt;- apply(silhouette_scores, 2, mean)<br/>best_k &lt;- names(which.max(silhouette_scores))</span></pre><p id="4249" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">或者，可以绘制给定<em class="np"> k </em>的所有<em class="np"> s(i) </em>值，并直观地评估聚类:</p><pre class="kg kh ki kj gt ns nt nu nv aw nw bi"><span id="0085" class="nx kx iq nt b gy ny nz l oa ob">k &lt;- 3<br/>kmeans_res &lt;- kmeans(df, k)<br/>cluster_id &lt;- kmeans_res$cluster<br/>silhouettes &lt;- silhouette(df, cluster_id)<br/>result_df &lt;- data.frame(cluster_id = cluster_id, silhouette = silhouettes)<br/>result_df &lt;- result_df[order(result_df$cluster_id, result_df$silhouette), ]</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi od"><img src="../Images/53e39af0b72a056fa4d9d840285aacfb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iU60cW94p4t_B2FPXZwfwg.png"/></div></div></figure></div><div class="ab cl nd ne hu nf" role="separator"><span class="ng bw bk nh ni nj"/><span class="ng bw bk nh ni nj"/><span class="ng bw bk nh ni"/></div><div class="ij ik il im in"><h1 id="fdce" class="kw kx iq bd ky kz nk lb lc ld nl lf lg jw nm jx li jz nn ka lk kc no kd lm ln bi translated">聚类结果评估</h1><h2 id="3554" class="nx kx iq bd ky oe of dn lc og oh dp lg lx oi oj li mb ok ol lk mf om on lm oo bi translated">集群基数</h2><p id="a523" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">我们已经知道集群基数是集群中事件的数量。根据您对数据的了解，您可能期望大小相等的簇，或者大小不同的簇。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi od"><img src="../Images/967ecc0a1b46cded97b57c6cccc4cf8d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pjJORgfvTc-Zz2-FzO3EnA.png"/></div></div></figure><p id="b94b" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">我看到一些人建议集群应该总是大小相等，但是我不认为这是真的。如果数据中真的有更多罕见的群体正在形成他们自己的群体呢？对于具有罕见特征的患者、社会网络分析、具有独特基因表达水平的细胞或只是你正在研究的商品制造过程中的差异，这可能是真的。因此，如果您的集群显示出不同的基数，不要太担心，但还是要研究离群值。</p><h2 id="e1aa" class="nx kx iq bd ky oe of dn lc og oh dp lg lx oi oj li mb ok ol lk mf om on lm oo bi translated">集群基数与集群数量</h2><p id="7818" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">聚类大小描述了一个聚类中所有点到其中心的总距离。幸运的是，R中的k-means实现已经为您计算了这些值:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi od"><img src="../Images/519b3737c6a51fef480629255ca39a99.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*K8kmPfFxSs7antpPlAIIEQ.png"/></div></div></figure><p id="fd63" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">这个值本身并不能提供太多的洞察力，类似于单独的集群基数。但是把两者结合起来会在结果中显示出更好的趋势。直观上，人们会期望具有很少成员(即低基数)的聚类也具有这些点到聚类中心的低总差异。所以这两个值应该是正相关的。如果不是这种情况，这将意味着分散的集群，这可能意味着这些实际上是噪声点。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi od"><img src="../Images/f57b4cc54ce8c0cd56f06f92bb98af71.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UJz-ojH_7QUB2y2zZr53ow.png"/></div></div></figure><p id="27c2" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">正如我们所看到的，这个图对于k<em class="np">和</em>的低值没有太大帮助。为了更好地说明这一点，让我们画出<em class="np"> k = 10 </em>的结果。我们可以观察到一个明显的异常值:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi od"><img src="../Images/031ccaafac527ad97a9fe351e7208e01.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*reM9MqbDPKS6jNdC_NOlmA.png"/></div></div></figure><h2 id="49af" class="nx kx iq bd ky oe of dn lc og oh dp lg lx oi oj li mb ok ol lk mf om on lm oo bi translated">集群特征</h2><p id="d1b0" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">最后，最后但可能是最重要的一步是描述您的集群。根据您所在的领域，这可能是维度的简单汇总统计(在我们的示例中，花瓣有多长/多宽)，或者其他描述，如图像强度、基因表达水平等。毕竟，当运行聚类分析时，您感兴趣的是出现了哪些组，所以在找到它们之后详细描述它们！</p></div><div class="ab cl nd ne hu nf" role="separator"><span class="ng bw bk nh ni nj"/><span class="ng bw bk nh ni nj"/><span class="ng bw bk nh ni"/></div><div class="ij ik il im in"><h1 id="c3d0" class="kw kx iq bd ky kz nk lb lc ld nl lf lg jw nm jx li jz nn ka lk kc no kd lm ln bi translated">k均值的优势</h1><p id="602b" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">在深入研究了实现之后，让我们关注k-means的一些更高级的特征。这是一种流行的算法，有几个优点:</p><h2 id="a04b" class="nx kx iq bd ky oe of dn lc og oh dp lg lx oi oj li mb ok ol lk mf om on lm oo bi translated">概念上很容易理解</h2><p id="2274" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">正如我们所看到的，这个理论很简单，人们可以很容易地想象和理解k-means背后的概念。</p><h2 id="54a6" class="nx kx iq bd ky oe of dn lc og oh dp lg lx oi oj li mb ok ol lk mf om on lm oo bi translated">它基本上是用所有编程语言实现的</h2><p id="e538" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">几乎每种常见的编程语言都有k-means的实现，这使得在数据集上运行它变得很容易。</p><h2 id="ea0f" class="nx kx iq bd ky oe of dn lc og oh dp lg lx oi oj li mb ok ol lk mf om on lm oo bi translated">它只有一个超参数</h2><p id="8535" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated"><em class="np"> k </em>，集群的数量</p><h2 id="85bd" class="nx kx iq bd ky oe of dn lc og oh dp lg lx oi oj li mb ok ol lk mf om on lm oo bi translated">它与观察次数成正比。</h2><p id="17d6" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">其复杂度为<em class="np"> O(n </em> k <em class="np"> d </em> i)，其中</p><ul class=""><li id="6712" class="mp mq iq lq b lr mk lu ml lx mr mb ms mf mt mj mu mv mw mx bi translated"><em class="np">n</em>= d维向量的数量</li><li id="bad1" class="mp mq iq lq b lr my lu mz lx na mb nb mf nc mj mu mv mw mx bi translated"><em class="np"> k </em> =集群的数量</li><li id="48ae" class="mp mq iq lq b lr my lu mz lx na mb nb mf nc mj mu mv mw mx bi translated"><em class="np"> i </em> =收敛前的迭代次数</li></ul><h1 id="550c" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">k-means的缺点</h1><p id="be44" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">另一方面，k-means的简单性也带来了一些主要的缺点:</p><h2 id="6b9c" class="nx kx iq bd ky oe of dn lc og oh dp lg lx oi oj li mb ok ol lk mf om on lm oo bi translated">它需要k的先验知识</h2><p id="a873" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">这是一个主要缺点，因为人们通常不知道数据中存在的聚类数。此外，它将总是准确地返回k个集群，不管数据中实际上有多少个是有意义的。我们可以通过尝试不同的<em class="np"> k </em>并选择最好的一个来减轻这一点，但是正如我们所看到的，这也并不总是奏效。</p><h2 id="12a0" class="nx kx iq bd ky oe of dn lc og oh dp lg lx oi oj li mb ok ol lk mf om on lm oo bi translated">这是一种启发式算法</h2><p id="af83" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">由于实际问题是NP难解决的，通常使用启发式算法来寻找解决方案，主要是Loyd算法。这意味着返回的结果也可能只是局部最优。</p><h2 id="2472" class="nx kx iq bd ky oe of dn lc og oh dp lg lx oi oj li mb ok ol lk mf om on lm oo bi translated">它不是决定性的</h2><p id="9f1a" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">由于初始化是随机的，我们使用启发式算法，多次运行k-means可以返回非常不同的结果。</p><h2 id="518f" class="nx kx iq bd ky oe of dn lc og oh dp lg lx oi oj li mb ok ol lk mf om on lm oo bi translated">它受到离群值的严重影响</h2><p id="c557" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">由于欧几里德距离是两点之间的差的平方，异常值会严重影响结果。</p><h2 id="dc8e" class="nx kx iq bd ky oe of dn lc og oh dp lg lx oi oj li mb ok ol lk mf om on lm oo bi translated">随着维度的增加，它的伸缩性很差</h2><p id="5d62" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">随着维数的增加，它们之间的欧几里德距离变得非常相似。这就是众所周知的维数灾难。但是如果距离不再有意义，我们的算法也会崩溃。</p></div><div class="ab cl nd ne hu nf" role="separator"><span class="ng bw bk nh ni nj"/><span class="ng bw bk nh ni nj"/><span class="ng bw bk nh ni"/></div><div class="ij ik il im in"><h1 id="40f9" class="kw kx iq bd ky kz nk lb lc ld nl lf lg jw nm jx li jz nn ka lk kc no kd lm ln bi translated">k-means做出的假设</h1><p id="9b20" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">与任何算法一样，有一些明确或隐含的假设</p><h2 id="38f0" class="nx kx iq bd ky oe of dn lc og oh dp lg lx oi oj li mb ok ol lk mf om on lm oo bi translated">聚类是球形的/到中心点的方差是均值</h2><p id="d3ad" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">由于距离度量是对称的，它将总是找到具有球形边界的聚类。请注意，这并不意味着它适用于所有圆形的点云，例如考虑环形集群。k-means将无法从外环中分离内部点云。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi od"><img src="../Images/8677667bc925d840f66178b46be4061e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*c4gkcCBs0fZGt_hWxhm3Aw.png"/></div></div></figure><h2 id="6c0a" class="nx kx iq bd ky oe of dn lc og oh dp lg lx oi oj li mb ok ol lk mf om on lm oo bi translated">集群大小相等</h2><p id="382b" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">有几篇关于堆栈溢出的帖子和帖子在讨论k-means是否假设集群应该大小相等。基本度量标准没有明确地做出这种假设，但是从算法的工作方式可以看出这是有意义的。假设我们有两个大小非常不同的集群。因为我们随机选择初始中心，所以我们有很高的概率在大的集群中初始化这两个中心。小集群将无法“吸引”其中一个中心，从而导致错误的结果。为了了解实际情况，让我们构建两个大小分别为50和10000的集群:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi od"><img src="../Images/1279be5f402443f8d5b048ed6a689702.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*94wnG1iPcasd1JDjfM3K8g.png"/></div></div></figure><h2 id="6278" class="nx kx iq bd ky oe of dn lc og oh dp lg lx oi oj li mb ok ol lk mf om on lm oo bi translated">尺寸同样重要</h2><p id="4db0" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">这与星团大致呈球形的假设方向一致。如果一个维度的规模比其他维度大得多，它将主导聚类。因此，在聚类之前对数据进行规范化非常重要。</p></div><div class="ab cl nd ne hu nf" role="separator"><span class="ng bw bk nh ni nj"/><span class="ng bw bk nh ni nj"/><span class="ng bw bk nh ni"/></div><div class="ij ik il im in"><h1 id="08a4" class="kw kx iq bd ky kz nk lb lc ld nl lf lg jw nm jx li jz nn ka lk kc no kd lm ln bi translated">与其他聚类方法的关系</h1><p id="6f68" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">K-means是许多不同聚类变体发展的起点:</p><h2 id="cde9" class="nx kx iq bd ky oe of dn lc og oh dp lg lx oi oj li mb ok ol lk mf om on lm oo bi translated">k-表示++的意思</h2><p id="90cc" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">k-means++使用了改进的初始化。它不是随机选择所有初始中心，而是随机选择第一个中心。然后，选择所有后续中心，使它们在新中心和所有现有中心之间具有最大距离。</p><h2 id="f1e3" class="nx kx iq bd ky oe of dn lc og oh dp lg lx oi oj li mb ok ol lk mf om on lm oo bi translated">k-中间值</h2><p id="d367" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">k-medians选择中位数而不是平均值来计算聚类中心。通常，它使用曼哈顿距离而不是欧几里德距离作为其度量，但也可以使用更复杂的度量，如Kullback-Leibler散度。</p><h2 id="66a8" class="nx kx iq bd ky oe of dn lc og oh dp lg lx oi oj li mb ok ol lk mf om on lm oo bi translated">k-mediods/PAM/k-center</h2><p id="e084" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">k-mediods选择聚类中最有代表性的点作为中心，因此中心始终是数据集中的一个实际数据点。这不要与k-中值混淆，k-中值的中心不必是实际的数据点。这是可以看到的，因为二维的中线可能不是来自同一点！</p><p id="a808" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">它还使用了一个略有不同的迭代步骤:在初始化之后，对于每个聚类，它尝试将其包含的每个点作为潜在的新的聚类中心，并挑选一个最小化所选择的相异度度量的点。</p><p id="e995" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">最后，它在想要使用的不同度量方面也非常灵活。</p><h2 id="8634" class="nx kx iq bd ky oe of dn lc og oh dp lg lx oi oj li mb ok ol lk mf om on lm oo bi translated">模糊c均值</h2><p id="71a9" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">它不是硬聚类，而是分配每个点属于给定聚类的概率。这意味着一个点可以是多个簇的成员，这在某些情况下可能是有意义的(例如基因属于多个途径)。</p><h2 id="0ab2" class="nx kx iq bd ky oe of dn lc og oh dp lg lx oi oj li mb ok ol lk mf om on lm oo bi translated">高斯混合模型</h2><p id="1af9" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">也可以将k-means解释为不考虑协方差的高斯混合模型的硬聚类变体。期望步骤是将每个观察值分配到其关闭中心的步骤。最大化步骤是重新计算中心的步骤。</p></div><div class="ab cl nd ne hu nf" role="separator"><span class="ng bw bk nh ni nj"/><span class="ng bw bk nh ni nj"/><span class="ng bw bk nh ni"/></div><div class="ij ik il im in"><h1 id="51da" class="kw kx iq bd ky kz nk lb lc ld nl lf lg jw nm jx li jz nn ka lk kc no kd lm ln bi translated">摘要</h1><p id="9e23" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">总之，我们已经看到k-means是一种简单而直观的算法，可以在基本上任何编程语言中快速应用于数据集。它的主要缺点是需要输入预期的聚类数。尽管如此，它的丰富性和简单性使它成为一个有用的工具，可以快速地在您的数据上运行它以使您熟悉它。</p></div></div>    
</body>
</html>