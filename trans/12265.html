<html>
<head>
<title>Exploring the Various Ways to Evaluate Features For Feature Selection</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">探索评估特征的各种方法以进行特征选择</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/exploring-the-various-ways-to-evaluate-features-for-feature-selection-1142f7788aeb?source=collection_archive---------25-----------------------#2021-12-13">https://towardsdatascience.com/exploring-the-various-ways-to-evaluate-features-for-feature-selection-1142f7788aeb?source=collection_archive---------25-----------------------#2021-12-13</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="0e5e" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">概述用于为机器学习模型过滤掉不想要的特征的常用方法</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/bf925ca6eea91915aa02791f17314773.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uSMeFw48sEw1i7R6DRnsDw.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">照片由<a class="ae kv" href="https://www.pexels.com/photo/architecture-black-and-white-challenge-chance-277593/?utm_content=attributionCopyText&amp;utm_medium=referral&amp;utm_source=pexels" rel="noopener ugc nofollow" target="_blank">像素</a>的<a class="ae kv" href="https://www.pexels.com/@pixabay?utm_content=attributionCopyText&amp;utm_medium=referral&amp;utm_source=pexels" rel="noopener ugc nofollow" target="_blank">皮克斯拜</a>拍摄</p></figure><p id="5d6f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">特征选择是许多机器学习任务中的常见组成部分。</p><p id="63a2" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这是用于降低维数的方法之一，从而确保高模型性能，同时减少过拟合和计算需求。</p><p id="d11d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">无论您是在构建推理模型还是预测模型，通过首先验证您已经选择了用于训练模型的最佳功能集，您将获得更好的结果。</p><p id="67be" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在这里，我将快速概述一些常见的方法，以确定最适合构建机器学习模型的特征。</p><h1 id="34ed" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">特征选择的例子</h1><p id="9dd6" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">有许多标准可用于决定保留或省略哪些功能。</p><p id="2ee2" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">将使用存储房价信息的数据集(无版权)演示每种类型的特征选择。数据集可在此访问<a class="ae kv" href="https://www.kaggle.com/elakiricoder/jiffs-house-price-prediction-dataset" rel="noopener ugc nofollow" target="_blank">。</a></p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="mp mq l"/></div></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mr"><img src="../Images/8b885f7319800688c5639b67355f04ae.png" data-original-src="https://miro.medium.com/v2/resize:fit:1110/format:webp/1*lk2eUCr4BlxZRmpuRld8cw.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">代码输出(由作者创建)</p></figure><p id="f6b0" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">此数据集中的目标标签是“property_value”。在功能选择过程中不会考虑此功能。</p><h2 id="e8ef" class="ms lt iq bd lu mt mu dn ly mv mw dp mc lf mx my me lj mz na mg ln nb nc mi nd bi translated">1.基于缺失值选择要素</h2><p id="95bc" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">有许多方法可以处理数据集中的缺失值。虽然删除没有数据的记录是一种选择，但不鼓励这样做，因为这意味着放弃有价值的信息。</p><p id="4284" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">也就是说，在某些情况下，如果大多数记录没有为某个要素赋值，则完全移除该要素可能是唯一的选择。</p><p id="e8a6" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在python中，您可以轻松识别是否有要素缺少太多值。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="mp mq l"/></div></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ne"><img src="../Images/1be7ada691791b0a1d29f5f5e7ccca2b.png" data-original-src="https://miro.medium.com/v2/resize:fit:648/format:webp/1*cm9CGBkoYz7DBCUyfG6OZg.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">代码输出(由作者创建)</p></figure><p id="8ee2" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">如输出中所示，所有要素都没有任何缺失值，不需要消除任何要素。</p><h2 id="56c8" class="ms lt iq bd lu mt mu dn ly mv mw dp mc lf mx my me lj mz na mg ln nb nc mi nd bi translated">2.基于方差选择要素</h2><p id="02ae" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">为了具有预测能力，要素的值需要显示某种程度的差异。</p><p id="7e9e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">因此，评估特性的一个标准是特性值的方差。</p><p id="2f20" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">方便的是，sklearn模块提供了<a class="ae kv" href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.VarianceThhttps://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.VarianceThreshold.htmlreshold.html" rel="noopener ugc nofollow" target="_blank"> VarianceThreshold </a>，这是一个基于给定方差阈值选择特性的特性选择器。</p><p id="789b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">下面是如何实现VarianceThreshold的简单示例:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="mp mq l"/></div></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nf"><img src="../Images/76bcc50b7a20cf3e44ccacbd36766504.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7iio2HtEAGqugavv-CSEKQ.png"/></div></div></figure><p id="edac" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">VarianceThreshold在数据集中选择满足所需方差的9个预测特征，同时忽略不满足方差的6个特征。</p><h2 id="1579" class="ms lt iq bd lu mt mu dn ly mv mw dp mc lf mx my me lj mz na mg ln nb nc mi nd bi translated">3.基于与其他特征的相关性选择特征</h2><p id="7d4c" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">对于构建推理模型，特征之间没有关系是理想的。强相关的预测特征只会产生更多的噪声，导致特征系数估计的更高的方差。</p><p id="64ae" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这将使得从基于这种模型的分析中获得洞察力变得更加困难。</p><p id="3aad" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">预测要素与其他要素有很强相关性的现象称为多重共线性。我在这里给主题<a class="ae kv" rel="noopener" target="_blank" href="/targeting-multicollinearity-with-python-3bd3b4088d0b">一个概述。</a></p><p id="040e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">多重共线性可以通过首先识别表现出强相关性的要素来检测。</p><p id="e160" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">查找此类要素的一种方法是构建一个显示所有要素对的相关系数值的热图。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="mp mq l"/></div></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ng"><img src="../Images/9faf7a926676b84291cbe66d42185f2d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*y_emS0X4AeuRQfW9fDaM4A.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">代码输出(由作者创建)</p></figure><p id="0c80" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">对于该数据集，很明显，以下各项之间存在很强的相关性:</p><ul class=""><li id="6d46" class="nh ni iq ky b kz la lc ld lf nj lj nk ln nl lr nm nn no np bi translated">“房屋面积平方米”和“土地面积平方米”</li><li id="4946" class="nh ni iq ky b kz nq lc nr lf ns lj nt ln nu lr nm nn no np bi translated">土地面积平方米和房间数量</li><li id="a3d2" class="nh ni iq ky b kz nq lc nr lf ns lj nt ln nu lr nm nn no np bi translated">“房屋面积平方米”和“房间数量”</li></ul><p id="d647" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">直觉上，这些观察是有意义的。例如，“房子大小平方米”和“土地大小平方米”实际上给了我们相同的信息。包括这两个特征将降低用该数据训练的任何因果模型的可靠性。</p><p id="7d9a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">要确定应消除哪些要素，我们可以找到具有高方差膨胀因子(VIF)值的要素并将其移除。</p><p id="4b08" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">注:通常，10或更大的VIF值被认为太高。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="mp mq l"/></div></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nv"><img src="../Images/ee912f44f55a8b13909634d1b0d4bcf5.png" data-original-src="https://miro.medium.com/v2/resize:fit:430/format:webp/1*ZU-QQF3xe9yybkdIsWrKBg.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">代码输出(由作者创建)</p></figure><p id="beb6" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">要素“land_size_sqm”具有最高的VIF值。移除此特征后，让我们再次计算VIF值。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="mp mq l"/></div></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nw"><img src="../Images/991382b2e5dcf3a54bc09609f5f1de38.png" data-original-src="https://miro.medium.com/v2/resize:fit:432/format:webp/1*9yTb2e_jGFb_t4_EXT93Rw.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">代码输出(由作者创建)</p></figure><p id="9f4d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">很明显，“land_size_sqm”是唯一必须移除的要素，以解决数据集中的多重共线性问题。</p><h2 id="da80" class="ms lt iq bd lu mt mu dn ly mv mw dp mc lf mx my me lj mz na mg ln nb nc mi nd bi translated">4.基于模型性能选择特征</h2><p id="2ac2" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">也可以让机器学习模型选择特征。</p><p id="bd3f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">一种众所周知的利用模型的特征选择算法被称为递归特征消除(RFE)。</p><p id="4094" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">RFE不同于其他特征选择方法，因为它特别要求应该选择的特征的数量。</p><p id="d88a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">令人欣慰的是，sklearn模块带着它自己的<a class="ae kv" href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFE.html" rel="noopener ugc nofollow" target="_blank"> RFE估计器</a>再次前来救援。</p><p id="c8d9" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在使用RFE进行特征选择之前，需要将数据分成训练集和测试集，并进行归一化处理。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="mp mq l"/></div></figure><p id="f475" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">让我们使用RFE来确定应该选择的10个特征。RFE将使用线性回归模型来选择其要素。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="mp mq l"/></div></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nx"><img src="../Images/df49f771f86c499ab9fe6d72656f0dde.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1-Ah1nS09WhmMCamerZ3rw.png"/></div></div></figure><p id="a844" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">RFE的一个有价值的特征是，它可以显示从考虑因素中移除已消除要素的顺序。这可以通过。排名_ '法。</p><p id="fb65" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">注意:等级为1的特征是选定的特征。任何其他排名都代表被淘汰的功能。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="mp mq l"/></div></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/b110cf498a15fe9ecb3b9a91ad6c5e2f.png" data-original-src="https://miro.medium.com/v2/resize:fit:520/format:webp/1*G9-gs1UJ2E4ZTq0Qwih9FA.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">代码输出(由作者创建)</p></figure><p id="ceac" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">如表中所示，RFE删除了“无房间”、“大客厅”、“无浴室”、“房间大小”和“停车位”功能(按此顺序)。</p><h1 id="b802" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">结论</h1><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nz"><img src="../Images/c6f6e563f5215ed9efc6e99b40e18f21.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*rW4HQDRKjgn_lJTR"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">照片由<a class="ae kv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的<a class="ae kv" href="https://unsplash.com/@prateekkatyal?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Prateek Katyal </a>拍摄</p></figure><p id="071c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在，您已经更加熟悉了用于特征选择的各种方法。</p><p id="5062" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">请记住，特性选择的最佳方法因项目而异。</p><p id="e1ab" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">不要不分青红皂白地应用每一种特性选择方法，而要考虑哪一种最适合您的项目。</p><p id="f35a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我祝你在机器学习的努力中好运！</p><h1 id="4220" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">参考</h1><p id="50b4" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">J.伊萨丁。(2020).Jiffs房价预测数据集，第3版。2021年12月12日检索自<a class="ae kv" href="https://www.kaggle.com/elakiricoder/jiffs-house-price-prediction-dataset" rel="noopener ugc nofollow" target="_blank">https://www . ka ggle . com/elakiricoder/jiffs-house-price-prediction-dataset</a>。</p></div></div>    
</body>
</html>