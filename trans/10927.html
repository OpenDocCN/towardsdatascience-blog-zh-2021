<html>
<head>
<title>BetaBoosting</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">BetaBoosting</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/betaboosting-2cd6c697eb93?source=collection_archive---------8-----------------------#2021-10-24">https://towardsdatascience.com/betaboosting-2cd6c697eb93?source=collection_archive---------8-----------------------#2021-10-24</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="046b" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsdatascience.com/tagged/thoughts-and-theory" rel="noopener" target="_blank">思想和理论</a></h2><div class=""/><div class=""><h2 id="1a8c" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">XGBoost的学习速度非常快</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/559c4491020046ec31e8bc79ba5668a0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Lyrq_Mwca_rGsLrmt0sRVw.jpeg"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated"><a class="ae lh" href="https://unsplash.com/@kyawthutun" rel="noopener ugc nofollow" target="_blank">觉吞</a>在<a class="ae lh" href="https://unsplash.com/" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="5f76" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><strong class="lk jd"> TLDR: </strong>传统上，像XGBoost这样的梯度推进实现使用静态学习率。我们建议使用一个函数，其参数可以传递到一个标准的超参数调整过程，以学习“最佳”的学习率形状。</p><p id="c1bc" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">所有代码都在这里:<a class="ae lh" href="https://github.com/tblume1992/BetaBoost" rel="noopener ugc nofollow" target="_blank"> BetaBoost Github </a></p><h1 id="d603" class="me mf it bd mg mh mi mj mk ml mm mn mo ki mp kj mq kl mr km ms ko mt kp mu mv bi translated">关于升压的简要复习</h1><p id="0543" class="pw-post-body-paragraph li lj it lk b ll mw kd ln lo mx kg lq lr my lt lu lv mz lx ly lz na mb mc md im bi translated">在这一点上，我们都知道<a class="ae lh" href="https://xgboost.ai/" rel="noopener ugc nofollow" target="_blank"> XGBoost </a>，因为它在Kaggle等平台上举办的众多数据科学竞赛中取得了巨大成功。随着它的成功，我们已经看到了一些变化，如<a class="ae lh" href="https://catboost.ai/" rel="noopener ugc nofollow" target="_blank"> CatBoost </a>和<a class="ae lh" href="https://github.com/microsoft/LightGBM" rel="noopener ugc nofollow" target="_blank"> LightGBM </a>。所有这些实现都基于Friedman开发的梯度推进算法，该算法涉及迭代地构建弱学习器的集合(通常是决策树)，其中每个后续学习器都根据前一个学习器的错误进行训练。让我们从统计学习的元素来看一些算法的通用伪代码:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi nb"><img src="../Images/ed6f33502d6d0e70cc6044026f99e02d.png" data-original-src="https://miro.medium.com/v2/resize:fit:868/0*4fegvO9iwbXAzLjG.PNG"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">来自ESL的伪代码(图片由作者提供)</p></figure><p id="cdc8" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">然而，这还不完整！一个核心机制是一个收缩参数，它在每一轮提升中惩罚每个学习者，通常称为“学习率”。它的主要功能是防止过度拟合，它是对梯度下降算法的一个很好的回调，该算法启发了梯度增强的许多方面。为了利用学习率，弗里德曼将2.d改为:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi nc"><img src="../Images/0fe1de350f6f730b9170696c1848c4df.png" data-original-src="https://miro.medium.com/v2/resize:fit:754/0*2L12gkX-4-XV30Rk.PNG"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">作者图片</p></figure><p id="e1ce" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">现在，我们将每一轮的学习者乘以一个常数。但是我们应该用什么常数呢？这是一个常见问题，通常用超参数调整来处理，通常会产生一个常数，如. 1或. 01。一些数字远低于1，或者换句话说，一个严重惩罚每一轮的数字。由于人数少，我们无法最大限度地利用每个学员。</p><h1 id="52d8" class="me mf it bd mg mh mi mj mk ml mm mn mo ki mp kj mq kl mr km ms ko mt kp mu mv bi translated">使用动态学习率</h1><p id="a343" class="pw-post-body-paragraph li lj it lk b ll mw kd ln lo mx kg lq lr my lt lu lv mz lx ly lz na mb mc md im bi translated">当具体查看学习率参数时，下一个自然的问题(尽管通常是不重要的)是为什么使用常数？梯度下降具有允许学习速率改变的动量策略。为什么渐变提升不能用类似的思路？</p><p id="af99" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">嗯，我们可以…</p><p id="89ae" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">…它就是<strong class="lk jd">不起作用</strong><strong class="lk jd">。</strong></p><p id="cd51" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">快速的谷歌搜索显示，有一些工作是利用一个衰减的学习率完成的，这个学习率开始时很大，每一轮都在缩小。但是我们通常在交叉验证中没有看到准确性的提高，并且当查看测试误差图时，它的性能和使用常规的旧常数之间的差异很小。为什么像这样的方法不起作用背后的推理似乎是一个谜。这让我们想到了启发BetaBoost的一个核心主题:</p><p id="ac74" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><strong class="lk jd"> <em class="nd">这方面应该多做研究。</em> </strong></p><h1 id="999f" class="me mf it bd mg mh mi mj mk ml mm mn mo ki mp kj mq kl mr km ms ko mt kp mu mv bi translated">贝塔密度函数</h1><p id="c359" class="pw-post-body-paragraph li lj it lk b ll mw kd ln lo mx kg lq lr my lt lu lv mz lx ly lz na mb mc md im bi translated">为了与我们的主题保持一致，我们进行了研究，并发现了一个似乎很有前途的功能:beta功能(当然还有一些护栏！).或者更具体地说，贝塔概率分布函数。这个函数最初是通过蛮力方法发现的，一旦它提供了令人鼓舞的结果，一些<a class="ae lh" href="https://github.com/tblume1992/portfolio/blob/master/GradientBoostedTrees/3_Dynamic_GBT.ipynb" rel="noopener ugc nofollow" target="_blank">半连贯的基本原理</a>就被挖掘出来了。</p><p id="2515" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">那么，什么是beta PDF呢？顾名思义，这是一个主要由两个参数描述的概率分布:<em class="nd"> α </em>和<em class="nd"> β。</em>除了这些参数之外，还增加了几个参数以及一些更进一步的“护栏”,这些参数似乎会有所帮助。出于我们的目的，我们很大程度上不关心函数作为概率密度函数的任何属性，只关心它与boosting的结果。</p><p id="2837" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">与许多其他功能相比，Beta PDF的一个明显优势是您可以使用它实现大量的形状:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi ne"><img src="../Images/2429eb81f1228abb3a526cc1d0be5a66.png" data-original-src="https://miro.medium.com/v2/resize:fit:650/format:webp/0*4XMQi8MZKxNPJ3zh.png"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">图片来自<a class="ae lh" href="https://en.wikipedia.org/wiki/Beta_distribution" rel="noopener ugc nofollow" target="_blank">维基百科</a></p></figure><p id="becc" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">如果我们想做类似指数衰减的事情，或者在中间产生一个大尖峰，只需要改变一个参数。这意味着如果我们将整个过程交给一个超参数调整包，比如<a class="ae lh" href="https://github.com/hyperopt/hyperopt" rel="noopener ugc nofollow" target="_blank"> Hyperopt </a>，我们可以发现最适合(或过度适合)的学习率形状！)我们的数据。虽然，我们主要看到一个形状出现在顶部。</p><p id="e2f7" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">最终，我不知道为什么这些会有任何好处，所以:</p><p id="3ba1" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><strong class="lk jd"> <em class="nd">在这方面应该做更多的研究。</em> </strong></p><h1 id="3008" class="me mf it bd mg mh mi mj mk ml mm mn mo ki mp kj mq kl mr km ms ko mt kp mu mv bi translated">BetaBoosting</h1><p id="994a" class="pw-post-body-paragraph li lj it lk b ll mw kd ln lo mx kg lq lr my lt lu lv mz lx ly lz na mb mc md im bi translated">这一切把我们带到了BetaBoosting。这不是一个新的方法来建立一棵树或提出分裂。它只是在XGBoost的<a class="ae lh" href="https://xgboost.readthedocs.io/en/latest/python/python_api.html#module-xgboost.training" rel="noopener ugc nofollow" target="_blank">学习API </a>中使用回调在每一轮提升中分配不同的学习速率。我们的具体实现基于Beta PDf分配学习率，因此我们得到了“BetaBoosting”这个名字。</p><p id="3a57" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">该代码可通过pip安装，以便于使用，并要求xgboost==1.5:</p><pre class="ks kt ku kv gt nf ng nh ni aw nj bi"><span id="606d" class="nk mf it ng b gy nl nm l nn no">pip install BetaBoost==0.0.5</span></pre><p id="bc0c" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">如前所述，特定的形状似乎比其他形状做得更好。这个形状在早期有一个很大的尖峰，由于我们的护栏，它很快就变成了一个“地板”参数。让我们看看使用类默认参数的BetaBoost。这个函数将默认参数传递给<a class="ae lh" href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.beta.html" rel="noopener ugc nofollow" target="_blank"> scipy Beta PDF </a>函数，还有一些附加的护栏。下面是一个简单的独立函数和实际的BetaBoost实现:</p><pre class="ks kt ku kv gt nf ng nh ni aw nj bi"><span id="4c7b" class="nk mf it ng b gy nl nm l nn no">def beta_pdf(scalar=1.5,<br/>             a=26,<br/>             b=1,<br/>             scale=80,<br/>             loc=-68,<br/>             floor=0.01,<br/>             n_boosting_rounds=100):<br/>    """<br/>    Get the learning rate from the beta PDF<br/>    Returns<br/>    -------<br/>    lrs : list<br/>        the resulting learning rates to use.<br/>    """<br/>    lrs = [scalar*beta.pdf(i,<br/>                           a=a, <br/>                           b=b, <br/>                           scale=scale, <br/>                           loc=loc) <br/>           + floor for i in range(n_boosting_rounds)]<br/>    return lrs</span></pre><p id="d77d" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">为了便于使用，这里列出了使用BetaBoost类的相同学习率:</p><pre class="ks kt ku kv gt nf ng nh ni aw nj bi"><span id="137c" class="nk mf it ng b gy nl nm l nn no">from BetaBoost import BetaBoost as bb<br/>import matplotlib.pyplot as plt<br/>booster = bb.BetaBoost(n_boosting_rounds=100)<br/>plt.plot(booster.beta_kernel())<br/>plt.show()</span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi np"><img src="../Images/1918f25c6e03c7e1c49c59755dfcda05.png" data-original-src="https://miro.medium.com/v2/resize:fit:744/format:webp/1*NRTSqHITCpjKAKabA1FBpQ.png"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">作者图片</p></figure><p id="bb7d" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">鉴于这种形状做得很好，这表明采取更大的步骤，不是在开始或结束时，而是在被促进的树发育的“青少年时期”提供最大的帮助。</p><p id="a67e" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">为了进一步研究，我们可以用模拟数据运行它，并与其他学习率进行比较。</p><p id="0bb6" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">(下面的例子来自几年前的一个帖子，关于使用一个衰减的学习率，我找不到，如果有人找到了请告诉我，这样我就可以引用这篇文章了！)</p><p id="ed61" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">首先，我们将进行导入并模拟一些数据:</p><pre class="ks kt ku kv gt nf ng nh ni aw nj bi"><span id="4542" class="nk mf it ng b gy nl nm l nn no">import numpy as np<br/>import xgboost as xgb<br/>import matplotlib.pyplot as plt<br/><br/><br/>def generate_data():<br/>    y = np.random.gamma(2, 4, OBS)<br/>    X = np.random.normal(5, 2, [OBS, FEATURES])<br/>    return X, y<br/><br/>max_iter = 300<br/>eta_base = 0.2<br/>eta_min = 0.1<br/>eta_decay = np.linspace(eta_base, eta_min, max_iter).tolist()<br/>OBS = 10 ** 4<br/>FEATURES = 20<br/>PARAMS = {<br/>    'eta': eta_base,<br/>    "booster": "gbtree",<br/>}<br/><br/><br/>X_train, y_train = generate_data()<br/>X_test, y_test = generate_data()<br/>dtrain = xgb.DMatrix(X_train, label=y_train)<br/>dtest = xgb.DMatrix(X_test, label=y_test)<br/>evals_result = {'train': dtrain}</span></pre><p id="31b5" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">接下来，让我们使用衰减学习率，并将我们的结果保存在一个字典中，我们最终将绘制该字典:</p><pre class="ks kt ku kv gt nf ng nh ni aw nj bi"><span id="6b23" class="nk mf it ng b gy nl nm l nn no">progress1 = dict()<br/>model1 = xgb.train(<br/>    maximize=True,<br/>    params=PARAMS,<br/>    dtrain=dtrain,<br/>    num_boost_round=max_iter,<br/>    early_stopping_rounds=max_iter,<br/>    evals=[(dtrain, 'train'),(dtest, 'test')],<br/>    evals_result=progress1,<br/>    verbose_eval=False,<br/>    callbacks=[xgb.callback.LearningRateScheduler(eta_decay)]<br/>)</span></pre><p id="33e7" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">现在，0.01的标准常数:</p><pre class="ks kt ku kv gt nf ng nh ni aw nj bi"><span id="f352" class="nk mf it ng b gy nl nm l nn no">progress2 = dict()<br/>model2 = xgb.train(<br/>    maximize=True,<br/>    params=PARAMS,<br/>    dtrain=dtrain,<br/>    num_boost_round=max_iter,<br/>    early_stopping_rounds=max_iter,<br/>    evals=[(dtrain, 'train'),(dtest, 'test')],<br/>    evals_result=progress2,<br/>    verbose_eval=False,<br/>    callbacks=[xgb.callback.LearningRateScheduler(list(np.ones(max_iter)*0.01))]<br/>)</span></pre><p id="1ef5" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">常数0.1</p><pre class="ks kt ku kv gt nf ng nh ni aw nj bi"><span id="d7ec" class="nk mf it ng b gy nl nm l nn no">progress3 = dict()<br/>model3 = xgb.train(<br/>    maximize=True,<br/>    params=PARAMS,<br/>    dtrain=dtrain,<br/>    num_boost_round=max_iter,<br/>    early_stopping_rounds=max_iter,<br/>    evals=[(dtrain, 'train'),(dtest, 'test')],<br/>    evals_result=progress3,<br/>    verbose_eval=False,<br/>    callbacks=[xgb.callback.LearningRateScheduler(list(np.ones(max_iter)*0.1))]<br/>)</span></pre><p id="e358" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">最后，我们的BetaBoost，其中fit方法只是返回与XGBoost学习API中的train方法相同的输出</p><pre class="ks kt ku kv gt nf ng nh ni aw nj bi"><span id="0119" class="nk mf it ng b gy nl nm l nn no">#Here we call the BetaBoost, the wrapper parameters are passed in the class init<br/>bb_evals = dict()<br/>from BetaBoost import BetaBoost as bb<br/>betabooster = bb.BetaBoost(n_boosting_rounds=max_iter)<br/>betabooster.fit(dtrain=dtrain,<br/>                maximize=True,<br/>                params=PARAMS,<br/>                early_stopping_rounds=max_iter,<br/>                evals=[(dtrain, 'train'),(dtest, 'test')],<br/>                evals_result=bb_evals,<br/>                verbose_eval=False)</span></pre><p id="fec6" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">让我们回顾一下每一轮测试准确性的结果:</p><pre class="ks kt ku kv gt nf ng nh ni aw nj bi"><span id="81e3" class="nk mf it ng b gy nl nm l nn no">plt.plot(progress1['test']['rmse'], linestyle = 'dashed', color = 'b', label = 'eta test decay')<br/>plt.plot(progress2['test']['rmse'], linestyle = 'dashed', color = 'r', label = '0.01 test')<br/>plt.plot(progress3['test']['rmse'], linestyle = 'dashed', color = 'black', label = '0.1 test')<br/>plt.plot(bb_evals['test']['rmse'], linestyle = 'dashed', color = 'y', label = 'bb test')<br/>plt.legend()<br/>plt.show()</span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/b2dac78437ab40296e39a83f3bff4253.png" data-original-src="https://miro.medium.com/v2/resize:fit:746/0*QjnN-LCGzlEPLLTp"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">作者图片</p></figure><p id="883d" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">查看最小测试集误差，我们看到衰减的学习率实际上最快地达到最小值，但这也是一个<em class="nd">不稳定</em>的结果，因为它很快开始过度适应测试集。下一个达到“拐点”的是betaboosted测试集。然而，与衰减测试集不同的是，我们可以看到，随着它的收敛，它的误差继续缓慢减小。最终，在迭代300左右，它遇到了对应于恒定的0.01学习率的错误率。因此，看起来我们在这两个方面都得到了最好的结果:我们非常快地收敛到接近最优的测试精度，然后我们可以抵抗过度拟合。但是，它真的表现最好吗？</p><p id="e41f" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><strong class="lk jd">号</strong></p><p id="86b9" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">在接下来的100次迭代中，静态的0.01略胜一筹。然而，情况并非总是如此，在下一篇文章中，我们将看到一些5倍CV结果，其中优化的BetaBooster实际上在真实世界数据上优于优化的vanilla XGBoost！</p><h1 id="982b" class="me mf it bd mg mh mi mj mk ml mm mn mo ki mp kj mq kl mr km ms ko mt kp mu mv bi translated">摘要</h1><p id="20b9" class="pw-post-body-paragraph li lj it lk b ll mw kd ln lo mx kg lq lr my lt lu lv mz lx ly lz na mb mc md im bi translated">使用β密度函数作为其学习速率的梯度增强似乎给了我们更快的收敛和对过拟合的鲁棒性。这样做的代价是要调整更多的参数。此外，随着学习速度调整某些函数的想法可以很快地添加到主要的增强包中，如XGBoost和LightGBM，使用它们的回调功能。这看起来像是我们目前正在从岩石中榨出水的田地里的一个低垂的果实。</p><p id="883f" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">最终，BetaBoosting回避了这个问题:</p><p id="8793" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">有更好的功能吗？</p><p id="c64d" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我们的回答是…</p><p id="8a65" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><strong class="lk jd"> <em class="nd">这方面应该多做研究。</em> </strong></p><p id="25d2" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">参考</p><ol class=""><li id="b9ad" class="nr ns it lk b ll lm lo lp lr nt lv nu lz nv md nw nx ny nz bi translated">随机梯度推进。(1999年3月)</li><li id="383b" class="nr ns it lk b ll oa lo ob lr oc lv od lz oe md nw nx ny nz bi translated">Hastie，Tibshirani，r .，，j . h . Friedman(2009年)。统计学习的要素:数据挖掘、推理和预测。第二版。纽约:斯普林格。</li><li id="01ee" class="nr ns it lk b ll oa lo ob lr oc lv od lz oe md nw nx ny nz bi translated"><a class="ae lh" href="https://en.wikipedia.org/wiki/Beta_distribution" rel="noopener ugc nofollow" target="_blank">https://en.wikipedia.org/wiki/Beta_distribution</a></li></ol></div></div>    
</body>
</html>