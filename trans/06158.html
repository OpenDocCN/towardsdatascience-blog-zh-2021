<html>
<head>
<title>How to choose between different Boosting Algorithms</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如何在不同的升压算法之间进行选择</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/how-to-select-between-boosting-algorithm-e8d1b15924f7?source=collection_archive---------13-----------------------#2021-06-02">https://towardsdatascience.com/how-to-select-between-boosting-algorithm-e8d1b15924f7?source=collection_archive---------13-----------------------#2021-06-02</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="f942" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">AdaBoost、梯度增强、XGBoost、Light Gbm、CatBoost</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/e68dd8500d76d11220d9d722173bf764.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DmUf9smSja1BOTHOTtIFew.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><a class="ae ky" href="https://unsplash.com/@craftedbygc?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">绿色变色龙</a>在<a class="ae ky" href="https://unsplash.com/s/photos/learning?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="b2c9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">几年前我第一次听说boosting算法。当时我还在学校读本科，使用线性回归/逻辑回归作为机器学习的主要算法。那时候，boosting听起来像是一个非常困难和复杂的算法，这让我害怕！</p><p id="390c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这些年来，我逐渐学会了不同的boosting算法。我意识到这其实并不是一个很难学习和应用的概念。当然，如果你想对算法的数学和编码了如指掌，这将是困难和巨大的努力。但是，如果你只是想用算法，知道什么时候用，我们只需要<strong class="lb iu">知道特点，了解不同boosting算法的区别</strong>。</p><p id="b435" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">本文将重点介绍<strong class="lb iu">不同升压算法的概述</strong>。我将向您展示主要思想、示例代码、如何使用它们以及每种boosting方法的优缺点。然而，我不会深入讨论每一种提升算法的细节。</p><blockquote class="lv lw lx"><p id="17b3" class="kz la ly lb b lc ld ju le lf lg jx lh lz lj lk ll ma ln lo lp mb lr ls lt lu im bi translated">Boosting是一种<strong class="lb iu">顺序</strong>集成学习技术，其中不同的模型被迭代添加，以提高整体模型性能。</p></blockquote><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mc"><img src="../Images/5b6eedd7918ebeb333af2ffe3f5a1028.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0xWny_DiMZxN3Ajpg6G5tg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片:Boosting算法的演变</p></figure><p id="f6ef" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如上图所示，我将按照发布时间的顺序介绍5种提升方法。Light Gbm和CatBoost是在相似的时间推出的，所以我们不能说一个是另一个的改进版本。</p><h1 id="ca3a" class="md me it bd mf mg mh mi mj mk ml mm mn jz mo ka mp kc mq kd mr kf ms kg mt mu bi translated">我们正在使用的数据</h1><p id="e52b" class="pw-post-body-paragraph kz la it lb b lc mv ju le lf mw jx lh li mx lk ll lm my lo lp lq mz ls lt lu im bi translated">这次我将使用来自UCI的<a class="ae ky" href="https://archive.ics.uci.edu/ml/datasets/Adult" rel="noopener ugc nofollow" target="_blank">成人工资预测数据集</a>进行演示。该数据集的主要目标是预测此人的年收入是否超过5万英镑。这是一个分类问题，它包含相当多的分类变量，如教育背景、婚姻状况和职业。</p><p id="5ec1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我将在salary数据集上展示每个boosting算法的示例代码。对于前4种方法，我已经使用<a class="ae ky" href="https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html" rel="noopener ugc nofollow" target="_blank">标签编码</a>将文本列转换成整数。标签编码将文本类别转换为0，1，2，3…我使用这种方法是因为一个热编码(转换为0/1)会创建太多的列。我将使用最后一个方法CatBoost的原始文本列，因为这是算法内置功能的一部分。</p><p id="a154" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">此外，为了查看运行时性能，我将所有算法标准化为运行100次迭代来比较运行时。</p><h2 id="0f9e" class="na me it bd mf nb nc dn mj nd ne dp mn li nf ng mp lm nh ni mr lq nj nk mt nl bi translated">adaboost算法</h2><p id="59df" class="pw-post-body-paragraph kz la it lb b lc mv ju le lf mw jx lh li mx lk ll lm my lo lp lq mz ls lt lu im bi translated">AdaBoost是自适应增强的简称。我们称之为自适应，因为该算法利用了来自先前模型的<strong class="lb iu">未分类案例，并创建了一个<strong class="lb iu">新的加权数据样本</strong>，其中未分类案例具有更大的权重。这样，新添加的模型在解决先前模型的“错误”时应该更具适应性。</strong></p><p id="9eb7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">AdaBoost是最早开发的boosting算法之一。现在很少使用它，但它是大多数boosting算法的基础。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nm"><img src="../Images/22a65c782bcb84f3bfa19e36acfe89bd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zjRK20spnClU9CWtpX-1eg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">由西拉科恩——自己的作品，CC BY-SA 4.0，<a class="ae ky" href="https://commons.wikimedia.org/w/index.php?curid=85888769" rel="noopener ugc nofollow" target="_blank">https://commons.wikimedia.org/w/index.php?curid=85888769</a></p></figure><pre class="kj kk kl km gt nn no np nq aw nr bi"><span id="88c9" class="na me it no b gy ns nt l nu nv">from sklearn.ensemble import AdaBoostClassifier<br/>clf = AdaBoostClassifier(n_estimators=100,random_state=1)<br/>clf.fit(X_train, Y_train)<br/>predicted=clf.predict(X_test)<br/>print('Accuracy of the result is:')<br/>print(np.mean(predicted==Y_test))</span><span id="9a92" class="na me it no b gy nw nt l nu nv">#Accuracy: 0.8659023441498618<br/>#Runtime: 0.9506289958953857</span></pre><h2 id="b5fb" class="na me it bd mf nb nc dn mj nd ne dp mn li nf ng mp lm nh ni mr lq nj nk mt nl bi translated">梯度推进</h2><p id="c464" class="pw-post-body-paragraph kz la it lb b lc mv ju le lf mw jx lh li mx lk ll lm my lo lp lq mz ls lt lu im bi translated">与AdaBoost相比，梯度提升<strong class="lb iu">不惩罚漏分类情况，而是使用损失函数</strong>。损失函数可以是回归的平均误差或分类问题的对数损失。此外，梯度提升算法使用<strong class="lb iu">梯度下降法来不断最小化损失函数，以找到最优点</strong>。</p><p id="e745" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">梯度提升方法理论上比AdaBoost执行得更好。然而，它更容易出现过拟合问题，并且梯度增强的运行时间更长。可以设置早期停止条件，以减轻过拟合并减少运行时间。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/6fbc8e00dd723fa759593b2b68e236b0.png" data-original-src="https://miro.medium.com/v2/resize:fit:994/format:webp/1*NKNzHZebRQ3_OyCQyU0ohg.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">通过使用R的<a class="ae ky" href="https://bradleyboehmke.github.io/HOML/gbm.html" rel="noopener ugc nofollow" target="_blank">动手机器学习进行梯度提升</a></p></figure><pre class="kj kk kl km gt nn no np nq aw nr bi"><span id="0b86" class="na me it no b gy ns nt l nu nv">from sklearn.ensemble import GradientBoostingClassifier<br/>clf = GradientBoostingClassifier(n_estimators=100,random_state=0)<br/>clf.fit(X_train, Y_train)<br/>predicted=clf.predict(X_test)<br/>print('Accuracy of the result is:')<br/>print(np.mean(predicted==Y_test))</span><span id="980f" class="na me it no b gy nw nt l nu nv">#Accuracy: 0.8646739686764254<br/>#Runtime: 1.6930928230285645</span></pre><h2 id="20a6" class="na me it bd mf nb nc dn mj nd ne dp mn li nf ng mp lm nh ni mr lq nj nk mt nl bi translated">XGBoost</h2><p id="e730" class="pw-post-body-paragraph kz la it lb b lc mv ju le lf mw jx lh li mx lk ll lm my lo lp lq mz ls lt lu im bi translated">XGBoost代表极限梯度提升，它指的是工程师们推动梯度提升方法的计算资源极限的目标。</p><p id="5298" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">XGBoost是梯度增强方法的增强版本。首先，它<strong class="lb iu">通过使用正则化</strong>改进过拟合。其次，通过使用并行运行优化排序<strong class="lb iu">，提高了运行速度。最后，使用决策树的<strong class="lb iu">最大深度作为参数</strong>来修剪该树，这显著减少了运行时间。</strong></p><pre class="kj kk kl km gt nn no np nq aw nr bi"><span id="2e6d" class="na me it no b gy ns nt l nu nv">from xgboost import XGBClassifier<br/>clf = XGBClassifier(n_estimators=100,random_state=0)<br/>clf.fit(X_train, Y_train)<br/>predicted=clf.predict(X_test)<br/>print('Accuracy of the result is:')<br/>print(np.mean(predicted==Y_test))</span><span id="f4b4" class="na me it no b gy nw nt l nu nv">#Accuracy: 0.8710205752891801<br/>#Runtime: 0.7643740177154541</span></pre><h2 id="c0a6" class="na me it bd mf nb nc dn mj nd ne dp mn li nf ng mp lm nh ni mr lq nj nk mt nl bi translated">轻型GBM</h2><p id="bb70" class="pw-post-body-paragraph kz la it lb b lc mv ju le lf mw jx lh li mx lk ll lm my lo lp lq mz ls lt lu im bi translated">顾名思义，<a class="ae ky" href="https://lightgbm.readthedocs.io/en/latest/Features.html" rel="noopener ugc nofollow" target="_blank"> Light Gbm </a>进一步<strong class="lb iu">通过让计算工作量‘轻’来提高程序的运行时间</strong>。但是，与其他算法相比，它仍然可以保持相同或更高的模型性能。</p><p id="b32d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">Light Gbm主要通过两种方式优化运行速度和准确性。</p><ol class=""><li id="4d73" class="ny nz it lb b lc ld lf lg li oa lm ob lq oc lu od oe of og bi translated">它采用基于直方图的算法，<strong class="lb iu">将连续变量拆分到不同的桶中</strong>(而不是单独排序)。这大大提高了运行时间。</li><li id="b005" class="ny nz it lb b lc oh lf oi li oj lm ok lq ol lu od oe of og bi translated">它使用<strong class="lb iu">逐叶树生长方法，而不是逐层树生长方法</strong>(被大多数其他基于决策树的方法使用)。从下面的图片可以看出，它允许损失较高的叶片部分继续生长(<strong class="lb iu">最佳拟合切割</strong>)，从而将整体损失函数降至最低。</li></ol><div class="kj kk kl km gt ab cb"><figure class="om kn on oo op oq or paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><img src="../Images/90bea121a4bee8e7720ccf723863202b.png" data-original-src="https://miro.medium.com/v2/resize:fit:936/format:webp/1*CATHENc4SLqh49eaA3mV0A.png"/></div></figure><figure class="om kn os oo op oq or paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><img src="../Images/bc1e612cf9aff104d942fd3822dbfb68.png" data-original-src="https://miro.medium.com/v2/resize:fit:1066/format:webp/1*eN8B4P21vYiS-99DlUBQfw.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk ot di ou ov translated">来自<a class="ae ky" href="https://lightgbm.readthedocs.io/en/latest/Features.html" rel="noopener ugc nofollow" target="_blank">轻型gbm文档</a>的水平方向树生长与叶方向树生长</p></figure></div><pre class="kj kk kl km gt nn no np nq aw nr bi"><span id="fa2c" class="na me it no b gy ns nt l nu nv">import lightgbm as lgb<br/>clf = lgb.LGBMClassifier()<br/>clf.fit(X=X_train, y=Y_train,feature_name=list(X_train.columns),categorical_feature=list(df_cat.columns))<br/>predicted=clf.predict(X_test)<br/>print('Accuracy of the result is:')<br/>print(np.mean(predicted==Y_test))</span><span id="bcdb" class="na me it no b gy nw nt l nu nv">#Accuracy: 0.871839492271471<br/>#Runtime: 0.15074729919433594</span></pre><p id="1cc2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">使用Light Gbm的另一个好处是，它有100多个参数可以根据数据集和问题进行调优和调整。这里一个非常有用的特性是，您可以在fit函数中定义分类特性列表(您需要先将类别转换为整数)。</p><h2 id="25ff" class="na me it bd mf nb nc dn mj nd ne dp mn li nf ng mp lm nh ni mr lq nj nk mt nl bi translated">CatBoost</h2><p id="1b4f" class="pw-post-body-paragraph kz la it lb b lc mv ju le lf mw jx lh li mx lk ll lm my lo lp lq mz ls lt lu im bi translated">最后，CatBoost代表分类推进。它有一个很大的特点就是<strong class="lb iu">自动处理分类变量，而不需要将它们转换成数字</strong>。</p><p id="1676" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">CatBoost是5种boosting算法中最新开发的，但非常接近轻型Gbm。分类变量越多，性能越好。</p><pre class="kj kk kl km gt nn no np nq aw nr bi"><span id="15a3" class="na me it no b gy ns nt l nu nv">from catboost import CatBoostClassifier<br/>clf=CatBoostClassifier(iterations=100)<br/>clf.fit(X_train, Y_train,cat_features=list(df_cat.columns))<br/>predicted=clf.predict(X_test)<br/>print('Accuracy of the result is:')<br/>print(np.mean(predicted==Y_test))</span><span id="8192" class="na me it no b gy nw nt l nu nv">#Accuracy: 0.8731702323676938<br/>#Runtime: 0.5496680736541748</span></pre><h1 id="42a3" class="md me it bd mf mg mh mi mj mk ml mm mn jz mo ka mp kc mq kd mr kf ms kg mt mu bi translated">整体性能比较</h1><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ow"><img src="../Images/441fa5eac82707dcc158525c2abe2161.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2dudlczo75DLl1zqHEel6w.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">结果对照表</p></figure><p id="6a64" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">不同的增强方法在准确性方面的表现相当一致。CatBoost具有最高的精度，但差异领先是最小的。就运行时性能而言，Light Gbm具有最快的执行速度和最高的准确率。</p><p id="efa7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">梯度增强在运行时间方面表现不佳，并且它具有最低的精度(略低于AdaBoost)。从这次演习来看，轻型Gbm似乎是冠军。</p><h1 id="4f77" class="md me it bd mf mg mh mi mj mk ml mm mn jz mo ka mp kc mq kd mr kf ms kg mt mu bi translated">建议</h1><p id="8bc9" class="pw-post-body-paragraph kz la it lb b lc mv ju le lf mw jx lh li mx lk ll lm my lo lp lq mz ls lt lu im bi translated">就个人而言，我会<strong class="lb iu">推荐使用轻型GBM和CatBoost </strong>，因为它们的<strong class="lb iu">性能/速度优势</strong>以及<strong class="lb iu">大量用于模型调整的参数</strong>。</p><p id="889c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">当预测器中有许多分类变量时，CatBoost表现更好。分类变量包括文本、音频和图像。它还为您节省了转换分类变量的工作量。</p><p id="91a4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">轻型GBM的模型训练速度比CatBoost快。同时也不牺牲精度水平。当您有一个大型数据集和相对较少的分类预测器时，您可以选择使用LightGBM</p><h1 id="9774" class="md me it bd mf mg mh mi mj mk ml mm mn jz mo ka mp kc mq kd mr kf ms kg mt mu bi translated">潜在的未来工作</h1><ol class=""><li id="566b" class="ny nz it lb b lc mv lf mw li ox lm oy lq oz lu od oe of og bi translated">比较是在30k行数据上进行的。数据集可以有偏差，大小也不是很大。您可以通过使用<a class="ae ky" href="https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_classification.html" rel="noopener ugc nofollow" target="_blank"> make_classification </a>包来尝试更大的分类数据集</li><li id="18ce" class="ny nz it lb b lc oh lf oi li oj lm ok lq ol lu od oe of og bi translated">这个练习只是为了分类，你也可以尝试回归的一面</li><li id="7786" class="ny nz it lb b lc oh lf oi li oj lm ok lq ol lu od oe of og bi translated">在评估模型性能方面，仅使用准确性和运行时间，<strong class="lb iu">其他分类评估如ROC评分、F1评分、召回率、精确度也可用于查看全貌</strong>。</li><li id="41f7" class="ny nz it lb b lc oh lf oi li oj lm ok lq ol lu od oe of og bi translated">建模最耗时的部分是超参数调整。当前的练习或比较不包括超参数调整部分。我会有一篇新的文章，请继续关注。</li></ol><h1 id="f5d3" class="md me it bd mf mg mh mi mj mk ml mm mn jz mo ka mp kc mq kd mr kf ms kg mt mu bi translated">最后</h1><p id="8f21" class="pw-post-body-paragraph kz la it lb b lc mv ju le lf mw jx lh li mx lk ll lm my lo lp lq mz ls lt lu im bi translated">这是对不同类型的升压方法的非常基本的概述。你可以阅读每种方法的更多内容，也许可以在几个数据集上尝试模型以进一步理解它。</p><p id="68a8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">另外，boosting只是集成学习算法中的一种。如果你有兴趣了解更多关于ensembled learning的内容，可以参考我下面的文章:</p><div class="pa pb gp gr pc pd"><a rel="noopener follow" target="_blank" href="/overview-of-ensemble-learning-c216a9f6c04"><div class="pe ab fo"><div class="pf ab pg cl cj ph"><h2 class="bd iu gy z fp pi fr fs pj fu fw is bi translated">什么是集成学习</h2><div class="pk l"><h3 class="bd b gy z fp pi fr fs pj fu fw dk translated">简单的集合方法，装袋，引导，助推</h3></div><div class="pl l"><p class="bd b dl z fp pi fr fs pj fu fw dk translated">towardsdatascience.com</p></div></div><div class="pm l"><div class="pn l po pp pq pm pr ks pd"/></div></div></a></div><p id="99c0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">感谢阅读！希望这篇文章能帮到你。</p><p id="4768" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果你对你想让我介绍的某个特定的助推方法或主题感兴趣，请在评论中提出来！</p></div></div>    
</body>
</html>