<html>
<head>
<title>Distillation of BERT-Like Models: The Theory</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">类伯特模型的提炼:理论</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/distillation-of-bert-like-models-the-theory-32e19a02641f?source=collection_archive---------2-----------------------#2021-12-10">https://towardsdatascience.com/distillation-of-bert-like-models-the-theory-32e19a02641f?source=collection_archive---------2-----------------------#2021-12-10</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="347e" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsdatascience.com/tagged/thoughts-and-theory" rel="noopener" target="_blank">思想和理论</a></h2><div class=""/><div class=""><h2 id="321f" class="pw-subtitle-paragraph jw iz iq bd b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn dk translated">探索蒸馏方法背后的机制</h2></div><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ko"><img src="../Images/a82af93efcb5004981516034ca858db1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*v-O4GGBJiCf1PAtgcV4f4w.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">类伯特模型的提取过程。图片作者。</p></figure><p id="4fb8" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">如果你曾经训练过像BERT或RoBERTa这样的大型NLP模型，你就会知道这个过程是极其漫长的。训练这样的模型可能要拖上好几天，因为它们体积庞大。当需要在小型设备上运行它们时，你可能会发现，你正在为今天不断提高的性能付出巨大的<strong class="lg ja">内存和时间</strong>成本。</p><p id="a01d" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">幸运的是，有一些方法可以减轻这些痛苦，而对你的模特的表现几乎没有影响，这种技术叫做<strong class="lg ja">蒸馏</strong>。在本文中，我们将探索DistilBERT方法[1]背后的机制，它可用于提取任何类似BERT的模型。</p><p id="cc01" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">首先，我们将讨论蒸馏的一般情况，以及为什么我们选择DistilBERT的方法，然后如何初始化该过程，以及蒸馏过程中使用的特殊损耗，最后是一些相关的额外细节，可以单独提及。</p></div><div class="ab cl ma mb hu mc" role="separator"><span class="md bw bk me mf mg"/><span class="md bw bk me mf mg"/><span class="md bw bk me mf"/></div><div class="ij ik il im in"><h1 id="ee80" class="mh mi iq bd mj mk ml mm mn mo mp mq mr kf ms kg mt ki mu kj mv kl mw km mx my bi translated">摘要</h1><p id="df1c" class="pw-post-body-paragraph le lf iq lg b lh mz ka lj lk na kd lm ln nb lp lq lr nc lt lu lv nd lx ly lz ij bi translated">一、DistilBERT <br/>二简介。抄袭老师的架构<br/>三。蒸馏损失<br/> IV。更多详情<br/>五.结论</p></div><div class="ab cl ma mb hu mc" role="separator"><span class="md bw bk me mf mg"/><span class="md bw bk me mf mg"/><span class="md bw bk me mf"/></div><div class="ij ik il im in"><h1 id="57ec" class="mh mi iq bd mj mk ml mm mn mo mp mq mr kf ms kg mt ki mu kj mv kl mw km mx my bi translated">一、蒸馏器简介</h1><p id="a703" class="pw-post-body-paragraph le lf iq lg b lh mz ka lj lk na kd lm ln nb lp lq lr nc lt lu lv nd lx ly lz ij bi translated"><strong class="lg ja">什么是蒸馏？<br/> </strong>蒸馏的概念相当直观:它是<strong class="lg ja">训练一个小的学生模型尽可能地模仿一个更大的教师模型</strong>的过程。如果我们只在用于微调的集群上运行机器学习模型，蒸馏将毫无用处，但遗憾的是，事实并非如此。因此，每当我们想要将一个模型移植到更小的硬件上时，比如有限的笔记本电脑或手机，蒸馏就派上了用场，因为蒸馏模型运行更快，占用的空间更少。</p><p id="078d" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated"><strong class="lg ja">BERT蒸馏的必要性<br/> </strong>正如你可能已经注意到的，<strong class="lg ja">基于BERT的模型</strong>在NLP中风靡一时，自从它们在[2]中被首次引入以来。随着性能的提高，出现了许许多多的参数。准确地说，伯特的收入超过了1.1亿英镑，我们甚至还没有谈论伯特-拉奇。因此，蒸馏的需要是显而易见的，因为伯特是如此的多才多艺和出色。此外，随后的模型基本上是以相同的方式构建的，类似于RoBERTa [3]，所以通过学习适当地提取BERT，你可以一举两得。</p><p id="420f" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated"><strong class="lg ja">蒸馏伯特的方法<br/> </strong>第一篇关于伯特蒸馏的论文是我们要用来作为灵感的，即[1]。但其他人紧随其后，像[4]或[5]，所以很自然地想知道为什么我们把自己局限于蒸馏。答案有三点:第一，它相当简单，所以是很好的蒸馏入门；二是导致好的结果；第三，它还允许对基于BERT的模型进行提炼。</p><p id="0521" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">DistilBERT的蒸馏有两个步骤，我们将在下面详述。</p></div><div class="ab cl ma mb hu mc" role="separator"><span class="md bw bk me mf mg"/><span class="md bw bk me mf mg"/><span class="md bw bk me mf"/></div><div class="ij ik il im in"><h1 id="e218" class="mh mi iq bd mj mk ml mm mn mo mp mq mr kf ms kg mt ki mu kj mv kl mw km mx my bi translated">二。抄袭老师的建筑</h1><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ne"><img src="../Images/07a21f90e2e40265aef674d433287f9b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WJXZZhJhj6Ff7IRLwFOEaA.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">伯特的建筑。图片作者。</p></figure><p id="bfb6" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">伯特的理论主要是基于一系列相互叠加的<strong class="lg ja">注意力层</strong>。因此，这意味着伯特学到的“隐藏知识”就包含在那些层中。我们不会关心这些是如何工作的，但是对于那些想要更多细节的人来说，除了原始论文[1]，我可以推荐这篇TDS文章，它做得非常好[6]。现在，我们可以把注意力层当作一个黑盒，这对我们来说并不重要。</p><p id="c3a9" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">从一个BERT到另一个BERT，层数N是变化的，但是当然模型的大小与N成比例。因此，训练模型所花费的时间和正向传递的持续时间也取决于N，以及存储模型所花费的内存。因此提取BERT的逻辑结论是<strong class="lg ja">减少N </strong>。</p><p id="908e" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">DistilBERT的方法是将<strong class="lg ja">的层数减半</strong>，并从老师的开始初始化学生的层数。简单而高效:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi nf"><img src="../Images/a35072502e9f172461b4bf8eb79f3c19.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9kE4oa0M3io2hd58LiwYgw.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">学生模型初始化。图片作者。</p></figure><p id="a50c" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">DistilBERT在一个复制层和一个忽略层之间交替，根据[4]这似乎是最好的启发式，它尝试优先复制顶层或底层。</p><p id="1c06" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">多亏了huggingface的变形金刚模块和对其内部工作原理的一点了解，这可以很容易地实现。我们将在另一篇文章中展示如何做到这一点，因为在这篇文章中我们将仅限于理论。</p><p id="25d9" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">当然，如果你正在为一个特定的任务使用一个基于BERT的模型，比方说序列分类，那么你还需要为学生复制老师的头，但是一般来说，BERT头的大小与其注意力层的大小相比就相形见绌了。</p><p id="d9fd" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">我们现在有了一个学生模型，可以开始教学了。然而，提炼过程并不是一个经典的拟合程序:我们不是像通常那样教学生学习一种模式，我们的目标也是模仿老师。因此，我们必须调整我们的训练程序，尤其是我们的损失函数。</p></div><div class="ab cl ma mb hu mc" role="separator"><span class="md bw bk me mf mg"/><span class="md bw bk me mf mg"/><span class="md bw bk me mf"/></div><div class="ij ik il im in"><h1 id="a4c3" class="mh mi iq bd mj mk ml mm mn mo mp mq mr kf ms kg mt ki mu kj mv kl mw km mx my bi translated">三。蒸馏损失</h1><p id="65b6" class="pw-post-body-paragraph le lf iq lg b lh mz ka lj lk na kd lm ln nb lp lq lr nc lt lu lv nd lx ly lz ij bi translated">本文顶部的图片展示了蒸馏程序。<br/>我们的训练程序将基于损失，如前所述，这寻求实现几个目标:<strong class="lg ja">最小化</strong>老师训练的经典损失函数和<strong class="lg ja">模仿</strong>老师本身。更糟糕的是，模仿老师需要混合两种损失函数。因此，我们将从更简单的目标开始:最小化传统损失。</p><h2 id="87c3" class="ng mi iq bd mj nh ni dn mn nj nk dp mr ln nl nm mt lr nn no mv lv np nq mx iw bi translated"><strong class="ak">经典损失</strong></h2><p id="778d" class="pw-post-body-paragraph le lf iq lg b lh mz ka lj lk na kd lm ln nb lp lq lr nc lt lu lv nd lx ly lz ij bi translated">关于这一部分没有太多要说的:类似BERT的模型都以同样的方式工作，一个核心输出一个嵌入到特定问题的头部。教师被微调的任务有它自己的损失函数。为了计算这个损失，因为这个模型是由注意力层组成的，和老师有着相同的特定问题的头脑，我们只需要插入学生的嵌入和标签。</p><h2 id="2004" class="ng mi iq bd mj nh ni dn mn nj nk dp mr ln nl nm mt lr nn no mv lv np nq mx iw bi translated"><strong class="ak">师生交叉熵损失</strong></h2><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi nr"><img src="../Images/08176700c23cba84a2d248c09cda0399.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GlUANi9Naz_RrA7-dxVWGQ.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">交叉熵损失对两个三维矢量的影响。图片作者。</p></figure><p id="9a97" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">这是第一次损失，旨在减少学生和教师概率分布之间的差距。当类BERT模型正向传递输入时，无论是用于屏蔽语言建模、标记分类、序列分类等，它都会输出逻辑，然后通过softmax层转换为概率分布。</p><p id="af39" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">对于输入x，教师输出:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ns"><img src="../Images/5b76d2a9085867b1ea95f15e0f674862.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*QGOWF6DOlWBgvpvk.jpg"/></div></div></figure><p id="989f" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">学生的输出是:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi nt"><img src="../Images/1b81eca9242c9899671fe7f531754d11.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*YyTUae2qmBARKn_t"/></div></div></figure><p id="d3f3" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">请记住softmax和它附带的符号，我们稍后将回到它。无论如何，如果我们希望T和S接近，我们可以以T为目标对S应用交叉熵损失。这就是我们所说的师生交叉熵损失:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi nu"><img src="../Images/38591fb2ab3d5cffd3092378f29b6c6b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*e7WSiUgFe_O_IhZn.jpg"/></div></div></figure><h2 id="f42e" class="ng mi iq bd mj nh ni dn mn nj nk dp mr ln nl nm mt lr nn no mv lv np nq mx iw bi translated"><strong class="ak">师生余弦损失</strong></h2><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi nv"><img src="../Images/10c679124d8c9fb19de02adfc84dbcd7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*t2U6Bds-MKvDgfDPNrKYSA.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">余弦损失对两个三维向量的影响。图片作者。</p></figure><p id="86dd" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">帮助学生成为大师的第二个损失是余弦损失。余弦损失是有趣的，因为它不是试图使向量x等于目标y，而是试图将x与y对齐，而不考虑它们各自的范数或空间原点。我们使用这种损失，以便教师和学生模型中的隐藏向量对齐。使用与之前相同的符号:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi nw"><img src="../Images/247ae0f9c99ee4e9eef25af9ec5dc24b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*h4jLc3rcJdhmERz-"/></div></div></figure><p id="11bf" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">实际上，余弦损失有两种版本，一种是对齐向量，一种是将一个向量拉向另一个向量的相反方向。在本文中，我们只对第一个感兴趣。</p><h2 id="a013" class="ng mi iq bd mj nh ni dn mn nj nk dp mr ln nl nm mt lr nn no mv lv np nq mx iw bi translated"><strong class="ak">完全蒸馏损失</strong></h2><p id="5af1" class="pw-post-body-paragraph le lf iq lg b lh mz ka lj lk na kd lm ln nb lp lq lr nc lt lu lv nd lx ly lz ij bi translated">完全蒸馏损失是上述三种损失的组合:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi nx"><img src="../Images/2380031088c91fb6c8c7449d621fd61d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*fCbzp1Q3trNK4pmp"/></div></div></figure></div><div class="ab cl ma mb hu mc" role="separator"><span class="md bw bk me mf mg"/><span class="md bw bk me mf mg"/><span class="md bw bk me mf"/></div><div class="ij ik il im in"><h1 id="0922" class="mh mi iq bd mj mk ml mm mn mo mp mq mr kf ms kg mt ki mu kj mv kl mw km mx my bi translated">四。其他详细信息</h1><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ny"><img src="../Images/51d1bb838df64b63b1ab5fa19777f09c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oRqw7kWld7qvdkkQjvsyzQ.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">softmax温度对概率分布的影响。图片作者。</p></figure><h2 id="0854" class="ng mi iq bd mj nh ni dn mn nj nk dp mr ln nl nm mt lr nn no mv lv np nq mx iw bi translated"><strong class="ak">蒸馏程序</strong></h2><p id="a552" class="pw-post-body-paragraph le lf iq lg b lh mz ka lj lk na kd lm ln nb lp lq lr nc lt lu lv nd lx ly lz ij bi translated">解释了损失之后，剩下的蒸馏程序就相当简单了。模型训练与其他模型非常相似，唯一不同的是你必须并行运行两个类似BERT的模型。感谢您的GPU的健康和内存，教师模型不需要梯度，因为反向传播只在学生身上进行。当然，实现损失仍然需要做，就像蒸馏过程一样，但是我们将在以后的文章中讨论它。</p><h2 id="2000" class="ng mi iq bd mj nh ni dn mn nj nk dp mr ln nl nm mt lr nn no mv lv np nq mx iw bi translated"><strong class="ak">温度</strong></h2><p id="6c9a" class="pw-post-body-paragraph le lf iq lg b lh mz ka lj lk na kd lm ln nb lp lq lr nc lt lu lv nd lx ly lz ij bi translated">如前所述，让我们回到《T2》三中使用的符号。师生交叉熵损失:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi oa"><img src="../Images/d3c968ed6856f81a21881a9fa5bca910.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*iGZwCqmPc7yyaFDK"/></div></div></figure><p id="ecea" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">DistilBERT使用[7]中的温度概念，这有助于软化softmax。温度是一个θ ≥ 1的变量，当温度上升时会降低softmax的“置信度”。正常的softmax描述如下:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ob"><img src="../Images/982397b9df1c027c151f20c7902902ae.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*EoYxuINLa4ypbmxk.jpg"/></div></div></figure><p id="8e1a" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">现在，让我们无用地把它重写为:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi oc"><img src="../Images/bc5c2e74353a8581e1379cd9ef264d69.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*rTwM4OQlsopXuQCX.jpg"/></div></div></figure><p id="93d5" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">每个人都会同意这是正确的。1实际上对应于温度θ。正常的软最大值是其温度设置为1的软最大值，具有常规温度的软最大值的公式为:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi od"><img src="../Images/f034e400ec3423b98e353b2ce58934b7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*cuIbCjMm0LnXwXye.jpg"/></div></div></figure><p id="a17c" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">随着θ上升，θ上的商变为零，因此整个商变为1/n，并且softmax概率分布变为均匀分布。这可以在上图中观察到。</p><p id="50c5" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">在DistilBERT中，学生和老师的softmax在训练时都以相同的温度θ为条件，推理时温度设置为1。</p></div><div class="ab cl ma mb hu mc" role="separator"><span class="md bw bk me mf mg"/><span class="md bw bk me mf mg"/><span class="md bw bk me mf"/></div><div class="ij ik il im in"><h1 id="caaf" class="mh mi iq bd mj mk ml mm mn mo mp mq mr kf ms kg mt ki mu kj mv kl mw km mx my bi translated">动词 （verb的缩写）结论</h1><p id="8acb" class="pw-post-body-paragraph le lf iq lg b lh mz ka lj lk na kd lm ln nb lp lq lr nc lt lu lv nd lx ly lz ij bi translated">现在您已经知道了类似BERT模型的提取是如何为DistilBERT工作的，唯一要做的就是选择一个模型并提取它！<br/>显然，您仍然需要实现蒸馏过程，但是我们将很快介绍如何实现。</p><h2 id="5439" class="ng mi iq bd mj nh ni dn mn nj nk dp mr ln nl nm mt lr nn no mv lv np nq mx iw bi translated"><strong class="ak">参考文献</strong></h2><p id="84ef" class="pw-post-body-paragraph le lf iq lg b lh mz ka lj lk na kd lm ln nb lp lq lr nc lt lu lv nd lx ly lz ij bi translated">[1]维克多·桑，弗拉达利出道，朱利安·肖蒙德，托马斯·沃尔夫，<a class="ae oe" href="https://arxiv.org/pdf/1910.01108.pdf" rel="noopener ugc nofollow" target="_blank">蒸馏伯特，伯特的蒸馏版:更小，更快，更便宜，更轻</a> (2019)，拥抱脸</p><p id="c3e4" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">[2]雅各布·德夫林(Jacob Devlin)，张明蔚(Ming-Wei Chang)，肯顿·李(Kenton Lee)，克里斯蒂娜·图坦诺娃(Kristina Toutanova)，<a class="ae oe" href="https://arxiv.org/pdf/1810.04805.pdf" rel="noopener ugc nofollow" target="_blank">伯特:语言理解的深度双向转换器的预训练</a> (2018)，谷歌AI语言</p><p id="efdb" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">[3]刘、米莱奥特、纳曼戈亚尔、杜、曼达尔乔希、陈、奥梅尔列维、、卢克塞特勒莫耶、韦塞林斯托扬诺夫、<a class="ae oe" href="https://arxiv.org/pdf/1907.11692.pdf" rel="noopener ugc nofollow" target="_blank">罗伯塔:稳健优化的伯特预训练方法</a> (2019)、arXiv</p><p id="2b15" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">[4]，焦，尹宜春，，尚，，，，李，，，，，<a class="ae oe" href="https://arxiv.org/pdf/1909.10351.pdf" rel="noopener ugc nofollow" target="_blank"> TinyBERT:为自然语言理解提取BERT</a>(2019)，arXiv</p><p id="e70e" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">[5]孙志清，俞鸿坤，，宋，，刘，，周，<a class="ae oe" href="https://arxiv.org/pdf/2004.02984.pdf" rel="noopener ugc nofollow" target="_blank"> MobileBERT:一种面向资源受限设备的紧凑任务不可知BERT</a>(2020)，arXiv</p><p id="1c42" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">[6]莱米·卡里姆，<a class="ae oe" rel="noopener" target="_blank" href="/illustrated-self-attention-2d627e33b20a">插图:自我关注</a> (2019)，走向数据科学</p><p id="fb49" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">[7] Geoffrey Hinton，Oriol Vinyals，Jeff Dean，<a class="ae oe" href="https://arxiv.org/pdf/1503.02531.pdf" rel="noopener ugc nofollow" target="_blank">在神经网络中提取知识</a> (2015)，arXiv</p></div></div>    
</body>
</html>