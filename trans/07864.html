<html>
<head>
<title>Boruta and SHAP for better Feature Selection</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">博鲁塔和SHAP为更好的功能选择</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/boruta-and-shap-for-better-feature-selection-20ea97595f4a?source=collection_archive---------9-----------------------#2021-07-19">https://towardsdatascience.com/boruta-and-shap-for-better-feature-selection-20ea97595f4a?source=collection_archive---------9-----------------------#2021-07-19</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="0a28" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">不同特征选择技术的比较及其选择</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/25eb2d38131c27eb51792cc05bfb9034.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*Ci2s3prBzYttxRQA"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">克里斯汀·塔博瑞在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="0dbc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">当我们执行监督任务时，我们面临的问题是在我们的机器学习管道中加入适当的特征选择。简单地在网上搜索，我们可以访问各种各样的关于特征选择过程的资源和内容。</p><p id="132f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">总之，有不同的方法来执行特征选择。文献中最著名的是基于<strong class="lb iu">过滤器的</strong>和基于<strong class="lb iu">包装器的</strong>技术。在基于过滤器的程序中，非监督算法或统计用于查询最重要的预测值。在基于包装器的方法中，监督学习算法被迭代拟合以排除不太重要的特征。</p><p id="e8c6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">通常，基于包装器的方法是最有效的，因为它们可以提取特征之间的相关性和依赖性。另一方面，他们显示更容易过度适应。为了避免这种缺乏并利用基于包装器的技术的优点，我们需要做的就是采用一些简单而强大的技巧。我们可以通过一点数据理解和一个<em class="lv">秘密成分</em>来实现更好的特征选择。别担心，我们不用黑魔法而是用<strong class="lb iu"> SHAP的力量</strong>。</p><p id="052c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为了在特性选择过程中更好地利用SHAP的能力，我们发布了<a class="ae ky" href="https://github.com/cerlymarco/shap-hypetune" rel="noopener ugc nofollow" target="_blank"><strong class="lb iu">shap-hype tune</strong></a>:<em class="lv">一个python包，用于同时进行超参数调整和特性选择。</em>它允许在为梯度推进模型定制的单个管道中组合特征选择和参数调整。它支持网格搜索或随机搜索，并提供基于包装器的<strong class="lb iu"> </strong>特征选择算法，如<strong class="lb iu">递归特征消除(RFE)、递归特征添加(RFA)、</strong>或<strong class="lb iu"> Boruta </strong>。进一步的添加包括使用SHAP重要性进行特征选择，而不是传统的基于原生树的特征重要性。</p><p id="c471" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在本帖中，我们将展示正确执行功能选择的实用程序。如果我们高估了梯度增强的解释能力，或者仅仅是我们没有一个总体的数据理解，这就揭示了事情并不像预期的那么简单。我们的范围是检测各种特征选择技术的表现如何，以及为什么SHAP的使用会有所帮助。</p><h1 id="5546" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">博鲁塔是什么？</h1><p id="6b98" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">每个人都知道(或者很容易理解)一个<strong class="lb iu">递归特征消除</strong>是如何工作的。它递归地适合考虑较小特征集的监督算法。其中被排除的特征是根据一些权重的大小(例如，线性模型的系数或基于树的模型的特征重要性)被认为<em class="lv">不太重要</em>的特征。</p><p id="9e8b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu"> Boruta </strong>和RFE一样，是一种基于包装器的特征选择技术。它不太为人所知，但同样强大。Boruta背后的想法真的很简单。给定一个表格数据集，我们迭代地将监督算法(通常是基于树的模型)应用于数据的扩展版本。在每次迭代中，扩展版本由原始数据和水平附加的混洗列的副本组成。我们只维护每个迭代中的特性:</p><ul class=""><li id="9b80" class="mt mu it lb b lc ld lf lg li mv lm mw lq mx lu my mz na nb bi translated">比最好的混洗特征具有更高的重要性；</li><li id="76e2" class="mt mu it lb b lc nc lf nd li ne lm nf lq ng lu my mz na nb bi translated">比随机机会(使用二项式分布)更好。</li></ul><p id="b4b2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">RFE和博鲁塔都使用监督学习算法来提供特征重要性排名。这个模型是这两种技术的核心，因为它判断每个特征的好坏。这里可能会出现问题。<strong class="lb iu">决策树的标准特征重要性方法倾向于高估高频或高基数变量的重要性</strong>。对于博鲁塔和RFE来说，这可能导致错误的特征选择。</p><h1 id="1bff" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">实验</h1><p id="6ca1" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">我们从<a class="ae ky" href="https://www.kaggle.com/santoshd3/bank-customers" rel="noopener ugc nofollow" target="_blank"> Kaggle </a>收集数据集。我们选择一个银行客户数据集，试图预测客户是否很快会流失。在开始之前，<strong class="lb iu">我们向数据集添加一些由简单噪声</strong>产生的随机列。我们这样做是为了理解我们的模型是如何计算特征重要性的。我们开始拟合和调整我们的梯度推进(LGBM)。我们用不同的分裂种子重复这个过程不同的时间，以克服数据选择的随机性。平均特征重要性如下所示。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nh"><img src="../Images/dda3b830e0144779abf63d3e0cf35c0b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DMG6lcSjcMhaghuzjC-TMQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">各种分裂试验的平均特征重要性[图片由作者提供]</p></figure><p id="3337" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">令人惊讶的是，随机特征对我们的模型非常重要。另一个错误的假设是认为<em class="lv"> CustomerId </em>是一个有用的预测器。这是客户的唯一标识符，被梯度推进错误地认为是重要的。</p><p id="1950" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">鉴于这些前提，让我们在数据上尝试一些特征选择技术。我们从RFE开始。我们将参数调整与特征选择过程相结合。如前所述，我们对不同的分裂种子重复整个过程，以减轻数据选择的随机性。对于每个试验，我们考虑标准的基于树的特征重要性和SHAP重要性来存储所选择的特征。通过这种方式，我们可以画出在试验结束时一个特征被选择了多少次。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ni"><img src="../Images/de1af2dc9e800966548185915fea82b2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Qst6dIKpdAkhpWitRNUdbQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">用RFE(左)选择一个特征的次数；用RFE + SHAP(右)选了多少次特写[图片由作者提供]</p></figure><p id="cfdf" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在我们的案例中，具有标准重要性的RFE显得不准确。它通常选择与<em class="lv"> CustomerId相关的随机预测值。</em> SHAP + RFE最好不要选择无用的功能，但同时也要承认一些错误的选择。</p><p id="720a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">作为最后一步，我们重复相同的程序，但是使用Boruta。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ni"><img src="../Images/b310e79d940d6c3f2d56977f78d1a84b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*T_EYwd43QQ4kviVlUGlwAg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">用Boruta选择一个特征的次数(左)；用博鲁塔+ SHAP(右)选了多少次特征[图片由作者提供]</p></figure><p id="088c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">标准的Boruta在不考虑随机变量和<em class="lv"> CustomerId </em>方面做得很好。SHAP +博鲁塔似乎也做得更好，减少了选择过程中的差异。</p><h1 id="9d76" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">摘要</h1><p id="b878" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">在这篇文章中，我们介绍了RFE和博鲁塔(来自<a class="ae ky" href="https://github.com/cerlymarco/shap-hypetune" rel="noopener ugc nofollow" target="_blank"> <strong class="lb iu"> shap-hypetune </strong> </a>)这两个有价值的特性选择包装器方法。此外，我们用SHAP代替了特征重要性的计算。<strong class="lb iu"> SHAP帮助减轻了选择高频或高基数变量的影响</strong>。总之，当我们有一个完整的数据理解时，RFE可以单独使用。博鲁塔和SHAP可以消除对经过适当验证的选拔过程的任何疑虑。</p><p id="1d05" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">如果你对题目感兴趣，我建议:</strong></p><ul class=""><li id="ab17" class="mt mu it lb b lc ld lf lg li mv lm mw lq mx lu my mz na nb bi translated"><a class="ae ky" rel="noopener" target="_blank" href="/shap-for-feature-selection-and-hyperparameter-tuning-a330ec0ea104"> <strong class="lb iu"> SHAP用于特征选择和超参数调谐</strong> </a></li><li id="cbb5" class="mt mu it lb b lc nc lf nd li ne lm nf lq ng lu my mz na nb bi translated"><a class="ae ky" rel="noopener" target="_blank" href="/recursive-feature-selection-addition-or-elimination-755e5d86a791"> <strong class="lb iu">递归特征选择:增加还是消除？</strong>T19】</a></li><li id="b28a" class="mt mu it lb b lc nc lf nd li ne lm nf lq ng lu my mz na nb bi translated"><a class="ae ky" rel="noopener" target="_blank" href="/boruta-shap-for-temporal-feature-selection-96a7840c7713">T21【博鲁塔】SHAP为时态特征选择</a></li><li id="ab41" class="mt mu it lb b lc nc lf nd li ne lm nf lq ng lu my mz na nb bi translated"><a class="ae ky" rel="noopener" target="_blank" href="/shap-for-drift-detection-effective-data-shift-monitoring-c7fb9590adb0"> <strong class="lb iu">漂移检测SHAP:有效的数据漂移监控</strong> </a></li></ul></div><div class="ab cl nj nk hx nl" role="separator"><span class="nm bw bk nn no np"/><span class="nm bw bk nn no np"/><span class="nm bw bk nn no"/></div><div class="im in io ip iq"><p id="b2af" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><a class="ae ky" href="https://github.com/cerlymarco/MEDIUM_NoteBook" rel="noopener ugc nofollow" target="_blank"> <strong class="lb iu">查看我的GITHUB回购</strong> </a></p><p id="e432" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">保持联系:<a class="ae ky" href="https://www.linkedin.com/in/marco-cerliani-b0bba714b/" rel="noopener ugc nofollow" target="_blank"> Linkedin </a></p></div></div>    
</body>
</html>