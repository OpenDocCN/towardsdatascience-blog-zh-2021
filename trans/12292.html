<html>
<head>
<title>Compressing unsupervised fastText models</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">压缩无监督的快速文本模型</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/compressing-unsupervised-fasttext-models-eb212e9919ca?source=collection_archive---------15-----------------------#2021-12-14">https://towardsdatascience.com/compressing-unsupervised-fasttext-models-eb212e9919ca?source=collection_archive---------15-----------------------#2021-12-14</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="a513" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">如何将单词嵌入模型减少 300 倍，同时在下游 NLP 任务上具有几乎相同的性能</h2></div><p id="3c1f" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><a class="ae lb" href="https://fasttext.cc/" rel="noopener ugc nofollow" target="_blank"> FastText </a>是一种将单词编码为数字向量的方法，由脸书于 2016 年开发。预训练的快速文本嵌入有助于解决文本分类或命名实体识别等问题，并且比 BERT 等深度神经网络更快、更易于维护。然而，典型的快速文本模型是非常巨大的:例如，<a class="ae lb" href="https://fasttext.cc/docs/en/crawl-vectors.html" rel="noopener ugc nofollow" target="_blank">脸书</a>的英语模型，解压缩后占据了 7GB 的磁盘空间。</p><p id="252e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在这篇文章中，我展示了 Python 包<a class="ae lb" href="https://github.com/avidale/compress-fasttext" rel="noopener ugc nofollow" target="_blank"> compress-fasttext </a>，它可以将这个模型压缩到 21MB (x300！)在准确性上只有轻微的损失。这使得 fastText 在磁盘或内存有限的环境中更有用。</p><figure class="ld le lf lg gt lh gh gi paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="gh gi lc"><img src="../Images/8fdc0f51627e510c5921d18bd4de663b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*haP6Y29QGU9lin5d8QMAgw.png"/></div></div><p class="lo lp gj gh gi lq lr bd b be z dk translated">作者图片:利用 smolness 迷因制作 fastText(由<a class="ae lb" href="https://www.meme-arsenal.com/create/template/3082202" rel="noopener ugc nofollow" target="_blank">迷因库</a>生成)</p></figure><p id="5019" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在本文的第一部分，我展示了如何使用压缩的 fastText 模型。在第二部分，我解释了 fastText 及其压缩背后的一些数学原理。</p><h1 id="2ecb" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">怎么用？</h1><h2 id="60eb" class="mk lt iq bd lu ml mm dn ly mn mo dp mc ko mp mq me ks mr ms mg kw mt mu mi mv bi translated">使用现有模型</h2><p id="910a" class="pw-post-body-paragraph kf kg iq kh b ki mw jr kk kl mx ju kn ko my kq kr ks mz ku kv kw na ky kz la ij bi translated">简单用<code class="fe nb nc nd ne b">pip install compress-fasttext</code>就可以安装包了。它基于 fastText 的<a class="ae lb" href="https://radimrehurek.com/gensim/models/fasttext.html" rel="noopener ugc nofollow" target="_blank"> Gensim 实现，具有相同的接口。模型可以直接从 web 加载到 Python 中:</a></p><pre class="ld le lf lg gt nf ne ng nh aw ni bi"><span id="49ab" class="mk lt iq ne b gy nj nk l nl nm">import compress_fasttext<br/>small_model = compress_fasttext.models.CompressedFastTextKeyedVectors.load(<br/>'https://github.com/avidale/compress-fasttext/releases/download/v0.0.4/cc.en.300.compressed.bin')</span></pre><p id="9f63" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">您可以将这个模型视为一个字典，它将任何单词映射到它的 300 维向量表示(也称为<a class="ae lb" href="https://lena-voita.github.io/nlp_course/word_embeddings.html" rel="noopener ugc nofollow" target="_blank">嵌入</a>):</p><pre class="ld le lf lg gt nf ne ng nh aw ni bi"><span id="ffc5" class="mk lt iq ne b gy nj nk l nl nm">print(small_model['hello'])<br/># [ 1.847366e-01  6.326839e-03  4.439018e-03 ... -2.884310e-02]  <br/># a 300-dimensional numpy array</span></pre><p id="6b31" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">词义相近的词往往有相似的嵌入。因为嵌入是向量，它们的相似性可以用<a class="ae lb" href="https://en.wikipedia.org/wiki/Cosine_similarity" rel="noopener ugc nofollow" target="_blank">余弦度量</a>来评估。对于相关的单词(例如“猫”和“狗”)，余弦相似度接近 1，而对于不相关的单词，余弦相似度接近 0:</p><pre class="ld le lf lg gt nf ne ng nh aw ni bi"><span id="a717" class="mk lt iq ne b gy nj nk l nl nm">def cosine_sim(x, y):<br/>    return sum(x * y) / (sum(x**2) * sum(y**2)) ** 0.5</span><span id="45e3" class="mk lt iq ne b gy nn nk l nl nm">print(cosine_sim(small_model['cat'], small_model['cat']))<br/># 1.0</span><span id="c74e" class="mk lt iq ne b gy nn nk l nl nm">print(cosine_sim(small_model['cat'], small_model['dog']))<br/># 0.6768642734684225</span><span id="da42" class="mk lt iq ne b gy nn nk l nl nm">print(cosine_sim(small_model['cat'], small_model['car']))<br/># 0.18485135055040858</span></pre><p id="1382" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">实际上，你可以使用余弦相似度来查找一个单词的最近邻。例如，我们的压缩 fastText 模型知道 Python 是一种编程语言，并认为它类似于 PHP 和 Java。</p><pre class="ld le lf lg gt nf ne ng nh aw ni bi"><span id="36f1" class="mk lt iq ne b gy nj nk l nl nm">print(small_model.most_similar('Python'))<br/># [('PHP', 0.5253), ('.NET', 0.5027), ('Java', 0.4897),  ... ]</span></pre><p id="9f01" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在实际应用中，您通常将快速文本嵌入提供给其他模型。例如，您可以在 fastText 上训练一个分类器来区分可食用和不可食用的东西:</p><pre class="ld le lf lg gt nf ne ng nh aw ni bi"><span id="1868" class="mk lt iq ne b gy nj nk l nl nm">import numpy as np<br/>from sklearn.pipeline import make_pipeline<br/>from sklearn.linear_model import LogisticRegression<br/>from sklearn.base import BaseEstimator, TransformerMixin</span><span id="7e77" class="mk lt iq ne b gy nn nk l nl nm">class FastTextTransformer(BaseEstimator, TransformerMixin):<br/>    """ Convert texts into their mean fastText vectors """<br/>    def __init__(self, model):<br/>        self.model = model</span><span id="02e2" class="mk lt iq ne b gy nn nk l nl nm">    def fit(self, X, y=None):<br/>        return self</span><span id="2549" class="mk lt iq ne b gy nn nk l nl nm">    def transform(self, X):<br/>        return np.stack([<br/>            np.mean([self.model[w] for w in text.split()], 0)<br/>            for text in X<br/>        ])<br/>    <br/>classifier = make_pipeline(<br/>    FastTextTransformer(model=small_model), <br/>    LogisticRegression()<br/>).fit(<br/>    ['banana', 'soup', 'burger', 'car', 'tree', 'city'],<br/>    [1, 1, 1, 0, 0, 0]<br/>)</span><span id="ddd0" class="mk lt iq ne b gy nn nk l nl nm">classifier.predict(['jet', 'cake'])<br/># array([0, 1])</span></pre><h2 id="bfeb" class="mk lt iq bd lu ml mm dn ly mn mo dp mc ko mp mq me ks mr ms mg kw mt mu mi mv bi translated">有什么型号</h2><p id="b39d" class="pw-post-body-paragraph kf kg iq kh b ki mw jr kk kl mx ju kn ko my kq kr ks mz ku kv kw na ky kz la ij bi translated">一些英语和俄语的模型可以从<a class="ae lb" href="https://github.com/avidale/compress-fasttext/releases/tag/gensim-4-draft" rel="noopener ugc nofollow" target="_blank">发布页面</a>下载。对于英语，建议使用<a class="ae lb" href="https://github.com/avidale/compress-fasttext/releases/download/gensim-4-draft/ft_cc.en.300_freqprune_100K_20K_pq_100.bin" rel="noopener ugc nofollow" target="_blank">25MB 型号</a>，尺寸和精度平衡良好。</p><figure class="ld le lf lg gt lh gh gi paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="gh gi no"><img src="../Images/788337a4daaff7140985721caec2016f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vQips-lz9zHQ_-tBt2FTiQ.png"/></div></div><p class="lo lp gj gh gi lq lr bd b be z dk translated">作者图片:<a class="ae lb" href="https://github.com/avidale/compress-fasttext/releases/tag/gensim-4-draft" rel="noopener ugc nofollow" target="_blank">https://github . com/avidale/compress-fast text/releases/tag/gensim-4-draft</a>截图。</p></figure><p id="b87f" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如果你需要其他语言，你可以看看 Liebl Bernhard 的集合，它包含了 101 种语言的压缩快速文本模型。</p><h2 id="3f37" class="mk lt iq bd lu ml mm dn ly mn mo dp mc ko mp mq me ks mr ms mg kw mt mu mi mv bi translated">压缩模型</h2><p id="f4a3" class="pw-post-body-paragraph kf kg iq kh b ki mw jr kk kl mx ju kn ko my kq kr ks mz ku kv kw na ky kz la ij bi translated">如果您想要创建自己的压缩模型，您需要安装带有一些额外依赖项的库:</p><pre class="ld le lf lg gt nf ne ng nh aw ni bi"><span id="2870" class="mk lt iq ne b gy nj nk l nl nm">pip install compress-fasttext[full]</span></pre><p id="8d45" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">首先，您需要加载您将要压缩的模型。如果模型已经使用脸书包进行了训练，则按如下方式加载它:</p><pre class="ld le lf lg gt nf ne ng nh aw ni bi"><span id="1b94" class="mk lt iq ne b gy nj nk l nl nm">from gensim.models.fasttext import load_facebook_model<br/>big_model = load_facebook_model('path-to-original-model').wv</span></pre><p id="a3a3" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">否则，如果模型是 Gensim 格式的，则将其加载为</p><pre class="ld le lf lg gt nf ne ng nh aw ni bi"><span id="5796" class="mk lt iq ne b gy nj nk l nl nm">import gensim<br/>big_model = gensim.models.fasttext.FastTextKeyedVectors.load('path-to-original-model')</span></pre><p id="3b7a" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在，您可以用三行代码压缩模型:</p><pre class="ld le lf lg gt nf ne ng nh aw ni bi"><span id="6279" class="mk lt iq ne b gy nj nk l nl nm">import compress_fasttext<br/>small_model = compress_fasttext.prune_ft_freq(big_model, pq=True)<br/>small_model.save('path-to-new-model')</span></pre><p id="2e67" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如果您愿意，可以使用额外的参数来控制模型的大小:</p><pre class="ld le lf lg gt nf ne ng nh aw ni bi"><span id="cffd" class="mk lt iq ne b gy nj nk l nl nm">small_model = compress_fasttext.prune_ft_freq(<br/>    big_model, <br/>    new_vocab_size=20_000,   # number of words<br/>    new_ngrams_size=100_000, # number of character ngrams<br/>    pq=True,                 # use product quantization<br/>    qdim=100,                # dimensionality of quantization<br/>)</span></pre><p id="f7b3" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">通过使用较小的词汇大小和维数，您创建了一个较小的模型，但相对于原始模型降低了其准确性。</p><h1 id="46e3" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">它是如何工作的？</h1><h2 id="7377" class="mk lt iq bd lu ml mm dn ly mn mo dp mc ko mp mq me ks mr ms mg kw mt mu mi mv bi translated">相关著作</h2><p id="eb43" class="pw-post-body-paragraph kf kg iq kh b ki mw jr kk kl mx ju kn ko my kq kr ks mz ku kv kw na ky kz la ij bi translated">最初的 fastText 库<a class="ae lb" href="https://fasttext.cc/blog/2017/10/02/blog-post.html#model-compression" rel="noopener ugc nofollow" target="_blank">确实支持模型压缩</a>(甚至有<a class="ae lb" href="https://arxiv.org/abs/1612.03651" rel="noopener ugc nofollow" target="_blank">一篇关于它的论文</a>)，但是只针对在特定分类任务上训练的监督模型。另一方面，<a class="ae lb" href="https://github.com/avidale/compress-fasttext" rel="noopener ugc nofollow" target="_blank"> compress-fastText </a>用于无监督模型，提供可用于多项任务的单词向量。我的作品部分基于 Andrey Vasnetsov 2019 年的文章，部分基于俄罗斯 navec 图书馆。它最初是在我的俄语帖子中描述的<a class="ae lb" href="https://habr.com/ru/post/489474/" rel="noopener ugc nofollow" target="_blank">。</a></p><h2 id="71c8" class="mk lt iq bd lu ml mm dn ly mn mo dp mc ko mp mq me ks mr ms mg kw mt mu mi mv bi translated">fastText 可以压缩到什么程度？</h2><p id="f65c" class="pw-post-body-paragraph kf kg iq kh b ki mw jr kk kl mx ju kn ko my kq kr ks mz ku kv kw na ky kz la ij bi translated">FastText 不同于其他单词嵌入方法，因为它将单词的嵌入与字符 n 元语法(即几个连续字符的序列)的嵌入相结合。单词和 n-grams 的嵌入被平均在一起。如果单词不在词汇表中，则它只由 n-grams 组成，这使得 fastText 可以顺利地处理拼写错误的单词、新词和具有丰富词法的语言。</p><p id="edea" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">以下伪代码演示了计算过程:</p><pre class="ld le lf lg gt nf ne ng nh aw ni bi"><span id="cfc2" class="mk lt iq ne b gy nj nk l nl nm">def embed(word, model):<br/>    if word in model.vocab:<br/>        result = model.vectors_vocab[word]<br/>    else:<br/>        result = zeros()<br/>    n = 1<br/>    for ngram in get_ngrams(word, model.min_n, model.max_n):<br/>        result += model.vectors_ngrams[hash(ngram)]<br/>        n += 1<br/>    return result / n</span></pre><p id="e14e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">正如你所看到的，所有的信息都来自两个矩阵，<code class="fe nb nc nd ne b">vectors_vocab</code>和<code class="fe nb nc nd ne b">vectors_ngrams</code>，这些矩阵占据了空间。大型矩阵的大小可以通过几种方式来减小:</p><ul class=""><li id="c7bc" class="np nq iq kh b ki kj kl km ko nr ks ns kw nt la nu nv nw nx bi translated">删除矩阵中的大多数行，只保留最常用的单词和 n-gram；</li><li id="9163" class="np nq iq kh b ki ny kl nz ko oa ks ob kw oc la nu nv nw nx bi translated">存储精度较低的矩阵(<code class="fe nb nc nd ne b">float16 </code>代替<code class="fe nb nc nd ne b">float32</code>)；</li><li id="dac9" class="np nq iq kh b ki ny kl nz ko oa ks ob kw oc la nu nv nw nx bi translated">将矩阵分成小块，对它们进行聚类，并存储相应聚类的 id，而不是每个块。这叫<em class="od">产品量化；</em></li><li id="18b7" class="np nq iq kh b ki ny kl nz ko oa ks ob kw oc la nu nv nw nx bi translated">将矩阵因式分解为两个较小矩阵的乘积(不推荐，因为精度低)。</li></ul><p id="b84d" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">库<a class="ae lb" href="https://github.com/avidale/compress-fasttext" rel="noopener ugc nofollow" target="_blank"> compress-fasttext </a>支持全部四种压缩方式，建议结合前三种。这就是方法<code class="fe nb nc nd ne b">prune_ft_freq</code>实际做的事情。</p><h2 id="c397" class="mk lt iq bd lu ml mm dn ly mn mo dp mc ko mp mq me ks mr ms mg kw mt mu mi mv bi translated">压缩的型号够好吗？</h2><p id="1816" class="pw-post-body-paragraph kf kg iq kh b ki mw jr kk kl mx ju kn ko my kq kr ks mz ku kv kw na ky kz la ij bi translated">为了确认压缩后模型仍然保留有用的信息，我们可以使用工具<a class="ae lb" href="https://github.com/facebookresearch/SentEval" rel="noopener ugc nofollow" target="_blank"> SentEval </a>对它们进行评估。该工具包括 17 个下游任务，如短文本分类、文本间语义相似性评估和自然语言推理，以及 10 个诊断任务。对于每项任务，要么直接使用嵌入，要么在顶部使用小型线性模型。我修改了<a class="ae lb" href="https://github.com/facebookresearch/SentEval/blob/main/examples/bow.py" rel="noopener ugc nofollow" target="_blank"> bow.py </a>示例以使用 fastText，并使用<a class="ae lb" href="https://rusvectores.org/ru/models/" rel="noopener ugc nofollow" target="_blank">完整的 7GB 英文版本</a>和<a class="ae lb" href="https://github.com/avidale/compress-fasttext/releases/tag/gensim-4-draft" rel="noopener ugc nofollow" target="_blank">其 25MB 压缩版本</a>来执行它。下表显示了两种模型在所有任务中的表现:</p><figure class="ld le lf lg gt lh gh gi paragraph-image"><div class="gh gi oe"><img src="../Images/a0348c67b2f8d553c39189ab1cd28fdc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1112/format:webp/1*7dhtmucE1BN125OiMjeKrQ.png"/></div><p class="lo lp gj gh gi lq lr bd b be z dk translated">作者图片:SentEval 上的模型评估</p></figure><p id="863d" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">平均而言，小型号的性能分数是全型号性能的 0.9579，而小型号几乎是全型号的 300 倍。这证实了如果由于某种原因不希望模型太大，那么压缩 fastText 模型并不是一个坏主意。</p><h1 id="5a8d" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">结论</h1><p id="de07" class="pw-post-body-paragraph kf kg iq kh b ki mw jr kk kl mx ju kn ko my kq kr ks mz ku kv kw na ky kz la ij bi translated">FastText 是计算有意义的单词嵌入的一种很好的方法，但是典型的 fastText 模型的大小限制了它在移动设备或适度的免费主机上的使用。在<a class="ae lb" href="https://github.com/avidale/compress-fasttext" rel="noopener ugc nofollow" target="_blank"> compress-fastText </a>中收集的方法允许将模型大小减少数百倍，而不会显著影响下游性能。我发布了一个<a class="ae lb" href="https://github.com/avidale/compress-fasttext" rel="noopener ugc nofollow" target="_blank">包</a>和<a class="ae lb" href="https://github.com/avidale/compress-fasttext/releases/tag/gensim-4-draft" rel="noopener ugc nofollow" target="_blank">压缩模型</a>，可以用来高效地解决各种 NLP 问题。</p><p id="b4aa" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在未来，我计划发布更多的微型模型，例如<a class="ae lb" href="https://huggingface.co/cointegrated/rubert-tiny" rel="noopener ugc nofollow" target="_blank">一个用于英语和俄语的 45MB BERT 模型</a>。订阅，敬请关注！</p></div><div class="ab cl of og hu oh" role="separator"><span class="oi bw bk oj ok ol"/><span class="oi bw bk oj ok ol"/><span class="oi bw bk oj ok"/></div><div class="ij ik il im in"><p id="3498" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这篇文章是由大卫·戴尔(<a class="ae lb" href="https://daviddale.ru/en" rel="noopener ugc nofollow" target="_blank">https://daviddale.ru/en</a>)写的，他是 NLP 的研究科学家和聊天机器人的开发者。</p><p id="b06d" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如果你在科学论文中提到这篇文章(或文章包),请注明出处:</p><pre class="ld le lf lg gt nf ne ng nh aw ni bi"><span id="0206" class="mk lt iq ne b gy nj nk l nl nm">@misc{dale_compress_fasttext, <br/>   author = "Dale, David",<br/>   title  = "<!-- -->Compressing unsupervised fastText models<!-- -->", <br/>   editor = "<!-- -->towardsdatascience.com<!-- -->", <br/>   url    = "<a class="ae lb" rel="noopener" target="_blank" href="/eb212e9919ca">https://towardsdatascience.com/eb212e9919ca</a>", <br/>   month  = {December},<br/>   year   = {2021},   <br/>   note = {[Online; posted 12-December-2021]},<br/>}</span></pre></div></div>    
</body>
</html>