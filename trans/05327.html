<html>
<head>
<title>Generalized Advantage Estimate: Maths and Code</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">广义优势估计:数学与代码</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/generalized-advantage-estimate-maths-and-code-b5d5bd3ce737?source=collection_archive---------6-----------------------#2021-05-11">https://towardsdatascience.com/generalized-advantage-estimate-maths-and-code-b5d5bd3ce737?source=collection_archive---------6-----------------------#2021-05-11</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi jn"><img src="../Images/bbee9cec5e1a8d3095fbb4077fad1e99.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*MJ3welvKt8gpSVs7"/></div></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">由<a class="ae kc" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的<a class="ae kc" href="https://unsplash.com/@edge2edgemedia?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Edge2Edge媒体</a>拍摄</p></figure><p id="6b20" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在我的文章实现<a class="ae kc" rel="noopener" target="_blank" href="/phasic-policy-gradient-ppg-part-2-c93afeaf37d4"> <strong class="kf ir">阶段策略梯度(PPG)算法</strong> </a>中，我有一个关于广义优势估计(GAE)的问题，所以我想我应该在这里跟进一些额外的细节😊</p><p id="2d2d" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><a class="ae kc" href="https://arxiv.org/abs/1506.02438" rel="noopener ugc nofollow" target="_blank"> <strong class="kf ir">这里是原文</strong> </a>。我将更直观地描述这个理论，并解释如何用代码实现它。如果你想尝试这个和其他RL技术，请查看我的开源库，<a class="ae kc" href="https://github.com/LondonNode/Pearl" rel="noopener ugc nofollow" target="_blank"> <strong class="kf ir"> Pearl </strong> </a>。</p><p id="1b59" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">总之，这种方法允许我们估计强化学习中的优势和价值函数，并且容易控制偏差方差权衡。让我们开始吧！</p><h1 id="ab4d" class="lb lc iq bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">设置场景</h1><p id="da50" class="pw-post-body-paragraph kd ke iq kf b kg lz ki kj kk ma km kn ko mb kq kr ks mc ku kv kw md ky kz la ij bi translated">我们正在尝试教一些代理解决一个环境(例如，一个游戏)。为了做到这一点，代理人必须了解其行动的预期结果。在这里，我们将正式方程，将给予代理这一信息。</p><h2 id="7ccb" class="me lc iq bd ld mf mg dn lh mh mi dp ll ko mj mk lp ks ml mm lt kw mn mo lx mp bi translated">轨道</h2><p id="b9a7" class="pw-post-body-paragraph kd ke iq kf b kg lz ki kj kk ma km kn ko mb kq kr ks mc ku kv kw md ky kz la ij bi translated">当代理浏览环境状态时，它收集奖励，告诉它刚刚采取的行动是好是坏。随着时间的推移，一个代理人遇到的行动、状态和奖励的序列被称为轨迹，我们在这里用符号<strong class="kf ir"> τ </strong>来定义它。</p><h2 id="e1f6" class="me lc iq bd ld mf mg dn lh mh mi dp ll ko mj mk lp ks ml mm lt kw mn mo lx mp bi translated">返回</h2><p id="4b6d" class="pw-post-body-paragraph kd ke iq kf b kg lz ki kj kk ma km kn ko mb kq kr ks mc ku kv kw md ky kz la ij bi translated">在一个轨迹的终点，我们可以回头看，看到轨迹中每一步的未来回报的贴现总和，也称为<strong class="kf ir">回报</strong>。更正式地说，回报是这样定义的:</p><figure class="mr ms mt mu gt jr gh gi paragraph-image"><div class="gh gi mq"><img src="../Images/7769bdfa26b470bb8fde8aa3f3614ded.png" data-original-src="https://miro.medium.com/v2/resize:fit:612/format:webp/1*FqaDjy7BBvFIo7XVzTSoJQ.png"/></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">作者图片:轨迹的回归</p></figure><p id="cb8d" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir"> γ </strong>术语引入的折扣是一个介于0和1之间的数字。我们想这么做有几个原因:</p><ol class=""><li id="ade5" class="mv mw iq kf b kg kh kk kl ko mx ks my kw mz la na nb nc nd bi translated">从数学上来说，奖励的无穷大可能不会收敛到一个有限值，并且很难用等式来处理。折扣因素对此有所帮助。</li><li id="ae46" class="mv mw iq kf b kg ne kk nf ko ng ks nh kw ni la na nb nc nd bi translated">更直观地说，未来的奖励不会带来直接的好处。我们更喜欢即时的满足感；其实我们在给股票和衍生品定价的时候也是这么做的！</li></ol><h2 id="fd3c" class="me lc iq bd ld mf mg dn lh mh mi dp ll ko mj mk lp ks ml mm lt kw mn mo lx mp bi translated">价值函数</h2><p id="2d1e" class="pw-post-body-paragraph kd ke iq kf b kg lz ki kj kk ma km kn ko mb kq kr ks mc ku kv kw md ky kz la ij bi translated">价值函数<strong class="kf ir"> V </strong>定义为一个状态的预期收益:</p><figure class="mr ms mt mu gt jr gh gi paragraph-image"><div class="gh gi nj"><img src="../Images/1f5c8461be768cfaab04eecae3a80f7e.png" data-original-src="https://miro.medium.com/v2/resize:fit:446/format:webp/1*72kQMnfD5gCDP0SVEjF0VQ.png"/></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">作者图片:价值函数</p></figure><p id="2578" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">代理应该偏好具有较高值的状态，因为这些状态具有与其相关联的较高的预期总轨迹回报。几乎每个强化学习算法都会用到这一点。</p><h2 id="b8ac" class="me lc iq bd ld mf mg dn lh mh mi dp ll ko mj mk lp ks ml mm lt kw mn mo lx mp bi translated">优势函数</h2><p id="1aff" class="pw-post-body-paragraph kd ke iq kf b kg lz ki kj kk ma km kn ko mb kq kr ks mc ku kv kw md ky kz la ij bi translated">优势函数<strong class="kf ir"> A </strong>，是从状态中给定动作的期望回报中减去状态(值)的期望回报。更直观地说，它告诉我们所采取的行动与总体预期回报相比有多好或多差:</p><figure class="mr ms mt mu gt jr gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/aabdaae45e48d6d3e4d57ffd5ff87400.png" data-original-src="https://miro.medium.com/v2/resize:fit:786/format:webp/1*F6LfRYENNRovq_TWVFsrDQ.png"/></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">作者图片:优势功能</p></figure><p id="cbd6" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">为什么我们用这个，而不仅仅是在一个状态下给定一个动作的预期收益(第一项)？直观地说，该指标在蒙特卡洛运行中具有较低的方差，因为减去确定性值函数通常会导致较小的量值。</p><h1 id="0e1a" class="lb lc iq bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">偏差方差权衡</h1><p id="c00f" class="pw-post-body-paragraph kd ke iq kf b kg lz ki kj kk ma km kn ko mb kq kr ks mc ku kv kw md ky kz la ij bi translated">首先，让我们开始一些定义:</p><ul class=""><li id="0f21" class="mv mw iq kf b kg kh kk kl ko mx ks my kw mz la nl nb nc nd bi translated"><strong class="kf ir">偏差</strong>:有偏差的估计量不能很好地代表/符合原始度量。从形式上讲，如果估计量的期望值等于原始度量，则估计量是无偏的。</li><li id="236b" class="mv mw iq kf b kg ne kk nf ko ng ks nh kw ni la nl nb nc nd bi translated"><strong class="kf ir">方差</strong>:方差较大的估计量，其值的分布范围较大。理想情况下，无偏估计量应该具有较低的方差，以便始终如一地匹配输入的原始度量。在形式上，这是衡量相同的方差的任何随机变量的衡量。</li></ul><p id="001a" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">不幸的是，我们通常没有价值函数或优势函数的精确形式。让我们使用神经网络来模拟价值函数，因为这仅需要状态输入而不是状态动作输入。鉴于此，<strong class="kf ir">我们如何用一个不完美的价值函数估计器来估计优势函数？</strong>首先，注意:</p><figure class="mr ms mt mu gt jr gh gi paragraph-image"><div class="gh gi nm"><img src="../Images/13f672f44fbcccf3df6eff4b67913f7e.png" data-original-src="https://miro.medium.com/v2/resize:fit:738/format:webp/1*92C7WZkHDJSeBNEv7DiH_A.png"/></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">作者图片</p></figure><p id="b501" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">也就是说，一个状态下给定一个动作的预期收益，等于动作状态对的回报(这里假设是确定性的)加上下一个状态的贴现预期收益。</p><p id="dbd4" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">因此，优势可以估计为:</p><figure class="mr ms mt mu gt jr gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/c6836d48aff7ba36699c1a5a70039724.png" data-original-src="https://miro.medium.com/v2/resize:fit:602/format:webp/1*PtsF0edZPXMy0Oc-wO_NZg.png"/></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">作者图片:优势函数的时间差异(TD)估计</p></figure><p id="4a00" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">注意，这个估计量高度依赖于价值函数估计量。如果这个有高偏差，那么TD估计也会有高偏差！</p><figure class="mr ms mt mu gt jr gh gi paragraph-image"><div class="gh gi no"><img src="../Images/82cdbfb92c5348fcdf6b0f020a063c51.png" data-original-src="https://miro.medium.com/v2/resize:fit:1252/format:webp/1*q4CjKW5wDKgxHkQN-QnL5Q.png"/></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">图片作者:如果价值函数估计有偏差，则TD估计有偏差</p></figure><p id="c8d4" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">理想情况下，我们希望尝试绕过这个问题。在强化学习设置中，我们通常在每次更新的环境中采取<strong class="kf ir"> n </strong>个步骤。因此，延长额外时间步长的TD估计值相当容易:</p><figure class="mr ms mt mu gt jr gh gi paragraph-image"><div class="gh gi np"><img src="../Images/f1894aa508c3cdc6aa17831e96050283.png" data-original-src="https://miro.medium.com/v2/resize:fit:1068/format:webp/1*-wHvYiTr_g5w0bxjbAU3sw.png"/></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">作者图片:超过<strong class="bd nq"> n </strong>步的扩展优势评估</p></figure><p id="4dee" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这样做减少了偏差，因为我们增加了来自完全优势函数的不依赖于价值函数估计的项的比例，并且我们将第n个状态的价值估计的幅度缩放了小得多的数。</p><figure class="mr ms mt mu gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nr"><img src="../Images/a6a730be883d7ab75b8dcad8408ec01c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FentFVfZvavzf95qD1t1nA.png"/></div></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">作者图片:回报期望项与原始优势函数相匹配</p></figure><p id="1dcd" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">然而，这样做也有不利的一面。随着许多额外项的加入，新的估计量具有增加的方差。</p><p id="b2aa" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">总之，扩展TD估计以包括更多的奖励步骤以增加方差为代价减少了估计量的偏差。我们可以在1和n之间选择任意一个数字，来放入扩展的优势估计，<strong class="kf ir"> A^{(i)}(s，a)。</strong>问题是，我们如何挑选这个数字？</p><h1 id="f8ff" class="lb lc iq bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">GAE方程</h1><p id="2225" class="pw-post-body-paragraph kd ke iq kf b kg lz ki kj kk ma km kn ko mb kq kr ks mc ku kv kw md ky kz la ij bi translated">一个很好的解决方案是只取1和n之间的一个指数平均值作为扩展优势估计器的输入。让我们直接从纸上看最终的形式，其中<strong class="kf ir"> δ_t </strong>是时间步长<strong class="kf ir"> t </strong>的TD优势估计。</p><figure class="mr ms mt mu gt jr gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/77bf72fe53aad38509a4093bae34d586.png" data-original-src="https://miro.medium.com/v2/resize:fit:920/format:webp/1*OBNGhJZNztR9YTRbhFkSKg.png"/></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">[1]广义优势估计</p></figure><p id="0b9b" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这里，<strong class="kf ir"> λ </strong>是指数权重折扣。重要的是，<strong class="kf ir">这是控制偏差方差权衡的杠杆！</strong>注意，如果我们将其设置为0，那么我们将得到TD优势估计(高偏差、低方差)，如果我们将其设置为1，这相当于选择<strong class="kf ir"> i = n </strong>作为扩展优势估计(低偏差、高方差)。</p><h1 id="6904" class="lb lc iq bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">密码</h1><p id="f5cd" class="pw-post-body-paragraph kd ke iq kf b kg lz ki kj kk ma km kn ko mb kq kr ks mc ku kv kw md ky kz la ij bi translated">GAE在Python中的实现如下所示:</p><figure class="mr ms mt mu gt jr"><div class="bz fp l di"><div class="nt nu l"/></div></figure><p id="1a17" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">是我的错觉还是事情出奇的简单？这就是指数平均的美妙之处！它可以容易地实现，并且在计算上是线性的😁</p><p id="eb6e" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">诀窍是从末尾开始，然后往回算，这样我们就不会一遍又一遍地计算相同的量。让我们用一个<strong class="kf ir"> n </strong>步进轨迹来更清楚地展示这一点:</p><figure class="mr ms mt mu gt jr gh gi paragraph-image"><div class="gh gi nv"><img src="../Images/834687b3bd42a166d9e7d909e4a9eb7c.png" data-original-src="https://miro.medium.com/v2/resize:fit:938/format:webp/1*H1Q8YNrEI-iXGfBU2ywt7Q.png"/></div></figure><figure class="mr ms mt mu gt jr gh gi paragraph-image"><div class="gh gi nw"><img src="../Images/13aa03c986a1500091157c5c154100fa.png" data-original-src="https://miro.medium.com/v2/resize:fit:940/format:webp/1*9FlSfRKRYcey8W7lwqpXLw.png"/></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">作者图片</p></figure><p id="ef11" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">因此:</p><figure class="mr ms mt mu gt jr gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/db5a7d67000f3051ad0eb281a6050396.png" data-original-src="https://miro.medium.com/v2/resize:fit:466/format:webp/1*1ift7Dg85lnuIy0KeS8K1w.png"/></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">作者图片</p></figure><p id="1199" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">此外，请注意此处添加了“完成”标志，表示一集是否已经结束(0 =完成，1 =未完成)。包括这一点很重要，这样我们在计算当前剧集中某一步的优势时就不会考虑未来剧集的奖励。</p></div><div class="ab cl ny nz hu oa" role="separator"><span class="ob bw bk oc od oe"/><span class="ob bw bk oc od oe"/><span class="ob bw bk oc od"/></div><div class="ij ik il im in"><p id="b160" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">唷！坚持到了最后🎉🎉</p><p id="fbbe" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如果您觉得这篇文章有用，请考虑:</p><ul class=""><li id="e86a" class="mv mw iq kf b kg kh kk kl ko mx ks my kw mz la nl nb nc nd bi translated">跟踪我🙌</li><li id="0082" class="mv mw iq kf b kg ne kk nf ko ng ks nh kw ni la nl nb nc nd bi translated"><a class="ae kc" href="https://medium.com/subscribe/@rohan.tangri" rel="noopener"> <strong class="kf ir">订阅我的邮件通知</strong> </a>永不错过上传📧</li><li id="75a9" class="mv mw iq kf b kg ne kk nf ko ng ks nh kw ni la nl nb nc nd bi translated">使用我的媒介<a class="ae kc" href="https://medium.com/@rohan.tangri/membership" rel="noopener"> <strong class="kf ir">推荐链接</strong> </a> <strong class="kf ir"> </strong>直接支持我并获得无限量的优质文章🤗</li></ul><p id="0d66" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">促销的方式，让我知道你对这个话题的想法和快乐学习！！</p><h1 id="ff38" class="lb lc iq bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">参考</h1><p id="0c06" class="pw-post-body-paragraph kd ke iq kf b kg lz ki kj kk ma km kn ko mb kq kr ks mc ku kv kw md ky kz la ij bi translated">[1]使用广义优势估计的高维连续控制，作者:约翰·舒尔曼、菲利普·莫里茨、谢尔盖·莱文、迈克尔·乔丹、彼得·阿比尔:<a class="ae kc" href="https://arxiv.org/abs/1506.02438" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1506.02438</a></p></div></div>    
</body>
</html>