<html>
<head>
<title>Everything Product People Need to Know About Transformers (Part 3: BERT)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">产品人员需要知道的关于变形金刚的一切(第三部分:伯特)</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/everything-product-people-need-to-know-about-transformers-part-3-bert-a1227cead488?source=collection_archive---------13-----------------------#2021-03-19">https://towardsdatascience.com/everything-product-people-need-to-know-about-transformers-part-3-bert-a1227cead488?source=collection_archive---------13-----------------------#2021-03-19</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="7677" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">确定变压器供电产品范围所需的一切</h2></div><blockquote class="kf kg kh"><p id="4bbc" class="ki kj kk kl b km kn jr ko kp kq ju kr ks kt ku kv kw kx ky kz la lb lc ld le ij bi translated">这是面向产品人员的变形金刚系列的第3部分。<a class="ae lf" href="https://yacovlewis.medium.com/everything-product-people-need-to-know-about-transformers-gpt-3-and-huggingface-6a2e8a45316a" rel="noopener">点击此处查看第一部分</a>。本文依赖于前几篇文章中介绍的概念和信息。如果你对变形金刚和GPT不熟悉，建议从第1部分开始。</p></blockquote><p id="a33a" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lg kt ku kv lh kx ky kz li lb lc ld le ij bi translated">如果你正在读这篇文章，你正在设计、管理或投资技术产品。你也可能只是一个聪明的工程师或数据科学家，他们发现这些文章对于理解新的研究非常有用。那么，你真的需要了解变压器模型吗？我们的变形金刚模型有什么好的吗？</p><p id="1e67" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lg kt ku kv lh kx ky kz li lb lc ld le ij bi translated">号码</p><figure class="lk ll lm ln gt lo gh gi paragraph-image"><div role="button" tabindex="0" class="lp lq di lr bf ls"><div class="gh gi lj"><img src="../Images/31e79e7145e8fdbac60f899a5178cefe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pJIPy9ne5ufUmvzz00B6sg.jpeg"/></div></div><p class="lv lw gj gh gi lx ly bd b be z dk translated">这家伙明白了</p></figure><p id="6fc4" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lg kt ku kv lh kx ky kz li lb lc ld le ij bi translated">要打造尖端产品，了解变压器模型。谷歌的产品人员非常了解变形金刚，带领他们<a class="ae lf" href="https://blog.google/products/search/search-language-understanding-bert/" rel="noopener ugc nofollow" target="_blank">改进他们核心业务的产品线</a>:搜索。哼唱查找歌曲—由BERT提供支持。特色片段——由BERT提供。视频中的关键时刻——由BERT提供。这些功能您可能已经很熟悉了:</p><figure class="lk ll lm ln gt lo gh gi paragraph-image"><div role="button" tabindex="0" class="lp lq di lr bf ls"><div class="gh gi lz"><img src="../Images/0f6220380415f36ac493e0c018d402e7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CCuaoJ706TRaPzpQH2n8yQ.png"/></div></div><p class="lv lw gj gh gi lx ly bd b be z dk translated">熟悉的基于变形金刚模型的谷歌特性</p></figure><p id="b199" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lg kt ku kv lh kx ky kz li lb lc ld le ij bi translated">要理解变压器模型，就要理解伯特和GPT。每个都有一个独特的架构，为特定的训练任务进行了优化。虽然我现在可以告诉您每个型号最适合哪些应用，但解释设计选择和型号之间的差异将使您能够识别尚未尝试的新应用。</p><p id="f81c" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lg kt ku kv lh kx ky kz li lb lc ld le ij bi translated">以下部分将介绍BERT，这需要了解生成性预培训(GPT)。在<a class="ae lf" rel="noopener" target="_blank" href="/everything-product-people-need-to-know-about-transformers-part-2-gpt-34c065ba07bf">第2部分</a>中，我深入解释了GPT模型架构，我建议在从这里继续之前先阅读那篇文章。警告:如果你还没有阅读第2部分，下面的部分会让你困惑，这是你的错。在解释了BERT架构之后，我将把这个模型放在我的关于transformer模型的更大系列中，以便能够完全识别和评估潜在的应用。</p></div><div class="ab cl ma mb hu mc" role="separator"><span class="md bw bk me mf mg"/><span class="md bw bk me mf mg"/><span class="md bw bk me mf"/></div><div class="ij ik il im in"><h1 id="7631" class="mh mi iq bd mj mk ml mm mn mo mp mq mr jw ms jx mt jz mu ka mv kc mw kd mx my bi translated">伯特解释道</h1><p id="b983" class="pw-post-body-paragraph ki kj iq kl b km mz jr ko kp na ju kr lg nb ku kv lh nc ky kz li nd lc ld le ij bi translated">在OpenAI推出GPT四个月后，谷歌发布了《变形金刚:双向编码器表示》。BERT利用了预先训练的变压器的能力，同时解决了GPT架构带来的一些限制。通过这样做，BERT极大地扩展了变形金刚可以有效处理的任务集。</p><p id="9559" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lg kt ku kv lh kx ky kz li lb lc ld le ij bi translated">我们将从总结<strong class="kl ir">开始，只总结</strong>伯特创新的GPT的那些组成部分:</p><blockquote class="kf kg kh"><p id="1301" class="ki kj kk kl b km kn jr ko kp kq ju kr ks kt ku kv kw kx ky kz la lb lc ld le ij bi translated">1.GPT的设计方法是从变压器(T-ED)中去掉编码器组件，只保留解码器(T-D)。</p><p id="faa0" class="ki kj kk kl b km kn jr ko kp kq ju kr ks kt ku kv kw kx ky kz la lb lc ld le ij bi translated">2.解码器有时被称为生成器，其功能相当于语言模型，这意味着它被优化来预测句子中的下一个单词。</p><p id="a2d8" class="ki kj kk kl b km kn jr ko kp kq ju kr ks kt ku kv kw kx ky kz la lb lc ld le ij bi translated">3.在这个模型中，注意力的作用是单向的，这意味着该模型在预测下一个单词时只能查看前面的单词，而不能查看将跟随神秘标记的单词。</p></blockquote><p id="2227" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lg kt ku kv lh kx ky kz li lb lc ld le ij bi translated">GPT是第一个建立在transformer架构上的基于微调的语言模型，这意味着它创建了一个使用下一个单词预测训练的可预训练的transformer。基于下一个单词预测语言建模产生了一个主要的限制:只有先前的上下文可以用来理解意思。伯特的作者指出:</p><blockquote class="kf kg kh"><p id="813e" class="ki kj kk kl b km kn jr ko kp kq ju kr ks kt ku kv kw kx ky kz la lb lc ld le ij bi translated">[单向注意力的局限性]对于句子级任务来说是次优的，并且在将基于微调的方法应用于标记级任务(如问答)时可能是非常有害的，在标记级任务中，从两个方向结合上下文是至关重要的。</p></blockquote><p id="80c8" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lg kt ku kv lh kx ky kz li lb lc ld le ij bi translated">本质上，如果需要理解一个句子中的<em class="kk">，那么仅仅依靠句子的第一部分将会太受限制。这里有一个例子可以说明这个问题:“今天，我去<strong class="kl ir">超市</strong>买了一些面包和花生酱。”自动更正有两个潜在的候选者:<strong class="kl ir"><em class="kk"/></strong>，<strong class="kl ir"> <em class="kk">流</em> </strong>。两者在拼写上同样接近于“strome ”,当只考虑到这一点的句子时，两者都是可信的。建立在GPT上的自动更正工具将依赖于文档上下文，直到“strome”作出决定。如果批判性语境直接跟着，我们就不能做得更好吗？</em></p><p id="a592" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lg kt ku kv lh kx ky kz li lb lc ld le ij bi translated">Transformer模型需要双向关注，以便将来自两个方向的上下文合并到模型决策中。变压器-解码器(T-D)适合语言建模，因此预先训练，只有单向注意。双向注意力的预训练需要利用转换器-编码器(T-E)，这是双向注意力在T-ED中发生的组件。</p><p id="e228" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lg kt ku kv lh kx ky kz li lb lc ld le ij bi translated">因此，BERT架构基于仅保留T-ED的编码器组件。与GPT的T-D相比，伯特是一个T-E。训练一个T-E不如训练一个T-D简单，我们会看到。尽管如此，现在是回顾我们在过去三篇文章中看到的不同变压器架构的好时机。简而言之，三个占主导地位的NLP模型是:</p><ol class=""><li id="8b57" class="ne nf iq kl b km kn kp kq lg ng lh nh li ni le nj nk nl nm bi translated">变形金刚:T型</li><li id="0c0c" class="ne nf iq kl b km nn kp no lg np lh nq li nr le nj nk nl nm bi translated">GPT: T-D</li><li id="9e3c" class="ne nf iq kl b km nn kp no lg np lh nq li nr le nj nk nl nm bi translated">伯特:T-E</li></ol><p id="b1b5" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lg kt ku kv lh kx ky kz li lb lc ld le ij bi translated">而其他型号的名称有RoBERTa、DistilBert、T5、DialoGPT等。所有这些模型都具有上述三种架构之一。这就是你需要了解的最高层次的变压器架构。</p><h1 id="0e4e" class="mh mi iq bd mj mk ns mm mn mo nt mq mr jw nu jx mt jz nv ka mv kc nw kd mx my bi translated">训练伯特</h1><p id="2e60" class="pw-post-body-paragraph ki kj iq kl b km mz jr ko kp na ju kr lg nb ku kv lh nc ky kz li nd lc ld le ij bi translated">请特别注意这里描述的语言建模任务，因为它们直接决定了这些模型的潜在应用。</p><p id="68c9" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lg kt ku kv lh kx ky kz li lb lc ld le ij bi translated">T-E花了最长的时间才被发表(几乎比T-ED晚了一年)，因为它最不容易被训练。这个问题源于给T-E提供单语任务的全文。如果T-E被输入了完整的输入句子，你能用什么训练任务来挑战它？如果一开始就给它输入了正确的句子，你怎么能让它决定“流”和“存储”中的哪一个构成正确的句子呢？为了解决这个问题，BERT发布了两个不同的训练任务。</p><p id="2a83" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lg kt ku kv lh kx ky kz li lb lc ld le ij bi translated">伯特首先被训练成一个蒙面语言模型(MLM)。MLM需要递给伯特一句话，比如“我坐在椅子上[戴着面具]”，并要求伯特预测这个带面具的单词。下一个单词预测语言建模可以被认为是MLM的一个特例，其中句子中的最后一个单词总是被屏蔽的单词。因此，MLM可以被认为是一种比训练GPT的任务更一般化的语言模型。</p><p id="aeaf" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lg kt ku kv lh kx ky kz li lb lc ld le ij bi translated">在MLM之后，伯特接受了一项名为“下一句话预测”的任务训练。在这个任务中，向BERT传递由指示符分隔的句子对。伯特被训练来预测第二句话是否应该跟在第一句话后面，或者实际上是不相关的。一个例子是这样的:</p><figure class="lk ll lm ln gt lo gh gi paragraph-image"><div role="button" tabindex="0" class="lp lq di lr bf ls"><div class="gh gi nx"><img src="../Images/1e863bf3b0e9709f5a1c82dec73699c1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AKXPVOoEOctekZ4BEz_XbA.png"/></div></div><p class="lv lw gj gh gi lx ly bd b be z dk translated">训练前的下一句预测任务</p></figure><p id="c96e" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lg kt ku kv lh kx ky kz li lb lc ld le ij bi translated">伯特和你一样，应该预测“低概率”。</p><p id="16be" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lg kt ku kv lh kx ky kz li lb lc ld le ij bi translated">这两项任务构成了使BERT容易适应新任务的预训练。</p><h1 id="5d0e" class="mh mi iq bd mj mk ns mm mn mo nt mq mr jw nu jx mt jz nv ka mv kc nw kd mx my bi translated">BERT的应用</h1><p id="fc7d" class="pw-post-body-paragraph ki kj iq kl b km mz jr ko kp na ju kr lg nb ku kv lh nc ky kz li nd lc ld le ij bi translated">理解BERT训练任务对于确定其应用是至关重要的。它是这样工作的:如果你能表明你希望你的产品为客户执行的任务可以被框定为这些培训任务中的一个，那么你的任务就是一个可行的应用。让我们看看谷歌的特色片段，并以谷歌产品经理(PM)鲁本为例。</p><p id="e545" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lg kt ku kv lh kx ky kz li lb lc ld le ij bi translated">鲁本是一名在谷歌搜索工作的项目经理，他对伯特很熟悉。鲁本注意到，许多用户在谷歌搜索栏中输入完整的问题，然后浏览建议的结果来找到他们的答案。他已经确定了一个客户需求:用户有他们需要回答的问题，但他们希望保持他们现有的查询Google搜索栏的行为。</p><p id="e414" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lg kt ku kv lh kx ky kz li lb lc ld le ij bi translated">Reuben密切跟踪一个用户的会话，并观察到以下行为:他的用户在谷歌搜索中键入以下问题:“美国有多少数据科学家。”用户然后点击第一个网站，看到下面的句子:“根据Glassdoor的数据，2019年至2020年，美国数据科学家职位的增长持平，约为6500个。”用户复制这句话，并粘贴到他的谷歌文档。</p><p id="fdde" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lg kt ku kv lh kx ky kz li lb lc ld le ij bi translated">考虑到他目前的产品，Reuben注意到Google的解决方案检索了可能包含用户问题答案的文档集。然后，用户需要筛选这些文档来找到答案。用户以这样一句话开始:“美国有多少数据科学家。”然后选择句子:“根据Glassdoor的数据，2019年至2020年，美国数据科学家职位的增长持平，约为6500个。”鲁本意识到这项任务可以转化为下一句预测的伯特训练任务。他可以使用BERT基于句子将“跟随”查询句子的概率来对检索到的语料库中的句子进行排序。</p><p id="d8ed" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lg kt ku kv lh kx ky kz li lb lc ld le ij bi translated">实际上，他希望他的BERT输出看起来像这样:</p><figure class="lk ll lm ln gt lo gh gi paragraph-image"><div role="button" tabindex="0" class="lp lq di lr bf ls"><div class="gh gi ny"><img src="../Images/d1e2bd01cbc8c7add6e7bae7a1e437e8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*p1OKvOfSi7fs0dMZMlOODQ.png"/></div></div><p class="lv lw gj gh gi lx ly bd b be z dk translated">用于微调的BERT问答任务</p></figure><p id="4564" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lg kt ku kv lh kx ky kz li lb lc ld le ij bi translated">将一个领域特定的任务转化为一个transformer培训任务本质上是对您的模型进行微调的过程。为了开发有特色的片段，Reuben希望对BERT进行微调，将它的行为从查找最有可能跟随查询的句子转变为查找最有可能回答用户问题的句子。鲁本可以通过在一组更直接符合这种模式的句子对上进一步训练伯特来做到这一点。</p><h1 id="c135" class="mh mi iq bd mj mk ns mm mn mo nt mq mr jw nu jx mt jz nv ka mv kc nw kd mx my bi translated">现在都在一起</h1><p id="5f03" class="pw-post-body-paragraph ki kj iq kl b km mz jr ko kp na ju kr lg nb ku kv lh nc ky kz li nd lc ld le ij bi translated">同样的过程可以用来理解变压器模型的一般应用。让我们快速回顾一下我们所涉及的三种模型架构的培训任务及其关注机制:</p><ol class=""><li id="90fa" class="ne nf iq kl b km kn kp kq lg ng lh nh li ni le nj nk nl nm bi translated">T-ED。任务:翻译。</li><li id="0df8" class="ne nf iq kl b km nn kp no lg np lh nq li nr le nj nk nl nm bi translated">任务:下一个单词的预测。</li><li id="5d1c" class="ne nf iq kl b km nn kp no lg np lh nq li nr le nj nk nl nm bi translated">任务:1。MLM。2.下一句预测。</li></ol><p id="b219" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lg kt ku kv lh kx ky kz li lb lc ld le ij bi translated">在句子中间进行拼写检查:用掩码替换拼错的单词:MLM。将非正式英语转换成正式英语:翻译。建议如何回复短信:下一个单词预测。等等。等等。</p><p id="3f00" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lg kt ku kv lh kx ky kz li lb lc ld le ij bi translated">确定应用与型号选择相关，但并不相同。正如谷歌研究人员在T5型号(T-ED)的<a class="ae lf" href="https://ai.googleblog.com/2020/02/exploring-transfer-learning-with-t5.html" rel="noopener ugc nofollow" target="_blank">发布</a>中指出的:</p><blockquote class="kf kg kh"><p id="8b1a" class="ki kj kk kl b km kn jr ko kp kq ju kr ks kt ku kv kw kx ky kz la lb lc ld le ij bi translated">我们建议将所有NLP任务重新构建为统一的文本到文本格式，其中输入和输出始终是文本字符串…我们甚至可以将T5应用于回归任务，通过训练它来预测数字的字符串表示而不是数字本身。</p></blockquote><p id="a319" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lg kt ku kv lh kx ky kz li lb lc ld le ij bi translated">正如作者指出的，在某种程度上，所有的任务都可以用文本到文本的格式来组织。也许T-ED可以用于所有的任务，而且任何任务都应该适合T-ED模型。虽然这在数学上有意义，但在商业上却常常没有意义。你可以把写博士论文框定为一项下一个单词的预测任务，进入GPT:“这是我关于人工智能的博士论文”，然后等着GPT完成剩下的工作。不幸的是，他们最终可能会上交垃圾，让他们的项目失败，然后质疑他们的未来。</p><p id="e357" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lg kt ku kv lh kx ky kz li lb lc ld le ij bi translated">那么，如何才能知道一个应用程序对于一个transformer模型是否真的可行呢？根据经验，应用程序任务越直接转化为训练任务，模型的性能就越好。如上所述，每个下一句预测任务都可以被公式化为一个翻译任务，但是T-E会以更低的成本产生更好的结果。</p><p id="8168" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lg kt ku kv lh kx ky kz li lb lc ld le ij bi translated">总之，请记住，这些模型通过利用在来自训练的大量文本语料库中检测到的模式来确定新的基于文本的任务中最有可能的输出。模型性能受限于训练语料库中包含的内容。变形金刚模型不会有所发现或者搞清楚事情。它们自动执行基于文本的任务，这些任务通过包含在数百万现有文档中的模式变得可预测。尽管如此，他们承诺彻底改革我们所知的人工智能。</p></div><div class="ab cl ma mb hu mc" role="separator"><span class="md bw bk me mf mg"/><span class="md bw bk me mf mg"/><span class="md bw bk me mf"/></div><div class="ij ik il im in"><p id="6d11" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lg kt ku kv lh kx ky kz li lb lc ld le ij bi translated">参考</p><p id="4dd5" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lg kt ku kv lh kx ky kz li lb lc ld le ij bi translated">[1]纳亚克，潘杜。“比以往任何时候都更好地理解搜索。”<em class="kk">谷歌</em>，谷歌，2019年10月25日，blog . Google/products/search/search-language-understanding-Bert/。</p><p id="fd17" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lg kt ku kv lh kx ky kz li lb lc ld le ij bi translated">[2] Jacob Devlin、张明蔚、Kenton Lee和Kristina Toutanova。Bert:用于语言理解的深度双向转换器的预训练。arXiv预印本arXiv:1810.04805，2018。</p><p id="39dc" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lg kt ku kv lh kx ky kz li lb lc ld le ij bi translated">[3]“用T5探索迁移学习:文本到文本的迁移转换器。”<em class="kk">谷歌人工智能博客</em>，2020年2月24日，AI . Google Blog . com/2020/02/exploring-transfer-learning-with-t5 . html</p><p id="0f05" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lg kt ku kv lh kx ky kz li lb lc ld le ij bi translated">作者创作的所有图像</p></div></div>    
</body>
</html>