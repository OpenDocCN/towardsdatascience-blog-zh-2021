<html>
<head>
<title>Lipschitz Continuity and Spectral Normalization</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Lipschitz连续性和谱归一化</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/lipschitz-continuity-and-spectral-normalization-b03b36066b0d?source=collection_archive---------10-----------------------#2021-08-21">https://towardsdatascience.com/lipschitz-continuity-and-spectral-normalization-b03b36066b0d?source=collection_archive---------10-----------------------#2021-08-21</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="c1e7" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">如何让神经网络更健壮</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/1b40e3051ae251d57c09cd45cc7b418f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*sLGVKrDbq-QAepLO"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">戴维·布鲁克·马丁在<a class="ae kv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="bd05" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">神经网络最大的弱点之一是它们对输入过于敏感。神经网络缺乏鲁棒性的最著名的例子可能是计算机视觉中的对立例子，其中图像像素的微小而难以察觉的扰动能够完全改变神经网络的输出。理想情况下，如果我们有相似的输入，我们希望输出也相似。李普希茨连续性是一个数学性质，使这一概念具体化。在本文中，我们将看到Lipschitz连续性如何用于深度学习，以及它如何激发一种称为谱归一化的新正则化方法。</p></div><div class="ab cl ls lt hu lu" role="separator"><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx"/></div><div class="ij ik il im in"><h2 id="6b38" class="lz ma iq bd mb mc md dn me mf mg dp mh lf mi mj mk lj ml mm mn ln mo mp mq mr bi translated">李普希茨连续性</h2><p id="318f" class="pw-post-body-paragraph kw kx iq ky b kz ms jr lb lc mt ju le lf mu lh li lj mv ll lm ln mw lp lq lr ij bi translated">让我们从李普希茨连续性的定义开始:</p><pre class="kg kh ki kj gt mx my mz na aw nb bi"><span id="4950" class="lz ma iq my b gy nc nd l ne nf">A function <em class="ng">f </em>: <em class="ng">Rᴹ</em> → <em class="ng">Rᴺ</em> is Lipschitz continuous if there is a constant <em class="ng">L</em> such that</span><span id="0fee" class="lz ma iq my b gy nh nd l ne nf">∥<em class="ng">f</em>(<em class="ng">x</em>) - <em class="ng">f</em>(<em class="ng">y</em>)∥  ≦  <em class="ng">L </em>∥<em class="ng">x</em> - <em class="ng">y</em>∥ for every <em class="ng">x</em>, <em class="ng">y</em>.</span></pre><p id="e236" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这里∨∨表示通常的欧几里德距离。最小的这种<em class="ng"> L </em>是<em class="ng"> f </em>的李普希兹常数，记为<em class="ng"> Lip </em> ( <em class="ng"> f </em>)。注意，这个定义可以推广到任意度量空间之间的函数。</p><p id="42ba" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在我们的例子中，<em class="ng"> f </em>是我们的神经网络，我们希望它是Lipschitz连续的，带有一个小的<em class="ng"> Lip </em> ( <em class="ng"> f </em>)。这将为输出的扰动提供一个上限。Lipschitz连续性还具有以下性质:</p><pre class="kg kh ki kj gt mx my mz na aw nb bi"><span id="8283" class="lz ma iq my b gy nc nd l ne nf">Let <em class="ng">f</em> = <em class="ng">g</em> ∘ <em class="ng">h</em>. If <em class="ng">g</em> and <em class="ng">h</em> are Lipschitz continuous, then <em class="ng">f </em>is also Lipschitz continuous with <em class="ng">Lip</em>(<em class="ng">f</em>)<em class="ng"> </em>≦ <em class="ng">Lip</em>(<em class="ng">g</em>)<em class="ng"> Lip</em>(<em class="ng">h</em>)<em class="ng">.</em></span></pre><p id="bc26" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">因此，只要我们使一个神经网络的每个分量都是具有小的Lipschitz常数的Lipschitz连续的，那么整个神经网络也将是具有小的Lipschitz常数的Lipschitz连续的。</p><p id="8850" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">作为一个具体的例子，用于二进制分类的标准2层前馈网络可以写成</p><p id="b7f2" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">西格蒙德·∘fc₂∘·雷卢·∘fc₁</p><p id="e9ac" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">其中fcᵢ(<em class="ng">x</em>)=<em class="ng">w</em>ᵢ<em class="ng">x</em>+<em class="ng">b</em>ᵢ为全连通层。<em class="ng"> f </em>的部件有FC₁、热卢、FC₂和乙状结肠。</p></div><div class="ab cl ls lt hu lu" role="separator"><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx"/></div><div class="ij ik il im in"><h2 id="f951" class="lz ma iq bd mb mc md dn me mf mg dp mh lf mi mj mk lj ml mm mn ln mo mp mq mr bi translated">激活功能和池化</h2><p id="3928" class="pw-post-body-paragraph kw kx iq ky b kz ms jr lb lc mt ju le lf mu lh li lj mv ll lm ln mw lp lq lr ij bi translated">神经网络中常用的函数(如ReLU、sigmoid、softmax、tanh、max-pooling)的Lipschitz常数= 1。因此我们可以简单地继续使用它们。</p></div><div class="ab cl ls lt hu lu" role="separator"><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx"/></div><div class="ij ik il im in"><h2 id="bc20" class="lz ma iq bd mb mc md dn me mf mg dp mh lf mi mj mk lj ml mm mn ln mo mp mq mr bi translated">光谱归一化</h2><p id="460d" class="pw-post-body-paragraph kw kx iq ky b kz ms jr lb lc mt ju le lf mu lh li lj mv ll lm ln mw lp lq lr ij bi translated">让我们考虑一个全连接层。为了简单起见，我们省略了偏差项，所以对于某些权重矩阵<em class="ng"> W </em>，FC( <em class="ng"> x </em> ) = <em class="ng"> Wx </em>。可以看出FC有Lipschitz常数<em class="ng">Lip</em>(FC)<em class="ng"/>=σ(<em class="ng">W</em>)的谱范数<em class="ng"> W </em>，相当于<em class="ng"> W </em>的最大奇异值。因此一般来说，<em class="ng"> Lip </em> (FC) <em class="ng"> </em>可以任意大。</p><p id="9810" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">光谱归一化通过归一化<em class="ng"> W </em>的光谱范数发挥作用:</p><pre class="kg kh ki kj gt mx my mz na aw nb bi"><span id="58d6" class="lz ma iq my b gy nc nd l ne nf"><em class="ng">W</em> ↤ <em class="ng">W</em> / σ(<em class="ng">W</em>)</span></pre><p id="23c9" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">归一化矩阵的谱范数= 1，因此<em class="ng"> Lip </em> (FC)也是1。</p><p id="5cfb" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">同样的想法也可以应用于卷积层。请注意，卷积是线性运算，因此可以将它们重写为适当大小的矩阵，然后重用上面的思想。然而，这些矩阵及其谱范数的计算并不简单，我们可以参考[3]了解详细信息。</p></div><div class="ab cl ls lt hu lu" role="separator"><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx"/></div><div class="ij ik il im in"><h2 id="2c61" class="lz ma iq bd mb mc md dn me mf mg dp mh lf mi mj mk lj ml mm mn ln mo mp mq mr bi translated">光谱归一化的实现</h2><ul class=""><li id="e76b" class="ni nj iq ky b kz ms lc mt lf nk lj nl ln nm lr nn no np nq bi translated">张量流:<a class="ae kv" href="https://www.tensorflow.org/addons/api_docs/python/tfa/layers/SpectralNormalization" rel="noopener ugc nofollow" target="_blank">TFA . layers . spectral normalization</a></li><li id="84b3" class="ni nj iq ky b kz nr lc ns lf nt lj nu ln nv lr nn no np nq bi translated">py torch:<a class="ae kv" href="https://pytorch.org/docs/stable/generated/torch.nn.utils.parametrizations.spectral_norm.html" rel="noopener ugc nofollow" target="_blank">torch . nn . utils . parameterizations . spectral _ norm</a></li></ul></div><div class="ab cl ls lt hu lu" role="separator"><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx"/></div><div class="ij ik il im in"><h2 id="bb7e" class="lz ma iq bd mb mc md dn me mf mg dp mh lf mi mj mk lj ml mm mn ln mo mp mq mr bi translated">光谱归一化的其他好处</h2><p id="d739" class="pw-post-body-paragraph kw kx iq ky b kz ms jr lb lc mt ju le lf mu lh li lj mv ll lm ln mw lp lq lr ij bi translated">假设我们有一个由全连接层、卷积层和常用激活函数组成的神经网络<em class="ng"> f </em>。通过对完全连接的层和卷积层应用频谱归一化，我们从上面用1来限制<em class="ng"> Lip </em> ( <em class="ng"> f </em>)。</p><p id="693a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">如果我们的神经网络具有其他种类的组件，那么对全连接层和卷积层应用频谱归一化不足以限制<em class="ng"> Lip </em> ( <em class="ng"> f </em>)。尽管如此，这样做可能仍然是有益的，因为光谱归一化控制爆炸梯度和消失梯度，详细分析见[6]。</p></div><div class="ab cl ls lt hu lu" role="separator"><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx"/></div><div class="ij ik il im in"><h2 id="4c42" class="lz ma iq bd mb mc md dn me mf mg dp mh lf mi mj mk lj ml mm mn ln mo mp mq mr bi translated">进一步阅读</h2><ol class=""><li id="2c21" class="ni nj iq ky b kz ms lc mt lf nk lj nl ln nm lr nw no np nq bi translated">[3]研究了除欧几里德范数之外的其他ₚ范数的李普希茨连续性。</li><li id="48a8" class="ni nj iq ky b kz nr lc ns lf nt lj nu ln nv lr nw no np nq bi translated">谱归一化首先在[7]中被引入，以稳定生成对抗网络(GAN)的鉴别器的训练。在GAN框架下，对频谱归一化与其他归一化和正则化方法进行了大量比较研究，例如参见[5]。最近，谱归一化在强化学习中也取得了成功，参见[2]。</li><li id="412f" class="ni nj iq ky b kz nr lc ns lf nt lj nu ln nv lr nw no np nq bi translated">在上一节中，我们已经讨论了完全连接层和卷积层。这里我们为其他架构提供参考:<br/> —剩余连接的Lipschitz常数在【3】中研究。<br/> —注意层不是Lipschitz连续的。[1]和[4]提出了不同的修改以使它们Lipschitz连续。</li></ol></div><div class="ab cl ls lt hu lu" role="separator"><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx"/></div><div class="ij ik il im in"><h2 id="15be" class="lz ma iq bd mb mc md dn me mf mg dp mh lf mi mj mk lj ml mm mn ln mo mp mq mr bi translated">参考</h2><ol class=""><li id="bbc7" class="ni nj iq ky b kz ms lc mt lf nk lj nl ln nm lr nw no np nq bi translated">G.达苏拉斯，k .斯卡曼和a .维尔莫。<a class="ae kv" href="http://proceedings.mlr.press/v139/dasoulas21a/dasoulas21a.pdf" rel="noopener ugc nofollow" target="_blank"/>(2021)，ICML 2021，自我注意层的李普希茨标准化及其在图形神经网络中的应用。</li><li id="5cee" class="ni nj iq ky b kz nr lc ns lf nt lj nu ln nv lr nw no np nq bi translated">F.Gogianu、T. Berariu、M. Rosca、C. Clopath、L. Busoniu和R. Pascanu。<a class="ae kv" href="https://arxiv.org/abs/2105.05246" rel="noopener ugc nofollow" target="_blank">深度强化学习的光谱归一化:优化视角</a> (2021)，ICML 2021。</li><li id="e620" class="ni nj iq ky b kz nr lc ns lf nt lj nu ln nv lr nw no np nq bi translated">H.Gouk，E. Frank，B. Pfahringer和M. J. Cree。<a class="ae kv" href="https://doi.org/10.1007/s10994-020-05929-w" rel="noopener ugc nofollow" target="_blank">通过加强Lipschitz连续性来调整神经网络</a> (2021)，Mach Learn <strong class="ky ir"> 110 </strong>，393–416。</li><li id="7e8d" class="ni nj iq ky b kz nr lc ns lf nt lj nu ln nv lr nw no np nq bi translated">H.Kim、G. Papamakarios和A. Mnih。<a class="ae kv" href="https://arxiv.org/abs/2006.04710" rel="noopener ugc nofollow" target="_blank">自我注意的李普希茨常数</a> (2021)，ICML 2021。</li><li id="2647" class="ni nj iq ky b kz nr lc ns lf nt lj nu ln nv lr nw no np nq bi translated">K.Kurach、M. Lucic、X. Zhai、M. Michalski和S. Gelly。<a class="ae kv" href="https://arxiv.org/abs/1807.04720" rel="noopener ugc nofollow" target="_blank">GANs正规化、规范化大规模研究</a> (2019)，ICML 2019。</li><li id="c641" class="ni nj iq ky b kz nr lc ns lf nt lj nu ln nv lr nw no np nq bi translated">Z.林、塞卡尔和范蒂。<a class="ae kv" href="https://arxiv.org/abs/2009.02773" rel="noopener ugc nofollow" target="_blank">为什么光谱归一化会稳定GANs:分析和改进</a> (2021)，arXiv预印本。</li><li id="7fe3" class="ni nj iq ky b kz nr lc ns lf nt lj nu ln nv lr nw no np nq bi translated">T.宫藤，t .片冈，m .小山和y .吉田。<a class="ae kv" href="https://arxiv.org/abs/1802.05957" rel="noopener ugc nofollow" target="_blank">生成对抗网络的谱归一化</a> (2018)，ICLR 2018。</li></ol></div></div>    
</body>
</html>