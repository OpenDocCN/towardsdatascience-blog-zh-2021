<html>
<head>
<title>Build Better Regression Models With LASSO</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用LASSO构建更好的回归模型</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/build-better-regression-models-with-lasso-271ce0f22bd?source=collection_archive---------10-----------------------#2021-09-01">https://towardsdatascience.com/build-better-regression-models-with-lasso-271ce0f22bd?source=collection_archive---------10-----------------------#2021-09-01</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="183d" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">学习如何在SciKit中实现LASSO学习提高线性回归的可解释性和性能</h2></div><p id="7a60" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">由<a class="ae le" href="https://www.linkedin.com/in/edkrueger/" rel="noopener ugc nofollow" target="_blank">爱德华·克鲁格</a>和<a class="ae le" href="https://www.linkedin.com/in/erin-oefelein-3105a878/" rel="noopener ugc nofollow" target="_blank">艾琳·欧菲琳</a></p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi lf"><img src="../Images/1ebd92da43841a490a868f57ff8370ad.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3c4B6ftMDLYF9-WeE8hqNw.jpeg"/></div></div><p class="lr ls gj gh gi lt lu bd b be z dk translated">照片由<a class="ae le" href="https://unsplash.com/@priscilladupreez?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">普里西拉·杜·普里兹</a>在<a class="ae le" href="https://unsplash.com/s/photos/lasso?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>拍摄</p></figure><p id="b70c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">在本文</strong>中，我们将介绍使用LASSO回归需要了解的基础知识:</p><ul class=""><li id="f0ce" class="lv lw it kk b kl km ko kp kr lx kv ly kz lz ld ma mb mc md bi translated">我们将简要介绍拉索背后的<strong class="kk iu">理论</strong>。</li><li id="bd39" class="lv lw it kk b kl me ko mf kr mg kv mh kz mi ld ma mb mc md bi translated">我们将讨论为什么正确使用套索需要具有<strong class="kk iu">相似比例</strong>的特征。</li><li id="27cc" class="lv lw it kk b kl me ko mf kr mg kv mh kz mi ld ma mb mc md bi translated">我们将讲述<strong class="kk iu">如何用<strong class="kk iu">标准化特征</strong>解释线性回归和套索回归中的系数</strong>。</li><li id="5470" class="lv lw it kk b kl me ko mf kr mg kv mh kz mi ld ma mb mc md bi translated">我们将介绍数据集，并给出一些见解<strong class="kk iu">为什么LASSO帮助</strong>。</li><li id="4c25" class="lv lw it kk b kl me ko mf kr mg kv mh kz mi ld ma mb mc md bi translated">我们将在<strong class="kk iu"> SciKit-Learn </strong>中展示如何实现线性回归、<strong class="kk iu">套索回归</strong>和岭回归。</li></ul><h1 id="5a07" class="mj mk it bd ml mm mn mo mp mq mr ms mt jz mu ka mv kc mw kd mx kf my kg mz na bi translated">浏览理论的表面</h1><p id="979b" class="pw-post-body-paragraph ki kj it kk b kl nb ju kn ko nc jx kq kr nd kt ku kv ne kx ky kz nf lb lc ld im bi translated">在上一篇文章中，我们讨论了LASSO如何以及为什么提高广义线性模型的可解释性和准确性。我们将在这里重述基础知识，但是如果你有兴趣深入研究这个理论，可以看看下面的文章。</p><div class="ng nh gp gr ni nj"><a href="https://edkruegerdata.com/lasso-increases-the-interpretability-and-accuracy-of-linear-models-c1b340561c10" rel="noopener  ugc nofollow" target="_blank"><div class="nk ab fo"><div class="nl ab nm cl cj nn"><h2 class="bd iu gy z fp no fr fs np fu fw is bi translated">LASSO增加了线性模型的可解释性和准确性</h2><div class="nq l"><h3 class="bd b gy z fp no fr fs np fu fw dk translated">了解套索的工作原理和原因</h3></div><div class="nr l"><p class="bd b dl z fp no fr fs np fu fw dk translated">edkruegerdata.com</p></div></div><div class="ns l"><div class="nt l nu nv nw ns nx lp nj"/></div></div></a></div><p id="d339" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们可以使用套索通过选择特征来改善模型中的过度拟合。它与线性回归，逻辑回归和其他几个模型。本质上，如果模型有系数，可以使用LASSO。与其他特征选择技术不同，LASSO中的特征选择是内生的。即，它出现在模型的算法内部。LASSO不是查看要素的每个组合或实施逐步子集选择，而是在算法中选择要素，自动生成子集选择。</p><p id="9762" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">LASSO的工作原理是对损失函数应用L1罚函数。由于L1罚函数所隐含的约束区域的形状，LASSO可能选择系数的备用估计。即，一些系数可能被设置为0，并且实际上，这些特征被移除。惩罚系数越大，趋势越明显。</p><p id="a031" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">具有较少的特征通常会减少模型中的过度拟合，这可能会提高模型的可解释性。提高模型的复杂性通常会减少测试误差。</p><h1 id="e9af" class="mj mk it bd ml mm mn mo mp mq mr ms mt jz mu ka mv kc mw kd mx kf my kg mz na bi translated">数据集</h1><p id="1878" class="pw-post-body-paragraph ki kj it kk b kl nb ju kn ko nc jx kq kr nd kt ku kv ne kx ky kz nf lb lc ld im bi translated">我们将使用SciKit-Learn的糖尿病数据集演示LASSO。你可以在这里找到数据集<a class="ae le" href="https://scikit-learn.org/stable/datasets/toy_dataset.html#diabetes-dataset" rel="noopener ugc nofollow" target="_blank"/>【1】。该数据集非常适合演示LASSO，因为它的所有要素都是数值型的。当分类变量存在时，可以使用LASSO，但当缩放时，它们可能很难解释。</p><p id="86ff" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">数据集中的目标是对糖尿病进展的数值评估。这些特征包括年龄、性别、体重指数(身体质量指数)、血压和标记为“s1”到“s6”的六个血清测量值。</p><p id="1452" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">使用与<code class="fe ny nz oa ob b">StandardScaler</code>使用的过程相同的过程对特征进行缩放。但是，整个数据集已被用于计算每个要素的平均值和标准差，因此从技术上讲，存在一些可能无害的数据泄漏。我们将使用管道中的<code class="fe ny nz oa ob b">StandardScaler</code>来重新调整数据集，该管道将仅使用训练集来重新调整数据，以计算平均值和标准差。这将使我们的演示适用于尚未缩放的数据。</p><p id="077e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们来看看特征之间的相关性。我们在下面包括相关矩阵和相关的绝对值。通过查看绝对值，可以更容易地看出相关性的强度。</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi oc"><img src="../Images/6cc6c2a0c520a49e3af0f6761bc6dfaf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uF8RsDZUY5iPLkrppUrhug.png"/></div></div><p class="lr ls gj gh gi lt lu bd b be z dk translated">相关矩阵(来源:作者)</p></figure><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi od"><img src="../Images/ba1b728a50643c56418fcf013af04e2b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_OnSp-CgWvOpp1qHd4KPWA.png"/></div></div><p class="lr ls gj gh gi lt lu bd b be z dk translated">相关矩阵的逐元素绝对值(来源:作者)</p></figure><p id="a184" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">查看特性之间的相关性，我们可以看到特性之间存在一些共线性。特别地，s3和s4高度负相关，s2和s4高度相关。所有的血清测量值都有显著的相关性。LASSO更有可能移除相关要素集中的要素，因此如果LASSO移除要素s2、s3和s4中的一个或两个，也不会令人惊讶。另一方面，LASSO基于交叉验证的度量选择正则化强度，即惩罚强度。因此，如果一个特征背后有一些额外的解释力，即使它与另一个特征密切相关，也可能不会被删除。</p><h1 id="18d9" class="mj mk it bd ml mm mn mo mp mq mr ms mt jz mu ka mv kc mw kd mx kf my kg mz na bi translated">缩放对套索很重要</h1><p id="a352" class="pw-post-body-paragraph ki kj it kk b kl nb ju kn ko nc jx kq kr nd kt ku kv ne kx ky kz nf lb lc ld im bi translated">在我们进入代码之前，我们需要更深入地理解缩放。如果我们想在SciKit-Learn中正确地应用LASSO，我们需要首先缩放我们的数据。与线性回归不同，LASSO中的要素缩放至关重要。这是因为LASSO的罚函数包含了特征系数的绝对值之和。</p><p id="4415" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">为了更好地理解这一点，让我们来理解缩放是如何影响线性回归的。</p><h2 id="90b8" class="oe mk it bd ml of og dn mp oh oi dp mt kr oj ok mv kv ol om mx kz on oo mz op bi translated">线性回归的拟合不受线性变换的影响</h2><p id="7af8" class="pw-post-body-paragraph ki kj it kk b kl nb ju kn ko nc jx kq kr nd kt ku kv ne kx ky kz nf lb lc ld im bi translated">虽然这并不能完全构成一个证明，但让我们看看为什么“<em class="oq">特征的线性变换不会影响线性回归做出的预测”</em>的说法是正确的。我们会看看一元线性回归，但是</p><p id="84fd" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">给定一个特征“x”，对于任何数字“a”和“b”，该特征的线性变换可以写成如下形式。</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi or"><img src="../Images/437a18c4333bd98ff7b0712921bcbc13.png" data-original-src="https://miro.medium.com/v2/resize:fit:784/format:webp/1*PC0X6L-xaqJSMeC7reKFWA.png"/></div></div><p class="lr ls gj gh gi lt lu bd b be z dk translated">x的线性变换(来源:作者)</p></figure><p id="ccac" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">回想一下，特征“x”的单变量线性回归的形式可以写成如下形式。优化选择系数，使平方和最小。</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi os"><img src="../Images/8f3d0bf850f42d2892ad317eee73ef69.png" data-original-src="https://miro.medium.com/v2/resize:fit:892/format:webp/1*xH5tpHp8BaeNYWNQwPjqSA.png"/></div><p class="lr ls gj gh gi lt lu bd b be z dk translated">x的线性回归形式(来源:作者)</p></figure><p id="4d8c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">对于转换后的特征，可以写成如下形式。再次选择系数以最小化平方和。</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi ot"><img src="../Images/80ddb57efd7a473acd0a4d258eae90eb.png" data-original-src="https://miro.medium.com/v2/resize:fit:924/format:webp/1*q9AujOxmV0odhFUFhgQYfg.png"/></div><p class="lr ls gj gh gi lt lu bd b be z dk translated">变换特征的线性回归形式(来源:作者)</p></figure><p id="8308" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在定义中代入转换后的特征，我们得到以下结果。</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi ou"><img src="../Images/00398b5d8f5717d533829604ae69a39a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NVM2vCFF6x--6z69gfOxeQ.png"/></div></div><p class="lr ls gj gh gi lt lu bd b be z dk translated">重写的变换特征的线性形式(来源:作者)</p></figure><p id="f08c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们看看分组术语。对于这两项，总项可以取任何值，并且我们已经从未变换的问题中知道了最佳系数。因此，实际上，转换后的形式等同于未转换的形式。</p><h2 id="8520" class="oe mk it bd ml of og dn mp oh oi dp mt kr oj ok mv kv ol om mx kz on oo mz op bi translated">这是系数的变化</h2><p id="9a0e" class="pw-post-body-paragraph ki kj it kk b kl nb ju kn ko nc jx kq kr nd kt ku kv ne kx ky kz nf lb lc ld im bi translated">在其他条件不变的情况下，如果我们将一个特征除以1000，那么系数会发生什么变化？举个更具体的例子，如果我们以“米”为特征，1米对目标的作用是1，那么1公里对目标的作用是什么？</p><p id="e349" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">看看上面的数学，或者咨询一下你的直觉。转换的效果是将系数乘以100。</p><p id="ee08" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">记住LASSO根据系数的大小增加了一个惩罚，现在看到缩放可以影响系数，我们可以看到不同的缩放会如何导致LASSO中最佳系数的不同选择。</p><h2 id="0917" class="oe mk it bd ml of og dn mp oh oi dp mt kr oj ok mv kv ol om mx kz on oo mz op bi translated">标准化是一种线性转换</h2><p id="ee7d" class="pw-post-body-paragraph ki kj it kk b kl nb ju kn ko nc jx kq kr nd kt ku kv ne kx ky kz nf lb lc ld im bi translated">在SciKit-Learn中作为<code class="fe ny nz oa ob b">StandardScaler</code>提供的标准化是一种线性转换。如果你想确定的话，看看SciKit-Learn文档中定义的<a class="ae le" href="http://SciKit-Learn" rel="noopener ugc nofollow" target="_blank">这里的</a>缩放程序，并把它改写成线性变换的形式。<code class="fe ny nz oa ob b">MinMaxScalar</code>也采用线性变换。</p><h2 id="9779" class="oe mk it bd ml of og dn mp oh oi dp mt kr oj ok mv kv ol om mx kz on oo mz op bi translated">用比例特征解释线性回归</h2><p id="22ec" class="pw-post-body-paragraph ki kj it kk b kl nb ju kn ko nc jx kq kr nd kt ku kv ne kx ky kz nf lb lc ld im bi translated"><code class="fe ny nz oa ob b">StandardScaler</code>通过将每个观测值转换为一个新的尺度来单独转换每个特征，该尺度表示观测值相对于平均值的标准偏差。</p><h1 id="74cd" class="mj mk it bd ml mm mn mo mp mq mr ms mt jz mu ka mv kc mw kd mx kf my kg mz na bi translated">让我们来看看代码</h1><p id="9bf6" class="pw-post-body-paragraph ki kj it kk b kl nb ju kn ko nc jx kq kr nd kt ku kv ne kx ky kz nf lb lc ld im bi translated">我们将跳过如何加载数据集并将其分成训练集和测试集。如果您想更深入地了解这一部分，可以在这里查看与本文<a class="ae le" href="https://github.com/edkrueger/lasso-demo" rel="noopener ugc nofollow" target="_blank">相关的资源库。</a></p><h2 id="e9f3" class="oe mk it bd ml of og dn mp oh oi dp mt kr oj ok mv kv ol om mx kz on oo mz op bi translated">线性回归</h2><p id="49cd" class="pw-post-body-paragraph ki kj it kk b kl nb ju kn ko nc jx kq kr nd kt ku kv ne kx ky kz nf lb lc ld im bi translated">为了预先用缩放步骤拟合线性回归，我们可以使用SciKit-Learn <code class="fe ny nz oa ob b">Pipeline</code>。SciKit-Learn提供了一个简单的接口来创建一个带有<code class="fe ny nz oa ob b">make_pipline</code>的<code class="fe ny nz oa ob b">Pipeline</code>，我们将在下面使用。</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi ov"><img src="../Images/9cc73d695ddefef617b9eb87874258dd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LwLffXZiuI4cbf47Oe3KHA.png"/></div></div><p class="lr ls gj gh gi lt lu bd b be z dk translated">制作和训练线性回归管道(来源:作者)</p></figure><p id="3b1e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">因为以模型结尾的<code class="fe ny nz oa ob b">Pipeline</code>——从技术上讲是<code class="fe ny nz oa ob b">Estimator</code>的子类——行为就像模型一样。由于这条管道在一个模型实例<code class="fe ny nz oa ob b">LinearRegression</code>中结束，我们可以用<code class="fe ny nz oa ob b">.fit()</code>方法训练它。</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi ow"><img src="../Images/4bd89ada67f20a607dca34ea27cf2544.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Szo5RBWICeqQHC-ShkwGvg.png"/></div></div><p class="lr ls gj gh gi lt lu bd b be z dk translated">计算MSE(来源:作者)</p></figure><p id="7c3a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">为了验证<code class="fe ny nz oa ob b">Pipline</code>实例，我们可以使用它的predict方法来获得估计值并计算均方误差(MSE)。这里我们的MSE是2900左右。</p><p id="679d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">为了得到系数，我们必须从流水线的最后一步提取它们。幸运的是，<code class="fe ny nz oa ob b">Pipeline</code>中的步骤可以像数组元素一样被索引。我们使用<code class="fe ny nz oa ob b">-1</code>索引糖来检索线性回归。</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi ox"><img src="../Images/e7d5b7b14cc1ae8b4d787574306cc64e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pPtXeyXfvFmVW7NEDOzImg.png"/></div></div><p class="lr ls gj gh gi lt lu bd b be z dk translated">提取和显示线性回归系数(来源:作者)</p></figure><p id="544c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">为了解释这些系数，请记住，每个特征都经过缩放，因此一个单位的变化代表标准差的一个单位的变化。因此，对于每个特征，我们可以将每个系数解释为目标变化对平均值的一个标准差的影响。例如，以身体质量指数为例，身体质量指数值与平均值的一个标准差会使一个人的糖尿病进展增加25.6倍。</p><h2 id="7b47" class="oe mk it bd ml of og dn mp oh oi dp mt kr oj ok mv kv ol om mx kz on oo mz op bi translated">套索回归</h2><p id="62c8" class="pw-post-body-paragraph ki kj it kk b kl nb ju kn ko nc jx kq kr nd kt ku kv ne kx ky kz nf lb lc ld im bi translated">sci kit-了解回归问题中LASSO的一些不同实现。最常见的两个<code class="fe ny nz oa ob b">Lasso</code>和<code class="fe ny nz oa ob b">LassoCV</code>。你可以在这里找到文档页面<a class="ae le" href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html#sklearn.linear_model.Lasso" rel="noopener ugc nofollow" target="_blank">这里</a>和<a class="ae le" href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LassoCV.html#sklearn.linear_model.LassoCV" rel="noopener ugc nofollow" target="_blank">这里</a>。两者的区别在于<code class="fe ny nz oa ob b">Lasso</code>希望您设置惩罚，而<code class="fe ny nz oa ob b">LassoCV</code>使用交叉验证的MSE (CV-MSE)执行网格搜索，以找到正则化强度的最佳选择。实际上，除非您以前处理过相同或非常相似的数据集，否则您永远不会知道罚超参数的理想值。因此，在实践中，您几乎总是会使用<code class="fe ny nz oa ob b">LassoCV</code>——这里我们也是这样做的。</p><p id="93ff" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">与使用管道时的情况一样，用于训练和验证的代码几乎是相同的——这是使用管道的优势之一。除了从<code class="fe ny nz oa ob b">LassoCV</code>的实例中提取系数，我们还可以根据CV-MSE提取最优的正则化str。让我们看看结果。</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi oy"><img src="../Images/23a42704e8b1c409bf70b4e475d8e8a2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1262/format:webp/1*CVGQRwIclK4obFw1jgb5Nw.png"/></div><p class="lr ls gj gh gi lt lu bd b be z dk translated">Lasso回归的代码和结果(来源:作者)</p></figure><p id="65eb" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">请注意，MSE略有下降，这意味着我们得到了一个更准确的模型。通过查看系数，我们可以看到线性回归和套索之间的变化。LASSO将年龄s2和s4设置为0，有效地将它们从模型中排除。</p><h2 id="bf57" class="oe mk it bd ml of og dn mp oh oi dp mt kr oj ok mv kv ol om mx kz on oo mz op bi translated">里脊回归</h2><p id="a343" class="pw-post-body-paragraph ki kj it kk b kl nb ju kn ko nc jx kq kr nd kt ku kv ne kx ky kz nf lb lc ld im bi translated">值得一提的是，有一种在计算上和理论上与LASSO非常相似的技术叫做岭回归。它将稍微不同的约束区域应用于损失函数，并且不是选择特征，而是收缩所有的系数。此外，SciKit-Learn中的代码几乎是相同的，因此不涵盖它将是一个遗憾。那么，让我们来看看。</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi oz"><img src="../Images/420bff5dd2ea47f4abf384348a142b24.png" data-original-src="https://miro.medium.com/v2/resize:fit:1174/format:webp/1*h4XGXTIiHiDql3ZVwrQ0Og.png"/></div><p class="lr ls gj gh gi lt lu bd b be z dk translated">岭回归的代码和结果(来源:作者)</p></figure><p id="fa22" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">对于这个数据集，岭回归没有做得很好，但我们可以看到，一般来说，系数较小。</p><p id="5e2d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如果你想了解更多关于岭回归的知识，可以看看下面这篇由<a class="pa pb ep" href="https://medium.com/u/1fd952af85cc?source=post_page-----271ce0f22bd--------------------------------" rel="noopener" target="_blank"> Qshick </a>撰写的文章！</p><div class="ng nh gp gr ni nj"><a rel="noopener follow" target="_blank" href="/ridge-regression-for-better-usage-2f19b3a202db"><div class="nk ab fo"><div class="nl ab nm cl cj nn"><h2 class="bd iu gy z fp no fr fs np fu fw is bi translated">更好使用的岭回归</h2><div class="nq l"><h3 class="bd b gy z fp no fr fs np fu fw dk translated">回答“什么是岭回归”这个问题的最简单方法是“线性回归的变异”。在…</h3></div><div class="nr l"><p class="bd b dl z fp no fr fs np fu fw dk translated">towardsdatascience.com</p></div></div><div class="ns l"><div class="pc l nu nv nw ns nx lp nj"/></div></div></a></div><h2 id="7796" class="oe mk it bd ml of og dn mp oh oi dp mt kr oj ok mv kv ol om mx kz on oo mz op bi translated">比较系数</h2><p id="127f" class="pw-post-body-paragraph ki kj it kk b kl nb ju kn ko nc jx kq kr nd kt ku kv ne kx ky kz nf lb lc ld im bi translated">最后，让我们将三种不同技术的系数可视化！我们可以看到，岭回归发现几乎所有系数都更小，更接近于0，其中LASSO将一些系数设置为0。对于这个数据集，更稀疏的系数也导致具有更低MSE的更精确的模型。拥有更少的特征，它也更容易被理解。</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi pd"><img src="../Images/6a4296406d57a7bc4eb5f9e1526be666.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XOOgdmQkQD59uCXfjezqeA.png"/></div></div><p class="lr ls gj gh gi lt lu bd b be z dk translated">系数图(来源:作者)</p></figure><p id="3776" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们希望你喜欢这篇文章！要了解更多关于数据科学、机器学习和开发的内容，请查看<a class="ae le" href="https://www.youtube.com/channel/UCmvdvjDaSjjMRIAxE5s7EZA" rel="noopener ugc nofollow" target="_blank"> Edward的YouTube频道</a>，并订阅我下面的邮件列表，成为第一个听到新文章的人！</p><div class="ng nh gp gr ni nj"><a href="https://edkruegerdata.com/subscribe" rel="noopener  ugc nofollow" target="_blank"><div class="nk ab fo"><div class="nl ab nm cl cj nn"><h2 class="bd iu gy z fp no fr fs np fu fw is bi translated">每当爱德华·克鲁格发表文章时，就收到一封电子邮件。</h2><div class="nr l"><p class="bd b dl z fp no fr fs np fu fw dk translated">edkruegerdata.com</p></div></div><div class="ns l"><div class="pe l nu nv nw ns nx lp nj"/></div></div></a></div><h2 id="f6b6" class="oe mk it bd ml of og dn mp oh oi dp mt kr oj ok mv kv ol om mx kz on oo mz op bi translated">参考</h2><p id="69d7" class="pw-post-body-paragraph ki kj it kk b kl nb ju kn ko nc jx kq kr nd kt ku kv ne kx ky kz nf lb lc ld im bi translated">[1]<a class="ae le" href="http://jmlr.csail.mit.edu/papers/v12/pedregosa11a.html" rel="noopener ugc nofollow" target="_blank">sci kit-learn:Python中的机器学习</a>，Pedregosa <em class="oq">等人</em>，JMLR 12，第2825–2830页，2011年。</p></div></div>    
</body>
</html>