<html>
<head>
<title>Deep Learning models for Grammatical Error Handling in Low Resource languages</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">低资源语言语法错误处理的深度学习模型</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/deep-learning-models-for-grammatical-error-handling-in-low-resource-languages-2ac2c5d154f5?source=collection_archive---------32-----------------------#2021-10-11">https://towardsdatascience.com/deep-learning-models-for-grammatical-error-handling-in-low-resource-languages-2ac2c5d154f5?source=collection_archive---------32-----------------------#2021-10-11</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="6c46" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">卡纳达语语法错误纠正个案研究</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/a9229779c7231148b2bbd7ef6b2b245a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*9oftURIUuBN3PSnO"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">在<a class="ae kv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上由<a class="ae kv" href="https://unsplash.com/@kryptonitenicky?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Nikhita S </a>拍摄的照片</p></figure></div><div class="ab cl kw kx hu ky" role="separator"><span class="kz bw bk la lb lc"/><span class="kz bw bk la lb lc"/><span class="kz bw bk la lb"/></div><div class="ij ik il im in"><ol class=""><li id="4ad9" class="ld le iq lf b lg lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">介绍</li><li id="465c" class="ld le iq lf b lg lv li lw lk lx lm ly lo lz lq lr ls lt lu bi translated">商业问题</li><li id="5c71" class="ld le iq lf b lg lv li lw lk lx lm ly lo lz lq lr ls lt lu bi translated">映射到ML/DL问题</li><li id="8709" class="ld le iq lf b lg lv li lw lk lx lm ly lo lz lq lr ls lt lu bi translated">理解数据</li><li id="7812" class="ld le iq lf b lg lv li lw lk lx lm ly lo lz lq lr ls lt lu bi translated">数据处理</li><li id="7d5c" class="ld le iq lf b lg lv li lw lk lx lm ly lo lz lq lr ls lt lu bi translated">先决条件</li><li id="e319" class="ld le iq lf b lg lv li lw lk lx lm ly lo lz lq lr ls lt lu bi translated">结构</li><li id="f626" class="ld le iq lf b lg lv li lw lk lx lm ly lo lz lq lr ls lt lu bi translated">韵律学</li><li id="1f6a" class="ld le iq lf b lg lv li lw lk lx lm ly lo lz lq lr ls lt lu bi translated">部署</li><li id="3213" class="ld le iq lf b lg lv li lw lk lx lm ly lo lz lq lr ls lt lu bi translated">结论和未来工作</li><li id="e0f9" class="ld le iq lf b lg lv li lw lk lx lm ly lo lz lq lr ls lt lu bi translated">轮廓</li><li id="2f75" class="ld le iq lf b lg lv li lw lk lx lm ly lo lz lq lr ls lt lu bi translated">参考</li></ol></div><div class="ab cl kw kx hu ky" role="separator"><span class="kz bw bk la lb lc"/><span class="kz bw bk la lb lc"/><span class="kz bw bk la lb"/></div><div class="ij ik il im in"><p id="1b39" class="pw-post-body-paragraph ma mb iq lf b lg lh jr mc li lj ju md lk me mf mg lm mh mi mj lo mk ml mm lq ij bi translated"><strong class="lf ir"> 1。简介</strong></p><p id="39e9" class="pw-post-body-paragraph ma mb iq lf b lg lh jr mc li lj ju md lk me mf mg lm mh mi mj lo mk ml mm lq ij bi translated"><em class="mn">语法(语言学)</em></p><p id="f5f5" class="pw-post-body-paragraph ma mb iq lf b lg lh jr mc li lj ju md lk me mf mg lm mh mi mj lo mk ml mm lq ij bi translated"><em class="mn">中的</em> <a class="ae kv" href="https://en.wikipedia.org/wiki/Linguistics" rel="noopener ugc nofollow" target="_blank"> <em class="mn">语言学中的</em> </a> <em class="mn">，语法(来源于</em> <a class="ae kv" href="https://en.wikipedia.org/wiki/Ancient_Greek" rel="noopener ugc nofollow" target="_blank"> <em class="mn">古希腊语</em></a><em class="mn">γραμματικήgrammatikḗ)中的一种</em> <a class="ae kv" href="https://en.wikipedia.org/wiki/Natural_language" rel="noopener ugc nofollow" target="_blank"> <em class="mn">自然语言</em> </a> <em class="mn">是其套</em> <a class="ae kv" href="https://en.wikipedia.org/wiki/Structure" rel="noopener ugc nofollow" target="_blank"> <em class="mn">结构上的</em> </a> <em class="mn">约束说话者或写作者的</em> <a class="ae kv" href="https://en.wikipedia.org/wiki/Clause_(linguistics)" rel="noopener ugc nofollow" target="_blank"> <em class="mn">从句</em></a> <em class="mn">这个术语也可以指对这样的约束条件的研究，一个领域包括诸如</em><a class="ae kv" href="https://en.wikipedia.org/wiki/Phonology" rel="noopener ugc nofollow" target="_blank"><em class="mn"/></a><em class="mn"/><a class="ae kv" href="https://en.wikipedia.org/wiki/Morphology_(linguistics)" rel="noopener ugc nofollow" target="_blank"><em class="mn">词法</em> </a> <em class="mn">和</em> <a class="ae kv" href="https://en.wikipedia.org/wiki/Syntax" rel="noopener ugc nofollow" target="_blank"> <em class="mn">句法</em> </a> <em class="mn">之类的领域，通常由</em> <a class="ae kv" href="https://en.wikipedia.org/wiki/Phonetics" rel="noopener ugc nofollow" target="_blank"> <em class="mn">语音学</em></a><em class="mn"/>补充【维基百科】</p><p id="6597" class="pw-post-body-paragraph ma mb iq lf b lg lh jr mc li lj ju md lk me mf mg lm mh mi mj lo mk ml mm lq ij bi translated">缺乏大型单语或平行语料库和手工制作的语言资源的语言被称为低资源语言。在NLP中，与流行语言相比，这些语言很多，比如英语，它们有相对成熟的人工智能框架和工具。在这个案例研究中，我们认为卡纳达语是一种低资源语言。</p><h2 id="2de5" class="mo mp iq bd mq mr ms dn mt mu mv dp mw lk mx my mz lm na nb nc lo nd ne nf ng bi translated">卡纳达语(卡纳达语语法)</h2><p id="389e" class="pw-post-body-paragraph ma mb iq lf b lg nh jr mc li ni ju md lk nj mf mg lm nk mi mj lo nl ml mm lq ij bi translated">卡纳达语是印度西南部卡纳塔克邦人主要使用的一种语言。</p><p id="7279" class="pw-post-body-paragraph ma mb iq lf b lg lh jr mc li lj ju md lk me mf mg lm mh mi mj lo mk ml mm lq ij bi translated">语法是正确使用和推广语言的指南。卡纳达语语法是基于Keshiraja的“<strong class="lf ir"> S </strong> habdamanidarpana”(公元1260年)。除此之外，在“<strong class="lf ir"> K </strong> avirajamarga”和Nagavarma II的“<strong class="lf ir"> K </strong> avyalocana”和“<strong class="lf ir"> K </strong> arnataka Bhasha Bhushan”中提到了卡纳达语语法。</p><p id="8232" class="pw-post-body-paragraph ma mb iq lf b lg lh jr mc li lj ju md lk me mf mg lm mh mi mj lo mk ml mm lq ij bi translated">卡纳达语字母表中有49个字母。主要部分是</p><ol class=""><li id="8e5a" class="ld le iq lf b lg lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">Swaras + Yogavahagalu(元音)</li><li id="fce5" class="ld le iq lf b lg lv li lw lk lx lm ly lo lz lq lr ls lt lu bi translated">Vyanjana(辅音)</li><li id="5c09" class="ld le iq lf b lg lv li lw lk lx lm ly lo lz lq lr ls lt lu bi translated">Nama pada(名词)</li><li id="f70f" class="ld le iq lf b lg lv li lw lk lx lm ly lo lz lq lr ls lt lu bi translated">克里亚帕达(动词)</li><li id="0b7d" class="ld le iq lf b lg lv li lw lk lx lm ly lo lz lq lr ls lt lu bi translated">Sarvanama(代词)</li><li id="0c87" class="ld le iq lf b lg lv li lw lk lx lm ly lo lz lq lr ls lt lu bi translated">林加语(性别)</li><li id="91a7" class="ld le iq lf b lg lv li lw lk lx lm ly lo lz lq lr ls lt lu bi translated">Vibhakti和Pratyaya(客体，主体)</li><li id="c857" class="ld le iq lf b lg lv li lw lk lx lm ly lo lz lq lr ls lt lu bi translated">Tatsama和Tadbhava(梵语和本地语)</li><li id="36c3" class="ld le iq lf b lg lv li lw lk lx lm ly lo lz lq lr ls lt lu bi translated">变调(复调)</li><li id="a764" class="ld le iq lf b lg lv li lw lk lx lm ly lo lz lq lr ls lt lu bi translated">Samasa(复利)</li></ol><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nm"><img src="../Images/776ae59a9c25a89adc2a2e8572c1eeaf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4AnfFPP11ujW0S2dy_Ndwg.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">卡纳达字母(作者图片)</p></figure></div><div class="ab cl kw kx hu ky" role="separator"><span class="kz bw bk la lb lc"/><span class="kz bw bk la lb lc"/><span class="kz bw bk la lb"/></div><div class="ij ik il im in"><p id="25f6" class="pw-post-body-paragraph ma mb iq lf b lg lh jr mc li lj ju md lk me mf mg lm mh mi mj lo mk ml mm lq ij bi translated"><strong class="lf ir"> 2。商业问题</strong></p><p id="14e6" class="pw-post-body-paragraph ma mb iq lf b lg lh jr mc li lj ju md lk me mf mg lm mh mi mj lo mk ml mm lq ij bi translated">业务问题是，检测文本中至少30%的语法错误，并在合理的周转时间和最佳的CPU利用率内纠正它们。在低资源设置中的GEC系统可以用作文字处理器、帖子编辑器以及作为语言学习者的学习辅助。</p><p id="34f7" class="pw-post-body-paragraph ma mb iq lf b lg lh jr mc li lj ju md lk me mf mg lm mh mi mj lo mk ml mm lq ij bi translated"><strong class="lf ir"> 3。映射到机器学习问题</strong></p><p id="430f" class="pw-post-body-paragraph ma mb iq lf b lg lh jr mc li lj ju md lk me mf mg lm mh mi mj lo mk ml mm lq ij bi translated">上述业务问题可以使用统计模型、基于规则的模型和神经机器翻译模型来解决。在这个案例研究中，我们实验了一种神经机器翻译方法。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nn"><img src="../Images/6cdd3d4bdc252ab45fe59f7fc5f83fd1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GPCUgW0eoaaSb0VC--2pMQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">框图(图片由作者提供)</p></figure><p id="2a76" class="pw-post-body-paragraph ma mb iq lf b lg lh jr mc li lj ju md lk me mf mg lm mh mi mj lo mk ml mm lq ij bi translated">我们这里使用的损失函数是稀疏分类交叉熵，这是使用标签和预测的损失计算。</p><p id="f45a" class="pw-post-body-paragraph ma mb iq lf b lg lh jr mc li lj ju md lk me mf mg lm mh mi mj lo mk ml mm lq ij bi translated"><strong class="lf ir"> 4。理解数据</strong></p><p id="9d13" class="pw-post-body-paragraph ma mb iq lf b lg lh jr mc li lj ju md lk me mf mg lm mh mi mj lo mk ml mm lq ij bi translated">我们从IndicCorp收集的数据集是最大的公开可用的印度语言语料库之一。</p><p id="10b1" class="pw-post-body-paragraph ma mb iq lf b lg lh jr mc li lj ju md lk me mf mg lm mh mi mj lo mk ml mm lq ij bi translated">我们通过运行curl wget小部件来提取数据集。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="no np l"/></div></figure><p id="d150" class="pw-post-body-paragraph ma mb iq lf b lg lh jr mc li lj ju md lk me mf mg lm mh mi mj lo mk ml mm lq ij bi translated">我们检查了使用head命令提取的文本文件的前几行。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="no np l"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="286c" class="pw-post-body-paragraph ma mb iq lf b lg lh jr mc li lj ju md lk me mf mg lm mh mi mj lo mk ml mm lq ij bi translated"><strong class="lf ir"> 5。数据预处理</strong></p><p id="b89b" class="pw-post-body-paragraph ma mb iq lf b lg lh jr mc li lj ju md lk me mf mg lm mh mi mj lo mk ml mm lq ij bi translated">在数据预处理中，我们需要为数据集中的每条记录构建真实的和屈折的句子对。为简单起见，我们将把来自数据集的输入文本视为原始的正确(真实)句子。由此，我们需要构建错误的句子。为了构造错误的句子，我们需要对整个数据集进行标记，并通过一个词性标注器单独解析每个句子，以获得每个标记的实体。</p><p id="6ec8" class="pw-post-body-paragraph ma mb iq lf b lg lh jr mc li lj ju md lk me mf mg lm mh mi mj lo mk ml mm lq ij bi translated">我们使用IndicNLP[ <a class="ae kv" href="https://pypi.org/project/indic-nlp-library/" rel="noopener ugc nofollow" target="_blank"> indicnlp </a> ]库来标记我们的文本。我们将结果文件传递到POS Tagger[ <a class="ae kv" href="https://aclanthology.org/W11-3603.pdf" rel="noopener ugc nofollow" target="_blank"> postagger </a> ]中。我们得到了生成的令牌以及它们的pos标签。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nq"><img src="../Images/c8f7f831b70ab03c3a1cfad63d37b715.png" data-original-src="https://miro.medium.com/v2/resize:fit:956/format:webp/1*n-COdS7AN75JNIU16gRjVA.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="9631" class="pw-post-body-paragraph ma mb iq lf b lg lh jr mc li lj ju md lk me mf mg lm mh mi mj lo mk ml mm lq ij bi translated">使用python中的regex和dictionary，我们根据词性生成了每个句子的词形变化，并将它们存储在单独的pickle文件中。</p><p id="7346" class="pw-post-body-paragraph ma mb iq lf b lg lh jr mc li lj ju md lk me mf mg lm mh mi mj lo mk ml mm lq ij bi translated"><strong class="lf ir">产生拐点的另一种方法</strong></p><p id="c1f5" class="pw-post-body-paragraph ma mb iq lf b lg lh jr mc li lj ju md lk me mf mg lm mh mi mj lo mk ml mm lq ij bi translated">我们安装了inltk[ <a class="ae kv" href="https://pypi.org/project/inltk/" rel="noopener ugc nofollow" target="_blank"> inltk </a> ]库，然后使用joblib库和并行处理生成了原始句子的3个变形。这就像图像处理中的数据扩充。使用这种方法生成的句子，其中一些有屈折变化，但其他的保持不变。这种方法没有涵盖语法错误变化的所有功能点。因此，这种方法不能产生拐点。在以后的章节中，我们将讨论我们如何使用这种方法训练模型，而模型却失败了。</p><p id="5118" class="pw-post-body-paragraph ma mb iq lf b lg lh jr mc li lj ju md lk me mf mg lm mh mi mj lo mk ml mm lq ij bi translated"><strong class="lf ir"> 6。先决条件</strong></p><p id="f722" class="pw-post-body-paragraph ma mb iq lf b lg lh jr mc li lj ju md lk me mf mg lm mh mi mj lo mk ml mm lq ij bi translated">本文的读者应该熟悉Python编程、机器学习和深度学习、TensorFlow和构建数据驱动的应用程序的基础知识。</p><p id="9c16" class="pw-post-body-paragraph ma mb iq lf b lg lh jr mc li lj ju md lk me mf mg lm mh mi mj lo mk ml mm lq ij bi translated"><strong class="lf ir"> 7。架构</strong></p><p id="20e2" class="pw-post-body-paragraph ma mb iq lf b lg lh jr mc li lj ju md lk me mf mg lm mh mi mj lo mk ml mm lq ij bi translated">有许多架构，在Google Colaboratory上对30k数据集进行了实验，CPU作为硬件加速器用于数据集收集和预处理步骤，GPU作为硬件加速器用于模型训练和预测。对于那些体系结构，语法错误处理一点也不令人满意。</p><p id="2c7d" class="pw-post-body-paragraph ma mb iq lf b lg lh jr mc li lj ju md lk me mf mg lm mh mi mj lo mk ml mm lq ij bi translated">最后，我们选择了<a class="ae kv" href="https://arxiv.org/pdf/1409.0473.pdf" rel="noopener ugc nofollow" target="_blank"> Bahdanau的附加注意力</a>，在TensorFlow博客中详细介绍了机器翻译。</p><p id="a46f" class="pw-post-body-paragraph ma mb iq lf b lg lh jr mc li lj ju md lk me mf mg lm mh mi mj lo mk ml mm lq ij bi translated">我们在编码器层使用BPEmb来获得子词嵌入。我们分别训练了3个模型</p><ul class=""><li id="216b" class="ld le iq lf b lg lh li lj lk ll lm ln lo lp lq nr ls lt lu bi translated">模式1:句子的字数&lt;= 6. The default parameters left as it is for the Adam optimizer. Without any regularization, the model seemed to overfit.</li><li id="3b04" class="ld le iq lf b lg lv li lw lk lx lm ly lo lz lq nr ls lt lu bi translated">Model2: The number of words in sentence &lt;= 6. The learning rate of the Adam optimizer was chosen to be 5e-5. We used an l2 regularizer for the Encoder Dense layer. The predictions were average on the validation dataset</li><li id="d208" class="ld le iq lf b lg lv li lw lk lx lm ly lo lz lq nr ls lt lu bi translated">Model3: The number of words in sentence &gt; 6。Adam优化器的学习速率被选择为5e-5。我们对编码器密集层使用l2正则化。验证数据集上的预测也很好。</li></ul><h2 id="a0bf" class="mo mp iq bd mq mr ms dn mt mu mv dp mw lk mx my mz lm na nb nc lo nd ne nf ng bi translated">模型1</h2><p id="8118" class="pw-post-body-paragraph ma mb iq lf b lg nh jr mc li ni ju md lk nj mf mg lm nk mi mj lo nl ml mm lq ij bi translated">我们已经训练了该模型，并且在50个时期内，快速获取的单批输入减少到零。因此，这是一个过拟合模型。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/c11a65022e1877a093e010cbc8007604.png" data-original-src="https://miro.medium.com/v2/resize:fit:1196/format:webp/1*pL_9UJbR74augE5j9mY9Hw.png"/></div></figure><p id="de1d" class="pw-post-body-paragraph ma mb iq lf b lg lh jr mc li lj ju md lk me mf mg lm mh mi mj lo mk ml mm lq ij bi translated">我们还对训练数据集中的少数记录进行了随机预测。错误被处理，但是那些从输入句子到输出没有变化的单词被随机预测。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="no np l"/></div></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/ee1ee9091b949d9234e83163d8ffba32.png" data-original-src="https://miro.medium.com/v2/resize:fit:1364/format:webp/1*4k5tJQcVxGRQW7e3fKKPnw.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">训练预测(图片由作者提供)</p></figure><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="no np l"/></div></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/07306ab457493ec8f554b32b0732db92.png" data-original-src="https://miro.medium.com/v2/resize:fit:1360/format:webp/1*X1D_R0QAurOF7cq2ADxNQQ.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">验证预测(作者图片)</p></figure><h2 id="41b5" class="mo mp iq bd mq mr ms dn mt mu mv dp mw lk mx my mz lm na nb nc lo nd ne nf ng bi translated">模型2</h2><p id="24f4" class="pw-post-body-paragraph ma mb iq lf b lg nh jr mc li ni ju md lk nj mf mg lm nk mi mj lo nl ml mm lq ij bi translated">我们已经训练了500个模型，并且很快地取得了不减少到零的单批输入。因此，该模型不是过拟合模型。然而，预测并不令人满意。这可能主要是由于输入数据集中的拐点数量较少，这是因为输入数据集中的拐点长度减少了。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nv"><img src="../Images/55cd2e2cf135a21bcaf8ab961e21e93e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1130/format:webp/1*Rk8rx18GJ6tSCdA4KZj88w.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">模型2单批次损失(图片由作者提供)</p></figure><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="no np l"/></div></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nw"><img src="../Images/f45b6eca84e0fa64523c695576c7a29e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xT_yZfmGAC6UMa4N7Fu_aw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">验证数据集预测(图片由作者提供)</p></figure><p id="adc7" class="pw-post-body-paragraph ma mb iq lf b lg lh jr mc li lj ju md lk me mf mg lm mh mi mj lo mk ml mm lq ij bi translated"><strong class="lf ir">模型3 </strong></p><p id="d5aa" class="pw-post-body-paragraph ma mb iq lf b lg lh jr mc li lj ju md lk me mf mg lm mh mi mj lo mk ml mm lq ij bi translated">我们已经训练了模型，整个批次的损失在500个时期内减少到零。因此，这是我们当前环境设置中的最佳模式。在翻译时，我们根据大部分句子长度将每个句子的最大长度限制为10。但是该模型似乎忠实地纠正了它在给定时间点呈现的所有令牌，在较短的周转时间内实现了最佳的CPU利用率。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="no np l"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nx"><img src="../Images/5ca238753ea7a0098fe9af736a870c11.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*y0lI9SbwViwjFFE-hyAAQw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">验证数据集的预测(图片由作者提供)</p></figure><p id="6f85" class="pw-post-body-paragraph ma mb iq lf b lg lh jr mc li lj ju md lk me mf mg lm mh mi mj lo mk ml mm lq ij bi translated"><strong class="lf ir"> 8。指标</strong></p><p id="b5ae" class="pw-post-body-paragraph ma mb iq lf b lg lh jr mc li lj ju md lk me mf mg lm mh mi mj lo mk ml mm lq ij bi translated">我们选择使用BLEU评分和ROUGE-L评分作为所有模型的性能指标。以下是所有模型的得分对比。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ny"><img src="../Images/a9ee3d88657d8319975ca7a67d7a936a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IWmCmMKHhD-U5EbcCu_KVg.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">BLEU分数比较(图片由作者提供)</p></figure><p id="cac5" class="pw-post-body-paragraph ma mb iq lf b lg lh jr mc li lj ju md lk me mf mg lm mh mi mj lo mk ml mm lq ij bi translated">以下是训练和验证数据集的所有三个模型的rouge-l分数。原始数据集的总大小为30k。我们考虑在训练和验证集中随机抽取100个句子来计算rouge-l分数。下表总结了训练和验证数据集的F1分数。根据f1分数，我们可以得出结论，模型3在给定模型中表现最佳。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nz"><img src="../Images/7f884578834cbfbd6c3c819632316b7a.png" data-original-src="https://miro.medium.com/v2/resize:fit:526/format:webp/1*pBvZcALBto3y1uFGu0q3Xw.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">分数比较</p></figure><p id="4516" class="pw-post-body-paragraph ma mb iq lf b lg lh jr mc li lj ju md lk me mf mg lm mh mi mj lo mk ml mm lq ij bi translated"><strong class="lf ir"> 9。部署</strong></p><p id="d14e" class="pw-post-body-paragraph ma mb iq lf b lg lh jr mc li lj ju md lk me mf mg lm mh mi mj lo mk ml mm lq ij bi translated">使用TensorFlow API将translate函数公开为TensorFlow中的服务。并且可以部署在TensorFlow服务器上。</p><p id="92c8" class="pw-post-body-paragraph ma mb iq lf b lg lh jr mc li lj ju md lk me mf mg lm mh mi mj lo mk ml mm lq ij bi translated">以下链接加载了谷歌联合实验室<strong class="lf ir">中的预测演示。</strong></p><div class="oa ob gp gr oc od"><a href="https://drive.google.com/file/d/1YN-KYZkc-I9wM1-h8i7DABWX7ek9QczB/view?usp=sharing" rel="noopener  ugc nofollow" target="_blank"><div class="oe ab fo"><div class="of ab og cl cj oh"><h2 class="bd ir gy z fp oi fr fs oj fu fw ip bi translated">finetune_less_length.mp4</h2><div class="ok l"><h3 class="bd b gy z fp oi fr fs oj fu fw dk translated">编辑描述</h3></div><div class="ol l"><p class="bd b dl z fp oi fr fs oj fu fw dk translated">drive.google.com</p></div></div><div class="om l"><div class="on l oo op oq om or kp od"/></div></div></a></div><div class="oa ob gp gr oc od"><a href="https://drive.google.com/file/d/1VOV9YmoZlH4JE8Sgm8LEG2jnXjK2dSTA/view?usp=sharing" rel="noopener  ugc nofollow" target="_blank"><div class="oe ab fo"><div class="of ab og cl cj oh"><h2 class="bd ir gy z fp oi fr fs oj fu fw ip bi translated">m3.mp4</h2><div class="ok l"><h3 class="bd b gy z fp oi fr fs oj fu fw dk translated">编辑描述</h3></div><div class="ol l"><p class="bd b dl z fp oi fr fs oj fu fw dk translated">drive.google.com</p></div></div><div class="om l"><div class="os l oo op oq om or kp od"/></div></div></a></div><p id="bbea" class="pw-post-body-paragraph ma mb iq lf b lg lh jr mc li lj ju md lk me mf mg lm mh mi mj lo mk ml mm lq ij bi translated"><strong class="lf ir"> 10。结论和未来工作</strong></p><p id="f5a6" class="pw-post-body-paragraph ma mb iq lf b lg lh jr mc li lj ju md lk me mf mg lm mh mi mj lo mk ml mm lq ij bi translated">我们可以得出结论，对于给定的设置，模型3在其余模型中表现最好。</p><p id="1288" class="pw-post-body-paragraph ma mb iq lf b lg lh jr mc li lj ju md lk me mf mg lm mh mi mj lo mk ml mm lq ij bi translated">我正在研究以下想法</p><ul class=""><li id="71a5" class="ld le iq lf b lg lh li lj lk ll lm ln lo lp lq nr ls lt lu bi translated">为错误变化生成更多的平行语料库和数据集</li><li id="cf31" class="ld le iq lf b lg lv li lw lk lx lm ly lo lz lq nr ls lt lu bi translated">超参数调谐更高级的语法概念，如短语和从句</li></ul><p id="d642" class="pw-post-body-paragraph ma mb iq lf b lg lh jr mc li lj ju md lk me mf mg lm mh mi mj lo mk ml mm lq ij bi translated"><strong class="lf ir"> 11。轮廓</strong></p><p id="8b8b" class="pw-post-body-paragraph ma mb iq lf b lg lh jr mc li lj ju md lk me mf mg lm mh mi mj lo mk ml mm lq ij bi translated">完整源代码可从<a class="ae kv" href="https://github.com/jayaBalaR/casestudy2" rel="noopener ugc nofollow" target="_blank"> Githublink2 </a>获得</p><p id="a097" class="pw-post-body-paragraph ma mb iq lf b lg lh jr mc li lj ju md lk me mf mg lm mh mi mj lo mk ml mm lq ij bi translated">如果您对以上任何方面有任何问题或建议，请通过<a class="ae kv" href="http://www.linkedin.com/in/mypage1" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>与我联系</p><p id="717b" class="pw-post-body-paragraph ma mb iq lf b lg lh jr mc li lj ju md lk me mf mg lm mh mi mj lo mk ml mm lq ij bi translated"><strong class="lf ir"> 12。参考文献</strong></p><ul class=""><li id="be82" class="ld le iq lf b lg lh li lj lk ll lm ln lo lp lq nr ls lt lu bi translated"><a class="ae kv" href="https://www.appliedaicourse.com/" rel="noopener ugc nofollow" target="_blank">应用课程(appliedaicourse.com)</a></li><li id="cd34" class="ld le iq lf b lg lv li lw lk lx lm ly lo lz lq nr ls lt lu bi translated">T21</li><li id="5b84" class="ld le iq lf b lg lv li lw lk lx lm ly lo lz lq nr ls lt lu bi translated"><a class="ae kv" href="https://aclanthology.org/W11-3603.pdf" rel="noopener ugc nofollow" target="_blank">印度语言的跨语言POS标签(和其他工具):使用泰卢固语资源的卡纳达语实验(aclanthology.org)</a>，<a class="ae kv" href="http://sivareddy.in/downloads/index.html#kannada_tools" rel="noopener ugc nofollow" target="_blank">下载| Siva Reddy </a></li><li id="6803" class="ld le iq lf b lg lv li lw lk lx lm ly lo lz lq nr ls lt lu bi translated">【h-its.org T4】</li><li id="27fa" class="ld le iq lf b lg lv li lw lk lx lm ly lo lz lq nr ls lt lu bi translated"><a class="ae kv" href="https://www.tensorflow.org/text/tutorials/nmt_with_attention#define_the_loss_function" rel="noopener ugc nofollow" target="_blank">注意力神经机器翻译| Text | TensorFlow </a></li></ul></div></div>    
</body>
</html>