<html>
<head>
<title>A guide to XGBoost hyperparameters</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">XGBoost超参数指南</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/a-guide-to-xgboost-hyperparameters-87980c7f44a9?source=collection_archive---------6-----------------------#2021-08-23">https://towardsdatascience.com/a-guide-to-xgboost-hyperparameters-87980c7f44a9?source=collection_archive---------6-----------------------#2021-08-23</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/dc5c0d718697d60a201551502b11d555.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*e2TnQ3XzWhISuV8f"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">Jeremy Allouche 在<a class="ae jg" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><div class=""/><div class=""><h2 id="da23" class="pw-subtitle-paragraph kg ji jj bd b kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx dk translated">常用超参数清单</h2></div></div><div class="ab cl ky kz hx la" role="separator"><span class="lb bw bk lc ld le"/><span class="lb bw bk lc ld le"/><span class="lb bw bk lc ld"/></div><div class="im in io ip iq"><p id="2468" class="pw-post-body-paragraph lf lg jj lh b li lj kk lk ll lm kn ln lo lp lq lr ls lt lu lv lw lx ly lz ma im bi translated">如果你问，有哪一种机器学习算法能够在回归和分类中持续提供卓越的性能？</p><p id="803a" class="pw-post-body-paragraph lf lg jj lh b li lj kk lk ll lm kn ln lo lp lq lr ls lt lu lv lw lx ly lz ma im bi translated">XGBoost就是了。它可以说是最强大的算法，越来越多地用于所有行业和所有问题领域，从客户分析和销售预测到欺诈检测和信用审批等等。</p><p id="aeec" class="pw-post-body-paragraph lf lg jj lh b li lj kk lk ll lm kn ln lo lp lq lr ls lt lu lv lw lx ly lz ma im bi translated">也是很多机器学习比赛中的获奖算法。事实上，XGBoost在Kaggle平台上的29场数据科学竞赛中有17场<a class="ae jg" href="https://dl.acm.org/doi/pdf/10.1145/2939672.2939785" rel="noopener ugc nofollow" target="_blank">使用了。</a></p><p id="0e5d" class="pw-post-body-paragraph lf lg jj lh b li lj kk lk ll lm kn ln lo lp lq lr ls lt lu lv lw lx ly lz ma im bi translated">不仅仅是在商业和比赛中，XGBoost已经被用于大型强子对撞机(<a class="ae jg" href="https://home.cern/news/news/computing/higgs-boson-machine-learning-challenge" rel="noopener ugc nofollow" target="_blank">希格斯玻色子机器学习挑战</a>)等科学实验中。</p><p id="0dd8" class="pw-post-body-paragraph lf lg jj lh b li lj kk lk ll lm kn ln lo lp lq lr ls lt lu lv lw lx ly lz ma im bi translated">其性能的关键是其超参数。虽然XGBoost非常容易实现，但是困难的部分是调整超参数。在本文中，我将讨论一些关键的超参数，它们的作用以及如何选择它们的值。</p><p id="d2b6" class="pw-post-body-paragraph lf lg jj lh b li lj kk lk ll lm kn ln lo lp lq lr ls lt lu lv lw lx ly lz ma im bi translated">但是在我去之前，我们先来谈谈XGBoost是如何工作的。</p><h1 id="e1e4" class="mb mc jj bd md me mf mg mh mi mj mk ml kp mm kq mn ks mo kt mp kv mq kw mr ms bi translated">XGBoost一览</h1><p id="ff2f" class="pw-post-body-paragraph lf lg jj lh b li mt kk lk ll mu kn ln lo mv lq lr ls mw lu lv lw mx ly lz ma im bi translated">XGBoost(或e<strong class="lh jk">X</strong>treme<strong class="lh jk">G</strong>radient<strong class="lh jk">Boost</strong>)不是传统意义上的独立算法。它更像是一个“提升”其他算法性能的开源库。它优化了算法的性能，主要是在梯度推进框架中的决策树，同时通过正则化最小化过拟合/偏差。</p><p id="7675" class="pw-post-body-paragraph lf lg jj lh b li lj kk lk ll lm kn ln lo lp lq lr ls lt lu lv lw lx ly lz ma im bi translated">XGBoost 的主要<a class="ae jg" href="https://xgboost.ai/" rel="noopener ugc nofollow" target="_blank">优势是:</a></p><p id="cfa7" class="pw-post-body-paragraph lf lg jj lh b li lj kk lk ll lm kn ln lo lp lq lr ls lt lu lv lw lx ly lz ma im bi translated"><strong class="lh jk">灵活性:</strong>可以<strong class="lh jk"> </strong>执行回归、分类、排名等用户自定义目标的机器学习任务。</p><p id="90de" class="pw-post-body-paragraph lf lg jj lh b li lj kk lk ll lm kn ln lo lp lq lr ls lt lu lv lw lx ly lz ma im bi translated">可移植性:它可以在Windows、Linux和OS X以及云平台上运行。</p><p id="2f0b" class="pw-post-body-paragraph lf lg jj lh b li lj kk lk ll lm kn ln lo lp lq lr ls lt lu lv lw lx ly lz ma im bi translated"><strong class="lh jk">语言支持:</strong>支持多种语言包括C++、Python、R、Java、Scala、Julia。</p><p id="ac2f" class="pw-post-body-paragraph lf lg jj lh b li lj kk lk ll lm kn ln lo lp lq lr ls lt lu lv lw lx ly lz ma im bi translated"><strong class="lh jk">云系统上的分布式培训:</strong> XGBoost <strong class="lh jk"> s </strong>支持多台机器上的分布式培训，包括AWS、GCE、Azure和Yarn clusters。</p><p id="af5c" class="pw-post-body-paragraph lf lg jj lh b li lj kk lk ll lm kn ln lo lp lq lr ls lt lu lv lw lx ly lz ma im bi translated">XGBoost的其他重要特性包括:</p><ul class=""><li id="98eb" class="my mz jj lh b li lj ll lm lo na ls nb lw nc ma nd ne nf ng bi translated">大型数据集的并行处理能力</li><li id="c698" class="my mz jj lh b li nh ll ni lo nj ls nk lw nl ma nd ne nf ng bi translated">可以处理缺失值</li><li id="39f9" class="my mz jj lh b li nh ll ni lo nj ls nk lw nl ma nd ne nf ng bi translated">允许调整以防止过度拟合</li><li id="aca1" class="my mz jj lh b li nh ll ni lo nj ls nk lw nl ma nd ne nf ng bi translated">具有内置的交叉验证</li></ul><p id="42a2" class="pw-post-body-paragraph lf lg jj lh b li lj kk lk ll lm kn ln lo lp lq lr ls lt lu lv lw lx ly lz ma im bi translated">下面我将首先介绍XGBoost的一个简单的5步实现，然后我们可以讨论超参数以及如何使用它们来优化性能。</p><h1 id="a598" class="mb mc jj bd md me mf mg mh mi mj mk ml kp mm kq mn ks mo kt mp kv mq kw mr ms bi translated">履行</h1><h2 id="8fbd" class="nm mc jj bd md nn no dn mh np nq dp ml lo nr ns mn ls nt nu mp lw nv nw mr nx bi translated"><strong class="ak"> 1)导入库</strong></h2><p id="e337" class="pw-post-body-paragraph lf lg jj lh b li mt kk lk ll mu kn ln lo mv lq lr ls mw lu lv lw mx ly lz ma im bi translated">对于这个演示，我们不需要太多。从<code class="fe ny nz oa ob b">sklearn</code>库中，我们可以导入用于分割训练和测试数据以及准确性度量的模块。注意，首先您需要安装(pip install)这个<code class="fe ny nz oa ob b">XGBoost </code>库，然后才能导入它。</p><pre class="oc od oe of gt og ob oh oi aw oj bi"><span id="eaff" class="nm mc jj ob b gy ok ol l om on"># loading data<br/>from sklearn.datasets import load_iris</span><span id="e6d4" class="nm mc jj ob b gy oo ol l om on"># to split data into training and testing set<br/>from sklearn.model_selection import train_test_split</span><span id="cf94" class="nm mc jj ob b gy oo ol l om on"># XGBoost library<br/>import xgboost as xgb</span><span id="0d3a" class="nm mc jj ob b gy oo ol l om on"># evaluation metrics<br/>from sklearn.metrics import accuracy_score</span></pre><h2 id="1cc1" class="nm mc jj bd md nn no dn mh np nq dp ml lo nr ns mn ls nt nu mp lw nv nw mr nx bi translated"><strong class="ak"> 2。导入数据</strong></h2><p id="f2be" class="pw-post-body-paragraph lf lg jj lh b li mt kk lk ll mu kn ln lo mv lq lr ls mw lu lv lw mx ly lz ma im bi translated">我们使用的是Scikit Learn library内置的<code class="fe ny nz oa ob b">iris </code>数据集，所以不需要做数据预处理。加载数据后，我们只需分别存储输入要素和目标变量。</p><pre class="oc od oe of gt og ob oh oi aw oj bi"><span id="5047" class="nm mc jj ob b gy ok ol l om on">df = load_iris()<br/>X = df.data<br/>y = df.target</span></pre><p id="025d" class="pw-post-body-paragraph lf lg jj lh b li lj kk lk ll lm kn ln lo lp lq lr ls lt lu lv lw lx ly lz ma im bi translated">数据集有4个特征(预测值):</p><pre class="oc od oe of gt og ob oh oi aw oj bi"><span id="9604" class="nm mc jj ob b gy ok ol l om on">df.feature_names</span><span id="9095" class="nm mc jj ob b gy oo ol l om on">&gt;&gt; <br/>['sepal length (cm)',<br/> 'sepal width (cm)',<br/> 'petal length (cm)',<br/> 'petal width (cm)']</span></pre><p id="d7ad" class="pw-post-body-paragraph lf lg jj lh b li lj kk lk ll lm kn ln lo lp lq lr ls lt lu lv lw lx ly lz ma im bi translated">目标有3个类。</p><pre class="oc od oe of gt og ob oh oi aw oj bi"><span id="2e94" class="nm mc jj ob b gy ok ol l om on">df.target_names</span><span id="e4e6" class="nm mc jj ob b gy oo ol l om on">&gt;&gt; array(['setosa', 'versicolor', 'virginica'], dtype='&lt;U10')</span></pre><h2 id="e254" class="nm mc jj bd md nn no dn mh np nq dp ml lo nr ns mn ls nt nu mp lw nv nw mr nx bi translated"><strong class="ak"> 3。准备输入数据</strong></h2><p id="1ed3" class="pw-post-body-paragraph lf lg jj lh b li mt kk lk ll mu kn ln lo mv lq lr ls mw lu lv lw mx ly lz ma im bi translated">首先，我们将数据分为训练集(70%)和测试集(30%)。作为一个额外的步骤，我们需要将数据存储到一个兼容的DMatrix对象中，以实现XGBoost兼容性。</p><pre class="oc od oe of gt og ob oh oi aw oj bi"><span id="ce14" class="nm mc jj ob b gy ok ol l om on">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state = 1)</span><span id="321f" class="nm mc jj ob b gy oo ol l om on">train = xgb.DMatrix(X_train, label = y_train)<br/>test = xgb.DMatrix(X_test, label = y_test)</span></pre><h2 id="5b5e" class="nm mc jj bd md nn no dn mh np nq dp ml lo nr ns mn ls nt nu mp lw nv nw mr nx bi translated"><strong class="ak"> 4。超参数</strong></h2><p id="929a" class="pw-post-body-paragraph lf lg jj lh b li mt kk lk ll mu kn ln lo mv lq lr ls mw lu lv lw mx ly lz ma im bi translated">由于这是一个简单的演示，让我们只选择3个超参数。</p><pre class="oc od oe of gt og ob oh oi aw oj bi"><span id="f435" class="nm mc jj ob b gy ok ol l om on"># specify hyperparameters<br/>params = {<br/>    'max_depth': 4,<br/>    'eta': 0.3,<br/>    'num_class': 3<br/>}</span><span id="aa9f" class="nm mc jj ob b gy oo ol l om on">epochs = 10</span></pre><p id="520c" class="pw-post-body-paragraph lf lg jj lh b li lj kk lk ll lm kn ln lo lp lq lr ls lt lu lv lw lx ly lz ma im bi translated"><strong class="lh jk"> 5。建模</strong></p><p id="3a16" class="pw-post-body-paragraph lf lg jj lh b li lj kk lk ll lm kn ln lo lp lq lr ls lt lu lv lw lx ly lz ma im bi translated">现在您已经指定了超参数，绘制模型并进行预测只需要几行代码。</p><pre class="oc od oe of gt og ob oh oi aw oj bi"><span id="238e" class="nm mc jj ob b gy ok ol l om on"># train model<br/>model = xgb.train(params, train, epochs)</span><span id="66f3" class="nm mc jj ob b gy oo ol l om on"># prediction<br/>y_pred = model.predict(test)</span></pre><figure class="oc od oe of gt iv gh gi paragraph-image"><div class="gh gi op"><img src="../Images/ac1b3be10aa8476e969f8826ee036d0f.png" data-original-src="https://miro.medium.com/v2/resize:fit:698/format:webp/1*BaKtFk80xjRRwclSXB-yYQ.png"/></div></figure><p id="83ae" class="pw-post-body-paragraph lf lg jj lh b li lj kk lk ll lm kn ln lo lp lq lr ls lt lu lv lw lx ly lz ma im bi translated">因此，即使使用这个简单的实现，该模型也能够获得98%的准确性。</p><h1 id="bab0" class="mb mc jj bd md me mf mg mh mi mj mk ml kp mm kq mn ks mo kt mp kv mq kw mr ms bi translated"><strong class="ak">超参数</strong></h1><p id="8913" class="pw-post-body-paragraph lf lg jj lh b li mt kk lk ll mu kn ln lo mv lq lr ls mw lu lv lw mx ly lz ma im bi translated">现在让我们进入这篇文章的核心——超参数。</p><p id="642c" class="pw-post-body-paragraph lf lg jj lh b li lj kk lk ll lm kn ln lo lp lq lr ls lt lu lv lw lx ly lz ma im bi translated">XGBoost中有许多不同的参数，它们大致分为<a class="ae jg" href="https://xgboost.readthedocs.io/en/latest/parameter.html" rel="noopener ugc nofollow" target="_blank"> 3种类型</a>:</p><ul class=""><li id="caac" class="my mz jj lh b li lj ll lm lo na ls nb lw nc ma nd ne nf ng bi translated">一般参数</li><li id="2c72" class="my mz jj lh b li nh ll ni lo nj ls nk lw nl ma nd ne nf ng bi translated">助推器参数</li><li id="cd56" class="my mz jj lh b li nh ll ni lo nj ls nk lw nl ma nd ne nf ng bi translated">任务参数</li></ul><p id="c293" class="pw-post-body-paragraph lf lg jj lh b li lj kk lk ll lm kn ln lo lp lq lr ls lt lu lv lw lx ly lz ma im bi translated">XGBoost <a class="ae jg" href="https://xgboost.readthedocs.io/en/latest/" rel="noopener ugc nofollow" target="_blank">文档</a>中列出了大约35个不同的超参数。然而，并不是所有的都同样重要，有些比其他的更有影响力。</p><figure class="oc od oe of gt iv gh gi paragraph-image"><div class="gh gi oq"><img src="../Images/2bf56660181ec82aa41172536bd05da9.png" data-original-src="https://miro.medium.com/v2/resize:fit:882/format:webp/1*K7eB1EDtwpT3TSqFX2ZKkQ.png"/></div><p class="jc jd gj gh gi je jf bd b be z dk translated">XGBoost超参数的部分列表(合成者:作者)</p></figure><p id="3cd1" class="pw-post-body-paragraph lf lg jj lh b li lj kk lk ll lm kn ln lo lp lq lr ls lt lu lv lw lx ly lz ma im bi translated">下面是在网格搜索中经常调整的一些参数，以找到最佳平衡。</p><p id="237f" class="pw-post-body-paragraph lf lg jj lh b li lj kk lk ll lm kn ln lo lp lq lr ls lt lu lv lw lx ly lz ma im bi translated"><strong class="lh jk">频繁调整的超参数</strong></p><ul class=""><li id="f7a5" class="my mz jj lh b li lj ll lm lo na ls nb lw nc ma nd ne nf ng bi translated"><code class="fe ny nz oa ob b"><strong class="lh jk"><em class="or">n_estimators</em></strong></code> <strong class="lh jk"> <em class="or"> : </em> </strong>指定要提升的决策树数量。如果n_estimator = 1，这意味着只生成一棵树，因此没有提升在起作用。默认值是100，但是您可以使用这个数字来获得最佳性能。</li><li id="b8e1" class="my mz jj lh b li nh ll ni lo nj ls nk lw nl ma nd ne nf ng bi translated"><code class="fe ny nz oa ob b"><strong class="lh jk"><em class="or">subsample</em></strong></code> <strong class="lh jk"> <em class="or"> : </em> </strong>表示训练样本的子样本比率。子样本= 0.5意味着在生长树之前使用了50%的训练数据。该值可以是任何分数，但默认值是1。</li><li id="6f88" class="my mz jj lh b li nh ll ni lo nj ls nk lw nl ma nd ne nf ng bi translated"><code class="fe ny nz oa ob b"><strong class="lh jk"><em class="or">max_depth</em></strong></code>:限制每棵树能长多深。默认值为6，但是如果模型中存在过度拟合问题，您可以尝试其他值。</li><li id="e703" class="my mz jj lh b li nh ll ni lo nj ls nk lw nl ma nd ne nf ng bi translated"><code class="fe ny nz oa ob b"><strong class="lh jk"><em class="or">learning_rate </em></strong></code> <strong class="lh jk"> <em class="or"> ( </em> </strong> <em class="or">别名</em><strong class="lh jk"><em class="or">:</em></strong><code class="fe ny nz oa ob b"><strong class="lh jk"><em class="or">eta</em></strong></code><strong class="lh jk"><em class="or">)</em></strong>:是一个正则化参数，在每个boosting步骤中收缩特征权重。默认值为0.3，但人们通常使用0.01、0.1、0.2等值进行调整。</li><li id="1770" class="my mz jj lh b li nh ll ni lo nj ls nk lw nl ma nd ne nf ng bi translated"><code class="fe ny nz oa ob b"><strong class="lh jk"><em class="or">gamma </em></strong></code> <strong class="lh jk"> <em class="or"> ( </em> </strong> <em class="or">别名</em><strong class="lh jk"><em class="or">:</em></strong><code class="fe ny nz oa ob b"><strong class="lh jk"><em class="or">min_split_loss</em></strong></code><strong class="lh jk"><em class="or">)</em></strong>:是树修剪的另一个正则化参数。它规定了种植一棵树所需的最小损失减少量。默认值设置为0。</li><li id="a364" class="my mz jj lh b li nh ll ni lo nj ls nk lw nl ma nd ne nf ng bi translated"><code class="fe ny nz oa ob b"><strong class="lh jk"><em class="or">reg_alpha </em></strong></code> <strong class="lh jk"> <em class="or"> ( </em> </strong> <em class="or">别名</em><strong class="lh jk"><em class="or">:</em></strong><strong class="lh jk"><em class="or">)</em></strong>:它是L1正则化参数，增加它的值使模型更加保守。默认值为0。</li><li id="5cf0" class="my mz jj lh b li nh ll ni lo nj ls nk lw nl ma nd ne nf ng bi translated"><code class="fe ny nz oa ob b">r<strong class="lh jk"><em class="or">eg_lambda </em></strong></code> <strong class="lh jk"> <em class="or"> ( </em> </strong> <em class="or">别名</em><strong class="lh jk"><em class="or">:</em></strong><code class="fe ny nz oa ob b"><strong class="lh jk"><em class="or">lambda</em></strong></code><strong class="lh jk"><em class="or">)</em></strong>:L2正则化参数，增加其值也会使模型保守。默认值为1。</li></ul><p id="2d13" class="pw-post-body-paragraph lf lg jj lh b li lj kk lk ll lm kn ln lo lp lq lr ls lt lu lv lw lx ly lz ma im bi translated"><strong class="lh jk">特殊用途超参数</strong></p><p id="61b1" class="pw-post-body-paragraph lf lg jj lh b li lj kk lk ll lm kn ln lo lp lq lr ls lt lu lv lw lx ly lz ma im bi translated">上述参数集是“通用”参数，您可以随时调整这些参数以优化模型性能。有一些特殊用途的超参数在特定情况下使用:</p><p id="b13d" class="pw-post-body-paragraph lf lg jj lh b li lj kk lk ll lm kn ln lo lp lq lr ls lt lu lv lw lx ly lz ma im bi translated"><code class="fe ny nz oa ob b"><strong class="lh jk"><em class="or">scale_pos_weight</em></strong></code> <strong class="lh jk"> : </strong>该参数在数据集不平衡的情况下非常有用，尤其是在分类问题中，一个类别的比例只占总观察值的一小部分(例如信用卡欺诈)。默认值为1，但可以使用以下比率:<em class="or">总负面实例(如无欺诈)/总正面实例(如欺诈)</em>。</p><p id="39a3" class="pw-post-body-paragraph lf lg jj lh b li lj kk lk ll lm kn ln lo lp lq lr ls lt lu lv lw lx ly lz ma im bi translated"><code class="fe ny nz oa ob b"><strong class="lh jk"><em class="or">monotone_constraints</em></strong></code>:如果您想要增加对预测因子的约束，例如，一个非线性的、增加的、具有较高信用评分的信用贷款批准的可能性，您可以激活该参数。</p><p id="76a7" class="pw-post-body-paragraph lf lg jj lh b li lj kk lk ll lm kn ln lo lp lq lr ls lt lu lv lw lx ly lz ma im bi translated"><code class="fe ny nz oa ob b"><strong class="lh jk"><em class="or">booster</em></strong></code>:你可以选择使用哪种升压方式。您有三个选项:“dart”、“gbtree”(基于树)和“gblinear”(岭回归)。</p><p id="6c30" class="pw-post-body-paragraph lf lg jj lh b li lj kk lk ll lm kn ln lo lp lq lr ls lt lu lv lw lx ly lz ma im bi translated"><code class="fe ny nz oa ob b"><strong class="lh jk"><em class="or">missing</em></strong></code>:不完全是缺失值处理，而是用来指定在什么情况下算法应该将一个值视为缺失值(例如，客户年龄的负值肯定是不可能的，因此算法将其视为缺失值)。</p><p id="7e84" class="pw-post-body-paragraph lf lg jj lh b li lj kk lk ll lm kn ln lo lp lq lr ls lt lu lv lw lx ly lz ma im bi translated"><code class="fe ny nz oa ob b"><strong class="lh jk"><em class="or">eval_metric</em></strong></code>:指定使用什么损失函数，如回归使用MAE、MSE、RMSE，分类使用对数损失。</p></div><div class="ab cl ky kz hx la" role="separator"><span class="lb bw bk lc ld le"/><span class="lb bw bk lc ld le"/><span class="lb bw bk lc ld"/></div><div class="im in io ip iq"><h1 id="e4dd" class="mb mc jj bd md me os mg mh mi ot mk ml kp ou kq mn ks ov kt mp kv ow kw mr ms bi translated">摘要</h1><p id="6b22" class="pw-post-body-paragraph lf lg jj lh b li mt kk lk ll mu kn ln lo mv lq lr ls mw lu lv lw mx ly lz ma im bi translated">就在5年前，还没有多少人知道XGBoost(它最初于2014年发布<a class="ae jg" href="https://en.wikipedia.org/wiki/XGBoost" rel="noopener ugc nofollow" target="_blank">，现在它正在彻底改变数据科学和机器学习领域，并应用于各种行业。训练XGBoost非常简单，但最难的部分是参数调整。在本文中，我展示了一个简单的演示实现，并讨论了最常用的超参数。</a></p><p id="97d1" class="pw-post-body-paragraph lf lg jj lh b li lj kk lk ll lm kn ln lo lp lq lr ls lt lu lv lw lx ly lz ma im bi translated">感谢阅读。请随意<a class="ae jg" href="https://mab-datasc.medium.com/subscribe" rel="noopener">订阅</a>以获得我的媒体文章通知，或者通过<a class="ae jg" href="https://twitter.com/DataEnthus" rel="noopener ugc nofollow" target="_blank"> Twitter、</a>或<a class="ae jg" href="https://www.linkedin.com/in/mab-alam/" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>与我联系。</p></div></div>    
</body>
</html>