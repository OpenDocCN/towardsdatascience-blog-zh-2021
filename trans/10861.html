<html>
<head>
<title>Deploying Pretrained TF Object Detection Models on Android</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">在Android上部署预训练的TF对象检测模型</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/deploying-pretrained-tf-object-detection-models-on-android-25c3de92caab?source=collection_archive---------10-----------------------#2021-10-21">https://towardsdatascience.com/deploying-pretrained-tf-object-detection-models-on-android-25c3de92caab?source=collection_archive---------10-----------------------#2021-10-21</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="c65f" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph">📱<a class="ae ep" href="https://equipintelligence.medium.com/list/stories-on-mobile-ml-with-kotlin-and-tf-lite-3ebee822c87b" rel="noopener">移动机器学习</a></h2><div class=""/><div class=""><h2 id="fabd" class="pw-subtitle-paragraph jw iz iq bd b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn dk translated">从训练有素的检查站到安卓应用</h2></div><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ko"><img src="../Images/3d121fed147a055cdde911b0e6a9ae60.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*fGvR0atoCaHtep6u"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">由<a class="ae le" href="https://unsplash.com/@abeso?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">塞巴斯蒂安·贝德纳雷克</a>在<a class="ae le" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="8b8c" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">在移动设备上部署机器学习模型是ML即将开始的新阶段。视觉模型，主要是对象检测模型，已经与语音识别、图像分类、文本完成等一起进入移动设备。这些模型通常运行在支持GPU的计算机上，当部署在移动设备上时，有大量的用例。</p><p id="bcf9" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">为了演示如何将ML模型，特别是对象检测模型引入Android的端到端示例，我们将使用来自<a class="ae le" href="https://github.com/victordibia/handtracking" rel="noopener ugc nofollow" target="_blank">Victor Dibia/hand tracking</a>repo的<a class="ae le" href="https://github.com/victordibia" rel="noopener ugc nofollow" target="_blank"> Victor Dibia的</a>手部检测模型进行演示。该模型可以从图像中检测人手，并使用TensorFlow对象检测API制作。我们将使用来自Victor Dibia的repo的经过训练的检查点，并将其转换为TensorFlow Lite ( TFLite)格式，该格式可用于在Android(甚至iOS、Raspberry Pi)上运行模型。</p><p id="eb0d" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">接下来，我们转到Android应用程序，创建运行模型所需的所有必要的类/方法，并通过实时摄像头显示其预测(边界框)。</p><p id="5fb8" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">我们开始吧！</p><h1 id="9031" class="mb mc iq bd md me mf mg mh mi mj mk ml kf mm kg mn ki mo kj mp kl mq km mr ms bi translated">内容</h1><p id="12ad" class="pw-post-body-paragraph lf lg iq lh b li mt ka lk ll mu kd ln lo mv lq lr ls mw lu lv lw mx ly lz ma ij bi translated"><a class="ae le" href="#4bc9" rel="noopener ugc nofollow"> <strong class="lh ja">将模型检查点转换为TFLite </strong> </a></p><p id="f634" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated"><strong class="lh ja">👉<em class="my"> 1。</em> </strong> <a class="ae le" href="#64b6" rel="noopener ugc nofollow"> <strong class="lh ja"> <em class="my">设置TF对象检测API </em> </strong> </a></p><p id="5bba" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated"><strong class="lh ja">👉<em class="my"> 2。</em> </strong> <a class="ae le" href="#73f1" rel="noopener ugc nofollow"> <strong class="lh ja"> <em class="my">将检查点转换为冻结图形</em> </strong> </a></p><p id="721c" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated"><strong class="lh ja">👉<em class="my"> 3。</em> </strong> <a class="ae le" href="#3841" rel="noopener ugc nofollow"> <strong class="lh ja"> <em class="my">将冻结的图形转换到TFLite缓冲区</em> </strong> </a></p><p id="9fc4" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated"><a class="ae le" href="#50b5" rel="noopener ugc nofollow"> <strong class="lh ja">在Android中集成TFLite模型</strong> </a></p><p id="c0f4" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated"><strong class="lh ja">👉<em class="my"> 1。</em> </strong> <a class="ae le" href="#ccc1" rel="noopener ugc nofollow"> <strong class="lh ja"> <em class="my">为CameraX、Coroutines和TF Lite添加依赖</em> </strong> </a></p><p id="5732" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated"><strong class="lh ja">👉<em class="my"> 2。</em> </strong> <a class="ae le" href="#ad18" rel="noopener ugc nofollow"> <strong class="lh ja"> <em class="my">初始化CameraX和ImageAnalysis。分析器</em> </strong> </a></p><p id="8c15" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated"><strong class="lh ja">👉<em class="my"> 3。</em> </strong> <a class="ae le" href="#75a5" rel="noopener ugc nofollow"> <strong class="lh ja"> <em class="my">实现手部检测模式</em> </strong> </a></p><p id="74f3" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated"><strong class="lh ja">👉<em class="my"> 4。</em> </strong> <a class="ae le" href="#2264" rel="noopener ugc nofollow"> <strong class="lh ja"> <em class="my">在相机进给上绘制边界框</em> </strong> </a></p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="mz na l"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">作者的项目/博客</p></figure></div><div class="ab cl nb nc hu nd" role="separator"><span class="ne bw bk nf ng nh"/><span class="ne bw bk nf ng nh"/><span class="ne bw bk nf ng"/></div><div class="ij ik il im in"><h1 id="4bc9" class="mb mc iq bd md me ni mg mh mi nj mk ml kf nk kg mn ki nl kj mp kl nm km mr ms bi translated">将模型检查点转换为TFLite</h1><p id="25f7" class="pw-post-body-paragraph lf lg iq lh b li mt ka lk ll mu kd ln lo mv lq lr ls mw lu lv lw mx ly lz ma ij bi translated">我们的第一步将是将<a class="ae le" href="https://github.com/victordibia/handtracking" rel="noopener ugc nofollow" target="_blank"> Victor Dibia的repo </a>(麻省理工学院许可证)中提供的训练模型检查点转换为<a class="ae le" href="https://ww.tensorflow.org/lite" rel="noopener ugc nofollow" target="_blank"> TensorFlow Lite </a>格式。TensorFlow Lite提供了一个在Android、iOS和微控制器设备上运行TensorFlow模型的高效网关。为了运行转换脚本，我们需要在我们的机器上设置<a class="ae le" href="https://github.com/tensorflow/models/tree/master/research/object_detection" rel="noopener ugc nofollow" target="_blank"> TensorFlow对象检测API </a>。你也可以使用这个<a class="ae le" href="https://github.com/shubham0204/Google_Colab_Notebooks/blob/main/Hand_Tracking_Model_TFLite_Conversion.ipynb" rel="noopener ugc nofollow" target="_blank"> Colab笔记本</a>在云中执行所有转换。</p><blockquote class="nn no np"><p id="b2a5" class="lf lg my lh b li lj ka lk ll lm kd ln nq lp lq lr nr lt lu lv ns lx ly lz ma ij bi translated">我推荐你使用<a class="ae le" href="https://github.com/shubham0204/Google_Colab_Notebooks/blob/main/Hand_Tracking_Model_TFLite_Conversion.ipynb" rel="noopener ugc nofollow" target="_blank"> Colab笔记本</a>(特别是针对Windows)，因为我个人在这样做的时候犯了很多错误。</p></blockquote><h2 id="64b6" class="nt mc iq bd md nu nv dn mh nw nx dp ml lo ny nz mn ls oa ob mp lw oc od mr iw bi translated">1.设置TF对象检测API</h2><p id="8c2e" class="pw-post-body-paragraph lf lg iq lh b li mt ka lk ll mu kd ln lo mv lq lr ls mw lu lv lw mx ly lz ma ij bi translated">TensorFlow对象检测API提供了许多<a class="ae le" href="https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf2_detection_zoo.md" rel="noopener ugc nofollow" target="_blank">预训练的对象检测模型</a>，这些模型可以在自定义数据集上进行微调，并直接部署到移动设备、web或云中。我们只需要能够帮助我们将模型检查点转换成TF Lite缓冲区的转换脚本。</p><p id="9955" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">手部检测模型本身是使用TensorFlow 1.x的TF OD API制作的。因此，首先我们需要<a class="ae le" href="https://www.tensorflow.org/install" rel="noopener ugc nofollow" target="_blank">安装TensorFlow 1.x </a>或TF 1 . 15 . 0(1 . x系列中的最新版本)，然后克隆包含TF OD API的<a class="ae le" href="https://github.com/tensorflow/models" rel="noopener ugc nofollow" target="_blank"> tensorflow/models </a> repo。</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="mz na l"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">代码片段1:设置TF OD API。在<a class="ae le" href="https://github.com/shubham0204" rel="noopener ugc nofollow" target="_blank">作者的GitHub </a>上托管。</p></figure><p id="7e8c" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">此外，我们将克隆<a class="ae le" href="https://github.com/victordibia/handtracking" rel="noopener ugc nofollow" target="_blank"> Victor Dibia的手跟踪回购</a>来获得模型检查点，</p><pre class="kp kq kr ks gt oe of og oh aw oi bi"><span id="92c6" class="nt mc iq of b gy oj ok l ol om">!git clone https://github.com/victordibia/handtracking</span></pre><h2 id="73f1" class="nt mc iq bd md nu nv dn mh nw nx dp ml lo ny nz mn ls oa ob mp lw oc od mr iw bi translated">2.将检查点转换为冻结图</h2><p id="3ae6" class="pw-post-body-paragraph lf lg iq lh b li mt ka lk ll mu kd ln lo mv lq lr ls mw lu lv lw mx ly lz ma ij bi translated">现在，在<code class="fe on oo op of b"><a class="ae le" href="https://github.com/tensorflow/models/tree/master/research/object_detection" rel="noopener ugc nofollow" target="_blank">models/research/object_detection</a></code>目录中，您将看到一个Python脚本<code class="fe on oo op of b"><a class="ae le" href="https://github.com/tensorflow/models/blob/master/research/object_detection/export_tflite_ssd_graph.py" rel="noopener ugc nofollow" target="_blank">export_tflite_ssd_graph.py</a></code>，我们将使用它将模型检查点转换成一个TFLite兼容的图形。这些检查点可以在<code class="fe on oo op of b"><a class="ae le" href="https://github.com/victordibia/handtracking/tree/master/model-checkpoint" rel="noopener ugc nofollow" target="_blank">handtracking/model-checkpoint</a></code>目录中找到。<code class="fe on oo op of b">ssd</code>代表“<a class="ae le" href="https://arxiv.org/abs/1512.02325" rel="noopener ugc nofollow" target="_blank">单次检测器</a>”，这是手部检测模型的架构，而<code class="fe on oo op of b">mobilenet</code>表示<a class="ae le" href="https://arxiv.org/abs/1704.04861" rel="noopener ugc nofollow" target="_blank"> MobileNet </a> ( v1或v2)的主干架构，这是一种专门用于移动设备的CNN架构。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi oq"><img src="../Images/2ab1791b4ace3059bb01d44e89e54c50.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wLZAvYe7rMlz18o6WbVP1w.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">工作流-&gt;将模型检查点转换为TFLite缓冲区。(图片来源:作者)</p></figure><p id="0d68" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">导出的TFLite图包含固定的输入和输出节点。我们可以在<code class="fe on oo op of b">export_ssd_tflite_graph.py</code>脚本中找到这些节点(或张量)的名称和形状。使用这个脚本，我们将把模型检查点转换成一个TFLite兼容图，给出三个参数，</p><ol class=""><li id="8796" class="or os iq lh b li lj ll lm lo ot ls ou lw ov ma ow ox oy oz bi translated"><code class="fe on oo op of b">pipeline_config_path</code>:包含所用SSD Lite型号配置的<code class="fe on oo op of b">.config</code>文件路径。</li><li id="307e" class="or os iq lh b li pa ll pb lo pc ls pd lw pe ma ow ox oy oz bi translated"><code class="fe on oo op of b">trained_checkpoint_prefix</code>:我们希望转换的已训练模型检查点的前缀。</li><li id="d87b" class="or os iq lh b li pa ll pb lo pc ls pd lw pe ma ow ox oy oz bi translated"><code class="fe on oo op of b">max_detections</code>:要预测的包围盒的数量。这很重要，因为它是添加到图表中的<a class="ae le" href="https://learnopencv.com/non-maximum-suppression-theory-and-implementation-in-pytorch/" rel="noopener ugc nofollow" target="_blank">非最大抑制</a>后处理操作的重要参数。</li></ol><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="mz na l"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">代码片段2:将模型检查点转换为TFLite兼容图。在<a class="ae le" href="https://github.com/shubham0204" rel="noopener ugc nofollow" target="_blank">作者的GitHub </a>上托管。</p></figure><p id="a57f" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">脚本执行后，我们剩下两个文件，<code class="fe on oo op of b">tflite_graph.pb</code>和<code class="fe on oo op of b">tflite_graph.pbtxt</code>，它们是TFLite兼容的图形。</p><h2 id="3841" class="nt mc iq bd md nu nv dn mh nw nx dp ml lo ny nz mn ls oa ob mp lw oc od mr iw bi translated">3.将冻结的图形转换到TFLite缓冲区</h2><p id="a212" class="pw-post-body-paragraph lf lg iq lh b li mt ka lk ll mu kd ln lo mv lq lr ls mw lu lv lw mx ly lz ma ij bi translated">现在我们将使用第二个脚本(或者更准确地说，一个实用程序)将步骤2中生成的冻结图形转换成TFLite缓冲区(<code class="fe on oo op of b">.tflite</code>)。由于TensorFlow 2.x排除了<code class="fe on oo op of b">Session</code>和<code class="fe on oo op of b">Placeholder</code>的使用，我们无法在这里将冻结的图形转换为TFLite。这也是我们在第一步安装TensorFlow 1.x的原因之一。</p><p id="9a31" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">我们将使用<code class="fe on oo op of b"><a class="ae le" href="https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/python/tflite_convert.py" rel="noopener ugc nofollow" target="_blank">tflite_convert</a></code>实用程序将冻结的图形转换成TFLite缓冲区。我们也可以使用<code class="fe on oo op of b"><a class="ae le" href="https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/lite/TFLiteConverter" rel="noopener ugc nofollow" target="_blank">tf.lite.TFLiteConverter</a></code> API，但是现在我们将坚持使用命令行实用程序。</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="mz na l"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">代码片段3:将冻结的图形转换成TFLite缓冲区。在<a class="ae le" href="https://github.com/shubham0204" rel="noopener ugc nofollow" target="_blank">作者的GitHub </a>上托管。</p></figure><p id="d550" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">一旦执行完成，您将在<code class="fe on oo op of b">outputs</code>目录中看到一个<code class="fe on oo op of b">model.tflite</code>文件。为了检查输入/输出形状，我们将使用<code class="fe on oo op of b"><a class="ae le" href="https://www.tensorflow.org/api_docs/python/tf/lite/Interpreter" rel="noopener ugc nofollow" target="_blank">tf.lite.Interpreter</a></code>加载TFLite模型，并调用<code class="fe on oo op of b">.get_input_details()</code>或<code class="fe on oo op of b">.get_output_details()</code>分别获取输入和输出细节。</p><blockquote class="nn no np"><p id="2062" class="lf lg my lh b li lj ka lk ll lm kd ln nq lp lq lr nr lt lu lv ns lx ly lz ma ij bi translated"><strong class="lh ja">提示</strong>:使用<code class="fe on oo op of b">pprint</code>获得漂亮的输出。</p></blockquote><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="mz na l"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">代码片段4:检查TFLite模型的输入和输出形状。在<a class="ae le" href="https://github.com/shubham0204" rel="noopener ugc nofollow" target="_blank">作者的GitHub </a>上托管。</p></figure></div><div class="ab cl nb nc hu nd" role="separator"><span class="ne bw bk nf ng nh"/><span class="ne bw bk nf ng nh"/><span class="ne bw bk nf ng"/></div><div class="ij ik il im in"><h1 id="50b5" class="mb mc iq bd md me ni mg mh mi nj mk ml kf nk kg mn ki nl kj mp kl nm km mr ms bi translated">在Android中集成TFLite模型</h1><p id="19f8" class="pw-post-body-paragraph lf lg iq lh b li mt ka lk ll mu kd ln lo mv lq lr ls mw lu lv lw mx ly lz ma ij bi translated">一旦我们得到了TFLite模型及其输入和输出形状的所有细节，我们就可以在Android应用程序中运行它了。在Android Studio中创建一个新项目，或者随意<a class="ae le" href="https://github.com/shubham0204/Hand_Detection_TFLite_Android/archive/refs/heads/main.zip" rel="noopener ugc nofollow" target="_blank">派生/克隆GitHub repo </a>来开始！</p><h2 id="ccc1" class="nt mc iq bd md nu nv dn mh nw nx dp ml lo ny nz mn ls oa ob mp lw oc od mr iw bi translated">1.为CameraX、协程和TF Lite添加依赖项</h2><p id="858b" class="pw-post-body-paragraph lf lg iq lh b li mt ka lk ll mu kd ln lo mv lq lr ls mw lu lv lw mx ly lz ma ij bi translated">因为我们要在直播摄像头上检测手，我们需要在我们的Android应用程序中添加<a class="ae le" href="https://developer.android.com/training/camerax" rel="noopener ugc nofollow" target="_blank"> CameraX </a>依赖项。类似地，为了运行TFLite模型，我们将需要<code class="fe on oo op of b">tensorflow-lite</code>依赖项以及Kotlin协程依赖项，以帮助我们异步运行模型。在应用程序级的<code class="fe on oo op of b">build.gradle</code>文件中，我们将添加以下依赖项:</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="mz na l"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">代码片段5:为CameraX、Coroutines和TensorFlow Lite添加依赖项。在<a class="ae le" href="https://github.com/shubham0204" rel="noopener ugc nofollow" target="_blank">作者的GitHub </a>上托管。</p></figure><p id="5de2" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">确保你添加了<code class="fe on oo op of b">aaptOptions{ noCompress "tflite" }</code>,这样模型就不会被系统压缩来缩小你的应用程序。现在，为了在我们的应用程序中放置TFLite模型，我们将在<code class="fe on oo op of b">app/src/main</code>下创建一个<code class="fe on oo op of b">assets</code>文件夹。将TFLite文件(<code class="fe on oo op of b">.tflite</code>)粘贴到该文件夹中。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi pf"><img src="../Images/7e535654e76b1f7fca5f71ec575111ca.png" data-original-src="https://miro.medium.com/v2/resize:fit:806/format:webp/1*h8rXQymfwD1RhXblENQeMA.png"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">将“model.tflite”放在资产文件夹中。(图片来源:作者)</p></figure><h2 id="ad18" class="nt mc iq bd md nu nv dn mh nw nx dp ml lo ny nz mn ls oa ob mp lw oc od mr iw bi translated">2.初始化CameraX和ImageAnalysis。分析者</h2><p id="6624" class="pw-post-body-paragraph lf lg iq lh b li mt ka lk ll mu kd ln lo mv lq lr ls mw lu lv lw mx ly lz ma ij bi translated">我们将使用CameraX包中的一个<code class="fe on oo op of b"><a class="ae le" href="https://developer.android.com/reference/kotlin/androidx/camera/view/PreviewView" rel="noopener ugc nofollow" target="_blank">PreviewView</a></code>向用户显示实时摄像机反馈。在它上面，我们将放置一个覆盖图，称为<code class="fe on oo op of b"><a class="ae le" href="https://github.com/shubham0204/Hand_Detection_TFLite_Android/blob/main/app/src/main/java/com/shubham0204/ml/handdetection/BoundingBoxOverlay.kt" rel="noopener ugc nofollow" target="_blank">BoundingBoxOverlay</a></code>，在摄像机画面上绘制边界框。我不会在这里讨论实现，但是您可以从源代码或我的这个故事中了解它，</p><div class="pg ph gp gr pi pj"><a href="https://proandroiddev.com/realtime-selfie-segmentation-in-android-with-mlkit-38637c8502ba" rel="noopener  ugc nofollow" target="_blank"><div class="pk ab fo"><div class="pl ab pm cl cj pn"><h2 class="bd ja gy z fp po fr fs pp fu fw iz bi translated">基于MLKit的Android实时自拍分割</h2><div class="pq l"><h3 class="bd b gy z fp po fr fs pp fu fw dk translated">在Android中执行有史以来最快的图像分割</h3></div><div class="pr l"><p class="bd b dl z fp po fr fs pp fu fw dk translated">proandroiddev.com</p></div></div><div class="ps l"><div class="pt l pu pv pw ps px ky pj"/></div></div></a></div><p id="1d1a" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">因为我们要预测实时帧数据上的手的边界框，所以我们还需要一个<code class="fe on oo op of b">ImageAnalysis.Analyzer</code>对象，它返回来自实时摄像机馈送的每一帧。请看来自<code class="fe on oo op of b"><a class="ae le" href="https://github.com/shubham0204/Hand_Detection_TFLite_Android/blob/main/app/src/main/java/com/shubham0204/ml/handdetection/FrameAnalyser.kt" rel="noopener ugc nofollow" target="_blank">FrameAnalyzer.kt</a></code>的片段，</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="mz na l"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">代码片段FrameAnalyser类。在<a class="ae le" href="https://github.com/shubham0204" rel="noopener ugc nofollow" target="_blank">作者的GitHub </a>上托管。</p></figure><p id="b1fb" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated"><code class="fe on oo op of b"><a class="ae le" href="https://github.com/shubham0204/Hand_Detection_TFLite_Android/blob/main/app/src/main/java/com/shubham0204/ml/handdetection/BitmapUtils.kt" rel="noopener ugc nofollow" target="_blank">BitmapUtils</a></code>包含一些有用的静态方法来操作<code class="fe on oo op of b">Bitmap</code>。<code class="fe on oo op of b">isFrameProcessing</code>是一个布尔变量，它决定了输入帧是否必须被丢弃或传递给模型。正如您可能观察到的，我们在<code class="fe on oo op of b">CoroutineScope</code>中运行模型，因此您将不会观察到模型产生推理时的延迟。</p><h2 id="75a5" class="nt mc iq bd md nu nv dn mh nw nx dp ml lo ny nz mn ls oa ob mp lw oc od mr iw bi translated">3.实现手部检测模型</h2><p id="9581" class="pw-post-body-paragraph lf lg iq lh b li mt ka lk ll mu kd ln lo mv lq lr ls mw lu lv lw mx ly lz ma ij bi translated">接下来，我们将创建一个名为<code class="fe on oo op of b"><a class="ae le" href="https://github.com/shubham0204/Hand_Detection_TFLite_Android/blob/main/app/src/main/java/com/shubham0204/ml/handdetection/HandDetectionModel.kt" rel="noopener ugc nofollow" target="_blank">HandDetectionModel</a></code>的类，它将处理所有的TFLite操作并返回给定图像的预测(作为<code class="fe on oo op of b">Bitmap</code>)。</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="mz na l"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">代码片段7:创建“HandDetectionModel”类。</p></figure><p id="7f74" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">我们将在上面的代码片段中分别理解每个术语，</p><ol class=""><li id="b36c" class="or os iq lh b li lj ll lm lo ot ls ou lw ov ma ow ox oy oz bi translated"><code class="fe on oo op of b">modelImageInputDim</code>是我们模型的输入图像的大小。我们的模型将接受大小为300 * 300的图像。</li><li id="ea92" class="or os iq lh b li pa ll pb lo pc ls pd lw pe ma ow ox oy oz bi translated"><code class="fe on oo op of b">maxDetections</code>代表我们的模型做出的预测的最大数量。它决定了<code class="fe on oo op of b">boundingBoxesTensorShape</code>、<code class="fe on oo op of b">confidenceScoresTensorShape</code>、<code class="fe on oo op of b">classesTensorShape</code>和<code class="fe on oo op of b">numTensorShape</code>的形状。</li><li id="07f2" class="or os iq lh b li pa ll pb lo pc ls pd lw pe ma ow ox oy oz bi translated"><code class="fe on oo op of b">outputConfidenceThreshold</code>用于过滤我们的模型做出的预测。这不是NMS，但我们只拿分数大于这个阈值的盒子。</li><li id="26a3" class="or os iq lh b li pa ll pb lo pc ls pd lw pe ma ow ox oy oz bi translated"><code class="fe on oo op of b">inputImageProcessorQuantized</code>和<code class="fe on oo op of b">inputImageProcessorNonQuantized</code>是<code class="fe on oo op of b">TensorOperator</code>的实例，它们将给定的图像调整到<code class="fe on oo op of b">modelImageInputDim</code> * <code class="fe on oo op of b">modelInputImageDim</code>的大小。在量化模型的情况下，我们用平均值和标准偏差都等于127.5来标准化给定的图像。</li></ol><p id="b9b0" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">现在，我们将实现一个方法<code class="fe on oo op of b">run()</code>，它将获取一个<code class="fe on oo op of b">Bitmap</code>图像并以<code class="fe on oo op of b">List&lt;Prediction&gt;</code>的形式输出边界框。<code class="fe on oo op of b"><a class="ae le" href="https://github.com/shubham0204/Hand_Detection_TFLite_Android/blob/main/app/src/main/java/com/shubham0204/ml/handdetection/Prediction.kt" rel="noopener ugc nofollow" target="_blank">Prediction</a></code>是一个保存预测数据的类，比如置信度得分和边界框坐标。</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="mz na l"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">代码片段8:hand detection model类。</p></figure><p id="56cb" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated"><code class="fe on oo op of b">confidenceScores</code>、<code class="fe on oo op of b">boundingBoxes</code>、<code class="fe on oo op of b">classes</code>和<code class="fe on oo op of b">numBoxes</code>是保存模型输出的四个张量。<code class="fe on oo op of b">processOutputs</code>方法将过滤边界框，只返回那些置信度得分大于阈值的框。</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="mz na l"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">代码片段9:过滤我们模型的输出。</p></figure><h2 id="2264" class="nt mc iq bd md nu nv dn mh nw nx dp ml lo ny nz mn ls oa ob mp lw oc od mr iw bi translated">4.在相机馈送上绘制边界框</h2><p id="2b0d" class="pw-post-body-paragraph lf lg iq lh b li mt ka lk ll mu kd ln lo mv lq lr ls mw lu lv lw mx ly lz ma ij bi translated">一旦我们收到了边界框，我们想把它们画在摄像机的画面上，就像我们用OpenCV做的那样。我们将创建一个新的类<code class="fe on oo op of b"><a class="ae le" href="https://github.com/shubham0204/Hand_Detection_TFLite_Android/blob/main/app/src/main/java/com/shubham0204/ml/handdetection/BoundingBoxOverlay.kt" rel="noopener ugc nofollow" target="_blank">BoundingBoxOverlay</a></code>并将其添加到<code class="fe on oo op of b"><a class="ae le" href="https://github.com/shubham0204/Hand_Detection_TFLite_Android/blob/main/app/src/main/res/layout/activity_main.xml" rel="noopener ugc nofollow" target="_blank">activity_main.xml</a></code>中。这个类看起来像，</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="mz na l"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">代码片段BoundingBoxOverlay类。</p></figure><p id="9408" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">仅此而已！我们刚刚在一个Android应用程序中实现了一个手部检测器！你可以在检查完所有代码后运行这个应用程序。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi py"><img src="../Images/db0fc5b3a492e8cbfe39664aa99cfc2b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/1*7SThk4MMfF9n9XNaqCm1sA.png"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">运行手部检测模型的Android应用程序。每个框中间的文本表示该预测的可信度。</p></figure><h1 id="2626" class="mb mc iq bd md me mf mg mh mi mj mk ml kf mm kg mn ki mo kj mp kl mq km mr ms bi translated">结束了</h1><p id="19ff" class="pw-post-body-paragraph lf lg iq lh b li mt ka lk ll mu kd ln lo mv lq lr ls mw lu lv lw mx ly lz ma ij bi translated">希望你喜欢这个故事！欢迎在<strong class="lh ja"><em class="my">【equipintelligence@gmail.com】</em></strong>或在下面的评论中发表你的想法。</p><p id="0505" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">亲爱的开发者，祝你有美好的一天！</p></div></div>    
</body>
</html>