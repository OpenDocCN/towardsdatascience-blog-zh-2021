<html>
<head>
<title>MLP Mixer Is All You Need?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">你只需要MLP搅拌机。</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/mlp-mixer-is-all-you-need-20dbc7587fe4?source=collection_archive---------11-----------------------#2021-06-13">https://towardsdatascience.com/mlp-mixer-is-all-you-need-20dbc7587fe4?source=collection_archive---------11-----------------------#2021-06-13</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="c84b" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph">🤖<a class="ae ep" href="https://equipintelligence.medium.com/list/deep-learning-techniques-methods-and-how-tos-01015cf5f917" rel="noopener">深度学习</a></h2><div class=""/><div class=""><h2 id="c1f4" class="pw-subtitle-paragraph jw iz iq bd b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn dk translated">从头到尾了解MLP混频器，带TF Keras代码</h2></div><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ko"><img src="../Images/a4e1515a9d6c016159b561511061ef1c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lv7NqlThS5_EDX9C7KjO4w.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">来源:<a class="ae le" href="https://arxiv.org/abs/2105.01601" rel="noopener ugc nofollow" target="_blank"><strong class="bd lf"><em class="lg">【MLP-混音器:一个全MLP架构的设想】</em> </strong> </a></p></figure><p id="f5d0" class="pw-post-body-paragraph lh li iq lj b lk ll ka lm ln lo kd lp lq lr ls lt lu lv lw lx ly lz ma mb mc ij bi translated">今年5月初，谷歌的一组研究人员发布了一篇论文<a class="ae le" href="https://arxiv.org/abs/2105.01601" rel="noopener ugc nofollow" target="_blank"><strong class="lj ja"><em class="md">“MLP混合器:一个全MLP的视觉架构”</em> </strong> </a>介绍了他们的MLP混合器(简称为<em class="md">混合器</em>)模型，用于解决计算机视觉问题。该研究表明，MLP混合器在图像分类基准测试中取得了有竞争力的分数，如<a class="ae le" href="https://www.image-net.org/" rel="noopener ugc nofollow" target="_blank"> ImageNet </a>。</p><p id="bdf9" class="pw-post-body-paragraph lh li iq lj b lk ll ka lm ln lo kd lp lq lr ls lt lu lv lw lx ly lz ma mb mc ij bi translated">有一件事会引起每个ML开发者的注意，那就是他们没有在他们的架构中使用卷积。自从卷积能够有效地从图像和视频中提取空间信息以来，它就一直统治着计算机视觉。最近，最初用于NLP问题的<a class="ae le" href="https://arxiv.org/abs/1706.03762" rel="noopener ugc nofollow" target="_blank">变形金刚</a>，在计算机视觉问题上也表现出了显著的效果。MLP混合器的研究论文表明，</p><blockquote class="me"><p id="feda" class="mf mg iq bd mh mi mj mk ml mm mn mc dk translated">在本文中，我们表明，虽然卷积和注意力对于良好的性能都是足够的，但它们都不是必要的。</p></blockquote><blockquote class="mo mp mq"><p id="62f1" class="lh li md lj b lk mr ka lm ln ms kd lp mt mu ls lt mv mw lw lx mx my ma mb mc ij bi translated">关于MLP混合器是否是“无conv”的，曾经有过一些争议。从<a class="mz na ep" href="https://medium.com/u/6fb4b78f0265?source=post_page-----20dbc7587fe4--------------------------------" rel="noopener" target="_blank">权重和偏见</a>浏览此博客以了解更多信息，</p></blockquote><div class="nb nc gp gr nd ne"><a href="https://wandb.ai/wandb_fc/pytorch-image-models/reports/Is-MLP-Mixer-a-CNN-in-Disguise---Vmlldzo4NDE1MTU" rel="noopener  ugc nofollow" target="_blank"><div class="nf ab fo"><div class="ng ab nh cl cj ni"><h2 class="bd ja gy z fp nj fr fs nk fu fw iz bi translated">MLP混合器是一个伪装的CNN吗？</h2><div class="nl l"><h3 class="bd b gy z fp nj fr fs nk fu fw dk translated">最近，一种新的建筑——MLP混合器:全MLP视觉建筑(Tolstikhin等人，2021年)——</h3></div><div class="nm l"><p class="bd b dl z fp nj fr fs nk fu fw dk translated">wandb.ai</p></div></div><div class="nn l"><div class="no l np nq nr nn ns ky ne"/></div></div></a></div><p id="ae03" class="pw-post-body-paragraph lh li iq lj b lk ll ka lm ln lo kd lp lq lr ls lt lu lv lw lx ly lz ma mb mc ij bi translated">我们将更多地讨论MLP混合器的架构和相关的底层技术。最后，我们提供了一个使用TensorFlow Keras的MLP混合器的代码实现。</p><p id="f06d" class="pw-post-body-paragraph lh li iq lj b lk ll ka lm ln lo kd lp lq lr ls lt lu lv lw lx ly lz ma mb mc ij bi translated">此外，这个博客已经在<a class="ae le" href="https://devlibrary.withgoogle.com/authors/equipintelligence" rel="noopener ugc nofollow" target="_blank">谷歌开发库</a>上展示。</p><blockquote class="mo mp mq"><p id="96ab" class="lh li md lj b lk ll ka lm ln lo kd lp mt lr ls lt mv lv lw lx mx lz ma mb mc ij bi translated">你现在可以在TensorFlow Hub，<a class="ae le" href="https://tfhub.dev/sayakpaul/collections/mlp-mixer/1" rel="noopener ugc nofollow" target="_blank">https://tfhub.dev/sayakpaul/collections/mlp-mixer/1</a>上找到预训练的MLP混合器模型</p></blockquote><p id="3723" class="pw-post-body-paragraph lh li iq lj b lk ll ka lm ln lo kd lp lq lr ls lt lu lv lw lx ly lz ma mb mc ij bi translated">我也使用MLP混合器进行文本分类，</p><div class="nb nc gp gr nd ne"><a href="https://www.kaggle.com/shubham0204/tweet-classification-with-mlp-mixers-tf-keras" rel="noopener  ugc nofollow" target="_blank"><div class="nf ab fo"><div class="ng ab nh cl cj ni"><h2 class="bd ja gy z fp nj fr fs nk fu fw iz bi translated">基于MLP混合器的推文分类</h2><div class="nl l"><h3 class="bd b gy z fp nj fr fs nk fu fw dk translated">使用Kaggle笔记本探索和运行机器学习代码|使用自然语言处理中的数据应对灾难…</h3></div><div class="nm l"><p class="bd b dl z fp nj fr fs nk fu fw dk translated">www.kaggle.com</p></div></div><div class="nn l"><div class="nt l np nq nr nn ns ky ne"/></div></div></a></div><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="nu nv l"/></div></figure><h2 id="e3b0" class="nw nx iq bd ny nz oa dn ob oc od dp oe lq of og oh lu oi oj ok ly ol om on iw bi translated">📃<strong class="ak"> <em class="lg">内容</em> </strong></h2><ol class=""><li id="f891" class="oo op iq lj b lk oq ln or lq os lu ot ly ou mc ov ow ox oy bi translated">👉<a class="ae le" href="#c567" rel="noopener ugc nofollow"> <strong class="lj ja"> <em class="md">独霸卷积，变形金刚</em> </strong> </a>问世</li><li id="195c" class="oo op iq lj b lk oz ln pa lq pb lu pc ly pd mc ov ow ox oy bi translated">👉<a class="ae le" href="#de7d" rel="noopener ugc nofollow"> <strong class="lj ja"> <em class="md">【多层感知器】(MLP)和</em> </strong> </a>的GELU激活功能</li><li id="e999" class="oo op iq lj b lk oz ln pa lq pb lu pc ly pd mc ov ow ox oy bi translated">👉<a class="ae le" href="#7418" rel="noopener ugc nofollow"><strong class="lj ja"><em class="md">MLP-密炼机架构组件</em> </strong> </a></li><li id="9d08" class="oo op iq lj b lk oz ln pa lq pb lu pc ly pd mc ov ow ox oy bi translated">👉<a class="ae le" href="#6ad7" rel="noopener ugc nofollow"> <strong class="lj ja"> <em class="md">终局</em> </strong> </a></li><li id="abb5" class="oo op iq lj b lk oz ln pa lq pb lu pc ly pd mc ov ow ox oy bi translated">👉<a class="ae le" href="#e61e" rel="noopener ugc nofollow"> <strong class="lj ja"> <em class="md">更多项目/博客/资源来自作者</em> </strong> </a></li></ol></div><div class="ab cl pe pf hu pg" role="separator"><span class="ph bw bk pi pj pk"/><span class="ph bw bk pi pj pk"/><span class="ph bw bk pi pj"/></div><div class="ij ik il im in"><h1 id="c567" class="pl nx iq bd ny pm pn po ob pp pq pr oe kf ps kg oh ki pt kj ok kl pu km on pv bi translated">👼回旋的优势，变形金刚的出现</h1><p id="d0be" class="pw-post-body-paragraph lh li iq lj b lk oq ka lm ln or kd lp lq pw ls lt lu px lw lx ly py ma mb mc ij bi translated">在计算机视觉问题中使用<a class="ae le" href="https://machinelearningmastery.com/convolutional-layers-for-deep-learning-neural-networks/" rel="noopener ugc nofollow" target="_blank">卷积</a>是由<a class="ae le" href="https://en.wikipedia.org/wiki/Yann_LeCun" rel="noopener ugc nofollow" target="_blank"> Yann LeCun </a>推广的，从那时起卷积就成为了计算机视觉问题的支柱。每个滤波器在输入体积上进行卷积，以计算由神经元构成的激活图，如下所示。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi pz"><img src="../Images/1e1e37b2cb5b18679443ec8f1c479bdb.png" data-original-src="https://miro.medium.com/v2/resize:fit:488/1*cFMF_uWgUFdVRMZAZ0Bfzg.gif"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">图1:内核大小=3，步长=1的卷积运算(没有填充)。<a class="ae le" href="https://github.com/vdumoulin/conv_arithmetic" rel="noopener ugc nofollow" target="_blank">来源:卷积运算</a></p></figure><p id="0010" class="pw-post-body-paragraph lh li iq lj b lk ll ka lm ln lo kd lp lq lr ls lt lu lv lw lx ly lz ma mb mc ij bi translated">输出图中的每一个神经元都连接到输入体积的特定部分，这可以在<strong class="lj ja">图1 </strong>中清晰观察到。输出映射然后被传递给激活函数(例如<a class="ae le" href="https://arxiv.org/pdf/1803.08375.pdf" rel="noopener ugc nofollow" target="_blank"> ReLU </a>)。为了降低输出图的维数，使用了<a class="ae le" href="https://machinelearningmastery.com/pooling-layers-for-convolutional-neural-networks/" rel="noopener ugc nofollow" target="_blank">池</a>操作。卷积用于学习图像的局部特征，这是计算机视觉问题的目标。几乎所有的架构像<a class="ae le" href="https://arxiv.org/abs/1704.04861" rel="noopener ugc nofollow" target="_blank"> MobileNets </a>、<a class="ae le" href="https://arxiv.org/abs/1409.4842" rel="noopener ugc nofollow" target="_blank"> Inception </a>、<a class="ae le" href="https://arxiv.org/abs/1512.03385" rel="noopener ugc nofollow" target="_blank"> ResNet </a>、<a class="ae le" href="https://arxiv.org/abs/1608.06993" rel="noopener ugc nofollow" target="_blank"> DenseNet </a>等等。使用卷积层(也就是卷积+激活)来学习图像特征。</p><p id="d0c9" class="pw-post-body-paragraph lh li iq lj b lk ll ka lm ln lo kd lp lq lr ls lt lu lv lw lx ly lz ma mb mc ij bi translated"><a class="ae le" href="https://arxiv.org/abs/1706.03762" rel="noopener ugc nofollow" target="_blank"> Transformers </a>是为NLP问题而创造的，但在图像分类方面已经显示出相当大的成果。我将在这里为<a class="ae le" href="https://arxiv.org/abs/2010.11929" rel="noopener ugc nofollow" target="_blank">视觉变形金刚</a> ( ViTs)留下一些资源，</p><div class="nb nc gp gr nd ne"><a href="https://keras.io/examples/vision/image_classification_with_vision_transformer/" rel="noopener  ugc nofollow" target="_blank"><div class="nf ab fo"><div class="ng ab nh cl cj ni"><h2 class="bd ja gy z fp nj fr fs nk fu fw iz bi translated">Keras文档:使用视觉转换器进行图像分类</h2><div class="nl l"><h3 class="bd b gy z fp nj fr fs nk fu fw dk translated">作者:Khalid Salama创建日期:2021/01/18最近修改时间:2021/01/18描述:实现愿景…</h3></div><div class="nm l"><p class="bd b dl z fp nj fr fs nk fu fw dk translated">keras.io</p></div></div><div class="nn l"><div class="qa l np nq nr nn ns ky ne"/></div></div></a></div><div class="nb nc gp gr nd ne"><a href="https://www.analyticsvidhya.com/blog/2021/03/an-image-is-worth-16x16-words-transformers-for-image-recognition-at-scale-vision-transformers/" rel="noopener  ugc nofollow" target="_blank"><div class="nf ab fo"><div class="ng ab nh cl cj ni"><h2 class="bd ja gy z fp nj fr fs nk fu fw iz bi translated">一幅图像相当于16x16个字:用于大规模图像识别的变形金刚(视觉变形金刚)</h2><div class="nl l"><h3 class="bd b gy z fp nj fr fs nk fu fw dk translated">这篇文章作为数据科学博客的一部分发表。简介虽然变压器架构具有…</h3></div><div class="nm l"><p class="bd b dl z fp nj fr fs nk fu fw dk translated">www.analyticsvidhya.com</p></div></div><div class="nn l"><div class="qb l np nq nr nn ns ky ne"/></div></div></a></div><div class="nb nc gp gr nd ne"><a rel="noopener follow" target="_blank" href="/are-you-ready-for-vision-transformer-vit-c9e11862c539"><div class="nf ab fo"><div class="ng ab nh cl cj ni"><h2 class="bd ja gy z fp nj fr fs nk fu fw iz bi translated">你准备好接受视觉变形器(ViT)了吗？</h2><div class="nl l"><h3 class="bd b gy z fp nj fr fs nk fu fw dk translated">《一张图像抵得上16x16个字:大规模图像识别的变形金刚》可能会给计算机带来又一次突破…</h3></div><div class="nm l"><p class="bd b dl z fp nj fr fs nk fu fw dk translated">towardsdatascience.com</p></div></div><div class="nn l"><div class="qc l np nq nr nn ns ky ne"/></div></div></a></div></div><div class="ab cl pe pf hu pg" role="separator"><span class="ph bw bk pi pj pk"/><span class="ph bw bk pi pj pk"/><span class="ph bw bk pi pj"/></div><div class="ij ik il im in"><h1 id="de7d" class="pl nx iq bd ny pm pn po ob pp pq pr oe kf ps kg oh ki pt kj ok kl pu km on pv bi translated">🤠多层感知器(MLP)和GELU激活函数</h1><h2 id="5eb8" class="nw nx iq bd ny nz oa dn ob oc od dp oe lq of og oh lu oi oj ok ly ol om on iw bi translated">多层感知器(MLP)</h2><blockquote class="mo mp mq"><p id="1c4e" class="lh li md lj b lk ll ka lm ln lo kd lp mt lr ls lt mv lv lw lx mx lz ma mb mc ij bi translated">如果你是一个有经验的ML开发者，你可能在古代就已经学会了。</p></blockquote><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi qd"><img src="../Images/889ffc61da330045143b40ed97dc864d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1136/format:webp/1*a3wYYvsllYNrzLm66Q-1dQ.png"/></div></figure><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi qe"><img src="../Images/51855a768264c2809fe09ddbf1706225.png" data-original-src="https://miro.medium.com/v2/resize:fit:1194/format:webp/1*exFuMQUc-q7fjz-03nWyNw.png"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">图2:多层感知器。来源:<a class="ae le" href="https://github.com/rcassani/mlp-example" rel="noopener ugc nofollow" target="_blank">多层感知器示例</a></p></figure><p id="f386" class="pw-post-body-paragraph lh li iq lj b lk ll ka lm ln lo kd lp lq lr ls lt lu lv lw lx ly lz ma mb mc ij bi translated"><a class="ae le" href="https://en.wikipedia.org/wiki/Multilayer_perceptron" rel="noopener ugc nofollow" target="_blank">多层感知器</a>是一个人工神经网络，有一个输入层、多个隐含层和一个输出层。除了输入节点，每个节点都使用一个<a class="ae le" rel="noopener" target="_blank" href="/activation-functions-neural-networks-1cbd9f8d91d6">非线性激活函数</a>。</p><p id="3ee7" class="pw-post-body-paragraph lh li iq lj b lk ll ka lm ln lo kd lp lq lr ls lt lu lv lw lx ly lz ma mb mc ij bi translated">在我们的例子中，研究论文提出了一个具有两个全连接(密集)层的MLP，带有一个<a class="ae le" href="https://arxiv.org/abs/1606.08415" rel="noopener ugc nofollow" target="_blank"> GeLU </a>(我们将在接下来的部分中对此进行更多讨论)激活函数，</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi qf"><img src="../Images/fe67799a56d82d4aa501685f0e0f42de.png" data-original-src="https://miro.medium.com/v2/resize:fit:466/format:webp/1*5AjqUTiDHLNO_RG6s_OSWA.png"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">图3:混频器架构的MLP。来源:<a class="ae le" href="https://arxiv.org/abs/2105.01601" rel="noopener ugc nofollow" target="_blank"><em class="lg">《MLP混合器:一个全MLP的建筑愿景》</em> </a></p></figure><p id="c439" class="pw-post-body-paragraph lh li iq lj b lk ll ka lm ln lo kd lp lq lr ls lt lu lv lw lx ly lz ma mb mc ij bi translated">每个混合器层将由两个MLP组成，一个用于<strong class="lj ja"> <em class="md">令牌混合</em> </strong>，另一个用于<strong class="lj ja"> <em class="md">通道混合</em> </strong>。我们将在后面的章节中详细讨论<strong class="lj ja"> <em class="md">令牌混合</em> </strong>和<strong class="lj ja"> <em class="md">通道混合</em> </strong>。下面是我们将用来堆叠两个<code class="fe qg qh qi qj b">Dense</code>层(带有一个GELU)激活的代码，从而向现有的层(<code class="fe qg qh qi qj b">x</code>)添加一个MLP，</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="nu nv l"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">片段1:MLP</p></figure><blockquote class="mo mp mq"><p id="13d1" class="lh li md lj b lk ll ka lm ln lo kd lp mt lr ls lt mv lv lw lx mx lz ma mb mc ij bi translated">注意:图3中看不到<a class="ae le" href="https://jmlr.org/papers/v15/srivastava14a.html" rel="noopener ugc nofollow" target="_blank">叉头</a>。我们添加它是为了规范我们的模型。你也可以在这个<a class="ae le" href="https://keras.io/examples/vision/mlp_image_classification" rel="noopener ugc nofollow" target="_blank">例子</a>中注意到它们。</p></blockquote><p id="8f30" class="pw-post-body-paragraph lh li iq lj b lk ll ka lm ln lo kd lp lq lr ls lt lu lv lw lx ly lz ma mb mc ij bi translated">注意，我们大多数人会认为<code class="fe qg qh qi qj b">Dense</code>层接受形状为<code class="fe qg qh qi qj b">( batch_size , input_dims )</code>的输入，输出形状为<code class="fe qg qh qi qj b">( batch_size, output_dims )</code>的张量。但在我们的例子中，这些<code class="fe qg qh qi qj b">Dense</code>层将接收三维形状的输入，即形状<code class="fe qg qh qi qj b">( batch_size , num_patches , channels )</code>或其转置<code class="fe qg qh qi qj b">( batch_size , channels , num_patches )</code>。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi qk"><img src="../Images/0f1310c777c150d09287686a460b1f3d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VMSG30qU8AtPR2xkmNFAoQ.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">图4:关于密集层的输入/输出形状的信息。来源:<a class="ae le" href="https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense" rel="noopener ugc nofollow" target="_blank">致密层张量流文件</a>。</p></figure><p id="28ee" class="pw-post-body-paragraph lh li iq lj b lk ll ka lm ln lo kd lp lq lr ls lt lu lv lw lx ly lz ma mb mc ij bi translated">我们将在故事的后面部分了解更多关于<code class="fe qg qh qi qj b">num_channels</code>和<code class="fe qg qh qi qj b">num_patches</code>的内容。</p><h2 id="9a44" class="nw nx iq bd ny nz oa dn ob oc od dp oe lq of og oh lu oi oj ok ly ol om on iw bi translated">高斯误差线性单位激活</h2><blockquote class="mo mp mq"><p id="dd8b" class="lh li md lj b lk ll ka lm ln lo kd lp mt lr ls lt mv lv lw lx mx lz ma mb mc ij bi translated">现代但不太流行的激活功能</p></blockquote><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi ql"><img src="../Images/fe0516fe28f4ad55c24f01582e43697e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1048/format:webp/1*oYMo7fAkaKlgw6901jRXTg.png"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">图5:雷鲁、葛鲁和eLU的曲线图。来源:<a class="ae le" href="https://arxiv.org/abs/1606.08415" rel="noopener ugc nofollow" target="_blank">ArXiv</a>上的高斯误差线性单位(GELUs)</p></figure><p id="386d" class="pw-post-body-paragraph lh li iq lj b lk ll ka lm ln lo kd lp lq lr ls lt lu lv lw lx ly lz ma mb mc ij bi translated"><a class="ae le" href="https://arxiv.org/abs/1606.08415" rel="noopener ugc nofollow" target="_blank">高斯误差线性单位</a>是一个激活函数，使用标准<a class="ae le" href="https://en.wikipedia.org/wiki/Normal_distribution#Standard_deviation_and_coverage" rel="noopener ugc nofollow" target="_blank">高斯累积分布函数</a>对输入进行加权。在ReLU(校正线性单位)的情况下，使用其符号对输入进行加权。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi qm"><img src="../Images/865affcfdf36705d471c6e119d5ae61d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1082/format:webp/1*wj6IIwaAi2A0aM0NLoQkBg.png"/></div></div></figure><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi qn"><img src="../Images/dd3bd768f34d7ce8c71aa0f4f12a09d0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1398/format:webp/1*sXuXxS7AdCDGUf3syvKeIg.png"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">图6: ReLU和GELU激活函数。来源:作者创作。</p></figure><p id="76f3" class="pw-post-body-paragraph lh li iq lj b lk ll ka lm ln lo kd lp lq lr ls lt lu lv lw lx ly lz ma mb mc ij bi translated">随着<strong class="lj ja"> <em class="md"> x </em> </strong>减少，输入很有可能被丢弃。<strong class="lj ja"> <em class="md"> x </em> </strong>上的变换是随机的，但它取决于<strong class="lj ja"> <em class="md"> x </em> </strong>的值。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi qo"><img src="../Images/6fc94b12ed08efb8569eab8becf11cdd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RqAfi9RL9_jzPFGh2zW_0w.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">图7:作者提供的GELU的近似值。来源:<a class="ae le" href="https://arxiv.org/abs/1606.08415" rel="noopener ugc nofollow" target="_blank">ArXiv上的高斯误差线性单位</a></p></figure><p id="492d" class="pw-post-body-paragraph lh li iq lj b lk ll ka lm ln lo kd lp lq lr ls lt lu lv lw lx ly lz ma mb mc ij bi translated">选择<a class="ae le" href="https://en.wikipedia.org/wiki/Normal_distribution" rel="noopener ugc nofollow" target="_blank">正态分布</a>的原因是当使用<a class="ae le" href="https://arxiv.org/abs/1502.03167" rel="noopener ugc nofollow" target="_blank">批量归一化</a>层时，神经元的输出遵循正态分布。GELU激活在变压器模型中广泛用于解决NLP问题。</p><p id="66f5" class="pw-post-body-paragraph lh li iq lj b lk ll ka lm ln lo kd lp lq lr ls lt lu lv lw lx ly lz ma mb mc ij bi translated">正如在<strong class="lj ja">片段1 </strong>中观察到的，我们将使用<code class="fe qg qh qi qj b"><a class="ae le" href="https://www.tensorflow.org/api_docs/python/tf/nn/gelu" rel="noopener ugc nofollow" target="_blank">tf.nn.gelu</a></code>将GELU激活添加到MLPs中。如果你想要一个Keras层，在<a class="ae le" href="https://www.tensorflow.org/addons" rel="noopener ugc nofollow" target="_blank"> TensorFlow Addons </a>包中有一个<code class="fe qg qh qi qj b"><a class="ae le" href="https://www.tensorflow.org/addons/api_docs/python/tfa/layers/GELU" rel="noopener ugc nofollow" target="_blank">tfa.layers.GELU</a></code>层。</p><p id="9a29" class="pw-post-body-paragraph lh li iq lj b lk ll ka lm ln lo kd lp lq lr ls lt lu lv lw lx ly lz ma mb mc ij bi translated">这里有一个很好的博客解释了各种激活功能(包括GELU)，</p><div class="nb nc gp gr nd ne"><a href="https://mlfromscratch.com/activation-functions-explained/#/" rel="noopener  ugc nofollow" target="_blank"><div class="nf ab fo"><div class="ng ab nh cl cj ni"><h2 class="bd ja gy z fp nj fr fs nk fu fw iz bi translated">激活功能解释-格卢，SELU，ELU，雷卢和更多</h2><div class="nl l"><h3 class="bd b gy z fp nj fr fs nk fu fw dk translated">在计算每一层的激活值时，我们使用一个激活函数…</h3></div><div class="nm l"><p class="bd b dl z fp nj fr fs nk fu fw dk translated">mlfromscratch.com</p></div></div><div class="nn l"><div class="qp l np nq nr nn ns ky ne"/></div></div></a></div></div><div class="ab cl pe pf hu pg" role="separator"><span class="ph bw bk pi pj pk"/><span class="ph bw bk pi pj pk"/><span class="ph bw bk pi pj"/></div><div class="ij ik il im in"><h1 id="7418" class="pl nx iq bd ny pm pn po ob pp pq pr oe kf ps kg oh ki pt kj ok kl pu km on pv bi translated">🔧MLP混频器架构组件</h1><p id="e654" class="pw-post-body-paragraph lh li iq lj b lk oq ka lm ln or kd lp lq pw ls lt lu px lw lx ly py ma mb mc ij bi translated">我们将详细讨论每个组件，然后将它们集成到一段代码中。</p><h2 id="60a5" class="nw nx iq bd ny nz oa dn ob oc od dp oe lq of og oh lu oi oj ok ly ol om on iw bi translated">⚙️Mixer层</h2><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi qq"><img src="../Images/1d51df4659e98ba6bd07ace08b3d1dc2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*seI3nhRUQ03o0nfM7Io11g.jpeg"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">图8:混合层。来源:作者创作</p></figure><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="nu nv l"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">代码片段2:混音器层</p></figure><p id="2057" class="pw-post-body-paragraph lh li iq lj b lk ll ka lm ln lo kd lp lq lr ls lt lu lv lw lx ly lz ma mb mc ij bi translated">混频器层是MLP混频器架构的构建模块。每个混合器层包含两个MLP，一个用于<strong class="lj ja">令牌混合</strong>，另一个用于<strong class="lj ja">通道混合</strong>。在MLPs旁边，你会注意到图层正常化，跳过连接和箭头上面写的<strong class="lj ja"><em class="md">【T】</em></strong>。指张量的<strong class="lj ja"> <em class="md">转置* </em> </strong>，保持批量维度不变。</p><blockquote class="mo mp mq"><p id="a19a" class="lh li md lj b lk ll ka lm ln lo kd lp mt lr ls lt mv lv lw lx mx lz ma mb mc ij bi translated"><a class="ae le" href="https://math.stackexchange.com/questions/906254/how-do-you-transpose-tensors" rel="noopener ugc nofollow" target="_blank"> <strong class="lj ja">转置</strong> </a> <strong class="lj ja"> * </strong>:我们将使用<code class="fe qg qh qi qj b"><em class="iq">tf.keras.layers.Permute</em></code>层，通过在该层的参数中设置<code class="fe qg qh qi qj b"><em class="iq">dims=[ 2 , 1 ]</em></code>来进行转置。我们不会在接下来的章节中详细讨论这个问题。</p></blockquote><p id="cee5" class="pw-post-body-paragraph lh li iq lj b lk ll ka lm ln lo kd lp lq lr ls lt lu lv lw lx ly lz ma mb mc ij bi translated">我建议您彻底理解该图，因为在讨论组件时，我会不时地引用它。在以下部分中，我们将讨论，</p><ol class=""><li id="0ed9" class="oo op iq lj b lk ll ln lo lq qr lu qs ly qt mc ov ow ox oy bi translated"><em class="md">什么是补丁(混音器层的输入)</em></li><li id="3503" class="oo op iq lj b lk oz ln pa lq pb lu pc ly pd mc ov ow ox oy bi translated"><em class="md">令牌混合MLPs </em></li><li id="7ce1" class="oo op iq lj b lk oz ln pa lq pb lu pc ly pd mc ov ow ox oy bi translated"><em class="md">通道混合MLPs </em></li><li id="3030" class="oo op iq lj b lk oz ln pa lq pb lu pc ly pd mc ov ow ox oy bi translated"><em class="md">图层归一化</em></li></ol><h2 id="54d8" class="nw nx iq bd ny nz oa dn ob oc od dp oe lq of og oh lu oi oj ok ly ol om on iw bi translated">🧽什么是补丁？</h2><blockquote class="mo mp mq"><p id="3090" class="lh li md lj b lk ll ka lm ln lo kd lp mt lr ls lt mv lv lw lx mx lz ma mb mc ij bi translated">我们如何使用RGB图像(MLP混合器模型的典型输入)来创建它们？</p></blockquote><p id="3672" class="pw-post-body-paragraph lh li iq lj b lk ll ka lm ln lo kd lp lq lr ls lt lu lv lw lx ly lz ma mb mc ij bi translated">一个混合层将接受一个形状为<code class="fe qg qh qi qj b">( batch_size , num_patches , num_channels )</code>的张量，并产生一个相同形状的输出。你可能想知道我们如何从RGB图像(这是MLP混合器模型的实际输入)中产生这样的张量？参考下图，</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi qu"><img src="../Images/15430c732a0b7bcdeeff7f2036f0637d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*08Y9Bf_hUKiVuUFQGY2CAA.jpeg"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">图9:使用2D卷积从图像创建补丁。来源:作者创作</p></figure><p id="76e6" class="pw-post-body-paragraph lh li iq lj b lk ll ka lm ln lo kd lp lq lr ls lt lu lv lw lx ly lz ma mb mc ij bi translated">假设，给我们一个大小为<em class="md"> 4 * 4 </em>的RGB图像。我们使用2D卷积来创建<strong class="lj ja">非重叠* </strong>的面片。假设，我们需要大小为<em class="md"> 2 * 2 </em>的正方形小块。如图9所示，我们可以从<em class="md"> 4 * 4 </em>输入图像中创建4个<strong class="lj ja">不重叠的</strong>面片(图中一个面片用阴影表示)。同样，使用<strong class="lj ja"> <em class="md"> C </em> </strong>过滤器，我们将尺寸<code class="fe qg qh qi qj b">image_dims * image_dims * 3</code>的输入图像转换为形状<code class="fe qg qh qi qj b">num_patches * num_patches * C</code>的张量，其中<code class="fe qg qh qi qj b">num_patches = image_dims / patch_size</code>。注意，我们假设<code class="fe qg qh qi qj b">image_dims</code>可以被<code class="fe qg qh qi qj b">patch_size</code>整除。考虑到<strong class="lj ja">我们的例子* </strong>，<code class="fe qg qh qi qj b">num_patches = ( 4 * 4 ) / ( 2 * 2 ) = 4</code>。</p><p id="e77d" class="pw-post-body-paragraph lh li iq lj b lk ll ka lm ln lo kd lp lq lr ls lt lu lv lw lx ly lz ma mb mc ij bi translated">在上面的例子中，我们有<code class="fe qg qh qi qj b">num_patches=2</code>。</p><blockquote class="mo mp mq"><p id="3de8" class="lh li md lj b lk ll ka lm ln lo kd lp mt lr ls lt mv lv lw lx mx lz ma mb mc ij bi translated"><strong class="lj ja">我们的例子* </strong>:特别感谢我们的读者<a class="ae le" href="https://abderhasan.medium.com/" rel="noopener">Abder-Rahman Ali</a>博士<a class="ae le" href="https://abderhasan.medium.com/thanks-so-much-for-your-nice-tutorial-bde39fd71fe0" rel="noopener">指出了<code class="fe qg qh qi qj b">num_patches</code>计算中的错误</a>。我们非常感谢他为改进这个故事所做的努力。</p><p id="d08b" class="lh li md lj b lk ll ka lm ln lo kd lp mt lr ls lt mv lv lw lx mx lz ma mb mc ij bi translated"><strong class="lj ja">不重叠* </strong>:为了创建不重叠的面片，我们在Keras的<code class="fe qg qh qi qj b">Conv2D</code>层设置了<code class="fe qg qh qi qj b">kernel_size=patch_size</code>和<code class="fe qg qh qi qj b">strides=patch_size</code>。</p></blockquote><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi qv"><img src="../Images/5f2f745a86aa81ee2376e3747c92deb8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-otSnRrBzNpFiMYtHjXlyA.jpeg"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">图10:调整补丁大小。来源:作者创作</p></figure><p id="e5c9" class="pw-post-body-paragraph lh li iq lj b lk ll ka lm ln lo kd lp lq lr ls lt lu lv lw lx ly lz ma mb mc ij bi translated">最后，我们调整形状为<code class="fe qg qh qi qj b">num_patches * num_patches * C</code>到<code class="fe qg qh qi qj b">num_patches^2 * C</code>的面片的大小。</p><h2 id="5981" class="nw nx iq bd ny nz oa dn ob oc od dp oe lq of og oh lu oi oj ok ly ol om on iw bi translated">🧱令牌混合MLPs</h2><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi qw"><img src="../Images/3f5c94c201982fabe81a180a1280255b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MqCxltyoUgQZRCX0q90UaA.jpeg"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">图11:令牌混合MLP。来源:作者创作</p></figure><p id="2fe6" class="pw-post-body-paragraph lh li iq lj b lk ll ka lm ln lo kd lp lq lr ls lt lu lv lw lx ly lz ma mb mc ij bi translated">如前所述，每个混合器层由一个<strong class="lj ja">令牌混合MLP </strong>组成。我们想了解记号的含义，这在论文中被强调为，</p><blockquote class="me"><p id="6702" class="mf mg iq bd mh mi mj mk ml mm mn mc dk translated">它[ MLP混合器]接受形状为“面片*通道”表的一系列线性投影图像面片(也称为标记)作为输入，并保持这种维数。</p></blockquote><p id="d2a2" class="pw-post-body-paragraph lh li iq lj b lk mr ka lm ln ms kd lp lq mu ls lt lu mw lw lx ly my ma mb mc ij bi translated">下面是令牌混合MLPs的代码(从<strong class="lj ja">图8 </strong>中可以看出<code class="fe qg qh qi qj b">LayerNormalization</code>和<code class="fe qg qh qi qj b">Permute</code>的作用)</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="nu nv l"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">代码片段3:令牌混合MLP</p></figure><p id="6fe8" class="pw-post-body-paragraph lh li iq lj b lk ll ka lm ln lo kd lp lq lr ls lt lu lv lw lx ly lz ma mb mc ij bi translated">顾名思义，它<em class="md">混合</em>令牌，或者换句话说，允许同一通道中不同补丁之间的通信。如图<strong class="lj ja">图11 </strong>所示，通道<strong class="lj ja"> <em class="md"> C </em> </strong>的数量没有被修改，只有<strong class="lj ja"><em class="md"/></strong>P即面片数量被扩展到某个维度(<code class="fe qg qh qi qj b">token_mixing_mlp_dims</code>)并被带回<strong class="lj ja"> <em class="md"> P </em> </strong>。</p><h2 id="7593" class="nw nx iq bd ny nz oa dn ob oc od dp oe lq of og oh lu oi oj ok ly ol om on iw bi translated">🧱通道混合MLPs</h2><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi qx"><img src="../Images/dea485cfff7c013d9f79a6c3bc80f24d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZyxOULctBwPajuFELZuk5g.jpeg"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">图12:通道混合MLP。来源:作者创作</p></figure><p id="c28d" class="pw-post-body-paragraph lh li iq lj b lk ll ka lm ln lo kd lp lq lr ls lt lu lv lw lx ly lz ma mb mc ij bi translated">通道混合MLP的工作类似于令牌混合MLP。它们混合信道信息，从而实现信道间的通信。</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="nu nv l"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">代码片段4:通道混合MLP</p></figure><p id="69ea" class="pw-post-body-paragraph lh li iq lj b lk ll ka lm ln lo kd lp lq lr ls lt lu lv lw lx ly lz ma mb mc ij bi translated">如图<strong class="lj ja">图12 </strong>所示，面片<strong class="lj ja"> <em class="md"> P </em> </strong>的数量没有修改，只有<strong class="lj ja"> <em class="md"> C </em> </strong>即通道的数量扩展到某个维度(<code class="fe qg qh qi qj b">channel_mixing_mlp_dims</code>)并恢复到<strong class="lj ja"> <em class="md"> C </em> </strong>。</p><h2 id="41da" class="nw nx iq bd ny nz oa dn ob oc od dp oe lq of og oh lu oi oj ok ly ol om on iw bi translated">⚖️层标准化</h2><blockquote class="mo mp mq"><p id="dc4e" class="lh li md lj b lk ll ka lm ln lo kd lp mt lr ls lt mv lv lw lx mx lz ma mb mc ij bi translated">这不同于批量标准化</p></blockquote><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi qy"><img src="../Images/39daee5dc9fdab7571696a8dd3b2753b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*i5WU9DvuRM6MiRirCe-CQg.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">图13:批量标准化与层标准化。来源:<a class="ae le" href="https://paperswithcode.com/method/layer-normalization" rel="noopener ugc nofollow" target="_blank">图层归一化说明| PapersWithCode </a></p></figure><p id="7e89" class="pw-post-body-paragraph lh li iq lj b lk ll ka lm ln lo kd lp lq lr ls lt lu lv lw lx ly lz ma mb mc ij bi translated"><a class="ae le" href="https://arxiv.org/abs/1502.03167" rel="noopener ugc nofollow" target="_blank">批量标准化</a>使用整批的平均值和方差来标准化激活。在<a class="ae le" href="https://arxiv.org/abs/1607.06450v1" rel="noopener ugc nofollow" target="_blank">层归一化</a>的情况下(特别是对于RNNs)，使用一个神经元的所有求和输入的平均值和方差来执行归一化。正如论文中提到的，</p><blockquote class="me"><p id="354c" class="mf mg iq bd mh mi mj mk ml mm mn mc dk translated">在本文中，我们通过计算用于归一化的均值和方差，将批量归一化转换为层归一化，这些均值和方差来自单个训练案例中一层中神经元的所有求和输入。像批量归一化一样，我们也给每个神经元它自己的自适应偏置和增益，它们在归一化之后但在非线性之前被应用。</p></blockquote><p id="cfad" class="pw-post-body-paragraph lh li iq lj b lk mr ka lm ln ms kd lp lq mu ls lt lu mw lw lx ly my ma mb mc ij bi translated">TF-Keras团队提供了一个<code class="fe qg qh qi qj b">tf.keras.layers.LayerNormalization</code>层来执行这个操作。以下是一些了解图层规范化的资源，</p><div class="nb nc gp gr nd ne"><a href="https://leimao.github.io/blog/Layer-Normalization/" rel="noopener  ugc nofollow" target="_blank"><div class="nf ab fo"><div class="ng ab nh cl cj ni"><h2 class="bd ja gy z fp nj fr fs nk fu fw iz bi translated">解释了层标准化</h2><div class="nl l"><h3 class="bd b gy z fp nj fr fs nk fu fw dk translated">最近我在机器翻译的Transformer模型中遇到了层规范化，我发现…</h3></div><div class="nm l"><p class="bd b dl z fp nj fr fs nk fu fw dk translated">雷猫. github.io</p></div></div><div class="nn l"><div class="qz l np nq nr nn ns ky ne"/></div></div></a></div><div class="nb nc gp gr nd ne"><a rel="noopener follow" target="_blank" href="/different-normalization-layers-in-deep-learning-1a7214ff71d6"><div class="nf ab fo"><div class="ng ab nh cl cj ni"><h2 class="bd ja gy z fp nj fr fs nk fu fw iz bi translated">深度学习中的不同规范化层</h2><div class="nl l"><h3 class="bd b gy z fp nj fr fs nk fu fw dk translated">目前，深度学习已经彻底改变了许多子领域，如自然语言处理，计算机视觉…</h3></div><div class="nm l"><p class="bd b dl z fp nj fr fs nk fu fw dk translated">towardsdatascience.com</p></div></div><div class="nn l"><div class="ra l np nq nr nn ns ky ne"/></div></div></a></div><div class="nb nc gp gr nd ne"><a href="https://www.programmersought.com/article/62405248279/" rel="noopener  ugc nofollow" target="_blank"><div class="nf ab fo"><div class="ng ab nh cl cj ni"><h2 class="bd ja gy z fp nj fr fs nk fu fw iz bi translated">批规范化和层规范化的区别-程序员寻求</h2><div class="nl l"><h3 class="bd b gy z fp nj fr fs nk fu fw dk translated">网上有很多解释，包括下面这张图光靠图片是不够理解的…</h3></div><div class="nm l"><p class="bd b dl z fp nj fr fs nk fu fw dk translated">www.programmersought.com</p></div></div><div class="nn l"><div class="rb l np nq nr nn ns ky ne"/></div></div></a></div></div><div class="ab cl pe pf hu pg" role="separator"><span class="ph bw bk pi pj pk"/><span class="ph bw bk pi pj pk"/><span class="ph bw bk pi pj"/></div><div class="ij ik il im in"><p id="9d45" class="pw-post-body-paragraph lh li iq lj b lk ll ka lm ln lo kd lp lq lr ls lt lu lv lw lx ly lz ma mb mc ij bi translated">现在，有了混频器层的完整知识，我们可以继续实现我们的MLP混频器模型进行分类。该模型将接受输入RGB图像并输出类别概率。</p></div><div class="ab cl pe pf hu pg" role="separator"><span class="ph bw bk pi pj pk"/><span class="ph bw bk pi pj pk"/><span class="ph bw bk pi pj"/></div><div class="ij ik il im in"><h1 id="6ad7" class="pl nx iq bd ny pm pn po ob pp pq pr oe kf ps kg oh ki pt kj ok kl pu km on pv bi translated">⚔️终结游戏</h1><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="nu nv l"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">代码片段5:组装模型</p></figure><p id="c694" class="pw-post-body-paragraph lh li iq lj b lk ll ka lm ln lo kd lp lq lr ls lt lu lv lw lx ly lz ma mb mc ij bi translated">我们将逐行检查这段代码。</p><ol class=""><li id="b1ea" class="oo op iq lj b lk ll ln lo lq qr lu qs ly qt mc ov ow ox oy bi translated">首先创建一个输入层，它接收一些所需大小的RGB图像。</li><li id="c8ee" class="oo op iq lj b lk oz ln pa lq pb lu pc ly pd mc ov ow ox oy bi translated">实现一个创建补丁的<code class="fe qg qh qi qj b">Conv2D</code>(记住，我们几十年前讨论过这个问题)。此外，添加一个<code class="fe qg qh qi qj b">Reshape</code>层来重塑令牌，并将其转换为3D张量。</li><li id="c996" class="oo op iq lj b lk oz ln pa lq pb lu pc ly pd mc ov ow ox oy bi translated">在模型中添加<code class="fe qg qh qi qj b">num_mixer_layers</code>混合器层。</li><li id="25ed" class="oo op iq lj b lk oz ln pa lq pb lu pc ly pd mc ov ow ox oy bi translated">最后，一个<code class="fe qg qh qi qj b">LayerNormalization</code>层加上一个<code class="fe qg qh qi qj b">GlobalAveragePooling1D</code>层。</li><li id="5c3a" class="oo op iq lj b lk oz ln pa lq pb lu pc ly pd mc ov ow ox oy bi translated">最后，我们最喜欢的softmax激活的<code class="fe qg qh qi qj b">Dense</code>层。</li></ol><p id="def9" class="pw-post-body-paragraph lh li iq lj b lk ll ka lm ln lo kd lp lq lr ls lt lu lv lw lx ly lz ma mb mc ij bi translated">下面是<code class="fe qg qh qi qj b">tf.keras.utils.plot_model</code>的输出，描绘了一个单独的混合器层，</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi rc"><img src="../Images/524396b55464588a32da26ceb8d04052.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Z3pIsSeEGwHvHmcVlMGSHg.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">图14:绘图模型的输出。来源:作者创作。</p></figure><p id="e738" class="pw-post-body-paragraph lh li iq lj b lk ll ka lm ln lo kd lp lq lr ls lt lu lv lw lx ly lz ma mb mc ij bi translated"><code class="fe qg qh qi qj b">model.summary()</code>的输出，</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi rd"><img src="../Images/ac72d3210e950eda6ea65bc5b61b2c38.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xajfqkOn7HEV8HaR4X7VRg.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">图15:模型摘要()的输出。来源:作者创作</p></figure><p id="c930" class="pw-post-body-paragraph lh li iq lj b lk ll ka lm ln lo kd lp lq lr ls lt lu lv lw lx ly lz ma mb mc ij bi translated">就这样，我们刚刚在TensorFlow中实现了一个MLP混合器模型！</p><h1 id="e61e" class="pl nx iq bd ny pm re po ob pp rf pr oe kf rg kg oh ki rh kj ok kl ri km on pv bi translated">💪🏼来自作者的更多项目/博客/资源</h1><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="nu nv l"/></div></figure><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="nu nv l"/></div></figure><h1 id="4c4f" class="pl nx iq bd ny pm re po ob pp rf pr oe kf rg kg oh ki rh kj ok kl ri km on pv bi translated">谢谢</h1><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi rj"><img src="../Images/9ffcfe6dfd3edf6d5e05900da4cc5c28.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*UJD5R_2mJFAKvbS7"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">皮特·佩德罗萨在Unsplash<a class="ae le" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">上的照片</a></p></figure><p id="230d" class="pw-post-body-paragraph lh li iq lj b lk ll ka lm ln lo kd lp lq lr ls lt lu lv lw lx ly lz ma mb mc ij bi translated">希望你喜欢这个故事！请随时拨打equipintelligence@gmail.com的<strong class="lj ja"><em class="md"/></strong>联系我。谢谢你，祝你有美好的一天！</p></div></div>    
</body>
</html>