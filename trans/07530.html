<html>
<head>
<title>5-Minute Data Science Design Patterns I: Callback</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">5分钟数据科学设计模式I:回访</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/5-minutes-data-science-design-patterns-i-callback-b5c0738be277?source=collection_archive---------23-----------------------#2021-07-09">https://towardsdatascience.com/5-minutes-data-science-design-patterns-i-callback-b5c0738be277?source=collection_archive---------23-----------------------#2021-07-09</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="ecc6" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">数据科学设计模式的迷你集合——从回调开始</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/50e02b08e19968b9b17805cddece93ec.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*94jcuCqbl9butjmz"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><a class="ae kv" href="https://unsplash.com/photos/71CjSSB83Wo" rel="noopener ugc nofollow" target="_blank">Unsplash</a><a class="kw kx ep" href="https://medium.com/u/1867b379dd15?source=post_page-----b5c0738be277--------------------------------" rel="noopener" target="_blank">Pavan Trikutam</a></p></figure><p id="4e2a" class="pw-post-body-paragraph ky kz iq la b lb lc jr ld le lf ju lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated"><em class="lu">注意:这些系列是为数据科学家编写的软件设计快速入门，比设计模式圣经更轻量级的东西— </em> <a class="ae kv" href="https://www.amazon.com/Clean-Code-Handbook-Software-Craftsmanship/dp/0132350882" rel="noopener ugc nofollow" target="_blank"> <em class="lu">干净代码</em> </a> <em class="lu">我希望在我刚开始学习时就存在。设计模式指的是一些常见问题的可重用解决方案，有些恰好对数据科学有用。很有可能其他人已经解决了你的问题。如果使用得当，它有助于降低代码的复杂性。</em></p><p id="7bee" class="pw-post-body-paragraph ky kz iq la b lb lc jr ld le lf ju lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">由于代码块比较多，这里的<a class="ae kv" href="https://noklam.github.io/blog/python/kedro/2021/07/10/3minutes-data-science-design-pattern-callback.html" rel="noopener ugc nofollow" target="_blank">是语法高亮的版本。</a></p><h1 id="c2d7" class="lv lw iq bd lx ly lz ma mb mc md me mf jw mg jx mh jz mi ka mj kc mk kd ml mm bi translated">那么，回调到底是什么？</h1><p id="5995" class="pw-post-body-paragraph ky kz iq la b lb mn jr ld le mo ju lg lh mp lj lk ll mq ln lo lp mr lr ls lt ij bi translated"><code class="fe ms mt mu mv b">Callback</code> function，或call after，简单来说就是一个函数会在另一个函数之后被调用。它是作为参数传递给另一个函数的一段可执行代码(函数)。<a class="ae kv" href="https://stackoverflow.com/questions/824234/what-is-a-callback-function" rel="noopener ugc nofollow" target="_blank">【1】</a></p><pre class="kg kh ki kj gt mw mv mx my aw mz bi"><span id="054e" class="na lw iq mv b gy nb nc l nd ne"><strong class="mv ir">def</strong> <strong class="mv ir">foo</strong>(x, callback<strong class="mv ir">=None</strong>):<br/>    print('foo!')<br/>    <strong class="mv ir">if</strong> callback:<br/>        callback(x)<br/>    <strong class="mv ir">return</strong> <strong class="mv ir">None</strong></span><span id="79b9" class="na lw iq mv b gy nf nc l nd ne">&gt;&gt;&gt; foo('123')<br/>foo!</span><span id="6da9" class="na lw iq mv b gy nf nc l nd ne">&gt;&gt;&gt; foo('123', print)<br/>foo!<br/>123</span></pre><p id="b3b5" class="pw-post-body-paragraph ky kz iq la b lb lc jr ld le lf ju lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">这里我将函数<code class="fe ms mt mu mv b">print</code>作为回调函数传递，因此字符串<code class="fe ms mt mu mv b">123</code>在<code class="fe ms mt mu mv b">foo!</code>之后被打印。</p><h1 id="4c59" class="lv lw iq bd lx ly lz ma mb mc md me mf jw mg jx mh jz mi ka mj kc mk kd ml mm bi translated">为什么我需要使用回调？</h1><p id="0988" class="pw-post-body-paragraph ky kz iq la b lb mn jr ld le mo ju lg lh mp lj lk ll mq ln lo lp mr lr ls lt ij bi translated">回调在高级深度学习库中非常常见，很可能你会在训练循环中找到它们。</p><ul class=""><li id="97d9" class="ng nh iq la b lb lc le lf lh ni ll nj lp nk lt nl nm nn no bi translated">fastai  — fastai为PyTorch提供高级API</li><li id="2688" class="ng nh iq la b lb np le nq lh nr ll ns lp nt lt nl nm nn no bi translated">Keras—tensor flow的高级API</li><li id="f87b" class="ng nh iq la b lb np le nq lh nr ll ns lp nt lt nl nm nn no bi translated"><a class="ae kv" href="https://github.com/pytorch/ignite" rel="noopener ugc nofollow" target="_blank">点燃</a> —使用事件&amp;处理程序，这在他们看来提供了更多的灵活性</li></ul><pre class="kg kh ki kj gt mw mv mx my aw mz bi"><span id="673b" class="na lw iq mv b gy nb nc l nd ne">import numpy <strong class="mv ir">as</strong> np<br/><br/><em class="lu"># A boring training Loop</em><br/><strong class="mv ir">def</strong> <strong class="mv ir">train</strong>(x):<br/>    n_epochs <strong class="mv ir">=</strong> 3<br/>    n_batches <strong class="mv ir">=</strong> 2<br/>    loss <strong class="mv ir">=</strong> 20<br/><br/>    <strong class="mv ir">for</strong> epoch <strong class="mv ir">in</strong> range(n_epochs):<br/>        <strong class="mv ir">for</strong> batch <strong class="mv ir">in</strong> range(n_batches):<br/>            loss <strong class="mv ir">=</strong> loss <strong class="mv ir">-</strong> 1 <em class="lu"># Pretend we are training the model</em><br/>    <strong class="mv ir">return</strong> loss</span><span id="dbd2" class="na lw iq mv b gy nf nc l nd ne">&gt;&gt;&gt; x <strong class="mv ir">=</strong> np<strong class="mv ir">.</strong>ones(10)<br/>&gt;&gt;&gt; train(x)<br/>14</span></pre><p id="5d20" class="pw-post-body-paragraph ky kz iq la b lb lc jr ld le lf ju lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">因此，假设您现在想要打印一个时期结束时的损失。您可以只添加一行代码。</p><h1 id="5efd" class="lv lw iq bd lx ly lz ma mb mc md me mf jw mg jx mh jz mi ka mj kc mk kd ml mm bi translated">简单的方法</h1><pre class="kg kh ki kj gt mw mv mx my aw mz bi"><span id="6c76" class="na lw iq mv b gy nb nc l nd ne"><strong class="mv ir">def</strong> <strong class="mv ir">train_with_print</strong>(x):<br/>    n_epochs <strong class="mv ir">=</strong> 3<br/>    n_batches <strong class="mv ir">=</strong> 2<br/>    loss <strong class="mv ir">=</strong> 20</span><span id="a4b5" class="na lw iq mv b gy nf nc l nd ne">    <strong class="mv ir">for</strong> epoch <strong class="mv ir">in</strong> range(n_epochs):<br/>        <strong class="mv ir">for</strong> batch <strong class="mv ir">in</strong> range(n_batches):<br/>            loss <strong class="mv ir">=</strong> loss <strong class="mv ir">-</strong> 1 <em class="lu"># Pretend we are training the model</em><br/>        print(f'End of Epoch. Epoch: {epoch}, Loss: {loss}')<br/>    <strong class="mv ir">return</strong> loss</span><span id="5cd3" class="na lw iq mv b gy nf nc l nd ne">train_with_print(x)</span><span id="fb67" class="na lw iq mv b gy nf nc l nd ne">End of Epoch. Epoch: 0, Loss: 18<br/>End of Epoch. Epoch: 1, Loss: 16<br/>End of Epoch. Epoch: 2, Loss: 14</span></pre><h1 id="1c9f" class="lv lw iq bd lx ly lz ma mb mc md me mf jw mg jx mh jz mi ka mj kc mk kd ml mm bi translated">回调方法</h1><p id="acd6" class="pw-post-body-paragraph ky kz iq la b lb mn jr ld le mo ju lg lh mp lj lk ll mq ln lo lp mr lr ls lt ij bi translated">或者您调用来添加一个<strong class="la ir"> PrintCallback </strong>，它做同样的事情，但是用了更多的代码。</p><pre class="kg kh ki kj gt mw mv mx my aw mz bi"><span id="a9ee" class="na lw iq mv b gy nb nc l nd ne"><strong class="mv ir">class</strong> <strong class="mv ir">Callback</strong>:<br/>    <strong class="mv ir">def</strong> <strong class="mv ir">on_epoch_start</strong>(self, x):<br/>        <strong class="mv ir">pass</strong></span><span id="6f4c" class="na lw iq mv b gy nf nc l nd ne">    <strong class="mv ir">def</strong> <strong class="mv ir">on_epoch_end</strong>(self, x):<br/>        <strong class="mv ir">pass</strong></span><span id="e415" class="na lw iq mv b gy nf nc l nd ne">    <strong class="mv ir">def</strong> <strong class="mv ir">on_batch_start</strong>(self, x):<br/>        <strong class="mv ir">pass</strong></span><span id="2cf9" class="na lw iq mv b gy nf nc l nd ne">    <strong class="mv ir">def</strong> <strong class="mv ir">on_batch_end</strong>(self, x):<br/>        <strong class="mv ir">pass</strong><br/></span><span id="436e" class="na lw iq mv b gy nf nc l nd ne"><strong class="mv ir">class</strong> <strong class="mv ir">PrintCallback</strong>(Callback):<br/>    <strong class="mv ir">def</strong> <strong class="mv ir">on_epoch_end</strong>(self, x):<br/>        print(f'End of Epoch. Epoch: {epoch}, Loss: {x}')</span><span id="b714" class="na lw iq mv b gy nf nc l nd ne"><strong class="mv ir">def</strong> <strong class="mv ir">train_with_callback</strong>(x, callback<strong class="mv ir">=None</strong>):<br/>    n_epochs <strong class="mv ir">=</strong> 3<br/>    n_batches <strong class="mv ir">=</strong> 2<br/>    loss <strong class="mv ir">=</strong> 20</span><span id="ee3a" class="na lw iq mv b gy nf nc l nd ne">    <strong class="mv ir">for</strong> epoch <strong class="mv ir">in</strong> range(n_epochs):</span><span id="c9fb" class="na lw iq mv b gy nf nc l nd ne">        callback<strong class="mv ir">.</strong>on_epoch_start(loss)</span><span id="2f04" class="na lw iq mv b gy nf nc l nd ne">        <strong class="mv ir">for</strong> batch <strong class="mv ir">in</strong> range(n_batches):<br/>            callback<strong class="mv ir">.</strong>on_batch_start(loss)<br/>            loss <strong class="mv ir">=</strong> loss <strong class="mv ir">-</strong> 1  <em class="lu"># Pretend we are training the model</em><br/>            callback<strong class="mv ir">.</strong>on_batch_end(loss)</span><span id="8c08" class="na lw iq mv b gy nf nc l nd ne">        callback<strong class="mv ir">.</strong>on_epoch_end(loss)<br/>    <strong class="mv ir">return</strong> loss</span><span id="2323" class="na lw iq mv b gy nf nc l nd ne">&gt;&gt;&gt; train_with_callback(x, callback<strong class="mv ir">=</strong>PrintCallback())</span><span id="220b" class="na lw iq mv b gy nf nc l nd ne">End of Epoch. Epoch: 2, Loss: 18<br/>End of Epoch. Epoch: 2, Loss: 16<br/>End of Epoch. Epoch: 2, Loss: 14</span></pre><p id="1e7c" class="pw-post-body-paragraph ky kz iq la b lb lc jr ld le lf ju lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">通常，一个回调定义了几个特定的事件<code class="fe ms mt mu mv b">on_xxx_xxx</code>，表示函数将根据相应的条件执行。所以所有回调都将继承基类<code class="fe ms mt mu mv b">Callback</code>，并覆盖期望的函数，这里我们只实现了<code class="fe ms mt mu mv b">on_epoch_end</code>方法，因为我们只想在最后显示损失。</p><p id="2086" class="pw-post-body-paragraph ky kz iq la b lb lc jr ld le lf ju lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">为做一件简单的事情而写这么多代码似乎有些笨拙，但是有很好的理由。考虑现在你需要添加更多的功能，你会怎么做？</p><ul class=""><li id="fa9c" class="ng nh iq la b lb lc le lf lh ni ll nj lp nk lt nl nm nn no bi translated">模型检查点</li><li id="b320" class="ng nh iq la b lb np le nq lh nr ll ns lp nt lt nl nm nn no bi translated">提前停止</li><li id="373c" class="ng nh iq la b lb np le nq lh nr ll ns lp nt lt nl nm nn no bi translated">学习率计划程序</li></ul><p id="eba2" class="pw-post-body-paragraph ky kz iq la b lb lc jr ld le lf ju lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">你可以只在循环中添加代码，但是它将开始成长为一个真正的大函数。测试这个功能是不可能的，因为它同时做10件事。此外，额外的代码甚至可能与训练逻辑无关，它们只是用来保存模型或绘制图表。所以，最好把逻辑分开，一个功能按照<a class="ae kv" href="https://en.wikipedia.org/wiki/SOLID" rel="noopener ugc nofollow" target="_blank">单一责任原则</a>应该只做1件事。这有助于你降低复杂性，因为你不需要担心你会不小心打碎10件东西，一次只考虑一件事情会容易得多。</p><p id="9a38" class="pw-post-body-paragraph ky kz iq la b lb lc jr ld le lf ju lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">当使用回调模式时，我可以多实现几个类，而训练循环几乎没有被触及。我不得不改变训练函数一点，因为它应该接受1个以上的回调。</p><p id="b7fe" class="pw-post-body-paragraph ky kz iq la b lb lc jr ld le lf ju lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">包装回调列表的<strong class="la ir">回调</strong>类</p><pre class="kg kh ki kj gt mw mv mx my aw mz bi"><span id="ee1f" class="na lw iq mv b gy nb nc l nd ne"><strong class="mv ir">class</strong> <strong class="mv ir">Callbacks</strong>:<br/>    """<br/>    It is the container for callback<br/>    """<br/><br/>    <strong class="mv ir">def</strong> __init__(self, callbacks):<br/>        self<strong class="mv ir">.</strong>callbacks <strong class="mv ir">=</strong> callbacks<br/><br/>    <strong class="mv ir">def</strong> <strong class="mv ir">on_epoch_start</strong>(self, x):<br/>        <strong class="mv ir">for</strong> callback <strong class="mv ir">in</strong> self<strong class="mv ir">.</strong>callbacks:<br/>            callback<strong class="mv ir">.</strong>on_epoch_start(x)<br/><br/>    <strong class="mv ir">def</strong> <strong class="mv ir">on_epoch_end</strong>(self, x):<br/>        <strong class="mv ir">for</strong> callback <strong class="mv ir">in</strong> self<strong class="mv ir">.</strong>callbacks:<br/>            callback<strong class="mv ir">.</strong>on_epoch_end(x)<br/><br/>    <strong class="mv ir">def</strong> <strong class="mv ir">on_batch_start</strong>(self, x):<br/>        <strong class="mv ir">for</strong> callback <strong class="mv ir">in</strong> self<strong class="mv ir">.</strong>callbacks:<br/>            callback<strong class="mv ir">.</strong>on_batch_start(x)<br/><br/>    <strong class="mv ir">def</strong> <strong class="mv ir">on_batch_end</strong>(self, x):<br/>        <strong class="mv ir">for</strong> callback <strong class="mv ir">in</strong> self<strong class="mv ir">.</strong>callbacks:<br/>            callback<strong class="mv ir">.</strong>on_batch_end(x)</span></pre><p id="c733" class="pw-post-body-paragraph ky kz iq la b lb lc jr ld le lf ju lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">附加回调的伪实现</p><pre class="kg kh ki kj gt mw mv mx my aw mz bi"><span id="29ae" class="na lw iq mv b gy nb nc l nd ne"><br/><strong class="mv ir">class</strong> <strong class="mv ir">PrintCallback</strong>(Callback):<br/>    <strong class="mv ir">def</strong> <strong class="mv ir">on_epoch_end</strong>(self, x):<br/>        print(f'[{type(self)<strong class="mv ir">.</strong>__name__}]: End of Epoch. Epoch: {epoch}, Loss: {x}')<br/>        <br/><strong class="mv ir">class</strong> <strong class="mv ir">ModelCheckPoint</strong>(Callback):<br/>    <strong class="mv ir">def</strong> <strong class="mv ir">on_epoch_end</strong>(self, x):<br/>        print(f'[{type(self)<strong class="mv ir">.</strong>__name__}]: Save Model')<br/><br/><br/><strong class="mv ir">class</strong> <strong class="mv ir">EarlyStoppingCallback</strong>(Callback):<br/>    <strong class="mv ir">def</strong> <strong class="mv ir">on_epoch_end</strong>(self, x):<br/>        <strong class="mv ir">if</strong> loss <strong class="mv ir">&lt;</strong> 3:<br/>            print(f'[{type(self)<strong class="mv ir">.</strong>__name__}]: Early Stopped')<br/><br/><br/><strong class="mv ir">class</strong> <strong class="mv ir">LearningRateScheduler</strong>(Callback):<br/>    <strong class="mv ir">def</strong> <strong class="mv ir">on_batch_end</strong>(self, x):<br/>        print(f'    [{type(self)<strong class="mv ir">.</strong>__name__}]: Reduce learning rate')<br/><br/><br/><strong class="mv ir">def</strong> <strong class="mv ir">train_with_callbacks</strong>(x, callbacks<strong class="mv ir">=None</strong>):<br/>    n_epochs <strong class="mv ir">=</strong> 3<br/>    n_batches <strong class="mv ir">=</strong> 6<br/>    loss <strong class="mv ir">=</strong> 20<br/><br/>    <strong class="mv ir">for</strong> epoch <strong class="mv ir">in</strong> range(n_epochs):<br/><br/>        callbacks<strong class="mv ir">.</strong>on_epoch_start(loss)                             <em class="lu"># on_epoch_start</em><br/>        <strong class="mv ir">for</strong> batch <strong class="mv ir">in</strong> range(n_batches):<br/>            callbacks<strong class="mv ir">.</strong>on_batch_start(loss)                         <em class="lu"># on_batch_start</em><br/>            loss <strong class="mv ir">=</strong> loss <strong class="mv ir">-</strong> 1  <em class="lu"># Pretend we are training the model</em><br/>            callbacks<strong class="mv ir">.</strong>on_batch_end(loss)                           <em class="lu"># on_batch_end</em><br/>        callbacks<strong class="mv ir">.</strong>on_epoch_end(loss)                               <em class="lu"># on_epoch_end</em><br/>    <strong class="mv ir">return</strong> loss</span></pre><p id="9cc9" class="pw-post-body-paragraph ky kz iq la b lb lc jr ld le lf ju lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">这是结果。</p><pre class="kg kh ki kj gt mw mv mx my aw mz bi"><span id="467d" class="na lw iq mv b gy nb nc l nd ne">&gt;&gt;&gt; callbacks <strong class="mv ir">=</strong> Callbacks([PrintCallback(), ModelCheckPoint(),<br/>                      EarlyStoppingCallback(), LearningRateScheduler()])<br/>&gt;&gt;&gt; train_with_callbacks(x, callbacks<strong class="mv ir">=</strong>callbacks)</span><span id="9217" class="na lw iq mv b gy nf nc l nd ne">[LearningRateScheduler]: Reduce learning rate<br/>    [LearningRateScheduler]: Reduce learning rate<br/>    [LearningRateScheduler]: Reduce learning rate<br/>    [LearningRateScheduler]: Reduce learning rate<br/>    [LearningRateScheduler]: Reduce learning rate<br/>    [LearningRateScheduler]: Reduce learning rate<br/>[PrintCallback]: End of Epoch. Epoch: 2, Loss: 14<br/>[ModelCheckPoint]: Save Model<br/>    [LearningRateScheduler]: Reduce learning rate<br/>    [LearningRateScheduler]: Reduce learning rate<br/>    [LearningRateScheduler]: Reduce learning rate<br/>    [LearningRateScheduler]: Reduce learning rate<br/>    [LearningRateScheduler]: Reduce learning rate<br/>    [LearningRateScheduler]: Reduce learning rate<br/>[PrintCallback]: End of Epoch. Epoch: 2, Loss: 8<br/>[ModelCheckPoint]: Save Model<br/>    [LearningRateScheduler]: Reduce learning rate<br/>    [LearningRateScheduler]: Reduce learning rate<br/>    [LearningRateScheduler]: Reduce learning rate<br/>    [LearningRateScheduler]: Reduce learning rate<br/>    [LearningRateScheduler]: Reduce learning rate<br/>    [LearningRateScheduler]: Reduce learning rate<br/>[PrintCallback]: End of Epoch. Epoch: 2, Loss: 2<br/>[ModelCheckPoint]: Save Model</span></pre><p id="0147" class="pw-post-body-paragraph ky kz iq la b lb lc jr ld le lf ju lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">希望它能让你相信回调使代码更干净，更容易维护。如果你只是使用简单的<code class="fe ms mt mu mv b">if-else</code>语句，你可能会得到一大块<code class="fe ms mt mu mv b">if-else</code>从句。</p><h1 id="5d12" class="lv lw iq bd lx ly lz ma mb mc md me mf jw mg jx mh jz mi ka mj kc mk kd ml mm bi translated">参考</h1><ol class=""><li id="a05a" class="ng nh iq la b lb mn le mo lh nu ll nv lp nw lt nx nm nn no bi translated"><a class="ae kv" href="https://stackoverflow.com/questions/824234/what-is-a-callback-function" rel="noopener ugc nofollow" target="_blank">https://stack overflow . com/questions/824234/what-a-callback-function</a></li></ol></div></div>    
</body>
</html>