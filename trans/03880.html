<html>
<head>
<title>Image-to-Text Generation for New Yorker Cartoons</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">《纽约客》漫画的图像到文本生成</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/image-to-text-generation-for-new-yorker-cartoons-f5f145eb6208?source=collection_archive---------27-----------------------#2021-03-31">https://towardsdatascience.com/image-to-text-generation-for-new-yorker-cartoons-f5f145eb6208?source=collection_archive---------27-----------------------#2021-03-31</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="d060" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">我如何使用深度学习来创建《纽约客》的标题</h2></div><p id="f00a" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">从来没有一台电脑赢得过《纽约客》漫画字幕大赛的冠军，理由很充分:这很难。《纽约客》杂志每周举办一次竞赛，让读者为一幅无字幕的漫画提交字幕，如下图所示。获奖说明刊登在杂志上。</p><figure class="ld le lf lg gt lh gh gi paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="gh gi lc"><img src="../Images/73cf303281c9e0f44b3bca116e5242e5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rlS0VRRc70RRQAYuGBKLXA.png"/></div></div><p class="lo lp gj gh gi lq lr bd b be z dk translated">上一届《纽约客》字幕大赛的图片示例。图片来自<a class="ae lb" href="https://github.com/nextml/caption-contest-data" rel="noopener ugc nofollow" target="_blank"> NextML </a>。<a class="ae lb" href="https://creativecommons.org/licenses/by/4.0/" rel="noopener ugc nofollow" target="_blank"> CC乘4.0 </a>。</p></figure><p id="77d0" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">人们已经创建了纽约客字幕生成器，但没有一个真正使用漫画的图像来生成字幕。相反，他们使用以前的标题来预测新的标题会是什么样子。</p><p id="c223" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">直接从卡通形象转到标题很难，原因如下:</p><ul class=""><li id="21aa" class="ls lt iq kh b ki kj kl km ko lu ks lv kw lw la lx ly lz ma bi translated">卡通通常描绘现实生活中从未发生的情景和图像。(一条坐在酒吧的鱼？)</li><li id="1eca" class="ls lt iq kh b ki mb kl mc ko md ks me kw mf la lx ly lz ma bi translated">即使我们有一个模型可以识别漫画中的主题，也很难将这种理解转化为诙谐的标题。</li></ul><p id="05a9" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我将这项任务作为图像到文本算法的学习练习，经过大量的反复试验，我创建了一个图像到文本的模型，仅基于图像就可以为《纽约客》漫画制作像样的字幕。我是这样做的。</p><figure class="ld le lf lg gt lh gh gi paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="gh gi mg"><img src="../Images/879e8458064e01847f058529d869cf9a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4Kn5j61-qCphRzDKarhCLg.png"/></div></div><p class="lo lp gj gh gi lq lr bd b be z dk translated">由模型产生的样本标题。图片来自<a class="ae lb" href="https://github.com/nextml/caption-contest-data" rel="noopener ugc nofollow" target="_blank"> NextML </a>。<a class="ae lb" href="https://creativecommons.org/licenses/by/4.0/" rel="noopener ugc nofollow" target="_blank"> CC乘4.0 </a>。</p></figure><h1 id="0908" class="mh mi iq bd mj mk ml mm mn mo mp mq mr jw ms jx mt jz mu ka mv kc mw kd mx my bi translated"><strong class="ak">数据</strong></h1><p id="0ede" class="pw-post-body-paragraph kf kg iq kh b ki mz jr kk kl na ju kn ko nb kq kr ks nc ku kv kw nd ky kz la ij bi translated">第一步是获取训练数据。NEXTml有一个<a class="ae lb" href="https://github.com/nextml/caption-contest-data" rel="noopener ugc nofollow" target="_blank"> Github知识库</a>，在那里你可以访问来自186场比赛的原始卡通图像和候选人说明。每场比赛平均有大约5000个候选字幕。在下载了这个数据集之后，我将它分成了分别包含127个和59个图像的训练和测试数据集。</p><h1 id="6acd" class="mh mi iq bd mj mk ml mm mn mo mp mq mr jw ms jx mt jz mu ka mv kc mw kd mx my bi translated"><strong class="ak">型号</strong></h1><p id="a1eb" class="pw-post-body-paragraph kf kg iq kh b ki mz jr kk kl na ju kn ko nb kq kr ks nc ku kv kw nd ky kz la ij bi translated">我从<a class="ae lb" href="https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Image-Captioning" rel="noopener ugc nofollow" target="_blank"> sgrvinod的教程</a>中的深度图像到文本模型开始。给定一幅图像，该模型通过将图像编码到描述图像中内容的向量空间中来工作。然后，该模型获取编码图像，并将其翻译成文字(“解码”)。在将编码图像翻译成文字时，注意力层会告诉模型应该关注图像的哪一部分。注意力层特别酷，因为它可以让你看到模型在选择每个单词时的焦点在哪里。</p><figure class="ld le lf lg gt lh gh gi paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="gh gi ne"><img src="../Images/b8c22ca747a01e55eac4553f1b602ec7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*L7tVZT9NKvQmg-3EIJyQAw.png"/></div></div><p class="lo lp gj gh gi lq lr bd b be z dk translated">该模型集中在滑雪板上，以确定“一个人骑着滑雪板”，并围绕滑雪板确定“雪覆盖的斜坡”图片来自<a class="ae lb" href="https://cocodataset.org/#home" rel="noopener ugc nofollow" target="_blank"> COCO2014 </a>。<a class="ae lb" href="https://creativecommons.org/licenses/by-sa/2.0/" rel="noopener ugc nofollow" target="_blank"> CC BY-SA 2.0 </a>。</p></figure><p id="5c4f" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我在大约16万张图片和说明的<a class="ae lb" href="https://cocodataset.org/#home" rel="noopener ugc nofollow" target="_blank"> COCO2014 </a>数据集上训练了一个模型(“COCO模型”)。然而，如下图所示，COCO2014的图片和说明与我们想为《纽约客》制作的图片和说明大相径庭。这些图像是真实生活中的照片，描述非常简单。看不到喜剧。下一步是弄清楚:我们如何把这个预先训练好的模型改编成《纽约客》的字幕？这是最困难的部分。</p><figure class="ld le lf lg gt lh gh gi paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="gh gi nf"><img src="../Images/acbb5c4422361f626ceb1d6fa1e8786a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pBxSJP41xUIH7x3n2QxleA.png"/></div></div><p class="lo lp gj gh gi lq lr bd b be z dk translated">来自<a class="ae lb" href="https://cocodataset.org/#home" rel="noopener ugc nofollow" target="_blank"> COCO2014 </a>数据集的样本图像和标题。<a class="ae lb" href="https://creativecommons.org/licenses/by-sa/2.0/" rel="noopener ugc nofollow" target="_blank"> CC BY-SA 2.0 </a>。</p></figure><h1 id="249e" class="mh mi iq bd mj mk ml mm mn mo mp mq mr jw ms jx mt jz mu ka mv kc mw kd mx my bi translated">文本预处理</h1><p id="55f3" class="pw-post-body-paragraph kf kg iq kh b ki mz jr kk kl na ju kn ko nb kq kr ks nc ku kv kw nd ky kz la ij bi translated">对上述问题的回答最终是“非常仔细的预处理”</p><h2 id="a10a" class="ng mi iq bd mj nh ni dn mn nj nk dp mr ko nl nm mt ks nn no mv kw np nq mx nr bi translated">词汇扩展</h2><p id="78c0" class="pw-post-body-paragraph kf kg iq kh b ki mz jr kk kl na ju kn ko nb kq kr ks nc ku kv kw nd ky kz la ij bi translated">我需要做的第一件事是扩展COCO模型的词汇。《纽约客》漫画中包含的“雪人”或“穴居人”等概念从未出现在COCO2014数据集中。为了解决这个问题，我将《纽约客》数据集中的单词添加到COCO模型的词汇表中，并重新训练了COCO模型。这使词汇量从9490个单词增加到11865个单词。</p><h2 id="26e3" class="ng mi iq bd mj nh ni dn mn nj nk dp mr ko nl nm mt ks nn no mv kw np nq mx nr bi translated">字幕过滤</h2><p id="b5f2" class="pw-post-body-paragraph kf kg iq kh b ki mz jr kk kl na ju kn ko nb kq kr ks nc ku kv kw nd ky kz la ij bi translated">在《纽约客》的数据集中，一幅漫画的候选标题彼此大相径庭。有的提到了图像中的物体，有的没有。字幕也以不同的单词开始，并且具有不同的语法结构:</p><blockquote class="ns nt nu"><p id="2cb6" class="kf kg nv kh b ki kj jr kk kl km ju kn nw kp kq kr nx kt ku kv ny kx ky kz la ij bi translated">“你什么意思？你只在星期五供应鱼”<br/>“每个学校都拒绝了我”<br/>“我知道什么东西被掺水了，伙计。”</p></blockquote><p id="499b" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">为了使模型更容易解决图像到文本的问题，我用两种方法过滤了候选标题:</p><ul class=""><li id="a87b" class="ls lt iq kh b ki kj kl km ko lu ks lv kw lw la lx ly lz ma bi translated">结构:标题必须以代词开头(如“我”、“你”、“他们”)。这是为了使标题结构更容易预测。</li><li id="fb3a" class="ls lt iq kh b ki mb kl mc ko md ks me kw mf la lx ly lz ma bi translated">内容:标题必须引用图像中的对象。这有两个原因。首先，由于我们直接从图像到标题，我想强制模型使用图像中的对象来获得标题。第二是关键词(例如“鱼”)出现在标题中，使模型更容易学习。为了弄清楚图像中有哪些物体，我使用了一种粗略的启发式方法，从漫画的描述中提取名词，并迫使标题至少引用一两个名词。</li></ul><p id="7084" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这些过滤器将每部漫画的平均标题数量从5000个减少到10个，但这仍然是足够训练模型的数据[3]。</p><h1 id="8c39" class="mh mi iq bd mj mk ml mm mn mo mp mq mr jw ms jx mt jz mu ka mv kc mw kd mx my bi translated">迁移学习</h1><p id="9fd0" class="pw-post-body-paragraph kf kg iq kh b ki mz jr kk kl na ju kn ko nb kq kr ks nc ku kv kw nd ky kz la ij bi translated">清理完数据后，我继续训练COCO模型，给它提供来自《纽约客》数据集的图片和说明。额外的训练意味着模型可以对其参数进行小的调整以适应新的任务。这种技术通常被称为迁移学习。值得注意的是，我将学习率保持在较高的水平(本例中为1e-4)，以迫使模型学习一种新的“有趣”的句法结构，并识别黑白图像中的对象。</p><h1 id="6753" class="mh mi iq bd mj mk ml mm mn mo mp mq mr jw ms jx mt jz mu ka mv kc mw kd mx my bi translated">结果</h1><p id="2788" class="pw-post-body-paragraph kf kg iq kh b ki mz jr kk kl na ju kn ko nb kq kr ks nc ku kv kw nd ky kz la ij bi translated">这里有一些由测试集上的模型生成的示例字幕。</p><figure class="ld le lf lg gt lh gh gi paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="gh gi nz"><img src="../Images/0f95d45756068e6c737691ca6ddd7fca.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EiRn8GwredbD7q5oXuQUmw.png"/></div></div><p class="lo lp gj gh gi lq lr bd b be z dk translated">模型生成的示例标题。来自<a class="ae lb" href="https://github.com/nextml/caption-contest-data" rel="noopener ugc nofollow" target="_blank"> NextML </a>的图片。<a class="ae lb" href="https://creativecommons.org/licenses/by/4.0/" rel="noopener ugc nofollow" target="_blank"> CC乘4.0 </a>。</p></figure><p id="fd9e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">许多字幕有70%在那里，只需要一些调整就可以使它们配得上字幕(例如，中间底部的照片:“对不起，这是一个<em class="nv">社交</em>派对”→“对不起，这是一个<em class="nv">数字</em>派对”或“<em class="nv">反社交</em>派对”)。话虽如此，我对迁移学习如何让模型识别卡通黑白图像中的主题印象深刻(例如，洞穴画中的“洞穴”，走进厨房的人的“厨师”)。也许再做一些调整，我们就能拥有一个有竞争力的人工智能卡通字幕了。</p><p id="60f4" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这是这位模特提交给本周《纽约客》字幕竞赛(竞赛#749)的作品。</p><figure class="ld le lf lg gt lh gh gi paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="gh gi nz"><img src="../Images/d3f7960e15883f81d99d641fb1137aab.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PfgyFRZZbtkw1rCaUfergw.png"/></div></div><p class="lo lp gj gh gi lq lr bd b be z dk translated">模特参加《纽约客》字幕竞赛的作品。</p></figure><h1 id="79aa" class="mh mi iq bd mj mk ml mm mn mo mp mq mr jw ms jx mt jz mu ka mv kc mw kd mx my bi translated">结束语</h1><p id="d819" class="pw-post-body-paragraph kf kg iq kh b ki mz jr kk kl na ju kn ko nb kq kr ks nc ku kv kw nd ky kz la ij bi translated">有很多方法可以改进这个模型，但希望这里的技术和知识可以让你在创建自己的有趣人工智能方面领先一步。我将会更多地使用这个模型，并且我很乐意听到你关于我们如何改进它的任何想法！大部分建模代码来自于<a class="ae lb" href="https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Image-Captioning" rel="noopener ugc nofollow" target="_blank"> sgrvinod的教程</a>，你可以在这个<a class="ae lb" href="https://github.com/eugenet12/newyorker-caption-preprocessing" rel="noopener ugc nofollow" target="_blank"> Github repo </a>中找到我用来预处理纽约客字幕的代码。</p><h1 id="20a1" class="mh mi iq bd mj mk ml mm mn mo mp mq mr jw ms jx mt jz mu ka mv kc mw kd mx my bi translated">笔记</h1><p id="68bf" class="pw-post-body-paragraph kf kg iq kh b ki mz jr kk kl na ju kn ko nb kq kr ks nc ku kv kw nd ky kz la ij bi translated">[1]更准确地说，以前的尝试在以前的字幕上训练语言模型来生成新的字幕。比如说马尔可夫模型。一些简单的例子包括:</p><ul class=""><li id="bb7e" class="ls lt iq kh b ki kj kl km ko lu ks lv kw lw la lx ly lz ma bi translated"><a class="ae lb" href="https://www.theverge.com/2015/8/27/9200709/new-yorker-cartoon-caption-generator-markov-program" rel="noopener ugc nofollow" target="_blank">基于过去入围者的马尔可夫模型</a>。</li><li id="1480" class="ls lt iq kh b ki mb kl mc ko md ks me kw mf la lx ly lz ma bi translated"><a class="ae lb" href="https://coolposts.online/2019/07/01/project-four/" rel="noopener ugc nofollow" target="_blank">题目→ RNN文本生成</a>。</li><li id="bc59" class="ls lt iq kh b ki mb kl mc ko md ks me kw mf la lx ly lz ma bi translated"><a class="ae lb" href="https://yale-lily.github.io/public/danfriedman.pdf" rel="noopener ugc nofollow" target="_blank">漫画的文字描述→ RNN文字生成</a>。</li></ul><p id="0317" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">[2]技术说明:我根据漫画是否带有图像内容的文本描述来分割数据，我帮助使用这些数据来过滤用于训练的标题。如果这幅漫画没有文字描述，我就把它放在测试集中。此外，GitHub的readme声明有155个图像，但在删除重复项后，数据库实际上有184个。</p><p id="1d25" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">[3]在这种情况下，图像到文本模型的主要数据约束是字幕图像的数量，而不是每个图像的字幕数量。相比之下，COCO2014每张图片有5个标题。</p></div></div>    
</body>
</html>