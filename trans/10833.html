<html>
<head>
<title>How To Select Rows From PySpark DataFrames Based on Column Values</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如何根据列值从PySpark数据帧中选择行</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/select-rows-pyspark-df-based-on-column-values-3146afe4dee3?source=collection_archive---------4-----------------------#2021-10-20">https://towardsdatascience.com/select-rows-pyspark-df-based-on-column-values-3146afe4dee3?source=collection_archive---------4-----------------------#2021-10-20</a></blockquote><div><div class="fc ii ij ik il im"/><div class="in io ip iq ir"><div class=""/><div class=""><h2 id="9e29" class="pw-subtitle-paragraph jr it iu bd b js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki dk translated">探索如何根据PySpark数据帧中的特定条件选择一系列行</h2></div><figure class="kk kl km kn gu ko gi gj paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gi gj kj"><img src="../Images/503badd72d217fb25bf3f06af72e4786.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PrCuDndwpZ-P-pXMrAncxg.jpeg"/></div></div><p class="kv kw gk gi gj kx ky bd b be z dk translated">Anthony Yin 在<a class="ae kz" href="https://unsplash.com/collections/317099/unsplash-editorial?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><h2 id="de85" class="la lb iu bd lc ld le dn lf lg lh dp li lj lk ll lm ln lo lp lq lr ls lt lu lv bi translated">介绍</h2><p id="395e" class="pw-post-body-paragraph lw lx iu ly b lz ma jv mb mc md jy me lj mf mg mh ln mi mj mk lr ml mm mn mo in bi translated">过滤数据帧行是PySpark中最常执行的操作之一。在今天的简短指南中，我们将讨论如何以几种不同的方式根据特定条件选择一系列行。</p><p id="a209" class="pw-post-body-paragraph lw lx iu ly b lz mp jv mb mc mq jy me lj mr mg mh ln ms mj mk lr mt mm mn mo in bi translated">具体来说，我们将探索如何使用</p><ul class=""><li id="2954" class="mu mv iu ly b lz mp mc mq lj mw ln mx lr my mo mz na nb nc bi translated"><code class="fe nd ne nf ng b">filter()</code>功能</li><li id="f817" class="mu mv iu ly b lz nh mc ni lj nj ln nk lr nl mo mz na nb nc bi translated"><code class="fe nd ne nf ng b">where()</code>功能</li><li id="f71b" class="mu mv iu ly b lz nh mc ni lj nj ln nk lr nl mo mz na nb nc bi translated">Spark SQL</li></ul></div><div class="ab cl nm nn hy no" role="separator"><span class="np bw bk nq nr ns"/><span class="np bw bk nq nr ns"/><span class="np bw bk nq nr"/></div><div class="in io ip iq ir"><p id="26a7" class="pw-post-body-paragraph lw lx iu ly b lz mp jv mb mc mq jy me lj mr mg mh ln ms mj mk lr mt mm mn mo in bi translated">首先，让我们创建一个示例数据框架，我们将在本文中引用它来演示几个概念。</p><pre class="kk kl km kn gu nt ng nu nv aw nw bi"><span id="0491" class="la lb iu ng b gz nx ny l nz oa">from pyspark.sql import SparkSession</span><span id="1a08" class="la lb iu ng b gz ob ny l nz oa"># Create an instance of spark session<br/>spark_session = SparkSession.builder \<br/>    .master('local[1]') \<br/>    .appName('Example') \<br/>    .getOrCreate()</span><span id="d795" class="la lb iu ng b gz ob ny l nz oa">df = spark_session.createDataFrame(<br/>    [<br/>        (1, True, 1.0, 100),<br/>        (2, False, 2.0, 200),<br/>        (3, False, 3.0, 300),<br/>        (4, True, 4.0, 400),<br/>        (5, True, 5.0, 500),<br/>    ],<br/>    ['colA', 'colB', 'colC', 'colD']<br/>)<br/><br/>df.show()<br/><em class="oc">+----+-----+----+----+<br/>|colA| colB|colC|colD|<br/>+----+-----+----+----+<br/>|   1| true| 1.0| 100|<br/>|   2|false| 2.0| 200|<br/>|   3|false| 3.0| 300|<br/>|   4| true| 4.0| 400|<br/>|   5| true| 5.0| 500|<br/>+----+-----+----+----+</em></span></pre></div><div class="ab cl nm nn hy no" role="separator"><span class="np bw bk nq nr ns"/><span class="np bw bk nq nr ns"/><span class="np bw bk nq nr"/></div><div class="in io ip iq ir"><h2 id="3425" class="la lb iu bd lc ld le dn lf lg lh dp li lj lk ll lm ln lo lp lq lr ls lt lu lv bi translated">使用filter()函数选择行</h2><p id="868d" class="pw-post-body-paragraph lw lx iu ly b lz ma jv mb mc md jy me lj mf mg mh ln mi mj mk lr ml mm mn mo in bi translated">过滤数据帧行的第一个选项是基于指定条件执行过滤的<code class="fe nd ne nf ng b"><a class="ae kz" href="https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.DataFrame.filter.html" rel="noopener ugc nofollow" target="_blank">pyspark.sql.DataFrame.filter()</a></code>函数。</p><p id="b17f" class="pw-post-body-paragraph lw lx iu ly b lz mp jv mb mc mq jy me lj mr mg mh ln ms mj mk lr mt mm mn mo in bi translated">例如，假设我们只想保留那些在<code class="fe nd ne nf ng b">colC</code>中的值大于或等于<code class="fe nd ne nf ng b">3.0</code>的行。下面的表达式可以解决这个问题:</p><pre class="kk kl km kn gu nt ng nu nv aw nw bi"><span id="9a76" class="la lb iu ng b gz nx ny l nz oa"><strong class="ng iv">df = df.filter(df.colC &gt;= 3.0)</strong></span><span id="db90" class="la lb iu ng b gz ob ny l nz oa">df.show()<br/><em class="oc">+----+-----+----+----+<br/>|colA| colB|colC|colD|<br/>+----+-----+----+----+<br/>|   3|false| 3.0| 300|<br/>|   4| true| 4.0| 400|<br/>|   5| true| 5.0| 500|<br/>+----+-----+----+----+</em></span></pre><p id="18fd" class="pw-post-body-paragraph lw lx iu ly b lz mp jv mb mc mq jy me lj mr mg mh ln ms mj mk lr mt mm mn mo in bi translated">您甚至可以指定<code class="fe nd ne nf ng b">Column</code>函数，比如<code class="fe nd ne nf ng b">pyspark.sql.Column.between</code>，以便只保留指定的下限和上限之间的行，如下所示。</p><pre class="kk kl km kn gu nt ng nu nv aw nw bi"><span id="b73f" class="la lb iu ng b gz nx ny l nz oa"><strong class="ng iv">df = df.filter(df.colD.between(200, 400))</strong></span><span id="92dc" class="la lb iu ng b gz ob ny l nz oa">df.show()<br/><em class="oc">+----+-----+----+----+<br/>|colA| colB|colC|colD|<br/>+----+-----+----+----+<br/>|   2|false| 2.0| 200|<br/>|   3|false| 3.0| 300|<br/>|   4| true| 4.0| 400|<br/>+----+-----+----+----+</em></span></pre></div><div class="ab cl nm nn hy no" role="separator"><span class="np bw bk nq nr ns"/><span class="np bw bk nq nr ns"/><span class="np bw bk nq nr"/></div><div class="in io ip iq ir"><h2 id="0e0c" class="la lb iu bd lc ld le dn lf lg lh dp li lj lk ll lm ln lo lp lq lr ls lt lu lv bi translated">使用where()函数选择行</h2><p id="7dc2" class="pw-post-body-paragraph lw lx iu ly b lz ma jv mb mc md jy me lj mf mg mh ln mi mj mk lr ml mm mn mo in bi translated"><code class="fe nd ne nf ng b"><a class="ae kz" href="https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.sql.DataFrame.where.html" rel="noopener ugc nofollow" target="_blank">pyspark.sql.DataFrame.where()</a></code>是我们在上一节中讨论的<code class="fe nd ne nf ng b">filter()</code>的别名。它可以以同样的方式使用，以便根据提供的条件过滤数据帧的行。</p><pre class="kk kl km kn gu nt ng nu nv aw nw bi"><span id="5e14" class="la lb iu ng b gz nx ny l nz oa"><strong class="ng iv">df = df.where(~df.colB)</strong></span><span id="9dcb" class="la lb iu ng b gz ob ny l nz oa">df.show()<br/><em class="oc">+----+-----+----+----+<br/>|colA| colB|colC|colD|<br/>+----+-----+----+----+<br/>|   2|false| 2.0| 200|<br/>|   3|false| 3.0| 300|<br/>+----+-----+----+----+</em></span></pre></div><div class="ab cl nm nn hy no" role="separator"><span class="np bw bk nq nr ns"/><span class="np bw bk nq nr ns"/><span class="np bw bk nq nr"/></div><div class="in io ip iq ir"><h2 id="e3a6" class="la lb iu bd lc ld le dn lf lg lh dp li lj lk ll lm ln lo lp lq lr ls lt lu lv bi translated">使用Spark SQL选择行</h2><p id="ba69" class="pw-post-body-paragraph lw lx iu ly b lz ma jv mb mc md jy me lj mf mg mh ln mi mj mk lr ml mm mn mo in bi translated">或者，您甚至可以使用Spark SQL来使用SQL表达式查询数据帧。举个例子，</p><pre class="kk kl km kn gu nt ng nu nv aw nw bi"><span id="93f0" class="la lb iu ng b gz nx ny l nz oa"># Create a view for the dataframe<br/>df.createOrReplaceTempView("df_view")<br/><br/><strong class="ng iv">df = spark_session.sql(<br/>    """<br/>    SELECT * <br/>    FROM df_view<br/>    WHERE colC &gt;= 2.0<br/>    """<br/>)</strong><br/><br/>df.show()<br/><em class="oc">+----+-----+----+----+<br/>|colA| colB|colC|colD|<br/>+----+-----+----+----+<br/>|   2|false| 2.0| 200|<br/>|   3|false| 3.0| 300|<br/>|   4| true| 4.0| 400|<br/>|   5| true| 5.0| 500|<br/>+----+-----+----+----+</em></span></pre></div><div class="ab cl nm nn hy no" role="separator"><span class="np bw bk nq nr ns"/><span class="np bw bk nq nr ns"/><span class="np bw bk nq nr"/></div><div class="in io ip iq ir"><h2 id="78f3" class="la lb iu bd lc ld le dn lf lg lh dp li lj lk ll lm ln lo lp lq lr ls lt lu lv bi translated">最后的想法</h2><p id="f311" class="pw-post-body-paragraph lw lx iu ly b lz ma jv mb mc md jy me lj mf mg mh ln mi mj mk lr ml mm mn mo in bi translated">在今天的简短指南中，我们讨论了如何根据特定条件从PySpark数据帧中执行行选择。具体来说，我们展示了如何使用<code class="fe nd ne nf ng b">filter()</code>和<code class="fe nd ne nf ng b">where()</code>方法以及Spark SQL来实现这一点。</p></div><div class="ab cl nm nn hy no" role="separator"><span class="np bw bk nq nr ns"/><span class="np bw bk nq nr ns"/><span class="np bw bk nq nr"/></div><div class="in io ip iq ir"><p id="1b77" class="pw-post-body-paragraph lw lx iu ly b lz mp jv mb mc mq jy me lj mr mg mh ln ms mj mk lr mt mm mn mo in bi translated"><a class="ae kz" href="https://gmyrianthous.medium.com/membership" rel="noopener"> <strong class="ly iv">成为会员</strong> </a> <strong class="ly iv">阅读介质上的每一个故事。你的会员费直接支持我和你看的其他作家。你也可以在媒体上看到所有的故事。</strong></p></div><div class="ab cl nm nn hy no" role="separator"><span class="np bw bk nq nr ns"/><span class="np bw bk nq nr ns"/><span class="np bw bk nq nr"/></div><div class="in io ip iq ir"><p id="676e" class="pw-post-body-paragraph lw lx iu ly b lz mp jv mb mc mq jy me lj mr mg mh ln ms mj mk lr mt mm mn mo in bi translated"><strong class="ly iv">你可能也会喜欢</strong></p><div class="od oe gq gs of og"><a rel="noopener follow" target="_blank" href="/how-to-efficiently-convert-a-pyspark-dataframe-to-pandas-8bda2c3875c3"><div class="oh ab fp"><div class="oi ab oj cl cj ok"><h2 class="bd iv gz z fq ol fs ft om fv fx it bi translated">加快PySpark和Pandas数据帧之间的转换</h2><div class="on l"><h3 class="bd b gz z fq ol fs ft om fv fx dk translated">将大火花数据帧转换为熊猫时节省时间</h3></div><div class="oo l"><p class="bd b dl z fq ol fs ft om fv fx dk translated">towardsdatascience.com</p></div></div><div class="op l"><div class="oq l or os ot op ou kt og"/></div></div></a></div></div><div class="ab cl nm nn hy no" role="separator"><span class="np bw bk nq nr ns"/><span class="np bw bk nq nr ns"/><span class="np bw bk nq nr"/></div><div class="in io ip iq ir"><div class="kk kl km kn gu og"><a rel="noopener follow" target="_blank" href="/sparksession-vs-sparkcontext-vs-sqlcontext-vs-hivecontext-741d50c9486a"><div class="oh ab fp"><div class="oi ab oj cl cj ok"><h2 class="bd iv gz z fq ol fs ft om fv fx it bi translated">spark session vs spark context vs SQLContext vs hive context</h2><div class="on l"><h3 class="bd b gz z fq ol fs ft om fv fx dk translated">SparkSession、SparkContext HiveContext和SQLContext有什么区别？</h3></div><div class="oo l"><p class="bd b dl z fq ol fs ft om fv fx dk translated">towardsdatascience.com</p></div></div><div class="op l"><div class="ov l or os ot op ou kt og"/></div></div></a></div></div><div class="ab cl nm nn hy no" role="separator"><span class="np bw bk nq nr ns"/><span class="np bw bk nq nr ns"/><span class="np bw bk nq nr"/></div><div class="in io ip iq ir"><div class="kk kl km kn gu og"><a rel="noopener follow" target="_blank" href="/mastering-indexing-and-slicing-in-python-443e23457125"><div class="oh ab fp"><div class="oi ab oj cl cj ok"><h2 class="bd iv gz z fq ol fs ft om fv fx it bi translated">掌握Python中的索引和切片</h2><div class="on l"><h3 class="bd b gz z fq ol fs ft om fv fx dk translated">深入研究有序集合的索引和切片</h3></div><div class="oo l"><p class="bd b dl z fq ol fs ft om fv fx dk translated">towardsdatascience.com</p></div></div><div class="op l"><div class="ow l or os ot op ou kt og"/></div></div></a></div></div></div>    
</body>
</html>