<html>
<head>
<title>Interpretable Machine Learning using SHAP — theory and applications</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用SHAP的可解释机器学习——理论和应用</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/interpretable-machine-learning-using-shap-theory-and-applications-26c12f7a7f1a?source=collection_archive---------7-----------------------#2021-10-01">https://towardsdatascience.com/interpretable-machine-learning-using-shap-theory-and-applications-26c12f7a7f1a?source=collection_archive---------7-----------------------#2021-10-01</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="d50e" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">SHAP是一种越来越流行的用于可解释机器学习的方法。本文对Shapley附加值理论进行了分解，并用几个实例进行了说明。</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/12dd67475692929228fd53daaff9f350.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*zMBibKREpykRpAsn"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">Johannes Plenio 在<a class="ae kv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><h1 id="3f2f" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">介绍</h1><p id="ed0b" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">XGBoost等复杂的机器学习算法在预测问题上变得越来越流行。传统上，在解释和准确性之间有一个平衡，简单的模型，如线性回归，有时出于透明性和可解释性的原因是首选的。</p><p id="68ce" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">但是SHAP(或Shapley Additive Values)是改变这一趋势的许多方法之一，我们正在逐步走向复杂、准确和可解释的<strong class="lq ir">模型。</strong></p><p id="d659" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">这篇文章主要基于Scott Lundberg的演讲，我在下面的参考资料中提到了他的演讲。我将把重点放在SHAP的理论上，然后转移到一些应用上。因为代码和教程非常丰富，所以我将在源代码一节中链接一些。</p><h1 id="1bf5" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated"><strong class="ak">(一)导致SHAP的理论</strong></h1><blockquote class="mp"><p id="9583" class="mq mr iq bd ms mt mu mv mw mx my mj dk translated">SHAP的方法是<strong class="ak">解释机器学习模型的小部分复杂性。所以我们从解释单个预测开始，一次一个。</strong></p></blockquote><p id="b9b7" class="pw-post-body-paragraph lo lp iq lq b lr mz jr lt lu na ju lw lx nb lz ma mb nc md me mf nd mh mi mj ij bi translated">记住这一点很重要:我们正在解释每个特征<strong class="lq ir">对单个预测值</strong>的贡献。在线性回归中，我们通过构造(X*Beta)知道每个特征X的贡献，这些贡献的总和解释了结果Y，导致对模型的直观解释(见图1)。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ne"><img src="../Images/3817fdd91586c184c6af881a6abae773.png" data-original-src="https://miro.medium.com/v2/resize:fit:850/format:webp/1*24TAW0mKioebxg0xztC7tg.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图1:线性模型中的贡献，其中X变量预测结果Y，图片作者</p></figure><p id="db71" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">SHAP的想法是找到一个函数Phi来将功劳归于预测，就像线性模型中一样:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nf"><img src="../Images/80a355ae0b5f711978ba8b57571758e9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1122/format:webp/1*KoOS8UFJT7RivOpUvQ6bgw.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图2:机器学习模型中的贡献，其中X变量预测结果Y，作者图片</p></figure><p id="11e4" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">那么，我们如何获得钓鱼网站呢？(注意图2中Phi与数据X和机器学习模型f的相关性)</p><p id="4b39" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">为了便于说明，让我们假设我们正在使用XGBoost算法，根据两个变量X1和X2来预测Y，即未来住院的概率:</p><p id="fe7f" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated"><strong class="lq ir"> X1 </strong>:患者去年住院<br/> <strong class="lq ir"> X2 </strong>:患者去年去过急诊室</p><p id="ba26" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">从这个算法中，我们得到的平均预测概率(未来住院)为5%。</p><p id="ee52" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">对于SHAP，我们试图解释一个单独的预测。所以让我们以病人A为例，试着解释他们住院的概率。让我们想象一下，根据我们的机器学习模型，这个概率是27%。我们可以问自己:<strong class="lq ir">我们是如何偏离平均模型预测(5%)而得到患者A的预测(27%)的？</strong></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ng"><img src="../Images/c9fab60a646da55379cad09180ec564c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*y5bNfFmlXzPRztyRKsVVIg.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图3:患者A入院的预测概率，作者提供的图片</p></figure><p id="530e" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">获得患者A的Phi(对预测概率的贡献)的一种方法是根据观察到的特征值查看f(X)的期望值。我们知道病人A在过去的一年里既住院又急诊，所以X1和X2都等于1。</p><p id="0f49" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">我们首先计算:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nh"><img src="../Images/d8897833a136fc01fddcf47514ce93d7.png" data-original-src="https://miro.medium.com/v2/resize:fit:300/format:webp/1*B6qdADygrKP1DXYZkwr_4w.png"/></div></figure><p id="be1d" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">然后:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/0dab82d55cff2354acd5b1aa9f06f7e7.png" data-original-src="https://miro.medium.com/v2/resize:fit:486/format:webp/1*uL9uahPNZgZQg6XlXvGJUQ.png"/></div></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nj"><img src="../Images/d60c6cb3272726c4dd598357fd1dd2d0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VRxJ3JDophFefwWFcdgdZQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图4:根据特征贡献分解的患者A<strong class="bd nk">入院的预测概率，作者提供的图像</strong></p></figure><p id="6684" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">在这个例子中，我们从图4中看到，Phi1(变量X1的贡献)是:<br/> 20% -5% =15%</p><p id="a495" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">而φ2(变量X2的贡献)为:<br/> 27% -20% =7%</p><p id="1cc6" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">但是我们不能就此止步。事实证明，获得Phi并不那么简单，因为<strong class="lq ir">排序关系到</strong>计算它们(我们从哪个变量开始？).这是因为在存在相互作用的情况下，相互作用对最终预测的贡献会到达顺序中的最后一个变量。</p><p id="5ae2" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">例如，如果我们的变量X1和X2之间存在相互作用，使得如果它们都等于1，则最终预测概率更高，当我们计算第二项时，相互作用的贡献将被第二有序变量X2吸收(或者简单地说，添加到第二有序变量的贡献中):</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/0dab82d55cff2354acd5b1aa9f06f7e7.png" data-original-src="https://miro.medium.com/v2/resize:fit:486/format:webp/1*uL9uahPNZgZQg6XlXvGJUQ.png"/></div></figure><p id="49d5" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">让我们通过颠倒条件作用的顺序来想象这一点。所以这一次，我们开始以X2而不是X1为条件。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nl"><img src="../Images/82cf88d54208ac88be6fb177b76139fb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EBcXfUZWejHFknYzkiM1Bg.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图5:患者A住院的预测概率，<strong class="bd nk">按特征贡献</strong>，<strong class="bd nk"> </strong>作者提供的图像</p></figure><p id="cae3" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">现在我们从图5中看到Phi1(变量X1的贡献)是:<br/> 27% -10% =17%(之前我们发现这是15%)</p><p id="a681" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">而Phi2(变量X2的贡献)是:<br/> 10% -5% =5%(前面我们发现这是7%)</p><p id="65e9" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">因此，通过顺序处理这个问题，我们看到，由于潜在的相互作用，我们会得到不稳定的Phi值，这取决于我们开始处理的变量。因此，问题变成了:<strong class="lq ir">有没有一种好的方法在一组输入中分配贡献，而不必考虑顺序？</strong></p><blockquote class="mp"><p id="b06f" class="mq mr iq bd ms mt mu mv mw mx my mj dk translated"><strong class="ak">解决方案来自博弈论，特别是沙普利勋爵在20世纪50年代提出的沙普利价值观。</strong></p></blockquote><p id="6243" class="pw-post-body-paragraph lo lp iq lq b lr mz jr lt lu na ju lw lx nb lz ma mb nc md me mf nd mh mi mj ij bi translated">在这种背景下，玩家(特征)一起玩一个游戏(一个观察)来赢得一个奖励(一个预测)。一些玩家在赢得游戏中比其他人贡献更多，但是玩家彼此互动(交互)来赢得游戏。那么，我们如何公平地分配奖金呢？</p><p id="546e" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">Shapley建立了一些假设，定义了公平的属性，导致了奖金分配的唯一解决方案:“Shapley值”。理论上，Shapley值很容易计算，因为它们是所有可能的排序N！(当N是特征的数量时)。但是在实践中<strong class="lq ir">这种方法计算量很大</strong>，因此作者找到了更快的方法来计算一类函数的Shapley值(例如树)。</p><p id="630e" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">在我们继续看一些插图之前，让我们注意一个主要的警告:</p><blockquote class="mp"><p id="acdb" class="mq mr iq bd ms mt mu mv mw mx my mj dk translated">如果特征不是独立的，Phi就不会完全准确</p></blockquote><p id="1429" class="pw-post-body-paragraph lo lp iq lq b lr mz jr lt lu na ju lw lx nb lz ma mb nc md me mf nd mh mi mj ij bi translated"><strong class="lq ir">模型的一个主要假设是独立性，</strong>这有助于条件预期的计算。让我们看看怎么做。<br/>我们以ε(f(X)| X1 = 1)为例，其计算公式如下:</p><p id="e126" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">(a)将感兴趣的特征固定到其值(在我们的示例中，X1=1) <br/> (b)从其他特征随机采样值(在我们的示例中，从变量X2随机采样值)<br/> (c)将从(a)和(b)生成的合成观测值(X1，X2)馈送到模型f(X)中以获得预测<br/> (d)取预测的平均值</p><p id="7602" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">这些步骤在理论上将近似ε(f(X)| X1 = 1，但是如果特征之间存在高度相关性，则显然步骤(b)会中断:通过在(a)中固定X1，并且在(b)中随机采样X2，我们中断了X1和X2之间的相关性。</p><p id="3be8" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">现在我们已经讨论了一些理论，让我们看看我们能在实践中用SHAP做些什么！</p><h1 id="ecac" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated"><strong class="ak">(二)SHAP实践中的几个例证</strong></h1><h2 id="3c0b" class="nm kx iq bd ky nn no dn lc np nq dp lg lx nr ns li mb nt nu lk mf nv nw lm nx bi translated"><strong class="ak"> (a) SHAP为每个单独的观察给出特征贡献</strong></h2><p id="9128" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">我们讨论了SHAP是如何主要关注于估计个人贡献的。一个<strong class="lq ir"> <em class="ny">力plo </em> t </strong>总结了每个特征如何对一个单独的预测做出贡献。</p><p id="cec9" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">在下面的例子中，研究人员正在使用一个黑盒模型来预测手术室中低氧血症(低氧血症)的风险。红色要素显示对预测赔率有正面(增加)影响，而绿色要素显示对预测赔率有负面(减少)影响。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nz"><img src="../Images/95455ddac01012b5b60068bc0abf82f1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BjW78u1NLWnm5IqNDdq4Mw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图6:力图示例，来源:Scott Lundberg的演示，如下所示</p></figure><p id="5ee8" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">图6显示了特定患者的力图(记住，我们一次解释一个观察结果！).我们可以看到，他们的低潮气量(进出肺部的空气量)导致了低氧血症风险的增加(约0.5)。</p><blockquote class="mp"><p id="f484" class="mq mr iq bd ms mt mu mv mw mx my mj dk translated">这些关系不一定是因果关系。特征影响可能是由于相关性。</p></blockquote><h2 id="adff" class="nm kx iq bd ky nn oa dn lc np ob dp lg lx oc ns li mb od nu lk mf oe nw lm nx bi translated">(b) SHAP给出了全局解释和特征重要性</h2><p id="7751" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">(a)中描述的局部解释可以放在一起得到一个<strong class="lq ir"> <em class="ny">全局解释</em> </strong>。由于SHAP的公理化假设，全球SHAP解释可能比基尼指数等其他指标更可靠。</p><blockquote class="mp"><p id="1ced" class="mq mr iq bd ms mt mu mv mw mx my mj dk translated">SHAP可以提供更好的特征选择能力</p></blockquote><p id="53aa" class="pw-post-body-paragraph lo lp iq lq b lr mz jr lt lu na ju lw lx nb lz ma mb nc md me mf nd mh mi mj ij bi translated">在下面的例子中，研究人员基于一组基线变量预测死亡率风险。图7显示了全局特征的重要性。一个简单的例子是，年龄是死亡最具预测性的变量。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi of"><img src="../Images/5a181ac206333b14a1a57da069e50653.png" data-original-src="https://miro.medium.com/v2/resize:fit:1072/format:webp/1*XTK_mLqbRRc72q8ClDvtHQ.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图7:特性重要性的例子，来源:下面列出的Scott Lundberg的演示</p></figure><p id="21af" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">具有全球特征重要性的问题是<strong class="lq ir">流行与量级</strong>混合。这意味着:</p><blockquote class="mp"><p id="04b2" class="mq mr iq bd ms mt mu mv mw mx my mj dk translated"><strong class="ak">罕见的高幅值效应不会出现在特征重要性图</strong>中。</p></blockquote><p id="3ff2" class="pw-post-body-paragraph lo lp iq lq b lr mz jr lt lu na ju lw lx nb lz ma mb nc md me mf nd mh mi mj ij bi translated">让我们用斯科特·伦德伯格的例子来说明。图7中的“血液蛋白”是全球最不重要的特征。但是，当我们查看本地解释摘要(下图8，右图)时，我们发现高血液蛋白水平对某些人来说是非常具有预测性的。</p><p id="520e" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">因此，尽管这一变量在预测平均死亡风险方面似乎不如年龄重要，但对少数人来说，它与死亡风险高度相关:因此<strong class="lq ir"> SHAP值揭示了人口的异质性</strong>。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi og"><img src="../Images/6cd989f9e2ecb7b92e9e7889ca3ba60d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7pGXP7W2wWA2DArR8RmAGA.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图8:本地解释摘要示例，来源:下面列出的Scott Lundberg的演示</p></figure><h2 id="2c24" class="nm kx iq bd ky nn no dn lc np nq dp lg lx nr ns li mb nt nu lk mf nv nw lm nx bi translated">(c) SHAP揭示了异质性和相互作用</h2><p id="e010" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">我们可以用SHAP值来进一步理解异质性的来源。一种方法是使用SHAP <strong class="lq ir"> <em class="ny">部分依赖图</em> </strong>(图9)。部分相关图显示特定特征的SHAP值，并根据另一个特征对观察值进行着色。在本例中，SHAP值是相对于收缩压绘制的，观察值根据其年龄进行着色。</p><p id="f151" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">让我们再来看一下局部解释摘要图(图8，右图)。我们可以看到，较高的收缩压与较高的死亡风险相关。从下面图9中的部分相关性图，我们可以进一步说:高收缩压与更高的死亡风险相关，<strong class="lq ir">特别是如果它在年轻时开始。</strong></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/e083cdeb4bbf5a6735d413658b77d773.png" data-original-src="https://miro.medium.com/v2/resize:fit:1390/format:webp/1*-bzraVOBeIs3agMK-JLrig.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图9:SHAP部分相关图的例子，来源:下面列出的Scott Lundberg的演示</p></figure><p id="94fc" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">总结这篇文章，注意除了前面提到的，<strong class="lq ir"> SHAP值有更多的应用</strong>。值得一提的是它们在部署后监控机器学习模型的有用性。例如，SHAP值可以揭示由于源数据中的异常，一些变量突然导致机器学习模型中的更多损失。为了获得更多的见解，我鼓励任何人观看Scott Lundberg的演讲，链接如下。</p><h1 id="7375" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">未来的研究</h1><p id="6741" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">根据作者的说法，未来的研究领域包括相关特征存在时的可解释性，以及将因果假设纳入Shapley解释。</p><p id="de96" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">【来源:】<br/>(1)<a class="ae kv" href="https://youtu.be/B-c8tIgchu0" rel="noopener ugc nofollow" target="_blank">https://youtu.be/B-c8tIgchu0</a><br/>(2)<a class="ae kv" href="https://shap.readthedocs.io/en/latest/index.html" rel="noopener ugc nofollow" target="_blank">https://shap.readthedocs.io/en/latest/index.html</a><br/>(3)<a class="ae kv" href="https://christophm.github.io/interpretable-ml-book/shap.html" rel="noopener ugc nofollow" target="_blank">https://christophm . github . io/interpretable-ml-book/shap . html</a><br/>(4)<a class="ae kv" rel="noopener" target="_blank" href="/interpretable-machine-learning-with-xgboost-9ec80d148d27">https://towardsdatascience . com/interpretable-machine-learning-with-xgboost-9ec 80d 148 d27</a><br/></p></div></div>    
</body>
</html>