<html>
<head>
<title>Building a Convolutional VAE in PyTorch</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">在PyTorch构建卷积VAE</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/building-a-convolutional-vae-in-pytorch-a0f54c947f71?source=collection_archive---------2-----------------------#2021-05-02">https://towardsdatascience.com/building-a-convolutional-vae-in-pytorch-a0f54c947f71?source=collection_archive---------2-----------------------#2021-05-02</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="1ac3" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">用神经网络生成新图像？</h2></div><p id="49a4" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">深度学习在计算机视觉中的应用已经从图像分类等简单任务扩展到自动驾驶等高级任务——神经网络揭示的最迷人的领域之一是图像生成。</p><p id="859c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">随着生成对抗网络(GANs)在内容生成方面的能力和成功，我们经常忽略另一种类型的生成网络:变分自动编码器(VAE)。本文讨论了VAE的基本概念，包括体系结构和损失设计背后的直觉，并提供了一个简单卷积VAE的基于PyTorch的实现，以基于MNIST数据集生成图像。</p><h1 id="15e1" class="le lf it bd lg lh li lj lk ll lm ln lo jz lp ka lq kc lr kd ls kf lt kg lu lv bi translated">什么是VAE？</h1><h2 id="280b" class="lw lf it bd lg lx ly dn lk lz ma dp lo kr mb mc lq kv md me ls kz mf mg lu mh bi translated">自动编码器</h2><p id="d5b9" class="pw-post-body-paragraph ki kj it kk b kl mi ju kn ko mj jx kq kr mk kt ku kv ml kx ky kz mm lb lc ld im bi translated">为了理解VAE的概念，我们首先描述一个传统的自动编码器及其应用。</p><figure class="mo mp mq mr gt ms gh gi paragraph-image"><div role="button" tabindex="0" class="mt mu di mv bf mw"><div class="gh gi mn"><img src="../Images/0f17ed157227ddf1ad500056eb0a8036.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cyTVANJnh_yfTg_1kRr3hA.png"/></div></div><p class="mz na gj gh gi nb nc bd b be z dk translated">图一。传统自动编码器的示意图。</p></figure><p id="b94a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在传统的计算机科学中，我们总是试图找到最佳的方法来将某个文件(无论是图像还是文档)压缩成更小的表示形式。自动编码器是一种特殊类型的神经网络，具有用于降维的瓶颈层，即潜在表示:</p><figure class="mo mp mq mr gt ms gh gi paragraph-image"><div class="gh gi nd"><img src="../Images/2b2bda704eb5af8df9d7848e790b6325.png" data-original-src="https://miro.medium.com/v2/resize:fit:156/format:webp/1*kbAZYaq2bEDll3-qDXnv1A.png"/></div></figure><figure class="mo mp mq mr gt ms gh gi paragraph-image"><div class="gh gi ne"><img src="../Images/9377c25a3880f2e5c5d4168c74bfe31b.png" data-original-src="https://miro.medium.com/v2/resize:fit:416/format:webp/1*pXHMrIQEoDWyPBtfL5I31A.png"/></div></figure><p id="3085" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">其中<em class="nf"> x </em>为原始输入，<em class="nf"> z </em>为潜在表示，<em class="nf">x’</em>为重构输入，函数<em class="nf"> f </em>和<em class="nf"> g </em>分别为编码器和解码器。目标是最小化重构输出<em class="nf"> g(f(x)) </em>和原始<em class="nf"> x </em>之间的差异，以便我们知道较小尺寸的潜在表示<em class="nf"> f(x) </em>实际上保留了足够的特征用于重构。</p><p id="6f88" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">除了满足降维的需要，自动编码器还可以用于去噪等目的，即，将扰动的<em class="nf"> x </em>输入自动编码器，并让潜在表示学习仅检索图像本身，而不检索噪声。当去噪自动编码器用深度网络构建时，我们称之为堆叠去噪自动编码器。</p><h2 id="0e38" class="lw lf it bd lg lx ly dn lk lz ma dp lo kr mb mc lq kv md me ls kz mf mg lu mh bi translated"><strong class="ak">在简单的词语中添加“变化”</strong></h2><p id="9a6d" class="pw-post-body-paragraph ki kj it kk b kl mi ju kn ko mj jx kq kr mk kt ku kv ml kx ky kz mm lb lc ld im bi translated">在对自动编码器进行简短描述后，人们可能会问，如何改变这种网络设计来生成内容——这就是“变化”概念的来源。</p><p id="b4d8" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">当我们正则化自动编码器，使其潜在表示不会过度拟合到单个数据点，而是整个数据分布时(关于防止过度拟合的技术，请参考本文<a class="ae ng" rel="noopener" target="_blank" href="/8-simple-techniques-to-prevent-overfitting-4d443da2ef7d">)，我们可以从潜在空间执行随机采样，从而从分布中生成看不见的图像，使我们的自动编码器变得“可变”。为此，我们将KL散度的思想融入到我们的损失函数设计中(关于KL散度的更多细节，请参考</a><a class="ae ng" href="https://wiseodd.github.io/techblog/2016/12/10/variational-autoencoder/" rel="noopener ugc nofollow" target="_blank">这篇文章</a>)。下面几节将深入介绍使用PyTorch从头开始构建VAE的具体过程。</p><h1 id="e2e2" class="le lf it bd lg lh li lj lk ll lm ln lo jz lp ka lq kc lr kd ls kf lt kg lu lv bi translated">计算环境</h1><h2 id="18c5" class="lw lf it bd lg lx ly dn lk lz ma dp lo kr mb mc lq kv md me ls kz mf mg lu mh bi translated">图书馆</h2><p id="050b" class="pw-post-body-paragraph ki kj it kk b kl mi ju kn ko mj jx kq kr mk kt ku kv ml kx ky kz mm lb lc ld im bi translated">整个程序仅通过PyTorch库(包括torchvision)构建。在评估结果时，我们还使用Matplotlib和NumPy库进行数据可视化。这些库可以按如下方式导入:</p><figure class="mo mp mq mr gt ms"><div class="bz fp l di"><div class="nh ni l"/></div></figure><h2 id="f055" class="lw lf it bd lg lx ly dn lk lz ma dp lo kr mb mc lq kv md me ls kz mf mg lu mh bi translated">资料组</h2><p id="4c2d" class="pw-post-body-paragraph ki kj it kk b kl mi ju kn ko mj jx kq kr mk kt ku kv ml kx ky kz mm lb lc ld im bi translated">为了简化演示，我们从最简单的视觉数据集MNIST训练了整个VAE。MNIST包含60000幅训练图像和10000幅测试图像，显示从0到9的手写数字字符。</p><h2 id="b337" class="lw lf it bd lg lx ly dn lk lz ma dp lo kr mb mc lq kv md me ls kz mf mg lu mh bi translated">硬件要求</h2><p id="5294" class="pw-post-body-paragraph ki kj it kk b kl mi ju kn ko mj jx kq kr mk kt ku kv ml kx ky kz mm lb lc ld im bi translated">由于MNIST是一个相当小的数据集，因此可以纯粹在CPU上训练和评估网络。然而，当在其他更大的数据集上使用时，建议使用GPU进行计算。为了确定是否使用GPU进行训练，我们可以首先根据可用性创建一个可变的<em class="nf">设备</em> CPU/GPU:</p><figure class="mo mp mq mr gt ms"><div class="bz fp l di"><div class="nh ni l"/></div></figure><h1 id="0756" class="le lf it bd lg lh li lj lk ll lm ln lo jz lp ka lq kc lr kd ls kf lt kg lu lv bi translated">网络体系结构</h1><figure class="mo mp mq mr gt ms gh gi paragraph-image"><div role="button" tabindex="0" class="mt mu di mv bf mw"><div class="gh gi nj"><img src="../Images/615f3cc2f4f1041832ea81444096f0f4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5c68EczVkeEl1cvV5K_6vA.png"/></div></div><p class="mz na gj gh gi nb nc bd b be z dk translated">图二。VAE的示意图。</p></figure><p id="0ae8" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们的VAE结构如上图所示，它包括一个编码器，一个解码器，在两者之间的潜在表示被重新参数化。</p><p id="8149" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><em class="nf">编码器— </em>编码器由两个卷积层组成，后面是两个独立的全连接层，两个层都将卷积后的特征图作为输入。两个全连接层在我们预期的潜在空间的维度上输出两个向量，其中一个是均值，另一个是方差。这是VAEs和传统自动编码器之间的主要结构差异。</p><p id="ff56" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><em class="nf">重新参数化— </em>通过计算平均值和方差，我们随机抽取一个可能出现在给定分布中的点，该点将被用作潜在表示，并输入解码阶段。</p><p id="8b2f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><em class="nf">解码器</em> —该解码器类似于传统的自动编码器，具有一个全连接层，后跟两个卷积层，以基于给定的潜在表示来重建图像。</p><p id="cf0d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们可以使用PyTorch构建VAE结构的上述组件，如下所示:</p><figure class="mo mp mq mr gt ms"><div class="bz fp l di"><div class="nh ni l"/></div></figure><h1 id="6661" class="le lf it bd lg lh li lj lk ll lm ln lo jz lp ka lq kc lr kd ls kf lt kg lu lv bi translated">培训程序</h1><h2 id="49bb" class="lw lf it bd lg lx ly dn lk lz ma dp lo kr mb mc lq kv md me ls kz mf mg lu mh bi translated"><em class="nk">损失函数</em></h2><p id="54dd" class="pw-post-body-paragraph ki kj it kk b kl mi ju kn ko mj jx kq kr mk kt ku kv ml kx ky kz mm lb lc ld im bi translated">VAE的核心概念之一是其损失函数的设计。简而言之，我们试图设计这样的损失，即它基于给定的图像重建得很好，但也适合整个分布，而不是仅过度适合图像本身。因此，VAE损失是以下因素的组合:</p><p id="fe0f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><em class="nf">二进制交叉熵(BCE)损失— </em>计算重建图像与原始图像的像素间差异，以最大化重建的相似性。BCE损失的计算方法如下:</p><figure class="mo mp mq mr gt ms gh gi paragraph-image"><div class="gh gi nl"><img src="../Images/53eeebabca5cf0af4b19741800286b00.png" data-original-src="https://miro.medium.com/v2/resize:fit:570/format:webp/1*It9gkzCRsF0gY_H-hWK9hQ.png"/></div></figure><p id="bc72" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">其中<em class="nf"> xᵢ </em>和<em class="nf"> x'ᵢ </em>分别表示原始和重建图像像素(总共<em class="nf"> n </em>个像素)。</p><p id="dda3" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><em class="nf">KL-散度损失— </em> KL散度衡量两个分布的相似性。在这种情况下，我们假设分布为正态分布，因此损耗设计如下:</p><figure class="mo mp mq mr gt ms gh gi paragraph-image"><div class="gh gi nm"><img src="../Images/9caf191d24584fc10929ede5c58cc26c.png" data-original-src="https://miro.medium.com/v2/resize:fit:474/format:webp/1*2llhrQuoYv0dF5jc-bzWFw.png"/></div></figure><p id="58f4" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这是通过我们预测的潜在向量(大小为<em class="nf"> m </em>)中每个值的平均值和sigma来计算的。</p><h2 id="a9bf" class="lw lf it bd lg lx ly dn lk lz ma dp lo kr mb mc lq kv md me ls kz mf mg lu mh bi translated">培养</h2><p id="91ff" class="pw-post-body-paragraph ki kj it kk b kl mi ju kn ko mj jx kq kr mk kt ku kv ml kx ky kz mm lb lc ld im bi translated">下面的代码显示了培训过程。我们将批量大小设置为128，学习速率设置为1e-3，总的时期数设置为10。</p><figure class="mo mp mq mr gt ms"><div class="bz fp l di"><div class="nh ni l"/></div></figure><p id="d1fb" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">注意，为了简单起见，我们在这里只进行纯训练。然而，建议在每个时期之后，我们在测试集上计算有效性，以防止在训练期间的任何过度拟合。当验证损失达到最低点时，也应该保存检查点。</p><figure class="mo mp mq mr gt ms gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/0f02ea715e1b98b509b93e41ffc4f8a6.png" data-original-src="https://miro.medium.com/v2/resize:fit:960/format:webp/1*tmT9vhFGJ3RQJZaCY6Pb_g.png"/></div></figure><h1 id="4c6c" class="le lf it bd lg lh li lj lk ll lm ln lo jz lp ka lq kc lr kd ls kf lt kg lu lv bi translated">(英)可视化(= visualization)</h1><p id="29cb" class="pw-post-body-paragraph ki kj it kk b kl mi ju kn ko mj jx kq kr mk kt ku kv ml kx ky kz mm lb lc ld im bi translated">训练之后，我们可以用下面的代码来可视化结果:</p><figure class="mo mp mq mr gt ms"><div class="bz fp l di"><div class="nh ni l"/></div></figure><figure class="mo mp mq mr gt ms gh gi paragraph-image"><div role="button" tabindex="0" class="mt mu di mv bf mw"><div class="gh gi no"><img src="../Images/2609c2211085dbcedd4070185cfdd940.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZGB8xD7a6DMF2lLckV9GLA.png"/></div></div><p class="mz na gj gh gi nb nc bd b be z dk translated">图3。重建结果。左边是原始图像，右边是生成的图像。</p></figure><p id="41cf" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">从可视化中我们可以看到，我们已经成功地在原始图形的基础上生成了略有不同的数字图形，这就是VAE最终要实现的目标！</p><h1 id="87b4" class="le lf it bd lg lh li lj lk ll lm ln lo jz lp ka lq kc lr kd ls kf lt kg lu lv bi translated">结论</h1><p id="636a" class="pw-post-body-paragraph ki kj it kk b kl mi ju kn ko mj jx kq kr mk kt ku kv ml kx ky kz mm lb lc ld im bi translated">所以你有它！希望这篇文章给你一个基本的概述和指导，告诉你如何从头开始构建你的第一个VAE。完整的实现可以在下面的Github资源库中找到:</p><div class="np nq gp gr nr ns"><a href="https://github.com/ttchengab/VAE.git" rel="noopener  ugc nofollow" target="_blank"><div class="nt ab fo"><div class="nu ab nv cl cj nw"><h2 class="bd iu gy z fp nx fr fs ny fu fw is bi translated">ttchengab/VAE</h2><div class="nz l"><h3 class="bd b gy z fp nx fr fs ny fu fw dk translated">此时您不能执行该操作。您已使用另一个标签页或窗口登录。您已在另一个选项卡中注销，或者…</h3></div><div class="oa l"><p class="bd b dl z fp nx fr fs ny fu fw dk translated">github.com</p></div></div><div class="ob l"><div class="oc l od oe of ob og mx ns"/></div></div></a></div><p id="74a0" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><em class="nf">感谢您坚持到现在</em>🙏！<em class="nf">我会在计算机视觉/深度学习的不同领域发布更多内容。一定要看看我的另一篇关于一次性学习的文章！</em></p></div></div>    
</body>
</html>