<html>
<head>
<title>How to Use Singular Value Decomposition (SVD) for Image Classification in Python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Python中如何使用奇异值分解进行图像分类</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/how-to-use-singular-value-decomposition-svd-for-image-classification-in-python-20b1b2ac4990?source=collection_archive---------3-----------------------#2021-10-26">https://towardsdatascience.com/how-to-use-singular-value-decomposition-svd-for-image-classification-in-python-20b1b2ac4990?source=collection_archive---------3-----------------------#2021-10-26</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="ffb3" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">用一个简单的例子揭开SVD背后的线性代数概念</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/8ca7d7fe33c67e5d89269ff6702a08b0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*LiFqFRGAzzH7doBh"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">Marcel strau在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="0349" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">线性代数中最难理解的主题之一是<strong class="lb iu">奇异值分解</strong> (SVD)方法。它也是最基本的技术之一，因为它为理解主成分分析(PCA)、潜在狄利克雷分配(LDA)和一般矩阵分解的概念铺平了道路。</p><p id="a984" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">奇异值分解之所以难以捉摸，是因为虽然这种方法需要大量的数学和线性代数知识才能掌握，但实际应用却常常被忽视。有很多人以为自己掌握了SVD的概念，其实不然。这不仅仅是一种降维技术:本质上，SVD的神奇之处在于<em class="lv">任何矩阵A都可以写成秩1矩阵的和</em>！这一点以后会变得明显。</p><p id="96f4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">本文的目的是通过将SVD应用于一个众所周知的例子:手写数字分类，展示SVD的用途和基本机制。</p><h1 id="489e" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">快速提醒(高级可选)</h1><p id="93c9" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">奇异值分解的基本关系是</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mt"><img src="../Images/1787aa5985756b51751ac3468fda538c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*AlKJC4XX3O5ze6Fe.jpeg"/></div></figure><p id="3698" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">其中:<br/> U和V是正交矩阵，<br/> S是对角矩阵</p><p id="f1d0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">更具体地说:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mu"><img src="../Images/5fbb00d5ba34cd951d434d9ea219821c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*xi1Tp6F9FZXbmJWE.png"/></div></div></figure><p id="2d11" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">其中显示了前述的权利要求，<strong class="lb iu">任何矩阵A都可以写成秩1矩阵的和</strong>。</p><p id="df28" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">奇异值分解的一些有用特性:</strong></p><ol class=""><li id="e5bb" class="mv mw it lb b lc ld lf lg li mx lm my lq mz lu na nb nc nd bi translated">u矩阵和v矩阵分别由<strong class="lb iu"><em class="lv"/></strong><em class="lv">和</em><strong class="lb iu"><em class="lv">aᵀa</em></strong><em class="lv">的特征向量构成。</em></li><li id="a6aa" class="mv mw it lb b lc ne lf nf li ng lm nh lq ni lu na nb nc nd bi translated">任何矩阵都有SVD分解。这是因为<strong class="lb iu"> <em class="lv"> AAᵀ </em> </strong>和<strong class="lb iu"> <em class="lv"> AᵀA </em> </strong>矩阵<strong class="lb iu"> <em class="lv"> </em> </strong>有一个特殊的性质(以及其他性质):它们至少是半正定的(这意味着它们的特征值要么是正的，要么是零)。</li><li id="42b3" class="mv mw it lb b lc ne lf nf li ng lm nh lq ni lu na nb nc nd bi translated">S矩阵包含正特征值的平方根。这些也被称为<strong class="lb iu">奇异值。</strong></li><li id="b4bf" class="mv mw it lb b lc ne lf nf li ng lm nh lq ni lu na nb nc nd bi translated">在包括Python在内的大多数编程语言中，U和V的列的排列方式是特征值较高的列在特征值较小的列之前。</li><li id="9bdf" class="mv mw it lb b lc ne lf nf li ng lm nh lq ni lu na nb nc nd bi translated"><em class="lv"> u，u </em> …向量也被称为左奇异向量，它们形成了一个<strong class="lb iu">正交基</strong>。相应的，<em class="lv"> v，v </em> …向量称为右奇异向量。</li><li id="250d" class="mv mw it lb b lc ne lf nf li ng lm nh lq ni lu na nb nc nd bi translated">矩阵A的秩是S矩阵的非零奇异值的个数。</li><li id="c27e" class="mv mw it lb b lc ne lf nf li ng lm nh lq ni lu na nb nc nd bi translated"><a class="ae ky" href="https://en.wikipedia.org/wiki/Low-rank_approximation" rel="noopener ugc nofollow" target="_blank"> <strong class="lb iu">埃克哈特-杨-米尔斯基定理</strong> </a> <strong class="lb iu"> : </strong>一个秩的最佳<em class="lv"> k </em>秩逼近<em class="lv"> k &lt; r </em>一个矩阵在2-范数和F-范数下是:</li></ol><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nj"><img src="../Images/d6c31a896277acf48d0614ef9c79b53a.png" data-original-src="https://miro.medium.com/v2/resize:fit:402/format:webp/0*AExZR6rx0MV5xhqO.png"/></div></figure><p id="660e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">换句话说:</p><blockquote class="nk nl nm"><p id="b901" class="kz la lv lb b lc ld ju le lf lg jx lh nn lj lk ll no ln lo lp np lr ls lt lu im bi translated">如果你想用一个较低秩的k来逼近任何矩阵A，最佳的方法是对A应用SVD，只取前k个具有最高k个奇异值的基向量。</p></blockquote><h1 id="69fa" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">Python中的SVD</h1><p id="c750" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">对于这个例子，我们将使用<a class="ae ky" href="https://www.kaggle.com/bistaumanga/usps-dataset?select=usps.h5" rel="noopener ugc nofollow" target="_blank">手写数字USPS(美国邮政服务)数据集</a>。该数据集包含7291个训练和2007个[0–9]之间的手写数字测试图像。图像是16*16灰度像素。首先，我们加载数据:</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="6b0c" class="nv lx it nr b gy nw nx l ny nz">import numpy as np<br/>import pandas as pd<br/>import matplotlib.pyplot as plt<br/>from scipy.linalg import svd, norm<br/>from sklearn.metrics import accuracy_score, confusion_matrix, classification_report<br/>import h5py<br/>import os</span><span id="ae22" class="nv lx it nr b gy oa nx l ny nz"># define class labels<br/>labels = {<br/>    0: "0", <br/>    1: "1", <br/>    2: "2", <br/>    3: "3", <br/>    4: "4", <br/>    5: "5", <br/>    6: "6", <br/>    7: "7", <br/>    8: "8",<br/>    9: "9"<br/>}</span><span id="b533" class="nv lx it nr b gy oa nx l ny nz"># load the dataset<br/>with h5py.File(os.path.join(os.getcwd(), 'usps.h5'), 'r') as hf:<br/>        train = hf.get('train')<br/>        test = hf.get('test')</span><span id="d3c9" class="nv lx it nr b gy oa nx l ny nz">        x_train = pd.DataFrame(train.get('data')[:]).T<br/>        y_train = pd.DataFrame(train.get('target')[:]).T<br/>        x_test = pd.DataFrame(test.get('data')[:]).T<br/>        y_test = pd.DataFrame(test.get('target')[:]).T</span><span id="ef27" class="nv lx it nr b gy oa nx l ny nz">print(x_train.shape)<br/>print(y_train.shape)<br/>print(x_test.shape)<br/>print(y_test.shape)</span><span id="a834" class="nv lx it nr b gy oa nx l ny nz">#Output:<br/>#(256, 7291)<br/>#(1, 7291)<br/>#(256, 2007)<br/>#(1, 2007)</span></pre><p id="7fea" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">数据的加载方式与上面快速提醒部分的尺寸相匹配。<em class="lv"> x_train </em>和<em class="lv"> x_test </em>的列包含作为向量的数字，这些数字被展平成大小等于256的数组(因为每个数字的大小都是16x16)。另一方面，<em class="lv"> y_train </em>和<em class="lv"> y_test </em>是行向量，分别包含训练和测试数据集的每个数字(0到9之间的值)的实际类。</p><p id="c528" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">图1 </strong>显示了训练数据集中的第一幅图像:</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="8276" class="nv lx it nr b gy nw nx l ny nz">digit_image=x_train[0]<br/>plt.imshow(digit_image.to_numpy().reshape(16,16),cmap='binary')</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ob"><img src="../Images/7f6ab0cda78cc7f58f2a8b7ae2ee8865.png" data-original-src="https://miro.medium.com/v2/resize:fit:510/format:webp/1*DsQOd1a4ugBrWjXtE_1zPA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图1:来自训练数据集的图像</p></figure><p id="28f7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">数字分类的方法由以下步骤组成:</p><ol class=""><li id="6984" class="mv mw it lb b lc ld lf lg li mx lm my lq mz lu na nb nc nd bi translated">我们将<em class="lv"> x_train </em>数据帧分成10个矩阵(按列)，每个矩阵对应一个数字[0–9]。这些是之前提到的<em class="lv"> A的</em>矩阵。目标是将SVD分别应用于它们中的每一个。例如，A0矩阵仅包含数字0的图像，其形状为(256，1194)，因为数据集中有1194个0。</li><li id="f5dd" class="mv mw it lb b lc ne lf nf li ng lm nh lq ni lu na nb nc nd bi translated">接下来，我们将SVD应用于[A0，A1..A9]矩阵。对于每个A矩阵，我们存储相应的U、S和V矩阵。我们将主要使用U矩阵。</li><li id="2ca9" class="mv mw it lb b lc ne lf nf li ng lm nh lq ni lu na nb nc nd bi translated">用一个数字表示的每个数据矩阵A都有一个“独特的特征”。这种区别反映在前几个左奇异向量<em class="lv"> (u，u …)</em>。由于这些特征向量本质上是基向量，如果一个未知数字可以用另一个数字(例如数字3)的基更好地近似，那么我们可以假设该未知数字被分类为那个数字(如3)。这将在后面的编程示例中变得更加明显。</li></ol><p id="18de" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">第一步</strong></p><p id="5cdb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这相当容易。我们只是创建了[A0，A1..A9]矩阵，并将它们存储在名为alpha _ matrices的字典中:</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="8960" class="nv lx it nr b gy nw nx l ny nz">alpha_matrices={}</span><span id="6e02" class="nv lx it nr b gy oa nx l ny nz">for i in range(10):<br/>    alpha_matrices.update({"A"+str(i):x_train.loc[:,list(y_train.loc[0,:]==i)]})</span><span id="f88f" class="nv lx it nr b gy oa nx l ny nz">print(alpha_matrices['A0'].shape)<br/>#(256, 1194)</span></pre><p id="0060" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">第二步</strong></p><p id="841b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这一步也很简单。我们将U、S和V矩阵分别存储在<em class="lv">左奇异</em>、<em class="lv">奇异_矩阵</em>和<em class="lv">右奇异</em>字典中:</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="3315" class="nv lx it nr b gy nw nx l ny nz">left_singular={}<br/>singular_matix={}<br/>right_singular={}</span><span id="6490" class="nv lx it nr b gy oa nx l ny nz">for i in range(10):<br/>    u, s, v_t = svd(alpha_matrices['A'+str(i)], full_matrices=False)<br/>    left_singular.update({"u"+str(i):u})<br/>    singular_matix.update({"s"+str(i):s})<br/>    right_singular.update({"v_t"+str(i):v_t})</span><span id="edbd" class="nv lx it nr b gy oa nx l ny nz">print(left_singular['u0'].shape)<br/>#(256, 256)</span></pre><p id="637f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们展示一下这些矩阵中包含了哪些信息。我们将使用数字3的U、S和V矩阵作为示例，在我们的示例中，它们对应于以下变量:</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="73e1" class="nv lx it nr b gy nw nx l ny nz">#left_singular[‘u3’]<br/>#right_singular[‘s3]<br/>#singular_matix[‘v_t3]</span><span id="07d8" class="nv lx it nr b gy oa nx l ny nz">plt.figure(figsize=(20,10))<br/>columns = 5<br/>for i in range(10):<br/>   plt.subplot(10/ columns + 1, columns, i + 1)<br/>   plt.imshow(left_singular["u3"][:,i].reshape(16,16),cmap='binary')</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oc"><img src="../Images/b4aa03715a3ac5d3c93f23b784877834.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HJ_gp3EQVYICyfNmEGzQfA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图2:U3矩阵的前10个奇异向量</p></figure><p id="db4f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">图2 </strong>显示了前10个左奇异向量[u，u … u ⁰](共256个)。它们都描绘了数字3，其中第一个(u1向量)最为清晰。</p><p id="5595" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">图3 </strong>以对数标度显示了S矩阵中数字3的奇异值:</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="9fdd" class="nv lx it nr b gy nw nx l ny nz">plt.figure(figsize = (9, 6))<br/>plt.plot(singular_matix[‘s3’], color=’coral’, marker=’o’)<br/>plt.title(‘Singular values for digit $3$’,fontsize=15,weight=”bold”,pad=20)<br/>plt.ylabel(‘Singular values’ ,fontsize=15)<br/>plt.yscale(“log”)<br/>plt.show()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi od"><img src="../Images/cb2881f5972f7ecb5b64eb8afc391c09.png" data-original-src="https://miro.medium.com/v2/resize:fit:1134/format:webp/1*xvrdJ7CRl5CyYxJKOcPbng.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图3:数字3的所有奇异值</p></figure><p id="fd52" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">给定奇异值排序，前几个是最高的(就值而言)，并遵循“陡峭的曲线模式”。</p><p id="d3b8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">通过考虑<strong class="lb iu">图2 </strong>和<strong class="lb iu">图3 </strong>，我们可以图形化地确认3位SVD的矩阵逼近性质(记住<strong class="lb iu"> Eckart-Young-Mirsky定理</strong>):第一个左边的奇异向量代表矩阵A3的固有属性值。事实上，在图2 的<strong class="lb iu">中，第一个奇异向量<em class="lv"> u1 </em>看起来像数字3，随后的左奇异向量代表u1周围的训练集的最重要的变化。</strong></p><p id="1a00" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">问题是我们是否可以只使用第一个<em class="lv"> k个</em>奇异向量，并且仍然有一个很好的近似基。我们可以通过实验来验证。</p><p id="8399" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">第三步</strong></p><p id="a18e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">给定一个由(1256)个向量表示的未知数字称为<em class="lv"> z </em>，给定左奇异向量的集合<em class="lv"> u1，u2… uk </em>，其中每个集合表示对应的数字矩阵/A矩阵，那么<em class="lv"> z </em>的目标值是多少？注意我们的索引是<em class="lv"> k </em>(第一主奇异特征向量)而不是<em class="lv"> n </em>(全部)。要解决这个问题，我们要做的就是以下几点:</p><blockquote class="oe"><p id="e82c" class="of og it bd oh oi oj ok ol om on lu dk translated">目标是计算测试集中的一个数字在10个不同的标准正交基中的表现程度。</p></blockquote><p id="052c" class="pw-post-body-paragraph kz la it lb b lc oo ju le lf op jx lh li oq lk ll lm or lo lp lq os ls lt lu im bi translated">这可以通过计算以下类型的最小二乘问题中的剩余向量来实现:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ot"><img src="../Images/a136cdf81d1a27e798db47c0c26da260.png" data-original-src="https://miro.medium.com/v2/resize:fit:932/format:webp/0*0wqF-Jr9vOCWFyTp.png"/></div></figure><p id="8da0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">最小二乘问题的解是</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ou"><img src="../Images/8c62e2483358c9afe39b866c8c540a4b.png" data-original-src="https://miro.medium.com/v2/resize:fit:636/format:webp/0*sGchq1SS5gwSatYe.png"/></div></figure><p id="d55b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">记住U矩阵是正交的。剩余范数向量变成:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ov"><img src="../Images/cadafb834332e5e0eab07ff3990b78ee.png" data-original-src="https://miro.medium.com/v2/resize:fit:354/format:webp/0*aCommhpoRWMjhrsT.png"/></div></figure><p id="e6a9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">就是这样！现在我们需要的东西都有了。使用最后一个公式，我们继续计算不同<em class="lv"> k </em>值的测试精度:</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="ffb4" class="nv lx it nr b gy nw nx l ny nz">I = np.eye(x_test.shape[0])<br/>kappas=np.arange(5,21)<br/>len_test=x_test.shape[1]</span><span id="16c5" class="nv lx it nr b gy oa nx l ny nz">predictions=np.empty((y_test.shape[1],0), dtype = int)</span><span id="742d" class="nv lx it nr b gy oa nx l ny nz">for t in list(kappas):<br/>    prediction = []<br/>    for i in range(len_test):<br/>        residuals = []<br/>        for j in range(10):<br/>            u=left_singular["u"+str(j)][:,0:t]<br/>            res=norm( np.dot(I-np.dot(u,u.T), x_test[i]  ))<br/>            residuals.append(res)<br/>        index_min = np.argmin(residuals)<br/>        prediction.append(index_min)<br/>        <br/>    prediction=np.array(prediction)<br/>    predictions=np.hstack((predictions,prediction.reshape(-1,1)))</span><span id="15ef" class="nv lx it nr b gy oa nx l ny nz">scores=[]</span><span id="7d8e" class="nv lx it nr b gy oa nx l ny nz">for i in range(len(kappas)):<br/>    score=accuracy_score(y_test.loc[0,:],predictions[:,i])<br/>    scores.append(score)</span><span id="1acc" class="nv lx it nr b gy oa nx l ny nz">data={"Number of basis vectors":list(thresholds), "accuracy_score":scores}<br/>df=pd.DataFrame(data).set_index("Number of basis vectors")</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ow"><img src="../Images/88c3acf71dc4a392694003f501e17f5a.png" data-original-src="https://miro.medium.com/v2/resize:fit:970/format:webp/1*mNofdazVQS-v9S6oFyhk2Q.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">表1:不同k值的测试准确度得分</p></figure><p id="bed8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们也可以用图形显示这个结果:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ox"><img src="../Images/4f3dfb284985c3e74bab12a96af31ac1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1128/format:webp/1*Ao9jFguac2CemZGrsdM48Q.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图4:不同k值的测试准确度分数</p></figure><p id="9c99" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">表1 </strong>和<strong class="lb iu">图4 </strong>都显示了不同数量的基向量的准确度分数。使用k=12可以获得最好的分数。</p><p id="4a37" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">接下来，显示混淆矩阵:</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="71c9" class="nv lx it nr b gy nw nx l ny nz">pd.set_option(‘display.max_colwidth’,12)<br/>confusion_matrix_df = pd.DataFrame( confusion_matrix(y_test.loc[0,:],predictions[:,7]) )</span><span id="6969" class="nv lx it nr b gy oa nx l ny nz">confusion_matrix_df = confusion_matrix_df.rename(columns = labels, index = labels)<br/>confusion_matrix_df</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oy"><img src="../Images/b5ebb3c07ae6b23991aab63c514e36a6.png" data-original-src="https://miro.medium.com/v2/resize:fit:734/format:webp/1*bUXgh4WhqyBQYdDrhlQsiQ.png"/></div></figure><p id="9569" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">f1分数(宏观平均值和每节课的分数) :</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="5487" class="nv lx it nr b gy nw nx l ny nz">print(classification_report(y_test.loc[0,:],predictions[:,7]))</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oz"><img src="../Images/f86a47455423b4310125cf9a735f0cbd.png" data-original-src="https://miro.medium.com/v2/resize:fit:998/format:webp/1*FS_JPPowCidB1Vi0azdqPQ.png"/></div></figure><p id="9fcb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">点评:<br/> </strong> *数字0、1、6、7在f1-score方面表现最好。<br/> *数字5和3在f1得分方面表现最差。</p><p id="1e85" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们来看一些分类错误的图片的例子:</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="9d04" class="nv lx it nr b gy nw nx l ny nz">misclassified = np.where(y_test.loc[0,:] != predictions[:,7])</span><span id="f7a0" class="nv lx it nr b gy oa nx l ny nz">plt.figure(figsize=(20,10))<br/>columns = 5<br/>for i in range(2,12):<br/>    misclassified_id=misclassified[0][i]<br/>    image=x_test[misclassified_id]<br/>    <br/>    plt.subplot(10/ columns + 1, columns, i-1)<br/>    plt.imshow(image.to_numpy().reshape(16,16),cmap='binary')<br/>    plt.title("True label:"+str(y_test.loc[0,misclassified_id]) + '\n'+ "Predicted label:"+str(predictions[misclassified_id,12]))</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oc"><img src="../Images/e70b09cfe4ffde1b438c4236bac22345.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vQ0EWvBs13z7H5s6CJGAFg.png"/></div></div></figure><p id="7e01" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">显然，其中一些数字写得非常糟糕。</p><p id="02f1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">结论</strong></p><p id="de8d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">实际上，A数据矩阵本质上是一个低秩矩阵加噪声:A = A' + N。通过应用<strong class="lb iu"> Eckart-Young-Mirsk </strong>定理，我们用一个正确秩为k的矩阵来逼近数据矩阵A。这具有保持矩阵固有属性完整的效果，同时去除了额外的噪声。但是我们如何找到正确的秩k呢？</p><p id="1d19" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在我们的案例中，我们通过对第一个主要奇异值进行实验，根据测试精度经验地找到了k。然而，应该注意的是，当然还有其他算法优于这种技术。然而，本案例研究的重点是通过调整一种众所周知的矩阵分解技术来展示手写数字分类的另一种方法。有关这个例子的理论技术和原理的更多信息，请查阅本<a class="ae ky" href="https://epdf.pub/matrix-methods-in-data-mining-and-pattern-recognition32204.html" rel="noopener ugc nofollow" target="_blank">书</a>。</p></div></div>    
</body>
</html>