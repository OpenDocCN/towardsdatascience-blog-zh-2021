<html>
<head>
<title>Transformer Networks: A mathematical explanation why scaling the dot products leads to more stable gradients</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">变压器网络:为什么缩放点积会导致更稳定的梯度的数学解释</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/transformer-networks-a-mathematical-explanation-why-scaling-the-dot-products-leads-to-more-stable-414f87391500?source=collection_archive---------9-----------------------#2021-04-28">https://towardsdatascience.com/transformer-networks-a-mathematical-explanation-why-scaling-the-dot-products-leads-to-more-stable-414f87391500?source=collection_archive---------9-----------------------#2021-04-28</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="b398" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">一个小细节如何能产生巨大的影响</h2></div><p id="f6b8" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在变压器网络中使用的自注意机制的主要目的是生成考虑了周围单词的上下文的单词嵌入。自我注意机制通过将句子中的每个单词与句子中的每个其他单词进行比较，并随后将上下文相关的单词组合在一起来完成这项任务。</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi lb"><img src="../Images/9abd3718d02a2d875c36465dfff0ea9e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*U6PBYT_hymOIfxuOta4Jjw.png"/></div></div><p class="ln lo gj gh gi lp lq bd b be z dk translated">计算第一个单词的自我关注分数(图片由作者提供)</p></figure><p id="5aec" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">自我注意机制首先为句子中的每个单词计算三个向量(查询、键和值)。为了找到所选单词的上下文相关单词，我们将查询向量与句子中其他单词的关键向量进行点积，见上图。点积产生负无穷大和正无穷大之间的任何值，因此应用softmax将这些值映射到[0，1]，并确保它们在整个序列中的总和为1。如此获得的自我注意分数对于与所选单词无关的单词来说是很小的。</p><p id="07be" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><em class="lr">上图未示:自我关注分数随后用于构建所谓的价值向量的</em> <a class="ae ls" href="https://en.wikipedia.org/wiki/Convex_combination" rel="noopener ugc nofollow" target="_blank"> <em class="lr">凸组合</em> </a> <em class="lr">。关于变压器网络如何工作的更多细节，见这篇</em> <a class="ae ls" rel="noopener" target="_blank" href="/drawing-the-transformer-network-from-scratch-part-1-9269ed9a2c5e"> <em class="lr">帖子</em> </a> <em class="lr">。</em></p><p id="8581" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">但为什么点积在馈入softmax函数之前要用√64进行缩放？在大多数关于变形金刚的教程中，我们听到一些关于点积在数量级上变大的事情，从而将softmax函数推到梯度极小的区域。</p><p id="2881" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在这篇文章中，我们想从数学上理解为什么这句话成立。为此，我们将首先探讨softmax函数对于大幅度输入的行为。然后，我们将分析大幅度对softmax函数导数的影响。</p><h1 id="0e5a" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated">Softmax函数</h1><p id="9560" class="pw-post-body-paragraph kf kg iq kh b ki ml jr kk kl mm ju kn ko mn kq kr ks mo ku kv kw mp ky kz la ij bi translated">在变压器网络中，softmax函数的主要目的是获取一系列任意实数(正数和负数),并将它们转换为正数，其总和为1:</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi mq"><img src="../Images/efbb750cabb715fefcd1dbc9d2143dd6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gctBX5YHUUpBEK3MWD6r3Q.png"/></div></div><p class="ln lo gj gh gi lp lq bd b be z dk translated">(图片由作者提供)</p></figure><p id="0daa" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">上面公式中的指数函数确保获得的值是非负的。由于分母中的归一化项，获得的值总和为1。</p><p id="73bc" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">但是，softmax函数不是比例不变的，如以下动画所示:</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div class="gh gi mr"><img src="../Images/4ad507f98bc19bbb70a18c15a6cbc219.png" data-original-src="https://miro.medium.com/v2/resize:fit:962/1*Ml0ajRhvXN3oUfRIBitvEg.gif"/></div><p class="ln lo gj gh gi lp lq bd b be z dk translated">(图片由作者提供)</p></figure><p id="8ebd" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们对输入的比例越大，最大的输入就越能控制输出。随着比例的增加，softmax函数会将接近1的值分配给最大输入值，将0分配给所有其他值。这是由指数函数的性质造成的，指数函数增长越快，其输入越大。另一方面，如果我们降低输入的比例，softmax输出会变得非常相似。</p><h1 id="b3c9" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated">雅可比矩阵</h1><p id="7f78" class="pw-post-body-paragraph kf kg iq kh b ki ml jr kk kl mm ju kn ko mn kq kr ks mo ku kv kw mp ky kz la ij bi translated">在我们继续之前，我们必须澄清一件事:softmax在形式上是一个所谓的<em class="lr">向量函数</em>，它以一个向量作为输入，并产生一个向量作为输出:</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div class="gh gi ms"><img src="../Images/76116ede998281dd8211665a53b5998a.png" data-original-src="https://miro.medium.com/v2/resize:fit:348/format:webp/1*84z9Btynqk7bVg_sDsDcxA.png"/></div></figure><p id="3898" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">所以，我们在讲softmax函数的导数时，实际上讲的是它的雅可比矩阵(而不是梯度)，它是所有一阶偏导数的矩阵:</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div class="gh gi mt"><img src="../Images/cc427b05ec11cc077f9f93e7a022b730.png" data-original-src="https://miro.medium.com/v2/resize:fit:594/format:webp/1*_6wVUPr4_8NTEsh-34Dz4Q.png"/></div></figure><p id="d8d1" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在哪里</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div class="gh gi mu"><img src="../Images/8599ec30ccf976e6746ce63c7637a108.png" data-original-src="https://miro.medium.com/v2/resize:fit:510/format:webp/1*1lTrY5tOSTPw4frG_ioPBg.png"/></div></figure><p id="773e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在<a class="ae ls" rel="noopener" target="_blank" href="/derivative-of-the-softmax-function-and-the-categorical-cross-entropy-loss-ffceefc081d1">之前的帖子</a>中，我们已经推导出了雅可比矩阵元素的封闭表达式:</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div class="gh gi mv"><img src="../Images/1a5d57eeef347697c3f619ce9052e3b1.png" data-original-src="https://miro.medium.com/v2/resize:fit:710/format:webp/1*fqqKPeJwxX02e-eZzu0hKw.png"/></div></figure><p id="c65a" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">看，在上面的公式中，𝑠的偏导数是如何用𝑠本身来表示的。为了看到雅可比矩阵的完整结构，让我们把𝑛 = 4写在纸上:</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div class="gh gi mw"><img src="../Images/5e19c0f32249c9610328fa543cff7176.png" data-original-src="https://miro.medium.com/v2/resize:fit:1118/format:webp/1*9luFbVpx7h0_U5ngQbwqhw.png"/></div></figure><p id="2e64" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们看到对角线元素不同于非对角线元素。此外，我们看到softmax的雅可比矩阵是对称的。</p><p id="9c3e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">接下来让我们寻找一种情况，其中雅可比矩阵的所有元素都变为零。很容易看出，当任何一个𝑠取值为0或1时，对角线元素变为零。当非对角线元素的一个或两个因子为零时，非对角线元素变为零。因此，雅可比矩阵在以下四种情况下成为零矩阵:</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div class="gh gi mx"><img src="../Images/ca77ca9f4f7bcc193047d2500aab31b7.png" data-original-src="https://miro.medium.com/v2/resize:fit:904/format:webp/1*YFLsJarYpQBU0uv39bEN1Q.png"/></div></figure><p id="b6c4" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们在上一节中已经看到，对于较大的输入，softmax函数产生的输出与上述输出非常相似。</p><h1 id="ead3" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated">通过Softmax层反向传播</h1><p id="0b54" class="pw-post-body-paragraph kf kg iq kh b ki ml jr kk kl mm ju kn ko mn kq kr ks mo ku kv kw mp ky kz la ij bi translated">这个难题的最后一部分是展示，为什么softmax的大输入值会导致梯度在反向传播过程中消失。</p><p id="b779" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">假设使用反向传播，我们已经计算了softmax函数输出端的梯度，如下图所示:</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div class="gh gi my"><img src="../Images/b7d378daf74b12708a8f7bfc44801cb5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1158/format:webp/1*7YQxjzGfeHGaWSPATDk2vA.png"/></div><p class="ln lo gj gh gi lp lq bd b be z dk translated">(图片由作者提供)</p></figure><p id="4470" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">接下来，我们希望通过softmax函数反向传播，并获得输入端的梯度。请记住，softmax函数的每个输出都依赖于它的所有输入。使用我们为𝑗-th输入获得的链式法则:</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div class="gh gi mz"><img src="../Images/97cdc7048c4e85af4bf072037f40871e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1082/format:webp/1*TEIU8YEs9OEMgpVbOpyo4A.png"/></div></figure><p id="01e2" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">右手边的行向量可以被识别为雅可比矩阵的𝑗-th列。因此，通过softmax层反向传播相当于乘以其雅可比矩阵:</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div class="gh gi na"><img src="../Images/60dbe9ef5eb902ecf27b8497f27ffdac.png" data-original-src="https://miro.medium.com/v2/resize:fit:622/format:webp/1*ObH4PGmHKdILNNnsl-iTYQ.png"/></div></figure><p id="0976" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们已经看到，当输入大幅增加时，softmax函数的雅可比矩阵收敛为零矩阵。在这种情况下，梯度流(误差传播)因此被softmax层扼杀，并且在softmax层之前的所有元素减慢学习或者甚至完全停止学习。</p><p id="8a3d" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在变压器网络中，softmax函数的输入由关键向量和查询向量之间的点积组成。关键向量和查询向量的维度𝑑越大，点积就越大。在原始论文中，关键向量的维数是64。作者在原始论文中应用的补救措施是用查询和键的维度的平方根来划分点积。这样，不管键和查询向量的维数是多少，学习都会进行得很好。</p><h1 id="f1f1" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated">参考</h1><p id="4cf3" class="pw-post-body-paragraph kf kg iq kh b ki ml jr kk kl mm ju kn ko mn kq kr ks mo ku kv kw mp ky kz la ij bi translated"><a class="ae ls" href="https://arxiv.org/pdf/1706.03762.pdf" rel="noopener ugc nofollow" target="_blank">注意你所需要的一切</a> <br/> <a class="ae ls" rel="noopener" target="_blank" href="/drawing-the-transformer-network-from-scratch-part-1-9269ed9a2c5e">从零开始绘制变压器网络</a> <br/> <a class="ae ls" href="http://peterbloem.nl/blog/transformers" rel="noopener ugc nofollow" target="_blank">从零开始绘制变压器</a><br/><a class="ae ls" rel="noopener" target="_blank" href="/derivative-of-the-softmax-function-and-the-categorical-cross-entropy-loss-ffceefc081d1">soft max的雅可比矩阵</a></p></div></div>    
</body>
</html>