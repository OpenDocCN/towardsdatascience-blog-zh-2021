<html>
<head>
<title>Creating and Training Custom Layers in TensorFlow 2</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">在TensorFlow 2中创建和训练自定义图层</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/creating-and-training-custom-layers-in-tensorflow-2-6382292f48c2?source=collection_archive---------18-----------------------#2021-06-24">https://towardsdatascience.com/creating-and-training-custom-layers-in-tensorflow-2-6382292f48c2?source=collection_archive---------18-----------------------#2021-06-24</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="e710" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">学习创建自己的自定义图层，并在TensorFlow 2中对其进行培训</h2></div><ol class=""><li id="3898" class="ki kj it kk b kl km kn ko kp kq kr ks kt ku kv kw kx ky kz bi translated">之前我们已经看到了如何创建自定义损失函数— <a class="ae la" rel="noopener" target="_blank" href="/creating-custom-loss-functions-using-tensorflow-2-96c123d5ce6c">使用TensorFlow 2 </a>创建自定义损失函数</li><li id="3e30" class="ki kj it kk b kl lb kn lc kp ld kr le kt lf kv kw kx ky kz bi translated">接下来，我写了使用Lambda层创建自定义激活函数— <a class="ae la" rel="noopener" target="_blank" href="/creating-custom-activation-functions-with-lambda-layers-in-tensorflow-691398b8a52d">在TensorFlow 2中使用Lambda层创建自定义激活函数</a></li></ol><p id="2e15" class="pw-post-body-paragraph lg lh it kk b kl km ju li kn ko jx lj kp lk ll lm kr ln lo lp kt lq lr ls kv im bi translated">这是该系列的第三部分，我们创建自定义的密集层，并在TensorFlow 2中训练它们。</p></div><div class="ab cl lt lu hx lv" role="separator"><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly"/></div><div class="im in io ip iq"><h2 id="7b80" class="ma mb it bd mc md me dn mf mg mh dp mi kp mj mk ml kr mm mn mo kt mp mq mr ms bi translated"><strong class="ak">简介:</strong></h2><p id="3ade" class="pw-post-body-paragraph lg lh it kk b kl mt ju li kn mu jx lj kp mv ll lm kr mw lo lp kt mx lr ls kv im bi translated">Lambda层是TensorFlow中的简单层，可用于创建一些自定义激活函数。但是lambda层有许多限制，尤其是在训练这些层的时候。因此，我们的想法是使用TensorFlow中的可继承Keras层来创建可训练的自定义层，特别关注密集层。</p><p id="e9e2" class="pw-post-body-paragraph lg lh it kk b kl km ju li kn ko jx lj kp lk ll lm kr ln lo lp kt lq lr ls kv im bi translated"><strong class="kk iu">什么是层？</strong></p><figure class="mz na nb nc gt nd gh gi paragraph-image"><div role="button" tabindex="0" class="ne nf di ng bf nh"><div class="gh gi my"><img src="../Images/90dc9217cf484a3931ba0cb09e27d658.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*16IK1GQUEwcmpLQYISmZSA.png"/></div></div><p class="nk nl gj gh gi nm nn bd b be z dk translated">图一。图层-密集图层表示(来源:作者创建的图像)</p></figure><p id="d34e" class="pw-post-body-paragraph lg lh it kk b kl km ju li kn ko jx lj kp lk ll lm kr ln lo lp kt lq lr ls kv im bi translated">层是一个类，它接收一些参数，通过状态和计算传递这些参数，并根据神经网络的要求传递输出。每个模型架构都包含多个层，无论是顺序的还是功能的API。</p><p id="d234" class="pw-post-body-paragraph lg lh it kk b kl km ju li kn ko jx lj kp lk ll lm kr ln lo lp kt lq lr ls kv im bi translated"><strong class="kk iu"> <em class="no">状态</em></strong>——主要是在“model.fit”期间训练的可训练特征。在密集层中，状态构成权重和偏差，如图1所示。这些值会随着模型的训练而更新，以提供更好的结果。在某些层中，状态还可以包含不可训练的特征。</p><p id="1534" class="pw-post-body-paragraph lg lh it kk b kl km ju li kn ko jx lj kp lk ll lm kr ln lo lp kt lq lr ls kv im bi translated"><strong class="kk iu"> <em class="no">计算</em> </strong> —计算帮助将一批输入数据转换成一批输出数据。在该层的这一部分中，进行计算。在密集层中，计算执行以下计算—</p><p id="affa" class="pw-post-body-paragraph lg lh it kk b kl km ju li kn ko jx lj kp lk ll lm kr ln lo lp kt lq lr ls kv im bi translated"><em class="no"> Y = (w*X+c) </em>，返回Y。</p><p id="dd43" class="pw-post-body-paragraph lg lh it kk b kl km ju li kn ko jx lj kp lk ll lm kr ln lo lp kt lq lr ls kv im bi translated">y是输出，X是输入，w =权重，c =偏差。</p><h2 id="9125" class="ma mb it bd mc md me dn mf mg mh dp mi kp mj mk ml kr mm mn mo kt mp mq mr ms bi translated"><strong class="ak">创建自定义密集层:</strong></h2><p id="9371" class="pw-post-body-paragraph lg lh it kk b kl mt ju li kn mu jx lj kp mv ll lm kr mw lo lp kt mx lr ls kv im bi translated">现在我们知道了密集层内部发生了什么，让我们看看如何创建我们自己的密集层并在模型中使用它。</p><pre class="mz na nb nc gt np nq nr ns aw nt bi"><span id="0d7b" class="ma mb it nq b gy nu nv l nw nx"><strong class="nq iu"><br/>import tensorflow as tf<br/>from tensorflow.keras.layers import Layer<br/><br/>class SimpleDense(Layer):<br/><br/>    def __init__(self, units=32):</strong></span><span id="181d" class="ma mb it nq b gy ny nv l nw nx"><strong class="nq iu">      </strong>  <em class="no">'''Initializes the instance attributes'''</em></span><span id="5d1a" class="ma mb it nq b gy ny nv l nw nx"><strong class="nq iu">        super(SimpleDense, self).__init__()<br/>        self.units = units<br/><br/>    def build(self, input_shape):</strong></span><span id="71ae" class="ma mb it nq b gy ny nv l nw nx"><strong class="nq iu">        </strong><em class="no">'''Create the state of the layer (weights)'''</em></span><span id="3acb" class="ma mb it nq b gy ny nv l nw nx"><strong class="nq iu">       </strong> <em class="no"># initialize the weights</em><strong class="nq iu"><br/>        w_init = tf.random_normal_initializer()<br/>        self.w = tf.Variable(name="kernel",   initial_value=w_init(shape=(input_shape[-1], self.units),<br/>                 dtype='float32'),trainable=True)<br/><br/>        </strong><em class="no"># initialize the biases</em><strong class="nq iu"><br/>        b_init = tf.zeros_initializer()<br/>        self.b = tf.Variable(name="bias",initial_value=b_init(shape=(self.units,), dtype='float32'),trainable=True)<br/><br/>    def call(self, inputs):</strong></span><span id="5a22" class="ma mb it nq b gy ny nv l nw nx"><strong class="nq iu">        </strong><em class="no">'''Defines the computation from inputs to outputs'''</em></span><span id="dc21" class="ma mb it nq b gy ny nv l nw nx"><strong class="nq iu">        return tf.matmul(inputs, self.w) + self.b</strong></span></pre><p id="11ee" class="pw-post-body-paragraph lg lh it kk b kl km ju li kn ko jx lj kp lk ll lm kr ln lo lp kt lq lr ls kv im bi translated"><strong class="kk iu"> <em class="no">上图</em> </strong>代码解释——类命名为SimpleDense。当我们创建一个自定义层时，我们必须继承Keras的层类。这是在“class SimpleDense(Layer)”行中完成的。</p><p id="5740" class="pw-post-body-paragraph lg lh it kk b kl km ju li kn ko jx lj kp lk ll lm kr ln lo lp kt lq lr ls kv im bi translated"><strong class="kk iu"> <em class="no"> '__init__' </em> </strong>是类中第一个帮助初始化类的方法。“init”接受参数并将它们转换成可在类中使用的变量。这是从“层”类继承的，因此需要一些初始化。这种初始化是使用“super”关键字完成的。“units”是一个局部类变量。这类似于致密层中的单元数量。默认值设置为32，但在调用该类时总是可以更改。</p><p id="07e4" class="pw-post-body-paragraph lg lh it kk b kl km ju li kn ko jx lj kp lk ll lm kr ln lo lp kt lq lr ls kv im bi translated"><strong class="kk iu"><em class="no">‘建’</em></strong>是类中的下一种方法。这用于指定状态。在密集层中，对于权重和偏差，所需的两个状态是“w”和“b”。当创建密集层时，我们不只是创建网络隐藏层的一个神经元，而是一次创建多个神经元(在这种情况下将创建32个神经元)。该层中的每个神经元都需要初始化，并被赋予一些随机权重和偏差值。TensorFlow包含许多内置函数来初始化这些值。</p><p id="9540" class="pw-post-body-paragraph lg lh it kk b kl km ju li kn ko jx lj kp lk ll lm kr ln lo lp kt lq lr ls kv im bi translated">为了初始化权重，我们使用TensorFlow的“random_normal_initializer”函数，它将使用正态分布随机初始化权重。“self.w”包含张量变量形式的权重状态。这些状态将使用“w_init”进行初始化。作为权重包含的值将采用“float_32”格式。它被设置为“可训练”，这意味着每次运行后，这些初始权重将根据损失函数和优化器进行更新。添加了“内核”这个名称，以便以后可以很容易地跟踪它。</p><p id="2088" class="pw-post-body-paragraph lg lh it kk b kl km ju li kn ko jx lj kp lk ll lm kr ln lo lp kt lq lr ls kv im bi translated">为了初始化偏差，使用TensorFlow的“zeros_initializer”函数。这将所有初始偏置值设置为零。self . b’是一个张量，其大小与单元的大小相同(这里是32)，并且这32个偏置项中的每一个最初都被设置为零。这也被设置为“可训练”，因此偏差项将随着训练的开始而更新。添加“偏差”这个名称是为了以后能够跟踪它。</p><p id="019f" class="pw-post-body-paragraph lg lh it kk b kl km ju li kn ko jx lj kp lk ll lm kr ln lo lp kt lq lr ls kv im bi translated"><strong class="kk iu"><em class="no">【call】</em></strong>是执行计算的最后一个方法。在这种情况下，由于它是一个密集图层，它将输入乘以权重，加上偏差，最后返回输出。由于self.w和self.b是张量，而不是单个数值，因此使用“matmul”运算。</p><pre class="mz na nb nc gt np nq nr ns aw nt bi"><span id="91d6" class="ma mb it nq b gy nu nv l nw nx"><em class="no"># declare an instance of the class</em> </span><span id="45c8" class="ma mb it nq b gy ny nv l nw nx"><strong class="nq iu">my_dense = SimpleDense(units=1)  </strong></span><span id="f9b7" class="ma mb it nq b gy ny nv l nw nx"><em class="no"># define an input and feed into the layer</em> </span><span id="8f94" class="ma mb it nq b gy ny nv l nw nx"><strong class="nq iu">x = tf.ones((1, 1)) <br/>y = my_dense(x)  </strong></span><span id="e038" class="ma mb it nq b gy ny nv l nw nx"><em class="no"># parameters of the base Layer class like `variables` can be used</em> </span><span id="75f4" class="ma mb it nq b gy ny nv l nw nx"><strong class="nq iu">print(my_dense.variables)</strong></span></pre><p id="6f34" class="pw-post-body-paragraph lg lh it kk b kl km ju li kn ko jx lj kp lk ll lm kr ln lo lp kt lq lr ls kv im bi translated"><strong class="kk iu"> <em class="no">输出:</em> </strong></p><pre class="mz na nb nc gt np nq nr ns aw nt bi"><span id="bb9b" class="ma mb it nq b gy nu nv l nw nx">[&lt;tf.Variable 'simple_dense/kernel:0' shape=(1, 1) dtype=float32, numpy=array([[0.00382898]], dtype=float32)&gt;, </span><span id="568a" class="ma mb it nq b gy ny nv l nw nx">&lt;tf.Variable 'simple_dense/bias:0' shape=(1,) dtype=float32, numpy=array([0.], dtype=float32)&gt;]</span></pre><p id="55e5" class="pw-post-body-paragraph lg lh it kk b kl km ju li kn ko jx lj kp lk ll lm kr ln lo lp kt lq lr ls kv im bi translated"><strong class="kk iu"> <em class="no">对上面</em> </strong>代码的解释——第一行创建了一个仅包含一个神经元(unit =1)的密集层。x(输入)是值为1的形状(1，1)的张量。Y = my_dense(x)，帮助初始化密集层。。“变量”帮助我们查看密集层内部初始化的值(权重和偏差)。</p><p id="51f5" class="pw-post-body-paragraph lg lh it kk b kl km ju li kn ko jx lj kp lk ll lm kr ln lo lp kt lq lr ls kv im bi translated">“my_dense.variable”的输出显示在代码块下方。它表明‘simple _ dense’中有两个变量叫做‘kernel’和‘bias’。内核‘w’被初始化为值0.0038，一个随机正态分布值，偏差‘b’被初始化为值0。这只是层的初始状态。一旦被训练，这些值将相应地改变。</p><pre class="mz na nb nc gt np nq nr ns aw nt bi"><span id="4d48" class="ma mb it nq b gy nu nv l nw nx"><strong class="nq iu">import numpy as np</strong></span><span id="76d1" class="ma mb it nq b gy ny nv l nw nx"><em class="no"># define the dataset</em> </span><span id="6dd1" class="ma mb it nq b gy ny nv l nw nx"><strong class="nq iu">xs = np.array([-1.0,  0.0, 1.0, 2.0, 3.0, 4.0], dtype=float) <br/>ys = np.array([-3.0, -1.0, 1.0, 3.0, 5.0, 7.0], dtype=float)  </strong> </span><span id="8889" class="ma mb it nq b gy ny nv l nw nx"><em class="no"># use the Sequential API to build a model with our custom layer</em> </span><span id="dc02" class="ma mb it nq b gy ny nv l nw nx"><strong class="nq iu">my_layer = SimpleDense(units=1) <br/>model = tf.keras.Sequential([my_layer])  </strong></span><span id="d22e" class="ma mb it nq b gy ny nv l nw nx"><em class="no"># configure and train the model</em> </span><span id="8f89" class="ma mb it nq b gy ny nv l nw nx"><strong class="nq iu">model.compile(optimizer='sgd', loss='mean_squared_error') model.fit(xs, ys, epochs=500,verbose=0) </strong> </span><span id="d41a" class="ma mb it nq b gy ny nv l nw nx"><em class="no"># perform inference</em> </span><span id="2626" class="ma mb it nq b gy ny nv l nw nx"><strong class="nq iu">print(model.predict([10.0]))  </strong></span><span id="ab3d" class="ma mb it nq b gy ny nv l nw nx"><em class="no"># see the updated state of the variables</em> </span><span id="aaca" class="ma mb it nq b gy ny nv l nw nx"><strong class="nq iu">print(my_layer.variables)</strong></span></pre><p id="d9c2" class="pw-post-body-paragraph lg lh it kk b kl km ju li kn ko jx lj kp lk ll lm kr ln lo lp kt lq lr ls kv im bi translated"><strong class="kk iu"> <em class="no">输出:</em> </strong></p><pre class="mz na nb nc gt np nq nr ns aw nt bi"><span id="9461" class="ma mb it nq b gy nu nv l nw nx">[[18.981567]]</span><span id="d074" class="ma mb it nq b gy ny nv l nw nx">[&lt;tf.Variable 'sequential/simple_dense_1/kernel:0' shape=(1, 1) dtype=float32, numpy=array([[1.9973286]], dtype=float32)&gt;, </span><span id="10c4" class="ma mb it nq b gy ny nv l nw nx">&lt;tf.Variable 'sequential/simple_dense_1/bias:0' shape=(1,) dtype=float32, numpy=array([-0.99171764], dtype=float32)&gt;]</span></pre><p id="713d" class="pw-post-body-paragraph lg lh it kk b kl km ju li kn ko jx lj kp lk ll lm kr ln lo lp kt lq lr ls kv im bi translated"><strong class="kk iu"> <em class="no">上面代码的解释</em></strong>—上面使用的代码是一种非常简单的检查自定义层是否工作的方法。设置输入和输出，使用自定义层编译模型，最后训练500个历元。重要的是要看到，在训练模型之后，权重和偏差的值现在已经改变了。最初设置为0.0038的权重现在是1.9973，而最初设置为零的偏差现在是-0.9917。</p></div><div class="ab cl lt lu hx lv" role="separator"><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly"/></div><div class="im in io ip iq"><h2 id="41e4" class="ma mb it bd mc md me dn mf mg mh dp mi kp mj mk ml kr mm mn mo kt mp mq mr ms bi translated">向自定义密集层添加激活函数:</h2><p id="7fcf" class="pw-post-body-paragraph lg lh it kk b kl mt ju li kn mu jx lj kp mv ll lm kr mw lo lp kt mx lr ls kv im bi translated">以前我们创建了自定义密集层，但我们没有添加任何激活随着这一层。当然，要添加激活，我们可以在模型中单独写一行激活，或者添加一个Lambda层激活。但是，我们如何在上面创建的同一个自定义层中实现激活呢？</p><p id="f06c" class="pw-post-body-paragraph lg lh it kk b kl km ju li kn ko jx lj kp lk ll lm kr ln lo lp kt lq lr ls kv im bi translated">答案是对自定义密集层中的“__init__”和“call”方法进行简单的调整。</p><pre class="mz na nb nc gt np nq nr ns aw nt bi"><span id="0b6b" class="ma mb it nq b gy nu nv l nw nx"><strong class="nq iu">class</strong> <strong class="nq iu">SimpleDense(Layer):</strong><br/><br/>    <em class="no"># add an activation parameter</em></span><span id="03f9" class="ma mb it nq b gy ny nv l nw nx">  <strong class="nq iu">  def __init__(self, units=32, activation=None):<br/>        super(SimpleDense, self).__init__()<br/>        self.units = units</strong><br/>        <br/>        <em class="no"># define the activation to get from the built-in activation layers in Keras</em></span><span id="5c27" class="ma mb it nq b gy ny nv l nw nx">        <strong class="nq iu">self.activation = tf.keras.activations.get(activation)</strong><br/><br/><br/>    <strong class="nq iu">def</strong> <strong class="nq iu">build(self, input_shape):</strong></span><span id="9ff7" class="ma mb it nq b gy ny nv l nw nx"><strong class="nq iu">        w_init = tf.random_normal_initializer()<br/>        self.w = tf.Variable(name="kernel",<br/>            initial_value=w_init(shape=(input_shape[-1], self.units),dtype='float32'),trainable=True)</strong></span><span id="1c4a" class="ma mb it nq b gy ny nv l nw nx"><strong class="nq iu">        b_init = tf.zeros_initializer()<br/>        self.b = tf.Variable(name="bias",<br/>            initial_value=b_init(shape=(self.units,), dtype='float32'),trainable=True)<br/>        super().build(input_shape)</strong><br/><br/><br/><strong class="nq iu">    def call(self, inputs):</strong><br/>        <br/>        <em class="no"># pass the computation to the activation layer</em></span><span id="3535" class="ma mb it nq b gy ny nv l nw nx">      <strong class="nq iu">  return self.activation(tf.matmul(inputs, self.w) + self.b)</strong></span></pre><p id="1cf9" class="pw-post-body-paragraph lg lh it kk b kl km ju li kn ko jx lj kp lk ll lm kr ln lo lp kt lq lr ls kv im bi translated"><strong class="kk iu"> <em class="no">上图</em> </strong>的代码解释——大部分代码和我们之前用的代码一模一样。</p><p id="1044" class="pw-post-body-paragraph lg lh it kk b kl km ju li kn ko jx lj kp lk ll lm kr ln lo lp kt lq lr ls kv im bi translated">要添加激活，我们需要在“__init__”中指定我们需要激活。一个字符串或者一个激活对象的实例可以被传递到这个激活中。它默认设置为None，所以如果没有提到激活函数，它不会抛出错误。接下来，我们必须将激活函数初始化为“TF . keras . activations . get(activation)”。</p><p id="26c7" class="pw-post-body-paragraph lg lh it kk b kl km ju li kn ko jx lj kp lk ll lm kr ln lo lp kt lq lr ls kv im bi translated">最后的编辑是在“调用”方法中，在计算权重和偏差之前，我们需要添加自激活来激活计算。所以现在回报是计算和激活。</p><h2 id="31b9" class="ma mb it bd mc md me dn mf mg mh dp mi kp mj mk ml kr mm mn mo kt mp mq mr ms bi translated">在mnist数据集上激活的自定义密集层的完整代码:</h2><pre class="mz na nb nc gt np nq nr ns aw nt bi"><span id="b30a" class="ma mb it nq b gy nu nv l nw nx"><strong class="nq iu"><br/>import</strong> <strong class="nq iu">tensorflow</strong> <strong class="nq iu">as</strong> <strong class="nq iu">tf</strong><br/><strong class="nq iu">from</strong> <strong class="nq iu">tensorflow.keras.layers</strong> <strong class="nq iu">import</strong> <strong class="nq iu">Layer</strong></span><span id="0d37" class="ma mb it nq b gy ny nv l nw nx"><strong class="nq iu">class</strong> <strong class="nq iu">SimpleDense(Layer):</strong><br/></span><span id="5d46" class="ma mb it nq b gy ny nv l nw nx"><strong class="nq iu">def __init__(self, units=32, activation=None):<br/>        super(SimpleDense, self).__init__()<br/>        self.units = units</strong><br/>        <br/>        <em class="no"># define the activation to get from the built-in activation layers in Keras</em></span><span id="2a38" class="ma mb it nq b gy ny nv l nw nx"><strong class="nq iu">self.activation = tf.keras.activations.get(activation)</strong><br/><br/><br/>    <strong class="nq iu">def</strong> <strong class="nq iu">build(self, input_shape):</strong></span><span id="8d66" class="ma mb it nq b gy ny nv l nw nx"><strong class="nq iu">w_init = tf.random_normal_initializer()<br/>        self.w = tf.Variable(name="kernel",<br/>            initial_value=w_init(shape=(input_shape[-1], self.units),dtype='float32'),trainable=True)</strong></span><span id="c43e" class="ma mb it nq b gy ny nv l nw nx"><strong class="nq iu">b_init = tf.zeros_initializer()<br/>        self.b = tf.Variable(name="bias",<br/>            initial_value=b_init(shape=(self.units,), dtype='float32'),trainable=True)<br/>        super().build(input_shape)</strong><br/><br/><br/><strong class="nq iu">    def call(self, inputs):</strong><br/>        <br/>        <em class="no"># pass the computation to the activation layer</em></span><span id="777a" class="ma mb it nq b gy ny nv l nw nx"><strong class="nq iu">return self.activation(tf.matmul(inputs, self.w) + self.b)</strong></span><span id="b958" class="ma mb it nq b gy ny nv l nw nx"><strong class="nq iu">mnist = tf.keras.datasets.mnist<br/><br/>(x_train, y_train),(x_test, y_test) = mnist.load_data()<br/>x_train, x_test = x_train / 255.0, x_test / 255.0</strong></span><span id="2997" class="ma mb it nq b gy ny nv l nw nx"># build the model<strong class="nq iu"><br/>model = tf.keras.models.Sequential([<br/>    tf.keras.layers.Flatten(input_shape=(28, 28)),</strong></span><span id="340e" class="ma mb it nq b gy ny nv l nw nx"><strong class="nq iu">    </strong><em class="no"># our custom Dense layer with activation</em><strong class="nq iu"><br/>    SimpleDense(128, activation='relu'),<br/>    tf.keras.layers.Dropout(0.2),<br/>    tf.keras.layers.Dense(10, activation='softmax')<br/>])</strong></span><span id="e723" class="ma mb it nq b gy ny nv l nw nx"><em class="no"># compile the model</em><strong class="nq iu"><br/>model.compile(optimizer='adam',<br/>              loss='sparse_categorical_crossentropy',<br/>              metrics=['accuracy'])</strong></span><span id="3d6b" class="ma mb it nq b gy ny nv l nw nx"><em class="no"># fit the model</em><strong class="nq iu"><br/>model.fit(x_train, y_train, epochs=5)<br/>model.evaluate(x_test, y_test)</strong></span></pre><p id="f6aa" class="pw-post-body-paragraph lg lh it kk b kl km ju li kn ko jx lj kp lk ll lm kr ln lo lp kt lq lr ls kv im bi translated">使用我们的自定义密集层和激活来训练模型，训练准确率为97.8%，验证准确率为97.7%。</p><h2 id="08b5" class="ma mb it bd mc md me dn mf mg mh dp mi kp mj mk ml kr mm mn mo kt mp mq mr ms bi translated">结论:</h2><p id="d243" class="pw-post-body-paragraph lg lh it kk b kl mt ju li kn mu jx lj kp mv ll lm kr mw lo lp kt mx lr ls kv im bi translated">这是在TensorFlow中创建自定义图层的方法。尽管我们只看到密集层的工作，但它可以很容易地被任何其他层所取代，例如执行以下计算的二次层——</p><p id="a928" class="pw-post-body-paragraph lg lh it kk b kl km ju li kn ko jx lj kp lk ll lm kr ln lo lp kt lq lr ls kv im bi translated">它有<strong class="kk iu"> 3个状态变量</strong> : <em class="no"> a，b，c，</em></p><p id="df16" class="pw-post-body-paragraph lg lh it kk b kl km ju li kn ko jx lj kp lk ll lm kr ln lo lp kt lq lr ls kv im bi translated">计算:</p><figure class="mz na nb nc gt nd gh gi paragraph-image"><div class="gh gi nz"><img src="../Images/e61eb367459f8ae54d2d8f73a3b43d04.png" data-original-src="https://miro.medium.com/v2/resize:fit:850/format:webp/1*QhvmyjNaeRy1FZbEkLMG7g.png"/></div></figure><p id="6cb5" class="pw-post-body-paragraph lg lh it kk b kl km ju li kn ko jx lj kp lk ll lm kr ln lo lp kt lq lr ls kv im bi translated"><strong class="kk iu">用二次层替换密集层:</strong></p><pre class="mz na nb nc gt np nq nr ns aw nt bi"><span id="a754" class="ma mb it nq b gy nu nv l nw nx"><strong class="nq iu">import</strong> <strong class="nq iu">tensorflow</strong> <strong class="nq iu">as</strong> <strong class="nq iu">tf</strong><br/><strong class="nq iu">from</strong> <strong class="nq iu">tensorflow.keras.layers</strong> <strong class="nq iu">import</strong> <strong class="nq iu">Layer</strong></span><span id="5cd4" class="ma mb it nq b gy ny nv l nw nx"><strong class="nq iu">class</strong> <strong class="nq iu">SimpleQuadratic(Layer):</strong><br/><br/>    <strong class="nq iu">def __init__(self, units=32, activation=None):</strong></span><span id="ce6a" class="ma mb it nq b gy ny nv l nw nx">        <em class="no">'''Initializes the class and sets up the internal variables'''</em><br/>        <br/>       <strong class="nq iu"> super(SimpleQuadratic,self).__init__()<br/>        self.units=units<br/>        self.activation=tf.keras.activations.get(activation)</strong><br/>    <br/>   <strong class="nq iu"> def build(self, input_shape):</strong></span><span id="7e47" class="ma mb it nq b gy ny nv l nw nx">        <em class="no">'''Create the state of the layer (weights)'''</em><br/>        <br/>        <strong class="nq iu">a_init = tf.random_normal_initializer()<br/>        a_init_val = a_init(shape=(input_shape[-1],self.units),dtype= 'float32')<br/>        self.a = tf.Variable(initial_value=a_init_val, trainable='true')<br/>        <br/>        b_init = tf.random_normal_initializer()<br/>        b_init_val = b_init(shape=(input_shape[-1],self.units),dtype= 'float32')<br/>        self.b = tf.Variable(initial_value=b_init_val, trainable='true')<br/>        <br/>        c_init= tf.zeros_initializer()<br/>        c_init_val = c_init(shape=(self.units,),dtype='float32')<br/>        self.c = tf.Variable(initial_value=c_init_val,trainable='true')</strong><br/>        <br/>   <br/>   <strong class="nq iu"> def call(self, inputs):</strong></span><span id="3d99" class="ma mb it nq b gy ny nv l nw nx">        <em class="no">'''Defines the computation from inputs to outputs'''</em></span><span id="64dc" class="ma mb it nq b gy ny nv l nw nx">    <strong class="nq iu">    x_squared= tf.math.square(inputs)<br/>        x_squared_times_a = tf.matmul(x_squared,self.a)<br/>        x_times_b= tf.matmul(inputs,self.b)<br/>        x2a_plus_xb_plus_c = x_squared_times_a+x_times_b+self.c<br/>        <br/>        return self.activation(x2a_plus_xb_plus_c)</strong></span><span id="7fe6" class="ma mb it nq b gy ny nv l nw nx"><strong class="nq iu">mnist = tf.keras.datasets.mnist<br/><br/>(x_train, y_train),(x_test, y_test) = mnist.load_data()<br/>x_train, x_test = x_train / 255.0, x_test / 255.0<br/><br/>model = tf.keras.models.Sequential([<br/>  tf.keras.layers.Flatten(input_shape=(28, 28)),<br/>  SimpleQuadratic(128, activation='relu'),<br/>  tf.keras.layers.Dropout(0.2),<br/>  tf.keras.layers.Dense(10, activation='softmax')<br/>])<br/><br/>model.compile(optimizer='adam',<br/>              loss='sparse_categorical_crossentropy',<br/>              metrics=['accuracy'])<br/><br/>model.fit(x_train, y_train, epochs=5)<br/>model.evaluate(x_test, y_test)</strong></span></pre><p id="6550" class="pw-post-body-paragraph lg lh it kk b kl km ju li kn ko jx lj kp lk ll lm kr ln lo lp kt lq lr ls kv im bi translated">这个二次层在mnist数据集上给出了97.8%的验证准确率。</p><p id="bb45" class="pw-post-body-paragraph lg lh it kk b kl km ju li kn ko jx lj kp lk ll lm kr ln lo lp kt lq lr ls kv im bi translated">因此，我们可以实现我们自己的层，并根据需要激活TensorFlow模型，以编辑甚至提高整体精度。</p></div></div>    
</body>
</html>