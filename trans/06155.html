<html>
<head>
<title>Battle of the Ensemble — Random Forest vs Gradient Boosting</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">整体之战——随机森林vs梯度推进</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/battle-of-the-ensemble-random-forest-vs-gradient-boosting-6fbfed14cb7?source=collection_archive---------10-----------------------#2021-06-02">https://towardsdatascience.com/battle-of-the-ensemble-random-forest-vs-gradient-boosting-6fbfed14cb7?source=collection_archive---------10-----------------------#2021-06-02</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="39e9" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">机器学习领域最流行的两种算法，谁会赢？</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/a65d5b2e50ff7911b71e003446b43476.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*newKWr1Ib_2ZDCI9"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">Geran de Klerk 在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="18e4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果你在机器学习领域呆过一段时间，你肯定听说过一个叫做偏差-方差权衡的概念。这是任何机器学习实践者都应该学习和意识到的最重要的概念之一。</p><p id="ad9b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">本质上，偏差-方差权衡是机器学习中的一个难题，即具有低偏差的模型通常具有高方差，反之亦然。</p><p id="a7b9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">偏差是模型预测的实际值和期望值之间的差异。结果，一个高偏差的模型被认为是过于简单，数据拟合不足。</p><p id="9f3a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">另一方面，方差表示模型对训练数据中微小波动的敏感性。具有高方差的模型对噪声敏感，因此会过度拟合数据。换句话说，该模型非常适合训练数据，但不能概括看不见的(测试)数据。</p><p id="c0ca" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">考虑到这一点，在本文中，我想分享几种平衡偏倚和方差的技术之一:集成方法。</p><p id="2666" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">首先，什么是合奏法？</p><blockquote class="lv lw lx"><p id="606e" class="kz la ly lb b lc ld ju le lf lg jx lh lz lj lk ll ma ln lo lp mb lr ls lt lu im bi translated">集成方法涉及聚合多个机器学习模型，目的是减少偏差和方差。理想情况下，集成方法的结果将优于任何单独的机器学习模型。</p></blockquote><p id="4be0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">有3种主要的集合方法:</p><ul class=""><li id="7409" class="mc md it lb b lc ld lf lg li me lm mf lq mg lu mh mi mj mk bi translated">制袋材料</li><li id="d15c" class="mc md it lb b lc ml lf mm li mn lm mo lq mp lu mh mi mj mk bi translated">助推</li><li id="ffda" class="mc md it lb b lc ml lf mm li mn lm mo lq mp lu mh mi mj mk bi translated">堆垛</li></ul><p id="432c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">出于本文的目的，我们将只关注前两个:打包和提升。具体来说，我们将检查和对比两种机器学习模型:<strong class="lb iu">随机森林</strong>和<strong class="lb iu">梯度增强</strong>，它们分别利用了打包和增强技术。</p><p id="bf05" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">此外，我们将在本文的后半部分应用这两个算法来解决泰坦尼克号生存预测竞赛，以了解它们在实践中是如何工作的。</p><div class="mq mr gp gr ms mt"><a href="https://www.kaggle.com/c/titanic" rel="noopener  ugc nofollow" target="_blank"><div class="mu ab fo"><div class="mv ab mw cl cj mx"><h2 class="bd iu gy z fp my fr fs mz fu fw is bi translated">泰坦尼克号-机器从灾难中学习</h2><div class="na l"><h3 class="bd b gy z fp my fr fs mz fu fw dk translated">从这里开始！预测泰坦尼克号上的生存并熟悉ML基础知识</h3></div><div class="nb l"><p class="bd b dl z fp my fr fs mz fu fw dk translated">www.kaggle.com</p></div></div><div class="nc l"><div class="nd l ne nf ng nc nh ks mt"/></div></div></a></div></div><div class="ab cl ni nj hx nk" role="separator"><span class="nl bw bk nm nn no"/><span class="nl bw bk nm nn no"/><span class="nl bw bk nm nn"/></div><div class="im in io ip iq"><h1 id="9e92" class="np nq it bd nr ns nt nu nv nw nx ny nz jz oa ka ob kc oc kd od kf oe kg of og bi translated">决策图表</h1><p id="2a75" class="pw-post-body-paragraph kz la it lb b lc oh ju le lf oi jx lh li oj lk ll lm ok lo lp lq ol ls lt lu im bi translated">在我们开始之前，重要的是我们首先理解什么是决策树，因为它是随机森林和梯度推进的基础算法。</p><blockquote class="lv lw lx"><p id="c86d" class="kz la ly lb b lc ld ju le lf lg jx lh lz lj lk ll ma ln lo lp mb lr ls lt lu im bi translated">决策树是一种监督学习算法，它为任何基于树的模型(如随机森林和梯度推进)奠定了基础。决策树可用于分类和回归问题。</p></blockquote><p id="4814" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">树的每个节点代表一个变量和该变量上的一个分割点(假设该变量是数值型的)。树的叶节点包含树用来进行预测的输出变量。</p><p id="ad9e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们以Kaggle房价预测<a class="ae ky" href="https://www.kaggle.com/c/house-prices-advanced-regression-techniques" rel="noopener ugc nofollow" target="_blank">比赛</a>为例。</p><p id="62d4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">假设我们正在构建一个决策树模型，该模型将考虑一所房子的各种特征，例如卧室数量、地段大小、邻居位置等，以便对其最终销售价格进行预测。</p><p id="6068" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为简单起见，假设我们模型的最终结果看起来像这样:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi om"><img src="../Images/46ee2e9700cef030a7bbbb4223f1ef9f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eE4BZMHgkH1kzT92JANmKA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图片来源<a class="ae ky" href="https://www.kaggle.com/dansbecker/how-models-work" rel="noopener ugc nofollow" target="_blank">丹·贝克尔，卡格尔</a></p></figure><p id="3ce9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">给定一个随机房屋，我们的模型现在能够从决策树的最顶端(根节点)向下遍历到树的底部(叶节点),并给出该特定房屋的预测价格。</p><p id="bfa9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">更具体地说，基于这个模型，一栋拥有两个以上卧室、面积大于11，500平方英尺的房子的预测价格为233，000美元，以此类推。</p><p id="3dbc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">当然，决策树可以变得比上面显示的更加复杂和精密，具有更大的深度和更多的节点，这反过来将使树能够捕捉预测器和目标变量之间更详细的关系。</p></div><div class="ab cl ni nj hx nk" role="separator"><span class="nl bw bk nm nn no"/><span class="nl bw bk nm nn no"/><span class="nl bw bk nm nn"/></div><div class="im in io ip iq"><h1 id="ee5f" class="np nq it bd nr ns nt nu nv nw nx ny nz jz oa ka ob kc oc kd od kf oe kg of og bi translated">随机森林(装袋)</h1><p id="c6b6" class="pw-post-body-paragraph kz la it lb b lc oh ju le lf oi jx lh li oj lk ll lm ok lo lp lq ol ls lt lu im bi translated">既然我们已经了解了什么是决策树以及它是如何工作的，让我们来检查一下我们的第一个集成方法，bagging。</p><blockquote class="lv lw lx"><p id="1376" class="kz la ly lb b lc ld ju le lf lg jx lh lz lj lk ll ma ln lo lp mb lr ls lt lu im bi translated">Bagging也称为bootstrap aggregating，是指使用不同的训练数据子集(引导数据集)创建和合并独立、并行决策树集合的过程。</p></blockquote><p id="6597" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">密切注意的话，<em class="ly">独立</em>和<em class="ly">平行。</em>使用随机森林构建的决策树对模型中的其他树没有任何知识和影响。这是装袋的一个关键特征。</p><p id="f8be" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">一旦构建了所有的树，模型将选择由单个决策树做出的所有预测的模式(多数投票),并将结果作为最终预测返回。</p><p id="c7d8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我希望现在已经很清楚，装袋通过在多棵树上分散错误的风险来减少对单棵树的依赖，这也间接减少了过度拟合的风险。</p><p id="6d34" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">显然，随机森林并不是没有缺陷和不足。以下是一些应该和不应该使用随机森林的情况:</p><h2 id="6e0c" class="on nq it bd nr oo op dn nv oq or dp nz li os ot ob lm ou ov od lq ow ox of oy bi translated">何时使用随机森林</h2><ul class=""><li id="f0d2" class="mc md it lb b lc oh lf oi li oz lm pa lq pb lu mh mi mj mk bi translated">它可用于分类(<a class="ae ky" href="http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html" rel="noopener ugc nofollow" target="_blank"> RandomForestClassifier </a>)和回归(<a class="ae ky" href="http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html" rel="noopener ugc nofollow" target="_blank"> RandomForestRegressor </a>)问题</li><li id="3d0c" class="mc md it lb b lc ml lf mm li mn lm mo lq mp lu mh mi mj mk bi translated">您对预测值的重要性(特征重要性)感兴趣</li><li id="c59b" class="mc md it lb b lc ml lf mm li mn lm mo lq mp lu mh mi mj mk bi translated">您需要一个快速的基准模型，因为随机森林可以快速训练，并且需要最少的预处理，例如特征缩放</li><li id="28a9" class="mc md it lb b lc ml lf mm li mn lm mo lq mp lu mh mi mj mk bi translated">如果您有杂乱的数据，例如缺失数据、异常值等</li></ul><h2 id="a2d5" class="on nq it bd nr oo op dn nv oq or dp nz li os ot ob lm ou ov od lq ow ox of oy bi translated">何时不使用随机森林</h2><ul class=""><li id="1b7c" class="mc md it lb b lc oh lf oi li oz lm pa lq pb lu mh mi mj mk bi translated">如果你正在解决一个复杂、新奇的问题</li><li id="2b1d" class="mc md it lb b lc ml lf mm li mn lm mo lq mp lu mh mi mj mk bi translated">透明度很重要</li><li id="d240" class="mc md it lb b lc ml lf mm li mn lm mo lq mp lu mh mi mj mk bi translated">预测时间很重要，因为模型需要时间来聚合来自多个决策树的结果，然后才能得出最终预测</li></ul><p id="efaf" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">总结一下随机森林，这里有一些要考虑的关键超参数:</p><ul class=""><li id="5aad" class="mc md it lb b lc ld lf lg li me lm mf lq mg lu mh mi mj mk bi translated"><strong class="lb iu"> n_estimators: </strong>控制将建立多少个单独的决策树</li><li id="21c9" class="mc md it lb b lc ml lf mm li mn lm mo lq mp lu mh mi mj mk bi translated"><strong class="lb iu"> max_depth: </strong>控制每个单独的决策树可以有多深</li></ul></div><div class="ab cl ni nj hx nk" role="separator"><span class="nl bw bk nm nn no"/><span class="nl bw bk nm nn no"/><span class="nl bw bk nm nn"/></div><div class="im in io ip iq"><h1 id="8a4f" class="np nq it bd nr ns nt nu nv nw nx ny nz jz oa ka ob kc oc kd od kf oe kg of og bi translated">梯度推进(推进)</h1><blockquote class="lv lw lx"><p id="7290" class="kz la ly lb b lc ld ju le lf lg jx lh lz lj lk ll ma ln lo lp mb lr ls lt lu im bi translated">另一方面，Boosting采用迭代方法，通过关注先前迭代中的错误，将多个弱的顺序模型组合起来，创建一个强模型。</p></blockquote><p id="43cf" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">弱模型是仅比随机猜测稍好的模型，而强模型是与真实分类强相关的模型。</p><p id="4d2d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">boosting不同于bagging的一个重要区别是，boosting下的决策树不是独立构建的，而是以连续的方式构建的，其中每棵树都有效地从之前的错误中学习错误。</p><p id="0f41" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">还值得注意的是，还有其他的增强变体，例如<a class="ae ky" href="https://www.youtube.com/watch?v=LsK-xG1cLYA" rel="noopener ugc nofollow" target="_blank"> AdaBoost </a>(自适应增强)<a class="ae ky" href="https://xgboost.readthedocs.io/en/latest/" rel="noopener ugc nofollow" target="_blank"> XGBoost </a>(极端梯度增强)和<a class="ae ky" href="https://lightgbm.readthedocs.io/en/latest/" rel="noopener ugc nofollow" target="_blank"> LightGBM </a>(轻度梯度增强)，但是对于本文的目的，我们将只关注梯度增强。</p><p id="ce22" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">与上一节类似，以下是一些应该和不应该使用梯度增强的情况:</p><h2 id="3a6e" class="on nq it bd nr oo op dn nv oq or dp nz li os ot ob lm ou ov od lq ow ox of oy bi translated">何时使用梯度增强</h2><ul class=""><li id="c0cf" class="mc md it lb b lc oh lf oi li oz lm pa lq pb lu mh mi mj mk bi translated">它可用于分类(<a class="ae ky" href="http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html" rel="noopener ugc nofollow" target="_blank">GradientBoostingClassifier</a>)和回归(<a class="ae ky" href="http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html" rel="noopener ugc nofollow" target="_blank">GradientBoostingRegressor</a>)问题</li><li id="350c" class="mc md it lb b lc ml lf mm li mn lm mo lq mp lu mh mi mj mk bi translated">您对预测值的重要性(特征重要性)感兴趣</li><li id="e230" class="mc md it lb b lc ml lf mm li mn lm mo lq mp lu mh mi mj mk bi translated">预测时间很重要，因为与随机森林不同，梯度推进下的决策树不能并行构建，因此构建连续树的过程需要一些时间</li></ul><h2 id="b9cd" class="on nq it bd nr oo op dn nv oq or dp nz li os ot ob lm ou ov od lq ow ox of oy bi translated">何时不使用梯度增强</h2><ul class=""><li id="4111" class="mc md it lb b lc oh lf oi li oz lm pa lq pb lu mh mi mj mk bi translated">透明度很重要</li><li id="1aad" class="mc md it lb b lc ml lf mm li mn lm mo lq mp lu mh mi mj mk bi translated">培训时间很重要，或者当您的计算能力有限时</li><li id="192f" class="mc md it lb b lc ml lf mm li mn lm mo lq mp lu mh mi mj mk bi translated">您的数据非常嘈杂，因为梯度增强往往会强调甚至最小的误差，因此，它可能会过度适应数据中的噪声</li></ul><p id="3245" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">此外，以下是梯度增强需要考虑的一些关键超参数:</p><ul class=""><li id="731c" class="mc md it lb b lc ld lf lg li me lm mf lq mg lu mh mi mj mk bi translated"><strong class="lb iu"> learning_rate: </strong>促进了算法找到最优解的速度和速度</li><li id="f3f5" class="mc md it lb b lc ml lf mm li mn lm mo lq mp lu mh mi mj mk bi translated"><strong class="lb iu"> max_depth: </strong>控制将建立多少个单独的决策树(梯度推进下的树通常比随机森林下的树浅)</li><li id="3718" class="mc md it lb b lc ml lf mm li mn lm mo lq mp lu mh mi mj mk bi translated"><strong class="lb iu"> n_estimators: </strong>控制要建立多少棵连续的树(梯度推进下的树的数量通常比随机森林多)</li></ul></div><div class="ab cl ni nj hx nk" role="separator"><span class="nl bw bk nm nn no"/><span class="nl bw bk nm nn no"/><span class="nl bw bk nm nn"/></div><div class="im in io ip iq"><h1 id="2dc0" class="np nq it bd nr ns nt nu nv nw nx ny nz jz oa ka ob kc oc kd od kf oe kg of og bi translated">泰坦尼克号案例研究</h1><p id="de49" class="pw-post-body-paragraph kz la it lb b lc oh ju le lf oi jx lh li oj lk ll lm ok lo lp lq ol ls lt lu im bi translated">正如所承诺的，现在让我们在一个实际的项目中应用随机森林和梯度推进，泰坦尼克号生存预测竞赛，以加强我们在本文中已经覆盖的内容。</p><p id="96fa" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果你想继续，请点击这里查看我的GitHub上的完整笔记本。</p><p id="2853" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们先来看看数据集的前5行。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pc"><img src="../Images/076589ce9b2859ea2b5cfb17939f1b76.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*w0oGJWT-tscIjmVxU6FlzA.png"/></div></div></figure><p id="8018" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在，我们将执行一些特征工程和数据预处理，以便为建模准备好数据。具体来说，我们将执行以下操作:</p><ul class=""><li id="1624" class="mc md it lb b lc ld lf lg li me lm mf lq mg lu mh mi mj mk bi translated">在年龄栏中填入乘客的平均年龄</li><li id="8ace" class="mc md it lb b lc ml lf mm li mn lm mo lq mp lu mh mi mj mk bi translated">将SibSp和Parch特性组合成一个特性:family_size</li><li id="75e9" class="mc md it lb b lc ml lf mm li mn lm mo lq mp lu mh mi mj mk bi translated">创建一个新特征cabin_missing，它作为cabin列中缺失数据的指示器</li><li id="3819" class="mc md it lb b lc ml lf mm li mn lm mo lq mp lu mh mi mj mk bi translated">对性别列进行编码，将0分配给男性乘客，1分配给女性乘客</li><li id="f49f" class="mc md it lb b lc ml lf mm li mn lm mo lq mp lu mh mi mj mk bi translated">训练测试分割(80%训练集和20%测试集)</li></ul><p id="7499" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我将在本文中省略细节，但是如果您对这些步骤背后的基本原理和实际代码感兴趣，请参考我的<a class="ae ky" href="https://github.com/chongjason914/bagging-vs-boosting" rel="noopener ugc nofollow" target="_blank">笔记本</a>。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pd"><img src="../Images/bc0fdeda37bbb5a4417918d62bf74035.png" data-original-src="https://miro.medium.com/v2/resize:fit:1124/format:webp/1*nmgWuuo-IRPFs5C5KPhNzQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">我们的数据现在可以用于建模了！</p></figure><h2 id="c543" class="on nq it bd nr oo op dn nv oq or dp nz li os ot ob lm ou ov od lq ow ox of oy bi translated">随机森林分类器</h2><p id="3066" class="pw-post-body-paragraph kz la it lb b lc oh ju le lf oi jx lh li oj lk ll lm ok lo lp lq ol ls lt lu im bi translated">要查看该模型的默认超参数:</p><pre class="kj kk kl km gt pe pf pg ph aw pi bi"><span id="e34b" class="on nq it pf b gy pj pk l pl pm"># Default hyperparameters for RandomForestClassifier <br/>print(RandomForestClassifier())</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pn"><img src="../Images/13a9412f60d9fdabca2801eac1c55b72.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uMh7CauEtlJ2asWFw_MT3g.png"/></div></div></figure><p id="6803" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在我们将模型拟合到训练数据之前，我们可以使用<a class="ae ky" href="http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html" rel="noopener ugc nofollow" target="_blank"> GridSearchCV </a>来找到超参数的最优集合。</p><pre class="kj kk kl km gt pe pf pg ph aw pi bi"><span id="9f83" class="on nq it pf b gy pj pk l pl pm"># Set up GridSearchCV<br/>rf = RandomForestClassifier(n_jobs = -1, random_state = 10)<br/>params = {<br/>    'n_estimators': [5, 50, 250],<br/>    'max_depth': [2, 4, 8, 16, 32, None]<br/>}<br/>cv = GridSearchCV(rf, params, cv = 5, n_jobs = -1)</span><span id="35f5" class="on nq it pf b gy po pk l pl pm"># Fit GridSearchCV to training set <br/>cv.fit(X_train, Y_train)</span><span id="cffe" class="on nq it pf b gy po pk l pl pm"># Best parameters<br/>cv.best_params_</span></pre><ul class=""><li id="3784" class="mc md it lb b lc ld lf lg li me lm mf lq mg lu mh mi mj mk bi translated"><strong class="lb iu">最大深度:</strong> 4</li><li id="4e2b" class="mc md it lb b lc ml lf mm li mn lm mo lq mp lu mh mi mj mk bi translated"><strong class="lb iu">n _估计量:</strong> 50</li></ul><p id="0a11" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">换句话说，这个训练集的最理想的随机森林模型包含50个最大深度为4的决策树。</p><p id="e307" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">最后，我们可以继续使用这组超参数来拟合我们的模型，并随后评估它在测试集上的性能。</p><pre class="kj kk kl km gt pe pf pg ph aw pi bi"><span id="afce" class="on nq it pf b gy pj pk l pl pm"># Instantiate RandomForestClassifier with best hyperparameters <br/>rf = RandomForestClassifier(n_estimators = 50, max_depth = 4, n_jobs = -1, random_state = 42)</span><span id="25dd" class="on nq it pf b gy po pk l pl pm"># Fit model<br/>start = time.time()<br/>rf_model = rf.fit(X_train, Y_train)<br/>end = time.time()<br/>fit_time = end - start</span><span id="a420" class="on nq it pf b gy po pk l pl pm"># Predict <br/>start = time.time()<br/>Y_pred = rf_model.predict(X_test)<br/>end = time.time()<br/>pred_time = end - start</span><span id="873c" class="on nq it pf b gy po pk l pl pm"># Time and prediction results<br/>precision, recall, fscore, support = score(Y_test, Y_pred, average = 'binary')<br/>print(f"Fit time: {round(fit_time, 3)} / Predict time: {round(pred_time, 3)}")<br/>print(f"Precision: {round(precision, 3)} / Recall: {round(recall, 3)} / Accuracy: {round((Y_pred==Y_test).sum() / len(Y_pred), 3)}")</span></pre><ul class=""><li id="21e7" class="mc md it lb b lc ld lf lg li me lm mf lq mg lu mh mi mj mk bi translated"><strong class="lb iu">拟合时间:</strong> 0.469</li><li id="8566" class="mc md it lb b lc ml lf mm li mn lm mo lq mp lu mh mi mj mk bi translated"><strong class="lb iu">预测时间:</strong> 0.141</li><li id="83e0" class="mc md it lb b lc ml lf mm li mn lm mo lq mp lu mh mi mj mk bi translated"><strong class="lb iu">精度:</strong> 0.797</li><li id="6406" class="mc md it lb b lc ml lf mm li mn lm mo lq mp lu mh mi mj mk bi translated"><strong class="lb iu">召回:</strong> 0.689</li><li id="0f65" class="mc md it lb b lc ml lf mm li mn lm mo lq mp lu mh mi mj mk bi translated"><strong class="lb iu">精度:</strong> 0.799</li></ul><pre class="kj kk kl km gt pe pf pg ph aw pi bi"><span id="ae43" class="on nq it pf b gy pj pk l pl pm"># Confusion matrix for RandomForestClassifier<br/>matrix = confusion_matrix(Y_test, Y_pred)<br/>sns.heatmap(matrix, annot = True, fmt = 'd')</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pp"><img src="../Images/a755f062fea7be5745e48217121cce30.png" data-original-src="https://miro.medium.com/v2/resize:fit:892/format:webp/1*2dDF7o0Ski7nvHMzSP4ANA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片:随机森林的混淆矩阵</p></figure><h2 id="bdb6" class="on nq it bd nr oo op dn nv oq or dp nz li os ot ob lm ou ov od lq ow ox of oy bi translated">梯度推进分级机</h2><p id="5eb0" class="pw-post-body-paragraph kz la it lb b lc oh ju le lf oi jx lh li oj lk ll lm ok lo lp lq ol ls lt lu im bi translated">现在，让我们来看看梯度增强如何对抗随机森林。</p><p id="8b3b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">类似地，要查看该模型的默认超参数:</p><pre class="kj kk kl km gt pe pf pg ph aw pi bi"><span id="5857" class="on nq it pf b gy pj pk l pl pm"># Default hyperparameters for GradientBoostingClassifier<br/>print(GradientBoostingClassifier())</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pq"><img src="../Images/3e16bf46c651d98bf99079a610f66204.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_Gjuyc4roc_WVHRBmIxNrw.png"/></div></div></figure><p id="31dc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">使用GridSearchCV查找最佳超参数。</p><pre class="kj kk kl km gt pe pf pg ph aw pi bi"><span id="96c8" class="on nq it pf b gy pj pk l pl pm"># Set up GridSearchCV<br/>gb = GradientBoostingClassifier(random_state = 10)<br/>params = {<br/>    'n_estimators': [5, 50, 250, 500],<br/>    'max_depth': [1, 3, 5, 7, 9],<br/>    'learning_rate': [0.01, 0.1, 1, 10, 100]<br/>}cv = GridSearchCV(gb, params, cv = 5, n_jobs = -1)</span><span id="3ce0" class="on nq it pf b gy po pk l pl pm"># Fit GridSearchCV to training set<br/>cv.fit(X_train, Y_train)</span><span id="a751" class="on nq it pf b gy po pk l pl pm"># Best parameters<br/>cv.best_params_</span></pre><ul class=""><li id="067c" class="mc md it lb b lc ld lf lg li me lm mf lq mg lu mh mi mj mk bi translated"><strong class="lb iu">学习率:</strong> 0.01</li><li id="1675" class="mc md it lb b lc ml lf mm li mn lm mo lq mp lu mh mi mj mk bi translated"><strong class="lb iu">最大深度:</strong> 3</li><li id="0661" class="mc md it lb b lc ml lf mm li mn lm mo lq mp lu mh mi mj mk bi translated"><strong class="lb iu">n _估计数:</strong> 250</li></ul><p id="c9f8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">正如我们所看到的，使用梯度推进构建的树比使用随机森林构建的树更浅，但更重要的是两种模型之间估计量的差异。梯度增强比随机森林有更多的树。</p><p id="ea01" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这证实了我们之前讨论的随机森林和梯度增强的结构以及它们的工作方式。</p><p id="4ae1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">接下来，让我们将我们的梯度推进模型拟合到训练数据。</p><pre class="kj kk kl km gt pe pf pg ph aw pi bi"><span id="a6f4" class="on nq it pf b gy pj pk l pl pm"># Instantiate GradientBoostingClassifier with best hyperparameters <br/>rf = GradientBoostingClassifier(n_estimators = 250, max_depth = 3, learning_rate = 0.01, random_state = 42)</span><span id="5da9" class="on nq it pf b gy po pk l pl pm"># Fit model<br/>start = time.time()<br/>rf_model = rf.fit(X_train, Y_train)<br/>end = time.time()<br/>fit_time = end - start</span><span id="67f6" class="on nq it pf b gy po pk l pl pm"># Predict <br/>start = time.time()<br/>Y_pred = rf_model.predict(X_test)<br/>end = time.time()<br/>pred_time = end - start</span><span id="576a" class="on nq it pf b gy po pk l pl pm"># Time and prediction results<br/>precision, recall, fscore, support = score(Y_test, Y_pred, average = 'binary')<br/>print(f"Fit time: {round(fit_time, 3)} / Predict time: {round(pred_time, 3)}")<br/>print(f"Precision: {round(precision, 3)} / Recall: {round(recall, 3)} / Accuracy: {round((Y_pred==Y_test).sum() / len(Y_pred), 3)}")</span></pre><ul class=""><li id="2457" class="mc md it lb b lc ld lf lg li me lm mf lq mg lu mh mi mj mk bi translated"><strong class="lb iu">适合时间:</strong> 1.112</li><li id="6c01" class="mc md it lb b lc ml lf mm li mn lm mo lq mp lu mh mi mj mk bi translated">预测时间: 0.006</li><li id="d4c5" class="mc md it lb b lc ml lf mm li mn lm mo lq mp lu mh mi mj mk bi translated"><strong class="lb iu">精度:</strong> 0.812</li><li id="140f" class="mc md it lb b lc ml lf mm li mn lm mo lq mp lu mh mi mj mk bi translated"><strong class="lb iu">召回:</strong> 0.703</li><li id="90d4" class="mc md it lb b lc ml lf mm li mn lm mo lq mp lu mh mi mj mk bi translated"><strong class="lb iu">精度:</strong> 0.81</li></ul><pre class="kj kk kl km gt pe pf pg ph aw pi bi"><span id="b2d4" class="on nq it pf b gy pj pk l pl pm"># Confusion matrix for GradientBoostingClassifier<br/>matrix = confusion_matrix(Y_test, Y_pred)<br/>sns.heatmap(matrix, annot = True, fmt = 'd')</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pr"><img src="../Images/821eae55d142a9e0af54efad929146b3.png" data-original-src="https://miro.medium.com/v2/resize:fit:868/format:webp/1*dmFQXJFkGP-kHNNpXtT5Qg.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图片由作者提供:渐变增强的混淆矩阵</p></figure><p id="e18c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这里，我们观察到，与随机森林相比，梯度增强具有更长的拟合时间，但是预测时间要短得多。</p><p id="6ddd" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">同样，这与我们最初的预期一致，因为训练是在梯度增强下迭代完成的，这解释了更长的拟合时间。但是，一旦模型准备就绪，与随机森林相比，梯度增强进行预测所需的时间要短得多。</p></div><div class="ab cl ni nj hx nk" role="separator"><span class="nl bw bk nm nn no"/><span class="nl bw bk nm nn no"/><span class="nl bw bk nm nn"/></div><div class="im in io ip iq"><p id="feae" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">概括一下，<strong class="lb iu">随机森林</strong>:</p><ul class=""><li id="21f3" class="mc md it lb b lc ld lf lg li me lm mf lq mg lu mh mi mj mk bi translated">创建独立、并行的决策树</li><li id="7d99" class="mc md it lb b lc ml lf mm li mn lm mo lq mp lu mh mi mj mk bi translated">使用一些深度决策树可以更好地工作</li><li id="8b41" class="mc md it lb b lc ml lf mm li mn lm mo lq mp lu mh mi mj mk bi translated">拟合时间短，但预测时间长</li></ul><p id="a549" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">相比之下，<strong class="lb iu">梯度增强</strong>:</p><ul class=""><li id="0c9d" class="mc md it lb b lc ld lf lg li me lm mf lq mg lu mh mi mj mk bi translated">以连续的方式构建树，其中每一棵树都在前一棵树所犯错误的基础上进行改进</li><li id="2852" class="mc md it lb b lc ml lf mm li mn lm mo lq mp lu mh mi mj mk bi translated">与多个浅层决策树一起使用效果更好</li><li id="069f" class="mc md it lb b lc ml lf mm li mn lm mo lq mp lu mh mi mj mk bi translated">拟合时间长，但预测时间短</li></ul><p id="bb4c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">感谢您的阅读。请随意查看我下面的其他文章！</p><div class="mq mr gp gr ms mt"><a href="https://medium.com/geekculture/70-data-science-interview-questions-you-need-to-know-before-your-next-technical-interview-ccfbd37a37b3" rel="noopener follow" target="_blank"><div class="mu ab fo"><div class="mv ab mw cl cj mx"><h2 class="bd iu gy z fp my fr fs mz fu fw is bi translated">下一次技术面试前你需要知道的70个数据科学面试问题</h2><div class="na l"><h3 class="bd b gy z fp my fr fs mz fu fw dk translated">概率和统计，机器学习算法，脑筋急转弯等等</h3></div><div class="nb l"><p class="bd b dl z fp my fr fs mz fu fw dk translated">medium.com</p></div></div><div class="nc l"><div class="ps l ne nf ng nc nh ks mt"/></div></div></a></div><div class="mq mr gp gr ms mt"><a href="https://chongjason.medium.com/i-finally-got-a-data-science-job-39f58774785" rel="noopener follow" target="_blank"><div class="mu ab fo"><div class="mv ab mw cl cj mx"><h2 class="bd iu gy z fp my fr fs mz fu fw is bi translated">我终于找到了一份数据科学的工作</h2><div class="na l"><h3 class="bd b gy z fp my fr fs mz fu fw dk translated">我在澳大利亚一家大型数据科学公司面试全职职位的经历</h3></div><div class="nb l"><p class="bd b dl z fp my fr fs mz fu fw dk translated">chongjason.medium.com</p></div></div><div class="nc l"><div class="pt l ne nf ng nc nh ks mt"/></div></div></a></div></div></div>    
</body>
</html>