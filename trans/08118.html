<html>
<head>
<title>20 Burning XGBoost FAQs Answered to Use the Library Like a Pro</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">20烧XGBoost常见问题解答像专业人士一样使用库</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/20-burning-xgboost-faqs-answered-to-use-the-library-like-a-pro-f8013b8df3e4?source=collection_archive---------8-----------------------#2021-07-26">https://towardsdatascience.com/20-burning-xgboost-faqs-answered-to-use-the-library-like-a-pro-f8013b8df3e4?source=collection_archive---------8-----------------------#2021-07-26</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="e609" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">梯度-通过学习这些重要的课程来提升你的XGBoost知识</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/124cb345f0532c8260da6d99b95fcd9c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*n6aa_ZbeL5c4O5vvKKJMVg.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd ky">照片由</strong><a class="ae kz" href="https://unsplash.com/@haithemfrd_off?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"><strong class="bd ky">hai them Ferdi</strong></a><strong class="bd ky">上</strong> <a class="ae kz" href="https://unsplash.com/s/photos/boost?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> <strong class="bd ky"> Unsplash。</strong> </a> <strong class="bd ky">除特别注明外，所有图片均为作者所有。</strong></p></figure><p id="9535" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">XGBoost是一个真正的野兽。</p><p id="9439" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">这是一个基于树的动力之马，是许多表格竞赛和数据马拉松获胜解决方案的幕后推手。目前，它是世界上“最性感”工作的“最热门”ML框架。</p><p id="f9b2" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">虽然使用XGBoost进行基本建模可能很简单，但是您需要掌握细节才能获得最佳性能。</p><p id="c02e" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">也就是说，我向你展示这篇文章，它是</p><ul class=""><li id="6495" class="lw lx it lc b ld le lg lh lj ly ln lz lr ma lv mb mc md me bi translated">数小时阅读文档(这并不有趣)</li><li id="86f8" class="lw lx it lc b ld mf lg mg lj mh ln mi lr mj lv mb mc md me bi translated">哭着通过一些可怕但有用的Kaggle内核</li><li id="4e9a" class="lw lx it lc b ld mf lg mg lj mh ln mi lr mj lv mb mc md me bi translated">数百个谷歌关键词搜索</li><li id="6aa3" class="lw lx it lc b ld mf lg mg lj mh ln mi lr mj lv mb mc md me bi translated">通过阅读大量文章，我完全耗尽了我的中等会员资格</li></ul><p id="6efa" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">这篇文章回答了关于XGBoost及其API的20个最棘手的问题。这些应该足够让你看起来像是一直在用库了。</p><div class="mk ml gp gr mm mn"><a href="https://ibexorigin.medium.com/membership" rel="noopener follow" target="_blank"><div class="mo ab fo"><div class="mp ab mq cl cj mr"><h2 class="bd iu gy z fp ms fr fs mt fu fw is bi translated">通过我的推荐链接加入Medium-BEXGBoost</h2><div class="mu l"><h3 class="bd b gy z fp ms fr fs mt fu fw dk translated">获得独家访问我的所有⚡premium⚡内容和所有媒体没有限制。支持我的工作，给我买一个…</h3></div><div class="mv l"><p class="bd b dl z fp ms fr fs mt fu fw dk translated">ibexorigin.medium.com</p></div></div><div class="mw l"><div class="mx l my mz na mw nb ks mn"/></div></div></a></div><p id="2225" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">获得由强大的AI-Alpha信号选择和总结的最佳和最新的ML和AI论文:</p><div class="mk ml gp gr mm mn"><a href="https://alphasignal.ai/?referrer=Bex" rel="noopener  ugc nofollow" target="_blank"><div class="mo ab fo"><div class="mp ab mq cl cj mr"><h2 class="bd iu gy z fp ms fr fs mt fu fw is bi translated">阿尔法信号|机器学习的极品。艾总结的。</h2><div class="mu l"><h3 class="bd b gy z fp ms fr fs mt fu fw dk translated">留在循环中，不用花无数时间浏览下一个突破；我们的算法识别…</h3></div><div class="mv l"><p class="bd b dl z fp ms fr fs mt fu fw dk translated">alphasignal.ai</p></div></div><div class="mw l"><div class="nc l my mz na mw nb ks mn"/></div></div></a></div></div><div class="ab cl nd ne hx nf" role="separator"><span class="ng bw bk nh ni nj"/><span class="ng bw bk nh ni nj"/><span class="ng bw bk nh ni"/></div><div class="im in io ip iq"><h1 id="6e71" class="nk nl it bd nm nn no np nq nr ns nt nu jz nv ka nw kc nx kd ny kf nz kg oa ob bi translated">1.应该选择哪个API——Scikit-learn还是核心学习API？</h1><p id="d59d" class="pw-post-body-paragraph la lb it lc b ld oc ju lf lg od jx li lj oe ll lm ln of lp lq lr og lt lu lv im bi translated">Python中的XGBoost有两个API——Scikit-learn兼容的(估计器有熟悉的<code class="fe oh oi oj ok b">fit/predict</code>模式)和核心的XGBoost-native API(用于分类和回归的全局<code class="fe oh oi oj ok b">train</code>函数)。</p><p id="a664" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">Python社区的大多数人，包括Kagglers和我自己，都使用Scikit-learn API。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ol om l"/></div></figure><p id="9748" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">这个API使您能够将XGBoost估算器集成到您熟悉的工作流中。好处包括(但不限于)</p><ul class=""><li id="abc3" class="lw lx it lc b ld le lg lh lj ly ln lz lr ma lv mb mc md me bi translated">能够将核心XGB算法传递到<a class="ae kz" rel="noopener" target="_blank" href="/how-to-use-sklearn-pipelines-for-ridiculously-neat-code-a61ab66ca90d?source=your_stories_page-------------------------------------"> Sklearn管道</a></li><li id="7380" class="lw lx it lc b ld mf lg mg lj mh ln mi lr mj lv mb mc md me bi translated">使用更高效的交叉验证工作流程</li><li id="b260" class="lw lx it lc b ld mf lg mg lj mh ln mi lr mj lv mb mc md me bi translated">避免学习新API带来的麻烦，等等。</li></ul></div><div class="ab cl nd ne hx nf" role="separator"><span class="ng bw bk nh ni nj"/><span class="ng bw bk nh ni nj"/><span class="ng bw bk nh ni"/></div><div class="im in io ip iq"><h1 id="759f" class="nk nl it bd nm nn no np nq nr ns nt nu jz nv ka nw kc nx kd ny kf nz kg oa ob bi translated">2.我如何完全控制XGBoost中的随机性？</h1><blockquote class="on oo op"><p id="8eb4" class="la lb oq lc b ld le ju lf lg lh jx li or lk ll lm os lo lp lq ot ls lt lu lv im bi translated">其余对XGBoost算法的引用主要是指与Sklearn兼容的XGBRegressor和XGBClassifier(或类似的)估计器。</p></blockquote><p id="ebc6" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">估计器有参数<code class="fe oh oi oj ok b">random_state</code>(替代的参数<code class="fe oh oi oj ok b">seed</code>已经被弃用，但仍然有效)。然而，使用默认参数运行XGBoost将会产生相同的结果，即使种子不同。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ol om l"/></div></figure><p id="abdd" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">这是因为XGBoost只有在使用了<code class="fe oh oi oj ok b">subsample</code>或任何其他以<code class="fe oh oi oj ok b">colsample_by*</code>前缀开头的参数时才会产生随机性。顾名思义，这些参数与<a class="ae kz" rel="noopener" target="_blank" href="/why-bootstrap-sampling-is-the-badass-tool-of-probabilistic-thinking-5d8c7343fb67?source=your_stories_page-------------------------------------">随机抽样</a>有很大关系。</p></div><div class="ab cl nd ne hx nf" role="separator"><span class="ng bw bk nh ni nj"/><span class="ng bw bk nh ni nj"/><span class="ng bw bk nh ni"/></div><div class="im in io ip iq"><h1 id="3bf9" class="nk nl it bd nm nn no np nq nr ns nt nu jz nv ka nw kc nx kd ny kf nz kg oa ob bi translated">3.XGBoost中的目标是什么，如何为不同的任务指定它们？</h1><p id="ca47" class="pw-post-body-paragraph la lb it lc b ld oc ju lf lg od jx li lj oe ll lm ln of lp lq lr og lt lu lv im bi translated">回归和分类任务有不同的类型。它们根据目标函数、它们可以处理的分布以及它们的损失函数而变化。</p><p id="bf14" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">您可以使用<code class="fe oh oi oj ok b">objective</code>参数在这些实现之间切换。它接受XGBoost提供的特殊代码字符串。</p><p id="1ba6" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">回归目标以<code class="fe oh oi oj ok b">reg:</code>为前缀，而分类以<code class="fe oh oi oj ok b">binary:</code>或<code class="fe oh oi oj ok b">multi:</code>开始。</p><p id="19fe" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">我将让您从本文档的第<a class="ae kz" href="https://xgboost.readthedocs.io/en/latest/parameter.html#learning-task-parameters" rel="noopener ugc nofollow" target="_blank">页</a>中探索完整的目标列表，因为有很多目标。</p><p id="063a" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">此外，指定正确的目标和度量可以消除在装配XGB分类器时出现的令人难以置信的恼人的警告。</p></div><div class="ab cl nd ne hx nf" role="separator"><span class="ng bw bk nh ni nj"/><span class="ng bw bk nh ni nj"/><span class="ng bw bk nh ni"/></div><div class="im in io ip iq"><h1 id="9415" class="nk nl it bd nm nn no np nq nr ns nt nu jz nv ka nw kc nx kd ny kf nz kg oa ob bi translated">4.XGBoost中应该用哪个booster——GB linear，gbtree，dart？</h1><blockquote class="on oo op"><p id="2009" class="la lb oq lc b ld le ju lf lg lh jx li or lk ll lm os lo lp lq ot ls lt lu lv im bi translated">XGBoost有3种类型的梯度增强学习器，它们是梯度增强(GB)线性函数、GB树和DART树。您可以使用<code class="fe oh oi oj ok b">booster</code>参数切换学习者。</p></blockquote><p id="4400" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">如果你问Kagglers，他们会在任何一天选择提升树而不是线性函数(我也是)。原因是树可以捕捉非线性的、复杂的关系，而线性函数不能。</p><p id="2fd4" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">所以，唯一的问题是您应该将哪个树助推器传递给<code class="fe oh oi oj ok b">booster</code>参数- <code class="fe oh oi oj ok b">gbtree</code>还是<code class="fe oh oi oj ok b">dart</code>？</p><p id="b322" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">我不会用这里的全部差异来烦你。你应该知道的是，XGBoost在与<code class="fe oh oi oj ok b">gbtree</code> booster一起使用时，使用了一组基于决策树的模型。</p><p id="de29" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">DART树是一种改进(有待验证),它引入了决策树子集的随机丢弃，以防止过度拟合。</p><p id="f905" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">在我用默认参数<code class="fe oh oi oj ok b">gbtree</code>和<code class="fe oh oi oj ok b">dart</code>做的几个小实验中，当我把<code class="fe oh oi oj ok b">rate_drop</code>设置在0.1和0.3之间时，我用<code class="fe oh oi oj ok b">dart</code>得到了稍微好一点的分数。</p><p id="3330" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">有关更多详细信息，我建议您参考XGB文档的<a class="ae kz" href="https://xgboost.readthedocs.io/en/latest/tutorials/dart.html" rel="noopener ugc nofollow" target="_blank">本页</a>，以了解细微差别和附加的超参数。</p></div><div class="ab cl nd ne hx nf" role="separator"><span class="ng bw bk nh ni nj"/><span class="ng bw bk nh ni nj"/><span class="ng bw bk nh ni"/></div><div class="im in io ip iq"><h1 id="3ecf" class="nk nl it bd nm nn no np nq nr ns nt nu jz nv ka nw kc nx kd ny kf nz kg oa ob bi translated">5.在XGBoost中应该使用哪种树方法？</h1><p id="5405" class="pw-post-body-paragraph la lb it lc b ld oc ju lf lg od jx li lj oe ll lm ln of lp lq lr og lt lu lv im bi translated">有5种控制树结构的算法。如果你在做分布式培训，你应该通过<code class="fe oh oi oj ok b">hist</code>到<code class="fe oh oi oj ok b">tree_method</code>。</p><p id="561f" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">对于其他场景，默认的(也是推荐的)是<code class="fe oh oi oj ok b">auto</code>，对于中小型数据集从<code class="fe oh oi oj ok b">exact</code>变为大型数据集的<code class="fe oh oi oj ok b">approx.</code>。</p></div><div class="ab cl nd ne hx nf" role="separator"><span class="ng bw bk nh ni nj"/><span class="ng bw bk nh ni nj"/><span class="ng bw bk nh ni"/></div><div class="im in io ip iq"><h1 id="1396" class="nk nl it bd nm nn no np nq nr ns nt nu jz nv ka nw kc nx kd ny kf nz kg oa ob bi translated">6.XGBoost中的助推轮是什么？</h1><p id="820c" class="pw-post-body-paragraph la lb it lc b ld oc ju lf lg od jx li lj oe ll lm ln of lp lq lr og lt lu lv im bi translated">正如我们所说的，XGBoost是梯度增强决策树的集合。集合中的每棵树被称为基础或弱学习器。弱学习者是任何比随机猜测表现稍好的算法。</p><p id="4037" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">通过组合多个弱学习者的预测，XGBoost产生了最终的、健壮的预测(现在跳过了很多细节)。</p><blockquote class="ou"><p id="c015" class="ov ow it bd ox oy oz pa pb pc pd lv dk translated">每次我们用一棵树来拟合数据，就叫做一轮提升。</p></blockquote><p id="205c" class="pw-post-body-paragraph la lb it lc b ld pe ju lf lg pf jx li lj pg ll lm ln ph lp lq lr pi lt lu lv im bi translated">因此，要指定要构建的树的数量，请将一个整数传递给学习API的<code class="fe oh oi oj ok b">num_boost_round</code>或Sklearn API的<code class="fe oh oi oj ok b">n_estimators</code>。</p><p id="5942" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">通常，太少的树会导致拟合不足，太多的树会导致拟合过度。您通常会使用超参数优化来调整该参数。</p></div><div class="ab cl nd ne hx nf" role="separator"><span class="ng bw bk nh ni nj"/><span class="ng bw bk nh ni nj"/><span class="ng bw bk nh ni"/></div><div class="im in io ip iq"><h1 id="5550" class="nk nl it bd nm nn no np nq nr ns nt nu jz nv ka nw kc nx kd ny kf nz kg oa ob bi translated">7.XGBoost中的<code class="fe oh oi oj ok b">early_stopping_rounds</code>是什么？</h1><p id="0531" class="pw-post-body-paragraph la lb it lc b ld oc ju lf lg od jx li lj oe ll lm ln of lp lq lr og lt lu lv im bi translated">从一轮提升到下一轮，XGBoost建立在最后一棵树的预测之上。</p><p id="50c2" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">如果预测在一系列回合后没有改善，那么停止训练是明智的，即使我们没有在<code class="fe oh oi oj ok b">num_boost_round</code>或<code class="fe oh oi oj ok b">n_estimators</code>中硬停下来。</p><p id="8eed" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">为了实现这一点，XGBoost提供了<code class="fe oh oi oj ok b">early_stopping_rounds</code>参数。例如，将其设置为50意味着如果预测在最后50轮中没有改善，我们就停止训练。</p><p id="543a" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">为<code class="fe oh oi oj ok b">n_estimators</code>设置一个较大的数值，并相应地改变提前停止以获得更好的结果，这是一个很好的做法。</p><p id="9298" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">在我展示如何用代码实现它的例子之前，还有两个其他的XGBoost参数要讨论。</p></div><div class="ab cl nd ne hx nf" role="separator"><span class="ng bw bk nh ni nj"/><span class="ng bw bk nh ni nj"/><span class="ng bw bk nh ni"/></div><div class="im in io ip iq"><h1 id="da83" class="nk nl it bd nm nn no np nq nr ns nt nu jz nv ka nw kc nx kd ny kf nz kg oa ob bi translated">8.XGBoost中的<code class="fe oh oi oj ok b">eval_set</code>有哪些？</h1><p id="834d" class="pw-post-body-paragraph la lb it lc b ld oc ju lf lg od jx li lj oe ll lm ln of lp lq lr og lt lu lv im bi translated">只有当您将一组评估数据传递给<code class="fe oh oi oj ok b">fit</code>方法时，才能启用提前停止。这些评估集用于跟踪从一轮到下一轮的整体表现。</p><p id="e4af" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">在每一轮通过的训练集上训练一棵树，并且为了查看分数是否一直在提高，它对通过的评估集进行预测。下面是它在代码中的样子:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ol om l"/></div></figure><blockquote class="on oo op"><p id="4a7c" class="la lb oq lc b ld le ju lf lg lh jx li or lk ll lm os lo lp lq ot ls lt lu lv im bi translated">将<code class="fe oh oi oj ok b">verbose</code>设置为False以删除日志消息。</p></blockquote><p id="a76d" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">第14次迭代后，分数开始下降。所以训练在第19次迭代时停止，因为应用了5轮早期停止。</p><p id="0b84" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">也可以将多个评估集作为一个元组传递给<code class="fe oh oi oj ok b">eval_set</code>，但是当与早期停止一起使用时，只有最后一对会被使用。</p><blockquote class="on oo op"><p id="658a" class="la lb oq lc b ld le ju lf lg lh jx li or lk ll lm os lo lp lq ot ls lt lu lv im bi translated">查看<a class="ae kz" href="https://machinelearningmastery.com/avoid-overfitting-by-early-stopping-with-xgboost-in-python/" rel="noopener ugc nofollow" target="_blank">这篇文章</a>以了解更多关于提前停止和评估集的信息。</p></blockquote></div><div class="ab cl nd ne hx nf" role="separator"><span class="ng bw bk nh ni nj"/><span class="ng bw bk nh ni nj"/><span class="ng bw bk nh ni"/></div><div class="im in io ip iq"><h1 id="046b" class="nk nl it bd nm nn no np nq nr ns nt nu jz nv ka nw kc nx kd ny kf nz kg oa ob bi translated">9.评估指标什么时候影响XGBoost？</h1><p id="3346" class="pw-post-body-paragraph la lb it lc b ld oc ju lf lg od jx li lj oe ll lm ln of lp lq lr og lt lu lv im bi translated">您可以使用<code class="fe oh oi oj ok b">fit</code>方法的<code class="fe oh oi oj ok b">eval_metric</code>指定各种评估指标。通过的指标只在内部产生影响——例如，它们用于评估早期停止期间的预测质量。</p><p id="b3d5" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">您应该根据您选择的目标来更改指标。您可以在文档的<a class="ae kz" href="https://xgboost.readthedocs.io/en/latest/parameter.html#learning-task-parameters" rel="noopener ugc nofollow" target="_blank">本页</a>中找到目标及其支持指标的完整列表。</p><p id="2608" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">以下是一个XGBoost分类器的示例，以多类对数损失和ROC AUC作为度量:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ol om l"/></div></figure><p id="d4ef" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">无论您传递给<code class="fe oh oi oj ok b">eval_metric</code>什么度量，它只影响<code class="fe oh oi oj ok b">fit</code>函数。所以，当你在分类器上调用<code class="fe oh oi oj ok b">score()</code>时，它仍然会产生<em class="oq">精度</em>，这是Sklearn中的默认。</p></div><div class="ab cl nd ne hx nf" role="separator"><span class="ng bw bk nh ni nj"/><span class="ng bw bk nh ni nj"/><span class="ng bw bk nh ni"/></div><div class="im in io ip iq"><h1 id="6ab3" class="nk nl it bd nm nn no np nq nr ns nt nu jz nv ka nw kc nx kd ny kf nz kg oa ob bi translated">10.XGBoost中的学习率(eta)是多少？</h1><p id="e553" class="pw-post-body-paragraph la lb it lc b ld oc ju lf lg od jx li lj oe ll lm ln of lp lq lr og lt lu lv im bi translated">每次XGBoost添加一个新的树到集成中，它被用来校正最后一组树的残差。</p><p id="208c" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">问题是这种方法快速而强大，使得算法能够快速学习和过度拟合训练数据。因此，XGBoost或任何其他梯度增强算法都有一个<code class="fe oh oi oj ok b">learning_rate</code>参数来控制拟合的速度，并防止过度拟合。</p><p id="4d00" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated"><code class="fe oh oi oj ok b">learning_rate</code>的典型值范围从0.1到0.3，但也可能超过这些值，特别是接近0。</p><p id="5252" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">无论传递给<code class="fe oh oi oj ok b">learning_rate</code>的值是什么，它都作为新树所做修正的加权因子。因此，较低的学习率意味着我们不太重视新树的修正，从而避免过度拟合。</p><p id="50fe" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">一个好的实践是为<code class="fe oh oi oj ok b">learning_rate</code>设置一个较低的数字，并使用具有较大数量估计器的早期停止(<code class="fe oh oi oj ok b">n_estimators</code>):</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ol om l"/></div></figure><p id="f801" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">您将立即看到慢速<code class="fe oh oi oj ok b">learning_rate</code>的效果，因为早期停止将在训练期间的更晚时间应用(在上述情况下，在第430次迭代之后)。</p><p id="1ff1" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">但是，每个数据集都是不同的，所以需要用超参数优化来调优这个参数。</p><blockquote class="on oo op"><p id="6593" class="la lb oq lc b ld le ju lf lg lh jx li or lk ll lm os lo lp lq ot ls lt lu lv im bi translated">查看<a class="ae kz" href="https://machinelearningmastery.com/tune-learning-rate-for-gradient-boosting-with-xgboost-in-python/" rel="noopener ugc nofollow" target="_blank">这篇关于如何调整学习率的文章</a>。</p></blockquote></div><div class="ab cl nd ne hx nf" role="separator"><span class="ng bw bk nh ni nj"/><span class="ng bw bk nh ni nj"/><span class="ng bw bk nh ni"/></div><div class="im in io ip iq"><h1 id="f44c" class="nk nl it bd nm nn no np nq nr ns nt nu jz nv ka nw kc nx kd ny kf nz kg oa ob bi translated">11.应该让XGBoost处理缺失值吗？</h1><p id="2d07" class="pw-post-body-paragraph la lb it lc b ld oc ju lf lg od jx li lj oe ll lm ln of lp lq lr og lt lu lv im bi translated">为此，我将给出我从两位不同的围棋比赛大师那里得到的建议。</p><ol class=""><li id="696a" class="lw lx it lc b ld le lg lh lj ly ln lz lr ma lv pj mc md me bi translated">如果您将<code class="fe oh oi oj ok b">np.nan</code>赋予基于树的模型，那么，在每个节点分裂时，丢失的值要么被发送到节点的左边子节点，要么被发送到节点的右边子节点，这取决于哪一个是最好的。因此，在每次分割时，缺失值会得到特殊处理，这可能会导致过度拟合。一个对树非常有效的简单解决方案是用不同于其他示例的值填充空值，比如-999。</li><li id="db6f" class="lw lx it lc b ld mf lg mg lj mh ln mi lr mj lv pj mc md me bi translated">尽管像XGBoost和LightGBM这样的包可以不经过预处理就处理空值，但是开发自己的插补策略总是一个好主意。</li></ol><p id="29f7" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">对于真实世界的数据集，您应该始终调查缺失的类型(MCAR、马尔、马尔)，并选择插补策略(基于值的[平均值、中值、众数]或基于模型的[KNN插补器或基于树的插补器])。</p><p id="8df2" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">如果您不熟悉这些术语，我在这里为您介绍了<a class="ae kz" rel="noopener" target="_blank" href="/going-beyond-the-simpleimputer-for-missing-data-imputation-dd8ba168d505?source=your_stories_page-------------------------------------"/>。</p></div><div class="ab cl nd ne hx nf" role="separator"><span class="ng bw bk nh ni nj"/><span class="ng bw bk nh ni nj"/><span class="ng bw bk nh ni"/></div><div class="im in io ip iq"><h1 id="b914" class="nk nl it bd nm nn no np nq nr ns nt nu jz nv ka nw kc nx kd ny kf nz kg oa ob bi translated">12.用XGBoost进行交叉验证的最好方法是什么？</h1><p id="1fe5" class="pw-post-body-paragraph la lb it lc b ld oc ju lf lg od jx li lj oe ll lm ln of lp lq lr og lt lu lv im bi translated">尽管XGBoost带有内置的CV支持，但请始终使用Sklearn CV拆分器。</p><p id="54e8" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">我说的Sklearn，并不是指<code class="fe oh oi oj ok b">cross_val_score</code>或者<code class="fe oh oi oj ok b">cross_validate</code>这样的基础效用函数。</p><p id="31b1" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">2021年没有人会交叉验证这种方式(至少在Kaggle上没有)。</p><p id="28fc" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">为CV过程提供更多灵活性和控制的方法是使用Sklearn CV分离器的<code class="fe oh oi oj ok b">.split</code>功能，并在<code class="fe oh oi oj ok b">for</code>循环中实现您自己的CV逻辑。</p><p id="ca13" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">下面是一个五重简历的代码:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ol om l"/></div></figure><p id="7095" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">在<code class="fe oh oi oj ok b">for</code>循环中执行CV可以使用评估集和提前停止，而像<code class="fe oh oi oj ok b">cross_val_score</code>这样的简单函数则不能。</p></div><div class="ab cl nd ne hx nf" role="separator"><span class="ng bw bk nh ni nj"/><span class="ng bw bk nh ni nj"/><span class="ng bw bk nh ni"/></div><div class="im in io ip iq"><h1 id="797a" class="nk nl it bd nm nn no np nq nr ns nt nu jz nv ka nw kc nx kd ny kf nz kg oa ob bi translated">13.如何在Sklearn管道中使用XGBoost？</h1><p id="4d53" class="pw-post-body-paragraph la lb it lc b ld oc ju lf lg od jx li lj oe ll lm ln of lp lq lr og lt lu lv im bi translated">如果使用Sklearn API，可以将XGBoost估计器作为管道的最后一步(就像其他Sklearn类一样):</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ol om l"/></div></figure><p id="6739" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">如果想在管道中使用XGBoost的<code class="fe oh oi oj ok b">fit</code>参数，可以很容易地将它们传递给管道的<code class="fe oh oi oj ok b">fit</code>方法。唯一的区别是你应该使用<code class="fe oh oi oj ok b">stepname__parameter</code>语法:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ol om l"/></div></figure><p id="7faf" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">因为我们在管道中将XGBoost步骤命名为<code class="fe oh oi oj ok b">clf</code>，所以每个<code class="fe oh oi oj ok b">fit</code>参数都应该以<code class="fe oh oi oj ok b">clf__</code>为前缀，这样管道才能正常工作。</p><blockquote class="on oo op"><p id="5852" class="la lb oq lc b ld le ju lf lg lh jx li or lk ll lm os lo lp lq ot ls lt lu lv im bi translated">此外，由于<code class="fe oh oi oj ok b">StandardScaler</code>删除了熊猫数据帧的列名，XGBoost不断抛出错误，因为<code class="fe oh oi oj ok b">eval_set</code>和训练数据不匹配。因此，我在两组中都使用了<code class="fe oh oi oj ok b">.values</code>来避免这种情况。</p></blockquote></div><div class="ab cl nd ne hx nf" role="separator"><span class="ng bw bk nh ni nj"/><span class="ng bw bk nh ni nj"/><span class="ng bw bk nh ni"/></div><div class="im in io ip iq"><h1 id="631a" class="nk nl it bd nm nn no np nq nr ns nt nu jz nv ka nw kc nx kd ny kf nz kg oa ob bi translated">14.如何显著提高默认分数？</h1><p id="d4f1" class="pw-post-body-paragraph la lb it lc b ld oc ju lf lg od jx li lj oe ll lm ln of lp lq lr og lt lu lv im bi translated">在使用默认的XGBoost设置建立了基本性能之后，您可以做些什么来显著提高分数呢？</p><p id="a4e3" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">许多人匆忙转向超参数调优，但它并不总是给你想要的分数带来巨大的飞跃。通常，参数优化带来的改进可能微不足道。</p><p id="e514" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">在实践中，任何实质性的分数增加主要来自于适当的特征工程和使用模型混合或堆叠等技术。</p><p id="a70b" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">你应该把大部分时间花在特性工程上——有效的FE来自于正确的EDA和对数据集的深刻理解。特别是，创建特定于领域的特性可能会对性能产生奇迹。</p><p id="b649" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">然后，尝试将多个模型组合成一个整体。在Kaggle上运行良好的方法是堆叠三大组件——XGBoost、CatBoost和LightGBM。</p></div><div class="ab cl nd ne hx nf" role="separator"><span class="ng bw bk nh ni nj"/><span class="ng bw bk nh ni nj"/><span class="ng bw bk nh ni"/></div><div class="im in io ip iq"><h1 id="1352" class="nk nl it bd nm nn no np nq nr ns nt nu jz nv ka nw kc nx kd ny kf nz kg oa ob bi translated">15.XGBoost中最重要的超参数有哪些？</h1><p id="1a45" class="pw-post-body-paragraph la lb it lc b ld oc ju lf lg od jx li lj oe ll lm ln of lp lq lr og lt lu lv im bi translated">超参数调整应该始终是项目工作流程的最后一步。</p><p id="152f" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">如果时间紧迫，应该优先调优XGBoost的超参数来控制过度拟合。这些是:</p><ul class=""><li id="6d35" class="lw lx it lc b ld le lg lh lj ly ln lz lr ma lv mb mc md me bi translated"><code class="fe oh oi oj ok b">n_estimators</code>:要训练的树木数量</li><li id="eb39" class="lw lx it lc b ld mf lg mg lj mh ln mi lr mj lv mb mc md me bi translated"><code class="fe oh oi oj ok b">learning_rate</code>:步进收缩或<code class="fe oh oi oj ok b">eta</code></li><li id="3560" class="lw lx it lc b ld mf lg mg lj mh ln mi lr mj lv mb mc md me bi translated"><code class="fe oh oi oj ok b">max_depth</code>:每棵树的深度</li><li id="9fad" class="lw lx it lc b ld mf lg mg lj mh ln mi lr mj lv mb mc md me bi translated"><code class="fe oh oi oj ok b">gamma</code>:复杂度控制-伪正则化参数</li><li id="5c0d" class="lw lx it lc b ld mf lg mg lj mh ln mi lr mj lv mb mc md me bi translated"><code class="fe oh oi oj ok b">min_child_weight</code>:控制树深度的另一个参数</li><li id="e502" class="lw lx it lc b ld mf lg mg lj mh ln mi lr mj lv mb mc md me bi translated"><code class="fe oh oi oj ok b">reg_alpha</code> : L1正则项(如LASSO回归)</li><li id="297c" class="lw lx it lc b ld mf lg mg lj mh ln mi lr mj lv mb mc md me bi translated"><code class="fe oh oi oj ok b">reg_lambda</code> : L2正则项(如在岭回归中)</li></ul></div><div class="ab cl nd ne hx nf" role="separator"><span class="ng bw bk nh ni nj"/><span class="ng bw bk nh ni nj"/><span class="ng bw bk nh ni"/></div><div class="im in io ip iq"><h1 id="f145" class="nk nl it bd nm nn no np nq nr ns nt nu jz nv ka nw kc nx kd ny kf nz kg oa ob bi translated">16.如何在XGBoost中调优max_depth？</h1><p id="7577" class="pw-post-body-paragraph la lb it lc b ld oc ju lf lg od jx li lj oe ll lm ln of lp lq lr og lt lu lv im bi translated"><code class="fe oh oi oj ok b">max_depth</code>是树的根节点和叶节点之间的最大长度。它是控制过拟合的最重要的参数之一。</p><p id="8173" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">典型值范围是3-10，但很少需要高于5-7。此外，使用更深的树会使XGBoost非常消耗内存。</p></div><div class="ab cl nd ne hx nf" role="separator"><span class="ng bw bk nh ni nj"/><span class="ng bw bk nh ni nj"/><span class="ng bw bk nh ni"/></div><div class="im in io ip iq"><h1 id="80aa" class="nk nl it bd nm nn no np nq nr ns nt nu jz nv ka nw kc nx kd ny kf nz kg oa ob bi translated">17.如何在XGBoost中调min_child_weight？</h1><p id="2e71" class="pw-post-body-paragraph la lb it lc b ld oc ju lf lg od jx li lj oe ll lm ln of lp lq lr og lt lu lv im bi translated"><code class="fe oh oi oj ok b">min_child_weight</code>控制创建新节点时数据中所有样本的权重之和。当该值较小时，每个节点将在每个节点中分组越来越少的样本。</p><p id="6a94" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">如果它足够小，树将很可能过度适应训练数据中的特性。因此，请为该参数设置一个较高的值，以避免过度拟合。</p><p id="b899" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">默认值为1，其值仅限于定型数据中的行数。然而，一个很好的调优范围是2–10或高达20。</p></div><div class="ab cl nd ne hx nf" role="separator"><span class="ng bw bk nh ni nj"/><span class="ng bw bk nh ni nj"/><span class="ng bw bk nh ni"/></div><div class="im in io ip iq"><h1 id="7e68" class="nk nl it bd nm nn no np nq nr ns nt nu jz nv ka nw kc nx kd ny kf nz kg oa ob bi translated">18.如何在XGBoost中调gamma？</h1><p id="bc05" class="pw-post-body-paragraph la lb it lc b ld oc ju lf lg od jx li lj oe ll lm ln of lp lq lr og lt lu lv im bi translated">一个更有挑战性的参数是<code class="fe oh oi oj ok b">gamma</code>，对于我这样的外行来说，你可以把它看成是模型的复杂度控制。伽玛越高，应用的正则化越多。</p><p id="f35e" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">它的范围从0到无穷大——因此，调整它可能很困难。此外，它高度依赖于数据集和其他超参数。这意味着一个模型可以有多个最佳伽马值。</p><p id="cb1d" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">通常，您可以在0–20之间找到最佳的伽玛。</p></div><div class="ab cl nd ne hx nf" role="separator"><span class="ng bw bk nh ni nj"/><span class="ng bw bk nh ni nj"/><span class="ng bw bk nh ni"/></div><div class="im in io ip iq"><h1 id="68a9" class="nk nl it bd nm nn no np nq nr ns nt nu jz nv ka nw kc nx kd ny kf nz kg oa ob bi translated">19.XGBoost中reg_alpha和reg_lambda如何调优？</h1><p id="7bdb" class="pw-post-body-paragraph la lb it lc b ld oc ju lf lg od jx li lj oe ll lm ln of lp lq lr og lt lu lv im bi translated">这些参数指的是特征权重的正则化强度。换句话说，增加它们将使算法更保守，因为对具有低系数(或权重)的特征不太重视。</p><p id="80db" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated"><code class="fe oh oi oj ok b">reg_alpha</code>指Lasso回归的L1正则化，<code class="fe oh oi oj ok b">reg_lambda</code>指岭回归。</p><p id="66fc" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">调整它们可能是一个真正的挑战，因为它们的值也可以从0到无穷大。</p><p id="0509" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">首先，选择一个较宽的区间，如[1e5，1e2，0.01，10，100]。然后，根据从该范围返回的最佳值，选择其他几个附近的值。</p><div class="mk ml gp gr mm mn"><a rel="noopener follow" target="_blank" href="/intro-to-regularization-with-ridge-and-lasso-regression-with-sklearn-edcf4c117b7a"><div class="mo ab fo"><div class="mp ab mq cl cj mr"><h2 class="bd iu gy z fp ms fr fs mt fu fw is bi translated">介绍使用Sklearn的岭和套索回归正则化</h2><div class="mu l"><h3 class="bd b gy z fp ms fr fs mt fu fw dk translated">你还不如放弃线性回归</h3></div><div class="mv l"><p class="bd b dl z fp ms fr fs mt fu fw dk translated">towardsdatascience.com</p></div></div><div class="mw l"><div class="pk l my mz na mw nb ks mn"/></div></div></a></div></div><div class="ab cl nd ne hx nf" role="separator"><span class="ng bw bk nh ni nj"/><span class="ng bw bk nh ni nj"/><span class="ng bw bk nh ni"/></div><div class="im in io ip iq"><h1 id="c7df" class="nk nl it bd nm nn no np nq nr ns nt nu jz nv ka nw kc nx kd ny kf nz kg oa ob bi translated">20.如何调优XGBoost中的随机采样超参数？</h1><p id="78a9" class="pw-post-body-paragraph la lb it lc b ld oc ju lf lg od jx li lj oe ll lm ln of lp lq lr og lt lu lv im bi translated">以上6个参数之后，强烈建议调优那些控制随机采样的。事实上，随机采样是算法中应用的另一种方法，以进一步防止过拟合。</p><ul class=""><li id="4ce7" class="lw lx it lc b ld le lg lh lj ly ln lz lr ma lv mb mc md me bi translated"><code class="fe oh oi oj ok b">subsample</code>:推荐值【0.5 - 0.9】。每个增强回合中随机抽样(不替换)的所有训练样本的比例。</li><li id="878d" class="lw lx it lc b ld mf lg mg lj mh ln mi lr mj lv mb mc md me bi translated"><code class="fe oh oi oj ok b">colsample_by*</code>:以此前缀开头的参数是指随机选择的列的比例</li><li id="a135" class="lw lx it lc b ld mf lg mg lj mh ln mi lr mj lv mb mc md me bi translated"><code class="fe oh oi oj ok b">colsample_bytree</code>:每一轮助推</li><li id="63e2" class="lw lx it lc b ld mf lg mg lj mh ln mi lr mj lv mb mc md me bi translated"><code class="fe oh oi oj ok b">colsample_bylevel</code>:树中达到的每个深度级别</li><li id="dc22" class="lw lx it lc b ld mf lg mg lj mh ln mi lr mj lv mb mc md me bi translated"><code class="fe oh oi oj ok b">colsample_bynode</code>:每个节点创建或在每个拆分</li></ul><p id="fe00" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">像<code class="fe oh oi oj ok b">subsample</code>，推荐范围是【0.5 - 0.9】。</p></div><div class="ab cl nd ne hx nf" role="separator"><span class="ng bw bk nh ni nj"/><span class="ng bw bk nh ni nj"/><span class="ng bw bk nh ni"/></div><div class="im in io ip iq"><h1 id="de8f" class="nk nl it bd nm nn no np nq nr ns nt nu jz nv ka nw kc nx kd ny kf nz kg oa ob bi translated">摘要</h1><p id="9a9f" class="pw-post-body-paragraph la lb it lc b ld oc ju lf lg od jx li lj oe ll lm ln of lp lq lr og lt lu lv im bi translated">终于，这篇痛苦漫长但希望有用的文章结束了。</p><p id="c791" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">我们已经介绍了很多——如何为任务选择正确的API、正确的目标和指标、<code class="fe oh oi oj ok b">fit</code>最重要的参数，以及从不断更新的来源(如Kaggle)收集的一些有价值的XGBoost最佳实践。</p><p id="178b" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">如果你对XGBoost有更多的问题，请在评论中发表。在StackExchange网站上，我会试着比其他人回答得更快(在写这篇文章的时候，我还没有得到这个恩惠😉).</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pl"><img src="../Images/f91b8a21f12d3c3b7d428e0f8d5f48be.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*KeMS7gxVGsgx8KC36rSTcg.gif"/></div></div></figure></div><div class="ab cl nd ne hx nf" role="separator"><span class="ng bw bk nh ni nj"/><span class="ng bw bk nh ni nj"/><span class="ng bw bk nh ni"/></div><div class="im in io ip iq"><p id="7dfb" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">这里有些你可能会感兴趣的东西…</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><a href="https://towardsdatascience.com/love-3blue1brown-animations-learn-how-to-create-your-own-in-python-in-10-minutes-8e0430cf3a6d"><div class="gh gi pm"><img src="../Images/735b0439c5891f3beeb750ed5f4fa895.png" data-original-src="https://miro.medium.com/v2/resize:fit:1398/format:webp/1*fy66XTy9i3DtrwRu0C4GQQ.png"/></div></a></figure></div><div class="ab cl nd ne hx nf" role="separator"><span class="ng bw bk nh ni nj"/><span class="ng bw bk nh ni nj"/><span class="ng bw bk nh ni"/></div><div class="im in io ip iq"><h2 id="e05d" class="pn nl it bd nm po pp dn nq pq pr dp nu lj ps pt nw ln pu pv ny lr pw px oa py bi translated">特别感谢这些帖子:</h2><ul class=""><li id="2584" class="lw lx it lc b ld oc lg od lj pz ln qa lr qb lv mb mc md me bi translated"><a class="ae kz" href="https://www.kaggle.com/prashant111/a-guide-on-xgboost-hyperparameters-tuning/notebook" rel="noopener ugc nofollow" target="_blank">XGBoost超参数指南</a></li><li id="7e7d" class="lw lx it lc b ld mf lg mg lj mh ln mi lr mj lv mb mc md me bi translated"><a class="ae kz" rel="noopener" target="_blank" href="/mastering-xgboost-2eb6bce6bc76">掌握XGBoost </a></li><li id="d24c" class="lw lx it lc b ld mf lg mg lj mh ln mi lr mj lv mb mc md me bi translated"><a class="ae kz" href="https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/" rel="noopener ugc nofollow" target="_blank">使用Python代码在XGBoost中调整参数的完整指南</a></li></ul></div></div>    
</body>
</html>