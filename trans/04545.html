<html>
<head>
<title>Extract keywords from documents, an unsupervised solution</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">从文档中提取关键词，一种无监督的解决方案</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/extract-keywords-from-documents-unsupervised-d6474ed38179?source=collection_archive---------11-----------------------#2021-04-19">https://towardsdatascience.com/extract-keywords-from-documents-unsupervised-d6474ed38179?source=collection_archive---------11-----------------------#2021-04-19</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="de72" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">一种从文档中自动提取关键词的解决方案。用Python实现了NLTK和Scikit-learn。</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/e72f17eec032f2acfa45180ce7890dfb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nh3nhS1Q-qOJYgJuJEORaA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图片由安德鲁·朱提供，路透社旧新闻</p></figure><p id="507c" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">假设您手中有数百万(也许数十亿)的文本文档。无论是客户支持单、社交媒体数据还是社区论坛帖子。数据生成时没有标签。你正在绞尽脑汁给那些随机的文档添加标签。</p><p id="b7b0" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">手动标记是不切实际的；给出一个现有的标签列表很快就会过时。雇佣一个供应商公司来做标记工作是非常昂贵的。</p><p id="468b" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">你可能会说，为什么不用机器学习呢？比如，网络深度学习。但是，神经网络首先需要一些训练数据。适合您数据集的训练数据。</p><p id="7af8" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">那么，有没有一个解决方案可以让我们给文档加标签满足:</p><ol class=""><li id="bbe3" class="lu lv it la b lb lc le lf lh lw ll lx lp ly lt lz ma mb mc bi translated">不需要预先请求训练数据。</li><li id="51f9" class="lu lv it la b lb md le me lh mf ll mg lp mh lt lz ma mb mc bi translated">人工干预最小，可自动运行。</li><li id="d129" class="lu lv it la b lb md le me lh mf ll mg lp mh lt lz ma mb mc bi translated">自动捕捉新单词和短语。</li></ol><p id="7f98" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这篇文章记录了我是如何用Python提取关键字的，它是如何工作的，walkarounds。</p><p id="5c6f" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">注意，本文中的代码是在Jupyter Notebook中运行和测试的。如果您运行一个代码块，但是受到缺少导入包错误的欢迎，那么这个包一定已经在前面的某个地方被导入了。</p><h2 id="9edd" class="mi mj it bd mk ml mm dn mn mo mp dp mq lh mr ms mt ll mu mv mw lp mx my mz na bi translated">核心理念</h2><p id="95b5" class="pw-post-body-paragraph ky kz it la b lb nb ju ld le nc jx lg lh nd lj lk ll ne ln lo lp nf lr ls lt im bi translated">TF-IDF是一种广泛使用的算法，用于评估一个单词与文档集合中的一个文档的相关程度。</p><p id="6aa4" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在我的上一篇文章:<a class="ae ng" rel="noopener" target="_blank" href="/measure-text-weight-using-tf-idf-in-python-plain-code-and-scikit-learn-50cb1e4375ad">在Python和scikit-learn中使用TF-IDF测量文本权重</a>中，我使用了一个简单的示例来展示如何计算文档中所有单词的TF-IDF值。纯Python代码和使用scikit-learn包。</p><p id="cf90" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">基于TF-IDF，那些唯一且重要的词在某个文档中应该有很高的TF-IDF值。因此，理论上，我们应该能够利用文本权重#来提取文档中最重要的单词。</p><p id="da24" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">例如，一个谈论scikit-learn的文档应该包括更高密度的关键字<strong class="la iu"> scikit-learn </strong>，而另一个谈论“熊猫”的文档应该具有针对<strong class="la iu">熊猫</strong>的高TF-IDF值。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nh"><img src="../Images/9a496d92107ebd7ade119d27752c1a61.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DjRnDPEbhmAziQmx_MFAWA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">从文档语料库中提取关键词的步骤</p></figure><h2 id="01b3" class="mi mj it bd mk ml mm dn mn mo mp dp mq lh mr ms mt ll mu mv mw lp mx my mz na bi translated">目标文档</h2><p id="1b58" class="pw-post-body-paragraph ky kz it la b lb nb ju ld le nc jx lg lh nd lj lk ll ne ln lo lp nf lr ls lt im bi translated">因为我不能在这里使用我的日常工作数据库，也要确保你可以在你的本地机器上用最少的努力执行关键字提取示例代码。我发现来自<a class="ae ng" href="https://www.nltk.org/" rel="noopener ugc nofollow" target="_blank"> NLTK </a>的路透社文档语料库是一个很好的关键词提取目标。</p><p id="73e6" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">如果您不熟悉NLTK语料库，这篇文章可能有助于在不到一个小时的时间内开始使用NLTK:<a class="ae ng" rel="noopener" target="_blank" href="/book-writing-pattern-analysis-625f7c47c9ad">书籍写作模式分析—使用一个用例</a>开始使用NLTK和Python文本分析。</p><p id="8809" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">下载路透社文集。运行Python代码:</p><pre class="kj kk kl km gt ni nj nk nl aw nm bi"><span id="3859" class="mi mj it nj b gy nn no l np nq">import nltk<br/>nltk.download("reuters")</span></pre><p id="7c52" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">列出我们刚刚下载的语料库中的所有文档id。</p><pre class="kj kk kl km gt ni nj nk nl aw nm bi"><span id="9e0a" class="mi mj it nj b gy nn no l np nq">from nltk.corpus import reuters<br/>reuters.fileids()</span></pre><p id="5b70" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">检查一个文档的内容及其类别。</p><pre class="kj kk kl km gt ni nj nk nl aw nm bi"><span id="6050" class="mi mj it nj b gy nn no l np nq">fileid = reuters.fileids()[202]<br/>print(fileid,"\n"<br/>      ,reuters.raw(fileid),"\n"<br/>      ,reuters.categories(fileid),"\n")</span></pre><p id="ca16" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">路透社的语料库是由重叠的类别组织的。我们还可以通过类别名称获取文档。关于完整的NLTK语料库操作，请查看这篇精彩的文章:<a class="ae ng" href="https://www.nltk.org/book/ch02.html#fig-inaugural2" rel="noopener ugc nofollow" target="_blank">访问文本语料库和词汇资源</a></p><h2 id="2dd6" class="mi mj it bd mk ml mm dn mn mo mp dp mq lh mr ms mt ll mu mv mw lp mx my mz na bi translated">建立忽略单词列表</h2><p id="701c" class="pw-post-body-paragraph ky kz it la b lb nb ju ld le nc jx lg lh nd lj lk ll ne ln lo lp nf lr ls lt im bi translated">为了节省时间和计算资源，我们最好排除停用词，如“am”、“I”、“should”。NLTK提供了一个很好的英语停用词表。</p><pre class="kj kk kl km gt ni nj nk nl aw nm bi"><span id="a831" class="mi mj it nj b gy nn no l np nq">from nltk.corpus import stopwords<br/>ignored_words = list(stopwords.words('english'))</span></pre><p id="027b" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">您还可以使用NLTK停用词列表中没有的停用词来扩展该列表。</p><pre class="kj kk kl km gt ni nj nk nl aw nm bi"><span id="32d7" class="mi mj it nj b gy nn no l np nq">ignored_words.extend(<br/>'''get see seeing seems back join <br/>excludes has have other that are likely like <br/>due since next 100 take based high day set ago still <br/>however long early much help sees would will say says said <br/>applying apply remark explain explaining<br/>'''.split())</span></pre><h2 id="2c59" class="mi mj it bd mk ml mm dn mn mo mp dp mq lh mr ms mt ll mu mv mw lp mx my mz na bi translated">构建关键词词汇—单个单词</h2><p id="f5c8" class="pw-post-body-paragraph ky kz it la b lb nb ju ld le nc jx lg lh nd lj lk ll ne ln lo lp nf lr ls lt im bi translated">在使用TF-IDF提取关键字之前，我将构建自己的词汇表，包括单个单词(例如“Python”)和两个单词(例如“white house”)。</p><p id="ced1" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这里我将使用scikit-learn中的<code class="fe nr ns nt nj b">CountVectorizer</code>来执行单个单词的提取工作。</p><pre class="kj kk kl km gt ni nj nk nl aw nm bi"><span id="722e" class="mi mj it nj b gy nn no l np nq">from sklearn.feature_extraction.text import CountVectorizer<br/>import pandas as pd<br/>count_vec = CountVectorizer(<br/>    ngram_range = (1,1)   <strong class="nj iu">#1</strong><br/>    ,stop_words = ignored_words<br/>)<br/>text_set     = [reuters.raw(fileid).lower() for fileid in reuters.fileids()] <strong class="nj iu">#2</strong><br/>tf_result    = count_vec.fit_transform(text_set)<br/>tf_result_df = pd.DataFrame(tf_result.toarray()<br/>                               ,columns=count_vec.get_feature_names()) <strong class="nj iu">#3</strong><br/>the_sum_s = tf_result_df.sum(axis=0) <strong class="nj iu">#4</strong><br/>the_sum_df = pd.DataFrame({ <strong class="nj iu">#5</strong><br/>    'keyword':the_sum_s.index<br/>    ,'tf_sum':the_sum_s.values<br/>})<br/>the_sum_df = the_sum_df[<br/>    the_sum_df['tf_sum']&gt;2  <strong class="nj iu">#6</strong><br/>].sort_values(by=['tf_sum'],ascending=False)</span></pre><p id="bc60" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">代码<strong class="la iu"> #1 </strong>，指定CountVectorizer将只计算单个单词。又名，1克字。你可能会问，为什么不使用<code class="fe nr ns nt nj b">ngram_range = (1,2)</code>然后同时获得单个和双元单词呢？这是因为在这里捕获bigram将得到类似于<code class="fe nr ns nt nj b">"they are"</code>、<code class="fe nr ns nt nj b">"I will"</code>和<code class="fe nr ns nt nj b">"will be</code>的短语。这些是连接短语，通常不是文档的关键字或关键短语。</p><p id="c079" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">另一个原因是为了节省内存资源，由于组合太多，在这个阶段捕获二元短语将使用大量内存。</p><p id="3876" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">代码<strong class="la iu"> #2 </strong>，使用Python理解将所有路透社文章放在一行代码中。</p><p id="2518" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">代码<strong class="la iu"> #3 </strong>，将计数向量结果转换为可读的Pandas数据帧。</p><p id="14b4" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">代码<strong class="la iu"> #4 </strong>，产生一系列包括关键字及其在语料库中总出现次数的列表。</p><p id="37fb" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">代码<strong class="la iu"> #5 </strong>，将系列转换为数据帧，以便于阅读和数据操作。</p><p id="d00a" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">代码<strong class="la iu"> #6 </strong>，取只出现2次以上的词。</p><p id="c6e7" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">如果你偷看一下<code class="fe nr ns nt nj b">the_sum_df[:10]</code>设置的前10个结果，你会看到那些最常用的词:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/c6727c11135cb2ec163c35a7673dbc47.png" data-original-src="https://miro.medium.com/v2/resize:fit:704/format:webp/1*odmE7KGYQJoAelk-thhpnA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">最常用的10个词</p></figure><p id="5851" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">最常见但无意义的是，我们可以通过Python切片轻松地按比例排除这些:</p><pre class="kj kk kl km gt ni nj nk nl aw nm bi"><span id="adfc" class="mi mj it nj b gy nn no l np nq">start_index     = int(len(the_sum_df)*0.01) # exclude the top 1%<br/>my_word_df      = the_sum_df.iloc[start_index:]<br/>my_word_df      = my_word_df[my_word_df['keyword'].str.len()&gt;2]</span></pre><p id="1230" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">也删除少于2个字符的单词，如“vs”，“lt”等。</p><p id="c898" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">注意，我用的是<code class="fe nr ns nt nj b">.iloc</code>而不是<code class="fe nr ns nt nj b">.loc</code>。因为原始数据集是按TF(词频)值重新排序的。<code class="fe nr ns nt nj b">iloc</code>将切片放在索引的索引上(或索引标签的序列上)。但是<code class="fe nr ns nt nj b">loc</code>会在索引标签上切片。</p><h2 id="4074" class="mi mj it bd mk ml mm dn mn mo mp dp mq lh mr ms mt ll mu mv mw lp mx my mz na bi translated">构建关键词词汇—两个单词的短语(二元短语)</h2><p id="a9f6" class="pw-post-body-paragraph ky kz it la b lb nb ju ld le nc jx lg lh nd lj lk ll ne ln lo lp nf lr ls lt im bi translated">为了构建二元短语列表，我们不仅需要考虑一起出现的频率，还需要考虑它与相邻词的关系。</p><p id="71fc" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">例如短语<code class="fe nr ns nt nj b">they are</code>，多次出现在一起，但<code class="fe nr ns nt nj b">they are</code>只能跟随着有限的词，如<code class="fe nr ns nt nj b">they are brothers</code>、<code class="fe nr ns nt nj b">they are nice people</code>，这些词具有高的内部粘性，但低的外部连接灵活性。</p><p id="6090" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">外部连接灵活性通常可以用<a class="ae ng" href="https://www.youtube.com/watch?v=2s3aJfRr9gE&amp;ab_channel=KhanAcademyLabs" rel="noopener ugc nofollow" target="_blank">信息熵</a>来度量。熵值越高，表示与其他单词一起使用的可能性越高。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nv"><img src="../Images/f8c51da55acc35df6fe36ba89267daf4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6ec84-T0lZhopvFRa0jGsg.png"/></div></div></figure><p id="4ecb" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">对于我们的大脑来说，具有高内部粘性(计数频率)和高外部熵的短语，我们称之为“常用短语”，这些是我们想要添加到提取词汇中的内容。</p><p id="3bcd" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">NLTK为解决二元短语提取问题提供了类似的解决方案。</p><pre class="kj kk kl km gt ni nj nk nl aw nm bi"><span id="bff1" class="mi mj it nj b gy nn no l np nq">from nltk.collocations import BigramAssocMeasures<br/>from nltk.collocations import BigramCollocationFinder<br/>from nltk.tokenize import word_tokenize<br/>text_set_words  = [word_tokenize(reuters.raw(fileid).lower()) <br/>                   for fileid in reuters.fileids()] <strong class="nj iu">#1</strong><br/>bigram_measures = BigramAssocMeasures()<br/>finder = BigramCollocationFinder.from_documents(text_set_words) <strong class="nj iu">#2</strong><br/>finder.apply_freq_filter(3) <strong class="nj iu">#3</strong><br/>finder.apply_word_filter(lambda w: <br/>                         len(w) &lt; 3 <br/>                         or len(w) &gt; 15 <br/>                         or w.lower() in ignored_words) <strong class="nj iu">#4</strong><br/>phrase_result = finder.nbest(bigram_measures.pmi, 20000) <strong class="nj iu">#5</strong><br/>colloc_strings = [w1+' '+w2 for w1,w2 in phrase_result] <strong class="nj iu">#6</strong></span></pre><p id="afac" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">代码<strong class="la iu"> #1 </strong>，在这个Python理解表达式中，我使用<code class="fe nr ns nt nj b">word_tokenize</code>将文档标记为单词列表。输出将是这样的:</p><pre class="kj kk kl km gt ni nj nk nl aw nm bi"><span id="a7b7" class="mi mj it nj b gy nn no l np nq">[<br/>    ['word1','word2',...,'wordn'], <br/>    ['word1','word2',...,'wordn'],<br/>    ...<br/>    ['word1','word2',...,'wordn']<br/>]</span></pre><p id="5698" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">代码<strong class="la iu"> #2 </strong>，从标记化文档列表中启动bigram finder对象。还有另外一个功能<code class="fe nr ns nt nj b">from_words()</code>可以处理分词词表。</p><p id="15c9" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">代码<strong class="la iu"> #3 </strong>，删除频率小于3的候选项。</p><p id="b2ec" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">编码<strong class="la iu"> #4 </strong>，删除字长小于3或大于15的候选字。还有那些在<code class="fe nr ns nt nj b">ignored_words</code>列表里的。</p><p id="d647" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">代码<strong class="la iu"> #5 </strong>，使用<code class="fe nr ns nt nj b">BigramAssocMeasures</code>中的<code class="fe nr ns nt nj b">pmi</code>函数测量2个单词短语的可能性。你可以在<a class="ae ng" href="https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.121.2604&amp;rep=rep1&amp;type=pdf" rel="noopener ugc nofollow" target="_blank">静态自然语言处理基础</a>的第5.4节中找到它是如何工作的。和<a class="ae ng" href="https://tedboy.github.io/nlps/generated/generated/nltk.BigramAssocMeasures.html#methods" rel="noopener ugc nofollow" target="_blank">该链接</a>列出所有其他测量功能和源。</p><p id="1d20" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">编码<strong class="la iu"> #6 </strong>，将结果转换成可读性更好的格式。</p><p id="74cd" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">通过将<code class="fe nr ns nt nj b">BigramAssocMeasures</code>、<code class="fe nr ns nt nj b">BigramCollocationFinder</code>替换为<code class="fe nr ns nt nj b">TrigramAssocMeasures</code>和<code class="fe nr ns nt nj b">TrigramCollocationFinder</code>，您将获得3字短语提取器。在Reuters关键字提取示例中，我将跳过3个单词的短语。我在这里发布了示例代码，以备您需要。</p><pre class="kj kk kl km gt ni nj nk nl aw nm bi"><span id="1028" class="mi mj it nj b gy nn no l np nq">from nltk.collocations import TrigramAssocMeasures<br/>from nltk.collocations import TrigramCollocationFinder<br/>from nltk.tokenize import word_tokenize<br/>text_set_words  = [word_tokenize(reuters.raw(fileid).lower()) <br/>                   for fileid in reuters.fileids()]<br/>trigram_measures = TrigramAssocMeasures()<br/>finder = TrigramCollocationFinder.from_documents(text_set_words)<br/>finder.apply_freq_filter(3)<br/>finder.apply_word_filter(lambda w: <br/>                         len(w) &lt; 3 <br/>                         or len(w) &gt; 15 <br/>                         or w.lower() in ignored_words)<br/>tri_phrase_result = finder.nbest(bigram_measures.pmi, 1000)<br/>tri_colloc_strings = [<strong class="nj iu">w1+' '+w2+' '+w3 for w1,w2,w3</strong> in tri_phrase_result] <br/>tri_colloc_strings[:10]</span></pre><h2 id="5aff" class="mi mj it bd mk ml mm dn mn mo mp dp mq lh mr ms mt ll mu mv mw lp mx my mz na bi translated">激动人心的时刻，用TF-IDF测量关键词权重</h2><p id="e01e" class="pw-post-body-paragraph ky kz it la b lb nb ju ld le nc jx lg lh nd lj lk ll ne ln lo lp nf lr ls lt im bi translated">现在，让我们将单个单词和两个单词的短语结合在一起，构建路透社定制词汇列表。</p><pre class="kj kk kl km gt ni nj nk nl aw nm bi"><span id="2e7a" class="mi mj it nj b gy nn no l np nq">my_vocabulary = []<br/>my_vocabulary.extend(my_word_df['keyword'].tolist()) <br/>my_vocabulary.extend(colloc_strings)</span></pre><p id="39e8" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">让我们发动引擎。请注意，请找一台内存至少为16g的机器来运行代码。TF-IDF的计算需要一段时间，并且可能会占用您的大量内存。</p><pre class="kj kk kl km gt ni nj nk nl aw nm bi"><span id="0fa1" class="mi mj it nj b gy nn no l np nq">from sklearn.feature_extraction.text import TfidfVectorizer<br/>vec          = TfidfVectorizer(<br/>                    analyzer     ='word'<br/>                    ,ngram_range =(1, 2)<br/>                    ,vocabulary  =my_vocabulary)<br/>text_set     = [reuters.raw(fileid) for fileid in reuters.fileids()]<br/>tf_idf       = vec.fit_transform(text_set)<br/>result_tfidf = pd.DataFrame(tf_idf.toarray()<br/>                            , columns=vec.get_feature_names()) #1</span></pre><p id="a1ed" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在将结果集转换为代码#1中的Dateframe之后，<code class="fe nr ns nt nj b">result_tfidf</code>保存所有关键字的TF-IDF值:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nw"><img src="../Images/939cb4e15154b13f5eee593e36676e77.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Zzh1wb55vF3lH3aKikq_Gw.png"/></div></div></figure><h2 id="c279" class="mi mj it bd mk ml mm dn mn mo mp dp mq lh mr ms mt ll mu mv mw lp mx my mz na bi translated">看看结果</h2><p id="c1e4" class="pw-post-body-paragraph ky kz it la b lb nb ju ld le nc jx lg lh nd lj lk ll ne ln lo lp nf lr ls lt im bi translated">让我们检查其中一篇文章，并与上面的提取器提取的关键字进行比较，以验证有效性。</p><p id="1e0c" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">通过指定<code class="fe nr ns nt nj b">fileid</code>索引输出一个原始文件。</p><pre class="kj kk kl km gt ni nj nk nl aw nm bi"><span id="7d23" class="mi mj it nj b gy nn no l np nq">file_index= 202 # change number to check different articles<br/>fileid = reuters.fileids()[file_index]<br/>print(fileid,"\n"<br/>        ,reuters.raw(fileid),"\n"<br/>        ,reuters.categories(fileid),"\n")</span></pre><p id="0169" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">返回<strong class="la iu"> fileid </strong>，<strong class="la iu">原始内容</strong>，及其<strong class="la iu">类别</strong>。(嗯，很多年前，美国和日本打了一场关税战)</p><pre class="kj kk kl km gt ni nj nk nl aw nm bi"><span id="a494" class="mi mj it nj b gy nn no l np nq">test/15223 <br/> WHITE HOUSE SAYS JAPANESE TARRIFFS LIKELY<br/>  The White House said high U.S.<br/>  Tariffs on Japanese electronic goods would likely be imposed as<br/>  scheduled on April 17, despite an all-out effort by Japan to<br/>  avoid them.<br/>      Presidential spokesman Marlin Fitzwater made the remark one<br/>  day before U.S. And Japanese officials are to meet under the<br/>  emergency provisions of a July 1986 semiconductor pact to<br/>  discuss trade and the punitive tariffs.<br/>      Fitzwater said: "I would say Japan is applying the<br/>  full-court press...They certainly are putting both feet forward<br/>  in terms of explaining their position." But he added that "all<br/>  indications are they (the tariffs) will take effect."<br/><br/> ['trade']</span></pre><p id="76e4" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">从我们刚刚酝酿的<code class="fe nr ns nt nj b">result_tfidf</code> dataframe对象中打印出前10个关键字。</p><pre class="kj kk kl km gt ni nj nk nl aw nm bi"><span id="7fee" class="mi mj it nj b gy nn no l np nq">test_tfidf_row = result_tfidf.loc[file_index]<br/>keywords_df = pd.DataFrame({<br/>    'keyword':test_tfidf_row.index,<br/>    'tf-idf':test_tfidf_row.values<br/>})<br/>keywords_df = keywords_df[<br/>    keywords_df['tf-idf'] &gt;0<br/>].sort_values(by=['tf-idf'],ascending=False)<br/>keywords_df[:10]</span></pre><p id="6780" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">十大关键词:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/baca4114737e94f3a94b7ca5d1e14fb3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1216/format:webp/1*VtcOI0wMAXfm2wpWdksn9A.png"/></div></figure><p id="e9da" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">看起来这里的<code class="fe nr ns nt nj b">white</code>和<code class="fe nr ns nt nj b">house</code>与<code class="fe nr ns nt nj b">white house</code>是重复的。我们需要删除那些已经出现在两个单词短语中的单词。</p><pre class="kj kk kl km gt ni nj nk nl aw nm bi"><span id="f0fa" class="mi mj it nj b gy nn no l np nq">bigram_words = [item.split() <br/>                    for item in keywords_df['keyword'].tolist() <br/>                    if len(item.split())==2]<br/>bigram_words_set = set(subitem <br/>                        for item in bigram_words <br/>                        for subitem in item) <br/>keywords_df_new = keywords_df[<strong class="nj iu">~</strong>keywords_df['keyword'].<strong class="nj iu">isin</strong>(bigram_words_set)]</span></pre><p id="5441" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">上面的代码首先构建了一个单词<code class="fe nr ns nt nj b">set</code>，它包含了一个双单词短语中的单词。然后，通过<code class="fe nr ns nt nj b">~xxxx.isin(xxxx)</code>过滤掉已经在二字短语中使用的单字。</p><h2 id="5b7a" class="mi mj it bd mk ml mm dn mn mo mp dp mq lh mr ms mt ll mu mv mw lp mx my mz na bi translated">其他考虑</h2><p id="a8dd" class="pw-post-body-paragraph ky kz it la b lb nb ju ld le nc jx lg lh nd lj lk ll ne ln lo lp nf lr ls lt im bi translated">您拥有的文本语料库越大，TF-IDF在提取关键词方面的表现就越好。路透社的语料库包含10788篇文章，结果表明它是有效的。我相信这个解决方案对于更大的文本数据库会更好。</p><p id="f824" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">上面的代码在我的Macbook Air M1上运行不到2分钟，这意味着每日刷新结果集是可行的。</p><p id="f1e6" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">如果您有数百GB甚至TB大小的数据。您可能需要考虑用C/C++或Go重写逻辑，还可能利用GPU的能力来提高性能。</p><p id="2ae2" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">本文描述的解决方案远非完美，例如，我没有过滤掉动词和形容词。解决方案的主干可以扩展到其他语言。</p><h2 id="790e" class="mi mj it bd mk ml mm dn mn mo mp dp mq lh mr ms mt ll mu mv mw lp mx my mz na bi translated">提取的关键词</h2><p id="df6e" class="pw-post-body-paragraph ky kz it la b lb nb ju ld le nc jx lg lh nd lj lk ll ne ln lo lp nf lr ls lt im bi translated">让我们再次打印出最终结果。</p><pre class="kj kk kl km gt ni nj nk nl aw nm bi"><span id="0cd7" class="mi mj it nj b gy nn no l np nq">keywords_df_new[:10]</span></pre><p id="6180" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">获得最高的TF-IDF值，其余的关键字看起来很适合代表这篇路透社文章。目标达成！</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/a6bbc3b7dde83d6195d7223a9a4a2287.png" data-original-src="https://miro.medium.com/v2/resize:fit:972/format:webp/1*9EEunIWStYnI8s2RbTTh2Q.png"/></div></figure></div></div>    
</body>
</html>