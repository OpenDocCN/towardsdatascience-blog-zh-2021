<html>
<head>
<title>No code introduction to neural networks</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">神经网络无代码介绍</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/no-code-introduction-to-neural-networks-7b4187a8d100?source=collection_archive---------50-----------------------#2021-06-07">https://towardsdatascience.com/no-code-introduction-to-neural-networks-7b4187a8d100?source=collection_archive---------50-----------------------#2021-06-07</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="9921" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">简单的架构解释</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/3c13f9ecc98b930841e173f5d2159301.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*W_p1J2-V3LmoXuuV"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">美国宇航局在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>拍摄的照片</p></figure><p id="374b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">神经网络已经存在了很长一段时间，是在20世纪60年代作为一种模拟神经活动的方法为人工智能系统的开发而开发的。然而，自那时以来，它们已发展成为一种有用的分析工具，经常用来取代或结合标准的统计模型，如回归或分类，因为它们可以用来预测或更具体的产出。这方面的主要区别和优势在于，神经网络对数据背后的关系或分布形式不做任何初始假设，这意味着它们可以更加灵活，并捕捉输入和输出变量之间的非标准和非线性关系，使它们在当今数据丰富的环境中具有难以置信的价值。</p><p id="4f93" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在这个意义上，它们的使用已经持续了十年左右，随着成本的下降和一般计算能力的提高，大型数据集的兴起允许这些模型被训练，以及TensforFlow和Keras等框架的发展，这些框架允许人们拥有足够的硬件(在某些情况下，这甚至不再是云计算的要求)，正确的数据和对给定编码语言的理解来实现它们。因此，这篇文章试图提供一个没有代码的介绍，介绍它们的架构和工作原理，以便更好地理解它们的实现和好处。</p><p id="e237" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">首先，这些模型的工作方式是，有一个输入层，一个或多个隐藏层和一个输出层，每个层通过突触权重层连接。输入层(X)用于接收输入的缩放值，通常在0-1的标准化范围内。然后，隐藏层(Z)用于使用权重和激活函数来定义输入和输出之间的关系。然后，输出层(Y)将隐藏层的结果转换为预测值，预测值通常也在0–1范围内。连接这些层的突触权重(W)用于模型训练，以确定分配给每个输入和预测的权重，从而获得最佳模型拟合。视觉上，这表现为:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi lv"><img src="../Images/2ca1213a5a3a96af95e296eaa0bd75f2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_4Ve8vs7sleYw7by0x5ukw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">X =输入节点，Z =隐藏层节点，Y =输出层节点</p></figure><p id="f180" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">该结构意味着，到隐藏节点z𝒹的每个输入xᵢ乘以权重wⱼᵢ，然后与引入权重矩阵w₀ᵢ的附加偏差相加，该附加偏差形成进入隐藏节点的值。在上图中，这由从每个xᵢ和W₀𝒹到每个z𝒹.的价值流来表示</p><p id="db9c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然后，节点处的值通过非线性传递函数g进行变换，传递函数g通常采用sigmoid函数的形式，但也可以采用其他性能良好的形式(有界、单调递增和可微)，包括tanh和relu函数。这样做的目的是将非线性引入网络，这允许我们对数据中的非线性关系进行建模。</p><p id="f9f2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然后，根据输出节点的数量，并假设一个单独的隐藏层，来自隐藏节点的值乘以一个权重并求和，再加上应用于所有值的附加隐藏偏差，并通过输出传递函数将其转换为最终的输出估计值。该单层神经网络可以采取如下形式:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi lw"><img src="../Images/5b7b3eb7187eb4951d2f4b747b1e5a52.png" data-original-src="https://miro.medium.com/v2/resize:fit:742/format:webp/1*xRi4-wCUrFYr_pbOGXMpyQ.png"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi lx"><img src="../Images/5afa84f541213c11fe63d9c1c2dc0b7a.png" data-original-src="https://miro.medium.com/v2/resize:fit:914/format:webp/1*yBqfh3onPRyY1QzEs48qSA.png"/></div></figure><p id="3bc2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">其中非线性激活函数由sigmoid函数给出:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ly"><img src="../Images/2c12fc7df0c0bc23a7a3a749c747e379.png" data-original-src="https://miro.medium.com/v2/resize:fit:538/format:webp/1*stmcS4pqKdBhCwpUPByLtw.png"/></div></figure><p id="4c4e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在这种情况下，g是用作隐藏层传递函数(内部g)和输出单元传递函数(外部g)的相同函数，但是在某些情况下，它们可能不同，并且可能取决于模型的类型和期望的输出。此外，虽然我们已经通过额外的权重W₀将偏差添加到模型中，但是这些可以被吸收到权重矩阵中，使得这些额外的权重可以被丢弃，从而使得上面的等式更简单:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi lz"><img src="../Images/8178350aff7c04e830879c8deecc7a95.png" data-original-src="https://miro.medium.com/v2/resize:fit:916/format:webp/1*H4LBnyAk9bDHIfK0RQAZwQ.png"/></div></figure><p id="e7d7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">事实上，我们称之为单层神经网络是指单个隐藏层，但这通常也可以称为输入、隐藏和输出层的三层神经网络或单个隐藏层神经网络。惯例是将层数称为隐藏层数，因为输入和输出层始终是网络的一部分。</p><p id="0870" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">训练它们的方式是通过反向传播，由此调整隐藏层中的权重以反映由输出和目标value⁴.之间的关系确定的模型中的误差因此，这被称为反向传播，因为误差在每个时期(模型运行的周期数)之后通过网络反向传播。这种误差关系可以采取多种形式，包括二元交叉熵损失和均方误差(MSE)损失，并且由您正在使用的模型类型决定。例如，回归通常与均方误差损失相关，而分类通常与二元交叉Entropy⁵.相关后者可以表示为:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ma"><img src="../Images/6aafcac42103ebb77d220828565edd69.png" data-original-src="https://miro.medium.com/v2/resize:fit:858/format:webp/1*cdodmecMKAnyaaYXlKTwBg.png"/></div></figure><p id="ce13" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">其中e是模型的误差，tᵢ是目标，yᵢ是输出值。这通过模型中的权重矩阵转换回来，因为权重由以下各项更新:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mb"><img src="../Images/eb471f7576798e260e9afb9f1747b615.png" data-original-src="https://miro.medium.com/v2/resize:fit:584/format:webp/1*nMKMF7WqhrtVONIUoNiUrg.png"/></div></figure><p id="b1c8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">其中:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mc"><img src="../Images/fa14c6db4c79a82e0a545af8ae0eaf90.png" data-original-src="https://miro.medium.com/v2/resize:fit:518/format:webp/1*Bc9lAvY4As7Y37Kt6z2cvw.png"/></div></figure><p id="6834" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">权重的变化由学习率δ确定，δ是预先设置的常数，用于确定权重变化对模型内的误差有多敏感。进行这种调整，直到观察不到进一步的收敛，因此最小值是reached⁴.因此，设置学习率是模型的一个重要参数，如果调整太大，那么结果会不稳定地反弹，达不到最小值，而如果调整太小，那么模型会陷入局部最小值而不是全局最小值。在现实中，即使有正确的学习率，找到全局最小值也是很难的，这就是为什么经常用不同的起始值进行多次试运行。</p><p id="9a39" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">开发这些模型通常遵循三个主要阶段:选择合适的网络架构、网络学习和确定网络参数，以及测试model⁶.的推广虽然该过程的第一和第三部分由所使用的确切数据和该领域内的惯例来确定，但是第二部分通常由反复试验来确定，以选择正确的节点数量、要运行的时期数量和学习rate⁴.这包括关注在预期的参数范围内，哪组参数在测试或验证集上表现最好。例如，每个隐藏层中隐藏单元的数量通常通过找到产生最佳拟合的数量来设置。</p><p id="1a75" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这些模型的优势在于，它们可以应用于各种问题，以确定传统统计测试无法应用的关系，并且它们可以检查否则不会被发现的非线性关系。然而，这些是数据饥渴模型，通常需要数千甚至数百万个数据点来精确训练，并且通常被描述为黑盒模型，因为我们无法知道它们是如何得到最终结果的。因此，在决定选择哪种模式时，个人应该问questions⁷:几个关键问题</p><ol class=""><li id="8f14" class="md me it lb b lc ld lf lg li mf lm mg lq mh lu mi mj mk ml bi translated">准确性和可解释性的要求是什么</li><li id="d001" class="md me it lb b lc mm lf mn li mo lm mp lq mq lu mi mj mk ml bi translated">这个问题有什么先验知识</li><li id="8dfa" class="md me it lb b lc mm lf mn li mo lm mp lq mq lu mi mj mk ml bi translated">传统的统计模型可以足够精确地使用吗</li><li id="9a45" class="md me it lb b lc mm lf mn li mo lm mp lq mq lu mi mj mk ml bi translated">神经网络的设计和评估要求是什么</li></ol></div><div class="ab cl mr ms hx mt" role="separator"><span class="mu bw bk mv mw mx"/><span class="mu bw bk mv mw mx"/><span class="mu bw bk mv mw"/></div><div class="im in io ip iq"><p id="b276" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">参考文献</strong></p><p id="6fcf" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">[1]布莱克，w .，1995年。基于人工神经网络的空间交互建模。<em class="my">《运输地理杂志》，</em> 3卷3期，第159–166页。</p><p id="ac4d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">[2]费希尔和戈帕尔，s .，1994年。人工神经网络:区域间电信流建模的新方法。<em class="my">《区域科学杂志》，</em>第34卷第4期，第503–527页。</p><p id="ffeb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">[3]麻省理工深度学习入门:<a class="ae ky" href="https://www.youtube.com/watch?v=5tvmMX8r_OM&amp;list=PLtBw6njQRU-rwp5__7C0oIVt26ZgjG9NI&amp;index=2" rel="noopener ugc nofollow" target="_blank">https://www.youtube.com/watch?v=5tvmMX8r_OM&amp;list = PLT bw 6 njqru-rwp 5 _ _ 7c 0 oivt 26 zgjg 9 ni&amp;index = 2</a></p><p id="2d76" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">[4]h .切利克，2004年。用人工神经网络模拟货运分配。<em class="my">《运输地理杂志》，</em> 12卷2期，第141–148页。</p><p id="2c50" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">[5]<a class="ae ky" href="https://machinelearningmastery.com/loss-and-loss-functions-for-training-deep-learning-neural-networks/" rel="noopener ugc nofollow" target="_blank">https://machine learning mastery . com/loss-and-loss-functions-for-training-deep-learning-neural-networks/</a></p><p id="c41a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">[6]菲舍尔，硕士，2013年。<em class="my">神经空间相互作用模型:网络训练、模型复杂性和泛化性能。</em>何池敏市，ICCSA。</p><p id="bf98" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">[7]m . Karlaftis和e . Vlahogianni，2011年。交通研究中的统计方法与神经网络:差异、相似性和一些见解。<em class="my">运输研究C部分:新兴技术，</em> 19(3)，第387-399页。</p></div><div class="ab cl mr ms hx mt" role="separator"><span class="mu bw bk mv mw mx"/><span class="mu bw bk mv mw mx"/><span class="mu bw bk mv mw"/></div><div class="im in io ip iq"><div class="kj kk kl km gt mz"><a rel="noopener follow" target="_blank" href="/introduction-to-hierarchical-clustering-part-1-theory-linkage-and-affinity-e3b6a4817702"><div class="na ab fo"><div class="nb ab nc cl cj nd"><h2 class="bd iu gy z fp ne fr fs nf fu fw is bi translated">层次聚类简介(第1部分——理论、联系和相似性)</h2><div class="ng l"><h3 class="bd b gy z fp ne fr fs nf fu fw dk translated">什么是层次聚类、亲和度和关联度</h3></div><div class="nh l"><p class="bd b dl z fp ne fr fs nf fu fw dk translated">towardsdatascience.com</p></div></div><div class="ni l"><div class="nj l nk nl nm ni nn ks mz"/></div></div></a></div><div class="no np gp gr nq mz"><a rel="noopener follow" target="_blank" href="/introduction-to-random-forest-classifiers-9a3b8d8d3fa7"><div class="na ab fo"><div class="nb ab nc cl cj nd"><h2 class="bd iu gy z fp ne fr fs nf fu fw is bi translated">随机森林分类器简介</h2><div class="ng l"><h3 class="bd b gy z fp ne fr fs nf fu fw dk translated">预测NBA球员的位置——我们正在看到一个真正的“无位置”联盟吗？</h3></div><div class="nh l"><p class="bd b dl z fp ne fr fs nf fu fw dk translated">towardsdatascience.com</p></div></div><div class="ni l"><div class="nr l nk nl nm ni nn ks mz"/></div></div></a></div><div class="no np gp gr nq mz"><a rel="noopener follow" target="_blank" href="/introduction-to-decision-tree-classifiers-from-scikit-learn-32cd5d23f4d"><div class="na ab fo"><div class="nb ab nc cl cj nd"><h2 class="bd iu gy z fp ne fr fs nf fu fw is bi translated">scikit-learn决策树分类器简介</h2><div class="nh l"><p class="bd b dl z fp ne fr fs nf fu fw dk translated">towardsdatascience.com</p></div></div><div class="ni l"><div class="ns l nk nl nm ni nn ks mz"/></div></div></a></div></div></div>    
</body>
</html>