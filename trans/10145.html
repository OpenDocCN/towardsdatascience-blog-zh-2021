<html>
<head>
<title>Massive Pretraining for Bilingual Machine Translation</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">双语机器翻译的大量预训练</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/massive-pretraining-for-bilingual-machine-translation-3e26bfd85432?source=collection_archive---------16-----------------------#2021-09-25">https://towardsdatascience.com/massive-pretraining-for-bilingual-machine-translation-3e26bfd85432?source=collection_archive---------16-----------------------#2021-09-25</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="964f" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsdatascience.com/tagged/thoughts-and-theory" rel="noopener" target="_blank">思想和理论</a></h2><div class=""/><div class=""><h2 id="7954" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">mBART的引导之旅，这是一个编码器-解码器语言模型，打开了多语言序列到序列任务的有趣视角。</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/020129bb75725770865c575efb21e86a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*kHL0CplZwGcWILuQ"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">我不知道你怎么想，但这是我在阅读这些非常大的预训练模型时得到的感觉。<a class="ae lh" href="https://unsplash.com/@arjunken?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">阿琼肯</a>在<a class="ae lh" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="5f18" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">注意:这是三篇系列文章中的第一篇。<br/>第一部分:<strong class="lk jd">面向双语机器翻译的海量预训练</strong> <br/>第二部分:<a class="ae lh" rel="noopener" target="_blank" href="/mbart50-multilingual-fine-tuning-of-extensible-multilingual-pretraining-70a7305d4838"> mBART50:可扩展多语言预训练的多语言微调</a> <br/>第三部分:<a class="ae lh" rel="noopener" target="_blank" href="/multilingual-speech-translation-with-multi-phase-pretraining-305d642b8a66">多阶段预训练的多语言语音翻译</a></p></div><div class="ab cl me mf hx mg" role="separator"><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj"/></div><div class="im in io ip iq"><p id="779c" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">如果您在过去三年中从事过任何自然语言处理(NLP)任务，您肯定会注意到BERT或类似的大型预训练模型的广泛使用，作为对感兴趣的任务进行微调以实现出色结果的基础。</p><p id="3906" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">预先训练的模型允许人们用相对少的数据和训练时间在下游任务上实现高精度。通过大量的预训练，他们已经了解了自然语言的统计结构，并需要学习如何回答特定的任务。然而，由于它们庞大的体积，大多数人没有所需的资源来训练它们中的一个，并且不得不依赖于公开存在的模型。</p><p id="f4fb" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">尽管它们在NLP中广泛使用，但迄今为止，大量预训练的模型对机器翻译的影响相对较低。当然，也有过一些尝试，如<a class="ae lh" href="https://aclanthology.org/D19-5611.pdf" rel="noopener ugc nofollow" target="_blank">【1】</a><a class="ae lh" href="https://arxiv.org/abs/1901.07291" rel="noopener ugc nofollow" target="_blank">【2】</a><a class="ae lh" href="https://aclanthology.org/N19-1409.pdf" rel="noopener ugc nofollow" target="_blank">【3】</a>或<a class="ae lh" href="https://aclanthology.org/2020.aacl-srw.15.pdf" rel="noopener ugc nofollow" target="_blank">【4】</a>(针对文档级翻译)，但其影响有限，在实践中并未得到广泛部署。</p><p id="bab9" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我认为，这种有限影响的原因有三:</p><ol class=""><li id="0c8f" class="ml mm it lk b ll lm lo lp lr mn lv mo lz mp md mq mr ms mt bi translated">对于研究最多的语言对来说，机器翻译是一项资源非常丰富的任务。</li><li id="c915" class="ml mm it lk b ll mv lo mw lr mx lv my lz mz md mq mr ms mt bi translated">机器翻译模型遵循编码器-解码器结构，而预训练模型仅由编码器组成，因此需要一些适应来将它们用于机器翻译。</li><li id="548b" class="ml mm it lk b ll mv lo mw lr mx lv my lz mz md mq mr ms mt bi translated">这些模型非常大，并且它们在推断期间的计算时间对于编码器-解码器架构来说是不可行的。</li></ol><p id="0aac" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">mBART是一个针对大量多语言数据进行预处理的编码器-解码器模型，其目标是改变机器翻译和相关任务的游戏。</p><h1 id="e263" class="na nb it bd nc nd ne nf ng nh ni nj nk ki nl kj nm kl nn km no ko np kp nq nr bi translated">姆巴特:它是什么，怎么训练的？</h1><p id="7f59" class="pw-post-body-paragraph li lj it lk b ll ns kd ln lo nt kg lq lr nu lt lu lv nv lx ly lz nw mb mc md im bi translated">mBART [5]是一个基于Transformer[6]的编码器-解码器模型，它根据来自多种语言的单语数据进行预训练，以便在机器翻译任务中进行微调。在这篇论文中，我们正在研究[5]它在来自不同语系的25种欧洲和亚洲语言上进行训练，这些语言是从<a class="ae lh" href="https://commoncrawl.org" rel="noopener ugc nofollow" target="_blank"> common crawl </a> (CC25)中收集的。</p><p id="2dc3" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">训练目标是去噪损失。给定输入序列<strong class="lk jd"> <em class="mu"> X，</em></strong><strong class="lk jd"><em class="mu"/></strong>模型在源端接收由噪声函数g( <strong class="lk jd"> <em class="mu"> X </em> </strong>)生成的<strong class="lk jd"> <em class="mu"> X </em> </strong>的讹误版本作为输入。在目标端，目标是用自回归解码来重建原始序列。</p><p id="c575" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">噪声函数随机屏蔽连续区间中35%的输入标记。而且多个句子同时馈入输入，噪声函数也会对它们的顺序进行置换。多输入句子的预训练允许模型在文档级机器翻译上进行微调。</p><p id="a572" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">模型本身具有庞大的规模:编码器和解码器中有12层，模型维度为1024个单元和16个注意头，总共约680M个参数。与GPT-3相比不算什么，但它仍然需要大量的计算资源来训练！</p><p id="c7b3" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">训练使用256个Nvidia V100 GPUs进行500K步，这相当于2.5周的训练，尽管有巨大的计算能力和使用float16 precision进行更快的训练。不要在家里尝试这个！</p><h1 id="a8f2" class="na nb it bd nc nd ne nf ng nh ni nj nk ki nl kj nm kl nn km no ko np kp nq nr bi translated">微调</h1><p id="6a28" class="pw-post-body-paragraph li lj it lk b ll ns kd ln lo nt kg lq lr nu lt lu lv nv lx ly lz nw mb mc md im bi translated">作者在所有25种预训练语言的双语环境中微调了这个模型。英语总是这两种语言中的一种。此外，对句子级和文档级机器翻译都执行微调。这些模型与不同的基线进行比较，包括在较少语言(2或6)上预训练的mBART、单语BART和随机初始化。</p><p id="4a3b" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">语言对分为以下几类:</p><ul class=""><li id="87ef" class="ml mm it lk b ll lm lo lp lr mn lv mo lz mp md nx mr ms mt bi translated">●资源:&lt; 1M sentence pairs</li><li id="3a7e" class="ml mm it lk b ll mv lo mw lr mx lv my lz mz md nx mr ms mt bi translated">medium resources: &lt; 10M sentence pairs</li><li id="bf90" class="ml mm it lk b ll mv lo mw lr mx lv my lz mz md nx mr ms mt bi translated">high resources: &gt; 10M句对</li></ul><p id="ed1f" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">结果可以总结如下:</p><ul class=""><li id="db84" class="ml mm it lk b ll lm lo lp lr mn lv mo lz mp md nx mr ms mt bi translated">在低和中等资源语言对的BLEU分数方面有显著的提高。它区分了可用和不可用的系统，因为许多结果提高了10个以上的BLEU点。</li><li id="6713" class="ml mm it lk b ll mv lo mw lr mx lv my lz mz md nx mr ms mt bi translated">mBART对极低资源的语言对没有帮助(&lt;10k sentence pairs), but read the section about unsupervised learning to see how this was overcome.</li><li id="1253" class="ml mm it lk b ll mv lo mw lr mx lv my lz mz md nx mr ms mt bi translated">Marginal gains for high-resourced language pairs with little more than 10M sentence pairs. For the highest-resourced languages a decrement in BLEU score is observed.</li><li id="04cc" class="ml mm it lk b ll mv lo mw lr mx lv my lz mz md nx mr ms mt bi translated"><a class="ae lh" href="https://paperswithcode.com/method/bpe" rel="noopener ugc nofollow" target="_blank">回译</a> [7]提供了对mBart预训练的进一步改进。然而，在预训练期间，可能需要更大的目标语言单语数据才真正有益。</li><li id="e652" class="ml mm it lk b ll mv lo mw lr mx lv my lz mz md nx mr ms mt bi translated">当目标语言资源不足时，对多种语言进行预训练会更有帮助。当高资源可用时，更多的语言会降低性能。也许在这种情况下，模型达到了它的容量极限。</li></ul><p id="8cc4" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">对于资源非常丰富的语言对来说，预训练仍然没有用，但是对于其他语言来说，一些结果的改善是巨大的。并且单个预训练模型可以产生所有这些。</p><p id="f140" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">此外，我知道所有的目标语言对都使用相同的超参数集，所以仔细调优可能会得到更好的结果。显然，基线也是如此，但我不认为差距可以明显缩小。</p><p id="7d69" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">然后，作者让我们高兴地研究了mBART在微调过程中对不可见语言的影响。</p><h1 id="793d" class="na nb it bd nc nd ne nf ng nh ni nj nk ki nl kj nm kl nn km no ko np kp nq nr bi translated"><strong class="ak">对看不见的语言进行微调</strong></h1><p id="78b9" class="pw-post-body-paragraph li lj it lk b ll ns kd ln lo nt kg lq lr nu lt lu lv nv lx ly lz nw mb mc md im bi translated">在另一项实验中，作者在2或6种语言上训练mBART，然后在至少有一种语言在预训练中没有出现的语言对上进行微调。结果如下:</p><ul class=""><li id="007c" class="ml mm it lk b ll lm lo lp lr mn lv mo lz mp md nx mr ms mt bi translated">mBART预训练对看不见的语言也很有帮助<strong class="lk jd">。</strong>在预训练中有相似的语言是有益的，但不是必需的，并且在原始模型词汇中也没有必要有新语言的字母表。</li><li id="0583" class="ml mm it lk b ll mv lo mw lr mx lv my lz mz md nx mr ms mt bi translated">对两种看不见的语言进行微调会导致比一种看不见的语言更糟糕的结果。始终使用所有可用的数据！</li><li id="4628" class="ml mm it lk b ll mv lo mw lr mx lv my lz mz md nx mr ms mt bi translated">当看不见的语言在源端时，结果更糟。在源端进行归纳似乎比在目标端更困难。</li></ul><p id="ca6d" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">一般来说，编码器-解码器模型在编码器中学习目标侧的特征，而解码器的工作更容易(例如在<a class="ae lh" href="https://arxiv.org/abs/2006.10369" rel="noopener ugc nofollow" target="_blank">【8】</a>处)。这可以解释为什么看不见的源语言更难获得更好的结果。对于不同的任务和不同的输入方式，都观察到了这种行为。我个人认为，这是由于注意力模型使得编码器在反向传播过程中可以从目标端接收到很多信息。然而，我不能在这里形成一个理论，我不知道这种现象是否被详细研究过。如果不是，它代表了一个有趣的研究课题。</p><p id="c38d" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">尽管到目前为止信息量很大，但仍然缺少两个实验:文档级机器翻译的微调和无监督学习的微调。</p><h1 id="f6b7" class="na nb it bd nc nd ne nf ng nh ni nj nk ki nl kj nm kl nn km no ko np kp nq nr bi translated">文档级MT</h1><p id="2d3e" class="pw-post-body-paragraph li lj it lk b ll ns kd ln lo nt kg lq lr nu lt lu lv nv lx ly lz nw mb mc md im bi translated">通过在WMT 19恩-德和特德15恩-ZH两个方向上微调模型来进行实验。基线是一个模型(<a class="ae lh" href="https://github.com/idiap/HAN_NMT" rel="noopener ugc nofollow" target="_blank">韩</a>【9】)，它是利用文档的层次结构，专门为文档级任务设计的。然而，[9]发表于2018年，我不确定是否能找到更好的基线。</p><p id="f471" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">结果表明，当在句子级工作时，在mBART上预训练的模型比随机初始化的相同模型的性能好得多。此外，只有经过微调的版本才能在文档级别产生任何有意义的结果。<a class="ae lh" href="https://arxiv.org/pdf/1907.06170.pdf" rel="noopener ugc nofollow" target="_blank">之前的工作</a> [10]应用了BERT损失，包括识别两个句子是否属于同一个文档，并且能够在没有预训练的情况下训练文档级模型。显然，需要特定于文档级别的损失，但可以在不同的培训阶段使用。如果在预训练期间完成，微调阶段会变得更容易。</p><p id="1a99" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">最终结果是，预训练的模型优于韩的模型，也优于在句子层次上微调的模型。这也与[10]一致，并不令人惊讶，因为句子级机器翻译可以接收脱离上下文时有歧义的句子，但在更广泛的上下文中却可以准确翻译。</p><p id="e7ba" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">最后，作者还表明，mBART是一个伟大的初始化无监督机器翻译。</p><h1 id="abd2" class="na nb it bd nc nd ne nf ng nh ni nj nk ki nl kj nm kl nn km no ko np kp nq nr bi translated">无人监管的MT</h1><p id="1c17" class="pw-post-body-paragraph li lj it lk b ll ns kd ln lo nt kg lq lr nu lt lu lv nv lx ly lz nw mb mc md im bi translated">最后一个实验的目的是在无监督的学习环境下评估mBART。作者提出了两种无监督的双语机器翻译场景:</p><ol class=""><li id="00e3" class="ml mm it lk b ll lm lo lp lr mn lv mo lz mp md mq mr ms mt bi translated">通过回译学习:<br/>用mBART初始化的模型用于生成从目标语言到源语言的即时回译，并通过这种方式学习双语任务。当没有给出平行文本时，这是唯一可能的情况。</li><li id="49df" class="ml mm it lk b ll mv lo mw lr mx lv my lz mz md mq mr ms mt bi translated">通过语言迁移学习:<br/>用mBART初始化的模型在一个语言对上进行微调，在另一个具有相同目标端语言的语言对上进行评估。当目标语言存在平行数据，而源-目标对不存在平行数据时，这种训练是有用的。</li></ol><p id="1e83" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">这里的结果是复杂的，但可以找到一些模式。</p><ul class=""><li id="8ad8" class="ml mm it lk b ll lm lo lp lr mn lv mo lz mp md nx mr ms mt bi translated">对于基于回译的无监督机器翻译:<br/> mBART在对<strong class="lk jd">相似语言、</strong>进行微调时类似于更传统的方法，而它为<strong class="lk jd">不相似语言</strong>提供了第一个非退化结果。我们仍然在谈论不能在实践中使用的模型，但这是重要的第一步。</li><li id="32f3" class="ml mm it lk b ll mv lo mw lr mx lv my lz mz md nx mr ms mt bi translated">对于语言迁移:<br/>当在高资源语言上进行微调时，结果通常令人惊讶地好，在某些情况下，如果源端测试语言与微调语言非常不同，结果也是如此。然而，使用相似的语言通常会得到最好的结果。值得注意的是古吉拉特语的情况，这是一种资源非常匮乏的印度语言，它通过对其他印度语言进行微调而受益匪浅，而它自己的数据导致随机翻译(使用印地语-英语的13.8 BLEU，而使用古吉拉特语-英语的13.8 BLEU)。</li><li id="c0c8" class="ml mm it lk b ll mv lo mw lr mx lv my lz mz md nx mr ms mt bi translated">这两种方法也可以结合起来:从语言迁移开始，应用迭代回译。这些改进确实值得付出额外的努力。</li></ul><p id="fc21" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我邀请读者深入到论文中去研究所有的结果，它们很多而且非常详细。因此，可能有适合您的用例的东西。值得注意的是，在无监督学习实验中，英语总是两种语言中的一种，在语言迁移实验中，英语总是目标语言。此外，在预训练期间观察到了两种微调语言，尽管是单语数据。</p><h1 id="e3fb" class="na nb it bd nc nd ne nf ng nh ni nj nk ki nl kj nm kl nn km no ko np kp nq nr bi translated">结束语</h1><p id="5502" class="pw-post-body-paragraph li lj it lk b ll ns kd ln lo nt kg lq lr nu lt lu lv nv lx ly lz nw mb mc md im bi translated">mBART在机器翻译和其他基于一个文本到另一个文本的自然语言处理任务领域开辟了新的机会，例如参见[11][12][13]。</p><p id="76af" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我发现更有趣的是，模型在预训练期间学习了语言的一些结构，这种结构似乎超越了语言的边界，允许语言之间的内在知识转移。特别是，看到语言转换可以明显优于对目标语言对的微调，对于某些应用程序来说，这确实是改变游戏规则的。</p><p id="16c9" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">现在，我邀请你通过检查结果来批判性地阅读这篇论文，如果你想尝试mBART，你可以在<a class="ae lh" href="https://huggingface.co/transformers/model_doc/mbart.html" rel="noopener ugc nofollow" target="_blank">拥抱脸变形金刚</a>或<a class="ae lh" href="https://github.com/pytorch/fairseq/tree/main/examples/mbart" rel="noopener ugc nofollow" target="_blank">公平序列</a>中找到它。</p><p id="a3fd" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">你觉得多语预训后双语微调太局限？在本系列的下一篇文章中，我们将介绍mBART50 (50种预训练语言),它提出了一种通过多语言微调来改善结果的方法。</p><div class="ny nz gp gr oa ob"><a rel="noopener follow" target="_blank" href="/tips-for-reading-and-writing-an-ml-research-paper-a505863055cf"><div class="oc ab fo"><div class="od ab oe cl cj of"><h2 class="bd jd gy z fp og fr fs oh fu fw jc bi translated">阅读和撰写ML研究论文的技巧</h2><div class="oi l"><h3 class="bd b gy z fp og fr fs oh fu fw dk translated">从几十次同行评审中获得的经验教训</h3></div><div class="oj l"><p class="bd b dl z fp og fr fs oh fu fw dk translated">towardsdatascience.com</p></div></div><div class="ok l"><div class="ol l om on oo ok op lb ob"/></div></div></a></div><h1 id="c389" class="na nb it bd nc nd ne nf ng nh ni nj nk ki nl kj nm kl nn km no ko np kp nq nr bi translated">参考</h1><p id="50a3" class="pw-post-body-paragraph li lj it lk b ll ns kd ln lo nt kg lq lr nu lt lu lv nv lx ly lz nw mb mc md im bi translated">[1] Clinchant、Stéphane、Kweon Woo Jung和Vassilina Nikoulina。"使用BERT进行神经机器翻译."<em class="mu">第三届神经生成和翻译研讨会会议录</em>。2019</p><p id="34f1" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">[2]兰普勒，纪尧姆和亚历克西斯·康诺。"跨语言语言模型预训练."<em class="mu"> arXiv预印本arXiv:1901.07291 </em> (2019)。</p><p id="4da5" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">[3] Edunov、Sergey、Alexei Baevski和Michael Auli。"用于语言生成的预训练语言模型表示."<em class="mu">计算语言学协会北美分会2019年会议论文集:人类语言技术，第1卷(长短论文)</em>。2019</p><p id="06f1" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">[4]郭、智宇和阮明乐。"使用BERT作为上下文编码器的文档级神经机器翻译."<em class="mu">计算语言学协会亚太分会第一届会议暨第十届国际自然语言处理联合会议论文集:学生研究工作坊</em>。2020.</p><p id="d0d2" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">[5]刘，，等.“面向神经机器翻译的多语种去噪预训练”计算语言学协会汇刊8(2020):726–742。</p><p id="ef76" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">[6]瓦斯瓦尼、阿希什等人，“你所需要的只是关注。”<em class="mu">神经信息处理系统的进展</em>。2017.</p><p id="70e4" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">[7]森里奇、里科、巴里·哈多和亚历山德拉·伯奇。"具有子词单元的稀有词的神经机器翻译."<em class="mu">计算语言学协会第54届年会论文集(第1卷:长篇论文)</em>。2016.</p><p id="a6bf" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">[8] Kasai，Jungo等,《深层编码器，浅层解码器:重新评估非自回归机器翻译》</p><p id="8b41" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">[9] Werlen，Lesly Miculicich，等，“使用分层注意网络的文档级神经机器翻译”<em class="mu">2018自然语言处理经验方法会议论文集</em>。2018.</p><p id="1cb2" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">[10]马钦·容奇斯-道蒙。“WMT 2019微软翻译机:走向大规模文档级神经机器翻译。”</p><p id="abd3" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">[11]马丁、路易斯等，“多语言无监督句子简化”arXiv预印本arXiv:2005.00352  (2020)。</p><p id="b2cc" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">[12]胜俣、仓知和住井护·小町。"使用预训练的编码器-解码器模型的更强的语法错误校正基线."<em class="mu">计算语言学协会亚太分会第一届会议暨第十届国际自然语言处理联合会议论文集</em>。2020.</p><p id="3ba2" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">[13]李，冼，等.“<em class="mu"> arXiv预印本arXiv:2010.12829 </em> (2020)。</p><h1 id="7d33" class="na nb it bd nc nd ne nf ng nh ni nj nk ki nl kj nm kl nn km no ko np kp nq nr bi translated">中等会员</h1><p id="faf2" class="pw-post-body-paragraph li lj it lk b ll ns kd ln lo nt kg lq lr nu lt lu lv nv lx ly lz nw mb mc md im bi translated">你喜欢我的文章吗？你是否正在考虑申请一个中级会员来无限制地阅读我的文章？</p><p id="d7fd" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">如果您通过此链接订阅，您将通过您的订阅支持我，无需为您支付额外费用【https://medium.com/@mattiadigangi/membership<a class="ae lh" href="https://medium.com/@mattiadigangi/membership" rel="noopener"/></p></div></div>    
</body>
</html>