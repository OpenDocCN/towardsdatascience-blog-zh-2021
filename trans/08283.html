<html>
<head>
<title>Practical Guide to Ensemble Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">集成学习实用指南</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/practical-guide-to-ensemble-learning-d34c74e022a0?source=collection_archive---------10-----------------------#2021-07-30">https://towardsdatascience.com/practical-guide-to-ensemble-learning-d34c74e022a0?source=collection_archive---------10-----------------------#2021-07-30</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="49ed" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">通过投票、打包、提升和堆叠来改进您的模型</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/d4e1385c28cf6436fa0a723e6a6aff2b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*4PEmHFnOiFbBLnou"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">由<a class="ae ky" href="https://unsplash.com/@justinroyphoto?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">贾斯汀·罗伊</a>在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><p id="9886" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">集成学习</strong>是机器学习中使用的一种技术，用于将多个模型组合成一个组模型，换句话说，组合成一个<em class="lv">集成模型</em>。集合模型旨在比单独的每个模型表现得更好，或者如果不是，至少表现得与组中最好的单个模型一样好。</p><p id="2296" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在本文中，您将学习流行的集成方法:<strong class="lb iu"> <em class="lv">投票</em></strong><strong class="lb iu"><em class="lv">打包</em></strong><strong class="lb iu"><em class="lv">助推、</em></strong><strong class="lb iu"><em class="lv">堆叠</em> </strong>以及它们的Python实现。我们将使用<code class="fe lw lx ly lz b"><a class="ae ky" href="https://scikit-learn.org/stable/index.html" rel="noopener ugc nofollow" target="_blank">scikit-learn</a></code>等库进行投票、装袋和升压，使用<code class="fe lw lx ly lz b"><a class="ae ky" href="http://rasbt.github.io/mlxtend/" rel="noopener ugc nofollow" target="_blank">mlxtend</a></code>进行堆栈。</p><p id="86b7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在阅读本文的同时，我鼓励您查看我的GitHub上的J <a class="ae ky" href="https://github.com/Idilismiguzel/Machine-Learning/blob/master/Ensemble_Learning/Ensemble_Learning_Practice.ipynb" rel="noopener ugc nofollow" target="_blank"> upyter笔记本</a>以获得完整的分析和代码。🌻</p></div><div class="ab cl ma mb hx mc" role="separator"><span class="md bw bk me mf mg"/><span class="md bw bk me mf mg"/><span class="md bw bk me mf"/></div><div class="im in io ip iq"><h1 id="1077" class="mh mi it bd mj mk ml mm mn mo mp mq mr jz ms ka mt kc mu kd mv kf mw kg mx my bi translated">介绍</h1><p id="d06c" class="pw-post-body-paragraph kz la it lb b lc mz ju le lf na jx lh li nb lk ll lm nc lo lp lq nd ls lt lu im bi translated">集成学习背后的直觉通常用一种叫做<strong class="lb iu">群体智慧<em class="lv"> </em> </strong>的现象来描述，这意味着由一组个体做出的集合决策通常比个体决策更好。创建聚合模型(或<strong class="lb iu">集成</strong>)有多种方法，我们可以将它们分类为异类和同类集成。</p><p id="3216" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在<strong class="lb iu">异构集成中，</strong>我们组合在同一数据集上训练的多个<strong class="lb iu"> <em class="lv">不同的</em> </strong>微调模型，以生成集成模型。这种方法通常涉及<em class="lv">投票</em>、<em class="lv">平均、</em>或<em class="lv">叠加</em>技术。另一方面，在<strong class="lb iu">同质系综中，</strong>我们使用<strong class="lb iu"> <em class="lv">相同的</em> </strong>模型，我们称之为“弱模型”,并使用诸如<em class="lv">打包</em>和<em class="lv">增强</em>的技术，我们将这个弱模型转换为更强的模型。</p><p id="88d6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们从异构集成的基本集成学习方法开始:投票和平均。</p><h1 id="d2bc" class="mh mi it bd mj mk ne mm mn mo nf mq mr jz ng ka mt kc nh kd mv kf ni kg mx my bi translated">1.投票(硬投票)</h1><p id="038c" class="pw-post-body-paragraph kz la it lb b lc mz ju le lf na jx lh li nb lk ll lm nc lo lp lq nd ls lt lu im bi translated">硬投票集成用于分类任务，它结合来自多个微调模型的预测，这些模型基于<strong class="lb iu">多数投票</strong>原则对相同数据进行训练。例如，如果我们集成3个分类器，它们的预测为“A类”、“A类”、“B类”，那么集成模型将基于多数投票，或者换句话说，基于各个模型预测的分布模式，将输出预测为“A类”。如您所见，我们倾向于使用奇数个型号(例如3、5、7个型号)，以确保我们不会获得相同的票数。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nj"><img src="../Images/f8b061ee48a79d2b3d3b9a9740c4fa34.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vBCXgsWI9FaXEEvUS6vrdw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">硬投票:用多个模型和集合预测新实例<strong class="bd nk">投票</strong>多数投票的最终结果—图片由作者提供</p></figure><pre class="kj kk kl km gt nl lz nm nn aw no bi"><span id="9f0d" class="np mi it lz b gy nq nr l ns nt"># Instantiate individual models</span><span id="80eb" class="np mi it lz b gy nu nr l ns nt">clf_1 = KNeighborsClassifier()<br/>clf_2 = LogisticRegression()<br/>clf_3 = DecisionTreeClassifier()</span><span id="fedc" class="np mi it lz b gy nu nr l ns nt"># Create voting classifier</span><span id="3613" class="np mi it lz b gy nu nr l ns nt">voting_ens = VotingClassifier(<br/>estimators=[('knn', clf_1), ('lr', clf_2), ('dt', clf_3)], voting='hard')</span><span id="cf91" class="np mi it lz b gy nu nr l ns nt"># Fit and predict with the models and ensemble<br/>for clf in (clf_1, clf_2, clf_3, voting_ens):<br/>   clf.fit(X_train, y_train)<br/>   y_pred = clf.predict(X_test)<br/>   print(clf.__class__.__name__, accuracy_score(y_test, y_pred))</span></pre><p id="7482" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">准确度分数:</p><blockquote class="nv nw nx"><p id="49cf" class="kz la lv lb b lc ld ju le lf lg jx lh ny lj lk ll nz ln lo lp oa lr ls lt lu im bi translated">近邻分类器0.93 <br/>逻辑回归0.92 <br/>决策树分类器0.93 <br/>投票分类器0.94 ✅</p></blockquote><p id="f778" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">正如我们所见，投票分类器具有最高的准确性得分！由于集合将结合单个模型预测，每个模型应该已经微调，并已表现良好。在上面的代码中，我只是出于演示的目的对它进行了初始化。</p><h1 id="c288" class="mh mi it bd mj mk ne mm mn mo nf mq mr jz ng ka mt kc nh kd mv kf ni kg mx my bi translated">2.平均(软投票)</h1><p id="8195" class="pw-post-body-paragraph kz la it lb b lc mz ju le lf na jx lh li nb lk ll lm nc lo lp lq nd ls lt lu im bi translated">软投票用于分类和回归任务，它通过<strong class="lb iu">平均</strong>对基于相同数据训练的多个微调模型的预测进行组合。对于分类，它使用<em class="lv">预测概率</em>，对于回归，它使用<em class="lv">预测值</em>。我们不需要像硬投票一样的奇数个单独的模型，但我们需要至少2个模型来建立一个集合。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ob"><img src="../Images/1870e8e1864727e582b51c228fcff1ba.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mrpxQVexMv75iND0t6TC6w.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">软投票:用等权重模型(w)预测新实例，集合<strong class="bd nk">通过平均选择</strong>最终结果—作者图像</p></figure><p id="70ae" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">软投票的一个优点是，您可以决定是对每个模型进行平均加权(平均)还是按分类器的重要性(一个输入参数)进行加权。如果您喜欢使用加权平均值，那么集合模型的输出预测将是加权概率/值的最大和。</p><pre class="kj kk kl km gt nl lz nm nn aw no bi"><span id="d980" class="np mi it lz b gy nq nr l ns nt"># Instantiate individual models<br/>reg1 = DecisionTreeRegressor()<br/>reg2 = LinearRegression()</span><span id="e13e" class="np mi it lz b gy nu nr l ns nt"># Create voting regressor<br/>voting_ens = VotingRegressor(<br/>estimators=[('dt', reg1), ('lr', reg2)], weights=[2,1])</span><span id="8180" class="np mi it lz b gy nu nr l ns nt"># Fit and predict with the models and ensemble<br/>for reg in (reg1, reg2, voting_ens):<br/>   reg.fit(X_train, y_train)<br/>   y_pred = reg.predict(X_test)<br/>   print(reg.__class__.__name__, mean_absolute_error(y_test, y_pred))</span></pre><p id="3026" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">平均绝对误差:</p><blockquote class="nv nw nx"><p id="33c4" class="kz la lv lb b lc ld ju le lf lg jx lh ny lj lk ll nz ln lo lp oa lr ls lt lu im bi translated">决策树回归器3.0 <br/>线性回归3.2 <br/>投票回归器2.5 ✅</p></blockquote><p id="ef01" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">重要的是要明白<strong class="lb iu">投票群体</strong>(硬投票和软投票)<strong class="lb iu"> </strong>的表现在很大程度上取决于单个模特的表现。如果我们集合一个好的和两个表现一般的模型，那么集合模型将显示接近平均模型的结果。在这种情况下，我们要么需要改进表现一般的模型，要么我们不应该做一个集合，而是使用表现良好的模型。📌</p><p id="c01f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">理解了投票和平均之后，我们可以继续最后一个异构集成技术:堆叠。</p><h1 id="f8ad" class="mh mi it bd mj mk ne mm mn mo nf mq mr jz ng ka mt kc nh kd mv kf ni kg mx my bi translated">3.堆垛</h1><p id="f71c" class="pw-post-body-paragraph kz la it lb b lc mz ju le lf na jx lh li nb lk ll lm nc lo lp lq nd ls lt lu im bi translated">Stacking代表“Stacked Generalization ”,它将多个单独的模型(或基础模型)与一个最终模型(或元模型)相结合，该最终模型通过基础模型的预测进行<strong class="lb iu">训练。它既可以用于分类任务，也可以用于回归任务，并且可以选择使用值或概率来执行分类任务。</strong></p><p id="7da5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">与投票集成的区别在于，在堆叠中，元模型也是一个可训练的模型，事实上，它是使用基本模型的预测来训练的。因为这些预测是元模型的输入特征，所以它们也被称为元特征。我们可以选择将初始数据集包含到元要素中，或者仅使用预测。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oc"><img src="../Images/8811955929cb2ca7c7e8597aacbf0563.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*C9HQBiljlqM9xjg3Ch6qIg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">堆叠:元模型训练中使用的基础模型预测，以<strong class="bd nk">预测</strong>最终输出——图片由作者提供</p></figure><p id="b886" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">堆叠可以用多于两层来实现:<strong class="lb iu">多层堆叠</strong>，在这里我们定义基础模型，与另一层模型聚合，然后是最终的元模型。即使这可以产生更好的结果，我们也应该考虑到由于复杂性所带来的时间成本。</p><p id="c466" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为了防止<strong class="lb iu">过度拟合，</strong>我们可以使用<strong class="lb iu">堆叠和交叉验证</strong>来代替标准堆叠，mlxtend库对两个版本都有实现。下面，我将实现:</p><p id="8dce" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi od translated"><span class="l oe of og bm oh oi oj ok ol di"> 1。</span>分类任务的标准堆叠</p><pre class="kj kk kl km gt nl lz nm nn aw no bi"><span id="a789" class="np mi it lz b gy nq nr l ns nt">from mlxtend.classifier import StackingClassifier</span><span id="e947" class="np mi it lz b gy nu nr l ns nt"># Initialize individual models<br/>clf_1 = KNeighborsClassifier()<br/>clf_2 = GaussianNB()<br/>clf_3 = DecisionTreeClassifier()</span><span id="575b" class="np mi it lz b gy nu nr l ns nt"># Initialize meta-model<br/>clf_meta = LogisticRegression()</span><span id="70d3" class="np mi it lz b gy nu nr l ns nt"># Create stacking classifier<br/>clf_stack = StackingClassifier(<br/>classifiers=[clf_1, clf_2, clf_3], meta_classifier=clf_meta,<br/>use_probas=False, use_features_in_secondary=False)</span><span id="5daa" class="np mi it lz b gy nu nr l ns nt"># Fit and predict with the models and ensemble<br/>for clf in (clf_1, clf_2, clf_3, clf_meta, clf_stack):<br/>   clf.fit(X_train, y_train)<br/>   y_pred = clf.predict(X_test)<br/>   print(clf.__class__.__name__, accuracy_score(y_test, y_pred))</span></pre><blockquote class="nv nw nx"><p id="5c61" class="kz la lv lb b lc ld ju le lf lg jx lh ny lj lk ll nz ln lo lp oa lr ls lt lu im bi translated">近邻分类器0.84 <br/>高斯分类器0.83 <br/>决策树分类器0.89 <br/>逻辑回归0.85 <br/>堆栈分类器0.90 ✅</p></blockquote><p id="846e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi od translated"><span class="l oe of og bm oh oi oj ok ol di"> 2。</span>回归任务的交叉验证堆叠</p><pre class="kj kk kl km gt nl lz nm nn aw no bi"><span id="890c" class="np mi it lz b gy nq nr l ns nt">from mlxtend.regressor import StackingCVRegressor</span><span id="69a4" class="np mi it lz b gy nu nr l ns nt"># Initialize individual models<br/>reg1 = DecisionTreeRegressor()<br/>reg2 = SVR()</span><span id="2554" class="np mi it lz b gy nu nr l ns nt"># Create meta-model<br/>meta_model = LinearRegression()</span><span id="9ea5" class="np mi it lz b gy nu nr l ns nt"># Create stacking classifier<br/>reg_stack = StackingCVRegressor(<br/>regressors=[reg1, reg2], meta_regressor=meta_model,<br/>use_features_in_secondary=False)</span><span id="b806" class="np mi it lz b gy nu nr l ns nt"># Fit and predict with the models and ensemble<br/>for reg in (reg1, reg2, meta_model, reg_stack):<br/>   reg.fit(X_train, y_train)<br/>   y_pred = reg.predict(X_test)<br/>   print(reg.__class__.__name__, mean_absolute_error(y_test, y_pred))</span></pre><p id="15d2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">平均绝对误差:</p><blockquote class="nv nw nx"><p id="625c" class="kz la lv lb b lc ld ju le lf lg jx lh ny lj lk ll nz ln lo lp oa lr ls lt lu im bi translated">决策树回归器3.3 <br/>支持向量回归5.2 <br/>线性回归3.2 <br/>堆栈回归器2.9 ✅</p></blockquote><h1 id="ed1e" class="mh mi it bd mj mk ne mm mn mo nf mq mr jz ng ka mt kc nh kd mv kf ni kg mx my bi translated">4.制袋材料</h1><p id="f331" class="pw-post-body-paragraph kz la it lb b lc mz ju le lf na jx lh li nb lk ll lm nc lo lp lq nd ls lt lu im bi translated">Bootstrap聚合或简而言之“Bagging”聚合多个估计器，这些估计器使用用训练数据的不同子集训练的<em class="lv">相同的</em>算法。它可用于分类和回归任务，使用<strong class="lb iu">引导</strong>通过随机采样为每个估计器创建训练数据。</p><blockquote class="nv nw nx"><p id="c00e" class="kz la lv lb b lc ld ju le lf lg jx lh ny lj lk ll nz ln lo lp oa lr ls lt lu im bi translated">自举<em class="it">是一种从原始数据创建替换样本的方法。这是通过</em> <strong class="lb iu"> <em class="it">替换</em> </strong> <em class="it">来完成的，以使每个数据点被选取的概率相等。由于替换选择，一些数据点可能被多次选取，而一些可能永远不会被选取。我们可以使用以下公式计算大小为</em> n <em class="it">的bootstrap样本中某个数据点未被选中的概率。</em> <em class="it">(优选n为大数)。</em></p></blockquote><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi om"><img src="../Images/104acdd5da2119490bceb6f744ca3da4.png" data-original-src="https://miro.medium.com/v2/resize:fit:972/format:webp/1*jnLbe63pToYoMbMgXVFGJw.png"/></div></figure><p id="83af" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这意味着使用大约63%的训练数据集训练每个bagging估计器，我们将剩余的37% <strong class="lb iu">随机(OOB) </strong>样本<strong class="lb iu">。</strong></p><p id="540c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">综上所述，bagging从原始训练数据中为<em class="lv"> n </em>个估计器抽取了<em class="lv"> n </em>个训练数据集。每个估计器在其采样训练数据集<strong class="lb iu">上被并行训练</strong>以进行预测。然后，bagging使用硬投票或软投票等技术聚集这些预测。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi on"><img src="../Images/6054eb9a9f33a5f660bcc080575e8b53.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*O5Cz0U3Jf97g3KrgoQ53_A.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">Bagging:估计器和预测器使用的自举训练样本与投票技术相结合</p></figure><p id="3e46" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在scikit-learn中，我们可以将参数<code class="fe lw lx ly lz b">n_estimators</code>定义为等于<em class="lv">n</em>——我们想要生成的估计器/模型的数量，如果我们想要评估每个估计器对其出袋样本的性能，可以将<code class="fe lw lx ly lz b">oob_score</code>设置为“真”。通过这样做，我们可以很容易地了解估计者对未知数据的表现，而无需使用交叉验证或单独的测试集。<code class="fe lw lx ly lz b">oob_score_</code>函数计算所有<strong class="lb iu"> <em class="lv"> n </em> </strong> oob_scores的平均值，默认情况下，使用指标<strong class="lb iu">准确性</strong> <strong class="lb iu">得分</strong>进行分类，使用<strong class="lb iu"> R^2 </strong>进行回归。</p><pre class="kj kk kl km gt nl lz nm nn aw no bi"><span id="b0c4" class="np mi it lz b gy nq nr l ns nt">from sklearn.ensemble import BaggingClassifier</span><span id="01f6" class="np mi it lz b gy nu nr l ns nt"># Initialize weak model<br/>base_model = DecisionTreeClassifier(max_depth=3)</span><span id="03ad" class="np mi it lz b gy nu nr l ns nt"># Create bagging classifier<br/>clf_bagging = BaggingClassifier(base_estimator=base_model, n_estimators=1000, oob_score=True)</span><span id="3911" class="np mi it lz b gy nu nr l ns nt">clf_bagging.fit(X_train, y_train)</span><span id="0449" class="np mi it lz b gy nu nr l ns nt"># Check oob score<br/>print(clf_bagging.oob_score_)</span></pre><blockquote class="nv nw nx"><p id="d4bd" class="kz la lv lb b lc ld ju le lf lg jx lh ny lj lk ll nz ln lo lp oa lr ls lt lu im bi translated">oob_score_ : 0.918</p></blockquote><pre class="kj kk kl km gt nl lz nm nn aw no bi"><span id="1a57" class="np mi it lz b gy nq nr l ns nt"># Compare with test set<br/>pred = clf_bagging.predict(X_test)<br/>print(accuracy_score(y_test, pred))</span></pre><blockquote class="nv nw nx"><p id="5d1d" class="kz la lv lb b lc ld ju le lf lg jx lh ny lj lk ll nz ln lo lp oa lr ls lt lu im bi translated">准确率_得分:0.916</p></blockquote><p id="f570" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">随机采样的训练数据集使训练不容易偏离原始数据，因此装袋<strong class="lb iu">减少了单个估计量的方差</strong>。</p><p id="4eb7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">一种非常流行的打包技术是<strong class="lb iu">随机森林</strong>，其中估值器被选为决策树。随机森林使用引导来创建具有替换的训练数据集，并且它还选择一组特征(没有替换)来最大化每个训练数据集上的随机化。通常，所选特征的数量等于特征总数的平方根。</p><h1 id="7e96" class="mh mi it bd mj mk ne mm mn mo nf mq mr jz ng ka mt kc nh kd mv kf ni kg mx my bi translated">5.助推</h1><p id="28b3" class="pw-post-body-paragraph kz la it lb b lc mz ju le lf na jx lh li nb lk ll lm nc lo lp lq nd ls lt lu im bi translated">Boosting使用<strong class="lb iu">渐进学习</strong>，这是一个迭代过程，专注于最小化先前估计器的误差。这是一种<strong class="lb iu">顺序方法</strong>，其中每个估计器依赖于前一个估计器来改进预测。最流行的增强方法是自适应增强(AdaBoost)和梯度增强。</p><p id="f180" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu"> AdaBoost </strong>对每个<em class="lv"> n </em>估计器使用整个训练数据集，并做了一些重要的修改。第一个估计器(弱模型)在具有相等加权数据点的原始数据集上训练。在进行第一次预测并计算误差后，与正确预测的数据点相比，错误预测的数据点被分配有<strong class="lb iu">更高的权重</strong>。通过这样做，下一个评估者将关注这些难以预测的实例。这个过程将一直持续到所有的<em class="lv"> n </em>个估计器(比如1000个)被顺序训练。最后，集合的预测将通过加权多数投票或加权平均来获得。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oo"><img src="../Images/b5886ca2e7d531654891aee8049a8665.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gBnpZrk4edTzRVvW9X3uOg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">AdaBoost:训练数据中权重更新的序列模型训练——图片由作者提供</p></figure><pre class="kj kk kl km gt nl lz nm nn aw no bi"><span id="32d8" class="np mi it lz b gy nq nr l ns nt">from sklearn.ensemble import AdaBoostRegressor</span><span id="8995" class="np mi it lz b gy nu nr l ns nt"># Initialize weak model<br/>base_model = LinearRegression(normalize=True)</span><span id="6efd" class="np mi it lz b gy nu nr l ns nt"># Create AdaBoost regressor<br/>reg_adaboost = AdaBoostRegressor(base_estimator=base_model, n_estimators=1000)</span><span id="a6e3" class="np mi it lz b gy nu nr l ns nt">reg_adaboost.fit(X_train, y_train)</span><span id="c01b" class="np mi it lz b gy nu nr l ns nt"># Predict and compare with y_test<br/>pred = reg_adaboost.predict(X_test)<br/>rmse = np.sqrt(mean_squared_error(y_test, pred))<br/>print('RMSE:', rmse)</span></pre><blockquote class="nv nw nx"><p id="639e" class="kz la lv lb b lc ld ju le lf lg jx lh ny lj lk ll nz ln lo lp oa lr ls lt lu im bi translated">RMSE: 4.18</p></blockquote><p id="53b4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">由于每个下一个估计器的目的是纠正错误分类/错误预测的数据点，增强<strong class="lb iu">减少了每个估计器的偏差</strong>。</p><p id="e389" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">梯度提升</strong>，非常类似于AdaBoost，通过顺序迭代改进了以前的估计器，但它不是更新训练数据的权重，而是使新的估计器适应来自以前估计器的<strong class="lb iu">残差</strong>。XGBoost、LightGBM和CatBoost是流行的梯度提升算法，尤其是XGBoost是许多竞赛的获胜者，并因非常快速和可扩展而流行。</p><h1 id="43d4" class="mh mi it bd mj mk ne mm mn mo nf mq mr jz ng ka mt kc nh kd mv kf ni kg mx my bi translated">结论</h1><p id="7af7" class="pw-post-body-paragraph kz la it lb b lc mz ju le lf na jx lh li nb lk ll lm nc lo lp lq nd ls lt lu im bi translated">在本文中，我们学习了主要的集成学习技术来提高模型性能。我们介绍了每种技术的理论背景以及相关的Python库来演示这些机制。</p><p id="f36f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">集成学习在机器学习中占有很大一部分，它对每个数据科学家和机器学习实践者都很重要。你可能会发现有很多东西要学，但我相信你永远不会后悔！！💯</p><p id="6a4c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果你需要关于自举的复习，或者如果你想学习更多关于采样技术的知识，你可以看看我下面的文章。</p><div class="op oq gp gr or os"><a rel="noopener follow" target="_blank" href="/resampling-methods-for-inference-analysis-e75fecfefcb2"><div class="ot ab fo"><div class="ou ab ov cl cj ow"><h2 class="bd iu gy z fp ox fr fs oy fu fw is bi translated">推理分析的重采样方法</h2><div class="oz l"><h3 class="bd b gy z fp ox fr fs oy fu fw dk translated">当你有一个样本，但你想了解人口</h3></div><div class="pa l"><p class="bd b dl z fp ox fr fs oy fu fw dk translated">towardsdatascience.com</p></div></div><div class="pb l"><div class="pc l pd pe pf pb pg ks os"/></div></div></a></div><p id="65ac" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我希望你喜欢阅读关于集成学习方法的文章，并发现这篇文章对你的分析有用！</p><p id="ffe1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="lv">如果你喜欢这篇文章，你可以</em><strong class="lb iu"><em class="lv"/></strong><a class="ae ky" href="https://medium.com/@idilismiguzel" rel="noopener"><strong class="lb iu"><em class="lv">在这里阅读我的其他文章</em></strong></a><strong class="lb iu"><em class="lv"/></strong><em class="lv">和</em> <a class="ae ky" href="http://medium.com/@idilismiguzel/follow" rel="noopener"> <strong class="lb iu"> <em class="lv">关注我上媒</em></strong></a><strong class="lb iu"><em class="lv"/></strong>如果有任何问题或建议，请告诉我。✨</p><p id="575c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">喜欢这篇文章吗？ <a class="ae ky" href="https://idilismiguzel.medium.com/membership" rel="noopener"> <strong class="lb iu">成为更多会员！</strong> </a></p></div><div class="ab cl ma mb hx mc" role="separator"><span class="md bw bk me mf mg"/><span class="md bw bk me mf mg"/><span class="md bw bk me mf"/></div><div class="im in io ip iq"><h2 id="7f71" class="np mi it bd mj ph pi dn mn pj pk dp mr li pl pm mt lm pn po mv lq pp pq mx pr bi translated">参考</h2><ol class=""><li id="d011" class="ps pt it lb b lc mz lf na li pu lm pv lq pw lu px py pz qa bi translated"><a class="ae ky" href="https://sebastianraschka.com/pdf/lecture-notes/stat479fs18/07_ensembles_notes.pdf" rel="noopener ugc nofollow" target="_blank">合奏学习延伸阅读</a></li><li id="0f39" class="ps pt it lb b lc qb lf qc li qd lm qe lq qf lu px py pz qa bi translated"><a class="ae ky" href="http://rasbt.github.io/mlxtend/" rel="noopener ugc nofollow" target="_blank"> mlxtend库</a></li></ol></div></div>    
</body>
</html>