<html>
<head>
<title>Mask Detection using YOLOv5</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用YOLOv5进行掩模检测</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/mask-detection-using-yolov5-ae40979227a6?source=collection_archive---------2-----------------------#2021-08-06">https://towardsdatascience.com/mask-detection-using-yolov5-ae40979227a6?source=collection_archive---------2-----------------------#2021-08-06</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><figure class="ip iq gp gr ir is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi io"><img src="../Images/99e863ffb019e2d13c6252b12be3d491.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GFL-KOxw7O-BT4TsEa0crg.jpeg"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">图片来自<a class="ae jd" href="https://en.wikipedia.org/wiki/Object_detection" rel="noopener ugc nofollow" target="_blank">维基百科</a></p></figure><div class=""/><div class=""><h2 id="8311" class="pw-subtitle-paragraph kd jf jg bd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku dk translated">解释关键概念，然后使用YOLOv5实现简单的屏蔽检测</h2></div><h2 id="25a0" class="kv kw jg bd kx ky kz dn la lb lc dp ld le lf lg lh li lj lk ll lm ln lo lp lq bi translated">介绍</h2><p id="9eab" class="pw-post-body-paragraph lr ls jg lt b lu lv kh lw lx ly kk lz le ma mb mc li md me mf lm mg mh mi mj ij bi translated">我最近在Coursera上完成了吴恩达教授的DeepLearningAI的卷积神经网络课程，课程的一部分深入到了计算机视觉领域。我对计算机视觉的能力以及它如何融入我们的日常生活非常着迷，这促使我更深入地研究计算机视觉，并启动项目来加强我的学习。</p></div><div class="ab cl mk ml hu mm" role="separator"><span class="mn bw bk mo mp mq"/><span class="mn bw bk mo mp mq"/><span class="mn bw bk mo mp"/></div><div class="ij ik il im in"><h2 id="5869" class="kv kw jg bd kx ky kz dn la lb lc dp ld le lf lg lh li lj lk ll lm ln lo lp lq bi translated"><strong class="ak">如果您只想查看YOLOv5模型的项目实施情况，请向下滚动</strong></h2></div><div class="ab cl mk ml hu mm" role="separator"><span class="mn bw bk mo mp mq"/><span class="mn bw bk mo mp mq"/><span class="mn bw bk mo mp"/></div><div class="ij ik il im in"><h1 id="6427" class="mr kw jg bd kx ms mt mu la mv mw mx ld km my kn lh kp mz kq ll ks na kt lp nb bi translated">物体检测的工作原理</h1><p id="9399" class="pw-post-body-paragraph lr ls jg lt b lu lv kh lw lx ly kk lz le ma mb mc li md me mf lm mg mh mi mj ij bi translated">在实施一个项目之前，最好了解一些关于对象检测的基本概念以及它是如何协同工作的。让我们从定义对象检测开始:</p><h2 id="9963" class="kv kw jg bd kx ky kz dn la lb lc dp ld le lf lg lh li lj lk ll lm ln lo lp lq bi translated"><strong class="ak">{分类+定位} →检测</strong></h2><p id="c2b4" class="pw-post-body-paragraph lr ls jg lt b lu lv kh lw lx ly kk lz le ma mb mc li md me mf lm mg mh mi mj ij bi translated">图像分类是将一种算法应用于图像来预测一个对象的类别，例如汽车。物体定位不仅预测物体的类别，而且通过在物体周围画一个包围盒来计算出物体的位置。目标检测包括分类和定位，并且检测不止一个目标&amp;甚至不止一个类别。</p><figure class="nd ne nf ng gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nc"><img src="../Images/8a046395dc466104218ed0759a1851cc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sFk_eJLoo9U-VgEnu0Kwbg.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">3个术语之间的视觉比较。汽车图片由<a class="ae jd" href="https://unsplash.com/photos/T_ZdgxzPS5k" rel="noopener ugc nofollow" target="_blank">马特·安东尼奥利·Unsplash.com</a></p></figure><h2 id="ad8a" class="kv kw jg bd kx ky kz dn la lb lc dp ld le lf lg lh li lj lk ll lm ln lo lp lq bi translated"><strong class="ak">包围盒</strong></h2><p id="c0ea" class="pw-post-body-paragraph lr ls jg lt b lu lv kh lw lx ly kk lz le ma mb mc li md me mf lm mg mh mi mj ij bi translated">一个标准的分类任务将涉及一幅图像通过一个多层的Convnet，其中的矢量特征被输入到一个softmax单元，例如，输出预测的类别(算法试图检测的对象类别，即汽车、树木、行人)。诸如YOLO的对象检测模型通过将图像分割成网格单元来工作，其中如果边界框的中心位于单元中，则每个网格单元负责预测边界框。然后，它将输出预测的类，即边界框的坐标，如下所示:</p><figure class="nd ne nf ng gt is gh gi paragraph-image"><div class="gh gi nh"><img src="../Images/e38abd64e9ea6360de657d57a6347981.png" data-original-src="https://miro.medium.com/v2/resize:fit:1366/format:webp/1*GxjyN_eB_zaaWqH7aOvNig.png"/></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">标有蓝色的单元格预测边界框的中心位于单元格中。作者图片</p></figure><figure class="nd ne nf ng gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi ni"><img src="../Images/2ca4b1f026e4d309f4dfc1de5b9b186a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9cq4yetjw8Dnf9I6CTjoTA.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">显示与边界框关联的变量的图像。作者图片</p></figure><p id="0207" class="pw-post-body-paragraph lr ls jg lt b lu nj kh lw lx nk kk lz le nl mb mc li nm me mf lm nn mh mi mj ij bi translated">要了解更多关于卷积神经网络的信息，请查阅由Mayank Mishra 解释的<a class="ae jd" rel="noopener" target="_blank" href="/convolutional-neural-networks-explained-9cc5188c4939">卷积神经网络。他很好地解释了Convnet的工作原理</a></p><h2 id="d089" class="kv kw jg bd kx ky kz dn la lb lc dp ld le lf lg lh li lj lk ll lm ln lo lp lq bi translated"><strong class="ak">并集上的交集</strong></h2><p id="436b" class="pw-post-body-paragraph lr ls jg lt b lu lv kh lw lx ly kk lz le ma mb mc li md me mf lm mg mh mi mj ij bi translated">当算法输出定位检测到的对象的包围盒时，如何判断算法是否运行良好？这就是并集上的交集(IoU)发挥作用的地方。通常，IoU <strong class="lt jh">是两个边界框</strong>之间重叠的度量:算法预测边界框和地面真实边界框。</p><figure class="nd ne nf ng gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi no"><img src="../Images/e43f28a12fedc683b5ace35295d03c0c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*O6FQ6ZtjfHclYXulYpdRlg.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">作者图片</p></figure><p id="cb8d" class="pw-post-body-paragraph lr ls jg lt b lu nj kh lw lx nk kk lz le nl mb mc li nm me mf lm nn mh mi mj ij bi translated">IoU的公式是交叉点的大小除以两个边界框的并集的大小。IoU的阈值约为0.5。值≥ 0.5的借据被视为“正确”预测。</p><h2 id="1db3" class="kv kw jg bd kx ky kz dn la lb lc dp ld le lf lg lh li lj lk ll lm ln lo lp lq bi translated"><strong class="ak">非最大抑制</strong></h2><p id="79bc" class="pw-post-body-paragraph lr ls jg lt b lu lv kh lw lx ly kk lz le ma mb mc li md me mf lm mg mh mi mj ij bi translated">参考下面的边界框图像，标记为1–33的单元格都预测边界框的中心位于它们的单元格中。</p><figure class="nd ne nf ng gt is gh gi paragraph-image"><div class="gh gi np"><img src="../Images/6281bea6609eeb31a53cbc518bcca6ec.png" data-original-src="https://miro.medium.com/v2/resize:fit:888/format:webp/1*3RgY2fGUdJKrFv2MI53nxg.png"/></div></figure><p id="313f" class="pw-post-body-paragraph lr ls jg lt b lu nj kh lw lx nk kk lz le nl mb mc li nm me mf lm nn mh mi mj ij bi translated">这将导致算法多次检测对象，而不是仅一次。这就是非最大值抑制的作用，它确保算法只检测每个对象一次<strong class="lt jh"/>。如前所述，每个单元输出y = (P𝒸，bₓ，bᵧ，b𝓌，bₕ，c)，P𝒸是存在物体的概率。非最大值抑制的作用是获取具有最高P𝒸的边界框，丢弃任何p𝒸≤0.6的边界框，并“抑制”其他IoU ≥ 0.5的边界框。</p><figure class="nd ne nf ng gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nq"><img src="../Images/4a639f20b147588499f8f885b36b468b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*anPRBEMKm2pQdCTUyv-xtw.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">视觉描绘非最大抑制。作者图片</p></figure><h2 id="a025" class="kv kw jg bd kx ky kz dn la lb lc dp ld le lf lg lh li lj lk ll lm ln lo lp lq bi translated"><strong class="ak">锚箱</strong></h2><p id="de86" class="pw-post-body-paragraph lr ls jg lt b lu lv kh lw lx ly kk lz le ma mb mc li md me mf lm mg mh mi mj ij bi translated">如果多个对象位于同一个网格单元中会怎样？边界框会是什么样的？锚盒的想法可以用在这里。查看下图，注意人和汽车的中点如何位于网格单元内。(为了简单起见，我将图像分成3×3的网格)</p><figure class="nd ne nf ng gt is gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/1cff8f33e77c9c1fcade515556710833.png" data-original-src="https://miro.medium.com/v2/resize:fit:1260/format:webp/1*16gkRw5xtFJV87CAemTFDQ.png"/></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">描述包含两个对象中点的网格单元的图像。图片由<a class="ae jd" href="https://unsplash.com/photos/HS4FvZzoREM" rel="noopener ugc nofollow" target="_blank">丹学长，</a></p></figure><p id="4b7d" class="pw-post-body-paragraph lr ls jg lt b lu nj kh lw lx nk kk lz le nl mb mc li nm me mf lm nn mh mi mj ij bi translated">当前单元格输出y = (P𝒸，bₓ，bᵧ，b𝓌，bₕ，c)，单元格只能选择两个对象中的一个进行检测。但是对于锚盒(通常使用训练数据集的k-means分析预先定义)，成本标签y变成(P𝒸、bₓ、bᵧ、b𝓌、bₕ、c、P𝒸₁、bₓ₁、bᵧ₁、b、b、c、p……)。)基本上根据锚盒有多少而重复，第一输出用锚盒1编码，第二输出用锚盒2编码，依此类推。每个输出单元检测一个对象类；锚盒1类似于汽车，因此输出c将用于汽车，下一个输出c将用于人，因为它被编码到锚盒2。</p><figure class="nd ne nf ng gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi ns"><img src="../Images/b4224073942f217eb4fda63cb5c2ac19.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YrC_33dE7iYaYv5MOHwgPA.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">作者图片</p></figure><p id="4cb4" class="pw-post-body-paragraph lr ls jg lt b lu nj kh lw lx nk kk lz le nl mb mc li nm me mf lm nn mh mi mj ij bi translated">注意，表示对象类的每个输出单元c都受到对象的地面真实边界框的高IoU的影响。</p><h1 id="98ad" class="mr kw jg bd kx ms nt mu la mv nu mx ld km nv kn lh kp nw kq ll ks nx kt lp nb bi translated">使用YOLOv5进行掩模检测</h1><h2 id="ab6d" class="kv kw jg bd kx ky kz dn la lb lc dp ld le lf lg lh li lj lk ll lm ln lo lp lq bi translated">模型</h2><p id="eecb" class="pw-post-body-paragraph lr ls jg lt b lu lv kh lw lx ly kk lz le ma mb mc li md me mf lm mg mh mi mj ij bi translated">对于这个项目，我将使用YOLOv5来训练一个对象检测模型。YOLO是“你只看一次”的首字母缩写。一种流行的架构，因为:</p><ul class=""><li id="b319" class="ny nz jg lt b lu nj lx nk le oa li ob lm oc mj od oe of og bi translated">速度(基本型号—每秒45帧，快速型号—每秒155帧，比R-CNN快1000倍)</li><li id="23de" class="ny nz jg lt b lu oh lx oi le oj li ok lm ol mj od oe of og bi translated">该架构仅包括单个神经网络(可以直接针对检测性能进行端到端优化)</li><li id="abff" class="ny nz jg lt b lu oh lx oi le oj li ok lm ol mj od oe of og bi translated">能够学习物体的一般表示法(图像的全局上下文提供了预测信息)</li><li id="883d" class="ny nz jg lt b lu oh lx oi le oj li ok lm ol mj od oe of og bi translated">开源</li></ul><figure class="nd ne nf ng gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi om"><img src="../Images/537f2278c780c3a793da1cff4d7a5e9c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TnpzQ4Ap1P5fDg5atOBT5w.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">YOLOv5型号与EfficientDet之间的比较，由<a class="ae jd" href="https://github.com/ultralytics/yolov5" rel="noopener ugc nofollow" target="_blank"> Ultralytics </a>提供图像</p></figure><p id="5872" class="pw-post-body-paragraph lr ls jg lt b lu nj kh lw lx nk kk lz le nl mb mc li nm me mf lm nn mh mi mj ij bi translated">要了解关于该模型的更多信息，请访问他们的知识库:<a class="ae jd" href="https://github.com/ultralytics/yolov5" rel="noopener ugc nofollow" target="_blank"> Ultralytics YOLOv5 Github知识库</a>。</p><h2 id="5a5d" class="kv kw jg bd kx ky kz dn la lb lc dp ld le lf lg lh li lj lk ll lm ln lo lp lq bi translated">资料组</h2><p id="61c1" class="pw-post-body-paragraph lr ls jg lt b lu lv kh lw lx ly kk lz le ma mb mc li md me mf lm mg mh mi mj ij bi translated">我在Kaggle上找到了这个<a class="ae jd" href="https://www.kaggle.com/andrewmvd/face-mask-detection" rel="noopener ugc nofollow" target="_blank">人脸面具检测</a>数据集，由853张图片组成，分为3类:带面具、不带面具和面具佩戴不当。每幅图像都带有一个PASCAL VOC格式的XML文件，其中包含其边界框的注释。这里有一个例子:</p><figure class="nd ne nf ng gt is gh gi paragraph-image"><div class="gh gi on"><img src="../Images/be106ff7dc9ba3ef47e92a4202d0f2a9.png" data-original-src="https://miro.medium.com/v2/resize:fit:602/format:webp/1*pMhelb3w6ZkqFwFWMiNHKg.png"/></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">来自<a class="ae jd" href="https://www.kaggle.com/andrewmvd/face-mask-detection" rel="noopener ugc nofollow" target="_blank">面罩检测数据集</a>的示例图像</p></figure><pre class="nd ne nf ng gt oo op oq or aw os bi"><span id="474f" class="kv kw jg op b gy ot ou l ov ow">&lt;annotation&gt;</span><span id="53fc" class="kv kw jg op b gy ox ou l ov ow">   &lt;folder&gt;images&lt;/folder&gt;</span><span id="270c" class="kv kw jg op b gy ox ou l ov ow">   &lt;filename&gt;maksssksksss4.png&lt;/filename&gt;</span><span id="51b2" class="kv kw jg op b gy ox ou l ov ow">   &lt;size&gt;</span><span id="2341" class="kv kw jg op b gy ox ou l ov ow">       &lt;width&gt;301&lt;/width&gt;</span><span id="243a" class="kv kw jg op b gy ox ou l ov ow">       &lt;height&gt;400&lt;/height&gt;</span><span id="d1f2" class="kv kw jg op b gy ox ou l ov ow">       &lt;depth&gt;3&lt;/depth&gt;</span><span id="c115" class="kv kw jg op b gy ox ou l ov ow">   &lt;/size&gt;</span><span id="8987" class="kv kw jg op b gy ox ou l ov ow">   &lt;segmented&gt;0&lt;/segmented&gt;</span><span id="08c6" class="kv kw jg op b gy ox ou l ov ow">   &lt;object&gt;</span><span id="feba" class="kv kw jg op b gy ox ou l ov ow">       &lt;name&gt;with_mask&lt;/name&gt;</span><span id="fea7" class="kv kw jg op b gy ox ou l ov ow">       &lt;pose&gt;Unspecified&lt;/pose&gt;</span><span id="00dc" class="kv kw jg op b gy ox ou l ov ow">       &lt;truncated&gt;0&lt;/truncated&gt;</span><span id="4978" class="kv kw jg op b gy ox ou l ov ow">       &lt;occluded&gt;0&lt;/occluded&gt;</span><span id="c1f9" class="kv kw jg op b gy ox ou l ov ow">       &lt;difficult&gt;0&lt;/difficult&gt;</span><span id="b170" class="kv kw jg op b gy ox ou l ov ow">       &lt;bndbox&gt;</span><span id="20d2" class="kv kw jg op b gy ox ou l ov ow">            &lt;xmin&gt;70&lt;/xmin&gt;</span><span id="bdd3" class="kv kw jg op b gy ox ou l ov ow">            &lt;ymin&gt;185&lt;/ymin&gt;</span><span id="0c10" class="kv kw jg op b gy ox ou l ov ow">            &lt;xmax&gt;176&lt;/xmax&gt;</span><span id="a28e" class="kv kw jg op b gy ox ou l ov ow">            &lt;ymax&gt;321&lt;/ymax&gt;</span><span id="505e" class="kv kw jg op b gy ox ou l ov ow">       &lt;/bndbox&gt;</span><span id="9a74" class="kv kw jg op b gy ox ou l ov ow">   &lt;/object&gt;</span><span id="e3e8" class="kv kw jg op b gy ox ou l ov ow">&lt;/annotation&gt;</span></pre><p id="dfef" class="pw-post-body-paragraph lr ls jg lt b lu nj kh lw lx nk kk lz le nl mb mc li nm me mf lm nn mh mi mj ij bi translated">本项目需要的关键信息是:</p><ol class=""><li id="6899" class="ny nz jg lt b lu nj lx nk le oa li ob lm oc mj oy oe of og bi translated"><width>和<height>:图像的尺寸，单位为像素</height></width></li><li id="9e39" class="ny nz jg lt b lu oh lx oi le oj li ok lm ol mj oy oe of og bi translated">整体<bndbox> : xmin，ymin表示边界框左上角的位置，而xmax，ymax表示边界框右下角的像素</bndbox></li></ol><h2 id="fd34" class="kv kw jg bd kx ky kz dn la lb lc dp ld le lf lg lh li lj lk ll lm ln lo lp lq bi translated">格式化</h2><p id="7398" class="pw-post-body-paragraph lr ls jg lt b lu lv kh lw lx ly kk lz le ma mb mc li md me mf lm mg mh mi mj ij bi translated">在用模型训练数据之前，PASCAL VOC XML格式的注释数据必须转换成YOLO格式，每个图像一个<code class="fe oz pa pb op b">*.txt</code>文件，其规格如下:(也用下面的示例图像maksssksksss4.png说明)</p><ul class=""><li id="781c" class="ny nz jg lt b lu nj lx nk le oa li ob lm oc mj od oe of og bi translated">每个对象一行</li><li id="4761" class="ny nz jg lt b lu oh lx oi le oj li ok lm ol mj od oe of og bi translated">每一行都是<code class="fe oz pa pb op b">class x_center y_center width height</code>格式。</li><li id="14f4" class="ny nz jg lt b lu oh lx oi le oj li ok lm ol mj od oe of og bi translated">框坐标必须是标准化的xywh格式(从0到1)。如果你的盒子是以<strong class="lt jh">像素</strong>为单位，那么用图像宽度划分<code class="fe oz pa pb op b">x_center</code>和<code class="fe oz pa pb op b">width</code>，用图像高度划分<code class="fe oz pa pb op b">y_center</code>和<code class="fe oz pa pb op b">height</code>。</li><li id="20ab" class="ny nz jg lt b lu oh lx oi le oj li ok lm ol mj od oe of og bi translated">类别号是零索引的(从0开始)。</li></ul><figure class="nd ne nf ng gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi no"><img src="../Images/e7e9e38c6f01949e7d873715ba64a579.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4_TyU82M7RWy_GRCvwbaaA.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">作者图片</p></figure><p id="c7e6" class="pw-post-body-paragraph lr ls jg lt b lu nj kh lw lx nk kk lz le nl mb mc li nm me mf lm nn mh mi mj ij bi translated">我编写了一个函数，使用<a class="ae jd" href="https://docs.python.org/3/library/xml.etree.elementtree.html" rel="noopener ugc nofollow" target="_blank"> XML.etree库</a>从XML文件中提取所需的信息，并计算x_centre和y_centre。因为注释数据是以图像像素为单位的，所以我对最终值进行了归一化，以满足需求。</p><figure class="nd ne nf ng gt is"><div class="bz fp l di"><div class="pc pd l"/></div></figure><p id="779a" class="pw-post-body-paragraph lr ls jg lt b lu nj kh lw lx nk kk lz le nl mb mc li nm me mf lm nn mh mi mj ij bi translated">结果将是这样的:</p><figure class="nd ne nf ng gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi pe"><img src="../Images/ab76fb0f617f2c76dd504554d5f49bb8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nSbnLqOx-S5b_3DY3jTXzg.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">作者图片</p></figure><p id="8a25" class="pw-post-body-paragraph lr ls jg lt b lu nj kh lw lx nk kk lz le nl mb mc li nm me mf lm nn mh mi mj ij bi translated">还需要以如下所示的特定方式格式化目录，其中训练和验证图像和标签被分离到每个独立的文件夹中</p><figure class="nd ne nf ng gt is gh gi paragraph-image"><div class="gh gi pf"><img src="../Images/51e74baffc213b63eb9b53c96b25f8c1.png" data-original-src="https://miro.medium.com/v2/resize:fit:466/format:webp/1*2ELzJ8VdpxDRqAw-A4cD0g.png"/></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">作者图片</p></figure><p id="46a5" class="pw-post-body-paragraph lr ls jg lt b lu nj kh lw lx nk kk lz le nl mb mc li nm me mf lm nn mh mi mj ij bi translated">我使用<a class="ae jd" href="https://docs.python.org/3/library/pathlib.html" rel="noopener ugc nofollow" target="_blank"> Pathlib库</a>编写了另一个简单的函数。</p><figure class="nd ne nf ng gt is"><div class="bz fp l di"><div class="pc pd l"/></div></figure><p id="b8ce" class="pw-post-body-paragraph lr ls jg lt b lu nj kh lw lx nk kk lz le nl mb mc li nm me mf lm nn mh mi mj ij bi translated"><strong class="lt jh">请注意，上面提到的所有格式都可以使用</strong> <a class="ae jd" href="https://roboflow.com/" rel="noopener ugc nofollow" target="_blank"> <strong class="lt jh"> Roboflow </strong> </a> <strong class="lt jh">来完成，这是一个简单、代码化且没有麻烦的替代方法。用代码</strong>手动做是我个人的偏好</p><p id="dbdc" class="pw-post-body-paragraph lr ls jg lt b lu nj kh lw lx nk kk lz le nl mb mc li nm me mf lm nn mh mi mj ij bi translated">在训练模型之前，我们需要创建一个<em class="pg"> projectdata。yaml </em>文件，指定训练和验证图像的位置和标签数量<strong class="lt jh"> </strong>以及我们训练数据的标签名称。该文件的结构应该如下所示:</p><pre class="nd ne nf ng gt oo op oq or aw os bi"><span id="cedc" class="kv kw jg op b gy ot ou l ov ow"><em class="pg"># specify pathway which the val and training data is at<br/># Train/val/test sets as 1) dir: path/to/imgs, 2) file: path/to/imgs.txt, or 3) list: [path/to/imgs1, path/to/imgs2, ..]</em></span><span id="e597" class="kv kw jg op b gy ox ou l ov ow">path: ../mask_detection/projectdata<br/>train: images/train<br/>val: images/val<br/><br/><em class="pg"># Classes<br/></em>nc: 3<br/>names: ['no mask', 'mask worn incorrectly', 'mask on']</span></pre><h2 id="c0ee" class="kv kw jg bd kx ky kz dn la lb lc dp ld le lf lg lh li lj lk ll lm ln lo lp lq bi translated">训练模型</h2><p id="8e93" class="pw-post-body-paragraph lr ls jg lt b lu lv kh lw lx ly kk lz le ma mb mc li md me mf lm mg mh mi mj ij bi translated">为了使用自定义数据集训练模型，我在本地终端中使用以下参数运行了<em class="pg"> train.py </em>:</p><ul class=""><li id="29c1" class="ny nz jg lt b lu nj lx nk le oa li ob lm oc mj od oe of og bi translated"><strong class="lt jh"> img: </strong>输入图像尺寸</li><li id="3c88" class="ny nz jg lt b lu oh lx oi le oj li ok lm ol mj od oe of og bi translated"><strong class="lt jh">批量:</strong>批量大小</li><li id="18e1" class="ny nz jg lt b lu oh lx oi le oj li ok lm ol mj od oe of og bi translated"><strong class="lt jh">历元:</strong>历元数</li><li id="3b22" class="ny nz jg lt b lu oh lx oi le oj li ok lm ol mj od oe of og bi translated"><strong class="lt jh">数据:</strong>到<em class="pg"> projectdata.yaml </em>文件的路径</li><li id="17d3" class="ny nz jg lt b lu oh lx oi le oj li ok lm ol mj od oe of og bi translated"><strong class="lt jh"> cfg: </strong>在预先存在的模型中进行选择📁<strong class="lt jh">型号</strong></li><li id="a1ae" class="ny nz jg lt b lu oh lx oi le oj li ok lm ol mj od oe of og bi translated"><strong class="lt jh">权重:</strong>初始权重路径，默认为<em class="pg"> yolov5s.pt </em></li><li id="93a2" class="ny nz jg lt b lu oh lx oi le oj li ok lm ol mj od oe of og bi translated"><strong class="lt jh">缓存:</strong>缓存图像以加快训练速度</li></ul><p id="16b9" class="pw-post-body-paragraph lr ls jg lt b lu nj kh lw lx nk kk lz le nl mb mc li nm me mf lm nn mh mi mj ij bi translated">我选择了最小最快的型号yolov5s。我还使用模型的预训练权重进行迁移学习，而不是重新训练新的权重，这非常耗时，并且由于高处理能力要求，不可能在笔记本电脑上训练。我使用“1”的批量大小，并为10个时期训练模型。</p><pre class="nd ne nf ng gt oo op oq or aw os bi"><span id="892e" class="kv kw jg op b gy ot ou l ov ow">python mask_detection\yolov5\train.py --img 640 --batch 1 --epochs 10 --data projectdata.yaml<br/>--weights yolov5s.pt --cfg mask_detection\yolov5\models\yolov5s.yaml --cache</span></pre><figure class="nd ne nf ng gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi ph"><img src="../Images/ac90c50f85ba46adb51df650a119a8a2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kEDXgwVZMoP3cFZQFvYaog.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">作者图片</p></figure><p id="81c8" class="pw-post-body-paragraph lr ls jg lt b lu nj kh lw lx nk kk lz le nl mb mc li nm me mf lm nn mh mi mj ij bi translated">如果所有步骤都正确完成，该命令将输出以下内容并开始训练。关注mAP@.5，看看model表现如何。一旦训练开始，在“运行”文件夹下，Yolov5训练管道在测试图像上输入地面实况和预测结果，如下所示。</p><figure class="nd ne nf ng gt is gh gi paragraph-image"><div class="gh gi pi"><img src="../Images/41038c34b42e75e25e8cfbeddfba75b1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*rktUeGKGSm11t2KprEbFFQ.jpeg"/></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">地面实况训练图像。作者图片</p></figure><figure class="nd ne nf ng gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi pj"><img src="../Images/6b9e812e3a8c21eb02d4b69ee0976b23.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QMUJwLF9frezk0AbzNHA3Q.jpeg"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">测试图像。作者图片</p></figure><p id="6716" class="pw-post-body-paragraph lr ls jg lt b lu nj kh lw lx nk kk lz le nl mb mc li nm me mf lm nn mh mi mj ij bi translated">训练完成后，经过训练的模型将保存在您的“权重”文件夹/目录中，验证指标将记录在Tensorboard上。(按照建议，我选择将数据记录到wandb中)</p><figure class="nd ne nf ng gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi om"><img src="../Images/b2b3fec262180f95e9ef717cb4a3c2f6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3SxD8HBdCE9FPPbhpeaU5A.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">培训结果。作者图片</p></figure><figure class="nd ne nf ng gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi pk"><img src="../Images/57b3c452142b82e5c3fb1ef68b7e5447.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2Xy_UabMKV5HWj2XTh6jCw.jpeg"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">图像高度和宽度的相关图</p></figure><figure class="nd ne nf ng gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi pl"><img src="../Images/47219f6ab6bdc956b445421ba0c6e608.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PXrj3FOIdcCD2zG6LNh-nA.jpeg"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">数据相关图。作者图片</p></figure><h2 id="0686" class="kv kw jg bd kx ky kz dn la lb lc dp ld le lf lg lh li lj lk ll lm ln lo lp lq bi translated">目标检测模型的评估</h2><p id="f1a6" class="pw-post-body-paragraph lr ls jg lt b lu lv kh lw lx ly kk lz le ma mb mc li md me mf lm mg mh mi mj ij bi translated">受欢迎的挑战和比赛，例如:</p><ul class=""><li id="a877" class="ny nz jg lt b lu nj lx nk le oa li ob lm oc mj od oe of og bi translated"><a class="ae jd" href="http://host.robots.ox.ac.uk/pascal/VOC/voc2012/htmldoc/devkit_doc.html" rel="noopener ugc nofollow" target="_blank">帕斯卡VOC挑战</a> (Everingham等人，2010年)</li><li id="930e" class="ny nz jg lt b lu oh lx oi le oj li ok lm ol mj od oe of og bi translated"><a class="ae jd" href="http://cocodataset.org/#detection-eval" rel="noopener ugc nofollow" target="_blank">可可目标探测挑战赛</a>(林等2014)</li><li id="d497" class="ny nz jg lt b lu oh lx oi le oj li ok lm ol mj od oe of og bi translated"><a class="ae jd" href="https://storage.googleapis.com/openimages/web/object_detection_metric.html" rel="noopener ugc nofollow" target="_blank">开放影像挑战赛</a>(库兹涅佐娃2018)</li></ul><p id="28c5" class="pw-post-body-paragraph lr ls jg lt b lu nj kh lw lx nk kk lz le nl mb mc li nm me mf lm nn mh mi mj ij bi translated">所有3项挑战都使用平均精度(mAP)作为评估物体探测器的主要指标。地图到底是什么？首先，让我们来了解一些基本概念。</p><ul class=""><li id="86e3" class="ny nz jg lt b lu nj lx nk le oa li ob lm oc mj od oe of og bi translated"><strong class="lt jh">置信度:</strong>锚框包含分类器预测的对象的概率</li><li id="d462" class="ny nz jg lt b lu oh lx oi le oj li ok lm ol mj od oe of og bi translated">I <strong class="lt jh">并集上的交集(IoU): </strong>边界框的交集面积除以预测边界框的并集面积</li><li id="5dd1" class="ny nz jg lt b lu oh lx oi le oj li ok lm ol mj od oe of og bi translated"><strong class="lt jh">精度:</strong>真阳性数(<strong class="lt jh"> TP </strong>)除以真阳性数(<strong class="lt jh"> TP </strong> ) &amp;假阳性数(<strong class="lt jh"> FP </strong>)</li><li id="9707" class="ny nz jg lt b lu oh lx oi le oj li ok lm ol mj od oe of og bi translated"><strong class="lt jh">召回:</strong>真阳性的数量除以真阳性的总和(<strong class="lt jh"> TP </strong> ) &amp;假阴性(<strong class="lt jh"> FP </strong>)</li></ul><p id="f2fb" class="pw-post-body-paragraph lr ls jg lt b lu nj kh lw lx nk kk lz le nl mb mc li nm me mf lm nn mh mi mj ij bi translated">只有满足以下条件，置信度得分和IoU才用于确定预测检测是<strong class="lt jh"> TP </strong>还是<strong class="lt jh"> FP </strong>(注意:任何违反后两个条件的情况都使其成为<strong class="lt jh"> FP </strong></p><ol class=""><li id="55de" class="ny nz jg lt b lu nj lx nk le oa li ob lm oc mj oy oe of og bi translated">置信度得分&gt;阈值(如果&lt; threshold, detection counts as a False Negative (<strong class="lt jh"> FN </strong>))</li><li id="1ce0" class="ny nz jg lt b lu oh lx oi le oj li ok lm ol mj oy oe of og bi translated">预测的边界框具有比阈值更高的IoU</li><li id="0a2b" class="ny nz jg lt b lu oh lx oi le oj li ok lm ol mj oy oe of og bi translated">预测类与地面真实类相匹配</li></ol><p id="46f4" class="pw-post-body-paragraph lr ls jg lt b lu nj kh lw lx nk kk lz le nl mb mc li nm me mf lm nn mh mi mj ij bi translated">随着置信度分数的增加，回忆单调下降，而精确度可以上升和下降，但是对于这个项目，所有类别(除了不正确佩戴的面具)都增加。</p><figure class="nd ne nf ng gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi pm"><img src="../Images/7cb0af0273c511683cc46365451a6698.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*P7DQRxkzdjq_5fH5jO8inA.png"/></div></div></figure><figure class="nd ne nf ng gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi pm"><img src="../Images/11f510cf88d5c3dbbd07d91b73263638.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dt82yZ_qRnXIYouW3pjbEA.png"/></div></div></figure><figure class="nd ne nf ng gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi pm"><img src="../Images/b7f458caf5878bac07a4f646abf9b3b7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*q7hzcn09nAd0q8yVxcOR1A.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">精确召回曲线。作者图片</p></figure><p id="41b8" class="pw-post-body-paragraph lr ls jg lt b lu nj kh lw lx nk kk lz le nl mb mc li nm me mf lm nn mh mi mj ij bi translated">虽然精确召回率可以用来评估目标检测器的性能，但是不容易在不同的检测器之间进行比较。为了查看整个事情进行比较，基于精度-召回曲线的<strong class="lt jh">平均精度(AP) </strong>开始起作用。根据定义，AP正在寻找上面精确回忆曲线下的区域。<strong class="lt jh">均值平均精度(mAP) </strong>是AP的均值。</p><p id="bf65" class="pw-post-body-paragraph lr ls jg lt b lu nj kh lw lx nk kk lz le nl mb mc li nm me mf lm nn mh mi mj ij bi translated">请注意，mAP的定义和实现有所不同。</p><blockquote class="pn po pp"><p id="e9e1" class="lr ls pg lt b lu nj kh lw lx nk kk lz pq nl mb mc pr nm me mf ps nn mh mi mj ij bi translated">在PASCAL VOC挑战中，一个对象类的AP是针对0.5的IoU阈值计算的。因此，贴图是所有对象类的平均值。</p><p id="7725" class="lr ls pg lt b lu nj kh lw lx nk kk lz pq nl mb mc pr nm me mf ps nn mh mi mj ij bi translated">对于COCO对象检测挑战，该图是所有对象类别和10个IoU阈值的平均值。</p></blockquote><h2 id="53cb" class="kv kw jg bd kx ky kz dn la lb lc dp ld le lf lg lh li lj lk ll lm ln lo lp lq bi translated">推理</h2><p id="1337" class="pw-post-body-paragraph lr ls jg lt b lu lv kh lw lx ly kk lz le ma mb mc li md me mf lm mg mh mi mj ij bi translated">既然模型已经训练好了，接下来是有趣的部分:对图像或视频进行推理！在本地终端中调用detect.py，使用以下参数运行推理:(查看detect.py解析器以获得完整的参数列表)</p><ul class=""><li id="853f" class="ny nz jg lt b lu nj lx nk le oa li ob lm oc mj od oe of og bi translated"><strong class="lt jh">权重:</strong>训练模型的权重</li><li id="9c94" class="ny nz jg lt b lu oh lx oi le oj li ok lm ol mj od oe of og bi translated"><strong class="lt jh">来源:</strong>输入运行推理的文件/文件夹，0表示网络摄像头</li><li id="b105" class="ny nz jg lt b lu oh lx oi le oj li ok lm ol mj od oe of og bi translated"><strong class="lt jh"> iou-thres </strong>:非最大抑制的iou阈值，默认值:0.45</li></ul><pre class="nd ne nf ng gt oo op oq or aw os bi"><span id="f1fd" class="kv kw jg op b gy ot ou l ov ow">python yolov5\detect.py --source vid.mp4 --weights runs\train\exp\weights\best.pt --img 640 --iou-thres 0.5</span></pre><figure class="nd ne nf ng gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi pt"><img src="../Images/2d31521e8687b8bae074ac3162eead97.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*nCCIPamadXuWDxpHTwhsnw.gif"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">作者GIF</p></figure><p id="cb4b" class="pw-post-body-paragraph lr ls jg lt b lu nj kh lw lx nk kk lz le nl mb mc li nm me mf lm nn mh mi mj ij bi translated">如你所见，该模型能够检测到口罩的存在！</p><h2 id="57c8" class="kv kw jg bd kx ky kz dn la lb lc dp ld le lf lg lh li lj lk ll lm ln lo lp lq bi translated">结尾注释</h2><p id="f114" class="pw-post-body-paragraph lr ls jg lt b lu lv kh lw lx ly kk lz le ma mb mc li md me mf lm mg mh mi mj ij bi translated">这一模式远非完美，仍有改进的空间</p><ul class=""><li id="5537" class="ny nz jg lt b lu nj lx nk le oa li ob lm oc mj od oe of og bi translated">该模型不能检测不正确佩戴的面具，这是其中一个类别。这很可能是由于数据的巨大不平衡。只有一小部分数据由不正确佩戴的面具组成。一种可能的解决方案是应用数据平衡，以便该模型将更好地识别不正确佩戴的面具。</li><li id="83ee" class="ny nz jg lt b lu oh lx oi le oj li ok lm ol mj od oe of og bi translated">可以使用更大的框架，例如Yolov5x，而不是Yolov5s，以实现更高的mAP。但一个可能的缺点是训练如此庞大的模型需要额外的时间。</li></ul><p id="9d90" class="pw-post-body-paragraph lr ls jg lt b lu nj kh lw lx nk kk lz le nl mb mc li nm me mf lm nn mh mi mj ij bi translated">就这样，我的文章到此结束！这个项目对我来说真的很有趣，我喜欢学习新的和有趣的概念，尤其是在开始的时候，学习计算机视觉似乎是一项艰巨的任务，但我很高兴能够完成它！干杯！</p><p id="d349" class="pw-post-body-paragraph lr ls jg lt b lu nj kh lw lx nk kk lz le nl mb mc li nm me mf lm nn mh mi mj ij bi translated">LinkedIn简介:<a class="ae jd" href="https://www.linkedin.com/in/seanyckang/" rel="noopener ugc nofollow" target="_blank">肖恩·雅普</a></p><h2 id="2cd2" class="kv kw jg bd kx ky kz dn la lb lc dp ld le lf lg lh li lj lk ll lm ln lo lp lq bi translated">参考</h2><p id="a351" class="pw-post-body-paragraph lr ls jg lt b lu lv kh lw lx ly kk lz le ma mb mc li md me mf lm mg mh mi mj ij bi translated">[1] <a class="ae jd" href="https://arxiv.org/search/cs?searchtype=author&amp;query=Redmon%2C+J" rel="noopener ugc nofollow" target="_blank">约瑟夫·雷德蒙</a>，<a class="ae jd" href="https://arxiv.org/search/cs?searchtype=author&amp;query=Divvala%2C+S" rel="noopener ugc nofollow" target="_blank">桑托什·迪夫瓦拉</a>，<a class="ae jd" href="https://arxiv.org/search/cs?searchtype=author&amp;query=Girshick%2C+R" rel="noopener ugc nofollow" target="_blank">罗斯·吉斯克</a>，<a class="ae jd" href="https://arxiv.org/search/cs?searchtype=author&amp;query=Farhadi%2C+A" rel="noopener ugc nofollow" target="_blank">阿里·法尔哈迪</a>，<a class="ae jd" href="https://arxiv.org/pdf/1506.02640.pdf" rel="noopener ugc nofollow" target="_blank">你只看一次:统一的、实时的物体检测</a> (2015)</p><p id="0eb5" class="pw-post-body-paragraph lr ls jg lt b lu nj kh lw lx nk kk lz le nl mb mc li nm me mf lm nn mh mi mj ij bi translated">[2]吴恩达，<a class="ae jd" href="https://www.coursera.org/learn/convolutional-neural-networks/home/welcome" rel="noopener ugc nofollow" target="_blank">卷积神经网络，深度学习。AI Coursera </a></p><p id="b063" class="pw-post-body-paragraph lr ls jg lt b lu nj kh lw lx nk kk lz le nl mb mc li nm me mf lm nn mh mi mj ij bi translated">[3]由Mark Everingham、Luc Van Gool、Christopher K. I. Williams、John Winn和Andrew Zisserman提出的PASCAL视觉对象类(VOC)挑战</p></div></div>    
</body>
</html>