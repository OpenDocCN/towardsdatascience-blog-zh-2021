<html>
<head>
<title>Directing your company towards ethical AI</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">引导你的公司走向道德人工智能</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/ai-ethics-558db9895bd?source=collection_archive---------42-----------------------#2021-06-27">https://towardsdatascience.com/ai-ethics-558db9895bd?source=collection_archive---------42-----------------------#2021-06-27</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="a7dd" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">一个全面的战略，使组织能够将道德置于其人工智能建模的核心</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/8bbae2b1b4aa1be90f6cb2b325bb6d84.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eGpEhhVKMhhkgUwbIrFxeg.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">照片由<a class="ae kv" href="https://unsplash.com/@aaronburden" rel="noopener ugc nofollow" target="_blank"> Aaron Burden </a>在<a class="ae kv" href="https://unsplash.com/photos/NXt5PrOb_7U" rel="noopener ugc nofollow" target="_blank"> Unsplash </a></p></figure><p id="04d1" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">对于大多数组织来说，人工智能伦理相当于牙线:他们知道这对他们有好处，但他们更喜欢做一些事情——可能是任何事情——其他事情。但是随着来自政府、媒体和国际组织的监督越来越多，公司必须确保他们考虑他们的模型的道德性。</p><p id="bee3" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在这篇文章中，我们将探索一个产生负责任的人工智能模型的综合策略。为此，我们将考虑几个因素，包括:(<em class="ls"> 1) </em>公平性、(<em class="ls"> 2 </em>)可说明性和透明度、(<em class="ls"> 3 </em>)问责制、(<em class="ls"> 4 </em>)安全性和安全性，以及(<em class="ls"> 5 </em>)以人为本的水平和对整个社会的益处。</p><h1 id="fde2" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated">公平的</h1><p id="eb74" class="pw-post-body-paragraph kw kx iq ky b kz ml jr lb lc mm ju le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated"><strong class="ky ir">组织应该通过仔细选择模型的特征、测试模型并采用IBM的人工智能公平算法来确保他们的人工智能模型是公平的</strong></p><p id="e06c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">目前，我们生活的社会是不公平的，但我们部署的模型应该旨在使社会更加公平。因此，公平应该是一个组织所有模式的核心，尤其是因为歧视会受到严厉的法律处罚。然而，“公平”是一个主观的术语。一方面，公平可以被认为是平等地对待每个人，不管他们的情况如何。然而，另一方面，公平可以被认为是根据每个人自己的情况来对待他们。谷歌的一份报告发现，看似“公平的决定”，比如让弱势群体获得信贷，会对他们的信用评分产生负面影响。然而，机器学习模型没有上下文意识，因此也没有公平性。例如，亚马逊的人工智能招聘筛选模型对男性申请者有偏见，因为历史上男性担任这些角色。虽然没有快速解决实施人工智能模型的组织所面临的任何这些道德问题的方法，但他们应该考虑一个道德框架，这可以在模型的整个生命周期中增加其模型的公平性，包括:</p><ul class=""><li id="3d5e" class="mq mr iq ky b kz la lc ld lf ms lj mt ln mu lr mv mw mx my bi translated"><strong class="ky ir">选择功能。</strong>歧视可分为两个主要部分:有意的(差别待遇)和无意的(不同影响)。然而，避开不同的影响，并不是简单地删除受保护的功能；一些明显中立的功能可以作为受保护功能的代理。因此，组织应该仔细考虑其模型中使用的每个功能的影响。</li><li id="8b40" class="mq mr iq ky b kz mz lc na lf nb lj nc ln nd lr mv mw mx my bi translated"><strong class="ky ir">采用IBM的公平360。</strong>组织应该考虑采用IBM的AI Fairness 360 toolkit来检测和减轻给定数据集中的偏差。可以在该库中找到的三种偏差优化预处理技术是突出的，包括组辨别、个体失真和效用保持。在整个设计和开发过程中，对于人工智能模型，开发人员应该考虑使用这些技术来最小化算法中的偏差。</li><li id="95ad" class="mq mr iq ky b kz mz lc na lf nb lj nc ln nd lr mv mw mx my bi translated"><strong class="ky ir">验证模型。</strong>在开发过程中，组织的开发人员应该测试人工智能模型，包括使用“百分之八十规则”。统计学家经常使用的这条规则是通过将弱势群体的比例除以优势群体来计算的。除此之外，组织应该寻求另一个外部团队来验证模型的性能，特别是从伦理的角度。</li></ul><p id="73cc" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">关键要点:</strong></p><ul class=""><li id="a5d8" class="mq mr iq ky b kz la lc ld lf ms lj mt ln mu lr mv mw mx my bi translated">组织应该对所有的人工智能模型决策采用公平设计的方法。</li><li id="5585" class="mq mr iq ky b kz mz lc na lf nb lj nc ln nd lr mv mw mx my bi translated">公平没有放之四海而皆准的方法。然而，组织应该采纳两个核心原则:第一，记录他们的方法以最小化不同的影响；第二，清楚地证明他们所有的决定。</li></ul><h1 id="e3d7" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated"><strong class="ak">可解释且透明</strong></h1><p id="6028" class="pw-post-body-paragraph kw kx iq ky b kz ml jr lb lc mm ju le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated"><strong class="ky ir">为了指导人工智能模型的制作，组织应确保权衡与其模型相关的风险和准确性要求</strong></p><p id="d9ae" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">人工智能模型伦理中的一个关键权衡是模型的准确性和可解释性之间的权衡。一方面，人们很容易相信，在所有情况下，一个更准确的模型比它更难解释的成本更重要。当然，这在一些低风险、低影响的情况下是正确的，比如流媒体服务推荐你接下来应该看哪部电影。然而，另一方面，拥有一个难以解释的模型在很多情况下可能<em class="ls">不</em>被接受，比如向客户提供贷款。那么，一个组织应该如何决定他们的模型应该如何解释呢？首先，他们应该权衡风险和准确性要求。第二，如果缺乏透明度是可以接受的，那么组织可以考虑黑盒建模技术(如神经网络、随机森林)。然而，如果可解释性很重要，那么他们应该专注于使用可解释的模型(例如，决策树，逻辑回归)。</p><p id="865d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">随着组织拥抱人工智能，它们应该:</strong></p><ul class=""><li id="f9c7" class="mq mr iq ky b kz la lc ld lf ms lj mt ln mu lr mv mw mx my bi translated"><strong class="ky ir">明确可解释性要求。</strong>组织应与业务和法律利益相关者一起，考虑其模型的预期可解释程度。</li><li id="fe70" class="mq mr iq ky b kz mz lc na lf nb lj nc ln nd lr mv mw mx my bi translated">利用你的数据科学家。机器学习算法种类繁多，从高透明(如决策树)、部分透明(如随机森林)、低透明(如神经网络)。因此，组织应该利用他们的数据科学家来确定他们期望的用例所需的透明度级别。</li><li id="c852" class="mq mr iq ky b kz mz lc na lf nb lj nc ln nd lr mv mw mx my bi translated"><strong class="ky ir">聘请艾伦理学家。</strong>组织应该考虑聘用对该模型有深刻理解的人，并且能够根据要求解释该模型产生给定结果的方法。</li></ul><p id="41bd" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">关键要点:</strong></p><ul class=""><li id="557f" class="mq mr iq ky b kz la lc ld lf ms lj mt ln mu lr mv mw mx my bi translated">组织——尤其是银行、政府机构和制药公司——面临大量审查；因此，他们应该专注于让他们的算法变得可解释。为此，他们应该使用高度透明或部分透明的模型，除非使用透明度较低的模型(如神经网络)有明显的竞争优势，超过了与之相关的风险。然而，请注意，在大多数用例中，人类不太可能需要审查人工智能的所有决策，只针对高风险决策。</li></ul><h1 id="a779" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated">负有责任的</h1><p id="86f3" class="pw-post-body-paragraph kw kx iq ky b kz ml jr lb lc mm ju le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated"><strong class="ky ir">组织应该确保人工智能模型的设计者、开发者和管理者对他们的解决方案的社会和诉讼影响负责</strong></p><p id="01a3" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">如果不加检查，算法会影响企业、个人和社会。因此，组织制定人工智能治理政策至关重要。虽然，起初，人工智能的逻辑决策似乎是客观的；人类的判断在模型的设计和开发中起着重要的作用。因此，数据科学家和他们的管理层应该对他们的模型的行为负责。下面的列表探索了三种使组织的人工智能更加负责的方法，包括实施强有力的人工智能治理，定义与人工智能系统的后果相关的责任，以及记录所有道德决策。</p><ul class=""><li id="547a" class="mq mr iq ky b kz la lc ld lf ms lj mt ln mu lr mv mw mx my bi translated"><strong class="ky ir">治理。在承担任何人工智能建模项目之前，组织应该确保他们为他们的伦理考虑制定了清晰和可理解的政策、标准和程序。因此，最大限度地减少了关于谁对与模型生产相关的任何道德后果负责的混淆。</strong></li><li id="6dfb" class="mq mr iq ky b kz mz lc na lf nb lj nc ln nd lr mv mw mx my bi translated"><strong class="ky ir">责任。</strong>数据科学家及其管理层应该对他们的模型负责。因此，组织应该确保他们为在人工智能领域工作的员工提供相关的合规培训，以便他们了解自己的责任，以确保团队正在产生一个负责任的、对社会有益的模型。一项《快速公司》的研究发现，50%的开发人员认为开发人工智能模型的开发人员应该对他们模型的后果负责。</li><li id="13e7" class="mq mr iq ky b kz mz lc na lf nb lj nc ln nd lr mv mw mx my bi translated"><strong class="ky ir">文献资料。</strong>所有的设计决策都应该清晰地记录下来，并且能够被领导层和程序员所理解。此外，如果一个组织的顾问或承包商离职，他们应该考虑聘请全职伦理学家来监督该模型的整个生命周期，并确保任何道德问题都得到迅速有效的解决。</li></ul><p id="623e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">关键要点:</strong></p><ul class=""><li id="570a" class="mq mr iq ky b kz la lc ld lf ms lj mt ln mu lr mv mw mx my bi translated">制定清晰易懂的政策、标准和程序，概述与人工智能模型开发相关的伦理考虑。</li><li id="f621" class="mq mr iq ky b kz mz lc na lf nb lj nc ln nd lr mv mw mx my bi translated">数据科学家及其管理层将对组织的客户流失模型的含义负责。</li><li id="5136" class="mq mr iq ky b kz mz lc na lf nb lj nc ln nd lr mv mw mx my bi translated">为道德考量制作详细记录。</li></ul><h1 id="8712" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated">安全可靠</h1><p id="a36e" class="pw-post-body-paragraph kw kx iq ky b kz ml jr lb lc mm ju le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated"><strong class="ky ir">组织应该确保他们尊重用户的隐私，监控算法，并且不伤害他们的客户</strong></p><p id="ba4d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">人工智能模型应该以安全的方式实现，包括尊重客户隐私、监控算法和不造成伤害。</p><ul class=""><li id="017c" class="mq mr iq ky b kz la lc ld lf ms lj mt ln mu lr mv mw mx my bi translated"><strong class="ky ir">尊重隐私。</strong>公司越来越多地为客户提供对数据的更大控制权。然而，这种类型的数据隐私优化需要谨慎的平衡行为。一方面，收集更多数据的公司可能面临更大的诉讼处罚风险。然而，另一方面，拥有强大的数据隐私标准的公司可能会放弃数据的金钱利益。那么，组织应该做些什么？最佳策略是收集和使用不比竞争对手更多但也不少的数据。除此之外，组织应该清楚地向其客户阐明其人工智能模型的复杂性。</li><li id="0a97" class="mq mr iq ky b kz mz lc na lf nb lj nc ln nd lr mv mw mx my bi translated"><strong class="ky ir">监控算法。</strong>分析团队应确保他们清楚地阐述了人工智能模型的方法。因此，如果关键团队成员离开，组织可以回应客户提出的道德问题。此外，组织应该考虑为人工智能伦理学家创造一个永久的角色，他们将能够在人工智能算法的整个生命周期中对其进行监控。监控过程应包括考虑输入、输出和当地法规。</li><li id="d751" class="mq mr iq ky b kz mz lc na lf nb lj nc ln nd lr mv mw mx my bi translated">不伤害任何人。确保人工智能模型系统是<em class="ls">而不是</em>用来伤害组织的任何客户。举个例子，一个信用卡客户在管理他们的财务方面遇到了麻烦，可能已经欠下了大量的债务——组织鼓励这个客户向银行借更多的钱是不合适的。相反，组织应该专注于支持客户，帮助他们实现目标。如果这还不值得骄傲，那什么值得骄傲呢？因此，算法应该确保人类可以在必要时介入，以确保算法不会伤害其他个体。</li></ul><p id="2323" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">关键要点:</strong></p><ul class=""><li id="33c6" class="mq mr iq ky b kz la lc ld lf ms lj mt ln mu lr mv mw mx my bi translated">对所有与人工智能模型的设计、开发和测试相关的决策制定采用隐私设计方法。</li><li id="b838" class="mq mr iq ky b kz mz lc na lf nb lj nc ln nd lr mv mw mx my bi translated">产生一个以客户为中心的人工智能模型。</li></ul><h1 id="551f" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated"><strong class="ak">以人为本，对社会有益</strong></h1><p id="336f" class="pw-post-body-paragraph kw kx iq ky b kz ml jr lb lc mm ju le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated"><strong class="ky ir">组织应该确保他们的系统由一个人来管理，对社会有益，并且合法。为了做到这一点，高风险任务将由人来完成，低风险任务将自动完成</strong></p><p id="dba7" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在设计和开发人工智能系统时，组织应该考虑各种社会和道德因素，并且组织应该只在收益明显大于风险的情况下生产算法。三项原则在以人为本和造福社会的潜力方面都很突出。这些都是人在回路中，对社会有益的，合法的。</p><ul class=""><li id="c779" class="mq mr iq ky b kz la lc ld lf ms lj mt ln mu lr mv mw mx my bi translated"><strong class="ky ir">以人为中心。人类应该在任何时候都负责人工智能模型。但这并不意味着人类应该总是认可人工智能做出的每一个决定。然而，组织应该确保高风险决策得到了人类的支持。</strong></li><li id="ed87" class="mq mr iq ky b kz mz lc na lf nb lj nc ln nd lr mv mw mx my bi translated"><strong class="ky ir">对社会有益。人工智能应该积极地扩大社会，特别是帮助解决社会中的重大问题。然而，请注意，并不是所有对社会有益的事情都是以人为中心的。例如，中国的社会信用评分旨在保护整个社会，但它没有考虑个人的隐私需求。</strong></li><li id="572f" class="mq mr iq ky b kz mz lc na lf nb lj nc ln nd lr mv mw mx my bi translated"><strong class="ky ir">合法。</strong>组织应始终确保其人工智能模型和数字解决方案符合所有本地和域外政策、标准和程序，包括DPA '18、GDPR和数据共享协议。</li></ul><p id="42e1" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">关键要点</strong></p><ul class=""><li id="7a75" class="mq mr iq ky b kz la lc ld lf ms lj mt ln mu lr mv mw mx my bi translated">专注于确保算法有益于最终客户，并符合当地和域外法律。</li></ul><h1 id="41a0" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated">结论</h1><p id="4771" class="pw-post-body-paragraph kw kx iq ky b kz ml jr lb lc mm ju le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated">围绕人工智能的任何伦理问题都没有快速解决方案，有时会让人感到瘫痪。但教训是明确的:组织应该确保他们的系统是公平的，可以向非技术观众解释，由他们的设计师负责，尊重用户的隐私，并对社会有益。然而，更重要的是，通过确保组织将道德置于其所有人工智能模型的核心，他们可以与客户建立信任。因此，这种额外的信任可能会让他们的客户更愿意分享额外的数据，这可能会改善人工智能模型的功效，进而改善他们客户的体验，这可能比降低模型的透明度更重要。这是每个组织的使命——把客户放在第一位。</p></div></div>    
</body>
</html>