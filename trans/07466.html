<html>
<head>
<title>A detailed guide to PyTorch’s nn.Transformer() module.</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">PyTorch神经网络详细指南。变压器()模块。</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/a-detailed-guide-to-pytorchs-nn-transformer-module-c80afbc9ffb1?source=collection_archive---------0-----------------------#2021-07-08">https://towardsdatascience.com/a-detailed-guide-to-pytorchs-nn-transformer-module-c80afbc9ffb1?source=collection_archive---------0-----------------------#2021-07-08</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="d370" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">一步一步的指导，以充分了解如何实施，培训，并推断创新的变压器模型。</h2></div><p id="5da2" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我最近越来越多地涉足机器学习领域。当我在理解复杂问题或编写神经网络时遇到问题时，互联网似乎有所有的答案:从简单的线性回归到复杂的卷积网络。至少我是这么认为的…</p><p id="9ca3" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">一旦我开始在这种深度学习的事情上变得更好，我偶然发现了无与伦比的变形金刚。原论文:“<a class="ae lb" href="https://arxiv.org/abs/1706.03762" rel="noopener ugc nofollow" target="_blank">注意力是你所需要的全部</a>”，提出了一种构建神经网络的创新方法。不要再绕圈子了！本文提出了一种由重复的编码器和解码器块组成的编码器-解码器神经网络。其结构如下:</p><figure class="ld le lf lg gt lh gh gi paragraph-image"><div class="gh gi lc"><img src="../Images/5b3d89bf3cc38b02bd1b9c4cad56fa0f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1158/format:webp/0*9AyLObnUZmIlLy5a.png"/></div><p class="lk ll gj gh gi lm ln bd b be z dk translated">变压器结构:【https://arxiv.org/abs/1706.03762 T2】</p></figure><p id="e94f" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">左边的模块是编码器，右边的模块是解码器。如果你还不理解这个模型的部分，我强烈推荐阅读哈佛的“<a class="ae lb" href="https://nlp.seas.harvard.edu/2018/04/03/attention.html" rel="noopener ugc nofollow" target="_blank">带注释的变形金刚</a>”指南，在那里他们从头开始用PyTorch <strong class="kh ir">编写变形金刚模型。在本教程中，我不会涉及像“多头注意力”或“前馈层”这样的重要概念，所以你应该在继续阅读之前了解它们。如果您已经从头开始看了一遍代码，您可能会想，您是否需要为您所做的每个项目到处复制粘贴这些代码。谢天谢地，<strong class="kh ir">没有</strong>。像PyTorch和<a class="ae lb" href="https://www.tensorflow.org/" rel="noopener ugc nofollow" target="_blank"> Tensorflow </a>这样的现代python库已经通过导入包含了易于访问的transformer模型。然而，这不仅仅是导入模型并插入其中。今天我将解释如何使用和调整PyTorch <em class="lo"> nn。变压器()</em>模块。我个人努力寻找关于如何实施、培训和从中推断的信息，所以我决定为你们所有人创建我自己的指南。</strong></p><h1 id="5f5d" class="lp lq iq bd lr ls lt lu lv lw lx ly lz jw ma jx mb jz mc ka md kc me kd mf mg bi translated">第一步</h1><h2 id="8419" class="mh lq iq bd lr mi mj dn lv mk ml dp lz ko mm mn mb ks mo mp md kw mq mr mf ms bi translated">进口</h2><p id="e755" class="pw-post-body-paragraph kf kg iq kh b ki mt jr kk kl mu ju kn ko mv kq kr ks mw ku kv kw mx ky kz la ij bi translated">首先，我们需要导入PyTorch和我们将要使用的其他一些库:</p><figure class="ld le lf lg gt lh"><div class="bz fp l di"><div class="my mz l"/></div></figure><h2 id="c2c2" class="mh lq iq bd lr mi mj dn lv mk ml dp lz ko mm mn mb ks mo mp md kw mq mr mf ms bi translated">基本变压器结构</h2><p id="6bfb" class="pw-post-body-paragraph kf kg iq kh b ki mt jr kk kl mu ju kn ko mv kq kr ks mw ku kv kw mx ky kz la ij bi translated">现在，让我们仔细看看变压器模块。我建议从阅读PyTorch的文档开始。正如他们解释的那样，没有强制参数。该模块带有“你所需要的只是注意力”模型超参数。为了使用它，让我们从创建一个简单的PyTorch模型开始。我将只更改一些默认参数，这样我们的模型就不会花费不必要的时间来训练。我将这些参数作为我们课程的一部分:</p><figure class="ld le lf lg gt lh"><div class="bz fp l di"><div class="my mz l"/></div></figure><h2 id="a4e1" class="mh lq iq bd lr mi mj dn lv mk ml dp lz ko mm mn mb ks mo mp md kw mq mr mf ms bi translated">位置编码</h2><p id="506d" class="pw-post-body-paragraph kf kg iq kh b ki mt jr kk kl mu ju kn ko mv kq kr ks mw ku kv kw mx ky kz la ij bi translated">变压器模块不关心输入序列的顺序。这当然是个问题。说“我吃了一个有菠萝的披萨”和说“一个菠萝吃了我的披萨”是不一样的。谢天谢地，我们有一个解决方案:位置编码。这是一种根据位置“赋予”元素重要性的方式。关于它如何工作的详细解释可以在<a class="ae lb" href="https://kazemnejad.com/blog/transformer_architecture_positional_encoding/" rel="noopener ugc nofollow" target="_blank">这里</a>找到，但是一个简单的解释是我们为每个元素创建一个向量，代表它相对于序列中每个其他元素的位置。位置编码遵循这个看起来非常复杂的公式，实际上，我们不需要理解它:</p><figure class="ld le lf lg gt lh gh gi paragraph-image"><div role="button" tabindex="0" class="nb nc di nd bf ne"><div class="gh gi na"><img src="../Images/bcfa18e77c01c900afed457726470b1c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*WDeWlMbeFO9KCZWi.png"/></div></div><p class="lk ll gj gh gi lm ln bd b be z dk translated">位置编码公式:作者图片</p></figure><p id="840a" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">出于组织和可重用性的考虑，让我们为位置编码层创建一个单独的类(它看起来很难，但实际上只是公式、漏接和一个<a class="ae lb" rel="noopener" target="_blank" href="/residual-blocks-building-blocks-of-resnet-fd90ca15d6ec">剩余连接</a>):</p><figure class="ld le lf lg gt lh"><div class="bz fp l di"><div class="my mz l"/></div></figure><h1 id="ebeb" class="lp lq iq bd lr ls lt lu lv lw lx ly lz jw ma jx mb jz mc ka md kc me kd mf mg bi translated">完成我们的模型</h1><p id="840b" class="pw-post-body-paragraph kf kg iq kh b ki mt jr kk kl mu ju kn ko mv kq kr ks mw ku kv kw mx ky kz la ij bi translated">现在我们有了PyTorch中唯一没有包含的层，我们准备完成我们的模型。在添加位置编码之前，我们需要一个<a class="ae lb" href="https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html" rel="noopener ugc nofollow" target="_blank">嵌入层</a>，以便我们序列中的每个元素都被转换成我们可以操作的向量(而不是固定的整数)。我们还需要一个最终的线性层，这样我们就可以将模型的输出转换成我们想要的输出尺寸。最终的模型应该是这样的:</p><figure class="ld le lf lg gt lh"><div class="bz fp l di"><div class="my mz l"/></div></figure><p id="9ac1" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我知道…这看起来很吓人，但是如果你理解每个部分的作用，这实际上是一个非常简单的实现模型。</p><h2 id="9154" class="mh lq iq bd lr mi mj dn lv mk ml dp lz ko mm mn mb ks mo mp md kw mq mr mf ms bi translated">刷新一些重要信息:目标掩蔽</h2><p id="627a" class="pw-post-body-paragraph kf kg iq kh b ki mt jr kk kl mu ju kn ko mv kq kr ks mw ku kv kw mx ky kz la ij bi translated">您可能还记得模型结构中有一个特殊的模块，叫做“<strong class="kh ir">屏蔽的</strong>多头注意力”:</p><figure class="ld le lf lg gt lh gh gi paragraph-image"><div class="gh gi nf"><img src="../Images/6800275a5411df778d06e1bb8ef8fe44.png" data-original-src="https://miro.medium.com/v2/resize:fit:686/format:webp/1*HZnY5vV9JDJGItRargn-TQ.png"/></div><p class="lk ll gj gh gi lm ln bd b be z dk translated">蒙面多头注意:<a class="ae lb" href="https://arxiv.org/abs/1706.03762" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1706.03762</a></p></figure><p id="b1d6" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">那么…什么是掩蔽？在我向你解释之前，让我们快速概括一下，当我们把张量输入到模型中时，它们是怎么回事。首先，我们嵌入并编码(位置编码)我们的<strong class="kh ir">源</strong>张量。然后，我们的源张量被编码成一个难以理解的编码张量，我们将它与嵌入和编码的(位置上的)<strong class="kh ir">目标</strong>向量一起输入到我们的<strong class="kh ir">解码器</strong>。我们的模型要学习，不能只给它看整个目标张量！这会直接给他答案。</p><p id="346b" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这个问题的解决方案是一个掩蔽张量。这个张量由大小(序列长度x序列长度)组成，因为对于序列中的每个元素，我们向模型多显示了一个元素。这个矩阵将被添加到我们的目标向量中，因此矩阵将在转换器可以访问元素的位置由零组成，在转换器不能访问元素的位置由负无穷大组成。有插图的解释可能对你更有帮助:</p><figure class="ld le lf lg gt lh gh gi paragraph-image"><div role="button" tabindex="0" class="nb nc di nd bf ne"><div class="gh gi ng"><img src="../Images/895ebb0d8e833506f1051229040880d5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UJlsQ-JvWhCd_2tqdhK2hw.png"/></div></div><p class="lk ll gj gh gi lm ln bd b be z dk translated">目标矢量蒙版:作者图片</p></figure><h2 id="342a" class="mh lq iq bd lr mi mj dn lv mk ml dp lz ko mm mn mb ks mo mp md kw mq mr mf ms bi translated">刷新一些重要信息:填充遮罩</h2><p id="929b" class="pw-post-body-paragraph kf kg iq kh b ki mt jr kk kl mu ju kn ko mv kq kr ks mw ku kv kw mx ky kz la ij bi translated">如果你不知道，张量是可以存储在GPU中的矩阵，因为它们是矩阵，所以所有维度都必须有相同大小的元素。当然，在处理NLP或不同大小的图像等任务时，这种情况不会发生。因此，我们使用所谓的<strong class="kh ir">特殊令牌</strong>。这些标记允许我们的模型知道句子的开始在哪里(&lt; SOS &gt;)，句子的结束在哪里(&lt; EOS &gt;)，以及哪些元素正好填充剩余的空间，以便我们的矩阵具有sam序列大小(&lt; PAD &gt;)。这些标记还必须转换成它们相应的整数id(在我们的例子中，它们分别是2、3和4)。填充序列看起来像这样:</p><figure class="ld le lf lg gt lh gh gi paragraph-image"><div role="button" tabindex="0" class="nb nc di nd bf ne"><div class="gh gi nh"><img src="../Images/db44f27155f0d9628ecfbaac7498f7d0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wBrLXpvJJvDG94nxuoOhJg.png"/></div></div><p class="lk ll gj gh gi lm ln bd b be z dk translated">填充一批序列:作者图像</p></figure><p id="9ecb" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">为了告诉我们的模型这些标记应该是不相关的，我们使用一个二进制矩阵，其中填充标记所在的位置上有一个<em class="lo">真</em>值，填充标记不在的位置上有<em class="lo">假</em>:</p><figure class="ld le lf lg gt lh gh gi paragraph-image"><div role="button" tabindex="0" class="nb nc di nd bf ne"><div class="gh gi ni"><img src="../Images/b795a6c117ab46804c13e4cc8788f6f5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*dneSPVcOvjxj0wfi.png"/></div></div><p class="lk ll gj gh gi lm ln bd b be z dk translated">填充遮罩:作者的图像</p></figure><h2 id="b0cc" class="mh lq iq bd lr mi mj dn lv mk ml dp lz ko mm mn mb ks mo mp md kw mq mr mf ms bi translated">创建屏蔽方法</h2><p id="d5ab" class="pw-post-body-paragraph kf kg iq kh b ki mt jr kk kl mu ju kn ko mv kq kr ks mw ku kv kw mx ky kz la ij bi translated">为了创建我们讨论过的两个屏蔽矩阵，我们需要扩展我们的transformer模型。如果你懂一点NumPy，理解这些方法做什么是没有问题的。如果你看不懂，我推荐你打开一个<a class="ae lb" href="https://jupyter.org/" rel="noopener ugc nofollow" target="_blank"> Jupyter笔记本</a>，一步一步去了解他们是做什么的。</p><p id="312f" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">完整的扩展模型如下所示(请注意forward方法的变化):</p><figure class="ld le lf lg gt lh"><div class="bz fp l di"><div class="my mz l"/></div></figure><h1 id="4268" class="lp lq iq bd lr ls lt lu lv lw lx ly lz jw ma jx mb jz mc ka md kc me kd mf mg bi translated">获取我们的数据</h1><p id="b455" class="pw-post-body-paragraph kf kg iq kh b ki mt jr kk kl mu ju kn ko mv kq kr ks mw ku kv kw mx ky kz la ij bi translated">为了这个项目，我将创建一组假数据，我们可以用它来训练我们的模型。该数据将由如下序列组成:</p><ul class=""><li id="f060" class="nj nk iq kh b ki kj kl km ko nl ks nm kw nn la no np nq nr bi">1, 1, 1, 1, 1, 1, 1, 1 → 1, 1, 1, 1, 1, 1, 1, 1</li><li id="3ecd" class="nj nk iq kh b ki ns kl nt ko nu ks nv kw nw la no np nq nr bi">0, 0, 0, 0, 0, 0, 0, 0 → 0, 0, 0, 0, 0, 0, 0, 0</li><li id="b3d4" class="nj nk iq kh b ki ns kl nt ko nu ks nv kw nw la no np nq nr bi">1, 0, 1, 0, 1, 0, 1, 0 → 1, 0, 1, 0, 1, 0, 1, 0</li><li id="756c" class="nj nk iq kh b ki ns kl nt ko nu ks nv kw nw la no np nq nr bi">0, 1, 0, 1, 0, 1, 0, 1 → 0, 1, 0, 1, 0, 1, 0, 1</li></ul><p id="a105" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如果您对数据创建部分不感兴趣，请随意跳到下一节。</p><p id="9175" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我不想解释这些函数是做什么的，因为用基本的数字知识很容易理解它们。我将创建所有大小为8的句子，这样我就不需要填充，我将把它们随机组织成大小为16的批次:</p><figure class="ld le lf lg gt lh"><div class="bz fp l di"><div class="my mz l"/></div></figure><h1 id="51ef" class="lp lq iq bd lr ls lt lu lv lw lx ly lz jw ma jx mb jz mc ka md kc me kd mf mg bi translated">培训和验证</h1><h2 id="1a69" class="mh lq iq bd lr mi mj dn lv mk ml dp lz ko mm mn mb ks mo mp md kw mq mr mf ms bi translated">培养</h2><p id="c423" class="pw-post-body-paragraph kf kg iq kh b ki mt jr kk kl mu ju kn ko mv kq kr ks mw ku kv kw mx ky kz la ij bi translated">现在我们有了数据，可以开始训练我们的模型了。让我们从创建我们的模型、损失函数和优化器的实例开始。我们将使用<a class="ae lb" href="https://pytorch.org/docs/stable/generated/torch.optim.SGD.html" rel="noopener ugc nofollow" target="_blank">随机梯度下降优化器</a>、<a class="ae lb" href="https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html" rel="noopener ugc nofollow" target="_blank">交叉熵损失函数</a>，以及0.01的学习率。我也将使用我的图形卡进行此培训，因为它需要的时间较少，但这不是必需的。</p><figure class="ld le lf lg gt lh"><div class="bz fp l di"><div class="my mz l"/></div></figure><p id="9638" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在继续之前，我们需要理解的一个重要概念是，我们作为输入给转换器的目标张量必须向右移动一(与目标输出张量相比)。换句话说，我们想要给模型用于训练的张量必须在开始时有一个额外的元素，在结束时少一个元素，并且我们用来计算损失函数的张量必须向另一个方向移动。这是因为如果我们在推理过程中给模型一个元素，它就会给我们下一个元素。</p><figure class="ld le lf lg gt lh gh gi paragraph-image"><div role="button" tabindex="0" class="nb nc di nd bf ne"><div class="gh gi nx"><img src="../Images/f241d351f0ec087a5a81f88a1aef7625.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*87DnBur3EX5XL3SGGt-gRA.png"/></div></div><p class="lk ll gj gh gi lm ln bd b be z dk translated">目标向量转换:作者图片</p></figure><p id="8c0f" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">既然我们已经掌握了这个概念，让我们开始编码吧！训练循环是标准训练循环，除了:</p><ul class=""><li id="707b" class="nj nk iq kh b ki kj kl km ko nl ks nm kw nn la no np nq nr bi translated">目标张量在预测期间被传递给模型</li><li id="2af9" class="nj nk iq kh b ki ns kl nt ko nu ks nv kw nw la no np nq nr bi translated">生成目标掩码来隐藏接下来的单词</li><li id="c598" class="nj nk iq kh b ki ns kl nt ko nu ks nv kw nw la no np nq nr bi translated">可能还会生成填充遮罩并将其传递给模型</li></ul><figure class="ld le lf lg gt lh"><div class="bz fp l di"><div class="my mz l"/></div></figure><h2 id="d75a" class="mh lq iq bd lr mi mj dn lv mk ml dp lz ko mm mn mb ks mo mp md kw mq mr mf ms bi translated">确认</h2><p id="9058" class="pw-post-body-paragraph kf kg iq kh b ki mt jr kk kl mu ju kn ko mv kq kr ks mw ku kv kw mx ky kz la ij bi translated">验证循环与我们的训练循环完全相同，只是我们不读取或更新梯度:</p><figure class="ld le lf lg gt lh"><div class="bz fp l di"><div class="my mz l"/></div></figure><h2 id="cff5" class="mh lq iq bd lr mi mj dn lv mk ml dp lz ko mm mn mb ks mo mp md kw mq mr mf ms bi translated">执行培训和验证</h2><p id="b37a" class="pw-post-body-paragraph kf kg iq kh b ki mt jr kk kl mu ju kn ko mv kq kr ks mw ku kv kw mx ky kz la ij bi translated">在本例中，我为模型训练了10个时期。为了简化训练，我创建了一个fit函数，它在每个时期调用训练和验证循环，并打印损失:</p><figure class="ld le lf lg gt lh"><div class="bz fp l di"><div class="my mz l"/></div></figure><p id="84f3" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这会产生以下结果</p><pre class="ld le lf lg gt ny nz oa ob aw oc bi"><span id="7857" class="mh lq iq nz b gy od oe l of og">Training and validating model<br/>------------------------- Epoch 1 -------------------------<br/>Training loss: 0.5878<br/>Validation loss: 0.4172</span><span id="66d2" class="mh lq iq nz b gy oh oe l of og">------------------------- Epoch 2 -------------------------<br/>Training loss: 0.4384<br/>Validation loss: 0.3981</span><span id="d12f" class="mh lq iq nz b gy oh oe l of og">------------------------- Epoch 3 -------------------------<br/>Training loss: 0.4155<br/>Validation loss: 0.3852</span><span id="1130" class="mh lq iq nz b gy oh oe l of og">------------------------- Epoch 4 -------------------------<br/>Training loss: 0.4003<br/>Validation loss: 0.3700</span><span id="1249" class="mh lq iq nz b gy oh oe l of og">------------------------- Epoch 5 -------------------------<br/>Training loss: 0.3842<br/>Validation loss: 0.3443</span><span id="be70" class="mh lq iq nz b gy oh oe l of og">------------------------- Epoch 6 -------------------------<br/>Training loss: 0.3592<br/>Validation loss: 0.3069</span><span id="14bd" class="mh lq iq nz b gy oh oe l of og">------------------------- Epoch 7 -------------------------<br/>Training loss: 0.3291<br/>Validation loss: 0.2652</span><span id="9313" class="mh lq iq nz b gy oh oe l of og">------------------------- Epoch 8 -------------------------<br/>Training loss: 0.2956<br/>Validation loss: 0.2195</span><span id="5493" class="mh lq iq nz b gy oh oe l of og">------------------------- Epoch 9 -------------------------<br/>Training loss: 0.2684<br/>Validation loss: 0.1947</span><span id="f5d4" class="mh lq iq nz b gy oh oe l of og">------------------------- Epoch 10 ------------------------- Training loss: 0.2501<br/>Validation loss: 0.1930</span></pre><h1 id="fd56" class="lp lq iq bd lr ls lt lu lv lw lx ly lz jw ma jx mb jz mc ka md kc me kd mf mg bi translated">结果</h1><p id="4670" class="pw-post-body-paragraph kf kg iq kh b ki mt jr kk kl mu ju kn ko mv kq kr ks mw ku kv kw mx ky kz la ij bi translated">训练后，我们获得每个时期的以下损失:</p><figure class="ld le lf lg gt lh gh gi paragraph-image"><div class="gh gi oi"><img src="../Images/6e2e0fb4c3b5a7d797c10cb9a171a383.png" data-original-src="https://miro.medium.com/v2/resize:fit:1358/format:webp/1*2Kgu1mMuO6Y7aysJSOgAqA.png"/></div><p class="lk ll gj gh gi lm ln bd b be z dk translated">训练结果:作者图片</p></figure><h1 id="53c9" class="lp lq iq bd lr ls lt lu lv lw lx ly lz jw ma jx mb jz mc ka md kc me kd mf mg bi translated">推理</h1><p id="ec85" class="pw-post-body-paragraph kf kg iq kh b ki mt jr kk kl mu ju kn ko mv kq kr ks mw ku kv kw mx ky kz la ij bi translated">我们可以看到，我们的模型似乎学到了一些东西。是时候检查它是否存在了，但是…我们如何检查它呢？对于我们从未见过的数据，我们没有目标张量。这就是移动我们的输入目标和输出目标张量产生影响的地方。正如我们之前看到的，当给定一个元素时，我们的模型学会了预测下一个令牌。因此，我们应该能够给我们的模型输入张量和开始令牌，它应该给我们返回下一个元素。如果当模型预测一个令牌时，我们将它与我们以前的输入连接起来，我们应该能够慢慢地将单词添加到我们的输出中，直到我们的模型预测到<eos>令牌。</eos></p><p id="4e68" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">以下是该过程的代码:</p><figure class="ld le lf lg gt lh"><div class="bz fp l di"><div class="my mz l"/></div></figure><p id="5314" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">运行这段代码的输出是:</p><pre class="ld le lf lg gt ny nz oa ob aw oc bi"><span id="2d19" class="mh lq iq nz b gy od oe l of og">Example 0<br/>Input: [0, 0, 0, 0, 0, 0, 0, 0]<br/>Continuation: [0, 0, 0, 0, 0, 0, 0, 0, 0]</span><span id="e3e8" class="mh lq iq nz b gy oh oe l of og">Example 1<br/>Input: [1, 1, 1, 1, 1, 1, 1, 1]<br/>Continuation: [1, 1, 1, 1, 1, 1, 1, 1, 1]</span><span id="a135" class="mh lq iq nz b gy oh oe l of og">Example 2<br/>Input: [1, 0, 1, 0, 1, 0, 1, 0]<br/>Continuation: [1, 0, 1, 0, 1, 0, 1, 0]</span><span id="c3c1" class="mh lq iq nz b gy oh oe l of og">Example 3<br/>Input: [0, 1, 0, 1, 0, 1, 0, 1]<br/>Continuation: [1, 0, 1, 0, 1, 0, 1, 0]</span><span id="4731" class="mh lq iq nz b gy oh oe l of og">Example 4<br/>Input: [0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0]<br/>Continuation: [0, 1, 0, 1, 0, 1, 0, 1, 0]</span><span id="cb77" class="mh lq iq nz b gy oh oe l of og">Example 5<br/>Input: [0, 1]<br/>Continuation: [0, 1, 0, 1, 0, 1, 0]</span></pre><p id="ead2" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">所以这个模型确实得到了我们序列的要点，但是当它试图预测延续时仍然会犯一些错误。例如，在“示例4”中，模型应该预测1作为第一个令牌，因为输入的结尾是0。我们还可以看到，在推理过程中，我们的句子不需要有相同的长度，输出也不会有相同的长度(见“例5”)。</p><h1 id="90c4" class="lp lq iq bd lr ls lt lu lv lw lx ly lz jw ma jx mb jz mc ka md kc me kd mf mg bi translated">结论</h1><p id="53a3" class="pw-post-body-paragraph kf kg iq kh b ki mt jr kk kl mu ju kn ko mv kq kr ks mw ku kv kw mx ky kz la ij bi translated">我相信这篇文章可以帮助许多初级/中级机器学习开发人员学习如何在PyTorch中使用transformer模型，并且，由于其他语言中的结构是相同的，所以这篇教程可能也对其他框架(如Tensorflow)有用(希望如此)。</p><p id="bc79" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如果您有任何建议或发现任何错误，请随时留下评论，我会尽快修复它。</p><p id="d6fa" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">全Colab笔记本:</strong><a class="ae lb" href="https://colab.research.google.com/drive/15gyTrsd-OU6YZVyjwis48ysrXFPEcv9r" rel="noopener ugc nofollow" target="_blank">https://drive . Google . com/file/d/15 gy trsd-ou 6 yzvyjwis 48 ysrxfpecv 9 r/view？usp =共享</a></p><p id="9fed" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">【https://danielmelchor.com】网站:<a class="ae lb" href="https://danielmelchor.com" rel="noopener ugc nofollow" target="_blank"/></p><p id="1d7f" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">联系人:</strong><a class="ae lb" href="mailto:dmh672@gmail.com" rel="noopener ugc nofollow" target="_blank">dmh672@gmail.com</a></p></div></div>    
</body>
</html>